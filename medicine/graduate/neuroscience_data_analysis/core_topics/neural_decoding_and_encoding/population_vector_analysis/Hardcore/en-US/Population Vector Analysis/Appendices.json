{
    "hands_on_practices": [
        {
            "introduction": "Before we can analyze a neural population, we must first understand its components: the individual neurons. This exercise provides fundamental practice in estimating a neuron's directional tuning curve from its spike train, a critical first step in many population-level analyses. You will implement a widely-used statistical method, the Generalized Linear Model (GLM) with a Poisson noise assumption, to extract a neuron's preferred direction vector from synthetic spike count data .",
            "id": "4186979",
            "problem": "You are given a set of synthetic spike count datasets for single neurons recorded under a set of planar stimulus directions. Each dataset consists of repeated trials at several stimulus angles. The neuronal response is assumed to follow a Generalized Linear Model (GLM), where spike counts are modeled as conditionally independent Poisson random variables with a canonical logarithmic link function. The GLM uses a first-harmonic basis of direction on the circle. Your task is to implement a method to estimate the direction-dependent tuning of each neuron from spike counts and then extract the neuron's preferred direction vector used in population vector analysis.\n\nFundamental base:\n- Empirical evidence supports modeling spike counts $y$ as a Poisson random variable with mean rate $\\lambda$ over a fixed counting window, that is $y \\sim \\mathrm{Poisson}(\\lambda)$.\n- In a Generalized Linear Model (GLM), the conditional mean $\\lambda$ is related to a linear predictor $\\eta$ through a link function $g$, with $g(\\lambda) = \\eta$; the canonical link for the Poisson is the logarithmic function, $g(\\lambda) = \\log \\lambda$.\n- A direction $\\theta$ on the circle can be represented via the first harmonic basis functions $\\phi_1(\\theta) = 1$, $\\phi_2(\\theta) = \\cos \\theta$, and $\\phi_3(\\theta) = \\sin \\theta$.\n\nProblem requirements:\n1. For each dataset, construct a design matrix $X \\in \\mathbb{R}^{N \\times 3}$ with rows $X_n = \\left[1, \\cos \\theta_n, \\sin \\theta_n\\right]$ for $n = 1, \\dots, N$, where $N$ is the total number of trials across all angles. Assume the GLM form $g(\\lambda_n) = X_n \\boldsymbol{\\beta}$ with $g(\\lambda_n) = \\log \\lambda_n$ and parameter vector $\\boldsymbol{\\beta} \\in \\mathbb{R}^3$ to be estimated by maximum likelihood from observed spike counts $y_n$.\n2. Use Newton–Raphson or Iteratively Reweighted Least Squares (IRLS) to compute the maximum likelihood estimate $\\widehat{\\boldsymbol{\\beta}}$. You must derive the update using the Poisson log-likelihood and its derivatives, ensuring numerically stable updates.\n3. From the fitted $\\widehat{\\boldsymbol{\\beta}} = \\left[\\widehat{\\beta}_0, \\widehat{\\beta}_1, \\widehat{\\beta}_2\\right]^\\top$, extract a neuron's preferred direction and strength on the circle as follows:\n   - Define the preferred angle $\\widehat{\\varphi} = \\mathrm{atan2}\\left(\\widehat{\\beta}_2, \\widehat{\\beta}_1\\right)$ in radians.\n   - Define the first-harmonic magnitude $\\widehat{m} = \\sqrt{\\widehat{\\beta}_1^2 + \\widehat{\\beta}_2^2}$.\n   - Define the preferred direction vector used in population vector analysis as $\\mathbf{p} = \\left[\\widehat{\\beta}_1, \\widehat{\\beta}_2\\right]^\\top$, which equals $\\widehat{m}\\left[\\cos \\widehat{\\varphi}, \\sin \\widehat{\\varphi}\\right]^\\top$ by trigonometric identities.\n4. Boundary handling: if $\\widehat{m}$ equals $0$ within a numerical tolerance, set $\\widehat{\\varphi} = 0$ by convention and $\\mathbf{p} = [0, 0]^\\top$.\n5. Angle unit specification: all angles $\\theta$ and the preferred angle $\\widehat{\\varphi}$ must be expressed in radians.\n6. Data generation for testing: You must generate synthetic datasets using the GLM and Poisson noise model with a fixed pseudorandom number generator seed. For each test case, let angles be uniformly spaced on $[0, 2\\pi)$, specifically $\\theta_k = \\frac{2\\pi k}{M}$ for $k = 0, 1, \\dots, M - 1$ and repeat each angle $R$ times. For each trial at angle $\\theta$, generate a spike count $y$ from $\\mathrm{Poisson}\\left(\\lambda(\\theta)\\right)$, where $\\lambda(\\theta) = \\exp\\left(\\beta_0 + \\beta_1 \\cos \\theta + \\beta_2 \\sin \\theta\\right)$. Use the fixed seed $12345$ for the pseudorandom number generator to ensure reproducibility.\n7. Test suite: Implement your solution for the following parameter sets, each defining one dataset.\n   - Case $1$: $(\\beta_0, \\beta_1, \\beta_2) = (2.0, 0.8, 0.4)$, $M = 16$, $R = 40$.\n   - Case $2$: $(\\beta_0, \\beta_1, \\beta_2) = (1.0, 0.0, 0.0)$, $M = 16$, $R = 40$.\n   - Case $3$: $(\\beta_0, \\beta_1, \\beta_2) = (1.2, -0.6, 0.6)$, $M = 12$, $R = 25$.\n   - Case $4$: $(\\beta_0, \\beta_1, \\beta_2) = (3.0, 0.05, -0.05)$, $M = 8$, $R = 5$.\n8. Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case in order and is itself a list of four floats $[\\widehat{\\varphi}, \\widehat{m}, p_x, p_y]$. For example, the output should look like $[[\\widehat{\\varphi}_1, \\widehat{m}_1, p_{x1}, p_{y1}], [\\widehat{\\varphi}_2, \\widehat{m}_2, p_{x2}, p_{y2}], \\dots]$.\n\nYour implementation must be self-contained and runnable, generate the synthetic data internally with the specified seed, estimate the GLM parameters, and report the final preferred angle in radians and preferred direction vector components for each test case.",
            "solution": "The user-provided problem is assessed as valid. It is scientifically grounded in the principles of computational neuroscience and statistical modeling, well-posed with a unique and stable solution, and expressed in objective, formal language. It presents a standard but non-trivial task: fitting a Generalized Linear Model (GLM) to neuronal spike count data and extracting parameters of neuroscientific interest. All necessary data, models, and constraints are provided, and there are no contradictions or ambiguities. I will therefore proceed with a complete solution.\n\n### 1. Model Formulation\n\nThe response variable, the neuronal spike count $y_n$ for a given trial $n$, is assumed to follow a Poisson distribution. The probability mass function is given by:\n$$\nP(y_n | \\lambda_n) = \\frac{\\lambda_n^{y_n} e^{-\\lambda_n}}{y_n!}\n$$\nwhere $\\lambda_n$ is the mean spike rate for that trial.\n\nThe GLM framework connects this mean rate $\\lambda_n$ to a set of predictors through a link function. The problem specifies a canonical logarithmic link function, $g(\\lambda) = \\log \\lambda$. The linear predictor, $\\eta_n$, is a linear combination of basis functions of the stimulus direction $\\theta_n$.\nThe relationship is thus:\n$$\ng(\\lambda_n) = \\log \\lambda_n = \\eta_n\n$$\nInverting the link function gives the mean rate as a function of the linear predictor:\n$$\n\\lambda_n = e^{\\eta_n}\n$$\nThe problem uses a first-harmonic basis for direction on the circle, $[1, \\cos \\theta, \\sin \\theta]$. The parameter vector to be estimated is $\\boldsymbol{\\beta} = [\\beta_0, \\beta_1, \\beta_2]^\\top$. The linear predictor for trial $n$ with stimulus direction $\\theta_n$ is:\n$$\n\\eta_n = \\beta_0 \\cdot 1 + \\beta_1 \\cos \\theta_n + \\beta_2 \\sin \\theta_n = X_n \\boldsymbol{\\beta}\n$$\nwhere $X_n = [1, \\cos \\theta_n, \\sin \\theta_n]$ is the $n$-th row of the design matrix $X$. The full model for the spike rate is:\n$$\n\\lambda_n(\\theta_n; \\boldsymbol{\\beta}) = \\exp(\\beta_0 + \\beta_1 \\cos \\theta_n + \\beta_2 \\sin \\theta_n)\n$$\n\n### 2. Maximum Likelihood Estimation\n\nTo find the optimal parameter vector $\\widehat{\\boldsymbol{\\beta}}$, we maximize the log-likelihood of the observed data. Given a dataset of $N$ independent trials with spike counts $\\boldsymbol{y} = [y_1, \\dots, y_N]^\\top$ and corresponding design matrix $X$, the total log-likelihood is the sum of individual log-likelihoods:\n$$\n\\mathcal{L}(\\boldsymbol{\\beta}) = \\sum_{n=1}^{N} \\log P(y_n | \\lambda_n) = \\sum_{n=1}^{N} \\left( y_n \\log \\lambda_n - \\lambda_n - \\log(y_n!) \\right)\n$$\nSubstituting $\\lambda_n = \\exp(X_n \\boldsymbol{\\beta})$ and dropping the constant term $\\log(y_n!)$ which does not depend on $\\boldsymbol{\\beta}$:\n$$\n\\mathcal{L}(\\boldsymbol{\\beta}) \\propto \\sum_{n=1}^{N} \\left( y_n (X_n \\boldsymbol{\\beta}) - e^{X_n \\boldsymbol{\\beta}} \\right)\n$$\nTo maximize this function, we use an iterative numerical optimization method. The problem suggests Newton-Raphson or its equivalent for GLMs, Iteratively Reweighted Least Squares (IRLS). This requires the gradient (score vector) and the Hessian matrix of the log-likelihood function.\n\n**Gradient (Score Vector):**\nThe gradient of the log-likelihood with respect to $\\boldsymbol{\\beta}$ is a vector of partial derivatives $\\nabla_{\\boldsymbol{\\beta}} \\mathcal{L} = \\left[ \\frac{\\partial \\mathcal{L}}{\\partial \\beta_0}, \\frac{\\partial \\mathcal{L}}{\\partial \\beta_1}, \\frac{\\partial \\mathcal{L}}{\\partial \\beta_2} \\right]^\\top$. Using the chain rule:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{\\beta}} = \\sum_{n=1}^N \\frac{\\partial \\mathcal{L}_n}{\\partial \\lambda_n} \\frac{\\partial \\lambda_n}{\\partial \\eta_n} \\frac{\\partial \\eta_n}{\\partial \\boldsymbol{\\beta}}\n$$\nThe components are: $\\frac{\\partial \\mathcal{L}_n}{\\partial \\lambda_n} = \\frac{y_n}{\\lambda_n} - 1$, $\\frac{\\partial \\lambda_n}{\\partial \\eta_n} = e^{\\eta_n} = \\lambda_n$, and $\\frac{\\partial \\eta_n}{\\partial \\boldsymbol{\\beta}} = X_n^\\top$.\nCombining these gives:\n$$\n\\nabla_{\\boldsymbol{\\beta}} \\mathcal{L}_n = \\left(\\frac{y_n}{\\lambda_n} - 1\\right) \\lambda_n X_n^\\top = (y_n - \\lambda_n) X_n^\\top\n$$\nSumming over all $N$ observations yields the full gradient:\n$$\n\\boldsymbol{g}(\\boldsymbol{\\beta}) = \\nabla_{\\boldsymbol{\\beta}} \\mathcal{L} = \\sum_{n=1}^N (y_n - \\lambda_n) X_n^\\top = X^\\top (\\boldsymbol{y} - \\boldsymbol{\\lambda})\n$$\nwhere $\\boldsymbol{\\lambda} = [\\lambda_1, \\dots, \\lambda_N]^\\top$.\n\n**Hessian Matrix:**\nThe Hessian matrix $H$ contains the second partial derivatives, $H_{jk} = \\frac{\\partial^2 \\mathcal{L}}{\\partial \\beta_j \\partial \\beta_k}$.\n$$\n\\frac{\\partial^2 \\mathcal{L}}{\\partial \\boldsymbol{\\beta} \\partial \\boldsymbol{\\beta}^\\top} = \\frac{\\partial}{\\partial \\boldsymbol{\\beta}^\\top} \\left( \\sum_{n=1}^N (y_n - \\lambda_n) X_n^\\top \\right) = \\sum_{n=1}^N - \\frac{\\partial \\lambda_n}{\\partial \\boldsymbol{\\beta}^\\top} X_n^\\top\n$$\nSince $\\frac{\\partial \\lambda_n}{\\partial \\boldsymbol{\\beta}^\\top} = \\lambda_n X_n$:\n$$\nH(\\boldsymbol{\\beta}) = \\sum_{n=1}^N - \\lambda_n X_n^\\top X_n = - X^\\top W X\n$$\nwhere $W$ is a diagonal matrix with elements $W_{nn} = \\lambda_n$. For a Poisson GLM with a log link, the log-likelihood is globally concave, as the Hessian is negative semidefinite (and typically negative definite), ensuring a unique maximum.\n\n**Newton-Raphson Update:**\nThe Newton-Raphson algorithm iteratively refines an estimate $\\boldsymbol{\\beta}^{(t)}$ by moving in the direction that maximizes the quadratic approximation of the function:\n$$\n\\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} - [H(\\boldsymbol{\\beta}^{(t)})]^{-1} \\boldsymbol{g}(\\boldsymbol{\\beta}^{(t)})\n$$\nSubstituting our gradient and Hessian:\n$$\n\\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} - [-X^\\top W^{(t)} X]^{-1} [X^\\top (\\boldsymbol{y} - \\boldsymbol{\\lambda}^{(t)})]\n$$\n$$\n\\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} + (X^\\top W^{(t)} X)^{-1} X^\\top (\\boldsymbol{y} - \\boldsymbol{\\lambda}^{(t)})\n$$\nThis is the core update rule. In practice, we solve the linear system $ (X^\\top W^{(t)} X) \\Delta \\boldsymbol{\\beta} = X^\\top (\\boldsymbol{y} - \\boldsymbol{\\lambda}^{(t)}) $ for $\\Delta \\boldsymbol{\\beta}$ and update $\\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} + \\Delta \\boldsymbol{\\beta}$. This is numerically more stable than computing the matrix inverse directly. This update is also equivalent to the IRLS algorithm, where at each step one solves a weighted least squares problem.\n\n### 3. Algorithmic Implementation and Parameter Extraction\n\nThe algorithm proceeds as follows:\n1.  **Data Generation**: For each test case, generate stimulus angles $\\theta_k$ and repeat them $R$ times. Construct the design matrix $X$. Using the true $\\boldsymbol{\\beta}$, compute the mean rates $\\lambda_n$ and generate spike counts $y_n$ from $\\mathrm{Poisson}(\\lambda_n)$. The PRNG seed is fixed to $12345$ for reproducibility.\n2.  **Initialization**: Start the iterative estimation with an initial guess, e.g., $\\boldsymbol{\\beta}^{(0)} = \\boldsymbol{0}$.\n3.  **Iteration**: Loop until convergence:\n    a. Calculate the current linear predictor $\\boldsymbol{\\eta}^{(t)} = X \\boldsymbol{\\beta}^{(t)}$.\n    b. Compute the mean rates $\\boldsymbol{\\lambda}^{(t)} = \\exp(\\boldsymbol{\\eta}^{(t)})$.\n    c. Form the weight matrix $W^{(t)} = \\mathrm{diag}(\\boldsymbol{\\lambda}^{(t)})$.\n    d. Calculate the update step $\\Delta \\boldsymbol{\\beta}$ by solving the system $(X^\\top W^{(t)} X) \\Delta \\boldsymbol{\\beta} = X^\\top (\\boldsymbol{y} - \\boldsymbol{\\lambda}^{(t)})$.\n    e. Update the parameters: $\\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} + \\Delta \\boldsymbol{\\beta}$.\n    f. Check for convergence, e.g., if the norm of the update step $||\\Delta \\boldsymbol{\\beta}||$ is smaller than a pre-defined tolerance.\n4.  **Parameter Extraction**: Once the algorithm converges to the MLE $\\widehat{\\boldsymbol{\\beta}} = [\\widehat{\\beta}_0, \\widehat{\\beta}_1, \\widehat{\\beta}_2]^\\top$, extract the quantities of interest:\n    -   The first-harmonic magnitude, a measure of tuning strength: $\\widehat{m} = \\sqrt{\\widehat{\\beta}_1^2 + \\widehat{\\beta}_2^2}$.\n    -   The preferred direction vector used in population vector analysis: $\\mathbf{p} = [\\widehat{\\beta}_1, \\widehat{\\beta}_2]^\\top$.\n    -   The preferred angle in radians: $\\widehat{\\varphi} = \\mathrm{atan2}(\\widehat{\\beta}_2, \\widehat{\\beta}_1)$.\n5.  **Boundary Handling**: A special case is required when the neuron has no directional tuning, meaning $\\widehat{\\beta}_1$ and $\\widehat{\\beta}_2$ are both zero. Due to numerical noise, they will be small but non-zero. If the magnitude $\\widehat{m}$ is below a small numerical tolerance, we set the preferred angle $\\widehat{\\varphi}$ to $0$ by convention and the preferred direction vector $\\mathbf{p}$ to $[0, 0]^\\top$.\n\nThis completes the theoretical framework and algorithmic design for solving the problem. The final Python code will implement these steps.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem. It generates synthetic neural data,\n    fits a Poisson GLM to estimate tuning parameters, and formats the output.\n    \"\"\"\n\n    def generate_data(betas_true, M, R, rng):\n        \"\"\"\n        Generates synthetic spike count data for a single neuron.\n\n        Args:\n            betas_true (np.ndarray): True GLM parameters [beta0, beta1, beta2].\n            M (int): Number of unique stimulus directions.\n            R (int): Number of repeated trials per direction.\n            rng (np.random.Generator): A numpy random number generator.\n\n        Returns:\n            tuple: A tuple containing the design matrix X (np.ndarray) and\n                   the spike counts y (np.ndarray).\n        \"\"\"\n        # Create stimulus angles uniformly spaced on [0, 2*pi)\n        angles_base = np.linspace(0.0, 2.0 * np.pi, M, endpoint=False)\n        all_angles = np.repeat(angles_base, R)\n        N = M * R\n\n        # Construct the design matrix X\n        X = np.c_[np.ones(N), np.cos(all_angles), np.sin(all_angles)]\n\n        # Generate spike counts from the Poisson GLM\n        eta = X @ betas_true\n        lambdas = np.exp(eta)\n        y = rng.poisson(lambdas)\n\n        return X, y\n\n    def fit_poisson_glm(X, y, max_iter=100, tol=1e-8):\n        \"\"\"\n        Fits a Poisson GLM with a log link using Newton-Raphson/IRLS.\n\n        Args:\n            X (np.ndarray): The design matrix.\n            y (np.ndarray): The vector of observed spike counts.\n            max_iter (int): Maximum number of iterations for the optimization.\n            tol (float): Convergence tolerance for the norm of the beta update.\n\n        Returns:\n            np.ndarray: The estimated GLM parameters (beta_hat).\n        \"\"\"\n        # Initialize beta parameters to zero\n        beta = np.zeros(X.shape[1])\n\n        for _ in range(max_iter):\n            # Current estimate of linear predictor and mean rate\n            eta = X @ beta\n            lambdas = np.exp(eta)\n\n            # Gradient (score vector)\n            grad = X.T @ (y - lambdas)\n\n            # Hessian matrix components\n            # W is a diagonal matrix of weights (the lambdas)\n            # The Hessian is H = -X.T @ W @ X\n            W_diag = lambdas\n            \n            # The system to solve is (X.T @ W @ X) * delta_beta = grad\n            # This is the Newton-Raphson update step\n            # delta_beta = inv(H) @ grad = -inv(-X.T @ W @ X) @ grad\n            # To solve for delta_beta, we use np.linalg.solve for stability\n            # A = X.T @ W @ X\n            # b = grad\n            # A @ delta_beta = b -> delta_beta = solve(A, b)\n            \n            # We construct the matrix for the linear system solver.\n            # Using (X.T * W_diag[:, None]) is an efficient way to compute X.T @ W\n            A = X.T @ (W_diag[:, np.newaxis] * X)\n\n            # Solve the system for the update step delta_beta\n            try:\n                delta_beta = np.linalg.solve(A, grad)\n            except np.linalg.LinAlgError:\n                # In case of singular matrix, stop and return current estimate\n                # This may happen with pathological data, though unlikely here\n                break\n\n            # Update beta\n            beta = beta + delta_beta\n\n            # Check for convergence\n            if np.linalg.norm(delta_beta) < tol:\n                break\n        \n        return beta\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # ( (beta0, beta1, beta2), M, R )\n        ((2.0, 0.8, 0.4), 16, 40),\n        ((1.0, 0.0, 0.0), 16, 40),\n        ((1.2, -0.6, 0.6), 12, 25),\n        ((3.0, 0.05, -0.05), 8, 5)\n    ]\n\n    # Fixed seed for reproducibility\n    seed = 12345\n    rng = np.random.default_rng(seed)\n\n    results = []\n    # Machine epsilon based tolerance for checking if magnitude is zero\n    m_tol = np.finfo(float).eps * 10.0\n\n    for case in test_cases:\n        betas_true, M, R = case\n        \n        # 1. Generate synthetic data\n        X, y = generate_data(np.array(betas_true), M, R, rng)\n\n        # 2. Fit GLM to get estimated parameters\n        beta_hat = fit_poisson_glm(X, y)\n\n        # 3. Extract preferred direction and related quantities\n        b0_hat, b1_hat, b2_hat = beta_hat\n        \n        m_hat = np.sqrt(b1_hat**2 + b2_hat**2)\n\n        if m_hat < m_tol:\n            # Boundary handling for untuned neurons\n            phi_hat = 0.0\n            px, py = 0.0, 0.0\n        else:\n            phi_hat = np.arctan2(b2_hat, b1_hat)\n            px, py = b1_hat, b2_hat\n        \n        # Store results for this case\n        results.append([phi_hat, m_hat, px, py])\n\n    # Final print statement in the exact required format.\n    # The string representation of a list is exactly what's needed.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "With the ability to characterize individual neural tuning, we can now assemble a population and use the classic population vector algorithm to decode a stimulus. This simulation-based exercise explores the mechanics of the population vector decoder and, more importantly, investigates how its accuracy is affected by biologically realistic nonlinearities such as firing rate rectification. By quantifying the decoding bias introduced by these nonlinearities, you will gain crucial intuition about the limitations of idealized models and the sources of error in neural decoding .",
            "id": "4186987",
            "problem": "Consider a two-dimensional population of direction-selective neurons with uniformly distributed preferred directions. The preferred direction of neuron $i$ is denoted by $\\phi_i \\in [0,2\\pi)$, and the stimulus direction is denoted by $\\theta \\in [0,2\\pi)$. The neurons follow a directional tuning based on a cosine function before any nonlinearity, where the pre-nonlinearity response is $r_i^{\\text{pre}} = b + g \\cos(\\theta - \\phi_i)$, with $b$ the baseline and $g$ the gain. Two nonlinear distortions are applied to this response: half-wave rectification and thresholding. Under half-wave rectification, the observed response is $r_i^{\\text{rect}} = \\max(0, r_i^{\\text{pre}})$. Under thresholding followed by rectification, the observed response is $r_i^{\\text{thresh}} = \\max(0, r_i^{\\text{pre}} - T)$, where $T$ is a nonnegative threshold.\n\nPopulation vector decoding reconstructs an estimate of the stimulus direction by vectorially summing each neuron’s contribution, which is proportional to its observed response and points along its preferred direction. The resultant vector $\\mathbf{V}$ aggregates these contributions over all neurons, and the decoded direction $\\hat{\\theta}$ is the argument of $\\mathbf{V}$ computed from its Cartesian components. Angles must be expressed in radians throughout. If the resultant vector has zero magnitude for a particular condition (no active neuron), define the decoding error for that condition to be $\\pi$.\n\nStarting from the core definition of cosine directional tuning and the standard population vector decoding construction, design a programmatic simulation to quantify how half-wave rectification and thresholding distort the decoded direction relative to the ideal cosine-tuned case. For a given parameter set $(N,g,b,T,\\theta)$, where $N$ is the number of neurons, compute:\n- the decoding error under rectification, defined as $e_{\\text{rect}} = |\\mathrm{wrap}_{\\pi}(\\hat{\\theta}_{\\text{rect}} - \\theta)|$,\n- the decoding error under thresholding, defined as $e_{\\text{thresh}} = |\\mathrm{wrap}_{\\pi}(\\hat{\\theta}_{\\text{thresh}} - \\theta)|$,\nwhere $\\mathrm{wrap}_{\\pi}(\\alpha)$ returns the unique angle in $(-\\pi,\\pi]$ congruent to $\\alpha$ modulo $2\\pi$. The ideal cosine-tuned case serves as the unbiased reference, where the decode matches $\\theta$ under uniform preferred directions and no nonlinearity.\n\nYour program must implement the following test suite of parameter values, with angles in radians:\n- Test case $1$: $N = 64$, $g = 1.0$, $b = 0.2$, $T = 0.0$, $\\theta = \\pi/4$.\n- Test case $2$: $N = 64$, $g = 1.0$, $b = 0.2$, $T = 1.0$, $\\theta = \\pi/4$.\n- Test case $3$: $N = 4$, $g = 1.0$, $b = 0.0$, $T = 0.0$, $\\theta = \\pi/2$.\n- Test case $4$: $N = 8$, $g = 1.0$, $b = -0.3$, $T = 0.1$, $\\theta = -\\pi/3$.\n- Test case $5$: $N = 360$, $g = 1.0$, $b = 0.0$, $T = 0.5$, $\\theta = 2.5$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered as $[e_{\\text{rect},1}, e_{\\text{thresh},1}, e_{\\text{rect},2}, e_{\\text{thresh},2}, \\dots, e_{\\text{rect},5}, e_{\\text{thresh},5}]$, where $e_{\\text{rect},k}$ and $e_{\\text{thresh},k}$ denote the errors for test case $k$. All error values must be expressed in radians as real numbers without units in the output.",
            "solution": "### Step 1: Extract Givens\n- **Population Model**: A two-dimensional population of $N$ direction-selective neurons.\n- **Preferred Directions**: The preferred direction of neuron $i$ is $\\phi_i$, which are uniformly distributed in $[0, 2\\pi)$. For a finite population of size $N$ in a simulation, this is interpreted as being evenly spaced, i.e., $\\phi_i = \\frac{2\\pi i}{N}$ for $i=0, 1, \\dots, N-1$.\n- **Stimulus Direction**: $\\theta \\in [0, 2\\pi)$.\n- **Pre-nonlinearity Response**: The response before nonlinearity is $r_i^{\\text{pre}} = b + g \\cos(\\theta - \\phi_i)$, where $b$ is the baseline and $g$ is the gain.\n- **Nonlinear Response Models**:\n    1.  **Half-wave Rectification**: $r_i^{\\text{rect}} = \\max(0, r_i^{\\text{pre}})$.\n    2.  **Thresholding**: $r_i^{\\text{thresh}} = \\max(0, r_i^{\\text{pre}} - T)$, where $T$ is a nonnegative threshold.\n- **Population Vector Decoding**:\n    - The resultant population vector is $\\mathbf{V}$. Its Cartesian components for a generic response $r_i$ are $V_x = \\sum_i r_i \\cos(\\phi_i)$ and $V_y = \\sum_i r_i \\sin(\\phi_i)$.\n    - The decoded direction is $\\hat{\\theta} = \\mathrm{atan2}(V_y, V_x)$.\n- **Zero Magnitude Condition**: If $|\\mathbf{V}|=0$, the decoding error is defined as $\\pi$.\n- **Decoding Error**:\n    - For rectification: $e_{\\text{rect}} = |\\mathrm{wrap}_{\\pi}(\\hat{\\theta}_{\\text{rect}} - \\theta)|$.\n    - For thresholding: $e_{\\text{thresh}} = |\\mathrm{wrap}_{\\pi}(\\hat{\\theta}_{\\text{thresh}} - \\theta)|$.\n- **Wrap Function**: $\\mathrm{wrap}_{\\pi}(\\alpha)$ maps an angle $\\alpha$ to the unique congruent angle in the interval $(-\\pi, \\pi]$.\n- **Test Cases**:\n    - Case $1$: $(N, g, b, T, \\theta) = (64, 1.0, 0.2, 0.0, \\pi/4)$.\n    - Case $2$: $(N, g, b, T, \\theta) = (64, 1.0, 0.2, 1.0, \\pi/4)$.\n    - Case $3$: $(N, g, b, T, \\theta) = (4, 1.0, 0.0, 0.0, \\pi/2)$.\n    - Case $4$: $(N, g, b, T, \\theta) = (8, 1.0, -0.3, 0.1, -\\pi/3)$.\n    - Case $5$: $(N, g, b, T, \\theta) = (360, 1.0, 0.0, 0.5, 2.5)$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is well-grounded in computational neuroscience. Cosine tuning curves, half-wave rectification, and thresholding are standard, albeit simplified, models of neural activity. Population vector analysis is a classical method for decoding information from neural ensembles.\n- **Well-Posed**: The problem is well-posed. All parameters and functions are explicitly defined. The instruction to use evenly spaced preferred directions for the simulation provides a deterministic basis for computation. The procedure for calculating the decoded angle and the error is specified unambiguously. The edge case of a zero-magnitude population vector is explicitly handled.\n- **Objective**: The problem statement is objective, quantitative, and free of subjective or speculative content.\n\n**Flaw Checklist**:\n1.  **Scientific or Factual Unsoundness**: The model is a valid scientific simplification. No flaws.\n2.  **Non-Formalizable or Irrelevant**: The problem is formalizable as a computational simulation and is directly relevant to its stated field. No flaws.\n3.  **Incomplete or Contradictory Setup**: The setup is complete and consistent. All necessary parameters are provided for each test case. No flaws.\n4.  **Unrealistic or Infeasible**: The parameters are within a reasonable range for a theoretical simulation. No flaws.\n5.  **Ill-Posed or Poorly Structured**: The problem is clearly structured, and the definitions (e.g., `atan2`, `wrap_pi`, error metric) ensure a unique, meaningful solution for each test case. No flaws.\n6.  **Pseudo-Profound, Trivial, or Tautological**: The problem requires a non-trivial simulation and an understanding of how nonlinearities introduce decoding biases, which is a conceptually important topic in the field. No flaws.\n7.  **Outside Scientific Verifiability**: The results of the simulation are fully verifiable by repeating the specified calculations. No flaws.\n\n### Step 3: Verdict and Action\nThe problem is **valid** as it is scientifically grounded, well-posed, and objective. A programmatic solution can be designed based on the provided specifications.\n\n### Solution\nThe problem requires a simulation to compute the decoding error of a population vector decoder under two different nonlinear response functions: half-wave rectification and thresholding. The core of the solution is to implement the population vector decoding algorithm for a given set of neural and stimulus parameters.\n\n**1. Theoretical Framework**\n\nThe population vector $\\mathbf{V}$ is a weighted sum of vectors pointing in the preferred directions $\\phi_i$ of each neuron in the population. The weight for each neuron is its firing rate $r_i$.\n$$\n\\mathbf{V} = \\sum_{i=0}^{N-1} r_i \\begin{pmatrix} \\cos(\\phi_i) \\\\ \\sin(\\phi_i) \\end{pmatrix}\n$$\nThe Cartesian components of the population vector are:\n$$\nV_x = \\sum_{i=0}^{N-1} r_i \\cos(\\phi_i) \\\\\nV_y = \\sum_{i=0}^{N-1} r_i \\sin(\\phi_i)\n$$\nThe decoded stimulus direction, $\\hat{\\theta}$, is the angle of this resultant vector. It is computed using the two-argument arctangent function to resolve quadrant ambiguity:\n$$\n\\hat{\\theta} = \\mathrm{atan2}(V_y, V_x)\n$$\nThe ideal linear model with a pure cosine tuning curve, $r_i = g \\cos(\\theta - \\phi_i)$, and a sufficiently large, uniformly distributed population of neurons, results in a population vector that points exactly in the stimulus direction $\\theta$, thus yielding zero decoding error. The inclusion of a baseline firing rate $b$ does not introduce a bias for a uniform population, as the contributions from $b$ sum to zero vectorially.\n\nNonlinearities such as rectification and thresholding disrupt this symmetry. They selectively silence neurons whose pre-nonlinearity response is below a certain level. This selective removal of contributions breaks the vector summation symmetry, typically causing the population vector to be biased and thus introducing a decoding error.\n\n**2. Simulation Algorithm**\n\nFor each parameter set $(N, g, b, T, \\theta)$, the simulation proceeds as follows:\n\n**Step 2.1: Initialize Neuronal Population**\nAn array of $N$ preferred directions $\\phi_i$ is generated, representing the neuronal population. These directions are spaced uniformly over the interval $[0, 2\\pi)$:\n$$\n\\phi_i = \\frac{2\\pi i}{N} \\quad \\text{for } i = 0, 1, \\dots, N-1\n$$\n\n**Step 2.2: Compute Neuronal Responses**\nFirst, the pre-nonlinearity response $r_i^{\\text{pre}}$ is calculated for all neurons based on the stimulus direction $\\theta$:\n$$\nr_i^{\\text{pre}} = b + g \\cos(\\theta - \\phi_i)\n$$\nNext, the two nonlinear response models are applied:\n-   **Half-wave Rectification**: The response is set to zero if it is negative.\n    $$\n    r_i^{\\text{rect}} = \\max(0, r_i^{\\text{pre}})\n    $$\n-   **Thresholding**: The response is first reduced by the threshold $T$ and then rectified.\n    $$\n    r_i^{\\text{thresh}} = \\max(0, r_i^{\\text{pre}} - T)\n    $$\n\n**Step 2.3: Compute Population Vectors**\nTwo population vectors are computed, one for each response model.\n\n-   For **rectification**:\n    $$\n    V_{x, \\text{rect}} = \\sum_{i=0}^{N-1} r_i^{\\text{rect}} \\cos(\\phi_i) \\\\\n    V_{y, \\text{rect}} = \\sum_{i=0}^{N-1} r_i^{\\text{rect}} \\sin(\\phi_i)\n    $$\n-   For **thresholding**:\n    $$\n    V_{x, \\text{thresh}} = \\sum_{i=0}^{N-1} r_i^{\\text{thresh}} \\cos(\\phi_i) \\\\\n    V_{y, \\text{thresh}} = \\sum_{i=0}^{N-1} r_i^{\\text{thresh}} \\sin(\\phi_i)\n    $$\n\n**Step 2.4: Decode Stimulus Direction**\nThe decoded angle is found for each model. A check for zero active neurons (i.e., zero vector magnitude) is performed. If all $r_i$ are zero for a given model, the total vector magnitude is zero, and the error is defined as $\\pi$. Otherwise, the angle is computed via $\\mathrm{atan2}$:\n$$\n\\hat{\\theta}_{\\text{rect}} = \\mathrm{atan2}(V_{y, \\text{rect}}, V_{x, \\text{rect}}) \\\\\n\\hat{\\theta}_{\\text{thresh}} = \\mathrm{atan2}(V_{y, \\text{thresh}}, V_{x, \\text{thresh}})\n$$\n\n**Step 2.5: Calculate Decoding Error**\nThe decoding error is the absolute value of the wrapped angular difference between the decoded angle $\\hat{\\theta}$ and the true stimulus angle $\\theta$. The wrapping function $\\mathrm{wrap}_{\\pi}(\\alpha)$ ensures the difference lies in $(-\\pi, \\pi]$.\n$$\n\\Delta\\theta = \\hat{\\theta} - \\theta \\\\\ne = |\\mathrm{wrap}_{\\pi}(\\Delta\\theta)|\n$$\nTo implement $\\mathrm{wrap}_{\\pi}(\\alpha)$, we can map $\\alpha$ to $[-\\pi, \\pi)$ using the modulo operator, and then explicitly handle the lower boundary case where the result is $-\\pi$ by mapping it to $\\pi$.\n$$\n\\alpha' = (\\alpha + \\pi) \\pmod{2\\pi} - \\pi\n$$\nIf $\\alpha' = -\\pi$, it is mapped to $\\pi$. This procedure yields the required angle in $(-\\pi, \\pi]$.\n\nThis completes the simulation for one set of parameters. The process is repeated for all test cases provided.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the population vector decoding problem for a suite of test cases.\n    \"\"\"\n\n    # Test cases defined in the problem statement.\n    # Format: (N, g, b, T, theta)\n    test_cases = [\n        (64, 1.0, 0.2, 0.0, np.pi / 4),\n        (64, 1.0, 0.2, 1.0, np.pi / 4),\n        (4, 1.0, 0.0, 0.0, np.pi / 2),\n        (8, 1.0, -0.3, 0.1, -np.pi / 3),\n        (360, 1.0, 0.0, 0.5, 2.5),\n    ]\n\n    results = []\n\n    def wrap_to_pi(x):\n        \"\"\"\n        Wraps angle(s) to the interval (-pi, pi].\n        The expression (x + np.pi) % (2 * np.pi) - np.pi maps to [-pi, pi).\n        This function adjusts the lower bound -pi to pi to match the interval (-pi, pi].\n        \"\"\"\n        y = (x + np.pi) % (2 * np.pi) - np.pi\n        # Use np.where for vectorized conditional logic\n        return np.where(y == -np.pi, np.pi, y)\n\n    def calculate_errors(N, g, b, T, theta):\n        \"\"\"\n        Calculates decoding errors for one set of parameters.\n        \"\"\"\n        # Step 1: Generate uniformly spaced preferred directions\n        phi = np.linspace(0, 2 * np.pi, N, endpoint=False)\n\n        # Step 2: Calculate pre-nonlinearity and post-nonlinearity responses\n        r_pre = b + g * np.cos(theta - phi)\n        r_rect = np.maximum(0, r_pre)\n        r_thresh = np.maximum(0, r_pre - T)\n\n        # Calculate error for rectification\n        # Check for zero magnitude vector (no active neurons)\n        if np.isclose(np.sum(r_rect), 0.0):\n            e_rect = np.pi\n        else:\n            # Step 3: Compute population vector components\n            vx_rect = np.sum(r_rect * np.cos(phi))\n            vy_rect = np.sum(r_rect * np.sin(phi))\n            \n            # Step 4: Decode direction\n            theta_hat_rect = np.arctan2(vy_rect, vx_rect)\n            \n            # Step 5: Calculate decoding error\n            delta_theta_rect = theta_hat_rect - theta\n            e_rect = np.abs(wrap_to_pi(delta_theta_rect))\n\n        # Calculate error for thresholding\n        # Check for zero magnitude vector (no active neurons)\n        if np.isclose(np.sum(r_thresh), 0.0):\n            e_thresh = np.pi\n        else:\n            # Step 3: Compute population vector components\n            vx_thresh = np.sum(r_thresh * np.cos(phi))\n            vy_thresh = np.sum(r_thresh * np.sin(phi))\n\n            # Step 4: Decode direction\n            theta_hat_thresh = np.arctan2(vy_thresh, vx_thresh)\n\n            # Step 5: Calculate decoding error\n            delta_theta_thresh = theta_hat_thresh - theta\n            e_thresh = np.abs(wrap_to_pi(delta_theta_thresh))\n            \n        return e_rect, e_thresh\n\n    # Iterate through all test cases\n    for case in test_cases:\n        N, g, b, T, theta = case\n        e_rect, e_thresh = calculate_errors(N, g, b, T, theta)\n        results.extend([e_rect, e_thresh])\n\n    # Format the final output string\n    # Using a high precision format to avoid rounding issues in verification\n    formatted_results = [f\"{r:.15f}\".rstrip('0').rstrip('.') for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Moving beyond analyzing a given population, this advanced practice addresses a fundamental question in experimental design: how do we select a subset of neurons that forms an optimal code? This exercise introduces the concept of Fisher Information, $J(\\theta)$, a powerful tool for quantifying the amount of information a neural population encodes about a stimulus. You will derive a tractable criterion for maximizing information and implement a greedy algorithm to select a neuron subset that provides robust encoding across all stimulus directions, thereby minimizing the theoretical lower bound on decoding error .",
            "id": "4186953",
            "problem": "You are given a set of candidate neurons intended to encode a circular variable (direction) using cosine tuning. Each neuron has a preferred direction, a baseline rate, and a modulation amplitude. Your task is to design and implement an algorithm to select a subset of neurons of fixed size that maximizes coverage of the circle so as to minimize decoding error, and to justify the selection criterion from first principles.\n\nThe neural encoding model is specified as follows. There are $N$ candidate neurons indexed by $i \\in \\{1,\\dots,N\\}$. Each neuron has a preferred direction $\\phi_i \\in [0, 2\\pi)$ (in radians), a baseline firing rate $b_i > 0$ (in spikes per second), and a cosine modulation amplitude $a_i \\ge 0$ (in spikes per second). The mean firing rate of neuron $i$ when the true stimulus direction is $\\theta \\in [0,2\\pi)$ is\n$$\nr_i(\\theta) = b_i + a_i \\cos(\\theta - \\phi_i).\n$$\nAssume that the observed spike counts from each neuron are independent and distributed according to a Poisson process with mean rate $r_i(\\theta)$ over a fixed observation window of duration $1$ second (so the rate equals the expected count). Consider decoding $\\theta$ using any unbiased estimator; a canonical benchmark is the Population Vector (PV) decoder, defined as the angle of the vector sum of individual preferred direction unit vectors weighted by observed activity. For a given subset of neurons, the Cramér-Rao Lower Bound (CRLB) implies that the variance of any unbiased estimator $\\hat{\\theta}$ of $\\theta$ is bounded below by the reciprocal of the Fisher Information (FI). For independent Poisson observations with mean rates $r_i(\\theta)$, the scalar FI for the parameter $\\theta$ is\n$$\nJ(\\theta) = \\sum_{i \\in S} \\frac{\\left(\\frac{d}{d\\theta} r_i(\\theta)\\right)^2}{r_i(\\theta)} = \\sum_{i \\in S} \\frac{\\left(a_i \\sin(\\theta - \\phi_i)\\right)^2}{b_i + a_i \\cos(\\theta - \\phi_i)},\n$$\nwhere $S \\subset \\{1,\\dots,N\\}$ is the selected subset of neurons. The CRLB states $\\mathrm{Var}(\\hat{\\theta}) \\ge \\frac{1}{J(\\theta)}$.\n\nThe design objective is to select a subset $S$ of fixed cardinality $\\lvert S \\rvert = K$ that maximizes coverage of the circle in a way that minimizes worst-case or average decoding error quantified via the CRLB. Directly maximizing $\\min_{\\theta} J(\\theta)$ over all subsets is combinatorial. You must propose a tractable selection criterion based on an analytically justified surrogate and implement a selection algorithm that approximates the optimal coverage.\n\nStarting from the core definitions above, derive a surrogate criterion that is valid when modulation amplitudes are small relative to baselines, and show how this surrogate motivates a coverage-maximizing algorithm based on angular diversity of preferred directions. Then implement the algorithm to choose $K$ neurons from $N$, and evaluate the exact FI and the approximate FI over a dense grid of stimulus angles for the selected subset, reporting both the average and the worst-case CRLB values. Angles must be treated in radians, and CRLB values must be reported in radians squared.\n\nImplement the following algorithmic requirements:\n- Define selection weights $w_i$ as a function of $a_i$ and $b_i$ based on your analytical justification. Use these weights to embody the contribution of neuron $i$ to FI under the small-amplitude regime.\n- Transform preferred directions to a space where uniform coverage corresponds to minimizing concentration of a harmonic component that dominates FI variation with $\\theta$. Use this to guide a greedy max-min coverage algorithm:\n  1. Initialize the subset $S$ with the index $i$ that has the largest weight $w_i$.\n  2. Iteratively add to $S$ the neuron $j \\notin S$ that most reduces the magnitude of the current weighted harmonic sum when added, with a tie-break by larger $w_j$, and then by smaller index.\n- After selecting $S$, compute the exact FI $J(\\theta)$ over a uniform grid $\\Theta$ of $M$ angles in $[0,2\\pi)$, where $M$ is specified below for each test case. Also compute the small-amplitude approximation to FI,\n$$\nJ_{\\text{approx}}(\\theta) = \\sum_{i \\in S} \\frac{a_i^2}{b_i} \\sin^2(\\theta - \\phi_i) = \\frac{1}{2}\\sum_{i \\in S} \\frac{a_i^2}{b_i} \\left(1 - \\cos\\left(2(\\theta - \\phi_i)\\right)\\right).\n$$\n- Report, for each test case, four quantities: the average CRLB using exact FI, the worst-case CRLB using exact FI, the average CRLB using approximate FI, and the worst-case CRLB using approximate FI. The averages are computed over the grid $\\Theta$. Worst-case means the maximum CRLB over $\\Theta$.\n\nPhysical and numerical units:\n- Angles must be in radians.\n- CRLB values must be expressed in radians squared.\n- CRLB values must be real-valued floats.\n\nTest suite and parameters:\nUse the following test suite of three parameter sets. For each case, $N$ is the number of candidate neurons, $\\phi$ is the list of preferred directions (in radians), $a$ is the list of modulation amplitudes (in spikes per second), $b$ is the list of baselines (in spikes per second), $K$ is the subset size to select, and $M$ is the number of angle samples in the evaluation grid $\\Theta$.\n\n- Case $1$ (happy path with near-uniform candidates):\n  - $N = 12$\n  - $\\phi = [0, \\frac{\\pi}{6}, \\frac{\\pi}{3}, \\frac{\\pi}{2}, \\frac{2\\pi}{3}, \\frac{5\\pi}{6}, \\pi, \\frac{7\\pi}{6}, \\frac{4\\pi}{3}, \\frac{3\\pi}{2}, \\frac{5\\pi}{3}, \\frac{11\\pi}{6}]$\n  - $a_i = 5$ for all $i$\n  - $b_i = 20$ for all $i$\n  - $K = 6$\n  - $M = 360$\n\n- Case $2$ (clustered preferred directions with heterogeneous amplitudes):\n  - $N = 10$\n  - $\\phi = [0, 0.1, 0.2, \\frac{\\pi}{2}, \\frac{\\pi}{2} + 0.05, \\pi, \\pi + 0.1, \\frac{3\\pi}{2}, \\frac{3\\pi}{2} + 0.2, \\frac{3\\pi}{2} + 0.3]$\n  - $a = [8, 6, 4, 7, 5, 9, 3, 6, 4, 2]$\n  - $b_i = 25$ for all $i$\n  - $K = 5$\n  - $M = 720$\n\n- Case $3$ (heterogeneous baselines and amplitudes, potential edge behavior):\n  - $N = 8$\n  - $\\phi = [0, \\frac{\\pi}{4}, \\frac{\\pi}{2}, \\frac{3\\pi}{4}, \\pi, \\frac{5\\pi}{4}, \\frac{3\\pi}{2}, \\frac{7\\pi}{4}]$\n  - $a = [3, 10, 2, 8, 4, 6, 1, 9]$\n  - $b = [15, 30, 12, 28, 18, 22, 16, 26]$\n  - $K = 4$\n  - $M = 512$\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a list of four floats in the order $[\\text{avg\\_CRLB\\_exact}, \\text{worst\\_CRLB\\_exact}, \\text{avg\\_CRLB\\_approx}, \\text{worst\\_CRLB\\_approx}]$. For example, the output must have the form $[[x_1,y_1,z_1,w_1],[x_2,y_2,z_2,w_2],[x_3,y_3,z_3,w_3]]$ with no additional text. All floats must be in radians squared.",
            "solution": "The problem statement is scientifically grounded, well-posed, and contains all necessary information to derive and implement a solution. The models provided, such as cosine tuning curves and Poisson spike statistics, are canonical in computational neuroscience. The use of Fisher Information and the Cramér-Rao Lower Bound as a measure of decoding performance is a standard and rigorous approach. The task of deriving a surrogate optimization criterion from a principled approximation is a non-trivial and valid scientific reasoning exercise. Therefore, the problem is deemed valid.\n\nThe core of the problem is to select a subset of $K$ neurons from a pool of $N$ candidates to optimize the encoding of a circular variable $\\theta$. The quality of encoding is inversely related to the variance of an optimal decoder, which is bounded by the Cramér-Rao Lower Bound (CRLB), $\\mathrm{Var}(\\hat{\\theta}) \\ge J(\\theta)^{-1}$, where $J(\\theta)$ is the Fisher Information. To ensure good decoding for all possible stimuli, a desirable property for the selected population is that the Fisher Information $J(\\theta)$ be as large and as constant as possible across all angles $\\theta \\in [0, 2\\pi)$. This is equivalent to minimizing the worst-case CRLB, $\\max_{\\theta} J(\\theta)^{-1}$.\n\nThe exact Fisher Information for a set of independent Poisson neurons $S$ is given by:\n$$\nJ(\\theta) = \\sum_{i \\in S} \\frac{\\left( \\frac{d}{d\\theta} r_i(\\theta) \\right)^2}{r_i(\\theta)} = \\sum_{i \\in S} \\frac{(-a_i \\sin(\\theta - \\phi_i))^2}{b_i + a_i \\cos(\\theta - \\phi_i)}\n$$\nDirectly maximizing $\\min_{\\theta} J(\\theta)$ over all $\\binom{N}{K}$ possible subsets $S$ is computationally intractable. The problem thus requires the derivation of a tractable surrogate criterion based on a simplifying assumption.\n\n**Derivation of the Surrogate Criterion**\n\nWe are asked to consider the regime where modulation amplitudes are small compared to baseline firing rates, i.e., $a_i \\ll b_i$. In this case, the denominator of the Fisher Information term for neuron $i$ can be approximated by its dominant term:\n$$\nb_i + a_i \\cos(\\theta - \\phi_i) \\approx b_i\n$$\nThis is the zeroth-order term in the Taylor expansion of the denominator around $a_i/b_i = 0$. Substituting this into the exact Fisher Information formula yields the approximate Fisher Information, $J_{\\text{approx}}(\\theta)$:\n$$\nJ_{\\text{approx}}(\\theta) = \\sum_{i \\in S} \\frac{a_i^2 \\sin^2(\\theta - \\phi_i)}{b_i}\n$$\nLet us define a weight $w_i = a_i^2/b_i$ for each neuron. This weight represents the neuron's potential contribution to the Fisher Information under this approximation. It is proportional to the square of its \"signal\" ($a_i$) and inversely proportional to its baseline rate $b_i$, which contributes to the overall Poisson noise. With this definition, the approximate FI becomes:\n$$\nJ_{\\text{approx}}(\\theta) = \\sum_{i \\in S} w_i \\sin^2(\\theta - \\phi_i)\n$$\nTo analyze the dependence on $\\theta$, we use the identity $\\sin^2(x) = \\frac{1 - \\cos(2x)}{2}$:\n$$\nJ_{\\text{approx}}(\\theta) = \\sum_{i \\in S} w_i \\left( \\frac{1 - \\cos(2(\\theta - \\phi_i))}{2} \\right) = \\frac{1}{2}\\sum_{i \\in S} w_i - \\frac{1}{2}\\sum_{i \\in S} w_i \\cos(2\\theta - 2\\phi_i)\n$$\nThe first term, $\\frac{1}{2}\\sum_{i \\in S} w_i$, is the average value of $J_{\\text{approx}}(\\theta)$ over the circle. The second term represents the fluctuation of the FI as a function of $\\theta$. To make the FI as uniform and high as possible, we must pursue two goals:\n1. Maximize the average FI, which means maximizing $\\sum_{i \\in S} w_i$.\n2. Minimize the amplitude of the FI fluctuations, which means minimizing the magnitude of the sum $\\sum_{i \\in S} w_i \\cos(2\\theta - 2\\phi_i)$ for all $\\theta$.\n\nThe fluctuating part can be analyzed by representing it as the real part of a complex vector sum. Let us define a complex number $C_S$ for a subset $S$ as:\n$$\nC_S = \\sum_{i \\in S} w_i e^{i 2\\phi_i}\n$$\nThen the fluctuating term is $-\\frac{1}{2} \\text{Re} \\left\\{ \\sum_{i \\in S} w_i e^{-i2\\theta} e^{i2\\phi_i} \\right\\} = -\\frac{1}{2} \\text{Re} \\left\\{ e^{-i2\\theta} C_S \\right\\}$. The amplitude of this oscillation is $\\frac{1}{2}|C_S|$.\nTherefore, our surrogate optimization problem is to select a subset $S$ of size $K$ that maximizes $\\sum_{i \\in S} w_i$ while simultaneously minimizing $|C_S| = |\\sum_{i \\in S} w_i e^{i2\\phi_i}|$. The term $e^{i2\\phi_i}$ shows that we are interested in the uniformity of preferred directions in a \"doubled-angle\" space, weighted by each neuron's information potential $w_i$.\n\n**Algorithmic Design**\n\nThe problem specifies a greedy algorithm to approximately solve this dual objective. The algorithm is as follows:\n1.  **Initialization**: Calculate the weights $w_i = a_i^2/b_i$ for all $N$ neurons. To maximize the average FI from the start, initialize the selected set $S$ with the single neuron having the largest weight $w_i$.\n2.  **Greedy Selection**: Iteratively add $K-1$ more neurons to $S$. At each step, select a neuron $j \\notin S$ that, when added, results in the smallest possible magnitude for the complex sum $C_{S \\cup \\{j\\}}$. That is, for the current sum $C_S = \\sum_{k \\in S} w_k e^{i2\\phi_k}$, choose the neuron $j \\notin S$ that minimizes $|\\,C_S + w_j e^{i2\\phi_j}\\,|$. This step directly targets the objective of minimizing FI fluctuation.\n3.  **Tie-Breaking**: If multiple candidate neurons produce the same minimal magnitude, the tie is broken by choosing the one with the larger weight $w_j$. If there is still a tie, it is broken by choosing the neuron with the smaller original index $j$.\n\n**Evaluation**\n\nAfter the selection of the subset $S$, the performance is evaluated using both the exact FI, $J(\\theta)$, and the approximate FI, $J_{\\text{approx}}(\\theta)$, on a fine grid of $M$ stimulus angles $\\Theta$. The reported metrics are the average and worst-case (maximum) CRLB values for both FI formulations. The CRLB is computed as the reciprocal of the FI, $1/J(\\theta)$. The average is taken over the grid $\\Theta$, and the worst-case is the maximum value on the grid. This procedure allows for a direct comparison between the performance predicted by the simplified model used for selection and the performance of the exact, physically grounded model.\n\nThe algorithm will be implemented in Python using NumPy for efficient numerical computation. The steps involve:\n- Calculating weights $w_i$.\n- Implementing the greedy selection loop with the specified logic and tie-breaking rules.\n- Generating the angle grid $\\Theta$.\n- Using vectorized operations to compute $J(\\theta)$ and $J_{\\text{approx}}(\\theta)$ for the selected neurons over the grid.\n- Calculating the four required CRLB statistics for each test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the neuron selection algorithm and evaluation\n    for the specified test cases.\n    \"\"\"\n    test_cases = [\n        # Case 1 (happy path with near-uniform candidates)\n        {\n            \"N\": 12,\n            \"phi\": np.array([0, np.pi/6, np.pi/3, np.pi/2, 2*np.pi/3, 5*np.pi/6, \n                           np.pi, 7*np.pi/6, 4*np.pi/3, 3*np.pi/2, 5*np.pi/3, 11*np.pi/6]),\n            \"a\": np.full(12, 5.0),\n            \"b\": np.full(12, 20.0),\n            \"K\": 6,\n            \"M\": 360\n        },\n        # Case 2 (clustered preferred directions with heterogeneous amplitudes)\n        {\n            \"N\": 10,\n            \"phi\": np.array([0, 0.1, 0.2, np.pi/2, np.pi/2 + 0.05, np.pi, \n                           np.pi + 0.1, 3*np.pi/2, 3*np.pi/2 + 0.2, 3*np.pi/2 + 0.3]),\n            \"a\": np.array([8, 6, 4, 7, 5, 9, 3, 6, 4, 2]),\n            \"b\": np.full(10, 25.0),\n            \"K\": 5,\n            \"M\": 720\n        },\n        # Case 3 (heterogeneous baselines and amplitudes)\n        {\n            \"N\": 8,\n            \"phi\": np.array([0, np.pi/4, np.pi/2, 3*np.pi/4, np.pi, \n                            5*np.pi/4, 3*np.pi/2, 7*np.pi/4]),\n            \"a\": np.array([3, 10, 2, 8, 4, 6, 1, 9]),\n            \"b\": np.array([15, 30, 12, 28, 18, 22, 16, 26]),\n            \"K\": 4,\n            \"M\": 512\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        N, phi, a, b, K, M = case[\"N\"], case[\"phi\"], case[\"a\"], case[\"b\"], case[\"K\"], case[\"M\"]\n        \n        # 1. Calculate weights\n        # Handle b_i=0 case to avoid division by zero, although not in test cases.\n        w = np.divide(a**2, b, out=np.zeros_like(a, dtype=float), where=b!=0)\n\n        # 2. Greedy selection algorithm\n        # Initialize with the neuron having the max weight\n        initial_neuron_idx = np.argmax(w)\n        selected_indices = {initial_neuron_idx}\n        \n        # Complex sum C_S = sum(w_i * exp(j*2*phi_i)) for i in S\n        sum_vec = w[initial_neuron_idx] * np.exp(1j * 2 * phi[initial_neuron_idx])\n\n        for _ in range(K - 1):\n            candidate_indices = list(set(range(N)) - selected_indices)\n            \n            candidate_scores = []\n            for j in candidate_indices:\n                # Calculate magnitude of the potential new complex sum\n                new_vec = sum_vec + w[j] * np.exp(1j * 2 * phi[j])\n                magnitude = np.abs(new_vec)\n                # Store tuple for sorting: (magnitude, -weight, index)\n                # We use -weight because we want to break ties with larger weight.\n                candidate_scores.append((magnitude, -w[j], j))\n\n            # Sort to find the best candidate based on the specified criteria\n            candidate_scores.sort()\n            \n            best_next_idx = candidate_scores[0][2]\n            \n            # Add best candidate to the selected set and update the sum vector\n            selected_indices.add(best_next_idx)\n            sum_vec += w[best_next_idx] * np.exp(1j * 2 * phi[best_next_idx])\n\n        # Convert set of indices to a list for indexing\n        S_indices = list(selected_indices)\n        \n        # Extract parameters for the selected subset of neurons\n        phi_S = phi[S_indices]\n        a_S = a[S_indices]\n        b_S = b[S_indices]\n        w_S = w[S_indices]\n\n        # 3. Evaluation\n        theta_grid = np.linspace(0, 2 * np.pi, M, endpoint=False)\n        \n        # Use broadcasting for efficient computation\n        # theta_grid shape (M,) -> (M, 1)\n        # S params shape (K,) -> (1, K)\n        # Resulting angle_diff shape (M, K)\n        angle_diff = theta_grid[:, np.newaxis] - phi_S[np.newaxis, :]\n        \n        # Compute exact Fisher Information J(theta)\n        numerator_exact = (a_S * np.sin(angle_diff))**2\n        denominator_exact = b_S + a_S * np.cos(angle_diff)\n        # Sum over the selected neurons (axis=1)\n        J_exact = np.sum(numerator_exact / denominator_exact, axis=1)\n\n        # Compute approximate Fisher Information J_approx(theta)\n        J_approx = np.sum(w_S * (np.sin(angle_diff))**2, axis=1)\n\n        # 4. Calculate CRLB values\n        # Add a small epsilon to avoid division by zero, although unlikely with these problems\n        # A zero FI would mean infinite CRLB, which np.inf correctly represents.\n        crlb_exact = 1.0 / J_exact\n        crlb_approx = 1.0 / J_approx\n        \n        avg_crlb_exact = np.mean(crlb_exact)\n        worst_crlb_exact = np.max(crlb_exact)\n        \n        avg_crlb_approx = np.mean(crlb_approx)\n        worst_crlb_approx = np.max(crlb_approx)\n        \n        case_result = [avg_crlb_exact, worst_crlb_exact, avg_crlb_approx, worst_crlb_approx]\n        all_results.append(case_result)\n\n    # Format output to match specification exactly: [[r1,r2,r3,r4],[...],...]\n    result_str = \"[\" + \",\".join([f\"[{r[0]},{r[1]},{r[2]},{r[3]}]\" for r in all_results]) + \"]\"\n    print(result_str)\n\nsolve()\n```"
        }
    ]
}