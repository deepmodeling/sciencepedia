{
    "hands_on_practices": [
        {
            "introduction": "在构建编码模型时，我们经常会遇到一个挑战：实验中的多个特征（即协变量或回归量）彼此相关。这种现象称为多重共线性，它使得解释单个回归量的系数变得困难，因为我们无法轻易地将共享的方差归因于某一个特定特征。 这个实践将指导你使用一种常见的技术——格拉姆-施密特（Gram-Schmidt）序列正交化——来处理这个问题。通过亲手实现这个过程，你将直接观察到正交化如何影响系数的估计值，从而对这个棘手的统计问题获得具体而深入的理解。",
            "id": "4155403",
            "problem": "在神经科学数据分析中，您将处理一个逐试次编码模型，其中使用相关的参数化调制器来解释神经响应。这些调制器被称为振幅和持续时间，它们构成两个通常相互关联的回归量。您的任务是构建并分析一个编码模型，通过正交化来分离每个回归量的独特贡献，然后比较原始相关设计中的系数与正交化设计中的系数。\n\n从以下基本概念开始：通用线性模型（GLM）将响应向量表示为回归量的线性组合加上噪声。设响应向量为 $y \\in \\mathbb{R}^{N}$，设计矩阵为 $X \\in \\mathbb{R}^{N \\times p}$，系数向量为 $\\beta \\in \\mathbb{R}^{p}$，噪声为 $\\varepsilon \\in \\mathbb{R}^{N}$。GLM 的表达式为 $y = X \\beta + \\varepsilon$。普通最小二乘法（OLS）估计量通过最小化残差平方和来求解，当 $X$ 为满秩时，其解为 $\\hat{\\beta} = (X^{\\top} X)^{-1} X^{\\top} y$。当 $X$ 是秩亏的，摩尔-彭若斯伪逆（Moore-Penrose pseudoinverse） $X^{+}$ 提供了最小范数最小二乘解 $\\hat{\\beta} = X^{+} y$。\n\n应通过格拉姆-施密特（Gram-Schmidt）过程对中心化后的调制器的列空间进行正交化。具体来说，给定一个中心化的振幅回归量 $x_{1} \\in \\mathbb{R}^{N}$ 和一个中心化的持续时间回归量 $x_{2} \\in \\mathbb{R}^{N}$，通过移除 $x_{2}$ 在 $x_{1}$ 上的投影来定义正交化的持续时间回归量：\n$$\nx_{2}^{\\perp} = x_{2} - \\frac{x_{1}^{\\top} x_{2}}{x_{1}^{\\top} x_{1}} x_{1}.\n$$\n正交化设计使用回归量 $[\\mathbf{1}, x_{1}, x_{2}^{\\perp}]$，其中 $\\mathbf{1}$ 表示截距列，$x_{1}, x_{2}$ 均已进行均值中心化，以使截距与调制器正交。\n\n为每个测试用例实施以下步骤：\n- 根据给定公式构造序列 $x_{1}$ 和 $x_{2}$，其中索引 $i$ 从 $1$ 到 $N$。\n- 计算中心化后的调制器 $x_{1}^{c} = x_{1} - \\bar{x}_{1}$ 和 $x_{2}^{c} = x_{2} - \\bar{x}_{2}$，其中 $\\bar{x}$ 表示样本均值。\n- 使用上述格拉姆-施密特公式计算 $x_{2}^{\\perp}$，其中用 $x_{1}^{c}$ 代替 $x_{1}$，用 $x_{2}^{c}$ 代替 $x_{2}$。\n- 构建原始相关设计矩阵 $X_{\\text{orig}} = [\\mathbf{1}, x_{1}^{c}, x_{2}^{c}]$ 和正交化设计矩阵 $X_{\\text{orth}} = [\\mathbf{1}, x_{1}^{c}, x_{2}^{\\perp}]$。\n- 使用每个案例指定的系数和确定性噪声序列，通过确定性线性生成模型 $y_{i} = \\beta_{0} + \\beta_{1} x_{1,i} + \\beta_{2} x_{2,i} + \\varepsilon_{i}$ 生成 $y$。\n- 使用摩尔-彭若斯伪逆估计 $\\hat{\\beta}_{\\text{orig}}$ 和 $\\hat{\\beta}_{\\text{orth}}$，并仅报告调制器的系数（不包括截距），顺序为 $[\\hat{\\beta}_{1,\\text{orig}}, \\hat{\\beta}_{2,\\text{orig}}, \\hat{\\beta}_{1,\\text{orth}}, \\hat{\\beta}_{2,\\text{orth}}]$，格式为浮点数。\n\n测试套件规范：\n- 案例 1（正常路径，中度相关）：\n  - 观测数量 $N = 12$。\n  - 对于索引 $i \\in \\{1, 2, \\dots, 12\\}$：\n    - 振幅 $x_{1,i} = i$。\n    - 持续时间 $x_{2,i} = 0.8 \\, i + 0.4 \\sin\\!\\left(\\frac{2 \\pi i}{12}\\right)$。\n    - 噪声 $\\varepsilon_{i} = 0.1 \\sin\\!\\left(\\frac{4 \\pi i}{12}\\right)$。\n  - 系数 $\\beta_{0} = 0.2$, $\\beta_{1} = 0.7$, $\\beta_{2} = 0.3$。\n- 案例 2（边界情况，完全共线性）：\n  - 观测数量 $N = 10$。\n  - 对于索引 $i \\in \\{1, 2, \\dots, 10\\}$：\n    - 振幅 $x_{1,i} = i$。\n    - 持续时间 $x_{2,i} = 2 \\, x_{1,i}$。\n    - 噪声 $\\varepsilon_{i} = 0.05 \\cos\\!\\left(\\frac{2 \\pi i}{10}\\right)$。\n  - 系数 $\\beta_{0} = 0.0$, $\\beta_{1} = 1.0$, $\\beta_{2} = 0.5$。\n- 案例 3（边缘情况，调制器间接近零相关）：\n  - 观测数量 $N = 8$。\n  - 对于索引 $i \\in \\{1, 2, \\dots, 8\\}$：\n    - 振幅 $x_{1,i} = i$。\n    - 持续时间 $x_{2,i} = 3 \\, (-1)^{i}$。\n    - 噪声 $\\varepsilon_{i} = 0.02 \\sin\\!\\left(\\frac{2 \\pi i}{8}\\right)$。\n  - 系数 $\\beta_{0} = -0.1$, $\\beta_{1} = 0.5$, $\\beta_{2} = -0.4$。\n\n要求的最终输出格式：\n- 您的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表。列表中的每个元素对应一个测试用例，并且本身是一个包含四个浮点数的列表，顺序为 $[\\hat{\\beta}_{1,\\text{orig}}, \\hat{\\beta}_{2,\\text{orig}}, \\hat{\\beta}_{1,\\text{orth}}, \\hat{\\beta}_{2,\\text{orth}}]$。例如，外层列表应类似于 $[[b_{11},b_{12},b_{13},b_{14}],[b_{21},b_{22},b_{23},b_{24}],[b_{31},b_{32},b_{33},b_{34}]]$，其中每个 $b_{jk}$ 都是一个浮点数。",
            "solution": "该问题要求构建和分析一个通用线性模型（GLM），以理解回归量之间的多重共线性（multicollinearity）的影响以及正交化对模型系数估计值的影响。我们将首先建立理论基础，然后对每个测试用例进行逐步实现。\n\n**1. 理论基础：通用线性模型与多重共线性**\n\n通用线性模型（GLM）假设因变量 $y$ 与设计矩阵 $X$ 中包含的一组解释变量（回归量）之间存在线性关系。该模型表示为：\n$$\ny = X \\beta + \\varepsilon\n$$\n其中 $y \\in \\mathbb{R}^{N}$ 是包含 $N$ 个观测值的向量，$X \\in \\mathbb{R}^{N \\times p}$ 是包含 $p$ 个回归量（包括一个截距）的设计矩阵，$\\beta \\in \\mathbb{R}^{p}$ 是未知系数的向量，$\\varepsilon \\in \\mathbb{R}^{N}$ 是误差项向量。\n\n目标是估计系数 $\\beta$。普通最小二乘法（OLS）通过最小化残差平方和 $\\|y - X\\beta\\|^2$ 来找到 $\\hat{\\beta}$。当 $X$ 的列线性无关（即 $X$ 具有满列秩）时，唯一的 OLS 解由下式给出：\n$$\n\\hat{\\beta} = (X^{\\top} X)^{-1} X^{\\top} y\n$$\n当回归量相关时，会出现一个关键问题，这种情况被称为多重共线性。如果相关性是完美的（即一个回归量是其他回归量的线性组合），$X$ 的列将变得线性相关，并且 $X$ 是秩亏的。在这种情况下，矩阵 $X^{\\top} X$ 是奇异的，无法求逆。OLS 解不再是唯一的；存在无限组系数向量可以产生相同的最小残差平方和。\n\n为了即使在秩亏的情况下也能获得一个单一、明确定义的解，我们使用摩尔-彭若斯伪逆，记作 $X^{+}$。解由下式给出：\n$$\n\\hat{\\beta} = X^{+} y\n$$\n这个解是在所有可能的最小二乘解中具有最小欧几里得范数（$\\|\\hat{\\beta}\\|_2$）的解。\n\n**2. 通过格拉姆-施密特（Gram-Schmidt）进行正交化**\n\n当回归量相关时，其估计系数难以解释。一个系数 $\\hat{\\beta}_j$ 表示在保持所有其他回归量不变的情况下，回归量 $x_j$ 每变化一个单位时 $y$ 的变化量。但是，当回归量相关时，改变其中一个而保持其他不变在统计上和概念上都是有问题的。\n\n正交化是将一组相关回归量转换为一组不相关（正交）回归量的过程。这将重新归因回归量之间的共享方差，通常能澄清它们的贡献。我们使用格拉姆-施密特过程。给定两个中心化回归量 $x_1^c$ 和 $x_2^c$，我们可以创建一个与 $x_1^c$ 正交的新回归量 $x_2^{\\perp}$。这是通过减去 $x_2^c$ 在 $x_1^c$ 上的投影来实现的：\n$$\nx_{2}^{\\perp} = x_{2}^{c} - \\text{proj}_{x_1^c}(x_2^c) = x_{2}^{c} - \\frac{(x_{1}^{c})^{\\top} x_{2}^{c}}{(x_{1}^{c})^{\\top} x_{1}^{c}} x_{1}^{c}\n$$\n根据构造，$(x_{1}^{c})^{\\top} x_{2}^{\\perp} = 0$。然后我们可以用设计矩阵 $X_{\\text{orth}} = [\\mathbf{1}, x_1^c, x_2^{\\perp}]$ 构建一个新的 GLM。由于回归量现在是正交的，它们的估计系数相互独立。正交化的顺序很重要；在这里，$x_1^c$ 和 $x_2^c$ 之间的共享方差被归因于 $x_1^c$，而 $x_2^{\\perp}$ 仅代表 $x_2^c$ 中独立于 $x_1^c$ 的方差。\n\n**3. 系数之间的关系**\n\n设原始模型和正交化模型为：\n1.  原始模型：$y = \\hat{\\beta}_{0,\\text{orig}}\\mathbf{1} + \\hat{\\beta}_{1,\\text{orig}} x_1^c + \\hat{\\beta}_{2,\\text{orig}} x_2^c + e_{\\text{orig}}$\n2.  正交化模型：$y = \\hat{\\beta}_{0,\\text{orth}}\\mathbf{1} + \\hat{\\beta}_{1,\\text{orth}} x_1^c + \\hat{\\beta}_{2,\\text{orth}} x_2^{\\perp} + e_{\\text{orth}}$\n\n由于 OLS 拟合提供了最佳的线性预测，两个模型的拟合值和残差必须相同，因为它们张成的是同一个子空间（除非其中一个是秩亏的，这种情况我们将会讨论）。将 $x_2^{\\perp}$ 的定义代入第二个方程，我们得到：\n$$\ny = \\hat{\\beta}_{0,\\text{orth}}\\mathbf{1} + \\hat{\\beta}_{1,\\text{orth}} x_1^c + \\hat{\\beta}_{2,\\text{orth}} \\left( x_{2}^{c} - \\frac{(x_{1}^{c})^{\\top} x_{2}^{c}}{(x_{1}^{c})^{\\top} x_{1}^{c}} x_{1}^{c} \\right) + e_{\\text{orth}}\n$$\n$$\ny = \\hat{\\beta}_{0,\\text{orth}}\\mathbf{1} + \\left( \\hat{\\beta}_{1,\\text{orth}} - \\hat{\\beta}_{2,\\text{orth}} \\frac{(x_{1}^{c})^{\\top} x_{2}^{c}}{(x_{1}^{c})^{\\top} x_{1}^{c}} \\right) x_1^c + \\hat{\\beta}_{2,\\text{orth}} x_2^c + e_{\\text{orth}}\n$$\n将 $x_1^c$ 和 $x_2^c$ 的系数与原始模型进行比较，我们发现以下关系：\n$$\n\\hat{\\beta}_{2,\\text{orig}} = \\hat{\\beta}_{2,\\text{orth}}\n$$\n$$\n\\hat{\\beta}_{1,\\text{orig}} = \\hat{\\beta}_{1,\\text{orth}} - \\hat{\\beta}_{2,\\text{orth}} \\frac{(x_{1}^{c})^{\\top} x_{2}^{c}}{(x_{1}^{c})^{\\top} x_{1}^{c}}\n$$\n这揭示了被正交化的回归量（$x_2^c \\to x_2^{\\perp}$）的系数保持不变。然而，第一个回归量 $\\hat{\\beta}_1$ 的系数发生了显著变化。它在正交化模型中的值 $\\hat{\\beta}_{1,\\text{orth}}$ 吸收了 $x_2^c$ 中与 $x_1^c$ 相关的部分的解释力。\n\n**4. 实现步骤**\n\n对于每个测试用例，我们将执行以下计算：\n\n1.  **数据生成**：对于给定的观测数量 $N$，我们使用提供的公式（索引 $i$ 从 $1$ 到 $N$）生成基础回归量 $x_1$ 和 $x_2$ 以及噪声项 $\\varepsilon$。\n2.  **响应生成**：使用真实的生成模型合成响应变量 $y$：$y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + \\varepsilon_i$。请注意，该模型是使用非中心化的回归量指定的。\n3.  **回归量准备**：我们计算中心化回归量 $x_1^c = x_1 - \\bar{x}_1$ 和 $x_2^c = x_2 - \\bar{x}_2$。GLM 中的截距将解释均值。\n4.  **正交化**：在中心化回归量上使用格拉姆-施密特公式计算正交回归量 $x_2^{\\perp}$。\n5.  **设计矩阵构建**：构建两个设计矩阵：\n    -   用于原始相关模型的 $X_{\\text{orig}} = [\\mathbf{1}, x_1^c, x_2^c]$。\n    -   用于正交化模型的 $X_{\\text{orth}} = [\\mathbf{1}, x_1^c, x_2^{\\perp}]$。\n6.  **系数估计**：我们应用摩尔-彭若斯伪逆方法估计系数向量 $\\hat{\\beta}_{\\text{orig}}$ 和 $\\hat{\\beta}_{\\text{orth}}$：$\\hat{\\beta} = X^{+} y$。\n7.  **结果提取**：从估计的系数向量 $\\hat{\\beta}_{\\text{orig}} = [\\hat{\\beta}_{0,\\text{orig}}, \\hat{\\beta}_{1,\\text{orig}}, \\hat{\\beta}_{2,\\text{orig}}]^{\\top}$ 和 $\\hat{\\beta}_{\\text{orth}} = [\\hat{\\beta}_{0,\\text{orth}}, \\hat{\\beta}_{1,\\text{orth}}, \\hat{\\beta}_{2,\\text{orth}}]^{\\top}$ 中，我们提取两个调制器的系数，并按指定顺序报告：$[\\hat{\\beta}_{1,\\text{orig}}, \\hat{\\beta}_{2,\\text{orig}}, \\hat{\\beta}_{1,\\text{orth}}, \\hat{\\beta}_{2,\\text{orth}}]$。\n\n此过程将应用于所有三个测试用例，这些用例涵盖了中度相关、完全共线性和接近零相关的场景，从而展示了在这些不同条件下系数估计值的行为。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the neuroscience encoding model problem for three test cases.\n\n    For each case, it constructs correlated and orthogonalized design matrices,\n    simulates neural response data, fits a GLM to both designs using the\n    Moore-Penrose pseudoinverse, and reports the resulting coefficients\n    for the non-intercept regressors.\n    \"\"\"\n\n    test_cases = [\n        # Case 1: Moderate correlation\n        {\n            \"N\": 12,\n            \"x1_func\": lambda i: i,\n            \"x2_func\": lambda i: 0.8 * i + 0.4 * np.sin(2 * np.pi * i / 12),\n            \"eps_func\": lambda i: 0.1 * np.sin(4 * np.pi * i / 12),\n            \"betas\": [0.2, 0.7, 0.3]  # beta_0, beta_1, beta_2\n        },\n        # Case 2: Perfect collinearity\n        {\n            \"N\": 10,\n            \"x1_func\": lambda i: i,\n            \"x2_func\": lambda i: 2.0 * i,\n            \"eps_func\": lambda i: 0.05 * np.cos(2 * np.pi * i / 10),\n            \"betas\": [0.0, 1.0, 0.5]\n        },\n        # Case 3: Near-zero correlation\n        {\n            \"N\": 8,\n            \"x1_func\": lambda i: i,\n            \"x2_func\": lambda i: 3.0 * (-1)**i,\n            \"eps_func\": lambda i: 0.02 * np.sin(2 * np.pi * i / 8),\n            \"betas\": [-0.1, 0.5, -0.4]\n        }\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        N = case[\"N\"]\n        beta0, beta1, beta2 = case[\"betas\"]\n        \n        # Step 1: Construct x1, x2, and epsilon sequences.\n        # Index i runs from 1 to N.\n        i = np.arange(1, N + 1)\n        x1 = case[\"x1_func\"](i)\n        x2 = case[\"x2_func\"](i)\n        eps = case[\"eps_func\"](i)\n\n        # Step 2: Compute centered modulators.\n        x1_c = x1 - np.mean(x1)\n        x2_c = x2 - np.mean(x2)\n\n        # Step 3: Compute orthogonalized duration regressor x2_perp.\n        # Handle the case where x1_c is a zero vector to avoid division by zero.\n        dot_x1c_x1c = np.dot(x1_c, x1_c)\n        if np.isclose(dot_x1c_x1c, 0):\n            # If x1_c is zero, its projection is zero, so x2_perp is just x2_c.\n            x2_perp = x2_c\n        else:\n            proj_x2_on_x1 = (np.dot(x1_c, x2_c) / dot_x1c_x1c) * x1_c\n            x2_perp = x2_c - proj_x2_on_x1\n\n        # Step 4: Form the design matrices.\n        intercept = np.ones(N)\n        X_orig = np.column_stack([intercept, x1_c, x2_c])\n        X_orth = np.column_stack([intercept, x1_c, x2_perp])\n\n        # Step 5: Generate y using the deterministic linear generative model.\n        # Note: The generative model uses the original, non-centered regressors.\n        y = beta0 + beta1 * x1 + beta2 * x2 + eps\n\n        # Step 6: Estimate beta_orig and beta_orth using Moore-Penrose pseudoinverse.\n        beta_hat_orig = np.linalg.pinv(X_orig) @ y\n        beta_hat_orth = np.linalg.pinv(X_orth) @ y\n\n        # Report only the coefficients for the modulators (exclude the intercept).\n        # Order: [beta1_orig, beta2_orig, beta1_orth, beta2_orth]\n        results = [\n            beta_hat_orig[1], \n            beta_hat_orig[2], \n            beta_hat_orth[1], \n            beta_hat_orth[2]\n        ]\n        all_results.append(results)\n\n    # Format the final output string.\n    output_str = \"[\" + \",\".join([f\"[{','.join(map(str, res))}]\" for res in all_results]) + \"]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "从构建通用线性模型（GLM）的实践转向对神经科学中一个基础估计量——脉冲触发平均（Spike-Triggered Average, STA）——的理论性质的理解。STA 是一种直观衡量神经元感受野的方法，但它作为无偏估计量的有效性，严重依赖于刺激的统计特性。 这个练习将引导你完成一个数学推导，以精确地揭示刺激相关性（即非白噪声刺激）如何引入一种可预测的偏差。这将加深你对实验设计和模型解释之间关键联系的认识。",
            "id": "4155364",
            "problem": "考虑一个单个神经元的离散时间编码模型，其中时间索引 $t$ 处的刺激是一个向量 $x_t \\in \\mathbb{R}^{p}$，代表了过去 $p$ 个延迟的近期刺激历史，脉冲产生于一个线性-非线性-泊松（LNP）模型，其条件强度为 $\\lambda_t = \\exp(\\alpha + k^{\\top} x_t)$，其中 $k \\in \\mathbb{R}^{p}$ 是感受野，$\\alpha \\in \\mathbb{R}$ 是一个基线参数。假设 $\\{x_t\\}$ 是从一个零均值多元高斯分布中进行的独立同分布抽样，其协方差矩阵为 $\\Sigma \\in \\mathbb{R}^{p \\times p}$，且该矩阵是对称正定的。将任意时间的脉冲触发平均（STA）定义为在给定时刻 $t$ 发生脉冲的条件下的条件平均刺激，即 $\\mathrm{STA} := \\mathbb{E}[x_t \\mid \\text{在 } t \\text{ 发生脉冲}]$。\n\n从点过程的条件期望的基本定义以及多元正态分布的矩生成性质出发，用 $\\Sigma$ 和 $k$ 表示来推导出 $\\mathrm{STA}$。利用这个表达式，从数学上解释为什么只有当刺激是单位协方差的高斯白噪声（即 $\\Sigma = I$）时，STA 才能得出无偏的感受野估计，并描述当 $\\Sigma \\neq I$ 时，如果不进行任何校正直接使用 STA 估计 $k$ 会产生的偏差。\n\n你的最终答案应为相关刺激下偏差向量的闭式解析表达式。不需要数值近似，也不涉及单位。不要将最终答案表示为方程；只提供偏差的表达式。",
            "solution": "该问题陈述经评估有效。它在科学上植根于神经编码模型的理论，问题设定良好，信息充分，足以进行唯一的数学推导，并且表述客观。该问题需要在计算神经科学领域进行一个标准但并非无足轻重的推导。\n\n我们的任务是为一个由线性-非线性-泊松（LNP）模型描述的神经元推导脉冲触发平均（STA），并分析其作为神经元感受野估计量的偏差。\n\n首先，我们对 STA 的定义进行形式化。STA 定义为在给定时间 $t$ 发生脉冲的条件下，刺激 $x_t$ 的条件期望：\n$$\n\\mathrm{STA} := \\mathbb{E}[x_t \\mid \\text{在 } t \\text{ 发生脉冲}]\n$$\n使用条件期望的定义，这可以写成在给定脉冲的条件下，对刺激的后验分布的积分：\n$$\n\\mathrm{STA} = \\int_{\\mathbb{R}^p} x \\, p(x \\mid \\text{脉冲}) \\, dx\n$$\n根据贝叶斯法则，后验概率密度 $p(x \\mid \\text{脉冲})$ 与刺激的先验概率 $p(x)$ 和给定刺激下脉冲的条件概率 $P(\\text{脉冲} \\mid x)$ 相关：\n$$\np(x \\mid \\text{脉冲}) = \\frac{P(\\text{脉冲} \\mid x) p(x)}{P(\\text{脉冲})}\n$$\n在泊松点过程模型中，在微小时间间隔内观测到脉冲的概率与瞬时发放率 $\\lambda(x)$ 成正比。因此，我们可以设 $P(\\text{脉冲} \\mid x) \\propto \\lambda(x)$。脉冲的无条件概率 $P(\\text{脉冲})$ 则与平均发放率 $\\mathbb{E}[\\lambda(x)]$ 成正比。比例常数相互抵消，从而得到广泛使用的 STA 公式，也称为 Stein 公式：\n$$\n\\mathrm{STA} = \\frac{\\int x \\lambda(x) p(x) \\, dx}{\\int \\lambda(x) p(x) \\, dx} = \\frac{\\mathbb{E}[x \\lambda(x)]}{\\mathbb{E}[\\lambda(x)]}\n$$\n问题指定了刺激分布和 LNP 模型的组成部分：\n1.  刺激 $x_t$ 从均值为 0、协方差为 $\\Sigma$ 的多元高斯分布中抽取，即 $x \\sim \\mathcal{N}(0, \\Sigma)$。其概率密度函数（PDF）为：\n    $$\n    p(x) = \\frac{1}{\\sqrt{(2\\pi)^p \\det(\\Sigma)}} \\exp\\left(-\\frac{1}{2} x^{\\top} \\Sigma^{-1} x\\right)\n    $$\n2.  条件强度（发放率）由 $\\lambda(x) = \\exp(\\alpha + k^{\\top} x)$ 给出，其中 $k$ 是感受野，$\\alpha$ 是基线参数。\n\n现在我们计算 STA 表达式的分母和分子。\n\n分母是期望发放率 $\\mathbb{E}[\\lambda(x)]$：\n$$\n\\mathbb{E}[\\lambda(x)] = \\mathbb{E}[\\exp(\\alpha + k^{\\top} x)] = \\exp(\\alpha) \\mathbb{E}[\\exp(k^{\\top} x)]\n$$\n项 $\\mathbb{E}[\\exp(k^{\\top} x)]$ 是随机变量 $x \\sim \\mathcal{N}(0, \\Sigma)$ 的矩生成函数（MGF），在向量 $k$ 处求值。多元正态分布 $\\mathcal{N}(\\mu, \\Sigma)$ 的 MGF 为 $M_x(t) = \\exp(\\mu^{\\top}t + \\frac{1}{2} t^{\\top}\\Sigma t)$。在我们的情况下，$\\mu=0$，MGF 为 $M_x(t) = \\exp(\\frac{1}{2} t^{\\top}\\Sigma t)$。在 $t=k$ 处求值，我们得到：\n$$\n\\mathbb{E}[\\exp(k^{\\top} x)] = \\exp\\left(\\frac{1}{2} k^{\\top}\\Sigma k\\right)\n$$\n因此，分母为：\n$$\n\\mathbb{E}[\\lambda(x)] = \\exp\\left(\\alpha + \\frac{1}{2} k^{\\top}\\Sigma k\\right)\n$$\n\n分子是刺激与发放率乘积的期望，$\\mathbb{E}[x \\lambda(x)]$：\n$$\n\\mathbb{E}[x \\lambda(x)] = \\int_{\\mathbb{R}^p} x \\, \\exp(\\alpha + k^{\\top} x) \\, p(x) \\, dx = \\exp(\\alpha) \\int_{\\mathbb{R}^p} x \\, \\exp(k^{\\top} x) \\frac{\\exp(-\\frac{1}{2} x^{\\top} \\Sigma^{-1} x)}{\\sqrt{(2\\pi)^p \\det(\\Sigma)}} \\, dx\n$$\n积分内指数的参数可以合并：\n$$\nk^{\\top} x - \\frac{1}{2} x^{\\top} \\Sigma^{-1} x\n$$\n我们对这个二次型进行配方。一个关键的恒等式是 $(x-v)^{\\top}A(x-v) = x^{\\top}Ax - 2v^{\\top}Ax + v^{\\top}Av$。通过设 $A=\\Sigma^{-1}$ 和 $v = \\Sigma k$，我们发现：\n$$\nx^{\\top}\\Sigma^{-1}x - 2k^{\\top}x = (x-\\Sigma k)^{\\top}\\Sigma^{-1}(x-\\Sigma k) - k^{\\top}\\Sigma\\Sigma^{-1}\\Sigma k = (x-\\Sigma k)^{\\top}\\Sigma^{-1}(x-\\Sigma k) - k^{\\top}\\Sigma k\n$$\n重新整理这个恒等式，我们得到：\n$$\nk^{\\top}x - \\frac{1}{2}x^{\\top}\\Sigma^{-1}x = -\\frac{1}{2}\\left[(x-\\Sigma k)^{\\top}\\Sigma^{-1}(x-\\Sigma k) - k^{\\top}\\Sigma k\\right]\n$$\n将此代回分子的积分中：\n$$\n\\mathbb{E}[x \\lambda(x)] = \\exp(\\alpha) \\int x \\frac{\\exp\\left(-\\frac{1}{2}(x-\\Sigma k)^{\\top}\\Sigma^{-1}(x-\\Sigma k) + \\frac{1}{2}k^{\\top}\\Sigma k\\right)}{\\sqrt{(2\\pi)^p \\det(\\Sigma)}} \\, dx\n$$\n我们可以将不依赖于 $x$ 的项提出来：\n$$\n\\mathbb{E}[x \\lambda(x)] = \\exp\\left(\\alpha + \\frac{1}{2}k^{\\top}\\Sigma k\\right) \\int x \\frac{\\exp\\left(-\\frac{1}{2}(x - \\Sigma k)^{\\top}\\Sigma^{-1}(x - \\Sigma k)\\right)}{\\sqrt{(2\\pi)^p \\det(\\Sigma)}} \\, dx\n$$\n积分内的表达式是 $x$ 与一个新的多元高斯分布 $\\mathcal{N}(\\Sigma k, \\Sigma)$ 的 PDF 的乘积。因此，该积分代表从这个新分布中抽取的随机向量的期望值。期望值就是它的均值，即 $\\Sigma k$。\n因此，分子的计算结果为：\n$$\n\\mathbb{E}[x \\lambda(x)] = \\exp\\left(\\alpha + \\frac{1}{2}k^{\\top}\\Sigma k\\right) (\\Sigma k)\n$$\n\n现在我们将分子和分母结合起来求 STA：\n$$\n\\mathrm{STA} = \\frac{\\mathbb{E}[x \\lambda(x)]}{\\mathbb{E}[\\lambda(x)]} = \\frac{\\exp\\left(\\alpha + \\frac{1}{2}k^{\\top}\\Sigma k\\right) (\\Sigma k)}{\\exp\\left(\\alpha + \\frac{1}{2}k^{\\top}\\Sigma k\\right)} = \\Sigma k\n$$\n这给出了用感受野 $k$ 和刺激协方差 $\\Sigma$ 表示的 STA 表达式。这是问题的第一部分。\n\n接下来，我们分析 STA 作为 $k$ 的估计量。如果一个估计量的期望值等于真实参数，则该估计量是无偏的。在这里，我们将 STA（一个可从数据中计算出的量）视为真实感受野 $k$ 的估计量。如果 $\\mathrm{STA} = k$，则估计量是无偏的。\n根据我们的结果，我们有 $\\mathrm{STA} = \\Sigma k$。因此，无偏估计的条件是 $\\Sigma k = k$。由于这对任意感受野 $k \\in \\mathbb{R}^p$ 都必须成立，因此该等式要求矩阵 $\\Sigma$ 是单位矩阵，即 $\\Sigma = I$。这对应于高斯白噪声刺激的情况，其中所有刺激维度不相关且具有单位方差。\n\n当 $\\Sigma \\neq I$ 时，STA 是 $k$ 的一个有偏估计量。估计量的偏差定义为估计量与真实参数值之间的差。偏差向量是：\n$$\n\\text{偏差} = \\mathrm{STA} - k = \\Sigma k - k\n$$\n提出因子 $k$，我们得到偏差的闭式表达式：\n$$\n\\text{偏差} = (\\Sigma - I)k\n$$\n这个表达式描述了由刺激相关性引入的失真。STA 实际上是真实感受野 $k$ 的一个版本，它被刺激的协方差结构 $\\Sigma$ “着色”了。在一般情况下，要从 STA 获得 $k$ 的无偏估计，需要对协方差矩阵求逆并计算 $k = \\Sigma^{-1} \\mathrm{STA}$，这个过程被称为对 STA 进行“白化”。",
            "answer": "$$\n\\boxed{(\\Sigma - I)k}\n$$"
        },
        {
            "introduction": "本章的最后一个实践将分析尺度从单被试模型提升到功能更强大、更贴近现实的群体水平分析。我们将介绍分层贝叶斯模型（Hierarchical Bayesian Model）作为一种原则性的方法，用于整合来自多个被试的数据。 这种方法允许模型在被试之间“借用”统计强度，通过一种称为“收缩”（shrinkage）的现象，得到更稳健和正则化的参数估计。在这个实践中，你将实现一个分层模型来量化这种收缩效应，并观察到在数据存在噪声时，每个被试的参数估计是如何被拉向群体平均值的。",
            "id": "4155376",
            "problem": "一个实验室记录了在呈现共同刺激期间的多被试神经时间序列。目标是设计一个带有明确协变量集的、有原则的编码模型，并量化跨被试的层级共享如何引导致被试特定参数向总体均值发生后验收缩。该编码模型使用一个包含刺激回归量和冗余协变量的线性设计矩阵，以及一个将各被试特定系数与共享组均值联系起来的层级高斯先验。\n\n假设对于每个被试 $s \\in \\{1,\\dots,S\\}$，在时间样本 $t \\in \\{0,1,\\dots,T-1\\}$ 处，存在以下线性高斯编码模型：\n- 观测响应向量为 $y_s \\in \\mathbb{R}^T$。\n- 设计矩阵为 $X \\in \\mathbb{R}^{T \\times p}$，包含 $p$ 个协变量。$X$ 的各列代表：\n  - 刺激特征 $x_{\\text{stim},t}$，一个设计用于捕捉刺激锁定结构的时间确定性函数。\n  - 冗余漂移协变量 $x_{\\text{drift},t}$，用于捕捉缓慢的基线波动。\n  - 截距 $x_{\\text{int},t}$，用于对恒定偏移进行建模。\n\n编码模型和层级先验如下：\n- 似然（线性高斯）：$y_s \\mid \\beta_s \\sim \\mathcal{N}(X \\beta_s, \\sigma^2 I_T)$，其中 $\\beta_s \\in \\mathbb{R}^p$ 是被试特定的参数向量，$\\sigma^2 > 0$ 是已知的噪声方差，$I_T$ 是 $T \\times T$ 单位矩阵。\n- 被试水平先验：$\\beta_s \\mid \\mu \\sim \\mathcal{N}(\\mu, \\Sigma_b)$，其中 $\\mu \\in \\mathbb{R}^p$ 是共享的组均值，$\\Sigma_b \\in \\mathbb{R}^{p \\times p}$ 是已知的正定协方差矩阵。\n- 组均值的超先验：$\\mu \\sim \\mathcal{N}(m_0, \\Sigma_0)$，其中 $m_0 \\in \\mathbb{R}^p$ 是已知的，$\\Sigma_0 \\in \\mathbb{R}^{p \\times p}$ 是已知的正定协方差矩阵。\n\n这是一个共轭线性高斯层级结构。从高斯密度之积仍为高斯分布，以及高斯随机变量的线性变换仍为高斯分布这些基本事实出发，可以推导出联合后验分布 $p(\\mu, \\{\\beta_s\\}_{s=1}^S \\mid \\{y_s\\}_{s=1}^S)$ 是多元正态分布。后验均值可以通过求解由联合高斯自然参数构成的线性系统得到。通过这些后验均值，后验收缩的量化方式是：相对于独立（非层级）估计，被试特定的后验均值向组均值移动了多少。\n\n将被试 $s$ 的独立估计定义为普通最小二乘解 $\\hat{\\beta}^{\\text{ind}}_s = (X^\\top X)^{-1} X^\\top y_s$。设组均值的后验均值为 $\\hat{\\mu}^{\\text{post}}$，被试特定参数的后验均值为 $\\hat{\\beta}^{\\text{post}}_s$。对每个被试 $s$，定义归一化收缩幅度\n$$\n\\text{shr}_s = \\frac{\\left\\lVert \\hat{\\beta}^{\\text{ind}}_s - \\hat{\\mu}^{\\text{post}} \\right\\rVert_2 - \\left\\lVert \\hat{\\beta}^{\\text{post}}_s - \\hat{\\mu}^{\\text{post}} \\right\\rVert_2}{\\max\\left(\\left\\lVert \\hat{\\beta}^{\\text{ind}}_s - \\hat{\\mu}^{\\text{post}} \\right\\rVert_2, \\varepsilon\\right)},\n$$\n其中 $\\varepsilon$ 是一个小的正常数，用于在分母为零时避免除以零。那么，跨被试的平均收缩为\n$$\n\\overline{\\text{shr}} = \\frac{1}{S} \\sum_{s=1}^S \\text{shr}_s,\n$$\n其结果必须以十进制数报告。任何三角函数中使用的所有角度都必须以弧度为单位进行解释。\n\n您的程序必须实现上述层级编码模型，并为以下每个测试用例计算 $\\overline{\\text{shr}}$。设计矩阵 $X$ 和响应 $y_s$ 必须严格按照规定构建，以确保结果的确定性。\n\n对于所有测试用例，设计矩阵有 $p=3$ 列：\n- 第1列（刺激协变量）：$x_{\\text{stim},t} = \\cos\\left(\\frac{2\\pi t}{T}\\right)$。\n- 第2列（冗余漂移）：$x_{\\text{drift},t} = \\frac{t}{T}$。\n- 第3列（截距）：$x_{\\text{int},t} = 1$。\n\n对于所有测试用例，被试 $s \\in \\{1,2,3\\}$ 的真实参数是固定的：\n- 被试1：$\\beta^{\\text{true}}_1 = [1.5, -0.5, 0.2]^\\top$。\n- 被试2：$\\beta^{\\text{true}}_2 = [1.0, -0.6, 0.1]^\\top$。\n- 被试3：$\\beta^{\\text{true}}_3 = [1.8, -0.4, 0.3]^\\top$。\n\n超参数在所有测试用例中保持不变：\n- 组先验均值：$m_0 = [0, 0, 0]^\\top$。\n- 组先验协方差：$\\Sigma_0 = \\text{diag}([1.0, 1.0, 1.0])$。\n- 被试先验协方差：$\\Sigma_b = \\text{diag}([0.4, 0.4, 0.4])$。\n\n对于每个测试用例，使用以下公式构建 $y_s$\n$$\ny_s = X \\beta^{\\text{true}}_s + \\eta_s,\n$$\n其中 $\\eta_s \\in \\mathbb{R}^T$ 是一个噪声向量，其独立分量使用下面指定的固定伪随机数生成器种子从 $\\mathcal{N}(0, \\sigma^2)$ 中抽取。\n\n测试套件：\n- 用例1（理想情况）：$S = 3$, $T = 12$, $\\sigma^2 = 0.2$，噪声种子为 $0$。\n- 用例2（高噪声边界情况）：$S = 3$, $T = 12$, $\\sigma^2 = 3.0$，噪声种子为 $1$。\n- 用例3（大样本边缘情况）：$S = 3$, $T = 60$, $\\sigma^2 = 0.05$，噪声种子为 $2$。\n\n要求：\n- 通过构建并求解由多元正态后验的自然参数形式所蕴含的联合线性系统，来实现层级后验计算。不要依赖任何黑盒贝叶斯采样方法。\n- 对每个用例，计算独立估计 $\\hat{\\beta}^{\\text{ind}}_s$、后验均值 $\\hat{\\mu}^{\\text{post}}$ 和 $\\hat{\\beta}^{\\text{post}}_s$，以及平均收缩 $\\overline{\\text{shr}}$，其中 $\\varepsilon = 10^{-12}$。\n- 最终输出必须是单行文本，包含三个测试用例的平均收缩值列表，按所列顺序排列，形式为方括号内用逗号分隔的列表（例如，$[\\text{result}_1,\\text{result}_2,\\text{result}_3]$）。每个结果必须是浮点数（十进制数）。不得打印任何额外文本。",
            "solution": "该问题是有效的。它在科学上基于贝叶斯统计建模，这是神经科学数据分析中的一种标准技术。该问题是适定的，提供了所有必要的方程、参数和数据生成过程，以得出一个唯一的、有意义的解。它是客观且自洽的，没有任何可能妨碍严格形式化的模糊之处。\n\n### 1. 模型设定与后验推导\n该问题描述了一个三层层级高斯模型。对每个被试 $s \\in \\{1, \\dots, S\\}$，模型定义如下：\n- **第一层 (似然):** $p(y_s | \\beta_s) = \\mathcal{N}(y_s; X \\beta_s, \\sigma^2 I_T)$\n- **第二层 (被试水平先验):** $p(\\beta_s | \\mu) = \\mathcal{N}(\\beta_s; \\mu, \\Sigma_b)$\n- **第三层 (组水平超先验):** $p(\\mu) = \\mathcal{N}(\\mu; m_0, \\Sigma_0)$\n\n所有未知参数 $\\theta = [\\mu^\\top, \\beta_1^\\top, \\dots, \\beta_S^\\top]^\\top$ 的联合后验分布由贝叶斯定理给出：\n$$\np(\\mu, \\{\\beta_s\\}_{s=1}^S | \\{y_s\\}_{s=1}^S) \\propto p(\\mu) \\prod_{s=1}^S p(y_s | \\beta_s) p(\\beta_s | \\mu)\n$$\n由于似然和先验都是高斯分布，它们的乘积也是一个高斯分布。联合后验的对数（不计常数项）是参数的二次函数：\n$$\n\\log p(\\theta | \\{y_s\\}) = C - \\frac{1}{2} \\left[ (\\mu - m_0)^\\top \\Sigma_0^{-1}(\\mu - m_0) + \\sum_{s=1}^S (\\beta_s - \\mu)^\\top \\Sigma_b^{-1}(\\beta_s - \\mu) + \\frac{1}{\\sigma^2} \\sum_{s=1}^S (y_s - X\\beta_s)^\\top(y_s - X\\beta_s) \\right]\n$$\n对于多元高斯分布，后验均值与后验众数重合。我们可以通过将对数后验关于 $\\theta$ 的梯度设为零来找到它。这等价于求解由联合后验分布的自然参数定义的线性系统。\n\n### 2. 用于求解后验均值的线性系统\n一个高斯分布 $p(\\theta) \\propto \\exp(-\\frac{1}{2}\\theta^\\top J \\theta + h^\\top\\theta)$ 由其精度矩阵 $J$ 和势向量 $h$ 参数化。其均值可通过求解系统 $J\\theta = h$ 得到。\n通过展开对数后验中的二次型，我们可以识别出联合精度矩阵 $J_{\\text{post}}$ 和联合势向量 $h_{\\text{post}}$ 的块结构。参数向量为 $\\theta = [\\mu^\\top, \\beta_1^\\top, \\dots, \\beta_S^\\top]^\\top$。\n\n联合精度矩阵 $J_{\\text{post}}$ 是一个 $(S+1)p \\times (S+1)p$ 的块矩阵：\n$$\nJ_{\\text{post}} =\n\\begin{pmatrix}\nS\\Sigma_b^{-1} + \\Sigma_0^{-1}  & -\\Sigma_b^{-1}  & \\cdots  & -\\Sigma_b^{-1} \\\\\n-\\Sigma_b^{-1}  & \\frac{1}{\\sigma^2}X^\\top X + \\Sigma_b^{-1}  & \\cdots  & 0 \\\\\n\\vdots  & \\vdots  & \\ddots  & \\vdots \\\\\n-\\Sigma_b^{-1}  & 0  & \\cdots  & \\frac{1}{\\sigma^2}X^\\top X + \\Sigma_b^{-1}\n\\end{pmatrix}\n$$\n联合势向量 $h_{\\text{post}}$ 是一个 $(S+1)p$ 维的向量：\n$$\nh_{\\text{post}} =\n\\begin{pmatrix}\n\\Sigma_0^{-1} m_0 \\\\\n\\frac{1}{\\sigma^2}X^\\top y_1 \\\\\n\\vdots \\\\\n\\frac{1}{\\sigma^2}X^\\top y_S\n\\end{pmatrix}\n$$\n后验均值 $\\hat{\\theta}^{\\text{post}} = [\\hat{\\mu}^{\\text{post}\\top}, \\hat{\\beta}_1^{\\text{post}\\top}, \\dots, \\hat{\\beta}_S^{\\text{post}\\top}]^\\top$ 是线性系统 $J_{\\text{post}} \\hat{\\theta}^{\\text{post}} = h_{\\text{post}}$ 的解。\n\n### 3. 通过分块分解求解系统\n直接求解这个大型系统效率低下。我们可以通过分块分解更有效地求解它。该方程组为：\n$$\n(S\\Sigma_b^{-1} + \\Sigma_0^{-1}) \\hat{\\mu}^{\\text{post}} - \\sum_{s=1}^S \\Sigma_b^{-1} \\hat{\\beta}_s^{\\text{post}} = \\Sigma_0^{-1} m_0 \\quad (1)\n$$\n$$\n-\\Sigma_b^{-1} \\hat{\\mu}^{\\text{post}} + \\left(\\frac{1}{\\sigma^2}X^\\top X + \\Sigma_b^{-1}\\right) \\hat{\\beta}_s^{\\text{post}} = \\frac{1}{\\sigma^2}X^\\top y_s \\quad \\text{for } s=1, \\dots, S \\quad (2)\n$$\n我们定义被试水平的后验精度（假设 $\\mu$ 已知）为 $J_\\beta = \\frac{1}{\\sigma^2}X^\\top X + \\Sigma_b^{-1}$，以及来自数据的被试特定的势贡献为 $h_s = \\frac{1}{\\sigma^2}X^\\top y_s$。方程 $(2)$ 变为：\n$$\n-\\Sigma_b^{-1} \\hat{\\mu}^{\\text{post}} + J_\\beta \\hat{\\beta}_s^{\\text{post}} = h_s\n$$\n由此，我们可以用 $\\hat{\\mu}^{\\text{post}}$ 来表示 $\\hat{\\beta}_s^{\\text{post}}$：\n$$\n\\hat{\\beta}_s^{\\text{post}} = J_\\beta^{-1} (h_s + \\Sigma_b^{-1} \\hat{\\mu}^{\\text{post}}) \\quad (3)\n$$\n将 $(3)$ 代入 $(1)$：\n$$\n(S\\Sigma_b^{-1} + \\Sigma_0^{-1}) \\hat{\\mu}^{\\text{post}} - \\sum_{s=1}^S \\Sigma_b^{-1} J_\\beta^{-1} (h_s + \\Sigma_b^{-1} \\hat{\\mu}^{\\text{post}}) = \\Sigma_0^{-1} m_0\n$$\n重新整理各项以求解 $\\hat{\\mu}^{\\text{post}}$：\n$$\n\\left(S\\Sigma_b^{-1} + \\Sigma_0^{-1} - S\\Sigma_b^{-1} J_\\beta^{-1} \\Sigma_b^{-1}\\right) \\hat{\\mu}^{\\text{post}} = \\Sigma_0^{-1} m_0 + \\sum_{s=1}^S \\Sigma_b^{-1} J_\\beta^{-1} h_s\n$$\n这是一个关于 $\\hat{\\mu}^{\\text{post}}$ 的 $p \\times p$ 线性系统。令 $J_\\mu = S\\Sigma_b^{-1} + \\Sigma_0^{-1} - S\\Sigma_b^{-1} J_\\beta^{-1} \\Sigma_b^{-1}$ 且 $h_\\mu = \\Sigma_0^{-1} m_0 + \\Sigma_b^{-1} J_\\beta^{-1} \\sum_{s=1}^S h_s$。那么 $\\hat{\\mu}^{\\text{post}}$ 就是 $J_\\mu \\hat{\\mu}^{\\text{post}} = h_\\mu$ 的解。\n\n### 4. 算法流程\n计算步骤如下：\n1.  对于给定的测试用例，使用提供的公式和伪随机数种子构建设计矩阵 $X \\in \\mathbb{R}^{T \\times p}$ 和被试响应向量 $y_s \\in \\mathbb{R}^T$（$s=1, \\dots, S$）。\n2.  为每个被试计算独立的普通最小二乘（OLS）估计：$\\hat{\\beta}^{\\text{ind}}_s = (X^\\top X)^{-1} X^\\top y_s$。\n3.  计算后验计算所需的矩阵：\n    *   $\\Sigma_0^{-1}$ 和 $\\Sigma_b^{-1}$（因为它们是对角矩阵，所以计算很简单）。\n    *   $J_\\beta = \\frac{1}{\\sigma^2}X^\\top X + \\Sigma_b^{-1}$。\n4.  求解后验组均值 $\\hat{\\mu}^{\\text{post}}$：\n    *   计算 $\\mu$ 的精度：$J_\\mu = S\\Sigma_b^{-1} + \\Sigma_0^{-1} - S\\Sigma_b^{-1} J_\\beta^{-1} \\Sigma_b^{-1}$。\n    *   计算 $\\mu$ 的势：$h_\\mu = \\Sigma_0^{-1} m_0 + \\Sigma_b^{-1} J_\\beta^{-1} \\sum_{s=1}^S (\\frac{1}{\\sigma^2}X^\\top y_s)$。\n    *   求解系统 $J_\\mu \\hat{\\mu}^{\\text{post}} = h_\\mu$。\n5.  使用方程 $(3)$ 为每个被试求解后验被试特定均值 $\\hat{\\beta}_s^{\\text{post}}$：\n    *   求解系统 $J_\\beta \\hat{\\beta}_s^{\\text{post}} = \\frac{1}{\\sigma^2}X^\\top y_s + \\Sigma_b^{-1} \\hat{\\mu}^{\\text{post}}$。\n6.  对于每个被试 $s$，计算归一化的收缩幅度：\n    $$\n    \\text{shr}_s = \\frac{\\left\\lVert \\hat{\\beta}^{\\text{ind}}_s - \\hat{\\mu}^{\\text{post}} \\right\\rVert_2 - \\left\\lVert \\hat{\\beta}^{\\text{post}}_s - \\hat{\\mu}^{\\text{post}} \\right\\rVert_2}{\\max\\left(\\left\\lVert \\hat{\\beta}^{\\text{ind}}_s - \\hat{\\mu}^{\\text{post}} \\right\\rVert_2, \\varepsilon\\right)}\n    $$\n7.  计算所有被试的平均收缩：$\\overline{\\text{shr}} = \\frac{1}{S} \\sum_{s=1}^S \\text{shr}_s$。对每个测试用例实施此流程以获得最终结果。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the solution for the hierarchical Bayesian encoding model problem.\n    \"\"\"\n    \n    # Define test cases as per the problem statement.\n    test_cases = [\n        {'S': 3, 'T': 12, 'sigma2': 0.2, 'seed': 0},\n        {'S': 3, 'T': 12, 'sigma2': 3.0, 'seed': 1},\n        {'S': 3, 'T': 60, 'sigma2': 0.05, 'seed': 2}\n    ]\n\n    # Fixed parameters across all test cases\n    beta_true_all = [\n        np.array([1.5, -0.5, 0.2]),\n        np.array([1.0, -0.6, 0.1]),\n        np.array([1.8, -0.4, 0.3])\n    ]\n    m0 = np.array([0.0, 0.0, 0.0])\n    Sigma0 = np.diag([1.0, 1.0, 1.0])\n    Sigma_b = np.diag([0.4, 0.4, 0.4])\n    epsilon = 1e-12\n    \n    # Pre-compute inverse matrices (diagonal, so inversion is trivial)\n    Sigma0_inv = np.linalg.inv(Sigma0)\n    Sigma_b_inv = np.linalg.inv(Sigma_b)\n\n    results = []\n    for case in test_cases:\n        S = case['S']\n        T = case['T']\n        sigma2 = case['sigma2']\n        seed = case['seed']\n        p = 3\n\n        # 1. Construct design matrix X and response vectors y_s\n        t = np.arange(T)\n        x_stim = np.cos(2 * np.pi * t / T)\n        x_drift = t / T\n        x_int = np.ones(T)\n        X = np.stack([x_stim, x_drift, x_int], axis=1)\n\n        rng = np.random.default_rng(seed)\n        y_all = []\n        for s in range(S):\n            noise = rng.normal(0, np.sqrt(sigma2), size=T)\n            y_s = X @ beta_true_all[s] + noise\n            y_all.append(y_s)\n\n        # 2. Compute independent OLS estimates\n        XTX = X.T @ X\n        XTX_inv = np.linalg.inv(XTX)\n        beta_ind_all = []\n        for s in range(S):\n            Xy_s = X.T @ y_all[s]\n            beta_ind_s = XTX_inv @ Xy_s\n            beta_ind_all.append(beta_ind_s)\n\n        # 3. Compute matrices for posterior calculation\n        J_beta = (1 / sigma2) * XTX + Sigma_b_inv\n        J_beta_inv = np.linalg.inv(J_beta)\n\n        # 4. Solve for posterior group mean mu_post\n        J_mu = S * Sigma_b_inv + Sigma0_inv - S * (Sigma_b_inv @ J_beta_inv @ Sigma_b_inv)\n        \n        sum_h_s = np.zeros(p)\n        h_s_all = []\n        for s in range(S):\n            h_s = (1/sigma2) * (X.T @ y_all[s])\n            h_s_all.append(h_s)\n            sum_h_s += h_s\n            \n        h_mu_rhs = Sigma0_inv @ m0 + Sigma_b_inv @ J_beta_inv @ sum_h_s\n        mu_post = np.linalg.solve(J_mu, h_mu_rhs)\n\n        # 5. Solve for posterior subject-specific means beta_post_s\n        beta_post_all = []\n        for s in range(S):\n            beta_post_rhs = h_s_all[s] + Sigma_b_inv @ mu_post\n            beta_post_s = np.linalg.solve(J_beta, beta_post_rhs)\n            beta_post_all.append(beta_post_s)\n\n        # 6. Calculate normalized shrinkage for each subject\n        shr_s_all = []\n        for s in range(S):\n            dist_ind = np.linalg.norm(beta_ind_all[s] - mu_post)\n            dist_post = np.linalg.norm(beta_post_all[s] - mu_post)\n            \n            denominator = max(dist_ind, epsilon)\n            shr_s = (dist_ind - dist_post) / denominator\n            shr_s_all.append(shr_s)\n\n        # 7. Compute average shrinkage\n        avg_shr = np.mean(shr_s_all)\n        results.append(avg_shr)\n    \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}