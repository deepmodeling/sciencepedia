{
    "hands_on_practices": [
        {
            "introduction": "The spike-triggered average (STA) is a widely used and intuitive method for estimating a neuron's receptive field. However, this simple average is only an unbiased estimator under very specific assumptions about the stimulus statistics. This practice guides you through a formal derivation to reveal how correlations in the stimulus systematically distort the STA, transforming it into a \"colored\" version of the true receptive field. Mastering this derivation  is fundamental to understanding the intimate link between stimulus design and the interpretability of encoding model parameters.",
            "id": "4155364",
            "problem": "Consider a discrete-time encoding model for a single neuron in which the stimulus at time index $t$ is a vector $x_t \\in \\mathbb{R}^{p}$ representing the recent stimulus history over $p$ lags, and spikes arise from a Linear-Nonlinear-Poisson (LNP) model with conditional intensity $\\lambda_t = \\exp(\\alpha + k^{\\top} x_t)$, where $k \\in \\mathbb{R}^{p}$ is the receptive field and $\\alpha \\in \\mathbb{R}$ is a baseline parameter. Assume $\\{x_t\\}$ are independent and identically distributed draws from a zero-mean multivariate Gaussian distribution with covariance matrix $\\Sigma \\in \\mathbb{R}^{p \\times p}$ that is symmetric positive definite. Define the spike-triggered average (STA) at a generic time as the conditional mean stimulus given that a spike occurs, i.e., $\\mathrm{STA} := \\mathbb{E}[x_t \\mid \\text{spike at } t]$.\n\nStarting from fundamental definitions of conditional expectation for point processes and the moment-generating properties of the multivariate normal distribution, derive $\\mathbb{E}[\\mathrm{STA}]$ in terms of $\\Sigma$ and $k$. Using this expression, explain mathematically why the STA yields an unbiased receptive field estimate only when the stimulus is Gaussian white noise with unit covariance, i.e., $\\Sigma = I$, and characterize the bias incurred when $\\Sigma \\neq I$ if one uses the STA directly to estimate $k$ without any correction.\n\nYour final answer should be the closed-form analytical expression for the bias vector under correlated stimuli. No numerical approximation is required, and no units are involved. Do not express the final answer as an equation; provide only the expression for the bias.",
            "solution": "The problem statement is evaluated as valid. It is scientifically grounded in the theory of neural encoding models, well-posed with sufficient information for a unique mathematical derivation, and objectively formulated. The problem requires a standard, albeit non-trivial, derivation in computational neuroscience.\n\nWe are tasked with deriving the spike-triggered average (STA) for a neuron described by a Linear-Nonlinear-Poisson (LNP) model and analyzing its bias as an estimator for the neuron's receptive field.\n\nFirst, we formalize the definition of the STA. The STA is defined as the conditional expectation of the stimulus $x_t$ given that a spike occurred at time $t$:\n$$\n\\mathrm{STA} := \\mathbb{E}[x_t \\mid \\text{spike at } t]\n$$\nUsing the definition of conditional expectation, this can be written as an integral over the posterior distribution of the stimulus given a spike:\n$$\n\\mathrm{STA} = \\int_{\\mathbb{R}^p} x \\, p(x \\mid \\text{spike}) \\, dx\n$$\nBy Bayes' rule, the posterior density $p(x \\mid \\text{spike})$ is related to the stimulus prior $p(x)$ and the conditional probability of a spike given the stimulus, $P(\\text{spike} \\mid x)$:\n$$\np(x \\mid \\text{spike}) = \\frac{P(\\text{spike} \\mid x) p(x)}{P(\\text{spike})}\n$$\nIn a Poisson point process model, the probability of observing a spike in a small time interval is proportional to the instantaneous firing rate, $\\lambda(x)$. Thus, we can set $P(\\text{spike} \\mid x) \\propto \\lambda(x)$. The unconditional probability of a spike, $P(\\text{spike})$, is then proportional to the average firing rate, $\\mathbb{E}[\\lambda(x)]$. The constants of proportionality cancel, leading to the widely used formula for the STA, also known as Stein's formula:\n$$\n\\mathrm{STA} = \\frac{\\int x \\lambda(x) p(x) \\, dx}{\\int \\lambda(x) p(x) \\, dx} = \\frac{\\mathbb{E}[x \\lambda(x)]}{\\mathbb{E}[\\lambda(x)]}\n$$\nThe problem specifies the stimulus distribution and the LNP model components:\n1.  The stimulus $x_t$ is drawn from a multivariate Gaussian distribution with mean $0$ and covariance $\\Sigma$, i.e., $x \\sim \\mathcal{N}(0, \\Sigma)$. Its probability density function (PDF) is:\n    $$\n    p(x) = \\frac{1}{\\sqrt{(2\\pi)^p \\det(\\Sigma)}} \\exp\\left(-\\frac{1}{2} x^{\\top} \\Sigma^{-1} x\\right)\n    $$\n2.  The conditional intensity (firing rate) is given by $\\lambda(x) = \\exp(\\alpha + k^{\\top} x)$, where $k$ is the receptive field and $\\alpha$ is a baseline parameter.\n\nWe now compute the denominator and the numerator of the STA expression.\n\nThe denominator is the expected firing rate, $\\mathbb{E}[\\lambda(x)]$:\n$$\n\\mathbb{E}[\\lambda(x)] = \\mathbb{E}[\\exp(\\alpha + k^{\\top} x)] = \\exp(\\alpha) \\mathbb{E}[\\exp(k^{\\top} x)]\n$$\nThe term $\\mathbb{E}[\\exp(k^{\\top} x)]$ is the moment-generating function (MGF) of the random variable $x \\sim \\mathcal{N}(0, \\Sigma)$, evaluated at the vector $k$. The MGF for a multivariate normal distribution $\\mathcal{N}(\\mu, \\Sigma)$ is $M_x(t) = \\exp(\\mu^{\\top}t + \\frac{1}{2} t^{\\top}\\Sigma t)$. For our case, with $\\mu=0$, the MGF is $M_x(t) = \\exp(\\frac{1}{2} t^{\\top}\\Sigma t)$. Evaluating at $t=k$, we obtain:\n$$\n\\mathbb{E}[\\exp(k^{\\top} x)] = \\exp\\left(\\frac{1}{2} k^{\\top}\\Sigma k\\right)\n$$\nTherefore, the denominator is:\n$$\n\\mathbb{E}[\\lambda(x)] = \\exp\\left(\\alpha + \\frac{1}{2} k^{\\top}\\Sigma k\\right)\n$$\n\nThe numerator is the expectation of the product of the stimulus and the firing rate, $\\mathbb{E}[x \\lambda(x)]$:\n$$\n\\mathbb{E}[x \\lambda(x)] = \\int_{\\mathbb{R}^p} x \\, \\exp(\\alpha + k^{\\top} x) \\, p(x) \\, dx = \\exp(\\alpha) \\int_{\\mathbb{R}^p} x \\, \\exp(k^{\\top} x) \\frac{\\exp(-\\frac{1}{2} x^{\\top} \\Sigma^{-1} x)}{\\sqrt{(2\\pi)^p \\det(\\Sigma)}} \\, dx\n$$\nThe argument of the exponentials inside the integral can be combined:\n$$\nk^{\\top} x - \\frac{1}{2} x^{\\top} \\Sigma^{-1} x\n$$\nWe complete the square for this quadratic form. A key identity is $(x-v)^{\\top}A(x-v) = x^{\\top}Ax - 2v^{\\top}Ax + v^{\\top}Av$. By setting $A=\\Sigma^{-1}$ and $v = \\Sigma k$, we find:\n$$\nx^{\\top}\\Sigma^{-1}x - 2k^{\\top}x = (x-\\Sigma k)^{\\top}\\Sigma^{-1}(x-\\Sigma k) - k^{\\top}\\Sigma\\Sigma^{-1}\\Sigma k = (x-\\Sigma k)^{\\top}\\Sigma^{-1}(x-\\Sigma k) - k^{\\top}\\Sigma k\n$$\nRearranging this identity, we have:\n$$\nk^{\\top}x - \\frac{1}{2}x^{\\top}\\Sigma^{-1}x = -\\frac{1}{2}\\left[(x-\\Sigma k)^{\\top}\\Sigma^{-1}(x-\\Sigma k) - k^{\\top}\\Sigma k\\right]\n$$\nSubstituting this back into the integral for the numerator:\n$$\n\\mathbb{E}[x \\lambda(x)] = \\exp(\\alpha) \\int x \\frac{\\exp\\left(-\\frac{1}{2}(x-\\Sigma k)^{\\top}\\Sigma^{-1}(x-\\Sigma k) + \\frac{1}{2}k^{\\top}\\Sigma k\\right)}{\\sqrt{(2\\pi)^p \\det(\\Sigma)}} \\, dx\n$$\nWe can factor out the term that does not depend on $x$:\n$$\n\\mathbb{E}[x \\lambda(x)] = \\exp\\left(\\alpha + \\frac{1}{2}k^{\\top}\\Sigma k\\right) \\int x \\frac{\\exp\\left(-\\frac{1}{2}(x - \\Sigma k)^{\\top}\\Sigma^{-1}(x - \\Sigma k)\\right)}{\\sqrt{(2\\pi)^p \\det(\\Sigma)}} \\, dx\n$$\nThe expression inside the integral is the product of $x$ and the PDF of a new multivariate Gaussian distribution, $\\mathcal{N}(\\Sigma k, \\Sigma)$. The integral, therefore, represents the expected value of a random vector drawn from this new distribution. The expected value is simply its mean, which is $\\Sigma k$.\nThus, the numerator evaluates to:\n$$\n\\mathbb{E}[x \\lambda(x)] = \\exp\\left(\\alpha + \\frac{1}{2}k^{\\top}\\Sigma k\\right) (\\Sigma k)\n$$\n\nNow we combine the numerator and denominator to find the STA:\n$$\n\\mathrm{STA} = \\frac{\\mathbb{E}[x \\lambda(x)]}{\\mathbb{E}[\\lambda(x)]} = \\frac{\\exp\\left(\\alpha + \\frac{1}{2}k^{\\top}\\Sigma k\\right) (\\Sigma k)}{\\exp\\left(\\alpha + \\frac{1}{2}k^{\\top}\\Sigma k\\right)} = \\Sigma k\n$$\nThis gives the expression for the STA in terms of the receptive field $k$ and the stimulus covariance $\\Sigma$. This is the first part of the problem.\n\nNext, we analyze the STA as an estimator for $k$. An estimator is unbiased if its expected value equals the true parameter. Here, we treat the STA, a quantity calculable from data, as the estimator for the true receptive field $k$. The estimator is unbiased if $\\mathrm{STA} = k$.\nFrom our result, we have $\\mathrm{STA} = \\Sigma k$. The condition for an unbiased estimate is therefore $\\Sigma k = k$. Since this must hold for any arbitrary receptive field $k \\in \\mathbb{R}^p$, the equality requires that the matrix $\\Sigma$ be the identity matrix, $\\Sigma = I$. This corresponds to the case of Gaussian white noise stimuli, where all stimulus dimensions are uncorrelated and have unit variance.\n\nWhen $\\Sigma \\neq I$, the STA is a biased estimator of $k$. The bias of an estimator is defined as the difference between the estimator and the true parameter value. The bias vector is:\n$$\n\\text{Bias} = \\mathrm{STA} - k = \\Sigma k - k\n$$\nFactoring out $k$, we obtain the closed-form expression for the bias:\n$$\n\\text{Bias} = (\\Sigma - I)k\n$$\nThis expression characterizes the distortion introduced by stimulus correlations. The STA is effectively a version of the true receptive field $k$ that has been \"colored\" by the covariance structure $\\Sigma$ of the stimulus. To obtain an unbiased estimate of $k$ from the STA in the general case, one would need to invert the covariance matrix and compute $k = \\Sigma^{-1} \\mathrm{STA}$, a procedure known as \"whitening\" the STA.",
            "answer": "$$\n\\boxed{(\\Sigma - I)k}\n$$"
        },
        {
            "introduction": "Building on the insight that correlated inputs can complicate interpretation, we now tackle a common practical challenge in constructing encoding models: multicollinearity among regressors. When modeling neural responses to complex events, features like a stimulus's perceived intensity and its duration are often correlated, making it difficult to isolate their unique neural correlates. This exercise  provides hands-on coding practice with Gram-Schmidt orthogonalization, a procedure to explicitly disentangle shared variance and clarify the contribution of each regressor. By fitting models with both the original and orthogonalized designs, you will develop a concrete understanding of how this procedure impacts coefficient estimates.",
            "id": "4155403",
            "problem": "You are given a trial-wise encoding model in neuroscience data analysis where correlated parametric modulators are used to explain a neural response. The modulators, called amplitude and duration, form two regressors that are often correlated. Your task is to construct and analyze an encoding model in a way that isolates unique contributions of each regressor by orthogonalization, and then to compare coefficients from the original correlated design to the coefficients from the orthogonalized design.\n\nStart from the following fundamental base: A General Linear Model (GLM) represents a response vector as a linear combination of regressors plus noise. Let the response vector be denoted by $y \\in \\mathbb{R}^{N}$, the design matrix by $X \\in \\mathbb{R}^{N \\times p}$, the coefficient vector by $\\beta \\in \\mathbb{R}^{p}$, and the noise by $\\varepsilon \\in \\mathbb{R}^{N}$. The GLM is $y = X \\beta + \\varepsilon$. The Ordinary Least Squares (OLS) estimator minimizes the sum of squared residuals and, when $X$ is full rank, yields the solution $\\hat{\\beta} = (X^{\\top} X)^{-1} X^{\\top} y$. When $X$ is rank-deficient, the Moore-Penrose pseudoinverse $X^{+}$ provides the minimal-norm least-squares solution $\\hat{\\beta} = X^{+} y$.\n\nOrthogonalization should be performed via the Gram-Schmidt procedure on the column space of the centered modulators. Specifically, given a centered amplitude regressor $x_{1} \\in \\mathbb{R}^{N}$ and a centered duration regressor $x_{2} \\in \\mathbb{R}^{N}$, define the orthogonalized duration regressor by removing the projection of $x_{2}$ onto $x_{1}$:\n$$\nx_{2}^{\\perp} = x_{2} - \\frac{x_{1}^{\\top} x_{2}}{x_{1}^{\\top} x_{1}} x_{1}.\n$$\nThe orthogonalized design uses regressors $[\\mathbf{1}, x_{1}, x_{2}^{\\perp}]$, where the $\\mathbf{1}$ denotes the intercept column and $x_{1}, x_{2}$ are both mean-centered so that the intercept is orthogonal to the modulators.\n\nImplement the following steps for each test case:\n- Construct $x_{1}$ and $x_{2}$ sequences according to the given formulas, with index $i$ running from $1$ to $N$.\n- Compute the centered modulators $x_{1}^{c} = x_{1} - \\bar{x}_{1}$ and $x_{2}^{c} = x_{2} - \\bar{x}_{2}$, where $\\bar{x}$ denotes the sample mean.\n- Compute $x_{2}^{\\perp}$ using the Gram-Schmidt formula above, with $x_{1}^{c}$ in place of $x_{1}$ and $x_{2}^{c}$ in place of $x_{2}$.\n- Form the original correlated design matrix $X_{\\text{orig}} = [\\mathbf{1}, x_{1}^{c}, x_{2}^{c}]$, and the orthogonalized design matrix $X_{\\text{orth}} = [\\mathbf{1}, x_{1}^{c}, x_{2}^{\\perp}]$.\n- Generate $y$ using the deterministic linear generative model $y_{i} = \\beta_{0} + \\beta_{1} x_{1,i} + \\beta_{2} x_{2,i} + \\varepsilon_{i}$ with the specified coefficients and deterministic noise sequence for each case.\n- Estimate $\\hat{\\beta}_{\\text{orig}}$ and $\\hat{\\beta}_{\\text{orth}}$ using the Moore-Penrose pseudoinverse, and report only the coefficients for the modulators (exclude the intercept) in the order $[\\hat{\\beta}_{1,\\text{orig}}, \\hat{\\beta}_{2,\\text{orig}}, \\hat{\\beta}_{1,\\text{orth}}, \\hat{\\beta}_{2,\\text{orth}}]$ as floats.\n\nTest suite specification:\n- Case $1$ (happy path, moderate correlation):\n  - Number of observations $N = 12$.\n  - For index $i \\in \\{1, 2, \\dots, 12\\}$:\n    - Amplitude $x_{1,i} = i$.\n    - Duration $x_{2,i} = 0.8 \\, i + 0.4 \\sin\\!\\left(\\frac{2 \\pi i}{12}\\right)$.\n    - Noise $\\varepsilon_{i} = 0.1 \\sin\\!\\left(\\frac{4 \\pi i}{12}\\right)$.\n  - Coefficients $\\beta_{0} = 0.2$, $\\beta_{1} = 0.7$, $\\beta_{2} = 0.3$.\n- Case $2$ (boundary, perfect collinearity):\n  - Number of observations $N = 10$.\n  - For index $i \\in \\{1, 2, \\dots, 10\\}$:\n    - Amplitude $x_{1,i} = i$.\n    - Duration $x_{2,i} = 2 \\, x_{1,i}$.\n    - Noise $\\varepsilon_{i} = 0.05 \\cos\\!\\left(\\frac{2 \\pi i}{10}\\right)$.\n  - Coefficients $\\beta_{0} = 0.0$, $\\beta_{1} = 1.0$, $\\beta_{2} = 0.5$.\n- Case $3$ (edge, near-zero correlation between modulators):\n  - Number of observations $N = 8$.\n  - For index $i \\in \\{1, 2, \\dots, 8\\}$:\n    - Amplitude $x_{1,i} = i$.\n    - Duration $x_{2,i} = 3 \\, (-1)^{i}$.\n    - Noise $\\varepsilon_{i} = 0.02 \\sin\\!\\left(\\frac{2 \\pi i}{8}\\right)$.\n  - Coefficients $\\beta_{0} = -0.1$, $\\beta_{1} = 0.5$, $\\beta_{2} = -0.4$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list of four floats in the order $[\\hat{\\beta}_{1,\\text{orig}}, \\hat{\\beta}_{2,\\text{orig}}, \\hat{\\beta}_{1,\\text{orth}}, \\hat{\\beta}_{2,\\text{orth}}]$. For example, the outer list should look like $[[b_{11},b_{12},b_{13},b_{14}],[b_{21},b_{22},b_{23},b_{24}],[b_{31},b_{32},b_{33},b_{34}]]$, where each $b_{jk}$ is a float.",
            "solution": "The problem requires the construction and analysis of a General Linear Model (GLM) to understand the impact of multicollinearity among regressors and the effect of orthogonalization on the estimated model coefficients. We will first establish the theoretical foundation, then proceed with the step-by-step implementation for each test case.\n\n**1. Theoretical Foundation: The General Linear Model and Multicollinearity**\n\nA General Linear Model (GLM) posits a linear relationship between a dependent variable $y$ and a set of explanatory variables (regressors) contained in a design matrix $X$. The model is expressed as:\n$$\ny = X \\beta + \\varepsilon\n$$\nwhere $y \\in \\mathbb{R}^{N}$ is the vector of $N$ observations, $X \\in \\mathbb{R}^{N \\times p}$ is the design matrix with $p$ regressors (including an intercept), $\\beta \\in \\mathbb{R}^{p}$ is the vector of unknown coefficients, and $\\varepsilon \\in \\mathbb{R}^{N}$ is a vector of error terms.\n\nThe goal is to estimate the coefficients $\\beta$. The Ordinary Least Squares (OLS) method finds the $\\hat{\\beta}$ that minimizes the sum of squared residuals, $\\|y - X\\beta\\|^2$. When the columns of $X$ are linearly independent (i.e., $X$ has full column rank), the unique OLS solution is given by:\n$$\n\\hat{\\beta} = (X^{\\top} X)^{-1} X^{\\top} y\n$$\nA critical issue arises when the regressors are correlated, a condition known as multicollinearity. If the correlation is perfect (i.e., one regressor is a linear combination of others), the columns of $X$ become linearly dependent, and $X$ is rank-deficient. In this case, the matrix $X^{\\top} X$ is singular and cannot be inverted. The OLS solution is no longer unique; there exists an infinite set of coefficient vectors that produce the same minimal sum of squared errors.\n\nTo obtain a single, well-defined solution even in the rank-deficient case, we use the Moore-Penrose pseudoinverse, denoted $X^{+}$. The solution is given by:\n$$\n\\hat{\\beta} = X^{+} y\n$$\nThis solution is the one with the minimum Euclidean norm ($\\|\\hat{\\beta}\\|_2$) among all possible least-squares solutions.\n\n**2. Orthogonalization via Gram-Schmidt**\n\nWhen regressors are correlated, their estimated coefficients are difficult to interpret. A coefficient $\\hat{\\beta}_j$ represents the change in $y$ for a one-unit change in the regressor $x_j$, holding all other regressors constant. But when regressors are correlated, it is statistically and conceptually problematic to change one while holding the others constant.\n\nOrthogonalization is a procedure to transform a set of correlated regressors into a set of uncorrelated (orthogonal) ones. This re-attributes the shared variance between regressors, often clarifying their contributions. We use the Gram-Schmidt process. Given two centered regressors, $x_1^c$ and $x_2^c$, we can create a new regressor, $x_2^{\\perp}$, that is orthogonal to $x_1^c$. This is achieved by subtracting the projection of $x_2^c$ onto $x_1^c$:\n$$\nx_{2}^{\\perp} = x_{2}^{c} - \\text{proj}_{x_1^c}(x_2^c) = x_{2}^{c} - \\frac{(x_{1}^{c})^{\\top} x_{2}^{c}}{(x_{1}^{c})^{\\top} x_{1}^{c}} x_{1}^{c}\n$$\nBy construction, $(x_{1}^{c})^{\\top} x_{2}^{\\perp} = 0$. We can then build a new GLM with the design matrix $X_{\\text{orth}} = [\\mathbf{1}, x_1^c, x_2^{\\perp}]$. Because the regressors are now orthogonal, their estimated coefficients are independent of each other. The order of orthogonalization matters; here, variance shared between $x_1^c$ and $x_2^c$ is attributed to $x_1^c$, and $x_2^{\\perp}$ represents only the variance in $x_2^c$ that is unique from $x_1^c$.\n\n**3. Relationship Between Coefficients**\n\nLet the original and orthogonalized models be:\n1.  Original model: $y = \\hat{\\beta}_{0,\\text{orig}}\\mathbf{1} + \\hat{\\beta}_{1,\\text{orig}} x_1^c + \\hat{\\beta}_{2,\\text{orig}} x_2^c + e_{\\text{orig}}$\n2.  Orthogonalized model: $y = \\hat{\\beta}_{0,\\text{orth}}\\mathbf{1} + \\hat{\\beta}_{1,\\text{orth}} x_1^c + \\hat{\\beta}_{2,\\text{orth}} x_2^{\\perp} + e_{\\text{orth}}$\n\nSince the OLS fit provides the best possible linear prediction, the fitted values and residuals from both models must be identical, as they span the same subspace (unless one is rank-deficient, a case we will address). By substituting the definition of $x_2^{\\perp}$ into the second equation, we get:\n$$\ny = \\hat{\\beta}_{0,\\text{orth}}\\mathbf{1} + \\hat{\\beta}_{1,\\text{orth}} x_1^c + \\hat{\\beta}_{2,\\text{orth}} \\left( x_{2}^{c} - \\frac{(x_{1}^{c})^{\\top} x_{2}^{c}}{(x_{1}^{c})^{\\top} x_{1}^{c}} x_{1}^{c} \\right) + e_{\\text{orth}}\n$$\n$$\ny = \\hat{\\beta}_{0,\\text{orth}}\\mathbf{1} + \\left( \\hat{\\beta}_{1,\\text{orth}} - \\hat{\\beta}_{2,\\text{orth}} \\frac{(x_{1}^{c})^{\\top} x_{2}^{c}}{(x_{1}^{c})^{\\top} x_{1}^{c}} \\right) x_1^c + \\hat{\\beta}_{2,\\text{orth}} x_2^c + e_{\\text{orth}}\n$$\nComparing the coefficients of $x_1^c$ and $x_2^c$ with the original model, we find the following relationships:\n$$\n\\hat{\\beta}_{2,\\text{orig}} = \\hat{\\beta}_{2,\\text{orth}}\n$$\n$$\n\\hat{\\beta}_{1,\\text{orig}} = \\hat{\\beta}_{1,\\text{orth}} - \\hat{\\beta}_{2,\\text{orth}} \\frac{(x_{1}^{c})^{\\top} x_{2}^{c}}{(x_{1}^{c})^{\\top} x_{1}^{c}}\n$$\nThis reveals that the coefficient for the regressor that was orthogonalized ($x_2^c \\to x_2^{\\perp}$) remains unchanged. However, the coefficient for the first regressor, $\\hat{\\beta}_1$, changes significantly. Its value in the orthogonalized model, $\\hat{\\beta}_{1,\\text{orth}}$, absorbs the explanatory power of the part of $x_2^c$ that is correlated with $x_1^c$.\n\n**4. Implementation Steps**\n\nFor each test case, we will perform the following calculations:\n\n1.  **Data Generation**: For a given number of observations $N$, we generate the base regressors $x_1$ and $x_2$, and the noise term $\\varepsilon$, using the provided formulas with an index $i$ from $1$ to $N$.\n2.  **Response Generation**: The response variable $y$ is synthesized using the true generative model: $y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + \\varepsilon_i$. Note that the model is specified with the non-centered regressors.\n3.  **Regressor Preparation**: We compute the centered regressors $x_1^c = x_1 - \\bar{x}_1$ and $x_2^c = x_2 - \\bar{x}_2$. The intercept in the GLM will account for the means.\n4.  **Orthogonalization**: The orthogonal regressor $x_2^{\\perp}$ is calculated using the Gram-Schmidt formula on the centered regressors.\n5.  **Design Matrix Construction**: Two design matrices are formed:\n    -   $X_{\\text{orig}} = [\\mathbf{1}, x_1^c, x_2^c]$ for the original correlated model.\n    -   $X_{\\text{orth}} = [\\mathbf{1}, x_1^c, x_2^{\\perp}]$ for the orthogonalized model.\n6.  **Coefficient Estimation**: We estimate the coefficient vectors $\\hat{\\beta}_{\\text{orig}}$ and $\\hat{\\beta}_{\\text{orth}}$ by applying the Moore-Penrose pseudoinverse method: $\\hat{\\beta} = X^{+} y$.\n7.  **Result Extraction**: From the estimated coefficient vectors $\\hat{\\beta}_{\\text{orig}} = [\\hat{\\beta}_{0,\\text{orig}}, \\hat{\\beta}_{1,\\text{orig}}, \\hat{\\beta}_{2,\\text{orig}}]^{\\top}$ and $\\hat{\\beta}_{\\text{orth}} = [\\hat{\\beta}_{0,\\text{orth}}, \\hat{\\beta}_{1,\\text{orth}}, \\hat{\\beta}_{2,\\text{orth}}]^{\\top}$, we extract the coefficients for the two modulators and report them in the specified order: $[\\hat{\\beta}_{1,\\text{orig}}, \\hat{\\beta}_{2,\\text{orig}}, \\hat{\\beta}_{1,\\text{orth}}, \\hat{\\beta}_{2,\\text{orth}}]$.\n\nThis procedure is applied to all three test cases, which cover scenarios of moderate correlation, perfect collinearity, and near-zero correlation, demonstrating the behavior of the coefficient estimates under these different conditions.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the neuroscience encoding model problem for three test cases.\n\n    For each case, it constructs correlated and orthogonalized design matrices,\n    simulates neural response data, fits a GLM to both designs using the\n    Moore-Penrose pseudoinverse, and reports the resulting coefficients\n    for the non-intercept regressors.\n    \"\"\"\n\n    test_cases = [\n        # Case 1: Moderate correlation\n        {\n            \"N\": 12,\n            \"x1_func\": lambda i: i,\n            \"x2_func\": lambda i: 0.8 * i + 0.4 * np.sin(2 * np.pi * i / 12),\n            \"eps_func\": lambda i: 0.1 * np.sin(4 * np.pi * i / 12),\n            \"betas\": [0.2, 0.7, 0.3]  # beta_0, beta_1, beta_2\n        },\n        # Case 2: Perfect collinearity\n        {\n            \"N\": 10,\n            \"x1_func\": lambda i: i,\n            \"x2_func\": lambda i: 2.0 * i,\n            \"eps_func\": lambda i: 0.05 * np.cos(2 * np.pi * i / 10),\n            \"betas\": [0.0, 1.0, 0.5]\n        },\n        # Case 3: Near-zero correlation\n        {\n            \"N\": 8,\n            \"x1_func\": lambda i: i,\n            \"x2_func\": lambda i: 3.0 * (-1)**i,\n            \"eps_func\": lambda i: 0.02 * np.sin(2 * np.pi * i / 8),\n            \"betas\": [-0.1, 0.5, -0.4]\n        }\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        N = case[\"N\"]\n        beta0, beta1, beta2 = case[\"betas\"]\n        \n        # Step 1: Construct x1, x2, and epsilon sequences.\n        # Index i runs from 1 to N.\n        i = np.arange(1, N + 1)\n        x1 = case[\"x1_func\"](i)\n        x2 = case[\"x2_func\"](i)\n        eps = case[\"eps_func\"](i)\n\n        # Step 2: Compute centered modulators.\n        x1_c = x1 - np.mean(x1)\n        x2_c = x2 - np.mean(x2)\n\n        # Step 3: Compute orthogonalized duration regressor x2_perp.\n        # Handle the case where x1_c is a zero vector to avoid division by zero.\n        dot_x1c_x1c = np.dot(x1_c, x1_c)\n        if np.isclose(dot_x1c_x1c, 0):\n            # If x1_c is zero, its projection is zero, so x2_perp is just x2_c.\n            proj_x2_on_x1 = np.zeros_like(x1_c)\n        else:\n            proj_x2_on_x1 = (np.dot(x1_c, x2_c) / dot_x1c_x1c) * x1_c\n        \n        x2_perp = x2_c - proj_x2_on_x1\n\n        # Step 4: Form the design matrices.\n        intercept = np.ones(N)\n        X_orig = np.column_stack([intercept, x1_c, x2_c])\n        X_orth = np.column_stack([intercept, x1_c, x2_perp])\n\n        # Step 5: Generate y using the deterministic linear generative model.\n        # Note: The generative model uses the original, non-centered regressors.\n        y = beta0 + beta1 * x1 + beta2 * x2 + eps\n\n        # Step 6: Estimate beta_orig and beta_orth using Moore-Penrose pseudoinverse.\n        beta_hat_orig = np.linalg.pinv(X_orig) @ y\n        beta_hat_orth = np.linalg.pinv(X_orth) @ y\n\n        # Report only the coefficients for the modulators (exclude the intercept).\n        # Order: [beta1_orig, beta2_orig, beta1_orth, beta2_orth]\n        results = [\n            beta_hat_orig[1], \n            beta_hat_orig[2], \n            beta_hat_orth[1], \n            beta_hat_orth[2]\n        ]\n        all_results.append(results)\n\n    # Format the final output string.\n    output_str = \"[\" + \",\".join([f\"[{','.join(map(str, res))}]\" for res in all_results]) + \"]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "After mastering challenges within single-subject models, the next step is to combine data from multiple subjects to draw more generalizable conclusions. Hierarchical Bayesian models offer a principled solution, balancing individual variability with group-level consistency by allowing subjects to \"borrow statistical strength\" from one another. This practice  moves beyond black-box packages, guiding you to implement the analytical solution for a conjugate linear-Gaussian model. You will directly compute the posterior parameter estimates and quantify the resulting \"shrinkage\" effect, a hallmark of hierarchical modeling that yields more robust and powerful inferences.",
            "id": "4155376",
            "problem": "A laboratory has recorded multi-subject neural time series during presentation of a common stimulus. The goal is to design a principled encoding model with an explicit set of covariates and to quantify how hierarchical sharing across subjects induces posterior shrinkage of subject-specific parameters toward a population mean. The encoding model uses a linear design matrix with a stimulus regressor and nuisance covariates, and a hierarchical Gaussian prior tying subject-specific coefficients to a shared group mean.\n\nAssume the following linear-Gaussian encoding model for each subject $s \\in \\{1,\\dots,S\\}$ at time samples $t \\in \\{0,1,\\dots,T-1\\}$:\n- The observed response vector is $y_s \\in \\mathbb{R}^T$.\n- The design matrix is $X \\in \\mathbb{R}^{T \\times p}$ with $p$ covariates. Columns of $X$ represent:\n  - A stimulus feature $x_{\\text{stim},t}$, a deterministic function of time designed to capture stimulus-locked structure.\n  - A nuisance drift covariate $x_{\\text{drift},t}$ to capture slow baseline fluctuations.\n  - An intercept $x_{\\text{int},t}$ to model constant offsets.\n\nThe encoding model and hierarchical priors are:\n- Likelihood (linear Gaussian): $y_s \\mid \\beta_s \\sim \\mathcal{N}(X \\beta_s, \\sigma^2 I_T)$, where $\\beta_s \\in \\mathbb{R}^p$ is the subject-specific parameter vector, $\\sigma^2 > 0$ is the known noise variance, and $I_T$ is the $T \\times T$ identity.\n- Subject-level prior: $\\beta_s \\mid \\mu \\sim \\mathcal{N}(\\mu, \\Sigma_b)$, where $\\mu \\in \\mathbb{R}^p$ is a shared group mean and $\\Sigma_b \\in \\mathbb{R}^{p \\times p}$ is a known positive definite covariance.\n- Hyperprior on the group mean: $\\mu \\sim \\mathcal{N}(m_0, \\Sigma_0)$, with known $m_0 \\in \\mathbbR}^p$ and positive definite $\\Sigma_0 \\in \\mathbb{R}^{p \\times p}$.\n\nThis is a conjugate linear-Gaussian hierarchy. Starting from the fundamental base facts that the product of Gaussian densities yields another Gaussian and that linear transformations of Gaussian random variables remain Gaussian, one can derive that the joint posterior $p(\\mu, \\{\\beta_s\\}_{s=1}^S \\mid \\{y_s\\}_{s=1}^S)$ is multivariate normal. The posterior mean can be obtained by solving a linear system formed by the joint Gaussian natural parameters. From these posterior means, posterior shrinkage is quantified by how much the subject-specific posterior mean moves toward the group mean relative to the independent (non-hierarchical) estimate.\n\nDefine the independent estimate for subject $s$ as the ordinary least squares solution $\\hat{\\beta}^{\\text{ind}}_s = (X^\\top X)^{-1} X^\\top y_s$. Let the posterior mean of the group mean be $\\hat{\\mu}^{\\text{post}}$ and the posterior mean of the subject-specific parameters be $\\hat{\\beta}^{\\text{post}}_s$. For each subject $s$, define the normalized shrinkage magnitude\n$$\n\\text{shr}_s = \\frac{\\left\\lVert \\hat{\\beta}^{\\text{ind}}_s - \\hat{\\mu}^{\\text{post}} \\right\\rVert_2 - \\left\\lVert \\hat{\\beta}^{\\text{post}}_s - \\hat{\\mu}^{\\text{post}} \\right\\rVert_2}{\\max\\left(\\left\\lVert \\hat{\\beta}^{\\text{ind}}_s - \\hat{\\mu}^{\\text{post}} \\right\\rVert_2, \\varepsilon\\right)},\n$$\nwhere $\\varepsilon$ is a small positive constant to avoid division by zero if the denominator vanishes. The average shrinkage across subjects is then\n$$\n\\overline{\\text{shr}} = \\frac{1}{S} \\sum_{s=1}^S \\text{shr}_s,\n$$\nwhich must be reported as a decimal number. All angles used in any trigonometric function must be interpreted in radians.\n\nYour program must implement the above hierarchical encoding model and compute $\\overline{\\text{shr}}$ for each of the following test cases. The design matrix $X$ and responses $y_s$ must be constructed exactly as specified to ensure deterministic results.\n\nFor all test cases, the design matrix has $p = 3$ columns:\n- Column $1$ (stimulus covariate): $x_{\\text{stim},t} = \\cos\\left(\\frac{2\\pi t}{T}\\right)$.\n- Column $2$ (nuisance drift): $x_{\\text{drift},t} = \\frac{t}{T}$.\n- Column $3$ (intercept): $x_{\\text{int},t} = 1$.\n\nThe true subject-specific parameters for subjects $s \\in \\{1,2,3\\}$ are fixed across all test cases:\n- Subject $1$: $\\beta^{\\text{true}}_1 = [1.5, -0.5, 0.2]^\\top$.\n- Subject $2$: $\\beta^{\\text{true}}_2 = [1.0, -0.6, 0.1]^\\top$.\n- Subject $3$: $\\beta^{\\text{true}}_3 = [1.8, -0.4, 0.3]^\\top$.\n\nThe hyperparameters are constant across all test cases:\n- Group prior mean: $m_0 = [0, 0, 0]^\\top$.\n- Group prior covariance: $\\Sigma_0 = \\text{diag}([1.0, 1.0, 1.0])$.\n- Subject prior covariance: $\\Sigma_b = \\text{diag}([0.4, 0.4, 0.4])$.\n\nFor each test case, construct $y_s$ using\n$$\ny_s = X \\beta^{\\text{true}}_s + \\eta_s,\n$$\nwhere $\\eta_s \\in \\mathbb{R}^T$ is a noise vector with independent entries drawn from $\\mathcal{N}(0, \\sigma^2)$ using a fixed pseudorandom generator seed as specified below.\n\nTest Suite:\n- Case $1$ (happy path): $S = 3$, $T = 12$, $\\sigma^2 = 0.2$, noise seed $0$.\n- Case $2$ (high-noise boundary): $S = 3$, $T = 12$, $\\sigma^2 = 3.0$, noise seed $1$.\n- Case $3$ (large-sample edge): $S = 3$, $T = 60$, $\\sigma^2 = 0.05$, noise seed $2$.\n\nRequirements:\n- Implement the hierarchical posterior computation by constructing and solving the joint linear system implied by the multivariate normal posterior in natural parameter form. Do not rely on any black-box Bayesian sampling.\n- Compute the independent estimates $\\hat{\\beta}^{\\text{ind}}_s$, the posterior means $\\hat{\\mu}^{\\text{post}}$ and $\\hat{\\beta}^{\\text{post}}_s$, and the average shrinkage $\\overline{\\text{shr}}$ for each case, with $\\varepsilon = 10^{-12}$.\n- The final output must be a single line containing a list of the average shrinkage values for the three test cases, in the order listed, as a comma-separated list enclosed in square brackets (for example, $[\\text{result}_1,\\text{result}_2,\\text{result}_3]$). Each result must be a float (decimal number). No additional text may be printed.",
            "solution": "The problem is valid. It is scientifically grounded in Bayesian statistical modeling, a standard technique in neuroscience data analysis. The problem is well-posed, providing all necessary equations, parameters, and data generation procedures to arrive at a unique, meaningful solution. It is objective and self-contained, with no ambiguities that would prevent a rigorous formalization.\n\n### 1. Model Specification and Posterior Derivation\nThe problem describes a three-level hierarchical Gaussian model. For each subject $s \\in \\{1, \\dots, S\\}$, the model is defined as:\n- **Level 1 (Likelihood):** $p(y_s | \\beta_s) = \\mathcal{N}(y_s; X \\beta_s, \\sigma^2 I_T)$\n- **Level 2 (Subject-level prior):** $p(\\beta_s | \\mu) = \\mathcal{N}(\\beta_s; \\mu, \\Sigma_b)$\n- **Level 3 (Group-level hyperprior):** $p(\\mu) = \\mathcal{N}(\\mu; m_0, \\Sigma_0)$\n\nThe joint posterior distribution over all unknown parameters, $\\theta = [\\mu^\\top, \\beta_1^\\top, \\dots, \\beta_S^\\top]^\\top$, is given by Bayes' theorem:\n$$\np(\\mu, \\{\\beta_s\\}_{s=1}^S | \\{y_s\\}_{s=1}^S) \\propto p(\\mu) \\prod_{s=1}^S p(y_s | \\beta_s) p(\\beta_s | \\mu)\n$$\nSince the likelihood and priors are all Gaussian, their product is also a Gaussian distribution. The log of the joint posterior, up to a constant, is a quadratic function of the parameters:\n$$\n\\log p(\\theta | \\{y_s\\}) = C - \\frac{1}{2} \\left[ (\\mu - m_0)^\\top \\Sigma_0^{-1}(\\mu - m_0) + \\sum_{s=1}^S (\\beta_s - \\mu)^\\top \\Sigma_b^{-1}(\\beta_s - \\mu) + \\frac{1}{\\sigma^2} \\sum_{s=1}^S (y_s - X\\beta_s)^\\top(y_s - X\\beta_s) \\right]\n$$\nFor a multivariate Gaussian distribution, the posterior mean coincides with the posterior mode. We can find this by setting the gradient of the log-posterior with respect to $\\theta$ to zero. This is equivalent to solving the linear system defined by the natural parameters of the joint posterior distribution.\n\n### 2. The Linear System for Posterior Means\nA Gaussian distribution $p(\\theta) \\propto \\exp(-\\frac{1}{2}\\theta^\\top J \\theta + h^\\top\\theta)$ is parameterized by its precision matrix $J$ and potential vector $h$. The mean is found by solving the system $J\\theta = h$.\nBy expanding the quadratic forms in the log-posterior, we can identify the block structure of the joint precision matrix $J_{\\text{post}}$ and the joint potential vector $h_{\\text{post}}$. The parameter vector is $\\theta = [\\mu^\\top, \\beta_1^\\top, \\dots, \\beta_S^\\top]^\\top$.\n\nThe joint precision matrix $J_{\\text{post}}$ is a $(S+1)p \\times (S+1)p$ block matrix:\n$$\nJ_{\\text{post}} =\n\\begin{pmatrix}\nS\\Sigma_b^{-1} + \\Sigma_0^{-1} & -\\Sigma_b^{-1} & \\cdots & -\\Sigma_b^{-1} \\\\\n-\\Sigma_b^{-1} & \\frac{1}{\\sigma^2}X^\\top X + \\Sigma_b^{-1} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n-\\Sigma_b^{-1} & 0 & \\cdots & \\frac{1}{\\sigma^2}X^\\top X + \\Sigma_b^{-1}\n\\end{pmatrix}\n$$\nThe joint potential vector $h_{\\text{post}}$ is a $(S+1)p$-dimensional vector:\n$$\nh_{\\text{post}} =\n\\begin{pmatrix}\n\\Sigma_0^{-1} m_0 \\\\\n\\frac{1}{\\sigma^2}X^\\top y_1 \\\\\n\\vdots \\\\\n\\frac{1}{\\sigma^2}X^\\top y_S\n\\end{pmatrix}\n$$\nThe posterior means $\\hat{\\theta}^{\\text{post}} = [\\hat{\\mu}^{\\text{post}\\top}, \\hat{\\beta}_1^{\\text{post}\\top}, \\dots, \\hat{\\beta}_S^{\\text{post}\\top}]^\\top$ are the solution to the linear system $J_{\\text{post}} \\hat{\\theta}^{\\text{post}} = h_{\\text{post}}$.\n\n### 3. Solving the System via Block-wise Decomposition\nSolving this large system directly is inefficient. We can solve it more effectively by block-wise decomposition. The system of equations is:\n$$\n(S\\Sigma_b^{-1} + \\Sigma_0^{-1}) \\hat{\\mu}^{\\text{post}} - \\sum_{s=1}^S \\Sigma_b^{-1} \\hat{\\beta}_s^{\\text{post}} = \\Sigma_0^{-1} m_0 \\quad (1)\n$$\n$$\n-\\Sigma_b^{-1} \\hat{\\mu}^{\\text{post}} + \\left(\\frac{1}{\\sigma^2}X^\\top X + \\Sigma_b^{-1}\\right) \\hat{\\beta}_s^{\\text{post}} = \\frac{1}{\\sigma^2}X^\\top y_s \\quad \\text{for } s=1, \\dots, S \\quad (2)\n$$\nLet's define the subject-level posterior precision, assuming $\\mu$ were known, as $J_\\beta = \\frac{1}{\\sigma^2}X^\\top X + \\Sigma_b^{-1}$, and the subject-specific potential contribution from the data as $h_s = \\frac{1}{\\sigma^2}X^\\top y_s$. Equation $(2)$ becomes:\n$$\n-\\Sigma_b^{-1} \\hat{\\mu}^{\\text{post}} + J_\\beta \\hat{\\beta}_s^{\\text{post}} = h_s\n$$\nFrom this, we can express $\\hat{\\beta}_s^{\\text{post}}$ in terms of $\\hat{\\mu}^{\\text{post}}$:\n$$\n\\hat{\\beta}_s^{\\text{post}} = J_\\beta^{-1} (h_s + \\Sigma_b^{-1} \\hat{\\mu}^{\\text{post}}) \\quad (3)\n$$\nSubstituting $(3)$ into $(1)$:\n$$\n(S\\Sigma_b^{-1} + \\Sigma_0^{-1}) \\hat{\\mu}^{\\text{post}} - \\sum_{s=1}^S \\Sigma_b^{-1} J_\\beta^{-1} (h_s + \\Sigma_b^{-1} \\hat{\\mu}^{\\text{post}}) = \\Sigma_0^{-1} m_0\n$$\nRearranging terms to solve for $\\hat{\\mu}^{\\text{post}}$:\n$$\n\\left(S\\Sigma_b^{-1} + \\Sigma_0^{-1} - S\\Sigma_b^{-1} J_\\beta^{-1} \\Sigma_b^{-1}\\right) \\hat{\\mu}^{\\text{post}} = \\Sigma_0^{-1} m_0 + \\sum_{s=1}^S \\Sigma_b^{-1} J_\\beta^{-1} h_s\n$$\nThis is a $p \\times p$ linear system for $\\hat{\\mu}^{\\text{post}}$. Let $J_\\mu = S\\Sigma_b^{-1} + \\Sigma_0^{-1} - S\\Sigma_b^{-1} J_\\beta^{-1} \\Sigma_b^{-1}$ and $h_\\mu = \\Sigma_0^{-1} m_0 + \\Sigma_b^{-1} J_\\beta^{-1} \\sum_{s=1}^S h_s$. Then $\\hat{\\mu}^{\\text{post}}$ is the solution to $J_\\mu \\hat{\\mu}^{\\text{post}} = h_\\mu$.\n\n### 4. Algorithmic Procedure\nThe computational steps are as follows:\n1.  For a given test case, construct the design matrix $X \\in \\mathbb{R}^{T \\times p}$ and the subject response vectors $y_s \\in \\mathbb{R}^T$ for $s=1, \\dots, S$ using the provided formulas and pseudorandom seed.\n2.  Compute the independent ordinary least squares (OLS) estimate for each subject: $\\hat{\\beta}^{\\text{ind}}_s = (X^\\top X)^{-1} X^\\top y_s$.\n3.  Compute the matrices required for the posterior calculation:\n    *   $\\Sigma_0^{-1}$ and $\\Sigma_b^{-1}$ (which are simple to compute as they are diagonal).\n    *   $J_\\beta = \\frac{1}{\\sigma^2}X^\\top X + \\Sigma_b^{-1}$.\n4.  Solve for the posterior group mean $\\hat{\\mu}^{\\text{post}}$:\n    *   Compute the precision for $\\mu$: $J_\\mu = S\\Sigma_b^{-1} + \\Sigma_0^{-1} - S\\Sigma_b^{-1} J_\\beta^{-1} \\Sigma_b^{-1}$.\n    *   Compute the potential for $\\mu$: $h_\\mu = \\Sigma_0^{-1} m_0 + \\Sigma_b^{-1} J_\\beta^{-1} \\sum_{s=1}^S (\\frac{1}{\\sigma^2}X^\\top y_s)$.\n    *   Solve the system $J_\\mu \\hat{\\mu}^{\\text{post}} = h_\\mu$.\n5.  Solve for the posterior subject-specific means $\\hat{\\beta}_s^{\\text{post}}$ for each subject using equation $(3)$:\n    *   Solve the system $J_\\beta \\hat{\\beta}_s^{\\text{post}} = \\frac{1}{\\sigma^2}X^\\top y_s + \\Sigma_b^{-1} \\hat{\\mu}^{\\text{post}}$.\n6.  For each subject $s$, calculate the normalized shrinkage magnitude:\n    $$\n    \\text{shr}_s = \\frac{\\left\\lVert \\hat{\\beta}^{\\text{ind}}_s - \\hat{\\mu}^{\\text{post}} \\right\\rVert_2 - \\left\\lVert \\hat{\\beta}^{\\text{post}}_s - \\hat{\\mu}^{\\text{post}} \\right\\rVert_2}{\\max\\left(\\left\\lVert \\hat{\\beta}^{\\text{ind}}_s - \\hat{\\mu}^{\\text{post}} \\right\\rVert_2, \\varepsilon\\right)}\n    $$\n7.  Compute the average shrinkage across all subjects: $\\overline{\\text{shr}} = \\frac{1}{S} \\sum_{s=1}^S \\text{shr}_s$.\nThis procedure is implemented for each test case to obtain the final results.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the solution for the hierarchical Bayesian encoding model problem.\n    \"\"\"\n    \n    # Define test cases as per the problem statement.\n    test_cases = [\n        {'S': 3, 'T': 12, 'sigma2': 0.2, 'seed': 0},\n        {'S': 3, 'T': 12, 'sigma2': 3.0, 'seed': 1},\n        {'S': 3, 'T': 60, 'sigma2': 0.05, 'seed': 2}\n    ]\n\n    # Fixed parameters across all test cases\n    beta_true_all = [\n        np.array([1.5, -0.5, 0.2]),\n        np.array([1.0, -0.6, 0.1]),\n        np.array([1.8, -0.4, 0.3])\n    ]\n    m0 = np.array([0.0, 0.0, 0.0])\n    Sigma0 = np.diag([1.0, 1.0, 1.0])\n    Sigma_b = np.diag([0.4, 0.4, 0.4])\n    epsilon = 1e-12\n    \n    # Pre-compute inverse matrices (diagonal, so inversion is trivial)\n    Sigma0_inv = np.linalg.inv(Sigma0)\n    Sigma_b_inv = np.linalg.inv(Sigma_b)\n\n    results = []\n    for case in test_cases:\n        S = case['S']\n        T = case['T']\n        sigma2 = case['sigma2']\n        seed = case['seed']\n        p = 3\n\n        # 1. Construct design matrix X and response vectors y_s\n        t = np.arange(T)\n        x_stim = np.cos(2 * np.pi * t / T)\n        x_drift = t / T\n        x_int = np.ones(T)\n        X = np.stack([x_stim, x_drift, x_int], axis=1)\n\n        rng = np.random.default_rng(seed)\n        y_all = []\n        for s in range(S):\n            noise = rng.normal(0, np.sqrt(sigma2), size=T)\n            y_s = X @ beta_true_all[s] + noise\n            y_all.append(y_s)\n\n        # 2. Compute independent OLS estimates\n        XTX = X.T @ X\n        XTX_inv = np.linalg.inv(XTX)\n        beta_ind_all = []\n        for s in range(S):\n            Xy_s = X.T @ y_all[s]\n            beta_ind_s = XTX_inv @ Xy_s\n            beta_ind_all.append(beta_ind_s)\n\n        # 3. Compute matrices for posterior calculation\n        J_beta = (1 / sigma2) * XTX + Sigma_b_inv\n        J_beta_inv = np.linalg.inv(J_beta)\n\n        # 4. Solve for posterior group mean mu_post\n        J_mu = S * Sigma_b_inv + Sigma0_inv - S * (Sigma_b_inv @ J_beta_inv @ Sigma_b_inv)\n        \n        sum_h_s = np.zeros(p)\n        h_s_all = []\n        for s in range(S):\n            h_s = (1/sigma2) * (X.T @ y_all[s])\n            h_s_all.append(h_s)\n            sum_h_s += h_s\n            \n        h_mu_rhs = Sigma0_inv @ m0 + Sigma_b_inv @ J_beta_inv @ sum_h_s\n        mu_post = np.linalg.solve(J_mu, h_mu_rhs)\n\n        # 5. Solve for posterior subject-specific means beta_post_s\n        beta_post_all = []\n        for s in range(S):\n            beta_post_rhs = h_s_all[s] + Sigma_b_inv @ mu_post\n            beta_post_s = np.linalg.solve(J_beta, beta_post_rhs)\n            beta_post_all.append(beta_post_s)\n\n        # 6. Calculate normalized shrinkage for each subject\n        shr_s_all = []\n        for s in range(S):\n            dist_ind = np.linalg.norm(beta_ind_all[s] - mu_post)\n            dist_post = np.linalg.norm(beta_post_all[s] - mu_post)\n            \n            denominator = max(dist_ind, epsilon)\n            shr_s = (dist_ind - dist_post) / denominator\n            shr_s_all.append(shr_s)\n\n        # 7. Compute average shrinkage\n        avg_shr = np.mean(shr_s_all)\n        results.append(avg_shr)\n    \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}