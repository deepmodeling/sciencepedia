{
    "hands_on_practices": [
        {
            "introduction": "本章节的第一个实践练习将从最基本的构件开始：单个二元事件的熵，我们用伯努利试验对其建模。这项练习对于建立信息熵的直观理解至关重要，因为它将熵的数学公式与不确定性的概念直接联系起来，揭示了当结果最不可预测时熵如何达到最大值。这是分析神经科学中离散事件（例如在特定时间窗内是否存在神经脉冲）的基础模型。",
            "id": "4170225",
            "problem": "在感觉刺激范式期间，对单个皮层神经元进行细胞外记录。时间被离散化为宽度为 $\\Delta t$ 的时间窗，使得在任何一个时间窗内观察到多于一个脉冲的概率可以忽略不计。为每个时间窗定义一个二元随机变量 $X \\in \\{0,1\\}$，其中 $X=1$ 表示存在至少一个脉冲，$X=0$ 表示没有脉冲。假设在时期内具有平稳性，因此 $X$ 服从伯努利$(\\theta)$分布，即随机变量 $\\mathbb{P}(X=1)=\\theta$ 和 $\\mathbb{P}(X=0)=1-\\theta$，其中 $\\theta \\in (0,1)$。\n\n仅使用离散随机变量的香农熵等于该分布下的期望自信息这一定义，并采用自然对数（因此信息单位为奈特），完成以下任务：\n\n1. 推导熵 $H(X)$ 关于 $\\theta$ 的闭式表达式。\n2. 通过计算关于 $\\theta$ 的一阶和二阶导数，分析 $H(X)$ 如何依赖于 $\\theta$，并确定使 $H(X)$ 最大化的值 $\\theta^{\\star}$ 以及相应的最大值。您的分析应明确证明在 $(0,\\tfrac{1}{2}]$ 和 $[\\tfrac{1}{2},1)$ 上的单调性以及在 $(0,1)$ 上的凹性。\n3. 在脉冲序列分析的背景下，简要解释为什么最大化值 $\\theta^{\\star}$ 与时间窗占用情况的最高不确定性相一致。\n\n以奈特为单位表示最终的熵表达式。您最终报告的答案必须是 $H(X)$ 作为 $\\theta$ 的函数的单个闭式解析表达式，不含任何额外文本或证明。无需四舍五入。",
            "solution": "该问题陈述经证实具有科学依据、问题明确、客观且自洽。它是在信息论应用于神经数据分析方面一个标准的基础练习。所有前提都合理，并提供了所有必需的信息。\n\n该问题要求分三部分作答：推导熵公式、分析此公式以及解释结果。\n\n### 1. 熵表达式 $H(X)$ 的推导\n\n对于概率质量函数为 $p(y) = \\mathbb{P}(Y=y)$ 的离散随机变量 $Y$，Shannon熵的基本定义是其期望自信息。一个结果 $y$ 的自信息为 $I(y) = -\\ln(p(y))$，其中使用自然对数，信息单位为奈特。熵 $H(Y)$ 是 $I(Y)$ 的期望值：\n$$H(Y) = \\mathbb{E}[I(Y)] = \\sum_{y \\in \\mathcal{Y}} p(y) I(y) = -\\sum_{y \\in \\mathcal{Y}} p(y) \\ln(p(y))$$\n在本问题中，随机变量是 $X$，它服从伯努利分布，$X \\sim \\text{Bernoulli}(\\theta)$。其取值集合为 $\\mathcal{X} = \\{0, 1\\}$，两种结果的概率如下：\n$$ \\mathbb{P}(X=1) = \\theta $$\n$$ \\mathbb{P}(X=0) = 1-\\theta $$\n将熵的定义应用于此特定情况，我们对 $X$ 的两种可能结果求和：\n$$ H(X) = - \\big[ \\mathbb{P}(X=0)\\ln(\\mathbb{P}(X=0)) + \\mathbb{P}(X=1)\\ln(\\mathbb{P}(X=1)) \\big] $$\n代入给定的概率，我们得到二元随机变量 $X$ 的熵作为参数 $\\theta$ 的函数表达式：\n$$ H(X) = H(\\theta) = - \\big[ (1-\\theta)\\ln(1-\\theta) + \\theta\\ln(\\theta) \\big] $$\n这可以写成：\n$$ H(\\theta) = -(1-\\theta)\\ln(1-\\theta) - \\theta\\ln(\\theta) $$\n这就是二元熵函数，也是 $H(X)$ 关于 $\\theta$ 的闭式表达式。\n\n### 2. $H(\\theta)$ 的分析\n\n为了分析 $H(\\theta)$ 如何依赖于 $\\theta$，我们计算它对于 $\\theta \\in (0,1)$ 的一阶和二阶导数。\n\n**一阶导数与单调性：**\n我们对 $H(\\theta)$ 的每一项使用乘积法则求导，$\\frac{d}{dx}(uv) = u\\frac{dv}{dx} + v\\frac{du}{dx}$。\n$$ \\frac{d}{d\\theta} \\big[ -(1-\\theta)\\ln(1-\\theta) \\big] = - \\big[ (-1)\\ln(1-\\theta) + (1-\\theta)\\frac{-1}{1-\\theta} \\big] = \\ln(1-\\theta) + 1 $$\n$$ \\frac{d}{d\\theta} \\big[ -\\theta\\ln(\\theta) \\big] = - \\big[ (1)\\ln(\\theta) + \\theta\\frac{1}{\\theta} \\big] = -\\ln(\\theta) - 1 $$\n将这些结果相加，得到 $H(\\theta)$ 的一阶导数：\n$$ \\frac{dH}{d\\theta} = (\\ln(1-\\theta) + 1) + (-\\ln(\\theta) - 1) = \\ln(1-\\theta) - \\ln(\\theta) = \\ln\\left(\\frac{1-\\theta}{\\theta}\\right) $$\n为了找到临界点，我们将一阶导数设为零：\n$$ \\frac{dH}{d\\theta} = 0 \\implies \\ln\\left(\\frac{1-\\theta}{\\theta}\\right) = 0 $$\n$$ \\frac{1-\\theta}{\\theta} = \\exp(0) = 1 $$\n$$ 1-\\theta = \\theta \\implies 2\\theta = 1 \\implies \\theta = \\frac{1}{2} $$\n在区间 $(0,1)$ 内唯一的临界点是 $\\theta^{\\star} = \\frac{1}{2}$。\n\n现在，我们通过考察 $\\frac{dH}{d\\theta}$ 的符号来证明在指定区间上的单调性：\n- 对于 $\\theta \\in (0, \\frac{1}{2})$：我们有 $1-\\theta > \\theta$，所以比值 $\\frac{1-\\theta}{\\theta} > 1$。因为自然对数是增函数，所以 $\\ln\\left(\\frac{1-\\theta}{\\theta}\\right) > \\ln(1) = 0$。因此，$\\frac{dH}{d\\theta} > 0$，并且 $H(\\theta)$ 在 $(0, \\frac{1}{2}]$ 上是严格递增的。\n- 对于 $\\theta \\in (\\frac{1}{2}, 1)$：我们有 $1-\\theta  \\theta$，所以比值 $\\frac{1-\\theta}{\\theta}  1$。这意味着 $\\ln\\left(\\frac{1-\\theta}{\\theta}\\right)  \\ln(1) = 0$。因此，$\\frac{dH}{d\\theta}  0$，并且 $H(\\theta)$ 在 $[\\frac{1}{2}, 1)$ 上是严格递减的。\n\n此分析证实 $H(\\theta)$ 在 $\\theta^{\\star} = \\frac{1}{2}$ 处达到局部最大值。\n\n**二阶导数与凹性：**\n为了分析凹性，我们计算 $H(\\theta)$ 的二阶导数：\n$$ \\frac{d^2H}{d\\theta^2} = \\frac{d}{d\\theta}\\left[\\ln(1-\\theta) - \\ln(\\theta)\\right] = \\frac{-1}{1-\\theta} - \\frac{1}{\\theta} $$\n$$ \\frac{d^2H}{d\\theta^2} = -\\left(\\frac{1}{1-\\theta} + \\frac{1}{\\theta}\\right) = -\\left(\\frac{\\theta + (1-\\theta)}{\\theta(1-\\theta)}\\right) = -\\frac{1}{\\theta(1-\\theta)} $$\n对于任何 $\\theta \\in (0,1)$，$\\theta$ 和 $1-\\theta$ 都是正数。因此，它们的乘积 $\\theta(1-\\theta)$ 也是正数。所以，二阶导数 $\\frac{d^2H}{d\\theta^2} = -\\frac{1}{\\theta(1-\\theta)}$ 对于所有 $\\theta \\in (0,1)$ 都是严格为负的。这证明了函数 $H(\\theta)$ 在整个区间 $(0,1)$ 上是严格凹的。一个严格凹函数至多有一个最大值，因此在 $\\theta^{\\star} = \\frac{1}{2}$ 处的局部最大值是唯一的全局最大值。\n\n**熵的最大值：**\n通过计算 $H(\\theta)$ 在 $\\theta^{\\star} = \\frac{1}{2}$ 处的值来获得熵的最大值：\n$$ H\\left(\\frac{1}{2}\\right) = -\\left(1-\\frac{1}{2}\\right)\\ln\\left(1-\\frac{1}{2}\\right) - \\frac{1}{2}\\ln\\left(\\frac{1}{2}\\right) $$\n$$ H\\left(\\frac{1}{2}\\right) = -\\frac{1}{2}\\ln\\left(\\frac{1}{2}\\right) - \\frac{1}{2}\\ln\\left(\\frac{1}{2}\\right) = -\\ln\\left(\\frac{1}{2}\\right) = \\ln(2) $$\n最大熵为 $\\ln(2)$ 奈特。\n\n### 3. 在脉冲序列分析中的解释\n\n在脉冲序列分析的背景下，熵 $H(X)$ 量化了在一个随机选择的时间窗中观察到脉冲 ($X=1$) 与没有脉冲 ($X=0$) 相关的平均不确定性。分析表明，当参数 $\\theta = \\theta^{\\star} = \\frac{1}{2}$ 时，这种不确定性达到最大。\n$\\theta = \\frac{1}{2}$ 的值意味着在一个时间窗中出现脉冲的概率是 $\\frac{1}{2}$，而没有脉冲的概率也是 $\\frac{1}{2}$。两种可能的结果是等可能的。这代表了最大不可预测性的状态。如果一个观察者要预测下一个时间窗是否包含脉冲，这是他们的预测最不准确的条件，不比抛一枚均匀的硬币更好。\n\n相反，当 $\\theta$ 趋近于 $0$ 或 $1$ 的极端值时，熵 $H(\\theta)$ 趋近于 $0$。\n- 如果 $\\theta \\to 0$，脉冲极其罕见。观察者可以高确定性地预测任何给定的时间窗都将是空的 ($X=0$)。不确定性非常低。\n- 如果 $\\theta \\to 1$，脉冲几乎总是存在。观察者可以高确定性地预测任何给定的时间窗都将包含一个脉冲 ($X=1$)。同样，不确定性非常低。\n\n因此，最大化值 $\\theta^{\\star} = \\frac{1}{2}$ 与“所有可能结果上的均匀概率分布产生最高不确定性”这一原则是一致的。对于一个二元系统，这对应于两种结果等概率发生的情况。",
            "answer": "$$\n\\boxed{-(1-\\theta)\\ln(1-\\theta) - \\theta\\ln(\\theta)}\n$$"
        },
        {
            "introduction": "在熵概念的基础上，本练习将介绍库尔贝克-莱布勒（Kullback-Leibler, KL）散度，这是一种衡量两个概率分布之间差异的强大工具。您将把它应用于泊松分布（Poisson distribution）——这是神经脉冲计数建模的基石——以理解如何量化模型的“误差”。通过分析其曲率，您将揭示出它与费雪信息（Fisher information）的深刻联系，从而将信息论与统计估计的效率联系起来。",
            "id": "4170231",
            "problem": "一位计算神经科学家将固定持续时间记录窗口内的尖峰计数建模为由泊松分布生成的离散时间计数。假设真实尖峰计数分布的速率参数为 $\\lambda0$，而一个不匹配的模型使用速率参数 $\\mu0$。仅使用核心定义，推导从真实分布 $\\mathrm{Pois}(\\lambda)$ 到不匹配模型 $\\mathrm{Pois}(\\mu)$ 的 Kullback-Leibler 散度（KL；Kullback–Leibler）的解析表达式。然后，将该散度视为 $\\lambda$ 的函数（固定 $\\mu$），通过计算其关于 $\\lambda$ 的二阶导数并在 $\\lambda=\\mu$ 处求值，来分析其在等值点 $\\lambda=\\mu$ 处的局部曲率。\n\n请基于以下基本要素进行推导：\n- 离散分布 $P$ 和 $Q$ 在共同支撑集上的 Kullback-Leibler 散度定义：$D_{\\mathrm{KL}}(P\\Vert Q)=\\sum_{k}P(k)\\,\\ln\\!\\big(P(k)/Q(k)\\big)$。\n- 速率为 $\\theta0$ 的泊松分布的概率质量函数：$p(k\\mid \\theta)=\\exp(-\\theta)\\,\\theta^{k}/k!$ 对于 $k\\in\\{0,1,2,\\dots\\}$。\n\n请以散度及等值点曲率的闭式解析表达式形式提供您的最终结果。无需进行数值舍入。答案应不含单位，并使用自然对数表示。",
            "solution": "该问题陈述清晰、科学基础扎实且内容自洽。所有必要的定义均已提供，任务是信息论在统计建模中应用的一个标准推导，这是计算神经科学中的常见实践。因此，该问题被认为是有效的，并将提供解答。\n\n任务有两部分：首先，推导从速率参数为 $\\lambda  0$ 的真实泊松分布 $P$ 到速率参数为 $\\mu  0$ 的模型泊松分布 $Q$ 的 Kullback-Leibler (KL) 散度；其次，计算该散度关于 $\\lambda$ 在点 $\\lambda = \\mu$ 处的局部曲率。\n\n提供的基本定义是：\n1. 离散分布 $P$ 和 $Q$ 的 KL 散度：$D_{\\mathrm{KL}}(P\\Vert Q)=\\sum_{k}P(k)\\,\\ln\\left(\\frac{P(k)}{Q(k)}\\right)$。\n2. 速率为 $\\theta  0$ 的泊松分布的概率质量函数 (PMF)：$p(k\\mid \\theta)=\\frac{\\exp(-\\theta)\\,\\theta^{k}}{k!}$ 对于 $k\\in\\{0,1,2,\\dots\\}$。\n\n令 $P(k)$ 为真实分布 $\\mathrm{Pois}(\\lambda)$ 的概率质量函数，令 $Q(k)$ 为模型分布 $\\mathrm{Pois}(\\mu)$ 的概率质量函数。\n$$ P(k) = \\frac{\\exp(-\\lambda)\\lambda^k}{k!} $$\n$$ Q(k) = \\frac{\\exp(-\\mu)\\mu^k}{k!} $$\n两个分布共享相同的支撑集，即非负整数集 $k \\in \\{0, 1, 2, \\ldots\\}$。\n\n首先，我们计算比率 $\\frac{P(k)}{Q(k)}$：\n$$ \\frac{P(k)}{Q(k)} = \\frac{\\frac{\\exp(-\\lambda)\\lambda^k}{k!}}{\\frac{\\exp(-\\mu)\\mu^k}{k!}} = \\frac{\\exp(-\\lambda)\\lambda^k}{\\exp(-\\mu)\\mu^k} = \\exp(\\mu - \\lambda) \\left(\\frac{\\lambda}{\\mu}\\right)^k $$\n接下来，我们取该比率的自然对数：\n$$ \\ln\\left(\\frac{P(k)}{Q(k)}\\right) = \\ln\\left(\\exp(\\mu - \\lambda) \\left(\\frac{\\lambda}{\\mu}\\right)^k\\right) = \\ln(\\exp(\\mu - \\lambda)) + \\ln\\left(\\left(\\frac{\\lambda}{\\mu}\\right)^k\\right) = (\\mu - \\lambda) + k \\ln\\left(\\frac{\\lambda}{\\mu}\\right) $$\n现在，我们将此表达式代入 KL 散度的定义中：\n$$ D_{\\mathrm{KL}}(P\\Vert Q) = \\sum_{k=0}^{\\infty} P(k) \\left[ (\\mu - \\lambda) + k \\ln\\left(\\frac{\\lambda}{\\mu}\\right) \\right] $$\n根据求和的线性性质，我们可以将其分成两部分：\n$$ D_{\\mathrm{KL}}(P\\Vert Q) = \\sum_{k=0}^{\\infty} P(k)(\\mu - \\lambda) + \\sum_{k=0}^{\\infty} P(k) k \\ln\\left(\\frac{\\lambda}{\\mu}\\right) $$\n我们可以将不依赖于求和索引 $k$ 的项提取出来：\n$$ D_{\\mathrm{KL}}(P\\Vert Q) = (\\mu - \\lambda) \\sum_{k=0}^{\\infty} P(k) + \\ln\\left(\\frac{\\lambda}{\\mu}\\right) \\sum_{k=0}^{\\infty} k P(k) $$\n我们现在计算这两个和。第一个和 $\\sum_{k=0}^{\\infty} P(k)$ 是分布 $P$ 在其整个支撑集上的概率之和，必须等于 $1$。\n$$ \\sum_{k=0}^{\\infty} P(k) = 1 $$\n第二个和 $\\sum_{k=0}^{\\infty} k P(k)$ 是随机变量 $k$ 在分布 $P$ 下的期望值的定义。对于泊松分布 $\\mathrm{Pois}(\\lambda)$，其期望值等于其速率参数 $\\lambda$。\n$$ \\mathbb{E}_P[k] = \\sum_{k=0}^{\\infty} k P(k) = \\lambda $$\n将这些结果代回 KL 散度的表达式中：\n$$ D_{\\mathrm{KL}}(P\\Vert Q) = (\\mu - \\lambda)(1) + \\ln\\left(\\frac{\\lambda}{\\mu}\\right)(\\lambda) $$\n重新排列各项，得到 KL 散度的最终解析表达式：\n$$ D_{\\mathrm{KL}}(\\mathrm{Pois}(\\lambda) \\Vert \\mathrm{Pois}(\\mu)) = \\lambda \\ln\\left(\\frac{\\lambda}{\\mu}\\right) - \\lambda + \\mu $$\n这完成了问题的第一部分。\n\n对于第二部分，我们分析该散度的局部曲率。我们将 $D_{\\mathrm{KL}}$ 视为真实速率 $\\lambda$ 的函数，记为 $D(\\lambda)$，同时保持模型速率 $\\mu$ 固定。\n$$ D(\\lambda) = \\lambda \\ln\\left(\\frac{\\lambda}{\\mu}\\right) - \\lambda + \\mu $$\n为便于求导，我们可以重写对数项：\n$$ D(\\lambda) = \\lambda (\\ln\\lambda - \\ln\\mu) - \\lambda + \\mu = \\lambda\\ln\\lambda - \\lambda\\ln\\mu - \\lambda + \\mu $$\n我们计算 $D(\\lambda)$ 关于 $\\lambda$ 的一阶导数：\n$$ \\frac{dD}{d\\lambda} = \\frac{d}{d\\lambda} (\\lambda\\ln\\lambda - \\lambda\\ln\\mu - \\lambda + \\mu) $$\n对 $\\lambda\\ln\\lambda$ 项使用乘法法则，我们得到 $\\frac{d}{d\\lambda}(\\lambda\\ln\\lambda) = 1\\cdot\\ln\\lambda + \\lambda\\cdot\\frac{1}{\\lambda} = \\ln\\lambda + 1$。其他项的导数很简单：\n$$ \\frac{dD}{d\\lambda} = (\\ln\\lambda + 1) - \\ln\\mu - 1 + 0 = \\ln\\lambda - \\ln\\mu = \\ln\\left(\\frac{\\lambda}{\\mu}\\right) $$\n接下来，我们计算 $D(\\lambda)$ 关于 $\\lambda$ 的二阶导数，它代表曲率：\n$$ \\frac{d^2D}{d\\lambda^2} = \\frac{d}{d\\lambda} \\left(\\ln\\left(\\frac{\\lambda}{\\mu}\\right)\\right) = \\frac{d}{d\\lambda}(\\ln\\lambda - \\ln\\mu) $$\n因为 $\\mu$ 是关于 $\\lambda$ 的常数，所以它的对数也是常数。\n$$ \\frac{d^2D}{d\\lambda^2} = \\frac{1}{\\lambda} - 0 = \\frac{1}{\\lambda} $$\n问题要求在等值点 $\\lambda=\\mu$ 处计算该曲率。\n$$ \\left. \\frac{d^2D}{d\\lambda^2} \\right|_{\\lambda=\\mu} = \\frac{1}{\\mu} $$\n该值，即 KL 散度在模型与真实分布匹配点处的曲率，等于泊松参数的费雪信息（Fisher information）。\n\n所要求的两个结果是散度的解析表达式 $D_{\\mathrm{KL}}(\\mathrm{Pois}(\\lambda) \\Vert \\mathrm{Pois}(\\mu)) = \\lambda \\ln\\left(\\frac{\\lambda}{\\mu}\\right) - \\lambda + \\mu$，以及等值点曲率 $\\left. \\frac{d^2D}{d\\lambda^2} \\right|_{\\lambda=\\mu} = \\frac{1}{\\mu}$。",
            "answer": "$$\n\\boxed{ \\begin{pmatrix} \\lambda \\ln\\left(\\frac{\\lambda}{\\mu}\\right) - \\lambda + \\mu \\\\ \\frac{1}{\\mu} \\end{pmatrix} }\n$$"
        },
        {
            "introduction": "最后的这项实践练习将探讨在真实数据上应用信息论时的一个关键挑战：估计偏差。您将推导“代入式”互信息估计量的系统性正偏差，而这种估计方法常被人们不加鉴别地使用。通过将互信息与统计假设检验联系起来，这个问题阐释了为何即便变量间不存在真实关联，有限的数据也可能表现出信息量。对于神经科学领域的数据科学家而言，这是一个至关重要的教训。",
            "id": "4170211",
            "problem": "一位系统神经科学家正在分析离散感觉刺激 $S$ 与离散化的单神经元响应 $R$ 之间的统计依赖性。刺激 $S$ 有 $K$ 个类别 $\\{s_{1},\\ldots,s_{K}\\}$，响应 $R$ 有 $M$ 个类别 $\\{r_{1},\\ldots,r_{M}\\}$，这是通过对发放计数进行分箱得到的。该实验由 $N$ 次独立同分布 (i.i.d.) 试验组成，产生计数 $\\{n_{ij}\\}_{i=1,\\ldots,K;\\,j=1,\\ldots,M}$，且满足 $\\sum_{i=1}^{K}\\sum_{j=1}^{M} n_{ij}=N$。经验联合分布为 $\\hat{p}_{SR}(s_{i},r_{j})=n_{ij}/N$，经验边缘分布为 $\\hat{p}_{S}(s_{i})=\\sum_{j=1}^{M} n_{ij}/N$ 和 $\\hat{p}_{R}(r_{j})=\\sum_{i=1}^{K} n_{ij}/N$。这位神经科学家使用互信息 (MI) 的朴素“即插即用”估计量，\n$$\n\\hat{I}(S;R)=\\sum_{i=1}^{K}\\sum_{j=1}^{M} \\hat{p}_{SR}(s_{i},r_{j}) \\,\\ln\\!\\left(\\frac{\\hat{p}_{SR}(s_{i},r_{j})}{\\hat{p}_{S}(s_{i})\\,\\hat{p}_{R}(r_{j})}\\right),\n$$\n以自然单位 (nats) 计量。假设真实的数据生成分布满足独立性零假设，即对所有 $i$ 和 $j$，都有 $p_{SR}(s_{i},r_{j})=p_{S}(s_{i})\\,p_{R}(r_{j})$，并且所有单元格概率都严格为正，以确保渐近性的有效性。\n\n仅从离散互信息的核心定义和多项式模型的似然出发，并利用一个经过充分检验的大样本结论——即在零假设下，用于检验 $K\\times M$ 列联表中独立性的对数似然比统计量依分布收敛于一个自由度为 $(K-1)(M-1)$ 的卡方分布——推导出在独立性零假设下，朴素“即插即用”互信息期望值的大 $N$ 主阶表达式。然后利用这个结果解释为什么即使在总体中 $S$ 和 $R$ 是统计独立的，对于有限的 $N$，朴素“即插即用”估计量也是正偏的。\n\n将主阶期望的最终结果以 $K$、$M$ 和 $N$ 的闭式解析表达式表示，单位为 nats。最终答案必须是单个表达式。不要在最终答案框中包含单位。不需要四舍五入。",
            "solution": "该问题陈述经评估有效。它在科学上基于信息论和统计学的标准原理，问题阐述清晰，为得出唯一解提供了所有必要信息，并且语言客观。其任务是推导一个已知的统计结果，这是一项合理的科学实践。\n\n该问题要求在刺激 $S$ 和响应 $R$ 之间统计独立的零假设下，朴素“即插即用”互信息估计量 $\\hat{I}(S;R)$ 的期望值的大 $N$ 主阶表达式。\n\n互信息 (MI) 的朴素“即插即用”估计量由下式给出：\n$$\n\\hat{I}(S;R) = \\sum_{i=1}^{K}\\sum_{j=1}^{M} \\hat{p}_{SR}(s_{i},r_{j}) \\,\\ln\\!\\left(\\frac{\\hat{p}_{SR}(s_{i},r_{j})}{\\hat{p}_{S}(s_{i})\\,\\hat{p}_{R}(r_{j})}\\right)\n$$\n其中 $\\hat{p}_{SR}(s_{i},r_{j}) = n_{ij}/N$ 是经验联合概率，$\\hat{p}_{S}(s_{i}) = (\\sum_{j} n_{ij})/N$ 和 $\\hat{p}_{R}(r_{j}) = (\\sum_{i} n_{ij})/N$ 是经验边缘概率。为简化符号，我们分别用 $\\hat{p}_{i\\cdot}$ 和 $\\hat{p}_{\\cdot j}$ 表示边缘概率，用 $\\hat{p}_{ij}$ 表示联合概率。该表达式变为：\n$$\n\\hat{I}(S;R) = \\sum_{i=1}^{K}\\sum_{j=1}^{M} \\hat{p}_{ij} \\,\\ln\\!\\left(\\frac{\\hat{p}_{ij}}{\\hat{p}_{i\\cdot}\\,\\hat{p}_{\\cdot j}}\\right)\n$$\n该表达式在形式上等价于经验联合分布 $\\hat{p}_{SR}$ 与其边缘分布乘积 $\\hat{p}_{S}\\hat{p}_{R}$ 之间的 Kullback-Leibler 散度，即 $\\hat{I}(S;R) = D_{KL}(\\hat{p}_{SR} || \\hat{p}_{S}\\hat{p}_{R})$。\n\n问题引导我们使用关于列联表中独立性检验的对数似然比统计量的结论。让我们来构建这个统计量。我们正在检验独立性的零假设 $H_0$（其中真实单元格概率为 $p_{ij} = p_{i\\cdot} p_{\\cdot j}$）与饱和备择假设 $H_1$（其中 $p_{ij}$ 不受约束，除了总和为 $1$）相对立。\n\n在多项式模型下，观测到计数 $\\{n_{ij}\\}$ 的似然为 $L(\\{p_{ij}\\}) \\propto \\prod_{i,j} (p_{ij})^{n_{ij}}$。\n\n在备择假设 $H_1$ 下，$p_{ij}$ 的最大似然估计 (MLE) 是 $\\hat{p}_{ij} = n_{ij}/N$。最大化后的对数似然为：\n$$\n\\ln L_1^* = \\sum_{i,j} n_{ij} \\ln(\\hat{p}_{ij}) = \\sum_{i,j} n_{ij} \\ln(n_{ij}/N)\n$$\n\n在零假设 $H_0$ 下，边缘概率的 MLE 是 $\\hat{p}_{i\\cdot} = (\\sum_j n_{ij})/N$ 和 $\\hat{p}_{\\cdot j} = (\\sum_i n_{ij})/N$。在 $H_0$ 下最大化后的对数似然为：\n$$\n\\ln L_0^* = \\sum_{i,j} n_{ij} \\ln(\\hat{p}_{i\\cdot} \\hat{p}_{\\cdot j})\n$$\n\n对数似然比统计量，通常表示为 $\\Lambda$ 或 $G^2$，定义为最大化对数似然之间差值的两倍：\n$$\n\\Lambda = 2(\\ln L_1^* - \\ln L_0^*) = 2 \\left( \\sum_{i,j} n_{ij} \\ln(n_{ij}/N) - \\sum_{i,j} n_{ij} \\ln(\\hat{p}_{i\\cdot} \\hat{p}_{\\cdot j}) \\right)\n$$\n合并求和号内的项：\n$$\n\\Lambda = 2 \\sum_{i,j} n_{ij} \\ln\\left(\\frac{n_{ij}/N}{\\hat{p}_{i\\cdot} \\hat{p}_{\\cdot j}}\\right)\n$$\n我们可以提出总样本数 $N$：\n$$\n\\Lambda = 2N \\sum_{i,j} \\frac{n_{ij}}{N} \\ln\\left(\\frac{n_{ij}/N}{\\hat{p}_{i\\cdot} \\hat{p}_{\\cdot j}}\\right)\n$$\n注意到 $\\hat{p}_{ij} = n_{ij}/N$，我们可以将其重写为：\n$$\n\\Lambda = 2N \\sum_{i,j} \\hat{p}_{ij} \\ln\\left(\\frac{\\hat{p}_{ij}}{\\hat{p}_{i\\cdot} \\hat{p}_{\\cdot j}}\\right)\n$$\n求和项正是朴素 MI 估计量 $\\hat{I}(S;R)$ 的定义。因此，我们建立了一个关键的恒等式：\n$$\n\\Lambda = 2N \\hat{I}(S;R)\n$$\n这表明，朴素 MI 估计量乘以 $2N$ 后，与用于独立性检验的 G 检验统计量是相同的。\n\n问题指出，对于大 $N$，在独立性零假设下，该对数似然比统计量依分布收敛于卡方 ($\\chi^2$) 分布。在 $K \\times M$ 列联表中进行独立性检验的自由度 $\\nu$ 由备择模型中的自由参数数量减去零假设模型中的自由参数数量给出。\n饱和模型有 $KM-1$ 个自由参数。独立性模型有 $(K-1) + (M-1)$ 个自由参数。\n因此，$\\nu = (KM-1) - ((K-1) + (M-1)) = KM - 1 - K + 1 - M + 1 = KM - K - M + 1 = (K-1)(M-1)$。\n因此，对于大 $N$，我们有如下渐近分布结果：\n$$\n2N \\hat{I}(S;R) \\quad \\xrightarrow{d} \\quad \\chi^2_{(K-1)(M-1)}\n$$\n为了找到 $\\hat{I}(S;R)$ 期望值的主阶表达式，我们对这个关系式的两边取期望。对于大 $N$，该统计量的期望将接近其极限分布的期望。\n$$\nE[2N \\hat{I}(S;R)] \\approx E[\\chi^2_{(K-1)(M-1)}]\n$$\n由于对于给定的样本量，$N$ 是一个常数，我们可以写出：\n$$\n2N E[\\hat{I}(S;R)] \\approx E[\\chi^2_{(K-1)(M-1)}]\n$$\n自由度为 $\\nu$ 的卡方随机变量的期望值就是 $\\nu$。在我们的例子中，$\\nu = (K-1)(M-1)$。\n$$\n2N E[\\hat{I}(S;R)] \\approx (K-1)(M-1)\n$$\n求解 $E[\\hat{I}(S;R)]$，我们得到在独立性零假设下，朴素 MI 估计量期望值的大 $N$ 主阶表达式：\n$$\nE[\\hat{I}(S;R)] \\approx \\frac{(K-1)(M-1)}{2N}\n$$\n这就是所求的结果。\n\n为了解释为什么该估计量是正偏的，我们将其期望值与它所估计的参数的真实值进行比较。参数 $\\theta$ 的估计量 $\\hat{\\theta}$ 的偏差为 $B(\\hat{\\theta}) = E[\\hat{\\theta}] - \\theta$。\n在我们的例子中，估计量是 $\\hat{I}(S;R)$。真实的参数值是真实的互信息 $I(S;R)$。在独立性零假设下，真实的互信息为零，即 $I(S;R) = 0$。\n因此，零假设下的偏差为：\n$$\nB(\\hat{I}) = E[\\hat{I}(S;R)] - 0 = E[\\hat{I}(S;R)]\n$$\n根据我们推导出的结果，我们有：\n$$\nB(\\hat{I}) \\approx \\frac{(K-1)(M-1)}{2N}\n$$\n对于任何具有多于一个刺激类别 ($K > 1$) 和多于一个响应类别 ($M > 1$) 的非平凡实验设计，以及对于任何有限次数的试验 $N$，表达式 $(K-1)(M-1)/(2N)$ 都严格为正。\n这意味着 $E[\\hat{I}(S;R)] > I(S;R) = 0$。一个期望值大于参数真实值的估计量被称为正偏的。产生正偏差的原因是抽样噪声；即使当潜在分布是独立的，有限的抽样也会导致经验计数 $\\{n_{ij}\\}$ 产生波动，从而使得经验联合分布 $\\hat{p}_{SR}$ 偏离经验边缘分布的乘积 $\\hat{p}_{S}\\hat{p}_{R}$。由于互信息（作为一种 Kullback-Leibler 散度）是一个非负量，并且只有当这些分布完全相同时才为零，因此任何随机偏差都会产生一个正的 $\\hat{I}(S;R)$ 值。我们的计算量化了对于大样本量，这个伪正值的平均大小。",
            "answer": "$$\n\\boxed{\\frac{(K-1)(M-1)}{2N}}\n$$"
        }
    ]
}