{
    "hands_on_practices": [
        {
            "introduction": "神经科学建模常常需要在不同复杂度的模型之间做出选择。一个更复杂的模型几乎总能更好地拟合训练数据，但这是否意味着它是一个更好的模型？本练习介绍两种关键的模型选择工具——赤池信息准则（AIC）和贝叶斯信息准则（BIC）。通过应用这些准则，您将学习如何量化模型拟合优度与模型复杂度之间的权衡，这对于避免过拟合并选择最能推广到新数据的模型至关重要。",
            "id": "4159626",
            "problem": "记录了单个皮层神经元在 $n=1200$ 次独立刺激呈现中的脉冲计数。两个竞争的编码模型，均为泊松广义线性模型 (GLM)，通过最大似然法对同一数据集进行了拟合。模型 $1$ 使用了 $k_1=10$ 个参数（包括一个截距和 $9$ 个刺激特征权重）。模型 $2$ 使用了 $k_2=30$ 个参数（包括一个截距、$9$ 个特征权重和 $20$ 个时间基权重）。拟合软件报告了在训练数据上的最大化对数似然（自然对数）值，模型 $1$ 为 $\\ln L_1=-5600.5$，模型 $2$ 为 $\\ln L_2=-5550.5$。\n\n根据最大似然理论中赤池信息准则 (AIC) 和贝叶斯信息准则 (BIC) 的定义，计算每个模型的 AIC 和 BIC，并确定每个准则偏好哪个模型。明确说明这两个准则在偏好模型上是否一致。将所有信息准则值四舍五入到四位有效数字。将您的最终答案表示为一个行向量 $\\left(\\text{AIC}_{1},\\ \\text{AIC}_{2},\\ \\text{BIC}_{1},\\ \\text{BIC}_{2},\\ m_{\\text{AIC}},\\ m_{\\text{BIC}}\\right)$，其中 $m_{\\text{AIC}}$ 和 $m_{\\text{BIC}}$ 分别是在 AIC 和 BIC 准则下偏好模型的索引（$1$ 或 $2$）。无需单位。",
            "solution": "该问题要求计算两个竞争统计模型的赤池信息准则 (AIC) 和贝叶斯信息准则 (BIC)。对于给定的准则，值较低的模型被认为更优。\n\nAIC的定义是：\n$$\n\\text{AIC} = 2k - 2\\ln L\n$$\n其中 $k$ 是模型中估计参数的数量，$\\ln L$ 是对数似然函数的最大化值。\n\n对于模型 $1$，我们有 $k_1=10$ 和 $\\ln L_1 = -5600.5$。其 AIC 为：\n$$\n\\text{AIC}_1 = 2k_1 - 2\\ln L_1 = 2(10) - 2(-5600.5) = 20 + 11201 = 11221\n$$\n四舍五入到四位有效数字得到 $\\text{AIC}_1 \\approx 11220$。\n\n对于模型 $2$，我们有 $k_2=30$ 和 $\\ln L_2 = -5550.5$。其 AIC 为：\n$$\n\\text{AIC}_2 = 2k_2 - 2\\ln L_2 = 2(30) - 2(-5550.5) = 60 + 11101 = 11161\n$$\n四舍五入到四位有效数字得到 $\\text{AIC}_2 \\approx 11160$。\n\n为了确定 AIC 偏好的模型，我们比较它们各自的 AIC 值。AIC 值较低的模型更受偏好。\n$$\n\\text{AIC}_2 = 11161  11221 = \\text{AIC}_1\n$$\n因此，赤池信息准则偏好模型 $2$。偏好模型的索引为 $m_{\\text{AIC}} = 2$。\n\nBIC 的定义是：\n$$\n\\text{BIC} = k\\ln(n) - 2\\ln L\n$$\n其中 $k$ 是参数数量，$n$ 是数据点数量，$\\ln L$ 是最大化对数似然。\n\n对于这个问题，$n=1200$。我们首先计算 $\\ln(n)$：\n$$\n\\ln(1200) \\approx 7.0900755\n$$\n\n对于模型 $1$，有 $k_1=10$ 和 $\\ln L_1 = -5600.5$，其 BIC 为：\n$$\n\\text{BIC}_1 = k_1\\ln(n) - 2\\ln L_1 = 10 \\times \\ln(1200) - 2(-5600.5) \\approx 10(7.0900755) + 11201 \\approx 70.900755 + 11201 = 11271.900755\n$$\n四舍五入到四位有效数字得到 $\\text{BIC}_1 \\approx 11270$。\n\n对于模型 $2$，有 $k_2=30$ 和 $\\ln L_2 = -5550.5$，其 BIC 为：\n$$\n\\text{BIC}_2 = k_2\\ln(n) - 2\\ln L_2 = 30 \\times \\ln(1200) - 2(-5550.5) \\approx 30(7.0900755) + 11101 \\approx 212.702265 + 11101 = 11313.702265\n$$\n四舍五入到四位有效数字得到 $\\text{BIC}_2 \\approx 11310$。\n\n为了确定 BIC 偏好的模型，我们比较它们各自的 BIC 值。\n$$\n\\text{BIC}_1 \\approx 11271.9  11313.7 \\approx \\text{BIC}_2\n$$\n因此，贝叶斯信息准则偏好模型 $1$。偏好模型的索引为 $m_{\\text{BIC}} = 1$。\n\n总结如下：\n- AIC 偏好模型 $2$（索引 $2$）。\n- BIC 偏好模型 $1$（索引 $1$）。\n\n这两个准则在偏好模型上不一致。这种不一致的产生是因为 AIC ($2k$) 和 BIC ($k\\ln(n)$) 对模型复杂度的惩罚项不同。对于 $n=1200$，$\\ln(n) \\approx 7.09$，大于 $2$。因此，BIC 对额外参数施加的惩罚比 AIC 强得多。虽然模型 $2$ 对数据的拟合更好（对数似然更高），但其增加的复杂度（$k_2=30$ 对比 $k_1=10$）被 BIC 严重惩罚，以至于更简单的模型 $1$ 最终受到青睐。AIC 较弱的惩罚使得模型 $2$ 更好的拟合度超过了其复杂度的影响。\n\n最终答案是一个由计算值组成的行向量：$(\\text{AIC}_{1}, \\text{AIC}_{2}, \\text{BIC}_{1}, \\text{BIC}_{2}, m_{\\text{AIC}}, m_{\\text{BIC}})$。\n$\\text{AIC}_1 \\approx 11220$\n$\\text{AIC}_2 \\approx 11160$\n$\\text{BIC}_1 \\approx 11270$\n$\\text{BIC}_2 \\approx 11310$\n$m_{\\text{AIC}} = 2$\n$m_{\\text{BIC}} = 1$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n11220  11160  11270  11310  2  1\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "时间重整定理是评估点过程编码模型（如用于神经元尖峰放电的模型）拟合优度的基石。这项编程实践不仅仅是调用一个现有函数，它要求您从第一性原理出发，亲手实现整个检验流程 。您将首先根据一个已知的“真实”模型生成尖峰序列，然后用一个“待拟合”模型对这些尖峰进行分析，从而构建并实施一个完整的拟合优度检验，深入理解这一核心诊断工具的理论基础和实际应用。",
            "id": "4159588",
            "problem": "给定一个由条件强度函数 $\\hat{\\lambda}(t)$ 指定的点过程编码模型，旨在描述神经元在给定其过去历史的情况下的瞬时发放倾向。其基础是具有条件强度 $\\lambda(t \\mid \\mathcal{H}_t)$ 的点过程的生存特性：在区间 $\\left[t, t+\\Delta\\right]$ 内没有事件发生的概率为\n$$\nS(\\Delta \\mid \\mathcal{H}_t) = \\exp\\left(-\\int_{t}^{t+\\Delta} \\lambda(s \\mid \\mathcal{H}_s)\\, ds\\right).\n$$\n基于此，可以推导出一个时间重标度变换，它将事件间间隔映射到单位区间上的随机变量。为了进行模型检验，在一个正确指定的模型下，使用拟合的条件强度 $\\hat{\\lambda}(t)$ 计算出的重标度变量应在 $\\left[0,1\\right]$ 上均匀分布。均匀性可以通过针对 $\\left[0,1\\right]$ 上均匀分布的柯尔莫哥洛夫-斯米尔诺夫（KS）检验进行评估。\n\n您的任务是实现一个程序，该程序能够：\n- 通过求解区间 $\\Delta_i$ 使得积分后的真实强度 $\\lambda_{\\text{true}}(t)$ 满足 $\\int_{t_{i-1}}^{t_i} \\lambda_{\\text{true}}(s)\\, ds = m_i$（其中 $m_i = -\\log\\left(1 - u_i^{\\star}\\right)$ 且 $t_0 = 0$ 秒），从一个指定的目标重标度变量序列 $\\{u_i^{\\star}\\}_{i=1}^n$ 构建确定性的事件时间 $\\{t_i\\}_{i=1}^n$（单位为秒）。这确保了当使用 $\\lambda_{\\text{true}}(t)$ 进行评估时，推导出的重标度变量与给定的目标 $\\{u_i^{\\star}\\}$ 相匹配。\n- 使用拟合强度 $\\hat{\\lambda}(t)$，通过对 $\\int_{t_{i-1}}^{t_i} \\hat{\\lambda}(s)\\, ds$ 进行数值积分以及上述生存特性所隐含的变换，为相同的事件时间计算重标度变量 $\\{u_i\\}_{i=1}^n$。\n- 对每个测试用例的 $\\{u_i\\}_{i=1}^n$ 执行均匀性的柯尔莫哥洛夫-斯米尔诺夫检验。\n\n所有时间都必须以秒为单位处理。不涉及角度。最终输出必须是实值小数。\n\n实现数值积分来评估任意给定的强度函数 $\\int_{a}^{b} \\lambda(s)\\, ds$。实现一个稳健的一维求根程序，以针对每个区间求解 $\\int_{t_{i-1}}^{t_{i-1}+\\Delta_i} \\lambda_{\\text{true}}(s)\\, ds = m_i$ 中的 $\\Delta_i  0$。\n\n使用以下参数值测试套件，其设计旨在探究正确性（模型匹配）、误设（不匹配）、分段强度中的跨界问题以及小样本边缘行为。在每种情况下，设置 $t_0 = 0$ 秒，并定义 $u_i^{\\star} = i/(n+1)$，其中 $i \\in \\{1,\\dots,n\\}$。\n\n- 测试用例 1（匹配的恒定强度，理想路径）：\n  - $n = 20$，$\\lambda_{\\text{true}}(t) = 12$ (事件/秒)，$\\hat{\\lambda}(t) = 12$ (事件/秒)。\n- 测试用例 2（不匹配的恒定强度，模型误设）：\n  - $n = 20$，$\\lambda_{\\text{true}}(t) = 6$ (事件/秒)，$\\hat{\\lambda}(t) = 12$ (事件/秒)。\n- 测试用例 3（匹配的跨界分段恒定强度）：\n  - $n = 25$，$\\lambda_{\\text{true}}(t) = 8$ (事件/秒) 当 $t  5$ (秒)时，$\\lambda_{\\text{true}}(t) = 16$ (事件/秒) 当 $t \\ge 5$ (秒)时；$\\hat{\\lambda}(t)$ 等于 $\\lambda_{\\text{true}}(t)$。\n- 测试用例 4（不匹配的小样本边缘情况）：\n  - $n = 5$，$\\lambda_{\\text{true}}(t) = 12$ (事件/秒)，$\\hat{\\lambda}(t) = 8$ (事件/秒) 当 $t  5$ (秒)时，$\\hat{\\lambda}(t) = 16$ (事件/秒) 当 $t \\ge 5$ (秒)时。\n\n您的程序应对每个测试用例执行针对 $\\left[0,1\\right]$ 上均匀分布的柯尔莫哥洛夫-斯米尔诺夫检验，并按顺序为每个测试用例输出两个数字：KS 检验统计量及其 p 值，两者均四舍五入到六位小数。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果，例如 $\\left[\\text{stat}_1,\\text{pval}_1,\\text{stat}_2,\\text{pval}_2,\\dots\\right]$。",
            "solution": "该问题要求实现一个基于时间重标度定理的点过程编码模型的拟合优度检验。该过程涉及两个主要阶段：首先，从一个已知的“真实”条件强度函数 $\\lambda_{\\text{true}}(t)$ 生成一个合成的事件时间序列；其次，使用一个候选或“拟合”模型 $\\hat{\\lambda}(t)$ 分析这些事件时间，以确定拟合模型是否充分描述了数据。\n\n此程序的理论基础是时间重标度定理。对于一个由条件强度函数 $\\lambda(t \\mid \\mathcal{H}_t)$ 表征的点过程（该函数给出在时间 $t$ 发生事件的瞬时概率，给定截至时间 $t$ 的事件历史 $\\mathcal{H}_t$），该定理指出，变换后的事件间间隔\n$$\n\\tau_i = \\int_{t_{i-1}}^{t_i} \\lambda(s \\mid \\mathcal{H}_s) \\, ds\n$$\n是独立同分布（i.i.d.）的指数随机变量，其速率参数为 $\\lambda=1$。在这个问题中，强度函数是时间的确定性函数，因此我们将其写作 $\\lambda(t)$。\n\n通过将标准指数分布的累积分布函数（CDF）$F(x) = 1 - e^{-x}$ 应用于这些重标度的区间，我们可以得到一组新的变量：\n$$\nu_i = F(\\tau_i) = 1 - \\exp\\left(-\\int_{t_{i-1}}^{t_i} \\lambda(s) \\, ds\\right)\n$$\n如果在积分中使用的函数 $\\lambda(t)$ 是真实的数据生成强度，那么得到的变量 $\\{u_i\\}$ 将是独立同分布的，并且在区间 $[0, 1]$ 上均匀分布。这个特性构成了我们拟合优度检验的基础。我们将使用单样本柯尔莫哥洛夫-斯米尔诺夫（KS）检验来评估计算出的 $\\{u_i\\}$ 的均匀性。\n\n总体方法实现如下：\n\n**第 1 步：生成合成事件时间**\n我们的任务是从给定的真实强度函数 $\\lambda_{\\text{true}}(t)$ 和一个指定的目标均匀变量序列 $\\{u_i^{\\star}\\}_{i=1}^n$ 构建一个确定性的事件时间序列 $\\{t_i\\}_{i=1}^n$。目标变量被设置为均匀分布的分位数，$u_i^{\\star} = i/(n+1)$，其中 $i \\in \\{1, \\dots, n\\}$。\n\n首先，我们对 CDF 变换求逆，以找到来自标准指数分布的相应值，记为 $m_i$：\n$$\nm_i = F^{-1}(u_i^{\\star}) = -\\log(1 - u_i^{\\star})\n$$\n这些 $m_i$ 值代表每个事件间间隔的目标积分强度。然后通过迭代求解积分方程来确定事件时间 $\\{t_i\\}$：\n$$\n\\int_{t_{i-1}}^{t_i} \\lambda_{\\text{true}}(s) \\, ds = m_i\n$$\n从 $t_0 = 0$ 开始。这等效于求解从原点开始的总积分强度。设 $F_{\\text{true}}(t) = \\int_0^t \\lambda_{\\text{true}}(s) \\, ds$ 为累积强度函数。第 $i$ 个事件时间 $t_i$ 必须满足：\n$$\nF_{\\text{true}}(t_i) = F_{\\text{true}}(t_{i-1}) + m_i = \\sum_{j=1}^{i} m_j\n$$\n然后通过应用累积强度函数的反函数来找到事件时间 $t_i$：\n$$\nt_i = F_{\\text{true}}^{-1}\\left(\\sum_{j=1}^{i} m_j\\right)\n$$\n对于问题中指定的恒定和分段恒定强度函数，累积强度函数 $F_{\\text{true}}(t)$ 及其反函数 $F_{\\text{true}}^{-1}(y)$ 可以解析推导。\n- 对于恒定强度 $\\lambda_{\\text{true}}(t) = c$，$F_{\\text{true}}(t) = ct$，其反函数为 $F_{\\text{true}}^{-1}(y) = y/c$。\n- 对于分段恒定强度，$F_{\\text{true}}(t)$ 是一个连续的分段线性函数，其反函数可以很容易地计算出来。这种解析方法比数值求根更精确且计算效率更高。\n\n**第 2 步：计算重标度区间和模型评估**\n使用第 1 步中生成的事件时间 $\\{t_i\\}$，我们现在评估候选模型 $\\hat{\\lambda}(t)$。对于每个事件间间隔 $[t_{i-1}, t_i]$，我们根据候选模型计算积分强度：\n$$\nM_i = \\int_{t_{i-1}}^{t_i} \\hat{\\lambda}(s) \\, ds\n$$\n这可以计算为 $M_i = \\hat{F}(t_i) - \\hat{F}(t_{i-1})$，其中 $\\hat{F}(t) = \\int_0^t \\hat{\\lambda}(s) \\, ds$ 是候选模型的累积强度函数。和之前一样，对于给定的测试用例，$\\hat{F}(t)$ 是解析确定的。\n\n然后，我们将这些积分强度转换为一个假定是均匀分布的变量样本 $\\{u_i\\}_{i=1}^n$：\n$$\nu_i = 1 - \\exp(-M_i)\n$$\n如果 $\\hat{\\lambda}(t)$ 是生成事件时间过程的正确表示（即，如果 $\\hat{\\lambda}(t) = \\lambda_{\\text{true}}(t)$），样本 $\\{u_i\\}$ 应在 $[0,1]$ 上均匀分布。\n\n**第 3 步：统计检验**\n最后一步是对计算出的样本 $\\{u_i\\}$ 与 $[0,1]$ 上的标准均匀分布进行单样本柯尔莫哥洛夫-斯米尔诺夫检验。KS 检验得出一个统计量 $D_n$（即样本的经验累积分布函数与理论均匀累积分布函数之间的最大绝对差）和一个 p 值。一个大的 p 值（例如，$p  0.05$）表明我们不能拒绝原假设，即样本来自均匀分布，这表明模型 $\\hat{\\lambda}(t)$ 是一个良好的拟合。一个小的 p 值表明模型存在误设。为每个测试用例报告 KS 统计量和 p 值。",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import kstest\n\ndef solve():\n    \"\"\"\n    Main function to execute all test cases and print the results.\n    \"\"\"\n\n    def get_cumulative_integral_func_piecewise_const(pieces_desc):\n        \"\"\"\n        Creates a function F(t) that analytically computes the integral of a \n        piecewise-constant function from 0 to t.\n\n        Args:\n            pieces_desc: A list of (value, end_time) tuples, e.g., [(8, 5.0), (16, np.inf)].\n\n        Returns:\n            A vectorized function F(t).\n        \"\"\"\n        boundaries = [0.0]\n        cum_integrals_at_boundaries = [0.0]\n        last_t = 0.0\n        \n        for val, end_t in pieces_desc:\n            if np.isinf(end_t):\n                break\n            integral_of_piece = val * (end_t - last_t)\n            boundaries.append(end_t)\n            cum_integrals_at_boundaries.append(cum_integrals_at_boundaries[-1] + integral_of_piece)\n            last_t = end_t\n        \n        boundaries = np.array(boundaries)\n        cum_integrals_at_boundaries = np.array(cum_integrals_at_boundaries)\n        rates = np.array([p[0] for p in pieces_desc])\n\n        def F(t):\n            t_arr = np.atleast_1d(t)\n            results = np.zeros_like(t_arr, dtype=float)\n            \n            # Find which segment each t is in\n            indices = np.searchsorted(boundaries, t_arr, side='right') - 1\n            \n            boundary_t = boundaries[indices]\n            cum_integral_at_boundary = cum_integrals_at_boundaries[indices]\n            rate_for_segment = rates[indices]\n            \n            results = cum_integral_at_boundary + rate_for_segment * (t_arr - boundary_t)\n            results[t_arr == 0] = 0.0\n            \n            return results if np.ndim(t)  0 else results[0]\n\n        return F\n\n    def get_inverse_cumulative_integral_func_piecewise_const(pieces_desc):\n        \"\"\"\n        Creates a function F_inv(y) that analytically computes t such that F(t)=y.\n\n        Args:\n            pieces_desc: A list of (value, end_time) tuples.\n        \n        Returns:\n            A vectorized function F_inv(y).\n        \"\"\"\n        boundaries = [0.0]\n        cum_integrals_at_boundaries = [0.0]\n        last_t = 0.0\n\n        for val, end_t in pieces_desc:\n            if np.isinf(end_t):\n                break\n            if val == 0:\n                raise ValueError(\"Intensity rates must be positive.\")\n            integral_of_piece = val * (end_t - last_t)\n            boundaries.append(end_t)\n            cum_integrals_at_boundaries.append(cum_integrals_at_boundaries[-1] + integral_of_piece)\n            last_t = end_t\n        \n        boundaries = np.array(boundaries)\n        cum_integrals_at_boundaries = np.array(cum_integrals_at_boundaries)\n        rates = np.array([p[0] for p in pieces_desc])\n\n        def F_inv(y):\n            y_arr = np.atleast_1d(y)\n            results = np.zeros_like(y_arr, dtype=float)\n            \n            # Find which integral value segment y is in\n            indices = np.searchsorted(cum_integrals_at_boundaries, y_arr, side='right') - 1\n            \n            boundary_t = boundaries[indices]\n            cum_integral_at_boundary = cum_integrals_at_boundaries[indices]\n            rate_for_segment = rates[indices]\n            \n            results = boundary_t + (y_arr - cum_integral_at_boundary) / rate_for_segment\n            results[y_arr == 0] = 0.0\n\n            return results if np.ndim(y)  0 else results[0]\n\n        return F_inv\n\n    def process_case(n, lambda_true_desc, lambda_hat_desc, t0=0.0):\n        # Step 1: Generate event times from lambda_true\n        u_stars = (np.arange(1, n + 1)) / (n + 1.0)\n        m_values = -np.log(1.0 - u_stars)\n        \n        event_times = np.zeros(n + 1)\n        event_times[0] = t0\n\n        if isinstance(lambda_true_desc, (int, float)):\n            # Constant intensity case\n            rate = float(lambda_true_desc)\n            if rate == 0: raise ValueError(\"Intensity must be positive.\")\n            deltas = m_values / rate\n            event_times[1:] = t0 + np.cumsum(deltas)\n        else:\n            # Piecewise constant intensity case\n            F_true_inv = get_inverse_cumulative_integral_func_piecewise_const(lambda_true_desc)\n            F_true_t0 = get_cumulative_integral_func_piecewise_const(lambda_true_desc)(t0)\n            cumulative_m = F_true_t0 + np.cumsum(m_values)\n            event_times[1:] = F_true_inv(cumulative_m)\n\n        # Step 2: Compute rescaled intervals using lambda_hat\n        if isinstance(lambda_hat_desc, (int, float)):\n            # Constant intensity case\n            rate = float(lambda_hat_desc)\n            deltas = np.diff(event_times)\n            M_values = rate * deltas\n        else:\n            # Piecewise constant intensity case\n            F_hat = get_cumulative_integral_func_piecewise_const(lambda_hat_desc)\n            F_hat_t_values = F_hat(event_times)\n            M_values = np.diff(F_hat_t_values)\n\n        u_computed = 1.0 - np.exp(-M_values)\n        \n        # Step 3: Perform Kolmogorov-Smirnov test\n        ks_stat, p_val = kstest(u_computed, 'uniform')\n        \n        return ks_stat, p_val\n\n    test_cases = [\n        # n, lambda_true, lambda_hat\n        (20, 12.0, 12.0),\n        (20, 6.0, 12.0),\n        (25, [(8.0, 5.0), (16.0, np.inf)], [(8.0, 5.0), (16.0, np.inf)]),\n        (5, 12.0, [(8.0, 5.0), (16.0, np.inf)]),\n    ]\n\n    all_results = []\n    for n, l_true, l_hat in test_cases:\n        stat, pval = process_case(n, l_true, l_hat)\n        all_results.extend([f\"{stat:.6f}\", f\"{pval:.6f}\"])\n\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "生成拟合优度图只是模型检验过程的一半，关键的下一步是准确地解读它。本练习聚焦于时间重整定理中一个强大的诊断工具——柯尔莫哥洛夫-斯米尔诺夫（KS）图的解释 。您将面对几种典型的KS图偏离理想情况的模式，并需要将这些抽象的图形特征与具体的模型设定失当联系起来，例如模型未能捕捉到神经元的不应期或数据中存在过度离散现象，这是改进和完善神经编码模型的关键诊断技能。",
            "id": "4159628",
            "problem": "您已经使用广义线性模型 (GLM) 为单个神经元拟合了一个点过程编码模型，其中拟合的条件强度表示为 $\\hat{\\lambda}(t \\mid H_t, x(t))$，$H_t$ 是脉冲历史，$x(t)$ 是刺激。为了评估拟合优度，您应用了时间重标度定理，并通过将每个脉冲间隔转换为一个重标度变量来构建柯尔莫哥洛夫-斯米尔诺夫 (KS) 图。该重标度变量的经验累积分布函数 (CDF) 与 $\\text{Uniform}(0,1)$ 分布的 CDF 进行比较。在正确指定的模型下，KS 图应与从 $(0,0)$ 到 $(1,1)$ 的对角参考线对齐，误差在抽样变异性范围内。\n\n考虑在匹配的刺激条件下记录的不同细胞的 KS 图中观察到的两种系统性偏差模式：\n- 模式 $\\mathcal{A}$：对于大多数 $u \\in (0,1)$，经验 CDF 位于对角线下方，并呈向上凹的形状（即，斜率随 $u$ 增大而增大），仅在 $u \\approx 1$ 附近接近对角线。\n- 模式 $\\mathcal{B}$：经验 CDF 相对于对角线呈 S 形：对于较小的 $u$，它位于对角线上方；在中间的 $u$ 值处，它降到对角线下方；在接近 $u \\approx 1$ 时再次升至对角线上方。\n\n假设刺激协变量 $x(t)$ 已被很好地捕捉，并且潜在拟合不良的主要来源是 (i) 缺失短时间尺度的脉冲历史结构（如不应期）和 (ii) 未观测到的导致脉冲发放相对于条件泊松 GLM 产生过离散的慢增益波动。\n\n下列哪种解释最符合时间重标度变换和点过程的柯尔莫哥洛夫-斯米尔诺夫 (KS) 检验的基本原理？\n\nA. 模式 $\\mathcal{A}$ 表明存在未建模的不应期，它抑制了脉冲发放后真实的条件强度，使其低于 $\\hat{\\lambda}(t \\mid H_t, x(t))$；而模式 $\\mathcal{B}$ 表明存在由潜在速率波动（随机增益）驱动的过离散，导致相对于 $\\text{Uniform}(0,1)$ 分布，重标度变量在小值和大值处都产生了过多的质量。\n\nB. 模式 $\\mathcal{A}$ 表明存在未建模的不应期，而模式 $\\mathcal{B}$ 表明存在欠离散，这是由于伽马形状参数大于1的更新过程所特有的过于规则的脉冲间隔造成的。\n\nC. 模式 $\\mathcal{A}$ 表明模型平均低估了基线发放率，而模式 $\\mathcal{B}$ 表明模型缺少一个绝对不应期，这导致了过多的极短重标度值。\n\nD. 模式 $\\mathcal{A}$ 表明存在由潜在速率波动引起的过离散，而模式 $\\mathcal{B}$ 表明脉冲历史项过强，导致欠离散和极端重标度值的缺失。",
            "solution": "该分析基于点过程的时间重标度定理。设脉冲时间序列为 $\\{t_i\\}$，真实条件强度函数为 $\\lambda(t \\mid H_t, x(t))$。一个拟合模型提供了该强度的估计值 $\\hat{\\lambda}(t \\mid H_t, x(t))$。该定理指出，如果模型是正确的（即，对于所有 $t$，都有 $\\hat{\\lambda}(t) = \\lambda(t)$），那么重标度的脉冲间隔 (ISIs)\n$$ \\Delta_i = \\int_{t_{i-1}}^{t_i} \\hat{\\lambda}(\\tau \\mid H_\\tau, x(\\tau)) \\, d\\tau $$\n是从标准指数分布 $\\text{Exponential}(1)$ 中抽取的独立同分布的随机变量。\n\nKS 图是通过进一步的变换构建的。如果 $\\Delta_i \\sim \\text{Exponential}(1)$，那么变量 $u_i = 1 - \\exp(-\\Delta_i)$ 是从 $\\text{Uniform}(0,1)$ 分布中抽取的独立同分布变量。这是概率积分变换的一个应用。KS 图将观测到的 $\\{u_i\\}$ 的经验累积分布函数 (CDF)，记为 $\\hat{F}_U(u)$，与 $\\text{Uniform}(0,1)$ 分布的 CDF（即从 $(0,0)$ 到 $(1,1)$ 的对角线 $F_U(u) = u$）进行比较。与这条对角线的偏离表明了特定类型的模型设定错误。\n\n我们来分析模型失配与重标度区间 $\\Delta_i$ 之间的关系：\n- 如果在一个脉冲间隔 (ISI) 内，模型的速率 $\\hat{\\lambda}(t)$ 系统性地高估了真实速率 $\\lambda(t)$，那么得到的 $\\Delta_i$ 将在随机意义上大于一个 $\\text{Exponential}(1)$ 变量。\n- 如果 $\\hat{\\lambda}(t)$ 系统性地低估了 $\\lambda(t)$，那么得到的 $\\Delta_i$ 将在随机意义上小于一个 $\\text{Exponential}(1)$ 变量。\n\n现在我们分析给定的模式。\n\n**模式 $\\mathcal{A}$ 的分析：** 对于大多数 $u \\in (0,1)$，经验 CDF $\\hat{F}_U(u)$ 位于对角线下方。\n这意味着 $\\hat{F}_U(u)  u$。由于 $\\hat{F}_U(u) \\approx F_{\\hat{\\Delta}}(-\\ln(1-u))$，其中 $F_{\\hat{\\Delta}}$ 是 $\\{\\Delta_i\\}$ 的 CDF，这意味着 $F_{\\hat{\\Delta}}(-\\ln(1-u))  u$。代入 $x = -\\ln(1-u)$ 得到 $F_{\\hat{\\Delta}}(x)  1-e^{-x}$，后者是 $\\text{Exponential}(1)$ 分布的 CDF。一个向右移动的 CDF（即小于参考 CDF）对应于一个随机意义上值更大的分布。这意味着重标度区间 $\\{\\Delta_i\\}$ 系统性地过大了。\n\n这种情况发生在模型速率 $\\hat{\\lambda}(t)$ 平均高估了真实速率 $\\lambda(t)$ 的时候。考虑**未建模的不应期**的情况。一个真实的神经元在一次脉冲发放后立即再次发放的可能性较小。因此，真实的速率 $\\lambda(t)$ 在脉冲后的一小段时间内受到抑制。如果 GLM 未能捕捉到这一点（或低估了其强度），其预测速率 $\\hat{\\lambda}(t)$ 在此期间将高于真实速率 $\\lambda(t)$。对于任何给定的 ISI，尤其是短的 ISI，积分 $\\int \\hat{\\lambda}(t) dt$ 将是一个高估值，导致 $\\Delta_i$ 值膨胀。这造成了小 $\\Delta_i$ 值的缺失，从而导致经验 CDF 位于对角线下方。向上凹的形状反映了最大的偏差（最小的斜率）出现在开始阶段，这对应于短 ISI 的缺失。因此，模式 $\\mathcal{A}$ 是未建模不应期的标志。\n\n**模式 $\\mathcal{B}$ 的分析：** 经验 CDF 呈 S 形，开始时在对角线上方，然后下降到下方，再重新上升。\n- **对于小的 $u$ 值在对角线上方**：对于小的 $u$，$\\hat{F}_U(u)  u$。这意味着对于小的 $x$，$F_{\\hat{\\Delta}}(x)  1-e^{-x}$。这表明与 $\\text{Exponential}(1)$ 分布相比，存在过量的小 $\\Delta_i$ 值。\n- **对于中间的 $u$ 值在对角线下方**：对于中间的 $u$，$\\hat{F}_U(u)  u$。这意味着对于中间的 $x$，$F_{\\hat{\\Delta}}(x)  1-e^{-x}$。这表明中等大小的 $\\Delta_i$ 值存在缺失。\n“在接近 $u \\approx 1$ 时再次升至对角线上方”的描述略有不精确，因为 CDF 必须在 $(1,1)$ 处结束，因此当 $u \\to 1$ 时不可能在对角线上方。然而，这种 S 形（先上后下）是相对于 $\\text{Exponential}(1)$ 分布而言，$\\Delta_i$ 值分布呈过离散的典型特征：它在零附近和尾部有更多的值，而在中间部分的值较少。\n\n考虑**未观测到的慢增益波动**的情况，这会导致过离散。这意味着神经元的兴奋性随时间缓慢波动，而这个因素并未包含在模型 $\\hat{\\lambda}(t)$ 中。模型实质上是对这些增益状态进行了平均。\n- 在高增益（高兴奋性）时期，真实速率 $\\lambda(t)$ 大于模型的平均速率 $\\hat{\\lambda}(t)$。神经元更频繁地发放脉冲，产生短的 ISI。这些 ISI 的重标度区间 $\\Delta_i = \\int \\hat{\\lambda} d\\tau$ 将是低估值，从而产生过量的小 $\\Delta_i$ 值。这解释了为什么 KS 图开始时在对角线上方。\n- 在低增益（低兴奋性）时期，真实速率 $\\lambda(t)$ 小于 $\\hat{\\lambda}(t)$。神经元发放脉冲的频率较低，产生长的 ISI。这些 ISI 的重标度区间将是高估值，从而产生过量的大 $\\Delta_i$ 值。\n这两种效应的结合为 $\\{\\Delta_i\\}$ 创建了一个混合分布，与 $\\text{Exponential}(1)$ 分布相比，其在两端（非常小和非常大的值）有更多的质量，而在中间部分的质量较少。这在这种背景下是过离散的定义，并产生了模式 $\\mathcal{B}$ 的 S 形 KS 图。\n\n### 逐项分析\n\n**A. 模式 $\\mathcal{A}$ 表明存在未建模的不应期，它抑制了脉冲发放后真实的条件强度，使其低于 $\\hat{\\lambda}(t \\mid H_t, x(t))$；而模式 $\\mathcal{B}$ 表明存在由潜在速率波动（随机增益）驱动的过离散，导致相对于 $\\text{Uniform}(0,1)$ 分布，重标度变量在小值和大值处都产生了过多的质量。**\n该选项对模式 $\\mathcal{A}$ 的解释（未建模的不应期）是正确的，如上所述。其对模式 $\\mathcal{B}$ 的解释（由潜在波动引起的过离散）也是正确的，因为它正确地指出了其机制及其后果——重标度值分布在两端有过多的质量，导致了 S 形图。\n**结论：正确。**\n\n**B. 模式 $\\mathcal{A}$ 表明存在未建模的不应期，而模式 $\\mathcal{B}$ 表明存在欠离散，这是由于伽马形状参数大于1的更新过程所特有的过于规则的脉冲间隔造成的。**\n对模式 $\\mathcal{A}$ 的解释是正确的。然而，对模式 $\\mathcal{B}$ 的解释是错误的。欠离散（更规则的脉冲发放，如形状参数 $1$ 的伽马过程）会导致极短和极长 ISI 的缺失。这导致 $\\{\\Delta_i\\}$ 的分布比 $\\text{Exponential}(1)$ 更集中在其均值周围。相应的 KS 图将是一个倒 S 形：对于小的 $u$ 在对角线下方，对于大的 $u$ 在对角线上方。这与模式 $\\mathcal{B}$ 相反。\n**结论：错误。**\n\n**C. 模式 $\\mathcal{A}$ 表明模型平均低估了基线发放率，而模式 $\\mathcal{B}$ 表明模型缺少一个绝对不应期，这导致了过多的极短重标度值。**\n对模式 $\\mathcal{A}$ 的解释是错误的。如果模型低估了速率（$\\hat{\\lambda}(t)  \\lambda(t)$），重标度区间 $\\{\\Delta_i\\}$ 将系统性地过小，KS 图将位于对角线的*上方*，而不是下方。对模式 $\\mathcal{B}$ 的解释也是错误的。缺失不应期是模式 $\\mathcal{A}$ 的原因，而不是模式 $\\mathcal{B}$。此外，它导致重标度值过*大*，而不是过短。\n**结论：错误。**\n\n**D. 模式 $\\mathcal{A}$ 表明存在由潜在速率波动引起的过离散，而模式 $\\mathcal{B}$ 表明脉冲历史项过强，导致欠离散和极端重标度值的缺失。**\n此选项将原因与结果错误地对应。模式 $\\mathcal{A}$ 源于未建模的不应期，而非过离散。模式 $\\mathcal{B}$ 源于过离散，而非欠离散（由过强的脉冲历史项可能导致的）。\n**结论：错误。**",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}