{
    "hands_on_practices": [
        {
            "introduction": "Standard accuracy is often the first metric learned, but it can be highly misleading in many neuroscience applications where the events of interest are rare. This exercise  challenges you to derive why accuracy fails in these scenarios by expressing it as a prevalence-weighted average of per-class performance. By re-weighting to a uniform class prior, you will derive Balanced Accuracy and see firsthand why it provides a more honest assessment of a classifier's ability to detect rare events.",
            "id": "4147499",
            "problem": "A research team is analyzing a binary event detector for intracranial Electroencephalography (EEG) data to flag epileptic seizures in continuous recordings. Seizure segments are rare relative to baseline activity. The detector outputs a binary label $\\hat{Y} \\in \\{0,1\\}$ for each segment, and the ground-truth label is $Y \\in \\{0,1\\}$, where $Y=1$ denotes a seizure and $Y=0$ denotes non-seizure. Let $\\mathrm{TP}$, $\\mathrm{FP}$, $\\mathrm{FN}$, and $\\mathrm{TN}$ denote the entries of the confusion matrix.\n\nStarting from the fundamental definition that the accuracy of a classifier is the expected value of the indicator of correct classification under the empirical data distribution, that is $\\mathrm{Accuracy} = \\mathbb{E}[\\mathbf{1}\\{\\hat{Y} = Y\\}]$, use only core definitions of conditional probability and confusion-matrix terms to:\n- express $\\mathrm{Accuracy}$ as a weighted combination of the per-class recall rates;\n- define a prevalence-independent summary by evaluating the same expectation under a hypothetical uniform class prior over $Y$;\n- and use these derivations to argue, solely from these definitions, why the prevalence-independent summary is preferable to $\\mathrm{Accuracy}$ for rare events.\n\nThen, for a hypothetical dataset with $\\mathrm{TP}=40$, $\\mathrm{FN}=10$, $\\mathrm{FP}=50$, and $\\mathrm{TN}=900$, compute the prevalence-independent summary derived above. Express your final value as a decimal and round to four significant figures.",
            "solution": "The problem is valid as it is scientifically grounded in the principles of statistics and machine learning, well-posed with all necessary information, and stated objectively. The task is to perform a series of derivations concerning classifier evaluation metrics and then compute a specific value.\n\nThe analysis begins with the formal definition of accuracy as the expected value of the indicator function for a correct prediction, $\\mathbf{1}\\{\\hat{Y} = Y\\}$, where the expectation is taken over the empirical data distribution.\n$$\n\\mathrm{Accuracy} = \\mathbb{E}[\\mathbf{1}\\{\\hat{Y} = Y\\}]\n$$\nThis expectation is equivalent to the probability of a correct classification, $P(\\hat{Y}=Y)$. We can expand this probability using the law of total probability, conditioning on the true class label $Y$:\n$$\nP(\\hat{Y}=Y) = P(\\hat{Y}=Y | Y=1)P(Y=1) + P(\\hat{Y}=Y | Y=0)P(Y=0)\n$$\nThe conditional probabilities in this expression correspond to per-class performance metrics.\nThe term $P(\\hat{Y}=Y | Y=1)$ is the probability that the classifier correctly predicts the label as $1$ given that the true label is $1$. This is the definition of the recall for the positive class (class $1$), also known as sensitivity or the true positive rate ($\\mathrm{TPR}$). In terms of confusion matrix components, it is given by:\n$$\n\\mathrm{Recall}_{1} = P(\\hat{Y}=1 | Y=1) = \\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FN}}\n$$\nThe term $P(\\hat{Y}=Y | Y=0)$ is the probability that the classifier correctly predicts the label as $0$ given that the true label is $0$. This is the definition of the recall for the negative class (class $0$), also known as specificity or the true negative rate ($\\mathrm{TNR}$). It is given by:\n$$\n\\mathrm{Recall}_{0} = P(\\hat{Y}=0 | Y=0) = \\frac{\\mathrm{TN}}{\\mathrm{TN}+\\mathrm{FP}}\n$$\nThe probabilities $P(Y=1)$ and $P(Y=0)$ represent the prevalence of the positive and negative classes in the dataset, respectively. Let $p = P(Y=1)$ be the prevalence of the positive class. Then $P(Y=0) = 1-p$. Substituting these definitions back into the expression for accuracy, we get:\n$$\n\\mathrm{Accuracy} = \\mathrm{Recall}_{1} \\cdot p + \\mathrm{Recall}_{0} \\cdot (1-p)\n$$\nThis expression shows that accuracy is a weighted average of the per-class recall rates, where the weights are the class prevalences.\n\nNext, we define a prevalence-independent summary metric by evaluating the same expectation, $\\mathbb{E}[\\mathbf{1}\\{\\hat{Y} = Y\\}]$, under a hypothetical uniform class prior. A uniform prior implies that each class is equally likely, so we set $P(Y=1) = P(Y=0) = \\frac{1}{2}$. Let us denote this summary metric by $S$. Substituting these hypothetical prevalences into our derived expression yields:\n$$\nS = \\mathrm{Recall}_{1} \\cdot \\frac{1}{2} + \\mathrm{Recall}_{0} \\cdot \\frac{1}{2} = \\frac{1}{2}(\\mathrm{Recall}_{1} + \\mathrm{Recall}_{0})\n$$\nThis metric is the arithmetic mean of the recall for each class and is commonly known as balanced accuracy.\n\nWe can now use these derivations to argue why this prevalence-independent summary $S$ is preferable to accuracy for evaluating performance on datasets with rare events. For rare events, such as epileptic seizures, the prevalence of the positive class, $p = P(Y=1)$, is very small ($p \\ll 1$). Consequently, the prevalence of the negative class, $1-p$, is very close to $1$.\nIn the expression for accuracy, $\\mathrm{Accuracy} = \\mathrm{Recall}_{1} \\cdot p + \\mathrm{Recall}_{0} \\cdot (1-p)$, the contribution of the positive class recall, $\\mathrm{Recall}_{1}$, is scaled by the small factor $p$, while the contribution of the negative class recall, $\\mathrm{Recall}_{0}$, is scaled by a factor $(1-p)$ close to $1$. Therefore, accuracy becomes dominated by the model's performance on the majority (negative) class:\n$$\n\\mathrm{Accuracy} \\approx \\mathrm{Recall}_{0}\n$$\nA trivial classifier that always predicts the majority class (i.e., $\\hat{Y}=0$ for all inputs) would have $\\mathrm{TP}=0$ and $\\mathrm{FN}$ equal to the total number of positive instances, resulting in $\\mathrm{Recall}_{1}=0$. However, it would have $\\mathrm{FP}=0$ and $\\mathrm{TN}$ equal to the total number of negative instances, yielding $\\mathrm{Recall}_{0}=1$. The accuracy for this useless classifier would be approximately $1-p$, which is very high if $p$ is small. This demonstrates that standard accuracy is a misleading metric for rare event detection, as it fails to penalize poor performance on the rare class of interest.\n\nIn contrast, the prevalence-independent summary $S = \\frac{1}{2}(\\mathrm{Recall}_{1} + \\mathrm{Recall}_{0})$ gives equal weight ($\\frac{1}{2}$) to the recall of both the positive and negative classes, irrespective of their prevalence in the data. A low $\\mathrm{Recall}_{1}$ will substantially decrease the value of $S$, correctly reflecting the classifier's failure to identify the rare event. Thus, $S$ provides a more balanced and honest assessment of a classifier's utility in an imbalanced class setting.\n\nFinally, we compute the value of this prevalence-independent summary for the given dataset: $\\mathrm{TP}=40$, $\\mathrm{FN}=10$, $\\mathrm{FP}=50$, and $\\mathrm{TN}=900$.\nFirst, we calculate the per-class recall rates:\n$$\n\\mathrm{Recall}_{1} = \\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FN}} = \\frac{40}{40+10} = \\frac{40}{50} = 0.8\n$$\n$$\n\\mathrm{Recall}_{0} = \\frac{\\mathrm{TN}}{\\mathrm{TN}+\\mathrm{FP}} = \\frac{900}{900+50} = \\frac{900}{950} = \\frac{18}{19}\n$$\nNow, we compute the summary metric $S$:\n$$\nS = \\frac{1}{2}(\\mathrm{Recall}_{1} + \\mathrm{Recall}_{0}) = \\frac{1}{2}\\left(\\frac{4}{5} + \\frac{18}{19}\\right)\n$$\nTo sum the fractions, we find a common denominator, which is $5 \\times 19 = 95$:\n$$\nS = \\frac{1}{2}\\left(\\frac{4 \\times 19}{95} + \\frac{18 \\times 5}{95}\\right) = \\frac{1}{2}\\left(\\frac{76}{95} + \\frac{90}{95}\\right) = \\frac{1}{2}\\left(\\frac{76+90}{95}\\right) = \\frac{1}{2}\\left(\\frac{166}{95}\\right) = \\frac{83}{95}\n$$\nConverting this fraction to a decimal gives:\n$$\nS = \\frac{83}{95} \\approx 0.87368421...\n$$\nRounding to four significant figures, we get $0.8737$.",
            "answer": "$$\n\\boxed{0.8737}\n$$"
        },
        {
            "introduction": "While metrics like precision, recall, and the F1-score are valuable, interpreting them together can be cumbersome. The Matthews Correlation Coefficient (MCC) offers a single, robust value, but its formula can seem opaque. This practice  demystifies the MCC by guiding you to derive it directly from the Pearson correlation coefficient, revealing it as a direct measure of the correlation between the true and predicted binary labels.",
            "id": "4147553",
            "problem": "A laboratory is evaluating a binary detector that flags hippocampal sharp-wave ripple events in local field potential recordings. For each of $n$ time windows, the true event label $Y_{i} \\in \\{0,1\\}$ and the predicted label $\\hat{Y}_{i} \\in \\{0,1\\}$ are recorded, where $1$ denotes an event and $0$ denotes no event. The empirical $2 \\times 2$ confusion matrix counts are defined as follows: true positives $TP$, false positives $FP$, true negatives $TN$, and false negatives $FN$, with $n = TP + FP + TN + FN$.\n\nStarting from the fundamental definition of the Pearson product-moment correlation coefficient between two random variables,\n$$\nr = \\frac{\\sum_{i=1}^{n} \\left(X_{i} - \\bar{X}\\right)\\left(Z_{i} - \\bar{Z}\\right)}{\\sqrt{\\left(\\sum_{i=1}^{n} \\left(X_{i} - \\bar{X}\\right)^{2}\\right)\\left(\\sum_{i=1}^{n} \\left(Z_{i} - \\bar{Z}\\right)^{2}\\right)}},\n$$\nderive the closed-form expression of the correlation between the binary true labels $Y_{i}$ and the binary predicted labels $\\hat{Y}_{i}$ in terms of only the confusion matrix counts $TP$, $FP$, $TN$, and $FN$. Interpret the resulting expression as the Matthews Correlation Coefficient (MCC), and explain why it can be viewed as a correlation between predicted and true binary labels. Express your final result as a single symbolic expression involving only $TP$, $FP$, $TN$, and $FN$.",
            "solution": "The problem requires the derivation of the Matthews Correlation Coefficient (MCC) by starting with the Pearson product-moment correlation coefficient, $r$, applied to two binary vectors: the true labels, $Y_i \\in \\{0,1\\}$, and the predicted labels, $\\hat{Y}_i \\in \\{0,1\\}$, for $i=1, \\dots, n$.\n\nTo derive the expression for the correlation between the true labels $Y_i$ and predicted labels $\\hat{Y}_i$, we will use the computational formula for the Pearson correlation coefficient, which is algebraically equivalent to the one provided:\n$$\nr = \\frac{n \\sum_{i=1}^{n} Y_i \\hat{Y}_i - \\left(\\sum_{i=1}^{n} Y_i\\right) \\left(\\sum_{i=1}^{n} \\hat{Y}_i\\right)}{\\sqrt{\\left[n \\sum_{i=1}^{n} Y_i^2 - \\left(\\sum_{i=1}^{n} Y_i\\right)^2\\right] \\left[n \\sum_{i=1}^{n} \\hat{Y}_i^2 - \\left(\\sum_{i=1}^{n} \\hat{Y}_i\\right)^2\\right]}}\n$$\nOur task is to express each sum in this formula in terms of the confusion matrix counts: $TP$, $FP$, $TN$, and $FN$.\n\n1.  **Express sums involving $Y_i$ and $\\hat{Y}_i$**:\n    The sum $\\sum_{i=1}^{n} Y_i$ counts the total number of actual positive cases (where $Y_i = 1$). By definition, this is the sum of true positives and false negatives.\n    $$ \\sum_{i=1}^{n} Y_i = TP + FN $$\n    Similarly, the sum $\\sum_{i=1}^{n} \\hat{Y}_i$ counts the total number of predicted positive cases (where $\\hat{Y}_i = 1$). This is the sum of true positives and false positives.\n    $$ \\sum_{i=1}^{n} \\hat{Y}_i = TP + FP $$\n\n2.  **Express sum of squares**:\n    Since both $Y_i$ and $\\hat{Y}_i$ are binary variables taking values in $\\{0,1\\}$, we have $Y_i^2 = Y_i$ and $\\hat{Y}_i^2 = \\hat{Y}_i$.\n    Therefore, the sum of squares is equal to the sum of the variables themselves.\n    $$ \\sum_{i=1}^{n} Y_i^2 = \\sum_{i=1}^{n} Y_i = TP + FN $$\n    $$ \\sum_{i=1}^{n} \\hat{Y}_i^2 = \\sum_{i=1}^{n} \\hat{Y}_i = TP + FP $$\n\n3.  **Express sum of products**:\n    The product $Y_i \\hat{Y}_i$ is equal to $1$ if and only if both $Y_i=1$ and $\\hat{Y}_i=1$. This condition corresponds to a true positive. The sum $\\sum_{i=1}^{n} Y_i \\hat{Y}_i$ counts the number of such instances.\n    $$ \\sum_{i=1}^{n} Y_i \\hat{Y}_i = TP $$\n\nNow, we substitute these expressions back into the formula for $r$.\n\n**Numerator**:\nThe numerator is $n \\sum Y_i \\hat{Y}_i - (\\sum Y_i)(\\sum \\hat{Y}_i)$.\n$$ \\text{Numerator} = n(TP) - (TP + FN)(TP + FP) $$\nSubstitute $n = TP + FP + TN + FN$:\n$$ \\text{Numerator} = (TP + FP + TN + FN)TP - (TP + FN)(TP + FP) $$\n$$ = (TP^2 + FP \\cdot TP + TN \\cdot TP + FN \\cdot TP) - (TP^2 + TP \\cdot FP + FN \\cdot TP + FN \\cdot FP) $$\n$$ = TP^2 + FP \\cdot TP + TN \\cdot TP + FN \\cdot TP - TP^2 - TP \\cdot FP - FN \\cdot TP - FN \\cdot FP $$\nAfter canceling terms, we are left with:\n$$ \\text{Numerator} = TP \\cdot TN - FP \\cdot FN $$\n\n**Denominator**:\nThe denominator is the square root of the product of two terms. Let's evaluate each term separately.\n\nFirst term in the radicand: $n \\sum Y_i^2 - (\\sum Y_i)^2$.\n$$ n(TP + FN) - (TP + FN)^2 = (TP + FN) [n - (TP + FN)] $$\nSince $n = TP + FP + TN + FN$, we have $n - (TP + FN) = FP + TN$.\n$$ \\text{First Term} = (TP + FN)(FP + TN) $$\n\nSecond term in the radicand: $n \\sum \\hat{Y}_i^2 - (\\sum \\hat{Y}_i)^2$.\n$$ n(TP + FP) - (TP + FP)^2 = (TP + FP) [n - (TP + FP)] $$\nSince $n = TP + FP + TN + FN$, we have $n - (TP + FP) = TN + FN$.\n$$ \\text{Second Term} = (TP + FP)(TN + FN) $$\n\nThe full denominator is the square root of the product of these two terms:\n$$ \\text{Denominator} = \\sqrt{ (TP + FN)(FP + TN) \\cdot (TP + FP)(TN + FN) } $$\n\n**Final Assembly**:\nCombining the numerator and denominator gives the final expression for the correlation coefficient $r$:\n$$ r = \\frac{TP \\cdot TN - FP \\cdot FN}{\\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}} $$\n\nThis expression is precisely the definition of the Matthews Correlation Coefficient (MCC). This derivation demonstrates that the MCC is not an arbitrary metric but is fundamentally the Pearson product-moment correlation coefficient calculated between the binary vectors of true and predicted labels.\n\nThe interpretation of this result is that MCC directly measures the quality of a binary classification as a correlation. A value of $r = MCC = +1$ indicates a perfect prediction, where the predicted labels are perfectly correlated with the true labels. A value of $r = MCC = -1$ indicates a total disagreement, where the predictions are perfectly anti-correlated with the true labels. A value of $r = MCC = 0$ indicates that the prediction is no better than random guessing with respect to the true labels. This grounding in correlation theory is what makes MCC a comprehensive and balanced metric, as it accounts for all four values in the confusion matrix and is robust to class imbalance.",
            "answer": "$$\n\\boxed{\\frac{TP \\cdot TN - FP \\cdot FN}{\\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}}}\n$$"
        },
        {
            "introduction": "In machine learning, we rarely evaluate a model on a single data split; cross-validation is the standard. This raises a critical question: how should we aggregate performance metrics across folds? This exercise  explores the crucial, and often overlooked, distinction between macro-averaging (averaging the scores) and micro-averaging (calculating a score from summed counts), demonstrating why these two methods can yield different results for nonlinear metrics like the F1-score.",
            "id": "4147572",
            "problem": "A neuroscience laboratory is evaluating a binary classifier that detects epileptiform spikes in intracranial Electroencephalography (EEG) segments. The classifier is assessed using two-fold Cross-Validation (CV). For each fold, the confusion matrix entries are recorded as counts of True Positives ($\\mathrm{TP}$), False Positives ($\\mathrm{FP}$), False Negatives ($\\mathrm{FN}$), and True Negatives ($\\mathrm{TN}$). The recorded counts are:\n- Fold $1$: $\\mathrm{TP}_1 = 90$, $\\mathrm{FP}_1 = 10$, $\\mathrm{FN}_1 = 90$, $\\mathrm{TN}_1 = 810$.\n- Fold $2$: $\\mathrm{TP}_2 = 10$, $\\mathrm{FP}_2 = 90$, $\\mathrm{FN}_2 = 10$, $\\mathrm{TN}_2 = 890$.\n\nInvestigators consider two ways to summarize the $F1$ score across folds:\n- Macro-averaging: compute the $F1$ score per fold and then take the unweighted average across folds.\n- Micro-averaging: aggregate the confusion matrix counts across folds and then compute a single $F1$ score from the aggregated counts.\n\nUsing only core definitions of the confusion matrix, precision, recall, and harmonic mean, and reasoning from first principles, decide which of the following statements are correct about why aggregating per-fold $F1$ is generally not equivalent to computing $F1$ from the aggregated confusion matrix.\n\nA. The macro-averaged $F1$ equals the $F1$ computed from the aggregated confusion matrix regardless of class imbalance across folds.\n\nB. Because the $F1$ score is a nonlinear function of the confusion counts, the unweighted average of per-fold $F1$ (macro-averaging) generally differs from the $F1$ computed after aggregating counts (micro-averaging); they coincide only under special conditions.\n\nC. If for each fold $i$ there exist constants $a$ and $b$ such that $\\mathrm{FP}_i = a \\,\\mathrm{TP}_i$ and $\\mathrm{FN}_i = b \\,\\mathrm{TP}_i$, then the macro-averaged $F1$ equals the $F1$ computed from aggregated counts.\n\nD. For the folds provided above, the macro-averaged $F1$ equals the $F1$ computed from the aggregated confusion matrix.\n\nE. By Jensen’s inequality, the macro-averaged $F1$ is always strictly less than the micro-averaged $F1$ in CV settings.\n\nSelect all correct options.",
            "solution": "Begin from core definitions. For a binary classifier with confusion matrix entries $\\mathrm{TP}$, $\\mathrm{FP}$, $\\mathrm{FN}$, and $\\mathrm{TN}$, define precision $P$ and recall $R$ by\n$$\nP = \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FP}}, \\qquad R = \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FN}}.\n$$\nThe $F1$ score is defined as the harmonic mean of precision and recall:\n$$\nF1 = \\frac{2 P R}{P + R}.\n$$\nExpress $F1$ directly in terms of confusion counts. Substitute the definitions of $P$ and $R$ into the harmonic mean:\n$$\nF1 = \\frac{2 \\left( \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FP}} \\right) \\left( \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FN}} \\right)}{\\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FP}} + \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FN}}}\n= \\frac{2 \\mathrm{TP}}{2 \\mathrm{TP} + \\mathrm{FP} + \\mathrm{FN}}.\n$$\nThus, for a single set of counts, $F1$ is a rational function of $\\mathrm{TP}$, $\\mathrm{FP}$, and $\\mathrm{FN}$.\n\nConsider two aggregation strategies across folds $i = 1,2,\\dots,k$:\n- Macro-averaging (unweighted): compute $F1_i$ per fold and average $F1_{\\text{macro}} = \\frac{1}{k} \\sum_{i=1}^k F1_i$.\n- Micro-averaging via the aggregated confusion matrix: compute\n$$\n\\mathrm{TP}_{\\text{agg}} = \\sum_{i=1}^k \\mathrm{TP}_i, \\quad \\mathrm{FP}_{\\text{agg}} = \\sum_{i=1}^k \\mathrm{FP}_i, \\quad \\mathrm{FN}_{\\text{agg}} = \\sum_{i=1}^k \\mathrm{FN}_i,\n$$\nthen\n$$\nF1_{\\text{micro}} = \\frac{2 \\,\\mathrm{TP}_{\\text{agg}}}{2 \\,\\mathrm{TP}_{\\text{agg}} + \\mathrm{FP}_{\\text{agg}} + \\mathrm{FN}_{\\text{agg}}}.\n$$\nIn general,\n$$\n\\frac{1}{k} \\sum_{i=1}^k \\frac{2 \\,\\mathrm{TP}_i}{2 \\,\\mathrm{TP}_i + \\mathrm{FP}_i + \\mathrm{FN}_i}\n\\neq\n\\frac{2 \\sum_{i=1}^k \\mathrm{TP}_i}{2 \\sum_{i=1}^k \\mathrm{TP}_i + \\sum_{i=1}^k \\mathrm{FP}_i + \\sum_{i=1}^k \\mathrm{FN}_i},\n$$\nbecause averaging a nonlinear rational function is not, in general, equal to the function evaluated at averaged (or summed) arguments.\n\nDemonstrate with the provided folds. Compute per-fold precision, recall, and $F1$.\n\nFold $1$:\n$$\nP_1 = \\frac{90}{90 + 10} = \\frac{90}{100} = 0.9, \\qquad R_1 = \\frac{90}{90 + 90} = \\frac{90}{180} = 0.5,\n$$\n$$\nF1_1 = \\frac{2 \\cdot 0.9 \\cdot 0.5}{0.9 + 0.5} = \\frac{0.9}{1.4} \\approx 0.642857.\n$$\nAlternatively via counts,\n$$\nF1_1 = \\frac{2\\cdot 90}{2\\cdot 90 + 10 + 90} = \\frac{180}{280} = 0.642857.\n$$\n\nFold $2$:\n$$\nP_2 = \\frac{10}{10 + 90} = \\frac{10}{100} = 0.1, \\qquad R_2 = \\frac{10}{10 + 10} = \\frac{10}{20} = 0.5,\n$$\n$$\nF1_2 = \\frac{2 \\cdot 0.1 \\cdot 0.5}{0.1 + 0.5} = \\frac{0.1}{0.6} \\approx 0.1666667,\n$$\nequivalently\n$$\nF1_2 = \\frac{2\\cdot 10}{2\\cdot 10 + 90 + 10} = \\frac{20}{120} = 0.1666667.\n$$\n\nMacro-averaged $F1$:\n$$\nF1_{\\text{macro}} = \\frac{F1_1 + F1_2}{2} \\approx \\frac{0.642857 + 0.1666667}{2} \\approx 0.4047619.\n$$\n\nAggregate the counts:\n$$\n\\mathrm{TP}_{\\text{agg}} = 90 + 10 = 100, \\quad \\mathrm{FP}_{\\text{agg}} = 10 + 90 = 100, \\quad \\mathrm{FN}_{\\text{agg}} = 90 + 10 = 100,\n$$\nthen\n$$\nF1_{\\text{micro}} = \\frac{2\\cdot 100}{2\\cdot 100 + 100 + 100} = \\frac{200}{400} = 0.5.\n$$\nWe observe $F1_{\\text{macro}} \\approx 0.405 \\neq 0.5 = F1_{\\text{micro}}$, demonstrating non-equivalence.\n\nCharacterize conditions for equality. Suppose for each fold $i$ there exist constants $a$ and $b$, independent of $i$, such that\n$$\n\\mathrm{FP}_i = a \\,\\mathrm{TP}_i, \\qquad \\mathrm{FN}_i = b \\,\\mathrm{TP}_i.\n$$\nThen per-fold $F1$ simplifies to\n$$\nF1_i = \\frac{2 \\,\\mathrm{TP}_i}{2 \\,\\mathrm{TP}_i + a \\,\\mathrm{TP}_i + b \\,\\mathrm{TP}_i} = \\frac{2}{2 + a + b},\n$$\nwhich is constant across folds. Thus,\n$$\nF1_{\\text{macro}} = \\frac{1}{k} \\sum_{i=1}^k F1_i = \\frac{2}{2 + a + b}.\n$$\nAggregated counts satisfy\n$$\n\\mathrm{TP}_{\\text{agg}} = \\sum_i \\mathrm{TP}_i, \\quad \\mathrm{FP}_{\\text{agg}} = \\sum_i a \\,\\mathrm{TP}_i = a \\sum_i \\mathrm{TP}_i, \\quad \\mathrm{FN}_{\\text{agg}} = \\sum_i b \\,\\mathrm{TP}_i = b \\sum_i \\mathrm{TP}_i,\n$$\nhence\n$$\nF1_{\\text{micro}} = \\frac{2 \\sum_i \\mathrm{TP}_i}{2 \\sum_i \\mathrm{TP}_i + a \\sum_i \\mathrm{TP}_i + b \\sum_i \\mathrm{TP}_i} = \\frac{2}{2 + a + b} = F1_{\\text{macro}}.\n$$\nTherefore equality holds under the special proportionality condition on error counts relative to $\\mathrm{TP}$ across folds; otherwise, nonlinearity produces differences.\n\nFinally, discuss common misstatements. Jensen’s inequality provides bounds for expectations of convex or concave functions; however, $F1$ as a rational function of counts is neither globally convex nor concave over the relevant domain, and the averaging here mixes different denominators across folds. Consequently, no general inequality guarantees that the macro-averaged $F1$ is always less than the micro-averaged $F1$; it can be less, equal, or greater depending on the distribution of counts.\n\nOption-by-option analysis:\n\nA. Claims equality regardless of class imbalance. The demonstration above shows $F1_{\\text{macro}} \\approx 0.405$ and $F1_{\\text{micro}} = 0.5$ for imbalanced folds, so equality does not generally hold. Verdict: Incorrect.\n\nB. States the correct reason (nonlinearity) and correctly distinguishes macro versus micro aggregation, noting equality only under special conditions. Verdict: Correct.\n\nC. Provides a concrete special condition, $\\mathrm{FP}_i = a \\,\\mathrm{TP}_i$ and $\\mathrm{FN}_i = b \\,\\mathrm{TP}_i$ for all folds, under which macro and micro $F1$ coincide. The derivation above proves this. Verdict: Correct.\n\nD. Asserts equality for the provided folds. Numerical computation shows they differ ($0.4047619$ versus $0.5$). Verdict: Incorrect.\n\nE. Invokes Jensen’s inequality to claim macro $F1$ is always strictly less than micro $F1$. Because $F1$ is not globally convex or concave in the relevant variables and the averaging is across ratios with different denominators, no such universal ordering holds. Verdict: Incorrect.",
            "answer": "$$\\boxed{BC}$$"
        }
    ]
}