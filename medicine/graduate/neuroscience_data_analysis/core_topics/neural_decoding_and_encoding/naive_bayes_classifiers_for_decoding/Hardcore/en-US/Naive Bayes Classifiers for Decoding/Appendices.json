{
    "hands_on_practices": [
        {
            "introduction": "The elegance of the Naive Bayes classifier lies in its simplicity and interpretability, particularly in how it combines evidence from different neurons. This exercise  asks you to derive precisely how the model's confidence, expressed as log-posterior odds, changes when a single neuron is removed. This practice solidifies the understanding that under the independence assumption, evidence is combined additively in the log-domain, making it possible to quantify each neuron's individual contribution to the decoding decision.",
            "id": "4180807",
            "problem": "Consider a two-class decoding task between stimuli $A$ and $B$ using a Naive Bayes classifier in a fixed observation window of duration $T$. You record spike counts from $N$ neurons, collecting the count vector $\\mathbf{x} = (x_1, x_2, \\dots, x_N)$ in the window. Assume conditional independence of neurons given the stimulus, and model each neuron's spike count as a Poisson random variable with stimulus-specific mean rates in the window: for neuron $i$, under stimulus $s \\in \\{A,B\\}$, $x_i \\sim \\mathrm{Poisson}(\\lambda_{i s})$, where $\\lambda_{i s} > 0$ denotes the expected spike count in the window under stimulus $s$. Let the prior probabilities be $\\pi_A = P(S=A)$ and $\\pi_B = P(S=B)$, with $\\pi_A + \\pi_B = 1$. Define the log-posterior odds as\n$$\n\\mathrm{LPO}(\\mathbf{x}) = \\ln\\!\\left(\\frac{P(S=A \\mid \\mathbf{x})}{P(S=B \\mid \\mathbf{x})}\\right).\n$$\nYou now remove a single neuron $j$ from the decoder, forming the reduced count vector $\\mathbf{x}_{-j} = (x_1, \\dots, x_{j-1}, x_{j+1}, \\dots, x_N)$ and recompute the log-posterior odds $\\mathrm{LPO}(\\mathbf{x}_{-j})$ under the same assumptions and priors. Starting strictly from Bayes' theorem and the conditional independence model, derive an exact, closed-form expression for the change in log-posterior odds defined by\n$$\n\\Delta \\mathrm{LPO}_j = \\mathrm{LPO}(\\mathbf{x}_{-j}) - \\mathrm{LPO}(\\mathbf{x}),\n$$\nand express it purely as a function of the single-neuron observation $x_j$ and the two stimulus-specific Poisson means $\\lambda_{jA}$ and $\\lambda_{jB}$. Your final answer must be a single analytic expression. Do not approximate or round.",
            "solution": "The problem is well-posed and scientifically sound, based on standard models in computational neuroscience. We can proceed with the derivation.\n\nThe objective is to derive an expression for the change in log-posterior odds, $\\Delta \\mathrm{LPO}_j = \\mathrm{LPO}(\\mathbf{x}_{-j}) - \\mathrm{LPO}(\\mathbf{x})$. We begin by defining the log-posterior odds for the full dataset, $\\mathrm{LPO}(\\mathbf{x})$.\n\nBy definition, the log-posterior odds are:\n$$\n\\mathrm{LPO}(\\mathbf{x}) = \\ln\\left(\\frac{P(S=A \\mid \\mathbf{x})}{P(S=B \\mid \\mathbf{x})}\\right)\n$$\nWe apply Bayes' theorem to the posterior probabilities $P(S=s \\mid \\mathbf{x}) = \\frac{P(\\mathbf{x} \\mid S=s)P(S=s)}{P(\\mathbf{x})}$, where $s \\in \\{A,B\\}$. Substituting these into the ratio, the evidence term $P(\\mathbf{x})$ cancels:\n$$\n\\frac{P(S=A \\mid \\mathbf{x})}{P(S=B \\mid \\mathbf{x})} = \\frac{P(\\mathbf{x} \\mid S=A)P(S=A)}{P(\\mathbf{x} \\mid S=B)P(S=B)}\n$$\nTaking the natural logarithm of both sides gives:\n$$\n\\mathrm{LPO}(\\mathbf{x}) = \\ln\\left(\\frac{P(S=A)}{P(S=B)}\\right) + \\ln\\left(\\frac{P(\\mathbf{x} \\mid S=A)}{P(\\mathbf{x} \\mid S=B)}\\right)\n$$\nThe first term is the log-prior odds, which we can write using the given priors $\\pi_A$ and $\\pi_B$:\n$$\n\\ln\\left(\\frac{\\pi_A}{\\pi_B}\\right)\n$$\nThe second term is the log-likelihood ratio. We use the conditional independence assumption, which states that the joint likelihood is the product of the individual likelihoods for each neuron:\n$$\nP(\\mathbf{x} \\mid S=s) = \\prod_{i=1}^{N} P(x_i \\mid S=s)\n$$\nThe log-likelihood ratio then becomes a sum of individual log-likelihood ratios:\n$$\n\\ln\\left(\\frac{P(\\mathbf{x} \\mid S=A)}{P(\\mathbf{x} \\mid S=B)}\\right) = \\ln\\left(\\frac{\\prod_{i=1}^{N} P(x_i \\mid S=A)}{\\prod_{i=1}^{N} P(x_i \\mid S=B)}\\right) = \\sum_{i=1}^{N} \\ln\\left(\\frac{P(x_i \\mid S=A)}{P(x_i \\mid S=B)}\\right)\n$$\nNow, we incorporate the Poisson model for the spike counts. The probability mass function for a Poisson random variable is $P(x_i \\mid S=s) = \\frac{\\lambda_{is}^{x_i} \\exp(-\\lambda_{is})}{x_i!}$. Substituting this into the per-neuron log-likelihood ratio:\n$$\n\\ln\\left(\\frac{P(x_i \\mid S=A)}{P(x_i \\mid S=B)}\\right) = \\ln\\left(\\frac{\\lambda_{iA}^{x_i} \\exp(-\\lambda_{iA}) / x_i!}{\\lambda_{iB}^{x_i} \\exp(-\\lambda_{iB}) / x_i!}\\right) = \\ln\\left(\\frac{\\lambda_{iA}^{x_i} \\exp(-\\lambda_{iA})}{\\lambda_{iB}^{x_i} \\exp(-\\lambda_{iB})}\\right)\n$$\nUsing the properties of logarithms, $\\ln(a/b) = \\ln(a) - \\ln(b)$ and $\\ln(a^b) = b\\ln(a)$:\n$$\n\\ln\\left(\\frac{P(x_i \\mid S=A)}{P(x_i \\mid S=B)}\\right) = (x_i \\ln(\\lambda_{iA}) - \\lambda_{iA}) - (x_i \\ln(\\lambda_{iB}) - \\lambda_{iB}) = x_i \\ln\\left(\\frac{\\lambda_{iA}}{\\lambda_{iB}}\\right) - (\\lambda_{iA} - \\lambda_{iB})\n$$\nCombining all terms, the full log-posterior odds are:\n$$\n\\mathrm{LPO}(\\mathbf{x}) = \\ln\\left(\\frac{\\pi_A}{\\pi_B}\\right) + \\sum_{i=1}^{N} \\left[ x_i \\ln\\left(\\frac{\\lambda_{iA}}{\\lambda_{iB}}\\right) - (\\lambda_{iA} - \\lambda_{iB}) \\right]\n$$\nThis expression shows that the total log-posterior odds is a sum of a constant prior term and individual contributions from each of the $N$ neurons.\n\nNext, we compute the log-posterior odds for the reduced dataset, $\\mathrm{LPO}(\\mathbf{x}_{-j})$. This calculation is identical, but the summation is over the set of neurons excluding neuron $j$:\n$$\n\\mathrm{LPO}(\\mathbf{x}_{-j}) = \\ln\\left(\\frac{\\pi_A}{\\pi_B}\\right) + \\sum_{i \\neq j} \\left[ x_i \\ln\\left(\\frac{\\lambda_{iA}}{\\lambda_{iB}}\\right) - (\\lambda_{iA} - \\lambda_{iB}) \\right]\n$$\nFinally, we compute the desired change, $\\Delta \\mathrm{LPO}_j = \\mathrm{LPO}(\\mathbf{x}_{-j}) - \\mathrm{LPO}(\\mathbf{x})$.\nWe can split the sum in the expression for $\\mathrm{LPO}(\\mathbf{x})$ into two parts: the term for neuron $j$ and the sum over all other neurons.\n$$\n\\mathrm{LPO}(\\mathbf{x}) = \\ln\\left(\\frac{\\pi_A}{\\pi_B}\\right) + \\sum_{i \\neq j} \\left[ x_i \\ln\\left(\\frac{\\lambda_{iA}}{\\lambda_{iB}}\\right) - (\\lambda_{iA} - \\lambda_{iB}) \\right] + \\left[ x_j \\ln\\left(\\frac{\\lambda_{jA}}{\\lambda_{jB}}\\right) - (\\lambda_{jA} - \\lambda_{jB}) \\right]\n$$\nNotice that the first two terms in this expression are identical to the expression for $\\mathrm{LPO}(\\mathbf{x}_{-j})$.\n$$\n\\mathrm{LPO}(\\mathbf{x}) = \\mathrm{LPO}(\\mathbf{x}_{-j}) + \\left[ x_j \\ln\\left(\\frac{\\lambda_{jA}}{\\lambda_{jB}}\\right) - (\\lambda_{jA} - \\lambda_{jB}) \\right]\n$$\nRearranging this equation to solve for $\\Delta \\mathrm{LPO}_j$:\n$$\n\\Delta \\mathrm{LPO}_j = \\mathrm{LPO}(\\mathbf{x}_{-j}) - \\mathrm{LPO}(\\mathbf{x}) = - \\left[ x_j \\ln\\left(\\frac{\\lambda_{jA}}{\\lambda_{jB}}\\right) - (\\lambda_{jA} - \\lambda_{jB}) \\right]\n$$\nSimplifying the expression yields the final result:\n$$\n\\Delta \\mathrm{LPO}_j = (\\lambda_{jA} - \\lambda_{jB}) - x_j \\ln\\left(\\frac{\\lambda_{jA}}{\\lambda_{jB}}\\right)\n$$\nThis represents the change in log-posterior odds when neuron $j$ is removed. As required, it is expressed solely as a function of the observation for that neuron, $x_j$, and its stimulus-specific mean spike counts, $\\lambda_{jA}$ and $\\lambda_{jB}$.",
            "answer": "$$\n\\boxed{(\\lambda_{jA} - \\lambda_{jB}) - x_j \\ln\\left(\\frac{\\lambda_{jA}}{\\lambda_{jB}}\\right)}\n$$"
        },
        {
            "introduction": "Translating probabilistic models from theory into robust code requires careful attention to numerical stability. When working with many neurons, the joint likelihood can become vanishingly small, leading to numerical underflow and incorrect results. This hands-on coding challenge  guides you through implementing the \"log-sum-exp\" trick, a vital technique for stably computing the log-evidence and other normalizing constants in the logarithmic domain, a foundational skill for any work in computational modeling.",
            "id": "4180768",
            "problem": "You are analyzing stimulus decoding from neural population responses using the Naive Bayes classifier. Start from the following foundational base: Bayes’ theorem and the Naive Bayes independence assumption. Specifically, for a discrete stimulus set indexed by $s$ and a neural response vector $\\mathbf{r}=(r_1,\\dots,r_I)$ across $I$ neurons, define the posterior by Bayes’ theorem, the evidence, and the Naive Bayes factorization as follows: $p(s\\mid \\mathbf{r})=\\dfrac{p(\\mathbf{r}\\mid s)p(s)}{p(\\mathbf{r})}$, $p(\\mathbf{r})=\\sum_s p(\\mathbf{r}\\mid s)p(s)$, and $p(\\mathbf{r}\\mid s)=\\prod_{i=1}^I p(r_i\\mid s)$, respectively. The computational task is to evaluate the log-evidence $\\log p(\\mathbf{r})$ given per-class per-neuron log-likelihoods and log-priors, while ensuring numerical robustness when the number of neurons $I$ is large or the log-likelihoods are very negative.\n\nYour program must implement a numerically stable computation of $\\log p(\\mathbf{r})$ via the log-sum-exp transformation applied to $\\sum_s \\exp\\!\\left(\\sum_{i=1}^I \\log p(r_i\\mid s)+\\log p(s)\\right)$. Explicitly, you must compute $\\log p(\\mathbf{r})$ from the inputs $L_{s,i}=\\log p(r_i\\mid s)$ and $\\ell_s=\\log p(s)$, by first forming $a_s=\\sum_{i=1}^I L_{s,i}+\\ell_s$ for each class $s$, and then aggregating these $a_s$ values via a numerically stable log-sum-exp transformation. You must design your implementation to handle $-\\infty$ values in $a_s$ (which represent zero probability for a class) and return $-\\infty$ when all classes are impossible.\n\nImplement the computation for the following test suite. In each case, $L$ is a list of lists containing $L_{s,i}$ for $s=0,\\dots,S-1$ and $i=1,\\dots,I$, and $\\ell$ is the list of $\\ell_s=\\log p(s)$ for $s=0,\\dots,S-1$. All logarithms are natural logarithms. For each case, the output is a single real number representing $\\log p(\\mathbf{r})$. The final output must be a single line containing a comma-separated list in square brackets aggregating the results of all cases, in the order listed. No physical units are involved; all outputs are real numbers.\n\nCase $1$ (happy path, moderate values):\n- $S=3$, $I=5$.\n- $L$:\n  - Class $0$: $[-2.3,-0.7,-1.1,-0.5,-3.0]$,\n  - Class $1$: $[-1.9,-1.2,-0.9,-1.5,-2.2]$,\n  - Class $2$: $[-3.2,-0.6,-0.8,-0.4,-2.8]$.\n- $\\ell$: $[\\log(0.2),\\log(0.3),\\log(0.5)] = [-1.6094379124341003,-1.2039728043259361,-0.6931471805599453]$.\n\nCase $2$ (extreme underflow risk, very negative sums):\n- $S=2$, $I=3$.\n- $L$:\n  - Class $0$: $[-1000.0,-800.0,-1200.0]$,\n  - Class $1$: $[-1001.0,-799.0,-1199.0]$.\n- $\\ell$: $[\\log(0.5),\\log(0.5)] = [-0.6931471805599453,-0.6931471805599453]$.\n\nCase $3$ (single-class boundary):\n- $S=1$, $I=4$.\n- $L$:\n  - Class $0$: $[-0.2,-0.4,-0.1,-0.3]$.\n- $\\ell$: $[\\log(1.0)] = [0.0]$.\n\nCase $4$ (balanced classes with equal aggregated scores):\n- $S=2$, $I=2$.\n- $L$:\n  - Class $0$: $[-0.5,-1.0]$,\n  - Class $1$: $[-0.5,-1.0]$.\n- $\\ell$: $[\\log(0.5),\\log(0.5)] = [-0.6931471805599453,-0.6931471805599453]$.\n\nCase $5$ (one class impossible, one class finite):\n- $S=2$, $I=3$.\n- $L$:\n  - Class $0$: $[-\\infty,-0.5,-0.5]$,\n  - Class $1$: $[-0.4,-0.6,-0.3]$.\n- $\\ell$: $[\\log(0.6),\\log(0.4)] = [-0.5108256237659907,-0.916290731874155]$.\n\nCase $6$ (all classes impossible):\n- $S=2$, $I=2$.\n- $L$:\n  - Class $0$: $[-\\infty,-0.2]$,\n  - Class $1$: $[-\\infty,-0.3]$.\n- $\\ell$: $[\\log(0.7),\\log(0.3)] = [-0.35667494393873245,-1.2039728043259361]$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[\\text{result1},\\text{result2},\\dots]$). The results must be ordered exactly as Cases $1$ through $6$ above. Each result must be a real number (floating-point), and it is acceptable for a result to be $-\\infty$ when mathematically warranted.",
            "solution": "The problem is valid. It presents a well-posed, scientifically grounded task in computational neuroscience and machine learning, which is to compute the log-evidence for a Naive Bayes classifier in a numerically stable manner. All required data and definitions are provided, and the problem is free of contradictions or ambiguities.\n\nThe objective is to compute the log-evidence, $\\log p(\\mathbf{r})$, for a given neural response vector $\\mathbf{r}$ and a discrete set of stimuli $\\{s\\}$. The evidence $p(\\mathbf{r})$ is the marginal probability of observing the response $\\mathbf{r}$, and it is defined by marginalizing over all possible stimuli:\n$$p(\\mathbf{r}) = \\sum_s p(\\mathbf{r}, s)$$\nUsing the definition of joint probability, this can be written as:\n$$p(\\mathbf{r}) = \\sum_s p(\\mathbf{r}\\mid s)p(s)$$\nHere, $p(s)$ is the prior probability of stimulus $s$, and $p(\\mathbf{r}\\mid s)$ is the likelihood of observing response $\\mathbf{r}$ given that stimulus $s$ was presented.\n\nThe problem specifies the use of a Naive Bayes model. This model assumes that, conditional on the stimulus $s$, the responses of the individual neurons $r_i$ are independent. For a population of $I$ neurons with response vector $\\mathbf{r}=(r_1, \\dots, r_I)$, this assumption is formulated as:\n$$p(\\mathbf{r}\\mid s) = \\prod_{i=1}^I p(r_i\\mid s)$$\nSubstituting this into the expression for the evidence gives:\n$$p(\\mathbf{r}) = \\sum_s \\left( p(s) \\prod_{i=1}^I p(r_i\\mid s) \\right)$$\nDirect computation of this expression is numerically perilous. For a large number of neurons $I$, the product term $\\prod_{i=1}^I p(r_i\\mid s)$ can easily underflow to zero, even if individual probabilities $p(r_i\\mid s)$ are not exceedingly small. To circumvent this, we perform the computation in the logarithmic domain.\n\nThe desired quantity is the log-evidence, $\\log p(\\mathbf{r})$:\n$$\\log p(\\mathbf{r}) = \\log\\left( \\sum_s \\left( p(s) \\prod_{i=1}^I p(r_i\\mid s) \\right) \\right)$$\nTo work with sums of logarithms instead of products of probabilities, we can express each term in the summation using the exponential function, i.e., $x = \\exp(\\log x)$:\n$$\\log p(\\mathbf{r}) = \\log\\left( \\sum_s \\exp\\left( \\log\\left( p(s) \\prod_{i=1}^I p(r_i\\mid s) \\right) \\right) \\right)$$\nUsing the properties of logarithms, $\\log(ab) = \\log a + \\log b$ and $\\log(\\prod_i a_i) = \\sum_i \\log a_i$, the term inside the exponential becomes:\n$$\\log\\left( p(s) \\prod_{i=1}^I p(r_i\\mid s) \\right) = \\log p(s) + \\sum_{i=1}^I \\log p(r_i\\mid s)$$\nThe problem provides these logarithmic quantities as inputs: the per-neuron log-likelihoods $L_{s,i} = \\log p(r_i\\mid s)$ and the log-priors $\\ell_s = \\log p(s)$. Let us define $a_s$ as the log-joint probability term for each class $s$:\n$$a_s = \\ell_s + \\sum_{i=1}^I L_{s,i}$$\nSubstituting this back, the log-evidence is:\n$$\\log p(\\mathbf{r}) = \\log\\left( \\sum_s \\exp(a_s) \\right)$$\nThis expression is known as the log-sum-exp function, $\\text{LSE}(a_1, a_2, \\dots)$. A naive implementation, which first computes $\\exp(a_s)$ for each $s$ and then sums the results, is still susceptible to numerical issues. If any $a_s$ is a large positive number, $\\exp(a_s)$ can overflow. If all $a_s$ are large negative numbers, $\\exp(a_s)$ can underflow to zero for all $s$, leading to a final result of $\\log(0) = -\\infty$ and a loss of all precision.\n\nTo ensure numerical stability, we use the \"log-sum-exp trick\". Let $a_{\\max} = \\max_s \\{a_s\\}$. We can factor out $\\exp(a_{\\max})$ from the sum:\n$$\\log p(\\mathbf{r}) = \\log\\left( \\exp(a_{\\max}) \\sum_s \\exp(a_s - a_{\\max}) \\right)$$\n$$= a_{\\max} + \\log\\left( \\sum_s \\exp(a_s - a_{\\max}) \\right)$$\nThis formulation is numerically robust. The arguments of the exponential function, $(a_s - a_{\\max})$, are now all less than or equal to $0$. This prevents overflow, as the maximum value of $\\exp(x)$ for $x \\le 0$ is $1$. It also ensures that the terms with the largest contribution to the sum are computed with high precision.\n\nThis approach must also correctly handle special cases involving $-\\infty$. An input value of $L_{s,i} = -\\infty$ or $\\ell_s = -\\infty$ signifies a zero-probability event. Their sum, $a_s$, will also be $-\\infty$. The contribution of such a class to the evidence is $\\exp(-\\infty) = 0$, so these terms correctly vanish from the sum. If all classes are impossible, i.e., all $a_s = -\\infty$, then the total evidence is $0$, and the log-evidence is $\\log(0) = -\\infty$. A robust implementation, such as the `logsumexp` function available in the `scipy.special` module, correctly handles these edge cases.\n\nThe algorithm for each test case is as follows:\n$1$. For each class $s$ from $0$ to $S-1$, calculate $a_s = \\ell_s + \\sum_{i=1}^I L_{s,i}$.\n$2$. Compute the log-evidence by applying the numerically stable log-sum-exp operation to the vector containing all $a_s$ values.\nThis procedure will be applied to each of the provided test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import logsumexp\n\ndef solve():\n    \"\"\"\n    Solves the problem of computing the log-evidence for a Naive Bayes classifier\n    for a suite of test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"L\": [\n                [-2.3, -0.7, -1.1, -0.5, -3.0],\n                [-1.9, -1.2, -0.9, -1.5, -2.2],\n                [-3.2, -0.6, -0.8, -0.4, -2.8]\n            ],\n            \"ell\": [np.log(0.2), np.log(0.3), np.log(0.5)]\n        },\n        {\n            \"L\": [\n                [-1000.0, -800.0, -1200.0],\n                [-1001.0, -799.0, -1199.0]\n            ],\n            \"ell\": [np.log(0.5), np.log(0.5)]\n        },\n        {\n            \"L\": [\n                [-0.2, -0.4, -0.1, -0.3]\n            ],\n            \"ell\": [np.log(1.0)]\n        },\n        {\n            \"L\": [\n                [-0.5, -1.0],\n                [-0.5, -1.0]\n            ],\n            \"ell\": [np.log(0.5), np.log(0.5)]\n        },\n        {\n            \"L\": [\n                [-np.inf, -0.5, -0.5],\n                [-0.4, -0.6, -0.3]\n            ],\n            \"ell\": [np.log(0.6), np.log(0.4)]\n        },\n        {\n            \"L\": [\n                [-np.inf, -0.2],\n                [-np.inf, -0.3]\n            ],\n            \"ell\": [np.log(0.7), np.log(0.3)]\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        # Convert inputs to NumPy arrays for vectorized operations.\n        # L_s,i = log p(r_i|s)\n        # l_s = log p(s)\n        L = np.array(case[\"L\"], dtype=np.float64)\n        ell = np.array(case[\"ell\"], dtype=np.float64)\n\n        # Step 1: Compute a_s for each class s.\n        # a_s = log p(s) + sum_i log p(r_i|s)\n        # This is the log of the joint probability, log p(r, s).\n        # The sum is over neurons (axis=1 of L).\n        a = L.sum(axis=1) + ell\n\n        # Step 2: Compute log-evidence using the log-sum-exp transformation.\n        # log p(r) = log(sum_s exp(a_s))\n        # The scipy.special.logsumexp function provides a numerically stable\n        # implementation that handles -inf correctly.\n        log_evidence = logsumexp(a)\n        \n        results.append(log_evidence)\n\n    # The str() function appropriately formats floating-point numbers, including -inf.\n    # The final output must be a single line containing a comma-separated list\n    # in square brackets.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A powerful decoder is not just one that fits the training data well, but one that generalizes to new, unseen neural activity. This requires carefully tuning model hyperparameters, such as the amount of smoothing applied when estimating firing rates from limited data. This exercise  challenges you to think critically about the principles of cross-validation and identify the correct statistical objectives for selecting a hyperparameter that maximizes a decoder's out-of-sample performance, a crucial step in building reliable brain-computer interfaces and scientific models.",
            "id": "4180789",
            "problem": "A laboratory is decoding stimulus identity from population spike counts recorded from $N$ simultaneously measured neurons. For each trial $i$, the observed count vector is $x_{i} \\in \\mathbb{N}_{0}^{N}$ and the stimulus label is $s_{i} \\in \\{1,\\dots,K\\}$. Assume a Naive Bayes model in which, conditional on the stimulus $s$, neurons are independent and each neuron's spike count in a fixed window of duration $T$ is modeled as Poisson with rate parameter $\\lambda_{n,s}$, i.e., for neuron $n$, $x_{i,n} \\mid s_{i}=s \\sim \\mathrm{Poisson}(\\lambda_{n,s} T)$, independently over $n$. To smooth rate estimates, place a Gamma prior on each $\\lambda_{n,s}$ with shape hyperparameter $\\alpha > 0$ and fixed rate hyperparameter $\\beta > 0$, independent over $n$ and $s$. The class prior $p(s)$ is estimated by empirical frequency on the training data within each fold.\n\nYou are asked to select the smoothing hyperparameter $\\alpha$ using $L$-fold cross-validation. Let $\\mathcal{D}=\\{(x_{i},s_{i})\\}_{i=1}^{M}$ be the full dataset, partitioned into $L$ disjoint test sets $\\{\\mathcal{D}_{\\mathrm{test}}^{(\\ell)}\\}_{\\ell=1}^{L}$ with corresponding training sets $\\mathcal{D}_{\\mathrm{train}}^{(\\ell)}=\\mathcal{D}\\setminus \\mathcal{D}_{\\mathrm{test}}^{(\\ell)}$. For a given $\\alpha$, and within each fold $\\ell$, let $p(x \\mid s, \\mathcal{D}_{\\mathrm{train}}^{(\\ell)}, \\alpha)$ denote the posterior predictive likelihood obtained by integrating the Poisson likelihood over the Gamma prior updated by $\\mathcal{D}_{\\mathrm{train}}^{(\\ell)}$, and let $p(s \\mid x, \\mathcal{D}_{\\mathrm{train}}^{(\\ell)}, \\alpha)$ denote the corresponding posterior over classes via Bayes’ rule.\n\nStarting from first principles (Bayes’ rule for $p(s \\mid x)$ and the definition of cross-validation as an out-of-sample estimate of generalization performance), which of the following candidate objectives correctly define a cross-validated criterion for choosing $\\alpha$ that maximizes held-out log-posterior probability of the true class or held-out classification accuracy? Select all that apply.\n\nA. $\\displaystyle \\hat{\\alpha} \\;=\\; \\arg\\max_{\\alpha>0} \\;\\sum_{\\ell=1}^{L} \\;\\sum_{(x_{i},s_{i}) \\in \\mathcal{D}_{\\mathrm{test}}^{(\\ell)}} \\log p\\!\\left(s_{i} \\,\\middle|\\, x_{i}, \\mathcal{D}_{\\mathrm{train}}^{(\\ell)}, \\alpha\\right)$\n\nB. $\\displaystyle \\hat{\\alpha} \\;=\\; \\arg\\max_{\\alpha>0} \\;\\sum_{\\ell=1}^{L} \\;\\sum_{(x_{i},s_{i}) \\in \\mathcal{D}_{\\mathrm{test}}^{(\\ell)}} \\mathbf{1}\\!\\left\\{\\;\\hat{s}_{\\alpha}^{(\\ell)}(x_{i}) \\;=\\; s_{i}\\;\\right\\}$, where $\\displaystyle \\hat{s}_{\\alpha}^{(\\ell)}(x) \\;=\\; \\arg\\max_{s \\in \\{1,\\dots,K\\}} p\\!\\left(s \\,\\middle|\\, x, \\mathcal{D}_{\\mathrm{train}}^{(\\ell)}, \\alpha\\right)$\n\nC. $\\displaystyle \\hat{\\alpha} \\;=\\; \\arg\\max_{\\alpha>0} \\;\\sum_{i=1}^{M} \\log p\\!\\left(s_{i} \\,\\middle|\\, x_{i}, \\mathcal{D}, \\alpha\\right)$\n\nD. $\\displaystyle \\hat{\\alpha} \\;=\\; \\arg\\max_{\\alpha>0} \\;\\sum_{\\ell=1}^{L} \\;\\sum_{(x_{i},s_{i}) \\in \\mathcal{D}_{\\mathrm{test}}^{(\\ell)}} \\log p\\!\\left(x_{i} \\,\\middle|\\, s_{i}, \\mathcal{D}_{\\mathrm{train}}^{(\\ell)}, \\alpha\\right)$\n\nE. $\\displaystyle \\hat{\\alpha} \\;=\\; \\arg\\max_{\\alpha>0} \\;\\sum_{\\ell=1}^{L} \\;\\sum_{(x_{i},s_{i}) \\in \\mathcal{D}_{\\mathrm{test}}^{(\\ell)}} \\log p\\!\\left(s_{i} \\,\\middle|\\, x_{i}, \\mathcal{D}, \\alpha\\right)$, using $p\\!\\left(s \\mid x, \\mathcal{D}, \\alpha\\right)$ computed with parameters fit on $\\mathcal{D}$ for every fold\n\nEach candidate is to be interpreted exactly as written. No additional smoothing is applied beyond the single hyperparameter $\\alpha$ on the Poisson rates and the empirical class prior. Assume all integrals defining posterior predictive quantities exist and are finite. Provide your selection and justify it from first principles of Bayesian decoding and cross-validation.",
            "solution": "The user wants me to validate the problem statement and, if valid, derive the correct selection of hyperparameters based on the provided objectives.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n- **Model Type:** Naive Bayes classifier for decoding stimulus identity.\n- **Data:** Population spike counts $x_{i} \\in \\mathbb{N}_{0}^{N}$ from $N$ neurons for trial $i$, with stimulus label $s_{i} \\in \\{1,\\dots,K\\}$. Total dataset is $\\mathcal{D}=\\{(x_{i},s_{i})\\}_{i=1}^{M}$.\n- **Likelihood Model:** For a given stimulus $s$, neuron spike counts are independent. For neuron $n$, the count $x_{n}$ in a window of duration $T$ is Poisson distributed: $x_{n} \\mid s \\sim \\mathrm{Poisson}(\\lambda_{n,s} T)$.\n- **Prior on Rate Parameters:** Each rate parameter $\\lambda_{n,s}$ has an independent Gamma prior: $\\lambda_{n,s} \\sim \\mathrm{Gamma}(\\alpha, \\beta)$, where $\\alpha > 0$ is a shape hyperparameter to be tuned, and $\\beta > 0$ is a fixed rate hyperparameter.\n- **Class Prior:** The class prior $p(s)$ is estimated by the empirical frequency on the training data within each fold.\n- **Hyperparameter Selection Method:** $L$-fold cross-validation. The dataset $\\mathcal{D}$ is partitioned into $L$ disjoint test sets $\\{\\mathcal{D}_{\\mathrm{test}}^{(\\ell)}\\}_{\\ell=1}^{L}$ and corresponding training sets $\\mathcal{D}_{\\mathrm{train}}^{(\\ell)}=\\mathcal{D}\\setminus \\mathcal{D}_{\\mathrm{test}}^{(\\ell)}$.\n- **Notation:**\n    - $p(x \\mid s, \\mathcal{D}_{\\mathrm{train}}^{(\\ell)}, \\alpha)$: Posterior predictive likelihood, integrating over the prior on $\\lambda_{n,s}$ updated by the training data $\\mathcal{D}_{\\mathrm{train}}^{(\\ell)}$.\n    - $p(s \\mid x, \\mathcal{D}_{\\mathrm{train}}^{(\\ell)}, \\alpha)$: Posterior probability of the class, derived from the posterior predictive likelihood and class priors via Bayes' rule.\n- **Objective:** Identify the correct cross-validated criterion for choosing $\\alpha$ to maximize either:\n    1.  Held-out log-posterior probability of the true class.\n    2.  Held-out classification accuracy.\n\n**Step 2: Validate Using Extracted Givens**\n\n- **Scientifically Grounded:** The problem is firmly grounded in computational neuroscience and Bayesian statistics. The Poisson model for spike counts, the Gamma prior (which is conjugate to the Poisson likelihood), the Naive Bayes assumption, and the use of cross-validation for hyperparameter tuning are all standard and well-established methods in the field.\n- **Well-Posed:** The problem is clearly defined. It presents a complete statistical model and a specific task (identifying the correct mathematical formulation for hyperparameter selection). A unique solution set exists based on the fundamental principles of cross-validation and probabilistic classification.\n- **Objective:** The language is formal and unambiguous. All terms are defined mathematically.\n\nThe problem statement has no scientific or factual unsoundness, is not non-formalizable, is complete and consistent, is realistic, and is well-posed. It is a standard, non-trivial problem in statistical machine learning applied to neuroscience data.\n\n**Step 3: Verdict and Action**\n\nThe problem is valid. I will proceed with the derivation and analysis.\n\n### Derivation and Option Analysis\n\nThe goal is to select the hyperparameter $\\alpha$ by estimating the model's generalization performance using $L$-fold cross-validation. The core principle of cross-validation is to train the model on a subset of the data (the training set) and evaluate its performance on a disjoint, held-out subset (the test set). This process is repeated for each fold, and the performance metrics are aggregated. The hyperparameter value that yields the best aggregated performance is selected.\n\nLet's formalize the two optimization goals stated in the problem.\n\n**1. Maximizing Held-Out Log-Posterior Probability of the True Class**\n\nFor a chosen hyperparameter $\\alpha$, and for each fold $\\ell \\in \\{1, \\dots, L\\}$, the model is trained using $\\mathcal{D}_{\\mathrm{train}}^{(\\ell)}$. This involves:\n- Estimating the class priors $p(s \\mid \\mathcal{D}_{\\mathrm{train}}^{(\\ell)}, \\alpha)$ from the empirical frequencies of classes in $\\mathcal{D}_{\\mathrm{train}}^{(\\ell)}$.\n- For each class $s$ and neuron $n$, updating the Gamma prior on $\\lambda_{n,s}$ using the training data for that class in $\\mathcal{D}_{\\mathrm{train}}^{(\\ell)}$ to obtain the posterior $p(\\lambda_{n,s} \\mid \\mathcal{D}_{\\mathrm{train}}^{(\\ell)}, \\alpha)$.\n- Using these to define the posterior predictive likelihood $p(x \\mid s, \\mathcal{D}_{\\mathrm{train}}^{(\\ell)}, \\alpha)$.\n\nThe model is then evaluated on the test set $\\mathcal{D}_{\\mathrm{test}}^{(\\ell)}$. For a single test data point $(x_i, s_i) \\in \\mathcal{D}_{\\mathrm{test}}^{(\\ell)}$, the quantity of interest is the posterior probability of the *true* class $s_i$, which is given by Bayes' rule:\n$$\np(s_i \\mid x_i, \\mathcal{D}_{\\mathrm{train}}^{(\\ell)}, \\alpha) = \\frac{p(x_i \\mid s_i, \\mathcal{D}_{\\mathrm{train}}^{(\\ell)}, \\alpha) \\, p(s_i \\mid \\mathcal{D}_{\\mathrm{train}}^{(\\ell)})}{\\sum_{s'=1}^K p(x_i \\mid s', \\mathcal{D}_{\\mathrm{train}}^{(\\ell)}, \\alpha) \\, p(s' \\mid \\mathcal{D}_{\\mathrm{train}}^{(\\ell)})}\n$$\nThe problem asks to maximize the held-out *log-posterior* probability. This corresponds to the objective function, often called the log-likelihood of the held-out labels or negative log-loss. The total score is obtained by summing this quantity over all data points in the test set, and then summing the scores for each fold. This gives the cross-validated objective:\n$$\n\\sum_{\\ell=1}^{L} \\sum_{(x_{i},s_{i}) \\in \\mathcal{D}_{\\mathrm{test}}^{(\\ell)}} \\log p(s_{i} \\mid x_{i}, \\mathcal{D}_{\\mathrm{train}}^{(\\ell)}, \\alpha)\n$$\nThe optimal hyperparameter $\\hat{\\alpha}$ is the one that maximizes this value.\n\n**2. Maximizing Held-Out Classification Accuracy**\n\nAccuracy is the fraction of correctly classified examples. For a test data point $(x_i, s_i) \\in \\mathcal{D}_{\\mathrm{test}}^{(\\ell)}$, we first determine the predicted class, $\\hat{s}_{\\alpha}^{(\\ell)}(x_i)$. This is done by finding the class with the maximum a posteriori (MAP) probability:\n$$\n\\hat{s}_{\\alpha}^{(\\ell)}(x_{i}) = \\arg\\max_{s \\in \\{1,\\dots,K\\}} p(s \\mid x_i, \\mathcal{D}_{\\mathrm{train}}^{(\\ell)}, \\alpha)\n$$\nThe prediction is correct if $\\hat{s}_{\\alpha}^{(\\ell)}(x_{i}) = s_{i}$. We can represent this with an indicator function, $\\mathbf{1}\\{\\hat{s}_{\\alpha}^{(\\ell)}(x_{i}) = s_{i}\\}$, which is $1$ if the prediction is correct and $0$ otherwise.\nTo find the total number of correct predictions across all folds, we sum this indicator function over all test points and all folds. Maximizing accuracy is equivalent to maximizing this total count. The cross-validated objective is:\n$$\n\\sum_{\\ell=1}^{L} \\sum_{(x_{i},s_{i}) \\in \\mathcal{D}_{\\mathrm{test}}^{(\\ell)}} \\mathbf{1}\\{\\hat{s}_{\\alpha}^{(\\ell)}(x_{i}) = s_{i}\\}\n$$\nThe optimal hyperparameter $\\hat{\\alpha}$ is the one that maximizes this sum.\n\nNow, we evaluate each candidate objective.\n\n**A. $\\displaystyle \\hat{\\alpha} \\;=\\; \\arg\\max_{\\alpha>0} \\;\\sum_{\\ell=1}^{L} \\;\\sum_{(x_{i},s_{i}) \\in \\mathcal{D}_{\\mathrm{test}}^{(\\ell)}} \\log p\\!\\left(s_{i} \\,\\middle|\\, x_{i}, \\mathcal{D}_{\\mathrm{train}}^{(\\ell)}, \\alpha\\right)$**\nThis expression calculates, for each fold $\\ell$, the sum of the log-posterior probabilities of the true classes $s_i$ for all points $(x_i, s_i)$ in the test set $\\mathcal{D}_{\\mathrm{test}}^{(\\ell)}$. The posterior probability is conditioned on the training data for that fold, $\\mathcal{D}_{\\mathrm{train}}^{(\\ell)}$. It then sums these values across all $L$ folds. This perfectly matches the derivation for maximizing the held-out log-posterior probability of the true class.\n**Verdict: Correct**\n\n**B. $\\displaystyle \\hat{\\alpha} \\;=\\; \\arg\\max_{\\alpha>0} \\;\\sum_{\\ell=1}^{L} \\;\\sum_{(x_{i},s_{i}) \\in \\mathcal{D}_{\\mathrm{test}}^{(\\ell)}} \\mathbf{1}\\!\\left\\{\\;\\hat{s}_{\\alpha}^{(\\ell)}(x_{i}) \\;=\\; s_{i}\\;\\right\\}$, where $\\displaystyle \\hat{s}_{\\alpha}^{(\\ell)}(x) \\;=\\; \\arg\\max_{s \\in \\{1,\\dots,K\\}} p\\!\\left(s \\,\\middle|\\, x, \\mathcal{D}_{\\mathrm{train}}^{(\\ell)}, \\alpha\\right)$**\nThis expression defines the predicted class $\\hat{s}_{\\alpha}^{(\\ell)}(x_i)$ as the one that maximizes the posterior probability, based on the model trained on $\\mathcal{D}_{\\mathrm{train}}^{(\\ell)}$. It then counts the number of times this prediction matches the true class $s_i$ for all points in the test set $\\mathcal{D}_{\\mathrm{test}}^{(\\ell)}$, and sums these counts over all folds. This is the total number of correct classifications in a full cross-validation run. Maximizing this is equivalent to maximizing cross-validated accuracy. This perfectly matches the derivation for maximizing held-out classification accuracy.\n**Verdict: Correct**\n\n**C. $\\displaystyle \\hat{\\alpha} \\;=\\; \\arg\\max_{\\alpha>0} \\;\\sum_{i=1}^{M} \\log p\\!\\left(s_{i} \\,\\middle|\\, x_{i}, \\mathcal{D}, \\alpha\\right)$**\nThis expression calculates the log-posterior probability for each point $(x_i, s_i)$ in the full dataset $\\mathcal{D}$, using a model that was also trained on the full dataset $\\mathcal{D}$. This is an *in-sample* performance metric. It does not use held-out data and thus is not a cross-validation criterion. This metric is susceptible to overfitting; a model might achieve a high score on the data it was trained on but fail to generalize to new data. Therefore, it is an improper objective for selecting a hyperparameter intended to improve generalization.\n**Verdict: Incorrect**\n\n**D. $\\displaystyle \\hat{\\alpha} \\;=\\; \\arg\\max_{\\alpha>0} \\;\\sum_{\\ell=1}^{L} \\;\\sum_{(x_{i},s_{i}) \\in \\mathcal{D}_{\\mathrm{test}}^{(\\ell)}} \\log p\\!\\left(x_{i} \\,\\middle|\\, s_{i}, \\mathcal{D}_{\\mathrm{train}}^{(\\ell)}, \\alpha\\right)$**\nThis expression sums the log *posterior predictive likelihood* of the features $x_i$ given the true class $s_i$. While this term, $\\log p(x_i|s_i, \\dots)$, is a component of the full log-posterior, it is not the full objective. The objective of a classifier is to predict $s$ from $x$, so the performance metric should be based on $p(s|x)$, not $p(x|s)$. For example, this metric ignores the class priors $p(s)$ and the normalizing evidence term $p(x_i)$. A model could be very good at modeling the features for a rare class (high $p(x|s_{\\text{rare}})$) but a poor classifier overall because it never predicts that class. This objective does not align with the stated goals of maximizing classification accuracy or the posterior probability of the class.\n**Verdict: Incorrect**\n\n**E. $\\displaystyle \\hat{\\alpha} \\;=\\; \\arg\\max_{\\alpha>0} \\;\\sum_{\\ell=1}^{L} \\;\\sum_{(x_{i},s_{i}) \\in \\mathcal{D}_{\\mathrm{test}}^{(\\ell)}} \\log p\\!\\left(s_{i} \\,\\middle|\\, x_{i}, \\mathcal{D}, \\alpha\\right)$, using $p\\!\\left(s \\mid x, \\mathcal{D}, \\alpha\\right)$ computed with parameters fit on $\\mathcal{D}$ for every fold**\nThis expression sums a quantity over the cross-validation test sets. However, the quantity being evaluated, $\\log p(s_i \\mid x_i, \\mathcal{D}, \\alpha)$, is computed using a model trained on the entire dataset $\\mathcal{D}$. This means that for any test point $(x_i, s_i) \\in \\mathcal{D}_{\\mathrm{test}}^{(\\ell)}$, the model has already been trained on $(x_i, s_i)$ because $\\mathcal{D}_{\\mathrm{test}}^{(\\ell)} \\subset \\mathcal{D}$. This is a form of data leakage, where test data is used in the training process. It violates the fundamental principle of cross-validation. The objective is mathematically identical to that in option C, because $\\mathcal{D} = \\bigcup_{\\ell=1}^L \\mathcal{D}_{\\mathrm{test}}^{(\\ell)}$ and the posterior term does not depend on $\\ell$. It is an in-sample evaluation deceptively written in a cross-validation format.\n**Verdict: Incorrect**",
            "answer": "$$\\boxed{AB}$$"
        }
    ]
}