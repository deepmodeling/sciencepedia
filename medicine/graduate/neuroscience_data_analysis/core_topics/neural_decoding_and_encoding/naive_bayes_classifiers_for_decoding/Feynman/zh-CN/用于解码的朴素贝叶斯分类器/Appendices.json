{
    "hands_on_practices": [
        {
            "introduction": "为了打下坚实的基础，我们首先剖析朴素贝叶斯分类器的结构。这项练习要求你精确推导当移除单个神经元时，模型的决策变量——对数后验比（log-posterior odds）——会如何变化。完成此推导将阐明每个神经元如何提供独立的证据，这些证据在对数域中相加，共同构成最终的决策 。",
            "id": "4180807",
            "problem": "考虑一个在持续时间为 $T$ 的固定观察窗口内，使用朴素贝叶斯分类器对刺激 $A$ 和 $B$ 进行的两类解码任务。您记录了来自 $N$ 个神经元的脉冲计数，在窗口内收集到计数向量 $\\mathbf{x} = (x_1, x_2, \\dots, x_N)$。假设在给定刺激的情况下，神经元是条件独立的，并将每个神经元的脉冲计数建模为泊松随机变量，其在窗口内的平均速率特定于刺激：对于神经元 $i$，在刺激 $s \\in \\{A,B\\}$ 下，$x_i \\sim \\mathrm{Poisson}(\\lambda_{i s})$，其中 $\\lambda_{i s} > 0$ 表示在刺激 $s$ 下窗口内的期望脉冲计数。设先验概率为 $\\pi_A = P(S=A)$ 和 $\\pi_B = P(S=B)$，且 $\\pi_A + \\pi_B = 1$。定义对数后验几率为\n$$\n\\mathrm{LPO}(\\mathbf{x}) = \\ln\\!\\left(\\frac{P(S=A \\mid \\mathbf{x})}{P(S=B \\mid \\mathbf{x})}\\right).\n$$\n现在，您从解码器中移除单个神经元 $j$，形成简化的计数向量 $\\mathbf{x}_{-j} = (x_1, \\dots, x_{j-1}, x_{j+1}, \\dots, x_N)$，并在相同的假设和先验下重新计算对数后验几率 $\\mathrm{LPO}(\\mathbf{x}_{-j})$。从贝叶斯定理和条件独立模型严格出发，推导由下式定义的对数后验几率变化量的精确、封闭形式的表达式\n$$\n\\Delta \\mathrm{LPO}_j = \\mathrm{LPO}(\\mathbf{x}_{-j}) - \\mathrm{LPO}(\\mathbf{x}),\n$$\n并将其纯粹表示为单个神经元观测值 $x_j$ 和两个特定于刺激的泊松均值 $\\lambda_{jA}$ 与 $\\lambda_{jB}$ 的函数。您的最终答案必须是单个解析表达式。不要进行近似或四舍五入。",
            "solution": "该问题基于计算神经科学中的标准模型，是良定的且科学上合理的。我们可以开始推导。\n\n目标是推导对数后验几率变化量的表达式 $\\Delta \\mathrm{LPO}_j = \\mathrm{LPO}(\\mathbf{x}_{-j}) - \\mathrm{LPO}(\\mathbf{x})$。我们首先定义完整数据集的对数后验几率 $\\mathrm{LPO}(\\mathbf{x})$。\n\n根据定义，对数后验几率为：\n$$\n\\mathrm{LPO}(\\mathbf{x}) = \\ln\\left(\\frac{P(S=A \\mid \\mathbf{x})}{P(S=B \\mid \\mathbf{x})}\\right)\n$$\n我们将贝叶斯定理应用于后验概率 $P(S=s \\mid \\mathbf{x}) = \\frac{P(\\mathbf{x} \\mid S=s)P(S=s)}{P(\\mathbf{x})}$，其中 $s \\in \\{A,B\\}$。将它们代入比率中，证据项 $P(\\mathbf{x})$ 被消去：\n$$\n\\frac{P(S=A \\mid \\mathbf{x})}{P(S=B \\mid \\mathbf{x})} = \\frac{P(\\mathbf{x} \\mid S=A)P(S=A)}{P(\\mathbf{x} \\mid S=B)P(S=B)}\n$$\n对两边取自然对数，得到：\n$$\n\\mathrm{LPO}(\\mathbf{x}) = \\ln\\left(\\frac{P(S=A)}{P(S=B)}\\right) + \\ln\\left(\\frac{P(\\mathbf{x} \\mid S=A)}{P(\\mathbf{x} \\mid S=B)}\\right)\n$$\n第一项是对数先验几率，我们可以使用给定的先验 $\\pi_A$ 和 $\\pi_B$ 来表示：\n$$\n\\ln\\left(\\frac{\\pi_A}{\\pi_B}\\right)\n$$\n第二项是对数似然比。我们使用条件独立性假设，该假设指出联合似然是每个神经元各自似然的乘积：\n$$\nP(\\mathbf{x} \\mid S=s) = \\prod_{i=1}^{N} P(x_i \\mid S=s)\n$$\n于是，对数似然比变成了各个对数似然比之和：\n$$\n\\ln\\left(\\frac{P(\\mathbf{x} \\mid S=A)}{P(\\mathbf{x} \\mid S=B)}\\right) = \\ln\\left(\\frac{\\prod_{i=1}^{N} P(x_i \\mid S=A)}{\\prod_{i=1}^{N} P(x_i \\mid S=B)}\\right) = \\sum_{i=1}^{N} \\ln\\left(\\frac{P(x_i \\mid S=A)}{P(x_i \\mid S=B)}\\right)\n$$\n现在，我们引入脉冲计数的泊松模型。泊松随机变量的概率质量函数为 $P(x_i \\mid S=s) = \\frac{\\lambda_{is}^{x_i} \\exp(-\\lambda_{is})}{x_i!}$。将其代入每个神经元的对数似然比中：\n$$\n\\ln\\left(\\frac{P(x_i \\mid S=A)}{P(x_i \\mid S=B)}\\right) = \\ln\\left(\\frac{\\lambda_{iA}^{x_i} \\exp(-\\lambda_{iA}) / x_i!}{\\lambda_{iB}^{x_i} \\exp(-\\lambda_{iB}) / x_i!}\\right) = \\ln\\left(\\frac{\\lambda_{iA}^{x_i} \\exp(-\\lambda_{iA})}{\\lambda_{iB}^{x_i} \\exp(-\\lambda_{iB})}\\right)\n$$\n使用对数的性质 $\\ln(a/b) = \\ln(a) - \\ln(b)$ 和 $\\ln(a^b) = b\\ln(a)$：\n$$\n\\ln\\left(\\frac{P(x_i \\mid S=A)}{P(x_i \\mid S=B)}\\right) = (x_i \\ln(\\lambda_{iA}) - \\lambda_{iA}) - (x_i \\ln(\\lambda_{iB}) - \\lambda_{iB}) = x_i \\ln\\left(\\frac{\\lambda_{iA}}{\\lambda_{iB}}\\right) - (\\lambda_{iA} - \\lambda_{iB})\n$$\n综合所有项，完整的对数后验几率为：\n$$\n\\mathrm{LPO}(\\mathbf{x}) = \\ln\\left(\\frac{\\pi_A}{\\pi_B}\\right) + \\sum_{i=1}^{N} \\left[ x_i \\ln\\left(\\frac{\\lambda_{iA}}{\\lambda_{iB}}\\right) - (\\lambda_{iA} - \\lambda_{iB}) \\right]\n$$\n该表达式表明，总对数后验几率是一个恒定的先验项与来自 $N$ 个神经元中每一个的单独贡献之和。\n\n接下来，我们计算简化数据集的对数后验几率 $\\mathrm{LPO}(\\mathbf{x}_{-j})$。这个计算过程是相同的，但求和是针对除神经元 $j$ 之外的所有神经元集合：\n$$\n\\mathrm{LPO}(\\mathbf{x}_{-j}) = \\ln\\left(\\frac{\\pi_A}{\\pi_B}\\right) + \\sum_{i \\neq j} \\left[ x_i \\ln\\left(\\frac{\\lambda_{iA}}{\\lambda_{iB}}\\right) - (\\lambda_{iA} - \\lambda_{iB}) \\right]\n$$\n最后，我们计算所求的变化量 $\\Delta \\mathrm{LPO}_j = \\mathrm{LPO}(\\mathbf{x}_{-j}) - \\mathrm{LPO}(\\mathbf{x})$。\n我们可以将 $\\mathrm{LPO}(\\mathbf{x})$ 表达式中的求和项分为两部分：神经元 $j$ 的项和所有其他神经元的求和项。\n$$\n\\mathrm{LPO}(\\mathbf{x}) = \\ln\\left(\\frac{\\pi_A}{\\pi_B}\\right) + \\sum_{i \\neq j} \\left[ x_i \\ln\\left(\\frac{\\lambda_{iA}}{\\lambda_{iB}}\\right) - (\\lambda_{iA} - \\lambda_{iB}) \\right] + \\left[ x_j \\ln\\left(\\frac{\\lambda_{jA}}{\\lambda_{jB}}\\right) - (\\lambda_{jA} - \\lambda_{jB}) \\right]\n$$\n注意，此表达式中的前两项与 $\\mathrm{LPO}(\\mathbf{x}_{-j})$ 的表达式完全相同。\n$$\n\\mathrm{LPO}(\\mathbf{x}) = \\mathrm{LPO}(\\mathbf{x}_{-j}) + \\left[ x_j \\ln\\left(\\frac{\\lambda_{jA}}{\\lambda_{jB}}\\right) - (\\lambda_{jA} - \\lambda_{jB}) \\right]\n$$\n重新整理此方程以求解 $\\Delta \\mathrm{LPO}_j$：\n$$\n\\Delta \\mathrm{LPO}_j = \\mathrm{LPO}(\\mathbf{x}_{-j}) - \\mathrm{LPO}(\\mathbf{x}) = - \\left[ x_j \\ln\\left(\\frac{\\lambda_{jA}}{\\lambda_{jB}}\\right) - (\\lambda_{jA} - \\lambda_{jB}) \\right]\n$$\n化简该表达式得到最终结果：\n$$\n\\Delta \\mathrm{LPO}_j = (\\lambda_{jA} - \\lambda_{jB}) - x_j \\ln\\left(\\frac{\\lambda_{jA}}{\\lambda_{jB}}\\right)\n$$\n这表示当神经元 $j$ 被移除时对数后验几率的变化量。正如所要求的，它仅表示为该神经元的观测值 $x_j$ 及其特定于刺激的平均脉冲计数 $\\lambda_{jA}$ 和 $\\lambda_{jB}$ 的函数。",
            "answer": "$$\n\\boxed{(\\lambda_{jA} - \\lambda_{jB}) - x_j \\ln\\left(\\frac{\\lambda_{jA}}{\\lambda_{jB}}\\right)}\n$$"
        },
        {
            "introduction": "从理论转向实践，我们需要应对实施概率模型时的一个关键挑战：数值稳定性。这个编程练习将指导你通过实现“log-sum-exp”技巧来稳健地计算对数证据项 $\\log p(\\mathbf{r})$，该项在朴素的实现中容易因数值下溢而出错。掌握这项技术对于构建能够处理高维神经数据的可靠解码器至关重要 。",
            "id": "4180768",
            "problem": "您正在使用朴素贝叶斯分类器分析来自神经元群体响应的刺激解码。请从以下基础开始：贝叶斯定理和朴素贝叶斯独立性假设。具体来说，对于由 $s$ 索引的离散刺激集和跨越 $I$ 个神经元的神经响应向量 $\\mathbf{r}=(r_1,\\dots,r_I)$，通过贝叶斯定理定义后验概率、证据以及朴素贝叶斯分解如下：$p(s\\mid \\mathbf{r})=\\dfrac{p(\\mathbf{r}\\mid s)p(s)}{p(\\mathbf{r})}$，$p(\\mathbf{r})=\\sum_s p(\\mathbf{r}\\mid s)p(s)$ 和 $p(\\mathbf{r}\\mid s)=\\prod_{i=1}^I p(r_i\\mid s)$。计算任务是在给定每个类别每个神经元的对数似然和对数先验的情况下，评估对数证据 $\\log p(\\mathbf{r})$，同时确保在神经元数量 $I$ 很大或对数似然为非常小的负数时，计算过程具有数值鲁棒性。\n\n您的程序必须通过对 $\\sum_s \\exp\\!\\left(\\sum_{i=1}^I \\log p(r_i\\mid s)+\\log p(s)\\right)$ 应用 log-sum-exp 变换，来实现 $\\log p(\\mathbf{r})$ 的数值稳定计算。具体来说，您必须从输入 $L_{s,i}=\\log p(r_i\\mid s)$ 和 $\\ell_s=\\log p(s)$ 计算 $\\log p(\\mathbf{r})$，首先对每个类别 $s$ 形成 $a_s=\\sum_{i=1}^I L_{s,i}+\\ell_s$，然后通过数值稳定的 log-sum-exp 变换聚合这些 $a_s$ 值。您的实现必须能够处理 $a_s$ 中的 $-\\infty$ 值（代表某个类别的概率为零），并在所有类别都不可能时返回 $-\\infty$。\n\n请为以下测试套件实现该计算。在每种情况下，$L$ 是一个列表的列表，包含 $s=0,\\dots,S-1$ 和 $i=1,\\dots,I$ 的 $L_{s,i}$ 值；$\\ell$ 是 $s=0,\\dots,S-1$ 的 $\\ell_s=\\log p(s)$ 值的列表。所有对数均为自然对数。对于每种情况，输出是代表 $\\log p(\\mathbf{r})$ 的单个实数。最终输出必须是单行，包含一个方括号内的逗号分隔列表，按所列顺序汇总所有情况的结果。不涉及物理单位；所有输出均为实数。\n\n情况 $1$（理想路径，中等数值）：\n- $S=3$， $I=5$。\n- $L$：\n  - 类别 $0$：$[-2.3,-0.7,-1.1,-0.5,-3.0]$，\n  - 类别 $1$：$[-1.9,-1.2,-0.9,-1.5,-2.2]$，\n  - 类别 $2$：$[-3.2,-0.6,-0.8,-0.4,-2.8]$。\n- $\\ell$：$[\\log(0.2),\\log(0.3),\\log(0.5)] = [-1.6094379124341003,-1.2039728043259361,-0.6931471805599453]$。\n\n情况 $2$（极端下溢风险，非常小的负数和）：\n- $S=2$， $I=3$。\n- $L$：\n  - 类别 $0$：$[-1000.0,-800.0,-1200.0]$，\n  - 类别 $1$：$[-1001.0,-799.0,-1199.0]$。\n- $\\ell$：$[\\log(0.5),\\log(0.5)] = [-0.6931471805599453,-0.6931471805599453]$。\n\n情况 $3$（单类别边界情况）：\n- $S=1$， $I=4$。\n- $L$：\n  - 类别 $0$：$[-0.2,-0.4,-0.1,-0.3]$。\n- $\\ell$：$[\\log(1.0)] = [0.0]$。\n\n情况 $4$（具有相等聚合分数的平衡类别）：\n- $S=2$， $I=2$。\n- $L$：\n  - 类别 $0$：$[-0.5,-1.0]$，\n  - 类别 $1$：$[-0.5,-1.0]$。\n- $\\ell$：$[\\log(0.5),\\log(0.5)] = [-0.6931471805599453,-0.6931471805599453]$。\n\n情况 $5$（一个类别不可能，一个类别有限）：\n- $S=2$， $I=3$。\n- $L$：\n  - 类别 $0$：$[-\\infty,-0.5,-0.5]$，\n  - 类别 $1$：$[-0.4,-0.6,-0.3]$。\n- $\\ell$：$[\\log(0.6),\\log(0.4)] = [-0.5108256237659907,-0.916290731874155]$。\n\n情况 $6$（所有类别都不可能）：\n- $S=2$， $I=2$。\n- $L$：\n  - 类别 $0$：$[-\\infty,-0.2]$，\n  - 类别 $1$：$[-\\infty,-0.3]$。\n- $\\ell$：$[\\log(0.7),\\log(0.3)] = [-0.35667494393873245,-1.2039728043259361]$。\n\n您的程序应生成单行输出，其中包含一个方括号内的逗号分隔列表的结果（例如，$[\\text{result1},\\text{result2},\\dots]$）。结果必须严格按照上述情况 1 到 6 的顺序排列。每个结果都必须是一个实数（浮点数），当数学上保证时，结果为 $-\\infty$ 是可接受的。",
            "solution": "该问题有效。它提出了一个在计算神经科学和机器学习领域中定义明确且有科学依据的任务，即以数值稳定的方式计算朴素贝叶斯分类器的对数证据。所有需要的数据和定义均已提供，问题没有矛盾或歧义。\n\n目标是针对给定的神经响应向量 $\\mathbf{r}$ 和离散刺激集 $\\{s\\}$，计算对数证据 $\\log p(\\mathbf{r})$。证据 $p(\\mathbf{r})$ 是观测到响应 $\\mathbf{r}$ 的边缘概率，它通过对所有可能的刺激进行边缘化来定义：\n$$p(\\mathbf{r}) = \\sum_s p(\\mathbf{r}, s)$$\n使用联合概率的定义，可以写成：\n$$p(\\mathbf{r}) = \\sum_s p(\\mathbf{r}\\mid s)p(s)$$\n此处，$p(s)$ 是刺激 $s$ 的先验概率，$p(\\mathbf{r}\\mid s)$ 是在呈现刺激 $s$ 的条件下观测到响应 $\\mathbf{r}$ 的似然。\n\n问题指定使用朴素贝叶斯模型。该模型假设，在给定刺激 $s$ 的条件下，单个神经元 $r_i$ 的响应是相互独立的。对于具有响应向量 $\\mathbf{r}=(r_1, \\dots, r_I)$ 的 $I$ 个神经元群体，该假设可表述为：\n$$p(\\mathbf{r}\\mid s) = \\prod_{i=1}^I p(r_i\\mid s)$$\n将此代入证据的表达式中，得到：\n$$p(\\mathbf{r}) = \\sum_s \\left( p(s) \\prod_{i=1}^I p(r_i\\mid s) \\right)$$\n直接计算该表达式在数值上是危险的。对于大量的神经元 $I$，即使单个概率 $p(r_i\\mid s)$ 不是非常小，乘积项 $\\prod_{i=1}^I p(r_i\\mid s)$ 也很容易下溢为零。为避免此问题，我们在对数域中执行计算。\n\n所需的量是对数证据 $\\log p(\\mathbf{r})$：\n$$\\log p(\\mathbf{r}) = \\log\\left( \\sum_s \\left( p(s) \\prod_{i=1}^I p(r_i\\mid s) \\right) \\right)$$\n为了处理对数的和而不是概率的积，我们可以使用指数函数来表示求和中的每一项，即 $x = \\exp(\\log x)$：\n$$\\log p(\\mathbf{r}) = \\log\\left( \\sum_s \\exp\\left( \\log\\left( p(s) \\prod_{i=1}^I p(r_i\\mid s) \\right) \\right) \\right)$$\n使用对数的性质 $\\log(ab) = \\log a + \\log b$ 和 $\\log(\\prod_i a_i) = \\sum_i \\log a_i$，指数内的项变为：\n$$\\log\\left( p(s) \\prod_{i=1}^I p(r_i\\mid s) \\right) = \\log p(s) + \\sum_{i=1}^I \\log p(r_i\\mid s)$$\n问题将这些对数量作为输入提供：每个神经元的对数似然 $L_{s,i} = \\log p(r_i\\mid s)$ 和对数先验 $\\ell_s = \\log p(s)$。我们定义 $a_s$ 为每个类别 $s$ 的对数联合概率项：\n$$a_s = \\ell_s + \\sum_{i=1}^I L_{s,i}$$\n将此代回，对数证据为：\n$$\\log p(\\mathbf{r}) = \\log\\left( \\sum_s \\exp(a_s) \\right)$$\n此表达式被称为 log-sum-exp 函数，即 $\\text{LSE}(a_1, a_2, \\dots)$。一个朴素的实现方法是先为每个 $s$ 计算 $\\exp(a_s)$，然后将结果相加，但这种方法仍然容易出现数值问题。如果任何一个 $a_s$ 是一个大的正数，$\\exp(a_s)$ 可能会上溢。如果所有的 $a_s$ 都是大的负数，那么对所有的 $s$，$\\exp(a_s)$ 都可能下溢为零，导致最终结果为 $\\log(0) = -\\infty$ 并损失所有精度。\n\n为了确保数值稳定性，我们使用“log-sum-exp 技巧”。令 $a_{\\max} = \\max_s \\{a_s\\}$。我们可以从和中提出因子 $\\exp(a_{\\max})$：\n$$\\log p(\\mathbf{r}) = \\log\\left( \\exp(a_{\\max}) \\sum_s \\exp(a_s - a_{\\max}) \\right)$$\n$$= a_{\\max} + \\log\\left( \\sum_s \\exp(a_s - a_{\\max}) \\right)$$\n这个公式在数值上是鲁棒的。指数函数的参数 $(a_s - a_{\\max})$ 现在都小于或等于 0。这可以防止上溢，因为当 $x \\le 0$ 时，$\\exp(x)$ 的最大值为 1。它还确保了对和贡献最大的项能够以高精度计算。\n\n这种方法还必须正确处理涉及 $-\\infty$ 的特殊情况。输入值 $L_{s,i} = -\\infty$ 或 $\\ell_s = -\\infty$ 表示一个零概率事件。它们的和 $a_s$ 也将是 $-\\infty$。这样一个类别对证据的贡献是 $\\exp(-\\infty) = 0$，因此这些项会正确地从和中消失。如果所有类别都不可能，即所有的 $a_s = -\\infty$，那么总证据为 0，对数证据为 $\\log(0) = -\\infty$。一个鲁棒的实现，例如 `scipy.special` 模块中可用的 `logsumexp` 函数，可以正确处理这些边界情况。\n\n每个测试用例的算法如下：\n$1$. 对于从 $0$ 到 $S-1$ 的每个类别 $s$，计算 $a_s = \\ell_s + \\sum_{i=1}^I L_{s,i}$。\n$2$. 通过对包含所有 $a_s$ 值的向量应用数值稳定的 log-sum-exp 运算来计算对数证据。\n此过程将应用于所提供的每个测试用例。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import logsumexp\n\ndef solve():\n    \"\"\"\n    Solves the problem of computing the log-evidence for a Naive Bayes classifier\n    for a suite of test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"L\": [\n                [-2.3, -0.7, -1.1, -0.5, -3.0],\n                [-1.9, -1.2, -0.9, -1.5, -2.2],\n                [-3.2, -0.6, -0.8, -0.4, -2.8]\n            ],\n            \"ell\": [np.log(0.2), np.log(0.3), np.log(0.5)]\n        },\n        {\n            \"L\": [\n                [-1000.0, -800.0, -1200.0],\n                [-1001.0, -799.0, -1199.0]\n            ],\n            \"ell\": [np.log(0.5), np.log(0.5)]\n        },\n        {\n            \"L\": [\n                [-0.2, -0.4, -0.1, -0.3]\n            ],\n            \"ell\": [np.log(1.0)]\n        },\n        {\n            \"L\": [\n                [-0.5, -1.0],\n                [-0.5, -1.0]\n            ],\n            \"ell\": [np.log(0.5), np.log(0.5)]\n        },\n        {\n            \"L\": [\n                [-np.inf, -0.5, -0.5],\n                [-0.4, -0.6, -0.3]\n            ],\n            \"ell\": [np.log(0.6), np.log(0.4)]\n        },\n        {\n            \"L\": [\n                [-np.inf, -0.2],\n                [-np.inf, -0.3]\n            ],\n            \"ell\": [np.log(0.7), np.log(0.3)]\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        # Convert inputs to NumPy arrays for vectorized operations.\n        # L_s,i = log p(r_i|s)\n        # l_s = log p(s)\n        L = np.array(case[\"L\"], dtype=np.float64)\n        ell = np.array(case[\"ell\"], dtype=np.float64)\n\n        # Step 1: Compute a_s for each class s.\n        # a_s = log p(s) + sum_i log p(r_i|s)\n        # This is the log of the joint probability, log p(r, s).\n        # The sum is over neurons (axis=1 of L).\n        a = L.sum(axis=1) + ell\n\n        # Step 2: Compute log-evidence using the log-sum-exp transformation.\n        # log p(r) = log(sum_s exp(a_s))\n        # The scipy.special.logsumexp function provides a numerically stable\n        # implementation that handles -inf correctly.\n        log_evidence = logsumexp(a)\n        \n        results.append(log_evidence)\n\n    # The str() function appropriately formats floating-point numbers, including -inf.\n    # The final output must be a single line containing a comma-separated list\n    # in square brackets.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "一个稳健的解码器必须能够优雅地处理有限和稀疏数据带来的现实问题。这项练习探讨了“零频率问题”，即训练数据中未观测到的事件被赋予零概率，导致分类器行为变得脆弱。你将探索其发生的原因，并为一个有原则的贝叶斯解决方案——伪计数平滑——提供理据，从而加深你对共轭先验和偏差-方差权衡的理解 。",
            "id": "4180846",
            "problem": "考虑使用朴素贝叶斯分类器从群体神经元响应中解码离散刺激。令刺激类别由 $y \\in \\{s_1, s_2\\}$ 表示，响应特征为来自 $M$ 个神经元的脉冲计数分箱，$\\mathbf{x} = (x_1, \\dots, x_M)$，其中每个 $x_j \\in \\{1, \\dots, K\\}$ 索引 $K$ 个分箱之一。朴素贝叶斯独立性假设设定 $p(\\mathbf{x} \\mid y) = \\prod_{j=1}^{M} p(x_j \\mid y)$，解码过程通过贝叶斯法则计算 $p(y \\mid \\mathbf{x})$，即 $p(y \\mid \\mathbf{x}) \\propto p(y)\\,p(\\mathbf{x} \\mid y)$。\n\n你通过来自带标签训练数据的相对频率来估计每个类条件分类分布 $p(x_j \\mid y)$：对于每个类别 $y$、神经元 $j$ 和分箱 $b \\in \\{1,\\dots,K\\}$，令 $N_{yjb}$ 为类别 $y$ 下 $x_j=b$ 的训练试验次数，而 $N_{yj\\cdot} = \\sum_{b=1}^{K} N_{yjb}$ 是类别 $y$ 下神经元 $j$ 的总试验次数。假设训练数据是有限且稀疏的，以至于对于某些类条件概率，一些分箱未被观测到，例如 $N_{s_1,1,2} = 0$ 而 $N_{s_1,1\\cdot} > 0$ 且 $K \\geq 2$。\n\n为了数值稳定性，解码实现中使用概率的对数，即通过 $\\log p(y) + \\sum_{j=1}^{M} \\log p(x_j \\mid y)$ 计算 $\\log p(y \\mid \\mathbf{x})$（相差一个常数）。从第一性原理出发，分析零经验似然的后果，并论证引入基于共轭先验的伪计数平滑的合理性。\n\n在这种情况下，下列哪些陈述是正确的？\n\nA. 在 $K$ 个分箱上的分类分布使用浓度为 $\\alpha > 0$ 的对称 Dirichlet 先验时，分箱概率的平滑估计为 $\\hat{p}(x_j=b \\mid y) = \\dfrac{N_{yjb} + \\alpha}{N_{yj\\cdot} + K\\alpha}$，该值对每个分箱 $b$ 都是严格为正的，从而确保了对数计算的有限性。\n\nB. 在对数空间中，零似然会贡献一个常数偏移量，在比较 $\\log p(y \\mid \\mathbf{x})$ 时该偏移量会在不同类别之间抵消，因此无需任何平滑，解码仍然是良定义且有限的。\n\nC. 选择 $\\alpha = 0$ 可以消除先验引起的偏差，同时保证后验预测分布为所有分箱分配非零概率。\n\nD. 如果任何类条件项满足 $p(x_j=b \\mid y) = 0$ 且观测值为 $x_j=b$，则 $p(\\mathbf{x} \\mid y) = 0$ 且 $\\log p(\\mathbf{x} \\mid y) = -\\infty$，这意味着 $\\log p(y \\mid \\mathbf{x}) = -\\infty$（相差来自先验和证据的有限加性常数）。\n\nE. 基于伪计数的平滑通过为每个分箱增加正质量来修改估计值，并且在归一化之后，保留了对于每个神经元 $j$ 和类别 $y$，$\\sum_{b=1}^{K} p(x_j=b \\mid y) = 1$ 的性质。\n\nF. 相对于 $\\alpha = 0$，在对称 Dirichlet 先验中取 $\\alpha > 0$ 会降低估计量的方差，但代价是使估计值偏向于分箱上的均匀分布，这反映了偏差-方差权衡。\n\n选择所有适用项。从贝叶斯法则、朴素贝叶斯独立性假设以及 Dirichlet 先验与分类（多项式）模型的共轭性出发提供推理；不要诉诸于启发式捷径。",
            "solution": "问题陈述在科学上是合理的、良构的、客观的且内部一致的。它描述了计算统计学和机器学习应用于神经科学中的一个标准场景：从稀疏数据为朴素贝叶斯分类器估计概率时出现的零频率事件问题。该问题是有效的。\n\n问题的核心在于类条件概率 $p(x_j \\mid y)$ 的估计。使用原始相对频率，即最大似然估计（MLE），特定特征值（分箱 $b$）的概率估计为：\n$$\n\\hat{p}_{MLE}(x_j=b \\mid y) = \\frac{N_{yjb}}{N_{yj\\cdot}}\n$$\n如果训练数据是稀疏的，那么对于给定的类别 $y$、神经元 $j$ 和分箱 $b$，可能没有观测到任何试验，即 $N_{yjb} = 0$。这导致 $\\hat{p}_{MLE}(x_j=b \\mid y) = 0$。如果一个待解码的新观测值 $\\mathbf{x}$ 具有特征 $x_j=b$，由于朴素贝叶斯假设，类别 $y$ 的整个类条件似然会坍缩为零：\n$$\np(\\mathbf{x} \\mid y) = \\prod_{i=1}^{M} p(x_i \\mid y) = 0\n$$\n因为乘积中的一项为零。因此，后验概率 $p(y \\mid \\mathbf{x})$ 也变为零（假设证据 $p(\\mathbf{x}) > 0$），这实际上是仅根据一个未见过的特征值就排除了类别 $y$，而不管来自其他 $M-1$ 个神经元的证据如何。这是一种极端且不希望出现的行为。在对数空间中，这对应于一个 $\\log(0)$ 项，其值为 $-\\infty$，使得计算在数值上不稳定且存在问题。\n\n标准的解决方案是使用贝叶斯估计框架。对于分类分布（或其母分布，多项式分布）的参数，共轭先验是 Dirichlet 分布。对于 $K$ 个分箱，概率向量 $\\mathbf{p}_{yj} = (p(x_j=1|y), \\dots, p(x_j=K|y))$ 的先验是 $p(\\mathbf{p}_{yj}) \\sim \\text{Dir}(\\boldsymbol{\\alpha})$，其中 $\\boldsymbol{\\alpha} = (\\alpha_1, \\dots, \\alpha_K)$ 是浓度参数。\n\n在观测到计数 $\\mathbf{N}_{yj} = (N_{yj1}, \\dots, N_{yjK})$ 之后，后验分布也是一个 Dirichlet 分布：\n$$\np(\\mathbf{p}_{yj} \\mid \\mathbf{N}_{yj}) \\sim \\text{Dir}(\\alpha_1 + N_{yj1}, \\dots, \\alpha_K + N_{yjK})\n$$\n单个分箱 $b$ 的概率的贝叶斯点估计是后验分布的期望值，称为后验预测概率。对于 Dirichlet 分布的第 $b$ 个分量，其值为：\n$$\n\\hat{p}(x_j=b \\mid y) = \\frac{\\alpha_b + N_{yjb}}{\\sum_{k=1}^K (\\alpha_k + N_{yjk})} = \\frac{N_{yjb} + \\alpha_b}{N_{yj\\cdot} + \\sum_{k=1}^K \\alpha_k}\n$$\n问题指定了一个对称 Dirichlet 先验，其中所有的 $\\alpha_k$ 都等于同一个值 $\\alpha$。在这种情况下，$\\sum_{k=1}^K \\alpha_k = K\\alpha$。公式简化为：\n$$\n\\hat{p}(x_j=b \\mid y) = \\frac{N_{yjb} + \\alpha}{N_{yj\\cdot} + K\\alpha}\n$$\n该技术被称为加 $\\alpha$ 平滑、Lidstone 平滑，或者当 $\\alpha=1$ 时称为拉普拉斯平滑。参数 $\\alpha$ 充当添加到每个分箱的“伪计数”。\n\n现在，我们根据这些原则评估每个选项。\n\nA. **在 $K$ 个分箱上的分类分布使用浓度为 $\\alpha > 0$ 的对称 Dirichlet 先验时，分箱概率的平滑估计为 $\\hat{p}(x_j=b \\mid y) = \\dfrac{N_{yjb} + \\alpha}{N_{yj\\cdot} + K\\alpha}$，该值对每个分箱 $b$ 都是严格为正的，从而确保了对数计算的有限性。**\n在对称 Dirichlet 先验下推导出的平滑估计公式确实是 $\\hat{p}(x_j=b \\mid y) = \\frac{N_{yjb} + \\alpha}{N_{yj\\cdot} + K\\alpha}$。由于计数 $N_{yjb}$ 是非负整数（$N_{yjb} \\ge 0$），且问题陈述 $\\alpha > 0$，分子 $N_{yjb} + \\alpha$ 总是严格为正的。类似地，总计数 $N_{yj\\cdot}$ 是非负的（且对于所讨论的神经元类别假定为正），$K \\ge 2$ 且 $\\alpha > 0$，所以分母 $N_{yj\\cdot} + K\\alpha$ 也严格为正。因此，该比率是严格为正的。一个严格为正的概率具有有限的对数，从而避免了 $-\\infty$ 问题。\n**正确**。\n\nB. **在对数空间中，零似然会贡献一个常数偏移量，在比较 $\\log p(y \\mid \\mathbf{x})$ 时该偏移量会在不同类别之间抵消，因此无需任何平滑，解码仍然是良定义且有限的。**\n零似然 $p(x_j=b \\mid y) = 0$ 对应于对数似然 $\\log(0)$，其值为 $-\\infty$。这不是一个有限的常数偏移量。如果对于一个给定的观测值，一个类别的似然项为零，其总对数似然分数将变为 $-\\infty$，而另一个类别可能有一个有限的分数。结果不会抵消；它将第一个类别的后验概率推向零。由于涉及到 $-\\infty$，计算不是“有限的”。\n**不正确**。\n\nC. **选择 $\\alpha = 0$ 可以消除先验引起的偏差，同时保证后验预测分布为所有分箱分配非零概率。**\n在平滑估计公式中设置 $\\alpha = 0$ 会得到 $\\hat{p}(x_j=b \\mid y) = \\frac{N_{yjb} + 0}{N_{yj\\cdot} + K \\cdot 0} = \\frac{N_{yjb}}{N_{yj\\cdot}}$。这是 MLE，它没有由先验引入的偏差。然而，这个估计正是导致零频率问题的原因。如果一个分箱未被观测到（$N_{yjb}=0$），其估计概率为 $0$。这与陈述的第二部分相矛盾，该部分声称它保证了为所有分箱分配非零概率。\n**不正确**。\n\nD. **如果任何类条件项满足 $p(x_j=b \\mid y) = 0$ 且观测值为 $x_j=b$，则 $p(\\mathbf{x} \\mid y) = 0$ 且 $\\log p(\\mathbf{x} \\mid y) = -\\infty$，这意味着 $\\log p(y \\mid \\mathbf{x}) = -\\infty$（相差来自先验和证据的有限加性常数）。**\n给定一个观测值 $\\mathbf{x}$ 其中 $x_j=b$，类条件似然是 $p(\\mathbf{x} \\mid y) = \\prod_{i=1}^{M} p(x_i \\mid y)$。如果 $p(x_j=b \\mid y)=0$，乘积中的一项为零，使得整个乘积 $p(\\mathbf{x} \\mid y) = 0$。其对数为 $\\log(0) = -\\infty$。根据贝叶斯法则，后验概率是 $p(y \\mid \\mathbf{x}) = \\frac{p(\\mathbf{x} \\mid y) p(y)}{p(\\mathbf{x})}$。在对数空间中，这是 $\\log p(y \\mid \\mathbf{x}) = \\log p(\\mathbf{x} \\mid y) + \\log p(y) - \\log p(\\mathbf{x})$。将 $-\\infty$ 代入 $\\log p(\\mathbf{x} \\mid y)$，并假设对数先验 $\\log p(y)$ 和对数证据 $\\log p(\\mathbf{x})$ 为有限值（如果至少有另一个类别对此观测值具有非零似然，则此假设成立），对数后验概率变为 $-\\infty$。该陈述是对这一后果的准确描述。\n**正确**。\n\nE. **基于伪计数的平滑通过为每个分箱增加正质量来修改估计值，并且在归一化之后，保留了对于每个神经元 $j$ 和类别 $y$，$\\sum_{b=1}^{K} p(x_j=b \\mid y) = 1$ 的性质。**\n该陈述准确地描述了该过程。我们为每个分箱计数 $N_{yjb}$ 添加一个伪计数 $\\alpha > 0$。归一化因子是这些新“计数”的总和。让我们验证其和：\n$$\n\\sum_{b=1}^{K} \\hat{p}(x_j=b \\mid y) = \\sum_{b=1}^{K} \\frac{N_{yjb} + \\alpha}{N_{yj\\cdot} + K\\alpha} = \\frac{\\sum_{b=1}^{K}(N_{yjb} + \\alpha)}{N_{yj\\cdot} + K\\alpha}\n$$\n$$\n= \\frac{(\\sum_{b=1}^{K}N_{yjb}) + (\\sum_{b=1}^{K}\\alpha)}{N_{yj\\cdot} + K\\alpha} = \\frac{N_{yj\\cdot} + K\\alpha}{N_{yj\\cdot} + K\\alpha} = 1\n$$\n该过程确保了最终的估计值构成一个总和为 $1$ 的有效概率分布。\n**正确**。\n\nF. **相对于 $\\alpha = 0$，在对称 Dirichlet 先验中取 $\\alpha > 0$ 会降低估计量的方差，但代价是使估计值偏向于分箱上的均匀分布，这反映了偏差-方差权衡。**\n选择 $\\alpha=0$ 得到 MLE，它是在训练集上期望意义上的无偏估计量。选择 $\\alpha > 0$ 会引入偏差。平滑估计可以写成 MLE 和均匀分布 $1/K$ 的加权平均：\n$$\n\\hat{p}(x_j=b \\mid y) = \\frac{N_{yj\\cdot}}{N_{yj\\cdot} + K\\alpha} \\left(\\frac{N_{yjb}}{N_{yj\\cdot}}\\right) + \\frac{K\\alpha}{N_{yj\\cdot} + K\\alpha} \\left(\\frac{1}{K}\\right)\n$$\n这表明估计值被拉向或“偏向”于均匀分布。分箱概率 $p_b$ 的 MLE 方差是 $\\frac{p_b(1-p_b)}{N}$。平滑估计量的方差大约是 $\\frac{p_b(1-p_b)}{(N+K\\alpha)^2} \\times N$，这比 MLE 的方差小，因为有效样本数量从 $N$ 增加到 $N+K\\alpha$。因此，引入先验（取 $\\alpha > 0$）会以引入偏差为代价来降低方差。这是偏差-方差权衡的一个经典例子。\n**正确**。\n\n最终选择包括陈述 A、D、E 和 F。",
            "answer": "$$\\boxed{ADEF}$$"
        }
    ]
}