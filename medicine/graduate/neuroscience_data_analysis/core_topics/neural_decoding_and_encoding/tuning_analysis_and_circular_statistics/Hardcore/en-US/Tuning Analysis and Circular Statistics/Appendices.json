{
    "hands_on_practices": [
        {
            "introduction": "The first step in analyzing neuronal tuning is often to determine the stimulus feature to which a neuron responds most strongly. For circular data like head direction, this is the \"preferred direction.\" This practice grounds this fundamental calculation in the intuitive physics of vector addition, where each spike's direction is a vector and the preferred direction is the angle of their average. You will learn how to compute this circular mean and how to incorporate other biological signals, such as the phase of local field potential oscillations, as weights to refine your estimate. This exercise also introduces the essential concept of stability, using circular distance to quantify how consistent a neuron's tuning is across different recording sessions .",
            "id": "4202390",
            "problem": "You are given multiple recording sessions for a single neuron. Each session consists of a set of spike-phase pairs, where each pair contains a head direction angle and a local field potential theta phase at the spike time. Let each pair be denoted by $(\\theta_i, \\phi_i)$, with $\\theta_i \\in [0, 2\\pi)$ and $\\phi_i \\in [0, 2\\pi)$, both in radians. The goal is to estimate the preferred head direction for each session using a circular mean that incorporates spike-phase information as reliability weights, and then assess the stability of the preferred direction across sessions using a circular distance metric.\n\nStarting only from the fundamental definition that an angle $\\theta$ corresponds to a unit vector on the plane with Cartesian components $(\\cos\\theta, \\sin\\theta)$, and that a weighted average of vectors is given by the sum of each vector scaled by its nonnegative weight, derive the preferred head direction for a session as the argument of the weighted resultant vector formed from the head direction unit vectors. Use nonnegative weights defined from spike phases by a cosine modulation around a reference phase:\n$$\nw_i = 1 + \\alpha \\cos(\\phi_i - \\phi_{\\text{ref}}),\n$$\nwhere $0 \\le \\alpha < 1$ and $\\phi_{\\text{ref}} \\in [0, 2\\pi)$ are specified parameters. You must express the preferred head direction $\\hat{\\mu}$ as the argument of the weighted sum of unit vectors, and ensure $\\hat{\\mu} \\in [0, 2\\pi)$ in radians.\n\nTo assess stability across sessions, define the circular distance between two preferred directions $\\hat{\\mu}_1$ and $\\hat{\\mu}_2$ as the smallest absolute angular difference on the circle:\n$$\nd = |\\operatorname{wrap}(\\hat{\\mu}_1 - \\hat{\\mu}_2)|,\n$$\nwhere $\\operatorname{wrap}(\\Delta)$ maps any real $\\Delta$ to the unique representative in $(-\\pi, \\pi]$ that is congruent to $\\Delta$ modulo $2\\pi$. A pair of sessions is called stable if $d  \\tau$ for a given threshold $\\tau$ specified in radians.\n\nImplement a program that, for each provided test case, computes:\n- The preferred head direction $\\hat{\\mu}_1$ for session $1$ in radians;\n- The preferred head direction $\\hat{\\mu}_2$ for session $2$ in radians;\n- The circular distance $d$ in radians;\n- A boolean stability indicator given by whether $d  \\tau$.\n\nAll angles must be in radians. All float outputs must be rounded to $6$ decimal places. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is formatted as a list [$\\hat{\\mu}_1$, $\\hat{\\mu}_2$, $d$, stable].\n\nUse the following test suite, which exercises typical, boundary, and edge-case scenarios:\n\nTest Case $1$ (unweighted mean, moderate concentration):\n- Parameters: $\\alpha = 0.0$, $\\phi_{\\text{ref}} = 0.0$, $\\tau = 0.5$.\n- Session $1$ pairs $(\\theta_i, \\phi_i)$:\n  $(0.9, 2.2)$, $(1.1, 0.5)$, $(0.95, 4.7)$, $(1.2, 1.0)$, $(1.05, 3.3)$.\n- Session $2$ pairs $(\\theta_i, \\phi_i)$:\n  $(0.8, 3.1)$, $(1.0, 2.5)$, $(1.15, 3.8)$, $(0.95, 0.2)$, $(1.05, 5.4)$.\n\nTest Case $2$ (phase-weighted mean emphasizing $\\phi \\approx \\phi_{\\text{ref}}$):\n- Parameters: $\\alpha = 0.6$, $\\phi_{\\text{ref}} = 0.0$, $\\tau = 0.5$.\n- Session $1$ pairs $(\\theta_i, \\phi_i)$:\n  $(1.4, 0.1)$, $(1.5, 0.05)$, $(1.6, 0.2)$, $(0.5, 3.1)$, $(0.6, 3.0)$, $(0.4, 2.9)$, $(1.55, 0.0)$, $(0.45, 3.2)$.\n- Session $2$ pairs $(\\theta_i, \\phi_i)$:\n  $(1.65, 0.0)$, $(1.7, 0.1)$, $(1.75, 0.05)$, $(0.6, 3.0)$, $(0.5, 3.1)$, $(0.4, 3.2)$, $(1.68, 0.15)$, $(0.55, 2.9)$.\n\nTest Case $3$ (wrap-around boundary near $0$ and $2\\pi$):\n- Parameters: $\\alpha = 0.3$, $\\phi_{\\text{ref}} = \\pi$, $\\tau = 0.5$.\n- Session $1$ pairs $(\\theta_i, \\phi_i)$:\n  $(0.02, 3.10)$, $(0.04, 3.05)$, $(0.06, 3.20)$, $(6.27, 3.15)$.\n- Session $2$ pairs $(\\theta_i, \\phi_i)$:\n  $(6.22, 3.00)$, $(6.25, 3.05)$, $(0.01, 3.10)$, $(6.20, 3.00)$.\n\nTest Case $4$ (single-spike edge case):\n- Parameters: $\\alpha = 0.9$, $\\phi_{\\text{ref}} = 0.0$, $\\tau = 0.5$.\n- Session $1$ pairs $(\\theta_i, \\phi_i)$: $(2.0, 0.5)$.\n- Session $2$ pairs $(\\theta_i, \\phi_i)$: $(2.3, 1.0)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., `[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4]`), where each `text{result}_k` is a list [$\\hat{\\mu}_1$, $\\hat{\\mu}_2$, $d$, stable] with all angles in radians and floats rounded to $6$ decimal places.",
            "solution": "The user-provided problem is valid as it is scientifically grounded in circular statistics, well-posed with all necessary information, and free of contradictions or ambiguities. We shall now proceed with a complete solution.\n\nThe problem requires us to perform two main tasks for each test case, which involves two sessions of neuronal recordings: first, to compute the preferred head direction for each session, and second, to assess the stability of these preferred directions across the two sessions.\n\n### Part 1: Estimation of Preferred Head Direction $\\hat{\\mu}$\n\nThe problem states that we must begin from the fundamental definition of an angle $\\theta$ as a unit vector on the Cartesian plane, $(\\cos\\theta, \\sin\\theta)$. The preferred direction $\\hat{\\mu}$ for a session is defined as the argument of the weighted resultant vector formed from the head direction unit vectors.\n\nLet a session consist of $N$ spike-phase pairs $(\\theta_i, \\phi_i)$ for $i=1, \\dots, N$.\n\n**Step 1: Represent Head Directions as Unit Vectors**\nEach head direction angle $\\theta_i$ corresponds to a unit vector $\\vec{v}_i$ in the 2D Cartesian plane:\n$$\n\\vec{v}_i = \\begin{pmatrix} \\cos\\theta_i \\\\ \\sin\\theta_i \\end{pmatrix}\n$$\n\n**Step 2: Define Weights from Spike Phases**\nThe contribution of each vector $\\vec{v}_i$ is weighted by a value $w_i$ derived from the corresponding spike phase $\\phi_i$. The weight $w_i$ is given by a cosine modulation around a reference phase $\\phi_{\\text{ref}}$:\n$$\nw_i = 1 + \\alpha \\cos(\\phi_i - \\phi_{\\text{ref}})\n$$\nThe problem specifies that the parameter $\\alpha$ satisfies $0 \\le \\alpha  1$. Since the cosine function has a range of $[-1, 1]$, the weights $w_i$ are bounded by:\n$$\n1 + \\alpha(-1) \\le w_i \\le 1 + \\alpha(1) \\implies 1 - \\alpha \\le w_i \\le 1 + \\alpha\n$$\nGiven $\\alpha  1$, it follows that $1 - \\alpha  0$, ensuring all weights $w_i$ are strictly positive and thus well-defined for a weighted average.\n\n**Step 3: Compute the Weighted Resultant Vector**\nThe weighted resultant vector, $\\vec{R}$, is the sum of all unit vectors $\\vec{v}_i$ each scaled by its corresponding weight $w_i$:\n$$\n\\vec{R} = \\sum_{i=1}^{N} w_i \\vec{v}_i = \\sum_{i=1}^{N} w_i \\begin{pmatrix} \\cos\\theta_i \\\\ \\sin\\theta_i \\end{pmatrix} = \\begin{pmatrix} \\sum_{i=1}^{N} w_i \\cos\\theta_i \\\\ \\sum_{i=1}^{N} w_i \\sin\\theta_i \\end{pmatrix}\n$$\nLet the Cartesian components of $\\vec{R}$ be $R_x$ and $R_y$:\n$$\nR_x = \\sum_{i=1}^{N} w_i \\cos\\theta_i\n$$\n$$\nR_y = \\sum_{i=1}^{N} w_i \\sin\\theta_i\n$$\n\n**Step 4: Determine the Preferred Direction $\\hat{\\mu}$**\nThe preferred direction $\\hat{\\mu}$ is the angle (or argument) of the resultant vector $\\vec{R}$. This angle is determined using the two-argument arctangent function, $\\operatorname{atan2}(y, x)$, which correctly resolves the angle in all four quadrants. The result, $\\hat{\\mu}_{\\text{raw}}$, is typically in the interval $(-\\pi, \\pi]$.\n$$\n\\hat{\\mu}_{\\text{raw}} = \\operatorname{atan2}(R_y, R_x)\n$$\nThe problem requires the final angle $\\hat{\\mu}$ to be in the range $[0, 2\\pi)$. We can map $\\hat{\\mu}_{\\text{raw}}$ to this range by adding $2\\pi$ to any negative result:\n$$\n\\hat{\\mu} = \\begin{cases}\n\\hat{\\mu}_{\\text{raw}}  \\text{if } \\hat{\\mu}_{\\text{raw}} \\ge 0 \\\\\n\\hat{\\mu}_{\\text{raw}} + 2\\pi  \\text{if } \\hat{\\mu}_{\\text{raw}}  0\n\\end{cases}\n$$\nThis is equivalent to the modulo operation $(\\hat{\\mu}_{\\text{raw}} \\pmod{2\\pi})$ if the language's modulo operator handles negative numbers appropriately to produce a non-negative result. In Python/NumPy, `(mu_raw + 2*np.pi) % (2*np.pi)` achieves this mapping.\n\n### Part 2: Assessment of Stability Across Sessions\n\nStability is evaluated by comparing the preferred directions from two sessions, $\\hat{\\mu}_1$ and $\\hat{\\mu}_2$, using a circular distance metric.\n\n**Step 1: Define Circular Distance**\nThe circular distance $d$ is the smallest absolute angular difference on a circle. Given two angles $\\hat{\\mu}_1, \\hat{\\mu}_2 \\in [0, 2\\pi)$, their difference is $\\Delta = \\hat{\\mu}_1 - \\hat{\\mu}_2$. The circular distance is based on wrapping this difference to the interval $(-\\pi, \\pi]$.\nThe `wrap` function is defined as:\n$$\n\\operatorname{wrap}(\\Delta) = (\\Delta + \\pi) \\pmod{2\\pi} - \\pi\n$$\nThis operation maps any real number $\\Delta$ to its unique congruent value in $(-\\pi, \\pi]$. For example, if $\\Delta = 1.9\\pi$, $\\operatorname{wrap}(\\Delta)$ would be $-0.1\\pi$. If $\\Delta= -1.9\\pi$, $\\operatorname{wrap}(\\Delta)$ would be $0.1\\pi$.\n\nThe circular distance $d$ is the absolute value of this wrapped difference:\n$$\nd = |\\operatorname{wrap}(\\hat{\\mu}_1 - \\hat{\\mu}_2)|\n$$\nThis ensures $d \\in [0, \\pi]$.\n\n**Step 2: Apply Stability Criterion**\nA pair of sessions is deemed stable if their circular distance $d$ is less than a given threshold $\\tau$.\n$$\n\\text{stable} = (d  \\tau)\n$$\nThe result is a boolean value (`True` or `False`).\n\n### Summary of Algorithmic Steps for one Test Case\n1.  For each session (session 1 and session 2):\n    a. Extract the spike-phase pairs $(\\theta_i, \\phi_i)$ and parameters $\\alpha, \\phi_{\\text{ref}}$.\n    b. Compute the weights $w_i = 1 + \\alpha \\cos(\\phi_i - \\phi_{\\text{ref}})$ for all spikes.\n    c. Calculate the components of the resultant vector, $R_x = \\sum w_i \\cos\\theta_i$ and $R_y = \\sum w_i \\sin\\theta_i$.\n    d. Compute the preferred direction $\\hat{\\mu}_{\\text{raw}} = \\operatorname{atan2}(R_y, R_x)$ and map it to $[0, 2\\pi)$ to get $\\hat{\\mu}$.\n2.  Having computed $\\hat{\\mu}_1$ and $\\hat{\\mu}_2$:\n    a. Calculate the circular distance $d = |\\operatorname{wrap}(\\hat{\\mu}_1 - \\hat{\\mu}_2)|$.\n    b. Determine stability by comparing $d$ with the threshold $\\tau$.\n3.  Round all floating-point results ($\\hat{\\mu}_1, \\hat{\\mu}_2, d$) to $6$ decimal places and assemble the final list $[\\hat{\\mu}_1, \\hat{\\mu}_2, d, \\text{stable}]$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the neuroscience tuning analysis problem for a suite of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"alpha\": 0.0,\n            \"phi_ref\": 0.0,\n            \"tau\": 0.5,\n            \"session1_pairs\": np.array([\n                (0.9, 2.2), (1.1, 0.5), (0.95, 4.7), (1.2, 1.0), (1.05, 3.3)\n            ]),\n            \"session2_pairs\": np.array([\n                (0.8, 3.1), (1.0, 2.5), (1.15, 3.8), (0.95, 0.2), (1.05, 5.4)\n            ]),\n        },\n        {\n            \"alpha\": 0.6,\n            \"phi_ref\": 0.0,\n            \"tau\": 0.5,\n            \"session1_pairs\": np.array([\n                (1.4, 0.1), (1.5, 0.05), (1.6, 0.2), (0.5, 3.1), (0.6, 3.0),\n                (0.4, 2.9), (1.55, 0.0), (0.45, 3.2)\n            ]),\n            \"session2_pairs\": np.array([\n                (1.65, 0.0), (1.7, 0.1), (1.75, 0.05), (0.6, 3.0), (0.5, 3.1),\n                (0.4, 3.2), (1.68, 0.15), (0.55, 2.9)\n            ]),\n        },\n        {\n            \"alpha\": 0.3,\n            \"phi_ref\": np.pi,\n            \"tau\": 0.5,\n            \"session1_pairs\": np.array([\n                (0.02, 3.10), (0.04, 3.05), (0.06, 3.20), (6.27, 3.15)\n            ]),\n            \"session2_pairs\": np.array([\n                (6.22, 3.00), (6.25, 3.05), (0.01, 3.10), (6.20, 3.00)\n            ]),\n        },\n        {\n            \"alpha\": 0.9,\n            \"phi_ref\": 0.0,\n            \"tau\": 0.5,\n            \"session1_pairs\": np.array([(2.0, 0.5)]),\n            \"session2_pairs\": np.array([(2.3, 1.0)]),\n        },\n    ]\n\n    results = []\n\n    def compute_preferred_direction(pairs, alpha, phi_ref):\n        \"\"\"\n        Computes the weighted circular mean for a set of spike-phase pairs.\n        \n        Args:\n            pairs (np.ndarray): An array of (theta, phi) pairs.\n            alpha (float): The modulation strength parameter.\n            phi_ref (float): The reference phase.\n\n        Returns:\n            float: The preferred direction mu in [0, 2*pi).\n        \"\"\"\n        if pairs.shape[0] == 0:\n            return 0.0 # Or handle as an error, though not specified for this problem\n\n        thetas = pairs[:, 0]\n        phis = pairs[:, 1]\n\n        # Calculate weights based on phase\n        weights = 1 + alpha * np.cos(phis - phi_ref)\n\n        # Calculate components of the weighted resultant vector\n        Rx = np.sum(weights * np.cos(thetas))\n        Ry = np.sum(weights * np.sin(thetas))\n\n        # Calculate the raw angle using atan2\n        mu_raw = np.arctan2(Ry, Rx)\n        \n        # Map the angle to the range [0, 2*pi)\n        mu = (mu_raw + 2 * np.pi) % (2 * np.pi)\n        \n        return mu\n\n    def compute_circular_distance(mu1, mu2):\n        \"\"\"\n        Computes the circular distance between two angles.\n        \n        Args:\n            mu1 (float): First angle in radians.\n            mu2 (float): Second angle in radians.\n\n        Returns:\n            float: The circular distance d in [0, pi].\n        \"\"\"\n        delta = mu1 - mu2\n        # Wrap the difference to the interval (-pi, pi]\n        wrapped_delta = (delta + np.pi) % (2 * np.pi) - np.pi\n        # The distance is the absolute value of the wrapped difference\n        distance = np.abs(wrapped_delta)\n        return distance\n\n    for case in test_cases:\n        alpha = case[\"alpha\"]\n        phi_ref = case[\"phi_ref\"]\n        tau = case[\"tau\"]\n\n        # Compute preferred direction for session 1\n        mu1 = compute_preferred_direction(case[\"session1_pairs\"], alpha, phi_ref)\n\n        # Compute preferred direction for session 2\n        mu2 = compute_preferred_direction(case[\"session2_pairs\"], alpha, phi_ref)\n        \n        # Compute circular distance\n        d = compute_circular_distance(mu1, mu2)\n\n        # Check for stability\n        stable = d  tau\n\n        # Round float outputs to 6 decimal places\n        mu1_rounded = round(mu1, 6)\n        mu2_rounded = round(mu2, 6)\n        d_rounded = round(d, 6)\n\n        results.append([mu1_rounded, mu2_rounded, d_rounded, stable])\n\n    # Format the output as a single-line string representation of a list of lists.\n    # The map(str, ...) will convert each inner list like [1.2, 3.4, 0.5, True]\n    # into its string representation, e.g., \"'[1.2, 3.4, 0.5, True]'\".\n    # Joining these with commas and enclosing in brackets gives the desired format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "A simple mean direction, while useful, does not capture the full complexity of a neuron's tuning profile. For instance, a cell in the visual cortex might respond equally to a bar moving left-to-right and right-to-left, preferring an *axis* of motion rather than a single *direction*. This practice introduces a more sophisticated analytical tool, trigonometric moments, to diagnose these properties. By calculating the first and second trigonometric moments, you can quantitatively distinguish between direction-selective and orientation-selective responses, a crucial distinction in sensory neuroscience . This technique elegantly uses angle-doubling to transform an orientation-tuning problem into a direction-tuning problem, providing a powerful diagnostic for response symmetry.",
            "id": "4202367",
            "problem": "A neuron is probed with oriented stimuli, and its responses are recorded as nonnegative weights for discrete stimulus angles on the unit circle. Treat the stimulus angle as a circular random variable with angle in radians on the interval $\\left[0,2\\pi\\right)$. The fundamental base is the definition of the $k$-th trigonometric moment of a circular random variable, namely $m_k=\\mathbb{E}\\left[e^{\\mathrm{i}k\\Theta}\\right]$ for integer $k$, where $\\Theta$ is the random angle and $\\mathrm{i}$ is the imaginary unit. Orientation symmetry means invariance under a rotation by $\\pi$, so the density satisfies $f(\\theta)=f(\\theta+\\pi)$ for all $\\theta$, which implies two-fold periodic structure in angle. The core insight to be derived is that doubling the angle removes antipodal ambiguity: replacing $\\Theta$ by $2\\Theta$ makes orientation symmetry manifest in the second trigonometric moment $m_2$, whose magnitude $\\left|\\widehat{m}_2\\right|\\in\\left[0,1\\right]$ quantifies concentration around an orientation axis.\n\nStarting from the definition of $m_k$, derive a finite-sample, weighted estimator for $m_2$ based on angles $\\{\\theta_j\\}$ with corresponding nonnegative weights $\\{w_j\\}$, and then derive its magnitude $\\left|\\widehat{m}_2\\right|$. Use the same derivation to obtain $\\left|\\widehat{m}_1\\right|$ for interpretive disambiguation between true direction selectivity and axis-symmetric orientation tuning. Construct an algorithm that:\n- Accepts a set of angles $\\{\\theta_j\\}$ (in radians) and weights $\\{w_j\\}\\ge 0$.\n- Computes the weighted estimators $\\widehat{m}_1$ and $\\widehat{m}_2$ and returns $\\left|\\widehat{m}_2\\right|$.\n- Interprets orientation symmetry via the decision rule: classify as orientation-consistent if $|\\widehat{m}_2|\\ge T_2$ and simultaneously $|\\widehat{m}_1|T_1$, where $T_1$ and $T_2$ are user-specified thresholds in the unit interval.\n\nAngle unit must be radians. The final program must implement the following test suite (each test case provides the angles and weights, and thresholds $T_1$ and $T_2$):\n\n- Test case $1$ (happy path: strong axis-symmetric concentration). Let $\\theta_0=\\pi/4$. Angles are $\\{\\theta_0-0.1,\\ \\theta_0,\\ \\theta_0+0.1,\\ \\theta_0+\\pi-0.1,\\ \\theta_0+\\pi,\\ \\theta_0+\\pi+0.1\\}$ with weights $\\{5,10,5,5,10,5\\}$. Use $T_1=0.6$ and $T_2=0.7$.\n- Test case $2$ (direction-only concentration without antipodal partner). Let $\\theta_0=\\pi/4$. Angles are $\\{\\theta_0-0.1,\\ \\theta_0,\\ \\theta_0+0.1\\}$ with weights $\\{5,10,5\\}$. Use $T_1=0.6$ and $T_2=0.7$.\n- Test case $3$ (boundary: near-uniform coverage). Angles are $12$ equally spaced points on $\\left[0,2\\pi\\right)$, i.e., $\\{0,\\ 2\\pi/12,\\ 2\\cdot(2\\pi/12),\\ \\dots,\\ 11\\cdot(2\\pi/12)\\}$, with all weights equal to $1$. Use $T_1=0.6$ and $T_2=0.7$.\n- Test case $4$ (edge: four-fold symmetric pattern). Angles are $\\{0,\\ \\pi/2,\\ \\pi,\\ 3\\pi/2\\}$ with weights $\\{1,1,1,1\\}$. Use $T_1=0.6$ and $T_2=0.7$.\n- Test case $5$ (near-threshold axis symmetry with broad spread). Let $\\theta_0=0.3$ and $\\delta=0.4$. Angles are $\\{\\theta_0-\\delta,\\ \\theta_0+\\delta,\\ \\theta_0+\\pi-\\delta,\\ \\theta_0+\\pi+\\delta\\}$ with weights $\\{1,1,1,1\\}$. Use $T_1=0.6$ and $T_2=0.7$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, report a two-element list [$|\\widehat{m}_2|$, label], where $|\\widehat{m}_2|$ is a float rounded to $6$ decimal places, and label is an integer equal to $1$ if the decision rule deems the case orientation-consistent and equal to $0$ otherwise. The overall output should be a list of these per-test-case lists, in the order of the test suite.",
            "solution": "The problem requires the derivation and implementation of an algorithm to classify a neuron's stimulus-response profile as orientation-consistent. This involves analyzing a set of responses (weights, $w_j$) to oriented stimuli presented at various angles ($\\theta_j$). The core of the analysis rests on the concept of trigonometric moments from circular statistics.\n\n### 1. Derivation of the Weighted Trigonometric Moment Estimator\n\nThe theoretical $k$-th trigonometric moment, $m_k$, for a circular random variable $\\Theta$ with probability density function $f(\\theta)$ is defined by the expectation:\n$$m_k = \\mathbb{E}\\left[e^{\\mathrm{i}k\\Theta}\\right] = \\int_0^{2\\pi} e^{\\mathrm{i}k\\theta} f(\\theta) \\, d\\theta$$\nwhere $\\mathrm{i}$ is the imaginary unit, $k$ is an integer, and $\\theta \\in [0, 2\\pi)$.\n\nIn practice, we work with a finite set of discrete measurements: $N$ angles $\\{\\theta_j\\}_{j=1}^N$ with corresponding non-negative weights $\\{w_j\\}_{j=1}^N$. The weights represent the strength of the neuron's response to each stimulus angle. We can construct a discrete probability distribution where the probability mass $p_j$ associated with angle $\\theta_j$ is proportional to its weight $w_j$. Normalizing by the sum of all weights, $W = \\sum_{j=1}^N w_j$, gives the probability mass for each observation:\n$$p_j = \\frac{w_j}{W} = \\frac{w_j}{\\sum_{i=1}^N w_i}$$\nThe integral for the expectation is then replaced by a weighted sum, yielding the finite-sample estimator for the $k$-th trigonometric moment, denoted $\\widehat{m}_k$:\n$$\\widehat{m}_k = \\sum_{j=1}^N p_j e^{\\mathrm{i}k\\theta_j} = \\sum_{j=1}^N \\frac{w_j}{\\sum_{i=1}^N w_i} e^{\\mathrm{i}k\\theta_j}$$\nThis can be written as:\n$$\\widehat{m}_k = \\frac{\\sum_{j=1}^N w_j e^{\\mathrm{i}k\\theta_j}}{\\sum_{j=1}^N w_j}$$\n\n### 2. Magnitude of the Estimator\n\nThe estimator $\\widehat{m}_k$ is a complex number. To find its magnitude, $|\\widehat{m}_k|$, we first use Euler's formula, $e^{\\mathrm{i}\\phi} = \\cos(\\phi) + \\mathrm{i}\\sin(\\phi)$, to express the sum in the numerator:\n$$\\sum_{j=1}^N w_j e^{\\mathrm{i}k\\theta_j} = \\sum_{j=1}^N w_j (\\cos(k\\theta_j) + \\mathrm{i}\\sin(k\\theta_j)) = \\left(\\sum_{j=1}^N w_j \\cos(k\\theta_j)\\right) + \\mathrm{i}\\left(\\sum_{j=1}^N w_j \\sin(k\\theta_j)\\right)$$\nThe magnitude of a complex number $z = x + \\mathrm{i}y$ is $|z| = \\sqrt{x^2 + y^2}$. Applying this, the magnitude of the numerator is:\n$$\\left| \\sum_{j=1}^N w_j e^{\\mathrm{i}k\\theta_j} \\right| = \\sqrt{\\left(\\sum_{j=1}^N w_j \\cos(k\\theta_j)\\right)^2 + \\left(\\sum_{j=1}^N w_j \\sin(k\\theta_j)\\right)^2}$$\nThe magnitude of the estimator $\\widehat{m}_k$ is therefore the magnitude of the numerator divided by the (real and positive) total weight $W$:\n$$|\\widehat{m}_k| = \\frac{\\sqrt{\\left(\\sum_{j=1}^N w_j \\cos(k\\theta_j)\\right)^2 + \\left(\\sum_{j=1}^N w_j \\sin(k\\theta_j)\\right)^2}}{\\sum_{j=1}^N w_j}$$\nThis single formula is used to compute both $|\\widehat{m}_1|$ (by setting $k=1$) and $|\\widehat{m}_2|$ (by setting $k=2$).\n\n### 3. Interpreting Moments for Orientation and Direction Tuning\n\nThe problem distinguishes between direction tuning (preference for a single direction, e.g., $\\theta = \\pi/4$) and orientation tuning (preference for an axis, e.g., $\\theta = \\pi/4$ and $\\theta = 5\\pi/4$).\n\n**Orientation Symmetry ($f(\\theta) = f(\\theta+\\pi)$):** For a distribution with perfect orientation symmetry, all odd trigonometric moments are zero. For $k=1, 3, 5, \\dots$, we have $e^{\\mathrm{i}k(\\theta+\\pi)} = e^{\\mathrm{i}k\\theta}e^{\\mathrm{i}k\\pi} = e^{\\mathrm{i}k\\theta}(-1)^k = -e^{\\mathrm{i}k\\theta}$. The integral components from $\\theta$ and $\\theta+\\pi$ cancel out, leading to $m_k=0$. Therefore, a small value for $|\\widehat{m}_1|$ is indicative of orientation symmetry, as it suggests a lack of a single preferred direction. $|\\widehat{m}_1| \\in [0,1]$ is also known as the mean resultant length and measures concentration around a single mean direction.\n\n**Angle Doubling:** Orientation data is $\\pi$-periodic. To analyze it with tools designed for $2\\pi$-periodicity, we double the angles. An angle $\\theta$ and its opposite $\\theta+\\pi$ are mapped to $2\\theta$ and $2(\\theta+\\pi) = 2\\theta+2\\pi$. Since angles are considered modulo $2\\pi$, both original orientations map to the single angle $2\\theta$. This transformation removes the ambiguity between antipodal directions. Consequently, the magnitude of the second moment of the original angles, $|\\widehat{m}_2|$, is equivalent to the mean resultant length of the doubled angles. It measures the concentration of the data around a single preferred *axis*. A value of $|\\widehat{m}_2|$ close to $1$ implies strong orientation tuning.\n\n### 4. Algorithmic Design and Decision Rule\n\nThe algorithm formalizes the principles above:\n\n1.  **Inputs**: A set of angles $\\{\\theta_j\\}$, corresponding weights $\\{w_j\\}$, and two thresholds $T_1 \\in [0,1]$ and $T_2 \\in [0,1]$.\n2.  **Calculation of $|\\widehat{m}_1|$**:\n    *   Compute the total weight $W = \\sum_j w_j$.\n    *   Compute the sum of weighted vectors for $k=1$: $C_1 = \\sum_j w_j \\cos(\\theta_j)$ and $S_1 = \\sum_j w_j \\sin(\\theta_j)$.\n    *   Calculate $|\\widehat{m}_1| = \\frac{\\sqrt{C_1^2 + S_1^2}}{W}$.\n3.  **Calculation of $|\\widehat{m}_2|$**:\n    *   Compute the sum of weighted vectors for $k=2$: $C_2 = \\sum_j w_j \\cos(2\\theta_j)$ and $S_2 = \\sum_j w_j \\sin(2\\theta_j)$.\n    *   Calculate $|\\widehat{m}_2| = \\frac{\\sqrt{C_2^2 + S_2^2}}{W}$.\n4.  **Decision Rule**:\n    *   The neuron's response is classified as \"orientation-consistent\" if it exhibits strong tuning to an axis (high $|\\widehat{m}_2|$) but weak tuning to a specific direction (low $|\\widehat{m}_1|$).\n    *   The formal condition is: $|\\widehat{m}_2| \\ge T_2$ AND $|\\widehat{m}_1|  T_1$.\n    *   The output label is an integer: $1$ if the condition is met, $0$ otherwise.\n5.  **Output**: For each test case, the algorithm returns a two-element list containing the value of $|\\widehat{m}_2|$ (rounded to $6$ decimal places) and the integer label.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the tuning analysis problem for a series of test cases.\n    \"\"\"\n\n    def analyze_tuning(angles, weights, T1, T2):\n        \"\"\"\n        Calculates trigonometric moments and classifies tuning profile.\n\n        Args:\n            angles (np.ndarray): Array of stimulus angles in radians.\n            weights (np.ndarray): Array of corresponding non-negative weights.\n            T1 (float): Threshold for the first moment magnitude.\n            T2 (float): Threshold for the second moment magnitude.\n\n        Returns:\n            list: A list containing [|m2_hat|, label], where |m2_hat| is the\n                  magnitude of the second moment estimator rounded to 6 decimal\n                  places, and label is 1 for orientation-consistent, 0 otherwise.\n        \"\"\"\n        \n        # Ensure inputs are numpy arrays for vectorized operations\n        angles = np.array(angles, dtype=float)\n        weights = np.array(weights, dtype=float)\n\n        total_weight = np.sum(weights)\n\n        # Handle the edge case of zero total weight to avoid division by zero\n        if total_weight == 0:\n            return [0.0, 0]\n\n        # Calculate components for the first trigonometric moment (k=1)\n        cos_sum_1 = np.sum(weights * np.cos(angles))\n        sin_sum_1 = np.sum(weights * np.sin(angles))\n        \n        # Calculate the magnitude of the estimator m_hat_1\n        # This is the mean resultant length.\n        abs_m1_hat = np.sqrt(cos_sum_1**2 + sin_sum_1**2) / total_weight\n\n        # Calculate components for the second trigonometric moment (k=2)\n        # We use 2*theta to map orientation data to a circular space.\n        cos_sum_2 = np.sum(weights * np.cos(2 * angles))\n        sin_sum_2 = np.sum(weights * np.sin(2 * angles))\n\n        # Calculate the magnitude of the estimator m_hat_2\n        abs_m2_hat = np.sqrt(cos_sum_2**2 + sin_sum_2**2) / total_weight\n        \n        # Apply the decision rule for orientation consistency\n        # High |m2| (strong axis tuning) and low |m1| (weak direction tuning)\n        is_orientation_consistent = (abs_m2_hat = T2) and (abs_m1_hat  T1)\n        \n        label = 1 if is_orientation_consistent else 0\n        \n        return [round(abs_m2_hat, 6), label]\n\n    # Define the test cases from the problem statement\n    theta0_1 = np.pi / 4\n    test_cases = [\n        {\n            \"angles\": [theta0_1 - 0.1, theta0_1, theta0_1 + 0.1, theta0_1 + np.pi - 0.1, theta0_1 + np.pi, theta0_1 + np.pi + 0.1],\n            \"weights\": [5, 10, 5, 5, 10, 5],\n            \"T1\": 0.6,\n            \"T2\": 0.7\n        },\n        {\n            \"angles\": [theta0_1 - 0.1, theta0_1, theta0_1 + 0.1],\n            \"weights\": [5, 10, 5],\n            \"T1\": 0.6,\n            \"T2\": 0.7\n        },\n        {\n            \"angles\": [(j * 2 * np.pi / 12) for j in range(12)],\n            \"weights\": [1] * 12,\n            \"T1\": 0.6,\n            \"T2\": 0.7\n        },\n        {\n            \"angles\": [0, np.pi / 2, np.pi, 3 * np.pi / 2],\n            \"weights\": [1, 1, 1, 1],\n            \"T1\": 0.6,\n            \"T2\": 0.7\n        },\n        {\n            \"theta0\": 0.3,\n            \"delta\": 0.4,\n            \"angles\": [0.3 - 0.4, 0.3 + 0.4, 0.3 + np.pi - 0.4, 0.3 + np.pi + 0.4],\n            \"weights\": [1, 1, 1, 1],\n            \"T1\": 0.6,\n            \"T2\": 0.7\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = analyze_tuning(case[\"angles\"], case[\"weights\"], case[\"T1\"], case[\"T2\"])\n        results.append(result)\n\n    # Format the final output string as specified\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While descriptive statistics provide valuable initial insights, rigorous conclusions in neuroscience demand formal statistical modeling. This advanced practice guides you through fitting a Generalized Linear Model (GLM) with circular predictors to neuronal spike counts, a cornerstone of modern quantitative neuroscience. You will implement the Iteratively Reweighted Least Squares (IRLS) algorithm from first principles to fit a Poisson GLM and then use a likelihood ratio test to compare nested models. This allows you to formally test hypotheses, such as whether a neuron's firing rate is significantly modulated by the first harmonic of direction, providing a statistically robust method for validating tuning properties .",
            "id": "4202411",
            "problem": "You are given a task in neuroscience data analysis to assess directional tuning in neuronal firing rates using circular predictors on the unit circle. The fundamental base is the definition of the Poisson Generalized Linear Model (GLM) with the canonical logarithmic link, the construction of harmonic predictors for circular variables, and the likelihood ratio test between nested models grounded in Wilks' theorem.\n\nYou must implement a program that does the following for a set of synthetic datasets:\n\n1. Generate spike count data under a Poisson model. For each dataset, define angles $\\theta_i$ uniformly spaced on $[0,2\\pi)$ in radians, and simulate counts $y_i$ as draws from a Poisson distribution with mean\n$$\n\\lambda_i = \\exp\\left(\\beta_0 + A_1 \\cos(\\theta_i - \\phi_1) + A_2 \\cos\\left(2(\\theta_i - \\phi_2)\\right)\\right).\n$$\nAll angles must be treated in radians. The random number generator must be seeded deterministically to ensure reproducibility.\n\n2. Fit two nested Poisson GLMs with a logarithmic link using Iteratively Reweighted Least Squares (IRLS):\n   - Reduced model (without the first harmonic):\n     $$\n     \\eta_i^{\\text{red}} = \\beta_0 + \\gamma_c \\cos(2\\theta_i) + \\gamma_s \\sin(2\\theta_i),\n     $$\n     which corresponds to predictors $\\{1, \\cos(2\\theta_i), \\sin(2\\theta_i)\\}$.\n   - Full model (with the first harmonic added):\n     $$\n     \\eta_i^{\\text{full}} = \\beta_0 + \\beta_c \\cos(\\theta_i) + \\beta_s \\sin(\\theta_i) + \\gamma_c \\cos(2\\theta_i) + \\gamma_s \\sin(2\\theta_i),\n     $$\n     which corresponds to predictors $\\{1, \\cos(\\theta_i), \\sin(\\theta_i), \\cos(2\\theta_i), \\sin(2\\theta_i)\\}$.\n\n   The log-likelihood (up to an additive constant independent of parameters) is\n   $$\n   \\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} \\left( y_i \\eta_i - \\exp(\\eta_i) \\right),\n   $$\n   where $\\eta_i$ is the linear predictor for the model and $\\boldsymbol{\\beta}$ denotes the parameter vector.\n\n3. Compute the likelihood ratio statistic for testing the significance of the first harmonic under the null hypothesis $H_0: \\beta_c = 0, \\beta_s = 0$ versus the alternative that at least one of these is non-zero:\n   $$\n   T = 2\\left(\\ell_{\\text{full}} - \\ell_{\\text{reduced}}\\right).\n   $$\n   Under regularity conditions and $H_0$, $T$ is asymptotically distributed as a chi-square with $k=2$ degrees of freedom. Compute the $p$-value using the chi-square survival function with $k=2$, and decide significance at level $\\alpha = 0.05$.\n\nYour program must implement IRLS from first principles for the Poisson GLM. Specifically, starting from an initial parameter vector, update the parameter estimates using the Fisher scoring step for Poisson with logarithmic link:\n- At each iteration, compute $\\eta_i = \\mathbf{x}_i^\\top \\boldsymbol{\\beta}$, $\\mu_i = \\exp(\\eta_i)$, weights $w_i = \\mu_i$, working response $z_i = \\eta_i + \\frac{y_i - \\mu_i}{\\mu_i}$, and solve the weighted least squares system\n$$\n\\left(\\mathbf{X}^\\top \\mathbf{W} \\mathbf{X}\\right) \\boldsymbol{\\beta}^{\\text{new}} = \\mathbf{X}^\\top \\mathbf{W} \\mathbf{z},\n$$\nwhere $\\mathbf{X}$ is the design matrix, $\\mathbf{W} = \\mathrm{diag}(w_i)$, and $\\mathbf{z}$ is the vector of working responses. Iterate until convergence in parameter norm.\n\nAngle units: All angles must be in radians.\n\nTest suite specification:\nImplement the following four datasets, each specified by the tuple $(n, \\beta_0, A_1, \\phi_1, A_2, \\phi_2)$, where $n$ is the number of angles, $\\beta_0$ is the baseline log-rate, $A_1$ and $\\phi_1$ are the amplitude and preferred direction for the first harmonic, and $A_2$ and $\\phi_2$ are the amplitude and preferred direction for the second harmonic. Use the same deterministic random seed across all datasets. The data generation for each dataset must use the stated $\\lambda_i$ formula.\n\n- Case 1 (general case with strong first-harmonic tuning): $(60, 1.2, 0.8, 0.7, 0.3, 2.0)$\n- Case 2 (no first-harmonic tuning but strong second harmonic): $(60, 1.2, 0.0, 1.0, 0.9, 0.5)$\n- Case 3 (small sample with weak first harmonic, no second harmonic): $(24, 0.1, 0.2, 2.5, 0.0, 1.0)$\n- Case 4 (moderate sample with both harmonics present): $(36, 2.0, 0.4, -1.0, 0.6, 1.7)$\n\nFinal output format:\nYour program should produce a single line of output containing the results for the four test cases as a comma-separated list enclosed in square brackets, where each element is a boolean indicating whether the first harmonic is significant at level $\\alpha = 0.05$ for that test case (i.e., $p  0.05$), in the order of the test cases above. For example: \"[True,False,True,False]\". No other text should be printed.",
            "solution": "The task is to implement a statistical procedure to test for the significance of directional tuning in synthetic neuronal spike count data. This involves generating data from a known Poisson process, fitting two nested Generalized Linear Models (GLMs) using a custom implementation of the Iteratively Reweighted Least Squares (IRLS) algorithm, and performing a likelihood ratio test. The entire process is grounded in fundamental principles of statistical modeling and hypothesis testing.\n\n### 1. Data Generation\n\nThe first step is to generate synthetic spike count data. For each dataset, a set of $n$ angles, $\\theta_i$, is uniformly spaced over the interval $[0, 2\\pi)$. The spike count, $y_i$, for each angle is a random draw from a Poisson distribution, $y_i \\sim \\text{Poisson}(\\lambda_i)$. The mean firing rate, $\\lambda_i$, is a function of the angle $\\theta_i$ and is defined by a log-linear model incorporating first and second-order harmonics of the angle:\n$$\n\\lambda_i = \\exp\\left(\\beta_0 + A_1 \\cos(\\theta_i - \\phi_1) + A_2 \\cos\\left(2(\\theta_i - \\phi_2)\\right)\\right)\n$$\nHere, $\\beta_0$ is the baseline log firing rate. $A_1$ and $\\phi_1$ are the amplitude and preferred direction (phase) of the first harmonic component, respectively, which typically models unimodal tuning. $A_2$ and $\\phi_2$ are the amplitude and preferred direction of the second harmonic, which can capture bimodal tuning patterns. All angular quantities are in radians. Data generation requires a deterministic random number generator for reproducibility.\n\n### 2. Model Specification\n\nTo test the significance of the first harmonic component, we fit two nested Poisson GLMs with a logarithmic link function, $\\log(\\mu_i) = \\eta_i$, where $\\mu_i = E[y_i]$ and $\\eta_i$ is the linear predictor.\n\nThe linear predictors are constructed using trigonometric identities. The term $A_k \\cos(k(\\theta_i - \\phi_k))$ can be expanded as $A_k (\\cos(k\\theta_i)\\cos(k\\phi_k) + \\sin(k\\theta_i)\\sin(k\\phi_k))$. This is a linear combination of $\\cos(k\\theta_i)$ and $\\sin(k\\theta_i)$, with coefficients that depend on $A_k$ and $\\phi_k$.\n\n**Full Model ($M_{\\text{full}}$):** This model includes both first and second harmonics. Its linear predictor is:\n$$\n\\eta_i^{\\text{full}} = \\beta_0 + \\beta_c \\cos(\\theta_i) + \\beta_s \\sin(\\theta_i) + \\gamma_c \\cos(2\\theta_i) + \\gamma_s \\sin(2\\theta_i)\n$$\nThis corresponds to a design matrix, $\\mathbf{X}_{\\text{full}}$, with five columns: an intercept ($1$), $\\cos(\\theta_i)$, $\\sin(\\theta_i)$, $\\cos(2\\theta_i)$, and $\\sin(2\\theta_i)$. The parameter vector is $\\boldsymbol{\\beta}_{\\text{full}} = [\\beta_0, \\beta_c, \\beta_s, \\gamma_c, \\gamma_s]^\\top$.\n\n**Reduced Model ($M_{\\text{red}}$):** This model is nested within the full model and represents the null hypothesis that the first harmonic component is absent ($A_1=0$, which implies $\\beta_c = 0$ and $\\beta_s = 0$). The linear predictor omits the first harmonic terms:\n$$\n\\eta_i^{\\text{red}} = \\beta_0 + \\gamma_c \\cos(2\\theta_i) + \\gamma_s \\sin(2\\theta_i)\n$$\nThe corresponding design matrix, $\\mathbf{X}_{\\text{red}}$, has three columns: an intercept, $\\cos(2\\theta_i)$, and $\\sin(2\\theta_i)$. The parameter vector is $\\boldsymbol{\\beta}_{\\text{red}} = [\\beta_0, \\gamma_c, \\gamma_s]^\\top$.\n\n### 3. Model Fitting via Iteratively Reweighted Least Squares (IRLS)\n\nThe parameters for both models are estimated by maximizing the Poisson log-likelihood. For a Poisson GLM, the log-likelihood function, ignoring terms that are constant with respect to the parameters $\\boldsymbol{\\beta}$, is:\n$$\n\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} \\left( y_i \\eta_i - \\exp(\\eta_i) \\right) = \\sum_{i=1}^{n} \\left( y_i (\\mathbf{x}_i^\\top \\boldsymbol{\\beta}) - \\exp(\\mathbf{x}_i^\\top \\boldsymbol{\\beta}) \\right)\n$$\nwhere $\\mathbf{x}_i^\\top$ is the $i$-th row of the design matrix $\\mathbf{X}$. The IRLS algorithm is a numerical method to find the maximum likelihood estimate of $\\boldsymbol{\\beta}$. It iteratively solves a series of weighted least squares problems. For a Poisson model with a canonical log link, the update step for $\\boldsymbol{\\beta}$ at each iteration is given by the Fisher scoring algorithm, which is equivalent to Newton-Raphson in this case.\n\nThe algorithm proceeds as follows:\n1.  Initialize the parameter vector $\\boldsymbol{\\beta}$, for instance, as a vector of zeros.\n2.  Iterate until convergence:\n    a.  Compute the linear predictor for each observation: $\\boldsymbol{\\eta} = \\mathbf{X} \\boldsymbol{\\beta}$.\n    b.  Compute the estimated mean response using the inverse link function: $\\boldsymbol{\\mu} = \\exp(\\boldsymbol{\\eta})$.\n    c.  Define the diagonal weight matrix $\\mathbf{W}$ with diagonal elements $w_i = \\text{Var}(y_i) / (\\text{g}'(\\mu_i))^2$. For the Poisson distribution, $\\text{Var}(y_i)=\\mu_i$, and for the log link, $g(\\mu_i)=\\log(\\mu_i)$, so $g'(\\mu_i)=1/\\mu_i$. This gives $w_i = \\mu_i / (1/\\mu_i)^2 = \\mu_i^2 / \\mu_i = \\mu_i$.\n    d.  Compute the working response (or \"adjusted dependent variable\"): $z_i = \\eta_i + (y_i - \\mu_i) / \\mu_i$.\n    e.  Solve the weighted least squares system for the new parameter estimate $\\boldsymbol{\\beta}^{\\text{new}}$:\n       $$\n       \\left(\\mathbf{X}^\\top \\mathbf{W} \\mathbf{X}\\right) \\boldsymbol{\\beta}^{\\text{new}} = \\mathbf{X}^\\top \\mathbf{W} \\mathbf{z}\n       $$\n    f.  Check for convergence by examining the norm of the change in parameters, $\\|\\boldsymbol{\\beta}^{\\text{new}} - \\boldsymbol{\\beta}\\|$. If it is below a specified tolerance, terminate. Otherwise, update $\\boldsymbol{\\beta} \\leftarrow \\boldsymbol{\\beta}^{\\text{new}}$ and continue.\n\nThis procedure is applied separately to fit both the full and reduced models, yielding parameter estimates $\\hat{\\boldsymbol{\\beta}}_{\\text{full}}$ and $\\hat{\\boldsymbol{\\beta}}_{\\text{red}}$, respectively.\n\n### 4. Likelihood Ratio Test\n\nWith the maximized log-likelihood values for both models, $\\ell_{\\text{full}} = \\ell(\\hat{\\boldsymbol{\\beta}}_{\\text{full}})$ and $\\ell_{\\text{red}} = \\ell(\\hat{\\boldsymbol{\\beta}}_{\\text{red}})$, we can test the null hypothesis $H_0: \\beta_c = \\beta_s = 0$. The likelihood ratio test (LRT) statistic is:\n$$\nT = 2(\\ell_{\\text{full}} - \\ell_{\\text{reduced}})\n$$\nAccording to Wilks' theorem, under the null hypothesis, $T$ asymptotically follows a chi-square ($\\chi^2$) distribution. The degrees of freedom, $k$, for this distribution is the difference in the number of parameters between the full and reduced models. In this case, $k = 5 - 3 = 2$.\n\nThe $p$-value is the probability of observing a test statistic as extreme as, or more extreme than, the one computed, assuming the null hypothesis is true. This is calculated using the survival function (1 minus the cumulative distribution function) of the $\\chi^2_2$ distribution:\n$$\np = P(\\chi^2_2 \\ge T)\n$$\nFinally, we compare the $p$-value to a pre-defined significance level, $\\alpha = 0.05$. If $p  0.05$, we reject the null hypothesis and conclude that the first harmonic component is statistically significant. Otherwise, we fail to reject the null hypothesis.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef fit_poisson_glm_irls(y, X, max_iter=100, tol=1e-8):\n    \"\"\"\n    Fits a Poisson GLM with a log link using IRLS from first principles.\n\n    Args:\n        y (np.ndarray): The response variable (spike counts), shape (n_samples,).\n        X (np.ndarray): The design matrix, shape (n_samples, n_features).\n        max_iter (int): Maximum number of iterations for the IRLS algorithm.\n        tol (float): Convergence tolerance for the norm of parameter updates.\n\n    Returns:\n        tuple[np.ndarray, float]: Tuple containing the fitted coefficients (beta)\n                                  and the maximized log-likelihood.\n    \"\"\"\n    # Initialize beta coefficients to zero\n    beta = np.zeros(X.shape[1])\n    \n    for _ in range(max_iter):\n        eta = X @ beta\n        # Clamp eta to prevent numerical overflow/underflow in np.exp\n        eta = np.clip(eta, -25, 25)\n        mu = np.exp(eta)\n        \n        # Guard against division by zero if mu is very small\n        mu[mu  1e-10] = 1e-10\n        \n        # Compute weights and working response for Poisson with log link\n        weights = mu\n        z = eta + (y - mu) / mu\n        \n        # Form and solve the weighted least squares system\n        # X.T @ W @ X * beta_new = X.T @ W @ z\n        # W is a diagonal matrix, so we can use broadcasting for efficiency\n        X_T_W = X.T * weights[np.newaxis, :]  # (p, n)\n        \n        # Left-hand side: Hessian or Fisher Information Matrix\n        hessian = X_T_W @ X  # (p, p)\n        \n        # Right-hand side\n        rhs = X_T_W @ z  # (p,)\n        \n        try:\n            beta_new = np.linalg.solve(hessian, rhs)\n        except np.linalg.LinAlgError:\n            # If solver fails, use pseudo-inverse as a fallback\n            beta_new = np.linalg.pinv(hessian) @ rhs\n\n        # Check for convergence\n        if np.linalg.norm(beta_new - beta)  tol:\n            beta = beta_new\n            break\n        \n        beta = beta_new\n    \n    # Calculate final log-likelihood\n    final_eta = X @ beta\n    log_likelihood = np.sum(y * final_eta - np.exp(final_eta))\n    \n    return beta, log_likelihood\n\n\ndef solve():\n    \"\"\"\n    Main function to run the tuning analysis for all specified test cases.\n    \"\"\"\n    # Test suite specification: (n, beta0, A1, phi1, A2, phi2)\n    test_cases = [\n        (60, 1.2, 0.8, 0.7, 0.3, 2.0),\n        (60, 1.2, 0.0, 1.0, 0.9, 0.5),\n        (24, 0.1, 0.2, 2.5, 0.0, 1.0),\n        (36, 2.0, 0.4, -1.0, 0.6, 1.7)\n    ]\n    \n    # Use a deterministic random seed for reproducibility\n    rng = np.random.default_rng(seed=42)\n    alpha = 0.05\n    results = []\n\n    for n, beta0, A1, phi1, A2, phi2 in test_cases:\n        # 1. Generate synthetic data\n        theta = np.linspace(0, 2 * np.pi, n, endpoint=False)\n        \n        log_lambda = beta0 + A1 * np.cos(theta - phi1) + A2 * np.cos(2 * (theta - phi2))\n        lambda_i = np.exp(log_lambda)\n        \n        y = rng.poisson(lam=lambda_i)\n        \n        # 2. Define design matrices for reduced and full models\n        # Reduced model: Intercept, cos(2*theta), sin(2*theta)\n        X_red = np.c_[np.ones(n), np.cos(2 * theta), np.sin(2 * theta)]\n        \n        # Full model: Intercept, cos/sin(theta), cos/sin(2*theta)\n        X_full = np.c_[\n            np.ones(n), \n            np.cos(theta), \n            np.sin(theta), \n            np.cos(2 * theta), \n            np.sin(2 * theta)\n        ]\n        \n        # 3. Fit both models using IRLS\n        _, ll_red = fit_poisson_glm_irls(y, X_red)\n        _, ll_full = fit_poisson_glm_irls(y, X_full)\n        \n        # 4. Perform the Likelihood Ratio Test\n        # Test statistic\n        lrt_statistic = 2 * (ll_full - ll_red)\n        \n        # Degrees of freedom: difference in number of parameters\n        dof = X_full.shape[1] - X_red.shape[1]\n        \n        # Compute p-value from chi-square survival function\n        p_value = chi2.sf(lrt_statistic, df=dof)\n        \n        # 5. Decide significance\n        is_significant = p_value  alpha\n        results.append(is_significant)\n        \n    # Print the final result in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}