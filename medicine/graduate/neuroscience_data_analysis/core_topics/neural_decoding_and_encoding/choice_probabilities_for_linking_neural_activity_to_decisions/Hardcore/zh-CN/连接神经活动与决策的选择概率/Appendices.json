{
    "hands_on_practices": [
        {
            "introduction": "这项实践是理解选择概率的理论基石。你将从基本定义出发，推导选择概率 ($CP$) 与来自信号检测论的神经元敏感性指数 ($d'$) 之间的经典关系。通过在理想化的高斯模型下完成此推导，你将深刻理解 $CP$ 是如何量化神经活动与决策关联的。",
            "id": "4145816",
            "problem": "在一个双抉择强制选择决策任务中，考虑在固定时间窗口内测量的单个神经元的放电计数响应 $r$。对于一个固定的刺激条件 $s$，假设给定被试最终选择的情况下，$r$ 的条件分布是方差相等但均值不同的高斯分布：$r \\mid (\\text{choice} = +, s) \\sim \\mathcal{N}(\\mu_{+}(s), \\sigma^{2}(s))$ 和 $r \\mid (\\text{choice} = -, s) \\sim \\mathcal{N}(\\mu_{-}(s), \\sigma^{2}(s))$。假设试验间相互独立，并且从这两个条件分布中抽取的样本在试验间也是独立的。\n\n将该神经元在刺激 $s$ 下的神经度量敏感性指数 $d^{\\prime}(s)$ 定义为\n$$\nd^{\\prime}(s) \\equiv \\frac{\\mu_{+}(s) - \\mu_{-}(s)}{\\sigma(s)}.\n$$\n将刺激 $s$ 下的选择概率 (CP)，记作 $\\mathrm{CP}(s)$，定义为在以 $s$ 为条件时，根据单次试验响应 $r$ 对被试选择进行分类的受试者工作特征 (ROC) 曲线下的面积。等价地，$\\mathrm{CP}(s)$ 等于从 $r \\mid (\\text{choice} = +, s)$ 中独立抽取的一个响应值超过从 $r \\mid (\\text{choice} = -, s)$ 中独立抽取的一个响应值的概率。\n\n仅从这些定义和高斯随机变量的性质出发，推导在等方差高斯情况下，$\\mathrm{CP}(s)$ 作为 $d^{\\prime}(s)$ 函数的闭式表达式。你的推导应明确阐述所使用的概率变换，并证明每一步的合理性。同时，请用文字解释在你推导的映射中，以刺激 $s$ 为条件的作用，以及在何种情况下，如果响应是跨多个刺激值汇集的，或者如果等方差假设被违反，该映射会失效。\n\n请以 $\\mathrm{CP}(s)$ 关于 $d^{\\prime}(s)$ 的单个闭式解析表达式的形式提供你的最终结果。不需要进行数值近似或四舍五入。",
            "solution": "该问题在应用于神经科学的信号检测论框架内是适定的，并具有科学依据。所有定义都是标准的，其假设虽然是简化的，但在该领域是常见的。下面开始推导。\n\n目标是推导选择概率 $\\mathrm{CP}(s)$ 作为神经度量敏感性指数 $d^{\\prime}(s)$ 函数的闭式表达式。\n\n我们得到了选择概率 $\\mathrm{CP}(s)$ 的两种定义。我们将使用概率定义，即 $\\mathrm{CP}(s)$ 是从选择“$+$”的分布中独立抽取的一个响应值超过从选择“$-$”的分布中独立抽取的一个响应值的概率。设 $r_{+}$ 是在选择“$+$”和刺激 $s$ 条件下的神经响应的随机变量，设 $r_{-}$ 是在选择“$-$”和刺激 $s$ 条件下的神经响应的随机变量。根据问题陈述，它们的分布是：\n$$\nr_{+} \\sim \\mathcal{N}(\\mu_{+}(s), \\sigma^{2}(s))\n$$\n$$\nr_{-} \\sim \\mathcal{N}(\\mu_{-}(s), \\sigma^{2}(s))\n$$\n题目说明变量 $r_{+}$ 和 $r_{-}$ 是独立抽取的。那么选择概率定义为：\n$$\n\\mathrm{CP}(s) = P(r_{+}  r_{-})\n$$\n这个不等式可以通过考虑两个随机变量之差来改写。我们定义一个新的随机变量 $\\Delta r$ 为它们的差：\n$$\n\\Delta r = r_{+} - r_{-}\n$$\n我们要求的概率就是 $P(\\Delta r  0)$。\n\n为了计算这个概率，我们必须首先确定 $\\Delta r$ 的概率分布。高斯随机变量的一个基本性质是它们的线性组合也是一个高斯随机变量。由于 $r_{+}$ 和 $r_{-}$ 是独立的高斯变量，它们的差 $\\Delta r$ 也是高斯分布的。\n\n$\\Delta r$ 的均值，记作 $E[\\Delta r]$，是各自均值之差：\n$$\nE[\\Delta r] = E[r_{+}] - E[r_{-}] = \\mu_{+}(s) - \\mu_{-}(s)\n$$\n$\\Delta r$ 的方差，记作 $\\mathrm{Var}(\\Delta r)$，是各自方差之和，因为 $r_{+}$ 和 $r_{-}$ 是独立的：\n$$\n\\mathrm{Var}(\\Delta r) = \\mathrm{Var}(r_{+}) + \\mathrm{Var}(r_{-}) = \\sigma^{2}(s) + \\sigma^{2}(s) = 2\\sigma^{2}(s)\n$$\n因此，$\\Delta r$ 的标准差是 $\\mathrm{SD}(\\Delta r) = \\sqrt{2\\sigma^{2}(s)} = \\sqrt{2}\\sigma(s)$。\n\n所以，差值变量的分布是：\n$$\n\\Delta r \\sim \\mathcal{N}(\\mu_{+}(s) - \\mu_{-}(s), 2\\sigma^{2}(s))\n$$\n我们现在计算该分布的概率 $P(\\Delta r  0)$。为此，我们将随机变量 $\\Delta r$ 标准化，即减去其均值再除以其标准差，这会将其转换为一个标准正态变量 $Z \\sim \\mathcal{N}(0, 1)$：\n$$\nZ = \\frac{\\Delta r - E[\\Delta r]}{\\mathrm{SD}(\\Delta r)} = \\frac{\\Delta r - (\\mu_{+}(s) - \\mu_{-}(s))}{\\sqrt{2}\\sigma(s)}\n$$\n我们可以用标准正态变量 $Z$ 来重写不等式 $\\Delta r  0$：\n$$\n\\Delta r  0 \\implies \\Delta r - (\\mu_{+}(s) - \\mu_{-}(s))  -(\\mu_{+}(s) - \\mu_{-}(s))\n$$\n$$\n\\frac{\\Delta r - (\\mu_{+}(s) - \\mu_{-}(s))}{\\sqrt{2}\\sigma(s)}  \\frac{-(\\mu_{+}(s) - \\mu_{-}(s))}{\\sqrt{2}\\sigma(s)}\n$$\n$$\nZ  -\\frac{\\mu_{+}(s) - \\mu_{-}(s)}{\\sqrt{2}\\sigma(s)}\n$$\n问题将神经度量敏感性指数定义为 $d^{\\prime}(s) = \\frac{\\mu_{+}(s) - \\mu_{-}(s)}{\\sigma(s)}$。将此定义代入我们的不等式，得到：\n$$\nZ  -\\frac{d^{\\prime}(s)}{\\sqrt{2}}\n$$\n选择概率就是这个事件的概率：\n$$\n\\mathrm{CP}(s) = P\\left(Z  -\\frac{d^{\\prime}(s)}{\\sqrt{2}}\\right)\n$$\n设 $\\Phi(z)$ 表示标准正态分布的累积分布函数 (CDF)，即 $\\Phi(z) = P(Z \\le z)$。概率 $P(Z  a)$ 等于 $1 - P(Z \\le a) = 1 - \\Phi(a)$。应用此公式，我们得到：\n$$\n\\mathrm{CP}(s) = 1 - \\Phi\\left(-\\frac{d^{\\prime}(s)}{\\sqrt{2}}\\right)\n$$\n由于标准正态分布关于 $0$ 对称，我们有性质 $\\Phi(-z) = 1 - \\Phi(z)$ 对任意 $z$ 成立。因此，我们可以简化 $\\mathrm{CP}(s)$ 的表达式：\n$$\n\\mathrm{CP}(s) = 1 - \\left(1 - \\Phi\\left(\\frac{d^{\\prime}(s)}{\\sqrt{2}}\\right)\\right) = \\Phi\\left(\\frac{d^{\\prime}(s)}{\\sqrt{2}}\\right)\n$$\n这就是选择概率作为神经度量敏感性指数函数的闭式表达式。\n\n关于问题的第二部分：\n以刺激 $s$ 为条件的作用是至关重要的。神经响应分布的参数——$\\mu_{+}(s)$、$\\mu_{-}(s)$ 和 $\\sigma(s)$——都是刺激 $s$ 的函数。因此，$d^{\\prime}(s)$ 和 $\\mathrm{CP}(s)$ 本身也都是刺激的函数。所推导的映射关系 $\\mathrm{CP}(s) = \\Phi(d^{\\prime}(s)/\\sqrt{2})$ 仅在单个、固定的刺激水平 $s$ 下定义的分布对上成立。如果将不同刺激值（例如 $s_{1}, s_{2}, \\dots$）下的神经响应汇集起来，得到的汇集数据将不再是从单个高斯分布中抽取的。相反，它会是一个高斯混合分布。这里推导出的简单关系（依赖于单对高斯分布的性质）将会失效。总的选择概率将是特定刺激下选择概率的加权平均，但这无法以简单的方式与从汇集的非高斯数据中草率计算出的单个 $d^{\\prime}$ 值相关联。\n\n等方差假设也至关重要。如果方差不相等，使得 $r_{+} \\sim \\mathcal{N}(\\mu_{+}(s), \\sigma_{+}^{2}(s))$ 且 $r_{-} \\sim \\mathcal{N}(\\mu_{-}(s), \\sigma_{-}^{2}(s))$，那么差值 $\\Delta r$ 的方差将是 $\\mathrm{Var}(\\Delta r) = \\sigma_{+}^{2}(s) + \\sigma_{-}^{2}(s)$。标准化变量 $Z$ 的不等式将变为：\n$$\nZ  -\\frac{\\mu_{+}(s) - \\mu_{-}(s)}{\\sqrt{\\sigma_{+}^{2}(s) + \\sigma_{-}^{2}(s)}}\n$$\n累积分布函数的自变量将变为 $\\frac{\\mu_{+}(s) - \\mu_{-}(s)}{\\sqrt{\\sigma_{+}^{2}(s) + \\sigma_{-}^{2}(s)}}$。这个表达式不能被简化为仅是 $d^{\\prime}(s) \\equiv \\frac{\\mu_{+}(s) - \\mu_{-}(s)}{\\sigma(s)}$ 的函数，其中 $\\sigma(s)$ 通常是某种形式的汇集标准差。CP 和 $d^{\\prime}$ 之间的关系将变得更加复杂，还会额外依赖于标准差的比率 $\\sigma_{+}(s)/\\sigma_{-}(s)$。",
            "answer": "$$\n\\boxed{\\Phi\\left(\\frac{d^{\\prime}(s)}{\\sqrt{2}}\\right)}\n$$"
        },
        {
            "introduction": "在理解了基于特定刺激的 $CP$ 定义后 ，本实践将通过计算模拟，揭示一个常见的数据分析陷阱——辛普森悖论。你将亲眼见证，当忽略刺激这个混淆变量并将数据混合分析时，我们对神经活动与决策之间关系的判断会发生戏剧性的逆转。这项练习旨在强调在分析神经-行为关联时，严谨控制实验变量的必要性。",
            "id": "4145788",
            "problem": "您的任务是构建一个模拟，在选择概率的背景下展示将神经活动与决策联系起来时出现的辛普森悖论，然后通过对刺激水平进行条件化来解决该悖论。使用线性高斯生成模型，以纯数学术语来描述一个二元决策和单个神经元的响应。该问题要求严格遵守概率论和经典信号检测分析中的定义。\n\n基本设定：\n- 令刺激为一个离散随机变量 $S \\in \\{s_1,\\dots,s_m\\}$。\n- 令决策变量为 $D = w S + \\xi$，其中 $w \\in \\mathbb{R}$ 且 $\\xi \\sim \\mathcal{N}(0,\\sigma_d^2)$ 是独立高斯噪声。\n- 令二元选择为 $C = \\mathbb{I}[D \\ge 0]$，其中 $\\mathbb{I}[\\cdot]$ 是指示函数。\n- 令神经元的标量响应为 $R = a S + b D + \\varepsilon$，其中 $a,b \\in \\mathbb{R}$ 且 $\\varepsilon \\sim \\mathcal{N}(0,\\sigma_r^2)$ 是独立高斯噪声。\n\n定义：\n- 对于一个神经元，在固定刺激 $S=s$ 的条件下，其选择概率 (CP) 定义为：在相同刺激下，一次随机选择的 $C=1$ 试验中的神经元响应超过一次随机选择的 $C=0$ 试验中的神经元响应的概率：$$\\mathrm{CP}(s) = \\mathbb{P}\\!\\left(R^{(1)}  R^{(0)} \\mid S=s\\right),$$ 其中 $R^{(1)}$ 从以 $(C=1,S=s)$ 为条件的 $R$ 的分布中抽取，$R^{(0)}$ 从以 $(C=0,S=s)$ 为条件的 $R$ 的分布中抽取。该量等于根据这两个条件响应分布计算出的受试者工作特征 (ROC) 曲线下的面积。\n- 忽略刺激的聚合选择概率为 $$\\mathrm{CP}_{\\mathrm{agg}} = \\mathbb{P}\\!\\left(R^{(1)}  R^{(0)}\\right),$$ 其中 $R^{(1)}$ 从以 $C=1$ 为条件的 $R$ 的分布中抽取，$R^{(0)}$ 从以 $C=0$ 为条件的 $R$ 的分布中抽取，对 $S$ 没有约束。根据全概率定律，这个聚合是在不同刺激水平的混合体上进行的，因此可能会将 $R$ 中依赖于刺激的效应与决策相关的效应混为一谈。\n\n目标：\n- 对于几组参数集，在指定的生成模型下模拟 $N$ 次试验。对于每组参数集：\n  1. 生成 $N$ 个 $(S,D,C,R)$ 的独立样本。\n  2. 使用由 $C=1$ 和 $C=0$ 的 $R$ 分布形成的 ROC 曲线下的经验面积，计算 $\\mathrm{CP}_{\\mathrm{agg}}$。\n  3. 对于每个刺激水平 $s \\in \\{s_1,\\dots,s_m\\}$，仅使用 $S=s$ 的试验来计算 $\\mathrm{CP}(s)$。\n  4. 判断是否出现辛普森悖论，此处定义为事件：$$\\mathrm{CP}_{\\mathrm{agg}}  0.5 \\quad \\text{and} \\quad \\mathrm{CP}(s)  0.5 \\ \\text{for all stimulus levels } s.$$\n- 使用 Mann–Whitney $U$ 统计量公式来估计 ROC 曲线下的面积，并以适当的方式处理平局情况：如果 $X$ 是 $C=1$ 时的响应，$Y$ 是 $C=0$ 时的响应，$$\\widehat{\\mathrm{AUC}} = \\frac{U}{n_X n_Y},$$ 其中 $U$ 是根据合并样本的平均秩计算得出的，$n_X$ 和 $n_Y$ 是样本大小。\n\n科学真实性：\n- 所有变量都必须根据指定的、带有独立噪声项 $\\xi$ 和 $\\varepsilon$ 的线性高斯模型进行模拟。\n- 必须选择适当的刺激水平，以确保在每个 $s$ 下，两种选择都以非零频率出现，从而保证经验 $\\mathrm{CP}(s)$ 有良好定义。\n\n测试套件：\n- 提供一个包含4个参数集的测试套件，用于探究不同的机制：\n  1. 一个旨在展示辛普森悖论的案例（聚合CP相对于刺激内CP的符号相反）：\n     - $N = 4000$，$S \\in \\{-1,0,1\\}$ 均匀分布，$w = 2.0$, $a = -3.0$, $b = 0.7$, $\\sigma_d = 1.0$, $\\sigma_r = 1.0$，种子 $= 1$。\n  2. 一个没有反转的案例（聚合CP和条件CP的符号一致）：\n     - $N = 4000$，$S \\in \\{-1,0,1\\}$ 均匀分布，$w = 2.0$, $a = 0.5$, $b = 0.7$, $\\sigma_d = 1.0$, $\\sigma_r = 1.0$，种子 $= 2$。\n  3. 一个边界案例，其中聚合CP约等于 $0.5$，而刺激内CP偏离 $0.5$：\n     - $N = 4000$，$S \\in \\{-1,0,1\\}$ 均匀分布，$w = 2.0$, $a = -1.0$, $b = 0.7$, $\\sigma_d = 1.0$, $\\sigma_r = 1.0$，种子 $= 3$。\n  4. 一个极端案例，由于与决策变量的耦合为零，刺激内CP恰好为 $0.5$：\n     - $N = 4000$，$S \\in \\{-1,0,1\\}$ 均匀分布，$w = 2.0$, $a = -3.0$, $b = 0.0$, $\\sigma_d = 1.0$, $\\sigma_r = 1.0$，种子 $= 4$。\n\n输出规格：\n- 对于每个参数集，输出一个形式为 $[\\mathrm{CP}_{\\mathrm{agg}}, [\\mathrm{CP}(s_1),\\dots,\\mathrm{CP}(s_m)], \\mathrm{rev}]$ 的列表，其中 $\\mathrm{rev}$ 是一个布尔值，指示是否根据上述定义发生了辛普森悖论。\n- 您的程序应生成单行输出，其中包含所有4个测试用例的结果，格式为方括号括起来的逗号分隔列表，例如 $[[\\dots],[\\dots],[\\dots],[\\dots]]$。不应打印任何其他文本。",
            "solution": "问题陈述经过了仔细验证，被认为是具有科学依据、适定且内部一致的。它在计算神经科学领域提出了一个清晰且可形式化的任务。所有变量、参数和过程都得到了明确定义，可以直接、无歧义地实现。这个问题是使用信号检测理论中的标准线性高斯模型对辛普森悖论进行的一个经典演示，是一项有效且富有启发性的练习。\n\n问题的核心在于为一个感知决策任务模拟一个生成模型。我们已知：\n- 一个离散刺激 $S$，从集合 $\\{s_1, \\dots, s_m\\}$ 中均匀抽取。在本问题中，$S \\in \\{-1, 0, 1\\}$。\n- 一个连续决策变量 $D = w S + \\xi$，其中 $w \\in \\mathbb{R}$ 是一个权重，$\\xi$ 是独立高斯噪声，$\\xi \\sim \\mathcal{N}(0, \\sigma_d^2)$。该变量代表为决策所积累的内部证据。\n- 一个二元选择 $C = \\mathbb{I}[D \\ge 0]$，其中 $\\mathbb{I}[\\cdot]$ 是指示函数。这代表对决策变量进行的一次跨阈值操作，产生两种选择之一，我们将其标记为 $C=1$ 和 $C=0$。\n- 一个标量神经响应 $R = a S + b D + \\varepsilon$，其中 $a, b \\in \\mathbb{R}$ 是权重，$\\varepsilon$ 是独立高斯噪声，$\\varepsilon \\sim \\mathcal{N}(0, \\sigma_r^2)$。这模拟了一个神经元，其发放率同时受到外部刺激 $S$ 和内部决策变量 $D$ 的调制。\n\n目标是计算并比较两个量：\n1. 条件选择概率 $\\mathrm{CP}(s)$，为每个刺激水平 $s$ 定义。这是在刺激固定为 $S=s$ 的条件下，一次 $C=1$ 选择试验中的神经响应 $R$ 大于一次 $C=0$ 选择试验中的神经响应的概率。形式上，$\\mathrm{CP}(s) = \\mathbb{P}(R^{(1)}  R^{(0)} \\mid S=s)$。这个量旨在分离神经响应和选择之间的关系，而不受刺激本身的任何混杂影响。\n2. 聚合选择概率 $\\mathrm{CP}_{\\mathrm{agg}} = \\mathbb{P}(R^{(1)}  R^{(0)})$，其中比较是在所有试验中进行的，不考虑刺激水平。这个量在不考虑变化的刺激的情况下，简单地评估神经响应和选择之间的关系。\n\n辛普森悖论是一种统计现象，即在几个不同的数据组中出现一种趋势，但当这些组合并时，该趋势消失或反转。在此背景下，该悖论由特定条件定义：$\\mathrm{CP}_{\\mathrm{agg}}  0.5$ 同时对所有刺激水平 $s$ 都有 $\\mathrm{CP}(s)  0.5$。\n\n这个悖论的机制可以通过分析模型方程来理解。将 $D$ 的表达式代入 $R$ 的表达式中，得到：\n$$R = aS + bD + \\varepsilon = aS + b(wS + \\xi) + \\varepsilon = (a+bw)S + b\\xi + \\varepsilon$$\n当我们在固定刺激 $S=s$ 上进行条件化时，响应为 $R_s = (a+bw)s + b\\xi + \\varepsilon$。选择 $C$ 由 $D_s = ws + \\xi$ 的符号决定，这取决于决策噪声项 $\\xi$ 的值。因此，响应 $R_s$ 和选择 $C$ 之间的相关性是由项 $b\\xi$ 介导的。如果 $b>0$，一个较大的 $\\xi$ 值（使得选择 $C=1$ 的可能性更大）会导致一个较大的神经响应 $R_s$。因此，$C=1$ 时 $R_s$ 的分布将随机地大于 $C=0$ 时的分布，从而导致 $\\mathrm{CP}(s)  0.5$。因此，参数 $b$ 代表了神经元活动与决策过程之间的直接耦合。$b>0$ 的测试用例旨在展示 $\\mathrm{CP}(s)  0.5$。\n\n然而，在聚合情况下，我们不对 $S$ 进行控制，而 $S$ 充当了一个混杂变量。决策规则 $C=\\mathbb{I}[wS+\\xi \\ge 0]$（其中 $w>0$）的设计确保了高刺激值（例如 $S=1$）与选择 $C=1$ 强相关，而低刺激值（例如 $S=-1$）与选择 $C=0$ 相关。刺激 $S$ 也通过项 $aS$（以及 $(a+bw)S$）影响响应 $R$。如果系数 $a$ 足够负（如测试用例1中），高 $S$ 值（主要与 $C=1$ 相关）将系统性地产生较低的 $R$ 值，而低 $S$ 值（主要与 $C=0$ 相关）将系统性地产生较高的 $R$ 值。这种由参数 $a$ 驱动的混杂效应，可能会压倒由 $b$ 驱动的直接选择相关效应。这可能导致 $C=1$ 的聚合响应分布随机地小于 $C=0$ 的分布，从而产生 $\\mathrm{CP}_{\\mathrm{agg}}  0.5$，并因此造成悖论。\n\n实现将按以下步骤进行。对于每个参数集：\n1. 使用 `numpy` 从其指定分布中为 $S$、$\\xi$ 和 $\\varepsilon$ 生成随机样本，从而模拟 $N$ 次独立试验。为保证可复现性，每个案例都使用提供的种子初始化一个专用的随机数生成器。\n2. 基于生成模型为每次试验计算变量 $D$、$C$ 和 $R$。\n3. 选择概率被估计为受试者工作特征 (ROC) 曲线下的面积。按照规定，这通过 Mann-Whitney $U$ 统计量来计算：$\\widehat{\\mathrm{AUC}} = U / (n_1 n_0)$。我们使用 `scipy.stats.mannwhitneyu` 函数并设置 `alternative='greater'` 选项，该选项提供了与 $\\mathbb{P}(R^{(1)}  R^{(0)})$ 对应的正确 $U$ 统计量。\n4. 首先，使用所有 $N$ 次试验计算 $\\mathrm{CP}_{\\mathrm{agg}}$，仅按选择 $C \\in \\{0,1\\}$ 进行划分。\n5. 然后，对于每个刺激水平 $s \\in \\{-1,0,1\\}$，选择对应于 $S=s$ 的试验子集，并通过按选择划分该子集来计算 $\\mathrm{CP}(s)$。\n6. 最后，评估辛普森悖论的逻辑条件 $\\mathrm{CP}_{\\mathrm{agg}}  0.5 \\land (\\forall s, \\mathrm{CP}(s) > 0.5)$。将结果汇编成指定的列表格式。对所有四个测试用例重复此过程。",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import mannwhitneyu\n\ndef solve():\n    \"\"\"\n    Main function to run the Simpson's paradox simulation for all test cases.\n    \"\"\"\n    test_cases = [\n        # Case 1: Designed to exhibit Simpson’s paradox. a is strongly negative.\n        {\n            'N': 4000, 's_levels': [-1, 0, 1], 'w': 2.0, 'a': -3.0, 'b': 0.7, \n            'sigma_d': 1.0, 'sigma_r': 1.0, 'seed': 1\n        },\n        # Case 2: No reversal. a is positive, so stimulus and decision effects align.\n        {\n            'N': 4000, 's_levels': [-1, 0, 1], 'w': 2.0, 'a': 0.5, 'b': 0.7, \n            'sigma_d': 1.0, 'sigma_r': 1.0, 'seed': 2\n        },\n        # Case 3: Boundary case. a is negative but smaller, confounding is weaker.\n        {\n            'N': 4000, 's_levels': [-1, 0, 1], 'w': 2.0, 'a': -1.0, 'b': 0.7, \n            'sigma_d': 1.0, 'sigma_r': 1.0, 'seed': 3\n        },\n        # Case 4: Edge case. b=0, so no direct link from decision to response.\n        {\n            'N': 4000, 's_levels': [-1, 0, 1], 'w': 2.0, 'a': -3.0, 'b': 0.0, \n            'sigma_d': 1.0, 'sigma_r': 1.0, 'seed': 4\n        }\n    ]\n\n    all_results = []\n    \n    for params in test_cases:\n        result = _run_single_case(**params)\n        all_results.append(result)\n\n    # The final print must be a single line with the specified format.\n    # Python's str() on lists produces the required bracketed, comma-separated format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\n\ndef _calculate_auc(responses_1, responses_0):\n    \"\"\"\n    Calculates the Area Under the ROC Curve (AUC) using the Mann-Whitney U statistic.\n    AUC = P(response_1  response_0), estimated as U / (n1 * n0).\n    \"\"\"\n    n1 = len(responses_1)\n    n0 = len(responses_0)\n    \n    # If one of the choices was not made, the AUC is undefined.\n    # Problem statement guarantees this won't happen for the given parameters.\n    # We return 0.5 as a neutral value, though np.nan would also be appropriate.\n    if n1 == 0 or n0 == 0:\n        return 0.5\n\n    # 'alternative'='greater' tests H1: P(responses_1  responses_0)  0.5.\n    # The statistic returned is U1, where U = U1 / (n1 * n0).\n    u_statistic, _ = mannwhitneyu(responses_1, responses_0, alternative='greater')\n    \n    return u_statistic / (n1 * n0)\n\n\ndef _run_single_case(N, s_levels, w, a, b, sigma_d, sigma_r, seed):\n    \"\"\"\n    Runs the simulation for a single set of parameters and computes the results.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # Step 1: Generate N independent samples of (S, D, C, R)\n    # Generate stimulus S from a uniform discrete distribution\n    S = rng.choice(s_levels, size=N, replace=True)\n    \n    # Generate independent Gaussian noise terms\n    xi = rng.normal(loc=0.0, scale=sigma_d, size=N)\n    epsilon = rng.normal(loc=0.0, scale=sigma_r, size=N)\n    \n    # Compute decision variable D, binary choice C, and neural response R\n    D = w * S + xi\n    C = (D = 0).astype(int)  # C=1 if D=0, C=0 otherwise\n    R = a * S + b * D + epsilon\n\n    # Step 2: Compute aggregated Choice Probability (CP_agg)\n    R_agg_1 = R[C == 1]\n    R_agg_0 = R[C == 0]\n    cp_agg = _calculate_auc(R_agg_1, R_agg_0)\n\n    # Step 3: Compute conditional Choice Probability CP(s) for each stimulus level\n    cp_conditional_list = []\n    unique_s_levels = sorted(s_levels)\n    for s_val in unique_s_levels:\n        mask_s = (S == s_val)\n        \n        R_s = R[mask_s]\n        C_s = C[mask_s]\n        \n        R_cond_1 = R_s[C_s == 1]\n        R_cond_0 = R_s[C_s == 0]\n        \n        cp_s = _calculate_auc(R_cond_1, R_cond_0)\n        cp_conditional_list.append(cp_s)\n\n    # Step 4: Determine if Simpson's paradox occurred\n    # Condition: CP_agg   0.5 AND CP(s)  0.5 for all s\n    is_paradox = False\n    if cp_agg  0.5 and all(cp  0.5 for cp in cp_conditional_list):\n        is_paradox = True\n    \n    # Format the output a [CP_agg, [CP(s1), ..., CP(sm)], rev]\n    return [cp_agg, cp_conditional_list, is_paradox]\n\nsolve()\n```"
        },
        {
            "introduction": "认识到刺激作为混淆变量的潜在危害后 ，下一个关键问题便是如何有效地消除其影响。本实践将指导你实现并评估两种在实际研究中广泛使用的技术：回归残差法和分箱法。通过在已知真实解的模拟数据上比较这两种方法的表现，你将获得宝贵的实践经验，了解它们各自的优缺点与适用范围。",
            "id": "4145772",
            "problem": "您将获得模拟的单神经元试验数据，其中包含刺激值、神经反应以及二元选择。科学目标是估计选择概率（Choice Probability, CP），其定义为受试者工作特征（ROC）曲线下的面积，用于衡量神经活动预测选择的优劣，同时消除混淆性的刺激效应。您将实现并比较两种方法：一种是回归-残差方法，另一种是基于分箱的方法。您的程序必须为指定的测试套件生成量化结果。\n\n使用的基本原理和定义：\n- 选择概率（CP）定义为以选择为条件的标量神经反应分布之间的受试者工作特征（ROC）曲线下的面积。如果 $r_{1}$ 和 $r_{0}$ 分别表示在选择 $y=1$ 和 $y=0$ 条件下从反应分布中抽取的独立样本，那么 CP 等于\n$$CP \\equiv \\mathbb{P}\\left(r_{1}  r_{0}\\right) + \\frac{1}{2}\\,\\mathbb{P}\\left(r_{1} = r_{0}\\right)。$$\n- 从在 $y=1$ 条件下抽取的样本 $\\{x_{i}\\}_{i=1}^{n_{1}}$ 和在 $y=0$ 条件下抽取的样本 $\\{z_{j}\\}_{j=1}^{n_{0}}$ 中，CP 的无偏 U-统计量估计量为\n$$\\widehat{CP} = \\frac{1}{n_{1} n_{0}} \\sum_{i=1}^{n_{1}} \\sum_{j=1}^{n_{0}} \\left[ \\mathbf{1}\\{x_{i}  z_{j}\\} + \\frac{1}{2}\\mathbf{1}\\{x_{i} = z_{j}\\} \\right]，$$\n这等价于表示为曲线下面积的 Mann–Whitney 统计量。这种等价性允许通过秩次实现稳定的计算。\n- 由刺激驱动的基线活动由刺激 $s$ 的函数 $f(s)$ 建模，残差活动为 $r' = r - f(s)$，其中 $r$ 是观测到的反应。估计的残差使用通过经验风险最小化获得的 $\\hat{f}(s)$：\n$$\\hat{f} = \\arg\\min_{g \\in \\mathcal{G}} \\sum_{t=1}^{n} \\left(r_{t} - g(s_{t})\\right)^{2}，$$\n其中 $\\mathcal{G}$ 指定为 $s$ 的线性多项式或二次多项式。残差为 $r'_{t} = r_{t} - \\hat{f}(s_{t})$，CP 在 $r'$ 上计算。\n- 分箱方法将刺激轴划分为 $K$ 个箱，并计算箱内 CP（使用 $r$ 而不进行回归）以近似在 $s$ 上的条件化。总 CP 是各箱的加权平均：\n$$\\widehat{CP}_{\\text{bin}} = \\frac{\\sum_{k=1}^{K} n_{1k} n_{0k} \\,\\widehat{CP}_{k}}{\\sum_{k=1}^{K} n_{1k} n_{0k}}，$$\n其中 $n_{1k}$ 和 $n_{0k}$ 是在箱 $k$ 中 $y=1$ 和 $y=0$ 的试验次数，$\\widehat{CP}_{k}$ 是在箱 $k$ 内计算的 U-统计量 CP。$n_{1k} = 0$ 或 $n_{0k} = 0$ 的箱被排除。\n- 当残差反应为具有相同方差和不同均值的高斯分布时，即 $r' \\mid y=1 \\sim \\mathcal{N}(\\mu_{1}, \\sigma^{2})$ 和 $r' \\mid y=0 \\sim \\mathcal{N}(\\mu_{0}, \\sigma^{2})$，真实的 CP 等于\n$$CP^{\\star} = \\Phi\\!\\left( \\frac{\\mu_{1} - \\mu_{0}}{\\sqrt{2}\\,\\sigma} \\right)，$$\n其中 $\\Phi(\\cdot)$ 是标准正态分布的累积分布函数。在下面的模拟中，残差将被构造成满足此模型，且 $\\mu_{0} = 0$ 和 $\\mu_{1} = m$，因此基准真相 CP 为 $CP^{\\star} = \\Phi\\!\\left( \\frac{m}{\\sqrt{2}\\,\\sigma} \\right)$。\n\n每个测试用例的数据生成过程：\n- 从标准正态分布 $\\mathcal{N}(0,1)$ 中抽取 $n$ 个独立刺激 $s_{t}$。\n- 通过对带噪声的决策变量 $d_{t}$ 设置确定性阈值来生成选择，其中 $d_{t} = k\\,s_{t} + \\xi_{t}$ 且 $\\xi_{t} \\sim \\mathcal{N}(0,1)$ 独立同分布，以及 $y_{t} = \\mathbf{1}\\{d_{t}  0\\}$。这在刺激和选择之间创建了由 $k$ 控制的强依赖关系。\n- 生成残差活动 $r'_{t} = m\\,y_{t} + \\varepsilon_{t}$，其中 $\\varepsilon_{t} \\sim \\mathcal{N}(0, \\sigma^{2})$。这实现了具有均值偏移 $m$ 的等方差高斯模型。\n- 生成观测活动 $r_{t} = f(s_{t}) + r'_{t}$，其中 $f(s) = \\gamma_{1}\\,s + \\gamma_{2}\\,s^{2}$。\n\n算法任务：\n- 通过在指定的多项式类 $\\mathcal{G}$（线性或二次）上使用普通最小二乘法拟合 $\\hat{f}(s)$，计算残差 $r'$，并通过基于秩的 U-统计量计算 $\\widehat{CP}$，从而实现回归-残差 CP 估计器。\n- 实现基于分箱的 CP 估计器，在 $s$ 的范围内使用 $K$ 个等宽分箱，在每个包含两种选择的箱内计算 $\\widehat{CP}_{k}$，并使用上述加权平均公式进行聚合。\n- 还要计算在 $r$ 上的朴素 CP，不进行条件化或残差化，以显示由刺激-选择相关性引起的混淆效应。\n\n您的实现必须为每个测试用例生成以下结果：\n- 根据 $m$ 和 $\\sigma$ 使用 $CP^{\\star} = \\Phi\\!\\left( \\frac{m}{\\sqrt{2}\\,\\sigma} \\right)$ 计算的 $CP^{\\star}$。\n- 在残差 $r' = r - \\hat{f}(s)$ 上计算的 $\\widehat{CP}_{\\text{reg}}$。\n- 通过对 $r$ 进行分箱计算的 $\\widehat{CP}_{\\text{bin}}$。\n- 在 $r$ 上忽略 $s$ 计算的 $\\widehat{CP}_{\\text{naive}}$。\n- 相对于基准真相的差异：$\\widehat{CP}_{\\text{reg}} - CP^{\\star}$、$\\widehat{CP}_{\\text{bin}} - CP^{\\star}$ 和 $\\widehat{CP}_{\\text{naive}} - CP^{\\star}$。\n\n测试套件：\n- 案例 1（理想情况，模型正确，大样本）：\n    - $n = 4000$， $k = 3.0$， $m = 0.3$， $\\sigma = 1.0$， $\\gamma_{1} = 1.0$， $\\gamma_{2} = 0.0$，回归模型类 $\\mathcal{G}$ 为线性， $K = 10$，随机种子 $= 123$。\n- 案例 2（模型设定错误，二次基线但线性拟合）：\n    - $n = 4000$， $k = 3.0$， $m = 0.3$， $\\sigma = 1.0$， $\\gamma_{1} = 0.0$， $\\gamma_{2} = 1.5$，回归模型类 $\\mathcal{G}$ 为线性， $K = 20$，随机种子 $= 456$。\n- 案例 3（跨刺激的类别不平衡，小样本，二次拟合）：\n    - $n = 800$， $k = 8.0$， $m = 0.2$， $\\sigma = 1.0$， $\\gamma_{1} = 1.0$， $\\gamma_{2} = 0.5$，回归模型类 $\\mathcal{G}$ 为二次， $K = 8$，随机种子 $= 789$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个测试用例由一个包含七个十进制数的方括号括起来的逗号分隔列表表示，顺序如下：\n$[CP^{\\star}, \\widehat{CP}_{\\text{reg}}, \\widehat{CP}_{\\text{bin}}, \\widehat{CP}_{\\text{naive}}, \\widehat{CP}_{\\text{reg}} - CP^{\\star}, \\widehat{CP}_{\\text{bin}} - CP^{\\star}, \\widehat{CP}_{\\text{naive}} - CP^{\\star}]$。\n例如，总输出应类似于 $[[\\cdots],[\\cdots],[\\cdots]]$，不含空格。",
            "solution": "该问题要求估计选择概率（Choice Probability, CP），这是一个衡量神经元活动预测二元选择优劣的指标，同时需要考虑外部刺激的混淆影响。我们的任务是在基准真相 CP 已知的模拟数据上，实现并比较两种消除混淆的方法——一种是回归-残差方法，另一种是基于分箱的方法。\n\n### 数据生成过程\n\n对于每个试验 $t=1, \\dots, n$，数据按照以下过程模拟生成：\n1. 从标准正态分布中抽取一个刺激值 $s_t$：\n    $$s_t \\sim \\mathcal{N}(0, 1)$$\n2. 生成一个二元选择 $y_t \\in \\{0, 1\\}$。首先，一个潜决策变量 $d_t$ 被计算为刺激的函数加上噪声：\n    $$d_t = k \\cdot s_t + \\xi_t, \\quad \\text{其中} \\quad \\xi_t \\sim \\mathcal{N}(0, 1)$$\n    然后，选择由 $d_t$ 上的一个阈值确定：\n    $$y_t = \\mathbf{1}\\{d_t  0\\}$$\n    参数 $k$ 控制刺激 $s_t$ 和选择 $y_t$ 之间的相关强度。较大的 $k$ 意味着刺激对选择的预测性更强。\n3. 神经活动被建模为刺激依赖分量和选择依赖分量之和。我们称之为“残差活动”的选择依赖分量 $r'_t$ 生成如下：\n    $$r'_t = m \\cdot y_t + \\varepsilon_t, \\quad \\text{其中} \\quad \\varepsilon_t \\sim \\mathcal{N}(0, \\sigma^2)$$\n    这种构造确保了残差活动在以选择为条件的条件下服从高斯分布：$r'_t | (y_t=0) \\sim \\mathcal{N}(0, \\sigma^2)$ 以及 $r'_t | (y_t=1) \\sim \\mathcal{N}(m, \\sigma^2)$。\n4. 观测到的神经反应 $r_t$ 是残差活动与刺激依赖的基线活动 $f(s_t)$ 之和：\n    $$r_t = f(s_t) + r'_t$$\n    基线是刺激的二次函数：\n    $$f(s) = \\gamma_1 s + \\gamma_2 s^2$$\n\n### 基准真相选择概率 ($CP^{\\star}$)\n\n给定残差活动 $r'$ 的生成模型，以选择为条件的 $r'$ 分布为 $r' \\mid y=0 \\sim \\mathcal{N}(\\mu_0, \\sigma^2)$ 和 $r' \\mid y=1 \\sim \\mathcal{N}(\\mu_1, \\sigma^2)$，其中 $\\mu_0=0$ 和 $\\mu_1=m$。理论上的 CP 是从 $y=1$ 分布中随机抽取的值大于从 $y=0$ 分布中随机抽取的值的概率。对于两个独立的高斯变量，它们的差也是高斯分布。设 $r'_1 \\sim \\mathcal{N}(m, \\sigma^2)$ 和 $r'_0 \\sim \\mathcal{N}(0, \\sigma^2)$。它们的差 $r'_1 - r'_0$ 服从 $\\mathcal{N}(m, 2\\sigma^2)$。CP 是 $\\mathbb{P}(r'_1  r'_0) = \\mathbb{P}(r'_1 - r'_0  0)$。对这个变量进行标准化得到：\n$$CP^{\\star} = \\mathbb{P}\\left(\\frac{(r'_1 - r'_0) - m}{\\sqrt{2\\sigma^2}}  \\frac{0 - m}{\\sqrt{2\\sigma^2}}\\right) = \\mathbb{P}\\left(Z  -\\frac{m}{\\sqrt{2}\\sigma}\\right) = 1 - \\Phi\\left(-\\frac{m}{\\sqrt{2}\\sigma}\\right) = \\Phi\\left(\\frac{m}{\\sqrt{2}\\sigma}\\right)$$\n其中 $Z \\sim \\mathcal{N}(0,1)$，$\\Phi(\\cdot)$ 是标准正态分布的累积分布函数 (CDF)。\n\n### 选择概率估计量\n\n我们将使用三种不同的方法在模拟数据上计算 CP。每种方法的核心计算都依赖于 Mann-Whitney U-统计量，它为 CP 提供了一个无偏估计量。给定选择 $y=1$ 的反应样本 $\\{x_i\\}_{i=1}^{n_1}$ 和选择 $y=0$ 的反应样本 $\\{z_j\\}_{j=1}^{n_0}$，估计量为：\n$$\\widehat{CP} = \\frac{1}{n_1 n_0} \\sum_{i=1}^{n_1} \\sum_{j=1}^{n_0} \\left[ \\mathbf{1}\\{x_i  z_j\\} + \\frac{1}{2}\\mathbf{1}\\{x_i = z_j\\} \\right]$$\n这等价于 ROC 曲线下面积 (AUC)，并且可以使用基于秩的统计高效计算。\n\n1.  **朴素估计量 ($\\widehat{CP}_{\\text{naive}}$)**：该估计量直接在观测到的神经反应 $\\{r_t\\}_{t=1}^n$ 上计算，忽略了刺激信息 $\\{s_t\\}_{t=1}^n$。因为反应 $r_t$ 和选择 $y_t$ 都受到刺激 $s_t$ 的影响，该估计量预计会有偏差，反映了刺激的混淆效应。\n\n2.  **回归-残差估计量 ($\\widehat{CP}_{\\text{reg}}$)**：该方法试图通过首先对刺激和反应之间的关系进行建模，然后对残差计算 CP 来消除刺激的混淆。\n    -   通过对 $r_t$ 在 $s_t$ 的多项式基上进行普通最小二乘 (OLS) 回归，获得刺激-反应函数 $\\hat{f}(s)$ 的一个估计。每个测试用例都指定了多项式类 $\\mathcal{G}$（线性或二次）。对于线性模型，我们拟合 $g(s) = \\beta_0 + \\beta_1 s$。对于二次模型，我们拟合 $g(s) = \\beta_0 + \\beta_1 s + \\beta_2 s^2$。系数 $\\hat{\\beta}$ 通过最小化平方误差和来找到：$\\sum_{t=1}^n (r_t - g(s_t))^2$。\n    -   然后计算残差为 $\\hat{r}'_t = r_t - \\hat{f}(s_t)$，其中 $\\hat{f}(s_t)$ 是拟合模型的预测值。\n    -   $\\widehat{CP}_{\\text{reg}}$ 使用 U-统计量估计量在残差集 $\\{\\hat{r}'_t\\}_{t=1}^n$ 和选择集 $\\{y_t\\}_{t=1}^n$ 上计算。如果回归模型正确地捕捉了 $f(s)$ 的真实形式，该方法应能产生 $CP^{\\star}$ 的无偏估计。模型设定错误可能导致残留的混淆和有偏的估计。\n\n3.  **基于分箱的估计量 ($\\widehat{CP}_{\\text{bin}}$)**：这种非参数方法通过将刺激的范围划分为 $K$ 个离散的、等宽的箱来控制刺激。\n    -   将观测到的刺激范围 $[\\min(s), \\max(s)]$ 划分为 $K$ 个区间。\n    -   对于每个箱 $k=1, \\dots, K$，使用 U-统计量对落入该箱的试验的原始反应 $r_t$ 计算一个局部的 CP，即 $\\widehat{CP}_k$。\n    -   该计算仅对包含两种选择类别的试验的箱进行（即，$n_{1k}  0$ 且 $n_{0k}  0$，其中 $n_{yk}$ 是在箱 $k$ 中选择为 $y$ 的试验次数）。\n    -   总估计量 $\\widehat{CP}_{\\text{bin}}$ 是各箱 CP 的加权平均，权重与每个箱中两种选择的样本量乘积成正比：\n    $$\\widehat{CP}_{\\text{bin}} = \\frac{\\sum_{k=1}^{K} n_{1k} n_{0k} \\,\\widehat{CP}_{k}}{\\sum_{k=1}^{K} n_{1k} n_{0k}}$$\n    这种方法近似于以刺激为条件，并且比回归方法对 $f(s)$ 的函数形式更不敏感，但其准确性可能取决于箱数 $K$ 和每个箱中的数据量。\n\n所实现的程序将对三个不同的测试用例执行这些计算，每个用例都旨在探测估计量在不同条件下的性能，例如正确的模型设定、模型设定错误和小样本量。",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import mannwhitneyu\nfrom scipy.special import erf\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n\n    # Manually implement norm.cdf to avoid scipy version issues,\n    # as scipy versions  1.12 might not have it as a ufunc.\n    # The definition is Phi(x) = 0.5 * (1 + erf(x/sqrt(2))).\n    def norm_cdf(x):\n        return 0.5 * (1.0 + erf(x / np.sqrt(2.0)))\n\n    def compute_cp(responses, choices):\n        \"\"\"\n        Computes Choice Probability using the Mann-Whitney U statistic.\n        \n        Args:\n            responses (np.array): Neural responses.\n            choices (np.array): Binary choices (0 or 1).\n            \n        Returns:\n            float: The estimated Choice Probability (AUC).\n        \"\"\"\n        r1 = responses[choices == 1]\n        r0 = responses[choices == 0]\n        n1, n0 = len(r1), len(r0)\n        \n        if n1 == 0 or n0 == 0:\n            return np.nan\n        \n        # mannwhitneyu calculates U for r1  r0, including ties.\n        # CP = U / (n1 * n0)\n        u_statistic, _ = mannwhitneyu(r1, r0, alternative='greater')\n        return u_statistic / (n1 * n0)\n\n    def run_case(n, k, m, sigma, gamma1, gamma2, reg_model_type, K, seed):\n        \"\"\"\n        Runs one full simulation and analysis case.\n        \"\"\"\n        rng = np.random.default_rng(seed)\n\n        # 1. Data Generation\n        s = rng.normal(loc=0, scale=1, size=n)\n        xi = rng.normal(loc=0, scale=1, size=n)\n        d = k * s + xi\n        y = (d  0).astype(int)\n        \n        epsilon = rng.normal(loc=0, scale=sigma, size=n)\n        r_prime = m * y + epsilon\n        \n        f_s = gamma1 * s + gamma2 * s**2\n        r = f_s + r_prime\n\n        # 2. Calculate ground truth CP\n        cp_star = norm_cdf(m / (np.sqrt(2) * sigma))\n\n        # 3. Calculate Naive CP\n        cp_naive = compute_cp(r, y)\n\n        # 4. Calculate Regression-Residual CP\n        if reg_model_type == 'linear':\n            # Design matrix for g(s) = beta_0 + beta_1*s\n            X = np.vstack([np.ones(n), s]).T\n        elif reg_model_type == 'quadratic':\n            # Design matrix for g(s) = beta_0 + beta_1*s + beta_2*s^2\n            X = np.vstack([np.ones(n), s, s**2]).T\n        else:\n            raise ValueError(\"Invalid regression model type\")\n\n        coeffs, _, _, _ = np.linalg.lstsq(X, r, rcond=None)\n        f_hat = X @ coeffs\n        r_residuals = r - f_hat\n        cp_reg = compute_cp(r_residuals, y)\n\n        # 5. Calculate Binning-Based CP\n        s_min, s_max = np.min(s), np.max(s)\n        # Add small epsilon to include the max value in the last bin\n        bin_edges = np.linspace(s_min, s_max + 1e-9, K + 1)\n        \n        s_bin_indices = np.digitize(s, bin_edges)\n\n        cp_bin_num = 0.0\n        cp_bin_den = 0.0\n\n        for i in range(1, K + 1):\n            bin_mask = (s_bin_indices == i)\n            if not np.any(bin_mask):\n                continue\n\n            r_bin = r[bin_mask]\n            y_bin = y[bin_mask]\n\n            n1k = np.sum(y_bin == 1)\n            n0k = np.sum(y_bin == 0)\n\n            if n1k  0 and n0k  0:\n                cp_k = compute_cp(r_bin, y_bin)\n                weight = n1k * n0k\n                cp_bin_num += weight * cp_k\n                cp_bin_den += weight\n        \n        cp_bin = cp_bin_num / cp_bin_den if cp_bin_den  0 else np.nan\n\n        # 6. Calculate differences\n        diff_reg = cp_reg - cp_star\n        diff_bin = cp_bin - cp_star\n        diff_naive = cp_naive - cp_star\n\n        return [cp_star, cp_reg, cp_bin, cp_naive, diff_reg, diff_bin, diff_naive]\n\n    test_cases = [\n        # Case 1 (happy path, correct model, large sample)\n        {'n': 4000, 'k': 3.0, 'm': 0.3, 'sigma': 1.0, 'gamma1': 1.0, 'gamma2': 0.0, 'reg_model_type': 'linear', 'K': 10, 'seed': 123},\n        # Case 2 (model mis-specification, quadratic baseline but linear fit)\n        {'n': 4000, 'k': 3.0, 'm': 0.3, 'sigma': 1.0, 'gamma1': 0.0, 'gamma2': 1.5, 'reg_model_type': 'linear', 'K': 20, 'seed': 456},\n        # Case 3 (class imbalance across stimulus, small sample, quadratic fit)\n        {'n': 800, 'k': 8.0, 'm': 0.2, 'sigma': 1.0, 'gamma1': 1.0, 'gamma2': 0.5, 'reg_model_type': 'quadratic', 'K': 8, 'seed': 789},\n    ]\n\n    all_results = []\n    for case_params in test_cases:\n        results = run_case(**case_params)\n        all_results.append(results)\n    \n    # Format the final output string\n    # e.g., [[val1,val2,...],[val1,val2,...],[val1,val2,...]]\n    case_strings = []\n    for case_result in all_results:\n        # Each number is converted to a string using default float formatting\n        case_strings.append(f\"[{','.join(map(str, case_result))}]\")\n    \n    final_output = f\"[{','.join(case_strings)}]\"\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}