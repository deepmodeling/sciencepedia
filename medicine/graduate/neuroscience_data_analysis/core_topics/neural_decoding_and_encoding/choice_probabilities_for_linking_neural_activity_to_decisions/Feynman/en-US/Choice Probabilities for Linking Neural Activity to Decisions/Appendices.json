{
    "hands_on_practices": [
        {
            "introduction": "To build a solid foundation, we begin with the theoretical relationship between a neuron's ability to discriminate stimuli and its correlation with an animal's choice. This exercise asks you to derive the precise mathematical link between the neurometric sensitivity index, $d'$, and the Choice Probability (CP) under the idealized, but foundational, assumption of equal-variance Gaussian distributions . Mastering this derivation provides a crucial anchor, connecting the abstract concept of CP to the well-established framework of Signal Detection Theory and clarifying how neural coding properties translate into observable choice-related signals.",
            "id": "4145816",
            "problem": "In a two-alternative forced choice decision task, consider a single neuron's spike count response $r$ measured in a fixed time window. For a fixed stimulus condition $s$, suppose the conditional distributions of $r$ given the subject's eventual choice are Gaussian with equal variance but different means: $r \\mid (\\text{choice} = +, s) \\sim \\mathcal{N}(\\mu_{+}(s), \\sigma^{2}(s))$ and $r \\mid (\\text{choice} = -, s) \\sim \\mathcal{N}(\\mu_{-}(s), \\sigma^{2}(s))$. Assume trial-to-trial independence and that draws from these two conditional distributions are independent across trials.\n\nDefine the neurometric sensitivity index $d^{\\prime}(s)$ for this neuron at stimulus $s$ by\n$$\nd^{\\prime}(s) \\equiv \\frac{\\mu_{+}(s) - \\mu_{-}(s)}{\\sigma(s)}.\n$$\nDefine the choice probability (CP) at stimulus $s$, denoted $\\mathrm{CP}(s)$, as the area under the Receiver Operating Characteristic (ROC) curve for classifying the subject’s choice from the single-trial response $r$ while conditioning on $s$. Equivalently, $\\mathrm{CP}(s)$ equals the probability that a response drawn independently from $r \\mid (\\text{choice} = +, s)$ exceeds a response drawn independently from $r \\mid (\\text{choice} = -, s)$.\n\nStarting only from these definitions and the properties of Gaussian random variables, derive a closed-form expression for $\\mathrm{CP}(s)$ as a function of $d^{\\prime}(s)$ in the equal-variance Gaussian case. Your derivation should explicitly articulate the probabilistic transformation(s) you use and justify each step. In words, also explain the role of conditioning on the stimulus $s$ in the mapping you derive, and under what circumstances the mapping would fail if responses were pooled across multiple stimulus values or if the equal-variance assumption were violated.\n\nProvide your final result as a single closed-form analytic expression for $\\mathrm{CP}(s)$ in terms of $d^{\\prime}(s)$. No numerical approximation or rounding is required.",
            "solution": "The problem is well-posed and scientifically grounded within the framework of signal detection theory applied to neuroscience. All definitions are standard and the assumptions, while simplifying, are common in the field. I will now proceed with the derivation.\n\nThe objective is to derive a closed-form expression for the choice probability, $\\mathrm{CP}(s)$, as a function of the neurometric sensitivity index, $d^{\\prime}(s)$.\n\nWe are given two definitions for the choice probability, $\\mathrm{CP}(s)$. We will use the probabilistic definition, which states that $\\mathrm{CP}(s)$ is the probability that a response drawn independently from the distribution for choice `$+$` exceeds a response drawn independently from the distribution for choice `$-$`. Let $r_{+}$ be a random variable representing the neural response conditioned on choice `$+$` and stimulus $s$, and let $r_{-}$ be a random variable representing the neural response conditioned on choice `$-$` and stimulus $s$. According to the problem statement, their distributions are:\n$$\nr_{+} \\sim \\mathcal{N}(\\mu_{+}(s), \\sigma^{2}(s))\n$$\n$$\nr_{-} \\sim \\mathcal{N}(\\mu_{-}(s), \\sigma^{2}(s))\n$$\nThe variables $r_{+}$ and $r_{-}$ are stated to be drawn independently. The choice probability is then defined as:\n$$\n\\mathrm{CP}(s) = P(r_{+} > r_{-})\n$$\nThis inequality can be rewritten by considering the difference between the two random variables. Let us define a new random variable, $\\Delta r$, as their difference:\n$$\n\\Delta r = r_{+} - r_{-}\n$$\nThe probability we seek is then $P(\\Delta r > 0)$.\n\nTo compute this probability, we must first determine the probability distribution of $\\Delta r$. A fundamental property of Gaussian random variables is that their linear combination is also a Gaussian random variable. Since $r_{+}$ and $r_{-}$ are independent Gaussian variables, their difference, $\\Delta r$, is also Gaussian.\n\nThe mean of $\\Delta r$, denoted $E[\\Delta r]$, is the difference of the individual means:\n$$\nE[\\Delta r] = E[r_{+}] - E[r_{-}] = \\mu_{+}(s) - \\mu_{-}(s)\n$$\nThe variance of $\\Delta r$, denoted $\\mathrm{Var}(\\Delta r)$, is the sum of the individual variances because $r_{+}$ and $r_{-}$ are independent:\n$$\n\\mathrm{Var}(\\Delta r) = \\mathrm{Var}(r_{+}) + \\mathrm{Var}(r_{-}) = \\sigma^{2}(s) + \\sigma^{2}(s) = 2\\sigma^{2}(s)\n$$\nThe standard deviation of $\\Delta r$ is therefore $\\mathrm{SD}(\\Delta r) = \\sqrt{2\\sigma^{2}(s)} = \\sqrt{2}\\sigma(s)$.\n\nSo, the distribution of the difference variable is:\n$$\n\\Delta r \\sim \\mathcal{N}(\\mu_{+}(s) - \\mu_{-}(s), 2\\sigma^{2}(s))\n$$\nWe now compute the probability $P(\\Delta r > 0)$ for this distribution. To do this, we standardize the random variable $\\Delta r$ by subtracting its mean and dividing by its standard deviation, which transforms it into a standard normal variable $Z \\sim \\mathcal{N}(0, 1)$:\n$$\nZ = \\frac{\\Delta r - E[\\Delta r]}{\\mathrm{SD}(\\Delta r)} = \\frac{\\Delta r - (\\mu_{+}(s) - \\mu_{-}(s))}{\\sqrt{2}\\sigma(s)}\n$$\nWe can rewrite the inequality $\\Delta r > 0$ in terms of the standard normal variable $Z$:\n$$\n\\Delta r > 0 \\implies \\Delta r - (\\mu_{+}(s) - \\mu_{-}(s)) > -(\\mu_{+}(s) - \\mu_{-}(s))\n$$\n$$\n\\frac{\\Delta r - (\\mu_{+}(s) - \\mu_{-}(s))}{\\sqrt{2}\\sigma(s)} > \\frac{-(\\mu_{+}(s) - \\mu_{-}(s))}{\\sqrt{2}\\sigma(s)}\n$$\n$$\nZ > -\\frac{\\mu_{+}(s) - \\mu_{-}(s)}{\\sqrt{2}\\sigma(s)}\n$$\nThe problem defines the neurometric sensitivity index as $d^{\\prime}(s) = \\frac{\\mu_{+}(s) - \\mu_{-}(s)}{\\sigma(s)}$. Substituting this definition into our inequality gives:\n$$\nZ > -\\frac{d^{\\prime}(s)}{\\sqrt{2}}\n$$\nThe choice probability is the probability of this event:\n$$\n\\mathrm{CP}(s) = P\\left(Z > -\\frac{d^{\\prime}(s)}{\\sqrt{2}}\\right)\n$$\nLet $\\Phi(z)$ denote the cumulative distribution function (CDF) of the standard normal distribution, i.e., $\\Phi(z) = P(Z \\le z)$. The probability $P(Z > a)$ is equal to $1 - P(Z \\le a) = 1 - \\Phi(a)$. Applying this, we get:\n$$\n\\mathrm{CP}(s) = 1 - \\Phi\\left(-\\frac{d^{\\prime}(s)}{\\sqrt{2}}\\right)\n$$\nDue to the symmetry of the standard normal distribution about $0$, we have the property that $\\Phi(-z) = 1 - \\Phi(z)$ for any $z$. Therefore, we can simplify the expression for $\\mathrm{CP}(s)$:\n$$\n\\mathrm{CP}(s) = 1 - \\left(1 - \\Phi\\left(\\frac{d^{\\prime}(s)}{\\sqrt{2}}\\right)\\right) = \\Phi\\left(\\frac{d^{\\prime}(s)}{\\sqrt{2}}\\right)\n$$\nThis is the closed-form expression for the choice probability as a function of the neurometric sensitivity index.\n\nRegarding the second part of the question:\nThe role of conditioning on the stimulus $s$ is paramount. The parameters of the neural response distributions—$\\mu_{+}(s)$, $\\mu_{-}(s)$, and $\\sigma(s)$—are all functions of the stimulus $s$. Consequently, both $d^{\\prime}(s)$ and $\\mathrm{CP}(s)$ are themselves functions of the stimulus. The derived mapping, $\\mathrm{CP}(s) = \\Phi(d^{\\prime}(s)/\\sqrt{2})$, holds specifically for the pair of distributions defined at a single, fixed stimulus level $s$. If one were to pool neural responses across different stimulus values (e.g., $s_{1}, s_{2}, \\dots$), the resulting pooled data would no longer be drawn from a single Gaussian distribution. Instead, it would be a mixture of Gaussians. The simple relationship derived here, which relies on the properties of a single pair of Gaussians, would fail. The overall choice probability would be a weighted average of the stimulus-specific choice probabilities, but this cannot be related in a simple way to a single $d^{\\prime}$ value computed naively from the pooled, non-Gaussian data.\n\nThe equal-variance assumption is also critical. If the variances were unequal, such that $r_{+} \\sim \\mathcal{N}(\\mu_{+}(s), \\sigma_{+}^{2}(s))$ and $r_{-} \\sim \\mathcal{N}(\\mu_{-}(s), \\sigma_{-}^{2}(s))$, the variance of the difference $\\Delta r$ would be $\\mathrm{Var}(\\Delta r) = \\sigma_{+}^{2}(s) + \\sigma_{-}^{2}(s)$. The inequality for the standardized variable $Z$ would become:\n$$\nZ > -\\frac{\\mu_{+}(s) - \\mu_{-}(s)}{\\sqrt{\\sigma_{+}^{2}(s) + \\sigma_{-}^{2}(s)}}\n$$\nThe argument of the CDF would then be $\\frac{\\mu_{+}(s) - \\mu_{-}(s)}{\\sqrt{\\sigma_{+}^{2}(s) + \\sigma_{-}^{2}(s)}}$. This expression cannot be simplified to a function of only $d^{\\prime}(s) \\equiv \\frac{\\mu_{+}(s) - \\mu_{-}(s)}{\\sigma(s)}$, where $\\sigma(s)$ would typically be some form of pooled standard deviation. The relationship between CP and $d^{\\prime}$ would become more complex, additionally depending on the ratio of the standard deviations, $\\sigma_{+}(s)/\\sigma_{-}(s)$.",
            "answer": "$$\n\\boxed{\\Phi\\left(\\frac{d^{\\prime}(s)}{\\sqrt{2}}\\right)}\n$$"
        },
        {
            "introduction": "The previous exercise highlights that both CP and $d'$ are functions of the stimulus, $s$. But what happens if we ignore this fact and pool data across different stimulus conditions? This hands-on simulation practice demonstrates a critical pitfall in data analysis known as Simpson's paradox, where a statistical trend observed within sub-groups reverses when the groups are combined . By constructing a scenario where the true, conditional CP is consistently above $0.5$ but the aggregated, naive CP is below $0.5$, you will gain a powerful and intuitive understanding of how confounding variables can create entirely misleading conclusions, cementing the importance of careful conditioning in neuroscience data analysis.",
            "id": "4145788",
            "problem": "You are tasked with constructing a simulation that demonstrates Simpson’s paradox in the context of choice probability for linking neural activity to decisions, and then resolving the paradox by conditioning on stimulus level. Work in purely mathematical terms, using a linear-Gaussian generative model for a binary decision and a single neuron’s response. The problem requires careful adherence to definitions from probability theory and classical signal detection analysis.\n\nFundamental base:\n- Let the stimulus be a discrete random variable $S \\in \\{s_1,\\dots,s_m\\}$.\n- Let the decision variable be $D = w S + \\xi$, where $w \\in \\mathbb{R}$ and $\\xi \\sim \\mathcal{N}(0,\\sigma_d^2)$ is independent Gaussian noise.\n- Let the binary choice be $C = \\mathbb{I}[D \\ge 0]$, where $\\mathbb{I}[\\cdot]$ is the indicator function.\n- Let the neuron’s scalar response be $R = a S + b D + \\varepsilon$, where $a,b \\in \\mathbb{R}$ and $\\varepsilon \\sim \\mathcal{N}(0,\\sigma_r^2)$ is independent Gaussian noise.\n\nDefinitions:\n- The Choice Probability (CP) for a neuron, conditioned on a fixed stimulus $S=s$, is defined as the probability that the neuron’s response on a randomly selected trial with choice $C=1$ exceeds the neuron’s response on a randomly selected trial with choice $C=0$, at the same stimulus: $$\\mathrm{CP}(s) = \\mathbb{P}\\!\\left(R^{(1)} > R^{(0)} \\mid S=s\\right),$$ where $R^{(1)}$ is drawn from the distribution of $R$ conditioned on $(C=1,S=s)$ and $R^{(0)}$ is drawn from the distribution of $R$ conditioned on $(C=0,S=s)$. This quantity is equal to the area under the Receiver Operating Characteristic (ROC) curve computed from the two conditional response distributions.\n- The aggregated Choice Probability, ignoring stimulus, is $$\\mathrm{CP}_{\\mathrm{agg}} = \\mathbb{P}\\!\\left(R^{(1)} > R^{(0)}\\right),$$ where $R^{(1)}$ is drawn from the distribution of $R$ conditioned on $C=1$ and $R^{(0)}$ is drawn from the distribution of $R$ conditioned on $C=0$, with no constraint on $S$. By the law of total probability, this aggregates over a mixture of stimulus levels and can therefore conflate stimulus-dependent effects in $R$ with decision-related effects.\n\nObjective:\n- Simulate $N$ trials under the specified generative model for several parameter sets. For each parameter set:\n  1. Generate $N$ independent samples of $(S,D,C,R)$.\n  2. Compute $\\mathrm{CP}_{\\mathrm{agg}}$ using the empirical area under the ROC curve formed by the distributions of $R$ for $C=1$ and $C=0$.\n  3. Compute $\\mathrm{CP}(s)$ for each stimulus level $s \\in \\{s_1,\\dots,s_m\\}$ using only trials with $S=s$.\n  4. Determine whether Simpson’s paradox occurs, defined here as the event that $$\\mathrm{CP}_{\\mathrm{agg}} < 0.5 \\quad \\text{and} \\quad \\mathrm{CP}(s) > 0.5 \\ \\text{for all stimulus levels } s.$$\n- Use the Mann–Whitney $U$ statistic formulation to estimate the area under the ROC curve in a way that appropriately handles ties: if $X$ are the responses for $C=1$ and $Y$ are the responses for $C=0$, $$\\widehat{\\mathrm{AUC}} = \\frac{U}{n_X n_Y},$$ where $U$ is computed from average ranks over the concatenated sample and $n_X$, $n_Y$ are sample sizes.\n\nScientific realism:\n- All variables must be simulated according to the specified linear-Gaussian model with independent noise terms $\\xi$ and $\\varepsilon$.\n- Stimulus levels must be chosen so that both choices occur with nonzero frequency at each $s$, ensuring empirical $\\mathrm{CP}(s)$ is well-defined.\n\nTest suite:\n- Provide a test suite of $4$ parameter sets that probe distinct regimes:\n  1. A case designed to exhibit Simpson’s paradox (aggregated reverse sign relative to within-stimulus CP):\n     - $N = 4000$, $S \\in \\{-1,0,1\\}$ uniformly, $w = 2.0$, $a = -3.0$, $b = 0.7$, $\\sigma_d = 1.0$, $\\sigma_r = 1.0$, seed $= 1$.\n  2. A case with no reversal (aggregated and conditioned CP align in sign):\n     - $N = 4000$, $S \\in \\{-1,0,1\\}$ uniformly, $w = 2.0$, $a = 0.5$, $b = 0.7$, $\\sigma_d = 1.0$, $\\sigma_r = 1.0$, seed $= 2$.\n  3. A boundary case where aggregated CP is approximately $0.5$ while within-stimulus CP deviates from $0.5$:\n     - $N = 4000$, $S \\in \\{-1,0,1\\}$ uniformly, $w = 2.0$, $a = -1.0$, $b = 0.7$, $\\sigma_d = 1.0$, $\\sigma_r = 1.0$, seed $= 3$.\n  4. An edge case where within-stimulus CP is exactly $0.5$ due to zero coupling to the decision variable:\n     - $N = 4000$, $S \\in \\{-1,0,1\\}$ uniformly, $w = 2.0$, $a = -3.0$, $b = 0.0$, $\\sigma_d = 1.0$, $\\sigma_r = 1.0$, seed $= 4$.\n\nOutput specification:\n- For each parameter set, output a list of the form $[\\mathrm{CP}_{\\mathrm{agg}}, [\\mathrm{CP}(s_1),\\dots,\\mathrm{CP}(s_m)], \\mathrm{rev}]$, where $\\mathrm{rev}$ is a boolean indicating whether Simpson’s paradox occurred under the above definition.\n- Your program should produce a single line of output containing the results for all $4$ test cases as a comma-separated list enclosed in square brackets, for example, $[[\\dots],[\\dots],[\\dots],[\\dots]]$. No additional text should be printed.",
            "solution": "The problem statement has been meticulously validated and is found to be scientifically grounded, well-posed, and internally consistent. It presents a clear and formalizable task in computational neuroscience. All variables, parameters, and procedures are explicitly defined, enabling a direct and unambiguous implementation. The problem is a classical demonstration of Simpson's paradox using a standard linear-Gaussian model from signal detection theory, which is a valid and instructive exercise.\n\nThe core of the problem lies in simulating a generative model for a perceptual decision-making task. We are given:\n- A discrete stimulus $S$ drawn uniformly from a set $\\{s_1, \\dots, s_m\\}$. In this problem, $S \\in \\{-1, 0, 1\\}$.\n- A continuous decision variable $D = w S + \\xi$, where $w \\in \\mathbb{R}$ is a weight and $\\xi$ is independent Gaussian noise, $\\xi \\sim \\mathcal{N}(0, \\sigma_d^2)$. This variable represents the internal evidence accumulated for the decision.\n- A binary choice $C = \\mathbb{I}[D \\ge 0]$, where $\\mathbb{I}[\\cdot]$ is the indicator function. This represents a threshold-crossing operation on the decision variable, resulting in one of two choices, which we label $C=1$ and $C=0$.\n- A scalar neural response $R = a S + b D + \\varepsilon$, where $a, b \\in \\mathbb{R}$ are weights, and $\\varepsilon$ is independent Gaussian noise, $\\varepsilon \\sim \\mathcal{N}(0, \\sigma_r^2)$. This models a neuron whose firing rate is modulated by both the external stimulus $S$ and the internal decision variable $D$.\n\nThe objective is to compute and compare two quantities:\n1. The conditional Choice Probability, $\\mathrm{CP}(s)$, defined for each stimulus level $s$. This is the probability that the neural response $R$ is larger on a trial with choice $C=1$ than on a trial with choice $C=0$, given that the stimulus is fixed to $S=s$. Formally, $\\mathrm{CP}(s) = \\mathbb{P}(R^{(1)} > R^{(0)} \\mid S=s)$. This quantity is designed to isolate the relationship between the neural response and the choice, free from any confounding influence of the stimulus itself.\n2. The aggregated Choice Probability, $\\mathrm{CP}_{\\mathrm{agg}} = \\mathbb{P}(R^{(1)} > R^{(0)})$, where the comparison is made across all trials, irrespective of the stimulus level. This quantity naively assesses the relationship between neural response and choice without accounting for the varying stimulus.\n\nSimpson's paradox is a statistical phenomenon where a trend appears in several different groups of data but disappears or reverses when these groups are combined. In this context, the paradox is defined by the specific condition $\\mathrm{CP}_{\\mathrm{agg}} < 0.5$ while concurrently $\\mathrm{CP}(s) > 0.5$ for all stimulus levels $s$.\n\nThe mechanism for this paradox can be understood by analyzing the model equations. Substituting the expression for $D$ into the expression for $R$ yields:\n$$R = aS + bD + \\varepsilon = aS + b(wS + \\xi) + \\varepsilon = (a+bw)S + b\\xi + \\varepsilon$$\nWhen we condition on a fixed stimulus $S=s$, the response is $R_s = (a+bw)s + b\\xi + \\varepsilon$. The choice $C$ is determined by the sign of $D_s = ws + \\xi$, which depends on the value of the decision noise term $\\xi$. The correlation between the response $R_s$ and the choice $C$ is therefore mediated by the term $b\\xi$. If $b>0$, a larger value of $\\xi$ (which makes choice $C=1$ more likely) leads to a larger neural response $R_s$. Consequently, the distribution of $R_s$ for $C=1$ will be stochastically greater than that for $C=0$, leading to $\\mathrm{CP}(s) > 0.5$. The parameter $b$ thus represents the direct coupling between the neuron's activity and the decision-making process. The test cases with $b>0$ are designed to exhibit $\\mathrm{CP}(s) > 0.5$.\n\nIn the aggregated case, however, we do not control for $S$, which acts as a confounding variable. The design of the decision rule, $C=\\mathbb{I}[wS+\\xi \\ge 0]$ with $w>0$, ensures that high stimulus values (e.g., $S=1$) are strongly correlated with choice $C=1$, while low stimulus values (e.g., $S=-1$) are correlated with choice $C=0$. The stimulus $S$ also influences the response $R$ via the term $aS$ (and also $(a+bw)S$). If the coefficient $a$ is sufficiently negative (as in test case 1), high values of $S$ (predominantly associated with $C=1$) will produce systematically lower values of $R$, and low values of $S$ (predominantly associated with $C=0$) will produce systematically higher values of $R$. This confounding effect, driven by the parameter $a$, can dominate the direct choice-related effect, which is driven by $b$. This can result in the aggregated response distribution for $C=1$ being stochastically smaller than that for $C=0$, yielding $\\mathrm{CP}_{\\mathrm{agg}} < 0.5$ and thus creating the paradox.\n\nThe implementation will proceed as follows. For each parameter set:\n1. $N$ independent trials are simulated by generating random samples for $S$, $\\xi$, and $\\varepsilon$ from their specified distributions using `numpy`. A dedicated random number generator is initialized for each case with the provided seed for reproducibility.\n2. The variables $D$, $C$, and $R$ are computed for each trial based on the generative model.\n3. The Choice Probabilities are estimated as the area under the Receiver Operating Characteristic (ROC) curve. As specified, this is computed using the Mann-Whitney $U$ statistic: $\\widehat{\\mathrm{AUC}} = U / (n_1 n_0)$. We use the `scipy.stats.mannwhitneyu` function with the `alternative='greater'` option, which provides the correct $U$ statistic corresponding to $\\mathbb{P}(R^{(1)} > R^{(0)})$.\n4. First, $\\mathrm{CP}_{\\mathrm{agg}}$ is computed using all $N$ trials, partitioned only by the choice $C \\in \\{0,1\\}$.\n5. Then, for each stimulus level $s \\in \\{-1,0,1\\}$, the subset of trials corresponding to $S=s$ is selected, and $\\mathrm{CP}(s)$ is computed by partitioning this subset by choice.\n6. Finally, the logical condition for Simpson's paradox, $\\mathrm{CP}_{\\mathrm{agg}} < 0.5 \\land (\\forall s, \\mathrm{CP}(s) > 0.5)$, is evaluated. The results are compiled into the specified list format. This procedure is repeated for all four test cases.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import mannwhitneyu\n\ndef solve():\n    \"\"\"\n    Main function to run the Simpson's paradox simulation for all test cases.\n    \"\"\"\n    test_cases = [\n        # Case 1: Designed to exhibit Simpson’s paradox. a is strongly negative.\n        {\n            'N': 4000, 's_levels': [-1, 0, 1], 'w': 2.0, 'a': -3.0, 'b': 0.7, \n            'sigma_d': 1.0, 'sigma_r': 1.0, 'seed': 1\n        },\n        # Case 2: No reversal. a is positive, so stimulus and decision effects align.\n        {\n            'N': 4000, 's_levels': [-1, 0, 1], 'w': 2.0, 'a': 0.5, 'b': 0.7, \n            'sigma_d': 1.0, 'sigma_r': 1.0, 'seed': 2\n        },\n        # Case 3: Boundary case. a is negative but smaller, confounding is weaker.\n        {\n            'N': 4000, 's_levels': [-1, 0, 1], 'w': 2.0, 'a': -1.0, 'b': 0.7, \n            'sigma_d': 1.0, 'sigma_r': 1.0, 'seed': 3\n        },\n        # Case 4: Edge case. b=0, so no direct link from decision to response.\n        {\n            'N': 4000, 's_levels': [-1, 0, 1], 'w': 2.0, 'a': -3.0, 'b': 0.0, \n            'sigma_d': 1.0, 'sigma_r': 1.0, 'seed': 4\n        }\n    ]\n\n    all_results = []\n    \n    for params in test_cases:\n        result = _run_single_case(**params)\n        all_results.append(result)\n\n    # The final print must be a single line with the specified format.\n    # Python's str() on lists produces the required bracketed, comma-separated format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\n\ndef _calculate_auc(responses_1, responses_0):\n    \"\"\"\n    Calculates the Area Under the ROC Curve (AUC) using the Mann-Whitney U statistic.\n    AUC = P(response_1 > response_0), estimated as U / (n1 * n0).\n    \"\"\"\n    n1 = len(responses_1)\n    n0 = len(responses_0)\n    \n    # If one of the choices was not made, the AUC is undefined.\n    # Problem statement guarantees this won't happen for the given parameters.\n    # We return 0.5 as a neutral value, though np.nan would also be appropriate.\n    if n1 == 0 or n0 == 0:\n        return 0.5\n\n    # 'alternative'='greater' tests H1: P(responses_1 > responses_0) > 0.5.\n    # The statistic returned is U1, where U = U1 / (n1 * n0).\n    u_statistic, _ = mannwhitneyu(responses_1, responses_0, alternative='greater')\n    \n    return u_statistic / (n1 * n0)\n\n\ndef _run_single_case(N, s_levels, w, a, b, sigma_d, sigma_r, seed):\n    \"\"\"\n    Runs the simulation for a single set of parameters and computes the results.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # Step 1: Generate N independent samples of (S, D, C, R)\n    # Generate stimulus S from a uniform discrete distribution\n    S = rng.choice(s_levels, size=N, replace=True)\n    \n    # Generate independent Gaussian noise terms\n    xi = rng.normal(loc=0.0, scale=sigma_d, size=N)\n    epsilon = rng.normal(loc=0.0, scale=sigma_r, size=N)\n    \n    # Compute decision variable D, binary choice C, and neural response R\n    D = w * S + xi\n    C = (D >= 0).astype(int)  # C=1 if D>=0, C=0 otherwise\n    R = a * S + b * D + epsilon\n\n    # Step 2: Compute aggregated Choice Probability (CP_agg)\n    R_agg_1 = R[C == 1]\n    R_agg_0 = R[C == 0]\n    cp_agg = _calculate_auc(R_agg_1, R_agg_0)\n\n    # Step 3: Compute conditional Choice Probability CP(s) for each stimulus level\n    cp_conditional_list = []\n    unique_s_levels = sorted(s_levels)\n    for s_val in unique_s_levels:\n        mask_s = (S == s_val)\n        \n        R_s = R[mask_s]\n        C_s = C[mask_s]\n        \n        R_cond_1 = R_s[C_s == 1]\n        R_cond_0 = R_s[C_s == 0]\n        \n        cp_s = _calculate_auc(R_cond_1, R_cond_0)\n        cp_conditional_list.append(cp_s)\n\n    # Step 4: Determine if Simpson's paradox occurred\n    # Condition: CP_agg < 0.5 AND CP(s) > 0.5 for all s\n    is_paradox = False\n    if cp_agg < 0.5 and all(cp > 0.5 for cp in cp_conditional_list):\n        is_paradox = True\n    \n    # Format the output a [CP_agg, [CP(s1), ..., CP(sm)], rev]\n    return [cp_agg, cp_conditional_list, is_paradox]\n\nsolve()\n```"
        },
        {
            "introduction": "Having learned *why* we must control for stimulus confounds, we now turn to *how*. This practical coding exercise guides you through implementing and comparing two widely used methods for estimating CP in the presence of a stimulus-driven response: the regression-residual approach and the binning approach . By applying these techniques to simulated data where the ground-truth CP is known, you will directly assess their performance, including their robustness to model misspecification and their behavior in different data regimes. This practice equips you with essential data analysis skills for robustly linking neural activity to behavior in real-world experimental data.",
            "id": "4145772",
            "problem": "You are given simulated single-neuron trial data consisting of stimulus values and neural responses together with binary choices. The scientific objective is to estimate Choice Probability (CP), defined as the area under the Receiver Operating Characteristic (ROC) curve measuring how well the neural activity predicts the choice, while removing confounding stimulus effects. You will implement and compare two methods: a regression-residual approach and a binning-based approach. Your program must produce quantitative results for the specified test suite.\n\nFundamental base and definitions to be used:\n- Choice Probability (CP) is defined as the area under the Receiver Operating Characteristic (ROC) curve between the distributions of a scalar neural response conditioned on choice. If $r_{1}$ and $r_{0}$ denote independent draws from the response distributions conditional on choice $y=1$ and $y=0$, respectively, then CP equals\n$$CP \\equiv \\mathbb{P}\\left(r_{1} > r_{0}\\right) + \\frac{1}{2}\\,\\mathbb{P}\\left(r_{1} = r_{0}\\right).$$\n- The unbiased U-statistic estimator of CP from samples $\\{x_{i}\\}_{i=1}^{n_{1}}$ drawn under $y=1$ and $\\{z_{j}\\}_{j=1}^{n_{0}}$ drawn under $y=0$ is\n$$\\widehat{CP} = \\frac{1}{n_{1} n_{0}} \\sum_{i=1}^{n_{1}} \\sum_{j=1}^{n_{0}} \\left[ \\mathbf{1}\\{x_{i} > z_{j}\\} + \\frac{1}{2}\\mathbf{1}\\{x_{i} = z_{j}\\} \\right],$$\nwhich is equivalent to the Mann–Whitney statistic expressed as an area under the curve. This equivalence allows a stable implementation via ranks.\n- Stimulus-driven baseline activity is modeled by a function $f(s)$ of stimulus $s$, and residual activity is $r' = r - f(s)$ where $r$ is the observed response. Estimated residuals use $\\hat{f}(s)$ obtained by empirical risk minimization:\n$$\\hat{f} = \\arg\\min_{g \\in \\mathcal{G}} \\sum_{t=1}^{n} \\left(r_{t} - g(s_{t})\\right)^{2},$$\nwith $\\mathcal{G}$ specified as either linear polynomials or quadratic polynomials of $s$. Residuals are $r'_{t} = r_{t} - \\hat{f}(s_{t})$, and CP is computed on $r'$.\n- The binning approach partitions the stimulus axis into $K$ bins and computes a within-bin CP (using $r$ without regression) to approximate conditioning on $s$. The overall CP is a weighted average across bins:\n$$\\widehat{CP}_{\\text{bin}} = \\frac{\\sum_{k=1}^{K} n_{1k} n_{0k} \\,\\widehat{CP}_{k}}{\\sum_{k=1}^{K} n_{1k} n_{0k}},$$\nwhere $n_{1k}$ and $n_{0k}$ are the counts of trials with $y=1$ and $y=0$ in bin $k$, and $\\widehat{CP}_{k}$ is the U-statistic CP computed within bin $k$. Bins with $n_{1k} = 0$ or $n_{0k} = 0$ are excluded.\n- When residual responses are Gaussian with equal variance and different means, i.e., $r' \\mid y=1 \\sim \\mathcal{N}(\\mu_{1}, \\sigma^{2})$ and $r' \\mid y=0 \\sim \\mathcal{N}(\\mu_{0}, \\sigma^{2})$, the true CP equals\n$$CP^{\\star} = \\Phi\\!\\left( \\frac{\\mu_{1} - \\mu_{0}}{\\sqrt{2}\\,\\sigma} \\right),$$\nwhere $\\Phi(\\cdot)$ is the cumulative distribution function of the standard normal distribution. In the simulation below, residuals will be constructed to satisfy this model with $\\mu_{0} = 0$ and $\\mu_{1} = m$, so the ground-truth CP is $CP^{\\star} = \\Phi\\!\\left( \\frac{m}{\\sqrt{2}\\,\\sigma} \\right)$.\n\nData-generating process for each test case:\n- Draw $n$ independent stimuli $s_{t}$ from a standard normal distribution $\\mathcal{N}(0,1)$.\n- Generate choices via a deterministic threshold on a noisy decision variable $d_{t}$, $d_{t} = k\\,s_{t} + \\xi_{t}$ with $\\xi_{t} \\sim \\mathcal{N}(0,1)$ independently, and $y_{t} = \\mathbf{1}\\{d_{t} > 0\\}$. This creates a strong dependence between stimulus and choice controlled by $k$.\n- Generate residual activity $r'_{t} = m\\,y_{t} + \\varepsilon_{t}$ with $\\varepsilon_{t} \\sim \\mathcal{N}(0, \\sigma^{2})$. This implements the equal-variance Gaussian model with a mean shift $m$ between choices.\n- Generate observed activity $r_{t} = f(s_{t}) + r'_{t}$ with $f(s) = \\gamma_{1}\\,s + \\gamma_{2}\\,s^{2}$.\n\nAlgorithmic tasks:\n- Implement the regression-residual CP estimator by fitting $\\hat{f}(s)$ using ordinary least squares over the specified polynomial class $\\mathcal{G}$ (linear or quadratic), computing residuals $r'$, and computing $\\widehat{CP}$ via a rank-based U-statistic.\n- Implement the binning-based CP estimator with $K$ equal-width bins over the range of $s$, computing $\\widehat{CP}_{k}$ within each bin that contains both choices, and aggregating with the weighted average formula above.\n- Also compute the naive CP on $r$ without conditioning or residualization to show confounding effects due to stimulus-choice correlations.\n\nYour implementation must produce the following results per test case:\n- $CP^{\\star}$ computed from $m$ and $\\sigma$ using $CP^{\\star} = \\Phi\\!\\left( \\frac{m}{\\sqrt{2}\\,\\sigma} \\right)$.\n- $\\widehat{CP}_{\\text{reg}}$ computed on residuals $r' = r - \\hat{f}(s)$.\n- $\\widehat{CP}_{\\text{bin}}$ computed via binning on $r$.\n- $\\widehat{CP}_{\\text{naive}}$ computed on $r$ ignoring $s$.\n- Differences relative to ground truth: $\\widehat{CP}_{\\text{reg}} - CP^{\\star}$, $\\widehat{CP}_{\\text{bin}} - CP^{\\star}$, and $\\widehat{CP}_{\\text{naive}} - CP^{\\star}$.\n\nTest suite:\n- Case $1$ (happy path, correct model, large sample):\n    - $n = 4000$, $k = 3.0$, $m = 0.3$, $\\sigma = 1.0$, $\\gamma_{1} = 1.0$, $\\gamma_{2} = 0.0$, regression model class $\\mathcal{G}$ is linear, $K = 10$, random seed $= 123$.\n- Case $2$ (model mis-specification, quadratic baseline but linear fit):\n    - $n = 4000$, $k = 3.0$, $m = 0.3$, $\\sigma = 1.0$, $\\gamma_{1} = 0.0$, $\\gamma_{2} = 1.5$, regression model class $\\mathcal{G}$ is linear, $K = 20$, random seed $= 456$.\n- Case $3$ (class imbalance across stimulus, small sample, quadratic fit):\n    - $n = 800$, $k = 8.0$, $m = 0.2$, $\\sigma = 1.0$, $\\gamma_{1} = 1.0$, $\\gamma_{2} = 0.5$, regression model class $\\mathcal{G}$ is quadratic, $K = 8$, random seed $= 789$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case is represented by a bracketed, comma-separated list of seven decimal numbers in the following order:\n$[CP^{\\star}, \\widehat{CP}_{\\text{reg}}, \\widehat{CP}_{\\text{bin}}, \\widehat{CP}_{\\text{naive}}, \\widehat{CP}_{\\text{reg}} - CP^{\\star}, \\widehat{CP}_{\\text{bin}} - CP^{\\star}, \\widehat{CP}_{\\text{naive}} - CP^{\\star}]$.\nFor example, the overall output should look like $[[\\cdots],[\\cdots],[\\cdots]]$ with no spaces.",
            "solution": "The problem requires the estimation of Choice Probability (CP), a measure of how well a neuron's activity predicts a binary choice, while accounting for the confounding influence of an external stimulus. We are tasked with implementing and comparing two methods for confound removal—a regression-residual approach and a binning-based approach—on simulated data where the ground-truth CP is known.\n\n### Data-Generating Process\n\nFor each trial $t=1, \\dots, n$, the data are simulated according to the following process:\n1.  A stimulus value $s_t$ is drawn from a standard normal distribution:\n    $$s_t \\sim \\mathcal{N}(0, 1)$$\n2.  A binary choice $y_t \\in \\{0, 1\\}$ is generated. A latent decision variable $d_t$ is first computed as a function of the stimulus plus noise:\n    $$d_t = k \\cdot s_t + \\xi_t, \\quad \\text{where} \\quad \\xi_t \\sim \\mathcal{N}(0, 1)$$\n    The choice is then determined by a threshold on $d_t$:\n    $$y_t = \\mathbf{1}\\{d_t > 0\\}$$\n    The parameter $k$ controls the strength of the correlation between the stimulus $s_t$ and the choice $y_t$. A larger $k$ implies that the stimulus is more predictive of the choice.\n3.  The neural activity is modeled as a sum of a stimulus-dependent component and a choice-dependent component. The choice-dependent component, which we term the \"residual activity\" $r'_t$, is generated as:\n    $$r'_t = m \\cdot y_t + \\varepsilon_t, \\quad \\text{where} \\quad \\varepsilon_t \\sim \\mathcal{N}(0, \\sigma^2)$$\n    This construction ensures that the residual activity follows a Gaussian distribution conditioned on the choice: $r'_t | (y_t=0) \\sim \\mathcal{N}(0, \\sigma^2)$ and $r'_t | (y_t=1) \\sim \\mathcal{N}(m, \\sigma^2)$.\n4.  The observed neural response $r_t$ is the sum of the residual activity and a stimulus-dependent baseline activity, $f(s_t)$:\n    $$r_t = f(s_t) + r'_t$$\n    The baseline is a quadratic function of the stimulus:\n    $$f(s) = \\gamma_1 s + \\gamma_2 s^2$$\n\n### Ground-Truth Choice Probability ($CP^{\\star}$)\n\nGiven the generative model for the residual activity $r'$, the distributions of $r'$ conditional on choice are $r' \\mid y=0 \\sim \\mathcal{N}(\\mu_0, \\sigma^2)$ and $r' \\mid y=1 \\sim \\mathcal{N}(\\mu_1, \\sigma^2)$, with $\\mu_0=0$ and $\\mu_1=m$. The theoretical CP is the probability that a random draw from the $y=1$ distribution is greater than a random draw from the $y=0$ distribution. For two independent Gaussian variables, their difference is also Gaussian. Let $r'_1 \\sim \\mathcal{N}(m, \\sigma^2)$ and $r'_0 \\sim \\mathcal{N}(0, \\sigma^2)$. Their difference $r'_1 - r'_0$ follows $\\mathcal{N}(m, 2\\sigma^2)$. The CP is $\\mathbb{P}(r'_1 > r'_0) = \\mathbb{P}(r'_1 - r'_0 > 0)$. Standardizing this variable gives:\n$$CP^{\\star} = \\mathbb{P}\\left(\\frac{(r'_1 - r'_0) - m}{\\sqrt{2\\sigma^2}} > \\frac{0 - m}{\\sqrt{2\\sigma^2}}\\right) = \\mathbb{P}\\left(Z > -\\frac{m}{\\sqrt{2}\\sigma}\\right) = 1 - \\Phi\\left(-\\frac{m}{\\sqrt{2}\\sigma}\\right) = \\Phi\\left(\\frac{m}{\\sqrt{2}\\sigma}\\right)$$\nwhere $Z \\sim \\mathcal{N}(0,1)$ and $\\Phi(\\cdot)$ is the cumulative distribution function (CDF) of the standard normal distribution.\n\n### Choice Probability Estimators\n\nWe will compute CP using three different methods on the simulated data. The core calculation for each method relies on the Mann-Whitney U-statistic, which provides an unbiased estimator for CP. Given response samples $\\{x_i\\}_{i=1}^{n_1}$ for choice $y=1$ and $\\{z_j\\}_{j=1}^{n_0}$ for choice $y=0$, the estimator is:\n$$\\widehat{CP} = \\frac{1}{n_1 n_0} \\sum_{i=1}^{n_1} \\sum_{j=1}^{n_0} \\left[ \\mathbf{1}\\{x_i > z_j\\} + \\frac{1}{2}\\mathbf{1}\\{x_i = z_j\\} \\right]$$\nThis is equivalent to the Area Under the ROC Curve (AUC) and can be efficiently computed using rank-based statistics.\n\n1.  **Naive Estimator ($\\widehat{CP}_{\\text{naive}}$)**: This estimator is computed directly on the observed neural responses $\\{r_t\\}_{t=1}^n$, ignoring the stimulus information $\\{s_t\\}_{t=1}^n$. Because both the response $r_t$ and the choice $y_t$ are influenced by the stimulus $s_t$, this estimator is expected to be biased, reflecting the confounded effect of the stimulus.\n\n2.  **Regression-Residual Estimator ($\\widehat{CP}_{\\text{reg}}$)**: This method attempts to remove the stimulus confound by first modeling the relationship between stimulus and response, and then computing CP on the residuals.\n    -   An estimate of the stimulus-response function, $\\hat{f}(s)$, is obtained via Ordinary Least Squares (OLS) regression of $r_t$ onto a polynomial basis of $s_t$. The polynomial class $\\mathcal{G}$ (linear or quadratic) is specified for each test case. For a linear model, we fit $g(s) = \\beta_0 + \\beta_1 s$. For a quadratic model, we fit $g(s) = \\beta_0 + \\beta_1 s + \\beta_2 s^2$. The coefficients $\\hat{\\beta}$ are found by minimizing the sum of squared errors: $\\sum_{t=1}^n (r_t - g(s_t))^2$.\n    -   The residuals are then computed as $\\hat{r}'_t = r_t - \\hat{f}(s_t)$, where $\\hat{f}(s_t)$ is the prediction from the fitted model.\n    -   $\\widehat{CP}_{\\text{reg}}$ is calculated using the U-statistic estimator on the set of residuals $\\{\\hat{r}'_t\\}_{t=1}^n$ and choices $\\{y_t\\}_{t=1}^n$. If the regression model correctly captures the true form of $f(s)$, this method should yield an unbiased estimate of $CP^{\\star}$. Model misspecification can lead to remaining confounds and a biased estimate.\n\n3.  **Binning-Based Estimator ($\\widehat{CP}_{\\text{bin}}$)**: This non-parametric method controls for the stimulus by partitioning its range into $K$ discrete, equal-width bins.\n    -   The range of observed stimuli, $[\\min(s), \\max(s)]$, is divided into $K$ intervals.\n    -   For each bin $k=1, \\dots, K$, a local CP, $\\widehat{CP}_k$, is computed using the U-statistic on the raw responses $r_t$ of trials that fall within that bin.\n    -   This calculation is only performed for bins that contain trials from both choice categories (i.e., $n_{1k} > 0$ and $n_{0k} > 0$, where $n_{yk}$ is the number of trials with choice $y$ in bin $k$).\n    -   The overall estimator, $\\widehat{CP}_{\\text{bin}}$, is the weighted average of the per-bin CPs, with weights proportional to the product of the sample sizes of the two choices in each bin:\n    $$\\widehat{CP}_{\\text{bin}} = \\frac{\\sum_{k=1}^{K} n_{1k} n_{0k} \\,\\widehat{CP}_{k}}{\\sum_{k=1}^{K} n_{1k} n_{0k}}$$\n    This method approximates conditioning on the stimulus and is less sensitive to the functional form of $f(s)$ than the regression approach, but its accuracy can depend on the number of bins $K$ and the amount of data in each bin.\n\nThe implemented program will execute these calculations for three distinct test cases, each designed to probe the performance of the estimators under different conditions, such as correct model specification, model misspecification, and small sample sizes.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm, mannwhitneyu\nfrom scipy.special import erf\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n\n    # Manually implement norm.cdf to avoid scipy version issues,\n    # as scipy versions < 1.12 might not have it as a ufunc.\n    # The definition is Phi(x) = 0.5 * (1 + erf(x/sqrt(2))).\n    def norm_cdf(x):\n        return 0.5 * (1.0 + erf(x / np.sqrt(2.0)))\n\n    def compute_cp(responses, choices):\n        \"\"\"\n        Computes Choice Probability using the Mann-Whitney U statistic.\n        \n        Args:\n            responses (np.array): Neural responses.\n            choices (np.array): Binary choices (0 or 1).\n            \n        Returns:\n            float: The estimated Choice Probability (AUC).\n        \"\"\"\n        r1 = responses[choices == 1]\n        r0 = responses[choices == 0]\n        n1, n0 = len(r1), len(r0)\n        \n        if n1 == 0 or n0 == 0:\n            return np.nan\n        \n        # mannwhitneyu calculates U for r1 > r0, including ties.\n        # CP = U / (n1 * n0)\n        u_statistic, _ = mannwhitneyu(r1, r0, alternative='greater')\n        return u_statistic / (n1 * n0)\n\n    def run_case(n, k, m, sigma, gamma1, gamma2, reg_model_type, K, seed):\n        \"\"\"\n        Runs one full simulation and analysis case.\n        \"\"\"\n        rng = np.random.default_rng(seed)\n\n        # 1. Data Generation\n        s = rng.normal(loc=0, scale=1, size=n)\n        xi = rng.normal(loc=0, scale=1, size=n)\n        d = k * s + xi\n        y = (d > 0).astype(int)\n        \n        epsilon = rng.normal(loc=0, scale=sigma, size=n)\n        r_prime = m * y + epsilon\n        \n        f_s = gamma1 * s + gamma2 * s**2\n        r = f_s + r_prime\n\n        # 2. Calculate ground truth CP\n        cp_star = norm_cdf(m / (np.sqrt(2) * sigma))\n\n        # 3. Calculate Naive CP\n        cp_naive = compute_cp(r, y)\n\n        # 4. Calculate Regression-Residual CP\n        if reg_model_type == 'linear':\n            # Design matrix for g(s) = beta_0 + beta_1*s\n            X = np.vstack([np.ones(n), s]).T\n        elif reg_model_type == 'quadratic':\n            # Design matrix for g(s) = beta_0 + beta_1*s + beta_2*s^2\n            X = np.vstack([np.ones(n), s, s**2]).T\n        else:\n            raise ValueError(\"Invalid regression model type\")\n\n        coeffs, _, _, _ = np.linalg.lstsq(X, r, rcond=None)\n        f_hat = X @ coeffs\n        r_residuals = r - f_hat\n        cp_reg = compute_cp(r_residuals, y)\n\n        # 5. Calculate Binning-Based CP\n        s_min, s_max = np.min(s), np.max(s)\n        # Add small epsilon to include the max value in the last bin\n        bin_edges = np.linspace(s_min, s_max + 1e-9, K + 1)\n        \n        s_bin_indices = np.digitize(s, bin_edges)\n\n        cp_bin_num = 0.0\n        cp_bin_den = 0.0\n\n        for i in range(1, K + 1):\n            bin_mask = (s_bin_indices == i)\n            if not np.any(bin_mask):\n                continue\n\n            r_bin = r[bin_mask]\n            y_bin = y[bin_mask]\n\n            n1k = np.sum(y_bin == 1)\n            n0k = np.sum(y_bin == 0)\n\n            if n1k > 0 and n0k > 0:\n                cp_k = compute_cp(r_bin, y_bin)\n                weight = n1k * n0k\n                cp_bin_num += weight * cp_k\n                cp_bin_den += weight\n        \n        cp_bin = cp_bin_num / cp_bin_den if cp_bin_den > 0 else np.nan\n\n        # 6. Calculate differences\n        diff_reg = cp_reg - cp_star\n        diff_bin = cp_bin - cp_star\n        diff_naive = cp_naive - cp_star\n\n        return [cp_star, cp_reg, cp_bin, cp_naive, diff_reg, diff_bin, diff_naive]\n\n    test_cases = [\n        # Case 1 (happy path, correct model, large sample)\n        {'n': 4000, 'k': 3.0, 'm': 0.3, 'sigma': 1.0, 'gamma1': 1.0, 'gamma2': 0.0, 'reg_model_type': 'linear', 'K': 10, 'seed': 123},\n        # Case 2 (model mis-specification, quadratic baseline but linear fit)\n        {'n': 4000, 'k': 3.0, 'm': 0.3, 'sigma': 1.0, 'gamma1': 0.0, 'gamma2': 1.5, 'reg_model_type': 'linear', 'K': 20, 'seed': 456},\n        # Case 3 (class imbalance across stimulus, small sample, quadratic fit)\n        {'n': 800, 'k': 8.0, 'm': 0.2, 'sigma': 1.0, 'gamma1': 1.0, 'gamma2': 0.5, 'reg_model_type': 'quadratic', 'K': 8, 'seed': 789},\n    ]\n\n    all_results = []\n    for case_params in test_cases:\n        results = run_case(**case_params)\n        all_results.append(results)\n    \n    # Format the final output string\n    # e.g., [[val1,val2,...],[val1,val2,...],[val1,val2,...]]\n    case_strings = []\n    for case_result in all_results:\n        # Each number is converted to a string using default float formatting\n        case_strings.append(f\"[{','.join(map(str, case_result))}]\")\n    \n    final_output = f\"[{','.join(case_strings)}]\"\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}