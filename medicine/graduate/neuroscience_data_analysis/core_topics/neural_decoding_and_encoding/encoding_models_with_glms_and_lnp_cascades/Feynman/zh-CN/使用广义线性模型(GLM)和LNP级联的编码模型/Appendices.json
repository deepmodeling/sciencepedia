{
    "hands_on_practices": [
        {
            "introduction": "神经编码模型中的一个核心挑战是如何有效地表示时间滤波器（或感受野）。直接为每个时间延迟估计一个独立的权重参数，会导致模型参数过多，容易产生过拟合。本练习将引导你实践一种强大的降维技术：使用一组基函数来表示时间滤波器。通过将滤波器投影到一个低维的基函数空间，我们不仅能让模型更加简洁，还能在参数估计过程中引入平滑性，从而提高模型的稳健性和泛化能力。",
            "id": "4158967",
            "problem": "考虑一个神经科学中的编码模型，其中，时间核用于广义线性模型 (GLM, Generalized Linear Model) 或线性-非线性-泊松 (LNP, Linear-Nonlinear-Poisson) 级联模型。时间核 $k(t)$ 将通过时间基函数的低维展开进行近似。您的任务是在对数拉伸的时间轴上构建一个升余弦时间基函数族，并计算已知真实核函数的最小二乘岭回归近似，然后通过离散近似报告在连续时间极限下的相对平方误差。\n\n请使用以下定义和设置，这些在编码模型中是标准的，但需要明确的实现细节：\n\n1. 时间网格和核函数：\n   - 设总时长为 $T = 0.2$ 秒，时间步长为 $\\Delta t = 0.001$ 秒。定义采样时间 $t_n = n \\Delta t$，其中 $n = 0, 1, \\ldots, N$ 且 $N = T / \\Delta t$。\n   - 在此网格上定义真实时间核为 $k_{\\text{true}}(t) = t \\exp(-t / \\tau)$，时间常数 $\\tau = 0.03$ 秒。\n\n2. 对数时间扭曲和升余弦基函数构建：\n   - 对于选定的偏移量 $c > 0$，定义拉伸坐标 $\\psi(t) = \\log(t + c)$。\n   - 对于选定的基函数数量 $M \\geq 2$，定义范围 $\\psi_{\\min} = \\log(0 + c)$ 和 $\\psi_{\\max} = \\log(T + c)$。定义 $M$ 个中心点 $\\{ \\mu_j \\}_{j=1}^M$，它们在 $\\psi_{\\min}$ 和 $\\psi_{\\max}$ 之间（含两端点）线性间隔分布。设间距为 $\\Delta_{\\psi} = \\mu_{j+1} - \\mu_j$（当 $M \\geq 2$ 时为常数）。\n   - 在原始时间轴上定义第 $j$ 个基函数为\n     $$\n     b_j(t) =\n     \\begin{cases}\n     \\dfrac{1}{2} \\left( 1 + \\cos\\left( \\pi \\dfrac{\\psi(t) - \\mu_j}{\\Delta_{\\psi}} \\right) \\right),  \\text{若 } \\left| \\psi(t) - \\mu_j \\right| \\le \\Delta_{\\psi}, \\\\\n     0,  \\text{其他情况}.\n     \\end{cases}\n     $$\n   - 对于网格上的每个 $t_n$，构建设计矩阵 $B \\in \\mathbb{R}^{(N+1) \\times M}$，其元素为 $B_{n j} = b_j(t_n)$，其中 $j=1,\\ldots,M$。\n\n3. 岭回归正则化的最小二乘投影：\n   - 对于给定的岭参数 $\\lambda \\ge 0$，定义系数向量 $w \\in \\mathbb{R}^M$，其最小化离散化目标函数\n     $$\n     J(w) = \\sum_{n=0}^{N} \\left( k_{\\text{true}}(t_n) - \\sum_{j=1}^{M} w_j \\, b_j(t_n) \\right)^2 \\Delta t + \\lambda \\sum_{j=1}^{M} w_j^2.\n     $$\n   - 最小化器满足正规方程\n     $$\n     \\left( B^\\top B \\, \\Delta t + \\lambda I \\right) w = B^\\top k \\, \\Delta t,\n     $$\n     其中 $k \\in \\mathbb{R}^{N+1}$ 的元素为 $k_n = k_{\\text{true}}(t_n)$，$I$ 是 $M \\times M$ 的单位矩阵。\n\n4. 重建和误差度量：\n   - 定义重建核 $\\hat{k}(t_n) = \\sum_{j=1}^{M} w_j \\, b_j(t_n)$ 对于所有 $n$。\n   - 使用连续 $L^2$ 范数的黎曼和近似来计算相对 $L^2$ 误差：\n     $$\n     E = \\dfrac{\\left\\| \\hat{k} - k \\right\\|_{2,\\Delta t}}{\\left\\| k \\right\\|_{2,\\Delta t}} = \\dfrac{\\sqrt{ \\sum_{n=0}^{N} \\left( \\hat{k}(t_n) - k_{\\text{true}}(t_n) \\right)^2 \\Delta t }}{\\sqrt{ \\sum_{n=0}^{N} \\left( k_{\\text{true}}(t_n) \\right)^2 \\Delta t }}.\n     $$\n\n实现一个程序，对以下测试套件中的每个参数三元组 $(M, \\lambda, c)$ 执行上述步骤，并返回相应的 $E$ 值：\n\n- 测试用例 1：$M = 8$，$\\lambda = 0.0$， $c = 0.005$。\n- 测试用例 2：$M = 3$，$\\lambda = 0.0$， $c = 0.005$。\n- 测试用例 3：$M = 8$，$\\lambda = 0.001$，$c = 0.005$。\n- 测试用例 4：$M = 8$，$\\lambda = 0.0$， $c = 0.0005$。\n- 测试用例 5：$M = 12$，$\\lambda = 0.1$， $c = 0.005$。\n\n您的程序必须：\n- 使用上面指定的精确网格，$T = 0.2$ 秒且 $\\Delta t = 0.001$ 秒。\n- 按照定义构建升余弦基，不进行任何额外的归一化或修改。\n- 对每个测试用例求解 $w$ 并精确计算 $E$。\n- 生成单行输出，包含用方括号括起来的逗号分隔列表形式的结果，每个 $E$ 值四舍五入到六位小数（例如，$[0.123456,0.234567,0.345678,0.456789,0.567890]$）。\n\n此任务中没有随机性，最终的误差值不需要物理单位。角度单位不适用。测试套件的输出是实数，最终程序输出必须是按顺序汇总所有测试用例的指定格式的单行文本。",
            "solution": "该问题是有效的，因为它具有科学依据、定义明确且客观。它提供了一套完整且一致的定义和参数，用以解决计算神经科学中的一个标准问题：使用基函数展开来近似一个时间核。解决方案需要实现指定的数值程序。\n\n解决方案分为四个主要步骤：建立离散时间网格和真实核函数，根据升余弦基函数构建设计矩阵，求解岭回归问题以获得基权重，最后计算相对近似误差。\n\n首先，我们建立离散时间域。总时长为 $T = 0.2$ 秒，时间步长为 $\\Delta t = 0.001$ 秒。这定义了一个包含 $N+1$ 个时间点的网格 $t_n = n \\Delta t$，其中 $N = T/\\Delta t = 200$，因此 $n$ 的范围是从 $0$ 到 $200$。因此，时间点为 $t = \\{0, 0.001, 0.002, \\ldots, 0.2\\}$。在此网格上，我们使用给定的时间常数 $\\tau = 0.03$ 秒来评估真实核函数，即一个向量 $k \\in \\mathbb{R}^{N+1}$，其元素为 $k_n = k_{\\text{true}}(t_n) = t_n \\exp(-t_n / \\tau)$。\n\n其次，对于由参数三元组 $(M, \\lambda, c)$ 定义的每个测试用例，我们构建相应的设计矩阵 $B \\in \\mathbb{R}^{(N+1) \\times M}$。这涉及到对时间轴进行对数扭曲。扭曲后的坐标定义为 $\\psi(t) = \\log(t + c)$，其中 $c$ 是一个小的正常数偏移量。在我们的时间域上，这个扭曲坐标的范围是 $[\\psi_{\\min}, \\psi_{\\max}]$，其中 $\\psi_{\\min} = \\log(c)$ 且 $\\psi_{\\max} = \\log(T + c)$。我们定义 $M$ 个中心点 $\\{\\mu_j\\}_{j=1}^M$，它们在此区间内（含端点）线性等距分布。对于 $M \\ge 2$，相邻中心点之间的恒定间距为 $\\Delta_{\\psi} = (\\psi_{\\max} - \\psi_{\\min}) / (M - 1)$。\n\n第 $j$ 个基函数 $b_j(t)$ 是一个在扭曲的 $\\psi$ 空间中以 $\\mu_j$ 为中心的升余弦函数。它在时间 $t$ 处的值由下式给出\n$$\nb_j(t) = \\frac{1}{2} \\left( 1 + \\cos\\left( \\pi \\frac{\\psi(t) - \\mu_j}{\\Delta_{\\psi}} \\right) \\right)\n$$\n如果其参数在其支撑集内，即 $|\\psi(t) - \\mu_j| \\le \\Delta_{\\psi}$ 时，否则 $b_j(t) = 0$。通过在 $N+1$ 个时间点中的每一个点上评估 $M$ 个基函数中的每一个，来填充设计矩阵 $B$，使得 $B_{nj} = b_j(t_n)$。\n\n第三，我们找到最优权重向量 $w \\in \\mathbb{R}^M$，它将真实核函数近似为基函数的线性组合 $\\hat{k} = Bw$。这些权重通过最小化一个岭回归正则化的最小二乘目标函数来确定。这个最小化过程导出正规方程，这是一个关于 $w$ 的线性方程组：\n$$\n\\left( B^\\top B \\, \\Delta t + \\lambda I \\right) w = B^\\top k \\, \\Delta t\n$$\n这里，$B^\\top$ 是 $B$ 的转置，$I$ 是 $M \\times M$ 的单位矩阵，而 $\\lambda$ 是岭回归正则化参数。这个线性系统具有 $A w = b_{rhs}$ 的形式，其中 $A = (B^\\top B \\, \\Delta t + \\lambda I)$ 且 $b_{rhs} = (B^\\top k \\, \\Delta t)$。我们使用标准的数值线性代数方法求解该系统以得到 $w$。\n\n第四，计算出最优权重 $w$ 后，我们构建重建核 $\\hat{k}$，它是一个由值 $\\hat{k}_n = \\hat{k}(t_n) = \\sum_{j=1}^{M} w_j b_j(t_n)$ 组成的向量。该近似的误差由相对 $L^2$ 误差 $E$ 来量化，它是误差向量和真实核向量的范数之比。其离散形式为：\n$$\nE = \\frac{\\left\\| \\hat{k} - k \\right\\|_{2,\\Delta t}}{\\left\\| k \\right\\|_{2,\\Delta t}} = \\frac{\\sqrt{ \\sum_{n=0}^{N} ( \\hat{k}_n - k_n )^2 \\Delta t }}{\\sqrt{ \\sum_{n=0}^{N} k_n^2 \\Delta t }}\n$$\n因子 $\\sqrt{\\Delta t}$ 同时出现在分子和分母中，因此它们相互抵消。该误差可以简单地计算为向量 $(\\hat{k} - k)$ 和 $k$ 的标准欧几里得范数之比。对每个测试用例都执行此计算，并收集所得的误差值。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the relative squared-error for ridge-regularized least-squares\n    approximations of a temporal kernel using raised-cosine basis functions.\n    \"\"\"\n\n    # 1. Define global parameters and ground-truth kernel\n    T = 0.2\n    dt = 0.001\n    N = int(T / dt)\n    t = np.linspace(0, T, N + 1)\n    \n    tau = 0.03\n    k_true = t * np.exp(-t / tau)\n\n    def calculate_approximation_error(params):\n        \"\"\"\n        Calculates the error for a single set of parameters (M, lam, c).\n        \"\"\"\n        M, lam, c = params\n        \n        # 2. Construct raised-cosine basis functions\n        psi = lambda time: np.log(time + c)\n        psi_t = psi(t)\n        \n        psi_min = psi(0.0)\n        psi_max = psi(T)\n\n        mu_centers = np.linspace(psi_min, psi_max, M)\n        \n        if M  2:\n            # The problem states M >= 2. If M=1, delta_psi is ill-defined.\n            # A reasonable value could be psi_max - psi_min, but we follow the spec.\n            # This path will not be taken by the test cases.\n            raise ValueError(\"M must be >= 2 for this basis construction.\")\n        \n        delta_psi = mu_centers[1] - mu_centers[0]\n\n        # Populate the design matrix B\n        B = np.zeros((N + 1, M))\n        for j in range(M):\n            mu_j = mu_centers[j]\n            # Argument for the cosine, normalized by required spacing\n            x = (psi_t - mu_j) / delta_psi\n            \n            # Basis is non-zero only where |x| = 1\n            valid_indices = np.abs(x) = 1\n            \n            # Calculate basis function values on its support\n            B[valid_indices, j] = 0.5 * (1 + np.cos(np.pi * x[valid_indices]))\n\n        # 3. Solve for weights using ridge regression normal equations\n        # (B.T @ B * dt + lam * I) @ w = B.T @ k_true * dt\n        identity_M = np.eye(M)\n        A = B.T @ B * dt + lam * identity_M\n        b_rhs = B.T @ k_true * dt\n        \n        w = np.linalg.solve(A, b_rhs)\n        \n        # 4. Reconstruct the kernel and compute the error\n        k_hat = B @ w\n        \n        # The error E is the ratio of L2 norms. The sqrt(dt) term cancels.\n        norm_error_vec = np.linalg.norm(k_hat - k_true)\n        norm_true_vec = np.linalg.norm(k_true)\n        \n        if norm_true_vec == 0:\n            return 0.0 if norm_error_vec == 0 else np.inf\n        \n        error = norm_error_vec / norm_true_vec\n        return error\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (8, 0.0, 0.005),\n        (3, 0.0, 0.005),\n        (8, 0.001, 0.005),\n        (8, 0.0, 0.0005),\n        (12, 0.1, 0.005),\n    ]\n\n    results = []\n    for case in test_cases:\n        error = calculate_approximation_error(case)\n        results.append(error)\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在确定了模型的参数化方法后，下一步便是将模型与实验数据进行拟合。这个练习将带你深入广义线性模型（GLM）估计的核心，要求你从第一性原理出发，使用牛顿法实现完整的最大似然估计算法。通过亲手实现从统计理论到数值优化实践的全过程，你将深刻理解这些编码模型是如何从神经活动数据中被“训练”出来的，并掌握处理实际问题中可能出现的数值挑战的技巧。",
            "id": "4158982",
            "problem": "您需要为一个具有泊松观测和典则对数连接函数的广义线性模型（GLM）实现并测试最大似然估计，该模型应用于一个包含线性-非线性-泊松（LNP）级联的编码模型。您的实现必须有原则性，从泊松分布和 GLM 框架的定义出发，并且必须使用带有回溯线搜索的牛顿法，以保证目标函数的单调改进。您还必须通过解处的 Hessian 矩阵来验证目标的凸性。程序必须是自包含的，并产生确定性的结果。\n\n基本原理：\n- 一个率参数为 $\\lambda$ 的泊松随机变量 $y$，其概率质量函数为 $p(y \\mid \\lambda) = \\exp(-\\lambda)\\,\\lambda^{y}/y!$，其中 $y \\in \\{0,1,2,\\dots\\}$ 且 $\\lambda  0$。\n- 在广义线性模型（GLM）中，条件期望 $E[y \\mid x]$ 通过一个连接函数与线性预测器 $x^{\\top}\\beta$ 相关联。对于泊松 GLM 中的典则对数连接函数，平均率为 $\\lambda = \\exp(x^{\\top}\\beta)$。\n- 在线性-非线性-泊松（LNP）级联中，驱动是通过将一个线性时空滤波器应用于刺激得到的，然后通过一个逐点非线性函数将驱动映射到发放率，最后是一个泊松尖峰生成过程。\n\n需要实现的任务：\n1.  给定设计矩阵 $X \\in \\mathbb{R}^{T \\times p}$ 和计数观测值 $y \\in \\mathbb{N}^{T}$，使用观测间的独立性假设，为具有对数连接函数的泊松 GLM 推导出负对数似然目标函数，并实现带有回溯线搜索的牛顿法来最小化此目标。您的推导必须从泊松概率质量函数和 GLM 典则对数连接的定义开始，不假设任何“捷径”公式。\n2.  使用您的估计器来拟合 $\\hat{\\beta}$，并为一个指定的测试套件计算所需量。您必须通过在牛顿更新中使用 Hessian 矩阵的正阻尼项和回溯线搜索来保证数值稳健性，以确保在每个接受的步骤中目标函数都减小。\n3.  对于一个共线性边界情况，在拟合解 $\\hat{\\beta}$ 处计算负对数似然的 Hessian 矩阵，并使用一个带有小数值容差的基于特征值的准则，返回一个布尔值，指示它是否是半正定的。\n4.  对于一个 LNP 级联情况，从时间刺激延迟构建一个设计矩阵，并通过 GLM 拟合恢复线性滤波器系数和偏置项。\n\n数据生成与测试套件：\n所有随机量必须使用指定的种子生成，并如下所示进行独立抽取，所有矩阵构造在给定这些种子的情况下必须是确定性的。共有四个测试用例：\n\n- 用例 A（正常路径；带截距的多元协变量）：\n  - 设 $T = 80$，特征数 $F = 3$，随机种子为 $42$。\n  - 从 $\\text{Uniform}(-0.5, 0.5)$ 中独立抽取 $X_{\\text{base}} \\in \\mathbb{R}^{T \\times F}$ 的元素，添加一列全为1的截距项，得到 $X \\in \\mathbb{R}^{T \\times (F+1)}$。\n  - 设真实系数为 $\\beta_{\\text{true}} = [0.4, -0.6, 0.5, 0.2]^{\\top}$（最后一项是截距）。\n  - 独立生成计数 $y_t \\sim \\text{Poisson}(\\lambda_t)$，其中 $\\lambda_t = \\exp(x_t^{\\top}\\beta_{\\text{true}})$，对于 $t = 1,\\dots,T$。\n  - 拟合 GLM 得到 $\\hat{\\beta}$，并为查询向量 $\\tilde{x} = [0.5, -1.0, 0.8, 1]^{\\top}$ 计算预测率 $\\hat{\\lambda} = \\exp(\\tilde{x}^{\\top}\\hat{\\beta})$。返回 $\\hat{\\lambda}$ 作为浮点数。\n\n- 用例 B（边界情况；仅含截距的模型）：\n  - 设 $T = 60$，随机种子为 $123$。\n  - 使用 $X \\in \\mathbb{R}^{T \\times 1}$，它由单个全为1的截距列组成。\n  - 设真实恒定率为 $\\lambda = 1.8$；对于 $t = 1,\\dots,T$，独立生成 $y_t \\sim \\text{Poisson}(\\lambda)$。\n  - 拟合 GLM 得到 $\\hat{\\beta} \\in \\mathbb{R}$（仅含截距）。返回标量截距估计值 $\\hat{\\beta}$ 作为浮点数。\n\n- 用例 C（边界情况；近似共线性）：\n  - 设 $T = 100$, $F = 2$，随机种子为 $21$。\n  - 独立抽取 $x^{(1)}_t \\sim \\text{Uniform}(-0.5, 0.5)$。设 $x^{(2)}_t = x^{(1)}_t + \\epsilon_t$，其中 $\\epsilon_t \\sim \\text{Normal}(0, 10^{-4})$ 独立抽取。用列 $x^{(1)}$ 和 $x^{(2)}$ 构成 $X_{\\text{base}}$，并添加一列全为1的截距项，得到 $X \\in \\mathbb{R}^{T \\times 3}$。\n  - 设 $\\beta_{\\text{true}} = [0.7, 0.7, -0.1]^{\\top}$（两个特征权重和一个截距）。\n  - 独立生成计数 $y_t \\sim \\text{Poisson}(\\lambda_t)$，其中 $\\lambda_t = \\exp(x_t^{\\top}\\beta_{\\text{true}})$。\n  - 拟合 GLM 得到 $\\hat{\\beta}$，然后计算在 $\\hat{\\beta}$ 处的负对数似然的 Hessian 矩阵 $H(\\hat{\\beta})$。返回一个布尔值，指示 $H(\\hat{\\beta})$ 是否为半正定，定义为所有特征值大于或等于 $-10^{-8}$。\n\n- 用例 D（LNP 级联；时间滤波器恢复）：\n  - 设 $T = 150$，滤波器长度 $L = 5$，随机种子为 $7$。\n  - 对于 $t = 1,\\dots,T$，独立抽取刺激样本 $s_t \\sim \\text{Uniform}(-1, 1)$。\n  - 设真实滤波器为 $k_{\\text{true}} = [0.5, -0.2, 0.15, 0.0, -0.1]^{\\top}$，偏置为 $b = -0.05$。\n  - 对于每个有效的时间索引 $t = L,\\dots,T$，用刺激延迟 $[s_t, s_{t-1}, \\dots, s_{t-L+1}]$ 构建设计矩阵的一行，并附加一个截距 $1$，得到 $X \\in \\mathbb{R}^{(T-L+1) \\times (L+1)}$。\n  - 独立生成计数 $y_t \\sim \\text{Poisson}(\\lambda_t)$，其中 $\\lambda_t = \\exp(k_{\\text{true}}^{\\top}\\ell_t + b)$，$\\ell_t$ 是时间 $t$ 的延迟向量。\n  - 拟合 GLM 得到 $\\hat{\\beta} = [\\hat{k}^{\\top}, \\hat{b}]^{\\top}$，并返回滤波器系数列表 $\\hat{k}$，作为一个长度为 $L$ 的浮点数列表。\n\n输出规范：\n- 您的程序必须生成单行输出，其中包含四个用例的结果，按 A、B、C、D 的顺序排列，作为一个用方括号括起来的逗号分隔列表。前三个条目必须是用于用例 A 和 B 的浮点数以及用于用例 C 的布尔值，第四个条目必须是用于用例 D 的浮点数列表。例如，格式应类似于 $[\\hat{\\lambda}, \\hat{\\beta}_0, \\text{boolean}, [\\hat{k}_1, \\dots, \\hat{k}_L]]$。\n- 不涉及物理单位或角度单位；所有输出都是无量纲的。",
            "solution": "该问题要求为具有典则对数连接函数的泊松广义线性模型（GLM）实现一个最大似然估计器。估计过程将使用带有回溯线搜索的牛顿法来确保单调收敛。该实现必须通过一系列测试用例进行验证，包括一个标准的多元案例、一个仅含截距的边界案例、一个近似共线性的边界案例，以及一个在神经科学中应用于线性-非线性-泊松（LNP）编码模型的案例。\n\n### 1. 模型公式与目标函数\n\n假设观测数据由数据对 $\\{(x_t, y_t)\\}_{t=1}^T$ 组成，其中 $x_t \\in \\mathbb{R}^p$ 是协变量向量（来自设计矩阵 $X \\in \\mathbb{R}^{T \\times p}$ 的一行），$y_t \\in \\{0, 1, 2, \\dots\\}$ 是一个计数观测值。\n\n该模型由两个部分定义：\n1.  **随机部分**：给定协变量 $x_t$，每个观测值 $y_t$ 的条件分布被假定为率参数为 $\\lambda_t$ 的泊松分布。其概率质量函数（PMF）为：\n    $$p(y_t | \\lambda_t) = \\frac{\\lambda_t^{y_t} e^{-\\lambda_t}}{y_t!}$$\n2.  **系统部分与连接函数**：在 GLM 中，线性预测器 $\\eta_t = x_t^\\top \\beta$ 与分布的均值 $E[y_t | x_t] = \\lambda_t$ 通过连接函数 $g$ 相关联。对于具有典则对数连接函数的泊松 GLM，此关系为 $g(\\lambda_t) = \\log(\\lambda_t) = \\eta_t$。因此，反连接函数是指数函数：\n    $$\\lambda_t = e^{\\eta_t} = e^{x_t^\\top \\beta}$$\n    其中 $\\beta \\in \\mathbb{R}^p$ 是待估计的模型参数向量。\n\n将连接函数代入 PMF，得到观测值 $y_t$ 作为参数 $\\beta$ 的函数的概率：\n$$p(y_t | x_t, \\beta) = \\frac{(e^{x_t^\\top \\beta})^{y_t} \\exp(-e^{x_t^\\top \\beta})}{y_t!}$$\n\n假设在给定协变量的情况下，观测值 $\\{y_t\\}_{t=1}^T$ 是条件独立的，则数据的总似然是各个概率的乘积：\n$$\\mathcal{L}(\\beta) = \\prod_{t=1}^T p(y_t | x_t, \\beta)$$\n为便于分析和数值计算，我们使用对数似然 $L(\\beta) = \\log \\mathcal{L}(\\beta)$：\n$$L(\\beta) = \\sum_{t=1}^T \\log p(y_t | x_t, \\beta) = \\sum_{t=1}^T \\left( y_t \\log(e^{x_t^\\top \\beta}) - e^{x_t^\\top \\beta} - \\log(y_t!) \\right)$$\n$$L(\\beta) = \\sum_{t=1}^T \\left( y_t (x_t^\\top \\beta) - e^{x_t^\\top \\beta} \\right) - \\sum_{t=1}^T \\log(y_t!)$$\n最大似然估计（MLE）旨在找到使 $L(\\beta)$ 最大化的参数向量 $\\hat{\\beta}$。这等价于最小化负对数似然（NLL）。$\\sum \\log(y_t!)$ 项相对于 $\\beta$ 是常数，可以从优化目标中去掉。需要最小化的 NLL 是：\n$$NLL(\\beta) = -\\sum_{t=1}^T \\left( y_t (x_t^\\top \\beta) - e^{x_t^\\top \\beta} \\right) = \\sum_{t=1}^T \\left( e^{x_t^\\top \\beta} - y_t(x_t^\\top \\beta) \\right)$$\n\n### 2. 用于牛顿法的梯度和 Hessian 矩阵\n\n为了应用牛顿法，我们必须计算 NLL 的梯度向量和 Hessian 矩阵。\n\n**梯度**：梯度 $\\nabla NLL(\\beta)$ 是由一阶偏导数 $\\frac{\\partial NLL}{\\partial \\beta_j}$ 组成的向量。\n$$\\frac{\\partial NLL(\\beta)}{\\partial \\beta_j} = \\sum_{t=1}^T \\left( \\frac{\\partial}{\\partial \\beta_j} e^{x_t^\\top \\beta} - \\frac{\\partial}{\\partial \\beta_j} y_t(x_t^\\top \\beta) \\right) = \\sum_{t=1}^T \\left( e^{x_t^\\top \\beta} x_{tj} - y_t x_{tj} \\right)$$\n其中 $x_{tj}$ 是向量 $x_t$ 的第 $j$ 个分量。\n在向量表示法中，设 $\\lambda$ 是由分量 $\\lambda_t = e^{x_t^\\top \\beta}$ 组成的率向量， $y$ 是由观测值 $y_t$ 组成的向量。梯度为：\n$$\\nabla NLL(\\beta) = \\sum_{t=1}^T (e^{x_t^\\top \\beta} - y_t) x_t = X^\\top (\\lambda - y)$$\n\n**Hessian 矩阵**：Hessian 矩阵 $H(\\beta)$ 是由二阶偏导数组成的矩阵，$H_{jk}(\\beta) = \\frac{\\partial^2 NLL}{\\partial \\beta_j \\partial \\beta_k}$。\n$$\\frac{\\partial^2 NLL(\\beta)}{\\partial \\beta_j \\partial \\beta_k} = \\frac{\\partial}{\\partial \\beta_k} \\left( \\sum_{t=1}^T (\\lambda_t - y_t) x_{tj} \\right) = \\sum_{t=1}^T \\frac{\\partial \\lambda_t}{\\partial \\beta_k} x_{tj} = \\sum_{t=1}^T (e^{x_t^\\top \\beta} x_{tk}) x_{tj} = \\sum_{t=1}^T \\lambda_t x_{tj} x_{tk}$$\n在矩阵表示法中，这是：\n$$H(\\beta) = \\sum_{t=1}^T \\lambda_t x_t x_t^\\top = X^\\top W X$$\n其中 $W$ 是一个 $T \\times T$ 的对角矩阵，其对角线元素为 $W_{tt} = \\lambda_t = e^{x_t^\\top \\beta}$。\n\n**凸性**：由于对所有 $t$ 都有 $\\lambda_t  0$，对角矩阵 $W$ 是正定的。对于任意非零向量 $v \\in \\mathbb{R}^p$，二次型为 $v^\\top H v = v^\\top X^\\top W X v = (Xv)^\\top W (Xv)$。该值始终非负。如果设计矩阵 $X$ 具有满列秩，则对于 $v \\neq 0$，有 $Xv \\neq 0$，这使得 Hessian 矩阵是正定的。在一般情况下，Hessian 矩阵是半正定的。这证明了 NLL 目标函数是凸函数，确保找到的任何局部最小值都是全局最小值。\n\n### 3. 优化算法：带线搜索的阻尼牛顿法\n\n牛顿法通过在每一步用二次碗型函数逼近原函数来迭代地寻找最小值。\n更新规则是：\n$$\\beta_{k+1} = \\beta_k - \\alpha_k H(\\beta_k)^{-1} \\nabla NLL(\\beta_k)$$\n其中 $\\alpha_k$ 是步长。搜索方向为 $\\Delta \\beta_k = -H(\\beta_k)^{-1} \\nabla NLL(\\beta_k)$。\n\n**阻尼与数值稳定性**：如果设计矩阵 $X$ 的列是共线或近似共线的（如用例 C 中），Hessian 矩阵 $H(\\beta_k)$ 可能是病态或奇异的。为确保矩阵可逆且步长定义良好，我们在对角线上添加一个小的正阻尼项：\n$$H_{\\delta}(\\beta_k) = H(\\beta_k) + \\delta I$$\n其中 $\\delta  0$ 是一个小的标量，$I$ 是单位矩阵。然后通过求解线性系统来计算搜索方向：\n$$H_{\\delta}(\\beta_k) \\Delta\\beta_k = - \\nabla NLL(\\beta_k)$$\n\n**回溯线搜索**：完整的牛顿步长（$\\alpha_k = 1$）不保证 NLL 的减小。为确保单调收敛，我们使用回溯线搜索来找到合适的步长 $\\alpha_k$。从 $\\alpha=1$ 开始，我们迭代地将其乘以一个因子 $\\tau \\in (0, 1)$，直到满足 Armijo 条件：\n$$NLL(\\beta_k + \\alpha \\Delta\\beta_k) \\le NLL(\\beta_k) + c \\alpha (\\nabla NLL(\\beta_k))^\\top \\Delta\\beta_k$$\n对于某个常数 $c \\in (0, 1)$。这个条件确保了目标函数在每一步都有充分的下降。\n\n### 4. 测试用例的实现\n\n所描述的算法在一个名为 `fit_poisson_glm` 的函数中实现，然后用该函数解决四个指定的测试用例。\n\n- **用例 A 和 B**：这些是标准的 GLM 拟合问题。在用例 B 中，设计矩阵只是一个全为 1 的列，用于测试仅含截距的模型。用例 B 的解析解 $\\hat{\\beta} = \\log(\\bar{y})$ 可作为合理性检查。\n- **用例 C**：此用例在设计矩阵 $X$ 中引入了近似共线性。牛顿求解器中的阻尼对于数值稳定性至关重要。找到解 $\\hat{\\beta}$ 后，计算未阻尼的 Hessian 矩阵 $H(\\hat{\\beta})$，并检查其特征值以验证其半正定性，这是模型凸性的直接结果。\n- **用例 D**：此用例将 GLM 应用于 LNP 模型。从一个刺激时间序列构建一个设计矩阵。矩阵的每一行对应一个时间点 $t$，并包含 $L$ 个先前延迟的刺激值 $[s_t, s_{t-1}, \\dots, s_{t-L+1}]$ 以及一个截距。响应向量 $y$ 与这些时间点对齐。通过将 GLM 拟合到这个构建的数据，可以恢复线性滤波器系数 $\\hat{k}$ 和偏置项 $\\hat{b}$ 作为估计的参数 $\\hat{\\beta}$。\n\n这种从模型的统计定义出发并推导优化组件的有原则的方法，导出了一个稳健且正确的实现。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Implements and tests maximum likelihood estimation for a Poisson GLM\n    using Newton's method with backtracking line search.\n    \"\"\"\n\n    def fit_poisson_glm(X, y, max_iter=100, tol=1e-9, damping=1e-8,\n                        line_search_c=0.25, line_search_tau=0.5):\n        \"\"\"\n        Fits a Poisson GLM with a log link using Newton's method.\n\n        Args:\n            X (np.ndarray): Design matrix of shape (T, p).\n            y (np.ndarray): Observation vector of shape (T,).\n            max_iter (int): Maximum number of iterations.\n            tol (float): Convergence tolerance for the norm of the step.\n            damping (float): Damping factor for the Hessian.\n            line_search_c (float): Armijo condition parameter.\n            line_search_tau (float): Backtracking step size reduction factor.\n\n        Returns:\n            np.ndarray: Estimated parameter vector beta_hat of shape (p,).\n        \"\"\"\n        p = X.shape[1]\n        beta = np.zeros(p)\n\n        def nll(b):\n            eta = np.clip(X @ b, -700, 700) # clip to avoid overflow in exp\n            return np.sum(np.exp(eta) - y * eta)\n\n        for i in range(max_iter):\n            eta = np.clip(X @ beta, -700, 700)\n            lam = np.exp(eta)\n\n            # Gradient\n            grad = X.T @ (lam - y)\n\n            # Hessian\n            W = np.diag(lam)\n            H = X.T @ W @ X\n            \n            # Damped Hessian for numerical stability\n            H_damped = H + damping * np.identity(p)\n\n            # Newton step direction\n            try:\n                delta_beta = np.linalg.solve(H_damped, -grad)\n            except np.linalg.LinAlgError:\n                # Fallback to gradient descent if Hessian is singular despite damping\n                delta_beta = -grad\n            \n            # Backtracking line search\n            alpha = 1.0\n            current_nll = nll(beta)\n            grad_dot_delta = grad.T @ delta_beta\n\n            while nll(beta + alpha * delta_beta) > current_nll + line_search_c * alpha * grad_dot_delta:\n                alpha *= line_search_tau\n                if alpha  1e-12: # Prevent infinite loop\n                    break\n\n            # Update beta\n            step = alpha * delta_beta\n            beta += step\n\n            if np.linalg.norm(step)  tol:\n                break\n        \n        return beta\n\n    results = []\n\n    # Case A: happy path\n    T_A, F_A, seed_A = 80, 3, 42\n    rng_A = np.random.default_rng(seed_A)\n    X_base_A = rng_A.uniform(-0.5, 0.5, size=(T_A, F_A))\n    X_A = np.c_[X_base_A, np.ones(T_A)]\n    beta_true_A = np.array([0.4, -0.6, 0.5, 0.2])\n    lambda_A = np.exp(X_A @ beta_true_A)\n    y_A = rng_A.poisson(lambda_A)\n    beta_hat_A = fit_poisson_glm(X_A, y_A)\n    x_query_A = np.array([0.5, -1.0, 0.8, 1.0])\n    lambda_hat_A = np.exp(x_query_A @ beta_hat_A)\n    results.append(lambda_hat_A)\n\n    # Case B: intercept-only model\n    T_B, seed_B = 60, 123\n    rng_B = np.random.default_rng(seed_B)\n    X_B = np.ones((T_B, 1))\n    lambda_true_B = 1.8\n    y_B = rng_B.poisson(lambda_true_B, size=T_B)\n    beta_hat_B = fit_poisson_glm(X_B, y_B)\n    results.append(beta_hat_B[0])\n\n    # Case C: near collinearity\n    T_C, F_C, seed_C = 100, 2, 21\n    rng_C = np.random.default_rng(seed_C)\n    x1_C = rng_C.uniform(-0.5, 0.5, size=T_C)\n    eps_C = rng_C.normal(0, 1e-4, size=T_C)\n    x2_C = x1_C + eps_C\n    X_base_C = np.c_[x1_C, x2_C]\n    X_C = np.c_[X_base_C, np.ones(T_C)]\n    beta_true_C = np.array([0.7, 0.7, -0.1])\n    lambda_C = np.exp(X_C @ beta_true_C)\n    y_C = rng_C.poisson(lambda_C)\n    beta_hat_C = fit_poisson_glm(X_C, y_C)\n    \n    eta_hat_C = np.clip(X_C @ beta_hat_C, -700, 700)\n    lambda_hat_C = np.exp(eta_hat_C)\n    W_C = np.diag(lambda_hat_C)\n    H_C = X_C.T @ W_C @ X_C\n    eigenvalues_C = np.linalg.eigvalsh(H_C)\n    is_psd_C = np.all(eigenvalues_C >= -1e-8)\n    results.append(is_psd_C)\n    \n    # Case D: LNP cascade\n    T_D, L_D, seed_D = 150, 5, 7\n    rng_D = np.random.default_rng(seed_D)\n    stimulus_D = rng_D.uniform(-1, 1, size=T_D)\n    k_true_D = np.array([0.5, -0.2, 0.15, 0.0, -0.1])\n    b_true_D = -0.05\n    \n    num_samples_D = T_D - L_D + 1\n    X_D = np.zeros((num_samples_D, L_D + 1))\n    y_D = np.zeros(num_samples_D)\n    \n    for i in range(num_samples_D):\n        t = i + L_D - 1 # Current time index in stimulus array\n        lag_vector = stimulus_D[t-L_D+1 : t+1][::-1]\n        X_D[i, :L_D] = lag_vector\n        X_D[i, L_D] = 1.0\n        \n        rate = np.exp(lag_vector @ k_true_D + b_true_D)\n        y_D[i] = rng_D.poisson(rate)\n        \n    beta_hat_D = fit_poisson_glm(X_D, y_D)\n    k_hat_D = beta_hat_D[:L_D].tolist()\n    results.append(k_hat_D)\n    \n    # Final print statement in the exact required format.\n    # Convert boolean to lowercase string 'true'/'false'\n    results[2] = str(results[2]).lower()\n    print(f\"[{results[0]},{results[1]},{results[2]},{results[3]}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "建立并拟合一个模型只是完成了任务的一半，我们还必须严格地评估其性能，判断它在多大程度上捕捉了神经响应的真实规律。本练习聚焦于模型验证，指导你计算一系列关键的拟合优度指标。通过应用时间重整化定理（time-rescaling theorem）和计算偏差（deviance）等方法，你将学会如何定量地评估模型对神经脉冲序列统计结构的解释能力，这是验证和比较不同编码模型的关键一步。",
            "id": "4158979",
            "problem": "给定分箱后的尖峰计数数据以及一个编码模型的相应预测条件均值。该模型可以被看作是带有对数连接函数的广义线性模型（GLM），也可以被看作是一个线性-非线性-泊松（LNP）级联模型。在任何一种情况下，模型都指定每个分箱的尖峰计数（用 $y_t$ 表示）是条件独立的泊松随机变量，其每个分箱的均值为 $\\mu_t = \\mathbb{E}[y_t \\mid x_t]$，其中 $t$ 是时间分箱的索引。您可以假设分箱宽度足够小，以至于对于所有分箱都有 $y_t \\in \\{0,1\\}$。拟合优度必须根据泊松模型的基本原理和时间重标度定理进行评估。\n\n需要使用的基本依据和定义：\n- 均值为 $\\mu_t$ 的泊松分布的概率质量函数为 $p(y_t \\mid \\mu_t) = \\frac{\\mu_t^{y_t} e^{-\\mu_t}}{y_t!}$，每个分箱的对数似然为 $\\ell_t = y_t \\log \\mu_t - \\mu_t - \\log(y_t!)$。\n- 在 $T$ 个分箱上的总对数似然为 $\\mathrm{LL} = \\sum_{t=1}^{T} \\ell_t$。\n- 零模型是一个具有恒定均值 $\\bar{y} = \\frac{1}{T} \\sum_{t=1}^{T} y_t$ 的泊松模型，因此对于所有的 $t$ 都有 $\\mu_t^{\\mathrm{null}} = \\bar{y}$。\n- McFadden伪决定系数定义为 $R^2_{\\mathrm{McF}} = 1 - \\frac{\\mathrm{LL}_{\\mathrm{model}}}{\\mathrm{LL}_{\\mathrm{null}}}$。\n- 相对于饱和模型，该模型的泊松偏差为 $D = 2 \\sum_{t=1}^{T} \\left[ y_t \\log\\left(\\frac{y_t}{\\mu_t}\\right) - (y_t - \\mu_t) \\right]$，并使用约定 $0 \\cdot \\log(0/\\mu_t) = 0$。\n- 分箱 $t$ 中的皮尔逊残差为 $r_t = \\frac{y_t - \\mu_t}{\\sqrt{\\mu_t}}$，离散度估计为 $\\hat{\\phi} = \\frac{1}{T} \\sum_{t=1}^{T} r_t^2$。\n- 对于每个分箱最多一个尖峰的离散时间情况下的时间重标度，定义尖峰所在分箱的索引为 $1 \\le t_1  t_2  \\dots  t_n \\le T$，其中 $y_{t_k} = 1$。定义尖峰之间的积分强度为 $w_k = \\sum_{t=t_{k-1}+1}^{t_k} \\mu_t$，并设 $t_0 = 0$。然后定义 $u_k = 1 - e^{-w_k}$。在一个正确的模型下，$u_k$ 是独立同分布的 Uniform$(0,1)$ 随机变量。柯尔莫哥洛夫-斯米尔诺夫统计量为 $D_{\\mathrm{KS}} = \\max_{1 \\le i \\le n} \\max\\left( \\frac{i}{n} - u_{(i)},\\, u_{(i)} - \\frac{i-1}{n} \\right)$，其中 $u_{(i)}$ 表示第 $i$ 个顺序统计量。\n\n您的任务是实现一个程序，为每个提供的测试用例计算以下四个量：\n- $R^2_{\\mathrm{McF}}$,\n- $D$,\n- $D_{\\mathrm{KS}}$（基于所定义的时间重标度定理）,\n- $\\hat{\\phi}$.\n\n将所有浮点数结果四舍五入到恰好 $6$ 位小数。\n\n测试套件：\n- 案例 $1$ ($T = 20$): $\\mu$ 和 $y$ 分别为\n  - $\\mu = [0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.30, 0.20, 0.10, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45]$,\n  - $y = [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1]$.\n- 案例 $2$ ($T = 20$): $\\mu$ 是一个常数，等于案例 1 中 $y$ 的经验均值，即 $\\mu = [0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]$，而 $y$ 与案例 1 相同。\n- 案例 $3$ ($T = 20$): $\\mu$ 是案例 1 中 $\\mu$ 的反转，即 $\\mu = [0.45, 0.40, 0.35, 0.30, 0.25, 0.20, 0.15, 0.10, 0.05, 0.10, 0.20, 0.30, 0.40, 0.35, 0.30, 0.25, 0.20, 0.15, 0.10, 0.05]$，而 $y$ 与案例 1 相同。\n\n实现要求：\n- 使用上述源自泊松模型和时间重标度定理的定义，完全按照说明来计算这些指标。\n- 所有对数均为自然对数。\n- 对于形如 $y_t \\log(y_t/\\mu_t)$ 的项，按照约定使用 $0 \\cdot \\log(0/\\mu_t) = 0$ 以避免未定义的值。\n- 对于柯尔莫哥洛夫-斯米尔诺夫统计量，请使用基于已排序的 $u_k$ 值的经验分布函数的定义，不要进行任何近似或随机化处理。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含按测试用例顺序连接的所有结果，每个测试用例贡献四个值 $[R^2_{\\mathrm{McF}}, D, D_{\\mathrm{KS}}, \\hat{\\phi}]$，并展平成一个列表。该行必须是一个用方括号括起来的逗号分隔列表。例如，输出格式必须与 $[r_1, d_1, k_1, \\phi_1, r_2, d_2, k_2, \\phi_2, r_3, d_3, k_3, \\phi_3]$ 完全一样，每个值都四舍五入到恰好 $6$ 位小数。",
            "solution": "用户在神经科学数据分析领域提供了一个定义明确的计算问题，特别是关于点过程模型（GLM/LNP）的评估。该问题具有科学依据，内部一致，并包含唯一解所需的所有必要信息。因此，该问题被认为是有效的。解决方案通过为三个测试用例中的每一个实现指定的拟合优度指标来展开。\n\n问题的核心是根据观测到的尖峰计数 $y_t$ 和模型预测的条件平均尖峰率 $\\mu_t$ 来计算四个不同的模型性能指标。数据由一系列 $T$ 个时间分箱组成，其中 $y_t \\in \\{0, 1\\}$ 表示是否存在尖峰。\n\n每个指标的计算方法如下：\n\n**1. McFadden伪决定系数 ($R^2_{\\mathrm{McF}}$)**\n\n该指标将所提出模型的对数似然 $\\mathrm{LL}_{\\mathrm{model}}$ 与一个更简单的零模型 $\\mathrm{LL}_{\\mathrm{null}}$ 的对数似然进行比较。零模型假设一个恒定的发放率，等于经验平均率 $\\bar{y} = \\frac{1}{T} \\sum_{t=1}^{T} y_t$。公式为 $R^2_{\\mathrm{McF}} = 1 - \\frac{\\mathrm{LL}_{\\mathrm{model}}}{\\mathrm{LL}_{\\mathrm{null}}}$。\n\n一个泊松模型在 $T$ 个分箱上的对数似然由 $\\mathrm{LL} = \\sum_{t=1}^{T} \\ell_t = \\sum_{t=1}^{T} [y_t \\log \\mu_t - \\mu_t - \\log(y_t!)]$ 给出。由于问题说明 $y_t \\in \\{0, 1\\}$，项 $\\log(y_t!)$ 总是 $\\log(1) = 0$。因此，简化的对数似然为 $\\mathrm{LL} = \\sum_{t=1}^{T} (y_t \\log \\mu_t - \\mu_t)$。\n\n-   $\\mathrm{LL}_{\\mathrm{model}}$ 使用所提供的模型预测的平均率 $\\mu_t$ 计算。\n-   $\\mathrm{LL}_{\\mathrm{null}}$ 使用一个对所有 $t$ 都为常数率 $\\mu_t^{\\mathrm{null}} = \\bar{y}$ 来计算。计算变为 $\\mathrm{LL}_{\\mathrm{null}} = \\sum_{t=1}^{T} (y_t \\log \\bar{y} - \\bar{y})$。\n\n**2. 泊松偏差 ($D$)**\n\n偏差衡量的是模型相对于一个能完美拟合数据（即 $\\mu_t^{\\mathrm{sat}} = y_t$）的“饱和”模型的拟合优度。公式为 $D = 2 \\sum_{t=1}^{T} \\left[ y_t \\log\\left(\\frac{y_t}{\\mu_t}\\right) - (y_t - \\mu_t) \\right]$。\n\n项 $y_t \\log(y_t/\\mu_t)$ 由于可能出现 $\\log(0)$ 而需要小心处理。\n-   如果 $y_t = 1$，该项变为 $1 \\cdot \\log(1/\\mu_t) = -\\log(\\mu_t)$。\n-   如果 $y_t = 0$，问题指定了约定 $0 \\cdot \\log(0/\\mu_t) = 0$。\n因此，表达式的这部分求和只需要在发生尖峰的时间分箱（$y_t=1$）上进行。总和计算为 $D = 2 \\left( \\sum_{t: y_t=1} (-\\log \\mu_t) - \\sum_{t=1}^{T}(y_t - \\mu_t) \\right)$。\n\n**3. 柯尔莫哥洛夫-斯米尔诺夫统计量 ($D_{\\mathrm{KS}}$)**\n\n该指标基于时间重标度定理。对于一个正确的模型，转换后的尖峰时间应呈均匀分布。步骤如下：\n-   首先，识别发生尖峰的从0开始索引的时间分箱，设它们为 $s_0, s_1, \\dots, s_{n-1}$。这些对应于问题描述中从1开始索引的时间 $t_1, t_2, \\dots, t_n$。\n-   接下来，计算连续尖峰之间的积分强度，$w_k = \\sum_{t=t_{k-1}+1}^{t_k} \\mu_t$，并设 $t_0=0$。这可以通过首先计算 $\\mu_t$ 的累积和来高效实现，我们称之为 $C_{\\mu}[i] = \\sum_{j=0}^{i} \\mu_j$。积分强度即为 $w_1 = C_{\\mu}[s_0]$ 以及当 $k  1$ 时 $w_k = C_{\\mu}[s_{k-1}] - C_{\\mu}[s_{k-2}]$。\n-   将这些积分强度转换为新的随机变量 $u_k = 1 - e^{-w_k}$。如果模型正确，集合 $\\{u_k\\}$ 将是来自 Uniform$(0,1)$ 分布的样本。\n-   最后，计算柯尔莫哥洛夫-斯米尔诺夫统计量 $D_{\\mathrm{KS}}$ 来量化 $\\{u_k\\}$ 的经验分布与均匀分布累积分布函数（CDF）的偏差。设 $u_{(i)}$ 是 $u_k$ 的排序值。该统计量为 $D_{\\mathrm{KS}} = \\max_{1 \\le i \\le n} \\max\\left( \\frac{i}{n} - u_{(i)},\\, u_{(i)} - \\frac{i-1}{n} \\right)$，其中 $n$ 是尖峰的总数。\n\n**4. 离散度估计 ($\\hat{\\phi}$)**\n\n离散度提供了一个度量，用于判断数据的方差是否与泊松模型一致，泊松模型假设方差等于均值。它是根据皮尔逊残差 $r_t = \\frac{y_t - \\mu_t}{\\sqrt{\\mu_t}}$ 计算的。离散度估计是皮尔逊残差平方的均值：$\\hat{\\phi} = \\frac{1}{T} \\sum_{t=1}^{T} r_t^2 = \\frac{1}{T} \\sum_{t=1}^{T} \\frac{(y_t - \\mu_t)^2}{\\mu_t}$。对于一个真正的泊松过程，我们期望 $\\hat{\\phi} \\approx 1$。值 $\\hat{\\phi}  1$ 表明过离散（方差大于均值），而 $\\hat{\\phi}  1$ 表明欠离散。\n\n这四项计算将应用于问题中指定的三个测试用例中的每一个。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes goodness-of-fit metrics for Poisson encoding models based on\n    provided spike data and predicted means.\n    \"\"\"\n    # Define the data for the three test cases as specified in the problem.\n    mu_1_list = [0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.30, 0.20, 0.10, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45]\n    y_1_list = [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1]\n    \n    y_1_arr = np.array(y_1_list, dtype=float)\n    \n    # Case 1\n    mu_1_arr = np.array(mu_1_list, dtype=float)\n    \n    # Case 2: mu is constant at the empirical mean of y from Case 1\n    y_bar_1 = np.mean(y_1_arr)\n    mu_2_arr = np.full_like(y_1_arr, y_bar_1, dtype=float)\n    \n    # Case 3: mu is the reversal of mu from Case 1\n    mu_3_arr = mu_1_arr[::-1]\n\n    test_cases = [\n        (y_1_arr, mu_1_arr),\n        (y_1_arr, mu_2_arr),\n        (y_1_arr, mu_3_arr)\n    ]\n\n    all_results = []\n    \n    for y, mu in test_cases:\n        T = float(len(y))\n\n        # 1. McFadden's Pseudo R^2\n        # Log-likelihood for Poisson model with y in {0,1} is sum(y*log(mu) - mu).\n        # np.log(mu) is safe as all provided mu > 0.\n        ll_model = np.sum(y * np.log(mu) - mu)\n        \n        y_bar = np.mean(y)\n        # Check for y_bar=0 is good practice, but not needed for this problem's data.\n        ll_null = np.sum(y * np.log(y_bar) - y_bar) if y_bar > 0 else 0.0\n        \n        # If ll_null is 0, R^2 is undefined or handled by convention.\n        # This occurs if y_bar=0, meaning no spikes. Here y_bar = 0.25.\n        r2_mcf = 1.0 - ll_model / ll_null\n        all_results.append(r2_mcf)\n\n        # 2. Poisson Deviance\n        # D = 2 * sum(y*log(y/mu) - (y-mu))\n        # The term y*log(y/mu) is 0 if y=0, and -log(mu) if y=1.\n        spike_indices = (y == 1)\n        \n        log_term_sum = np.sum(-np.log(mu[spike_indices]))\n        diff_term_sum = -np.sum(y - mu)\n\n        deviance = 2.0 * (log_term_sum + diff_term_sum)\n        all_results.append(deviance)\n\n        # 3. Kolmogorov-Smirnov Statistic from Time-Rescaling\n        t_spike_indices = np.where(y == 1)[0]\n        n_spikes = len(t_spike_indices)\n        \n        if n_spikes == 0:\n            ks_stat = 0.0\n        else:\n            # Calculate integrated intensities w_k between spikes\n            cum_mu = np.cumsum(mu)\n            cum_mu_at_spikes = cum_mu[t_spike_indices]\n            w_k = np.diff(cum_mu_at_spikes, prepend=0)\n            \n            # Rescale to u_k = 1 - exp(-w_k)\n            u_k = 1.0 - np.exp(-w_k)\n            u_sorted = np.sort(u_k)\n            \n            # Calculate KS statistic D_KS\n            i_vec = np.arange(1, n_spikes + 1)\n            term1 = i_vec / n_spikes - u_sorted\n            term2 = u_sorted - (i_vec - 1) / n_spikes\n            ks_stat = np.max(np.maximum(term1, term2))\n        all_results.append(ks_stat)\n\n        # 4. Dispersion Estimate\n        # phi_hat = (1/T) * sum( (y - mu)^2 / mu )\n        pearson_residuals_sq = ((y - mu)**2) / mu\n        phi_hat = np.mean(pearson_residuals_sq)\n        all_results.append(phi_hat)\n\n    # Format the final output string exactly as required, with 6 decimal places.\n    formatted_results = [f'{x:.6f}' for x in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}