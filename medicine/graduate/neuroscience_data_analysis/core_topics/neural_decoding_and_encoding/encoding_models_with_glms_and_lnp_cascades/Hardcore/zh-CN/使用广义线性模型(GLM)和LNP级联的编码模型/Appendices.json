{
    "hands_on_practices": [
        {
            "introduction": "在构建编码模型时，一个关键步骤是表示神经元如何整合随时间变化的刺激。通常，这是通过一个线性滤波器（或称为核）来实现的。此练习将指导您使用一组基函数来构建这样一个滤波器，这是一种在降低模型复杂性的同时捕捉平滑时间动态的强大技术。",
            "id": "4158967",
            "problem": "考虑一个神经科学中的编码模型，其中时间核用于广义线性模型（GLM, Generalized Linear Model）或线性-非线性-泊松（LNP, Linear-Nonlinear-Poisson）级联。时间核 $k(t)$ 将通过在时间基函数中的低维展开来近似。您的任务是在对数拉伸的时间轴上构建一个升余弦时间基函数族，计算已知真实核的最小二乘岭近似，然后通过离散近似报告在连续时间极限下的相对平方误差。\n\n使用以下在编码模型中标准但需要明确实现细节的定义和设置：\n\n1. 时间网格和核函数：\n   - 设总持续时间为 $T = 0.2$ 秒，时间步长为 $\\Delta t = 0.001$ 秒。定义采样时间 $t_n = n \\Delta t$，其中 $n = 0, 1, \\ldots, N$ 且 $N = T / \\Delta t$。\n   - 在此网格上定义真实时间核为 $k_{\\text{true}}(t) = t \\exp(-t / \\tau)$，时间常数 $\\tau = 0.03$ 秒。\n\n2. 对数时间扭曲和升余弦基的构造：\n   - 对于选定的偏移量 $c  0$，定义拉伸坐标 $\\psi(t) = \\log(t + c)$。\n   - 对于选定的基函数数量 $M \\geq 2$，定义范围 $\\psi_{\\min} = \\log(0 + c)$ 和 $\\psi_{\\max} = \\log(T + c)$。定义 $M$ 个中心 $\\{ \\mu_j \\}_{j=1}^M$，它们在 $\\psi_{\\min}$ 和 $\\psi_{\\max}$ 之间（含端点）线性间隔。设间距为 $\\Delta_{\\psi} = \\mu_{j+1} - \\mu_j$（当 $M \\geq 2$ 时为常数）。\n   - 在原始时间轴上定义第 $j$ 个基函数为\n     $$\n     b_j(t) =\n     \\begin{cases}\n     \\dfrac{1}{2} \\left( 1 + \\cos\\left( \\pi \\dfrac{\\psi(t) - \\mu_j}{\\Delta_{\\psi}} \\right) \\right),  \\text{若 } \\left| \\psi(t) - \\mu_j \\right| \\le \\Delta_{\\psi}, \\\\\n     0,  \\text{其他情况}。\n     \\end{cases}\n     $$\n   - 对于网格上的每个 $t_n$，构建设计矩阵 $B \\in \\mathbb{R}^{(N+1) \\times M}$，其元素为 $B_{n j} = b_j(t_n)$，$j=1,\\ldots,M$。\n\n3. 岭正则化最小二乘投影：\n   - 对于给定的岭参数 $\\lambda \\ge 0$，定义系数向量 $w \\in \\mathbb{R}^M$，使其最小化离散化目标\n     $$\n     J(w) = \\sum_{n=0}^{N} \\left( k_{\\text{true}}(t_n) - \\sum_{j=1}^{M} w_j \\, b_j(t_n) \\right)^2 \\Delta t + \\lambda \\sum_{j=1}^{M} w_j^2.\n     $$\n   - 最小化器满足正规方程\n     $$\n     \\left( B^\\top B \\, \\Delta t + \\lambda I \\right) w = B^\\top k \\, \\Delta t,\n     $$\n     其中 $k \\in \\mathbb{R}^{N+1}$ 的元素为 $k_n = k_{\\text{true}}(t_n)$，$I$ 是 $M \\times M$ 的单位矩阵。\n\n4. 重建与误差度量：\n   - 为所有 $n$ 定义重建核 $\\hat{k}(t_n) = \\sum_{j=1}^{M} w_j \\, b_j(t_n)$。\n   - 使用连续 $L^2$ 范数的黎曼和近似计算相对 $L^2$ 误差：\n     $$\n     E = \\dfrac{\\left\\| \\hat{k} - k \\right\\|_{2,\\Delta t}}{\\left\\| k \\right\\|_{2,\\Delta t}} = \\dfrac{\\sqrt{ \\sum_{n=0}^{N} \\left( \\hat{k}(t_n) - k_{\\text{true}}(t_n) \\right)^2 \\Delta t }}{\\sqrt{ \\sum_{n=0}^{N} \\left( k_{\\text{true}}(t_n) \\right)^2 \\Delta t }}.\n     $$\n\n实现一个程序，对以下测试套件中的每个参数三元组 $(M, \\lambda, c)$ 执行上述步骤，并返回相应的 $E$ 值：\n\n- 测试用例 1：$M = 8$，$\\lambda = 0.0$， $c = 0.005$。\n- 测试用例 2：$M = 3$，$\\lambda = 0.0$， $c = 0.005$。\n- 测试用例 3：$M = 8$，$\\lambda = 0.001$，$c = 0.005$。\n- 测试用例 4：$M = 8$，$\\lambda = 0.0$， $c = 0.0005$。\n- 测试用例 5：$M = 12$，$\\lambda = 0.1$， $c = 0.005$。\n\n您的程序必须：\n- 使用上面指定的精确网格，$T = 0.2$ 秒，$\\Delta t = 0.001$ 秒。\n- 按照定义构造升余弦基，不进行任何额外的归一化或修改。\n- 对每个测试用例求解 $w$ 并精确计算 $E$。\n- 生成单行输出，包含方括号括起来的逗号分隔列表形式的结果，每个 $E$ 值四舍五入到六位小数（例如，$[0.123456,0.234567,0.345678,0.456789,0.567890]$）。\n\n此任务中没有随机性，最终的误差值不需要物理单位。角度单位不适用。测试套件的输出是实数，最终的程序输出必须是按顺序汇总所有测试用例的、符合指定格式的单行文本。",
            "solution": "该问题是有效的，因为它是科学上合理、适定且客观的。它提供了一套完整且一致的定义和参数，以解决计算神经科学中的一个标准问题：使用基函数展开来近似一个时间核。解决方案需要实现指定的数值程序。\n\n解决方案主要分四步进行：建立离散时间网格和真实核，根据升余弦基函数构造设计矩阵，求解基权重的岭回归问题，最后计算相对近似误差。\n\n首先，我们建立离散时间域。总持续时间为 $T = 0.2$ 秒，时间步长为 $\\Delta t = 0.001$ 秒。这定义了一个包含 $N+1$ 个时间点的网格 $t_n = n \\Delta t$，其中 $N = T/\\Delta t = 200$，因此 $n$ 的范围从 $0$ 到 $200$。时间点即为 $t = \\{0, 0.001, 0.002, \\ldots, 0.2\\}$。在这个网格上，我们使用给定的时间常数 $\\tau = 0.03$ 秒来评估真实核，得到一个向量 $k \\in \\mathbb{R}^{N+1}$，其元素为 $k_n = k_{\\text{true}}(t_n) = t_n \\exp(-t_n / \\tau)$。\n\n其次，对于由参数三元组 $(M, \\lambda, c)$ 定义的每个测试用例，我们构造相应的设计矩阵 $B \\in \\mathbb{R}^{(N+1) \\times M}$。这涉及到对时间轴进行对数扭曲。扭曲坐标定义为 $\\psi(t) = \\log(t + c)$，其中 $c$ 是一个小的正常数偏移量。这个扭曲坐标在我们的时间域上的范围是 $[\\psi_{\\min}, \\psi_{\\max}]$，其中 $\\psi_{\\min} = \\log(c)$ 和 $\\psi_{\\max} = \\log(T + c)$。我们定义 $M$ 个中心 $\\{\\mu_j\\}_{j=1}^M$，它们在此区间内线性间隔，包含端点。对于 $M \\ge 2$，相邻中心之间的恒定间距为 $\\Delta_{\\psi} = (\\psi_{\\max} - \\psi_{\\min}) / (M - 1)$。\n\n第 $j$ 个基函数 $b_j(t)$ 是一个在扭曲的 $\\psi$ 空间中以 $\\mu_j$ 为中心的升余弦函数。其在时间 $t$ 的值为\n$$\nb_j(t) = \\frac{1}{2} \\left( 1 + \\cos\\left( \\pi \\frac{\\psi(t) - \\mu_j}{\\Delta_{\\psi}} \\right) \\right)\n$$\n如果其参数在其支撑域内，具体来说是当 $|\\psi(t) - \\mu_j| \\le \\Delta_{\\psi}$ 时，否则 $b_j(t) = 0$。通过在 $N+1$ 个时间点上评估 $M$ 个基函数中的每一个来填充设计矩阵 $B$，使得 $B_{nj} = b_j(t_n)$。\n\n第三，我们找到最优权重向量 $w \\in \\mathbb{R}^M$，它将真实核近似为基函数的线性组合 $\\hat{k} = Bw$。这些权重是通过最小化一个岭正则化最小二乘目标函数来确定的。这个最小化过程导出正规方程，即一个关于 $w$ 的线性方程组：\n$$\n\\left( B^\\top B \\, \\Delta t + \\lambda I \\right) w = B^\\top k \\, \\Delta t\n$$\n这里，$B^\\top$ 是 $B$ 的转置，$I$ 是 $M \\times M$ 的单位矩阵，$\\lambda$ 是岭正则化参数。这个线性系统形式为 $A w = b_{rhs}$，其中 $A = (B^\\top B \\, \\Delta t + \\lambda I)$ 且 $b_{rhs} = (B^\\top k \\, \\Delta t)$。我们使用标准的数值线性代数方法求解这个系统以得到 $w$。\n\n第四，计算出最优权重 $w$ 后，我们构建重建核 $\\hat{k}$，它是一个由值 $\\hat{k}_n = \\hat{k}(t_n) = \\sum_{j=1}^{M} w_j b_j(t_n)$ 组成的向量。此近似的误差通过相对 $L^2$ 误差 $E$ 来量化，即误差向量和真实核向量的范数之比。其离散形式为：\n$$\nE = \\frac{\\left\\| \\hat{k} - k \\right\\|_{2,\\Delta t}}{\\left\\| k \\right\\|_{2,\\Delta t}} = \\frac{\\sqrt{ \\sum_{n=0}^{N} ( \\hat{k}_n - k_n )^2 \\Delta t }}{\\sqrt{ \\sum_{n=0}^{N} k_n^2 \\Delta t }}\n$$\n因子 $\\sqrt{\\Delta t}$ 同时出现在分子和分母中，因此可以消去。误差可以简单地计算为向量 $(\\hat{k} - k)$ 和 $k$ 的标准欧几里得范数之比。对每个测试用例执行此计算，并收集所得的误差值。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the relative squared-error for ridge-regularized least-squares\n    approximations of a temporal kernel using raised-cosine basis functions.\n    \"\"\"\n\n    # 1. Define global parameters and ground-truth kernel\n    T = 0.2\n    dt = 0.001\n    N = int(T / dt)\n    t = np.linspace(0, T, N + 1)\n    \n    tau = 0.03\n    k_true = t * np.exp(-t / tau)\n\n    def calculate_approximation_error(params):\n        \"\"\"\n        Calculates the error for a single set of parameters (M, lam, c).\n        \"\"\"\n        M, lam, c = params\n        \n        # 2. Construct raised-cosine basis functions\n        psi = lambda time: np.log(time + c)\n        psi_t = psi(t)\n        \n        psi_min = psi(0.0)\n        psi_max = psi(T)\n\n        if M  2:\n            # The problem states M >= 2. If M=1, delta_psi is ill-defined.\n            # A reasonable value could be psi_max - psi_min, but we follow the spec.\n            # This path will not be taken by the test cases.\n            raise ValueError(\"M must be >= 2 for this basis construction.\")\n        \n        mu_centers = np.linspace(psi_min, psi_max, M)\n        delta_psi = mu_centers[1] - mu_centers[0]\n\n        # Populate the design matrix B\n        B = np.zeros((N + 1, M))\n        for j in range(M):\n            mu_j = mu_centers[j]\n            # Argument for the cosine, normalized by required spacing\n            x = (psi_t - mu_j) / delta_psi\n            \n            # Basis is non-zero only where |x| = 1\n            valid_indices = np.abs(x) = 1\n            \n            # Calculate basis function values on its support\n            B[valid_indices, j] = 0.5 * (1 + np.cos(np.pi * x[valid_indices]))\n\n        # 3. Solve for weights using ridge regression normal equations\n        # (B.T @ B * dt + lam * I) @ w = B.T @ k_true * dt\n        identity_M = np.eye(M)\n        A = B.T @ B * dt + lam * identity_M\n        b_rhs = B.T @ k_true * dt\n        \n        w = np.linalg.solve(A, b_rhs)\n        \n        # 4. Reconstruct the kernel and compute the error\n        k_hat = B @ w\n        \n        # The error E is the ratio of L2 norms. The sqrt(dt) term cancels.\n        norm_error_vec = np.linalg.norm(k_hat - k_true)\n        norm_true_vec = np.linalg.norm(k_true)\n        \n        if norm_true_vec == 0:\n            return 0.0 if norm_error_vec == 0 else np.inf\n        \n        error = norm_error_vec / norm_true_vec\n        return error\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (8, 0.0, 0.005),\n        (3, 0.0, 0.005),\n        (8, 0.001, 0.005),\n        (8, 0.0, 0.0005),\n        (12, 0.1, 0.005),\n    ]\n\n    results = []\n    for case in test_cases:\n        error = calculate_approximation_error(case)\n        results.append(error)\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在定义了模型的特征（例如，使用基函数构建的刺激历史）之后，下一个核心任务是估计模型的参数，以使模型预测与观测到的神经元发放数据尽可能吻合。本练习将带您从第一性原理出发，为泊松广义线性模型（GLM）实现最大似然估计算法。通过亲自编写优化器，您将深入理解这些编码模型的拟合过程。",
            "id": "4158982",
            "problem": "你的任务是，在一个包含线性-非线性-泊松 (LNP) 级联的编码模型背景下，为具有泊松观测和典范对数链接函数的广义线性模型 (GLM) 实现并测试最大似然估计。你的实现必须基于原理，从泊松分布和 GLM 框架的定义开始，并且必须使用带有回溯线搜索的 Newton 方法，以保证目标函数的单调改进。你还必须通过解处的 Hessian 矩阵来验证目标函数的凸性。程序必须是自包含的，并产生确定性的结果。\n\n基本原理：\n- 对于一个率参数为 $\\lambda$ 的泊松随机变量 $y$，其概率质量函数为 $p(y \\mid \\lambda) = \\exp(-\\lambda)\\,\\lambda^{y}/y!$，其中 $y \\in \\{0,1,2,\\dots\\}$ 且 $\\lambda  0$。\n- 在广义线性模型 (GLM) 中，条件期望 $E[y \\mid x]$ 通过一个链接函数与线性预测器 $x^{\\top}\\beta$ 相关联。对于泊松 GLM 中的典范对数链接函数，平均率是 $\\lambda = \\exp(x^{\\top}\\beta)$。\n- 在线性-非线性-泊松 (LNP) 级联中，驱动是通过将一个线性时空滤波器应用于刺激得到的，然后经过一个将驱动映射到发放率的逐点非线性变换，最后是一个泊松尖峰生成过程。\n\n需要实现的任务：\n1. 给定设计矩阵 $X \\in \\mathbb{R}^{T \\times p}$ 和计数观测值 $y \\in \\mathbb{N}^{T}$，使用观测间的独立性假设，为带有对数链接函数的泊松 GLM 推导出负对数似然目标函数，并实现带有回溯线搜索的 Newton 方法来最小化该目标。你的推导必须从泊松概率质量函数和典范对数链接的 GLM 定义开始，不得假定任何“捷径”公式。\n2. 使用你的估计器来拟合 $\\hat{\\beta}$，并为一个指定的测试套件计算所需的量。你必须通过在 Newton 更新中为 Hessian 矩阵使用一个正阻尼项以及使用回溯线搜索来保证每个被接受的步骤都使目标函数下降，从而确保数值鲁棒性。\n3. 对于一个共线性边界情况，计算拟合解 $\\hat{\\beta}$ 处的负对数似然的 Hessian 矩阵，并使用一个带有小数值容差的基于特征值的准则，返回一个布尔值，指示其是否为半正定。\n4. 对于一个 LNP 级联情况，根据时间刺激延迟构建一个设计矩阵，并通过 GLM 拟合恢复线性滤波器系数和偏置项。\n\n数据生成和测试套件：\n所有随机量都必须使用指定的种子通过如下所示的独立抽样生成，并且所有矩阵构建在给定这些种子的情况下必须是确定性的。共有四个测试用例：\n\n- 用例 A (正常路径；带截距的多变量协变量)：\n  - 设 $T = 80$，特征数量 $F = 3$，随机种子为 $42$。\n  - 从 $\\text{Uniform}(-0.5, 0.5)$ 中独立抽取元素生成 $X_{\\text{base}} \\in \\mathbb{R}^{T \\times F}$，然后添加一个全为 1 的截距列以获得 $X \\in \\mathbb{R}^{T \\times (F+1)}$。\n  - 设真实系数为 $\\beta_{\\text{true}} = [0.4, -0.6, 0.5, 0.2]^{\\top}$（最后一项是截距）。\n  - 对于 $t = 1,\\dots,T$，独立地生成计数 $y_t \\sim \\text{Poisson}(\\lambda_t)$，其中 $\\lambda_t = \\exp(x_t^{\\top}\\beta_{\\text{true}})$。\n  - 拟合 GLM 得到 $\\hat{\\beta}$，并计算查询向量 $\\tilde{x} = [0.5, -1.0, 0.8, 1]^{\\top}$ 的预测率 $\\hat{\\lambda} = \\exp(\\tilde{x}^{\\top}\\hat{\\beta})$。以浮点数形式返回 $\\hat{\\lambda}$。\n\n- 用例 B (边界情况；仅截距模型)：\n  - 设 $T = 60$，随机种子为 $123$。\n  - 使用一个由全为 1 的截距列组成的 $X \\in \\mathbb{R}^{T \\times 1}$。\n  - 设真实的恒定率为 $\\lambda = 1.8$；对于 $t = 1,\\dots,T$，独立地生成 $y_t \\sim \\text{Poisson}(\\lambda)$。\n  - 拟合 GLM 得到 $\\hat{\\beta} \\in \\mathbb{R}$ (仅截距)。以浮点数形式返回标量截距估计值 $\\hat{\\beta}$。\n\n- 用例 C (边界情况；近似共线性)：\n  - 设 $T = 100$, $F = 2$，随机种子为 $21$。\n  - 独立地抽取 $x^{(1)}_t \\sim \\text{Uniform}(-0.5, 0.5)$。独立地设 $x^{(2)}_t = x^{(1)}_t + \\epsilon_t$，其中 $\\epsilon_t \\sim \\text{Normal}(0, 10^{-4})$。用列 $x^{(1)}$ 和 $x^{(2)}$ 构建 $X_{\\text{base}}$，并添加一个全为 1 的截距列以获得 $X \\in \\mathbb{R}^{T \\times 3}$。\n  - 设 $\\beta_{\\text{true}} = [0.7, 0.7, -0.1]^{\\top}$（两个特征权重和一个截距）。\n  - 独立地生成计数 $y_t \\sim \\text{Poisson}(\\lambda_t)$，其中 $\\lambda_t = \\exp(x_t^{\\top}\\beta_{\\text{true}})$。\n  - 拟合 GLM 得到 $\\hat{\\beta}$，然后计算在 $\\hat{\\beta}$ 处的负对数似然的 Hessian 矩阵 $H(\\hat{\\beta})$。返回一个布尔值，指示 $H(\\hat{\\beta})$ 是否为半正定，其定义为所有特征值大于或等于 $-10^{-8}$。\n\n- 用例 D (LNP 级联；时间滤波器恢复)：\n  - 设 $T = 150$，滤波器长度 $L = 5$，随机种子为 $7$。\n  - 对于 $t = 1,\\dots,T$，独立地抽取刺激样本 $s_t \\sim \\text{Uniform}(-1, 1)$。\n  - 设真实的滤波器为 $k_{\\text{true}} = [0.5, -0.2, 0.15, 0.0, -0.1]^{\\top}$，偏置为 $b = -0.05$。\n  - 对于每个有效的时间索引 $t = L,\\dots,T$，用刺激延迟 $[s_t, s_{t-1}, \\dots, s_{t-L+1}]$ 构建设计矩阵的一行，并附加一个截距 1，得到 $X \\in \\mathbb{R}^{(T-L+1) \\times (L+1)}$。\n  - 独立地生成计数 $y_t \\sim \\text{Poisson}(\\lambda_t)$，其中 $\\lambda_t = \\exp(k_{\\text{true}}^{\\top}\\ell_t + b)$，$\\ell_t$ 是时间 $t$ 的延迟向量。\n  - 拟合 GLM 得到 $\\hat{\\beta} = [\\hat{k}^{\\top}, \\hat{b}]^{\\top}$，并以长度为 $L$ 的浮点数列表形式返回滤波器系数 $\\hat{k}$。\n\n输出规范：\n- 你的程序必须生成单行输出，其中包含四个用例 A、B、C、D 的结果，按顺序排列，形式为用方括号括起来的逗号分隔列表。前三项必须是用例 A 和用例 B 的浮点数以及用例 C 的布尔值，第四项必须是用例 D 的浮点数列表。例如，格式应类似于 $[\\hat{\\lambda}, \\hat{\\beta}_0, \\text{boolean}, [\\hat{k}_1, \\dots, \\hat{k}_L]]$。\n- 不涉及物理单位或角度单位；所有输出均为无量纲。",
            "solution": "本问题要求为一个带有典范对数链接函数的泊松广义线性模型 (GLM) 实现最大似然估计器。估计过程需使用带有回溯线搜索的 Newton 方法来执行，以确保单调收敛。该实现必须通过一系列测试用例进行验证，包括一个标准的多变量用例、一个仅截距的边界用例、一个近似共线性的边界用例，以及一个应用于神经科学中的线性-非线性-泊松 (LNP) 编码模型的用例。\n\n### 1. 模型构建与目标函数\n\n设观测数据由数据对 $\\{(x_t, y_t)\\}_{t=1}^T$ 组成，其中 $x_t \\in \\mathbb{R}^p$ 是一个协变量向量（来自设计矩阵 $X \\in \\mathbb{R}^{T \\times p}$ 的一行），$y_t \\in \\{0, 1, 2, \\dots\\}$ 是一个计数观测值。\n\n该模型由两个部分定义：\n1.  **随机部分**：给定协变量 $x_t$，每个观测值 $y_t$ 的条件分布被假定为率参数为 $\\lambda_t$ 的泊松分布。其概率质量函数 (PMF) 为：\n    $$p(y_t | \\lambda_t) = \\frac{\\lambda_t^{y_t} e^{-\\lambda_t}}{y_t!}$$\n2.  **系统部分与链接函数**：在 GLM 中，线性预测器 $\\eta_t = x_t^\\top \\beta$ 通过一个链接函数 $g$ 与分布的均值 $E[y_t | x_t] = \\lambda_t$ 相关联。对于带有典范对数链接函数的泊松 GLM，此关系为 $g(\\lambda_t) = \\log(\\lambda_t) = \\eta_t$。因此，反链接函数是指数函数：\n    $$\\lambda_t = e^{\\eta_t} = e^{x_t^\\top \\beta}$$\n    其中 $\\beta \\in \\mathbb{R}^p$ 是待估计的模型参数向量。\n\n将链接函数代入 PMF，得到观测值 $y_t$ 作为参数 $\\beta$ 的函数的概率：\n$$p(y_t | x_t, \\beta) = \\frac{(e^{x_t^\\top \\beta})^{y_t} \\exp(-e^{x_t^\\top \\beta})}{y_t!}$$\n\n假设在给定协变量的情况下，观测值 $\\{y_t\\}_{t=1}^T$ 是条件独立的，则数据的总似然是各个概率的乘积：\n$$\\mathcal{L}(\\beta) = \\prod_{t=1}^T p(y_t | x_t, \\beta)$$\n为分析和计算方便，我们使用对数似然 $L(\\beta) = \\log \\mathcal{L}(\\beta)$：\n$$L(\\beta) = \\sum_{t=1}^T \\log p(y_t | x_t, \\beta) = \\sum_{t=1}^T \\left( y_t \\log(e^{x_t^\\top \\beta}) - e^{x_t^\\top \\beta} - \\log(y_t!) \\right)$$\n$$L(\\beta) = \\sum_{t=1}^T \\left( y_t (x_t^\\top \\beta) - e^{x_t^\\top \\beta} \\right) - \\sum_{t=1}^T \\log(y_t!)$$\n最大似然估计 (MLE) 旨在找到使 $L(\\beta)$ 最大化的参数向量 $\\hat{\\beta}$。这等价于最小化负对数似然 (NLL)。项 $\\sum \\log(y_t!)$ 相对于 $\\beta$ 是一个常数，可以从优化目标中去掉。需要最小化的 NLL 是：\n$$NLL(\\beta) = -\\sum_{t=1}^T \\left( y_t (x_t^\\top \\beta) - e^{x_t^\\top \\beta} \\right) = \\sum_{t=1}^T \\left( e^{x_t^\\top \\beta} - y_t(x_t^\\top \\beta) \\right)$$\n\n### 2. Newton 方法的梯度和 Hessian 矩阵\n\n为了应用 Newton 方法，我们必须计算 NLL 的梯度向量和 Hessian 矩阵。\n\n**梯度**：梯度 $\\nabla NLL(\\beta)$ 是由一阶偏导数 $\\frac{\\partial NLL}{\\partial \\beta_j}$ 构成的向量。\n$$\\frac{\\partial NLL(\\beta)}{\\partial \\beta_j} = \\sum_{t=1}^T \\left( \\frac{\\partial}{\\partial \\beta_j} e^{x_t^\\top \\beta} - \\frac{\\partial}{\\partial \\beta_j} y_t(x_t^\\top \\beta) \\right) = \\sum_{t=1}^T \\left( e^{x_t^\\top \\beta} x_{tj} - y_t x_{tj} \\right)$$\n其中 $x_{tj}$ 是向量 $x_t$ 的第 $j$ 个分量。\n用向量表示法，令 $\\lambda$ 为率向量，其分量为 $\\lambda_t = e^{x_t^\\top \\beta}$，令 $y$ 为观测值向量 $y_t$。梯度为：\n$$\\nabla NLL(\\beta) = \\sum_{t=1}^T (e^{x_t^\\top \\beta} - y_t) x_t = X^\\top (\\lambda - y)$$\n\n**Hessian 矩阵**：Hessian 矩阵 $H(\\beta)$ 是由二阶偏导数 $H_{jk}(\\beta) = \\frac{\\partial^2 NLL}{\\partial \\beta_j \\partial \\beta_k}$ 构成的矩阵。\n$$\\frac{\\partial^2 NLL(\\beta)}{\\partial \\beta_j \\partial \\beta_k} = \\frac{\\partial}{\\partial \\beta_k} \\left( \\sum_{t=1}^T (\\lambda_t - y_t) x_{tj} \\right) = \\sum_{t=1}^T \\frac{\\partial \\lambda_t}{\\partial \\beta_k} x_{tj} = \\sum_{t=1}^T (e^{x_t^\\top \\beta} x_{tk}) x_{tj} = \\sum_{t=1}^T \\lambda_t x_{tj} x_{tk}$$\n用矩阵表示法，即为：\n$$H(\\beta) = \\sum_{t=1}^T \\lambda_t x_t x_t^\\top = X^\\top W X$$\n其中 $W$ 是一个 $T \\times T$ 的对角矩阵，其对角线元素为 $W_{tt} = \\lambda_t = e^{x_t^\\top \\beta}$。\n\n**凸性**：由于对所有 $t$ 都有 $\\lambda_t  0$，对角矩阵 $W$ 是正定的。对于任何非零向量 $v \\in \\mathbb{R}^p$，其二次型为 $v^\\top H v = v^\\top X^\\top W X v = (Xv)^\\top W (Xv)$。这个量总是非负的。如果设计矩阵 $X$ 是满列秩的，那么对于 $v \\neq 0$，有 $Xv \\neq 0$，这使得 Hessian 矩阵是正定的。在一般情况下，Hessian 矩阵是半正定的。这证明了 NLL 目标函数是凸函数，从而确保找到的任何局部最小值都是全局最小值。\n\n### 3. 优化算法：带线搜索的阻尼 Newton 方法\n\nNewton 方法通过在每一步用一个二次碗形函数来近似原函数，从而迭代地找到最小值。\n更新规则是：\n$$\\beta_{k+1} = \\beta_k - \\alpha_k H(\\beta_k)^{-1} \\nabla NLL(\\beta_k)$$\n其中 $\\alpha_k$ 是步长。搜索方向是 $\\Delta \\beta_k = -H(\\beta_k)^{-1} \\nabla NLL(\\beta_k)$。\n\n**阻尼与数值稳定性**：如果设计矩阵 $X$ 具有共线或近似共线的列（如用例 C 所示），Hessian 矩阵 $H(\\beta_k)$ 可能是病态的或奇异的。为确保矩阵可逆且步长定义良好，可在对角线上添加一个小的正阻尼项：\n$$H_{\\delta}(\\beta_k) = H(\\beta_k) + \\delta I$$\n其中 $\\delta  0$ 是一个很小的标量，$I$ 是单位矩阵。然后通过求解线性系统计算搜索方向：\n$$H_{\\delta}(\\beta_k) \\Delta\\beta_k = - \\nabla NLL(\\beta_k)$$\n\n**回溯线搜索**：一个完整的 Newton 步长 ($\\alpha_k = 1$) 并不能保证 NLL 的下降。为确保单调收敛，我们使用回溯线搜索来寻找合适的步长 $\\alpha_k$。从 $\\alpha=1$ 开始，我们通过一个因子 $\\tau \\in (0, 1)$ 迭代地减小它，直到满足 Armijo 条件：\n$$NLL(\\beta_k + \\alpha \\Delta\\beta_k) \\le NLL(\\beta_k) + c \\alpha (\\nabla NLL(\\beta_k))^\\top \\Delta\\beta_k$$\n其中 $c \\in (0, 1)$ 是某个常数。此条件确保目标函数在每一步都有充分的下降。\n\n### 4. 测试用例的实现\n\n所述算法在一个名为 `fit_poisson_glm` 的函数中实现，然后用它来解决四个指定的测试用例。\n\n-   **用例 A 和 B**：这些是标准的 GLM 拟合问题。在用例 B 中，设计矩阵只是一个全为 1 的列，用于测试仅截距模型。用例 B 的解析解 $\\hat{\\beta} = \\log(\\bar{y})$ 可用作健全性检查。\n-   **用例 C**：此用例在设计矩阵 $X$ 中引入了近似共线性。Newton 求解器中的阻尼对于数值稳定性至关重要。找到解 $\\hat{\\beta}$ 后，计算无阻尼的 Hessian 矩阵 $H(\\hat{\\beta})$，并检查其特征值以验证其半正定性，这是模型凸性的直接结果。\n-   **用例 D**：此用例将 GLM 应用于 LNP 模型。设计矩阵是根据刺激时间序列构建的。矩阵的每一行对应一个时间点 $t$，并包含 $L$ 个先前延迟的刺激值 $[s_t, s_{t-1}, \\dots, s_{t-L+1}]$ 外加一个截距。响应向量 $y$ 与这些时间点对齐。将 GLM 拟合到这个构建的数据上，可以恢复出线性滤波器系数 $\\hat{k}$ 和偏置项 $\\hat{b}$ 作为估计参数 $\\hat{\\beta}$。\n\n这种从模型的统计定义出发并推导优化组件的原则性方法，可以得到一个鲁棒且正确的实现。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Implements and tests maximum likelihood estimation for a Poisson GLM\n    using Newton's method with backtracking line search.\n    \"\"\"\n\n    def fit_poisson_glm(X, y, max_iter=100, tol=1e-9, damping=1e-8,\n                        line_search_c=0.25, line_search_tau=0.5):\n        \"\"\"\n        Fits a Poisson GLM with a log link using Newton's method.\n\n        Args:\n            X (np.ndarray): Design matrix of shape (T, p).\n            y (np.ndarray): Observation vector of shape (T,).\n            max_iter (int): Maximum number of iterations.\n            tol (float): Convergence tolerance for the norm of the step.\n            damping (float): Damping factor for the Hessian.\n            line_search_c (float): Armijo condition parameter.\n            line_search_tau (float): Backtracking step size reduction factor.\n\n        Returns:\n            np.ndarray: Estimated parameter vector beta_hat of shape (p,).\n        \"\"\"\n        p = X.shape[1]\n        beta = np.zeros(p)\n\n        def nll(b):\n            eta = np.clip(X @ b, -700, 700) # clip to avoid overflow in exp\n            return np.sum(np.exp(eta) - y * eta)\n\n        for i in range(max_iter):\n            eta = np.clip(X @ beta, -700, 700)\n            lam = np.exp(eta)\n\n            # Gradient\n            grad = X.T @ (lam - y)\n\n            # Hessian\n            W = np.diag(lam)\n            H = X.T @ W @ X\n            \n            # Damped Hessian for numerical stability\n            H_damped = H + damping * np.identity(p)\n\n            # Newton step direction\n            try:\n                delta_beta = np.linalg.solve(H_damped, -grad)\n            except np.linalg.LinAlgError:\n                # Fallback to gradient descent if Hessian is singular despite damping\n                delta_beta = -grad\n            \n            # Backtracking line search\n            alpha = 1.0\n            current_nll = nll(beta)\n            grad_dot_delta = grad.T @ delta_beta\n\n            while nll(beta + alpha * delta_beta) > current_nll + line_search_c * alpha * grad_dot_delta:\n                alpha *= line_search_tau\n                if alpha  1e-12: # Prevent infinite loop\n                    break\n\n            # Update beta\n            step = alpha * delta_beta\n            beta += step\n\n            if np.linalg.norm(step)  tol:\n                break\n        \n        return beta\n\n    results = []\n\n    # Case A: happy path\n    T_A, F_A, seed_A = 80, 3, 42\n    rng_A = np.random.default_rng(seed_A)\n    X_base_A = rng_A.uniform(-0.5, 0.5, size=(T_A, F_A))\n    X_A = np.c_[X_base_A, np.ones(T_A)]\n    beta_true_A = np.array([0.4, -0.6, 0.5, 0.2])\n    lambda_A = np.exp(X_A @ beta_true_A)\n    y_A = rng_A.poisson(lambda_A)\n    beta_hat_A = fit_poisson_glm(X_A, y_A)\n    x_query_A = np.array([0.5, -1.0, 0.8, 1.0])\n    lambda_hat_A = np.exp(x_query_A @ beta_hat_A)\n    results.append(lambda_hat_A)\n\n    # Case B: intercept-only model\n    T_B, seed_B = 60, 123\n    rng_B = np.random.default_rng(seed_B)\n    X_B = np.ones((T_B, 1))\n    lambda_true_B = 1.8\n    y_B = rng_B.poisson(lambda_true_B, size=T_B)\n    beta_hat_B = fit_poisson_glm(X_B, y_B)\n    results.append(beta_hat_B[0])\n\n    # Case C: near collinearity\n    T_C, F_C, seed_C = 100, 2, 21\n    rng_C = np.random.default_rng(seed_C)\n    x1_C = rng_C.uniform(-0.5, 0.5, size=T_C)\n    eps_C = rng_C.normal(0, 1e-4, size=T_C)\n    x2_C = x1_C + eps_C\n    X_base_C = np.c_[x1_C, x2_C]\n    X_C = np.c_[X_base_C, np.ones(T_C)]\n    beta_true_C = np.array([0.7, 0.7, -0.1])\n    lambda_C = np.exp(X_C @ beta_true_C)\n    y_C = rng_C.poisson(lambda_C)\n    beta_hat_C = fit_poisson_glm(X_C, y_C)\n    \n    eta_hat_C = np.clip(X_C @ beta_hat_C, -700, 700)\n    lambda_hat_C = np.exp(eta_hat_C)\n    W_C = np.diag(lambda_hat_C)\n    H_C = X_C.T @ W_C @ X_C\n    eigenvalues_C = np.linalg.eigvalsh(H_C)\n    is_psd_C = np.all(eigenvalues_C >= -1e-8)\n    results.append(is_psd_C)\n    \n    # Case D: LNP cascade\n    T_D, L_D, seed_D = 150, 5, 7\n    rng_D = np.random.default_rng(seed_D)\n    stimulus_D = rng_D.uniform(-1, 1, size=T_D)\n    k_true_D = np.array([0.5, -0.2, 0.15, 0.0, -0.1])\n    b_true_D = -0.05\n    \n    num_samples_D = T_D - L_D + 1\n    X_D = np.zeros((num_samples_D, L_D + 1))\n    y_D = np.zeros(num_samples_D)\n    \n    for i in range(num_samples_D):\n        t = i + L_D - 1 # Current time index in stimulus array\n        lag_vector = stimulus_D[t-L_D+1 : t+1][::-1]\n        X_D[i, :L_D] = lag_vector\n        X_D[i, L_D] = 1.0\n        \n        rate = np.exp(lag_vector @ k_true_D + b_true_D)\n        y_D[i] = rng_D.poisson(rate)\n        \n    beta_hat_D = fit_poisson_glm(X_D, y_D)\n    k_hat_D = beta_hat_D[:L_D].tolist()\n    results.append(k_hat_D)\n    \n    # Final print statement in the exact required format.\n    # Convert boolean to lowercase string 'true'/'false'\n    results[2] = str(results[2]).lower()\n    print(f\"[{results[0]},{results[1]},{results[2]},{results[3]}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "拟合模型只是工作的一半，关键的下一步是评估其性能。一个好的模型不仅应能拟合数据，还应能捕捉数据中潜在的统计特性。此练习将向您介绍一套用于评估神经编码模型的关键指标，包括基于似然的度量、残差分析以及检验泊松模型核心假设的时间重整化方法。",
            "id": "4158979",
            "problem": "给定分箱的脉冲计数数据以及来自一个编码模型的相应预测条件均值，该模型可以被看作是带有对数连接函数的广义线性模型 (GLM) 或线性-非线性-泊松 (LNP) 级联模型。在任一情况下，模型都指定每个时间箱的脉冲计数（用 $y_t$ 表示）是条件独立的泊松随机变量，其每个时间箱的均值为 $\\mu_t = \\mathbb{E}[y_t \\mid x_t]$，其中 $t$ 是时间箱的索引。您可以假设时间箱的宽度足够小，以至于对于所有时间箱，$y_t \\in \\{0,1\\}$。拟合优度必须根据泊松模型的基本原理和时间重标度定理进行评估。\n\n使用的基本依据和定义：\n- 均值为 $\\mu_t$ 的泊松分布具有概率质量函数 $p(y_t \\mid \\mu_t) = \\frac{\\mu_t^{y_t} e^{-\\mu_t}}{y_t!}$，每个时间箱的对数似然为 $\\ell_t = y_t \\log \\mu_t - \\mu_t - \\log(y_t!)$。\n- 在 $T$ 个时间箱上的总对数似然为 $\\mathrm{LL} = \\sum_{t=1}^{T} \\ell_t$。\n- 零模型是一个具有恒定均值 $\\bar{y} = \\frac{1}{T} \\sum_{t=1}^{T} y_t$ 的泊松模型，因此对于所有 $t$，$\\mu_t^{\\mathrm{null}} = \\bar{y}$。\n- McFadden 伪决定系数定义为 $R^2_{\\mathrm{McF}} = 1 - \\frac{\\mathrm{LL}_{\\mathrm{model}}}{\\mathrm{LL}_{\\mathrm{null}}}$。\n- 模型相对于饱和模型的泊松偏差为 $D = 2 \\sum_{t=1}^{T} \\left[ y_t \\log\\left(\\frac{y_t}{\\mu_t}\\right) - (y_t - \\mu_t) \\right]$，使用约定 $0 \\cdot \\log(0/\\mu_t) = 0$。\n- 在时间箱 $t$ 中的 Pearson 残差是 $r_t = \\frac{y_t - \\mu_t}{\\sqrt{\\mu_t}}$，离散度估计是 $\\hat{\\phi} = \\frac{1}{T} \\sum_{t=1}^{T} r_t^2$。\n- 对于离散时间且每个时间箱最多一个脉冲的时间重标度，定义脉冲所在时间箱的索引为 $1 \\le t_1  t_2  \\dots  t_n \\le T$，其中 $y_{t_k} = 1$。定义脉冲之间的积分强度为 $w_k = \\sum_{t=t_{k-1}+1}^{t_k} \\mu_t$，其中 $t_0 = 0$。然后定义 $u_k = 1 - e^{-w_k}$。在一个正确的模型下，$u_k$ 是独立同分布的 Uniform$(0,1)$ 随机变量。Kolmogorov–Smirnov 统计量是 $D_{\\mathrm{KS}} = \\max_{1 \\le i \\le n} \\max\\left( \\frac{i}{n} - u_{(i)},\\, u_{(i)} - \\frac{i-1}{n} \\right)$，其中 $u_{(i)}$ 表示第 $i$ 个顺序统计量。\n\n您的任务是实现一个程序，为每个提供的测试用例计算以下四个量：\n- $R^2_{\\mathrm{McF}}$,\n- $D$,\n- $D_{\\mathrm{KS}}$（基于所定义的时间重标度定理），\n- $\\hat{\\phi}$.\n\n将所有浮点数结果四舍五入到恰好 $6$ 位小数。\n\n测试套件：\n- 用例 1 ($T = 20$): $\\mu$ 和 $y$ 分别是\n  - $\\mu = [0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.30, 0.20, 0.10, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45]$,\n  - $y = [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1]$.\n- 用例 2 ($T = 20$): $\\mu$ 是恒定的，等于用例 1 中 $y$ 的经验均值，即 $\\mu = [0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]$, 且 $y$ 与用例 1 相同。\n- 用例 3 ($T = 20$): $\\mu$ 是用例 1 中 $\\mu$ 的反转，即 $\\mu = [0.45, 0.40, 0.35, 0.30, 0.25, 0.20, 0.15, 0.10, 0.05, 0.10, 0.20, 0.30, 0.40, 0.35, 0.30, 0.25, 0.20, 0.15, 0.10, 0.05]$, 且 $y$ 与用例 1 相同。\n\n实现要求：\n- 使用上述从泊松模型和时间重标度定理推导出的定义，完全按照说明计算这些指标。\n- 所有对数均为自然对数。\n- 对于形如 $y_t \\log(y_t/\\mu_t)$ 的项，按照约定应用 $0 \\cdot \\log(0/\\mu_t) = 0$ 以避免未定义值。\n- 对于 Kolmogorov–Smirnov 统计量，请使用基于已排序 $u_k$ 值的经验分布函数的定义，不进行任何近似或随机化处理。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含按测试用例顺序连接的所有结果，每个测试用例贡献四个值 $[R^2_{\\mathrm{McF}}, D, D_{\\mathrm{KS}}, \\hat{\\phi}]$，并展平成一个列表。该行必须是一个用方括号括起来的逗号分隔列表。例如，输出格式必须与 $[r_1, d_1, k_1, \\phi_1, r_2, d_2, k_2, \\phi_2, r_3, d_3, k_3, \\phi_3]$ 完全一样，每个值都四舍五入到恰好 $6$ 位小数。",
            "solution": "用户提供了一个神经科学数据分析领域定义明确的计算问题，特别是关于点过程模型 (GLM/LNP) 的评估。该问题具有科学依据、内部一致，并包含获得唯一解所需的所有必要信息。因此，该问题被认为是有效的。解决方案通过为三个测试用例中的每一个实现指定的拟合优度指标来展开。\n\n问题的核心是基于观测到的脉冲计数 $y_t$ 和模型预测的条件平均脉冲率 $\\mu_t$ 来计算四个不同的模型性能指标。数据由一系列 $T$ 个时间箱组成，其中 $y_t \\in \\{0, 1\\}$ 表示脉冲的有无。\n\n每个指标的计算过程如下：\n\n**1. McFadden 伪决定系数 ($R^2_{\\mathrm{McF}}$)**\n\n该指标将所提出模型的对数似然 $\\mathrm{LL}_{\\mathrm{model}}$ 与一个更简单的零模型 $\\mathrm{LL}_{\\mathrm{null}}$ 的对数似然进行比较。零模型假设一个恒定的发放率，该发放率等于经验平均率 $\\bar{y} = \\frac{1}{T} \\sum_{t=1}^{T} y_t$。公式为 $R^2_{\\mathrm{McF}} = 1 - \\frac{\\mathrm{LL}_{\\mathrm{model}}}{\\mathrm{LL}_{\\mathrm{null}}}$。\n\n在 $T$ 个时间箱上的泊松模型的对数似然由 $\\mathrm{LL} = \\sum_{t=1}^{T} \\ell_t = \\sum_{t=1}^{T} [y_t \\log \\mu_t - \\mu_t - \\log(y_t!)]$ 给出。由于问题说明 $y_t \\in \\{0, 1\\}$，项 $\\log(y_t!)$ 始终为 $\\log(1) = 0$。因此，简化的对数似然为 $\\mathrm{LL} = \\sum_{t=1}^{T} (y_t \\log \\mu_t - \\mu_t)$。\n\n-   $\\mathrm{LL}_{\\mathrm{model}}$ 使用提供的模型预测的平均率 $\\mu_t$ 计算。\n-   $\\mathrm{LL}_{\\mathrm{null}}$ 使用一个恒定率 $\\mu_t^{\\mathrm{null}} = \\bar{y}$ （对所有 $t$）计算。计算变为 $\\mathrm{LL}_{\\mathrm{null}} = \\sum_{t=1}^{T} (y_t \\log \\bar{y} - \\bar{y})$。\n\n**2. 泊松偏差 ($D$)**\n\n偏差衡量的是模型相对于一个能完美拟合数据（即 $\\mu_t^{\\mathrm{sat}} = y_t$）的“饱和”模型的拟合优度。公式为 $D = 2 \\sum_{t=1}^{T} \\left[ y_t \\log\\left(\\frac{y_t}{\\mu_t}\\right) - (y_t - \\mu_t) \\right]$。\n\n项 $y_t \\log(y_t/\\mu_t)$ 由于可能出现 $\\log(0)$ 而需要小心处理。\n-   如果 $y_t = 1$，该项变为 $1 \\cdot \\log(1/\\mu_t) = -\\log(\\mu_t)$。\n-   如果 $y_t = 0$，问题指定了约定 $0 \\cdot \\log(0/\\mu_t) = 0$。\n因此，表达式这一部分的求和只需在发生脉冲的时间箱（$y_t=1$）上进行。总和计算为 $D = 2 \\left( \\sum_{t: y_t=1} (-\\log \\mu_t) - \\sum_{t=1}^{T}(y_t - \\mu_t) \\right)$。\n\n**3. Kolmogorov-Smirnov 统计量 ($D_{\\mathrm{KS}}$)**\n\n该指标基于时间重标度定理。对于一个正确的模型，转换后的脉冲时间应呈均匀分布。步骤如下：\n-   首先，识别出发生脉冲的以 $0$ 为起始索引的时间箱，设为 $s_0, s_1, \\dots, s_{n-1}$。这些对应于问题描述中以 $1$ 为起始索引的时间 $t_1, t_2, \\dots, t_n$。\n-   接着，计算连续脉冲之间的积分强度 $w_k = \\sum_{t=t_{k-1}+1}^{t_k} \\mu_t$，其中 $t_0=0$。这可以通过首先计算 $\\mu_t$ 的累积和来高效实现，我们称之为 $C_{\\mu}[i] = \\sum_{j=0}^{i} \\mu_j$。然后积分强度为 $w_1 = C_{\\mu}[s_0]$ 以及对于 $k  1$ 的 $w_k = C_{\\mu}[s_{k-1}] - C_{\\mu}[s_{k-2}]$。\n-   将这些积分强度转换为新的随机变量 $u_k = 1 - e^{-w_k}$。如果模型是正确的，集合 $\\{u_k\\}$ 将是来自 Uniform$(0,1)$ 分布的一个样本。\n-   最后，计算 Kolmogorov-Smirnov 统计量 $D_{\\mathrm{KS}}$ 来量化 $\\{u_k\\}$ 的经验分布与均匀累积分布函数 (CDF) 之间的偏差。设 $u_{(i)}$ 为 $u_k$ 的排序值。该统计量为 $D_{\\mathrm{KS}} = \\max_{1 \\le i \\le n} \\max\\left( \\frac{i}{n} - u_{(i)},\\, u_{(i)} - \\frac{i-1}{n} \\right)$，其中 $n$ 是脉冲总数。\n\n**4. 离散度估计 ($\\hat{\\phi}$)**\n\n离散度提供了一个衡量数据方差是否与泊松模型一致的度量，泊松模型假设方差等于均值。它由 Pearson 残差 $r_t = \\frac{y_t - \\mu_t}{\\sqrt{\\mu_t}}$ 计算得出。离散度估计是 Pearson 残差平方的均值：$\\hat{\\phi} = \\frac{1}{T} \\sum_{t=1}^{T} r_t^2 = \\frac{1}{T} \\sum_{t=1}^{T} \\frac{(y_t - \\mu_t)^2}{\\mu_t}$。对于一个真实的泊松过程，我们期望 $\\hat{\\phi} \\approx 1$。值 $\\hat{\\phi}  1$ 表明过度离散（方差大于均值），而 $\\hat{\\phi}  1$ 表明低度离散。\n\n将这四种计算应用于问题中指定的三个测试用例中的每一个。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes goodness-of-fit metrics for Poisson encoding models based on\n    provided spike data and predicted means.\n    \"\"\"\n    # Define the data for the three test cases as specified in the problem.\n    mu_1_list = [0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.30, 0.20, 0.10, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45]\n    y_1_list = [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1]\n    \n    y_1_arr = np.array(y_1_list, dtype=float)\n    \n    # Case 1\n    mu_1_arr = np.array(mu_1_list, dtype=float)\n    \n    # Case 2: mu is constant at the empirical mean of y from Case 1\n    y_bar_1 = np.mean(y_1_arr)\n    mu_2_arr = np.full_like(y_1_arr, y_bar_1, dtype=float)\n    \n    # Case 3: mu is the reversal of mu from Case 1\n    mu_3_arr = mu_1_arr[::-1]\n\n    test_cases = [\n        (y_1_arr, mu_1_arr),\n        (y_1_arr, mu_2_arr),\n        (y_1_arr, mu_3_arr)\n    ]\n\n    all_results = []\n    \n    for y, mu in test_cases:\n        T = float(len(y))\n\n        # 1. McFadden's a Pseudo R^2\n        # Log-likelihood for Poisson model with y in {0,1} is sum(y*log(mu) - mu).\n        # np.log(mu) is safe as all provided mu > 0.\n        ll_model = np.sum(y * np.log(mu) - mu)\n        \n        y_bar = np.mean(y)\n        # Check for y_bar=0 is good practice, but not needed for this problem's data.\n        ll_null = np.sum(y * np.log(y_bar) - y_bar) if y_bar > 0 else 0.0\n        \n        # If ll_null is 0, R^2 is undefined or handled by convention.\n        # This occurs if y_bar=0, meaning no spikes. Here y_bar = 0.25.\n        r2_mcf = 1.0 - ll_model / ll_null\n        all_results.append(r2_mcf)\n\n        # 2. Poisson Deviance\n        # D = 2 * sum(y*log(y/mu) - (y-mu))\n        # The term y*log(y/mu) is 0 if y=0, and -log(mu) if y=1.\n        spike_indices = (y == 1)\n        \n        log_term_sum = np.sum(-np.log(mu[spike_indices]))\n        diff_term_sum = -np.sum(y - mu)\n\n        deviance = 2.0 * (log_term_sum + diff_term_sum)\n        all_results.append(deviance)\n\n        # 3. Kolmogorov-Smirnov Statistic from Time-Rescaling\n        t_spike_indices = np.where(y == 1)[0]\n        n_spikes = len(t_spike_indices)\n        \n        if n_spikes == 0:\n            ks_stat = 0.0\n        else:\n            # Calculate integrated intensities w_k between spikes\n            cum_mu = np.cumsum(mu)\n            cum_mu_at_spikes = cum_mu[t_spike_indices]\n            w_k = np.diff(cum_mu_at_spikes, prepend=0)\n            \n            # Rescale to u_k = 1 - exp(-w_k)\n            u_k = 1.0 - np.exp(-w_k)\n            u_sorted = np.sort(u_k)\n            \n            # Calculate KS statistic D_KS\n            i_vec = np.arange(1, n_spikes + 1)\n            term1 = i_vec / n_spikes - u_sorted\n            term2 = u_sorted - (i_vec - 1) / n_spikes\n            ks_stat = np.max(np.maximum(term1, term2))\n        all_results.append(ks_stat)\n\n        # 4. Dispersion Estimate\n        # phi_hat = (1/T) * sum( (y - mu)^2 / mu )\n        pearson_residuals_sq = ((y - mu)**2) / mu\n        phi_hat = np.mean(pearson_residuals_sq)\n        all_results.append(phi_hat)\n\n    # Format the final output string exactly as required, with 6 decimal places.\n    formatted_results = [f'{x:.6f}' for x in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}