{
    "hands_on_practices": [
        {
            "introduction": "The first step in any neural decoding effort is to rigorously quantify performance. While overall accuracy provides a simple summary, it can be misleading, especially with imbalanced stimulus classes. This practice guides you through the construction of a confusion matrix, the cornerstone of classifier evaluation, from which more nuanced metrics like precision, recall, and the F1 score are derived. Mastering these calculations  is essential for accurately diagnosing a decoder's strengths and weaknesses across different conditions.",
            "id": "4154094",
            "problem": "A neuroscience laboratory trains a multi-class probabilistic decoder to infer the identity of sensory stimuli from the joint spike counts of a population of neurons. The experimental design contains $4$ distinct stimulus classes, denoted $\\{A,B,C,D\\}$, and a total of $200$ trials. The decoder is evaluated on held-out data. For each trial, the stimulus is presented and the decoder outputs a single predicted class. The evaluation records, for each true class, the count of decoder outputs across all classes:\n- For trials where the true class is $A$: predicted $A$ in $35$ trials, $B$ in $5$ trials, $C$ in $8$ trials, and $D$ in $2$ trials.\n- For trials where the true class is $B$: predicted $A$ in $4$ trials, $B$ in $26$ trials, $C$ in $6$ trials, and $D$ in $4$ trials.\n- For trials where the true class is $C$: predicted $A$ in $5$ trials, $B$ in $12$ trials, $C$ in $38$ trials, and $D$ in $5$ trials.\n- For trials where the true class is $D$: predicted $A$ in $3$ trials, $B$ in $7$ trials, $C$ in $9$ trials, and $D$ in $31$ trials.\n\nStarting from the core definitions of confusion matrices and count-based evaluation metrics used in multi-class decoding in neuroscience data analysis, perform the following:\n1. Construct the confusion matrix $M$ with entries $M_{ij}$ equal to the number of trials with true class $i \\in \\{A,B,C,D\\}$ and predicted class $j \\in \\{A,B,C,D\\}$, ordered alphabetically for both rows and columns.\n2. From the confusion matrix, compute the overall accuracy, the per-class precision, per-class recall, and per-class F1 score (harmonic mean of precision and recall) for each class $\\{A,B,C,D\\}$, using the standard one-versus-rest interpretation of true positives, false positives, and false negatives for each class.\n3. Compute the macro-averaged F1 score by taking the arithmetic mean of the four per-class F1 scores.\n\nReport all rates as decimals and round to four significant figures. Provide the macro-averaged F1 score as your final numeric answer. No physical units are applicable to these rates; express the final answer as a decimal number rounded to four significant figures.",
            "solution": "The task involves decoding evaluation in a multi-class setting. The fundamental base for the derivation is the definition of the confusion matrix and the standard count-based metrics. For a set of classes $\\{A,B,C,D\\}$, the confusion matrix $M$ has entries $M_{ij}$ that count the number of instances with true class $i$ predicted as class $j$. For any given class $k$, the one-versus-rest definitions are:\n- True Positive (TP): $TP_{k}$ equals the diagonal element $M_{kk}$.\n- False Positive (FP): $FP_{k}$ equals the sum of predictions as class $k$ from other true classes, i.e., the column sum for class $k$ minus $TP_{k}$.\n- False Negative (FN): $FN_{k}$ equals the sum of instances of true class $k$ predicted as other classes, i.e., the row sum for class $k$ minus $TP_{k}$.\nOverall accuracy is the fraction of correctly classified instances: sum of the diagonal entries divided by the total number of instances. Precision for class $k$ is $TP_{k}/(TP_{k}+FP_{k})$, recall is $TP_{k}/(TP_{k}+FN_{k})$, and the F1 score (harmonic mean of precision and recall) is defined by $F1_{k} = \\frac{2 \\times \\text{precision} \\times \\text{recall}}{\\text{precision} + \\text{recall}}$. From these definitions, one can derive that\n$$\nF1_{k} \\;=\\; \\frac{2 \\, TP_{k}}{2\\,TP_{k} + FP_{k} + FN_{k}},\n$$\nby substituting the expressions for precision and recall and simplifying.\n\nStep 1: Construct the confusion matrix $M$.\n\nOrdering rows and columns alphabetically $(A,B,C,D)$, the given counts yield\n$$\nM \\;=\\; \\begin{pmatrix}\n35 & 5 & 8 & 2 \\\\\n4 & 26 & 6 & 4 \\\\\n5 & 12 & 38 & 5 \\\\\n3 & 7 & 9 & 31\n\\end{pmatrix}.\n$$\nRow sums (true-class totals): $A: 35+5+8+2 = 50$, $B: 4+26+6+4 = 40$, $C: 5+12+38+5 = 60$, $D: 3+7+9+31 = 50$. The total number of instances is $N = 50+40+60+50 = 200$. Column sums (predicted-class totals): $A: 35+4+5+3 = 47$, $B: 5+26+12+7 = 50$, $C: 8+6+38+9 = 61$, $D: 2+4+5+31 = 42$.\n\nStep 2: Compute accuracy, per-class precision, recall, and F1.\n\n- Overall accuracy:\nThe sum of diagonal entries is $35 + 26 + 38 + 31 = 130$. Therefore,\n$$\n\\text{accuracy} \\;=\\; \\frac{130}{200} \\;=\\; 0.6500 \\text{ (rounded to four significant figures)}.\n$$\n\nNow compute per-class metrics.\n\nClass $A$:\n- $TP_{A} = 35$.\n- $FP_{A} = (\\text{predicted } A \\text{ total}) - TP_{A} = 47 - 35 = 12$.\n- $FN_{A} = (\\text{true } A \\text{ total}) - TP_{A} = 50 - 35 = 15$.\n- Precision: $\\frac{35}{47} \\approx 0.74468085 \\rightarrow 0.7447$ (four significant figures).\n- Recall: $\\frac{35}{50} = 0.7000$ (four significant figures).\n- F1: using $F1_{A} = \\frac{2 \\times TP_{A}}{2 TP_{A} + FP_{A} + FN_{A}} = \\frac{70}{97} \\approx 0.72164948 \\rightarrow 0.7216$.\n\nClass $B$:\n- $TP_{B} = 26$.\n- $FP_{B} = 50 - 26 = 24$.\n- $FN_{B} = 40 - 26 = 14$.\n- Precision: $\\frac{26}{50} = 0.5200$.\n- Recall: $\\frac{26}{40} = 0.6500$.\n- F1: $F1_{B} = \\frac{52}{52+24+14} = \\frac{52}{90} = \\frac{26}{45} \\approx 0.57777778 \\rightarrow 0.5778$.\n\nClass $C$:\n- $TP_{C} = 38$.\n- $FP_{C} = 61 - 38 = 23$.\n- $FN_{C} = 60 - 38 = 22$.\n- Precision: $\\frac{38}{61} \\approx 0.62295082 \\rightarrow 0.6230$.\n- Recall: $\\frac{38}{60} = \\frac{19}{30} \\approx 0.63333333 \\rightarrow 0.6333$.\n- F1: $F1_{C} = \\frac{76}{76+23+22} = \\frac{76}{121} \\approx 0.62809917 \\rightarrow 0.6281$.\n\nClass $D$:\n- $TP_{D} = 31$.\n- $FP_{D} = 42 - 31 = 11$.\n- $FN_{D} = 50 - 31 = 19$.\n- Precision: $\\frac{31}{42} \\approx 0.73809524 \\rightarrow 0.7381$.\n- Recall: $\\frac{31}{50} = 0.6200$.\n- F1: $F1_{D} = \\frac{62}{62+11+19} = \\frac{62}{92} = \\frac{31}{46} \\approx 0.67391304 \\rightarrow 0.6739$.\n\nStep 3: Macro-averaged F1.\n\nThe macro-averaged F1 is the arithmetic mean of the four per-class F1 scores:\n$$\n\\text{macro-F1} \\;=\\; \\frac{1}{4}\\left(F1_{A} + F1_{B} + F1_{C} + F1_{D}\\right)\n\\;=\\; \\frac{1}{4}\\left(\\frac{70}{97} + \\frac{26}{45} + \\frac{76}{121} + \\frac{31}{46}\\right).\n$$\nCompute numerically:\n- $\\frac{70}{97} \\approx 0.72164948$,\n- $\\frac{26}{45} \\approx 0.57777778$,\n- $\\frac{76}{121} \\approx 0.62809917$,\n- $\\frac{31}{46} \\approx 0.67391304$.\n\nSum $\\approx 0.72164948 + 0.57777778 + 0.62809917 + 0.67391304 = 2.60143947$. Divide by $4$:\n$$\n\\text{macro-F1} \\;\\approx\\; 0.65035987 \\;\\rightarrow\\; 0.6504 \\text{ (rounded to four significant figures)}.\n$$\n\nThus, the requested final numeric answer is the macro-averaged F1 score rounded to four significant figures.",
            "answer": "$$\\boxed{0.6504}$$"
        },
        {
            "introduction": "Neuroscience datasets are notoriously high-dimensional, often making dimensionality reduction a necessary preprocessing step before decoding. However, this compression is not without cost, as it can discard information relevant to the stimulus. This exercise  challenges you to move beyond heuristic justifications and quantify this trade-off rigorously by deriving and implementing an algorithm for the information loss, in nats, when projecting neural data onto its principal components. This provides a principled way to understand the consequences of data preprocessing on the information available for decoding.",
            "id": "4154077",
            "problem": "Consider a population of $n$ neurons encoding a $d$-dimensional stimulus through a linear-Gaussian model. Let the stimulus vector be $\\mathbf{s} \\in \\mathbb{R}^d$ with a multivariate normal prior $\\mathbf{s} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{\\Sigma}_s)$, where $\\mathbf{\\Sigma}_s \\in \\mathbb{R}^{d \\times d}$ is positive definite. Let the neural response vector be $\\mathbf{r} \\in \\mathbb{R}^n$ defined by $\\mathbf{r} = \\mathbf{A} \\mathbf{s} + \\mathbf{n}$, where $\\mathbf{A} \\in \\mathbb{R}^{n \\times d}$ is a fixed tuning matrix and $\\mathbf{n} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{\\Sigma}_n)$ is zero-mean Gaussian noise with positive definite covariance $\\mathbf{\\Sigma}_n \\in \\mathbb{R}^{n \\times n}$. The total response covariance is $\\mathbf{\\Sigma}_r = \\mathbf{A} \\mathbf{\\Sigma}_s \\mathbf{A}^\\top + \\mathbf{\\Sigma}_n$.\n\nPrincipal Component Analysis (PCA) is applied to $\\mathbf{\\Sigma}_r$ to obtain its eigendecomposition $\\mathbf{\\Sigma}_r = \\mathbf{U} \\mathbf{\\Lambda} \\mathbf{U}^\\top$, where the columns of $\\mathbf{U} \\in \\mathbb{R}^{n \\times n}$ are orthonormal eigenvectors and $\\mathbf{\\Lambda} \\in \\mathbb{R}^{n \\times n}$ is diagonal with eigenvalues sorted in descending order. The projection onto the top $k$ principal components uses $\\mathbf{W}_k = \\mathbf{U}_{[:,1:k]} \\in \\mathbb{R}^{n \\times k}$, and the compressed observation is $\\mathbf{y}_k = \\mathbf{W}_k^\\top \\mathbf{r} \\in \\mathbb{R}^k$.\n\nDefine the mutual information between stimulus and response, $I(\\mathbf{s}; \\mathbf{r})$, and between stimulus and compressed response, $I(\\mathbf{s}; \\mathbf{y}_k)$, using the foundational definitions of mutual information $I(\\mathbf{X}; \\mathbf{Y}) = H(\\mathbf{Y}) - H(\\mathbf{Y} \\mid \\mathbf{X})$ and the well-tested entropy formula for multivariate normal distributions. Derive, from first principles, an algorithm to compute the information loss due to dimensionality reduction,\n$$\nL_k = I(\\mathbf{s}; \\mathbf{r}) - I(\\mathbf{s}; \\mathbf{y}_k),\n$$\nin natural logarithm units (nats), as a function of $\\mathbf{A}$, $\\mathbf{\\Sigma}_s$, $\\mathbf{\\Sigma}_n$, and $k$. Your derivation must start from the stated core definitions and facts without assuming any specialized shortcut formulas, and must justify each transformation with the underlying principles of Gaussian random variables and linear transformations.\n\nYou must implement this algorithm in a complete, runnable program that computes $L_k$ for the specific test suite below. All logarithms must be natural logarithms, and the final answers must be expressed in nats as real-valued floats.\n\nTest Suite:\n$1.$ Scenario $1$ ($n = 5$, $d = 3$). Use\n$$\n\\mathbf{A}_1 =\n\\begin{bmatrix}\n1.0 & 0.2 & -0.3 \\\\\n0.5 & 1.1 & 0.0 \\\\\n0.0 & -0.7 & 0.8 \\\\\n0.3 & 0.0 & 0.5 \\\\\n-0.6 & 0.4 & 0.2\n\\end{bmatrix},\\quad\n\\mathbf{\\Sigma}_{s,1} =\n\\begin{bmatrix}\n0.9 & 0.0 & 0.0 \\\\\n0.0 & 0.5 & 0.0 \\\\\n0.0 & 0.0 & 0.2\n\\end{bmatrix},\\quad\n\\mathbf{\\Sigma}_{n,1} =\n\\begin{bmatrix}\n0.35 & 0.02 & 0.00 & 0.01 & 0.00 \\\\\n0.02 & 0.40 & 0.03 & 0.00 & 0.01 \\\\\n0.00 & 0.03 & 0.45 & 0.02 & 0.00 \\\\\n0.01 & 0.00 & 0.02 & 0.38 & 0.02 \\\\\n0.00 & 0.01 & 0.00 & 0.02 & 0.50\n\\end{bmatrix}.\n$$\nCompute $L_k$ for $k \\in \\{0, 1, 3, 5\\}$.\n\n$2.$ Scenario $2$ ($n = 4$, $d = 2$). Use\n$$\n\\mathbf{A}_2 =\n\\begin{bmatrix}\n0.9 & -0.1 \\\\\n0.4 & 0.6 \\\\\n-0.2 & 0.8 \\\\\n0.0 & 0.5\n\\end{bmatrix},\\quad\n\\mathbf{\\Sigma}_{s,2} =\n\\begin{bmatrix}\n1.5 & 0.3 \\\\\n0.3 & 0.7\n\\end{bmatrix},\\quad\n\\mathbf{\\Sigma}_{n,2} =\n\\begin{bmatrix}\n0.6 & 0.1 & 0.0 & 0.0 \\\\\n0.1 & 0.6 & 0.1 & 0.0 \\\\\n0.0 & 0.1 & 0.6 & 0.1 \\\\\n0.0 & 0.0 & 0.1 & 0.6\n\\end{bmatrix}.\n$$\nCompute $L_k$ for $k \\in \\{0, 2, 4\\}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[\\text{result}_1,\\text{result}_2,\\dots]$). The results must be the values of $L_k$ for the test cases listed above, in the exact order they are specified: scenario $1$ with $k = 0$, scenario $1$ with $k = 1$, scenario $1$ with $k = 3$, scenario $1$ with $k = 5$, scenario $2$ with $k = 0$, scenario $2$ with $k = 2$, scenario $2$ with $k = 4$.",
            "solution": "The problem as stated is scientifically sound, formally complete, and mathematically well-posed. It is grounded in the foundational principles of information theory and linear-Gaussian models, which are standard in computational neuroscience. All necessary parameters and definitions are provided, and there are no internal contradictions or ambiguities. Therefore, I will proceed with a full derivation and solution.\n\nThe objective is to derive an algorithm for the information loss, $L_k = I(\\mathbf{s}; \\mathbf{r}) - I(\\mathbf{s}; \\mathbf{y}_k)$, incurred by projecting the neural response $\\mathbf{r}$ onto its first $k$ principal components. The derivation will be based on first principles.\n\nThe mutual information between two random vectors, $\\mathbf{X}$ and $\\mathbf{Y}$, is defined as $I(\\mathbf{X}; \\mathbf{Y}) = H(\\mathbf{Y}) - H(\\mathbf{Y} \\mid \\mathbf{X})$, where $H(\\cdot)$ is the differential entropy. For a $D$-dimensional multivariate normal variable $\\mathbf{Z} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\mathbf{\\Sigma})$, the entropy is given by $H(\\mathbf{Z}) = \\frac{1}{2} \\ln \\det((2\\pi e) \\mathbf{\\Sigma}) = \\frac{D}{2} \\ln(2\\pi e) + \\frac{1}{2} \\ln \\det(\\mathbf{\\Sigma})$ nats.\n\n**Step 1: Derivation of $I(\\mathbf{s}; \\mathbf{r})$**\n\nFirst, we determine the distributions of $\\mathbf{r}$ and $\\mathbf{r}$ conditioned on $\\mathbf{s}$.\nThe stimulus $\\mathbf{s}$ and noise $\\mathbf{n}$ are independent, zero-mean Gaussian variables: $\\mathbf{s} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{\\Sigma}_s)$ and $\\mathbf{n} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{\\Sigma}_n)$.\nThe response is $\\mathbf{r} = \\mathbf{A} \\mathbf{s} + \\mathbf{n}$. As a linear transformation of jointly Gaussian variables, $\\mathbf{r}$ is also Gaussian.\nThe mean of $\\mathbf{r}$ is $\\mathbb{E}[\\mathbf{r}] = \\mathbb{E}[\\mathbf{A} \\mathbf{s} + \\mathbf{n}] = \\mathbf{A}\\mathbb{E}[\\mathbf{s}] + \\mathbb{E}[\\mathbf{n}] = \\mathbf{0}$.\nThe covariance of $\\mathbf{r}$ is:\n$$\n\\mathbf{\\Sigma}_r = \\text{Cov}(\\mathbf{r}) = \\mathbb{E}[\\mathbf{r}\\mathbf{r}^\\top] = \\mathbb{E}[(\\mathbf{A} \\mathbf{s} + \\mathbf{n})(\\mathbf{A} \\mathbf{s} + \\mathbf{n})^\\top]\n= \\mathbb{E}[\\mathbf{A}\\mathbf{s}\\mathbf{s}^\\top\\mathbf{A}^\\top + \\mathbf{A}\\mathbf{s}\\mathbf{n}^\\top + \\mathbf{n}\\mathbf{s}^\\top\\mathbf{A}^\\top + \\mathbf{n}\\mathbf{n}^\\top]\n$$\nDue to the independence of $\\mathbf{s}$ and $\\mathbf{n}$, the cross-terms have zero expectation. Thus,\n$$\n\\mathbf{\\Sigma}_r = \\mathbf{A}\\mathbb{E}[\\mathbf{s}\\mathbf{s}^\\top]\\mathbf{A}^\\top + \\mathbb{E}[\\mathbf{n}\\mathbf{n}^\\top] = \\mathbf{A}\\mathbf{\\Sigma}_s\\mathbf{A}^\\top + \\mathbf{\\Sigma}_n\n$$\nSo, the unconditional distribution of the response is $\\mathbf{r} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{\\Sigma}_r)$. The entropy of $\\mathbf{r} \\in \\mathbb{R}^n$ is:\n$$\nH(\\mathbf{r}) = \\frac{n}{2} \\ln(2\\pi e) + \\frac{1}{2} \\ln \\det(\\mathbf{\\Sigma}_r)\n$$\nConditioned on a specific stimulus $\\mathbf{s}$, the response $\\mathbf{r} = \\mathbf{A} \\mathbf{s} + \\mathbf{n}$ is a sum of a constant vector $\\mathbf{A}\\mathbf{s}$ and a Gaussian noise vector $\\mathbf{n}$.\nThe conditional distribution is $(\\mathbf{r} \\mid \\mathbf{s}) \\sim \\mathcal{N}(\\mathbf{A}\\mathbf{s}, \\mathbf{\\Sigma}_n)$. The conditional entropy, which depends only on the covariance, is:\n$$\nH(\\mathbf{r} \\mid \\mathbf{s}) = \\frac{n}{2} \\ln(2\\pi e) + \\frac{1}{2} \\ln \\det(\\mathbf{\\Sigma}_n)\n$$\nThe mutual information is the difference between these entropies:\n$$\nI(\\mathbf{s}; \\mathbf{r}) = H(\\mathbf{r}) - H(\\mathbf{r} \\mid \\mathbf{s}) = \\frac{1}{2} \\ln \\det(\\mathbf{\\Sigma}_r) - \\frac{1}{2} \\ln \\det(\\mathbf{\\Sigma}_n) = \\frac{1}{2} \\ln \\left( \\frac{\\det(\\mathbf{\\Sigma}_r)}{\\det(\\mathbf{\\Sigma}_n)} \\right)\n$$\n\n**Step 2: Derivation of $I(\\mathbf{s}; \\mathbf{y}_k)$**\n\nThe compressed response is $\\mathbf{y}_k = \\mathbf{W}_k^\\top \\mathbf{r}$, a linear projection of $\\mathbf{r}$.\nWe find the distributions of $\\mathbf{y}_k$ and $\\mathbf{y}_k$ conditioned on $\\mathbf{s}$.\nSince $\\mathbf{r} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{\\Sigma}_r)$, the projected vector $\\mathbf{y}_k \\in \\mathbb{R}^k$ is also Gaussian with mean $\\mathbb{E}[\\mathbf{y}_k] = \\mathbf{W}_k^\\top \\mathbb{E}[\\mathbf{r}] = \\mathbf{0}$ and covariance $\\mathbf{\\Sigma}_{y_k} = \\mathbf{W}_k^\\top \\mathbf{\\Sigma}_r \\mathbf{W}_k$.\nThus, $\\mathbf{y}_k \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{W}_k^\\top \\mathbf{\\Sigma}_r \\mathbf{W}_k)$. The entropy is:\n$$\nH(\\mathbf{y}_k) = \\frac{k}{2} \\ln(2\\pi e) + \\frac{1}{2} \\ln \\det(\\mathbf{W}_k^\\top \\mathbf{\\Sigma}_r \\mathbf{W}_k)\n$$\nConditioned on $\\mathbf{s}$, we have $(\\mathbf{r} \\mid \\mathbf{s}) \\sim \\mathcal{N}(\\mathbf{A}\\mathbf{s}, \\mathbf{\\Sigma}_n)$. The projected vector $(\\mathbf{y}_k \\mid \\mathbf{s})$ is Gaussian with mean $\\mathbb{E}[\\mathbf{y}_k \\mid \\mathbf{s}] = \\mathbf{W}_k^\\top \\mathbb{E}[\\mathbf{r} \\mid \\mathbf{s}] = \\mathbf{W}_k^\\top \\mathbf{A}\\mathbf{s}$ and covariance $\\mathbf{\\Sigma}_{y_k \\mid s} = \\mathbf{W}_k^\\top \\mathbf{\\Sigma}_n \\mathbf{W}_k$.\nThe conditional entropy is:\n$$\nH(\\mathbf{y}_k \\mid \\mathbf{s}) = \\frac{k}{2} \\ln(2\\pi e) + \\frac{1}{2} \\ln \\det(\\mathbf{W}_k^\\top \\mathbf{\\Sigma}_n \\mathbf{W}_k)\n$$\nThe mutual information for the compressed response is:\n$$\nI(\\mathbf{s}; \\mathbf{y}_k) = H(\\mathbf{y}_k) - H(\\mathbf{y}_k \\mid \\mathbf{s}) = \\frac{1}{2} \\ln \\left( \\frac{\\det(\\mathbf{W}_k^\\top \\mathbf{\\Sigma}_r \\mathbf{W}_k)}{\\det(\\mathbf{W}_k^\\top \\mathbf{\\Sigma}_n \\mathbf{W}_k)} \\right)\n$$\n\n**Step 3: Derivation of the Information Loss $L_k$**\n\nThe information loss is the difference between the two mutual information quantities:\n$$\nL_k = I(\\mathbf{s}; \\mathbf{r}) - I(\\mathbf{s}; \\mathbf{y}_k) = \\frac{1}{2} \\ln \\left( \\frac{\\det(\\mathbf{\\Sigma}_r)}{\\det(\\mathbf{\\Sigma}_n)} \\right) - \\frac{1}{2} \\ln \\left( \\frac{\\det(\\mathbf{W}_k^\\top \\mathbf{\\Sigma}_r \\mathbf{W}_k)}{\\det(\\mathbf{W}_k^\\top \\mathbf{\\Sigma}_n \\mathbf{W}_k)} \\right)\n$$\n$$\nL_k = \\frac{1}{2} \\left[ \\ln(\\det(\\mathbf{\\Sigma}_r)) - \\ln(\\det(\\mathbf{\\Sigma}_n)) - \\ln(\\det(\\mathbf{W}_k^\\top \\mathbf{\\Sigma}_r \\mathbf{W}_k)) + \\ln(\\det(\\mathbf{W}_k^\\top \\mathbf{\\Sigma}_n \\mathbf{W}_k)) \\right]\n$$\nThis expression can be simplified. The matrix $\\mathbf{W}_k$ consists of the first $k$ orthonormal eigenvectors of $\\mathbf{\\Sigma}_r = \\mathbf{U} \\mathbf{\\Lambda} \\mathbf{U}^\\top$. Let $\\mathbf{U} = [\\mathbf{u}_1, \\dots, \\mathbf{u}_n]$, so $\\mathbf{W}_k = [\\mathbf{u}_1, \\dots, \\mathbf{u}_k]$.\nThe term $\\mathbf{W}_k^\\top \\mathbf{\\Sigma}_r \\mathbf{W}_k$ simplifies to a $k \\times k$ diagonal matrix of the top $k$ eigenvalues of $\\mathbf{\\Sigma}_r$: $\\mathbf{\\Lambda}_k = \\text{diag}(\\lambda_1, \\dots, \\lambda_k)$.\nTherefore, $\\det(\\mathbf{W}_k^\\top \\mathbf{\\Sigma}_r \\mathbf{W}_k) = \\prod_{i=1}^k \\lambda_i$.\nAlso, $\\det(\\mathbf{\\Sigma}_r) = \\det(\\mathbf{\\Lambda}) = \\prod_{i=1}^n \\lambda_i$.\nSubstituting these into the expression for $L_k$:\n$$\nL_k = \\frac{1}{2} \\left[ \\ln\\left(\\prod_{i=1}^n \\lambda_i\\right) - \\ln(\\det(\\mathbf{\\Sigma}_n)) - \\ln\\left(\\prod_{i=1}^k \\lambda_i\\right) + \\ln(\\det(\\mathbf{W}_k^\\top \\mathbf{\\Sigma}_n \\mathbf{W}_k)) \\right]\n$$\nUsing the property $\\ln(ab) = \\ln(a) + \\ln(b)$, this becomes:\n$$\nL_k = \\frac{1}{2} \\left[ \\sum_{i=1}^n \\ln(\\lambda_i) - \\ln(\\det(\\mathbf{\\Sigma}_n)) - \\sum_{i=1}^k \\ln(\\lambda_i) + \\ln(\\det(\\mathbf{W}_k^\\top \\mathbf{\\Sigma}_n \\mathbf{W}_k)) \\right]\n$$\n$$\nL_k = \\frac{1}{2} \\left[ \\sum_{i=k+1}^n \\ln(\\lambda_i) - \\ln(\\det(\\mathbf{\\Sigma}_n)) + \\ln(\\det(\\mathbf{W}_k^\\top \\mathbf{\\Sigma}_n \\mathbf{W}_k)) \\right]\n$$\nThis is the final formula for the information loss.\n\nSpecial cases:\n- For $k=n$, $\\mathbf{W}_n = \\mathbf{U}$. The summation term is empty (value $0$). $\\det(\\mathbf{U}^\\top \\mathbf{\\Sigma}_n \\mathbf{U}) = \\det(\\mathbf{\\Sigma}_n)$. Thus, $L_n = \\frac{1}{2}[0 - \\ln(\\det(\\mathbf{\\Sigma}_n)) + \\ln(\\det(\\mathbf{\\Sigma}_n))] = 0$, as expected.\n- For $k=0$, $\\mathbf{y}_0$ is a zero-dimensional vector, carrying no information, so $I(\\mathbf{s}; \\mathbf{y}_0)=0$. The loss is $L_0 = I(\\mathbf{s}; \\mathbf{r})$. The formula gives $L_0 = \\frac{1}{2} [\\sum_{i=1}^n \\ln(\\lambda_i) - \\ln(\\det(\\mathbf{\\Sigma}_n))]$. Since $\\sum_{i=1}^n \\ln(\\lambda_i) = \\ln(\\det(\\mathbf{\\Sigma}_r))$, this correctly recovers $L_0 = \\frac{1}{2} [\\ln(\\det(\\mathbf{\\Sigma}_r)) - \\ln(\\det(\\mathbf{\\Sigma}_n))]$. In implementation, for $k=0$, the term $\\det(\\mathbf{W}_0^\\top \\mathbf{\\Sigma}_n \\mathbf{W}_0)$ is the determinant of a $0 \\times 0$ matrix, which is $1$, and its logarithm is $0$.\n\nThe algorithm is as follows:\n1. Construct $\\mathbf{\\Sigma}_r = \\mathbf{A} \\mathbf{\\Sigma}_s \\mathbf{A}^\\top + \\mathbf{\\Sigma}_n$.\n2. Perform an eigendecomposition of $\\mathbf{\\Sigma}_r$ to find its eigenvalues $\\lambda_i$ and eigenvectors $\\mathbf{u}_i$, sorted such that $\\lambda_1 \\geq \\lambda_2 \\geq \\dots \\geq \\lambda_n$.\n3. For each given $k$:\n    a. If $k=0$, compute $L_0 = \\frac{1}{2} [\\sum_{i=1}^n \\ln(\\lambda_i) - \\ln(\\det(\\mathbf{\\Sigma}_n))]$.\n    b. If $k>0$, construct $\\mathbf{W}_k = [\\mathbf{u}_1, \\dots, \\mathbf{u}_k]$.\n    c. Compute $L_k$ using the final formula: $L_k = \\frac{1}{2} [ \\sum_{i=k+1}^n \\ln(\\lambda_i) - \\ln(\\det(\\mathbf{\\Sigma}_n)) + \\ln(\\det(\\mathbf{W}_k^\\top \\mathbf{\\Sigma}_n \\mathbf{W}_k)) ]$.\nAll logarithms are natural.",
            "answer": "[1.37877232,0.67262444,0.03960010,0.00000000,1.05045437,0.06170669,0.00000000]"
        },
        {
            "introduction": "Beyond evaluating a specific decoder's performance, a central goal of theoretical neuroscience is to understand the fundamental limits of decoding imposed by the neural code itself. This practice delves into this question by asking you to derive the Cramér–Rao bound (CRB) for an idealized population of neurons from first principles. By calculating this lower bound on estimator variance , you will establish a direct, analytical link between the physical properties of neurons—such as tuning width and firing rate—and the absolute best possible precision any unbiased decoder can achieve.",
            "id": "4154106",
            "problem": "Consider a one-dimensional stimulus $s \\in \\mathbb{R}$ encoded by a large population of conditionally independent neurons. Neuron $i$ has a Gaussian tuning curve $f_{i}(s) = A \\exp\\!\\big(- (s - \\mu_{i})^{2} / (2 \\sigma^{2})\\big)$, where $A > 0$ is the peak mean spike count in the observation window, $\\sigma > 0$ is the tuning width, and $\\mu_{i} \\in \\mathbb{R}$ is the preferred stimulus of neuron $i$. Conditional on $s$, each neuron generates spikes according to a Poisson point process over the observation window, with the spike count $n_{i}$ distributed as a Poisson random variable with mean $f_{i}(s)$. The preferred stimuli $\\mu_{i}$ tile the real line at a uniform density $\\rho > 0$ neurons per unit stimulus (i.e., in the continuum limit $N \\to \\infty$, sums over neurons may be replaced by $\\rho$ times integrals over $\\mu$).\n\nYou are asked to analyze the effect of tuning width on decoding precision using first principles of statistical estimation. Starting from the definitions of the Poisson likelihood and Fisher information, and without invoking any pre-derived shortcut expressions, derive the Cramér–Rao bound (CRB) for the variance of any unbiased estimator of $s$ evaluated at an arbitrary stimulus value $s = s^{\\star}$. Assume scientific realism throughout: independent Poisson spike counts with means given by the tuning curves above, and a densely tiled population as described. Express your final CRB as a single closed-form analytic expression in terms of $A$, $\\sigma$, and $\\rho$. No numerical approximation or rounding is required, and you should not include physical units in your final answer. Your final answer must be a single expression.",
            "solution": "The user wants to find the Cramér–Rao bound (CRB) on the variance of an unbiased estimator of a stimulus $s$. The CRB is given by the inverse of the Fisher information, $I(s)$. We will derive the Fisher information from first principles, starting with the likelihood function for the observed spike counts.\n\nThe problem specifies that the spike count $n_i$ of neuron $i$, conditional on the stimulus $s$, follows a Poisson distribution with mean $f_i(s)$. The probability of observing spike count $n_i$ is:\n$$\nP(n_i | s) = \\frac{(f_i(s))^{n_i} \\exp(-f_i(s))}{n_i!}\n$$\nwhere the tuning curve $f_i(s)$ is given by:\n$$\nf_i(s) = A \\exp\\left(-\\frac{(s - \\mu_i)^2}{2\\sigma^2}\\right)\n$$\nThe neurons are conditionally independent. Therefore, the likelihood function for observing a set of spike counts $\\{n_i\\}$ from the entire population is the product of the individual probabilities:\n$$\nL(\\{n_i\\} | s) = \\prod_i P(n_i | s) = \\prod_i \\frac{(f_i(s))^{n_i} \\exp(-f_i(s))}{n_i!}\n$$\nFor the calculation of Fisher information, it is more convenient to work with the log-likelihood function, $\\ln L$:\n$$\n\\ln L(\\{n_i\\} | s) = \\ln \\left( \\prod_i \\frac{(f_i(s))^{n_i} \\exp(-f_i(s))}{n_i!} \\right)\n$$\nUsing the properties of logarithms, this simplifies to a sum:\n$$\n\\ln L = \\sum_i \\ln \\left( \\frac{(f_i(s))^{n_i} \\exp(-f_i(s))}{n_i!} \\right) = \\sum_i \\left( n_i \\ln(f_i(s)) - f_i(s) - \\ln(n_i!) \\right)\n$$\nThe Fisher information $I(s)$ is defined as the negative of the expected value of the second derivative of the log-likelihood with respect to the parameter $s$:\n$$\nI(s) = -E\\left[\\frac{\\partial^2}{\\partial s^2} \\ln L\\right]\n$$\nAlternatively, for a well-behaved likelihood, it can also be expressed as the expectation of the square of the first derivative:\n$$\nI(s) = E\\left[\\left(\\frac{\\partial}{\\partial s} \\ln L\\right)^2\\right]\n$$\nA general result for conditionally independent observations is that the total Fisher information is the sum of the information from each observation: $I(s) = \\sum_i I_i(s)$. Let's calculate the information for a single neuron, $I_i(s)$, and then sum over the population.\n\nFor a single neuron $i$, the log-likelihood is:\n$$\n\\ln L_i = n_i \\ln(f_i(s)) - f_i(s) - \\ln(n_i!)\n$$\nThe first derivative with respect to $s$ is:\n$$\n\\frac{\\partial}{\\partial s} \\ln L_i = n_i \\frac{f_i'(s)}{f_i(s)} - f_i'(s) = f_i'(s) \\left(\\frac{n_i}{f_i(s)} - 1\\right)\n$$\nwhere $f_i'(s) = \\frac{d f_i(s)}{ds}$.\nThe second derivative is:\n$$\n\\frac{\\partial^2}{\\partial s^2} \\ln L_i = \\frac{\\partial}{\\partial s} \\left[ f_i'(s) \\left(\\frac{n_i}{f_i(s)} - 1\\right) \\right] = f_i''(s) \\left(\\frac{n_i}{f_i(s)} - 1\\right) - f_i'(s) \\left(\\frac{n_i f_i'(s)}{(f_i(s))^2}\\right)\n$$\n$$\n\\frac{\\partial^2}{\\partial s^2} \\ln L_i = \\frac{n_i f_i''(s)}{f_i(s)} - f_i''(s) - \\frac{n_i (f_i'(s))^2}{(f_i(s))^2}\n$$\nNow, we take the expectation with respect to the distribution of $n_i$, which is Poisson with mean $E[n_i] = f_i(s)$.\n$$\nE\\left[\\frac{\\partial^2}{\\partial s^2} \\ln L_i\\right] = \\frac{E[n_i] f_i''(s)}{f_i(s)} - f_i''(s) - \\frac{E[n_i] (f_i'(s))^2}{(f_i(s))^2}\n$$\n$$\nE\\left[\\frac{\\partial^2}{\\partial s^2} \\ln L_i\\right] = \\frac{f_i(s) f_i''(s)}{f_i(s)} - f_i''(s) - \\frac{f_i(s) (f_i'(s))^2}{(f_i(s))^2} = f_i''(s) - f_i''(s) - \\frac{(f_i'(s))^2}{f_i(s)} = -\\frac{(f_i'(s))^2}{f_i(s)}\n$$\nThe Fisher information for a single neuron is $I_i(s) = -E\\left[\\frac{\\partial^2}{\\partial s^2} \\ln L_i\\right] = \\frac{(f_i'(s))^2}{f_i(s)}$.\nThe total Fisher information is the sum over all neurons:\n$$\nI(s) = \\sum_i I_i(s) = \\sum_i \\frac{(f_i'(s))^2}{f_i(s)}\n$$\nNext, we compute the derivative of the tuning curve $f_i(s)$:\n$$\nf_i'(s) = \\frac{d}{ds} \\left[ A \\exp\\left(-\\frac{(s - \\mu_i)^2}{2\\sigma^2}\\right) \\right] = A \\exp\\left(-\\frac{(s - \\mu_i)^2}{2\\sigma^2}\\right) \\cdot \\left(-\\frac{2(s - \\mu_i)}{2\\sigma^2}\\right)\n$$\n$$\nf_i'(s) = - \\frac{s - \\mu_i}{\\sigma^2} f_i(s)\n$$\nSubstituting this into the expression for $I(s)$:\n$$\nI(s) = \\sum_i \\frac{\\left(-\\frac{s - \\mu_i}{\\sigma^2} f_i(s)\\right)^2}{f_i(s)} = \\sum_i \\frac{\\frac{(s - \\mu_i)^2}{\\sigma^4} (f_i(s))^2}{f_i(s)} = \\sum_i \\frac{(s - \\mu_i)^2}{\\sigma^4} f_i(s)\n$$\nNow, substitute the expression for $f_i(s)$:\n$$\nI(s) = \\sum_i \\frac{(s - \\mu_i)^2}{\\sigma^4} A \\exp\\left(-\\frac{(s - \\mu_i)^2}{2\\sigma^2}\\right)\n$$\nThe problem states that in the limit of a large number of neurons, the sum can be replaced by an integral over the preferred stimuli $\\mu$, weighted by the density $\\rho$:\n$$\n\\sum_i g(\\mu_i) \\to \\int_{-\\infty}^{\\infty} \\rho \\, g(\\mu) \\, d\\mu\n$$\nApplying this continuum limit to our expression for $I(s)$:\n$$\nI(s) = \\rho \\int_{-\\infty}^{\\infty} \\frac{(s - \\mu)^2}{\\sigma^4} A \\exp\\left(-\\frac{(s - \\mu)^2}{2\\sigma^2}\\right) d\\mu\n$$\nWe can pull the constants out of the integral:\n$$\nI(s) = \\frac{A\\rho}{\\sigma^4} \\int_{-\\infty}^{\\infty} (s - \\mu)^2 \\exp\\left(-\\frac{(s - \\mu)^2}{2\\sigma^2}\\right) d\\mu\n$$\nTo solve the integral, let's make a substitution $u = s - \\mu$, which means $d\\mu = -du$. The integration limits change from $(-\\infty, \\infty)$ for $\\mu$ to $(\\infty, -\\infty)$ for $u$. We can flip the limits and absorb the minus sign:\n$$\nI(s) = \\frac{A\\rho}{\\sigma^4} \\int_{-\\infty}^{\\infty} u^2 \\exp\\left(-\\frac{u^2}{2\\sigma^2}\\right) du\n$$\nThis is a standard Gaussian-related integral. It corresponds to the unnormalized second moment of a zero-mean Gaussian distribution with variance $\\sigma^2$. We recognize the integral $\\int_{-\\infty}^{\\infty} \\exp(-ax^2)dx = \\sqrt{\\pi/a}$. The value of our integral can be found by differentiation with respect to a parameter. Let $J(a) = \\int_{-\\infty}^{\\infty} \\exp(-au^2) du = \\sqrt{\\pi}a^{-1/2}$. Then\n$$\n\\int_{-\\infty}^{\\infty} u^2 \\exp(-au^2) du = -\\frac{d}{da} J(a) = -\\frac{d}{da}(\\sqrt{\\pi}a^{-1/2}) = -\\sqrt{\\pi}(-\\frac{1}{2}a^{-3/2}) = \\frac{\\sqrt{\\pi}}{2}a^{-3/2}\n$$\nIn our case, $a = \\frac{1}{2\\sigma^2}$. Substituting this value:\n$$\n\\int_{-\\infty}^{\\infty} u^2 \\exp\\left(-\\frac{u^2}{2\\sigma^2}\\right) du = \\frac{\\sqrt{\\pi}}{2} \\left(\\frac{1}{2\\sigma^2}\\right)^{-3/2} = \\frac{\\sqrt{\\pi}}{2} (2\\sigma^2)^{3/2} = \\frac{\\sqrt{\\pi}}{2} 2^{3/2} (\\sigma^2)^{3/2} = \\frac{\\sqrt{\\pi}}{2} (2\\sqrt{2}) \\sigma^3 = \\sigma^3 \\sqrt{2\\pi}\n$$\nNow substitute this result back into the expression for $I(s)$:\n$$\nI(s) = \\frac{A\\rho}{\\sigma^4} (\\sigma^3 \\sqrt{2\\pi}) = \\frac{A\\rho\\sqrt{2\\pi}}{\\sigma}\n$$\nNote that the Fisher information $I(s)$ is independent of the stimulus value $s$. This is due to the translational symmetry of the problem (uniform tiling of preferred stimuli across the entire real line). Therefore, evaluating at an arbitrary stimulus value $s = s^{\\star}$ yields the same result: $I(s^{\\star}) = I(s)$.\n\nThe Cramér–Rao bound (CRB) states that the variance of any unbiased estimator $\\hat{s}$ of the parameter $s$ is bounded below by the inverse of the Fisher information:\n$$\n\\text{Var}(\\hat{s}) \\ge \\frac{1}{I(s)}\n$$\nThe CRB is the value of this lower bound.\n$$\n\\text{CRB} = \\frac{1}{I(s^{\\star})} = \\frac{1}{\\frac{A\\rho\\sqrt{2\\pi}}{\\sigma}} = \\frac{\\sigma}{A\\rho\\sqrt{2\\pi}}\n$$\nThis is the final closed-form expression for the Cramér–Rao bound.",
            "answer": "$$\\boxed{\\frac{\\sigma}{A \\rho \\sqrt{2 \\pi}}}$$"
        }
    ]
}