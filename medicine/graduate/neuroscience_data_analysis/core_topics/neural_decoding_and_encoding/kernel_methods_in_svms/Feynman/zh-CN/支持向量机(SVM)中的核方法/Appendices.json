{
    "hands_on_practices": [
        {
            "introduction": "在应用复杂的核函数之前，理解驱动支持向量机的优化机制至关重要。第一个练习将引导您针对一个简单的、线性可分的神经科学数据集，从原始公式推导出对偶二次规划问题。通过手动求解拉格朗日乘子并验证Karush-Kuhn-Tucker (KKT) 条件，您将对SVM如何找到最大间隔解建立起具体的直觉。",
            "id": "4172634",
            "problem": "一位临床神经科学家正在使用支持向量机（SVM）原型设计一个二元分类器，用于从一个小的功能性磁共振成像（fMRI）感兴趣区域数据集中分离两种任务条件。该科学家在每次试验中提取两个特征，代表两个经典网络中的归一化激活摘要，并按条件为每次试验打上标签。对于三次试验，特征向量和标签由 $x_{1} = (1, 0)$ 且 $y_{1} = +1$，$x_{2} = (0, 1)$ 且 $y_{2} = +1$，以及 $x_{3} = (1, 1)$ 且 $y_{3} = -1$ 给出。该科学家选择了线性核 $K(x, z) = x^{\\top} z$ 和硬间隔公式。\n\n从硬间隔支持向量机原始问题的基本定义和拉格朗日对偶性原理出发，完成以下任务：\n\n1. 针对此数据集和核函数，推导以对偶变量 $\\alpha_{1}$、$\\alpha_{2}$ 和 $\\alpha_{3}$ 表示的显式对偶二次规划，包括等式约束和非负约束。您的推导必须从原始优化定义开始，经过构造拉格朗日函数、对原始变量应用驻点条件，并通过消去原始变量来获得对偶问题。\n\n2. 解析求解对偶问题，以获得满足等式约束的最优 $\\boldsymbol{\\alpha} = (\\alpha_{1}, \\alpha_{2}, \\alpha_{3})$。然后，使用决策函数 $f(x) = \\sum_{j=1}^{3} \\alpha_{j} y_{j} K(x_{j}, x) + b$ 的定义以及适用于硬间隔情况的互补松弛条件来计算偏置项 $b$。\n\n3. 通过对 $i \\in \\{1, 2, 3\\}$ 显式计算 $\\alpha_{i}\\big(y_{i} f(x_{i}) - 1\\big)$，来验证每个样本的卡鲁什-库恩-塔克（KKT）互补条件。\n\n在每一步提供清晰的推理，并避免使用未从所述基础推导出的快捷公式。作为最终输出，请报告偏置项 $b$ 的确切值，作为一个数字。无需四舍五入。",
            "solution": "该问题被评估为有效，因为它在科学上基于机器学习和优化的原理，问题设定良好，数据充分且一致，可得出唯一解，并以客观、正式的语言表述。所提供的数据点是线性可分的，这是存在硬间隔支持向量机解的必要条件。\n\n解答按要求分为三个部分呈现。\n\n### 第1部分：对偶二次规划的推导\n\n硬间隔支持向量机（SVM）旨在找到一个具有最大可能间隔的分离超平面。这被表述为一个凸优化问题。\n\n**原始问题**\n目标是最小化 $\\frac{1}{2} \\|\\boldsymbol{w}\\|^2$，这等价于最大化间隔 $\\frac{2}{\\|\\boldsymbol{w}\\|}$，其约束条件是所有数据点都被正确分类且位于间隔区域之外。对于一个包含 $N$ 个点 $(\\boldsymbol{x}_i, y_i)$ 的数据集，其中 $\\boldsymbol{x}_i \\in \\mathbb{R}^d$ 且 $y_i \\in \\{-1, +1\\}$，原始问题是：\n$$\n\\min_{\\boldsymbol{w}, b} \\frac{1}{2} \\boldsymbol{w}^{\\top} \\boldsymbol{w}\n$$\n约束条件为：\n$$\ny_i(\\boldsymbol{w}^{\\top} \\boldsymbol{x}_i + b) \\ge 1, \\quad \\text{for } i = 1, \\dots, N\n$$\n对于此问题，$N=3$。数据为：$\\boldsymbol{x}_{1} = (1, 0)$ 且 $y_{1} = +1$；$\\boldsymbol{x}_{2} = (0, 1)$ 且 $y_{2} = +1$；以及 $\\boldsymbol{x}_{3} = (1, 1)$ 且 $y_{3} = -1$。\n\n**拉格朗日公式**\n我们为 $N=3$ 个约束中的每一个引入非负的拉格朗日乘子 $\\alpha_i \\ge 0$。拉格朗日函数 $L_P$ 构建如下：\n$$\nL_P(\\boldsymbol{w}, b, \\boldsymbol{\\alpha}) = \\frac{1}{2} \\boldsymbol{w}^{\\top} \\boldsymbol{w} - \\sum_{i=1}^{3} \\alpha_i \\left[ y_i (\\boldsymbol{w}^{\\top} \\boldsymbol{x}_i + b) - 1 \\right]\n$$\n其中 $\\boldsymbol{\\alpha} = (\\alpha_1, \\alpha_2, \\alpha_3)^{\\top}$。\n\n**驻点条件**\n为了找到拉格朗日函数的鞍点，我们通过将其对原始变量 $\\boldsymbol{w}$ 和 $b$ 的偏导数设为零，来最小化 $L_P$。\n1. 关于 $\\boldsymbol{w}$ 的导数：\n$$\n\\frac{\\partial L_P}{\\partial \\boldsymbol{w}} = \\boldsymbol{w} - \\sum_{i=1}^{3} \\alpha_i y_i \\boldsymbol{x}_i = \\boldsymbol{0} \\implies \\boldsymbol{w} = \\sum_{i=1}^{3} \\alpha_i y_i \\boldsymbol{x}_i\n$$\n2. 关于 $b$ 的导数：\n$$\n\\frac{\\partial L_P}{\\partial b} = - \\sum_{i=1}^{3} \\alpha_i y_i = 0 \\implies \\sum_{i=1}^{3} \\alpha_i y_i = 0\n$$\n\n**对偶问题**\n我们将这些驻点条件代回到拉格朗日函数 $L_P$ 中，以获得对偶拉格朗日函数 $L_D$。\n$$\nL_D(\\boldsymbol{\\alpha}) = \\frac{1}{2} \\left(\\sum_{i=1}^{3} \\alpha_i y_i \\boldsymbol{x}_i\\right)^{\\top} \\left(\\sum_{j=1}^{3} \\alpha_j y_j \\boldsymbol{x}_j\\right) - \\sum_{i=1}^{3} \\alpha_i y_i \\left(\\left(\\sum_{j=1}^{3} \\alpha_j y_j \\boldsymbol{x}_j\\right)^{\\top} \\boldsymbol{x}_i + b\\right) + \\sum_{i=1}^{3} \\alpha_i\n$$\n重新整理并分配各项：\n$$\nL_D(\\boldsymbol{\\alpha}) = \\frac{1}{2} \\sum_{i=1}^{3} \\sum_{j=1}^{3} \\alpha_i \\alpha_j y_i y_j \\boldsymbol{x}_i^{\\top} \\boldsymbol{x}_j - \\sum_{i=1}^{3} \\sum_{j=1}^{3} \\alpha_i \\alpha_j y_i y_j \\boldsymbol{x}_j^{\\top} \\boldsymbol{x}_i - b \\sum_{i=1}^{3} \\alpha_i y_i + \\sum_{i=1}^{3} \\alpha_i\n$$\n涉及 $b$ 的项因第二个驻点条件 $\\sum_{i=1}^{3} \\alpha_i y_i = 0$ 而消失。合并前两项，并使用指定的线性核 $K(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = \\boldsymbol{x}_i^{\\top} \\boldsymbol{x}_j$：\n$$\nL_D(\\boldsymbol{\\alpha}) = \\sum_{i=1}^{3} \\alpha_i - \\frac{1}{2} \\sum_{i=1}^{3} \\sum_{j=1}^{3} \\alpha_i \\alpha_j y_i y_j K(\\boldsymbol{x}_i, \\boldsymbol{x}_j)\n$$\n对偶问题是在从驻点条件和原始拉格朗日设定中推导出的约束条件下，最大化 $L_D(\\boldsymbol{\\alpha})$。这等价于最小化 $-L_D(\\boldsymbol{\\alpha})$。因此，对偶二次规划为：\n$$\n\\min_{\\boldsymbol{\\alpha}} \\frac{1}{2} \\sum_{i=1}^{3} \\sum_{j=1}^{3} \\alpha_i \\alpha_j y_i y_j K(\\boldsymbol{x}_i, \\boldsymbol{x}_j) - \\sum_{i=1}^{3} \\alpha_i\n$$\n约束条件为：\n$$\n\\sum_{i=1}^{3} \\alpha_i y_i = 0 \\quad \\text{和} \\quad \\alpha_i \\ge 0 \\text{ for } i=1, 2, 3.\n$$\n\n**数据集的显式公式**\n首先，我们计算格拉姆矩阵 $K_{ij} = K(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = \\boldsymbol{x}_i^{\\top} \\boldsymbol{x}_j$：\n$K_{11} = (1, 0)(1, 0)^{\\top} = 1$；$K_{12} = (1, 0)(0, 1)^{\\top} = 0$；$K_{13} = (1, 0)(1, 1)^{\\top} = 1$。\n$K_{22} = (0, 1)(0, 1)^{\\top} = 1$；$K_{23} = (0, 1)(1, 1)^{\\top} = 1$。\n$K_{33} = (1, 1)(1, 1)^{\\top} = 2$。\n根据对称性，$K_{21}=K_{12}$，$K_{31}=K_{13}$，$K_{32}=K_{23}$。所以格拉姆矩阵是 $K = \\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\\\ 1  1  2 \\end{pmatrix}$。\n\n二次项的矩阵是 $H_{ij} = y_i y_j K_{ij}$。当 $y_1=+1$, $y_2=+1$, $y_3=-1$ 时：\n$H_{11} = y_1^2 K_{11} = 1$；$H_{12} = y_1 y_2 K_{12} = 0$；$H_{13} = y_1 y_3 K_{13} = -1$。\n$H_{22} = y_2^2 K_{22} = 1$；$H_{23} = y_2 y_3 K_{23} = -1$。\n$H_{33} = y_3^2 K_{33} = 2$。\n海森矩阵是 $H = \\begin{pmatrix} 1  0  -1 \\\\ 0  1  -1 \\\\ -1  -1  2 \\end{pmatrix}$。\n\n要最小化的二次目标函数是 $\\frac{1}{2} \\boldsymbol{\\alpha}^{\\top} H \\boldsymbol{\\alpha} - \\mathbf{1}^{\\top} \\boldsymbol{\\alpha}$：\n$$\n\\frac{1}{2} (\\alpha_1^2 + \\alpha_2^2 + 2\\alpha_3^2 - 2\\alpha_1\\alpha_3 - 2\\alpha_2\\alpha_3) - (\\alpha_1 + \\alpha_2 + \\alpha_3)\n$$\n等式约束是 $\\sum_i \\alpha_i y_i = \\alpha_1(1) + \\alpha_2(1) + \\alpha_3(-1) = 0$，即 $\\alpha_1 + \\alpha_2 - \\alpha_3 = 0$。\n\n因此，显式的对偶二次规划是：\n**最小化：**\n$$\n\\frac{1}{2} \\alpha_1^2 + \\frac{1}{2} \\alpha_2^2 + \\alpha_3^2 - \\alpha_1\\alpha_3 - \\alpha_2\\alpha_3 - \\alpha_1 - \\alpha_2 - \\alpha_3\n$$\n**约束条件：**\n$$\n\\alpha_1 + \\alpha_2 - \\alpha_3 = 0 \\quad \\text{和} \\quad \\alpha_1 \\ge 0, \\alpha_2 \\ge 0, \\alpha_3 \\ge 0\n$$\n\n### 第2部分：求解对偶问题并计算偏置项\n\n为了解决这个约束优化问题，我们将等式约束 $\\alpha_3 = \\alpha_1 + \\alpha_2$ 代入目标函数，得到一个关于 $\\alpha_1$ 和 $\\alpha_2$ 的函数：\n$$\nJ(\\alpha_1, \\alpha_2) = \\frac{1}{2} \\alpha_1^2 + \\frac{1}{2} \\alpha_2^2 + (\\alpha_1+\\alpha_2)^2 - \\alpha_1(\\alpha_1+\\alpha_2) - \\alpha_2(\\alpha_1+\\alpha_2) - \\alpha_1 - \\alpha_2 - (\\alpha_1+\\alpha_2)\n$$\n展开并简化表达式：\n$$\nJ(\\alpha_1, \\alpha_2) = (\\frac{1}{2}\\alpha_1^2 + \\frac{1}{2}\\alpha_2^2) + (\\alpha_1^2 + 2\\alpha_1\\alpha_2 + \\alpha_2^2) - (\\alpha_1^2 + \\alpha_1\\alpha_2) - (\\alpha_1\\alpha_2 + \\alpha_2^2) - (2\\alpha_1 + 2\\alpha_2)\n$$\n$$\nJ(\\alpha_1, \\alpha_2) = \\frac{1}{2}\\alpha_1^2 + \\frac{1}{2}\\alpha_2^2 - 2\\alpha_1 - 2\\alpha_2\n$$\n现在我们最小化 $J(\\alpha_1, \\alpha_2)$，约束条件为 $\\alpha_1 \\ge 0$ 和 $\\alpha_2 \\ge 0$。我们通过将偏导数设为零来找到驻点：\n$$\n\\frac{\\partial J}{\\partial \\alpha_1} = \\alpha_1 - 2 = 0 \\implies \\alpha_1 = 2\n$$\n$$\n\\frac{\\partial J}{\\partial \\alpha_2} = \\alpha_2 - 2 = 0 \\implies \\alpha_2 = 2\n$$\n由于 $\\alpha_1=2$ 和 $\\alpha_2=2$ 都满足非负约束，这是最优解。我们从等式约束中求出 $\\alpha_3$：\n$$\n\\alpha_3 = \\alpha_1 + \\alpha_2 = 2 + 2 = 4\n$$\n最优对偶变量为 $\\boldsymbol{\\alpha} = (2, 2, 4)$。\n\n接下来，我们计算偏置项 $b$。卡鲁什-库恩-塔克（KKT）互补松弛条件表明，对于每个 $i$，都有 $\\alpha_i [y_i(\\boldsymbol{w}^{\\top}\\boldsymbol{x}_i + b) - 1] = 0$。任何满足 $\\alpha_i > 0$ 的数据点 $\\boldsymbol{x}_i$ 都是一个支持向量，并且必须位于间隔边界上，满足 $y_i(\\boldsymbol{w}^{\\top}\\boldsymbol{x}_i + b) = 1$。在本例中，所有三个 $\\alpha_i$ 均为正值，因此所有三个点都是支持向量。我们可以使用其中任何一个来求解 $b$。\n首先，我们计算权重向量 $\\boldsymbol{w}$：\n$$\n\\boldsymbol{w} = \\sum_{i=1}^{3} \\alpha_i y_i \\boldsymbol{x}_i = \\alpha_1 y_1 \\boldsymbol{x}_1 + \\alpha_2 y_2 \\boldsymbol{x}_2 + \\alpha_3 y_3 \\boldsymbol{x}_3\n$$\n$$\n\\boldsymbol{w} = (2)(+1)(1, 0)^{\\top} + (2)(+1)(0, 1)^{\\top} + (4)(-1)(1, 1)^{\\top}\n$$\n$$\n\\boldsymbol{w} = (2, 0)^{\\top} + (0, 2)^{\\top} - (4, 4)^{\\top} = (-2, -2)^{\\top}\n$$\n使用支持向量 $\\boldsymbol{x}_1$：\n$$\ny_1(\\boldsymbol{w}^{\\top}\\boldsymbol{x}_1 + b) = 1 \\implies (+1) \\left( (-2, -2)(1, 0)^{\\top} + b \\right) = 1\n$$\n$$\n-2 + b = 1 \\implies b = 3\n$$\n为验证，使用支持向量 $\\boldsymbol{x}_2$：\n$$\ny_2(\\boldsymbol{w}^{\\top}\\boldsymbol{x}_2 + b) = 1 \\implies (+1) \\left( (-2, -2)(0, 1)^{\\top} + b \\right) = 1\n$$\n$$\n-2 + b = 1 \\implies b = 3\n$$\n以及使用支持向量 $\\boldsymbol{x}_3$：\n$$\ny_3(\\boldsymbol{w}^{\\top}\\boldsymbol{x}_3 + b) = 1 \\implies (-1) \\left( (-2, -2)(1, 1)^{\\top} + b \\right) = 1\n$$\n$$\n(-1)(-2 - 2 + b) = 1 \\implies -(-4+b) = 1 \\implies 4-b=1 \\implies b=3\n$$\n所有三个支持向量都得出相同的偏置值 $b=3$。\n\n### 第3部分：KKT互补条件的验证\n\n条件是 $\\alpha_i(y_i f(\\boldsymbol{x}_i) - 1) = 0$，其中决策函数为 $f(\\boldsymbol{x}) = \\boldsymbol{w}^{\\top}\\boldsymbol{x} + b$。我们有 $\\boldsymbol{w}=(-2, -2)^{\\top}$ 和 $b=3$。\n\n对于 $i=1$：$\\boldsymbol{x}_1=(1,0), y_1=+1, \\alpha_1=2$。\n$$\n\\alpha_1 \\left( y_1 (\\boldsymbol{w}^{\\top}\\boldsymbol{x}_1 + b) - 1 \\right) = 2 \\left( (+1) ((-2)(1) + (-2)(0) + 3) - 1 \\right) = 2 \\left( (-2+3) - 1 \\right) = 2(1 - 1) = 0\n$$\n\n对于 $i=2$：$\\boldsymbol{x}_2=(0,1), y_2=+1, \\alpha_2=2$。\n$$\n\\alpha_2 \\left( y_2 (\\boldsymbol{w}^{\\top}\\boldsymbol{x}_2 + b) - 1 \\right) = 2 \\left( (+1) ((-2)(0) + (-2)(1) + 3) - 1 \\right) = 2 \\left( (-2+3) - 1 \\right) = 2(1 - 1) = 0\n$$\n\n对于 $i=3$：$\\boldsymbol{x}_3=(1,1), y_3=-1, \\alpha_3=4$。\n$$\n\\alpha_3 \\left( y_3 (\\boldsymbol{w}^{\\top}\\boldsymbol{x}_3 + b) - 1 \\right) = 4 \\left( (-1) ((-2)(1) + (-2)(1) + 3) - 1 \\right) = 4 \\left( (-1)(-4+3) - 1 \\right) = 4(1 - 1) = 0\n$$\n卡鲁什-库恩-塔克（KKT）互补条件对所有三个样本都成立，证实了我们解的正确性。最后要求的值是偏置项 $b$。",
            "answer": "$$\n\\boxed{3}\n$$"
        },
        {
            "introduction": "核SVM的威力在于“核技巧”，它使我们能够在非常高维的空间中计算决策边界，而无需显式地映射数据。这个练习将这个抽象概念具体化，让您针对应用于神经元放电率数据的多项式核，实践技巧的两面。您将首先构建显式的特征映射，在高维原始空间中进行计算，然后使用计算上更高效的对偶核公式来验证您的结果，从而巩固您对这一核心原理的理解。",
            "id": "4172673",
            "problem": "在一项关于神经元群体编码的研究中，每次试验提供两个发放率特征，形成一个二维输入 $\\mathbf{x} = (x_{1}, x_{2})$，单位为脉冲/秒。该研究提出了一个二元分类问题，旨在区分不同的刺激类别，使用带二次核函数的支持向量机（SVM）。从以下核心定义开始。\n\n- 由特征映射 $\\boldsymbol{\\phi}$ 定义的特征空间中的线性决策函数为 $f(\\mathbf{x}) = \\mathbf{w}^{\\top}\\boldsymbol{\\phi}(\\mathbf{x}) + b$，其中 $\\mathbf{w} = \\sum_{i=1}^{n} \\alpha_{i} y_{i} \\boldsymbol{\\phi}(\\mathbf{x}_{i})$，$\\alpha_{i} \\ge 0$ 是拉格朗日乘子，$y_{i} \\in \\{-1, +1\\}$ 是标签，$b \\in \\mathbb{R}$ 是偏置。\n- 一个半正定核 $k(\\mathbf{x}, \\mathbf{z})$ 对应于一个可能更高维的特征空间中的内积，通过 $k(\\mathbf{x}, \\mathbf{z}) = \\boldsymbol{\\phi}(\\mathbf{x})^{\\top} \\boldsymbol{\\phi}(\\mathbf{z})$ 实现。\n- 决策函数可以用其对偶形式写为 $f(\\mathbf{x}) = \\sum_{i=1}^{n} \\alpha_{i} y_{i} k(\\mathbf{x}_{i}, \\mathbf{x}) + b$。\n\n考虑非齐次二次多项式核 $k(\\mathbf{x}, \\mathbf{z}) = (\\mathbf{x}^{\\top}\\mathbf{z} + c)^{2}$，其中偏移参数 $c = 1$。构造一个显式二次特征映射 $\\boldsymbol{\\phi} : \\mathbb{R}^{2} \\to \\mathbb{R}^{m}$，使得对于所有 $\\mathbf{x}, \\mathbf{z} \\in \\mathbb{R}^{2}$，等式 $k(\\mathbf{x}, \\mathbf{z}) = \\boldsymbol{\\phi}(\\mathbf{x})^{\\top} \\boldsymbol{\\phi}(\\mathbf{z})$ 成立。\n\n在以下神经元发放率数据集上训练一个硬间隔支持向量机（每个 $\\mathbf{x}_{i}$ 是一个带标签 $y_{i}$ 的试验）：\n\n- 刺激 $+1$：$\\mathbf{x}_{1} = (2, 1)$，标签 $y_{1} = +1$；以及 $\\mathbf{x}_{2} = (1, 2)$，标签 $y_{2} = +1$。\n- 刺激 $-1$：$\\mathbf{x}_{3} = (1, 3)$，标签 $y_{3} = -1$；以及 $\\mathbf{x}_{4} = (0, 1)$，标签 $y_{4} = -1$。\n\n训练后，只有 $\\mathbf{x}_{1}$ 和 $\\mathbf{x}_{3}$ 是具有非零拉格朗日乘子的支持向量，具体为 $\\alpha_{1} = \\frac{1}{2}$ 和 $\\alpha_{3} = \\frac{1}{4}$，学习到的偏置为 $b = 0$。\n\n任务：\n\n1. 使用你为二次核（$c = 1$）构造的特征映射 $\\boldsymbol{\\phi}$，计算 $\\mathbf{w} = \\sum_{i \\in \\{1,3\\}} \\alpha_{i} y_{i} \\boldsymbol{\\phi}(\\mathbf{x}_{i})$。\n2. 对于测试输入 $\\mathbf{x}^{\\ast} = (2, 0)$，在映射空间中计算决策函数 $f(\\mathbf{x}^{\\ast}) = \\mathbf{w}^{\\top}\\boldsymbol{\\phi}(\\mathbf{x}^{\\ast}) + b$。\n3. 通过再次使用核表示 $f(\\mathbf{x}^{\\ast}) = \\sum_{i \\in \\{1,3\\}} \\alpha_{i} y_{i} (\\mathbf{x}_{i}^{\\top}\\mathbf{x}^{\\ast} + 1)^{2} + b$ 计算 $f(\\mathbf{x}^{\\ast})$ 来验证等价性，并确认两次计算得出相同的值。\n\n精确地给出 $f(\\mathbf{x}^{\\ast})$ 的最终数值（不进行四舍五入）。",
            "solution": "在尝试提供解决方案之前，将对问题进行验证。\n\n### 步骤 1：提取已知条件\n- 输入空间：$\\mathbf{x} = (x_{1}, x_{2}) \\in \\mathbb{R}^{2}$。\n- 分类标签：$y_{i} \\in \\{-1, +1\\}$。\n- 非齐次二次多项式核：$k(\\mathbf{x}, \\mathbf{z}) = (\\mathbf{x}^{\\top}\\mathbf{z} + c)^{2}$，其中 $c = 1$。\n- 特征映射定义：$k(\\mathbf{x}, \\mathbf{z}) = \\boldsymbol{\\phi}(\\mathbf{x})^{\\top} \\boldsymbol{\\phi}(\\mathbf{z})$。\n- 原始决策函数：$f(\\mathbf{x}) = \\mathbf{w}^{\\top}\\boldsymbol{\\phi}(\\mathbf{x}) + b$。\n- 原始权重向量：$\\mathbf{w} = \\sum_{i=1}^{n} \\alpha_{i} y_{i} \\boldsymbol{\\phi}(\\mathbf{x}_{i})$。\n- 对偶决策函数：$f(\\mathbf{x}) = \\sum_{i=1}^{n} \\alpha_{i} y_{i} k(\\mathbf{x}_{i}, \\mathbf{x}) + b$。\n- 训练数据：\n  - $\\mathbf{x}_{1} = (2, 1)$，$y_{1} = +1$。\n  - $\\mathbf{x}_{2} = (1, 2)$，$y_{2} = +1$。\n  - $\\mathbf{x}_{3} = (1, 3)$，$y_{3} = -1$。\n  - $\\mathbf{x}_{4} = (0, 1)$，$y_{4} = -1$。\n- SVM训练结果（硬间隔）：\n  - 支持向量为 $\\mathbf{x}_{1}$ 和 $\\mathbf{x}_{3}$。\n  - 拉格朗日乘子：$\\alpha_{1} = \\frac{1}{2}$，$\\alpha_{3} = \\frac{1}{4}$，且 $\\alpha_{2} = \\alpha_{4} = 0$。\n  - 偏置：$b = 0$。\n- 测试输入：$\\mathbf{x}^{\\ast} = (2, 0)$。\n\n### 步骤 2：使用提取的已知条件进行验证\n根据验证标准对问题进行评估：\n- **科学依据**：该问题基于支持向量机和核方法的标准数学理论，这些是机器学习和数据分析中的成熟技术。分析神经元发放率的用例是这些方法的常见且有效的应用领域。\n- **适定性**：该问题是适定的。它提供了执行所要求的计算所需的所有数据点、参数（$\\alpha_i$、$c$、$b$）和定义。任务具体，并能得出一个唯一的、确定性的答案。\n- **客观性**：该问题使用精确的数学语言和客观数据进行陈述。没有主观或含糊的术语。\n- **完整性与一致性**：该问题是自洽的。所有变量和常量都已定义。所提供的拉格朗日乘子、支持向量和偏置值与一个已训练的SVM模型的结果是一致的（尽管我们不被要求验证训练过程本身，只需使用其结果）。\n- **未触发其他无效标志。** 该问题是应用核方法定义的一个标准练习。\n\n### 步骤 3：结论与行动\n该问题有效。将提供完整的解决方案。\n\n**解题推导**\n\n该问题需要分三部分计算：构造特征映射 $\\boldsymbol{\\phi}(\\mathbf{x})$ 并计算权重向量 $\\mathbf{w}$；在特征空间中评估决策函数 $f(\\mathbf{x}^{\\ast})$；以及使用核技巧验证此结果。\n\n**任务1：构造 $\\boldsymbol{\\phi}(\\mathbf{x})$ 并计算 $\\mathbf{w}$**\n\n首先，我们为给定的核函数 $k(\\mathbf{x}, \\mathbf{z}) = (\\mathbf{x}^{\\top}\\mathbf{z} + 1)^{2}$ 找到一个显式的特征映射 $\\boldsymbol{\\phi}(\\mathbf{x})$。\n设 $\\mathbf{x} = (x_1, x_2)^{\\top}$ 和 $\\mathbf{z} = (z_1, z_2)^{\\top}$。\n内积为 $\\mathbf{x}^{\\top}\\mathbf{z} = x_1 z_1 + x_2 z_2$。\n核函数为 $k(\\mathbf{x}, \\mathbf{z}) = (x_1 z_1 + x_2 z_2 + 1)^{2}$。\n展开此表达式：\n$$k(\\mathbf{x}, \\mathbf{z}) = (x_1 z_1)^2 + (x_2 z_2)^2 + 1^2 + 2(x_1 z_1)(x_2 z_2) + 2(x_1 z_1)(1) + 2(x_2 z_2)(1)$$\n$$k(\\mathbf{x}, \\mathbf{z}) = x_1^2 z_1^2 + x_2^2 z_2^2 + 1 + 2 x_1 x_2 z_1 z_2 + 2 x_1 z_1 + 2 x_2 z_2$$\n为了满足 $k(\\mathbf{x}, \\mathbf{z}) = \\boldsymbol{\\phi}(\\mathbf{x})^{\\top} \\boldsymbol{\\phi}(\\mathbf{z})$，我们根据项对 $\\mathbf{x}$ 和 $\\mathbf{z}$ 的依赖关系进行分组：\n$$k(\\mathbf{x}, \\mathbf{z}) = (x_1^2)(z_1^2) + (x_2^2)(z_2^2) + (\\sqrt{2} x_1 x_2)(\\sqrt{2} z_1 z_2) + (\\sqrt{2} x_1)(\\sqrt{2} z_1) + (\\sqrt{2} x_2)(\\sqrt{2} z_2) + (1)(1)$$\n这对应于一个 $6$ 维空间 $\\mathbb{R}^{6}$ 中的内积。一个有效的特征映射是：\n$$\\boldsymbol{\\phi}(\\mathbf{x}) = \\begin{pmatrix} x_1^2 \\\\ x_2^2 \\\\ \\sqrt{2} x_1 x_2 \\\\ \\sqrt{2} x_1 \\\\ \\sqrt{2} x_2 \\\\ 1 \\end{pmatrix}$$\n接下来，我们计算权重向量 $\\mathbf{w} = \\sum_{i=1}^{n} \\alpha_{i} y_{i} \\boldsymbol{\\phi}(\\mathbf{x}_{i})$。求和仅针对支持向量 $\\mathbf{x}_1$ 和 $\\mathbf{x}_3$。\n$$\\mathbf{w} = \\alpha_1 y_1 \\boldsymbol{\\phi}(\\mathbf{x}_1) + \\alpha_3 y_3 \\boldsymbol{\\phi}(\\mathbf{x}_3)$$\n我们将支持向量映射到特征空间：\n对于 $\\mathbf{x}_1 = (2, 1)$：\n$$\\boldsymbol{\\phi}(\\mathbf{x}_1) = \\begin{pmatrix} 2^2 \\\\ 1^2 \\\\ \\sqrt{2}(2)(1) \\\\ \\sqrt{2}(2) \\\\ \\sqrt{2}(1) \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ 1 \\\\ 2\\sqrt{2} \\\\ 2\\sqrt{2} \\\\ \\sqrt{2} \\\\ 1 \\end{pmatrix}$$\n对于 $\\mathbf{x}_3 = (1, 3)$：\n$$\\boldsymbol{\\phi}(\\mathbf{x}_3) = \\begin{pmatrix} 1^2 \\\\ 3^2 \\\\ \\sqrt{2}(1)(3) \\\\ \\sqrt{2}(1) \\\\ \\sqrt{2}(3) \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 9 \\\\ 3\\sqrt{2} \\\\ \\sqrt{2} \\\\ 3\\sqrt{2} \\\\ 1 \\end{pmatrix}$$\n代入值 $\\alpha_1=\\frac{1}{2}$，$y_1=+1$，$\\alpha_3=\\frac{1}{4}$ 和 $y_3=-1$：\n$$\\mathbf{w} = \\frac{1}{2}(+1)\\begin{pmatrix} 4 \\\\ 1 \\\\ 2\\sqrt{2} \\\\ 2\\sqrt{2} \\\\ \\sqrt{2} \\\\ 1 \\end{pmatrix} + \\frac{1}{4}(-1)\\begin{pmatrix} 1 \\\\ 9 \\\\ 3\\sqrt{2} \\\\ \\sqrt{2} \\\\ 3\\sqrt{2} \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1/2 \\\\ \\sqrt{2} \\\\ \\sqrt{2} \\\\ \\sqrt{2}/2 \\\\ 1/2 \\end{pmatrix} - \\begin{pmatrix} 1/4 \\\\ 9/4 \\\\ 3\\sqrt{2}/4 \\\\ \\sqrt{2}/4 \\\\ 3\\sqrt{2}/4 \\\\ 1/4 \\end{pmatrix}$$\n$$\\mathbf{w} = \\begin{pmatrix} 2 - 1/4 \\\\ 1/2 - 9/4 \\\\ \\sqrt{2} - 3\\sqrt{2}/4 \\\\ \\sqrt{2} - \\sqrt{2}/4 \\\\ \\sqrt{2}/2 - 3\\sqrt{2}/4 \\\\ 1/2 - 1/4 \\end{pmatrix} = \\begin{pmatrix} 7/4 \\\\ -7/4 \\\\ \\sqrt{2}/4 \\\\ 3\\sqrt{2}/4 \\\\ -\\sqrt{2}/4 \\\\ 1/4 \\end{pmatrix}$$\n\n**任务2：在映射空间中计算 $f(\\mathbf{x}^{\\ast})$**\n\n我们使用 $f(\\mathbf{x}^{\\ast}) = \\mathbf{w}^{\\top}\\boldsymbol{\\phi}(\\mathbf{x}^{\\ast}) + b$ 计算测试输入 $\\mathbf{x}^{\\ast} = (2, 0)$ 的决策函数值。给定的偏置 $b$ 为 $0$。\n首先，将 $\\mathbf{x}^{\\ast}$ 映射到特征空间：\n$$\\boldsymbol{\\phi}(\\mathbf{x}^{\\ast}) = \\boldsymbol{\\phi}((2, 0)) = \\begin{pmatrix} 2^2 \\\\ 0^2 \\\\ \\sqrt{2}(2)(0) \\\\ \\sqrt{2}(2) \\\\ \\sqrt{2}(0) \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ 0 \\\\ 0 \\\\ 2\\sqrt{2} \\\\ 0 \\\\ 1 \\end{pmatrix}$$\n现在，计算内积 $\\mathbf{w}^{\\top}\\boldsymbol{\\phi}(\\mathbf{x}^{\\ast})$：\n$$f(\\mathbf{x}^{\\ast}) = \\begin{pmatrix} 7/4  -7/4  \\sqrt{2}/4  3\\sqrt{2}/4  -\\sqrt{2}/4  1/4 \\end{pmatrix} \\begin{pmatrix} 4 \\\\ 0 \\\\ 0 \\\\ 2\\sqrt{2} \\\\ 0 \\\\ 1 \\end{pmatrix} + 0$$\n$$f(\\mathbf{x}^{\\ast}) = (\\frac{7}{4})(4) + (-\\frac{7}{4})(0) + (\\frac{\\sqrt{2}}{4})(0) + (\\frac{3\\sqrt{2}}{4})(2\\sqrt{2}) + (-\\frac{\\sqrt{2}}{4})(0) + (\\frac{1}{4})(1)$$\n$$f(\\mathbf{x}^{\\ast}) = 7 + 0 + 0 + \\frac{3 \\cdot 2 \\cdot (\\sqrt{2})^2}{4} + 0 + \\frac{1}{4}$$\n$$f(\\mathbf{x}^{\\ast}) = 7 + \\frac{12}{4} + \\frac{1}{4} = 7 + 3 + \\frac{1}{4} = 10 + \\frac{1}{4} = \\frac{41}{4}$$\n\n**任务3：使用核表示进行验证**\n\n我们现在使用对偶形式 $f(\\mathbf{x}^{\\ast}) = \\sum_{i \\in \\{1,3\\}} \\alpha_{i} y_{i} k(\\mathbf{x}_{i}, \\mathbf{x}^{\\ast}) + b$ 重新计算 $f(\\mathbf{x}^{\\ast})$：\n$$f(\\mathbf{x}^{\\ast}) = \\alpha_1 y_1 k(\\mathbf{x}_1, \\mathbf{x}^{\\ast}) + \\alpha_3 y_3 k(\\mathbf{x}_3, \\mathbf{x}^{\\ast}) + b$$\n我们需要计算核函数值 $k(\\mathbf{x}_1, \\mathbf{x}^{\\ast})$ 和 $k(\\mathbf{x}_3, \\mathbf{x}^{\\ast})$。\n对于 $\\mathbf{x}_1 = (2, 1)$ 和 $\\mathbf{x}^{\\ast} = (2, 0)$：\n$$\\mathbf{x}_1^{\\top}\\mathbf{x}^{\\ast} = (2)(2) + (1)(0) = 4$$\n$$k(\\mathbf{x}_1, \\mathbf{x}^{\\ast}) = (\\mathbf{x}_1^{\\top}\\mathbf{x}^{\\ast} + 1)^2 = (4 + 1)^2 = 5^2 = 25$$\n对于 $\\mathbf{x}_3 = (1, 3)$ 和 $\\mathbf{x}^{\\ast} = (2, 0)$：\n$$\\mathbf{x}_3^{\\top}\\mathbf{x}^{\\ast} = (1)(2) + (3)(0) = 2$$\n$$k(\\mathbf{x}_3, \\mathbf{x}^{\\ast}) = (\\mathbf{x}_3^{\\top}\\mathbf{x}^{\\ast} + 1)^2 = (2 + 1)^2 = 3^2 = 9$$\n现在将这些值代入决策函数：\n$$f(\\mathbf{x}^{\\ast}) = (\\frac{1}{2})(+1)(25) + (\\frac{1}{4})(-1)(9) + 0$$\n$$f(\\mathbf{x}^{\\ast}) = \\frac{25}{2} - \\frac{9}{4}$$\n为了合并这些项，我们使用公分母 $4$：\n$$f(\\mathbf{x}^{\\ast}) = \\frac{50}{4} - \\frac{9}{4} = \\frac{41}{4}$$\n原始形式计算的结果（$\\frac{41}{4}$）与对偶（核）形式计算的结果（$\\frac{41}{4}$）相匹配，按要求验证了等价性。最终值为 $\\frac{41}{4}$。",
            "answer": "$$\n\\boxed{\\frac{41}{4}}\n$$"
        },
        {
            "introduction": "在实践中，SVM分类器的性能，尤其是使用径向基函数 (RBF) 核时，严重依赖于其超参数。这个练习深入探讨了RBF核的$\\gamma$参数的理论行为，而这个参数常常是混淆的来源。通过将几何间隔推导为$\\gamma$的函数并分析其极限行为，您将获得关于该参数如何控制模型复杂度，并将核的抽象数学与神经解码中过拟合和泛化的实际问题联系起来的关键见解。",
            "id": "4172674",
            "problem": "在一个双条件神经解码任务中，从 $p$ 个神经元记录了条件 $+$ 和条件 $-$ 各一次试验的群体活动。设得到的试验平均活动向量为 $x_{+} \\in \\mathbb{R}^{p}$ 和 $x_{-} \\in \\mathbb{R}^{p}$，其对应的标签为 $y_{+} = +1$ 和 $y_{-} = -1$。一个硬间隔支持向量机 (SVM) 在由径向基函数 (RBF) 核 $K_{\\gamma}(x, x') = \\exp(-\\gamma \\|x - x'\\|^{2})$ 诱导的再生核希尔伯特空间 (RKHS) 中进行训练，其中 $\\gamma > 0$ 是核宽度参数。将两个活动向量之间的欧几里得距离的平方记为 $\\Delta = \\|x_{+} - x_{-}\\|^{2}$。\n\n从标准硬间隔 SVM 对偶公式和 RKHS 中几何间隔的定义出发，推导间隔 $\\rho(\\gamma)$ 作为 $\\gamma$ 和 $\\Delta$ 的函数的精确表达式。然后计算两个极限 $\\lim_{\\gamma \\to 0} \\rho(\\gamma)$ 和 $\\lim_{\\gamma \\to \\infty} \\rho(\\gamma)$，并基于经典的基于间隔的泛化论证，解释这些极限如何定性地反映分类器从这个双样本训练集泛化到未见神经数据的能力。仅使用精确的解析表达式，将你的最终答案表示为有序对 $\\left(\\lim_{\\gamma \\to 0} \\rho(\\gamma), \\lim_{\\gamma \\to \\infty} \\rho(\\gamma)\\right)$。无需进行数值舍入。",
            "solution": "该问题在科学上基于机器学习理论，提法恰当、客观且内部一致，因此被评估为有效。我们可以进行形式化推导。\n\n该问题要求推导在一个硬间隔支持向量机 (SVM) 上的几何间隔，该 SVM 在两个数据点 $(x_+, y_+)$ 和 $(x_-, y_-)$ 上训练，其中 $x_+, x_- \\in \\mathbb{R}^{p}$ 是活动向量，$y_+=+1, y_-=-1$ 是对应的标签。分类在由径向基函数 (RBF) 核 $K_{\\gamma}(x, x') = \\exp(-\\gamma \\|x - x'\\|^{2})$ 诱导的再生核希尔伯特空间 (RKHS) 中进行，记为 $\\mathcal{H}$。输入向量之间的欧几里得距离平方为 $\\Delta = \\|x_{+} - x_{-}\\|^{2}$。\n\n我们从硬间隔 SVM 问题的对偶公式开始。对于一个包含 $N$ 个点 $(x_i, y_i)$ 的数据集，对偶目标是关于乘子 $\\alpha_i$ 最大化拉格朗日函数 $W(\\alpha)$：\n$$ W(\\alpha) = \\sum_{i=1}^{N} \\alpha_i - \\frac{1}{2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_i \\alpha_j y_i y_j K(x_i, x_j) $$\n约束条件为 $\\sum_{i=1}^{N} \\alpha_i y_i = 0$ 和 $\\alpha_i \\ge 0$ (对 $i=1, \\dots, N$ 成立)。\n\n在我们的特定情况下，$N=2$。令 $(x_1, y_1) = (x_+, +1)$ 且 $(x_2, y_2) = (x_-, -1)$。约束 $\\sum \\alpha_i y_i = 0$ 变为：\n$$ \\alpha_1 y_1 + \\alpha_2 y_2 = 0 $$\n$$ \\alpha_1 (+1) + \\alpha_2 (-1) = 0 \\implies \\alpha_1 = \\alpha_2 $$\n由于问题是非平凡的（即，点是可分的，意味着 $x_+ \\neq x_-$ 因而 $\\Delta > 0$），这两个支持向量的拉格朗日乘子 $\\alpha_i$ 将严格为正。我们将公共乘子记为 $\\alpha = \\alpha_1 = \\alpha_2 > 0$。\n\n将 $N=2$ 和 $\\alpha_1 = \\alpha_2 = \\alpha$ 代入对偶目标 $W(\\alpha)$：\n$$ W(\\alpha) = \\alpha_1 + \\alpha_2 - \\frac{1}{2} \\left( \\alpha_1^2 y_1^2 K(x_1, x_1) + \\alpha_2^2 y_2^2 K(x_2, x_2) + 2 \\alpha_1 \\alpha_2 y_1 y_2 K(x_1, x_2) \\right) $$\n$$ W(\\alpha) = 2\\alpha - \\frac{1}{2} \\left( \\alpha^2 (+1)^2 K(x_+, x_+) + \\alpha^2 (-1)^2 K(x_-, x_-) + 2 \\alpha^2 (+1)(-1) K(x_+, x_-) \\right) $$\n$$ W(\\alpha) = 2\\alpha - \\frac{1}{2} \\alpha^2 \\left( K(x_+, x_+) + K(x_-, x_-) - 2K(x_+, x_-) \\right) $$\n现在，我们使用 $K_{\\gamma}(x, x') = \\exp(-\\gamma \\|x - x'\\|^{2})$ 和 $\\Delta = \\|x_+ - x_-\\|^2$ 来计算核函数项：\n- $K(x_+, x_+) = \\exp(-\\gamma \\|x_+ - x_+\\|^2) = \\exp(0) = 1$。\n- $K(x_-, x_-) = \\exp(-\\gamma \\|x_- - x_-\\|^2) = \\exp(0) = 1$。\n- $K(x_+, x_-) = \\exp(-\\gamma \\|x_+ - x_-\\|^2) = \\exp(-\\gamma \\Delta)$。\n\n将这些代入 $W(\\alpha)$ 的表达式中：\n$$ W(\\alpha) = 2\\alpha - \\frac{1}{2} \\alpha^2 (1 + 1 - 2\\exp(-\\gamma \\Delta)) = 2\\alpha - \\alpha^2 (1 - \\exp(-\\gamma \\Delta)) $$\n为了找到最大化 $W(\\alpha)$ 的最优 $\\alpha$，我们对 $\\alpha$ 求导并令其为零：\n$$ \\frac{d W}{d \\alpha} = 2 - 2\\alpha (1 - \\exp(-\\gamma \\Delta)) = 0 $$\n解出 $\\alpha$：\n$$ \\alpha = \\frac{1}{1 - \\exp(-\\gamma \\Delta)} $$\n该解是有效的，因为 $\\gamma > 0$ 和 $\\Delta > 0$ 意味着 $0  \\exp(-\\gamma \\Delta)  1$，所以 $\\alpha$ 是正且有限的。\n\n几何间隔 $\\rho$ 定义为 $\\rho = 1/\\|w\\|_{\\mathcal{H}}$，其中 $\\|w\\|_{\\mathcal{H}}$ 是 RKHS 中权重向量的范数。该范数通过最优拉格朗日乘子与以下公式相关：\n$$ \\|w\\|_{\\mathcal{H}}^2 = \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_i \\alpha_j y_i y_j K(x_i, x_j) $$\n在我们的情况下，这简化为我们已在 $W(\\alpha)$ 内部找到的项：\n$$ \\|w\\|_{\\mathcal{H}}^2 = \\alpha^2 (K(x_+, x_+) + K(x_-, x_-) - 2K(x_+, x_-)) = 2\\alpha^2 (1 - \\exp(-\\gamma\\Delta)) $$\n代入 $\\alpha$ 的最优值：\n$$ \\|w\\|_{\\mathcal{H}}^2 = 2 \\left( \\frac{1}{1 - \\exp(-\\gamma \\Delta)} \\right)^2 (1 - \\exp(-\\gamma \\Delta)) = \\frac{2}{1 - \\exp(-\\gamma \\Delta)} $$\n几何间隔的平方为 $\\rho^2 = 1 / \\|w\\|_{\\mathcal{H}}^2$。\n$$ \\rho(\\gamma)^2 = \\frac{1 - \\exp(-\\gamma \\Delta)}{2} $$\n因此，作为 $\\gamma$ 和 $\\Delta$ 的函数的几何间隔是：\n$$ \\rho(\\gamma) = \\sqrt{\\frac{1 - \\exp(-\\gamma \\Delta)}{2}} $$\n\n接下来，我们计算所要求的两个极限。\n首先，当 $\\gamma \\to 0$ 时的极限：\n$$ \\lim_{\\gamma \\to 0} \\rho(\\gamma) = \\lim_{\\gamma \\to 0} \\sqrt{\\frac{1 - \\exp(-\\gamma \\Delta)}{2}} $$\n当 $\\gamma \\to 0$ 时，指数 $-\\gamma\\Delta \\to 0$。因此，$\\exp(-\\gamma\\Delta) \\to \\exp(0) = 1$。\n$$ \\lim_{\\gamma \\to 0} \\rho(\\gamma) = \\sqrt{\\frac{1 - 1}{2}} = \\sqrt{0} = 0 $$\n其次，当 $\\gamma \\to \\infty$ 时的极限：\n$$ \\lim_{\\gamma \\to \\infty} \\rho(\\gamma) = \\lim_{\\gamma \\to \\infty} \\sqrt{\\frac{1 - \\exp(-\\gamma \\Delta)}{2}} $$\n当 $\\gamma \\to \\infty$ 时，并假设 $\\Delta  0$，指数 $-\\gamma\\Delta \\to -\\infty$。因此，$\\exp(-\\gamma\\Delta) \\to 0$。\n$$ \\lim_{\\gamma \\to \\infty} \\rho(\\gamma) = \\sqrt{\\frac{1 - 0}{2}} = \\sqrt{\\frac{1}{2}} = \\frac{\\sqrt{2}}{2} $$\n\n最后，我们基于泛化对这些结果提供一个定性解释。经典的间隔理论表明，更大的间隔对应于更低的分类器容量和更好的泛化能力。\n- 在极限 $\\gamma \\to 0$ 时，RBF 核变得无限宽，$K(x, x') \\to 1$ 对任意点对成立。特征映射 $\\phi(x)$ 将所有输入向量映射到 RKHS 中几乎同一点，导致 $\\phi(x_+)$ 和 $\\phi(x_-)$ 之间的距离消失。因此，间隔 $\\rho(\\gamma)$ 塌陷为 $0$。分类器失去了所有判别能力，因为它无法在特征空间中分离这些点。这对应于一种“欠拟合”状态，即模型过于简单，无法捕捉数据的结构，从而导致泛化能力差。\n- 在极限 $\\gamma \\to \\infty$ 时，RBF 核变成一个类 δ 函数，仅在输入相同时才非零。核矩阵趋近于单位矩阵，这意味着特征映射将两个不同的点 $x_+$ 和 $x_-$ 发送到 RKHS 中的两个正交单位向量。它们变得最大程度地分离，间隔 $\\rho(\\gamma)$ 趋近于其最大可能值 $\\sqrt{2}/2$（两个正交单位向量之间距离的一半）。虽然大间隔通常与好的泛化能力相关联，但在这种情况下，它表示极端的“过拟合”。分类器实际上记住了这两个训练点，创建了一个在它们周围高度敏感的决策函数，但无法泛化到任何新的、未见的点（除非它与某个训练点完全相同）。模型容量变得过高，以牺牲任何更广泛的适用性为代价，完美地拟合了特定的训练样本。\n\n因此，$\\gamma$ 的两个极端都导致从这个极小的训练集泛化能力差，尽管是出于相反的原因（欠拟合 vs. 过拟合）。实际应用将需要通过像交叉验证这样的方法选择一个中间值 $\\gamma$，以平衡模型复杂度和在未见数据上实现最佳性能。\n\n最终答案是这两个极限的有序对。\n$$ \\left( \\lim_{\\gamma \\to 0} \\rho(\\gamma), \\lim_{\\gamma \\to \\infty} \\rho(\\gamma) \\right) = \\left( 0, \\frac{\\sqrt{2}}{2} \\right) $$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0  \\frac{\\sqrt{2}}{2}\n\\end{pmatrix}\n}\n$$"
        }
    ]
}