{
    "hands_on_practices": [
        {
            "introduction": "The Short-Time Fourier Transform (STFT) builds a spectrogram by analyzing successive, overlapping segments of a signal. The choice of how much these segments overlap, defined by the hop size $H$, is not arbitrary; it directly impacts the smoothness and redundancy of the resulting time-frequency representation. This exercise  guides you through deriving the relationship between overlap and hop size, solidifying your understanding of why a 50% overlap is a principled choice for constructing a robust time axis for your spectrogram.",
            "id": "4194041",
            "problem": "You are analyzing local field potential data from rodent prefrontal cortex recorded at a sampling frequency $F_{s}$, and you plan to compute a spectrogram using the Short-Time Fourier Transform (STFT). The STFT is defined by applying a windowed Fourier transform to overlapping segments of the discrete-time signal $x[n]$ using an analysis window $w[n]$ of fixed length $L$ samples, with successive segments beginning at indices $n_{m} = m H$, where $H$ is the hop size in samples and $m$ is an integer frame index. The overlap between consecutive segments is defined as the ratio of the number of shared samples to the window length.\n\nStarting from these definitions and the discrete-time indexing relations of successive windows, derive the relationship between the overlap fraction and the hop size $H$ for a fixed window length $L$. Use this relationship to determine the hop size $H$ that achieves $50\\%$ overlap. Then, justify how this choice of overlap affects (i) the time-domain redundancy of the STFT representation and (ii) the smoothness of the spectrogram magnitude as a function of time, invoking appropriate well-tested properties of window families used in neuroscience signal processing.\n\nExpress your final answer for the hop size in samples. No rounding is required. Your justification must be based on first principles and well-tested facts and should not rely on unproven heuristics or shortcut formulas.",
            "solution": "The problem statement is found to be valid as it is scientifically grounded in the principles of digital signal processing, specifically the Short-Time Fourier Transform (STFT), and is well-posed, objective, and contains sufficient information for a unique solution.\n\nThe problem requires a derivation of the relationship between the overlap fraction, window length $L$, and hop size $H$, followed by a calculation of $H$ for a specific overlap and a justification of this choice.\n\nFirst, we derive the relationship between the overlap fraction, window length $L$, and hop size $H$. The STFT analyzes a discrete-time signal $x[n]$ by segmenting it into frames. Each frame is a product of the signal and an analysis window $w[n]$ of length $L$ samples.\n\nLet us consider two consecutive analysis frames. The $m$-th frame starts at sample index $n_m = m H$. The window $w[n]$ has a support from $n=0$ to $n=L-1$. Thus, the $m$-th segment of the signal being analyzed spans the indices from $m H$ to $m H + L - 1$.\n\nThe next frame is the $(m+1)$-th frame, which starts at sample index $n_{m+1} = (m+1) H$. This segment spans the indices from $(m+1) H$ to $(m+1) H + L - 1$.\n\nFor an overlap to occur, we must have $H < L$. The overlap consists of the samples that are part of both the $m$-th and the $(m+1)$-th segments. The $m$-th segment ends at index $m H + L - 1$. The $(m+1)$-th segment begins at index $(m+1) H$. The overlapping samples are therefore in the index range from $(m+1) H$ to $m H + L - 1$.\n\nThe number of overlapping samples, let's denote it $N_{overlap}$, is the number of integers in this range.\n$$\nN_{overlap} = (m H + L - 1) - ((m+1) H) + 1\n$$\n$$\nN_{overlap} = m H + L - 1 - m H - H + 1\n$$\n$$\nN_{overlap} = L - H\n$$\nThe problem defines the overlap fraction, let's call it $O$, as the ratio of the number of shared samples to the window length $L$.\n$$\nO = \\frac{N_{overlap}}{L}\n$$\nSubstituting our expression for $N_{overlap}$:\n$$\nO = \\frac{L - H}{L} = 1 - \\frac{H}{L}\n$$\nThis is the general relationship between the overlap fraction $O$, the window length $L$, and the hop size $H$.\n\nNext, we are asked to determine the hop size $H$ that achieves a $50\\%$ overlap. A $50\\%$ overlap corresponds to an overlap fraction $O = 0.5$.\nWe substitute this value into the derived relationship:\n$$\n0.5 = 1 - \\frac{H}{L}\n$$\nRearranging the equation to solve for $H$:\n$$\n\\frac{H}{L} = 1 - 0.5\n$$\n$$\n\\frac{H}{L} = 0.5\n$$\n$$\nH = 0.5 L = \\frac{L}{2}\n$$\nThus, a hop size equal to half the window length achieves a $50\\%$ overlap.\n\nFinally, we must justify this choice of overlap with respect to (i) time-domain redundancy and (ii) the smoothness of the spectrogram magnitude.\n\n(i) Time-domain redundancy:\nThe ratio $L/H$ represents the oversampling factor in the time domain. It indicates how many analysis windows, on average, cover a single sample of the input signal $x[n]$. With a hop size of $H = L/2$, the oversampling factor is $L / (L/2) = 2$. This means that every sample in the signal (except for those within the first and last $L/2$ samples of the signal) is included in exactly two consecutive analysis windows. This duplication of information creates redundancy in the time axis of the STFT. While successive time frames in the spectrogram are now correlated, this redundancy is beneficial for robustly tracking spectral changes and is a prerequisite for perfect reconstruction of the signal from its modified STFT, if needed.\n\n(ii) Smoothness of the spectrogram magnitude as a function of time:\nThe smoothness of the spectrogram's magnitude across time frames is crucial for accurately representing the temporal evolution of the signal's frequency content. A significant issue in STFT analysis is spectral leakage and the \"picket-fence effect\", which can be compounded by \"scalloping loss\" on the time axis. Scalloping loss refers to the modulation of the estimated amplitude of a signal component as its position shifts relative to the center of the analysis window.\n\nWindow functions used in neuroscience (e.g., Hann, Hamming, Blackman) are tapered, meaning their amplitude is maximal at the center and decays to zero (or near zero) at the edges. If a transient signal event or a portion of a continuous sinusoid falls near the edge of one window, its contribution to that frame's spectrum will be heavily attenuated. However, with a $50\\%$ overlap ($H = L/2$), if an event is at the edge of frame $m$, it will be at the center of the subsequent frame $m+1$.\n\nThis property is formally captured by the Constant Overlap-Add (COLA) criterion. For many standard window functions, a $50\\%$ overlap ensures that the sum of the squared window values is constant, i.e., $\\sum_{m} w^2[n - mH] = C$ for some constant $C > 0$. For instance, for the Hann window defined as $w[n] = 0.5 - 0.5 \\cos(2\\pi n / (L-1))$ for $n \\in [0, L-1]$, choosing $H=L/2$ (for even $L$) results in a constant sum of squares. This specific property is known as the Princen-Bradley condition.\n\nThis constant-energy weighting ensures that the total energy of a signal component is represented consistently across frames, regardless of its alignment with the window centers. This mitigates the temporal scalloping loss, preventing artificial amplitude modulation in the spectrogram. The result is a spectrogram where the magnitude changes smoothly and faithfully reflect the underlying dynamics of the signal's power spectrum, rather than being an artifact of the windowing procedure. Therefore, a $50\\%$ overlap is a standard and well-justified choice for producing smooth and reliable spectrograms.",
            "answer": "$$\n\\boxed{\\frac{L}{2}}\n$$"
        },
        {
            "introduction": "A central challenge in time-frequency analysis is the inherent trade-off between resolving events in time and resolving their constituent frequencies, a direct consequence of the uncertainty principle. The length of the analysis window, $L$, is the primary parameter governing this trade-off, as a longer window provides finer frequency resolution at the cost of temporal precision. This practice  provides a concrete example of this principle, tasking you with calculating the minimum window length needed to distinguish closely spaced neural oscillations, a critical skill for designing analyses sensitive to specific scientific questions.",
            "id": "4194060",
            "problem": "A cortical local field potential (LFP) recording is sampled at a rate of $f_{s} = 2000$ Hz and analyzed with a spectrogram computed via the Short-Time Fourier Transform (STFT). The time-frequency tiles are produced by applying a length-$L$ Hann window to successive data segments. You aim to resolve narrowband high-gamma activity centered near $80$ Hz such that distinct components separated by $5$ Hz are not smeared together by the window’s spectral main lobe. Using only fundamental definitions of the STFT and the windowed Fourier transform, and the well-tested approximation that for a Hann window the main-lobe width between the first zeros in digital angular frequency is approximately $\\Delta \\omega_{\\mathrm{ML}} \\approx 8\\pi/L$, derive from first principles the minimum window duration $T_{\\min}$ (in seconds) such that the Hann main-lobe width in Hertz does not exceed $5$ Hz. Then, compute the corresponding minimal integer window length $L_{\\min}$ (in samples) at $f_{s} = 2000$ Hz. Report only the integer $L_{\\min}$ in samples as your final answer. No rounding by significant figures is required beyond taking the minimal integer number of samples.",
            "solution": "The problem is to determine the minimum window length in samples, $L_{\\min}$, for a Short-Time Fourier Transform (STFT) analysis that provides a specified frequency resolution. The analysis uses a Hann window.\n\nFirst, we must establish the relationship between the different domains of frequency. The continuous-time (analog) frequency $f$, measured in Hertz (Hz), is related to the discrete-time (digital) angular frequency $\\omega$, measured in radians per sample, through the sampling rate $f_s$ (in Hz) or, equivalently, the sampling period $T_s = 1/f_s$ (in seconds). The relationship is given by:\n$$ \\omega = 2\\pi f T_s = \\frac{2\\pi f}{f_s} $$\nThis equation defines the mapping from the analog frequency axis to the digital frequency axis. Consequently, a frequency interval $\\Delta f$ in the analog domain corresponds to a digital angular frequency interval $\\Delta \\omega$ in the digital domain. By differentiating the above expression, or by considering the interval between two frequencies, we find the conversion for frequency widths:\n$$ \\Delta \\omega = \\frac{2\\pi \\Delta f}{f_s} $$\nRearranging this equation allows us to convert a width in the digital angular frequency domain ($\\Delta \\omega$) to a width in the analog frequency domain ($\\Delta f$):\n$$ \\Delta f = \\frac{f_s}{2\\pi} \\Delta \\omega $$\nThe problem provides a well-known approximation for the main-lobe width of a Hann window of length $L$ samples. This width, measured between the first nulls (zeros) of its Fourier transform in the digital angular frequency domain, is:\n$$ \\Delta \\omega_{\\mathrm{ML}} \\approx \\frac{8\\pi}{L} $$\nTo find the main-lobe width in Hertz, $\\Delta f_{\\mathrm{ML}}$, we substitute this expression into our conversion formula:\n$$ \\Delta f_{\\mathrm{ML}} = \\frac{f_s}{2\\pi} \\Delta \\omega_{\\mathrm{ML}} \\approx \\frac{f_s}{2\\pi} \\left(\\frac{8\\pi}{L}\\right) = \\frac{4f_s}{L} $$\nThe problem requires that we can resolve spectral components separated by $5$ Hz. A common criterion for resolving two frequencies is that the main-lobe width of the spectral analysis window must be less than or equal to the frequency separation of interest. This ensures that the peak of one component does not fall within the main lobe of the other, preventing them from being smeared into a single peak. Therefore, we must satisfy the condition:\n$$ \\Delta f_{\\mathrm{ML}} \\le 5 \\, \\text{Hz} $$\nSubstituting our derived expression for $\\Delta f_{\\mathrm{ML}}$ gives the inequality:\n$$ \\frac{4f_s}{L} \\le 5 $$\nThe problem asks for the minimum window duration $T_{\\min}$ as part of the derivation. The duration of the window in seconds, $T$, is related to its length in samples, $L$, by $T = L T_s = L/f_s$. We can rewrite the inequality in terms of $T$:\n$$ \\frac{4}{L/f_s} \\le 5 \\implies \\frac{4}{T} \\le 5 $$\nSolving for $T$, we get:\n$$ T \\ge \\frac{4}{5} \\, \\text{s} = 0.8 \\, \\text{s} $$\nThus, the minimum required window duration is $T_{\\min} = 0.8$ seconds.\n\nThe primary goal is to find the minimum integer window length $L_{\\min}$. We return to the inequality involving $L$:\n$$ \\frac{4f_s}{L} \\le 5 $$\nWe solve for $L$:\n$$ L \\ge \\frac{4f_s}{5} $$\nNow, we substitute the given sampling rate, $f_s = 2000$ Hz:\n$$ L \\ge \\frac{4 \\times 2000}{5} = \\frac{8000}{5} = 1600 $$\nThe window length $L$ must be an integer number of samples. The minimum integer value of $L$ that satisfies this condition is $1600$. Therefore, the minimal integer window length is:\n$$ L_{\\min} = 1600 $$",
            "answer": "$$\\boxed{1600}$$"
        },
        {
            "introduction": "After computing spectrograms, the ultimate goal is to determine if observed patterns of neural activity are statistically significant, which poses a major multiple comparisons problem across thousands of time-frequency points. Cluster-based permutation testing is a powerful non-parametric technique that elegantly solves this issue by considering the contiguous structure of neural signals, rather than testing each point in isolation. This capstone practice  walks you through a complete analysis pipeline, from data simulation to implementing this advanced statistical method, which is essential for drawing robust conclusions from modern neuroscience data.",
            "id": "4194056",
            "problem": "You are given a simulated multitrial electrophysiology dataset representing time series $\\{x_i[t]\\}_{i=1}^{N}$, each $x_i[t]$ measured in volts over time $t$ in seconds, where $N$ denotes the number of trials. The sampling rate is $f_s$ in Hertz (Hz). A stimulus occurs at time $t_0$ (in seconds), and some trials include an induced oscillation in the gamma band (defined as $30$–$80$ Hz). Your task is to compute a time-frequency spectrogram, construct a trial-wise baseline-corrected gamma power difference, and perform a cluster-based permutation test to detect increases in gamma power after the stimulus. You must then report significant time-frequency clusters with family-wise error corrected $p$-values.\n\nStart from the Short-Time Fourier Transform (STFT) definition. For a discrete-time signal $x[t]$ sampled at $f_s$ Hz and a window $w[n]$ of length $L$, the STFT at time frame index $m$ and frequency bin index $k$ is\n$$\nX[m,k] = \\sum_{n=0}^{L-1} x[n + mR] \\, w[n] \\, e^{-i 2 \\pi k n / L},\n$$\nwhere $R$ is the hop size in samples. The spectrogram power is $P[m,k] = |X[m,k]|^2$. Let $f[k]$ denote the frequency (in Hz) for bin $k$ and $t[m]$ denote the center time (in seconds) for frame $m$.\n\nDefine a post-stimulus time window $W_{\\text{post}} = [t_0 + \\Delta_1, t_0 + \\Delta_2]$ and a baseline window $W_{\\text{base}} = [t_0 - \\Delta_2, t_0 - \\Delta_1]$ with $\\Delta_1$ and $\\Delta_2$ specified in seconds. Restrict analysis to the gamma band, i.e., frequencies $f[k] \\in [30,80]$ Hz. For each trial $i$, each gamma frequency bin $k$, and each post-stimulus time frame $m$ with $t[m] \\in W_{\\text{post}}$, compute a baseline-corrected power difference\n$$\nD_i[k,m] = P_i[k,m] - \\frac{1}{|M_{\\text{base}}(k)|} \\sum_{m' \\in M_{\\text{base}}(k)} P_i[k,m'],\n$$\nwhere $P_i$ is the spectrogram power for trial $i$ and $M_{\\text{base}}(k)$ is the index set of baseline frames with $t[m'] \\in W_{\\text{base}}$. This yields a time-frequency grid of differences $\\{D_i[k,m]\\}$ across trials.\n\nFormulate the one-sample null hypothesis that there is no increase in gamma power after the stimulus, i.e., for each time-frequency bin $(k,m)$,\n$$\nH_0: \\mu_{D}[k,m] = 0,\n$$\nwhere $\\mu_{D}[k,m]$ is the across-trial mean of $D_i[k,m]$. Compute the Student’s $t$-statistic at each $(k,m)$ across trials using\n$$\nt[k,m] = \\frac{\\overline{D}[k,m]}{s_D[k,m]/\\sqrt{N}},\n$$\nwhere $\\overline{D}[k,m]$ is the sample mean and $s_D[k,m]$ is the sample standard deviation across trials, with $N$ trials.\n\nApply a cluster-forming threshold using the one-sided critical value for the Student’s $t$-distribution at level $\\alpha_{\\text{cf}}$ with $N-1$ degrees of freedom, keeping only bins where $t[k,m]$ exceeds this threshold. Define clusters as sets of contiguous bins in the time-frequency grid using $4$-connectivity (adjacent in time or frequency). For each cluster $C$, define its mass as the sum of $t$-statistics:\n$$\nS(C) = \\sum_{(k,m) \\in C} t[k,m].\n$$\n\nPerform a cluster-based permutation test using sign-flips across trials (valid under the symmetric null for paired differences). For each permutation $p = 1, \\dots, P$, draw independent random signs $\\sigma_i^{(p)} \\in \\{-1, +1\\}$ for trials $i = 1, \\dots, N$, form $\\tilde{D}_i^{(p)}[k,m] = \\sigma_i^{(p)} \\cdot D_i[k,m]$, recompute the $t$-map and cluster masses, and record the maximum cluster mass\n$$\nM^{(p)} = \\max_{C^{(p)}} S\\left(C^{(p)}\\right),\n$$\nwith $M^{(p)} = 0$ if no cluster forms under permutation $p$. For each observed cluster $C$, compute a family-wise error corrected $p$-value using the maximum-statistic distribution,\n$$\np_{\\text{corr}}(C) = \\frac{1 + \\left|\\left\\{p : M^{(p)} \\ge S(C)\\right\\}\\right|}{P + 1}.\n$$\nDeclare $C$ significant if $p_{\\text{corr}}(C) < \\alpha$ for a specified $\\alpha$.\n\nImplement the above as a complete, runnable program that simulates trials with and without a stimulus-induced gamma burst. Use the following fixed analysis parameters: sampling rate $f_s = 500$ Hz, total duration $T = 2.0$ seconds, stimulus time $t_0 = 1.0$ seconds, STFT window length $L = 256$ samples, hop size $R = 64$ samples, Hann window $w[n]$. Use $W_{\\text{base}} = [0.6, 0.9]$ seconds and $W_{\\text{post}} = [1.1, 1.5]$ seconds. The gamma burst, when present, is a sinusoid at frequency $f_{\\gamma} = 40$ Hz whose amplitude is tapered with a Hanning envelope over $[1.15, 1.45]$ seconds.\n\nYour program should implement the test suite with the following parameter sets, each specified as a tuple $(N, A_{\\gamma}, P, \\alpha_{\\text{cf}}, \\alpha, \\text{seed})$:\n- Test Case $1$: $(60, 2.0, 500, 0.05, 0.05, 42)$.\n- Test Case $2$: $(60, 0.0, 500, 0.05, 0.05, 43)$.\n- Test Case $3$: $(20, 0.8, 300, 0.05, 0.05, 44)$.\n\nHere, $N$ is the number of trials, $A_{\\gamma}$ is the gamma burst amplitude (unitless relative to noise, where $A_{\\gamma} = 0.0$ means no gamma burst), $P$ is the number of permutations, $\\alpha_{\\text{cf}}$ is the cluster-forming threshold level, $\\alpha$ is the family-wise error level, and $\\text{seed}$ is the random seed controlling reproducibility.\n\nFor each test case, compute the number of significant clusters within the gamma band and the smallest corrected $p$-value across all observed clusters (report $1.0$ if no cluster is observed). Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case’s result is reported as a two-element list $[n_{\\text{sig}}, p_{\\min}]$. For example, the output format should be like $[[n_{\\text{sig}}^{(1)}, p_{\\min}^{(1)}],[n_{\\text{sig}}^{(2)}, p_{\\min}^{(2)}],[n_{\\text{sig}}^{(3)}, p_{\\min}^{(3)}]]$. No physical unit is required in the output, since counts and $p$-values are unitless. Angles, if any were to be used, must be in radians, but no angle quantities appear in this task.",
            "solution": "The user has provided a problem that requires the implementation of a complete pipeline for analyzing simulated electrophysiology data, culminating in a cluster-based permutation test to identify significant time-frequency patterns. The problem is scientifically sound, well-posed, and all necessary parameters are specified. I will proceed with a full solution.\n\nThe solution is structured around fundamental principles of time-frequency analysis and non-parametric statistics commonly applied in neuroscience.\n\n1.  **Principle 1: Data Simulation for Method Validation**\n    To test and validate the analysis pipeline, we first synthesize a dataset that mimics key features of real neural data. Each trial consists of a signal plus noise.\n    -   **Noise**: The background neural activity is modeled as Gaussian white noise, $\\mathcal{N}(0,1)$. This represents the stochastic, non-stimulus-related fluctuations in the recorded signal.\n    -   **Signal**: The stimulus-induced effect is a gamma-band oscillation. This is represented by a sinusoid at frequency $f_{\\gamma} = 40$ Hz. The sinusoid's amplitude is modulated by a Hanning window, $A_{\\gamma} w_{\\text{Hanning}}(t)$, to create a transient burst that is localized in a specific post-stimulus time interval, $[1.15, 1.45]$ seconds. The parameter $A_{\\gamma}$ controls the signal-to-noise ratio. The signal for trial $i$ is $x_i[t] = \\text{signal}[t] + \\text{noise}_i[t]$.\n\n2.  **Principle 2: Time-Frequency Decomposition via STFT**\n    Neural signals are non-stationary, meaning their frequency content changes over time. The Short-Time Fourier Transform (STFT) is a standard method to analyze such signals. It works by dividing the signal into short, overlapping segments, applying a window function, and computing the Fourier Transform for each segment.\n    -   **Spectrogram**: For a signal $x[t]$, the STFT, $X[m,k]$, gives a complex value for each time frame $m$ and frequency bin $k$. The power spectrogram, $P[m,k] = |X[m,k]|^2$, quantifies the signal's energy at that specific time-frequency coordinate.\n    -   **Implementation**: We use a Hann window of length $L=256$ samples and a hop size of $R=64$ samples. This choice balances temporal resolution (determined by $R$) and frequency resolution (determined by $L$).\n\n3.  **Principle 3: Baseline Correction to Isolate Stimulus Effects**\n    To isolate power changes induced by the stimulus, we compare the power in a post-stimulus window to the power in a pre-stimulus baseline window.\n    -   **Difference Metric**: For each trial $i$ and frequency bin $k$, we compute the average power within the baseline window $W_{\\text{base}} = [0.6, 0.9]$ s. Let this be $\\bar{P}_{i, \\text{base}}[k]$. We then compute the difference $D_i[k,m] = P_i[k,m] - \\bar{P}_{i, \\text{base}}[k]$ for each time frame $m$ in the post-stimulus window $W_{\\text{post}} = [1.1, 1.5]$ s. This subtractive correction helps to account for trial-to-trial variability and frequency-specific power differences that are not related to the stimulus.\n\n4.  **Principle 4: Point-wise Hypothesis Testing**\n    To assess whether the observed power increase is statistically meaningful, we formulate a hypothesis test at each time-frequency point $(k,m)$ in the analysis window.\n    -   **Null Hypothesis**: The null hypothesis, $H_0: \\mu_{D}[k,m] = 0$, states that there is no average change in power from baseline to the post-stimulus period.\n    -   **Test Statistic**: We use a one-sample Student's $t$-statistic to test this hypothesis, computed across the $N$ trials:\n        $$\n        t[k,m] = \\frac{\\overline{D}[k,m]}{s_D[k,m]/\\sqrt{N}}\n        $$\n        where $\\overline{D}[k,m]$ and $s_D[k,m]$ are the sample mean and standard deviation of $D_i[k,m]$ across trials. A large positive $t$-value suggests a significant increase in power.\n\n5.  **Principle 5: Cluster-Based Permutation Test for Multiple Comparisons Correction**\n    Performing thousands of $t$-tests (one for each time-frequency bin) creates a severe multiple comparisons problem, leading to a high rate of false positives if uncorrected. The cluster-based permutation test is a powerful method to address this by incorporating the spatial structure (i.e., contiguity in time and frequency) of the data.\n    -   **Step 5a: Cluster Formation**: We first identify candidate clusters. The observed $t$-map, $\\{t[k,m]\\}$, is thresholded using a critical value from the Student’s $t$-distribution (e.g., at $\\alpha_{\\text{cf}} = 0.05$). Spatially adjacent points that exceed this threshold are grouped into clusters. The \"mass\" of a cluster $C$ is defined as the sum of the $t$-values within it, $S(C) = \\sum_{(k,m) \\in C} t[k,m]$.\n    -   **Step 5b: Generation of a Null Distribution via Permutations**: To determine if the observed cluster masses are larger than what would be expected by chance, we generate a null distribution. This is done by repeatedly permuting the data. Under the null hypothesis, the sign of the difference value $D_i[k,m]$ for any given trial is arbitrary. We can therefore create a surrogate dataset by randomly flipping the signs of the entire data for a random subset of trials: $\\tilde{D}_i[k,m] = \\sigma_i \\cdot D_i[k,m]$, where $\\sigma_i \\in \\{-1, +1\\}$.\n    -   **Step 5c: The Maximum Statistic**: For each of the $P$ permutations, we re-compute the entire $t$-map, form clusters, and find the maximum cluster mass, $M^{(p)} = \\max_{C^{(p)}} S(C^{(p)})$. If no clusters form, $M^{(p)}=0$. The distribution of these maximum statistics, $\\{M^{(p)}\\}_{p=1}^P$, serves as our empirical null distribution. Using the maximum statistic automatically corrects for searching across all possible clusters.\n    -   **Step 5d: Family-Wise Error (FWE) Corrected P-value**: The significance of an original, observed cluster $C$ is assessed by comparing its mass $S(C)$ to the null distribution of maximum masses. The FWE-corrected $p$-value is:\n        $$\n        p_{\\text{corr}}(C) = \\frac{1 + \\left|\\left\\{p : M^{(p)} \\ge S(C)\\right\\}\\right|}{P + 1}\n        $$\n    -   **Inference**: An observed cluster is declared statistically significant if its corrected $p$-value is below the desired family-wise error rate, $\\alpha$.\n\nThis comprehensive procedure, from data simulation to statistical inference, is implemented in the following Python code, adhering to the specified parameters for each test case.",
            "answer": "```python\nimport numpy as np\nfrom scipy import signal\nfrom scipy import stats\n\ndef find_clusters(grid):\n    \"\"\"\n    Finds 4-connectivity clusters in a 2D boolean grid using BFS.\n\n    Args:\n        grid (np.ndarray): A 2D boolean numpy array.\n\n    Returns:\n        list: A list of clusters, where each cluster is a list of (row, col) tuples.\n    \"\"\"\n    rows, cols = grid.shape\n    visited = np.zeros_like(grid, dtype=bool)\n    clusters = []\n    for r in range(rows):\n        for c in range(cols):\n            if grid[r, c] and not visited[r, c]:\n                # Start of a new cluster\n                cluster = []\n                q = [(r, c)]\n                visited[r, c] = True\n                \n                head = 0\n                while head < len(q):\n                    curr_r, curr_c = q[head]\n                    head += 1\n                    cluster.append((curr_r, curr_c))\n                    \n                    # Check 4 neighbors (up, down, left, right)\n                    for dr, dc in [(0, 1), (0, -1), (1, 0), (-1, 0)]:\n                        next_r, next_c = curr_r + dr, curr_c + dc\n                        \n                        if 0 <= next_r < rows and 0 <= next_c < cols:\n                            if grid[next_r, next_c] and not visited[next_r, next_c]:\n                                visited[next_r, next_c] = True\n                                q.append((next_r, next_c))\n                clusters.append(cluster)\n    return clusters\n\ndef run_analysis(N, A_gamma, P, alpha_cf, alpha, seed):\n    \"\"\"\n    Runs the full simulation and cluster-based permutation test for one test case.\n    \"\"\"\n    # 1. Set up RNG and constants\n    rng = np.random.default_rng(seed)\n    fs = 500\n    T = 2.0\n    t0 = 1.0\n    L = 256\n    R = 64\n    noverlap = L - R\n    \n    w_base = [0.6, 0.9]\n    w_post = [1.1, 1.5]\n    f_gamma_band = [30, 80]\n    f_gamma_burst = 40\n    t_burst_window = [1.15, 1.45]\n\n    # 2. Simulate data\n    t_vec = np.arange(0, T, 1 / fs)\n    n_samples = len(t_vec)\n    data = rng.standard_normal((N, n_samples))\n\n    if A_gamma > 0:\n        gamma_signal = np.sin(2 * np.pi * f_gamma_burst * t_vec)\n        burst_indices = (t_vec >= t_burst_window[0]) & (t_vec <= t_burst_window[1])\n        hanning_taper = np.zeros(n_samples)\n        hanning_taper[burst_indices] = np.hanning(np.sum(burst_indices))\n        gamma_burst = A_gamma * gamma_signal * hanning_taper\n        data += gamma_burst[np.newaxis, :]\n\n    # 3. Compute STFT and power for all trials\n    f_stft, t_stft, Zxx = signal.stft(data, fs=fs, window='hann', nperseg=L, noverlap=noverlap)\n    power_spectrograms = np.abs(Zxx)**2\n\n    # 4. Select frequency and time windows\n    gamma_freq_mask = (f_stft >= f_gamma_band[0]) & (f_stft <= f_gamma_band[1])\n    base_time_mask = (t_stft >= w_base[0]) & (t_stft <= w_base[1])\n    post_time_mask = (t_stft >= w_post[0]) & (t_stft <= w_post[1])\n    \n    # 5. Baseline Correction\n    power_gamma = power_spectrograms[:, gamma_freq_mask, :]\n    baseline_mean_power = np.mean(power_gamma[:, :, base_time_mask], axis=2, keepdims=True)\n    post_stim_power = power_gamma[:, :, post_time_mask]\n    \n    D = post_stim_power - baseline_mean_power\n\n    # 6. Compute observed t-statistics and find clusters\n    mean_D = np.mean(D, axis=0)\n    std_D = np.std(D, axis=0, ddof=1)\n    \n    # Handle division by zero\n    t_obs = np.zeros_like(mean_D)\n    non_zero_std = std_D > np.finfo(float).eps\n    t_obs[non_zero_std] = mean_D[non_zero_std] / (std_D[non_zero_std] / np.sqrt(N))\n    \n    t_crit = stats.t.ppf(1 - alpha_cf, df=N - 1)\n    t_map_thresh = t_obs > t_crit\n    \n    observed_clusters = find_clusters(t_map_thresh)\n    observed_masses = [np.sum(t_obs[tuple(zip(*c))]) for c in observed_clusters]\n\n    # 7. Permutation Test\n    max_perm_masses = np.zeros(P)\n    d_shape_for_perm = D.shape\n    \n    for i in range(P):\n        signs = rng.choice([-1, 1], size=(N, 1, 1))\n        D_perm = D * signs\n        \n        mean_D_perm = np.mean(D_perm, axis=0)\n        std_D_perm = np.std(D_perm, axis=0, ddof=1)\n        \n        t_perm = np.zeros_like(mean_D_perm)\n        non_zero_std_perm = std_D_perm > np.finfo(float).eps\n        t_perm[non_zero_std_perm] = mean_D_perm[non_zero_std_perm] / (std_D_perm[non_zero_std_perm] / np.sqrt(N))\n        \n        t_perm_thresh = t_perm > t_crit\n        perm_clusters = find_clusters(t_perm_thresh)\n        \n        if perm_clusters:\n            perm_masses = [np.sum(t_perm[tuple(zip(*c))]) for c in perm_clusters]\n            max_perm_masses[i] = np.max(perm_masses)\n\n    # 8. Compute FWE-corrected p-values\n    if not observed_clusters:\n        return [0, 1.0]\n\n    p_values = []\n    for mass in observed_masses:\n        p_val = (1 + np.sum(max_perm_masses >= mass)) / (P + 1)\n        p_values.append(p_val)\n    \n    p_values = np.array(p_values)\n    n_sig = np.sum(p_values < alpha)\n    p_min = np.min(p_values)\n\n    return [int(n_sig), p_min]\n\ndef solve():\n    # Test cases: (N, A_gamma, P, alpha_cf, alpha, seed)\n    test_cases = [\n        (60, 2.0, 500, 0.05, 0.05, 42),\n        (60, 0.0, 500, 0.05, 0.05, 43),\n        (20, 0.8, 300, 0.05, 0.05, 44),\n    ]\n\n    results = []\n    for case in test_cases:\n        n_sig, p_min = run_analysis(*case)\n        results.append([n_sig, p_min])\n    \n    # Format the output string as per requirements\n    formatted_results = [f\"[{n},{p}]\" for n, p in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}