{
    "hands_on_practices": [
        {
            "introduction": "Before we can correct for temporal noise in fMRI data, we must first understand how it arises. Physiological processes like respiration and heartbeats occur at frequencies much higher than the typical fMRI sampling rate, leading to a phenomenon known as aliasing. This exercise will guide you through the fundamental signal processing principles that determine how these high-frequency noise sources appear as lower-frequency artifacts in your data, a crucial first step in designing appropriate filters.",
            "id": "4197945",
            "problem": "Consider a functional Magnetic Resonance Imaging (fMRI) time series sampled with a repetition time (TR), where a continuous-time physiological process at frequency $f$ (for example, respiration) is sampled uniformly every $\\mathrm{TR}$ seconds. In the context of temporal filtering and prewhitening for fMRI data analysis, it is often necessary to predict how a continuous-time sinusoidal component at frequency $f$ appears in the sampled data to design filters that target aliased noise.\n\nStarting from the sampling of a continuous-time sinusoid $x(t) = \\cos(2\\pi f t)$ at sampling period $\\mathrm{TR}$, use the periodicity of discrete-time complex exponentials and the well-tested fact that sampling replicates the spectrum at integer multiples of the sampling frequency to derive a closed-form expression for the baseband alias of $f$ in the sampled discrete-time sequence. Your derivation must:\n- Begin from the definition of the sampled sequence $x[n] = x(n\\,\\mathrm{TR})$ and the periodicity of discrete-time frequency with fundamental period $2\\pi$.\n- Use the relationship between the sampling frequency $f_{s} = 1/\\mathrm{TR}$ and the spectral replicas introduced by sampling.\n- Show that there exists an integer $k \\in \\mathbb{Z}$ such that the aliased frequency lies in the Nyquist interval $[0, f_{s}/2]$ and provide an explicit analytic expression in terms of $f$, $\\mathrm{TR}$, and $k$ that satisfies this constraint.\n\nThen, apply your result to compute the aliased frequency when $f = 1\\ \\mathrm{Hz}$ and $\\mathrm{TR} = 0.7\\ \\mathrm{s}$. Express the final aliased frequency in Hz. Do not include units inside the boxed final answer.",
            "solution": "The problem requires the derivation of a closed-form expression for the aliased frequency of a continuous-time sinusoid sampled below its Nyquist rate, and then to apply this expression to a specific case relevant to fMRI data analysis. The problem is scientifically grounded, well-posed, and contains all necessary information for a unique solution.\n\nLet the continuous-time sinusoidal signal be denoted by $x(t)$, with frequency $f$:\n$$x(t) = \\cos(2\\pi f t)$$\nThis signal is sampled uniformly with a sampling period of $\\mathrm{TR}$ seconds. The resulting discrete-time sequence, $x[n]$, is obtained by evaluating $x(t)$ at time instances $t = n\\,\\mathrm{TR}$ for integer values of $n$:\n$$x[n] = x(n\\,\\mathrm{TR}) = \\cos(2\\pi f n\\,\\mathrm{TR})$$\nThis expression can be written in the canonical form for a discrete-time sinusoid, $x[n] = \\cos(\\omega n)$, where $\\omega$ is the discrete-time angular frequency. By comparison, we identify $\\omega$ as:\n$$\\omega = 2\\pi f \\,\\mathrm{TR}$$\n\nA fundamental property of discrete-time sinusoids is their periodicity in the frequency domain. Specifically, a discrete-time complex exponential $e^{j\\omega n}$ is periodic in $\\omega$ with period $2\\pi$. For a real-valued sinusoid, $\\cos(\\omega n) = \\frac{1}{2}(e^{j\\omega n} + e^{-j\\omega n})$, this periodicity implies that frequencies $\\omega$ and $\\omega'$ are indistinguishable if $\\omega' = \\pm \\omega + 2\\pi k$ for any integer $k \\in \\mathbb{Z}$. This is because:\n$$\\cos((\\pm \\omega + 2\\pi k)n) = \\cos(\\pm \\omega n + 2\\pi k n) = \\cos(\\pm \\omega n) = \\cos(\\omega n)$$\nwhere the last equality follows from the even property of the cosine function, $\\cos(-\\theta) = \\cos(\\theta)$.\n\nThis principle of frequency ambiguity in the discrete-time domain has a direct parallel in the continuous-time domain, known as aliasing. Let $f_a$ be an aliased frequency that produces the same sampled sequence as the original frequency $f$. The discrete-time angular frequency corresponding to $f_a$ is $\\omega_a = 2\\pi f_a \\mathrm{TR}$. For $f_a$ to be an alias of $f$, their corresponding discrete-time frequencies must be equivalent:\n$$\\omega_a = \\pm \\omega + 2\\pi k$$\nSubstituting the expressions for $\\omega_a$ and $\\omega$:\n$$2\\pi f_a \\mathrm{TR} = \\pm (2\\pi f \\mathrm{TR}) + 2\\pi k$$\nDividing by $2\\pi \\mathrm{TR}$:\n$$f_a = \\pm f + \\frac{k}{\\mathrm{TR}}$$\nUsing the definition of the sampling frequency, $f_s = 1/\\mathrm{TR}$, we find that the set of all frequencies that are indistinguishable from $f$ after sampling is given by:\n$$f' = \\pm f + k f_s, \\quad k \\in \\mathbb{Z}$$\nThis result formalizes the concept that sampling replicates the original signal's spectrum (with components at $\\pm f$) at integer multiples of the sampling frequency $f_s$.\n\nThe problem asks for the *baseband alias*, which is the unique frequency within this set that lies in the Nyquist interval $[0, f_s/2]$. Let this baseband aliased frequency be denoted $f_{alias}$. We must find an integer $k$ such that one of the frequencies $\\pm f + k f_s$ falls into this interval. A more direct way to conceptualize this is to find the frequency in $[0, f_s/2]$ that has the same \"distance\" to a multiple of the sampling frequency as the original frequency $f$. This is equivalent to finding the integer $k$ that minimizes the absolute difference $|f - k f_s|$. The value of $k$ that achieves this minimum is the integer closest to the ratio $f/f_s$:\n$$k = \\text{round}\\left(\\frac{f}{f_s}\\right) = \\text{round}(f \\cdot \\mathrm{TR})$$\nwhere $\\text{round}(\\cdot)$ denotes the function that rounds a real number to the nearest integer.\n\nWith this specific integer $k$, the aliased frequency $f_{alias}$ is given by the magnitude of the difference between $f$ and the nearest integer multiple of $f_s$:\n$$f_{alias} = |f - k f_s| = |f - \\text{round}(f \\cdot \\mathrm{TR}) \\cdot f_s|$$\nBy the definition of the rounding function, we have $|f/f_s - k| \\le 1/2$. Multiplying by $f_s$ confirms that our derived $f_{alias}$ is indeed within the required range:\n$$|f - k f_s| \\le \\frac{f_s}{2}$$\nSince $f_{alias}$ is an absolute value, it is non-negative. Thus, $0 \\le f_{alias} \\le f_s/2$, which is the Nyquist interval $[0, f_s/2]$. This is the required closed-form expression.\n\nNow, we apply this result to the given numerical values:\n- Original frequency: $f = 1\\ \\mathrm{Hz}$\n- Repetition time: $\\mathrm{TR} = 0.7\\ \\mathrm{s}$\n\nFirst, we calculate the sampling frequency $f_s$:\n$$f_s = \\frac{1}{\\mathrm{TR}} = \\frac{1}{0.7} = \\frac{10}{7}\\ \\mathrm{Hz}$$\nThe Nyquist frequency is $f_s/2 = (10/7)/2 = 5/7\\ \\mathrm{Hz}$. Since $f = 1\\ \\mathrm{Hz}  5/7\\ \\mathrm{Hz}$, the signal is sampled below its Nyquist rate and will be aliased.\n\nNext, we find the integer $k$ by calculating and rounding the product $f \\cdot \\mathrm{TR}$:\n$$f \\cdot \\mathrm{TR} = (1\\ \\mathrm{Hz}) \\cdot (0.7\\ \\mathrm{s}) = 0.7$$\n$$k = \\text{round}(0.7) = 1$$\nFinally, we compute the aliased frequency $f_{alias}$ using the derived formula:\n$$f_{alias} = |f - k f_s| = \\left|1 - 1 \\cdot \\frac{10}{7}\\right| = \\left|1 - \\frac{10}{7}\\right| = \\left|\\frac{7-10}{7}\\right| = \\left|-\\frac{3}{7}\\right| = \\frac{3}{7}$$\nThe aliased frequency is $\\frac{3}{7}\\ \\mathrm{Hz}$. We can verify that this result lies in the Nyquist interval $[0, 5/7]\\ \\mathrm{Hz}$, as $0 \\le 3/7 \\le 5/7$.",
            "answer": "$$\\boxed{\\frac{3}{7}}$$"
        },
        {
            "introduction": "Once we recognize the presence of structured noise, the next step in many analysis pipelines is to create a mathematical model of its temporal characteristics. This practice focuses on estimating the parameters of a first-order autoregressive ($\\text{AR}(1)$) model, a common choice for describing fMRI noise, directly from the data's residuals. By deriving the Yule-Walker estimator and applying it to a sample time series, you will gain hands-on experience with the core estimation step that precedes prewhitening.",
            "id": "4197953",
            "problem": "You are analyzing noise autocorrelation in a short calibration run of fMRI data acquired with a repetition time (TR) of $2$ seconds. After fitting a General Linear Model (GLM) with temporally filtered regressors, you obtain residuals $\\hat{\\varepsilon}_{t}$ for $t=1,\\dots,T$. For prewhitening, assume the GLM residuals follow a stationary first-order autoregressive model $\\mathrm{AR}(1)$ with zero mean,\n$$\n\\hat{\\varepsilon}_{t} = \\phi \\hat{\\varepsilon}_{t-1} + w_{t},\n$$\nwhere $w_{t}$ is zero-mean independent and identically distributed white noise with variance $\\sigma_{w}^{2}$ and $|\\phi|1$. Starting from the definition of the autocovariance function for a strictly stationary process,\n$$\n\\gamma(k) = \\mathbb{E}\\left[ \\hat{\\varepsilon}_{t}\\hat{\\varepsilon}_{t-k} \\right],\n$$\nderive an estimator for $\\phi$ by replacing population quantities with their sample analogs,\n$$\n\\hat{\\gamma}(k) = \\frac{1}{T} \\sum_{t=k+1}^{T} \\hat{\\varepsilon}_{t}\\hat{\\varepsilon}_{t-k},\n$$\nand show that it can be expressed in closed form using the residuals. Then, using the following mean-centered GLM residuals from a short run of length $T=8$,\n$$\n\\left\\{ \\hat{\\varepsilon}_{t} \\right\\}_{t=1}^{8} = \\{ 0.7,\\; 0.4,\\; 0.2,\\; -0.1,\\; -0.3,\\; -0.5,\\; -0.2,\\; -0.2 \\},\n$$\ncompute the numerical value of the estimator. Round your numerical answer to four significant figures. Finally, briefly justify within your derivation why short runs can induce bias in this estimator, and indicate the sign of the bias when $\\phi0$ under the given sample-autocovariance convention. Express the final answer as a single real number without units.",
            "solution": "The problem requires the derivation of an estimator for the first-order autoregressive parameter $\\phi$ from the residuals of a General Linear Model (GLM), its numerical computation for a given data set, and a justification for its bias in short time series.\n\nFirst, we begin with the provided model for the GLM residuals, a stationary first-order autoregressive process, $\\mathrm{AR}(1)$, with zero mean:\n$$\n\\hat{\\varepsilon}_{t} = \\phi \\hat{\\varepsilon}_{t-1} + w_{t}\n$$\nwhere $w_{t}$ is i.i.d. white noise with $\\mathbb{E}[w_t] = 0$ and variance $\\sigma_{w}^{2}$, and the stationarity condition requires $|\\phi|1$. The autocovariance function for this strictly stationary process is defined as $\\gamma(k) = \\mathbb{E}\\left[ \\hat{\\varepsilon}_{t}\\hat{\\varepsilon}_{t-k} \\right]$.\n\nTo find an expression for $\\phi$ in terms of population autocovariances, we compute $\\gamma(k)$ for $k=0$ and $k=1$.\nFor $k=0$, the autocovariance is the variance of the process:\n$$\n\\gamma(0) = \\mathbb{E}\\left[ \\hat{\\varepsilon}_{t}\\hat{\\varepsilon}_{t} \\right] = \\mathbb{E}\\left[ \\hat{\\varepsilon}_{t}^{2} \\right] = \\sigma_{\\varepsilon}^{2}\n$$\nFor $k=1$, the autocovariance is:\n$$\n\\gamma(1) = \\mathbb{E}\\left[ \\hat{\\varepsilon}_{t}\\hat{\\varepsilon}_{t-1} \\right]\n$$\nSubstitute the $\\mathrm{AR}(1)$ model definition into the expression for $\\gamma(1)$:\n$$\n\\gamma(1) = \\mathbb{E}\\left[ (\\phi \\hat{\\varepsilon}_{t-1} + w_{t}) \\hat{\\varepsilon}_{t-1} \\right]\n$$\nBy linearity of expectation, this becomes:\n$$\n\\gamma(1) = \\mathbb{E}\\left[ \\phi \\hat{\\varepsilon}_{t-1}^{2} \\right] + \\mathbb{E}\\left[ w_{t}\\hat{\\varepsilon}_{t-1} \\right] = \\phi \\mathbb{E}\\left[ \\hat{\\varepsilon}_{t-1}^{2} \\right] + \\mathbb{E}\\left[ w_{t}\\hat{\\varepsilon}_{t-1} \\right]\n$$\nDue to stationarity, the variance is constant over time, so $\\mathbb{E}\\left[ \\hat{\\varepsilon}_{t-1}^{2} \\right] = \\mathbb{E}\\left[ \\hat{\\varepsilon}_{t}^{2} \\right] = \\gamma(0)$. The noise term $w_{t}$ is, by definition, an innovation that is independent of all past values of the process, including $\\hat{\\varepsilon}_{t-1}$. Therefore, the expectation of their product is the product of their expectations:\n$$\n\\mathbb{E}\\left[ w_{t}\\hat{\\varepsilon}_{t-1} \\right] = \\mathbb{E}\\left[ w_{t} \\right] \\mathbb{E}\\left[ \\hat{\\varepsilon}_{t-1} \\right] = 0 \\times 0 = 0\n$$\nSubstituting these results back into the equation for $\\gamma(1)$ yields the Yule-Walker equation for an $\\mathrm{AR}(1)$ process:\n$$\n\\gamma(1) = \\phi \\gamma(0)\n$$\nFrom this, we can express the true parameter $\\phi$ as the ratio of the lag-$1$ autocovariance to the lag-$0$ autocovariance (the variance):\n$$\n\\phi = \\frac{\\gamma(1)}{\\gamma(0)}\n$$\nThe problem specifies that an estimator for $\\phi$, denoted $\\hat{\\phi}$, should be derived by replacing the population quantities $\\gamma(k)$ with their sample analogs $\\hat{\\gamma}(k)$. The sample autocovariance is given as:\n$$\n\\hat{\\gamma}(k) = \\frac{1}{T} \\sum_{t=k+1}^{T} \\hat{\\varepsilon}_{t}\\hat{\\varepsilon}_{t-k}\n$$\nUsing this definition, we construct the estimator $\\hat{\\phi}$:\n$$\n\\hat{\\phi} = \\frac{\\hat{\\gamma}(1)}{\\hat{\\gamma}(0)} = \\frac{\\frac{1}{T} \\sum_{t=2}^{T} \\hat{\\varepsilon}_{t}\\hat{\\varepsilon}_{t-1}}{\\frac{1}{T} \\sum_{t=1}^{T} \\hat{\\varepsilon}_{t}^{2}}\n$$\nThe factor of $\\frac{1}{T}$ cancels, leaving the closed-form estimator expressed using the residuals:\n$$\n\\hat{\\phi} = \\frac{\\sum_{t=2}^{T} \\hat{\\varepsilon}_{t}\\hat{\\varepsilon}_{t-1}}{\\sum_{t=1}^{T} \\hat{\\varepsilon}_{t}^{2}}\n$$\nThis is the ordinary least squares (OLS) estimator of the coefficient in a regression of $\\hat{\\varepsilon}_{t}$ on $\\hat{\\varepsilon}_{t-1}$.\n\nRegarding bias, this estimator is known to be biased in finite samples (i.e., for small $T$). The bias, $\\mathbb{E}[\\hat{\\phi}] - \\phi$, is of order $O(1/T)$ and thus diminishes as the length of the time series $T$ increases, making the estimator asymptotically unbiased (consistent). The bias arises because the regressor, $\\hat{\\varepsilon}_{t-1}$, is not strictly exogenous; it is correlated with the history of the error terms $\\{w_{t-1}, w_{t-2}, \\dots\\}$. This leads to a correlation between the numerator and denominator of the stochastic part of the estimator, $\\sum w_t \\hat{\\varepsilon}_{t-1}$ and $\\sum \\hat{\\varepsilon}_t^2$, respectively. For an $\\mathrm{AR}(1)$ process with $\\phi  0$, this bias is characteristically negative. That is, $\\mathbb{E}[\\hat{\\phi}]  \\phi$. This means that for short data runs, the estimator tends to underestimate the true magnitude of the temporal autocorrelation. This downward bias is a well-established result in time series analysis (e.g., Hurwicz bias).\n\nNow, we compute the numerical value of $\\hat{\\phi}$ for the given data set:\n$$\n\\left\\{ \\hat{\\varepsilon}_{t} \\right\\}_{t=1}^{8} = \\{ 0.7,\\; 0.4,\\; 0.2,\\; -0.1,\\; -0.3,\\; -0.5,\\; -0.2,\\; -0.2 \\}\n$$\nThe length of the run is $T=8$.\n\nFirst, we calculate the numerator, the sum of lagged products, $\\sum_{t=2}^{8} \\hat{\\varepsilon}_{t}\\hat{\\varepsilon}_{t-1}$:\n\\begin{align*}\n\\sum_{t=2}^{8} \\hat{\\varepsilon}_{t}\\hat{\\varepsilon}_{t-1} = (0.4)(0.7) + (0.2)(0.4) + (-0.1)(0.2) + (-0.3)(-0.1) + (-0.5)(-0.3) + (-0.2)(-0.5) + (-0.2)(-0.2) \\\\\n= 0.28 + 0.08 - 0.02 + 0.03 + 0.15 + 0.10 + 0.04 \\\\\n= 0.66\n\\end{align*}\n\nNext, we calculate the denominator, the sum of squares, $\\sum_{t=1}^{8} \\hat{\\varepsilon}_{t}^{2}$:\n\\begin{align*}\n\\sum_{t=1}^{8} \\hat{\\varepsilon}_{t}^{2} = (0.7)^{2} + (0.4)^{2} + (0.2)^{2} + (-0.1)^{2} + (-0.3)^{2} + (-0.5)^{2} + (-0.2)^{2} + (-0.2)^{2} \\\\\n= 0.49 + 0.16 + 0.04 + 0.01 + 0.09 + 0.25 + 0.04 + 0.04 \\\\\n= 1.12\n\\end{align*}\n\nFinally, we compute the estimator $\\hat{\\phi}$:\n$$\n\\hat{\\phi} = \\frac{0.66}{1.12} = \\frac{33}{56} \\approx 0.5892857...\n$$\nRounding the numerical value to four significant figures gives $0.5893$.",
            "answer": "$$\n\\boxed{0.5893}\n$$"
        },
        {
            "introduction": "Applying a prewhitening procedure is not the end of the story; we must always verify that the correction was successful and did not introduce new artifacts. This exercise introduces the Durbin-Watson statistic, a classic diagnostic tool for assessing whether the residuals of a model are truly \"white\" or if significant autocorrelation remains. By calculating the statistic for a specific scenario of imperfect prewhitening, you will learn how to quantitatively evaluate the outcome of your noise correction strategy.",
            "id": "4197966",
            "problem": "Consider a single-voxel time series from an fMRI experiment sampled every $2$ seconds across $8$ scans, modeled by a general linear model (GLM) with regressors capturing task-related effects and nuisance drift terms. Suppose the noise process is well-approximated as first-order autoregressive with true parameter $\\phi_{\\text{true}}$, but the prewhitening stage uses a parameter $\\phi_{\\text{pw}}$ that overestimates $\\phi_{\\text{true}}$, leading to residual anti-correlation after fitting the GLM.\n\nAfter prewhitening and estimation, the residuals for this voxel at times $t=1,\\dots,8$ are observed to be\n$$\ne_1=1,\\quad e_2=-1,\\quad e_3=1,\\quad e_4=-1,\\quad e_5=1,\\quad e_6=-1,\\quad e_7=1,\\quad e_8=-1.\n$$\n\nStarting from the definitions of the general linear model residuals and the sample lag-$1$ covariance structure implied by a first-order autoregression, derive the Durbin–Watson statistic as a function of the residuals $\\{e_t\\}$, and compute its value for the sequence above. Explain how values deviating from $2$ indicate residual autocorrelation and interpret the computed value in terms of remaining correlation after prewhitening. Report the Durbin–Watson statistic as an exact number with no rounding and no units.",
            "solution": "The problem asks for the derivation and computation of the Durbin–Watson statistic for a given sequence of residuals from a general linear model (GLM) applied to fMRI data, and an interpretation of the result in the context of prewhitening.\n\nFirst, we address the derivation and definition of the Durbin–Watson statistic, denoted by $d$. For a time series of residuals $\\{e_t\\}_{t=1}^T$, where $T$ is the number of observations, the statistic is defined as the ratio of the sum of squared differences of successive residuals to the sum of squared residuals:\n$$\nd = \\frac{\\sum_{t=2}^T (e_t - e_{t-1})^2}{\\sum_{t=1}^T e_t^2}\n$$\nThis is the function of the residuals $\\{e_t\\}$ requested by the problem.\n\nTo understand how this statistic relates to autocorrelation, we can expand the numerator:\n$$\n\\sum_{t=2}^T (e_t - e_{t-1})^2 = \\sum_{t=2}^T (e_t^2 - 2e_t e_{t-1} + e_{t-1}^2) = \\sum_{t=2}^T e_t^2 + \\sum_{t=2}^T e_{t-1}^2 - 2\\sum_{t=2}^T e_t e_{t-1}\n$$\nThe second term in the expansion is $\\sum_{t=2}^T e_{t-1}^2 = \\sum_{k=1}^{T-1} e_k^2$. For a sufficiently large sample size $T$, the following approximations hold: $\\sum_{t=2}^T e_t^2 \\approx \\sum_{t=1}^T e_t^2$ and $\\sum_{t=1}^{T-1} e_t^2 \\approx \\sum_{t=1}^T e_t^2$. Using these approximations, the numerator becomes approximately $2 \\sum_{t=1}^T e_t^2 - 2 \\sum_{t=2}^T e_t e_{t-1}$.\n\nSubstituting this approximation back into the definition of $d$:\n$$\nd \\approx \\frac{2 \\sum_{t=1}^T e_t^2 - 2 \\sum_{t=2}^T e_t e_{t-1}}{\\sum_{t=1}^T e_t^2} = 2 \\left( 1 - \\frac{\\sum_{t=2}^T e_t e_{t-1}}{\\sum_{t=1}^T e_t^2} \\right)\n$$\nThe sample lag-$1$ autocorrelation coefficient, $\\hat{\\rho}$, for a series with zero mean is estimated as $\\hat{\\rho} = \\frac{\\sum_{t=2}^T e_t e_{t-1}}{\\sum_{t=1}^T e_t^2}$ (assuming the variance is constant, which allows replacing the denominator $\\sqrt{(\\sum_{t=2}^T e_{t-1}^2)(\\sum_{t=2}^T e_t^2)}$ with $\\sum_{t=1}^T e_t^2$). Thus, we arrive at the well-known approximate relationship:\n$$\nd \\approx 2(1 - \\hat{\\rho})\n$$\n\nNext, we compute the value of $d$ for the given sequence of residuals. The number of scans is $T=8$, and the residuals are:\n$e_1=1$, $e_2=-1$, $e_3=1$, $e_4=-1$, $e_5=1$, $e_6=-1$, $e_7=1$, $e_8=-1$.\n\nFirst, we compute the denominator, the sum of squared residuals:\n$$\n\\sum_{t=1}^8 e_t^2 = (1)^2 + (-1)^2 + (1)^2 + (-1)^2 + (1)^2 + (-1)^2 + (1)^2 + (-1)^2 = 1+1+1+1+1+1+1+1 = 8\n$$\n\nNext, we compute the numerator, the sum of squared differences of successive residuals:\n\\begin{align*}\n\\sum_{t=2}^8 (e_t - e_{t-1})^2 = (e_2 - e_1)^2 + (e_3 - e_2)^2 + (e_4 - e_3)^2 + (e_5 - e_4)^2 + (e_6 - e_5)^2 + (e_7 - e_6)^2 + (e_8 - e_7)^2 \\\\\n= (-1 - 1)^2 + (1 - (-1))^2 + (-1 - 1)^2 + (1 - (-1))^2 + (-1 - 1)^2 + (1 - (-1))^2 + (-1 - 1)^2 \\\\\n= (-2)^2 + (2)^2 + (-2)^2 + (2)^2 + (-2)^2 + (2)^2 + (-2)^2 \\\\\n= 4 + 4 + 4 + 4 + 4 + 4 + 4 \\\\\n= 7 \\times 4 = 28\n\\end{align*}\n\nNow, we can compute the Durbin–Watson statistic:\n$$\nd = \\frac{28}{8} = \\frac{7}{2} = 3.5\n$$\n\nFinally, we interpret this result. The Durbin–Watson statistic $d$ ranges approximately from $0$ to $4$.\n- If there is no first-order autocorrelation, $\\hat{\\rho} \\approx 0$, and $d \\approx 2(1-0) = 2$. A value of $d$ near $2$ suggests the residuals are uncorrelated.\n- If there is perfect positive autocorrelation, $\\hat{\\rho} \\approx 1$, and $d \\approx 2(1-1) = 0$. A value of $d$ close to $0$ indicates strong positive autocorrelation.\n- If there is perfect negative autocorrelation (anti-correlation), $\\hat{\\rho} \\approx -1$, and $d \\approx 2(1 - (-1)) = 4$. A value of $d$ close to $4$ indicates strong negative autocorrelation.\n\nThe computed value is $d = 3.5$. This value is much closer to $4$ than to $2$, which indicates the presence of strong negative autocorrelation (anti-correlation) in the residuals. This finding is consistent with the problem's premise. The noise was modeled as an AR($1$) process with true parameter $\\phi_{\\text{true}}$. The prewhitening procedure, intended to remove this correlation, used a parameter $\\phi_{\\text{pw}}$ that was an overestimate, i.e., $\\phi_{\\text{pw}}  \\phi_{\\text{true}}  0$. This over-correction effectively \"subtracts too much\" of the previous time point's signal, inducing a new, artificial negative correlation in the transformed data. The observed residual sequence $e_t = (-1)^{t+1}$ is a textbook example of perfect anti-correlation, and the resulting Durbin–Watson statistic of $3.5$ provides a quantitative confirmation of this structure. The slight deviation from the theoretical maximum of $4$ is due to the small sample size ($T=8$) and the associated edge effects in the summation, which cause the approximation $d \\approx 2(1-\\hat{\\rho})$ to be inexact.",
            "answer": "$$\\boxed{\\frac{7}{2}}$$"
        }
    ]
}