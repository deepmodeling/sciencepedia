{
    "hands_on_practices": [
        {
            "introduction": "The foundation of any General Linear Model (GLM) analysis in fMRI is the accurate modeling of the expected Blood Oxygenation Level Dependent (BOLD) signal. This practice walks you through the core signal processing pipeline used to generate a single task regressor . By treating neural activity as a series of discrete events and modeling the brain's vascular response with a Hemodynamic Response Function (HRF), you will use convolution to create a high-fidelity prediction of the BOLD time course and then sample it to match the scanner's acquisition timing, translating a theoretical design into a concrete predictor.",
            "id": "4191942",
            "problem": "You are modeling one column of a functional Magnetic Resonance Imaging (fMRI) design matrix as the convolution between a discrete event stick function and a discrete Hemodynamic Response Function (HRF) sampled at microtime resolution. The fMRI modeling assumption is that the Blood Oxygenation Level Dependent (BOLD) signal is well-approximated by a Linear Time-Invariant (LTI) system, which implies that the system output to an input is given by convolution. The event stick function represents neural events as impulses at specified onsets, while the HRF characterizes the system’s impulse response. The goal is to construct a single column of the design matrix by computing this convolution at microtime resolution and downsampling to the repetition time sampling grid.\n\nStart from the following definitions and facts:\n- The system is Linear Time-Invariant (LTI), so the output for input $u[n]$ and impulse response $h[n]$ is the discrete convolution $y[n] = \\sum_{k=-\\infty}^{\\infty} u[k] \\, h[n-k]$, with appropriate zero padding for finite-length sequences.\n- Let the repetition time be $T_R$ in seconds, and the microtime factor be $m \\in \\mathbb{N}$, implying a microtime step $\\Delta t = T_R / m$ in seconds.\n- Let there be $N_{\\text{TR}}$ scans, so the total microtime length is $N_{\\mu} = N_{\\text{TR}} \\cdot m$ samples.\n- Event onsets are given in seconds relative to the start of the scan period, and the event stick function is defined on the microtime grid by placing an impulse of amplitude $1$ at index $n_e = \\lfloor t_e / \\Delta t \\rfloor$, where $t_e$ is an event onset time in seconds. Only indices satisfying $0 \\le n_e  N_{\\mu}$ are included; out-of-range events are ignored.\n- The HRF $h[n]$ is provided as a discrete sequence sampled at the same microtime step $\\Delta t$ and has finite length $L$. The microtime-domain response $y[n]$ is computed as the full discrete convolution of $u[n]$ with $h[n]$, followed by cropping to the first $N_{\\mu}$ samples: $y[0], y[1], \\dots, y[N_{\\mu}-1]$.\n- Downsampling to the repetition time grid yields the design column $x[j] = y[j \\cdot m]$ for $j = 0, 1, \\dots, N_{\\text{TR}} - 1$.\n\nImplement a program that, for each provided test case, constructs the event stick function $u[n]$, performs the discrete convolution with the provided HRF $h[n]$, crops the result to $N_{\\mu}$, and downsamples by selecting every $m$-th sample starting at index $0$ to form the design column $x[j]$. Use the mapping $n_e = \\lfloor t_e / \\Delta t \\rfloor$ for event onsets $t_e$. All time quantities must be handled in seconds.\n\nTest Suite:\nFor each test case, the parameters are $(T_R, m, N_{\\text{TR}}, \\text{events}, h)$ where $T_R$ is the repetition time in seconds, $m$ is the microtime factor (unitless integer), $N_{\\text{TR}}$ is the number of scans (unitless integer), events is a list of onset times in seconds, and $h$ is the discrete HRF sampled at microtime resolution (unitless amplitude). Use the following test cases:\n\n1. General case:\n   - $T_R = 2.0$\n   - $m = 5$\n   - $N_{\\text{TR}} = 10$\n   - $\\text{events} = [4.0, 12.0]$\n   - $h = [0.0, 0.005, 0.020, 0.050, 0.090, 0.140, 0.180, 0.220, 0.250, 0.270, 0.280, 0.280, 0.270, 0.250, 0.220, 0.190, 0.160, 0.130, 0.100, 0.080, 0.060, 0.045, 0.030, 0.020, 0.010, 0.005, 0.000, -0.005, -0.008, -0.010]$\n\n2. Boundary events near start and end:\n   - $T_R = 2.0$\n   - $m = 10$\n   - $N_{\\text{TR}} = 10$\n   - $\\text{events} = [0.0, 19.6]$\n   - $h = [0.0, 0.003, 0.012, 0.028, 0.050, 0.075, 0.100, 0.125, 0.145, 0.160, 0.170, 0.175, 0.176, 0.172, 0.165, 0.155, 0.140, 0.120, 0.100, 0.085, 0.070, 0.058, 0.047, 0.038, 0.030, 0.024, 0.019, 0.015, 0.012, 0.009, 0.007, 0.005, 0.003, 0.001, 0.000, -0.002, -0.004, -0.006, -0.008, -0.010]$\n\n3. No events:\n   - $T_R = 1.5$\n   - $m = 4$\n   - $N_{\\text{TR}} = 8$\n   - $\\text{events} = []$\n   - $h = [0.0, 0.010, 0.040, 0.090, 0.140, 0.180, 0.200, 0.210, 0.200, 0.180, 0.150, 0.120, 0.090, 0.070, 0.050, 0.035, 0.025, 0.015, 0.005, 0.000]$\n\n4. Multiple closely spaced events and one near the end:\n   - $T_R = 1.25$\n   - $m = 8$\n   - $N_{\\text{TR}} = 16$\n   - $\\text{events} = [3.10, 3.25, 3.30, 19.99]$\n   - $h = [0.0, 0.002, 0.008, 0.018, 0.034, 0.055, 0.082, 0.115, 0.150, 0.180, 0.205, 0.220, 0.228, 0.230, 0.225, 0.215, 0.200, 0.180, 0.160, 0.140, 0.120, 0.105, 0.090, 0.078, 0.067, 0.057, 0.048, 0.040, 0.033, 0.027, 0.022, 0.018, 0.014, 0.011, 0.009, 0.007, 0.005, 0.004, 0.003, 0.002, 0.001, 0.0005, 0.0000, -0.0010, -0.0020, -0.0030, -0.0040, -0.0050, -0.0060, -0.0070]$\n\nYour program must compute, for each test case, the downsampled design column $x[j]$ for $j = 0, 1, \\dots, N_{\\text{TR}} - 1$ and produce a single line of output containing a comma-separated list enclosed in square brackets, where each element corresponds to one test case’s result and is itself a bracketed comma-separated list of floats. Express all floats rounded to six decimal places, and do not include any spaces anywhere in the output. For example, an output with two test cases could look like:\n\"[[0.000000,0.123456],[0.000000,0.654321]]\".",
            "solution": "The problem requires the construction of a single column for a design matrix used in functional Magnetic Resonance Imaging (fMRI) analysis. This process is based on the principle that the measured Blood Oxygenation Level Dependent (BOLD) signal can be modeled as the output of a Linear Time-Invariant (LTI) system. The solution involves a sequence of signal processing steps: generating a neural event model, convolving it with a hemodynamic response function, and sampling the result at the scanner's acquisition times.\n\nThe process begins by defining the temporal grids. The experiment consists of $N_{\\text{TR}}$ scans, with each scan acquired every $T_R$ seconds. To model the biological response with greater fidelity than the scan resolution allows, a finer \"microtime\" grid is established. This grid has a temporal resolution of $\\Delta t = T_R / m$, where $m$ is the integer microtime factor. The total duration of the experiment in microtime samples is $N_{\\mu} = N_{\\text{TR}} \\cdot m$.\n\nThe input to the LTI system is an idealized representation of neural activity, termed the \"event stick function,\" denoted as $u[n]$. This is a discrete-time signal of length $N_{\\mu}$ on the microtime grid. For each stimulus or cognitive event occurring at a continuous time $t_e$ (in seconds), we place a discrete impulse of amplitude $1$ on the microtime grid. The corresponding discrete-time index $n_e$ is determined by quantizing the event time:\n$$n_e = \\lfloor t_e / \\Delta t \\rfloor$$\nThe floor function $\\lfloor \\cdot \\rfloor$ ensures that the continuous time $t_e$ is mapped to the start of the corresponding microtime bin. The event stick function is then constructed as a vector of zeros, except at these indices. If multiple events fall within the same microtime bin, their corresponding impulses summate, which is a direct consequence of the linearity assumption. Formally, for a set of event times $\\{t_{e_i}\\}$, the stick function is:\n$$u[n] = \\sum_i \\delta[n - \\lfloor t_{e_i} / \\Delta t \\rfloor]$$\nwhere $\\delta[k]$ is the Kronecker delta function. Events with onsets outside the scanning window (i.e., where $n_e  0$ or $n_e \\ge N_{\\mu}$) are disregarded.\n\nThe LTI system itself is characterized by its impulse response, the Hemodynamic Response Function (HRF), denoted as $h[n]$. The problem provides $h[n]$ as a finite-length discrete sequence of length $L$, sampled at the same microtime resolution $\\Delta t$.\n\nThe core of the LTI model is the convolution operation. The predicted BOLD signal at microtime resolution, $y[n]$, is the discrete convolution of the input $u[n]$ with the system's impulse response $h[n]$:\n$$y[n] = (u * h)[n] = \\sum_{k=-\\infty}^{\\infty} u[k] h[n-k]$$\nFor the finite-length sequences $u[n]$ (length $N_{\\mu}$) and $h[n]$ (length $L$), the \"full\" convolution results in a sequence of length $N_{\\mu} + L - 1$. This correctly models the response to events occurring late in the scan, where the HRF may extend beyond the final time point $N_{\\mu}-1$. However, since fMRI data is only collected up to this final time point, the problem specifies that the resulting convolved signal must be cropped to the first $N_{\\mu}$ samples. Let's call this cropped signal $y'[n]$, where $y'[n] = y[n]$ for $n \\in \\{0, 1, \\dots, N_{\\mu}-1\\}$.\n\nFinally, this high-resolution model of the BOLD signal must be sampled at the times corresponding to actual fMRI scanner acquisitions. Scans are acquired at intervals of $T_R$, corresponding to times $t_j = j \\cdot T_R$ for $j \\in \\{0, 1, \\dots, N_{\\text{TR}}-1\\}$. In the microtime domain, these acquisition times correspond to indices $j \\cdot m$. Therefore, the final design matrix column, $x[j]$, is obtained by downsampling the microtime signal $y'[n]$:\n$$x[j] = y'[j \\cdot m] \\quad \\text{for } j = 0, 1, \\dots, N_{\\text{TR}}-1$$\nThis procedure yields a vector $x$ of length $N_{\\text{TR}}$ that serves as a regressor in the General Linear Model (GLM) for explaining variance in the measured fMRI time series. The implementation will follow these steps precisely for each given test case, using numerical libraries to perform the convolution and array manipulations efficiently.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and formats the fMRI design matrix columns for all test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (\n            2.0, 5, 10, [4.0, 12.0],\n            [0.0, 0.005, 0.020, 0.050, 0.090, 0.140, 0.180, 0.220, 0.250, 0.270, 0.280, 0.280, 0.270, 0.250, 0.220, 0.190, 0.160, 0.130, 0.100, 0.080, 0.060, 0.045, 0.030, 0.020, 0.010, 0.005, 0.000, -0.005, -0.008, -0.010]\n        ),\n        (\n            2.0, 10, 10, [0.0, 19.6],\n            [0.0, 0.003, 0.012, 0.028, 0.050, 0.075, 0.100, 0.125, 0.145, 0.160, 0.170, 0.175, 0.176, 0.172, 0.165, 0.155, 0.140, 0.120, 0.100, 0.085, 0.070, 0.058, 0.047, 0.038, 0.030, 0.024, 0.019, 0.015, 0.012, 0.009, 0.007, 0.005, 0.003, 0.001, 0.000, -0.002, -0.004, -0.006, -0.008, -0.010]\n        ),\n        (\n            1.5, 4, 8, [],\n            [0.0, 0.010, 0.040, 0.090, 0.140, 0.180, 0.200, 0.210, 0.200, 0.180, 0.150, 0.120, 0.090, 0.070, 0.050, 0.035, 0.025, 0.015, 0.005, 0.000]\n        ),\n        (\n            1.25, 8, 16, [3.10, 3.25, 3.30, 19.99],\n            [0.0, 0.002, 0.008, 0.018, 0.034, 0.055, 0.082, 0.115, 0.150, 0.180, 0.205, 0.220, 0.228, 0.230, 0.225, 0.215, 0.200, 0.180, 0.160, 0.140, 0.120, 0.105, 0.090, 0.078, 0.067, 0.057, 0.048, 0.040, 0.033, 0.027, 0.022, 0.018, 0.014, 0.011, 0.009, 0.007, 0.005, 0.004, 0.003, 0.002, 0.001, 0.0005, 0.0000, -0.0010, -0.0020, -0.0030, -0.0040, -0.0050, -0.0060, -0.0070]\n        )\n    ]\n    \n    results = []\n    for case in test_cases:\n        T_R, m, N_TR, events, h = case\n\n        # Calculate microtime parameters\n        dt = T_R / m\n        N_micro = N_TR * m\n\n        # Create the event stick function u[n]\n        u = np.zeros(N_micro)\n        for t_e in events:\n            n_e = int(np.floor(t_e / dt))\n            if 0 = n_e  N_micro:\n                u[n_e] += 1\n        \n        # Ensure h is a numpy array\n        h_np = np.array(h)\n        \n        # Compute the full discrete convolution\n        y_full = np.convolve(u, h_np, mode='full')\n        \n        # Crop the result to the first N_micro samples\n        y = y_full[:N_micro]\n        \n        # Downsample to the repetition time grid\n        x = y[::m]\n        \n        # Format the result as a string\n        formatted_list = [f\"{val:.6f}\" for val in x]\n        result_str = \"[\" + \",\".join(formatted_list) + \"]\"\n        results.append(result_str)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "After generating individual regressors for task conditions and nuisance variables, you assemble them into the design matrix, $X$. The mathematical integrity of your GLM analysis depends critically on this matrix being \"full rank,\" meaning its columns are linearly independent. This practice challenges you to assess the rank of a typical design matrix and, more importantly, to reason about the common experimental design flaws that lead to collinearity, a state where the model becomes unsolvable and parameter estimates are not uniquely defined .",
            "id": "4191926",
            "problem": "An investigator is constructing a General Linear Model (GLM) for functional Magnetic Resonance Imaging (fMRI) analysis. The single-run experiment consists of a time series of length $N$ sampled every $T_R$ seconds, with $N = 300$ and $T_R = 2\\,\\mathrm{s}$, so the total scan duration is $600\\,\\mathrm{s}$. The design matrix has the following columns:\n\n- An intercept column, i.e., a constant column of ones.\n- Two task regressors for conditions $\\mathcal{A}$ and $\\mathcal{B}$ constructed by convolving their binary stimulus time series (each a boxcar of $2\\,\\mathrm{s}$ duration on each event) with a fixed, known Hemodynamic Response Function (HRF). There are $20$ events for $\\mathcal{A}$ and $20$ events for $\\mathcal{B}$, and their onsets are nonoverlapping; at many time points neither condition is active (i.e., there are true rest periods).\n- Six head-motion nuisance regressors corresponding to translations and rotations measured at each $T_R$.\n\nAssume the following empirically realistic conditions hold:\n\n1. The motion regressors are not exactly constant and are not exact linear combinations of each other.\n2. None of the motion regressors is an exact linear combination of the task regressors or the intercept.\n3. The HRF is fixed and linear time-invariant, and the convolution is implemented exactly (no aliasing or numerical approximation that would induce exact degeneracies).\n4. The stimulus time series for $\\mathcal{A}$ and $\\mathcal{B}$ are not scalar multiples of one another.\n\nStart from the fundamental definition of matrix rank as the dimension of its column space, and the definition of linear independence of vectors. Using only these definitions and the linearity of convolution for linear time-invariant systems, reason about the presence or absence of exact linear dependencies among the columns of the design matrix under the stated assumptions. In your reasoning, explicitly articulate at least two concrete modifications to the described design that would induce exact rank deficiency, even if they do not apply under the current assumptions.\n\nCompute the rank of the design matrix for this run under the conditions above. Report only the rank as your final answer. The rank is an integer and is unitless; no rounding is required.",
            "solution": "The problem requires us to determine the rank of a specified design matrix for a functional Magnetic Resonance Imaging (fMRI) experiment. The rank of a matrix is defined as the dimension of its column space, which is equivalent to the maximum number of linearly independent columns in the matrix.\n\nFirst, let us define the design matrix, denoted as $X$. It is an $N \\times P$ matrix, where $N$ is the number of time points (scans) and $P$ is the number of regressors (columns).\nFrom the problem statement, we have:\n- $N = 300$ time points.\n- The number of regressors $P$ is the sum of an intercept, two task regressors, and six motion regressors.\n- $P = 1 (\\text{intercept}) + 2 (\\text{task}) + 6 (\\text{motion}) = 9$.\nThus, the design matrix $X$ has dimensions $300 \\times 9$. The maximum possible rank for $X$ is $\\min(300, 9) = 9$. The rank will be exactly $9$ if and only if all $9$ of its columns are linearly independent.\n\nLet the columns of the matrix $X$ be denoted as follows:\n- $\\mathbf{c}_0$: the intercept column, a vector of ones of length $300$.\n- $\\mathbf{r_A}$ and $\\mathbf{r_B}$: the two task regressors for conditions $\\mathcal{A}$ and $\\mathcal{B}$, respectively.\n- $\\mathbf{m}_1, \\mathbf{m}_2, \\dots, \\mathbf{m}_6$: the six head-motion nuisance regressors.\n\nA set of vectors $\\{\\mathbf{v}_1, \\dots, \\mathbf{v}_P\\}$ is linearly independent if the only solution to the equation $k_1\\mathbf{v}_1 + k_2\\mathbf{v}_2 + \\dots + k_P\\mathbf{v}_P = \\mathbf{0}$ (where $\\mathbf{0}$ is the zero vector) is the trivial solution $k_1 = k_2 = \\dots = k_P = 0$. We must assess if the set of $9$ column vectors $\\{\\mathbf{c}_0, \\mathbf{r_A}, \\mathbf{r_B}, \\mathbf{m}_1, \\dots, \\mathbf{m}_6\\}$ is linearly independent. We will use the provided assumptions to reason about this.\n\n1.  **Linear Independence of the Motion Regressors:**\n    Assumption (1) states that the motion regressors are not exact linear combinations of each other. In the context of a set of vectors, this is the definition of linear independence. Thus, the set of six motion regressors $\\{\\mathbf{m}_1, \\dots, \\mathbf{m}_6\\}$ is linearly independent.\n\n2.  **Linear Independence of the Task Regressors and the Intercept:**\n    Let us consider the set $\\{\\mathbf{c}_0, \\mathbf{r_A}, \\mathbf{r_B}\\}$. For these to be linearly independent, the equation $k_0\\mathbf{c}_0 + k_A\\mathbf{r_A} + k_B\\mathbf{r_B} = \\mathbf{0}$ must only have the solution $k_0=k_A=k_B=0$.\n    The task regressors $\\mathbf{r_A}$ and $\\mathbf{r_B}$ are formed by convolving binary stimulus time series, let's call them $S_A$ and $S_B$, with a Hemodynamic Response Function (HRF), $h(t)$. The problem states that there are true rest periods, meaning there are time intervals where neither stimulus $\\mathcal{A}$ nor $\\mathcal{B}$ is active. During a sufficiently long rest period, the HRF will have decayed back to zero. At such a time point $t_j$, the values of the task regressors will be $\\mathbf{r_A}(t_j) \\approx 0$ and $\\mathbf{r_B}(t_j) \\approx 0$. The intercept, however, is constant: $\\mathbf{c}_0(t_j) = 1$. The linear dependency equation at this time point becomes $k_0(1) + k_A(0) + k_B(0) = 0$, which implies $k_0=0$.\n    With $k_0=0$, the equation simplifies to $k_A\\mathbf{r_A} + k_B\\mathbf{r_B} = \\mathbf{0}$.\n    Convolution is a linear operation. Therefore, $k_A(S_A * h) + k_B(S_B * h) = (k_A S_A + k_B S_B) * h = \\mathbf{0}$. Since the HRF $h(t)$ is a fixed, known function (and implicitly not identically zero), for the output of the convolution to be zero, the input must be zero. Thus, $k_A S_A + k_B S_B = \\mathbf{0}$.\n    The problem states that the stimulus onsets for $\\mathcal{A}$ and $\\mathcal{B}$ are non-overlapping. This means there exists at least one time point $t_a$ where $S_A(t_a) = 1$ and $S_B(t_a) = 0$. At this point, the equation becomes $k_A(1) + k_B(0) = 0$, which implies $k_A=0$. Similarly, there must be a time point $t_b$ where $S_A(t_b) = 0$ and $S_B(t_b) = 1$, which implies $k_B=0$.\n    Thus, we have shown $k_0=k_A=k_B=0$, which proves that the set $\\{\\mathbf{c}_0, \\mathbf{r_A}, \\mathbf{r_B}\\}$ is linearly independent. This conclusion also relies on Assumption (4), which guarantees $S_A$ and $S_B$ are not multiples of one another. Our reasoning with non-overlapping events is a specific case that ensures a stronger form of independence.\n\n3.  **Linear Independence of the Full Set of Regressors:**\n    We have established that the subset of task-related regressors $\\{\\mathbf{c}_0, \\mathbf{r_A}, \\mathbf{r_B}\\}$ is linearly independent, and the subset of motion regressors $\\{\\mathbf{m}_1, \\dots, \\mathbf{m}_6\\}$ is also linearly independent. We now must consider the entire set of $9$ regressors.\n    A linear dependency would exist if a vector in the column space of the motion regressors could be perfectly represented by a vector in the column space of the task/intercept regressors.\n    Assumption (2) states that none of the individual motion regressors is an exact linear combination of the task regressors or the intercept. While this does not mathematically forbid a linear combination of motion regressors from being a linear combination of task regressors, the problem is framed under \"empirically realistic conditions\". The task regressors are derived from a deterministic, pre-specified experimental design. The motion regressors are derived from a stochastic, biological process (subject head movement). In any realistic scenario, these two sets of time series, originating from physically and causally independent sources, will not exhibit an exact linear dependency over $N=300$ samples. The given assumptions are designed to mathematically rule out the most common forms of \"designed\" collinearity. The conclusion drawn from this physical reasoning is that there is no linear dependence between the subspace spanned by $\\{\\mathbf{c}_0, \\mathbf{r_A}, \\mathbf{r_B}\\}$ and the subspace spanned by $\\{\\mathbf{m}_1, \\dots, \\mathbf{m}_6\\}$, other than the zero vector.\n    Therefore, the full set of $9$ columns is linearly independent.\n\nSince the $300 \\times 9$ design matrix $X$ has $9$ linearly independent columns, the dimension of its column space is $9$.\n\nAs part of the reasoning, we must articulate two modifications that would induce rank deficiency (i.e., exact collinearity).\n1.  **Introducing collinearity between task regressors:** If the experimental design were changed such that condition $\\mathcal{A}$ and condition $\\mathcal{B}$ always occurred at the exact same times, their stimulus time series would be identical, $S_A = S_B$. Due to the linearity of convolution, their corresponding regressors would also be identical, $\\mathbf{r_A} = \\mathbf{r_B}$. This would introduce a linear dependency, as $1 \\cdot \\mathbf{r_A} - 1 \\cdot \\mathbf{r_B} = \\mathbf{0}$. The rank of the design matrix would drop to at most $8$. This modification violates the premise of non-overlapping onsets and Assumption (4).\n2.  **Introducing collinearity with the intercept:** If one of the motion measurement devices failed and output a constant value for the entire duration of the scan, the corresponding regressor, say $\\mathbf{m}_1$, would be a constant vector. For example, $\\mathbf{m}_1 = k \\cdot \\mathbf{1}$ for some constant $k$. The intercept column is $\\mathbf{c}_0 = \\mathbf{1}$. This would create the linear dependency $k \\cdot \\mathbf{c}_0 - 1 \\cdot \\mathbf{m}_1 = \\mathbf{0}$. The matrix would become rank-deficient, with a rank of at most $8$. This modification violates Assumption (1), which states that motion regressors are not exactly constant.\n\nBased on the provided assumptions and reasoning from the definitions of rank and linear independence, the $9$ columns of the specified design matrix are linearly independent. The rank of the matrix is therefore $9$.",
            "answer": "$$\n\\boxed{9}\n$$"
        },
        {
            "introduction": "A key strength of the GLM framework is its ability to statistically control for sources of variance not related to your hypothesis. This advanced practice demonstrates how failing to model known sources of variability, such as mean signal shifts between scanning runs, can systematically bias your results . By deriving the parameter estimates from first principles in models with and without nuisance regressors, you will gain a deep understanding of omitted variable bias and appreciate why a well-specified design matrix is essential for drawing accurate and robust scientific conclusions.",
            "id": "4192019",
            "problem": "An investigator runs a two-session functional Magnetic Resonance Imaging (fMRI) experiment to estimate a condition effect while controlling for differences in session means. The observed blood-oxygen-level dependent time series $y \\in \\mathbb{R}^{N}$ is modeled at the sample level by a General Linear Model (GLM), with scans concatenated across runs. Assume the following data-generating process holds at the level of the mean for each scan:\n$$\n\\mathbb{E}[y] \\;=\\; \\alpha_{1}\\, r_{1} \\;+\\; \\alpha_{2}\\, r_{2} \\;+\\; \\beta\\, s,\n$$\nwhere $r_{1}, r_{2} \\in \\mathbb{R}^{N}$ are run indicators for run $1$ and run $2$ (so that $r_{1,i} = 1$ if scan $i$ belongs to run $1$ and $0$ otherwise, and analogously for $r_{2}$), $\\alpha_{1}, \\alpha_{2} \\in \\mathbb{R}$ are run-specific mean levels (session means), $\\beta \\in \\mathbb{R}$ is the true condition effect of interest, and $s \\in \\mathbb{R}^{N}$ is a condition regressor already incorporating the canonical Hemodynamic Response Function (HRF) and scaled such that $s_{i} \\in \\{+1,-1\\}$ denotes condition $A$ ($+1$) versus condition $B$ ($-1$) at scan $i$. Noise is zero-mean and otherwise unspecified.\n\nThe experiment has $N_{1} = 60$ scans in run $1$ and $N_{2} = 40$ scans in run $2$ (so $N = N_{1} + N_{2} = 100$). In run $1$, there are $N_{A1} = 40$ scans labeled $A$ and $N_{B1} = 20$ scans labeled $B$; in run $2$, there are $N_{A2} = 10$ scans labeled $A$ and $N_{B2} = 30$ scans labeled $B$. Hence, across all runs, there are $N_{A} = 50$ scans of $A$ and $N_{B} = 50$ scans of $B$, so $\\sum_{i=1}^{N} s_{i} = 0$.\n\nTwo GLM specifications are considered for Ordinary Least Squares (OLS) estimation:\n- Model $\\mathcal{M}_{0}$ (no run-specific intercepts): design $X_{0} = [\\mathbf{1}_{N},\\, s]$, where $\\mathbf{1}_{N}$ is the $N$-vector of ones.\n- Model $\\mathcal{M}_{1}$ (with run-specific intercepts): design $X_{1} = [r_{1},\\, r_{2},\\, s]$.\n\nStarting only from the GLM definition $y = X\\theta + \\varepsilon$ with $\\mathbb{E}[\\varepsilon] = 0$ and the OLS estimator $\\hat{\\theta} = (X^{\\top}X)^{-1}X^{\\top}y$, and without invoking any shortcut formulas, derive the expected value of the estimated condition effect under each model, $\\mathbb{E}[\\hat{\\beta}_{0}]$ for $\\mathcal{M}_{0}$ and $\\mathbb{E}[\\hat{\\beta}_{1}]$ for $\\mathcal{M}_{1}$. Use these derivations to explain, from first principles, why including run-specific intercepts controls for session mean differences for the within-run contrast of condition $A$ versus $B$, and to characterize how across-run contrasts (run $1$ versus run $2$) are represented in $\\mathcal{M}_{1}$ but not in $\\mathcal{M}_{0}$.\n\nFinally, compute the bias term in the condition-effect estimate when run-specific intercepts are omitted, defined as\n$$\n\\text{Bias} \\;=\\; \\mathbb{E}[\\hat{\\beta}_{0}] \\;-\\; \\beta,\n$$\nfor the specified counts and coding. Express your final answer as a single closed-form analytic expression in $\\alpha_{1}$ and $\\alpha_{2}$. Do not include units.",
            "solution": "The analysis proceeds from the definition of the Ordinary Least Squares (OLS) estimator $\\hat{\\theta} = (X^{\\top}X)^{-1}X^{\\top}y$ and its expected value, given the true data-generating process for the mean signal $\\mathbb{E}[y]$. The expected value of the estimator is $\\mathbb{E}[\\hat{\\theta}] = (X^{\\top}X)^{-1}X^{\\top}\\mathbb{E}[y]$.\n\nThe true mean signal is given by:\n$$\n\\mathbb{E}[y] \\;=\\; \\alpha_{1}\\, r_{1} \\;+\\; \\alpha_{2}\\, r_{2} \\;+\\; \\beta\\, s\n$$\nThe provided numerical values are:\n- Run $1$: $N_{1} = 60$ scans, with $N_{A1} = 40$ (condition $A$, $s_i=+1$) and $N_{B1} = 20$ (condition $B$, $s_i=-1$).\n- Run $2$: $N_{2} = 40$ scans, with $N_{A2} = 10$ (condition $A$, $s_i=+1$) and $N_{B2} = 30$ (condition $B$, $s_i=-1$).\n- Total scans: $N = N_{1} + N_{2} = 100$.\n\nFrom these counts, we can compute the necessary inner products of the regressors:\n- $r_{1}^{\\top}r_{1} = N_{1} = 60$\n- $r_{2}^{\\top}r_{2} = N_{2} = 40$\n- $r_{1}^{\\top}r_{2} = 0$, as the runs are distinct.\n- $\\mathbf{1}_{N}^{\\top}\\mathbf{1}_{N} = N = 100$, where $\\mathbf{1}_{N} = r_{1} + r_{2}$.\n- $s^{\\top}s = \\sum_{i=1}^{N} s_{i}^{2} = \\sum_{i=1}^{N} (\\pm 1)^{2} = N = 100$.\n- $s^{\\top}\\mathbf{1}_{N} = \\sum_{i=1}^{N} s_{i} = (N_{A1}+N_{A2}) - (N_{B1}+N_{B2}) = (40+10) - (20+30) = 50 - 50 = 0$.\n- $s^{\\top}r_{1} = \\sum_{i \\in \\text{run } 1} s_{i} = N_{A1} - N_{B1} = 40 - 20 = 20$.\n- $s^{\\top}r_{2} = \\sum_{i \\in \\text{run } 2} s_{i} = N_{A2} - N_{B2} = 10 - 30 = -20$.\n\n### Analysis of Model $\\mathcal{M}_{0}$ (no run-specific intercepts)\n\nFor model $\\mathcal{M}_{0}$, the design matrix is $X_{0} = [\\mathbf{1}_{N}, s]$ and the parameter vector is $\\theta_{0} = [\\alpha_{0}, \\beta_{0}]^{\\top}$.\n\nFirst, we compute $X_{0}^{\\top}X_{0}$:\n$$\nX_{0}^{\\top}X_{0} = \\begin{pmatrix} \\mathbf{1}_{N}^{\\top}\\mathbf{1}_{N}  \\mathbf{1}_{N}^{\\top}s \\\\ s^{\\top}\\mathbf{1}_{N}  s^{\\top}s \\end{pmatrix} = \\begin{pmatrix} 100  0 \\\\ 0  100 \\end{pmatrix}\n$$\nThe matrix is diagonal because the regressor $s$ is orthogonal to the intercept $\\mathbf{1}_{N}$ ($s^{\\top}\\mathbf{1}_{N} = 0$).\nThe inverse is:\n$$\n(X_{0}^{\\top}X_{0})^{-1} = \\begin{pmatrix} 1/100  0 \\\\ 0  1/100 \\end{pmatrix}\n$$\nNext, we compute $X_{0}^{\\top}\\mathbb{E}[y]$:\n$$\nX_{0}^{\\top}\\mathbb{E}[y] = \\begin{pmatrix} \\mathbf{1}_{N}^{\\top} \\\\ s^{\\top} \\end{pmatrix} (\\alpha_{1}r_{1} + \\alpha_{2}r_{2} + \\beta s) = \\begin{pmatrix} \\alpha_{1}\\mathbf{1}_{N}^{\\top}r_{1} + \\alpha_{2}\\mathbf{1}_{N}^{\\top}r_{2} + \\beta\\mathbf{1}_{N}^{\\top}s \\\\ \\alpha_{1}s^{\\top}r_{1} + \\alpha_{2}s^{\\top}r_{2} + \\beta s^{\\top}s \\end{pmatrix}\n$$\nSubstituting the inner product values:\n$$\nX_{0}^{\\top}\\mathbb{E}[y] = \\begin{pmatrix} \\alpha_{1}N_{1} + \\alpha_{2}N_{2} + \\beta(0) \\\\ \\alpha_{1}(20) + \\alpha_{2}(-20) + \\beta(100) \\end{pmatrix} = \\begin{pmatrix} 60\\alpha_{1} + 40\\alpha_{2} \\\\ 20\\alpha_{1} - 20\\alpha_{2} + 100\\beta \\end{pmatrix}\n$$\nThe expected value of the parameter estimates $\\hat{\\theta}_{0} = [\\hat{\\alpha}_{0}, \\hat{\\beta}_{0}]^{\\top}$ is:\n$$\n\\mathbb{E}[\\hat{\\theta}_{0}] = (X_{0}^{\\top}X_{0})^{-1} X_{0}^{\\top}\\mathbb{E}[y] = \\begin{pmatrix} 1/100  0 \\\\ 0  1/100 \\end{pmatrix} \\begin{pmatrix} 60\\alpha_{1} + 40\\alpha_{2} \\\\ 20\\alpha_{1} - 20\\alpha_{2} + 100\\beta \\end{pmatrix} = \\begin{pmatrix} \\frac{60\\alpha_{1} + 40\\alpha_{2}}{100} \\\\ \\frac{20\\alpha_{1} - 20\\alpha_{2} + 100\\beta}{100} \\end{pmatrix}\n$$\nThe expected value of the estimated condition effect $\\hat{\\beta}_{0}$ is the second element:\n$$\n\\mathbb{E}[\\hat{\\beta}_{0}] = \\frac{20\\alpha_{1} - 20\\alpha_{2} + 100\\beta}{100} = \\beta + \\frac{20}{100}(\\alpha_{1} - \\alpha_{2}) = \\beta + \\frac{1}{5}(\\alpha_{1} - \\alpha_{2})\n$$\n\n### Analysis of Model $\\mathcal{M}_{1}$ (with run-specific intercepts)\n\nFor model $\\mathcal{M}_{1}$, the design matrix is $X_{1} = [r_{1}, r_{2}, s]$ and the parameter vector to be estimated is $\\theta_{1} = [\\alpha'_{1}, \\alpha'_{2}, \\beta_{1}]^{\\top}$.\n\nThe true mean signal $\\mathbb{E}[y] = \\alpha_{1}r_{1} + \\alpha_{2}r_{2} + \\beta s$ can be written as $\\mathbb{E}[y] = X_{1}\\theta_{\\text{true}}$, where $\\theta_{\\text{true}} = [\\alpha_{1}, \\alpha_{2}, \\beta]^{\\top}$. This means that the true data-generating model for the mean is perfectly specified by the design matrix $X_{1}$. In this situation, the OLS estimator is unbiased for the true parameters. We can prove this from first principles:\n$$\n\\mathbb{E}[\\hat{\\theta}_{1}] = (X_{1}^{\\top}X_{1})^{-1}X_{1}^{\\top}\\mathbb{E}[y] = (X_{1}^{\\top}X_{1})^{-1}X_{1}^{\\top}(X_{1}\\theta_{\\text{true}}) = [(X_{1}^{\\top}X_{1})^{-1}(X_{1}^{\\top}X_{1})]\\theta_{\\text{true}} = I\\theta_{\\text{true}} = \\theta_{\\text{true}}\n$$\nThis requires that $X_{1}^{\\top}X_{1}$ is invertible, which is true as the columns of $X_{1}$ are linearly independent.\nTherefore, the expected values of the parameter estimates are equal to the true parameters:\n$$\n\\mathbb{E}[\\hat{\\theta}_{1}] = \\begin{pmatrix} \\mathbb{E}[\\hat{\\alpha}'_{1}] \\\\ \\mathbb{E}[\\hat{\\alpha}'_{2}] \\\\ \\mathbb{E}[\\hat{\\beta}_{1}] \\end{pmatrix} = \\begin{pmatrix} \\alpha_{1} \\\\ \\alpha_{2} \\\\ \\beta \\end{pmatrix}\n$$\nFrom this, the expected value of the estimated condition effect is:\n$$\n\\mathbb{E}[\\hat{\\beta}_{1}] = \\beta\n$$\n\n### Explanation of Results\n\nThe derivations show that $\\mathbb{E}[\\hat{\\beta}_{1}] = \\beta$ while $\\mathbb{E}[\\hat{\\beta}_{0}] = \\beta + \\frac{1}{5}(\\alpha_{1} - \\alpha_{2})$.\n\n**Controlling for Session Means**: Model $\\mathcal{M}_{1}$ is said to \"control for\" session mean differences because its estimate of the condition effect, $\\hat{\\beta}_{1}$, is unbiased with respect to the true effect $\\beta$, regardless of the values of the run-specific means $\\alpha_{1}$ and $\\alpha_{2}$. This occurs because the GLM estimation procedure for $\\hat{\\beta}_{1}$ effectively considers only the portion of the task regressor $s$ that is orthogonal to the space spanned by the nuisance regressors ($r_{1}$ and $r_{2}$). This orthogonalization process, described by the Frisch-Waugh-Lovell theorem, is equivalent to de-meaning the regressor $s$ within each run. Since the resulting regressor has zero mean within each run, its correlation with the run-specific mean levels is mathematically zero, and thus the estimate for its coefficient ($\\beta$) is not contaminated by these means.\n\nIn contrast, model $\\mathcal{M}_{0}$ fails to control for session means. The estimator $\\hat{\\beta}_{0}$ is biased by the term $\\frac{1}{5}(\\alpha_{1} - \\alpha_{2})$. This is a classic case of omitted variable bias. The true model contains separate run means ($\\alpha_{1}, \\alpha_{2}$), but $\\mathcal{M}_{0}$ omits this structure, forcing any true difference in run means to be absorbed by the included regressors. The task regressor $s$ is not orthogonal to the difference between runs (since $s^{\\top}r_{1} \\neq 0$ and $s^{\\top}r_{2} \\neq 0$), so a portion of the effect of $(\\alpha_{1} - \\alpha_{2})$ is incorrectly attributed to $s$, creating bias. The bias is zero only if $\\alpha_{1} = \\alpha_{2}$ or if the task is balanced within each run, making $s$ orthogonal to $r_1$ and $r_2$.\n\n**Representation of Across-Run Contrasts**: In $\\mathcal{M}_{1}$, the parameters $\\hat{\\alpha}'_{1}$ and $\\hat{\\alpha}'_{2}$ are direct estimates of the mean BOLD signal for run $1$ and run $2$, respectively (after accounting for task effects). An across-run contrast, such as the difference in session means, can be formally tested using a contrast vector like $c=[1, -1, 0]^{\\top}$ to evaluate the hypothesis $\\alpha'_{1} - \\alpha'_{2} = 0$. Thus, $\\mathcal{M}_{1}$ explicitly models and allows for the statistical inference of across-run effects. In $\\mathcal{M}_{0}$, no such parameters exist. There is only a single intercept $\\hat{\\alpha}_{0}$ representing the average signal over all scans. It is impossible to formulate a contrast to test for differences between runs, as this information has been collapsed and is confounded with the estimate of $\\beta$.\n\n### Bias Calculation\n\nThe bias in the condition-effect estimate when run-specific intercepts are omitted is defined as $\\text{Bias} = \\mathbb{E}[\\hat{\\beta}_{0}] - \\beta$.\nUsing our derived expression for $\\mathbb{E}[\\hat{\\beta}_{0}]$:\n$$\n\\text{Bias} = \\left( \\beta + \\frac{1}{5}(\\alpha_{1} - \\alpha_{2}) \\right) - \\beta = \\frac{1}{5}(\\alpha_{1} - \\alpha_{2})\n$$\nThis expression quantifies the systematic error in the estimated condition effect as a function of the true, unobserved difference in session means. The bias is proportional to the difference $\\alpha_1 - \\alpha_2$ and its sign depends on the direction of this difference and the nature of the task imbalance. Here, with more 'A' trials in run $1$ and more 'B' trials in run $2$, if run $1$ has a higher mean signal ($\\alpha_1  \\alpha_2$), the estimate $\\hat{\\beta}_0$ will be artificially inflated.\nIn general symbolic terms, the bias is:\n$$\n\\text{Bias} = \\frac{(N_{A1} - N_{B1})\\alpha_{1} + (N_{A2} - N_{B2})\\alpha_{2}}{N} = \\frac{20\\alpha_1 - 20\\alpha_2}{100} = \\frac{1}{5}(\\alpha_{1} - \\alpha_{2})\n$$",
            "answer": "$$\\boxed{\\frac{1}{5}(\\alpha_{1} - \\alpha_{2})}$$"
        }
    ]
}