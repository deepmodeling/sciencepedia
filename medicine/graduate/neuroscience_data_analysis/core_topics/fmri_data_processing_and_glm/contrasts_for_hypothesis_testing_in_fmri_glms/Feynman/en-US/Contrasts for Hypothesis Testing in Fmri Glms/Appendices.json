{
    "hands_on_practices": [
        {
            "introduction": "Before testing a hypothesis, we must first ensure it is \"estimable\"—a question the data can actually answer. This fundamental property is directly linked to the algebraic structure of your design matrix, $X$. This practice  guides you from the first principles of defining a condition comparison to verifying the estimability condition, solidifying the crucial link between a scientific hypothesis and the linear algebra of the GLM.",
            "id": "4149034",
            "problem": "A single-subject functional Magnetic Resonance Imaging (fMRI) experiment is modeled with a General Linear Model (GLM) having three task conditions $\\mathcal{A}$, $\\mathcal{B}$, and $\\mathcal{C}$. The design matrix $X \\in \\mathbb{R}^{n \\times p}$ uses an intercept and two condition dummy regressors, where the intercept encodes a baseline and the dummies indicate conditions $\\mathcal{B}$ and $\\mathcal{C}$ relative to $\\mathcal{A}$. There are $n=6$ scans, ordered such that scans $1$ and $4$ present condition $\\mathcal{A}$, scans $2$ and $5$ present condition $\\mathcal{B}$, and scans $3$ and $6$ present condition $\\mathcal{C}$. The design matrix rows are\n$$\nx_{\\mathcal{A}}^{\\top} = \\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix}, \\quad\nx_{\\mathcal{B}}^{\\top} = \\begin{pmatrix} 1 & 1 & 0 \\end{pmatrix}, \\quad\nx_{\\mathcal{C}}^{\\top} = \\begin{pmatrix} 1 & 0 & 1 \\end{pmatrix},\n$$\nand $X$ repeats these rows in the order $\\mathcal{A},\\mathcal{B},\\mathcal{C},\\mathcal{A},\\mathcal{B},\\mathcal{C}$. The GLM is $y = X \\beta + \\varepsilon$, with $\\beta \\in \\mathbb{R}^{3}$ and $\\varepsilon$ zero-mean noise with covariance $\\sigma^{2} I$.\n\nStarting from the definitions that (i) the GLM encodes condition-specific predicted means as $x^{\\top}\\beta$ and (ii) a contrast for comparing two condition means is a row vector $c^{\\top}$ such that $c^{\\top}\\beta$ equals the difference of the corresponding predicted means, perform the following:\n\n1. Construct the contrast row vectors $c_{\\mathcal{B}-\\mathcal{A}}^{\\top}$, $c_{\\mathcal{C}-\\mathcal{A}}^{\\top}$, and $c_{\\mathcal{B}-\\mathcal{C}}^{\\top}$ that represent the differences in predicted means between conditions $\\mathcal{B}$ and $\\mathcal{A}$, $\\mathcal{C}$ and $\\mathcal{A}$, and $\\mathcal{B}$ and $\\mathcal{C}$, respectively.\n\n2. Verify from first principles of linear algebra that each constructed contrast lies in the row space of $X$ by explicitly expressing each as a linear combination of rows of $X$.\n\n3. Let $P_{\\mathrm{row}(X)}$ denote the Euclidean orthogonal projector onto the row space of $X$. Define\n$$\nS \\equiv \\left\\| c_{\\mathcal{B}-\\mathcal{A}}^{\\top} - P_{\\mathrm{row}(X)} c_{\\mathcal{B}-\\mathcal{A}}^{\\top} \\right\\|_{2}^{2}\n+ \\left\\| c_{\\mathcal{C}-\\mathcal{A}}^{\\top} - P_{\\mathrm{row}(X)} c_{\\mathcal{C}-\\mathcal{A}}^{\\top} \\right\\|_{2}^{2}\n+ \\left\\| c_{\\mathcal{B}-\\mathcal{C}}^{\\top} - P_{\\mathrm{row}(X)} c_{\\mathcal{B}-\\mathcal{C}}^{\\top} \\right\\|_{2}^{2}.\n$$\nCompute the exact value of $S$. No rounding is required. Provide your final answer as a single real number with no units.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded in the principles of linear models in neuroscience, is well-posed with a unique and meaningful solution, and is stated using objective, formal mathematical language. It contains no scientific flaws, ambiguities, or contradictions.\n\nThe problem requires a three-part analysis of contrasts in a General Linear Model (GLM) for fMRI data. The GLM is given by $y = X \\beta + \\varepsilon$, where $y \\in \\mathbb{R}^{n}$ is the data, $X \\in \\mathbb{R}^{n \\times p}$ is the design matrix, $\\beta \\in \\mathbb{R}^{p}$ is the vector of parameters, and $\\varepsilon$ is a noise term. Here, $n=6$ and $p=3$.\n\nThe design of the experiment involves three conditions: $\\mathcal{A}$, $\\mathcal{B}$, and $\\mathcal{C}$. The parameterization uses an intercept representing the baseline activity of condition $\\mathcal{A}$, and two dummy regressors for the additional effects of conditions $\\mathcal{B}$ and $\\mathcal{C}$. The corresponding rows of the design matrix are given as:\n$$\nx_{\\mathcal{A}}^{\\top} = \\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix}\n$$\n$$\nx_{\\mathcal{B}}^{\\top} = \\begin{pmatrix} 1 & 1 & 0 \\end{pmatrix}\n$$\n$$\nx_{\\mathcal{C}}^{\\top} = \\begin{pmatrix} 1 & 0 & 1 \\end{pmatrix}\n$$\nThe full design matrix $X$ consists of these rows repeated in the order $\\mathcal{A},\\mathcal{B},\\mathcal{C},\\mathcal{A},\\mathcal{B},\\mathcal{C}$.\n\nThe parameter vector is $\\beta = \\begin{pmatrix} \\beta_1 & \\beta_2 & \\beta_3 \\end{pmatrix}^{\\top}$. The predicted mean activity for each condition is given by $x^{\\top}\\beta$.\nFor condition $\\mathcal{A}$, the predicted mean is $\\mu_{\\mathcal{A}} = x_{\\mathcal{A}}^{\\top}\\beta = \\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} \\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\end{pmatrix} = \\beta_1$.\nFor condition $\\mathcal{B}$, the predicted mean is $\\mu_{\\mathcal{B}} = x_{\\mathcal{B}}^{\\top}\\beta = \\begin{pmatrix} 1 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} \\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\end{pmatrix} = \\beta_1 + \\beta_2$.\nFor condition $\\mathcal{C}$, the predicted mean is $\\mu_{\\mathcal{C}} = x_{\\mathcal{C}}^{\\top}\\beta = \\begin{pmatrix} 1 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} \\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\end{pmatrix} = \\beta_1 + \\beta_3$.\nThis confirms that $\\beta_1$ is the mean of condition $\\mathcal{A}$, $\\beta_2$ is the difference $\\mu_{\\mathcal{B}} - \\mu_{\\mathcal{A}}$, and $\\beta_3$ is the difference $\\mu_{\\mathcal{C}} - \\mu_{\\mathcal{A}}$.\n\n**1. Constructing the contrast vectors**\n\nA contrast is a row vector $c^{\\top}$ such that $c^{\\top}\\beta$ represents a specific linear combination of parameters, typically a comparison between condition means.\n\n- **Contrast for $\\mathcal{B} - \\mathcal{A}$:**\nThe difference in predicted means is $\\mu_{\\mathcal{B}} - \\mu_{\\mathcal{A}} = (\\beta_1 + \\beta_2) - \\beta_1 = \\beta_2$.\nWe need to find $c_{\\mathcal{B}-\\mathcal{A}}^{\\top}$ such that $c_{\\mathcal{B}-\\mathcal{A}}^{\\top}\\beta = \\beta_2$. This is satisfied by:\n$$\nc_{\\mathcal{B}-\\mathcal{A}}^{\\top} = \\begin{pmatrix} 0 & 1 & 0 \\end{pmatrix}\n$$\n\n- **Contrast for $\\mathcal{C} - \\mathcal{A}$:**\nThe difference in predicted means is $\\mu_{\\mathcal{C}} - \\mu_{\\mathcal{A}} = (\\beta_1 + \\beta_3) - \\beta_1 = \\beta_3$.\nWe need to find $c_{\\mathcal{C}-\\mathcal{A}}^{\\top}$ such that $c_{\\mathcal{C}-\\mathcal{A}}^{\\top}\\beta = \\beta_3$. This is satisfied by:\n$$\nc_{\\mathcal{C}-\\mathcal{A}}^{\\top} = \\begin{pmatrix} 0 & 0 & 1 \\end{pmatrix}\n$$\n\n- **Contrast for $\\mathcal{B} - \\mathcal{C}$:**\nThe difference in predicted means is $\\mu_{\\mathcal{B}} - \\mu_{\\mathcal{C}} = (\\beta_1 + \\beta_2) - (\\beta_1 + \\beta_3) = \\beta_2 - \\beta_3$.\nWe need to find $c_{\\mathcal{B}-\\mathcal{C}}^{\\top}$ such that $c_{\\mathcal{B}-\\mathcal{C}}^{\\top}\\beta = \\beta_2 - \\beta_3$. This is satisfied by:\n$$\nc_{\\mathcal{B}-\\mathcal{C}}^{\\top} = \\begin{pmatrix} 0 & 1 & -1 \\end{pmatrix}\n$$\n\n**2. Verifying the contrasts lie in the row space of $X$**\n\nThe row space of $X$, denoted $\\mathrm{row}(X)$, is the vector space spanned by the rows of $X$. Since the matrix $X$ is formed by repeating the three unique rows $x_{\\mathcal{A}}^{\\top}$, $x_{\\mathcal{B}}^{\\top}$, and $x_{\\mathcal{C}}^{\\top}$, its row space is the span of these three vectors: $\\mathrm{row}(X) = \\mathrm{span}\\{x_{\\mathcal{A}}^{\\top}, x_{\\mathcal{B}}^{\\top}, x_{\\mathcal{C}}^{\\top}\\}$.\nA vector lies in this space if it can be written as a linear combination of these basis vectors.\n\n- For $c_{\\mathcal{B}-\\mathcal{A}}^{\\top} = \\begin{pmatrix} 0 & 1 & 0 \\end{pmatrix}$:\nWe observe that $x_{\\mathcal{B}}^{\\top} - x_{\\mathcal{A}}^{\\top} = \\begin{pmatrix} 1 & 1 & 0 \\end{pmatrix} - \\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 0 & 1 & 0 \\end{pmatrix}$.\nThus, $c_{\\mathcal{B}-\\mathcal{A}}^{\\top} = (1)x_{\\mathcal{B}}^{\\top} + (-1)x_{\\mathcal{A}}^{\\top}$, which is a linear combination of rows of $X$. Therefore, $c_{\\mathcal{B}-\\mathcal{A}}^{\\top} \\in \\mathrm{row}(X)$.\n\n- For $c_{\\mathcal{C}-\\mathcal{A}}^{\\top} = \\begin{pmatrix} 0 & 0 & 1 \\end{pmatrix}$:\nWe observe that $x_{\\mathcal{C}}^{\\top} - x_{\\mathcal{A}}^{\\top} = \\begin{pmatrix} 1 & 0 & 1 \\end{pmatrix} - \\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 & 1 \\end{pmatrix}$.\nThus, $c_{\\mathcal{C}-\\mathcal{A}}^{\\top} = (1)x_{\\mathcal{C}}^{\\top} + (-1)x_{\\mathcal{A}}^{\\top}$, which is a linear combination of rows of $X$. Therefore, $c_{\\mathcal{C}-\\mathcal{A}}^{\\top} \\in \\mathrm{row}(X)$.\n\n- For $c_{\\mathcal{B}-\\mathcal{C}}^{\\top} = \\begin{pmatrix} 0 & 1 & -1 \\end{pmatrix}$:\nWe observe that $x_{\\mathcal{B}}^{\\top} - x_{\\mathcal{C}}^{\\top} = \\begin{pmatrix} 1 & 1 & 0 \\end{pmatrix} - \\begin{pmatrix} 1 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 0 & 1 & -1 \\end{pmatrix}$.\nThus, $c_{\\mathcal{B}-\\mathcal{C}}^{\\top} = (1)x_{\\mathcal{B}}^{\\top} + (-1)x_{\\mathcal{C}}^{\\top}$, which is a linear combination of rows of $X$. Therefore, $c_{\\mathcal{B}-\\mathcal{C}}^{\\top} \\in \\mathrm{row}(X)$.\n\nThis explicitly verifies that all three contrast vectors lie in the row space of the design matrix $X$. This is a general requirement for a contrast $c^{\\top}\\beta$ to be estimable.\n\n**3. Computing the value of $S$**\n\nThe quantity to compute is:\n$$\nS \\equiv \\left\\| c_{\\mathcal{B}-\\mathcal{A}}^{\\top} - P_{\\mathrm{row}(X)} c_{\\mathcal{B}-\\mathcal{A}}^{\\top} \\right\\|_{2}^{2}\n+ \\left\\| c_{\\mathcal{C}-\\mathcal{A}}^{\\top} - P_{\\mathrm{row}(X)} c_{\\mathcal{C}-\\mathcal{A}}^{\\top} \\right\\|_{2}^{2}\n+ \\left\\| c_{\\mathcal{B}-\\mathcal{C}}^{\\top} - P_{\\mathrm{row}(X)} c_{\\mathcal{B}-\\mathcal{C}}^{\\top} \\right\\|_{2}^{2}\n$$\nwhere $P_{\\mathrm{row}(X)}$ is the Euclidean orthogonal projector onto the subspace $\\mathrm{row}(X)$.\n\nA fundamental property of an orthogonal projector $P_V$ onto a vector subspace $V$ is that for any vector $v \\in V$, the projection of $v$ onto $V$ is the vector $v$ itself. That is, if $v \\in V$, then $P_V v = v$.\n\nIn step 2, we demonstrated that each of the contrast vectors, $c_{\\mathcal{B}-\\mathcal{A}}^{\\top}$, $c_{\\mathcal{C}-\\mathcal{A}}^{\\top}$, and $c_{\\mathcal{B}-\\mathcal{C}}^{\\top}$, is an element of the row space of $X$.\nApplying the property of the orthogonal projector:\n\n- For $c_{\\mathcal{B}-\\mathcal{A}}^{\\top}$: Since $c_{\\mathcal{B}-\\mathcal{A}}^{\\top} \\in \\mathrm{row}(X)$, we have $P_{\\mathrm{row}(X)} c_{\\mathcal{B}-\\mathcal{A}}^{\\top} = c_{\\mathcal{B}-\\mathcal{A}}^{\\top}$.\nThe first term in the sum for $S$ is $\\left\\| c_{\\mathcal{B}-\\mathcal{A}}^{\\top} - c_{\\mathcal{B}-\\mathcal{A}}^{\\top} \\right\\|_{2}^{2} = \\left\\| \\mathbf{0}^{\\top} \\right\\|_{2}^{2} = 0$.\n\n- For $c_{\\mathcal{C}-\\mathcal{A}}^{\\top}$: Since $c_{\\mathcal{C}-\\mathcal{A}}^{\\top} \\in \\mathrm{row}(X)$, we have $P_{\\mathrm{row}(X)} c_{\\mathcal{C}-\\mathcal{A}}^{\\top} = c_{\\mathcal{C}-\\mathcal{A}}^{\\top}$.\nThe second term is $\\left\\| c_{\\mathcal{C}-\\mathcal{A}}^{\\top} - c_{\\mathcal{C}-\\mathcal{A}}^{\\top} \\right\\|_{2}^{2} = \\left\\| \\mathbf{0}^{\\top} \\right\\|_{2}^{2} = 0$.\n\n- For $c_{\\mathcal{B}-\\mathcal{C}}^{\\top}$: Since $c_{\\mathcal{B}-\\mathcal{C}}^{\\top} \\in \\mathrm{row}(X)$, we have $P_{\\mathrm{row}(X)} c_{\\mathcal{B}-\\mathcal{C}}^{\\top} = c_{\\mathcal{B}-\\mathcal{C}}^{\\top}$.\nThe third term is $\\left\\| c_{\\mathcal{B}-\\mathcal{C}}^{\\top} - c_{\\mathcal{B}-\\mathcal{C}}^{\\top} \\right\\|_{2}^{2} = \\left\\| \\mathbf{0}^{\\top} \\right\\|_{2}^{2} = 0$.\n\nCombining these results, the value of $S$ is the sum of these three terms:\n$$\nS = 0 + 0 + 0 = 0\n$$\n\nThe expression $\\| v - P_V v \\|_2$ represents the distance from a vector $v$ to the subspace $V$. The calculation confirms that this distance is zero for all three contrast vectors, as they are elements of the subspace $\\mathrm{row}(X)$ by construction.",
            "answer": "$$\\boxed{0}$$"
        },
        {
            "introduction": "A single experimental design can be represented by multiple, mathematically equivalent parameterizations, such as treatment coding versus sum-to-zero coding. While the scientific question (e.g., \"Is condition $\\mathcal{B}$ different from $\\mathcal{A}$?\") remains constant, the contrast vector used to test it must change with the parameterization. This exercise  challenges you to translate contrasts between two common coding schemes, revealing the principles of how estimable functions remain invariant even as their vector representations change.",
            "id": "4148931",
            "problem": "A single-voxel time series from functional Magnetic Resonance Imaging (fMRI) is modeled with a General Linear Model (GLM). There are $3$ experimental conditions: $A$, $B$, and $C$. Each condition’s regressor is formed by convolving condition-specific event onsets with the same hemodynamic response function, so that amplitudes are directly comparable across conditions. Errors are pre-whitened so that ordinary least squares applies, and the design matrix has full column rank.\n\nTwo parameterizations of the same design are used:\n\n$1.$ Treatment coding with $A$ as baseline: there is an intercept column and two condition columns corresponding to $B$ and $C$. Let the parameter vector be $\\boldsymbol{\\beta}^{(t)} = \\begin{pmatrix}\\mu \\\\ \\beta_{B} \\\\ \\beta_{C}\\end{pmatrix}$, where $\\mu$ is the baseline amplitude for condition $A$, and $\\beta_{B}$ and $\\beta_{C}$ are incremental amplitudes for conditions $B$ and $C$ relative to $A$. Under this coding, the condition means $(m_{A}, m_{B}, m_{C})$ satisfy $m_{A} = \\mu$, $m_{B} = \\mu + \\beta_{B}$, and $m_{C} = \\mu + \\beta_{C}$.\n\n$2.$ Sum-to-zero (effect) coding with an intercept: there is an intercept column of ones and two effect-coded columns $E_{1}$ and $E_{2}$ defined per time point by the condition label as follows: for condition $A$, $(E_{1}, E_{2}) = (-1, -1)$; for condition $B$, $(E_{1}, E_{2}) = (1, 0)$; and for condition $C$, $(E_{1}, E_{2}) = (0, 1)$. Let the parameter vector be $\\boldsymbol{\\theta} = \\begin{pmatrix}\\alpha \\\\ \\gamma_{1} \\\\ \\gamma_{2}\\end{pmatrix}$. Under this coding, the condition means satisfy $m_{A} = \\alpha - \\gamma_{1} - \\gamma_{2}$, $m_{B} = \\alpha + \\gamma_{1}$, and $m_{C} = \\alpha + \\gamma_{2}$.\n\nTasks:\n\n(a) Using the definition of a contrast as a linear functional of the parameter vector, write the contrast vectors under treatment coding for testing $B$ versus $A$ and $C$ versus $A$. Denote these by $\\mathbf{c}^{(t)}_{B:A}$ and $\\mathbf{c}^{(t)}_{C:A}$, respectively.\n\n(b) By equating the condition means under the two parameterizations, derive the linear mapping matrix $\\mathbf{M}$ such that $\\boldsymbol{\\beta}^{(t)} = \\mathbf{M}\\,\\boldsymbol{\\theta}$.\n\n(c) Using only the definition that two parameterizations represent the same model if they yield the same condition means, and that a contrast evaluates an estimable function identically under any full-rank reparameterization, re-express the contrasts from part (a) under sum-to-zero coding. That is, find $\\mathbf{c}^{(s)}_{B:A}$ and $\\mathbf{c}^{(s)}_{C:A}$ such that $\\mathbf{c}^{(t)\\top}_{B:A}\\boldsymbol{\\beta}^{(t)} = \\mathbf{c}^{(s)\\top}_{B:A}\\boldsymbol{\\theta}$ and $\\mathbf{c}^{(t)\\top}_{C:A}\\boldsymbol{\\beta}^{(t)} = \\mathbf{c}^{(s)\\top}_{C:A}\\boldsymbol{\\theta}$ for all $\\boldsymbol{\\theta}$.\n\n(d) Let $\\langle \\mathbf{u}, \\mathbf{v} \\rangle$ denote the standard Euclidean inner product. Compute the scalar $\\langle \\mathbf{c}^{(s)}_{B:A}, \\mathbf{c}^{(s)}_{C:A} \\rangle$. Provide the final answer as a single exact number without units. No rounding is required.",
            "solution": "The problem statement has been validated and found to be scientifically grounded, well-posed, and objective. It presents a standard, well-defined problem in the statistical analysis of functional neuroimaging data using the General Linear Model (GLM). All necessary information is provided, and the tasks are direct applications of the theory of linear models and contrast testing.\n\nWe will proceed with a step-by-step solution to the four tasks.\n\n(a) Contrast vectors under treatment coding.\n\nA contrast is a linear combination of model parameters, $\\mathbf{c}^\\top\\boldsymbol{\\beta}$, that corresponds to a testable hypothesis about the condition means. The parameter vector for the treatment coding is $\\boldsymbol{\\beta}^{(t)} = \\begin{pmatrix}\\mu \\\\ \\beta_{B} \\\\ \\beta_{C}\\end{pmatrix}$. The condition means are given by $m_{A} = \\mu$, $m_{B} = \\mu + \\beta_{B}$, and $m_{C} = \\mu + \\beta_{C}$.\n\nThe hypothesis \"$B$ versus $A$\" corresponds to testing if the difference between the mean activation for condition $B$ and condition $A$ is zero, i.e., testing $m_{B} - m_{A} = 0$. The linear function of the parameters representing this difference is the estimable function.\n$$\nm_{B} - m_{A} = (\\mu + \\beta_{B}) - \\mu = \\beta_{B}\n$$\nTo express this as a dot product $\\mathbf{c}^{(t)\\top}_{B:A}\\boldsymbol{\\beta}^{(t)}$, we write:\n$$\n\\beta_{B} = (0)\\mu + (1)\\beta_{B} + (0)\\beta_{C} = \\begin{pmatrix} 0 & 1 & 0 \\end{pmatrix} \\begin{pmatrix}\\mu \\\\ \\beta_{B} \\\\ \\beta_{C}\\end{pmatrix}\n$$\nTherefore, the contrast vector for testing $B$ versus $A$ is $\\mathbf{c}^{(t)}_{B:A} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}$.\n\nSimilarly, the hypothesis \"$C$ versus $A$\" corresponds to testing $m_{C} - m_{A} = 0$. The estimable function is:\n$$\nm_{C} - m_{A} = (\\mu + \\beta_{C}) - \\mu = \\beta_{C}\n$$\nExpressing this as a dot product $\\mathbf{c}^{(t)\\top}_{C:A}\\boldsymbol{\\beta}^{(t)}$:\n$$\n\\beta_{C} = (0)\\mu + (0)\\beta_{B} + (1)\\beta_{C} = \\begin{pmatrix} 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix}\\mu \\\\ \\beta_{B} \\\\ \\beta_{C}\\end{pmatrix}\n$$\nTherefore, the contrast vector for testing $C$ versus $A$ is $\\mathbf{c}^{(t)}_{C:A} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}$.\n\n(b) Derivation of the linear mapping matrix $\\mathbf{M}$.\n\nWe are asked to find the matrix $\\mathbf{M}$ such that $\\boldsymbol{\\beta}^{(t)} = \\mathbf{M}\\,\\boldsymbol{\\theta}$. This involves expressing the parameters of the treatment coding ($\\mu, \\beta_{B}, \\beta_{C}$) in terms of the parameters of the sum-to-zero coding ($\\alpha, \\gamma_{1}, \\gamma_{2}$). We do this by equating the expressions for the condition means under both parameterizations.\n\nTreatment coding means:\n$m_{A} = \\mu$\n$m_{B} = \\mu + \\beta_{B}$\n$m_{C} = \\mu + \\beta_{C}$\n\nSum-to-zero coding means:\n$m_{A} = \\alpha - \\gamma_{1} - \\gamma_{2}$\n$m_{B} = \\alpha + \\gamma_{1}$\n$m_{C} = \\alpha + \\gamma_{2}$\n\nFirst, we solve for $\\mu$ by equating the expressions for $m_{A}$:\n$$\n\\mu = \\alpha - \\gamma_{1} - \\gamma_{2}\n$$\n\nNext, we solve for $\\beta_{B}$. We know $m_{B} = \\mu + \\beta_{B}$, so $\\beta_{B} = m_{B} - \\mu$. Substituting the expressions for $m_{B}$ from sum-to-zero coding and the expression for $\\mu$ we just found:\n$$\n\\beta_{B} = (\\alpha + \\gamma_{1}) - (\\alpha - \\gamma_{1} - \\gamma_{2}) = \\alpha + \\gamma_{1} - \\alpha + \\gamma_{1} + \\gamma_{2} = 2\\gamma_{1} + \\gamma_{2}\n$$\n\nFinally, we solve for $\\beta_{C}$. We know $m_{C} = \\mu + \\beta_{C}$, so $\\beta_{C} = m_{C} - \\mu$. Substituting the expressions for $m_{C}$ and $\\mu$:\n$$\n\\beta_{C} = (\\alpha + \\gamma_{2}) - (\\alpha - \\gamma_{1} - \\gamma_{2}) = \\alpha + \\gamma_{2} - \\alpha + \\gamma_{1} + \\gamma_{2} = \\gamma_{1} + 2\\gamma_{2}\n$$\n\nWe can now write these relationships in matrix form:\n$$\n\\mu = (1)\\alpha + (-1)\\gamma_{1} + (-1)\\gamma_{2}\n$$\n$$\n\\beta_{B} = (0)\\alpha + (2)\\gamma_{1} + (1)\\gamma_{2}\n$$\n$$\n\\beta_{C} = (0)\\alpha + (1)\\gamma_{1} + (2)\\gamma_{2}\n$$\nThis gives the system $\\boldsymbol{\\beta}^{(t)} = \\mathbf{M}\\,\\boldsymbol{\\theta}$:\n$$\n\\begin{pmatrix}\\mu \\\\ \\beta_{B} \\\\ \\beta_{C}\\end{pmatrix} = \\begin{pmatrix} 1 & -1 & -1 \\\\ 0 & 2 & 1 \\\\ 0 & 1 & 2 \\end{pmatrix} \\begin{pmatrix}\\alpha \\\\ \\gamma_{1} \\\\ \\gamma_{2}\\end{pmatrix}\n$$\nThe linear mapping matrix is therefore:\n$$\n\\mathbf{M} = \\begin{pmatrix} 1 & -1 & -1 \\\\ 0 & 2 & 1 \\\\ 0 & 1 & 2 \\end{pmatrix}\n$$\n\n(c) Re-expression of contrasts under sum-to-zero coding.\n\nA fundamental property of estimable functions in a GLM is that their value is invariant under any valid reparameterization of the model. This means that the value of the contrast, which is the estimable function, must be the same regardless of the chosen parameterization.\n$$\n\\mathbf{c}^{(t)\\top}\\boldsymbol{\\beta}^{(t)} = \\mathbf{c}^{(s)\\top}\\boldsymbol{\\theta}\n$$\nWe substitute the relationship $\\boldsymbol{\\beta}^{(t)} = \\mathbf{M}\\,\\boldsymbol{\\theta}$ into this equation:\n$$\n\\mathbf{c}^{(t)\\top}(\\mathbf{M}\\,\\boldsymbol{\\theta}) = \\mathbf{c}^{(s)\\top}\\boldsymbol{\\theta}\n$$\nUsing the associative property of matrix multiplication, $(\\mathbf{A}\\mathbf{B})\\mathbf{C} = \\mathbf{A}(\\mathbf{B}\\mathbf{C})$, we can group the terms:\n$$\n(\\mathbf{c}^{(t)\\top}\\mathbf{M})\\boldsymbol{\\theta} = \\mathbf{c}^{(s)\\top}\\boldsymbol{\\theta}\n$$\nSince this equality must hold for any parameter vector $\\boldsymbol{\\theta}$, the row vectors multiplying $\\boldsymbol{\\theta}$ must be equal:\n$$\n\\mathbf{c}^{(s)\\top} = \\mathbf{c}^{(t)\\top}\\mathbf{M}\n$$\nTransposing both sides gives the relationship for the column vectors:\n$$\n\\mathbf{c}^{(s)} = (\\mathbf{c}^{(t)\\top}\\mathbf{M})^\\top = \\mathbf{M}^\\top \\mathbf{c}^{(t)}\n$$\nWe first need the transpose of $\\mathbf{M}$:\n$$\n\\mathbf{M}^\\top = \\begin{pmatrix} 1 & 0 & 0 \\\\ -1 & 2 & 1 \\\\ -1 & 1 & 2 \\end{pmatrix}\n$$\nNow we can find the new contrast vectors. For $B$ versus $A$:\n$$\n\\mathbf{c}^{(s)}_{B:A} = \\mathbf{M}^\\top \\mathbf{c}^{(t)}_{B:A} = \\begin{pmatrix} 1 & 0 & 0 \\\\ -1 & 2 & 1 \\\\ -1 & 1 & 2 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} (1)(0) + (0)(1) + (0)(0) \\\\ (-1)(0) + (2)(1) + (1)(0) \\\\ (-1)(0) + (1)(1) + (2)(0) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 2 \\\\ 1 \\end{pmatrix}\n$$\nFor $C$ versus $A$:\n$$\n\\mathbf{c}^{(s)}_{C:A} = \\mathbf{M}^\\top \\mathbf{c}^{(t)}_{C:A} = \\begin{pmatrix} 1 & 0 & 0 \\\\ -1 & 2 & 1 \\\\ -1 & 1 & 2 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} (1)(0) + (0)(0) + (0)(1) \\\\ (-1)(0) + (2)(0) + (1)(1) \\\\ (-1)(0) + (1)(0) + (2)(1) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 2 \\end{pmatrix}\n$$\nAs a check, we can directly evaluate the estimable functions with the sum-to-zero parameters:\n$m_{B} - m_{A} = (\\alpha + \\gamma_{1}) - (\\alpha - \\gamma_{1} - \\gamma_{2}) = 2\\gamma_{1} + \\gamma_{2}$. This corresponds to the dot product $\\begin{pmatrix} 0 & 2 & 1 \\end{pmatrix} \\boldsymbol{\\theta}$, which verifies $\\mathbf{c}^{(s)}_{B:A} = \\begin{pmatrix} 0 \\\\ 2 \\\\ 1 \\end{pmatrix}$.\n$m_{C} - m_{A} = (\\alpha + \\gamma_{2}) - (\\alpha - \\gamma_{1} - \\gamma_{2}) = \\gamma_{1} + 2\\gamma_{2}$. This corresponds to the dot product $\\begin{pmatrix} 0 & 1 & 2 \\end{pmatrix} \\boldsymbol{\\theta}$, which verifies $\\mathbf{c}^{(s)}_{C:A} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 2 \\end{pmatrix}$.\n\n(d) Scalar product of the new contrast vectors.\n\nWe need to compute the standard Euclidean inner product (dot product) of the two contrast vectors found in part (c).\n$$\n\\mathbf{c}^{(s)}_{B:A} = \\begin{pmatrix} 0 \\\\ 2 \\\\ 1 \\end{pmatrix}, \\quad \\mathbf{c}^{(s)}_{C:A} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 2 \\end{pmatrix}\n$$\nThe inner product is:\n$$\n\\langle \\mathbf{c}^{(s)}_{B:A}, \\mathbf{c}^{(s)}_{C:A} \\rangle = \\mathbf{c}^{(s)\\top}_{B:A} \\mathbf{c}^{(s)}_{C:A} = \\begin{pmatrix} 0 & 2 & 1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\\\ 2 \\end{pmatrix}\n$$\n$$\n\\langle \\mathbf{c}^{(s)}_{B:A}, \\mathbf{c}^{(s)}_{C:A} \\rangle = (0)(0) + (2)(1) + (1)(2) = 0 + 2 + 2 = 4\n$$\nThe scalar product is $4$. This non-zero value indicates that the two contrasts are not orthogonal. In the context of fMRI analysis, this means that the statistical tests for \"$B$ versus $A$\" and \"$C$ versus $A$\" are not independent under this specific sum-to-zero parameterization.",
            "answer": "$$\\boxed{4}$$"
        },
        {
            "introduction": "Correlated regressors, such as a main task effect and its parametric modulator, are common in fMRI and present a significant interpretational challenge. The meaning of a regressor's coefficient—and thus the hypothesis tested by its contrast—changes depending on whether the shared variance is modeled symmetrically or is serially orthogonalized. This exercise  uses the Frisch–Waugh–Lovell theorem to dissect this critical issue, contrasting the test of a regressor's *unique* contribution with the test of its *total* contribution.",
            "id": "4148912",
            "problem": "A single-voxel functional Magnetic Resonance Imaging (fMRI) time series is modeled using the General Linear Model (GLM), defined by $y = X \\beta + \\varepsilon$, where $y \\in \\mathbb{R}^{T}$ is the observed Blood Oxygen Level Dependent (BOLD) signal, $X \\in \\mathbb{R}^{T \\times p}$ is the design matrix, $\\beta \\in \\mathbb{R}^{p}$ are unknown coefficients, and $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I)$ are independently and identically distributed Gaussian errors. Consider a design with an intercept and two task-related regressors: a primary task regressor $x_{p}$ and a correlated parametric modulator $x_{m}$, both column-mean centered. Let $X_{\\text{raw}} = [\\mathbf{1}, x_{p}, x_{m}]$ denote the non-orthogonalized design. A second model, $X_{\\text{orth}} = [\\mathbf{1}, x_{p}, \\tilde{x}_{m}]$, is formed by orthogonalizing $x_{m}$ with respect to $x_{p}$ using the standard residualization operator,\n$$\n\\tilde{x}_{m} \\equiv M_{x_{p}} x_{m}, \\quad M_{x_{p}} \\equiv I - P_{x_{p}}, \\quad P_{x_{p}} \\equiv x_{p} (x_{p}^{\\top} x_{p})^{-1} x_{p}^{\\top}.\n$$\nAssume that contrasts are tested in the model that is actually fit, i.e., $X_{\\text{raw}}$ for the non-orthogonalized model and $X_{\\text{orth}}$ for the orthogonalized model. A linear contrast $c \\in \\mathbb{R}^{p}$ tests the null hypothesis $H_{0}: c^{\\top} \\beta = 0$ via the usual $t$-statistic under the GLM assumptions.\n\nUsing only core definitions and consequences of the GLM, orthogonal projections, and the Frisch–Waugh–Lovell theorem, reason carefully about the interpretational difference between contrasts on the raw regressors and contrasts on the orthogonalized regressors when orthogonalization has been applied as above. In particular, compare the meaning of $c_{p} = [0, 1, 0]^{\\top}$ (testing the primary task) and $c_{m} = [0, 0, 1]^{\\top}$ (testing the modulator) across $X_{\\text{raw}}$ and $X_{\\text{orth}}$.\n\nWhich of the following statements are correct?\n\nA. In the orthogonalized model $X_{\\text{orth}}$, the contrast $c_{m} = [0, 0, 1]^{\\top}$ tests the unique effect of the modulator after removing variance shared with the primary task, and its $t$-statistic is equal to the $t$-statistic for $c_{m} = [0, 0, 1]^{\\top}$ in the raw model $X_{\\text{raw}}$.\n\nB. In the orthogonalized model $X_{\\text{orth}}$, the contrast $c_{p} = [0, 1, 0]^{\\top}$ tests the unique effect of the primary task after removing variance shared with the modulator, and its $t$-statistic equals that for $c_{p} = [0, 1, 0]^{\\top}$ in the raw model $X_{\\text{raw}}$.\n\nC. Because orthogonalization is a full-rank linear reparameterization that preserves the column space, any linear contrast defined for $X_{\\text{raw}}$ can be represented as a (generally different) linear contrast for $X_{\\text{orth}}$ that tests the same null hypothesis about $y$.\n\nD. If $x_{p}$ and $x_{m}$ are mean-centered before orthogonalization, then the interpretation of $c_{p} = [0, 1, 0]^{\\top}$ is identical in $X_{\\text{raw}}$ and $X_{\\text{orth}}$.\n\nE. In the orthogonalized model $X_{\\text{orth}}$, the null hypothesis tested by $c_{p} = [0, 1, 0]^{\\top}$ can be stated as $H_{0}: \\operatorname{cov}(y, x_{p} \\mid x_{m}) = 0$, i.e., zero partial covariance between $y$ and $x_{p}$ given $x_{m}$.\n\nSelect all that apply.",
            "solution": "The user has provided a problem concerning the interpretation of linear contrasts in a General Linear Model (GLM) for fMRI data, specifically comparing a model with correlated regressors to one where a regressor has been orthogonalized.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- The model is a GLM: $y = X \\beta + \\varepsilon$.\n- $y \\in \\mathbb{R}^{T}$ is the BOLD signal.\n- $X \\in \\mathbb{R}^{T \\times p}$ is the design matrix.\n- $\\beta \\in \\mathbb{R}^{p}$ are unknown coefficients.\n- $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I)$ are i.i.d. Gaussian errors.\n- The design includes an intercept and two mean-centered task regressors, $x_{p}$ and $x_{m}$.\n- The non-orthogonalized (raw) design is $X_{\\text{raw}} = [\\mathbf{1}, x_{p}, x_{m}]$. The coefficients for this model will be denoted $\\beta = [\\beta_0, \\beta_p, \\beta_m]^{\\top}$.\n- The orthogonalized design is $X_{\\text{orth}} = [\\mathbf{1}, x_{p}, \\tilde{x}_{m}]$. The coefficients for this model will be denoted $\\gamma = [\\gamma_0, \\gamma_p, \\gamma_m]^{\\top}$.\n- The orthogonalized regressor is $\\tilde{x}_{m} \\equiv M_{x_{p}} x_{m}$, where $M_{x_{p}} \\equiv I - P_{x_{p}}$ is the residual-forming operator and $P_{x_{p}} \\equiv x_{p} (x_{p}^{\\top} x_{p})^{-1} x_{p}^{\\top}$ is the projection operator onto the column space of $x_p$.\n- Contrasts are tested in the model that is actually fit.\n- The primary task contrast is $c_{p} = [0, 1, 0]^{\\top}$.\n- The modulator contrast is $c_{m} = [0, 0, 1]^{\\top}$.\n- The question is to compare the meaning and properties of these contrasts across the two models.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem statement is scientifically sound, well-posed, and objective.\n- **Scientifically Grounded:** The GLM, orthogonalization, and contrast testing are foundational concepts in statistical modeling and are the standard for fMRI analysis. The Frisch–Waugh–Lovell (FWL) theorem is a cornerstone result in econometrics and statistics. The setup is entirely consistent with established theory.\n- **Well-Posed:** The problem is clearly defined with all necessary mathematical definitions. It asks a specific, answerable question about the interpretation of statistical parameters.\n- **Objective:** The language is formal and precise, with no subjectivity or ambiguity.\n- **No Flaws Detected:** The problem does not violate any of the invalidity criteria. It is a standard, albeit non-trivial, exercise in understanding linear models.\n\n**Step 3: Verdict and Action**\nThe problem is **valid**. The solution process will now proceed.\n\n### Derivation and Analysis\n\nThe core of the problem lies in understanding the relationship between the coefficients ($\\beta$ and $\\gamma$) of the two models. Both models span the same vector space, as the transformation from $X_{\\text{raw}}$ to $X_{\\text{orth}}$ is a full-rank linear transformation. Specifically, $X_{\\text{orth}}$'s columns can be expressed as linear combinations of $X_{\\text{raw}}$'s columns, and vice-versa.\n\nThe columns of $X_{\\text{raw}}$ are $[\\mathbf{1}, x_{p}, x_{m}]$.\nThe columns of $X_{\\text{orth}}$ are $[\\mathbf{1}, x_{p}, \\tilde{x}_{m}]$.\nThe relationship between them is:\n$\\tilde{x}_{m} = x_m - x_{p} (x_{p}^{\\top} x_{p})^{-1} x_{p}^{\\top} x_m$.\nLet $k = (x_{p}^{\\top} x_{p})^{-1} x_{p}^{\\top} x_m$, which is the coefficient from a simple regression of $x_m$ onto $x_p$.\nThen, $\\tilde{x}_m = x_m - k x_p$, which implies $x_m = \\tilde{x}_m + k x_p$.\n\nWe can substitute this into the equation for the raw model:\n$y = \\beta_0 \\mathbf{1} + \\beta_p x_p + \\beta_m x_m + \\varepsilon$\n$y = \\beta_0 \\mathbf{1} + \\beta_p x_p + \\beta_m (\\tilde{x}_m + k x_p) + \\varepsilon$\n$y = \\beta_0 \\mathbf{1} + (\\beta_p + k \\beta_m) x_p + \\beta_m \\tilde{x}_m + \\varepsilon$\n\nThis equation has the same form as the orthogonalized model:\n$y = \\gamma_0 \\mathbf{1} + \\gamma_p x_p + \\gamma_m \\tilde{x}_m + \\varepsilon$\n\nBy comparing the coefficients for the same basis vectors ($[\\mathbf{1}, x_p, \\tilde{x}_m]$), we establish the relationship between the true parameters of the two models:\n1. $\\gamma_0 = \\beta_0$\n2. $\\gamma_m = \\beta_m$\n3. $\\gamma_p = \\beta_p + k \\beta_m$\n\nThis relationship also holds for their Ordinary Least Squares (OLS) estimates, i.e., $\\hat{\\gamma}_m = \\hat{\\beta}_m$ and $\\hat{\\gamma}_p = \\hat{\\beta}_p + k \\hat{\\beta}_m$. This result is a direct application of the Frisch–Waugh–Lovell (FWL) theorem.\n\nNow we can analyze the contrasts.\n\n**Contrast on the modulator ($c_{m} = [0, 0, 1]^{\\top}$):**\n- In the raw model $X_{\\text{raw}}$, this contrast tests the null hypothesis $H_{0}: \\beta_m = 0$.\n- In the orthogonalized model $X_{\\text{orth}}$, this contrast tests the null hypothesis $H_{0}: \\gamma_m = 0$.\n- Since $\\gamma_m = \\beta_m$, these two contrasts test the **exact same null hypothesis**.\n- The FWL theorem further states that the OLS estimate $\\hat{\\beta}_m$ in the multiple regression of $y$ on $[\\mathbf{1}, x_p, x_m]$ is identical to the estimate obtained from regressing the residuals of $y$ on $[\\mathbf{1}, x_p]$ onto the residuals of $x_m$ on $[\\mathbf{1}, x_p]$. The latter regressor is precisely $\\tilde{x}_m$ (since $x_p$ and $x_m$ are mean-centered, they are orthogonal to $\\mathbf{1}$). The coefficient $\\hat{\\gamma}_m$ in the orthogonal model is also calculated based on regressing $y$ on $\\tilde{x}_m$ (controlling for $\\mathbf{1}$ and $x_p$). The theorem guarantees that not only are the point estimates equal ($\\hat{\\beta}_m = \\hat{\\gamma}_m$), but their standard errors, and thus their $t$-statistics, are also identical.\n- The interpretation of this hypothesis ($\\beta_m=0$ or $\\gamma_m=0$) is a test of the unique contribution of $x_m$ after accounting for any shared variance with $x_p$. This is because the estimate is based on $\\tilde{x}_m$, the part of $x_m$ orthogonal to $x_p$.\n\n**Contrast on the primary task ($c_{p} = [0, 1, 0]^{\\top}$):**\n- In the raw model $X_{\\text{raw}}$, this contrast tests the null hypothesis $H_{0}: \\beta_p = 0$. This tests the unique contribution of $x_p$ after accounting for variance shared with $x_m$.\n- In the orthogonalized model $X_{\\text{orth}}$, this contrast tests the null hypothesis $H_{0}: \\gamma_p = 0$.\n- Using our relationship, $H_{0}: \\gamma_p = 0$ is equivalent to $H_{0}: \\beta_p + k \\beta_m = 0$.\n- This is a fundamentally different hypothesis from $H_{0}: \\beta_p = 0$ (unless $k=0$, meaning $x_p$ and $x_m$ were already orthogonal, or $\\beta_m=0$).\n- In the orthogonalized model $X_{\\text{orth}}$, the regressors $[\\mathbf{1}, x_p, \\tilde{x}_m]$ are mutually orthogonal (since $x_p, x_m$ are mean-centered, and $\\tilde{x}_m$ is constructed to be orthogonal to $x_p$). Due to this orthogonality, the estimate $\\hat{\\gamma}_p$ is simply $(x_p^\\top x_p)^{-1} x_p^\\top y$ (ignoring the intercept). This is the coefficient one would obtain from a simple regression of $y$ on $x_p$. It represents the total effect of $x_p$, including variance it shares with $x_m$. In contrast, $\\hat{\\beta}_p$ represents the unique effect of $x_p$ after controlling for $x_m$.\n- Therefore, the meaning of the contrast $c_p$ and its associated $t$-statistic are different between the two models.\n\n### Option-by-Option Analysis\n\n**A. In the orthogonalized model $X_{\\text{orth}}$, the contrast $c_{m} = [0, 0, 1]^{\\top}$ tests the unique effect of the modulator after removing variance shared with the primary task, and its $t$-statistic is equal to the $t$-statistic for $c_{m} = [0, 0, 1]^{\\top}$ in the raw model $X_{\\text{raw}}$.**\n- The contrast tests $H_0: \\gamma_m=0$. The parameter $\\gamma_m$ is the coefficient for $\\tilde{x}_m$, which is $x_m$ with variance shared with $x_p$ removed. Thus, it tests the unique effect of the modulator.\n- As derived from the FWL theorem, the hypothesis $\\gamma_m=0$ is identical to $\\beta_m=0$, and the corresponding $t$-statistics for these tests are equal.\n- **Verdict: Correct.**\n\n**B. In the orthogonalized model $X_{\\text{orth}}$, the contrast $c_{p} = [0, 1, 0]^{\\top}$ tests the unique effect of the primary task after removing variance shared with the modulator, and its $t$-statistic equals that for $c_{p} = [0, 1, 0]^{\\top}$ in the raw model $X_{\\text{raw}}$.**\n- The contrast tests $H_0: \\gamma_p=0$. As shown in our derivation, the parameter $\\gamma_p$ reflects the combined effects $\\beta_p + k\\beta_m$. Its estimate, $\\hat{\\gamma}_p$, is equivalent to the simple regression coefficient of $y$ on $x_p$. This represents the *total* effect of $x_p$, not its *unique* effect after removing variance shared with the modulator. The hypotheses are different, and therefore the $t$-statistics will generally be different.\n- **Verdict: Incorrect.**\n\n**C. Because orthogonalization is a full-rank linear reparameterization that preserves the column space, any linear contrast defined for $X_{\\text{raw}}$ can be represented as a (generally different) linear contrast for $X_{\\text{orth}}$ that tests the same null hypothesis about $y$.**\n- The relationship between the design matrices is $X_{\\text{raw}} = X_{\\text{orth}} B$, where $B = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & k \\\\ 0 & 0 & 1 \\end{pmatrix}$. This matrix $B$ is invertible. The relationship between the coefficient vectors is $\\gamma = B \\beta$, which implies $\\beta = B^{-1} \\gamma$.\n- A general contrast on the raw model is $c_{\\text{raw}}^{\\top} \\beta = 0$. Substituting $\\beta = B^{-1} \\gamma$, the same hypothesis is written as $c_{\\text{raw}}^{\\top} (B^{-1} \\gamma) = 0$, or $( (B^{-1})^{\\top} c_{\\text{raw}} )^{\\top} \\gamma = 0$.\n- Letting $c_{\\text{orth}} = (B^{-1})^{\\top} c_{\\text{raw}}$, we have found a contrast vector for the orthogonalized model that tests the exact same null hypothesis. Since $B$ is invertible, this mapping exists for any $c_{\\text{raw}}$.\n- **Verdict: Correct.**\n\n**D. If $x_{p}$ and $x_{m}$ are mean-centered before orthogonalization, then the interpretation of $c_{p} = [0, 1, 0]^{\\top}$ is identical in $X_{\\text{raw}}$ and $X_{\\text{orth}}$.**\n- The problem assumes this condition. Our analysis has shown that the interpretations are fundamentally different. In $X_{\\text{raw}}$, $c_p$ tests the unique effect of $x_p$. In $X_{\\text{orth}}$, $c_p$ tests the total effect of $x_p$. The statement is therefore false.\n- **Verdict: Incorrect.**\n\n**E. In the orthogonalized model $X_{\\text{orth}}$, the null hypothesis tested by $c_{p} = [0, 1, 0]^{\\top}$ can be stated as $H_{0}: \\operatorname{cov}(y, x_{p} \\mid x_{m}) = 0$, i.e., zero partial covariance between $y$ and $x_{p}$ given $x_{m}$.**\n- The null hypothesis of zero partial covariance, $H_0: \\operatorname{cov}(y, x_p \\mid x_m) = 0$, is tested by the coefficient $\\beta_p$ in the *raw* multiple regression model, i.e., the contrast $c_p = [0, 1, 0]^{\\top}$ in $X_{\\text{raw}}$.\n- The contrast $c_p = [0, 1, 0]^{\\top}$ in the *orthogonalized* model tests $H_0: \\gamma_p = 0$. As shown, this corresponds to the simple regression of $y$ on $x_p$, which tests $H_0: \\operatorname{cov}(y, x_p) = 0$ (zero simple covariance, not partial). This statement misattributes the hypothesis from the raw model to the orthogonalized model.\n- **Verdict: Incorrect.**",
            "answer": "$$\\boxed{AC}$$"
        }
    ]
}