{
    "hands_on_practices": [
        {
            "introduction": "To begin our practical exploration, we start with the Finite Impulse Response (FIR) model, one of the most flexible approaches for estimating the Hemodynamic Response Function (HRF) without strong prior assumptions about its shape. This exercise is a fundamental drill in constructing a design matrix from first principles. By manually calculating a design matrix element, you will gain a concrete understanding of how discrete event onsets and the chosen time resolution ($TR$) interact to build the regressors used in the General Linear Model (GLM). ",
            "id": "4140272",
            "problem": "Consider a single-run event-related functional Magnetic Resonance Imaging (fMRI) experiment with Blood Oxygenation Level Dependent (BOLD) signal modeled as a linear time-invariant system. The BOLD signal is the convolution of a neural event time series and the Hemodynamic Response Function (HRF), and the measurement times are sampled at a fixed Repetition Time (TR). Assume a Finite Impulse Response (FIR) basis representation of the HRF in which the HRF is expressed as a linear combination of non-overlapping indicator functions, each spanning one TR-wide lag bin.\n\nStart from the linear time-invariant convolution model and the General Linear Model (GLM) representation. Define the FIR basis functions as indicator functions that are shifted to each event onset and sampled at the scan times. Derive from these principles a mathematically precise expression for the FIR design matrix entry $X_{n,k}$, where $n$ indexes scans and $k$ indexes FIR lags.\n\nUse the following specifications to instantiate your derivation and then compute a specific entry:\n- The sampling interval (TR) is $\\Delta = 0.8$ seconds.\n- Scan $n$ occurs at time $t_{n} = n \\Delta$ with $n \\in \\{0,1,2,\\dots\\}$.\n- The FIR basis consists of lag bins indexed by $k \\in \\{0,1,\\dots,L-1\\}$, each defined by an indicator function $b_{k}(\\tau)$ that equals $1$ if $\\tau \\in [k\\Delta,(k+1)\\Delta)$ and $0$ otherwise.\n- Event onsets (in seconds) for a single condition are at times $t^{(e)} \\in \\{10.0,\\,10.4,\\,13.2\\}$.\n- The design matrix entry $X_{n,k}$ is defined as the sum over all events of the indicator function evaluated at the peristimulus time $t_{n} - t^{(e)}$.\n\nUsing these definitions, derive the general expression for $X_{n,k}$ as a function of $\\{t^{(e)}\\}$, $\\Delta$, $n$, and $k$. Then, for the specific scan index $n = 15$ and lag index $k = 2$, compute the value of $X_{n,k}$. Express your final answer as a single dimensionless number. No rounding is required.",
            "solution": "The problem asks for the derivation of a general expression for a Finite Impulse Response (FIR) design matrix element, $X_{n,k}$, and the computation of its specific value given a set of experimental parameters. The problem is well-posed and scientifically grounded in the standard principles of fMRI data analysis using the General Linear Model (GLM).\n\nFirst, we establish the theoretical framework. The BOLD signal, $y(t)$, is modeled as the output of a linear time-invariant (LTI) system where a neural event time series, $s(t)$, is convolved with a Hemodynamic Response Function (HRF), $h(\\tau)$. Mathematically, this is expressed as:\n$$y(t) = (s * h)(t) = \\int_{-\\infty}^{\\infty} s(\\tau) h(t - \\tau) d\\tau$$\nFor event-related designs, the neural event time series $s(t)$ for a single condition is represented as a sum of Dirac delta functions, $\\delta(t)$, at the onset times $t^{(e)}$ of each event:\n$$s(t) = \\sum_{e} \\delta(t - t^{(e)})$$\nThe HRF, $h(\\tau)$, is modeled using a set of basis functions. In this case, an FIR basis set is used, where the HRF is a linear combination of non-overlapping indicator functions, $b_k(\\tau)$:\n$$h(\\tau) = \\sum_{k=0}^{L-1} \\beta_k b_k(\\tau)$$\nHere, $\\beta_k$ are the unknown weights, or parameters, to be estimated, and $L$ is the number of FIR lags.\n\nSubstituting the expressions for $s(t)$ and $h(\\tau)$ into the convolution integral gives the predicted signal at time $t$:\n$$y(t) = \\int_{-\\infty}^{\\infty} \\left( \\sum_{e} \\delta(\\tau - t^{(e)}) \\right) \\left( \\sum_{k=0}^{L-1} \\beta_k b_k(t - \\tau) \\right) d\\tau$$\nBy the linearity of the integral, we can interchange the summations and the integral:\n$$y(t) = \\sum_{k=0}^{L-1} \\beta_k \\sum_{e} \\int_{-\\infty}^{\\infty} \\delta(\\tau - t^{(e)}) b_k(t - \\tau) d\\tau$$\nApplying the sifting property of the Dirac delta function, which states $\\int f(\\tau)\\delta(\\tau-a)d\\tau = f(a)$, we get:\n$$y(t) = \\sum_{k=0}^{L-1} \\beta_k \\sum_{e} b_k(t - t^{(e)})$$\nThe fMRI signal is sampled at discrete time points, $t_n = n\\Delta$, where $n$ is the scan index and $\\Delta$ is the Repetition Time (TR). The predicted signal for scan $n$ is:\n$$y(t_n) = \\sum_{k=0}^{L-1} \\beta_k \\left( \\sum_{e} b_k(t_n - t^{(e)}) \\right)$$\nThe GLM represents the measured signal at each scan, $y_n$, as a linear combination of predictors (regressors) plus an error term:\n$$y_n = \\sum_{k=0}^{L-1} X_{n,k} \\beta_k + \\varepsilon_n$$\nBy comparing this canonical GLM equation with our derived expression for $y(t_n)$, we can identify the design matrix entry $X_{n,k}$ as the term multiplying its corresponding parameter $\\beta_k$. This confirms the definition provided in the problem statement:\n$$X_{n,k} = \\sum_{e} b_k(t_n - t^{(e)})$$\nThe problem defines the FIR basis function $b_k(\\tau)$ as an indicator function:\n$$b_k(\\tau) = \\begin{cases} 1 & \\text{if } k\\Delta \\le \\tau < (k+1)\\Delta \\\\ 0 & \\text{otherwise} \\end{cases}$$\nFor a term in the summation for $X_{n,k}$ to be $1$, the argument of the basis function, which is the peristimulus time $\\tau = t_n - t^{(e)}$, must fall within the interval $[k\\Delta, (k+1)\\Delta)$. Thus, $b_k(t_n - t^{(e)})$ is non-zero only if:\n$$k\\Delta \\le t_n - t^{(e)} < (k+1)\\Delta$$\nSubstituting $t_n = n\\Delta$, we obtain the condition on the event onset time $t^{(e)}$:\n$$k\\Delta \\le n\\Delta - t^{(e)} < (k+1)\\Delta$$\nThis compound inequality can be rearranged to isolate $t^{(e)}$:\n$$(n-k-1)\\Delta < t^{(e)} \\le (n-k)\\Delta$$\nThis is the general expression for the condition an event must satisfy. The value of $X_{n,k}$ is the total count of events whose onset times $t^{(e)}$ fall within this specific time window, which is determined by the scan index $n$, the lag index $k$, and the TR $\\Delta$. We can write the general expression for $X_{n,k}$ using the Iverson bracket notation, where $[P]$ is $1$ if proposition $P$ is true and $0$ otherwise:\n$$X_{n,k} = \\sum_{t^{(e)}} \\left[ k\\Delta \\le n\\Delta - t^{(e)} < (k+1)\\Delta \\right]$$\n\nNow, we compute the specific value of $X_{n,k}$ for $n=15$ and $k=2$.\nThe given parameters are:\n- Sampling interval (TR): $\\Delta = 0.8$ s.\n- Scan index: $n = 15$.\n- FIR lag index: $k = 2$.\n- Event onset times: $t^{(e)} \\in \\{10.0, 10.4, 13.2\\}$ s.\n\nFirst, we calculate the time of scan $n=15$:\n$$t_{15} = n \\Delta = 15 \\times 0.8 = 12.0 \\text{ s}$$\nNext, we determine the time interval for which the basis function $b_2(\\tau)$ is non-zero. With $k=2$ and $\\Delta=0.8$ s, the interval is:\n$$[k\\Delta, (k+1)\\Delta) = [2 \\times 0.8, (2+1) \\times 0.8) = [1.6, 2.4) \\text{ s}$$\nThe value of $X_{15,2}$ is the sum of $b_2(t_{15} - t^{(e)})$ over all events. We evaluate this for each event onset time $t^{(e)}$:\n1. For the event at $t^{(e_1)} = 10.0$ s:\n   The peristimulus time is $\\tau_1 = t_{15} - t^{(e_1)} = 12.0 - 10.0 = 2.0$ s.\n   We check if $\\tau_1$ is in the interval $[1.6, 2.4)$. Since $1.6 \\le 2.0 < 2.4$ is true, $b_2(2.0) = 1$.\n\n2. For the event at $t^{(e_2)} = 10.4$ s:\n   The peristimulus time is $\\tau_2 = t_{15} - t^{(e_2)} = 12.0 - 10.4 = 1.6$ s.\n   We check if $\\tau_2$ is in the interval $[1.6, 2.4)$. Since $1.6 \\le 1.6 < 2.4$ is true, $b_2(1.6) = 1$.\n\n3. For the event at $t^{(e_3)} = 13.2$ s:\n   The peristimulus time is $\\tau_3 = t_{15} - t^{(e_3)} = 12.0 - 13.2 = -1.2$ s.\n   We check if $\\tau_3$ is in the interval $[1.6, 2.4)$. Since $1.6 \\le -1.2 < 2.4$ is false, $b_2(-1.2) = 0$.\n\nFinally, we sum these values to find the design matrix entry:\n$$X_{15,2} = \\sum_{e} b_2(t_{15} - t^{(e)}) = 1 + 1 + 0 = 2$$\nThe value of the design matrix entry $X_{15,2}$ is $2$. This dimensionless number indicates that two events occurred at times such that they contribute to the predicted BOLD signal for scan $15$ in the time bin corresponding to the second FIR lag.",
            "answer": "$$\n\\boxed{2}\n$$"
        },
        {
            "introduction": "While FIR models are flexible, they can be statistically inefficient. A common alternative is to use a constrained basis set, such as a single canonical HRF. This exercise addresses a critical challenge with this approach: the potential mismatch between precise stimulus timing and the coarse temporal resolution of fMRI scans. You will use a Taylor series approximation to mathematically demonstrate how this misalignment can bias amplitude estimates and, more importantly, how including a temporal derivative basis function provides a first-order correction, thereby increasing model accuracy. ",
            "id": "4140259",
            "problem": "You are analyzing a single-trial Blood Oxygen Level Dependent (BOLD) response in functional Magnetic Resonance Imaging (fMRI), modeled as a linear time-invariant system with a differentiable Hemodynamic Response Function (HRF). Let the HRF be a smooth function $h(t)$ with continuous first derivative $h^{\\prime}(t)$, and assume the General Linear Model (GLM) holds with a single event at time $t=0$ producing a response of amplitude $\\beta$. The acquisition uses a repetition time (TR) of $\\Delta$, producing sampled observations at times $t_{n} = n \\Delta$ for $n = 0, 1, \\dots, N-1$, where $N$ is chosen to fully cover the effective support of the HRF. Due to millisecond stimulus timing precision and coarse TR, suppose there is a small onset misalignment $\\varepsilon$ with $0 < \\varepsilon < \\Delta/2$, so the noise-free sampled data are\n$$\ny_{n} = \\beta \\, h(t_{n} - \\varepsilon) \\quad \\text{for} \\quad n=0,1,\\dots,N-1.\n$$\n\nDefine the canonical HRF basis vector sampled at the TR grid as $x_{n} = h(t_{n})$ and the temporal derivative basis vector as $d_{n} = h^{\\prime}(t_{n})$. Consider two GLM designs:\n\n1. Single-regressor design using only the canonical basis vector $x$, with least-squares amplitude estimate\n$$\n\\hat{\\beta}_{x} = \\frac{\\langle x, y \\rangle}{\\langle x, x \\rangle},\n$$\nwhere the discrete inner product is $\\langle u, v \\rangle = \\sum_{n=0}^{N-1} u_{n} v_{n}$.\n\n2. Two-regressor design using both the canonical and temporal derivative basis vectors, with design matrix $X = [\\,x \\ \\ d\\,]$ and least-squares coefficient vector $\\hat{\\theta} = (\\hat{\\beta}_{0}, \\hat{\\beta}_{1})^{\\top}$ satisfying\n$$\n\\hat{\\theta} = \\arg\\min_{\\theta} \\|\\, y - X \\theta \\,\\|_{2}^{2}.\n$$\n\nUsing only the following foundations:\n- Linear time-invariant modeling of the BOLD signal as a convolution of stimulus with HRF and its sampling at the TR grid,\n- The General Linear Model (GLM) and least-squares estimation,\n- First-order Taylor expansion for small timing misalignment $h(t - \\varepsilon) \\approx h(t) - \\varepsilon h^{\\prime}(t)$,\n\nderive, to first order in $\\varepsilon$, the fractional amplitude bias when (i) only the canonical basis is used, and (ii) both the canonical and temporal derivative bases are used. Specifically, define the fractional amplitude biases\n$$\n\\delta_{1} = \\frac{\\hat{\\beta}_{x} - \\beta}{\\beta}\n\\quad \\text{and} \\quad\n\\delta_{2} = \\frac{\\hat{\\beta}_{0} - \\beta}{\\beta},\n$$\nand express both $\\delta_{1}$ and $\\delta_{2}$ as closed-form symbolic expressions in terms of $\\varepsilon$, $\\langle x, x \\rangle$, $\\langle x, d \\rangle$, and $\\langle d, d \\rangle$, neglecting all terms of order higher than $\\varepsilon$. You may assume $x$ and $d$ are linearly independent over the sampling window. Return your final answer as a row matrix containing $\\delta_{1}$ and $\\delta_{2}$. No numerical rounding is required, and no physical units are required in the final expressions.",
            "solution": "The problem as stated is scientifically grounded, well-posed, and contains all necessary information for a unique solution. The premises are based on standard methodologies in fMRI data analysis, specifically the General Linear Model (GLM) and the use of basis functions to account for signal variability. Therefore, the problem is valid, and we proceed to the derivation.\n\nThe core of the problem lies in analyzing the effect of a small timing misalignment, $\\varepsilon$, on the estimation of the BOLD response amplitude, $\\beta$. The noise-free sampled data are given by\n$$\ny_{n} = \\beta \\, h(t_{n} - \\varepsilon)\n$$\nwhere $t_n = n\\Delta$. The problem specifies using a first-order Taylor expansion for $h(t - \\varepsilon)$ around $t$:\n$$\nh(t - \\varepsilon) \\approx h(t) - \\varepsilon h^{\\prime}(t)\n$$\nSubstituting this into the expression for $y_n$ and using the definitions of the basis vectors $x_n = h(t_n)$ and $d_n = h^{\\prime}(t_n)$, we obtain a first-order approximation for the data vector $y$:\n$$\ny_{n} \\approx \\beta (x_n - \\varepsilon d_n)\n$$\nIn vector notation, this can be written as:\n$$\ny \\approx \\beta (x - \\varepsilon d)\n$$\nThis approximation is accurate up to terms of order $\\varepsilon$. We will use this expression for $y$ to derive the estimators and their biases.\n\n**Part (i): Single-regressor design**\n\nIn this design, only the canonical HRF basis vector $x$ is used as a regressor. The least-squares estimate of the amplitude is given by:\n$$\n\\hat{\\beta}_{x} = \\frac{\\langle x, y \\rangle}{\\langle x, x \\rangle}\n$$\nwhere $\\langle u, v \\rangle = \\sum_{n=0}^{N-1} u_{n} v_{n}$ is the discrete inner product. We substitute the first-order approximation for $y$:\n$$\n\\hat{\\beta}_{x} \\approx \\frac{\\langle x, \\beta (x - \\varepsilon d) \\rangle}{\\langle x, x \\rangle}\n$$\nUsing the linearity of the inner product, we can expand the numerator:\n$$\n\\langle x, \\beta (x - \\varepsilon d) \\rangle = \\beta \\langle x, x - \\varepsilon d \\rangle = \\beta (\\langle x, x \\rangle - \\varepsilon \\langle x, d \\rangle)\n$$\nSubstituting this back into the expression for $\\hat{\\beta}_{x}$:\n$$\n\\hat{\\beta}_{x} \\approx \\frac{\\beta (\\langle x, x \\rangle - \\varepsilon \\langle x, d \\rangle)}{\\langle x, x \\rangle} = \\beta \\left( \\frac{\\langle x, x \\rangle}{\\langle x, x \\rangle} - \\varepsilon \\frac{\\langle x, d \\rangle}{\\langle x, x \\rangle} \\right) = \\beta \\left( 1 - \\varepsilon \\frac{\\langle x, d \\rangle}{\\langle x, x \\rangle} \\right)\n$$\nThe fractional amplitude bias $\\delta_1$ is defined as $\\delta_{1} = \\frac{\\hat{\\beta}_{x} - \\beta}{\\beta}$. Using our result for $\\hat{\\beta}_{x}$:\n$$\n\\delta_{1} \\approx \\frac{\\beta \\left( 1 - \\varepsilon \\frac{\\langle x, d \\rangle}{\\langle x, x \\rangle} \\right) - \\beta}{\\beta} = \\frac{\\beta - \\beta \\varepsilon \\frac{\\langle x, d \\rangle}{\\langle x, x \\rangle} - \\beta}{\\beta} = \\frac{-\\beta \\varepsilon \\frac{\\langle x, d \\rangle}{\\langle x, x \\rangle}}{\\beta}\n$$\nThis simplifies to the first-order fractional bias:\n$$\n\\delta_{1} = -\\varepsilon \\frac{\\langle x, d \\rangle}{\\langle x, x \\rangle}\n$$\n\n**Part (ii): Two-regressor design**\n\nIn this design, both the canonical basis vector $x$ and its temporal derivative $d$ are used. The design matrix is $X = [\\,x \\ \\ d\\,]$, and the vector of coefficients is $\\theta = (\\beta_0, \\beta_1)^{\\top}$. The least-squares estimate $\\hat{\\theta}$ is given by the normal equations:\n$$\n\\hat{\\theta} = (X^{\\top}X)^{-1} X^{\\top}y\n$$\nFirst, we construct the matrix $X^{\\top}X$:\n$$\nX^{\\top}X = \\begin{pmatrix} x^{\\top} \\\\ d^{\\top} \\end{pmatrix} [\\,x \\ \\ d\\,] = \\begin{pmatrix} x^{\\top}x & x^{\\top}d \\\\ d^{\\top}x & d^{\\top}d \\end{pmatrix} = \\begin{pmatrix} \\langle x, x \\rangle & \\langle x, d \\rangle \\\\ \\langle x, d \\rangle & \\langle d, d \\rangle \\end{pmatrix}\n$$\nThe inverse of this $2 \\times 2$ matrix is:\n$$\n(X^{\\top}X)^{-1} = \\frac{1}{\\langle x, x \\rangle \\langle d, d \\rangle - \\langle x, d \\rangle^{2}} \\begin{pmatrix} \\langle d, d \\rangle & -\\langle x, d \\rangle \\\\ -\\langle x, d \\rangle & \\langle x, x \\rangle \\end{pmatrix}\n$$\nThe determinant in the denominator is non-zero because $x$ and $d$ are assumed to be linearly independent.\n\nNext, we compute the vector $X^{\\top}y$ using our approximation $y \\approx \\beta(x - \\varepsilon d)$:\n$$\nX^{\\top}y = \\begin{pmatrix} \\langle x, y \\rangle \\\\ \\langle d, y \\rangle \\end{pmatrix} \\approx \\begin{pmatrix} \\langle x, \\beta(x - \\varepsilon d) \\rangle \\\\ \\langle d, \\beta(x - \\varepsilon d) \\rangle \\end{pmatrix} = \\beta \\begin{pmatrix} \\langle x, x \\rangle - \\varepsilon \\langle x, d \\rangle \\\\ \\langle x, d \\rangle - \\varepsilon \\langle d, d \\rangle \\end{pmatrix}\n$$\nNow we compute $\\hat{\\theta} = (\\hat{\\beta}_{0}, \\hat{\\beta}_{1})^{\\top}$ by multiplying $(X^{\\top}X)^{-1}$ and $X^{\\top}y$. We are interested in $\\hat{\\beta}_0$, the first component of $\\hat{\\theta}$:\n$$\n\\hat{\\beta}_{0} \\approx \\frac{\\beta}{\\langle x, x \\rangle \\langle d, d \\rangle - \\langle x, d \\rangle^{2}} \\left[ \\langle d, d \\rangle (\\langle x, x \\rangle - \\varepsilon \\langle x, d \\rangle) - \\langle x, d \\rangle (\\langle x, d \\rangle - \\varepsilon \\langle d, d \\rangle) \\right]\n$$\nLet's expand the terms in the square brackets:\n$$\n\\left[ \\langle d, d \\rangle\\langle x, x \\rangle - \\varepsilon \\langle d, d \\rangle\\langle x, d \\rangle - \\langle x, d \\rangle^{2} + \\varepsilon \\langle x, d \\rangle\\langle d, d \\rangle \\right]\n$$\nThe two terms containing $\\varepsilon$ are $-\\varepsilon \\langle d, d \\rangle\\langle x, d \\rangle$ and $+\\varepsilon \\langle x, d \\rangle\\langle d, d \\rangle$. They cancel each other out. The expression in the square brackets simplifies to:\n$$\n\\langle x, x \\rangle \\langle d, d \\rangle - \\langle x, d \\rangle^{2}\n$$\nThis is precisely the determinant of $X^{\\top}X$. Substituting this back into the expression for $\\hat{\\beta}_0$:\n$$\n\\hat{\\beta}_{0} \\approx \\frac{\\beta \\left( \\langle x, x \\rangle \\langle d, d \\rangle - \\langle x, d \\rangle^{2} \\right)}{\\langle x, x \\rangle \\langle d, d \\rangle - \\langle x, d \\rangle^{2}} = \\beta\n$$\nThis result shows that $\\hat{\\beta}_0$ is an unbiased estimator of $\\beta$ up to first order in $\\varepsilon$. The fractional amplitude bias $\\delta_2$ is defined as $\\delta_{2} = \\frac{\\hat{\\beta}_{0} - \\beta}{\\beta}$.\n$$\n\\delta_{2} \\approx \\frac{\\beta - \\beta}{\\beta} = 0\n$$\nAny residual bias in $\\hat{\\beta}_0$ must be of order $\\varepsilon^2$ or higher, which is neglected per the problem statement. Thus, to first order in $\\varepsilon$, the fractional bias is zero.\n\nIn summary, for the single-regressor model, the fractional bias is $\\delta_{1} = -\\varepsilon \\frac{\\langle x, d \\rangle}{\\langle x, x \\rangle}$. For the two-regressor model, the fractional bias is $\\delta_{2} = 0$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} -\\varepsilon \\frac{\\langle x, d \\rangle}{\\langle x, x \\rangle} & 0 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Having established the value of including derivative bases for model accuracy, we now turn to a finer point: the interpretability of the resulting model parameters. When basis regressors are correlated, it can be difficult to uniquely attribute signal variance to each one. This advanced problem delves into the linear algebra of the GLM to show how orthogonalizing the derivative regressors with respect to the canonical HRF clarifies the meaning of the amplitude coefficient, isolating it from the effects of small timing or dispersion variations. ",
            "id": "4140212",
            "problem": "Consider a discrete-time General Linear Model (GLM) for a single-voxel blood-oxygen-level-dependent time series in functional magnetic resonance imaging, in which the Hemodynamic Response Function (HRF) is modeled by a canonical basis and its two derivatives. Let the original design matrix be $X = [\\mathbf{h}\\ \\ \\mathbf{d}\\ \\ \\mathbf{q}] \\in \\mathbb{R}^{n \\times 3}$, where $\\mathbf{h} \\in \\mathbb{R}^{n}$ is the canonical HRF regressor, $\\mathbf{d} \\in \\mathbb{R}^{n}$ is a temporal derivative regressor, and $\\mathbf{q} \\in \\mathbb{R}^{n}$ is a dispersion derivative regressor, all evaluated on the same $n$ time points. Assume $X$ has full column rank.\n\nDefine the residualized (orthogonalized-to-canonical) derivative regressors by projecting each derivative onto the orthogonal complement of $\\mathbf{h}$:\n$$\n\\mathbf{d}_{\\perp} \\equiv \\mathbf{d} - \\mathbf{h}\\,\\frac{\\mathbf{h}^{\\top}\\mathbf{d}}{\\mathbf{h}^{\\top}\\mathbf{h}}, \n\\qquad\n\\mathbf{q}_{\\perp} \\equiv \\mathbf{q} - \\mathbf{h}\\,\\frac{\\mathbf{h}^{\\top}\\mathbf{q}}{\\mathbf{h}^{\\top}\\mathbf{h}}.\n$$\nLet the residualized design be $\\widetilde{X} = [\\mathbf{h}\\ \\ \\mathbf{d}_{\\perp}\\ \\ \\mathbf{q}_{\\perp}]$, and consider fitting the same GLM response $\\mathbf{y} \\in \\mathbb{R}^{n}$ with either $X$ or $\\widetilde{X}$ using ordinary least squares.\n\nStarting only from the defining properties of least squares, orthogonal projection, and inner products in Euclidean space, do the following:\n\n1. Explain, via the normal equations and projection geometry, how the coefficient multiplying $\\mathbf{h}$ in the residualized model, say $\\gamma_{0}$ from $\\mathbf{y} = \\widetilde{X}\\,\\boldsymbol{\\gamma} + \\boldsymbol{\\varepsilon}$, attains an interpretation as the isolated amplitude of the canonical HRF under small timing and dispersion deviations. Your explanation must rely only on orthogonality relations implied by the definitions above and on a first-order Taylor linearization of modest timing or width perturbations.\n\n2. Derive the exact back-transformation that maps coefficients from the residualized parameterization to the original parameterization. That is, determine the unique $3\\times 3$ matrix $T$ such that for any $\\boldsymbol{\\gamma} \\in \\mathbb{R}^{3}$ there exists $\\boldsymbol{\\beta} \\in \\mathbb{R}^{3}$ with $X\\,\\boldsymbol{\\beta} = \\widetilde{X}\\,\\boldsymbol{\\gamma}$ and $\\boldsymbol{\\beta} = T\\,\\boldsymbol{\\gamma}$. Express your final result in closed form using only the inner products $s_{00} \\equiv \\mathbf{h}^{\\top}\\mathbf{h}$, $s_{01} \\equiv \\mathbf{h}^{\\top}\\mathbf{d}$, and $s_{02} \\equiv \\mathbf{h}^{\\top}\\mathbf{q}$.\n\nYour final answer must be the single, closed-form analytic expression for the $3\\times 3$ matrix $T$.",
            "solution": "The problem as stated is scientifically grounded, well-posed, and contains all necessary information for a unique solution. It describes a standard procedure in functional neuroimaging data analysis, namely the use of a basis set for the Hemodynamic Response Function (HRF) within a General Linear Model (GLM) and the orthogonalization of basis regressors to improve the interpretability of regression coefficients. The derivation relies on fundamental principles of linear algebra, least-squares estimation, and calculus. We may therefore proceed with a solution.\n\nThe problem is divided into two parts. The first part requires an explanation of the interpretation of the coefficient for the canonical HRF regressor in a model with orthogonalized derivatives. The second part requires the derivation of a transformation matrix between the coefficients of the original and the residualized models.\n\n**Part 1: Interpretation of the Coefficient $\\gamma_0$**\n\nThe model using the residualized design matrix $\\widetilde{X} = [\\mathbf{h}\\ \\ \\mathbf{d}_{\\perp}\\ \\ \\mathbf{q}_{\\perp}]$ is given by:\n$$ \\mathbf{y} = \\widetilde{X}\\boldsymbol{\\gamma} + \\boldsymbol{\\varepsilon} = \\gamma_{0}\\mathbf{h} + \\gamma_{1}\\mathbf{d}_{\\perp} + \\gamma_{2}\\mathbf{q}_{\\perp} + \\boldsymbol{\\varepsilon} $$\nThe ordinary least squares (OLS) estimate $\\hat{\\boldsymbol{\\gamma}}$ is found by solving the normal equations:\n$$ (\\widetilde{X}^{\\top}\\widetilde{X})\\hat{\\boldsymbol{\\gamma}} = \\widetilde{X}^{\\top}\\mathbf{y} $$\nThe key to interpreting $\\hat{\\gamma}_{0}$ lies in the structure of the Gram matrix $\\widetilde{X}^{\\top}\\widetilde{X}$. The regressors $\\mathbf{d}_{\\perp}$ and $\\mathbf{q}_{\\perp}$ are constructed by projecting $\\mathbf{d}$ and $\\mathbf{q}$ onto the orthogonal complement of the subspace spanned by $\\mathbf{h}$. By definition, this makes them orthogonal to $\\mathbf{h}$. We can verify this using the definition of the inner product in $\\mathbb{R}^{n}$ as $\\mathbf{u}^{\\top}\\mathbf{v}$:\n$$ \\mathbf{h}^{\\top}\\mathbf{d}_{\\perp} = \\mathbf{h}^{\\top}\\left(\\mathbf{d} - \\mathbf{h}\\,\\frac{\\mathbf{h}^{\\top}\\mathbf{d}}{\\mathbf{h}^{\\top}\\mathbf{h}}\\right) = \\mathbf{h}^{\\top}\\mathbf{d} - (\\mathbf{h}^{\\top}\\mathbf{h})\\frac{\\mathbf{h}^{\\top}\\mathbf{d}}{\\mathbf{h}^{\\top}\\mathbf{h}} = \\mathbf{h}^{\\top}\\mathbf{d} - \\mathbf{h}^{\\top}\\mathbf{d} = 0 $$\nSimilarly, $\\mathbf{h}^{\\top}\\mathbf{q}_{\\perp} = 0$. This orthogonality simplifies the Gram matrix:\n$$ \\widetilde{X}^{\\top}\\widetilde{X} = \\begin{pmatrix} \\mathbf{h}^{\\top}\\mathbf{h} & \\mathbf{h}^{\\top}\\mathbf{d}_{\\perp} & \\mathbf{h}^{\\top}\\mathbf{q}_{\\perp} \\\\ \\mathbf{d}_{\\perp}^{\\top}\\mathbf{h} & \\mathbf{d}_{\\perp}^{\\top}\\mathbf{d}_{\\perp} & \\mathbf{d}_{\\perp}^{\\top}\\mathbf{q}_{\\perp} \\\\ \\mathbf{q}_{\\perp}^{\\top}\\mathbf{h} & \\mathbf{q}_{\\perp}^{\\top}\\mathbf{d}_{\\perp} & \\mathbf{q}_{\\perp}^{\\top}\\mathbf{q}_{\\perp} \\end{pmatrix} = \\begin{pmatrix} \\mathbf{h}^{\\top}\\mathbf{h} & 0 & 0 \\\\ 0 & \\mathbf{d}_{\\perp}^{\\top}\\mathbf{d}_{\\perp} & \\mathbf{d}_{\\perp}^{\\top}\\mathbf{q}_{\\perp} \\\\ 0 & \\mathbf{q}_{\\perp}^{\\top}\\mathbf{d}_{\\perp} & \\mathbf{q}_{\\perp}^{\\top}\\mathbf{q}_{\\perp} \\end{pmatrix} $$\nThe normal equations thus become:\n$$ \\begin{pmatrix} \\mathbf{h}^{\\top}\\mathbf{h} & 0 & 0 \\\\ 0 & \\mathbf{d}_{\\perp}^{\\top}\\mathbf{d}_{\\perp} & \\mathbf{d}_{\\perp}^{\\top}\\mathbf{q}_{\\perp} \\\\ 0 & \\mathbf{q}_{\\perp}^{\\top}\\mathbf{d}_{\\perp} & \\mathbf{q}_{\\perp}^{\\top}\\mathbf{q}_{\\perp} \\end{pmatrix} \\begin{pmatrix} \\hat{\\gamma}_{0} \\\\ \\hat{\\gamma}_{1} \\\\ \\hat{\\gamma}_{2} \\end{pmatrix} = \\begin{pmatrix} \\mathbf{h}^{\\top}\\mathbf{y} \\\\ \\mathbf{d}_{\\perp}^{\\top}\\mathbf{y} \\\\ \\mathbf{q}_{\\perp}^{\\top}\\mathbf{y} \\end{pmatrix} $$\nThe block-diagonal structure of the matrix decouples the equation for $\\hat{\\gamma}_{0}$ from the equations for $\\hat{\\gamma}_{1}$ and $\\hat{\\gamma}_{2}$:\n$$ (\\mathbf{h}^{\\top}\\mathbf{h})\\hat{\\gamma}_{0} = \\mathbf{h}^{\\top}\\mathbf{y} \\implies \\hat{\\gamma}_{0} = \\frac{\\mathbf{h}^{\\top}\\mathbf{y}}{\\mathbf{h}^{\\top}\\mathbf{h}} $$\nThis is the same formula for the coefficient that would be obtained from a simple regression of $\\mathbf{y}$ on only $\\mathbf{h}$. Geometrically, $\\hat{\\gamma}_{0}\\mathbf{h}$ is the orthogonal projection of the data vector $\\mathbf{y}$ onto the one-dimensional subspace spanned by $\\mathbf{h}$. Because the other regressors $\\mathbf{d}_{\\perp}$ and $\\mathbf{q}_{\\perp}$ are orthogonal to $\\mathbf{h}$, they do not contribute to or confound this projection.\n\nThe interpretative insight comes from the connection to Taylor series linearization. The canonical HRF, say $f(t)$, is modeled by the regressor $\\mathbf{h}$. A small delay $\\delta_t$ in the neural response would result in an HRF of the form $f(t-\\delta_t)$. A first-order Taylor expansion around $\\delta_t=0$ gives:\n$$ f(t-\\delta_t) \\approx f(t) - \\delta_t f'(t) $$\nThe temporal derivative regressor $\\mathbf{d}$ is constructed to be proportional to $f'(t)$. Thus, a linear combination of $\\mathbf{h}$ and $\\mathbf{d}$ can approximate a time-shifted canonical HRF. Similarly, the dispersion derivative $\\mathbf{q}$ is related to the partial derivative of $f$ with respect to its width parameter, allowing the basis set to model HRFs of slightly different widths.\nIn the original model with design matrix $X$, the regressors $\\mathbf{h}$, $\\mathbf{d}$, and $\\mathbf{q}$ are typically correlated. This means the OLS estimate for the amplitude of $\\mathbf{h}$ (i.e., $\\beta_0$) will be confounded by variance that could also be explained by a time-shift or dispersion change. It does not represent a \"pure\" amplitude.\nBy orthogonalizing the derivatives with respect to the canonical shape, the residualized model separates these effects. The coefficient $\\hat{\\gamma}_{0}$ quantifies the magnitude of the response that projects onto the exact canonical HRF shape. The coefficients $\\hat{\\gamma}_{1}$ and $\\hat{\\gamma}_{2}$ quantify the additional contributions from variations in timing and dispersion, respectively, that are orthogonal to the canonical form. Therefore, $\\gamma_0$ attains an interpretation as the isolated amplitude of the canonical HRF, uncontaminated by small deviations in latency or duration of the response.\n\n**Part 2: Derivation of the Transformation Matrix $T$**\n\nWe seek a $3 \\times 3$ matrix $T$ such that for any $\\boldsymbol{\\gamma} \\in \\mathbb{R}^{3}$, the equality of the fitted responses $X\\boldsymbol{\\beta} = \\widetilde{X}\\boldsymbol{\\gamma}$ implies a linear relationship $\\boldsymbol{\\beta} = T\\boldsymbol{\\gamma}$.\nThe equality of the fitted responses can be written as:\n$$ \\beta_0 \\mathbf{h} + \\beta_1 \\mathbf{d} + \\beta_2 \\mathbf{q} = \\gamma_0 \\mathbf{h} + \\gamma_1 \\mathbf{d}_{\\perp} + \\gamma_2 \\mathbf{q}_{\\perp} $$\nWe substitute the definitions of the residualized regressors, $\\mathbf{d}_{\\perp}$ and $\\mathbf{q}_{\\perp}$, into the right-hand side. Using the provided notation $s_{00} = \\mathbf{h}^{\\top}\\mathbf{h}$, $s_{01} = \\mathbf{h}^{\\top}\\mathbf{d}$, and $s_{02} = \\mathbf{h}^{\\top}\\mathbf{q}$:\n$$ \\mathbf{d}_{\\perp} = \\mathbf{d} - \\mathbf{h}\\,\\frac{s_{01}}{s_{00}} $$\n$$ \\mathbf{q}_{\\perp} = \\mathbf{q} - \\mathbf{h}\\,\\frac{s_{02}}{s_{00}} $$\nSubstituting these into the right-hand side of the vector equality:\n$$ \\text{RHS} = \\gamma_0 \\mathbf{h} + \\gamma_1 \\left(\\mathbf{d} - \\mathbf{h}\\,\\frac{s_{01}}{s_{00}}\\right) + \\gamma_2 \\left(\\mathbf{q} - \\mathbf{h}\\,\\frac{s_{02}}{s_{00}}\\right) $$\nWe now collect terms multiplying the basis vectors $\\mathbf{h}$, $\\mathbf{d}$, and $\\mathbf{q}$:\n$$ \\text{RHS} = \\left(\\gamma_0 - \\gamma_1 \\frac{s_{01}}{s_{00}} - \\gamma_2 \\frac{s_{02}}{s_{00}}\\right)\\mathbf{h} + (\\gamma_1)\\mathbf{d} + (\\gamma_2)\\mathbf{q} $$\nThe left-hand side is $\\text{LHS} = \\beta_0 \\mathbf{h} + \\beta_1 \\mathbf{d} + \\beta_2 \\mathbf{q}$.\nBy equating the left and right sides, we have two representations of the same vector in the basis given by the columns of $X$. Since $X$ is assumed to have full column rank, its columns $\\{\\mathbf{h}, \\mathbf{d}, \\mathbf{q}\\}$ are linearly independent. Therefore, the coefficients of the basis vectors must be equal:\n$$ \\beta_0 = \\gamma_0 - \\gamma_1 \\frac{s_{01}}{s_{00}} - \\gamma_2 \\frac{s_{02}}{s_{00}} $$\n$$ \\beta_1 = \\gamma_1 $$\n$$ \\beta_2 = \\gamma_2 $$\nThis system of linear equations relates the components of $\\boldsymbol{\\beta}$ to the components of $\\boldsymbol{\\gamma}$. We can express this in matrix form as $\\boldsymbol{\\beta} = T\\boldsymbol{\\gamma}$:\n$$ \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\end{pmatrix} = \\begin{pmatrix} 1 & -\\frac{s_{01}}{s_{00}} & -\\frac{s_{02}}{s_{00}} \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} \\gamma_0 \\\\ \\gamma_1 \\\\ \\gamma_2 \\end{pmatrix} $$\nFrom this, we can identify the transformation matrix $T$ as required.\n$$ T = \\begin{pmatrix} 1 & -\\frac{s_{01}}{s_{00}} & -\\frac{s_{02}}{s_{00}} \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} $$\nThis matrix $T$ provides the exact back-transformation from the coefficients of the residualized model ($\\boldsymbol{\\gamma}$) to the coefficients of the original model ($\\boldsymbol{\\beta}$).",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1 & -\\frac{s_{01}}{s_{00}} & -\\frac{s_{02}}{s_{00}} \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix}\n}\n$$"
        }
    ]
}