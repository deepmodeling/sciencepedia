{
    "hands_on_practices": [
        {
            "introduction": "A crucial first step in any saturation genome editing (SGE) experiment is defining its scope by designing a comprehensive variant library. This foundational exercise guides you through the combinatorial calculation of the required library size, a critical parameter that directly impacts experimental feasibility and cost. Mastering this calculation is essential for planning a robust and well-powered screen. ",
            "id": "4329417",
            "problem": "A research team is designing a saturation genome editing (SGE) library using Clustered Regularly Interspaced Short Palindromic Repeats (CRISPR) to map variant effects across a single coding exon. The exon has length $L$ measured in base pairs, and the design objective is to include every possible single-nucleotide substitution at every position in the exon. In this context, a single-nucleotide substitution is defined as changing exactly one base at one genomic position to any other base in the canonical deoxyribonucleic acid (DNA) alphabet $\\{ \\mathrm{A}, \\mathrm{C}, \\mathrm{G}, \\mathrm{T} \\}$, excluding the original base at that position. The team also requires redundancy, meaning each distinct variant must be represented by $r$ independently synthesized donor constructs (for example, unique molecularly barcoded oligonucleotides), so that experimental stochasticity does not underpower detection of variant effects.\n\nUsing only the following fundamental bases and definitions:\n- The Central Dogma of Molecular Biology establishes that coding exons are stretches of DNA that encode messenger ribonucleic acid (mRNA) and ultimately protein, with sequence measured in base pairs.\n- The DNA alphabet has cardinality $4$, and a single-nucleotide substitution at a position replaces the current base with any other base, yielding $4 - 1$ possible substitutions per position.\n- The rule of product from combinatorics states that if one process can occur in $a$ ways and independently another in $b$ ways, the combined process can occur in $a \\times b$ ways.\n\nAssume there are no insertions, deletions, or multi-nucleotide changes, and do not include any wild-type controls or additional design elements beyond the variants and their redundancy. Derive a symbolic expression, in simplest closed form, for the total number of distinct constructs required in the library as a function of $L$ and $r$. Express your final answer as a single analytic expression. No rounding is required, and no physical units should be included in the final expression.",
            "solution": "The user-provided problem has been analyzed and validated against the specified criteria.\n\n### Step 1: Extract Givens\n- Exon length: $L$ base pairs.\n- Library design objective: Include every possible single-nucleotide substitution at every position in the exon.\n- Definition of a single-nucleotide substitution: Changing one base at one position to any of the other $3$ canonical DNA bases. The DNA alphabet is $\\{ \\mathrm{A}, \\mathrm{C}, \\mathrm{G}, \\mathrm{T} \\}$.\n- Redundancy: Each distinct variant is represented by $r$ independently synthesized donor constructs.\n- Fundamental principles provided: The Central Dogma of Molecular Biology regarding coding exons; the DNA alphabet cardinality is $4$, leading to $4-1$ substitutions per position; the rule of product from combinatorics.\n- Exclusions: No insertions, deletions, multi-nucleotide changes, wild-type controls, or other design elements.\n- Goal: Derive a symbolic expression for the total number of distinct constructs required, as a function of $L$ and $r$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated as follows:\n- **Scientifically Grounded**: The problem is based on established principles of molecular biology, genetics, and CRISPR technology (specifically, saturation genome editing). The definitions provided for a coding exon, single-nucleotide substitution, and the DNA alphabet are factually correct and standard in the field.\n- **Well-Posed**: The problem is clearly defined. It provides all necessary parameters ($L$ and $r$) and constraints to derive a unique mathematical expression for the total number of library constructs.\n- **Objective**: The language is precise, technical, and free of subjective or ambiguous terminology.\n\nBased on this evaluation, the problem is deemed valid and solvable.\n\n### Step 3: Derivation of the Solution\nThe objective is to calculate the total number of constructs, $N$, required for a saturation genome editing library targeting an exon of length $L$ with a redundancy of $r$.\n\n1.  **Determine the number of possible unique variants.**\n    A unique variant is defined by a single-nucleotide substitution. To specify such a variant, we must make two independent choices:\n    a.  **The position of the substitution:** The exon has a length of $L$ base pairs. Therefore, there are $L$ distinct positions where a substitution can occur.\n    b.  **The identity of the substituted nucleotide:** At any given position, the DNA alphabet consists of $4$ bases. A substitution requires changing the original base to one of the other bases. This leaves $4 - 1 = 3$ possible choices for the new nucleotide at that specific position.\n\n2.  **Apply the rule of product.**\n    The problem provides the rule of product: if one process can occur in $a$ ways and an independent process can occur in $b$ ways, the combined process can occur in $a \\times b$ ways.\n    In this context, the process of creating a unique variant involves choosing a position ($L$ ways) and choosing a substitution ($3$ ways).\n    Let $V$ be the total number of distinct single-nucleotide variants possible for the exon. Applying the rule of product:\n    $$V = (\\text{Number of positions}) \\times (\\text{Number of substitutions per position})$$\n    $$V = L \\times 3 = 3L$$\n    So, there are $3L$ unique variants that must be included in the library.\n\n3.  **Incorporate the redundancy factor.**\n    The problem states that each distinct variant must be represented by $r$ independently synthesized constructs. This redundancy is designed to ensure robust detection of variant effects by overcoming experimental noise and stochasticity.\n    Let $N$ be the total number of constructs in the library. To calculate $N$, we multiply the total number of distinct variants, $V$, by the redundancy factor, $r$.\n    $$N = V \\times r$$\n    Substituting the expression for $V$:\n    $$N = (3L) \\times r = 3Lr$$\n\nThis expression represents the total number of constructs required for the library as a function of the exon length $L$ and the redundancy factor $r$. It is in its simplest closed form.",
            "answer": "$$\\boxed{3Lr}$$"
        },
        {
            "introduction": "Raw data from SGE experiments contain not only the biological signal of interest but also systematic noise from experimental artifacts, such as guide-specific cutting efficiency and locus-specific toxicity. This practice demonstrates how to de-confound these effects by constructing a linear model that leverages carefully designed controls, allowing for the computational isolation of a variant's true fitness effect. This approach is central to obtaining accurate and reliable variant effect maps. ",
            "id": "4329401",
            "problem": "A pool-based variant effect mapping experiment using Clustered Regularly Interspaced Short Palindromic Repeats (CRISPR) and Homology-Directed Repair (HDR) is performed to quantify the fitness impact of all single-nucleotide variants in a clinically actionable gene. The design tiles $K$ distinct CRISPR guide ribonucleoproteins across a locus $L$, and for each guide $k \\in \\{1,\\dots,K\\}$ and each biological replicate $r \\in \\{1,\\dots,R\\}$, two HDR templates are delivered: one template installs a specific variant $v$; the other template restores the exact wild-type sequence (the matched cut-and-repair control). After a competition of duration $T$, deep sequencing yields frequencies at time $0$ and time $T$ for each edited allele. For each $(k,r)$, define the measured log fold change for variant $v$ as\n$$\ny_{v,k,r} \\equiv \\ln\\!\\left(\\frac{f_{v,k,r}(T)}{f_{v,k,r}(0)}\\right),\n$$\nand for the matched wild-type HDR control as\n$$\ny_{w,k,r} \\equiv \\ln\\!\\left(\\frac{f_{w,k,r}(T)}{f_{w,k,r}(0)}\\right).\n$$\nAssume a pooled competition regime in which the expected change in log frequency over time for any allele is the sum of a variant-specific Malthusian fitness effect and shared components that arise from double-strand break (DSB) cutting at locus $L$ and replicate-level drift. Concretely, suppose that for each $(k,r)$,\n$$\ny_{v,k,r} \\;=\\; s_v\\,T \\;+\\; c_L\\,T \\;+\\; d_k\\,T \\;+\\; m_r\\,T \\;+\\; \\varepsilon_{v,k,r},\n$$\nand\n$$\ny_{w,k,r} \\;=\\; 0\\cdot T \\;+\\; c_L\\,T \\;+\\; d_k\\,T \\;+\\; m_r\\,T \\;+\\; \\varepsilon_{w,k,r},\n$$\nwhere $s_v$ is the true variant-specific fitness effect per unit time, $c_L$ is the locus-specific cutting toxicity per unit time (shared across all guides at locus $L$), $d_k$ is the guide-specific additive cutting toxicity per unit time, $m_r$ is the replicate-level drift per unit time, and $\\varepsilon_{v,k,r}$ and $\\varepsilon_{w,k,r}$ are independent, zero-mean noise terms. For each $(k,r)$, define a known inverse-variance weight\n$$\nw_{k,r} \\equiv \\frac{1}{\\operatorname{Var}\\!\\left(y_{v,k,r} - y_{w,k,r}\\right)},\n$$\nreflecting heteroscedastic measurement uncertainty across guides and replicates, and assume $\\varepsilon_{v,k,r}$ and $\\varepsilon_{w,k,r}$ are Gaussian with finite variances.\n\nStarting from the definition of Malthusian fitness in pooled competitions and the additive decomposition above, derive a single closed-form analytic expression for a minimum-variance unbiased linear estimator of the true variant-specific fitness $s_v$ that separates the variant effect from locus-specific and guide-specific cutting toxicities by leveraging the matched HDR wild-type controls and modeling shared components. Your final expression should be written entirely in terms of $T$, the weights $w_{k,r}$, and the measured quantities $y_{v,k,r}$ and $y_{w,k,r}$. The answer should be an analytical expression; do not provide any numerical evaluation. If you introduce any additional symbols, define them precisely. Express the final fitness effect per unit time (no unit needs to be printed in the final expression). No rounding is required.",
            "solution": "The problem requires the derivation of a minimum-variance unbiased linear estimator for the true variant-specific fitness effect, $s_v$. This is also known as the Best Linear Unbiased Estimator (BLUE). The derivation proceeds by first simplifying the given statistical model to isolate the parameter of interest, $s_v$, and then applying the principles of optimal estimation for linear models with heteroscedastic noise.\n\nLet us begin by analyzing the provided models for the log fold changes:\n$$\ny_{v,k,r} \\;=\\; s_v\\,T \\;+\\; c_L\\,T \\;+\\; d_k\\,T \\;+\\; m_r\\,T \\;+\\; \\varepsilon_{v,k,r}\n$$\n$$\ny_{w,k,r} \\;=\\; 0\\cdot T \\;+\\; c_L\\,T \\;+\\; d_k\\,T \\;+\\; m_r\\,T \\;+\\; \\varepsilon_{w,k,r}\n$$\nThe experimental design, which includes a matched wild-type control for each variant measurement, is crucial. By taking the difference between the log fold change for the variant and its corresponding wild-type control for each guide $k$ and replicate $r$, we can eliminate the shared nuisance parameters. Let us define this difference as $\\Delta y_{k,r}$:\n$$\n\\Delta y_{k,r} \\equiv y_{v,k,r} - y_{w,k,r}\n$$\nSubstituting the given model equations into this definition yields:\n$$\n\\Delta y_{k,r} = (s_v\\,T \\;+\\; c_L\\,T \\;+\\; d_k\\,T \\;+\\; m_r\\,T \\;+\\; \\varepsilon_{v,k,r}) - (c_L\\,T \\;+\\; d_k\\,T \\;+\\; m_r\\,T \\;+\\; \\varepsilon_{w,k,r})\n$$\nThe terms corresponding to locus-specific toxicity ($c_L$), guide-specific toxicity ($d_k$), and replicate-level drift ($m_r$) cancel out perfectly:\n$$\n\\Delta y_{k,r} = s_v\\,T + (\\varepsilon_{v,k,r} - \\varepsilon_{w,k,r})\n$$\nLet us define a new composite noise term $\\eta_{k,r} \\equiv \\varepsilon_{v,k,r} - \\varepsilon_{w,k,r}$. Since $\\varepsilon_{v,k,r}$ and $\\varepsilon_{w,k,r}$ are independent and have zero mean, the new noise term $\\eta_{k,r}$ also has zero mean:\n$$\n\\mathbb{E}[\\eta_{k,r}] = \\mathbb{E}[\\varepsilon_{v,k,r}] - \\mathbb{E}[\\varepsilon_{w,k,r}] = 0 - 0 = 0\n$$\nThe simplified model for each measurement pair is now:\n$$\n\\Delta y_{k,r} = s_v\\,T + \\eta_{k,r}\n$$\nAn issue of heteroscedasticity remains, as the variance of $\\eta_{k,r}$ may differ for each $(k,r)$. The variance of $\\eta_{k,r}$ is given by $\\operatorname{Var}(\\eta_{k,r}) = \\operatorname{Var}(\\varepsilon_{v,k,r} - \\varepsilon_{w,k,r})$. Due to the independence of the epsilon terms, this is $\\operatorname{Var}(\\eta_{k,r}) = \\operatorname{Var}(\\varepsilon_{v,k,r}) + \\operatorname{Var}(\\varepsilon_{w,k,r})$.\nThe problem provides the inverse-variance weight $w_{k,r}$ as:\n$$\nw_{k,r} \\equiv \\frac{1}{\\operatorname{Var}\\!\\left(y_{v,k,r} - y_{w,k,r}\\right)} = \\frac{1}{\\operatorname{Var}\\!\\left(\\Delta y_{k,r}\\right)}\n$$\nSince $s_v T$ is a constant for a given variant, $\\operatorname{Var}(\\Delta y_{k,r}) = \\operatorname{Var}(s_v T + \\eta_{k,r}) = \\operatorname{Var}(\\eta_{k,r})$. Therefore, we have $\\operatorname{Var}(\\eta_{k,r}) = 1/w_{k,r}$.\n\nTo estimate $s_v$, we can rearrange the simplified model. For each of the $K \\times R$ independent measurements, we can form an individual estimate of $s_v$:\n$$\n\\frac{\\Delta y_{k,r}}{T} = s_v + \\frac{\\eta_{k,r}}{T}\n$$\nLet $Z_{k,r} \\equiv \\frac{\\Delta y_{k,r}}{T}$. Then we have $Z_{k,r} = s_v + \\epsilon'_{k,r}$, where $\\epsilon'_{k,r} = \\eta_{k,r}/T$. Each $Z_{k,r}$ is an unbiased estimator of $s_v$ since $\\mathbb{E}[Z_{k,r}] = s_v + \\mathbb{E}[\\eta_{k,r}]/T = s_v$. However, these estimators have different variances:\n$$\n\\operatorname{Var}(Z_{k,r}) = \\operatorname{Var}\\left(\\frac{\\eta_{k,r}}{T}\\right) = \\frac{1}{T^2}\\operatorname{Var}(\\eta_{k,r}) = \\frac{1}{T^2 w_{k,r}}\n$$\nThe BLUE for $s_v$, denoted $\\hat{s}_v$, is the inverse-variance weighted average of the individual estimators $Z_{k,r}$. The optimal weights for this average, let's call them $\\alpha_{k,r}$, are proportional to the inverse of the variance of each $Z_{k,r}$:\n$$\n\\alpha_{k,r} \\propto \\frac{1}{\\operatorname{Var}(Z_{k,r})} = T^2 w_{k,r}\n$$\nTo ensure the final estimator is unbiased, these weights must sum to $1$. We normalize them accordingly:\n$$\n\\alpha_{k,r} = \\frac{T^2 w_{k,r}}{\\sum_{i=1}^K \\sum_{j=1}^R T^2 w_{i,j}} = \\frac{w_{k,r}}{\\sum_{i=1}^K \\sum_{j=1}^R w_{i,j}}\n$$\nThe BLUE for $s_v$, denoted $\\hat{s}_v$, is the linear combination of the $Z_{k,r}$ with these optimal weights:\n$$\n\\hat{s}_v = \\sum_{k=1}^K \\sum_{r=1}^R \\alpha_{k,r} Z_{k,r}\n$$\nSubstituting the expressions for $\\alpha_{k,r}$ and $Z_{k,r}$ gives:\n$$\n\\hat{s}_v = \\sum_{k=1}^K \\sum_{r=1}^R \\left( \\frac{w_{k,r}}{\\sum_{i=1}^K \\sum_{j=1}^R w_{i,j}} \\right) \\left( \\frac{y_{v,k,r} - y_{w,k,r}}{T} \\right)\n$$\nThis expression can be rearranged into a more compact form by factoring out terms that do not depend on the summation indices $k$ and $r$:\n$$\n\\hat{s}_v = \\frac{1}{T \\left(\\sum_{k=1}^K \\sum_{r=1}^R w_{k,r}\\right)} \\sum_{k=1}^K \\sum_{r=1}^R w_{k,r} (y_{v,k,r} - y_{w,k,r})\n$$\nThis is the final closed-form analytic expression for the minimum-variance unbiased linear estimator of $s_v$. It correctly combines all available measurements, weighted by their known inverse variances, to produce a single, optimal estimate of the variant's fitness effect, while being robust to the specified confounding factors.",
            "answer": "$$\n\\boxed{\\frac{\\sum_{k=1}^{K} \\sum_{r=1}^{R} w_{k,r} (y_{v,k,r} - y_{w,k,r})}{T \\sum_{k=1}^{K} \\sum_{r=1}^{R} w_{k,r}}}\n$$"
        },
        {
            "introduction": "Estimating a variant's fitness effect yields a point value, but robust scientific conclusions require us to also quantify the uncertainty of that estimate. This advanced problem delves into the statistical foundations of this task, guiding you through a first-principles derivation of a confidence interval for a selection coefficient. By applying the powerful frameworks of maximum likelihood estimation and Fisher information, you will learn how to move from a single estimate to a statistically grounded interval of plausible values. ",
            "id": "4329374",
            "problem": "In a pooled saturation genome editing (SGE) assay using Clustered Regularly Interspaced Short Palindromic Repeats (CRISPR) to perturb all single-nucleotide variants in a coding region, you wish to estimate the selection coefficient $s$ for a single variant $v$ under a competitive growth assay. The experimental design is as follows. A haploid cell population is edited to contain a library of variants with known initial frequencies $\\{q_i\\}_{i=1}^{K}$ measured by next-generation sequencing (NGS), and then propagated for $t$ mitotic doublings under a fixed condition. After propagation, a post-selection sample of size $N$ reads is obtained by NGS, yielding counts $\\{X_i\\}_{i=1}^{K}$ with $\\sum_{i=1}^{K} X_i = N$. Assume the standard multiplicative fitness model where the expected post-selection proportions are proportional to $q_i \\exp(s_i t)$, with $s_i$ the selection coefficient of variant $i$ relative to the reference background (set to $0$). Focus on a single variant $v$ whose selection coefficient is the only unknown parameter, denoted $s$, while all other variants have $s_i = 0$.\n\nUnder this model, the post-selection counts follow a multinomial distribution with cell probabilities\n$$\np_i(s) \\propto q_i \\exp(s_i t), \\quad \\text{so that} \\quad p_v(s) = \\frac{q_v \\exp(s t)}{q_v \\exp(s t) + \\sum_{j \\neq v} q_j}, \\quad p_j(s) = \\frac{q_j}{q_v \\exp(s t) + \\sum_{\\ell \\neq v} q_\\ell} \\text{ for } j \\neq v.\n$$\nLet $X_v = k$ denote the observed post-selection count for variant $v$.\n\nStarting from the definition of the multinomial likelihood and the definition of Fisher information as the negative expectation of the second derivative of the log-likelihood, do the following:\n1. Derive the log-likelihood for $s$ up to an additive constant not depending on $s$, and show that the maximum likelihood estimator (MLE) $\\hat{s}$ satisfies $p_v(\\hat{s}) = k/N$. Then obtain $\\hat{s}$ in closed form in terms of $k$, $N$, $q_v$, $t$, and $\\sum_{j \\neq v} q_j$.\n2. Derive the Fisher information for $s$ under this multinomial model and show it reduces to a simple function of $N$, $t$, and $p_v(s)$.\n3. Using the asymptotic normal approximation, construct a two-sided confidence interval for $s$ with nominal coverage $1-\\alpha$ based on the Fisher information evaluated at $\\hat{s}$.\n\nNow apply your derivation to the following realistic SGE dataset: there are $K$ variants with the focal variant having initial frequency $q_v = 10^{-3}$ and all other variants aggregated to $\\sum_{j \\neq v} q_j = 1 - q_v$. After $t = 10$ mitotic doublings, the post-selection read depth is $N = 2.0 \\times 10^{5}$ with the focal variant count $k = 90$. Take $\\alpha = 0.05$ and the corresponding standard normal quantile $\\Phi^{-1}(1 - \\alpha/2)$.\n\nReport, as your final result, the two-sided $95\\%$ confidence interval half-width for $s$ (that is, the quantity to be added and subtracted from $\\hat{s}$), expressed in natural-log fitness per generation. Round your answer to four significant figures.",
            "solution": "### Step 1: Extract Givens\n\nThe problem provides the following information for a pooled saturation genome editing (SGE) experiment:\n-   A haploid cell population is used.\n-   The experiment involves a library of $K$ variants with initial frequencies $\\{q_i\\}_{i=1}^{K}$ measured by next-generation sequencing (NGS), and then propagated for $t$ mitotic doublings under a fixed condition. After propagation, a post-selection sample of size $N$ reads is obtained by NGS, yielding counts $\\{X_i\\}_{i=1}^{K}$ with $\\sum_{i=1}^{K} X_i = N$.\n-   The multiplicative fitness model gives expected post-selection proportions proportional to $q_i \\exp(s_i t)$, where $s_i$ is the selection coefficient.\n-   A single variant $v$ has an unknown selection coefficient $s$, while all other variants have $s_i = 0$ for $i \\neq v$.\n-   The post-selection cell probabilities are:\n    $$\n    p_v(s) = \\frac{q_v \\exp(s t)}{q_v \\exp(s t) + \\sum_{j \\neq v} q_j}\n    $$\n    $$\n    p_j(s) = \\frac{q_j}{q_v \\exp(s t) + \\sum_{\\ell \\neq v} q_\\ell} \\text{ for } j \\neq v\n    $$\n-   The observed count for variant $v$ is $X_v = k$.\n-   Fisher information is defined as the negative expectation of the second derivative of the log-likelihood.\n-   An asymptotic normal approximation a two-sided confidence interval with nominal coverage $1-\\alpha$ is to be constructed.\n-   Numerical data for application:\n    -   Initial frequency of focal variant: $q_v = 10^{-3}$\n    -   Aggregate initial frequency of other variants: $\\sum_{j \\neq v} q_j = 1 - q_v$\n    -   Number of doublings: $t = 10$\n    -   Total post-selection read depth: $N = 2.0 \\times 10^{5}$\n    -   Focal variant count: $k = 90$\n    -   Significance level: $\\alpha = 0.05$\n    -   Standard normal quantile: $\\Phi^{-1}(1 - \\alpha/2)$\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem is scientifically grounded. The model described—competitive growth leading to changes in frequency, followed by multinomial sampling—is a standard and widely accepted framework for analyzing pooled CRISPR screens and other bulk competition assays in genetics and genomics. The use of maximum likelihood estimation (MLE), Fisher information, and asymptotic normality are canonical methods in statistical inference. The problem is well-posed, providing all necessary information to perform the requested derivations and calculations. The language is objective and precise. The numerical values are realistic for a deep sequencing experiment. All definitions are clear and the setup is internally consistent. No flaws are identified.\n\n### Step 3: Verdict and Action\n\nThe problem is valid. A full, reasoned solution follows.\n\n### Solution Derivation\n\nThe counts $\\{X_i\\}_{i=1}^{K}$ for the $K$ variants follow a multinomial distribution with total count $N$ and probabilities $\\{p_i(s)\\}_{i=1}^{K}$. The likelihood function is given by:\n$$L(s; \\{X_i\\}) = \\frac{N!}{X_1! \\dots X_K!} \\prod_{i=1}^{K} p_i(s)^{X_i}$$\nThe log-likelihood, $\\ell(s) = \\ln L(s)$, is:\n$$\\ell(s) = \\ln(N!) - \\sum_{i=1}^{K} \\ln(X_i!) + \\sum_{i=1}^{K} X_i \\ln p_i(s)$$\nUp to an additive constant that does not depend on the parameter $s$, the log-likelihood is:\n$$\\ell(s) \\propto \\sum_{i=1}^{K} X_i \\ln p_i(s) = X_v \\ln p_v(s) + \\sum_{j \\neq v} X_j \\ln p_j(s)$$\nLet the denominator of the probabilities be $D(s) = q_v \\exp(s t) + \\sum_{j \\neq v} q_j$. Substituting the expressions for $p_v(s)$ and $p_j(s)$:\n$$\\ell(s) \\propto X_v \\ln\\left(\\frac{q_v \\exp(st)}{D(s)}\\right) + \\sum_{j \\neq v} X_j \\ln\\left(\\frac{q_j}{D(s)}\\right)$$\n$$\\ell(s) \\propto X_v (\\ln(q_v) + st - \\ln D(s)) + \\sum_{j \\neq v} X_j (\\ln(q_j) - \\ln D(s))$$\nGrouping terms involving $s$:\n$$\\ell(s) \\propto X_v s t - \\left(X_v + \\sum_{j \\neq v} X_j\\right) \\ln D(s)$$\nSince $\\sum_{i=1}^{K} X_i = N$, this simplifies to:\n$$\\ell(s) \\propto X_v s t - N \\ln(q_v \\exp(st) + \\sum_{j \\neq v} q_j)$$\nThis is the log-likelihood for $s$ up to an additive constant, as requested in Part 1.\n\n**Part 1: Maximum Likelihood Estimator (MLE)**\n\nTo find the MLE $\\hat{s}$, we differentiate $\\ell(s)$ with respect to $s$ and set the result to zero.\n$$\\frac{d\\ell}{ds} = \\frac{d}{ds} \\left(X_v s t - N \\ln(q_v \\exp(st) + \\sum_{j \\neq v} q_j)\\right)$$\n$$\\frac{d\\ell}{ds} = X_v t - N \\frac{1}{q_v \\exp(st) + \\sum_{j \\neq v} q_j} \\cdot (q_v t \\exp(st))$$\nRecognizing the definition of $p_v(s)$, we can write:\n$$\\frac{d\\ell}{ds} = X_v t - N t \\left(\\frac{q_v \\exp(st)}{q_v \\exp(st) + \\sum_{j \\neq v} q_j}\\right) = X_v t - N t p_v(s)$$\n$$\\frac{d\\ell}{ds} = t (X_v - N p_v(s))$$\nSetting the derivative to zero to find the MLE $\\hat{s}$:\n$$t (X_v - N p_v(\\hat{s})) = 0$$\nSince $t > 0$, we must have $X_v - N p_v(\\hat{s}) = 0$. Given the observation $X_v=k$, this yields the condition for the MLE:\n$$p_v(\\hat{s}) = \\frac{k}{N}$$\nThis shows that the MLE is the value $\\hat{s}$ for which the model probability of variant $v$ matches its observed frequency in the post-selection sample.\n\nNow, we solve for $\\hat{s}$ in closed form. Let $Q_{\\text{other}} = \\sum_{j \\neq v} q_j$.\n$$\\frac{q_v \\exp(\\hat{s} t)}{q_v \\exp(\\hat{s} t) + Q_{\\text{other}}} = \\frac{k}{N}$$\n$$N q_v \\exp(\\hat{s} t) = k (q_v \\exp(\\hat{s} t) + Q_{\\text{other}})$$\n$$N q_v \\exp(\\hat{s} t) - k q_v \\exp(\\hat{s} t) = k Q_{\\text{other}}$$\n$$(N - k) q_v \\exp(\\hat{s} t) = k Q_{\\text{other}}$$\n$$\\exp(\\hat{s} t) = \\frac{k}{(N-k)} \\frac{Q_{\\text{other}}}{q_v}$$\nTaking the natural logarithm of both sides and dividing by $t$:\n$$\\hat{s} = \\frac{1}{t} \\ln\\left(\\frac{k}{N-k} \\frac{Q_{\\text{other}}}{q_v}\\right) = \\frac{1}{t} \\ln\\left(\\frac{k}{N-k} \\frac{\\sum_{j \\neq v} q_j}{q_v}\\right)$$\n\n**Part 2: Fisher Information**\n\nThe Fisher information, $I(s)$, is defined as $I(s) = -E\\left[\\frac{d^2\\ell}{ds^2}\\right]$. We first compute the second derivative of the log-likelihood.\n$$\\frac{d^2\\ell}{ds^2} = \\frac{d}{ds} \\left( t (X_v - N p_v(s)) \\right) = -N t \\frac{dp_v(s)}{ds}$$\nWe need to compute the derivative of $p_v(s)$ with respect to $s$. Using the quotient rule:\n$$p_v(s) = \\frac{q_v \\exp(st)}{D(s)}$$\n$$\\frac{dp_v(s)}{ds} = \\frac{(q_v t \\exp(st))D(s) - q_v \\exp(st) (q_v t \\exp(st))}{D(s)^2}$$\n$$\\frac{dp_v(s)}{ds} = \\frac{q_v t \\exp(st) (D(s) - q_v \\exp(st))}{D(s)^2}$$\nSince $D(s) - q_v \\exp(st) = \\sum_{j \\neq v} q_j = Q_{\\text{other}}$:\n$$\\frac{dp_v(s)}{ds} = \\frac{q_v t \\exp(st) Q_{\\text{other}}}{D(s)^2} = t \\left(\\frac{q_v \\exp(st)}{D(s)}\\right) \\left(\\frac{Q_{\\text{other}}}{D(s)}\\right)$$\nRecognizing that $\\sum_{j \\neq v} p_j(s) = \\frac{\\sum_{j \\neq v} q_j}{D(s)} = \\frac{Q_{\\text{other}}}{D(s)}$ and that $\\sum_{j \\neq v} p_j(s) = 1 - p_v(s)$:\n$$\\frac{dp_v(s)}{ds} = t \\cdot p_v(s) \\cdot (1-p_v(s))$$\nSubstituting this into the second derivative expression:\n$$\\frac{d^2\\ell}{ds^2} = -N t \\left(t \\cdot p_v(s) (1-p_v(s))\\right) = -N t^2 p_v(s) (1-p_v(s))$$\nThe second derivative does not depend on the random variable $X_v$, only on the parameter $s$ (through $p_v(s)$). Therefore, the expectation is the expression itself:\n$$E\\left[\\frac{d^2\\ell}{ds^2}\\right] = -N t^2 p_v(s) (1-p_v(s))$$\nThe Fisher information is:\n$$I(s) = -E\\left[\\frac{d^2\\ell}{ds^2}\\right] = N t^2 p_v(s) (1-p_v(s))$$\nThis is a simple function of $N$, $t$, and $p_v(s)$, as required.\n\n**Part 3: Confidence Interval and Application**\n\nThe asymptotic theory of MLE states that $\\hat{s}$ is approximately normally distributed with mean $s$ and variance $I(s)^{-1}$. We estimate the standard error of $\\hat{s}$ by evaluating the Fisher information at the MLE, $\\hat{s}$.\n$$\\text{SE}(\\hat{s}) = \\sqrt{I(\\hat{s})^{-1}} = \\frac{1}{\\sqrt{I(\\hat{s})}}$$\nUsing the result from Part 1, $p_v(\\hat{s}) = k/N$:\n$$I(\\hat{s}) = N t^2 p_v(\\hat{s})(1 - p_v(\\hat{s})) = N t^2 \\left(\\frac{k}{N}\\right) \\left(1 - \\frac{k}{N}\\right) = N t^2 \\frac{k(N-k)}{N^2} = t^2 \\frac{k(N-k)}{N}$$\nThe estimated standard error is:\n$$\\text{SE}(\\hat{s}) = \\frac{1}{\\sqrt{t^2 \\frac{k(N-k)}{N}}} = \\frac{1}{t} \\sqrt{\\frac{N}{k(N-k)}}$$\nA two-sided $1-\\alpha$ confidence interval for $s$ is given by $\\hat{s} \\pm z_{1-\\alpha/2} \\cdot \\text{SE}(\\hat{s})$, where $z_{1-\\alpha/2} = \\Phi^{-1}(1-\\alpha/2)$ is the quantile of the standard normal distribution. The half-width of this interval is:\n$$W = z_{1-\\alpha/2} \\cdot \\text{SE}(\\hat{s}) = z_{1-\\alpha/2} \\frac{1}{t} \\sqrt{\\frac{N}{k(N-k)}}$$\n\nNow we apply this to the given data:\n- $t = 10$\n- $N = 2.0 \\times 10^5$\n- $k = 90$\n- $\\alpha = 0.05$, so $1-\\alpha/2 = 0.975$. The corresponding quantile is $z_{0.975} = \\Phi^{-1}(0.975) \\approx 1.95996$.\n\nFirst, calculate the term inside the square root:\n$$\\frac{N}{k(N-k)} = \\frac{2.0 \\times 10^5}{90 \\times (2.0 \\times 10^5 - 90)} = \\frac{200000}{90 \\times 199910} = \\frac{200000}{17991900} \\approx 0.01111601$$\nNow, calculate the standard error:\n$$\\text{SE}(\\hat{s}) = \\frac{1}{10} \\sqrt{0.01111601} \\approx \\frac{1}{10} \\times 0.1054325 = 0.01054325$$\nFinally, calculate the half-width $W$:\n$$W = z_{0.975} \\cdot \\text{SE}(\\hat{s}) \\approx 1.95996 \\times 0.01054325 \\approx 0.0206643$$\nRounding to four significant figures, the half-width is $0.02066$.",
            "answer": "$$\\boxed{0.02066}$$"
        }
    ]
}