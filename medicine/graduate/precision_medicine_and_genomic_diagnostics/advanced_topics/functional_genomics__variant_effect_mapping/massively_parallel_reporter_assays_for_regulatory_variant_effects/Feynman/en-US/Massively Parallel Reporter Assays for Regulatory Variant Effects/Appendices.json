{
    "hands_on_practices": [
        {
            "introduction": "The primary goal of a Massively Parallel Reporter Assay (MPRA) is to quantify the regulatory activity of a DNA sequence. This is fundamentally achieved by comparing its transcriptional output (RNA abundance) to its initial input representation (DNA abundance). This practice  provides a direct application of the core calculation used in MPRA analysis, defining regulatory activity as the log-ratio of RNA to DNA counts, which transforms multiplicative biological effects into an additive scale for robust comparison.",
            "id": "4357321",
            "problem": "A massively parallel reporter assay (MPRA) quantifies regulatory activity by coupling candidate regulatory sequences to unique barcodes, inserting them into a plasmid library, delivering the library into cells, and sequencing both input deoxyribonucleic acid (DNA) barcodes and output ribonucleic acid (RNA) barcodes. Under the Central Dogma and standard reporter assay design, the transcriptional output attributed to a regulatory sequence is reflected in the abundance of its RNA barcodes, while the input dosage is reflected in the abundance of its DNA barcodes. A core, well-tested practice in MPRA analysis is to define the normalized activity for a sequence as the base-$2$ logarithm of the ratio of RNA barcode counts to DNA barcode counts, thereby converting multiplicative regulatory effects into additive quantities. The allelic fold-change on the base-$2$ logarithmic scale is then captured by the difference in normalized activities between alleles.\n\nConsider an assay comparing two alleles of a regulatory variant, measured in the same pool so that library size factors and sequencing depth effects are shared. Assume barcode aggregation has already been performed and that there are no zero counts requiring pseudocounts. For allele $1$, the aggregated RNA barcode count is $300$ and the aggregated DNA barcode count is $100$. For allele $2$, the aggregated RNA barcode count is $150$ and the aggregated DNA barcode count is $150$.\n\nUsing the definitions above, compute the normalized activities $A_{1}$ and $A_{2}$ and the base-$2$ logarithmic allelic fold-change $\\Delta$ defined as the difference in normalized activities between allele $1$ and allele $2$. Express your answers in exact closed form using base-$2$ logarithms; do not approximate numerically. No units are required.",
            "solution": "The normalized activity, $A$, is defined as the base-$2$ logarithm of the ratio of RNA counts to DNA counts. Let $C_{RNA}$ be the RNA barcode count and $C_{DNA}$ be the DNA barcode count. The formula is:\n$$A = \\log_{2}\\left(\\frac{C_{RNA}}{C_{DNA}}\\right)$$\n\nWe are given the counts for two alleles:\n- Allele 1: $C_{RNA,1} = 300$, $C_{DNA,1} = 100$\n- Allele 2: $C_{RNA,2} = 150$, $C_{DNA,2} = 150$\n\nFirst, we calculate the normalized activity for allele 1, $A_1$:\n$$A_{1} = \\log_{2}\\left(\\frac{300}{100}\\right) = \\log_{2}(3)$$\n\nNext, we calculate the normalized activity for allele 2, $A_2$:\n$$A_{2} = \\log_{2}\\left(\\frac{150}{150}\\right) = \\log_{2}(1) = 0$$\n\nFinally, the base-$2$ logarithmic allelic fold-change, $\\Delta$, is the difference in normalized activities:\n$$\\Delta = A_{1} - A_{2} = \\log_{2}(3) - 0 = \\log_{2}(3)$$\n\nThe requested values are the normalized activity for allele 1 ($A_1$), the normalized activity for allele 2 ($A_2$), and the logarithmic allelic fold-change ($\\Delta$).",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\log_{2}(3) & 0 & \\log_{2}(3)\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "While simple ratios are intuitive, a more robust analysis requires a statistical model that properly handles the count-based nature and high variability of sequencing data. This practice  introduces the use of a Generalized Linear Model (GLM) with a Negative Binomial distribution, the standard approach for analyzing MPRA data. By interpreting the model's coefficients, you will learn how to formally estimate an allele's effect on regulatory activity while properly normalizing for input DNA abundance.",
            "id": "4357264",
            "problem": "A Massively Parallel Reporter Assay (MPRA) measures regulatory activity of many allelic variants in parallel by sequencing barcoded constructs. For each barcode-indexed construct $i$, you observe deoxyribonucleic acid (DNA) input counts $d_i$ (reflecting plasmid abundance) and ribonucleic acid (RNA) output counts $y_i$ (reflecting transcriptional activity). You wish to estimate the allelic regulatory effect of the alternative allele relative to the reference allele while accounting for variation in DNA input. Let $\\text{allele}_i \\in \\{0,1\\}$ indicate the allele carried by construct $i$ ($0$ for reference, $1$ for alternative).\n\nStarting from the following fundamental bases:\n- Central Dogma and reporter assays: RNA output from a reporter construct is an observable proxy for transcriptional activity driven by its regulatory sequence, and DNA input abundance multiplicatively scales expected RNA counts when all else is equal.\n- Generalized Linear Model (GLM) definition: A GLM specifies a distribution for the response with mean $E[y_i \\mid \\mathbf{x}_i] = \\mu_i$, a link function $g(\\mu_i)$ that is a linear function of predictors, and a variance function governed by a dispersion parameter.\n- Negative Binomial distribution for sequencing counts: Under a Negative Binomial (NB) model, $y_i \\sim \\text{NB}(\\mu_i,\\phi)$ with mean $E[y_i]=\\mu_i$ and variance $\\text{Var}(y_i) = \\mu_i + \\phi \\mu_i^2$ for an overdispersion parameter $\\phi>0$.\n\nYou fit the GLM with log link and an offset for DNA input,\n$$\n\\log(\\mu_i) = \\beta_0 + \\beta_1 \\cdot \\text{allele}_i + \\log(d_i),\n$$\nwhere $\\log(d_i)$ enters the linear predictor as an offset rather than an estimated coefficient.\n\nUsing only the core GLM structure, the offset property of the log link, and the NB mean-variance characterization, derive the quantity that $\\beta_1$ estimates in terms of expected RNA activity normalized by DNA input, and state how to interpret $\\exp(\\beta_1)$ as an allelic effect in this MPRA context.\n\nWhich option best captures the correct interpretation of $\\beta_1$ (and $\\exp(\\beta_1)$) under this model?\n\nA. $\\exp(\\beta_1)$ is the multiplicative change in the expected RNA count per unit DNA input for the alternative allele relative to the reference allele; equivalently, it is the ratio of expected RNA-to-DNA means $E[y_i]/d_i$ comparing $\\text{allele}_i=1$ to $\\text{allele}_i=0$.\n\nB. $\\beta_1$ quantifies how overdispersion changes with allele; it measures the change in $\\phi$ attributable to the alternative allele, thereby reflecting variability rather than mean activity.\n\nC. $\\exp(\\beta_1)$ is the odds ratio of observing the alternative allele in RNA relative to DNA, analogous to a logistic regression effect, because the link is a log function.\n\nD. $\\beta_1$ is the absolute change in the expected RNA count $E[y_i]$ when switching from $\\text{allele}_i=0$ to $\\text{allele}_i=1$, holding $d_i$ fixed, and is independent of baseline activity $\\beta_0$.\n\nE. $\\exp(\\beta_1)$ is the multiplicative change in raw expected RNA counts $E[y_i]$ comparing $\\text{allele}_i=1$ to $\\text{allele}_i=0$, ignoring the DNA input offset $d_i$ because the NB mean already accounts for scaling.",
            "solution": "### Derivation and Solution\n\nThe objective is to derive the meaning of the parameter $\\beta_1$ from the specified GLM. The model is:\n$$\n\\log(\\mu_i) = \\beta_0 + \\beta_1 \\cdot \\text{allele}_i + \\log(d_i)\n$$\nwhere $\\mu_i$ is the expected RNA count, $E[y_i]$.\n\nThe problem highlights the use of an offset and the log link. A key property of the log link is that an offset in the linear predictor corresponds to a multiplicative scaling factor on the mean scale. We can see this by rearranging the equation:\n$$\n\\log(\\mu_i) - \\log(d_i) = \\beta_0 + \\beta_1 \\cdot \\text{allele}_i\n$$\nUsing the property of logarithms, $\\log(a) - \\log(b) = \\log(a/b)$, we get:\n$$\n\\log\\left(\\frac{\\mu_i}{d_i}\\right) = \\beta_0 + \\beta_1 \\cdot \\text{allele}_i\n$$\nThis rearranged equation provides the most direct interpretation. The quantity on the left, $\\mu_i/d_i = E[y_i]/d_i$, represents the expected RNA count per unit of DNA input. This ratio is a measure of the intrinsic transcriptional activity of the regulatory sequence, normalized for its initial abundance in the plasmid pool.\n\nLet's analyze the model for the two allele types:\n\n1.  **For the reference allele ($\\text{allele}_i = 0$):**\n    The model becomes:\n    $$\n    \\log\\left(\\frac{E[y_i | \\text{allele}_i=0]}{d_i}\\right) = \\beta_0 + \\beta_1 \\cdot 0 = \\beta_0\n    $$\n    Thus, $\\exp(\\beta_0)$ represents the baseline transcriptional activity (expected RNA per DNA) of the reference allele.\n\n2.  **For the alternative allele ($\\text{allele}_i = 1$):**\n    The model becomes:\n    $$\n    \\log\\left(\\frac{E[y_i | \\text{allele}_i=1]}{d_i}\\right) = \\beta_0 + \\beta_1 \\cdot 1 = \\beta_0 + \\beta_1\n    $$\n\nTo interpret $\\beta_1$, we can subtract the equation for the reference allele from the equation for the alternative allele:\n$$\n\\log\\left(\\frac{E[y_i|\\text{allele}_i=1]}{d_i}\\right) - \\log\\left(\\frac{E[y_i|\\text{allele}_i=0]}{d_i}\\right) = (\\beta_0 + \\beta_1) - \\beta_0 = \\beta_1\n$$\nApplying the logarithm subtraction rule again:\n$$\n\\beta_1 = \\log\\left( \\frac{E[y_i|\\text{allele}_i=1]/d_i}{E[y_i|\\text{allele}_i=0]/d_i} \\right)\n$$\nThis shows that $\\beta_1$ is the natural logarithm of the ratio of the normalized transcriptional activities (expected RNA per DNA) of the alternative allele relative to the reference allele.\n\nTo interpret $\\exp(\\beta_1)$, we simply exponentiate both sides:\n$$\n\\exp(\\beta_1) = \\frac{E[y_i|\\text{allele}_i=1]/d_i}{E[y_i|\\text{allele}_i=0]/d_i}\n$$\nTherefore, $\\exp(\\beta_1)$ is the ratio of the expected RNA-to-DNA means, comparing the alternative allele to the reference allele. This is precisely the multiplicative change in the expected RNA count per unit of DNA input for the alternative allele relative to the reference. This quantity is often referred to as the allelic skew or allelic effect on regulatory activity.\n\n### Option-by-Option Analysis\n\n**A. $\\exp(\\beta_1)$ is the multiplicative change in the expected RNA count per unit DNA input for the alternative allele relative to the reference allele; equivalently, it is the ratio of expected RNA-to-DNA means $E[y_i]/d_i$ comparing $\\text{allele}_i=1$ to $\\text{allele}_i=0$.**\nThis statement perfectly matches our derivation. The expression $\\exp(\\beta_1) = \\frac{E[y_i|\\text{allele}_i=1]/d_i}{E[y_i|\\text{allele}_i=0]/d_i}$ shows that it is the ratio of the expected RNA-to-DNA means, which is synonymous with the multiplicative change in the expected RNA count per unit DNA input.\n**Verdict: Correct.**\n\n**B. $\\beta_1$ quantifies how overdispersion changes with allele; it measures the change in $\\phi$ attributable to the alternative allele, thereby reflecting variability rather than mean activity.**\nThis is incorrect. The parameter $\\beta_1$ is part of the linear predictor for the mean, $\\mu_i$. The overdispersion parameter, $\\phi$, is part of the variance function, $\\text{Var}(y_i) = \\mu_i + \\phi\\mu_i^2$. In the specified model, $\\phi$ is treated as a single, constant parameter and is not dependent on any predictors, including $\\text{allele}_i$. Therefore, $\\beta_1$ models the effect on the mean, not the variance or overdispersion.\n**Verdict: Incorrect.**\n\n**C. $\\exp(\\beta_1)$ is the odds ratio of observing the alternative allele in RNA relative to DNA, analogous to a logistic regression effect, because the link is a log function.**\nThis is incorrect on multiple levels. First, the model is a Negative Binomial regression for count data, not a logistic regression for binary outcomes. Odds ratios are the natural interpretation of coefficients in a logistic regression (which uses a logit link), not a NB regression with a log link. Second, the quantity $\\exp(\\beta_1)$ is a ratio of means (or rates), not a ratio of odds. Third, the phrasing \"odds ratio of observing the alternative allele in RNA relative to DNA\" is conceptually incoherent in this context.\n**Verdict: Incorrect.**\n\n**D. $\\beta_1$ is the absolute change in the expected RNA count $E[y_i]$ when switching from $\\text{allele}_i=0$ to $\\text{allele}_i=1$, holding $d_i$ fixed, and is independent of baseline activity $\\beta_0$.**\nThis is incorrect. Because the model uses a log link, the predictors have a multiplicative, not additive, effect on the mean scale, $E[y_i]$. The absolute change in $E[y_i]$ is $E[y | \\text{allele}=1, d_i] - E[y | \\text{allele}=0, d_i] = d_i\\exp(\\beta_0)(\\exp(\\beta_1)-1)$. This change is not equal to $\\beta_1$ and clearly depends on both the baseline activity ($\\beta_0$) and the DNA input ($d_i$).\n**Verdict: Incorrect.**\n\n**E. $\\exp(\\beta_1)$ is the multiplicative change in raw expected RNA counts $E[y_i]$ comparing $\\text{allele}_i=1$ to $\\text{allele_i=0}$, ignoring the DNA input offset $d_i$ because the NB mean already accounts for scaling.**\nThis statement is misleading and its reasoning is flawed. While it is true that for a fixed $d_i$, $\\exp(\\beta_1) = \\frac{E[y_i|\\text{allele}=1, d_i]}{E[y_i|\\text{allele}=0, d_i]}$, the role of the offset is not to be ignored. The offset is precisely the mechanism by which the model accounts for the scaling effect of $d_i$. The model *corrects for* $d_i$, it does not ignore it. The reasoning \"because the NB mean already accounts for scaling\" is incorrect; the NB mean itself does not account for this scaling—the full GLM structure including the offset imposes this relationship. Option A provides a much more accurate and fundamental interpretation by focusing on the normalized activity, which is what the model is designed to estimate.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "The ultimate goal of identifying regulatory variants is to prioritize them for functional validation and potential clinical action, which requires integrating evidence from multiple sources in a principled manner. This capstone practice  guides you through building a Bayesian framework to combine MPRA effect sizes with genetic association evidence from expression Quantitative Trait Locus (eQTL) studies. By developing a scoring function to rank variants, you will apply statistical reasoning to a critical task in precision medicine and genomic diagnostics.",
            "id": "4357293",
            "problem": "You are designing a principled, programmatically implementable scoring function to rank noncoding genetic variants for clinical follow-up using data from Massively Parallel Reporter Assay (MPRA) measurements and expression Quantitative Trait Loci (eQTL) colocalization results. Your goal is to derive, from first principles, a scalar score that is monotone in the posterior probability that a variant is clinically relevant as a regulatory variant affecting gene expression in a disease-relevant context, and then to rank variants by their expected clinical utility.\n\nFundamental base to use:\n- Bayes’ theorem for combining prior odds and likelihood ratios.\n- Independence of evidence sources implies additivity of log-likelihood ratios.\n- Gaussian measurement model for MPRA effect sizes.\n- The logit and logistic functions for mapping between probabilities, odds, and log-odds.\n- Decision theory principle that expected utility is the product of utility and posterior probability when utility is realized only if the hypothesis is true.\n\nSet up the following latent binary hypotheses per variant: hypothesis $\\mathcal{H}_1$ denotes that the variant is a clinically relevant regulatory variant; hypothesis $\\mathcal{H}_0$ denotes that it is not. You are given two evidence sources per variant:\n- An MPRA effect estimate $e$ and its standard error $s$, where under a Gaussian measurement model, the observation is modeled by a Normal distribution.\n- An eQTL colocalization posterior probability $c$ that the same variant underlies both the eQTL and the disease association.\n\nAssume the following modeling choices:\n- Under $\\mathcal{H}_0$, the MPRA effect $e$ is distributed as a Normal distribution with mean $0$ and variance $s^2$.\n- Under $\\mathcal{H}_1$, the MPRA effect $e$ is distributed as a Normal distribution with mean $\\mu_A$ and variance $s^2 + \\tau^2$, where $\\mu_A$ and $\\tau$ are hyperparameters representing the expected shift and additional between-variant variance in the functional state.\n- The eQTL colocalization probability $c$ is treated as a posterior probability about colocalization, and the contribution of this evidence to the log-likelihood ratio can be represented by the difference between its posterior log-odds and a baseline prior log-odds for colocalization, with baseline prior probability $\\pi_c$.\n- The prior probability that any given variant in the locus panel is clinically relevant is $\\pi_0$.\n- For numerical stability, probabilities are clipped to the open interval $(\\epsilon, 1 - \\epsilon)$ before applying log-odds transformations, where $\\epsilon$ is a small positive constant.\n- Each variant has a nonnegative clinical utility weight $u$ representing the relative clinical value of validating that variant if it is truly relevant.\n\nYour task:\n- Derive a scalar score that is monotone in the posterior probability $\\Pr(\\mathcal{H}_1 \\mid \\text{data})$ by combining the MPRA and colocalization evidence using Bayes’ theorem and the independence assumption. Then, define the expected clinical utility score for ranking as $u \\times \\Pr(\\mathcal{H}_1 \\mid \\text{data})$.\n- Implement this as a program that, for each test case, takes arrays of $e$, $s$, $c$, and $u$ along with the constants $\\mu_A$, $\\tau$, $\\pi_c$, $\\pi_0$, and $\\epsilon$, and returns the zero-based index of the top-ranked variant that maximizes expected utility. In the event of exact numerical ties up to a tolerance $\\delta$, return the smallest index among the maximizers. Use tolerance $\\delta = 10^{-12}$.\n\nTest suite:\nImplement your method on the following four test cases. For each case, compute the single integer index of the top-ranked variant and output the list of the four indices in order.\n\nConstants (used in all test cases):\n- $\\mu_A = 0.5$\n- $\\tau = 0.3$\n- $\\pi_c = 0.2$\n- $\\pi_0 = 0.01$\n- $\\epsilon = 10^{-6}$\n- $\\delta = 10^{-12}$\n\nCase $1$ (general case):\n- $e = [\\,0.4,\\,1.2,\\,-0.1,\\,0.8,\\,0.0\\,]$\n- $s = [\\,0.2,\\,0.5,\\,0.3,\\,0.4,\\,0.1\\,]$\n- $c = [\\,0.6,\\,0.4,\\,0.9,\\,0.2,\\,0.5\\,]$\n- $u = [\\,1,\\,1,\\,1,\\,1,\\,1\\,]$\n\nCase $2$ (edge probabilities and precisions):\n- $e = [\\,0.0,\\,3.0,\\,0.1\\,]$\n- $s = [\\,10^{6},\\,1.0,\\,0.05\\,]$\n- $c = [\\,0.0,\\,1.0,\\,0.01\\,]$\n- $u = [\\,1,\\,1,\\,1\\,]$\n\nCase $3$ (exact tie, exercise tie-breaking):\n- $e = [\\,0.5,\\,0.5\\,]$\n- $s = [\\,0.2,\\,0.2\\,]$\n- $c = [\\,0.5,\\,0.5\\,]$\n- $u = [\\,1,\\,1\\,]$\n\nCase $4$ (different utilities with identical evidence):\n- $e = [\\,0.6,\\,0.6,\\,0.6\\,]$\n- $s = [\\,0.3,\\,0.3,\\,0.3\\,]$\n- $c = [\\,0.6,\\,0.6,\\,0.6\\,]$\n- $u = [\\,1.0,\\,5.0,\\,0.5\\,]$\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with no spaces. For example, an output for four cases should look like \"[0,2,1,3]\". Your program must implement the derivation described above and apply it to the test suite exactly as specified, returning a list of four integers in the specified format.",
            "solution": "The central task is to rank genetic variants by their expected clinical utility. The expected utility for a variant $i$, denoted $U_i$, is defined as the product of its clinical utility weight, $u_i$, and its posterior probability of being a clinically relevant regulatory variant, $\\Pr(\\mathcal{H}_1 \\mid \\text{data}_i)$.\n$$ U_i = u_i \\times \\Pr(\\mathcal{H}_1 \\mid \\text{data}_i) $$\nHere, $\\mathcal{H}_1$ is the hypothesis that the variant is clinically relevant, and $\\mathcal{H}_0$ is the null hypothesis that it is not. The data for each variant consists of a Massively Parallel Reporter Assay (MPRA) effect estimate $e_i$ with standard error $s_i$, and an expression Quantitative Trait Locus (eQTL) colocalization posterior probability $c_i$.\n\nTo calculate the posterior probability $\\Pr(\\mathcal{H}_1 \\mid \\text{data}_i)$, we employ Bayes' theorem. It is most convenient to work with log-odds, as the problem assumes independence of the evidence sources (MPRA and eQTL), which makes their contributions additive in the log-odds space.\n\nThe posterior log-odds of $\\mathcal{H}_1$ given the data is the sum of the prior log-odds and the log-likelihood ratios (LLR) from each evidence source:\n$$ \\underbrace{\\log\\left(\\frac{\\Pr(\\mathcal{H}_1 \\mid \\text{data}_i)}{\\Pr(\\mathcal{H}_0 \\mid \\text{data}_i)}\\right)}_{\\text{Posterior log-odds}} = \\underbrace{\\log\\left(\\frac{\\Pr(\\mathcal{H}_1)}{\\Pr(\\mathcal{H}_0)}\\right)}_{\\text{Prior log-odds}} + \\underbrace{\\log\\left(\\frac{p(e_i, s_i \\mid \\mathcal{H}_1)}{p(e_i, s_i \\mid \\mathcal{H}_0)}\\right)}_{\\text{LLR}_{\\text{MPRA}}} + \\underbrace{\\log\\left(\\frac{p(\\text{evidence from } c_i \\mid \\mathcal{H}_1)}{p(\\text{evidence from } c_i \\mid \\mathcal{H}_0)}\\right)}_{\\text{LLR}_{\\text{eQTL}}} $$\n\nLet's define each term based on the problem statement. For numerical stability, probabilities $p$ are clipped to the interval $(\\epsilon, 1 - \\epsilon)$ before applying the logit transformation, $\\text{logit}(p) = \\log(p/(1-p))$.\n\n1.  **Prior Log-Odds**: The prior probability of a variant being clinically relevant is $\\pi_0$. The prior log-odds term is calculated from the clipped prior probability $\\pi'_0 = \\text{clip}(\\pi_0, \\epsilon, 1-\\epsilon)$:\n    $$ \\text{Prior log-odds} = \\text{logit}(\\pi'_0) = \\log\\left(\\frac{\\pi'_0}{1-\\pi'_0}\\right) $$\n\n2.  **eQTL Log-Likelihood Ratio ($\\text{LLR}_{\\text{eQTL}}$)**: The problem states that this contribution is the difference between the posterior log-odds of colocalization (given by $c_i$) and the prior log-odds of colocalization (given by $\\pi_c$). Let $c'_i = \\text{clip}(c_i, \\epsilon, 1-\\epsilon)$ and $\\pi'_c = \\text{clip}(\\pi_c, \\epsilon, 1-\\epsilon)$.\n    $$ \\text{LLR}_{\\text{eQTL}, i} = \\text{logit}(c'_i) - \\text{logit}(\\pi'_c) = \\log\\left(\\frac{c'_i}{1-c'_i}\\right) - \\log\\left(\\frac{\\pi'_c}{1-\\pi'_c}\\right) $$\n    This term quantifies the evidence for $\\mathcal{H}_1$ provided by the eQTL colocalization analysis.\n\n3.  **MPRA Log-Likelihood Ratio ($\\text{LLR}_{\\text{MPRA}}$)**: This term is derived from the specified Gaussian measurement models. The likelihood of observing an effect size $e_i$ with standard error $s_i$ is given by the probability density function (PDF) of a Normal distribution, $\\mathcal{N}(\\cdot \\mid \\text{mean}, \\text{variance})$.\n    -   Under $\\mathcal{H}_0$: $p(e_i \\mid s_i, \\mathcal{H}_0) = \\mathcal{N}(e_i \\mid 0, s_i^2)$\n    -   Under $\\mathcal{H}_1$: $p(e_i \\mid s_i, \\mathcal{H}_1) = \\mathcal{N}(e_i \\mid \\mu_A, s_i^2 + \\tau^2)$\n    The LLR is the logarithm of the ratio of these two likelihoods. We can compute it as the difference of the log-PDFs:\n    $$ \\text{LLR}_{\\text{MPRA}, i} = \\log\\left(p(e_i \\mid s_i, \\mathcal{H}_1)\\right) - \\log\\left(p(e_i \\mid s_i, \\mathcal{H}_0)\\right) $$\n    Using the formula for the log-PDF of a Normal distribution, $\\log\\mathcal{N}(x \\mid \\mu, \\sigma^2) = -\\frac{1}{2}\\log(2\\pi\\sigma^2) - \\frac{(x-\\mu)^2}{2\\sigma^2}$, this becomes:\n    $$ \\text{LLR}_{\\text{MPRA}, i} = \\left[-\\frac{1}{2}\\log(2\\pi(s_i^2 + \\tau^2)) - \\frac{(e_i - \\mu_A)^2}{2(s_i^2 + \\tau^2)}\\right] - \\left[-\\frac{1}{2}\\log(2\\pi s_i^2) - \\frac{e_i^2}{2s_i^2}\\right] $$\n    $$ \\text{LLR}_{\\text{MPRA}, i} = \\frac{1}{2}\\log\\left(\\frac{s_i^2}{s_i^2 + \\tau^2}\\right) + \\frac{e_i^2}{2s_i^2} - \\frac{(e_i - \\mu_A)^2}{2(s_i^2 + \\tau^2)} $$\n\nCombining these terms, the total posterior log-odds for variant $i$, denoted $L_i$, is:\n$$ L_i = \\text{logit}(\\pi'_0) + \\text{LLR}_{\\text{MPRA}, i} + \\text{LLR}_{\\text{eQTL}, i} $$\nThis score $L_i$ is monotone in the posterior probability.\n\nTo obtain the posterior probability $\\Pr(\\mathcal{H}_1 \\mid \\text{data}_i)$, we apply the logistic (sigmoid) function, which is the inverse of the logit function:\n$$ \\Pr(\\mathcal{H}_1 \\mid \\text{data}_i) = \\sigma(L_i) = \\frac{1}{1 + e^{-L_i}} $$\nFinally, the expected utility for variant $i$ is calculated as:\n$$ U_i = u_i \\times \\frac{1}{1 + e^{-L_i}} $$\nThe program will compute $U_i$ for all variants in a given test case. It will then identify the maximum utility value, $U_{\\max} = \\max_i(U_i)$. The top-ranked variant is the one with the smallest index $i$ such that $|U_i - U_{\\max}| \\le \\delta$, where $\\delta = 10^{-12}$ is the specified tolerance for tie-breaking.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\n\ndef solve():\n    \"\"\"\n    Solves the variant ranking problem for the given test suite.\n    \"\"\"\n    # Define the constants from the problem statement.\n    mu_A = 0.5\n    tau = 0.3\n    pi_c = 0.2\n    pi_0 = 0.01\n    epsilon = 1e-6\n    delta = 1e-12\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (general case)\n        {\n            \"e\": np.array([0.4, 1.2, -0.1, 0.8, 0.0]),\n            \"s\": np.array([0.2, 0.5, 0.3, 0.4, 0.1]),\n            \"c\": np.array([0.6, 0.4, 0.9, 0.2, 0.5]),\n            \"u\": np.array([1, 1, 1, 1, 1]),\n        },\n        # Case 2 (edge probabilities and precisions)\n        {\n            \"e\": np.array([0.0, 3.0, 0.1]),\n            \"s\": np.array([1e6, 1.0, 0.05]),\n            \"c\": np.array([0.0, 1.0, 0.01]),\n            \"u\": np.array([1, 1, 1]),\n        },\n        # Case 3 (exact tie, exercise tie-breaking)\n        {\n            \"e\": np.array([0.5, 0.5]),\n            \"s\": np.array([0.2, 0.2]),\n            \"c\": np.array([0.5, 0.5]),\n            \"u\": np.array([1, 1]),\n        },\n        # Case 4 (different utilities with identical evidence)\n        {\n            \"e\": np.array([0.6, 0.6, 0.6]),\n            \"s\": np.array([0.3, 0.3, 0.3]),\n            \"c\": np.array([0.6, 0.6, 0.6]),\n            \"u\": np.array([1.0, 5.0, 0.5]),\n        },\n    ]\n\n    def logit(p):\n        \"\"\"Computes the logit function log(p / (1 - p)).\"\"\"\n        return np.log(p / (1.0 - p))\n\n    def calculate_expected_utilities(e, s, c, u):\n        \"\"\"\n        Calculates the expected clinical utility for a set of variants.\n        \"\"\"\n        # 1. Clip probabilities for numerical stability\n        pi0_clipped = np.clip(pi_0, epsilon, 1.0 - epsilon)\n        pic_clipped = np.clip(pi_c, epsilon, 1.0 - epsilon)\n        c_clipped = np.clip(c, epsilon, 1.0 - epsilon)\n        \n        # 2. Calculate prior log-odds\n        prior_log_odds = logit(pi0_clipped)\n        \n        # 3. Calculate eQTL log-likelihood ratio (LLR_eQTL)\n        llr_eqtl = logit(c_clipped) - logit(pic_clipped)\n        \n        # 4. Calculate MPRA log-likelihood ratio (LLR_MPRA)\n        # LLR = log(p(data|H1)) - log(p(data|H0))\n        # H0:\n        log_pdf_h0 = norm.logpdf(e, loc=0.0, scale=s)\n        # H1:\n        var_h1 = s**2 + tau**2\n        scale_h1 = np.sqrt(var_h1)\n        log_pdf_h1 = norm.logpdf(e, loc=mu_A, scale=scale_h1)\n        \n        llr_mpra = log_pdf_h1 - log_pdf_h0\n\n        # 5. Calculate total posterior log-odds\n        total_log_odds = prior_log_odds + llr_eqtl + llr_mpra\n        \n        # 6. Convert log-odds to posterior probability\n        # posterior_prob = 1 / (1 + exp(-total_log_odds))\n        posterior_prob = 1.0 / (1.0 + np.exp(-total_log_odds))\n        \n        # 7. Calculate expected utility\n        expected_utility = u * posterior_prob\n        \n        return expected_utility\n\n    results = []\n    for case in test_cases:\n        utilities = calculate_expected_utilities(case[\"e\"], case[\"s\"], case[\"c\"], case[\"u\"])\n        \n        # Find the index of the top-ranked variant\n        max_utility = np.max(utilities)\n        \n        # Find all indices that are maximizers within the tolerance delta\n        maximizer_indices = np.where(np.abs(utilities - max_utility) <= delta)[0]\n        \n        # Per the tie-breaking rule, return the smallest index among the maximizers\n        best_index = np.min(maximizer_indices)\n        results.append(best_index)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}