{
    "hands_on_practices": [
        {
            "introduction": "A core task in proteomics is to determine the relative abundance of thousands of proteins in a complex biological sample. This exercise introduces a classic method for label-free quantification, the Normalized Spectral Abundance Factor (NSAF), which estimates protein abundance from mass spectrometry data. By deriving and applying this metric, you will gain hands-on experience with a fundamental quantification technique and appreciate its underlying assumptions and limitations.",
            "id": "4362816",
            "problem": "In a label-free shotgun proteomics experiment using Liquid Chromatography–Tandem Mass Spectrometry (LC-MS/MS) with Data-Dependent Acquisition (DDA), the number of assigned peptide tandem spectra for a protein, called its spectral count, is often used as a proxy for protein abundance. Under the Central Dogma of molecular biology and the sampling properties of tandem mass spectrometry, assume the following foundational model: the expected spectral count for protein $i$, denoted $SpC_i$, is proportional to the product of its molar abundance $A_i$ and its length $L_i$ in amino acids, because longer proteins produce more distinct tryptic peptides that can be sampled. That is, there exists a constant of proportionality $k$ such that $SpC_i \\approx k\\,A_i\\,L_i$ for all proteins $i$ in a given sample and run. To estimate relative abundance, you decide to compute, for a protein of interest $i$, the fraction of its length-normalized spectral count relative to the sum of length-normalized spectral counts across all detected proteins in the run. This fraction is commonly known as the Normalized Spectral Abundance Factor (NSAF), although you should derive the expression from the stated assumptions rather than invoking any pre-memorized formula.\n\nGiven a single tumor biopsy run with the following proteins, lengths $L_i$ (in amino acids), and spectral counts $SpC_i$:\n- Protein $1$: $L_1 = 450$, $SpC_1 = 120$.\n- Protein $2$: $L_2 = 1100$, $SpC_2 = 150$.\n- Protein $3$: $L_3 = 300$, $SpC_3 = 80$.\n- Protein $4$: $L_4 = 800$, $SpC_4 = 60$.\n- Protein $5$: $L_5 = 1600$, $SpC_5 = 200$.\n\nTasks:\n1. Starting only from the assumption $SpC_i \\approx k\\,A_i\\,L_i$, derive a closed-form expression for the fraction described above for a specific protein $i$ in terms of the observable $SpC_j$ and $L_j$ over all proteins $j$.\n2. Using your derived expression, compute this fraction for Protein $2$. Express your final result as a decimal fraction and round to four significant figures.\n3. Briefly discuss, in no more than three sentences, two mechanistic sources of bias that can affect this length-normalized spectral counting approach in comparison to precursor ion intensity at MS1 (MS1)–based quantification in proteomics, linking each bias to an underlying physical or biochemical cause.\n\nReport only the numerical value requested in task $2$ as your final answer. The quantity is dimensionless; express it as a decimal fraction rounded to four significant figures.",
            "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded in the principles of proteomics, well-posed, and objective. It contains all necessary data and definitions to proceed with a solution.\n\n### Task 1: Derivation of the Expression\n\nThe problem asks for an expression for the \"fraction of its length-normalized spectral count relative to the sum of length-normalized spectral counts across all detected proteins.\" Let us formalize this statement.\n\nLet the set of all $N$ detected proteins in the experiment be indexed by $j=1, 2, \\dots, N$. For any given protein $i$ in this set, we are given its spectral count, $SpC_i$, and its length in amino acids, $L_i$.\n\nThe \"length-normalized spectral count\" for protein $i$ is defined as the ratio of its spectral count to its length. Let us denote this quantity as $q_i$:\n$$q_i = \\frac{SpC_i}{L_i}$$\n\nThe \"sum of length-normalized spectral counts across all detected proteins\" is the summation of $q_j$ over all proteins $j=1, \\dots, N$:\n$$\\sum_{j=1}^{N} q_j = \\sum_{j=1}^{N} \\frac{SpC_j}{L_j}$$\n\nThe desired fraction for protein $i$, which we will call the Normalized Spectral Abundance Factor ($NSAF_i$), is the ratio of its length-normalized spectral count to this sum:\n$$NSAF_i = \\frac{q_i}{\\sum_{j=1}^{N} q_j} = \\frac{\\frac{SpC_i}{L_i}}{\\sum_{j=1}^{N} \\frac{SpC_j}{L_j}}$$\nThis is the closed-form expression derived directly from the problem's definition.\n\nIt is instructive to relate this back to the foundational model provided: $SpC_i \\approx k\\,A_i\\,L_i$. By rearranging this approximation, we can express the molar abundance $A_i$ in terms of the observables:\n$$A_i \\approx \\frac{1}{k} \\frac{SpC_i}{L_i} = \\frac{1}{k} q_i$$\nThe total molar abundance of all proteins is $\\sum_{j=1}^{N} A_j \\approx \\sum_{j=1}^{N} \\frac{1}{k} q_j = \\frac{1}{k} \\sum_{j=1}^{N} q_j$.\nThe relative molar abundance of protein $i$ is the fraction of its abundance over the total abundance:\n$$\\frac{A_i}{\\sum_{j=1}^{N} A_j} \\approx \\frac{\\frac{1}{k} q_i}{\\frac{1}{k} \\sum_{j=1}^{N} q_j} = \\frac{q_i}{\\sum_{j=1}^{N} q_j} = NSAF_i$$\nThus, the derived expression represents an estimate for the relative molar abundance of protein $i$.\n\n### Task 2: Calculation for Protein 2\n\nWe are asked to compute $NSAF_2$ for a dataset of $N=5$ proteins. The expression is:\n$$NSAF_2 = \\frac{\\frac{SpC_2}{L_2}}{\\sum_{j=1}^{5} \\frac{SpC_j}{L_j}}$$\nFirst, we compute the length-normalized spectral count, $q_j = \\frac{SpC_j}{L_j}$, for each of the $5$ proteins:\n$q_1 = \\frac{SpC_1}{L_1} = \\frac{120}{450} = \\frac{12}{45} = \\frac{4}{15}$\n$q_2 = \\frac{SpC_2}{L_2} = \\frac{150}{1100} = \\frac{15}{110} = \\frac{3}{22}$\n$q_3 = \\frac{SpC_3}{L_3} = \\frac{80}{300} = \\frac{8}{30} = \\frac{4}{15}$\n$q_4 = \\frac{SpC_4}{L_4} = \\frac{60}{800} = \\frac{6}{80} = \\frac{3}{40}$\n$q_5 = \\frac{SpC_5}{L_5} = \\frac{200}{1600} = \\frac{2}{16} = \\frac{1}{8}$\n\nNext, we compute the sum of these values for the denominator:\n$$\\sum_{j=1}^{5} q_j = q_1 + q_2 + q_3 + q_4 + q_5 = \\frac{4}{15} + \\frac{3}{22} + \\frac{4}{15} + \\frac{3}{40} + \\frac{1}{8}$$\nTo sum these fractions, we find a common denominator for $15$, $22$, $40$, and $8$. The prime factorizations are $15 = 3 \\times 5$, $22 = 2 \\times 11$, $40 = 2^3 \\times 5$, and $8 = 2^3$. The least common multiple is $2^3 \\times 3 \\times 5 \\times 11 = 8 \\times 3 \\times 5 \\times 11 = 1320$.\n$$\\sum_{j=1}^{5} q_j = \\frac{4 \\times 88}{1320} + \\frac{3 \\times 60}{1320} + \\frac{4 \\times 88}{1320} + \\frac{3 \\times 33}{1320} + \\frac{1 \\times 165}{1320}$$\n$$\\sum_{j=1}^{5} q_j = \\frac{352 + 180 + 352 + 99 + 165}{1320} = \\frac{1148}{1320}$$\n\nNow we can calculate $NSAF_2$:\n$$NSAF_2 = \\frac{q_2}{\\sum_{j=1}^{5} q_j} = \\frac{\\frac{3}{22}}{\\frac{1148}{1320}} = \\frac{3}{22} \\times \\frac{1320}{1148}$$\nSince $1320 = 22 \\times 60$, we can simplify:\n$$NSAF_2 = \\frac{3}{22} \\times \\frac{22 \\times 60}{1148} = \\frac{3 \\times 60}{1148} = \\frac{180}{1148}$$\nSimplifying the fraction by dividing the numerator and denominator by $4$:\n$$NSAF_2 = \\frac{45}{287}$$\nFinally, we compute the decimal value and round to four significant figures:\n$$NSAF_2 = \\frac{45}{287} \\approx 0.156794425...$$\nRounding to four significant figures gives $0.1568$.\n\n### Task 3: Discussion of Biases\n\nTwo primary biases affect length-normalized spectral counting relative to MS1-based precursor intensity methods. First, the stochastic Data-Dependent Acquisition (DDA) sampling of precursors for MS/MS leads to undersampling of less abundant peptides, introducing sampling bias and variance not seen in comprehensive MS1 scans. Second, the method incorrectly assumes all peptides have uniform ionization efficiency and detectability, ignoring major physicochemical variations that MS1 approaches can mitigate by tracking specific, well-behaved proteotypic peptides.",
            "answer": "$$\n\\boxed{0.1568}\n$$"
        },
        {
            "introduction": "The power of multi-omics lies in its ability to integrate different layers of biological information to arrive at more confident conclusions. This practice demonstrates this principle by tackling a common challenge in genomics: distinguishing a true, low-frequency genetic variant from sequencing noise. You will use a statistical likelihood ratio test where key parameters, such as tumor purity and subclonal fraction, are informed by other omics data, showcasing a practical application of data synergy.",
            "id": "4362814",
            "problem": "In a precision oncology setting, a multi-omics workflow integrates information from genomics, transcriptomics, proteomics, and metabolomics to refine variant detection and interpretation. Consider a diploid locus in a tumor sample profiled by high-depth targeted DNA sequencing. You are given the following:\n- DNA read counts at the locus: reference allele reads $r$ and alternate allele reads $a$.\n- An empirically estimated per-base sequencing error rate $\\epsilon$ derived from quality control using control DNA and corroborated by orthogonal Liquid Chromatography–Mass Spectrometry (LC–MS) proteomics calibration of error-prone contexts.\n- Tumor purity $\\pi$ estimated by an integrative proteogenomic deconvolution that combines RNA sequencing (RNA-seq) cell-type deconvolution with protein abundance signatures.\n- A subclonal cancer cell fraction $c$ harboring the variant inferred from single-cell RNA sequencing and copy-number-aware clonal reconstruction.\n\nAssume the following modeling principles as the fundamental base:\n- Central Dogma of Molecular Biology: deoxyribonucleic acid (DNA) encodes ribonucleic acid (RNA), which encodes protein. The presence of a coding DNA variant is necessary (though not sufficient) for the corresponding protein alteration.\n- Sequencing read generation is modeled as independent Bernoulli trials conditional on the true allele fraction.\n- Under the genotype $G=\\text{reference}$, any observed alternate reads arise from sequencing errors at probability $\\epsilon$ per read.\n- Under the genotype $G=\\text{variant}$, the variant is heterozygous and present only in a subclone that constitutes a fraction $c$ of the tumor cells within a tumor purity $\\pi$ mixture, with total diploid copy number and no allelic bias. The expected alternate-allele fraction under this model is therefore $f=\\frac{1}{2}\\pi c$.\n\nGiven the observed counts $r=178$ and $a=22$ (so the total number of reads is $n=r+a$), $\\epsilon=1.5\\times 10^{-3}$, $\\pi=0.64$, and $c=0.35$, compute the natural-log likelihood ratio\n$$\n\\ln\\frac{P(D\\mid G=\\text{variant})}{P(D\\mid G=\\text{reference})},\n$$\nwhere $D$ denotes the observed reads, using the Binomial likelihood for $P(D\\mid G)$ under each hypothesis as implied by the assumptions above.\n\nReport your final score as a pure number (no units) and round to four significant figures. Additionally, in your reasoning, explain how this score is calibrated in terms of evidential strength for the variant model under equal priors, but do not include this interpretation in the final numeric answer.",
            "solution": "The problem statement is critically validated before attempting a solution.\n\n### Step 1: Extract Givens\nThe data, variables, and assumptions provided in the problem statement are as follows:\n- DNA read counts: reference allele reads $r=178$, alternate allele reads $a=22$.\n- Total number of reads: $n = r + a$.\n- Per-base sequencing error rate: $\\epsilon = 1.5 \\times 10^{-3}$.\n- Tumor purity: $\\pi = 0.64$.\n- Subclonal cancer cell fraction with the variant: $c = 0.35$.\n- Model for sequencing reads: Independent Bernoulli trials, leading to a Binomial distribution for counts.\n- Hypothesis $G=\\text{reference}$: The probability of observing an alternate read is the sequencing error rate, $\\epsilon$.\n- Hypothesis $G=\\text{variant}$: The variant is heterozygous, present in a subclone of fraction $c$ within the tumor cell population of purity $\\pi$. The expected alternate-allele fraction is explicitly defined as $f = \\frac{1}{2}\\pi c$.\n- Task: Compute the natural-log likelihood ratio $\\ln\\frac{P(D\\mid G=\\text{variant})}{P(D\\mid G=\\text{reference})}$, where $D$ represents the observed read counts $(r, a)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the validation criteria:\n- **Scientifically Grounded**: The problem is set in the context of precision oncology and multi-omics, using standard concepts like tumor purity, clonality, sequencing error, and allele fractions. The models (Binomial likelihood for read counts) and statistical methods (likelihood ratio test) are fundamental to computational biology and bioinformatics. The scenario is realistic and scientifically sound.\n- **Well-Posed**: The problem provides all necessary data and defines the models for the two hypotheses clearly. The objective is to compute a specific, well-defined quantity. A unique, stable, and meaningful solution exists.\n- **Objective**: The problem is stated using precise, quantitative language. The models and parameters are explicitly defined, leaving no room for subjective interpretation.\n- **Completeness**: The problem is self-contained. All values ($r, a, \\epsilon, \\pi, c$) and model definitions required for the calculation are provided.\n- **Consistency**: The givens and assumptions are internally consistent. The formula $f=\\frac{1}{2}\\pi c$ is a standard and correct model for a heterozygous diploid variant in a subclonal population within a mixed sample, assuming no copy number alterations.\n\n### Step 3: Verdict and Action\nThe problem is deemed valid as it is scientifically grounded, well-posed, objective, and self-contained. Therefore, a complete solution will be provided.\n\n### Solution\nThe objective is to calculate the natural-log likelihood ratio for two competing hypotheses about the genotype at a specific genomic locus, given the observed DNA sequencing data. The data consists of $a=22$ alternate allele reads and $r=178$ reference allele reads, for a total of $n = a+r = 22+178 = 200$ reads.\n\nThe observation of $a$ alternate reads in $n$ total reads is modeled by a Binomial distribution. The probability mass function for observing $a$ successes in $n$ trials with a success probability of $p$ is:\n$$\nP(a \\mid n, p) = \\binom{n}{a} p^a (1-p)^{n-a}\n$$\nHere, a \"success\" is observing an alternate allele read. The data $D$ corresponds to the observed counts $(a, r)$.\n\n**Hypothesis 1: $G=\\text{reference}$ (Null Hypothesis)**\nUnder this hypothesis, the true genotype is homozygous reference. Any observed alternate reads are the result of sequencing errors. The problem states that the probability of observing an alternate read is the error rate, $\\epsilon$.\nLet $p_0$ be the probability of an alternate read under the reference model.\n$$\np_0 = \\epsilon = 1.5 \\times 10^{-3}\n$$\nThe likelihood of the data under the reference hypothesis is:\n$$\nP(D \\mid G=\\text{reference}) = \\binom{n}{a} p_0^a (1-p_0)^{n-a} = \\binom{200}{22} \\epsilon^{22} (1-\\epsilon)^{178}\n$$\n\n**Hypothesis 2: $G=\\text{variant}$ (Alternative Hypothesis)**\nUnder this hypothesis, a heterozygous variant exists in a fraction $c$ of the tumor cells, which themselves constitute a fraction $\\pi$ of the total sample. For a diploid locus with no copy number changes or allelic bias, one of the two alleles is the variant. The problem defines the expected alternate-allele fraction, $f$, in the entire DNA sample as:\n$$\nf = \\frac{1}{2} \\pi c\n$$\nThis value $f$ represents the probability of drawing a DNA molecule with the alternate allele from the sequenced pool. The problem implies we should use this directly as the probability parameter for the Binomial model under the variant hypothesis.\nLet $p_1$ be the probability of an alternate read under the variant model.\n$$\np_1 = f = \\frac{1}{2} \\pi c\n$$\nSubstituting the given values for $\\pi$ and $c$:\n$$\np_1 = \\frac{1}{2} (0.64) (0.35) = 0.32 \\times 0.35 = 0.112\n$$\nThe likelihood of the data under the variant hypothesis is:\n$$\nP(D \\mid G=\\text{variant}) = \\binom{n}{a} p_1^a (1-p_1)^{n-a} = \\binom{200}{22} f^{22} (1-f)^{178}\n$$\n\n**Likelihood Ratio Calculation**\nThe likelihood ratio is the ratio of the likelihoods under the two hypotheses:\n$$\n\\frac{P(D \\mid G=\\text{variant})}{P(D \\mid G=\\text{reference})} = \\frac{\\binom{200}{22} f^{22} (1-f)^{178}}{\\binom{200}{22} \\epsilon^{22} (1-\\epsilon)^{178}}\n$$\nThe binomial coefficient $\\binom{200}{22}$ cancels out:\n$$\n\\frac{P(D \\mid G=\\text{variant})}{P(D \\mid G=\\text{reference})} = \\frac{f^{22} (1-f)^{178}}{\\epsilon^{22} (1-\\epsilon)^{178}} = \\left(\\frac{f}{\\epsilon}\\right)^{22} \\left(\\frac{1-f}{1-\\epsilon}\\right)^{178}\n$$\nThe problem asks for the natural-log likelihood ratio, which we denote as $\\mathcal{L}$:\n$$\n\\mathcal{L} = \\ln\\left( \\frac{P(D \\mid G=\\text{variant})}{P(D \\mid G=\\text{reference})} \\right) = \\ln\\left[ \\left(\\frac{f}{\\epsilon}\\right)^{a} \\left(\\frac{1-f}{1-\\epsilon}\\right)^{n-a} \\right]\n$$\nUsing the properties of logarithms, this becomes:\n$$\n\\mathcal{L} = a \\ln\\left(\\frac{f}{\\epsilon}\\right) + (n-a) \\ln\\left(\\frac{1-f}{1-\\epsilon}\\right)\n$$\n$$\n\\mathcal{L} = a (\\ln(f) - \\ln(\\epsilon)) + (n-a) (\\ln(1-f) - \\ln(1-\\epsilon))\n$$\nNow, we substitute the numerical values:\n- $a = 22$\n- $n-a = r = 178$\n- $f = 0.112$\n- $\\epsilon = 0.0015$\n\nThe terms are:\n- $1-f = 1 - 0.112 = 0.888$\n- $1-\\epsilon = 1 - 0.0015 = 0.9985$\n\nSubstituting these into the expression for $\\mathcal{L}$:\n$$\n\\mathcal{L} = 22 \\ln\\left(\\frac{0.112}{0.0015}\\right) + 178 \\ln\\left(\\frac{0.888}{0.9985}\\right)\n$$\nLet's compute the values of the ratios first:\n$$\n\\frac{0.112}{0.0015} \\approx 74.6666...\n$$\n$$\n\\frac{0.888}{0.9985} \\approx 0.889334...\n$$\nNow, compute the natural logarithms:\n$$\n\\ln(74.6666...) \\approx 4.3129517\n$$\n$$\n\\ln(0.889334...) \\approx -0.1172803\n$$\nFinally, compute $\\mathcal{L}$:\n$$\n\\mathcal{L} \\approx 22 \\times (4.3129517) + 178 \\times (-0.1172803)\n$$\n$$\n\\mathcal{L} \\approx 94.8849374 - 20.8759040\n$$\n$$\n\\mathcal{L} \\approx 74.0090334\n$$\nRounding the result to four significant figures gives $74.01$.\n\n**Interpretation of the Evidential Strength**\nThe computed value, $\\mathcal{L} = 74.01$, is the natural logarithm of the likelihood ratio. This quantity is also known as the log-Bayes factor, assuming the models have no free parameters to be integrated out. With equal prior probabilities for the two hypotheses ($P(G=\\text{variant}) = P(G=\\text{reference})$), the posterior odds equal the likelihood ratio (or Bayes factor).\n$$\n\\frac{P(G=\\text{variant} \\mid D)}{P(G=\\text{reference} \\mid D)} = \\frac{P(D \\mid G=\\text{variant})}{P(D \\mid G=\\text{reference})} \\times \\frac{P(G=\\text{variant})}{P(G=\\text{reference})} = \\exp(\\mathcal{L}) \\times 1\n$$\nThe posterior odds in favor of the variant model are $\\exp(74.01)$, which is an astronomically large number (approximately $1.16 \\times 10^{32}$). This indicates that the observed data are overwhelmingly more probable under the variant model than under the reference (error-only) model. Established scales for interpreting Bayes factors, such as the Kass-Raftery scale, categorize a log-Bayes factor greater than $5$ as \"very strong\" evidence. A value of $74.01$ provides exceptionally strong evidence supporting the hypothesis that a real subclonal variant exists, as opposed to the reads arising from sequencing artifacts alone.",
            "answer": "$$\\boxed{74.01}$$"
        },
        {
            "introduction": "Truly holistic multi-omics analysis requires methods that can synthesize diverse data types into a unified systems-level view. This hands-on coding exercise guides you through the implementation of one such powerful technique: Random Walk with Restart (RWR) on a protein-protein interaction network. You will build an algorithm that integrates signals from genomics, transcriptomics, proteomics, and metabolomics to prioritize disease-relevant nodes within a biological network, providing a tangible example of network medicine.",
            "id": "4362874",
            "problem": "You are given a scenario in precision medicine where multi-omics evidence from Genomics, Transcriptomics, Proteomics, and Metabolomics must be integrated on a Protein–Protein Interaction (PPI) graph to prioritize nodes by their network-contextualized relevance. The fundamental bases are the Central Dogma of Molecular Biology (Deoxyribonucleic Acid (DNA) to Ribonucleic Acid (RNA) to Protein), graph-based diffusion to model molecular interaction flow, and Markov chain dynamics to describe random walks on networks. Construct a network propagation algorithm on the PPI graph that integrates multi-omics node-level evidence as an initial distribution and computes the steady-state Random Walk with Restart (RWR) diffusion scores given a restart probability. The algorithm must follow first principles of probability conservation and Markov chain dynamics.\n\nAlgorithmic setting:\n- Let the PPI graph be represented by an adjacency matrix $A \\in \\mathbb{R}^{n \\times n}$ with nonnegative entries, where $A_{ij} > 0$ indicates an undirected interaction between node $i$ and node $j$. Use an undirected graph representation where $A$ is symmetric with $A_{ii} = 0$ for all $i$ unless specified otherwise.\n- Define a column-stochastic transition matrix $W \\in \\mathbb{R}^{n \\times n}$ constructed from $A$, where each column $j$ is normalized by its sum. For columns with zero sum, set a self-loop by $W_{jj} = 1$ and $W_{ij} = 0$ for all $i \\neq j$. This encodes the probability that a random walker at node $j$ moves to node $i$.\n- Let there be $L = 4$ omics layers: Genomics, Transcriptomics, Proteomics, and Metabolomics. For each layer $l \\in \\{1,2,3,4\\}$, you are given a raw node-level evidence vector $x^{(l)} \\in \\mathbb{R}^{n}$. Preprocess each $x^{(l)}$ into a nonnegative probability distribution $p^{(l)}$ using rectification and $\\ell_1$-normalization:\n  1. Rectify negatives: $u^{(l)}_i = \\max(0, x^{(l)}_i)$ for all $i$.\n  2. If $\\|u^{(l)}\\|_{1} > 0$, set $p^{(l)} = u^{(l)} / \\|u^{(l)}\\|_{1}$; otherwise, set $p^{(l)}$ to the all-zero vector of length $n$.\n- Given nonnegative layer weights $\\alpha_{l}$, form the aggregated seed $s' = \\sum_{l=1}^{L} \\alpha_{l} p^{(l)}$. If $\\|s'\\|_{1} > 0$, set $s = s' / \\|s'\\|_{1}$; otherwise, set the uniform distribution $s_i = 1/n$ for all $i$.\n- Let the restart probability be $r \\in (0,1)$, and define the RWR iteration by $f_{t+1} = (1 - r) W f_{t} + r s$ with $f_{0} = s$. Iterate until convergence in the $\\ell_1$ norm, i.e., stop at the first $t$ such that $\\|f_{t+1} - f_{t}\\|_{1} < \\varepsilon$, and return the steady-state vector $f^{*}$.\n\nYour task is to implement a complete, runnable program that builds $W$ from $A$, constructs $s$ from the given multi-omics inputs and weights, and computes $f^{*}$ using the iterative scheme described above for the specified test suite. All numerical outputs must be floats rounded to $6$ decimal places.\n\nTest suite:\n- Case $1$ (happy path, connected graph):\n  - $n = 5$,\n  - $$A = \\begin{bmatrix} 0  1  1  0  0 \\\\ 1  0  1  1  0 \\\\ 1  1  0  1  0 \\\\ 0  1  1  0  1 \\\\ 0  0  0  1  0 \\end{bmatrix}$$,\n  - Genomics $x^{(g)} = [2,0,1,0,0.5]$,\n  - Transcriptomics $x^{(t)} = [0.1,4,0,0,0]$,\n  - Proteomics $x^{(p)} = [0,0,3,0,1]$,\n  - Metabolomics $x^{(m)} = [0,0,0.5,2,0]$,\n  - Weights $(\\alpha_{g},\\alpha_{t},\\alpha_{p},\\alpha_{m}) = (0.25,0.25,0.25,0.25)$,\n  - Restart probability $r = 0.3$,\n  - Convergence tolerance $\\varepsilon = 10^{-12}$.\n- Case $2$ (boundary: small restart probability):\n  - Same $A$, $x^{(g)}$, $x^{(t)}$, $x^{(p)}$, $x^{(m)}$, and weights as Case $1$,\n  - Restart probability $r = 0.01$,\n  - Convergence tolerance $\\varepsilon = 10^{-12}$.\n- Case $3$ (boundary: large restart probability):\n  - Same $A$, $x^{(g)}$, $x^{(t)}$, $x^{(p)}$, $x^{(m)}$, and weights as Case $1$,\n  - Restart probability $r = 0.99$,\n  - Convergence tolerance $\\varepsilon = 10^{-12}$.\n- Case $4$ (edge case: disconnected graph with an isolated node):\n  - $n = 4$,\n  - $$A = \\begin{bmatrix} 0  1  0  0 \\\\ 1  0  1  0 \\\\ 0  1  0  0 \\\\ 0  0  0  0 \\end{bmatrix}$$,\n  - Genomics $x^{(g)} = [0,5,0,0]$,\n  - Transcriptomics $x^{(t)} = [0,0,0,0]$,\n  - Proteomics $x^{(p)} = [0,0,1,0]$,\n  - Metabolomics $x^{(m)} = [0,0,0,0]$,\n  - Weights $(\\alpha_{g},\\alpha_{t},\\alpha_{p},\\alpha_{m}) = (0.5,0,0.5,0)$,\n  - Restart probability $r = 0.5$,\n  - Convergence tolerance $\\varepsilon = 10^{-12}$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, where each test case result is itself a list of $n$ floats rounded to $6$ decimal places, with no spaces. For example, a valid format is $[[0.123456,0.234567],[0.111111,0.222222]]$.\n\nConstraints and notes:\n- Use only the specified runtime environment and libraries.\n- The algorithm must preserve probability mass at each iteration, and convergence must be assessed using the $\\ell_1$ norm.\n- If the aggregated seed $s'$ is the zero vector, use the uniform distribution $s_i = 1/n$.\n- For columns of $A$ with zero sum, set a self-loop $W_{jj} = 1$.\n- The computation must be performed and returned as floats rounded to $6$ decimal places.",
            "solution": "The problem presented is valid. It outlines a scientifically grounded and mathematically well-posed task: to implement the Random Walk with Restart (RWR) algorithm for integrating multi-omics data on a Protein-Protein Interaction (PPI) network. The problem provides all necessary definitions, data, and parameters, with no internal contradictions or ambiguities. The methodology is standard in network biology for node prioritization.\n\nThe solution will be developed by implementing the following sequence of steps, which directly correspond to the formal specification provided in the problem statement.\n\n**1. Construction of the Column-Stochastic Transition Matrix ($W$)**\n\nThe PPI network is given by a symmetric adjacency matrix $A$, where $A_{ij} > 0$ signifies an interaction between nodes $i$ and $j$. The first step is to convert this structural information into a probabilistic model of information flow. This is achieved by constructing a column-stochastic transition matrix $W \\in \\mathbb{R}^{n \\times n}$. Each entry $W_{ij}$ represents the probability of a random walker at node $j$ transitioning to an adjacent node $i$.\n\nFor a node $j$, its degree (or weighted degree) is the sum of its connections, $d_j = \\sum_{k=1}^{n} A_{kj}$, which is the sum of the $j$-th column of $A$. If $d_j > 0$, the transition probabilities in the $j$-th column of $W$ are obtained by normalizing the corresponding column of $A$:\n$$W_{ij} = \\frac{A_{ij}}{d_j} = \\frac{A_{ij}}{\\sum_{k=1}^{n} A_{kj}}$$\nThis ensures that $\\sum_{i=1}^{n} W_{ij} = 1$, meaning the probability of the walker moving from node $j$ to any other node is conserved.\n\nA special case arises for isolated nodes, where $d_j = 0$. For such a node, the walker has nowhere to move. The problem specifies that in this scenario, a self-loop is created. This is modeled by setting the diagonal entry $W_{jj} = 1$ and all other entries in that column, $W_{ij}$ for $i \\neq j$, to $0$. This maintains the column-stochastic property $\\sum_{i=1}^{n} W_{ij} = 1$ for all columns $j$.\n\n**2. Multi-Omics Evidence Integration and Seed Vector ($s$) Construction**\n\nThe RWR algorithm requires a starting probability distribution, or a \"seed\" vector $s \\in \\mathbb{R}^{n}$, which specifies the initial nodes of interest. This vector is constructed by integrating evidence from $L=4$ different omics layers.\n\nFor each omics layer $l \\in \\{g, t, p, m\\}$ (Genomics, Transcriptomics, Proteomics, Metabolomics), we are given a raw evidence vector $x^{(l)} \\in \\mathbb{R}^{n}$. These raw vectors must be processed into normalized probability distributions $p^{(l)}$. This is a two-step process:\n\na. **Rectification**: The raw evidence can contain negative values, which are not meaningful as probabilities. We rectify the vector by taking the maximum of each element and zero:\n$$u^{(l)}_i = \\max(0, x^{(l)}_i)$$\n\nb. **$\\ell_1$-Normalization**: The rectified vector $u^{(l)}$ is normalized to sum to $1$, converting it into a probability distribution $p^{(l)}$. If the $\\ell_1$-norm $\\|u^{(l)}\\|_1 = \\sum_{i=1}^n u^{(l)}_i$ is greater than zero, the normalization is:\n$$p^{(l)} = \\frac{u^{(l)}}{\\|u^{(l)}\\|_1}$$\nIf $\\|u^{(l)}\\|_1 = 0$, it implies no evidence from that layer, so $p^{(l)}$ is the all-zero vector.\n\nAfter processing each omics layer, the resulting probability vectors $p^{(l)}$ are combined into a single aggregated seed vector $s$. This is done via a weighted sum, using the provided layer weights $\\alpha_l$:\n$$s' = \\sum_{l=1}^{L} \\alpha_{l} p^{(l)}$$\nThe weights $\\alpha_l$ determine the relative contribution of each omics layer to the final seed. The resulting vector $s'$ is then normalized to ensure it is a valid probability distribution itself:\n$$s = \\frac{s'}{\\|s'\\|_1}$$\nThis normalization is performed if $\\|s'\\|_1 > 0$. If $\\|s'\\|_1 = 0$ (which occurs if all $p^{(l)}$ are zero or have zero weight), the algorithm reverts to a non-informative prior, setting $s$ to the uniform distribution, $s_i = 1/n$ for all $i=1, \\dots, n$.\n\n**3. Iterative Computation of the Steady-State Distribution ($f^*$): Random Walk with Restart**\n\nThe core of the algorithm is the iterative computation of the RWR steady-state distribution, denoted $f^*$. This vector represents the long-term probability of finding the random walker at each node, considering both the network structure and the initial seed distribution.\n\nThe process starts with an initial distribution $f_0 = s$. The distribution is then updated at each step $t$ according to the recurrence relation:\n$$f_{t+1} = (1 - r) W f_{t} + r s$$\nThis equation has two components:\n- The **walk** component, $(1 - r) W f_t$: With probability $(1-r)$, the walker takes a step on the graph from its current position, following the transition probabilities in $W$.\n- The **restart** component, $rs$: With probability $r$, the walker teleports back to a node chosen from the seed distribution $s$. This restart mechanism ensures that the final scores are influenced by the initial evidence, preventing the walker from diffusing infinitely far from the seed nodes.\n\nThe iteration continues until the process converges to a steady state. Convergence is achieved when the change between successive probability distributions is negligibly small. This is assessed by measuring the $\\ell_1$-norm of the difference between consecutive vectors:\n$$\\|f_{t+1} - f_{t}\\|_{1} = \\sum_{i=1}^{n} |(f_{t+1})_i - (f_t)_i|$$\nThe iteration stops at the first step $t$ for which this difference falls below a predefined tolerance $\\varepsilon$:\n$$\\|f_{t+1} - f_{t}\\|_{1}  \\varepsilon$$\nThe resulting vector $f_{t+1}$ is the desired steady-state distribution $f^*$, whose elements provide the network-contextualized relevance scores for each node. According to the Banach fixed-point theorem, for $r \\in (0,1)$ and a stochastic matrix $W$, this iterative process is guaranteed to converge to a unique fixed point. This completes the algorithmic procedure. The final scores are then rounded to $6$ decimal places as required for the output.",
            "answer": "```python\nimport numpy as np\n\ndef run_rwr(A, omics_data, weights, r, epsilon, n):\n    \"\"\"\n    Computes the Random Walk with Restart steady-state distribution.\n\n    Args:\n        A (np.ndarray): The adjacency matrix of the graph.\n        omics_data (list): A list of raw node-level evidence vectors [x_g, x_t, x_p, x_m].\n        weights (tuple): A tuple of weights (alpha_g, alpha_t, alpha_p, alpha_m).\n        r (float): The restart probability.\n        epsilon (float): The convergence tolerance.\n        n (int): The number of nodes in the graph.\n\n    Returns:\n        list: The steady-state vector f*, with elements rounded to 6 decimal places.\n    \"\"\"\n    # Step 1: Construct the column-stochastic transition matrix W\n    W = np.zeros((n, n), dtype=float)\n    col_sums = A.sum(axis=0)\n    \n    for j in range(n):\n        if col_sums[j] > 0:\n            W[:, j] = A[:, j] / col_sums[j]\n        else:\n            # Handle isolated nodes with a self-loop\n            W[j, j] = 1.0\n\n    # Step 2: Preprocess omics data and construct the seed vector s\n    p_vectors = []\n    for x_l in omics_data:\n        u_l = np.maximum(0, x_l)\n        norm_u = np.sum(u_l)\n        if norm_u > 0:\n            p_l = u_l / norm_u\n        else:\n            p_l = np.zeros(n, dtype=float)\n        p_vectors.append(p_l)\n\n    s_prime = np.zeros(n, dtype=float)\n    for i in range(len(weights)):\n        s_prime += weights[i] * p_vectors[i]\n\n    norm_s_prime = np.sum(s_prime)\n    if norm_s_prime > 0:\n        s = s_prime / norm_s_prime\n    else:\n        # Fallback to uniform distribution if aggregated seed is zero vector\n        s = np.full(n, 1.0 / n, dtype=float)\n\n    # Step 3: Iterative RWR computation\n    f_t = s.copy()\n    \n    while True:\n        # f_{t+1} = (1 - r) * W * f_t + r * s\n        f_t_plus_1 = (1 - r) * (W @ f_t) + r * s\n        \n        # Check for convergence using the l1 norm\n        diff = np.sum(np.abs(f_t_plus_1 - f_t))\n        \n        f_t = f_t_plus_1\n        \n        if diff  epsilon:\n            break\n            \n    # Round the final result to 6 decimal places\n    return [round(val, 6) for val in f_t.tolist()]\n\ndef solve():\n    \"\"\"\n    Defines and runs the test cases for the RWR algorithm.\n    \"\"\"\n    test_cases = [\n        {\n            \"n\": 5,\n            \"A\": np.array([\n                [0, 1, 1, 0, 0],\n                [1, 0, 1, 1, 0],\n                [1, 1, 0, 1, 0],\n                [0, 1, 1, 0, 1],\n                [0, 0, 0, 1, 0]\n            ], dtype=float),\n            \"omics_data\": [\n                np.array([2, 0, 1, 0, 0.5]),    # Genomics\n                np.array([0.1, 4, 0, 0, 0]),    # Transcriptomics\n                np.array([0, 0, 3, 0, 1]),      # Proteomics\n                np.array([0, 0, 0.5, 2, 0])     # Metabolomics\n            ],\n            \"weights\": (0.25, 0.25, 0.25, 0.25),\n            \"r\": 0.3,\n            \"epsilon\": 1e-12\n        },\n        {\n            \"n\": 5,\n            \"A\": np.array([\n                [0, 1, 1, 0, 0],\n                [1, 0, 1, 1, 0],\n                [1, 1, 0, 1, 0],\n                [0, 1, 1, 0, 1],\n                [0, 0, 0, 1, 0]\n            ], dtype=float),\n            \"omics_data\": [\n                np.array([2, 0, 1, 0, 0.5]),\n                np.array([0.1, 4, 0, 0, 0]),\n                np.array([0, 0, 3, 0, 1]),\n                np.array([0, 0, 0.5, 2, 0])\n            ],\n            \"weights\": (0.25, 0.25, 0.25, 0.25),\n            \"r\": 0.01,\n            \"epsilon\": 1e-12\n        },\n        {\n            \"n\": 5,\n            \"A\": np.array([\n                [0, 1, 1, 0, 0],\n                [1, 0, 1, 1, 0],\n                [1, 1, 0, 1, 0],\n                [0, 1, 1, 0, 1],\n                [0, 0, 0, 1, 0]\n            ], dtype=float),\n            \"omics_data\": [\n                np.array([2, 0, 1, 0, 0.5]),\n                np.array([0.1, 4, 0, 0, 0]),\n                np.array([0, 0, 3, 0, 1]),\n                np.array([0, 0, 0.5, 2, 0])\n            ],\n            \"weights\": (0.25, 0.25, 0.25, 0.25),\n            \"r\": 0.99,\n            \"epsilon\": 1e-12\n        },\n        {\n            \"n\": 4,\n            \"A\": np.array([\n                [0, 1, 0, 0],\n                [1, 0, 1, 0],\n                [0, 1, 0, 0],\n                [0, 0, 0, 0]\n            ], dtype=float),\n            \"omics_data\": [\n                np.array([0, 5, 0, 0]),\n                np.array([0, 0, 0, 0]),\n                np.array([0, 0, 1, 0]),\n                np.array([0, 0, 0, 0])\n            ],\n            \"weights\": (0.5, 0, 0.5, 0),\n            \"r\": 0.5,\n            \"epsilon\": 1e-12\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_rwr(\n            case[\"A\"],\n            case[\"omics_data\"],\n            case[\"weights\"],\n            case[\"r\"],\n            case[\"epsilon\"],\n            case[\"n\"]\n        )\n        results.append(result)\n\n    # Format output as a list of lists, without spaces.\n    formatted_results = [f\"[{','.join(map(str, res))}]\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}