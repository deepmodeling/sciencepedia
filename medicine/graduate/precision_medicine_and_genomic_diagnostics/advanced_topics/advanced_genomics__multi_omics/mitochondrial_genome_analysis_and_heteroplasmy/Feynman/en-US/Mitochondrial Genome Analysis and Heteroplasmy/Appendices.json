{
    "hands_on_practices": [
        {
            "introduction": "This exercise explores a fundamental source of variation in mitochondrial heteroplasmy transmission: the genetic bottleneck that occurs during the formation of oocytes. By modeling this biological event as a random sampling process, you will derive from first principles the variance in an offspring's heteroplasmy level based on the mother's. This practice is crucial for understanding the often-unpredictable inheritance patterns of mitochondrial diseases and for appreciating the stochastic nature of their transmission .",
            "id": "4360537",
            "problem": "A clinical genomics laboratory is modeling the change in mitochondrial heteroplasmy during the maternal germline bottleneck to better predict transmission risk of a pathogenic variant. Let heteroplasmy be defined as the fraction of mitochondrial deoxyribonucleic acid (mtDNA) molecules carrying the variant. Suppose that, at the bottleneck, an oocyte effectively samples $N_b$ mitochondrial genomes independently from a very large maternal pool in which the true heteroplasmy is $p$, with $0  p  1$. Let $\\hat{p}$ denote the observed post-bottleneck heteroplasmy in the oocyte, defined as the proportion of sampled genomes that carry the variant.\n\nStarting from the definitions of expectation and variance, and the independence of the sampled genomes, derive a closed-form expression for the variance of $\\hat{p}$ in terms of $p$ and $N_b$. Express your final answer as a single analytical expression. No rounding is required, and no physical units apply.",
            "solution": "The problem statement is deemed valid. It is scientifically grounded in the principles of population genetics and statistical theory, specifically modeling genetic drift via binomial sampling. The problem is well-posed, objective, and contains all necessary information to derive a unique solution.\n\nThe objective is to derive an expression for the variance of the observed post-bottleneck heteroplasmy, $\\hat{p}$, which is the sample proportion of mitochondrial genomes carrying a specific variant. The derivation begins from the fundamental definitions of expectation and variance.\n\nLet the sampling of $N_b$ mitochondrial genomes be modeled as a sequence of $N_b$ independent trials. We define a set of indicator random variables, $\\{X_1, X_2, \\dots, X_{N_b}\\}$, where $X_i$ corresponds to the $i$-th sampled genome. The variable $X_i$ takes a value of $1$ if the genome carries the variant and $0$ otherwise.\n$$\nX_i =\n\\begin{cases}\n1  \\text{if the } i\\text{-th genome has the variant} \\\\\n0  \\text{otherwise}\n\\end{cases}\n$$\nThe problem states that the underlying true heteroplasmy in the large maternal pool is $p$. Because the pool is very large and the sampling is independent, each $X_i$ is an independent and identically distributed (i.i.d.) Bernoulli random variable with parameter $p$. The probability mass function for each $X_i$ is:\n$$\nP(X_i = 1) = p\n$$\n$$\nP(X_i = 0) = 1-p\n$$\n\nThe problem requires a derivation starting from the definitions of expectation and variance. First, we determine the expectation, $E[X_i]$, and variance, $\\text{Var}(X_i)$, for a single trial.\n\nThe expectation of $X_i$ is defined as the sum of the possible outcomes weighted by their probabilities:\n$$\nE[X_i] = \\sum_{x \\in \\{0,1\\}} x \\cdot P(X_i=x) = (1)(p) + (0)(1-p) = p\n$$\nTo find the variance, we use the formula $\\text{Var}(X_i) = E[X_i^2] - (E[X_i])^2$. We first compute $E[X_i^2]$:\n$$\nE[X_i^2] = \\sum_{x \\in \\{0,1\\}} x^2 \\cdot P(X_i=x) = (1^2)(p) + (0^2)(1-p) = p\n$$\nNow, we can calculate the variance of $X_i$:\n$$\n\\text{Var}(X_i) = E[X_i^2] - (E[X_i])^2 = p - p^2 = p(1-p)\n$$\nThe observed heteroplasmy, $\\hat{p}$, is defined as the proportion of sampled genomes that carry the variant. This is the sample mean of the indicator variables:\n$$\n\\hat{p} = \\frac{1}{N_b} \\sum_{i=1}^{N_b} X_i\n$$\nWe wish to find $\\text{Var}(\\hat{p})$. Using the properties of variance, for any constant $c$ and random variable $Y$, $\\text{Var}(cY) = c^2\\text{Var}(Y)$. In our case, the constant is $\\frac{1}{N_b}$:\n$$\n\\text{Var}(\\hat{p}) = \\text{Var}\\left(\\frac{1}{N_b} \\sum_{i=1}^{N_b} X_i\\right) = \\left(\\frac{1}{N_b}\\right)^2 \\text{Var}\\left(\\sum_{i=1}^{N_b} X_i\\right) = \\frac{1}{N_b^2} \\text{Var}\\left(\\sum_{i=1}^{N_b} X_i\\right)\n$$\nA key property of variance is that for a sum of independent random variables, the variance of the sum is the sum of the variances. Since the $X_i$ are independent:\n$$\n\\text{Var}\\left(\\sum_{i=1}^{N_b} X_i\\right) = \\sum_{i=1}^{N_b} \\text{Var}(X_i)\n$$\nSince the $X_i$ are also identically distributed, we have $\\text{Var}(X_i) = p(1-p)$ for all $i$ from $1$ to $N_b$. Therefore, the sum of the variances is:\n$$\n\\sum_{i=1}^{N_b} \\text{Var}(X_i) = \\sum_{i=1}^{N_b} p(1-p) = N_b p(1-p)\n$$\nSubstituting this result back into the expression for $\\text{Var}(\\hat{p})$:\n$$\n\\text{Var}(\\hat{p}) = \\frac{1}{N_b^2} \\left[ N_b p(1-p) \\right]\n$$\nFinally, simplifying this expression gives the closed-form expression for the variance of $\\hat{p}$:\n$$\n\\text{Var}(\\hat{p}) = \\frac{p(1-p)}{N_b}\n$$\nThis result demonstrates that the variance of the observed heteroplasmy is inversely proportional to the bottleneck size $N_b$ and is maximized when the true heteroplasmy $p$ is $0.5$.",
            "answer": "$$\n\\boxed{\\frac{p(1-p)}{N_{b}}}\n$$"
        },
        {
            "introduction": "Accurate heteroplasmy quantification can be severely compromised by technical artifacts, with polymerase chain reaction (PCR) amplification bias being a primary culprit. This exercise challenges you to mathematically model how allele-specific amplification efficiencies systematically skew observed heteroplasmy measurements away from the true biological fraction . Furthermore, you will evaluate the power of modern tools like Unique Molecular Identifiers (UMIs) and computational correction to mitigate this bias, a skill essential for any robust quantitative sequencing study.",
            "id": "4360603",
            "problem": "A clinical mitochondrial genome assay aims to quantify heteroplasmy at a single nucleotide variant in mitochondrial DNA. Let the true per-molecule variant allele fraction be $p \\in (0,1)$ among $N$ original double-stranded mitochondrial genomes extracted from a single sample. Library preparation uses polymerase chain reaction (PCR) with $c$ cycles. Due to sequence-context effects, the per-cycle amplification efficiencies differ between the variant allele and the reference allele; let these per-cycle efficiencies be $e_{V}$ and $e_{R}$, respectively, and assume they are constant across cycles. Sequencing samples amplicons uniformly at random from the post-PCR pool to produce reads. The observed variant read fraction is $\\hat{p} = R_V / (R_V + R_R)$, where $R_V$ and $R_R$ are variant and reference read counts.\n\nStarting from the following fundamentals:\n- PCR is a branching process where, in each cycle, the expected copy number of a template is multiplied by its per-cycle efficiency.\n- Random sampling of molecules without allelic bias implies that the expected read fraction equals the fraction of molecules in the library prior to sampling.\n- A Unique Molecular Identifier (UMI) is a random barcode ligated to each original molecule before amplification; deduplicating reads by UMI collapses all reads from the same original molecule to a single consensus, so counts reflect the number of distinct captured templates rather than the number of PCR products.\n\nDerive, from these principles and without invoking any unproven shortcut formulas, how allele-specific PCR efficiency skews $\\hat{p}$ away from $p$ as a function of $c$, $e_{V}$, and $e_{R}$. Then consider the use of Unique Molecular Identifiers (UMIs) to correct PCR-induced bias by collapsing duplicates. Assume negligible UMI collisions and no allelic mapping bias.\n\nWhich of the following statements are correct? Select all that apply.\n\nA. If $\\rho = e_{V}/e_{R}$ is the per-cycle relative efficiency, then under large post-PCR molecule counts and unbiased read sampling,\n$$\n\\mathbb{E}[\\hat{p}] \\;=\\; \\frac{p\\,\\rho^{c}}{(1-p) + p\\,\\rho^{c}}.\n$$\n\nB. Under allele-specific efficiency, the expected read fraction is linear in the relative amplification, so\n$$\n\\mathbb{E}[\\hat{p}] \\;=\\; p\\,\\rho^{c},\n$$\nand no normalization by the reference allele is required.\n\nC. Increasing the total read depth to infinity eliminates PCR amplification bias, so $\\lim_{D\\to\\infty}\\mathbb{E}[\\hat{p}\\,|\\,\\text{depth}=D]=p$ even if $\\rho \\neq 1$.\n\nD. Deduplicating with UMIs can correct PCR bias only if UMIs are attached after PCR, because the bias must be observed before it can be removed.\n\nE. If the numbers of captured original variant and reference molecules are independent Poisson random variables with means $\\lambda p$ and $\\lambda(1-p)$, respectively, then deduplication by UMIs yields the estimator $\\tilde{p} = U_V / (U_V + U_R)$, which is unbiased for $p$ conditional on $U_V+U_R>0$ when $\\rho \\neq 1$, but has larger variance than $\\hat{p}$ because $U_V+U_R$ is typically much smaller than $R_V+R_R$.\n\nF. If $\\rho$ is known, a debiased estimator of $p$ based on the observed read fraction is\n$$\n\\tilde{p} \\;=\\; \\frac{\\hat{p}}{\\rho^{c}(1-\\hat{p}) + \\hat{p}},\n$$\nwhich is consistent for $p$ as the number of unique molecules grows, assuming equal pre-PCR capture probabilities for the two alleles and no allelic mapping bias.",
            "solution": "The problem statement will first be validated for scientific and logical soundness.\n\n### Step 1: Extract Givens\nThe givens from the problem statement are:\n- True per-molecule variant allele fraction: $p \\in (0,1)$.\n- Number of original double-stranded mitochondrial genomes: $N$.\n- Number of PCR cycles: $c$.\n- Per-cycle amplification efficiency for the variant allele: $e_V$.\n- Per-cycle amplification efficiency for the reference allele: $e_R$.\n- The efficiencies $e_V$ and $e_R$ are constant across cycles.\n- Observed variant read fraction: $\\hat{p} = R_{V}/(R_{V}+R_{R})$, where $R_V$ and $R_R$ are variant and reference read counts, respectively.\n- Fundamental Principle 1: PCR is a branching process where, in each cycle, the expected copy number of a template is multiplied by its per-cycle efficiency.\n- Fundamental Principle 2: Random sampling of molecules without allelic bias implies that the expected read fraction equals the fraction of molecules in the library prior to sampling.\n- Fundamental Principle 3: A Unique Molecular Identifier (UMI) is a random barcode ligated to each original molecule before amplification. Deduplicating reads by UMI collapses all reads from the same original molecule to a single consensus, so counts reflect the number of distinct captured templates.\n- Assumptions for the UMI part: Negligible UMI collisions and no allelic mapping bias.\n\n### Step 2: Validate Using Extracted Givens\nThe problem describes a standard, albeit simplified, model of quantitative next-generation sequencing (NGS), specifically addressing the issue of PCR amplification bias and its correction using either computational methods or UMIs.\n\n- **Scientifically Grounded:** The model is firmly based on established principles of molecular biology (PCR) and biostatistics (sampling theory). The concept of allele-specific PCR efficiency is a well-documented phenomenon in real-world assays. The use of UMIs to mitigate this bias is a cornerstone of modern quantitative sequencing. The model is a valid and widely used simplification of a real-world process.\n- **Well-Posed:** The problem is clearly stated. It asks for a derivation based on provided first principles and then an evaluation of specific statements based on that derivation. It provides all necessary variables and definitions to proceed. A unique mathematical relationship between the quantities can be derived.\n- **Objective:** The language is formal, technical, and devoid of subjectivity or bias. All terms are either standard in the field or explicitly defined.\n\nThe problem statement is free from the flaws listed in the instructions. It is scientifically sound, well-posed, and objective.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be derived.\n\n### Derivation from First Principles\n\nLet's derive the relationship between the true allele fraction $p$ and the expected observed read fraction $\\mathbb{E}[\\hat{p}]$ without UMIs.\n\n$1$. **Initial state:** We start with $N$ total molecules from a single sample. The number of variant molecules is $N_V = pN$ and the number of reference molecules is $N_R = (1-p)N$.\n\n$2$. **PCR Amplification:** According to Fundamental Principle 1, PCR acts as a branching process. After one cycle, the expected number of variant molecules is $(pN) \\cdot e_V$ and reference molecules is $((1-p)N) \\cdot e_R$. After $c$ cycles, assuming constant efficiency, the expected number of molecules for each allele is:\n$$ \\mathbb{E}[\\text{Number of variant molecules post-PCR}] = N_{V, post} = (pN) \\cdot (e_V)^c $$\n$$ \\mathbb{E}[\\text{Number of reference molecules post-PCR}] = N_{R, post} = ((1-p)N) \\cdot (e_R)^c $$\n\n$3$. **Post-PCR Molecular Fraction:** The fraction of variant molecules in the post-PCR library, let's call it $p'$, is the ratio of the expected number of variant molecules to the total expected number of molecules.\n$$ p' = \\frac{N_{V, post}}{N_{V, post} + N_{R, post}} = \\frac{(pN)(e_V)^c}{(pN)(e_V)^c + ((1-p)N)(e_R)^c} $$\nThe initial number of molecules, $N$, cancels out:\n$$ p' = \\frac{p(e_V)^c}{p(e_V)^c + (1-p)(e_R)^c} $$\n\n$4$. **Sequencing and Read Fraction:** According to Fundamental Principle 2, uniform random sampling of amplicons means the expected value of the observed read fraction $\\hat{p}$ is equal to the molecular fraction $p'$ in the library being sequenced.\n$$ \\mathbb{E}[\\hat{p}] = p' = \\frac{p(e_V)^c}{p(e_V)^c + (1-p)(e_R)^c} $$\n\n$5$. **Using Relative Efficiency:** Let the per-cycle relative efficiency be $\\rho = e_V/e_R$. We can divide the numerator and denominator of the expression for $\\mathbb{E}[\\hat{p}]$ by $(e_R)^c$:\n$$ \\mathbb{E}[\\hat{p}] = \\frac{p \\frac{(e_V)^c}{(e_R)^c}}{p \\frac{(e_V)^c}{(e_R)^c} + (1-p) \\frac{(e_R)^c}{(e_R)^c}} = \\frac{p \\left(\\frac{e_V}{e_R}\\right)^c}{p \\left(\\frac{e_V}{e_R}\\right)^c + (1-p)} = \\frac{p\\rho^c}{(1-p) + p\\rho^c} $$\nThis completes the required derivation. Now, we evaluate the options.\n\n### Option-by-Option Analysis\n\n**A. If $\\rho = e_{V}/e_{R}$ is the per-cycle relative efficiency, then under large post-PCR molecule counts and unbiased read sampling,  $\\mathbb{E}[\\hat{p}] \\;=\\; \\frac{p\\,\\rho^{c}}{(1-p) + p\\,\\rho^{c}}$.**\n\nThis statement presents the exact formula derived from first principles. The condition of \"large post-PCR molecule counts and unbiased read sampling\" formally justifies equating the expected read fraction with the fraction of molecules in the amplified library, consistent with Fundamental Principle 2 and the law of large numbers.\n\nVerdict: **Correct**.\n\n**B. Under allele-specific efficiency, the expected read fraction is linear in the relative amplification, so $\\mathbb{E}[\\hat{p}] \\;=\\; p\\,\\rho^{c},$ and no normalization by the reference allele is required.**\n\nThis statement claims $\\mathbb{E}[\\hat{p}] = p\\rho^c$. As shown in the derivation, the correct expression is $\\mathbb{E}[\\hat{p}] = \\frac{p\\rho^c}{(1-p) + p\\rho^c}$. The denominator, $(1-p) + p\\rho^c$, is a crucial normalization term that ensures the result is a fraction between $0$ and $1$. The expression $p\\rho^c$ can exceed $1$ (for instance, if $p=0.5$ and $\\rho=1.2$, $c=10$, then $\\rho^c \\approx 6.19$ and $p\\rho^c  3$), which is nonsensical for a fraction. The claim is therefore mathematically incorrect.\n\nVerdict: **Incorrect**.\n\n**C. Increasing the total read depth to infinity eliminates PCR amplification bias, so $\\lim_{D\\to\\infty}\\mathbb{E}[\\hat{p}\\,|\\,\\text{depth}=D]=p$ even if $\\rho \\neq 1$.**\n\nIncreasing the total read depth, $D = R_V + R_R$, reduces the random sampling error associated with sequencing the post-PCR library. As $D \\to \\infty$, the observed read fraction $\\hat{p}$ converges to its expected value, $\\mathbb{E}[\\hat{p}]$.\n$$ \\lim_{D\\to\\infty} \\hat{p} = \\mathbb{E}[\\hat{p}] = \\frac{p\\rho^c}{(1-p) + p\\rho^c} $$\nThe question asks if this limit equals the true fraction $p$. The expression equals $p$ if and only if $\\rho^c=1$ (which implies $\\rho=1$, since $c \\ge 1$), or for the trivial cases of $p=0$ or $p=1$, which are excluded by $p \\in (0,1)$. The problem specifies the case where $\\rho \\neq 1$. Therefore, increasing read depth does not correct the *systematic bias* introduced by PCR; it merely ensures the observed fraction accurately reflects the *biased* composition of the amplified library.\n\nVerdict: **Incorrect**.\n\n**D. Deduplicating with UMIs can correct PCR bias only if UMIs are attached after PCR, because the bias must be observed before it can be removed.**\n\nThis statement misrepresents the correctional mechanism of UMIs. According to Fundamental Principle 3, UMIs are ligated to each *original* molecule *before* amplification. This ensures that every molecule produced from a single original template carries the same unique barcode. When reads are deduplicated by UMI, all reads originating from one initial molecule are collapsed into a single count. This means the final count reflects the number of *original molecules captured*, not the number of PCR products. This process *prevents* the amplification bias from affecting the final quantification. If UMIs were attached *after* PCR, they would tag molecules from the already-biased pool, rendering them useless for bias correction. The reasoning \"bias must be observed before it can be removed\" is fallacious; UMIs work by preventing the measurement of the bias in the first place.\n\nVerdict: **Incorrect**.\n\n**E. If the numbers of captured original variant and reference molecules are independent Poisson random variables with means $\\lambda p$ and $\\lambda(1-p)$, respectively, then deduplication by UMIs yields the estimator $\\tilde{p}=U_{V}/(U_{V}+U_{R})$, which is unbiased for $p$ conditional on $U_{V}+U_{R}0$ when $\\rho \\neq 1$, but has larger variance than $\\hat{p}$ because $U_{V}+U_{R}$ is typically much smaller than $R_{V}+R_{R}$.**\n\nThis statement correctly describes both the benefit and the drawback of using UMIs.\n- **Unbiasedness:** After UMI deduplication, the counts $U_V$ and $U_R$ represent the number of *original* variant and reference molecules that were captured and sequenced. The differential amplification characterized by $\\rho \\neq 1$ has no effect on these counts. Modeling the capture process with independent Poisson variables $U_V \\sim \\text{Pois}(\\lambda p)$ and $U_R \\sim \\text{Pois}(\\lambda(1-p))$ is a standard approach. Given a total number of unique molecules $U_V + U_R = k$, the conditional distribution of $U_V$ is Binomial, $U_V | (U_V+U_R=k) \\sim \\text{Bin}(k, \\frac{\\lambda p}{\\lambda p + \\lambda(1-p)} = p)$. The conditional expectation is $\\mathbb{E}[\\tilde{p} | U_V+U_R=k] = \\mathbb{E}[U_V/k | U_V+U_R=k] = (1/k) \\cdot (kp) = p$. By the law of total expectation, the estimator $\\tilde{p}$ is unbiased for $p$.\n- **Variance:** The estimator $\\hat{p}$ without UMIs is based on the total number of reads, $R_V + R_R$, which is typically very large. The estimator $\\tilde{p}$ is based on the total number of unique molecules, $U_V + U_R$. In most NGS experiments, there is significant amplification, so $R_V+R_R \\gg U_V+U_R$. The variance of a proportion estimator is inversely proportional to the sample size. Therefore, $\\tilde{p}$, while unbiased, has higher sampling variance than the biased estimator $\\hat{p}$ because it is based on a much smaller effective sample size. This trade-off (removing bias at the cost of increasing variance) is a fundamental concept in this field.\n\nVerdict: **Correct**.\n\n**F. If $\\rho$ is known, a debiased estimator of $p$ based on the observed read fraction is $\\tilde{p} \\;=\\; \\frac{\\hat{p}}{\\rho^{c}(1-\\hat{p}) + \\hat{p}},$ which is consistent for $p$ as the number of unique molecules grows, assuming equal pre-PCR capture probabilities for the two alleles and no allelic mapping bias.**\n\nThis statement proposes a computational method to correct for PCR bias. We can check the formula by algebraically inverting the relationship between $p$ and $\\mathbb{E}[\\hat{p}]$. Let $P = \\mathbb{E}[\\hat{p}]$. We have:\n$$ P = \\frac{p\\rho^c}{1-p+p\\rho^c} $$\n$$ P(1-p+p\\rho^c) = p\\rho^c $$\n$$ P - Pp + Pp\\rho^c = p\\rho^c $$\n$$ P = p\\rho^c - Pp\\rho^c + Pp = p(\\rho^c(1-P) + P) $$\n$$ p = \\frac{P}{\\rho^c(1-P) + P} $$\nIf we have a consistent estimator for $P$ (the biased proportion), such as the observed read fraction $\\hat{p}$ from a very large number of reads, we can plug it into this formula to estimate $p$. The formula provided for $\\tilde{p}$ is a direct application of this algebraic inversion, with $\\hat{p}$ substituted for $P$. An estimator is consistent if it converges in probability to the true parameter value as the sample size increases. As the amount of data (molecules and reads) grows, $\\hat{p}$ converges to $P = \\mathbb{E}[\\hat{p}]$. Since the formula is a continuous function of $P$ (for $P \\in [0,1], \\rho^c  0$), the estimator $\\tilde{p}$ will converge to the true value $p$. Thus, it is a consistent estimator.\n\nVerdict: **Correct**.",
            "answer": "$$\\boxed{AEF}$$"
        },
        {
            "introduction": "The presence of nuclear mitochondrial DNA segments (NUMTs) creates significant ambiguity in sequence analysis, as reads originating from NUMTs can be misidentified as true mitochondrial variants. This hands-on programming task guides you through building a sophisticated bioinformatic filter from the ground up, using Bayesian principles to probabilistically assign sequencing reads to their most likely origin . Implementing this tool will solidify your understanding of how edit distance, error models, and prior knowledge are integrated to solve critical read-mapping challenges in genomic diagnostics.",
            "id": "4360543",
            "problem": "You are to design and implement a complete, runnable program that models an in silico filter to assign short sequencing reads probabilistically to their most likely genomic origin: mitochondrial deoxyribonucleic acid (mtDNA) or the nuclear genome. The assignment must rely only on edit distance and a principled probabilistic model grounded in core definitions and widely accepted error assumptions in genomic diagnostics.\n\nFundamental base and modeling assumptions:\n- Levenshtein edit distance is the minimal number of single-character insertions, deletions, or substitutions required to transform one string into another. For two strings $A$ and $B$ with lengths $m$ and $n$, respectively, define the dynamic programming matrix $D \\in \\mathbb{N}^{(m+1)\\times(n+1)}$ by the recurrence\n$$\nD_{i,0} = i,\\quad D_{0,j} = j,\\quad D_{i,j} = \\min\\left\\{\n\\begin{aligned}\n D_{i-1,j} + 1 \\quad \\text{(deletion)}\\\\\n D_{i,j-1} + 1 \\quad \\text{(insertion)}\\\\\n D_{i-1,j-1} + \\mathbf{1}_{A_i \\neq B_j} \\quad \\text{(substitution)}\n\\end{aligned}\n\\right\\},\n$$\nfor $1 \\le i \\le m$ and $1 \\le j \\le n$, where $\\mathbf{1}_{A_i \\neq B_j}$ equals $1$ if the characters differ and $0$ otherwise, and the edit distance equals $D_{m,n}$.\n- Sequencing errors and biological divergence are modeled as independent per-base Bernoulli processes. Let the per-base sequencing error rate be $\\epsilon \\in [0,1)$ and let biological divergence from the mtDNA reference be $\\eta \\in [0,1)$, while from the nuclear reference be $\\xi \\in [0,1)$. Under independence, the probability that an observed base deviates from its corresponding reference under the mtDNA-origin model is\n$$\nq_{\\mathrm{mt}} = 1 - (1 - \\epsilon)(1 - \\eta) = \\epsilon + \\eta - \\epsilon \\eta,\n$$\nand analogously for the nuclear-origin model\n$$\nq_{\\mathrm{nuc}} = 1 - (1 - \\epsilon)(1 - \\xi) = \\epsilon + \\xi - \\epsilon \\xi.\n$$\n- For a read of length $L$ and a measured edit distance $d$ to a candidate reference substring, the likelihood of observing $d$ deviations under a per-base deviation probability $q$ is modeled by the Binomial distribution:\n$$\n\\mathcal{L}(d \\mid L, q) = \\binom{L}{d} q^d (1-q)^{L-d}.\n$$\n- With prior probabilities $\\pi_{\\mathrm{mt}}$ and $\\pi_{\\mathrm{nuc}}$ for mtDNA and nuclear origins, respectively, and $\\pi_{\\mathrm{mt}} + \\pi_{\\mathrm{nuc}} = 1$, compute the posterior probability of mtDNA origin by Bayesâ€™ theorem:\n$$\n\\Pr(\\mathrm{mtDNA} \\mid \\text{data}) = \\frac{\\pi_{\\mathrm{mt}} \\, \\mathcal{L}(d_{\\mathrm{mt}} \\mid L, q_{\\mathrm{mt}})}{\\pi_{\\mathrm{mt}} \\, \\mathcal{L}(d_{\\mathrm{mt}} \\mid L, q_{\\mathrm{mt}}) + \\pi_{\\mathrm{nuc}} \\, \\mathcal{L}(d_{\\mathrm{nuc}} \\mid L, q_{\\mathrm{nuc}})}.\n$$\nYou must derive this posterior from first principles and implement it robustly using log-space to avoid numerical underflow. The edit distance $d_{\\mathrm{mt}}$ must be the minimal Levenshtein distance between the read and any substring of the mtDNA reference within a window-size band around $L$ to accommodate insertion/deletion variability. Similarly, compute $d_{\\mathrm{nuc}}$ as the minimal distance to any substring of the nuclear reference within the same band.\n\nAssignment rule:\n- The program must output for each read the posterior probability for mtDNA origin as a decimal (not a percentage), using the read length $L$, the minimal edit distances $d_{\\mathrm{mt}}$ and $d_{\\mathrm{nuc}}$, and the specified $\\epsilon$, $\\eta$, $\\xi$, and priors.\n\nWindow-size banding:\n- For each read of length $L$, evaluate substrings of each reference whose lengths lie in $[L-\\Delta, L+\\Delta] \\cap [1, \\text{length of reference}]$, with $\\Delta$ a small nonnegative integer specified per test case. The edit distance to a substring is the Levenshtein distance. Use the minimal distance across all such substrings as $d_{\\mathrm{mt}}$ and $d_{\\mathrm{nuc}}$.\n\nTest suite:\nUse the following references and reads. All probabilities must be expressed as decimals. There are no physical units or angles in this problem. The references are fixed across all test cases:\n\n- mtDNA reference (human revised Cambridge Reference Sequence fragment, synthetic truncation):\n\"GATCACAGGTCTATCACCCTATTAACCACTCACGGGAGCTCTCCATGCATTTGGTATTTCTCACCT\"\n\n- Nuclear reference (synthetic with an embedded segment homologous to mtDNA to mimic nuclear mitochondrial DNA sequences (NUMTs)):\n\"CCTGAACACCGTTGACCTAGCGGATCACAGGTCTATCACACTTTATCAAGGGTAGGCTCAGAGAT\"\n\nDefine five test cases that probe varied scenarios and boundary conditions:\n\n1. Happy path mtDNA read (exact match):\n   - Read: the mtDNA substring from indices $i=5$ to $i=35$ (zero-based, end exclusive).\n   - $\\epsilon = 0.01$, $\\eta = 0.001$, $\\xi = 0.010$, $\\pi_{\\mathrm{mt}} = 0.50$, $\\pi_{\\mathrm{nuc}} = 0.50$, $\\Delta = 2$.\n\n2. mtDNA read with sequencing errors and heteroplasmy:\n   - Read: start from the Case $1$ read; substitute the base at position $i=4$ with a different canonical nucleotide, substitute the base at position $i=12$ with a different canonical nucleotide, and insert the character 'A' immediately after position $i=9$.\n   - $\\epsilon = 0.01$, $\\eta = 0.020$, $\\xi = 0.010$, $\\pi_{\\mathrm{mt}} = 0.50$, $\\pi_{\\mathrm{nuc}} = 0.50$, $\\Delta = 2$.\n\n3. Nuclear read (exact match):\n   - Read: the nuclear substring from indices $i=10$ to $i=40$ (zero-based, end exclusive).\n   - $\\epsilon = 0.01$, $\\eta = 0.001$, $\\xi = 0.001$, $\\pi_{\\mathrm{mt}} = 0.40$, $\\pi_{\\mathrm{nuc}} = 0.60$, $\\Delta = 2$.\n\n4. Ambiguous read with equal distances to both references (one mismatch each):\n   - Read: take the shared segment \"GATCACAGGTCTATCAC\" present in both references and change its middle character to produce one mismatch relative to both references.\n   - $\\epsilon = 0.02$, $\\eta = 0.010$, $\\xi = 0.010$, $\\pi_{\\mathrm{mt}} = 0.70$, $\\pi_{\\mathrm{nuc}} = 0.30$, $\\Delta = 2$.\n\n5. NUMT-like perfect ambiguity:\n   - Read: the exact shared segment \"GATCACAGGTCTATCAC\" present in both references.\n   - $\\epsilon = 0.01$, $\\eta = 0.001$, $\\xi = 0.001$, $\\pi_{\\mathrm{mt}} = 0.50$, $\\pi_{\\mathrm{nuc}} = 0.50$, $\\Delta = 2$.\n\nAlgorithmic requirements:\n- Implement Levenshtein edit distance exactly via dynamic programming.\n- Compute minimal edit distances to substrings by exhaustive sliding over the specified window-size band per test case.\n- Compute posterior probabilities in stable log-space using the Binomial likelihood with $q_{\\mathrm{mt}}$ and $q_{\\mathrm{nuc}}$ derived from $\\epsilon$, $\\eta$, and $\\xi$.\n- To avoid degeneracy, clamp $q_{\\mathrm{mt}}$ and $q_{\\mathrm{nuc}}$ to the open interval $(10^{-12}, 1 - 10^{-12})$.\n\nFinal output format:\n- Your program should produce a single line of output containing the posterior probabilities of mtDNA origin for the five test cases as a comma-separated list enclosed in square brackets (e.g., \"[0.998765,0.756432,0.012345,0.612300,0.500000]\"). Each probability must be rounded to six decimal places.\n\nYour solution must be derived from the fundamental definitions and assumptions given above, without invoking any shortcut formulas not justified by those definitions, and must be scientifically realistic and self-consistent within the domain of precision medicine and genomic diagnostics focused on mitochondrial genome analysis and heteroplasmy.",
            "solution": "The problem requires the design of a probabilistic filter to classify short DNA sequencing reads as originating from either the mitochondrial (mtDNA) or nuclear (nuc) genome. This is a common task in genomic diagnostics, particularly for studying mitochondrial diseases, heteroplasmy, and nuclear mitochondrial DNA sequences (NUMTs). The solution is founded on Bayesian inference, using sequence similarity (edit distance) to inform the likelihood of each origin.\n\nThe core of the method is to compute the posterior probability of a read originating from mtDNA, given the read sequence itself and its minimal edit distances to the two reference genomes. As per Bayes' theorem, this posterior probability is given by:\n$$\n\\Pr(\\mathrm{mtDNA} \\mid \\text{data}) = \\frac{\\Pr(\\text{data} \\mid \\mathrm{mtDNA}) \\Pr(\\mathrm{mtDNA})}{\\Pr(\\text{data})}\n$$\nThe term $\\Pr(\\text{data})$ is the marginal likelihood or evidence, which normalizes the posterior. It is calculated by summing over all possible origins using the law of total probability:\n$$\n\\Pr(\\text{data}) = \\Pr(\\text{data} \\mid \\mathrm{mtDNA}) \\Pr(\\mathrm{mtDNA}) + \\Pr(\\text{data} \\mid \\mathrm{nuc}) \\Pr(\\mathrm{nuc})\n$$\nThe problem provides the notation for these terms. The prior probabilities of origin are $\\pi_{\\mathrm{mt}} = \\Pr(\\mathrm{mtDNA})$ and $\\pi_{\\mathrm{nuc}} = \\Pr(\\mathrm{nuc})$. The likelihood of the data given an origin, $\\Pr(\\text{data} \\mid \\text{origin})$, is denoted $\\mathcal{L}$. This leads to the specified formula:\n$$\n\\Pr(\\mathrm{mtDNA} \\mid \\text{data}) = \\frac{\\pi_{\\mathrm{mt}} \\, \\mathcal{L}(d_{\\mathrm{mt}} \\mid L, q_{\\mathrm{mt}})}{\\pi_{\\mathrm{mt}} \\, \\mathcal{L}(d_{\\mathrm{mt}} \\mid L, q_{\\mathrm{mt}}) + \\pi_{\\mathrm{nuc}} \\, \\mathcal{L}(d_{\\mathrm{nuc}} \\mid L, q_{\\mathrm{nuc}})}\n$$\nHere, $L$ is the length of the read, and $d_{\\mathrm{mt}}$ and $d_{\\mathrm{nuc}}$ are its minimal edit distances to the mtDNA and nuclear references, respectively. The likelihood $\\mathcal{L}(d \\mid L, q)$ is modeled by a Binomial distribution, which describes the probability of observing $d$ deviations (errors) in a sequence of $L$ independent trials, each with a deviation probability of $q$:\n$$\n\\mathcal{L}(d \\mid L, q) = \\binom{L}{d} q^d (1-q)^{L-d}\n$$\nThe per-base deviation probabilities, $q_{\\mathrm{mt}}$ and $q_{\\mathrm{nuc}}$, are derived from the independent probabilities of sequencing error ($\\epsilon$) and biological divergence ($\\eta$ for mtDNA, $\\xi$ for nuclear):\n$$\nq_{\\mathrm{mt}} = 1 - (1 - \\epsilon)(1 - \\eta) = \\epsilon + \\eta - \\epsilon \\eta\n$$\n$$\nq_{\\mathrm{nuc}} = 1 - (1 - \\epsilon)(1 - \\xi) = \\epsilon + \\xi - \\epsilon \\xi\n$$\nDirect computation of these probabilities is prone to numerical underflow, as binomial probabilities can become extremely small for typical genomic parameters. To ensure numerical stability, all calculations are performed in log-space. The log-likelihood is:\n$$\n\\log \\mathcal{L}(d \\mid L, q) = \\log\\binom{L}{d} + d \\log q + (L-d) \\log(1-q)\n$$\nThe logarithm of the binomial coefficient, $\\log\\binom{L}{d}$, is stably computed using the log-gamma function, $\\Gamma(z)$, since $\\log(n!) = \\log \\Gamma(n+1)$, which is implemented as `gammaln` in scientific computing libraries:\n$$\n\\log\\binom{L}{d} = \\log(L!) - \\log(d!) - \\log((L-d)!) = \\texttt{gammaln}(L+1) - \\texttt{gammaln}(d+1) - \\texttt{gammaln}(L-d+1)\n$$\nThis is valid for $0 \\le d \\le L$. If the measured edit distance $d$ is greater than the read length $L$, the likelihood is $0$, and its logarithm is $-\\infty$.\n\nLet $\\alpha_{\\mathrm{mt}} = \\log(\\pi_{\\mathrm{mt}}) + \\log \\mathcal{L}_{\\mathrm{mt}}$ and $\\alpha_{\\mathrm{nuc}} = \\log(\\pi_{\\mathrm{nuc}}) + \\log \\mathcal{L}_{\\mathrm{nuc}}$ be the log-joint probabilities. The log of the posterior probability is:\n$$\n\\log \\Pr(\\mathrm{mtDNA} \\mid \\text{data}) = \\alpha_{\\mathrm{mt}} - \\log(\\exp(\\alpha_{\\mathrm{mt}}) + \\exp(\\alpha_{\\mathrm{nuc}}))\n$$\nThe term $\\log(\\exp(\\alpha_{\\mathrm{mt}}) + \\exp(\\alpha_{\\mathrm{nuc}}))$ is a log-sum-exp operation, which is computed stably by factoring out the maximum value:\n$$\n\\log(e^x + e^y) = \\max(x, y) + \\log(1 + \\exp(-|x-y|))\n$$\nThis prevents floating-point overflow. The final posterior probability is obtained by exponentiating the result: $\\exp(\\log \\Pr(\\mathrm{mtDNA} \\mid \\text{data}))$.\n\nThe algorithmic steps are as follows:\n1.  **For each test case:**\n    a.  Define the read sequence, parameters ($\\epsilon, \\eta, \\xi, \\pi_{\\mathrm{mt}}$), and the window-size parameter $\\Delta$.\n2.  **Compute Minimal Edit Distances:**\n    a.  The edit distance is the Levenshtein distance, calculated using the standard dynamic programming algorithm defined in the problem.\n    b.  For the given read of length $L$, identify the range of substring lengths to search in the reference: $[L-\\Delta, L+\\Delta]$.\n    c.  For each reference (mtDNA and nuclear), iterate through all substrings whose lengths fall within this range. Calculate the Levenshtein distance between the read and each substring.\n    d.  The minimal distance found for each reference is taken as $d_{\\mathrm{mt}}$ and $d_{\\mathrm{nuc}}$.\n3.  **Compute Posterior Probability:**\n    a.  Calculate $q_{\\mathrm{mt}}$ and $q_{\\mathrm{nuc}}$ and clamp them to a small, positive interval $(10^{-12}, 1-10^{-12})$ to avoid taking logarithms of zero.\n    b.  Calculate the log-likelihoods for the mtDNA and nuclear hypotheses using the computed $d_{\\mathrm{mt}}$, $d_{\\mathrm{nuc}}$, $L$, $q_{\\mathrm{mt}}$, and $q_{\\mathrm{nuc}}$.\n    c.  Combine log-likelihoods with log-priors to get the log-joint probabilities $\\alpha_{\\mathrm{mt}}$ and $\\alpha_{\\mathrm{nuc}}$.\n    d.  Use the stable log-sum-exp method to compute the log-evidence, and from that, the log-posterior probability.\n    e.  Exponentiate the log-posterior to get the final probability value.\n4.  **Format Output:**\n    a.  Collect the posterior probabilities for all test cases.\n    b.  Format them into a single comma-separated string, enclosed in brackets, with each value rounded to six decimal places.\n\nThis structured, principle-based approach ensures a correct, robust, and reproducible solution that adheres strictly to the problem's scientific and algorithmic specifications.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef solve():\n    \"\"\"\n    Main function to run the complete analysis for all test cases.\n    \"\"\"\n\n    # --- Part 1: Levenshtein Distance and Minimal Distance Search ---\n\n    def levenshtein_distance(s1, s2):\n        \"\"\"Computes the Levenshtein edit distance between two strings using dynamic programming.\"\"\"\n        m, n = len(s1), len(s2)\n        # Using a list of lists for the DP table as numpy might not be available in all envs,\n        # but problem statement allows it. Using numpy for speed and conciseness.\n        dp = np.zeros((m + 1, n + 1), dtype=int)\n\n        for i in range(m + 1):\n            dp[i, 0] = i\n        for j in range(n + 1):\n            dp[0, j] = j\n\n        for i in range(1, m + 1):\n            for j in range(1, n + 1):\n                cost = 0 if s1[i - 1] == s2[j - 1] else 1\n                dp[i, j] = min(dp[i - 1, j] + 1,        # Deletion\n                               dp[i, j - 1] + 1,        # Insertion\n                               dp[i - 1, j - 1] + cost) # Substitution\n        return dp[m, n]\n\n    def find_min_distance(read, reference, delta):\n        \"\"\"\n        Finds the minimum Levenshtein distance between a read and all valid substrings\n        of a reference, according to the window-size banding rule.\n        \"\"\"\n        read_len = len(read)\n        ref_len = len(reference)\n        min_dist = float('inf')\n\n        min_sub_len = max(1, read_len - delta)\n        max_sub_len = min(ref_len, read_len + delta)\n\n        for sub_len in range(min_sub_len, max_sub_len + 1):\n            for start_pos in range(ref_len - sub_len + 1):\n                substring = reference[start_pos : start_pos + sub_len]\n                dist = levenshtein_distance(read, substring)\n                if dist  min_dist:\n                    min_dist = dist\n        \n        return min_dist\n\n    # --- Part 2: Posterior Probability Calculation ---\n\n    def calculate_posterior(L, d_mt, d_nuc, epsilon, eta, xi, pi_mt):\n        \"\"\"\n        Calculates the posterior probability of mtDNA origin in log-space for numerical stability.\n        \"\"\"\n        pi_nuc = 1.0 - pi_mt\n        clamp_val = 1e-12\n\n        # 1. Calculate per-base deviation probabilities\n        q_mt = epsilon + eta - epsilon * eta\n        q_nuc = epsilon + xi - epsilon * xi\n\n        # Clamp probabilities to avoid log(0)\n        q_mt = np.clip(q_mt, clamp_val, 1 - clamp_val)\n        q_nuc = np.clip(q_nuc, clamp_val, 1 - clamp_val)\n        \n        # 2. Calculate log-likelihoods\n        def get_log_likelihood(d, L_read, q):\n            if d  0 or d > L_read:\n                return -np.inf\n            # log(binom(L, d)) = gammaln(L+1) - gammaln(d+1) - gammaln(L-d+1)\n            log_binom_coeff = gammaln(L_read + 1) - gammaln(d + 1) - gammaln(L_read - d + 1)\n            log_lik = log_binom_coeff + d * np.log(q) + (L_read - d) * np.log(1 - q)\n            return log_lik\n\n        log_lik_mt = get_log_likelihood(d_mt, L, q_mt)\n        log_lik_nuc = get_log_likelihood(d_nuc, L, q_nuc)\n        \n        # Handle cases where data is impossible under one or both models.\n        if np.isneginf(log_lik_mt) and np.isneginf(log_lik_nuc):\n            return pi_mt\n        if np.isneginf(log_lik_mt):\n            return 0.0\n        if np.isneginf(log_lik_nuc):\n            return 1.0\n\n        # 3. Calculate log-joint probabilities\n        log_joint_mt = np.log(pi_mt) + log_lik_mt\n        log_joint_nuc = np.log(pi_nuc) + log_lik_nuc\n        \n        # 4. Calculate posterior using stable log-sum-exp\n        # log P_mt = log_joint_mt - log(exp(log_joint_mt) + exp(log_joint_nuc))\n        alpha_max = max(log_joint_mt, log_joint_nuc)\n        log_evidence = alpha_max + np.log(np.exp(log_joint_mt - alpha_max) + np.exp(log_joint_nuc - alpha_max))\n        \n        log_posterior_mt = log_joint_mt - log_evidence\n        \n        return np.exp(log_posterior_mt)\n\n    # --- Part 3: Test Suite Definition and Execution ---\n\n    # Fixed reference sequences\n    mt_ref = \"GATCACAGGTCTATCACCCTATTAACCACTCACGGGAGCTCTCCATGCATTTGGTATTTCTCACCT\"\n    nuc_ref = \"CCTGAACACCGTTGACCTAGCGGATCACAGGTCTATCACACTTTATCAAGGGTAGGCTCAGAGAT\"\n\n    # Define reads for each test case\n    # Case 1\n    read_1 = mt_ref[5:35]\n    # Case 2\n    read_1_orig_list = list(read_1)\n    read_1_orig_list[4] = 'G'   # Substitute T->G at pos 4\n    read_1_orig_list[12] = 'A'  # Substitute C->A at pos 12\n    read_1_orig_list.insert(10, 'A') # Insert 'A' after original pos 9\n    read_2 = \"\".join(read_1_orig_list)\n    # Case 3\n    read_3 = nuc_ref[10:40]\n    # Case 4\n    shared_seg = \"GATCACAGGTCTATCAC\"\n    read_4_list = list(shared_seg)\n    read_4_list[len(shared_seg) // 2] = 'A' # Change middle char G->A\n    read_4 = \"\".join(read_4_list)\n    # Case 5\n    read_5 = shared_seg\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'read': read_1, 'epsilon': 0.01, 'eta': 0.001, 'xi': 0.010, 'pi_mt': 0.50, 'delta': 2},\n        {'read': read_2, 'epsilon': 0.01, 'eta': 0.020, 'xi': 0.010, 'pi_mt': 0.50, 'delta': 2},\n        {'read': read_3, 'epsilon': 0.01, 'eta': 0.001, 'xi': 0.001, 'pi_mt': 0.40, 'delta': 2},\n        {'read': read_4, 'epsilon': 0.02, 'eta': 0.010, 'xi': 0.010, 'pi_mt': 0.70, 'delta': 2},\n        {'read': read_5, 'epsilon': 0.01, 'eta': 0.001, 'xi': 0.001, 'pi_mt': 0.50, 'delta': 2},\n    ]\n\n    results = []\n    for case in test_cases:\n        read = case['read']\n        L = len(read)\n        delta = case['delta']\n        \n        # Main logic to calculate the result for one case\n        d_mt = find_min_distance(read, mt_ref, delta)\n        d_nuc = find_min_distance(read, nuc_ref, delta)\n        \n        posterior_mt = calculate_posterior(\n            L, d_mt, d_nuc, \n            case['epsilon'], case['eta'], case['xi'], case['pi_mt']\n        )\n        results.append(posterior_mt)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{p:.6f}' for p in results)}]\")\n\nsolve()\n```"
        }
    ]
}