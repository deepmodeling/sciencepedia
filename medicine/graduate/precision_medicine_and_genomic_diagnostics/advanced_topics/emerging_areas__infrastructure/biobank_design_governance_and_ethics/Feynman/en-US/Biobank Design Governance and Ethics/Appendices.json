{
    "hands_on_practices": [
        {
            "introduction": "The long-term preservation of biological samples is a cornerstone of biobank stewardship, ensuring that these precious resources remain viable for future research. This exercise applies probability theory to model sample degradation from freeze-thaw cycles, a common operational challenge in all biobanks. By quantifying the risk of sample loss and exploring mitigation strategies like optimal aliquoting, you will develop a quantitative approach to specimen management that directly supports the ethical obligation to maximize the value of participant contributions .",
            "id": "4318618",
            "problem": "A biobank supporting precision medicine and genomic diagnostics stores each participant’s biological sample as multiple aliquots to reduce repeated freeze-thaw exposure. Assume that freeze-thaw–induced failure events are independent across cycles within an aliquot and that per-cycle failure probability is $p=0.001$. Over a $10$-year period, historical utilization indicates that each aliquot experiences, on average, $5$ freeze-thaw cycles. An aliquot is deemed lost if any of its freeze-thaw cycles causes failure, and each aliquot has equal volume. Starting from the foundational principles of independent Bernoulli trials and the definition of survival across repeated independent risks, derive the expected fraction of aliquots lost per sample over the $10$-year period. Compute the numerical value using the provided parameters and report the expected fraction as a decimal rounded to four significant figures.\n\nSeparately, consider mitigation by pre-allocating aliquots to minimize freeze-thaw exposure. Let a sample have fixed total volume $V$ split evenly across $n$ aliquots, and let total planned retrieval events over $10$ years be $T$, distributed evenly so that each aliquot experiences $T/n$ cycles. Using the same independence assumptions, derive the expected lost volume $L(n)$ as a function of $n$, $V$, $T$, and $p$. Identify the value of $n$ that minimizes $L(n)$ under fixed $V$ and $T$ (ignoring storage and handling costs), and briefly justify how this choice aligns with ethical biobank governance obligations such as minimizing waste and protecting participant-contributed specimens. The final answer you provide must be only the expected fraction of aliquots lost for the baseline parameters, expressed as instructed above.",
            "solution": "The problem statement is evaluated for validity before proceeding to a solution.\n\n### Step 1: Extract Givens\n- Freeze-thaw–induced failure events are independent across cycles.\n- Per-cycle failure probability: $p=0.001$.\n- Average number of freeze-thaw cycles per aliquot over $10$ years: $k=5$. (The term \"average\" is interpreted as the fixed number of cycles for a representative aliquot, a standard convention in such problems).\n- An aliquot is lost if any of its freeze-thaw cycles causes failure.\n- A sample has a total volume $V$ split evenly across $n$ aliquots.\n- Total planned retrieval events over $10$ years is $T$.\n- Retrievals are distributed evenly, so each aliquot experiences $T/n$ cycles.\n- The final answer should be the numerical value for the expected fraction of aliquots lost, rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed against the validation criteria.\n\n- **Scientifically Grounded:** The problem is based on established principles of reliability and probability theory (specifically, Bernoulli trials). The context of sample degradation due to freeze-thaw cycles in biobanking is a real and significant issue. The assumptions of independence and constant failure probability are standard modeling choices for such phenomena. The problem is scientifically realistic.\n- **Well-Posed:** The problem is clearly defined. The first part asks for an expected fraction, which is equivalent to a probability, given a specific number of trials ($k=5$) and a per-trial probability ($p=0.001$). The second part asks for the derivation of a function representing expected loss, $L(n)$, and its optimization. The parameters for this derivation ($V$, $T$, $p$, $n$) are defined, leading to a unique functional form and a clear optimization goal. The structure allows for a unique, stable, and meaningful solution.\n- **Objective:** The problem is stated in precise, quantitative, and unbiased language, free from subjective claims.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is a well-posed, scientifically grounded problem that can be formally solved using principles of probability and calculus. The solution process may proceed.\n\n### Part 1: Expected Fraction of Aliquots Lost\n\nThe process of a single freeze-thaw cycle for an aliquot can be modeled as a Bernoulli trial. Let $S_i$ be the event that the aliquot survives the $i$-th cycle, and $F_i$ be the event that it fails.\n\nThe probability of failure in a single cycle is given as $p = 0.001$.\nThe probability of survival in a single cycle is therefore $P(S_i) = 1 - p = 1 - 0.001 = 0.999$.\n\nAn aliquot is considered \"lost\" if it fails in any of its freeze-thaw cycles. It is easier to first calculate the probability of the complementary event: the aliquot survives all of its cycles. Let $k=5$ be the number of cycles an aliquot experiences.\n\nThe problem states that failure events are independent. Therefore, the probability that an aliquot survives all $k$ cycles is the product of the probabilities of surviving each individual cycle:\n$$P(\\text{Survival}) = P(S_1 \\cap S_2 \\cap \\dots \\cap S_k)$$\nDue to independence, this becomes:\n$$P(\\text{Survival}) = P(S_1) \\times P(S_2) \\times \\dots \\times P(S_k) = (1-p)^k$$\n\nThe probability that an aliquot is lost, $P(\\text{Loss})$, is the complement of survival:\n$$P(\\text{Loss}) = 1 - P(\\text{Survival}) = 1 - (1-p)^k$$\n\nBy the law of large numbers, the expected fraction of aliquots lost from a large population is equal to this probability.\nSubstituting the given values $p=0.001$ and $k=5$:\n$$P(\\text{Loss}) = 1 - (1 - 0.001)^5 = 1 - (0.999)^5$$\n\nWe compute the numerical value:\n$$(0.999)^5 \\approx 0.99500999$$\nTherefore,\n$$P(\\text{Loss}) \\approx 1 - 0.99500999 = 0.00499001$$\n\nThe problem requires rounding to four significant figures. The first four significant figures are $4$, $9$, $9$, and $0$. The fifth significant figure is $0$, so we round down.\n$P(\\text{Loss}) \\approx 0.004990$\n\nThis is the expected fraction of aliquots lost.\n\n### Part 2: Mitigation Strategy Analysis\n\nThis part requires deriving the expected lost volume $L(n)$ and determining the optimal number of aliquots, $n$.\n\n- Total sample volume: $V$\n- Number of aliquots: $n$\n- Volume per aliquot: $V_a = \\frac{V}{n}$\n- Total retrievals: $T$\n- Cycles per aliquot: $k_n = \\frac{T}{n}$\n- Per-cycle failure probability: $p$\n\nThe probability of a single aliquot being lost, $P_{\\text{loss}}(n)$, now depends on $n$ through the number of cycles $k_n$:\n$$P_{\\text{loss}}(n) = 1 - (1-p)^{k_n} = 1 - (1-p)^{T/n}$$\n\nThe expected lost volume from a single aliquot is its volume multiplied by its probability of being lost:\n$$E[\\text{Lost volume per aliquot}] = V_a \\times P_{\\text{loss}}(n) = \\frac{V}{n} \\left(1 - (1-p)^{T/n}\\right)$$\n\nThe total expected lost volume, $L(n)$, for the entire sample (all $n$ aliquots) is the sum of the expected losses from each aliquot. Since they are identical, we multiply by $n$:\n$$L(n) = n \\times \\frac{V}{n} \\left(1 - (1-p)^{T/n}\\right)$$\n$$L(n) = V \\left(1 - (1-p)^{T/n}\\right)$$\n\nTo minimize the expected lost volume $L(n)$ with respect to $n$ (for fixed $V$, $T$, and $p$), we need to analyze the function $L(n)$. Minimizing $L(n)$ is equivalent to minimizing $V \\left(1 - (1-p)^{T/n}\\right)$. Since $V$ is a positive constant, this is equivalent to minimizing $1 - (1-p)^{T/n}$. This, in turn, is equivalent to maximizing the term $(1-p)^{T/n}$.\n\nLet the function to be maximized be $f(n) = (1-p)^{T/n}$.\nThe base of the exponential, $b = 1-p$, is a constant such that $0  b  1$.\nThe exponent is $x(n) = T/n$.\nFor a fixed base $b \\in (0,1)$, the function $b^x$ is a strictly decreasing function of its exponent $x$. Therefore, to maximize $f(n)$, we must minimize its exponent $x(n) = T/n$.\n\nTo minimize the positive quantity $T/n$, we must make the denominator $n$ as large as possible. Thus, $L(n)$ is a monotonically decreasing function of $n$. The theoretical minimum is approached as $n \\to \\infty$:\n$$\\lim_{n\\to\\infty} L(n) = \\lim_{n\\to\\infty} V \\left(1 - (1-p)^{T/n}\\right) = V \\left(1 - (1-p)^{0}\\right) = V(1-1) = 0$$\n\nIn a practical biobanking context, this means that the risk of loss is minimized by using the largest possible number of aliquots. The most logical upper bound for $n$ is the total number of retrieval events, $T$. If we set $n=T$, then each aliquot is designated for a single retrieval, experiencing exactly one freeze-thaw cycle (since $k_n = T/T = 1$). This strategy, often called \"single-use aliquoting,\" minimizes freeze-thaw-induced damage across the entire sample. Any $n > T$ would imply some aliquots are never used, which does not further reduce risk for the used aliquots. Therefore, the optimal practical choice is $n=T$.\n\nThis choice aligns with ethical obligations of biobank governance. The principle of stewardship requires that donated biological specimens, which are a finite and precious resource, be managed to maximize their scientific value and minimize waste. By choosing $n=T$, the biobank minimizes the expected volume of lost sample material ($L(T) = Vp$, which is the minimum achievable non-zero loss). This demonstrates respect for the participant's contribution and ensures the long-term integrity and availability of the specimen for future research, fulfilling the ethical mandate to protect and preserve these valuable resources.",
            "answer": "$$\\boxed{0.004990}$$"
        },
        {
            "introduction": "A biobank's scientific value and ethical standing depend on its ability to support research that is generalizable and equitable across diverse populations. This practice delves into the statistical technique of post-stratification to correct for mismatches between cohort demographics and those of the target population, a frequent challenge arising from real-world recruitment patterns. You will learn to quantify and correct for sampling bias, ensuring that estimators of population-wide biomarkers are not skewed, a critical skill for responsible and fair genomic diagnostics research .",
            "id": "4318637",
            "problem": "A national precision medicine biobank supporting genomic diagnostics is designed to enable unbiased population-level inference about a continuous risk biomarker $Y$ used in algorithmic risk stratification. The target population consists of two strata: a minority group $M$ with proportion $p_{M}^{\\text{pop}} = 0.10$ and a majority group $A$ with proportion $p_{A}^{\\text{pop}} = 0.90$. Due to governance decisions that encouraged enrollment from historically underserved communities, the realized cohort composition differs from the population: $p_{M}^{\\text{coh}} = 0.20$ and $p_{A}^{\\text{coh}} = 0.80$, with total cohort size $N = 10000$, so $n_{M} = 2000$ and $n_{A} = 8000$. All sampling within each stratum is simple random sampling without replacement, and outcomes are independent across participants.\n\nYou will perform post-stratification to correct the cohort-based estimate of the population mean of $Y$ for the mismatch in stratum proportions. Use the following empirically plausible stratum-specific characteristics derived from prior studies in genomic diagnostics:\n- Minority group mean $\\mu_{M} = 1.6$ and variance $\\sigma_{M}^{2} = 2.25$.\n- Majority group mean $\\mu_{A} = 1.0$ and variance $\\sigma_{A}^{2} = 1.00$.\n\nStarting only from foundational definitions in probability sampling and properties of sample means under independent simple random sampling within strata, do the following:\n1. Define post-stratification sampling weights $w_{M}$ and $w_{A}$ that yield an estimator of the population mean of $Y$ that is unbiased under the stated assumptions.\n2. Derive the bias and variance of the unweighted cohort mean estimator of the population mean of $Y$.\n3. Derive the bias and variance of the post-stratified weighted estimator of the population mean of $Y$.\n4. Define the mean squared error $S$ ratio as the unweighted estimator mean squared error divided by the weighted estimator mean squared error, and compute its value.\n\nExpress the final answer as the single number $S$, a unitless factor. Round your answer to four significant figures.",
            "solution": "The problem statement is self-contained, scientifically grounded in statistical theory, and mathematically well-posed. We may proceed with the solution.\n\nThe true population mean of the biomarker $Y$, denoted $\\mu_{\\text{pop}}$, is the weighted average of the stratum-specific means, where the weights are the population proportions of the strata.\n$$ \\mu_{\\text{pop}} = p_{M}^{\\text{pop}} \\mu_{M} + p_{A}^{\\text{pop}} \\mu_{A} $$\nGiven $p_{M}^{\\text{pop}} = 0.10$, $\\mu_{M} = 1.6$, $p_{A}^{\\text{pop}} = 0.90$, and $\\mu_{A} = 1.0$, the true population mean is:\n$$ \\mu_{\\text{pop}} = (0.10)(1.6) + (0.90)(1.0) = 0.16 + 0.90 = 1.06 $$\n\nLet $Y_k$ be the biomarker value for an individual $k$. Let the sample means for the minority ($M$) and majority ($A$) strata be $\\bar{Y}_{M}$ and $\\bar{Y}_{A}$, respectively. Under simple random sampling (SRS), the sample mean is an unbiased estimator for the stratum population mean, and its variance is given by $\\sigma^2/n$ (assuming the finite population correction factor is negligible, a standard assumption for large populations such as a national biobank).\n$$ E[\\bar{Y}_M] = \\mu_M \\quad , \\quad \\text{Var}(\\bar{Y}_M) = \\frac{\\sigma_M^2}{n_M} $$\n$$ E[\\bar{Y}_A] = \\mu_A \\quad , \\quad \\text{Var}(\\bar{Y}_A) = \\frac{\\sigma_A^2}{n_A} $$\n\n**1. Definition of Post-Stratification Weights**\n\nPost-stratification aims to correct for the discrepancy between cohort proportions ($p_{s}^{\\text{coh}}$) and population proportions ($p_{s}^{\\text{pop}}$) for each stratum $s$. An unbiased estimator of the population mean is constructed by weighting the stratum sample means by the true population proportions:\n$$ \\hat{\\mu}_{\\text{w}} = p_{M}^{\\text{pop}} \\bar{Y}_{M} + p_{A}^{\\text{pop}} \\bar{Y}_{A} $$\nThis estimator can be represented as a weighted sum of individual observations, $\\sum_{k_s} w_s Y_{k_s}$, where $w_s$ is the weight for each individual in stratum $s$. To yield an unbiased estimator, these weights are typically defined to make the weighted sample size of each stratum proportional to its population size. The standard post-stratification weight for an individual in stratum $s$ is the ratio of the population proportion to the cohort proportion:\n$$ w_s = \\frac{p_s^{\\text{pop}}}{p_s^{\\text{coh}}} $$\nFor our two strata, the weights are:\n$$ w_{M} = \\frac{p_{M}^{\\text{pop}}}{p_{M}^{\\text{coh}}} = \\frac{0.10}{0.20} = 0.5 $$\n$$ w_{A} = \\frac{p_{A}^{\\text{pop}}}{p_{A}^{\\text{coh}}} = \\frac{0.90}{0.80} = \\frac{9}{8} = 1.125 $$\nThese weights ensure that the weighted estimator is unbiased. The correctly normalized estimator using these weights is $\\hat{\\mu} = \\frac{\\sum_k w_k Y_k}{\\sum_k w_k}$. The denominator is $\\sum_{i=1}^{n_M} w_M + \\sum_{j=1}^{n_A} w_A = n_M w_M + n_A w_A = n_M \\frac{p_M^{\\text{pop}}}{p_M^{\\text{coh}}} + n_A \\frac{p_A^{\\text{pop}}}{p_A^{\\text{coh}}}$. Since $p_s^{\\text{coh}} = n_s/N$, this is $n_M \\frac{p_M^{\\text{pop}}}{n_M/N} + n_A \\frac{p_A^{\\text{pop}}}{n_A/N} = N p_M^{\\text{pop}} + N p_A^{\\text{pop}} = N(p_M^{\\text{pop}} + p_A^{\\text{pop}}) = N$. The numerator is $\\sum w_k Y_k = w_M (n_M \\bar{Y}_M) + w_A (n_A \\bar{Y}_A) = p_M^{\\text{pop}} N \\bar{Y}_M + p_A^{\\text{pop}} N \\bar{Y}_A$. The estimator is therefore $\\frac{N(p_M^{\\text{pop}}\\bar{Y}_M + p_A^{\\text{pop}}\\bar{Y}_A)}{N} = p_{M}^{\\text{pop}} \\bar{Y}_{M} + p_{A}^{\\text{pop}} \\bar{Y}_{A}$, which is the form $\\hat{\\mu}_{\\text{w}}$.\n\n**2. Bias and Variance of the Unweighted Estimator**\n\nThe unweighted cohort mean estimator, $\\hat{\\mu}_{\\text{unw}}$, is the simple average of all samples. It can be expressed as a weighted average of the stratum sample means, using cohort proportions as weights:\n$$ \\hat{\\mu}_{\\text{unw}} = \\frac{n_M \\bar{Y}_M + n_A \\bar{Y}_A}{N} = p_{M}^{\\text{coh}} \\bar{Y}_{M} + p_{A}^{\\text{coh}} \\bar{Y}_{A} $$\nThe expectation of this estimator is:\n$$ E[\\hat{\\mu}_{\\text{unw}}] = p_{M}^{\\text{coh}} E[\\bar{Y}_{M}] + p_{A}^{\\text{coh}} E[\\bar{Y}_{A}] = p_{M}^{\\text{coh}} \\mu_{M} + p_{A}^{\\text{coh}} \\mu_{A} $$\nUsing the given values $p_{M}^{\\text{coh}}=0.20$ and $p_{A}^{\\text{coh}}=0.80$:\n$$ E[\\hat{\\mu}_{\\text{unw}}] = (0.20)(1.6) + (0.80)(1.0) = 0.32 + 0.80 = 1.12 $$\nThe bias is the difference between the expected value and the true population mean:\n$$ \\text{Bias}(\\hat{\\mu}_{\\text{unw}}) = E[\\hat{\\mu}_{\\text{unw}}] - \\mu_{\\text{pop}} = 1.12 - 1.06 = 0.06 $$\n\nThe variance of the unweighted estimator, assuming independence between strata samples, is:\n$$ \\text{Var}(\\hat{\\mu}_{\\text{unw}}) = \\text{Var}(p_{M}^{\\text{coh}} \\bar{Y}_{M} + p_{A}^{\\text{coh}} \\bar{Y}_{A}) = (p_{M}^{\\text{coh}})^2 \\text{Var}(\\bar{Y}_{M}) + (p_{A}^{\\text{coh}})^2 \\text{Var}(\\bar{Y}_{A}) $$\n$$ \\text{Var}(\\hat{\\mu}_{\\text{unw}}) = (p_{M}^{\\text{coh}})^2 \\frac{\\sigma_{M}^{2}}{n_{M}} + (p_{A}^{\\text{coh}})^2 \\frac{\\sigma_{A}^{2}}{n_{A}} $$\nSubstituting the given values $n_{M} = 2000$, $\\sigma_{M}^{2} = 2.25$, $n_{A} = 8000$, $\\sigma_{A}^{2} = 1.00$:\n$$ \\text{Var}(\\hat{\\mu}_{\\text{unw}}) = (0.20)^2 \\frac{2.25}{2000} + (0.80)^2 \\frac{1.00}{8000} = (0.04) \\frac{2.25}{2000} + (0.64) \\frac{1.00}{8000} $$\n$$ \\text{Var}(\\hat{\\mu}_{\\text{unw}}) = \\frac{0.09}{2000} + \\frac{0.64}{8000} = \\frac{4 \\times 0.09}{8000} + \\frac{0.64}{8000} = \\frac{0.36 + 0.64}{8000} = \\frac{1.00}{8000} = 0.000125 $$\nThe mean squared error (MSE) is the sum of the variance and the squared bias:\n$$ \\text{MSE}(\\hat{\\mu}_{\\text{unw}}) = \\text{Var}(\\hat{\\mu}_{\\text{unw}}) + [\\text{Bias}(\\hat{\\mu}_{\\text{unw}})]^2 = 0.000125 + (0.06)^2 = 0.000125 + 0.0036 = 0.003725 $$\n\n**3. Bias and Variance of the Weighted Estimator**\n\nThe post-stratified weighted estimator is $\\hat{\\mu}_{\\text{w}} = p_{M}^{\\text{pop}} \\bar{Y}_{M} + p_{A}^{\\text{pop}} \\bar{Y}_{A}$.\nIts expectation is:\n$$ E[\\hat{\\mu}_{\\text{w}}] = p_{M}^{\\text{pop}} E[\\bar{Y}_{M}] + p_{A}^{\\text{pop}} E[\\bar{Y}_{A}] = p_{M}^{\\text{pop}} \\mu_{M} + p_{A}^{\\text{pop}} \\mu_{A} = \\mu_{\\text{pop}} $$\nThe bias of the weighted estimator is therefore:\n$$ \\text{Bias}(\\hat{\\mu}_{\\text{w}}) = E[\\hat{\\mu}_{\\text{w}}] - \\mu_{\\text{pop}} = 0 $$\nThe variance of the weighted estimator is:\n$$ \\text{Var}(\\hat{\\mu}_{\\text{w}}) = \\text{Var}(p_{M}^{\\text{pop}} \\bar{Y}_{M} + p_{A}^{\\text{pop}} \\bar{Y}_{A}) = (p_{M}^{\\text{pop}})^2 \\text{Var}(\\bar{Y}_{M}) + (p_{A}^{\\text{pop}})^2 \\text{Var}(\\bar{Y}_{A}) $$\n$$ \\text{Var}(\\hat{\\mu}_{\\text{w}}) = (p_{M}^{\\text{pop}})^2 \\frac{\\sigma_{M}^{2}}{n_{M}} + (p_{A}^{\\text{pop}})^2 \\frac{\\sigma_{A}^{2}}{n_{A}} $$\nSubstituting the given values:\n$$ \\text{Var}(\\hat{\\mu}_{\\text{w}}) = (0.10)^2 \\frac{2.25}{2000} + (0.90)^2 \\frac{1.00}{8000} = (0.01) \\frac{2.25}{2000} + (0.81) \\frac{1.00}{8000} $$\n$$ \\text{Var}(\\hat{\\mu}_{\\text{w}}) = \\frac{0.0225}{2000} + \\frac{0.81}{8000} = \\frac{4 \\times 0.0225}{8000} + \\frac{0.81}{8000} = \\frac{0.09 + 0.81}{8000} = \\frac{0.90}{8000} = 0.0001125 $$\nSince the bias is zero, the MSE of the weighted estimator is equal to its variance:\n$$ \\text{MSE}(\\hat{\\mu}_{\\text{w}}) = \\text{Var}(\\hat{\\mu}_{\\text{w}}) = 0.0001125 $$\n\n**4. Mean Squared Error Ratio**\n\nThe mean squared error ratio $S$ is defined as the unweighted estimator MSE divided by the weighted estimator MSE.\n$$ S = \\frac{\\text{MSE}(\\hat{\\mu}_{\\text{unw}})}{\\text{MSE}(\\hat{\\mu}_{\\text{w}})} = \\frac{0.003725}{0.0001125} $$\n$$ S = \\frac{3725}{112.5} = \\frac{37250}{1125} = \\frac{1490}{45} = \\frac{298}{9} $$\n$$ S \\approx 33.1111... $$\nRounding to four significant figures, the value is $33.11$.",
            "answer": "$$\\boxed{33.11}$$"
        },
        {
            "introduction": "Sharing data is essential for accelerating research, but it must be executed without compromising participant privacy. This advanced problem introduces Differential Privacy, a rigorous mathematical framework for data release that provides provable confidentiality guarantees. You will calculate the privacy budget, $\\epsilon$, required to balance data utility and privacy protection, engaging with the state-of-the-art in ethical data governance through the Laplace mechanism and advanced composition theorems .",
            "id": "4318638",
            "problem": "A national genomic biobank intends to release simple cohort counts (for example, the number of consenting participants meeting a phenotypic filter) to researchers under a privacy-preserving protocol. Each count query is an integer-valued function on the dataset, and adding or removing one participant changes any single count by at most one under the standard definition of global $L_1$ sensitivity. The biobank’s Data Access Committee (DAC) and Institutional Review Board (IRB) have adopted Differential Privacy (DP) as the disclosure control framework and approved the Laplace mechanism for count release.\n\nIn one reporting cycle, the biobank plans to answer $Q=100$ independent count queries. To preserve utility for downstream precision medicine analyses, governance requires that the typical error be small: the median absolute error of the released counts must be no greater than $3$ individuals. To enable tighter accounting across multiple releases, the governance policy permits a nonzero total failure probability of $\\delta_{\\mathrm{tot}} = 10^{-6}$ across the reporting cycle.\n\nStarting from fundamental definitions and well-tested facts:\n- Global sensitivity for a count is $1$.\n- The Laplace mechanism adds noise $X$ distributed according to the Laplace distribution with zero mean and a scale parameter $b$ determined by $\\epsilon$ and sensitivity.\n- Advanced composition for Differential Privacy with a nonzero $\\delta$ can be used to track cumulative privacy loss across multiple mechanisms.\n\nDerive from first principles the minimal per-query privacy parameter $\\epsilon$ needed to satisfy the median absolute error requirement, and then compute the total composed privacy loss $\\epsilon_{\\mathrm{tot}}$ for the $Q$ releases using advanced composition with the approved $\\delta_{\\mathrm{tot}}$. Briefly justify whether advanced composition or basic composition is appropriate given the governance constraint on $\\delta_{\\mathrm{tot}}$. Report the numerical value of $\\epsilon_{\\mathrm{tot}}$ for $Q=100$ and $\\delta_{\\mathrm{tot}} = 10^{-6}$. Round your final numerical answer to four significant figures. Your final answer must be a single real-valued number with no units.",
            "solution": "The problem is first validated to ensure it is self-contained, scientifically grounded, and well-posed.\n\n### Step 1: Extract Givens\n- Number of independent count queries: $Q = 100$.\n- Maximum permissible median absolute error: $3$.\n- Total failure probability for the reporting cycle: $\\delta_{\\mathrm{tot}} = 10^{-6}$.\n- Global $L_1$ sensitivity for a count query: $\\Delta f = 1$.\n- Noise mechanism: Laplace mechanism.\n- Privacy composition method: Advanced composition for Differential Privacy with a nonzero $\\delta$.\n- Required outputs: Minimal per-query privacy parameter $\\epsilon$, total composed privacy loss $\\epsilon_{\\mathrm{tot}}$.\n- Final report requirement: Numerical value of $\\epsilon_{\\mathrm{tot}}$ rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded in the established mathematical framework of Differential Privacy. All concepts used—the Laplace mechanism, global sensitivity, and composition theorems—are standard and well-defined in the field. The problem is objective, using precise-technical language. It provides a complete set of givens ($Q$, error bound, $\\delta_{\\mathrm{tot}}$, sensitivity) to determine a unique solution for the requested parameters ($\\epsilon$ and $\\epsilon_{\\mathrm{tot}}$). The numerical values are realistic for a practical biobank application. There are no contradictions, ambiguities, or factual inaccuracies.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be derived.\n\n### Derivation of the Per-Query Privacy Parameter $\\epsilon$\n\nThe Laplace mechanism achieves $(\\epsilon, 0)$-Differential Privacy by adding noise drawn from a Laplace distribution to the true answer of a query. The noise variable $X$ has a probability density function (PDF) given by:\n$$f(x) = \\frac{1}{2b} \\exp\\left(-\\frac{|x|}{b}\\right)$$\nThe scale parameter $b$ is determined by the global sensitivity $\\Delta f$ and the privacy parameter $\\epsilon$ as:\n$$b = \\frac{\\Delta f}{\\epsilon}$$\nGiven that the global sensitivity for a count query is $\\Delta f = 1$, the scale parameter is $b = \\frac{1}{\\epsilon}$.\n\nThe error in a released count is the absolute value of the added noise, $|X|$. The problem states that the median absolute error must be no greater than $3$. Let $M$ denote the median of the distribution of $|X|$. The median is the value such that the cumulative probability is $0.5$, i.e., $P(|X| \\le M) = 0.5$.\n\nWe first find the cumulative distribution function (CDF) of $|X|$ for $y \\ge 0$:\n$$P(|X| \\le y) = \\int_{-y}^{y} \\frac{1}{2b} \\exp\\left(-\\frac{|x|}{b}\\right) dx$$\nDue to the symmetry of the Laplace distribution, this integral simplifies to:\n$$P(|X| \\le y) = 2 \\int_{0}^{y} \\frac{1}{2b} \\exp\\left(-\\frac{x}{b}\\right) dx = \\frac{1}{b} \\left[ -b \\exp\\left(-\\frac{x}{b}\\right) \\right]_{0}^{y}$$\n$$P(|X| \\le y) = -\\left(\\exp\\left(-\\frac{y}{b}\\right) - \\exp(0)\\right) = 1 - \\exp\\left(-\\frac{y}{b}\\right)$$\nTo find the median $M$, we set this probability to $0.5$:\n$$1 - \\exp\\left(-\\frac{M}{b}\\right) = 0.5$$\n$$\\exp\\left(-\\frac{M}{b}\\right) = 0.5$$\n$$-\\frac{M}{b} = \\ln(0.5) = -\\ln(2)$$\n$$M = b \\ln(2)$$\nThe governance requirement is that the median absolute error must not exceed $3$, so $M \\le 3$.\n$$b \\ln(2) \\le 3 \\implies b \\le \\frac{3}{\\ln(2)}$$\nTo find the minimal per-query privacy parameter $\\epsilon$, we must use the largest permissible noise, which corresponds to the largest possible scale parameter $b$. A larger $b$ implies a smaller $\\epsilon$ since $\\epsilon = \\frac{1}{b}$. Therefore, we seek to find the minimum $\\epsilon$ that satisfies the condition. Minimizing $\\epsilon$ is equivalent to maximizing $b$. The maximum allowed value for $b$ is:\n$$b_{\\max} = \\frac{3}{\\ln(2)}$$\nThe minimal per-query $\\epsilon$ is therefore:\n$$\\epsilon = \\frac{1}{b_{\\max}} = \\frac{\\ln(2)}{3}$$\n\n### Derivation of the Total Privacy Loss $\\epsilon_{\\mathrm{tot}}$\n\nThe biobank plans to answer $Q=100$ independent queries. Each query is protected by the $(\\epsilon, 0)$-DP Laplace mechanism with $\\epsilon = \\frac{\\ln(2)}{3}$.\n\nA basic composition theorem would state that the total privacy loss is $(Q\\epsilon, 0)$-DP, resulting in $\\epsilon_{\\mathrm{tot, basic}} = 100 \\times \\frac{\\ln(2)}{3} \\approx 23.1$, which is an unacceptably high privacy loss.\n\nHowever, the governance policy permits a non-zero total failure probability, $\\delta_{\\mathrm{tot}} = 10^{-6}$. This explicitly allows for the use of advanced composition theorems, which provide a significantly tighter bound on the total privacy parameter $\\epsilon_{\\mathrm{tot}}$ in exchange for this small failure probability. The use of advanced composition is therefore not only appropriate but essential to provide a useful privacy guarantee.\n\nThe standard advanced composition theorem states that for $\\epsilon \\le 1$, the composition of $Q$ independent $(\\epsilon, 0)$-DP mechanisms is $(\\epsilon_{\\mathrm{tot}}, \\delta_{\\mathrm{tot}})$-DP, where:\n$$\\epsilon_{\\mathrm{tot}} = \\sqrt{2Q \\ln(1/\\delta_{\\mathrm{tot}})}\\epsilon + Q\\epsilon(\\exp(\\epsilon)-1)$$\nWe must verify that our per-query $\\epsilon$ is sufficiently small. $\\epsilon = \\frac{\\ln(2)}{3} \\approx \\frac{0.6931}{3} \\approx 0.231$, which satisfies the condition $\\epsilon \\le 1$.\n\nWe can now substitute the given and derived values into the formula:\n- $Q = 100$\n- $\\delta_{\\mathrm{tot}} = 10^{-6}$\n- $\\epsilon = \\frac{\\ln(2)}{3}$\n\nThe total privacy loss is:\n$$\\epsilon_{\\mathrm{tot}} = \\sqrt{2(100) \\ln(1/10^{-6})} \\left(\\frac{\\ln(2)}{3}\\right) + 100 \\left(\\frac{\\ln(2)}{3}\\right) \\left(\\exp\\left(\\frac{\\ln(2)}{3}\\right)-1\\right)$$\nLet's compute the terms:\n$$\\ln(1/10^{-6}) = \\ln(10^6) = 6 \\ln(10)$$\n$$\\epsilon_{\\mathrm{tot}} = \\sqrt{1200 \\ln(10)} \\left(\\frac{\\ln(2)}{3}\\right) + \\frac{100 \\ln(2)}{3} \\left(\\exp\\left(\\frac{\\ln(2)}{3}\\right)-1\\right)$$\nWe can factor out the $\\epsilon = \\frac{\\ln(2)}{3}$ term:\n$$\\epsilon_{\\mathrm{tot}} = \\left(\\frac{\\ln(2)}{3}\\right) \\left[ \\sqrt{1200 \\ln(10)} + 100 \\left(\\exp\\left(\\frac{\\ln(2)}{3}\\right)-1\\right) \\right]$$\nNow, we compute the numerical values:\n- $\\ln(2) \\approx 0.693147$\n- $\\ln(10) \\approx 2.302585$\n- $\\epsilon = \\frac{\\ln(2)}{3} \\approx 0.231049$\n- $\\sqrt{1200 \\ln(10)} \\approx \\sqrt{1200 \\times 2.302585} = \\sqrt{2763.102} \\approx 52.5652$\n- $\\exp(\\epsilon) = \\exp(0.231049) \\approx 1.25993$\n\nSubstituting these values into the expression for $\\epsilon_{\\mathrm{tot}}$:\n$$\\epsilon_{\\mathrm{tot}} \\approx (0.231049) \\left[ 52.5652 + 100 (1.25993 - 1) \\right]$$\n$$\\epsilon_{\\mathrm{tot}} \\approx (0.231049) \\left[ 52.5652 + 100 (0.25993) \\right]$$\n$$\\epsilon_{\\mathrm{tot}} \\approx (0.231049) \\left[ 52.5652 + 25.993 \\right]$$\n$$\\epsilon_{\\mathrm{tot}} \\approx (0.231049) [78.5582]$$\n$$\\epsilon_{\\mathrm{tot}} \\approx 18.1511$$\nRounding the final numerical answer to four significant figures, we get $18.15$.",
            "answer": "$$\\boxed{18.15}$$"
        }
    ]
}