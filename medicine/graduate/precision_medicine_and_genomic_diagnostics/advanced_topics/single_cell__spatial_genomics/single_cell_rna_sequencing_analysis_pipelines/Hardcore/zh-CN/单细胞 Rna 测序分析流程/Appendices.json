{
    "hands_on_practices": [
        {
            "introduction": "在任何单细胞分析流程中，第一步也是最关键的一步是质量控制（QC）。原始数据中常常混杂着低质量的细胞，例如空液滴、双细胞或濒死细胞，它们会严重干扰下游分析。本实践  将指导你从基本原理出发，构建一个基于中位数和中位数绝对偏差（MAD）等稳健统计量的细胞筛选策略，这种方法能有效识别异常值，而自身不受异常值影响。",
            "id": "4382174",
            "problem": "给定单细胞核糖核酸测序 (scRNA-seq) 数据的三个质量控制指标：每个细胞的唯一分子标识 (UMI) 计数、每个细胞检测到的基因数，以及每个细胞中映射到线粒体基因的读段比例（线粒体分数）。您的目标是设计一个基于稳健统计学的可靠异常值过滤规则，并将其应用于具体的、指定的测试数据集。从第一性原理出发，从定义中位数和中位数绝对偏差 (MAD) 开始，在正态模型下（其中标准差通过一个固定的一致性常数与 MAD 相关），推导出一个针对 UMI 计数和检测到的基因数的对称异常值规则，以及一个针对线粒体分数的单侧异常值规则。选择一个异常值调整参数 $k$ 并说明选择的理由。通过使用具有科学合理性的下限对尺度进行正则化，处理因离散计数或相同值而可能出现的尺度估计中的退化情况。然后实现该过滤规则，以确定应移除哪些细胞。\n\n使用以下基本定义和事实：\n- 数据集的中位数是这样一个值 $m$：一半的观测值不大于 $m$，一半的观测值不小于 $m$。\n- 中位数绝对偏差 (MAD) 定义为 $\\mathrm{MAD} = \\mathrm{median}(|x_i - m|)$，其中 $m$ 是数据集 $\\{x_i\\}$ 的中位数。\n- 对于正态分布，标准差 $\\sigma$ 与中位数绝对偏差通过一个已知常数 $c$ 相关：$\\sigma \\approx c \\cdot \\mathrm{MAD}$。\n- 线粒体分数是一个比例，必须表示为 $[0,1]$ 范围内的十进制数，而非百分比。\n\n设计要求：\n1. 从 $\\mathrm{MAD}$ 推导出一个在正态模型下与标准差一致的稳健尺度估计量，并使用选定的 $k$ 值，围绕 UMI 计数和检测到的基因数的中位数构建对称阈值。使用相同的尺度概念和 $k$ 值，为线粒体分数构建一个单侧上阈值。\n2. 当由于相同值或离散计数导致 $\\mathrm{MAD}$ 计算为 $0$ 时，对尺度进行正则化以避免退化。对于计数类指标，使用一个按中位数缩放的分数下限。对于有界比例，如果可用，则使用观测到的与中位数的最小正偏差作为下限，否则使用一个尊重 $[0,1]$ 边界的微小正常数。\n3. 应用该规则以确定应被过滤的细胞索引集，如果一个细胞在三个指标中的任何一个上违反了阈值，则该细胞应被过滤。\n\n解释和预期的下游影响：从逻辑上解释过滤此类异常值如何影响 scRNA-seq 流程中的下游聚类分析，包括移除双细胞和排除以高线粒体分数为特征的受胁迫或濒死细胞，以及过于严格的阈值可能如何改变聚类边界。\n\n调整参数：\n- 使用 $k = 3$。\n\n测试套件和输入：\n您必须实现并运行一个包含以下三个测试用例的程序。每个测试用例为三个指标指定了等长的数组：UMI 计数、检测到的基因数和线粒体分数。细胞索引从 $0$ 开始。所有数字必须视为无单位的标量；线粒体分数必须视为十进制数，而非百分比。\n\n测试用例 1（数据集的中位数接近 $5{,}000$ 个 UMI，$1{,}600$ 个基因，线粒体分数集中在 $0.08$）：\n- UMI: $[5200,4800,5100,4950,5050,5300,4700,5000,5150,4850,5080,4920,4995,5010,4975,16000,600,5400,4600,5005]$\n- 基因: $[1650,1580,1620,1590,1610,1700,1550,1600,1630,1570,1615,1595,1605,1608,1592,3500,400,1680,1520,1602]$\n- 线粒体分数: $[0.07,0.09,0.08,0.085,0.075,0.06,0.1,0.08,0.07,0.09,0.082,0.078,0.08,0.081,0.079,0.04,0.25,0.065,0.11,0.08]$\n\n测试用例 2（在指定中位数附近存在退化的相同值，以及少数偏差值）：\n- UMI: $[5000,5000,5000,5000,5000,5000,5000,5000,5000,2000,8000,5000]$\n- 基因: $[1600,1600,1600,1600,1600,1600,1600,1600,1600,900,2800,1600]$\n- 线粒体分数: $[0.08,0.08,0.08,0.08,0.08,0.08,0.08,0.08,0.08,0.2,0.06,0.08]$\n\n测试用例 3（围绕指定中心值的重尾混合分布）：\n- UMI: $[5100,4950,5300,4700,5500,4800,6200,3000,5800,4900,5050,5150,4000,7000,5200]$\n- 基因: $[1620,1580,1700,1500,1750,1550,1800,1100,1720,1590,1610,1630,1200,1900,1600]$\n- 线粒体分数: $[0.09,0.07,0.06,0.12,0.05,0.08,0.04,0.18,0.06,0.09,0.08,0.07,0.16,0.03,0.085]$\n\n输出规范：\n- 对于每个测试用例，输出按升序排序的被过滤细胞索引列表（作为整数）。\n- 您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表。列表中的每个元素本身是对应测试用例的整数列表，例如：$[[i\\_1,i\\_2],[],[j\\_1]]$。",
            "solution": "任务是为单细胞 RNA 测序 (scRNA-seq) 质量控制 (QC) 指标设计并实现一种稳健的异常值过滤方法。该过滤方法必须基于稳健的统计学原理，特别是使用中位数和中位数绝对偏差 (MAD)，并且必须对尺度估计坍缩为零的退化情况具有稳健性。我们将从第一性原理出发推导过滤规则，证明所有参数选择的合理性，并将其应用于给定的测试用例。\n\n首先，我们为我们的方法建立统计学基础。在 scRNA-seq 分析中，异常细胞会严重扭曲下游分析，如标准化、降维和聚类。常见的异常细胞类型包括空液滴（UMI/基因计数极低）、双细胞或多细胞（UMI/基因计数被人为地抬高）以及受胁迫或濒死的细胞（线粒体基因表达比例高）。基于均值和标准差的经典异常值检测方法本身对异常值的存在很敏感。因此，需要使用稳健统计学。\n\n中位数是衡量中心趋势的稳健指标。对于一个数据集 $\\{x_1, x_2, \\dots, x_n\\}$，中位数 $m$ 是将数据分为较高一半和较低一半的值。对于偶数个数据点，它是两个中心值的平均值。\n\n中位数绝对偏差 (MAD) 是衡量统计离散度的稳健指标。它被定义为与数据中位数的绝对偏差的中位数：\n$$\n\\mathrm{MAD} = \\mathrm{median}(|x_i - m|)\n$$\n其中 $m = \\mathrm{median}(\\{x_i\\})$。\n\n为了在类似于流行的“三西格玛”规则的框架中使用 MAD，我们必须建立它与标准差 $\\sigma$ 的关系。对于一个服从正态分布 $\\mathcal{N}(\\mu, \\sigma^2)$ 的连续随机变量 $X$，其中位数是 $\\mu$。变量 $Y = |X - \\mu|$ 服从折叠正态分布。$Y$ 的中位数是满足 $P(Y \\le y_m) = 1/2$ 的值 $y_m$。这等价于 $P(|X - \\mu| \\le y_m) = 1/2$，或 $P(-y_m \\le X - \\mu \\le y_m) = 1/2$。标准化后，我们得到 $P(-y_m/\\sigma \\le Z \\le y_m/\\sigma) = 1/2$，其中 $Z \\sim \\mathcal{N}(0, 1)$。这意味着 $P(Z \\le y_m/\\sigma) - P(Z \\le -y_m/\\sigma) = 1/2$。根据正态分布的对称性，这可以简化为 $2 \\cdot P(Z \\le y_m/\\sigma) - 1 = 1/2$，从而得出 $P(Z \\le y_m/\\sigma) = 3/4$。\n因此，$y_m/\\sigma = \\Phi^{-1}(0.75)$，其中 $\\Phi^{-1}$ 是标准正态分布的分位数函数（或概率单位函数）。其值约为 $\\Phi^{-1}(0.75) \\approx 0.6745$。\nMAD 是 $y_m$ 的估计量，所以我们有 $\\mathrm{MAD} \\approx \\sigma \\cdot \\Phi^{-1}(0.75)$。\n由此，我们通过缩放 MAD 推导出一个稳健的标准差估计量 $\\hat{\\sigma}_{\\mathrm{rob}}$：\n$$\n\\hat{\\sigma}_{\\mathrm{rob}} = c \\cdot \\mathrm{MAD} = \\frac{1}{\\Phi^{-1}(0.75)} \\cdot \\mathrm{MAD} \\approx 1.4826 \\cdot \\mathrm{MAD}\n$$\n这个经过缩放的 MAD，即 $\\hat{\\sigma}_{\\mathrm{rob}}$，提供了一个标准差的估计值，它在正态模型下与 $\\sigma$ 一致，但对极端异常值保持稳健。\n\n现在我们构建异常值检测规则。异常值通常被定义为距离中心点若干个标准差之外的数据点。使用我们的稳健估计量，如果 $|x_i - m| > k \\cdot \\hat{\\sigma}_{\\mathrm{rob}}$，则值 $x_i$ 被标记。问题指定了调整参数 $k=3$。这个选择是“三西格玛规则”的一个稳健类似物，该规则将位于正态分布中心 $99.7\\%$ 之外的值识别为异常值。它为异常值检测提供了一个严格但标准的阈值。\n\n对于 UMI 计数和检测到的基因数，异常低和异常高的值都表示质量差。低值表明是空液滴或单细胞捕获失败，而高值则表明存在双细胞或多细胞。因此，我们使用对称的过滤规则。如果一个细胞的指标值 $x_i$ 落在区间之外，则该细胞被过滤：\n$$\n[m - k \\cdot \\hat{\\sigma}_{\\mathrm{rob}}, m + k \\cdot \\hat{\\sigma}_{\\mathrm{rob}}] = [m - 3 \\cdot c \\cdot \\mathrm{MAD}, m + 3 \\cdot c \\cdot \\mathrm{MAD}]\n$$\n\n对于线粒体分数，高值表示细胞处于胁迫、受损或凋亡状态，此时胞质 mRNA 丢失，导致线粒体转录本的比例更高。低值不是质量问题。因此，我们采用单侧上阈值。如果一个细胞的线粒体分数 $x_i$ 超过以下值，则该细胞被过滤：\n$$\nm + k \\cdot \\hat{\\sigma}_{\\mathrm{rob}} = m + 3 \\cdot c \\cdot \\mathrm{MAD}\n$$\n\n当 $\\mathrm{MAD} = 0$ 时会出现一个关键问题。如果超过一半的数据点相同，就会发生这种情况，这在离散计数数据或具有许多相同值的数据集中很常见。如果 $\\mathrm{MAD}=0$，则 $\\hat{\\sigma}_{\\mathrm{rob}}=0$，过滤区间退化为 $[m, m]$，可能过滤掉所有非中位数的值。为防止这种情况，我们必须对尺度估计进行正则化。\n1.  对于计数指标（UMI、基因），零尺度是没有信息的。我们为尺度估计量 $\\hat{\\sigma}_{\\mathrm{rob}}$ 引入一个与中位数成比例的下限。一个合理的选择是将此下限设为中位数的一个小部分，以确保尺度具有上下文相关性。我们将使用中位数的 $5\\%$，这个值足够小，在非退化情况下不会占主导地位，但又足够大以定义一个合理的范围。正则化后的尺度是 $\\max(\\hat{\\sigma}_{\\mathrm{rob}}, 0.05 \\cdot m)$。这仅在计算出的 $\\hat{\\sigma}_{\\mathrm{rob}}$ 为 $0$ 时应用。\n2.  对于线粒体分数，这是一个在 $[0,1]$ 范围内的比例，我们遵循指定的规则。如果 $\\mathrm{MAD}=0$，尺度被设置为与中位数的最小正偏差，即 $\\min(\\{|x_i-m| : |x_i-m|>0\\})$。如果所有值都相同且不存在正偏差，我们使用一个微小的正常数，考虑到指标的 $[0,1]$ 范围，$0.01$ 是一个合理的选择。\n\n对一个细胞的最终过滤决定是基于三个指标的异常值集合的并集。如果一个细胞因其 UMI 计数、基因计数 **或** 线粒体分数而被标记为异常值，则该细胞被移除。\n\n这种过滤对下游分析的影响是深远的。通过移除低质量细胞、双细胞和受胁迫细胞，我们确保后续的标准化和聚类是基于一组更同质的、有活力的单细胞。这可以防止因技术伪影（例如，“受胁迫细胞”聚类或“双细胞”聚类）而形成虚假聚类，并有助于恢复真实的生物学变异。然而，过于严格的过滤（例如，一个小的 $k$ 值）可能会移除稀有的细胞群体或处于有效生物学分布尾部的细胞，这可能导致组织细胞组成的有偏表示和聚类边界的改变。使用基于 MAD 的稳健阈值并取 $k=3$ 是一种标准做法，它在清理数据的需求与保留生物异质性之间取得了平衡。\n\n我们现在着手实现此方法并将其应用于指定的测试用例。对于所有计算，将使用常数 $c = 1/\\Phi^{-1}(0.75)$，其中 $\\Phi^{-1}(0.75)$ 由高精度数值库提供。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Solves the scRNA-seq QC filtering problem for the given test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"umi\": np.array([5200, 4800, 5100, 4950, 5050, 5300, 4700, 5000, 5150, 4850, 5080, 4920, 4995, 5010, 4975, 16000, 600, 5400, 4600, 5005]),\n            \"genes\": np.array([1650, 1580, 1620, 1590, 1610, 1700, 1550, 1600, 1630, 1570, 1615, 1595, 1605, 1608, 1592, 3500, 400, 1680, 1520, 1602]),\n            \"mito\": np.array([0.07, 0.09, 0.08, 0.085, 0.075, 0.06, 0.1, 0.08, 0.07, 0.09, 0.082, 0.078, 0.08, 0.081, 0.079, 0.04, 0.25, 0.065, 0.11, 0.08])\n        },\n        {\n            \"umi\": np.array([5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 2000, 8000, 5000]),\n            \"genes\": np.array([1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 900, 2800, 1600]),\n            \"mito\": np.array([0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.2, 0.06, 0.08])\n        },\n        {\n            \"umi\": np.array([5100, 4950, 5300, 4700, 5500, 4800, 6200, 3000, 5800, 4900, 5050, 5150, 4000, 7000, 5200]),\n            \"genes\": np.array([1620, 1580, 1700, 1500, 1750, 1550, 1800, 1100, 1720, 1590, 1610, 1630, 1200, 1900, 1600]),\n            \"mito\": np.array([0.09, 0.07, 0.06, 0.12, 0.05, 0.08, 0.04, 0.18, 0.06, 0.09, 0.08, 0.07, 0.16, 0.03, 0.085])\n        }\n    ]\n\n    results = []\n    \n    # Tuning parameters and constants\n    k = 3\n    # Consistency constant c = 1 / Φ⁻¹(0.75)\n    c = 1 / norm.ppf(0.75)\n\n    def find_outliers(data, is_symmetric, is_proportion, k_val, c_val):\n        \"\"\"\n        Identifies outliers based on the median and MAD.\n        \n        Args:\n            data (np.array): The input data vector.\n            is_symmetric (bool): True for two-sided filtering, False for one-sided (upper).\n            is_proportion (bool): True if the data is a proportion [0,1], for degeneracy handling.\n            k_val (float): The number of scaled MADs to use for the threshold.\n            c_val (float): The consistency constant for scaling MAD.\n\n        Returns:\n            set: A set of indices corresponding to outlier cells.\n        \"\"\"\n        median = np.median(data)\n        deviations = np.abs(data - median)\n        mad = np.median(deviations)\n        \n        scaled_mad = c_val * mad\n        \n        # Handle degeneracy (MAD = 0)\n        if scaled_mad == 0:\n            if is_proportion:\n                positive_devs = deviations[deviations > 0]\n                if len(positive_devs) > 0:\n                    # Regularize scale with the minimum positive deviation\n                    scaled_mad = np.min(positive_devs)\n                else:\n                    # All values are identical, use a small constant floor\n                    scaled_mad = 0.01 \n            else: # Count data\n                # Regularize scale with a fraction of the median, if median is non-zero\n                if median > 0:\n                    scaled_mad = max(scaled_mad, 0.05 * median)\n                # If median is also 0, a minimal absolute scale might be needed, but\n                # this case is unlikely for UMI/gene counts in a real pre-filtered dataset.\n                # Here, a scale of 0 would lead to no non-zero values being accepted.\n\n        # Define thresholds\n        upper_bound = median + k_val * scaled_mad\n        \n        if is_symmetric:\n            lower_bound = median - k_val * scaled_mad\n            outlier_indices = np.where((data < lower_bound) | (data > upper_bound))[0]\n        else: # One-sided upper threshold\n            outlier_indices = np.where(data > upper_bound)[0]\n            \n        return set(outlier_indices.tolist())\n\n    for case in test_cases:\n        umi_data = case[\"umi\"]\n        genes_data = case[\"genes\"]\n        mito_data = case[\"mito\"]\n        \n        umi_outliers = find_outliers(umi_data, is_symmetric=True, is_proportion=False, k_val=k, c_val=c)\n        genes_outliers = find_outliers(genes_data, is_symmetric=True, is_proportion=False, k_val=k, c_val=c)\n        mito_outliers = find_outliers(mito_data, is_symmetric=False, is_proportion=True, k_val=k, c_val=c)\n        \n        # A cell is filtered if it violates any of the three metric thresholds\n        total_outliers = sorted(list(umi_outliers | genes_outliers | mito_outliers))\n        results.append(total_outliers)\n\n    # Format the final output string exactly as specified\n    print(f\"[{','.join(map(str, results))}]\".replace(\" \", \"\"))\n\nsolve()\n```"
        },
        {
            "introduction": "经过质量控制后，我们面对的是一个高维的基因表达矩阵，其中包含了生物学信号和随机噪声。主成分分析（PCA）是揭示数据主要变异方向的强大工具，但关键问题在于如何确定哪些主成分（PCs）真正捕捉了生物学结构。本实践  将引导你使用置换检验这一严谨的统计方法来确定“显著”主成分的数量，并将这些主成分与预定义的生物学功能基因集联系起来，从而实现从数据驱动的模式到生物学洞见的转化。",
            "id": "4382117",
            "problem": "您的任务是设计一个可复现且自洽的计算流程，使用主成分分析 (PCA) 从标准化的单细胞核糖核酸测序 (scRNA-seq) 表达矩阵中提取结构，使用基于置换的零模型来决定保留的主成分数量，并根据预定义的生物学程序来解释主成分载荷。此任务属于精准医疗和基因组诊断流程的一部分，必须仅使用数学和算法原语进行形式化。\n\n本问题的基础是：分子生物学中心法则（脱氧核糖核酸到核糖核酸到蛋白质）、方差和协方差的统计定义，以及PCA通过协方差矩阵的特征分解找到最大方差的正交方向这一经过充分检验的程序。这些原则在单细胞数据集的基因表达解释和降维中被广泛接受和依赖。定义：主成分分析 (PCA) 被构建为一种对角化协方差矩阵的正交变换；其成分得分和载荷分别反映了细胞水平的投影和基因水平的贡献。本问题中的所有变量均被视为实数值。\n\n您的流程必须对每个测试用例遵循以下步骤：\n\n1. 数据模型和预处理：\n   - 设标准化表达矩阵为 $X \\in \\mathbb{R}^{n \\times g}$，其中 $n$ 是细胞数量，$g$ 是基因数量。列索引基因，行索引细胞。输入被模拟为与scRNA-seq流程一致的对数标准化表达。\n   - 执行按列标准化：对于每个基因 $j \\in \\{1,\\dots,g\\}$，计算样本均值 $\\mu_j = \\frac{1}{n}\\sum_{i=1}^n X_{ij}$ 和样本标准差 $\\sigma_j = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n (X_{ij}-\\mu_j)^2}$。通过 $Z_{ij} = \\frac{X_{ij}-\\mu_j}{\\sigma_j}$（对所有 $i,j$）定义标准化矩阵 $Z$。如果任何 $\\sigma_j = 0$，则将其替换为 $1$ 以避免除以零，同时保留该列。\n   - 理由：标准化可均衡基因特异性的尺度，这些尺度在scRNA-seq中因基因长度、捕获效率和生物学动态范围而异，确保PCA捕获的是协变结构而非绝对尺度。\n\n2. 主成分分析：\n   - 计算 $Z$ 的奇异值分解：$Z = U \\Sigma V^\\top$，其中 $U \\in \\mathbb{R}^{n \\times r}$，$ \\Sigma \\in \\mathbb{R}^{r \\times r}$ 是一个对角矩阵，其对角线元素为非负奇异值 $\\{s_1,\\dots,s_r\\}$，$V \\in \\mathbb{R}^{g \\times r}$ 具有正交标准列，且 $r = \\min(n,g)$。\n   - 样本协方差矩阵 $C = \\frac{1}{n-1}Z^\\top Z$ 的特征值为 $\\lambda_i = \\frac{s_i^2}{n-1}$（对于 $i \\in \\{1,\\dots,r\\}$），这些特征值量化了每个主成分解释的方差。\n   - 定义细胞得分为 $S = U \\Sigma \\in \\mathbb{R}^{n \\times r}$，基因载荷为 $L = V \\in \\mathbb{R}^{g \\times r}$，其中 $L_{\\cdot,i}$ 是主成分 $i$ 的载荷向量。\n\n3. 基于置换的零分布和成分保留：\n   - 通过独立地对每个基因列中的条目在细胞间进行置换来构建零模型，以打破细胞间的相关性，同时保留每个基因的边际分布。对于每次置换 $b \\in \\{1,\\dots,K\\}$，设 $X^{(b)}$ 为置换后的矩阵，使用与上述相同的定义将其标准化为 $Z^{(b)}$，计算其奇异值 $\\{s_i^{(b)}\\}$，并计算零特征值 $\\lambda_i^{(b)} = \\frac{(s_i^{(b)})^2}{n-1}$（对所有 $i \\in \\{1,\\dots,r\\}$）。\n   - 对于给定的分位数水平 $q \\in (0,1)$（以小数表示），将第 $i$ 个成分的零阈值定义为 $T_i(q) = \\operatorname{Quantile}_q\\left(\\{\\lambda_i^{(b)}\\}_{b=1}^K\\right)$。\n   - 从 $i=1$ 开始按顺序保留成分，只要 $\\lambda_i > T_i(q)$；在第一个不满足此不等式的 $i$ 处停止。设保留的成分数量为 $R$（一个整数）。\n\n4. 通过生物学程序进行解释：\n   - 给定 $P$ 个生物学程序，表示为不相交的基因索引集 $S_1, S_2, \\dots, S_P \\subset \\{1,\\dots,g\\}$。对于每个保留的主成分 $i \\in \\{1,\\dots,R\\}$，计算程序贡献得分 $s_{i,p} = \\sum_{j \\in S_p} L_{j,i}^2$（对所有 $p \\in \\{1,\\dots,P\\}$）。将使 $s_{i,p}$ 最大化的 $p$ 确定为成分 $i$ 的顶级程序。如有平局，必须以最小的 $p$ 为准。\n   - 以整数 $(0,1,\\dots,P-1)$ 报告程序标识符，对应于 $(S_1,S_2,\\dots,S_P)$。\n\n测试套件规范：\n- 使用由种子化的正态分布完全指定的随机生成，以确保确定性输出。每个测试用例的潜变量生成模型模拟了每个细胞的潜变量，这些潜变量线性映射到基因子集，并加上独立的高斯噪声，从而生成与对数标准化的scRNA-seq结构一致的矩阵 $X$。所有随机数均使用按指定种子初始化的伪随机数生成器生成。\n\n- 全局超参数：\n  - 置换次数 $K = \\;200$（整数）。\n  - 分位数水平 $q = \\;0.95$（小数）。\n\n- 测试用例 1（理想路径；多个结构化程序）：\n  - 维度：$n = \\;120$, $g = \\;30$。\n  - 种子：$s = \\;314159$。\n  - 程序集 ($P = \\;3$)：\n    - $S_1 = \\;\\{0,1,2,3,4,5,6,7,8,9\\}$，\n    - $S_2 = \\;\\{10,11,12,13,14,15,16,17,18,19\\}$，\n    - $S_3 = \\;\\{20,21,22,23,24\\}$。\n  - 纯噪声基因：$\\{25,26,27,28,29\\}$。\n  - 每个细胞 $i$ 的潜变量生成：\n    - 独立抽取 $z_{0,i} \\sim \\mathcal{N}(0,1)$，$z_{1,i} \\sim \\mathcal{N}(0,1)$，$z_{2,i} \\sim \\mathcal{N}(0,1)$。\n  - 数据生成：\n    - 对于 $j \\in S_1$：$X_{ij} = \\;2.0 \\cdot z_{0,i} + \\epsilon_{ij}$，其中 $\\epsilon_{ij} \\sim \\mathcal{N}(0,0.3^2)$。\n    - 对于 $j \\in S_2$：$X_{ij} = \\;1.8 \\cdot z_{1,i} + \\epsilon_{ij}$，其中 $\\epsilon_{ij} \\sim \\mathcal{N}(0,0.3^2)$。\n    - 对于 $j \\in S_3$：$X_{ij} = \\;1.2 \\cdot z_{2,i} + \\epsilon_{ij}$，其中 $\\epsilon_{ij} \\sim \\mathcal{N}(0,0.3^2)$。\n    - 对于噪声基因：$X_{ij} = \\;\\epsilon_{ij}$，其中 $\\epsilon_{ij} \\sim \\mathcal{N}(0,1.0^2)$。\n\n- 测试用例 2（边缘情况；近零结构）：\n  - 维度：$n = \\;120$, $g = \\;30$。\n  - 种子：$s = \\;271828$。\n  - 程序集 ($P = \\;3$) 与测试用例 1 相同：\n    - $S_1 = \\;\\{0,1,2,3,4,5,6,7,8,9\\}$，\n    - $S_2 = \\;\\{10,11,12,13,14,15,16,17,18,19\\}$，\n    - $S_3 = \\;\\{20,21,22,23,24\\}$。\n  - 所有基因均为噪声：\n    - 对于所有 $j \\in \\{0,1,\\dots,29\\}$ 和细胞 $i$：$X_{ij} = \\;\\epsilon_{ij}$，其中 $\\epsilon_{ij} \\sim \\mathcal{N}(0,1.0^2)$。\n\n- 测试用例 3（边界情况；单一主导程序）：\n  - 维度：$n = \\;120$, $g = \\;30$。\n  - 种子：$s = \\;161803$。\n  - 程序集 ($P = \\;3$)：\n    - $S_1 = \\;\\{0,1,2,3,4,5,6,7\\}$，\n    - $S_2 = \\;\\{10,11,12,13,14,15,16\\}$，\n    - $S_3 = \\;\\{20,21,22,23,24\\}$。\n  - 纯噪声基因：所有不在 $S_1 \\cup S_2 \\cup S_3$ 中的其他索引。\n  - 每个细胞 $i$ 的潜变量生成：\n    - 独立抽取 $z_{0,i} \\sim \\mathcal{N}(0,1)$。\n  - 数据生成：\n    - 对于 $j \\in S_1$：$X_{ij} = \\;2.5 \\cdot z_{0,i} + \\epsilon_{ij}$，其中 $\\epsilon_{ij} \\sim \\mathcal{N}(0,0.2^2)$。\n    - 对于所有其他基因：$X_{ij} = \\;\\epsilon_{ij}$，其中 $\\epsilon_{ij} \\sim \\mathcal{N}(0,1.0^2)$。\n\n程序输出规范：\n- 对于每个测试用例，计算保留的主成分数量 $R$（整数），并按顺序为保留的成分提供顶级程序标识符（作为整数列表）。如果 $R=0$，则程序列表为空。\n- 您的程序应生成单行输出，其中包含三个测试用例的结果，格式为方括号括起来的逗号分隔列表，不含空格。每个测试用例的结果必须格式化为 $[R,[p_1,p_2,\\dots,p_R]]$。例如，最终输出应类似于 $[[2,[0,1]],[0,[]],[1,[0]]]$，尽管实际数字将由计算确定。",
            "solution": "问题陈述已经过仔细验证。所有给定信息，包括数学定义、程序步骤和数值参数，都已提取和评估。\n\n**结论：** 该问题是 **有效的**。\n\n该问题具有科学依据，采用了计算生物学和统计学中标准且成熟的技术，如主成分分析 (PCA)、用于显著性估计的置换检验以及用于解释的基因集富集分析。问题陈述清晰，所有参数、数据生成模型和算法步骤都明确定义，确保在给定指定随机种子的情况下，每个测试用例都有唯一、确定的解。语言客观、精确。它没有任何验证清单中列出的缺陷，如科学上不合理、不完整或模棱两可。\n\n因此，下面提供一个完整的解决方案。\n\n### 方法论框架\n\n所指定的流程构成了一个严谨的流水线，用于识别和解释单细胞基因表达矩阵内的显著变异轴。这是通过四个顺序的、有原则的步骤实现的：数据标准化、通过PCA进行降维、对成分进行统计显著性评估以及对被认为显著的成分进行生物学解释。\n\n#### 步骤 1：数据模型和标准化\n\n输入是一个标准化的基因表达矩阵 $X \\in \\mathbb{R}^{n \\times g}$，其中 $n$ 代表细胞数量，$g$ 代表基因数量。条目 $X_{ij}$ 量化了细胞 $i$ 中基因 $j$ 的表达。为确保后续分析捕获的是基因表达的相关性结构，而不是被不同基因间广泛的表达值动态范围所偏倚，每个基因的表达向量（$X$ 中的一列）都经过标准化处理。\n\n对于每个基因 $j \\in \\{1,\\dots,g\\}$，我们计算它在所有 $n$ 个细胞中的样本均值 $\\mu_j$ 和样本标准差 $\\sigma_j$：\n$$\n\\mu_j = \\frac{1}{n}\\sum_{i=1}^n X_{ij}\n$$\n$$\n\\sigma_j = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n (X_{ij}-\\mu_j)^2}\n$$\n在 $\\sigma_j$ 的分母中使用 $n-1$ 对应于用于对总体方差进行无偏估计的Bessel校正。然后构建一个标准化矩阵 $Z$，其中每个元素 $Z_{ij}$ 由下式给出：\n$$\nZ_{ij} = \\frac{X_{ij}-\\mu_j}{\\sigma_j}\n$$\n这个变换，也称为计算Z-score，将每个基因的表达谱重新缩放，使其均值为 $0$，标准差为 $1$。如果出现罕见情况，即某个基因的方差为零（即 $\\sigma_j = 0$），其标准差将被设为 $1$ 以防止除以零。由于减去均值后的值将为零，这将使该列全为零。\n\n#### 步骤 2：主成分分析\n\nPCA应用于标准化矩阵 $Z$，以在高维基因空间中找到最大方差的正交方向。执行PCA的计算上最稳定的方法是通过 $Z$ 的奇异值分解 (SVD)：\n$$\nZ = U \\Sigma V^\\top\n$$\n这里，$U \\in \\mathbb{R}^{n \\times r}$ 是一个具有正交标准列的矩阵，代表细胞层面的投影；$\\Sigma \\in \\mathbb{R}^{r \\times r}$ 是一个对角矩阵，其对角线元素为非负奇异值 $s_1 \\ge s_2 \\ge \\dots \\ge s_r \\ge 0$；$V \\in \\mathbb{R}^{g \\times r}$ 是一个具有正交标准列的矩阵，代表基因空间中的主轴。秩 $r$ 是 $n$ 和 $g$ 中的较小者。\n\nPCA的关键输出源自SVD的各组成部分：\n-   **基因载荷 ($L$)：** 矩阵 $L = V$ 包含每个主成分的载荷向量。$L$ 的第 $i$ 列，表示为 $L_{\\cdot,i}$，是第 $i$ 个主轴，指示了每个基因对该成分的贡献。\n-   **细胞得分 ($S$)：** 矩阵 $S = U \\Sigma$ 包含每个细胞在新的主成分空间中的坐标。第 $i$ 列给出了所有细胞在第 $i$ 个主成分上的投影。\n-   **解释方差：** 第 $i$ 个主成分所捕获的方差是其相关联的特征值 $\\lambda_i$，该特征值是样本协方差矩阵 $C = \\frac{1}{n-1}Z^\\top Z$ 的特征值。这些特征值通过以下公式与 $Z$ 的SVD得到的奇异值直接相关：\n    $$\n    \\lambda_i = \\frac{s_i^2}{n-1}\n    $$\n\n#### 步骤 3：基于置换的成分保留\n\n虽然PCA总能找到成分，但并非所有成分都必然代表真实的生物学信号；许多可能仅仅捕获了随机噪声。为了区分显著成分与伪成分，我们为特征值构建一个零分布。零假设是细胞间的基因之间不存在系统的相关性结构。\n\n这是通过置换检验实现的。我们生成 $K = 200$ 个零数据集。对于每个零数据集 $b \\in \\{1, \\dots, K\\}$，我们通过独立地对每个基因（列）在所有细胞中的表达值进行混洗来创建一个置换矩阵 $X^{(b)}$。这个过程破坏了细胞层面真实的基因-基因相关性，同时保留了每个基因表达的边际分布。\n\n对于每个置换矩阵 $X^{(b)}$，我们重复标准化和PCA过程，以获得一组零特征值 $\\{\\lambda_i^{(b)}\\}_{i=1}^r$。经过 $K$ 次置换后，我们为每个特征值 $\\lambda_i$ 得到了一个经验零分布。然后，每个成分的显著性阈值 $T_i(q)$ 被定义为其对应零分布的上 $q$-分位数，其中 $q = 0.95$：\n$$\nT_i(q) = \\operatorname{Quantile}_q\\left(\\{\\lambda_i^{(b)}\\}_{b=1}^K\\right)\n$$\n如果一个主成分 $i$ 的观测特征值 $\\lambda_i$ 超过了这个零阈值，即 $\\lambda_i > T_i(q)$，则该成分被认为是显著的。我们按其方差贡献的降序（从 $i=1$ 到 $r$）评估成分，并保留所有成分，直到遇到第一个未通过此检验的成分为止。保留的成分总数表示为 $R$。\n\n#### 步骤 4：通过生物学程序进行解释\n\n最后一步是为 $R$ 个显著主成分赋予生物学意义。这是通过检查它们的基因载荷向量来完成的。一个大的载荷绝对值 $|L_{j,i}|$ 意味着基因 $j$ 与成分 $i$ 强相关。\n\n我们被提供了 $P$ 个预定义的生物学程序，每个程序由一组基因索引 $S_p$ 表示。为了量化一个成分 $i$ 在多大程度上代表一个程序 $p$，我们计算一个程序贡献得分 $s_{i,p}$。这个得分是属于该程序的所有基因的载荷平方和：\n$$\ns_{i,p} = \\sum_{j \\in S_p} L_{j,i}^2\n$$\n载荷的平方确保了贡献是根据关联的强度来衡量的，而不管基因与该成分的得分是正相关还是负相关。对于每个保留的成分 $i \\in \\{1,\\dots,R\\}$，我们识别使该得分最大化的程序 $p$，$p^* = \\arg\\max_{p} s_{i,p}$。这个程序被指定为该成分的主导生物学主题。如果出现平局，则选择索引 $p$ 最小的程序。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main solver function to execute all test cases and print the final result.\n    \"\"\"\n\n    # Global hyperparameters specified in the problem\n    K = 200  # Number of permutations\n    Q = 0.95  # Quantile level\n\n    # Test Case 1 Specifications\n    case1 = {\n        'n': 120, 'g': 30, 'seed': 314159,\n        'programs': [\n            set(range(0, 10)),\n            set(range(10, 20)),\n            set(range(20, 25))\n        ],\n        'gen_rules': {\n            'latent_factors': 3,\n            'noise_std_global': 1.0,\n            'program_rules': [\n                {'indices': set(range(0, 10)), 'factor_idx': 0, 'weight': 2.0, 'noise_std': 0.3},\n                {'indices': set(range(10, 20)), 'factor_idx': 1, 'weight': 1.8, 'noise_std': 0.3},\n                {'indices': set(range(20, 25)), 'factor_idx': 2, 'weight': 1.2, 'noise_std': 0.3},\n            ]\n        }\n    }\n\n    # Test Case 2 Specifications\n    case2 = {\n        'n': 120, 'g': 30, 'seed': 271828,\n        'programs': [\n            set(range(0, 10)),\n            set(range(10, 20)),\n            set(range(20, 25))\n        ],\n        'gen_rules': {\n            'latent_factors': 0,\n            'noise_std_global': 1.0,\n            'program_rules': []\n        }\n    }\n\n    # Test Case 3 Specifications\n    case3 = {\n        'n': 120, 'g': 30, 'seed': 161803,\n        'programs': [\n            set(range(0, 8)),\n            set(range(10, 17)),\n            set(range(20, 25))\n        ],\n        'gen_rules': {\n            'latent_factors': 1,\n            'noise_std_global': 1.0,\n            'program_rules': [\n                {'indices': set(range(0, 8)), 'factor_idx': 0, 'weight': 2.5, 'noise_std': 0.2},\n            ]\n        }\n    }\n    \n    test_cases = [case1, case2, case3]\n    results = []\n\n    for case in test_cases:\n        X = generate_data(case['n'], case['g'], case['seed'], case['gen_rules'])\n        R, top_programs = perform_pca_pipeline(X, K, Q, case['programs'])\n        results.append(f\"[{R},[{','.join(map(str, top_programs))}]]\")\n\n    print(f\"[{','.join(results)}]\")\n\ndef generate_data(n, g, seed, gen_rules):\n    \"\"\"\n    Generates the expression matrix X based on the specified generative model.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # Initialize with global noise\n    noise_std_global = gen_rules['noise_std_global']\n    X = rng.normal(loc=0, scale=noise_std_global, size=(n, g))\n\n    # Generate latent factors\n    num_latent = gen_rules['latent_factors']\n    if num_latent > 0:\n        latent_factors = rng.normal(loc=0, scale=1, size=(n, num_latent))\n\n    # Add program-specific signals\n    for rule in gen_rules['program_rules']:\n        indices = list(rule['indices'])\n        factor = latent_factors[:, rule['factor_idx']]\n        weight = rule['weight']\n        noise_std = rule['noise_std']\n        \n        # Override global noise with specific noise for these genes\n        noise = rng.normal(loc=0, scale=noise_std, size=(n, len(indices)))\n        \n        signal = np.outer(factor, np.ones(len(indices))) * weight\n        X[:, indices] = signal + noise\n        \n    return X\n\n\ndef perform_pca_pipeline(X, K, q, programs):\n    \"\"\"\n    Executes the full analysis pipeline: standardization, PCA, permutation test, and interpretation.\n    \"\"\"\n    n, g = X.shape\n    r = min(n, g)\n\n    # Step 1: Standardization\n    def standardize(mat):\n        mean = mat.mean(axis=0)\n        std = mat.std(axis=0, ddof=1)\n        std[std == 0] = 1.0\n        return (mat - mean) / std\n\n    Z = standardize(X)\n    \n    # Step 2: PCA on real data\n    _, s, Vt = np.linalg.svd(Z, full_matrices=False)\n    lambdas = (s**2) / (n - 1)\n    L = Vt.T  # Gene loadings matrix V\n\n    # Step 3: Permutation test\n    null_lambdas = np.zeros((K, r))\n    rng = np.random.default_rng(sum(X.shape) + X.size) # A deterministic seed based on data\n\n    for b in range(K):\n        X_perm = np.zeros_like(X)\n        for j in range(g):\n            X_perm[:, j] = rng.permutation(X[:, j])\n        \n        Z_perm = standardize(X_perm)\n        _, s_perm, _ = np.linalg.svd(Z_perm, full_matrices=False)\n        null_lambdas[b, :] = (s_perm**2) / (n - 1)\n\n    # Component retention\n    thresholds = np.quantile(null_lambdas, q, axis=0)\n    R = 0\n    for i in range(r):\n        if lambdas[i] > thresholds[i]:\n            R += 1\n        else:\n            break\n            \n    # Step 4: Interpretation\n    if R == 0:\n        return 0, []\n\n    top_programs = []\n    for i in range(R):\n        loadings_i = L[:, i]\n        \n        program_scores = []\n        for p_set in programs:\n            p_indices = list(p_set)\n            score = np.sum(loadings_i[p_indices]**2)\n            program_scores.append(score)\n            \n        # argmax breaks ties by smallest index, as required.\n        top_p = np.argmax(program_scores)\n        top_programs.append(top_p)\n        \n    return R, top_programs\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "识别细胞类型是单细胞分析的核心目标之一，通常通过对细胞进行聚类来实现。然而，选择合适的聚类“分辨率”（即簇的数量和大小）是一个关键且具挑战性的决策，它直接影响着生物学结论的可靠性。本实践  介绍了一种精妙的策略：通过评估在不同分辨率下差异表达（DE）基因标记的稳定性，来选择一个能产生最一致、最可复现生物学特征的“最佳”聚类方案。",
            "id": "4382215",
            "problem": "给定一个合成的单细胞RNA测序（scRNA-seq）表达矩阵和一组多分辨率下的聚类标签。您的任务是实现一个严谨且可复现的流程。对于每个聚类分辨率，该流程使用非参数检验计算差异表达（DE）标记基因，通过Benjamini–Hochberg（BH）程序控制假发现率（FDR），量化不同分辨率下DE基因集的重叠程度，并选择能够最大化标记基因发现稳定性的聚类分辨率，该稳定性通过平均成对Jaccard相似度来量化。\n\n使用的基本原理和定义：\n- 分子生物学的中心法则指出，基因被转录成核糖核酸，在这里以非负计数的形式进行测量。scRNA-seq的测量值可以被建模为每个细胞中每个基因的非负实数值观测。\n- Mann–Whitney U检验（也称为Wilcoxon秩和检验）是一种非参数检验，用于比较一个样本是否倾向于比另一个样本具有更大的值。给定两个大小分别为 $n_1$ 和 $n_2$ 的独立样本，它基于秩计算出一个检验统计量 $U$，并生成一个 $p$ 值，用于检验两个分布相同的原假设，其备择假设为一个分布倾向于比另一个分布更大。\n- Benjamini–Hochberg程序控制期望的假发现比例（FDR）。给定 $m$ 个 $p$ 值 $p_{(1)} \\le \\dots \\le p_{(m)}$，BH调整后的 $q$ 值满足\n$$\nq_{(i)} = \\min_{k \\ge i} \\left\\{ \\frac{m}{k} p_{(k)} \\right\\} \\wedge 1,\n$$\n将其映射回原始索引，即可得到每个假设的调整后 $q$ 值。\n- 对于一个基因，在均值分别为 $\\mu_1$ 和 $\\mu_2$ 的两组之间，其对数倍数变化（以2为底的对数）定义为\n$$\n\\log_2 \\left( \\frac{\\mu_1 + \\varepsilon}{\\mu_2 + \\varepsilon} \\right),\n$$\n其中 $\\varepsilon > 0$ 是一个很小的数，用以避免除以零。\n- 集合 $A$ 和 $B$ 之间的Jaccard相似度为\n$$\nJ(A,B) = \n\\begin{cases}\n\\frac{|A \\cap B|}{|A \\cup B|},  \\text{if } |A \\cup B| > 0, \\\\\n1,  \\text{if } |A|=|B|=0, \\\\\n0,  \\text{otherwise}.\n\\end{cases}\n$$\n\n从头开始实现的流程：\n1. 对于每个具有细胞聚类标签的聚类分辨率 $r$，为该分辨率下的每个聚类 $c$ 和每个基因 $g$ 执行“一对剩余”（one-versus-rest）的DE分析：\n   - 在单侧备择假设（聚类 $c$ 中的表达随机大于其余细胞中的表达）下，计算Mann–Whitney U检验的 $p$ 值。\n   - 计算对数倍数变化 $\\log_2\\left((\\mu_{\\text{in}} + \\varepsilon)/(\\mu_{\\text{out}} + \\varepsilon)\\right)$，其中 $\\mu_{\\text{in}}$ 是聚类 $c$ 内部的平均表达量，$\\mu_{\\text{out}}$ 是聚类 $c$ 外部的平均表达量，$\\varepsilon > 0$ 是一个很小的常数。\n   - 对同一聚类 $c$ 内的所有基因应用Benjamini–Hochberg程序以获得 $q$ 值。如果一个基因的 $q \\le \\alpha$ 并且其对数倍数变化至少为 $\\tau$，则该基因是聚类 $c$ 的一个DE标记基因。\n2. 对于分辨率 $r$，将DE基因集 $S_r$ 定义为在该分辨率下被判定为任何一个聚类的DE基因的所有基因的并集。\n3. 对于每个分辨率 $r$，计算 $S_r$ 与所有其他分辨率 $s \\neq r$ 的 $S_s$ 之间的平均成对Jaccard相似度 $\\bar{J}_r$：\n$$\n\\bar{J}_r = \\frac{1}{R-1} \\sum_{\\substack{s=1 \\\\ s \\ne r}}^{R} J(S_r, S_s),\n$$\n其中 $R$ 是分辨率的数量。\n4. 选择使 $\\bar{J}_r$ 最大化的分辨率索引 $\\hat{r}$。如果存在平局，则选择最小的索引。\n\n使用以下固定的数据和聚类标签。共有 $G = 8$ 个基因和 $C = 12$ 个细胞。表达矩阵 $X \\in \\mathbb{R}_{\\ge 0}^{G \\times C}$ 以每个基因包含 $C$ 个值的列表形式给出，按细胞索引 $0$ 到 $11$ 排序：\n- 基因 $g_0$：$[10,9,11,10,1,2,1,2,0,1,0,1]$。\n- 基因 $g_1$：$[1,1,1,2,12,11,13,12,1,0,1,0]$。\n- 基因 $g_2$：$[0,1,0,1,0,0,1,1,8,9,8,9]$。\n- 基因 $g_3$：$[6,7,6,7,7,6,7,6,1,1,1,1]$。\n- 基因 $g_4$：$[3,3,3,3,3,3,3,3,3,3,3,3]$。\n- 基因 $g_5$：$[4,5,4,5,2,2,2,2,2,2,2,2]$。\n- 基因 $g_6$：$[1,1,1,1,4,3,4,3,2,2,2,2]$。\n- 基因 $g_7$：$[0,0,1,0,0,0,0,1,0,0,0,0]$。\n\n定义 $R = 3$ 个聚类分辨率，每个分辨率为每个细胞（细胞索引为 $0,1,\\dots,11$）提供一个标签：\n- 分辨率 $r_0$（“粗略”，2个聚类）：标签 $[0,0,0,0,0,0,0,0,1,1,1,1]$。\n- 分辨率 $r_1$（“中等”，3个聚类）：标签 $[0,0,0,0,1,1,1,1,2,2,2,2]$。\n- 分辨率 $r_2$（“精细”，4个聚类）：标签 $[0,0,1,1,2,2,2,2,3,3,3,3]$。\n\nDE判定的参数是FDR阈值 $\\alpha$、最小对数倍数变化阈值 $\\tau$ 以及用于对数倍数变化的伪计数 $\\varepsilon$。使用单侧Mann–Whitney U检验，备择假设为“greater”（聚类与其余部分比较）。对于所有计算，对数的底数均视为 $2$。\n\n测试套件。在以下四种情况下运行该流程，每种情况由三元组 $(\\alpha,\\tau,\\varepsilon)$ 指定：\n- 情况 1：$(\\alpha,\\tau,\\varepsilon) = (0.05, 0.5, 10^{-3})$。\n- 情况 2：$(\\alpha,\\tau,\\varepsilon) = (0.001, 1.5, 10^{-3})$。\n- 情况 3：$(\\alpha,\\tau,\\varepsilon) = (0.10, 0.0, 10^{-3})$。\n- 情况 4：$(\\alpha,\\tau,\\varepsilon) = (0.05, 0.8, 10^{-3})$。\n\n对于每种情况，您必须计算：\n- 根据最大 $\\bar{J}_r$ 选择的分辨率索引 $\\hat{r}$（平局时取最小索引）。\n- 集合大小 $|S_{r_0}|$、$|S_{r_1}|$ 和 $|S_{r_2}|$。\n- 平均成对Jaccard相似度 $\\bar{J}_{r_0}$、$\\bar{J}_{r_1}$ 和 $\\bar{J}_{r_2}$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。\n- 每种情况的结果必须是 $[\\hat{r}, |S_{r_0}|, |S_{r_1}|, |S_{r_2}|, \\bar{J}_{r_0}, \\bar{J}_{r_1}, \\bar{J}_{r_2}]$ 形式的子列表。\n- 总输出必须是包含这四个子列表的单个列表，顺序与上述情况一致。不要打印任何额外文本。\n\n所有数值输出都应是普通数字；角度和物理单位不适用于此问题。在表示分数或概率时，请像上述规范一样使用小数。程序必须是自包含的，且不得读取任何外部输入或文件。",
            "solution": "该问题被评估为有效。它提出了一个与单细胞RNA测序分析相关的、自包含的、有科学依据且定义明确的计算任务。其定义、数据和算法步骤的规定足够精确，可以得出一个唯一的、确定性的解。一个小的排印错误，“Gene $g_y$”，根据 $G=8$ 个基因从0开始索引的上下文，可以明确地解释为“Gene $g_7$”。\n\n解决方案涉及实现指定的四步流程，从一组候选分辨率中识别出最优的聚类分辨率。这是通过评估跨分辨率的差异表达（DE）基因发现的稳定性来实现的。与其他分辨率相比，能够产生最一致的标记基因集的分辨率被视为最优分辨率。\n\n该方法的核心是在每个分辨率下对每个聚类进行系统的DE分析，然后对生成的基因集进行聚合和比较。整个过程针对四个不同的参数集 $(\\alpha, \\tau, \\varepsilon)$ 执行，为每个参数集生成一套相应的结果。\n\n步骤1：一对剩余（One-Versus-Rest）差异表达分析\n\n对于每个分辨率 $r$（总共 $R=3$ 个分辨率）以及该分辨率内的每个聚类 $c$，我们执行DE分析，以寻找在该聚类中相比所有其他细胞显著上调的基因（即“一对剩余”方案）。对于每个基因 $g$（总共 $G=8$ 个基因）：\n\n1.  数据分区：将基因 $g$ 在 $C=12$ 个细胞中的表达数据分为两组：“组内”（in-group）由属于聚类 $c$ 的细胞组成，“组外”（out-group）由所有其他细胞组成。\n\n2.  统计检验：对这两组数据应用Mann-Whitney U检验。这种非参数检验适用于在不假设正态性的情况下比较分布。按照规定，使用单侧检验来检验备择假设，即组内表达值随机大于组外表达值。这样可以为每个基因、每个聚类产生一个 $p$ 值。\n\n3.  效应量计算：计算以2为底的对数倍数变化（logFC），以量化表达差异的大小。公式为：\n    $$\n    \\text{logFC} = \\log_2 \\left( \\frac{\\mu_{\\text{in}} + \\varepsilon}{\\mu_{\\text{out}} + \\varepsilon} \\right)\n    $$\n    其中 $\\mu_{\\text{in}}$ 和 $\\mu_{\\text{out}}$ 分别是基因在组内和组外的平均表达水平。伪计数 $\\varepsilon$ 是一个小的正常数，用于在平均表达量低或为零时稳定比率。\n\n4.  多重检验校正：在单个聚类的DE分析中，我们检验 $m=G=8$ 个基因，这构成了多重假设检验。为了控制假发现率（FDR），将Benjamini-Hochberg（BH）程序应用于这 $m$ 个 $p$ 值集合。首先，对 $p$ 值进行排序：$p_{(1)} \\le p_{(2)} \\le \\dots \\le p_{(m)}$。然后为排序后的列表计算相应的调整后 $p$ 值（即 $q$ 值）：\n    $$\n    q_{(i)} = \\min \\left(1, \\min_{k=i}^{m} \\left\\{ \\frac{m \\cdot p_{(k)}}{k} \\right\\} \\right)\n    $$\n    然后将这些 $q$ 值映射回其原始的基因顺序。\n\n5.  DE标记基因识别：如果基因 $g$ 对应的 $q$ 值小于或等于显著性阈值 $\\alpha$ 并且其logFC大于或等于量级阈值 $\\tau$，则该基因被声明为聚类 $c$ 的DE标记基因。\n\n步骤2：按分辨率聚合DE基因集\n\n在对一个分辨率 $r$ 内的所有聚类执行DE分析后，将各个标记基因列表进行聚合。构建一个集合 $S_r$，它包含在该分辨率下任何一个聚类中被识别为DE标记基因的所有基因的并集：\n$$\nS_r = \\bigcup_{c \\in \\text{clusters}(r)} \\{\\text{DE markers for cluster } c\\}\n$$\n对所有 $R=3$ 个分辨率重复此过程，得到三个集合：$S_{r_0}$、$S_{r_1}$ 和 $S_{r_2}$。\n\n步骤3：通过Jaccard相似度量化稳定性\n\n给定分辨率下标记基因发现的稳定性，是通过衡量其DE基因集与其他分辨率的基因集的一致性程度来量化的。这通过Jaccard相似度来完成，对于两个集合 $A$ 和 $B$，其定义为：\n$$\nJ(A,B) = \n\\begin{cases}\n\\frac{|A \\cap B|}{|A \\cup B|},  \\text{if } |A \\cup B| > 0, \\\\\n1,  \\text{if } |A|=|B|=0, \\\\\n0,  \\text{otherwise}.\n\\end{cases}\n$$\n对于每个分辨率 $r$，我们计算其与所有其他分辨率 $s \\neq r$ 的平均成对Jaccard相似度 $\\bar{J}_r$：\n$$\n\\bar{J}_r = \\frac{1}{R-1} \\sum_{\\substack{s=1 \\\\ s \\ne r}}^{R} J(S_r, S_s)\n$$\n给定索引为0、1、2的 $R=3$ 个分辨率，该公式可简化为：\n$$\n\\bar{J}_{r_0} = \\frac{1}{2} (J(S_{r_0}, S_{r_1}) + J(S_{r_0}, S_{r_2}))\n$$\n$$\n\\bar{J}_{r_1} = \\frac{1}{2} (J(S_{r_1}, S_{r_0}) + J(S_{r_1}, S_{r_2}))\n$$\n$$\n\\bar{J}_{r_2} = \\frac{1}{2} (J(S_{r_2}, S_{r_0}) + J(S_{r_2}, S_{r_1}))\n$$\n\n步骤4：选择最优分辨率\n\n最优分辨率是使其标记基因集稳定性最大化的那个分辨率。我们选择与最大平均Jaccard相似度对应的分辨率索引 $\\hat{r}$：\n$$\n\\hat{r} = \\arg\\max_{r \\in \\{r_0, r_1, r_2\\}} \\bar{J}_r\n$$\n如果 $\\bar{J}_r$ 值出现平局，则选择索引最小的分辨率。\n\n最后，对于由 $(\\alpha, \\tau, \\varepsilon)$ 定义的四个测试用例中的每一个，都执行这整个流程来计算最终的输出向量：$[\\hat{r}, |S_{r_0}|, |S_{r_1}|, |S_{r_2}|, \\bar{J}_{r_0}, \\bar{J}_{r_1}, \\bar{J}_{r_2}]$。所提供的Python代码实现了这些步骤以生成所需的结果。",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import mannwhitneyu\n\ndef bh_procedure(p_values):\n    \"\"\"\n    Apply the Benjamini-Hochberg procedure to a list of p-values.\n    \"\"\"\n    p_values = np.asarray(p_values)\n    m = len(p_values)\n    if m == 0:\n        return np.array([])\n    \n    # Get the indices that would sort the p-values\n    sorted_indices = np.argsort(p_values)\n    \n    # Create an array to map the sorted q-values back to original positions\n    q_values = np.empty(m)\n    \n    # Get sorted p-values\n    sorted_p_values = p_values[sorted_indices]\n    \n    # Calculate the BH critical values (m/k * p_k)\n    ranks = np.arange(1, m + 1)\n    bh_critical_values = (m / ranks) * sorted_p_values\n    \n    # The formula q_(i) = min_{k>=i} {m*p_(k)/k} is a cumulative minimum\n    # from the end of the sorted list.\n    cumulative_min = np.minimum.accumulate(bh_critical_values[::-1])[::-1]\n    \n    # Ensure q-values do not exceed 1\n    q_sorted = np.minimum(cumulative_min, 1.0)\n    \n    # Map sorted q-values back to the original order of genes\n    q_values[sorted_indices] = q_sorted\n    \n    return q_values\n\ndef jaccard_similarity(set1, set2):\n    \"\"\"\n    Calculate the Jaccard similarity between two sets.\n    \"\"\"\n    union_len = len(set1.union(set2))\n    if union_len == 0:\n        # Per problem spec: J=1 if |A|=|B|=0\n        return 1.0\n    \n    intersection_len = len(set1.intersection(set2))\n    return intersection_len / union_len\n\ndef solve():\n    \"\"\"\n    Main function to execute the full pipeline for all test cases.\n    \"\"\"\n    # G=8 genes, C=12 cells\n    X_data = [\n        [10, 9, 11, 10, 1, 2, 1, 2, 0, 1, 0, 1],  # g0\n        [1, 1, 1, 2, 12, 11, 13, 12, 1, 0, 1, 0], # g1\n        [0, 1, 0, 1, 0, 0, 1, 1, 8, 9, 8, 9],    # g2\n        [6, 7, 6, 7, 7, 6, 7, 6, 1, 1, 1, 1],    # g3\n        [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],    # g4\n        [4, 5, 4, 5, 2, 2, 2, 2, 2, 2, 2, 2],    # g5\n        [1, 1, 1, 1, 4, 3, 4, 3, 2, 2, 2, 2],    # g6\n        [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0],    # g7\n    ]\n    X = np.array(X_data, dtype=float)\n    G, C = X.shape\n\n    # R=3 resolutions\n    resolutions_data = [\n        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1],    # r0\n        [0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2],    # r1\n        [0, 0, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3],    # r2\n    ]\n    resolutions = np.array(resolutions_data)\n    R = resolutions.shape[0]\n    \n    test_cases = [\n        (0.05, 0.5, 1e-3),\n        (0.001, 1.5, 1e-3),\n        (0.10, 0.0, 1e-3),\n        (0.05, 0.8, 1e-3),\n    ]\n\n    all_results = []\n\n    for alpha, tau, epsilon in test_cases:\n        S_sets = []\n        # Step 1  2: DE analysis and aggregation for each resolution\n        for r_idx in range(R):\n            labels = resolutions[r_idx, :]\n            S_r = set()\n            unique_clusters = np.unique(labels)\n\n            for cluster_id in unique_clusters:\n                in_mask = (labels == cluster_id)\n                out_mask = ~in_mask\n\n                if np.sum(in_mask) == 0 or np.sum(out_mask) == 0:\n                    continue\n\n                p_values = []\n                logFCs = []\n                for g_idx in range(G):\n                    in_expr = X[g_idx, in_mask]\n                    out_expr = X[g_idx, out_mask]\n                    \n                    # Mann-Whitney U test p-value\n                    # If all values are the same, the test has no power.\n                    if np.all(in_expr == in_expr[0]) and np.all(out_expr == out_expr[0]) and in_expr[0] == out_expr[0]:\n                        stat_p_val = 1.0\n                    else:\n                        _, stat_p_val = mannwhitneyu(in_expr, out_expr, alternative='greater', use_continuity=True)\n                    p_values.append(stat_p_val)\n\n                    # Log fold-change\n                    mu_in = np.mean(in_expr)\n                    mu_out = np.mean(out_expr)\n                    logFC = np.log2((mu_in + epsilon) / (mu_out + epsilon))\n                    logFCs.append(logFC)\n                \n                # Benjamini-Hochberg correction\n                q_values = bh_procedure(p_values)\n                \n                # Identify DE markers\n                for g_idx in range(G):\n                    if q_values[g_idx] = alpha and logFCs[g_idx] = tau:\n                        S_r.add(g_idx)\n            \n            S_sets.append(S_r)\n\n        # Step 3: Compute pairwise Jaccard similarities\n        S_sizes = [len(s) for s in S_sets]\n        J_matrix = np.ones((R, R))\n        for i in range(R):\n            for j in range(i + 1, R):\n                jaccard_val = jaccard_similarity(S_sets[i], S_sets[j])\n                J_matrix[i, j] = jaccard_val\n                J_matrix[j, i] = jaccard_val\n\n        # Compute average Jaccard similarity for each resolution\n        J_avg = []\n        for r_idx in range(R):\n            # Sum over row, subtract diagonal (1), and divide by (R-1)\n            avg = (np.sum(J_matrix[r_idx, :]) - 1.0) / (R - 1)\n            J_avg.append(avg)\n\n        # Step 4: Choose optimal resolution\n        # np.argmax breaks ties by returning the first index, which is the smallest.\n        hat_r = np.argmax(J_avg)\n        \n        case_result = [hat_r] + S_sizes + J_avg\n        all_results.append(case_result)\n\n    # Format the final output\n    output_str_parts = []\n    for res_list in all_results:\n        sublist_str = \"[{}]\".format(','.join(map(str, res_list)))\n        output_str_parts.append(sublist_str)\n    \n    print(\"[{}]\".format(','.join(output_str_parts)))\n\nsolve()\n```"
        }
    ]
}