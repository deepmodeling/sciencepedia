## 引言
[单细胞RNA测序](@entry_id:142269)（[scRNA-seq](@entry_id:155798)）技术彻底改变了我们解析生物系统复杂性的方式，它以前所未有的分辨率揭示了细胞间的异质性，为精准医学和基因组诊断领域带来了巨大的希望。然而，从数以百万计的原始测序读段到可靠的生物学洞见，这一过程充满了复杂的计算挑战和统计陷阱。分析流程中的每一步选择，都可能深刻影响最终的结论。因此，一个系统性、基于第一性原理的理解对于研究人员而言至关重要。本文旨在为读者提供一个关于现代[scRNA-seq分析](@entry_id:266931)流程的全面而深入的指南，弥合原始数据与生物学意义之间的知识鸿沟。

在接下来的章节中，我们将踏上一段从数据到发现的旅程。在第一章“原理与机制”中，我们将深入剖析分析流程的每个核心环节，从原始数据处理的统计基础，到[数据标准化](@entry_id:147200)、降维和聚类的先进算法，揭示其背后的数学逻辑。随后，在第二章“应用与跨学科连接”中，我们将展示这些原理如何在多样的研究场景中大放异彩，从绘制[细胞图谱](@entry_id:270083)、追踪细胞命运，到整合多组学数据并推动临床转化。最后，通过一系列精心设计的“动手实践”案例，您将有机会将理论付诸实践，巩固对关键概念的理解。本指南将为您驾驭复杂的单细胞数据世界提供坚实的理论基础和实践洞察力。

## 原理与机制

本章深入探讨了[单细胞RNA测序](@entry_id:142269)（scRNA-seq）分析流程中的核心原理与关键机制。在上一章介绍scRNA-seq技术背景的基础上，本章将系统性地阐述从原始测序数据到生物学洞见的全过程中，所涉及的统计学基础、计算方法和关键决策点。我们将遵循一个典型的分析流程，逐一解析每个步骤背后的科学逻辑，旨在为读者构建一个严谨、连贯且深入的知识框架，以应对精准医学和基因组诊断领域的复杂挑战。

### 从原始测[序数](@entry_id:150084)据到细胞计数矩阵

[scRNA-seq分析](@entry_id:266931)的起点是将高通量的原始测序读段（reads）转化为一个精确的基因-细胞表达谱矩阵。这一过程不仅是简单的计数，更涉及复杂的[统计推断](@entry_id:172747)和质量控制，以确保数据的准确性和可靠性。

#### 独有分子标识符（UMI）的数字化定量原理

现代高通量[scRNA-seq](@entry_id:155798)技术，特别是基于液滴的方法，面临的一个核心挑战是聚合酶链式反应（PCR）带来的扩增偏好。在文库构建过程中，不同的cDNA分子可能被扩增成千上万次，而扩增效率的差异会严重扭曲真实的基因表达丰度。为了克服这一问题，**独有分子标识符（Unique Molecular Identifiers, UMIs）**应运而生。

**UMI**是一段短的、随机的[核苷](@entry_id:195320)酸序列（通常为6-12个碱基），在逆转录过程中被连接到每个cDNA分子的末端。其核心机制在于：在任何扩增步骤发生之前，为每一个原始的RNA分子打上一个独特的分子条形码。理想情况下，来自同一个细胞的同一个基因的不同RNA分子，会被标记上不同的UMI。随后，无论这些分子经历多少轮PCR扩增，所有源自同一个初始分子的测序读段都会共享相同的[基因序列](@entry_id:191077)和相同的UMI序列。

在数据分析阶段，我们首先将测序读段根据其基因来源进行分组。在每个基因组内，我们将所有具有完全相同UMI序列的读段视为PCR扩增的副本。通过对这些副本进行去重（deduplication），只计算每个基因对应的**不同UMI**的数量，我们就能直接估计出原始RNA分子的数量。例如，即使一个基因检测到$10,000$个测序读段，但这些读段仅对应$50$个不同的UMI，那么其表达量就被记为$50$个分子，而非$10,000$。这种“数字化计数”方式，有效地消除了PCR扩增偏好对基因表达定量的影响，使得测量结果更加准确。

然而，UMI的应用引入了一个新的概率问题：**UMI碰撞（collision）**。如果两个或更多的不同分子被错误地标记上相同的UMI，它们在去重后会被误认为是一个分子，从而导致表达量的低估。碰撞的概率取决于UMI序列的长度（$L$）和需要标记的分子总数（$M$）。假设UMI序列由4种[核苷](@entry_id:195320)酸构成，那么总的UMI空间大小为$N = 4^L$。如果我们为$M$个分子独立、均匀地从这个空间中抽取UMI，那么至少发生一次碰撞的概率可以从其[补集](@entry_id:161099)事件——即所有$M$个分子都被分配到不同UMI——的概率来推导。

总的分配方式有$N^M = (4^L)^M$种。没有碰撞的分配方式数量为从$N$个UMI中选出$M$个并进行排列，即$P(N, M) = \frac{N!}{(N-M)!}$。因此，不发生碰撞的概率是：
$$
P(\text{无碰撞}) = \frac{N!}{N^M (N-M)!}
$$
所以，至少发生一次碰撞的概率为：
$$
P(\text{碰撞}) = 1 - \frac{(4^L)!}{(4^L)^M (4^L - M)!}
$$
这个公式凸显了在实验设计中选择足够长UMI的重要性，以确保在预期的分子捕获数量下，UMI碰撞的概率可以忽略不计，从而保证数字化计数的准确性。

#### 从环境RNA中识别真实细胞

在基于液滴的scRNA-seq技术（如10x Genomics）中，细胞悬液、逆转录酶和带有条形码的凝胶珠（gel beads）被包裹在油滴中。理想情况下，每个液滴应只含一个细胞和一个凝胶珠。然而，由于细胞裂解或制备过程中的污染，液滴周围的溶液中会存在**环境RNA（ambient RNA）**。因此，许多液滴可能不含细胞，只包裹了少量环境RNA；另一些液滴则可能包含一个真实细胞以及混入的环境RNA。区分“细胞液滴”和“空液滴”是[数据预处理](@entry_id:197920)的关键第一步。

早期的方法通常是根据每个条形码（barcode，代表一个液滴）的总UMI计数设定一个阈值，高于阈值的被认为是细胞。但这种方法对于UMI计数较低的细胞（如某些静息状态的免疫细胞）或环境RNA浓度较高的情况表现不佳。

现代方法，如**EmptyDrops**算法，采用了一种更严谨的统计检验框架。 其核心思想是：空液滴中的基因表达谱应该反映了环境中RNA的平均组成，而含有真实细胞的液滴，其表达谱则会偏离这个背景分布。

具体来说，该方法首先从总UMI计数非常低的液滴中估计出环境RNA的基因表达谱，记为一个[概率向量](@entry_id:200434) $\mathbf{p}_{0} = (p_{01}, \dots, p_{0G})$，其中$G$是基因总数，$p_{0i}$是基因$i$在环境RNA中的相对丰度。对于任何一个待检验的液滴，其基因计数向量为 $\mathbf{x} = (x_{1}, \dots, x_{G})$，总UMI数为 $N = \sum_{i=1}^{G} x_{i}$。

该方法的**零假设（$H_0$）**是：该液滴是空的，其基因计数向量$\mathbf{x}$服从以环境RNA表达谱$\mathbf{p}_0$为概率参数的[多项分布](@entry_id:189072)，即 $\mathbf{x} \sim \mathrm{Multinomial}(N, \mathbf{p}_{0})$。**[备择假设](@entry_id:167270)（$H_1$）**是：该液滴含有一个细胞，其表达谱$\mathbf{p}$不同于$\mathbf{p}_0$。

为了检验这一假设，可以构建一个**似然比检验（Likelihood Ratio Test, LRT）**。[多项分布](@entry_id:189072)的似然函数为 $L(\mathbf{p}; \mathbf{x}) = C \prod_{i=1}^{G} p_{i}^{x_{i}}$，其中$C$是组合系数。在零假设下，[似然函数](@entry_id:141927)的最大值为 $L(\mathbf{p}_0; \mathbf{x})$。在[备择假设](@entry_id:167270)下，似然函数在最大似然估计 $\hat{\mathbf{p}} = (\frac{x_1}{N}, \dots, \frac{x_G}{N})$ 处达到最大值 $L(\hat{\mathbf{p}}; \mathbf{x})$。[似然比检验统计量](@entry_id:169778) $\Lambda$ 定义为：
$$
\Lambda = \frac{L(\mathbf{p}_0; \mathbf{x})}{L(\hat{\mathbf{p}}; \mathbf{x})} = \frac{\prod_{i=1}^{G} p_{0i}^{x_{i}}}{\prod_{i=1}^{G} (\frac{x_i}{N})^{x_i}} = \prod_{i=1}^{G} \left( \frac{N p_{0i}}{x_i} \right)^{x_i}
$$
通常使用[对数似然比](@entry_id:274622) $-2\ln(\Lambda)$ 作为检验统计量。通过蒙特卡洛模拟，可以为每个液滴计算出一个$p$-值，表示观察到当前或更偏离环境RNA谱的概率。由于需要对成千上万个液滴进行检验，必须进行[多重检验校正](@entry_id:167133)，例如使用**[Benjamini-Hochberg](@entry_id:269887)**程序将**假发现率（False Discovery Rate, FDR）**控制在预设水平（如$0.01$）。通过这种方式，EmptyDrops能够更灵敏地识别出那些UMI总数不高但表达谱独特的真实细胞，极大地提升了数据质量。

### [数据标准化](@entry_id:147200)与统计建模

得到基因-细胞计数矩阵后，下一个关键步骤是进行[数据标准化](@entry_id:147200)，以消除细胞间的技术性差异（如捕获效率、[测序深度](@entry_id:178191)），使基因表达水平具有可比性。有效的标准化依赖于对计数数据统计特性的深刻理解。

#### [单细胞RNA测序](@entry_id:142269)计数数据的统计特性

scRNA-seq的UMI计数数据具有两个显著特征：**高度稀疏性**（矩阵中含有大量的零值）和**过离散性**（方差大于均值）。

“零”的来源是复杂的。一部分是**生物学零（biological zeros）**，即某个基因在特定细胞中确实没有表达。另一部分是**技术性零（technical zeros）**，即基因虽然有表达，但由于RNA分子捕获效率低、逆转录失败或[测序深度](@entry_id:178191)不足等[随机采样](@entry_id:175193)过程，导致其未能在最终数据中被检测到。这种技术性零也被称为**“脱扣”（dropout）**。 dropout是采样不足的直接后果，尤其是在低表达基因中更为常见。

理解这一过程的[生成模型](@entry_id:177561)至关重要。我们可以认为，一个细胞$j$中基因$i$的真实分子数$C_{ij}$服从一个泊松过程。而文库制备和测序过程可看作是对这些分子的二项采样，[采样效率](@entry_id:754496)为$\eta_j$。根据泊松稀疏化定理，观测到的UMI计数$X_{ij}$将服从一个新的泊松分布，其均值为$\lambda_{ij}\eta_j$，其中$\lambda_{ij}$是真实的表达速率。

然而，仅仅泊松分布（其方差等于均值）不足以描述scRNA-seq数据。在细胞群体中，即使是同一类型的细胞，基因的真实表达速率$\lambda_{ij}$也存在**生物学异质性**（如细胞周期差异、[转录爆发](@entry_id:156205)等）。这种异质性通常可以用伽马分布（Gamma distribution）来建模。当一个泊松分布的均值参数本身是一个服从伽马分布的随机变量时，其边缘分布就是**负二项分布（Negative Binomial, NB）**。

NB分布的方差$\text{Var}(X)$与均值$\mu$的关系为 $\text{Var}(X) = \mu + \phi\mu^2$，其中$\phi$是**[离散度](@entry_id:168823)参数（dispersion parameter）**，它捕捉了超出泊松分布所预期的额外方差（即过离散性）。N[B模型](@entry_id:159413)优雅地统一了生物学异质性（通过$\phi$）和采样噪声（通过泊松-伽马混合）。在该模型下，观察到零值的概率$P(X=0)$由NB分布的均值和[离散度](@entry_id:168823)参数完全决定。一个基因的均值$\mu$很低或[离散度](@entry_id:168823)$\phi$很高时，就可以产生大量的零值。

这意味着，对于UMI数据，我们通常不需要引入一个额外的“零膨胀”（zero-inflation）组件来解释大量的零。NB分布本身已经能够很好地拟合数据中的零。只有当观测到的零频率系统性地超过N[B模型](@entry_id:159413)预测的频率时，才需要考虑**零膨胀负二项（Zero-Inflated Negative Binomial, ZINB）**模型。诊断方法包括拟合N[B模型](@entry_id:159413)后，通过后验预测检查来比较模型预测的零频率与观测频率是否一致。 此外，利用已知浓度的**ERCC spike-in**外源RNA分子，可以建立技术性零值率与表达丰度之间的经验关系，从而帮助区分一个基因的高零值率是源于低表达还是额外的生物学抑制。

#### 基于模型的标准化与方差稳定化

传统的标准化方法，如“log-transform”，通常包括对每个细胞的总UMI计数进行归一化（即除以一个尺寸因子），然后加上一个伪计数（pseudocount）并取对数。这种方法虽然简单，但存在一些问题：它不能有效处理采样噪声，并且对数变换不能对具有不同丰度的基因实现方差稳定。

现代分析流程，如**sctransform**，采用了一种基于[广义线性模型](@entry_id:171019)（GLM）的更优越方法。 该方法直接对原始UMI计数进行建模，避免了对数变换的伪计数问题。对于每个基因$g$，其在细胞$i$中的UMI计数$y_{ig}$被建模为服从泊松分布，其均值$\mu_{ig}$通过[对数连接函数](@entry_id:163146)与一个线性预测器相关联：
$$
\ln(\mu_{ig}) = \eta_{ig} = \beta_{0g} + \mathbf{x}_i^T \boldsymbol{\beta}_g + \ln(s_i)
$$
在这里，$s_i$是细胞$i$的[测序深度](@entry_id:178191)（一个已知的**偏移量(offset)**），$\beta_{0g}$是基因特异性的截距，$\mathbf{x}_i$是细胞的技术协变量（如线粒体基因比例），$\boldsymbol{\beta}_g$是其对应的系数。模型通过正则化（如岭回归）来估计参数$(\beta_{0g}, \boldsymbol{\beta}_g)$，以提高稳健性。

[模型拟合](@entry_id:265652)后，标准化的核心是计算**皮尔逊残差（Pearson residuals）**。皮尔逊残差定义为观测值与模型预测均值之差，再用模型预测的标准差进行缩放：
$$
r_{ig} = \frac{y_{ig} - \hat{\mu}_{ig}}{\sqrt{\hat{V}(\hat{\mu}_{ig})}}
$$
对于泊松模型，方差函数$V(\mu) = \mu$。因此，皮尔逊残差为：
$$
r_{ig} = \frac{y_{ig} - \hat{\mu}_{ig}}{\sqrt{\hat{\mu}_{ig}}}
$$
皮尔逊残差的一个关键性质是**方差稳定化**。在模型假设成立且拟合良好的情况下，残差$r_{ig}$的[条件方差](@entry_id:183803)近似为1，与基因的平均表达水平$\mu_{ig}$无关。我们可以证明这一点：
$$
\operatorname{Var}(r_{ig} | \mathbf{x}_i, s_i) \approx \operatorname{Var}\left( \frac{y_{ig} - \mu_{ig}}{\sqrt{\mu_{ig}}} \right) = \frac{1}{\mu_{ig}} \operatorname{Var}(y_{ig}) = \frac{\mu_{ig}}{\mu_{ig}} = 1
$$
这个性质至关重要，因为它意味着标准化后的值（即皮尔遜残差）可以直接用于下游分析（如PCA、聚类），而无需担心高表达基因因其固有高方差而主导分析结果。这些残差代表了在控制了[测序深度](@entry_id:178191)和其他技术变量后，每个基因表达的“意外”变化，从而更好地反映了真实的生物学信号。

### 特征选择与降维

一个scRNA-seq数据集通常包含数万个基因，其中大部分基因要么表达水平极低，要么在所有细胞中表达模式相似，对区分细胞状态贡献甚微。因此，必须进行特征选择，即识别出那些最能反映细胞间生物学差异的基因，并在此基础上进行降维，以便在保留关键生物学结构的同时简化数据并去除噪声。

#### 识别高可变基因以富集生物学信号

[特征选择](@entry_id:177971)的核心任务是识别**高可变基因（Highly Variable Genes, HVGs）**。HVG是指那些在细胞间的表达变化程度超出了因技术噪声所能解释的范围的基因。识别HVG的挑战在于，基因表达的方差本身与均值相关。如前所述，即使是生物学上恒定的基因，其UMI计数的方差也会随均值增加而增加（对于NB分布，$\text{Var}(Y) = \mu + \alpha\mu^2$）。

因此，简单地选择方差最高的基因会不可避免地偏向于高表达基因。我们需要一个能够[解耦](@entry_id:160890)均值和方差关系的方法。一种经典策略是，对所有基因的表达均值和方差进行建模。 对于每个基因，我们计算其在所有细胞中经过尺寸因子（size factor）归一化后的表达均值$\hat{\mu}$和方差$\hat{v}$。然后，我们可以通过[矩估计法](@entry_id:270941)得到每个基因的[离散度](@entry_id:168823)参数$\hat{\alpha}$:
$$
\hat{\alpha} = \frac{\hat{v} - \hat{\mu}}{\hat{\mu}^2}
$$
这个$\hat{\alpha}$值量化了基因表达超出泊松采样噪声的生物学变异。

为了更稳健地识别HVGs，可以对所有基因的均值-方差关系进行拟合（例如，通过局部[多项式回归](@entry_id:176102)），从而得到在给定均值水平下预期的方差。然后，将每个基因的实际方差与预期方差进行比较。那些实际方差显著高于预期方差的基因，就被定义为HVGs。

另一种方法是进行**方差稳定化变换**。我们的目标是找到一个变换$T(\mu)$，使得变换后变量的[方差近似](@entry_id:268585)为常数。根据delta方法，这样的变换满足$T'(\mu) \propto V(\mu)^{-1/2}$。对于NB分布的方差函数$V(\mu) = \mu + \alpha\mu^2$，我们需求解：
$$
T(\mu) = \int \frac{d\mu}{\sqrt{\mu + \alpha\mu^2}}
$$
这个积分的解（忽略常数）是：
$$
T(\mu) = \frac{2}{\sqrt{\alpha}} \arcsinh(\sqrt{\alpha\mu})
$$
通过应用这样的变换，或者使用前述的皮尔逊残差，我们可以将所有基因的表达值转换到一个[方差近似](@entry_id:268585)恒定的尺度上。在这个尺度上，我们可以直接通过比较方差来选择HVGs，而不会受到均值-方差依赖性的干扰。

#### [主成分分析](@entry_id:145395)及其在[信噪比](@entry_id:271196)提升中的作用

选择了HVGs（通常为1000-5000个）之后，数据维度仍然很高。**主成分分析（Principal Component Analysis, PCA）**是[scRNA-seq分析](@entry_id:266931)中最常用的线性[降维](@entry_id:142982)方法。PCA的目标是找到一组新的[正交坐标](@entry_id:166074)轴（主成分，PCs），使得数据在这些轴上的投影方差最大化。

在[scRNA-seq](@entry_id:155798)中，PCA通常应用于一个$n \times m$的矩阵$Z$，其中$n$是细胞数，$m$是HVG数量，矩阵的每一列（基因）都已经被中心化（即减去其均值）。从数学上讲，PCA通过对基因-基因样本协方差矩阵$\hat{\Sigma} = \frac{1}{n} Z^T Z$进行[特征值分解](@entry_id:272091)来找到主成分。 主成分是$\hat{\Sigma}$的特征向量，对应的特征值则表示数据在相应主成分方向上的方差。

为什么要在HVGs上进行PCA，而不是所有基因？这涉及到提升**[信噪比](@entry_id:271196)**的关键思想。我们可以构建一个简化的生成模型来理解这一点。假设数据中存在两种细胞状态，其基因表达向量$x$可以建模为$x = \mu_y + \varepsilon$，其中$\mu_y$是状态$y$的平均表达谱，$\varepsilon$是均值为0、协方差为$\sigma^2 I_m$的噪声。根据[全方差公式](@entry_id:177482)，总体协方差矩阵为：
$$
\Sigma = \sigma^2 I_m + p(1-p)\Delta\Delta^T
$$
其中$\Delta = \mu_1 - \mu_0$是两种状态间的表达差异向量，$p$是状态1的比例。这是一个“尖峰[协方差模型](@entry_id:165727)”（spiked covariance model）。其最大的特征值（信号）对应于方向$\Delta$，而其余特征值均为$\sigma^2$（噪声）。第一个PC捕获的[方差比](@entry_id:162608)例为 $f = \lambda_1 / \text{tr}(\Sigma)$。

现在，假设只有$m_s$个HVGs在两种状态间存在差异（即$\Delta$中只有$m_s$个非零项），而其余$m-m_s$个基因没有差异。
- 如果我们在**所有$m$个基因**上做PCA，总方差包含所有$m$个基因的噪声贡献，[信噪比](@entry_id:271196)被稀释。第一个PC解释的[方差比](@entry_id:162608)例 $f_{\mathrm{all}}$ 相对较小。
- 如果我们**只在$m_s$个HVGs**上做PCA，我们有效地从总方差中剔除了那$m-m_s$个纯噪声基因的方差贡献。信号 component（$\Delta$相关的方差）保持不变，但总噪声 component 减小。因此，第一个PC解释的[方差比](@entry_id:162608)例 $f_{\mathrm{HVG}}$ 会显著增大。

以一个具体例子计算，对于$m=10000$个基因，其中$m_s=500$个是HVGs，在特定参数下，可以计算出 $f_{\mathrm{HVG}} / f_{\mathrm{all}}$ 的比值可以高达约$14.97$。 这清晰地表明，通过选择HVGs进行PCA，我们将分析聚焦于一个富含生物学信号的子空间，极大地增强了主成分区分不同细胞状态的能力，为下游的聚类和[轨迹推断](@entry_id:176370)奠定了坚实的基础。

### 细胞聚类与状态鉴定

降维之后，我们通常会选择前10-50个主成分，这些PCs捕获了数据中主要的生物学变异结构。下一步是将细胞组织成有意义的群体，即**细胞聚类**，这通常对应于不同的细胞类型或状态。

#### 构建细胞-细胞相似性图谱

直接在高维PCA空间中使用传统的[聚类算法](@entry_id:146720)（如k-means）效果往往不佳，因为欧氏距离在高维空间中可能失去区分能力。现代scRNA-seq流程普遍采用一种基于图的方法。该方法首先构建一个**$k$-近邻（$k$-nearest neighbor, kNN）图**。

在这个图中，每个细胞是一个节点。对于每个细胞，我们在PCA空间中找到其$k$个最近的邻居，并与它们建立连接（边）。这创建了一个[有向图](@entry_id:272310)。通常，我们会将其**对称化**，例如，如果细胞$i$在细胞$j$的$k$个近邻中，或者细胞$j$在细胞$i$的$k$个近邻中，就在它们之间建立一条无向边。这样得到的图谱捕捉了细胞间在转录组水平上的局部相似性结构。

在构建[kNN图](@entry_id:751051)时，选择合适的[距离度量](@entry_id:636073)至关重要。虽然**欧氏距离**是默认选择，但**余弦相似度（cosine similarity）**在某些情况下更具优势。余弦相似度衡量两个向量方向上的一致性，而忽略它们的模长。在PCA空间中，向量的模长可能与细胞的某些技术或生物学特性（如细胞周期）有关。如果我们的目标是根据基因表达程序的相对模式（方向）而非绝对强度（模长）来定义细胞相似性，余弦相似度可能更鲁棒。

为了进一步增强图的鲁棒性，特别是在处理具有不同“规模”的基因程序时，可以采用**共同$k$-近邻（mutual kNN）**准则。即，只有当细胞$i$和$j$互为对方的$k$-近邻时，才在它们之间建立连接。这会产生一个更稀疏但连接更可靠的图。

一个关键的步骤是确定距离或相似度的阈值，以过滤掉不可靠的连接。例如，我们可以推导归一化欧氏距离$d_{ij}$与余弦相似度$\operatorname{cos}(i,j)$之间的关系。对于单位向量$\mathbf{u}_i$和$\mathbf{u}_j$，它们之间的欧氏距离平方为 $d_{ij}^2 = \|\mathbf{u}_i - \mathbf{u}_j\|^2 = 2 - 2(\mathbf{u}_i \cdot \mathbf{u}_j) = 2(1-\operatorname{cos}(i,j))$。我们可以利用这个关系，设置一个基于距离分布的相似度阈值。例如，计算所有细胞对之间的归一化欧氏距离，并选择其75百[分位数](@entry_id:178417)作为距离上限，然后转换为相应的余弦相似度下限$\tau = 1 - d_{75}^2/2$。

#### 基于图的[社区发现](@entry_id:143791)算法

在构建了细胞-细胞相似性图之后，细胞聚类问题就转化为一个网络科学中的**[社区发现](@entry_id:143791)（community detection）**问题。社区是指网络中连接紧密、而与网络其余部分连接稀疏的节点[子图](@entry_id:273342)。在我们的场景中，一个社区就对应一个细胞簇。

**[Louvain算法](@entry_id:270022)**和**[Leiden算法](@entry_id:751237)**是两种广泛应用于[scRNA-seq分析](@entry_id:266931)的[启发式](@entry_id:261307)[社区发现](@entry_id:143791)算法。它们通过优化一个**[质量函数](@entry_id:158970)（quality function）**来迭代地将节点分配到社区中。

一个经典的[质量函数](@entry_id:158970)是**模块度（modularity）**。模块度的思想是比较图中社区内部实际存在的边数与一个**[零模型](@entry_id:181842)（null model）**下期望的边数之间的差异。一个好的社区划分应该有远多于随机期望的内部边。Reichardt-Bornholdt广义模块度定义为：
$$
Q_{\mathrm{RB}}(\gamma) = \sum_{c} \left( e_c - \gamma P_c \right)
$$
其中，$e_c$是社区$c$内部的边数，$P_c$是在[零模型](@entry_id:181842)下社区$c$内部的期望边数，$\gamma$是**分辨率参数（resolution parameter）**。

另一个[质量函数](@entry_id:158970)是**恒定波兹模型（Constant Potts Model, CPM）**，其形式为：
$$
H(\gamma) = \sum_{c} \left( e_c - \gamma |E_c| \right)
$$
在这里，$|E_c|$是社区$c$内部可能的最大边数，对于一个大小为$n_c$的社区，它等于$\frac{n_c(n_c-1)}{2}$。

分辨率参数$\gamma$在这些算法中扮演着至关重要的角色，它直接控制着聚类的粒度。
- **增大$\gamma$** 会增加对社区内部连接的惩罚（或说，提高对社区“纯度”的要求），从而倾向于将大的社区拆分成更小的、连接更紧密的子社区。
- **减小$\gamma$** 则会放松这一标准，允许将连接相对稀疏的社区合并成一个大的社区。

我们可以通过分析两个社区合并或分离的临界分辨率$\gamma^*$来理解这一点。假设有两个社区，大小分别为$n_1$和$n_2$，它们之间的边连接概率为$p_{out}$。
- 对于CPM，临界分辨率恰好等于$p_{out}$，即$\gamma_{\mathrm{CPM}}^{\ast} = p_{\mathrm{out}}$。
- 对于基于[构型模型](@entry_id:747676)（configuration model）的RB模块度，在[kNN图](@entry_id:751051)（每个节点度数近似为$k$）的近似下，临界分辨率为$\gamma_{\mathrm{RB}}^{\ast} = \frac{N p_{\mathrm{out}}}{k}$，其中$N$是总细胞数。

这两个公式揭示了，对于同一个图，两种[质量函数](@entry_id:158970)的分辨[率参数](@entry_id:265473)尺度是截然不同的。它们的比值为$\gamma_{\mathrm{RB}}^{\ast} / \gamma_{\mathrm{CPM}}^{\ast} = N/k$。例如，在一个$N=2000, k=20$的实验中，这个比值高达$100$。这意味着，要达到相似的聚类效果，R[B模](@entry_id:161767)块度需要的分辨率参数值远大于CPM。在实践中，研究者需要根据具体的生物学问题和[数据结构](@entry_id:262134)，探索不同的分辨率参数，以获得最有意义的细胞分群。[Leiden算法](@entry_id:751237)相比[Louvain算法](@entry_id:270022)的一个改进是它能保证产生的社区是完全连接的，并且在优化[质量函数](@entry_id:158970)方面通常更快、效果更好。

### 实验设计与混杂因素处理

[scRNA-seq分析](@entry_id:266931)流程的成功不仅依赖于精巧的计算方法，更取决于严谨的实验设计和对混杂因素的有效处理。

#### 实验方案的选择及其对分析的影响

在启动一个scRNA-seq项目之前，最基本的设计决策之一是选择合适的文库制备方案。不同的方案在**通量（throughput）**、**灵敏度（sensitivity）**和**基因体覆盖度（gene body coverage）**之间存在重要的权衡。

- **基于液滴的3' UMI方案（如10x Genomics Chromium）**：这类技术通量极高，一次实验可以处理数万甚至数十万个细胞。它们通过UMI进行数字化计数，有效校正PCR偏好。然而，其主要的局限性在于**3'末端偏好**，即测序读段主要富集在转录本的3'端（靠近polyA尾）。这种偏好使得它们非常适合进行基因表达丰度的定量，但几乎无法提供关于转录本内部结构的信息。

- **基于平板的全长方案（如Smart-seq2）**：这类技术通量较低，通常一次只能处理数百个细胞。它们致力于捕获和扩增完整的cDNA分子，从而实现对整个转录本的覆盖。虽然标准Smart-seq2协议不包含UMIs，使其对PCR偏好更敏感，但其**全长覆盖**的特性是独一无二的优势。

选择哪种方案取决于核心的生物学问题。例如，在一个精准肿瘤学研究中，如果目标是鉴定一个罕见癌细胞亚群（如占0.5%）中一个具有临床意义的**选择性剪接事件**（alternative splicing event），如MET基因的外显子14跳跃。
- 使用**10x Genomics**，尽管其高通量可以捕获大量细胞（例如，50000个细胞中可捕获250个癌细胞），但由于其3'偏好，几乎不可能产生跨越内部剪接位点的读段。因此，尽管捕获了目标细胞，却无法回答关于[选择性剪接](@entry_id:142813)的问题。
- 使用**Smart-seq2**，虽然其低通量可能只能捕获极少数癌细胞（例如，1000个细胞中仅捕获5个），但由于其全长覆盖，它有很大机会产生能够跨越MET基因外显子13和15之间连接点的读段，从而直接检测到外显子14的跳跃。其更高的[单细胞测序](@entry_id:198847)深度也增加了在少数细胞中检测到该事件的概率。

这个例子鲜明地说明了，任何分析流程都受限于输入数据的内在属性。对于需要解析转录本异构体（isoform）的问题，全长测序方案是不可替代的选择，即使这意味着牺牲细胞通量。

#### 批次效应的量化与诊断

在大型临床研究中，scRNA-seq样本通常需要在不同时间、由不同操作员、使用不同批次的试剂来处理。这些技术性差异会引入系统性的、非生物学的变异，即**批次效应（batch effects）**。批次效应可能非常强大，甚至会盖过微弱的生物学信号，导致错误的结论，例如将来自不同批次的同类型细胞错误地聚类成不同的细胞类型。

识别和量化[批次效应](@entry_id:265859)是保证[scRNA-seq分析](@entry_id:266931)可靠性的关键。**线性混合模型（Linear Mixed Models, LMMs）**提供了一个强大的框架来分解基因表达变异的来源。 我们可以为单个基因的log标准化表达值$y$构建一个模型，将观察到的变异归因于不同的生物学和技术因素：
$$
y = \mu + b_{p} + b_{c} + b_{d} + b_{o} + \varepsilon
$$
这里，除了固定效应$\mu$，模型还包括多个独立的随机效应：$b_p$代表患者间生物学差异（其方差为$\sigma_p^2$），而$b_c$（文库化学）、$b_d$（实验日期）、$b_o$（操作员）代表不同的批次因素（方差分别为$\sigma_c^2, \sigma_d^2, \sigma_o^2$）。$\varepsilon$是剩余的细胞间随机噪声（方差为$\sigma_\varepsilon^2$）。

通过拟合这个模型（例如使用限制性[最大似然](@entry_id:146147)法, REML），我们可以估计出每个方差组分。总方差可以分解为：
$$
\text{Var}(y) = \sigma_{p}^{2} + (\sigma_{c}^{2} + \sigma_{d}^{2} + \sigma_{o}^{2}) + \sigma_{\varepsilon}^{2} = \sigma_{\text{biology}}^{2} + \sigma_{\text{batch}}^{2} + \sigma_{\text{residual}}^{2}
$$
这个[方差分解](@entry_id:272134)提供了一种直接量化[批次效应](@entry_id:265859)强度的方法。例如，我们可以定义一个“[批次效应](@entry_id:265859)占比”指标$\rho = \frac{\sigma_{\text{batch}}^{2}}{\sigma_{\text{biology}}^{2} + \sigma_{\text{batch}}^{2}}$，它衡量了在系统性变异中，有多少比例是由技术批次造成的。如果对于许多基因，这个比例都很高，则表明实验存在严重的[批次效应](@entry_id:265859)，需要进行校正。

除了基于LMM的[方差分解](@entry_id:272134)，还有其他几种原则性的诊断方法：
- **主成分回归**：将PCA得到的主要PCs对生物学和批次变量进行回归。如果批次变量显著解释了顶层PCs的方差，说明[批次效应](@entry_id:265859)主导了数据的主要结构。
- **[kNN图](@entry_id:751051)纯度分析**：在[kNN图](@entry_id:751051)中，检查每个细胞的邻居。如果邻居主要来自同一批次而非同一生物学分组，则表明批次效应扭曲了细胞间的相似性度量。
- **[轮廓系数](@entry_id:754846)分析**：分别以生物学分组和批次分组作为“簇”来计算[轮廓系数](@entry_id:754846)。理想情况下，基于生物学分组的[轮廓系数](@entry_id:754846)应高，而基于批次分组的应接近于零。

通过这些定量的诊断方法，我们可以在分析流程的早期发现并评估批次效应的影响，从而指导后续是否需要以及如何选择合适的批次校正算法，确保最终的生物学结论是稳健和可信的。