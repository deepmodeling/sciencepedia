{
    "hands_on_practices": [
        {
            "introduction": "高质量的计算分析根植于高质量的实验数据。本练习模拟了实验方法开发中的一个关键步骤：优化实验参数（例如酶透化处理时间），以在信号产量和空间完整性之间取得平衡。通过这个实践，您将学会如何权衡利弊并做出数据驱动的决策，这在转化研究中是一项至关重要的技能。",
            "id": "4385508",
            "problem": "一个实验室正在为一个在具有固定直径圆形捕获点的平台上进行的空间分辨转录组学 (Spatially Resolved Transcriptomics, SRT) 检测优化酶透化处理。对于每个测试的透化时间，实验室测量了来自三个独立区域的每个捕获点的观测 Unique Molecular Identifier (UMI) 计数、估计的环境分数以及空间模糊半径。假设以下基本原则：\n- 每个捕获点的观测计数是来自点上 (on-spot) 分子和来自环境 (off-spot) 分子的计数的总和。\n- 环境分数，记为 $f_{A}$，定义为源自环境分子的观测计数的比例；因此，源自点上分子的观测计数的比例是 $1 - f_{A}$。\n- 空间模糊半径，记为 $L$，是释放的核糖核酸 (ribonucleic acid, RNA) 在被捕获前均方根位移的经验估计值。为限制捕获点之间的交叉污染，实验室施加了约束条件 $L \\le R/2$，其中 $R$ 是捕获点半径。\n- 对于给定的时间，重复实验是独立的，且具有相同的 $f_{A}$。使用所有重复实验的算术平均值来估计该时间的预期每个捕获点的观测计数。\n\n该平台的捕获点直径为 $55~\\mu\\mathrm{m}$，因此捕获点半径为 $R = 27.5~\\mu\\mathrm{m}$，模糊约束为 $L \\le R/2 = 13.75~\\mu\\mathrm{m}$。\n\n评估了三个透化时间：$5~\\mathrm{min}$、$10~\\mathrm{min}$ 和 $20~\\mathrm{min}$。对于每个时间，测量了以下数据：\n- 时间 $5~\\mathrm{min}$：\n  - 重复实验的每个捕获点观测 UMI 计数：$2{,}100$、$2{,}050$、$2{,}200$。\n  - 环境分数：$f_{A} = 0.08$。\n  - 空间模糊半径：$L = 7.5~\\mu\\mathrm{m}$。\n- 时间 $10~\\mathrm{min}$：\n  - 重复实验的每个捕获点观测 UMI 计数：$3{,}400$、$3{,}600$、$3{,}550$。\n  - 环境分数：$f_{A} = 0.12$。\n  - 空间模糊半径：$L = 12.0~\\mu\\mathrm{m}$。\n- 时间 $20~\\mathrm{min}$：\n  - 重复实验的每个捕获点观测 UMI 计数：$5{,}600$、$5{,}650$、$5{,}400$。\n  - 环境分数：$f_{A} = 0.20$。\n  - 空间模糊半径：$L = 21.0~\\mu\\mathrm{m}$。\n\n仅使用上述定义，对每个时间，通过使用环境分数校正平均每个捕获点的观测 UMI 计数，来估计预期的点上每个捕获点的 UMI 产出，然后强制执行模糊约束 $L \\le 13.75~\\mu\\mathrm{m}$。在满足约束条件的时间中，选择能最大化预期点上每个捕获点 UMI 产出的透化时间。\n\n以整数形式报告最佳透化时间（单位为分钟）。在最终的方框答案中不要包含单位。",
            "solution": "目标是从一组测试时间（$5~\\mathrm{min}$、$10~\\mathrm{min}$、$20~\\mathrm{min}$）中确定最佳的酶透化时间，该时间在满足空间模糊约束的条件下，能够最大化预期的点上 Unique Molecular Identifier (UMI) 产出。\n\n首先，我们将涉及的量形式化。令 $\\bar{C}_{obs}$ 为预期的每个捕获点观测 UMI 计数，它通过重复实验计数的算术平均值来估计。令 $f_A$ 为环境分数。问题陈述，预期的点上每个捕获点的 UMI 产出（我们记为 $C_{on-spot}$）可以通过校正观测计数中的环境分数来计算。点上计数的比例是 $(1 - f_A)$。因此，关系式为：\n$$C_{on-spot} = \\bar{C}_{obs} \\times (1 - f_A)$$\n\n实验室对空间模糊半径 $L$ 施加了一个约束，以限制捕获点之间的交叉污染。捕获点半径是直径的一半，所以 $R = \\frac{55~\\mu\\mathrm{m}}{2} = 27.5~\\mu\\mathrm{m}$。约束条件是 $L \\le R/2$，计算结果为：\n$$L \\le \\frac{27.5~\\mu\\mathrm{m}}{2} = 13.75~\\mu\\mathrm{m}$$\n任何导致模糊半径违反此约束的透化时间都必须被舍弃。\n\n我们现在将评估三个透化时间中的每一个。\n\n**1. 评估时间 $t_1 = 5~\\mathrm{min}$**\n提供的数据如下：\n- 重复实验的每个捕获点观测 UMI 计数：$2100$、$2050$、$2200$。\n- 环境分数：$f_{A,1} = 0.08$。\n- 空间模糊半径：$L_1 = 7.5~\\mu\\mathrm{m}$。\n\n首先，我们计算平均观测 UMI 计数 $\\bar{C}_{obs,1}$：\n$$\\bar{C}_{obs,1} = \\frac{2100 + 2050 + 2200}{3} = \\frac{6350}{3}$$\n接下来，我们计算预期的点上 UMI 产出 $C_{on-spot,1}$：\n$$C_{on-spot,1} = \\bar{C}_{obs,1} \\times (1 - f_{A,1}) = \\frac{6350}{3} \\times (1 - 0.08) = \\frac{6350}{3} \\times 0.92 = \\frac{5842}{3} \\approx 1947.33$$\n最后，我们检查模糊约束：\n$$L_1 = 7.5~\\mu\\mathrm{m}$$\n由于 $7.5 \\le 13.75$，约束条件得到满足。时间 $t_1 = 5~\\mathrm{min}$ 是一个有效的候选者。\n\n**2. 评估时间 $t_2 = 10~\\mathrm{min}$**\n提供的数据如下：\n- 重复实验的每个捕获点观测 UMI 计数：$3400$、$3600$、$3550$。\n- 环境分数：$f_{A,2} = 0.12$。\n- 空间模糊半径：$L_2 = 12.0~\\mu\\mathrm{m}$。\n\n首先，我们计算平均观测 UMI 计数 $\\bar{C}_{obs,2}$：\n$$\\bar{C}_{obs,2} = \\frac{3400 + 3600 + 3550}{3} = \\frac{10550}{3}$$\n接下来，我们计算预期的点上 UMI 产出 $C_{on-spot,2}$：\n$$C_{on-spot,2} = \\bar{C}_{obs,2} \\times (1 - f_{A,2}) = \\frac{10550}{3} \\times (1 - 0.12) = \\frac{10550}{3} \\times 0.88 = \\frac{9284}{3} \\approx 3094.67$$\n最后，我们检查模糊约束：\n$$L_2 = 12.0~\\mu\\mathrm{m}$$\n由于 $12.0 \\le 13.75$，约束条件得到满足。时间 $t_2 = 10~\\mathrm{min}$ 也是一个有效的候选者。\n\n**3. 评估时间 $t_3 = 20~\\mathrm{min}$**\n提供的数据如下：\n- 重复实验的每个捕获点观测 UMI 计数：$5600$、$5650$、$5400$。\n- 环境分数：$f_{A,3} = 0.20$。\n- 空间模糊半径：$L_3 = 21.0~\\mu\\mathrm{m}$。\n\n首先，我们计算平均观测 UMI 计数 $\\bar{C}_{obs,3}$：\n$$\\bar{C}_{obs,3} = \\frac{5600 + 5650 + 5400}{3} = \\frac{16650}{3} = 5550$$\n接下来，我们计算预期的点上 UMI 产出 $C_{on-spot,3}$：\n$$C_{on-spot,3} = \\bar{C}_{obs,3} \\times (1 - f_{A,3}) = 5550 \\times (1 - 0.20) = 5550 \\times 0.80 = 4440$$\n最后，我们检查模糊约束：\n$$L_3 = 21.0~\\mu\\mathrm{m}$$\n由于 $21.0 > 13.75$，约束条件 $L_3 \\le 13.75~\\mu\\mathrm{m}$ 被违反。因此，时间 $t_3 = 20~\\mathrm{min}$ 不是一个有效的候选者，尽管其点上 UMI 产出最高，我们也不在优化中考虑它。\n\n**结论**\n我们剩下两个有效的透化时间：$5~\\mathrm{min}$ 和 $10~\\mathrm{min}$。为了选择最佳时间，我们比较它们预期的点上 UMI 产出：\n$$C_{on-spot,1} = \\frac{5842}{3} \\approx 1947.33$$\n$$C_{on-spot,2} = \\frac{9284}{3} \\approx 3094.67$$\n\n比较这两个值，我们看到 $C_{on-spot,2} > C_{on-spot,1}$，因为 $\\frac{9284}{3} > \\frac{5842}{3}$。\n因此，在满足模糊约束的时间中，$10~\\mathrm{min}$ 的透化时间最大化了预期的点上 UMI 产出。\n\n最佳透化时间是 $10$ 分钟。",
            "answer": "$$\\boxed{10}$$"
        },
        {
            "introduction": "获得原始计数数据后，下一个关键步骤是标准化，以校正不同空间位置（spot）之间测序深度不一等技术性偏差。本练习将引导您实现三种核心的标准化方法，从简单的比例缩放到复杂的基于模型的转换。通过比较不同方法对基因表达方差的影响，您将深入理解为何选择合适的标准化策略对于准确识别生物学相关的可变基因至关重要。",
            "id": "4385438",
            "problem": "您将获得代表空间转录组学（Spatial Transcriptomics）测量的、按点计数的总数和基因-点计数矩阵。假设有 $G$ 个基因，索引为 $g \\in \\{1,\\dots,G\\}$，以及 $S$ 个空间点，索引为 $s \\in \\{1,\\dots,S\\}$。基因 $g$ 在点 $s$ 处的观测整数计数表示为 $x_{gs}$，点 $s$ 的总计数（文库大小）表示为 $c_s$。您必须计算三种归一化方法，并比较每种归一化方法下各个基因的方差。\n\n基本原理和定义：\n- 在基于计数的转录组学中，观测到的计数是由潜在丰度和抽样深度驱动的非负整数。每个点的总计数 $c_s$ 充当一个暴露标量。一个广泛接受的计数数据随机模型是负二项（NB）分布，其中 $x_{gs}$ 以均值 $\\mu_{gs}$ 和一个过度离散参数 $\\theta_g$ 进行建模，使得方差为 $\\operatorname{Var}(x_{gs}) = \\mu_{gs} + \\mu_{gs}^2 / \\theta_g$。在具有对数连接函数的广义线性模型（GLM）中，偏移量（offset）的公式设定 $\\log(\\mu_{gs}) = \\beta_{0g} + \\log(c_s)$，这意味着对于基因特异性速率 $p_g = e^{\\beta_{0g}}$，有 $\\mu_{gs} = c_s \\cdot p_g$。\n- 每百万计数（Counts Per Million, CPM）归一化将计数重新缩放到百万分之几的基准上，以调整文库大小。具体来说，对于每个点 $s$，基因 $g$ 的 CPM 归一化值为 $n_{gs}^{\\mathrm{CPM}} = 10^6 \\cdot x_{gs} / c_s$（如果 $c_s > 0$），以及 $n_{gs}^{\\mathrm{CPM}} = 0$（如果 $c_s = 0$）。\n- 带有伪计数的自然对数转换，记为“log1p”，定义为 $y_{gs}^{\\log 1\\mathrm{p}} = \\log\\left(1 + n_{gs}^{\\mathrm{CPM}}\\right)$。\n- 单细胞变换（sctransform）残差源自从一个将文库大小作为偏移量的 NB-GLM 模型。当 $\\mu_{gs} = c_s \\cdot p_g$ 且过度离散参数为 $\\theta_g$ 时，皮尔逊残差（Pearson residuals）定义为\n$$\nr_{gs} = \n\\begin{cases}\n\\dfrac{x_{gs} - \\mu_{gs}}{\\sqrt{\\mu_{gs} + \\mu_{gs}^2/\\theta_g}},  \\text{if } \\mu_{gs}  0,\\\\\n0,  \\text{if } \\mu_{gs} = 0.\n\\end{cases}\n$$\n为了确定 $\\theta_g$，使用一个经过充分检验的事实：在一个正确指定的NB模型下，皮尔逊残差的平方的期望值约等于1。求解 $\\theta_g > 0$ 使得\n$$\n\\frac{1}{|\\mathcal{I}_g|}\\sum_{s \\in \\mathcal{I}_g} \\frac{\\left(x_{gs} - \\mu_{gs}\\right)^2}{\\mu_{gs} + \\mu_{gs}^2/\\theta_g} = 1,\n$$\n其中 $\\mathcal{I}_g = \\{ s \\,|\\, \\mu_{gs} > 0 \\}$。如果方程左侧的值即使在 $\\theta_g \\to \\infty$（泊松极限）的情况下也小于 1，从而导致无正解，则将 $\\theta_g$ 设为一个大值（例如，$\\theta_g = 10^8$）。如果任何点 $s$ 的 $c_s = 0$，则为该点定义 $n_{gs}^{\\mathrm{CPM}} = 0$ 和 $\\mu_{gs} = 0$。\n\n任务：\n1. 实现CPM归一化，计算所有基因和点的 $n_{gs}^{\\mathrm{CPM}}$。\n2. 对CPM值实现log1p转换，计算所有基因和点的 $y_{gs}^{\\log 1\\mathrm{p}}$。\n3. 实现一个带偏移量的NB-GLM，以计算sctransform皮尔逊残差 $r_{gs}$：\n   - 在偏移量模型下，通过最大似然估计 $p_g$ 为 $p_g = \\left(\\sum_{s=1}^{S} x_{gs}\\right)\\Big/\\left(\\sum_{s : c_s > 0} c_s\\right)$。\n   - 为所有点设置 $\\mu_{gs} = c_s \\cdot p_g$。\n   - 使用稳健的一维求根方法，在 $\\theta_g \\in (0, +\\infty)$ 上求解上述方程以得到 $\\theta_g$（例如，在一个足够大的区间上使用二分法）。\n   - 计算 $r_{gs}$。\n4. 对于每种归一化，计算每个基因在所有点上的群体方差。对于任何向量 $z_{g\\cdot} = (z_{g1},\\dots,z_{gS})$，群体方差定义为\n$$\n\\operatorname{Var}_{\\text{pop}}(z_{g\\cdot}) = \\frac{1}{S} \\sum_{s=1}^{S} \\left(z_{gs} - \\frac{1}{S}\\sum_{t=1}^{S} z_{gt}\\right)^2.\n$$\n\n角度单位不适用。物理单位是计数；除了定义的CPM之外，不需要进行单位转换。\n\n测试套件：\n- 测试用例 1:\n  - $S = 5$, $G = 4$,\n  - $c = [5000, 7000, 6000, 8000, 5500]$,\n  - $X = \\begin{bmatrix}\n  50   70   60   80   55 \\\\\n  200   150   180   220   170 \\\\\n  0   5   10   15   0 \\\\\n  1000   1400   1100   1600   1200\n  \\end{bmatrix}$。\n- 测试用例 2:\n  - $S = 3$, $G = 3$,\n  - $c = [0, 3000, 4000]$,\n  - $X = \\begin{bmatrix}\n  0   30   40 \\\\\n  0   300   400 \\\\\n  0   0   10\n  \\end{bmatrix}$。\n- 测试用例 3:\n  - $S = 2$, $G = 5$,\n  - $c = [10000, 15000]$,\n  - $X = \\begin{bmatrix}\n  0   0 \\\\\n  4000   6000 \\\\\n  100   300 \\\\\n  5000   7000 \\\\\n  10   10\n  \\end{bmatrix}$。\n\n输出规格：\n- 对于每个测试用例，生成一个包含三个列表的列表：\n  1. $n_{gs}^{\\mathrm{CPM}}$ 在所有点上的每个基因的群体方差列表（每个基因一个浮点数）。\n  2. $y_{gs}^{\\log 1\\mathrm{p}}$ 在所有点上的每个基因的群体方差列表（每个基因一个浮点数）。\n  3. $r_{gs}$ 在所有点上的每个基因的群体方差列表（每个基因一个浮点数）。\n- 您的程序应生成单行输出，其中包含所有测试用例的结果，格式为一个用方括号括起来的逗号分隔列表，每个浮点数四舍五入到六位小数。\n- 最终格式必须是： “[[list_for_test_case_1],[list_for_test_case_2],[list_for_test_case_3]]”，其中每个 list_for_test_case_k 本身是 “[[var_CPM_gene_1,...,var_CPM_gene_G],[var_log1p_gene_1,...,var_log1p_gene_G],[var_sctransform_gene_1,...,var_sctransform_gene_G]]”。",
            "solution": "该问题要求为空间转录组学计数数据实现并比较三种不同的归一化方法。这些方法是每百万计数（CPM）、CPM值的对数转换（log1p）以及来自负二项广义线性模型（sctransform）的皮尔逊残差。对于每种方法，都需要计算每个基因在所有空间点上的群体方差。\n\n该问题已经过验证，被认为是科学上合理的、定义明确的且客观的。它基于单细胞和空间转录组学中已确立的统计和计算方法。所有必需的数据、模型和程序都已明确定义，从而可以得到唯一且可验证的解。\n\n解决方案将通过按规定实现每个归一化步骤，然后计算每个基因的群体方差来制定。\n\n设给定数据为基因-点计数矩阵 $X = \\{x_{gs}\\}$（其中 $g \\in \\{1,\\dots,G\\}$ 个基因，$s \\in \\{1,\\dots,S\\}$ 个点），以及总点计数（文库大小）向量 $c = \\{c_s\\}$。\n\n**1. 归一化方法**\n\n**a. 每百万计数（CPM）归一化**\n\n此方法通过每个点的总计数 $c_s$ 对原始计数 $x_{gs}$ 进行归一化，并将结果乘以一百万。这可以调整不同点之间的测序深度差异。公式为：\n$$\nn_{gs}^{\\mathrm{CPM}} = \n\\begin{cases}\n\\dfrac{x_{gs}}{c_s} \\times 10^6,  \\text{if } c_s > 0, \\\\\n0,  \\text{if } c_s = 0.\n\\end{cases}\n$$\n结果矩阵 $N^{\\mathrm{CPM}} = \\{n_{gs}^{\\mathrm{CPM}}\\}$ 表示的计数，就如同每个点的总计数都为一百万一样。\n\n**b. 对数转换 (log1p)**\n\n基因组学中的计数数据通常是高度偏斜的。应用对数转换有助于稳定方差，使数据分布更对称，这对于许多下游统计分析是有益的。在取自然对数之前加上一个为1的伪计数，以避免处理零计数值时出现问题（即 $\\log(0)$）。应用于CPM值的公式是：\n$$\ny_{gs}^{\\log 1\\mathrm{p}} = \\log(1 + n_{gs}^{\\mathrm{CPM}})\n$$\n结果矩阵为 $Y^{\\log 1\\mathrm{p}} = \\{y_{gs}^{\\log 1\\mathrm{p}}\\}$。\n\n**c. Sctransform 皮尔逊残差**\n\n此方法基于一个更复杂的计数数据统计模型。它使用负二项（NB）分布对计数 $x_{gs}$ 进行建模，并将文库大小 $c_s$ 作为暴露项。均值 $\\mu_{gs}$ 被建模为与文库大小成正比：$\\mu_{gs} = p_g c_s$，其中 $p_g$ 是一个基因特异性的相对丰度参数。NB分布的方差由 $\\operatorname{Var}(x_{gs}) = \\mu_{gs} + \\mu_{gs}^2 / \\theta_g$ 给出，其中 $\\theta_g$ 是基因特异性的过度离散参数。目标是计算皮尔逊残差，这是一种标准化值，在模型完美拟合的情况下，其均值应为0，方差应为1。\n\n对于每个基因 $g$，其步骤如下：\n\n**I. 估计速率参数 $p_g$：**\n参数 $p_g$ 使用最大似然估计，对于此模型公式，它简化为总基因计数与总有效文库大小的比率：\n$$\np_g = \\frac{\\sum_{s=1}^{S} x_{gs}}{\\sum_{s: c_s > 0} c_s}\n$$\n如果 $\\sum x_{gs} = 0$，则 $p_g=0$，所有期望值 $\\mu_{gs}$ 均为0，所有残差均为0，方差也为0。\n\n**II. 估计过度离散参数 $\\theta_g$：**\n参数 $\\theta_g$ 控制方差超过均值的程度。它通过求解以下方程来估计，该方程将平均皮尔逊残差平方设定为其期望值1：\n$$\n\\frac{1}{|\\mathcal{I}_g|}\\sum_{s \\in \\mathcal{I}_g} \\frac{\\left(x_{gs} - \\mu_{gs}\\right)^2}{\\mu_{gs} + \\mu_{gs}^2/\\theta_g} = 1\n$$\n其中 $\\mathcal{I}_g = \\{ s \\,|\\, \\mu_{gs} > 0 \\}$。该方程使用数值求根算法求解 $\\theta_g > 0$。方程左侧是 $\\theta_g$ 的单调递增函数。我们首先在极限 $\\theta_g \\to \\infty$（泊松情况）下评估该函数：\n$$\nL_{\\infty} = \\frac{1}{|\\mathcal{I}_g|}\\sum_{s \\in \\mathcal{I}_g} \\frac{\\left(x_{gs} - \\mu_{gs}\\right)^2}{\\mu_{gs}}\n$$\n如果 $L_{\\infty} \\le 1$，则数据相对于泊松模型是欠离散的，不存在 $\\theta_g$ 的正有限解。在这种情况下，按规定，我们将 $\\theta_g$ 设为一个大值，$\\theta_g = 10^8$。否则，存在唯一的根 $\\theta_g$，可以使用一维求解器（如 Brent 方法）在合适的区间（例如 $(0, 10^{12}]$）上找到它。\n\n**III. 计算皮尔逊残差 $r_{gs}$：**\n利用 $p_g$（以及由此得出的 $\\mu_{gs}$）和 $\\theta_g$ 的估计值，皮尔逊残差计算如下：\n$$\nr_{gs} = \n\\begin{cases}\n\\dfrac{x_{gs} - \\mu_{gs}}{\\sqrt{\\mu_{gs} + \\mu_{gs}^2/\\theta_g}},  \\text{if } \\mu_{gs} > 0, \\\\\n0,  \\text{if } \\mu_{gs} = 0.\n\\end{cases}\n$$\n残差矩阵为 $R = \\{r_{gs}\\}$。\n\n**2. 方差计算**\n\n对于每个基因 $g$ 和三个结果数据矩阵（$N^{\\mathrm{CPM}}$、$Y^{\\log 1\\mathrm{p}}$ 和 $R$）中的每一个，计算在 $S$ 个点上的群体方差。对于基因 $g$ 的转换值的一般向量 $z_{g\\cdot} = (z_{g1}, \\dots, z_{gS})$，群体方差为：\n$$\n\\operatorname{Var}_{\\text{pop}}(z_{g\\cdot}) = \\frac{1}{S} \\sum_{s=1}^{S} \\left(z_{gs} - \\bar{z}_g\\right)^2, \\quad \\text{where } \\bar{z}_g = \\frac{1}{S}\\sum_{t=1}^{S} z_{gt}\n$$\n将对每个基因执行此计算，每个测试用例产生三个方差列表，对应于三种归一化方法。\n\n该实现将处理每个测试用例，将这些步骤应用于每个基因，并根据指定的输出格式格式化结果方差列表。",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import brentq\nimport collections\n\n# The problem specifies numpy and scipy as permitted libraries.\n# The phrase \"No other libraries outside the Python standard library are permitted\"\n# is interpreted as excluding libraries *not* on the supplied list.\n\ndef sctransform_gene(x_g, c):\n    \"\"\"\n    Computes sctransform residuals and their variance for a single gene.\n    \n    Args:\n        x_g (np.ndarray): Count vector for one gene (shape S).\n        c (np.ndarray): Library size vector (shape S).\n\n    Returns:\n        float: The population variance of the sctransform residuals.\n    \"\"\"\n    S = len(c)\n    \n    # If gene is not expressed at all, residuals are all zero, variance is zero.\n    if np.sum(x_g) == 0:\n        return 0.0\n\n    # Identify effective spots (where library size > 0)\n    c_mask = c > 0\n    c_eff = c[c_mask]\n    \n    # If no spots have counts, variance is zero.\n    if len(c_eff) == 0:\n        return 0.0\n        \n    x_eff = x_g[c_mask]\n    sum_c_eff = np.sum(c_eff)\n\n    # Estimate gene-specific rate parameter p_g\n    p_g = np.sum(x_g) / sum_c_eff\n\n    # Compute expected counts mu_gs\n    mu = c * p_g\n\n    # Filter for spots where mu > 0 for theta estimation (I_g)\n    mu_eff = mu[c_mask]\n    \n    # Numerically stabilize division by mu_eff in case of underflow\n    mu_eff_reg = mu_eff + 1e-12\n    \n    # Check for under-dispersion by evaluating the limit as theta -> infinity\n    # which corresponds to the Poisson model.\n    limit_term = np.sum((x_eff - mu_eff)**2 / mu_eff_reg)\n    \n    # Average squared Pearson residual for the Poisson model\n    limit_lhs = limit_term / len(c_eff)\n\n    if limit_lhs = 1:\n        # Under-dispersed case: no solution for theta > 0. Set to a large value.\n        theta_g = 1e8\n    else:\n        # Over-dispersed case: find theta using root-finding.\n        def objective(theta):\n            # Objective function to find the root for theta\n            # Target: average squared Pearson residual = 1\n            if theta = 0: return -np.inf\n            var_nb = mu_eff + mu_eff**2 / theta\n            # Prevent division by zero if var_nb is somehow zero\n            var_nb[var_nb = 0] = 1e-12 \n            sum_sq_pearson = np.sum((x_eff - mu_eff)**2 / var_nb)\n            return sum_sq_pearson / len(c_eff) - 1\n\n        try:\n            # Brent's method is a robust and efficient root-finding algorithm.\n            # A large interval for theta is used.\n            theta_g = brentq(objective, a=1e-8, b=1e12, xtol=1e-6, rtol=1e-6)\n        except ValueError:\n            # If brentq fails (e.g., endpoints have same sign unexpectedly),\n            # fall back to the safe high-theta value.\n            theta_g = 1e8\n\n    # Compute Pearson residuals\n    residuals = np.zeros(S, dtype=float)\n    mu_pos_mask = mu > 0\n    \n    if np.any(mu_pos_mask):\n        var_nb = mu[mu_pos_mask] + mu[mu_pos_mask]**2 / theta_g\n        # Prevent sqrt of zero or negative values due to floating point inaccuracies\n        var_nb[var_nb = 0] = 1e-12\n        residuals[mu_pos_mask] = (x_g[mu_pos_mask] - mu[mu_pos_mask]) / np.sqrt(var_nb)\n\n    return np.var(residuals)\n\ndef process_case(X, c):\n    \"\"\"\n    Processes one test case: computes variances for all three normalizations.\n    \n    Args:\n        X (list of lists): GxS matrix of gene counts.\n        c (list): Vector of S library sizes.\n\n    Returns:\n        tuple: Three lists of floats (cpm_vars, log1p_vars, sct_vars).\n    \"\"\"\n    X = np.array(X, dtype=float)\n    c = np.array(c, dtype=float)\n    G, S = X.shape\n\n    # --- CPM and log1p Normalizations ---\n    # Create a regularized c vector to avoid division by zero, then correct.\n    c_reg = np.copy(c)\n    zero_c_mask = c == 0\n    c_reg[zero_c_mask] = 1.0 \n    \n    cpm = 1e6 * X / c_reg[:, np.newaxis].T\n    cpm[:, zero_c_mask] = 0.0 # Enforce 0 for spots with 0 library size\n    \n    log1p_cpm = np.log1p(cpm)\n\n    # Calculate population variance (ddof=0 is default) per gene (axis=1)\n    cpm_vars = list(np.var(cpm, axis=1))\n    log1p_vars = list(np.var(log1p_cpm, axis=1))\n    \n    # --- sctransform Normalization ---\n    sct_vars = []\n    for g in range(G):\n        var = sctransform_gene(X[g, :], c)\n        sct_vars.append(var)\n        \n    return cpm_vars, log1p_vars, sct_vars\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final output.\n    \"\"\"\n    test_cases = [\n        {\n            \"c\": [5000, 7000, 6000, 8000, 5500],\n            \"X\": [\n                [50, 70, 60, 80, 55],\n                [200, 150, 180, 220, 170],\n                [0, 5, 10, 15, 0],\n                [1000, 1400, 1100, 1600, 1200]\n            ]\n        },\n        {\n            \"c\": [0, 3000, 4000],\n            \"X\": [\n                [0, 30, 40],\n                [0, 300, 400],\n                [0, 0, 10]\n            ]\n        },\n        {\n            \"c\": [10000, 15000],\n            \"X\": [\n                [0, 0],\n                [4000, 6000],\n                [100, 300],\n                [5000, 7000],\n                [10, 10]\n            ]\n        }\n    ]\n\n    all_results_str = []\n    for case in test_cases:\n        cpm_vars, log1p_vars, sct_vars = process_case(case[\"X\"], case[\"c\"])\n        \n        cpm_str = f\"[{','.join(f'{v:.6f}' for v in cpm_vars)}]\"\n        log1p_str = f\"[{','.join(f'{v:.6f}' for v in log1p_vars)}]\"\n        sct_str = f\"[{','.join(f'{v:.6f}' for v in sct_vars)}]\"\n        \n        case_result_str = f\"[{cpm_str},{log1p_str},{sct_str}]\"\n        all_results_str.append(case_result_str)\n\n    # Format the final output string exactly as specified.\n    print(f\"[{','.join(all_results_str)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "要充分利用空间转录组学的“空间”维度，我们必须首先将测序点（spot）的物理位置关系形式化地定义出来。本练习的核心任务是构建一个$k$-近邻（k-NN）图，它是一种将几何坐标转化为图论表示的基础数据结构。掌握这一技能是进行所有下游空间分析（如空间域识别、空间统计和细胞通讯推断）的必要前提。",
            "id": "4385418",
            "problem": "您正在分析空间转录组学的空间点布局，以构建用于下游任务（如空间平滑和区域分割）的邻域图。给定一个有限的二维点坐标集和一个目标平均度，您必须确定一个整数邻域参数 $k$，用于构建一个 $k$-最近邻（k-NN）图。当有向 $k$-最近邻图通过无向并集对称化后，该图的平均度应与目标平均度最佳匹配。然后，您必须验证所得度分布的良构图论性质。\n\n使用以下基本原则：\n- 距离是 $\\mathbb{R}^2$ 中的欧几里得距离：对于位置在 $\\mathbf{x}_i$ 和 $\\mathbf{x}_j$ 的点 $i$ 和 $j$，距离为 $d(i,j) = \\|\\mathbf{x}_i - \\mathbf{x}_j\\|_2 = \\sqrt{(x_{i1} - x_{j1})^2 + (x_{i2} - x_{j2})^2}$。\n- 如果节点 $j$ 在所有 $j \\neq i$ 的 $d(i,j)$ 值中属于最小的 $k$ 个之一，则存在从节点 $i$ 到节点 $j$ 的有向 $k$-最近邻（k-NN）关系。\n- 为了从有向 $k$-最近邻图获得一个无向图，使用并集对称化：当且仅当 $i$ 将 $j$ 列为其 $k$ 个最近邻之一，或 $j$ 将 $i$ 列为其 $k$ 个最近邻之一时，$i$ 和 $j$ 之间存在一条无向边。\n- 在无向简单图中，节点的度是与其相连的边的数量。根据握手引理，$\\sum_{i=1}^{N} \\deg(i) = 2|E|$。\n\n距离相等情况下邻居选择的确定性要求：\n- 当距离相等时，优先选择节点索引较小的来打破平局。也就是说，要为节点 $i$ 选择 $k$ 个邻居，需将候选节点 $j \\neq i$ 按序对 $(d(i,j), j)$ 进行升序字典序排序，并取前 $k$ 个。\n\n任务：\n- 对于每个测试用例，给定 $N$ 个点的坐标和一个目标平均度 $\\bar{d}_{\\text{target}}$，在整数 $k \\in \\{1, 2, \\dots, N-1\\}$ 上进行搜索，以构建无向并集对称化 $k$-最近邻图，并计算其平均度 $\\bar{d}_k$。选择使 $|\\bar{d}_k - \\bar{d}_{\\text{target}}|$ 最小化的 $k^\\star$。如果有多个 $k$ 达到相同的最小绝对偏差，则选择其中最小的 $k$。\n- 验证所得度分布的两个性质：\n  1. 精确的握手一致性：$\\left(\\sum_{i=1}^{N} \\deg(i)\\right) = 2|E|$。\n  2. 与目标的接近程度：$|\\bar{d}_{k^\\star} - \\bar{d}_{\\text{target}}| \\le \\theta$，其中容差 $\\theta$ 固定为 $\\theta = 0.25$。\n\n对于每个测试用例，以列表 $[k^\\star, \\bar{d}_{k^\\star}, \\text{handshake\\_ok}, \\text{within\\_tolerance}]$ 的形式生成结果，其中 $k^\\star$ 是一个整数，$\\bar{d}_{k^\\star}$ 是一个实数（浮点数），两个验证标志是布尔值。\n\n测试套件：\n- 用例 A（线性阵列，理想路径）：\n  - 坐标：$(0.0, 0.0)$, $(1.0, 0.0)$, $(2.0, 0.0)$, $(3.0, 0.0)$, $(4.0, 0.0)$, $(5.0, 0.0)$。\n  - 目标平均度：$\\bar{d}_{\\text{target}} = 2.0$。\n- 用例 B（正方形，边界接近完全图）：\n  - 坐标：$(0.0, 0.0)$, $(1.0, 0.0)$, $(0.0, 1.0)$, $(1.0, 1.0)$。\n  - 目标平均度：$\\bar{d}_{\\text{target}} = 3.0$。\n- 用例 C（网格，内部结构）：\n  - 坐标：$(0.0, 0.0)$, $(1.0, 0.0)$, $(2.0, 0.0)$, $(0.0, 1.0)$, $(1.0, 1.0)$, $(2.0, 1.0)$, $(0.0, 2.0)$, $(1.0, 2.0)$, $(2.0, 2.0)$。\n  - 目标平均度：$\\bar{d}_{\\text{target}} = 4.0$。\n- 用例 D（重复点，平局处理边界情况）：\n  - 坐标：$(0.0, 0.0)$, $(0.0, 0.0)$, $(1.0, 0.0)$, $(2.0, 0.0)$, $(2.0, 0.0)$。\n  - 目标平均度：$\\bar{d}_{\\text{target}} = 2.0$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个测试用例的结果本身就是一个列表，格式为 $[k^\\star,\\bar{d}_{k^\\star},\\text{handshake\\_ok},\\text{within\\_tolerance}]$。例如：$[[1,2.0,True,True],[\\dots]]$。\n\n此任务不涉及物理单位或角度。所有计算都是无单位的。容差 $\\theta = 0.25$ 统一应用于所有用例。该图是无权重和简单的（无自环，无多重边）。",
            "solution": "用户提供了一个定义明确的计算问题，该问题位于空间转录组学数据分析领域。任务是确定一个最优邻域参数 $k$，用于构建一个能最佳逼近目标平均度的 $k$-最近邻（k-NN）图，并验证所得图的某些性质。\n\n该问题按如下方式进行验证：\n-   **科学依据**：该问题基于计算几何和图论中的既定方法（欧几里得距离、k-NN 图、图的度、握手引理），并应用于空间基因组学中的一个相关问题。所有前提在科学上和数学上都是合理的。\n-   **良构性**：问题是明确的。输入已清晰指定。构建图的过程，包括一个确定性的平局打破规则，都得到了明确的定义。选择最佳参数 $k^\\star$ 的优化标准也是明确的（最小化与目标的绝对偏差，并以最小的 $k$ 作为平局打破者），确保存在唯一解。\n-   **客观性**：问题陈述是形式化和定量的，没有主观性语言。\n-   **完整性**：问题提供了所有必要的信息，包括测试用例的坐标、每个用例的目标平均度以及容差参数 $\\theta$ 的值。\n\n该问题被认为是有效的，可以制定解决方案。\n\n解决方案通过实现指定的图构建和搜索算法来执行。对于每个测试用例，给定 $\\mathbb{R}^2$ 中的一组 $N$ 个点坐标 $\\{\\mathbf{x}_i\\}_{i=1}^N$ 和一个目标平均度 $\\bar{d}_{\\text{target}}$，算法会遍历邻域参数 $k$ 的所有可能的整数值 $k \\in \\{1, 2, \\dots, N-1\\}$。\n\n对于每个 $k$ 值，我们构建一个图 $G_k = (V, E_k)$，其中 $V$ 是 $N$ 个点的集合。构建过程包括两个主要步骤：\n1.  **有向 k-NN 图构建**：对于每个点 $i$，我们确定其 $k$ 个最近邻的集合，记为 $\\text{NN}_k(i)$。这是通过计算它与所有其他点 $j \\neq i$ 的欧几里得距离 $d(i,j) = \\|\\mathbf{x}_i - \\mathbf{x}_j\\|_2$ 来实现的。然后，候选点 $j$ 根据字典序对 $(d(i,j), j)$ 进行升序排序。此排序列表中的前 $k$ 个点构成 $\\text{NN}_k(i)$。此步骤为所有 $j \\in \\text{NN}_k(i)$ 定义了一组有向边 $(i, j)$。\n\n2.  **通过并集对称化**：当且仅当点 $j$ 是点 $i$ 的 $k$-最近邻，或点 $i$ 是点 $j$ 的 $k$-最近邻时，一条无向边 $\\{i, j\\}$ 才被包含在边集 $E_k$ 中。形式上，$E_k = \\{\\{i, j\\} \\mid j \\in \\text{NN}_k(i) \\lor i \\in \\text{NN}_k(j)\\}$。这种构造保证了一个简单的无向图，因为不允许自环，并且基于集合的 $E_k$ 定义隐式地处理了重复边。\n\n一旦图 $G_k$ 构建完成，我们计算其性质。节点 $i$ 的度 $\\deg_k(i)$ 是与其相连的边的数量。该图的平均度则为 $\\bar{d}_k = \\frac{1}{N} \\sum_{i=1}^{N} \\deg_k(i)$。\n\n在为所有可能的 $k$ 值计算出 $\\bar{d}_k$ 后，我们选择最优参数 $k^\\star$，即最小化绝对偏差 $|\\bar{d}_k - \\bar{d}_{\\text{target}}|$ 的那个。如果出现平局，即多个 $k$ 值产生相同的最小偏差，则根据问题规范选择这些 $k$ 值中最小的一个。\n\n最后，对最优图 $G_{k^\\star}$ 执行两个验证检查：\n1.  **握手一致性**：我们验证度的总和等于边数的两倍：$\\sum_{i=1}^{N} \\deg_{k^\\star}(i) = 2|E_{k^\\star}|$。这是任何无向图的一个基本性质（握手引理），可作为图数据结构实现正确性的合理性检查。度的总和必须是一个偶数。\n2.  **与目标的接近程度**：我们检查所达到的平均度 $\\bar{d}_{k^\\star}$ 是否在目标值的指定容差 $\\theta=0.25$ 范围内：$|\\bar{d}_{k^\\star} - \\bar{d}_{\\text{target}}| \\le \\theta$。\n\n该算法使用 Python 实现，利用 `numpy` 库对成对距离矩阵进行高效的向量化计算。核心逻辑被封装在一个处理单个测试用例的函数中。一个主脚本定义了测试套件并遍历每个用例，收集结果并将其格式化为指定的输出字符串。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test suite.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"name\": \"Case A (linear array, happy path)\",\n            \"coords\": np.array([\n                [0.0, 0.0], [1.0, 0.0], [2.0, 0.0], \n                [3.0, 0.0], [4.0, 0.0], [5.0, 0.0]\n            ]),\n            \"target_avg_degree\": 2.0\n        },\n        {\n            \"name\": \"Case B (square, boundary near completeness)\",\n            \"coords\": np.array([\n                [0.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 1.0]\n            ]),\n            \"target_avg_degree\": 3.0\n        },\n        {\n            \"name\": \"Case C (grid, interior structure)\",\n            \"coords\": np.array([\n                [0.0, 0.0], [1.0, 0.0], [2.0, 0.0],\n                [0.0, 1.0], [1.0, 1.0], [2.0, 1.0],\n                [0.0, 2.0], [1.0, 2.0], [2.0, 2.0]\n            ]),\n            \"target_avg_degree\": 4.0\n        },\n        {\n            \"name\": \"Case D (duplicates, tie-handling edge case)\",\n            \"coords\": np.array([\n                [0.0, 0.0], [0.0, 0.0], [1.0, 0.0], \n                [2.0, 0.0], [2.0, 0.0]\n            ]),\n            \"target_avg_degree\": 2.0\n        }\n    ]\n\n    theta = 0.25\n    results = []\n    \n    for case in test_cases:\n        coords = case[\"coords\"]\n        target_avg_degree = case[\"target_avg_degree\"]\n        result = analyze_graph_topology(coords, target_avg_degree, theta)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\".replace(\" \", \"\"))\n\ndef analyze_graph_topology(coords, target_avg_degree, theta):\n    \"\"\"\n    For a given set of coordinates and target average degree, finds the optimal k\n    and performs the required verifications.\n    \"\"\"\n    n_spots = coords.shape[0]\n    \n    # Pre-calculate all pairwise Euclidean distances efficiently.\n    # coords[:, np.newaxis, :] gives shape (N, 1, 2)\n    # coords[np.newaxis, :, :] gives shape (1, N, 2)\n    # Broadcasting computes the difference for all pairs.\n    diffs = coords[:, np.newaxis, :] - coords[np.newaxis, :, :]\n    dist_matrix = np.sqrt(np.sum(diffs**2, axis=-1))\n    \n    k_results = []\n\n    # Search over all possible k values from 1 to N-1\n    for k in range(1, n_spots):\n        # Adjacency list representation using sets for automatic duplicate edge handling\n        adj = [set() for _ in range(n_spots)]\n        \n        # Determine directed k-NN and build undirected graph via union symmetrization\n        for i in range(n_spots):\n            # Form a list of (distance, index) tuples for sorting\n            neighbors_with_dist = []\n            for j in range(n_spots):\n                if i == j:\n                    continue\n                neighbors_with_dist.append((dist_matrix[i, j], j))\n            \n            # Sort by distance, then by index for tie-breaking\n            neighbors_with_dist.sort()\n            \n            # Extract the k-nearest neighbors\n            k_nearest_indices = [neighbor[1] for neighbor in neighbors_with_dist[:k]]\n            \n            # Add undirected edges to the graph\n            for neighbor_idx in k_nearest_indices:\n                adj[i].add(neighbor_idx)\n                adj[neighbor_idx].add(i)\n        \n        # Calculate degrees and average degree for this k\n        degrees = [len(s) for s in adj]\n        sum_of_degrees = sum(degrees)\n        avg_degree = sum_of_degrees / n_spots\n        \n        # Store results for this k\n        deviation = abs(avg_degree - target_avg_degree)\n        k_results.append({\n            'k': k, 'avg_degree': avg_degree, \n            'deviation': deviation, 'sum_of_degrees': sum_of_degrees\n        })\n\n    # Select the best k based on minimum deviation, with k as a tie-breaker\n    best_result = sorted(k_results, key=lambda x: (x['deviation'], x['k']))[0]\n    \n    k_star = best_result['k']\n    d_k_star = best_result['avg_degree']\n    \n    # Perform the final verification steps\n    # 1. Handshake consistency: sum of degrees must be an even integer.\n    sum_of_degrees_star = best_result['sum_of_degrees']\n    # sum_of_degrees is an integer by construction. The handshake lemma implies it must\n    # be even for any undirected graph. This check verifies implementation correctness.\n    handshake_ok = (sum_of_degrees_star % 2 == 0)\n    \n    # 2. Proximity to target: absolute deviation must be within tolerance.\n    within_tolerance = (best_result['deviation'] = theta)\n    \n    return [k_star, d_k_star, handshake_ok, within_tolerance]\n\nsolve()\n```"
        }
    ]
}