{
    "hands_on_practices": [
        {
            "introduction": "Raw count data from sequencing-based spatial transcriptomics are subject to technical variability, most notably differences in library size (sequencing depth) across spots. Before we can make meaningful biological comparisons, we must apply a normalization procedure to adjust for these effects. This exercise  walks you through implementing and comparing three foundational normalization techniques, from the straightforward Counts Per Million (CPM) scaling to the more sophisticated model-based approach of `sctransform`, helping you understand their impact on data variance.",
            "id": "4385438",
            "problem": "You are given spot-wise total counts and gene-by-spot count matrices that represent Spatial Transcriptomics measurements. Let there be $G$ genes indexed by $g \\in \\{1,\\dots,G\\}$ and $S$ spatial spots indexed by $s \\in \\{1,\\dots,S\\}$. The observed integer counts for gene $g$ at spot $s$ are denoted by $x_{gs}$, and the total counts for spot $s$ (the library size) are denoted by $c_s$. You must compute three normalizations and compare the variance across genes for each normalization.\n\nFundamental base and definitions:\n- In count-based transcriptomics, observed counts are nonnegative integers driven by the underlying abundance and sampling depth. The total counts per spot $c_s$ act as an exposure scalar. A widely accepted stochastic model for count data is the Negative Binomial (NB) distribution, where $x_{gs}$ is modeled with mean $\\mu_{gs}$ and an overdispersion parameter $\\theta_g$ such that the variance is $\\operatorname{Var}(x_{gs}) = \\mu_{gs} + \\mu_{gs}^2 / \\theta_g$. The offset formulation in a Generalized Linear Model (GLM) with a log link sets $\\log(\\mu_{gs}) = \\beta_{0g} + \\log(c_s)$, implying $\\mu_{gs} = c_s \\cdot p_g$ for a gene-specific rate $p_g = e^{\\beta_{0g}}$.\n- Counts Per Million (CPM) normalization rescales counts to a per-million basis to adjust for library size. Specifically, for each spot $s$, the CPM-normalized value for gene $g$ is $n_{gs}^{\\mathrm{CPM}} = 10^6 \\cdot x_{gs} / c_s$ if $c_s > 0$ and $n_{gs}^{\\mathrm{CPM}} = 0$ if $c_s = 0$.\n- The natural logarithm transformation with a pseudocount, denoted \"log1p,\" is defined as $y_{gs}^{\\log 1\\mathrm{p}} = \\log\\left(1 + n_{gs}^{\\mathrm{CPM}}\\right)$.\n- The single-cell transform (sctransform) residuals are derived from an NB-GLM with library size as an offset. With $\\mu_{gs} = c_s \\cdot p_g$ and overdispersion parameter $\\theta_g$, the Pearson residuals are defined as\n$$\nr_{gs} = \n\\begin{cases}\n\\dfrac{x_{gs} - \\mu_{gs}}{\\sqrt{\\mu_{gs} + \\mu_{gs}^2/\\theta_g}}, & \\text{if } \\mu_{gs} > 0,\\\\\n0, & \\text{if } \\mu_{gs} = 0.\n\\end{cases}\n$$\nTo determine $\\theta_g$, use the well-tested fact that under a correctly specified NB model the expected squared Pearson residual is approximately $1$. Solve for $\\theta_g > 0$ such that\n$$\n\\frac{1}{|\\mathcal{I}_g|}\\sum_{s \\in \\mathcal{I}_g} \\frac{\\left(x_{gs} - \\mu_{gs}\\right)^2}{\\mu_{gs} + \\mu_{gs}^2/\\theta_g} = 1,\n$$\nwhere $\\mathcal{I}_g = \\{ s \\,|\\, \\mu_{gs} > 0 \\}$. If no such solution yields a value greater than or equal to $1$ even as $\\theta_g \\to \\infty$ (the Poisson limit), set $\\theta_g$ to a large value (e.g., $\\theta_g = 10^8$). If $c_s = 0$ for any spot $s$, define $n_{gs}^{\\mathrm{CPM}} = 0$ and $\\mu_{gs} = 0$ for that spot.\n\nTasks:\n1. Implement CPM normalization to compute $n_{gs}^{\\mathrm{CPM}}$ for all genes and spots.\n2. Implement log1p transformation on CPM values to compute $y_{gs}^{\\log 1\\mathrm{p}}$ for all genes and spots.\n3. Implement an NB-GLM with offset to compute sctransform Pearson residuals $r_{gs}$:\n   - Estimate $p_g$ by maximum likelihood under the offset model as $p_g = \\left(\\sum_{s=1}^{S} x_{gs}\\right)\\Big/\\left(\\sum_{s : c_s > 0} c_s\\right)$.\n   - Set $\\mu_{gs} = c_s \\cdot p_g$ for all spots.\n   - Solve for $\\theta_g$ using the equation above via a robust one-dimensional root-finding method on $\\theta_g \\in (0, +\\infty)$ (for example, bisection on a sufficiently large interval).\n   - Compute $r_{gs}$.\n4. For each normalization, compute the per-gene population variance across spots. For any vector $z_{g\\cdot} = (z_{g1},\\dots,z_{gS})$, define the population variance as\n$$\n\\operatorname{Var}_{\\text{pop}}(z_{g\\cdot}) = \\frac{1}{S} \\sum_{s=1}^{S} \\left(z_{gs} - \\frac{1}{S}\\sum_{t=1}^{S} z_{gt}\\right)^2.\n$$\n\nAngle units are not applicable. Physical units are counts; no unit conversion is required beyond CPM as defined.\n\nTest suite:\n- Test Case 1:\n  - $S = 5$, $G = 4$,\n  - $c = [5000, 7000, 6000, 8000, 5500]$,\n  - $X = \\begin{bmatrix}\n  50 & 70 & 60 & 80 & 55 \\\\\n  200 & 150 & 180 & 220 & 170 \\\\\n  0 & 5 & 10 & 15 & 0 \\\\\n  1000 & 1400 & 1100 & 1600 & 1200\n  \\end{bmatrix}$.\n- Test Case 2:\n  - $S = 3$, $G = 3$,\n  - $c = [0, 3000, 4000]$,\n  - $X = \\begin{bmatrix}\n  0 & 30 & 40 \\\\\n  0 & 300 & 400 \\\\\n  0 & 0 & 10\n  \\end{bmatrix}$.\n- Test Case 3:\n  - $S = 2$, $G = 5$,\n  - $c = [10000, 15000]$,\n  - $X = \\begin{bmatrix}\n  0 & 0 \\\\\n  4000 & 6000 \\\\\n  100 & 300 \\\\\n  5000 & 7000 \\\\\n  10 & 10\n  \\end{bmatrix}$.\n\nOutput specification:\n- For each test case, produce a list of three lists:\n  1. The list of per-gene population variances of $n_{gs}^{\\mathrm{CPM}}$ across spots (one float per gene).\n  2. The list of per-gene population variances of $y_{gs}^{\\log 1\\mathrm{p}}$ across spots (one float per gene).\n  3. The list of per-gene population variances of $r_{gs}$ across spots (one float per gene).\n- Your program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, with each float rounded to six decimal places. The final format must be:\n\"[[list_for_test_case_1],[list_for_test_case_2],[list_for_test_case_3]]\",\nwhere each list_for_test_case_k is itself \"[[var_CPM_gene_1,...,var_CPM_gene_G],[var_log1p_gene_1,...,var_log1p_gene_G],[var_sctransform_gene_1,...,var_sctransform_gene_G]]\".",
            "solution": "The problem requires the implementation and comparison of three different normalization methods for spatial transcriptomics count data. The methods are Counts Per Million (CPM), a logarithmic transformation of CPM values (log1p), and Pearson residuals from a Negative Binomial Generalized Linear Model (sctransform). For each method, the per-gene population variance across spatial spots is to be calculated.\n\nThe problem has been validated and found to be scientifically sound, well-posed, and objective. It is grounded in established statistical and computational methods used in single-cell and spatial transcriptomics. All necessary data, models, and procedures are explicitly defined, allowing for a unique and verifiable solution.\n\nThe solution will be developed by implementing each normalization step as specified, followed by the calculation of population variance for each gene.\n\nLet the given data be the gene-by-spot count matrix $X = \\{x_{gs}\\}$ for $g \\in \\{1,\\dots,G\\}$ genes and $s \\in \\{1,\\dots,S\\}$ spots, and the vector of total spot counts (library sizes) $c = \\{c_s\\}$.\n\n**1. Normalization Methods**\n\n**a. Counts Per Million (CPM) Normalization**\n\nThis method normalizes the raw counts $x_{gs}$ by the total counts $c_s$ for each spot and scales the result by a factor of one million. This adjusts for variations in sequencing depth across spots. The formula is:\n$$\nn_{gs}^{\\mathrm{CPM}} = \n\\begin{cases}\n\\dfrac{x_{gs}}{c_s} \\times 10^6, & \\text{if } c_s > 0, \\\\\n0, & \\text{if } c_s = 0.\n\\end{cases}\n$$\nThe resulting matrix $N^{\\mathrm{CPM}} = \\{n_{gs}^{\\mathrm{CPM}}\\}$ represents the counts as if every spot had a total count of one million.\n\n**b. Logarithmic Transformation (log1p)**\n\nCount data in genomics are often highly skewed. Applying a logarithmic transformation can help stabilize the variance and make the data distribution more symmetric, which is beneficial for many downstream statistical analyses. A pseudocount of $1$ is added before taking the natural logarithm to avoid issues with zero-count values (i.e., $\\log(0)$). The formula applied to the CPM values is:\n$$\ny_{gs}^{\\log 1\\mathrm{p}} = \\log(1 + n_{gs}^{\\mathrm{CPM}})\n$$\nThe resulting matrix is $Y^{\\log 1\\mathrm{p}} = \\{y_{gs}^{\\log 1\\mathrm{p}}\\}$.\n\n**c. Sctransform Pearson Residuals**\n\nThis method is based on a more sophisticated statistical model of the count data. It models the counts $x_{gs}$ using a Negative Binomial (NB) distribution with the library size $c_s$ as an exposure term. The mean $\\mu_{gs}$ is modeled as being proportional to the library size: $\\mu_{gs} = p_g c_s$, where $p_g$ is a gene-specific relative abundance parameter. The variance of the NB distribution is given by $\\operatorname{Var}(x_{gs}) = \\mu_{gs} + \\mu_{gs}^2 / \\theta_g$, where $\\theta_g$ is the gene-specific overdispersion parameter. The goal is to compute Pearson residuals, which are standardized values that, under a perfect model fit, should have a mean of $0$ and a variance of $1$.\n\nThe procedure is as follows for each gene $g$:\n\n**I. Estimate Rate Parameter $p_g$:**\nThe parameter $p_g$ is estimated using maximum likelihood, which, for this model formulation, simplifies to the ratio of total gene counts to total effective library sizes:\n$$\np_g = \\frac{\\sum_{s=1}^{S} x_{gs}}{\\sum_{s: c_s > 0} c_s}\n$$\nIf $\\sum x_{gs} = 0$, then $p_g=0$, all expected values $\\mu_{gs}$ are $0$, all residuals are $0$, and the variance is $0$.\n\n**II. Estimate Overdispersion Parameter $\\theta_g$:**\nThe parameter $\\theta_g$ controls how much the variance exceeds the mean. It is estimated by solving the following equation, which sets the average squared Pearson residual to its expected value of $1$:\n$$\n\\frac{1}{|\\mathcal{I}_g|}\\sum_{s \\in \\mathcal{I}_g} \\frac{\\left(x_{gs} - \\mu_{gs}\\right)^2}{\\mu_{gs} + \\mu_{gs}^2/\\theta_g} = 1\n$$\nwhere $\\mathcal{I}_g = \\{ s \\,|\\, \\mu_{gs} > 0 \\}$. This equation is solved for $\\theta_g > 0$ using a numerical root-finding algorithm. The left-hand side is a monotonically increasing function of $\\theta_g$. We first evaluate the function in the limit $\\theta_g \\to \\infty$ (the Poisson case):\n$$\nL_{\\infty} = \\frac{1}{|\\mathcal{I}_g|}\\sum_{s \\in \\mathcal{I}_g} \\frac{\\left(x_{gs} - \\mu_{gs}\\right)^2}{\\mu_{gs}}\n$$\nIf $L_{\\infty} < 1$, the data are under-dispersed relative to the Poisson model, and no positive finite solution for $\\theta_g$ exists. In this scenario, as specified, we set $\\theta_g$ to a large value, $\\theta_g = 10^8$. Otherwise, a unique root $\\theta_g$ exists and can be found using a one-dimensional solver like Brent's method on a suitable interval, e.g., $(0, 10^{12}]$.\n\n**III. Compute Pearson Residuals $r_{gs}$:**\nWith estimates for $p_g$ (and thus $\\mu_{gs}$) and $\\theta_g$, the Pearson residuals are calculated as:\n$$\nr_{gs} = \n\\begin{cases}\n\\dfrac{x_{gs} - \\mu_{gs}}{\\sqrt{\\mu_{gs} + \\mu_{gs}^2/\\theta_g}}, & \\text{if } \\mu_{gs} > 0, \\\\\n0, & \\text{if } \\mu_{gs} = 0.\n\\end{cases}\n$$\nThe matrix of residuals is $R = \\{r_{gs}\\}$.\n\n**2. Variance Calculation**\n\nFor each gene $g$ and for each of the three resulting data matrices ($N^{\\mathrm{CPM}}$, $Y^{\\log 1\\mathrm{p}}$, and $R$), the population variance across the $S$ spots is computed. For a generic vector of transformed values for gene $g$, $z_{g\\cdot} = (z_{g1}, \\dots, z_{gS})$, the population variance is:\n$$\n\\operatorname{Var}_{\\text{pop}}(z_{g\\cdot}) = \\frac{1}{S} \\sum_{s=1}^{S} \\left(z_{gs} - \\bar{z}_g\\right)^2, \\quad \\text{where } \\bar{z}_g = \\frac{1}{S}\\sum_{t=1}^{S} z_{gt}\n$$\nThis calculation will be performed for each gene, yielding three lists of variances per test case, corresponding to the three normalization methods.\n\nThe implementation will process each test case, apply these steps to each gene, and format the resulting variance lists according to the specified output format.",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import brentq\nimport collections\n\n# The problem specifies numpy and scipy as permitted libraries.\n# The phrase \"No other libraries outside the Python standard library are permitted\"\n# is interpreted as excluding libraries *not* on the supplied list.\n\ndef sctransform_gene(x_g, c):\n    \"\"\"\n    Computes sctransform residuals and their variance for a single gene.\n    \n    Args:\n        x_g (np.ndarray): Count vector for one gene (shape S).\n        c (np.ndarray): Library size vector (shape S).\n\n    Returns:\n        float: The population variance of the sctransform residuals.\n    \"\"\"\n    S = len(c)\n    \n    # If gene is not expressed at all, residuals are all zero, variance is zero.\n    if np.sum(x_g) == 0:\n        return 0.0\n\n    # Identify effective spots (where library size > 0)\n    c_mask = c > 0\n    c_eff = c[c_mask]\n    \n    # If no spots have counts, variance is zero.\n    if len(c_eff) == 0:\n        return 0.0\n        \n    x_eff = x_g[c_mask]\n    sum_c_eff = np.sum(c_eff)\n\n    # Estimate gene-specific rate parameter p_g\n    p_g = np.sum(x_g) / sum_c_eff\n\n    # Compute expected counts mu_gs\n    mu = c * p_g\n\n    # Filter for spots where mu > 0 for theta estimation (I_g)\n    mu_eff = mu[c_mask]\n    \n    # Numerically stabilize division by mu_eff in case of underflow\n    mu_eff_reg = mu_eff + 1e-12\n    \n    # Check for under-dispersion by evaluating the limit as theta -> infinity\n    # which corresponds to the Poisson model.\n    limit_term = np.sum((x_eff - mu_eff)**2 / mu_eff_reg)\n    \n    # Average squared Pearson residual for the Poisson model\n    limit_lhs = limit_term / len(c_eff)\n\n    if limit_lhs < 1:\n        # Under-dispersed case: no solution for theta > 0. Set to a large value.\n        theta_g = 1e8\n    else:\n        # Over-dispersed case: find theta using root-finding.\n        def objective(theta):\n            # Objective function to find the root for theta\n            # Target: average squared Pearson residual = 1\n            if theta <= 0: return -np.inf\n            var_nb = mu_eff + mu_eff**2 / theta\n            # Prevent division by zero if var_nb is somehow zero\n            var_nb[var_nb == 0] = 1e-12 \n            sum_sq_pearson = np.sum((x_eff - mu_eff)**2 / var_nb)\n            return sum_sq_pearson / len(c_eff) - 1\n\n        try:\n            # Brent's method is a robust and efficient root-finding algorithm.\n            # A large interval for theta is used.\n            theta_g = brentq(objective, a=1e-8, b=1e12, xtol=1e-6, rtol=1e-6)\n        except ValueError:\n            # If brentq fails (e.g., endpoints have same sign unexpectedly),\n            # fall back to the safe high-theta value.\n            theta_g = 1e8\n\n    # Compute Pearson residuals\n    residuals = np.zeros(S, dtype=float)\n    mu_pos_mask = mu > 0\n    \n    if np.any(mu_pos_mask):\n        var_nb = mu[mu_pos_mask] + mu[mu_pos_mask]**2 / theta_g\n        # Prevent sqrt of zero or negative values due to floating point inaccuracies\n        var_nb[var_nb <= 0] = 1e-12\n        residuals[mu_pos_mask] = (x_g[mu_pos_mask] - mu[mu_pos_mask]) / np.sqrt(var_nb)\n\n    return np.var(residuals)\n\ndef process_case(X, c):\n    \"\"\"\n    Processes one test case: computes variances for all three normalizations.\n    \n    Args:\n        X (list of lists): GxS matrix of gene counts.\n        c (list): Vector of S library sizes.\n\n    Returns:\n        tuple: Three lists of floats (cpm_vars, log1p_vars, sct_vars).\n    \"\"\"\n    X = np.array(X, dtype=float)\n    c = np.array(c, dtype=float)\n    G, S = X.shape\n\n    # --- CPM and log1p Normalizations ---\n    # Create a regularized c vector to avoid division by zero, then correct.\n    c_reg = np.copy(c)\n    zero_c_mask = c == 0\n    c_reg[zero_c_mask] = 1.0 \n    \n    cpm = 1e6 * X / c_reg[:, np.newaxis].T\n    cpm[:, zero_c_mask] = 0.0 # Enforce 0 for spots with 0 library size\n    \n    log1p_cpm = np.log1p(cpm)\n\n    # Calculate population variance (ddof=0 is default) per gene (axis=1)\n    cpm_vars = list(np.var(cpm, axis=1))\n    log1p_vars = list(np.var(log1p_cpm, axis=1))\n    \n    # --- sctransform Normalization ---\n    sct_vars = []\n    for g in range(G):\n        var = sctransform_gene(X[g, :], c)\n        sct_vars.append(var)\n        \n    return cpm_vars, log1p_vars, sct_vars\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final output.\n    \"\"\"\n    test_cases = [\n        {\n            \"c\": [5000, 7000, 6000, 8000, 5500],\n            \"X\": [\n                [50, 70, 60, 80, 55],\n                [200, 150, 180, 220, 170],\n                [0, 5, 10, 15, 0],\n                [1000, 1400, 1100, 1600, 1200]\n            ]\n        },\n        {\n            \"c\": [0, 3000, 4000],\n            \"X\": [\n                [0, 30, 40],\n                [0, 300, 400],\n                [0, 0, 10]\n            ]\n        },\n        {\n            \"c\": [10000, 15000],\n            \"X\": [\n                [0, 0],\n                [4000, 6000],\n                [100, 300],\n                [5000, 7000],\n                [10, 10]\n            ]\n        }\n    ]\n\n    all_results_str = []\n    for case in test_cases:\n        cpm_vars, log1p_vars, sct_vars = process_case(case[\"X\"], case[\"c\"])\n        \n        cpm_str = f\"[{','.join(f'{v:.6f}' for v in cpm_vars)}]\"\n        log1p_str = f\"[{','.join(f'{v:.6f}' for v in log1p_vars)}]\"\n        sct_str = f\"[{','.join(f'{v:.6f}' for v in sct_vars)}]\"\n        \n        case_result_str = f\"[{cpm_str},{log1p_str},{sct_str}]\"\n        all_results_str.append(case_result_str)\n\n    # Format the final output string exactly as specified.\n    print(f\"[{','.join(all_results_str)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A key feature of spatial transcriptomics is the coordinate information for each measurement, which allows us to analyze gene expression in its tissue context. A fundamental step in many spatial analyses is to formally define this context by constructing a neighborhood graph, where spots are connected to their spatial neighbors. This practice  guides you through building a $k$-Nearest Neighbors (k-NN) graph, a common approach for capturing local structure, and explores how the choice of the parameter $k$ influences the graph's properties.",
            "id": "4385418",
            "problem": "You are analyzing spatial spot layouts from spatial transcriptomics to construct neighborhood graphs used in downstream tasks such as spatial smoothing and domain segmentation. Given a finite set of two-dimensional spot coordinates and a target average degree, you must determine the integer neighborhood parameter $k$ for a $k$-Nearest Neighbors (k-NN) graph that best matches the target average degree when the directed $k$-Nearest Neighbors graph is symmetrized by undirected union. Then you must verify well-posed graph-theoretic properties of the resulting degree distribution.\n\nUse the following fundamental base:\n- Distances are Euclidean in $\\mathbb{R}^2$: for spots $i$ and $j$ at positions $\\mathbf{x}_i$ and $\\mathbf{x}_j$, the distance is $d(i,j) = \\|\\mathbf{x}_i - \\mathbf{x}_j\\|_2 = \\sqrt{(x_{i1} - x_{j1})^2 + (x_{i2} - x_{j2})^2}$.\n- A directed $k$-Nearest Neighbors (k-NN) relation from node $i$ to node $j$ exists if $j$ is among the $k$ smallest values of $d(i,j)$ over $j \\neq i$.\n- To obtain an undirected graph from the directed $k$-Nearest Neighbors graph, use union symmetrization: there is an undirected edge between $i$ and $j$ if and only if either $i$ lists $j$ among its $k$ nearest neighbors or $j$ lists $i$ among its $k$ nearest neighbors.\n- The degree of a node in an undirected simple graph is the number of incident edges. By the handshake lemma, $\\sum_{i=1}^{N} \\deg(i) = 2|E|$.\n\nDeterminism requirement for neighbor selection under ties:\n- When distances are equal, break ties by smaller node index first. That is, to select $k$ neighbors for node $i$, sort candidates $j \\neq i$ by the pair $(d(i,j), j)$ in ascending lexicographic order and take the first $k$.\n\nTask:\n- For each test case, given coordinates for $N$ spots and a target average degree $\\bar{d}_{\\text{target}}$, search over integer $k \\in \\{1, 2, \\dots, N-1\\}$ to construct the undirected union-symmetrized $k$-Nearest Neighbors graph and compute its average degree $\\bar{d}_k$. Choose $k^\\star$ that minimizes $|\\bar{d}_k - \\bar{d}_{\\text{target}}|$. If multiple $k$ achieve the same minimum absolute deviation, choose the smallest $k$ among them.\n- Verify two properties of the resulting degree distribution: \n  1. Exact handshake consistency: $\\left(\\sum_{i=1}^{N} \\deg(i)\\right) = 2|E|$.\n  2. Proximity to the target: $|\\bar{d}_{k^\\star} - \\bar{d}_{\\text{target}}| \\le \\theta$, where the tolerance $\\theta$ is fixed to $\\theta = 0.25$.\n\nFor each test case, produce the result as a list $[k^\\star, \\bar{d}_{k^\\star}, \\text{handshake\\_ok}, \\text{within\\_tolerance}]$ where $k^\\star$ is an integer, $\\bar{d}_{k^\\star}$ is a real number (float), and the two verification flags are booleans.\n\nTest suite:\n- Case A (linear array, happy path):\n  - Coordinates: $(0.0, 0.0)$, $(1.0, 0.0)$, $(2.0, 0.0)$, $(3.0, 0.0)$, $(4.0, 0.0)$, $(5.0, 0.0)$.\n  - Target average degree: $\\bar{d}_{\\text{target}} = 2.0$.\n- Case B (square, boundary near completeness):\n  - Coordinates: $(0.0, 0.0)$, $(1.0, 0.0)$, $(0.0, 1.0)$, $(1.0, 1.0)$.\n  - Target average degree: $\\bar{d}_{\\text{target}} = 3.0$.\n- Case C (grid, interior structure):\n  - Coordinates: $(0.0, 0.0)$, $(1.0, 0.0)$, $(2.0, 0.0)$, $(0.0, 1.0)$, $(1.0, 1.0)$, $(2.0, 1.0)$, $(0.0, 2.0)$, $(1.0, 2.0)$, $(2.0, 2.0)$.\n  - Target average degree: $\\bar{d}_{\\text{target}} = 4.0$.\n- Case D (duplicates, tie-handling edge case):\n  - Coordinates: $(0.0, 0.0)$, $(0.0, 0.0)$, $(1.0, 0.0)$, $(2.0, 0.0)$, $(2.0, 0.0)$.\n  - Target average degree: $\\bar{d}_{\\text{target}} = 2.0$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is itself a list in the format $[k^\\star,\\bar{d}_{k^\\star},\\text{handshake\\_ok},\\text{within\\_tolerance}]$. For example: $[[1,2.0,True,True],[\\dots]]$.\n\nNo physical units or angles are involved in this task. All computations are unitless. The tolerance is $\\theta = 0.25$ applied uniformly to all cases. The graph is unweighted and simple (no self-loops, no multiple edges).",
            "solution": "The user has provided a well-defined computational problem situated in the domain of spatial transcriptomics data analysis. The task is to determine an optimal neighborhood parameter, $k$, for constructing a $k$-Nearest Neighbors (k-NN) graph that best approximates a target average degree, and to verify certain properties of the resulting graph.\n\nThe problem is validated as follows:\n-   **Scientific Grounding**: The problem is based on established methods in computational geometry and graph theory (Euclidean distance, k-NN graphs, graph degrees, handshake lemma) applied to a relevant problem in spatial genomics. All premises are scientifically and mathematically sound.\n-   **Well-Posedness**: The problem is unambiguous. The inputs are clearly specified. The process for constructing the graph, including a deterministic tie-breaking rule, is explicitly defined. The optimization criterion for selecting the best parameter $k^\\star$ is also explicit (minimizing absolute deviation from a target, with the smallest $k$ as a tie-breaker), ensuring a unique solution exists.\n-   **Objectivity**: The problem statement is formal and quantitative, free from subjective language.\n-   **Completeness**: The problem provides all necessary information, including coordinates for test cases, the target average degree for each, and the value of the tolerance parameter $\\theta$.\n\nThe problem is deemed valid and a solution can be formulated.\n\nThe solution proceeds by implementing the specified graph construction and search algorithm. For each test case, given a set of $N$ spot coordinates $\\{\\mathbf{x}_i\\}_{i=1}^N$ in $\\mathbb{R}^2$ and a target average degree $\\bar{d}_{\\text{target}}$, the algorithm iterates through all possible integer values for the neighborhood parameter $k \\in \\{1, 2, \\dots, N-1\\}$.\n\nFor each value of $k$, we construct a graph $G_k = (V, E_k)$ where $V$ is the set of $N$ spots. The construction involves two main steps:\n1.  **Directed k-NN Graph Construction**: For each spot $i$, we identify its set of $k$ nearest neighbors, denoted $\\text{NN}_k(i)$. This is achieved by computing the Euclidean distance $d(i,j) = \\|\\mathbf{x}_i - \\mathbf{x}_j\\|_2$ to all other spots $j \\neq i$. The candidates $j$ are then sorted in ascending order based on the lexicographical pair $(d(i,j), j)$. The first $k$ spots in this sorted list constitute $\\text{NN}_k(i)$. This step defines a set of directed edges $(i, j)$ for all $j \\in \\text{NN}_k(i)$.\n\n2.  **Symmetrization by Union**: An undirected edge $\\{i, j\\}$ is included in the edge set $E_k$ if and only if spot $j$ is a $k$-nearest neighbor of spot $i$, or spot $i$ is a $k$-nearest neighbor of spot $j$. Formally, $E_k = \\{\\{i, j\\} \\mid j \\in \\text{NN}_k(i) \\lor i \\in \\text{NN}_k(j)\\}$. This construction guarantees a simple undirected graph, as self-loops are disallowed and duplicate edges are implicitly handled by the set-based definition of $E_k$.\n\nOnce the graph $G_k$ is constructed, we compute its properties. The degree of a node $i$, $\\deg_k(i)$, is the number of edges incident to it. The average degree for the graph is then $\\bar{d}_k = \\frac{1}{N} \\sum_{i=1}^{N} \\deg_k(i)$.\n\nAfter computing $\\bar{d}_k$ for all possible values of $k$, we select the optimal parameter $k^\\star$ as the one that minimizes the absolute deviation $|\\bar{d}_k - \\bar{d}_{\\text{target}}|$. If a tie occurs, meaning multiple values of $k$ yield the same minimum deviation, the smallest of these $k$ values is chosen, as per the problem specification.\n\nFinally, for the optimal graph $G_{k^\\star}$, two verification checks are performed:\n1.  **Handshake Consistency**: We verify that the sum of degrees equals twice the number of edges: $\\sum_{i=1}^{N} \\deg_{k^\\star}(i) = 2|E_{k^\\star}|$. This is a fundamental property of any undirected graph (Handshake Lemma) and serves as a sanity check for the correctness of the graph data structure implementation. The sum of degrees must be an even integer.\n2.  **Proximity to Target**: We check whether the achieved average degree $\\bar{d}_{k^\\star}$ is within a specified tolerance $\\theta=0.25$ of the target: $|\\bar{d}_{k^\\star} - \\bar{d}_{\\text{target}}| \\le \\theta$.\n\nThe algorithm is implemented in Python, using the `numpy` library for efficient vectorized computation of the pairwise distance matrix. The core logic is encapsulated in a function that processes a single test case. A main script defines the test suite and iterates through each case, collecting the results and formatting them into the specified output string.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test suite.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"name\": \"Case A (linear array, happy path)\",\n            \"coords\": np.array([\n                [0.0, 0.0], [1.0, 0.0], [2.0, 0.0], \n                [3.0, 0.0], [4.0, 0.0], [5.0, 0.0]\n            ]),\n            \"target_avg_degree\": 2.0\n        },\n        {\n            \"name\": \"Case B (square, boundary near completeness)\",\n            \"coords\": np.array([\n                [0.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 1.0]\n            ]),\n            \"target_avg_degree\": 3.0\n        },\n        {\n            \"name\": \"Case C (grid, interior structure)\",\n            \"coords\": np.array([\n                [0.0, 0.0], [1.0, 0.0], [2.0, 0.0],\n                [0.0, 1.0], [1.0, 1.0], [2.0, 1.0],\n                [0.0, 2.0], [1.0, 2.0], [2.0, 2.0]\n            ]),\n            \"target_avg_degree\": 4.0\n        },\n        {\n            \"name\": \"Case D (duplicates, tie-handling edge case)\",\n            \"coords\": np.array([\n                [0.0, 0.0], [0.0, 0.0], [1.0, 0.0], \n                [2.0, 0.0], [2.0, 0.0]\n            ]),\n            \"target_avg_degree\": 2.0\n        }\n    ]\n\n    theta = 0.25\n    results = []\n    \n    for case in test_cases:\n        coords = case[\"coords\"]\n        target_avg_degree = case[\"target_avg_degree\"]\n        result = analyze_graph_topology(coords, target_avg_degree, theta)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\".replace(\" \", \"\"))\n\ndef analyze_graph_topology(coords, target_avg_degree, theta):\n    \"\"\"\n    For a given set of coordinates and target average degree, finds the optimal k\n    and performs the required verifications.\n    \"\"\"\n    n_spots = coords.shape[0]\n    \n    # Pre-calculate all pairwise Euclidean distances efficiently.\n    # coords[:, np.newaxis, :] gives shape (N, 1, 2)\n    # coords[np.newaxis, :, :] gives shape (1, N, 2)\n    # Broadcasting computes the difference for all pairs.\n    diffs = coords[:, np.newaxis, :] - coords[np.newaxis, :, :]\n    dist_matrix = np.sqrt(np.sum(diffs**2, axis=-1))\n    \n    k_results = []\n\n    # Search over all possible k values from 1 to N-1\n    for k in range(1, n_spots):\n        # Adjacency list representation using sets for automatic duplicate edge handling\n        adj = [set() for _ in range(n_spots)]\n        \n        # Determine directed k-NN and build undirected graph via union symmetrization\n        for i in range(n_spots):\n            # Form a list of (distance, index) tuples for sorting\n            neighbors_with_dist = []\n            for j in range(n_spots):\n                if i == j:\n                    continue\n                neighbors_with_dist.append((dist_matrix[i, j], j))\n            \n            # Sort by distance, then by index for tie-breaking\n            neighbors_with_dist.sort()\n            \n            # Extract the k-nearest neighbors\n            k_nearest_indices = [neighbor[1] for neighbor in neighbors_with_dist[:k]]\n            \n            # Add undirected edges to the graph\n            for neighbor_idx in k_nearest_indices:\n                adj[i].add(neighbor_idx)\n                adj[neighbor_idx].add(i)\n        \n        # Calculate degrees and average degree for this k\n        degrees = [len(s) for s in adj]\n        sum_of_degrees = sum(degrees)\n        avg_degree = sum_of_degrees / n_spots\n        \n        # Store results for this k\n        deviation = abs(avg_degree - target_avg_degree)\n        k_results.append({\n            'k': k, 'avg_degree': avg_degree, \n            'deviation': deviation, 'sum_of_degrees': sum_of_degrees\n        })\n\n    # Select the best k based on minimum deviation, with k as a tie-breaker\n    best_result = sorted(k_results, key=lambda x: (x['deviation'], x['k']))[0]\n    \n    k_star = best_result['k']\n    d_k_star = best_result['avg_degree']\n    \n    # Perform the final verification steps\n    # 1. Handshake consistency: sum of degrees must be an even integer.\n    sum_of_degrees_star = best_result['sum_of_degrees']\n    # sum_of_degrees is an integer by construction. The handshake lemma implies it must\n    # be even for any undirected graph. This check verifies implementation correctness.\n    handshake_ok = (sum_of_degrees_star % 2 == 0)\n    \n    # 2. Proximity to target: absolute deviation must be within tolerance.\n    within_tolerance = (best_result['deviation'] <= theta)\n    \n    return [k_star, d_k_star, handshake_ok, within_tolerance]\n\nsolve()\n```"
        },
        {
            "introduction": "Translational and clinical studies often involve complex experimental designs with multiple patients and technical replicates, leading to variation from many sources. Distinguishing true biological heterogeneity from technical noise is critical for drawing sound scientific conclusions. This advanced exercise  introduces the use of hierarchical random-effects models to dissect the total observed variance into its biological and technical components, a powerful technique for assessing data quality and reproducibility.",
            "id": "4385420",
            "problem": "Consider a spatial transcriptomics experiment with replicate sections per patient. Let $y_{p,s,i}$ denote the log-normalized expression of a single gene measured at spot $i$ on section $s$ from patient $p$. Assume a hierarchical normal random-effects model for a balanced design with $P$ patients, $S$ sections per patient, and $I$ spots per section, where\n$$\ny_{p,s,i} = \\mu + b_p + t_{p,s} + e_{p,s,i},\n$$\nwith $b_p \\sim \\mathcal{N}(0,\\sigma_b^2)$ representing biological variability across patients, $t_{p,s} \\sim \\mathcal{N}(0,\\sigma_t^2)$ representing technical variability across replicate sections within a patient, and $e_{p,s,i} \\sim \\mathcal{N}(0,\\sigma_e^2)$ representing residual measurement variability across spots within a section. All random effects are mutually independent, and the design is balanced so that each patient has exactly $S$ sections and each section has exactly $I$ spots.\n\nStarting only from the independence of the random effects, additivity of variance under summation of independent terms, and the definitions of sample means and unbiased sample variances, derive a method-of-moments procedure to estimate the variance components $\\sigma_b^2$, $\\sigma_t^2$, and $\\sigma_e^2$. Use these variance component estimates to compute two Intraclass Correlation Coefficients (ICC): the patient-level ICC for spot-level measurements, defined as the correlation between two randomly selected spot-level measurements from the same patient (but potentially different sections), and the section-level ICC for spot-level measurements, defined as the correlation between two randomly selected spot-level measurements from the same section. The ICCs must be expressed as decimals.\n\nScientific realism constraints:\n- The variance components $\\sigma_b^2$, $\\sigma_t^2$, and $\\sigma_e^2$ are non-negative; if any method-of-moments estimate is negative due to sampling fluctuation, set it to $0$ before computing ICCs.\n- The ICC for the patient level must reflect the fraction of total spot-level variance attributable to random effects shared by observations within the same patient, and the ICC for the section level must reflect the fraction of total spot-level variance attributable to random effects shared by observations within the same section.\n\nTest suite (each case is a balanced nested dataset with $P=2$, $S=2$, and $I=3$; numbers are log-normalized arbitrary units):\n- Case $1$ (general case, moderate biological variability, modest residual variability, negligible section variability):\n  - Patient $1$, Section $1$: $\\{2.00, 2.20, 1.80\\}$\n  - Patient $1$, Section $2$: $\\{2.10, 2.30, 1.90\\}$\n  - Patient $2$, Section $1$: $\\{3.00, 3.10, 2.90\\}$\n  - Patient $2$, Section $2$: $\\{3.20, 3.00, 3.10\\}$\n- Case $2$ (edge case, technical variability dominates, low biological variability, modest residual variability):\n  - Patient $1$, Section $1$: $\\{2.10, 2.00, 2.20\\}$\n  - Patient $1$, Section $2$: $\\{3.10, 3.00, 3.20\\}$\n  - Patient $2$, Section $1$: $\\{2.05, 2.15, 2.10\\}$\n  - Patient $2$, Section $2$: $\\{3.05, 2.95, 3.15\\}$\n- Case $3$ (boundary case, high biological variability, negligible technical and residual variability):\n  - Patient $1$, Section $1$: $\\{1.00, 1.00, 1.00\\}$\n  - Patient $1$, Section $2$: $\\{1.00, 1.00, 1.00\\}$\n  - Patient $2$, Section $1$: $\\{4.00, 4.00, 4.00\\}$\n  - Patient $2$, Section $2$: $\\{4.00, 4.00, 4.00\\}$\n\nRequired outputs for each test case:\n- A list of two floats: $[\\text{ICC}_\\text{patient}, \\text{ICC}_\\text{section}]$, where both floats are rounded to $4$ decimal places.\n\nYour program should produce a single line of output containing the results for all three cases as a comma-separated list enclosed in square brackets, where each element is the per-case list described above (for example, $[[a_1,b_1],[a_2,b_2],[a_3,b_3]]$ but with actual numeric values).",
            "solution": "The problem requires the derivation of a method-of-moments procedure to estimate variance components in a nested random-effects model and subsequently compute two types of Intraclass Correlation Coefficients (ICCs).\n\n### Step 1: Problem Validation\n\n**1.1. Extract Givens**\n\n- **Model:** $y_{p,s,i} = \\mu + b_p + t_{p,s} + e_{p,s,i}$ for patient $p \\in \\{1, \\dots, P\\}$, section $s \\in \\{1, \\dots, S\\}$, and spot $i \\in \\{1, \\dots, I\\}$.\n- **Random Effects:**\n    - Patient effect: $b_p \\sim \\mathcal{N}(0, \\sigma_b^2)$.\n    - Section-within-patient effect: $t_{p,s} \\sim \\mathcal{N}(0, \\sigma_t^2)$.\n    - Residual error: $e_{p,s,i} \\sim \\mathcal{N}(0, \\sigma_e^2)$.\n- **Assumptions:** All random effects are mutually independent. The design is balanced.\n- **Definitions:**\n    - Patient-level ICC: Correlation between two randomly selected spot-level measurements from the same patient.\n    - Section-level ICC: Correlation between two randomly selected spot-level measurements from the same section.\n- **Constraints:**\n    - Variance component estimates must be non-negative. If an estimate is negative, it is set to $0$.\n    - The ICC definitions must reflect the fraction of total variance attributable to shared random effects.\n- **Test Data:** Three test cases are provided with $P=2$, $S=2$, $I=3$.\n- **Required Output:** For each case, a list of two floats $[\\text{ICC}_\\text{patient}, \\text{ICC}_\\text{section}]$ rounded to $4$ decimal places.\n\n**1.2. Validate Using Extracted Givens**\n\nThe problem is evaluated against the validation criteria:\n\n- **Scientifically Grounded:** The hierarchical linear model (or nested random-effects model) is a standard and fundamental statistical tool in biological sciences, including genomics, for analyzing sources of variation in structured experiments. The method of moments is a classical statistical technique for parameter estimation. The problem is scientifically sound.\n- **Well-Posed:** The problem is clearly specified. The balanced design and the defined model structure ensure that the method-of-moments estimators are well-defined. The instructions to truncate negative variance estimates resolve potential ambiguity in the estimation procedure, leading to a unique solution.\n- **Objective:** The problem statement is formal and mathematical, using precise and unambiguous terminology.\n\nThe problem does not exhibit any flaws such as scientific unsoundness, non-formalizability, incompleteness, contradiction, or unrealistic conditions. It is a standard, solvable problem in mathematical statistics applied to a relevant biological context.\n\n**1.3. Verdict and Action**\n\nThe problem is **valid**. A complete, reasoned solution will be provided.\n\n### Step 2: Derivation of Estimators and ICCs\n\nThe solution proceeds in two main parts: first, deriving the method-of-moments estimators for the variance components $\\sigma_b^2$, $\\sigma_t^2$, and $\\sigma_e^2$, and second, deriving the formulas for the ICCs based on these components.\n\n**2.1. Method-of-Moments Estimators for Variance Components**\n\nWe use the analysis of variance (ANOVA) framework, which is equivalent to the method of moments for this model. We define sums of squares (SS) corresponding to each level of the hierarchy and find their expected values.\n\nThe model is $y_{p,s,i} = \\mu + b_p + t_{p,s} + e_{p,s,i}$. Let's define the sample means:\n- Section mean: $\\bar{y}_{p,s,.} = \\frac{1}{I} \\sum_{i=1}^{I} y_{p,s,i}$\n- Patient mean: $\\bar{y}_{p,.,.} = \\frac{1}{S} \\sum_{s=1}^{S} \\bar{y}_{p,s,.} = \\frac{1}{SI} \\sum_{s=1}^{S} \\sum_{i=1}^{I} y_{p,s,i}$\n- Grand mean: $\\bar{y}_{.,.,.} = \\frac{1}{P} \\sum_{p=1}^{P} \\bar{y}_{p,.,.} = \\frac{1}{PSI} \\sum_{p=1}^{P} \\sum_{s=1}^{S} \\sum_{i=1}^{I} y_{p,s,i}$\n\nThe total sum of squares can be partitioned into three components:\n\n1.  **Sum of Squares Within sections (or Error sum of squares, SSE):** This measures the variability of spots within the same section.\n    $$SSE = \\sum_{p=1}^{P} \\sum_{s=1}^{S} \\sum_{i=1}^{I} (y_{p,s,i} - \\bar{y}_{p,s,.})^2$$\n    The corresponding **Mean Square Error (MSE)** is $MSE = \\frac{SSE}{P S (I-1)}$. Its expected value is:\n    $$E[MSE] = \\sigma_e^2$$\n\n2.  **Sum of Squares for Sections within patients (SST):** This measures the variability of section means within the same patient.\n    $$SST = I \\sum_{p=1}^{P} \\sum_{s=1}^{S} (\\bar{y}_{p,s,.} - \\bar{y}_{p,.,.})^2$$\n    The corresponding **Mean Square for Sections (MST)** is $MST = \\frac{SST}{P (S-1)}$. Its expected value is:\n    $$E[MST] = \\sigma_e^2 + I \\sigma_t^2$$\n\n3.  **Sum of Squares Between patients (SSB):** This measures the variability of patient means around the grand mean.\n    $$SSB = SI \\sum_{p=1}^{P} (\\bar{y}_{p,.,.} - \\bar{y}_{.,.,.})^2$$\n    The corresponding **Mean Square Between patients (MSB)** is $MSB = \\frac{SSB}{P-1}$. Its expected value is:\n    $$E[MSB] = \\sigma_e^2 + I \\sigma_t^2 + SI \\sigma_b^2$$\n\nBy equating the observed mean squares to their expected values, we obtain a system of equations:\n1.  $MSE = \\hat{\\sigma}_e^2$\n2.  $MST = \\hat{\\sigma}_e^2 + I \\hat{\\sigma}_t^2$\n3.  $MSB = \\hat{\\sigma}_e^2 + I \\hat{\\sigma}_t^2 + SI \\hat{\\sigma}_b^2$\n\nSolving this system for the variance component estimators ($\\hat{\\sigma}^2$) yields:\n$$ \\hat{\\sigma}_e^2 = MSE $$\n$$ \\hat{\\sigma}_t^2 = \\frac{MST - MSE}{I} $$\n$$ \\hat{\\sigma}_b^2 = \\frac{MSB - MST}{SI} $$\n\nAs per the problem's constraint, any negative estimate is truncated to zero. Let the final non-negative estimates be denoted by a superscript asterisk:\n$$ \\hat{\\sigma}_e^{2*} = MSE \\quad (\\text{note: } MSE \\ge 0) $$\n$$ \\hat{\\sigma}_t^{2*} = \\max\\left(0, \\frac{MST - MSE}{I}\\right) $$\n$$ \\hat{\\sigma}_b^{2*} = \\max\\left(0, \\frac{MSB - MST}{SI}\\right) $$\n\n**2.2. Intraclass Correlation Coefficients (ICCs)**\n\nThe ICC is defined as the correlation between two measurements, which for a stationary process simplifies to the ratio of the covariance between the measurements to the total variance of a single measurement.\nThe total variance of a single observation $y_{p,s,i}$ is:\n$$ \\text{Var}(y_{p,s,i}) = \\text{Var}(\\mu + b_p + t_{p,s} + e_{p,s,i}) = \\text{Var}(b_p) + \\text{Var}(t_{p,s}) + \\text{Var}(e_{p,s,i}) = \\sigma_b^2 + \\sigma_t^2 + \\sigma_e^2 $$\n\n1.  **Patient-level ICC ($\\text{ICC}_\\text{patient}$):** This is the correlation between two observations, $y_{p,s,i}$ and $y_{p,s',i'}$, from the same patient $p$ but potentially different sections and spots.\n    $$ \\text{Cov}(y_{p,s,i}, y_{p,s',i'}) = \\text{Cov}(\\mu + b_p + t_{p,s} + e_{p,s,i}, \\mu + b_p + t_{p,s'} + e_{p,s',i'}) $$\n    Since all random effects are independent, the covariance is non-zero only for shared effects. The only guaranteed shared effect is the patient effect $b_p$.\n    $$ \\text{Cov}(y_{p,s,i}, y_{p,s',i'}) = \\text{Var}(b_p) = \\sigma_b^2 $$\n    Therefore, the patient-level ICC is:\n    $$ \\text{ICC}_\\text{patient} = \\frac{\\text{Cov}(y_{p,s,i}, y_{p,s',i'})}{\\text{Var}(y_{p,s,i})} = \\frac{\\sigma_b^2}{\\sigma_b^2 + \\sigma_t^2 + \\sigma_e^2} $$\n\n2.  **Section-level ICC ($\\text{ICC}_\\text{section}$):** This is the correlation between two observations, $y_{p,s,i}$ and $y_{p,s,i'}$, from the same section $(p,s)$ but different spots ($i \\neq i'$).\n    $$ \\text{Cov}(y_{p,s,i}, y_{p,s,i'}) = \\text{Cov}(\\mu + b_p + t_{p,s} + e_{p,s,i}, \\mu + b_p + t_{p,s} + e_{p,s,i'}) $$\n    The shared random effects are the patient effect $b_p$ and the section effect $t_{p,s}$.\n    $$ \\text{Cov}(y_{p,s,i}, y_{p,s,i'}) = \\text{Var}(b_p) + \\text{Var}(t_{p,s}) = \\sigma_b^2 + \\sigma_t^2 $$\n    Therefore, the section-level ICC is:\n    $$ \\text{ICC}_\\text{section} = \\frac{\\text{Cov}(y_{p,s,i}, y_{p,s,i'})}{\\text{Var}(y_{p,s,i})} = \\frac{\\sigma_b^2 + \\sigma_t^2}{\\sigma_b^2 + \\sigma_t^2 + \\sigma_e^2} $$\n\nUsing the truncated estimates, the ICC estimators are:\n$$ \\widehat{\\text{ICC}}_\\text{patient} = \\frac{\\hat{\\sigma}_b^{2*}}{\\hat{\\sigma}_b^{2*} + \\hat{\\sigma}_t^{2*} + \\hat{\\sigma}_e^{2*}} $$\n$$ \\widehat{\\text{ICC}}_\\text{section} = \\frac{\\hat{\\sigma}_b^{2*} + \\hat{\\sigma}_t^{2*}}{\\hat{\\sigma}_b^{2*} + \\hat{\\sigma}_t^{2*} + \\hat{\\sigma}_e^{2*}} $$\nIf the total estimated variance is zero, all component variances must also be zero, and the ICCs are taken to be zero.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_iccs(data, P, S, I):\n    \"\"\"\n    Calculates variance components and ICCs for a balanced nested design.\n\n    Args:\n        data (np.ndarray): A PxSxI array of measurements.\n        P (int): Number of patients.\n        S (int): Number of sections per patient.\n        I (int): Number of spots per section.\n\n    Returns:\n        list: A list containing [ICC_patient, ICC_section] rounded to 4 decimals.\n    \"\"\"\n    # Reshape data into a convenient PxSxI format if not already\n    data = np.asarray(data).reshape((P, S, I))\n\n    # Calculate means\n    mean_sections = np.mean(data, axis=2)  # shape (P, S)\n    mean_patients = np.mean(mean_sections, axis=1)  # shape (P,)\n    grand_mean = np.mean(mean_patients)\n\n    # Calculate Sums of Squares (SS)\n    # SSB: Sum of Squares Between patients\n    ssb_term = (mean_patients - grand_mean)**2\n    SSB = S * I * np.sum(ssb_term)\n\n    # SST: Sum of Squares for Sections within patients\n    sst_term = (mean_sections - mean_patients[:, np.newaxis])**2\n    SST = I * np.sum(sst_term)\n\n    # SSE: Sum of Squares Error (Within sections)\n    sse_term = (data - mean_sections[:, :, np.newaxis])**2\n    SSE = np.sum(sse_term)\n\n    # Calculate Degrees of Freedom (df)\n    df_B = P - 1\n    df_T = P * (S - 1)\n    df_E = P * S * (I - 1)\n\n    # Calculate Mean Squares (MS)\n    # Handle df=0 case for robustness, though not expected with given test cases\n    MSB = SSB / df_B if df_B > 0 else 0.0\n    MST = SST / df_T if df_T > 0 else 0.0\n    MSE = SSE / df_E if df_E > 0 else 0.0\n\n    # Estimate variance components\n    sigma_e2_hat = MSE\n    sigma_t2_hat = (MST - MSE) / I\n    sigma_b2_hat = (MSB - MST) / (S * I)\n\n    # Truncate negative estimates to 0\n    sigma_e2_star = max(0, sigma_e2_hat)\n    sigma_t2_star = max(0, sigma_t2_hat)\n    sigma_b2_star = max(0, sigma_b2_hat)\n\n    # Calculate total variance\n    total_variance = sigma_b2_star + sigma_t2_star + sigma_e2_star\n\n    # Calculate ICCs\n    if total_variance == 0:\n        # If all data points are identical, variance is 0, correlation is undefined.\n        # Treat as 0, reflecting no correlated variability.\n        icc_patient = 0.0\n        icc_section = 0.0\n    else:\n        icc_patient = sigma_b2_star / total_variance\n        icc_section = (sigma_b2_star + sigma_t2_star) / total_variance\n\n    return [f\"{icc_patient:.4f}\", f\"{icc_section:.4f}\"]\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and print the results.\n    \"\"\"\n    # Test suite data\n    # Each case is a balanced nested dataset with P=2, S=2, I=3\n    test_cases = [\n        # Case 1\n        [\n            [[2.00, 2.20, 1.80], [2.10, 2.30, 1.90]],  # Patient 1\n            [[3.00, 3.10, 2.90], [3.20, 3.00, 3.10]]   # Patient 2\n        ],\n        # Case 2\n        [\n            [[2.10, 2.00, 2.20], [3.10, 3.00, 3.20]],  # Patient 1\n            [[2.05, 2.15, 2.10], [3.05, 2.95, 3.15]]   # Patient 2\n        ],\n        # Case 3\n        [\n            [[1.00, 1.00, 1.00], [1.00, 1.00, 1.00]],  # Patient 1\n            [[4.00, 4.00, 4.00], [4.00, 4.00, 4.00]]   # Patient 2\n        ]\n    ]\n\n    P, S, I = 2, 2, 3\n    results = []\n    for case_data in test_cases:\n        iccs = calculate_iccs(case_data, P, S, I)\n        # Convert string floats to actual floats for list representation\n        results.append([float(x) for x in iccs])\n\n    # Final print statement must produce a list of lists format without spaces.\n    # str(results) produces \"[...]\", and .replace(' ', '') removes spaces.\n    final_output = str(results).replace(' ', '')\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}