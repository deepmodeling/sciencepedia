{
    "hands_on_practices": [
        {
            "introduction": "在我们构建模型来预测剪接之前，我们必须首先理解如何从实验数据中量化剪接。这项练习旨在帮助您对“剪接包含百分比”（Percent Spliced-In, $\\Psi$）指标建立基础性的理解，这是衡量可变剪接事件的标准。通过从RNA测序（RNA-seq）的读数（read counts）中推导$\\Psi$值的估计量，您将掌握几乎所有定量剪接分析背后的统计假设。",
            "id": "4330953",
            "problem": "在一项旨在为精准医疗优先筛选改变剪接的变异的基因组诊断研究中，训练了一个深度学习模型，用于从序列上下文中预测盒式外显子的包含概率。对于训练队列中的每个外显子，短读长核糖核酸测序（RNA-seq）提供了明确跨越定义包含的连接（junction）和定义跳过的连接的读段（read）计数。对于一个给定的盒式外显子，用 $I$ 表示支持包含的读段数，用 $S$ 表示支持跳过的读段数。假设模糊读段已被过滤，且该外显子表现为一个简单的盒式事件，具有两种互斥的异构体。\n\n从概率论的第一性原理出发，并使用与标准RNA-seq抽样和比对假设一致的生成式计数模型，根据观测到的计数 $(I,S)$ 推导真实包含概率 $\\psi$ 的最大似然估计量。你的推导必须从双异构体混合物中独立抽样跨连接读段所隐含的似然性开始，并应清楚地说明为使估计量在期望上无偏所需的所有建模假设。然后，使用你推导出的估计量，计算观测计数 $I = 152$ 和 $S = 48$ 时的估计包含概率。将你的最终答案表示为在 $[0,1]$ 区间内、四舍五入到四位有效数字的小数。",
            "solution": "该问题要求根据RNA-seq实验中观测到的读段计数，推导盒式外显子包含概率（表示为 $\\psi$）的最大似然估计量。此外，还需要说明该估计量在何种假设下是无偏的，并对一组特定的计数值进行计算。\n\n### 第一部分：最大似然估计量（$\\hat{\\psi}$）的推导\n\n问题陈述，对于一个给定的盒式外显子，我们观测到 $I$ 个支持包含异构体的读段和 $S$ 个支持跳过异构体的读段。我们已知这是仅有的两种结果，这是简单盒式事件假设的结果。信息性跨连接读段的总数为 $N = I + S$。\n\n让我们对从RNA分子潜在群体中抽样这些读段的过程进行建模。我们将感兴趣的参数 $\\psi$ 定义为生物样本中两种可能性中包含异构体的真实比例。因此，$\\psi \\in [0, 1]$。相应地，跳过异构体的比例为 $1-\\psi$。\n\n在对分子进行测序时采用均匀和独立抽样的标准假设下，观测到单个跨连接读段的事件可以被建模为一次伯努利试验。令 $X_i$（其中 $i \\in \\{1, 2, \\dots, N\\}$）为一个随机变量，代表第 $i$ 个读段的结果。我们可以设定，如果读段支持包含，则 $X_i = 1$；如果支持跳过，则 $X_i = 0$。成功的概率（观测到包含读段）是 $P(X_i=1) = \\psi$，失败的概率是 $P(X_i=0) = 1-\\psi$。\n\n观测数据包括包含读段的总数 $I$ 和跳过读段的总数 $S$。包含读段的总数是这些伯努利试验的总和：$I = \\sum_{i=1}^N X_i$。试验次数 $N$ 由数据固定，即 $N=I+S$。因此，随机变量 $I$ 服从参数为 $N$ 和 $\\psi$ 的二项分布，记作 $I \\sim \\text{Binomial}(N, \\psi)$。\n\n二项分布的概率质量函数给出了在 $N$ 次试验中观测到恰好 $I$ 次成功的概率。当这个函数被看作是固定观测数据 $(I, S)$ 下参数 $\\psi$ 的函数时，它就是似然函数 $L(\\psi | I, S)$。\n\n$$L(\\psi | I, S) = P(I \\text{ successes in } N \\text{ trials}) = \\binom{N}{I} \\psi^I (1-\\psi)^{N-I}$$\n代入 $N = I+S$，我们得到：\n$$L(\\psi | I, S) = \\binom{I+S}{I} \\psi^I (1-\\psi)^S$$\n\n为了找到 $\\psi$ 的最大似然估计量（MLE），记作 $\\hat{\\psi}$，我们必须找到使 $L(\\psi | I, S)$ 最大化的 $\\psi$ 值。在计算上，最大化似然函数的自然对数，即对数似然函数 $\\ell(\\psi) = \\ln(L(\\psi))$，会更简单。由于对数是单调递增函数，最大化 $\\ell(\\psi)$ 等同于最大化 $L(\\psi)$。\n\n对数似然函数为：\n$$\\ell(\\psi) = \\ln \\left( \\binom{I+S}{I} \\psi^I (1-\\psi)^S \\right)$$\n$$\\ell(\\psi) = \\ln\\left(\\binom{I+S}{I}\\right) + I \\ln(\\psi) + S \\ln(1-\\psi)$$\n\n为了找到最大值，我们计算 $\\ell(\\psi)$ 相对于 $\\psi$ 的导数，并令其等于零。\n$$\\frac{d\\ell}{d\\psi} = \\frac{d}{d\\psi} \\left[ \\ln\\left(\\binom{I+S}{I}\\right) + I \\ln(\\psi) + S \\ln(1-\\psi) \\right]$$\n$$\\frac{d\\ell}{d\\psi} = 0 + \\frac{I}{\\psi} + \\frac{S}{1-\\psi} \\cdot (-1) = \\frac{I}{\\psi} - \\frac{S}{1-\\psi}$$\n\n将导数设为零以找到驻点：\n$$\\frac{I}{\\hat{\\psi}} - \\frac{S}{1-\\hat{\\psi}} = 0$$\n$$\\frac{I}{\\hat{\\psi}} = \\frac{S}{1-\\hat{\\psi}}$$\n$$I(1-\\hat{\\psi}) = S\\hat{\\psi}$$\n$$I - I\\hat{\\psi} = S\\hat{\\psi}$$\n$$I = (I+S)\\hat{\\psi}$$\n\n求解 $\\hat{\\psi}$，我们得到最大似然估计量：\n$$\\hat{\\psi} = \\frac{I}{I+S}$$\n\n为了确认这一点对应于一个最大值，我们检查对数似然函数的二阶导数：\n$$\\frac{d^2\\ell}{d\\psi^2} = \\frac{d}{d\\psi} \\left( \\frac{I}{\\psi} - \\frac{S}{1-\\psi} \\right) = -\\frac{I}{\\psi^2} - \\frac{S}{(1-\\psi)^2}(-1)^2 = -\\frac{I}{\\psi^2} - \\frac{S}{(1-\\psi)^2}$$\n在 $I > 0$ 和 $S > 0$ 的非平凡情况下，并且由于 $\\psi \\in (0,1)$，两项都为负。因此，$\\frac{d^2\\ell}{d\\psi^2}  0$，这证实了 $\\hat{\\psi}$ 确实是一个最大值。\n\n### 第二部分：无偏性的假设\n\n一个参数 $\\theta$ 的估计量 $\\hat{\\theta}$ 如果其期望值等于该参数的真实值，即 $E[\\hat{\\theta}] = \\theta$，则称其为无偏的。这里，我们检验 $E[\\hat{\\psi}]$。\n\n估计量为 $\\hat{\\psi} = \\frac{I}{I+S} = \\frac{I}{N}$。在期望值的计算中，总读段数 $N$ 可以被视为一个固定量，这个计算是以此事件的总测序深度为条件的。包含读段数 $I$ 是一个随机变量，服从分布 $I \\sim \\text{Binomial}(N, \\psi)$。\n\n二项随机变量的期望值为 $E[I] = N\\psi$。\n我们估计量的期望值为：\n$$E[\\hat{\\psi}] = E\\left[\\frac{I}{N}\\right] = \\frac{1}{N}E[I] = \\frac{1}{N}(N\\psi) = \\psi$$\n因此，估计量 $\\hat{\\psi}$ 是无偏的。\n\n然而，这一结果依赖于一个关键假设，即底层的生成模型 $\\text{Binomial}(N, \\psi)$ 是对物理过程的正确表述。为了使 $E[\\hat{\\psi}]$ 等于真实的生物学比例 $\\psi$，必须满足以下假设：\n1.  **读段的独立同分布**：每个跨连接读段都是从相同的基础异构体分布中进行的独立抽样。这是伯努利试验模型的核心。\n2.  **无系统性偏倚**：观测到支持某一特定异构体的读段的概率必须等于该异构体的真实相对丰度。这意味着RNA-seq工作流程中没有技术偏倚。任何系统性地偏好生成、测序或比对来自某一异构体的读段的过程都会违反这一假设并引入偏倚。潜在偏倚的具体来源包括：\n    -   **序列特异性偏倚**：GC含量或二级结构的差异可能导致在PCR或逆转录过程中的扩增效率不同。\n    -   **比对偏倚**：连接序列的序列复杂性或唯一性的差异可能导致来自一个异构体的读段比另一个异构体以更高或更低的置信度被比对。例如，如果一个连接的序列唯一性较低，源自它的读段可能会作为多重比对而被丢弃，从而使计数产生偏倚。\n    -   **读段长度和位置偏倚**：生成跨连接读段的效率可能取决于读段长度和剪接位点的具体位置，这可能为抽样包含与跳过连接创造出不同的“有效长度”。\n\n总之，为了使估计量 $\\hat{\\psi} = I/(I+S)$ 是无偏的，我们必须假设实验和计算流程能够无偏地反映两种剪接异构体的相对分子计数。\n\n### 第三部分：数值计算\n\n我们得到的观测计数为 $I = 152$ 和 $S = 48$。\n跨连接读段的总数为 $N = I + S = 152 + 48 = 200$。\n\n使用推导出的最大似然估计量：\n$$\\hat{\\psi} = \\frac{I}{I+S} = \\frac{152}{200}$$\n$$\\hat{\\psi} = \\frac{76}{100} = 0.76$$\n\n问题要求答案以小数形式表示，并四舍五入到四位有效数字。为了将 $0.76$ 表示为四位有效数字，我们添加尾随的零。\n$$\\hat{\\psi} = 0.7600$$\n该值代表基于所提供的读段证据，估计的盒式外显子被包含的概率。",
            "answer": "$$\\boxed{0.7600}$$"
        },
        {
            "introduction": "这项练习将从量化剪接转向从DNA序列预测剪接，并比较一种经典的生物信息学模型与一种深度学习方法。您将实现一个位置权重矩阵（Position Weight Matrix, PWM）和一个简单的卷积神经网络（Convolutional Neural Network, CNN），用以预测一个遗传变异如何影响剪接供体位点的强度。这种直接比较突出了两种模型在架构上的差异，以及CNN学习局部序列基序及其相互作用的能力，这是其相较于简单模型的一个关键优势。",
            "id": "4330978",
            "problem": "您将执行一项正式的比较任务，比较用于精准医疗和基因组诊断的两种基于序列的剪接供体强度模型：位置权重矩阵 (PWM) 和卷积神经网络 (CNN)。任务目标是计算单个核苷酸变异在指定供体位置的预测影响，并通过显式计算来调和两种模型之间的分歧。所有计算都必须从第一性原理出发，并在一个完整、可运行的程序中实现。\n\n供体位点窗口定义为一个长度为 $9$ 的核苷酸序列，覆盖相对于外显子-内含子边界的位置 $-3,-2,-1,+1,+2,+3,+4,+5,+6$，其中 $+1$ 是第一个内含子碱基，$+2$ 是第二个内含子碱基。在人类基因组中，经典的供体双核苷酸 $+1,+2$ 通常是 $G,T$，但在提供的测试用例中可能会出现任意碱基。设字母表为 $\\{A,C,G,T\\}$，采用独热编码将每个碱基映射到 $\\mathbb{R}^4$ 中的一个向量，基序为 $(A,C,G,T)$。\n\n需要实现的模型定义：\n\n1.  位置权重矩阵 (PWM) 模型。\n    *   假设：各位置之间条件独立。\n    *   分数定义：对于长度为 $9$ 的序列 $s$，PWM对数几率分数为\n        $$S_{\\text{PWM}}(s) = \\sum_{p=0}^{8} W_{p, b(s,p)} + b_0,$$\n        其中 $p$ 索引窗口位置 $0,\\dots,8$，分别对应于 $-3,-2,-1,+1,+2,+3,+4,+5,+6$，$b(s,p)\\in\\{0,1,2,3\\}$ 是序列 $s$ 中位置 $p$ 处碱基在排序 $(A,C,G,T)$下的索引，$W \\in \\mathbb{R}^{9\\times 4}$ 是对数几率贡献的位置权重矩阵，$b_0\\in\\mathbb{R}$ 是一个偏置项。变异的预测影响是分数的变化 $\\Delta_{\\text{PWM}} = S_{\\text{PWM}}(s_{\\text{alt}}) - S_{\\text{PWM}}(s_{\\text{ref}})$。\n    *   使用的参数：\n        *   偏置：$b_0 = 0$。\n        *   矩阵 $W$ 由行 $W_{p,:}$ 给出，顺序为 $(A,C,G,T)$：\n            *   $W_{0,:} = [0.2,0.0,0.0,0.0]$\n            *   $W_{1,:} = [0.0,0.0,0.2,0.0]$\n            *   $W_{2,:} = [0.0,0.0,0.5,0.0]$\n            *   $W_{3,:} = [0.0,0.0,2.5,0.0]$\n            *   $W_{4,:} = [0.0,0.0,0.0,2.5]$\n            *   $W_{5,:} = [0.3,0.0,0.0,0.0]$\n            *   $W_{6,:} = [0.0,0.0,0.2,0.0]$\n            *   $W_{7,:} = [-0.1,-0.2,0.4,-0.2]$\n            *   $W_{8,:} = [0.1,0.0,0.0,0.1]$\n\n2.  卷积神经网络 (CNN) 模型。\n    *   架构：独热输入 $X\\in\\{0,1\\}^{9\\times 4}$，两个宽度为 $6$ 且带偏置的一维卷积滤波器，修正线性单元 (ReLU)，在所有有效起始位置上的全局最大池化，然后是一个用于产生logit的线性头。令 $\\phi(x)=\\max(0,x)$ 为修正线性单元。\n    *   卷积：对于权重为 $F^{(j)}\\in \\mathbb{R}^{6\\times 4}$、偏置为 $c_j\\in \\mathbb{R}$ 的滤波器 $j\\in\\{0,1\\}$，在起始索引 $k\\in\\{0,1,2,3\\}$（覆盖位置 $k$ 到 $k+5$）处的有效卷积为\n        $$z^{(j)}_k = c_j + \\sum_{o=0}^{5} \\sum_{n=0}^{3} F^{(j)}_{o,n} \\, X_{k+o, n}.$$\n        滤波器 $j$ 的激活值为\n        $$a_j = \\max_{k\\in\\{0,1,2,3\\}} \\phi\\!\\left(z^{(j)}_k\\right)。$$\n        最终的CNN剪接强度logit为\n        $$Y_{\\text{CNN}}(s) = v_0 a_0 + v_1 a_1 + c,$$\n        其中 $v\\in\\mathbb{R}^2$ 和 $c\\in\\mathbb{R}$ 是线性头参数。预测影响为 $\\Delta_{\\text{CNN}} = Y_{\\text{CNN}}(s_{\\text{alt}}) - Y_{\\text{CNN}}(s_{\\text{ref}})$。\n    *   使用的参数：\n        *   滤波器 $0$ 的权重和偏置 $c_0$：\n            *   非零权重：在偏移量 $0$ 处对 $G$：$+2.0$；在偏移量 $1$ 处对 $T$：$+2.0$；在偏移量 $4$ 处对 $G$：$+1.0$。所有其他条目均为 $0$。\n            *   偏置：$c_0 = -2.0$。\n        *   滤波器 $1$ 的权重和偏置 $c_1$：\n            *   非零权重：在偏移量 $2$ 处对 $A$：$+1.0$；在偏移量 $3$ 处对 $A$：$+1.0$；在偏移量 $4$ 处对 $A$：$+1.5$。所有其他条目均为 $0$。\n            *   偏置：$c_1 = -2.0$。\n        *   线性头：$v = [1.0, 1.0]$，$c = 0.0$。\n\n变异定义和坐标映射：\n*   该变异是位置 $+5$ 处从 $G$ 到 $A$ 的单核苷酸变化，对应于窗口索引 $p=7$。如果索引 $p=7$ 处的参考碱基不是 $G$，那么根据定义，该变异不改变序列，预测影响为 $\\Delta_{\\text{PWM}} = 0$ 和 $\\Delta_{\\text{CNN}} = 0$。\n\n任务：\n*   对于下面的每个测试序列，计算两种模型下的参考分数和变异分数，并报告上述定义的变化 $\\Delta_{\\text{PWM}}$ 和 $\\Delta_{\\text{CNN}}$。然后，通过比较这两个变化的符号来报告两种模型在效应方向上是否一致，其中符号函数对于负数为 $-1$，对于正好为零的数为 $0$，对于正数为 $+1$。一致性定义为这些符号值相等。\n*   将所有浮点输出四舍五入到三位小数。\n\n测试套件（三个长度为 $9$ 的供体窗口，在索引 $7$ 处应用变异；碱基为来自 $\\{A,C,G,T\\}$ 的大写字母）：\n1.  案例1：$s_{\\text{ref}} =$ \"AGGGTCAGA\"。通过将索引 $7$ 从 $G$ 更改为 $A$ 获得备选序列 $s_{\\text{alt}}$。\n2.  案例2：$s_{\\text{ref}} =$ \"TAGGTAAGT\"。通过将索引 $7$ 从 $G$ 更改为 $A$ 获得备选序列 $s_{\\text{alt}}$。\n3.  案例3：$s_{\\text{ref}} =$ \"CAGGTGCAT\"。尝试将索引 $7$ 从 $G$ 更改为 $A$，但如果该碱基不是 $G$，则 $s_{\\text{alt}} = s_{\\text{ref}}$。\n\n最终输出格式：\n*   您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，顺序如下\n    $$[\\Delta_{\\text{PWM}}^{(1)}, \\Delta_{\\text{CNN}}^{(1)}, \\text{agree}^{(1)}, \\Delta_{\\text{PWM}}^{(2)}, \\Delta_{\\text{CNN}}^{(2)}, \\text{agree}^{(2)}, \\Delta_{\\text{PWM}}^{(3)}, \\Delta_{\\text{CNN}}^{(3)}, \\text{agree}^{(3)}],$$\n    其中，如果符号匹配，则 $\\text{agree}^{(i)}$ 为 $1$，否则为 $0$。所有 $\\Delta$ 值必须四舍五入到三位小数，一致性值必须是整数。不应打印任何附加文本。",
            "solution": "该问题要求对位置权重矩阵（PWM）和卷积神经网络（CNN）这两种模型进行实现和比较，以评估一个特定单核苷酸变异对剪接供体强度的影响。\n\n**PWM模型分析**\n\nPWM模型假设每个位置对总分数的贡献是独立的。分数的变化 $\\Delta_{\\text{PWM}}$ 仅取决于发生变异的位置上的权重变化。该变异是在索引 $p=7$ 处从 $G$ 变为 $A$。根据权重矩阵 $W_{7,:}=[-0.1, -0.2, 0.4, -0.2]$，我们可以计算出变化：\n$$\\Delta_{\\text{PWM}} = W_{7, \\text{A}} - W_{7, \\text{G}} = -0.1 - 0.4 = -0.6$$\n只要参考序列在索引7处是 $G$，这个变化值就是恒定的。\n\n**CNN模型分析**\n\nCNN模型更为复杂，因为它考虑了序列基序及其局部上下文。我们需要对每个参考序列和变异序列分别计算CNN的完整logit分数。\n\n**案例 1: $s_{\\text{ref}} = \\text{\"AGGGTCAGA\"}$**\n\n*   **变异**: 索引7处的碱基是 $G$，因此变异适用。$s_{\\text{alt}} = \\text{\"AGGTCAAGA\"}$。\n*   **PWM影响**: $\\Delta_{\\text{PWM}}^{(1)} = -0.6$。\n*   **CNN影响**:\n    *   **参考序列 $s_{\\text{ref}}$**:\n        *   滤波器0（检测`G`在偏移0，`T`在偏移1，`G`在偏移4）在起始位置 $k=3$（子序列`GTCAGA`）处被强烈激活，因为三个基序都匹配：$z_{3}^{(0)} = 2.0(\\text{G}) + 2.0(\\text{T}) + 1.0(\\text{G}) - 2.0 = 3.0$。其他位置激活值较低。因此，池化后激活值 $a_{0, \\text{ref}} = \\max(0, 3.0) = 3.0$。\n        *   滤波器1（检测`A`在偏移2、3、4）在任何位置都没有正激活。例如，对于 $k=2$（`GTCAG`），`A`在偏移3处匹配，得到 $z_{2}^{(1)} = 1.0 - 2.0 = -1.0$。因此 $a_{1, \\text{ref}} = 0.0$。\n        *   参考logit: $Y_{\\text{CNN}}(s_{\\text{ref}}) = 1.0 \\times a_{0, \\text{ref}} + 1.0 \\times a_{1, \\text{ref}} + 0.0 = 3.0$。\n    *   **变异序列 $s_{\\text{alt}}$**:\n        *   滤波器0：在 $k=3$ 处，`G`在偏移4处的匹配消失，激活值降为 $z_{3}^{(0)} = 2.0(\\text{G}) + 2.0(\\text{T}) - 2.0 = 2.0$。因此 $a_{0, \\text{alt}} = 2.0$。\n        *   滤波器1：在 $k=2$（`GTCAAG`）处，`A`在偏移3和4处匹配，得到 $z_{2}^{(1)} = 1.0(\\text{A@off3}) + 1.5(\\text{A@off4}) - 2.0 = 0.5$。因此 $a_{1, \\text{alt}} = 0.5$。\n        *   变异logit: $Y_{\\text{CNN}}(s_{\\text{alt}}) = 1.0 \\times 2.0 + 1.0 \\times 0.5 + 0.0 = 2.5$。\n    *   $\\Delta_{\\text{CNN}}^{(1)} = 2.5 - 3.0 = -0.5$。\n*   **一致性**: $\\text{sign}(-0.6) = -1$ 和 $\\text{sign}(-0.5) = -1$。符号匹配，$\\text{agree}^{(1)}=1$。\n\n**案例 2: $s_{\\text{ref}} = \\text{\"TAGGTAAGT\"}$**\n\n*   **变异**: 索引7处的碱基是 $G$，因此变异适用。$s_{\\text{alt}} = \\text{\"TAGGTAAAT\"}$。\n*   **PWM影响**: $\\Delta_{\\text{PWM}}^{(2)} = -0.6$。\n*   **CNN影响**:\n    *   **参考序列 $s_{\\text{ref}}$**:\n        *   滤波器0：在 $k=3$（`GTAAGT`）处，`G`在偏移0，`T`在偏移1，以及`G`在偏移4（即序列中的`G`在索引7处）匹配，得到 $z_{3}^{(0)} = 2.0 + 2.0 + 1.0 - 2.0 = 3.0$。因此 $a_{0, \\text{ref}} = 3.0$。\n        *   滤波器1：在任何位置都没有正激活。例如，在 $k=1$（`AGGTAA`）处，`A`在偏移4处匹配，得到 $z_{1}^{(1)} = 1.5 - 2.0 = -0.5$。因此 $a_{1, \\text{ref}} = 0.0$。\n        *   参考logit: $Y_{\\text{CNN}}(s_{\\text{ref}}) = 1.0 \\times 3.0 + 1.0 \\times 0.0 + 0.0 = 3.0$。\n    *   **变异序列 $s_{\\text{alt}}$**:\n        *   滤波器0：在 $k=3$ 处，`G`在偏移4处的匹配消失，激活值降为 $z_{3}^{(0)} = 2.0(\\text{G}) + 2.0(\\text{T}) - 2.0 = 2.0$。因此 $a_{0, \\text{alt}} = 2.0$。\n        *   滤波器1：在 $k=2$（`GGTAAA`）处，`A`在偏移3和4处匹配，得到 $z_{2}^{(1)} = 1.0 + 1.5 - 2.0 = 0.5$。因此 $a_{1, \\text{alt}} = 0.5$。\n        *   变异logit: $Y_{\\text{CNN}}(s_{\\text{alt}}) = 1.0 \\times 2.0 + 1.0 \\times 0.5 + 0.0 = 2.5$。\n    *   $\\Delta_{\\text{CNN}}^{(2)} = 2.5 - 3.0 = -0.5$。\n*   **一致性**: $\\text{sign}(-0.6) = -1$ 和 $\\text{sign}(-0.5) = -1$。符号匹配，$\\text{agree}^{(2)}=1$。这个例子说明，即使两个序列的CNN分数不同，变异引起的差异也可能因上下文而异，这与PWM的恒定差异形成对比。\n\n**案例 3: $s_{\\text{ref}} = \\text{\"CAGGTGCAT\"}$**\n\n*   **变异**: 索引7处的碱基是 `A`，不是 `G`。根据问题定义，变异不适用，序列不改变。\n*   **PWM影响**: $\\Delta_{\\text{PWM}}^{(3)} = 0.0$。\n*   **CNN影响**: $\\Delta_{\\text{CNN}}^{(3)} = 0.0$。\n*   **一致性**: $\\text{sign}(0.0) = 0$ 和 $\\text{sign}(0.0) = 0$。符号匹配，$\\text{agree}^{(3)}=1$。\n\n**总结**\n\n| 案例 | $\\Delta_{\\text{PWM}}$ | $\\Delta_{\\text{CNN}}$ | 一致性 |\n|:----|:-----------|:-----------|:----------|\n| 1   | -0.600     | -0.500     | 1         |\n| 2   | -0.600     | -0.500     | 1         |\n| 3   | 0.000      | 0.000      | 1         |\n\n最终结果将按照指定的格式排列。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the model comparison problem for splice site variants.\n    \"\"\"\n    # Define problem parameters\n    BASE_TO_IDX = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n    \n    # PWM model parameters\n    W_PWM = np.array([\n        [0.2, 0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.2, 0.0],\n        [0.0, 0.0, 0.5, 0.0],\n        [0.0, 0.0, 2.5, 0.0],\n        [0.0, 0.0, 0.0, 2.5],\n        [0.3, 0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.2, 0.0],\n        [-0.1, -0.2, 0.4, -0.2],\n        [0.1, 0.0, 0.0, 0.1]\n    ])\n\n    # CNN model parameters\n    # Filter 0\n    F0_CNN = np.zeros((6, 4))\n    F0_CNN[0, BASE_TO_IDX['G']] = 2.0\n    F0_CNN[1, BASE_TO_IDX['T']] = 2.0\n    F0_CNN[4, BASE_TO_IDX['G']] = 1.0\n    C0_CNN = -2.0\n\n    # Filter 1\n    F1_CNN = np.zeros((6, 4))\n    F1_CNN[2, BASE_TO_IDX['A']] = 1.0\n    F1_CNN[3, BASE_TO_IDX['A']] = 1.0\n    F1_CNN[4, BASE_TO_IDX['A']] = 1.5\n    C1_CNN = -2.0\n\n    # Linear Head\n    V_CNN = np.array([1.0, 1.0])\n    C_HEAD_CNN = 0.0\n\n    def one_hot_encode(seq: str) -> np.ndarray:\n        \"\"\"Converts a DNA sequence to its one-hot encoded representation.\"\"\"\n        encoding = np.zeros((len(seq), 4), dtype=np.int8)\n        for i, base in enumerate(seq):\n            if base in BASE_TO_IDX:\n                encoding[i, BASE_TO_IDX[base]] = 1\n        return encoding\n\n    def get_cnn_score(seq_one_hot: np.ndarray) -> float:\n        \"\"\"Calculates the CNN logit for a given one-hot encoded sequence.\"\"\"\n        num_positions = seq_one_hot.shape[0]\n        filter_width = 6\n        num_windows = num_positions - filter_width + 1\n\n        # Filter 0 convolution, ReLU, and max-pooling\n        z0 = np.zeros(num_windows)\n        for k in range(num_windows):\n            window = seq_one_hot[k:k+filter_width, :]\n            z0[k] = C0_CNN + np.sum(F0_CNN * window)\n        a0 = np.max(np.maximum(0, z0))\n\n        # Filter 1 convolution, ReLU, and max-pooling\n        z1 = np.zeros(num_windows)\n        for k in range(num_windows):\n            window = seq_one_hot[k:k+filter_width, :]\n            z1[k] = C1_CNN + np.sum(F1_CNN * window)\n        a1 = np.max(np.maximum(0, z1))\n\n        # Linear head\n        logit = V_CNN[0] * a0 + V_CNN[1] * a1 + C_HEAD_CNN\n        return logit\n\n    test_cases = [\n        \"AGGGTCAGA\",\n        \"TAGGTAAGT\",\n        \"CAGGTGCAT\",\n    ]\n\n    results = []\n    for s_ref_str in test_cases:\n        variant_pos = 7\n        ref_base = 'G'\n        alt_base = 'A'\n\n        if s_ref_str[variant_pos] != ref_base:\n            delta_pwm = 0.0\n            delta_cnn = 0.0\n        else:\n            s_alt_str = s_ref_str[:variant_pos] + alt_base + s_ref_str[variant_pos+1:]\n\n            # PWM delta calculation\n            delta_pwm = W_PWM[variant_pos, BASE_TO_IDX[alt_base]] - W_PWM[variant_pos, BASE_TO_IDX[ref_base]]\n            \n            # CNN delta calculation\n            s_ref_one_hot = one_hot_encode(s_ref_str)\n            s_alt_one_hot = one_hot_encode(s_alt_str)\n            \n            score_ref = get_cnn_score(s_ref_one_hot)\n            score_alt = get_cnn_score(s_alt_one_hot)\n            delta_cnn = score_alt - score_ref\n        \n        # Agreement calculation\n        sign_pwm = np.sign(delta_pwm)\n        sign_cnn = np.sign(delta_cnn)\n        agreement = 1 if sign_pwm == sign_cnn else 0\n\n        # Append formatted results\n        results.append(f\"{delta_pwm:.3f}\")\n        results.append(f\"{delta_cnn:.3f}\")\n        results.append(str(agreement))\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在构建了预测模型之后，一个关键步骤是理解其决策过程和稳健性，这在诊断等高风险应用中尤为重要。这项练习引入了对抗性攻击的概念，挑战您去寻找能够将模型预测从“剪接位点”翻转为“非剪接位点”（反之亦然）的最小序列改变。通过开发一种贪心算法来构建这些最小扰动，您将深入洞察模型的敏感性以及驱动其预测的特征权重。",
            "id": "4330944",
            "problem": "给定一个用于脱氧核糖核酸（DNA）序列上剪接位点检测的二元分类器，该分类器模型为一个线性logit函数后接一个sigmoid函数。一段固定长度为 $L$ 的DNA序列由字母表 $\\mathcal{A}=\\{\\text{A},\\text{C},\\text{G},\\text{T}\\}$ 表示。每个序列在固定的碱基顺序 $(\\text{A},\\text{C},\\text{G},\\text{T})$ 下被编码为一个独热编码矩阵 $\\mathbf{X}\\in\\{0,1\\}^{L\\times 4}$，因此在位置 $i\\in\\{0,\\dots,L-1\\}$，四个条目中恰好有一个为1。该分类器有一个权重矩阵 $\\mathbf{W}\\in\\mathbb{R}^{L\\times 4}$ 和一个标量偏置 $b\\in\\mathbb{R}$。对于一个输入序列 $\\mathbf{X}$，其logit为\n$$\nz(\\mathbf{X})=\\sum_{i=0}^{L-1}\\sum_{j=0}^{3} W_{i,j}\\,X_{i,j} + b,\n$$\n预测概率为 $p(\\mathbf{X})=\\sigma\\!\\left(z(\\mathbf{X})\\right)$，其中 $\\sigma(u)=\\frac{1}{1+\\exp(-u)}$ 是sigmoid函数。预测的标签为\n$$\n\\hat{y}(\\mathbf{X})=\\begin{cases}\n1  \\text{若 } p(\\mathbf{X})0.5 \\text{ (等价于 } z(\\mathbf{X})0\\text{)},\\\\\n0  \\text{其他情况 (等价于 } z(\\mathbf{X})\\le 0\\text{).}\n\\end{cases}\n$$\n在位置 $i$ 处的一次对抗性核苷酸替换，将位置 $i$ 的当前碱基 $a\\in\\mathcal{A}$ 更改为任何其他碱基 $b\\in\\mathcal{A}\\setminus\\{a\\}$，这会精确地改变 $\\mathbf{X}$ 的一行，同时保持独热编码的约束。原始序列和突变序列之间的汉明距离 $d_H$ 是被改变位置的数量。\n\n您的任务是构建能够翻转模型预测的最小对抗性替换。形式上，给定 $\\mathbf{W}$、$b$ 和一个序列 $\\mathbf{X}$，找到最小的整数 $k\\in\\mathbb{N}$ 和一个与 $\\mathbf{X}$ 在恰好 $k$ 个位置上不同的序列 $\\mathbf{X}'$（即 $d_H(\\mathbf{X},\\mathbf{X}')=k$），使得 $\\hat{y}(\\mathbf{X}')\\neq \\hat{y}(\\mathbf{X})$。在所有具有相同最小 $k$ 值的序列中，选择任何一个有效的 $\\mathbf{X}'$。如果不存在这样的 $\\mathbf{X}'$，则定义 $k=-1$ 并报告失败。\n\n实现一个算法，为每个测试序列计算：\n- 翻转预测所需的最少替换次数 $k$。\n- 一个布尔值，指示所构建的突变序列是否确实在该模型下翻转了预测。\n\n该算法必须仅依赖于给定的线性logit、独热编码以及替换对logit的加性效应。不要对所有可能的多位置突变进行穷举搜索；相反，应从线性模型和汉明距离的基本原理推导出最优的构建规则。\n\n使用以下固定的模型和测试套件：\n- 序列长度 $L=12$。\n- 偏置 $b=-2.0$。\n- 权重矩阵 $\\mathbf{W}$ 除以下位置外均为零：\n    - 在位置 $i=4$，碱基 $\\text{G}$ 的权重为 $+3.0$，碱基 $\\{\\text{A},\\text{C},\\text{T}\\}$ 的权重为 $-1.0$。\n    - 在位置 $i=5$，碱基 $\\text{T}$ 的权重为 $+3.0$，碱基 $\\{\\text{A},\\text{C},\\text{G}\\}$ 的权重为 $-1.0$。\n    - 在位置 $i=0$，碱基 $\\text{A}$ 的权重为 $+1.0$，碱基 $\\{\\text{C},\\text{G},\\text{T}\\}$ 的权重为 $0.0$。\n- $\\mathbf{W}$ 的所有其他条目均为 $0.0$。\n- 碱基顺序映射为 $(\\text{A},\\text{C},\\text{G},\\text{T})\\mapsto (0,1,2,3)$。\n\n测试序列（长度均为12）：\n- 案例1（阳性，易于翻转）：$s_1=\\text{\"CCCCGTCCCCCC\"}$。\n- 案例2（阴性，需要两次编辑）：$s_2=\\text{\"CCCCCACCCCCC\"}$。\n- 案例3（边界阴性，需要一次编辑）：$s_3=\\text{\"CCCCGACCCCCC\"}$。\n- 案例4（强阳性，需要两次编辑）：$s_4=\\text{\"ACCCGTCCCCCC\"}$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个元素都是对应测试案例的 $[k,\\text{flip}]$ 形式的双元素列表，其中 $k$ 是一个整数，$\\text{flip}$ 是一个布尔值。例如，输出格式必须完全像\n$$\n\\text{[[1,True],[2,False],[0,True],[3,True]]}\n$$\n但使用为所提供测试套件计算的实际值。不需要物理单位。不使用角度。不使用百分比。所有测试案例的最终输出必须只能表示为布尔值和整数。",
            "solution": "该问题的核心是为线性分类器找到最小的对抗性扰动，即翻转模型预测所需的最少核苷酸替换次数。由于模型是线性的，每次替换对logit分数的影响是独立且可加的。这使得我们可以采用贪心算法来解决。\n\n**算法思路**\n\n1.  **计算初始状态**：对于每个输入序列，首先计算其原始logit分数 $z_{\\text{orig}}$ 和预测标签 $\\hat{y}_{\\text{orig}}$。\n2.  **确定目标**：如果原始预测为1（$z_{\\text{orig}}  0$），目标是使logit变为 $z_{\\text{new}} \\le 0$。如果原始预测为0（$z_{\\text{orig}} \\le 0$），目标是使logit变为 $z_{\\text{new}}  0$。\n3.  **计算所有可能的单步变化**：对于序列中的每个位置，计算将当前碱基替换为其他三种碱基中任意一种所能带来的logit变化量 $\\Delta z$。$\\Delta z = W_{i, \\text{new}} - W_{i, \\text{orig}}$。\n4.  **贪心选择**：\n    *   如果目标是降低logit，我们找出所有可能的负向 $\\Delta z$ 值，并按升序（从最负的开始）排序。\n    *   如果目标是增加logit，我们找出所有可能的正向 $\\Delta z$ 值，并按降序（从最正的开始）排序。\n    这些排序后的 $\\Delta z$ 值代表了最有效的单步攻击。\n5.  **累加变化并确定最小k**：从 $z_{\\text{orig}}$ 开始，依次累加上述排序列表中的 $\\Delta z$ 值，直到logit分数穿过决策边界（0）。所用替换的次数即为最小的 $k$。\n6.  **验证**：构建应用了前 $k$ 次最优替换的序列，并重新计算其logit，以验证预测是否确实被翻转。\n\n**案例分析**\n\n-   **案例1 ($s_1=\\text{\"CCCCGTCCCCCC\"}$)**:\n    -   $z_{\\text{orig}} = W_{4,G} + W_{5,T} + b = 3.0 + 3.0 - 2.0 = 4.0$。预测为1。\n    -   目标：使 $z \\le 0$ (需减小至少 $4.0$)。\n    -   最优单步变化：将位置4的G变为C（$\\Delta z = -1 - 3 = -4.0$）或将位置5的T变为C（$\\Delta z = -1 - 3 = -4.0$）。\n    -   一次替换就足以使 $z_{\\text{new}} = 4.0 - 4.0 = 0.0$，从而翻转预测。\n    -   结果: $k=1$, 翻转成功。\n\n-   **案例2 ($s_2=\\text{\"CCCCCACCCCCC\"}$)**:\n    -   $z_{\\text{orig}} = W_{4,C} + W_{5,A} + b = -1.0 - 1.0 - 2.0 = -4.0$。预测为0。\n    -   目标：使 $z > 0$ (需增加超过 $4.0$)。\n    -   最优单步变化：将位置4的C变为G（$\\Delta z = 3 - (-1) = 4.0$）或将位置5的A变为T（$\\Delta z = 3 - (-1) = 4.0$）。\n    -   一次替换后 $z_{\\text{new}} = -4.0 + 4.0 = 0.0$，预测仍为0。\n    -   需要两次替换：$z_{\\text{new}} = -4.0 + 4.0 + 4.0 = 4.0$，预测翻转为1。\n    -   结果: $k=2$, 翻转成功。\n\n-   **案例3 ($s_3=\\text{\"CCCCGACCCCCC\"}$)**:\n    -   $z_{\\text{orig}} = W_{4,G} + W_{5,A} + b = 3.0 - 1.0 - 2.0 = 0.0$。预测为0。\n    -   目标：使 $z > 0$ (需增加任意正值)。\n    -   最优单步变化：将位置5的A变为T（$\\Delta z = 4.0$）。\n    -   一次替换后 $z_{\\text{new}} = 0.0 + 4.0 = 4.0$，预测翻转为1。\n    -   结果: $k=1$, 翻转成功。\n\n-   **案例4 ($s_4=\\text{\"ACCCGTCCCCCC\"}$)**:\n    -   $z_{\\text{orig}} = W_{0,A} + W_{4,G} + W_{5,T} + b = 1.0 + 3.0 + 3.0 - 2.0 = 5.0$。预测为1。\n    -   目标：使 $z \\le 0$ (需减小至少 $5.0$)。\n    -   最优单步变化：$\\Delta z = -4.0$（在位置4或5）。\n    -   一次替换后 $z_{\\text{new}} = 5.0 - 4.0 = 1.0$，预测未翻转。\n    -   需要两次替换：$z_{\\text{new}} = 5.0 - 4.0 - 4.0 = -3.0$，预测翻转为0。\n    -   结果: $k=2$, 翻转成功。\n\n**最终结果**\n\n根据上述分析，每个案例的最小替换次数和验证结果如下：\n1.  $[1, \\text{True}]$\n2.  $[2, \\text{True}]$\n3.  $[1, \\text{True}]$\n4.  $[2, \\text{True}]$",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the minimal adversarial substitutions for a linear DNA classifier.\n    \"\"\"\n    # Define model parameters and problem constants\n    L = 12\n    b = -2.0\n    base_map = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n    bases = ['A', 'C', 'G', 'T']\n    \n    # Initialize weight matrix W\n    W = np.zeros((L, 4))\n    \n    # Populate non-zero weights as specified\n    # Position 0: A is +1.0, others 0.0\n    W[0, base_map['A']] = 1.0\n    \n    # Position 4: G is +3.0, others -1.0\n    W[4, :] = -1.0\n    W[4, base_map['G']] = 3.0\n    \n    # Position 5: T is +3.0, others -1.0\n    W[5, :] = -1.0\n    W[5, base_map['T']] = 3.0\n    \n    # Test cases\n    test_cases = [\n        \"CCCCGTCCCCCC\",  # Case 1\n        \"CCCCCACCCCCC\",  # Case 2\n        \"CCCCGACCCCCC\",  # Case 3\n        \"ACCCGTCCCCCC\",  # Case 4\n    ]\n\n    results = []\n\n    def calculate_logit(sequence, W_mat, b_val, b_map):\n        \"\"\"Calculates the logit for a given DNA sequence.\"\"\"\n        logit = b_val\n        for i, base in enumerate(sequence):\n            logit += W_mat[i, b_map[base]]\n        return logit\n\n    for seq in test_cases:\n        # 1. Calculate initial logit and prediction\n        z_orig = calculate_logit(seq, W, b, base_map)\n        y_orig_is_1 = z_orig > 0\n\n        # 2. Determine goal: increase or decrease logit\n        needs_increase = not y_orig_is_1 # wants to flip 0 -> 1, needs z > 0\n\n        # 3. Identify all optimal single-position changes\n        best_changes = []\n        for i in range(L):\n            orig_base = seq[i]\n            w_orig = W[i, base_map[orig_base]]\n            \n            position_best_delta = 0\n            position_best_new_base = ''\n\n            if needs_increase:\n                # Find the substitution that gives the maximum positive delta_z\n                max_delta_z = -np.inf\n                for new_base in bases:\n                    if new_base != orig_base:\n                        w_new = W[i, base_map[new_base]]\n                        delta_z = w_new - w_orig\n                        if delta_z > max_delta_z:\n                            max_delta_z = delta_z\n                            position_best_new_base = new_base\n                if max_delta_z > 0:\n                    best_changes.append((max_delta_z, i, position_best_new_base))\n            else: # needs decrease\n                # Find the substitution that gives the maximum negative delta_z\n                min_delta_z = np.inf\n                for new_base in bases:\n                    if new_base != orig_base:\n                        w_new = W[i, base_map[new_base]]\n                        delta_z = w_new - w_orig\n                        if delta_z  min_delta_z:\n                            min_delta_z = delta_z\n                            position_best_new_base = new_base\n                if min_delta_z  0:\n                    best_changes.append((min_delta_z, i, position_best_new_base))\n        \n        # 4. Sort changes greedily\n        if needs_increase:\n            best_changes.sort(key=lambda x: x[0], reverse=True) # Descending\n        else:\n            best_changes.sort(key=lambda x: x[0]) # Ascending\n\n        # 5. Accumulate changes to find minimal k\n        min_k = -1\n        current_z = z_orig\n        \n        for i in range(len(best_changes)):\n            delta_z, _, _ = best_changes[i]\n            current_z += delta_z\n            \n            flipped = False\n            if needs_increase and current_z > 0:\n                flipped = True\n            elif not needs_increase and current_z = 0:\n                flipped = True\n            \n            if flipped:\n                min_k = i + 1\n                break\n        \n        # 6. Verify the flip with the constructed sequence\n        flip_verified = False\n        if min_k != -1:\n            mutations_to_apply = best_changes[:min_k]\n            seq_list = list(seq)\n            for _, pos_idx, new_base in mutations_to_apply:\n                seq_list[pos_idx] = new_base\n            \n            mutated_seq = \"\".join(seq_list)\n            z_new = calculate_logit(mutated_seq, W, b, base_map)\n            y_new_is_1 = z_new > 0\n            \n            if y_new_is_1 != y_orig_is_1:\n                flip_verified = True\n\n        results.append([min_k, flip_verified])\n\n    # Format the final output string\n    output_str = \"[\" + \",\".join([f\"[{k},{str(v)}]\" for k, v in results]) + \"]\"\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}