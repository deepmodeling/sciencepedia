{
    "hands_on_practices": [
        {
            "introduction": "深度学习模型为变异致病性预测提供了概率分数，但在临床应用中，我们通常需要一个明确的“致病”或“良性”的判断。这就要求我们选择一个最佳的决策阈值来平衡模型的精确率和召回率。本实践将指导你基于给定的分类器分数分布，从第一性原理出发推导出最大化 $F_1$ 分数的最佳操作阈值，这是将模型输出转化为可操作临床决策的关键一步。",
            "id": "4330573",
            "problem": "在一个用于基因组诊断的精准医疗流程中，一个深度学习分类器会输出一个校准后的后验概率 $p \\in [0,1]$，该概率表示在给定观测到的分子特征的条件下，某个单核苷酸变异是真正致病的。考虑一个组成均衡的留出评估队列，其中真正的致病性变异和真正的良性变异数量相等。令 $Y \\in \\{1,0\\}$ 表示真实类别（$Y=1$ 为致病性，$Y=0$ 为良性），并假设该模型的校准分数具有以下类条件分布：\n- 对于致病性变异（$Y=1$），$p \\mid Y=1 \\sim \\operatorname{Beta}(2,1)$。\n- 对于良性变异（$Y=0$），$p \\mid Y=0 \\sim \\operatorname{Beta}(1,2)$。\n\n假设 Beta 分布的概率密度函数为 $f(p; a,b) = \\frac{1}{B(a,b)} p^{a-1} (1-p)^{b-1}$（对于 $0 \\leq p \\leq 1$），其中 $B(a,b)$ 是 Beta 函数。同时假设，校准意味着对于任何决策阈值 $\\tau \\in [0,1]$，以下基本定义成立：\n- 真阳性率（召回率）为 $\\operatorname{TPR}(\\tau) = \\mathbb{P}(p \\geq \\tau \\mid Y=1)$。\n- 假阳性率是 $\\operatorname{FPR}(\\tau) = \\mathbb{P}(p \\geq \\tau \\mid Y=0)$。\n- 发生率为 $\\pi = \\mathbb{P}(Y=1)$，根据均衡队列设计，此处等于 $\\pi = \\frac{1}{2}$。\n- 在阈值 $\\tau$ 下的精确率（阳性预测值）是 $\\operatorname{PPV}(\\tau) = \\mathbb{P}(Y=1 \\mid p \\geq \\tau)$，它通过贝叶斯法则由上述概率和 $\\pi$ 得出。\n- 在 $\\tau$ 下的 $F_{1}$ 分数是精确率和召回率的调和平均数，$F_{1}(\\tau) = \\frac{2 \\,\\operatorname{PPV}(\\tau)\\,\\operatorname{TPR}(\\tau)}{\\operatorname{PPV}(\\tau) + \\operatorname{TPR}(\\tau)}$。\n\n用于变异分类的决策规则是：当且仅当 $p \\geq \\tau$ 时，将一个变异判定为“致病性”。仅使用这些基本定义和给定的分数分布，从第一性原理推导出能使该队列的 $F_{1}$ 分数最大化的操作阈值 $\\tau^{\\star}$。将最终结果以 $\\tau^{\\star}$ 的单个精确解析表达式形式给出，不带单位。如果你选择给出数值近似值，也必须包含其精确形式；除非你明确同时提供两者，否则不要四舍五入，并且不要包含任何单位。最终答案必须是单个数字或单个闭式表达式。",
            "solution": "问题陈述具有科学依据，定义明确，客观且内部一致。所有给出的定义和分布都是统计学和机器学习中的标准概念，基因组诊断的背景设定也是恰当的。分类器分数的分布不仅有效，而且对于给定的均衡队列满足完美校准的条件，这是一个优美的性质。因此，该问题是有效的，我们可以着手求解。\n\n目标是找到使 $F_{1}$ 分数最大化的操作阈值 $\\tau^{\\star} \\in [0,1]$。$F_{1}$ 分数的定义如下：\n$$F_{1}(\\tau) = \\frac{2 \\,\\operatorname{PPV}(\\tau)\\,\\operatorname{TPR}(\\tau)}{\\operatorname{PPV}(\\tau) + \\operatorname{TPR}(\\tau)}$$\n其中 $\\operatorname{PPV}(\\tau)$ 是在决策阈值 $\\tau$ 下的精确率（阳性预测值），$\\operatorname{TPR}(\\tau)$ 是真阳性率（召回率）。\n\n首先，我们推导真阳性率 $\\operatorname{TPR}(\\tau)$ 的表达式。根据定义，$\\operatorname{TPR}(\\tau) = \\mathbb{P}(p \\geq \\tau \\mid Y=1)$。致病性变异（$Y=1$）的分数服从分布 $p \\mid Y=1 \\sim \\operatorname{Beta}(2,1)$。\n$\\operatorname{Beta}(a,b)$ 分布的概率密度函数 (PDF) 为 $f(p; a,b) = \\frac{1}{B(a,b)} p^{a-1} (1-p)^{b-1}$。\nBeta 函数 $B(a,b)$ 由 $\\int_{0}^{1} x^{a-1}(1-x)^{b-1} dx$ 给出。\n对于 $a=2$ 和 $b=1$，$B(2,1) = \\int_{0}^{1} p^{2-1}(1-p)^{1-1} dp = \\int_{0}^{1} p \\,dp = [\\frac{p^2}{2}]_{0}^{1} = \\frac{1}{2}$。\n因此，致病性变异的 PDF 为 $f(p \\mid Y=1) = \\frac{p^{1}}{1/2} = 2p$ (对于 $p \\in [0,1]$)。\n现在我们可以计算 $\\operatorname{TPR}(\\tau)$：\n$$\\operatorname{TPR}(\\tau) = \\int_{\\tau}^{1} f(p \\mid Y=1) \\,dp = \\int_{\\tau}^{1} 2p \\,dp = [p^2]_{\\tau}^{1} = 1 - \\tau^2$$\n\n接下来，我们推导精确率 $\\operatorname{PPV}(\\tau) = \\mathbb{P}(Y=1 \\mid p \\geq \\tau)$ 的表达式。使用贝叶斯法则和全概率定律：\n$$\\operatorname{PPV}(\\tau) = \\frac{\\mathbb{P}(p \\geq \\tau \\mid Y=1)\\mathbb{P}(Y=1)}{\\mathbb{P}(p \\geq \\tau \\mid Y=1)\\mathbbP(Y=1) + \\mathbb{P}(p \\geq \\tau \\mid Y=0)\\mathbb{P}(Y=0)}$$\n这等价于：\n$$\\operatorname{PPV}(\\tau) = \\frac{\\operatorname{TPR}(\\tau)\\pi}{\\operatorname{TPR}(\\tau)\\pi + \\operatorname{FPR}(\\tau)(1-\\pi)}$$\n给定发生率 $\\pi = \\mathbb{P}(Y=1) = \\frac{1}{2}$。我们需要计算假阳性率 $\\operatorname{FPR}(\\tau) = \\mathbb{P}(p \\geq \\tau \\mid Y=0)$。\n良性变异（$Y=0$）的分数服从分布 $p \\mid Y=0 \\sim \\operatorname{Beta}(1,2)$。\n对于 $a=1$ 和 $b=2$，$B(1,2) = \\int_{0}^{1} p^{1-1}(1-p)^{2-1} dp = \\int_{0}^{1} (1-p) \\,dp = [p - \\frac{p^2}{2}]_{0}^{1} = 1 - \\frac{1}{2} = \\frac{1}{2}$。\n良性变异的 PDF 为 $f(p \\mid Y=0) = \\frac{(1-p)^{1}}{1/2} = 2(1-p)$ (对于 $p \\in [0,1]$)。\n现在我们可以计算 $\\operatorname{FPR}(\\tau)$：\n$$\\operatorname{FPR}(\\tau) = \\int_{\\tau}^{1} f(p \\mid Y=0) \\,dp = \\int_{\\tau}^{1} 2(1-p) \\,dp = 2[p - \\frac{p^2}{2}]_{\\tau}^{1} = 2\\left( (1-\\frac{1}{2}) - (\\tau - \\frac{\\tau^2}{2}) \\right) = 2(\\frac{1}{2} - \\tau + \\frac{\\tau^2}{2}) = 1 - 2\\tau + \\tau^2 = (1-\\tau)^2$$\n现在，我们将 $\\operatorname{TPR}(\\tau)$，$\\operatorname{FPR}(\\tau)$ 和 $\\pi$ 代入 $\\operatorname{PPV}(\\tau)$ 的表达式中：\n$$\\operatorname{PPV}(\\tau) = \\frac{(1-\\tau^2)(\\frac{1}{2})}{(1-\\tau^2)(\\frac{1}{2}) + (1-\\tau)^2(\\frac{1}{2})} = \\frac{1-\\tau^2}{1-\\tau^2 + (1-\\tau)^2}$$\n对于 $\\tau \\neq 1$，我们可以因式分解 $1-\\tau^2 = (1-\\tau)(1+\\tau)$ 和 $(1-\\tau)^2 = (1-\\tau)(1-\\tau)$：\n$$\\operatorname{PPV}(\\tau) = \\frac{(1-\\tau)(1+\\tau)}{(1-\\tau)(1+\\tau) + (1-\\tau)(1-\\tau)} = \\frac{1+\\tau}{(1+\\tau) + (1-\\tau)} = \\frac{1+\\tau}{2}$$\n\n现在我们可以将 $F_1(\\tau)$ 完全写成关于 $\\tau$ 的函数：\n$$F_{1}(\\tau) = \\frac{2 \\left(\\frac{1+\\tau}{2}\\right)(1-\\tau^2)}{\\frac{1+\\tau}{2} + (1-\\tau^2)} = \\frac{(1+\\tau)(1-\\tau^2)}{\\frac{1+\\tau+2(1-\\tau^2)}{2}} = \\frac{2(1+\\tau)(1-\\tau^2)}{1+\\tau+2-2\\tau^2} = \\frac{2(1+\\tau)(1-\\tau)(1+\\tau)}{-2\\tau^2+\\tau+3}$$\n分母可以因式分解为 $-2\\tau^2+\\tau+3 = (3-2\\tau)(1+\\tau)$。对于 $\\tau \\in [0,1)$， $1+\\tau \\neq 0$，因此我们可以化简：\n$$F_{1}(\\tau) = \\frac{2(1+\\tau)(1-\\tau)(1+\\tau)}{(3-2\\tau)(1+\\tau)} = \\frac{2(1-\\tau)(1+\\tau)}{3-2\\tau} = \\frac{2(1-\\tau^2)}{3-2\\tau}$$\n为了找到最大值，我们计算 $F_{1}(\\tau)$ 关于 $\\tau$ 的导数，并令其为 0。使用商法则：\n$$\\frac{dF_1}{d\\tau} = 2 \\frac{(-2\\tau)(3-2\\tau) - (1-\\tau^2)(-2)}{(3-2\\tau)^2} = 2 \\frac{-6\\tau+4\\tau^2+2-2\\tau^2}{(3-2\\tau)^2} = \\frac{2(2\\tau^2-6\\tau+2)}{(3-2\\tau)^2} = \\frac{4(\\tau^2-3\\tau+1)}{(3-2\\tau)^2}$$\n要使导数为零，分子必须为零：\n$$\\tau^2 - 3\\tau + 1 = 0$$\n我们解这个关于 $\\tau$ 的二次方程：\n$$\\tau = \\frac{-(-3) \\pm \\sqrt{(-3)^2 - 4(1)(1)}}{2(1)} = \\frac{3 \\pm \\sqrt{9-4}}{2} = \\frac{3 \\pm \\sqrt{5}}{2}$$\n这给出了两个潜在解：$\\tau_1 = \\frac{3 + \\sqrt{5}}{2}$ 和 $\\tau_2 = \\frac{3 - \\sqrt{5}}{2}$。\n$\\sqrt{5}$ 的值约等于 $2.236$。\n$\\tau_1 \\approx \\frac{3+2.236}{2} \\approx 2.618$。该值超出了概率阈值的有效域 $[0,1]$。\n$\\tau_2 \\approx \\frac{3-2.236}{2} \\approx 0.382$。该值在定义域 $[0,1]$ 内。\n因此，有效范围内的唯一临界点是 $\\tau^{\\star} = \\frac{3 - \\sqrt{5}}{2}$。\n\n为了确认这是一个最大值，我们可以检查一阶导数的符号。分母 $(3-2\\tau)^2$ 在 $\\tau \\neq 3/2$ 时恒为正。导数的符号由分子 $g(\\tau) = \\tau^2-3\\tau+1$ 决定，这是一个开口向上的抛物线。当 $\\tau  \\tau_2$ 时，$g(\\tau) > 0$ (例如 $g(0)=1$)，所以 $F_1(\\tau)$ 是递增的。当 $\\tau > \\tau_2$ (但小于 $\\tau_1$) 时，$g(\\tau)  0$，所以 $F_1(\\tau)$ 是递减的。这证实了 $\\tau^{\\star} = \\frac{3 - \\sqrt{5}}{2}$ 对应一个局部最大值。由于它是区间 $[0,1)$ 内唯一的临界点，并且 $F_1(0)=2/3 \\approx 0.667$ 和 $F_1(1)=0$，所以这个局部最大值就是该区间上的全局最大值。\n\n使 $F_1$ 分数最大化的最优阈值 $\\tau^{\\star}$ 是 $\\frac{3 - \\sqrt{5}}{2}$。",
            "answer": "$$\\boxed{\\frac{3 - \\sqrt{5}}{2}}$$"
        },
        {
            "introduction": "在真实的基因组学研究中，数据往往来自不同的实验批次，而这些批次效应可能会引入系统性的特征扭曲，从而影响模型的性能和泛化能力。本实践通过一个模拟场景，让你亲手构建一个包含批次效应的数据生成过程，并实现一个包含批次特异性标准化和截距的正则化逻辑回归模型来校正这些效应。这项练习对于理解和处理实际测序数据中的常见混杂因素至关重要。",
            "id": "4330521",
            "problem": "您的任务是构建一个基于原则的模拟和估计器，以研究批次效应和分析偏差对一个用于预测变异致病性的深度学习分类器的影响。需要建模的核心现象是，测量流程会在特征空间和标签的对数几率中引入批次特有的失真。设定如下。\n\n令特征向量为 $x \\in \\mathbb{R}^d$，批次索引为 $b \\in \\{0,1,\\dots,B-1\\}$。对于批次 $b$，假设存在一个仿射特征失真，该失真由一个加性均值偏移 $\\mu_b \\in \\mathbb{R}^d$ 和一个分量级乘性尺度向量 $s_b \\in \\mathbb{R}^d$（其元素严格为正）组成。假设生物信号由一个真实的权重向量 $w_{\\text{true}} \\in \\mathbb{R}^d$ 和一个批次特有的分析偏差（在对数几率中）参数化，该偏差由 $\\alpha_{\\text{true}} \\in \\mathbb{R}^B$ 参数化。批次 $b$ 中一个实例的数据生成模型为：\n$$\nz \\sim \\mathcal{N}(0, I_d), \\quad x = \\mu_b + s_b \\odot z, \\quad \\text{logit} = w_{\\text{true}}^\\top x + \\alpha_{\\text{true},b}, \\quad p = \\sigma(\\text{logit}), \\quad y \\sim \\text{Bernoulli}(p),\n$$\n其中 $\\odot$ 表示逐元素乘法，$\\sigma(t) = \\frac{1}{1+e^{-t}}$ 是 logistic 函数，$I_d$ 是 $d \\times d$ 单位矩阵。\n\n您必须实现一个估计器，该估计器首先尝试通过按批次进行特征标准化（z-score）来去除批次效应，然后拟合一个带有批次特有截距的正则化 logistic 回归。具体而言，在训练数据上，对每个批次 $b$ 计算经验均值 $\\hat{\\mu}_b \\in \\mathbb{R}^d$ 和经验标准差向量 $\\hat{\\sigma}_b \\in \\mathbb{R}^d$（逐分量计算），并通过下式为批次 $b(i)$ 中的样本 $i$ 定义标准化特征：\n$$\n\\tilde{x}_i = \\frac{x_i - \\hat{\\mu}_{b(i)}}{\\hat{\\sigma}_{b(i)}},\n$$\n其中除法是逐元素的。然后通过最小化正则化经验风险来拟合参数 $w \\in \\mathbb{R}^d$ 和 $\\alpha \\in \\mathbb{R}^B$：\n$$\n\\mathcal{L}(w,\\alpha) = \\frac{1}{n}\\sum_{i=1}^n \\left[ - y_i \\log\\!\\big(\\sigma(t_i)\\big) - (1 - y_i)\\log\\!\\big(1 - \\sigma(t_i)\\big) \\right] + \\frac{\\lambda_w}{2} \\|w\\|_2^2 + \\frac{\\lambda_\\alpha}{2} \\|\\alpha\\|_2^2,\n$$\n其中 $t_i = w^\\top \\tilde{x}_i + \\alpha_{b(i)}$，$n$ 是训练样本的总数。最小化过程必须使用批（全）梯度下降法，并采用固定的学习率和固定的迭代次数。\n\n从第一性原理出发，负对数似然项对应于 logistic 模型所蕴含的伯努利分布的交叉熵，而 $\\ell_2$ 惩罚项在存在批次失真的情况下强制施加权重正则性。您的程序必须实现从该损失函数推导出的基于梯度的更新。\n\n训练后，在一个独立生成的、来自相同生成模型的测试集上评估该估计器，评估时使用从训练数据计算出的相同的按批次标准化参数 $(\\hat{\\mu}_b, \\hat{\\sigma}_b)$。对于每个测试用例，报告以下指标：\n- 生物信号权重误差的欧几里得范数：$e_w = \\|w - w_{\\text{true}}\\|_2$。\n- 批次截距误差的欧几里得范数：$e_\\alpha = \\|\\alpha - \\alpha_{\\text{true}}\\|_2$。\n- 受试者工作特征曲线下面积（AUROC），表示为在 $[0,1]$ 区间内的小数，使用预测概率 $\\hat{p}_i = \\sigma(w^\\top \\tilde{x}_i + \\alpha_{b(i)})$ 在测试集上计算；AUROC 必须通过排序（Mann–Whitney 统计量）从定义中精确计算，平局情况通过平均秩处理。\n\n您的程序必须实现以下测试套件。在所有情况下，从标准正态分布中抽取 $w_{\\text{true}}$，并将其重新缩放，使其欧几里得范数等于 $1.5$。对于每个批次 $b$，从标准正态分布中抽取一个随机单位向量 $u_b \\in \\mathbb{R}^d$，并设置 $\\mu_b = m_b \\cdot u_b$，其中 $m_b$ 从指定的幅度区间中均匀抽取。对于每个分量 $j \\in \\{1,\\dots,d\\}$，从指定的尺度区间中独立且均匀地抽取 $s_{b,j}$。为保证可复现性，每个用例使用固定的随机种子。\n\n测试用例 A（一般情况）：\n- $d = 12$, $B = 3$。\n- 每个批次的训练样本数：$[3000, 3000, 3000]$。\n- 每个批次的测试样本数：$[1000, 1000, 1000]$。\n- 均值偏移幅度 $m_b \\sim \\text{Uniform}(0.3, 0.7)$，各批次独立。\n- 尺度 $s_{b,j} \\sim \\text{Uniform}(0.8, 1.5)$，独立抽取。\n- 真实批次截距 $\\alpha_{\\text{true}} = [-0.4, 0.2, 0.1]$。\n- 正则化强度：$\\lambda_w = 0.05$, $\\lambda_\\alpha = 0.10$。\n- 学习率：$0.10$。\n- 梯度下降迭代次数：$400$。\n- 随机种子：$123$。\n\n测试用例 B（严重的批次失真）：\n- $d = 12$, $B = 3$。\n- 每个批次的训练样本数：$[3000, 3000, 3000]$。\n- 每个批次的测试样本数：$[1000, 1000, 1000]$。\n- 均值偏移幅度 $m_b \\sim \\text{Uniform}(1.0, 2.0)$，各批次独立。\n- 尺度 $s_{b,j} \\sim \\text{Uniform}(0.5, 2.5)$，独立抽取。\n- 真实批次截距 $\\alpha_{\\text{true}} = [1.2, -0.8, 0.5]$。\n- 正则化强度：$\\lambda_w = 0.10$, $\\lambda_\\alpha = 0.20$。\n- 学习率：$0.08$。\n- 梯度下降迭代次数：$600$。\n- 随机种子：$456$。\n\n测试用例 C（小批次导致的类别不平衡）：\n- $d = 12$, $B = 3$。\n- 每个批次的训练样本数：$[200, 3000, 3000]$。\n- 每个批次的测试样本数：$[200, 1000, 1000]$。\n- 均值偏移幅度 $m_b \\sim \\text{Uniform}(0.5, 0.9)$，各批次独立。\n- 尺度 $s_{b,j} \\sim \\text{Uniform}(0.7, 1.8)$，独立抽取。\n- 真实批次截距 $\\alpha_{\\text{true}} = [-0.2, 0.3, 0.0]$。\n- 正则化强度：$\\lambda_w = 0.05$, $\\lambda_\\alpha = 0.10$。\n- 学习率：$0.10$。\n- 梯度下降迭代次数：$500$。\n- 随机种子：$789$。\n\n您的程序必须：\n1. 严格按照规范为每个用例生成训练和测试数据。\n2. 仅使用训练数据的统计信息执行按批次标准化。\n3. 使用指定的超参数，通过全批次梯度下降训练带有批次截距的正则化 logistic 回归。\n4. 为每个用例在测试集上计算 $e_w$、$e_\\alpha$ 和 AUROC。\n\n最终输出格式：您的程序应生成单行输出，其中包含九个结果（每个测试用例三个，按 A、B、C 的顺序排列为 $[e_w, e_\\alpha, \\text{AUROC}]$），形式为方括号括起来的逗号分隔列表（例如，“[rA1,rA2,rA3,rB1,rB2,rB3,rC1,rC2,rC3]”）。不应打印任何其他文本。所有值必须表示为 Python 浮点数（小数）。不涉及物理单位或角度；请将 AUROC 表示为小数而非百分比。",
            "solution": "目标是在用于变异致病性的 logistic 模型中，估计生物信号和批次截距，同时抵消批次引起的失真。我们从第一性原理出发，推导估计器及其梯度，然后指定程序必须实现的算法。\n\n基本基础和定义：\n- logistic 函数是 $\\sigma(t) = \\frac{1}{1+e^{-t}}$，它参数化了 logistic 回归中的伯努利概率。对于线性预测变量为 $t = w^\\top \\tilde{x} + \\alpha_{b}$ 的单个观测 $(\\tilde{x}, y)$，其负对数似然为 $- y \\log(\\sigma(t)) - (1-y)\\log(1-\\sigma(t))$。对于 $n$ 个样本，经验风险是这些项的平均值。\n- 批次效应被建模为通过 $(\\mu_b, s_b)$ 实现的特征仿射失真，以及通过 $\\alpha_{\\text{true},b}$ 实现的对数几率中的加性偏差。这与测量科学中经过充分检验的观察结果一致，即批次处理会引起均值和尺度偏移，并且特定于分析的校准会影响基线 logit。\n- 按批次 z-score 标准化通过使用经验估计 $(\\hat{\\mu}_b, \\hat{\\sigma}_b)$ 对特征进行中心化和缩放来去除仿射均值和尺度偏移，并援引大数定律来证明随着样本量的增加，这些估计会收敛到真实的批次参数。\n\n估计器和损失：\n我们在训练数据中按批次对特征进行标准化：对于批次 $b(i)$ 中特征为 $x_i$ 的样本 $i$，定义\n$$\n\\tilde{x}_i = \\frac{x_i - \\hat{\\mu}_{b(i)}}{\\hat{\\sigma}_{b(i)}},\n$$\n其中除法是逐分量的，并且 $\\hat{\\sigma}_{b}$ 的元素严格为正（如有必要，用一个小的正常数进行正则化）。然后我们最小化正则化的经验风险\n$$\n\\mathcal{L}(w,\\alpha) = \\frac{1}{n}\\sum_{i=1}^n \\left[ - y_i \\log\\!\\big(\\sigma(t_i)\\big) - (1 - y_i)\\log\\!\\big(1 - \\sigma(t_i)\\big) \\right] + \\frac{\\lambda_w}{2} \\|w\\|_2^2 + \\frac{\\lambda_\\alpha}{2} \\|\\alpha\\|_2^2,\n$$\n其中 $t_i = w^\\top \\tilde{x}_i + \\alpha_{b(i)}$。\n\n梯度推导：\n令 $p_i = \\sigma(t_i)$，并注意根据 logistic 损失的标准导数，有 $\\frac{\\partial}{\\partial t_i} \\left(- y_i \\log(p_i) - (1-y_i)\\log(1-p_i)\\right) = p_i - y_i$。使用链式法则，关于 $w$ 的梯度是\n$$\n\\nabla_w \\mathcal{L} = \\frac{1}{n} \\sum_{i=1}^n (p_i - y_i)\\, \\tilde{x}_i + \\lambda_w w = \\frac{1}{n} \\left( \\tilde{X}^\\top (p - y) \\right) + \\lambda_w w,\n$$\n其中 $\\tilde{X}$ 是标准化特征的 $n \\times d$ 矩阵，$p, y \\in \\mathbb{R}^n$ 分别是预测概率和标签的向量。关于每个批次截距 $\\alpha_b$ 的梯度是\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_b} = \\frac{1}{n} \\sum_{i: b(i)=b} (p_i - y_i) + \\lambda_\\alpha \\alpha_b.\n$$\n总的来说，$\\alpha \\in \\mathbb{R}^B$ 的梯度向量可以使用独热（one-hot）批次关联矩阵来编写，或者通过对批次索引进行聚合来计算。\n\n优化算法：\n我们使用全批次梯度下降法，学习率 $\\eta$ 和迭代次数 $T$ 均按照每个测试用例的规定固定。将 $w$ 和 $\\alpha$ 初始化为零向量，在每次迭代中：\n1. 计算 $t = \\tilde{X} w + \\alpha_{\\text{expanded}}$，其中 $\\alpha_{\\text{expanded}}$ 通过批次索引为每个样本索引 $\\alpha$。\n2. 计算 $p = \\sigma(t)$。\n3. 计算梯度：\n$$\ng_w = \\frac{1}{n} \\tilde{X}^\\top (p - y) + \\lambda_w w, \\quad\ng_\\alpha[b] = \\frac{1}{n} \\sum_{i: b(i)=b} (p_i - y_i) + \\lambda_\\alpha \\alpha_b.\n$$\n4. 更新 $w \\leftarrow w - \\eta g_w$，$\\alpha \\leftarrow \\alpha - \\eta g_\\alpha$。\n\n评估：\n- 生物信号误差：$e_w = \\|w - w_{\\text{true}}\\|_2$。\n- 批次截距误差：$e_\\alpha = \\|\\alpha - \\alpha_{\\text{true}}\\|_2$。\n- 受试者工作特征曲线下面积（AUROC）：使用预测概率 $\\hat{p}_i = \\sigma(w^\\top \\tilde{x}_i + \\alpha_{b(i)})$ 在测试集上计算。AUROC 是一个随机选择的正样本得分高于一个随机选择的负样本得分的概率，再加上得分相等概率的一半。在数值上，AUROC 可以通过使用平均秩的 Mann–Whitney 统计量来计算：\n  - 令 $r_i$ 为分数 $s_i$ 在所有 $n$ 个样本中按升序排序时的平均秩；平局情况下的秩取其秩区间的平均值。\n  - 令 $n_+$ 为正样本数，$n_-$ 为负样本数。\n  - AUROC 为\n  $$\n  \\text{AUROC} = \\frac{\\sum_{i: y_i=1} r_i - \\frac{n_+(n_+ + 1)}{2}}{n_+ n_-}.\n  $$\n  如果 $n_+=0$ 或 $n_-=0$，则将 AUROC 设置为中性值 $0.5$。\n\n数据生成细节：\n- 对于每个测试用例，从 $\\mathcal{N}(0, I_d)$ 中抽取 $w_{\\text{true}}$ 并将其重新缩放，使其欧几里得范数为 $1.5$。\n- 对于每个批次 $b$，从 $\\mathcal{N}(0, I_d)$ 中抽取 $u_b$ 并设置 $u_b \\leftarrow \\frac{u_b}{\\|u_b\\|_2}$ 使其成为单位向量。从特定于用例的区间中均匀抽取 $m_b$ 并设置 $\\mu_b = m_b u_b$。\n- 对于每个分量 $j$，从特定于用例的尺度区间中均匀抽取 $s_{b,j}$，确保其严格为正。\n- 每个样本独立地从 $\\mathcal{N}(0, I_d)$ 中生成 $z$，设置 $x = \\mu_b + s_b \\odot z$，计算 $p = \\sigma(w_{\\text{true}}^\\top x + \\alpha_{\\text{true},b})$，并从 $\\text{Bernoulli}(p)$ 中采样得到 $y$。\n\n标准化：\n- 在训练集上为每个批次 $b$ 计算每个特征的经验均值 $\\hat{\\mu}_b$ 和经验标准差 $\\hat{\\sigma}_b$。\n- 定义 $\\tilde{x}_i = (x_i - \\hat{\\mu}_{b(i)}) / \\hat{\\sigma}_{b(i)}$，如果 $\\hat{\\sigma}_b$ 的任何分量为零，则向其添加一个小的正常数以避免除以零。\n- 应用相同的 $(\\hat{\\mu}_b, \\hat{\\sigma}_b)$ 来标准化测试集。\n\n超参数：\n- 每个测试用例使用指定的正则化强度 $(\\lambda_w, \\lambda_\\alpha)$、学习率 $\\eta$ 和迭代次数 $T$。\n\n输出：\n- 对于每个测试用例（A、B、C），生成三元组 $(e_w, e_\\alpha, \\text{AUROC})$ 作为 Python 浮点数。\n- 将所有九个值按 A、B、C 的顺序聚合到一个列表中，并打印为由方括号括起来的、用逗号分隔值的单行字符串。\n\n该方法是合理的，因为在假定的生成模型下，按批次 z-score 标准化可以去除仿射批次效应，而批次特有的截距 $\\alpha_b$ 可以捕获分析引起的 logit 偏差。正则化 logistic 回归最小化了期望的负对数似然，而梯度下降在一个可微模型中有效地实现了最小化。通过秩计算的 AUROC 为学习到的致病性预测器在测试集上的判别能力提供了一个与阈值无关的度量。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef sigmoid(x):\n    # Numerically stable sigmoid\n    # Clip input to avoid overflow in exp\n    x = np.clip(x, -50, 50)\n    return 1.0 / (1.0 + np.exp(-x))\n\ndef compute_average_ranks(scores):\n    \"\"\"\n    Compute average ranks for scores (ascending).\n    Ties receive the average of their rank interval.\n    Returns an array of float ranks from 1 to n.\n    \"\"\"\n    n = scores.size\n    order = np.argsort(scores, kind='mergesort')  # stable sort\n    sorted_scores = scores[order]\n    ranks = np.empty(n, dtype=float)\n\n    i = 0\n    while i  n:\n        j = i + 1\n        # group equal scores\n        while j  n and sorted_scores[j] == sorted_scores[i]:\n            j += 1\n        # average rank for the group [i, j-1], ranks are 1-based\n        avg_rank = (i + 1 + j) / 2.0\n        ranks[i:j] = avg_rank\n        i = j\n\n    # scatter back to original order\n    inv_order = np.empty(n, dtype=int)\n    inv_order[order] = np.arange(n)\n    ranks_original = ranks[inv_order]\n    return ranks_original\n\ndef auc_roc(y_true, scores):\n    \"\"\"\n    Compute AUROC using the Mann-Whitney U statistic via average ranks.\n    y_true: binary array of 0/1\n    scores: predictor scores (probabilities or logits), higher means more positive\n    \"\"\"\n    y_true = np.asarray(y_true, dtype=int)\n    scores = np.asarray(scores, dtype=float)\n    n = y_true.size\n    n_pos = int(np.sum(y_true))\n    n_neg = n - n_pos\n    if n_pos == 0 or n_neg == 0:\n        return 0.5  # neutral AUROC if only one class present\n\n    ranks = compute_average_ranks(scores)  # ascending\n    # Sum ranks of positives\n    R_pos = np.sum(ranks[y_true == 1])\n    auc = (R_pos - n_pos * (n_pos + 1) / 2.0) / (n_pos * n_neg)\n    return float(auc)\n\ndef generate_case_data(d, B, n_train_per_batch, n_test_per_batch,\n                       shift_range, scale_range, alpha_true, seed):\n    \"\"\"\n    Generate training and test data for one case according to the specification.\n    Returns:\n      w_true: (d,)\n      train_X: (n_train, d)\n      train_y: (n_train,)\n      train_batches: (n_train,)\n      test_X: (n_test, d)\n      test_y: (n_test,)\n      test_batches: (n_test,)\n      mus: list of length B with mean vectors\n      sigmas: list of length B with std vectors (for info; not used directly)\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    # True biological weights\n    w_true = rng.normal(size=d)\n    w_true = (1.5 / np.linalg.norm(w_true)) * w_true\n\n    # Batch means and scales\n    mus = []\n    scales = []\n    for b in range(B):\n        u = rng.normal(size=d)\n        u_norm = np.linalg.norm(u)\n        if u_norm == 0:\n            u = np.zeros(d)\n            u[0] = 1.0\n            u_norm = 1.0\n        u = u / u_norm\n        m_mag = rng.uniform(shift_range[0], shift_range[1])\n        mu_b = m_mag * u\n        s_b = rng.uniform(scale_range[0], scale_range[1], size=d)\n        mus.append(mu_b)\n        scales.append(s_b)\n\n    # Generate training data\n    train_X_list = []\n    train_y_list = []\n    train_batches_list = []\n\n    for b in range(B):\n        n_b = n_train_per_batch[b]\n        z = rng.normal(size=(n_b, d))\n        x = mus[b] + z * scales[b]\n        logits = x @ w_true + alpha_true[b]\n        p = sigmoid(logits)\n        y = rng.binomial(1, p)\n        train_X_list.append(x)\n        train_y_list.append(y)\n        train_batches_list.append(np.full(n_b, b, dtype=int))\n\n    train_X = np.vstack(train_X_list)\n    train_y = np.concatenate(train_y_list)\n    train_batches = np.concatenate(train_batches_list)\n\n    # Generate test data\n    test_X_list = []\n    test_y_list = []\n    test_batches_list = []\n\n    for b in range(B):\n        n_b = n_test_per_batch[b]\n        z = rng.normal(size=(n_b, d))\n        x = mus[b] + z * scales[b]\n        logits = x @ w_true + alpha_true[b]\n        p = sigmoid(logits)\n        y = rng.binomial(1, p)\n        test_X_list.append(x)\n        test_y_list.append(y)\n        test_batches_list.append(np.full(n_b, b, dtype=int))\n\n    test_X = np.vstack(test_X_list)\n    test_y = np.concatenate(test_y_list)\n    test_batches = np.concatenate(test_batches_list)\n\n    return w_true, train_X, train_y, train_batches, test_X, test_y, test_batches, mus, scales\n\ndef per_batch_standardize(X, batches, B):\n    \"\"\"\n    Compute per-batch mean and std, and return standardized X along with stats.\n    \"\"\"\n    d = X.shape[1]\n    means = []\n    stds = []\n    X_std = np.empty_like(X)\n    eps = 1e-8\n    for b in range(B):\n        idx = (batches == b)\n        X_b = X[idx]\n        mu_b = X_b.mean(axis=0)\n        sigma_b = X_b.std(axis=0)\n        sigma_b = np.where(sigma_b == 0, eps, sigma_b)\n        means.append(mu_b)\n        stds.append(sigma_b)\n        X_std[idx] = (X_b - mu_b) / sigma_b\n    return X_std, means, stds\n\ndef apply_standardization(X, batches, means, stds):\n    \"\"\"\n    Apply precomputed per-batch means and stds to standardize X.\n    \"\"\"\n    X_std = np.empty_like(X)\n    for b in range(len(means)):\n        idx = (batches == b)\n        X_b = X[idx]\n        X_std[idx] = (X_b - means[b]) / stds[b]\n    return X_std\n\ndef train_logistic_with_batch_intercepts(X, y, batches, B, lambda_w, lambda_alpha, lr, iters):\n    \"\"\"\n    Train logistic regression with batch intercepts via gradient descent.\n    Returns learned w and alpha.\n    \"\"\"\n    n, d = X.shape\n    w = np.zeros(d, dtype=float)\n    alpha = np.zeros(B, dtype=float)\n\n    for _ in range(iters):\n        linear = X @ w + alpha[batches]\n        p = sigmoid(linear)\n        error = (p - y).astype(float)\n\n        # Gradients\n        grad_w = (X.T @ error) / n + lambda_w * w\n\n        # For alpha, aggregate errors per batch\n        grad_alpha = np.zeros(B, dtype=float)\n        # Weighted sum of error by batch divided by n\n        for b in range(B):\n            grad_alpha[b] = error[batches == b].sum() / n + lambda_alpha * alpha[b]\n\n        # Update\n        w -= lr * grad_w\n        alpha -= lr * grad_alpha\n\n    return w, alpha\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A\n        {\n            \"d\": 12,\n            \"B\": 3,\n            \"n_train_per_batch\": [3000, 3000, 3000],\n            \"n_test_per_batch\": [1000, 1000, 1000],\n            \"shift_range\": (0.3, 0.7),\n            \"scale_range\": (0.8, 1.5),\n            \"alpha_true\": np.array([-0.4, 0.2, 0.1], dtype=float),\n            \"lambda_w\": 0.05,\n            \"lambda_alpha\": 0.10,\n            \"lr\": 0.10,\n            \"iters\": 400,\n            \"seed\": 123,\n        },\n        # Case B\n        {\n            \"d\": 12,\n            \"B\": 3,\n            \"n_train_per_batch\": [3000, 3000, 3000],\n            \"n_test_per_batch\": [1000, 1000, 1000],\n            \"shift_range\": (1.0, 2.0),\n            \"scale_range\": (0.5, 2.5),\n            \"alpha_true\": np.array([1.2, -0.8, 0.5], dtype=float),\n            \"lambda_w\": 0.10,\n            \"lambda_alpha\": 0.20,\n            \"lr\": 0.08,\n            \"iters\": 600,\n            \"seed\": 456,\n        },\n        # Case C\n        {\n            \"d\": 12,\n            \"B\": 3,\n            \"n_train_per_batch\": [200, 3000, 3000],\n            \"n_test_per_batch\": [200, 1000, 1000],\n            \"shift_range\": (0.5, 0.9),\n            \"scale_range\": (0.7, 1.8),\n            \"alpha_true\": np.array([-0.2, 0.3, 0.0], dtype=float),\n            \"lambda_w\": 0.05,\n            \"lambda_alpha\": 0.10,\n            \"lr\": 0.10,\n            \"iters\": 500,\n            \"seed\": 789,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        d = case[\"d\"]\n        B = case[\"B\"]\n        n_train_per_batch = case[\"n_train_per_batch\"]\n        n_test_per_batch = case[\"n_test_per_batch\"]\n        shift_range = case[\"shift_range\"]\n        scale_range = case[\"scale_range\"]\n        alpha_true = case[\"alpha_true\"]\n        lambda_w = case[\"lambda_w\"]\n        lambda_alpha = case[\"lambda_alpha\"]\n        lr = case[\"lr\"]\n        iters = case[\"iters\"]\n        seed = case[\"seed\"]\n\n        # Generate data\n        w_true, X_train, y_train, batches_train, X_test, y_test, batches_test, mus, scales = generate_case_data(\n            d, B, n_train_per_batch, n_test_per_batch, shift_range, scale_range, alpha_true, seed\n        )\n\n        # Per-batch standardization on training data\n        X_train_std, means, stds = per_batch_standardize(X_train, batches_train, B)\n        X_test_std = apply_standardization(X_test, batches_test, means, stds)\n\n        # Train model\n        w_hat, alpha_hat = train_logistic_with_batch_intercepts(\n            X_train_std, y_train, batches_train, B,\n            lambda_w, lambda_alpha, lr, iters\n        )\n\n        # Metrics\n        ew = float(np.linalg.norm(w_hat - w_true))\n        ealpha = float(np.linalg.norm(alpha_hat - alpha_true))\n        test_probs = sigmoid(X_test_std @ w_hat + alpha_hat[batches_test])\n        auc = auc_roc(y_test, test_probs)\n\n        results.extend([ew, ealpha, auc])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "对于许多遗传病，致病原因并非单个变异，而是某个基因上多个罕见变异的累积效应，即“基因负荷”。本实践将带你从单个变异的致病性概率预测，提升到基因水平的综合评估。你将学习如何在一个符合贝叶斯理论的框架下，整合经过校准的、考虑了先验概率的变异分数，最终计算出对临床诊断更有意义的基因水平致病风险。",
            "id": "4330559",
            "problem": "给定一个在以患者为中心的环境中，使用深度学习输出对罕见变异负荷进行基因水平聚合的形式化表述。对于基因中观察到的每个变异和每个个体，深度神经网络（DNN）会产生一个原始的logit分数，该分数通过温度缩放进行校准。您必须实现一个基于贝叶斯概率的原则性聚合方法，以计算该个体在该基因中携带至少一个真正致病性变异的基因水平概率。\n\n从以下基本原理开始：\n- 贝叶斯定理和几率形式：对于任何先验概率为 $p$ 的事件，其先验几率为 $O = \\dfrac{p}{1-p}$。如果观察到似然比（贝叶斯因子）$B$，则后验几率为 $O' = B \\cdot O$，后验概率为 $p' = \\dfrac{O'}{1+O'}$。\n- 对于logistic模型，给定一个logit $z$，校准后的后验概率由logistic sigmoid函数给出：$\\sigma(z) = \\dfrac{1}{1 + e^{-z}}$。\n- 温度缩放通过一个正温度 $T$ 来调整logits，对于原始logit $\\ell$，通过 $\\sigma\\!\\left(\\dfrac{\\ell}{T}\\right)$ 产生校准后的概率。\n- 如果一个分类器在训练先验 $\\pi_{\\mathrm{m}}$（模型先验）下进行了校准，但应用于部署先验 $\\pi_{\\mathrm{t}}$（真实先验），那么令 $O_{\\mathrm{m}} = \\dfrac{\\pi_{\\mathrm{m}}}{1 - \\pi_{\\mathrm{m}}}$ 和 $O_{\\mathrm{t}} = \\dfrac{\\pi_{\\mathrm{t}}}{1 - \\pi_{\\mathrm{t}}}$，则真实先验下的后验几率通过将模型几率乘以因子 $\\dfrac{O_{\\mathrm{t}}}{O_{\\mathrm{m}}}$ 获得。\n- 假设在给定DNN输出的情况下，每个变异的致病性事件是条件独立的，则基因中（对于该个体）至少有一个变异是真正致病性的概率，可以通过补集的乘积的补集来计算：如果 $q_i$ 是个体中存在且通过罕见性筛选的各个变异的后验概率（经过所有校正后），则基因水平的概率为 $1 - \\prod_i (1 - q_i)$。\n\n任务：\n- 对于每个测试用例，您将获得：\n  - 一个温度 $T  0$。\n  - 一个模型训练先验 $\\pi_{\\mathrm{m}}$，其中 $0  \\pi_{\\mathrm{m}}  1$。\n  - 一个部署先验 $\\pi_{\\mathrm{t}}$，其中 $0  \\pi_{\\mathrm{t}}  1$。\n  - 一个等位基因频率（小数形式）的罕见性阈值 $\\tau$，其中 $0  \\tau  1$。\n  - 单个个体中单个基因的有限变异列表。每个变异是一个元组 $(\\ell, f, g)$，其中 $\\ell$ 是DNN的原始logit（一个实数），$f$ 是变异的次要等位基因频率（$[0,1]$范围内的小数），$g \\in \\{0,1\\}$ 是个体的基因型指示符，其中 $g = 1$ 表示个体携带该变异，$g = 0$ 表示不携带。\n\n- 对于每个变异：\n  1. 计算校准后的模型后验概率 $p_{\\mathrm{m}} = \\sigma\\!\\left(\\dfrac{\\ell}{T}\\right)$。\n  2. 转换为模型几率 $O_{\\mathrm{m,post}} = \\dfrac{p_{\\mathrm{m}}}{1 - p_{\\mathrm{m}}}$。\n  3. 通过将几率乘以 $\\dfrac{O_{\\mathrm{t}}}{O_{\\mathrm{m}}}$ 来调整部署先验，从而得到 $O_{\\mathrm{t,post}} = O_{\\mathrm{m,post}} \\cdot \\dfrac{O_{\\mathrm{t}}}{O_{\\mathrm{m}}}$，其中 $O_{\\mathrm{m}} = \\dfrac{\\pi_{\\mathrm{m}}}{1 - \\pi_{\\mathrm{m}}}$ 且 $O_{\\mathrm{t}} = \\dfrac{\\pi_{\\mathrm{t}}}{1 - \\pi_{\\mathrm{t}}}$。\n  4. 转换回校正后的后验概率 $p_{\\mathrm{t}} = \\dfrac{O_{\\mathrm{t,post}}}{1 + O_{\\mathrm{t,post}}}$。\n  5. 应用罕见变异负荷规则：当且仅当 $g = 1$ 且 $f \\le \\tau$ 时，将该变异包含在聚合中。对于被排除的变异，将其贡献概率设置为 $0$。\n\n- 将包含的变异 $\\{p_{\\mathrm{t}, i}\\}$ 聚合成至少有一个致病性变异的基因水平概率：\n  $$P_{\\text{gene}} \\;=\\; 1 \\;-\\; \\prod_i \\bigl(1 - p_{\\mathrm{t}, i}\\bigr).$$\n\n- 您的程序必须为每个测试用例计算 $P_{\\text{gene}}$，并输出一行，其中包含所有结果，形式为一个由逗号分隔并用方括号括起来的列表。每个数值结果必须四舍五入到六位小数。不涉及角度。没有物理单位；所有等位基因频率都是纯小数。百分比必须表示为小数。\n\n测试套件：\n- 用例 $1$：\n  - $T = 1.7$, $\\pi_{\\mathrm{m}} = 0.2$, $\\pi_{\\mathrm{t}} = 0.05$, $\\tau = 0.01$。\n  - 变异： $(2.2, 0.004, 1)$, $(-0.3, 0.008, 1)$, $(0.0, 0.015, 1)$, $(1.1, 0.0005, 0)$。\n- 用例 $2$：\n  - $T = 1.0$, $\\pi_{\\mathrm{m}} = 0.1$, $\\pi_{\\mathrm{t}} = 0.02$, $\\tau = 0.01$。\n  - 变异： $(3.0, 0.0001, 0)$, $(-2.0, 0.005, 0)$。\n- 用例 $3$：\n  - $T = 2.5$, $\\pi_{\\mathrm{m}} = 0.5$, $\\pi_{\\mathrm{t}} = 0.001$, $\\tau = 0.02$。\n  - 变异： $(-1.0, 0.005, 1)$, $(-0.5, 0.003, 1)$, $(-3.0, 0.0002, 1)$, $(-2.0, 0.018, 1)$。\n- 用例 $4$：\n  - $T = 0.5$, $\\pi_{\\mathrm{m}} = 0.3$, $\\pi_{\\mathrm{t}} = 0.1$, $\\tau = 0.05$。\n  - 变异： $(10.0, 0.0001, 1)$, $(-10.0, 0.001, 1)$, $(0.2, 0.2, 1)$。\n\n您的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表形式的结果（例如，[result1,result2,result3,result4]），每个数字四舍五入到六位小数。因此，要求的最终输出格式是“[x1,x2,x3,x4]”这样的一行。",
            "solution": "该问题提出了一个在计算基因组学中定义明确且具有科学依据的任务。它要求实现一个贝叶斯流程，将来自深度学习模型的变异水平致病性分数聚合成一个个体的单一基因水平概率。所提供的方法论与既定的概率建模原则相一致，包括使用几率形式的贝叶斯定理、用于模型校准的温度缩放以及先验概率校正。该问题是完整的，提供了所有必要的数据和数学公式。因此，该问题被认为是有效的，可以构建一个解决方案。\n\n总体目标是计算 $P_{\\text{gene}}$，即一个个体在特定基因中携带至少一个真正致病性罕见变异的概率。这通过以下步骤实现：首先，为每个变异计算校正后的致病性后验概率 $p_{\\mathrm{t}}$；然后，筛选出罕见且存在于该个体中的变异；最后，聚合这个筛选后集合的概率。\n\n计算的逻辑步骤如下：\n\n1.  **逐变异后验概率计算**：对于每个以原始logit $\\ell$为特征的变异，我们必须推导出其致病性的后验概率 $p_{\\mathrm{t}}$，该概率需同时针对模型校准和真实致病性先验概率进行调整。\n\n    a. **温度缩放**：由深度神经网络（DNN）产生的原始logit $\\ell$，首先使用给定的温度 $T  0$ 进行校准。校准后的logit为 $z = \\ell/T$。此步骤调整模型输出的置信度。温度 $T  1$ 会软化概率，使其更接近 $0.5$；而温度 $T  1$ 则会锐化概率，使其更接近 $0$ 或 $1$。\n\n    b. **模型后验概率和几率**：校准后的logit $z$ 使用logistic sigmoid函数 $\\sigma(x) = (1 + e^{-x})^{-1}$ 转换为模型训练先验 $\\pi_{\\mathrm{m}}$ 下的后验概率。\n    $$p_{\\mathrm{m}} = \\sigma(z) = \\sigma\\!\\left(\\frac{\\ell}{T}\\right) = \\frac{1}{1 + e^{-\\ell/T}}$$\n    在几率空间中进行计算，在计算上更稳定，在理论上更直接。对应于 $p_{\\mathrm{m}}$ 的后验几率为：\n    $$O_{\\mathrm{m,post}} = \\frac{p_{\\mathrm{m}}}{1 - p_{\\mathrm{m}}} = \\frac{\\sigma(\\ell/T)}{1 - \\sigma(\\ell/T)} = \\frac{1/(1+e^{-\\ell/T})}{e^{-\\ell/T}/(1+e^{-\\ell/T})} = e^{\\ell/T}$$\n    这表明，在模型隐含的均匀先验下，后验几率就是校准后logit的指数。\n\n    c. **先验校正**：模型是在特定的类别平衡下训练的，这由模型先验 $\\pi_{\\mathrm{m}}$ 反映。然而，在部署群体中致病性变异的真实流行率由 $\\pi_{\\mathrm{t}}$ 给出。为了获得部署场景下的正确后验概率，我们必须调整后验几率。后验几率、先验几率和贝叶斯因子（似然比）$B$ 之间的关系是 $O_{\\mathrm{post}} = B \\cdot O_{\\mathrm{prior}}$。贝叶斯因子 $B$ 是证据（变异特征）的内在属性，与先验无关。因此，我们可以从模型的角度找到 $B$：$B = O_{\\mathrm{m,post}} / O_{\\mathrm{m}}$，其中 $O_{\\mathrm{m}} = \\pi_{\\mathrm{m}} / (1-\\pi_{\\mathrm{m}})$ 是模型的先验几率。然后我们用这个贝叶斯因子来找到真实的后验几率：\n    $$O_{\\mathrm{t,post}} = B \\cdot O_{\\mathrm{t}} = \\left(\\frac{O_{\\mathrm{m,post}}}{O_{\\mathrm{m}}}\\right) O_{\\mathrm{t}} = O_{\\mathrm{m,post}} \\cdot \\frac{O_{\\mathrm{t}}}{O_{\\mathrm{m}}}$$\n    其中 $O_{\\mathrm{t}} = \\pi_{\\mathrm{t}} / (1-\\pi_{\\mathrm{t}})$ 是真实的先验几率。代入几率的表达式，校正因子为：\n    $$\\frac{O_{\\mathrm{t}}}{O_{\\mathrm{m}}} = \\frac{\\pi_{\\mathrm{t}} / (1 - \\pi_{\\mathrm{t}})}{\\pi_{\\mathrm{m}} / (1 - \\pi_{\\mathrm{m}})}$$\n    因此，真实的后验几率为：\n    $$O_{\\mathrm{t,post}} = e^{\\ell/T} \\cdot \\frac{\\pi_{\\mathrm{t}}(1 - \\pi_{\\mathrm{m}})}{\\pi_{\\mathrm{m}}(1 - \\pi_{\\mathrm{t}})}$$\n\n    d. **最终校正概率**：将真实的后验几率转换回概率：\n    $$p_{\\mathrm{t}} = \\frac{O_{\\mathrm{t,post}}}{1 + O_{\\mathrm{t,post}}}$$\n\n2.  **变异筛选**：一个变异 $(\\ell, f, g)$ 仅在满足以下两个条件时才被纳入基因水平负荷计算：\n    - 该个体携带此变异，由基因型 $g=1$ 指示。\n    - 该变异足够罕见，定义为其次要等位基因频率 $f$ 小于或等于阈值 $\\tau$，即 $f \\le \\tau$。\n    不满足这两个标准的变异将被排除。在聚合公式的语境下，这等同于将它们的致病性概率设为 $0$。\n\n3.  **基因水平聚合**：最后一步是将通过筛选的变异的概率组合成一个单一的基因水平分数。设 $\\{ p_{\\mathrm{t},i} \\}_{i=1}^N$ 是通过筛选步骤的 $N$ 个变异的校正后验概率集合。假设在给定模型输出的条件下，这些变异的致病性是条件独立的，那么它们中*没有一个*是致病性的概率是它们各自非致病性概率的乘积：\n    $$P(\\text{无致病性变异}) = \\prod_{i=1}^{N} (1 - p_{\\mathrm{t},i})$$\n    我们感兴趣的事件是*至少有一个*变异是致病性的，这是上述事件的补集。因此，基因水平的概率是：\n    $$P_{\\text{gene}} = 1 - P(\\text{无致病性变异}) = 1 - \\prod_{i=1}^{N} (1 - p_{\\mathrm{t},i})$$\n    如果没有变异通过筛选，则该集合为空，乘积（空积）为 $1$，因此 $P_{\\text{gene}} = 1 - 1 = 0$。\n\n实现将把这些步骤封装在一个函数中，该函数处理每个测试用例，遍历其变异，应用筛选器，为符合条件的变异计算 $p_{\\mathrm{t}}$，最后计算 $P_{\\text{gene}}$。结果将按要求四舍五入到六位小数。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the gene-level variant aggregation problem for a given set of test cases.\n    \"\"\"\n    \n    # Test cases defined in the problem statement.\n    test_cases = [\n        {\n            \"T\": 1.7, \"pi_m\": 0.2, \"pi_t\": 0.05, \"tau\": 0.01,\n            \"variants\": [(2.2, 0.004, 1), (-0.3, 0.008, 1), (0.0, 0.015, 1), (1.1, 0.0005, 0)]\n        },\n        {\n            \"T\": 1.0, \"pi_m\": 0.1, \"pi_t\": 0.02, \"tau\": 0.01,\n            \"variants\": [(3.0, 0.0001, 0), (-2.0, 0.005, 0)]\n        },\n        {\n            \"T\": 2.5, \"pi_m\": 0.5, \"pi_t\": 0.001, \"tau\": 0.02,\n            \"variants\": [(-1.0, 0.005, 1), (-0.5, 0.003, 1), (-3.0, 0.0002, 1), (-2.0, 0.018, 1)]\n        },\n        {\n            \"T\": 0.5, \"pi_m\": 0.3, \"pi_t\": 0.1, \"tau\": 0.05,\n            \"variants\": [(10.0, 0.0001, 1), (-10.0, 0.001, 1), (0.2, 0.2, 1)]\n        }\n    ]\n\n    def calculate_gene_prob(T, pi_m, pi_t, tau, variants):\n        \"\"\"\n        Calculates the gene-level probability for a single case.\n        \"\"\"\n        # Pre-calculate the prior odds correction factor to avoid redundant computation.\n        # This factor adjusts the model's posterior odds to the true deployment prior.\n        if pi_m == 1.0 or pi_m == 0.0 or pi_t == 1.0 or pi_t == 0.0:\n            # Handle edge cases, though problem constraints 0  pi  1 prevent this.\n            return 0.0\n\n        O_m = pi_m / (1.0 - pi_m)  # Model prior odds\n        O_t = pi_t / (1.0 - pi_t)  # True (deployment) prior odds\n        prior_correction_factor = O_t / O_m\n\n        # List to store the corrected posterior probabilities of variants that pass the filter.\n        filtered_probs = []\n\n        for l, f, g in variants:\n            # Step 1: Apply the filtering rule.\n            # The variant must be present in the individual (g=1) and rare (f = tau).\n            if g == 1 and f = tau:\n                # Step 2: Calculate model posterior odds from the calibrated logit.\n                # O_m_post = exp(l/T) is a more direct calculation than p/(1-p).\n                calibrated_logit = l / T\n                O_m_post = np.exp(calibrated_logit)\n\n                # Step 3: Adjust the posterior odds using the prior correction factor.\n                O_t_post = O_m_post * prior_correction_factor\n\n                # Step 4: Convert the corrected odds back to a probability.\n                p_t = O_t_post / (1.0 + O_t_post)\n                \n                filtered_probs.append(p_t)\n\n        # Step 5: Aggregate probabilities to get the gene-level score.\n        # The probability of at least one pathogenic event is 1 minus the probability\n        # that no pathogenic events occur.\n        if not filtered_probs:\n            return 0.0\n        \n        # Calculate product of complements: prod(1 - p_t_i)\n        prob_none_pathogenic = np.prod([1.0 - p for p in filtered_probs])\n        \n        # P(at least one) = 1 - P(none)\n        p_gene = 1.0 - prob_none_pathogenic\n        \n        return p_gene\n\n    results = []\n    for case in test_cases:\n        p_gene = calculate_gene_prob(\n            case[\"T\"], case[\"pi_m\"], case[\"pi_t\"], case[\"tau\"], case[\"variants\"]\n        )\n        results.append(p_gene)\n\n    # Format the results as specified: a comma-separated list in brackets,\n    # with each number rounded to six decimal places.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}