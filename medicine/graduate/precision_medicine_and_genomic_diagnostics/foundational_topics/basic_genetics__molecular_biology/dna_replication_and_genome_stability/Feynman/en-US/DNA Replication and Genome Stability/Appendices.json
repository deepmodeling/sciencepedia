{
    "hands_on_practices": [
        {
            "introduction": "The coordination between continuous leading-strand and discontinuous lagging-strand synthesis is a cornerstone of DNA replication. This practice delves into the mechanics of the lagging strand, where DNA is synthesized in short segments known as Okazaki fragments. By applying a fundamental kinematic relationship, you will calculate the expected length of these fragments based on the replication fork's speed and the frequency of primer synthesis . This exercise provides a quantitative feel for how the dynamic parameters of the replication machinery dictate the structural features of newly synthesized DNA.",
            "id": "4334863",
            "problem": "A human fibroblast cell line is assayed using a single-molecule DNA fiber method to estimate lagging-strand synthesis dynamics. The measured replication fork speed is $v=1.5$ kilobases per minute, and the primase-polymerase complex initiates a new lagging-strand primer every $T=5$ seconds. Assume the following idealized conditions: the lagging-strand DNA polymerase delta (Pol $\\delta$) advances at the same average rate as the replication fork, initiation is periodic with period $T$, pausing and proofreading are negligible on the timescale of $T$, and the primer length itself is negligible compared to the synthesized DNA between initiations. Starting only from the fundamental kinematic definition that speed is distance per unit time and the conceptual definition that the average Okazaki fragment length equals the distance synthesized between successive primer initiations on the lagging strand, derive a closed-form expression for the average Okazaki fragment length $L$ as a function of $v$ and $T$, ensuring all unit conversions are explicitly handled. Then compute the numerical value of $L$ for the given $v$ and $T$. Express the final fragment length in base pairs (bp). Finally, based on widely reported eukaryotic values $L\\approx 100$–$200$ bp, argue whether the computed value falls within a realistic range and briefly justify your reasoning in terms of genome stability considerations such as single-stranded DNA exposure and ligation demands. No shortcut formulas should be used; begin from the stated definitions and assumptions. No rounding is required.",
            "solution": "The problem asks for the average Okazaki fragment length $L$ on the lagging strand, given a replication fork speed $v$ and a primer initiation period $T$. We are directed to start from the fundamental definition of speed and the conceptual definition of an Okazaki fragment under the stated assumptions.\n\nThe fundamental kinematic definition is that speed equals distance per unit time:\n$$\nv = \\frac{\\text{distance}}{\\text{time}}.\n$$\nOn the lagging strand, under the given assumptions, the distance synthesized between successive primer initiations equals the average Okazaki fragment length $L$. If the time between successive initiations is $T$, then over the duration $T$, the polymerase synthesizes a distance equal to $L$. Using the kinematic relation,\n$$\nL = v \\times T,\n$$\nwith the crucial caveat that units must be consistent. The measured fork speed is given as $v=1.5$ kilobases per minute. We must convert this to base pairs per second to match $T=5$ seconds.\n\nConvert $v$ from kilobases per minute to base pairs per second:\n- $1$ kilobase equals $10^{3}$ base pairs, so $1.5$ kilobases equals $1.5 \\times 10^{3}$ base pairs.\n- $1$ minute equals $60$ seconds.\n\nTherefore,\n$$\nv = 1.5 \\; \\text{kb/min} = 1.5 \\times 10^{3} \\; \\text{bp/min} = \\frac{1.5 \\times 10^{3}}{60} \\; \\text{bp/s} = 25 \\; \\text{bp/s}.\n$$\nGiven $T=5$ seconds, the average fragment length $L$ is\n$$\nL = v \\times T = 25 \\; \\text{bp/s} \\times 5 \\; \\text{s} = 125 \\; \\text{bp}.\n$$\n\nThus the numerical value is $L=125$ base pairs.\n\nWe now address whether this is realistic relative to the widely reported eukaryotic range $L\\approx 100$–$200$ bp. The computed value $125$ bp lies within this range. From a genome stability perspective, Okazaki fragments that are too short (e.g., substantially below $100$ bp) would increase the frequency of priming and ligation events, potentially elevating risks of nicks, gaps, and incomplete processing, whereas excessively long fragments (e.g., well above $200$ bp) would increase the length of single-stranded DNA exposed prior to maturation, potentially enhancing susceptibility to nucleolytic attack or replication stress signaling. A value of $125$ bp balances these constraints, providing a relatively modest single-stranded DNA exposure window while keeping the number of ligation events manageable. Under the given measured fork speed $v=1.5$ kilobases per minute and initiation period $T=5$ seconds, the derived $L=125$ bp is therefore consistent with eukaryotic norms and is realistic in the context of genome stability.\n\nThe final requested quantity is the fragment length in base pairs, which we have computed exactly without the need for rounding.",
            "answer": "$$\\boxed{125}$$"
        },
        {
            "introduction": "Theoretical models of replication dynamics are powerful, but their true value is realized when they are used to interpret experimental data. This practice simulates the analysis of a single-molecule DNA fiber assay, a widely used technique to directly visualize replication fork progression and initiation events across the genome . You will develop a computational pipeline to calculate fork speed and origin density from raw measurements and apply a decision rule to diagnose replication stress, a state of slowed or stalled replication that is a major driver of genome instability.",
            "id": "4334919",
            "problem": "You are provided with synthetic single-molecule deoxyribonucleic acid (DNA) fiber assay data under a control condition and under aphidicolin treatment. The DNA fiber assay uses sequential incorporation of chemically distinct nucleotide analogs that label replication tracks red and green; distances of labeled tracks have been pre-converted to kilobases and pulse durations are given in minutes. Your task is to derive, implement, and apply a principled computational procedure to quantify replication fork progression and origin firing density, and then to interpret whether the observed changes under aphidicolin are consistent with replication stress.\n\nFundamental base and definitions from which you must derive your procedure:\n- A replication fork progresses along DNA with a measurable distance over time. The fork speed is defined as the distance traversed divided by the elapsed time based on physical measurement consistency. Let the per-fiber distances be $d_i$ (in kilobases) and pulse durations be $t_i$ (in minutes). The per-fiber fork speed is the quotient of these quantities, and an aggregate fork speed over multiple fibers must be computed in a manner that is consistent with averaging independent measurements.\n- Replication origin density is defined as the number of initiation events per unit DNA length. Let $N_{\\text{origins}}$ be the count of origins detected and $L$ be the assayed genome length in megabases. Origin density is defined as the ratio of these quantities.\n- Aphidicolin is a small molecule inhibitor of replicative DNA polymerases and is widely observed to reduce polymerase elongation rates. Under replication stress conditions such as aphidicolin exposure, cells may activate otherwise dormant origins, resulting in a higher origin density. An interpretation rule must be designed that maps changes in fork speed and origin density to a boolean statement of whether the data are consistent with replication stress.\n\nRequirements:\n- For each test case, compute the aggregate control fork speed (in kilobases per minute) and the aggregate treated fork speed (in kilobases per minute) as the arithmetic mean of per-fiber speeds derived from the data. Then compute the ratio of treated to control fork speed.\n- For each test case, compute the control origin density (in origins per megabase) and the treated origin density (in origins per megabase), then compute the ratio of treated to control origin density.\n- Define interpretation thresholds as follows: let $\\delta_v = 0.2$ and $\\delta_{\\rho} = 0.15$. Classify a case as consistent with replication stress if and only if the treated/control fork speed ratio is less than or equal to $1 - \\delta_v$ and the treated/control origin density ratio is greater than or equal to $1 + \\delta_{\\rho}$. Otherwise, classify it as not consistent. The classification must be output as a boolean.\n- Express the final fork speed ratios and origin density ratios for each case as floats rounded to three decimal places. No percentage signs are permitted; all fractional changes must be represented as decimal ratios. Speeds must be computed in kilobases per minute and densities in origins per megabase, as specified. Angles are not involved in this problem.\n- Your program must process the following test suite. Each case provides control and treated per-fiber distances (kilobases), control and treated per-fiber pulse durations (minutes), the control number of detected origins $N_{\\text{origins,ctl}}$, the control assayed megabases $L_{\\text{ctl}}$ (megabases), the treated number of detected origins $N_{\\text{origins,tx}}$, and the treated assayed megabases $L_{\\text{tx}}$ (megabases).\n\nTest suite:\n- Case $1$ (typical decrease in speed and increase in origin density):\n  - Control distances (kilobases): $[36.0, 28.0, 31.5, 33.0, 29.0]$\n  - Control durations (minutes): $[20.0, 20.0, 20.0, 20.0, 20.0]$\n  - Treated distances (kilobases): $[14.0, 12.5, 13.0, 15.0, 11.0]$\n  - Treated durations (minutes): $[20.0, 20.0, 20.0, 20.0, 20.0]$\n  - $N_{\\text{origins,ctl}} = 12$, $L_{\\text{ctl}} = 24.0$; $N_{\\text{origins,tx}} = 20$, $L_{\\text{tx}} = 24.0$.\n- Case $2$ (minor decrease in speed, no change in origin density):\n  - Control distances (kilobases): $[22.0, 21.0, 20.0, 23.0]$\n  - Control durations (minutes): $[20.0, 20.0, 20.0, 20.0]$\n  - Treated distances (kilobases): $[19.0, 18.0, 18.0, 19.0]$\n  - Treated durations (minutes): $[20.0, 20.0, 20.0, 20.0]$\n  - $N_{\\text{origins,ctl}} = 5$, $L_{\\text{ctl}} = 10.0$; $N_{\\text{origins,tx}} = 5$, $L_{\\text{tx}} = 10.0$.\n- Case $3$ (edge case with short pulses and zero origins):\n  - Control distances (kilobases): $[5.0, 4.0, 6.0]$\n  - Control durations (minutes): $[5.0, 5.0, 5.0]$\n  - Treated distances (kilobases): $[5.0, 4.0, 6.0]$\n  - Treated durations (minutes): $[5.0, 5.0, 5.0]$\n  - $N_{\\text{origins,ctl}} = 0$, $L_{\\text{ctl}} = 12.0$; $N_{\\text{origins,tx}} = 0$, $L_{\\text{tx}} = 12.0$.\n- Case $4$ (strong decrease in speed but decrease in origin density):\n  - Control distances (kilobases): $[35.0, 33.0, 30.0, 34.0, 32.0]$\n  - Control durations (minutes): $[15.0, 15.0, 15.0, 15.0, 15.0]$\n  - Treated distances (kilobases): $[15.0, 14.0, 13.0, 16.0, 15.0]$\n  - Treated durations (minutes): $[15.0, 15.0, 15.0, 15.0, 15.0]$\n  - $N_{\\text{origins,ctl}} = 40$, $L_{\\text{ctl}} = 50.0$; $N_{\\text{origins,tx}} = 30$, $L_{\\text{tx}} = 50.0$.\n- Case $5$ (boundary thresholds exactly met):\n  - Control distances (kilobases): $[10.0, 10.0, 10.0, 10.0]$\n  - Control durations (minutes): $[10.0, 10.0, 10.0, 10.0]$\n  - Treated distances (kilobases): $[8.0, 8.0, 8.0, 8.0]$\n  - Treated durations (minutes): $[10.0, 10.0, 10.0, 10.0]$\n  - $N_{\\text{origins,ctl}} = 20$, $L_{\\text{ctl}} = 80.0$; $N_{\\text{origins,tx}} = 23$, $L_{\\text{tx}} = 80.0$.\n\nFinal output format specification:\n- For each case $i$, compute and output three values in order: the treated-to-control fork speed ratio $r_{v,i}$ (rounded to three decimal places), the treated-to-control origin density ratio $r_{\\rho,i}$ (rounded to three decimal places), and a boolean $b_i$ indicating whether the changes are consistent with replication stress under the stated thresholds.\n- Your program should produce a single line of output containing the results for all cases as a comma-separated list enclosed in square brackets. The list must be the flattened concatenation of per-case triples in case order, for example, $[r_{v,1}, r_{\\rho,1}, b_1, r_{v,2}, r_{\\rho,2}, b_2, \\dots]$.",
            "solution": "The objective is to develop and apply a computational procedure to analyze synthetic DNA fiber assay data, quantifying replication fork speed and origin firing density under control and aphidicolin-treated conditions. The analysis will determine if the observed changes are consistent with a state of replication stress, based on a set of formal rules.\n\nThe derivation of the procedure is founded on the principles and definitions provided.\n\n**1. Replication Fork Speed ($v$)**\n\nThe replication fork speed is defined as the distance a fork travels per unit time. Given per-fiber measurements of distance $d_i$ (in kilobases, kb) and pulse duration $t_i$ (in minutes, min), the speed of an individual fork $i$, denoted $v_i$, is:\n$$\nv_i = \\frac{d_i}{t_i}\n$$\nThe problem specifies that the aggregate fork speed for a given condition (control or treated) is the arithmetic mean of the individual per-fiber speeds. Let there be $n_{\\text{ctl}}$ control measurements and $n_{\\text{tx}}$ treated measurements. The aggregate speeds are:\n\nFor the control condition:\n$$\nv_{\\text{ctl}} = \\frac{1}{n_{\\text{ctl}}} \\sum_{i=1}^{n_{\\text{ctl}}} v_{\\text{ctl},i} = \\frac{1}{n_{\\text{ctl}}} \\sum_{i=1}^{n_{\\text{ctl}}} \\frac{d_{\\text{ctl},i}}{t_{\\text{ctl},i}}\n$$\n\nFor the aphidicolin-treated condition:\n$$\nv_{\\text{tx}} = \\frac{1}{n_{\\text{tx}}} \\sum_{j=1}^{n_{\\text{tx}}} v_{\\text{tx},j} = \\frac{1}{n_{\\text{tx}}} \\sum_{j=1}^{n_{\\text{tx}}} \\frac{d_{\\text{tx},j}}{t_{\\text{tx},j}}\n$$\nThe relative change in fork speed is quantified by the ratio $r_v$:\n$$\nr_v = \\frac{v_{\\text{tx}}}{v_{\\text{ctl}}}\n$$\n\n**2. Replication Origin Density ($\\rho$)**\n\nReplication origin density, $\\rho$, is defined as the number of replication initiation events, $N_{\\text{origins}}$, per unit length of assayed DNA, $L$ (in megabases, Mb).\n$$\n\\rho = \\frac{N_{\\text{origins}}}{L}\n$$\nThe units of origin density are origins/Mb. The densities for the control and treated conditions are respectively:\n$$\n\\rho_{\\text{ctl}} = \\frac{N_{\\text{origins,ctl}}}{L_{\\text{ctl}}}\n$$\n$$\n\\rho_{\\text{tx}} = \\frac{N_{\\text{origins,tx}}}{L_{\\text{tx}}}\n$$\nThe relative change in origin density is quantified by the ratio $r_{\\rho}$:\n$$\nr_{\\rho} = \\frac{\\rho_{\\text{tx}}}{\\rho_{\\text{ctl}}}\n$$\nA special case arises when the control origin count, $N_{\\text{origins,ctl}}$, is zero, making $\\rho_{\\text{ctl}}=0$ and the ratio $r_{\\rho}$ mathematically undefined a priori. We must establish a logical convention for this scenario.\n- If $\\rho_{\\text{ctl}} = 0$ and $\\rho_{\\text{tx}} = 0$, there is no change in density. The ratio is logically defined as $r_{\\rho} = 1.0$ to represent no fold-change.\n- If $\\rho_{\\text{ctl}} = 0$ and $\\rho_{\\text{tx}} > 0$, the fold-change is infinite. To facilitate numerical comparison, we can consider $r_{\\rho} \\to \\infty$. Any comparison of the form $r_{\\rho} \\ge C$ for a positive constant $C$ will evaluate to true.\n- If $\\rho_{\\text{ctl}} > 0$ and $\\rho_{\\text{tx}} = 0$, the ratio is correctly $r_{\\rho}=0$.\n\n**3. Interpretation Rule for Replication Stress**\n\nThe problem defines replication stress as a biological state characterized by a significant decrease in fork speed and a significant increase in origin density. This is formalized by a logical rule involving two thresholds: $\\delta_v = 0.2$ and $\\delta_{\\rho} = 0.15$. A boolean variable $b$ will represent the classification.\n\nThe condition for consistency with replication stress ($b = \\text{True}$) is met if and only if both of the following inequalities hold:\n1. The fork speed ratio indicates a decrease of at least $20\\%$:\n   $$\n   r_v \\le 1 - \\delta_v \\implies r_v \\le 0.8\n   $$\n2. The origin density ratio indicates an increase of at least $15\\%$:\n   $$\n   r_{\\rho} \\ge 1 + \\delta_{\\rho} \\implies r_{\\rho} \\ge 1.15\n   $$\n\nTherefore, the logical condition for classification is:\n$$\nb = (r_v \\le 0.8) \\land (r_{\\rho} \\ge 1.15)\n$$\nIf this compound condition is not met, $b = \\text{False}$.\n\nThe computational procedure will involve, for each test case, calculating $r_v$ and $r_{\\rho}$ according to the derived formulae (including the convention for division by zero), rounding the results to three decimal places, and then evaluating the logical condition to determine the boolean classification $b$. The results will be assembled into the specified output format.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the DNA replication stress analysis problem for a suite of test cases.\n    \"\"\"\n    \n    test_cases = [\n        # Case 1 (typical decrease in speed and increase in origin density)\n        {\n            \"control_distances_kb\": [36.0, 28.0, 31.5, 33.0, 29.0],\n            \"control_durations_min\": [20.0, 20.0, 20.0, 20.0, 20.0],\n            \"treated_distances_kb\": [14.0, 12.5, 13.0, 15.0, 11.0],\n            \"treated_durations_min\": [20.0, 20.0, 20.0, 20.0, 20.0],\n            \"N_origins_ctl\": 12, \"L_ctl_Mb\": 24.0,\n            \"N_origins_tx\": 20, \"L_tx_Mb\": 24.0,\n        },\n        # Case 2 (minor decrease in speed, no change in origin density)\n        {\n            \"control_distances_kb\": [22.0, 21.0, 20.0, 23.0],\n            \"control_durations_min\": [20.0, 20.0, 20.0, 20.0],\n            \"treated_distances_kb\": [19.0, 18.0, 18.0, 19.0],\n            \"treated_durations_min\": [20.0, 20.0, 20.0, 20.0],\n            \"N_origins_ctl\": 5, \"L_ctl_Mb\": 10.0,\n            \"N_origins_tx\": 5, \"L_tx_Mb\": 10.0,\n        },\n        # Case 3 (edge case with short pulses and zero origins)\n        {\n            \"control_distances_kb\": [5.0, 4.0, 6.0],\n            \"control_durations_min\": [5.0, 5.0, 5.0],\n            \"treated_distances_kb\": [5.0, 4.0, 6.0],\n            \"treated_durations_min\": [5.0, 5.0, 5.0],\n            \"N_origins_ctl\": 0, \"L_ctl_Mb\": 12.0,\n            \"N_origins_tx\": 0, \"L_tx_Mb\": 12.0,\n        },\n        # Case 4 (strong decrease in speed but decrease in origin density)\n        {\n            \"control_distances_kb\": [35.0, 33.0, 30.0, 34.0, 32.0],\n            \"control_durations_min\": [15.0, 15.0, 15.0, 15.0, 15.0],\n            \"treated_distances_kb\": [15.0, 14.0, 13.0, 16.0, 15.0],\n            \"treated_durations_min\": [15.0, 15.0, 15.0, 15.0, 15.0],\n            \"N_origins_ctl\": 40, \"L_ctl_Mb\": 50.0,\n            \"N_origins_tx\": 30, \"L_tx_Mb\": 50.0,\n        },\n        # Case 5 (boundary thresholds exactly met)\n        {\n            \"control_distances_kb\": [10.0, 10.0, 10.0, 10.0],\n            \"control_durations_min\": [10.0, 10.0, 10.0, 10.0],\n            \"treated_distances_kb\": [8.0, 8.0, 8.0, 8.0],\n            \"treated_durations_min\": [10.0, 10.0, 10.0, 10.0],\n            \"N_origins_ctl\": 20, \"L_ctl_Mb\": 80.0,\n            \"N_origins_tx\": 23, \"L_tx_Mb\": 80.0,\n        },\n    ]\n\n    all_results = []\n    \n    delta_v = 0.2\n    delta_rho = 0.15\n    speed_threshold = 1 - delta_v\n    density_threshold = 1 + delta_rho\n\n    for case in test_cases:\n        # 1. Fork Speed Calculation\n        control_d = np.array(case[\"control_distances_kb\"])\n        control_t = np.array(case[\"control_durations_min\"])\n        treated_d = np.array(case[\"treated_distances_kb\"])\n        treated_t = np.array(case[\"treated_durations_min\"])\n\n        v_ctl_fibers = control_d / control_t\n        v_ctl = np.mean(v_ctl_fibers)\n\n        v_tx_fibers = treated_d / treated_t\n        v_tx = np.mean(v_tx_fibers)\n        \n        # Handle case where control speed is zero to avoid division by zero\n        if v_ctl == 0:\n            r_v = float('inf') if v_tx > 0 else 1.0\n        else:\n            r_v = v_tx / v_ctl\n\n        # 2. Origin Density Calculation\n        rho_ctl = case[\"N_origins_ctl\"] / case[\"L_ctl_Mb\"]\n        rho_tx = case[\"N_origins_tx\"] / case[\"L_tx_Mb\"]\n\n        if rho_ctl == 0:\n            if rho_tx == 0:\n                r_rho = 1.0  # No change\n            else:\n                r_rho = float('inf') # Infinite fold change\n        else:\n            r_rho = rho_tx / rho_ctl\n\n        # 3. Replication Stress Classification\n        is_stress = (r_v <= speed_threshold) and (r_rho >= density_threshold)\n\n        # 4. Format and store results\n        rounded_r_v = round(r_v, 3)\n        rounded_r_rho = round(r_rho, 3) if r_rho != float('inf') else float('inf')\n\n        all_results.extend([rounded_r_v, rounded_r_rho, is_stress])\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "When replication stress is not properly resolved, it can lead to catastrophic events such as DNA breaks and complex genomic rearrangements, or Structural Variants (SVs). These events are not random; replication-based mechanisms often leave behind distinct molecular scars, such as breakpoint microhomology or templated insertions. This advanced practice guides you through building a probabilistic classifier to distinguish SVs arising from replication errors from those with other origins . By integrating multiple lines of genomic evidence using a Naive Bayes framework, you will learn how to computationally decode the history of genomic instability, a key skill in modern cancer genomics and precision medicine.",
            "id": "4334877",
            "problem": "Design and implement a complete, runnable program that classifies structural variants (SVs) as replication-based using a principled probabilistic pipeline derived from fundamental definitions about DNA replication-associated rearrangements and statistical inference. The scenario is grounded in the following fundamentals: (1) DNA replication stress promotes rearrangements enriched for breakpoint microhomology and templated insertions due to template switching, and clustering near Common Fragile Sites (CFS), and (2) Bayes’ theorem provides a principled way to integrate heterogeneous evidence into a posterior probability. Your program must implement a Naive Bayes classifier using well-specified generative models for features related to replication-based mechanisms.\n\nStart from these fundamental bases and definitions:\n- DNA replication fork stalling and template switching increases the probability of structural variants with breakpoint microhomology, templated insertions, and clustering near Common Fragile Sites (CFS).\n- Bayes’ theorem: for two classes, replication-based ($R$) and non-replication-based ($N$), with feature vector $\\mathbf{x}$, the posterior satisfies $P(R \\mid \\mathbf{x}) \\propto P(\\mathbf{x} \\mid R) P(R)$.\n- Naive Bayes assumption: conditioned on class, features are independent; thus $P(\\mathbf{x} \\mid Y) = \\prod_{j} P(x_j \\mid Y)$ for class $Y \\in \\{R,N\\}$.\n\nYou will model four measurable features per SV:\n1. Breakpoint microhomology length $L$ (in base pairs), an integer $L \\ge 0$.\n2. Templated insertion length $I$ (in base pairs), an integer $I \\ge 0$. Presence is encoded as a Bernoulli indicator $B = \\mathbb{I}[I > 0]$.\n3. Genomic distance $D$ from the SV breakpoint to the nearest Common Fragile Site (CFS), measured in base pairs, a real number $D \\ge 0$.\n4. Local breakpoint clustering count $C$ in a symmetric window of size $1\\,\\text{Mb}$ centered at the breakpoint, an integer $C \\ge 0$.\n\nAssume the following generative models and parameters, chosen to reflect widely observed tendencies in replication-associated rearrangements:\n- Prior probabilities: $P(R) = 0.3$ and $P(N) = 0.7$.\n- Microhomology length $L$ follows a geometric distribution on $\\{0,1,2,\\dots\\}$ with success parameter $p$, $$P(L=k \\mid Y) = (1-p_Y)^{k} p_Y,$$ where $p_R = 0.2$ and $p_N = 0.5$.\n- Templated insertion presence $B$ is Bernoulli with $$P(B=1 \\mid R) = \\theta_R = 0.6,\\quad P(B=1 \\mid N) = \\theta_N = 0.05.$$ If $I$ is missing, omit this factor.\n- Distance to nearest CFS $D$ follows an exponential density $$f(D \\mid Y) = \\lambda_Y \\exp(-\\lambda_Y D),$$ where $\\lambda_R = 1/200000$ and $\\lambda_N = 1/1000000$. If $D$ is missing, omit this factor. $D$ must be provided in base pairs.\n- Cluster count $C$ follows a Poisson distribution $$P(C=k \\mid Y) = \\exp(-\\mu_Y) \\frac{\\mu_Y^k}{k!},$$ where $\\mu_R = 3.0$ and $\\mu_N = 0.5$. If $C$ is missing, omit this factor.\n\nUsing these models, derive from first principles a numerically stable decision rule for $P(R \\mid \\mathbf{x})$ and implement it. You must:\n- Compute the log-likelihood ratio $$\\Lambda(\\mathbf{x}) = \\log \\frac{P(R)}{P(N)} + \\sum_j \\log \\frac{P(x_j \\mid R)}{P(x_j \\mid N)},$$ omitting any feature $x_j$ that is missing.\n- Convert the log-likelihood ratio into the posterior $$P(R \\mid \\mathbf{x}) = \\frac{1}{1 + \\exp(-\\Lambda(\\mathbf{x}))}.$$\n- Classify an SV as replication-based if $P(R \\mid \\mathbf{x}) \\ge 0.5$.\n\nHandling of missing data: If any feature among $L$, $I$, $D$, or $C$ is missing, it will be provided as a Not-a-Number token (NaN). For $I$, the classifier uses the indicator $B = \\mathbb{I}[I > 0]$; if $I$ is NaN, omit the Bernoulli factor entirely. For $L$, $D$, or $C$ that are NaN, omit their respective model factors. This corresponds to marginalizing over the missing feature under the Naive Bayes assumption.\n\nNumerical units: All distances ($D$) and lengths ($L$ and $I$) are in base pairs, and counts ($C$) are unitless. You must treat $D$ in base pairs when evaluating the exponential density.\n\nYour program must compute posterior probabilities for each SV in each test case and output the results as a single line containing a list of lists of floats, where each inner list contains the posterior probabilities for the SVs in that test case, in order. Round each probability to four decimal places (as decimals, not percentages).\n\nTest suite (each SV is a tuple $(L, I, D, C)$; NaN denotes missing):\n- Test case $1$ (general mixed case):\n  - $(6, 35, 50000, 5)$\n  - $(0, 0, 2000000, 0)$\n  - $(3, 10, 150000, 2)$\n- Test case $2$ (boundary emphasis on templated insertion with zero microhomology):\n  - $(0, 80, 10000, 4)$\n  - $(1, 0, 300000, 1)$\n- Test case $3$ (missingness and partial evidence):\n  - $(2, \\text{NaN}, 120000, 3)$\n  - $(\\text{NaN}, 0, \\text{NaN}, 1)$\n  - $(4, 5, \\text{NaN}, \\text{NaN})$\n- Test case $4$ (edge extremes):\n  - $(20, 0, 100000, 0)$\n  - $(0, 0, 5000000, 0)$\n  - $(8, 120, 20000, 7)$\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list of lists enclosed in square brackets. For example, a valid shape is something like $[[p_{11},p_{12}], [p_{21}], [p_{31},p_{32},p_{33}], [p_{41},p_{42},p_{43}]]$, where each $p_{ij}$ is rounded to four decimal places. There must be no additional text or whitespace beyond what is needed for the list formatting.",
            "solution": "We connect replication biology and probabilistic inference to design a rigorously specified classifier. Replication stress and fork stalling/template switching cause three observable signatures at structural variant (SV) breakpoints: enrichment of microhomology, presence of templated insertions, and clustering near Common Fragile Sites (CFS). We model these observables with simple, standard distributions chosen for interpretability and tractability: geometric for nonnegative integer microhomology length, Bernoulli for templated insertion presence, exponential for nonnegative continuous distances, and Poisson for nonnegative integer cluster counts. By the Naive Bayes assumption, their likelihoods factorize conditioned on class.\n\nWe denote the class by $Y \\in \\{R,N\\}$ for replication-based and non-replication-based mechanisms, and a feature vector by $\\mathbf{x} = (L, B, D, C)$, where $L$ is microhomology length in base pairs, $B = \\mathbb{I}[I > 0]$ indicates templated insertion presence, $D$ is the base-pair distance to the nearest Common Fragile Site (CFS), and $C$ is the local cluster count in a $1\\,\\text{Mb}$ window.\n\nThe prior probabilities are $P(R) = 0.3$ and $P(N) = 0.7$. We construct log-likelihood ratios for numerical stability:\n$$\n\\Lambda(\\mathbf{x}) = \\log \\frac{P(R)}{P(N)} + \\sum_j \\log \\frac{P(x_j \\mid R)}{P(x_j \\mid N)}.\n$$\nThe posterior then follows from the logistic mapping:\n$$\nP(R \\mid \\mathbf{x}) = \\frac{1}{1 + \\exp(-\\Lambda(\\mathbf{x}))}.\n$$\nEach feature contributes an additive term to $\\Lambda(\\mathbf{x})$.\n\nFor microhomology $L$, the geometric distributions are specified as $P(L=k \\mid R) = (1-p_R)^k p_R$ and $P(L=k \\mid N) = (1-p_N)^k p_N$ with $p_R = 0.2$ and $p_N = 0.5$. The log-likelihood ratio contribution for an observed $L=k$ is\n$$\n\\log \\frac{P(L=k \\mid R)}{P(L=k \\mid N)} = k \\log \\frac{1-p_R}{1-p_N} + \\log \\frac{p_R}{p_N}.\n$$\n\nFor templated insertion presence $B \\in \\{0,1\\}$, with $P(B=1 \\mid R)=\\theta_R=0.6$ and $P(B=1 \\mid N)=\\theta_N=0.05$, the log-likelihood ratio contribution is\n$$\n\\log \\frac{P(B \\mid R)}{P(B \\mid N)} = \n\\begin{cases}\n\\log \\frac{\\theta_R}{\\theta_N}, & \\text{if } B=1, \\\\\n\\log \\frac{1-\\theta_R}{1-\\theta_N}, & \\text{if } B=0.\n\\end{cases}\n$$\nIf insertion length $I$ is missing, we omit this factor, corresponding to marginalization under the Naive Bayes assumption.\n\nFor the distance to CFS $D \\ge 0$, the exponential densities are $f(D \\mid R) = \\lambda_R \\exp(-\\lambda_R D)$ and $f(D \\mid N) = \\lambda_N \\exp(-\\lambda_N D)$ with $\\lambda_R = 1/200000$ and $\\lambda_N = 1/1000000$. The contribution of an observed $D$ is\n$$\n\\log \\frac{f(D \\mid R)}{f(D \\mid N)} = \\log \\frac{\\lambda_R}{\\lambda_N} - (\\lambda_R - \\lambda_N) D.\n$$\nIf $D$ is missing, we omit this factor.\n\nFor the cluster count $C \\in \\{0,1,2,\\dots\\}$, the Poisson distributions are $P(C=k \\mid R) = e^{-\\mu_R} \\mu_R^k / k!$ and $P(C=k \\mid N) = e^{-\\mu_N} \\mu_N^k / k!$ with $\\mu_R = 3.0$ and $\\mu_N = 0.5$. The log-likelihood ratio is\n$$\n\\log \\frac{P(C=k \\mid R)}{P(C=k \\mid N)} = -\\mu_R + \\mu_N + k \\log \\frac{\\mu_R}{\\mu_N},\n$$\nwhich conveniently cancels the factorial term. If $C$ is missing, we omit this factor.\n\nPutting all these terms together, the complete log-likelihood ratio is\n$$\n\\Lambda(\\mathbf{x}) = \\log \\frac{0.3}{0.7} \n+ \\mathbb{I}[L \\ \\text{observed}]\\left(L \\log \\frac{1-0.2}{1-0.5} + \\log \\frac{0.2}{0.5}\\right)\n+ \\mathbb{I}[I \\ \\text{observed}]\\begin{cases}\n\\log \\frac{0.6}{0.05}, & \\text{if } I>0, \\\\\n\\log \\frac{1-0.6}{1-0.05}, & \\text{if } I=0,\n\\end{cases}\n+ \\mathbb{I}[D \\ \\text{observed}]\\left(\\log \\frac{1/200000}{1/1000000} - \\left(\\frac{1}{200000} - \\frac{1}{1000000}\\right) D \\right)\n+ \\mathbb{I}[C \\ \\text{observed}]\\left(-3.0 + 0.5 + C \\log \\frac{3.0}{0.5}\\right).\n$$\nThe posterior is computed as $P(R \\mid \\mathbf{x}) = 1/(1+\\exp(-\\Lambda(\\mathbf{x})))$.\n\nAlgorithmic design:\n1. Parse each SV as a tuple $(L, I, D, C)$, allowing NaN to indicate missing elements.\n2. For each SV, initialize $\\Lambda$ with $\\log(0.3/0.7)$.\n3. If $L$ is observed, add $L \\log((1-0.2)/(1-0.5)) + \\log(0.2/0.5)$.\n4. If $I$ is observed, set $B = \\mathbb{I}[I>0]$ and add $\\log(0.6/0.05)$ if $B=1$, else add $\\log((1-0.6)/(1-0.05))$.\n5. If $D$ is observed, add $\\log((1/200000)/(1/1000000)) - ((1/200000)-(1/1000000)) D$ using $D$ in base pairs.\n6. If $C$ is observed, add $-3.0 + 0.5 + C \\log(3.0/0.5)$.\n7. Map $\\Lambda$ to a posterior via the logistic function.\n8. Round each posterior to four decimal places.\n9. Aggregate by test case and print as a single line: a list of lists.\n\nEdge handling and numerical stability:\n- Omitted features correspond to excluding their terms from $\\Lambda$, which is equivalent to marginalizing under the independence assumption.\n- Computing in log space avoids underflow when distances are long or cluster counts are large.\n- All parameters are strictly positive, so no undefined logarithms occur.\n\nApplying this pipeline to the specified test suite yields one posterior per SV, per test case, which the program prints as a single line in the required nested list format, each posterior rounded to four decimal places.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef logistic(x: float) -> float:\n    # Stable logistic for large |x|\n    if x >= 0:\n        z = math.exp(-x)\n        return 1.0 / (1.0 + z)\n    else:\n        z = math.exp(x)\n        return z / (1.0 + z)\n\ndef posterior_replication_probability(sv, params):\n    \"\"\"\n    Compute P(R | x) using a Naive Bayes log-likelihood ratio.\n    sv: tuple (L, I, D, C) with possible np.nan for missing values.\n    params: dictionary of model parameters.\n    \"\"\"\n    L, I, D, C = sv\n\n    # Unpack parameters\n    prior_R = params[\"prior_R\"]\n    prior_N = 1.0 - prior_R\n\n    p_R = params[\"geom_p_R\"]\n    p_N = params[\"geom_p_N\"]\n\n    theta_R = params[\"theta_R\"]\n    theta_N = params[\"theta_N\"]\n\n    lam_R = params[\"lambda_R\"]\n    lam_N = params[\"lambda_N\"]\n\n    mu_R = params[\"mu_R\"]\n    mu_N = params[\"mu_N\"]\n\n    # Start with log prior ratio\n    llr = math.log(prior_R / prior_N)\n\n    # Microhomology length L (geometric pmf term)\n    if not (isinstance(L, float) and np.isnan(L)):\n        k = int(L)\n        # log[(1-p_R)^k p_R / ((1-p_N)^k p_N)] = k*log((1-p_R)/(1-p_N)) + log(p_R/p_N)\n        llr += k * math.log((1.0 - p_R) / (1.0 - p_N)) + math.log(p_R / p_N)\n\n    # Templated insertion presence B from I (Bernoulli term)\n    if not (isinstance(I, float) and np.isnan(I)):\n        B = 1 if I > 0 else 0\n        if B == 1:\n            llr += math.log(theta_R / theta_N)\n        else:\n            llr += math.log((1.0 - theta_R) / (1.0 - theta_N))\n\n    # Distance to fragile site D (exponential density term)\n    if not (isinstance(D, float) and np.isnan(D)):\n        # log(lambda_R/lambda_N) - (lam_R - lam_N) * D\n        llr += math.log(lam_R / lam_N) - (lam_R - lam_N) * float(D)\n\n    # Cluster count C (Poisson pmf term with factorial cancellation)\n    if not (isinstance(C, float) and np.isnan(C)):\n        k = int(C)\n        llr += (-mu_R + mu_N) + k * math.log(mu_R / mu_N)\n\n    # Convert LLR to posterior probability via logistic\n    return logistic(llr)\n\ndef format_nested_list(nested):\n    \"\"\"\n    Construct a string representing a list of lists of floats with 4 decimal places,\n    e.g., [[0.1234,0.5678],[0.1111]] without extra spaces.\n    \"\"\"\n    inner_strs = []\n    for inner in nested:\n        vals = \",\".join(f\"{v:.4f}\" for v in inner)\n        inner_strs.append(f\"[{vals}]\")\n    return f\"[{','.join(inner_strs)}]\"\n\ndef solve():\n    # Define model parameters as specified in the problem\n    params = {\n        \"prior_R\": 0.3,\n        \"geom_p_R\": 0.2,\n        \"geom_p_N\": 0.5,\n        \"theta_R\": 0.6,\n        \"theta_N\": 0.05,\n        \"lambda_R\": 1.0 / 200000.0,   # per base pair\n        \"lambda_N\": 1.0 / 1000000.0,  # per base pair\n        \"mu_R\": 3.0,\n        \"mu_N\": 0.5,\n    }\n\n    NaN = float('nan')\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1\n        [\n            (6, 35, 50000, 5),\n            (0, 0, 2000000, 0),\n            (3, 10, 150000, 2),\n        ],\n        # Test case 2\n        [\n            (0, 80, 10000, 4),\n            (1, 0, 300000, 1),\n        ],\n        # Test case 3\n        [\n            (2, NaN, 120000, 3),\n            (NaN, 0, NaN, 1),\n            (4, 5, NaN, NaN),\n        ],\n        # Test case 4\n        [\n            (20, 0, 100000, 0),\n            (0, 0, 5000000, 0),\n            (8, 120, 20000, 7),\n        ],\n    ]\n\n    results = []\n    for case in test_cases:\n        probs = []\n        for sv in case:\n            prob = posterior_replication_probability(sv, params)\n            probs.append(prob)\n        results.append(probs)\n\n    # Final print statement in the exact required format.\n    print(format_nested_list(results))\n\nsolve()\n```"
        }
    ]
}