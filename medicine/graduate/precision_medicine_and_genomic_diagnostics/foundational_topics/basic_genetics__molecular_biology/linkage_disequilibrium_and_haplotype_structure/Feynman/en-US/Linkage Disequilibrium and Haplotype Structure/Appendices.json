{
    "hands_on_practices": [
        {
            "introduction": "When analyzing linkage disequilibrium, it is crucial to distinguish true genetic association from statistical noise arising from finite sample sizes. This exercise guides you through a first-principles derivation to quantify the expected amount of spurious correlation, $\\hat{r}^2$, that occurs between unlinked markers . Understanding this baseline noise is fundamental to correctly interpreting LD patterns in any genomic dataset.",
            "id": "4355695",
            "problem": "Consider two biallelic loci, denoted by alleles $\\{A,a\\}$ and $\\{B,b\\}$, in a human rare-variant sequencing panel used for precision medicine and genomic diagnostics. Assume Hardy–Weinberg Equilibrium (HWE) and random mating in the underlying population, and suppose the loci are physically unlinked so that their true Linkage Disequilibrium (LD) coefficient is zero. Let the population allele frequencies be $p_{A}$ for allele $A$ and $p_{B}$ for allele $B$, and let the four haplotype probabilities be $P_{AB}$, $P_{Ab}$, $P_{aB}$, and $P_{ab}$, with $P_{AB} = p_{A} p_{B}$ when LD is zero.\n\nDefine the LD coefficient as $D = P_{AB} - p_{A} p_{B}$, and the standardized LD measure as $r^{2}$. In a sample of $N$ phased chromosomes (haplotypes), let the empirical frequencies be $\\hat{P}_{AB}$, $\\hat{p}_{A}$, and $\\hat{p}_{B}$, and define the empirical LD estimator $\\hat{D} = \\hat{P}_{AB} - \\hat{p}_{A}\\hat{p}_{B}$ and $\\hat{r}^{2}$.\n$$r^{2} = \\frac{D^{2}}{p_{A}(1 - p_{A})p_{B}(1 - p_{B})}$$\n$$\\hat{r}^{2} = \\frac{\\hat{D}^{2}}{\\hat{p}_{A}(1 - \\hat{p}_{A})\\hat{p}_{B}(1 - \\hat{p}_{B})}$$\nA common practice in rare-variant association studies is to apply a Minor Allele Frequency (MAF) threshold $t$ (for example $t = 0.01$), retaining only variants with minor allele frequency in the interval $[t,\\,1 - t]$. In this setting, answer the following:\n\nStarting from core definitions and the properties of multinomial sampling of haplotypes under independence, derive an expression for the leading-order behavior of $\\mathbb{E}[\\hat{r}^{2}]$ as a function of the sample size $N$ and the MAF threshold $t$. Explain, from first principles, how the dependence of LD on minor allele frequency and sample size enters the expectation, and whether conditioning on MAF thresholds $p_{A}, p_{B} \\in [t, 1 - t]$ changes the leading-order value of $\\mathbb{E}[\\hat{r}^{2}]$.\n\nFinally, consider a cohort of $2400$ unrelated individuals that has been perfectly phased, so there are $N = 4800$ haplotypes, and a MAF threshold $t = 0.01$. Under the above assumptions, compute the numerical value of the leading-order $\\mathbb{E}[\\hat{r}^{2}]$ after applying the MAF threshold, and provide your final answer rounded to four significant figures. Express the final answer as a pure number using scientific notation.",
            "solution": "The problem statement has been critically evaluated and is determined to be valid. It is scientifically grounded in the principles of population genetics, well-posed, objective, and contains sufficient information to derive a unique solution. We may therefore proceed with the derivation and calculation.\n\nThe problem asks for the leading-order behavior of the expected value of the squared sample correlation coefficient, $\\mathbb{E}[\\hat{r}^2]$, between two physically unlinked loci under the null hypothesis of no linkage disequilibrium ($D=0$).\n\nLet the two biallelic loci have alleles $\\{A, a\\}$ and $\\{B, b\\}$. In a sample of $N$ phased chromosomes (haplotypes), let the counts of the four possible haplotypes be $n_{AB}$, $n_{Ab}$, $n_{aB}$, and $n_{ab}$, such that $n_{AB} + n_{Ab} + n_{aB} + n_{ab} = N$. These counts follow a multinomial distribution with parameters $N$ and probabilities $\\{P_{AB}, P_{Ab}, P_{aB}, P_{ab}\\}$.\n\nUnder the assumption of linkage equilibrium ($D=0$), these probabilities are given by the products of the respective allele frequencies:\n$P_{AB} = p_A p_B$\n$P_{Ab} = p_A (1-p_B)$\n$P_{aB} = (1-p_A) p_B$\n$P_{ab} = (1-p_A) (1-p_B)$\n\nThe sample allele frequencies are $\\hat{p}_A = (n_{AB} + n_{Ab})/N$ and $\\hat{p}_B = (n_{AB} + n_{aB})/N$. The sample haplotype frequency is $\\hat{P}_{AB} = n_{AB}/N$.\nThe sample LD coefficient is $\\hat{D} = \\hat{P}_{AB} - \\hat{p}_A \\hat{p}_B$.\nThe squared sample correlation is defined as $\\hat{r}^2 = \\frac{\\hat{D}^2}{\\hat{p}_A(1-\\hat{p}_A)\\hat{p}_B(1-\\hat{p}_B)}$.\n\nA direct and principled way to derive the expectation of $\\hat{r}^2$ is to relate it to the Pearson's chi-squared statistic for independence in a $2 \\times 2$ contingency table. The contingency table for our two loci would have the following counts:\n\n|          | Allele B | Allele b | Total    |\n|----------|----------|----------|----------|\n| Allele A | $n_{AB}$ | $n_{Ab}$ | $N\\hat{p}_A$ |\n| Allele a | $n_{aB}$ | $n_{ab}$ | $N(1-\\hat{p}_A)$ |\n| Total    | $N\\hat{p}_B$ | $N(1-\\hat{p}_B)$ | $N$        |\n\nThe Pearson's chi-squared statistic, $X^2$, for this table can be expressed in terms of the haplotype counts as:\n$$X^2 = \\frac{N (n_{AB}n_{ab} - n_{Ab}n_{aB})^2}{(n_{AB}+n_{Ab})(n_{aB}+n_{ab})(n_{AB}+n_{aB})(n_{Ab}+n_{ab})}$$\nWe can rewrite the numerator and denominator using the sample frequencies. The term $n_{AB}n_{ab} - n_{Ab}n_{aB}$ can be shown to be equal to $N^2 \\hat{D}$. The denominator is $N^4 \\hat{p}_A(1-\\hat{p}_A)\\hat{p}_B(1-\\hat{p}_B)$.\nSubstituting these into the expression for $X^2$:\n$$X^2 = \\frac{N (N^2 \\hat{D})^2}{N^4 \\hat{p}_A(1-\\hat{p}_A)\\hat{p}_B(1-\\hat{p}_B)} = \\frac{N^5 \\hat{D}^2}{N^4 \\hat{p}_A(1-\\hat{p}_A)\\hat{p}_B(1-\\hat{p}_B)} = N \\frac{\\hat{D}^2}{\\hat{p}_A(1-\\hat{p}_A)\\hat{p}_B(1-\\hat{p}_B)}$$\nThis reveals a direct relationship between the chi-squared statistic and the sample LD measure $\\hat{r}^2$:\n$$X^2 = N \\hat{r}^2$$\nUnder the null hypothesis that the two loci are independent (i.e., $D=0$), the statistic $X^2$ is known to be asymptotically distributed as a chi-squared distribution with one degree of freedom, denoted $\\chi^2_1$.\n\nThe expectation of a random variable with a $\\chi^2_k$ distribution is $k$. Therefore, for the asymptotic distribution of $X^2$, we have:\n$$\\mathbb{E}[X^2] = 1$$\nUsing the relationship $X^2 = N \\hat{r}^2$, we can find the approximate expectation of $\\hat{r}^2$:\n$$\\mathbb{E}[N \\hat{r}^2] \\approx 1$$\n$$\\implies N \\mathbb{E}[\\hat{r}^2] \\approx 1$$\n$$\\implies \\mathbb{E}[\\hat{r}^2] \\approx \\frac{1}{N}$$\nThis is the leading-order behavior of $\\mathbb{E}[\\hat{r}^2]$. More precise analyses show that a better approximation for finite samples is $\\frac{1}{N-1}$, but $\\frac{1}{N}$ is the correct leading-order term for large $N$.\n\nNow, we address the specific questions posed:\n\n1.  **Dependence on Sample Size ($N$) and Minor Allele Frequency (MAF)**:\n    The derived expression $\\mathbb{E}[\\hat{r}^2] \\approx 1/N$ shows a clear inverse dependence on the sample size $N$. This is a fundamental consequence of sampling variance. Even when the true LD is zero, any finite sample of haplotypes will exhibit random fluctuations in haplotype counts, leading to a non-zero sample LD, $\\hat{D}$. The variance of these sample estimates is inversely proportional to the sample size. Since $\\mathbb{E}[\\hat{r}^2]$ is a measure of this sampling noise (when $D=0$), it decreases as $N$ increases.\n    The leading-order expression for $\\mathbb{E}[\\hat{r}^2]$ is conspicuously independent of the allele frequencies $p_A$ and $p_B$. While the definition of $\\hat{r}^2$ itself has the allele frequency-dependent term $\\hat{p}_A(1-\\hat{p}_A)\\hat{p}_B(1-\\hat{p}_B)$ in its denominator, the numerator, $\\mathbb{E}[\\hat{D}^2]$, is also proportional to $p_A(1-p_A)p_B(1-p_B)$ to leading order. Specifically, under $D=0$, the variance of $\\hat{D}$ is $\\text{Var}(\\hat{D}) \\approx \\frac{p_A(1-p_A)p_B(1-p_B)}{N}$. When taking the expectation of the ratio, these frequency-dependent terms cancel out, leaving only the $1/N$ dependence.\n\n2.  **Effect of the MAF Threshold $t$**:\n    The problem specifies that variants are filtered based on a MAF threshold $t$, such that the allele frequencies satisfy $p_A, p_B \\in [t, 1-t]$. As established above, the leading-order expectation $\\mathbb{E}[\\hat{r}^2] \\approx 1/N$ is independent of the specific allele frequencies $p_A$ and $p_B$. Therefore, conditioning the analysis on loci whose allele frequencies fall within the range $[t, 1-t]$ does not change the leading-order value of the expectation. The result remains $1/N$. The primary role of such a filter in practice is to improve the stability of the $\\hat{r}^2$ statistic by avoiding cases where the denominator is extremely small (i.e., when an allele is very rare), which in turn makes the large-sample approximations used in this derivation more accurate. The MAF threshold $t$ itself does not appear in the final expression for the expectation.\n\n3.  **Numerical Calculation**:\n    Given a cohort of $2400$ individuals, the number of haplotypes is $N = 2 \\times 2400 = 4800$. The MAF threshold is $t = 0.01$. As explained, this threshold does not influence the calculation of the expected value.\n    Using the derived leading-order expression:\n    $$\\mathbb{E}[\\hat{r}^2] \\approx \\frac{1}{N} = \\frac{1}{4800}$$\n    To express this in scientific notation and round to four significant figures:\n    $$\\frac{1}{4800} = \\frac{1}{4.8 \\times 10^3} \\approx 0.208333... \\times 10^{-3} = 2.08333... \\times 10^{-4}$$\n    Rounding to four significant figures gives $2.083 \\times 10^{-4}$.",
            "answer": "$$ \\boxed{2.083 \\times 10^{-4}} $$"
        },
        {
            "introduction": "Haplotype-based analyses are powerful tools in precision medicine, but their accuracy hinges on correctly phased genomic data. In this practice, you will develop a rigorous framework for evaluating the performance of a statistical phasing algorithm by calculating the switch error rate, a key industry-standard metric . You will derive its estimator and confidence interval from fundamental statistical theory, connecting a practical quality control measure to its theoretical underpinnings.",
            "id": "4355742",
            "problem": "A common performance metric for statistical phasing in regions of non-random association among alleles, known as linkage disequilibrium, is the switch error rate for phased haplotypes. Consider a study in precision medicine and genomic diagnostics that uses parent-offspring trios to establish ground-truth phasing for a subset of consecutive heterozygous single-nucleotide polymorphism loci in offspring across linkage disequilibrium blocks. Assume Mendelian transmission provides the correct haplotype orientation in the offspring at a locus whenever parental genotypes make the transmission unambiguous, and that uninformative intervals are excluded.\n\nYou observe across all offspring that there are $13,640$ consecutive heterozygous site pairs within the analyzed linkage disequilibrium blocks. Of these, $1,100$ consecutive pairs are uninformative for transmission-based ground truth and are excluded, leaving $12,540$ informative switch opportunities. Comparing the algorithm’s phased haplotypes to the trio-derived ground truth, you count $173$ instances where the phase orientation between two consecutive heterozygous loci is inconsistent with the ground truth.\n\nUsing only fundamental definitions and well-tested statistical principles:\n- Define precisely the switch error rate for phased haplotypes in this setting.\n- Starting from a Bernoulli model for independent switch outcomes with unknown error probability $p$, derive the maximum likelihood estimator $\\hat{p}$ for the switch error rate using the $12,540$ opportunities and $173$ observed errors.\n- Derive a two-sided $(1-\\alpha)$ confidence interval for $p$ by inverting the large-sample score test for a binomial proportion (the Wilson score interval), in symbolic form, using the standard normal quantile $z_{1-\\alpha/2}$. Do not assume any continuity correction.\n- Finally, evaluate your expressions numerically for $\\alpha = 0.05$ using $z_{0.975} = 1.96$, the given counts, and report the maximum likelihood estimate and the two endpoints of the two-sided $95\\%$ confidence interval as decimal fractions.\n\nRound each reported quantity to four significant figures. Provide your final numerical answer as a single row matrix of the form $\\left(\\hat{p}, L, U\\right)$, where $\\hat{p}$ is the point estimate, $L$ is the lower endpoint, and $U$ is the upper endpoint. Express all quantities as pure numbers without units or percentage signs.",
            "solution": "The problem is deemed valid as it is scientifically grounded in statistical genetics, well-posed with sufficient data and clear objectives, and free of contradictions or ambiguities.\n\n### 1. Definition of Switch Error Rate\n\nThe switch error rate is a performance metric for haplotype phasing algorithms. It is defined as the probability, $p$, that the inferred phase relationship (i.e., which allele is on which parental chromosome) is incorrect between two consecutive heterozygous loci. In the context of this problem, an \"opportunity\" for a switch error is any pair of consecutive heterozygous sites where the true phase can be determined from parent-offspring trio data. The switch error rate is the proportion of these opportunities at which an error is observed. It is estimated from the data as the number of observed switch errors, $k$, divided by the total number of informative opportunities, $n$.\n\n### 2. Maximum Likelihood Estimator (MLE) for the Switch Error Rate\n\nThe problem specifies a Bernoulli model for the outcome at each of the $n$ informative opportunities. Let $X_i$ be an indicator random variable for the $i$-th opportunity, for $i \\in \\{1, 2, \\dots, n\\}$. We define $X_i=1$ if a switch error occurs (with probability $p$) and $X_i=0$ if the phase is correct (with probability $1-p$). The total number of observed errors, $k = \\sum_{i=1}^{n} X_i$, follows a Binomial distribution, $K \\sim \\text{Binomial}(n, p)$, assuming the errors are independent events.\n\nThe probability mass function for observing $k$ errors in $n$ trials is:\n$$ P(K=k | n, p) = \\binom{n}{k} p^k (1-p)^{n-k} $$\nThis function, when viewed as a function of the parameter $p$ for the observed data $(n, k)$, is the likelihood function $L(p)$:\n$$ L(p; k, n) = \\binom{n}{k} p^k (1-p)^{n-k} $$\nTo find the maximum likelihood estimator (MLE) $\\hat{p}$, we maximize $L(p)$. It is computationally simpler to maximize the natural logarithm of the likelihood function, the log-likelihood $\\ln L(p)$, as the logarithm is a monotonic function.\n$$ \\ln L(p) = \\ln\\left(\\binom{n}{k}\\right) + k \\ln(p) + (n-k) \\ln(1-p) $$\nWe find the maximum by taking the derivative with respect to $p$ and setting it to zero:\n$$ \\frac{d}{dp} \\ln L(p) = \\frac{k}{p} - \\frac{n-k}{1-p} = 0 $$\nSolving for $p$:\n$$ \\frac{k}{p} = \\frac{n-k}{1-p} $$\n$$ k(1-p) = p(n-k) $$\n$$ k - kp = np - kp $$\n$$ k = np $$\nThus, the maximum likelihood estimator for $p$ is:\n$$ \\hat{p} = \\frac{k}{n} $$\nThe second derivative, $\\frac{d^2}{dp^2} \\ln L(p) = -\\frac{k}{p^2} - \\frac{n-k}{(1-p)^2}$, is negative for $p \\in (0,1)$, confirming that this value corresponds to a maximum.\n\n### 3. Derivation of the Wilson Score Confidence Interval\n\nA two-sided $(1-\\alpha)$ confidence interval for $p$ can be derived by inverting the large-sample score test. The test statistic for the null hypothesis $H_0: p=p_0$ is given by:\n$$ Z = \\frac{\\hat{p} - p_0}{\\sqrt{p_0(1-p_0)/n}} $$\nwhere $\\hat{p} = k/n$. The confidence interval is the set of all values $p_0$ for which we would not reject $H_0$ at significance level $\\alpha$. This corresponds to the inequality $|Z| \\le z_{1-\\alpha/2}$, where $z_{1-\\alpha/2}$ is the upper $(1-\\alpha/2)$ quantile of the standard normal distribution. Let's denote $p_0$ by $p$ and $z_{1-\\alpha/2}$ by $z$ for simplicity.\n$$ \\left| \\frac{\\hat{p} - p}{\\sqrt{p(1-p)/n}} \\right| \\le z $$\nSquaring both sides yields:\n$$ (\\hat{p} - p)^2 \\le z^2 \\frac{p(1-p)}{n} $$\nMultiplying by $n$ and expanding the terms gives a quadratic inequality in $p$:\n$$ n(\\hat{p}^2 - 2\\hat{p}p + p^2) \\le z^2(p - p^2) $$\n$$ n\\hat{p}^2 - 2n\\hat{p}p + np^2 \\le z^2p - z^2p^2 $$\nRearranging the terms to the standard form $Ap^2 + Bp + C \\le 0$:\n$$ (n+z^2)p^2 - (2n\\hat{p} + z^2)p + n\\hat{p}^2 \\le 0 $$\nThe roots of the corresponding quadratic equation, $(n+z^2)p^2 - (2n\\hat{p} + z^2)p + n\\hat{p}^2 = 0$, provide the endpoints of the confidence interval. Using the quadratic formula, $p = \\frac{-B \\pm \\sqrt{B^2-4AC}}{2A}$, with $A=n+z^2$, $B=-(2n\\hat{p}+z^2)$, and $C=n\\hat{p}^2$:\n$$ p = \\frac{(2n\\hat{p} + z^2) \\pm \\sqrt{(-(2n\\hat{p} + z^2))^2 - 4(n+z^2)(n\\hat{p}^2)}}{2(n+z^2)} $$\nSimplifying the discriminant (the term under the square root):\n$$ (2n\\hat{p} + z^2)^2 - 4n(n+z^2)\\hat{p}^2 = 4n^2\\hat{p}^2 + 4n\\hat{p}z^2 + z^4 - 4n^2\\hat{p}^2 - 4nz^2\\hat{p}^2 \\\\ = 4n\\hat{p}z^2 - 4nz^2\\hat{p}^2 + z^4 = z^2(4n\\hat{p}(1-\\hat{p}) + z^2) $$\nSubstituting this back into the formula for the roots, the endpoints of the Wilson score interval $(L, U)$ are:\n$$ (L, U) = \\frac{2n\\hat{p} + z^2 \\mp z\\sqrt{4n\\hat{p}(1-\\hat{p}) + z^2}}{2(n+z^2)} $$\nwhere $z=z_{1-\\alpha/2}$. This can be expressed in terms of $k$ and $n$ as:\n$$ (L, U) = \\frac{2k + z^2 \\mp z\\sqrt{4k(1-k/n) + z^2}}{2(n+z^2)} $$\n### 4. Numerical Evaluation\n\nThe given values are:\n- Total informative switch opportunities: $n = 12,540$.\n- Observed switch errors: $k = 173$.\n- Significance level: $\\alpha = 0.05$.\n- Standard normal quantile: $z = z_{1-0.05/2} = z_{0.975} = 1.96$.\n\nFirst, we calculate the MLE point estimate $\\hat{p}$:\n$$ \\hat{p} = \\frac{k}{n} = \\frac{173}{12,540} \\approx 0.01379585... $$\nRounded to four significant figures, $\\hat{p} = 0.01380$.\n\nNext, we evaluate the Wilson score interval. We use the derived formula. First, calculate the necessary components:\n- $z^2 = (1.96)^2 = 3.8416$.\n- $2k = 346$.\n- $n+z^2 = 12,540 + 3.8416 = 12,543.8416$.\n- $2(n+z^2) = 2 \\times 12,543.8416 = 25,087.6832$.\n\nNow, for the term under the square root:\n$$ 4k\\left(1 - \\frac{k}{n}\\right) + z^2 = 4(173)\\left(1 - \\frac{173}{12,540}\\right) + 3.8416 $$\n$$ = 692(0.9862041...) + 3.8416 \\approx 682.45327 + 3.8416 = 686.29487 $$\nThe square root is $\\sqrt{686.29487} \\approx 26.19723$.\nThe term to be added and subtracted is $z\\sqrt{...} \\approx 1.96 \\times 26.19723 \\approx 51.34657$.\n\nThe interval endpoints are:\n$$ (L, U) = \\frac{346 + 3.8416 \\mp 51.34657}{25,087.6832} = \\frac{349.8416 \\mp 51.34657}{25,087.6832} $$\nThe lower bound $L$ is:\n$$ L = \\frac{349.8416 - 51.34657}{25,087.6832} = \\frac{298.49503}{25,087.6832} \\approx 0.0118980... $$\nThe upper bound $U$ is:\n$$ U = \\frac{349.8416 + 51.34657}{25,087.6832} = \\frac{401.18817}{25,087.6832} \\approx 0.0159913... $$\nRounding the results to four significant figures:\n- $\\hat{p} = 0.01380$\n- $L = 0.01190$\n- $U = 0.01599$\n\nThe final numerical results are the point estimate $\\hat{p}$, the lower confidence limit $L$, and the upper confidence limit $U$.",
            "answer": "$$ \\boxed{ \\begin{pmatrix} 0.01380 & 0.01190 & 0.01599 \\end{pmatrix} } $$"
        },
        {
            "introduction": "The structure of linkage disequilibrium allows us to capture most of the genetic variation in a region without genotyping every single variant, a principle that underpins cost-effective genomic studies. This hands-on challenge puts you in the role of a study designer, tasking you with creating a greedy algorithm to select a minimal, non-redundant set of 'tag SNPs' that provide maximum information . This exercise bridges the gap between the concept of LD and the computational practice of designing efficient genotyping assays.",
            "id": "4355690",
            "problem": "Consider a genomic region with $n$ single-nucleotide polymorphisms (SNPs). You are given, as inputs, the pairwise linkage disequilibrium matrix whose $(i,j)$ entry is the squared correlation coefficient $r^2_{ij}$ between SNP $i$ and SNP $j$, and the vector of minor allele frequencies for each SNP. The aim is to select a set of tag SNPs that jointly provide coverage for the region under a specified coverage definition, while minimizing the number of tags. Your task is to derive, from first principles, a precise, deterministic algorithm grounded in core definitions to select a minimal tag SNP set that achieves a specified coverage target, while making explicit the trade-offs with allele frequency and redundancy among tags.\n\nStart from the following core definitions and facts as your fundamental base:\n- Linkage disequilibrium between biallelic markers $A$ and $B$ is characterized by covariance $D = p_{AB} - p_A p_B$ and the squared correlation coefficient $r^2 = \\frac{D^2}{p_A (1 - p_A) p_B (1 - p_B)}$, where $p_A$ and $p_B$ are the allele frequencies at the two loci, and $p_{AB}$ is the haplotype frequency of the reference alleles. For this problem, $r^2$ is provided as input and is not to be recomputed.\n- The detectability of an association at a SNP under an additive model is proportional to the genotype variance at that SNP, which is $2 p (1 - p)$ where $p$ is the minor allele frequency. Use this as the allele-frequency weight for each SNP in your coverage objective.\n- The set cover problem is a well-known combinatorial optimization problem. You may leverage the notion of set cover and deterministic greedy selection to construct an algorithmic approach, but you must formalize your choice of objective, stopping criterion, and tie-breaking to ensure a unique, testable output.\n\nDefine the following:\n- For a fixed coverage threshold $\\tau \\in [0,1]$, say that tag SNP $t$ covers SNP $j$ if $r^2_{t j} \\ge \\tau$.\n- Let $w_j = 2 p_j (1 - p_j)$ be the weight for SNP $j$, where $p_j$ is its minor allele frequency.\n- The weighted coverage fraction of a selected tag set $T \\subseteq \\{0,1,\\dots,n-1\\}$ is\n$$ C(T) = \\frac{\\sum_{j \\in \\mathcal{U}(T)} w_j}{\\sum_{j=0}^{n-1} w_j}, $$\nwhere $\\mathcal{U}(T)$ is the set of SNPs covered by at least one tag in $T$ under the rule $r^2_{t j} \\ge \\tau$.\n- Redundancy among tags is operationalized via a redundancy threshold $\\tau_{\\mathrm{red}} \\in [0,1]$ so that two tags $t$ and $t'$ are considered redundant if $r^2_{t t'} \\ge \\tau_{\\mathrm{red}}$.\n\nYour derived algorithm must:\n1. Minimize the number of tags subject to achieving a target coverage fraction $\\alpha \\in [0,1]$ under $C(T)$ as defined above.\n2. Be deterministic and polynomial-time. To ensure deterministic, testable behavior, adopt the following greedy policy you justify from first principles: at each iteration, among all candidate tags $t \\notin T$ that yield strictly positive marginal benefit in weighted coverage of currently uncovered SNPs, choose the $t$ that maximizes a redundancy-penalized score $S(t \\mid T)$ given by\n$$ S(t \\mid T) = \\sum_{\\substack{j \\notin \\mathcal{U}(T) \\\\ r^2_{t j} \\ge \\tau}} w_j - \\lambda \\cdot |\\{ t' \\in T \\,:\\, r^2_{t t'} \\ge \\tau_{\\mathrm{red}} \\}|, $$\nwhere $\\lambda \\ge 0$ is a penalty coefficient controlling the trade-off between acquiring new coverage and avoiding redundant tags. Break ties by choosing the smallest index $t$.\n3. Stop when $C(T) \\ge \\alpha$ or when no candidate provides strictly positive marginal benefit. Return the set $T$.\n\nFor each test case below, compute and return:\n- The sorted list of selected tag indices in ascending order.\n- The achieved weighted coverage fraction $C(T)$, expressed as a decimal rounded to six digits after the decimal point (no percentage sign).\n- The integer count of redundant tag pairs in $T$, defined as the number of unordered pairs $\\{t,t'\\} \\subseteq T$ with $r^2_{t t'} \\ge \\tau_{\\mathrm{red}}$.\n\nImplement your algorithm as a complete, runnable program that takes no input and uses the hard-coded test suite below. Your program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets. For each test case, the result must be a list of the form [selected_indices_list,coverage_decimal,redundant_pair_count]. The final output should thus be a single list of such lists, printed on one line, with no spaces.\n\nTest suite (all indices are zero-based, and all matrices are symmetric with ones on the diagonal):\n- Test case 1 (block structure, full coverage):\n  - $n = 5$\n  - Minor allele frequencies: $[0.2,\\,0.2,\\,0.35,\\,0.35,\\,0.1]$\n  - $r^2$ matrix:\n    $$\n    \\begin{bmatrix}\n    1.00 & 0.92 & 0.10 & 0.15 & 0.05 \\\\\n    0.92 & 1.00 & 0.12 & 0.08 & 0.04 \\\\\n    0.10 & 0.12 & 1.00 & 0.88 & 0.06 \\\\\n    0.15 & 0.08 & 0.88 & 1.00 & 0.05 \\\\\n    0.05 & 0.04 & 0.06 & 0.05 & 1.00\n    \\end{bmatrix}\n    $$\n  - Parameters: $\\tau = 0.8$, $\\tau_{\\mathrm{red}} = 0.9$, $\\alpha = 1.0$, $\\lambda = 0.5$.\n- Test case 2 (same region, partial coverage target, no redundancy penalty):\n  - $n = 5$\n  - Minor allele frequencies: $[0.2,\\,0.2,\\,0.35,\\,0.35,\\,0.1]$\n  - Same $r^2$ matrix as in test case 1.\n  - Parameters: $\\tau = 0.8$, $\\tau_{\\mathrm{red}} = 0.9$, $\\alpha = 0.6$, $\\lambda = 0.0$.\n- Test case 3 (all SNPs highly correlated):\n  - $n = 4$\n  - Minor allele frequencies: $[0.2,\\,0.3,\\,0.4,\\,0.1]$\n  - $r^2$ matrix has $0.95$ off-diagonal and $1.0$ on the diagonal:\n    $$\n    \\begin{bmatrix}\n    1.00 & 0.95 & 0.95 & 0.95 \\\\\n    0.95 & 1.00 & 0.95 & 0.95 \\\\\n    0.95 & 0.95 & 1.00 & 0.95 \\\\\n    0.95 & 0.95 & 0.95 & 1.00\n    \\end{bmatrix}\n    $$\n  - Parameters: $\\tau = 0.8$, $\\tau_{\\mathrm{red}} = 0.9$, $\\alpha = 1.0$, $\\lambda = 0.5$.\n- Test case 4 (no linkage disequilibrium beyond self):\n  - $n = 3$\n  - Minor allele frequencies: $[0.2,\\,0.35,\\,0.5]$\n  - $r^2$ matrix:\n    $$\n    \\begin{bmatrix}\n    1.00 & 0.00 & 0.00 \\\\\n    0.00 & 1.00 & 0.00 \\\\\n    0.00 & 0.00 & 1.00\n    \\end{bmatrix}\n    $$\n  - Parameters: $\\tau = 0.8$, $\\tau_{\\mathrm{red}} = 0.9$, $\\alpha = 1.0$, $\\lambda = 0.5$.\n- Test case 5 (redundant cluster plus singleton):\n  - $n = 4$\n  - Minor allele frequencies: $[0.2,\\,0.2,\\,0.2,\\,0.2]$\n  - $r^2$ matrix:\n    $$\n    \\begin{bmatrix}\n    1.00 & 0.95 & 0.85 & 0.10 \\\\\n    0.95 & 1.00 & 0.85 & 0.10 \\\\\n    0.85 & 0.85 & 1.00 & 0.10 \\\\\n    0.10 & 0.10 & 0.10 & 1.00\n    \\end{bmatrix}\n    $$\n  - Parameters: $\\tau = 0.8$, $\\tau_{\\mathrm{red}} = 0.9$, $\\alpha = 1.0$, $\\lambda = 2.0$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[[...],[...],...]\"), with no spaces anywhere in the line.",
            "solution": "The goal is to construct a principled, deterministic algorithm for selecting a minimal set of tag single-nucleotide polymorphisms (SNPs) that achieve a target coverage of a region under linkage disequilibrium. The derivation proceeds from core definitions pertaining to linkage disequilibrium, allele frequency, and set coverage.\n\nFirst, the squared correlation coefficient $r^2$ encodes how well one SNP predicts another. A natural coverage model is to say that tag SNP $t$ can serve as a proxy for SNP $j$ if $r^2_{tj} \\ge \\tau$ for a chosen threshold $\\tau \\in [0,1]$. To tie coverage to statistical power in association studies, we weight each SNP by its genotype variance under an additive model, $w_j = 2 p_j (1 - p_j)$, where $p_j$ is the minor allele frequency. This choice is motivated by the fact that, under additivity, the variance of the genotype dosage is proportional to $2p(1-p)$, which affects detectable signal strength given a fixed effect size.\n\nThe optimization target is to minimize the number of tags while ensuring that the weighted coverage fraction\n$$ C(T) = \\frac{\\sum_{j \\in \\mathcal{U}(T)} w_j}{\\sum_{j=0}^{n-1} w_j} $$\nachieves at least a target $\\alpha \\in [0,1]$, where $\\mathcal{U}(T)$ denotes SNPs covered by any tag in $T$ through the rule $r^2_{tj} \\ge \\tau$. This is a weighted set cover problem: each candidate tag $t$ defines a set $S_t = \\{ j : r^2_{t j} \\ge \\tau \\}$ of SNPs it covers; selecting tags is selecting sets to cover the universe of SNPs with total weight at least a proportion $\\alpha$ of the total. Weighted set cover is combinatorial and, in general, computationally intractable to solve optimally for large $n$ (it is $\\mathcal{NP}$-hard). A well-tested approach is a greedy approximation that iteratively selects the set with maximal marginal gain.\n\nTo address redundancy among tags—where two tags are themselves in high linkage disequilibrium—we incorporate a redundancy penalty. Introduce a redundancy threshold $\\tau_{\\mathrm{red}}$ and a penalty coefficient $\\lambda \\ge 0$. When evaluating a candidate tag $t$ given the current selection $T$, define the score\n$$ S(t \\mid T) = \\underbrace{\\sum_{\\substack{j \\notin \\mathcal{U}(T) \\\\ r^2_{t j} \\ge \\tau}} w_j}_{\\text{marginal coverage benefit}} - \\underbrace{\\lambda \\cdot |\\{ t' \\in T : r^2_{t t'} \\ge \\tau_{\\mathrm{red}} \\}|}_{\\text{redundancy penalty}}. $$\nThe marginal coverage benefit is the incremental weighted coverage of uncovered SNPs if $t$ is added. The redundancy penalty decreases the score if $t$ is in high linkage with already selected tags, reflecting diminishing returns and genotyping inefficiency in precision medicine settings where cost and assay multiplexing matter. This trading between coverage and redundancy—tuned by $\\lambda$—mirrors real-world tag SNP design where one balances statistical power against assay redundancy.\n\nWe thus derive the following deterministic greedy algorithm:\n1. Initialize the selected tag set $T = \\varnothing$ and the covered set $\\mathcal{U}(T) = \\varnothing$.\n2. While $C(T) < \\alpha$:\n   - For each candidate tag $t \\notin T$, compute the marginal benefit\n     $$ B(t \\mid T) = \\sum_{\\substack{j \\notin \\mathcal{U}(T) \\\\ r^2_{t j} \\ge \\tau}} w_j. $$\n   - Restrict attention to candidates with $B(t \\mid T) > 0$; if no such candidates exist, break (coverage cannot be further improved).\n   - For each such $t$, compute the redundancy count\n     $$ R(t \\mid T) = |\\{ t' \\in T : r^2_{t t'} \\ge \\tau_{\\mathrm{red}} \\}|, $$\n     and the score $S(t \\mid T) = B(t \\mid T) - \\lambda R(t \\mid T)$.\n   - Choose the $t$ with maximal $S(t \\mid T)$, breaking ties by the smallest index $t$.\n   - Update $T \\leftarrow T \\cup \\{t\\}$ and $\\mathcal{U}(T) \\leftarrow \\mathcal{U}(T) \\cup \\{ j : r^2_{t j} \\ge \\tau \\}$.\n3. Return the sorted list of tag indices in $T$, the achieved $C(T)$ rounded to six decimals, and the redundancy pair count among selected tags, which is $|\\{ \\{t,t'\\} \\subseteq T : r^2_{t t'} \\ge \\tau_{\\mathrm{red}},\\, t < t' \\}|$.\n\nThis algorithm is polynomial-time and deterministic due to explicit tie-breaking. It is grounded in:\n- The definition of coverage via $r^2$,\n- The allele-frequency-based weighting $2p(1-p)$ reflecting genotype variance,\n- The set cover heuristic that maximizes marginal weighted coverage,\n- An explicit redundancy control that captures the trade-off between coverage and assay redundancy.\n\nWe now compute expected outputs for each test case by following the algorithm.\n\nFor test case 1:\n- Weights are $w = [0.32, 0.32, 0.455, 0.455, 0.18]$ with total $W = 1.73$.\n- Coverage threshold $\\tau = 0.8$ yields cover sets $S_0 = \\{0,1\\}$, $S_1 = \\{1,0\\}$, $S_2 = \\{2,3\\}$, $S_3 = \\{3,2\\}$, $S_4 = \\{4\\}$.\n- Iteration 1: Marginal benefits are $B(0)=0.64$, $B(1)=0.64$, $B(2)=0.91$, $B(3)=0.91$, $B(4)=0.18$. Scores with $\\lambda = 0.5$ and $T=\\varnothing$ equal benefits. Tie-breaking among the two best picks the smallest index $t=2$. Now covered weight is $0.91$.\n- Iteration 2: Uncovered are $\\{0,1,4\\}$. Marginal benefits are $B(0)=0.64$, $B(1)=0.64$, $B(4)=0.18$, and redundancy penalties are zero (no $r^2 \\ge 0.9$ with tag $2$). Select $t=0$. Covered weight becomes $1.55$.\n- Iteration 3: Uncovered $\\{4\\}$. Select $t=4$; covered weight becomes $1.73$.\n- Coverage fraction $C(T) = 1.73/1.73 = 1.0$; sorted tags $[0,2,4]$; redundancy pair count is $0$ because all inter-tag $r^2$ are below $\\tau_{\\mathrm{red}}$.\n\nFor test case 2:\n- Same $w$ and $S_t$ sets; $\\alpha = 0.6$, $\\lambda = 0.0$.\n- Iteration 1: Select $t=2$ (benefit $0.91$), coverage fraction $0.91/1.73 \\approx 0.525145$, not enough.\n- Iteration 2: Next best is $t=0$ (benefit $0.64$), reaching covered weight $1.55$.\n- Coverage fraction $C(T) = 1.55/1.73 = 155/173 \\approx 0.895954$; sorted tags $[0,2]$; redundancy count $0$.\n\nFor test case 3:\n- Weights $w = [0.32, 0.42, 0.48, 0.18]$ with total $W = 1.4$. Any single tag covers all SNPs since all off-diagonal $r^2 = 0.95 \\ge 0.8$.\n- Iteration 1: All candidates have equal benefit $1.4$; select the smallest index $t=0$.\n- Coverage fraction $C(T) = 1.0$; sorted tags $[0]$; redundancy count $0$.\n\nFor test case 4:\n- Weights $w = [0.32, 0.455, 0.5]$, $W = 1.275$; only self-coverage exists due to zero off-diagonal $r^2$.\n- Greedy picks $t=2$, then $t=1$, then $t=0$ to achieve full coverage.\n- Coverage fraction $C(T) = 1.0$; sorted tags $[0,1,2]$; redundancy count $0$.\n\nFor test case 5:\n- Weights all equal $w = [0.32, 0.32, 0.32, 0.32]$, $W = 1.28$. The first three SNPs form a redundant cluster ($r^2 \\ge 0.85$ among them), the fourth is nearly independent ($r^2 = 0.10$).\n- Iteration 1: The best benefit is $0.96$ for any of tags $0$, $1$, $2$; select $t=0$ (smallest index).\n- Iteration 2: Only SNP $3$ remains uncovered; select $t=3$.\n- Coverage fraction $C(T) = 1.0$; sorted tags $[0,3]$; redundancy count $0$ since $r^2_{0,3} = 0.10 < \\tau_{\\mathrm{red}}$.\n\nThe required program implements the above algorithm, runs the test suite, and prints the single-line result list with no spaces, with coverage fractions rounded to six decimals, and redundancy pair counts computed as the number of tag pairs with $r^2 \\ge \\tau_{\\mathrm{red}}$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef greedy_tag_selection(r2: np.ndarray,\n                         maf: np.ndarray,\n                         tau: float,\n                         tau_red: float,\n                         alpha: float,\n                         lam: float):\n    \"\"\"\n    Deterministic greedy tag SNP selection with redundancy penalty.\n\n    Parameters\n    ----------\n    r2 : np.ndarray\n        Pairwise r^2 matrix (n x n), symmetric, with ones on the diagonal.\n    maf : np.ndarray\n        Minor allele frequencies (length n).\n    tau : float\n        Coverage threshold; SNP j is covered by tag t if r2[t, j] >= tau.\n    tau_red : float\n        Redundancy threshold among tags; tags t and t' are redundant if r2[t, t'] >= tau_red.\n    alpha : float\n        Target weighted coverage fraction in [0, 1].\n    lam : float\n        Redundancy penalty coefficient (>= 0).\n\n    Returns\n    -------\n    selected_sorted : list of int\n        Sorted list of selected tag indices (ascending).\n    coverage_fraction : float\n        Achieved weighted coverage fraction.\n    redundancy_pair_count : int\n        Number of unordered selected tag pairs with r2 >= tau_red.\n    \"\"\"\n    n = r2.shape[0]\n    # Weights based on genotype variance under additive model\n    w = 2.0 * maf * (1.0 - maf)\n    total_weight = float(np.sum(w))\n    # Boolean coverage matrix\n    cover = (r2 >= tau)\n    # Initialize selection\n    selected = []\n    covered = np.zeros(n, dtype=bool)\n\n    # Helper to compute current coverage fraction\n    def current_coverage_fraction():\n        if total_weight == 0.0:\n            return 1.0\n        covered_weight = float(np.sum(w[covered]))\n        return covered_weight / total_weight\n\n    # Iterate until coverage target achieved or no positive benefit available\n    while current_coverage_fraction() < alpha:\n        best_score = None\n        best_idx = None\n        best_benefit = 0.0\n\n        for t in range(n):\n            if t in selected:\n                continue\n            # Compute marginal benefit: weights of uncovered SNPs covered by t\n            can_cover = cover[t, :] & (~covered)\n            benefit = float(np.sum(w[can_cover]))\n            if benefit <= 0.0:\n                continue  # skip candidates with no marginal gain\n            # Redundancy count with already selected tags\n            if len(selected) == 0:\n                redundancy_count = 0\n            else:\n                redundancy_count = int(np.sum(r2[t, selected] >= tau_red))\n            score = benefit - lam * redundancy_count\n            # Deterministic tie-breaking: choose smallest index when scores equal\n            if (best_score is None) or (score > best_score) or (np.isclose(score, best_score) and t < best_idx):\n                best_score = score\n                best_idx = t\n                best_benefit = benefit\n\n        # If no candidate improved coverage, break\n        if best_idx is None:\n            break\n\n        # Select the best index and update covered set\n        selected.append(best_idx)\n        covered = covered | cover[best_idx, :]\n\n    # Compute final coverage fraction\n    cov_frac = current_coverage_fraction()\n\n    # Redundancy pair count among selected tags\n    redundancy_pairs = 0\n    sel_sorted = sorted(selected)\n    for i in range(len(sel_sorted)):\n        for j in range(i + 1, len(sel_sorted)):\n            if r2[sel_sorted[i], sel_sorted[j]] >= tau_red:\n                redundancy_pairs += 1\n\n    return sel_sorted, cov_frac, redundancy_pairs\n\ndef serialize_no_spaces(obj):\n    \"\"\"\n    Serialize Python objects (ints, floats, lists) to a string with no spaces.\n    Floats are rounded to six decimals.\n    \"\"\"\n    if isinstance(obj, bool):\n        return \"True\" if obj else \"False\"\n    if isinstance(obj, int):\n        return str(obj)\n    if isinstance(obj, float):\n        # Round to six decimals and ensure a concise representation\n        x = round(obj, 6)\n        # Format to remove trailing zeros while keeping at least one decimal point for floats\n        s = f\"{x:.6f}\"\n        # Strip trailing zeros and possible trailing decimal point\n        s = s.rstrip('0').rstrip('.') if '.' in s else s\n        # Ensure at least one decimal place if it's an integer-like float\n        if '.' not in s:\n            s += \".0\"\n        return s\n    if isinstance(obj, list):\n        return \"[\" + \",\".join(serialize_no_spaces(x) for x in obj) + \"]\"\n    # Fallback for numpy scalars\n    if isinstance(obj, np.generic):\n        return serialize_no_spaces(obj.item())\n    raise TypeError(f\"Unsupported type for serialization: {type(obj)}\")\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = []\n\n    # Test case 1\n    r2_1 = np.array([\n        [1.00, 0.92, 0.10, 0.15, 0.05],\n        [0.92, 1.00, 0.12, 0.08, 0.04],\n        [0.10, 0.12, 1.00, 0.88, 0.06],\n        [0.15, 0.08, 0.88, 1.00, 0.05],\n        [0.05, 0.04, 0.06, 0.05, 1.00]\n    ], dtype=float)\n    maf_1 = np.array([0.2, 0.2, 0.35, 0.35, 0.1], dtype=float)\n    params_1 = (0.8, 0.9, 1.0, 0.5)  # tau, tau_red, alpha, lam\n    test_cases.append((r2_1, maf_1, *params_1))\n\n    # Test case 2\n    r2_2 = r2_1.copy()\n    maf_2 = maf_1.copy()\n    params_2 = (0.8, 0.9, 0.6, 0.0)\n    test_cases.append((r2_2, maf_2, *params_2))\n\n    # Test case 3\n    r2_3 = np.full((4, 4), 0.95, dtype=float)\n    np.fill_diagonal(r2_3, 1.0)\n    maf_3 = np.array([0.2, 0.3, 0.4, 0.1], dtype=float)\n    params_3 = (0.8, 0.9, 1.0, 0.5)\n    test_cases.append((r2_3, maf_3, *params_3))\n\n    # Test case 4\n    r2_4 = np.eye(3, dtype=float)\n    maf_4 = np.array([0.2, 0.35, 0.5], dtype=float)\n    params_4 = (0.8, 0.9, 1.0, 0.5)\n    test_cases.append((r2_4, maf_4, *params_4))\n\n    # Test case 5\n    r2_5 = np.array([\n        [1.00, 0.95, 0.85, 0.10],\n        [0.95, 1.00, 0.85, 0.10],\n        [0.85, 0.85, 1.00, 0.10],\n        [0.10, 0.10, 0.10, 1.00]\n    ], dtype=float)\n    maf_5 = np.array([0.2, 0.2, 0.2, 0.2], dtype=float)\n    params_5 = (0.8, 0.9, 1.0, 2.0)\n    test_cases.append((r2_5, maf_5, *params_5))\n\n    results = []\n    for r2, maf, tau, tau_red, alpha, lam in test_cases:\n        selected, cov_frac, red_pairs = greedy_tag_selection(r2, maf, tau, tau_red, alpha, lam)\n        # Build the result structure: [selected_indices_list, coverage_decimal, redundant_pair_count]\n        result = [selected, round(cov_frac, 6), int(red_pairs)]\n        results.append(result)\n\n    # Final print statement in the exact required format: no spaces.\n    print(serialize_no_spaces(results))\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}