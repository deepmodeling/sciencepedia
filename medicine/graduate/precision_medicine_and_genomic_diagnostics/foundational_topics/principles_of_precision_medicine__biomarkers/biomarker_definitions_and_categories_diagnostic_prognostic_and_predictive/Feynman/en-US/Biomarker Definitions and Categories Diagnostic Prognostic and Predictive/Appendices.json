{
    "hands_on_practices": [
        {
            "introduction": "Understanding a diagnostic biomarker's performance begins with its sensitivity and specificity, but its clinical utility lies in how it changes our assessment of a patient's disease probability. This practice challenges you to derive the fundamental tools for this update—likelihood ratios—from first principles. By applying these to a scenario with a cell-free DNA (cfDNA) biomarker, you will master the essential skill of calculating post-test odds, directly connecting a test's intrinsic characteristics to individual patient-level risk assessment .",
            "id": "4319496",
            "problem": "A continuous cell-free deoxyribonucleic acid (cfDNA) methylation signature, denoted by the scalar biomarker $B$, is being evaluated as a diagnostic biomarker for colorectal cancer in a symptomatic cohort. In the taxonomy of biomarkers used in precision medicine and genomic diagnostics, a diagnostic biomarker indicates the presence or absence of a current disease state, a prognostic biomarker stratifies the risk of future outcomes independent of therapy, and a predictive biomarker estimates the differential treatment response. Here, $B$ is used to classify disease status rather than future risk or treatment response, and a threshold $t^{\\ast}$ is chosen based on Receiver Operating Characteristic (ROC) analysis to produce a binary test result $T^{+}$ if $B \\ge t^{\\ast}$ and $T^{-}$ otherwise.\n\nAt the threshold $t^{\\ast}$ validated on an external cohort, the sensitivity and specificity are observed to be $sens = 0.83$ and $spec = 0.91$, respectively, where sensitivity is defined as $P(T^{+}\\mid D)$ and specificity is defined as $P(T^{-}\\mid \\neg D)$, with $D$ denoting the event \"disease present\" and $\\neg D$ denoting \"disease absent.\" For a particular patient, a well-calibrated clinical risk model provides a prior probability of disease $P(D) = 0.18$ before the biomarker measurement.\n\nStarting only from the fundamental definitions of sensitivity and specificity and Bayes' theorem, (i) derive the expressions for the positive and negative likelihood ratios, $LR^{+}$ and $LR^{-}$, in terms of $sens$ and $spec$, and (ii) derive the odds form of Bayes' theorem that relates post-test odds to pre-test odds and the appropriate likelihood ratio. Then, using these derived results, compute the post-test odds $odds(D\\mid T^{+})$ after a positive test result at threshold $t^{\\ast}$ for this patient.\n\nExpress the final numerical answer for $odds(D\\mid T^{+})$ as a unitless ratio and round your answer to four significant figures.",
            "solution": "The problem statement is evaluated to be valid. It is scientifically grounded in established principles of biostatistics and medical diagnostics, well-posed with all necessary information provided, and objective in its language and definitions. The data provided ($sens=0.83$, $spec=0.91$, $P(D)=0.18$) are internally consistent and plausible for a real-world clinical scenario. The problem is a standard, non-trivial application of Bayesian inference that requires rigorous derivation from first principles, as requested.\n\nThe solution proceeds by first deriving the required expressions for the likelihood ratios and the odds form of Bayes' theorem, and then applying these results to compute the final numerical answer.\n\n(i) Derivation of Likelihood Ratios\n\nThe positive likelihood ratio, $LR^{+}$, is defined as the ratio of the probability of a positive test result in the diseased population to the probability of a positive test result in the non-diseased population.\n$$LR^{+} \\equiv \\frac{P(T^{+}\\mid D)}{P(T^{+}\\mid \\neg D)}$$\nThe numerator is given by the definition of sensitivity, $sens$:\n$$P(T^{+}\\mid D) = sens$$\nThe denominator, $P(T^{+}\\mid \\neg D)$, is the false positive rate. It can be expressed in terms of specificity, $spec$. By definition, specificity is the probability of a negative test result in the non-diseased population:\n$$spec = P(T^{-}\\mid \\neg D)$$\nSince for any given condition (here, $\\neg D$), the test result must be either positive ($T^{+}$) or negative ($T^{-}$), the sum of their probabilities must be $1$:\n$$P(T^{+}\\mid \\neg D) + P(T^{-}\\mid \\neg D) = 1$$\nSolving for $P(T^{+}\\mid \\neg D)$ yields:\n$$P(T^{+}\\mid \\neg D) = 1 - P(T^{-}\\mid \\neg D) = 1 - spec$$\nSubstituting the expressions for the numerator and denominator back into the definition of $LR^{+}$ gives the desired expression:\n$$LR^{+} = \\frac{sens}{1 - spec}$$\n\nSimilarly, the negative likelihood ratio, $LR^{-}$, is defined as the ratio of the probability of a negative test result in the diseased population to the probability of a negative test result in the non-diseased population.\n$$LR^{-} \\equiv \\frac{P(T^{-}\\mid D)}{P(T^{-}\\mid \\neg D)}$$\nThe denominator is given directly by the definition of specificity:\n$$P(T^{-}\\mid \\neg D) = spec$$\nThe numerator, $P(T^{-}\\mid D)$, is the false negative rate. It can be expressed in terms of sensitivity. For the diseased population ($D$), the sum of probabilities of a positive and negative test result must be $1$:\n$$P(T^{+}\\mid D) + P(T^{-}\\mid D) = 1$$\nSolving for $P(T^{-}\\mid D)$ yields:\n$$P(T^{-}\\mid D) = 1 - P(T^{+}\\mid D) = 1 - sens$$\nSubstituting these expressions for the numerator and denominator into the definition of $LR^{-}$ gives the final expression:\n$$LR^{-} = \\frac{1 - sens}{spec}$$\n\n(ii) Derivation of the Odds Form of Bayes' Theorem\n\nThe odds of an event $A$ are defined as the ratio of the probability of the event occurring to the probability of it not occurring, $odds(A) = \\frac{P(A)}{P(\\neg A)}$. The post-test odds of disease given a positive test result $T^{+}$ are therefore:\n$$odds(D\\mid T^{+}) = \\frac{P(D\\mid T^{+})}{P(\\neg D\\mid T^{+})}$$\nWe apply Bayes' theorem to find expressions for the numerator and the denominator. For the numerator:\n$$P(D\\mid T^{+}) = \\frac{P(T^{+}\\mid D)P(D)}{P(T^{+})}$$\nAnd for the denominator:\n$$P(\\neg D\\mid T^{+}) = \\frac{P(T^{+}\\mid \\neg D)P(\\neg D)}{P(T^{+})}$$\nSubstituting these into the odds ratio, the common term $P(T^{+})$ in the denominator of both expressions cancels out:\n$$odds(D\\mid T^{+}) = \\frac{\\frac{P(T^{+}\\mid D)P(D)}{P(T^{+})}}{\\frac{P(T^{+}\\mid \\neg D)P(\\neg D)}{P(T^{+})}} = \\frac{P(T^{+}\\mid D)P(D)}{P(T^{+}\\mid \\neg D)P(\\neg D)}$$\nThis expression can be regrouped to separate the likelihoods from the prior probabilities:\n$$odds(D\\mid T^{+}) = \\left(\\frac{P(T^{+}\\mid D)}{P(T^{+}\\mid \\neg D)}\\right) \\left(\\frac{P(D)}{P(\\neg D)}\\right)$$\nThe first term is the definition of the positive likelihood ratio, $LR^{+}$. The second term is the definition of the pre-test (or prior) odds of disease, $odds(D)$. Therefore, we arrive at the odds form of Bayes' theorem:\n$$odds(D\\mid T^{+}) = LR^{+} \\times odds(D)$$\nThis fundamental relationship states that the post-test odds are equal to the pre-test odds multiplied by the likelihood ratio corresponding to the test result.\n\n(iii) Calculation of Post-Test Odds $odds(D\\mid T^{+})$\n\nUsing the derived results, we can now compute the post-test odds for the patient.\nFirst, we calculate the pre-test odds, $odds(D)$, from the given prior probability $P(D) = 0.18$.\nThe probability of no disease is $P(\\neg D) = 1 - P(D) = 1 - 0.18 = 0.82$.\nThe pre-test odds are:\n$$odds(D) = \\frac{P(D)}{P(\\neg D)} = \\frac{0.18}{0.82}$$\nNext, we calculate the positive likelihood ratio, $LR^{+}$, using the provided sensitivity $sens = 0.83$ and specificity $spec = 0.91$.\n$$LR^{+} = \\frac{sens}{1 - spec} = \\frac{0.83}{1 - 0.91} = \\frac{0.83}{0.09}$$\nFinally, we compute the post-test odds using the odds form of Bayes' theorem:\n$$odds(D\\mid T^{+}) = LR^{+} \\times odds(D) = \\left(\\frac{0.83}{0.09}\\right) \\times \\left(\\frac{0.18}{0.82}\\right)$$\nThe calculation is performed as follows:\n$$odds(D\\mid T^{+}) = \\frac{0.83 \\times 0.18}{0.09 \\times 0.82} = \\frac{0.1494}{0.0738}$$\n$$odds(D\\mid T^{+}) \\approx 2.024389...$$\nRounding the result to four significant figures gives $2.024$.",
            "answer": "$$\\boxed{2.024}$$"
        },
        {
            "introduction": "Ideal textbook scenarios for biomarker evaluation are rare; real-world studies are often affected by biases that can distort conclusions. This exercise confronts one of the most common challenges, verification bias, which occurs when the definitive \"gold standard\" test is not applied to all participants. You will implement the inverse-probability-weighting (IPW) method to generate unbiased estimates of sensitivity and specificity, a crucial skill for any researcher designing or critically appraising diagnostic accuracy studies .",
            "id": "4319561",
            "problem": "A cancer genomic diagnostic biomarker is being evaluated for its utility as a diagnostic biomarker, distinguishing individuals with disease from those without disease. The binary biomarker test result is denoted by $T \\in \\{+,-\\}$ and the disease status determined by a definitive reference standard (e.g., whole-exome sequencing) is denoted by $D \\in \\{1,0\\}$, where $D=1$ indicates disease present. Subjects are stratified by a pretest clinical risk stratum $S \\in \\{\\mathrm{High}, \\mathrm{Low}\\}$ based on standard clinical features. Due to ethical and logistical constraints, the reference standard is not applied to all screened subjects, leading to partial verification. The decision to verify is based on $S$ and $T$, and the probability of receiving the reference standard, $\\pi_{s,t} = \\Pr(\\text{verified} \\mid S=s, T=t)$, is known from the study design. Assume the reference standard is perfect and that missingness of $D$ is missing at random given $(S,T)$, that is, $D \\perp \\!\\!\\! \\perp \\text{verified} \\mid (S,T)$.\n\nAmong those who received the reference standard, the following counts are observed:\n- High-risk, $T=+$: $n_{\\mathrm{H},+}^{V} = 360$, with $d_{\\mathrm{H},+} = 300$ having $D=1$, and $\\pi_{\\mathrm{H},+} = 0.9$.\n- High-risk, $T=-$: $n_{\\mathrm{H},-}^{V} = 240$, with $d_{\\mathrm{H},-} = 48$ having $D=1$, and $\\pi_{\\mathrm{H},-} = 0.6$.\n- Low-risk, $T=+$: $n_{\\mathrm{L},+}^{V} = 250$, with $d_{\\mathrm{L},+} = 125$ having $D=1$, and $\\pi_{\\mathrm{L},+} = 0.5$.\n- Low-risk, $T=-$: $n_{\\mathrm{L},-}^{V} = 200$, with $d_{\\mathrm{L},-} = 10$ having $D=1$, and $\\pi_{\\mathrm{L},-} = 0.2$.\n\nStarting from the fundamental definitions $\\text{sensitivity} = \\Pr(T=+ \\mid D=1)$ and $\\text{specificity} = \\Pr(T=- \\mid D=0)$, and using probability laws under the given missingness assumption, derive an inverse-probability-weighted estimator that corrects for verification bias and apply it to compute unbiased estimates of sensitivity and specificity. Then compute the corrected Youden’s $J$ index, defined as $J = \\text{sensitivity} + \\text{specificity} - 1$. Report the final value of $J$ as a decimal and round your answer to four significant figures.",
            "solution": "The problem requires correcting for verification bias using inverse probability weighting (IPW) to obtain unbiased estimates of sensitivity and specificity. Verification bias occurs because the true disease status $D$ is only confirmed for a subset of the study population, and the decision to verify depends on the biomarker test result $T$ and clinical risk stratum $S$. The missing at random (MAR) assumption, $D \\perp \\!\\!\\! \\perp \\text{verified} \\mid (S,T)$, legitimizes the use of IPW.\n\n### Derivation of the IPW Estimator\nThe goal is to estimate sensitivity, $\\text{Se} = \\Pr(T=+ \\mid D=1)$, and specificity, $\\text{Sp} = \\Pr(T=- \\mid D=0)$. These can be expressed using the counts from the full (but partially unobserved) study population:\n$$\n\\text{Se} = \\frac{N_{T=+, D=1}}{N_{D=1}} \\quad \\text{and} \\quad \\text{Sp} = \\frac{N_{T=-, D=0}}{N_{D=0}}\n$$\nwhere $N_{t,d}$ is the number of subjects with test result $t$ and disease status $d$, and $N_d$ is the total number of subjects with disease status $d$.\n\nUnder the MAR assumption, we can estimate these unobserved population counts by weighting each verified subject by the inverse of their probability of being verified, $w_{s,t} = 1/\\pi_{s,t}$. Let $d_{s,t}$ be the number of observed diseased subjects and $h_{s,t} = n_{s,t}^V - d_{s,t}$ be the number of observed non-diseased subjects from the verified sample in stratum $(s,t)$.\n\nThe estimated total numbers for the four cells of the confusion matrix (True Positives, False Negatives, False Positives, True Negatives) are found by summing the weighted counts across the risk strata:\n- Estimated True Positives ($D=1, T=+$): $\\hat{N}_{TP} = \\sum_{s \\in \\{\\mathrm{H,L}\\}} \\frac{d_{s,+}}{\\pi_{s,+}}$\n- Estimated False Negatives ($D=1, T=-$): $\\hat{N}_{FN} = \\sum_{s \\in \\{\\mathrm{H,L}\\}} \\frac{d_{s,-}}{\\pi_{s,-}}$\n- Estimated False Positives ($D=0, T=+$): $\\hat{N}_{FP} = \\sum_{s \\in \\{\\mathrm{H,L}\\}} \\frac{n_{s,+}^V - d_{s,+}}{\\pi_{s,+}}$\n- Estimated True Negatives ($D=0, T=-$): $\\hat{N}_{TN} = \\sum_{s \\in \\{\\mathrm{H,L}\\}} \\frac{n_{s,-}^V - d_{s,-}}{\\pi_{s,-}}$\n\nFrom these, we estimate the total number of diseased and non-diseased individuals:\n$$\n\\hat{N}_{D=1} = \\hat{N}_{TP} + \\hat{N}_{FN}\n$$\n$$\n\\hat{N}_{D=0} = \\hat{N}_{FP} + \\hat{N}_{TN}\n$$\nThe IPW estimators for sensitivity and specificity are then:\n$$\n\\widehat{\\text{Se}} = \\frac{\\hat{N}_{TP}}{\\hat{N}_{D=1}} \\quad \\text{and} \\quad \\widehat{\\text{Sp}} = \\frac{\\hat{N}_{TN}}{\\hat{N}_{D=0}}\n$$\n\n### Calculation\nFirst, we substitute the given values into the formulas for the estimated counts.\n- Data for High Risk ($S=\\mathrm{H}$):\n    - $T=+$: $n_{\\mathrm{H},+}^V=360, d_{\\mathrm{H},+}=300, n_{\\mathrm{H},+}^V-d_{\\mathrm{H},+}=60, \\pi_{\\mathrm{H},+}=0.9$\n    - $T=-$: $n_{\\mathrm{H},-}^V=240, d_{\\mathrm{H},-}=48, n_{\\mathrm{H},-}^V-d_{\\mathrm{H},-}=192, \\pi_{\\mathrm{H},-}=0.6$\n- Data for Low Risk ($S=\\mathrm{L}$):\n    - $T=+$: $n_{\\mathrm{L},+}^V=250, d_{\\mathrm{L},+}=125, n_{\\mathrm{L},+}^V-d_{\\mathrm{L},+}=125, \\pi_{\\mathrm{L},+}=0.5$\n    - $T=-$: $n_{\\mathrm{L},-}^V=200, d_{\\mathrm{L},-}=10, n_{\\mathrm{L},-}^V-d_{\\mathrm{L},-}=190, \\pi_{\\mathrm{L},-}=0.2$\n\nThe estimated counts are:\n$$\n\\hat{N}_{TP} = \\frac{300}{0.9} + \\frac{125}{0.5} = \\frac{1000}{3} + 250 = \\frac{1000 + 750}{3} = \\frac{1750}{3}\n$$\n$$\n\\hat{N}_{FN} = \\frac{48}{0.6} + \\frac{10}{0.2} = 80 + 50 = 130\n$$\n$$\n\\hat{N}_{FP} = \\frac{60}{0.9} + \\frac{125}{0.5} = \\frac{200}{3} + 250 = \\frac{200 + 750}{3} = \\frac{950}{3}\n$$\n$$\n\\hat{N}_{TN} = \\frac{192}{0.6} + \\frac{190}{0.2} = 320 + 950 = 1270\n$$\n\nNext, we calculate the estimated total number of diseased and non-diseased individuals:\n$$\n\\hat{N}_{D=1} = \\hat{N}_{TP} + \\hat{N}_{FN} = \\frac{1750}{3} + 130 = \\frac{1750 + 390}{3} = \\frac{2140}{3}\n$$\n$$\n\\hat{N}_{D=0} = \\hat{N}_{FP} + \\hat{N}_{TN} = \\frac{950}{3} + 1270 = \\frac{950 + 3810}{3} = \\frac{4760}{3}\n$$\n\nNow, we compute the estimated sensitivity and specificity:\n$$\n\\widehat{\\text{Se}} = \\frac{\\hat{N}_{TP}}{\\hat{N}_{D=1}} = \\frac{1750/3}{2140/3} = \\frac{1750}{2140} = \\frac{175}{214}\n$$\n$$\n\\widehat{\\text{Sp}} = \\frac{\\hat{N}_{TN}}{\\hat{N}_{D=0}} = \\frac{1270}{4760/3} = \\frac{1270 \\times 3}{4760} = \\frac{127 \\times 3}{476} = \\frac{381}{476}\n$$\n\nFinally, we compute the corrected Youden's $J$ index:\n$$\n\\hat{J} = \\widehat{\\text{Se}} + \\widehat{\\text{Sp}} - 1 = \\frac{175}{214} + \\frac{381}{476} - 1\n$$\nTo compute the value, we convert the fractions to decimals:\n$$\n\\widehat{\\text{Se}} \\approx 0.817757009...\n$$\n$$\n\\widehat{\\text{Sp}} \\approx 0.800420168...\n$$\n$$\n\\hat{J} \\approx 0.817757009 + 0.800420168 - 1 = 1.618177177 - 1 = 0.618177177...\n$$\nRounding the result to four significant figures gives $0.6182$.",
            "answer": "$$\n\\boxed{0.6182}\n$$"
        },
        {
            "introduction": "When a biomarker's utility is based on a statistical model, its performance on the training data is often overly optimistic and may not reflect its true performance on new patients. This practice guides you through a powerful computational method, bootstrap resampling, to estimate and correct for this \"optimism\" in a key performance metric, the Area Under the Curve ($AUC$). By building a program to perform this correction, you will gain hands-on experience with a cornerstone of modern statistical learning for developing robust and generalizable biomarker models .",
            "id": "4319570",
            "problem": "You are given a formal task grounded in biomarker model evaluation within precision medicine and genomic diagnostics. In this setting, a biomarker can serve different roles: a diagnostic biomarker distinguishes disease from non-disease at a point in time, a prognostic biomarker stratifies baseline risk independent of treatment, and a predictive biomarker indicates differential benefit from a specific treatment. Classification models for diagnostic or predictive biomarkers often rely on probabilistic predictions of a binary clinical state. A rigorous measure of ranking performance for such models is the area under the receiver operating characteristic curve (AUC), which can be expressed as the probability that a randomly chosen positive case obtains a higher model score than a randomly chosen negative case. Apparent (in-sample) performance measured on the training data is susceptible to overfitting. Resampling-based methods, such as bootstrap resampling, can be used to estimate and correct the optimism (the expected inflation in apparent performance relative to performance on new data).\n\nFundamental base:\n- Let $X \\in \\mathbb{R}^{n \\times p}$ denote a design matrix of $n$ samples and $p$ features. Let $y \\in \\{0,1\\}^n$ denote binary outcomes representing the presence ($1$) or absence ($0$) of the clinical state associated with the biomarker.\n- Consider a logistic regression model with parameters $\\beta \\in \\mathbb{R}^{p+1}$ (including an intercept), producing predicted probabilities $\\hat{p}_i = s\\!\\left(\\beta_0 + \\sum_{j=1}^{p} x_{ij}\\beta_j \\right)$ for sample $i$, where $s(z) = 1/(1+e^{-z})$ is the logistic function. Parameters are estimated by maximizing the regularized likelihood or equivalently minimizing the regularized negative log-likelihood\n$$\n\\mathcal{L}(\\beta; X, y, \\lambda) = -\\sum_{i=1}^{n} \\left[ y_i \\log \\hat{p}_i + (1-y_i)\\log(1-\\hat{p}_i) \\right] + \\frac{\\lambda}{2}\\sum_{j=1}^{p} \\beta_j^2,\n$$\nwhere $\\lambda \\ge 0$ is an $L_2$ penalty weight applied to non-intercept coefficients to mitigate instability and reduce overfitting.\n- The area under the receiver operating characteristic curve (AUC) for a score vector $r \\in \\mathbb{R}^n$ is defined as the probability that a randomly chosen positive sample has a higher score than a randomly chosen negative sample, with ties contributing half weight. This can be computed via rank-based formulations derived from the Wilcoxon-Mann-Whitney statistic.\n- The bootstrap principle approximates expectations under the sampling distribution by resampling with replacement from the empirical distribution defined by the observed data. Bootstrap-based optimism estimation proceeds by training a model on each bootstrap sample and comparing its performance on the bootstrap sample versus the original sample.\n\nTask:\nImplement a program that, for each test case, performs the following steps:\n1. Generate synthetic data $(X,y)$ as follows. For given $n$, $p$, coefficient vector $\\beta^* \\in \\mathbb{R}^{p}$, and intercept $\\beta_0^* \\in \\mathbb{R}$, draw $X$ with independent standard normal entries. Compute linear predictors $z_i = \\beta_0^* + \\sum_{j=1}^{p} x_{ij}\\beta_j^*$ and sample outcomes $y_i \\sim \\text{Bernoulli}(s(z_i))$ independently for $i=1,\\dots,n$, where $s(\\cdot)$ is the logistic function. Use the specified random seed to ensure reproducibility.\n2. Fit a penalized logistic regression model by minimizing $\\mathcal{L}(\\beta; X, y, \\lambda)$ with respect to $\\beta$ using a numerical optimizer. The penalty term must exclude the intercept.\n3. Compute the apparent AUC on the original data by using the fitted model to produce predicted probabilities on $X$ and then computing the AUC against $y$ using a rank-based method that properly handles ties. If there are no positive or no negative samples, define the AUC as not-a-number.\n4. Perform bootstrap resampling $B$ times:\n   - For each bootstrap replicate $b \\in \\{1,\\dots,B\\}$, draw a bootstrap sample of indices of size $n$ with replacement from $\\{1,\\dots,n\\}$, yielding $(X^{(b)}, y^{(b)})$.\n   - Fit the penalized logistic regression on $(X^{(b)}, y^{(b)})$ to obtain parameters $\\hat{\\beta}^{(b)}$.\n   - Compute $AUC_{\\text{boot}}^{(b)}$ on $(X^{(b)}, y^{(b)})$ using predictions from $\\hat{\\beta}^{(b)}$.\n   - Compute $AUC_{\\text{orig}}^{(b)}$ on $(X, y)$ using predictions from $\\hat{\\beta}^{(b)}$.\n   - Define the optimism for replicate $b$ as $AUC_{\\text{boot}}^{(b)} - AUC_{\\text{orig}}^{(b)}$. If either AUC is not-a-number due to class degeneracy in the bootstrap sample, exclude that replicate from the optimism average.\n5. Compute the optimism-corrected AUC as the apparent AUC minus the average optimism computed over the valid bootstrap replicates.\n6. Return the optimism-corrected AUC for each test case as a decimal number rounded to $6$ decimal places.\n\nYour program must implement the above procedure exactly and handle tie-aware AUC computation based on ranks. It must use a penalized logistic regression with $L_2$ penalty weight $\\lambda$ applied only to non-intercept coefficients. If all bootstrap replicates are invalid due to class degeneracy, return the not-a-number indicator for that test case.\n\nTest suite:\nUse the following four test cases, each specified by a tuple $(\\text{seed}, n, p, \\beta^*, \\beta_0^*, \\lambda, B)$:\n- Case $1$: $(42, 120, 3, [0.6, -0.4, 0.8], -0.2, 1.0, 200)$.\n- Case $2$: $(314, 100, 2, [2.5, -2.0], 0.0, 2.0, 150)$.\n- Case $3$: $(7, 35, 4, [0.2, 0.1, -0.1, 0.0], -0.1, 0.5, 100)$.\n- Case $4$: $(2021, 200, 5, [0.3, 0.0, -0.2, 0.1, 0.05], -2.0, 1.0, 200)$.\n\nAnswer specification:\n- For each test case, compute the optimism-corrected AUC as described above and round to $6$ decimal places.\n- Express each final answer as a decimal (not as a percentage).\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[0.731245,0.892310,0.501234,0.763210]$). Do not print any other text.",
            "solution": "We begin by connecting the biomarker context to a formal statistical learning problem. A diagnostic biomarker distinguishes cases from controls, and a predictive biomarker indicates treatment benefit; both are commonly evaluated through binary classification performance. The area under the receiver operating characteristic curve (AUC) quantifies the model’s ranking ability: given scores $r \\in \\mathbb{R}^n$ and labels $y \\in \\{0,1\\}^n$, it equals the probability that a randomly chosen positive’s score exceeds a randomly chosen negative’s score, with ties contributing half weight. This can be computed via rank-based statistics derived from the Wilcoxon-Mann-Whitney formulation.\n\nOverfitting arises when a model captures idiosyncratic noise in the training data, inflating apparent performance (the performance measured on the same data used to train). The bootstrap principle provides a way to estimate and correct this inflation (optimism) by approximating expectations under the empirical distribution via resampling.\n\nPrinciple-based derivation and algorithmic design:\n1. Model and estimation. For a binary outcome, we model the conditional probability via the logistic function $s(z) = 1/(1+e^{-z})$. For sample $i$, with features $x_i \\in \\mathbb{R}^p$, the model is $\\hat{p}_i = s\\!\\left(\\beta_0 + \\sum_{j=1}^p x_{ij}\\beta_j\\right)$. Parameters $\\beta = (\\beta_0,\\ldots,\\beta_p)$ are estimated by minimizing the regularized negative log-likelihood\n   $$\n   \\mathcal{L}(\\beta; X, y, \\lambda) = -\\sum_{i=1}^n \\left[ y_i \\log \\hat{p}_i + (1-y_i)\\log(1-\\hat{p}_i) \\right] + \\frac{\\lambda}{2}\\sum_{j=1}^p \\beta_j^2,\n   $$\n   which includes an $L_2$ penalty on non-intercept coefficients $\\beta_j$ for $j \\ge 1$. This penalty reduces variance in parameter estimates, stabilizing learning especially when signal is strong or data are limited, thereby directly combating overfitting at the estimation stage.\n\n   The gradient of $\\mathcal{L}$ with respect to $\\beta$ is derived by differentiating the logistic loss. Writing $\\hat{p} = s(X\\tilde{\\beta})$, where $X$ is augmented with a column of ones to encode the intercept and $\\tilde{\\beta}$ is the full parameter vector, the gradient is\n   $$\n   \\nabla \\mathcal{L}(\\beta) = X^\\top(\\hat{p} - y) + \\lambda \\cdot (0, \\beta_1, \\ldots, \\beta_p)^\\top,\n   $$\n   where the intercept’s gradient has no penalty term. This enables efficient optimization via quasi-Newton methods such as Limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS).\n\n2. AUC computation by ranks. Let $r \\in \\mathbb{R}^n$ be predicted probabilities. Denote $n_+$ and $n_-$ as the counts of positives and negatives. Let $\\text{rank}(r)$ provide average ranks accounting for ties. Then\n   $$\n   \\text{AUC}(r, y) = \\frac{\\sum_{i: y_i = 1} \\text{rank}(r_i) - \\frac{n_+(n_+ + 1)}{2}}{n_+ n_-},\n   $$\n   provided $n_+ \\ge 1$ and $n_- \\ge 1$, otherwise it is undefined (not-a-number). This expression is algebraically equivalent to the empirical estimate of $\\mathbb{P}(r^+ > r^-)$ plus half the tie probability.\n\n3. Bootstrap optimism correction. The bootstrap approximates expectations under the sampling distribution by resampling with replacement from the observed data. For optimism estimation, we:\n   - Draw bootstrap samples $(X^{(b)}, y^{(b)})$ by sampling $n$ indices with replacement from $\\{1,\\ldots,n\\}$.\n   - Train the model on each bootstrap sample to obtain $\\hat{\\beta}^{(b)}$.\n   - Compute $AUC_{\\text{boot}}^{(b)}$ on the bootstrap sample and $AUC_{\\text{orig}}^{(b)}$ on the original sample using predictions from $\\hat{\\beta}^{(b)}$.\n   - Define optimism per replicate $b$ as $AUC_{\\text{boot}}^{(b)} - AUC_{\\text{orig}}^{(b)}$, excluding replicates where either AUC is undefined.\n   Averaging optimism across replicates yields an estimate of the expected inflation in apparent performance. The apparent AUC is computed by training on the original sample and evaluating on the same sample. Subtracting the average optimism from the apparent AUC yields the optimism-corrected AUC:\n   $$\n   \\text{AUC}_{\\text{corrected}} \\approx \\text{AUC}_{\\text{apparent}} - \\mathbb{E}[\\text{optimism}],\n   $$\n   where the expectation is approximated by the bootstrap average. This correction mitigates overfitting by quantitatively removing the portion of apparent performance that is likely due to fitting noise rather than signal, thereby better approximating out-of-sample performance expected for future data.\n\n4. Data generation for testability. For each test case, generate $X$ with independent standard normal entries using the provided seed, compute $z_i = \\beta_0^* + \\sum_{j} x_{ij} \\beta_j^*$, and sample $y_i \\sim \\text{Bernoulli}(s(z_i))$. This construction is scientifically realistic, consistent with generalized linear models, and enables controlled evaluation across different signal strengths and class imbalances.\n\nAlgorithm summary:\n- For each test case $(\\text{seed}, n, p, \\beta^*, \\beta_0^*, \\lambda, B)$:\n  1. Generate $(X, y)$; fit the penalized logistic regression to obtain $\\hat{\\beta}$; compute $\\text{AUC}_{\\text{apparent}}$.\n  2. For $b = 1,\\ldots,B$: bootstrap resample indices; fit on $(X^{(b)}, y^{(b)})$; compute $AUC_{\\text{boot}}^{(b)}$ and $AUC_{\\text{orig}}^{(b)}$; record optimism if both AUCs are defined.\n  3. Compute the mean optimism across valid replicates and subtract it from $\\text{AUC}_{\\text{apparent}}$ to yield $\\text{AUC}_{\\text{corrected}}$.\n  4. Round $\\text{AUC}_{\\text{corrected}}$ to $6$ decimal places.\n\nThe final program adheres to these principles, producing a single line containing the optimism-corrected AUC values for the specified test suite. The optimism correction directly addresses overfitting in biomarker model development by reducing bias in performance estimates, thus enhancing reliability of diagnostic or predictive biomarker claims for precision medicine.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats import rankdata\n\ndef sigmoid(z):\n    return 1.0 / (1.0 + np.exp(-z))\n\ndef fit_logistic_l2(X, y, lam):\n    \"\"\"\n    Fit penalized logistic regression with L2 penalty on non-intercept coefficients.\n    X: shape (n, p), features (no intercept column).\n    y: shape (n,), binary labels 0/1.\n    lam: float, L2 penalty weight.\n    Returns beta of shape (p+1,), including intercept as beta[0].\n    \"\"\"\n    n, p = X.shape\n    # Augment X with intercept column\n    X_aug = np.hstack([np.ones((n, 1)), X])\n\n    # Objective: regularized negative log-likelihood\n    def nll(beta):\n        z = X_aug @ beta\n        p_hat = sigmoid(z)\n        # Numerical stability: clip p_hat to avoid log(0)\n        eps = 1e-12\n        p_hat = np.clip(p_hat, eps, 1 - eps)\n        # Penalize non-intercept coefficients\n        penalty = 0.5 * lam * np.sum(beta[1:] ** 2)\n        return -np.sum(y * np.log(p_hat) + (1 - y) * np.log(1 - p_hat)) + penalty\n\n    # Gradient of the objective\n    def grad(beta):\n        z = X_aug @ beta\n        p_hat = sigmoid(z)\n        # Gradient from likelihood\n        g = X_aug.T @ (p_hat - y)\n        # Add penalty gradient (excluding intercept)\n        g[1:] += lam * beta[1:]\n        return g\n\n    # Initialize beta to zeros\n    beta0 = np.zeros(p + 1, dtype=float)\n    # Optimize using L-BFGS-B\n    res = minimize(nll, beta0, jac=grad, method=\"L-BFGS-B\")\n    return res.x\n\ndef auc_rank(scores, y):\n    \"\"\"\n    Compute AUC using rank-based method that handles ties.\n    Returns np.nan if there are no positives or no negatives.\n    \"\"\"\n    y = np.asarray(y)\n    scores = np.asarray(scores)\n    pos_mask = (y == 1)\n    neg_mask = (y == 0)\n    n_pos = np.sum(pos_mask)\n    n_neg = np.sum(neg_mask)\n    if n_pos == 0 or n_neg == 0:\n        return np.nan\n    ranks = rankdata(scores, method='average')\n    sum_ranks_pos = np.sum(ranks[pos_mask])\n    auc = (sum_ranks_pos - n_pos * (n_pos + 1) / 2.0) / (n_pos * n_neg)\n    return float(auc)\n\ndef bootstrap_optimism_corrected_auc(X, y, lam, B, rng):\n    \"\"\"\n    Compute optimism-corrected AUC via bootstrap resampling.\n    X: (n, p), y: (n,), lam: L2 penalty weight, B: number of bootstraps.\n    rng: numpy Generator for reproducibility.\n    \"\"\"\n    # Apparent fit\n    beta_app = fit_logistic_l2(X, y, lam)\n    p_app = sigmoid(np.hstack([np.ones((X.shape[0], 1)), X]) @ beta_app)\n    auc_app = auc_rank(p_app, y)\n\n    # Bootstrap optimism\n    n = X.shape[0]\n    optimisms = []\n    for b in range(B):\n        idx = rng.integers(0, n, size=n)\n        Xb = X[idx]\n        yb = y[idx]\n        # Fit on bootstrap sample\n        beta_b = fit_logistic_l2(Xb, yb, lam)\n        # AUC on bootstrap sample\n        pb_boot = sigmoid(np.hstack([np.ones((Xb.shape[0], 1)), Xb]) @ beta_b)\n        auc_boot = auc_rank(pb_boot, yb)\n        # AUC on original sample using bootstrap-fitted model\n        pb_orig = sigmoid(np.hstack([np.ones((X.shape[0], 1)), X]) @ beta_b)\n        auc_orig = auc_rank(pb_orig, y)\n        if not (np.isnan(auc_boot) or np.isnan(auc_orig)):\n            optimisms.append(auc_boot - auc_orig)\n    if len(optimisms) == 0 or np.isnan(auc_app):\n        return np.nan\n    mean_optimism = float(np.mean(optimisms))\n    corrected = auc_app - mean_optimism\n    # Bound within [0,1] for numerical sanity (AUC is in [0,1])\n    corrected = min(max(corrected, 0.0), 1.0)\n    return corrected\n\ndef generate_data(seed, n, p, beta_star, beta0_star):\n    \"\"\"\n    Generate synthetic data X, y with standard normal features and logistic Bernoulli responses.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    X = rng.normal(loc=0.0, scale=1.0, size=(n, p))\n    z = beta0_star + X @ np.asarray(beta_star)\n    prob = sigmoid(z)\n    y = rng.binomial(1, prob, size=n).astype(int)\n    return X, y, rng\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (seed, n, p, beta_star, beta0_star, lambda, B)\n    test_cases = [\n        (42, 120, 3, [0.6, -0.4, 0.8], -0.2, 1.0, 200),\n        (314, 100, 2, [2.5, -2.0], 0.0, 2.0, 150),\n        (7, 35, 4, [0.2, 0.1, -0.1, 0.0], -0.1, 0.5, 100),\n        (2021, 200, 5, [0.3, 0.0, -0.2, 0.1, 0.05], -2.0, 1.0, 200),\n    ]\n\n    results = []\n    for seed, n, p, beta_star, beta0_star, lam, B in test_cases:\n        X, y, rng = generate_data(seed, n, p, beta_star, beta0_star)\n        corrected_auc = bootstrap_optimism_corrected_auc(X, y, lam, B, rng)\n        # Round to 6 decimals as required\n        if np.isnan(corrected_auc):\n            results.append(\"nan\")\n        else:\n            results.append(f\"{np.round(corrected_auc, 6):.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}