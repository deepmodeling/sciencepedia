{
    "hands_on_practices": [
        {
            "introduction": "Understanding the relationship between sequencing effort and genome coverage is fundamental to designing any NGS experiment. Because reads are typically distributed randomly, average coverage alone is not a sufficient metric for success. This exercise  uses the Poisson distribution, a classic model in genomics, to explore the probabilistic nature of sequencing and to quantify the fraction of a genome that is expected to remain unsequenced, even when the total number of sequenced bases far exceeds the genome size.",
            "id": "2045416",
            "problem": "A synthetic biology research team is verifying the sequence of a custom-built Bacterial Artificial Chromosome (BAC). They perform a Next-Generation Sequencing (NGS) experiment and obtain a set of short DNA reads. The key parameters of their experiment are as follows:\n\n- The total length of the BAC genome, $L$, is 200 kilobase pairs (kbp).\n- The total number of reads generated, $N$, is 9,200.\n- The average length of each read, $R$, is 100 base pairs (bp).\n\nAssuming the reads are distributed randomly and uniformly across the BAC genome, the probability $P(k)$ that a given base is covered by exactly $k$ reads can be modeled by a Poisson distribution: $P(k) = \\frac{\\lambda^k \\exp(-\\lambda)}{k!}$, where $\\lambda$ is the average coverage depth across the genome.\n\nBased on this information, calculate the theoretical fraction of the BAC genome that is expected to have zero coverage (i.e., remain unsequenced). Express your answer as a decimal rounded to three significant figures.",
            "solution": "We model per-base coverage by a Poisson distribution with mean (average depth) $\\lambda$. Under random, uniform read placement, the average depth equals the total number of sequenced bases divided by the genome length:\n$$\n\\lambda = \\frac{N R}{L}.\n$$\nWith $N=9200$, $R=100$ bp, and $L=200\\,\\text{kbp}=200000$ bp,\n$$\n\\lambda = \\frac{9200 \\cdot 100}{200000} = \\frac{920000}{200000} = 4.6.\n$$\nFor a Poisson model, the probability that a base has zero coverage is\n$$\nP(0) = \\exp(-\\lambda) = \\exp(-4.6).\n$$\nNumerically,\n$$\n\\exp(-4.6) \\approx 0.0100518 \\approx 0.0101 \\text{ (three significant figures)}.\n$$\nThus, the theoretical fraction of the BAC genome expected to have zero coverage is $0.0101$.",
            "answer": "$$\\boxed{0.0101}$$"
        },
        {
            "introduction": "Raw sequencing data is rarely a perfect representation of the underlying biology, often containing systematic biases that can confound quantitative analysis. One of the most common artifacts is GC bias, where sequencing efficiency varies with the local Guanine-Cytosine content of the DNA template. This advanced computational exercise  guides you through implementing a robust normalization method using Locally Estimated Scatterplot Smoothing (LOESS), a critical data processing step for applications like copy number variation (CNV) analysis in precision oncology.",
            "id": "4353887",
            "problem": "You are given a fundamental bioinformatics task from Next-Generation Sequencing (NGS) in the domain of precision medicine and genomic diagnostics: correcting Guanine-Cytosine (GC) content bias in sequencing coverage. The phenomenon of GC bias is that observed coverage depends systematically on GC fraction, which can confound downstream inference. Build a program that, starting from first principles, estimates and removes this systematic bias using Locally Estimated Scatterplot Smoothing (LOESS), producing dimensionless normalized coverage suitable for comparison across bins.\n\nStart from the following foundational base: the Central Dogma of molecular biology (deoxyribonucleic acid to ribonucleic acid to protein) implies that sequencing reads reflect underlying deoxyribonucleic acid abundance, and in the absence of technical bias, the expected coverage of equally sized genomic bins would be constant up to stochastic variation; the definition of GC fraction as the proportion of guanine and cytosine bases in a bin; the interpretation of systematic bias as a smooth function of GC fraction; and the definition of LOESS as a local, weighted, least squares fit. Treat the GC bias as multiplicative on coverage and infer it by modeling the conditional expectation of the logarithm of coverage given GC fraction with a local linear approximation and a compactly supported kernel.\n\nImplement the following, entirely in mathematical and algorithmic terms:\n\n1. Let there be $n$ genomic bins indexed by $i \\in \\{1,\\dots,n\\}$. For each bin, you are given GC fraction $g_i \\in [0,1]$ and observed coverage $C_i \\ge 0$. Define a small pseudocount $\\epsilon$ to allow a logarithm when $C_i = 0$.\n\n2. Define the transformed response $y_i = \\log(C_i + \\epsilon)$. The systematic GC effect is the conditional expectation function $m(g) = \\mathbb{E}[y \\mid g]$, which you will estimate nonparametrically using LOESS with a local linear model and tricube weights. For a query $x_0$, use a neighborhood consisting of the $k$ nearest points by absolute GC distance, where $k = \\lceil s \\cdot n \\rceil$ for a given smoothing fraction $s \\in (0,1]$. Within that neighborhood, define distances $d_j = |x_j - x_0|$ and scale by $D = \\max_j d_j$; assign weights $w_j = \\left(1 - (d_j/D)^3\\right)^3$ for $d_j \\le D$, and $w_j = 0$ otherwise. Fit a weighted least squares local linear model $y \\approx \\beta_0 + \\beta_1 x$ to obtain $\\hat{m}(x_0) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_0$. If the weighted design is rank-deficient at $x_0$, fall back to the weighted average $\\hat{m}(x_0) = \\sum_j w_j y_j / \\sum_j w_j$.\n\n3. Define the normalized coverage for bin $i$ as $\\tilde{C}_i = \\exp\\left(y_i - \\hat{m}(g_i)\\right)$. This quantity is dimensionless. Round each $\\tilde{C}_i$ to $6$ decimal places.\n\n4. Apply the above to each provided test dataset. For each dataset, compute the vector $[\\tilde{C}_1, \\dots, \\tilde{C}_n]$.\n\nUse the following test suite of datasets. Each dataset is a tuple specifying the GC fractions, the coverages, the LOESS span $s$, and the pseudocount $\\epsilon$. All numerical values are given as decimals, and you must use them exactly as provided.\n\n- Test case A (typical monotone bias): \n  - GC fractions: $[0.20, 0.26, 0.32, 0.38, 0.44, 0.50, 0.56, 0.62, 0.68, 0.74, 0.80, 0.86]$\n  - Coverages: $[30, 31, 29, 28, 27, 30, 32, 34, 36, 37, 38, 39]$\n  - Span $s$: $0.60$\n  - Pseudocount $\\epsilon$: $0.50$\n\n- Test case B (U-shaped bias with extremes): \n  - GC fractions: $[0.00, 0.10, 0.30, 0.50, 0.70, 0.90, 1.00]$\n  - Coverages: $[15, 20, 28, 32, 28, 20, 16]$\n  - Span $s$: $0.60$\n  - Pseudocount $\\epsilon$: $0.50$\n\n- Test case C (presence of zeros): \n  - GC fractions: $[0.15, 0.35, 0.55, 0.75, 0.95]$\n  - Coverages: $[0, 10, 25, 0, 18]$\n  - Span $s$: $0.80$\n  - Pseudocount $\\epsilon$: $0.50$\n\n- Test case D (small sample, near-minimal neighborhood): \n  - GC fractions: $[0.40, 0.45, 0.90]$\n  - Coverages: $[22, 24, 30]$\n  - Span $s$: $0.60$\n  - Pseudocount $\\epsilon$: $0.50$\n\nYour program must implement the LOESS estimator and the normalization exactly as described, evaluate the fit at the observed $g_i$ values, and produce the normalized coverage vectors for each test case.\n\nFinal output format: Your program should produce a single line of output containing a list of four lists, one per test case, each inner list containing the normalized coverages in input order, rounded to $6$ decimal places, for example, $[[a_1,\\dots,a_n],[b_1,\\dots,b_m],[c_1,\\dots,c_p],[d_1,\\dots,d_q]]$. No additional text should be printed.",
            "solution": "The problem requires the implementation of a procedure to correct for Guanine-Cytosine (GC) content bias in sequencing coverage data, a common artifact in Next-Generation Sequencing (NGS). The provided methodology is based on Locally Estimated Scatterplot Smoothing (LOESS), a non-parametric regression technique. The solution will be developed from first principles as laid out in the problem description.\n\nThe underlying model assumes that the systematic bias is a multiplicative effect on coverage that can be described as a smooth function of the GC fraction. By transforming the data into a logarithmic scale, this multiplicative relationship becomes additive, making it suitable for regression modeling. The goal is to estimate this additive systematic effect, $m(g) = \\mathbb{E}[\\log(\\text{Coverage}) \\mid \\text{GC fraction}=g]$, and then subtract it from the log-transformed data to obtain a corrected signal.\n\nLet there be $n$ genomic bins, indexed by $i \\in \\{1,\\dots,n\\}$. For each bin $i$, we are given its GC fraction $g_i \\in [0,1]$ and its observed coverage $C_i \\ge 0$.\n\n**Step 1: Data Transformation**\nTo handle bins with zero coverage ($C_i=0$) and to convert the multiplicative bias into an additive one, we apply a logarithmic transformation. A small positive constant, the pseudocount $\\epsilon$, is added to the coverage before taking the natural logarithm. The transformed response variable for each bin $i$ is:\n$$ y_i = \\log(C_i + \\epsilon) $$\nThe systematic GC bias is then modeled as the conditional expectation function $m(g) = \\mathbb{E}[y \\mid g]$. Our task is to compute an estimate $\\hat{m}(g_i)$ for each observed $g_i$.\n\n**Step 2: Bias Estimation using LOESS**\nWe estimate the function $m(g)$ using LOESS, which involves fitting a simple model to a localized subset of the data. For each query point $x_0$ (which will be one of the $g_i$ values), we perform the following steps:\n\n**2a. Neighborhood Selection:**\nA local neighborhood around $x_0$ is defined. This neighborhood consists of the $k$ data points $(g_j, y_j)$ whose $g_j$ values are closest to $x_0$. The size of the neighborhood, $k$, is determined by the smoothing span parameter $s \\in (0,1]$:\n$$ k = \\lceil s \\cdot n \\rceil $$\nwhere $n$ is the total number of bins.\n\n**2b. Weight Calculation:**\nPoints within the neighborhood are assigned weights based on their distance from the query point $x_0$. This gives more influence to points closer to $x_0$. The specified weighting scheme uses the tricube function. First, the distances $d_j = |g_j - x_0|$ for all points $j$ in the neighborhood are calculated. These distances are then scaled by the maximum distance in the neighborhood, $D = \\max_j d_j$. The weight $w_j$ for each point $j$ in the neighborhood is:\n$$ w_j = \\left(1 - \\left(\\frac{d_j}{D}\\right)^3\\right)^3 $$\nIf $D=0$, which occurs if all points in the neighborhood are identical to $x_0$, all weights are taken to be $1$.\n\n**2c. Local Weighted Least Squares (WLS) Fit:**\nA local linear model, $y \\approx \\beta_0 + \\beta_1 x$, is fitted to the neighborhood points using the calculated weights. We seek the coefficients $\\beta = [\\beta_0, \\beta_1]^T$ that minimize the weighted sum of squared residuals. This is a standard WLS problem, expressed in matrix form as solving for $\\beta$ in the normal equations:\n$$ (X^T W X) \\beta = X^T W y $$\nHere, $y$ is the vector of response values $y_j$ in the neighborhood. $X$ is the design matrix, where each row corresponding to a point $j$ is $[1, g_j]$. $W$ is a diagonal matrix with the weights $w_j$ on its diagonal.\n\nOnce the coefficients $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ are estimated, the value of the smoothed function at $x_0$ is calculated as:\n$$ \\hat{m}(x_0) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_0 $$\n\n**2d. Rank-Deficiency Fallback:**\nIf the design matrix for the local regression, $X$, is rank-deficient, the WLS problem does not have a unique solution. This occurs if all predictor values $g_j$ in the neighborhood are identical. In this specific case, the problem specifies a fallback to a simpler estimator: the weighted average of the response values in the neighborhood.\n$$ \\hat{m}(x_0) = \\frac{\\sum_j w_j y_j}{\\sum_j w_j} $$\n\nBy iterating this procedure for each $x_0 = g_i$ where $i \\in \\{1, \\dots, n\\}$, we obtain the vector of estimated systematic effects, $[\\hat{m}(g_1), \\dots, \\hat{m}(g_n)]$.\n\n**Step 3: Coverage Normalization**\nThe final step is to correct the original coverage values. The estimated systematic effect $\\hat{m}(g_i)$ is subtracted from the log-transformed coverage $y_i$. The result is then exponentiated to revert to the original scale. This yields the normalized coverage $\\tilde{C}_i$:\n$$ \\tilde{C}_i = \\exp(y_i - \\hat{m}(g_i)) = \\frac{C_i + \\epsilon}{\\exp(\\hat{m}(g_i))} $$\nThis value represents the coverage of bin $i$ relative to the expected coverage for a bin with GC content $g_i$. As a ratio of two quantities with the same units (read counts), $\\tilde{C}_i$ is a dimensionless quantity, suitable for comparisons across different bins and samples. The final values are rounded to $6$ decimal places.\n\nThe described algorithm is implemented for each provided test case, yielding a vector of normalized coverages for each.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Main function to solve the GC bias correction problem for all test cases.\n    \"\"\"\n\n    def gc_bias_correction(gc_fractions, coverages, s, epsilon):\n        \"\"\"\n        Performs GC bias correction using a LOESS-based approach.\n\n        Args:\n            gc_fractions (list): List of GC fractions for each bin.\n            coverages (list): List of observed coverages for each bin.\n            s (float): The LOESS smoothing span parameter.\n            epsilon (float): A pseudocount to add to coverages before log-transform.\n\n        Returns:\n            list: A list of normalized coverage values, rounded to 6 decimal places.\n        \"\"\"\n        g_all = np.array(gc_fractions)\n        C_all = np.array(coverages, dtype=float)\n        n = len(g_all)\n\n        # Step 1: Data Transformation\n        y_all = np.log(C_all + epsilon)\n        \n        m_hat_values = np.zeros(n)\n\n        # Step 2: Bias Estimation using LOESS for each data point\n        for i in range(n):\n            x0 = g_all[i]\n            \n            # Step 2a: Neighborhood Selection\n            k = math.ceil(s * n)\n            distances = np.abs(g_all - x0)\n            neighbor_indices = np.argsort(distances)[:k]\n            \n            g_neigh = g_all[neighbor_indices]\n            y_neigh = y_all[neighbor_indices]\n\n            # Step 2b: Weight Calculation\n            local_distances = np.abs(g_neigh - x0)\n            max_dist = np.max(local_distances)\n            \n            if max_dist  1e-9: # Use a tolerance for float comparison\n                weights = np.ones(k)\n            else:\n                scaled_distances = local_distances / max_dist\n                weights = (1 - scaled_distances**3)**3\n\n            # Step 2c  2d: Local Weighted Least Squares (WLS) Fit with Fallback\n            if np.max(g_neigh) - np.min(g_neigh)  1e-9:\n                # Rank-deficiency fallback\n                m_hat = np.sum(weights * y_neigh) / np.sum(weights)\n            else:\n                # WLS computation\n                X = np.vstack([np.ones(k), g_neigh]).T\n                \n                # Efficient calculation of X.T @ W and (X.T @ W) @ X\n                XTW = X.T * weights\n                A = XTW @ X\n                b = XTW @ y_neigh\n                \n                try:\n                    beta = np.linalg.solve(A, b)\n                    # Estimate m(x0) = beta_0 + beta_1 * x0\n                    m_hat = beta[0] + beta[1] * x0\n                except np.linalg.LinAlgError:\n                    # Fallback in case of numerical instability\n                    m_hat = np.sum(weights * y_neigh) / np.sum(weights)\n\n            m_hat_values[i] = m_hat\n\n        # Step 3: Coverage Normalization\n        normalized_coverage = np.exp(y_all - m_hat_values)\n\n        return np.round(normalized_coverage, 6).tolist()\n\n    test_cases = [\n        # Test case A\n        (\n            [0.20, 0.26, 0.32, 0.38, 0.44, 0.50, 0.56, 0.62, 0.68, 0.74, 0.80, 0.86],\n            [30, 31, 29, 28, 27, 30, 32, 34, 36, 37, 38, 39],\n            0.60,\n            0.50\n        ),\n        # Test case B\n        (\n            [0.00, 0.10, 0.30, 0.50, 0.70, 0.90, 1.00],\n            [15, 20, 28, 32, 28, 20, 16],\n            0.60,\n            0.50\n        ),\n        # Test case C\n        (\n            [0.15, 0.35, 0.55, 0.75, 0.95],\n            [0, 10, 25, 0, 18],\n            0.80,\n            0.50\n        ),\n        # Test case D\n        (\n            [0.40, 0.45, 0.90],\n            [22, 24, 30],\n            0.60,\n            0.50\n        ),\n    ]\n\n    results = []\n    for case in test_cases:\n        gc_fractions, coverages, s, epsilon = case\n        result = gc_bias_correction(gc_fractions, coverages, s, epsilon)\n        results.append(result)\n\n    # Final print statement in the exact required format without spaces.\n    output_parts = []\n    for res_list in results:\n        inner_str = '[' + ','.join(map(str, res_list)) + ']'\n        output_parts.append(inner_str)\n    final_str = '[' + ','.join(output_parts) + ']'\n    print(final_str)\n\nsolve()\n```"
        },
        {
            "introduction": "In high-throughput clinical and research settings, multiplexing many samples in a single sequencing run is standard practice, but it introduces the risk of sample mis-assignment and contamination. Distinguishing between different sources of contamination—such as pre-analytical sample mixing, within-run index hopping, and cross-run instrument carryover—is a critical diagnostic skill. This problem  presents a realistic troubleshooting scenario, challenging you to use experimental design and specific data signatures to pinpoint the source of these confounding signals.",
            "id": "4353892",
            "problem": "In a multiplexed Next-Generation Sequencing (NGS) workflow for precision oncology, you are asked to investigate unexpected low-level variant signals appearing in samples that should not contain those variants. The facility uses patterned flow cells with exclusion amplification and performs sequencing-by-synthesis with index reads for sample demultiplexing. Multiplexing relies on synthetic index oligonucleotides attached during library preparation; each library is assigned one or more index sequences, and read assignment is performed by exact or near-exact index matching during demultiplexing. From first principles, note that sample identity in multiplexed NGS rests on the accurate association of sequence fragments to their intended index sequences, and that errors in this association can arise either from pre-sequencing mixing of molecules or from mis-assignment processes during cluster generation or instrument operation. Consider the following controlled study.\n\nA cohort of tumor and matched normal libraries is sequenced across $R=3$ runs on the same instrument. Run $1$ uses single indexing, Run $2$ uses Unique Dual Indexing (UDI), and Run $3$ uses UDI but is performed after an abbreviated instrument wash. Each run has $L=4$ lanes. Libraries are pooled at equimolar ratios and spiked with $1$ Population Reference In Error (PhiX) control for calibration. For simplicity, assume lane loading is balanced such that in some lanes, sample $B$ is present, and in other lanes, sample $B$ is absent. Variant $V_{B}$ is a private single-nucleotide variant (SNV) present only in sample $B$ at a true clonal fraction of $100\\%$ in $B$ and $0\\%$ in all other samples at the biological level. Observations:\n\n- In Run $1$ (single index), when samples $A$ and $B$ are co-loaded in a lane, sample $A$ shows $f_{1} \\approx 0.006$ of reads carrying $V_{B}$, but when $B$ is absent from a lane, $f_{1} \\approx 0.000$. Across different lanes in the same run, $f_{1}$ is nonzero only in lanes that contain $B$.\n- In Run $2$ (UDI), with the same pooling design, sample $A$ shows $f_{2} \\approx 0.0002$ of reads carrying $V_{B}$ in lanes containing $B$, and $f_{2} \\approx 0.000$ in lanes without $B$.\n- In Run $3$ (UDI, abbreviated wash), sample $C$—which was present only in Run $2$—produces reads with its distinctive index pair and multiple private variants appearing across all lanes despite sample $C$ not being loaded in Run $3$. The fraction is approximately $f_{3} \\approx 0.0005$ in each lane of Run $3$, independent of which samples are present in those lanes.\n- Separately, a particular library $D$ (prepared twice independently from the same biospecimen with fresh indexing oligos) shows variant $V_{X}$ at $f_{4} \\approx 0.05$ in both Run $2$ and Run $3$, on different instruments and lanes, with no dependence on which other samples are co-loaded.\n\nSuppose the probability that an index becomes reassigned within a lane due to free adapters during patterned flow cell clustering is $p$, and that in UDI the probability that both index reads jointly yield a valid index pair assigned to the wrong sample scales approximately as $p^{2}$ under independence, while physical mixing of molecules upstream of indexing induces a true mixture fraction that persists across instruments and runs. Using these premises and the observations above, choose the statement(s) that correctly define the phenomena and distinguish within-lane index hopping from cross-run carryover contamination in this study.\n\nA. The nonzero $f_{1}$ only in lanes containing sample $B$ and its strong reduction to $f_{2}$ with Unique Dual Indexing is characteristic of index cross-talk via within-lane index hopping; mitigation includes reducing free adapter concentration and using UDI, and the effect should not appear in lanes where $B$ is absent.\n\nB. The appearance of sample $C$’s reads in Run $3$ at approximately uniform $f_{3}$ across all lanes despite being absent from Run $3$ is characteristic of cross-run carryover contamination (instrument memory), which is distinguished from within-lane index hopping by persistence across runs and independence from lane co-loading; mitigation includes thorough instrument wash protocols and fluidics replacement.\n\nC. The persistent $f_{4} \\approx 0.05$ signal for variant $V_{X}$ in library $D$ across different runs, lanes, and instruments indicates sample contamination upstream of indexing (true molecular mixture), which will not be resolved by changing index design or instrument wash; mitigation requires re-extraction and stricter pre-PCR cleanroom practices.\n\nD. The Run $1$ pattern is more consistent with base-calling errors due to phasing and pre-phasing during sequencing-by-synthesis, because such errors selectively inject private variants from a specific co-loaded sample into another within the same lane; mitigation is to increase cycle-specific phasing correction rather than altering indexing strategy.\n\nE. The reduction of $f_{2}$ in Run $2$ proves that UDI eliminates cross-run carryover contamination and thus any signal like $f_{3}$ must be due solely to base-calling errors; therefore enhanced computational demultiplexing is sufficient to prevent such contamination without instrument wash.\n\nSelect all that apply.",
            "solution": "The problem requires an analysis of four distinct observations ($f_1$, $f_2$, $f_3$, $f_4$) to identify the underlying causes of anomalous variant signals in a multiplexed NGS study. The provided first principles allow for distinguishing between different sources of error based on their characteristic signatures.\n\n**Analysis of Observations**\n\n1.  **Observations on $f_1$ and $f_2$ (Within-Lane Index Hopping):**\n    - In Run $1$, using single indexing (SI), reads from sample $A$ show the private variant $V_B$ at a fraction $f_1 \\approx 0.006$. Critically, this only occurs in lanes where sample $B$ is physically co-loaded. This strong dependency on lane-level co-presence points to a mechanism occurring *within the lane* during the sequencing process.\n    - In Run $2$, switching to Unique Dual Indexing (UDI) reduces this mis-assignment fraction dramatically to $f_2 \\approx 0.0002$. The problem states that index mis-assignment has a probability $p$ with SI, and this scales to approximately $p^2$ with UDI. The observed reduction from $f_1 \\approx 0.006$ to $f_2 \\approx 0.0002$ is consistent with this model (e.g., if $p \\approx 0.006$, then $p^2 \\approx 3.6 \\times 10^{-5}$; the observed $f_2$ is in the expected order of magnitude).\n    - This phenomenon is known as **index hopping** or index cross-talk. On patterned flow cells with exclusion amplification, a proposed mechanism is that free-floating index adapters from one library can prime amplification for a fragment from another library, creating a chimeric molecule with the wrong index. Since this happens on the flow cell, it is lane-specific. UDI is a powerful mitigation because for a sample $B$ read to be mis-assigned to sample $A$, *both* indices must be swapped to form a valid, different index pair (that of sample $A$), which is a much rarer event.\n\n2.  **Observation on $f_3$ (Cross-Run Carryover Contamination):**\n    - In Run $3$, reads corresponding to sample $C$ are detected in *all lanes* at a fraction $f_3 \\approx 0.0005$, despite sample $C$ not being loaded in Run $3$ at all.\n    - The key characteristics are: (a) the source of contamination is from a *previous run* (Run $2$), and (b) the contamination is spread across *all lanes* of the current run, independent of the sample composition within each lane.\n    - This signature is inconsistent with index hopping, which is lane-specific and requires co-loading. It is the defining characteristic of **cross-run carryover contamination**, often called \"instrument memory.\" Residual library molecules from a previous run remain in the instrument's fluidics (e.g., tubing, valves) and are flushed onto the flow cell during the loading process of the subsequent run. The problem explicitly states that Run $3$ was preceded by an \"abbreviated instrument wash,\" which is the direct cause of this failure to clear the system. The UDI on sample $C$'s library molecules remains intact, allowing them to be correctly identified as belonging to $C$, thus revealing the physical contamination event.\n\n3.  **Observation on $f_4$ (Pre-Analytical Contamination):**\n    - Library $D$ consistently shows variant $V_X$ at a relatively high fraction, $f_4 \\approx 0.05$.\n    - This signal is independent of sequencing run, instrument, lane, or co-loaded samples. It was observed in independent library preparations from the same source biospecimen.\n    - These features rule out both index hopping (which is lane- and co-loading-dependent) and cross-run carryover (which is run- and instrument-dependent). The persistence of the signal across independent library preps and instruments indicates that the contamination is a property of the source material itself. It is a **true molecular mixture** that occurred upstream of any sequencing workflow step described, likely during sample collection, handling, or nucleic acid extraction. This is pre-analytical or sample-to-sample contamination. The fraction of $5\\%$ is also significantly higher than typical rates for index hopping or carryover.\n\n**Evaluation of Options**\n\n**A. The nonzero $f_{1}$ only in lanes containing sample $B$ and its strong reduction to $f_{2}$ with Unique Dual Indexing is characteristic of index cross-talk via within-lane index hopping; mitigation includes reducing free adapter concentration and using UDI, and the effect should not appear in lanes where $B$ is absent.**\nThis statement accurately summarizes the analysis of $f_{1}$ and $f_{2}$. The dependence on co-loading points to a within-lane phenomenon. The significant reduction with UDI is the expected behavior for mitigating index hopping. The listed mitigation strategies are correct. The condition that the effect is absent when the source sample is absent from the lane is a defining feature and was observed.\n\n**B. The appearance of sample $C$’s reads in Run $3$ at approximately uniform $f_{3}$ across all lanes despite being absent from Run $3$ is characteristic of cross-run carryover contamination (instrument memory), which is distinguished from within-lane index hopping by persistence across runs and independence from lane co-loading; mitigation includes thorough instrument wash protocols and fluidics replacement.**\nThis statement accurately describes the phenomenon observed for $f_{3}$. The source of the contamination is a previous run, and its signal is independent of lane composition in the current run, clearly distinguishing it from index hopping. The cause (carryover) and mitigation (thorough wash) are correctly identified.\n\n**C. The persistent $f_{4} \\approx 0.05$ signal for variant $V_{X}$ in library $D$ across different runs, lanes, and instruments indicates sample contamination upstream of indexing (true molecular mixture), which will not be resolved by changing index design or instrument wash; mitigation requires re-extraction and stricter pre-PCR cleanroom practices.**\nThis statement correctly interprets the evidence for $f_{4}$. The high fraction ($\\approx 5\\%$) and its independence from all sequencing-related variables (run, lane, instrument, co-loading) point to a contamination event that occurred before the library was created. Such a pre-analytical contamination cannot be fixed by downstream methods like UDI or instrument washing; it requires addressing the upstream laboratory workflow.\n\n**D. The Run $1$ pattern is more consistent with base-calling errors due to phasing and pre-phasing during sequencing-by-synthesis, because such errors selectively inject private variants from a specific co-loaded sample into another within the same lane; mitigation is to increase cycle-specific phasing correction rather than altering indexing strategy.**\nThis is an incorrect explanation. Phasing and pre-phasing affect the signal of a given cluster in subsequent sequencing cycles, primarily smearing base calls later in a read. They do not typically cause a wholesale mis-assignment of a read's identity from one sample to another, which is determined by the separate index read. Most importantly, this hypothesis is falsified by the observation that switching to UDI (an indexing strategy change) dramatically mitigates the problem. If phasing were the root cause, a change in indexing would have little to no effect.\n\n**E. The reduction of $f_{2}$ in Run $2$ proves that UDI eliminates cross-run carryover contamination and thus any signal like $f_{3}$ must be due solely to base-calling errors; therefore enhanced computational demultiplexing is sufficient to prevent such contamination without instrument wash.**\nThis statement is built on a series of logical errors. First, the reduction in $f_{2}$ demonstrates UDI's efficacy against *within-lane index hopping*, not *cross-run carryover*. These are distinct physical phenomena. Second, concluding that $f_3$ must be due to base-calling errors is a *non sequitur* and contradicts the evidence pointing to carryover (source from a previous run, abbreviated wash). Third, carryover contamination involves the physical presence of molecules from a previous run; no amount of computational demultiplexing can erase them. Demultiplexing will correctly identify them, revealing the contamination, not preventing it. The problem is physical and requires a physical solution (washing).",
            "answer": "$$\\boxed{ABC}$$"
        }
    ]
}