## 引言
在基因组的蓝图中，选择性剪接是创造生命复杂性的核心机制之一，它允许单个基因产生多种[信使RNA](@entry_id:262893)（mRNA）转录本，即异构体，从而极大地扩展了[蛋白质组](@entry_id:150306)的[功能多样性](@entry_id:148586)。然而，这种多样性也给研究带来了巨大挑战：我们如何从数以百万计的RNA测序（[RNA-seq](@entry_id:140811)）短片段中，准确地识别出所有存在的异构体，并精确量化它们的[相对丰度](@entry_id:754219)？这正是转录本发现与异构体定量所要解决的核心问题。错误或不精确的定量可能导致对基因调控、疾病机制和生物标志物的误判，因此掌握其背后的原理与方法至关重要。

本文旨在系统性地解析这一复杂领域，带领读者从基础理论走向前沿应用。我们将通过三个章节的深入探讨，为您构建一个完整的知识框架：
*   在“原理与机制”一章中，我们将深入剖析转录本组装的计算策略（有参与[从头组装](@entry_id:172264)）、异构体定量的统计基础（[混合模型](@entry_id:266571)与[EM算法](@entry_id:274778)），以及各种标准化指标（如FPKM与[TPM](@entry_id:170576)）的优劣，并揭示其根本性的局限。
*   在“应用与跨学科交叉”一章中，我们将展示这些理论如何在现实世界中发挥作用，涵盖从实验设计（长短读长测序的选择）到复杂的生物信息学流程，再到其在临床诊断和[蛋白质基因组学](@entry_id:167449)中的关键应用。
*   最后，在“动手实践”部分，通过精心设计的问题，您将有机会亲手应用所学知识，解决转录本定量中的具体挑战，从而巩固和深化理解。

通过本次学习，您将不仅掌握转录本分析的技术细节，更能理解其在[精准医疗](@entry_id:152668)和基础研究中的战略性地位。

## 原理与机制

在理解了转录本发现与定量的重要性之后，本章将深入探讨其背后的核心科学原理与计算机制。我们将从如何从数以百万计的短序列片段（reads）中重建转录本结构开始，然后转向如何精确量化这些转录本的表达丰度，最后阐述支撑这些方法的严谨[统计模型](@entry_id:755400)及其固有的局限性。

### 从Reads到转录本：组装问题

RNA测序（[RNA-seq](@entry_id:140811)）产生的短reads就像是一本被撕碎的书的碎片。[转录组](@entry_id:274025)发现的首要任务，即**转录本组装（transcript assembly）**，就是将这些碎片拼接起来，重建出书中完整的“句子”——即成熟的mRNA转录本。目前主要存在两种策略：有参组装和[从头组装](@entry_id:172264)。

#### 有参转录本组装

当研究的物种具有高质量的[参考基因组](@entry_id:269221)时，例如人类或小鼠，**有参转录本组装（reference-guided transcript assembly）**是首选策略。该方法的第一步是使用**[剪接感知比对](@entry_id:175766)工具（splice-aware aligner）**将RNA-seq的reads比对到参考基因组上。这些工具能够识别那些跨越外显子-内含子边界的reads（即“跨接reads”或“split reads”），从而直接揭示剪接事件的发生。

比对完成后，算法会构建一个**剪接图（splice graph）**来表示基因的转录结构 。在这个图中，节点（vertices）通常代表外显子的边界（即剪接供体位点和受体位点），而有向边（edges）则代表两种连接关系：连接同一个外显子内部的“外显子段边”（exonic segment edges）和连接不同外显子之间的“剪接边”（junction edges）。每一条剪接边都由支持该剪接事件的跨接reads所证实。

在这个图模型中，一个完整的、生物学上合理的转录本异构体（isoform）就对应于一条从代表[转录起始位点](@entry_id:263682)的“源”节点（source）到代表[转录终止](@entry_id:183504)位点的“汇”节点（sink）的完整路径。这条路径必须遵循基因组坐标的顺序和转录方向，并交替穿过外显子段和剪接边，形成一个连续的外显子链 。

有参组装的主要优势在于其**高[精确度](@entry_id:143382)（precision）**和**高灵敏度（sensitivity）**（对于与[参考基因组](@entry_id:269221)相符的转录本而言）。由于reads的比对被限制在基因组的特定区域，这极大地减少了因测序错误或基因组重复序列而产生的“[嵌合体](@entry_id:264354)”（chimeric）或伪迹转录本的可能性。因此，在需要高[可重复性](@entry_id:194541)和快速周转的临床应用场景中，如有参比对的人类肿瘤样本分析，该方法是理想选择 。然而，它的主要缺点是**参考偏倚（reference bias）**：它很难或不可能发现那些在[参考基因组](@entry_id:269221)中不存在的序列，例如由大规模结构变异、基因融合或病毒整合产生的新转录本。

#### 从头转录本组装

当研究的物种没有[参考基因组](@entry_id:269221)时，例如在对一个全新的非模式生物进行探索性研究时，**从头转录本组装（_de novo_ transcriptome assembly）**是唯一可行的方法 。该方法不依赖任何先验的基因组信息，而是直接从reads本身重建转录本。

其核心思想是首先将所有reads打碎成更小的、固定长度为$k$的重叠片段，称为**[k-mer](@entry_id:166084)s**。然后，算法会构建一个**[de Bruijn图](@entry_id:263552)**。在这个图中，每个k-mer是一个节点，如果两个k-mer有$k-1$个[核苷](@entry_id:195320)酸的重叠，就在它们之间建立一条有向边。理论上，一个完整的转录本对应于图中的一条路径。

[从头组装](@entry_id:172264)的最大优势在于它能够发现任何有足够reads覆盖的转录本，无论其序列多么新颖。这对于发现新基因和研究[演化距离](@entry_id:177968)较远的物种至关重要。然而，这种无偏倚性也带来了巨大的代价。由于测序错误、重复序列和高度相似的异构体，[de Bruijn图](@entry_id:263552)往往极其复杂，充满了分支、气泡和断裂。这导致组装结果的**[精确度](@entry_id:143382)通常较低**，常见的问题包括：低表达转录本组装不完整（形成碎片化的[重叠群](@entry_id:177271)，contigs）以及错误地将不同转录本的片段拼接在一起（形成[嵌合体](@entry_id:264354)）。此外，构建和遍历一个复杂真核生物的全[转录组](@entry_id:274025)[de Bruijn图](@entry_id:263552)在计算上是极其昂贵的，需要巨大的内存和计算时间 。

总之，这两种策略各有取舍：有参组装精确、高效，但受限于已知信息；[从头组装](@entry_id:172264)能够进行全新发现，但代价是更高的计算成本和更低的精确度。

### 量化转录本丰度：从Reads到浓度

在获得了转录本的集合（无论是通过组装还是直接使用已有的[基因注释](@entry_id:164186)数据库）之后，下一个关键任务是**转录本定量（transcript quantification）**：估计每个异构体的相对丰度。这个过程面临两大挑战：读段归属的模糊性和测序过程的系统性偏倚。

#### 核心挑战之一：读段归属的模糊性

许多[RNA-seq](@entry_id:140811)的reads并不能唯一地比对到某一个特定的异构体上。这种情况尤其常见，因为不同的异构体往往共享大部分外显子。例如，一个完全包含在某个共享外显子内部的read，可能与表达该外显子的所有异构体都兼容。

为了系统地处理这种模糊性，我们引入了**读段[等价类](@entry_id:156032)（read equivalence class）**的概念。一个[等价类](@entry_id:156032)是指与同一组转录本兼容的所有reads的集合。我们可以构建一个**转录本兼容性矩阵（transcript-compatibility matrix）** $A$ 来形式化这种关系 。在该矩阵中，每一行代表一个等价类，每一列代表一个转录本。如果转录本$j$与等价类$i$中的reads兼容，则[矩阵元](@entry_id:186505)素$A_{ij}$为1（或一个权重值），否则为0。这个矩阵清晰地揭示了哪些reads提供了区分异构体的信息（即它们所属的[等价类](@entry_id:156032)只兼容一个转录本），哪些reads则带来了模糊性。

#### 核心挑战之二：采样偏倚

即使一个read可以唯一地比对到一个转录本，其计数本身也并不直接等同于该转录本的分子丰度。一个核心的偏倚来源是转录本的长度：在均匀采样假设下，一个更长的转录本比较短的转录本有更多的位置可以产生测序片段。

为了校正这种偏倚，我们必须引入**[有效长度](@entry_id:184361)（effective length）** $L_{\text{eff}}$ 的概念。[有效长度](@entry_id:184361)不仅仅是转录本的物理长度，它是一个更复杂的量，综合考虑了转录本的实际长度、测序文库的片段长度分布、读长以及序列在基因组上的唯一可比对性（mappability） 。它代表了一个转录本上能够产生可唯一识别的测序片段的有效“靶区”大小。

综合考虑，RNA-seq的定量模型可以概括为：一个转录本$p$产生的预期reads数$C_p$与其真实的分子丰度$\theta_p$和其[有效长度](@entry_id:184361)$L_{p, \text{eff}}$成正比，即 $\mathbb{E}[C_{p}] \propto \theta_{p} \cdot L_{p, \text{eff}}$。

#### 表达丰度的归一化指标

基于上述模型，我们知道使用原始的read计数进行比较是错误的。因此，研究人员开发了多种归一化指标。

**FPKM (Fragments Per Kilobase of transcript per Million mapped reads)** 的定义为：
$$ \mathrm{FPKM}_{j} = 10^{9} \cdot \frac{c_{j}}{N \cdot L_{j, \mathrm{eff}}} $$
其中，$c_j$是映射到转录本$j$的片段数，$L_{j, \mathrm{eff}}$是其[有效长度](@entry_id:184361)（以千碱基为单位），$N$是文库中总的比对片段数。通过数学推导可以发现，FPKM的[期望值](@entry_id:150961)与一个依赖于整个样本中所有转录本丰度和[有效长度](@entry_id:184361)的复杂分母成反比 。这意味着，即使一个基因的真实表达水平在两个样本中完全相同，仅仅因为另一个高表达基因的长度或丰度发生变化，该基因的FPKM值也可能改变。这使得FPKM不适合于样本间的直接比较。

**[TPM](@entry_id:170576) (Transcripts Per Million)** 解决了这个问题。其定义为：
$$ \mathrm{TPM}_{j} = 10^{6} \cdot \frac{c_{j}/L_{j, \mathrm{eff}}}{\sum_{k} (c_{k}/L_{k, \mathrm{eff}})} $$
[TPM](@entry_id:170576)首先用[有效长度](@entry_id:184361)对每个转录本的read计数进行归一化，得到一个与分子丰度成正比的量。然后，它再用这些归一化后的“丰度”的总和对每个转录本进行二次归一化，并乘以$10^6$。通过这种方式，[TPM](@entry_id:170576)值直接反映了在一个由一百万个RNA分子组成的群体中，来自某个特定转录本的分子数量。其[期望值](@entry_id:150961)正比于该转录本的相对分子丰度 $\theta_j / \sum_k \theta_k$，与样本中其他转录本的长度无关，因此**TPM在样本间具有可比性** 。

对于特定的剪接事件，例如[外显子跳跃](@entry_id:275920)，研究人员更关心的是包含或排除某个外显子的转录本所占的比例，这个指标被称为**百分比剪接纳入（Percent Spliced In, PSI或$\Psi$）**。一个正确的PSI估计量必须校正[有效长度](@entry_id:184361)带来的偏倚 。给定一个包含事件的路径（incl）和排除事件的路径（excl），其PSI估计量为：
$$ \hat{\Psi} = \frac{C_{\text{incl}}/L_{\text{incl}}}{C_{\text{incl}}/L_{\text{incl}} + C_{\text{excl}}/L_{\text{excl}}} $$
这里，$C$是支持各路径的reads计数，$L$是对应路径的[有效长度](@entry_id:184361)。忽略[有效长度](@entry_id:184361)校正（即使用 $C_{\text{incl}} / (C_{\text{incl}} + C_{\text{excl}})$）将会导致系统性的偏差。

#### 现代定量方法：伪比对

为了加速定量过程，现代工具如Kallisto和Salmon采用了**伪比对（pseudoalignment）**技术。这种方法避免了传统比对中逐个碱基的精确匹配，从而极大地提高了速度。其核心是建立一个转录本序列的**k-mer索引**，该索引将每个[k-mer](@entry_id:166084)映射到包含它的所有转录本的集合 。

当一个read进入时，算法会提取其所有的k-mers，并在索引中查找它们。然后，通过对这些k-mer对应的转录本集合进行**交集（intersection）**运算，就可以快速确定该read的[等价类](@entry_id:156032)。例如，如果一个read包含三个[k-mer](@entry_id:166084)s，它们分别对应转录本集合$\{\tau_1, \tau_2\}$，$\{\tau_1\}$和$\{\tau_1, \tau_3\}$，那么该read的[等价类](@entry_id:156032)就是这三个集合的交集，即$\{\tau_1\}$。这个过程完全在[k-mer](@entry_id:166084)和集合的层面上进行，无需进行耗时的[序列比对](@entry_id:172191)。

### 异构体定量的统计基础

为了从模糊的、带偏倚的reads计数中解析出精确的异构体丰度，我们需要一个严谨的统计框架。这个框架就是**混合模型（mixture model）**，通常使用**[期望最大化](@entry_id:273892)（Expectation-Maximization, EM）算法**进行求解。

#### 混合模型[似然函数](@entry_id:141927)

我们将观察到的所有[RNA-seq](@entry_id:140811)片段看作是从一个由$J$个异构体组成的混合物中抽样得到的结果。每个异构体$j$的未知相对丰度为$\theta_j$（满足$\sum_{j=1}^{J} \theta_{j} = 1$）。

对于单个片段$i$，它源自异构体$j$的概率为$\theta_j$。而如果它确实源自异构体$j$，我们观察到这个特定片段的条件概率为$w_{ij}$（这个权重$w_{ij}$已经包含了[有效长度](@entry_id:184361)、序列偏好等所有偏倚的校正）。根据[全概率公式](@entry_id:194231)，我们观察到片段$i$的总概率是所有可能来源的加权和：$P(x_i | \theta) = \sum_{j=1}^{J} w_{ij} \theta_j$。

假设所有$N$个片段的抽样是相互独立的，那么观察到整个数据集的**似然函数（likelihood function）**就是所有单个片段概率的乘积 ：
$$ \mathcal{L}(\theta) = \prod_{i=1}^{N} \left( \sum_{j=1}^{J} w_{ij} \theta_j \right) $$
我们的目标就是找到一组参数$\theta = (\theta_1, \dots, \theta_J)$，使得这个似然函数最大化。

#### [期望最大化](@entry_id:273892)（EM）算法

直接最大化上述[似然函数](@entry_id:141927)在数学上非常困难。[EM算法](@entry_id:274778)通过引入[隐变量](@entry_id:150146)（latent variables）提供了一个优雅的迭代求解方案。这里的[隐变量](@entry_id:150146)就是每个read的真实来源异构体。

[EM算法](@entry_id:274778)在两个步骤之间交替进行，直至收敛：

1.  **E-步（Expectation Step）**：在这一步，我们使用当前对丰度$\theta$的估计值，来计算每个read $i$ 来自每个异构体$j$的后验概率。这个后验概率被称为**责任（responsibility）**，记为$r_{ij}$。根据贝叶斯定理，我们可以推导出 ：
    $$ r_{ij} = \mathbb{P}(\text{read } i \text{ from isoform } j \mid \text{data}, \theta) = \frac{\theta_{j} w_{ij}}{\sum_{k=1}^{J} \theta_{k} w_{ik}} $$
    这个公式的直观意义是：read $i$ 归属于异构体$j$的概率，正比于异构体$j$的先验丰度$\theta_j$与它产生该read的[条件概率](@entry_id:151013)$w_{ij}$的乘积。分母是一个归一化因子，确保对于每个read $i$，所有责任之和为1。这一步相当于对每个read进行“软分配”，而不是硬性地将其归于某一个异构体。

2.  **M-步（Maximization Step）**：在获得所有reads对所有异构体的责任后，我们重新估计异构体的丰度$\theta_j$。最大化期望[完全数](@entry_id:636981)据[对数似然](@entry_id:273783)的结果表明，新的丰度估计值正比于分配给该异构体的“有效reads计数” 。具体来说，分配给异构体$j$的总责任为$\sum_{i=1}^{N} r_{ij}$。因此，新的丰度估计为：
    $$ \theta_j^{\text{new}} = \frac{\sum_{i=1}^{N} r_{ij}}{\sum_{k=1}^{J}\sum_{i=1}^{N} r_{ik}} = \frac{\sum_{i=1}^{N} r_{ij}}{N} $$
    这个$\theta_j^{\text{new}}$代表的是源自异构体$j$的**片段比例**。为了得到反映真实**分子比例**的丰度（如[TPM](@entry_id:170576)），我们还需要对其进行[有效长度](@entry_id:184361)校正 ：
    $$ \alpha_j^{\text{norm}} = \frac{\frac{\sum_{i=1}^{N} r_{ij}}{L_{j}^{\mathrm{eff}}}}{\sum_{k=1}^{J} \frac{\sum_{i=1}^{N} r_{ik}}{L_{k}^{\mathrm{eff}}}} $$
    其中$\alpha_j^{\text{norm}}$是最终估计的、归一化后的分子丰度。

算法从一个初始的$\theta$猜测开始，交替执行E步和[M步](@entry_id:178892)，直到$\theta$值收敛，从而得到最终的异构体丰度估计。

### 基本限制：可识别性问题

尽管上述方法在理论上很强大，但它们也面临一个根本性的限制：**可识别性（identifiability）**问题。在某些情况下，即使拥有无限多的数据和完美的模型，我们也无法唯一地确定某些异构体的丰度。

这个问题在异构体之间序列高度相似或包含大量重复序列时尤为突出。一个极端的例子可以清晰地说明这一点 ：假设一个基因有两个异构体$T_{\text{incl}}$和$T_{\text{skip}}$，它们的序列完全由单一核苷酸（例如腺嘌呤A）组成，唯一的区别是$T_{\text{incl}}$多了一个同样由A组成的短外显子。如果测序读长短于这个额外外显子的长度，那么从这两个异构体上产生的任何read都将是完全相同的序列（例如`AAAA`）。

在这种情况下，所有的reads都属于同一个[等价类](@entry_id:156032)。这意味着在转录本兼容性矩阵$A$中，两列（代表两个异构体）是[线性相关](@entry_id:185830)的，导致该矩阵**[秩亏](@entry_id:754065)（rank-deficient）** 。从代数角度看，我们只有一个观测值（`AAAA`的总计数），却需要估计两个未知数（$T_{\text{incl}}$和$T_{\text{skip}}$的丰度），这是一个无唯一解的欠定方程组。这意味着任何将总read计数分配给这两个异构体的组合都是数学上可能的解。

这个例子揭示了一个深刻的原理：转录本定量的精确性不仅取决于算法的优劣，更从根本上受限于转录本自身的序列结构和我们能够获取的测序信息的性质。当信息不足以区分不同的生物学状态时，任何计算方法都将无能为力。