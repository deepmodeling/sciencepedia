## 引言
[差异基因表达](@entry_id:140753)（DGE）分析是现代生物学和精准医学研究的基石，它使我们能够量化和识别在不同生物学条件下（如疾病与健康、治疗前后）基因活动水平发生显著变化的基因。然而，从高通量测序产生的原始“读段计数”中得出可靠的生物学结论，远非直接比较数值那么简单。原始数据中固有的技术变异，如测序深度差异，以及数据本身的统计特性，如过离散，构成了巨大的挑战，若不妥善处理，极易导致错误的发现。

本文旨在为您构建一个关于[差异基因表达](@entry_id:140753)分析的坚实理论框架，从根本上理解其背后的统计学原理和实践考量。通过学习，您将能够超越黑箱操作，批判性地评估分析结果，并设计出更严谨、更具洞察力的实验。

- 在“**原理与机制**”一章中，我们将从零开始，解构整个分析流程。您将学习为何必须进行归一化，负二项分布如何成为描述基因表达计数的标准模型，以及广义线性模型（GLM）如何将这一切与复杂的实验设计联系起来。我们还将探讨在小样本情况下稳定估计的关键技术和在高通量背景下控制错误的统计策略。
- 在“**应用与跨学科联系**”一章中，理论将付诸实践。您将看到这些核心原理如何应用于解决复杂的生物学问题，例如处理[批次效应](@entry_id:265859)、分析[交互作用](@entry_id:164533)以探索[个性化医疗](@entry_id:152668)、追踪动态的时间序列变化，以及如何将分析扩展到单细胞数据、其他组学乃至非生物学领域。
- 最后，在“**动手实践**”部分，您将有机会通过编程练习来巩固所学知识，亲手实现归一化、[模型拟合](@entry_id:265652)和[多重检验校正](@entry_id:167133)等关键步骤，将抽象的理论转化为具体的技能。

## 原理与机制

在本章中，我们将深入探讨[差异基因表达](@entry_id:140753)分析背后的核心原理和统计机制。我们的讨论将从处理原始测序数据的基本挑战开始，逐步构建出现代[差异表达分析](@entry_id:266370)所依赖的复杂[统计模型](@entry_id:755400)。我们将阐明归一化、计数[数据建模](@entry_id:141456)、色散估计和[多重检验校正](@entry_id:167133)等关键步骤，最终将这些统计概念置于因果推断的框架内，以确保我们得出的生物学结论是严谨和可靠的。

### 原始计数的比较挑战：归一化的必要性

高通量测序实验，如RNA测序（RNA-seq），为每个样本中的每个基因生成了数以百万计的“读段”（reads）。经过生物信息学流程的比对和定量，这些读段被汇总为原始“计数”（raw counts），即映射到每个基因的读段数量。一个初学者可能会认为，可以直接比较不同条件下样本的原始计数值，以判断基因表达是否发生了变化。然而，这种简单的想法是错误的，因为它忽略了测序过程中产生的固有技术变异。

最主要的技术变异来源是**[测序深度](@entry_id:178191)**（sequencing depth），也称为**文库大小**（library size）。文库大小是指在一个样本中成功比对到基因组或[转录组](@entry_id:274025)特征的总读段数。这个数值在不同样本之间可能存在巨大差异，这通常是由于样本上样到测序仪流动槽（flow cell）时的浓度差异等随机技术因素造成的。

一个基因的[期望计数](@entry_id:162854)值与其在样本中的真实转录本丰度成正比，同时也与该样本的总文库大小成正比。为了阐明这一点，我们考虑一个思想实验：假设我们有两个生物学上完全相同的重复样本 $S_1$ 和 $S_2$，但它们的测序深度不同，$S_1$ 产生了一千万条读段，而 $S_2$ 产生了二千万条读段。对于任何一个基因，即使其在两个样本中的真实生物丰度完全相同，我们也会期望在 $S_2$ 中观察到的计数值大约是 $S_1$ 的两倍。如果直接比较原始计数，我们会错误地得出所有基因在 $S_2$ 中都上调了一倍的结论。这种差异纯粹是技术假象，而非真实的生物学变化。

因此，在进行任何[差异表达](@entry_id:748396)检验之前，必须对原始计数进行**归一化**（normalization）。归一化的目标是计算每个样本的样本特异性**缩放因子**（scaling factor），并用这些因子调整原始计数，从而消除[测序深度](@entry_id:178191)的影响，使样本之间的计数值具有可比性。

然而，仅仅用总读段数（即文库大小）作为缩放因子进行归一化，又会引入新的问题，即**文库组成偏差**（compositional bias）。[RNA-seq](@entry_id:140811)数据本质上是**组合型数据**（compositional data）：在一个固定的测序“预算”（总读段数）下，一个基因的计数值增加，必然意味着其他基因的计数值总和减少。如果样本中有一小部分高度表达的基因发生剧烈的单向表达变化（例如，全部强烈上调），它们将“占据”更大比例的测序资源。这会导致文库的总读段数被人为抬高。如果我们用这个被扭曲的总读段数进行归一化，所有其他未发生变化的基因的计数值在归一化后会看起来像下调了，从而产生大量的[假阳性](@entry_id:635878)结果。

为了解决这一问题，研究人员开发了多种更为稳健的归一化方法，它们能够抵抗少数极端基因变化的影响。这些方法的核心思想是，假设大多数基因在不同样本间并**没有**发生[差异表达](@entry_id:748396)。它们基于这一假设来估算能够最好地对齐这些“稳定”基因的缩放因子。两种被广泛应用的先进方法是：

1.  **M值的修剪均值（Trimmed Mean of M-values, TMM）**：此方法在成对样本比较中尤其有效。对于一对样本，它为每个基因计算两个量：$M$值（log-fold change，即表达变化的对数倍数）和$A$值（average log-intensity，即平均表达强度的对数）。TMM方法假设大多数基因的$M$值应该接近于零。为了稳健地估计全局的缩放因子，它会移除$M$值和$A$值都极端异常的基因（即变化最大和表达量最高/最低的基因），然后对剩余基因的$M$值进行加权平均。这个加权平均值经过指数变换后，就得到了能有效对齐两个样本的缩放因子 。

2.  **比率中位数法（Median-of-Ratios）**：此方法首先为每个基因创建一个“伪参考样本”（pseudo-reference sample）。这个参考样本是通过计算该基因在所有样本中计数的**[几何平均数](@entry_id:275527)**（geometric mean）得到的。然后，对于每一个样本，计算其所有基因的计数值与对应伪参考计数值的比率。最后，取这些比率的**中位数**作为该样本的缩放因子。这个方法的巧妙之处在于，对于一个非[差异表达](@entry_id:748396)的基因，其计数值与其缩放因子的比值应该是一个近似恒定的基因特异性丰度。因此，计数值与[几何平均数](@entry_id:275527)的比率会约掉基因特异性丰度项，主要反映样本的缩放因子。由于[中位数](@entry_id:264877)对离群值（即[差异表达](@entry_id:748396)基因）具有高达$50\%$的耐受性（breakdown point），只要[差异表达](@entry_id:748396)的基因不超过总数的一半，这种方法就能非常稳健地估计出缩放因子 。

这些先进的归一化方法产生的缩放因子，将在后续的[统计模型](@entry_id:755400)中以“偏移量”的形式发挥关键作用。

### 基因表达计数的[统计建模](@entry_id:272466)：从泊松到[负二项分布](@entry_id:262151)

归一化解决了样本间的可比性问题，但要进行严格的统计推断，我们还需要一个能够准确描述计数数据随机性的[概率模型](@entry_id:265150)。

计数数据最基础的模型是**泊松分布**（Poisson distribution）。一个服从泊松分布的随机变量 $Y \sim \text{Poisson}(\mu)$，其概率质量函数为 $p(y \mid \mu) = \exp(-\mu)\,\mu^{y}/y!$。泊松分布的一个核心特征是**等散布性**（equidispersion），即其方差等于均值：$\mathrm{Var}(Y) = \mathbb{E}[Y] = \mu$ 。这意味着一个基因的表达变异性完全由其平均表达水平决定。

然而，在真实的RNA-seq数据中，泊松分布的等散布性假设几乎总是被违背。当我们观察一个基因在一组生物学重复样本（例如，来自不同病人的肿瘤组织）中的计数值时，通常会发现其样本方差远大于样本均值。这种现象被称为**过离散**（overdispersion）。例如，对于某个基因在4个生物学重复中观察到的计数为$\{50, 120, 80, 200\}$，其样本均值为$112.5$，而样本方差高达$4225$，远超均值。

过离散的产生是有其深刻的生物学和技术根源的。我们可以通过一个层级模型来理解它。假设每个生物学重复样本 $i$ 的基因表达真实速率 $\mu_i$ 本身就是一个随机变量，它来自于一个描述群体异质性的分布。在这个给定的 $\mu_i$ 条件下，测序过程产生的计数值 $Y_i$ 服从泊松分布，即 $Y_i \mid \mu_i \sim \text{Poisson}(\mu_i)$。根据**[全方差定律](@entry_id:184705)**（Law of Total Variance），$Y_i$ 的边际方差为：

$ \mathrm{Var}(Y) = \mathbb{E}[\mathrm{Var}(Y \mid \mu)] + \mathrm{Var}(\mathbb{E}[Y \mid \mu]) = \mathbb{E}[\mu] + \mathrm{Var}(\mu) $

由于生物学重复之间存在差异（$\mathrm{Var}(\mu) > 0$），所以计数的总方差 $\mathrm{Var}(Y)$ 必然大于其总均值 $\mathbb{E}[Y] = \mathbb{E}[\mu]$。这个额外的方差项 $\mathrm{Var}(\mu)$ 正是过离散的来源，它量化了未被[模型解释](@entry_id:637866)的变异。这些变异的来源包括：
*   **生物学异质性**：这是最主要的来源，包括病人间的遗传背景差异、肿瘤内不同亚克隆细胞的比例变化、免疫细胞浸潤程度不同以及[基因表达调控](@entry_id:185479)的随机性（如[转录爆发](@entry_id:156205)）。
*   **技术变异**：尽管归一化处理了测序深度，但其他技术步骤如RNA提取效率、[逆转录](@entry_id:141572)效率和PCR扩增的随机性也会引入额外的变异。

为了捕捉过离散这一关键特性，[差异表达分析](@entry_id:266370)的标准做法是使用**[负二项分布](@entry_id:262151)**（Negative Binomial, NB）来代替泊松分布。[负二项分布](@entry_id:262151)可以被看作是一个泊松-伽马[混合分布](@entry_id:276506)，它天然地允许方差大于均值。在[RNA-seq分析](@entry_id:173715)中，其均值-方差关系通常被[参数化](@entry_id:265163)为：

$ \mathrm{Var}(y) = \mu + \alpha \mu^2 $

这里的 $\mu$ 是计数的均值，而 $\alpha \ge 0$ 是**色散参数**（dispersion parameter）。这个参数 $\alpha$ 直接量化了基因特异性的过离散程度。当 $\alpha = 0$ 时，[负二项分布](@entry_id:262151)退化为泊松分布。$\alpha$ 越大，表示相对于均值的变异性越大。第一项 $\mu$ 代表了源于泊松抽样的“[散粒噪声](@entry_id:140025)”（shot noise），在低表达基因中占主导；第二项 $\alpha\mu^2$ 代表了生物学和技术异质性带来的额外变异，在高表达基因中占主导。

通过分析平方[变异系数](@entry_id:272423)（$\mathrm{CV}^2 = \mathrm{Var}/\mu^2$），我们可以更清晰地看到这一点：

$ \mathrm{CV}^2 = \frac{1}{\mu} + \alpha $

这个关系式揭示了在[RNA-seq](@entry_id:140811)数据中普遍观察到的**均值-色散趋势**：对于低表达基因（$\mu$ 小），$\mathrm{CV}^2$ 主要由 $1/\mu$ 项决定，因此变异性很高；而对于高表达基因（$\mu$ 大），$1/ \mu$ 项趋近于零，$\mathrm{CV}^2$ 趋于一个由生物学变异决定的平台期，这个平台的高度就是色散参数 $\alpha$ 。

### 核心引擎：负二项[广义线性模型](@entry_id:171019)

我们将归一化缩放因子和负二项分布结合起来，构建了现代[差异表达分析](@entry_id:266370)的核心工具：**负二项广义线性模型**（Negative Binomial Generalized Linear Model, NB-GLM）。GLM提供了一个强大的框架，能够将基因的[期望计数](@entry_id:162854)值与实验设计中的各种变量（如处理条件、批次效应、临床协变量等）联系起来。

一个典型的NB-GLM包含三个部分：

1.  **随机部分**：指定了响应变量的分布。在这里，我们假设基因 $g$ 在样本 $i$ 中的计数值 $y_{gi}$ 服从负二项分布，其均值为 $\mu_{gi}$，色散参数为 $\alpha_g$。
    $ y_{gi} \sim \text{NB}(\mu_{gi}, \alpha_g) $

2.  **系统部分**：即**线性预测子**（linear predictor），它是协变量的[线性组合](@entry_id:155091)。对于基因 $g$ 和样本 $i$，[线性预测](@entry_id:180569)子为 $x_i^\top \beta_g$。其中，$x_i$ 是样本 $i$ 的**设计向量**（design vector），它编码了样本的属性（例如，对于一个简单的两组比较，可以编码为0代表[对照组](@entry_id:188599)，1代表处理组）。$\beta_g$ 是基因 $g$ 特异性的**系数向量**（coefficient vector），这些系数是我们想要估计和检验的量。例如，在两组比较中，其中一个系数就代表了处理组相对于[对照组](@entry_id:188599)的**[对数倍数变化](@entry_id:272578)**（log-fold change, LFC）。

3.  **连接函数**（Link Function）：连接了随机部分的均值和系统部分。对于计数数据，标准选择是**[对数连接函数](@entry_id:163146)**（log link），它确保了模型预测的均值总是正数。

将这三部分与归一化结合在一起，完整的模型可以表述为：

$ \log(\mu_{gi}) = o_i + x_i^\top \beta_g $

这里的关键新增项是 $o_i$，它被称为**偏移量**（offset）。$o_i$ 的值被设定为样本 $i$ 归一化缩放因子 $s_i$ 的对数，即 $o_i = \log(s_i)$。通过将 $\log(s_i)$ 作为一个具有固定系数1的已知项加入到线性预测子中，模型能够直接在对数尺度上对测序深度进行调整。这个公式可以改写为 $\mu_{gi} = s_i \exp(x_i^\top \beta_g)$，它清晰地表明，[期望计数](@entry_id:162854)值 $\mu_{gi}$ 同时取决于样本的缩放因子 $s_i$ 和由实验设计决定的表达水平 $\exp(x_i^\top \beta_g)$。

通[过拟合](@entry_id:139093)这个模型，我们可以为每个基因估计出系数向量 $\beta_g$，并对这些系数进行统计检验（例如，检验代表LFC的系数是否显著不为零），从而识别出[差异表达](@entry_id:748396)的基因 。

### [模型拟合](@entry_id:265652)的实践挑战：色散估计的稳定性

虽然NB-GLM框架非常强大，但在实践中，尤其是在生物学研究中常见的**小样本**情境下（例如，每组只有 $n=3$ 个重复），模型的一个关键参数——基因特异性色散参数 $\alpha_g$——的估计会变得非常困难和不稳定。从仅仅几个数据点来精确估计一个方差相关的参数，其结果的随机误差会非常大。

*   如果某个基因的几个重复碰巧非常接近，其 $\alpha_g$ 的估计值可能会过低（接近于0）。使用这个被低估的色散进行统计检验，会导致标准误过小，[检验统计量](@entry_id:167372)被人为拔高，从而产生大量的**[假阳性](@entry_id:635878)**（Type I error）。
*   反之，如果某基因的重复碰巧波动很大，其 $\alpha_g$ 的估计值会过高。这会导致检验过于保守，丧失发现真实差异表达基因的能力，即产生大量的**假阴性**（Type II error）。

为了解决这个问题，现代的[差异表达分析](@entry_id:266370)工具（如[DESeq2](@entry_id:167268)和edgeR）采用了一种被称为**[经验贝叶斯收缩](@entry_id:748954)**（Empirical Bayes shrinkage）的策略，其核心思想是**跨基因共享信息**。这种方法基于一个合理的假设：即具有相似平均表达水平的基因，它们的色散水平也应该相似。

该方法的步骤如下：
1.  首先，为每个基因单独计算一个“原始”的色散估计值。
2.  然后，将所有基因的原始色散估计值与其平均表达水平作图，拟合出一条平滑的**均值-色散趋势线**。这条趋势线代表了数据中“典型”的基因在特定表达水平下应有的色散大小。
3.  最后，将每个基因不稳定的原始色散估计值，向这条稳定的全局趋势线进行“收缩”或“拉动”。收缩的强度取决于样本量和原始估计的离散程度。样本量越小，原始估计越不可靠，收缩的力度就越大。

通过这种方式，最终用于下游检验的色散估计值，是基因自身信息和来自所有基因的全局信息的加权平均。这种“[借力](@entry_id:167067)”于全体基因的方法，以引入微小偏差为代价，极大地降低了色散估计的方差，从而显著提高了检验的稳定性和功效，尤其是在小样本实验中。

### 高通量背景下的假设检验与[误差控制](@entry_id:169753)

在对成千上万个基因拟合了NB-GLM并获得了稳定的色散估计之后，我们对每个基因的LFC系数进行统计检验（通常是[Wald检验](@entry_id:164095)或似然比检验），从而得到每个基因对应的一个**[p值](@entry_id:136498)**。现在，我们面临着一个源于高通量实验本质的新挑战：**[多重假设检验](@entry_id:171420)**（multiple hypothesis testing）。

当我们同时检验成千上万个假设时，即使对于单个检验，我们将犯错的概率（Type I error rate, $\alpha$）控制在一个很低的水平（例如$0.05$），我们预期出现的[假阳性](@entry_id:635878)总数也会变得非常大。例如，检验$20,000$个基因，即使所有基因都没有差异表达，在$\alpha=0.05$的水平下，我们也会预期发现$20,000 \times 0.05 = 1,000$个[假阳性](@entry_id:635878)。

为了应对这个问题，我们需要从控制单个检验的错误率，转向控制一个总体的错误率。在基因组学中，最常用的错误率度量是**[错误发现率](@entry_id:270240)**（False Discovery Rate, FDR）。FDR被定义为在所有被宣布为“显著”的发现（即被拒绝的原假设）中，错误发现（即实际上是真原假设）所占的期望比例。与更严格地控制犯至少一个错误的概率（即家[族错误率](@entry_id:165945), FWER）的[Bonferroni校正](@entry_id:261239)相比，控制FDR允许我们在可接受的[假阳性](@entry_id:635878)比例下，获得更强的统计功效来发现真实的差异。

控制FDR的标准程序是 **[Benjamini-Hochberg](@entry_id:269887) (BH) 程序**。该程序的步骤如下：
1.  将所有 $m$ 个基因的p值从小到大排序：$p_{(1)} \le p_{(2)} \le \dots \le p_{(m)}$。
2.  设定一个目标FDR水平 $q$（例如，$0.05$）。
3.  寻找最大的索引 $k$，使得 $p_{(k)} \le \frac{k}{m}q$。
4.  拒绝所有p值小于或等于 $p_{(k)}$ 的原假设，即拒绝 $H_{(1)}, \dots, H_{(k)}$。

BH程序在p值相互独立或满足某种正相关性（PRDS）的条件下，能够保证FDR不超过 $q$ 。

一个与FDR紧密相关的概念是**q值**。对于一个特定的基因检验，其q值被定义为：当我们将该检验宣布为显著时，所要接受的最小FDR。q值可以被视为[p值](@entry_id:136498)在FDR控制框架下的等价物。为了计算q值，通常需要估计一个重要的参数 $\pi_0$，即所有被检验的基因中，真正没有差异表达（即原假设为真）的基因所占的比例。在BH程序中，$\pi_0$ 被保守地假设为1。然而，通过观察p值分布（在大量真原假设下，[p值](@entry_id:136498)应均匀分布在$[0,1]$区间），我们可以从数据中估计 $\pi_0$。通常，真实的 $\pi_0$ 小于1。利用这个估计值 $\hat{\pi}_0$ 来调整FDR的计算，可以获得比标准BH程序更高的[统计功效](@entry_id:197129)，从而发现更多的差异基因，同时仍然控制FDR 。

### 从[统计关联](@entry_id:172897)到生物学洞察：因果推断视角

最后，我们必须认识到，到目前为止所描述的整个流程——从归一化到NB-GLM再到FDR控制——在本质上是识别**统计关联**。模型告诉我们，在控制了某些协变量之后，基因表达与某个条件（如治疗）之间存在显著的统计学关系。然而，在精准医学和药物研发中，我们真正关心的是**因果关系**：治疗是否**导致**了基因表达的变化？

要从关联走向因果，我们需要引入**因果推断**（causal inference）的框架。利用**潜在结果**（potential outcomes）的语言，我们可以为每个基因 $g$ 定义一个清晰的因果量（estimand），即**平均处理效应**（Average Treatment Effect, ATE）：

$ \tau_g = \mathbb{E}[y_g(1) - y_g(0)] $

这里，$y_g(1)$ 是该基因在接受处理（$T=1$）时的潜在表达水平，$y_g(0)$ 是其在接受对照（$T=0$）时的潜在表达水平。ATE代表了在目标人群中，处理平均导致的基因表达变化。

在随机对照试验（RCT）中，由于处理分配是随机的，处理组和[对照组](@entry_id:188599)在所有（已测量和未测量的）基线特征上是可比的，因此简单的组间均值差异就能无偏地估计ATE。然而，在许多临床研究中，我们处理的是**观察性数据**，病人并非随机接受治疗。这时，要使我们在NB-GLM中估计出的LFC系数 $\beta_g$ 能够被解释为因果效应的估计，必须满足一系列严格的假设：

1.  **稳定单位处理价值假设（SUTVA）**：一个病人的治疗不会影响另一个病人的基因表达，且治疗本身没有隐藏的不同版本。
2.  **一致性（Consistency）**：一个病人实际接受的治疗，其观察到的结果与该治疗对应的潜在结果一致。
3.  **正定性（Positivity）**：对于任何具有特定基线特征组合的病人，他们接受治疗和不接受治疗的概率都大于零。
4.  **条件[可交换性](@entry_id:263314)（Conditional Exchangeability）**：也称为“无未测量混杂”假设。这是最关键也最难满足的假设。它要求，在调整了所有已测量的**混杂因素**（confounders）后，处理分配与[潜在结果](@entry_id:753644)是条件独立的。混杂因素是同时影响处理选择和基因表达的变量，例如病人的年龄、性别、肿瘤分期、肿瘤纯度、样本的细胞类型组成和实验批次等。

只有当这些假设（尤其是条件[可交换性](@entry_id:263314)）成立时，我们通过在NB-GLM中包含所有相关混杂因素作为协变量进行调整，才能宣称我们估计的差异表达反映了治疗的生物学效应，而不仅仅是一个被混杂的关联。因此，严谨的实验设计和全面的协变量数据收集，对于从[差异表达分析](@entry_id:266370)中得出有意义的生物学和临床结论至关重要。