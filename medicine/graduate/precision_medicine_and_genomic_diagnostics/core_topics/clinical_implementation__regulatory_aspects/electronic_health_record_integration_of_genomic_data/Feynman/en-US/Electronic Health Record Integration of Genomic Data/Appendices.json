{
    "hands_on_practices": [
        {
            "introduction": "A foundational challenge in genomic data management is ensuring that every unique biological variant has a single, canonical representation. This is particularly difficult for short insertions and deletions (indels) that occur in repetitive regions of the genome, such as homopolymer runs. This exercise  asks you to reason from the first principles of sequence alignment why this ambiguity exists and to derive the standard solution: left-normalization. Mastering this concept is crucial for building robust variant databases and ensuring that queries for a specific indel do not fail simply because it was described using a different, but equally valid, alignment.",
            "id": "4336681",
            "problem": "Electronic Health Record (EHR) systems increasingly store discrete genomic variants for Clinical Decision Support (CDS). However, short insertions and deletions (indels) occurring within homopolymer runs or tandem repeats can be aligned in multiple equally scoring ways, which, if not canonicalized, leads to non-unique keys for the same biological event across laboratories and knowledge bases. Consider a reference sequence $R$ over the nucleotide alphabet and a patient haplotype $H$ that differs by a single short indel inside a maximal homopolymer run. Use first principles of pairwise sequence alignment via Dynamic Programming (DP) with a match reward $+m$, mismatch penalty $-u$, and linear or affine gap penalties (gap opening $-g$ and gap extension $-e$ with $e \\ge 0$) to reason about the existence of multiple equally optimal alignments. Then, using the Variant Call Format (VCF) minimal allele rule (trim common prefix and suffix while keeping the reference allele length at least $1$) as the canonical allele minimalization step, describe an algorithm to compute the leftmost equivalent indel representation within the equivalence class of representations that yield $H$ from $R$ when applied. Finally, apply your reasoning to the following example and report the left-normalized VCF representation:\n\n- Reference: $R = \\text{GAAAAC}$ (indexed $1$ to $6$).\n- Observed haplotype: $H = \\text{GAAAAAC}$ (an insertion of $\\text{A}$ within the $\\text{AAAA}$ run).\n- A laboratory reports the call as position $p = 4$ with $\\text{REF} = \\text{A}$ and $\\text{ALT} = \\text{AA}$ (i.e., an insertion of $\\text{A}$ after $R[4]$ using the VCF left-anchor rule).\n\nWhich option correctly derives, from DP alignment principles, why left-normalization is required to obtain a unique canonical representation for EHR integration, and also correctly specifies an algorithm that computes the leftmost equivalent representation for indels in a homopolymer, producing the correct normalized VCF for the example?\n\nA. In a homopolymer, any single-base gap placed anywhere in the run yields the same DP score under linear or affine gap penalties because the number of matches and the number of gap events are invariant under lateral shifts within identical characters. Therefore, multiple optimal alignments exist and the same biological indel has multiple equally optimal placements. To ensure a unique key for EHR and CDS, define variant equivalence as “produces the same haplotype $H$ from $R$” and choose the canonical representative by left-normalization: first make the allele minimal by trimming common suffixes and prefixes (preserving at least one base in $\\text{REF}$), then left-shift across the repeat by repeatedly testing the base immediately to the left. For a deletion of $D$ at position $p$, while $p > 1$ and $R[p-1] = R[p+|D|-1]$, decrement $p$ and slide the deleted window one base left; for an insertion with minimal $\\text{REF} = b$ and $\\text{ALT} = bI$, while $p > 1$ and $R[p-1] = I[|I|]$, set $p \\leftarrow p-1$, rotate $I$ by moving its last character to the front, and update $b \\leftarrow R[p]$. In a homopolymer of $\\text{A}$, this shifts to the left boundary of the run. Applying this to $R = \\text{GAAAAC}$ and the reported insertion at $p = 4$ of $I = \\text{A}$ gives successive shifts to $p = 3$, then $p = 2$, stopping because $R[1] = \\text{G} \\ne \\text{A}$. The left-normalized VCF is position $2$ with $\\text{REF} = \\text{A}$ and $\\text{ALT} = \\text{AA}$.\n\nB. Because reads are synthesized and aligned in the $3'$ to $5'$ direction, DP prefers rightmost placements of gaps in homopolymers; thus right-normalization yields a unique representation. The algorithm is: after minimal allele trimming, for a deletion of $D$ at position $p$, while $p+|D| \\le |R|$ and $R[p] = R[p+|D|]$, increment $p$; for an insertion with $\\text{ALT} = bI$, while $p+1 \\le |R|$ and $R[p+1] = I[1]$, increment $p$. Applying this to the example shifts to $p = 5$, giving $\\text{REF} = \\text{A}$ and $\\text{ALT} = \\text{AA}$ at position $5$.\n\nC. With affine gap penalties, DP breaks ties and produces a unique alignment even in homopolymers, so left-normalization is unnecessary. The correct procedure for EHR integration is to trust the aligner’s reported position without further adjustment. For the example, the original report at $p = 4$ with $\\text{REF} = \\text{A}$ and $\\text{ALT} = \\text{AA}$ is already canonical.\n\nD. Left-normalization is needed, but it should be defined by choosing the lexicographically smallest $(\\text{REF}, \\text{ALT})$ string pair over all equivalent representations, regardless of genomic position. The algorithm is to enumerate all possible placements within the homopolymer, construct every possible $(\\text{REF}, \\text{ALT})$, and pick the lexicographically smallest pair. For the example, $(\\text{REF}, \\text{ALT}) = (\\text{AA}, \\text{AAA})$ at position $p = 3$ is lexicographically smallest, so that is the normalized call.\n\nE. Multiple optimal DP alignments in homopolymers require a canonicalization rule. Left-normalization is achieved by first trimming the common prefix only (ignoring the suffix), then for an insertion with minimal $\\text{REF} = b$ and $\\text{ALT} = bI$, while $R[p-1] = R[p]$, decrement $p$; for a deletion of $D$, while $R[p-1] = R[p]$, decrement $p$. Applying this to the example yields position $2$ with $\\text{REF} = \\text{A}$ and $\\text{ALT} = \\text{AA}$.",
            "solution": "The user wants a critical validation of the problem statement, followed by a first-principles derivation of the solution and evaluation of all options.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   **Context:** Integration of discrete genomic variants into Electronic Health Record (EHR) systems for Clinical Decision Support (CDS).\n-   **Problem:** Short insertions and deletions (indels) within homopolymer runs or tandem repeats can have multiple equally scoring alignments, leading to non-unique representations for the same biological variant.\n-   **Reference Sequence:** $R = \\text{GAAAAC}$ (indexed $1$ to $6$).\n-   **Observed Haplotype:** $H = \\text{GAAAAAC}$. This corresponds to an insertion of one nucleotide, 'A', into the 'AAAA' homopolymer run of $R$.\n-   **Alignment Model:** Pairwise sequence alignment via Dynamic Programming (DP) with a match reward $+m$, mismatch penalty $-u$, and linear or affine gap penalties (gap opening $-g$, gap extension $-e$, with $e \\ge 0$).\n-   **Canonicalization Rule:** VCF (Variant Call Format) minimal allele rule, described as \"trim common prefix and suffix while keeping the reference allele length at least $1$\". followed by a left-normalization step.\n-   **Example Laboratory Call:** Position $p = 4$, $\\text{REF} = \\text{A}$, $\\text{ALT} = \\text{AA}$. This represents an insertion of 'A' where the reference base at position $4$ ('A') is replaced by 'AA'.\n-   **Task:**\n    1.  Explain, from DP first principles, why multiple equally optimal alignments exist for such indels.\n    2.  Describe an algorithm to compute the leftmost equivalent indel representation (left-normalization).\n    3.  Apply the reasoning and algorithm to the provided example and report the normalized VCF representation.\n    4.  Select the option that correctly performs these tasks.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientific Grounding:** The problem is scientifically sound and highly relevant to bioinformatics and clinical genomics. The ambiguity of variant representation in repetitive regions is a well-documented challenge. Dynamic programming for sequence alignment, VCF specifications, and variant normalization are all standard concepts in the field.\n-   **Well-Posed:** The problem is well-posed. It provides clear inputs ($R$, $H$, an initial representation) and asks for a specific output (the canonical, left-normalized representation) based on a defined set of principles (DP, VCF rules). A unique solution is expected.\n-   **Objectivity:** The problem is stated in precise, objective, and technical language.\n-   **Consistency Check:** The initial laboratory call ($p=4, \\text{REF}=\\text{A}, \\text{ALT}=\\text{AA}$) correctly generates the observed haplotype $H$ from the reference $R$.\n    -   Reference $R$: $\\text{G}_1\\text{A}_2\\text{A}_3\\text{A}_4\\text{A}_5\\text{C}_6$.\n    -   The call replaces the substring at position $4$ matching $\\text{REF}$ with the $\\text{ALT}$ string.\n    -   Result: $R[1..3] + \\text{ALT} + R[5..6] = (\\text{GAA}) + (\\text{AA}) + (\\text{AC}) = \\text{GAAAAAC}$. This matches $H$.\n-   **Conclusion:** The problem statement is valid. It is scientifically grounded, well-posed, objective, and internally consistent.\n\n**Step 3: Verdict and Action**\n-   **Verdict:** The problem is **valid**.\n-   **Action:** Proceed to solution derivation and option analysis.\n\n### Solution Derivation\n\n**1. Justification for Multiple Optimal Alignments**\n\nThe total score of a pairwise sequence alignment, calculated using a dynamic programming algorithm (such as Needleman-Wunsch or Smith-Waterman), is the sum of scores for each column in the alignment. With a scoring scheme of $+m$ for a match, $-u$ for a mismatch, and a penalty for gaps, the total score $S$ depends on the number of matches ($N_{match}$), mismatches ($N_{mismatch}$), and the number and length of gaps. For an affine gap penalty, the cost of a gap of length $L$ is $-(g + (L-1)e)$. For a linear penalty, the cost is $-L \\cdot g$.\n\nConsider aligning the haplotype $H = \\text{GAAAAAC}$ to the reference $R = \\text{GAAAAC}$. This requires introducing a $1$-base insertion in $H$ (or a $1$-base deletion in $R$). The indel must occur within the homopolymer run of 'A's. Let's examine the possible alignments in that region ($R' = \\text{AAAA}$, $H' = \\text{AAAAA}$).\n\nAlignment 1 (gap at pos. 2 of $R'$):\n$H': \\text{A A A A A}$\n$R': \\text{A - A A A}$\n\nAlignment 2 (gap at pos. 3 of $R'$):\n$H': \\text{A A A A A}$\n$R': \\text{A A - A A}$\n\n... and so on. There are $4$ such possibilities within the original $4$-base homopolymer.\n\nIn each of these alignments, the number of 'A'-'A' matches is constant ($4$ matches), the number of mismatches is constant ($0$), and there is exactly one gap of length $1$. The score for this gap is $-g$ for both affine and linear models (since $L=1$). Because the total score $S = (N_{matches}) \\cdot m - \\text{gap\\_penalty}$ is identical for all these alignments, the DP algorithm will find multiple alignment paths with the same maximal score. Each path corresponds to a different placement of the indel, leading to ambiguity in its reported position. To resolve this, a canonical representation must be chosen.\n\n**2. Algorithm for Left-Normalization**\n\nThe standard procedure for creating a canonical representation for indels involves two steps:\n1.  **Allele Minimization (or Trimming):** The VCF representation $(p, \\text{REF}, \\text{ALT})$ is made as small as possible. This is achieved by:\n    a. Repeatedly trimming identical characters from the end of both $\\text{REF}$ and $\\text{ALT}$ alleles, as long as both alleles remain non-empty.\n    b. Repeatedly trimming identical characters from the start of both $\\text{REF}$ and $\\text{ALT}$ alleles, incrementing the position $p$ for each character removed.\n    The process must ensure that the resulting $\\text{REF}$ and $\\text{ALT}$ alleles are not empty. If one becomes empty, a base from the reference immediately preceding the variant is prepended to both. The problem simplifies this to keeping $\\text{REF}$ length at least $1$.\n    For the given example, $p=4, \\text{REF}=\\text{A}, \\text{ALT}=\\text{AA}$, the representation is already minimal as $\\text{REF}$ has length $1$.\n\n2.  **Left-Shifting:** After minimizing, the variant is shifted as far to the left as possible while still representing the exact same haplotype change. An indel can be shifted one position to the left if the character at the new position in the reference is identical to the character being shifted over. For an indel within a homopolymer, this means shifting it until the left boundary of the homopolymer is reached.\n\nLet's trace this for the example:\n-   **Initial State:** $p = 4, \\text{REF} = \\text{A}, \\text{ALT} = \\text{AA}$. This is a valid representation of the change. $R = \\text{GAAAAC}$.\n-   **Equivalent Representations:** The same biological event can be represented by shifting the position of the 'A' insertion within the homopolymer run.\n    -   $p = 5, \\text{REF}=\\text{A}, \\text{ALT}=\\text{AA}$ produces $R[1..4] + \\text{ALT} + R[6..] = \\text{GAAA} + \\text{AA} + \\text{C} = \\text{GAAAAAC}$. Valid.\n    -   $p = 3, \\text{REF}=\\text{A}, \\text{ALT}=\\text{AA}$ produces $R[1..2] + \\text{ALT} + R[4..] = \\text{GA} + \\text{AA} + \\text{AAC} = \\text{GAAAAAC}$. Valid.\n    -   $p = 2, \\text{REF}=\\text{A}, \\text{ALT}=\\text{AA}$ produces $R[1..1] + \\text{ALT} + R[3..] = \\text{G} + \\text{AA} + \\text{AAAC} = \\text{GAAAAAC}$. Valid.\n-   **Shifting Algorithm Application:** We want to find the leftmost of these valid representations.\n    -   Start at $p=4$. Can we shift left to $p=3$? Yes, because the region between $p=3$ and $p=4$ is part of the 'A' homopolymer.\n    -   Can we shift left from $p=3$ to $p=2$? Yes, the region between $p=2$ and $p=3$ is also part of the 'A' homopolymer.\n    -   Can we shift left from $p=2$ to $p=1$? No. The reference base at position $1$ is 'G', which is not part of the 'A' homopolymer. The shift must stop at the boundary of the repetitive unit.\n-   **Final Canonical Representation:** The leftmost position is $p=2$. The representation is position $2$, $\\text{REF}=\\text{A}$, $\\text{ALT}=\\text{AA}$.\n\n### Option-by-Option Analysis\n\n**A. In a homopolymer, any single-base gap placed anywhere in the run yields the same DP score... Therefore, multiple optimal alignments exist... To ensure a unique key... choose the canonical representative by left-normalization... first make the allele minimal... then left-shift... For a homopolymer of A, this shifts to the left boundary of the run. Applying this to R = GAAAAC and the reported insertion at p = 4... gives successive shifts to p = 3, then p = 2, stopping because R[1] = G != A. The left-normalized VCF is position 2 with REF = A and ALT = AA.**\n\n-   **Justification:** The rationale for multiple optimal alignments is correct. The description of canonicalization via minimalization and left-shifting is conceptually correct and aligns with standard bioinformatics practice.\n-   **Algorithm:** The described algorithm for insertion, while convoluted and potentially incorrect for complex tandem repeats, simplifies correctly for the specific case of a homopolymer insertion as given in the problem. It correctly identifies the stopping condition as the boundary of the homopolymer run.\n-   **Application:** The application to the example is flawless. It correctly traces the shifts from $p=4$ to $p=3$ to $p=2$ and stops at the correct boundary, yielding the correct final normalized VCF.\n-   **Verdict:** **Correct**. This option provides a correct explanation, a functionally correct algorithm for the specified problem class, and the correct result.\n\n**B. Because reads are synthesized and aligned in the $3'$ to $5'$ direction, DP prefers rightmost placements of gaps in homopolymers; thus right-normalization yields a unique representation...**\n\n-   **Justification:** The premises are incorrect. DNA/RNA synthesis is $5' \\to 3'$. Standard DP alignment algorithms do not have an intrinsic preference for rightmost vs. leftmost gap placement; any consistent behavior is an implementation artifact unless explicitly coded. Right-normalization is a possible convention, but left-normalization is the community standard.\n-   **Verdict:** **Incorrect**. The reasoning is based on false scientific and computational premises.\n\n**C. With affine gap penalties, DP breaks ties and produces a unique alignment even in homopolymers, so left-normalization is unnecessary...**\n\n-   **Justification:** This statement is fundamentally false. As demonstrated in the derivation, the alignment score is invariant to gap placement within a homopolymer regardless of whether the gap penalty is linear or affine. Affine penalties change the cost of longer gaps relative to shorter ones but do not resolve positional ambiguity for a gap of a given length. Left-normalization is therefore necessary.\n-   **Verdict:** **Incorrect**. The core premise is false.\n\n**D. Left-normalization is needed, but it should be defined by choosing the lexicographically smallest $(\\text{REF}, \\text{ALT})$ string pair...**\n\n-   **Justification:** This proposes a non-standard and arbitrary normalization rule. The standard is positional (leftmost), not lexicographical. Furthermore, its application to the example suggests a result, `(AA, AAA)`, which is not a minimal representation (it can be trimmed to `(A, AA)`) and is not lexicographically smaller than `(A, AA)`.\n-   **Verdict:** **Incorrect**. The proposed normalization rule is not standard and is misapplied.\n\n**E. Multiple optimal DP alignments in homopolymers require a canonicalization rule. Left-normalization is achieved by first trimming the common prefix only (ignoring the suffix), then for an insertion... while $R[p-1] = R[p]$, decrement $p$...**\n\n-   **Justification:** The description of the allele minimalization step (\"trimming the common prefix only\") is incorrect; standard methods trim both prefix and suffix. The shifting algorithm `while R[p-1] = R[p], decrement p` is an oversimplification. While it happens to yield the correct position for this simple insertion, it does not describe how to update the $\\text{REF}$ and $\\text{ALT}$ alleles, which is necessary for more complex cases (e.g., deletions) and would lead to incorrect haplotypes if the alleles were not updated.\n-   **Verdict:** **Incorrect**. The description of the algorithm is incomplete and based on a flawed simplification of the VCF minimalization rule.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Genomic medicine relies on a shared coordinate system, the reference genome, but this reference is periodically updated (e.g., from GRCh37 to GRCh38). This creates a significant interoperability challenge, as variant data generated on an older build must be accurately translated to the current one for use in an up-to-date EHR system. This practice  demystifies the 'liftover' process by guiding you to implement a core transformation algorithm. By engaging with the logic of mapping coordinates between assemblies, you will gain a practical understanding of how this essential bioinformatic tool works and appreciate the complexities involved in its successful application, including the reasons for mapping failures.",
            "id": "4336648",
            "problem": "Electronic Health Record (EHR) systems increasingly store clinical variants in a canonical reference genome build to drive interoperable Clinical Decision Support. When clinical variants are generated against Genome Reference Consortium human build 37 (GRCh37) but the EHR operates on Genome Reference Consortium human build 38 (GRCh38), a coordinate transformation (\"liftover\") is required. In a simplified, algorithmically precise model consistent with well-tested genome assembly alignment principles, a liftover is defined by a set of alignment \"chains,\" each composed of one or more perfectly matching blocks between assemblies. Each block specifies a contiguous interval on the source assembly and its corresponding contiguous interval on the target assembly, possibly on the reverse complement strand. The fundamental facts and definitions are:\n- A chain consists of a source chromosome integer $c_s$, a target chromosome integer $c_t$, a strand $\\sigma \\in \\{+1,-1\\}$, and a finite list of alignment blocks $B = \\{(s_i, t_i, \\ell_i)\\}_{i=1}^n$. Each block $(s_i, t_i, \\ell_i)$ maps the half-open interval $[s_i, s_i + \\ell_i)$ on the source to the interval $[t_i, t_i + \\ell_i)$ on the target. All coordinates are $0$-based integers measured in base pairs.\n- A single-nucleotide variant at source coordinate $x$ on chromosome $c$ is said to be covered by a block $(s_i, t_i, \\ell_i)$ if and only if $s_i \\le x < s_i + \\ell_i$ and $c = c_s$.\n- If covered and $\\sigma = +1$, the mapped target coordinate is $y = t_i + (x - s_i)$ on chromosome $c_t$.\n- If covered and $\\sigma = -1$, the mapped target coordinate is $y = t_i + \\left(\\ell_i - 1 - (x - s_i)\\right)$ on chromosome $c_t$.\n- A variant has a unique liftover if and only if exactly one distinct target tuple $(c_t, y)$ arises from all blocks across all chains. If no blocks cover $x$, the mapping is missing. If more than one distinct target tuple arises, the mapping is ambiguous.\n\nYour task is to implement a program that, for each provided test case:\n- Computes the GRCh38 target coordinates $(c_t, y)$ for all GRCh37 variants by applying the definitions above to the provided chain set.\n- Classifies each variant as uniquely lifted, missing, or ambiguous.\n- Returns, for each test case, the fraction (as a decimal) of variants that fail liftover due to ambiguous or missing mappings, defined as $f = \\dfrac{N_{\\text{fail}}}{N_{\\text{total}}}$, where $N_{\\text{fail}}$ is the count of variants with missing or ambiguous mappings and $N_{\\text{total}}$ is the total number of variants in the test case.\n\nAssumptions and conventions:\n- All chromosomes are represented by positive integers (for example, $1$ for chromosome 1).\n- All coordinates are $0$-based integers counted in base pairs.\n- Input strand $\\sigma$ uses $+1$ for forward and $-1$ for reverse orientation.\n- A variant is a single base at coordinate $x$.\n- The mapping rule uses half-open intervals $[s_i, s_i + \\ell_i)$; a coordinate $x = s_i + \\ell_i$ is not covered by the block.\n\nTest suite to implement inside the program:\n- Test case $1$ (happy path, all unique):\n  - Chains:\n    - One chain with $(c_s = 1, c_t = 1, \\sigma = +1, B = [(10, 100, 10)])$.\n  - Variants as pairs $(c, x)$: $[(1, 10), (1, 12), (1, 15), (1, 18), (1, 19)]$.\n  - Expected failure fraction: $0.0$.\n- Test case $2$ (boundary and missing coverage):\n  - Chains:\n    - One chain with $(c_s = 1, c_t = 1, \\sigma = +1, B = [(10, 100, 10)])$.\n  - Variants: $[(1, 9), (1, 10), (1, 19), (1, 20)]$. Coordinates $x = 9$ and $x = 20$ are not covered because the block is $[10, 20)$.\n  - Expected failure fraction: $0.5$.\n- Test case $3$ (ambiguity and reverse strand):\n  - Chains:\n    - Chain A: $(c_s = 2, c_t = 2, \\sigma = +1, B = [(150, 3000, 30)])$.\n    - Chain B: $(c_s = 2, c_t = 2, \\sigma = +1, B = [(160, 4000, 30)])$.\n    - Chain C: $(c_s = 2, c_t = 2, \\sigma = -1, B = [(200, 5000, 10)])$.\n  - Variants: $[(2, 155), (2, 165), (2, 175), (2, 205)]$. Variants at $x = 165$ and $x = 175$ are covered by both Chain A and Chain B with different targets and are therefore ambiguous; $x = 155$ maps uniquely via Chain A; $x = 205$ maps uniquely via Chain C (reverse orientation).\n  - Expected failure fraction: $0.5$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain, in order, the failure fractions $f$ for test cases $1$, $2$, and $3$, each formatted to exactly $6$ decimal places (for example, $[0.000000,0.500000,0.500000]$).\n- Express each fraction as a decimal number, not a percentage.\n\nThe problem is fully self-contained; do not read from files or request user input. Implement the chains and variants exactly as specified above and compute the required failure fractions using the definitions provided.",
            "solution": "The problem statement presents a well-defined computational task based on a simplified but scientifically sound model of genomic coordinate transformation, a process known as \"liftover\" in bioinformatics. The problem is validated as sound, complete, and devoid of ambiguity. It is grounded in established principles of genome assembly alignment, using concepts such as alignment chains, matching blocks, chromosomal coordinates, and strand orientation. The definitions for coordinate coverage and mapping for both forward ($\\sigma = +1$) and reverse ($\\sigma = -1$) strands are precise and algorithmically formalizable. The criteria for classifying a mapping as unique, missing, or ambiguous are mutually exclusive and exhaustive. Therefore, the problem is valid and a definitive solution can be engineered.\n\nThe solution will be implemented by iterating through each variant provided in a given test case. For each variant, we must determine all possible target mappings by evaluating it against every block in every provided alignment chain. A set data structure is ideally suited to collect these mappings, as it automatically stores only the unique target tuples $(c_t, y)$.\n\nThe process for a single variant $(c, x)$ is as follows:\n1. Initialize an empty set, `mappings`, to store the distinct target coordinates found.\n2. Iterate through each chain defined for the test case. A chain is described by a source chromosome $c_s$, a target chromosome $c_t$, a strand orientation $\\sigma$, and a list of alignment blocks $B$.\n3. If the variant's chromosome $c$ does not match the chain's source chromosome $c_s$, this entire chain is irrelevant, and we proceed to the next chain.\n4. If the chromosomes match ($c = c_s$), iterate through each block $(s_i, t_i, \\ell_i)$ in the chain's list $B$.\n5. For each block, check if the variant's source coordinate $x$ is covered. According to the stated half-open interval rule, this condition is $s_i \\le x < s_i + \\ell_i$.\n6. If the coordinate $x$ is covered by the block, a target coordinate $y$ is calculated based on the chain's strand orientation $\\sigma$:\n   - If $\\sigma = +1$ (forward strand), the positional offset is preserved. The target coordinate is $y = t_i + (x - s_i)$.\n   - If $\\sigma = -1$ (reverse strand), the positional offset is inverted within the block. The target coordinate is $y = t_i + (\\ell_i - 1 - (x - s_i))$.\n   The resulting target mapping is the tuple $(c_t, y)$. This tuple is added to the `mappings` set.\n7. After checking all blocks of all chains, the size of the `mappings` set determines the liftover status for the variant $(c, x)$:\n   - If the set is empty (size is $0$), no block covered the variant's coordinate. The mapping is **missing**.\n   - If the set contains exactly one element (size is $1$), the mapping is **unique**.\n   - If the set contains more than one element (size is $> 1$), multiple, conflicting mappings were found. The mapping is **ambiguous**.\n\nA variant is considered to have failed the liftover process if its mapping is either missing or ambiguous. For each test case, we will count the total number of variants, $N_{\\text{total}}$, and the number of failed variants, $N_{\\text{fail}}$. The failure fraction is then computed as $f = \\dfrac{N_{\\text{fail}}}{N_{\\text{total}}}$. This procedure will be applied to all three test cases, and the resulting fractions will be formatted and presented as required.\n\nLet's trace Test Case 3's variant at $x=205$ on chromosome $c=2$ with the reverse strand mapping rule. The relevant chain is Chain C: $(c_s = 2, c_t = 2, \\sigma = -1, B = [(200, 5000, 10)])$. The block is $(s_i, t_i, \\ell_i) = (200, 5000, 10)$.\nThe source coordinate is $x=205$.\nThe variant is on the correct chromosome ($2=c_s$) and its coordinate is within the block's source interval $[200, 200+10) = [200, 210)$, since $200 \\le 205 < 210$.\nThe mapping is on the reverse strand ($\\sigma = -1$). The target coordinate $y$ is calculated as:\n$$ y = t_i + (\\ell_i - 1 - (x - s_i)) $$\n$$ y = 5000 + (10 - 1 - (205 - 200)) $$\n$$ y = 5000 + (9 - 5) $$\n$$ y = 5004 $$\nSince this is the only block covering this coordinate, the unique mapping is to $(c_t, y) = (2, 5004)$. This confirms the logic for reverse strand mapping is correctly applied. The algorithmic design is thereby verified against the problem's specifications.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats # Scipy is specified, importing a submodule for completeness.\n\ndef solve():\n    \"\"\"\n    Solves the liftover problem for the defined test cases.\n    \"\"\"\n\n    test_cases = [\n        # Test case 1 (happy path, all unique)\n        {\n            \"chains\": [\n                {\n                    \"cs\": 1, \"ct\": 1, \"sigma\": 1,\n                    \"blocks\": [(10, 100, 10)]\n                }\n            ],\n            \"variants\": [(1, 10), (1, 12), (1, 15), (1, 18), (1, 19)]\n        },\n        # Test case 2 (boundary and missing coverage)\n        {\n            \"chains\": [\n                {\n                    \"cs\": 1, \"ct\": 1, \"sigma\": 1,\n                    \"blocks\": [(10, 100, 10)]\n                }\n            ],\n            \"variants\": [(1, 9), (1, 10), (1, 19), (1, 20)]\n        },\n        # Test case 3 (ambiguity and reverse strand)\n        {\n            \"chains\": [\n                {  # Chain A\n                    \"cs\": 2, \"ct\": 2, \"sigma\": 1,\n                    \"blocks\": [(150, 3000, 30)]\n                },\n                {  # Chain B\n                    \"cs\": 2, \"ct\": 2, \"sigma\": 1,\n                    \"blocks\": [(160, 4000, 30)]\n                },\n                {  # Chain C\n                    \"cs\": 2, \"ct\": 2, \"sigma\": -1,\n                    \"blocks\": [(200, 5000, 10)]\n                }\n            ],\n            \"variants\": [(2, 155), (2, 165), (2, 175), (2, 205)]\n        }\n    ]\n\n    failure_fractions = []\n\n    for case in test_cases:\n        chains = case[\"chains\"]\n        variants = case[\"variants\"]\n        n_total = len(variants)\n        n_fail = 0\n\n        for c_var, x_var in variants:\n            found_mappings = set()\n\n            for chain in chains:\n                cs = chain[\"cs\"]\n                ct = chain[\"ct\"]\n                sigma = chain[\"sigma\"]\n                \n                # Skip chain if it's for a different source chromosome\n                if c_var != cs:\n                    continue\n\n                for s_i, t_i, l_i in chain[\"blocks\"]:\n                    # Check if the variant is covered by the block's source interval [s_i, s_i + l_i)\n                    if s_i <= x_var < s_i + l_i:\n                        y = 0\n                        # Forward strand mapping\n                        if sigma == 1:\n                            y = t_i + (x_var - s_i)\n                        # Reverse strand mapping\n                        elif sigma == -1:\n                            y = t_i + (l_i - 1 - (x_var - s_i))\n                        \n                        found_mappings.add((ct, y))\n            \n            # Classify the liftover result\n            # A failure is defined as a missing (0 mappings) or ambiguous (>1 mapping) result.\n            if len(found_mappings) != 1:\n                n_fail += 1\n\n        if n_total > 0:\n            fraction = n_fail / n_total\n        else:\n            fraction = 0.0\n        \n        failure_fractions.append(f\"{fraction:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(failure_fractions)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Once a variant is correctly represented and mapped, the critical next step is to interpret its clinical significance. This is rarely straightforward, as evidence may be spread across multiple, specialized databases (e.g., ClinVar for pathogenicity, gnomAD for population frequency) and may even be conflicting. This final practice  challenges you to move from data handling to data interpretation by designing a quantitative evidence-merging engine. You will derive a model from Bayesian first principles to synthesize signals from different sources, thoughtfully weighing them by credibility and recency to compute a unified posterior probability of clinical actionability.",
            "id": "4336602",
            "problem": "You are tasked with designing and implementing an evidence-merging algorithm for Electronic Health Record (EHR) integration of genomic data that synthesizes source-specific signals for variant actionability from three widely used resources: Genome Aggregation Database (gnomAD), ClinVar, and the Pharmacogenomics Knowledgebase (PharmGKB). The merger must compute a single posterior probability for each variant representing the probability that the variant should be surfaced as clinically actionable in the EHR. The merger must explicitly account for two dimensions of source quality: credibility and recency.\n\nUse the following fundamental base to derive your approach:\n- Probability theory and Bayes theorem: for a hypothesis $H$ and data $D$, $p(H \\mid D) = \\frac{p(D \\mid H) p(H)}{p(D)}$.\n- Odds and likelihood ratio algebra: the posterior odds $O(H \\mid D)$ equal the prior odds $O(H)$ multiplied by a data-driven likelihood ratio for $D$, under conditional independence assumptions across sources.\n- Exponential time decay as a model of recency: older evidence is discounted with an exponential decay factor.\n\nYour design goal is to derive a principled computation that:\n- Converts each source’s categorical or numeric evidence into a source-specific likelihood ratio $L_s$ about clinical actionability.\n- Applies a credibility weight $w_s$ and a recency weight via an exponential decay $d_s(t_s)$ to modulate each source’s contribution.\n- Combines the modulated evidence from all sources into a single posterior probability $p \\in [0,1]$ for each variant, based on a neutral prior.\n\nDo not use any shortcut formula not derived from the principles above. Implement the following well-tested mappings and decay model, and then derive the combined posterior computation from first principles:\n- ClinVar categorical evidence to likelihood ratio $L_{\\mathrm{ClinVar}}$:\n  - “pathogenic” $\\rightarrow L = 10^{3}$,\n  - “likely\\_pathogenic” $\\rightarrow L = 10^{2}$,\n  - “variant\\_of\\_unknown\\_significance” (VUS) $\\rightarrow L = 1$,\n  - “likely\\_benign” $\\rightarrow L = 10^{-2}$,\n  - “benign” $\\rightarrow L = 10^{-3}$.\n- gnomAD allele frequency $f$ to likelihood ratio $L_{\\mathrm{gnomAD}}(f)$:\n  - Use $L_{\\mathrm{gnomAD}}(f) = \\left(\\frac{f_{\\mathrm{ref}} + \\epsilon}{f + \\epsilon}\\right)^{\\alpha}$, clipped to $[10^{-3}, 10^{3}]$, with $f_{\\mathrm{ref}} = 10^{-3}$, $\\epsilon = 10^{-6}$, and $\\alpha = 1$. If $f$ is missing, use $L_{\\mathrm{gnomAD}} = 1$.\n- PharmGKB categorical evidence to likelihood ratio $L_{\\mathrm{PharmGKB}}$:\n  - “actionable\\_high” $\\rightarrow L = 10^{2}$,\n  - “actionable\\_moderate” $\\rightarrow L = 10^{1}$,\n  - “inconclusive” $\\rightarrow L = 1$,\n  - “no\\_evidence” $\\rightarrow L = 1$.\n- Recency decay per source $s \\in \\{\\mathrm{ClinVar}, \\mathrm{gnomAD}, \\mathrm{PharmGKB}\\}$ with age $t_s$ in years:\n  - $d_s(t_s) = e^{-\\lambda_s t_s}$, with $\\lambda_{\\mathrm{ClinVar}} = 0.05$, $\\lambda_{\\mathrm{gnomAD}} = 0.02$, $\\lambda_{\\mathrm{PharmGKB}} = 0.03$.\n- Credibility weights $w_s \\in [0,1]$ scale each source’s contribution. If evidence is missing from a source, treat it as neutral with $L_s = 1$ and $d_s = 1$.\n\nAssume a neutral prior odds $O_0 = 1$ (equivalently, prior probability $p_0 = 0.5$). Under a conditional independence assumption across sources given the hypothesis, derive the merger computation from these bases to produce the posterior probability for each variant. You must show, in your solution, how you derive the combination rule from Bayes theorem and the above mappings.\n\nYour program must implement the computation for the following test suite of five variants, each specified by source evidence, age since last update in years, and source credibility weights. The variant parameters are:\n\n- Variant A (conflict case):\n  - ClinVar: “pathogenic”, $t_{\\mathrm{ClinVar}} = 2$.\n  - gnomAD: allele frequency $f = 0.02$, $t_{\\mathrm{gnomAD}} = 1$.\n  - PharmGKB: “inconclusive”, $t_{\\mathrm{PharmGKB}} = 3$.\n  - Credibility weights: $w_{\\mathrm{ClinVar}} = 0.9$, $w_{\\mathrm{gnomAD}} = 0.8$, $w_{\\mathrm{PharmGKB}} = 0.7$.\n- Variant B (agreement on benign):\n  - ClinVar: “benign”, $t_{\\mathrm{ClinVar}} = 1$.\n  - gnomAD: allele frequency $f = 0.05$, $t_{\\mathrm{gnomAD}} = 0.5$.\n  - PharmGKB: “no\\_evidence”, $t_{\\mathrm{PharmGKB}} = 1$.\n  - Credibility weights: $w_{\\mathrm{ClinVar}} = 0.9$, $w_{\\mathrm{gnomAD}} = 0.8$, $w_{\\mathrm{PharmGKB}} = 0.7$.\n- Variant C (missing ClinVar, rare frequency, moderate pharmacogenomic actionability):\n  - ClinVar: missing.\n  - gnomAD: allele frequency $f = 0.0005$, $t_{\\mathrm{gnomAD}} = 4$.\n  - PharmGKB: “actionable\\_moderate”, $t_{\\mathrm{PharmGKB}} = 0.5$.\n  - Credibility weights: $w_{\\mathrm{ClinVar}} = 0.9$, $w_{\\mathrm{gnomAD}} = 0.8$, $w_{\\mathrm{PharmGKB}} = 0.7$.\n- Variant D (old pharmacogenomic evidence vs benign signals):\n  - ClinVar: “likely\\_benign”, $t_{\\mathrm{ClinVar}} = 8$.\n  - gnomAD: allele frequency $f = 0.01$, $t_{\\mathrm{gnomAD}} = 5$.\n  - PharmGKB: “actionable\\_high”, $t_{\\mathrm{PharmGKB}} = 12$.\n  - Credibility weights: $w_{\\mathrm{ClinVar}} = 0.9$, $w_{\\mathrm{gnomAD}} = 0.8$, $w_{\\mathrm{PharmGKB}} = 0.9$.\n- Variant E (zero credibility weight for gnomAD):\n  - ClinVar: “likely\\_pathogenic”, $t_{\\mathrm{ClinVar}} = 0.2$.\n  - gnomAD: allele frequency $f = 0.2$, $t_{\\mathrm{gnomAD}} = 2$.\n  - PharmGKB: “actionable\\_high”, $t_{\\mathrm{PharmGKB}} = 1$.\n  - Credibility weights: $w_{\\mathrm{ClinVar}} = 0.9$, $w_{\\mathrm{gnomAD}} = 0$, $w_{\\mathrm{PharmGKB}} = 0.7$.\n\nImplementation requirements:\n- Your program must not perform any network operations; simulate “queries” by hard-coding the above evidence parameters and mappings.\n- For each variant, compute the posterior probability $p \\in [0,1]$ of clinical actionability derived from the principles above.\n- Express each final probability as a decimal rounded to six decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., “[result1,result2,result3]”).\n\nDesign for coverage:\n- The test suite includes a general “happy path” conflict case, agreement case, missing-source edge case, heavy time decay edge case, and a zero-credibility boundary case.\n\nYour task: implement the merger, compute the posterior probabilities for the five variants, and output them exactly in the specified format.",
            "solution": "The problem as stated is valid. It is scientifically grounded in probability theory, well-posed with a complete set of parameters and definitions, and objective in its formulation. It requires the derivation of an evidence-merging algorithm from first principles and its application to a set of well-defined test cases. The problem does not violate any of the specified invalidity criteria.\n\nThe objective is to compute the posterior probability, $p(H|D)$, that a genetic variant is clinically actionable, where $H$ is the hypothesis \"the variant is clinically actionable\" and $D$ represents the total evidence from three sources: ClinVar ($D_C$), gnomAD ($D_G$), and PharmGKB ($D_P$).\n\nWe begin with Bayes' theorem, expressed in terms of odds. The odds of a hypothesis $H$ is defined as $O(H) = \\frac{p(H)}{p(\\neg H)} = \\frac{p(H)}{1 - p(H)}$. The relationship between probability $p$ and odds $O$ is $p = \\frac{O}{1+O}$. Bayes' theorem in odds form is:\n\n$$O(H|D) = O(H) \\times \\frac{p(D|H)}{p(D|\\neg H)}$$\n\nHere, $O(H|D)$ represents the posterior odds after observing data $D$, $O(H)$ is the prior odds, and the term $\\frac{p(D|H)}{p(D|\\neg H)}$ is the likelihood ratio, denoted as $L(D)$.\n\nThe total evidence $D$ is the collection of evidence from the three sources, $D = \\{D_C, D_G, D_P\\}$. The likelihood ratio for the combined evidence is thus $L(D_C, D_G, D_P) = \\frac{p(D_C, D_G, D_P|H)}{p(D_C, D_G, D_P|\\neg H)}$. The problem specifies the assumption of conditional independence of the sources given the hypothesis. This implies:\n\n$$p(D_C, D_G, D_P | H) = p(D_C|H) \\times p(D_G|H) \\times p(D_P|H)$$\n$$p(D_C, D_G, D_P | \\neg H) = p(D_C|\\neg H) \\times p(D_G|\\neg H) \\times p(D_P|\\neg H)$$\n\nSubstituting this into the likelihood ratio expression allows us to decompose the total likelihood ratio into a product of the individual likelihood ratios for each source:\n\n$$L(D_C, D_G, D_P) = \\frac{p(D_C|H)}{p(D_C|\\neg H)} \\times \\frac{p(D_G|H)}{p(D_G|\\neg H)} \\times \\frac{p(D_P|H)}{p(D_P|\\neg H)} = L_C \\times L_G \\times L_P$$\n\nwhere $L_s$ is the likelihood ratio for source $s$. The posterior odds can then be calculated by updating the prior odds with the product of the individual source likelihood ratios:\n\n$$O_{\\text{post}} = O_{\\text{prior}} \\times L_C \\times L_G \\times L_P$$\n\nThe problem requires that each source's contribution be modulated by a credibility weight $w_s \\in [0,1]$ and a recency decay factor $d_s(t_s) \\in [0,1]$. A principled way to apply these modulators is to scale the \"weight of evidence\" provided by each source. The weight of evidence is often defined on a logarithmic scale. The update rule in log-odds form is:\n\n$$\\log(O_{\\text{post}}) = \\log(O_{\\text{prior}}) + \\log(L_C) + \\log(L_G) + \\log(L_P)$$\n\nThe combined weight for a source $s$, which is the product of its credibility and recency, $W_s = w_s d_s$, can be used to scale the log-likelihood ratio from that source. This yields a modulated update:\n\n$$\\log(O_{\\text{post}}) = \\log(O_{\\text{prior}}) + (w_C d_C) \\log(L_C) + (w_G d_G) \\log(L_G) + (w_P d_P)$$\n\nExponentiating both sides of the equation brings us back to the odds domain:\n\n$$O_{\\text{post}} = O_{\\text{prior}} \\times e^{(w_C d_C) \\log(L_C)} \\times e^{(w_G d_G) \\log(L_G)} \\times e^{(w_P d_P)}$$\n\nUsing the identity $e^{a \\log(b)} = b^a$, we arrive at the final formula for the posterior odds:\n\n$$O_{\\text{post}} = O_{\\text{prior}} \\times (L_C)^{w_C d_C} \\times (L_G)^{w_G d_G} \\times (L_P)^{w_P d_P}$$\n\nThis can be written more generally as:\n\n$$O_{\\text{post}} = O_{\\text{prior}} \\times \\prod_{s \\in \\{C, G, P\\}} (L_s)^{w_s d_s(t_s)}$$\n\nThis formulation correctly handles the limiting cases. If a weight $w_s=0$, the exponent becomes $0$, and the source's effective likelihood ratio becomes $(L_s)^0=1$, contributing no information. Similarly, as evidence ages, $t_s \\to \\infty$, the recency factor $d_s \\to 0$, and the source's contribution is nullified. If a source provides neutral evidence ($L_s = 1$), it also contributes a factor of $1$ regardless of the weights.\n\nThe final posterior probability $p_{\\text{post}}$ is obtained by converting the posterior odds $O_{\\text{post}}$ back to a probability:\n\n$$p_{\\text{post}} = \\frac{O_{\\text{post}}}{1 + O_{\\text{post}}}$$\n\nThe calculation proceeds as follows for each variant:\n1.  Assume a neutral prior odds, $O_{\\text{prior}} = 1$, corresponding to a prior probability of $p_0=0.5$.\n2.  For each source $s \\in \\{\\mathrm{ClinVar}, \\mathrm{gnomAD}, \\mathrm{PharmGKB}\\}$:\n    a. Determine the raw likelihood ratio $L_s$ based on the provided evidence mappings. For gnomAD, $L_{\\mathrm{gnomAD}}(f) = \\left(\\frac{f_{\\mathrm{ref}} + \\epsilon}{f + \\epsilon}\\right)^{\\alpha}$, with $f_{\\mathrm{ref}} = 10^{-3}$, $\\epsilon = 10^{-6}$, and $\\alpha = 1$. This value is clipped to the range $[10^{-3}, 10^{3}]$. If evidence is missing, $L_s=1$.\n    b. Calculate the recency decay factor $d_s(t_s) = e^{-\\lambda_s t_s}$ using the given age $t_s$ and decay constant $\\lambda_s$.\n    c. Combine the raw likelihood ratio $L_s$ with the credibility weight $w_s$ and recency decay $d_s(t_s)$ to find the effective likelihood ratio for the source: $L_{s, \\text{eff}} = (L_s)^{w_s d_s(t_s)}$.\n3.  Multiply the effective likelihood ratios from all sources with the prior odds to compute the posterior odds: $O_{\\text{post}} = O_{\\text{prior}} \\times \\prod_{s} L_{s, \\text{eff}}$.\n4.  Convert $O_{\\text{post}}$ to the final posterior probability $p_{\\text{post}}$.\n\nThis procedure will be applied to each of the five test cases specified in the problem statement.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the posterior probability of clinical actionability for a set of genetic variants\n    based on evidence from ClinVar, gnomAD, and PharmGKB, modulated by source credibility\n    and recency.\n    \"\"\"\n\n    # Define evidence-to-likelihood ratio mappings\n    L_CLINVAR_MAP = {\n        \"pathogenic\": 1e3,\n        \"likely_pathogenic\": 1e2,\n        \"variant_of_unknown_significance\": 1.0,\n        \"likely_benign\": 1e-2,\n        \"benign\": 1e-3,\n    }\n\n    L_PHARMGKB_MAP = {\n        \"actionable_high\": 1e2,\n        \"actionable_moderate\": 1e1,\n        \"inconclusive\": 1.0,\n        \"no_evidence\": 1.0,\n    }\n\n    # Define recency decay constants\n    LAMBDA_VALS = {\n        \"ClinVar\": 0.05,\n        \"gnomAD\": 0.02,\n        \"PharmGKB\": 0.03,\n    }\n\n    # Define gnomAD parameters\n    GNOMAD_F_REF = 1e-3\n    GNOMAD_EPSILON = 1e-6\n    GNOMAD_ALPHA = 1.0\n    GNOMAD_L_MIN = 1e-3\n    GNOMAD_L_MAX = 1e3\n\n    # Define credibility weights for the test cases\n    test_suite = [\n        # Variant A\n        {\n            \"evidence\": {\"ClinVar\": \"pathogenic\", \"gnomAD\": 0.02, \"PharmGKB\": \"inconclusive\"},\n            \"ages\": {\"ClinVar\": 2, \"gnomAD\": 1, \"PharmGKB\": 3},\n            \"weights\": {\"ClinVar\": 0.9, \"gnomAD\": 0.8, \"PharmGKB\": 0.7},\n        },\n        # Variant B\n        {\n            \"evidence\": {\"ClinVar\": \"benign\", \"gnomAD\": 0.05, \"PharmGKB\": \"no_evidence\"},\n            \"ages\": {\"ClinVar\": 1, \"gnomAD\": 0.5, \"PharmGKB\": 1},\n            \"weights\": {\"ClinVar\": 0.9, \"gnomAD\": 0.8, \"PharmGKB\": 0.7},\n        },\n        # Variant C\n        {\n            \"evidence\": {\"ClinVar\": None, \"gnomAD\": 0.0005, \"PharmGKB\": \"actionable_moderate\"},\n            \"ages\": {\"ClinVar\": None, \"gnomAD\": 4, \"PharmGKB\": 0.5},\n            \"weights\": {\"ClinVar\": 0.9, \"gnomAD\": 0.8, \"PharmGKB\": 0.7},\n        },\n        # Variant D\n        {\n            \"evidence\": {\"ClinVar\": \"likely_benign\", \"gnomAD\": 0.01, \"PharmGKB\": \"actionable_high\"},\n            \"ages\": {\"ClinVar\": 8, \"gnomAD\": 5, \"PharmGKB\": 12},\n            \"weights\": {\"ClinVar\": 0.9, \"gnomAD\": 0.8, \"PharmGKB\": 0.9},\n        },\n        # Variant E\n        {\n            \"evidence\": {\"ClinVar\": \"likely_pathogenic\", \"gnomAD\": 0.2, \"PharmGKB\": \"actionable_high\"},\n            \"ages\": {\"ClinVar\": 0.2, \"gnomAD\": 2, \"PharmGKB\": 1},\n            \"weights\": {\"ClinVar\": 0.9, \"gnomAD\": 0, \"PharmGKB\": 0.7},\n        },\n    ]\n\n    results = []\n    \n    # Neutral prior odds\n    O_prior = 1.0\n\n    for variant_data in test_suite:\n        # 1. Calculate ClinVar contribution\n        clinvar_evidence = variant_data[\"evidence\"][\"ClinVar\"]\n        if clinvar_evidence is None:\n            L_clinvar = 1.0\n            d_clinvar = 1.0 # Per problem spec for missing data\n        else:\n            L_clinvar = L_CLINVAR_MAP[clinvar_evidence]\n            t_clinvar = variant_data[\"ages\"][\"ClinVar\"]\n            d_clinvar = np.exp(-LAMBDA_VALS[\"ClinVar\"] * t_clinvar)\n        \n        w_clinvar = variant_data[\"weights\"][\"ClinVar\"]\n        L_eff_clinvar = np.power(L_clinvar, w_clinvar * d_clinvar)\n\n        # 2. Calculate gnomAD contribution\n        gnomad_f = variant_data[\"evidence\"][\"gnomAD\"]\n        if gnomad_f is None:\n            L_gnomad = 1.0\n            d_gnomad = 1.0\n        else:\n            numerator = GNOMAD_F_REF + GNOMAD_EPSILON\n            denominator = gnomad_f + GNOMAD_EPSILON\n            L_gnomad_raw = np.power(numerator / denominator, GNOMAD_ALPHA)\n            L_gnomad = np.clip(L_gnomad_raw, GNOMAD_L_MIN, GNOMAD_L_MAX)\n            t_gnomad = variant_data[\"ages\"][\"gnomAD\"]\n            d_gnomad = np.exp(-LAMBDA_VALS[\"gnomAD\"] * t_gnomad)\n\n        w_gnomad = variant_data[\"weights\"][\"gnomAD\"]\n        L_eff_gnomad = np.power(L_gnomad, w_gnomad * d_gnomad)\n\n        # 3. Calculate PharmGKB contribution\n        pharmgkb_evidence = variant_data[\"evidence\"][\"PharmGKB\"]\n        if pharmgkb_evidence is None:\n            L_pharmgkb = 1.0\n            d_pharmgkb = 1.0\n        else:\n            L_pharmgkb = L_PHARMGKB_MAP[pharmgkb_evidence]\n            t_pharmgkb = variant_data[\"ages\"][\"PharmGKB\"]\n            d_pharmgkb = np.exp(-LAMBDA_VALS[\"PharmGKB\"] * t_pharmgkb)\n\n        w_pharmgkb = variant_data[\"weights\"][\"PharmGKB\"]\n        L_eff_pharmgkb = np.power(L_pharmgkb, w_pharmgkb * d_pharmgkb)\n        \n        # 4. Combine evidence to get posterior odds\n        O_post = O_prior * L_eff_clinvar * L_eff_gnomad * L_eff_pharmgkb\n        \n        # 5. Convert posterior odds to posterior probability\n        p_post = O_post / (1.0 + O_post)\n        \n        results.append(p_post)\n\n    # Format output as specified\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}