{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of treating Tourette syndrome involves modulating the dopaminergic system. This exercise delves into the fundamental neuropharmacological principles that govern the action of dopamine D2 receptor antagonists, a common class of medication used for tic suppression. By calculating receptor occupancy based on drug concentration and binding affinity, you will gain a quantitative understanding of the therapeutic window, bridging the gap between molecular interactions and clinical outcomes. ",
            "id": "4531186",
            "problem": "A patient with Tourette syndrome (TS) and functionally impairing motor and phonic tics is initiated on a dopamine D2 receptor (D2R) antagonist. At steady state, the unbound plasma concentration is measured at $C=5\\,\\mathrm{nM}$. In vitro radioligand binding assays report the drug’s inhibition constant $K_i=2\\,\\mathrm{nM}$ at the human D2R, and for the purpose of this calculation you may assume that the unbound plasma concentration approximates the receptor-accessible extracellular concentration in the relevant brain regions and that endogenous dopamine is negligible during the measurement interval. Using the law of mass action and the definition of the inhibition constant $K_i$ under competitive binding, derive the expression for the fractional receptor occupancy at equilibrium and compute the expected D2R occupancy for this drug at the given exposure. Round your numerical result to four significant figures and report it as a decimal fraction with no units.\n\nFor clinical context only (no additional calculation is required beyond the occupancy): assume that meaningful tic suppression for a D2R antagonist occurs for fractional occupancy between $0.60$ and $0.80$, with extrapyramidal symptoms (EPS) becoming common above $0.80$.",
            "solution": "The fundamental base here is the law of mass action governing reversible ligand–receptor binding. Consider the equilibrium:\n$$\nR + L \\rightleftharpoons RL,\n$$\nwith dissociation constant\n$$\nK_d = \\frac{[R][L]}{[RL]}.\n$$\nThe total receptor concentration is $[R_{\\mathrm{tot}}]=[R]+[RL]$. The fractional occupancy $O$ is defined as\n$$\nO = \\frac{[RL]}{[R_{\\mathrm{tot}}]}.\n$$\nSolving for $O$ using the definition of $K_d$ and algebraic substitution yields the classic Langmuir isotherm for one-site binding:\n$$\nO = \\frac{[L]}{[L] + K_d}.\n$$\nFor a competitive antagonist measured by radioligand assays, the inhibition constant $K_i$ equals the dissociation constant $K_d$ under the assay conditions after appropriate corrections (e.g., the Cheng–Prusoff relation reduces to $K_i=K_d$ for the case in which the reference radioligand directly measures the same site at equilibrium and endogenous agonist is negligible). Under the stated assumptions (negligible endogenous dopamine and unbound plasma concentration approximating the brain extracellular concentration), we substitute $[L]=C$ and $K_d=K_i$ to obtain\n$$\nO = \\frac{C}{C + K_i}.\n$$\nWith the given values $C=5\\,\\mathrm{nM}$ and $K_i=2\\,\\mathrm{nM}$,\n$$\nO = \\frac{5}{5+2} = \\frac{5}{7}.\n$$\nCompute the exact fraction:\n$$\n\\frac{5}{7} \\approx 0.714285\\ldots\n$$\nRounded to four significant figures as required, the occupancy is\n$$\n0.7143.\n$$\nClinical interpretation relative to the provided window: the predicted occupancy $0.7143$ lies within the assumed therapeutic window $[0.60, 0.80]$ for tic suppression and below the threshold where extrapyramidal symptoms (EPS) commonly emerge (above $0.80$). This supports that, under the simplifying assumptions, the exposure is compatible with the intended therapeutic range.\n\nCaveats for advanced interpretation include the distinction between $K_i$ and in vivo $K_d$ under physiological dopamine competition, potential differences between unbound plasma and brain extracellular free concentrations due to blood–brain barrier transport and nonspecific binding, and receptor reserve or downstream signaling efficacy, any of which can shift the occupancy–response relationship. However, within the stated assumptions, the calculated $O$ is the internally consistent estimate.",
            "answer": "$$\\boxed{0.7143}$$"
        },
        {
            "introduction": "Beyond understanding a treatment's mechanism, it is critical to rigorously evaluate its effectiveness in a clinical setting. This practice focuses on Comprehensive Behavioral Intervention for Tics (CBIT) and introduces a key biostatistical measure, Cohen's $d$, to quantify the magnitude of the intervention's effect. Calculating and interpreting this effect size will equip you with the skills to critically appraise clinical trial data and understand the practical significance of a treatment's impact on tic severity. ",
            "id": "4531192",
            "problem": "A single-arm trial evaluated the efficacy of Comprehensive Behavioral Intervention for Tics (CBIT) in adolescents with Tourette syndrome using the Yale Global Tic Severity Scale (YGTSS) Total Tic Severity Score, which ranges from $0$ to $50$. The standardized mean difference, Cohen’s $d$, is defined as the difference in means divided by a pooled standard deviation and yields a unitless estimate of effect magnitude. Assume that the YGTSS was measured at baseline and immediately post-treatment, and that the pooled standard deviation across the two time points was computed from the sample and provided to you.\n\nYou are given the following summary statistics for the YGTSS Total Tic Severity Score:\n- Baseline mean $= 28.7$,\n- Post-treatment mean $= 21.6$,\n- Pooled standard deviation $= 8.9$.\n\nUsing the definition of Cohen’s $d$ as a standardized mean difference based on the provided pooled standard deviation, compute the effect size for CBIT on the YGTSS Total Tic Severity Score. Round your answer to three significant figures and report it as a unitless number. Then, in your reasoning, interpret the magnitude of this effect size in clinical terms, grounded in the measurement properties of the YGTSS and commonly used thresholds for standardized effect sizes. The final reported answer must be a single real-valued number.",
            "solution": "The problem requires the computation and interpretation of Cohen's $d$ as a measure of effect size for a behavioral intervention in a single-arm trial. The provided data are from a study on Comprehensive Behavioral Intervention for Tics (CBIT) for adolescents with Tourette syndrome, with the outcome measured by the Yale Global Tic Severity Scale (YGTSS).\n\nFirst, the problem statement must be validated.\n\n**Step 1: Extract Givens**\n-   Outcome measure: YGTSS Total Tic Severity Score (range $0$ to $50$).\n-   Baseline mean YGTSS score: $\\mu_{\\text{baseline}} = 28.7$.\n-   Post-treatment mean YGTSS score: $\\mu_{\\text{post}} = 21.6$.\n-   Pooled standard deviation of YGTSS scores: $\\sigma_{\\text{pooled}} = 8.9$.\n-   Definition of effect size: Cohen’s $d$, the standardized mean difference, calculated as the difference in means divided by the pooled standard deviation.\n-   Task: Compute Cohen’s $d$, round to three significant figures, and interpret its magnitude.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, employing standard methodologies and terminology from biostatistics and clinical neurology (CBIT, YGTSS, Cohen's $d$). All necessary data for the calculation are provided, and the values are plausible within the clinical context of Tourette syndrome. The problem is well-posed, unambiguous, and requests a verifiable calculation and a corresponding interpretation based on established conventions.\n\n**Step 3: Verdict and Action**\nThe problem is deemed valid. A complete solution will now be derived.\n\n**Solution Derivation**\nThe problem defines Cohen's $d$ for a repeated-measures design (pre- vs. post-treatment) as the change in mean scores divided by the pooled standard deviation. The formula is:\n$$d = \\frac{\\mu_{\\text{baseline}} - \\mu_{\\text{post}}}{\\sigma_{\\text{pooled}}}$$\nIn this convention, a positive value of $d$ indicates a reduction in tic severity, which signifies a beneficial treatment effect.\n\nThe given values are:\n-   Mean score at baseline, $\\mu_{\\text{baseline}} = 28.7$.\n-   Mean score post-treatment, $\\mu_{\\text{post}} = 21.6$.\n-   Pooled standard deviation, $\\sigma_{\\text{pooled}} = 8.9$.\n\nSubstituting these values into the formula yields:\n$$d = \\frac{28.7 - 21.6}{8.9}$$\n\nFirst, we calculate the difference in means, which represents the average raw improvement in the YGTSS score:\n$$\\Delta\\mu = \\mu_{\\text{baseline}} - \\mu_{\\text{post}} = 28.7 - 21.6 = 7.1$$\n\nNext, this mean difference is divided by the pooled standard deviation to obtain the standardized effect size, Cohen's $d$:\n$$d = \\frac{7.1}{8.9}$$\n$$d \\approx 0.7977528...$$\n\nThe problem requires the answer to be rounded to three significant figures. The first three significant figures are $7$, $9$, and $7$. The fourth significant figure is $7$, which is greater than or equal to $5$, so we round up the third significant figure.\n$$d \\approx 0.798$$\n\n**Interpretation of the Effect Size**\nThe computed effect size is $d \\approx 0.798$. This value is interpreted in both statistical and clinical contexts.\n\nStatistically, according to the widely used conventions established by J. Cohen, an effect size is typically categorized as:\n-   Small: $d \\approx 0.2$\n-   Medium: $d \\approx 0.5$\n-   Large: $d \\ge 0.8$\n\nThe calculated value of $d \\approx 0.798$ is very close to the threshold for a large effect size. This indicates that the reduction in tic severity following the CBIT intervention was substantial relative to the observed variability of the scores in the sample. Specifically, the mean YGTSS score decreased by approximately $0.8$ standard deviations.\n\nClinically, the YGTSS is the gold standard for assessing tic severity, with scores ranging from $0$ to $50$ (higher scores indicate greater severity). The observed mean reduction of $7.1$ points, from a baseline of $28.7$ to a post-treatment score of $21.6$, is considered a clinically meaningful improvement. An effect size of $d \\approx 0.8$ is considered robust in behavioral intervention research. It suggests that CBIT is a highly effective treatment for adolescents with Tourette syndrome, producing a reduction in tic symptoms that is large in magnitude when standardized. An effect size of this magnitude also implies a substantial separation between the pre- and post-treatment score distributions. For a normal distribution, it means the average post-treatment score is at the $21$st percentile of the baseline scores, signifying a notable shift toward improvement for the average participant.",
            "answer": "$$\\boxed{0.798}$$"
        },
        {
            "introduction": "To synthesize our understanding of tic generation and treatment, we can turn to computational modeling, a powerful tool for formalizing and testing neurobiological hypotheses. This advanced exercise guides you through the implementation of a reinforcement learning model that simulates the competition between tic expression and a therapeutic competing response. Engaging with this model provides a hands-on perspective on how habit formation contributes to tics and how an intervention like CBIT can shift action selection to reduce tic probability over time. ",
            "id": "4531139",
            "problem": "Consider a simplified action selection model for tics in Tourette syndrome grounded in reinforcement learning principles and habit formation. The model has two actions: a tic action denoted by $T$ and a competing response action denoted by $C$. Comprehensive Behavioral Intervention for Tics (CBIT) introduces training that biases selection toward the competing response by shaping its expected value through psychoeducation, awareness training, and competing response practice. The goal is to derive, implement, and test a deterministic mean-field simulation of habit strength as a function of repetition and reward prediction error decay, and then quantify how introducing CBIT affects action selection.\n\nFundamental base and core definitions:\n- Reward prediction error (RPE) is given by $\\delta_t(a) = r_t(a) - V_t(a)$, where $r_t(a)$ is the immediate reinforcement and $V_t(a)$ is the learned expected value for action $a$ at time $t$. In dopaminergic learning models, the expected value updates as $V_{t+1}(a) = V_t(a) + \\alpha_t \\delta_t(a)$, where $\\alpha_t$ is a learning rate that may decay with repetition. This follows the Rescorla-Wagner framework and well-tested reinforcement learning facts linking dopaminergic signaling to prediction errors.\n- Habit strength is modeled as a cached bias that increases with repetition of an action and decays slowly over time. The expected habit update is $H_{t+1}(a) = (1 - \\rho) H_t(a) + \\eta \\, \\pi_t(a)$, where $\\rho$ is a forgetting rate, $\\eta$ is a habit formation rate, and $\\pi_t(a)$ is the selection probability of action $a$ at time $t$.\n- Action selection is governed by a softmax policy with inverse temperature $\\beta$, where the action value $Q_t(a)$ combines goal-directed value and habit bias: $Q_t(a) = V_t(a) + w_H H_t(a) + b_a$. The selection probability is $\\pi_t(a) = \\dfrac{\\exp\\left(\\beta Q_t(a)\\right)}{\\sum_{a' \\in \\{T, C\\}} \\exp\\left(\\beta Q_t(a')\\right)}$.\n- The effect of repetition on learning is captured by a decaying learning rate $\\alpha_t = \\alpha_0 e^{-\\lambda t}$ with $\\alpha_0 > 0$ and $\\lambda \\ge 0$, reflecting reduced dopaminergic plasticity with repetition. The relief from performing the tic is modeled as a decaying reinforcement $r_t(T) = r_0^T e^{-\\kappa t}$ with $r_0^T > 0$ and $\\kappa \\ge 0$, representing habituation and decreasing premonitory urge relief. The competing response has reinforcement $r_t(C)$ that is taken as constant $r_C$ during the simulation interval.\n- CBIT is modeled as introducing a constant bias $b_C = \\gamma$ to the competing response action value, reflecting increased expected utility from awareness and competing response practice. Pre-CBIT has $b_C = 0$. CBIT also increases the reinforcement associated with the competing response (for post-CBIT) to a specified $r_C$ value.\n\nDeterministic mean-field simulation protocol:\n- Initialize $V_0(T) = 0$, $V_0(C) = 0$, $H_0(T) = 0$, and $H_0(C) = 0$.\n- For each trial $t$ from $0$ to $N - 1$:\n  1. Compute $\\alpha_t = \\alpha_0 e^{-\\lambda t}$, $r_t(T) = r_0^T e^{-\\kappa t}$, and set $r_t(C) = r_C$.\n  2. Compute $Q_t(T) = V_t(T) + w_H H_t(T)$, and $Q_t(C) = V_t(C) + w_H H_t(C) + b_C$, with $b_C = 0$ pre-CBIT and $b_C = \\gamma$ post-CBIT.\n  3. Compute $\\pi_t(T)$ and $\\pi_t(C)$ via softmax with inverse temperature $\\beta$.\n  4. Update $V_{t+1}(T) = V_t(T) + \\alpha_t \\left[r_t(T) - V_t(T)\\right] \\pi_t(T)$ and $V_{t+1}(C) = V_t(C) + \\alpha_t \\left[r_t(C) - V_t(C)\\right] \\pi_t(C)$ (mean-field approximation).\n  5. Update $H_{t+1}(T) = (1 - \\rho) H_t(T) + \\eta \\pi_t(T)$ and $H_{t+1}(C) = (1 - \\rho) H_t(C) + \\eta \\pi_t(C)$.\n- After $N$ trials, compute final selection probabilities $\\pi_N(T)$ and $\\pi_N(C)$ from the final $Q_N(T)$ and $Q_N(C)$ as above.\n\nTask:\n- Implement the deterministic mean-field simulation twice per test case: pre-CBIT (with $b_C = 0$ and $r_C = r_C^{\\mathrm{pre}}$) and post-CBIT (with $b_C = \\gamma$ and $r_C = r_C^{\\mathrm{post}}$).\n- For each test case, return the single float $\\Delta = \\pi_N^{\\mathrm{post}}(T) - \\pi_N^{\\mathrm{pre}}(T)$, expressed as a decimal (not a percentage), which quantifies the change in tic selection probability due to CBIT. Negative $\\Delta$ indicates CBIT decreases the probability of selecting a tic.\n\nNumerical units and conventions:\n- All quantities are dimensionless scalars expressed as decimals between $0$ and $1$ where applicable. No physical units are involved. Probabilities must be expressed as decimals, not percentages.\n\nTest suite:\nUse the following parameter sets covering a general case, boundary conditions, and edge cases. Each test case defines $(N, \\alpha_0, \\lambda, r_0^T, \\kappa, r_C^{\\mathrm{pre}}, r_C^{\\mathrm{post}}, \\gamma, w_H, \\eta, \\rho, \\beta)$.\n\n1. General happy path:\n   - $N = 50$, $\\alpha_0 = 0.3$, $\\lambda = 0.05$, $r_0^T = 1.0$, $\\kappa = 0.02$, $r_C^{\\mathrm{pre}} = 0.05$, $r_C^{\\mathrm{post}} = 0.4$, $\\gamma = 0.6$, $w_H = 0.5$, $\\eta = 0.1$, $\\rho = 0.01$, $\\beta = 2.0$.\n2. Boundary, no CBIT effect:\n   - $N = 50$, $\\alpha_0 = 0.3$, $\\lambda = 0.05$, $r_0^T = 1.0$, $\\kappa = 0.02$, $r_C^{\\mathrm{pre}} = 0.05$, $r_C^{\\mathrm{post}} = 0.05$, $\\gamma = 0.0$, $w_H = 0.5$, $\\eta = 0.1$, $\\rho = 0.01$, $\\beta = 2.0$.\n3. Edge, high repetition and strong habit:\n   - $N = 200$, $\\alpha_0 = 0.25$, $\\lambda = 0.03$, $r_0^T = 1.2$, $\\kappa = 0.015$, $r_C^{\\mathrm{pre}} = 0.05$, $r_C^{\\mathrm{post}} = 0.8$, $\\gamma = 1.2$, $w_H = 1.0$, $\\eta = 0.3$, $\\rho = 0.02$, $\\beta = 2.5$.\n4. Edge, no RPE or reward decay:\n   - $N = 80$, $\\alpha_0 = 0.35$, $\\lambda = 0.0$, $r_0^T = 1.0$, $\\kappa = 0.0$, $r_C^{\\mathrm{pre}} = 0.05$, $r_C^{\\mathrm{post}} = 0.5$, $\\gamma = 0.5$, $w_H = 0.7$, $\\eta = 0.15$, $\\rho = 0.01$, $\\beta = 1.5$.\n5. Edge, near-deterministic policy:\n   - $N = 100$, $\\alpha_0 = 0.2$, $\\lambda = 0.04$, $r_0^T = 1.1$, $\\kappa = 0.01$, $r_C^{\\mathrm{pre}} = 0.05$, $r_C^{\\mathrm{post}} = 0.6$, $\\gamma = 0.9$, $w_H = 0.6$, $\\eta = 0.2$, $\\rho = 0.02$, $\\beta = 10.0$.\n\nRequired output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result1,result2,result3]$). Each entry must be the float $\\Delta$ defined above for the corresponding test case, in the same order as listed.",
            "solution": "The problem statement posits a deterministic mean-field model of action selection between a tic ($T$) and a competing response ($C$), grounded in principles of reinforcement learning and habit formation. The objective is to simulate the evolution of action values and habit strengths over a series of trials and to quantify the effect of a simulated Comprehensive Behavioral Intervention for Tics (CBIT) on the final probability of selecting the tic action.\n\nThe problem is scientifically grounded, drawing upon established computational neuroscience concepts such as the Rescorla-Wagner learning rule, softmax action selection, and the dual-system model of goal-directed versus habitual control. It is mathematically well-posed, providing a complete set of deterministic update equations, initial conditions, and parameters. The instructions are unambiguous, and the setup is internally consistent. Therefore, the problem is deemed valid and a solution can be formulated.\n\nThe solution is derived by implementing the specified simulation protocol. This involves tracking four state variables over time $t$: the expected value of the tic, $V_t(T)$; the expected value of the competing response, $V_t(C)$; the habit strength of the tic, $H_t(T)$; and the habit strength of the competing response, $H_t(C)$.\n\nThe simulation proceeds iteratively from an initial state at $t=0$ where all state variables are zero: $V_0(T) = 0$, $V_0(C) = 0$, $H_0(T) = 0$, and $H_0(C) = 0$. For each trial $t$ from $0$ to $N-1$, a series of calculations is performed.\n\nFirst, the time-dependent parameters are computed. The learning rate, $\\alpha_t$, decays exponentially, reflecting reduced neural plasticity with repetition:\n$$\n\\alpha_t = \\alpha_0 e^{-\\lambda t}\n$$\nSimilarly, the immediate reinforcement (reward) associated with the tic action, $r_t(T)$, also decays, modeling habituation to the relief experienced from tic expression:\n$$\nr_t(T) = r_0^T e^{-\\kappa t}\n$$\nThe reinforcement for the competing response, $r_t(C)$, is held constant at a value $r_C$ throughout the simulation.\n\nNext, the total action value, $Q_t(a)$, for each action $a \\in \\{T, C\\}$ is computed. This value is a linear combination of the goal-directed expected value $V_t(a)$, the habitual bias $H_t(a)$ weighted by a factor $w_H$, and an external bias $b_a$:\n$$\nQ_t(T) = V_t(T) + w_H H_t(T)\n$$\n$$\nQ_t(C) = V_t(C) + w_H H_t(C) + b_C\n$$\nThe bias $b_T$ is implicitly $0$. The bias $b_C$ is the primary mechanism for modeling CBIT, where $b_C=0$ for the pre-CBIT condition and $b_C=\\gamma > 0$ for the post-CBIT condition.\n\nFrom these action values, the probability of selecting each action, $\\pi_t(a)$, is determined using a softmax function. The inverse temperature parameter $\\beta$ controls the stochasticity of the choice, with higher $\\beta$ leading to more deterministic selection of the action with the higher $Q$-value.\n$$\n\\pi_t(a) = \\frac{\\exp(\\beta Q_t(a))}{\\exp(\\beta Q_t(T)) + \\exp(\\beta Q_t(C))}\n$$\n\nThe core of the simulation lies in the update rules for the state variables. The problem specifies a deterministic \"mean-field\" update. Instead of stochastically selecting one action and updating only its value, the updates for both actions are performed in proportion to their selection probabilities.\n\nThe expected value $V_t(a)$ is updated based on the reward prediction error (RPE), $\\delta_t(a) = r_t(a) - V_t(a)$, weighted by the learning rate $\\alpha_t$ and the action probability $\\pi_t(a)$:\n$$\nV_{t+1}(T) = V_t(T) + \\alpha_t [r_t(T) - V_t(T)] \\pi_t(T)\n$$\n$$\nV_{t+1}(C) = V_t(C) + \\alpha_t [r_t(C) - V_t(C)] \\pi_t(C)\n$$\nThis update rule, derived from the Rescorla-Wagner model, drives the learned value $V_t(a)$ toward the actual experienced reward $r_t(a)$.\n\nThe habit strength $H_t(a)$ is updated as a leaky integrator. It increases with the probability of selecting action $a$ at a rate $\\eta$, and decays over time with a forgetting rate $\\rho$:\n$$\nH_{t+1}(T) = (1 - \\rho) H_t(T) + \\eta \\pi_t(T)\n$$\n$$\nH_{t+1}(C) = (1 - \\rho) H_t(C) + \\eta \\pi_t(C)\n$$\n\nThis iterative process is repeated for $N$ trials (from $t=0$ to $t=N-1$), yielding the final state variables $V_N(T)$, $V_N(C)$, $H_N(T)$, and $H_N(C)$.\n\nFollowing the loop, the final selection probabilities, $\\pi_N(T)$ and $\\pi_N(C)$, are computed based on these final state variables. This requires one final calculation of the $Q$-values, $Q_N(a)$, and one final application of the softmax function:\n$$\nQ_N(T) = V_N(T) + w_H H_N(T)\n$$\n$$\nQ_N(C) = V_N(C) + w_H H_N(C) + b_C\n$$\n$$\n\\pi_N(T) = \\frac{\\exp(\\beta Q_N(T))}{\\exp(\\beta Q_N(T)) + \\exp(\\beta Q_N(C))}\n$$\n\nTo solve the problem, this entire simulation is executed twice for each test case.\n1.  **Pre-CBIT simulation**: The parameters $b_C = 0$ and $r_C = r_C^{\\mathrm{pre}}$ are used. The result is $\\pi_N^{\\mathrm{pre}}(T)$.\n2.  **Post-CBIT simulation**: The parameters $b_C = \\gamma$ and $r_C = r_C^{\\mathrm{post}}$ are used. The result is $\\pi_N^{\\mathrm{post}}(T)$.\n\nThe final metric to be reported is the change in the tic selection probability, $\\Delta$:\n$$\n\\Delta = \\pi_N^{\\mathrm{post}}(T) - \\pi_N^{\\mathrm{pre}}(T)\n$$\nA negative value of $\\Delta$ indicates that the simulated CBIT intervention was successful in reducing the propensity to select the tic action.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_simulation(N, alpha0, lam, r0_T, kappa, r_C, b_C, w_H, eta, rho, beta):\n    \"\"\"\n    Runs the deterministic mean-field simulation for action selection.\n\n    Args:\n        N (int): Number of trials.\n        alpha0 (float): Initial learning rate.\n        lam (float): Decay rate for the learning rate.\n        r0_T (float): Initial reinforcement for the tic action.\n        kappa (float): Decay rate for the tic reinforcement.\n        r_C (float): Constant reinforcement for the competing response.\n        b_C (float): Bias for the competing response action value.\n        w_H (float): Weight for the habit strength.\n        eta (float): Habit formation rate.\n        rho (float): Habit forgetting rate.\n        beta (float): Inverse temperature for the softmax policy.\n\n    Returns:\n        float: The final probability of selecting the tic action, pi_N(T).\n    \"\"\"\n    # Initialize state variables\n    V_T, V_C = 0.0, 0.0\n    H_T, H_C = 0.0, 0.0\n\n    for t in range(N):\n        # 1. Compute time-dependent parameters\n        alpha_t = alpha0 * np.exp(-lam * t)\n        r_t_T = r0_T * np.exp(-kappa * t)\n        r_t_C = r_C\n\n        # 2. Compute Q-values\n        Q_t_T = V_T + w_H * H_T\n        Q_t_C = V_C + w_H * H_C + b_C\n\n        # 3. Compute selection probabilities (softmax)\n        # Numerically stable softmax: subtract max(Q) before exp\n        max_Q = max(Q_t_T, Q_t_C)\n        exp_QT = np.exp(beta * (Q_t_T - max_Q))\n        exp_QC = np.exp(beta * (Q_t_C - max_Q))\n        sum_exp_Q = exp_QT + exp_QC\n        \n        pi_t_T = exp_QT / sum_exp_Q\n        pi_t_C = exp_QC / sum_exp_Q\n\n        # 4. Update V-values (mean-field)\n        V_T_next = V_T + alpha_t * (r_t_T - V_T) * pi_t_T\n        V_C_next = V_C + alpha_t * (r_t_C - V_C) * pi_t_C\n        \n        # 5. Update H-values\n        H_T_next = (1 - rho) * H_T + eta * pi_t_T\n        H_C_next = (1 - rho) * H_C + eta * pi_t_C\n        \n        # Assign next state values for the next iteration\n        V_T, V_C = V_T_next, V_C_next\n        H_T, H_C = H_T_next, H_C_next\n\n    # After N trials, compute final selection probabilities from final Q-values\n    Q_N_T = V_T + w_H * H_T\n    Q_N_C = V_C + w_H * H_C + b_C\n\n    max_Q_N = max(Q_N_T, Q_N_C)\n    exp_Q_N_T = np.exp(beta * (Q_N_T - max_Q_N))\n    exp_Q_N_C = np.exp(beta * (Q_N_C - max_Q_N))\n    sum_exp_Q_N = exp_Q_N_T + exp_Q_N_C\n    \n    pi_N_T = exp_Q_N_T / sum_exp_Q_N\n    \n    return pi_N_T\n\ndef solve():\n    \"\"\"\n    Solves the problem by running simulations for all test cases.\n    \"\"\"\n    # Test cases: (N, alpha0, lam, r0_T, kappa, r_C_pre, r_C_post, gamma, w_H, eta, rho, beta)\n    test_cases = [\n        (50, 0.3, 0.05, 1.0, 0.02, 0.05, 0.4, 0.6, 0.5, 0.1, 0.01, 2.0),\n        (50, 0.3, 0.05, 1.0, 0.02, 0.05, 0.05, 0.0, 0.5, 0.1, 0.01, 2.0),\n        (200, 0.25, 0.03, 1.2, 0.015, 0.05, 0.8, 1.2, 1.0, 0.3, 0.02, 2.5),\n        (80, 0.35, 0.0, 1.0, 0.0, 0.05, 0.5, 0.5, 0.7, 0.15, 0.01, 1.5),\n        (100, 0.2, 0.04, 1.1, 0.01, 0.05, 0.6, 0.9, 0.6, 0.2, 0.02, 10.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        N, alpha0, lam, r0_T, kappa, r_C_pre, r_C_post, gamma, w_H, eta, rho, beta = case\n        \n        # Pre-CBIT simulation\n        pi_N_pre_T = run_simulation(N, alpha0, lam, r0_T, kappa, r_C_pre, 0.0, w_H, eta, rho, beta)\n        \n        # Post-CBIT simulation\n        pi_N_post_T = run_simulation(N, alpha0, lam, r0_T, kappa, r_C_post, gamma, w_H, eta, rho, beta)\n\n        # Calculate the change\n        delta = pi_N_post_T - pi_N_pre_T\n        results.append(delta)\n\n    # Format and print the final output\n    print(f\"[{','.join(f'{r:.12f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}