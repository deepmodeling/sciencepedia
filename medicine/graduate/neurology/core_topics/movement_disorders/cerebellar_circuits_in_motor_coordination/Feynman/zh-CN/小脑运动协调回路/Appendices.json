{
    "hands_on_practices": [
        {
            "introduction": "在深入探讨神经回路复杂的数学模型之前，让我们从一个基础性的思想实验开始。这项练习将引导你对一个经典的视觉-运动任务进行建模，并实施一次“计算性损伤”，以探究当关键的长时程抑制（LTD）机制缺失时，运动学习会发生什么。通过解决这个问题 ，你将从第一性原理出发，推导出为何突触可塑性不仅仅是一种可观察的现象，而是适应性学习在计算上的必然要求。",
            "id": "4464819",
            "problem": "考虑一个棱镜适应任务，其中视野被旋转一个恒定角度，这被建模为大小为 $p$ 的固定视觉运动扰动。假设一个典型的Marr–Albus–Ito小脑回路，其中平行纤维-浦肯野细胞（PF–PC）长时程抑制（LTD）在产生误差的情境中减少浦肯野细胞（PC）的简单峰电位输出，从而去抑制小脑深部核团（DCN）并产生前馈运动校正以减少终点误差。爬行纤维（CF）输入传递一个标量性能误差信号。在小角度情况下，基于以下科学上标准且易于处理的假设进行分析：\n\n- PC简单峰电位输出是一个固定的、归一化的情境向量的线性函数，因此净PC输出可以表示为试验 $t$ 上的标量 $s_{t}$。\n- DCN输出因PC输出减少而去抑制，并以恒定增益线性驱动运动校正 $u_{t}$，在一个基线下，该基线在扰动前产生零校正。\n- 观测到的终点误差是固定扰动与运动校正负值之和，且CF误差信号与此观测误差成正比。\n- PF–PC LTD实现有监督学习，该学习根据CF误差成比例地更新标量PC输出状态 $s_{t}$，在与误差相关的情境中减少PC输出。\n\n从这些基础和线性近似出发，从第一性原理推导，PF-PC LTD的移除（缺失）（即将LTD学习率设为零）如何改变棱镜适应中的误差动态，并说明这对长时间训练后的稳态性能意味着什么。然后，对于一个施加大小为 $p = \\frac{\\pi}{12}$ 的恒定视觉运动旋转的棱镜，计算当PF-PC LTD完全缺失（学习率为零）且其他形式的适应可以忽略不计时，长时间训练（$t \\to \\infty$）后预测的残余终点误差。\n\n用弧度表示残余终点误差的最终数值答案，并四舍五入到四位有效数字。在最终的方框答案中不要包含单位；但是，答案必须对应于此处指定的弧度。",
            "solution": "问题要求在一个棱镜适应任务中推导误差动态，特别关注平行纤维-浦肯野细胞（PF-PC）突触处长时程抑制（LTD）缺失的后果。最后，它要求计算在给定扰动下，此条件下的残余终点误差。\n\n第一步是将问题陈述中提供的假设形式化为一个数学模型。设 $t$ 为试验次数。\n\n设 $p$ 为视觉运动扰动的恒定大小。\n设 $s_t$ 为试验 $t$ 上的标量PC简单峰电位输出状态。我们可以将初始的、扰动前的状态定义为 $t=0$ 时的 $s_0$。\n设 $u_t$ 为在试验 $t$ 上生成的运动校正。\n设 $e_t$ 为在试验 $t$ 上观测到的终点误差。\n\n这些假设可转化为以下方程：\n\n1.  **DCN输出与运动校正：** 运动校正 $u_t$ 由小脑深部核团（DCN）的去抑制线性驱动，而这种去抑制是由于PC输出减少所致。我们可以将其建模为相对于基线状态的具有恒定增益 $k > 0$ 的线性关系。设基线PC输出为 $s_0$。PC输出的减少量为 $(s_0 - s_t)$。问题陈述了一个在扰动前校正为零的基线，这对应于状态 $s_0$。\n    $$u_t = k(s_0 - s_t)$$\n    在试验 $t=0$ 时，扰动尚未引发任何学习，因此 $s_t = s_0$，初始校正为 $u_0 = k(s_0 - s_0) = 0$。\n\n2.  **终点误差：** 观测到的终点误差 $e_t$ 被明确定义为固定扰动 $p$ 与运动校正 $u_t$ 的负值之和。\n    $$e_t = p - u_t$$\n    因此，一个正的校正 $u_t$ 可以减少由扰动值 $p$ 引起的误差。\n\n3.  **爬行纤维信号与学习规则：** 爬行纤维（CF）输入传递一个与观测误差成正比的误差信号，我们设其为 $e_{CF, t} = \\gamma e_t$，其中比例常数 $\\gamma > 0$。PF-PC LTD 更新PC输出状态 $s_t$。此更新规则规定，一个正的误差信号会导致后续PC输出的减少（LTD）。这可以写成关于 $s_t$ 的一个递推关系：\n    $$s_{t+1} - s_t = -\\beta e_{CF, t}$$\n    其中 $\\beta \\ge 0$ 是与LTD相关的学习率。结合CF信号的定义，我们得到：\n    $$s_{t+1} = s_t - \\beta \\gamma e_t$$\n\n现在，我们推导终点误差 $e_t$ 逐次试验的动态变化。\n试验 $t+1$ 上的误差由下式给出：\n$$e_{t+1} = p - u_{t+1}$$\n使用运动校正 $u_{t+1}$ 的表达式：\n$$e_{t+1} = p - k(s_0 - s_{t+1})$$\n现在，代入 $s_{t+1}$ 的学习规则：\n$$e_{t+1} = p - k(s_0 - (s_t - \\beta \\gamma e_t))$$\n$$e_{t+1} = p - k(s_0 - s_t) - k\\beta\\gamma e_t$$\n我们识别出项 $p - k(s_0 - s_t)$ 是前一次试验误差 $e_t$ 的定义。代入此项，得到误差的递推关系：\n$$e_{t+1} = e_t - k\\beta\\gamma e_t$$\n$$e_{t+1} = (1 - k\\beta\\gamma) e_t$$\n\n这个线性递推关系描述了误差动态。\n在PF-PC LTD存在的正常情况下，学习率 $\\beta > 0$。为了使学习稳定并收敛，常数必须满足 $0  k\\beta\\gamma  2$。在这种情况下，项 $|1 - k\\beta\\gamma|  1$，并且当 $t \\to \\infty$ 时，误差 $e_t = e_0 (1 - k\\beta\\gamma)^t$ 会指数衰减至零。这代表了成功的适应，其中稳态误差为 $\\lim_{t \\to \\infty} e_t = 0$。系统学会了产生一个能够完全抵消扰动 $p$ 的运动校正 $u_\\infty$。\n\n问题要求我们分析PF-PC LTD缺失的情况。这通过将学习率设为零来建模：$\\beta = 0$。\n将 $\\beta = 0$ 代入误差动态方程：\n$$e_{t+1} = (1 - k \\cdot 0 \\cdot \\gamma) e_t$$\n$$e_{t+1} = e_t$$\n这个结果表明，任何一次试验的误差都与前一次试验的误差相同。误差动态是静态的；误差不随时间变化。\n\n这意味着对于所有试验 $t \\ge 0$，误差 $e_t$ 始终等于初始误差 $e_0$。为了找到稳态误差，我们只需要计算初始误差。在试验 $t=0$ 时，没有发生学习，所以运动校正 $u_0 = 0$。初始误差为：\n$$e_0 = p - u_0 = p - 0 = p$$\n因此，当 $\\beta=0$ 时，所有后续试验的误差都是恒定的：\n$$e_t = p \\quad \\text{对于所有 } t \\ge 0$$\n\n这对稳态性能（$t \\to \\infty$）的启示是系统无法适应。残余终点误差不会从其初始值减小，并始终等于扰动的全部大小 $p$。\n$$\\lim_{t \\to \\infty} e_t = p$$\n\n问题要求计算扰动为 $p = \\frac{\\pi}{12}$ 弧度时该残余误差的数值。根据我们的推导，残余误差就是 $p$。\n$$\\text{残余误差} = \\frac{\\pi}{12}$$\n计算其数值：\n$$\\frac{\\pi}{12} \\approx \\frac{3.14159265}{12} \\approx 0.26179938 \\text{ 弧度}$$\n四舍五入到四位有效数字，我们得到 $0.2618$。",
            "answer": "$$\\boxed{0.2618}$$"
        },
        {
            "introduction": "在确认了突触可塑性的必要性之后，我们现在转向一个动态的、真实世界行为的建模：前庭-眼动反射（VOR）。本练习要求你分析小脑如何响应持续的感觉误差，随时间调整VOR增益，这一过程被称为适应。通过求解描述该学习过程的微分方程 ，你将深入理解大脑如何通过连续的、由误差驱动的校准来维持运动的精确性。",
            "id": "4464878",
            "problem": "小脑在运动协调中的一个核心功能是调整感觉运动映射，以最小化感觉预测误差。在水平前庭-眼动反射 (VOR) 中，目标是减少视网膜滑移，其定义为当头部旋转时视网膜上的残余图像速度。考虑一个高等研究生水平的模型，其中小脑绒球实现一个线性控制器，其标量增益 $G(t)$ 通过由编码视网膜滑移的攀爬纤维信号驱动的监督学习缓慢适应。假设旋转为小角度，以便线性叠加原理成立。\n\n一名受试者进行正弦式头部旋转，其头部角速度为 $v_{\\mathrm{head}}(t) = V_{0} \\sin(\\omega t)$，单位为弧度/秒。引入棱镜，它将视觉场景的表观速度放大一个正因子 $s$，因此视网膜滑移误差为 $e(t) = s\\,v_{\\mathrm{head}}(t) - v_{\\mathrm{eye}}(t)$，其中 $v_{\\mathrm{eye}}(t)$ 是水平眼球角速度，单位为弧度/秒。小脑增益 $G(t)$ 根据一个周期平均的监督学习法则进行适应，该法则受平行纤维-浦肯野细胞突触可塑性和攀爬纤维误差教学信号（长时程抑制 (LTD) 相关性学习）的启发：\n\n$$\\frac{dG}{dt} = \\eta \\left\\langle v_{\\mathrm{head}}(t)\\, e(t) \\right\\rangle,$$\n\n其中 $\\eta > 0$ 是一个学习率常数，$\\langle \\cdot \\rangle$ 表示在一个刺激周期 $T = \\frac{2\\pi}{\\omega}$ 上的平均值：\n\n$$\\left\\langle f(t) \\right\\rangle = \\frac{1}{T} \\int_{0}^{T} f(t)\\, dt.$$\n\n假设从小脑输出到眼球速度的运动装置可以很好地近似为一个线性映射（对于小角度、低频旋转有效），并且适应后的控制器产生的期望眼球速度与头部速度成正比，比例为 $G(t)$，由上述学习动力学确定。在时间 $t=0$ 时的初始增益为 $G(0) = G_{0}$。\n\n从这些定义和假设出发，推导在适应时间 $t$ 后的期望眼球速度 $v_{\\mathrm{eye}}(t)$ 的闭式表达式，用 $V_{0}$、$\\omega$、$s$、$\\eta$ 和 $G_{0}$ 表示。你的最终表达式必须以弧度/秒为单位。没有提供数值；请将你的答案表示为单个闭式解析表达式。不需要四舍五入。",
            "solution": "增益 $G(t)$ 的适应过程由以下学习法则决定：\n$$ \\frac{dG}{dt} = \\eta \\left\\langle v_{\\mathrm{head}}(t)\\, e(t) \\right\\rangle $$\n视网膜滑移误差 $e(t)$ 由 $e(t) = s\\,v_{\\mathrm{head}}(t) - v_{\\mathrm{eye}}(t)$ 给出。运动装置由线性映射 $v_{\\mathrm{eye}}(t) = G(t) v_{\\mathrm{head}}(t)$ 描述。我们可以将此代入误差表达式中：\n$$ e(t) = s\\,v_{\\mathrm{head}}(t) - G(t) v_{\\mathrm{head}}(t) = (s - G(t)) v_{\\mathrm{head}}(t) $$\n现在，我们将此误差表达式代回学习法则中：\n$$ \\frac{dG}{dt} = \\eta \\left\\langle v_{\\mathrm{head}}(t)\\, (s - G(t)) v_{\\mathrm{head}}(t) \\right\\rangle = \\eta \\left\\langle (s - G(t)) v_{\\mathrm{head}}(t)^2 \\right\\rangle $$\n模型假设增益 $G(t)$ 的适应速度相对于刺激周期 $T = \\frac{2\\pi}{\\omega}$ 而言很慢。因此，我们可以在单周期平均区间内将 $G(t)$ 视为常数。这使我们能够将项 $(s - G(t))$ 从平均算子中提出：\n$$ \\frac{dG}{dt} = \\eta (s - G(t)) \\left\\langle v_{\\mathrm{head}}(t)^2 \\right\\rangle $$\n接下来，我们必须计算头部速度平方的时间平均值 $\\left\\langle v_{\\mathrm{head}}(t)^2 \\right\\rangle$。头部速度由 $v_{\\mathrm{head}}(t) = V_{0} \\sin(\\omega t)$ 给出。\n$$ \\left\\langle v_{\\mathrm{head}}(t)^2 \\right\\rangle = \\frac{1}{T} \\int_{0}^{T} (V_{0} \\sin(\\omega t))^2 dt = \\frac{V_{0}^2}{T} \\int_{0}^{T} \\sin^2(\\omega t) dt $$\n使用三角恒等式 $\\sin^2(\\theta) = \\frac{1}{2}(1 - \\cos(2\\theta))$，积分变为：\n$$ \\int_{0}^{T} \\sin^2(\\omega t) dt = \\int_{0}^{T} \\frac{1 - \\cos(2\\omega t)}{2} dt = \\frac{1}{2} \\left[ t - \\frac{\\sin(2\\omega t)}{2\\omega} \\right]_{0}^{T} $$\n代入周期 $T = \\frac{2\\pi}{\\omega}$：\n$$ \\frac{1}{2} \\left[ T - \\frac{\\sin(2\\omega T)}{2\\omega} - (0 - 0) \\right] = \\frac{1}{2} \\left[ \\frac{2\\pi}{\\omega} - \\frac{\\sin(2\\omega \\frac{2\\pi}{\\omega})}{2\\omega} \\right] = \\frac{1}{2} \\left[ \\frac{2\\pi}{\\omega} - \\frac{\\sin(4\\pi)}{2\\omega} \\right] $$\n由于 $\\sin(4\\pi) = 0$，积分计算结果为 $\\frac{T}{2}$。因此，平均值为：\n$$ \\left\\langle v_{\\mathrm{head}}(t)^2 \\right\\rangle = \\frac{V_{0}^2}{T} \\left( \\frac{T}{2} \\right) = \\frac{V_{0}^2}{2} $$\n将此结果代回关于 $G(t)$ 的微分方程中：\n$$ \\frac{dG}{dt} = \\eta (s - G(t)) \\frac{V_{0}^2}{2} $$\n这是一个一阶线性常微分方程。为方便起见，设 $k = \\frac{\\eta V_{0}^2}{2}$。方程为：\n$$ \\frac{dG}{dt} = k(s - G) $$\n该方程是可分离的：\n$$ \\frac{dG}{s - G} = k\\,dt $$\n我们可以对两边进行积分，时间从 $0$ 到 $t$，增益从 $G(0)=G_0$ 到 $G(t)$：\n$$ \\int_{G_0}^{G(t)} \\frac{dG'}{s - G'} = \\int_{0}^{t} k\\,dt' $$\n$$ \\left[ -\\ln|s - G'| \\right]_{G_0}^{G(t)} = \\left[ kt' \\right]_{0}^{t} $$\n$$ -\\ln|s - G(t)| - (-\\ln|s - G_0|) = kt $$\n$$ \\ln\\left(\\frac{|s - G_0|}{|s - G(t)|}\\right) = kt $$\n对两边取指数：\n$$ \\frac{|s - G_0|}{|s - G(t)|} = e^{kt} \\implies |s - G(t)| = |s - G_0| e^{-kt} $$\n由于增益 $G(t)$ 将从 $G_0$ 单调地趋近于目标 $s$（从上方或下方，但不会越过 $s$），因此项 $(s-G)$ 的符号不会改变。因此，我们可以去掉绝对值符号：\n$$ s - G(t) = (s - G_0) e^{-kt} $$\n求解 $G(t)$：\n$$ G(t) = s - (s - G_0) e^{-kt} $$\n代回 $k$ 的表达式：\n$$ G(t) = s - (s - G_0) \\exp\\left(-\\frac{\\eta V_{0}^2}{2} t\\right) $$\n问题要求期望的眼球速度 $v_{\\mathrm{eye}}(t)$。我们使用关系式 $v_{\\mathrm{eye}}(t) = G(t) v_{\\mathrm{head}}(t)$：\n$$ v_{\\mathrm{eye}}(t) = \\left[ s - (s - G_0) \\exp\\left(-\\frac{\\eta V_{0}^2}{2} t\\right) \\right] v_{\\mathrm{head}}(t) $$\n最后，代入 $v_{\\mathrm{head}}(t) = V_0 \\sin(\\omega t)$ 的表达式：\n$$ v_{\\mathrm{eye}}(t) = \\left[ s - (s - G_0) \\exp\\left(-\\frac{\\eta V_{0}^2}{2} t\\right) \\right] V_0 \\sin(\\omega t) $$\n这就是期望眼球速度作为时间函数和给定参数的闭式表达式。",
            "answer": "$$ \\boxed{\\left(s - (s - G_{0}) \\exp\\left(-\\frac{\\eta V_{0}^{2}}{2} t\\right)\\right) V_{0} \\sin(\\omega t)} $$"
        },
        {
            "introduction": "我们的最后一项练习将带你从抽象的方程走向神经编码的层面，将理论与神经生理学数据联系起来。这项练习要求你基于简单发放和复杂发放的精确时间，利用“资格痕迹”这一先进的突触可塑性概念，实现一个学习模型。通过编写程序来根据原始的发放时间序列预测行为表现的变化 ，你将对学习如何在突触层面被计算出来，建立一个具体、算法化的理解。",
            "id": "4464820",
            "problem": "您会获得在一项运动任务的离散试验中记录的浦肯野细胞简单峰电位和复杂峰电位的事件时间。假设存在以下在小脑生理学中被广泛接受的基本原理：当存在运动误差时，爬行纤维输入会诱发浦肯野细胞产生复杂峰电位；复杂峰电位指导平行纤维-浦肯野细胞突触发生可塑性变化；浦肯野细胞抑制小脑深部核团，从而塑造运动输出。在这些事实的基础上，从第一性原理推导出一个简化的、逐次试验的模型，其中每个浦肯野细胞的单一有效突触权重由作用于先前简单峰电位所形成的资格迹上的复杂峰电位进行更新，并用此模型预测运动表现的逐次试验变化。资格迹应为简单峰电位的因果函数，具有由单一时间常数表征的指数衰减特性。从突触变化到运动表现的映射必须反映出浦肯野细胞输出的减少会使小脑深部核团去抑制，从而改善表现。除这些基础原理外，不要假设或使用任何捷径公式；相反，您需要推导出实现所需的表达式。\n\n请在程序中实现您推导的规则，该程序在给定固定模型参数的情况下，仅根据该次试验中的简单峰电位和复杂峰电位时间，计算每次试验预测的运动表现变化。时间值的单位是秒，运动表现变化是无量纲的。\n\n使用以下参数值：指数资格迹时间常数 $\\tau_{e} = 0.05$ (秒)，学习率 $\\eta = 0.01$ (无量纲)，以及表现增益 $\\kappa = 1.0$ (无量纲)。试验持续时间为 $T = 1.0$ (秒)。\n\n您的程序必须处理以下测试套件，其中每个测试用例包含固定数量的试验，并为每次试验指定了简单峰电位时间和复杂峰电位时间：\n\n- 测试用例 $1$ (一般情况，混合时间点):\n  - 试验 $1$: 简单峰电位 $[0.10, 0.18, 0.32, 0.47, 0.55, 0.70, 0.82]$, 复杂峰电位 $[0.60]$。\n  - 试验 $2$: 简单峰电位 $[0.12, 0.19, 0.40, 0.58, 0.75]$, 复杂峰电位 $[0.20]$。\n  - 试验 $3$: 简单峰电位 $[0.08, 0.16, 0.30, 0.33, 0.51, 0.72]$, 复杂峰电位 $[]$。\n  - 试验 $4$: 简单峰电位 $[0.05, 0.15, 0.25, 0.35, 0.45, 0.55, 0.65]$, 复杂峰电位 $[0.50]$。\n  - 试验 $5$: 简单峰电位 $[0.22, 0.29, 0.41, 0.67, 0.83]$, 复杂峰电位 $[0.68, 0.90]$。\n- 测试用例 $2$ (边界情况，无复杂峰电位):\n  - 试验 $1$: 简单峰电位 $[0.10, 0.20, 0.30]$, 复杂峰电位 $[]$。\n  - 试验 $2$: 简单峰电位 $[0.15, 0.45, 0.80]$, 复杂峰电位 $[]$。\n  - 试验 $3$: 简单峰电位 $[0.25, 0.35, 0.55, 0.75]$, 复杂峰电位 $[]$。\n  - 试验 $4$: 简单峰电位 $[0.05, 0.65, 0.95]$, 复杂峰电位 $[]$。\n- 测试用例 $3$ (边缘情况：早期复杂峰电位，多个复杂峰电位，同时发生的复杂峰电位):\n  - 试验 $1$: 简单峰电位 $[0.20, 0.40]$, 复杂峰电位 $[0.05]$。\n  - 试验 $2$: 简单峰电位 $[0.05, 0.06, 0.07]$, 复杂峰电位 $[0.08, 0.50]$。\n  - 试验 $3$: 简单峰电位 $[0.10, 0.11, 0.12, 0.13, 0.14]$, 复杂峰电位 $[0.12, 0.12]$。\n\n对于每个测试用例，计算一个预测的逐次试验表现变化列表，其中每个元素对应于该次试验的预测变化。输出值必须是无单位（无量纲）的实数（浮点数）。您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。将所有测试用例的结果聚合到一个列表中，其中每个测试用例的结果本身就是一个列表。例如，最终输出应类似于 $[[a_{1}, a_{2}, \\dots], [b_{1}, b_{2}, \\dots], [c_{1}, c_{2}, \\dots]]$，其中每个 $a_{i}$、$b_{i}$ 和 $c_{i}$ 都是浮点数。",
            "solution": "问题陈述已经过验证，被认为是有效的。它在科学上基于已确立的小脑生理学原理，问题阐述清晰，提供了足够的信息以获得唯一解，并以客观、可形式化的语言表达。\n\n任务是基于浦肯野细胞的放电时间，推导出一个用于计算运动表现变化的逐次试验计算模型，然后实现它。根据给定的第一性原理，推导过程如下。\n\n**1. 资格迹 ($E(t)$) 的推导**\n\n问题指定，资格迹由简单峰电位 (SS) 形成，是一个具有指数衰减的因果函数。该迹代表了平行纤维-浦肯野细胞突触的临时状态，使其“有资格”发生可塑性变化。设单次试验内的简单峰电位时间集合为 $\\{t_{\\text{SS},i}\\}$。\n\n在时间 $t_{\\text{SS},i}$ 发生的单个简单峰电位，对资格迹产生一个随时间指数衰减的贡献。根据因果性要求，对于任何时间 $t \\le t_{\\text{SS},i}$，此贡献为零。对于时间 $t > t_{\\text{SS},i}$，其贡献与 $\\exp(-(t - t_{\\text{SS},i}) / \\tau_{e})$ 成正比，其中 $\\tau_{e}$ 是资格迹时间常数。假设每个峰电位的贡献幅度为单位1，则在试验中任意时间 $t$ 的总资格迹 $E(t)$ 是所有先前简单峰电位贡献的线性叠加。\n\n数学上，这表示为：\n$$\nE(t) = \\sum_{i \\text{ such that } t_{\\text{SS},i}  t} \\exp\\left(-\\frac{t - t_{\\text{SS},i}}{\\tau_{e}}\\right)\n$$\n这个方程将资格迹形式化为过去简单峰电位活动的衰减记忆。\n\n**2. 突触权重更新 ($\\Delta w$) 的推导**\n\n问题指出，为响应运动误差而由爬行纤维输入引发的复杂峰电位 (CS) 指示可塑性变化。这与平行纤维-浦肯野细胞突触的长时程抑制 (LTD) 理论一致，即当平行纤维活动（由 SS 发出信号）与爬行纤维活动（由 CS 发出信号）联合发生时，突触权重会降低。\n\n在时间 $t_{\\text{CS},j}$ 发生的复杂峰电位作为一个“指令”信号。突触变化的幅度由资格迹 $E(t)$ 在那一刻的值 $E(t_{\\text{CS},j})$ 决定。该值量化了近期发生了多少相关的简单峰电位活动。\n\n因此，有效突触权重的变化 $\\Delta w$ 与此采样的资格迹成正比。由于此过程介导 LTD，变化必须是负的。我们引入一个学习率 $\\eta$ 来缩放更新的幅度。对于单次试验，突触权重的总变化 $\\Delta w_{\\text{trial}}$ 是该试验中每个复杂峰电位引起的变化之和。设复杂峰电位时间集合为 $\\{t_{\\text{CS},j}\\}$。\n\n该试验的总权重更新为：\n$$\n\\Delta w_{\\text{trial}} = -\\eta \\sum_{j} E(t_{\\text{CS},j})\n$$\n如果一次试验没有复杂峰电位，则该和为空，$\\Delta w_{\\text{trial}} = 0$，这正确地模拟了没有误差信号时不会发生学习。\n\n**3. 运动表现变化 ($\\Delta P$) 的推导**\n\n最后一步是将突触权重的变化与运动表现的变化联系起来。给定的原理指出：\n- 浦肯野细胞 (PC) 向小脑深部核团 (DCN) 提供抑制性输出。\n- PC 输出的减少会使 DCN 去抑制，从而改善运动表现。\n\n让我们将其形式化。我们假设有效突触权重 $w$ 与 PC 的抑制性输出强度成正比。$w$ 的减小（即由于 LTD 导致的 $\\Delta w  0$）会导致 PC 对 DCN 的抑制性影响减弱。这种去抑制导致 DCN 放电增加，进而驱动运动表现 $P$ 的改善。\n\n因此，表现的变化 $\\Delta P$ 必须与突触权重的变化 $\\Delta w$ 负相关。我们可以使用给定的表现增益参数 $\\kappa$ 将此表示为线性关系：\n$$\n\\Delta P_{\\text{trial}} = -\\kappa \\cdot \\Delta w_{\\text{trial}}\n$$\n负号确保了负的 $\\Delta w$（突触抑制）导致正的 $\\Delta P$（表现改善）。\n\n**4. 完整的逐次试验模型**\n\n将 $\\Delta w_{\\text{trial}}$ 和 $E(t)$ 的表达式代入表现变化方程，我们得到用于逐次试验运动表现变化的完整简化模型：\n$$\n\\Delta P_{\\text{trial}} = -\\kappa \\left( -\\eta \\sum_{j} E(t_{\\text{CS},j}) \\right) = \\kappa \\eta \\sum_{j} E(t_{\\text{CS},j})\n$$\n代入资格迹的完整表达式：\n$$\n\\Delta P_{\\text{trial}} = \\kappa \\eta \\sum_{j} \\left( \\sum_{i \\text{ s.t. } t_{\\text{SS},i}  t_{\\text{CS},j}} \\exp\\left(-\\frac{t_{\\text{CS},j} - t_{\\text{SS},i}}{\\tau_{e}}\\right) \\right)\n$$\n这是需要实现的最终方程。对于每次试验，我们遍历每个复杂峰电位，通过对所有先前简单峰电位的贡献求和来计算该时间的资格迹值，将这些资格迹值相加，然后将结果乘以学习率 $\\eta$ 和表现增益 $\\kappa$ 的乘积。给定的参数是 $\\tau_{e} = 0.05$、$\\eta = 0.01$ 和 $\\kappa = 1.0$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and applies a model of cerebellar motor learning to predict\n    trial-by-trial changes in motor performance based on spike timings.\n    \"\"\"\n\n    # Define model parameters from the problem statement.\n    tau_e = 0.05  # exponential eligibility time constant in seconds\n    eta = 0.01    # dimensionless learning rate\n    kappa = 1.0   # dimensionless performance gain\n\n    # Pre-calculate the product of kappa and eta for efficiency.\n    k_eta_product = kappa * eta\n\n    # Define the test suite from the problem statement.\n    # Each test case is a list of trials.\n    # Each trial is a tuple of (simple_spike_times, complex_spike_times).\n    test_cases = [\n        # Test case 1 (general case, mixed timing)\n        [\n            ([0.10, 0.18, 0.32, 0.47, 0.55, 0.70, 0.82], [0.60]),\n            ([0.12, 0.19, 0.40, 0.58, 0.75], [0.20]),\n            ([0.08, 0.16, 0.30, 0.33, 0.51, 0.72], []),\n            ([0.05, 0.15, 0.25, 0.35, 0.45, 0.55, 0.65], [0.50]),\n            ([0.22, 0.29, 0.41, 0.67, 0.83], [0.68, 0.90]),\n        ],\n        # Test case 2 (boundary case, no complex spikes)\n        [\n            ([0.10, 0.20, 0.30], []),\n            ([0.15, 0.45, 0.80], []),\n            ([0.25, 0.35, 0.55, 0.75], []),\n            ([0.05, 0.65, 0.95], []),\n        ],\n        # Test case 3 (edge cases)\n        [\n            ([0.20, 0.40], [0.05]),  # Early complex spike\n            ([0.05, 0.06, 0.07], [0.08, 0.50]), # Multiple complex spikes\n            ([0.10, 0.11, 0.12, 0.13, 0.14], [0.12, 0.12]), # Simultaneous complex spikes\n        ]\n    ]\n\n    all_results = []\n    \n    # Iterate through each test case\n    for case in test_cases:\n        case_results = []\n        # Iterate through each trial in the current test case\n        for ss_times, cs_times in case:\n            total_eligibility_sum = 0.0\n\n            # If there are no complex spikes, no learning occurs.\n            if not cs_times:\n                case_results.append(0.0)\n                continue\n\n            # For each complex spike, calculate the eligibility trace value at that time.\n            for t_cs in cs_times:\n                eligibility_at_cs = 0.0\n                # Sum the contributions from all preceding simple spikes.\n                # The rule is causal, so only simple spikes t_ss  t_cs contribute.\n                for t_ss in ss_times:\n                    if t_ss  t_cs:\n                        delta_t = t_cs - t_ss\n                        eligibility_at_cs += np.exp(-delta_t / tau_e)\n                \n                total_eligibility_sum += eligibility_at_cs\n            \n            # Calculate the change in performance for the trial.\n            # delta_P = kappa * eta * sum(E(t_cs))\n            delta_p = k_eta_product * total_eligibility_sum\n            case_results.append(delta_p)\n            \n        all_results.append(case_results)\n\n    # Format the final output as a single-line string representation\n    # of a list of lists, with no spaces.\n    final_output_str = str(all_results).replace(\" \", \"\")\n    print(final_output_str)\n\nsolve()\n```"
        }
    ]
}