{
    "hands_on_practices": [
        {
            "introduction": "The final output of the cerebellar cortex is conveyed through the deep cerebellar nuclei (DCN). DCN neurons act as crucial integration hubs, balancing excitatory drive from mossy fiber collaterals with the powerful inhibitory output of Purkinje cells. This practice models a DCN neuron as a leaky integrator, a fundamental building block in computational neuroscience, allowing you to derive its steady-state voltage response to synaptic inputs. By solving this, you will gain a concrete understanding of how the cerebellum's ultimate output command is computed at the cellular level ().",
            "id": "4464877",
            "problem": "A deep cerebellar nucleus (DCN) neuron integrates convergent excitatory mossy fiber (MF) collateral input and inhibitory Purkinje cell (PC) input. Model the DCN membrane as an isopotential leaky integrator whose voltage $V(t)$ obeys charge conservation and Ohm’s law for the leak:\n$$\nC\\,\\frac{dV}{dt} \\;=\\; -\\,g_L\\left(V - E_L\\right) \\;+\\; I_M \\;-\\; I_P,\n$$\nwhere $C$ is the membrane capacitance, $g_L$ is the leak conductance, $E_L$ is the leak reversal potential, $I_M$ is the constant MF excitatory current, and $I_P$ is the constant PC inhibitory current. At time $t=0$, a sustained change in synaptic drive sets $I_M$ and $I_P$ to constant values.\n\nUsing only conservation of charge on the capacitor and the constitutive current–voltage relation for the leak channel as your fundamental starting point, derive the steady-state membrane potential $V_{\\infty}$ reached as $t \\to \\infty$. Then, for a DCN neuron with parameters $C = 150\\,\\mathrm{pF}$, $g_L = 20\\,\\mathrm{nS}$, $E_L = -65\\,\\mathrm{mV}$, $I_M = 0.40\\,\\mathrm{nA}$, and $I_P = 0.25\\,\\mathrm{nA}$, compute $V_{\\infty}$.\n\nExpress your final numerical answer in millivolts and round to three significant figures. State only the steady-state value $V_{\\infty}$.",
            "solution": "The problem requires the derivation of the steady-state membrane potential, $V_{\\infty}$, for a deep cerebellar nucleus (DCN) neuron modeled as a single-compartment leaky integrator, followed by a numerical calculation.\n\nThe derivation must begin from fundamental principles, namely the conservation of charge and the constitutive current-voltage relations for the membrane elements.\n\nThe principle of conservation of charge applied to the neuron's membrane, which acts as a capacitor, states that the rate of change of charge stored on the capacitor, $\\frac{dQ}{dt}$, must equal the total net current flowing across the membrane, $I_{net}$.\nThe charge $Q$ on a capacitor is related to the voltage $V$ across it and its capacitance $C$ by $Q = CV$. Therefore, the capacitive current is given by:\n$$\nI_C = \\frac{dQ}{dt} = C\\frac{dV}{dt}\n$$\nThis current represents the change in charge stored on the membrane. By convention, outward current is positive. The total current flowing across the membrane is the sum of the capacitive current, the leak current ($I_L$), the excitatory mossy fiber current ($I_M$), and the inhibitory Purkinje cell current ($I_P$). The sum of all currents flowing out of the neuron must be zero, which is a statement of Kirchhoff's current law or charge conservation.\n$$\nI_C + I_L + I_{syn} = 0\n$$\nwhere $I_{syn}$ is the net synaptic current.\n\nThe problem provides the following constitutive relations and definitions:\n1.  The leak current, $I_L$, follows Ohm's law. It is an outward ionic current given by $I_L = g_L(V - E_L)$, where $g_L$ is the leak conductance and $E_L$ is the leak reversal potential.\n2.  The mossy fiber current, $I_M$, is an excitatory input. By convention, excitatory currents are inward (depolarizing). We define the inward current as negative, so the contribution to the total outward current is $-I_M$.\n3.  The Purkinje cell current, $I_P$, is an inhibitory input. By convention, inhibitory currents are outward (hyperpolarizing). We define the outward current as positive, so its contribution to the total outward current is $+I_P$.\n\nSubstituting these components into the charge conservation equation:\n$$\nC\\frac{dV}{dt} + g_L(V - E_L) - I_M + I_P = 0\n$$\nRearranging this equation to match the form given in the problem statement confirms its physical basis:\n$$\nC\\frac{dV}{dt} = -g_L(V - E_L) + I_M - I_P\n$$\nThis is the differential equation governing the membrane potential $V(t)$.\n\nThe steady-state membrane potential, $V_{\\infty}$, is the value of $V$ reached as time $t$ approaches infinity ($t \\to \\infty$). At steady state, the membrane potential is constant, which implies that its time derivative is zero.\n$$\n\\frac{dV}{dt} = 0\n$$\nSubstituting this condition into the governing differential equation and replacing $V$ with $V_{\\infty}$:\n$$\n0 = -g_L(V_{\\infty} - E_L) + I_M - I_P\n$$\nWe now solve this algebraic equation for $V_{\\infty}$.\n$$\ng_L(V_{\\infty} - E_L) = I_M - I_P\n$$\n$$\nV_{\\infty} - E_L = \\frac{I_M - I_P}{g_L}\n$$\n$$\nV_{\\infty} = E_L + \\frac{I_M - I_P}{g_L}\n$$\nThis is the symbolic expression for the steady-state membrane potential.\n\nNext, we compute the numerical value of $V_{\\infty}$ using the provided parameters. The parameters must be in consistent SI units for calculation.\n- Leak conductance: $g_L = 20\\,\\mathrm{nS} = 20 \\times 10^{-9}\\,\\mathrm{S}$\n- Leak reversal potential: $E_L = -65\\,\\mathrm{mV} = -65 \\times 10^{-3}\\,\\mathrm{V}$\n- Excitatory current: $I_M = 0.40\\,\\mathrm{nA} = 0.40 \\times 10^{-9}\\,\\mathrm{A}$\n- Inhibitory current: $I_P = 0.25\\,\\mathrm{nA} = 0.25 \\times 10^{-9}\\,\\mathrm{A}$\n\nThe capacitance $C = 150\\,\\mathrm{pF}$ determines the time constant of the approach to steady state but is not needed for the steady-state value itself.\n\nFirst, calculate the net synaptic current, $I_{net} = I_M - I_P$:\n$$\nI_{net} = 0.40 \\times 10^{-9}\\,\\mathrm{A} - 0.25 \\times 10^{-9}\\,\\mathrm{A} = 0.15 \\times 10^{-9}\\,\\mathrm{A}\n$$\nNext, calculate the voltage shift due to this net current:\n$$\n\\frac{I_M - I_P}{g_L} = \\frac{0.15 \\times 10^{-9}\\,\\mathrm{A}}{20 \\times 10^{-9}\\,\\mathrm{S}} = \\frac{0.15}{20}\\,\\mathrm{V} = 0.0075\\,\\mathrm{V}\n$$\nThis voltage is equivalent to $7.5\\,\\mathrm{mV}$.\n\nFinally, add this voltage shift to the leak reversal potential to find $V_{\\infty}$:\n$$\nV_{\\infty} = -65 \\times 10^{-3}\\,\\mathrm{V} + 0.0075\\,\\mathrm{V} = -0.0575\\,\\mathrm{V}\n$$\nThe problem asks for the answer to be expressed in millivolts.\n$$\nV_{\\infty} = -0.0575\\,\\mathrm{V} \\times 1000\\,\\frac{\\mathrm{mV}}{\\mathrm{V}} = -57.5\\,\\mathrm{mV}\n$$\nThe problem also requires rounding to three significant figures. The value $-57.5$ has exactly three significant figures, so no further rounding is necessary.",
            "answer": "$$\\boxed{-57.5}$$"
        },
        {
            "introduction": "One of the most well-understood examples of cerebellar function is the adaptation of the vestibulo-ocular reflex (VOR), which ensures stable vision during head movements. The cerebellum adjusts the gain of this reflex by using retinal slip—the movement of the visual world on the retina—as an error signal to drive synaptic plasticity. This exercise challenges you to model this process by deriving the dynamics of VOR gain adaptation over time under a supervised learning rule. This practice bridges the gap between cellular plasticity and observable behavior, demonstrating how a simple, error-driven learning algorithm can produce precise motor adaptation ().",
            "id": "4464878",
            "problem": "A central function of the cerebellum in motor coordination is to adjust sensorimotor mappings to minimize sensory prediction errors. In the horizontal Vestibulo-Ocular Reflex (VOR), the goal is to reduce retinal slip, defined as the residual image velocity on the retina when the head rotates. Consider an advanced-graduate model in which the cerebellar flocculus implements a linear controller whose scalar gain $G(t)$ slowly adapts through supervised learning driven by climbing fiber signals encoding retinal slip. Assume small-angle rotations so that linear superposition holds.\n\nA subject undergoes sinusoidal head rotation with head angular velocity $v_{\\mathrm{head}}(t) = V_{0} \\sin(\\omega t)$ in radians per second. Prism goggles are introduced that magnify the apparent velocity of the visual scene by a positive factor $s$, so that the retinal slip error is $e(t) = s\\,v_{\\mathrm{head}}(t) - v_{\\mathrm{eye}}(t)$, where $v_{\\mathrm{eye}}(t)$ is the horizontal eye angular velocity in radians per second. The cerebellar gain $G(t)$ adapts according to a cycle-averaged supervised learning rule motivated by parallel fiber–Purkinje cell synaptic plasticity and climbing fiber error teaching signals (Long-Term Depression (LTD) correlational learning):\n\n$$\\frac{dG}{dt} = \\eta \\left\\langle v_{\\mathrm{head}}(t)\\, e(t) \\right\\rangle,$$\n\nwhere $\\eta > 0$ is a learning rate constant, and $\\langle \\cdot \\rangle$ denotes the average over one stimulus period $T = \\frac{2\\pi}{\\omega}$:\n\n$$\\left\\langle f(t) \\right\\rangle = \\frac{1}{T} \\int_{0}^{T} f(t)\\, dt.$$\n\nAssume that the motor plant from cerebellar output to eye velocity is well-approximated by a linear mapping (valid for small, low-frequency rotations) and that the adapted controller yields an expected eye velocity proportional to the head velocity with proportionality $G(t)$ determined by the above learning dynamics. The initial gain at time $t=0$ is $G(0) = G_{0}$.\n\nStarting from these definitions and assumptions, derive a closed-form expression for the expected eye velocity $v_{\\mathrm{eye}}(t)$ after adaptation time $t$ in terms of $V_{0}$, $\\omega$, $s$, $\\eta$, and $G_{0}$. Your final expression must be expressed in radians per second. No numerical values are provided; express your answer as a single closed-form analytic expression. No rounding is required.",
            "solution": "The task is to derive a closed-form expression for the expected eye velocity $v_{\\mathrm{eye}}(t)$, which requires first solving for the time-dependent cerebellar gain $G(t)$.\n\nThe adaptation of the gain $G(t)$ is governed by the learning rule:\n$$ \\frac{dG}{dt} = \\eta \\left\\langle v_{\\mathrm{head}}(t)\\, e(t) \\right\\rangle $$\nThe retinal slip error $e(t)$ is given by $e(t) = s\\,v_{\\mathrm{head}}(t) - v_{\\mathrm{eye}}(t)$. The problem states that the motor plant is a linear mapping $v_{\\mathrm{eye}}(t) = G(t) v_{\\mathrm{head}}(t)$. Substituting this into the error expression gives:\n$$ e(t) = s\\,v_{\\mathrm{head}}(t) - G(t) v_{\\mathrm{head}}(t) = (s - G(t)) v_{\\mathrm{head}}(t) $$\nNow, substitute this error expression back into the learning rule:\n$$ \\frac{dG}{dt} = \\eta \\left\\langle v_{\\mathrm{head}}(t)\\, (s - G(t)) v_{\\mathrm{head}}(t) \\right\\rangle = \\eta \\left\\langle (s - G(t)) v_{\\mathrm{head}}(t)^2 \\right\\rangle $$\nThe model assumes that the gain $G(t)$ adapts slowly compared to the stimulus period. We can therefore treat $G(t)$ as a constant over the single-cycle averaging interval and pull the term $(s - G(t))$ out of the averaging operator:\n$$ \\frac{dG}{dt} = \\eta (s - G(t)) \\left\\langle v_{\\mathrm{head}}(t)^2 \\right\\rangle $$\nNext, we calculate the time-averaged squared head velocity, $\\left\\langle v_{\\mathrm{head}}(t)^2 \\right\\rangle$, where $v_{\\mathrm{head}}(t) = V_{0} \\sin(\\omega t)$.\n$$ \\left\\langle v_{\\mathrm{head}}(t)^2 \\right\\rangle = \\frac{1}{T} \\int_{0}^{T} (V_{0} \\sin(\\omega t))^2 dt = \\frac{V_{0}^2}{T} \\int_{0}^{T} \\sin^2(\\omega t) dt $$\nUsing the identity $\\sin^2(\\theta) = \\frac{1}{2}(1 - \\cos(2\\theta))$, the integral becomes:\n$$ \\int_{0}^{T} \\sin^2(\\omega t) dt = \\int_{0}^{T} \\frac{1 - \\cos(2\\omega t)}{2} dt = \\frac{1}{2} \\left[ t - \\frac{\\sin(2\\omega t)}{2\\omega} \\right]_{0}^{T} $$\nSubstituting the period $T = 2\\pi/\\omega$ and evaluating the definite integral:\n$$ \\frac{1}{2} \\left[ T - \\frac{\\sin(2\\omega T)}{2\\omega} \\right] = \\frac{1}{2} \\left[ T - \\frac{\\sin(4\\pi)}{2\\omega} \\right] = \\frac{T}{2} $$\nTherefore, the average is:\n$$ \\left\\langle v_{\\mathrm{head}}(t)^2 \\right\\rangle = \\frac{V_{0}^2}{T} \\left( \\frac{T}{2} \\right) = \\frac{V_{0}^2}{2} $$\nSubstituting this result back into the differential equation for $G(t)$:\n$$ \\frac{dG}{dt} = \\eta (s - G(t)) \\frac{V_{0}^2}{2} $$\nThis is a first-order linear ordinary differential equation. Letting $k = \\frac{\\eta V_{0}^2}{2}$, the equation is $\\frac{dG}{dt} = k(s - G)$. We can solve this by separation of variables:\n$$ \\frac{dG}{s - G} = k\\,dt $$\nIntegrating both sides from time $0$ to $t$ (and gain $G_0$ to $G(t)$):\n$$ \\int_{G_0}^{G(t)} \\frac{dG'}{s - G'} = \\int_{0}^{t} k\\,dt' \\implies \\left[ -\\ln|s - G'| \\right]_{G_0}^{G(t)} = kt $$\n$$ \\ln\\left(\\frac{|s - G_0|}{|s - G(t)|}\\right) = kt \\implies |s - G(t)| = |s - G_0| e^{-kt} $$\nAssuming monotonic adaptation, we can remove the absolute values:\n$$ s - G(t) = (s - G_0) e^{-kt} $$\nSolving for $G(t)$ and substituting back the expression for $k$:\n$$ G(t) = s - (s - G_0) \\exp\\left(-\\frac{\\eta V_{0}^2}{2} t\\right) $$\nFinally, the expected eye velocity is $v_{\\mathrm{eye}}(t) = G(t) v_{\\mathrm{head}}(t)$. Substituting the expressions for $G(t)$ and $v_{\\mathrm{head}}(t) = V_0 \\sin(\\omega t)$:\n$$ v_{\\mathrm{eye}}(t) = \\left[ s - (s - G_0) \\exp\\left(-\\frac{\\eta V_{0}^2}{2} t\\right) \\right] V_0 \\sin(\\omega t) $$\nThis is the required closed-form expression.",
            "answer": "$$ \\boxed{\\left(s - (s - G_{0}) \\exp\\left(-\\frac{\\eta V_{0}^{2}}{2} t\\right)\\right) V_{0} \\sin(\\omega t)} $$"
        },
        {
            "introduction": "To fully appreciate the role of synaptic plasticity in motor learning, it is instructive to consider what happens when it is absent. This practice uses a simplified model of prism adaptation, a task where the brain must learn a new visuomotor mapping, to explore the consequences of a hypothetical 'lesion' that eliminates long-term depression (LTD) at the parallel fiber-Purkinje cell synapse. By deriving the error dynamics in the absence of this key learning mechanism, you will see why plasticity is not merely helpful but essential for adaptation. This exercise provides a powerful conceptual demonstration of the fundamental link between synaptic change and the ability to learn from error ().",
            "id": "4464819",
            "problem": "Consider a prism adaptation task in which the visual field is rotated by a constant angle, modeled as a fixed visuomotor perturbation of magnitude $p$. Assume a canonical Marr–Albus–Ito cerebellar circuit in which Parallel Fiber–Purkinje Cell (PF–PC) Long-Term Depression (LTD) reduces Purkinje Cell (PC) simple-spike output in error-generating contexts, thereby disinhibiting Deep Cerebellar Nuclei (DCN) and producing feedforward motor corrections that reduce endpoint error. Climbing Fiber (CF) input conveys a scalar performance error signal. Work under the following scientifically standard and tractable assumptions for small angles:\n\n- PC simple-spike output is a linear function of a fixed, normalized context vector, so the net PC output can be represented as a scalar $s_{t}$ on trial $t$.\n- DCN output is disinhibited by reduced PC output and drives a motor correction $u_{t}$ linearly, with a constant gain, under a baseline that yields zero correction pre-perturbation.\n- The observed endpoint error is the sum of the fixed perturbation and the negative of the motor correction, and the CF error signal is proportional to this observed error.\n- PF–PC LTD implements supervised learning that updates the scalar PC output state $s_{t}$ proportionally to the CF error, reducing PC output in error-associated contexts.\n\nStarting from these bases and linear approximations, derive from first principles how the removal (loss) of PF–PC LTD (i.e., setting the LTD learning rate to zero) alters the error dynamics in prism adaptation, and show what this implies for steady-state performance following extended training. Then, for a prism that imposes a constant visuomotor rotation of magnitude $p = \\frac{\\pi}{12}$, compute the predicted residual endpoint error after a long training block ($t \\to \\infty$) when PF–PC LTD is completely absent (learning rate zero) and other forms of adaptation are negligible.\n\nExpress your final numerical answer for the residual endpoint error in radians, and round to four significant figures. Do not include units in your final boxed answer; however, the answer must correspond to radians as specified here.",
            "solution": "The problem asks for a derivation of the error dynamics in a prism adaptation task under a simplified model of cerebellar learning, specifically focusing on the consequence of absent Long-Term Depression (LTD) at the Parallel Fiber-Purkinje Cell (PF-PC) synapse. Finally, it requires the computation of the residual endpoint error under this condition for a given perturbation.\n\nThe first step is to formalize the assumptions provided in the problem statement into a mathematical model. Let $t$ be the trial number.\n\nLet $p$ be the constant magnitude of the visuomotor perturbation.\nLet $s_t$ be the scalar PC simple-spike output state on trial $t$. We can define the initial, pre-perturbation state as $s_0$ at $t=0$.\nLet $u_t$ be the motor correction generated on trial $t$.\nLet $e_t$ be the observed endpoint error on trial $t$.\n\nThe assumptions are translated into equations as follows:\n\n1.  **DCN Output and Motor Correction:** The motor correction $u_t$ is driven linearly by the disinhibition of Deep Cerebellar Nuclei (DCN), which results from a reduction in PC output. We can model this as a linear relationship with a constant gain, $k > 0$, relative to a baseline state. Let the baseline PC output be $s_0$. The reduction in PC output is $(s_0 - s_t)$. The problem states a baseline of zero correction pre-perturbation, which corresponds to the state $s_0$.\n    $$u_t = k(s_0 - s_t)$$\n    At trial $t=0$, before any learning has occurred from the perturbation, $s_t = s_0$, so the initial correction is $u_0 = k(s_0 - s_0) = 0$.\n\n2.  **Endpoint Error:** The observed endpoint error $e_t$ is explicitly defined as the sum of the fixed perturbation $p$ and the negative of the motor correction $u_t$.\n    $$e_t = p - u_t$$\n    A positive correction $u_t$ thus serves to reduce the error from the perturbation value $p$.\n\n3.  **Climbing Fiber Signal and Learning Rule:** The Climbing Fiber (CF) input conveys an error signal proportional to the observed error, let's say $e_{CF, t} = \\gamma e_t$ with a proportionality constant $\\gamma > 0$. PF-PC LTD updates the PC output state $s_t$. This update rule dictates that a positive error signal leads to a reduction in subsequent PC output (LTD). This can be written as a recurrence relation for $s_t$:\n    $$s_{t+1} - s_t = -\\beta e_{CF, t}$$\n    where $\\beta \\ge 0$ is the learning rate associated with LTD. Combining with the CF signal definition, we get:\n    $$s_{t+1} = s_t - \\beta \\gamma e_t$$\n\nNow, we derive the dynamics of the endpoint error $e_t$ from trial to trial.\nThe error on trial $t+1$ is given by:\n$$e_{t+1} = p - u_{t+1}$$\nUsing the expression for the motor correction $u_{t+1}$:\n$$e_{t+1} = p - k(s_0 - s_{t+1})$$\nNow, substitute the learning rule for $s_{t+1}$:\n$$e_{t+1} = p - k(s_0 - (s_t - \\beta \\gamma e_t))$$\n$$e_{t+1} = p - k(s_0 - s_t) - k\\beta\\gamma e_t$$\nWe recognize the term $p - k(s_0 - s_t)$ as the definition of the error on the previous trial, $e_t$. Substituting this gives the recurrence relation for the error:\n$$e_{t+1} = e_t - k\\beta\\gamma e_t$$\n$$e_{t+1} = (1 - k\\beta\\gamma) e_t$$\n\nThis linear recurrence relation describes the error dynamics.\nUnder normal conditions where PF-PC LTD is present, the learning rate $\\beta > 0$. For learning to be stable and convergent, the constants must satisfy $0  k\\beta\\gamma  2$. In this case, the term $|1 - k\\beta\\gamma|  1$, and as $t \\to \\infty$, the error $e_t = e_0 (1 - k\\beta\\gamma)^t$ exponentially decays to zero. This represents successful adaptation, where the steady-state error is $\\lim_{t \\to \\infty} e_t = 0$. The system learns to produce a motor correction $u_\\infty$ that perfectly cancels the perturbation $p$.\n\nThe problem asks us to analyze the case where PF-PC LTD is lost. This is modeled by setting the learning rate to zero: $\\beta = 0$.\nSubstituting $\\beta = 0$ into the error dynamics equation:\n$$e_{t+1} = (1 - k \\cdot 0 \\cdot \\gamma) e_t$$\n$$e_{t+1} = e_t$$\nThis result shows that the error on any given trial is identical to the error on the previous trial. The error dynamics are static; the error does not change over time.\n\nThis implies that for all trials $t \\ge 0$, the error $e_t$ remains equal to the initial error $e_0$. To find the steady-state error, we only need to compute the initial error. At trial $t=0$, no learning has taken place, so the motor correction $u_0 = 0$. The initial error is:\n$$e_0 = p - u_0 = p - 0 = p$$\nTherefore, with $\\beta=0$, the error for all subsequent trials is constant:\n$$e_t = p \\quad \\text{for all } t \\ge 0$$\n\nThe implication for steady-state performance ($t \\to \\infty$) is that the system fails to adapt. The residual endpoint error does not decrease from its initial value and remains equal to the full magnitude of the perturbation, $p$.\n$$\\lim_{t \\to \\infty} e_t = p$$\n\nThe problem asks for the numerical value of this residual error for a perturbation $p = \\frac{\\pi}{12}$ radians. Based on our derivation, the residual error is simply $p$.\n$$\\text{Residual Error} = \\frac{\\pi}{12}$$\nCalculating the numerical value:\n$$\\frac{\\pi}{12} \\approx \\frac{3.14159265}{12} \\approx 0.26179938 \\text{ radians}$$\nRounding to four significant figures, we get $0.2618$.",
            "answer": "$$\\boxed{0.2618}$$"
        }
    ]
}