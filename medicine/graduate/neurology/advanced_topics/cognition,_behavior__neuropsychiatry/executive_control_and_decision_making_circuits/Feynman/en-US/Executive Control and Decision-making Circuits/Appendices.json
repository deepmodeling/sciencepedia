{
    "hands_on_practices": [
        {
            "introduction": "At the heart of executive control lies the ability to learn from outcomes and adapt future behavior. This exercise delves into the computational core of this process using a reinforcement learning framework. You will calculate a Q-learning update, which models how corticostriatal circuits adjust the value of an action based on the discrepancy between expected and received rewards—a process thought to be driven by phasic dopamine signals . This practice provides a quantitative look at the fundamental learning rule that underpins much of adaptive decision-making.",
            "id": "4479810",
            "problem": "A cortico-basal ganglia-thalamo-cortical loop model of executive control posits that corticostriatal synapses in the dorsal striatum encode state-action values $Q_{t}(s,a)$ that guide action selection through the basal ganglia output nuclei. Phasic dopamine release from the Ventral Tegmental Area (VTA) and Substantia Nigra pars compacta (SNc) conveys a reward prediction error (RPE), which modulates synaptic plasticity and updates these values. Under the standard reinforcement learning interpretation, the RPE at time $t$ compares experienced and expected return, and striatal value updates are proportional to this error with proportionality constant equal to the learning rate $\\alpha$. The expected return incorporates an immediate reward $r_{t}$ and a discounted estimate of future return using discount factor $\\gamma$ and the maximal estimated value in the next state $s'$ over available actions.\n\nConsider a single trial in which the prefrontal cortical state $s$ leads to the selection of action $a$, yielding immediate reward $r_{t}$ and transitioning to state $s'$. The current striatal estimate for the executed state-action pair is $Q_{t}(s,a)$. The next-state maximal value is $\\max_{a'} Q_{t}(s',a')$. Assume parameters $\\alpha=0.1$, $\\gamma=0.9$, $Q_{t}(s,a)=0.5$, $r_{t}=1$, and $\\max_{a'} Q_{t}(s',a')=0.8$. Using the definition of reward prediction error and the principle that the update to $Q_{t}(s,a)$ is proportional to this error, compute the updated value $Q_{t+1}(s,a)$. Express your final answer as a single real number, with no additional units or symbols. No rounding is required.",
            "solution": "The user wants me to validate and solve a problem from the field of computational neuroscience, specifically concerning reinforcement learning models of the basal ganglia.\n\n### Step 1: Extract Givens\n-   Model type: Cortico-basal ganglia-thalamo-cortical loop.\n-   Encoded values: State-action values $Q_{t}(s,a)$.\n-   Learning signal: Reward Prediction Error (RPE), conveyed by phasic dopamine.\n-   Update rule: Striatal value updates are proportional to the RPE.\n-   Proportionality constant: Learning rate $\\alpha$.\n-   Expected return components: Immediate reward $r_{t}$ and discounted future return.\n-   Discount factor: $\\gamma$.\n-   Future return estimate: Maximal estimated value in the next state, $\\max_{a'} Q_{t}(s',a')$.\n-   Given parameters:\n    -   $\\alpha = 0.1$\n    -   $\\gamma = 0.9$\n    -   $Q_{t}(s,a) = 0.5$ (current value of the executed state-action pair)\n    -   $r_{t} = 1$ (immediate reward)\n    -   $\\max_{a'} Q_{t}(s',a') = 0.8$ (maximal value in the successor state)\n-   Objective: Compute the updated value $Q_{t+1}(s,a)$.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientific Grounding**: The problem is well-grounded in computational neuroscience and reinforcement learning theory. It accurately describes the Temporal Difference (TD) learning framework, specifically the Q-learning algorithm, which is a canonical model for dopamine-mediated learning in the basal ganglia. All terms ($Q$-value, RPE, learning rate, discount factor) and their relationships are standard in this field.\n2.  **Well-Posedness**: The problem is well-posed. It provides all necessary numerical values and a clear, unambiguous objective. A unique solution can be calculated.\n3.  **Objectivity**: The problem is stated in precise, objective, and formal language. There are no subjective or ambiguous elements.\n4.  **Flaw Checklist**:\n    -   *Scientific Unsoundness*: None. The model is standard.\n    -   *Non-Formalizable/Irrelevant*: None. The problem is perfectly formalizable and directly relevant to the stated topic.\n    -   *Incomplete/Contradictory*: None. The setup is complete and consistent.\n    -   *Unrealistic/Infeasible*: None. The numerical values are typical for such simulations.\n    -   *Ill-Posed*: None. It leads to a unique, stable solution.\n    -   *Pseudo-Profound/Trivial*: While the final calculation is arithmetic, the problem requires a correct interpretation of the described neurobiological model into a specific mathematical formula, which is a non-trivial conceptual step.\n    -   *Outside Verifiability*: None. The result is mathematically verifiable.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A full solution will be provided.\n\nThe problem describes the update mechanism for a state-action value function, $Q(s,a)$, which is a central concept in reinforcement learning, particularly in the Q-learning algorithm. The update rule formalizes how an agent learns from experience by adjusting its value estimates based on new information.\n\nThe problem states that the update to the value $Q_{t}(s,a)$ is proportional to the Reward Prediction Error (RPE), with the learning rate $\\alpha$ as the constant of proportionality. The RPE, which we can denote as $\\delta_{t}$, is the difference between the experienced return and the expected return.\n\nThe expected return is simply the current estimate of the value of taking action $a$ in state $s$, which is $Q_{t}(s,a)$.\n\nThe experienced return, often called the TD (Temporal Difference) target, is the sum of the immediate reward received, $r_{t}$, and the discounted value of the best possible action in the next state, $s'$. This is given by the expression $r_{t} + \\gamma \\max_{a'} Q_{t}(s',a')$. The discount factor $\\gamma$ determines the present value of future rewards.\n\nTherefore, the RPE is mathematically defined as:\n$$\n\\delta_{t} = \\left( r_{t} + \\gamma \\max_{a'} Q_{t}(s',a') \\right) - Q_{t}(s,a)\n$$\nThe update to the Q-value, $\\Delta Q_{t}(s,a)$, is this error scaled by the learning rate $\\alpha$:\n$$\n\\Delta Q_{t}(s,a) = \\alpha \\cdot \\delta_{t}\n$$\nThe new Q-value at time $t+1$, $Q_{t+1}(s,a)$, is the old value plus this update:\n$$\nQ_{t+1}(s,a) = Q_{t}(s,a) + \\Delta Q_{t}(s,a)\n$$\nSubstituting the expressions for the update and the error, we arrive at the complete Q-learning update rule:\n$$\nQ_{t+1}(s,a) = Q_{t}(s,a) + \\alpha \\left[ \\left( r_{t} + \\gamma \\max_{a'} Q_{t}(s',a') \\right) - Q_{t}(s,a) \\right]\n$$\nWe are given the following values:\n-   Learning rate $\\alpha = 0.1$\n-   Discount factor $\\gamma = 0.9$\n-   Current Q-value $Q_{t}(s,a) = 0.5$\n-   Immediate reward $r_{t} = 1$\n-   Maximal next-state Q-value $\\max_{a'} Q_{t}(s',a') = 0.8$\n\nFirst, we compute the TD target, which is the term inside the brackets representing the new estimate of the value of $(s,a)$:\n$$\n\\text{TD Target} = r_{t} + \\gamma \\max_{a'} Q_{t}(s',a') = 1 + (0.9)(0.8) = 1 + 0.72 = 1.72\n$$\nNext, we compute the RPE, $\\delta_{t}$:\n$$\n\\delta_{t} = \\text{TD Target} - Q_{t}(s,a) = 1.72 - 0.5 = 1.22\n$$\nFinally, we compute the updated value $Q_{t+1}(s,a)$ by adding a fraction $\\alpha$ of the RPE to the old value:\n$$\nQ_{t+1}(s,a) = Q_{t}(s,a) + \\alpha \\cdot \\delta_{t} = 0.5 + 0.1 \\cdot (1.22)\n$$\n$$\nQ_{t+1}(s,a) = 0.5 + 0.122\n$$\n$$\nQ_{t+1}(s,a) = 0.622\n$$\nThus, the updated striatal estimate for the state-action pair $(s,a)$ is $0.622$.",
            "answer": "$$\n\\boxed{0.622}\n$$"
        },
        {
            "introduction": "Beyond learning action values, a key executive function is committing to a choice based on available evidence. The Drift Diffusion Model (DDM) provides a powerful framework for understanding the temporal dynamics of this decision process. This problem asks you to derive the relationship between the strength of evidence and the speed of a decision in a simplified, low-noise scenario . By working through this approximation, you will gain insight into how models like the DDM bridge the gap between neural activity (evidence accumulation) and observable behavior (decision time).",
            "id": "4479776",
            "problem": "In a simplified account of executive control and decision-making circuits, cortical populations in prefrontal and parietal cortices accumulate evidence over time and trigger a choice when an internal decision variable reaches a bound, with basal ganglia pathways mediating threshold crossing into action. A well-tested normative and mechanistic description of such accumulation is the Drift Diffusion Model (DDM), which models the decision variable as a one-dimensional stochastic process with constant drift and additive white noise. Consider the stochastic differential equation for the decision variable $X_{t}$,\n$$\n\\mathrm{d}X_{t} \\;=\\; v\\,\\mathrm{d}t \\;+\\; \\sigma\\,\\mathrm{d}W_{t},\n$$\nwhere $v$ is the drift rate reflecting the mean momentary evidence, $\\sigma$ is the diffusion amplitude reflecting within-trial variability, and $W_{t}$ is a standard Wiener process. Let the decision bounds be symmetric and absorbing at $X=\\pm a$ with $a0$, and let the process start at $X_{0}=0$. The decision time $T$ is the first-passage time to either bound. You may assume the starting point is unbiased and that any non-decision delays (for sensory encoding and motor execution) are excluded from $T$.\n\nTask: Starting from the stochastic differential equation and the definition of first-passage time and without invoking any pre-memorized closed-form first-passage-time formulas, derive the leading-order approximation of the mean decision time $E[T]$ in the limit of negligible within-trial noise. Your derivation must rely on first principles appropriate to this setting (e.g., deterministic limits of stochastic dynamics and absorbing boundary conditions) and must make explicit any assumptions used.\n\nThen, apply your approximation to the following neural recording scenario, in which simultaneous population activity from the lateral intraparietal area (LIP) is analyzed during a fixed-coherence motion discrimination task. The inferred parameters of the DDM mapping for a given condition are:\n- Drift rate $v = +0.394\\ \\mathrm{a.u.}/\\mathrm{s}$ (in arbitrary evidence units per second),\n- Bound magnitude $a = 0.123\\ \\mathrm{a.u.}$,\n- Diffusion amplitude $\\sigma = 0.030\\ \\mathrm{a.u.}/\\sqrt{\\mathrm{s}}$.\n\nCompute the approximate mean decision time using your derivation. Round your numerical answer to four significant figures. Express the final time in milliseconds.\n\nIn your derivation, clearly state the conditions on the neural data and task under which the negligible-noise approximation would be expected to hold in practice (e.g., properties of the drift, the noise, the bounds, and circuit timescales). Your final reported answer must be a single number corresponding to the time in milliseconds, rounded as specified.",
            "solution": "The problem asks for a derivation of the leading-order approximation for the mean decision time $E[T]$ in a Drift Diffusion Model (DDM) under the limit of negligible noise, followed by a numerical application.\n\nThe dynamics of the decision variable $X_t$ are governed by the stochastic differential equation (SDE):\n$$\n\\mathrm{d}X_{t} = v\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_{t}\n$$\nwhere $v$ is the drift rate, $\\sigma$ is the diffusion amplitude, and $W_t$ is a standard Wiener process. The process starts at $X_0 = 0$ and terminates when $X_t$ first reaches one of the absorbing boundaries at $\\pm a$, where $a  0$.\n\nThe task requires deriving the leading-order approximation of the mean decision time, $E[T]$, in the limit of \"negligible within-trial noise.\" Mathematically, this corresponds to the limit where the diffusion amplitude $\\sigma$ approaches zero, i.e., $\\sigma \\to 0$.\n\nIn this limit, the stochastic term $\\sigma\\,\\mathrm{d}W_t$ in the SDE vanishes. The equation describing the evolution of the decision variable $X_t$ reduces to a deterministic ordinary differential equation (ODE):\n$$\n\\frac{\\mathrm{d}X_t}{\\mathrm{d}t} = v\n$$\nThis ODE describes a purely ballistic motion of the decision variable, driven solely by the mean evidence drift $v$. To find the trajectory $X_t$, we integrate this equation with respect to time from the initial time $t=0$ to a subsequent time $T$. The initial condition is given as $X_0 = 0$.\n$$\n\\int_{X_0}^{X_T} \\mathrm{d}X = \\int_{0}^{T} v\\,\\mathrm{d}t\n$$\nGiven that $v$ is a constant, the integration yields:\n$$\nX_T - X_0 = v(T - 0)\n$$\nSubstituting the initial condition $X_0 = 0$, we obtain the deterministic trajectory:\n$$\nX_T = vT\n$$\nThe decision time $T$ is the first-passage time to one of the boundaries at $X = \\pm a$. For the given drift rate $v = +0.394\\ \\mathrm{a.u.}/\\mathrm{s}$, which is a positive constant, the decision variable $X_t$ will increase linearly from $X_0 = 0$. Consequently, it will inevitably reach the positive boundary at $X = +a$ and will never reach the negative boundary at $X = -a$.\n\nTherefore, the decision is made at the time $T$ when the decision variable reaches the value $a$. We set $X_T = a$ in the trajectory equation:\n$$\na = vT\n$$\nSolving for the decision time $T$ gives:\n$$\nT = \\frac{a}{v}\n$$\nIn this deterministic limit ($\\sigma=0$), the decision time $T$ is no longer a random variable; it is a single, determined value. Thus, its expectation, $E[T]$, is simply this value. This represents the leading-order (or zeroth-order) approximation for the mean decision time in a low-noise regime.\n$$\nE[T] \\approx \\frac{a}{v}\n$$\nThis derivation fulfills the requirement of using first principles without invoking pre-memorized formulas for first-passage times.\n\nThe validity of this negligible-noise approximation depends on the relative contribution of the drift and diffusion terms. The approximation $E[T] \\approx a/v$ is accurate when the deterministic drift towards the correct boundary is the dominant factor determining the decision time. The full expression for the mean decision time is $E[T] = \\frac{a}{v} \\tanh\\left(\\frac{av}{\\sigma^2}\\right)$. Our approximation is valid when $\\tanh\\left(\\frac{av}{\\sigma^2}\\right) \\approx 1$, which occurs when its argument is large:\n$$\n\\frac{av}{\\sigma^2} \\gg 1\n$$\nIn practical terms, this condition holds when:\n1. The drift rate $|v|$ is large (strong, unambiguous evidence leading to a rapid decision).\n2. The noise amplitude $\\sigma$ is small (low internal variability or neuronal noise).\n3. The decision boundary $|a|$ is large, though this effect is modulated by $v$ and $\\sigma^2$. A large $a$ can increase the influence of noise over a longer integration period, but the ratio $\\frac{av}{\\sigma^2}$ is what matters. This ratio can be interpreted as a measure of the signal-to-noise ratio of the integrated evidence at the point of decision.\n\nFor the specific neural recording scenario, the parameters are:\n- Drift rate $v = 0.394\\ \\mathrm{a.u.}/\\mathrm{s}$\n- Bound magnitude $a = 0.123\\ \\mathrm{a.u.}$\n- Diffusion amplitude $\\sigma = 0.030\\ \\mathrm{a.u.}/\\sqrt{\\mathrm{s}}$\n\nLet us first verify the validity of the approximation for this parameter set by computing the dimensionless quantity $\\frac{av}{\\sigma^2}$:\n$$\n\\frac{av}{\\sigma^2} = \\frac{(0.123)(0.394)}{(0.030)^2} = \\frac{0.048462}{0.0009} \\approx 53.846...\n$$\nSince $53.8 \\gg 1$, the condition for the negligible-noise approximation is met, and we can expect our derived formula to be highly accurate.\n\nNow, we compute the approximate mean decision time using the derived formula $T \\approx a/v$:\n$$\nT \\approx \\frac{a}{v} = \\frac{0.123\\ \\mathrm{a.u.}}{0.394\\ \\mathrm{a.u.}/\\mathrm{s}}\n$$\nThe arbitrary units ($\\mathrm{a.u.}$) cancel, yielding a result in seconds ($\\mathrm{s}$):\n$$\nT \\approx 0.31218274... \\ \\mathrm{s}\n$$\nThe problem requires the answer to be expressed in milliseconds ($\\mathrm{ms}$) and rounded to four significant figures.\n$$\nT_{\\mathrm{ms}} \\approx (0.31218274... \\ \\mathrm{s}) \\times (1000 \\ \\mathrm{ms}/\\mathrm{s}) = 312.18274... \\ \\mathrm{ms}\n$$\nRounding this value to four significant figures gives $312.2 \\ \\mathrm{ms}$.",
            "answer": "$$\\boxed{312.2}$$"
        },
        {
            "introduction": "Executive control is not monolithic; it often involves managing a conflict between different control strategies, such as goal-directed planning and ingrained habits. This exercise explores how the brain might arbitrate between these systems, modeled here as Model-Based (MB) and Model-Free (MF) controllers. You will derive the optimal weighting for combining information from these two sources based on their reliability, or variance—a principle known as inverse-variance weighting . This practice illuminates a normative solution for how executive circuits can flexibly and efficiently integrate multiple streams of information to guide behavior.",
            "id": "4479773",
            "problem": "Consider a cortico–basal ganglia decision-making architecture in which two subsystems contribute action values that are linearly combined to form a single control signal. Specifically, a planning subsystem associated with prefrontal cortex (PFC) implements a Model-Based (MB) strategy, and a habitual subsystem associated with dorsolateral striatum (DLS) implements a Model-Free (MF) strategy. Let the two subsystems produce unbiased estimates of a latent action value $x$, denoted by $\\hat{x}_{\\mathrm{MB}}$ and $\\hat{x}_{\\mathrm{MF}}$, respectively. Assume that, conditional on $x$, each estimate is corrupted by independent, zero-mean Gaussian noise with variances $\\sigma_{\\mathrm{MB}}^{2}$ and $\\sigma_{\\mathrm{MF}}^{2}$, respectively.\n\nThe arbitration mechanism constructs a mixture policy value\n$$\nQ_{\\mathrm{mix}} \\;=\\; w\\,Q_{\\mathrm{MB}} \\;+\\; \\left(1-w\\right)\\,Q_{\\mathrm{MF}},\n$$\nwhere $Q_{\\mathrm{MB}}$ and $Q_{\\mathrm{MF}}$ are proportional to $\\hat{x}_{\\mathrm{MB}}$ and $\\hat{x}_{\\mathrm{MF}}$, and $w \\in [0,1]$ is chosen to minimize the variance of the mixture while preserving unbiasedness under the assumptions given above. Starting from the stated probabilistic model and the requirement that the mixture be unbiased and of minimum variance, derive the closed-form expression for $w$ as a function of $\\sigma_{\\mathrm{MB}}^{2}$ and $\\sigma_{\\mathrm{MF}}^{2}$.\n\nThen, taking $\\sigma_{\\mathrm{MB}}^{2} = 0.25$ and $\\sigma_{\\mathrm{MF}}^{2} = 1$, compute the numerical value of $w$. Briefly interpret the resulting mixture policy in terms of the relative influence of the MB and MF pathways in action selection.\n\nExpress the final arbitration weight as an exact fraction or a decimal number; no rounding is required. No units should be included with the final number.",
            "solution": "### Step 1: Extract Givens\n- Two subsystems provide estimates of a latent action value $x$: a Model-Based (MB) subsystem with estimate $\\hat{x}_{\\mathrm{MB}}$ and a Model-Free (MF) subsystem with estimate $\\hat{x}_{\\mathrm{MF}}$.\n- Conditional on $x$, the estimates are unbiased: $E[\\hat{x}_{\\mathrm{MB}} | x] = x$ and $E[\\hat{x}_{\\mathrm{MF}} | x] = x$.\n- The estimates are corrupted by independent, zero-mean Gaussian noise.\n- The variances of the estimators are $\\mathrm{Var}(\\hat{x}_{\\mathrm{MB}}) = \\sigma_{\\mathrm{MB}}^{2}$ and $\\mathrm{Var}(\\hat{x}_{\\mathrm{MF}}) = \\sigma_{\\mathrm{MF}}^{2}$.\n- The estimators are independent, which means their covariance is zero: $\\mathrm{Cov}(\\hat{x}_{\\mathrm{MB}}, \\hat{x}_{\\mathrm{MF}}) = 0$.\n- A mixture policy value is formed: $Q_{\\mathrm{mix}} = w\\,Q_{\\mathrm{MB}} + (1-w)\\,Q_{\\mathrm{MF}}$, where $w \\in [0,1]$.\n- The action values are proportional to the estimates: $Q_{\\mathrm{MB}} \\propto \\hat{x}_{\\mathrm{MB}}$ and $Q_{\\mathrm{MF}} \\propto \\hat{x}_{\\mathrm{MF}}$. Let the constant of proportionality be $k  0$, so $Q_{\\mathrm{MB}} = k\\hat{x}_{\\mathrm{MB}}$ and $Q_{\\mathrm{MF}} = k\\hat{x}_{\\mathrm{MF}}$.\n- Objective: Choose $w$ to minimize the variance of the mixture while preserving unbiasedness.\n- Numerical values for calculation: $\\sigma_{\\mathrm{MB}}^{2} = 0.25$ and $\\sigma_{\\mathrm{MF}}^{2} = 1$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is scientifically grounded, presenting a simplified but standard model used in computational neuroscience and reinforcement learning to study the arbitration between different decision-making strategies. The model uses established statistical principles, such as combining unbiased estimators. The language is precise and objective. The problem is well-posed, as it asks for the minimization of a quadratic function of the weight $w$, which has a unique solution. The setup is self-contained and consistent. There are no contradictions, ambiguities, or physically impossible conditions. The problem is therefore deemed **valid**.\n\n### Step 3: Proceed to Solution\n\nThe problem requires us to find the optimal weight $w$ that minimizes the variance of a combined estimator, subject to the constraint that the estimator remains unbiased.\n\nLet's define the combined estimator of the latent value $x$ as $\\hat{x}_{\\mathrm{mix}}$. Since $Q_{\\mathrm{mix}}$ is proportional to $\\hat{x}_{\\mathrm{mix}}$, minimizing the variance of $Q_{\\mathrm{mix}}$ is equivalent to minimizing the variance of $\\hat{x}_{\\mathrm{mix}}$.\nThe mixture value $Q_{\\mathrm{mix}}$ is given by:\n$$\nQ_{\\mathrm{mix}} = w\\,Q_{\\mathrm{MB}} + (1-w)\\,Q_{\\mathrm{MF}} = w (k\\hat{x}_{\\mathrm{MB}}) + (1-w) (k\\hat{x}_{\\mathrm{MF}}) = k(w\\hat{x}_{\\mathrm{MB}} + (1-w)\\hat{x}_{\\mathrm{MF}})\n$$\nLet the combined estimator be $\\hat{x}_{\\mathrm{mix}} = w\\hat{x}_{\\mathrm{MB}} + (1-w)\\hat{x}_{\\mathrm{MF}}$.\n\nFirst, we verify the unbiasedness of this combined estimator. The expected value of $\\hat{x}_{\\mathrm{mix}}$ is:\n$$\nE[\\hat{x}_{\\mathrm{mix}}] = E[w\\hat{x}_{\\mathrm{MB}} + (1-w)\\hat{x}_{\\mathrm{MF}}]\n$$\nBy linearity of expectation:\n$$\nE[\\hat{x}_{\\mathrm{mix}}] = w E[\\hat{x}_{\\mathrm{MB}}] + (1-w) E[\\hat{x}_{\\mathrm{MF}}]\n$$\nGiven that both estimators are unbiased, we have $E[\\hat{x}_{\\mathrm{MB}}] = x$ and $E[\\hat{x}_{\\mathrm{MF}}] = x$. Substituting these into the equation:\n$$\nE[\\hat{x}_{\\mathrm{mix}}] = wx + (1-w)x = wx + x - wx = x\n$$\nThe combined estimator $\\hat{x}_{\\mathrm{mix}}$ is unbiased for any choice of $w$, as its weights sum to $1$. The unbiasedness constraint is automatically satisfied by the form of the mixture.\n\nNext, we derive the expression for the variance of $\\hat{x}_{\\mathrm{mix}}$. The variance is given by:\n$$\n\\mathrm{Var}(\\hat{x}_{\\mathrm{mix}}) = \\mathrm{Var}(w\\hat{x}_{\\mathrm{MB}} + (1-w)\\hat{x}_{\\mathrm{MF}})\n$$\nUsing the property for the variance of a sum of random variables, $\\mathrm{Var}(aX + bY) = a^2\\mathrm{Var}(X) + b^2\\mathrm{Var}(Y) + 2ab\\mathrm{Cov}(X,Y)$, we get:\n$$\n\\mathrm{Var}(\\hat{x}_{\\mathrm{mix}}) = w^2\\mathrm{Var}(\\hat{x}_{\\mathrm{MB}}) + (1-w)^2\\mathrm{Var}(\\hat{x}_{\\mathrm{MF}}) + 2w(1-w)\\mathrm{Cov}(\\hat{x}_{\\mathrm{MB}}, \\hat{x}_{\\mathrm{MF}})\n$$\nThe problem states that the noise sources are independent, which implies that the estimators $\\hat{x}_{\\mathrm{MB}}$ and $\\hat{x}_{\\mathrm{MF}}$ are independent. Therefore, their covariance is zero, $\\mathrm{Cov}(\\hat{x}_{\\mathrm{MB}}, \\hat{x}_{\\mathrm{MF}}) = 0$. The variance expression simplifies to:\n$$\n\\mathrm{Var}(\\hat{x}_{\\mathrm{mix}}) = w^2\\mathrm{Var}(\\hat{x}_{\\mathrm{MB}}) + (1-w)^2\\mathrm{Var}(\\hat{x}_{\\mathrm{MF}})\n$$\nSubstituting the given variances, $\\sigma_{\\mathrm{MB}}^{2}$ and $\\sigma_{\\mathrm{MF}}^{2}$:\n$$\nV(w) = w^2\\sigma_{\\mathrm{MB}}^{2} + (1-w)^2\\sigma_{\\mathrm{MF}}^{2}\n$$\nTo find the value of $w$ that minimizes this variance, we take the first derivative of $V(w)$ with respect to $w$ and set it to zero:\n$$\n\\frac{dV}{dw} = \\frac{d}{dw} \\left( w^2\\sigma_{\\mathrm{MB}}^{2} + (1-w)^2\\sigma_{\\mathrm{MF}}^{2} \\right) = 2w\\sigma_{\\mathrm{MB}}^{2} + 2(1-w)(-1)\\sigma_{\\mathrm{MF}}^{2} = 2w\\sigma_{\\mathrm{MB}}^{2} - 2(1-w)\\sigma_{\\mathrm{MF}}^{2}\n$$\nSetting the derivative to zero for minimization:\n$$\n2w\\sigma_{\\mathrm{MB}}^{2} - 2(1-w)\\sigma_{\\mathrm{MF}}^{2} = 0\n$$\n$$\nw\\sigma_{\\mathrm{MB}}^{2} = (1-w)\\sigma_{\\mathrm{MF}}^{2}\n$$\n$$\nw\\sigma_{\\mathrm{MB}}^{2} = \\sigma_{\\mathrm{MF}}^{2} - w\\sigma_{\\mathrm{MF}}^{2}\n$$\n$$\nw\\sigma_{\\mathrm{MB}}^{2} + w\\sigma_{\\mathrm{MF}}^{2} = \\sigma_{\\mathrm{MF}}^{2}\n$$\n$$\nw(\\sigma_{\\mathrm{MB}}^{2} + \\sigma_{\\mathrm{MF}}^{2}) = \\sigma_{\\mathrm{MF}}^{2}\n$$\nSolving for $w$, we obtain the closed-form expression:\n$$\nw = \\frac{\\sigma_{\\mathrm{MF}}^{2}}{\\sigma_{\\mathrm{MB}}^{2} + \\sigma_{\\mathrm{MF}}^{2}}\n$$\nTo confirm this is a minimum, we check the second derivative:\n$$\n\\frac{d^2V}{dw^2} = \\frac{d}{dw} \\left( 2w\\sigma_{\\mathrm{MB}}^{2} - 2\\sigma_{\\mathrm{MF}}^{2} + 2w\\sigma_{\\mathrm{MF}}^{2} \\right) = 2\\sigma_{\\mathrm{MB}}^{2} + 2\\sigma_{\\mathrm{MF}}^{2}\n$$\nSince variances $\\sigma^2$ are non-negative (and strictly positive in any meaningful scenario), $\\frac{d^2V}{dw^2}  0$, which confirms that the value of $w$ we found corresponds to a minimum.\n\nNow, we compute the numerical value of $w$ using the provided variances: $\\sigma_{\\mathrm{MB}}^{2} = 0.25$ and $\\sigma_{\\mathrm{MF}}^{2} = 1$.\n$$\nw = \\frac{1}{0.25 + 1} = \\frac{1}{1.25} = \\frac{1}{5/4} = \\frac{4}{5} = 0.8\n$$\nThe optimal arbitration weight is $w = 0.8$.\n\nBrief interpretation: The weight $w$ corresponds to the Model-Based (MB) subsystem. The derived formula, $w = \\sigma_{\\mathrm{MF}}^2 / (\\sigma_{\\mathrm{MB}}^2 + \\sigma_{\\mathrm{MF}}^2)$, shows that the optimal weight for an estimator is proportional to the variance (a measure of uncertainty) of the other estimator. This is a form of inverse-variance weighting. The system with lower uncertainty is trusted more.\nIn this case, $\\sigma_{\\mathrm{MB}}^2 = 0.25$ is significantly smaller than $\\sigma_{\\mathrm{MF}}^2 = 1.0$, indicating that the MB system is more precise, or less noisy, than the MF system. The resulting optimal weight $w=0.8$ reflects this. The mixture policy is $Q_{\\mathrm{mix}} = 0.8\\,Q_{\\mathrm{MB}} + 0.2\\,Q_{\\mathrm{MF}}$. This means the arbitration mechanism allocates $80\\%$ of the influence to the more reliable MB pathway and only $20\\%$ to the less reliable MF pathway. This strategy produces a combined action value estimate that is more precise (i.e., has a smaller variance) than either of the individual estimates alone.",
            "answer": "$$\n\\boxed{0.8}\n$$"
        }
    ]
}