{
    "hands_on_practices": [
        {
            "introduction": "在进行任何复杂的神经影像分析之前，首要任务是确保数据在空间上的一致性。来自不同扫描仪或会话的图像可能具有不同的方向编码，这会给群体分析和模型应用带来障碍。本练习将引导您深入了解NIfTI格式的核心——仿射变换矩阵，并从线性代数的第一性原理出发，实现一个将不同方向的图像转换为统一“标准方向”的算法。这项实践对于构建任何稳健的神经影像处理流程都至关重要。",
            "id": "4491629",
            "problem": "给定一小组用于模拟真实神经影像信息技术倡议（NIfTI）图像方向的合成神经影像头文件仿射矩阵，这些矩阵具有不同的轴顺序和翻转。在 NIfTI 中，体素坐标 $\\mathbf{i} = (i_x, i_y, i_z)$ 通过一个 $4\\times 4$ 的仿射矩阵 $A\\in\\mathbb{R}^{4\\times 4}$ 使用齐次坐标映射到扫描仪/世界坐标 $\\mathbf{x} = (x, y, z)$，其目标是遵循右-前-上 (RAS) 约定。左上角的 $3\\times 3$ 子块 $R\\in\\mathbb{R}^{3\\times 3}$ 编码了旋转、缩放和可能的剪切，最后一列编码了平移。世界坐标轴定义如下：$+x$ 朝向右侧增加，$+y$ 朝向前侧增加，$+z$ 朝向上侧增加。物理长度单位为毫米 (mm)。\n\n从线性映射的第一性原理出发，确定一个规范方向，该方向通过重排和翻转体素轴，使得变换后新的 $3\\times 3$ 子块 $R_{\\mathrm{can}}$ 的列在指定的容差范围内，依次与正向世界坐标轴 $(+x,+y,+z)$ 对齐。这需要：\n- 检查 $R$ 的列；$R$ 的第 $j$ 列描述了体素轴 $j$ 如何贡献于世界坐标。\n- 对于每个体素轴 $j\\in\\{0,1,2\\}$，识别出具有最大绝对贡献的世界轴索引 $k\\in\\{0,1,2\\}$，即最大化 $\\lvert R_{k,j}\\rvert$ 的索引 $k$，以及指示方向性的符号 $\\operatorname{sign}(R_{k,j})\\in\\{-1,+1\\}$（正号表示与 $+x,+y,+z$ 对齐，负号表示与 $-x,-y,-z$ 对齐）。\n- 构建一个规范化变换 $S\\in\\mathbb{R}^{4\\times 4}$，其左上角的 $3\\times 3$ 部分对体素轴进行置换和翻转，使得 $R_{\\mathrm{can}} = R S_{3\\times 3}$ 的列分别对应于 $(+x,+y,+z)$。使用 $A_{\\mathrm{can}} = A S$ 作为完整的头文件变换；由于缺少图像维度信息，平移项无需调整，且对齐验证应仅考虑 $R_{\\mathrm{can}}$。\n\n验证规则：规范化后，对于 $R_{\\mathrm{can}}$ 的每一列 $i\\in\\{0,1,2\\}$，\n- 最大幅值分量必须位于第 $i$ 行（即世界轴匹配），并且\n- 对角线元素 $R_{\\mathrm{can}}[i,i]$ 必须为严格正数，\n- 所有列上的最大非对角线幅值，经最大对角线幅值归一化后，不得超过容差 $\\tau$（设 $\\tau=0.25$）。\n\n如果从体素轴到世界轴的初始映射不是双射（例如，重复分配或零长度列），则将该情况视为无效映射并报告验证为假，但仍通过贪心法将每个世界轴 $i$ 分配给最大化 $\\lvert R_{i,j}\\rvert$ 的未分配体素轴 $j$，并根据 $R_{i,j}$ 的符号进行翻转，以计算出一个尽力而为的规范化结果，从而产生确定性输出。\n\n用 Python 实现一个程序，对于每个测试用例的仿射矩阵，计算：\n- 置换列表 $\\mathrm{perm} = [p_0,p_1,p_2]$，其中 $p_i$ 是规范化后分配给世界轴 $i$ 的原始体素轴索引（即 $R_{\\mathrm{can}}$ 的第 $i$ 列源自 $R$ 的第 $p_i$ 列）。\n- 翻转列表 $\\mathrm{flips} = [f_0,f_1,f_2]$，其中 $f_i\\in\\{0,1\\}$，$f_i=1$ 表示轴 $i$ 必须反转以与正向世界轴对齐。\n- 布尔值 $\\mathrm{ok}$，指示验证是否根据规则成功。\n- 浮点数 $\\mathrm{max\\_offdiag}$，等于 $R_{\\mathrm{can}}$ 中的最大绝对非对角线元素除以所有三列中的最大对角线幅值。\n\n测试套件（所有条目单位为 mm；角度由 $R$ 的线性分量隐含）：\n1. 理想情况（已是 RAS，单位体素大小）：\n   $$A_0 = \\begin{bmatrix}\n   1  0  0  0 \\\\\n   0  1  0  0 \\\\\n   0  0  1  0 \\\\\n   0  0  0  1\n   \\end{bmatrix}.$$\n2. 轴交换，带一次翻转和非均匀体素大小（包含平移，但验证仅使用 $R$）：\n   $$A_1 = \\begin{bmatrix}\n   0  1  0  10 \\\\\n   -2  0  0  -5 \\\\\n   0  0  3  2 \\\\\n   0  0  0  1\n   \\end{bmatrix}.$$\n3. 小剪切，带一个轴反转：\n   $$A_2 = \\begin{bmatrix}\n   1.0  0.0  0.01  0 \\\\\n   0.05  1.0  0.0  0 \\\\\n   0.0  0.02  -1.0  0 \\\\\n   0  0  0  1\n   \\end{bmatrix}.$$\n4. 退化情况，带重复的主导世界轴分配（无效映射）：\n   $$A_3 = \\begin{bmatrix}\n   1.0  0.8  0.0  3 \\\\\n   0.0  0.1  0.0  4 \\\\\n   0.0  0.0  1.0  5 \\\\\n   0  0  0  1\n   \\end{bmatrix}.$$\n\n算法要求：\n- 只应使用关于 $R$ 的线性代数；不要依赖外部神经影像库。\n- 使用一个小的数值阈值 $\\varepsilon = 10^{-8}$ 来检测零长度列和进行正性检查。\n- 在验证规则中使用容差 $\\tau = 0.25$。\n\n最终输出格式：\n你的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表。每个结果本身是一个形式为 $[\\mathrm{perm},\\mathrm{flips},\\mathrm{ok},\\mathrm{max\\_offdiag}]$ 的列表，其中 $\\mathrm{perm}$ 和 $\\mathrm{flips}$ 是整数列表，$\\mathrm{ok}$ 是布尔值，$\\mathrm{max\\_offdiag}$ 是浮点数。例如，输出必须如下所示：\n$$[\\,[ [p_0,p_1,p_2],[f_0,f_1,f_2],\\mathrm{ok},m_0 ],\\,\\ldots\\,]$$\n每个测试用例对应一个这样的子列表，顺序与测试套件相同。",
            "solution": "我们从神经影像信息技术倡议（NIfTI）中仿射变换的定义开始。体素坐标 $\\mathbf{i} = (i_x, i_y, i_z)$ 使用齐次坐标映射到世界坐标 $\\mathbf{x} = (x, y, z)$：\n$$\n\\begin{bmatrix}\nx \\\\ y \\\\ z \\\\ 1\n\\end{bmatrix}\n=\nA\n\\begin{bmatrix}\ni_x \\\\ i_y \\\\ i_z \\\\ 1\n\\end{bmatrix},\n\\quad\nA =\n\\begin{bmatrix}\nR  \\mathbf{t} \\\\\n\\mathbf{0}^\\top  1\n\\end{bmatrix},\n$$\n其中 $R\\in\\mathbb{R}^{3\\times 3}$ 编码旋转、缩放和剪切，$\\mathbf{t}\\in\\mathbb{R}^3$ 编码平移。右-前-上（RAS）约定将正向世界轴定义为：$+x$ 代表右，$+y$ 代表前，$+z$ 代表上。\n\n根据线性代数， $R$ 的第 $j$ 列给出了沿第 $j$ 个体素轴的单位步长对世界坐标向量的贡献。为了确定体素轴 $j$ 的主导世界轴，我们找到最大化 $\\lvert R_{k,j}\\rvert$ 的索引 $k$。$R_{k,j}$ 的符号指示了体素轴 $j$ 是与世界轴 $k$ 的正方向对齐（符号为 $+1$）还是负方向对齐（符号为 $-1$）。对每个 $j\\in\\{0,1,2\\}$ 执行此过程，可以得出一个从体素轴到世界轴的映射，以及其方向性（如果为负则翻转）。\n\n一个一致的规范方向旨在重排和翻转体素轴，以便在变换后的仿射矩阵 $A_{\\mathrm{can}} = A S$ 中，左上角的 $3\\times 3$ 子块 $R_{\\mathrm{can}}$ 的列分别与 $(+x,+y,+z)$ 对齐。变换矩阵 $S\\in\\mathbb{R}^{4\\times 4}$ 的形式如下：\n$$\nS =\n\\begin{bmatrix}\nS_{3\\times 3}  \\mathbf{0} \\\\\n\\mathbf{0}^\\top  1\n\\end{bmatrix},\n$$\n其中 $S_{3\\times 3}$ 是一个列选择和符号翻转矩阵。具体来说，$S_{3\\times 3}$ 的第 $i$ 列是 $\\pm \\mathbf{e}_{p_i}$，其中 $\\mathbf{e}_{p_i}$ 是第 $p_i$ 个标准基向量（选择原始体素轴 $p_i$），如果 $R_{i,p_i} > 0$ 则符号为正，否则为负。这种构造确保了：\n$$\nR_{\\mathrm{can}} = R S_{3\\times 3}, \\quad \\text{and} \\quad R_{\\mathrm{can}}[:,i] = \\operatorname{sign}(R_{i,p_i})\\, R[:,p_i].\n$$\n因此，$R_{\\mathrm{can}}$ 的列的最大幅值分量位于相应的对角线元素上，并且是正数。\n\n验证条件源于几何对齐要求：\n- 对于每一列 $i$，最大幅值分量位于第 $i$ 行（即 $\\arg\\max_{k\\in\\{0,1,2\\}} \\lvert R_{\\mathrm{can}}[k,i]\\rvert = i$）。这确保了正确的轴匹配。\n- 对角线元素 $R_{\\mathrm{can}}[i,i]$ 是严格正数，确保与正向世界轴对齐。\n- 剪切或离轴污染必须有界；我们测量所有列的最大非对角线幅值，并用最大对角线幅值进行归一化（一种尺度不变的度量）。如果这个归一化后的最大值最多为容差 $\\tau$，则认为该对齐对于规范化是可接受的。我们设置 $\\tau = 0.25$ 以允许重采样或轻微头文件不一致性所带来的典型小剪切。\n\n如果从体素轴到世界轴的初始映射不是双射的（例如，两个体素轴都主导同一个世界轴，或者存在一个零长度列表示没有贡献），则规范化是不可靠的。我们仍然通过贪心法为每个世界轴 $i$ 分配最大化 $\\lvert R_{i,j}\\rvert$ 的未分配体素轴 $j$，并根据 $\\operatorname{sign}(R_{i,j})$ 进行翻转，来生成一个确定性的尽力而为的 $S$。在这种情况下，验证标志 $\\mathrm{ok}$ 被设置为假。\n\n我们现在将该过程应用于测试套件：\n\n1. 情况 $A_0$：\n   $R = I_3$。对于 $j=0,1,2$，主导世界索引分别是 $0,1,2$，全部为正号。因此，$p = [0,1,2]$，flips $= [0,0,0]$，且 $S_{3\\times 3} = I_3$。那么 $R_{\\mathrm{can}} = I_3$，非对角线元素为零，所以 $\\mathrm{max\\_offdiag} = 0$，且 $\\mathrm{ok} = \\text{True}$。\n\n2. 情况 $A_1$：\n   $R$ 的列为 $R[:,0] = [0,-2,0]^\\top$，$R[:,1] = [1,0,0]^\\top$，$R[:,2] = [0,0,3]^\\top$。主导轴：体素 $0\\to y$（负号），体素 $1\\to x$（正号），体素 $2\\to z$（正号）。与 $(x,y,z)$ 对齐的置换是 $p = [1,0,2]$，flips $= [0,1,0]$。用这些列构建 $S_{3\\times 3}$ 会得到 $R_{\\mathrm{can}}$，其列为 $[1,0,0]^\\top$、$[0,2,0]^\\top$ 和 $[0,0,3]^\\top$，这满足条件；非对角线元素为零，所以 $\\mathrm{max\\_offdiag}=0$，且 $\\mathrm{ok} = \\text{True}$。\n\n3. 情况 $A_2$：\n   $R$ 的列为 $[1.0, 0.05, 0.0]^\\top$、$[0.0, 1.0, 0.02]^\\top$、$[0.01, 0.0, -1.0]^\\top$。主导轴：体素 $0\\to x$（正），体素 $1\\to y$（正），体素 $2\\to z$（负）。因此 $p = [0,1,2]$ 且 flips $= [0,0,1]$。翻转第三列得到 $R_{\\mathrm{can}}$ 的列为 $[1.0, 0.05, 0.0]^\\top$、$[0.0, 1.0, 0.02]^\\top$ 和 $[-0.01, 0.0, 1.0]^\\top$。最大分量位于对角线上且为正值，最大非对角线幅值为 $0.05$；最大对角线幅值为 $1.0$，归一化后的 $\\mathrm{max\\_offdiag}=0.05\\le \\tau$，所以 $\\mathrm{ok}=\\text{True}$。\n\n4. 情况 $A_3$：\n   $R$ 的列为 $[1.0, 0.0, 0.0]^\\top$、$[0.8, 0.1, 0.0]^\\top$ 和 $[0.0, 0.0, 1.0]^\\top$。两个体素轴都主要映射到 $+x$（重复），产生了一个非双射映射；这是无效的。为产生确定性输出而进行的贪心分配给出 $p = [0,1,2]$ 和 flips $= [0,0,0]$。那么 $R_{\\mathrm{can}}$ 的列为 $[1.0, 0.0, 0.0]^\\top$、$[0.8, 0.1, 0.0]^\\top$、$[0.0, 0.0, 1.0]^\\top$。第 $1$ 列的轴匹配失败（其最大分量在第 $0$ 行而不是第 $1$ 行），且归一化后的最大非对角线幅值为 $0.8/1.0 = 0.8 > \\tau$。因此 $\\mathrm{ok}=\\text{False}$。\n\n程序实现了这些步骤，强制使用一个小的阈值 $\\varepsilon = 10^{-8}$ 来检测零长度列和正性，使用 $\\tau = 0.25$ 作为剪切容差，并为每个测试用例生成列表 $[\\mathrm{perm},\\mathrm{flips},\\mathrm{ok},\\mathrm{max\\_offdiag}]$。最终输出将这些列表按顺序聚合成单行，用逗号分隔并置于方括号内。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef infer_mapping(R, eps=1e-8):\n    \"\"\"\n    Infer mapping from voxel axes (columns) to world axes (rows) based on the\n    dominant absolute contribution in each column.\n    Returns:\n        world_idx_by_voxel: list of length 3, each entry in {0,1,2}\n        sign_by_voxel: list of length 3, each entry in {+1,-1}\n        valid: boolean, True if mapping is a bijection and columns non-degenerate\n    \"\"\"\n    world_idx_by_voxel = []\n    sign_by_voxel = []\n    valid = True\n    # Check for zero-length columns\n    for j in range(3):\n        col = R[:, j]\n        norm = np.linalg.norm(col)\n        if norm  eps:\n            valid = False\n        k = int(np.argmax(np.abs(col)))\n        s = 1 if col[k] >= 0 else -1\n        world_idx_by_voxel.append(k)\n        sign_by_voxel.append(s)\n    # Check bijection (unique assignment of world axes)\n    counts = [world_idx_by_voxel.count(i) for i in range(3)]\n    if any(c == 0 for c in counts) or any(c > 1 for c in counts):\n        valid = False\n    return world_idx_by_voxel, sign_by_voxel, valid\n\ndef greedy_assign(R, eps=1e-8):\n    \"\"\"\n    Greedily assign world axes i to voxel axes j when mapping is invalid.\n    For each world axis i=0..2, choose the unassigned voxel axis j maximizing |R[i,j]|.\n    Returns:\n        perm: list p_i = selected voxel axis index for world axis i\n        flips: list f_i = 0 if R[i,p_i] >= 0 else 1\n    \"\"\"\n    assigned = set()\n    perm = [None, None, None]\n    flips = [0, 0, 0]\n    for i in range(3):\n        best_j = None\n        best_val = -np.inf\n        for j in range(3):\n            if j in assigned:\n                continue\n            val = abs(R[i, j])\n            if val > best_val + eps:  # strict preference\n                best_val = val\n                best_j = j\n        if best_j is None:\n            # fallback: pick any unassigned\n            for j in range(3):\n                if j not in assigned:\n                    best_j = j\n                    break\n        assigned.add(best_j)\n        perm[i] = best_j\n        flips[i] = 0 if R[i, best_j] >= 0 else 1\n    return perm, flips\n\ndef build_S_from_perm_flips(perm, flips):\n    \"\"\"\n    Build the 4x4 canonicalization transform S given perm and flips.\n    The top-left 3x3 columns select original voxel axes perm[i] and apply sign (flip).\n    \"\"\"\n    S = np.zeros((4, 4), dtype=float)\n    # Construct S_{3x3} by columns\n    for i in range(3):\n        j = perm[i]\n        s = -1.0 if flips[i] == 1 else 1.0\n        S[j, i] = s  # place s at row j, column i\n    S[3, 3] = 1.0\n    return S\n\ndef canonicalize_affine(A, eps=1e-8, tau=0.25):\n    \"\"\"\n    Given affine A, compute canonicalization transform and verification.\n    Returns:\n        perm: list of original voxel axis indices used for world axes [x,y,z]\n        flips: list of 0/1 indicating axis reversal\n        ok: boolean indicating verification success\n        max_offdiag: float, normalized maximum off-diagonal magnitude\n    \"\"\"\n    R = A[:3, :3].astype(float)\n    world_idx_by_voxel, sign_by_voxel, valid_mapping = infer_mapping(R, eps=eps)\n\n    # Build perm and flips\n    if valid_mapping:\n        # Construct perm: for each world axis i, find voxel j with world_idx[j] == i\n        perm = [None, None, None]\n        flips = [0, 0, 0]\n        for i in range(3):\n            # find j where world_idx_by_voxel[j] == i\n            candidates = [j for j in range(3) if world_idx_by_voxel[j] == i]\n            if len(candidates) == 1:\n                j = candidates[0]\n            else:\n                # Should not happen when valid_mapping is True, but handle gracefully\n                j = candidates[0] if candidates else i\n            perm[i] = j\n            flips[i] = 0 if sign_by_voxel[j] == 1 else 1\n    else:\n        # Greedy assignment\n        perm, flips = greedy_assign(R, eps=eps)\n\n    # Build S and canonicalize\n    S = build_S_from_perm_flips(perm, flips)\n    A_can = A @ S\n    R_can = A_can[:3, :3]\n\n    # Verification\n    # 1) Largest magnitude component at matching row i\n    axis_match = True\n    for i in range(3):\n        col = R_can[:, i]\n        if np.linalg.norm(col)  eps:\n            axis_match = False\n            break\n        k = int(np.argmax(np.abs(col)))\n        if k != i:\n            axis_match = False\n            break\n        # 2) Positive diagonal\n        if R_can[i, i] = eps:\n            axis_match = False\n            break\n\n    # 3) Off-diagonal tolerance\n    diag_mags = []\n    offdiag_max = 0.0\n    for i in range(3):\n        col = R_can[:, i]\n        diag_mags.append(abs(col[i]))\n        off = np.max(np.abs(np.delete(col, i)))  # max off-diagonal in this column\n        if off > offdiag_max:\n            offdiag_max = off\n    max_diag_mag = max(diag_mags) if diag_mags else 0.0\n    if max_diag_mag > eps:\n        max_offdiag = offdiag_max / max_diag_mag\n    else:\n        # If all diagonal magnitudes are near zero, treat as degenerate\n        max_offdiag = float('inf')\n        axis_match = False\n\n    ok = bool(axis_match and (max_offdiag = tau) and valid_mapping)\n\n    return perm, flips, ok, float(max_offdiag)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    A0 = np.array([\n        [1, 0, 0, 0],\n        [0, 1, 0, 0],\n        [0, 0, 1, 0],\n        [0, 0, 0, 1]\n    ], dtype=float)\n\n    A1 = np.array([\n        [0, 1, 0, 10],\n        [-2, 0, 0, -5],\n        [0, 0, 3, 2],\n        [0, 0, 0, 1]\n    ], dtype=float)\n\n    A2 = np.array([\n        [1.0, 0.0, 0.01, 0.0],\n        [0.05, 1.0, 0.0, 0.0],\n        [0.0, 0.02, -1.0, 0.0],\n        [0.0, 0.0, 0.0, 1.0]\n    ], dtype=float)\n\n    A3 = np.array([\n        [1.0, 0.8, 0.0, 3.0],\n        [0.0, 0.1, 0.0, 4.0],\n        [0.0, 0.0, 1.0, 5.0],\n        [0.0, 0.0, 0.0, 1.0]\n    ], dtype=float)\n\n    test_cases = [A0, A1, A2, A3]\n\n    results = []\n    for A in test_cases:\n        perm, flips, ok, max_offdiag = canonicalize_affine(A, eps=1e-8, tau=0.25)\n        # Assemble result as required: [perm, flips, ok, max_offdiag]\n        results.append([perm, flips, ok, max_offdiag])\n\n    # Final print statement in the exact required format.\n    # Single line containing the results as a comma-separated list enclosed in square brackets.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "将深度学习应用于体积数据（如MRI）时，一个核心的设计决策是如何将三维数据有效地输入到卷积神经网络（CNN）中。两种主流范式是：将体积逐层切片进行二维处理，或使用三维“块”（patches）进行处理。本练习旨在量化地探讨这两种方法之间的根本权衡，您将亲手构建模型来比较它们在计算内存消耗、捕获空间上下文信息量以及潜在的临床判别性能上的差异。理解这些权衡对于在资源受限的现实世界中设计高效的神经影像深度学习模型至关重要。",
            "id": "4491613",
            "problem": "要求您设计并实现一个程序，用于比较体积神经影像学中的两种卷积神经网络 (CNN) 输入范式：基于二维切片的模型与基于三维区块的模型。您的任务是为每种范式量化三个方面：每个训练批次的内存消耗、输入所捕获的物理上下文，以及在一个简单生成检测模型下的预测临床区分性能代理指标。该程序必须为多个给定的参数集计算结果，并以单一、精确指定的格式输出它们。\n\n请从以下基本依据和核心定义开始您的推导，您必须使用这些来构建您的解决方案，而不调用任何特定于问题的快捷公式：\n\n- 一个形状为 $n_1 \\times n_2 \\times \\cdots \\times n_k$、标量类型每个元素占用 $b$ 字节的浮点张量，其消耗的内存恰好为 $n_1 n_2 \\cdots n_k \\cdot b$ 字节。训练内存主要由激活值主导，一个恒定的乘法开销因子可以近似计算梯度和优化器状态所需的额外存储空间。\n- 卷积神经网络 (CNN) 在每个池化阶段后将空间维度减小一个因子 $2$。对于一个有 $L$ 个阶段的网络，在阶段 $l \\in \\{0,1,\\dots,L-1\\}$ 的特征图空间维度约等于输入维度除以 $2^l$，并通过向下取整来保持整数网格索引。在编码器式的特征金字塔中，每深入一个阶段，通道数翻倍是标准做法。\n- 对于从具有间距 $s_x, s_y, s_z$（单位为毫米）的体素网格派生的输入张量，其物理视场等于每个维度上的体素数量与其间距的乘积，从而给出每个轴向上的物理长度。三维物理视场体积是这三个物理长度的乘积。\n- 根据信号检测理论中的高斯等方差检测模型，聚合 $N_{\\text{eff}}$ 个独立观测值，每个观测值贡献的信号均值差为 $s$、噪声标准差为 $\\sigma$，产生的可区分性与有效独立观测数量的平方根成比例。体素级的空间相关性可以通过一个有效相关体积来概括。最终高斯模型的受试者工作特征曲线下面积 (AUC) 可以从可区分性指数推导出来。\n\n您必须实现以下精确的建模假设：\n\n1) 架构和内存模型\n- 考虑一个仅有编码器的特征金字塔，该金字塔有 $L$ 个阶段，索引为 $l \\in \\{0,\\dots,L-1\\}$。在阶段 $l$，通道数为 $C_0 \\cdot 2^l$，其中 $C_0$ 是基础通道数。\n- 在阶段 $l$ 的空间分辨率由 $\\left(\\max\\{1,\\left\\lfloor \\frac{n_x}{2^l} \\right\\rfloor\\}, \\max\\{1,\\left\\lfloor \\frac{n_y}{2^l} \\right\\rfloor\\}, \\max\\{1,\\left\\lfloor \\frac{n_z}{2^l} \\right\\rfloor\\}\\right)$ 给出，其中 $(n_x,n_y,n_z)$ 是呈现给 CNN 的样本的输入体素数。对于二维切片模型，根据构造，使用 $n_z = 1$。\n- 每个样本的总激活元素数量是所有阶段 $l$ 上，阶段 $l$ 的空间维度与通道数的乘积之和。\n- 每批次的激活内存（以字节为单位）等于 $\\gamma \\cdot B \\cdot E \\cdot b_f$，其中 $\\gamma$ 是一个恒定的开销乘数， $B$ 是批次大小， $E$ 是所有阶段上每个样本的激活元素数量之和， $b_f$ 是每个浮点数的字节数。内存以兆字节 (MB) 表示，其中 $1 \\text{ MB} = 1024^2$ 字节。\n\n2) 上下文捕获\n- 设体素间距为 $(s_x,s_y,s_z)$，单位为毫米。对于具有 $(n_x,n_y)$ 个体素的二维切片输入，其厚度被视为恰好一个切片的厚度，即物理厚度为 $s_z$。该切片输入的物理视场 (FOV) 体积为 $V_{\\text{2D}} = (n_x s_x) \\cdot (n_y s_y) \\cdot (1 \\cdot s_z)$，单位为立方毫米。\n- 对于具有 $(n_x,n_y,n_z)$ 个体素的三维区块，其物理 FOV 体积为 $V_{\\text{3D}} = (n_x s_x) \\cdot (n_y s_y) \\cdot (n_z s_z)$，单位为立方毫米。\n- 报告无量纲的上下文比率 $R_V = \\frac{V_{\\text{3D}}}{V_{\\text{2D}}}$。\n\n3) 预测的临床性能代理指标\n- 假设一个在高斯等方差模型下的二元病变检测任务。假设每个体素在类别之间贡献一个附加的信号差异 $s$ 和标准差为 $\\sigma$ 的独立噪声。空间相关性通过一个有效相关体积（以体素为单位）$v_c = \\max\\{1, \\lambda_x \\lambda_y \\lambda_z\\}$ 来概括，其中 $(\\lambda_x,\\lambda_y,\\lambda_z)$ 是沿各轴测量的相关范围（以体素为单位）。\n- 对于一个有 $n_x n_y n_z$ 个体素的输入，其有效独立观测数为 $N_{\\text{eff}} = \\max\\left\\{1, \\frac{n_x n_y n_z}{v_c}\\right\\}$。\n- 使用高斯等方差模型的信号检测理论，根据 $N_{\\text{eff}}$、$s$ 和 $\\sigma$ 推导出可区分性指数，然后根据可区分性指数推导出 AUC。报告的 AUC 为 $[0,1]$ 范围的小数，不带百分号。\n\n您的程序必须为每个测试用例计算：\n- 二维模型的批次内存（MB）。\n- 三维模型的批次内存（MB）。\n- 上下文比率 $R_V$（无量纲）。\n- 二维模型的 AUC。\n- 三维模型的 AUC。\n\n每个测试用例的架构配置和数据参数如下。所有整数和实数都是精确的，您必须严格遵守规范。\n\n所有用例的通用常量：\n- 浮点数存储大小：$b_f = 4$ 字节。\n\n测试套件：\n- 用例 A（理想路径，各向同性间距）：\n  - 二维切片输入尺寸 $(n_x,n_y) = (256, 256)$。\n  - 三维区块尺寸 $(n_x,n_y,n_z) = (64, 64, 16)$。\n  - 体素间距 $(s_x,s_y,s_z) = (1.0, 1.0, 1.0)$ 毫米。\n  - 网络阶段数 $L = 3$，基础通道数 $C_0 = 16$，开销 $\\gamma = 3.0$。\n  - 批次大小：二维 $B_{\\text{2D}} = 8$，三维 $B_{\\text{3D}} = 2$。\n  - 检测模型：每体素信号 $s = 0.5$，噪声标准差 $\\sigma = 1.0$，相关范围 $(\\lambda_x,\\lambda_y,\\lambda_z) = (3, 3, 3)$ 体素。\n\n- 用例 B（各向异性间距，浅深度区块）：\n  - 二维切片输入尺寸 $(n_x,n_y) = (512, 512)$。\n  - 三维区块尺寸 $(n_x,n_y,n_z) = (128, 128, 4)$。\n  - 体素间距 $(s_x,s_y,s_z) = (0.8, 0.8, 3.0)$ 毫米。\n  - 网络阶段数 $L = 4$，基础通道数 $C_0 = 8$，开销 $\\gamma = 2.5$。\n  - 批次大小：二维 $B_{\\text{2D}} = 4$，三维 $B_{\\text{3D}} = 1$。\n  - 检测模型：每体素信号 $s = 0.3$，噪声标准差 $\\sigma = 1.2$，相关范围 $(\\lambda_x,\\lambda_y,\\lambda_z) = (5, 5, 1)$ 体素。\n\n- 用例 C（边界情况，三维深度等于一）：\n  - 二维切片输入尺寸 $(n_x,n_y) = (64, 64)$。\n  - 三维区块尺寸 $(n_x,n_y,n_z) = (64, 64, 1)$。\n  - 体素间距 $(s_x,s_y,s_z) = (1.0, 1.0, 1.0)$ 毫米。\n  - 网络阶段数 $L = 2$，基础通道数 $C_0 = 16$，开销 $\\gamma = 3.0$。\n  - 批次大小：二维 $B_{\\text{2D}} = 8$，三维 $B_{\\text{3D}} = 8$。\n  - 检测模型：每体素信号 $s = 0.6$，噪声标准差 $\\sigma = 0.9$，相关范围 $(\\lambda_x,\\lambda_y,\\lambda_z) = (2, 2, 2)$ 体素。\n\n- 用例 D（边缘情况，高基础通道数，各向异性间距）：\n  - 二维切片输入尺寸 $(n_x,n_y) = (128, 128)$。\n  - 三维区块尺寸 $(n_x,n_y,n_z) = (96, 96, 24)$。\n  - 体素间距 $(s_x,s_y,s_z) = (0.5, 0.5, 2.0)$ 毫米。\n  - 网络阶段数 $L = 2$，基础通道数 $C_0 = 32$，开销 $\\gamma = 3.5$。\n  - 批次大小：二维 $B_{\\text{2D}} = 16$，三维 $B_{\\text{3D}} = 1$。\n  - 检测模型：每体素信号 $s = 0.8$，噪声标准差 $\\sigma = 0.8$，相关范围 $(\\lambda_x,\\lambda_y,\\lambda_z) = (2, 2, 2)$ 体素。\n\n实现细节：\n- 对于二维模型，在所有计算中将深度视为 $n_z = 1$。\n- 对于每个阶段 $l$，通过整数向下取整计算空间维度，每个维度的最小值为 1 个体素：对于维度 $d \\in \\{x,y,z\\}$，$n_d^{(l)} = \\max\\{1,\\left\\lfloor \\frac{n_d}{2^l} \\right\\rfloor\\}$。\n- 对于每个阶段 $l$，通道数为 $C^{(l)} = C_0 \\cdot 2^l$。\n- 每个样本的激活元素总数为 $E = \\sum_{l=0}^{L-1} \\left(n_x^{(l)} n_y^{(l)} n_z^{(l)} C^{(l)}\\right)$。\n- 每批次的内存（以兆字节为单位）为 $M_{\\text{MB}} = \\frac{\\gamma \\cdot B \\cdot E \\cdot b_f}{1024^2}$，以 MB 报告。\n- 物理视场体积为 $V_{\\text{2D}} = (n_x s_x)(n_y s_y)(1 \\cdot s_z)$ 和 $V_{\\text{3D}} = (n_x s_x)(n_y s_y)(n_z s_z)$，单位为立方毫米，上下文比率为 $R_V = \\frac{V_{\\text{3D}}}{V_{\\text{2D}}}$。\n- 有效相关体积为 $v_c = \\max\\{1, \\lambda_x \\lambda_y \\lambda_z\\}$，有效计数为 $N_{\\text{eff}} = \\max\\left\\{1, \\frac{n_x n_y n_z}{v_c}\\right\\}$，并且 AUC 必须从高斯等方差信号检测模型中推导出来。AUC 以小数形式报告。\n\n您的程序应生成一行输出，包含一个用方括号括起来的逗号分隔的结果列表。每个测试用例的结果本身必须是一个包含五个浮点数的列表，顺序如下：二维批次内存（MB）、三维批次内存（MB）、上下文比率 $R_V$、二维模型的 AUC、三维模型的 AUC。因此，最终输出必须是一个列表的列表，例如：“[[m2d_A,m3d_A,RV_A,auc2d_A,auc3d_A],[m2d_B,m3d_B,RV_B,auc2d_B,auc3d_B],...]”，除非数值格式需要，否则不添加空格。",
            "solution": "该问题要求对神经影像学中卷积神经网络 (CNN) 的两种输入范式进行定量比较：一种是基于二维 (2D) 切片的方法，另一种是基于三维 (3D) 区块的方法。比较基于三个指标：内存消耗、捕获的物理上下文以及临床性能的代理指标。解决方案将根据所提供的基本原理和建模假设推导得出。\n\n### 1. 内存消耗模型\n\n训练神经网络所需的总内存是复杂的，但它通常由用于反向传播的激活值的存储所主导。问题通过将每批次的内存建模为与激活元素总数成正比，并带有一个恒定的开销因子 $\\gamma$ 来简化此问题。\n\n设 CNN 的输入是一个尺寸为 $n_x \\times n_y \\times n_z$ 的张量。对于 2D 切片，根据定义我们使用 $n_z=1$。该网络是一个具有 $L$ 个阶段的编码器，索引为 $l \\in \\{0, 1, \\dots, L-1\\}$。\n\n在每个阶段 $l$，空间维度会减小一个因子 $2^l$。为了处理整数网格尺寸并防止维度消失，维度 $d \\in \\{x,y,z\\}$ 在阶段 $l$ 的尺寸由以下公式给出：\n$$ n_d^{(l)} = \\max\\left\\{1, \\left\\lfloor \\frac{n_d}{2^l} \\right\\rfloor\\right\\} $$\n\n通道数（特征图数量）在每个阶段都会翻倍，从一个基础数量 $C_0$ 开始：\n$$ C^{(l)} = C_0 \\cdot 2^l $$\n\n在阶段 $l$ 中，单个样本的激活元素总数是空间维度和通道数的乘积：$n_x^{(l)} n_y^{(l)} n_z^{(l)} C^{(l)}$。每个样本的激活元素总数 $E$ 是所有 $L$ 个阶段的总和：\n$$ E = \\sum_{l=0}^{L-1} n_x^{(l)} n_y^{(l)} n_z^{(l)} C^{(l)} $$\n\n给定一个包含 $B$ 个样本的批次、一个大小为 $b_f$ 字节的浮点数以及一个开销因子 $\\gamma$，总内存消耗（以字节为单位）为 $\\gamma \\cdot B \\cdot E \\cdot b_f$。为了将其转换为兆字节 (MB)，我们使用换算关系 $1 \\text{ MB} = 1024^2$ 字节。\n$$ M_{\\text{MB}} = \\frac{\\gamma \\cdot B \\cdot E \\cdot b_f}{1024^2} $$\n\n此计算将分别针对 2D 切片配置（使用其特定的维度 $n_x, n_y$ 和 $n_z=1$，以及批次大小 $B_{\\text{2D}}$）和 3D 区块配置（使用其维度 $n_x, n_y, n_z$ 和批次大小 $B_{\\text{3D}}$）进行。\n\n### 2. 物理上下文捕获模型\n\n输入捕获的物理上下文由其物理视场 (FOV) 体积来量化。这是沿每个轴的体素数量与这些体素的物理间距的乘积。\n\n给定体素间距 $(s_x, s_y, s_z)$（单位为毫米）：\n-   对于一个尺寸为 $(n_x^{\\text{2D}}, n_y^{\\text{2D}})$ 的 2D 切片输入，其厚度被建模为对应于 $z$ 维度上的一个体素。其 FOV 体积为：\n    $$ V_{\\text{2D}} = (n_x^{\\text{2D}} s_x) \\cdot (n_y^{\\text{2D}} s_y) \\cdot (1 \\cdot s_z) $$\n-   对于一个尺寸为 $(n_x^{\\text{3D}}, n_y^{\\text{3D}}, n_z^{\\text{3D}})$ 的 3D 区块输入，其 FOV 体积为：\n    $$ V_{\\text{3D}} = (n_x^{\\text{3D}} s_x) \\cdot (n_y^{\\text{3D}} s_y) \\cdot (n_z^{\\text{3D}} s_z) $$\n\n无量纲的上下文比率 $R_V$ 用于比较 3D 区块与 2D 切片的体积：\n$$ R_V = \\frac{V_{\\text{3D}}}{V_{\\text{2D}}} = \\frac{(n_x^{\\text{3D}} s_x) (n_y^{\\text{3D}} s_y) (n_z^{\\text{3D}} s_z)}{(n_x^{\\text{2D}} s_x) (n_y^{\\text{2D}} s_y) (s_z)} = \\frac{n_x^{\\text{3D}} n_y^{\\text{3D}} n_z^{\\text{3D}}}{n_x^{\\text{2D}} n_y^{\\text{2D}}} $$\n\n### 3. 预测性能代理指标 (AUC)\n\n性能代理指标基于高斯等方差信号检测模型。我们假设一个二元分类任务（例如，病变 vs. 非病变）。对于每个体素，这两个类别被建模为从两个具有相同标准差 $\\sigma$ 但均值相差一个信号差 $s$ 的正态分布中抽样。\n\n单个体素观测的可区分性由指数 $d'_1 = s/\\sigma$ 给出。聚合跨多个体素的信息可以提高可区分性。由于相邻体素之间的空间相关性，有效独立观测数 $N_{\\text{eff}}$ 小于总的体素数 $N_{\\text{vox}} = n_x n_y n_z$。这通过一个由相关范围 $(\\lambda_x, \\lambda_y, \\lambda_z)$ 定义的有效相关体积 $v_c$ 来建模：\n$$ v_c = \\max\\{1, \\lambda_x \\lambda_y \\lambda_z\\} $$\n有效独立观测数则为：\n$$ N_{\\text{eff}} = \\max\\left\\{1, \\frac{N_{\\text{vox}}}{v_c}\\right\\} = \\max\\left\\{1, \\frac{n_x n_y n_z}{v_c}\\right\\} $$\n对于 2D 切片模型，$n_z=1$。\n\n在整个输入体积上聚合信号的可区分性指数 $d'$ 与 $N_{\\text{eff}}$ 的平方根成正比：\n$$ d' = d'_1 \\sqrt{N_{\\text{eff}}} = \\frac{s}{\\sigma} \\sqrt{N_{\\text{eff}}} $$\n\n对于高斯等方差模型，受试者工作特征曲线下面积 (AUC) 通过标准正态分布的累积分布函数 (CDF) $\\Phi(z) = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^{z} e^{-t^2/2} dt$ 与可区分性指数 $d'$ 相关联。公式为：\n$$ \\text{AUC} = \\Phi\\left(\\frac{d'}{\\sqrt{2}}\\right) $$\n这可以使用误差函数 $\\text{erf}(z) = \\frac{2}{\\sqrt{\\pi}} \\int_0^z e^{-t^2} dt$ 通过恒等式 $\\Phi(z) = \\frac{1}{2}\\left(1 + \\text{erf}\\left(\\frac{z}{\\sqrt{2}}\\right)\\right)$ 来计算。代入 $z=d'/\\sqrt{2}$：\n$$ \\text{AUC} = \\frac{1}{2}\\left(1 + \\text{erf}\\left(\\frac{d'/\\sqrt{2}}{\\sqrt{2}}\\right)\\right) = \\frac{1}{2}\\left(1 + \\text{erf}\\left(\\frac{d'}{2}\\right)\\right) $$\n此计算将使用各自的总 voxel 数对 2D 和 3D 输入配置分别进行。\n\n### 计算流程\n实现将包含将这些推导模型编码为代码的函数。对于每个指定的测试用例，将使用适用于 2D 切片和 3D 区块模型的相应参数调用这些函数，以计算五个所需的输出值：2D 内存、3D 内存、上下文比率、2D AUC 和 3D AUC。",
            "answer": "```python\nimport math\nfrom scipy.special import erf\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    It calculates and prints the comparative metrics for 2D vs. 3D CNN inputs.\n    \"\"\"\n\n    def calculate_memory(dims, L, C0, gamma, B, bf):\n        \"\"\"\n        Calculates the estimated memory consumption for a single training batch.\n\n        Args:\n            dims (tuple): Input dimensions (nx, ny, nz).\n            L (int): Number of network stages.\n            C0 (int): Base number of channels.\n            gamma (float): Memory overhead factor.\n            B (int): Batch size.\n            bf (int): Bytes per floating-point number.\n\n        Returns:\n            float: Memory consumption in Megabytes (MB).\n        \"\"\"\n        nx, ny, nz = dims\n        total_elements_per_sample = 0\n        for l in range(L):\n            pow_2_l = 2**l\n            \n            # Spatial dimensions at stage l\n            nx_l = max(1, math.floor(nx / pow_2_l))\n            ny_l = max(1, math.floor(ny / pow_2_l))\n            nz_l = max(1, math.floor(nz / pow_2_l))\n            \n            # Channel count at stage l\n            channels_l = C0 * pow_2_l\n            \n            elements_at_stage = nx_l * ny_l * nz_l * channels_l\n            total_elements_per_sample += elements_at_stage\n            \n        mem_bytes = gamma * B * total_elements_per_sample * bf\n        mem_mb = mem_bytes / (1024**2)\n        return mem_mb\n\n    def calculate_auc(dims, s, sigma, lambdas):\n        \"\"\"\n        Calculates the predicted AUC using a signal detection theory model.\n\n        Args:\n            dims (tuple): Input dimensions (nx, ny, nz).\n            s (float): Per-voxel signal difference.\n            sigma (float): Per-voxel noise standard deviation.\n            lambdas (tuple): Correlation extents (lx, ly, lz) in voxels.\n\n        Returns:\n            float: The Area Under the ROC Curve (AUC).\n        \"\"\"\n        nx, ny, nz = dims\n        lx, ly, lz = lambdas\n        \n        n_vox = nx * ny * nz\n        v_c = max(1, lx * ly * lz)\n        n_eff = max(1, n_vox / v_c)\n        \n        d_prime = (s / sigma) * math.sqrt(n_eff)\n        \n        auc = 0.5 * (1 + erf(d_prime / 2.0))\n        return auc\n\n    # Constants common to all cases\n    bf = 4  # bytes per float\n\n    # Test suite definition\n    test_cases = [\n        {\n            \"name\": \"Case A\",\n            \"n_2d\": (256, 256),\n            \"n_3d\": (64, 64, 16),\n            \"s_xyz\": (1.0, 1.0, 1.0),\n            \"L\": 3, \"C0\": 16, \"gamma\": 3.0,\n            \"B_2d\": 8, \"B_3d\": 2,\n            \"s\": 0.5, \"sigma\": 1.0, \"lambdas\": (3, 3, 3)\n        },\n        {\n            \"name\": \"Case B\",\n            \"n_2d\": (512, 512),\n            \"n_3d\": (128, 128, 4),\n            \"s_xyz\": (0.8, 0.8, 3.0),\n            \"L\": 4, \"C0\": 8, \"gamma\": 2.5,\n            \"B_2d\": 4, \"B_3d\": 1,\n            \"s\": 0.3, \"sigma\": 1.2, \"lambdas\": (5, 5, 1)\n        },\n        {\n            \"name\": \"Case C\",\n            \"n_2d\": (64, 64),\n            \"n_3d\": (64, 64, 1),\n            \"s_xyz\": (1.0, 1.0, 1.0),\n            \"L\": 2, \"C0\": 16, \"gamma\": 3.0,\n            \"B_2d\": 8, \"B_3d\": 8,\n            \"s\": 0.6, \"sigma\": 0.9, \"lambdas\": (2, 2, 2)\n        },\n        {\n            \"name\": \"Case D\",\n            \"n_2d\": (128, 128),\n            \"n_3d\": (96, 96, 24),\n            \"s_xyz\": (0.5, 0.5, 2.0),\n            \"L\": 2, \"C0\": 32, \"gamma\": 3.5,\n            \"B_2d\": 16, \"B_3d\": 1,\n            \"s\": 0.8, \"sigma\": 0.8, \"lambdas\": (2, 2, 2)\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        # Unpack case parameters\n        dims_2d = (case[\"n_2d\"][0], case[\"n_2d\"][1], 1)\n        dims_3d = case[\"n_3d\"]\n        L, C0, gamma = case[\"L\"], case[\"C0\"], case[\"gamma\"]\n        B_2d, B_3d = case[\"B_2d\"], case[\"B_3d\"]\n        s, sigma, lambdas = case[\"s\"], case[\"sigma\"], case[\"lambdas\"]\n        \n        # 1. Memory Calculation\n        mem_2d = calculate_memory(dims_2d, L, C0, gamma, B_2d, bf)\n        mem_3d = calculate_memory(dims_3d, L, C0, gamma, B_3d, bf)\n\n        # 2. Context Ratio Calculation\n        v_ratio = (dims_3d[0] * dims_3d[1] * dims_3d[2]) / (dims_2d[0] * dims_2d[1])\n\n        # 3. AUC Calculation\n        auc_2d = calculate_auc(dims_2d, s, sigma, lambdas)\n        auc_3d = calculate_auc(dims_3d, s, sigma, lambdas)\n\n        case_results = [mem_2d, mem_3d, v_ratio, auc_2d, auc_3d]\n        all_results.append(case_results)\n\n    # Format the final output string as specified\n    result_strings = [f\"[{','.join(map(str, r))}]\" for r in all_results]\n    final_output = f\"[{','.join(result_strings)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "许多复杂的神经系统疾病需要整合来自多种成像模式（如结构、功能和扩散MRI）的信息才能得到更全面的理解。本练习将带您进入一个高级主题：多模态数据融合。您将构建一个简洁而强大的概率图模型，以符合贝叶斯原理的方式融合不同来源的数据，从而推断一个潜在的疾病严重性变量。这项实践还将让您直面并解决临床数据集中一个常见且棘手的挑战——如何优雅地处理部分模态数据缺失的情况。",
            "id": "4491601",
            "problem": "您的任务是设计和分析一个用于神经病学中多模态神经影像融合的概率图模型，其中结构磁共振成像 (sMRI)、功能性磁共振成像 (fMRI) 和弥散张量成像 (DTI) 等模态为一个未观测到的连续神经系统严重性变量提供条件独立观测。您必须从贝叶斯定理、概率图模型中的条件独立性以及高斯先验和高斯似然的共轭性这些基本原理出发。您必须推导其推断过程并将其实现为一个程序。\n\n考虑一个潜变量 $z \\in \\mathbb{R}$，代表神经系统疾病的严重程度。$z$ 的先验是高斯分布，$z \\sim \\mathcal{N}(\\mu_0, \\sigma_0^2)$。每个模态 $i$ 产生一个观测值 $x_i \\in \\mathbb{R}$，其模型为一个线性高斯条件分布，$x_i \\mid z \\sim \\mathcal{N}(a_i z + b_i, \\sigma_i^2)$，并且在给定 $z$ 的条件下，各模态是独立的。多模态融合使用这些似然与先验的乘积来形成 $z$ 的后验分布。缺失的模态必须通过边缘化来处理：在联合模型中对缺失的观测值进行积分，以获得仅给定已观测模态的后验分布。\n\n您的任务是：\n- 从所述基本原理出发，推导当仅观测到模态子集 $\\mathcal{O}$ 时的后验分布 $p(z \\mid \\{x_i\\}_{i \\in \\mathcal{O}})$。用 $(\\mu_0, \\sigma_0^2)$ 以及观测到的模态参数和数据来表示后验均值和方差。\n- 解释为什么在这个线性高斯模型中，对缺失模态 $\\mathcal{M}$ 的边缘化会消除它们的似然贡献，请使用对每个缺失模态 $i \\in \\mathcal{M}$ 的属性 $\\int p(x_i \\mid z) \\, dx_i = 1$。\n- 定义鲁棒性度量，用于比较缺失模态下的后验与所有模态可用时的后验：Kullback–Leibler 散度 $\\mathrm{KL}(\\mathcal{N}(\\mu_{\\text{miss}}, \\sigma^2_{\\text{miss}}) \\Vert \\mathcal{N}(\\mu_{\\text{full}}, \\sigma^2_{\\text{full}}))$、方差比 $r = \\sigma^2_{\\text{miss}} / \\sigma^2_{\\text{full}}$，以及对于给定的真实值 $z^\\star$，均方误差的变化 $\\Delta \\mathrm{MSE} = (\\mu_{\\text{miss}} - z^\\star)^2 - (\\mu_{\\text{full}} - z^\\star)^2$。\n\n实现一个程序，对于下方的每个测试用例，计算后验均值 $\\mu_{\\text{miss}}$、后验方差 $\\sigma^2_{\\text{miss}}$、与全模态后验的 Kullback–Leibler 散度、方差比 $r$ 以及相对于基线全模态后验的均方误差变化 $\\Delta \\mathrm{MSE}$。所有值必须作为浮点数返回，并四舍五入到 $6$ 位小数。\n\n使用以下测试套件，其中所有模态为 sMRI ($i=1$)、fMRI ($i=2$) 和 DTI ($i=3$)。对于每个测试用例，都指定了先验参数 $(\\mu_0, \\sigma_0^2)$、模态参数 $(a_i, b_i, \\sigma_i^2)$、完整观测向量 $(x_1, x_2, x_3)$、真实值 $z^\\star$ 和缺失模态集：\n\n- 测试用例 1（顺利路径，一个中等噪声的模态缺失）：$(\\mu_0, \\sigma_0^2) = (0.0, 1.0)$；sMRI $(a_1, b_1, \\sigma_1^2) = (1.2, 0.0, 0.25)$；fMRI $(a_2, b_2, \\sigma_2^2) = (0.8, 0.0, 0.64)$；DTI $(a_3, b_3, \\sigma_3^2) = (0.5, 0.0, 0.36)$；$(x_1, x_2, x_3) = (0.65, 0.30, 0.35)$；$z^\\star = 0.5$；缺失模态 $\\{2\\}$。\n- 测试用例 2（无缺失模态的恒等情况）：与测试用例 1 相同的参数和观测值；缺失模态 $\\varnothing$。\n- 测试用例 3（边界情况，所有模态均缺失）：与测试用例 1 相同的参数和观测值；缺失模态 $\\{1,2,3\\}$。\n- 测试用例 4（边缘情况，移除非信息性模态）：$(\\mu_0, \\sigma_0^2) = (0.0, 1.0)$；sMRI $(a_1, b_1, \\sigma_1^2) = (1.2, 0.0, 0.25)$；fMRI $(a_2, b_2, \\sigma_2^2) = (0.8, 0.0, 0.64)$；DTI $(a_3, b_3, \\sigma_3^2) = (0.0, 0.0, 0.36)$；$(x_1, x_2, x_3) = (0.65, 0.30, 0.35)$；$z^\\star = 0.5$；缺失模态 $\\{3\\}$。\n- 测试用例 5（鲁棒性压力测试，缺失信息最丰富的模态）：与测试用例 1 相同的参数和观测值；缺失模态 $\\{1\\}$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。对于从1到5的每个测试用例，按顺序附加以下五个四舍五入的值：$[\\mu_{\\text{miss}}, \\sigma^2_{\\text{miss}}, \\mathrm{KL}, r, \\Delta \\mathrm{MSE}]$。因此，最终输出包含25个浮点数，按测试用例的顺序排列，并四舍五入到6位小数，例如 $[\\text{tc1\\_mu},\\text{tc1\\_var},\\text{tc1\\_kl},\\text{tc1\\_r},\\text{tc1\\_dmse},\\ldots,\\text{tc5\\_dmse}]$。",
            "solution": "该问题是有效的，因为它在科学上基于贝叶斯统计，问题设定良好，具有唯一可解的结构，并且其表述是客观的。我们可以着手推导解决方案。\n\n该问题要求对用于多模态神经影像数据融合的概率图模型进行分析。该模型包含一个表示疾病严重程度的连续潜变量 $z \\in \\mathbb{R}$，其服从高斯先验分布；以及一组来自不同成像模态的观测值 $\\{x_i\\}$，每个观测值都通过以 $z$ 为条件的高斯似然进行建模。\n\n### 1. 后验分布的推导\n\n设 $\\mathcal{O}$ 是对应于已观测模态的索引集合。我们被要求推导后验分布 $p(z \\mid \\{x_i\\}_{i \\in \\mathcal{O}})$。根据贝叶斯定理和给定 $z$ 时各模态的条件独立性，后验分布正比于先验分布与已观测模态似然的乘积：\n$$\np(z \\mid \\{x_i\\}_{i \\in \\mathcal{O}}) \\propto p(z) \\prod_{i \\in \\mathcal{O}} p(x_i \\mid z)\n$$\n先验分布是高斯分布：\n$$\np(z) = \\mathcal{N}(z \\mid \\mu_0, \\sigma_0^2) \\propto \\exp\\left( -\\frac{(z - \\mu_0)^2}{2\\sigma_0^2} \\right)\n$$\n每个已观测模态 $i \\in \\mathcal{O}$ 的似然也是高斯分布：\n$$\np(x_i \\mid z) = \\mathcal{N}(x_i \\mid a_i z + b_i, \\sigma_i^2) \\propto \\exp\\left( -\\frac{(x_i - (a_i z + b_i))^2}{2\\sigma_i^2} \\right)\n$$\n因此，后验分布正比于这些指数项的乘积：\n$$\np(z \\mid \\{x_i\\}_{i \\in \\mathcal{O}}) \\propto \\exp\\left( -\\frac{(z - \\mu_0)^2}{2\\sigma_0^2} - \\sum_{i \\in \\mathcal{O}} \\frac{(x_i - a_i z - b_i)^2}{2\\sigma_i^2} \\right)\n$$\n高斯分布的乘积是一个未归一化的高斯分布。为了找到后验分布的参数，我们将其表示为 $\\mathcal{N}(z \\mid \\mu_{\\mathcal{O}}, \\sigma_{\\mathcal{O}}^2)$。其概率密度函数的形式为：\n$$\np(z \\mid \\{x_i\\}_{i \\in \\mathcal{O}}) \\propto \\exp\\left( -\\frac{(z - \\mu_{\\mathcal{O}})^2}{2\\sigma_{\\mathcal{O}}^2} \\right)\n$$\n我们可以通过展开后验分布指数中的二次项并匹配 $z^2$ 和 $z$ 的系数来找到 $\\mu_{\\mathcal{O}}$ 和 $\\sigma_{\\mathcal{O}}^2$。指数的参数为：\n$$\nL(z) = -\\frac{1}{2} \\left[ \\frac{(z - \\mu_0)^2}{\\sigma_0^2} + \\sum_{i \\in \\mathcal{O}} \\frac{(a_i z - (x_i - b_i))^2}{\\sigma_i^2} \\right] \\\\\n= -\\frac{1}{2} \\left[ \\frac{z^2 - 2z\\mu_0 + \\mu_0^2}{\\sigma_0^2} + \\sum_{i \\in \\mathcal{O}} \\frac{a_i^2 z^2 - 2a_i z (x_i - b_i) + (x_i - b_i)^2}{\\sigma_i^2} \\right]\n$$\n收集 $z^2$ 和 $z$ 的项：\n$$\nL(z) = -\\frac{1}{2} \\left[ z^2 \\left( \\frac{1}{\\sigma_0^2} + \\sum_{i \\in \\mathcal{O}} \\frac{a_i^2}{\\sigma_i^2} \\right) - 2z \\left( \\frac{\\mu_0}{\\sigma_0^2} + \\sum_{i \\in \\mathcal{O}} \\frac{a_i(x_i - b_i)}{\\sigma_i^2} \\right) + C \\right]\n$$\n其中 $C$ 包含不依赖于 $z$ 的项。\n通过将其与高斯指数的标准二次型 $-\\frac{1}{2\\sigma_{\\mathcal{O}}^2}(z^2 - 2z\\mu_{\\mathcal{O}} + \\mu_{\\mathcal{O}}^2)$ 进行比较，我们可以确定后验分布的逆方差（精度）和均值。\n\n$z^2$ 的系数给出了后验精度：\n$$\n\\frac{1}{\\sigma_{\\mathcal{O}}^2} = \\frac{1}{\\sigma_0^2} + \\sum_{i \\in \\mathcal{O}} \\frac{a_i^2}{\\sigma_i^2}\n$$\n所以，后验方差是：\n$$\n\\sigma_{\\mathcal{O}}^2 = \\left( \\frac{1}{\\sigma_0^2} + \\sum_{i \\in \\mathcal{O}} \\frac{a_i^2}{\\sigma_i^2} \\right)^{-1}\n$$\n$-2z$ 的系数让我们能够找到后验均值：\n$$\n\\frac{\\mu_{\\mathcal{O}}}{\\sigma_{\\mathcal{O}}^2} = \\frac{\\mu_0}{\\sigma_0^2} + \\sum_{i \\in \\mathcal{O}} \\frac{a_i(x_i - b_i)}{\\sigma_i^2}\n$$\n因此，后验均值是：\n$$\n\\mu_{\\mathcal{O}} = \\sigma_{\\mathcal{O}}^2 \\left( \\frac{\\mu_0}{\\sigma_0^2} + \\sum_{i \\in \\mathcal{O}} \\frac{a_i(x_i - b_i)}{\\sigma_i^2} \\right)\n$$\n这些方程定义了后验分布 $p(z \\mid \\{x_i\\}_{i \\in \\mathcal{O}}) = \\mathcal{N}(z \\mid \\mu_{\\mathcal{O}}, \\sigma_{\\mathcal{O}}^2)$ 的参数。对于给定的测试用例，$\\mu_{\\text{miss}}$ 和 $\\sigma^2_{\\text{miss}}$ 是使用这些公式计算的，其中 $\\mathcal{O}$ 是已观测模态的集合。全后验参数 $\\mu_{\\text{full}}$ 和 $\\sigma^2_{\\text{full}}$ 的计算方式类似，但此时 $\\mathcal{O}$ 是所有模态的集合。\n\n### 2. 对缺失模态的边缘化\n\n让所有模态索引的完整集合为 $\\mathcal{I}$，它是已观测模态 $\\mathcal{O}$ 和缺失模态 $\\mathcal{M}$ 的不相交并集，即 $\\mathcal{I} = \\mathcal{O} \\cup \\mathcal{M}$。我们寻求后验 $p(z \\mid \\{x_i\\}_{i \\in \\mathcal{O}})$。根据条件概率的定义：\n$$\np(z \\mid \\{x_i\\}_{i \\in \\mathcal{O}}) = \\frac{p(z, \\{x_i\\}_{i \\in \\mathcal{O}})}{p(\\{x_i\\}_{i \\in \\mathcal{O}})}\n$$\n$z$ 和已观测数据 $\\{x_i\\}_{i \\in \\mathcal{O}}$ 的联合分布是通过从完整联合分布 $p(z, \\{x_i\\}_{i \\in \\mathcal{I}})$ 中对缺失模态的变量 $\\{x_j\\}_{j \\in \\mathcal{M}}$ 进行边缘化（积分）得到的：\n$$\np(z, \\{x_i\\}_{i \\in \\mathcal{O}}) = \\int p(z, \\{x_i\\}_{i \\in \\mathcal{I}}) \\prod_{j \\in \\mathcal{M}} dx_j\n$$\n使用链式法则和条件独立性，完整联合分布为 $p(z, \\{x_i\\}_{i \\in \\mathcal{I}}) = p(z) \\prod_{i \\in \\mathcal{I}} p(x_i \\mid z)$。将其代入积分中：\n$$\np(z, \\{x_i\\}_{i \\in \\mathcal{O}}) = \\int p(z) \\left(\\prod_{i \\in \\mathcal{O}} p(x_i \\mid z)\\right) \\left(\\prod_{j \\in \\mathcal{M}} p(x_j \\mid z)\\right) \\prod_{j \\in \\mathcal{M}} dx_j\n$$\n仅依赖于 $z$ 和 $\\{x_i\\}_{i \\in \\mathcal{O}}$ 的项可以移到积分之外：\n$$\np(z, \\{x_i\\}_{i \\in \\mathcal{O}}) = p(z) \\left(\\prod_{i \\in \\mathcal{O}} p(x_i \\mid z)\\right) \\left(\\prod_{j \\in \\mathcal{M}} \\int p(x_j \\mid z) dx_j \\right)\n$$\n对于任何缺失模态 $j \\in \\mathcal{M}$，项 $p(x_j \\mid z)$ 是一个概率密度函数。根据定义，任何概率密度函数（PDF）在其整个定义域上的积分等于 $1$：\n$$\n\\int_{-\\infty}^{\\infty} p(x_j \\mid z) dx_j = 1\n$$\n无论 $z$ 的值如何，这都成立。因此，这些积分的乘积也为 $1$。这将联合分布简化为：\n$$\np(z, \\{x_i\\}_{i \\in \\mathcal{O}}) = p(z) \\prod_{i \\in \\mathcal{O}} p(x_i \\mid z)\n$$\n因此，后验变为：\n$$\np(z \\mid \\{x_i\\}_{i \\in \\mathcal{O}}) \\propto p(z) \\prod_{i \\in \\mathcal{O}} p(x_i \\mid z)\n$$\n这表明，通过边缘化处理缺失模态，在数学上等同于在贝叶斯法则的乘积中简单地省略它们的似然项。来自缺失模态的信息被消除了。\n\n### 3. 鲁棒性度量\n\n问题指定了三个度量来量化缺失数据对后验分布的影响。让来自完整模态集的后验为 $\\mathcal{N}_{\\text{full}} = \\mathcal{N}(\\mu_{\\text{full}}, \\sigma^2_{\\text{full}})$，来自已观测模态子集的后验为 $\\mathcal{N}_{\\text{miss}} = \\mathcal{N}(\\mu_{\\text{miss}}, \\sigma^2_{\\text{miss}})$。\n\n1.  **Kullback–Leibler (KL) 散度**：从 $\\mathcal{N}_{\\text{miss}}$ 到 $\\mathcal{N}_{\\text{full}}$ 的KL散度衡量了用 $\\mathcal{N}_{\\text{full}}$ 近似 $\\mathcal{N}_{\\text{miss}}$ 时丢失的信息。对于两个一元高斯分布 $\\mathcal{N}_1 = \\mathcal{N}(\\mu_1, \\sigma_1^2)$ 和 $\\mathcal{N}_2 = \\mathcal{N}(\\mu_2, \\sigma_2^2)$，其公式为：\n    $$\n    \\mathrm{KL}(\\mathcal{N}_1 \\Vert \\mathcal{N}_2) = \\log\\frac{\\sigma_2}{\\sigma_1} + \\frac{\\sigma_1^2 + (\\mu_1 - \\mu_2)^2}{2\\sigma_2^2} - \\frac{1}{2}\n    $$\n    此处，$\\mathcal{N}_1 = \\mathcal{N}_{\\text{miss}}$ 且 $\\mathcal{N}_2 = \\mathcal{N}_{\\text{full}}$。\n\n2.  **方差比**：这是有数据缺失时的后验方差与全数据时的后验方差之比。\n    $$\n    r = \\frac{\\sigma^2_{\\text{miss}}}{\\sigma^2_{\\text{full}}}\n    $$\n    由于观测更多数据只会增加后验精度（或使其保持不变），我们预期 $\\sigma^2_{\\text{miss}} \\geq \\sigma^2_{\\text{full}}$，因此 $r \\geq 1$。\n\n3.  **均方误差的变化 ($\\Delta \\mathrm{MSE}$)**：该度量评估了相对于真实值 $z^\\star$，$z$ 的点估计（取后验均值）因数据缺失而退化的程度。\n    $$\n    \\Delta \\mathrm{MSE} = (\\mu_{\\text{miss}} - z^\\star)^2 - (\\mu_{\\text{full}} - z^\\star)^2\n    $$\n    正值表示缺失数据增加了估计的平方误差。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the multimodal neuroimaging fusion problem for the given test cases.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"prior\": (0.0, 1.0),\n            \"modalities\": {\n                1: {\"params\": (1.2, 0.0, 0.25), \"obs\": 0.65},  # sMRI\n                2: {\"params\": (0.8, 0.0, 0.64), \"obs\": 0.30},  # fMRI\n                3: {\"params\": (0.5, 0.0, 0.36), \"obs\": 0.35},  # DTI\n            },\n            \"z_star\": 0.5,\n            \"missing\": {2}\n        },\n        {\n            \"prior\": (0.0, 1.0),\n            \"modalities\": {\n                1: {\"params\": (1.2, 0.0, 0.25), \"obs\": 0.65},\n                2: {\"params\": (0.8, 0.0, 0.64), \"obs\": 0.30},\n                3: {\"params\": (0.5, 0.0, 0.36), \"obs\": 0.35},\n            },\n            \"z_star\": 0.5,\n            \"missing\": set()\n        },\n        {\n            \"prior\": (0.0, 1.0),\n            \"modalities\": {\n                1: {\"params\": (1.2, 0.0, 0.25), \"obs\": 0.65},\n                2: {\"params\": (0.8, 0.0, 0.64), \"obs\": 0.30},\n                3: {\"params\": (0.5, 0.0, 0.36), \"obs\": 0.35},\n            },\n            \"z_star\": 0.5,\n            \"missing\": {1, 2, 3}\n        },\n        {\n            \"prior\": (0.0, 1.0),\n            \"modalities\": {\n                1: {\"params\": (1.2, 0.0, 0.25), \"obs\": 0.65},\n                2: {\"params\": (0.8, 0.0, 0.64), \"obs\": 0.30},\n                3: {\"params\": (0.0, 0.0, 0.36), \"obs\": 0.35},  # Non-informative DTI\n            },\n            \"z_star\": 0.5,\n            \"missing\": {3}\n        },\n        {\n            \"prior\": (0.0, 1.0),\n            \"modalities\": {\n                1: {\"params\": (1.2, 0.0, 0.25), \"obs\": 0.65},\n                2: {\"params\": (0.8, 0.0, 0.64), \"obs\": 0.30},\n                3: {\"params\": (0.5, 0.0, 0.36), \"obs\": 0.35},\n            },\n            \"z_star\": 0.5,\n            \"missing\": {1}\n        },\n    ]\n\n    def calculate_posterior(prior, modalities, observations, observed_indices):\n        \"\"\"\n        Calculates the posterior mean and variance given a set of observations.\n        \n        Args:\n            prior (tuple): (mu_0, sigma_0_sq).\n            modalities (dict): Dictionary of modality parameters.\n            observations (dict): Dictionary of modality observations.\n            observed_indices (set): Set of indices for observed modalities.\n\n        Returns:\n            tuple: (posterior_mean, posterior_variance).\n        \"\"\"\n        mu_0, sigma_0_sq = prior\n        \n        # Precision = 1 / variance\n        post_precision = 1.0 / sigma_0_sq\n        mu_prec_term = mu_0 / sigma_0_sq\n        \n        for i in observed_indices:\n            a_i, b_i, sigma_i_sq = modalities[i]\n            x_i = observations[i]\n            \n            # Add likelihood precision\n            lik_precision = (a_i ** 2) / sigma_i_sq\n            post_precision += lik_precision\n            \n            # Add weighted mean term from likelihood\n            mu_prec_term += (a_i * (x_i - b_i)) / sigma_i_sq\n            \n        post_variance = 1.0 / post_precision\n        post_mean = post_variance * mu_prec_term\n        \n        return post_mean, post_variance\n\n    final_results = []\n    \n    for case in test_cases:\n        prior_params = case[\"prior\"]\n        modality_params = {k: v[\"params\"] for k, v in case[\"modalities\"].items()}\n        observations = {k: v[\"obs\"] for k, v in case[\"modalities\"].items()}\n        z_star = case[\"z_star\"]\n        missing_indices = case[\"missing\"]\n        all_indices = set(modality_params.keys())\n        observed_indices = all_indices - missing_indices\n\n        # 1. Calculate full posterior (all modalities observed)\n        mu_full, var_full = calculate_posterior(\n            prior_params, modality_params, observations, all_indices\n        )\n\n        # 2. Calculate posterior with missing data\n        mu_miss, var_miss = calculate_posterior(\n            prior_params, modality_params, observations, observed_indices\n        )\n\n        # 3. Calculate metrics\n        # KL Divergence: KL(miss || full)\n        if var_miss = 0 or var_full = 0: # Avoid log(0) or division by zero\n            kl_div = float('inf') if var_full > 0 else 0.0\n        else:\n            term1 = np.log(np.sqrt(var_full) / np.sqrt(var_miss))\n            term2 = (var_miss + (mu_miss - mu_full)**2) / (2 * var_full)\n            kl_div = term1 + term2 - 0.5\n\n        # Variance Ratio\n        var_ratio = var_miss / var_full if var_full > 0 else float('inf')\n\n        # Change in MSE\n        delta_mse = (mu_miss - z_star)**2 - (mu_full - z_star)**2\n\n        # In case of perfect match (e.g. no missing data), KL can be tiny negative due to float precision. Clamp to 0.\n        if abs(kl_div)  1e-12: kl_div = 0.0\n        if abs(var_ratio - 1.0)  1e-12: var_ratio = 1.0\n        if abs(delta_mse)  1e-12: delta_mse = 0.0\n\n        case_results = [mu_miss, var_miss, kl_div, var_ratio, delta_mse]\n        final_results.extend(case_results)\n\n    # Format output\n    formatted_results = [f\"{val:.6f}\" for val in final_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n\n```"
        }
    ]
}