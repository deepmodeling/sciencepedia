{
    "hands_on_practices": [
        {
            "introduction": "Visual acuity is the cornerstone of visual function assessment, but its measurement is expressed in various notations across clinical and research settings. This exercise bridges the gap between the common Snellen fraction, the psychophysically rigorous logMAR scale, and the underlying geometry of the eye and test chart. By working through these conversions, you will gain a concrete understanding of how different acuity metrics relate to each other and to the physical stimulus presented to the patient .",
            "id": "4733066",
            "problem": "A patient’s distance visual acuity is recorded at a standard testing distance using the Snellen notation $20/40$. In clinical visual psychophysics, the decimal visual acuity is defined by the ratio of testing distance to the “optotype design distance” (the denominator of the Snellen fraction), the minimum angle of resolution (MAR) is defined as the smallest resolvable feature in the optotype expressed in arcminutes and is the reciprocal of the decimal acuity, and the logarithm of the minimum angle of resolution (logMAR) is defined as $\\log_{10}(\\mathrm{MAR})$. Separately, the geometric visual angle $\\theta$ subtended by an object of physical height $H$ at viewing distance $d$ follows the small-angle optical geometry of the eye, which can be derived from first principles of Euclidean geometry: the full subtended angle satisfies $\\theta = 2 \\arctan\\!\\big(H/(2d)\\big)$, and for sufficiently small angles one may approximate $\\theta \\approx H/d$ after converting $\\theta$ to radians. Assume the standard optotype design in which the overall letter height subtends $5$ arcminutes.\n\nStarting from these core definitions and geometric relations, perform the following for the given Snellen acuity $20/40$ and viewing distance $d = 4\\,\\mathrm{m}$:\n\n1. Convert the Snellen acuity $20/40$ to logMAR, where $\\log_{10}$ denotes base-$10$ logarithm.\n2. State the MAR in arcminutes.\n3. Compute the physical letter height $H$ in millimeters required at $d=4\\,\\mathrm{m}$ to subtend an overall angle of $5$ arcminutes (use the exact geometric relation $\\theta = 2 \\arctan\\!\\big(H/(2d)\\big)$ and convert the angle to radians for calculation; do not use the small-angle approximation in the final numerical computation).\n\nRound the logMAR and the letter height to four significant figures. Express the MAR as a real number in arcminutes without rounding. Report your final answer as three quantities in the order: logMAR (dimensionless), MAR (in arcminutes), and letter height (in millimeters).",
            "solution": "The Snellen notation $20/40$ encodes the ratio of testing distance to the “design distance” of the optotype line. The decimal visual acuity $A_{\\mathrm{dec}}$ is defined by\n$$\nA_{\\mathrm{dec}} = \\frac{20}{40} = \\frac{1}{2} = 0.5.\n$$\nBy definition in visual psychophysics, the minimum angle of resolution (MAR) is the smallest resolvable feature size in angular units (arcminutes), and is the reciprocal of decimal acuity:\n$$\n\\mathrm{MAR} = \\frac{1}{A_{\\mathrm{dec}}} = \\frac{1}{0.5} = 2 \\text{ arcminutes}.\n$$\nThe logarithm of the minimum angle of resolution (logMAR) is defined as\n$$\n\\mathrm{logMAR} = \\log_{10}(\\mathrm{MAR}) = \\log_{10}(2).\n$$\nSymbolically, $\\log_{10}(2)$ is exact; to report a numerical value to four significant figures, evaluate\n$$\n\\log_{10}(2) \\approx 0.3010 \\quad \\text{(four significant figures)}.\n$$\n\nFor the physical letter height, use the exact geometric relationship between height $H$, viewing distance $d$, and full subtended angle $\\theta$:\n$$\n\\theta = 2 \\arctan\\!\\left(\\frac{H}{2 d}\\right).\n$$\nWe are given that the standard overall letter height should subtend $\\theta = 5$ arcminutes. To use trigonometric functions consistently, convert $\\theta$ to radians. There are $60$ arcminutes in $1$ degree and $\\pi$ radians in $180$ degrees, so\n$$\n\\theta_{\\mathrm{rad}} = 5 \\times \\frac{1}{60} \\times \\frac{\\pi}{180} = \\frac{5 \\pi}{10800} \\text{ radians}.\n$$\nSolving the exact relation for $H$ gives\n$$\n\\theta = 2 \\arctan\\!\\left(\\frac{H}{2 d}\\right) \\;\\Rightarrow\\; \\arctan\\!\\left(\\frac{H}{2 d}\\right) = \\frac{\\theta}{2} \\;\\Rightarrow\\; \\frac{H}{2 d} = \\tan\\!\\left(\\frac{\\theta}{2}\\right) \\;\\Rightarrow\\; H = 2 d \\, \\tan\\!\\left(\\frac{\\theta}{2}\\right).\n$$\nSubstitute $d = 4\\,\\mathrm{m}$ and $\\theta_{\\mathrm{rad}} = \\frac{5 \\pi}{10800}$:\n$$\nH = 2 \\times 4 \\times \\tan\\!\\left(\\frac{1}{2} \\cdot \\frac{5 \\pi}{10800}\\right) \\,\\mathrm{m} = 8 \\, \\tan\\!\\left(\\frac{5 \\pi}{21600}\\right) \\,\\mathrm{m}.\n$$\nTo express the height in millimeters, multiply by $1000$:\n$$\nH_{\\mathrm{mm}} = 1000 \\times 8 \\, \\tan\\!\\left(\\frac{5 \\pi}{21600}\\right) = 8000 \\, \\tan\\!\\left(\\frac{5 \\pi}{21600}\\right) \\;\\text{mm}.\n$$\nNumerically, since $\\frac{5 \\pi}{21600}$ is a small angle, we still use the exact $\\tan$ for computation and then round:\n$$\nH_{\\mathrm{mm}} \\approx 5.81776 \\,\\text{mm}.\n$$\nRounded to four significant figures:\n$$\nH_{\\mathrm{mm}} \\approx 5.818 \\,\\text{mm}.\n$$\n\nCollecting results in the required order:\n- logMAR (dimensionless): $\\log_{10}(2) \\approx 0.3010$ (four significant figures),\n- MAR: $2$ arcminutes (exact real number, no rounding),\n- Letter height at $d=4\\,\\mathrm{m}$ to subtend $5$ arcminutes: $5.818$ millimeters (four significant figures).",
            "answer": "$$\\boxed{\\begin{pmatrix}0.3010 & 2 & 5.818\\end{pmatrix}}$$"
        },
        {
            "introduction": "Beyond central acuity, perimetry assesses the integrity of the entire visual field, where the principles of spatial summation become paramount. This practice explores how the visual system's ability to detect a stimulus changes with its size, a phenomenon described by Piper's Law in the peripheral retina. Mastering this calculation is key to understanding the design of automated perimeters and interpreting threshold changes across different stimulus settings in clinical practice .",
            "id": "4733089",
            "problem": "A laboratory is conducting static automated perimetry to compare detection thresholds for two standard Goldmann stimulus sizes at a peripheral retinal location. At an eccentricity of $30^\\circ$ in the horizontal meridian under a steady photopic background, both Goldmann size III and Goldmann size V stimuli are known to lie outside complete spatial summation. The experimenters wish to estimate how much the differential luminance threshold (the luminance increment above background required for detection) changes when switching from Goldmann size III to Goldmann size V, assuming spatial partial summation at this eccentricity.\n\nUse the following well-tested facts as the foundational base:\n- For spatial summation, the differential luminance threshold $T$ depends on stimulus area $A$. Under complete spatial summation (Ricco’s law), $T \\propto A^{-1}$. Beyond the critical area (partial summation), empirical data at peripheral locations are well fitted by a power law $T \\propto A^{-\\beta}$ with $0<\\beta<1$ (Piper-type behavior). At $30^\\circ$ eccentricity under photopic backgrounds, a widely observed slope is $\\beta = \\tfrac{1}{2}$.\n- The standard angular diameters of Goldmann size III and Goldmann size V are $0.43^\\circ$ and $1.72^\\circ$, respectively.\n- For circular stimuli, area scales with the square of angular diameter.\n- A change in threshold expressed in decibels (dB) is defined as $10 \\log_{10}\\!\\left(\\dfrac{T_{2}}{T_{1}}\\right)$, where $T_{1}$ and $T_{2}$ are the thresholds being compared.\n\nStarting from these bases, derive and compute the expected signed change in differential luminance threshold, in decibels, when moving from Goldmann size III to Goldmann size V at $30^\\circ$ eccentricity under partial summation. A negative value indicates that a lower luminance increment is required for detection with the larger stimulus. Round your final answer to four significant figures and express it in dB (decibels).",
            "solution": "Let $T_{III}$ and $T_V$ be the differential luminance thresholds for the Goldmann size III and V stimuli, respectively. Let their areas be $A_{III}$ and $A_V$, and their diameters be $d_{III}$ and $d_V$.\n\nAccording to the problem, the threshold $T$ in the partial summation regime follows the power law $T \\propto A^{-\\beta}$, with a given exponent $\\beta = \\frac{1}{2}$. Thus, $T \\propto A^{-1/2}$.\n\nThe ratio of the thresholds for the two stimuli is therefore:\n$$\\frac{T_V}{T_{III}} = \\left(\\frac{A_V}{A_{III}}\\right)^{-\\beta} = \\left(\\frac{A_V}{A_{III}}\\right)^{-1/2}$$\n\nSince the stimuli are circular, their area $A$ is proportional to the square of their diameter $d$ ($A \\propto d^2$). The area ratio can be expressed in terms of the diameter ratio:\n$$\\frac{A_V}{A_{III}} = \\left(\\frac{d_V}{d_{III}}\\right)^2$$\n\nSubstituting the area ratio into the threshold ratio equation gives:\n$$\\frac{T_V}{T_{III}} = \\left(\\left(\\frac{d_V}{d_{III}}\\right)^2\\right)^{-1/2} = \\left(\\frac{d_V}{d_{III}}\\right)^{-1} = \\frac{d_{III}}{d_V}$$\n\nUsing the given diameters $d_{III} = 0.43^\\circ$ and $d_V = 1.72^\\circ$:\n$$\\frac{T_V}{T_{III}} = \\frac{0.43}{1.72} = \\frac{1}{4} = 0.25$$\n\nThe change in threshold in decibels, $\\Delta_{\\text{dB}}$, is calculated as:\n$$\\Delta_{\\text{dB}} = 10 \\log_{10}\\left(\\frac{T_V}{T_{III}}\\right) = 10 \\log_{10}(0.25)$$\n\nEvaluating this expression:\n$$\\Delta_{\\text{dB}} = 10 \\log_{10}\\left(\\frac{1}{4}\\right) = -10 \\log_{10}(4) = -20 \\log_{10}(2)$$\n\nUsing $\\log_{10}(2) \\approx 0.30103$, we get:\n$$\\Delta_{\\text{dB}} \\approx -20 \\times 0.30103 = -6.0206$$\nRounding to four significant figures, the result is $-6.021$. The negative value confirms that the larger stimulus is easier to detect (i.e., has a lower threshold).",
            "answer": "$$\n\\boxed{-6.021}\n$$"
        },
        {
            "introduction": "The Contrast Sensitivity Function (CSF) offers a far more comprehensive picture of visual performance than a single acuity value, characterizing sensitivity across a wide range of spatial frequencies. This computational practice guides you through the process of fitting a standard mathematical model to CSF data, a fundamental skill in modern vision science. By transforming the model and applying linear regression, you will extract key parameters that define an individual's window of visibility, moving from raw data to a meaningful functional description .",
            "id": "4733125",
            "problem": "You are given a parametric model for the human Contrast Sensitivity Function (CSF), a standard model used in visual function testing in ophthalmology. The model describes sensitivity as a function of spatial frequency and is defined as follows: for spatial frequency $f$ (in cycles per degree), the sensitivity $S(f)$ is\n$$\nS(f) = S_0 \\exp\\!\\left(-\\alpha \\ln^2\\!\\left(\\frac{f}{f_0}\\right)\\right),\n$$\nwhere $S_0$ is the peak sensitivity (unitless), $f_0$ is the peak frequency (in cycles per degree), and $\\alpha$ is a known positive curvature parameter. All logarithms are natural logarithms.\n\nYour task is to design and implement a program that, for each dataset, estimates the peak sensitivity $S_0$ and the peak frequency $f_0$ by principled fitting from first principles. Assume that measurement noise in sensitivity is multiplicative (equivalently, additive in log-sensitivity), independent across frequencies, and approximately Gaussian in the log domain. Under this assumption, estimate $(S_0, f_0)$ by minimizing the sum of squared residuals between observed and model-predicted log-sensitivities. Constrain the estimates to $S_0 > 0$ and $f_0 > 0$. Use only the given data and fundamental principles; you must not assume or use any pre-derived closed-form \"shortcuts\" not derived from these principles.\n\nImportant requirements:\n- Use the natural logarithm throughout.\n- Report $f_0$ in cycles per degree (cpd).\n- Report $S_0$ as a unitless quantity.\n- Round $S_0$ to two decimal places and $f_0$ to three decimal places in the final output.\n\nTest suite (each test case provides $\\alpha$, spatial frequencies $f_i$ in cycles per degree, and observed sensitivities $S_i$):\n- Test Case 1 (noise-free, symmetric sampling around the peak):\n  - $\\alpha = 0.4$\n  - $f = [0.5, 1, 2, 4, 8, 16]$ (in cpd)\n  - $S = [44.33, 115.89, 206.35, 250.00, 206.35, 115.89]$\n- Test Case 2 (noisy, dense sampling around the peak and in the tails):\n  - $\\alpha = 0.5$\n  - $f = [0.5, 1, 2, 3, 3.5, 5, 7, 10, 16]$ (in cpd)\n  - $S = [34.77, 95.41, 191.91, 210.88, 220.00, 214.71, 166.13, 114.18, 58.87]$\n- Test Case 3 (sparse sampling, moderate curvature):\n  - $\\alpha = 0.6$\n  - $f = [0.75, 2, 6]$ (in cpd)\n  - $S = [84.23, 147.00, 74.15]$\n- Test Case 4 (flatter CSF, broader peak with moderate noise):\n  - $\\alpha = 0.15$\n  - $f = [2, 4, 8, 10, 12, 16, 24]$ (in cpd)\n  - $S = [131.59, 181.63, 202.48, 200.00, 195.03, 185.75, 165.78]$\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a two-element list $[S_0^{\\text{hat}}, f_0^{\\text{hat}}]$ for the corresponding test case in order.\n- The required format is:\n  - $[[S_{0,1}^{\\text{hat}}, f_{0,1}^{\\text{hat}}],[S_{0,2}^{\\text{hat}}, f_{0,2}^{\\text{hat}}],[S_{0,3}^{\\text{hat}}, f_{0,3}^{\\text{hat}}],[S_{0,4}^{\\text{hat}}, f_{0,4}^{\\text{hat}}]]$\n- Round $S_0^{\\text{hat}}$ to two decimal places and $f_0^{\\text{hat}}$ to three decimal places, and report $f_0^{\\text{hat}}$ in cycles per degree (cpd).",
            "solution": "The problem is assessed to be valid as it is scientifically grounded in the field of ophthalmology, well-posed as a parameter estimation task, and specified with all necessary data and objective constraints.\n\nThe objective is to estimate the peak sensitivity $S_0$ and peak frequency $f_0$ for the Contrast Sensitivity Function (CSF) model:\n$$\nS(f) = S_0 \\exp\\!\\left(-\\alpha \\ln^2\\!\\left(\\frac{f}{f_0}\\right)\\right)\n$$\ngiven a set of measurements $(f_i, S_i)$ and a known curvature parameter $\\alpha$. The estimation must be based on the principle of minimizing the sum of squared residuals in the log-sensitivity domain.\n\n### Derivation from First Principles\n\nThe core principle is least-squares estimation applied to a transformed version of the model. The noise is assumed to be multiplicative in sensitivity $S$, which is equivalent to being additive and Gaussian in log-sensitivity $\\ln(S)$. This implies that the appropriate objective function to minimize is the sum of squared errors in the log domain.\n\n1.  **Log-Transformation of the Model:**\n    We begin by taking the natural logarithm of the model equation to linearize its structure. Let $y = \\ln(S)$ and $x = \\ln(f)$.\n    $$\n    \\ln(S(f)) = \\ln\\left(S_0 \\exp\\left(-\\alpha \\ln^2\\left(\\frac{f}{f_0}\\right)\\right)\\right)\n    $$\n    Using logarithm properties, $\\ln(ab) = \\ln(a) + \\ln(b)$ and $\\ln(e^c) = c$:\n    $$\n    \\ln(S(f)) = \\ln(S_0) - \\alpha \\ln^2\\left(\\frac{f}{f_0}\\right)\n    $$\n    Further expanding the squared term using $\\ln(a/b) = \\ln(a) - \\ln(b)$:\n    $$\n    \\ln(S) = \\ln(S_0) - \\alpha (\\ln(f) - \\ln(f_0))^2\n    $$\n    This equation describes a parabola in the log-log plot of sensitivity versus frequency.\n\n2.  **Linearization through Change of Variables:**\n    To simplify the estimation, we can rearrange the equation into a form amenable to linear regression. Let $y_0 = \\ln(S_0)$ and $x_0 = \\ln(f_0)$ be the transformed parameters to estimate. The model for a single data point $(x_i, y_i)$, where $x_i = \\ln(f_i)$ and $y_i=\\ln(S_i)$, is:\n    $$\n    y_i \\approx y_0 - \\alpha(x_i - x_0)^2\n    $$\n    Expanding the quadratic term:\n    $$\n    y_i \\approx y_0 - \\alpha(x_i^2 - 2x_i x_0 + x_0^2)\n    $$\n    Rearranging the terms to group the unknowns:\n    $$\n    y_i + \\alpha x_i^2 \\approx (y_0 - \\alpha x_0^2) + (2\\alpha x_0) x_i\n    $$\n    This equation is now in the form of a linear model. Let us define a new dependent variable $z_i = y_i + \\alpha x_i^2$ and new coefficients $c_0 = y_0 - \\alpha x_0^2$ and $c_1 = 2\\alpha x_0$. The model becomes a simple linear regression:\n    $$\n    z_i \\approx c_0 + c_1 x_i\n    $$\n\n3.  **Least-Squares Minimization:**\n    We need to find the estimates $\\hat{c_0}$ and $\\hat{c_1}$ that minimize the sum of squared residuals, $Q$:\n    $$\n    Q(c_0, c_1) = \\sum_{i=1}^n (z_i - (c_0 + c_1 x_i))^2\n    $$\n    To find the minimum, we set the partial derivatives of $Q$ with respect to $c_0$ and $c_1$ to zero.\n    $$\n    \\frac{\\partial Q}{\\partial c_0} = \\sum_{i=1}^n 2(z_i - c_0 - c_1 x_i)(-1) = 0 \\implies \\sum_{i=1}^n (z_i - c_0 - c_1 x_i) = 0\n    $$\n    $$\n    \\frac{\\partial Q}{\\partial c_1} = \\sum_{i=1}^n 2(z_i - c_0 - c_1 x_i)(-x_i) = 0 \\implies \\sum_{i=1}^n (z_i x_i - c_0 x_i - c_1 x_i^2) = 0\n    $$\n    These yield the normal equations for a simple linear regression:\n    $$\n    \\begin{align*}\n    n c_0 + \\left(\\sum x_i\\right) c_1 &= \\sum z_i \\\\\n    \\left(\\sum x_i\\right) c_0 + \\left(\\sum x_i^2\\right) c_1 &= \\sum x_i z_i\n    \\end{align*}\n    $$\n    This is a $2 \\times 2$ system of linear equations for $(c_0, c_1)$. Solving this system gives the least-squares estimates $\\hat{c_0}$ and $\\hat{c_1}$:\n    $$\n    \\hat{c_1} = \\frac{n(\\sum x_i z_i) - (\\sum x_i)(\\sum z_i)}{n(\\sum x_i^2) - (\\sum x_i)^2}\n    $$\n    $$\n    \\hat{c_0} = \\bar{z} - \\hat{c_1} \\bar{x}\n    $$\n    where $\\bar{x} = \\frac{1}{n}\\sum x_i$ and $\\bar{z} = \\frac{1}{n}\\sum z_i$.\n\n4.  **Back-transformation to Original Parameters:**\n    Once $\\hat{c_0}$ and $\\hat{c_1}$ are computed, we can recover the estimates for the original log-domain parameters, $\\hat{x_0}$ and $\\hat{y_0}$.\n    From $c_1 = 2\\alpha x_0$, we have:\n    $$\n    \\hat{x_0} = \\frac{\\hat{c_1}}{2\\alpha}\n    $$\n    From $c_0 = y_0 - \\alpha x_0^2$, we have:\n    $$\n    \\hat{y_0} = \\hat{c_0} + \\alpha \\hat{x_0}^2\n    $$\n\n5.  **Final Parameter Estimates:**\n    Finally, we obtain the estimates for $S_0$ and $f_0$ by applying the exponential function:\n    $$\n    \\hat{f_0} = \\exp(\\hat{x_0})\n    $$\n    $$\n    \\hat{S_0} = \\exp(\\hat{y_0})\n    $$\n    The constraints $S_0 > 0$ and $f_0 > 0$ are automatically satisfied by this procedure since the exponential function's range is all positive real numbers.\n\nThis step-by-step procedure provides estimates for $S_0$ and $f_0$ derived from the first principle of least-squares minimization.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve_csf_params(alpha, f_data, s_data):\n    \"\"\"\n    Estimates the parameters S0 and f0 of the CSF model using linear least squares\n    on a transformed version of the model.\n\n    Args:\n        alpha (float): The known curvature parameter.\n        f_data (np.ndarray): Array of spatial frequencies in cycles per degree.\n        s_data (np.ndarray): Array of observed sensitivities.\n\n    Returns:\n        tuple[float, float]: A tuple containing the estimated (S0_hat, f0_hat).\n    \"\"\"\n    # Step 1: Log-transform the data.\n    # The model is S(f) = S0 * exp(-alpha * ln(f/f0)^2).\n    # In the log-log domain, this is ln(S) = ln(S0) - alpha * (ln(f) - ln(f0))^2.\n    # Let x = ln(f) and y = ln(S). The model becomes y = y0 - alpha * (x - x0)^2.\n    x = np.log(f_data)\n    y = np.log(s_data)\n    n = len(f_data)\n\n    # Step 2: Linearize the model.\n    # y = y0 - alpha * (x^2 - 2*x*x0 + x0^2)\n    # y + alpha*x^2 = (y0 - alpha*x0^2) + (2*alpha*x0)*x\n    # This is a linear model z = c0 + c1*x, where:\n    # z = y + alpha*x^2\n    # c1 = 2*alpha*x0\n    # c0 = y0 - alpha*x0^2\n    z = y + alpha * x**2\n\n    # Step 3: Perform linear regression of z on x to find c0 and c1.\n    # We solve the normal equations for simple linear regression.\n    sum_x = np.sum(x)\n    sum_z = np.sum(z)\n    sum_x_sq = np.sum(x**2)\n    sum_xz = np.sum(x * z)\n\n    # Denominator of the ordinary least squares (OLS) estimators\n    denom = n * sum_x_sq - sum_x**2\n\n    # Estimate c1 and c0\n    c1_hat = (n * sum_xz - sum_x * sum_z) / denom\n    c0_hat = np.mean(z) - c1_hat * np.mean(x)\n\n    # Step 4: Back-transform to get estimates for x0 and y0 (log-domain parameters).\n    # x0 = ln(f0), y0 = ln(S0)\n    x0_hat = c1_hat / (2 * alpha)\n    y0_hat = c0_hat + alpha * x0_hat**2\n\n    # Step 5: Transform back to the original parameter space (S0, f0).\n    s0_hat = np.exp(y0_hat)\n    f0_hat = np.exp(x0_hat)\n\n    return s0_hat, f0_hat\n\ndef solve():\n    \"\"\"\n    Main function to run the estimation on all test cases and print the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (0.4, np.array([0.5, 1, 2, 4, 8, 16]), np.array([44.33, 115.89, 206.35, 250.00, 206.35, 115.89])),\n        (0.5, np.array([0.5, 1, 2, 3, 3.5, 5, 7, 10, 16]), np.array([34.77, 95.41, 191.91, 210.88, 220.00, 214.71, 166.13, 114.18, 58.87])),\n        (0.6, np.array([0.75, 2, 6]), np.array([84.23, 147.00, 74.15])),\n        (0.15, np.array([2, 4, 8, 10, 12, 16, 24]), np.array([131.59, 181.63, 202.48, 200.00, 195.03, 185.75, 165.78]))\n    ]\n    \n    formatted_results = []\n    for alpha, f_data, s_data in test_cases:\n        # Calculate the estimates for the current test case\n        s0_hat, f0_hat = solve_csf_params(alpha, f_data, s_data)\n        \n        # Round the estimates to the specified decimal places\n        s0_rounded = round(s0_hat, 2)\n        f0_rounded = round(f0_hat, 3)\n        \n        # Format the result for this case as a string '[S0,f0]' ensuring trailing zeros\n        result_str = f\"[{s0_rounded:.2f},{f0_rounded:.3f}]\"\n        formatted_results.append(result_str)\n\n    # Final print statement in the exact required format: [[S_hat1, f_hat1],[S_hat2, f_hat2],...]\n    final_output = f\"[{','.join(formatted_results)}]\"\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}