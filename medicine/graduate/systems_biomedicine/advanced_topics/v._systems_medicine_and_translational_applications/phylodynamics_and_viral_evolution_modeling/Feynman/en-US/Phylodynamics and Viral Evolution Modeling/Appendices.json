{
    "hands_on_practices": [
        {
            "introduction": "Before undertaking complex demographic inference, a critical first step is to confirm that your sequence data actually contains a measurable temporal signal. This practice guides you through implementing a Date Randomization Test (DRT), a robust statistical method to assess the correlation between sampling time and root-to-tip genetic distance . By comparing the observed correlation against a null distribution generated by permuting sample dates, you will develop a core skill for validating time-structured phylogenetic data.",
            "id": "4374506",
            "problem": "You are tasked with implementing a Date Randomization Test (DRT) for assessing temporal signal in a molecular clock analysis within phylodynamics and viral evolution modeling. Consider a dataset consisting of $n = 100$ viral sequences sampled over a span of $3$ years. For each sequence $i$, you are provided two quantities: a sampling time $t_i$ measured in years relative to an arbitrary origin, and a root-to-tip genetic distance $d_i$ measured in substitutions per site from a time-calibrated phylogeny. Under the strict molecular clock assumption, the expected root-to-tip genetic distance is linear in sampling time. Formally, a well-tested model states that\n$$\n\\mathbb{E}[d_i \\mid t_i] = r \\cdot (t_i - t_0),\n$$\nwhere $r$ is the substitution rate in substitutions per site per year, and $t_0$ is the time of the root in the same units.\n\nThe Date Randomization Test (DRT) assesses whether there is a temporal signal by comparing the observed estimate of $r$ to a null distribution formed by randomly permuting sampling times among sequences to remove any association between sampling time and genetic divergence. The following definitions and procedures apply:\n\n1. The ordinary least squares estimate of the clock rate $r$ from data $\\{(t_i, d_i)\\}_{i=1}^n$ is the slope from regressing $d_i$ on $t_i$, given by\n$$\n\\hat{r} = \\frac{\\sum_{i=1}^{n} (t_i - \\bar{t})(d_i - \\bar{d})}{\\sum_{i=1}^{n} (t_i - \\bar{t})^2},\n$$\nwhere $\\bar{t}$ and $\\bar{d}$ are the sample means of $t_i$ and $d_i$ respectively.\n2. Under the null hypothesis of no temporal signal, the sampling times and genetic distances are independent, and permuting $\\{t_i\\}$ among sequences generates the null distribution of the estimator $\\hat{r}$. The empirical null distribution is obtained by computing $\\hat{r}^{(b)}$ for $b = 1, \\dots, B$ independent random permutations of the times.\n3. The one-sided p-value for detecting a positive temporal signal is computed using the permutation distribution as\n$$\np = \\frac{1 + \\sum_{b=1}^{B} \\mathbb{I}\\left(\\hat{r}^{(b)} \\ge \\hat{r}\\right)}{B + 1},\n$$\nwhere $\\mathbb{I}(\\cdot)$ is the indicator function. The detection decision is then made by comparing $p$ to a significance threshold $\\alpha$; detection occurs if $p  \\alpha$.\n\nYour program must implement the above test and produce results for the following test suite, where synthetic data are generated as follows. For each test case, let $t_i$ be independently drawn from a uniform distribution over the interval $[0, 3]$ years, with $n = 100$ sequences. Let $t_0 = -10$ years, and generate root-to-tip distances according to\n$$\nd_i = r_{\\text{true}} \\cdot (t_i - t_0) + \\epsilon_i,\n$$\nwith $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$ independently across $i$. To ensure scientific realism, root-to-tip distances must be nonnegative; after generation, any $d_i$ that is negative must be replaced by a small positive constant $10^{-6}$ substitutions per site. The units for the estimated rate must be substitutions per site per year, and all outputs should be expressed as floating-point numbers without percentage signs. Use a fixed pseudo-random number generator seed $12345$ for reproducibility.\n\nTest suite parameter sets:\n- Case A (happy path, strong temporal signal): $r_{\\text{true}} = 1.0 \\times 10^{-3}$, $\\sigma = 5.0 \\times 10^{-4}$, $B = 1000$, $\\alpha = 0.05$.\n- Case B (weak signal): $r_{\\text{true}} = 2.0 \\times 10^{-4}$, $\\sigma = 1.0 \\times 10^{-3}$, $B = 1000$, $\\alpha = 0.05$.\n- Case C (null, no signal): $r_{\\text{true}} = 0$, $\\sigma = 1.0 \\times 10^{-3}$, $B = 1000$, $\\alpha = 0.05$.\n- Case D (edge case, high noise overshadowing signal): $r_{\\text{true}} = 1.0 \\times 10^{-3}$, $\\sigma = 5.0 \\times 10^{-3}$, $B = 500$, $\\alpha = 0.05$.\n\nFor each case, compute:\n- The estimated clock rate $\\hat{r}$ in substitutions per site per year as defined above.\n- The one-sided permutation p-value $p$.\n- The detection decision as a boolean indicating whether $p  \\alpha$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to a test case and is itself a list of the form $[\\hat{r}, p, \\text{detected}]$. For example, the format should be $\\big[\\,[\\hat{r}_A, p_A, \\text{detected}_A], [\\hat{r}_B, p_B, \\text{detected}_B], [\\hat{r}_C, p_C, \\text{detected}_C], [\\hat{r}_D, p_D, \\text{detected}_D]\\,\\big]$, expressed using standard Python list syntax.",
            "solution": "This solution details the implementation of a Date Randomization Test (DRT) to assess the temporal signal in simulated viral sequence data, as specified in the problem statement. The methodology is grounded in fundamental statistical principles used in phylodynamics.\n\n### 1. Underlying Principle: The Strict Molecular Clock and Temporal Signal\n\nThe core assumption is the strict molecular clock model, which posits a linear relationship between the genetic divergence of a sequence from a common ancestor and the time elapsed. For a set of sequences $\\{i\\}_{i=1}^n$ with sampling times $\\{t_i\\}$ and root-to-tip genetic distances $\\{d_i\\}$, the expected distance is given by:\n$$\n\\mathbb{E}[d_i \\mid t_i] = r \\cdot (t_i - t_0)\n$$\nHere, $r$ is the constant substitution rate (the \"clock rate\"), and $t_0$ is the time of the most recent common ancestor (the root of the phylogeny). A \"temporal signal\" exists if there is a statistically significant positive correlation between $d_i$ and $t_i$, which implies that the clock rate $r$ is measurably greater than zero.\n\n### 2. The Date Randomization Test (DRT)\n\nThe DRT is a permutation-based approach to test the null hypothesis $H_0$ that there is no temporal signal ($r = 0$). This is equivalent to stating that the genetic distances $d_i$ and sampling times $t_i$ are independent. The test proceeds as follows:\n\n- **Test Statistic**: The strength of the temporal signal is quantified by the estimated clock rate, $\\hat{r}$. This is calculated as the slope of an ordinary least squares (OLS) linear regression of $d_i$ on $t_i$. The formula for this estimator is:\n$$\n\\hat{r} = \\frac{\\sum_{i=1}^{n} (t_i - \\bar{t})(d_i - \\bar{d})}{\\sum_{i=1}^{n} (t_i - \\bar{t})^2}\n$$\nwhere $\\bar{t}$ and $\\bar{d}$ are the sample means of the times and distances, respectively. A large positive value of $\\hat{r}$ suggests a strong temporal signal.\n\n- **Null Distribution**: To determine if the observed $\\hat{r}$ is statistically significant, it is compared to a null distribution. This distribution is generated by breaking the real association between times and distances. For a large number of permutations, $b = 1, \\dots, B$:\n    1. The set of sampling times $\\{t_i\\}$ is randomly shuffled to create a permuted set $\\{t_i^{(b)}\\}$.\n    2. The genetic distances $\\{d_i\\}$ are kept in their original order.\n    3. A new rate, $\\hat{r}^{(b)}$, is calculated using the permuted times $\\{t_i^{(b)}\\}$ and original distances $\\{d_i\\}$.\nThe collection of these rates, $\\{\\hat{r}^{(b)}\\}$, forms the empirical null distribution of the rate estimator under $H_0$.\n\n- **P-value and Decision**: The one-sided p-value, which is the probability of observing a rate at least as large as the actually observed $\\hat{r}$ under the null hypothesis, is calculated as:\n$$\np = \\frac{1 + \\sum_{b=1}^{B} \\mathbb{I}\\left(\\hat{r}^{(b)} \\ge \\hat{r}\\right)}{B + 1}\n$$\nwhere $\\mathbb{I}(\\cdot)$ is the indicator function. The inclusion of $1$ in the numerator and denominator accounts for the observed statistic itself and prevents a p-value of $0$. A temporal signal is \"detected\" if this p-value is less than a pre-defined significance level $\\alpha$, i.e., $p  \\alpha$.\n\n### 3. Implementation and Algorithmic Design\n\nThe solution is implemented in Python using the `numpy` library. A single pseudo-random number generator, seeded with $12345$, is used for all stochastic procedures to ensure full reproducibility.\n\n**Step 1: Data Simulation**\nFor each of the four test cases, a synthetic dataset of $n=100$ sequences is generated.\n- A `numpy.random.Generator` instance, initialized with the seed $12345$, is used.\n- Sampling times $t_i$ are drawn from a uniform distribution over the interval $[0, 3]$.\n- Gaussian noise terms $\\epsilon_i$ are drawn from $\\mathcal{N}(0, \\sigma^2)$, with $\\sigma$ specific to the test case.\n- Genetic distances are computed as $d_i = r_{\\text{true}} \\cdot (t_i - t_0) + \\epsilon_i$, where $r_{\\text{true}}$ is the true clock rate for the case and $t_0 = -10$.\n- To enforce physical realism, any resulting negative distance $d_i$ is floor-clipped to a small positive value, $10^{-6}$.\n\n**Step 2: Estimation and Permutation**\nA single function processes each test case.\n- It first calculates the observed rate $\\hat{r}$ from the simulated $(t_i, d_i)$ pairs using the OLS formula.\n- It then enters a loop for $B$ iterations (where $B$ is $1000$ or $500$ depending on the case). In each iteration, it calls the generator's `permutation` method on the time vector $t$ and re-computes the rate to build the null distribution $\\{\\hat{r}^{(b)}\\}$.\n\n**Step 3: Final Calculation**\n- After the permutation loop, the p-value $p$ is calculated by counting the number of null rates greater than or equal to the observed rate $\\hat{r}$ and applying the specified formula.\n- The boolean detection decision is made by comparing $p$ to the given $\\alpha$ of $0.05$.\n- The results for each case—$[\\hat{r}, p, \\text{detected}]$—are collected.\n\n**Step 4: Output Formatting**\nThe final list of results is converted to the required string representation, which is a standard Python list-of-lists format, and printed to standard output. This structured approach ensures that the complex statistical procedure is translated into a correct and verifiable computational algorithm.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the Date Randomization Test (DRT) for assessing temporal signal\n    in simulated viral evolution data, as per the problem specification.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    # Format: (r_true, sigma, B, alpha)\n    test_cases = [\n        (1.0e-3, 5.0e-4, 1000, 0.05),  # Case A: Strong signal\n        (2.0e-4, 1.0e-3, 1000, 0.05),  # Case B: Weak signal\n        (0.0, 1.0e-3, 1000, 0.05),     # Case C: Null, no signal\n        (1.0e-3, 5.0e-3, 500, 0.05),    # Case D: High noise\n    ]\n\n    # Initialize a single pseudo-random number generator for reproducibility.\n    # All random aspects of the simulation will use this generator instance.\n    rng = np.random.default_rng(12345)\n\n    # General parameters\n    n = 100          # Number of sequences\n    t0 = -10.0       # Time of the root in years\n    t_span_max = 3.0 # Sampling time upper bound in years\n    d_min = 1e-6     # Minimum allowed genetic distance\n\n    results = []\n    \n    def calculate_rate(times, distances):\n        \"\"\"\n        Calculates the OLS slope (rate) of distances regressed on times.\n        \"\"\"\n        t_mean = np.mean(times)\n        d_mean = np.mean(distances)\n        \n        # Denominator of OLS slope formula. Handle case of zero variance in times.\n        # This is extremely unlikely with float uniform random numbers but is good practice.\n        t_var_sum = np.sum((times - t_mean)**2)\n        if t_var_sum == 0:\n            return 0.0\n        \n        # Numerator of OLS slope formula\n        td_cov_sum = np.sum((times - t_mean) * (distances - d_mean))\n        \n        return td_cov_sum / t_var_sum\n\n    for r_true, sigma, B, alpha in test_cases:\n        # Step 1: Generate synthetic data for the current test case\n        # Sampling times t_i are drawn from a uniform distribution.\n        t = rng.uniform(low=0.0, high=t_span_max, size=n)\n\n        # Error terms epsilon_i are drawn from a normal distribution.\n        epsilon = rng.normal(loc=0.0, scale=sigma, size=n)\n\n        # Generate root-to-tip distances d_i based on the strict clock model.\n        d = r_true * (t - t0) + epsilon\n\n        # Enforce non-negativity constraint for genetic distances.\n        d[d  0] = d_min\n        \n        # Step 2: Calculate the observed clock rate\n        r_hat = calculate_rate(t, d)\n        \n        # Step 3: Perform the permutation test\n        null_rates = np.empty(B)\n        for i in range(B):\n            # Permute the sampling times randomly.\n            t_permuted = rng.permutation(t)\n            # Calculate the rate for the permuted data.\n            # This builds the null distribution.\n            null_rates[i] = calculate_rate(t_permuted, d)\n            \n        # Step 4: Compute the one-sided p-value and make a decision\n        # Count how many null rates are greater than or equal to the observed rate.\n        count_ge = np.sum(null_rates >= r_hat)\n        \n        # Calculate p-value. The +1s account for the observed statistic.\n        p_value = (1.0 + count_ge) / (B + 1.0)\n        \n        # Decision: A temporal signal is detected if p  alpha.\n        detected = p_value  alpha\n        \n        results.append([r_hat, p_value, detected])\n\n    # Final print statement in the exact required format.\n    # The string representation of a list of lists is desired.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "With temporal signal confirmed, we can begin to model the demographic history that shaped the viral phylogeny. This exercise takes you to the heart of phylodynamic inference: the coalescent likelihood. You will derive the log-likelihood function for a time-varying population under a simple exponential growth model, starting from the foundational principles of coalescent hazard rates . Implementing this calculation provides a concrete understanding of how coalescent event times in a tree are translated into evidence for a specific demographic trajectory.",
            "id": "4374502",
            "problem": "You are given an isochronous, dated phylogeny with all sampling at present time and with known coalescent event times. Consider the Kingman coalescent with time-varying effective population size under an exponential growth model. Let $t \\ge 0$ denote time measured backward from the present in years. Assume an effective population size trajectory $N_{e}(t) = N_{e0} \\exp(- r t)$ with growth rate $r$ in units of $\\text{year}^{-1}$ and present-time effective population size $N_{e0}$ in individuals. The phylogeny has $n$ tips sampled at time $t = 0$, and the $n-1$ internal nodes (coalescent events) occur at strictly increasing times $0  t_1  t_2  \\dots  t_{n-1}$ in years before present. At backward time $t \\in [t_{i-1}, t_{i})$ (with $t_0 = 0$), there are $k_i = n - (i-1)$ lineages.\n\nTask: Starting only from the two foundational facts\n- when there are $k$ lineages under the Kingman coalescent with time-varying effective population size $N_{e}(t)$, the instantaneous total coalescent hazard is the binomial coefficient $\\binom{k}{2}$ divided by $N_{e}(t)$, and\n- the density of event times for an inhomogeneous Poisson process with hazard $h(t)$ over an interval combines the survival factor $\\exp\\!\\left(- \\int h(u)\\,du \\right)$ and the hazard evaluated at the event time,\nderive the coalescent log-likelihood for the given tree under the model $N_{e}(t) = N_{e0} \\exp(- r t)$, expressed as a function of $r$ and $N_{e0}$. Your derivation should make clear what tree-derived quantities are required and how they combine, and it should explicitly handle the boundary case $r = 0$ as the limiting case of the exponential model.\n\nThen, implement a program that computes the natural log-likelihood (a dimensionless real number) for several specified test cases. Each test case provides: the number of tips $n$, the strictly increasing list of coalescent times $[t_1, \\dots, t_{n-1}]$ in years before present, the growth rate $r$ in $\\text{year}^{-1}$, and $N_{e0}$ in individuals. Your program must:\n- compute the log-likelihood using only the principles stated above and your derived expression,\n- treat $r = 0$ using the explicit limit of your formula,\n- assume all times are in years, $r$ is in $\\text{year}^{-1}$, and $N_{e0}$ is in individuals,\n- return a single list containing the log-likelihood for each test case as a floating-point number.\n\nAlso, indicate in your derivation which statistics of the tree are sufficient for evaluating the likelihood under this model.\n\nTest suite to implement inside your program (no input is read):\n- Case A (general): $n = 5$, times $[0.2, 0.5, 1.1, 1.8]$, $r = 0.5$, $N_{e0} = 8000$.\n- Case B (boundary $r=0$): $n = 5$, times $[0.2, 0.5, 1.1, 1.8]$, $r = 0.0$, $N_{e0} = 8000$.\n- Case C (few lineages, moderate growth): $n = 3$, times $[0.1, 2.0]$, $r = 0.2$, $N_{e0} = 1000$.\n- Case D (negative growth parameter, i.e., growth forward in time): $n = 4$, times $[0.05, 0.07, 0.09]$, $r = -0.1$, $N_{e0} = 2000$.\n- Case E (large positive $r$): $n = 4$, times $[0.3, 0.6, 1.2]$, $r = 1.5$, $N_{e0} = 500$.\n\nRequired final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[resultA,resultB,resultC,resultD,resultE]\"). Each result must be the natural log-likelihood value for the corresponding case in the same order as listed above. No units should be printed, and no additional text should be output.",
            "solution": "## Problem Validation\n\n### Step 1: Extract Givens\n- **Model:** Kingman coalescent with time-varying effective population size, $N_{e}(t)$.\n- **Population Size Trajectory:** $N_{e}(t) = N_{e0} \\exp(-r t)$, where $t \\ge 0$ is time measured backward from the present in years. $N_{e0}$ is the present-time effective population size, and $r$ is the exponential growth rate in $\\text{year}^{-1}$.\n- **Phylogeny Data:** An isochronous, dated phylogeny with $n$ tips sampled at $t=0$.\n- **Coalescent Times:** A set of $n-1$ strictly increasing event times: $0  t_1  t_2  \\dots  t_{n-1}$ years before present.\n- **Lineage Count:** In any backward time interval $[t_{i-1}, t_i)$ (with $t_0 = 0$), there are $k_i = n - (i-1)$ lineages.\n- **Foundational Fact 1:** For $k$ lineages, the instantaneous total coalescent hazard is $\\lambda_k(t) = \\binom{k}{2} / N_e(t)$.\n- **Foundational Fact 2:** The probability density function for an event time from an inhomogeneous Poisson process with hazard $h(t)$ is the product of the hazard at the event time and the survival factor: $p(t) = h(t) \\exp\\left(-\\int_{t_{start}}^{t} h(u)\\,du \\right)$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is based on the Kingman coalescent model, a fundamental framework in population genetics and phylodynamics. The exponential growth model for effective population size is a standard and well-characterized model. The relationship between hazard rates and likelihood for inhomogeneous Poisson processes is a core concept in probability theory. The problem is scientifically and mathematically sound.\n- **Well-Posed:** The problem provides all necessary definitions, constants, and data to derive the log-likelihood function. It requests a specific output (the log-likelihood value) for defined inputs, making the objective clear and computationally solvable.\n- **Objective:** The problem is stated in precise mathematical and scientific language, free of subjectivity or ambiguity.\n- **Completeness and Consistency:** The givens are self-contained and consistent. The definitions of $N_e(t)$, coalescent hazard, and likelihood density are standard and mutually compatible.\n- **All other criteria are met:** The problem is formalizable, realistic, well-structured, and verifiable.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete, reasoned solution will be provided.\n\n## Derivation of the Coalescent Log-Likelihood\n\nThe likelihood of observing the given set of coalescent times $\\{t_1, \\dots, t_{n-1}\\}$ is the product of the probabilities of two types of events for each interval $[t_{i-1}, t_i)$: (1) no coalescent event occurs in the open interval $(t_{i-1}, t_i)$, and (2) a coalescent event occurs at time $t_i$. This is governed by the theory of inhomogeneous Poisson processes.\n\n**1. Define the Hazard Function**\nThe time is measured backwards from the present ($t=0$). In the interval $[t_{i-1}, t_i)$, there are $k_i = n-(i-1)$ lineages. According to the problem statement, the instantaneous coalescent hazard is:\n$$ \\lambda_{k_i}(t) = \\frac{\\binom{k_i}{2}}{N_e(t)} $$\nSubstituting the model for effective population size, $N_e(t) = N_{e0} \\exp(-rt)$, we get:\n$$ \\lambda_{k_i}(t) = \\frac{\\binom{k_i}{2}}{N_{e0} \\exp(-rt)} = \\frac{\\binom{k_i}{2}}{N_{e0}} e^{rt} $$\n\n**2. Formulate the Likelihood Contribution for Each Interval**\nUsing the provided rule for the density of event times for an inhomogeneous Poisson process, the likelihood contribution for observing the $i$-th coalescent event at time $t_i$, conditional on no event happening in $[t_{i-1}, t_i)$, is:\n$$ L_i = \\lambda_{k_i}(t_i) \\exp\\left( - \\int_{t_{i-1}}^{t_i} \\lambda_{k_i}(u) \\, du \\right) $$\n\n**3. Formulate the Total Log-Likelihood**\nThe total likelihood for the entire tree is the product of the likelihood contributions from each coalescent event, as these events are conditionally independent:\n$$ L = \\prod_{i=1}^{n-1} L_i $$\nThe total log-likelihood, $\\mathcal{L} = \\ln(L)$, is the sum of the individual log-likelihoods:\n$$ \\mathcal{L} = \\sum_{i=1}^{n-1} \\ln(L_i) = \\sum_{i=1}^{n-1} \\left[ \\ln(\\lambda_{k_i}(t_i)) - \\int_{t_{i-1}}^{t_i} \\lambda_{k_i}(u) \\, du \\right] $$\nThis separates the log-likelihood into two components: a sum of log-hazards evaluated at the event times, and a sum of integrated hazards over the waiting intervals.\n\n**4. Case 1: General Case ($r \\neq 0$)**\nWe evaluate the two components of the log-likelihood.\n\n*   **Log-Hazard Component:**\n    $$ \\sum_{i=1}^{n-1} \\ln(\\lambda_{k_i}(t_i)) = \\sum_{i=1}^{n-1} \\ln\\left( \\frac{\\binom{k_i}{2}}{N_{e0}} e^{rt_i} \\right) = \\sum_{i=1}^{n-1} \\left[ \\ln\\binom{k_i}{2} - \\ln(N_{e0}) + rt_i \\right] $$\n    $$ = \\left( \\sum_{i=1}^{n-1} \\ln\\binom{k_i}{2} \\right) - (n-1)\\ln(N_{e0}) + r \\sum_{i=1}^{n-1} t_i $$\n\n*   **Integrated Hazard Component:**\n    $$ \\int_{t_{i-1}}^{t_i} \\lambda_{k_i}(u) \\, du = \\int_{t_{i-1}}^{t_i} \\frac{\\binom{k_i}{2}}{N_{e0}} e^{ru} \\, du = \\frac{\\binom{k_i}{2}}{N_{e0}} \\left[ \\frac{e^{ru}}{r} \\right]_{t_{i-1}}^{t_i} = \\frac{\\binom{k_i}{2}}{rN_{e0}} (e^{rt_i} - e^{rt_{i-1}}) $$\n    The total integrated hazard is the sum over all intervals:\n    $$ \\sum_{i=1}^{n-1} \\int_{t_{i-1}}^{t_i} \\lambda_{k_i}(u) \\, du = \\frac{1}{rN_{e0}} \\sum_{i=1}^{n-1} \\binom{k_i}{2} (e^{rt_i} - e^{rt_{i-1}}) $$\n\n*   **Full Log-Likelihood for $r \\neq 0$:**\n    Combining the components gives the final expression:\n    $$ \\mathcal{L}(r, N_{e0}) = \\left(\\sum_{i=1}^{n-1} \\ln\\binom{k_i}{2}\\right) - (n-1)\\ln(N_{e0}) + r\\sum_{i=1}^{n-1} t_i - \\frac{1}{rN_{e0}} \\sum_{i=1}^{n-1} \\binom{k_i}{2} (e^{rt_i} - e^{rt_{i-1}}) $$\n\n**5. Case 2: Boundary Case ($r = 0$)**\nWhen $r=0$, the population size is constant: $N_e(t) = N_{e0}$. The hazard is also constant within each interval with $k_i$ lineages: $\\lambda_{k_i}(t) = \\binom{k_i}{2} / N_{e0}$.\n\n*   **Log-Hazard Component:**\n    $$ \\sum_{i=1}^{n-1} \\ln(\\lambda_{k_i}(t_i)) = \\sum_{i=1}^{n-1} \\ln\\left( \\frac{\\binom{k_i}{2}}{N_{e0}} \\right) = \\left(\\sum_{i=1}^{n-1} \\ln\\binom{k_i}{2}\\right) - (n-1)\\ln(N_{e0}) $$\n\n*   **Integrated Hazard Component:**\n    $$ \\int_{t_{i-1}}^{t_i} \\lambda_{k_i}(u) \\, du = \\int_{t_{i-1}}^{t_i} \\frac{\\binom{k_i}{2}}{N_{e0}} \\, du = \\frac{\\binom{k_i}{2}}{N_{e0}} (t_i - t_{i-1}) $$\n    The total integrated hazard is:\n    $$ \\sum_{i=1}^{n-1} \\frac{\\binom{k_i}{2}}{N_{e0}} (t_i - t_{i-1}) = \\frac{1}{N_{e0}} \\sum_{i=1}^{n-1} \\binom{k_i}{2} (t_i - t_{i-1}) $$\n\n*   **Full Log-Likelihood for $r = 0$:**\n    $$ \\mathcal{L}(0, N_{e0}) = \\left(\\sum_{i=1}^{n-1} \\ln\\binom{k_i}{2}\\right) - (n-1)\\ln(N_{e0}) - \\frac{1}{N_{e0}} \\sum_{i=1}^{n-1} \\binom{k_i}{2} (t_i - t_{i-1}) $$\n\n**6. Consistency Check: Limit as $r \\to 0$**\nTo ensure consistency, we take the limit of the general formula for $\\mathcal{L}(r, N_{e0})$ as $r \\to 0$. The critical term is the integrated hazard. We use the Taylor expansion $e^x \\approx 1+x$ for small $x$:\n$$ \\lim_{r\\to 0} \\frac{1}{rN_{e0}} \\sum_{i=1}^{n-1} \\binom{k_i}{2} (e^{rt_i} - e^{rt_{i-1}}) $$\n$$ = \\lim_{r\\to 0} \\frac{1}{rN_{e0}} \\sum_{i=1}^{n-1} \\binom{k_i}{2} ((1+rt_i) - (1+rt_{i-1}) + O(r^2)) $$\n$$ = \\lim_{r\\to 0} \\frac{1}{rN_{e0}} \\sum_{i=1}^{n-1} \\binom{k_i}{2} (r(t_i - t_{i-1}) + O(r^2)) $$\n$$ = \\frac{1}{N_{e0}} \\sum_{i=1}^{n-1} \\binom{k_i}{2} (t_i - t_{i-1}) $$\nThe term $r \\sum t_i \\to 0$ as $r \\to 0$. Thus, the limit of the general expression equals the expression derived specifically for $r=0$, confirming the correctness of the formulas.\n\n**7. Sufficient Statistics**\nTo evaluate the log-likelihood, one needs the number of samples $n$ and the full set of coalescent times $\\{t_1, t_2, \\dots, t_{n-1}\\}$. The expressions involve sums that depend on each individual $t_i$ and the corresponding interval lengths $(t_i - t_{i-1})$, which cannot be reduced to a smaller set of summary statistics (like only the sum of times or the total tree height). Therefore, the pair $(n, \\{t_1, \\dots, t_{n-1}\\})$ constitutes the sufficient statistics of the tree for this model.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_log_likelihood(n: int, times: list[float], r: float, Ne0: float) - float:\n    \"\"\"\n    Computes the coalescent log-likelihood for a dated phylogeny under an exponential growth model.\n\n    Args:\n        n: The number of tips in the phylogeny.\n        times: A list of n-1 strictly increasing coalescent event times before present.\n        r: The exponential growth rate (r  0 for population decline backwards in time).\n        Ne0: The effective population size at present (t=0).\n\n    Returns:\n        The natural log-likelihood.\n    \"\"\"\n    if n  2:\n        return 0.0\n\n    num_coalescent_events = n - 1\n    # Prepend t_0 = 0 to the list of coalescent times.\n    all_times = [0.0] + times\n\n    # --- Term 1: Sum of log-hazards at event times ---\n    # This term is of the form:\n    # sum(ln(C(k_i, 2))) - (n-1)*ln(Ne0) + r*sum(t_i)\n    \n    # Calculate sum(ln(C(k_i, 2)))\n    sum_log_binom_k_2 = 0.0\n    for i in range(1, num_coalescent_events + 1):\n        # In the interval [t_{i-1}, t_i), there are k_i = n - (i-1) lineages.\n        k_i = n - (i - 1)\n        binom_k_2 = k_i * (k_i - 1) / 2.0\n        # If binom_k_2 is 0 or less (e.g., k_i  2), its log is -inf.\n        # This only happens if n  2, handled at start.\n        sum_log_binom_k_2 += np.log(binom_k_2)\n\n    # Calculate r * sum(t_i)\n    sum_of_times = sum(times)\n    r_sum_t = r * sum_of_times\n\n    log_hazard_term = sum_log_binom_k_2 - num_coalescent_events * np.log(Ne0) + r_sum_t\n\n    # --- Term 2: Sum of integrated hazards over intervals ---\n    # This term is subtracted from Term 1.\n    \n    # This will hold sum( C(k_i, 2) * (t_i - t_{i-1}) ) for r=0\n    # or sum( C(k_i, 2) * (exp(r*t_i) - exp(r*t_{i-1})) ) for r!=0\n    integrated_hazard_sum_raw = 0.0\n\n    if r == 0.0:\n        # Constant population size case\n        for i in range(1, num_coalescent_events + 1):\n            k_i = n - (i - 1)\n            binom_k_2 = k_i * (k_i - 1) / 2.0\n            t_curr = all_times[i]\n            t_prev = all_times[i-1]\n            interval_length = t_curr - t_prev\n            integrated_hazard_sum_raw += binom_k_2 * interval_length\n        integrated_hazard_term = integrated_hazard_sum_raw / Ne0\n    else:\n        # Exponential growth/decline case\n        for i in range(1, num_coalescent_events + 1):\n            k_i = n - (i - 1)\n            binom_k_2 = k_i * (k_i - 1) / 2.0\n            t_curr = all_times[i]\n            t_prev = all_times[i-1]\n            integrated_hazard_sum_raw += binom_k_2 * (np.exp(r * t_curr) - np.exp(r * t_prev))\n        integrated_hazard_term = integrated_hazard_sum_raw / (r * Ne0)\n    \n    # Final log-likelihood is Term 1 - Term 2\n    log_likelihood = log_hazard_term - integrated_hazard_term\n    \n    return log_likelihood\n\n\ndef solve():\n    \"\"\"\n    Solves for the log-likelihood for the predefined test cases.\n    \"\"\"\n    test_cases = [\n        # Case A (general)\n        {'n': 5, 'times': [0.2, 0.5, 1.1, 1.8], 'r': 0.5, 'Ne0': 8000},\n        # Case B (boundary r=0)\n        {'n': 5, 'times': [0.2, 0.5, 1.1, 1.8], 'r': 0.0, 'Ne0': 8000},\n        # Case C (few lineages, moderate growth)\n        {'n': 3, 'times': [0.1, 2.0], 'r': 0.2, 'Ne0': 1000},\n        # Case D (negative growth parameter)\n        {'n': 4, 'times': [0.05, 0.07, 0.09], 'r': -0.1, 'Ne0': 2000},\n        # Case E (large positive r)\n        {'n': 4, 'times': [0.3, 0.6, 1.2], 'r': 1.5, 'Ne0': 500},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = calculate_log_likelihood(case['n'], case['times'], case['r'], case['Ne0'])\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{res:.8f}' for res in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While simple parametric models are instructive, real-world epidemic trajectories are often more complex. This practice builds on the previous one by extending the likelihood calculation to a more flexible, piecewise-exponential model of effective population size, $N_e(t)$ . By accommodating non-constant growth rates and incorporating heterochronous sampling (sequences collected at different times), you will learn how to handle the complexities of realistic viral surveillance data and implement a method that forms the basis of widely used techniques like the Bayesian Skyline Plot.",
            "id": "4374557",
            "problem": "You are given a dated phylogenetic tree summarized as a sequence of event times and event types, together with a piecewise-exponential effective population size function over time. Consider the standard Kingman coalescent model, viewed as an inhomogeneous Poisson point process in backward time. The number of extant lineages changes only at event times (sampling or coalescent), and is constant between events. The effective population size function is specified as piecewise-exponential segments that cover the full backward-time window from the present. The task is to compute the log-likelihood of the observed coalescent event times under this model.\n\nStart from foundational principles in coalescent theory and point processes. Use the following bases without importing any target formulas:\n- The Kingman coalescent states that when there are $k$ extant lineages at backward time $t$, the instantaneous rate that any pair coalesces is the reciprocal of the effective population size, and the total coalescent hazard is given by the product of the number of unordered pairs and that per-pair rate. This yields a time-varying event intensity grounded in the model’s definition.\n- For an inhomogeneous Poisson process, the likelihood of observing specific event times in a window arises from the general definition of a point process likelihood with a time-varying intensity, combining an accumulation over event points and a survival factor over intervals with no events.\n\nThe piecewise-exponential effective population size is defined by segments indexed by $j$, each with a start time $s_j$, an end time $s_{j+1}$, a base level $N_j$ at time $s_j$, and a constant growth rate $g_j$ within the segment. For $t \\in [s_j, s_{j+1})$, define\n$$\nN_e(t) = N_j \\,\\exp\\!\\big(g_j \\,(t - s_j)\\big).\n$$\nAssume all $N_j$ are strictly positive and all segments together cover the complete time range required by the event list. Time is measured in years and the output log-likelihood must be reported in nats.\n\nEvents are given as an ordered set of times $t_1  t_2  \\dots  t_m$ in backward time from the present ($t=0$), with types either “sample” or “coal”. The initial number of extant lineages at $t=0$ is provided separately. Between events, the number of lineages $k(t)$ remains constant; at a sampling event, $k(t)$ increases by $1$ immediately after the event time; at a coalescent event, $k(t)$ decreases by $1$ immediately after the event time. Only the coalescent events contribute point-mass terms to the likelihood; the sampling events modify $k(t)$ and thereby the intensity between events. The program must:\n- Derive, from the stated principles, the correct expression for the log-likelihood of the observed coalescent events given the piecewise-exponential $N_e(t)$, carefully accounting for the event intensity between events and the point contributions at coalescent times.\n- Implement robust numerical evaluation of the required time integrals over $1/N_e(t)$ on each interval with constant $k(t)$, including intervals that cross segment boundaries and the special case of zero growth rate ($g_j = 0$).\n\nUse the following test suite, with all times in years and all log-likelihood outputs in nats:\n- Test case $1$ (constant effective population size):\n  - Segments: one segment with $s_0 = 0$, $s_1 = 3.0$, $N_0 = 10000$, $g_0 = 0$.\n  - Initial lineages: $k(0) = 4$.\n  - Events: $(0.5,\\text{coal})$, $(1.0,\\text{coal})$, $(2.0,\\text{coal})$.\n- Test case $2$ (crossing a growth-rate boundary):\n  - Segments: two segments with $s_0 = 0$, $s_1 = 1.0$, $s_2 = 3.0$, $N_0 = 8000$, $g_0 = 0.3$, and $N_1 = N_0 \\exp\\!\\big(g_0 (s_1-s_0)\\big)$, $g_1 = -0.2$.\n  - Initial lineages: $k(0) = 5$.\n  - Events: $(0.8,\\text{coal})$, $(1.5,\\text{coal})$, $(2.2,\\text{coal})$, $(2.7,\\text{coal})$.\n- Test case $3$ (event exactly at a segment boundary):\n  - Segments: two segments with $s_0 = 0$, $s_1 = 1.0$, $s_2 = 2.0$, $N_0 = 12000$, $g_0 = 0$, and $N_1 = N_0 \\exp\\!\\big(g_0 (s_1-s_0)\\big)$, $g_1 = 0.5$.\n  - Initial lineages: $k(0) = 3$.\n  - Events: $(1.0,\\text{coal})$, $(1.8,\\text{coal})$.\n- Test case $4$ (heterochronous sampling):\n  - Segments: two segments with $s_0 = 0$, $s_1 = 2.0$, $s_2 = 4.0$, $N_0 = 6000$, $g_0 = -0.1$, and $N_1 = N_0 \\exp\\!\\big(g_0 (s_1-s_0)\\big)$, $g_1 = 0$.\n  - Initial lineages: $k(0) = 2$.\n  - Events: $(0.5,\\text{coal})$, $(0.7,\\text{sample})$, $(0.9,\\text{coal})$, $(1.3,\\text{sample})$, $(2.8,\\text{coal})$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., “[result1,result2,result3,result4]”), where each entry is the computed log-likelihood for the corresponding test case, expressed in nats as a floating-point number. No user input should be read; all parameters must be embedded in the program.",
            "solution": "The user's problem is to compute the log-likelihood of a series of coalescent events under the Kingman coalescent model, where the effective population size $N_e(t)$ is a piecewise-exponential function of backward time $t$. The problem is well-posed, scientifically grounded in established principles of phylodynamics, and provides all necessary data for a unique solution. We proceed with the derivation and implementation.\n\n### Principle-Based Derivation of the Log-Likelihood\n\nThe standard Kingman coalescent can be modeled as an inhomogeneous Poisson point process. The \"events\" of this process are the coalescent events. The time-varying intensity or rate of this process at a backward time $t$, denoted $\\lambda(t)$, is determined by two factors: the number of ways to choose a pair from the currently existing lineages and the rate at which any single pair coalesces.\n\nLet $k(t)$ be the number of extant lineages at time $t$. The number of distinct pairs of lineages is given by the binomial coefficient $\\binom{k(t)}{2}$. The rate of coalescence for any specific pair is $\\frac{1}{N_e(t)}$, where $N_e(t)$ is the effective population size at time $t$. Therefore, the total instantaneous rate of any coalescent event is the product of these two factors:\n$$\n\\lambda(t) = \\binom{k(t)}{2} \\frac{1}{N_e(t)}\n$$\nFor this rate to be positive, we must have $k(t) \\ge 2$. If $k(t)  2$, the rate $\\lambda(t)$ is $0$.\n\nThe likelihood of observing a specific realization of an inhomogeneous Poisson process with rate $\\lambda(t)$ over a time interval $[0, T]$ with events at times $t_1, t_2, \\ldots, t_n$ is given by the general formula for a point process likelihood:\n$$\nL = \\left( \\prod_{i=1}^{n} \\lambda(t_i) \\right) \\exp\\left( - \\int_{0}^{T} \\lambda(\\tau) \\, d\\tau \\right)\n$$\nThis formula combines two components: the product of the intensities at the exact times the events occurred, and a \"survival\" probability term, which is the probability of observing no events in the intervals between the observed events.\n\nIt is more convenient to work with the log-likelihood, $\\ln L$:\n$$\n\\ln L = \\sum_{i=1}^{n} \\ln(\\lambda(t_i)) - \\int_{0}^{T} \\lambda(\\tau) \\, d\\tau\n$$\nIn our specific problem, the events are given as an ordered sequence of times $\\tau_1  \\tau_2  \\dots  \\tau_m$. Some of these are coalescent ('coal') events, and some are sampling ('sample') events. Only coalescent events correspond to the points in the Poisson process and contribute to the $\\ln(\\lambda(t_i))$ terms. All events, however, define the boundaries of intervals over which the number of lineages $k(t)$ is constant.\n\nLet the full sequence of event times be $0 = e_0  e_1  e_2  \\dots  e_p$, where $e_p$ is the time of the last event. The number of lineages $k(t)$ is a piecewise-constant function, changing value only at the event times $e_i$. Let $k_i$ be the number of lineages in the interval $[e_{i-1}, e_i)$. The integral term can be broken down into a sum over these intervals:\n$$\n\\int_{0}^{e_p} \\lambda(\\tau) \\, d\\tau = \\sum_{i=1}^{p} \\int_{e_{i-1}}^{e_i} \\lambda(\\tau) \\, d\\tau = \\sum_{i=1}^{p} \\int_{e_{i-1}}^{e_i} \\binom{k_i}{2} \\frac{1}{N_e(\\tau)} \\, d\\tau = \\sum_{i=1}^{p} \\binom{k_i}{2} \\int_{e_{i-1}}^{e_i} \\frac{1}{N_e(\\tau)} \\, d\\tau\n$$\nThe log-likelihood expression is thus:\n$$\n\\ln L = \\sum_{\\substack{\\text{events } j \\\\ \\text{at time } e_j \\\\ \\text{are 'coal'}}} \\ln\\left( \\binom{k(\\text{before } e_j)}{2} \\frac{1}{N_e(e_j)} \\right) - \\sum_{i=1}^{p} \\binom{k_i}{2} \\int_{e_{i-1}}^{e_i} \\frac{1}{N_e(\\tau)} \\, d\\tau\n$$\nwhere $k(\\text{before } e_j)$ is the number of lineages just before event $j$, which is $k_j$.\n\n### Evaluating the Integral of the Reciprocal Population Size\n\nThe core computational task is to evaluate the integral $\\int \\frac{1}{N_e(t)} \\, dt$ over various time intervals. The population size function $N_e(t)$ is piecewise-exponential, defined on segments $[s_j, s_{j+1})$ by:\n$$\nN_e(t) = N_j \\exp\\big(g_j (t - s_j)\\big)\n$$\nAn integration interval $(t_a, t_b)$ may span multiple such segments. The total integral must be calculated by summing the integrals over the parts of $(t_a, t_b)$ that fall into each segment. For a sub-interval $[t_{start}, t_{end}]$ fully contained within a single segment $j$, we compute:\n$$\n\\int_{t_{start}}^{t_{end}} \\frac{1}{N_j \\exp\\big(g_j (t - s_j)\\big)} \\, dt = \\frac{1}{N_j} \\int_{t_{start}}^{t_{end}} \\exp\\big(-g_j (t - s_j)\\big) \\, dt\n$$\nWe must consider two cases for the growth rate $g_j$:\n\nCase 1: $g_j = 0$. The population size is constant, $N_e(t) = N_j$. The integral is trivial:\n$$\n\\int_{t_{start}}^{t_{end}} \\frac{1}{N_j} \\, dt = \\frac{t_{end} - t_{start}}{N_j}\n$$\nCase 2: $g_j \\neq 0$. The integral is:\n$$\n\\frac{1}{N_j} \\left[ \\frac{\\exp\\big(-g_j (t - s_j)\\big)}{-g_j} \\right]_{t_{start}}^{t_{end}} = \\frac{1}{-g_j N_j} \\left( \\exp\\big(-g_j (t_{end} - s_j)\\big) - \\exp\\big(-g_j (t_{start} - s_j)\\big) \\right)\n$$\n$$\n= \\frac{\\exp\\big(-g_j (t_{start} - s_j)\\big) - \\exp\\big(-g_j (t_{end} - s_j)\\big)}{g_j N_j}\n$$\nFor numerical stability when $g_j$ is very small, we handle the $g_j = 0$ case separately using a threshold.\n\n### Algorithm Summary\n\nThe overall algorithm proceeds as follows:\n1. Initialize the log-likelihood $\\ln L = 0$, the current number of lineages $k$ to its initial value at time $t=0$, and the previous event time $t_{prev} = 0$.\n2. Process the events in chronological order. For each event at time $t_{curr}$:\n   a. Calculate the contribution from the interval $[t_{prev}, t_{curr}]$. If $k \\ge 2$, compute the number of pairs $\\binom{k}{2}$ and the integral $I = \\int_{t_{prev}}^{t_{curr}} \\frac{1}{N_e(\\tau)} d\\tau$. Subtract $\\binom{k}{2} I$ from $\\ln L$.\n   b. If the event at $t_{curr}$ is a coalescent event:\n      i. Calculate the log-intensity term $\\ln\\left(\\binom{k}{2} / N_e(t_{curr})\\right)$ and add it to $\\ln L$.\n      ii. Decrease the number of lineages: $k \\leftarrow k - 1$.\n   c. If the event at $t_{curr}$ is a sampling event:\n      i. Increase the number of lineages: $k \\leftarrow k + 1$.\n   d. Update the previous event time: $t_{prev} \\leftarrow t_{curr}$.\n3. The final value of $\\ln L$ is the desired log-likelihood.\n\nThis procedure correctly accumulates all terms of the log-likelihood formula, carefully handling the piecewise nature of both the number of lineages $k(t)$ and the effective population size $N_e(t)$.",
            "answer": "```python\nimport numpy as np\n\ndef _eval_Ne(t, segments):\n    \"\"\"\n    Evaluates the effective population size N_e(t) at a specific time t.\n    Segments are defined as [start, end, N_base, growth_rate].\n    The function N_e(t) = N_base * exp(g * (t - start)) is defined on [start, end).\n    \"\"\"\n    for s_start, s_end, N_base, g in segments:\n        # The interval is [s_start, s_end). A time t==s_end falls in the next segment.\n        if s_start = t  s_end:\n            return N_base * np.exp(g * (t - s_start))\n    \n    # Handle time t being exactly the end time of the last segment.\n    last_s_start, last_s_end, last_N_base, last_g = segments[-1]\n    # Use a small tolerance for floating point comparison.\n    if abs(t - last_s_end)  1e-9:\n        return last_N_base * np.exp(last_g * (t - last_s_start))\n        \n    raise ValueError(f\"Time {t} is outside the defined demographic segments.\")\n\ndef _integrate_inv_Ne(t_start, t_end, segments, epsilon=1e-9):\n    \"\"\"\n    Computes the definite integral of 1/N_e(t) from t_start to t_end.\n    \"\"\"\n    if t_start = t_end:\n        return 0.0\n\n    total_integral = 0.0\n    current_t = t_start\n\n    for s_start, s_end, N_base, g in segments:\n        # Determine the integration interval within the current segment\n        integration_start = max(current_t, s_start)\n        integration_end = min(t_end, s_end)\n\n        if integration_start = integration_end:\n            continue\n        \n        if abs(g)  epsilon:\n            # Case g is close to 0: N_e(t) is constant N_base\n            delta_t = integration_end - integration_start\n            total_integral += delta_t / N_base\n        else:\n            # Case g != 0: N_e(t) = N_base * exp(g * (t - s_start))\n            term1 = np.exp(-g * (integration_start - s_start))\n            term2 = np.exp(-g * (integration_end - s_start))\n            total_integral += (term1 - term2) / (g * N_base)\n        \n        current_t = integration_end\n        if current_t = t_end:\n            break\n            \n    return total_integral\n\ndef _compute_log_likelihood(initial_k, events, segments):\n    \"\"\"\n    Computes the coalescent log-likelihood for a given history.\n    \"\"\"\n    logL = 0.0\n    k = initial_k\n    t_prev = 0.0\n    \n    # Events are assumed to be sorted by time as per problem statement.\n    sorted_events = sorted(events, key=lambda x: x[0])\n\n    for t_curr, event_type in sorted_events:\n        # Survival part: integral over the interval [t_prev, t_curr]\n        if k = 2:\n            num_pairs = k * (k - 1) / 2.0\n            integral = _integrate_inv_Ne(t_prev, t_curr, segments)\n            logL -= num_pairs * integral\n        \n        # Point mass part for a coalescent event at t_curr\n        if event_type == 'coal':\n            if k  2:\n                # Invalid history: coalescence with fewer than 2 lineages.\n                return -np.inf \n            num_pairs = k * (k - 1) / 2.0\n            Ne_at_event = _eval_Ne(t_curr, segments)\n            logL += np.log(num_pairs) - np.log(Ne_at_event)\n            k -= 1\n        elif event_type == 'sample':\n            k += 1\n        \n        t_prev = t_curr\n        \n    return logL\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and print the results.\n    \"\"\"\n    test_cases = []\n\n    # Test case 1\n    N0_1, g0_1 = 10000.0, 0.0\n    segments1 = [(0.0, 3.0, N0_1, g0_1)]\n    events1 = [(0.5, 'coal'), (1.0, 'coal'), (2.0, 'coal')]\n    k0_1 = 4\n    test_cases.append({'initial_k': k0_1, 'events': events1, 'segments': segments1})\n\n    # Test case 2\n    N0_2, g0_2 = 8000.0, 0.3\n    s1_2 = 1.0\n    N1_2 = N0_2 * np.exp(g0_2 * (s1_2 - 0.0))\n    g1_2 = -0.2\n    segments2 = [(0.0, s1_2, N0_2, g0_2), (s1_2, 3.0, N1_2, g1_2)]\n    events2 = [(0.8, 'coal'), (1.5, 'coal'), (2.2, 'coal'), (2.7, 'coal')]\n    k0_2 = 5\n    test_cases.append({'initial_k': k0_2, 'events': events2, 'segments': segments2})\n    \n    # Test case 3\n    N0_3, g0_3 = 12000.0, 0.0\n    s1_3 = 1.0\n    N1_3 = N0_3 * np.exp(g0_3 * (s1_3 - 0.0))\n    g1_3 = 0.5\n    segments3 = [(0.0, s1_3, N0_3, g0_3), (s1_3, 2.0, N1_3, g1_3)]\n    events3 = [(1.0, 'coal'), (1.8, 'coal')]\n    k0_3 = 3\n    test_cases.append({'initial_k': k0_3, 'events': events3, 'segments': segments3})\n\n    # Test case 4\n    N0_4, g0_4 = 6000.0, -0.1\n    s1_4 = 2.0\n    N1_4 = N0_4 * np.exp(g0_4 * (s1_4 - 0.0))\n    g1_4 = 0.0\n    segments4 = [(0.0, s1_4, N0_4, g0_4), (s1_4, 4.0, N1_4, g1_4)]\n    events4 = [(0.5, 'coal'), (0.7, 'sample'), (0.9, 'coal'), (1.3, 'sample'), (2.8, 'coal')]\n    k0_4 = 2\n    test_cases.append({'initial_k': k0_4, 'events': events4, 'segments': segments4})\n\n    results = []\n    for case in test_cases:\n        result = _compute_log_likelihood(case['initial_k'], case['events'], case['segments'])\n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}