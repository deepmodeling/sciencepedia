{
    "hands_on_practices": [
        {
            "introduction": "The foundational workflow in patient stratification involves translating high-dimensional patient data into a network of similarities, from which distinct patient subgroups can be identified. This first exercise guides you through this core pipeline, starting from a hypothetical gene-expression matrix. You will learn to construct a patient similarity network (PSN) using cosine similarity and then partition it into clusters, evaluating the quality of your stratification using the fundamental concept of modularity .",
            "id": "4368693",
            "problem": "You are given an $m \\times p$ feature matrix $X$ of normalized gene-expression intensities for $m$ patients across $p$ genes, intended to model a bipartite patient-gene network where an edge between patient $i$ and gene $j$ has weight $X_{ij}$. Consider the systems biomedicine task of network-based patient stratification via projection from the bipartite graph to a patient-patient similarity graph and subsequent community assessment. Let $m=p=4$ and\n$$\nX \\;=\\;\n\\begin{pmatrix}\n4 & 4 & 0 & 0 \\\\\n2 & 2 & 0 & 0 \\\\\n0 & 0 & 3 & 3 \\\\\n0 & 0 & 1.5 & 1.5\n\\end{pmatrix}.\n$$\nConstruct the patient-patient projection using cosine similarity on the rows of $X$, that is, let $W_{ij}$ denote the cosine similarity between the patient vectors $x_i$ and $x_j$ in $\\mathbb{R}^p$. Form an undirected weighted adjacency by setting $A_{ij} = W_{ij}$ for $i \\neq j$ and $A_{ii} = 0$. Using only fundamental definitions, compute the weighted degree (strength) $s_i$ of each patient node, propose a simple $2$-cluster assignment of patients justified by the connectivity structure for a similarity threshold of $0.5$, and then evaluate the quality of this assignment using the standard weighted Newman-Girvan (NG) modularity with the configuration model null for undirected graphs. Report the final modularity value $Q$ for your $2$-cluster partition. Express the final $Q$ exactly as a rational number (no rounding) and do not include any units.",
            "solution": "The user has provided a valid problem statement. The task is to perform a network-based patient stratification analysis on a small gene-expression dataset. The steps involve constructing a patient-patient similarity network, partitioning it into clusters, and evaluating the partition quality using modularity.\n\nThe problem states that there are $m=4$ patients and $p=4$ genes. The gene-expression data is given by the matrix $X$:\n$$\nX =\n\\begin{pmatrix}\n4 & 4 & 0 & 0 \\\\\n2 & 2 & 0 & 0 \\\\\n0 & 0 & 3 & 3 \\\\\n0 & 0 & 1.5 & 1.5\n\\end{pmatrix}\n$$\nThe rows of $X$ represent the patient vectors, denoted as $x_i \\in \\mathbb{R}^4$ for $i \\in \\{1, 2, 3, 4\\}$:\n$x_1 = (4, 4, 0, 0)$\n$x_2 = (2, 2, 0, 0)$\n$x_3 = (0, 0, 3, 3)$\n$x_4 = (0, 0, 1.5, 1.5)$\n\nThe first step is to construct the patient-patient similarity matrix $W$ using cosine similarity. The cosine similarity between two vectors $u$ and $v$ is given by $W(u, v) = \\frac{u \\cdot v}{\\|u\\| \\|v\\|}$.\n\nWe first compute the Euclidean norms of the patient vectors:\n$\\|x_1\\| = \\sqrt{4^2 + 4^2 + 0^2 + 0^2} = \\sqrt{16 + 16} = \\sqrt{32} = 4\\sqrt{2}$\n$\\|x_2\\| = \\sqrt{2^2 + 2^2 + 0^2 + 0^2} = \\sqrt{4 + 4} = \\sqrt{8} = 2\\sqrt{2}$\n$\\|x_3\\| = \\sqrt{0^2 + 0^2 + 3^2 + 3^2} = \\sqrt{9 + 9} = \\sqrt{18} = 3\\sqrt{2}$\n$\\|x_4\\| = \\sqrt{0^2 + 0^2 + (1.5)^2 + (1.5)^2} = \\sqrt{\\frac{9}{4} + \\frac{9}{4}} = \\sqrt{\\frac{18}{4}} = \\frac{3\\sqrt{2}}{2}$\n\nNext, we compute the dot products and the corresponding cosine similarities $W_{ij}$:\n$W_{12} = \\frac{x_1 \\cdot x_2}{\\|x_1\\| \\|x_2\\|} = \\frac{4(2) + 4(2) + 0(0) + 0(0)}{(4\\sqrt{2})(2\\sqrt{2})} = \\frac{16}{16} = 1$\n$W_{13} = \\frac{x_1 \\cdot x_3}{\\|x_1\\| \\|x_3\\|} = \\frac{4(0) + 4(0) + 0(3) + 0(3)}{(4\\sqrt{2})(3\\sqrt{2})} = \\frac{0}{24} = 0$\n$W_{14} = \\frac{x_1 \\cdot x_4}{\\|x_1\\| \\|x_4\\|} = \\frac{4(0) + 4(0) + 0(1.5) + 0(1.5)}{(4\\sqrt{2})(3\\sqrt{2}/2)} = \\frac{0}{12} = 0$\n$W_{23} = \\frac{x_2 \\cdot x_3}{\\|x_2\\| \\|x_3\\|} = \\frac{2(0) + 2(0) + 0(3) + 0(3)}{(2\\sqrt{2})(3\\sqrt{2})} = \\frac{0}{12} = 0$\n$W_{24} = \\frac{x_2 \\cdot x_4}{\\|x_2\\| \\|x_4\\|} = \\frac{2(0) + 2(0) + 0(1.5) + 0(1.5)}{(2\\sqrt{2})(3\\sqrt{2}/2)} = \\frac{0}{6} = 0$\n$W_{34} = \\frac{x_3 \\cdot x_4}{\\|x_3\\| \\|x_4\\|} = \\frac{0(0) + 0(0) + 3(1.5) + 3(1.5)}{(3\\sqrt{2})(3\\sqrt{2}/2)} = \\frac{9}{9} = 1$\n\nThe similarity matrix $W$ is symmetric ($W_{ij} = W_{ji}$) with diagonal elements equal to $1$.\n\nThe problem defines the weighted adjacency matrix $A$ of the patient-patient graph by setting $A_{ij} = W_{ij}$ for $i \\neq j$ and $A_{ii} = 0$.\n$$\nA =\n\\begin{pmatrix}\n0 & 1 & 0 & 0 \\\\\n1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n0 & 0 & 1 & 0\n\\end{pmatrix}\n$$\nThis graph consists of two disconnected components: an edge between nodes $1$ and $2$ with weight $1$, and an edge between nodes $3$ and $4$ with weight $1$.\n\nNext, we compute the weighted degree (strength) $s_i$ for each node $i$, defined as $s_i = \\sum_j A_{ij}$:\n$s_1 = A_{12} = 1$\n$s_2 = A_{21} = 1$\n$s_3 = A_{34} = 1$\n$s_4 = A_{43} = 1$\nSo, the strength vector is $(s_1, s_2, s_3, s_4) = (1, 1, 1, 1)$.\n\nThe problem asks for a simple $2$-cluster assignment justified by the connectivity structure for a similarity threshold of $0.5$. The non-zero off-diagonal similarities are $W_{12}=1$ and $W_{34}=1$, both of which are greater than the threshold of $0.5$. All other similarities between different patient pairs are $0$, which is below the threshold. The graph structure clearly shows two separate pairs of connected nodes. This leads to a natural and unambiguous partition into two clusters:\nCluster $1$: $C_1 = \\{1, 2\\}$\nCluster $2$: $C_2 = \\{3, 4\\}$\nThis partition corresponds to the connected components of the graph.\n\nFinally, we evaluate this partition using the Newman-Girvan modularity $Q$. For a weighted, undirected graph, the modularity is given by:\n$$\nQ = \\frac{1}{2w} \\sum_{i,j} \\left( A_{ij} - \\frac{s_i s_j}{2w} \\right) \\delta(c_i, c_j)\n$$\nwhere $c_i$ is the community of node $i$, $\\delta(c_i, c_j)$ is $1$ if $c_i = c_j$ and $0$ otherwise, and $2w$ is the total weight of the graph, which is the sum of all entries in $A$ or, equivalently, the sum of all node strengths.\nTotal weight $2w = \\sum_i s_i = 1 + 1 + 1 + 1 = 4$.\n\nThe summation for $Q$ is over all pairs of nodes $(i, j)$ that are in the same community. We can split the sum into contributions from each community:\n$$\nQ = \\frac{1}{2w} \\left[ \\sum_{i,j \\in C_1} \\left( A_{ij} - \\frac{s_i s_j}{2w} \\right) + \\sum_{i,j \\in C_2} \\left( A_{ij} - \\frac{s_i s_j}{2w} \\right) \\right]\n$$\nFor community $C_1 = \\{1, 2\\}$:\nThe terms are for pairs $(1,1)$, $(1,2)$, $(2,1)$, and $(2,2)$.\n$\\sum_{i,j \\in C_1} \\left( A_{ij} - \\frac{s_i s_j}{4} \\right) = \\left( A_{11} - \\frac{s_1s_1}{4} \\right) + \\left( A_{12} - \\frac{s_1s_2}{4} \\right) + \\left( A_{21} - \\frac{s_2s_1}{4} \\right) + \\left( A_{22} - \\frac{s_2s_2}{4} \\right)$\n$= \\left( 0 - \\frac{1 \\cdot 1}{4} \\right) + \\left( 1 - \\frac{1 \\cdot 1}{4} \\right) + \\left( 1 - \\frac{1 \\cdot 1}{4} \\right) + \\left( 0 - \\frac{1 \\cdot 1}{4} \\right)$\n$= -\\frac{1}{4} + \\frac{3}{4} + \\frac{3}{4} - \\frac{1}{4} = \\frac{4}{4} = 1$\n\nFor community $C_2 = \\{3, 4\\}$:\nThe calculation is identical due to the symmetric structure of the problem. $s_3 = 1$, $s_4 = 1$, and the only non-zero edge weight is $A_{34}=A_{43}=1$.\n$\\sum_{i,j \\in C_2} \\left( A_{ij} - \\frac{s_i s_j}{4} \\right) = \\left( A_{33} - \\frac{s_3s_3}{4} \\right) + \\left( A_{34} - \\frac{s_3s_4}{4} \\right) + \\left( A_{43} - \\frac{s_4s_3}{4} \\right) + \\left( A_{44} - \\frac{s_4s_4}{4} \\right)$\n$= \\left( 0 - \\frac{1 \\cdot 1}{4} \\right) + \\left( 1 - \\frac{1 \\cdot 1}{4} \\right) + \\left( 1 - \\frac{1 \\cdot 1}{4} \\right) + \\left( 0 - \\frac{1 \\cdot 1}{4} \\right)$\n$= -\\frac{1}{4} + \\frac{3}{4} + \\frac{3}{4} - \\frac{1}{4} = 1$\n\nNow, we compute the final modularity value $Q$:\n$$\nQ = \\frac{1}{4} (1 + 1) = \\frac{2}{4} = \\frac{1}{2}\n$$\nThe modularity of the proposed $2$-cluster partition is $\\frac{1}{2}$.",
            "answer": "$$\\boxed{\\frac{1}{2}}$$"
        },
        {
            "introduction": "Patient similarity networks are not just for discovering discrete clusters; they can also model the flow of biological information to refine patient labels. This practice introduces the powerful concept of network diffusion, a form of graph-based semi-supervised learning. By minimizing an energy function that balances fidelity to initial labels with smoothness across the network, you will calculate how information from a single labeled patient propagates to its neighbors, yielding a continuous stratification score for the entire cohort .",
            "id": "4368694",
            "problem": "Consider a cohort of $9$ patients modeled as a $3 \\times 3$ grid graph with nodes indexed in row-major order $1$ through $9$. Two patients are connected by an undirected edge if and only if they are orthogonal neighbors on the grid (up, down, left, right). All edges have weight $w_{ij} = 1$. Let the initial label vector be $y \\in \\mathbb{R}^{9}$ with $y_{5} = 1$ (the center patient) and $y_{i} = 0$ for all $i \\neq 5$. We model diffusion-based refinement for patient stratification by minimizing the smoothness-regularized energy\n$$\nE(f) \\;=\\; \\sum_{i=1}^{9} (f_{i} - y_{i})^{2} \\;+\\; \\mu \\sum_{(i,j) \\in \\mathcal{E}} w_{ij} (f_{i} - f_{j})^{2},\n$$\nwhere $\\mathcal{E}$ is the set of undirected edges and $\\mu > 0$ is a regularization parameter controlling the strength of diffusion. Take $\\mu = 1$.\n\nStarting from the above definition of the energy and standard properties of graphs (degree and adjacency), derive the stationary diffusion scores $f^{\\star} \\in \\mathbb{R}^{9}$ that minimize $E(f)$ on this graph and compute them exactly. Report the final diffusion score vector as a single row in the node order $(1,2,3,4,5,6,7,8,9)$ using exact rational values. Also explain qualitatively, based on your derivation, how information from the center label spreads across edges to refine the stratification in this network. No rounding is required, and no physical units apply; scores are dimensionless.",
            "solution": "The user wants to find the stationary diffusion scores $f^{\\star}$ for a patient stratification model on a $3 \\times 3$ grid graph.\n\n### Problem Validation\nThe problem is first validated against the required criteria.\n\n**Step 1: Extract Givens**\n- Patient cohort size: $9$.\n- Graph model: A $3 \\times 3$ grid graph.\n- Node indexing: Row-major order, nodes $1$ through $9$.\n- Connectivity: Edges between orthogonal neighbors (up, down, left, right).\n- Edge weights: $w_{ij} = 1$ for all edges $(i,j) \\in \\mathcal{E}$.\n- Initial label vector: $y \\in \\mathbb{R}^{9}$, with $y_{5} = 1$ and $y_{i} = 0$ for $i \\neq 5$.\n- Energy function to minimize: $E(f) = \\sum_{i=1}^{9} (f_{i} - y_{i})^{2} + \\mu \\sum_{(i,j) \\in \\mathcal{E}} w_{ij} (f_{i} - f_{j})^{2}$.\n- Regularization parameter: $\\mu = 1$.\n- Task: Find the minimizing scores $f^{\\star} \\in \\mathbb{R}^{9}$ as exact rational values and provide a qualitative explanation.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem describes a standard approach in graph-based semi-supervised learning, specifically label propagation or diffusion processes on graphs. The energy function is a well-known Tikhonov regularization functional applied to a graph setting. This is a core concept in network science and systems biology. The model is mathematically and scientifically sound.\n- **Well-Posed:** The energy function $E(f)$ is a sum of squared terms, making it a quadratic function of the vector $f$. Since the graph Laplacian matrix is positive semi-definite, and we are adding an identity matrix term (from the first sum), the overall quadratic form is strictly convex. A strictly convex function has a unique minimum. Therefore, a unique, stable, and meaningful solution $f^{\\star}$ exists.\n- **Objective:** The problem is stated in precise mathematical language without any subjectivity or ambiguity.\n- All other validation checks (completeness, consistency, feasibility) are passed. The problem provides all necessary information to derive a unique solution.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A full solution will be provided.\n\n### Solution Derivation\nThe energy function to be minimized is:\n$$\nE(f) = \\sum_{i=1}^{9} (f_{i} - y_{i})^{2} + \\mu \\sum_{(i,j) \\in \\mathcal{E}} w_{ij} (f_{i} - f_{j})^{2}\n$$\nThis function can be expressed in matrix form. The first term is the squared Euclidean norm of the difference between the score vector $f$ and the initial label vector $y$, which is $(f-y)^T(f-y)$. The second term is a quadratic form involving the graph Laplacian. The graph Laplacian $L$ is defined as $L = D - A$, where $D$ is the diagonal degree matrix and $A$ is the adjacency matrix. The smoothness term $\\sum_{(i,j) \\in \\mathcal{E}} w_{ij} (f_i - f_j)^2$ is equal to $f^T L f$.\n\nThus, the energy function can be written as:\n$$\nE(f) = (f-y)^T I (f-y) + \\mu f^T L f\n$$\nwhere $I$ is the identity matrix. To find the vector $f^{\\star}$ that minimizes this convex function, we compute the gradient with respect to $f$ and set it to the zero vector.\n$$\n\\nabla_f E(f) = \\frac{\\partial}{\\partial f} \\left( (f-y)^T(f-y) + \\mu f^T L f \\right) = 0\n$$\nUsing standard matrix calculus identities, $\\nabla_x(x^T M x) = (M+M^T)x$ and $\\nabla_x((x-c)^T(x-c)) = 2(x-c)$, we get:\n$$\n\\nabla_f E(f) = 2(f-y) + \\mu(L+L^T)f\n$$\nSince the graph is undirected, its adjacency matrix $A$ is symmetric. The degree matrix $D$ is diagonal and thus symmetric. Therefore, the Laplacian $L=D-A$ is also symmetric ($L=L^T$). The gradient simplifies to:\n$$\n\\nabla_f E(f) = 2(f-y) + 2\\mu L f\n$$\nSetting the gradient to zero to find the minimum:\n$$\n2(f^{\\star}-y) + 2\\mu L f^{\\star} = 0 \\\\\n(f^{\\star}-y) + \\mu L f^{\\star} = 0 \\\\\nf^{\\star} + \\mu L f^{\\star} = y \\\\\n(I + \\mu L) f^{\\star} = y\n$$\nThe stationary diffusion scores $f^{\\star}$ are the solution to this linear system of equations.\n\n### Application to the $3 \\times 3$ Grid\nThe nodes are indexed in row-major order:\n$$\n\\begin{pmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{pmatrix}\n$$\nThe graph has three types of nodes based on their degree:\n- Corner nodes $\\{1, 3, 7, 9\\}$ have degree $d_C=2$.\n- Edge nodes $\\{2, 4, 6, 8\\}$ have degree $d_E=3$.\n- Center node $\\{5\\}$ has degree $d_M=4$.\n\nThe degree matrix $D$ is $\\text{diag}(2, 3, 2, 3, 4, 3, 2, 3, 2)$. The adjacency matrix $A$ captures the orthogonal connections. The Laplacian $L=D-A$. With $\\mu=1$, we need to solve $(I+L)f^{\\star} = y$. The matrix $M = I+L$ is given by $M_{ii} = 1+d_i$ and $M_{ij} = -A_{ij}$ for $i \\neq j$.\n\nThe system of equations $Mf=y$ is:\n\\begin{align*}\n1. & \\quad (1+2)f_1 - f_2 - f_4 = y_1=0 \\\\\n2. & \\quad (1+3)f_2 - f_1 - f_3 - f_5 = y_2=0 \\\\\n3. & \\quad (1+2)f_3 - f_2 - f_6 = y_3=0 \\\\\n4. & \\quad (1+3)f_4 - f_1 - f_5 - f_7 = y_4=0 \\\\\n5. & \\quad (1+4)f_5 - f_2 - f_4 - f_6 - f_8 = y_5=1 \\\\\n6. & \\quad (1+3)f_6 - f_3 - f_5 - f_9 = y_6=0 \\\\\n7. & \\quad (1+2)f_7 - f_4 - f_8 = y_7=0 \\\\\n8. & \\quad (1+3)f_8 - f_5 - f_7 - f_9 = y_8=0 \\\\\n9. & \\quad (1+2)f_9 - f_6 - f_8 = y_9=0\n\\end{align*}\nDue to the symmetry of the grid and the centered initial condition $y$, the solution $f^{\\star}$ must also be symmetric. We can classify the nodes:\n- Corner scores: $f_C = f_1 = f_3 = f_7 = f_9$.\n- Edge scores: $f_E = f_2 = f_4 = f_6 = f_8$.\n- Center score: $f_M = f_5$.\n\nThis symmetry reduces the $9 \\times 9$ system to a $3 \\times 3$ system.\n- From eq. 1: $3f_1 - f_2 - f_4 = 0 \\implies 3f_C - f_E - f_E = 0 \\implies 3f_C = 2f_E$.\n- From eq. 2: $4f_2 - f_1 - f_3 - f_5 = 0 \\implies 4f_E - f_C - f_C - f_M = 0 \\implies 4f_E - 2f_C = f_M$.\n- From eq. 5: $5f_5 - f_2 - f_4 - f_6 - f_8 = 1 \\implies 5f_M - 4f_E = 1$.\n\nWe now solve this reduced system:\n(A) $3f_C = 2f_E$\n(B) $4f_E - 2f_C = f_M$\n(C) $5f_M - 4f_E = 1$\n\nFrom (A), $f_E = \\frac{3}{2}f_C$.\nSubstitute this into (B): $f_M = 4(\\frac{3}{2}f_C) - 2f_C = 6f_C - 2f_C = 4f_C$.\nNow we have both $f_E$ and $f_M$ in terms of $f_C$. Substitute them into (C):\n$$\n5(4f_C) - 4\\left(\\frac{3}{2}f_C\\right) = 1 \\\\\n20f_C - 6f_C = 1 \\\\\n14f_C = 1 \\implies f_C = \\frac{1}{14}\n$$\nNow we can find the other scores:\n- $f_E = \\frac{3}{2}f_C = \\frac{3}{2} \\cdot \\frac{1}{14} = \\frac{3}{28}$.\n- $f_M = 4f_C = 4 \\cdot \\frac{1}{14} = \\frac{4}{14} = \\frac{2}{7}$.\n\nThe final diffusion scores are:\n- $f_1 = f_3 = f_7 = f_9 = f_C = \\frac{1}{14}$.\n- $f_2 = f_4 = f_6 = f_8 = f_E = \\frac{3}{28}$.\n- $f_5 = f_M = \\frac{2}{7}$.\n\nThe vector of scores $f^{\\star}$ in row-major order $(1, 2, ..., 9)$ is:\n$$\nf^{\\star} = \\left( \\frac{1}{14}, \\frac{3}{28}, \\frac{1}{14}, \\frac{3}{28}, \\frac{2}{7}, \\frac{3}{28}, \\frac{1}{14}, \\frac{3}{28}, \\frac{1}{14} \\right)\n$$\n\n### Qualitative Explanation\nThe energy function $E(f)$ provides the basis for understanding the information spread. It consists of two terms:\n1.  A **fidelity term** $\\sum_i (f_i - y_i)^2$: This term penalizes deviations of the final scores $f_i$ from the initial labels $y_i$. It acts as an anchor, pulling the score of each node towards its initial label. In this problem, it strongly pulls $f_5$ towards $1$ and all other scores $f_i$ towards $0$.\n2.  A **smoothness term** $\\mu \\sum_{(i,j) \\in \\mathcal{E}} (f_i - f_j)^2$: This term penalizes differences in scores between connected nodes. It forces the scores of adjacent patients to be similar, effectively causing the scores to be \"smooth\" across the network. This is the mechanism for diffusion.\n\nThe minimization of $E(f)$ finds a balance between these two competing pressures. The parameter $\\mu$ controls this balance. The resulting stationary scores $f^{\\star}$ represent an equilibrium state. Information, in the form of the initial label $y_5=1$, diffuses from the source node (patient $5$) across the network edges. The smoothness term ensures that this \"label information\" is shared with neighbors.\n\n- The center node $5$ starts with a label of $1$, but to maintain smoothness with its neighbors, it \"leaks\" some of its score, resulting in a final score $f_5 = \\frac{2}{7} < 1$.\n- The direct neighbors of node $5$ (nodes $2, 4, 6, 8$) start with labels of $0$ but are pulled up by the high score of their neighbor, node $5$. They receive the most score directly from the source, hence their scores ($f_E = \\frac{3}{28}$) are the second highest.\n- The corner nodes ($1, 3, 7, 9$) are not directly connected to the source node $5$. They only receive score information indirectly, through their neighbors (the edge nodes). Consequently, their scores are the lowest ($f_C = \\frac{1}{14}$), reflecting their greater distance from the initial label source in the network.\n\nIn summary, the model refines the initial binary stratification ($y$) into a continuous, smooth stratification ($f^{\\star}$) where a patient's score reflects their network proximity to the initially labeled patient. This diffusion process effectively stratifies the cohort based on the network structure.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{14} & \\frac{3}{28} & \\frac{1}{14} & \\frac{3}{28} & \\frac{2}{7} & \\frac{3}{28} & \\frac{1}{14} & \\frac{3}{28} & \\frac{1}{14}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "A central challenge in modern systems biomedicine is the integration of multiple 'omics' data types to gain a holistic view of each patient. This advanced exercise demystifies the powerful Similarity Network Fusion (SNF) algorithm, a state-of-the-art method for this task. By manually performing one iteration of SNF, you will see how the algorithm uses a cross-diffusion process to integrate information from two different data modalities, reinforcing shared patterns of patient similarity to build a single, more robust network for stratification .",
            "id": "4368745",
            "problem": "You are studying Similarity Network Fusion (SNF) for patient stratification across two heterogeneous data modalities. You are given two symmetric patient affinity matrices $W^{(1)}$ and $W^{(2)}$ for $n=3$ patients, with zero diagonal, constructed from two independent omics data types. The goal is to perform exactly one SNF diffusion iteration starting from $W^{(1)}$ and $W^{(2)}$, using only fundamental definitions of graph-based normalization and diffusion, and to quantify how the fused inter-patient similarity between patients $1$ and $2$ changes after one iteration.\n\nAssume the following, which are standard and well-tested in graph-based data integration and Markov diffusion on networks:\n\n1. Each layer $m \\in \\{1,2\\}$ yields an affinity matrix $W^{(m)} \\in \\mathbb{R}^{3 \\times 3}$, where $W^{(m)}$ is symmetric, nonnegative, and satisfies $W^{(m)}_{ii}=0$. From $W^{(m)}$, define a row-stochastic, self-regularized transition matrix $P^{(m)}$ by setting $P^{(m)}_{ii}=\\frac{1}{2}$ for all $i$, and for $i \\neq j$,\n$$\nP^{(m)}_{ij} \\;=\\; \\frac{W^{(m)}_{ij}}{2 \\sum_{k \\neq i} W^{(m)}_{ik}}.\n$$\nThis yields a valid Markov transition matrix where each row sums to $1$ and exactly half of the probability mass remains at the node (self-retention), implementing local regularization.\n\n2. For each layer $m$, construct a $K$-nearest neighbor kernel $S^{(m)}$ with $K=1$ using the local neighborhood $\\mathcal{N}^{(m)}_i$ of node $i$ in $W^{(m)}$ (excluding self-loops). Specifically,\n$$\nS^{(m)}_{ij} \\;=\\;\n\\begin{cases}\n\\dfrac{W^{(m)}_{ij}}{\\sum\\limits_{k \\in \\mathcal{N}^{(m)}_i} W^{(m)}_{ik}}, & \\text{if } j \\in \\mathcal{N}^{(m)}_i,\\\\[8pt]\n0, & \\text{otherwise,}\n\\end{cases}\n\\quad\\text{with } S^{(m)}_{ii}=0.\n$$\nWith $K=1$ and no ties, each row of $S^{(m)}$ has exactly one off-diagonal entry equal to $1$ (its nearest neighbor) and all other entries equal to $0$.\n\n3. Perform one SNF diffusion update using the cross-diffusion principle grounded in Markov propagation on graphs. For $M=2$ layers, update each layer by diffusing its $P^{(m)}$ through the other layer’s structure:\n$$\n\\widetilde{P}^{(m)} \\;=\\; S^{(m)} \\, P^{(\\ell)} \\, \\big(S^{(m)}\\big)^{\\top}, \\quad \\text{where } \\ell \\neq m.\n$$\nThen renormalize $\\widetilde{P}^{(m)}$ to $P_{\\text{new}}^{(m)}$ by setting $P_{\\text{new}}^{(m)}{}_{ii}=\\frac{1}{2}$ for all $i$ and rescaling each row’s off-diagonal entries to sum to $\\frac{1}{2}$, preserving row-stochasticity and the self-retention of $\\frac{1}{2}$.\n\n4. Define the one-step fused similarity as the simple average\n$$\n\\overline{P} \\;=\\; \\frac{1}{2}\\Big(P_{\\text{new}}^{(1)} + P_{\\text{new}}^{(2)}\\Big).\n$$\n\nThe given affinity matrices are\n$$\nW^{(1)} \\;=\\;\n\\begin{pmatrix}\n0 & 0.8 & 0.2 \\\\\n0.8 & 0 & 0.3 \\\\\n0.2 & 0.3 & 0\n\\end{pmatrix},\n\\qquad\nW^{(2)} \\;=\\;\n\\begin{pmatrix}\n0 & 0.4 & 0.7 \\\\\n0.4 & 0 & 0.1 \\\\\n0.7 & 0.1 & 0\n\\end{pmatrix}.\n$$\n\nTasks:\n- Using only the definitions above, carry out one full SNF iteration: construct $P^{(1)}$ and $P^{(2)}$ from $W^{(1)}$ and $W^{(2)}$, construct $S^{(1)}$ and $S^{(2)}$ with $K=1$, compute $\\widetilde{P}^{(1)}$ and $\\widetilde{P}^{(2)}$, renormalize to $P_{\\text{new}}^{(1)}$ and $P_{\\text{new}}^{(2)}$, and then compute $\\overline{P}$. Clearly show the normalization and update steps.\n- Briefly interpret how the local neighborhoods of patients $1$, $2$, and $3$ are altered across layers by comparing $S^{(1)}$ and $S^{(2)}$ and by inspecting $P_{\\text{new}}^{(1)}$ and $P_{\\text{new}}^{(2)}$.\n\nAnswer specification:\nReport only the scalar fused similarity between patients $1$ and $2$ after one iteration, that is $\\overline{P}_{12}$, as a single exact reduced fraction. Do not include any units. Do not round; an exact fraction is required as the final answer.",
            "solution": "The user-provided problem is validated as scientifically sound, well-posed, and objective. It describes a standard instance of the Similarity Network Fusion (SNF) algorithm, providing all necessary data and definitions for a unique, verifiable solution. The problem is free of contradictions or ambiguities. Therefore, a full solution is warranted.\n\nThe primary objective is to compute the fused similarity $\\overline{P}_{12}$ between patients $1$ and $2$ after a single SNF iteration. This will be accomplished by following the sequence of operations defined in the problem statement.\n\nStep 1: Construction of the initial transition matrices $P^{(1)}$ and $P^{(2)}$.\nThe transition matrix $P^{(m)}$ is defined by $P^{(m)}_{ii} = \\frac{1}{2}$ and $P^{(m)}_{ij} = \\frac{W^{(m)}_{ij}}{2 \\sum_{k \\neq i} W^{(m)}_{ik}}$ for $i \\neq j$.\n\nFor $W^{(1)} = \\begin{pmatrix} 0 & 0.8 & 0.2 \\\\ 0.8 & 0 & 0.3 \\\\ 0.2 & 0.3 & 0 \\end{pmatrix}$:\nThe off-diagonal row sums are:\nRow 1: $\\sum_{k \\neq 1} W^{(1)}_{1k} = 0.8 + 0.2 = 1.0$\nRow 2: $\\sum_{k \\neq 2} W^{(1)}_{2k} = 0.8 + 0.3 = 1.1$\nRow 3: $\\sum_{k \\neq 3} W^{(1)}_{3k} = 0.2 + 0.3 = 0.5$\n\nThe off-diagonal elements of $P^{(1)}$ are:\n$P^{(1)}_{12} = \\frac{0.8}{2(1.0)} = 0.4 = \\frac{2}{5}$, $P^{(1)}_{13} = \\frac{0.2}{2(1.0)} = 0.1 = \\frac{1}{10}$\n$P^{(1)}_{21} = \\frac{0.8}{2(1.1)} = \\frac{0.8}{2.2} = \\frac{8}{22} = \\frac{4}{11}$, $P^{(1)}_{23} = \\frac{0.3}{2(1.1)} = \\frac{0.3}{2.2} = \\frac{3}{22}$\n$P^{(1)}_{31} = \\frac{0.2}{2(0.5)} = 0.2 = \\frac{1}{5}$, $P^{(1)}_{32} = \\frac{0.3}{2(0.5)} = 0.3 = \\frac{3}{10}$\n\nSo, $P^{(1)} = \\begin{pmatrix} \\frac{1}{2} & \\frac{2}{5} & \\frac{1}{10} \\\\ \\frac{4}{11} & \\frac{1}{2} & \\frac{3}{22} \\\\ \\frac{1}{5} & \\frac{3}{10} & \\frac{1}{2} \\end{pmatrix}$.\n\nFor $W^{(2)} = \\begin{pmatrix} 0 & 0.4 & 0.7 \\\\ 0.4 & 0 & 0.1 \\\\ 0.7 & 0.1 & 0 \\end{pmatrix}$:\nThe off-diagonal row sums are:\nRow 1: $\\sum_{k \\neq 1} W^{(2)}_{1k} = 0.4 + 0.7 = 1.1$\nRow 2: $\\sum_{k \\neq 2} W^{(2)}_{2k} = 0.4 + 0.1 = 0.5$\nRow 3: $\\sum_{k \\neq 3} W^{(2)}_{3k} = 0.7 + 0.1 = 0.8$\n\nThe off-diagonal elements of $P^{(2)}$ are:\n$P^{(2)}_{12} = \\frac{0.4}{2(1.1)} = \\frac{0.4}{2.2} = \\frac{4}{22} = \\frac{2}{11}$, $P^{(2)}_{13} = \\frac{0.7}{2(1.1)} = \\frac{0.7}{2.2} = \\frac{7}{22}$\n$P^{(2)}_{21} = \\frac{0.4}{2(0.5)} = 0.4 = \\frac{2}{5}$, $P^{(2)}_{23} = \\frac{0.1}{2(0.5)} = 0.1 = \\frac{1}{10}$\n$P^{(2)}_{31} = \\frac{0.7}{2(0.8)} = \\frac{0.7}{1.6} = \\frac{7}{16}$, $P^{(2)}_{32} = \\frac{0.1}{2(0.8)} = \\frac{0.1}{1.6} = \\frac{1}{16}$\n\nSo, $P^{(2)} = \\begin{pmatrix} \\frac{1}{2} & \\frac{2}{11} & \\frac{7}{22} \\\\ \\frac{2}{5} & \\frac{1}{2} & \\frac{1}{10} \\\\ \\frac{7}{16} & \\frac{1}{16} & \\frac{1}{2} \\end{pmatrix}$.\n\nStep 2: Construction of the local affinity kernels $S^{(1)}$ and $S^{(2)}$.\nWith $K=1$ and no ties in the off-diagonal weights of any row in $W^{(m)}$, each row of $S^{(m)}$ will have a single entry of $1$ corresponding to the nearest neighbor, and all other entries will be $0$.\n\nFor $W^{(1)}$:\nRow 1: $\\max(W^{(1)}_{12}, W^{(1)}_{13}) = \\max(0.8, 0.2) = W^{(1)}_{12}$. NN of 1 is 2.\nRow 2: $\\max(W^{(1)}_{21}, W^{(1)}_{23}) = \\max(0.8, 0.3) = W^{(1)}_{21}$. NN of 2 is 1.\nRow 3: $\\max(W^{(1)}_{31}, W^{(1)}_{32}) = \\max(0.2, 0.3) = W^{(1)}_{32}$. NN of 3 is 2.\nThus, $S^{(1)} = \\begin{pmatrix} 0 & 1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{pmatrix}$.\n\nFor $W^{(2)}$:\nRow 1: $\\max(W^{(2)}_{12}, W^{(2)}_{13}) = \\max(0.4, 0.7) = W^{(2)}_{13}$. NN of 1 is 3.\nRow 2: $\\max(W^{(2)}_{21}, W^{(2)}_{23}) = \\max(0.4, 0.1) = W^{(2)}_{21}$. NN of 2 is 1.\nRow 3: $\\max(W^{(2)}_{31}, W^{(2)}_{32}) = \\max(0.7, 0.1) = W^{(2)}_{31}$. NN of 3 is 1.\nThus, $S^{(2)} = \\begin{pmatrix} 0 & 0 & 1 \\\\ 1 & 0 & 0 \\\\ 1 & 0 & 0 \\end{pmatrix}$.\n\nInterpretation of neighborhoods: The local affinity kernels reveal that the two data modalities imply different patient neighborhoods. In layer $1$, patients $1$ and $2$ form a reciprocally nearest-neighbor pair. In layer $2$, patient $1$ acts as a hub, being the nearest neighbor for both patients $2$ and $3$, while patient $1$'s nearest neighbor is patient $3$. The link between patients $2$ and $1$ is the only nearest-neighbor relationship conserved across both layers.\n\nStep 3: Computation of the updated (unnormalized) matrices $\\widetilde{P}^{(m)}$.\nThe update rule is $\\widetilde{P}^{(m)} = S^{(m)} P^{(\\ell)} (S^{(m)})^{\\top}$ for $\\ell \\neq m$.\n\nWe need the transposes: $(S^{(1)})^{\\top} = \\begin{pmatrix} 0 & 1 & 0 \\\\ 1 & 0 & 1 \\\\ 0 & 0 & 0 \\end{pmatrix}$ and $(S^{(2)})^{\\top} = \\begin{pmatrix} 0 & 1 & 1 \\\\ 0 & 0 & 0 \\\\ 1 & 0 & 0 \\end{pmatrix}$.\n\nFor $\\widetilde{P}^{(1)} = S^{(1)} P^{(2)} (S^{(1)})^{\\top}$:\n$$ \\widetilde{P}^{(1)} = \\begin{pmatrix} 0 & 1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} & \\frac{2}{11} & \\frac{7}{22} \\\\ \\frac{2}{5} & \\frac{1}{2} & \\frac{1}{10} \\\\ \\frac{7}{16} & \\frac{1}{16} & \\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} 0 & 1 & 0 \\\\ 1 & 0 & 1 \\\\ 0 & 0 & 0 \\end{pmatrix} $$\n$$ \\widetilde{P}^{(1)} = \\begin{pmatrix} \\frac{2}{5} & \\frac{1}{2} & \\frac{1}{10} \\\\ \\frac{1}{2} & \\frac{2}{11} & \\frac{7}{22} \\\\ \\frac{2}{5} & \\frac{1}{2} & \\frac{1}{10} \\end{pmatrix} \\begin{pmatrix} 0 & 1 & 0 \\\\ 1 & 0 & 1 \\\\ 0 & 0 & 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} & \\frac{2}{5} & \\frac{1}{2} \\\\ \\frac{2}{11} & \\frac{1}{2} & \\frac{2}{11} \\\\ \\frac{1}{2} & \\frac{2}{5} & \\frac{1}{2} \\end{pmatrix} $$\n\nFor $\\widetilde{P}^{(2)} = S^{(2)} P^{(1)} (S^{(2)})^{\\top}$:\n$$ \\widetilde{P}^{(2)} = \\begin{pmatrix} 0 & 0 & 1 \\\\ 1 & 0 & 0 \\\\ 1 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} & \\frac{2}{5} & \\frac{1}{10} \\\\ \\frac{4}{11} & \\frac{1}{2} & \\frac{3}{22} \\\\ \\frac{1}{5} & \\frac{3}{10} & \\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} 0 & 1 & 1 \\\\ 0 & 0 & 0 \\\\ 1 & 0 & 0 \\end{pmatrix} $$\n$$ \\widetilde{P}^{(2)} = \\begin{pmatrix} \\frac{1}{5} & \\frac{3}{10} & \\frac{1}{2} \\\\ \\frac{1}{2} & \\frac{2}{5} & \\frac{1}{10} \\\\ \\frac{1}{2} & \\frac{2}{5} & \\frac{1}{10} \\end{pmatrix} \\begin{pmatrix} 0 & 1 & 1 \\\\ 0 & 0 & 0 \\\\ 1 & 0 & 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} & \\frac{1}{5} & \\frac{1}{5} \\\\ \\frac{1}{10} & \\frac{1}{2} & \\frac{1}{2} \\\\ \\frac{1}{10} & \\frac{1}{2} & \\frac{1}{2} \\end{pmatrix} $$\n\nStep 4: Renormalization to obtain $P_{\\text{new}}^{(1)}$ and $P_{\\text{new}}^{(2)}$.\nThe diagonal is set to $\\frac{1}{2}$, and the off-diagonal elements of each row are rescaled to sum to $\\frac{1}{2}$.\n\nFor $P_{\\text{new}}^{(1)}$ from $\\widetilde{P}^{(1)}$:\nRow 1: Off-diagonal sum is $\\widetilde{P}^{(1)}_{12} + \\widetilde{P}^{(1)}_{13} = \\frac{2}{5} + \\frac{1}{2} = \\frac{9}{10}$. Scaling factor is $\\frac{1/2}{9/10} = \\frac{5}{9}$.\n$P_{\\text{new}, 12}^{(1)} = \\frac{2}{5} \\cdot \\frac{5}{9} = \\frac{2}{9}$. $P_{\\text{new}, 13}^{(1)} = \\frac{1}{2} \\cdot \\frac{5}{9} = \\frac{5}{18}$.\nRow 2: Off-diagonal sum is $\\widetilde{P}^{(1)}_{21} + \\widetilde{P}^{(1)}_{23} = \\frac{2}{11} + \\frac{2}{11} = \\frac{4}{11}$. Scaling factor is $\\frac{1/2}{4/11} = \\frac{11}{8}$.\n$P_{\\text{new}, 21}^{(1)} = \\frac{2}{11} \\cdot \\frac{11}{8} = \\frac{1}{4}$. $P_{\\text{new}, 23}^{(1)} = \\frac{2}{11} \\cdot \\frac{11}{8} = \\frac{1}{4}$.\nRow 3: Off-diagonal sum is $\\widetilde{P}^{(1)}_{31} + \\widetilde{P}^{(1)}_{32} = \\frac{1}{2} + \\frac{2}{5} = \\frac{9}{10}$. Scaling factor is $\\frac{5}{9}$.\n$P_{\\text{new}, 31}^{(1)} = \\frac{1}{2} \\cdot \\frac{5}{9} = \\frac{5}{18}$. $P_{\\text{new}, 32}^{(1)} = \\frac{2}{5} \\cdot \\frac{5}{9} = \\frac{2}{9}$.\nSo, $P_{\\text{new}}^{(1)} = \\begin{pmatrix} \\frac{1}{2} & \\frac{2}{9} & \\frac{5}{18} \\\\ \\frac{1}{4} & \\frac{1}{2} & \\frac{1}{4} \\\\ \\frac{5}{18} & \\frac{2}{9} & \\frac{1}{2} \\end{pmatrix}$.\n\nFor $P_{\\text{new}}^{(2)}$ from $\\widetilde{P}^{(2)}$:\nRow 1: Off-diagonal sum is $\\widetilde{P}^{(2)}_{12} + \\widetilde{P}^{(2)}_{13} = \\frac{1}{5} + \\frac{1}{5} = \\frac{2}{5}$. Scaling factor is $\\frac{1/2}{2/5} = \\frac{5}{4}$.\n$P_{\\text{new}, 12}^{(2)} = \\frac{1}{5} \\cdot \\frac{5}{4} = \\frac{1}{4}$. $P_{\\text{new}, 13}^{(2)} = \\frac{1}{5} \\cdot \\frac{5}{4} = \\frac{1}{4}$.\nRow 2: Off-diagonal sum is $\\widetilde{P}^{(2)}_{21} + \\widetilde{P}^{(2)}_{23} = \\frac{1}{10} + \\frac{1}{2} = \\frac{6}{10} = \\frac{3}{5}$. Scaling factor is $\\frac{1/2}{3/5} = \\frac{5}{6}$.\n$P_{\\text{new}, 21}^{(2)} = \\frac{1}{10} \\cdot \\frac{5}{6} = \\frac{5}{60} = \\frac{1}{12}$. $P_{\\text{new}, 23}^{(2)} = \\frac{1}{2} \\cdot \\frac{5}{6} = \\frac{5}{12}$.\nRow 3: Off-diagonal sum is $\\widetilde{P}^{(2)}_{31} + \\widetilde{P}^{(2)}_{32} = \\frac{1}{10} + \\frac{1}{2} = \\frac{3}{5}$. Scaling factor is $\\frac{5}{6}$.\n$P_{\\text{new}, 31}^{(2)} = \\frac{1}{10} \\cdot \\frac{5}{6} = \\frac{1}{12}$. $P_{\\text{new}, 32}^{(2)} = \\frac{1}{2} \\cdot \\frac{5}{6} = \\frac{5}{12}$.\nSo, $P_{\\text{new}}^{(2)} = \\begin{pmatrix} \\frac{1}{2} & \\frac{1}{4} & \\frac{1}{4} \\\\ \\frac{1}{12} & \\frac{1}{2} & \\frac{5}{12} \\\\ \\frac{1}{12} & \\frac{5}{12} & \\frac{1}{2} \\end{pmatrix}$.\n\nStep 5: Computation of the final fused similarity $\\overline{P}_{12}$.\nThe fused similarity matrix is $\\overline{P} = \\frac{1}{2}(P_{\\text{new}}^{(1)} + P_{\\text{new}}^{(2)})$. We are interested in the $(1,2)$ element.\n$$ \\overline{P}_{12} = \\frac{1}{2} \\left( P_{\\text{new}, 12}^{(1)} + P_{\\text{new}, 12}^{(2)} \\right) $$\n$$ \\overline{P}_{12} = \\frac{1}{2} \\left( \\frac{2}{9} + \\frac{1}{4} \\right) $$\nTo sum the fractions, we find a common denominator, which is $36$:\n$$ \\frac{2}{9} + \\frac{1}{4} = \\frac{2 \\cdot 4}{36} + \\frac{1 \\cdot 9}{36} = \\frac{8+9}{36} = \\frac{17}{36} $$\nNow, we complete the calculation for $\\overline{P}_{12}$:\n$$ \\overline{P}_{12} = \\frac{1}{2} \\cdot \\frac{17}{36} = \\frac{17}{72} $$\nThis fraction is in its reduced form.\nThe fusion process has synthesized the information from both networks. The resulting transition probability, or similarity score, from patient $1$ to patient $2$ is $\\frac{17}{72}$. This value reflects the combined evidence from both omics layers, mediated by their local neighborhood structures.",
            "answer": "$$\\boxed{\\frac{17}{72}}$$"
        }
    ]
}