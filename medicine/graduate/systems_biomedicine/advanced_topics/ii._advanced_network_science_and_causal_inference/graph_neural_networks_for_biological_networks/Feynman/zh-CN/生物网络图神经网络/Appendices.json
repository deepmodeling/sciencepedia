{
    "hands_on_practices": [
        {
            "introduction": "图神经网络（GNN）的核心在于其消息传递机制，即节点通过聚合其邻居的信息来更新自身的表示。为了具体理解这一过程，我们将从一个基本的计算练习开始 。这个练习将模拟单个图卷积层中对一个蛋白质节点的特征更新，通过亲手计算，你将清晰地看到信息是如何在蛋白质相互作用（PPI）网络的局部邻域中流动和整合的。",
            "id": "1436694",
            "problem": "在系统生物学领域，图神经网络 (GNN) 是用于学习网络结构化数据（例如蛋白质-蛋白质相互作用 (PPI) 网络）的强大工具。考虑一个简单的 GNN 模型，该模型旨在根据蛋白质之间的连接来更新其特征。\n\n我们关注的是一个 PPI 网络的一小部分，其中涉及一个中心蛋白质 V，它与两个相邻蛋白质 U1 和 U2 相互作用。每个蛋白质都由一个二维特征向量描述。向量的第一个元素代表蛋白质的归一化表达水平，第二个元素代表其线粒体定位分数。\n\n初始特征向量如下：\n- 蛋白质 V: $h_V = [0.1, 0.9]$\n- 蛋白质 U1: $h_{U1} = [0.2, 0.3]$\n- 蛋白质 U2: $h_{U2} = [0.6, 0.1]$\n\nGNN 在单个消息传递步骤中执行一次更新，以找到蛋白质 V 的新特征向量，记为 $h'_V$。该过程包括两个阶段：\n\n1.  **聚合：** 通过对蛋白质 V 的所有邻居的特征向量进行逐元素求和，创建一个聚合邻居向量 $h_{N(V)}$。\n2.  **更新：** 将聚合邻居向量 $h_{N(V)}$ 与蛋白质 V 的原始特征向量 $h_V$ 相加，然后对结果应用逐元素的修正线性单元 (ReLU) 激活函数，从而计算出新的特征向量 $h'_V$。ReLU 函数对向量的每个分量定义为 $\\text{ReLU}(x) = \\max(0, x)$。\n\n计算蛋白质 V 更新后的特征向量 $h'_V$。",
            "solution": "给定三种蛋白质的初始特征向量：\n$$\nh_{V}=\\begin{pmatrix}0.1  0.9\\end{pmatrix},\\quad\nh_{U1}=\\begin{pmatrix}0.2  0.3\\end{pmatrix},\\quad\nh_{U2}=\\begin{pmatrix}0.6  0.1\\end{pmatrix}.\n$$\n聚合步骤：聚合邻居向量是邻居特征的逐元素和，\n$$\nh_{N(V)}=h_{U1}+h_{U2}\n=\\begin{pmatrix}0.2  0.3\\end{pmatrix}+\\begin{pmatrix}0.6  0.1\\end{pmatrix}\n=\\begin{pmatrix}0.8  0.4\\end{pmatrix}.\n$$\n更新步骤：将逐元素 ReLU 应用于 $h_{V}$ 和 $h_{N(V)}$ 的和，\n$$\nh'_{V}=\\operatorname{ReLU}\\!\\big(h_{V}+h_{N(V)}\\big)\n=\\operatorname{ReLU}\\!\\left(\\begin{pmatrix}0.1  0.9\\end{pmatrix}+\\begin{pmatrix}0.8  0.4\\end{pmatrix}\\right)\n=\\operatorname{ReLU}\\!\\begin{pmatrix}0.9  1.3\\end{pmatrix}.\n$$\n由于每个分量都为正，因此对于两个分量都有 $\\operatorname{ReLU}(x)=x$，所以\n$$\nh'_{V}=\\begin{pmatrix}0.9  1.3\\end{pmatrix}.\n$$",
            "answer": "$$\\boxed{\\begin{pmatrix}0.9  1.3\\end{pmatrix}}$$"
        },
        {
            "introduction": "上一个练习展示了信息如何聚合，但GNN何以“学习”？答案在于可训练的参数，它们在整个网络中共享，以识别普适的模式。此练习  聚焦于GNN层中的一个关键组件——权重矩阵 $W$。通过确定这个矩阵的维度，你将理解GNN如何将输入特征线性变换到更高维度的嵌入空间，从而为下游任务提取更丰富、更有意义的表示。",
            "id": "1436719",
            "problem": "一位系统生物学家正在使用图神经网络 (GNN) 来研究一个基因-基因相互作用网络。在这个模型中，网络的 $N=500$ 个基因被表示为节点，如果对应的基因已知会相互作用，那么两个节点之间就有一条边连接。\n\n每个基因最初由一个表示一组生物学特性的特征向量来描述。这个初始表示是一个8维向量，意味着每个节点的输入特征维度为8。\n\n该生物学家采用了一个单图卷积层。该层的一个关键步骤是进行线性变换，以更新每个节点的特征向量。这个变换使用一个单一的、共享的、可学习的权重矩阵，记为 $W$。该步骤的目的是将8维的输入特征投影到一个新的16维特征空间中，为每个基因创建一个更丰富的“嵌入”。\n\n假设按照标准约定，节点特征的集合表示为一个矩阵，且变换是通过右乘权重矩阵 $W$ 来实现的，那么权重矩阵 $W$ 的维度（依次为行数和列数）是多少？",
            "solution": "设 $X$ 表示具有 $N$ 个节点和输入特征维度 $F_{\\text{in}}$ 的节点特征矩阵。在右乘约定下，图卷积层中的线性变换为\n$$\nX' = X W,\n$$\n其中 $X \\in \\mathbb{R}^{N \\times F_{\\text{in}}}$，$W \\in \\mathbb{R}^{F_{\\text{in}} \\times F_{\\text{out}}}$，因此根据矩阵乘法法则，$X' \\in \\mathbb{R}^{N \\times F_{\\text{out}}}$：如果 $A \\in \\mathbb{R}^{a \\times b}$ 且 $B \\in \\mathbb{R}^{b \\times c}$，那么 $AB \\in \\mathbb{R}^{a \\times c}$。\n\n在本题中，$F_{\\text{in}}=8$ 且期望的输出特征维度为 $F_{\\text{out}}=16$。因此，\n$$\nX \\in \\mathbb{R}^{500 \\times 8}, \\quad W \\in \\mathbb{R}^{8 \\times 16}, \\quad XW \\in \\mathbb{R}^{500 \\times 16}.\n$$\n因此，权重矩阵 $W$ 必须有 8 行和 16 列。在这种共享线性变换下，$N$ 的值不影响 $W$ 的维度。",
            "answer": "$$\\boxed{\\begin{pmatrix}8  16\\end{pmatrix}}$$"
        },
        {
            "introduction": "理解GNN的工作原理同样意味着要认识到它的局限性。消息传递范式虽然强大，但它有一个固有的约束：信息只能沿着图中存在的路径传播。这个概念性问题  探讨了当生物网络（如蛋白质相互作用网络）由许多不连通的子图组成时，GNN为何会表现不佳。这对于在处理稀疏或不完整的真实世界数据时，如何正确应用和评估GNN模型至关重要。",
            "id": "1436702",
            "problem": "一位系统生物学家正在使用图神经网络 (GNN) 研究蛋白质功能。数据集是一个蛋白质-蛋白质相互作用 (PPI) 网络，其中节点代表蛋白质，边代表经过实验验证的相互作用。目标是执行节点分类，预测每种蛋白质的功能类别。然而，由于实验数据的稀疏性，生成的图不是一个单一的连通实体。相反，它由许多小的、不连通的分量组成——这些分量是少数相互作用的蛋白质组成的孤立岛屿，岛屿之间没有已知的相互作用联系。\n\n在该网络上训练了一个标准的消息传递GNN后，这位生物学家观察到模型在留出的测试集上表现极差，几乎不比随机猜测好。下列哪个陈述为这一失败提供了最根本和最直接的解释？\n\nA. 消息传递机制根据节点的邻居来更新其特征，它将信息流完全限制在每个不连通的分量内部。这阻止了模型学习和泛化可能在图的不同分量之间共享的模式。\n\nB. 模型正在经历严重的过拟合，因为训练数据不够大。不连通分量的存在与模型的性能无关。\n\nC. GNN要求输入图表示为单个、稠密的邻接矩阵。具有不连通分量的图无法以这种格式表示，从而导致基本的数据结构不兼容。\n\nD. GNN中使用的非线性激活函数（如整流线性单元ReLU）在应用于连接数（度）非常低的节点时，在数学上是不稳定的，而这是小分量中节点的普遍特征。\n\nE. 在具有许多分量的图上，GNN中的消息传递在计算上变得低效和缓慢，导致训练过程在模型收敛到良好解之前过早终止。",
            "solution": "一个标准的消息传递GNN通过仅聚合来自相邻节点的信息来更新节点嵌入。一个典型的层可以写成\n$$\nh_{v}^{(k+1)}=\\phi\\!\\left(h_{v}^{(k)},\\;\\square_{u\\in\\mathcal{N}(v)}\\psi\\!\\left(h_{u}^{(k)}, e_{uv}\\right)\\right),\n$$\n其中 $h_{v}^{(k)}$ 是节点 $v$ 在第 $k$ 层的表示，$\\mathcal{N}(v)$ 是 $v$ 的邻居集合，$e_{uv}$ 编码了边的特征，$\\psi$ 是一个消息函数，$\\square$ 是一个排列不变的聚合器，而 $\\phi$ 是一个更新函数。经过 $K$ 层后，$h_{v}^{(K)}$ 仅依赖于 $v$ 的 $K$ 跳邻域内的节点。如果图由不连通的分量组成，那么对于不同分量中的任意两个节点，它们之间没有路径，因此任何消息传递层的序列都无法在分量之间传递信息。\n\n因此，对于节点分类任务，一个分量中可用的任何标签或结构信号都无法通过消息传递传播到另一个分量中的节点。如果训练集-测试集的划分将带标签的节点放在某些分量中，而将测试节点放在不同的、不连通的分量中，那么对这些测试节点的预测就无法从训练分量的关系信息或标签传播中受益。虽然GNN的参数在所有节点间全局共享，但消息传递模型的主要归纳偏置是利用图的连通性；当训练节点和测试节点之间的连通性缺失时，模型捕捉和传递关系模式的核心机制从根本上被阻断了。这直接解释了在孤立的测试分量上，模型性能接近随机猜测的原因。\n\n评估各个选项：\n- A 正确地指出消息传递将信息流限制在连通分量内部，这是在这种情况下失败的根本机制层面的原因。\n- B 将失败归因于过拟合，并忽略了不连通分量的作用，这在所描述的设置下不是最直接的解释。\n- C 是错误的：不连通的图可以通过块对角邻接矩阵来表示，不存在不兼容问题。\n- D 是错误的：像ReLU这样的标准激活函数在低度节点上并非数学上不稳定。\n- E 不是根本原因，并且通常是错误的；不连通性本身不会导致过早终止。\n\n因此，A 提供了最根本和最直接的解释。",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}