{
    "hands_on_practices": [
        {
            "introduction": "A core task in building Bayesian networks for biological systems is learning the model's parameters—the conditional probabilities governing how nodes influence each other—from experimental data. This exercise provides a concrete example of this process using conjugate priors, a cornerstone of Bayesian statistics. By working through this problem , you will see how a prior belief about a transcription factor's activity states, encoded in a Dirichlet distribution, is formally updated with new observational counts to yield a posterior distribution with refined estimates and quantified uncertainty.",
            "id": "4318094",
            "problem": "In a Bayesian network (BN) representing a causal model of a transcription factor’s tri-modal activity state in a systems biomedicine setting, consider a single ternary node with latent probability vector $\\boldsymbol{\\theta} = (\\theta_1, \\theta_2, \\theta_3)$ over three mutually exclusive cellular states. Prior knowledge from earlier RNA sequencing (RNA-seq) studies is encoded as a Dirichlet prior $\\operatorname{Dirichlet}(\\alpha_1, \\alpha_2, \\alpha_3)$ with hyperparameters $(\\alpha_1, \\alpha_2, \\alpha_3) = (2.4, 1.6, 0.8)$. A new cohort of $20$ matched perturbation experiments yields observed counts $(n_1, n_2, n_3) = (10, 7, 3)$ across the three states.\n\nStarting from the definition of the multinomial likelihood for counts given $\\boldsymbol{\\theta}$ and the Dirichlet prior as a conjugate family for the multinomial, derive the posterior distribution over $\\boldsymbol{\\theta}$ and, from first principles of the Dirichlet family, compute the posterior mean $\\mathbb{E}[\\theta_i \\mid \\text{data}]$ and the posterior marginal variance $\\operatorname{Var}[\\theta_i \\mid \\text{data}]$ for each category $i \\in \\{1,2,3\\}$.\n\nProvide the final numerical values for all three posterior means followed by all three posterior variances, in that order as a single row matrix. Round each value to four significant figures. No units are required.",
            "solution": "The user has provided a valid problem statement.\n\nThe problem requires the derivation of the posterior distribution for a latent probability vector $\\boldsymbol{\\theta}$ and the computation of its posterior mean and marginal variance. This is a classic Bayesian inference problem involving a multinomial likelihood and a Dirichlet prior, which are a conjugate pair.\n\n**Step 1: Define the Model Components (Likelihood and Prior)**\n\nLet the latent probability vector for the three cellular states be $\\boldsymbol{\\theta} = (\\theta_1, \\theta_2, \\theta_3)$, where $\\theta_i \\ge 0$ for all $i$ and $\\sum_{i=1}^3 \\theta_i = 1$. The observed data consists of counts $\\mathbf{n} = (n_1, n_2, n_3) = (10, 7, 3)$ from a total of $N = \\sum n_i = 20$ experiments.\n\nThe likelihood of observing these counts, given the probabilities $\\boldsymbol{\\theta}$, is described by a multinomial distribution. The probability mass function is:\n$$P(\\mathbf{n} \\mid \\boldsymbol{\\theta}) = \\frac{N!}{n_1! n_2! n_3!} \\theta_1^{n_1} \\theta_2^{n_2} \\theta_3^{n_3}$$\nFor the purpose of finding the posterior distribution of $\\boldsymbol{\\theta}$, the multinomial coefficient $\\frac{N!}{n_1! n_2! n_3!}$ is a constant. Therefore, the likelihood function is proportional to:\n$$P(\\mathbf{n} \\mid \\boldsymbol{\\theta}) \\propto \\prod_{i=1}^3 \\theta_i^{n_i}$$\n\nThe prior knowledge about $\\boldsymbol{\\theta}$ is encoded as a Dirichlet distribution with hyperparameter vector $\\boldsymbol{\\alpha} = (\\alpha_1, \\alpha_2, \\alpha_3) = (2.4, 1.6, 0.8)$. The probability density function of the prior is:\n$$P(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\alpha}) = \\frac{\\Gamma(\\sum_{i=1}^3 \\alpha_i)}{\\prod_{i=1}^3 \\Gamma(\\alpha_i)} \\prod_{i=1}^3 \\theta_i^{\\alpha_i - 1} = \\frac{\\Gamma(\\alpha_0)}{\\prod_{i=1}^3 \\Gamma(\\alpha_i)} \\prod_{i=1}^3 \\theta_i^{\\alpha_i - 1}$$\nwhere $\\alpha_0 = \\sum_{i=1}^3 \\alpha_i = 2.4 + 1.6 + 0.8 = 4.8$.\nThe prior distribution is thus $P(\\boldsymbol{\\theta}) \\propto \\prod_{i=1}^3 \\theta_i^{\\alpha_i - 1}$.\n\n**Step 2: Derive the Posterior Distribution**\n\nUsing Bayes' theorem, the posterior distribution $P(\\boldsymbol{\\theta} \\mid \\mathbf{n}, \\boldsymbol{\\alpha})$ is proportional to the product of the likelihood and the prior:\n$$P(\\boldsymbol{\\theta} \\mid \\mathbf{n}, \\boldsymbol{\\alpha}) \\propto P(\\mathbf{n} \\mid \\boldsymbol{\\theta}) P(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\alpha})$$\nSubstituting the proportional forms of the likelihood and prior:\n$$P(\\boldsymbol{\\theta} \\mid \\mathbf{n}, \\boldsymbol{\\alpha}) \\propto \\left( \\prod_{i=1}^3 \\theta_i^{n_i} \\right) \\left( \\prod_{i=1}^3 \\theta_i^{\\alpha_i - 1} \\right)$$\n$$P(\\boldsymbol{\\theta} \\mid \\mathbf{n}, \\boldsymbol{\\alpha}) \\propto \\prod_{i=1}^3 \\theta_i^{n_i + \\alpha_i - 1}$$\nThis resulting functional form is the kernel of a Dirichlet distribution. Thus, the posterior distribution is also a Dirichlet distribution, a property known as conjugacy. The posterior distribution is $\\operatorname{Dirichlet}(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\alpha}')$, where the updated hyperparameters $\\boldsymbol{\\alpha}' = (\\alpha'_1, \\alpha'_2, \\alpha'_3)$ are given by:\n$$\\alpha'_i = \\alpha_i + n_i$$\nLet's compute the specific values for the posterior hyperparameters:\n$$\\alpha'_1 = \\alpha_1 + n_1 = 2.4 + 10 = 12.4$$\n$$\\alpha'_2 = \\alpha_2 + n_2 = 1.6 + 7 = 8.6$$\n$$\\alpha'_3 = \\alpha_3 + n_3 = 0.8 + 3 = 3.8$$\nSo, the posterior distribution is $\\boldsymbol{\\theta} \\mid \\mathbf{n} \\sim \\operatorname{Dirichlet}(12.4, 8.6, 3.8)$.\n\n**Step 3: Compute Posterior Mean and Variance from First Principles**\n\nFor a random vector $\\boldsymbol{\\theta} \\sim \\operatorname{Dirichlet}(\\boldsymbol{\\alpha}')$, the sum of the hyperparameters is $\\alpha'_0 = \\sum_{i=1}^3 \\alpha'_i$. The expected value (mean) and variance of each component $\\theta_i$ are given by standard formulas for the Dirichlet distribution.\n\nThe sum of the posterior hyperparameters is:\n$$\\alpha'_0 = \\alpha'_1 + \\alpha'_2 + \\alpha'_3 = 12.4 + 8.6 + 3.8 = 24.8$$\nAs a consistency check, $\\alpha'_0 = \\alpha_0 + N = 4.8 + 20 = 24.8$.\n\nThe posterior mean for each component $\\theta_i$ is given by:\n$$\\mathbb{E}[\\theta_i \\mid \\mathbf{n}] = \\frac{\\alpha'_i}{\\alpha'_0}$$\nWe compute the mean for each state:\n$$\\mathbb{E}[\\theta_1 \\mid \\mathbf{n}] = \\frac{12.4}{24.8} = 0.5$$\n$$\\mathbb{E}[\\theta_2 \\mid \\mathbf{n}] = \\frac{8.6}{24.8} \\approx 0.346774...$$\n$$\\mathbb{E}[\\theta_3 \\mid \\mathbf{n}] = \\frac{3.8}{24.8} \\approx 0.153225...$$\n\nThe posterior marginal variance for each component $\\theta_i$ is given by:\n$$\\operatorname{Var}[\\theta_i \\mid \\mathbf{n}] = \\frac{\\alpha'_i (\\alpha'_0 - \\alpha'_i)}{\\alpha'_0^2 (\\alpha'_0 + 1)}$$\nWe compute the variance for each state:\n$$\\operatorname{Var}[\\theta_1 \\mid \\mathbf{n}] = \\frac{12.4 \\times (24.8 - 12.4)}{24.8^2 \\times (24.8 + 1)} = \\frac{12.4 \\times 12.4}{24.8^2 \\times 25.8} = \\frac{153.76}{615.04 \\times 25.8} = \\frac{153.76}{15868.032} \\approx 0.0096900...$$\n$$\\operatorname{Var}[\\theta_2 \\mid \\mathbf{n}] = \\frac{8.6 \\times (24.8 - 8.6)}{24.8^2 \\times (24.8 + 1)} = \\frac{8.6 \\times 16.2}{24.8^2 \\times 25.8} = \\frac{139.32}{15868.032} \\approx 0.0087799...$$\n$$\\operatorname{Var}[\\theta_3 \\mid \\mathbf{n}] = \\frac{3.8 \\times (24.8 - 3.8)}{24.8^2 \\times (24.8 + 1)} = \\frac{3.8 \\times 21.0}{24.8^2 \\times 25.8} = \\frac{79.8}{15868.032} \\approx 0.0050289...$$\n\n**Step 4: Final Numerical Values**\n\nRounding the results to four significant figures as requested:\nPosterior Means:\n$$\\mathbb{E}[\\theta_1 \\mid \\mathbf{n}] = 0.5000$$\n$$\\mathbb{E}[\\theta_2 \\mid \\mathbf{n}] \\approx 0.3468$$\n$$\\mathbb{E}[\\theta_3 \\mid \\mathbf{n}] \\approx 0.1532$$\n\nPosterior Variances:\n$$\\operatorname{Var}[\\theta_1 \\mid \\mathbf{n}] \\approx 0.009690$$\n$$\\operatorname{Var}[\\theta_2 \\mid \\mathbf{n}] \\approx 0.008780$$\n$$\\operatorname{Var}[\\theta_3 \\mid \\mathbf{n}] \\approx 0.005029$$\n\nThe final answer is a row matrix containing these six values in the specified order.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.5000 & 0.3468 & 0.1532 & 0.009690 & 0.008780 & 0.005029\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Understanding the structure of a causal graph is critical, as it reveals how statistical associations can be created or altered by the act of observation, a common pitfall in biomedical data analysis. This exercise demonstrates one of the most important of these phenomena: collider bias. Through a hypothetical but realistic scenario involving two biomarkers $X$ and $Y$ and a derived triage score $Z$, you will see how conditioning on a common effect, or \"collider,\" can dramatically change the statistical relationship between its causes . This practice provides a crucial quantitative lesson, showing how an analyst's seemingly reasonable step of \"adjusting\" for a variable can induce a strong, spurious association and lead to fundamentally incorrect conclusions.",
            "id": "4318057",
            "problem": "In a systems biomedicine study of inflammatory response, consider a causal data-generating process modeled as a linear Gaussian Structural Causal Model (SCM). Let $U$ be an unobserved systemic inflammation factor that affects two circulating biomarkers $X$ and $Y$. A triage score $Z$ is computed from the two biomarkers and used for admission decisions. The causal structure is given by the directed acyclic graph with arrows $U \\to X$, $U \\to Y$, $X \\to Z$, and $Y \\to Z$, so that $Z$ is a collider of $X$ and $Y$. Suppose the variables are generated by the following linear system with independent Gaussian noise:\n$$\nU \\sim \\mathcal{N}(0,1), \\quad \\varepsilon_{1} \\sim \\mathcal{N}(0,1), \\quad \\varepsilon_{2} \\sim \\mathcal{N}(0,1), \\quad \\varepsilon_{3} \\sim \\mathcal{N}\\!\\left(0,\\tfrac{1}{10}\\right),\n$$\n$$\nX = U + \\varepsilon_{1}, \\quad Y = U + \\varepsilon_{2}, \\quad Z = X + Y + \\varepsilon_{3},\n$$\nwith all four noise terms mutually independent and independent of $U$.\n\nUsing only fundamental definitions and well-tested facts about expectations, variances, covariances, and correlations for multivariate normal variables, do the following:\n\n1) Determine the marginal Pearson correlation between $X$ and $Y$ and state its sign.\n\n2) An analyst mistakenly adjusts for the collider $Z$. Compute the Pearson correlation between $X$ and $Y$ conditional on $Z$ in this linear Gaussian setting. Report this spurious conditioned correlation as a single exact real number.\n\nYour final answer must be the single value of the spurious conditioned correlation from part 2). No units are involved. Do not round; provide the exact value.",
            "solution": "The user has provided a problem statement regarding a linear Gaussian Structural Causal Model (SCM) and has asked for the calculation of a marginal correlation and a conditional correlation.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\nThe problem provides the following information:\n- A causal structure defined by the directed acyclic graph: $U \\to X$, $U \\to Y$, $X \\to Z$, $Y \\to Z$.\n- A set of structural equations:\n  - $X = U + \\varepsilon_{1}$\n  - $Y = U + \\varepsilon_{2}$\n  - $Z = X + Y + \\varepsilon_{3}$\n- The distributions of the exogenous variables:\n  - $U \\sim \\mathcal{N}(0,1)$\n  - $\\varepsilon_{1} \\sim \\mathcal{N}(0,1)$\n  - $\\varepsilon_{2} \\sim \\mathcal{N}(0,1)$\n  - $\\varepsilon_{3} \\sim \\mathcal{N}(0,\\frac{1}{10})$\n- An independence assumption: $U, \\varepsilon_{1}, \\varepsilon_{2}, \\varepsilon_{3}$ are mutually independent.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded**: The problem is grounded in the well-established framework of Structural Causal Models, a standard tool in statistics, econometrics, and systems biology for modeling causal relationships. The model described is a canonical example used to illustrate confounding and collider bias. It is scientifically sound.\n- **Well-Posed**: The problem is well-posed. The variables $(X, Y, Z)$ are linear combinations of independent Gaussian variables, which means they form a multivariate normal distribution. For such a distribution, all marginal and conditional correlations are uniquely defined and computable from the covariance matrix.\n- **Objective**: The problem is stated using precise mathematical language, free of subjective or ambiguous terminology.\n- The problem is self-contained, consistent, and does not violate any physical or mathematical principles. It is a standard exercise in causal inference.\n\n**Step 3: Verdict and Action**\nThe problem is valid. The solution will now be derived.\n\n### Solution Derivation\n\nThe solution requires calculating two quantities: the marginal correlation $\\operatorname{Corr}(X, Y)$ and the conditional correlation $\\operatorname{Corr}(X, Y | Z)$. The final answer is the value of the second quantity.\n\nThe Pearson correlation coefficient between two random variables $A$ and $B$ is defined as $\\operatorname{Corr}(A, B) = \\frac{\\operatorname{Cov}(A, B)}{\\sqrt{\\operatorname{Var}(A) \\operatorname{Var}(B)}}$.\n\nFirst, we determine the means, variances, and covariances of the variables $X$ and $Y$.\nThe means are:\n$E[X] = E[U + \\varepsilon_{1}] = E[U] + E[\\varepsilon_{1}] = 0 + 0 = 0$.\n$E[Y] = E[U + \\varepsilon_{2}] = E[U] + E[\\varepsilon_{2}] = 0 + 0 = 0$.\n\nThe variances are calculated using the independence of the source variables:\n$\\operatorname{Var}(X) = \\operatorname{Var}(U + \\varepsilon_{1}) = \\operatorname{Var}(U) + \\operatorname{Var}(\\varepsilon_{1}) = 1 + 1 = 2$.\n$\\operatorname{Var}(Y) = \\operatorname{Var}(U + \\varepsilon_{2}) = \\operatorname{Var}(U) + \\operatorname{Var}(\\varepsilon_{2}) = 1 + 1 = 2$.\n\nThe covariance between $X$ and $Y$ is:\n$\\operatorname{Cov}(X, Y) = \\operatorname{Cov}(U + \\varepsilon_{1}, U + \\varepsilon_{2})$.\nUsing the bilinearity of covariance and the mutual independence of $U, \\varepsilon_{1}, \\varepsilon_{2}$:\n$\\operatorname{Cov}(X, Y) = \\operatorname{Cov}(U, U) + \\operatorname{Cov}(U, \\varepsilon_{2}) + \\operatorname{Cov}(\\varepsilon_{1}, U) + \\operatorname{Cov}(\\varepsilon_{1}, \\varepsilon_{2})$\n$\\operatorname{Cov}(X, Y) = \\operatorname{Var}(U) + 0 + 0 + 0 = 1$.\n\n**1) Marginal Correlation between $X$ and $Y$**\n\nUsing the quantities derived above:\n$$\n\\operatorname{Corr}(X, Y) = \\frac{\\operatorname{Cov}(X, Y)}{\\sqrt{\\operatorname{Var}(X) \\operatorname{Var}(Y)}} = \\frac{1}{\\sqrt{2 \\cdot 2}} = \\frac{1}{2}\n$$\nThe marginal correlation is positive, which is expected as $U$ is a common cause of both $X$ and $Y$.\n\n**2) Spurious Correlation between $X$ and $Y$ conditional on $Z$**\n\nSince $(X, Y, Z)$ are jointly Gaussian, the correlation between $X$ and $Y$ conditional on $Z$ is the partial correlation $\\rho_{XY \\cdot Z}$, which can be calculated using the formula:\n$$\n\\rho_{XY \\cdot Z} = \\frac{\\rho_{XY} - \\rho_{XZ}\\rho_{YZ}}{\\sqrt{(1 - \\rho_{XZ}^2)(1 - \\rho_{YZ}^2)}}\n$$\nwhere $\\rho_{AB}$ denotes $\\operatorname{Corr}(A, B)$. We have already found $\\rho_{XY} = \\frac{1}{2}$. We now need to compute $\\rho_{XZ}$ and $\\rho_{YZ}$. This requires the variance of $Z$ and the covariances $\\operatorname{Cov}(X, Z)$ and $\\operatorname{Cov(Y, Z)}$.\n\nFirst, express $Z$ in terms of the independent source variables:\n$Z = X + Y + \\varepsilon_{3} = (U + \\varepsilon_{1}) + (U + \\varepsilon_{2}) + \\varepsilon_{3} = 2U + \\varepsilon_{1} + \\varepsilon_{2} + \\varepsilon_{3}$.\n\nThe variance of $Z$ is:\n$\\operatorname{Var}(Z) = \\operatorname{Var}(2U + \\varepsilon_{1} + \\varepsilon_{2} + \\varepsilon_{3})$\nDue to the mutual independence of $U, \\varepsilon_{1}, \\varepsilon_{2}, \\varepsilon_{3}$:\n$\\operatorname{Var}(Z) = \\operatorname{Var}(2U) + \\operatorname{Var}(\\varepsilon_{1}) + \\operatorname{Var}(\\varepsilon_{2}) + \\operatorname{Var}(\\varepsilon_{3}) = 2^2\\operatorname{Var}(U) + \\operatorname{Var}(\\varepsilon_{1}) + \\operatorname{Var}(\\varepsilon_{2}) + \\operatorname{Var}(\\varepsilon_{3})$\n$\\operatorname{Var}(Z) = 4(1) + 1 + 1 + \\frac{1}{10} = 6 + \\frac{1}{10} = \\frac{61}{10}$.\n\nNow, we compute the covariance $\\operatorname{Cov}(X, Z)$:\n$\\operatorname{Cov}(X, Z) = \\operatorname{Cov}(X, X + Y + \\varepsilon_{3}) = \\operatorname{Cov}(X, X) + \\operatorname{Cov}(X, Y) + \\operatorname{Cov}(X, \\varepsilon_{3})$\n$\\operatorname{Cov}(X, Z) = \\operatorname{Var}(X) + \\operatorname{Cov}(X, Y) + \\operatorname{Cov}(U + \\varepsilon_{1}, \\varepsilon_{3})$\nSince $U$ and $\\varepsilon_{1}$ are independent of $\\varepsilon_{3}$, $\\operatorname{Cov}(U + \\varepsilon_{1}, \\varepsilon_{3}) = 0$.\n$\\operatorname{Cov}(X, Z) = 2 + 1 + 0 = 3$.\n\nBy symmetry in the model structure with respect to $X$ and $Y$, we can deduce $\\operatorname{Cov}(Y, Z) = 3$. Let's verify:\n$\\operatorname{Cov}(Y, Z) = \\operatorname{Cov}(Y, X + Y + \\varepsilon_{3}) = \\operatorname{Cov}(Y, X) + \\operatorname{Var}(Y) + \\operatorname{Cov}(Y, \\varepsilon_{3}) = 1 + 2 + 0 = 3$.\n\nNow we can calculate the correlations $\\rho_{XZ}$ and $\\rho_{YZ}$:\n$$\n\\rho_{XZ} = \\operatorname{Corr}(X, Z) = \\frac{\\operatorname{Cov}(X, Z)}{\\sqrt{\\operatorname{Var}(X)\\operatorname{Var}(Z)}} = \\frac{3}{\\sqrt{2 \\cdot \\frac{61}{10}}} = \\frac{3}{\\sqrt{\\frac{61}{5}}}\n$$\n$$\n\\rho_{YZ} = \\operatorname{Corr}(Y, Z) = \\frac{\\operatorname{Cov}(Y, Z)}{\\sqrt{\\operatorname{Var}(Y)\\operatorname{Var}(Z)}} = \\frac{3}{\\sqrt{2 \\cdot \\frac{61}{10}}} = \\frac{3}{\\sqrt{\\frac{61}{5}}}\n$$\nWe need the squares of these correlations:\n$\\rho_{XZ}^2 = \\rho_{YZ}^2 = \\frac{3^2}{(\\sqrt{61/5})^2} = \\frac{9}{61/5} = \\frac{45}{61}$.\n\nNow we substitute all the correlation values into the partial correlation formula:\n$$\n\\rho_{XY \\cdot Z} = \\frac{\\rho_{XY} - \\rho_{XZ}\\rho_{YZ}}{\\sqrt{(1 - \\rho_{XZ}^2)(1 - \\rho_{YZ}^2)}} = \\frac{\\frac{1}{2} - \\left(\\frac{45}{61}\\right)}{\\sqrt{\\left(1 - \\frac{45}{61}\\right)\\left(1 - \\frac{45}{61}\\right)}} = \\frac{\\frac{1}{2} - \\frac{45}{61}}{1 - \\frac{45}{61}}\n$$\nWe compute the numerator and denominator:\nNumerator: $\\frac{1}{2} - \\frac{45}{61} = \\frac{61 \\cdot 1 - 45 \\cdot 2}{122} = \\frac{61 - 90}{122} = -\\frac{29}{122}$.\nDenominator: $1 - \\frac{45}{61} = \\frac{61 - 45}{61} = \\frac{16}{61}$.\n\nFinally, the conditional correlation is:\n$$\n\\rho_{XY \\cdot Z} = \\frac{-29/122}{16/61} = -\\frac{29}{122} \\cdot \\frac{61}{16}\n$$\nSince $122 = 2 \\cdot 61$, we can simplify:\n$$\n\\rho_{XY \\cdot Z} = -\\frac{29}{2 \\cdot 61} \\cdot \\frac{61}{16} = -\\frac{29}{2 \\cdot 16} = -\\frac{29}{32}\n$$\nThis result demonstrates the phenomenon of collider bias. Conditioning on the collider $Z$ has induced a strong negative correlation between $X$ and $Y$, altering their initial positive marginal correlation.\nThe spurious conditioned correlation is exactly $-\\frac{29}{32}$.",
            "answer": "$$\\boxed{-\\frac{29}{32}}$$"
        },
        {
            "introduction": "A central challenge in systems biomedicine is estimating the causal effect of one variable on another in the presence of unobserved confounding factors. While backdoor adjustment is a common solution, it fails when confounders are not measured. This exercise introduces a powerful alternative strategy: the front-door adjustment, which allows for causal identification by leveraging a mediating variable . You will work through a signaling pathway model where the effect of a receptor $X$ on a response $Y$ is confounded, but the full effect is transmitted through an intermediate protein $M$. By calculating the causal effect using the front-door formula, you will master a key technique for dissecting a causal pathway to overcome unobserved confounding, a situation frequently encountered in biological research.",
            "id": "4318079",
            "problem": "Consider a signaling module in systems biomedicine represented by a Directed Acyclic Graph (DAG) with nodes for receptor phosphorylation $X \\in \\{0,1\\}$, adaptor protein activation $M \\in \\{0,1\\}$, and downstream transcriptional response $Y \\in \\{0,1\\}$. Assume the causal structure satisfies the front-door criterion: there is a directed path $X \\to M \\to Y$; there may exist an unobserved common cause $U$ of $X$ and $Y$ with edges $U \\to X$ and $U \\to Y$; there is no direct effect from $X$ to $Y$ that bypasses $M$; and there is no unobserved common cause of $M$ and $Y$ once conditioning on $X$. The observational distributions are empirically estimated as follows: $p(M=1 \\mid X=1) = 0.8$, $p(M=1 \\mid X=0) = 0.2$, $p(Y=1 \\mid M=1, X) = 0.7$ for all $X$, $p(Y=1 \\mid M=0, X) = 0.1$ for all $X$, and $p(X=1) = 0.5$. Using only the semantics of interventions (do-operator) and probabilistic laws that apply under the front-door criterion, derive from first principles an expression for $E[Y \\mid do(X=x)]$ in terms of the observational quantities and compute the causal effect difference $E[Y \\mid do(X=1)] - E[Y \\mid do(X=0)]$. Express your final numerical answer as a decimal without units.",
            "solution": "The problem requires the computation of the causal effect difference, $E[Y \\mid do(X=1)] - E[Y \\mid do(X=0)]$, for a biological signaling system. The system is described by a Directed Acyclic Graph (DAG) involving a receptor phosphorylation state $X$, an adaptor protein activation state $M$, a downstream response $Y$, and an unobserved common cause $U$ of $X$ and $Y$. The variables $X$, $M$, and $Y$ are binary, taking values in $\\{0,1\\}$. For a binary variable such as $Y$, its expectation is equal to the probability of it being in state $1$. Therefore, we must compute $P(Y=1 \\mid do(X=1)) - P(Y=1 \\mid do(X=0))$.\n\nThe problem states that the causal structure satisfies the front-door criterion. This implies a specific set of conditional independencies that allow us to identify the causal effect of $X$ on $Y$ from observational data, even in the presence of the unobserved confounder $U$. The specified structure involves a causal pathway $X \\to M \\to Y$ and a confounding backdoor path $X \\leftarrow U \\to Y$. The front-door criterion allows us to estimate the causal effect by using the mediating variable $M$.\n\nWe will derive the expression for the interventional probability $P(Y=y \\mid do(X=x))$ from first principles, based on the semantics of the do-operator and the graphical properties guaranteed by the front-door criterion.\n\nFirst, we express the total causal effect of $X$ on $Y$ by summing over the states of the mediating variable $M$. Since the problem states there is no direct effect of $X$ on $Y$ that bypasses $M$, all of the influence of $X$ on $Y$ must be transmitted through $M$. This allows us to chain the causal effects using the mediational formula:\n$$ P(Y=y \\mid do(X=x)) = \\sum_{m} P(Y=y \\mid do(M=m)) P(M=m \\mid do(X=x)) $$\nThis formula decomposes the problem into two sub-problems:\n1.  Identifying the causal effect of $X$ on $M$, given by $P(M=m \\mid do(X=x))$.\n2.  Identifying the causal effect of $M$ on $Y$, given by $P(Y=y \\mid do(M=m))$.\n\nFor the first term, $P(M=m \\mid do(X=x))$, the front-door criterion requires that there be no unblocked backdoor path from $X$ to $M$. The given structure with the confounder $U$ affecting only $X$ and $Y$ satisfies this. Therefore, the causal effect of $X$ on $M$ is equal to the observed conditional probability distribution:\n$$ P(M=m \\mid do(X=x)) = P(M=m \\mid X=x) $$\n\nFor the second term, $P(Y=y \\mid do(M=m))$, the relationship between $M$ and $Y$ is confounded by the backdoor path $M \\leftarrow X \\leftarrow U \\to Y$. The front-door criterion requires that this path can be blocked by conditioning on $X$. We can thus use the backdoor adjustment formula to identify the causal effect of $M$ on $Y$, using $X$ as the adjustment set:\n$$ P(Y=y \\mid do(M=m)) = \\sum_{x'} P(Y=y \\mid M=m, X=x') P(X=x') $$\n\nCombining these two results gives the front-door adjustment formula:\n$$ P(Y=y \\mid do(X=x)) = \\sum_{m} P(M=m \\mid X=x) \\left[ \\sum_{x'} P(Y=y \\mid M=m, X=x') P(X=x') \\right] $$\nWe have all the necessary observational probabilities from the problem statement to compute this quantity. The variables $y$, $m$, $x$, and $x'$ range over $\\{0,1\\}$.\n\nLet's first compute the inner bracket, which represents the causal effect of $M$ on $Y$, let's denote it $P^*(Y=1 \\mid M=m) = P(Y=1 \\mid do(M=m))$.\nThe given data are:\n- $P(X=1) = 0.5$, which implies $P(X=0) = 1 - 0.5 = 0.5$.\n- $P(Y=1 \\mid M=1, X) = 0.7$ for all $X$.\n- $P(Y=1 \\mid M=0, X) = 0.1$ for all $X$.\n\nFor $m=1$:\n$$ P^*(Y=1 \\mid M=1) = \\sum_{x' \\in \\{0,1\\}} P(Y=1 \\mid M=1, X=x') P(X=x') $$\n$$ P^*(Y=1 \\mid M=1) = P(Y=1 \\mid M=1, X=0)P(X=0) + P(Y=1 \\mid M=1, X=1)P(X=1) $$\n$$ P^*(Y=1 \\mid M=1) = (0.7)(0.5) + (0.7)(0.5) = 0.35 + 0.35 = 0.7 $$\n\nFor $m=0$:\n$$ P^*(Y=1 \\mid M=0) = \\sum_{x' \\in \\{0,1\\}} P(Y=1 \\mid M=0, X=x') P(X=x') $$\n$$ P^*(Y=1 \\mid M=0) = P(Y=1 \\mid M=0, X=0)P(X=0) + P(Y=1 \\mid M=0, X=1)P(X=1) $$\n$$ P^*(Y=1 \\mid M=0) = (0.1)(0.5) + (0.1)(0.5) = 0.05 + 0.05 = 0.1 $$\n\nNow we compute the quantity $E[Y \\mid do(X=x)] = P(Y=1 \\mid do(X=x))$ for $x=1$ and $x=0$.\n\nFor $x=1$:\n$$ P(Y=1 \\mid do(X=1)) = \\sum_{m \\in \\{0,1\\}} P(M=m \\mid X=1) P^*(Y=1 \\mid M=m) $$\n$$ P(Y=1 \\mid do(X=1)) = P(M=0 \\mid X=1)P^*(Y=1 \\mid M=0) + P(M=1 \\mid X=1)P^*(Y=1 \\mid M=1) $$\nGiven $P(M=1 \\mid X=1) = 0.8$, we have $P(M=0 \\mid X=1) = 1 - 0.8 = 0.2$.\n$$ P(Y=1 \\mid do(X=1)) = (0.2)(0.1) + (0.8)(0.7) = 0.02 + 0.56 = 0.58 $$\nSo, $E[Y \\mid do(X=1)] = 0.58$.\n\nFor $x=0$:\n$$ P(Y=1 \\mid do(X=0)) = \\sum_{m \\in \\{0,1\\}} P(M=m \\mid X=0) P^*(Y=1 \\mid M=m) $$\n$$ P(Y=1 \\mid do(X=0)) = P(M=0 \\mid X=0)P^*(Y=1 \\mid M=0) + P(M=1 \\mid X=0)P^*(Y=1 \\mid M=1) $$\nGiven $P(M=1 \\mid X=0) = 0.2$, we have $P(M=0 \\mid X=0) = 1 - 0.2 = 0.8$.\n$$ P(Y=1 \\mid do(X=0)) = (0.8)(0.1) + (0.2)(0.7) = 0.08 + 0.14 = 0.22 $$\nSo, $E[Y \\mid do(X=0)] = 0.22$.\n\nFinally, we compute the causal effect difference:\n$$ E[Y \\mid do(X=1)] - E[Y \\mid do(X=0)] = 0.58 - 0.22 = 0.36 $$\nThis value represents the average increase in the probability of the downstream transcriptional response when the receptor phosphorylation is forced from an inactive to an active state.",
            "answer": "$$\n\\boxed{0.36}\n$$"
        }
    ]
}