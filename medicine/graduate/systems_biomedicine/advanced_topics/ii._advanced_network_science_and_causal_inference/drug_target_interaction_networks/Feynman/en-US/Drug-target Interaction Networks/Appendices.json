{
    "hands_on_practices": [
        {
            "introduction": "The biological effect of a drug is not static; it evolves as the drug's concentration changes in the body. This practice connects the principles of pharmacokinetics (PK), which describe how drug concentration varies over time, with pharmacodynamics (PD), which relate concentration to target engagement. By deriving the time-dependent fractional occupancy for multiple targets from fundamental laws, you will build a dynamic view of how a drug's polypharmacology profile is realized over time, a foundational skill for quantitative systems pharmacology .",
            "id": "4336175",
            "problem": "In a tissue-level drug-target interaction network, a single small-molecule ligand $L$ engages three distinct target nodes $T_{1}$, $T_{2}$, and $T_{3}$ via independent edges $L + T_{i} \\rightleftharpoons LT_{i}$, for $i \\in \\{1,2,3\\}$. The system is studied under conditions where binding kinetics for each target are fast relative to the time variation of the ligand concentration and targets are non-depleting, such that the free ligand concentration equals the pharmacokinetic concentration profile. The pharmacokinetic concentration profile of the ligand is given by the biexponential function\n$$\n[L](t) = 1.2 \\times 10^{4}\\,\\exp(-0.3\\,t) + 3.0 \\times 10^{3}\\,\\exp(-0.02\\,t),\n$$\nwhere $t$ is time in $\\mathrm{h}$ and $[L](t)$ is in $\\mathrm{nM}$. The equilibrium dissociation constants for the three target edges are $K_{d,1} = 50\\,\\mathrm{nM}$, $K_{d,2} = 500\\,\\mathrm{nM}$, and $K_{d,3} = 5000\\,\\mathrm{nM}$.\n\nStarting strictly from the law of mass action for reversible binding, the definition of the equilibrium dissociation constant $K_{d,i}$, and the conservation of target mass, derive an expression for the fractional occupancy $\\theta_{i}(t)$ of each target edge in terms of $[L](t)$ and $K_{d,i}$. Then, using that expression, construct the weighted network occupancy\n$$\nO(t) = w_{1}\\,\\theta_{1}(t) + w_{2}\\,\\theta_{2}(t) + w_{3}\\,\\theta_{3}(t),\n$$\nwith edge weights $w_{1} = 0.5$, $w_{2} = 0.3$, and $w_{3} = 0.2$, and evaluate $O(t)$ at $t = 24\\,\\mathrm{h}$. Report $O(24\\,\\mathrm{h})$ as a unitless decimal fraction, rounded to four significant figures.",
            "solution": "The problem statement is evaluated as scientifically grounded, well-posed, objective, and complete. All provided data, conditions, and assumptions are physically and mathematically consistent, forming a standard problem in chemical kinetics and pharmacology. The problem is therefore deemed valid and a solution will be provided.\n\nThe first task is to derive the expression for the fractional occupancy, $\\theta_{i}(t)$, for each target edge. The analysis starts from the fundamental principles stipulated.\n\nFor each target $T_i$, the reversible binding of the ligand $L$ is described by the equilibrium:\n$$\nL + T_{i} \\rightleftharpoons LT_{i}\n$$\nwhere $i \\in \\{1,2,3\\}$.\n\nThe law of mass action applied at equilibrium defines the equilibrium dissociation constant, $K_{d,i}$:\n$$\nK_{d,i} = \\frac{[L][T_i]}{[LT_i]}\n$$\nIn this expression, $[L]$ is the concentration of the free ligand, $[T_i]$ is the concentration of the free target, and $[LT_i]$ is the concentration of the ligand-target complex.\n\nThe principle of conservation of mass for the target requires that the total concentration of target, $[T_i]_{\\text{total}}$, is the sum of the free and bound forms:\n$$\n[T_i]_{\\text{total}} = [T_i] + [LT_i]\n$$\nThe fractional occupancy, $\\theta_i$, is defined as the fraction of the total target population that is bound to the ligand:\n$$\n\\theta_i = \\frac{[LT_i]}{[T_i]_{\\text{total}}}\n$$\nSubstituting the expression for target conservation into the denominator gives:\n$$\n\\theta_i = \\frac{[LT_i]}{[T_i] + [LT_i]}\n$$\nTo express $\\theta_i$ in terms of $[L]$ and $K_{d,i}$, we first rearrange the definition of $K_{d,i}$ to solve for the free target concentration, $[T_i]$:\n$$\n[T_i] = K_{d,i} \\frac{[LT_i]}{[L]}\n$$\nNow, substitute this expression for $[T_i]$ into the denominator of the equation for $\\theta_i$:\n$$\n\\theta_i = \\frac{[LT_i]}{K_{d,i} \\frac{[LT_i]}{[L]} + [LT_i]}\n$$\nAssuming a non-trivial amount of binding ($[LT_i] > 0$), we can factor out $[LT_i]$ from the denominator and cancel it with the numerator:\n$$\n\\theta_i = \\frac{1}{\\frac{K_{d,i}}{[L]} + 1}\n$$\nMultiplying the numerator and denominator by $[L]$ yields the final, more common form, known as the Hill-Langmuir equation:\n$$\n\\theta_i = \\frac{[L]}{[L] + K_{d,i}}\n$$\nThe problem states that binding kinetics are fast and targets are non-depleting, which justifies using this equilibrium expression at any given time $t$ and equating the free ligand concentration $[L]$ with the given pharmacokinetic concentration profile $[L](t)$. Thus, the time-dependent fractional occupancy for each target is:\n$$\n\\theta_i(t) = \\frac{[L](t)}{[L](t) + K_{d,i}}\n$$\nThis completes the derivation.\n\nThe second task is to evaluate the weighted network occupancy, $O(t)$, at $t = 24\\,\\mathrm{h}$. First, we must calculate the ligand concentration $[L](t)$ at $t=24\\,\\mathrm{h}$.\nThe pharmacokinetic profile is given by:\n$$\n[L](t) = 1.2 \\times 10^{4}\\,\\exp(-0.3\\,t) + 3.0 \\times 10^{3}\\,\\exp(-0.02\\,t)\n$$\nSubstituting $t = 24\\,\\mathrm{h}$:\n$$\n[L](24) = 1.2 \\times 10^{4}\\,\\exp(-0.3 \\times 24) + 3.0 \\times 10^{3}\\,\\exp(-0.02 \\times 24)\n$$\n$$\n[L](24) = 1.2 \\times 10^{4}\\,\\exp(-7.2) + 3.0 \\times 10^{3}\\,\\exp(-0.48)\n$$\nEvaluating the exponential terms and calculating the concentration:\n$$\n[L](24) \\approx (1.2 \\times 10^{4})(7.4658 \\times 10^{-4}) + (3.0 \\times 10^{3})(0.61878)\n$$\n$$\n[L](24) \\approx 8.9590 + 1856.35\n$$\n$$\n[L](24) \\approx 1865.31\\,\\mathrm{nM}\n$$\nLet's denote this value as $L_{24}$. So, $L_{24} \\approx 1865.31\\,\\mathrm{nM}$.\n\nNext, we calculate the fractional occupancy for each target at $t=24\\,\\mathrm{h}$ using the derived formula and the given $K_d$ values: $K_{d,1} = 50\\,\\mathrm{nM}$, $K_{d,2} = 500\\,\\mathrm{nM}$, and $K_{d,3} = 5000\\,\\mathrm{nM}$.\nFor target $T_1$:\n$$\n\\theta_1(24) = \\frac{L_{24}}{L_{24} + K_{d,1}} = \\frac{1865.31}{1865.31 + 50} = \\frac{1865.31}{1915.31} \\approx 0.97389\n$$\nFor target $T_2$:\n$$\n\\theta_2(24) = \\frac{L_{24}}{L_{24} + K_{d,2}} = \\frac{1865.31}{1865.31 + 500} = \\frac{1865.31}{2365.31} \\approx 0.78861\n$$\nFor target $T_3$:\n$$\n\\theta_3(24) = \\frac{L_{24}}{L_{24} + K_{d,3}} = \\frac{1865.31}{1865.31 + 5000} = \\frac{1865.31}{6865.31} \\approx 0.27170\n$$\nFinally, we calculate the weighted network occupancy $O(24\\,\\mathrm{h})$ using the given weights $w_{1} = 0.5$, $w_{2} = 0.3$, and $w_{3} = 0.2$:\n$$\nO(t) = w_{1}\\,\\theta_{1}(t) + w_{2}\\,\\theta_{2}(t) + w_{3}\\,\\theta_{3}(t)\n$$\n$$\nO(24) = (0.5)\\,\\theta_{1}(24) + (0.3)\\,\\theta_{2}(24) + (0.2)\\,\\theta_{3}(24)\n$$\n$$\nO(24) \\approx (0.5)(0.97389) + (0.3)(0.78861) + (0.2)(0.27170)\n$$\n$$\nO(24) \\approx 0.486945 + 0.236583 + 0.054340\n$$\n$$\nO(24) \\approx 0.777868\n$$\nRounding the result to four significant figures, as requested:\n$$\nO(24\\,\\mathrm{h}) \\approx 0.7779\n$$",
            "answer": "$$\\boxed{0.7779}$$"
        },
        {
            "introduction": "Drug-target interaction data is often represented as a bipartite graph, but to understand functional relationships between targets, it is useful to project this into a unipartite target-target network. This exercise explores a critical artifact in this process: how promiscuous drugs can create spurious links, obscuring true biological relationships. You will derive a principled correction to down-weight these promiscuous binders, learning how to refine network representations to better reflect underlying biology .",
            "id": "4336203",
            "problem": "Consider a Drug–Target Interaction (DTI) network represented as a bipartite graph between a set of targets of size $n_T$ and a set of drugs of size $n_D$. Let $B \\in \\{0,1\\}^{n_T \\times n_D}$ be the bipartite adjacency matrix, where $B_{i d} = 1$ indicates that target $i$ is bound by drug $d$ and $B_{i d} = 0$ otherwise. Denote the target degree by $k_i = \\sum_{d=1}^{n_D} B_{i d}$ and the drug degree (promiscuity) by $k_d = \\sum_{i=1}^{n_T} B_{i d}$. The unweighted target–target projection $P_T$ has entries $P_{i j} = \\sum_{d=1}^{n_D} B_{i d} B_{j d}$ for $i \\neq j$, which count the number of drugs that co-bind targets $i$ and $j$. In systems biomedicine, it is recognized that highly promiscuous drugs (large $k_d$) can produce spurious target–target edges in $P_T$.\n\nStarting from a null interaction model in which the bipartite degree sequences $\\{k_i\\}$ and $\\{k_d\\}$ are preserved (the bipartite configuration model), and the pairing of target stubs to drug stubs is uniformly random subject to the degree constraints, derive from first principles an expression for the expected co-binding count of drugs shared by a specific target pair $(i,j)$, as a function of $\\{k_i\\}$, $\\{k_j\\}$, and $\\{k_d\\}$, and use this to state the condition under which an observed edge in $P_T$ is attributed to promiscuity rather than specific co-binding. Then, propose a weighted target–target projection of the form\n$$\nw_{i j}^{\\star} = \\sum_{d=1}^{n_D} B_{i d} B_{j d} \\, f(k_d),\n$$\nwhere $f(k_d)$ is a drug-specific weight that depends only on $k_d$, designed to down-weight contributions from promiscuous drugs while preserving the interpretable node strength of targets. Impose the normalization axiom that for every target $i$ the weighted strength in the target–target projection equals its bipartite degree, that is,\n$$\n\\sum_{\\substack{j=1 \\\\ j \\neq i}}^{n_T} w_{i j}^{\\star} = k_i,\n$$\nand argue why this conservation is biologically meaningful for downstream systems-level analyses. Under these requirements, determine the closed-form analytic expression of $f(k_d)$ for all integer $k_d \\geq 2$. Provide your final answer as the explicit formula for $f(k_d)$ only. No numerical rounding is required, and no physical units apply.",
            "solution": "The problem statement has been evaluated and is deemed valid. It is scientifically grounded within the field of systems biomedicine, mathematically well-posed, objective, and contains sufficient information to derive the requested expression.\n\nThe problem addresses the common issue of correcting for high-degree nodes in network projections, a critical step for meaningful analysis in drug-target interaction (DTI) networks. We are asked to derive a weighting function $f(k_d)$ that renormalizes a target-target projection.\n\nFirst, to understand why a correction for drug promiscuity ($k_d$) is necessary, we derive the expected number of co-binding drugs for a target pair $(i, j)$ under the specified null model (bipartite configuration model). In this model, the $L = \\sum_i k_i = \\sum_d k_d$ total edges (stubs) are randomly paired. The probability that an edge exists between target $i$ and drug $d$ is approximately $\\mathbb{E}[B_{id}] = \\frac{k_i k_d}{L}$. The expected value of the projection $P_{ij} = \\sum_d B_{id} B_{jd}$ is, by linearity of expectation, $\\mathbb{E}[P_{ij}] = \\sum_d \\mathbb{E}[B_{id} B_{jd}]$. Assuming interactions are approximately independent events for a given drug $d$ with different targets $i$ and $j$ in the limit of a large, sparse network, $\\mathbb{E}[B_{id} B_{jd}] \\approx \\mathbb{E}[B_{id}] \\mathbb{E}[B_{jd}] = (\\frac{k_i k_d}{L})(\\frac{k_j k_d}{L})$. A more careful derivation considering that stubs are used up yields $\\mathbb{E}[B_{id}B_{jd}] \\approx \\frac{k_i k_j k_d(k_d-1)}{L(L-1)}$. Summing over all drugs gives the expected co-binding count:\n$$\n\\mathbb{E}[P_{ij}] \\approx \\frac{k_i k_j}{L(L-1)} \\sum_{d=1}^{n_D} k_d (k_d-1)\n$$\nThis expression demonstrates that drugs with high promiscuity (large $k_d$) contribute quadratically to the expected random co-binding count. Consequently, an observed edge in the unweighted projection $P_T$ (i.e., $P_{ij} > 0$) is attributed to promiscuity artefacts rather than specific co-binding if the observed count $P_{ij}$ is not significantly larger than this random expectation $\\mathbb{E}[P_{ij}]$. The purpose of the weighting function $f(k_d)$ is to counteract this bias.\n\nWe are tasked with finding the form of $f(k_d)$ for the weighted projection $w_{i j}^{\\star} = \\sum_{d=1}^{n_D} B_{i d} B_{j d} \\, f(k_d)$, subject to the normalization axiom that the weighted strength of each target $i$ in the projection equals its degree in the original bipartite graph:\n$$\n\\sum_{\\substack{j=1 \\\\ j \\neq i}}^{n_T} w_{i j}^{\\star} = k_i\n$$\nThis conservation axiom is biologically meaningful because it ensures that the overall connectivity of a target, a primary measure of its importance ($k_i$, the number of drugs that bind it), is preserved in the projected network. This allows for more faithful downstream analyses (e.g., centrality measures, module detection) on the target-target network, as the node strength directly reflects the original, interpretable biological information.\n\nTo derive $f(k_d)$, we substitute the definition of $w_{i j}^{\\star}$ into the axiom:\n$$\n\\sum_{\\substack{j=1 \\\\ j \\neq i}}^{n_T} \\left( \\sum_{d=1}^{n_D} B_{i d} B_{j d} \\, f(k_d) \\right) = k_i\n$$\nWe can interchange the order of summation, as the sums are finite:\n$$\n\\sum_{d=1}^{n_D} \\sum_{\\substack{j=1 \\\\ j \\neq i}}^{n_T} B_{i d} B_{j d} \\, f(k_d) = k_i\n$$\nThe terms $B_{id}$ and $f(k_d)$ do not depend on the index $j$, so they can be factored out of the inner sum:\n$$\n\\sum_{d=1}^{n_D} B_{i d} \\, f(k_d) \\left( \\sum_{\\substack{j=1 \\\\ j \\neq i}}^{n_T} B_{j d} \\right) = k_i\n$$\nThe inner sum, $\\sum_{j=1, j \\neq i}^{n_T} B_{j d}$, represents the total number of targets that drug $d$ binds to, excluding target $i$. This can be expressed in terms of the drug's total degree, $k_d = \\sum_{j=1}^{n_T} B_{j d}$. The sum is thus equal to $k_d - B_{id}$, where $B_{id}$ subtracts the potential interaction with target $i$ itself. Substituting this back gives:\n$$\n\\sum_{d=1}^{n_D} B_{i d} \\, f(k_d) (k_d - B_{i d}) = k_i\n$$\nThe adjacency matrix entry $B_{id}$ is either $1$ or $0$.\nIf $B_{id} = 0$, the drug $d$ does not bind target $i$, and the entire term $B_{id} \\, f(k_d) (k_d - B_{id})$ is $0$.\nIf $B_{id} = 1$, the drug $d$ binds target $i$, and the term becomes $1 \\cdot f(k_d) (k_d - 1) = f(k_d)(k_d - 1)$.\nTherefore, the summation is non-zero only for those drugs that actually bind to target $i$:\n$$\n\\sum_{d \\text{ s.t. } B_{id}=1} f(k_d) (k_d - 1) = k_i\n$$\nThe right-hand side, $k_i$, is the degree of target $i$, which by definition is the count of drugs that bind to it: $k_i = \\sum_{d=1}^{n_D} B_{i d} = \\sum_{d \\text{ s.t. } B_{id}=1} 1$. So we have the equality:\n$$\n\\sum_{d \\text{ s.t. } B_{id}=1} f(k_d) (k_d - 1) = \\sum_{d \\text{ s.t. } B_{id}=1} 1\n$$\nThis equation must hold true for any target $i$, and thus for any arbitrary set of drugs that might bind to it. This structural requirement imposes a strong constraint. For the equality to hold universally, regardless of the specific combination of drugs {d s.t. $B_{id}=1$}, the corresponding terms in the sums on both sides must be equal. This implies that for any drug $d$ involved in an interaction:\n$$\nf(k_d) (k_d - 1) = 1\n$$\nSolving for $f(k_d)$, we obtain the analytic expression for the weighting function. The problem asks for this for integers $k_d \\geq 2$. For $k_d=1$, a drug cannot mediate a co-binding link, making $f(1)$ irrelevant (our derivation correctly gives a factor of $k_d-B_{id} = 1-1=0$ for such a case). For $k_d \\geq 2$, the expression is well-defined:\n$$\nf(k_d) = \\frac{1}{k_d - 1}\n$$\nThis function appropriately down-weights contributions from promiscuous drugs, as $f(k_d) \\to 0$ for large $k_d$.",
            "answer": "$$\n\\boxed{\\frac{1}{k_d - 1}}\n$$"
        },
        {
            "introduction": "Predicting novel drug-target interactions is a central challenge in drug discovery, and rigorous model evaluation is paramount to ensure that predictions are genuinely useful. This problem delves into the subtleties of cross-validation for DTI models that leverage drug and target similarity, a context ripe for \"information leakage\" that can inflate performance metrics. By critically assessing different evaluation schemes, you will learn to design experiments that provide a true measure of a model's ability to generalize to new chemical scaffolds and protein families .",
            "id": "4336205",
            "problem": "Consider a drug-target interaction network represented as a bipartite graph with drug set $\\mathcal{D}$, target set $\\mathcal{T}$, and a binary interaction matrix $Y \\in \\{0,1\\}^{|\\mathcal{D}| \\times |\\mathcal{T}|}$, where $Y_{ij} = 1$ indicates an observed interaction between drug $i \\in \\mathcal{D}$ and target $j \\in \\mathcal{T}$. Let $S_{\\mathcal{D}} \\in \\mathbb{R}^{|\\mathcal{D}| \\times |\\mathcal{D}|}$ and $S_{\\mathcal{T}} \\in \\mathbb{R}^{|\\mathcal{T}| \\times |\\mathcal{T}|}$ denote similarity matrices among drugs and among targets, respectively, constructed from domain-appropriate features (for example, chemical fingerprints for drugs and sequence similarity for proteins). Many models for drug-target interaction prediction employ graph-based regularization that couples parameters across similar drugs and targets. A prototypical objective takes the form\n$$\n\\min_{\\Theta} \\sum_{(i,j) \\in \\Omega_{\\text{train}}} \\ell\\big(f_{\\Theta}(i,j), Y_{ij}\\big) + \\lambda_{\\mathcal{D}} \\operatorname{tr}\\big(F^{\\top} L_{\\mathcal{D}} F\\big) + \\lambda_{\\mathcal{T}} \\operatorname{tr}\\big(G^{\\top} L_{\\mathcal{T}} G\\big),\n$$\nwhere $\\ell(\\cdot,\\cdot)$ is a pointwise loss, $f_{\\Theta}(i,j)$ is the model’s score for the pair $(i,j)$ parameterized by $\\Theta$, $F \\in \\mathbb{R}^{|\\mathcal{D}| \\times p}$ and $G \\in \\mathbb{R}^{|\\mathcal{T}| \\times q}$ are drug and target representations indexed by drugs and targets, and $L_{\\mathcal{D}}$ and $L_{\\mathcal{T}}$ are graph Laplacians constructed from $S_{\\mathcal{D}}$ and $S_{\\mathcal{T}}$, respectively. Assume hyperparameters, including $\\lambda_{\\mathcal{D}}$ and $\\lambda_{\\mathcal{T}}$, are selected by Cross-Validation (CV).\n\nYou are tasked with selecting an evaluation scheme that estimates generalization to novel chemical scaffolds and novel target families without information leakage due to similarity-based regularization or kernels. Let $\\tau \\in (0,1)$ be a fixed similarity threshold used to define groups of similar entities by connecting pairs $(i,i')$ with $S_{\\mathcal{D}}(i,i') \\ge \\tau$ and pairs $(j,j')$ with $S_{\\mathcal{T}}(j,j') \\ge \\tau$, and then taking connected components.\n\nWhich of the following CV designs most rigorously avoids leakage across similar drugs or targets when evaluating such regularized models, while providing a valid nested procedure for hyperparameter selection? Choose the best option.\n\nA. Perform random edge-wise $K$-fold CV by partitioning $\\Omega = \\{(i,j) : Y_{ij} = 1\\}$ and an equal number of randomly sampled non-interactions into $K$ folds stratified by drug and target degrees. For each outer fold, train on the remaining edges, retain all drugs and targets in the training objective and regularizers (including all of $L_{\\mathcal{D}}$ and $L_{\\mathcal{T}}$), tune $\\lambda_{\\mathcal{D}}$ and $\\lambda_{\\mathcal{T}}$ by inner edge-wise CV within the training edges, and evaluate on the held-out edges.\n\nB. Construct drug similarity groups using threshold $\\tau$ on $S_{\\mathcal{D}}$ and assign entire drug groups to $K$ outer folds (Leave-Drug-Group-Out). For each outer fold, remove all drugs in the held-out drug groups from training, but keep all targets and the full target Laplacian $L_{\\mathcal{T}}$ intact. Tune hyperparameters by inner CV on the remaining edges involving the retained drugs and all targets. Evaluate on all pairs between held-out drug groups and all targets.\n\nC. Construct drug groups and target groups by thresholding $S_{\\mathcal{D}}$ and $S_{\\mathcal{T}}$ at $\\tau$ and taking connected components on each side. In the outer loop, select disjoint sets of drug groups and target groups as the test block. Define the outer test set as all pairs between the held-out drug groups and the held-out target groups, and completely exclude all test drugs and all test targets from the training objective and from any regularization graphs or kernels. Build training-only Laplacians $L_{\\mathcal{D}}^{\\text{train}}$ and $L_{\\mathcal{T}}^{\\text{train}}$ on the remaining drugs and targets. In the inner loop, perform group-wise CV exclusively within the training drugs and training targets to tune hyperparameters. Evaluate only on the outer test block. Pairs involving entities similar at level $\\tau$ to any test drug or target are absent from training by construction.\n\nD. Partition the drug set into chemical scaffolds and the target set into families at threshold $\\tau$, then create a grid of matrix blocks by crossing drug partitions and target partitions. In each outer fold, hold out one random block of pairs but keep all other blocks, including all drugs and targets, in the training objective and regularizers; further, precompute drug and target embeddings using the full bipartite graph (including the held-out block) and keep them fixed during training. Tune hyperparameters by edge-wise inner CV on the training edges and evaluate on the held-out block.",
            "solution": "The problem asks for the selection of the most rigorous cross-validation (CV) scheme to evaluate a drug-target interaction (DTI) prediction model's ability to generalize to novel chemical scaffolds and novel target families. The model class is specified by an objective function that includes graph-based regularization terms, which explicitly use similarity information among drugs and among targets. The key challenge is to prevent \"information leakage,\" where information about the test set inadvertently influences the training process, leading to an overestimation of performance. A rigorous evaluation of generalization to truly novel entities requires that the test entities (and entities similar to them) are completely isolated from the model's training and hyperparameter selection phases.\n\nLet us analyze the given model objective function:\n$$\n\\min_{\\Theta} \\sum_{(i,j) \\in \\Omega_{\\text{train}}} \\ell\\big(f_{\\Theta}(i,j), Y_{ij}\\big) + \\lambda_{\\mathcal{D}} \\operatorname{tr}\\big(F^{\\top} L_{\\mathcal{D}} F\\big) + \\lambda_{\\mathcal{T}} \\operatorname{tr}\\big(G^{\\top} L_{\\mathcal{T}} G\\big)\n$$\nThe regularization terms are crucial. The term $\\lambda_{\\mathcal{D}} \\operatorname{tr}(F^{\\top} L_{\\mathcal{D}} F)$ is a graph Laplacian regularizer. If we let $W_{\\mathcal{D}}$ be the weighted adjacency matrix derived from the similarity matrix $S_{\\mathcal{D}}$, the unnormalized graph Laplacian is $L_{\\mathcal{D}} = D_{\\mathcal{D}} - W_{\\mathcal{D}}$, where $D_{\\mathcal{D}}$ is the diagonal degree matrix. The regularizer can be written as:\n$$\n\\operatorname{tr}(F^{\\top} L_{\\mathcal{D}} F) = \\sum_{i,k \\in \\mathcal{D}} (W_{\\mathcal{D}})_{ik} \\| F_{i,:} - F_{k,:} \\|_2^2\n$$\nwhere $F_{i,:}$ is the $p$-dimensional representation vector for drug $i$. This term enforces that similar drugs (i.e., those with a large weight $(W_{\\mathcal{D}})_{ik}$) have similar representations. An analogous interpretation holds for the target regularization term.\n\nInformation leakage occurs if any information about the test set is used during training. In this context, this includes:\n1.  Using test interactions $(i,j)$ for training.\n2.  Using a test drug $i$ or a test target $j$ in other training interactions.\n3.  Using a test drug $i$ or test target $j$ (or any drug/target similar to them) in the construction of the Laplacians $L_{\\mathcal{D}}$ and $L_{\\mathcal{T}}$ that are then used in the training objective.\n4. Tuning hyperparameters ($\\lambda_{\\mathcal{D}}, \\lambda_{\\mathcal{T}}$) using a CV scheme that itself leaks information about the outer-loop test set.\n\nThe goal is to estimate generalization to *novel* chemical scaffolds and *novel* target families. This is a \"cold start\" prediction scenario where both the drug and the target in a test pair are new and dissimilar to entities seen during training. Therefore, the CV scheme must separate the data into folds based on groups of similar drugs and groups of similar targets.\n\nLet's evaluate each option based on these principles.\n\n**Evaluation of Option A**\nThis option describes random edge-wise $K$-fold CV. The set of known interactions (and some non-interactions) is randomly partitioned into $K$ folds. This approach is standard for many machine learning tasks, but it is inappropriate here.\n-   **Leakage Source 1:** A given drug $i$ and target $j$ will almost certainly appear in the training set, just in different pairings. For example, if $(d_1, t_1)$ is in the test fold, $(d_1, t_2)$ and $(d_3, t_1)$ could be in the training folds. The model thus learns representations for $d_1$ and $t_1$ directly. This setup evaluates generalization to new *interactions* among known entities, not generalization to *novel* entities.\n-   **Leakage Source 2:** It explicitly states that the full Laplacians $L_{\\mathcal{D}}$ and $L_{\\mathcal{T}}$ are used. This means that if a drug $d_{\\text{test}}$ is part of a held-out interaction, the regularization term still connects it to all other drugs $d_{\\text{train}}$ via the similarity graph. The model can thus infer properties of $d_{\\text{test}}$ via its similarity to training drugs. This is a severe form of information leakage.\n-   **Conclusion:** This scheme is highly optimistic and does not measure generalization to novel scaffolds or families. It is fundamentally flawed for the stated goal.\n\n**Final Verdict for A: Incorrect.**\n\n**Evaluation of Option B**\nThis option proposes a \"Leave-Drug-Group-Out\" CV. This is an improvement, as it attempts to test generalization to novel drugs by holding out entire groups of similar drugs.\n-   **Partial Segregation:** It correctly isolates held-out drug groups. This tests the \"cold drug\" scenario.\n-   **Leakage Source 1:** The scheme fails to do the same for targets. It states that \"all targets and the full target Laplacian $L_{\\mathcal{T}}$\" are kept intact. This means that for a test pair $(d_{\\text{test}}, t_{\\text{test}})$, the target $t_{\\text{test}}$ is known to the model from its interactions with training drugs. This does not evaluate generalization to novel target families.\n-   **Leakage Source 2:** Using the full $L_{\\mathcal{T}}$ means information about the similarity relationships among all targets, including those that might only appear in the test set, is used during training. This constitutes leakage.\n-   **Nested CV:** The inner CV is \"on the remaining edges,\" which suggests an edge-wise split. This would tune hyperparameters to be optimal for predicting new interactions of known entities, not for generalizing to new groups, creating a mismatch between the inner and outer CV goals.\n-   **Conclusion:** This scheme only addresses half of the problem (novel drugs) and does so imperfectly due to leakage on the target side. It does not meet the requirement of evaluating generalization to *both* novel scaffolds and families.\n\n**Final Verdict for B: Incorrect.**\n\n**Evaluation of Option C**\nThis option proposes a block-wise CV scheme based on both drug and target groups.\n-   **Full Segregation:** It holds out disjoint sets of *drug groups* and *target groups* simultaneously. The test set consists of pairs between these held-out groups. This structure directly simulates the desired \"cold start\" scenario for both drugs and targets.\n-   **No Leakage:** It explicitly states that \"all test drugs and all test targets\" are \"completely exclude[d]...from the training objective and from any regularization graphs or kernels.\" This is implemented by building \"training-only Laplacians $L_{\\mathcal{D}}^{\\text{train}}$ and $L_{\\mathcal{T}}^{\\text{train}}$\". This is the critical step to prevent similarity-based information leakage, ensuring the training process is blind to the test entities and their topological neighborhood in the similarity graph.\n-   **Consistent Nested CV:** The inner loop for hyperparameter tuning is specified as \"group-wise CV exclusively within the training drugs and training targets.\" This consistency is crucial. The hyperparameters are thus selected to optimize for the same type of generalization task (to new groups) that the outer loop is designed to evaluate.\n-   **Conclusion:** This scheme is meticulously designed to prevent all relevant forms of information leakage. It correctly structures the outer test fold, the training data, the construction of the regularizers, and the nested hyperparameter search to provide a rigorous and valid estimate of a model's performance on genuinely novel drugs and targets.\n\n**Final Verdict for C: Correct.**\n\n**Evaluation of Option D**\nThis option describes holding out a random block of pairs but contains multiple, severe design flaws.\n-   **Leakage Source 1:** It explicitly states that \"all drugs and targets\" are kept in the training objective and regularizers. If the block of pairs between drug group $D_k$ and target group $T_l$ is held out, drugs in $D_k$ are still trained on their interactions with targets *not* in $T_l$. This means the model learns a representation for the \"test\" drugs from training data.\n-   **Leakage Source 2 (Egregious):** It mandates to \"precompute drug and target embeddings using the full bipartite graph (including the held-out block)\". This means the features used for training are generated using the test data itself. This is a direct and catastrophic form of information leakage that invalidates the entire evaluation.\n-   **Conclusion:** This scheme is fundamentally unsound. Despite its block-wise structure, it institutionalizes massive information leakage, making any resulting performance estimate completely unreliable and meaningless for assessing generalization.\n\n**Final Verdict for D: Incorrect.**\n\nIn summary, Option C is the only one that describes a methodologically sound and rigorous procedure for the stated task. It demonstrates a sophisticated understanding of the subtleties of cross-validation in settings with non-independent data and explicit similarity-based models.",
            "answer": "$$\\boxed{C}$$"
        }
    ]
}