{
    "hands_on_practices": [
        {
            "introduction": "This exercise forms the bedrock of constraint-based causal discovery, challenging you to construct a causal graph from first principles. By systematically applying the concepts of d-separation and faithfulness to observational and interventional data, you will manually replicate the logic of algorithms like the PC algorithm to derive the correct Markov equivalence class of causal structures. This practice is essential for building intuition on how statistical constraints shape our understanding of causal systems. ",
            "id": "4322795",
            "problem": "A systems biomedicine team studies a cytokine signaling module involving six molecular variables measured at steady state under both observational and perfectly interventional regimes. Let the variables be $X_J$ (Janus kinase activity, JAK), $X_S$ (Signal Transducer and Activator of Transcription 3 phosphorylation, STAT3), $X_C$ (Suppressor of Cytokine Signaling 3 expression, SOCS3), $X_L$ (interleukin-6 concentration, IL-6), $X_K$ (nuclear factor kappa-light-chain-enhancer of activated B cells activation, $NF\\text{-}\\kappa B$), and $X_T$ (tumor necrosis factor concentration, TNF). Assume causal sufficiency, acyclicity, and that the observational joint distribution is faithful to the underlying causal Directed Acyclic Graph (DAG). The team estimates the following minimal conditional independence constraints from large-sample observational data (conditioning sets shown are minimal and no other conditional independences hold given any subset of the remaining variables):\n- $X_J \\perp X_L \\mid \\emptyset$\n- $X_J \\perp X_K \\mid \\emptyset$\n- $X_J \\perp X_T \\mid \\emptyset$\n- $X_J \\perp X_C \\mid \\{X_S\\}$\n- $X_L \\perp X_C \\mid \\{X_S\\}$\n- $X_K \\perp X_S \\mid \\{X_L\\}$\n- $X_T \\perp X_S \\mid \\{X_L\\}$\n- $X_K \\perp X_C \\mid \\{X_L, X_S\\}$\n- $X_T \\perp X_C \\mid \\{X_L, X_S\\}$\n\nThe team also performs perfect interventions in the sense of Pearlâ€™s $\\operatorname{do}(\\cdot)$ operator:\n- Under $\\operatorname{do}(X_J=x)$, only the distributions of $X_S$ and $X_C$ change; the distributions of $X_L$, $X_K$, and $X_T$ remain invariant.\n- Under $\\operatorname{do}(X_S=s)$, only the distribution of $X_C$ changes; the distributions of $X_J$, $X_L$, $X_K$, and $X_T$ remain invariant.\n- No interventions are performed on $X_L$, $X_K$, or $X_T$.\n\nTask: Using only fundamental definitions of d-separation, the Causal Markov condition, faithfulness, and the semantics of perfect interventions that sever incoming arrows to the intervened node, first construct the Completed Partially Directed Acyclic Graph (CPDAG) representing the Markov equivalence class consistent with the above constraints. Then, using the definition of a Markov equivalence class and properties of chain components in a CPDAG, determine the number of distinct DAGs in this equivalence class. Provide your final answer as a single exact integer. No rounding is required and no units are needed.",
            "solution": "The user wants me to solve a problem in causal discovery.\n\n### Step 1: Extract Givens\n**Variables:** The set of variables is $\\mathcal{V} = \\{X_J, X_S, X_C, X_L, X_K, X_T\\}$, representing $6$ molecular components.\n**Assumptions:**\n1.  Causal Sufficiency: There are no unmeasured common causes.\n2.  Acyclicity: The causal graph is a Directed Acyclic Graph (DAG).\n3.  Faithfulness: The observational joint distribution is faithful to the underlying causal DAG. This means all conditional independences in the distribution are entailed by the d-separation properties of the DAG.\n\n**Observational Data (Conditional Independence Constraints):**\nThe provided list of minimal conditional independences (CIs) is exhaustive.\n1.  $X_J \\perp X_L \\mid \\emptyset$\n2.  $X_J \\perp X_K \\mid \\emptyset$\n3.  $X_J \\perp X_T \\mid \\emptyset$\n4.  $X_J \\perp X_C \\mid \\{X_S\\}$\n5.  $X_L \\perp X_C \\mid \\{X_S\\}$\n6.  $X_K \\perp X_S \\mid \\{X_L\\}$\n7.  $X_T \\perp X_S \\mid \\{X_L\\}$\n8.  $X_K \\perp X_C \\mid \\{X_L, X_S\\}$\n9.  $X_T \\perp X_C \\mid \\{X_L, X_S\\}$\n\n**Interventional Data:**\n1.  Under $\\operatorname{do}(X_J=x)$: distributions of $X_S$ and $X_C$ change; distributions of $X_L, X_K, X_T$ are invariant.\n2.  Under $\\operatorname{do}(X_S=s)$: distribution of $X_C$ changes; distributions of $X_J, X_L, X_K, X_T$ are invariant.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded in the fields of causal inference and systems biology, utilizing standard and well-defined concepts such as DAGs, d-separation, faithfulness, and Pearl's $\\operatorname{do}$-calculus. The problem is objective, providing precise constraints. It is self-contained and does not appear to have internal contradictions. The data provided (both observational and interventional) are formal constraints used to infer graphical structure. The problem is well-posed, leading to a unique Markov equivalence class of causal graphs. The problem is not trivial and requires a systematic application of causal discovery algorithms. It does not violate any of the invalidity criteria.\n\n### Step 3: Verdict and Action\nThe problem is deemed valid. A complete solution will be provided.\n\n### Solution\n\nThe task is two-fold: first, to construct the Completed Partially Directed Acyclic Graph (CPDAG) representing the Markov equivalence class of DAGs consistent with the given data, and second, to determine the number of distinct DAGs in this class.\n\n#### Part 1: Constructing the Causal Graph from Observational Data\n\nWe use a constraint-based approach, akin to the PC algorithm, which consists of three main stages: skeleton discovery, v-structure orientation, and orientation propagation.\n\n**1. Skeleton Discovery:**\nThe skeleton of the causal graph is an undirected graph where an edge exists between two variables if and only if they are not conditionally independent given any subset of the other variables. The problem states that the given $9$ CIs are the only ones that hold. Therefore, an edge exists between any pair of variables not in this list. The total number of pairs of variables is $\\binom{6}{2} = 15$. The $9$ non-adjacent pairs are given. The remaining $15 - 9 = 6$ pairs must be adjacent.\n\nThe non-adjacent pairs (no edge) are:\n$(X_J, X_L), (X_J, X_K), (X_J, X_T), (X_J, X_C), (X_L, X_C), (X_K, X_S), (X_T, X_S), (X_K, X_C), (X_T, X_C)$.\n\nThe adjacent pairs (with an edge in the skeleton) are:\n1.  $X_J - X_S$\n2.  $X_S - X_C$\n3.  $X_S - X_L$\n4.  $X_L - X_K$\n5.  $X_L - X_T$\n6.  $X_K - X_T$\n\nThe skeleton is thus formed by these $6$ edges.\n\n**2. V-Structure Orientation:**\nWe identify unshielded colliders (v-structures). An unshielded triple is a set of three nodes $X-Y-Z$ where $X$ and $Z$ are not adjacent. Such a triple is oriented as a v-structure $X \\to Y \\leftarrow Z$ if the intermediate node $Y$ is NOT in the minimal separating set $S_{XZ}$ that makes $X$ and $Z$ conditionally independent.\n\nLet's examine all unshielded triples in the skeleton:\n-   $X_J-X_S-X_L$: The nodes $X_J$ and $X_L$ are not adjacent. The CI is $X_J \\perp X_L \\mid \\emptyset$. The separating set is $S_{JL} = \\emptyset$. The intermediate node $X_S$ is not in $S_{JL}$. Therefore, this is a v-structure: $X_J \\to X_S \\leftarrow X_L$.\n-   $X_J-X_S-X_C$: The nodes $X_J$ and $X_C$ are not adjacent. The CI is $X_J \\perp X_C \\mid \\{X_S\\}$. The separating set is $S_{JC} = \\{X_S\\}$. The intermediate node $X_S$ is in $S_{JC}$. This is not a v-structure.\n-   $X_C-X_S-X_L$: The nodes $X_C$ and $X_L$ are not adjacent. The CI is $X_L \\perp X_C \\mid \\{X_S\\}$. The separating set is $S_{LC} = \\{X_S\\}$. The intermediate node $X_S$ is in $S_{LC}$. This is not a v-structure.\n-   $X_S-X_L-X_K$: The nodes $X_S$ and $X_K$ are not adjacent. The CI is $X_K \\perp X_S \\mid \\{X_L\\}$. The separating set is $S_{SK} = \\{X_L\\}$. The intermediate node $X_L$ is in $S_{SK}$. This is not a v-structure.\n-   $X_S-X_L-X_T$: The nodes $X_S$ and $X_T$ are not adjacent. The CI is $X_T \\perp X_S \\mid \\{X_L\\}$. The separating set is $S_{ST} = \\{X_L\\}$. The intermediate node $X_L$ is in $S_{ST}$. This is not a v-structure.\n\nThe only v-structure identified is $X_J \\to X_S \\leftarrow X_L$.\n\n**3. Orientation Propagation (Meek's Rules):**\nWe apply orientation rules to the partially directed graph to orient more edges, ensuring no new v-structures or cycles are created.\n-   **Rule 1 (Avoid new v-structures):** If we have $A \\to B - C$ and $A, C$ are not adjacent, we must orient the edge as $B \\to C$.\n    -   We have the structure $X_J \\to X_S - X_C$. The nodes $X_J$ and $X_C$ are not adjacent. Applying this rule, we must orient $X_S-X_C$ as $X_S \\to X_C$. If we were to orient it as $X_S \\leftarrow X_C$, we would create a new v-structure $X_J \\to X_S \\leftarrow X_C$, which would imply $X_J \\perp X_C \\mid \\emptyset$ by faithfulness. This contradicts the given CI $X_J \\perp X_C \\mid \\{X_S\\}$.\n-   Repeated application of this and other Meek's rules finds no further compelled orientations based on the observational data. For example, considering a non-v-structure triple like $X_S-X_L-X_K$, we know $X_S \\leftarrow X_L$ (from the v-structure). The triple is either a chain $X_S \\leftarrow X_L \\leftarrow X_K$ or a fork $X_S \\leftarrow X_L \\to X_K$. Both are consistent with $X_L$ being in the separating set $S_{SK}$, so the edge $X_L-X_K$ cannot be oriented from this information alone.\n\nThe CPDAG derived from observational data is:\n-   Directed edges: $X_J \\to X_S$, $X_L \\to X_S$, $X_S \\to X_C$.\n-   Undirected edges: $X_L - X_K$, $X_L - X_T$, $X_K - X_T$. These form a clique on the vertex set $\\{X_L, X_K, X_T\\}$.\n\n#### Part 2: Incorporating Interventional Data\n\nThe effect of a perfect intervention $\\operatorname{do}(X=x)$ is to remove all incoming edges to $X$ in the true DAG. Consequently, only $X$ and its descendants can change their distribution.\n-   $\\operatorname{do}(X_J=x)$: $X_S$ and $X_C$ change, while $X_L, X_K, X_T$ are invariant. This implies that in the true DAG, $\\{X_S, X_C\\}$ must be descendants of $X_J$, and $\\{X_L, X_K, X_T\\}$ must not be. Our current CPDAG has the directed path $X_J \\to X_S \\to X_C$, which is consistent. It also shows no directed path from $X_J$ to any of $\\{X_L, X_K, X_T\\}$. This intervention confirms the orientations $X_J \\to X_S$ and $X_S \\to X_C$ but adds no new information to orient the remaining edges.\n-   $\\operatorname{do}(X_S=s)$: $X_C$ changes, while $X_J, X_L, X_K, X_T$ are invariant. This implies $X_C$ is a descendant of $X_S$, and $\\{X_J, X_L, X_K, X_T\\}$ are not.\n    -   The orientation $X_S \\to X_C$ is consistent.\n    -   $X_J$ and $X_L$ are parents of $X_S$ in our CPDAG ($X_J \\to X_S, X_L \\to S$), so they are ancestors, not descendants. This is consistent.\n    -   $X_K$ and $X_T$ must not be descendants of $X_S$. For a node in $\\{X_K, X_T\\}$ to be a descendant of $X_S$, there must be a directed path $X_S \\to \\dots \\to X_K$ (or $X_T$). Such a path would have to pass through $X_L$. However, the edge between $X_L$ and $X_S$ is a compelled edge $X_L \\to X_S$ in the CPDAG. Any DAG in the equivalence class must contain this edge. A directed path from $X_S$ to $X_L$ would create a cycle ($X_S \\to \\dots \\to X_L \\to X_S$), which is forbidden. Therefore, no directed path from $X_S$ to any node in $\\{X_L, X_K, X_T\\}$ can exist. This is already an entailment of the observational CPDAG.\n\nThe interventional data are consistent with the CPDAG derived from observational data but do not provide additional constraints to orient the remaining undirected edges. The final CPDAG is unchanged.\n\n#### Part 3: Counting the DAGs in the Equivalence Class\n\nThe final CPDAG has the following structure:\n-   Directed edges: $X_J \\to X_S$, $X_L \\to X_S$, $X_S \\to X_C$.\n-   An undirected clique on the vertices $\\{X_L, X_K, X_T\\}$.\n\nThis CPDAG represents a Markov equivalence class of DAGs. All DAGs in this class must contain the three directed edges. The differences between them lie in the orientation of the undirected edges. The set of nodes $\\{X_L, X_K, X_T\\}$ and the undirected edges between them form a single chain component.\n\nTo find the number of DAGs in the class, we must count the number of ways to orient the edges of this chain component without creating cycles or new v-structures.\n1.  **Cycles:** Orienting the edges within the $\\{X_L, X_K, X_T\\}$ clique can only create cycles within that clique. No cycle can be formed with the rest of the graph, as the only connection is $X_L \\to X_S$, and there is no path from $\\{J, S, C\\}$ back to $\\{L, K, T\\}$. Thus, any acyclic orientation of the clique is globally acyclic.\n2.  **New v-structures:** A new v-structure $A \\to B \\leftarrow C$ requires $A$ and $C$ to be non-adjacent. If we orient edges within the clique $\\{X_L, X_K, X_T\\}$, any potential collider $B \\in \\{X_L, X_K, X_T\\}$ would have parents $A, C$ that are also in the clique. But all nodes in a clique are adjacent to each other. Therefore, any $A \\to B \\leftarrow C$ structure formed would be a shielded collider, not a v-structure.\n\nThus, the number of DAGs in the equivalence class is simply the number of ways to orient the edges of the undirected clique $\\{X_L, X_K, X_T\\}$ to form a DAG. A complete undirected graph on $k$ vertices (a clique) can be oriented into a DAG in $k!$ ways, corresponding to the $k!$ possible topological orderings of the vertices.\n\nFor the clique on $\\{X_L, X_K, X_T\\}$, we have $k=3$ vertices.\nThe number of distinct DAGs is $k! = 3! = 3 \\times 2 \\times 1 = 6$.",
            "answer": "$$\\boxed{6}$$"
        },
        {
            "introduction": "While many introductory examples assume all common causes are measured, real biological systems are rife with latent confounders. This exercise provides the mathematical foundation for how modern causal discovery algorithms represent this uncertainty. You will analytically derive the confounding bias in a simple gene regulatory model, demonstrating how the discrepancy between observational correlation and the true causal effect provides the statistical evidence for a bidirected edge, such as $G_{1} \\leftrightarrow G_{2}$, in a Partial Ancestral Graph (PAG). ",
            "id": "4322754",
            "problem": "A systems biomedicine team studies a small gene regulatory module with two observed messenger RNA expression levels, $G_{1}$ and $G_{2}$, and an unobserved transcription factor $L$ that binds promoters of both genes. They collect two datasets: an observational dataset under baseline conditions and an interventional dataset in which $G_{1}$ is forcibly set to a fixed level by a CRISPR interference perturbation (modeled as a perfect intervention). The module is modeled by a linear Gaussian structural causal model with the following structural equations:\n$$\nG_{1} \\;=\\; a_{1} L + \\epsilon_{1}, \\quad G_{2} \\;=\\; a_{2} L + \\epsilon_{2},\n$$\nwhere $L \\sim \\mathcal{N}(0,\\sigma_{L}^{2})$, $\\epsilon_{1} \\sim \\mathcal{N}(0,\\sigma_{1}^{2})$, and $\\epsilon_{2} \\sim \\mathcal{N}(0,\\sigma_{2}^{2})$, with $L$, $\\epsilon_{1}$, and $\\epsilon_{2}$ mutually independent. There is no direct causal effect $G_{1} \\to G_{2}$; all association between $G_{1}$ and $G_{2}$, if present, arises through $L$. The intervention is the do-operator $\\operatorname{do}(G_{1} = g)$, which severs all incoming edges into $G_{1}$ and sets $G_{1}$ to the value $g$.\n\nFrom foundational principles in causal discovery, the Partial Ancestral Graph (PAG) is the appropriate summary graph for the Markov equivalence class when latent confounders may be present, and the Fast Causal Inference (FCI) algorithm outputs a PAG whose bidirected edges indicate potential latent confounding between observed variables.\n\nUsing only these definitions and the model above, carry out the following derivation:\n- Compute the observational least-squares regression coefficient of $G_{2}$ on $G_{1}$, denoted $\\beta_{\\mathrm{obs}}$, based on the covariance structure implied by the structural equations.\n- Compute the interventional causal effect of $G_{1}$ on $G_{2}$, defined as the derivative $\\beta_{\\operatorname{do}} = \\frac{d}{dg}\\,\\mathbb{E}[\\,G_{2} \\mid \\operatorname{do}(G_{1}=g)\\,]$.\n- Let $\\Delta = \\beta_{\\mathrm{obs}} - \\beta_{\\operatorname{do}}$. Provide a closed-form analytic expression for $\\Delta$ in terms of $a_{1}$, $a_{2}$, and $\\sigma_{L}^{2}$, $\\sigma_{1}^{2}$, and explain how the sign and magnitude of $\\Delta$ inform the presence of a bidirected edge in the PAG between $G_{1}$ and $G_{2}$ due to the unobserved transcription factor $L$.\n\nYour final answer must be the single closed-form expression for $\\Delta$. No rounding is required and no units are needed.",
            "solution": "The problem statement is assessed to be valid. It is scientifically grounded in the principles of structural causal models and causal discovery, well-posed with a clear objective and sufficient information, and uses objective, formal language. We may therefore proceed with the derivation.\n\nThe problem asks for the derivation of three quantities based on the provided linear Gaussian structural causal model (SCM): the observational regression coefficient $\\beta_{\\mathrm{obs}}$, the interventional causal effect $\\beta_{\\operatorname{do}}$, and their difference $\\Delta = \\beta_{\\mathrm{obs}} - \\beta_{\\operatorname{do}}$.\n\nThe SCM is given by:\n$$\nG_{1} = a_{1} L + \\epsilon_{1}\n$$\n$$\nG_{2} = a_{2} L + \\epsilon_{2}\n$$\nwhere $L \\sim \\mathcal{N}(0,\\sigma_{L}^{2})$, $\\epsilon_{1} \\sim \\mathcal{N}(0,\\sigma_{1}^{2})$, and $\\epsilon_{2} \\sim \\mathcal{N}(0,\\sigma_{2}^{2})$ are mutually independent random variables.\n\n**1. Computation of the observational regression coefficient, $\\beta_{\\mathrm{obs}}$**\n\nThe observational least-squares regression coefficient of $G_{2}$ on $G_{1}$ is defined as $\\beta_{\\mathrm{obs}} = \\frac{\\mathrm{Cov}(G_{1}, G_{2})}{\\mathrm{Var}(G_{1})}$. We first compute the variance of $G_{1}$ and the covariance between $G_{1}$ and $G_{2}$.\n\nThe variance of $G_{1}$ is:\n$$\n\\mathrm{Var}(G_{1}) = \\mathrm{Var}(a_{1} L + \\epsilon_{1})\n$$\nSince $L$ and $\\epsilon_{1}$ are independent, the variance of their sum is the sum of their variances:\n$$\n\\mathrm{Var}(G_{1}) = \\mathrm{Var}(a_{1} L) + \\mathrm{Var}(\\epsilon_{1}) = a_{1}^{2} \\mathrm{Var}(L) + \\mathrm{Var}(\\epsilon_{1})\n$$\nSubstituting the given variances:\n$$\n\\mathrm{Var}(G_{1}) = a_{1}^{2} \\sigma_{L}^{2} + \\sigma_{1}^{2}\n$$\n\nThe covariance between $G_{1}$ and $G_{2}$ is:\n$$\n\\mathrm{Cov}(G_{1}, G_{2}) = \\mathrm{Cov}(a_{1} L + \\epsilon_{1}, a_{2} L + \\epsilon_{2})\n$$\nUsing the bilinearity property of covariance:\n$$\n\\mathrm{Cov}(G_{1}, G_{2}) = \\mathrm{Cov}(a_{1} L, a_{2} L) + \\mathrm{Cov}(a_{1} L, \\epsilon_{2}) + \\mathrm{Cov}(\\epsilon_{1}, a_{2} L) + \\mathrm{Cov}(\\epsilon_{1}, \\epsilon_{2})\n$$\nSince $L$, $\\epsilon_{1}$, and $\\epsilon_{2}$ are mutually independent, the covariance terms involving different variables are zero:\n$$\n\\mathrm{Cov}(G_{1}, G_{2}) = a_{1}a_{2}\\mathrm{Cov}(L, L) + 0 + 0 + 0 = a_{1}a_{2}\\mathrm{Var}(L)\n$$\nSubstituting the given variance of $L$:\n$$\n\\mathrm{Cov}(G_{1}, G_{2}) = a_{1} a_{2} \\sigma_{L}^{2}\n$$\n\nNow, we can compute $\\beta_{\\mathrm{obs}}$:\n$$\n\\beta_{\\mathrm{obs}} = \\frac{\\mathrm{Cov}(G_{1}, G_{2})}{\\mathrm{Var}(G_{1})} = \\frac{a_{1} a_{2} \\sigma_{L}^{2}}{a_{1}^{2} \\sigma_{L}^{2} + \\sigma_{1}^{2}}\n$$\n\n**2. Computation of the interventional causal effect, $\\beta_{\\operatorname{do}}$**\n\nThe interventional causal effect is defined as $\\beta_{\\operatorname{do}} = \\frac{d}{dg}\\,\\mathbb{E}[\\,G_{2} \\mid \\operatorname{do}(G_{1}=g)\\,]$. The intervention $\\operatorname{do}(G_{1}=g)$ modifies the SCM by replacing the structural equation for $G_{1}$ with the assignment $G_{1}=g$, severing all incoming causal arrows to $G_{1}$. The modified SCM is:\n$$\nG_{1} = g\n$$\n$$\nG_{2} = a_{2} L + \\epsilon_{2}\n$$\nThe distributions of the exogenous variables $L$ and $\\epsilon_{2}$ are unaffected by this intervention. We compute the expectation of $G_{2}$ in this post-intervention model:\n$$\n\\mathbb{E}[\\,G_{2} \\mid \\operatorname{do}(G_{1}=g)\\,] = \\mathbb{E}[a_{2} L + \\epsilon_{2}]\n$$\nBy linearity of expectation:\n$$\n\\mathbb{E}[\\,G_{2} \\mid \\operatorname{do}(G_{1}=g)\\,] = a_{2}\\mathbb{E}[L] + \\mathbb{E}[\\epsilon_{2}]\n$$\nGiven $L \\sim \\mathcal{N}(0,\\sigma_{L}^{2})$ and $\\epsilon_{2} \\sim \\mathcal{N}(0,\\sigma_{2}^{2})$, their expectations are $\\mathbb{E}[L]=0$ and $\\mathbb{E}[\\epsilon_{2}]=0$.\n$$\n\\mathbb{E}[\\,G_{2} \\mid \\operatorname{do}(G_{1}=g)\\,] = a_{2}(0) + 0 = 0\n$$\nThe expected value of $G_{2}$ under the intervention is $0$, which is a constant that does not depend on the intervention value $g$. The causal effect $\\beta_{\\operatorname{do}}$ is the derivative of this expectation with respect to $g$:\n$$\n\\beta_{\\operatorname{do}} = \\frac{d}{dg}(0) = 0\n$$\nThis result is expected, as there is no direct causal path from $G_{1}$ to $G_{2}$ in the model. Any change in $G_1$ via intervention has no effect on $G_2$.\n\n**3. Computation and interpretation of $\\Delta$**\n\nThe quantity $\\Delta$ is the difference between the observational regression coefficient and the true causal effect:\n$$\n\\Delta = \\beta_{\\mathrm{obs}} - \\beta_{\\operatorname{do}}\n$$\nSubstituting the derived expressions for $\\beta_{\\mathrm{obs}}$ and $\\beta_{\\operatorname{do}}$:\n$$\n\\Delta = \\frac{a_{1} a_{2} \\sigma_{L}^{2}}{a_{1}^{2} \\sigma_{L}^{2} + \\sigma_{1}^{2}} - 0 = \\frac{a_{1} a_{2} \\sigma_{L}^{2}}{a_{1}^{2} \\sigma_{L}^{2} + \\sigma_{1}^{2}}\n$$\nThis quantity, $\\Delta$, represents the confounding bias. It is the difference between the association measured from observational data ($\\beta_{\\mathrm{obs}}$) and the true causal effect ($\\beta_{\\operatorname{do}}$). In this model, the true causal effect is zero, so $\\Delta=\\beta_{\\mathrm{obs}}$.\n\nThe presence of a bidirected edge $G_{1} \\leftrightarrow G_{2}$ in a Partial Ancestral Graph (PAG) signifies a latent common cause. An algorithm like FCI infers this structure by detecting statistical dependencies that cannot be explained by direct causation or other observed variables. In this two-variable system, the algorithm would test for the marginal dependence of $G_{1}$ and $G_{2}$. They are dependent if and only if their covariance is non-zero. From our calculation, $\\mathrm{Cov}(G_{1}, G_{2}) = a_{1} a_{2} \\sigma_{L}^{2}$. This covariance is non-zero provided that $a_{1} \\neq 0$, $a_{2} \\neq 0$, and $\\sigma_{L}^{2} > 0$, which are the conditions for $L$ to be an active common cause.\n\nThe quantity $\\Delta$ is a direct measure of this confounding.\n- If $\\Delta = 0$, it implies that either $a_{1}=0$, $a_{2}=0$, or $\\sigma_{L}^{2}=0$. In any of these cases, the confounding path $G_{1} \\leftarrow L \\rightarrow G_{2}$ is broken, there is no association between $G_{1}$ and $G_{2}$ ($\\beta_{\\mathrm{obs}}=0$), and thus no evidence for a bidirected edge.\n- If $\\Delta \\neq 0$, there is a non-zero statistical association ($\\beta_{\\mathrm{obs}} \\neq 0$) that is purely due to the common cause $L$ because the true causal effect $\\beta_{\\operatorname{do}}$ is zero. This discrepancy between association and causation ($\\Delta \\neq 0$) is precisely the signature of confounding. A causal discovery algorithm would detect this association and, in the absence of an explanation via a direct causal path, infer the presence of a latent confounder, which is represented by the bidirected edge $G_{1} \\leftrightarrow G_{2}$ in the PAG.\n- The sign and magnitude of $\\Delta$ quantify the confounding. The sign, determined by $a_{1}a_{2}$, indicates whether the confounder induces a positive or negative correlation. The magnitude reflects the strength of the confounding path relative to the total variance of $G_1$.\n\nIn summary, $\\Delta$ is the statistical evidence for confounding by $L$, which justifies drawing a bidirected edge between $G_{1}$ and $G_{2}$ in the corresponding PAG.",
            "answer": "$$\\boxed{\\frac{a_{1} a_{2} \\sigma_{L}^{2}}{a_{1}^{2} \\sigma_{L}^{2} + \\sigma_{1}^{2}}}$$"
        },
        {
            "introduction": "Causal discovery algorithms rely on accurate conditional independence tests, but measurement error in biological assays can corrupt these very tests. This practice explores this critical, real-world complication by asking you to quantify its effect in a simple linear system. By deriving the partial correlation among noise-corrupted variables, you will prove how a true conditional independence ($X \\perp Y \\mid Z$) can be masked, creating a spurious dependency in the observed data ($X^{\\ast} \\not\\perp Y^{\\ast} \\mid Z^{\\ast}$) and leading to potentially incorrect causal conclusions. ",
            "id": "4322765",
            "problem": "In systems biomedicine, high-throughput assays often report signals corrupted by measurement error. Define measurement error as an additive distortion in which the observed variable equals the unobserved biological signal plus an independent noise term with zero mean and finite variance. Consider a linear-Gaussian three-gene cascade representing a transcriptional chain, with true variables $X$ (upstream transcription factor activity), $Z$ (intermediate pathway component), and $Y$ (downstream target gene expression). The true data-generating process is\n$$\nZ \\leftarrow a X + \\varepsilon_Z,\\qquad Y \\leftarrow b Z + \\varepsilon_Y,\n$$\nwith $X \\sim \\mathcal{N}(0,\\sigma_X^2)$, $\\varepsilon_Z \\sim \\mathcal{N}(0,\\sigma_Z^2)$, and $\\varepsilon_Y \\sim \\mathcal{N}(0,\\sigma_Y^2)$ mutually independent. The assay produces observed variables $X^{\\ast}$, $Z^{\\ast}$, and $Y^{\\ast}$ via additive, independent, mean-zero measurement errors $\\eta_X$, $\\eta_Z$, and $\\eta_Y$, respectively:\n$$\nX^{\\ast} = X + \\eta_X,\\qquad Z^{\\ast} = Z + \\eta_Z,\\qquad Y^{\\ast} = Y + \\eta_Y,\n$$\nwith $\\eta_X \\sim \\mathcal{N}(0,\\tau_X^2)$, $\\eta_Z \\sim \\mathcal{N}(0,\\tau_Z^2)$, $\\eta_Y \\sim \\mathcal{N}(0,\\tau_Y^2)$, independent of each other and of $(X,\\varepsilon_Z,\\varepsilon_Y)$. Assume that $a=b=1$, $\\sigma_X^2=\\sigma_Z^2=\\sigma_Y^2=1$, and that the measurement error variances are unequal, with $\\tau_X^2=0.5$, $\\tau_Z^2=2.0$, and $\\tau_Y^2=0.5$. Under the true model without measurement error, the conditional independence $X \\perp Y \\mid Z$ holds, so any sound conditional independence test based on partial correlation should accept the null at the population limit. However, under measurement error with unequal variances across variables, such tests can be biased.\n\nDerive, from first principles of Gaussian graphical models and covariance propagation under additive noise, the population partial correlation $\\rho_{X^{\\ast}Y^{\\ast}\\cdot Z^{\\ast}}$ induced by the measurement errors. Report the value of $\\rho_{X^{\\ast}Y^{\\ast}\\cdot Z^{\\ast}}$ as a single real number rounded to four significant figures. No units are required.",
            "solution": "The analysis of this problem proceeds in three main stages. First, we establish the theoretical formula for the population partial correlation. Second, we derive the necessary variances and covariances of the observed variables ($X^{\\ast}$, $Z^{\\ast}$, $Y^{\\ast}$) from the given structural equations and noise parameters. Third, we substitute these derived quantities into the partial correlation formula to compute the final numerical value.\n\nThe population partial correlation of two variables, say $U$ and $V$, given a third variable $W$, is denoted by $\\rho_{UV \\cdot W}$ and is defined by the formula:\n$$\n\\rho_{UV \\cdot W} = \\frac{\\rho_{UV} - \\rho_{UW} \\rho_{VW}}{\\sqrt{(1 - \\rho_{UW}^2)(1 - \\rho_{VW}^2)}}\n$$\nIn this problem, we are asked to find $\\rho_{X^{\\ast}Y^{\\ast}\\cdot Z^{\\ast}}$. The formula is therefore:\n$$\n\\rho_{X^{\\ast}Y^{\\ast}\\cdot Z^{\\ast}} = \\frac{\\rho_{X^{\\ast}Y^{\\ast}} - \\rho_{X^{\\ast}Z^{\\ast}} \\rho_{Y^{\\ast}Z^{\\ast}}}{\\sqrt{(1 - \\rho_{X^{\\ast}Z^{\\ast}}^2)(1 - \\rho_{Y^{\\ast}Z^{\\ast}}^2)}}\n$$\nTo use this formula, we must first compute the simple pairwise correlations $\\rho_{X^{\\ast}Y^{\\ast}}$, $\\rho_{X^{\\ast}Z^{\\ast}}$, and $\\rho_{Y^{\\ast}Z^{\\ast}}$. A correlation $\\rho_{UV}$ is defined as $\\frac{\\text{Cov}(U, V)}{\\sqrt{\\text{Var}(U)\\text{Var}(V)}}$. Since all fundamental variables in the model are defined to have zero mean, all derived variables ($X, Z, Y, X^{\\ast}, Z^{\\ast}, Y^{\\ast}$) also have zero mean. This simplifies the covariance calculation to $\\text{Cov}(U, V) = E[UV]$.\n\nFirst, we express the observed variables in terms of the set of fundamental, mutually independent random variables: $X \\sim \\mathcal{N}(0,\\sigma_X^2)$, $\\varepsilon_Z \\sim \\mathcal{N}(0,\\sigma_Z^2)$, $\\varepsilon_Y \\sim \\mathcal{N}(0,\\sigma_Y^2)$, $\\eta_X \\sim \\mathcal{N}(0,\\tau_X^2)$, $\\eta_Z \\sim \\mathcal{N}(0,\\tau_Z^2)$, and $\\eta_Y \\sim \\mathcal{N}(0,\\tau_Y^2)$.\nGiven $a=1$ and $b=1$:\nThe true variables are:\n$Z = X + \\varepsilon_Z$\n$Y = Z + \\varepsilon_Y = (X + \\varepsilon_Z) + \\varepsilon_Y = X + \\varepsilon_Z + \\varepsilon_Y$\n\nThe observed variables are:\n$X^{\\ast} = X + \\eta_X$\n$Z^{\\ast} = Z + \\eta_Z = X + \\varepsilon_Z + \\eta_Z$\n$Y^{\\ast} = Y + \\eta_Y = X + \\varepsilon_Z + \\varepsilon_Y + \\eta_Y$\n\nNow, we compute the required variances. The variance of a sum of independent random variables is the sum of their variances.\n$\\text{Var}(X^{\\ast}) = \\text{Var}(X + \\eta_X) = \\text{Var}(X) + \\text{Var}(\\eta_X) = \\sigma_X^2 + \\tau_X^2$\n$\\text{Var}(Z^{\\ast}) = \\text{Var}(X + \\varepsilon_Z + \\eta_Z) = \\text{Var}(X) + \\text{Var}(\\varepsilon_Z) + \\text{Var}(\\eta_Z) = \\sigma_X^2 + \\sigma_Z^2 + \\tau_Z^2$\n$\\text{Var}(Y^{\\ast}) = \\text{Var}(X + \\varepsilon_Z + \\varepsilon_Y + \\eta_Y) = \\text{Var}(X) + \\text{Var}(\\varepsilon_Z) + \\text{Var}(\\varepsilon_Y) + \\text{Var}(\\eta_Y) = \\sigma_X^2 + \\sigma_Z^2 + \\sigma_Y^2 + \\tau_Y^2$\n\nNext, we compute the required covariances. For any two independent, zero-mean variables $U$ and $V$, $E[UV]=E[U]E[V]=0$.\n$\\text{Cov}(X^{\\ast}, Z^{\\ast}) = E[X^{\\ast}Z^{\\ast}] = E[(X+\\eta_X)(X+\\varepsilon_Z+\\eta_Z)] = E[X^2] = \\text{Var}(X) = \\sigma_X^2$\n$\\text{Cov}(X^{\\ast}, Y^{\\ast}) = E[X^{\\ast}Y^{\\ast}] = E[(X+\\eta_X)(X+\\varepsilon_Z+\\varepsilon_Y+\\eta_Y)] = E[X^2] = \\text{Var}(X) = \\sigma_X^2$\n$\\text{Cov}(Z^{\\ast}, Y^{\\ast}) = E[Z^{\\ast}Y^{\\ast}] = E[(X+\\varepsilon_Z+\\eta_Z)(X+\\varepsilon_Z+\\varepsilon_Y+\\eta_Y)] = E[X^2] + E[\\varepsilon_Z^2] = \\text{Var}(X)+\\text{Var}(\\varepsilon_Z) = \\sigma_X^2 + \\sigma_Z^2$\n\nNow, we substitute the given numerical values: $\\sigma_X^2=1$, $\\sigma_Z^2=1$, $\\sigma_Y^2=1$, $\\tau_X^2=0.5$, $\\tau_Z^2=2.0$, and $\\tau_Y^2=0.5$.\n\nVariances:\n$\\text{Var}(X^{\\ast}) = 1 + 0.5 = 1.5$\n$\\text{Var}(Z^{\\ast}) = 1 + 1 + 2.0 = 4.0$\n$\\text{Var}(Y^{\\ast}) = 1 + 1 + 1 + 0.5 = 3.5$\n\nCovariances:\n$\\text{Cov}(X^{\\ast}, Z^{\\ast}) = 1$\n$\\text{Cov}(X^{\\ast}, Y^{\\ast}) = 1$\n$\\text{Cov}(Z^{\\ast}, Y^{\\ast}) = 1 + 1 = 2$\n\nWe can now compute the simple pairwise correlations:\n$\\rho_{X^{\\ast}Z^{\\ast}} = \\frac{\\text{Cov}(X^{\\ast}, Z^{\\ast})}{\\sqrt{\\text{Var}(X^{\\ast})\\text{Var}(Z^{\\ast})}} = \\frac{1}{\\sqrt{1.5 \\times 4.0}} = \\frac{1}{\\sqrt{6}}$\n$\\rho_{Y^{\\ast}Z^{\\ast}} = \\frac{\\text{Cov}(Z^{\\ast}, Y^{\\ast})}{\\sqrt{\\text{Var}(Z^{\\ast})\\text{Var}(Y^{\\ast})}} = \\frac{2}{\\sqrt{4.0 \\times 3.5}} = \\frac{2}{\\sqrt{14}}$\n$\\rho_{X^{\\ast}Y^{\\ast}} = \\frac{\\text{Cov}(X^{\\ast}, Y^{\\ast})}{\\sqrt{\\text{Var}(X^{\\ast})\\text{Var}(Y^{\\ast})}} = \\frac{1}{\\sqrt{1.5 \\times 3.5}} = \\frac{1}{\\sqrt{5.25}}$\n\nFinally, we substitute these correlations into the partial correlation formula.\nNumerator:\n$\\rho_{X^{\\ast}Y^{\\ast}} - \\rho_{X^{\\ast}Z^{\\ast}} \\rho_{Y^{\\ast}Z^{\\ast}} = \\frac{1}{\\sqrt{5.25}} - \\left(\\frac{1}{\\sqrt{6}}\\right)\\left(\\frac{2}{\\sqrt{14}}\\right) = \\frac{1}{\\sqrt{21/4}} - \\frac{2}{\\sqrt{84}} = \\frac{2}{\\sqrt{21}} - \\frac{2}{2\\sqrt{21}} = \\frac{1}{\\sqrt{21}}$\n\nDenominator:\nThe terms under the square root are:\n$1 - \\rho_{X^{\\ast}Z^{\\ast}}^2 = 1 - \\left(\\frac{1}{\\sqrt{6}}\\right)^2 = 1 - \\frac{1}{6} = \\frac{5}{6}$\n$1 - \\rho_{Y^{\\ast}Z^{\\ast}}^2 = 1 - \\left(\\frac{2}{\\sqrt{14}}\\right)^2 = 1 - \\frac{4}{14} = 1 - \\frac{2}{7} = \\frac{5}{7}$\nSo, the denominator is:\n$\\sqrt{(1 - \\rho_{X^{\\ast}Z^{\\ast}}^2)(1 - \\rho_{Y^{\\ast}Z^{\\ast}}^2)} = \\sqrt{\\left(\\frac{5}{6}\\right)\\left(\\frac{5}{7}\\right)} = \\sqrt{\\frac{25}{42}} = \\frac{5}{\\sqrt{42}}$\n\nCombining the numerator and denominator:\n$$\n\\rho_{X^{\\ast}Y^{\\ast}\\cdot Z^{\\ast}} = \\frac{\\frac{1}{\\sqrt{21}}}{\\frac{5}{\\sqrt{42}}} = \\frac{1}{\\sqrt{21}} \\cdot \\frac{\\sqrt{42}}{5} = \\frac{\\sqrt{2 \\times 21}}{5\\sqrt{21}} = \\frac{\\sqrt{2}\\sqrt{21}}{5\\sqrt{21}} = \\frac{\\sqrt{2}}{5}\n$$\nThis non-zero result demonstrates that the conditional independence $X \\perp Y \\mid Z$, which holds for the true variables, is broken for the observed variables $X^{\\ast} \\not\\perp Y^{\\ast} \\mid Z^{\\ast}$ due to the presence of measurement error, particularly the error $\\eta_Z$ on the conditioning variable.\n\nTo provide the final numerical answer, we calculate the value of $\\frac{\\sqrt{2}}{5}$ and round it to four significant figures:\n$$\n\\frac{\\sqrt{2}}{5} \\approx \\frac{1.41421356}{5} \\approx 0.2828427...\n$$\nRounded to four significant figures, the value is $0.2828$.",
            "answer": "$$\\boxed{0.2828}$$"
        }
    ]
}