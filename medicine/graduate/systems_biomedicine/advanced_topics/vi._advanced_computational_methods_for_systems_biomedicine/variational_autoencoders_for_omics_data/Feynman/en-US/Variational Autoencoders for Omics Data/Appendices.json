{
    "hands_on_practices": [
        {
            "introduction": "The Evidence Lower Bound (ELBO) is the cornerstone of training Variational Autoencoders. This first exercise walks you through a foundational derivation of the ELBO for a VAE with standard Gaussian assumptions . By deriving the reconstruction and regularization terms from first principles, you will gain a deep, analytical understanding of the fundamental trade-off that the VAE objective function optimizes.",
            "id": "4397996",
            "problem": "Consider a single omics sample represented by a log-normalized gene expression vector $x \\in \\mathbb{R}^{d}$ obtained from bulk ribonucleic acid sequencing (RNA-seq). A Variational Autoencoder (VAE) is specified with latent variable $z \\in \\mathbb{R}^{k}$ and a standard Gaussian prior $p(z) = \\mathcal{N}(z; 0, I)$. The generative model (decoder) is $p_{\\theta}(x \\mid z) = \\mathcal{N}(x; \\mu(z), \\sigma^{2} I)$, where the mean is linear in $z$ as $\\mu(z) = W z + b$ with $W \\in \\mathbb{R}^{d \\times k}$ and $b \\in \\mathbb{R}^{d}$, and the scalar observation variance $\\sigma^{2} > 0$ is known. The inference model (encoder) is $q_{\\phi}(z \\mid x) = \\mathcal{N}(z; \\mu_{\\phi}(x), \\operatorname{diag}(\\sigma_{\\phi}^{2}(x)))$, with mean $\\mu_{\\phi}(x) \\in \\mathbb{R}^{k}$ and elementwise variances $\\sigma_{\\phi}^{2}(x) \\in \\mathbb{R}_{>0}^{k}$.\n\nStarting from the definition of the Evidence Lower Bound (ELBO) as the difference between the expected log-likelihood under the encoder and the Kullback–Leibler divergence between the encoder and the prior, derive a closed-form analytic expression for the ELBO for the given sample $x$ in terms of $x$, $W$, $b$, $\\sigma^{2}$, $\\mu_{\\phi}(x)$, and $\\sigma_{\\phi}^{2}(x)$, along with $d$ and $k$. Your derivation must begin from fundamental definitions of probability densities of multivariate normal distributions and the Kullback–Leibler divergence, and must not assume any shortcut formulas. Express your final answer as a single closed-form analytic expression. No numerical rounding is required.",
            "solution": "The problem is valid. It presents a standard, well-posed task in probabilistic machine learning, providing all necessary information and definitions without scientific or logical contradictions. We proceed to the derivation.\n\nThe Evidence Lower Bound (ELBO) is defined as:\n$$\n\\mathcal{L}(x; \\theta, \\phi) = \\mathbb{E}_{q_{\\phi}(z \\mid x)}[\\log p_{\\theta}(x \\mid z)] - D_{KL}(q_{\\phi}(z \\mid x) \\| p(z))\n$$\nWe will derive the two terms on the right-hand side separately.\n\n**Part 1: The Expected Log-Likelihood Term**\n\nThe first term is the expected log-likelihood of the data under the approximate posterior $q_{\\phi}(z \\mid x)$. The generative model (decoder) is given by $p_{\\theta}(x \\mid z) = \\mathcal{N}(x; W z + b, \\sigma^{2} I)$.\n\nThe probability density function (PDF) of this $d$-dimensional multivariate normal distribution is:\n$$\np_{\\theta}(x \\mid z) = \\frac{1}{(2\\pi)^{d/2} \\det(\\sigma^2 I)^{1/2}} \\exp\\left(-\\frac{1}{2}(x - (Wz+b))^T (\\sigma^2 I)^{-1} (x - (Wz+b))\\right)\n$$\nThe determinant of the covariance matrix is $\\det(\\sigma^2 I) = (\\sigma^2)^d$. The inverse is $(\\sigma^2 I)^{-1} = \\frac{1}{\\sigma^2}I$. Substituting these in gives:\n$$\np_{\\theta}(x \\mid z) = \\frac{1}{(2\\pi \\sigma^2)^{d/2}} \\exp\\left(-\\frac{1}{2\\sigma^2}\\|x - Wz - b\\|_2^2\\right)\n$$\nTaking the natural logarithm, we get:\n$$\n\\log p_{\\theta}(x \\mid z) = -\\frac{d}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\|x - Wz - b\\|_2^2\n$$\nNow, we must take the expectation of this quantity with respect to the encoder distribution $q_{\\phi}(z \\mid x) = \\mathcal{N}(z; \\mu_{\\phi}(x), \\operatorname{diag}(\\sigma_{\\phi}^{2}(x)))$. For notational simplicity, let $\\mu_q = \\mu_{\\phi}(x)$ and $\\Sigma_q = \\operatorname{diag}(\\sigma_{\\phi}^{2}(x))$.\n$$\n\\mathbb{E}_{q_{\\phi}(z \\mid x)}[\\log p_{\\theta}(x \\mid z)] = \\mathbb{E}_{q_{\\phi}}\\left[-\\frac{d}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\|x - Wz - b\\|_2^2\\right]\n$$\nBy linearity of expectation, this becomes:\n$$\n\\mathbb{E}_{q_{\\phi}}[\\log p_{\\theta}(x \\mid z)] = -\\frac{d}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\mathbb{E}_{q_{\\phi}}[\\|x - Wz - b\\|_2^2]\n$$\nWe need to evaluate the expectation of the squared norm. Let's expand the term inside the expectation:\n$$\n\\|x - Wz - b\\|_2^2 = (x - b - Wz)^T(x - b - Wz) = (x-b)^T(x-b) - 2(x-b)^T W z + z^T W^T W z\n$$\nNow, we take the expectation with respect to $z \\sim q_{\\phi}$:\n$$\n\\mathbb{E}_{q_{\\phi}}[\\|x - Wz - b\\|_2^2] = \\mathbb{E}_{q_{\\phi}}[(x-b)^T(x-b)] - 2(x-b)^T W \\mathbb{E}_{q_{\\phi}}[z] + \\mathbb{E}_{q_{\\phi}}[z^T W^T W z]\n$$\nWe know that $\\mathbb{E}_{q_{\\phi}}[z] = \\mu_q$. For the quadratic term, we use the identity $\\mathbb{E}[y^T A y] = \\text{Tr}(A \\text{Cov}[y]) + \\mathbb{E}[y]^T A \\mathbb{E}[y]$. Here, $y=z$, $A = W^T W$, $\\mathbb{E}[z]=\\mu_q$, and $\\text{Cov}[z]=\\Sigma_q$.\n$$\n\\mathbb{E}_{q_{\\phi}}[z^T W^T W z] = \\text{Tr}(W^T W \\Sigma_q) + \\mu_q^T W^T W \\mu_q\n$$\nSubstituting these back, the expected squared norm is:\n$$\n\\mathbb{E}_{q_{\\phi}}[\\|x - Wz - b\\|_2^2] = \\|x-b\\|_2^2 - 2(x-b)^T W \\mu_q + \\mu_q^T W^T W \\mu_q + \\text{Tr}(W^T W \\Sigma_q)\n$$\nThe first three terms can be regrouped into a squared norm:\n$$\n\\|x-b\\|_2^2 - 2(x-b)^T W \\mu_q + \\|W\\mu_q\\|_2^2 = \\|(x-b) - W\\mu_q\\|_2^2 = \\|x - W\\mu_q - b\\|_2^2\n$$\nSo, we have:\n$$\n\\mathbb{E}_{q_{\\phi}}[\\|x - Wz - b\\|_2^2] = \\|x - W\\mu_q - b\\|_2^2 + \\text{Tr}(W^T W \\Sigma_q)\n$$\nThe trace term can be explicitly written out. Let $w_j$ be the $j$-th column of $W$, and $(\\sigma_q^2)_j$ be the $j$-th component of the variance vector $\\sigma_{\\phi}^2(x)$.\n$$\n\\text{Tr}(W^T W \\Sigma_q) = \\sum_{j=1}^k (W^T W \\Sigma_q)_{jj} = \\sum_{j=1}^k \\left(\\sum_{l=1}^k (W^T W)_{jl} (\\Sigma_q)_{lj}\\right)\n$$\nSince $\\Sigma_q$ is diagonal, $(\\Sigma_q)_{lj} = (\\sigma_q^2)_j \\delta_{lj}$.\n$$\n\\text{Tr}(W^T W \\Sigma_q) = \\sum_{j=1}^k (W^T W)_{jj} (\\sigma_q^2)_j = \\sum_{j=1}^k \\left(\\sum_{i=1}^d W_{ij}^2\\right) (\\sigma_q^2)_j = \\sum_{j=1}^k \\|w_j\\|_2^2 (\\sigma_q^2)_j\n$$\nFinally, substituting this back gives the first term of the ELBO:\n$$\n\\mathbb{E}_{q_{\\phi}(z \\mid x)}[\\log p_{\\theta}(x \\mid z)] = -\\frac{d}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\left( \\|x - W\\mu_{\\phi}(x) - b\\|_2^2 + \\sum_{j=1}^k \\|w_j\\|_2^2 (\\sigma_{\\phi}^2(x))_j \\right)\n$$\n\n**Part 2: The Kullback-Leibler Divergence Term**\n\nThe second term is the KL divergence between the encoder distribution $q_{\\phi}(z \\mid x) = \\mathcal{N}(z; \\mu_q, \\Sigma_q)$ and the prior $p(z) = \\mathcal{N}(z; 0, I)$.\nThe KL divergence is defined as:\n$$\nD_{KL}(q_{\\phi} \\| p) = \\int q_{\\phi}(z \\mid x) \\log \\frac{q_{\\phi}(z \\mid x)}{p(z)} dz = \\mathbb{E}_{q_{\\phi}}[\\log q_{\\phi}(z \\mid x) - \\log p(z)]\n$$\nSince both $q_{\\phi}$ and $p$ are diagonal-covariance Gaussians, they factorize over the dimensions of $z$. Thus, the KL divergence is the sum of the KL divergences for each dimension:\n$$\nD_{KL}(q_{\\phi} \\| p) = \\sum_{j=1}^k D_{KL}(\\mathcal{N}(z_j; (\\mu_q)_j, (\\sigma_q^2)_j) \\| \\mathcal{N}(z_j; 0, 1))\n$$\nFor a single dimension $j$, we have $q_j(z_j) = \\mathcal{N}(z_j; (\\mu_q)_j, (\\sigma_q^2)_j)$ and $p_j(z_j) = \\mathcal{N}(z_j; 0, 1)$. Their log-PDFs are:\n$$\n\\log q_j(z_j) = -\\frac{1}{2}\\log(2\\pi(\\sigma_q^2)_j) - \\frac{(z_j - (\\mu_q)_j)^2}{2(\\sigma_q^2)_j}\n$$\n$$\n\\log p_j(z_j) = -\\frac{1}{2}\\log(2\\pi) - \\frac{z_j^2}{2}\n$$\nThe difference is:\n$$\n\\log q_j(z_j) - \\log p_j(z_j) = -\\frac{1}{2}\\log((\\sigma_q^2)_j) - \\frac{(z_j - (\\mu_q)_j)^2}{2(\\sigma_q^2)_j} + \\frac{z_j^2}{2}\n$$\nTaking the expectation with respect to $z_j \\sim q_j(z_j)$:\n$$\nD_{KL}(q_j \\| p_j) = \\mathbb{E}_{q_j}\\left[ -\\frac{1}{2}\\log((\\sigma_q^2)_j) - \\frac{(z_j - (\\mu_q)_j)^2}{2(\\sigma_q^2)_j} + \\frac{z_j^2}{2} \\right]\n$$\nUsing linearity of expectation:\n$$\nD_{KL}(q_j \\| p_j) = -\\frac{1}{2}\\log((\\sigma_q^2)_j) - \\frac{1}{2(\\sigma_q^2)_j}\\mathbb{E}_{q_j}[(z_j - (\\mu_q)_j)^2] + \\frac{1}{2}\\mathbb{E}_{q_j}[z_j^2]\n$$\nWe have $\\mathbb{E}_{q_j}[(z_j - (\\mu_q)_j)^2] = \\text{Var}_{q_j}(z_j) = (\\sigma_q^2)_j$, and $\\mathbb{E}_{q_j}[z_j^2] = \\text{Var}_{q_j}(z_j) + (\\mathbb{E}_{q_j}[z_j])^2 = (\\sigma_q^2)_j + (\\mu_q)_j^2$.\n$$\nD_{KL}(q_j \\| p_j) = -\\frac{1}{2}\\log((\\sigma_q^2)_j) - \\frac{(\\sigma_q^2)_j}{2(\\sigma_q^2)_j} + \\frac{(\\sigma_q^2)_j + (\\mu_q)_j^2}{2}\n$$\n$$\nD_{KL}(q_j \\| p_j) = \\frac{1}{2} \\left[ -\\log((\\sigma_q^2)_j) - 1 + (\\sigma_q^2)_j + (\\mu_q)_j^2 \\right]\n$$\nSumming over all $k$ dimensions:\n$$\nD_{KL}(q_{\\phi}(z \\mid x) \\| p(z)) = \\frac{1}{2} \\sum_{j=1}^k \\left( (\\mu_{\\phi}(x)_j)^2 + (\\sigma_{\\phi}^2(x))_j - \\log((\\sigma_{\\phi}^2(x))_j) - 1 \\right)\n$$\n\n**Final Expression for ELBO**\n\nCombining the two parts, ELBO = (Part 1) - (Part 2):\n$$\n\\mathcal{L}(x; \\theta, \\phi) = \\left[ -\\frac{d}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\left( \\|x - W\\mu_{\\phi}(x) - b\\|_2^2 + \\sum_{j=1}^k \\|w_j\\|_2^2 (\\sigma_{\\phi}^2(x))_j \\right) \\right] - \\left[ \\frac{1}{2} \\sum_{j=1}^k \\left( (\\mu_{\\phi}(x)_j)^2 + (\\sigma_{\\phi}^2(x))_j - \\log((\\sigma_{\\phi}^2(x))_j) - 1 \\right) \\right]\n$$\nRearranging the terms yields the final single expression:\n$$\n\\mathcal{L}(x; \\theta, \\phi) = -\\frac{d}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\|x - W\\mu_{\\phi}(x) - b\\|_2^2 - \\frac{1}{2\\sigma^2}\\sum_{j=1}^k \\|w_j\\|_2^2 (\\sigma_{\\phi}^2(x))_j + \\frac{1}{2} \\sum_{j=1}^k \\left(1 - (\\mu_{\\phi}(x)_j)^2 - (\\sigma_{\\phi}^2(x))_j + \\log((\\sigma_{\\phi}^2(x))_j) \\right)\n$$\nwhere $w_j$ is the $j$-th column of the matrix $W$.",
            "answer": "$$ \\boxed{-\\frac{d}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\|x - W\\mu_{\\phi}(x) - b\\|_2^2 - \\frac{1}{2\\sigma^2}\\sum_{j=1}^k \\|w_j\\|_2^2 (\\sigma_{\\phi}^2(x))_j + \\frac{1}{2} \\sum_{j=1}^k \\left(1 + \\log((\\sigma_{\\phi}^2(x))_j) - (\\mu_{\\phi}(x)_j)^2 - (\\sigma_{\\phi}^2(x))_j \\right)} $$"
        },
        {
            "introduction": "While powerful, standard VAEs often simplify the approximate posterior with a diagonal covariance, ignoring potential correlations in the latent space. This practice explores a more expressive 'low-rank plus diagonal' covariance structure, a technique to capture latent dependencies efficiently . Implementing this model and quantifying its effect on the ELBO will provide you with practical skills in designing more flexible variational families and understanding their impact on model fit.",
            "id": "4397881",
            "problem": "Consider a linear-Gaussian latent variable model for high-dimensional biological measurements (for example, transcriptomic profiles) where the latent representation captures underlying biological processes. Let the latent variable be $z \\in \\mathbb{R}^k$, the observed data be $x \\in \\mathbb{R}^p$, and define a generative model with a standard normal prior and isotropic Gaussian likelihood: the prior is $p(z) = \\mathcal{N}(0, I_k)$ and the likelihood is $p(x \\mid z) = \\mathcal{N}(W z + b, \\sigma_x^2 I_p)$, with $W \\in \\mathbb{R}^{p \\times k}$, $b \\in \\mathbb{R}^p$, and $\\sigma_x^2 > 0$. In Variational Autoencoders (VAE), the approximate posterior $q(z \\mid x)$ is commonly parameterized as a multivariate Gaussian with mean $m \\in \\mathbb{R}^k$ and covariance $\\Sigma \\in \\mathbb{R}^{k \\times k}$, and the Evidence Lower Bound (ELBO) objective for a single observation is defined as the expected log-likelihood minus the Kullback–Leibler divergence to the prior. In many high-throughput omics applications, latent correlations are present and diagonal covariances can be overly restrictive. A computationally efficient alternative is a “low-rank plus diagonal” covariance parameterization $\\Sigma = \\operatorname{diag}(d) + U U^\\top$ with $d \\in \\mathbb{R}^k$ satisfying $d_i > 0$ for all $i$, and $U \\in \\mathbb{R}^{k \\times r}$ with $r \\ll k$.\n\nYour task is to implement a program that, for a set of specified test cases, constructs two variational families sharing the same mean $m$ but differing in covariance structure: (i) a diagonal covariance $\\Sigma_{\\mathrm{diag}} = \\operatorname{diag}(d)$ and (ii) a low-rank plus diagonal covariance $\\Sigma_{\\mathrm{lr}} = \\operatorname{diag}(d) + U U^\\top$. For each test case, you must compute the ELBO for both families and report the ELBO improvement, defined as the scalar $I = \\mathrm{ELBO}(\\Sigma_{\\mathrm{lr}}) - \\mathrm{ELBO}(\\Sigma_{\\mathrm{diag}})$.\n\nUse the following foundational setup across all test cases:\n- The bias is $b = 0$.\n- The mean $m$ used by both variational families is the exact Bayesian posterior mean under the specified linear-Gaussian generative model for the given $W$, $\\sigma_x^2$, and $x$.\n- The diagonal entries $d \\in \\mathbb{R}^k$ are fixed deterministically by $d_i = 0.1 + 0.02 \\times i$ for $i \\in \\{1, 2, \\dots, k\\}$.\n- The low-rank factor $U \\in \\mathbb{R}^{k \\times r}$ is generated from independent and identically distributed Gaussian entries with zero mean and standard deviation $0.3$.\n- The loading matrix $W \\in \\mathbb{R}^{p \\times k}$ is generated from independent and identically distributed Gaussian entries with zero mean and standard deviation $1/\\sqrt{k}$.\n- The observed vector $x \\in \\mathbb{R}^p$ is generated by sampling $z_{\\mathrm{true}} \\sim \\mathcal{N}(0, I_k)$ and noise $\\epsilon \\sim \\mathcal{N}(0, \\sigma_x^2 I_p)$, then setting $x = W z_{\\mathrm{true}} + \\epsilon$.\n\nAll random draws must be performed using a pseudorandom number generator initialized with the specified seed for each test case to ensure reproducibility. Angles are not involved, and there are no physical units. The ELBO improvement $I$ must be returned as a real number for each test case.\n\nThe program must implement numerically stable computations for determinants and traces suitable for the low-rank plus diagonal parameterization. In particular, the determinant of $\\Sigma_{\\mathrm{lr}}$ should be computed using a numerically stable approach appropriate for a rank-$r$ update to a diagonal matrix, and the trace terms must be evaluated without constructing dense intermediate matrices when avoidable.\n\nTest Suite:\n- Test case $1$: $k = 8$, $p = 50$, $r = 3$, $\\sigma_x^2 = 0.25$, seed $= 42$.\n- Test case $2$: $k = 8$, $p = 50$, $r = 0$, $\\sigma_x^2 = 0.25$, seed $= 42$.\n- Test case $3$: $k = 8$, $p = 50$, $r = 3$, $\\sigma_x^2 = 5.0$, seed $= 7$.\n- Test case $4$: $k = 8$, $p = 50$, $r = 5$, $\\sigma_x^2 = 0.05$, seed $= 13$.\n- Test case $5$: $k = 30$, $p = 200$, $r = 5$, $\\sigma_x^2 = 0.5$, seed $= 101$.\n\nYour program should produce a single line of output containing the ELBO improvement results for all test cases, as a comma-separated list enclosed in square brackets. For example, if the computed improvements are $I_1$, $I_2$, $I_3$, $I_4$, and $I_5$, the program must print exactly the string $[I_1,I_2,I_3,I_4,I_5]$ on a single line. Each $I_j$ must be a real number represented in decimal form. The expected outputs are floats, and no other text should be printed.",
            "solution": "The task is to compute the Evidence Lower Bound (ELBO) improvement, defined as $I = \\mathrm{ELBO}(\\Sigma_{\\mathrm{lr}}) - \\mathrm{ELBO}(\\Sigma_{\\mathrm{diag}})$, for two variational distributions. These distributions share a common mean $m$ but have different covariance structures: a diagonal covariance $\\Sigma_{\\mathrm{diag}} = \\operatorname{diag}(d)$ and a low-rank plus diagonal covariance $\\Sigma_{\\mathrm{lr}} = \\operatorname{diag}(d) + UU^\\top$.\n\nThe ELBO for a single observation $x$ is defined as $\\mathrm{ELBO} = \\mathbb{E}_{q(z \\mid x)}[\\log p(x \\mid z)] - D_{KL}(q(z \\mid x) \\| p(z))$. The approximate posterior is $q(z \\mid x) = \\mathcal{N}(z; m, \\Sigma)$, the prior is $p(z) = \\mathcal{N}(z; 0, I_k)$, and the likelihood is $p(x \\mid z) = \\mathcal{N}(x; Wz, \\sigma_x^2 I_p)$, as the bias $b$ is specified to be $0$.\n\nFirst, we derive the expressions for the two components of the ELBO for a generic variational covariance $\\Sigma$. The Kullback-Leibler divergence between the approximate posterior $q(z \\mid x) = \\mathcal{N}(m, \\Sigma)$ and the prior $p(z) = \\mathcal{N}(0, I_k)$ is given by the standard formula:\n$$\nD_{KL}(q(z \\mid x) \\| p(z)) = \\frac{1}{2} \\left( \\mathrm{Tr}(\\Sigma) + m^\\top m - k - \\log \\det(\\Sigma) \\right)\n$$\nNext, we derive the expected log-likelihood term. The log-likelihood function is:\n$$\n\\log p(x \\mid z) = -\\frac{1}{2\\sigma_x^2} \\|x - Wz\\|_2^2 - \\frac{p}{2}\\log(2\\pi\\sigma_x^2)\n$$\nTaking the expectation with respect to $q(z \\mid x)$ yields:\n$$\n\\mathbb{E}_q[\\log p(x \\mid z)] = -\\frac{1}{2\\sigma_x^2} \\mathbb{E}_q[\\|x - Wz\\|_2^2] - \\frac{p}{2}\\log(2\\pi\\sigma_x^2)\n$$\nThe expected value of the squared norm can be expanded as $\\mathbb{E}_q[(x-Wz)^\\top(x-Wz)] = \\mathbb{E}_q[x^\\top x - 2x^\\top Wz + z^\\top W^\\top Wz]$. Using the linearity of expectation and the trace trick for quadratic forms, $\\mathbb{E}_q[z^\\top A z] = \\mathrm{Tr}(A \\cdot \\mathrm{Cov}_q[z]) + (\\mathbb{E}_q[z])^\\top A \\mathbb{E}_q[z]$, we get:\n$$\n\\mathbb{E}_q[\\|x - Wz\\|_2^2] = \\|x - Wm\\|_2^2 + \\mathrm{Tr}(W^\\top W \\Sigma)\n$$\nSo the expected log-likelihood is:\n$$\n\\mathbb{E}_q[\\log p(x \\mid z)] = -\\frac{1}{2\\sigma_x^2} \\left( \\|x - Wm\\|_2^2 + \\mathrm{Tr}(W^\\top W \\Sigma) \\right) - \\frac{p}{2}\\log(2\\pi\\sigma_x^2)\n$$\nCombining these components, the ELBO as a function of the covariance $\\Sigma$ is:\n$$\n\\mathrm{ELBO}(\\Sigma) = -\\frac{1}{2\\sigma_x^2} \\left( \\|x - Wm\\|_2^2 + \\mathrm{Tr}(W^\\top W \\Sigma) \\right) - \\frac{p}{2}\\log(2\\pi\\sigma_x^2) - \\frac{1}{2} \\left( \\mathrm{Tr}(\\Sigma) + m^\\top m - k - \\log \\det(\\Sigma) \\right)\n$$\nWe need to calculate the improvement $I = \\mathrm{ELBO}(\\Sigma_{\\mathrm{lr}}) - \\mathrm{ELBO}(\\Sigma_{\\mathrm{diag}})$. Since the mean $m$ is identical for both variational families, all terms that do not depend on $\\Sigma$ will cancel out. These terms are $-\\frac{1}{2\\sigma_x^2}\\|x - Wm\\|_2^2$, $-\\frac{p}{2}\\log(2\\pi\\sigma_x^2)$, $-\\frac{1}{2}m^\\top m$, and $\\frac{k}{2}$. This cancellation is a crucial simplification, as it means the final result for $I$ does not depend on the observed data $x$ or the posterior mean $m$.\n\nThe part of the ELBO that depends on $\\Sigma$ is:\n$$\n\\mathrm{ELBO}_{\\text{dep}}(\\Sigma) = -\\frac{1}{2\\sigma_x^2}\\mathrm{Tr}(W^\\top W \\Sigma) - \\frac{1}{2}\\mathrm{Tr}(\\Sigma) + \\frac{1}{2}\\log \\det(\\Sigma) = \\frac{1}{2}\\log \\det(\\Sigma) - \\frac{1}{2} \\mathrm{Tr}\\left(\\left(I_k + \\frac{1}{\\sigma_x^2}W^\\top W\\right)\\Sigma\\right)\n$$\nThe improvement $I$ is the difference in this quantity evaluated at $\\Sigma_{\\mathrm{lr}}$ and $\\Sigma_{\\mathrm{diag}}$:\n$$\nI = \\left(\\frac{1}{2}\\log \\det(\\Sigma_{\\mathrm{lr}}) - \\frac{1}{2} \\mathrm{Tr}\\left(A\\Sigma_{\\mathrm{lr}}\\right)\\right) - \\left(\\frac{1}{2}\\log \\det(\\Sigma_{\\mathrm{diag}}) - \\frac{1}{2} \\mathrm{Tr}\\left(A\\Sigma_{\\mathrm{diag}}\\right)\\right)\n$$\nwhere $A = I_k + \\frac{1}{\\sigma_x^2}W^\\top W$. Using linearity of the trace and properties of the logarithm, this simplifies to:\n$$\nI = \\frac{1}{2} \\log\\left(\\frac{\\det(\\Sigma_{\\mathrm{lr}})}{\\det(\\Sigma_{\\mathrm{diag}})}\\right) - \\frac{1}{2} \\mathrm{Tr}\\left(A(\\Sigma_{\\mathrm{lr}} - \\Sigma_{\\mathrm{diag}})\\right)\n$$\nSubstituting $\\Sigma_{\\mathrm{diag}} = D = \\operatorname{diag}(d)$ and $\\Sigma_{\\mathrm{lr}} = D + UU^\\top$, their difference is $\\Sigma_{\\mathrm{lr}} - \\Sigma_{\\mathrm{diag}} = UU^\\top$. The expression for $I$ becomes:\n$$\nI = \\frac{1}{2} \\log\\left(\\frac{\\det(D + UU^\\top)}{\\det(D)}\\right) - \\frac{1}{2} \\mathrm{Tr}\\left(\\left(I_k + \\frac{1}{\\sigma_x^2}W^\\top W\\right)UU^\\top\\right)\n$$\nTo compute this expression in a numerically stable and efficient manner, we use two matrix identities. First, the matrix determinant lemma, $\\det(A + CB) = \\det(A)\\det(I + BA^{-1}C)$, gives:\n$$\n\\frac{\\det(D + UU^\\top)}{\\det(D)} = \\det(I_r + U^\\top D^{-1} U)\n$$\nThis avoids computing the determinant of a large $k \\times k$ matrix, reducing it to an $r \\times r$ matrix, where $r \\ll k$.\nSecond, the cyclic property of the trace, $\\mathrm{Tr}(XYZ) = \\mathrm{Tr}(ZXY)$, allows us to rewrite the trace term:\n$$\n\\mathrm{Tr}\\left(A UU^\\top\\right) = \\mathrm{Tr}\\left(U^\\top A U\\right)\n$$\nThis also reduces the computation from a trace involving a $k \\times k$ matrix product to one involving an $r \\times r$ matrix.\nThe final formula for implementation is:\n$$\nI = \\frac{1}{2} \\left[ \\log \\det(I_r + U^\\top D^{-1} U) - \\mathrm{Tr}\\left(U^\\top \\left(I_k + \\frac{1}{\\sigma_x^2}W^\\top W\\right) U\\right) \\right]\n$$\nThis formula is implemented for each test case by generating the matrices $W$ and $U$, and the vector $d$, according to the specified parameters and random seed.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the ELBO improvement for several test cases comparing a low-rank plus diagonal\n    covariance with a diagonal covariance in a variational autoencoder setting.\n    \"\"\"\n    test_cases = [\n        # (k, p, r, sigma^2, seed)\n        (8, 50, 3, 0.25, 42),\n        (8, 50, 0, 0.25, 42),\n        (8, 50, 3, 5.0, 7),\n        (8, 50, 5, 0.05, 13),\n        (30, 200, 5, 0.5, 101),\n    ]\n\n    results = []\n\n    for k, p, r, sigma_sq, seed in test_cases:\n        rng = np.random.default_rng(seed)\n\n        # Generate the loading matrix W\n        w_std = 1 / np.sqrt(k)\n        W = rng.normal(loc=0.0, scale=w_std, size=(p, k))\n\n        # Generate the low-rank factor U\n        # For r=0, U will be a (k, 0) matrix. numpy handles this correctly.\n        U = rng.normal(loc=0.0, scale=0.3, size=(k, r))\n\n        # Generate the diagonal entries d of the covariance\n        # d_i = 0.1 + 0.02 * i for i in {1, ..., k}\n        indices = np.arange(1, k + 1)\n        d = 0.1 + 0.02 * indices\n\n        # --- Compute the ELBO improvement I ---\n        # The formula derived is:\n        # I = 0.5 * [log det(I_r + U.T @ D^-1 @ U) - Tr(U.T @ (I_k + W.T@W/sigma_sq) @ U)]\n        \n        # Term 1: Log determinant term\n        # log det(I_r + U.T @ D^-1 @ U)\n        # We compute U.T @ D^-1 @ U efficiently as U.T @ (U / d), avoiding explicit D^-1 matrix\n        if r > 0:\n            M = U.T @ (U / d[:, np.newaxis])\n            log_det_term = np.log(np.linalg.det(np.eye(r) + M))\n        else: # r=0 case\n            log_det_term = 0.0\n            \n        # Term 2: Trace term\n        # Tr(U.T @ (I_k + W.T@W/sigma_sq) @ U)\n        A = np.eye(k) + (W.T @ W) / sigma_sq\n        if r > 0:\n            trace_term = np.trace(U.T @ A @ U)\n        else: # r=0 case\n            trace_term = 0.0\n        \n        # Calculate the final improvement I\n        improvement = 0.5 * (log_det_term - trace_term)\n        results.append(improvement)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A key application of VAEs in systems biomedicine is to perform in-silico experiments by generating counterfactual predictions. This hands-on exercise introduces the Conditional VAE (CVAE), a powerful extension that can disentangle sources of variation like batch effects or drug treatments . By implementing a CVAE and using it to predict a cell's state under an altered condition, you will directly engage with one of the most impactful uses of generative models in modern biology.",
            "id": "4397973",
            "problem": "You are given a linear-Gaussian Conditional Variational Autoencoder (CVAE) tailored to high-dimensional biological measurements such as gene expression. The CVAE conditions on known batch labels and treatment indicators encoded in a conditioning vector. The goal is to compute, for several test cases, the Evidence Lower BOund (ELBO) for the observed conditioning and the counterfactual reconstruction mean under an altered conditioning. Your implementation must be a complete, runnable program that uses only the provided constants and test inputs, performs deterministic computations based on first principles, and prints the required outputs in the specified format.\n\nModel definition and assumptions:\n- The prior over latent variables is standard normal: $p(\\mathbf{z}) = \\mathcal{N}(\\mathbf{z}\\,;\\,\\mathbf{0}, \\mathbf{I})$ with latent dimension $L = 2$.\n- The likelihood is conditionally Gaussian with a linear decoder:\n  $$p(\\mathbf{x}\\mid \\mathbf{z}, \\mathbf{c}) = \\mathcal{N}\\!\\left(\\mathbf{x}\\,;\\, \\mathbf{W}\\mathbf{z} + \\mathbf{U}\\mathbf{c} + \\mathbf{b},\\, \\sigma_x^2 \\mathbf{I}\\right),$$\n  where the data dimension is $D = 4$ and the condition dimension is $C = 3$.\n- The approximate posterior (encoder) is a diagonal Gaussian with parameters affine in $(\\mathbf{x}, \\mathbf{c})$:\n  $$q(\\mathbf{z}\\mid \\mathbf{x}, \\mathbf{c}) = \\mathcal{N}\\!\\left(\\mathbf{z}\\,;\\, \\boldsymbol{\\mu}_z(\\mathbf{x},\\mathbf{c}),\\, \\operatorname{diag}(\\boldsymbol{\\sigma}^2_z(\\mathbf{x},\\mathbf{c}))\\right),$$\n  with\n  $$\\boldsymbol{\\mu}_z(\\mathbf{x},\\mathbf{c}) = \\mathbf{M}\\mathbf{x} + \\mathbf{V}\\mathbf{c} + \\mathbf{a}, \\quad \\log \\boldsymbol{\\sigma}^2_z(\\mathbf{x},\\mathbf{c}) = \\mathbf{S}_x \\mathbf{x} + \\mathbf{S}_c \\mathbf{c} + \\mathbf{s}_0,$$\n  where the logarithm is applied elementwise.\n\nAll parameters are fixed and given below. Vectors are column vectors unless noted.\n\nParameter values to be used:\n- Decoder parameters:\n  - $$\\mathbf{W} = \\begin{bmatrix}\n  0.8 & -0.3\\\\\n  0.1 & 0.5\\\\\n  -0.4 & 0.2\\\\\n  0.0 & 0.3\n  \\end{bmatrix}, \\quad\n  \\mathbf{U} = \\begin{bmatrix}\n  0.6 & -0.2 & 0.3\\\\\n  -0.1 & 0.4 & -0.5\\\\\n  0.2 & 0.1 & -0.2\\\\\n  0.5 & -0.3 & 0.2\n  \\end{bmatrix}, \\quad\n  \\mathbf{b} = \\begin{bmatrix} 0.05\\\\ -0.02\\\\ 0.01\\\\ 0.0 \\end{bmatrix}, \\quad \\sigma_x = 0.1.$$\n- Encoder parameters:\n  - $$\\mathbf{M} = \\begin{bmatrix}\n  0.2 & 0.0 & -0.1 & 0.3\\\\\n  -0.2 & 0.1 & 0.4 & -0.1\n  \\end{bmatrix}, \\quad\n  \\mathbf{V} = \\begin{bmatrix}\n  0.1 & -0.2 & 0.3\\\\\n  0.0 & 0.2 & -0.1\n  \\end{bmatrix}, \\quad\n  \\mathbf{a} = \\begin{bmatrix} 0.05\\\\ -0.05 \\end{bmatrix},$$\n  $$\\mathbf{S}_x = \\begin{bmatrix}\n  -0.1 & 0.2 & 0.0 & -0.2\\\\\n  0.1 & -0.1 & 0.3 & 0.0\n  \\end{bmatrix}, \\quad\n  \\mathbf{S}_c = \\begin{bmatrix}\n  0.05 & 0.05 & -0.1\\\\\n  -0.05 & 0.1 & 0.0\n  \\end{bmatrix}, \\quad\n  \\mathbf{s}_0 = \\begin{bmatrix} -1.0\\\\ -1.2 \\end{bmatrix}.$$\n\nConditioning vector construction:\n- The condition vector $\\mathbf{c} \\in \\mathbb{R}^3$ concatenates batch label and treatment indicator as follows: the first two entries are one-hot batch indicators for two batches (Batch A as $[1,0]^\\top$, Batch B as $[0,1]^\\top$), and the third entry is the treatment indicator in $\\{0,1\\}$.\n\nQuantities to compute:\n- For each test case with observed pair $(\\mathbf{x}_{\\mathrm{obs}}, \\mathbf{c}_{\\mathrm{obs}})$ and an altered counterfactual condition $\\mathbf{c}_{\\mathrm{alt}}$, compute:\n  1. The Evidence Lower BOund (ELBO) for the observed pair:\n     $$\\mathcal{L}(\\mathbf{x}_{\\mathrm{obs}}, \\mathbf{c}_{\\mathrm{obs}}) = \\mathbb{E}_{q(\\mathbf{z}\\mid \\mathbf{x}_{\\mathrm{obs}}, \\mathbf{c}_{\\mathrm{obs}})}\\!\\left[\\log p(\\mathbf{x}_{\\mathrm{obs}} \\mid \\mathbf{z}, \\mathbf{c}_{\\mathrm{obs}})\\right] - \\mathrm{KL}\\!\\left(q(\\mathbf{z}\\mid \\mathbf{x}_{\\mathrm{obs}}, \\mathbf{c}_{\\mathrm{obs}})\\,\\|\\, p(\\mathbf{z})\\right).$$\n     You must compute the expectation in closed form using linear-Gaussian identities, without Monte Carlo sampling.\n  2. The counterfactual reconstruction mean under the altered condition:\n     $$\\mathbb{E}_{q(\\mathbf{z}\\mid \\mathbf{x}_{\\mathrm{obs}}, \\mathbf{c}_{\\mathrm{obs}})}\\!\\left[\\mathbf{W}\\mathbf{z} + \\mathbf{U}\\mathbf{c}_{\\mathrm{alt}} + \\mathbf{b}\\right],$$\n     again in closed form.\n\nMathematical facts to start from:\n- For a Gaussian likelihood $p(\\mathbf{x}\\mid \\mathbf{z}) = \\mathcal{N}(\\mathbf{x}; \\mathbf{A}\\mathbf{z}+\\boldsymbol{\\mu}, \\sigma^2 \\mathbf{I})$ and a diagonal Gaussian $q(\\mathbf{z}) = \\mathcal{N}(\\mathbf{z}; \\boldsymbol{\\mu}_z, \\operatorname{diag}(\\boldsymbol{\\sigma}^2_z))$, the expectation\n  $$\\mathbb{E}_q\\!\\left[\\lVert \\mathbf{x} - (\\mathbf{A}\\mathbf{z}+\\boldsymbol{\\mu}) \\rVert_2^2\\right] = \\lVert \\mathbf{x} - (\\mathbf{A}\\boldsymbol{\\mu}_z+\\boldsymbol{\\mu}) \\rVert_2^2 + \\operatorname{tr}\\!\\left(\\mathbf{A}\\,\\operatorname{diag}(\\boldsymbol{\\sigma}^2_z)\\,\\mathbf{A}^\\top\\right).$$\n- The Kullback–Leibler divergence between a diagonal Gaussian $q(\\mathbf{z}) = \\mathcal{N}(\\boldsymbol{\\mu}_z, \\operatorname{diag}(\\boldsymbol{\\sigma}^2_z))$ and the standard normal $p(\\mathbf{z}) = \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ is\n  $$\\mathrm{KL}(q\\|p) = \\tfrac{1}{2} \\sum_{i=1}^{L} \\left( \\sigma_{z,i}^2 + \\mu_{z,i}^2 - 1 - \\log \\sigma_{z,i}^2 \\right).$$\n\nTest suite:\n- Use the following three test cases. For each case, you are given $\\mathbf{x}_{\\mathrm{obs}}$, $\\mathbf{c}_{\\mathrm{obs}}$, and $\\mathbf{c}_{\\mathrm{alt}}$.\n  - Case $1$ (happy path):\n    $$\\mathbf{x}_{\\mathrm{obs}}^{(1)} = \\begin{bmatrix} 1.2\\\\ 0.3\\\\ -0.5\\\\ 0.0 \\end{bmatrix}, \\quad \\mathbf{c}_{\\mathrm{obs}}^{(1)} = \\begin{bmatrix} 1\\\\ 0\\\\ 1 \\end{bmatrix}, \\quad \\mathbf{c}_{\\mathrm{alt}}^{(1)} = \\begin{bmatrix} 1\\\\ 0\\\\ 0 \\end{bmatrix}.$$\n  - Case $2$ (batch counterfactual):\n    $$\\mathbf{x}_{\\mathrm{obs}}^{(2)} = \\begin{bmatrix} -0.2\\\\ 0.5\\\\ 1.0\\\\ -1.0 \\end{bmatrix}, \\quad \\mathbf{c}_{\\mathrm{obs}}^{(2)} = \\begin{bmatrix} 0\\\\ 1\\\\ 0 \\end{bmatrix}, \\quad \\mathbf{c}_{\\mathrm{alt}}^{(2)} = \\begin{bmatrix} 1\\\\ 0\\\\ 0 \\end{bmatrix}.$$\n  - Case $3$ (edge case with zero input):\n    $$\\mathbf{x}_{\\mathrm{obs}}^{(3)} = \\begin{bmatrix} 0.0\\\\ 0.0\\\\ 0.0\\\\ 0.0 \\end{bmatrix}, \\quad \\mathbf{c}_{\\mathrm{obs}}^{(3)} = \\begin{bmatrix} 0\\\\ 1\\\\ 1 \\end{bmatrix}, \\quad \\mathbf{c}_{\\mathrm{alt}}^{(3)} = \\begin{bmatrix} 1\\\\ 0\\\\ 1 \\end{bmatrix}.$$\n\nRequired outputs:\n- For each case $k \\in \\{1,2,3\\}$, compute:\n  - The scalar ELBO value $\\mathcal{L}(\\mathbf{x}_{\\mathrm{obs}}^{(k)}, \\mathbf{c}_{\\mathrm{obs}}^{(k)})$.\n  - The $D$-dimensional counterfactual reconstruction mean vector under $\\mathbf{c}_{\\mathrm{alt}}^{(k)}$.\n- Your program should produce a single line of output containing a list of three elements, one per test case, where each element is a two-element list of the form $[\\text{ELBO}, [\\text{cf}_1,\\dots,\\text{cf}_D]]$, with all floating-point numbers rounded to exactly $6$ decimal places. For example:\n  $$[[\\ell_1,[r_{1,1},r_{1,2},r_{1,3},r_{1,4}]],[\\ell_2,[\\dots]],[\\ell_3,[\\dots]]].$$\n\nImplementation constraints:\n- No randomness or sampling is allowed; use only closed-form expectations based on the given linear-Gaussian identities.\n- The code must be entirely self-contained and must not read any input.\n- Use only the Python standard library and the specified numerical libraries.",
            "solution": "The problem is evaluated as valid, being scientifically grounded, well-posed, and objective. It specifies a linear-Gaussian Conditional Variational Autoencoder (CVAE) and requires the computation of the Evidence Lower Bound (ELBO) and a counterfactual reconstruction mean. All necessary parameters and input data are provided, and the required quantities can be derived in closed form. We shall proceed with the analytical derivation followed by the implementation.\n\nThe objective is to compute two quantities for each test case, which provides an observed data vector $\\mathbf{x}_{\\mathrm{obs}}$, an observed conditioning vector $\\mathbf{c}_{\\mathrm{obs}}$, and a counterfactual conditioning vector $\\mathbf{c}_{\\mathrm{alt}}$. The quantities are:\n1. The ELBO, $\\mathcal{L}(\\mathbf{x}_{\\mathrm{obs}}, \\mathbf{c}_{\\mathrm{obs}})$.\n2. The counterfactual reconstruction mean, $\\mathbb{E}_{q(\\mathbf{z}\\mid \\mathbf{x}_{\\mathrm{obs}}, \\mathbf{c}_{\\mathrm{obs}})}\\!\\left[\\mathbf{W}\\mathbf{z} + \\mathbf{U}\\mathbf{c}_{\\mathrm{alt}} + \\mathbf{b}\\right]$.\n\nFirst, for a given pair $(\\mathbf{x}_{\\mathrm{obs}}, \\mathbf{c}_{\\mathrm{obs}})$, we must determine the parameters of the approximate posterior distribution $q(\\mathbf{z}\\mid \\mathbf{x}_{\\mathrm{obs}}, \\mathbf{c}_{\\mathrm{obs}})$. The problem defines this as a diagonal Gaussian, $q(\\mathbf{z}\\mid \\mathbf{x}, \\mathbf{c}) = \\mathcal{N}\\!\\left(\\mathbf{z}\\,;\\, \\boldsymbol{\\mu}_z(\\mathbf{x},\\mathbf{c}),\\, \\operatorname{diag}(\\boldsymbol{\\sigma}^2_z(\\mathbf{x},\\mathbf{c}))\\right)$. The parameters are computed as follows:\n\nThe posterior mean vector $\\boldsymbol{\\mu}_z \\in \\mathbb{R}^L$ is:\n$$ \\boldsymbol{\\mu}_z(\\mathbf{x}_{\\mathrm{obs}},\\mathbf{c}_{\\mathrm{obs}}) = \\mathbf{M}\\mathbf{x}_{\\mathrm{obs}} + \\mathbf{V}\\mathbf{c}_{\\mathrm{obs}} + \\mathbf{a} $$\n\nThe posterior element-wise log-variance vector $\\log \\boldsymbol{\\sigma}^2_z \\in \\mathbb{R}^L$ is:\n$$ \\log \\boldsymbol{\\sigma}^2_z(\\mathbf{x}_{\\mathrm{obs}},\\mathbf{c}_{\\mathrm{obs}}) = \\mathbf{S}_x \\mathbf{x}_{\\mathrm{obs}} + \\mathbf{S}_c \\mathbf{c}_{\\mathrm{obs}} + \\mathbf{s}_0 $$\nFrom this, the posterior variance vector $\\boldsymbol{\\sigma}^2_z \\in \\mathbb{R}^L$ is obtained by element-wise exponentiation:\n$$ \\boldsymbol{\\sigma}^2_z(\\mathbf{x}_{\\mathrm{obs}},\\mathbf{c}_{\\mathrm{obs}}) = \\exp(\\log \\boldsymbol{\\sigma}^2_z(\\mathbf{x}_{\\mathrm{obs}},\\mathbf{c}_{\\mathrm{obs}})) $$\nFor brevity, we will denote these specific posterior parameters as $\\boldsymbol{\\mu}_z$ and $\\boldsymbol{\\sigma}_z^2$, and the corresponding distribution as $q(\\mathbf{z})$.\n\nWith the posterior parameters determined, we can now derive the expressions for the two required quantities.\n\n**1. The Evidence Lower Bound (ELBO)**\n\nThe ELBO is defined as:\n$$ \\mathcal{L}(\\mathbf{x}_{\\mathrm{obs}}, \\mathbf{c}_{\\mathrm{obs}}) = \\mathbb{E}_{q(\\mathbf{z})}\\!\\left[\\log p(\\mathbf{x}_{\\mathrm{obs}} \\mid \\mathbf{z}, \\mathbf{c}_{\\mathrm{obs}})\\right] - \\mathrm{KL}\\!\\left(q(\\mathbf{z})\\,\\|\\, p(\\mathbf{z})\\right) $$\n\nWe will compute each of the two terms separately.\n\nThe second term is the Kullback-Leibler (KL) divergence between the approximate posterior $q(\\mathbf{z}) = \\mathcal{N}(\\boldsymbol{\\mu}_z, \\operatorname{diag}(\\boldsymbol{\\sigma}_z^2))$ and the standard normal prior $p(\\mathbf{z}) = \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$. The provided formula is:\n$$ \\mathrm{KL}(q\\|p) = \\frac{1}{2} \\sum_{i=1}^{L} \\left( \\sigma_{z,i}^2 + \\mu_{z,i}^2 - 1 - \\log \\sigma_{z,i}^2 \\right) $$\nwhere $\\mu_{z,i}$ and $\\sigma_{z,i}^2$ are the elements of the vectors $\\boldsymbol{\\mu}_z$ and $\\boldsymbol{\\sigma}_z^2$ computed previously.\n\nThe first term is the expected reconstruction log-likelihood. The likelihood is given as $p(\\mathbf{x}\\mid \\mathbf{z}, \\mathbf{c}) = \\mathcal{N}\\!\\left(\\mathbf{x}\\,;\\, \\mathbf{W}\\mathbf{z} + \\mathbf{U}\\mathbf{c} + \\mathbf{b},\\, \\sigma_x^2 \\mathbf{I}\\right)$. The log-probability is:\n$$ \\log p(\\mathbf{x} \\mid \\mathbf{z}, \\mathbf{c}) = -\\frac{D}{2} \\log(2\\pi \\sigma_x^2) - \\frac{1}{2\\sigma_x^2} \\|\\mathbf{x} - (\\mathbf{W}\\mathbf{z} + \\mathbf{U}\\mathbf{c} + \\mathbf{b})\\|^2_2 $$\nWe must compute the expectation of this quantity under $q(\\mathbf{z})$. The first term is a constant with respect to $\\mathbf{z}$.\n$$ \\mathbb{E}_{q(\\mathbf{z})}\\!\\left[\\log p(\\mathbf{x}_{\\mathrm{obs}} \\mid \\mathbf{z}, \\mathbf{c}_{\\mathrm{obs}})\\right] = -\\frac{D}{2} \\log(2\\pi \\sigma_x^2) - \\frac{1}{2\\sigma_x^2} \\mathbb{E}_{q(\\mathbf{z})}\\!\\left[ \\|\\mathbf{x}_{\\mathrm{obs}} - (\\mathbf{W}\\mathbf{z} + \\mathbf{U}\\mathbf{c}_{\\mathrm{obs}} + \\mathbf{b})\\|^2_2 \\right] $$\nTo evaluate the expectation of the squared norm, we use the provided identity: $\\mathbb{E}_q\\!\\left[\\lVert \\mathbf{x} - (\\mathbf{A}\\mathbf{z}+\\boldsymbol{\\mu}) \\rVert_2^2\\right] = \\lVert \\mathbf{x} - (\\mathbf{A}\\boldsymbol{\\mu}_z+\\boldsymbol{\\mu}) \\rVert_2^2 + \\operatorname{tr}\\!\\left(\\mathbf{A}\\,\\operatorname{diag}(\\boldsymbol{\\sigma}^2_z)\\,\\mathbf{A}^\\top\\right)$. We make the substitutions: $\\mathbf{x} \\rightarrow \\mathbf{x}_{\\mathrm{obs}}$, $\\mathbf{A} \\rightarrow \\mathbf{W}$, and $\\boldsymbol{\\mu} \\rightarrow \\mathbf{U}\\mathbf{c}_{\\mathrm{obs}} + \\mathbf{b}$. The expectation becomes:\n$$ \\|\\mathbf{x}_{\\mathrm{obs}} - (\\mathbf{W}\\boldsymbol{\\mu}_z + \\mathbf{U}\\mathbf{c}_{\\mathrm{obs}} + \\mathbf{b})\\|^2_2 + \\operatorname{tr}\\!\\left(\\mathbf{W}\\,\\operatorname{diag}(\\boldsymbol{\\sigma}^2_z)\\,\\mathbf{W}^\\top\\right) $$\nThe first term is the squared Euclidean distance between the input $\\mathbf{x}_{\\mathrm{obs}}$ and its reconstruction from the posterior mean. The trace term can be simplified using the cyclic property of the trace and the diagonal nature of the covariance matrix $\\boldsymbol{\\Sigma}_z = \\operatorname{diag}(\\boldsymbol{\\sigma}^2_z)$:\n$$ \\operatorname{tr}(\\mathbf{W} \\boldsymbol{\\Sigma}_z \\mathbf{W}^\\top) = \\operatorname{tr}(\\mathbf{W}^\\top \\mathbf{W} \\boldsymbol{\\Sigma}_z) = \\sum_{i=1}^L (\\mathbf{W}^\\top \\mathbf{W})_{ii} \\sigma_{z,i}^2 = \\sum_{i=1}^L \\|\\mathbf{W}_{:,i}\\|_2^2 \\sigma_{z,i}^2 $$\nwhere $\\mathbf{W}_{:,i}$ is the $i$-th column of $\\mathbf{W}$.\n\nCombining everything, the ELBO is:\n$$ \\mathcal{L} = \\left[ -\\frac{D}{2}\\log(2\\pi\\sigma_x^2) - \\frac{1}{2\\sigma_x^2}\\left( \\|\\mathbf{x}_{\\mathrm{obs}} - (\\mathbf{W}\\boldsymbol{\\mu}_z + \\mathbf{U}\\mathbf{c}_{\\mathrm{obs}} + \\mathbf{b})\\|^2_2 + \\sum_{i=1}^L \\|\\mathbf{W}_{:,i}\\|_2^2 \\sigma_{z,i}^2 \\right) \\right] - \\left[ \\frac{1}{2}\\sum_{i=1}^L(\\mu_{z,i}^2 + \\sigma_{z,i}^2 - 1 - \\log\\sigma_{z,i}^2) \\right] $$\n\n**2. The Counterfactual Reconstruction Mean**\n\nThe second quantity to be computed is the mean of the reconstructed data distribution, where the latent variable $\\mathbf{z}$ is drawn from the posterior inferred from the observed data $(\\mathbf{x}_{\\mathrm{obs}}, \\mathbf{c}_{\\mathrm{obs}})$, but the decoding is performed using the altered condition $\\mathbf{c}_{\\mathrm{alt}}$.\n$$ \\mathbf{x}_{\\mathrm{cf}} = \\mathbb{E}_{q(\\mathbf{z}\\mid \\mathbf{x}_{\\mathrm{obs}}, \\mathbf{c}_{\\mathrm{obs}})}\\!\\left[ \\mathbf{W}\\mathbf{z} + \\mathbf{U}\\mathbf{c}_{\\mathrm{alt}} + \\mathbf{b} \\right] $$\nBy linearity of expectation, and since $\\mathbf{c}_{\\mathrm{alt}}$ and $\\mathbf{b}$ are constant with respect to the expectation over $\\mathbf{z}$:\n$$ \\mathbf{x}_{\\mathrm{cf}} = \\mathbf{W} \\mathbb{E}_{q(\\mathbf{z})}[\\mathbf{z}] + \\mathbf{U}\\mathbf{c}_{\\mathrm{alt}} + \\mathbf{b} $$\nThe expectation of $\\mathbf{z}$ under $q(\\mathbf{z})$ is simply its mean, $\\boldsymbol{\\mu}_z$. Therefore, the counterfactual reconstruction mean is:\n$$ \\mathbf{x}_{\\mathrm{cf}} = \\mathbf{W}\\boldsymbol{\\mu}_z + \\mathbf{U}\\mathbf{c}_{\\mathrm{alt}} + \\mathbf{b} $$\nwhere $\\boldsymbol{\\mu}_z$ is the posterior mean computed from $(\\mathbf{x}_{\\mathrm{obs}}, \\mathbf{c}_{\\mathrm{obs}})$.\n\nThe implementation will follow these derived closed-form expressions for each test case. All vector and matrix operations will be carried out using the `numpy` library.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the CVAE problem by computing the ELBO and counterfactual reconstruction mean\n    for three test cases based on the provided linear-Gaussian model.\n    \"\"\"\n    \n    # Model dimensions\n    L = 2  # Latent dimension\n    D = 4  # Data dimension\n    C = 3  # Condition dimension\n    \n    # Decoder parameters\n    W = np.array([\n        [0.8, -0.3],\n        [0.1, 0.5],\n        [-0.4, 0.2],\n        [0.0, 0.3]\n    ])\n    U = np.array([\n        [0.6, -0.2, 0.3],\n        [-0.1, 0.4, -0.5],\n        [0.2, 0.1, -0.2],\n        [0.5, -0.3, 0.2]\n    ])\n    b = np.array([0.05, -0.02, 0.01, 0.0]).reshape(D, 1)\n    sigma_x = 0.1\n    \n    # Encoder parameters\n    M = np.array([\n        [0.2, 0.0, -0.1, 0.3],\n        [-0.2, 0.1, 0.4, -0.1]\n    ])\n    V = np.array([\n        [0.1, -0.2, 0.3],\n        [0.0, 0.2, -0.1]\n    ])\n    a = np.array([0.05, -0.05]).reshape(L, 1)\n    \n    S_x = np.array([\n        [-0.1, 0.2, 0.0, -0.2],\n        [0.1, -0.1, 0.3, 0.0]\n    ])\n    S_c = np.array([\n        [0.05, 0.05, -0.1],\n        [-0.05, 0.1, 0.0]\n    ])\n    s_0 = np.array([-1.0, -1.2]).reshape(L, 1)\n    \n    # Test suite\n    test_cases = [\n        {\n            \"x_obs\": np.array([1.2, 0.3, -0.5, 0.0]).reshape(D, 1),\n            \"c_obs\": np.array([1, 0, 1]).reshape(C, 1),\n            \"c_alt\": np.array([1, 0, 0]).reshape(C, 1)\n        },\n        {\n            \"x_obs\": np.array([-0.2, 0.5, 1.0, -1.0]).reshape(D, 1),\n            \"c_obs\": np.array([0, 1, 0]).reshape(C, 1),\n            \"c_alt\": np.array([1, 0, 0]).reshape(C, 1)\n        },\n        {\n            \"x_obs\": np.array([0.0, 0.0, 0.0, 0.0]).reshape(D, 1),\n            \"c_obs\": np.array([0, 1, 1]).reshape(C, 1),\n            \"c_alt\": np.array([1, 0, 1]).reshape(C, 1)\n        }\n    ]\n    \n    results = []\n    \n    for case in test_cases:\n        x_obs = case[\"x_obs\"]\n        c_obs = case[\"c_obs\"]\n        c_alt = case[\"c_alt\"]\n        \n        # 1. Compute posterior parameters (mu_z, sigma_z^2)\n        # Use observed data and conditions\n        mu_z = M @ x_obs + V @ c_obs + a\n        log_sigma_z_sq = S_x @ x_obs + S_c @ c_obs + s_0\n        sigma_z_sq = np.exp(log_sigma_z_sq)\n        \n        # 2. Compute the KL divergence term of the ELBO\n        # KL(q(z|x,c) || p(z))\n        kl_div = 0.5 * np.sum(sigma_z_sq + mu_z**2 - 1 - log_sigma_z_sq)\n        \n        # 3. Compute the expected log-likelihood term of the ELBO\n        # E_q[log p(x|z,c)]\n        \n        # Term 1: Reconstruction error from posterior mean\n        recon_mean_obs = W @ mu_z + U @ c_obs + b\n        recon_error_sq_norm = np.sum((x_obs - recon_mean_obs)**2)\n        \n        # Term 2: Trace term from variance propagation\n        w_col_sq_norms = np.sum(W**2, axis=0) # shape (L,)\n        trace_term = np.sum(w_col_sq_norms * sigma_z_sq.flatten())\n        \n        # Constant from Gaussian PDF\n        log_p_constant = -D / 2.0 * np.log(2 * np.pi * sigma_x**2)\n        \n        # Combine parts for expected log-likelihood\n        expected_log_p = log_p_constant - (1 / (2 * sigma_x**2)) * (recon_error_sq_norm + trace_term)\n        \n        # 4. Compute the ELBO\n        elbo = expected_log_p - kl_div\n        \n        # 5. Compute the counterfactual reconstruction mean\n        # E_q[decoder(z, c_alt)]\n        cf_recon_mean = W @ mu_z + U @ c_alt + b\n        \n        # Format the results as required\n        elbo_rounded = round(elbo, 6)\n        cf_recon_mean_list = [round(x, 6) for x in cf_recon_mean.flatten()]\n        \n        results.append([elbo_rounded, cf_recon_mean_list])\n        \n    # Print the final result in the exact required format\n    # Manual string construction to avoid spaces added by standard list-to-string conversion\n    result_strings = []\n    for elbo_val, cf_mean_list in results:\n        cf_mean_str = f\"[{','.join(f'{x:.6f}' for x in cf_mean_list)}]\"\n        result_strings.append(f\"[{elbo_val:.6f},{cf_mean_str}]\")\n    \n    final_output = f\"[{','.join(result_strings)}]\"\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}