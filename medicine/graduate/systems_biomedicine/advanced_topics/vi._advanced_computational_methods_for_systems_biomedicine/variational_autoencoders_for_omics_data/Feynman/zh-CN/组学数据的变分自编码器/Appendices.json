{
    "hands_on_practices": [
        {
            "introduction": "变分自编码器（VAE）的核心在于其目标函数，即证据下界（ELBO）。深刻理解ELBO是掌握VAE的第一步。本练习将引导你从第一性原理出发，为一个具有高斯编码器和高斯解码器的标准VAE推导出ELBO的解析表达式。通过这个过程，你将清晰地看到ELBO的两个关键组成部分——重构损失和KL散度正则化项——是如何从模型的概率定义中自然产生的，从而为后续更复杂的应用打下坚实的数学基础。",
            "id": "4397996",
            "problem": "考虑一个单一组学样本，它由一个通过高通量RNA测序 (RNA-seq) 获得的对数归一化基因表达向量 $x \\in \\mathbb{R}^{d}$ 表示。一个变分自编码器 (VAE) 被指定，其潜变量为 $z \\in \\mathbb{R}^{k}$，先验分布为标准高斯分布 $p(z) = \\mathcal{N}(z; 0, I)$。生成模型 (解码器) 为 $p_{\\theta}(x \\mid z) = \\mathcal{N}(x; \\mu(z), \\sigma^{2} I)$，其中均值 $\\mu(z) = W z + b$ 是 $z$ 的线性函数，$W \\in \\mathbb{R}^{d \\times k}$ 且 $b \\in \\mathbb{R}^{d}$，标量观测方差 $\\sigma^{2} > 0$ 是已知的。推断模型 (编码器) 为 $q_{\\phi}(z \\mid x) = \\mathcal{N}(z; \\mu_{\\phi}(x), \\operatorname{diag}(\\sigma_{\\phi}^{2}(x)))$，其均值为 $\\mu_{\\phi}(x) \\in \\mathbb{R}^{k}$，逐元素方差为 $\\sigma_{\\phi}^{2}(x) \\in \\mathbb{R}_{>0}^{k}$。\n\n从证据下界 (ELBO) 的定义出发——即编码器下的对数似然期望与编码器和先验之间的 Kullback-Leibler 散度之差，为给定的样本 $x$ 推导出一个关于 $x$、$W$、$b$、$\\sigma^{2}$、$\\mu_{\\phi}(x)$ 和 $\\sigma_{\\phi}^{2}(x)$ 以及 $d$ 和 $k$ 的闭式解析表达式。您的推导必须从多元正态分布的概率密度和 Kullback-Leibler 散度的基本定义开始，并且不得使用任何简便公式。请将您的最终答案表示为单个闭式解析表达式。无需进行数值取整。",
            "solution": "该问题是有效的。它提出了一个概率机器学习中标准的、适定的任务，提供了所有必要的信息和定义，没有科学或逻辑上的矛盾。我们开始进行推导。\n\n证据下界 (ELBO) 的定义如下：\n$$\n\\mathcal{L}(x; \\theta, \\phi) = \\mathbb{E}_{q_{\\phi}(z \\mid x)}[\\log p_{\\theta}(x \\mid z)] - D_{KL}(q_{\\phi}(z \\mid x) \\| p(z))\n$$\n我们将分别推导等号右侧的两项。\n\n**第一部分：对数似然期望项**\n\n第一项是在近似后验 $q_{\\phi}(z \\mid x)$ 下数据的对数似然期望。生成模型 (解码器) 由 $p_{\\theta}(x \\mid z) = \\mathcal{N}(x; W z + b, \\sigma^{2} I)$ 给出。\n\n这个 $d$ 维多元正态分布的概率密度函数 (PDF) 是：\n$$\np_{\\theta}(x \\mid z) = \\frac{1}{(2\\pi)^{d/2} \\det(\\sigma^2 I)^{1/2}} \\exp\\left(-\\frac{1}{2}(x - (Wz+b))^T (\\sigma^2 I)^{-1} (x - (Wz+b))\\right)\n$$\n协方差矩阵的行列式为 $\\det(\\sigma^2 I) = (\\sigma^2)^d$。其逆矩阵为 $(\\sigma^2 I)^{-1} = \\frac{1}{\\sigma^2}I$。将这些代入可得：\n$$\np_{\\theta}(x \\mid z) = \\frac{1}{(2\\pi \\sigma^2)^{d/2}} \\exp\\left(-\\frac{1}{2\\sigma^2}\\|x - Wz - b\\|_2^2\\right)\n$$\n取自然对数，我们得到：\n$$\n\\log p_{\\theta}(x \\mid z) = -\\frac{d}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\|x - Wz - b\\|_2^2\n$$\n现在，我们必须计算这个量在编码器分布 $q_{\\phi}(z \\mid x) = \\mathcal{N}(z; \\mu_{\\phi}(x), \\operatorname{diag}(\\sigma_{\\phi}^{2}(x)))$ 下的期望。为简化符号，令 $\\mu_q = \\mu_{\\phi}(x)$ 和 $\\Sigma_q = \\operatorname{diag}(\\sigma_{\\phi}^{2}(x))$。\n$$\n\\mathbb{E}_{q_{\\phi}(z \\mid x)}[\\log p_{\\theta}(x \\mid z)] = \\mathbb{E}_{q_{\\phi}}\\left[-\\frac{d}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\|x - Wz - b\\|_2^2\\right]\n$$\n根据期望的线性性，上式变为：\n$$\n\\mathbb{E}_{q_{\\phi}}[\\log p_{\\theta}(x \\mid z)] = -\\frac{d}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\mathbb{E}_{q_{\\phi}}[\\|x - Wz - b\\|_2^2]\n$$\n我们需要计算平方范数的期望。让我们展开期望内的项：\n$$\n\\|x - Wz - b\\|_2^2 = (x - b - Wz)^T(x - b - Wz) = (x-b)^T(x-b) - 2(x-b)^T W z + z^T W^T W z\n$$\n现在，我们计算关于 $z \\sim q_{\\phi}$ 的期望：\n$$\n\\mathbb{E}_{q_{\\phi}}[\\|x - Wz - b\\|_2^2] = \\mathbb{E}_{q_{\\phi}}[(x-b)^T(x-b)] - 2(x-b)^T W \\mathbb{E}_{q_{\\phi}}[z] + \\mathbb{E}_{q_{\\phi}}[z^T W^T W z]\n$$\n我们知道 $\\mathbb{E}_{q_{\\phi}}[z] = \\mu_q$。对于二次项，我们使用恒等式 $\\mathbb{E}[y^T A y] = \\text{Tr}(A \\text{Cov}[y]) + \\mathbb{E}[y]^T A \\mathbb{E}[y]$。这里，$y=z$，$A = W^T W$，$\\mathbb{E}[z]=\\mu_q$，以及 $\\text{Cov}[z]=\\Sigma_q$。\n$$\n\\mathbb{E}_{q_{\\phi}}[z^T W^T W z] = \\text{Tr}(W^T W \\Sigma_q) + \\mu_q^T W^T W \\mu_q\n$$\n将这些代回，期望的平方范数为：\n$$\n\\mathbb{E}_{q_{\\phi}}[\\|x - Wz - b\\|_2^2] = \\|x-b\\|_2^2 - 2(x-b)^T W \\mu_q + \\mu_q^T W^T W \\mu_q + \\text{Tr}(W^T W \\Sigma_q)\n$$\n前三项可以重组为一个平方范数：\n$$\n\\|x-b\\|_2^2 - 2(x-b)^T W \\mu_q + \\|W\\mu_q\\|_2^2 = \\|(x-b) - W\\mu_q\\|_2^2 = \\|x - W\\mu_q - b\\|_2^2\n$$\n所以，我们有：\n$$\n\\mathbb{E}_{q_{\\phi}}[\\|x - Wz - b\\|_2^2] = \\|x - W\\mu_q - b\\|_2^2 + \\text{Tr}(W^T W \\Sigma_q)\n$$\n迹项可以明确地写出来。令 $w_j$ 为 $W$ 的第 $j$ 列，$(\\sigma_q^2)_j$ 为方差向量 $\\sigma_{\\phi}^2(x)$ 的第 $j$ 个分量。\n$$\n\\text{Tr}(W^T W \\Sigma_q) = \\sum_{j=1}^k (W^T W \\Sigma_q)_{jj} = \\sum_{j=1}^k \\left(\\sum_{l=1}^k (W^T W)_{jl} (\\Sigma_q)_{lj}\\right)\n$$\n由于 $\\Sigma_q$ 是对角矩阵，$(\\Sigma_q)_{lj} = (\\sigma_q^2)_j \\delta_{lj}$。\n$$\n\\text{Tr}(W^T W \\Sigma_q) = \\sum_{j=1}^k (W^T W)_{jj} (\\sigma_q^2)_j = \\sum_{j=1}^k \\left(\\sum_{i=1}^d W_{ij}^2\\right) (\\sigma_q^2)_j = \\sum_{j=1}^k \\|w_j\\|_2^2 (\\sigma_q^2)_j\n$$\n最后，将其代回，得到 ELBO 的第一项：\n$$\n\\mathbb{E}_{q_{\\phi}(z \\mid x)}[\\log p_{\\theta}(x \\mid z)] = -\\frac{d}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\left( \\|x - W\\mu_{\\phi}(x) - b\\|_2^2 + \\sum_{j=1}^k \\|w_j\\|_2^2 (\\sigma_{\\phi}^2(x))_j \\right)\n$$\n\n**第二部分：Kullback-Leibler 散度项**\n\n第二项是编码器分布 $q_{\\phi}(z \\mid x) = \\mathcal{N}(z; \\mu_q, \\Sigma_q)$ 与先验分布 $p(z) = \\mathcal{N}(z; 0, I)$ 之间的 KL 散度。\nKL 散度的定义为：\n$$\nD_{KL}(q_{\\phi} \\| p) = \\int q_{\\phi}(z \\mid x) \\log \\frac{q_{\\phi}(z \\mid x)}{p(z)} dz = \\mathbb{E}_{q_{\\phi}}[\\log q_{\\phi}(z \\mid x) - \\log p(z)]\n$$\n由于 $q_{\\phi}$ 和 $p$ 都是对角协方差高斯分布，它们可以在 $z$ 的维度上分解。因此，KL 散度是每个维度的 KL 散度之和：\n$$\nD_{KL}(q_{\\phi} \\| p) = \\sum_{j=1}^k D_{KL}(\\mathcal{N}(z_j; (\\mu_q)_j, (\\sigma_q^2)_j) \\| \\mathcal{N}(z_j; 0, 1))\n$$\n对于单个维度 $j$，我们有 $q_j(z_j) = \\mathcal{N}(z_j; (\\mu_q)_j, (\\sigma_q^2)_j)$ 和 $p_j(z_j) = \\mathcal{N}(z_j; 0, 1)$。它们的对数概率密度函数为：\n$$\n\\log q_j(z_j) = -\\frac{1}{2}\\log(2\\pi(\\sigma_q^2)_j) - \\frac{(z_j - (\\mu_q)_j)^2}{2(\\sigma_q^2)_j}\n$$\n$$\n\\log p_j(z_j) = -\\frac{1}{2}\\log(2\\pi) - \\frac{z_j^2}{2}\n$$\n两者的差为：\n$$\n\\log q_j(z_j) - \\log p_j(z_j) = -\\frac{1}{2}\\log((\\sigma_q^2)_j) - \\frac{(z_j - (\\mu_q)_j)^2}{2(\\sigma_q^2)_j} + \\frac{z_j^2}{2}\n$$\n计算关于 $z_j \\sim q_j(z_j)$ 的期望：\n$$\nD_{KL}(q_j \\| p_j) = \\mathbb{E}_{q_j}\\left[ -\\frac{1}{2}\\log((\\sigma_q^2)_j) - \\frac{(z_j - (\\mu_q)_j)^2}{2(\\sigma_q^2)_j} + \\frac{z_j^2}{2} \\right]\n$$\n利用期望的线性性：\n$$\nD_{KL}(q_j \\| p_j) = -\\frac{1}{2}\\log((\\sigma_q^2)_j) - \\frac{1}{2(\\sigma_q^2)_j}\\mathbb{E}_{q_j}[(z_j - (\\mu_q)_j)^2] + \\frac{1}{2}\\mathbb{E}_{q_j}[z_j^2]\n$$\n我们有 $\\mathbb{E}_{q_j}[(z_j - (\\mu_q)_j)^2] = \\text{Var}_{q_j}(z_j) = (\\sigma_q^2)_j$，并且 $\\mathbb{E}_{q_j}[z_j^2] = \\text{Var}_{q_j}(z_j) + (\\mathbb{E}_{q_j}[z_j])^2 = (\\sigma_q^2)_j + ((\\mu_q)_j)^2$。\n$$\nD_{KL}(q_j \\| p_j) = -\\frac{1}{2}\\log((\\sigma_q^2)_j) - \\frac{(\\sigma_q^2)_j}{2(\\sigma_q^2)_j} + \\frac{(\\sigma_q^2)_j + ((\\mu_q)_j)^2}{2}\n$$\n$$\nD_{KL}(q_j \\| p_j) = \\frac{1}{2} \\left[ -\\log((\\sigma_q^2)_j) - 1 + (\\sigma_q^2)_j + ((\\mu_q)_j)^2 \\right]\n$$\n对所有 $k$ 个维度求和：\n$$\nD_{KL}(q_{\\phi}(z \\mid x) \\| p(z)) = \\frac{1}{2} \\sum_{j=1}^k \\left( ((\\mu_{\\phi}(x))_j)^2 + (\\sigma_{\\phi}^2(x))_j - \\log((\\sigma_{\\phi}^2(x))_j) - 1 \\right)\n$$\n\n**ELBO 的最终表达式**\n\n结合这两个部分，ELBO = (第一部分) - (第二部分)：\n$$\n\\mathcal{L}(x; \\theta, \\phi) = \\left[ -\\frac{d}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\left( \\|x - W\\mu_{\\phi}(x) - b\\|_2^2 + \\sum_{j=1}^k \\|w_j\\|_2^2 (\\sigma_{\\phi}^2(x))_j \\right) \\right] - \\left[ \\frac{1}{2} \\sum_{j=1}^k \\left( ((\\mu_{\\phi}(x))_j)^2 + (\\sigma_{\\phi}^2(x))_j - \\log((\\sigma_{\\phi}^2(x))_j) - 1 \\right) \\right]\n$$\n整理各项得到最终的单一表达式：\n$$\n\\mathcal{L}(x; \\theta, \\phi) = -\\frac{d}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\|x - W\\mu_{\\phi}(x) - b\\|_2^2 - \\frac{1}{2\\sigma^2}\\sum_{j=1}^k \\|w_j\\|_2^2 (\\sigma_{\\phi}^2(x))_j + \\frac{1}{2} \\sum_{j=1}^k \\left(1 - ((\\mu_{\\phi}(x))_j)^2 - (\\sigma_{\\phi}^2(x))_j + \\log((\\sigma_{\\phi}^2(x))_j) \\right)\n$$\n其中 $w_j$ 是矩阵 $W$ 的第 $j$ 列。",
            "answer": "$$ \\boxed{-\\frac{d}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\|x - W\\mu_{\\phi}(x) - b\\|_2^2 - \\frac{1}{2\\sigma^2}\\sum_{j=1}^k \\|w_j\\|_2^2 (\\sigma_{\\phi}^2(x))_j + \\frac{1}{2} \\sum_{j=1}^k \\left(1 + \\log((\\sigma_{\\phi}^2(x))_j) - (\\mu_{\\phi}(x))_j^2 - (\\sigma_{\\phi}^2(x))_j \\right)} $$"
        },
        {
            "introduction": "标准的VAE通常为近似后验分布$q_{\\phi}(z \\mid x)$使用对角协方差矩阵，这隐含了潜在变量相互独立的假设，而对于复杂的生物系统来说，这一假设往往过于苛刻。本实践将探讨一种更具表达能力的协方差结构：低秩加对角矩阵。你将通过编程实现这种结构，并量化其相对于对角协方差在ELBO上带来的改进。这项编码练习不仅能让你学会如何构建更灵活的模型以突破默认假设的限制，还能让你掌握如何运用矩阵行列式引理等高效的数值计算技巧，使复杂模型在计算上变得可行。",
            "id": "4397881",
            "problem": "考虑一个用于高维生物测量（例如，转录组学谱）的线性高斯潜变量模型，其中潜在表示捕获了潜在的生物学过程。设潜在变量为 $z \\in \\mathbb{R}^k$，观测数据为 $x \\in \\mathbb{R}^p$，并定义一个具有标准正态先验和各向同性高斯似然的生成模型：先验为 $p(z) = \\mathcal{N}(0, I_k)$，似然为 $p(x \\mid z) = \\mathcal{N}(W z + b, \\sigma_x^2 I_p)$，其中 $W \\in \\mathbb{R}^{p \\times k}$，$b \\in \\mathbb{R}^p$，且 $\\sigma_x^2 > 0$。在变分自编码器 (VAE) 中，近似后验 $q(z \\mid x)$ 通常被参数化为一个均值为 $m \\in \\mathbb{R}^k$、协方差为 $\\Sigma \\in \\mathbb{R}^{k \\times k}$ 的多元高斯分布，单个观测的证据下界 (ELBO) 目标定义为期望对数似然减去与先验的 Kullback–Leibler 散度。在许多高通量组学应用中，存在潜在相关性，对角协方差可能限制性过强。一个计算上更高效的替代方案是“低秩加对角”协方差参数化 $\\Sigma = \\operatorname{diag}(d) + U U^\\top$，其中 $d \\in \\mathbb{R}^k$ 对所有 $i$ 满足 $d_i > 0$，$U \\in \\mathbb{R}^{k \\times r}$ 且 $r \\ll k$。\n\n您的任务是实现一个程序，针对一组指定的测试用例，构建两个共享相同均值 $m$ 但协方差结构不同的变分族：(i) 对角协方差 $\\Sigma_{\\mathrm{diag}} = \\operatorname{diag}(d)$ 和 (ii) 低秩加对角协方差 $\\Sigma_{\\mathrm{lr}} = \\operatorname{diag}(d) + U U^\\top$。对于每个测试用例，您必须计算这两个变分族的 ELBO，并报告 ELBO 提升量，定义为标量 $I = \\mathrm{ELBO}(\\Sigma_{\\mathrm{lr}}) - \\mathrm{ELBO}(\\Sigma_{\\mathrm{diag}})$。\n\n在所有测试用例中，使用以下基础设置：\n- 偏置为 $b = 0$。\n- 两个变分族使用的均值 $m$ 是在给定的 $W$, $\\sigma_x^2$, 和 $x$ 下，指定线性高斯生成模型的精确贝叶斯后验均值。\n- 对角项 $d \\in \\mathbb{R}^k$ 通过 $d_i = 0.1 + 0.02 \\times i$ 确定性地固定，其中 $i \\in \\{1, 2, \\dots, k\\}$。\n- 低秩因子 $U \\in \\mathbb{R}^{k \\times r}$ 从均值为零、标准差为 $0.3$ 的独立同分布高斯分布中生成。\n- 负载矩阵 $W \\in \\mathbb{R}^{p \\times k}$ 从均值为零、标准差为 $1/\\sqrt{k}$ 的独立同分布高斯分布中生成。\n- 观测向量 $x \\in \\mathbb{R}^p$ 通过采样 $z_{\\mathrm{true}} \\sim \\mathcal{N}(0, I_k)$ 和噪声 $\\epsilon \\sim \\mathcal{N}(0, \\sigma_x^2 I_p)$ 生成，然后设置 $x = W z_{\\mathrm{true}} + \\epsilon$。\n\n所有随机抽样必须使用为每个测试用例指定的种子初始化的伪随机数生成器来执行，以确保可复现性。不涉及角度，也没有物理单位。对于每个测试用例，ELBO 提升量 $I$ 必须作为实数返回。\n\n程序必须实现适用于低秩加对角参数化的行列式和迹的数值稳定计算。具体来说，$\\Sigma_{\\mathrm{lr}}$ 的行列式应使用适合对角矩阵的秩-r 更新的数值稳定方法来计算，并且迹项必须在可避免时，不构建稠密的中间矩阵进行评估。\n\n测试套件：\n- 测试用例 1：$k = 8$，$p = 50$，$r = 3$，$\\sigma_x^2 = 0.25$，seed $= 42$。\n- 测试用例 2：$k = 8$，$p = 50$，$r = 0$，$\\sigma_x^2 = 0.25$，seed $= 42$。\n- 测试用例 3：$k = 8$，$p = 50$，$r = 3$，$\\sigma_x^2 = 5.0$，seed $= 7$。\n- 测试用例 4：$k = 8$，$p = 50$，$r = 5$，$\\sigma_x^2 = 0.05$，seed $= 13$。\n- 测试用例 5：$k = 30$，$p = 200$，$r = 5$，$\\sigma_x^2 = 0.5$，seed $= 101$。\n\n您的程序应生成一行输出，其中包含所有测试用例的 ELBO 提升结果，格式为方括号括起来的逗号分隔列表。例如，如果计算出的提升量为 $I_1$, $I_2$, $I_3$, $I_4$, 和 $I_5$，程序必须在一行上精确打印字符串 $[I_1,I_2,I_3,I_4,I_5]$。每个 $I_j$ 必须是十进制形式表示的实数。期望的输出是浮点数，不应打印任何其他文本。",
            "solution": "任务是计算证据下界 (ELBO) 的提升量，定义为 $I = \\mathrm{ELBO}(\\Sigma_{\\mathrm{lr}}) - \\mathrm{ELBO}(\\Sigma_{\\mathrm{diag}})$，针对两个变分分布。这些分布共享一个共同的均值 $m$，但具有不同的协方差结构：对角协方差 $\\Sigma_{\\mathrm{diag}} = \\operatorname{diag}(d)$ 和低秩加对角协方差 $\\Sigma_{\\mathrm{lr}} = \\operatorname{diag}(d) + UU^\\top$。\n\n单个观测 $x$ 的 ELBO 定义为 $\\mathrm{ELBO} = \\mathbb{E}_{q(z \\mid x)}[\\log p(x \\mid z)] - D_{KL}(q(z \\mid x) \\| p(z))$。近似后验为 $q(z \\mid x) = \\mathcal{N}(z; m, \\Sigma)$，先验为 $p(z) = \\mathcal{N}(z; 0, I_k)$，似然为 $p(x \\mid z) = \\mathcal{N}(x; Wz, \\sigma_x^2 I_p)$，因为指定的偏置 $b$ 为 0。\n\n首先，我们推导对于一个通用的变分协方差 $\\Sigma$，ELBO 两个组成部分的表达式。近似后验 $q(z \\mid x) = \\mathcal{N}(m, \\Sigma)$ 和先验 $p(z) = \\mathcal{N}(0, I_k)$ 之间的 Kullback-Leibler 散度由标准公式给出：\n$$\nD_{KL}(q(z \\mid x) \\| p(z)) = \\frac{1}{2} \\left( \\mathrm{Tr}(\\Sigma) + m^\\top m - k - \\log \\det(\\Sigma) \\right)\n$$\n接下来，我们推导期望对数似然项。对数似然函数是：\n$$\n\\log p(x \\mid z) = -\\frac{1}{2\\sigma_x^2} \\|x - Wz\\|_2^2 - \\frac{p}{2}\\log(2\\pi\\sigma_x^2)\n$$\n对 $q(z \\mid x)$ 取期望，得到：\n$$\n\\mathbb{E}_q[\\log p(x \\mid z)] = -\\frac{1}{2\\sigma_x^2} \\mathbb{E}_q[\\|x - Wz\\|_2^2] - \\frac{p}{2}\\log(2\\pi\\sigma_x^2)\n$$\n平方范数的期望值可以展开为 $\\mathbb{E}_q[(x-Wz)^\\top(x-Wz)] = \\mathbb{E}_q[x^\\top x - 2x^\\top Wz + z^\\top W^\\top Wz]$。使用期望的线性和二次型的迹技巧 $\\mathbb{E}_q[z^\\top A z] = \\mathrm{Tr}(A \\cdot \\mathrm{Cov}_q[z]) + (\\mathbb{E}_q[z])^\\top A \\mathbb{E}_q[z]$，我们得到：\n$$\n\\mathbb{E}_q[\\|x - Wz\\|_2^2] = \\|x - Wm\\|_2^2 + \\mathrm{Tr}(W^\\top W \\Sigma)\n$$\n所以期望对数似然是：\n$$\n\\mathbb{E}_q[\\log p(x \\mid z)] = -\\frac{1}{2\\sigma_x^2} \\left( \\|x - Wm\\|_2^2 + \\mathrm{Tr}(W^\\top W \\Sigma) \\right) - \\frac{p}{2}\\log(2\\pi\\sigma_x^2)\n$$\n结合这些部分，ELBO作为协方差 $\\Sigma$ 的函数是：\n$$\n\\mathrm{ELBO}(\\Sigma) = -\\frac{1}{2\\sigma_x^2} \\left( \\|x - Wm\\|_2^2 + \\mathrm{Tr}(W^\\top W \\Sigma) \\right) - \\frac{p}{2}\\log(2\\pi\\sigma_x^2) - \\frac{1}{2} \\left( \\mathrm{Tr}(\\Sigma) + m^\\top m - k - \\log \\det(\\Sigma) \\right)\n$$\n我们需要计算提升量 $I = \\mathrm{ELBO}(\\Sigma_{\\mathrm{lr}}) - \\mathrm{ELBO}(\\Sigma_{\\mathrm{diag}})$。由于两个变分族的均值 $m$ 相同，所有不依赖于 $\\Sigma$ 的项都将被抵消。这些项是 $-\\frac{1}{2\\sigma_x^2}\\|x - Wm\\|_2^2$, $-\\frac{p}{2}\\log(2\\pi\\sigma_x^2)$, $-\\frac{1}{2}m^\\top m$, 和 $\\frac{k}{2}$。这种抵消是一个关键的简化，因为它意味着 $I$ 的最终结果不依赖于观测数据 $x$ 或后验均值 $m$。\n\nELBO 中依赖于 $\\Sigma$ 的部分是：\n$$\n\\mathrm{ELBO}_{\\text{dep}}(\\Sigma) = -\\frac{1}{2\\sigma_x^2}\\mathrm{Tr}(W^\\top W \\Sigma) - \\frac{1}{2}\\mathrm{Tr}(\\Sigma) + \\frac{1}{2}\\log \\det(\\Sigma) = \\frac{1}{2}\\log \\det(\\Sigma) - \\frac{1}{2} \\mathrm{Tr}\\left(\\left(I_k + \\frac{1}{\\sigma_x^2}W^\\top W\\right)\\Sigma\\right)\n$$\n提升量 $I$ 是此数量在 $\\Sigma_{\\mathrm{lr}}$ 和 $\\Sigma_{\\mathrm{diag}}$ 处的差值：\n$$\nI = \\left(\\frac{1}{2}\\log \\det(\\Sigma_{\\mathrm{lr}}) - \\frac{1}{2} \\mathrm{Tr}\\left(A\\Sigma_{\\mathrm{lr}}\\right)\\right) - \\left(\\frac{1}{2}\\log \\det(\\Sigma_{\\mathrm{diag}}) - \\frac{1}{2} \\mathrm{Tr}\\left(A\\Sigma_{\\mathrm{diag}}\\right)\\right)\n$$\n其中 $A = I_k + \\frac{1}{\\sigma_x^2}W^\\top W$。利用迹的线性和对数的性质，这可以简化为：\n$$\nI = \\frac{1}{2} \\log\\left(\\frac{\\det(\\Sigma_{\\mathrm{lr}})}{\\det(\\Sigma_{\\mathrm{diag}})}\\right) - \\frac{1}{2} \\mathrm{Tr}\\left(A(\\Sigma_{\\mathrm{lr}} - \\Sigma_{\\mathrm{diag}})\\right)\n$$\n代入 $\\Sigma_{\\mathrm{diag}} = D = \\operatorname{diag}(d)$ 和 $\\Sigma_{\\mathrm{lr}} = D + UU^\\top$，它们的差为 $\\Sigma_{\\mathrm{lr}} - \\Sigma_{\\mathrm{diag}} = UU^\\top$。$I$ 的表达式变为：\n$$\nI = \\frac{1}{2} \\log\\left(\\frac{\\det(D + UU^\\top)}{\\det(D)}\\right) - \\frac{1}{2} \\mathrm{Tr}\\left(\\left(I_k + \\frac{1}{\\sigma_x^2}W^\\top W\\right)UU^\\top\\right)\n$$\n为了以数值稳定和高效的方式计算此表达式，我们使用两个矩阵恒等式。首先，矩阵行列式引理 $\\det(A + CB) = \\det(A)\\det(I + BA^{-1}C)$ 给出：\n$$\n\\frac{\\det(D + UU^\\top)}{\\det(D)} = \\det(I_r + U^\\top D^{-1} U)\n$$\n这避免了计算一个大的 $k \\times k$ 矩阵的行列式，而将其简化为一个 $r \\times r$ 矩阵的行列式，其中 $r \\ll k$。\n其次，迹的循环性质 $\\mathrm{Tr}(XYZ) = \\mathrm{Tr}(ZXY)$ 允许我们重写迹项：\n$$\n\\mathrm{Tr}\\left(A UU^\\top\\right) = \\mathrm{Tr}\\left(U^\\top A U\\right)\n$$\n这也将计算从涉及一个 $k \\times k$ 矩阵乘积的迹简化为涉及一个 $r \\times r$ 矩阵的迹。\n用于实现的最终公式是：\n$$\nI = \\frac{1}{2} \\left[ \\log \\det(I_r + U^\\top D^{-1} U) - \\mathrm{Tr}\\left(U^\\top \\left(I_k + \\frac{1}{\\sigma_x^2}W^\\top W\\right) U\\right) \\right]\n$$\n根据指定的参数和随机种子生成矩阵 $W$ 和 $U$ 以及向量 $d$，为每个测试用例实现此公式。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the ELBO improvement for several test cases comparing a low-rank plus diagonal\n    covariance with a diagonal covariance in a variational autoencoder setting.\n    \"\"\"\n    test_cases = [\n        # (k, p, r, sigma^2, seed)\n        (8, 50, 3, 0.25, 42),\n        (8, 50, 0, 0.25, 42),\n        (8, 50, 3, 5.0, 7),\n        (8, 50, 5, 0.05, 13),\n        (30, 200, 5, 0.5, 101),\n    ]\n\n    results = []\n\n    for k, p, r, sigma_sq, seed in test_cases:\n        rng = np.random.default_rng(seed)\n\n        # Generate the loading matrix W\n        w_std = 1 / np.sqrt(k)\n        W = rng.normal(loc=0.0, scale=w_std, size=(p, k))\n\n        # Generate the low-rank factor U\n        # For r=0, U will be a (k, 0) matrix. numpy handles this correctly.\n        U = rng.normal(loc=0.0, scale=0.3, size=(k, r))\n\n        # Generate the diagonal entries d of the covariance\n        # d_i = 0.1 + 0.02 * i for i in {1, ..., k}\n        indices = np.arange(1, k + 1)\n        d = 0.1 + 0.02 * indices\n\n        # --- Compute the ELBO improvement I ---\n        # The formula derived is:\n        # I = 0.5 * [log det(I_r + U.T @ D^-1 @ U) - Tr(U.T @ (I_k + W.T@W/sigma_sq) @ U)]\n        \n        # Term 1: Log determinant term\n        # log det(I_r + U.T @ D^-1 @ U)\n        # We compute U.T @ D^-1 @ U efficiently as U.T @ (U / d), avoiding explicit D^-1 matrix\n        if r > 0:\n            M = U.T @ (U / d[:, np.newaxis])\n            log_det_term = np.log(np.linalg.det(np.eye(r) + M))\n        else: # r=0 case\n            log_det_term = 0.0\n            \n        # Term 2: Trace term\n        # Tr(U.T @ (I_k + W.T@W/sigma_sq) @ U)\n        A = np.eye(k) + (W.T @ W) / sigma_sq\n        if r > 0:\n            trace_term = np.trace(U.T @ A @ U)\n        else: # r=0 case\n            trace_term = 0.0\n        \n        # Calculate the final improvement I\n        improvement = 0.5 * (log_det_term - trace_term)\n        results.append(improvement)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "生成模型在生物医学领域最强大的应用之一是预测干预（如药物治疗或基因扰动）的效果。这要求模型能够将内在的生物学变异与外部的实验条件（如批次效应）分离开。条件变分自编码器（CVAE）通过将条件变量（如批次或处理标签）显式地整合到编码器和解码器中来实现这一目标，从而能够生成“反事实”数据。在这个实践练习中，你将实现一个CVAE来计算一个细胞在不同处理条件下的反事实基因表达谱，这展示了CVAE在进行“计算机模拟实验”和探索因果关系方面的巨大潜力。",
            "id": "4397973",
            "problem": "给定一个线性高斯条件变分自编码器 (CVAE)，它专为处理高维生物测量数据（如基因表达）而设计。该 CVAE 以已知的批次标签和处理指标为条件，这些信息被编码在一个条件向量中。目标是针对几个测试用例，计算在观测到的条件下的证据下界 (ELBO)，以及在改变后的条件下反事实重构的均值。您的实现必须是一个完整、可运行的程序，仅使用提供的常数和测试输入，基于第一性原理进行确定性计算，并以指定格式打印所需的输出。\n\n模型定义与假设：\n- 潜变量的先验分布是标准正态分布：$p(\\mathbf{z}) = \\mathcal{N}(\\mathbf{z}\\,;\\,\\mathbf{0}, \\mathbf{I})$，其中潜变量维度 $L = 2$。\n- 似然是带有线性解码器的条件高斯分布：\n  $$p(\\mathbf{x}\\mid \\mathbf{z}, \\mathbf{c}) = \\mathcal{N}\\!\\left(\\mathbf{x}\\,;\\, \\mathbf{W}\\mathbf{z} + \\mathbf{U}\\mathbf{c} + \\mathbf{b},\\, \\sigma_x^2 \\mathbf{I}\\right),$$\n  其中数据维度为 $D = 4$，条件维度为 $C = 3$。\n- 近似后验（编码器）是一个对角高斯分布，其参数是 $(\\mathbf{x}, \\mathbf{c})$ 的仿射函数：\n  $$q(\\mathbf{z}\\mid \\mathbf{x}, \\mathbf{c}) = \\mathcal{N}\\!\\left(\\mathbf{z}\\,;\\, \\boldsymbol{\\mu}_z(\\mathbf{x},\\mathbf{c}),\\, \\operatorname{diag}(\\boldsymbol{\\sigma}^2_z(\\mathbf{x},\\mathbf{c}))\\right),$$\n  形式为\n  $$\\boldsymbol{\\mu}_z(\\mathbf{x},\\mathbf{c}) = \\mathbf{M}\\mathbf{x} + \\mathbf{V}\\mathbf{c} + \\mathbf{a}, \\quad \\log \\boldsymbol{\\sigma}^2_z(\\mathbf{x},\\mathbf{c}) = \\mathbf{S}_x \\mathbf{x} + \\mathbf{S}_c \\mathbf{c} + \\mathbf{s}_0,$$\n  其中对数是逐元素应用的。\n\n所有参数都是固定的，并在下面给出。除非另有说明，向量均为列向量。\n\n待使用的参数值：\n- 解码器参数：\n  - $$\\mathbf{W} = \\begin{bmatrix}\n  0.8  -0.3\\\\\n  0.1  0.5\\\\\n  -0.4  0.2\\\\\n  0.0  0.3\n  \\end{bmatrix}, \\quad\n  \\mathbf{U} = \\begin{bmatrix}\n  0.6  -0.2  0.3\\\\\n  -0.1  0.4  -0.5\\\\\n  0.2  0.1  -0.2\\\\\n  0.5  -0.3  0.2\n  \\end{bmatrix}, \\quad\n  \\mathbf{b} = \\begin{bmatrix} 0.05\\\\ -0.02\\\\ 0.01\\\\ 0.0 \\end{bmatrix}, \\quad \\sigma_x = 0.1.$$\n- 编码器参数：\n  - $$\\mathbf{M} = \\begin{bmatrix}\n  0.2  0.0  -0.1  0.3\\\\\n  -0.2  0.1  0.4  -0.1\n  \\end{bmatrix}, \\quad\n  \\mathbf{V} = \\begin{bmatrix}\n  0.1  -0.2  0.3\\\\\n  0.0  0.2  -0.1\n  \\end{bmatrix}, \\quad\n  \\mathbf{a} = \\begin{bmatrix} 0.05\\\\ -0.05 \\end{bmatrix},$$\n  $$\\mathbf{S}_x = \\begin{bmatrix}\n  -0.1  0.2  0.0  -0.2\\\\\n  0.1  -0.1  0.3  0.0\n  \\end{bmatrix}, \\quad\n  \\mathbf{S}_c = \\begin{bmatrix}\n  0.05  0.05  -0.1\\\\\n  -0.05  0.1  0.0\n  \\end{bmatrix}, \\quad\n  \\mathbf{s}_0 = \\begin{bmatrix} -1.0\\\\ -1.2 \\end{bmatrix}.$$\n\n条件向量的构建：\n- 条件向量 $\\mathbf{c} \\in \\mathbb{R}^3$ 按如下方式连接批次标签和处理指标：前两个条目是两个批次的 one-hot 指示符（批次 A 为 $[1,0]^\\top$，批次 B 为 $[0,1]^\\top$），第三个条目是 $\\{0,1\\}$ 中的处理指标。\n\n待计算的量：\n- 对于每个具有观测对 $(\\mathbf{x}_{\\mathrm{obs}}, \\mathbf{c}_{\\mathrm{obs}})$ 和改变后的反事实条件 $\\mathbf{c}_{\\mathrm{alt}}$ 的测试用例，计算：\n  1. 针对观测对的证据下界 (ELBO)：\n     $$\\mathcal{L}(\\mathbf{x}_{\\mathrm{obs}}, \\mathbf{c}_{\\mathrm{obs}}) = \\mathbb{E}_{q(\\mathbf{z}\\mid \\mathbf{x}_{\\mathrm{obs}}, \\mathbf{c}_{\\mathrm{obs}})}\\!\\left[\\log p(\\mathbf{x}_{\\mathrm{obs}} \\mid \\mathbf{z}, \\mathbf{c}_{\\mathrm{obs}})\\right] - \\mathrm{KL}\\!\\left(q(\\mathbf{z}\\mid \\mathbf{x}_{\\mathrm{obs}}, \\mathbf{c}_{\\mathrm{obs}})\\,\\|\\, p(\\mathbf{z})\\right).$$\n     您必须使用线性高斯恒等式以闭式形式计算期望，而不能使用蒙特卡洛采样。\n  2. 在改变后的条件下的反事实重构均值：\n     $$\\mathbb{E}_{q(\\mathbf{z}\\mid \\mathbf{x}_{\\mathrm{obs}}, \\mathbf{c}_{\\mathrm{obs}})}\\!\\left[\\mathbf{W}\\mathbf{z} + \\mathbf{U}\\mathbf{c}_{\\mathrm{alt}} + \\mathbf{b}\\right],$$\n     同样以闭式形式计算。\n\n可作为起点的数学事实：\n- 对于高斯似然 $p(\\mathbf{x}\\mid \\mathbf{z}) = \\mathcal{N}(\\mathbf{x}; \\mathbf{A}\\mathbf{z}+\\boldsymbol{\\mu}, \\sigma^2 \\mathbf{I})$ 和对角高斯分布 $q(\\mathbf{z}) = \\mathcal{N}(\\mathbf{z}; \\boldsymbol{\\mu}_z, \\operatorname{diag}(\\boldsymbol{\\sigma}^2_z))$，期望为\n  $$\\mathbb{E}_q\\!\\left[\\lVert \\mathbf{x} - (\\mathbf{A}\\mathbf{z}+\\boldsymbol{\\mu}) \\rVert_2^2\\right] = \\lVert \\mathbf{x} - (\\mathbf{A}\\boldsymbol{\\mu}_z+\\boldsymbol{\\mu}) \\rVert_2^2 + \\operatorname{tr}\\!\\left(\\mathbf{A}\\,\\operatorname{diag}(\\boldsymbol{\\sigma}^2_z)\\,\\mathbf{A}^\\top\\right).$$\n- 对角高斯分布 $q(\\mathbf{z}) = \\mathcal{N}(\\boldsymbol{\\mu}_z, \\operatorname{diag}(\\boldsymbol{\\sigma}^2_z))$ 与标准正态分布 $p(\\mathbf{z}) = \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ 之间的 Kullback–Leibler 散度是\n  $$\\mathrm{KL}(q\\|p) = \\tfrac{1}{2} \\sum_{i=1}^{L} \\left( \\sigma_{z,i}^2 + \\mu_{z,i}^2 - 1 - \\log \\sigma_{z,i}^2 \\right).$$\n\n测试套件：\n- 使用以下三个测试用例。对于每个用例，都给定了 $\\mathbf{x}_{\\mathrm{obs}}$、$\\mathbf{c}_{\\mathrm{obs}}$ 和 $\\mathbf{c}_{\\mathrm{alt}}$。\n  - 用例 1 (正常路径):\n    $$\\mathbf{x}_{\\mathrm{obs}}^{(1)} = \\begin{bmatrix} 1.2\\\\ 0.3\\\\ -0.5\\\\ 0.0 \\end{bmatrix}, \\quad \\mathbf{c}_{\\mathrm{obs}}^{(1)} = \\begin{bmatrix} 1\\\\ 0\\\\ 1 \\end{bmatrix}, \\quad \\mathbf{c}_{\\mathrm{alt}}^{(1)} = \\begin{bmatrix} 1\\\\ 0\\\\ 0 \\end{bmatrix}.$$\n  - 用例 2 (批次反事实):\n    $$\\mathbf{x}_{\\mathrm{obs}}^{(2)} = \\begin{bmatrix} -0.2\\\\ 0.5\\\\ 1.0\\\\ -1.0 \\end{bmatrix}, \\quad \\mathbf{c}_{\\mathrm{obs}}^{(2)} = \\begin{bmatrix} 0\\\\ 1\\\\ 0 \\end{bmatrix}, \\quad \\mathbf{c}_{\\mathrm{alt}}^{(2)} = \\begin{bmatrix} 1\\\\ 0\\\\ 0 \\end{bmatrix}.$$\n  - 用例 3 (零输入边缘用例):\n    $$\\mathbf{x}_{\\mathrm{obs}}^{(3)} = \\begin{bmatrix} 0.0\\\\ 0.0\\\\ 0.0\\\\ 0.0 \\end{bmatrix}, \\quad \\mathbf{c}_{\\mathrm{obs}}^{(3)} = \\begin{bmatrix} 0\\\\ 1\\\\ 1 \\end{bmatrix}, \\quad \\mathbf{c}_{\\mathrm{alt}}^{(3)} = \\begin{bmatrix} 1\\\\ 0\\\\ 1 \\end{bmatrix}.$$\n\n所需输出：\n- 对于每个用例 $k \\in \\{1,2,3\\}$，计算：\n  - 标量 ELBO 值 $\\mathcal{L}(\\mathbf{x}_{\\mathrm{obs}}^{(k)}, \\mathbf{c}_{\\mathrm{obs}}^{(k)})$。\n  - 在 $\\mathbf{c}_{\\mathrm{alt}}^{(k)}$ 条件下的 $D$ 维反事实重构均值向量。\n- 您的程序应生成单行输出，其中包含一个含三个元素的列表，每个元素对应一个测试用例，且每个元素本身是一个形式为 $[\\text{ELBO}, [\\text{cf}_1,\\dots,\\text{cf}_D]]$ 的双元素列表。所有浮点数需四舍五入到恰好 6 位小数。例如：\n  $$[[\\ell_1,[r_{1,1},r_{1,2},r_{1,3},r_{1,4}]],[\\ell_2,[\\dots]],[\\ell_3,[\\dots]]].$$\n\n实现约束：\n- 不允许随机性或采样；仅使用基于给定线性高斯恒等式的闭式期望。\n- 代码必须完全自包含，并且不得读取任何输入。\n- 仅使用 Python 标准库和指定的数值库。",
            "solution": "此问题被评估为有效，它具有科学依据、问题适定且客观。它指定了一个线性高斯条件变分自编码器 (CVAE)，并要求计算证据下界 (ELBO) 和一个反事实重构均值。所有必要的参数和输入数据均已提供，所需的量可以以闭式形式推导出来。我们将首先进行解析推导，然后进行实现。\n\n目标是为每个测试用例计算两个量，每个测试用例提供一个观测数据向量 $\\mathbf{x}_{\\mathrm{obs}}$、一个观测条件向量 $\\mathbf{c}_{\\mathrm{obs}}$ 和一个反事实条件向量 $\\mathbf{c}_{\\mathrm{alt}}$。这些量是：\n1. ELBO, $\\mathcal{L}(\\mathbf{x}_{\\mathrm{obs}}, \\mathbf{c}_{\\mathrm{obs}})$。\n2. 反事实重构均值, $\\mathbb{E}_{q(\\mathbf{z}\\mid \\mathbf{x}_{\\mathrm{obs}}, \\mathbf{c}_{\\mathrm{obs}})}\\!\\left[\\mathbf{W}\\mathbf{z} + \\mathbf{U}\\mathbf{c}_{\\mathrm{alt}} + \\mathbf{b}\\right]$。\n\n首先，对于给定的对 $(\\mathbf{x}_{\\mathrm{obs}}, \\mathbf{c}_{\\mathrm{obs}})$，我们必须确定近似后验分布 $q(\\mathbf{z}\\mid \\mathbf{x}_{\\mathrm{obs}}, \\mathbf{c}_{\\mathrm{obs}})$ 的参数。问题将此定义为对角高斯分布，$q(\\mathbf{z}\\mid \\mathbf{x}, \\mathbf{c}) = \\mathcal{N}\\!\\left(\\mathbf{z}\\,;\\, \\boldsymbol{\\mu}_z(\\mathbf{x},\\mathbf{c}),\\, \\operatorname{diag}(\\boldsymbol{\\sigma}^2_z(\\mathbf{x},\\mathbf{c}))\\right)$。参数计算如下：\n\n后验均值向量 $\\boldsymbol{\\mu}_z \\in \\mathbb{R}^L$ 为：\n$$ \\boldsymbol{\\mu}_z(\\mathbf{x}_{\\mathrm{obs}},\\mathbf{c}_{\\mathrm{obs}}) = \\mathbf{M}\\mathbf{x}_{\\mathrm{obs}} + \\mathbf{V}\\mathbf{c}_{\\mathrm{obs}} + \\mathbf{a} $$\n\n后验逐元素对数方差向量 $\\log \\boldsymbol{\\sigma}^2_z \\in \\mathbb{R}^L$ 为：\n$$ \\log \\boldsymbol{\\sigma}^2_z(\\mathbf{x}_{\\mathrm{obs}},\\mathbf{c}_{\\mathrm{obs}}) = \\mathbf{S}_x \\mathbf{x}_{\\mathrm{obs}} + \\mathbf{S}_c \\mathbf{c}_{\\mathrm{obs}} + \\mathbf{s}_0 $$\n由此，通过逐元素取指数可得到后验方差向量 $\\boldsymbol{\\sigma}^2_z \\in \\mathbb{R}^L$：\n$$ \\boldsymbol{\\sigma}^2_z(\\mathbf{x}_{\\mathrm{obs}},\\mathbf{c}_{\\mathrm{obs}}) = \\exp(\\log \\boldsymbol{\\sigma}^2_z(\\mathbf{x}_{\\mathrm{obs}},\\mathbf{c}_{\\mathrm{obs}})) $$\n为简洁起见，我们将这些特定的后验参数表示为 $\\boldsymbol{\\mu}_z$ 和 $\\boldsymbol{\\sigma}_z^2$，并将相应的分布表示为 $q(\\mathbf{z})$。\n\n在确定了后验参数后，我们现在可以推导这两个所需量的表达式。\n\n**1. 证据下界 (ELBO)**\n\nELBO 定义为：\n$$ \\mathcal{L}(\\mathbf{x}_{\\mathrm{obs}}, \\mathbf{c}_{\\mathrm{obs}}) = \\mathbb{E}_{q(\\mathbf{z})}\\!\\left[\\log p(\\mathbf{x}_{\\mathrm{obs}} \\mid \\mathbf{z}, \\mathbf{c}_{\\mathrm{obs}})\\right] - \\mathrm{KL}\\!\\left(q(\\mathbf{z})\\,\\|\\, p(\\mathbf{z})\\right) $$\n\n我们将分别计算这两项。\n\n第二项是近似后验 $q(\\mathbf{z}) = \\mathcal{N}(\\boldsymbol{\\mu}_z, \\operatorname{diag}(\\boldsymbol{\\sigma}_z^2))$ 与标准正态先验 $p(\\mathbf{z}) = \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ 之间的 Kullback-Leibler (KL) 散度。所提供的公式为：\n$$ \\mathrm{KL}(q\\|p) = \\frac{1}{2} \\sum_{i=1}^{L} \\left( \\sigma_{z,i}^2 + \\mu_{z,i}^2 - 1 - \\log \\sigma_{z,i}^2 \\right) $$\n其中 $\\mu_{z,i}$ 和 $\\sigma_{z,i}^2$ 是先前计算的向量 $\\boldsymbol{\\mu}_z$ 和 $\\boldsymbol{\\sigma}_z^2$ 的元素。\n\n第一项是期望重构对数似然。似然由 $p(\\mathbf{x}\\mid \\mathbf{z}, \\mathbf{c}) = \\mathcal{N}\\!\\left(\\mathbf{x}\\,;\\, \\mathbf{W}\\mathbf{z} + \\mathbf{U}\\mathbf{c} + \\mathbf{b},\\, \\sigma_x^2 \\mathbf{I}\\right)$ 给出。对数概率为：\n$$ \\log p(\\mathbf{x} \\mid \\mathbf{z}, \\mathbf{c}) = -\\frac{D}{2} \\log(2\\pi \\sigma_x^2) - \\frac{1}{2\\sigma_x^2} \\|\\mathbf{x} - (\\mathbf{W}\\mathbf{z} + \\mathbf{U}\\mathbf{c} + \\mathbf{b})\\|^2_2 $$\n我们必须在 $q(\\mathbf{z})$ 下计算这个量的期望。第一项相对于 $\\mathbf{z}$ 是一个常数。\n$$ \\mathbb{E}_{q(\\mathbf{z})}\\!\\left[\\log p(\\mathbf{x}_{\\mathrm{obs}} \\mid \\mathbf{z}, \\mathbf{c}_{\\mathrm{obs}})\\right] = -\\frac{D}{2} \\log(2\\pi \\sigma_x^2) - \\frac{1}{2\\sigma_x^2} \\mathbb{E}_{q(\\mathbf{z})}\\!\\left[ \\|\\mathbf{x}_{\\mathrm{obs}} - (\\mathbf{W}\\mathbf{z} + \\mathbf{U}\\mathbf{c}_{\\mathrm{obs}} + \\mathbf{b})\\|^2_2 \\right] $$\n为了评估平方范数的期望，我们使用提供的恒等式：$\\mathbb{E}_q\\!\\left[\\lVert \\mathbf{x} - (\\mathbf{A}\\mathbf{z}+\\boldsymbol{\\mu}) \\rVert_2^2\\right] = \\lVert \\mathbf{x} - (\\mathbf{A}\\boldsymbol{\\mu}_z+\\boldsymbol{\\mu}) \\rVert_2^2 + \\operatorname{tr}\\!\\left(\\mathbf{A}\\,\\operatorname{diag}(\\boldsymbol{\\sigma}^2_z)\\,\\mathbf{A}^\\top\\right)$。我们进行替换：$\\mathbf{x} \\rightarrow \\mathbf{x}_{\\mathrm{obs}}$，$\\mathbf{A} \\rightarrow \\mathbf{W}$，以及 $\\boldsymbol{\\mu} \\rightarrow \\mathbf{U}\\mathbf{c}_{\\mathrm{obs}} + \\mathbf{b}$。期望变为：\n$$ \\|\\mathbf{x}_{\\mathrm{obs}} - (\\mathbf{W}\\boldsymbol{\\mu}_z + \\mathbf{U}\\mathbf{c}_{\\mathrm{obs}} + \\mathbf{b})\\|^2_2 + \\operatorname{tr}\\!\\left(\\mathbf{W}\\,\\operatorname{diag}(\\boldsymbol{\\sigma}^2_z)\\,\\mathbf{W}^\\top\\right) $$\n第一项是输入 $\\mathbf{x}_{\\mathrm{obs}}$ 与其从后验均值得出的重构之间的欧几里得距离的平方。迹项可以使用迹的循环性质和协方差矩阵 $\\boldsymbol{\\Sigma}_z = \\operatorname{diag}(\\boldsymbol{\\sigma}^2_z)$ 的对角特性来简化：\n$$ \\operatorname{tr}(\\mathbf{W} \\boldsymbol{\\Sigma}_z \\mathbf{W}^\\top) = \\operatorname{tr}(\\mathbf{W}^\\top \\mathbf{W} \\boldsymbol{\\Sigma}_z) = \\sum_{i=1}^L (\\mathbf{W}^\\top \\mathbf{W})_{ii} \\sigma_{z,i}^2 = \\sum_{i=1}^L \\|\\mathbf{W}_{:,i}\\|_2^2 \\sigma_{z,i}^2 $$\n其中 $\\mathbf{W}_{:,i}$ 是 $\\mathbf{W}$ 的第 $i$ 列。\n\n将所有部分结合起来，ELBO 为：\n$$ \\mathcal{L} = \\left[ -\\frac{D}{2}\\log(2\\pi\\sigma_x^2) - \\frac{1}{2\\sigma_x^2}\\left( \\|\\mathbf{x}_{\\mathrm{obs}} - (\\mathbf{W}\\boldsymbol{\\mu}_z + \\mathbf{U}\\mathbf{c}_{\\mathrm{obs}} + \\mathbf{b})\\|^2_2 + \\sum_{i=1}^L \\|\\mathbf{W}_{:,i}\\|_2^2 \\sigma_{z,i}^2 \\right) \\right] - \\left[ \\frac{1}{2}\\sum_{i=1}^L(\\mu_{z,i}^2 + \\sigma_{z,i}^2 - 1 - \\log\\sigma_{z,i}^2) \\right] $$\n\n**2. 反事实重构均值**\n\n要计算的第二个量是重构数据分布的均值，其中潜变量 $\\mathbf{z}$ 从根据观测数据 $(\\mathbf{x}_{\\mathrm{obs}}, \\mathbf{c}_{\\mathrm{obs}})$ 推断出的后验分布中抽取，但解码是使用改变后的条件 $\\mathbf{c}_{\\mathrm{alt}}$ 执行的。\n$$ \\mathbf{x}_{\\mathrm{cf}} = \\mathbb{E}_{q(\\mathbf{z}\\mid \\mathbf{x}_{\\mathrm{obs}}, \\mathbf{c}_{\\mathrm{obs}})}\\!\\left[ \\mathbf{W}\\mathbf{z} + \\mathbf{U}\\mathbf{c}_{\\mathrm{alt}} + \\mathbf{b} \\right] $$\n根据期望的线性性质，并且由于 $\\mathbf{c}_{\\mathrm{alt}}$ 和 $\\mathbf{b}$ 相对于关于 $\\mathbf{z}$ 的期望是常数：\n$$ \\mathbf{x}_{\\mathrm{cf}} = \\mathbf{W} \\mathbb{E}_{q(\\mathbf{z})}[\\mathbf{z}] + \\mathbf{U}\\mathbf{c}_{\\mathrm{alt}} + \\mathbf{b} $$\n$q(\\mathbf{z})$ 下 $\\mathbf{z}$ 的期望就是其均值 $\\boldsymbol{\\mu}_z$。因此，反事实重构均值为：\n$$ \\mathbf{x}_{\\mathrm{cf}} = \\mathbf{W}\\boldsymbol{\\mu}_z + \\mathbf{U}\\mathbf{c}_{\\mathrm{alt}} + \\mathbf{b} $$\n其中 $\\boldsymbol{\\mu}_z$ 是从 $(\\mathbf{x}_{\\mathrm{obs}}, \\mathbf{c}_{\\mathrm{obs}})$ 计算出的后验均值。\n\n实现将遵循为每个测试用例推导出的这些闭式表达式。所有向量和矩阵运算将使用 `numpy` 库进行。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the CVAE problem by computing the ELBO and counterfactual reconstruction mean\n    for three test cases based on the provided linear-Gaussian model.\n    \"\"\"\n    \n    # Model dimensions\n    L = 2  # Latent dimension\n    D = 4  # Data dimension\n    C = 3  # Condition dimension\n    \n    # Decoder parameters\n    W = np.array([\n        [0.8, -0.3],\n        [0.1, 0.5],\n        [-0.4, 0.2],\n        [0.0, 0.3]\n    ])\n    U = np.array([\n        [0.6, -0.2, 0.3],\n        [-0.1, 0.4, -0.5],\n        [0.2, 0.1, -0.2],\n        [0.5, -0.3, 0.2]\n    ])\n    b = np.array([0.05, -0.02, 0.01, 0.0]).reshape(D, 1)\n    sigma_x = 0.1\n    \n    # Encoder parameters\n    M = np.array([\n        [0.2, 0.0, -0.1, 0.3],\n        [-0.2, 0.1, 0.4, -0.1]\n    ])\n    V = np.array([\n        [0.1, -0.2, 0.3],\n        [0.0, 0.2, -0.1]\n    ])\n    a = np.array([0.05, -0.05]).reshape(L, 1)\n    \n    S_x = np.array([\n        [-0.1, 0.2, 0.0, -0.2],\n        [0.1, -0.1, 0.3, 0.0]\n    ])\n    S_c = np.array([\n        [0.05, 0.05, -0.1],\n        [-0.05, 0.1, 0.0]\n    ])\n    s_0 = np.array([-1.0, -1.2]).reshape(L, 1)\n    \n    # Test suite\n    test_cases = [\n        {\n            \"x_obs\": np.array([1.2, 0.3, -0.5, 0.0]).reshape(D, 1),\n            \"c_obs\": np.array([1, 0, 1]).reshape(C, 1),\n            \"c_alt\": np.array([1, 0, 0]).reshape(C, 1)\n        },\n        {\n            \"x_obs\": np.array([-0.2, 0.5, 1.0, -1.0]).reshape(D, 1),\n            \"c_obs\": np.array([0, 1, 0]).reshape(C, 1),\n            \"c_alt\": np.array([1, 0, 0]).reshape(C, 1)\n        },\n        {\n            \"x_obs\": np.array([0.0, 0.0, 0.0, 0.0]).reshape(D, 1),\n            \"c_obs\": np.array([0, 1, 1]).reshape(C, 1),\n            \"c_alt\": np.array([1, 0, 1]).reshape(C, 1)\n        }\n    ]\n    \n    results = []\n    \n    for case in test_cases:\n        x_obs = case[\"x_obs\"]\n        c_obs = case[\"c_obs\"]\n        c_alt = case[\"c_alt\"]\n        \n        # 1. Compute posterior parameters (mu_z, sigma_z^2)\n        # Use observed data and conditions\n        mu_z = M @ x_obs + V @ c_obs + a\n        log_sigma_z_sq = S_x @ x_obs + S_c @ c_obs + s_0\n        sigma_z_sq = np.exp(log_sigma_z_sq)\n        \n        # 2. Compute the KL divergence term of the ELBO\n        # KL(q(z|x,c) || p(z))\n        kl_div = 0.5 * np.sum(sigma_z_sq + mu_z**2 - 1 - log_sigma_z_sq)\n        \n        # 3. Compute the expected log-likelihood term of the ELBO\n        # E_q[log p(x|z,c)]\n        \n        # Term 1: Reconstruction error from posterior mean\n        recon_mean_obs = W @ mu_z + U @ c_obs + b\n        recon_error_sq_norm = np.sum((x_obs - recon_mean_obs)**2)\n        \n        # Term 2: Trace term from variance propagation\n        w_col_sq_norms = np.sum(W**2, axis=0) # shape (L,)\n        trace_term = np.sum(w_col_sq_norms * sigma_z_sq.flatten())\n        \n        # Constant from Gaussian PDF\n        log_p_constant = -D / 2.0 * np.log(2 * np.pi * sigma_x**2)\n        \n        # Combine parts for expected log-likelihood\n        expected_log_p = log_p_constant - (1 / (2 * sigma_x**2)) * (recon_error_sq_norm + trace_term)\n        \n        # 4. Compute the ELBO\n        elbo = expected_log_p - kl_div\n        \n        # 5. Compute the counterfactual reconstruction mean\n        # E_q[decoder(z, c_alt)]\n        cf_recon_mean = W @ mu_z + U @ c_alt + b\n        \n        # Format the results as required\n        elbo_rounded = round(elbo, 6)\n        cf_recon_mean_list = [round(x, 6) for x in cf_recon_mean.flatten()]\n        \n        results.append([elbo_rounded, cf_recon_mean_list])\n        \n    # Print the final result in the exact required format\n    # Manual string construction to avoid spaces added by standard list-to-string conversion\n    result_strings = []\n    for elbo_val, cf_mean_list in results:\n        cf_mean_str = f\"[{','.join(f'{x:.6f}' for x in cf_mean_list)}]\"\n        result_strings.append(f\"[{elbo_val:.6f},{cf_mean_str}]\")\n    \n    final_output = f\"[{','.join(result_strings)}]\"\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}