{
    "hands_on_practices": [
        {
            "introduction": "要为基因组学等应用有效设计卷积神经网络（CNN），首要步骤是深刻理解其核心构建模块。本练习将带您深入研究一维卷积层的基本力学，超越简单的API调用。通过从第一性原理计算输出维度并分析感受野对基序检测的影响，您将建立起关于内核大小、步幅和填充等超参数如何共同决定模型观察和学习序列模式能力的直观理解 。",
            "id": "4553876",
            "problem": "您正在设计一个卷积神经网络（CNN; Convolutional Neural Network）中的第一层一维卷积，用于检测以独热编码（one-hot）形式沿通道编码的脱氧核糖核酸（DNA; Deoxyribonucleic Acid）序列中的转录因子结合基序。考虑一个长度为 $L_{\\mathrm{in}} = 1{,}000$ 个碱基的输入序列。该卷积层使用的卷积核长度为 $k = 15$，步幅为 $s = 3$，两端各有 $p = 2$ 个碱基的对称零填充，膨胀率为 $d = 1$。\n\n从带步幅和填充的离散一维卷积在填充后输入上的定义出发，推导确定输出位置数量（即有效卷积核对齐次数）的通用表达式。使用此推理计算给定参数下的输出序列长度 $L_{\\mathrm{out}}$。然后，基于感受野和步幅引起的采样概念，分析此配置如何影响检测跨越 $M = 20$ 个碱基的基序的能力，特别是在单次滤波器应用内的完全覆盖、位置敏感性以及序列边界附近的检测方面。\n\n将输出长度 $L_{\\mathrm{out}}$ 报告为单个无单位的整数。无需四舍五入。您的定性分析应包含在推理过程中，但最终答案必须仅为计算出的 $L_{\\mathrm{out}}$。",
            "solution": "用户要求推导一维卷积层的输出长度，并对其在生物信息学特定应用中的属性进行定性分析。本解答将首先建立输出维度的通用公式，然后将其应用于给定参数，最后分析该配置的意义。\n\n**问题验证**\n\n在继续之前，对问题陈述进行严格验证。\n1.  **提取的已知条件**：\n    -   输入序列长度：$L_{\\mathrm{in}} = 1{,}000$ 个碱基。\n    -   卷积核长度：$k = 15$。\n    -   步幅：$s = 3$。\n    -   对称零填充：两端各 $p = 2$。\n    -   膨胀率：$d = 1$。\n    -   用于分析的目标基序长度：$M = 20$ 个碱基。\n\n2.  **验证结论**：问题是**有效的**。它在科学上基于深度学习（特别是卷积神经网络）的标准原理，并应用于成熟的生物信息学领域进行基因组分析。问题提法得当，提供了一套完整且一致的参数（$L_{\\mathrm{in}}, k, s, p, d$），可以从中确定一个唯一且有意义的输出长度（$L_{\\mathrm{out}}$）解。语言客观并采用标准术语。这些参数对于所描述的任务是现实的。\n\n**输出长度通用表达式的推导**\n\n一维卷积层的输出位置数量，即输出序列长度 $L_{\\mathrm{out}}$，由卷积核沿输入序列的有效放置次数决定。这可以从第一性原理推导出来。\n\n1.  长度为 $L_{\\mathrm{in}}$ 的输入序列首先进行对称填充，在其两端各添加 $p$ 个元素。这个新的、经过填充的序列的长度，记为 $L_{\\mathrm{padded}}$，为：\n    $$L_{\\mathrm{padded}} = L_{\\mathrm{in}} + 2p$$\n\n2.  长度为 $k$ 的卷积核可能会被膨胀。膨胀因子 $d$ 在连续的卷积核元素之间引入 $d-1$ 个间隙。因此，卷积核在输入上的有效空间范围，称为有效卷积核大小 $k_{\\mathrm{eff}}$，为：\n    $$k_{\\mathrm{eff}} = k + (k-1)(d-1)$$\n\n3.  卷积核在填充后的序列上移动，或“跨步”。每个位置都会计算一个输出。卷积核的第一次放置从填充后序列的索引 $0$ 开始。覆盖的范围是从索引 $0$ 到 $k_{\\mathrm{eff}}-1$。最后一次可能的放置也必须完全容纳在填充后的序列之内。卷积核的最后一个元素必须与不大于 $L_{\\mathrm{padded}}-1$ 的索引对齐。这意味着卷积核的最后一个起始位置在索引 $L_{\\mathrm{padded}} - k_{\\mathrm{eff}}$ 处。\n\n4.  因此，可能的起始位置的总范围是从 $0$ 到 $L_{\\mathrm{padded}} - k_{\\mathrm{eff}}$，如果步幅为 $1$，这将包含 $L_{\\mathrm{padded}} - k_{\\mathrm{eff}} + 1$ 个可能的位置。\n\n5.  当步幅为 $s$ 时，卷积核不会占据所有可能的位置。相反，它占据位置 $0, s, 2s, \\dots, Ns$，其中 $Ns$ 是最后一个有效的起始位置。这个最后的位置必须满足 $Ns \\le L_{\\mathrm{padded}} - k_{\\mathrm{eff}}$。因此，步数 $N$ 是满足此条件的最大整数，可以使用向下取整函数找到：\n    $$N = \\left\\lfloor \\frac{L_{\\mathrm{padded}} - k_{\\mathrm{eff}}}{s} \\right\\rfloor$$\n    输出位置的总数 $L_{\\mathrm{out}}$ 是这些放置的数量，即 $N+1$（包括在位置 $0$ 的初始放置）。\n\n6.  代入 $L_{\\mathrm{padded}}$ 和 $k_{\\mathrm{eff}}$ 的表达式，得到输出长度的通用公式：\n    $$L_{\\mathrm{out}} = \\left\\lfloor \\frac{(L_{\\mathrm{in}} + 2p) - (k + (k-1)(d-1))}{s} \\right\\rfloor + 1$$\n\n**根据给定参数计算 $L_{\\mathrm{out}}$**\n\n问题提供了以下参数：$L_{\\mathrm{in}} = 1{,}000$，$k = 15$，$s = 3$，$p = 2$，以及 $d = 1$。\n\n首先，我们将这些值代入推导出的通用表达式中。由于膨胀率为 $d=1$，有效卷积核长度 $k_{\\mathrm{eff}}$ 就是 $k$：\n$$k_{\\mathrm{eff}} = 15 + (15-1)(1-1) = 15 + (14)(0) = 15$$\n$L_{\\mathrm{out}}$ 的公式简化为：\n$$L_{\\mathrm{out}} = \\left\\lfloor \\frac{L_{\\mathrm{in}} + 2p - k}{s} \\right\\rfloor + 1$$\n代入数值：\n$$L_{\\mathrm{out}} = \\left\\lfloor \\frac{1{,}000 + 2(2) - 15}{3} \\right\\rfloor + 1$$\n$$L_{\\mathrm{out}} = \\left\\lfloor \\frac{1{,}000 + 4 - 15}{3} \\right\\rfloor + 1$$\n$$L_{\\mathrm{out}} = \\left\\lfloor \\frac{989}{3} \\right\\rfloor + 1$$\n除法产生一个非整数值：\n$$\\frac{989}{3} \\approx 329.667$$\n应用向下取整函数，得到不大于此值的最大整数：\n$$\\lfloor 329.667 \\rfloor = 329$$\n最后，加 1 得到输出长度：\n$$L_{\\mathrm{out}} = 329 + 1 = 330$$\n\n**配置的定性分析**\n\n问题进一步要求分析此配置检测长度为 $M = 20$ 个碱基的基序的能力。\n\n1.  **完全覆盖与感受野**：此卷积层中任何单个神经元的感受野是其有效卷积核大小，即 $k_{\\mathrm{eff}} = 15$ 个碱基。目标基序长度为 $M=20$ 个碱基。由于 $M > k_{\\mathrm{eff}}$，**没有任何一次单独的滤波器应用能够看到整个基序**。滤波器最多只能学会识别长度不超过 $15$ 个碱基的基序片段。要识别完整的 $20$ 碱基基序，网络必须学会组合来自输出特征图中空间上相邻神经元的信息，这是后续层（例如，另一个卷积层或全连接层）的任务。\n\n2.  **位置敏感性与步幅**：步幅 $s=3$ 引入了对输入的下采样。这意味着卷积核每移动一步会“跳过”2个碱基。因此，该层对单碱基平移不具有平移等变性。一个从输入位置 $i$ 开始的基序与一个从位置 $i+1$ 或 $i+2$ 开始的基序，将被卷积核以不同的方式看待。这种对模 $s$ 的位置移位的高敏感性可能是有害的，因为基序位置的微小移动可能导致其与卷积核的步幅错位，从而导致激活值减弱和可能的检测失败。\n\n3.  **序列边界附近的检测**：该层使用 $p=2$ 的对称填充。对于大小为 $k=15$ 的卷积核，这种填充不足以使卷积核在序列的极两端对齐基序中心。要将一个大小为 $k$ 的卷积核在第一个碱基（索引 $0$）上居中，需要 $p_{\\mathrm{center}} = \\lfloor k/2 \\rfloor = \\lfloor 15/2 \\rfloor = 7$ 的填充。而当 $p=2$ 时，第一个输出神经元的感受野中心位于原始序列位置 $7-2 = 5$。这意味着序列开头约 $5$ 个碱基和结尾约 $5$ 个碱基内的基序永远无法在卷积核的视野中居中，这可能会损害网络检测它们的能力。\n\n总之，由于其卷积核尺寸过小，这个特定的层配置在结构上对于检测 $20$ 碱基的基序是受限的，并且其性能会因大步幅和不充分的填充而进一步受损，这些因素会影响位置敏感性和边界检测。",
            "answer": "$$\\boxed{330}$$"
        },
        {
            "introduction": "在构建了对单个卷积层的理解之后，下一步是考虑如何将它们组合成一个更深的网络。池化层通过降低采样率来创建特征层次结构，但在这一过程中会牺牲空间精度。这项练习探讨了在特征抽象和空间定位精度之间的基本权衡，这对于例如预测核酸酶精确切割位点等任务至关重要，通过将池化引起的误差建模为量化误差，您将量化这种信息损失，从而为架构设计决策提供一个严谨的框架 。",
            "id": "4331413",
            "problem": "在系统生物医学中，卷积神经网络（Convolutional Neural Networks, CNN）被用于从脱氧核糖核酸（DNA）序列中预测核酸酶切位点。考虑一个一维CNN，它接收一个以每碱基对 $1$ 个样本采样的DNA序列，应用步长为 $1$ 的卷积层，然后应用 $K$ 个最大池化层，其中第 $i$ 个池化层使用大小为 $s_i$ 碱基对的窗口和 $s_i$ 的步长（非重叠池化）。该网络输出一个粗略的空间图，从中选择一个网格位置（例如，通过对粗略图进行 $\\arg\\max$ 操作），预测的切位点位置 $\\hat{x}$ 被取为所选粗粒度区间映射回输入坐标后的中心。\n\n假设如下：\n- 由于真实位置与池化网格之间的对齐未知，真实的切位点位置 $x^{\\ast}$（相对于池化所产生的粗略网格）在任何粗粒度区间内均匀分布。\n- 上游的卷积操作在输入分辨率下保持平移等变性，并且在输出端不执行区间内插值；预测被限制在由池化产生的粗略网格上。\n- 你可以借助 Nyquist–Shannon 采样定理和最大池化的定义来形式化描述堆叠池化层如何改变有效采样分辨率。将这一结果建模为对输入坐标的均匀量化，其步长由 $\\{s_i\\}_{i=1}^{K}$ 决定。\n\n从这些基础出发，推导期望平方定位误差\n$$\nE \\;=\\; \\mathbb{E}\\big[(\\hat{x} - x^{\\ast})^{2}\\big]\n$$\n的闭式解析表达式，该表达式以池化步长 $\\{s_i\\}_{i=1}^{K}$ 表示。解释为什么增加 $K$ 或任何 $s_i$ 会因减少位置信息而增加 $E$。以平方碱基对为单位表示你最终的 $E$，并以单一闭式解析表达式的形式给出你的答案。无需四舍五入，最终的方框答案中不包含单位。",
            "solution": "该问题被认为是有效的，因为它具有科学依据、适定且客观。它提出了一个简化但标准的模型，用于描述卷积神经网络中下采样操作（最大池化）所产生的量化误差，这是信号处理和应用于基因组学的机器学习中的一个常见主题。所有必要的参数和假设都已提供，以推导出唯一的解析解。\n\n问题要求计算期望平方定位误差 $E = \\mathbb{E}[(\\hat{x} - x^{\\ast})^{2}]$，其中 $\\hat{x}$ 是预测的切位点位置， $x^{\\ast}$ 是真实位置。这个问题的核心在于理解最大池化层的顺序应用如何影响输入信号的空间分辨率。\n\n首先，让我们确定最终输出图中单个分辨单元（一个“粗粒度区间”）在回溯到输入坐标系时的大小。输入的DNA序列以每碱基对 $1$ 个样本进行采样。网络架构包括步长为 $1$ 的卷积层（其保持空间分辨率），之后是 $K$ 个最大池化层。第 $i$ 个池化层具有窗口大小 $s_i$ 和步长 $s_i$。这种非重叠池化操作将数据点的数量减少了 $s_i$ 倍。当这 $K$ 个层堆叠时，总下采样因子是各个步长因子的乘积。\n\n设 $S$ 是在原始输入坐标中一个粗粒度区间的大小，以碱基对为单位。这个大小是所有 $K$ 个池化操作的累积效应。\n$$\nS = \\prod_{i=1}^{K} s_i\n$$\n最终粗略图中的单个网格位置对应于长度为 $S$ 碱基对的一段输入DNA。我们用区间 $[a, a+S)$ 来表示这样一个区间，其中 $a$ 是某个起始位置。\n\n问题陈述，预测的切位点位置 $\\hat{x}$ 是所选粗粒度区间的中心。因此，对于跨越 $[a, a+S)$ 的区间，预测为：\n$$\n\\hat{x} = a + \\frac{S}{2}\n$$\n由于对齐未知，真实的切位点位置 $x^{\\ast}$ 被假定在此粗粒度区间内均匀分布。这可以表示为：\n$$\nx^{\\ast} \\sim U[a, a+S)\n$$\n其中 $U[c, d)$ 表示在区间 $[c, d)$ 上的连续均匀分布。\n\n我们关心的是定位误差，即差值 $\\hat{x} - x^{\\ast}$。我们定义一个随机变量 $\\epsilon$ 来表示这个误差：\n$$\n\\epsilon = \\hat{x} - x^{\\ast} = \\left(a + \\frac{S}{2}\\right) - x^{\\ast}\n$$\n由于 $x^{\\ast}$ 在 $[a, a+S)$ 上均匀分布，误差变量 $\\epsilon$ 在以下区间上均匀分布：\n$$\n\\left[ \\left(a + \\frac{S}{2}\\right) - (a+S), \\left(a + \\frac{S}{2}\\right) - a \\right] = \\left[ -\\frac{S}{2}, \\frac{S}{2} \\right]\n$$\n所以, $\\epsilon \\sim U[-S/2, S/2]$。 $\\epsilon$ 的概率密度函数 (PDF)，记作 $p(\\epsilon)$，是：\n$$\np(\\epsilon) = \\begin{cases} \\frac{1}{(S/2) - (-S/2)} = \\frac{1}{S}  \\text{对于 } \\epsilon \\in [-S/2, S/2] \\\\ 0  \\text{其它情况} \\end{cases}\n$$\n需要推导的量是期望平方定位误差 $E$：\n$$\nE = \\mathbb{E}[(\\hat{x} - x^{\\ast})^{2}] = \\mathbb{E}[\\epsilon^2]\n$$\n这个期望可以通过将 $\\epsilon^2$ 在其支撑集上对其PDF进行积分来计算：\n$$\nE = \\int_{-\\infty}^{\\infty} \\epsilon^2 p(\\epsilon) \\, d\\epsilon = \\int_{-S/2}^{S/2} \\epsilon^2 \\left(\\frac{1}{S}\\right) \\, d\\epsilon\n$$\n我们现在可以计算这个积分：\n$$\nE = \\frac{1}{S} \\left[ \\frac{\\epsilon^3}{3} \\right]_{-S/2}^{S/2} = \\frac{1}{3S} \\left[ \\left(\\frac{S}{2}\\right)^3 - \\left(-\\frac{S}{2}\\right)^3 \\right]\n$$\n$$\nE = \\frac{1}{3S} \\left[ \\frac{S^3}{8} - \\left(-\\frac{S^3}{8}\\right) \\right] = \\frac{1}{3S} \\left[ \\frac{S^3}{8} + \\frac{S^3}{8} \\right] = \\frac{1}{3S} \\left[ \\frac{2S^3}{8} \\right] = \\frac{1}{3S} \\left[ \\frac{S^3}{4} \\right]\n$$\n$$\nE = \\frac{S^2}{12}\n$$\n这个结果是一个以 0 为中心的均匀分布的方差，因为 $\\epsilon$ 的均值为 $\\mathbb{E}[\\epsilon]=0$。\n\n最后，我们将总区间大小 $S$ 的表达式（用各个池化步长 $\\{s_i\\}_{i=1}^{K}$ 表示）代入：\n$$\nE = \\frac{\\left( \\prod_{i=1}^{K} s_i \\right)^2}{12}\n$$\n这就是以平方碱基对为单位的期望平方定位误差的闭式解析表达式。\n\n这个表达式解释了为什么增加池化层的数量 $K$ 或任何给定池化层的步长 $s_i$ 会导致期望误差 $E$ 的增加。总下采样因子 $S = \\prod s_i$ 代表了空间量化区间的宽度。任何 $K$ 的增加都会引入一个新的乘法因子 $s_K > 1$（假设是非平凡的池化），而增加任何 $s_i$ 会直接增大乘积 $S$。期望平方误差 $E$ 与 $S^2$ 成正比。一个更大的区间大小 $S$ 意味着真实位置 $x^{\\ast}$ 在该区间内的不确定性更大。这种位置信息的损失在数学上被捕捉为在更宽区间上的均匀分布的更大方差，从而导致期望定位误差的平方级增大。这说明了网络设计中特征不变性/抽象与空间精度之间的一个基本权衡。",
            "answer": "$$\n\\boxed{\\frac{\\left( \\prod_{i=1}^{K} s_i \\right)^2}{12}}\n$$"
        },
        {
            "introduction": "一个强大的模型架构只是成功的一半；有效的训练同样至关重要，尤其是在处理真实世界基因组数据中普遍存在的类别不平衡问题时。本练习将重点从架构转移到训练动态上，通过对焦点损失（focal loss）进行分析，您将通过推导和比较梯度来理解它如何巧妙地调整学习过程。这项实践揭示了如何通过定制损失函数来应对数据挑战，将模型的注意力集中在最具信息量的困难样本上 。",
            "id": "4331425",
            "problem": "在系统生物医学中，训练一个卷积神经网络（CNN）来从脱氧核糖核酸（DNA）序列中预测转录因子结合位点，其中正类（结合）相对于负类（非结合）是罕见的。每个序列都经过独热编码，并由卷积层处理，然后是一个最终的逻辑（sigmoid）输出。设模型的 logit 为 $z \\in \\mathbb{R}$，其预测概率为 $p = \\sigma(z) = \\frac{1}{1 + \\exp(-z)}$。对于一个真实标签为 $y \\in \\{0,1\\}$ 的序列，二元交叉熵（逻辑损失）定义为 $L_{\\mathrm{log}}(p,y) = - y \\ln(p) - (1-y) \\ln(1-p)$，而焦点损失定义为 $L_{\\mathrm{foc}}(p,y) = - \\alpha \\,(1-p)^{\\gamma} y \\ln(p) - \\alpha\\, p^{\\gamma} (1-y) \\ln(1-p)$，其中 $\\alpha \\in (0,1]$ 是一个平衡因子，$\\gamma \\geq 0$ 是聚焦参数。考虑正类 $y = 1$ 并设置 $\\alpha = 1$。\n\n从上述定义和链式法则出发，推导在 $y=1$ 条件下，二元交叉熵和焦点损失相对于 logit $z$ 的梯度 $\\frac{\\partial L}{\\partial z}$。然后，定义梯度幅度缩放比\n$$S(p,\\gamma) = \\frac{\\left|\\frac{\\partial L_{\\mathrm{foc}}(p,1)}{\\partial z}\\right|}{\\left|\\frac{\\partial L_{\\mathrm{log}}(p,1)}{\\partial z}\\right|}.$$\n在基因组序列分类中，针对罕见正样本，在两个感兴趣的操作点上评估 $S(p,\\gamma)$：\n- 一个“简单”正样本，其中 $p = 0.99$，\n- 一个“困难”正样本，其中 $p = 0.01$，\n使用 $\\gamma = 2$。\n\n将 $S(0.99,2)$ 和 $S(0.01,2)$ 的两个数值以行矩阵的形式报告，顺序与简单和困难情况相对应。将每个条目四舍五入到四位有效数字。不需要单位。",
            "solution": "用户在计算生物学的机器学习领域提供了一个定义明确的问题。我将首先根据指定的协议验证问题陈述。\n\n### 第1步：提取已知条件\n- 模型 logit: $z \\in \\mathbb{R}$\n- 预测概率: $p = \\sigma(z) = \\frac{1}{1 + \\exp(-z)}$\n- 真实标签: $y \\in \\{0,1\\}$\n- 二元交叉熵（逻辑损失）: $L_{\\mathrm{log}}(p,y) = - y \\ln(p) - (1-y) \\ln(1-p)$\n- 焦点损失: $L_{\\mathrm{foc}}(p,y) = - \\alpha \\,(1-p)^{\\gamma} y \\ln(p) - \\alpha\\, p^{\\gamma} (1-y) \\ln(1-p)$\n- 平衡因子: $\\alpha \\in (0,1]$\n- 聚焦参数: $\\gamma \\geq 0$\n- 分析条件: 正类, $y = 1$\n- 参数值假设: $\\alpha = 1$\n- 梯度幅度缩放比定义: $S(p,\\gamma) = \\frac{\\left|\\frac{\\partial L_{\\mathrm{foc}}(p,1)}{\\partial z}\\right|}{\\left|\\frac{\\partial L_{\\mathrm{log}}(p,1)}{\\partial z}\\right|}$\n- 用于评估的参数值: $\\gamma = 2$\n- 评估点:\n  - “简单”正样本: $p = 0.99$\n  - “困难”正样本: $p = 0.01$\n- 要求输出: $S(0.99,2)$ 和 $S(0.01,2)$ 的数值，四舍五入到四位有效数字。\n\n### 第2步：使用提取的已知条件进行验证\n1.  **科学依据**：该问题在机器学习理论中有充分的依据。sigmoid 函数、二元交叉熵和焦点损失的定义都是标准的。它们在基因组序列分类中的应用是系统生物医学的一个主流研究领域。\n2.  **适定性**：该问题的结构保证了其具有唯一解。它提供了所有必需的函数、参数和评估点。推导梯度和计算特定比率的要求是明确的。\n3.  **客观性**：该问题使用精确的数学定义进行陈述。术语“简单”和“困难”样本由预测概率值（对于真阳性 $y=1$，$p=0.99$ 和 $p=0.01$）客观定义，这是一种标准惯例。\n4.  **完整性和一致性**：该问题是自洽且内部一致的。所有必需的参数（$\\alpha$, $\\gamma$）和变量（$y$, $p$）都为最终计算指定了值。\n5.  **现实性**：在现代生物信息学中，使用此类模型识别转录因子结合位点并处理类别不平衡问题的前提是非常现实的。参数值也是典型的。\n\n### 第3步：结论与行动\n该问题是有效的。我将继续进行完整的推导和求解。\n\n任务是推导对于一个正样本（$y=1$），二元交叉熵损失和焦点损失相对于 logit $z$ 的梯度，然后在特定点计算它们幅度的比率。\n\n首先，我们需要预测概率 $p$ 相对于 logit $z$ 的导数。函数 $p = \\sigma(z)$ 是 sigmoid 函数。\n$$p = \\sigma(z) = \\frac{1}{1 + \\exp(-z)} = (1 + \\exp(-z))^{-1}$$\n使用链式法则，我们求其关于 $z$ 的导数：\n$$\\frac{dp}{dz} = \\frac{d\\sigma(z)}{dz} = -1 (1 + \\exp(-z))^{-2} \\cdot (-\\exp(-z)) = \\frac{\\exp(-z)}{(1 + \\exp(-z))^2}$$\n我们可以用 $p$ 来表示这个导数：\n$$\\frac{dp}{dz} = \\frac{1}{1 + \\exp(-z)} \\cdot \\frac{\\exp(-z)}{1 + \\exp(-z)} = p \\cdot \\left(1 - \\frac{1}{1 + \\exp(-z)}\\right) = p(1-p)$$\n这是一个标准结果。现在可以使用链式法则求出损失函数关于 $z$ 的导数：$\\frac{\\partial L}{\\partial z} = \\frac{\\partial L}{\\partial p} \\frac{dp}{dz}$。\n\n**1. 二元交叉熵损失（$L_{\\mathrm{log}}$）的梯度**\n\n对于一个正样本，$y=1$。二元交叉熵损失简化为：\n$$L_{\\mathrm{log}}(p,1) = - (1) \\ln(p) - (1-1) \\ln(1-p) = -\\ln(p)$$\n关于 $p$ 的导数是：\n$$\\frac{\\partial L_{\\mathrm{log}}(p,1)}{\\partial p} = -\\frac{1}{p}$$\n应用链式法则求关于 $z$ 的梯度：\n$$\\frac{\\partial L_{\\mathrm{log}}(p,1)}{\\partial z} = \\frac{\\partial L_{\\mathrm{log}}(p,1)}{\\partial p} \\frac{dp}{dz} = \\left(-\\frac{1}{p}\\right) \\cdot (p(1-p)) = -(1-p) = p-1$$\n\n**2. 焦点损失（$L_{\\mathrm{foc}}$）的梯度**\n\n对于一个正样本（$y=1$）且平衡因子 $\\alpha=1$ 时，焦点损失简化为：\n$$L_{\\mathrm{foc}}(p,1) = - (1) (1-p)^{\\gamma} (1) \\ln(p) - (1) p^{\\gamma} (1-1) \\ln(1-p) = -(1-p)^{\\gamma} \\ln(p)$$\n我们使用乘法法则对这个表达式求关于 $p$ 的导数：\n$$\\frac{\\partial L_{\\mathrm{foc}}(p,1)}{\\partial p} = \\frac{\\partial}{\\partial p} \\left[ -(1-p)^{\\gamma} \\ln(p) \\right]$$\n$$= -\\left( (\\gamma(1-p)^{\\gamma-1}(-1)) \\ln(p) + (1-p)^{\\gamma} \\frac{1}{p} \\right)$$\n$$= -\\left( -\\gamma(1-p)^{\\gamma-1} \\ln(p) + \\frac{(1-p)^{\\gamma}}{p} \\right)$$\n$$= \\gamma(1-p)^{\\gamma-1} \\ln(p) - \\frac{(1-p)^{\\gamma}}{p}$$\n现在，我们应用链式法则求关于 $z$ 的梯度：\n$$\\frac{\\partial L_{\\mathrm{foc}}(p,1)}{\\partial z} = \\frac{\\partial L_{\\mathrm{foc}}(p,1)}{\\partial p} \\frac{dp}{dz} = \\left( \\gamma(1-p)^{\\gamma-1} \\ln(p) - \\frac{(1-p)^{\\gamma}}{p} \\right) \\cdot (p(1-p))$$\n分配 $p(1-p)$ 项：\n$$\\frac{\\partial L_{\\mathrm{foc}}(p,1)}{\\partial z} = \\gamma p (1-p)^{\\gamma} \\ln(p) - (1-p)^{\\gamma+1}$$\n$$= (1-p)^{\\gamma} \\left[ \\gamma p \\ln(p) - (1-p) \\right] = (1-p)^{\\gamma} \\left[ \\gamma p \\ln(p) + p - 1 \\right]$$\n\n**3. 梯度幅度缩放比（$S(p,\\gamma)$）**\n\n该比率定义为：\n$$S(p,\\gamma) = \\frac{\\left|\\frac{\\partial L_{\\mathrm{foc}}(p,1)}{\\partial z}\\right|}{\\left|\\frac{\\partial L_{\\mathrm{log}}(p,1)}{\\partial z}\\right|}$$\n我们已经得到梯度：\n$$\\frac{\\partial L_{\\mathrm{log}}(p,1)}{\\partial z} = p-1$$\n$$\\frac{\\partial L_{\\mathrm{foc}}(p,1)}{\\partial z} = (1-p)^{\\gamma} (\\gamma p \\ln(p) + p-1)$$\n对于 $p \\in (0,1)$，我们有 $p-1  0$ 和 $\\ln(p)  0$。由于 $\\gamma \\ge 0$，项 $\\gamma p \\ln(p)$ 是非正的。因此，整个括号内的项 $(\\gamma p \\ln(p) + p-1)$ 是负的。项 $(1-p)^{\\gamma}$ 是正的。因此，对于 $p \\in (0,1)$，两个梯度都是负的。\n它们的绝对值是它们的相反数：\n$$ \\left|\\frac{\\partial L_{\\mathrm{log}}(p,1)}{\\partial z}\\right| = -(p-1) = 1-p $$\n$$ \\left|\\frac{\\partial L_{\\mathrm{foc}}(p,1)}{\\partial z}\\right| = -(1-p)^{\\gamma} (\\gamma p \\ln(p) + p-1) = (1-p)^{\\gamma} (1-p - \\gamma p \\ln(p)) $$\n该比率为：\n$$S(p,\\gamma) = \\frac{(1-p)^{\\gamma} (1-p - \\gamma p \\ln(p))}{1-p} = (1-p)^{\\gamma-1} (1-p - \\gamma p \\ln(p))$$\n\n**4. 数值评估**\n\n我们必须在 $p=0.99$ 和 $p=0.01$ 处评估当 $\\gamma=2$ 时的 $S(p,\\gamma)$。当 $\\gamma=2$ 时的具体公式为：\n$$S(p,2) = (1-p)^{2-1} (1-p - 2p \\ln(p)) = (1-p)(1-p - 2p \\ln(p))$$\n\n情况1：“简单”正样本，$p = 0.99$。\n$$S(0.99,2) = (1-0.99)(1 - 0.99 - 2(0.99)\\ln(0.99))$$\n$$S(0.99,2) = (0.01)(0.01 - 1.98 \\ln(0.99))$$\n使用 $\\ln(0.99) \\approx -0.01005033585$：\n$$S(0.99,2) \\approx (0.01)(0.01 - 1.98(-0.01005033585))$$\n$$S(0.99,2) \\approx (0.01)(0.01 + 0.019899665)$$\n$$S(0.99,2) \\approx (0.01)(0.029899665) = 0.00029899665$$\n四舍五入到四位有效数字，我们得到 $0.0002990$。\n\n情况2：“困难”正样本，$p = 0.01$。\n$$S(0.01,2) = (1-0.01)(1 - 0.01 - 2(0.01)\\ln(0.01))$$\n$$S(0.01,2) = (0.99)(0.99 - 0.02 \\ln(0.01))$$\n使用 $\\ln(0.01) = \\ln(10^{-2}) = -2\\ln(10) \\approx -4.605170186$：\n$$S(0.01,2) \\approx (0.99)(0.99 - 0.02(-4.605170186))$$\n$$S(0.01,2) \\approx (0.99)(0.99 + 0.0921034037)$$\n$$S(0.01,2) \\approx (0.99)(1.0821034037) = 1.07128237$$\n四舍五入到四位有效数字，我们得到 $1.071$。\n\n结果展示了焦点损失的“聚焦”特性。对于一个简单的正样本（$p=0.99$），梯度幅度被缩小了约 $3 \\times 10^{-4}$ 倍，从而有效地减少了其对权重更新的贡献。对于一个困难的正样本（$p=0.01$），梯度幅度与标准交叉熵损失的梯度幅度相当（且略大），确保模型将其学习重点放在这个被错误分类的样本上。\n\n所要求的值是 $S(0.99,2) = 0.0002990$ 和 $S(0.01,2) = 1.071$。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.0002990  1.071\n\\end{pmatrix}\n}\n$$"
        }
    ]
}