{
    "hands_on_practices": [
        {
            "introduction": "To truly grasp Canonical Correlation Analysis (CCA), it is essential to build it from the ground up. This first practice exercise guides you through the fundamental calculations, starting from raw, hypothetical data matrices representing two different 'omics' views. By computing the empirical means and covariance matrices by hand, you will reinforce your understanding of these basic statistical building blocks before proceeding to derive and solve the core CCA eigenproblem, all while considering the practical implications of sample size in high-dimensional settings .",
            "id": "4322590",
            "problem": "A systems biomedicine study collects two omics views on the same cohort of $n$ patients. Let $X \\in \\mathbb{R}^{n \\times p}$ denote transcript abundance for $p$ genes and $Y \\in \\mathbb{R}^{n \\times q}$ denote metabolite intensities for $q$ metabolites, both measured on the same $n$ patients. Consider the following empirical data from $n = 4$ patients with $p = 2$ and $q = 2$:\n$$\nX \\;=\\; \\begin{pmatrix}\n2 & 0 \\\\\n0 & 2 \\\\\n2 & 2 \\\\\n4 & 2\n\\end{pmatrix},\n\\qquad\nY \\;=\\; \\begin{pmatrix}\n1 & 1 \\\\\n1 & 3 \\\\\n2 & 3 \\\\\n3 & 4\n\\end{pmatrix}.\n$$\nStarting from first principles (that is, the definitions of empirical mean and covariance as sample analogs of population mean and covariance), do the following:\n1) Compute the column-wise empirical means for $X$ and $Y$ and form the column-centered data matrices. Using the maximum-likelihood normalization for a multivariate normal model with unknown mean, compute the empirical covariances $\\hat{\\Sigma}_{XX} \\in \\mathbb{R}^{p \\times p}$, $\\hat{\\Sigma}_{YY} \\in \\mathbb{R}^{q \\times q}$, and the empirical cross-covariance $\\hat{\\Sigma}_{XY} \\in \\mathbb{R}^{p \\times q}$, each defined with a factor of $1/n$.\n2) Briefly discuss, in the context of multi-view learning for systems biomedicine, how the relationship of $n$ to $p$ and $q$ affects bias and conditioning of these estimators, and what principled remedies are commonly adopted when $n$ is not large relative to $p$ or $q$.\n3) Using only the variational definition of Canonical Correlation Analysis (CCA)—namely, maximizing the empirical correlation between $a^{\\top}X$ and $b^{\\top}Y$ over linear combinations $a \\in \\mathbb{R}^{p}$ and $b \\in \\mathbb{R}^{q}$ subject to unit empirical variance constraints computed from the $1/n$-normalized covariances—derive the algebraic condition that determines the canonical correlations and then evaluate the largest canonical correlation for the given $X$ and $Y$.\n\nGive your final numeric answer to part $3$ rounded to four significant figures. No physical units are required, and angles are not involved.",
            "solution": "The problem is valid as it is scientifically grounded in standard multivariate statistics, well-posed, objective, and contains all necessary information for a unique solution. We proceed by addressing the three parts of the problem in sequence.\n\nThe given data matrices are $X \\in \\mathbb{R}^{n \\times p}$ and $Y \\in \\mathbb{R}^{n \\times q}$, with $n=4$, $p=2$, and $q=2$.\n$$\nX \\;=\\; \\begin{pmatrix}\n2 & 0 \\\\\n0 & 2 \\\\\n2 & 2 \\\\\n4 & 2\n\\end{pmatrix},\n\\qquad\nY \\;=\\; \\begin{pmatrix}\n1 & 1 \\\\\n1 & 3 \\\\\n2 & 3 \\\\\n3 & 4\n\\end{pmatrix}.\n$$\n\nPart 1: Computation of Empirical Means and Covariances\n\nFirst, we compute the column-wise empirical means, $\\bar{x} \\in \\mathbb{R}^{p}$ and $\\bar{y} \\in \\mathbb{R}^{q}$.\nFor matrix $X$, the mean vector $\\bar{x}$ has components:\n$$ \\bar{x}_1 = \\frac{1}{4}(2+0+2+4) = \\frac{8}{4} = 2 $$\n$$ \\bar{x}_2 = \\frac{1}{4}(0+2+2+2) = \\frac{6}{4} = \\frac{3}{2} = 1.5 $$\nSo, $\\bar{x} = \\begin{pmatrix} 2 \\\\ 1.5 \\end{pmatrix}$.\n\nFor matrix $Y$, the mean vector $\\bar{y}$ has components:\n$$ \\bar{y}_1 = \\frac{1}{4}(1+1+2+3) = \\frac{7}{4} = 1.75 $$\n$$ \\bar{y}_2 = \\frac{1}{4}(1+3+3+4) = \\frac{11}{4} = 2.75 $$\nSo, $\\bar{y} = \\begin{pmatrix} 1.75 \\\\ 2.75 \\end{pmatrix}$.\n\nNext, we form the column-centered data matrices, $X_c = X - \\mathbf{1}\\bar{x}^{\\top}$ and $Y_c = Y - \\mathbf{1}\\bar{y}^{\\top}$, where $\\mathbf{1}$ is a column vector of $n=4$ ones.\n$$\nX_c = \\begin{pmatrix}\n2-2 & 0-1.5 \\\\\n0-2 & 2-1.5 \\\\\n2-2 & 2-1.5 \\\\\n4-2 & 2-1.5\n\\end{pmatrix}\n= \\begin{pmatrix}\n0 & -1.5 \\\\\n-2 & 0.5 \\\\\n0 & 0.5 \\\\\n2 & 0.5\n\\end{pmatrix}\n$$\n$$\nY_c = \\begin{pmatrix}\n1-1.75 & 1-2.75 \\\\\n1-1.75 & 3-2.75 \\\\\n2-1.75 & 3-2.75 \\\\\n3-1.75 & 4-2.75\n\\end{pmatrix}\n= \\begin{pmatrix}\n-0.75 & -1.75 \\\\\n-0.75 & 0.25 \\\\\n0.25 & 0.25 \\\\\n1.25 & 1.25\n\\end{pmatrix}\n$$\nNow, we compute the empirical covariance matrices using the specified maximum-likelihood normalization factor of $1/n$.\nThe empirical covariance for $X$ is $\\hat{\\Sigma}_{XX} = \\frac{1}{n} X_c^{\\top}X_c$:\n$$\n\\hat{\\Sigma}_{XX} = \\frac{1}{4} \\begin{pmatrix} 0 & -2 & 0 & 2 \\\\ -1.5 & 0.5 & 0.5 & 0.5 \\end{pmatrix} \\begin{pmatrix} 0 & -1.5 \\\\ -2 & 0.5 \\\\ 0 & 0.5 \\\\ 2 & 0.5 \\end{pmatrix} = \\frac{1}{4} \\begin{pmatrix} 8 & 0 \\\\ 0 & 3 \\end{pmatrix} = \\begin{pmatrix} 2 & 0 \\\\ 0 & 0.75 \\end{pmatrix}\n$$\nThe empirical covariance for $Y$ is $\\hat{\\Sigma}_{YY} = \\frac{1}{n} Y_c^{\\top}Y_c$:\n$$\n\\hat{\\Sigma}_{YY} = \\frac{1}{4} \\begin{pmatrix} -0.75 & -0.75 & 0.25 & 1.25 \\\\ -1.75 & 0.25 & 0.25 & 1.25 \\end{pmatrix} \\begin{pmatrix} -0.75 & -1.75 \\\\ -0.75 & 0.25 \\\\ 0.25 & 0.25 \\\\ 1.25 & 1.25 \\end{pmatrix} = \\frac{1}{4} \\begin{pmatrix} 2.75 & 2.75 \\\\ 2.75 & 4.75 \\end{pmatrix} = \\begin{pmatrix} 11/16 & 11/16 \\\\ 11/16 & 19/16 \\end{pmatrix}\n$$\nThe empirical cross-covariance is $\\hat{\\Sigma}_{XY} = \\frac{1}{n} X_c^{\\top}Y_c$:\n$$\n\\hat{\\Sigma}_{XY} = \\frac{1}{4} \\begin{pmatrix} 0 & -2 & 0 & 2 \\\\ -1.5 & 0.5 & 0.5 & 0.5 \\end{pmatrix} \\begin{pmatrix} -0.75 & -1.75 \\\\ -0.75 & 0.25 \\\\ 0.25 & 0.25 \\\\ 1.25 & 1.25 \\end{pmatrix} = \\frac{1}{4} \\begin{pmatrix} 4 & 2 \\\\ 1.5 & 3.5 \\end{pmatrix} = \\begin{pmatrix} 1 & 0.5 \\\\ 0.375 & 0.875 \\end{pmatrix}\n$$\n\nPart 2: Discussion of Estimator Properties\n\nIn multi-view learning applications like systems biomedicine, it is common to encounter a \"high-dimension, low-sample-size\" setting where the number of features ($p$ and $q$) vastly exceeds the number of samples ($n$), i.e., $p \\gg n$ and $q \\gg n$. This scenario severely affects the properties of the empirical covariance estimators.\n\n1.  Bias and Variance: The MLE estimator for covariance (with $1/n$ normalization) is known to be biased, underestimating the true variance. While the unbiased estimator (with $1/(n-1)$ normalization) corrects this, the difference is marginal for large $n$. In high-dimensional settings, the dominant problem is the high variance of the estimator. The sample eigenvalues are more dispersed than the population eigenvalues, leading to instability and poor out-of-sample performance.\n\n2.  Conditioning and Singularity: When $p > n-1$ or $q > n-1$, the centered data matrix $X_c$ or $Y_c$ will be rank-deficient (its column rank is at most $n-1$). Consequently, the empirical covariance matrices $\\hat{\\Sigma}_{XX} = \\frac{1}{n} X_c^{\\top}X_c$ and $\\hat{\\Sigma}_{YY} = \\frac{1}{n} Y_c^{\\top}Y_c$ become singular (non-invertible). This is catastrophic for classical CCA, as its solution requires the inversion of these matrices.\n\nPrincipled remedies are necessary to address these issues:\n- Regularization: The most common approach is Regularized CCA (RCCA). A multiple of the identity matrix is added to the empirical covariance matrices, i.e., $\\hat{\\Sigma}_{XX} + \\lambda_x I$ and $\\hat{\\Sigma}_{YY} + \\lambda_y I$. This Tikhonov regularization makes the matrices positive definite and invertible, improving their conditioning. The regularization parameters $\\lambda_x, \\lambda_y > 0$ are typically chosen via cross-validation. This introduces bias but reduces variance, leading to a better overall model.\n- Sparsity: In high dimensions, it is often assumed that only a subset of features is relevant. Sparse CCA methods impose penalties (e.g., L1-norm) on the canonical vectors $a$ and $b$. This encourages sparsity (many zero elements), effectively performing feature selection. This not only regularizes the problem but also enhances interpretability, a crucial aspect in systems biomedicine for identifying key biomarkers.\n\nPart 3: Derivation and Evaluation of the Largest Canonical Correlation\n\nCanonical Correlation Analysis (CCA) seeks linear combinations of the variables in $X$ and $Y$ that are maximally correlated. Let the canonical variates be $u = X_c a$ and $v = Y_c b$, where $a \\in \\mathbb{R}^p$ and $b \\in \\mathbb{R}^q$ are the canonical weight vectors. The variational problem is to maximize the correlation $\\rho = \\text{corr}(u,v)$ subject to normalization constraints.\nThe correlation is defined as:\n$$ \\rho = \\frac{\\text{cov}(u, v)}{\\sqrt{\\text{var}(u) \\text{var}(v)}} = \\frac{a^{\\top}\\hat{\\Sigma}_{XY}b}{\\sqrt{(a^{\\top}\\hat{\\Sigma}_{XX}a)(b^{\\top}\\hat{\\Sigma}_{YY}b)}} $$\nThe optimization problem is formulated as maximizing the covariance $a^{\\top}\\hat{\\Sigma}_{XY}b$ subject to unit variance constraints:\n$$ \\max_{a,b} a^{\\top}\\hat{\\Sigma}_{XY}b \\quad \\text{subject to} \\quad a^{\\top}\\hat{\\Sigma}_{XX}a = 1 \\text{ and } b^{\\top}\\hat{\\Sigma}_{YY}b = 1 $$\nWe solve this using Lagrange multipliers. The Lagrangian is:\n$$ \\mathcal{L}(a, b, \\lambda_a, \\lambda_b) = a^{\\top}\\hat{\\Sigma}_{XY}b - \\frac{\\lambda_a}{2}(a^{\\top}\\hat{\\Sigma}_{XX}a - 1) - \\frac{\\lambda_b}{2}(b^{\\top}\\hat{\\Sigma}_{YY}b - 1) $$\nTaking derivatives and setting them to zero yields the coupled equations:\n$$ \\frac{\\partial\\mathcal{L}}{\\partial a} = \\hat{\\Sigma}_{XY}b - \\lambda_a \\hat{\\Sigma}_{XX}a = 0 \\implies \\hat{\\Sigma}_{XY}b = \\lambda_a \\hat{\\Sigma}_{XX}a $$\n$$ \\frac{\\partial\\mathcal{L}}{\\partial b} = \\hat{\\Sigma}_{YX}a - \\lambda_b \\hat{\\Sigma}_{YY}b = 0 \\implies \\hat{\\Sigma}_{YX}a = \\lambda_b \\hat{\\Sigma}_{YY}b $$\nwhere $\\hat{\\Sigma}_{YX} = \\hat{\\Sigma}_{XY}^{\\top}$. Left-multiplying the first equation by $a^{\\top}$ and the second by $b^{\\top}$ and applying the constraints shows that $\\lambda_a = \\lambda_b = a^{\\top}\\hat{\\Sigma}_{XY}b = \\rho$. The canonical correlation $\\rho$ is the Lagrange multiplier.\n\nTo derive the algebraic condition, we solve for $a$ and $b$. Assuming $\\hat{\\Sigma}_{XX}$ and $\\hat{\\Sigma}_{YY}$ are invertible (which they are for this problem, as $n-1 > p,q$):\n$$ a = \\frac{1}{\\rho}\\hat{\\Sigma}_{XX}^{-1}\\hat{\\Sigma}_{XY}b $$\nSubstituting this into the second equation:\n$$ \\hat{\\Sigma}_{YX}\\left(\\frac{1}{\\rho}\\hat{\\Sigma}_{XX}^{-1}\\hat{\\Sigma}_{XY}b\\right) = \\rho\\hat{\\Sigma}_{YY}b $$\n$$ \\hat{\\Sigma}_{YX}\\hat{\\Sigma}_{XX}^{-1}\\hat{\\Sigma}_{XY}b = \\rho^2\\hat{\\Sigma}_{YY}b $$\n$$ (\\hat{\\Sigma}_{YY}^{-1}\\hat{\\Sigma}_{YX}\\hat{\\Sigma}_{XX}^{-1}\\hat{\\Sigma}_{XY})b = \\rho^2 b $$\nThis is a standard eigenvalue problem. The squared canonical correlations $\\rho^2$ are the eigenvalues of the matrix $K = \\hat{\\Sigma}_{YY}^{-1}\\hat{\\Sigma}_{YX}\\hat{\\Sigma}_{XX}^{-1}\\hat{\\Sigma}_{XY}$. The largest canonical correlation is the square root of the largest eigenvalue of $K$. An equivalent problem finds $\\rho^2$ as the eigenvalues of $K' = \\hat{\\Sigma}_{XX}^{-1}\\hat{\\Sigma}_{XY}\\hat{\\Sigma}_{YY}^{-1}\\hat{\\Sigma}_{YX}$.\n\nWe now compute the largest canonical correlation for the given data. We use the matrix $K'$:\n$$ \\hat{\\Sigma}_{XX}^{-1} = \\begin{pmatrix} 2 & 0 \\\\ 0 & 3/4 \\end{pmatrix}^{-1} = \\begin{pmatrix} 1/2 & 0 \\\\ 0 & 4/3 \\end{pmatrix} $$\n$$ \\det(\\hat{\\Sigma}_{YY}) = (\\frac{11}{16})(\\frac{19}{16}) - (\\frac{11}{16})^2 = \\frac{11}{16}\\frac{8}{16} = \\frac{11}{32} $$\n$$ \\hat{\\Sigma}_{YY}^{-1} = \\frac{32}{11} \\begin{pmatrix} 19/16 & -11/16 \\\\ -11/16 & 11/16 \\end{pmatrix} = \\frac{2}{11} \\begin{pmatrix} 19 & -11 \\\\ -11 & 11 \\end{pmatrix} $$\nWe compute $K' = (\\hat{\\Sigma}_{XX}^{-1} \\hat{\\Sigma}_{XY}) (\\hat{\\Sigma}_{YY}^{-1} \\hat{\\Sigma}_{YX})$:\n$$ \\hat{\\Sigma}_{XX}^{-1} \\hat{\\Sigma}_{XY} = \\begin{pmatrix} 1/2 & 0 \\\\ 0 & 4/3 \\end{pmatrix} \\begin{pmatrix} 1 & 1/2 \\\\ 3/8 & 7/8 \\end{pmatrix} = \\begin{pmatrix} 1/2 & 1/4 \\\\ 1/2 & 7/6 \\end{pmatrix} $$\n$$ \\hat{\\Sigma}_{YY}^{-1} \\hat{\\Sigma}_{YX} = \\frac{2}{11} \\begin{pmatrix} 19 & -11 \\\\ -11 & 11 \\end{pmatrix} \\begin{pmatrix} 1 & 3/8 \\\\ 1/2 & 7/8 \\end{pmatrix} = \\frac{1}{11} \\begin{pmatrix} 27 & -5 \\\\ -11 & 11 \\end{pmatrix} $$\n$$ K' = \\begin{pmatrix} 1/2 & 1/4 \\\\ 1/2 & 7/6 \\end{pmatrix} \\frac{1}{11} \\begin{pmatrix} 27 & -5 \\\\ -11 & 11 \\end{pmatrix} = \\frac{1}{11} \\begin{pmatrix} 27/2-11/4 & -5/2+11/4 \\\\ 27/2-77/6 & -5/2+77/6 \\end{pmatrix} $$\n$$ K' = \\frac{1}{11} \\begin{pmatrix} 43/4 & 1/4 \\\\ 4/6 & 62/6 \\end{pmatrix} = \\frac{1}{11} \\begin{pmatrix} 43/4 & 1/4 \\\\ 2/3 & 31/3 \\end{pmatrix} = \\begin{pmatrix} 43/44 & 1/44 \\\\ 2/33 & 31/33 \\end{pmatrix} $$\nThe eigenvalues $\\lambda = \\rho^2$ of $K'$ are the roots of the characteristic equation $\\det(K' - \\lambda I) = 0$:\n$$ ( \\frac{43}{44} - \\lambda ) ( \\frac{31}{33} - \\lambda ) - ( \\frac{1}{44} ) ( \\frac{2}{33} ) = 0 $$\n$$ \\lambda^2 - (\\frac{43}{44}+\\frac{31}{33})\\lambda + \\frac{43 \\cdot 31 - 2}{44 \\cdot 33} = 0 $$\nThe trace is $\\text{tr}(K') = \\frac{129+124}{132} = \\frac{253}{132} = \\frac{23 \\cdot 11}{12 \\cdot 11} = \\frac{23}{12}$.\nThe determinant is $\\det(K') = \\frac{1333-2}{1452} = \\frac{1331}{1452} = \\frac{11^3}{12 \\cdot 11^2} = \\frac{11}{12}$.\nThe characteristic equation is $\\lambda^2 - \\frac{23}{12}\\lambda + \\frac{11}{12} = 0$, which is $12\\lambda^2 - 23\\lambda + 11 = 0$.\nFactoring this quadratic equation: $(12\\lambda - 11)(\\lambda - 1) = 0$.\nThe roots are $\\lambda_1 = 1$ and $\\lambda_2 = 11/12$.\nThe eigenvalues are the squared canonical correlations, $\\rho^2$. The largest eigenvalue is $\\lambda_{\\max} = 1$. The largest canonical correlation is thus $\\rho_{\\max} = \\sqrt{1} = 1$.\nThis result is expected. A canonical correlation of $1$ is guaranteed if $p+q > n-1$. In this problem, $p+q = 2+2=4$ and $n-1 = 4-1=3$. Since $4>3$, at least one perfect correlation is guaranteed.\nThe value rounded to four significant figures is $1.000$.",
            "answer": "$$\\boxed{1.000}$$"
        },
        {
            "introduction": "Beyond the classical eigenvalue formulation, CCA can be elegantly solved through data whitening and Singular Value Decomposition (SVD), a perspective that is both computationally stable and conceptually insightful. This exercise demonstrates this powerful approach by having you first transform the data into a \"whitened\" space where the within-view covariances are identity matrices, simplifying the problem significantly. You will then see how the SVD of the whitened cross-covariance matrix directly yields the canonical correlations and directions, revealing the deep connection between CCA and SVD .",
            "id": "4322598",
            "problem": "A systems biomedicine study investigates how gene expression measurements and protein abundance measurements co-vary across a cohort of patients. Let the random vectors be $x \\in \\mathbb{R}^{2}$ (gene expression) and $y \\in \\mathbb{R}^{2}$ (protein abundance), each centered to have zero mean across patients. Assume the following empirically estimated second-order statistics are scientifically plausible and internally consistent:\n- The within-modality covariance matrices are $\\Sigma_{xx} = \\begin{pmatrix}4 & 0 \\\\ 0 & 1\\end{pmatrix}$ and $\\Sigma_{yy} = \\begin{pmatrix}9 & 0 \\\\ 0 & 4\\end{pmatrix}$, which are symmetric and positive definite.\n- The cross-modality covariance matrix is $\\Sigma_{xy} = \\begin{pmatrix}3 & 1 \\\\ \\frac{3}{4} & 1\\end{pmatrix}$.\n\nCanonical Correlation Analysis (CCA) seeks linear combinations $a^{\\top} x$ and $b^{\\top} y$ that maximize the correlation under the normalization constraints $a^{\\top} \\Sigma_{xx} a = 1$ and $b^{\\top} \\Sigma_{yy} b = 1$. In multi-view learning, a standard approach to solve this is to whiten each view and analyze the singular values of an appropriately normalized cross-covariance.\n\nStarting from the fundamental definitions of covariance, correlation under linear transformations, and the spectral properties of symmetric positive definite matrices, perform the following tasks:\n- Compute the symmetric inverse square roots $\\Sigma_{xx}^{-1/2}$ and $\\Sigma_{yy}^{-1/2}$.\n- Using these, compute the whitened cross-covariance $C = \\Sigma_{xx}^{-1/2} \\Sigma_{xy} \\Sigma_{yy}^{-1/2}$.\n- Perform the Singular Value Decomposition (SVD), that is, the factorization $C = U \\,\\mathrm{diag}(\\rho_{1}, \\rho_{2})\\, V^{\\top}$, where $U$ and $V$ are orthonormal, and identify the top singular components $(\\rho_{1}, u_{1}, v_{1})$.\n- Reconstruct the top canonical directions $a$ and $b$ in the original variable space such that $a^{\\top} \\Sigma_{xx} a = 1$ and $b^{\\top} \\Sigma_{yy} b = 1$.\n\nReport, as your final answer, the largest canonical correlation $\\rho_{1}$ as a single real number. No rounding is required; provide the exact value with no units.",
            "solution": "The problem asks for the largest canonical correlation between two random vectors, $x \\in \\mathbb{R}^{2}$ and $y \\in \\mathbb{R}^{2}$, given their covariance matrices. The problem statement has been validated and is deemed sound, well-posed, and self-contained.\n\nCanonical Correlation Analysis (CCA) seeks to find linear combinations of the variables, $u = a^{\\top}x$ and $v = b^{\\top}y$, that are maximally correlated. The correlation is given by:\n$$\n\\rho = \\frac{\\text{Cov}(u, v)}{\\sqrt{\\text{Var}(u) \\text{Var}(v)}} = \\frac{a^{\\top} \\Sigma_{xy} b}{\\sqrt{(a^{\\top} \\Sigma_{xx} a)(b^{\\top} \\Sigma_{yy} b)}}\n$$\nThe problem specifies normalization constraints $a^{\\top} \\Sigma_{xx} a = 1$ and $b^{\\top} \\Sigma_{yy} b = 1$, which simplifies the objective to maximizing $\\rho = a^{\\top} \\Sigma_{xy} b$.\n\nThis constrained optimization problem can be solved by first \"whitening\" the variables. We define whitened variables $\\tilde{x} = \\Sigma_{xx}^{-1/2} x$ and $\\tilde{y} = \\Sigma_{yy}^{-1/2} y$. The covariance matrices of these new variables are identity matrices:\n$$\n\\text{Cov}(\\tilde{x}) = \\Sigma_{xx}^{-1/2} \\Sigma_{xx} (\\Sigma_{xx}^{-1/2})^{\\top} = \\Sigma_{xx}^{-1/2} \\Sigma_{xx} \\Sigma_{xx}^{-1/2} = I\n$$\n$$\n\\text{Cov}(\\tilde{y}) = \\Sigma_{yy}^{-1/2} \\Sigma_{yy} (\\Sigma_{yy}^{-1/2})^{\\top} = \\Sigma_{yy}^{-1/2} \\Sigma_{yy} \\Sigma_{yy}^{-1/2} = I\n$$\nThe cross-covariance matrix of the whitened variables is:\n$$\nC = \\text{Cov}(\\tilde{x}, \\tilde{y}) = \\Sigma_{xx}^{-1/2} \\Sigma_{xy} (\\Sigma_{yy}^{-1/2})^{\\top} = \\Sigma_{xx}^{-1/2} \\Sigma_{xy} \\Sigma_{yy}^{-1/2}\n$$\nIn terms of the whitened variables, the canonical variables are defined by projection vectors $u$ and $v$ such that $a = \\Sigma_{xx}^{-1/2} u$ and $b = \\Sigma_{yy}^{-1/2} v$. The normalization constraints become:\n$$\na^{\\top} \\Sigma_{xx} a = (\\Sigma_{xx}^{-1/2} u)^{\\top} \\Sigma_{xx} (\\Sigma_{xx}^{-1/2} u) = u^{\\top} \\Sigma_{xx}^{-1/2} \\Sigma_{xx} \\Sigma_{xx}^{-1/2} u = u^{\\top}u = 1\n$$\n$$\nb^{\\top} \\Sigma_{yy} b = (\\Sigma_{yy}^{-1/2} v)^{\\top} \\Sigma_{yy} (\\Sigma_{yy}^{-1/2} v) = v^{\\top} \\Sigma_{yy}^{-1/2} \\Sigma_{yy} \\Sigma_{yy}^{-1/2} v = v^{\\top}v = 1\n$$\nThe correlation to maximize is $\\rho = u^{\\top} C v$, subject to $u^{\\top}u = 1$ and $v^{\\top}v = 1$. The solutions to this problem are given by the Singular Value Decomposition (SVD) of the matrix $C$. The maximum value of $\\rho$ is the largest singular value of $C$.\n\nThe steps are as follows:\n\n1.  **Compute the symmetric inverse square roots $\\Sigma_{xx}^{-1/2}$ and $\\Sigma_{yy}^{-1/2}$.**\n    The given covariance matrices are diagonal:\n    $$\n    \\Sigma_{xx} = \\begin{pmatrix}4 & 0 \\\\ 0 & 1\\end{pmatrix}, \\quad \\Sigma_{yy} = \\begin{pmatrix}9 & 0 \\\\ 0 & 4\\end{pmatrix}\n    $$\n    For a diagonal matrix, the inverse square root is found by taking the inverse square root of each diagonal element.\n    $$\n    \\Sigma_{xx}^{-1/2} = \\begin{pmatrix}4^{-1/2} & 0 \\\\ 0 & 1^{-1/2}\\end{pmatrix} = \\begin{pmatrix}\\frac{1}{2} & 0 \\\\ 0 & 1\\end{pmatrix}\n    $$\n    $$\n    \\Sigma_{yy}^{-1/2} = \\begin{pmatrix}9^{-1/2} & 0 \\\\ 0 & 4^{-1/2}\\end{pmatrix} = \\begin{pmatrix}\\frac{1}{3} & 0 \\\\ 0 & \\frac{1}{2}\\end{pmatrix}\n    $$\n\n2.  **Compute the whitened cross-covariance matrix $C$.**\n    $C = \\Sigma_{xx}^{-1/2} \\Sigma_{xy} \\Sigma_{yy}^{-1/2}$, with $\\Sigma_{xy} = \\begin{pmatrix}3 & 1 \\\\ \\frac{3}{4} & 1\\end{pmatrix}$.\n    $$\n    C = \\begin{pmatrix}\\frac{1}{2} & 0 \\\\ 0 & 1\\end{pmatrix} \\begin{pmatrix}3 & 1 \\\\ \\frac{3}{4} & 1\\end{pmatrix} \\begin{pmatrix}\\frac{1}{3} & 0 \\\\ 0 & \\frac{1}{2}\\end{pmatrix}\n    $$\n    First, we compute the product $\\Sigma_{xx}^{-1/2} \\Sigma_{xy}$:\n    $$\n    \\begin{pmatrix}\\frac{1}{2} & 0 \\\\ 0 & 1\\end{pmatrix} \\begin{pmatrix}3 & 1 \\\\ \\frac{3}{4} & 1\\end{pmatrix} = \\begin{pmatrix}(\\frac{1}{2})(3) & (\\frac{1}{2})(1) \\\\ (1)(\\frac{3}{4}) & (1)(1)\\end{pmatrix} = \\begin{pmatrix}\\frac{3}{2} & \\frac{1}{2} \\\\ \\frac{3}{4} & 1\\end{pmatrix}\n    $$\n    Next, we multiply this result by $\\Sigma_{yy}^{-1/2}$:\n    $$\n    C = \\begin{pmatrix}\\frac{3}{2} & \\frac{1}{2} \\\\ \\frac{3}{4} & 1\\end{pmatrix} \\begin{pmatrix}\\frac{1}{3} & 0 \\\\ 0 & \\frac{1}{2}\\end{pmatrix} = \\begin{pmatrix}(\\frac{3}{2})(\\frac{1}{3}) & (\\frac{1}{2})(\\frac{1}{2}) \\\\ (\\frac{3}{4})(\\frac{1}{3}) & (1)(\\frac{1}{2})\\end{pmatrix} = \\begin{pmatrix}\\frac{1}{2} & \\frac{1}{4} \\\\ \\frac{1}{4} & \\frac{1}{2}\\end{pmatrix}\n    $$\n\n3.  **Find the singular values of $C$.**\n    The canonical correlations are the singular values of $C$. The singular values of a matrix are the square roots of the eigenvalues of $C^{\\top}C$. Since our matrix $C$ is symmetric, its singular values are the absolute values of its eigenvalues. We find the eigenvalues $\\rho$ of $C$ by solving the characteristic equation $\\det(C - \\rho I) = 0$.\n    $$\n    \\det\\begin{pmatrix}\\frac{1}{2} - \\rho & \\frac{1}{4} \\\\ \\frac{1}{4} & \\frac{1}{2} - \\rho\\end{pmatrix} = 0\n    $$\n    $$\n    \\left(\\frac{1}{2} - \\rho\\right)^2 - \\left(\\frac{1}{4}\\right)^2 = 0\n    $$\n    $$\n    \\left(\\frac{1}{2} - \\rho\\right)^2 = \\frac{1}{16}\n    $$\n    Taking the square root of both sides:\n    $$\n    \\frac{1}{2} - \\rho = \\pm\\frac{1}{4}\n    $$\n    This gives two eigenvalues:\n    $$\n    \\rho = \\frac{1}{2} \\mp \\frac{1}{4}\n    $$\n    The eigenvalues are $\\rho_{1} = \\frac{1}{2} + \\frac{1}{4} = \\frac{3}{4}$ and $\\rho_{2} = \\frac{1}{2} - \\frac{1}{4} = \\frac{1}{4}$.\n    Since both are positive, they are also the singular values of $C$. The largest canonical correlation is the largest singular value, $\\rho_{1}$.\n\nTherefore, the largest canonical correlation is $\\frac{3}{4}$. The other steps requested, finding the canonical directions $a$ and $b$, are not required for the final answer but are derived from the singular vectors corresponding to this singular value.",
            "answer": "$$\\boxed{\\frac{3}{4}}$$"
        },
        {
            "introduction": "Moving from theory to application, this final practice is a capstone coding exercise that simulates a complete multi-omics data integration workflow. You will implement a full pipeline that includes data standardization, regularized CCA to handle collinearity, and the crucial step of interpreting the results through structure coefficients. This exercise culminates in aggregating variable-level importance scores into pathway-level summaries, bridging the gap between a statistical model and actionable biological insight, a common and vital task in modern systems biomedicine .",
            "id": "4322575",
            "problem": "You are given a two-view multivariate setting typical of systems biomedicine multi-omics integration. One view contains gene-level measurements and the other view contains metabolite-level measurements. The relationship between the two views is to be summarized via Canonical Correlation Analysis (CCA), and variable-level importance scores are to be computed from the CCA solution and aggregated to pathway-level scores.\n\nYour task is to write a complete, runnable program that, for each test case in the suite below, computes pathway-level scores from CCA-derived variable importance and returns the index (zero-based) of the highest-scoring pathway. The program must follow the algorithmic specification below and produce a single-line output containing the indices as a comma-separated list enclosed in square brackets.\n\nFundamental base and definitions to use:\n- For data matrices $X \\in \\mathbb{R}^{n \\times p_x}$ and $Y \\in \\mathbb{R}^{n \\times p_y}$ with columns standardized to zero mean and unit variance, the sample covariance matrices are defined as $S_{xx} = \\frac{1}{n} X^\\top X$, $S_{yy} = \\frac{1}{n} Y^\\top Y$, and the cross-covariance $S_{xy} = \\frac{1}{n} X^\\top Y$. Canonical Correlation Analysis (CCA) seeks weight matrices $A \\in \\mathbb{R}^{p_x \\times k}$ and $B \\in \\mathbb{R}^{p_y \\times k}$ such that the canonical variates $U = X A$ and $V = Y B$ maximize the pairwise correlations between corresponding columns of $U$ and $V$, subject to orthogonality constraints.\n- To compute CCA in a numerically stable way, use whitening and singular value decomposition (SVD): let $\\tilde{S}_{xx} = S_{xx} + \\lambda I_{p_x}$ and $\\tilde{S}_{yy} = S_{yy} + \\lambda I_{p_y}$ for a given ridge parameter $\\lambda \\ge 0$. Compute inverse square roots $\\tilde{S}_{xx}^{-1/2}$ and $\\tilde{S}_{yy}^{-1/2}$ via eigen-decomposition, form $K = \\tilde{S}_{xx}^{-1/2} S_{xy} \\tilde{S}_{yy}^{-1/2}$, and compute the SVD $K = U \\Sigma V^\\top$. The canonical weight matrices are then $A = \\tilde{S}_{xx}^{-1/2} U$ and $B = \\tilde{S}_{yy}^{-1/2} V$. The canonical correlations are given by the singular values (the diagonal of $\\Sigma$).\n- For variable importance, use structure coefficients: for the $r$-th canonical variate pair, define $s_{jr} = \\operatorname{corr}(X_{\\cdot j}, U_{\\cdot r})$ for variable $j$ in the $X$ view and $t_{\\ell r} = \\operatorname{corr}(Y_{\\cdot \\ell}, V_{\\cdot r})$ for variable $\\ell$ in the $Y$ view. Aggregate across the first $k$ canonical pairs with weights equal to squared canonical correlations: if $\\rho_r$ is the $r$-th canonical correlation, define $w_r = \\rho_r^2$, and set the per-variable importance as $I^X_j = \\sum_{r=1}^k w_r \\, s_{jr}^2$ and $I^Y_\\ell = \\sum_{r=1}^k w_r \\, t_{\\ell r}^2$.\n- Normalize each view’s importances to sum to one: $\\tilde{I}^X_j = I^X_j / \\sum_{j=1}^{p_x} I^X_j$ and $\\tilde{I}^Y_\\ell = I^Y_\\ell / \\sum_{\\ell=1}^{p_y} I^Y_\\ell$, with the convention that if a sum is zero the normalized vector is the zero vector.\n- Given pathway mappings $P_X : \\{1,\\dots,p_x\\} \\to \\{0,\\dots,m-1\\}$ and $P_Y : \\{1,\\dots,p_y\\} \\to \\{0,\\dots,m-1\\}$, define the unnormalized pathway score for pathway $p \\in \\{0,\\dots,m-1\\}$ as $S_p = \\sum_{j: P_X(j)=p} \\tilde{I}^X_j + \\sum_{\\ell: P_Y(\\ell)=p} \\tilde{I}^Y_\\ell$. Normalize pathway scores across pathways to sum to one: $\\hat{S}_p = S_p / \\sum_{q=0}^{m-1} S_q$ (if the denominator is zero, take all $\\hat{S}_p = 0$). Finally, return the index $p$ with the largest $\\hat{S}_p$ (breaking ties by choosing the smallest index).\n\nImplementation requirements:\n1. Standardize the columns of $X$ and $Y$ to zero mean and unit variance before computing $S_{xx}$, $S_{yy}$, and $S_{xy}$. Use the sample mean and the population standard deviation (divide by $n$ inside variance calculations for consistency).\n2. Use the SVD-based CCA procedure with ridge parameter $\\lambda$ as described above.\n3. Compute structure coefficients and variable importance as specified, aggregate to pathways, and output the index of the pathway with the highest normalized score.\n\nAngle units are not applicable. No physical units are involved.\n\nTest suite:\nFor each test case below, construct $X$ and $Y$ exactly as specified. All numbers are real scalars. Every number must be treated as written.\n\n- Test case 1 (happy path, two correlated latent factors):\n  - Sample size $n = 8$, number of variables $p_x = 4$, $p_y = 3$, number of canonical pairs $k = 2$, ridge parameter $\\lambda = 0.001$.\n  - Latent factors and noise:\n    - $z_1 = [-1.0,\\,-0.5,\\,0.0,\\,0.5,\\,1.0,\\,-0.8,\\,0.3,\\,0.9]$,\n      $z_2 = [0.7,\\,-1.2,\\,0.5,\\,-0.3,\\,1.1,\\,-0.4,\\,0.2,\\,-0.6]$.\n    - $e_{X0} = [0.02,\\,-0.01,\\,0.00,\\,0.01,\\,-0.02,\\,0.03,\\,-0.01,\\,0.00]$,\n      $e_{X1} = [0.01,\\,-0.02,\\,0.02,\\,-0.01,\\,0.00,\\,0.01,\\,-0.02,\\,0.03]$,\n      $e_{X2} = [0.05,\\,-0.04,\\,0.02,\\,-0.01,\\,0.00,\\,0.01,\\,-0.02,\\,0.03]$,\n      $e_{X3} = [0.01,\\,0.00,\\,-0.01,\\,0.02,\\,-0.02,\\,0.01,\\,0.00,\\,-0.01]$.\n    - $e_{Y0} = [0.01,\\,-0.02,\\,0.00,\\,0.02,\\,-0.01,\\,0.00,\\,0.01,\\,-0.01]$,\n      $e_{Y1} = [0.02,\\,-0.01,\\,0.01,\\,-0.02,\\,0.00,\\,0.02,\\,-0.01,\\,0.00]$,\n      $e_{Y2} = [0.03,\\,-0.02,\\,0.01,\\,0.00,\\,-0.01,\\,0.02,\\,-0.02,\\,0.01]$.\n  - Construct $X$ (columns $X_0,\\dots,X_3$) and $Y$ (columns $Y_0,\\dots,Y_2$) by:\n    - $X_0 = z_1 + 0.1 \\cdot e_{X0}$,\n      $X_1 = -0.5 \\cdot z_2 + 0.1 \\cdot e_{X1}$,\n      $X_2 = 0.1 \\cdot e_{X2}$,\n      $X_3 = 0.5 \\cdot z_1 + 0.5 \\cdot z_2 + 0.1 \\cdot e_{X3}$.\n    - $Y_0 = 0.9 \\cdot z_1 + 0.1 \\cdot e_{Y0}$,\n      $Y_1 = -0.4 \\cdot z_2 + 0.1 \\cdot e_{Y1}$,\n      $Y_2 = 0.1 \\cdot e_{Y2}$.\n  - Pathway mappings:\n    - $P_X = [0,\\,1,\\,2,\\,2]$ mapping $X$ variables $0,1,2,3$ to pathways $0,1,2,2$.\n    - $P_Y = [0,\\,1,\\,2]$ mapping $Y$ variables $0,1,2$ to pathways $0,1,2$.\n  - There are $m = 3$ pathways indexed $0,1,2$.\n\n- Test case 2 (single dominant correlation, near-perfect pair):\n  - $n = 6$, $p_x = 2$, $p_y = 3$, $k = 1$, $\\lambda = 0.000001$.\n  - Define:\n    - $X_0 = [-1.0,\\,-0.6,\\,-0.2,\\,0.2,\\,0.6,\\,1.0]$,\n      $X_1 = [0.05,\\,-0.03,\\,0.02,\\,-0.01,\\,0.00,\\,-0.02]$.\n    - $Y_0 = X_0 + 0.01 \\cdot [0.1,\\,-0.2,\\,0.0,\\,0.2,\\,-0.1,\\,0.0]$,\n      $Y_1 = [0.20,\\,-0.10,\\,0.00,\\,0.10,\\,-0.05,\\,0.00]$,\n      $Y_2 = [0.03,\\,-0.02,\\,0.01,\\,0.00,\\,-0.01,\\,0.02]$.\n  - Pathway mappings:\n    - $P_X = [0,\\,1]$,\n    - $P_Y = [0,\\,1,\\,1]$.\n  - There are $m = 2$ pathways indexed $0,1$.\n\n- Test case 3 (collinearity requiring ridge regularization):\n  - $n = 6$, $p_x = 3$, $p_y = 1$, $k = 1$, $\\lambda = 0.1$.\n  - Define:\n    - $X_0 = [-1.0,\\,-0.5,\\,0.0,\\,0.5,\\,1.0,\\,1.5]$,\n      $X_1 = 2.0 \\cdot X_0 + 0.01 \\cdot [0.1,\\,-0.2,\\,0.0,\\,0.1,\\,-0.1,\\,0.0]$,\n      $X_2 = [0.05,\\,-0.04,\\,0.03,\\,-0.02,\\,0.01,\\,0.00]$.\n    - $Y_0 = X_0 + 0.05 \\cdot [0.1,\\,-0.1,\\,0.0,\\,0.1,\\,-0.1,\\,0.2]$.\n  - Pathway mappings:\n    - $P_X = [0,\\,1,\\,2]$,\n    - $P_Y = [0]$.\n  - There are $m = 3$ pathways indexed $0,1,2$.\n\nOutput specification:\n- Your program must implement the above procedure for each test case in the suite. For each test case, compute the normalized pathway scores $\\{\\hat{S}_p\\}_{p=0}^{m-1}$ and output the index $p$ with the largest $\\hat{S}_p$ (if there is any tie, choose the smallest index).\n- The final program output must be a single line containing a list of the three indices for the three test cases, formatted exactly as a comma-separated list enclosed in square brackets, for example, `[0,1,2]`.",
            "solution": "The problem requires the implementation of a multi-step analytical pipeline to identify the most significant biological pathway from two sets of multi-omics data, represented by matrices $X$ and $Y$. The pipeline involves data standardization, regularized Canonical Correlation Analysis (CCA), calculation of variable importance via structure coefficients, and aggregation of these importances into pathway-level scores. The solution proceeds by methodically implementing each of these specified steps.\n\n### Step 1: Data Standardization\nLet the input data matrices be $X \\in \\mathbb{R}^{n \\times p_x}$ and $Y \\in \\mathbb{R}^{n \\times p_y}$, where $n$ is the number of samples, and $p_x$ and $p_y$ are the number of variables (e.g., genes and metabolites) in each view. The first step is to standardize each variable (column) to have a mean of $0$ and a population standard deviation of $1$. For a given column vector $v \\in \\mathbb{R}^n$ with sample mean $\\mu = \\frac{1}{n}\\sum_{i=1}^n v_i$ and population standard deviation $\\sigma = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n (v_i - \\mu)^2}$, its standardized version $v_{std}$ is given by $v_{std, i} = (v_i - \\mu)/\\sigma$. This ensures all variables are on a comparable scale, which is a prerequisite for covariance-based methods like CCA. If a variable has zero standard deviation (i.e., it is constant), its standardized form is a vector of zeros.\n\n### Step 2: Regularized Canonical Correlation Analysis (CCA)\nCCA aims to find linear combinations of variables in $X$ and $Y$ that are maximally correlated. These linear combinations form the canonical variates. The problem specifies a numerically stable, SVD-based approach for CCA.\n\n**2a. Covariance Matrices:** After standardization, the sample covariance matrices are computed. For the standardized data matrices $X_{std}$ and $Y_{std}$, these are:\n$$\nS_{xx} = \\frac{1}{n} X_{std}^\\top X_{std} \\quad \\in \\mathbb{R}^{p_x \\times p_x} \\\\\nS_{yy} = \\frac{1}{n} Y_{std}^\\top Y_{std} \\quad \\in \\mathbb{R}^{p_y \\times p_y} \\\\\nS_{xy} = \\frac{1}{n} X_{std}^\\top Y_{std} \\quad \\in \\mathbb{R}^{p_x \\times p_y}\n$$\n\n**2b. Ridge Regularization:** To handle potential collinearity and ensure the covariance matrices are invertible, a small ridge penalty $\\lambda \\ge 0$ is added to the diagonal of $S_{xx}$ and $S_{yy}$:\n$$\n\\tilde{S}_{xx} = S_{xx} + \\lambda I_{p_x} \\\\\n\\tilde{S}_{yy} = S_{yy} + \\lambda I_{p_y}\n$$\nThis makes the matrices strictly positive definite.\n\n**2c. Matrix Whitening via Inverse Square Root:** The data is \"whitened\" by transforming it using the inverse square root of the regularized covariance matrices. For a symmetric positive definite matrix $M$, its inverse square root $M^{-1/2}$ can be computed via eigendecomposition. If $M = Q \\Lambda Q^\\top$, where $Q$ is the matrix of eigenvectors and $\\Lambda$ is the diagonal matrix of eigenvalues $\\lambda_i$, then $M^{-1/2} = Q \\Lambda^{-1/2} Q^\\top$, where $\\Lambda^{-1/2}$ is a diagonal matrix with entries $1/\\sqrt{\\lambda_i}$. This is applied to compute $\\tilde{S}_{xx}^{-1/2}$ and $\\tilde{S}_{yy}^{-1/2}$.\n\n**2d. SVD of the Kernel Matrix:** A kernel matrix $K$ is formed:\n$$\nK = \\tilde{S}_{xx}^{-1/2} S_{xy} \\tilde{S}_{yy}^{-1/2}\n$$\nThe Singular Value Decomposition (SVD) of $K$ is then computed: $K = U_{svd} \\Sigma V_{svd}^\\top$. The singular values on the diagonal of $\\Sigma$ are the canonical correlations, $\\rho_r$. The canonical weight matrices, $A \\in \\mathbb{R}^{p_x \\times k}$ and $B \\in \\mathbb{R}^{p_y \\times k}$, which define the linear combinations, are recovered using the singular vectors:\n$$\nA = \\tilde{S}_{xx}^{-1/2} U_{svd} \\\\\nB = \\tilde{S}_{yy}^{-1/2} V_{svd}\n$$\n\n### Step 3: Variable Importance Calculation\nThe importance of each original variable is assessed using structure coefficients.\n\n**3a. Canonical Variates and Structure Coefficients:** The canonical variates for the top $k$ components are $U = X_{std} A_{\\cdot, 1:k}$ and $V = Y_{std} B_{\\cdot, 1:k}$. The structure coefficient for the $j$-th variable in view $X$ and the $r$-th canonical variate pair is defined as their Pearson correlation: $s_{jr} = \\operatorname{corr}(X_{std, \\cdot j}, U_{\\cdot r})$. Similarly, $t_{\\ell r} = \\operatorname{corr}(Y_{std, \\cdot \\ell}, V_{\\cdot r})$ for view $Y$. Since the columns of $X_{std}$ and $Y_{std}$ have unit variance and zero mean, and the canonical variates $U$ and $V$ also have zero mean, the correlation formula simplifies to $\\operatorname{corr}(X_{std, \\cdot j}, U_{\\cdot r}) = \\operatorname{cov}(X_{std, \\cdot j}, U_{\\cdot r}) / \\sigma_{U_{\\cdot r}}$.\n\n**3b. Aggregated and Normalized Importance:** The importance of each variable is aggregated across the top $k$ canonical components, weighted by the squared canonical correlations, $w_r = \\rho_r^2$.\n$$\nI^X_j = \\sum_{r=1}^k w_r \\, s_{jr}^2 \\quad \\text{and} \\quad I^Y_\\ell = \\sum_{r=1}^k w_r \\, t_{\\ell r}^2\n$$\nThese raw importance scores are then normalized within each view to sum to one, yielding $\\tilde{I}^X_j$ and $\\tilde{I}^Y_\\ell$. If a view's total importance is zero, all its normalized scores are zero.\n\n### Step 4: Pathway Score Aggregation\nThe variable-level importances are mapped to pathway-level scores using predefined sets.\n\n**4a. Unnormalized Pathway Scores:** Given mappings $P_X$ and $P_Y$ from variables to one of $m$ pathways, the score for pathway $p$ is the sum of the normalized importances of all variables belonging to it:\n$$\nS_p = \\sum_{j: P_X(j)=p} \\tilde{I}^X_j + \\sum_{\\ell: P_Y(\\ell)=p} \\tilde{I}^Y_\\ell\n$$\n\n**4b. Normalized Pathway Scores:** These scores are then normalized to sum to one across all pathways, creating a final probability distribution over the pathways:\n$$\n\\hat{S}_p = S_p / \\sum_{q=0}^{m-1} S_q\n$$\n\n### Step 5: Final Selection\nThe final result is the zero-based index of the pathway with the highest normalized score, $\\hat{S}_p$. Ties are resolved by selecting the smallest index, a behavior inherent in standard `argmax` functions. This procedure is applied independently to each test case provided.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import eigh, svd\n\ndef solve():\n    \"\"\"\n    Main solver function that encapsulates the entire process for all test cases.\n    It defines helper functions, test data, and runs the analysis, printing the final result.\n    \"\"\"\n\n    def standardize(M):\n        \"\"\"Standardizes columns of a matrix to have zero mean and unit population variance.\"\"\"\n        mean = np.mean(M, axis=0)\n        std = np.std(M, axis=0, ddof=0)\n        \n        # Create a safe denominator, replacing 0 with 1 to avoid division by zero.\n        std_safe = np.where(std == 0, 1.0, std)\n        M_std = (M - mean) / std_safe\n        \n        # Where std was zero, the original column was constant.\n        # The standardized column should be all zeros.\n        M_std[:, std == 0] = 0.0\n        \n        return M_std\n\n    def matrix_inv_sqrt(M):\n        \"\"\"\n        Computes the inverse square root of a symmetric positive definite matrix\n        using eigendecomposition.\n        \"\"\"\n        evals, evecs = eigh(M)\n        # eigh ensures eigenvalues are real. For S_tilde, they are must be > 0.\n        evals_inv_sqrt = np.diag(1.0 / np.sqrt(evals))\n        return evecs @ evals_inv_sqrt @ evecs.T\n\n    def cca_pathway_analysis(X, Y, k, lmbda, P_X, P_Y, m):\n        \"\"\"\n        Performs the full CCA-based pathway analysis pipeline for a given dataset.\n        \"\"\"\n        n, px = X.shape\n        py = Y.shape[1]\n\n        # 1. Standardize data matrices\n        X_std = standardize(X)\n        Y_std = standardize(Y)\n\n        # 2. Compute sample covariance matrices\n        S_xx = (1/n) * X_std.T @ X_std\n        S_yy = (1/n) * Y_std.T @ Y_std\n        S_xy = (1/n) * X_std.T @ Y_std\n\n        # 3. Perform regularized CCA\n        S_xx_reg = S_xx + lmbda * np.eye(px)\n        S_yy_reg = S_yy + lmbda * np.eye(py)\n\n        S_xx_inv_sqrt = matrix_inv_sqrt(S_xx_reg)\n        S_yy_inv_sqrt = matrix_inv_sqrt(S_yy_reg)\n\n        K = S_xx_inv_sqrt @ S_xy @ S_yy_inv_sqrt\n\n        U_svd, s, Vh_svd = svd(K, full_matrices=False)\n        V_svd = Vh_svd.T\n\n        # Recover canonical weights\n        A = S_xx_inv_sqrt @ U_svd\n        B = S_yy_inv_sqrt @ V_svd\n        \n        A_k = A[:, :k]\n        B_k = B[:, :k]\n        \n        # Compute canonical variates\n        U_var = X_std @ A_k\n        V_var = Y_std @ B_k\n\n        # 4. Compute structure coefficients\n        # corr(X_std, U_var) = cov(X_std, U_var) / std(U_var) since std(X_std)=1\n        cov_XU = (1/n) * X_std.T @ U_var\n        std_U = np.std(U_var, axis=0, ddof=0)\n        struct_X = np.divide(cov_XU, std_U, out=np.zeros_like(cov_XU), where=std_U!=0)\n        \n        cov_YV = (1/n) * Y_std.T @ V_var\n        std_V = np.std(V_var, axis=0, ddof=0)\n        struct_Y = np.divide(cov_YV, std_V, out=np.zeros_like(cov_YV), where=std_V!=0)\n\n        # 5. Compute and normalize variable importance\n        rho = s[:k]\n        weights = rho**2\n\n        I_X = np.sum(struct_X**2 * weights, axis=1)\n        I_Y = np.sum(struct_Y**2 * weights, axis=1)\n\n        sum_I_X = np.sum(I_X)\n        norm_I_X = I_X / sum_I_X if sum_I_X > 0 else np.zeros_like(I_X)\n\n        sum_I_Y = np.sum(I_Y)\n        norm_I_Y = I_Y / sum_I_Y if sum_I_Y > 0 else np.zeros_like(I_Y)\n        \n        # 6. Aggregate to pathway scores\n        pathway_scores = np.zeros(m)\n        np.add.at(pathway_scores, P_X, norm_I_X)\n        np.add.at(pathway_scores, P_Y, norm_I_Y)\n\n        sum_scores = np.sum(pathway_scores)\n        norm_pathway_scores = pathway_scores / sum_scores if sum_scores > 0 else np.zeros_like(pathway_scores)\n\n        # 7. Find index of the highest-scoring pathway\n        return np.argmax(norm_pathway_scores)\n\n    # ----- Test Suite Definition -----\n    test_cases = [\n        {\n            \"n\": 8, \"px\": 4, \"py\": 3, \"k\": 2, \"lambda\": 0.001,\n            \"data_gen_params\": {\n                \"z1\": np.array([-1.0,-0.5,0.0,0.5,1.0,-0.8,0.3,0.9]),\n                \"z2\": np.array([0.7,-1.2,0.5,-0.3,1.1,-0.4,0.2,-0.6]),\n                \"eX\": np.array([\n                    [0.02,-0.01,0.00,0.01,-0.02,0.03,-0.01,0.00],\n                    [0.01,-0.02,0.02,-0.01,0.00,0.01,-0.02,0.03],\n                    [0.05,-0.04,0.02,-0.01,0.00,0.01,-0.02,0.03],\n                    [0.01,0.00,-0.01,0.02,-0.02,0.01,0.00,-0.01]\n                ]).T,\n                \"eY\": np.array([\n                    [0.01,-0.02,0.00,0.02,-0.01,0.00,0.01,-0.01],\n                    [0.02,-0.01,0.01,-0.02,0.00,0.02,-0.01,0.00],\n                    [0.03,-0.02,0.01,0.00,-0.01,0.02,-0.02,0.01]\n                ]).T\n            },\n            \"PX\": [0, 1, 2, 2], \"PY\": [0, 1, 2], \"m\": 3\n        },\n        {\n            \"n\": 6, \"px\": 2, \"py\": 3, \"k\": 1, \"lambda\": 0.000001,\n            \"X\": np.array([\n                [-1.0, -0.6, -0.2,  0.2, 0.6, 1.0],\n                [0.05, -0.03, 0.02, -0.01, 0.00, -0.02]\n            ]).T,\n            \"Y\": np.array([\n                np.array([-1.0, -0.6, -0.2,  0.2, 0.6, 1.0]) + 0.01 * np.array([0.1, -0.2, 0.0, 0.2, -0.1, 0.0]),\n                [0.20, -0.10, 0.00,  0.10, -0.05, 0.00],\n                [0.03, -0.02, 0.01,  0.00, -0.01, 0.02]\n            ]).T,\n            \"PX\": [0, 1], \"PY\": [0, 1, 1], \"m\": 2\n        },\n        {\n            \"n\": 6, \"px\": 3, \"py\": 1, \"k\": 1, \"lambda\": 0.1,\n            \"data_gen_params\": {\n                \"X0\": np.array([-1.0, -0.5, 0.0, 0.5, 1.0, 1.5]),\n                \"X1_noise\": 0.01 * np.array([0.1, -0.2, 0.0, 0.1, -0.1, 0.0]),\n                \"X2\": np.array([0.05, -0.04, 0.03, -0.02, 0.01, 0.00]),\n                \"Y0_noise\": 0.05 * np.array([0.1, -0.1, 0.0, 0.1, -0.1, 0.2]),\n            },\n            \"PX\": [0, 1, 2], \"PY\": [0], \"m\": 3\n        }\n    ]\n\n    results = []\n    for i, case in enumerate(test_cases):\n        if i == 0:\n            params = case[\"data_gen_params\"]\n            z1, z2, eX, eY = params[\"z1\"], params[\"z2\"], params[\"eX\"], params[\"eY\"]\n            X = np.zeros((case[\"n\"], case[\"px\"]))\n            Y = np.zeros((case[\"n\"], case[\"py\"]))\n            X[:, 0] = z1 + 0.1 * eX[:, 0]\n            X[:, 1] = -0.5 * z2 + 0.1 * eX[:, 1]\n            X[:, 2] = 0.1 * eX[:, 2]\n            X[:, 3] = 0.5 * z1 + 0.5 * z2 + 0.1 * eX[:, 3]\n            Y[:, 0] = 0.9 * z1 + 0.1 * eY[:, 0]\n            Y[:, 1] = -0.4 * z2 + 0.1 * eY[:, 1]\n            Y[:, 2] = 0.1 * eY[:, 2]\n        elif i == 1:\n            X, Y = case[\"X\"], case[\"Y\"]\n        elif i == 2:\n            params = case[\"data_gen_params\"]\n            X = np.zeros((case[\"n\"], case[\"px\"]))\n            Y = np.zeros((case[\"n\"], case[\"py\"]))\n            X[:, 0] = params[\"X0\"]\n            X[:, 1] = 2.0 * params[\"X0\"] + params[\"X1_noise\"]\n            X[:, 2] = params[\"X2\"]\n            Y[:, 0] = params[\"X0\"] + params[\"Y0_noise\"]\n\n        result = cca_pathway_analysis(X, Y, case[\"k\"], case[\"lambda\"], case[\"PX\"], case[\"PY\"], case[\"m\"])\n        results.append(result)\n        \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}