{
    "hands_on_practices": [
        {
            "introduction": "A phylogenetic profile is often represented as a binary vector indicating the presence or absence of a gene across species. A foundational task is to quantify the co-occurrence between two such profiles. This practice solidifies the mathematical relationship between the familiar Pearson correlation coefficient and the phi coefficient, demonstrating their algebraic equivalence for binary data and providing a formal basis for their use in functional genomics .",
            "id": "4375029",
            "problem": "In systems biomedicine, phylogenetic profiling represents the presence or absence of genomic features (for example, orthologous gene families) across a panel of species as binary vectors. Consider two such profiles, $x,y \\in \\{0,1\\}^{N}$, for $N$ species. Define the empirical $2 \\times 2$ contingency counts by\n$$\nn_{11}=\\sum_{i=1}^{N}\\mathbf{1}\\{x_{i}=1,y_{i}=1\\},\\quad\nn_{10}=\\sum_{i=1}^{N}\\mathbf{1}\\{x_{i}=1,y_{i}=0\\},\\quad\nn_{01}=\\sum_{i=1}^{N}\\mathbf{1}\\{x_{i}=0,y_{i}=1\\},\\quad\nn_{00}=\\sum_{i=1}^{N}\\mathbf{1}\\{x_{i}=0,y_{i}=0\\},\n$$\nand note that $N=n_{11}+n_{10}+n_{01}+n_{00}$. Let $n_{1.}=n_{11}+n_{10}$, $n_{0.}=n_{01}+n_{00}$, $n_{.1}=n_{11}+n_{01}$, and $n_{.0}=n_{10}+n_{00}$ denote the marginal counts.\n\nStarting from first principles, use the definition of the Pearson correlation coefficient (PCC) for empirical data,\n$$\nr_{xy}=\\frac{\\sum_{i=1}^{N}\\left(x_{i}-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right)}{\\sqrt{\\sum_{i=1}^{N}\\left(x_{i}-\\bar{x}\\right)^{2}}\\sqrt{\\sum_{i=1}^{N}\\left(y_{i}-\\bar{y}\\right)^{2}}},\n$$\nwhere $\\bar{x}=\\frac{1}{N}\\sum_{i=1}^{N}x_{i}$ and $\\bar{y}=\\frac{1}{N}\\sum_{i=1}^{N}y_{i}$, and the core definition of the phi coefficient as the standardized measure of association for $2 \\times 2$ contingency tables, to derive a single closed-form analytic expression for $r_{xy}$ entirely in terms of the four joint counts $n_{11}$, $n_{10}$, $n_{01}$, and $n_{00}$. Then, explain in words, without computing any additional formulas, the condition under which the Pearson correlation and the phi coefficient yield identical rankings of gene-pair associations in phylogenetic profiling.\n\nExpress the final answer as a single analytic expression in terms of $n_{11}$, $n_{10}$, $n_{01}$, and $n_{00}$ only. No units and no rounding are required.",
            "solution": "The user wants me to first validate and then solve a problem concerning the relationship between the Pearson correlation coefficient (PCC) and the phi coefficient for binary phylogenetic profiles.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n*   **Data:** Two binary vectors (phylogenetic profiles) $x, y \\in \\{0,1\\}^{N}$ for $N$ species.\n*   **Contingency Counts:**\n    *   $n_{11} = \\sum_{i=1}^{N}\\mathbf{1}\\{x_{i}=1,y_{i}=1\\}$\n    *   $n_{10} = \\sum_{i=1}^{N}\\mathbf{1}\\{x_{i}=1,y_{i}=0\\}$\n    *   $n_{01} = \\sum_{i=1}^{N}\\mathbf{1}\\{x_{i}=0,y_{i}=1\\}$\n    *   $n_{00} = \\sum_{i=1}^{N}\\mathbf{1}\\{x_{i}=0,y_{i}=0\\}$\n*   **Total Count:** $N=n_{11}+n_{10}+n_{01}+n_{00}$.\n*   **Marginal Counts:**\n    *   $n_{1.} = n_{11}+n_{10}$\n    *   $n_{0.} = n_{01}+n_{00}$\n    *   $n_{.1} = n_{11}+n_{01}$\n    *   $n_{.0} = n_{10}+n_{00}$\n*   **Pearson Correlation Coefficient (PCC) Definition:**\n    $$\n    r_{xy}=\\frac{\\sum_{i=1}^{N}\\left(x_{i}-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right)}{\\sqrt{\\sum_{i=1}^{N}\\left(x_{i}-\\bar{x}\\right)^{2}}\\sqrt{\\sum_{i=1}^{N}\\left(y_{i}-\\bar{y}\\right)^{2}}}\n    $$\n*   **Mean Definitions:** $\\bar{x}=\\frac{1}{N}\\sum_{i=1}^{N}x_{i}$ and $\\bar{y}=\\frac{1}{N}\\sum_{i=1}^{N}y_{i}$.\n*   **Task:**\n    1.  Derive a closed-form expression for $r_{xy}$ in terms of $n_{11}$, $n_{10}$, $n_{01}$, and $n_{00}$.\n    2.  Explain the condition under which PCC and the phi coefficient yield identical rankings for gene-pair associations. The problem statement refers to the phi coefficient via its definition as \"the standardized measure of association for $2 \\times 2$ contingency tables.\"\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem is scientifically and mathematically sound. It asks for a standard derivation in statistics, specifically the proof that the Pearson correlation coefficient for binary variables is equivalent to the phi coefficient.\n\n*   **Scientific Grounding:** The problem is firmly grounded in biostatistics and bioinformatics. Phylogenetic profiling and the use of contingency tables to measure association between gene presences/absences are standard techniques. The Pearson correlation and phi coefficient are fundamental statistical measures.\n*   **Well-Posed:** The problem is well-posed. The definitions are precise, and the objective is clear: to perform an algebraic derivation. The result is a unique, exact analytical expression.\n*   **Objective:** The language is formal, precise, and free of any subjective or ambiguous terminology.\n*   **Flaw Checklist:**\n    1.  **Scientific/Factual Unsoundness:** None. The premises are correct.\n    2.  **Non-Formalizable/Irrelevant:** None. The problem is formal and directly relevant to the specified field.\n    3.  **Incomplete/Contradictory:** None. While the formula for the phi coefficient is not explicitly given, the problem directs the user to derive the formula for $r_{xy}$ from first principles. The context implies that this derived formula will be that of the phi coefficient. This is a standard result, and all information needed for the derivation is provided.\n    4.  **Unrealistic/Infeasible:** None. The context is a standard application in bioinformatics.\n    5.  **Ill-Posed/Poorly Structured:** None. A unique solution exists.\n    6.  **Pseudo-Profound/Trivial/Tautological:** None. The derivation requires multiple non-trivial algebraic steps.\n    7.  **Outside Scientific Verifiability:** None. The derivation is a mathematical proof.\n\n**Step 3: Verdict and Action**\n\nThe problem is **valid**. I will proceed with the derivation and explanation.\n\n### Solution\n\nThe objective is to derive an expression for the Pearson correlation coefficient, $r_{xy}$, for binary data entirely in terms of the four joint contingency counts $n_{11}$, $n_{10}$, $n_{01}$, and $n_{00}$. We will start from the provided definition of $r_{xy}$ and systematically express each of its components using the given counts.\n\nThe Pearson correlation coefficient is defined as:\n$$\nr_{xy}=\\frac{\\text{Cov}(x, y)}{\\sigma_x \\sigma_y} = \\frac{\\sum_{i=1}^{N}\\left(x_{i}-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right)}{\\sqrt{\\sum_{i=1}^{N}\\left(x_{i}-\\bar{x}\\right)^{2}}\\sqrt{\\sum_{i=1}^{N}\\left(y_{i}-\\bar{y}\\right)^{2}}}\n$$\n\nFirst, we express the sample means, $\\bar{x}$ and $\\bar{y}$, in terms of the counts.\nThe sum $\\sum_{i=1}^{N}x_{i}$ counts the number of species where gene $x$ is present (i.e., $x_i=1$). This corresponds to the cases where ($x_i=1, y_i=1$) or ($x_i=1, y_i=0$). Therefore, $\\sum_{i=1}^{N}x_{i} = n_{11} + n_{10} = n_{1.}$. The mean $\\bar{x}$ is:\n$$\n\\bar{x} = \\frac{1}{N}\\sum_{i=1}^{N}x_{i} = \\frac{n_{1.}}{N} = \\frac{n_{11}+n_{10}}{N}\n$$\nBy symmetry, the mean $\\bar{y}$ is:\n$$\n\\bar{y} = \\frac{1}{N}\\sum_{i=1}^{N}y_{i} = \\frac{n_{.1}}{N} = \\frac{n_{11}+n_{01}}{N}\n$$\n\nNext, we evaluate the terms in the denominator, which are related to the variances.\nFor the term $\\sum_{i=1}^{N}\\left(x_{i}-\\bar{x}\\right)^{2}$, we use the identity $\\sum (z - \\bar{z})^2 = \\sum z^2 - N\\bar{z}^2$. Since the data are binary, $x_i \\in \\{0,1\\}$, we have the property that $x_i^2 = x_i$. Thus, $\\sum_{i=1}^{N}x_i^2 = \\sum_{i=1}^{N}x_i = n_{1.}$.\nThe term becomes:\n$$\n\\sum_{i=1}^{N}\\left(x_{i}-\\bar{x}\\right)^{2} = \\sum_{i=1}^{N}x_{i}^2 - N\\bar{x}^2 = n_{1.} - N\\left(\\frac{n_{1.}}{N}\\right)^2 = n_{1.} - \\frac{n_{1.}^2}{N} = \\frac{Nn_{1.} - n_{1.}^2}{N} = \\frac{n_{1.}(N-n_{1.})}{N}\n$$\nSince $N = n_{1.} + n_{0.}$, we have $N - n_{1.} = n_{0.}$. So, we get:\n$$\n\\sum_{i=1}^{N}\\left(x_{i}-\\bar{x}\\right)^{2} = \\frac{n_{1.}n_{0.}}{N}\n$$\nBy symmetry for the variable $y$:\n$$\n\\sum_{i=1}^{N}\\left(y_{i}-\\bar{y}\\right)^{2} = \\frac{n_{.1}n_{.0}}{N}\n$$\n\nNow, we evaluate the numerator, which is related to the covariance.\nWe use the identity $\\sum (x_i - \\bar{x})(y_i - \\bar{y}) = \\sum x_iy_i - N\\bar{x}\\bar{y}$.\nThe product $x_i y_i$ is equal to $1$ if and only if both $x_i=1$ and $y_i=1$. In all other cases, the product is $0$. Therefore, the sum $\\sum_{i=1}^{N}x_i y_i$ simply counts the number of species for which both genes are present, which is by definition $n_{11}$.\nThe numerator term becomes:\n$$\n\\sum_{i=1}^{N}\\left(x_{i}-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right) = n_{11} - N\\left(\\frac{n_{1.}}{N}\\right)\\left(\\frac{n_{.1}}{N}\\right) = n_{11} - \\frac{n_{1.}n_{.1}}{N} = \\frac{Nn_{11} - n_{1.}n_{.1}}{N}\n$$\nTo simplify the term $Nn_{11} - n_{1.}n_{.1}$, we substitute the definitions of $N$, $n_{1.}$, and $n_{.1}$:\n\\begin{align*}\nNn_{11} - n_{1.}n_{.1} &= (n_{11}+n_{10}+n_{01}+n_{00})n_{11} - (n_{11}+n_{10})(n_{11}+n_{01}) \\\\\n&= (n_{11}^2 + n_{10}n_{11} + n_{01}n_{11} + n_{00}n_{11}) - (n_{11}^2 + n_{11}n_{01} + n_{10}n_{11} + n_{10}n_{01}) \\\\\n&= n_{11}^2 + n_{10}n_{11} + n_{01}n_{11} + n_{00}n_{11} - n_{11}^2 - n_{11}n_{01} - n_{10}n_{11} - n_{10}n_{01}\n\\end{align*}\nCanceling terms, we are left with:\n$$\nNn_{11} - n_{1.}n_{.1} = n_{11}n_{00} - n_{10}n_{01}\n$$\nSo the numerator of $r_{xy}$ is $\\frac{n_{11}n_{00} - n_{10}n_{01}}{N}$.\n\nFinally, we assemble the complete expression for $r_{xy}$:\n$$\nr_{xy} = \\frac{\\frac{n_{11}n_{00} - n_{10}n_{01}}{N}}{\\sqrt{\\left(\\frac{n_{1.}n_{0.}}{N}\\right)\\left(\\frac{n_{.1}n_{.0}}{N}\\right)}} = \\frac{\\frac{n_{11}n_{00} - n_{10}n_{01}}{N}}{\\frac{\\sqrt{n_{1.}n_{0.}n_{.1}n_{.0}}}{N}}\n$$\nThe factor of $N$ in the denominator of the numerator and the denominator of the overall expression cancels out, yielding:\n$$\nr_{xy} = \\frac{n_{11}n_{00} - n_{10}n_{01}}{\\sqrt{n_{1.}n_{0.}n_{.1}n_{.0}}}\n$$\nThis expression is the definition of the phi coefficient, $\\phi$. To provide the final answer solely in terms of the four joint counts, we substitute the definitions of the marginal counts ($n_{1.}, n_{0.}, n_{.1}, n_{.0}$):\n$$\nr_{xy} = \\frac{n_{11}n_{00} - n_{10}n_{01}}{\\sqrt{(n_{11}+n_{10})(n_{01}+n_{00})(n_{11}+n_{01})(n_{10}+n_{00})}}\n$$\nThis completes the derivation.\n\nRegarding the second part of the question: \"explain in words, without computing any additional formulas, the condition under which the Pearson correlation and the phi coefficient yield identical rankings of gene-pair associations in phylogenetic profiling.\"\n\nThe derivation above demonstrates that for binary variables, such as the presence/absence vectors in phylogenetic profiling, the Pearson correlation coefficient is algebraically identical to the phi coefficient. They are not two different measures that happen to coincide under certain conditions; rather, the phi coefficient *is* the Pearson correlation coefficient specialized for the case of two binary variables. Consequently, since $r_{xy}$ and $\\phi$ represent the same mathematical quantity in this context, they will always produce identical values for any given gene pair. Therefore, they will necessarily yield identical rankings of associations across any set of gene pairs. The only \"condition\" for this identity to hold is that the data being correlated must be binary, which is a foundational assumption of the problem statement itself (\"phylogenetic profiling represents the presence or absence ... as binary vectors\").",
            "answer": "$$\n\\boxed{\\frac{n_{11}n_{00} - n_{10}n_{01}}{\\sqrt{(n_{11}+n_{10})(n_{01}+n_{00})(n_{11}+n_{01})(n_{10}+n_{00})}}}\n$$"
        },
        {
            "introduction": "In practice, we don't analyze just one gene pair; we perform a genome-wide scan, leading to millions of hypothesis tests. This requires a robust method to correct for multiple comparisons. This exercise challenges you to design a valid statistical procedure to control the False Discovery Rate (FDR) while navigating the complex dependencies that arise from shared phylogenetic history, a critical skill for any large-scale 'omics' study .",
            "id": "4374991",
            "problem": "In phylogenetic profiling for functional annotation, suppose there are $G$ genes and $S$ species. For each gene, one observes a binary presence or absence profile across the $S$ species. To identify candidate functional relationships, one conducts pairwise tests of co-occurrence for all unordered gene pairs, yielding $m = G(G - 1)/2$ hypothesis tests and associated $p$-values. The goal is to control the False Discovery Rate (FDR) at target level $\\alpha$.\n\nStarting from first principles, use core definitions and well-tested facts to reason about what conditions are needed for valid FDR control when applying the Benjamini–Hochberg (BH) procedure. Specifically, consider how dependence among the $p$-values arises because each test reuses the same $S$ species and because presence–absence patterns may be structured by phylogeny, and whether such dependence is compatible with the known sufficient conditions for BH FDR control.\n\nChoose the option that correctly proposes a BH-based procedure to control FDR at level $\\alpha$ and provides a scientifically justified set of assumptions under which the procedure is valid in this phylogenetic profiling context.\n\nA. Use a test for co-occurrence that yields valid null $p$-values by conditioning on margins and respecting phylogenetic structure (for example, a parametric model that includes a phylogenetic covariance for species or a permutation scheme constrained within clades so that exchangeability is preserved under the null). Then apply the step-up Benjamini–Hochberg rule to the ordered $p$-values. Justify validity by invoking the requirement that null $p$-values are super-uniform and that the dependence among $p$-values across true nulls is positive regression dependent on a subset (PRDS), which is satisfied when test statistics are coordinate-wise non-decreasing functions of the underlying presence–absence counts given the margins, so BH controls FDR at level $\\alpha$.\n\nB. Apply the Benjamini–Hochberg rule directly to all $m$ $p$-values computed from unadjusted Fisher’s exact tests on the raw presence–absence tables, ignoring phylogenetic non-independence, and assert that BH controls FDR regardless of dependence among tests.\n\nC. Replace the Benjamini–Hochberg rule with the Benjamini–Yekutieli (BY) procedure, which controls FDR under arbitrary dependence, and claim this is an acceptable way to “use BH” to control FDR in the presence of phylogeny-induced dependence.\n\nD. Apply a weighted Benjamini–Hochberg procedure using weights constructed from the observed per-gene number of presences across the $S$ species, arguing that higher weights for more informative genes increase power while still controlling FDR because the weights are derived from the same data that produced the $p$-values.",
            "solution": "The problem asks for a valid statistical procedure, based on the Benjamini-Hochberg (BH) method, to control the False Discovery Rate (FDR) in the context of phylogenetic profiling. This requires a careful examination of the assumptions underlying the BH procedure and how they relate to the data structure inherent in phylogenetic profiling.\n\nFirst, let's establish the foundational principles.\n\n1.  **False Discovery Rate (FDR):** The FDR is defined as the expected value of the proportion of falsely rejected null hypotheses (Type I errors) among all rejected hypotheses. Let $R$ be the total number of rejected hypotheses and $V$ be the number of true null hypotheses that are incorrectly rejected. The FDR is $E[V/R]$ (with the fraction defined as $0$ if $R=0$). The goal is to ensure $\\text{FDR} \\leq \\alpha$, where $\\alpha$ is the desired significance level.\n\n2.  **Benjamini-Hochberg (BH) Procedure:** This is a step-up procedure for controlling FDR. Given a set of $m$ $p$-values $p_1, p_2, \\dots, p_m$, they are first ordered $p_{(1)} \\leq p_{(2)} \\leq \\dots \\leq p_{(m)}$. The procedure finds the largest index $k$ such that $p_{(k)} \\leq \\frac{k}{m}\\alpha$. All null hypotheses corresponding to $p_{(1)}, \\dots, p_{(k)}$ are then rejected.\n\n3.  **Conditions for BH Validity:** The validity of the BH procedure, i.e., its ability to control FDR at level $\\alpha$, depends on two key properties:\n    *   **Validity of $p$-values:** Under a true null hypothesis, each $p$-value must be stochastically greater than or equal to a uniform random variable on $[0, 1]$. That is, for any true null hypothesis $H_{0i}$, its $p$-value $p_i$ must satisfy $P(p_i \\leq x) \\leq x$ for all $x \\in [0, 1]$. This property is often called \"super-uniformity\". If the $p$-values are exactly uniform under the null, this condition is met.\n    *   **Dependence Structure:**\n        *   If all $m$ $p$-values are independent, the BH procedure controls FDR at $\\text{FDR} \\leq \\frac{m_0}{m}\\alpha \\leq \\alpha$, where $m_0$ is the number of true null hypotheses.\n        *   Crucially, Benjamini and Yekutieli (2001) proved that the BH procedure also controls FDR at the same level under a specific type of positive dependence, known as Positive Regression Dependence on a Subset (PRDS). This condition holds for many realistic scenarios in genomics and other fields where tests are not independent.\n\nNow, we evaluate the phylogenetic profiling problem in light of these principles.\n\nThe setup involves $m = G(G-1)/2$ pairwise tests of co-occurrence between $G$ genes across $S$ species. Two major statistical issues arise:\n\n1.  **Violation of Sample Independence:** The $S$ species are not independent observations. They are related by a phylogeny, which induces statistical dependence in their traits, including the presence or absence of genes. Standard co-occurrence tests, such as the chi-squared test or Fisher's exact test, assume that the observations (species, in this case) are independent. Applying these tests naively to phylogenetically structured data will produce invalid $p$-values. Specifically, the null distribution of these $p$-values will not be uniform, typically being skewed towards smaller values, leading to an inflated rate of false positives and a loss of FDR control. Therefore, any valid procedure **must** begin with a statistical test that properly accounts for the phylogenetic non-independence of the species to generate valid (super-uniform) null $p$-values.\n\n2.  **Dependence Among Hypothesis Tests:** The $m$ hypothesis tests are not independent of each other. For example, the test for gene pair $(g_1, g_2)$ and the test for $(g_1, g_3)$ both reuse the presence-absence profile of gene $g_1$. This shared data structure induces dependencies among the resulting $p$-values. Therefore, the simple independence assumption for the BH procedure is violated. For the BH procedure to be valid, this dependence structure must conform to the PRDS condition. The PRDS condition is often satisfied if the test statistics are, for example, non-decreasing functions of underlying independent random variables. In our context, if the test statistics are properly constructed (e.g., from a phylogenetically-aware model), they are often based on counts or correlations that exhibit this kind of monotonic behavior, making the PRDS assumption plausible.\n\nWith this framework, we can now evaluate each option.\n\n**A. Use a test for co-occurrence that yields valid null p-values by conditioning on margins and respecting phylogenetic structure (for example, a parametric model that includes a phylogenetic covariance for species or a permutation scheme constrained within clades so that exchangeability is preserved under the null). Then apply the step-up Benjamini–Hochberg rule to the ordered p-values. Justify validity by invoking the requirement that null p-values are super-uniform and that the dependence among p-values across true nulls is positive regression dependent on a subset (PRDS), which is satisfied when test statistics are coordinate-wise non-decreasing functions of the underlying presence–absence counts given the margins, so BH controls FDR at level $\\alpha$.**\n\nThis option correctly identifies and addresses both critical issues.\n1.  It mandates a statistical test that \"respecting phylogenetic structure\" to generate \"valid null $p$-values\". This is the essential first step. The examples given (phylogenetic generalized least squares, clade-based permutations) are standard, correct methods for this.\n2.  It correctly states that the BH procedure can then be applied.\n3.  It provides the correct theoretical justification: the $p$-values must be super-uniform, and the dependence structure must satisfy the PRDS condition for the BH procedure to control FDR. The claim that this condition is often met by a wide class of test statistics is also accurate.\nThis option presents a complete and conceptually sound workflow.\n\n**Verdict: Correct**\n\n**B. Apply the Benjamini–Hochberg rule directly to all $m$ $p$-values computed from unadjusted Fisher’s exact tests on the raw presence–absence tables, ignoring phylogenetic non-independence, and assert that BH controls FDR regardless of dependence among tests.**\n\nThis option is incorrect on two fundamental points.\n1.  It proposes using \"unadjusted Fisher’s exact tests... ignoring phylogenetic non-independence\". As explained above, this violates the assumptions of the test and will produce invalid $p$-values that are not uniformly distributed under the null. An FDR procedure applied to invalid $p$-values cannot provide FDR control.\n2.  It falsely asserts that \"BH controls FDR regardless of dependence among tests\". The BH procedure is proven to control FDR under independence and positive dependence (PRDS), but *not* under arbitrary dependence structures. For arbitrary dependence, a more conservative procedure like the Benjamini-Yekutieli (BY) procedure is required.\n\n**Verdict: Incorrect**\n\n**C. Replace the Benjamini–Hochberg rule with the Benjamini–Yekutieli (BY) procedure, which controls FDR under arbitrary dependence, and claim this is an acceptable way to “use BH” to control FDR in the presence of phylogeny-induced dependence.**\n\nThis option is flawed. While the BY procedure does control FDR under arbitrary dependence, it does not solve the primary problem. The BY procedure, like the BH procedure, fundamentally relies on the input $p$-values being valid (i.e., super-uniform under their respective null hypotheses). This option, by not specifying how the $p$-values are generated, implicitly condones using phylogeny-naive tests (as in option B). Applying the BY procedure to invalid $p$-values will not guarantee FDR control. It corrects for dependence among tests, but it cannot correct for misspecified tests. Furthermore, calling the BY procedure a way to \"use BH\" is a misnomer; it is a distinct, more conservative method.\n\n**Verdict: Incorrect**\n\n**D. Apply a weighted Benjamini–Hochberg procedure using weights constructed from the observed per-gene number of presences across the $S$ species, arguing that higher weights for more informative genes increase power while still controlling FDR because the weights are derived from the same data that produced the $p$-values.**\n\nThis option is incorrect for two reasons.\n1.  Like options B and C, it fails to address the critical issue of generating valid, phylogeny-aware $p$-values in the first place.\n2.  It proposes an invalid weighting scheme for the weighted BH procedure. A core requirement for the weighted BH procedure to be valid is that the weights $w_i$ must be independent of the $p$-values $p_i$ under the null hypothesis. Constructing weights from a feature of the data (the \"per-gene number of presences\") that is also used to compute the $p$-value itself violates this independence condition. This data-dependent weighting, in the manner described, invalidates the theoretical guarantee of FDR control.\n\n**Verdict: Incorrect**\n\nIn summary, option A is the only one that presents a statistically rigorous and complete plan that addresses both a) the need for phylogenetically-aware tests to obtain valid $p$-values and b) the correct dependence conditions (PRDS) under which the standard BH procedure remains valid.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Simple association scores can be confounded by the evolutionary relationships between species. More sophisticated methods account for the species phylogeny directly, often by modeling evolution as a continuous process on a tree. This computational practice guides you through implementing a phylogenetically-corrected association score and, crucially, propagating uncertainty from the tree itself to construct a confidence interval for your result, reflecting a complete and robust analytical pipeline .",
            "id": "4375055",
            "problem": "You are given a collection of species-level covariance matrices that arise from bootstrap replicates of an estimated species phylogeny under the Brownian motion model of trait evolution. In phylogenetic profiling for functional annotation, an association score between two gene profiles across species should account for shared evolutionary history. Under the Brownian motion model, if a continuous trait evolves along a species tree with total root-to-tip time $T$, then the trait covariance matrix across species equals the shared evolutionary time among species, which can be estimated from the species tree. Bootstrap replicates of the species tree induce a set of covariance matrices that approximate the uncertainty due to tree inference.\n\nYour task is to write a program that, for each test case, computes a phylogenetically corrected association score between two gene profiles by propagating the variation across bootstrap covariance matrices, and then reports a two-sided confidence interval. The program must implement the following procedure from first principles:\n\n1. For a given covariance matrix $\\mathbf{C} \\in \\mathbb{R}^{n \\times n}$ that is symmetric positive definite, compute a phylogenetically corrected association score between two vectors $\\mathbf{x} \\in \\mathbb{R}^{n}$ and $\\mathbf{y} \\in \\mathbb{R}^{n}$ representing gene profiles across $n$ species. Use the following steps:\n   - Compute the Cholesky factorization $\\mathbf{C} = \\mathbf{L}\\mathbf{L}^{\\top}$ with $\\mathbf{L}$ lower triangular.\n   - Solve $\\mathbf{L}\\mathbf{z}_{x} = \\mathbf{x}$ and $\\mathbf{L}\\mathbf{z}_{y} = \\mathbf{y}$ to obtain whitened vectors $\\mathbf{z}_{x}$ and $\\mathbf{z}_{y}$.\n   - Center each whitened vector by subtracting its arithmetic mean: $\\tilde{\\mathbf{z}}_{x} = \\mathbf{z}_{x} - \\bar{z}_{x}\\mathbf{1}$ and $\\tilde{\\mathbf{z}}_{y} = \\mathbf{z}_{y} - \\bar{z}_{y}\\mathbf{1}$, where $\\bar{z}_{x} = \\frac{1}{n}\\sum_{i=1}^{n} z_{x,i}$ and $\\bar{z}_{y} = \\frac{1}{n}\\sum_{i=1}^{n} z_{y,i}$, and $\\mathbf{1}$ is the vector of ones in $\\mathbb{R}^{n}$.\n   - Define the phylogenetically corrected association score as the Pearson correlation of the centered whitened vectors,\n     $$ r = \\frac{\\tilde{\\mathbf{z}}_{x}^{\\top}\\tilde{\\mathbf{z}}_{y}}{\\|\\tilde{\\mathbf{z}}_{x}\\|_{2}\\,\\|\\tilde{\\mathbf{z}}_{y}\\|_{2}}. $$\n     If $\\|\\tilde{\\mathbf{z}}_{x}\\|_{2} = 0$ or $\\|\\tilde{\\mathbf{z}}_{y}\\|_{2} = 0$, define $r = 0$ for that replicate to avoid division by zero.\n   This construction implements generalized least squares via whitening, justified by the Brownian motion model and the assumption of Gaussian increments.\n\n2. For a set of $K$ bootstrap covariance matrices $\\{\\mathbf{C}^{(k)}\\}_{k=1}^{K}$, compute $K$ association scores $\\{r^{(k)}\\}_{k=1}^{K}$, one per replicate covariance matrix. For a given two-sided confidence level $1-\\alpha$, with $\\alpha \\in (0,1)$, compute the lower and upper confidence limits as empirical quantiles at $q_{\\mathrm{low}} = \\alpha/2$ and $q_{\\mathrm{high}} = 1 - \\alpha/2$. Use the following quantile definition (piecewise linear interpolation between order statistics, also known as the type-$7$ quantile):\n   - Let the sorted values be $v_{(1)} \\le \\cdots \\le v_{(K)}$.\n   - For a quantile level $q \\in [0,1]$, compute\n     $$ h = (K - 1)q + 1, \\quad k = \\lfloor h \\rfloor, \\quad \\gamma = h - k. $$\n   - The quantile is\n     $$ Q(q) = \\begin{cases}\n     v_{(k)} & \\text{if } \\gamma = 0,\\\\\n     v_{(k)} + \\gamma\\left(v_{(k+1)} - v_{(k)}\\right) & \\text{if } \\gamma \\in (0,1).\n     \\end{cases} $$\n\n3. Apply this procedure to each test case below with $\\alpha = 0.05$ (that is, $q_{\\mathrm{low}} = 0.025$ and $q_{\\mathrm{high}} = 0.975$). For numerical stability, if a covariance matrix is nearly singular, you may add a small diagonal jitter $\\varepsilon \\mathbf{I}$ with $\\varepsilon = 10^{-10}$ before Cholesky factorization.\n\nTest suite. Each test case consists of a pair $(\\mathbf{x}, \\mathbf{y})$ and a list of bootstrap covariance matrices. All numbers are real-valued. Vectors are listed in species order $\\{s_{1}, s_{2}, s_{3}, s_{4}\\}$.\n\n- Test case $1$ (happy path; strong concordance):\n  - $\\mathbf{x} = [\\,1,\\,1,\\,0,\\,0\\,]$, $\\mathbf{y} = [\\,1,\\,1,\\,0,\\,0\\,]$.\n  - Bootstrap covariance matrices ($K = 5$):\n    - $\\mathbf{C}^{(1)} = \\begin{bmatrix}\n      1.0 & 0.8 & 0.2 & 0.2\\\\\n      0.8 & 1.0 & 0.2 & 0.2\\\\\n      0.2 & 0.2 & 1.0 & 0.7\\\\\n      0.2 & 0.2 & 0.7 & 1.0\n      \\end{bmatrix}$,\n      $\\mathbf{C}^{(2)} = \\begin{bmatrix}\n      1.0 & 0.75 & 0.25 & 0.25\\\\\n      0.75 & 1.0 & 0.25 & 0.25\\\\\n      0.25 & 0.25 & 1.0 & 0.65\\\\\n      0.25 & 0.25 & 0.65 & 1.0\n      \\end{bmatrix}$,\n      $\\mathbf{C}^{(3)} = \\begin{bmatrix}\n      1.0 & 0.85 & 0.15 & 0.15\\\\\n      0.85 & 1.0 & 0.15 & 0.15\\\\\n      0.15 & 0.15 & 1.0 & 0.6\\\\\n      0.15 & 0.15 & 0.6 & 1.0\n      \\end{bmatrix}$,\n      $\\mathbf{C}^{(4)} = \\begin{bmatrix}\n      1.0 & 0.7 & 0.3 & 0.2\\\\\n      0.7 & 1.0 & 0.2 & 0.3\\\\\n      0.3 & 0.2 & 1.0 & 0.55\\\\\n      0.2 & 0.3 & 0.55 & 1.0\n      \\end{bmatrix}$,\n      $\\mathbf{C}^{(5)} = \\begin{bmatrix}\n      1.0 & 0.78 & 0.22 & 0.22\\\\\n      0.78 & 1.0 & 0.22 & 0.22\\\\\n      0.22 & 0.22 & 1.0 & 0.68\\\\\n      0.22 & 0.22 & 0.68 & 1.0\n      \\end{bmatrix}$.\n\n- Test case $2$ (boundary; near-star trees, perfect concordance):\n  - $\\mathbf{x} = [\\,0,\\,1,\\,0,\\,1\\,]$, $\\mathbf{y} = [\\,0,\\,1,\\,0,\\,1\\,]$.\n  - Bootstrap covariance matrices ($K = 5$):\n    - $\\mathbf{C}^{(1)} = \\begin{bmatrix}\n      1.0 & 0.4 & 0.4 & 0.4\\\\\n      0.4 & 1.0 & 0.4 & 0.4\\\\\n      0.4 & 0.4 & 1.0 & 0.4\\\\\n      0.4 & 0.4 & 0.4 & 1.0\n      \\end{bmatrix}$,\n      $\\mathbf{C}^{(2)} = \\begin{bmatrix}\n      1.0 & 0.3 & 0.3 & 0.3\\\\\n      0.3 & 1.0 & 0.3 & 0.3\\\\\n      0.3 & 0.3 & 1.0 & 0.3\\\\\n      0.3 & 0.3 & 0.3 & 1.0\n      \\end{bmatrix}$,\n      $\\mathbf{C}^{(3)} = \\begin{bmatrix}\n      1.0 & 0.2 & 0.2 & 0.2\\\\\n      0.2 & 1.0 & 0.2 & 0.2\\\\\n      0.2 & 0.2 & 1.0 & 0.2\\\\\n      0.2 & 0.2 & 0.2 & 1.0\n      \\end{bmatrix}$,\n      $\\mathbf{C}^{(4)} = \\begin{bmatrix}\n      1.0 & 0.5 & 0.5 & 0.5\\\\\n      0.5 & 1.0 & 0.5 & 0.5\\\\\n      0.5 & 0.5 & 1.0 & 0.5\\\\\n      0.5 & 0.5 & 0.5 & 1.0\n      \\end{bmatrix}$,\n      $\\mathbf{C}^{(5)} = \\begin{bmatrix}\n      1.0 & 0.35 & 0.35 & 0.35\\\\\n      0.35 & 1.0 & 0.35 & 0.35\\\\\n      0.35 & 0.35 & 1.0 & 0.35\\\\\n      0.35 & 0.35 & 0.35 & 1.0\n      \\end{bmatrix}$.\n\n- Test case $3$ (edge case; discordant patterns across bootstrap trees):\n  - $\\mathbf{x} = [\\,1,\\,1,\\,0,\\,0\\,]$, $\\mathbf{y} = [\\,1,\\,0,\\,1,\\,0\\,]$.\n  - Bootstrap covariance matrices ($K = 5$):\n    - $\\mathbf{C}^{(1)} = \\begin{bmatrix}\n      1.0 & 0.8 & 0.2 & 0.2\\\\\n      0.8 & 1.0 & 0.2 & 0.2\\\\\n      0.2 & 0.2 & 1.0 & 0.7\\\\\n      0.2 & 0.2 & 0.7 & 1.0\n      \\end{bmatrix}$,\n      $\\mathbf{C}^{(2)} = \\begin{bmatrix}\n      1.0 & 0.2 & 0.8 & 0.2\\\\\n      0.2 & 1.0 & 0.2 & 0.75\\\\\n      0.8 & 0.2 & 1.0 & 0.2\\\\\n      0.2 & 0.75 & 0.2 & 1.0\n      \\end{bmatrix}$,\n      $\\mathbf{C}^{(3)} = \\begin{bmatrix}\n      1.0 & 0.2 & 0.2 & 0.8\\\\\n      0.2 & 1.0 & 0.75 & 0.2\\\\\n      0.2 & 0.75 & 1.0 & 0.2\\\\\n      0.8 & 0.2 & 0.2 & 1.0\n      \\end{bmatrix}$,\n      $\\mathbf{C}^{(4)} = \\begin{bmatrix}\n      1.0 & 0.6 & 0.4 & 0.3\\\\\n      0.6 & 1.0 & 0.3 & 0.4\\\\\n      0.4 & 0.3 & 1.0 & 0.6\\\\\n      0.3 & 0.4 & 0.6 & 1.0\n      \\end{bmatrix}$,\n      $\\mathbf{C}^{(5)} = \\begin{bmatrix}\n      1.0 & 0.3 & 0.3 & 0.3\\\\\n      0.3 & 1.0 & 0.3 & 0.3\\\\\n      0.3 & 0.3 & 1.0 & 0.3\\\\\n      0.3 & 0.3 & 0.3 & 1.0\n      \\end{bmatrix}$.\n\nImplementation requirements:\n- Use the exact quantile definition above with $q_{\\mathrm{low}} = 0.025$ and $q_{\\mathrm{high}} = 0.975$.\n- If Cholesky factorization fails due to numerical issues, you may add a jitter $\\varepsilon = 10^{-10}$ to the diagonal.\n- For each test case, report the confidence interval as a pair $[L, U]$, where $L$ and $U$ are real numbers rounded to $6$ decimal places.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for the three test cases as a list of three intervals, each interval itself being a two-element list, with no spaces, and numbers rounded to $6$ decimal places. For example: $[[0.123456,0.654321],[0.100000,0.900000],[\\dots,\\dots]]$.",
            "solution": "The problem requires the implementation of a procedure to compute a phylogenetically corrected association score between two gene profiles and to construct a confidence interval for this score based on bootstrap replicates of the underlying species phylogeny. The problem is scientifically sound, mathematically well-defined, and provides all necessary data and specifications. We may therefore proceed with a solution.\n\nThe solution is grounded in three main principles:\n1.  Correcting for non-independence of data using Generalized Least Squares (GLS) via data whitening.\n2.  Calculating the association score as a Pearson correlation coefficient on the whitened data.\n3.  Quantifying uncertainty using a bootstrap percentile confidence interval.\n\n**1. Phylogenetic Correction via Data Whitening**\n\nIn evolutionary biology, traits measured on a set of related species are not statistically independent. The shared evolutionary history induces a covariance structure that must be accounted for in statistical analyses. The Brownian motion model of trait evolution posits that a continuous trait vector $\\mathbf{x} \\in \\mathbb{R}^{n}$ for $n$ species follows a multivariate normal distribution, $\\mathbf{x} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\mathbf{C})$, where $\\boldsymbol{\\mu}$ is the mean vector and $\\mathbf{C} \\in \\mathbb{R}^{n \\times n}$ is the covariance matrix. The elements $C_{ij}$ of this matrix are proportional to the shared branch length (evolutionary time) from the root of the phylogeny to the most recent common ancestor of species $i$ and $j$.\n\nStandard statistical methods like Pearson correlation assume independent and identically distributed (i.i.d.) observations. To use such methods, we must first transform the data to remove the phylogenetic correlations. This process is known as \"whitening.\" We seek a linear transformation matrix $\\mathbf{M}$ such that the transformed vector $\\mathbf{z} = \\mathbf{M}\\mathbf{x}$ has a covariance matrix equal to the identity matrix $\\mathbf{I}$. The covariance of $\\mathbf{z}$ is given by:\n$$ \\text{Cov}(\\mathbf{z}) = \\text{Cov}(\\mathbf{M}\\mathbf{x}) = \\mathbf{M} \\text{Cov}(\\mathbf{x}) \\mathbf{M}^{\\top} = \\mathbf{M}\\mathbf{C}\\mathbf{M}^{\\top} $$\nWe require $\\mathbf{M}\\mathbf{C}\\mathbf{M}^{\\top} = \\mathbf{I}$.\n\nA standard method to find such a matrix $\\mathbf{M}$ is through the Cholesky factorization of the symmetric positive definite covariance matrix $\\mathbf{C}$. The Cholesky factorization is $\\mathbf{C} = \\mathbf{L}\\mathbf{L}^{\\top}$, where $\\mathbf{L}$ is a unique lower triangular matrix with positive diagonal entries.\n\nBy choosing the whitening matrix to be the inverse of the Cholesky factor, $\\mathbf{M} = \\mathbf{L}^{-1}$, we achieve the desired property:\n$$ \\mathbf{M}\\mathbf{C}\\mathbf{M}^{\\top} = \\mathbf{L}^{-1} (\\mathbf{L}\\mathbf{L}^{\\top}) (\\mathbf{L}^{-1})^{\\top} = (\\mathbf{L}^{-1}\\mathbf{L}) (\\mathbf{L}^{\\top}(\\mathbf{L}^{\\top})^{-1}) = \\mathbf{I} \\mathbf{I} = \\mathbf{I} $$\nThus, the whitened vector is $\\mathbf{z} = \\mathbf{L}^{-1}\\mathbf{x}$. Instead of explicitly computing the inverse $\\mathbf{L}^{-1}$, it is more numerically stable to obtain $\\mathbf{z}$ by solving the lower triangular linear system $\\mathbf{L}\\mathbf{z} = \\mathbf{x}$ using forward substitution. A small diagonal jitter $\\varepsilon\\mathbf{I}$ (with $\\varepsilon = 10^{-10}$) may be added to $\\mathbf{C}$ to ensure it is numerically positive definite before factorization.\n\n**2. Calculation of the Phylogenetically Corrected Association Score**\n\nOnce we have transformed the gene profiles $\\mathbf{x}$ and $\\mathbf{y}$ into their whitened counterparts $\\mathbf{z}_{x}$ and $\\mathbf{z}_{y}$, the components of these new vectors are uncorrelated and have unit variance. We can now compute a meaningful association score using the standard Pearson correlation coefficient.\n\nThe Pearson correlation coefficient measures the linear relationship between two datasets. It is defined for two vectors $\\mathbf{a}$ and $\\mathbf{b}$ as the covariance of the two vectors divided by the product of their standard deviations. This is equivalent to computing the dot product of their centered versions, normalized by their Euclidean norms.\n\nFirst, we center the whitened vectors by subtracting their respective arithmetic means:\n$$ \\tilde{\\mathbf{z}}_{x} = \\mathbf{z}_{x} - \\bar{z}_{x}\\mathbf{1}, \\quad \\text{where} \\quad \\bar{z}_{x} = \\frac{1}{n}\\sum_{i=1}^{n} z_{x,i} $$\n$$ \\tilde{\\mathbf{z}}_{y} = \\mathbf{z}_{y} - \\bar{z}_{y}\\mathbf{1}, \\quad \\text{where} \\quad \\bar{z}_{y} = \\frac{1}{n}\\sum_{i=1}^{n} z_{y,i} $$\nHere, $\\mathbf{1}$ is the vector of ones. The phylogenetically corrected association score, $r$, is the Pearson correlation of $\\mathbf{z}_{x}$ and $\\mathbf{z}_{y}$:\n$$ r = \\frac{\\tilde{\\mathbf{z}}_{x}^{\\top}\\tilde{\\mathbf{z}}_{y}}{\\|\\tilde{\\mathbf{z}}_{x}\\|_{2}\\,\\|\\tilde{\\mathbf{z}}_{y}\\|_{2}} $$\nIf the norm of either centered whitened vector is zero (i.e., $\\|\\tilde{\\mathbf{z}}_{x}\\|_{2} = 0$ or $\\|\\tilde{\\mathbf{z}}_{y}\\|_{2} = 0$), which occurs if all components of the original whitened vector are identical, the correlation is undefined. In this specific scenario, the problem states to define the score as $r = 0$.\n\n**3. Bootstrap Confidence Interval**\n\nThe covariance matrix $\\mathbf{C}$ is derived from an estimated phylogeny, which itself is subject to uncertainty. To propagate this uncertainty into our association score, we use a set of $K$ bootstrap replicate phylogenies, which yields a set of $K$ covariance matrices $\\{\\mathbf{C}^{(k)}\\}_{k=1}^{K}$.\n\nFor each matrix $\\mathbf{C}^{(k)}$, we compute the association score $r^{(k)}$ as described above. This gives us a sample distribution $\\{r^{(k)}\\}_{k=1}^{K}$ of the association score, which reflects the uncertainty in the phylogeny.\n\nA two-sided confidence interval at level $1-\\alpha$ can be constructed by finding the empirical quantiles of this bootstrap distribution. The lower limit is the quantile $q_{\\mathrm{low}} = \\alpha/2$ and the upper limit is the quantile $q_{\\mathrm{high}} = 1 - \\alpha/2$. The problem specifies the type-7 quantile definition, which involves linear interpolation.\n\nLet the sorted scores be $v_{(1)} \\le v_{(2)} \\le \\cdots \\le v_{(K)}$. For a desired quantile $q \\in [0,1]$, we calculate a 1-based index $h = (K - 1)q + 1$. We find its integer part $k = \\lfloor h \\rfloor$ and fractional part $\\gamma = h - k$. The quantile value $Q(q)$ is then interpolated as:\n$$ Q(q) = v_{(k)} + \\gamma(v_{(k+1)} - v_{(k)}) $$\nwith the edge case $Q(q) = v_{(k)}$ if $\\gamma = 0$. This method is a standard approach for estimating quantiles from a finite sample. For the given problem, $\\alpha = 0.05$, so we compute the quantiles for $q=0.025$ and $q=0.975$.\n\nThis completes the formal specification of the algorithm. We will now proceed with its implementation.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import cholesky, solve_triangular\nfrom scipy.linalg.lapack import dpotrf\n\ndef solve():\n    \"\"\"\n    Computes phylogenetically corrected association scores and confidence intervals\n    for a suite of test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"x\": np.array([1.0, 1.0, 0.0, 0.0]),\n            \"y\": np.array([1.0, 1.0, 0.0, 0.0]),\n            \"C_list\": [\n                np.array([\n                    [1.0, 0.8, 0.2, 0.2],\n                    [0.8, 1.0, 0.2, 0.2],\n                    [0.2, 0.2, 1.0, 0.7],\n                    [0.2, 0.2, 0.7, 1.0]\n                ]),\n                np.array([\n                    [1.0, 0.75, 0.25, 0.25],\n                    [0.75, 1.0, 0.25, 0.25],\n                    [0.25, 0.25, 1.0, 0.65],\n                    [0.25, 0.25, 0.65, 1.0]\n                ]),\n                np.array([\n                    [1.0, 0.85, 0.15, 0.15],\n                    [0.85, 1.0, 0.15, 0.15],\n                    [0.15, 0.15, 1.0, 0.6],\n                    [0.15, 0.15, 0.6, 1.0]\n                ]),\n                np.array([\n                    [1.0, 0.7, 0.3, 0.2],\n                    [0.7, 1.0, 0.2, 0.3],\n                    [0.3, 0.2, 1.0, 0.55],\n                    [0.2, 0.3, 0.55, 1.0]\n                ]),\n                np.array([\n                    [1.0, 0.78, 0.22, 0.22],\n                    [0.78, 1.0, 0.22, 0.22],\n                    [0.22, 0.22, 1.0, 0.68],\n                    [0.22, 0.22, 0.68, 1.0]\n                ])\n            ]\n        },\n        {\n            \"x\": np.array([0.0, 1.0, 0.0, 1.0]),\n            \"y\": np.array([0.0, 1.0, 0.0, 1.0]),\n            \"C_list\": [\n                np.array([\n                    [1.0, 0.4, 0.4, 0.4], [0.4, 1.0, 0.4, 0.4],\n                    [0.4, 0.4, 1.0, 0.4], [0.4, 0.4, 0.4, 1.0]\n                ]),\n                np.array([\n                    [1.0, 0.3, 0.3, 0.3], [0.3, 1.0, 0.3, 0.3],\n                    [0.3, 0.3, 1.0, 0.3], [0.3, 0.3, 0.3, 1.0]\n                ]),\n                np.array([\n                    [1.0, 0.2, 0.2, 0.2], [0.2, 1.0, 0.2, 0.2],\n                    [0.2, 0.2, 1.0, 0.2], [0.2, 0.2, 0.2, 1.0]\n                ]),\n                np.array([\n                    [1.0, 0.5, 0.5, 0.5], [0.5, 1.0, 0.5, 0.5],\n                    [0.5, 0.5, 1.0, 0.5], [0.5, 0.5, 0.5, 1.0]\n                ]),\n                np.array([\n                    [1.0, 0.35, 0.35, 0.35], [0.35, 1.0, 0.35, 0.35],\n                    [0.35, 0.35, 1.0, 0.35], [0.35, 0.35, 0.35, 1.0]\n                ])\n            ]\n        },\n        {\n            \"x\": np.array([1.0, 1.0, 0.0, 0.0]),\n            \"y\": np.array([1.0, 0.0, 1.0, 0.0]),\n            \"C_list\": [\n                np.array([\n                    [1.0, 0.8, 0.2, 0.2], [0.8, 1.0, 0.2, 0.2],\n                    [0.2, 0.2, 1.0, 0.7], [0.2, 0.2, 0.7, 1.0]\n                ]),\n                np.array([\n                    [1.0, 0.2, 0.8, 0.2], [0.2, 1.0, 0.2, 0.75],\n                    [0.8, 0.2, 1.0, 0.2], [0.2, 0.75, 0.2, 1.0]\n                ]),\n                np.array([\n                    [1.0, 0.2, 0.2, 0.8], [0.2, 1.0, 0.75, 0.2],\n                    [0.2, 0.75, 1.0, 0.2], [0.8, 0.2, 0.2, 1.0]\n                ]),\n                np.array([\n                    [1.0, 0.6, 0.4, 0.3], [0.6, 1.0, 0.3, 0.4],\n                    [0.4, 0.3, 1.0, 0.6], [0.3, 0.4, 0.6, 1.0]\n                ]),\n                np.array([\n                    [1.0, 0.3, 0.3, 0.3], [0.3, 1.0, 0.3, 0.3],\n                    [0.3, 0.3, 1.0, 0.3], [0.3, 0.3, 0.3, 1.0]\n                ])\n            ]\n        }\n    ]\n\n    alpha = 0.05\n    q_low = alpha / 2.0\n    q_high = 1.0 - alpha / 2.0\n    epsilon = 1e-10\n\n    results = []\n    \n    for case in test_cases:\n        x = case[\"x\"]\n        y = case[\"y\"]\n        C_list = case[\"C_list\"]\n        n = x.shape[0]\n\n        r_scores = []\n        for C_orig in C_list:\n            C = C_orig.copy()\n            # Perform Cholesky factorization, adding jitter if not positive definite\n            try:\n                # Use raw LAPACK call to check for non-positive-definiteness\n                # dpotrf returns a positive info value if not positive definite\n                _, info = dpotrf(C, lower=True, overwrite_a=False, clean=True)\n                if info > 0:\n                    raise np.linalg.LinAlgError(\"Matrix is not positive definite\")\n                L = cholesky(C, lower=True)\n            except np.linalg.LinAlgError:\n                C_jittered = C + np.eye(n) * epsilon\n                L = cholesky(C_jittered, lower=True)\n\n            # Whiten the vectors by solving L * z = x (and y)\n            zx = solve_triangular(L, x, lower=True)\n            zy = solve_triangular(L, y, lower=True)\n\n            # Center the whitened vectors\n            zx_tilde = zx - np.mean(zx)\n            zy_tilde = zy - np.mean(zy)\n\n            # Compute norms\n            norm_zx = np.linalg.norm(zx_tilde)\n            norm_zy = np.linalg.norm(zy_tilde)\n\n            # Compute the phylogenetically corrected association score (Pearson correlation)\n            if norm_zx == 0 or norm_zy == 0:\n                r = 0.0\n            else:\n                numerator = np.dot(zx_tilde, zy_tilde)\n                denominator = norm_zx * norm_zy\n                r = numerator / denominator\n            \n            r_scores.append(r)\n\n        # Compute the confidence interval using type-7 quantiles\n        # np.quantile with interpolation='linear' implements type-7\n        lower_bound = np.quantile(r_scores, q_low, interpolation='linear')\n        upper_bound = np.quantile(r_scores, q_high, interpolation='linear')\n        \n        results.append([round(lower_bound, 6), round(upper_bound, 6)])\n\n    # Format the final output string\n    output_str = \"[[\" + \"],[\".join([\",\".join([f\"{v:.6f}\" for v in r]) for r in results]) + \"]]\"\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}