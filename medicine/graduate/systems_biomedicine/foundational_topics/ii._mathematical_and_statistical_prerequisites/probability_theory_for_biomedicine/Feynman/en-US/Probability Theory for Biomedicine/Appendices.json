{
    "hands_on_practices": [
        {
            "introduction": "The Poisson distribution is a cornerstone for modeling discrete events in biology, such as the number of mutations in a gene or the branching of microvasculature. This exercise challenges you to derive the Moment Generating Function ($M_X(t)$) from first principles, a powerful mathematical tool that encapsulates a distribution's key properties. By working through this derivation and using it to find the moments of the distribution , you will gain a deep, functional understanding of how to characterize and analyze count-based biological data.",
            "id": "4378385",
            "problem": "A tissue-engineered construct is imaged to quantify microvasculature branching events within a standardized field-of-view. Under the assumptions that branching events occur independently and sparsely, the count of observed branches in a single field-of-view is modeled by a Poisson random variable $X$ with parameter $\\lambda>0$, where $\\lambda$ is the expected number of branches per field-of-view. Starting from the definition of the probability mass function (PMF) of the Poisson distribution and the definition of the moment generating function (MGF), derive an explicit closed-form expression for the MGF $M_{X}(t)$ of $X$. Then, using only the definition of moments in terms of derivatives of the MGF evaluated at $t=0$, compute the first three raw moments $\\mathbb{E}[X]$, $\\mathbb{E}[X^{2}]$, and $\\mathbb{E}[X^{3}]$ in symbolic form.\n\nYour derivation must begin from the following foundational bases:\n- The PMF of a Poisson random variable: for $x\\in\\{0,1,2,\\dots\\}$, $\\mathbb{P}(X=x)$ depends on $\\lambda$.\n- The definition of the moment generating function (MGF): for any real $t$, $M_{X}(t)=\\mathbb{E}[\\exp(tX)]$.\n\nExpress your final results symbolically in terms of $\\lambda$ and $t$; do not introduce any numerical values. No rounding is required. Provide the MGF and the three raw moments.",
            "solution": "The problem statement is subjected to validation according to the specified criteria.\n\n**Problem Validation**\n\n**Step 1: Extracted Givens**\n- A random variable $X$ represents the count of microvasculature branching events.\n- $X$ follows a Poisson distribution with parameter $\\lambda > 0$.\n- $\\lambda$ is the expected number of branches per field-of-view.\n- The probability mass function (PMF) for $X$ is given for $x \\in \\{0, 1, 2, \\dots\\}$, defined as $\\mathbb{P}(X=x)$.\n- The moment generating function (MGF) is defined as $M_{X}(t) = \\mathbb{E}[\\exp(tX)]$.\n- The task is to derive the MGF, $M_{X}(t)$, in closed form.\n- Subsequently, the first three raw moments, $\\mathbb{E}[X]$, $\\mathbb{E}[X^2]$, and $\\mathbb{E}[X^3]$, must be computed using only the MGF and its derivatives.\n- Results must be symbolic in terms of $\\lambda$ and $t$.\n\n**Step 2: Validation Using Extracted Givens**\nThe problem is assessed as valid.\n- **Scientifically Grounded:** The Poisson distribution is the standard and appropriate model for counting rare, independent events in a fixed domain, such as the described biological context. The problem is based on fundamental principles of probability theory.\n- **Well-Posed:** The problem is clearly formulated. It requests the derivation of standard statistical properties (MGF and moments) for a well-defined probability distribution (Poisson). A unique and meaningful solution exists.\n- **Objective:** The problem is expressed using precise, standard mathematical and statistical terminology, free of any subjectivity or ambiguity.\n- **Completeness and Consistency:** All necessary definitions and constraints are provided. The reference to the \"definition of the PMF of the Poisson distribution\" is unambiguous, as this function is universally defined. There are no contradictions.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\n**Derivation of the Moment Generating Function (MGF)**\n\nThe analysis begins with the foundational definitions provided. For a discrete random variable $X$, the expected value of a function of $X$, say $g(X)$, is given by $\\mathbb{E}[g(X)] = \\sum_{x} g(x) \\mathbb{P}(X=x)$, where the sum is over all possible values $x$ that $X$ can take. The random variable $X$ follows a Poisson distribution with parameter $\\lambda$, so its support is the set of non-negative integers $\\{0, 1, 2, \\dots\\}$, and its probability mass function (PMF) is given by:\n$$\n\\mathbb{P}(X=x) = \\frac{\\lambda^{x} \\exp(-\\lambda)}{x!}, \\quad \\text{for } x = 0, 1, 2, \\dots\n$$\nThe moment generating function (MGF) is defined as $M_{X}(t) = \\mathbb{E}[\\exp(tX)]$. Applying the expectation formula with $g(X) = \\exp(tX)$, we have:\n$$\nM_{X}(t) = \\sum_{x=0}^{\\infty} \\exp(tx) \\mathbb{P}(X=x)\n$$\nSubstituting the PMF of the Poisson distribution into this definition yields:\n$$\nM_{X}(t) = \\sum_{x=0}^{\\infty} \\exp(tx) \\frac{\\lambda^{x} \\exp(-\\lambda)}{x!}\n$$\nThe term $\\exp(-\\lambda)$ does not depend on the summation index $x$ and can be factored out of the summation:\n$$\nM_{X}(t) = \\exp(-\\lambda) \\sum_{x=0}^{\\infty} \\frac{\\exp(tx) \\lambda^{x}}{x!}\n$$\nWe can combine the terms raised to the power of $x$: $\\exp(tx) \\lambda^{x} = (\\lambda \\exp(t))^{x}$.\n$$\nM_{X}(t) = \\exp(-\\lambda) \\sum_{x=0}^{\\infty} \\frac{(\\lambda \\exp(t))^{x}}{x!}\n$$\nThe summation is recognized as the Maclaurin series expansion for the exponential function, $\\exp(z) = \\sum_{k=0}^{\\infty} \\frac{z^{k}}{k!}$. In this expression, $z = \\lambda \\exp(t)$. Therefore, the sum evaluates to $\\exp(\\lambda \\exp(t))$.\nSubstituting this result back, we obtain the closed-form expression for the MGF:\n$$\nM_{X}(t) = \\exp(-\\lambda) \\exp(\\lambda \\exp(t)) = \\exp(-\\lambda + \\lambda \\exp(t))\n$$\nFactoring out $\\lambda$ in the exponent gives the final, standard form for the MGF of a Poisson random variable:\n$$\nM_{X}(t) = \\exp(\\lambda(\\exp(t) - 1))\n$$\nThis derivation fulfills the first part of the problem.\n\n**Computation of an Explicit Closed-Form Expression for Raw Moments**\n\nThe $n$-th raw moment of $X$, denoted $\\mathbb{E}[X^n]$, is obtained by computing the $n$-th derivative of the MGF with respect to $t$ and evaluating it at $t=0$.\n$$\n\\mathbb{E}[X^n] = \\frac{d^n}{dt^n} M_{X}(t) \\bigg|_{t=0}\n$$\nWe will now compute the first three raw moments using this property.\n\n**First Raw Moment: $\\mathbb{E}[X]$**\nThe first moment is the first derivative of $M_X(t)$ evaluated at $t=0$.\n$$\nM'_{X}(t) = \\frac{d}{dt} \\exp(\\lambda(\\exp(t) - 1))\n$$\nUsing the chain rule, where the outer function is $\\exp(u)$ and the inner function is $u(t) = \\lambda(\\exp(t) - 1)$, we have $u'(t)=\\lambda\\exp(t)$.\n$$\nM'_{X}(t) = \\exp(\\lambda(\\exp(t) - 1)) \\cdot (\\lambda \\exp(t)) = \\lambda \\exp(t) M_{X}(t)\n$$\nEvaluating at $t=0$:\n$$\n\\mathbb{E}[X] = M'_{X}(0) = \\lambda \\exp(0) M_{X}(0) = \\lambda \\cdot 1 \\cdot \\exp(\\lambda(\\exp(0) - 1)) = \\lambda \\cdot \\exp(\\lambda(1-1)) = \\lambda \\exp(0) = \\lambda\n$$\nThus, the first raw moment is $\\mathbb{E}[X] = \\lambda$.\n\n**Second Raw Moment: $\\mathbb{E}[X^2]$**\nThe second moment is the second derivative of $M_X(t)$ evaluated at $t=0$. We differentiate $M'_{X}(t) = \\lambda \\exp(t) M_{X}(t)$ using the product rule.\n$$\nM''_{X}(t) = \\frac{d}{dt} (\\lambda \\exp(t) M_{X}(t)) = \\left(\\frac{d}{dt} (\\lambda \\exp(t))\\right) M_{X}(t) + (\\lambda \\exp(t)) \\left(\\frac{d}{dt} M_{X}(t)\\right)\n$$\n$$\nM''_{X}(t) = (\\lambda \\exp(t)) M_{X}(t) + (\\lambda \\exp(t)) M'_{X}(t)\n$$\nSubstituting $M'_{X}(t) = \\lambda \\exp(t) M_{X}(t)$:\n$$\nM''_{X}(t) = \\lambda \\exp(t) M_{X}(t) + (\\lambda \\exp(t))(\\lambda \\exp(t) M_{X}(t)) = M_{X}(t)[\\lambda \\exp(t) + \\lambda^2 \\exp(2t)]\n$$\nEvaluating at $t=0$:\n$$\n\\mathbb{E}[X^2] = M''_{X}(0) = M_{X}(0)[\\lambda \\exp(0) + \\lambda^2 \\exp(2 \\cdot 0)] = 1 \\cdot [\\lambda \\cdot 1 + \\lambda^2 \\cdot 1] = \\lambda + \\lambda^2\n$$\nThus, the second raw moment is $\\mathbb{E}[X^2] = \\lambda^2 + \\lambda$.\n\n**Third Raw Moment: $\\mathbb{E}[X^3]$**\nThe third moment is the third derivative of $M_X(t)$ evaluated at $t=0$. We differentiate $M''_{X}(t) = M_{X}(t)[\\lambda \\exp(t) + \\lambda^2 \\exp(2t)]$ using the product rule.\n$$\nM'''_{X}(t) = \\frac{d}{dt} \\left(M_{X}(t)[\\lambda \\exp(t) + \\lambda^2 \\exp(2t)]\\right)\n$$\n$$\nM'''_{X}(t) = M'_{X}(t)[\\lambda \\exp(t) + \\lambda^2 \\exp(2t)] + M_{X}(t)[\\lambda \\exp(t) + 2\\lambda^2 \\exp(2t)]\n$$\nEvaluating at $t=0$, and using the previously computed values $M_X(0)=1$ and $M'_X(0)=\\lambda$:\n$$\n\\mathbb{E}[X^3] = M'''_{X}(0) = M'_{X}(0)[\\lambda \\exp(0) + \\lambda^2 \\exp(0)] + M_X(0)[\\lambda \\exp(0) + 2\\lambda^2 \\exp(0)]\n$$\n$$\n\\mathbb{E}[X^3] = \\lambda[\\lambda + \\lambda^2] + 1[\\lambda + 2\\lambda^2] = \\lambda^2 + \\lambda^3 + \\lambda + 2\\lambda^2\n$$\nCombining terms, we find the third raw moment:\n$$\n\\mathbb{E}[X^3] = \\lambda^3 + 3\\lambda^2 + \\lambda\n$$\nThis completes the required derivations. The expressions for the MGF and the first three raw moments are provided in the final answer.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\exp(\\lambda(\\exp(t) - 1)) & \\lambda & \\lambda^2 + \\lambda & \\lambda^3 + 3\\lambda^2 + \\lambda\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Beyond describing data, probability theory provides a framework for making optimal decisions under uncertainty, a central task in clinical practice. This problem delves into Bayesian decision theory by asking you to construct an optimal classifier for sepsis detection . You will derive a decision threshold that minimizes expected loss, crucially accounting for the asymmetric costs of misdiagnosisâ€”a vital consideration in medicine where a false negative can be far more dangerous than a false positive.",
            "id": "4378388",
            "problem": "A precision-medicine triage algorithm in systems biomedicine screens for sepsis using a plasma biomarker concentration measured in ng/mL. Let $B$ denote the biomarker concentration and define the transformed variable $X = \\ln(B)$. In a target inpatient population, assume the following scientifically motivated model:\n- The disease label is $Y \\in \\{0,1\\}$, where $Y=1$ indicates sepsis and $Y=0$ indicates no sepsis.\n- The prior probabilities are $\\pi_{1} = 0.12$ and $\\pi_{0} = 0.88$.\n- The class-conditional distributions of $X$ are Gaussian with equal variance: $X \\mid Y=0 \\sim \\mathcal{N}(\\mu_{0}, \\sigma^{2})$ and $X \\mid Y=1 \\sim \\mathcal{N}(\\mu_{1}, \\sigma^{2})$, where $\\mu_{0} = -0.2$, $\\mu_{1} = 0.6$, and $\\sigma = 0.5$.\n- The decision rule outputs $\\hat{Y} \\in \\{0,1\\}$ based on a threshold applied to $X$ or equivalently to $B$. A false negative (predicting $0$ when $Y=1$) incurs loss $C_{\\mathrm{FN}} = 40$, and a false positive (predicting $1$ when $Y=0$) incurs loss $C_{\\mathrm{FP}} = 1$. Correct classifications incur zero loss.\n\nFrom first principles of Bayesian decision theory, derive the Bayes-optimal threshold on the original concentration scale, that is, find the value $B^{\\ast}$ (in ng/mL) such that the Bayes classifier that minimizes expected misclassification loss predicts sepsis if and only if $B \\geq B^{\\ast}$. Your derivation must start from the definition of expected loss and priors, and use the given class-conditional distributions without invoking any pre-stated final decision formulas. Provide the final Bayes-optimal threshold $B^{\\ast}$ in ng/mL, and round your answer to four significant figures.",
            "solution": "The problem requires the derivation of the Bayes-optimal decision threshold for diagnosing sepsis from first principles. The goal is to find a threshold $B^{\\ast}$ for the biomarker concentration $B$ such that the expected loss is minimized. The decision rule is to predict sepsis ($\\hat{Y}=1$) if $B \\ge B^{\\ast}$ and no sepsis ($\\hat{Y}=0$) if $B < B^{\\ast}$. Since the analysis is performed on the transformed variable $X = \\ln(B)$, we will first derive the threshold $X^{\\ast}$ for $X$.\n\nLet $x$ be an observed value of the random variable $X$. According to Bayesian decision theory, we must choose the action (predict $\\hat{Y}=0$ or $\\hat{Y}=1$) that minimizes the conditional expected loss, given the observation $X=x$. The losses for correct classifications are zero, while the losses for misclassifications are given as $C_{\\mathrm{FN}} = 40$ for a false negative ($\\hat{Y}=0, Y=1$) and $C_{\\mathrm{FP}} = 1$ for a false positive ($\\hat{Y}=1, Y=0$).\n\nThe conditional expected loss, or risk, of deciding $\\hat{Y}=1$ (predicting sepsis) given the observation $X=x$ is:\n$$R(\\hat{Y}=1 | X=x) = (\\text{loss for } \\hat{Y}=1, Y=0) P(Y=0|X=x) + (\\text{loss for } \\hat{Y}=1, Y=1) P(Y=1|X=x)$$\n$$R(\\hat{Y}=1 | X=x) = C_{\\mathrm{FP}} P(Y=0|X=x) + 0 \\cdot P(Y=1|X=x) = C_{\\mathrm{FP}} P(Y=0|X=x)$$\n\nThe conditional expected loss of deciding $\\hat{Y}=0$ (predicting no sepsis) given the observation $X=x$ is:\n$$R(\\hat{Y}=0 | X=x) = (\\text{loss for } \\hat{Y}=0, Y=0) P(Y=0|X=x) + (\\text{loss for } \\hat{Y}=0, Y=1) P(Y=1|X=x)$$\n$$R(\\hat{Y}=0 | X=x) = 0 \\cdot P(Y=0|X=x) + C_{\\mathrm{FN}} P(Y=1|X=x) = C_{\\mathrm{FN}} P(Y=1|X=x)$$\n\nThe Bayes-optimal decision rule is to predict sepsis ($\\hat{Y}=1$) if the risk of doing so is less than the risk of predicting no sepsis. That is, we predict $\\hat{Y}=1$ if:\n$$R(\\hat{Y}=1 | X=x) < R(\\hat{Y}=0 | X=x)$$\n$$C_{\\mathrm{FP}} P(Y=0|X=x) < C_{\\mathrm{FN}} P(Y=1|X=x)$$\n\nTo express this inequality in terms of the given class-conditional distributions, we use Bayes' theorem for the posterior probabilities $P(Y=k|X=x)$:\n$$P(Y=k|X=x) = \\frac{p(x|Y=k)P(Y=k)}{p(x)} = \\frac{p(x|Y=k)\\pi_k}{p(x)}$$\nwhere $p(x|Y=k)$ is the class-conditional probability density function (the likelihood), $\\pi_k$ is the prior probability of class $k$, and $p(x)$ is the marginal probability density of $X$ (the evidence).\n\nSubstituting this into the inequality gives:\n$$C_{\\mathrm{FP}} \\frac{p(x|Y=0)\\pi_0}{p(x)} < C_{\\mathrm{FN}} \\frac{p(x|Y=1)\\pi_1}{p(x)}$$\nThe evidence term $p(x)$ is positive and cancels from both sides, yielding a condition on the likelihoods and priors:\n$$C_{\\mathrm{FP}} p(x|Y=0)\\pi_0 < C_{\\mathrm{FN}} p(x|Y=1)\\pi_1$$\nSince we are looking for a rule of the form $X \\ge X^{\\ast}$, and we know sepsis ($Y=1$) corresponds to a higher mean biomarker level ($\\mu_1 > \\mu_0$), we expect to classify as sepsis for larger values of $x$. We can rearrange the inequality into a likelihood ratio test:\n$$\\frac{p(x|Y=1)}{p(x|Y=0)} > \\frac{C_{\\mathrm{FP}}\\pi_0}{C_{\\mathrm{FN}}\\pi_1}$$\n\nThe class-conditional distributions are Gaussian: $X|Y=0 \\sim \\mathcal{N}(\\mu_{0}, \\sigma^{2})$ and $X|Y=1 \\sim \\mathcal{N}(\\mu_{1}, \\sigma^{2})$. Their probability density functions are:\n$$p(x|Y=0) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{(x - \\mu_{0})^2}{2\\sigma^2}\\right)$$\n$$p(x|Y=1) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{(x - \\mu_{1})^2}{2\\sigma^2}\\right)$$\nThe likelihood ratio is:\n$$\\frac{p(x|Y=1)}{p(x|Y=0)} = \\frac{\\exp\\left(-\\frac{(x - \\mu_{1})^2}{2\\sigma^2}\\right)}{\\exp\\left(-\\frac{(x - \\mu_{0})^2}{2\\sigma^2}\\right)} = \\exp\\left(\\frac{(x - \\mu_{0})^2 - (x - \\mu_{1})^2}{2\\sigma^2}\\right)$$\nThe exponent simplifies as:\n$$(x - \\mu_{0})^2 - (x - \\mu_{1})^2 = (x^2 - 2x\\mu_{0} + \\mu_{0}^2) - (x^2 - 2x\\mu_{1} + \\mu_{1}^2) = 2x(\\mu_{1} - \\mu_{0}) - (\\mu_{1}^2 - \\mu_{0}^2)$$\nSubstituting this back, the decision rule is:\n$$\\exp\\left(\\frac{2x(\\mu_{1} - \\mu_{0}) - (\\mu_{1}^2 - \\mu_{0}^2)}{2\\sigma^2}\\right) > \\frac{C_{\\mathrm{FP}}\\pi_0}{C_{\\mathrm{FN}}\\pi_1}$$\nTaking the natural logarithm of both sides, which is a monotonic transformation and preserves the inequality:\n$$\\frac{2x(\\mu_{1} - \\mu_{0}) - (\\mu_{1}^2 - \\mu_{0}^2)}{2\\sigma^2} > \\ln\\left(\\frac{C_{\\mathrm{FP}}\\pi_0}{C_{\\mathrm{FN}}\\pi_1}\\right)$$\nNow, we solve for $x$:\n$$2x(\\mu_{1} - \\mu_{0}) - (\\mu_{1}^2 - \\mu_{0}^2) > 2\\sigma^2 \\ln\\left(\\frac{C_{\\mathrm{FP}}\\pi_0}{C_{\\mathrm{FN}}\\pi_1}\\right)$$\n$$2x(\\mu_{1} - \\mu_{0}) > (\\mu_{1}^2 - \\mu_{0}^2) + 2\\sigma^2 \\ln\\left(\\frac{C_{\\mathrm{FP}}\\pi_0}{C_{\\mathrm{FN}}\\pi_1}\\right)$$\nSince $\\mu_{1} > \\mu_{0}$, $(\\mu_1 - \\mu_0)$ is positive, so dividing by it does not change the inequality direction:\n$$x > \\frac{\\mu_{1}^2 - \\mu_{0}^2}{2(\\mu_{1} - \\mu_{0})} + \\frac{2\\sigma^2}{2(\\mu_{1} - \\mu_{0})} \\ln\\left(\\frac{C_{\\mathrm{FP}}\\pi_0}{C_{\\mathrm{FN}}\\pi_1}\\right)$$\nUsing the identity $\\mu_1^2 - \\mu_0^2 = (\\mu_1 - \\mu_0)(\\mu_1 + \\mu_0)$, we simplify the first term:\n$$x > \\frac{\\mu_{1} + \\mu_{0}}{2} + \\frac{\\sigma^2}{\\mu_{1} - \\mu_{0}} \\ln\\left(\\frac{C_{\\mathrm{FP}}\\pi_0}{C_{\\mathrm{FN}}\\pi_1}\\right)$$\nThis gives the decision rule for $X$. The threshold $X^{\\ast}$ is the value at which the two risks are equal:\n$$X^{\\ast} = \\frac{\\mu_{1} + \\mu_{0}}{2} + \\frac{\\sigma^2}{\\mu_{1} - \\mu_{0}} \\ln\\left(\\frac{C_{\\mathrm{FP}}\\pi_0}{C_{\\mathrm{FN}}\\pi_1}\\right)$$\nWe substitute the given values: $\\mu_{0} = -0.2$, $\\mu_{1} = 0.6$, $\\sigma = 0.5$ (so $\\sigma^2 = 0.25$), $\\pi_{0} = 0.88$, $\\pi_{1} = 0.12$, $C_{\\mathrm{FP}} = 1$, and $C_{\\mathrm{FN}} = 40$.\n$$X^{\\ast} = \\frac{0.6 - 0.2}{2} + \\frac{0.25}{0.6 - (-0.2)} \\ln\\left(\\frac{1 \\times 0.88}{40 \\times 0.12}\\right)$$\n$$X^{\\ast} = \\frac{0.4}{2} + \\frac{0.25}{0.8} \\ln\\left(\\frac{0.88}{4.8}\\right)$$\n$$X^{\\ast} = 0.2 + 0.3125 \\ln\\left(\\frac{11}{60}\\right)$$\nNumerically evaluating this expression:\n$$\\ln\\left(\\frac{11}{60}\\right) \\approx -1.69648$$\n$$X^{\\ast} \\approx 0.2 + 0.3125 \\times (-1.69648) \\approx 0.2 - 0.53015 = -0.33015$$\nThe problem asks for the threshold $B^{\\ast}$ on the original concentration scale. The decision rule is to predict sepsis if $X \\ge X^{\\ast}$. Since $X = \\ln(B)$, this is equivalent to $\\ln(B) \\ge X^{\\ast}$. As the exponential function is monotonically increasing, this becomes $B \\ge \\exp(X^{\\ast})$.\nTherefore, the optimal threshold is $B^{\\ast} = \\exp(X^{\\ast})$.\n$$B^{\\ast} = \\exp(-0.33015)$$\n$$B^{\\ast} \\approx 0.7188236$$\nRounding the final answer to four significant figures gives $0.7188$. The unit is ng/mL.\nThe classifier predicts sepsis if the measured biomarker concentration $B$ is greater than or equal to $0.7188$ ng/mL.",
            "answer": "$$\\boxed{0.7188}$$"
        },
        {
            "introduction": "Modeling dynamic biological processes over time is a key challenge in systems biomedicine. This practice introduces Gaussian Processes (GPs), a flexible and powerful Bayesian framework for function-space inference, perfectly suited for time-series data like cytokine concentrations. By deriving the GP posterior mean and covariance from the foundational rules of conditioning multivariate Gaussian distributions , you will gain insight into how prior beliefs about a function's behavior are systematically updated in light of noisy observations.",
            "id": "4378382",
            "problem": "A systems biomedicine group models the latent time course of a circulating cytokine concentration, denoted by the latent function $f(t)$ measured in arbitrary concentration units, following a controlled immune stimulus at time $t = 0$. The experimental assay produces noisy observations $y_i$ at times $t_i$, modeled as $y_i = f(t_i) + \\epsilon_i$ with independent noise $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma_n^2)$. The prior over $f$ is a Gaussian process (GP), specifically a Gaussian process (GP) with zero mean and squared exponential kernel, that is $f \\sim \\mathcal{GP}(0, k)$ with\n$$\nk(t, t') = \\sigma_f^2 \\exp\\!\\left(-\\frac{(t - t')^2}{2 \\ell^2}\\right).\n$$\nYou may assume the following widely accepted foundational facts as the starting point: (i) a Gaussian process implies that any finite collection of function values is jointly multivariate normal with covariance given by the kernel; (ii) conditioning a jointly multivariate normal vector on an observed subset yields a multivariate normal distribution whose mean and covariance are given by the standard block matrix conditioning rules.\n\nPart A. Using only these foundations and without invoking any pre-stated Gaussian process regression formulas, derive from first principles the posterior mean function $m_{\\text{post}}(t)$ and posterior covariance function $k_{\\text{post}}(t, t')$ of the latent function $f(t)$ given noisy observations $\\{(t_i, y_i)\\}_{i=1}^n$ with independent Gaussian noise of variance $\\sigma_n^2$.\n\nPart B. Consider a specific immuno-monitoring experiment with $n = 2$ observation times $t_1 = 0$ and $t_2 = 2$ days after stimulus. The hyperparameters are $\\sigma_f^2 = 1$ (in squared concentration units) and $\\ell = 1$ day. The observation noise variance is $\\sigma_n^2 = 0.1$ (in squared concentration units). Compute the predictive variance of the latent cytokine concentration at a new time $t_\\star = 1$ day, that is $\\operatorname{Var}[f(t_\\star) \\mid \\{(t_i, y_i)\\}_{i=1}^{2}]$, in squared concentration units. Round your answer to four significant figures. Express the final predictive variance in squared concentration units.",
            "solution": "The problem is found to be valid as it is scientifically grounded, mathematically well-posed, objective, and contains all necessary information for a unique solution.\n\nPart A: Derivation of the Posterior Mean and Covariance Functions\n\nThe problem requires deriving the posterior distribution of the latent function $f(t)$ given a set of noisy observations $\\mathbf{y} = \\{y_i\\}_{i=1}^n$ at times $\\mathbf{t} = \\{t_i\\}_{i=1}^n$. The model is specified as:\n1.  Prior on the latent function: $f \\sim \\mathcal{GP}(0, k)$, where $k(t, t') = \\sigma_f^2 \\exp\\left(-\\frac{(t - t')^2}{2 \\ell^2}\\right)$.\n2.  Likelihood of observations: $y_i = f(t_i) + \\epsilon_i$, where the noise terms $\\epsilon_i$ are independent and identically distributed as $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma_n^2)$.\n\nWe proceed from first principles, using the foundational fact that a Gaussian process implies that any finite set of function values is jointly multivariate normal.\n\nLet $\\mathbf{f} = (f(t_1), \\dots, f(t_n))^T$ be the vector of latent function values at the observation times $\\mathbf{t}$. Let $\\mathbf{y} = (y_1, \\dots, y_n)^T$ be the vector of observations. The observation model can be written in vector form as $\\mathbf{y} = \\mathbf{f} + \\boldsymbol{\\epsilon}$, where $\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma_n^2 I_n)$, with $I_n$ being the $n \\times n$ identity matrix.\n\nOur goal is to find the posterior distribution of the latent function $f$ given the data $(\\mathbf{t}, \\mathbf{y})$. Since the prior is a GP, the posterior is also a GP. A GP is fully specified by its mean and covariance functions. To find these, we determine the predictive distribution for arbitrary test points. Let us consider two arbitrary test points, $t$ and $t'$, and let $f(t)$ and $f(t')$ be the corresponding latent function values.\n\nFirst, we establish the joint distribution of the observations $\\mathbf{y}$ and the latent values $f(t)$ and $f(t')$. According to the GP prior, the collection of random variables $(\\mathbf{f}^T, f(t), f(t'))^T$ is jointly Gaussian with zero mean and a block covariance matrix constructed from the kernel $k$:\n$$\n\\begin{pmatrix} \\mathbf{f} \\\\ f(t) \\\\ f(t') \\end{pmatrix} \\sim \\mathcal{N} \\left( \\mathbf{0}, \\begin{pmatrix} K & \\mathbf{k}(t) & \\mathbf{k}(t') \\\\ \\mathbf{k}(t)^T & k(t,t) & k(t,t') \\\\ \\mathbf{k}(t')^T & k(t',t) & k(t',t') \\end{pmatrix} \\right)\n$$\nwhere $K$ is the $n \\times n$ covariance matrix of the training points, with entries $K_{ij} = k(t_i, t_j)$, and $\\mathbf{k}(t)$ is an $n \\times 1$ column vector with entries $(\\mathbf{k}(t))_i = k(t_i, t)$.\n\nThe observations $\\mathbf{y}$ are a linear transformation of $\\mathbf{f}$ with added independent Gaussian noise. The joint distribution of $(\\mathbf{y}^T, f(t), f(t'))^T$ is therefore also Gaussian. Its mean is $E[(\\mathbf{y}^T, f(t), f(t'))^T] = \\mathbf{0}$. The covariance structure is derived as follows:\n- $\\operatorname{Cov}(\\mathbf{y}) = \\operatorname{Cov}(\\mathbf{f} + \\boldsymbol{\\epsilon}) = \\operatorname{Cov}(\\mathbf{f}) + \\operatorname{Cov}(\\boldsymbol{\\epsilon}) = K + \\sigma_n^2 I_n$, since $\\mathbf{f}$ and $\\boldsymbol{\\epsilon}$ are independent.\n- $\\operatorname{Cov}(\\mathbf{y}, f(t)) = \\operatorname{Cov}(\\mathbf{f} + \\boldsymbol{\\epsilon}, f(t)) = \\operatorname{Cov}(\\mathbf{f}, f(t)) + \\operatorname{Cov}(\\boldsymbol{\\epsilon}, f(t)) = \\mathbf{k}(t)$, since $\\boldsymbol{\\epsilon}$ and $f$ are independent.\n- Similarly, $\\operatorname{Cov}(\\mathbf{y}, f(t')) = \\mathbf{k}(t')$.\n\nThus, the joint distribution of the observations and the values at the test points is:\n$$\n\\begin{pmatrix} \\mathbf{y} \\\\ f(t) \\\\ f(t') \\end{pmatrix} \\sim \\mathcal{N} \\left( \\mathbf{0}, \\begin{pmatrix} K + \\sigma_n^2 I_n & \\mathbf{k}(t) & \\mathbf{k}(t') \\\\ \\mathbf{k}(t)^T & k(t,t) & k(t,t') \\\\ \\mathbf{k}(t')^T & k(t',t) & k(t',t') \\end{pmatrix} \\right)\n$$\nWe wish to find the conditional distribution of $(f(t), f(t'))^T$ given $\\mathbf{y}$. We use the standard formulas for conditioning a multivariate Gaussian distribution. For a general joint Gaussian vector partitioned as $(\\mathbf{x}_1^T, \\mathbf{x}_2^T)^T$ with mean $(\\boldsymbol{\\mu}_1^T, \\boldsymbol{\\mu}_2^T)^T$ and block covariance $\\begin{pmatrix} \\Sigma_{11} & \\Sigma_{12} \\\\ \\Sigma_{21} & \\Sigma_{22} \\end{pmatrix}$, the conditional distribution of $\\mathbf{x}_2$ given $\\mathbf{x}_1$ is Gaussian with:\n- Mean: $\\boldsymbol{\\mu}_{2|1} = \\boldsymbol{\\mu}_2 + \\Sigma_{21} \\Sigma_{11}^{-1} (\\mathbf{x}_1 - \\boldsymbol{\\mu}_1)$\n- Covariance: $\\Sigma_{22|1} = \\Sigma_{22} - \\Sigma_{21} \\Sigma_{11}^{-1} \\Sigma_{12}$\n\nIn our case, we identify:\n- $\\mathbf{x}_1 \\equiv \\mathbf{y}$ and $\\mathbf{x}_2 \\equiv (f(t), f(t'))^T$.\n- All prior means are zero: $\\boldsymbol{\\mu}_1 = \\mathbf{0}$, $\\boldsymbol{\\mu}_2 = \\mathbf{0}$.\n- The covariance blocks are:\n  - $\\Sigma_{11} = K + \\sigma_n^2 I_n$\n  - $\\Sigma_{12} = \\begin{pmatrix} \\mathbf{k}(t) & \\mathbf{k}(t') \\end{pmatrix}$, which implies $\\Sigma_{21} = \\Sigma_{12}^T = \\begin{pmatrix} \\mathbf{k}(t)^T \\\\ \\mathbf{k}(t')^T \\end{pmatrix}$\n  - $\\Sigma_{22} = \\begin{pmatrix} k(t,t) & k(t,t') \\\\ k(t',t) & k(t',t') \\end{pmatrix}$\n\nApplying the conditioning formulas, the posterior distribution of $(f(t), f(t'))^T$ given $\\mathbf{y}$ is Gaussian.\nThe posterior mean vector is:\n$$\nE\\left[ \\begin{pmatrix} f(t) \\\\ f(t') \\end{pmatrix} \\mid \\mathbf{y} \\right] = \\mathbf{0} + \\begin{pmatrix} \\mathbf{k}(t)^T \\\\ \\mathbf{k}(t')^T \\end{pmatrix} (K + \\sigma_n^2 I_n)^{-1} (\\mathbf{y} - \\mathbf{0}) = \\begin{pmatrix} \\mathbf{k}(t)^T (K + \\sigma_n^2 I_n)^{-1} \\mathbf{y} \\\\ \\mathbf{k}(t')^T (K + \\sigma_n^2 I_n)^{-1} \\mathbf{y} \\end{pmatrix}\n$$\nFrom this, we identify the posterior mean function $m_{\\text{post}}(t)$ as the expected value of $f(t)$ given the data:\n$$\nm_{\\text{post}}(t) = \\mathbf{k}(t)^T (K + \\sigma_n^2 I_n)^{-1} \\mathbf{y}\n$$\nThe posterior covariance matrix is:\n$$\n\\operatorname{Cov}\\left( \\begin{pmatrix} f(t) \\\\ f(t') \\end{pmatrix} \\mid \\mathbf{y} \\right) = \\Sigma_{22} - \\Sigma_{21} \\Sigma_{11}^{-1} \\Sigma_{12}\n$$\n$$\n= \\begin{pmatrix} k(t,t) & k(t,t') \\\\ k(t',t) & k(t',t') \\end{pmatrix} - \\begin{pmatrix} \\mathbf{k}(t)^T \\\\ \\mathbf{k}(t')^T \\end{pmatrix} (K + \\sigma_n^2 I_n)^{-1} \\begin{pmatrix} \\mathbf{k}(t) & \\mathbf{k}(t') \\end{pmatrix}\n$$\n$$\n= \\begin{pmatrix} k(t,t) - \\mathbf{k}(t)^T (K + \\sigma_n^2 I_n)^{-1} \\mathbf{k}(t) & k(t,t') - \\mathbf{k}(t)^T (K + \\sigma_n^2 I_n)^{-1} \\mathbf{k}(t') \\\\ k(t',t) - \\mathbf{k}(t')^T (K + \\sigma_n^2 I_n)^{-1} \\mathbf{k}(t) & k(t',t') - \\mathbf{k}(t')^T (K + \\sigma_n^2 I_n)^{-1} \\mathbf{k}(t') \\end{pmatrix}\n$$\nThe posterior covariance function $k_{\\text{post}}(t, t')$ is the off-diagonal element of this matrix:\n$$\nk_{\\text{post}}(t, t') = k(t, t') - \\mathbf{k}(t)^T (K + \\sigma_n^2 I_n)^{-1} \\mathbf{k}(t')\n$$\n\nPart B: Calculation of Predictive Variance\n\nWe are asked to compute the predictive variance of the latent function at $t_\\star = 1$ day, given observations at $t_1 = 0$ and $t_2 = 2$. This quantity is $\\operatorname{Var}[f(t_\\star) \\mid \\mathbf{y}]$, which is given by the posterior covariance function evaluated at $(t_\\star, t_\\star)$:\n$$\n\\operatorname{Var}[f(t_\\star) \\mid \\mathbf{y}] = k_{\\text{post}}(t_\\star, t_\\star) = k(t_\\star, t_\\star) - \\mathbf{k}(t_\\star)^T (K + \\sigma_n^2 I_n)^{-1} \\mathbf{k}(t_\\star)\n$$\nThe given parameters are:\n- Hyperparameters: $\\sigma_f^2 = 1$, $\\ell = 1$ day, $\\sigma_n^2 = 0.1$ squared concentration units.\n- Observation times: $\\mathbf{t} = (t_1, t_2)^T = (0, 2)^T$.\n- Test time: $t_\\star = 1$ day.\n\nThe kernel function is $k(t, t') = \\exp\\left(-\\frac{(t - t')^2}{2}\\right)$. We compute the necessary components:\n1.  The matrix $K$:\n    $K_{11} = k(0, 0) = \\exp(0) = 1$.\n    $K_{22} = k(2, 2) = \\exp(0) = 1$.\n    $K_{12} = K_{21} = k(0, 2) = \\exp\\left(-\\frac{(0-2)^2}{2}\\right) = \\exp(-2)$.\n    So, $K = \\begin{pmatrix} 1 & \\exp(-2) \\\\ \\exp(-2) & 1 \\end{pmatrix}$.\n\n2.  The vector $\\mathbf{k}(t_\\star)$:\n    $(\\mathbf{k}(t_\\star))_1 = k(t_1, t_\\star) = k(0, 1) = \\exp\\left(-\\frac{(0-1)^2}{2}\\right) = \\exp(-0.5)$.\n    $(\\mathbf{k}(t_\\star))_2 = k(t_2, t_\\star) = k(2, 1) = \\exp\\left(-\\frac{(2-1)^2}{2}\\right) = \\exp(-0.5)$.\n    So, $\\mathbf{k}(t_\\star) = \\begin{pmatrix} \\exp(-0.5) \\\\ \\exp(-0.5) \\end{pmatrix}$.\n\n3.  The scalar $k(t_\\star, t_\\star)$:\n    $k(t_\\star, t_\\star) = k(1, 1) = \\exp(0) = 1$.\n\nNow, we assemble the matrix to be inverted, $C = K + \\sigma_n^2 I_2$:\n$$\nC = \\begin{pmatrix} 1 & \\exp(-2) \\\\ \\exp(-2) & 1 \\end{pmatrix} + 0.1 \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1.1 & \\exp(-2) \\\\ \\exp(-2) & 1.1 \\end{pmatrix}\n$$\nThe inverse of a $2 \\times 2$ matrix $\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$ is $\\frac{1}{ad-bc}\\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix}$.\n$$\nC^{-1} = \\frac{1}{(1.1)^2 - (\\exp(-2))^2} \\begin{pmatrix} 1.1 & -\\exp(-2) \\\\ -\\exp(-2) & 1.1 \\end{pmatrix} = \\frac{1}{1.21 - \\exp(-4)} \\begin{pmatrix} 1.1 & -\\exp(-2) \\\\ -\\exp(-2) & 1.1 \\end{pmatrix}\n$$\nWe need to compute the quadratic form $\\mathbf{k}(t_\\star)^T C^{-1} \\mathbf{k}(t_\\star)$:\n$$\n\\mathbf{k}(t_\\star)^T C^{-1} \\mathbf{k}(t_\\star) = \\begin{pmatrix} \\exp(-0.5) & \\exp(-0.5) \\end{pmatrix} C^{-1} \\begin{pmatrix} \\exp(-0.5) \\\\ \\exp(-0.5) \\end{pmatrix}\n$$\n$$\n= \\exp(-1) \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\frac{1}{1.21 - \\exp(-4)} \\begin{pmatrix} 1.1 & -\\exp(-2) \\\\ -\\exp(-2) & 1.1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n$$\n$$\n= \\frac{\\exp(-1)}{1.21 - \\exp(-4)} \\begin{pmatrix} 1.1 - \\exp(-2) & 1.1 - \\exp(-2) \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n$$\n$$\n= \\frac{\\exp(-1)}{1.21 - \\exp(-4)} (1.1 - \\exp(-2) + 1.1 - \\exp(-2)) = \\frac{2 \\exp(-1) (1.1 - \\exp(-2))}{1.21 - \\exp(-4)}\n$$\nUsing the difference of squares factorization $a^2 - b^2 = (a-b)(a+b)$, where $a=1.1$ and $b=\\exp(-2)$, we have $1.21 - \\exp(-4) = (1.1 - \\exp(-2))(1.1 + \\exp(-2))$. This simplifies the expression:\n$$\n\\mathbf{k}(t_\\star)^T C^{-1} \\mathbf{k}(t_\\star) = \\frac{2 \\exp(-1) (1.1 - \\exp(-2))}{(1.1 - \\exp(-2))(1.1 + \\exp(-2))} = \\frac{2 \\exp(-1)}{1.1 + \\exp(-2)}\n$$\nFinally, the predictive variance is:\n$$\n\\operatorname{Var}[f(t_\\star) \\mid \\mathbf{y}] = 1 - \\frac{2 \\exp(-1)}{1.1 + \\exp(-2)}\n$$\nNow we compute the numerical value:\n$$\n\\operatorname{Var}[f(t_\\star) \\mid \\mathbf{y}] = 1 - \\frac{2 \\times 0.36787944...}{1.1 + 0.13533528...} = 1 - \\frac{0.73575888...}{1.23533528...} \\approx 1 - 0.59558005... = 0.40441994...\n$$\nRounding to four significant figures, the predictive variance is $0.4044$ in squared concentration units.",
            "answer": "$$\n\\boxed{0.4044}\n$$"
        }
    ]
}