{
    "hands_on_practices": [
        {
            "introduction": "This first practice is a foundational exercise in probability theory, focusing on the normal distribution, which is central to modeling noise and biological variability in countless biomedical applications. By deriving the moment generating function (MGF), $M_X(t)$, from scratch, you will gain a deeper appreciation for the mathematical properties that make the normal distribution so tractable. This exercise will hone your skills in calculus and demonstrate how a single, powerful function can be used to systematically derive all moments of a distribution, such as its variance and kurtosis .",
            "id": "4387180",
            "problem": "In a systems biomedicine assay, the residual measurement error of a calibrated quantitative biomarker readout is well modeled by a normal random variable $X$ with mean $\\mu$ and variance $\\sigma^{2}$. The probability density function of $X$ is given by\n$$\nf_{X}(x)=\\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\!\\left(-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}\\right).\n$$\nStarting from the definition of the moment generating function (MGF),\n$$\nM_{X}(t)=\\mathbb{E}\\!\\left[\\exp(tX)\\right]=\\int_{-\\infty}^{\\infty}\\exp(tx)\\,f_{X}(x)\\,dx,\n$$\nderive a closed-form expression for $M_{X}(t)$ that is valid for all real $t$. Then, using only first principles (the definition above and standard theorems justifying differentiation under the integral sign), justify how repeated differentiation of $M_{X}(t)$ at $t=0$ yields the raw moments $\\mathbb{E}[X^{n}]$ for any nonnegative integer $n$. Finally, apply this framework to the centered variable $Y=X-\\mu$ and compute the fourth central moment $\\mathbb{E}\\!\\left[(X-\\mu)^{4}\\right]$ explicitly by differentiating the appropriate MGF. \n\nProvide the final answer as a single closed-form analytic expression in terms of $\\sigma$. Do not round.",
            "solution": "The problem is valid as it is a standard, well-posed problem in probability theory and statistics, grounded in correct mathematical and scientific principles. We will proceed with the derivation in three parts as requested.\n\nPart 1: Derivation of the Moment Generating Function (MGF) of a Normal Random Variable\n\nThe problem defines a normal random variable $X$ with mean $\\mu$ and variance $\\sigma^2$, having the probability density function (PDF):\n$$\nf_{X}(x)=\\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left(-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}\\right)\n$$\nThe moment generating function (MGF), $M_X(t)$, is defined as the expectation of $\\exp(tX)$:\n$$\nM_{X}(t) = \\mathbb{E}[\\exp(tX)] = \\int_{-\\infty}^{\\infty} \\exp(tx) f_{X}(x) dx\n$$\nSubstituting the PDF of the normal distribution into this definition, we have:\n$$\nM_{X}(t) = \\int_{-\\infty}^{\\infty} \\exp(tx) \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}\\right) dx\n$$\nWe can combine the arguments of the exponential functions:\n$$\nM_{X}(t) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} \\exp\\left(tx - \\frac{(x-\\mu)^{2}}{2\\sigma^{2}}\\right) dx\n$$\nTo solve this integral, we complete the square in the exponent. Let's focus on the argument of the exponential:\n$$\ntx - \\frac{(x-\\mu)^{2}}{2\\sigma^{2}} = tx - \\frac{x^2 - 2\\mu x + \\mu^2}{2\\sigma^2} = \\frac{2\\sigma^2 tx - (x^2 - 2\\mu x + \\mu^2)}{2\\sigma^2}\n$$\n$$\n= -\\frac{1}{2\\sigma^2} \\left[ x^2 - 2\\mu x - 2\\sigma^2 tx + \\mu^2 \\right] = -\\frac{1}{2\\sigma^2} \\left[ x^2 - 2(\\mu + \\sigma^2 t)x + \\mu^2 \\right]\n$$\nNow, we complete the square for the terms involving $x$. We add and subtract $(\\mu + \\sigma^2 t)^2$:\n$$\n= -\\frac{1}{2\\sigma^2} \\left[ \\left(x^2 - 2(\\mu + \\sigma^2 t)x + (\\mu + \\sigma^2 t)^2\\right) - (\\mu + \\sigma^2 t)^2 + \\mu^2 \\right]\n$$\n$$\n= -\\frac{1}{2\\sigma^2} \\left[ (x - (\\mu + \\sigma^2 t))^2 - (\\mu^2 + 2\\mu\\sigma^2 t + \\sigma^4 t^2) + \\mu^2 \\right]\n$$\n$$\n= -\\frac{(x - (\\mu + \\sigma^2 t))^2}{2\\sigma^2} + \\frac{2\\mu\\sigma^2 t + \\sigma^4 t^2}{2\\sigma^2} = -\\frac{(x - (\\mu + \\sigma^2 t))^2}{2\\sigma^2} + \\mu t + \\frac{1}{2}\\sigma^2 t^2\n$$\nSubstituting this back into the integral for $M_X(t)$:\n$$\nM_{X}(t) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} \\exp\\left( -\\frac{(x - (\\mu + \\sigma^2 t))^2}{2\\sigma^2} + \\mu t + \\frac{1}{2}\\sigma^2 t^2 \\right) dx\n$$\nThe term $\\exp(\\mu t + \\frac{1}{2}\\sigma^2 t^2)$ does not depend on $x$ and can be factored out of the integral:\n$$\nM_{X}(t) = \\exp\\left(\\mu t + \\frac{1}{2}\\sigma^2 t^2\\right) \\int_{-\\infty}^{\\infty} \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left( -\\frac{(x - (\\mu + \\sigma^2 t))^2}{2\\sigma^2} \\right) dx\n$$\nThe remaining integral is the integral of a normal PDF with mean $\\mu' = \\mu + \\sigma^2 t$ and variance $\\sigma^2$ over its entire domain $(-\\infty, \\infty)$. The total area under any PDF is equal to $1$. Therefore, the integral evaluates to $1$.\nThis leaves us with the closed-form expression for the MGF of a normal distribution:\n$$\nM_{X}(t) = \\exp\\left(\\mu t + \\frac{1}{2}\\sigma^2 t^2\\right)\n$$\nThis expression is valid for all real values of $t$.\n\nPart 2: Justification of Moment Generation via Differentiation\n\nThe $n$-th raw moment of $X$ is defined as $\\mathbb{E}[X^n]$. We are asked to show how this relates to the derivatives of $M_X(t)$ at $t=0$. Let's differentiate $M_X(t)$ with respect to $t$:\n$$\n\\frac{d^n}{dt^n} M_X(t) = \\frac{d^n}{dt^n} \\int_{-\\infty}^{\\infty} \\exp(tx) f_X(x) dx\n$$\nUnder certain regularity conditions, we can interchange the order of differentiation and integration (a result formally justified by theorems such as the Leibniz integral rule or the Dominated Convergence Theorem). For the normal distribution, the integrand's partial derivatives with respect to $t$ exist and are continuous. Furthermore, for any $t$ in a compact interval around $0$, the integral of the absolute value of the derivative is bounded, because the exponential decay of $f_X(x)$ as $|x| \\to \\infty$ dominates any polynomial growth from the differentiation. This justifies the interchange.\n$$\n\\frac{d^n}{dt^n} M_X(t) = \\int_{-\\infty}^{\\infty} \\frac{\\partial^n}{\\partial t^n} [\\exp(tx)] f_X(x) dx\n$$\nThe $n$-th partial derivative of $\\exp(tx)$ with respect to $t$ is $x^n \\exp(tx)$.\n$$\n\\frac{d^n}{dt^n} M_X(t) = \\int_{-\\infty}^{\\infty} x^n \\exp(tx) f_X(x) dx = \\mathbb{E}[X^n \\exp(tX)]\n$$\nTo find the $n$-th raw moment, we evaluate this expression at $t=0$:\n$$\n\\left. \\frac{d^n}{dt^n} M_X(t) \\right|_{t=0} = \\mathbb{E}[X^n \\exp(0 \\cdot X)] = \\mathbb{E}[X^n \\cdot 1] = \\mathbb{E}[X^n]\n$$\nThis demonstrates that the $n$-th raw moment $\\mathbb{E}[X^n]$ can be obtained by taking the $n$-th derivative of the MGF $M_X(t)$ and evaluating it at $t=0$.\n\nPart 3: Calculation of the Fourth Central Moment\n\nThe fourth central moment is $\\mathbb{E}[(X-\\mu)^4]$. Let's define a centered random variable $Y = X-\\mu$. The moments of $Y$ are the central moments of $X$. We need to find the MGF of $Y$, denoted $M_Y(t)$.\n$$\nM_Y(t) = \\mathbb{E}[\\exp(tY)] = \\mathbb{E}[\\exp(t(X-\\mu))] = \\mathbb{E}[\\exp(tX)\\exp(-t\\mu)]\n$$\nSince $\\exp(-t\\mu)$ is a constant with respect to the random variable $X$, we can factor it out of the expectation:\n$$\nM_Y(t) = \\exp(-t\\mu) \\mathbb{E}[\\exp(tX)] = \\exp(-t\\mu) M_X(t)\n$$\nSubstituting the expression for $M_X(t)$ we derived in Part 1:\n$$\nM_Y(t) = \\exp(-t\\mu) \\exp\\left(\\mu t + \\frac{1}{2}\\sigma^2 t^2\\right) = \\exp\\left(-t\\mu + \\mu t + \\frac{1}{2}\\sigma^2 t^2\\right) = \\exp\\left(\\frac{1}{2}\\sigma^2 t^2\\right)\n$$\nNote that $Y$ is a normal random variable with mean $\\mathbb{E}[Y] = \\mathbb{E}[X-\\mu] = \\mu-\\mu=0$ and variance $\\mathrm{Var}(Y) = \\mathrm{Var}(X) = \\sigma^2$. The MGF we found for $Y$ is consistent with the general formula for a normal distribution with mean $0$ and variance $\\sigma^2$.\n\nTo find the fourth central moment, $\\mathbb{E}[Y^4]$, we must compute the fourth derivative of $M_Y(t)$ and evaluate it at $t=0$.\n$$\nM_Y(t) = \\exp\\left(\\frac{1}{2}\\sigma^2 t^2\\right)\n$$\nFirst derivative:\n$$\nM_Y'(t) = \\frac{d}{dt}\\exp\\left(\\frac{1}{2}\\sigma^2 t^2\\right) = (\\sigma^2 t)\\exp\\left(\\frac{1}{2}\\sigma^2 t^2\\right)\n$$\nSecond derivative (using the product rule):\n$$\nM_Y''(t) = \\sigma^2 \\exp\\left(\\frac{1}{2}\\sigma^2 t^2\\right) + (\\sigma^2 t)(\\sigma^2 t)\\exp\\left(\\frac{1}{2}\\sigma^2 t^2\\right) = (\\sigma^2 + \\sigma^4 t^2)\\exp\\left(\\frac{1}{2}\\sigma^2 t^2\\right)\n$$\nThird derivative:\n$$\nM_Y'''(t) = (2\\sigma^4 t)\\exp\\left(\\frac{1}{2}\\sigma^2 t^2\\right) + (\\sigma^2 + \\sigma^4 t^2)(\\sigma^2 t)\\exp\\left(\\frac{1}{2}\\sigma^2 t^2\\right)\n$$\n$$\n= (2\\sigma^4 t + \\sigma^4 t + \\sigma^6 t^3)\\exp\\left(\\frac{1}{2}\\sigma^2 t^2\\right) = (3\\sigma^4 t + \\sigma^6 t^3)\\exp\\left(\\frac{1}{2}\\sigma^2 t^2\\right)\n$$\nFourth derivative:\n$$\nM_Y^{(4)}(t) = (3\\sigma^4 + 3\\sigma^6 t^2)\\exp\\left(\\frac{1}{2}\\sigma^2 t^2\\right) + (3\\sigma^4 t + \\sigma^6 t^3)(\\sigma^2 t)\\exp\\left(\\frac{1}{2}\\sigma^2 t^2\\right)\n$$\n$$\n= (3\\sigma^4 + 3\\sigma^6 t^2 + 3\\sigma^6 t^2 + \\sigma^8 t^4)\\exp\\left(\\frac{1}{2}\\sigma^2 t^2\\right)\n$$\n$$\n= (3\\sigma^4 + 6\\sigma^6 t^2 + \\sigma^8 t^4)\\exp\\left(\\frac{1}{2}\\sigma^2 t^2\\right)\n$$\nFinally, we evaluate the fourth derivative at $t=0$ to obtain the fourth central moment:\n$$\n\\mathbb{E}[(X-\\mu)^4] = \\mathbb{E}[Y^4] = M_Y^{(4)}(0) = (3\\sigma^4 + 6\\sigma^6(0)^2 + \\sigma^8(0)^4)\\exp(0) = 3\\sigma^4\n$$\nThe fourth central moment is $3\\sigma^4$.",
            "answer": "$$\n\\boxed{3\\sigma^{4}}\n$$"
        },
        {
            "introduction": "In practice, we often analyze not the raw data itself, but transformations of itâ€”for instance, taking the logarithm of concentration data to stabilize variance. This exercise introduces the Delta Method, a vital tool for approximating the statistical properties of such transformed variables. By using a first-order Taylor expansion, you will derive an asymptotic approximation for the variance of a function of a sample mean, a technique essential for constructing confidence intervals and understanding the behavior of complex estimators in systems biology .",
            "id": "4387137",
            "problem": "In a systems biomedicine study of secreted cytokine concentration, suppose $X_{1}, X_{2}, \\ldots, X_{n}$ are independent and identically distributed measurements of concentration from $n$ distinct cell cultures, modeled as $X_{i} \\sim \\mathcal{N}(\\mu, \\sigma^{2})$ with $\\mu > 0$ to reflect strictly positive concentration. Researchers often report log-transformed summaries to linearize multiplicative variability and stabilize variance. Consider the estimator $\\hat{\\theta} = g(\\bar{X})$ for a once continuously differentiable function $g$ in a neighborhood of $\\mu$, where $\\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_{i}$.\n\nStarting only from the definition of the sample mean, its variance under independence, and the Central Limit Theorem (CLT), derive an asymptotic approximation for $\\operatorname{Var}(\\hat{\\theta})$ valid for large $n$ by linearizing $g$ around $\\mu$ using a first-order expansion. Then specialize your derivation to the case $g(x) = \\ln(x)$, and provide the resulting closed-form analytic expression for the large-$n$ approximation to $\\operatorname{Var}(\\ln(\\bar{X}))$ in terms of $\\mu$, $\\sigma^{2}$, and $n$.\n\nExpress the final answer as a single analytic expression. Do not round.",
            "solution": "The user wants to find an asymptotic approximation for the variance of a function of the sample mean, $\\operatorname{Var}(g(\\bar{X}))$, and then specialize this result for the natural logarithm function. The derivation must start from first principles as outlined in the problem statement.\n\nThe problem is deemed valid.\n- It is scientifically grounded in standard statistical theory (Central Limit Theorem, Delta Method).\n- It is well-posed, with all necessary information provided.\n- The language is objective and precise.\n\nWe begin the derivation as requested.\n\nLet $X_{1}, X_{2}, \\ldots, X_{n}$ be independent and identically distributed (i.i.d.) random variables from a distribution with mean $\\mathbb{E}[X_{i}] = \\mu$ and variance $\\operatorname{Var}(X_{i}) = \\sigma^{2}$. The problem specifies a normal distribution, $X_{i} \\sim \\mathcal{N}(\\mu, \\sigma^{2})$, but this derivation holds more generally. The sample mean is defined as $\\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_{i}$.\n\nThe variance of the sample mean, $\\operatorname{Var}(\\bar{X})$, is derived using the properties of variance. Since the $X_{i}$ are independent:\n$$\n\\operatorname{Var}(\\bar{X}) = \\operatorname{Var}\\left(\\frac{1}{n}\\sum_{i=1}^{n} X_{i}\\right) = \\frac{1}{n^{2}} \\operatorname{Var}\\left(\\sum_{i=1}^{n} X_{i}\\right)\n$$\nDue to independence, the variance of the sum is the sum of the variances:\n$$\n\\operatorname{Var}\\left(\\sum_{i=1}^{n} X_{i}\\right) = \\sum_{i=1}^{n} \\operatorname{Var}(X_{i}) = \\sum_{i=1}^{n} \\sigma^{2} = n\\sigma^{2}\n$$\nSubstituting this back, we obtain the variance of the sample mean:\n$$\n\\operatorname{Var}(\\bar{X}) = \\frac{1}{n^{2}}(n\\sigma^{2}) = \\frac{\\sigma^{2}}{n}\n$$\nThe problem specifies that $n$ is large. The Central Limit Theorem (CLT) states that as $n \\to \\infty$, the distribution of the sample mean $\\bar{X}$ approaches a normal distribution. More formally, $\\sqrt{n}(\\bar{X} - \\mu)$ converges in distribution to $\\mathcal{N}(0, \\sigma^{2})$. This implies that for large $n$, $\\bar{X}$ is concentrated in a small neighborhood around its mean $\\mu$. This concentration justifies using a linear approximation for the function $g$ around the point $\\mu$.\n\nWe are interested in the estimator $\\hat{\\theta} = g(\\bar{X})$, where $g$ is a once continuously differentiable function. We perform a first-order Taylor expansion of $g(\\bar{X})$ around the point $\\mu$:\n$$\ng(\\bar{X}) \\approx g(\\mu) + g'(\\mu)(\\bar{X} - \\mu)\n$$\nThis is an approximation where we have neglected higher-order terms, which is valid for large $n$ because $\\bar{X}$ is close to $\\mu$. Now we compute the variance of this linear approximation:\n$$\n\\operatorname{Var}(\\hat{\\theta}) = \\operatorname{Var}(g(\\bar{X})) \\approx \\operatorname{Var}(g(\\mu) + g'(\\mu)(\\bar{X} - \\mu))\n$$\nUsing the property that for any random variable $Y$ and constants $a, b$, we have $\\operatorname{Var}(aY + b) = a^{2}\\operatorname{Var}(Y)$. Here, $g(\\mu)$ and $g'(\\mu)$ are constants, as is $\\mu$. Let $Y = \\bar{X}$, $a = g'(\\mu)$, and the constant term be $g(\\mu) - g'(\\mu)\\mu$. The expression simplifies to:\n$$\n\\operatorname{Var}(g(\\bar{X})) \\approx \\operatorname{Var}(g'(\\mu)\\bar{X}) = (g'(\\mu))^{2} \\operatorname{Var}(\\bar{X})\n$$\nSubstituting the previously derived expression for $\\operatorname{Var}(\\bar{X}) = \\frac{\\sigma^{2}}{n}$, we arrive at the general asymptotic approximation for the variance of $\\hat{\\theta}$:\n$$\n\\operatorname{Var}(g(\\bar{X})) \\approx (g'(\\mu))^{2} \\frac{\\sigma^{2}}{n}\n$$\nThis result is often referred to as the Delta Method.\n\nNext, we specialize this derivation to the case where the function is the natural logarithm, $g(x) = \\ln(x)$. The problem states that $\\mu > 0$, which ensures that $\\ln(\\mu)$ is well-defined. For large $n$, $\\bar{X}$ will be close to $\\mu$, so $\\bar{X}$ will also be positive with high probability, and thus $\\ln(\\bar{X})$ will be well-defined.\n\nFirst, we find the derivative of $g(x)$:\n$$\ng'(x) = \\frac{d}{dx}(\\ln(x)) = \\frac{1}{x}\n$$\nNext, we evaluate this derivative at the point $x=\\mu$:\n$$\ng'(\\mu) = \\frac{1}{\\mu}\n$$\nNow, we substitute this specific derivative into our general variance approximation formula:\n$$\n\\operatorname{Var}(\\ln(\\bar{X})) \\approx \\left(\\frac{1}{\\mu}\\right)^{2} \\operatorname{Var}(\\bar{X}) = \\frac{1}{\\mu^{2}} \\frac{\\sigma^{2}}{n}\n$$\nThis gives the final closed-form analytic expression for the large-$n$ approximation to $\\operatorname{Var}(\\ln(\\bar{X}))$:\n$$\n\\operatorname{Var}(\\ln(\\bar{X})) \\approx \\frac{\\sigma^{2}}{n\\mu^{2}}\n$$",
            "answer": "$$\n\\boxed{\\frac{\\sigma^{2}}{n\\mu^{2}}}\n$$"
        },
        {
            "introduction": "While many statistical tests rely on assumptions about the underlying data distribution, non-parametric methods offer a robust alternative when these assumptions are questionable. This hands-on coding practice challenges you to build a permutation test from its fundamental principles: exchangeability and randomization. By enumerating all possible labelings for small sample sizes, you will construct the exact null distribution for a difference-in-means test, providing a powerful and intuitive understanding of hypothesis testing that does not depend on asymptotic theory .",
            "id": "4387115",
            "problem": "Consider a two-sample comparison problem arising in systems biomedicine, where the scientific question is whether two biological conditions (for example, control versus treatment) yield different average molecular measurements (for example, log-transformed single-cell protein abundance). You are tasked with developing a rigorous permutation test for the difference in means under the assumption of exchangeability of condition labels and, for small samples, deriving the exact null distribution by enumerating all labelings consistent with the observed sample sizes.\n\nBase your derivation on the following principles:\n- Exchangeability under the null hypothesis: under the null that the two conditions have the same distribution of measurements, observed labels are uninformative, and any reassignment of labels that preserves the original group sizes is equally likely.\n- The randomization principle for hypothesis testing: the reference null distribution of a test statistic is constructed by evaluating the statistic under all labelings admissible by the experimental design, treating these as equally likely under the null.\n- The definition of the difference in means as a test statistic: for pooled observations, the statistic is the difference between the average of measurements assigned to group $A$ and the average of measurements assigned to group $B$.\n\nFormally, suppose you observe $n_A$ measurements in group $A$ and $n_B$ measurements in group $B$, with pooled data $x_1, x_2, \\dots, x_N$ where $N = n_A + n_B$. Under the null hypothesis and exchangeability of labels, all labelings that assign exactly $n_A$ of the pooled observations to $A$ and $n_B$ to $B$ are equally likely. Define the test statistic $T$ as the difference in means $T = \\bar{X}_A - \\bar{X}_B$. The exact null distribution for small samples is the multiset of all values of $T$ obtained by enumerating all labelings, equipped with the uniform probability over labelings. The two-sided $p$-value is the probability, under this null distribution, of observing a statistic with absolute value at least as large as the observed statistic from the original labels.\n\nYour program must:\n- Compute the observed difference in means $T_{\\text{obs}}$ from the provided labeled data.\n- Enumerate all admissible labelings (that preserve the original group sizes) and compute the exact null distribution of $T$ by counting the frequency of each unique value of $T$ and dividing by the total number of labelings.\n- Compute the exact two-sided $p$-value as the total null probability of $|T| \\geq |T_{\\text{obs}}|$, expressed as a decimal.\n- For each test case, output a list containing three components: the two-sided $p$-value, the sorted list of unique support points of the exact null distribution, and the corresponding list of probabilities aligned to this support. All floating-point outputs must be rounded to six decimal places and expressed as decimals (not fractions and not using the percentage sign).\n\nImplement this as a complete, runnable program that produces results for the following test suite:\n- Test case $1$ (balanced, moderate difference): group $A$ measurements $[1.24, 1.37, 1.51, 1.63]$, group $B$ measurements $[1.10, 1.21, 1.28, 1.30]$.\n- Test case $2$ (boundary, smallest nontrivial sample): group $A$ measurements $[2.80]$, group $B$ measurements $[2.10]$.\n- Test case $3$ (unequal group sizes): group $A$ measurements $[0.95, 1.02, 1.08]$, group $B$ measurements $[0.89, 0.92, 1.00, 1.05, 1.10]$.\n- Test case $4$ (balanced with repeated values): group $A$ measurements $[3.20, 3.20, 3.40]$, group $B$ measurements $[3.10, 3.20, 3.30]$.\n- Test case $5$ (degenerate equal values): group $A$ measurements $[1.50, 1.50]$, group $B$ measurements $[1.50, 1.50]$.\n\nYour program should produce a single line of output containing the results for all five test cases as a comma-separated list enclosed in square brackets. Each test case result must itself be a list of the form $[p,\\;S,\\;P]$, where $p$ is the two-sided $p$-value, $S$ is the sorted list of unique support points of the exact null distribution, and $P$ is the list of probabilities aligned to $S$. For example, your output must have the shape $[[p_1,S_1,P_1],[p_2,S_2,P_2],[p_3,S_3,P_3],[p_4,S_4,P_4],[p_5,S_5,P_5]]$ with all floats rounded to six decimals and expressed as decimals.\n\nNo physical units or angle units apply. All probabilities and $p$-values must be decimals (not fractions and not percentages).",
            "solution": "### Solution\n\nThe task is to implement an exact permutation test for the difference in means between two samples, group $A$ and group $B$. The core of the solution rests on the randomization principle, which posits that under the null hypothesis ($H_0$) of no difference between the groups, the labels 'A' and 'B' are arbitrary. Thus, any partition of the pooled data into new groups of the original sizes $n_A$ and $n_B$ is equally likely.\n\nLet the observed data be $X_A = \\{x_{A,1}, \\dots, x_{A,n_A}\\}$ and $X_B = \\{x_{B,1}, \\dots, x_{B,n_B}\\}$. The total number of observations is $N = n_A + n_B$. The pooled dataset is $X_{pool} = X_A \\cup X_B$.\n\n**1. Observed Test Statistic**\n\nThe test statistic is the difference in the sample means, $T = \\bar{X}_A - \\bar{X}_B$. We first compute this value for the original, observed labeling. This is denoted as $T_{\\text{obs}}$.\n$$\nT_{\\text{obs}} = \\frac{1}{n_A} \\sum_{i=1}^{n_A} x_{A,i} - \\frac{1}{n_B} \\sum_{j=1}^{n_B} x_{B,j}\n$$\n\n**2. Constructing the Exact Null Distribution**\n\nUnder $H_0$, all possible ways of assigning $n_A$ labels of 'A' and $n_B$ labels of 'B' to the $N$ observations in $X_{pool}$ are equally probable. The total number of such unique assignments, or permutations of labels, is given by the binomial coefficient:\n$$\nC_{total} = \\binom{N}{n_A} = \\frac{N!}{n_A! (N-n_A)!} = \\frac{N!}{n_A! n_B!}\n$$\nThe algorithm proceeds by enumerating every possible way to form a new group $A'$ of size $n_A$ from the pooled data $X_{pool}$. The remaining $n_B$ elements automatically form the complementary group $B'$. For each such permutation $k = 1, \\dots, C_{total}$, we calculate the test statistic $T_k'$:\n$$\nT_k' = \\bar{X}_{A',k} - \\bar{X}_{B',k}\n$$\nThe collection of these $C_{total}$ values $\\{T_1', T_2', \\dots, T_{C_{total}}'\\}$ constitutes the exact null distribution of the test statistic $T$.\n\nA computational note: let $S_{pool} = \\sum_{i=1}^{N} x_i$ be the sum of all pooled observations. For any permutation, if the sum of elements in group $A'$ is $S_{A'}$, then the sum for group $B'$ is $S_{B'} = S_{pool} - S_{A'}$. The statistic $T'$ can be written as:\n$$\nT' = \\frac{S_{A'}}{n_A} - \\frac{S_{pool} - S_{A'}}{n_B} = S_{A'} \\left( \\frac{1}{n_A} + \\frac{1}{n_B} \\right) - \\frac{S_{pool}}{n_B}\n$$\nThis shows that $T'$ is a monotonic linear function of $S_{A'}$. The distribution can be derived by first finding the distribution of the sums of all $n_A$-element subsets, and then transforming those sums to the corresponding difference-in-means values. In implementation, direct calculation of means is clear and efficient enough for the given constraints.\n\n**3. Computing Support and Probabilities**\n\nThe multiset of $T'$ values is processed to find the support of the null distribution, $S$, which is the set of unique values $\\{t_1, t_2, \\dots, t_m\\}$. We sort these values in ascending order. For each unique value $t_j \\in S$, we find its frequency, $f_j$, in the multiset of all $T'$ values. The probability of observing $t_j$ under the null hypothesis is:\n$$\nP(T = t_j) = \\frac{f_j}{C_{total}}\n$$\nThis yields the list of probabilities $P = \\{P(T=t_1), \\dots, P(T=t_m)\\}$ aligned with the sorted support $S$.\n\n**4. Two-Sided $p$-value Calculation**\n\nThe two-sided $p$-value is the probability, under the null distribution, of obtaining a test statistic at least as extreme as the one observed. \"As extreme\" is measured by the absolute value. The $p$-value is calculated as:\n$$\np = P(|T| \\ge |T_{\\text{obs}}|) = \\frac{\\text{Number of permutations } k \\text{ where } |T_k'| \\ge |T_{\\text{obs}}|}{C_{total}}\n$$\nDue to potential floating-point inaccuracies, the comparison $|T_k'| \\ge |T_{\\text{obs}}|$ should be implemented carefully, for instance, by comparing up to a high degree of precision, e.g., $|T_k'| - |T_{\\text{obs}}| \\ge -\\epsilon$ for a small $\\epsilon>0$. Given that the values are derived from rational arithmetic, direct comparison after rounding to a safe number of decimal places (e.g., 12) is a robust strategy.\n\nThe algorithm to be implemented will perform these steps for each test case provided. The use of `itertools.combinations` in Python is ideal for enumerating the partitions of the pooled data.\n```python\nimport numpy as np\nfrom itertools import combinations\nimport math\n\ndef run_permutation_test(data_a, data_b):\n    \"\"\"\n    Performs an exact two-sample permutation test for the difference in means.\n\n    Args:\n        data_a (list): A list of numerical measurements for group A.\n        data_b (list): A list of numerical measurements for group B.\n\n    Returns:\n        list: A list containing [p_value, support, probabilities], where\n              p_value is the two-sided p-value,\n              support is a sorted list of unique values in the null distribution,\n              and probabilities is the list of corresponding probabilities.\n    \"\"\"\n    n_a = len(data_a)\n    n_b = len(data_b)\n    N = n_a + n_b\n\n    # Convert to numpy arrays for vectorized operations\n    data_a = np.array(data_a)\n    data_b = np.array(data_b)\n    \n    # 1. Pool the data\n    pooled_data = np.concatenate((data_a, data_b))\n\n    # 2. Calculate the observed test statistic\n    mean_a_obs = np.mean(data_a) if n_a > 0 else 0.0\n    mean_b_obs = np.mean(data_b) if n_b > 0 else 0.0\n    T_obs = mean_a_obs - mean_b_obs\n    \n    # Rounding for consistent floating point comparisons later\n    # A high precision is used internally to avoid collapsing distinct but close values.\n    precision = 12\n    T_obs_rounded = round(T_obs, precision)\n\n    # 3. Generate the null distribution by enumerating all permutations\n    null_distribution_T_values = []\n    \n    # Each combination represents a possible group A\n    for indices_a in combinations(range(N), n_a):\n        indices_a = np.array(indices_a)\n        \n        # Create permuted groups A and B using indices\n        perm_a_data = pooled_data[indices_a]\n        \n        # The remaining indices form group B\n        mask = np.ones(N, dtype=bool)\n        mask[indices_a] = False\n        perm_b_data = pooled_data[mask]\n        \n        # Calculate the test statistic for this permutation\n        mean_a_perm = np.mean(perm_a_data) if n_a > 0 else 0.0\n        mean_b_perm = np.mean(perm_b_data) if n_b > 0 else 0.0\n        T_perm = mean_a_perm - mean_b_perm\n        \n        null_distribution_T_values.append(round(T_perm, precision))\n\n    null_distribution_T_values = np.array(null_distribution_T_values)\n    total_permutations = len(null_distribution_T_values)\n\n    # 4. Calculate the two-sided p-value\n    # Count permutations where |T_perm| is >= |T_obs|\n    # Use a small tolerance for floating point comparison robustness\n    # A more robust way, given we rounded, is direct comparison.\n    count_extreme = np.sum(np.abs(null_distribution_T_values) >= abs(T_obs_rounded))\n    p_value = count_extreme / total_permutations if total_permutations > 0 else 1.0\n\n    # 5. Get the support and probabilities of the null distribution\n    if total_permutations > 0:\n        unique_T, counts = np.unique(null_distribution_T_values, return_counts=True)\n        # Sorting is guaranteed by np.unique\n        support = unique_T.tolist()\n        probabilities = (counts / total_permutations).tolist()\n    else:\n        support = [0.0] if T_obs_rounded == 0.0 else [T_obs_rounded]\n        probabilities = [1.0]\n\n    # 6. Round final results to six decimal places for output\n    p_value_rounded = round(p_value, 6)\n    support_rounded = [round(v, 6) for v in support]\n    probabilities_rounded = [round(p, 6) for p in probabilities]\n\n    return [p_value_rounded, support_rounded, probabilities_rounded]\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        {'a': [1.24, 1.37, 1.51, 1.63], 'b': [1.10, 1.21, 1.28, 1.30]},\n        {'a': [2.80], 'b': [2.10]},\n        {'a': [0.95, 1.02, 1.08], 'b': [0.89, 0.92, 1.00, 1.05, 1.10]},\n        {'a': [3.20, 3.20, 3.40], 'b': [3.10, 3.20, 3.30]},\n        {'a': [1.50, 1.50], 'b': [1.50, 1.50]},\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result = run_permutation_test(case['a'], case['b'])\n        all_results.append(result)\n\n    # Custom formatter to match the precise output format including spacing.\n    def format_float(f):\n        return f\"{f:.6f}\"\n    \n    def format_list_of_floats(lst):\n        return f\"[{','.join(format_float(x) for x in lst)}]\"\n        \n    def format_case_result(res):\n        p, s, P = res\n        p_str = format_float(p)\n        s_str = format_list_of_floats(s)\n        P_str = format_list_of_floats(P)\n        return f\"[{p_str}, {s_str}, {P_str}]\"\n        \n    output_str = f\"[{','.join(format_case_result(r) for r in all_results)}]\"\n    print(output_str)\n```",
            "answer": "```text\n[[0.028571, [-0.222500,-0.190000,-0.175000,-0.157500,-0.142500,-0.125000,-0.110000,-0.092500,-0.085000,-0.077500,-0.060000,-0.052500,-0.045000,-0.035000,-0.027500,-0.010000,0.007500,0.010000,0.027500,0.035000,0.045000,0.052500,0.060000,0.077500,0.085000,0.092500,0.110000,0.125000,0.142500,0.157500,0.175000,0.190000,0.222500], [0.014286,0.014286,0.014286,0.014286,0.014286,0.028571,0.014286,0.014286,0.014286,0.014286,0.014286,0.014286,0.014286,0.028571,0.014286,0.028571,0.028571,0.028571,0.014286,0.028571,0.014286,0.014286,0.014286,0.014286,0.014286,0.014286,0.014286,0.028571,0.014286,0.014286,0.014286,0.014286,0.014286]], [1.000000, [-0.700000,0.700000], [0.500000,0.500000]], [0.214286, [-0.148000,-0.140000,-0.123333,-0.118000,-0.115333,-0.106667,-0.100000,-0.091333,-0.088000,-0.083333,-0.074667,-0.070000,-0.066667,-0.058000,-0.056000,-0.041333,-0.033333,-0.028000,-0.020000,-0.011333,-0.003333,0.004000,0.008667,0.014667,0.025333,0.026667,0.032000,0.034667,0.043333,0.050000,0.051333,0.059333,0.066667,0.074000,0.076667,0.082000,0.088667,0.092000,0.100000,0.103333,0.112000,0.114667,0.126667,0.143333,0.151333], [0.017857,0.017857,0.017857,0.017857,0.017857,0.017857,0.035714,0.017857,0.017857,0.017857,0.017857,0.017857,0.017857,0.017857,0.017857,0.017857,0.017857,0.017857,0.017857,0.017857,0.017857,0.017857,0.017857,0.017857,0.017857,0.017857,0.017857,0.017857,0.017857,0.017857,0.017857,0.017857,0.035714,0.017857,0.017857,0.017857,0.017857,0.017857,0.017857,0.017857,0.017857,0.017857,0.017857,0.017857,0.017857]], [0.400000, [-0.100000,-0.066667,-0.033333,0.000000,0.033333,0.066667,0.100000,0.133333,0.166667], [0.050000,0.150000,0.150000,0.100000,0.150000,0.150000,0.100000,0.100000,0.050000]], [1.000000, [0.000000], [1.000000]]]\n```"
        }
    ]
}