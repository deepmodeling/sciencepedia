{
    "hands_on_practices": [
        {
            "introduction": "In biomedical research, particularly with observational data, hidden variables can distort associations and lead to dangerously incorrect conclusions. This exercise tackles the classic statistical pitfall known as Simpson's paradox, where a trend appears in different groups of data but reverses when the groups are combined. By applying the fundamental law of total probability, you will learn to dissect this paradox and understand how conditioning on a confounding variable is essential for revealing the true underlying effect .",
            "id": "4318462",
            "problem": "A hospital system is evaluating a targeted therapy in observational Electronic Health Records (EHR) data for sepsis patients. Let $X \\in \\{0,1\\}$ denote treatment assignment, with $X=1$ indicating targeted therapy and $X=0$ indicating standard care. Let $Y \\in \\{0,1\\}$ denote $30$-day mortality, with $Y=1$ indicating death. A composite severity index $Z \\in \\{\\mathrm{L}, \\mathrm{H}\\}$, taking values $\\mathrm{L}$ for low severity and $\\mathrm{H}$ for high severity, is a potential confounder because severity is associated with both treatment decisions and mortality.\n\nThe following observational probabilities are estimated:\n- Within each severity stratum:\n  - $P(Y=1 \\mid X=1, Z=\\mathrm{L}) = 0.04$, $P(Y=1 \\mid X=0, Z=\\mathrm{L}) = 0.05$.\n  - $P(Y=1 \\mid X=1, Z=\\mathrm{H}) = 0.20$, $P(Y=1 \\mid X=0, Z=\\mathrm{H}) = 0.24$.\n- Severity distributions differ by treatment group:\n  - $P(Z=\\mathrm{H} \\mid X=1) = 0.90$, $P(Z=\\mathrm{L} \\mid X=1) = 0.10$.\n  - $P(Z=\\mathrm{H} \\mid X=0) = 0.10$, $P(Z=\\mathrm{L} \\mid X=0) = 0.90$.\n\nUsing only the fundamental definitions of conditional probability and the law of total probability, perform the following:\n1. Starting from the definition $P(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}$ and the law of total probability, derive an expression for the aggregated mortality $P(Y=1 \\mid X=x)$ in terms of conditional mortality within severity strata $P(Y=1 \\mid X=x, Z=z)$ and severity distributions $P(Z=z \\mid X=x)$ for $x \\in \\{0,1\\}$ and $z \\in \\{\\mathrm{L}, \\mathrm{H}\\}$.\n2. Explain mathematically why aggregation over $Z$ can produce a reversal (Simpson’s paradox) relative to the stratified comparisons, and identify the role of $P(Z=z \\mid X=x)$ in this reversal.\n3. Compute the aggregated risk difference\n$$\\mathrm{RD} = P(Y=1 \\mid X=1) - P(Y=1 \\mid X=0),$$\nusing the numerical values above. Provide your final numerical value for $\\mathrm{RD}$ as an exact fraction. Do not round. Express the final answer with no units and no percentage sign.",
            "solution": "The problem is well-posed, scientifically grounded, and provides all necessary information to proceed. The objective is to analyze the effect of a targeted therapy on mortality, considering a confounding variable, which is a classic application of conditional probability in biostatistics.\n\nThe solution is divided into three parts as requested by the problem statement. First, we derive the formula for aggregated mortality. Second, we explain the mechanism of Simpson's paradox in this context. Third, we compute the aggregated risk difference using the provided numerical values.\n\n1. Derivation of the aggregated mortality expression\n\nLet $Y \\in \\{0,1\\}$ be the mortality outcome, $X \\in \\{0,1\\}$ be the treatment assignment, and $Z \\in \\{\\mathrm{L}, \\mathrm{H}\\}$ be the severity stratum. We are asked to derive an expression for the aggregated mortality $P(Y=1 \\mid X=x)$ for a given treatment arm $x \\in \\{0,1\\}$.\n\nThe derivation starts from the definition of conditional probability and the law of total probability. The definition of conditional probability states that for any two events $A$ and $B$, with $P(B) > 0$, we have $P(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}$.\nLet $A$ be the event $\\{Y=1\\}$ and $B$ be the event $\\{X=x\\}$. We want to find $P(Y=1 \\mid X=x) = \\frac{P(Y=1 \\cap X=x)}{P(X=x)}$.\n\nThe law of total probability allows us to compute the probability of an event by considering a partition of the sample space. The severity index $Z$ partitions the patient population into two disjoint sets: those with low severity ($Z=\\mathrm{L}$) and those with high severity ($Z=\\mathrm{H}$). We can apply the law of total probability to the joint event $\\{Y=1 \\cap X=x\\}$:\n$$P(Y=1 \\cap X=x) = P(Y=1 \\cap X=x \\cap Z=\\mathrm{L}) + P(Y=1 \\cap X=x \\cap Z=\\mathrm{H})$$\nThis can be written in summation form as:\n$$P(Y=1 \\cap X=x) = \\sum_{z \\in \\{\\mathrm{L}, \\mathrm{H}\\}} P(Y=1, X=x, Z=z)$$\nNow, we use the chain rule of probability, which is a repeated application of the definition of conditional probability: $P(E_1 \\cap E_2 \\cap E_3) = P(E_1 \\mid E_2 \\cap E_3) P(E_2 \\cap E_3) = P(E_1 \\mid E_2 \\cap E_3) P(E_2 \\mid E_3) P(E_3)$.\nIn our notation, for a specific stratum $z$:\n$$P(Y=1, X=x, Z=z) = P(Y=1 \\mid X=x, Z=z) P(X=x, Z=z)$$\nWe can further expand the term $P(X=x, Z=z)$ using the definition of conditional probability as $P(Z=z \\mid X=x) P(X=x)$. Substituting this in, we get:\n$$P(Y=1, X=x, Z=z) = P(Y=1 \\mid X=x, Z=z) P(Z=z \\mid X=x) P(X=x)$$\nNow we substitute this back into the sum:\n$$P(Y=1 \\cap X=x) = \\sum_{z \\in \\{\\mathrm{L}, \\mathrm{H}\\}} P(Y=1 \\mid X=x, Z=z) P(Z=z \\mid X=x) P(X=x)$$\nSince $P(X=x)$ is a common factor in the sum, we can factor it out:\n$$P(Y=1 \\cap X=x) = P(X=x) \\sum_{z \\in \\{\\mathrm{L}, \\mathrm{H}\\}} P(Y=1 \\mid X=x, Z=z) P(Z=z \\mid X=x)$$\nFinally, we substitute this expression for the numerator in our original definition of $P(Y=1 \\mid X=x)$:\n$$P(Y=1 \\mid X=x) = \\frac{P(X=x) \\sum_{z \\in \\{\\mathrm{L}, \\mathrm{H}\\}} P(Y=1 \\mid X=x, Z=z) P(Z=z \\mid X=x)}{P(X=x)}$$\nAssuming that each treatment group is non-empty, i.e., $P(X=x) > 0$, we can cancel this term from the numerator and denominator, yielding the desired expression:\n$$P(Y=1 \\mid X=x) = \\sum_{z \\in \\{\\mathrm{L}, \\mathrm{H}\\}} P(Y=1 \\mid X=x, Z=z) P(Z=z \\mid X=x)$$\nThis expression shows that the aggregated (marginal) mortality risk in a treatment group is a weighted average of the stratum-specific risks, where the weights are the probabilities of being in each stratum, given the treatment.\n\n2. Mathematical explanation of Simpson's paradox\n\nSimpson's paradox is a phenomenon in which a trend or association observed in several different groups of data disappears or reverses when these groups are combined. In this problem, the association is between treatment $X$ and mortality $Y$, and the groups are the severity strata $Z$.\n\nFirst, let's examine the stratified risk differences:\nFor the low-severity stratum ($Z=\\mathrm{L}$):\n$$\\mathrm{RD}_{\\mathrm{L}} = P(Y=1 \\mid X=1, Z=\\mathrm{L}) - P(Y=1 \\mid X=0, Z=\\mathrm{L}) = 0.04 - 0.05 = -0.01$$\nFor the high-severity stratum ($Z=\\mathrm{H}$):\n$$\\mathrm{RD}_{\\mathrm{H}} = P(Y=1 \\mid X=1, Z=\\mathrm{H}) - P(Y=1 \\mid X=0, Z=\\mathrm{H}) = 0.20 - 0.24 = -0.04$$\nIn both strata, the risk difference is negative, indicating that the targeted therapy ($X=1$) is associated with a lower risk of mortality compared to standard care ($X=0$).\n\nThe paradox arises when we aggregate over the confounder $Z$. The aggregated risk difference is $\\mathrm{RD} = P(Y=1 \\mid X=1) - P(Y=1 \\mid X=0)$. Using the formula from part 1, we write $P(Y=1 \\mid X=x)$ as a weighted sum:\n$$P(Y=1 \\mid X=x) = P(Y=1 \\mid X=x, Z=\\mathrm{L})P(Z=\\mathrm{L} \\mid X=x) + P(Y=1 \\mid X=x, Z=\\mathrm{H})P(Z=\\mathrm{H} \\mid X=x)$$\nThe reversal of association is caused by the weighting factors, $P(Z=z \\mid X=x)$. These factors represent the distribution of the confounder $Z$ within each treatment group $X=x$. A reversal can occur if the confounder is associated with both the treatment and the outcome. Here, $Z$ is a confounder because:\n1.  It is associated with the outcome $Y$: Mortality is higher in the high-severity group ($0.20$ and $0.24$) than in the low-severity group ($0.04$ and $0.05$) regardless of treatment.\n2.  It is associated with the treatment $X$: The distribution of severity differs between treatment groups. Specifically, $P(Z=\\mathrm{H} \\mid X=1) = 0.90$, while $P(Z=\\mathrm{H} \\mid X=0) = 0.10$.\n\nThis means patients receiving the targeted therapy ($X=1$) are much more likely to be in the high-severity group ($90\\%$), whereas patients receiving standard care ($X=0$) are much more likely to be in the low-severity group ($90\\%$). This is a typical scenario in observational studies where physicians may preferentially give a newer, more aggressive, or experimental therapy to sicker patients.\n\nThe aggregated mortality for the treated group, $P(Y=1 \\mid X=1)$, is a weighted average of the low risk ($0.04$) and high risk ($0.20$), but with a heavy weight ($0.90$) on the high risk.\n$$P(Y=1 \\mid X=1) = (0.04)(0.10) + (0.20)(0.90)$$\nConversely, the aggregated mortality for the control group, $P(Y=1 \\mid X=0)$, is a weighted average of its corresponding risks ($0.05$ and $0.24$), but with a heavy weight ($0.90$) on the low risk.\n$$P(Y=1 \\mid X=0) = (0.05)(0.90) + (0.24)(0.10)$$\nThe comparison of $P(Y=1 \\mid X=1)$ with $P(Y=1 \\mid X=0)$ is thus a comparison between a predominantly high-risk group and a predominantly low-risk group. This confounding by severity can easily overwhelm the true beneficial effect of the treatment within each stratum, leading to a reversal of the conclusion at the aggregated level.\n\n3. Computation of the aggregated risk difference\n\nWe use the derived formula and the given numerical values to compute the aggregated mortalities for each treatment group.\n\nFor the targeted therapy group ($X=1$):\n$$P(Y=1 \\mid X=1) = P(Y=1 \\mid X=1, Z=\\mathrm{L})P(Z=\\mathrm{L} \\mid X=1) + P(Y=1 \\mid X=1, Z=\\mathrm{H})P(Z=\\mathrm{H} \\mid X=1)$$\n$$P(Y=1 \\mid X=1) = (0.04)(0.10) + (0.20)(0.90)$$\n$$P(Y=1 \\mid X=1) = 0.004 + 0.180 = 0.184$$\n\nFor the standard care group ($X=0$):\n$$P(Y=1 \\mid X=0) = P(Y=1 \\mid X=0, Z=\\mathrm{L})P(Z=\\mathrm{L} \\mid X=0) + P(Y=1 \\mid X=0, Z=\\mathrm{H})P(Z=\\mathrm{H} \\mid X=0)$$\n$$P(Y=1 \\mid X=0) = (0.05)(0.90) + (0.24)(0.10)$$\n$$P(Y=1 \\mid X=0) = 0.045 + 0.024 = 0.069$$\n\nThe aggregated risk difference is:\n$$\\mathrm{RD} = P(Y=1 \\mid X=1) - P(Y=1 \\mid X=0) = 0.184 - 0.069 = 0.115$$\nThe aggregated risk difference is positive, suggesting that the targeted therapy is associated with a higher mortality risk. This is the opposite of the conclusion from the stratum-specific analyses, confirming the presence of Simpson's paradox.\n\nTo express this as an exact fraction:\n$$0.115 = \\frac{115}{1000}$$\nWe can simplify this fraction by dividing the numerator and denominator by their greatest common divisor, which is $5$:\n$$\\mathrm{RD} = \\frac{115 \\div 5}{1000 \\div 5} = \\frac{23}{200}$$\nSince $23$ is a prime number and $200 = 2^3 \\cdot 5^2$, the fraction is fully simplified.",
            "answer": "$$\\boxed{\\frac{23}{200}}$$"
        },
        {
            "introduction": "A central task in Bayesian analysis is updating our beliefs about an unknown quantity as we collect data. This practice provides a hands-on derivation of the posterior distribution in the canonical Beta-Binomial model, a cornerstone for analyzing proportional data . More importantly, you will see how to leverage this posterior to predict future observations and discover how the result naturally balances information from local data with a population-level prior, a powerful concept known as partial pooling.",
            "id": "4318486",
            "problem": "A consortium of hospital-based systems biomedicine laboratories is monitoring the carriage of a specific antibiotic resistance gene in rectal swabs using quantitative Polymerase Chain Reaction (qPCR). Within any single hospital $j$, patient-level carriage outcomes on a given day are modeled as exchangeable Bernoulli trials with an unknown latent prevalence $\\theta_{j} \\in (0,1)$. Across hospitals, the latent prevalences $\\theta_{j}$ are assumed to arise from a common population-level distribution that encodes between-hospital heterogeneity.\n\nSuppose a previously conducted meta-analysis over many hospitals yields a scientifically justified population-level prior for the latent prevalence as a Beta distribution with hyperparameters $\\alpha$ and $\\beta$, that is, $\\theta_{j} \\sim \\operatorname{Beta}(\\alpha,\\beta)$. In a particular hospital $j^{\\star}$ on a given day, $n$ independent and identically distributed patient swabs are processed, and $x$ of them test positive for the resistance gene. Under the Beta–Binomial exchangeable model for hospital $j^{\\star}$, derive from first principles the posterior predictive distribution for the next Bernoulli trial $Y_{\\text{next}} \\in \\{0,1\\}$ at this hospital by integrating over the latent prevalence $\\theta_{j^{\\star}}$. Explicitly show how partial pooling emerges from this integration as a convex combination of the hospital-specific empirical proportion and the population-level prior mean.\n\nFor a concrete calculation, take $\\alpha = 1.5$, $\\beta = 6.5$, $n = 12$, and $x = 7$ for hospital $j^{\\star}$. Compute the posterior predictive probability $\\mathbb{P}(Y_{\\text{next}} = 1 \\mid \\text{data from } j^{\\star})$ under the model. Express the final probability as a decimal and round your answer to four significant figures.",
            "solution": "The problem is well-posed, scientifically grounded within the framework of Bayesian statistics and its application to systems biomedicine, and provides all necessary information for a unique solution.\n\nLet $\\theta_{j^{\\star}} \\in (0,1)$ represent the latent prevalence of the antibiotic resistance gene in the specific hospital $j^{\\star}$. We will denote this simply as $\\theta$ for notational convenience. The problem provides a hierarchical model structure.\n\nFirst, we define the prior distribution for the latent prevalence $\\theta$. Based on a prior meta-analysis, this is given as a Beta distribution with hyperparameters $\\alpha$ and $\\beta$. The probability density function (PDF) of the prior is:\n$$\np(\\theta) = \\operatorname{Beta}(\\theta; \\alpha, \\beta) = \\frac{\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}}{B(\\alpha, \\beta)}\n$$\nwhere $B(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}$ is the Beta function.\n\nSecond, we define the likelihood of observing the data given the latent prevalence $\\theta$. The data consist of $n$ independent and identically distributed (i.i.d.) patient swabs, which are modeled as exchangeable Bernoulli trials. The number of positive swabs, $x$, out of $n$ trials follows a Binomial distribution. The likelihood function is:\n$$\np(x \\mid n, \\theta) = \\binom{n}{x} \\theta^x (1-\\theta)^{n-x}\n$$\nFor brevity, we denote the observed data as $D = \\{x, n\\}$.\n\nTo derive the posterior predictive distribution, we must first find the posterior distribution of $\\theta$ given the data $D$. Using Bayes' theorem, the posterior PDF $p(\\theta \\mid D)$ is proportional to the product of the likelihood and the prior:\n$$\np(\\theta \\mid D) \\propto p(D \\mid \\theta) p(\\theta)\n$$\nSubstituting the expressions for the likelihood and the prior:\n$$\np(\\theta \\mid D) \\propto \\left[ \\binom{n}{x} \\theta^x (1-\\theta)^{n-x} \\right] \\left[ \\frac{\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}}{B(\\alpha, \\beta)} \\right]\n$$\nSince $\\binom{n}{x}$ and $B(\\alpha, \\beta)$ are constants with respect to $\\theta$, we can combine the terms involving $\\theta$:\n$$\np(\\theta \\mid D) \\propto \\theta^x (1-\\theta)^{n-x} \\theta^{\\alpha-1} (1-\\theta)^{\\beta-1} = \\theta^{\\alpha+x-1} (1-\\theta)^{\\beta+n-x-1}\n$$\nThis expression is the kernel of a Beta distribution. Therefore, the posterior distribution of $\\theta$ is also a Beta distribution, a property known as conjugacy. The updated hyperparameters are $\\alpha' = \\alpha+x$ and $\\beta' = \\beta+n-x$. The full posterior PDF is:\n$$\np(\\theta \\mid D) = \\operatorname{Beta}(\\theta; \\alpha+x, \\beta+n-x) = \\frac{\\theta^{\\alpha+x-1} (1-\\theta)^{\\beta+n-x-1}}{B(\\alpha+x, \\beta+n-x)}\n$$\n\nNow, we can derive the posterior predictive distribution for a new Bernoulli trial, $Y_{\\text{next}} \\in \\{0, 1\\}$. We are interested in the probability that this next trial is a success (i.e., the swab is positive), $\\mathbb{P}(Y_{\\text{next}}=1 \\mid D)$. This probability is obtained by marginalizing (integrating) the likelihood of the new observation over the posterior distribution of the parameter $\\theta$:\n$$\n\\mathbb{P}(Y_{\\text{next}}=1 \\mid D) = \\int_{0}^{1} \\mathbb{P}(Y_{\\text{next}}=1 \\mid \\theta, D) p(\\theta \\mid D) \\,d\\theta\n$$\nGiven the assumption of exchangeability, the outcome of the next trial is conditionally independent of the past data $D$ given the parameter $\\theta$. Thus, $\\mathbb{P}(Y_{\\text{next}}=1 \\mid \\theta, D) = \\mathbb{P}(Y_{\\text{next}}=1 \\mid \\theta) = \\theta$. The integral becomes:\n$$\n\\mathbb{P}(Y_{\\text{next}}=1 \\mid D) = \\int_{0}^{1} \\theta \\cdot p(\\theta \\mid D) \\,d\\theta\n$$\nThis integral is, by definition, the expected value of $\\theta$ under the posterior distribution, $\\mathbb{E}[\\theta \\mid D]$. The mean of a Beta distribution $\\operatorname{Beta}(\\alpha', \\beta')$ is given by $\\frac{\\alpha'}{\\alpha'+\\beta'}$. For our posterior distribution $\\operatorname{Beta}(\\theta; \\alpha+x, \\beta+n-x)$, the mean is:\n$$\n\\mathbb{E}[\\theta \\mid D] = \\frac{\\alpha+x}{(\\alpha+x) + (\\beta+n-x)} = \\frac{\\alpha+x}{\\alpha+\\beta+n}\n$$\nSo, the posterior predictive probability of a positive result is:\n$$\n\\mathbb{P}(Y_{\\text{next}}=1 \\mid D) = \\frac{\\alpha+x}{\\alpha+\\beta+n}\n$$\nThe posterior predictive distribution for the Bernoulli trial $Y_{\\text{next}}$ is a Bernoulli distribution with parameter $p = \\mathbb{P}(Y_{\\text{next}}=1 \\mid D)$.\n\nTo show how this result embodies partial pooling, we can rewrite the expression as a weighted average. The two quantities being averaged are the hospital-specific empirical proportion of positives, $\\frac{x}{n}$, and the prior mean prevalence, $\\mathbb{E}[\\theta] = \\frac{\\alpha}{\\alpha+\\beta}$. We want to express the posterior predictive probability in the form $\\lambda \\left(\\frac{x}{n}\\right) + (1-\\lambda) \\left(\\frac{\\alpha}{\\alpha+\\beta}\\right)$. Let's start from this form and show it is equivalent to our result:\n$$\n\\frac{n}{\\alpha+\\beta+n} \\left(\\frac{x}{n}\\right) + \\frac{\\alpha+\\beta}{\\alpha+\\beta+n} \\left(\\frac{\\alpha}{\\alpha+\\beta}\\right) = \\frac{x}{\\alpha+\\beta+n} + \\frac{\\alpha}{\\alpha+\\beta+n} = \\frac{\\alpha+x}{\\alpha+\\beta+n}\n$$\nThis confirms the structure. The posterior predictive probability is a convex combination:\n$$\n\\mathbb{P}(Y_{\\text{next}}=1 \\mid D) = \\left( \\frac{n}{n + \\alpha+\\beta} \\right) \\left( \\frac{x}{n} \\right) + \\left( \\frac{\\alpha+\\beta}{n + \\alpha+\\beta} \\right) \\left( \\frac{\\alpha}{\\alpha+\\beta} \\right)\n$$\nHere, the weight $\\frac{n}{n+\\alpha+\\beta}$ is applied to the hospital-specific empirical proportion ($\\frac{x}{n}$), and the complementary weight $\\frac{\\alpha+\\beta}{n+\\alpha+\\beta}$ is applied to the population-level prior mean ($\\frac{\\alpha}{\\alpha+\\beta}$). The term $\\alpha+\\beta$ can be interpreted as a \"prior sample size\" or a measure of the strength of the prior belief. The posterior estimate is thus \"pulled\" away from the local data towards the global mean, an effect known as partial pooling or shrinkage. The degree of shrinkage depends on the relative sizes of the observed sample ($n$) and the prior effective sample size ($\\alpha+\\beta$).\n\nFor the concrete calculation, the given values are $\\alpha = 1.5$, $\\beta = 6.5$, $n = 12$, and $x = 7$.\nThe posterior predictive probability $\\mathbb{P}(Y_{\\text{next}} = 1 \\mid D)$ is calculated using the formula derived above:\n$$\n\\mathbb{P}(Y_{\\text{next}}=1 \\mid D) = \\frac{\\alpha+x}{\\alpha+\\beta+n} = \\frac{1.5+7}{1.5+6.5+12} = \\frac{8.5}{8+12} = \\frac{8.5}{20}\n$$\nCalculating the decimal value:\n$$\n\\frac{8.5}{20} = 0.425\n$$\nThe problem requires the answer to be rounded to four significant figures. This gives $0.4250$.\nLet's also compute the components of the partial pooling to verify:\nPrior mean: $\\frac{\\alpha}{\\alpha+\\beta} = \\frac{1.5}{1.5+6.5} = \\frac{1.5}{8} = 0.1875$.\nEmpirical proportion: $\\frac{x}{n} = \\frac{7}{12} \\approx 0.5833$.\nThe weights are:\nWeight on data: $\\frac{n}{n+\\alpha+\\beta} = \\frac{12}{12+8} = \\frac{12}{20} = 0.6$.\nWeight on prior: $\\frac{\\alpha+\\beta}{n+\\alpha+\\beta} = \\frac{8}{12+8} = \\frac{8}{20} = 0.4$.\nThe posterior mean is $(0.6) \\times (\\frac{7}{12}) + (0.4) \\times (0.1875) = 0.6 \\times 0.58333... + 0.4 \\times 0.1875 = 0.35 + 0.075 = 0.425$. The calculation is consistent. The final probability is $0.4250$.",
            "answer": "$$\\boxed{0.4250}$$"
        },
        {
            "introduction": "Systems biology models the intricate web of interactions between molecular components, often represented as a graphical network. This exercise introduces the concept of a Bayesian network to model a gene regulatory module and asks you to compute the conditional probability of a gene's state given its local neighborhood, known as the Markov blanket . Mastering this calculation is key to understanding how powerful algorithms like Gibbs sampling can efficiently perform inference on complex, high-dimensional systems by focusing on a sequence of local updates.",
            "id": "4318431",
            "problem": "In a systems biomedicine setting, consider a fragment of a Bayesian network modeling a gene regulatory module. A binary gene-activation node $G \\in \\{0,1\\}$ is regulated by two binary parents: a transcription factor activity node $T \\in \\{0,1\\}$ and an upstream stimulus node $S \\in \\{0,1\\}$. The activated gene $G$ has two children: a binary downstream pathway readout $C \\in \\{0,1\\}$ that also depends on an auxiliary modulator $H \\in \\{0,1\\}$, and a binary measurement node $M \\in \\{0,1\\}$ representing a detection assay that directly depends on $G$. The directed edges are $T \\to G$, $S \\to G$, $G \\to C$, $H \\to C$, and $G \\to M$. You will focus on the conditional distribution of $G$ given its Markov blanket.\n\nAssume the following conditional probability tables are scientifically calibrated and fixed:\n- For $G$ given its parents $(T,S)$:\n  - $P(G=1 \\mid T=1,S=1) = \\frac{3}{4}$,\n  - $P(G=1 \\mid T=1,S=0) = \\frac{1}{2}$,\n  - $P(G=1 \\mid T=0,S=1) = \\frac{1}{3}$,\n  - $P(G=1 \\mid T=0,S=0) = \\frac{1}{5}$.\n- For the readout $C$ given $(G,H)$:\n  - $P(C=1 \\mid G=1,H=1) = \\frac{4}{5}$,\n  - $P(C=1 \\mid G=0,H=1) = \\frac{2}{5}$,\n  - $P(C=1 \\mid G=1,H=0) = \\frac{1}{2}$,\n  - $P(C=1 \\mid G=0,H=0) = \\frac{1}{10}$.\n- For the assay $M$ given $G$:\n  - $P(M=0 \\mid G=1) = \\frac{1}{4}$,\n  - $P(M=0 \\mid G=0) = \\frac{4}{5}$.\n\nSuppose at the current state of a Gibbs sampler used for approximate inference, the variables in the Markov blanket of $G$ have the following values: $T=1$, $S=0$, $H=1$, $C=1$, and $M=0$. You may assume that any variables not in the Markov blanket are conditionally independent of $G$ given this blanket.\n\nTask:\n- Using only foundational definitions (the product factorization of a Bayesian network, Bayes' rule, and the definition of a Markov blanket), derive from first principles the full conditional distribution $P(G=1 \\mid T=1,S=0,H=1,C=1,M=0)$ and compute its exact value. Provide your final answer as a reduced fraction; do not round.\n- Briefly justify, in one sentence, why nodes outside the Markov blanket of $G$ do not enter the full conditional for $G$ in a Gibbs sampling update.\n\nYour final reported answer must be a single real-valued number in exact fractional form, with no units. Do not round.",
            "solution": "The task is to compute the full conditional probability $P(G=1 \\mid T=1,S=0,H=1,C=1,M=0)$ for the gene activation node $G$ within a given Bayesian network, and to justify the use of the Markov blanket.\n\nFirst, we must identify the Markov blanket of node $G$, denoted $MB(G)$. The Markov blanket of a node in a Bayesian network consists of its parents, its children, and its children's other parents (co-parents).\nFrom the problem statement, the directed edges are $T \\to G$, $S \\to G$, $G \\to C$, $H \\to C$, and $G \\to M$.\n- The parents of $G$ are the nodes with edges directed into $G$: $\\{T, S\\}$.\n- The children of $G$ are the nodes with edges directed from $G$: $\\{C, M\\}$.\n- The co-parents of $G$ are the other parents of $G$'s children. The child $C$ has parent $H$ in addition to $G$. The child $M$ has no other parent. Thus, the only co-parent is $\\{H\\}$.\n\nCombining these sets, the Markov blanket of $G$ is $MB(G) = \\{T, S, C, H, M\\}$. The set of conditioning variables given in the problem, $\\{T=1, S=0, H=1, C=1, M=0\\}$, corresponds to an assignment of values to the nodes in $G$'s Markov blanket. Let this assignment be denoted by $mb_G$.\n\nA fundamental property of Bayesian networks is that a node is conditionally independent of all other nodes in the network given its Markov blanket. This means that for the purpose of a Gibbs sampling update or any conditional probability query, we only need to consider the nodes within this local neighborhood. The expression for the full conditional probability of $G$ is:\n$$ P(G \\mid mb_G) = P(G \\mid T, S, H, C, M) $$\nBy the definition of conditional probability and the chain rule for Bayesian networks, this distribution is proportional to the product of the conditional probability of $G$ given its parents and the conditional probabilities of each of $G$'s children given their respective parents.\n$$ P(G \\mid mb_G) \\propto P(G, mb_G) = P(G, T, S, H, C, M) $$\nThe joint probability of all variables in the network factorizes as $P(T,S,H,G,C,M) = P(T)P(S)P(H)P(G \\mid T,S)P(C \\mid G,H)P(M \\mid G)$, assuming $T$, $S$, and $H$ are root nodes. When conditioning on $mb_G$, the terms $P(T)$, $P(S)$, and $P(H)$ are constant for all values of $G$ and will cancel out during normalization. Thus, the full conditional for $G$ simplifies to be proportional to only the terms in the full joint distribution that involve $G$:\n$$ P(G \\mid T, S, H, C, M) \\propto P(G \\mid T, S) P(C \\mid G, H) P(M \\mid G) $$\nWe are asked to compute $P(G=1 \\mid T=1,S=0,H=1,C=1,M=0)$. We can use the proportionality by calculating the unnormalized values for $G=1$ and $G=0$ and then normalizing.\n\nLet $\\tilde{P}(G=g)$ be the unnormalized probability for state $g \\in \\{0,1\\}$, given the observed state of the Markov blanket $\\{T=1,S=0,H=1,C=1,M=0\\}$.\nFor $G=1$:\n$$ \\tilde{P}(G=1) = P(G=1 \\mid T=1, S=0) \\times P(C=1 \\mid G=1, H=1) \\times P(M=0 \\mid G=1) $$\nUsing the values from the provided conditional probability tables (CPTs):\n- $P(G=1 \\mid T=1, S=0) = \\frac{1}{2}$\n- $P(C=1 \\mid G=1, H=1) = \\frac{4}{5}$\n- $P(M=0 \\mid G=1) = \\frac{1}{4}$\nSo, the unnormalized term for $G=1$ is:\n$$ \\tilde{P}(G=1) = \\frac{1}{2} \\times \\frac{4}{5} \\times \\frac{1}{4} = \\frac{4}{40} = \\frac{1}{10} $$\nFor $G=0$:\n$$ \\tilde{P}(G=0) = P(G=0 \\mid T=1, S=0) \\times P(C=1 \\mid G=0, H=1) \\times P(M=0 \\mid G=0) $$\nWe need to derive the required probabilities from the given CPTs. The sum of probabilities for a variable must be $1$.\n- $P(G=0 \\mid T=1,S=0) = 1 - P(G=1 \\mid T=1,S=0) = 1 - \\frac{1}{2} = \\frac{1}{2}$\n- $P(C=1 \\mid G=0, H=1) = \\frac{2}{5}$\n- $P(M=0 \\mid G=0) = \\frac{4}{5}$\nSo, the unnormalized term for $G=0$ is:\n$$ \\tilde{P}(G=0) = \\frac{1}{2} \\times \\frac{2}{5} \\times \\frac{4}{5} = \\frac{8}{50} = \\frac{4}{25} $$\nTo find the final probability, we normalize these values:\n$$ P(G=1 \\mid mb_G) = \\frac{\\tilde{P}(G=1)}{\\tilde{P}(G=1) + \\tilde{P}(G=0)} = \\frac{\\frac{1}{10}}{\\frac{1}{10} + \\frac{4}{25}} $$\nTo perform the sum in the denominator, we find a common denominator, which is $50$:\n$$ \\frac{1}{10} + \\frac{4}{25} = \\frac{5}{50} + \\frac{8}{50} = \\frac{13}{50} $$\nSubstituting this back into the expression for the probability:\n$$ P(G=1 \\mid mb_G) = \\frac{\\frac{1}{10}}{\\frac{13}{50}} = \\frac{1}{10} \\times \\frac{50}{13} = \\frac{50}{130} = \\frac{5}{13} $$\nThe exact value of the full conditional probability is $\\frac{5}{13}$.\n\nRegarding the second part of the task, the justification for why nodes outside the Markov blanket do not enter the full conditional is a direct consequence of the definition of a Bayesian network.\nDue to the conditional independence properties inherent in a Bayesian network's structure, formally captured by the d-separation criterion, any node is conditionally independent of all other nodes in the graph given its Markov blanket.",
            "answer": "$$\n\\boxed{\\frac{5}{13}}\n$$"
        }
    ]
}