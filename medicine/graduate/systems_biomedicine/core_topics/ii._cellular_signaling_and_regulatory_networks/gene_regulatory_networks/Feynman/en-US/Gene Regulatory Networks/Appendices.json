{
    "hands_on_practices": [
        {
            "introduction": "Before we can analyze the complex dynamics of a gene regulatory network, we must first establish a formal mathematical language to describe its structure. This practice  introduces the representation of a GRN as a directed, signed graph, which can be encoded in an adjacency matrix—a fundamental tool in network science. By constructing this matrix and computing its properties, you will translate qualitative biological relationships into a quantitative format amenable to computational analysis.",
            "id": "4345473",
            "problem": "Consider a gene regulatory network (GRN) with $5$ genes labeled $g_1, g_2, g_3, g_4, g_5$. The network is represented by a directed, signed adjacency matrix $A \\in \\mathbb{R}^{5 \\times 5}$, where $A_{ij} = 1$ if gene $g_i$ activates gene $g_j$, $A_{ij} = -1$ if gene $g_i$ represses gene $g_j$, and $A_{ij} = 0$ otherwise. Self-regulation is permitted. The following experimentally supported regulatory relationships are observed:\n\n- $g_1$ activates $g_2$.\n- $g_2$ represses $g_1$.\n- $g_3$ activates $g_1$.\n- $g_3$ represses $g_2$.\n- $g_4$ activates itself.\n- $g_4$ represses $g_5$.\n- $g_5$ activates $g_4$.\n- $g_2$ activates $g_5$.\n- $g_5$ represses $g_3$.\n- $g_1$ represses $g_5$.\n\nTasks:\n1. Construct the adjacency matrix $A$ that encodes the above relationships under the stated convention.\n2. Compute, for each gene $g_i$, the out-degree $d_i^{\\mathrm{out}}$ and the in-degree $d_i^{\\mathrm{in}}$, where $d_i^{\\mathrm{out}} = \\sum_{j=1}^{5} \\mathbf{1}\\{A_{ij} \\neq 0\\}$ and $d_i^{\\mathrm{in}} = \\sum_{j=1}^{5} \\mathbf{1}\\{A_{ji} \\neq 0\\}$. Self-loops count toward both in-degree and out-degree.\n3. Let $S = \\mathrm{tr}(A^2)$, the trace of the square of the adjacency matrix. Compute $S$ exactly.\n\nProvide only the value of $S$ as your final reported result. No rounding is required.",
            "solution": "The problem requires the construction of a signed adjacency matrix $A$ for a given gene regulatory network and the computation of the trace of its square, $S = \\mathrm{tr}(A^2)$. This quantity is significant in network analysis as it relates to the number of 2-cycles in the graph. The problem is well-defined and all necessary relationships are provided.\n\n**Step 1: Construct the Adjacency Matrix A**\n\nBased on the 10 specified regulatory relationships, we construct the $5 \\times 5$ matrix $A$, where rows represent the source gene and columns represent the target gene. $A_{ij}=1$ for activation, $A_{ij}=-1$ for repression, and $A_{ij}=0$ for no direct regulation. The relationships translate to the following matrix entries:\n- $g_1 \\to g_2$ (+1) $\\implies A_{12} = 1$\n- $g_2 \\to g_1$ (-1) $\\implies A_{21} = -1$\n- $g_3 \\to g_1$ (+1) $\\implies A_{31} = 1$\n- $g_3 \\to g_2$ (-1) $\\implies A_{32} = -1$\n- $g_4 \\to g_4$ (+1) $\\implies A_{44} = 1$\n- $g_4 \\to g_5$ (-1) $\\implies A_{45} = -1$\n- $g_5 \\to g_4$ (+1) $\\implies A_{54} = 1$\n- $g_2 \\to g_5$ (+1) $\\implies A_{25} = 1$\n- $g_5 \\to g_3$ (-1) $\\implies A_{53} = -1$\n- $g_1 \\to g_5$ (-1) $\\implies A_{15} = -1$\n\nThis results in the matrix:\n$$\nA = \\begin{pmatrix}\n0 & 1 & 0 & 0 & -1 \\\\\n-1 & 0 & 0 & 0 & 1 \\\\\n1 & -1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & -1 \\\\\n0 & 0 & -1 & 1 & 0\n\\end{pmatrix}\n$$\n\n**Step 2: Compute S = tr(A^2)**\n\nThe trace of $A^2$ is the sum of the diagonal elements of the matrix $A^2$. The $i$-th diagonal element of $A^2$ is given by the formula:\n$$\n(A^2)_{ii} = \\sum_{k=1}^{5} A_{ik} A_{ki}\n$$\nThis term counts the weighted number of 2-step paths that start at node $i$ and end at node $i$. Each reciprocal edge pair $(i \\to k, k \\to i)$ contributes $A_{ik}A_{ki}$ to the sum. A self-loop $(i \\to i)$ contributes $A_{ii}A_{ii} = (A_{ii})^2$.\n\nLet's compute each diagonal element:\n- $(A^2)_{11} = \\sum_{k=1}^{5} A_{1k} A_{k1} = A_{12}A_{21} = (1)(-1) = -1$.\n- $(A^2)_{22} = \\sum_{k=1}^{5} A_{2k} A_{k2} = A_{21}A_{12} = (-1)(1) = -1$.\n- $(A^2)_{33} = \\sum_{k=1}^{5} A_{3k} A_{k3} = A_{31}A_{13} + A_{32}A_{23} + A_{35}A_{53} = (1)(0) + (-1)(0) + (0)(-1) = 0$.\n- $(A^2)_{44} = \\sum_{k=1}^{5} A_{4k} A_{k4} = A_{44}A_{44} + A_{45}A_{54} = (1)(1) + (-1)(1) = 1 - 1 = 0$.\n- $(A^2)_{55} = \\sum_{k=1}^{5} A_{5k} A_{k5} = A_{52}A_{25} + A_{54}A_{45} = (0)(1) + (1)(-1) = -1$.\n\nThe trace $S$ is the sum of these diagonal elements:\n$$\nS = \\mathrm{tr}(A^2) = (A^2)_{11} + (A^2)_{22} + (A^2)_{33} + (A^2)_{44} + (A^2)_{55}\n$$\n$$\nS = (-1) + (-1) + 0 + 0 + (-1) = -3\n$$\nWhile the problem also asked for the in-degree and out-degree of each gene, these are not required for the final reported result $S$.\n\nThe final value is -3.",
            "answer": "$$\n\\boxed{-3}\n$$"
        },
        {
            "introduction": "Moving beyond static structure, we explore the dynamic behaviors that arise from network architecture, using the genetic toggle switch as a model system. This exercise  delves into linear stability analysis, a powerful technique to determine if a system's steady states are stable or unstable. By constructing the Jacobian matrix and analyzing its eigenvalues, you will learn to predict how a network responds to small perturbations, a key to understanding cellular decision-making.",
            "id": "4345428",
            "problem": "Consider a minimal two-gene regulatory network in a single well-mixed cell, modeled at the level of protein concentrations. Let $x(t)$ and $y(t)$ denote the protein concentrations of gene $X$ and gene $Y$, respectively, measured in nanomolar (nM). Gene $Y$ represses production of $X$ via a Hill-type repression with coefficient $n$, and gene $X$ represses production of $Y$ via a Hill-type repression with coefficient $m$. Assume that synthesis and degradation follow standard mass-action kinetics and Hill-type regulatory functions, consistent with the Central Dogma of molecular biology and widely used phenomenological models of gene regulatory networks (GRNs). The ordinary differential equations (ODEs) are\n$$\n\\frac{dx}{dt} \\;=\\; \\frac{\\alpha_{x}}{1 + \\left(\\frac{y}{K_{y}}\\right)^{n}} \\;-\\; \\delta_{x}\\,x,\n\\qquad\n\\frac{dy}{dt} \\;=\\; \\frac{\\alpha_{y}}{1 + \\left(\\frac{x}{K_{x}}\\right)^{m}} \\;-\\; \\delta_{y}\\,y,\n$$\nwhere $K_{x}$ and $K_{y}$ are repression thresholds (nM), $\\alpha_{x}$ and $\\alpha_{y}$ are maximal synthesis rates (nM/min), and $\\delta_{x}$ and $\\delta_{y}$ are first-order degradation rate constants (min$^{-1}$). The parameter values are:\n$$\n\\delta_{x} = 0.20,\\quad \\delta_{y} = 0.25,\\quad K_{x} = 100,\\quad K_{y} = 80,\\quad n = 2,\\quad m = 1,\n$$\nand the synthesis rates are chosen so that the steady state occurs at the repression thresholds:\n$$\n\\alpha_{x} = 2\\,\\delta_{x}\\,K_{x},\\qquad \\alpha_{y} = 2\\,\\delta_{y}\\,K_{y}.\n$$\nStarting from the stated ODEs and definitions, do the following:\n1. Verify that $\\big(x^{\\ast}, y^{\\ast}\\big) = \\big(K_{x}, K_{y}\\big)$ is a steady state under the given parameter choices.\n2. Linearize the system around $\\big(x^{\\ast}, y^{\\ast}\\big)$ and construct the Jacobian matrix $J$ with entries $J_{ij} = \\left.\\frac{\\partial f_{i}}{\\partial z_{j}}\\right|_{(x^{\\ast}, y^{\\ast})}$, where $\\mathbf{f} = \\big(f_{x}, f_{y}\\big)$ are the right-hand sides of the ODEs, and $\\mathbf{z} = (x,y)$.\n3. Using first principles of linear stability analysis, assess the local stability at $\\big(x^{\\ast}, y^{\\ast}\\big)$ by computing the eigenvalues of $J$ and report the largest real part among these eigenvalues.\n\nAnswer specification:\n- Provide the largest real part of the eigenvalues of $J$ as a single real number.\n- Express your final numerical answer in min$^{-1}$.\n- Round your answer to four significant figures.",
            "solution": "The problem asks for an analysis of the local stability of a two-gene regulatory network at a specified steady state. The analysis proceeds in three steps: first, verification of the steady state; second, construction of the Jacobian matrix by linearizing the system at that steady state; and third, calculation of the eigenvalues of the Jacobian to determine stability.\n\nThe system of ordinary differential equations (ODEs) is given by:\n$$\n\\frac{dx}{dt} = f_{x}(x, y) = \\frac{\\alpha_{x}}{1 + \\left(\\frac{y}{K_{y}}\\right)^{n}} - \\delta_{x}\\,x,\n$$\n$$\n\\frac{dy}{dt} = f_{y}(x, y) = \\frac{\\alpha_{y}}{1 + \\left(\\frac{x}{K_{x}}\\right)^{m}} - \\delta_{y}\\,y.\n$$\n\nThe parameter values are:\n$\\delta_{x} = 0.20$, $\\delta_{y} = 0.25$, $K_{x} = 100$, $K_{y} = 80$, $n = 2$, and $m = 1$.\nThe synthesis rates are defined as $\\alpha_{x} = 2\\,\\delta_{x}\\,K_{x}$ and $\\alpha_{y} = 2\\,\\delta_{y}\\,K_{y}$.\n\n**Step 1: Verification of the Steady State**\n\nA steady state $(x^{\\ast}, y^{\\ast})$ is a point where both time derivatives are zero, i.e., $f_{x}(x^{\\ast}, y^{\\ast}) = 0$ and $f_{y}(x^{\\ast}, y^{\\ast}) = 0$. We must verify that $(x^{\\ast}, y^{\\ast}) = (K_{x}, K_{y})$ is a steady state.\n\nSubstituting $x = K_{x}$ and $y = K_{y}$ into the first ODE:\n$$\n\\left.\\frac{dx}{dt}\\right|_{(K_{x}, K_{y})} = \\frac{\\alpha_{x}}{1 + \\left(\\frac{K_{y}}{K_{y}}\\right)^{n}} - \\delta_{x}K_{x} = \\frac{\\alpha_{x}}{1 + 1^{n}} - \\delta_{x}K_{x} = \\frac{\\alpha_{x}}{2} - \\delta_{x}K_{x}.\n$$\nUsing the given relation $\\alpha_{x} = 2\\,\\delta_{x}\\,K_{x}$, we have:\n$$\n\\frac{2\\,\\delta_{x}\\,K_{x}}{2} - \\delta_{x}K_{x} = \\delta_{x}K_{x} - \\delta_{x}K_{x} = 0.\n$$\nSubstituting $x = K_{x}$ and $y = K_{y}$ into the second ODE:\n$$\n\\left.\\frac{dy}{dt}\\right|_{(K_{x}, K_{y})} = \\frac{\\alpha_{y}}{1 + \\left(\\frac{K_{x}}{K_{x}}\\right)^{m}} - \\delta_{y}K_{y} = \\frac{\\alpha_{y}}{1 + 1^{m}} - \\delta_{y}K_{y} = \\frac{\\alpha_{y}}{2} - \\delta_{y}K_{y}.\n$$\nUsing the given relation $\\alpha_{y} = 2\\,\\delta_{y}\\,K_{y}$, we have:\n$$\n\\frac{2\\,\\delta_{y}\\,K_{y}}{2} - \\delta_{y}K_{y} = \\delta_{y}K_{y} - \\delta_{y}K_{y} = 0.\n$$\nSince both derivatives are zero, $(x^{\\ast}, y^{\\ast}) = (K_{x}, K_{y})$ is confirmed to be a steady state of the system.\n\n**Step 2: Construction of the Jacobian Matrix**\n\nThe Jacobian matrix $J$ of the system is defined as:\n$$\nJ = \\begin{pmatrix} \\frac{\\partial f_x}{\\partial x} & \\frac{\\partial f_x}{\\partial y} \\\\ \\frac{\\partial f_y}{\\partial x} & \\frac{\\partial f_y}{\\partial y} \\end{pmatrix}.\n$$\nWe compute the four partial derivatives:\n$$\nJ_{11} = \\frac{\\partial f_x}{\\partial x} = -\\delta_x\n$$\n$$\nJ_{12} = \\frac{\\partial f_x}{\\partial y} = \\frac{\\partial}{\\partial y} \\left( \\alpha_x \\left(1 + \\left(\\frac{y}{K_y}\\right)^n \\right)^{-1} \\right) = -\\alpha_x \\left(1 + \\left(\\frac{y}{K_y}\\right)^n \\right)^{-2} \\left( n\\frac{y^{n-1}}{K_y^n} \\right)\n$$\n$$\nJ_{21} = \\frac{\\partial f_y}{\\partial x} = \\frac{\\partial}{\\partial x} \\left( \\alpha_y \\left(1 + \\left(\\frac{x}{K_x}\\right)^m \\right)^{-1} \\right) = -\\alpha_y \\left(1 + \\left(\\frac{x}{K_x}\\right)^m \\right)^{-2} \\left( m\\frac{x^{m-1}}{K_x^m} \\right)\n$$\n$$\nJ_{22} = \\frac{\\partial f_y}{\\partial y} = -\\delta_y\n$$\nNow, we evaluate these derivatives at the steady state $(x^{\\ast}, y^{\\ast}) = (K_x, K_y)$:\n$$\nJ_{11} = -\\delta_x\n$$\n$$\nJ_{12} = -\\alpha_x \\left(1 + \\left(\\frac{K_y}{K_y}\\right)^n \\right)^{-2} \\left( n\\frac{K_y^{n-1}}{K_y^n} \\right) = -\\alpha_x (1+1)^{-2} \\left( \\frac{n}{K_y} \\right) = -\\frac{n\\alpha_x}{4K_y}\n$$\n$$\nJ_{21} = -\\alpha_y \\left(1 + \\left(\\frac{K_x}{K_x}\\right)^m \\right)^{-2} \\left( m\\frac{K_x^{m-1}}{K_x^m} \\right) = -\\alpha_y (1+1)^{-2} \\left( \\frac{m}{K_x} \\right) = -\\frac{m\\alpha_y}{4K_x}\n$$\n$$\nJ_{22} = -\\delta_y\n$$\nSubstituting $\\alpha_x = 2\\delta_x K_x$ and $\\alpha_y = 2\\delta_y K_y$:\n$$\nJ_{12} = -\\frac{n(2\\delta_x K_x)}{4K_y} = -\\frac{n\\delta_x K_x}{2K_y}\n$$\n$$\nJ_{21} = -\\frac{m(2\\delta_y K_y)}{4K_x} = -\\frac{m\\delta_y K_y}{2K_x}\n$$\nThus, the symbolic Jacobian at the steady state is:\n$$\nJ = \\begin{pmatrix} -\\delta_x & -\\frac{n\\delta_x K_x}{2K_y} \\\\ -\\frac{m\\delta_y K_y}{2K_x} & -\\delta_y \\end{pmatrix}\n$$\nSubstituting the numerical values:\n$$\nJ_{11} = -0.20\n$$\n$$\nJ_{12} = -\\frac{2 \\times 0.20 \\times 100}{2 \\times 80} = -\\frac{40}{160} = -0.25\n$$\n$$\nJ_{21} = -\\frac{1 \\times 0.25 \\times 80}{2 \\times 100} = -\\frac{20}{200} = -0.10\n$$\n$$\nJ_{22} = -0.25\n$$\nThe numerical Jacobian matrix is:\n$$\nJ = \\begin{pmatrix} -0.20 & -0.25 \\\\ -0.10 & -0.25 \\end{pmatrix}\n$$\n\n**Step 3: Stability Analysis via Eigenvalues**\n\nThe stability of the steady state is determined by the eigenvalues of $J$. The eigenvalues $\\lambda$ are the roots of the characteristic equation $\\det(J - \\lambda I) = 0$, which for a $2 \\times 2$ matrix is $\\lambda^2 - \\text{tr}(J)\\lambda + \\det(J) = 0$.\n\nFirst, we compute the trace and determinant of $J$:\n$$\n\\text{tr}(J) = J_{11} + J_{22} = -0.20 + (-0.25) = -0.45\n$$\n$$\n\\det(J) = J_{11}J_{22} - J_{12}J_{21} = (-0.20)(-0.25) - (-0.25)(-0.10) = 0.050 - 0.025 = 0.025\n$$\nThe characteristic equation is:\n$$\n\\lambda^2 - (-0.45)\\lambda + 0.025 = 0 \\implies \\lambda^2 + 0.45\\lambda + 0.025 = 0\n$$\nWe solve this quadratic equation for $\\lambda$ using the formula $\\lambda = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$:\n$$\n\\lambda = \\frac{-0.45 \\pm \\sqrt{(0.45)^2 - 4(1)(0.025)}}{2} = \\frac{-0.45 \\pm \\sqrt{0.2025 - 0.1}}{2} = \\frac{-0.45 \\pm \\sqrt{0.1025}}{2}\n$$\nThe discriminant $\\Delta = 0.1025$ is positive, so there are two distinct real eigenvalues. The largest real part of the eigenvalues will be the larger of these two real roots.\n$$\n\\lambda_{\\text{max}} = \\frac{-0.45 + \\sqrt{0.1025}}{2}\n$$\nNumerically evaluating this expression:\n$$\n\\sqrt{0.1025} \\approx 0.3201562\n$$\n$$\n\\lambda_{\\text{max}} \\approx \\frac{-0.45 + 0.3201562}{2} = \\frac{-0.1298438}{2} = -0.0649219\n$$\nThe problem requires rounding to four significant figures.\n$$\n\\lambda_{\\text{max}} \\approx -0.06492\n$$\nThe units of the eigenvalues are the same as the units of the diagonal elements of the Jacobian, which are min$^{-1}$. Since the largest real part is negative, the steady state $(K_x, K_y)$ is locally stable.\n\nThe largest real part of the eigenvalues is approximately $-0.06492$ min$^{-1}$.",
            "answer": "$$\\boxed{-0.06492}$$"
        },
        {
            "introduction": "While analyzing known models is insightful, a central goal of systems biology is to infer network structure from experimental data. This practice  tackles this reverse-engineering challenge by using time-series gene expression data to discover the underlying regulatory 'wiring diagram'. You will implement a Bayesian inference framework to calculate the posterior probability of each potential regulatory link, providing a rigorous, data-driven method for hypothesis generation about network topology.",
            "id": "4278329",
            "problem": "You are given a time-lagged gene expression time series for a small gene regulatory network modeled as a first-order Dynamic Bayesian Network (DBN). The DBN assumes that, for each gene (target) at time step $t$, its expression is a linear combination of all genes (potential parents, including itself) at time step $t-1$, corrupted by conditionally independent Gaussian noise. Specifically, for each target gene index $g \\in \\{1,\\dots,G\\}$ and time steps $t \\in \\{2,\\dots,T\\}$, the model is\n$$\nx_{t,g} \\mid \\mathbf{x}_{t-1}, \\mathbf{w}_g, \\sigma^2 \\sim \\mathcal{N}\\!\\left(\\sum_{h=1}^{G} w_{g,h}\\, x_{t-1,h},\\, \\sigma^2\\right),\n$$\nwith independent spike-and-slab priors encoding sparsity on the edge indicators. Let $z_{g,h} \\in \\{0,1\\}$ be the indicator of whether an edge from source gene $h$ to target gene $g$ exists. The prior factorizes as\n$$\nz_{g,h} \\stackrel{\\text{i.i.d.}}{\\sim} \\text{Bernoulli}(p), \\quad\nw_{g,h} \\mid z_{g,h} = 1 \\sim \\mathcal{N}(0,\\tau^2), \\quad\nw_{g,h} \\mid z_{g,h} = 0 = 0,\n$$\nwith hyperparameters $p \\in (0,1)$, $\\sigma^2 > 0$, and $\\tau^2 > 0$. For each target gene $g$, let $\\mathcal{S} \\subseteq \\{1,\\dots,G\\}$ denote a subset of active parents (those indices $h$ such that $z_{g,h} = 1$). Then, conditional on $\\mathcal{S}$, the regression weights $\\mathbf{w}_{g,\\mathcal{S}}$ have prior $\\mathcal{N}(\\mathbf{0}, \\tau^2 \\mathbf{I})$, and inactive weights are exactly zero.\n\nFrom the fundamental rules of probability and properties of Gaussian integrals, the marginal likelihood for target $g$ under parent set $\\mathcal{S}$ is obtained by integrating out the Gaussian regression weights:\n$$\np(\\mathbf{y}_g \\mid \\mathbf{X}, \\mathcal{S}, \\sigma^2, \\tau^2) = \\mathcal{N}\\!\\left(\\mathbf{y}_g; \\mathbf{0},\\, \\boldsymbol{\\Sigma}_{\\mathcal{S}}\\right), \\quad\n\\boldsymbol{\\Sigma}_{\\mathcal{S}} = \\sigma^2 \\mathbf{I} + \\tau^2 \\mathbf{X}_{\\mathcal{S}} \\mathbf{X}_{\\mathcal{S}}^\\top,\n$$\nwhere $\\mathbf{y}_g \\in \\mathbb{R}^{N}$ stacks the target values at times $t \\in \\{2,\\dots,T\\}$, $\\mathbf{X} \\in \\mathbb{R}^{N \\times G}$ stacks predictors at times $t-1 \\in \\{1,\\dots,T-1\\}$, $\\mathbf{X}_{\\mathcal{S}}$ denotes the submatrix of $\\mathbf{X}$ with columns restricted to indices in $\\mathcal{S}$, and $N = T-1$.\n\nUsing Bayes' rule with the independent Bernoulli prior on $\\mathcal{S}$,\n$$\n\\Pr(\\mathcal{S} \\mid \\mathbf{y}_g, \\mathbf{X}, p, \\sigma^2, \\tau^2) \\propto p^{|\\mathcal{S}|} (1-p)^{G-|\\mathcal{S}|} \\cdot p(\\mathbf{y}_g \\mid \\mathbf{X}, \\mathcal{S}, \\sigma^2, \\tau^2).\n$$\nThe posterior edge inclusion probability for edge $(h \\to g)$ is then\n$$\n\\Pr(z_{g,h} = 1 \\mid \\mathbf{y}_g, \\mathbf{X}, p, \\sigma^2, \\tau^2) \\;=\\; \\sum_{\\mathcal{S} \\subseteq \\{1,\\dots,G\\}: h \\in \\mathcal{S}} \\Pr(\\mathcal{S} \\mid \\mathbf{y}_g, \\mathbf{X}, p, \\sigma^2, \\tau^2).\n$$\n\nYour task is to implement a program that, given a fixed small dataset and a set of hyperparameters, enumerates all parent sets $\\mathcal{S}$ for each target gene $g$, computes the exact posterior edge inclusion probabilities using the conjugate marginal likelihood and the sparsity-encoding prior, and outputs a flattened matrix of posterior edge probabilities for all ordered pairs $(h \\to g)$.\n\nUse the following fixed dataset with $G = 3$ genes and $T = 6$ time points. The matrix below lists $\\mathbf{x}_t \\in \\mathbb{R}^3$ for $t \\in \\{1,\\dots,6\\}$ as rows in temporal order. All numbers are unitless real-valued expression levels. You must use these exact values:\n- Row $1$: $[\\,1.0,\\;0.2,\\;-0.5\\,]$\n- Row $2$: $[\\,0.7,\\;0.56,\\;-0.22\\,]$\n- Row $3$: $[\\,0.49,\\;0.518,\\;-0.38\\,]$\n- Row $4$: $[\\,0.343,\\;0.4004,\\;-0.3868\\,]$\n- Row $5$: $[\\,0.2401,\\;0.29162,\\;-0.3176\\,]$\n- Row $6$: $[\\,0.16807,\\;0.207536,\\;-0.238492\\,]$\n\nConstruct $\\mathbf{X}$ as the first $T-1$ rows (rows $1$ through $5$) and the response for target $g$ as $\\mathbf{y}_g$ from the corresponding column of rows $2$ through $6$.\n\nTo ensure numerical stability and universal applicability:\n- Compute the log marginal likelihood using the matrix determinant lemma and Woodbury identity to avoid inverting $\\boldsymbol{\\Sigma}_{\\mathcal{S}}$ directly:\n  - Let $\\mathbf{G}_{\\mathcal{S}} = \\mathbf{X}_{\\mathcal{S}}^\\top \\mathbf{X}_{\\mathcal{S}}$, and define $\\mathbf{B}_{\\mathcal{S}} = \\mathbf{I} + \\frac{\\tau^2}{\\sigma^2} \\mathbf{G}_{\\mathcal{S}}$.\n  - Then\n    $$\n    \\log \\lvert \\boldsymbol{\\Sigma}_{\\mathcal{S}} \\rvert = N \\log \\sigma^2 + \\log \\lvert \\mathbf{B}_{\\mathcal{S}} \\rvert,\n    $$\n    $$\n    \\mathbf{y}_g^\\top \\boldsymbol{\\Sigma}_{\\mathcal{S}}^{-1} \\mathbf{y}_g = \\frac{1}{\\sigma^2}\\left(\\mathbf{y}_g^\\top \\mathbf{y}_g - \\frac{\\tau^2}{\\sigma^2}\\,\\mathbf{y}_g^\\top \\mathbf{X}_{\\mathcal{S}} \\mathbf{B}_{\\mathcal{S}}^{-1} \\mathbf{X}_{\\mathcal{S}}^\\top \\mathbf{y}_g\\right).\n    $$\n- For each target $g$, enumerate all $2^G$ subsets $\\mathcal{S} \\subseteq \\{1,2,3\\}$ and compute unnormalized log posterior scores\n  $$\n  \\log \\pi(\\mathcal{S}) = |\\mathcal{S}| \\log p + (G-|\\mathcal{S}|)\\log(1-p) + \\log p(\\mathbf{y}_g \\mid \\mathbf{X}, \\mathcal{S}, \\sigma^2, \\tau^2),\n  $$\n  then normalize via log-sum-exp to obtain $\\Pr(\\mathcal{S} \\mid \\cdot)$ exactly.\n\nTest suite. Your program must compute the posterior edge inclusion probabilities for the following four hyperparameter settings, each applied to the fixed dataset above:\n- Case $1$: $p = 0.3$, $\\sigma^2 = 0.01$, $\\tau^2 = 1.0$.\n- Case $2$: $p = 0.05$, $\\sigma^2 = 0.02$, $\\tau^2 = 1.0$.\n- Case $3$: $p = 0.5$, $\\sigma^2 = 0.1$, $\\tau^2 = 0.1$.\n- Case $4$: $p = 0.5$, $\\sigma^2 = 0.0001$, $\\tau^2 = 10.0$.\n\nRequired output. For each case, produce a list of $G \\times G$ floating-point numbers equal to the posterior edge inclusion probabilities $\\Pr(z_{g,h} = 1 \\mid \\text{data})$ arranged in row-major order with source index $h \\in \\{1,2,3\\}$ as the outer loop and target index $g \\in \\{1,2,3\\}$ as the inner loop. That is, the flattened list is\n$$\n[\\,\\Pr(h{=}1 \\to g{=}1),\\;\\Pr(h{=}1 \\to g{=}2),\\;\\Pr(h{=}1 \\to g{=}3),\\;\\Pr(h{=}2 \\to g{=}1),\\dots,\\Pr(h{=}3 \\to g{=}3)\\,].\n$$\nRound each probability to exactly $6$ decimal places.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list of the four case results, where each case result is itself a comma-separated list enclosed in square brackets. For example, your output must look like\n$$\n[\\,[a_1,a_2,\\dots,a_9],\\,[b_1,\\dots,b_9],\\,[c_1,\\dots,c_9],\\,[d_1,\\dots,d_9]\\,],\n$$\nwith each $a_i$, $b_i$, $c_i$, $d_i$ a float rounded to exactly $6$ decimal places and no additional text. No physical units are involved in this task. Angles are not involved. Percentages must be expressed as decimals, not with a percent sign.",
            "solution": "The problem is valid. It presents a well-defined, scientifically sound, and computationally feasible task in Bayesian statistical inference for network structure learning. All necessary data, model parameters, and computational procedures are provided without ambiguity or contradiction.\n\nThe objective is to compute the posterior edge inclusion probabilities for a small gene regulatory network modeled as a first-order Dynamic Bayesian Network (DBN). The network has $G=3$ genes, and the expression data is provided over $T=6$ time points. The solution involves a full Bayesian analysis, enumerating all possible network structures and calculating their posterior probabilities.\n\n### Step 1: Data Preparation\nThe provided time series data consists of $T=6$ measurements for $G=3$ genes. Let this data be represented by a matrix $\\mathbf{D} \\in \\mathbb{R}^{T \\times G}$. The DBN model relates the expression at time $t$ to the expression at time $t-1$. We therefore construct a predictor matrix $\\mathbf{X} \\in \\mathbb{R}^{N \\times G}$ and a response matrix $\\mathbf{Y} \\in \\mathbb{R}^{N \\times G}$, where $N=T-1=5$.\nThe matrix $\\mathbf{X}$ consists of the first $T-1=5$ rows of $\\mathbf{D}$, representing the gene states at times $t \\in \\{1, 2, 3, 4, 5\\}$.\n$$\n\\mathbf{X} = \\begin{pmatrix}\n1.0 & 0.2 & -0.5 \\\\\n0.7 & 0.56 & -0.22 \\\\\n0.49 & 0.518 & -0.38 \\\\\n0.343 & 0.4004 & -0.3868 \\\\\n0.2401 & 0.29162 & -0.3176\n\\end{pmatrix}\n$$\nThe matrix $\\mathbf{Y}$ consists of the last $T-1=5$ rows of $\\mathbf{D}$, representing the gene states at times $t \\in \\{2, 3, 4, 5, 6\\}$. The $g$-th column of $\\mathbf{Y}$ serves as the target vector $\\mathbf{y}_g \\in \\mathbb{R}^N$ for gene $g$.\n$$\n\\mathbf{Y} = \\begin{pmatrix}\n0.7 & 0.56 & -0.22 \\\\\n0.49 & 0.518 & -0.38 \\\\\n0.343 & 0.4004 & -0.3868 \\\\\n0.2401 & 0.29162 & -0.3176 \\\\\n0.16807 & 0.207536 & -0.238492\n\\end{pmatrix}\n$$\n\n### Step 2: Bayesian Inference Framework\nThe core of the problem is to compute the posterior probability of each potential edge $(h \\to g)$. The model is specified such that, for a given target gene $g$, the inference for its parent set is independent of the other target genes. Therefore, we can perform the analysis for each target gene $g \\in \\{1, 2, 3\\}$ separately.\n\nFor each target gene $g$, there are $G=3$ potential parent genes (including self-regulation). This results in $2^G = 2^3 = 8$ possible parent sets $\\mathcal{S} \\subseteq \\{1, 2, 3\\}$. The task requires us to enumerate all these sets, calculate their posterior probabilities, and then marginalize to find the edge inclusion probabilities.\n\n### Step 3: Log Marginal Likelihood Calculation\nThe central quantity for Bayesian model comparison is the marginal likelihood, $p(\\mathbf{y}_g \\mid \\mathbf{X}, \\mathcal{S})$, obtained by integrating out the model parameters—in this case, the regression weights $\\mathbf{w}_g$. The problem provides the result of this integration for the specified spike-and-slab prior on the weights. The marginal likelihood for a parent set $\\mathcal{S}$ is given by a multivariate normal distribution:\n$$\np(\\mathbf{y}_g \\mid \\mathbf{X}, \\mathcal{S}, \\sigma^2, \\tau^2) = \\mathcal{N}(\\mathbf{y}_g; \\mathbf{0}, \\boldsymbol{\\Sigma}_{\\mathcal{S}})\n$$\nwhere the covariance matrix is $\\boldsymbol{\\Sigma}_{\\mathcal{S}} = \\sigma^2 \\mathbf{I} + \\tau^2 \\mathbf{X}_{\\mathcal{S}} \\mathbf{X}_{\\mathcal{S}}^\\top$. Here, $\\mathbf{X}_{\\mathcal{S}}$ is the submatrix of $\\mathbf{X}$ containing only the columns corresponding to the parent genes in $\\mathcal{S}$.\n\nThe log of this marginal likelihood is:\n$$\n\\log p(\\mathbf{y}_g \\mid \\cdot) = -\\frac{N}{2} \\log(2\\pi) - \\frac{1}{2}\\log |\\boldsymbol{\\Sigma}_{\\mathcal{S}}| - \\frac{1}{2} \\mathbf{y}_g^\\top \\boldsymbol{\\Sigma}_{\\mathcal{S}}^{-1} \\mathbf{y}_g\n$$\nTo compute this stably and efficiently, we use the provided identities derived from the matrix determinant lemma and the Woodbury matrix identity. Let $|\\mathcal{S}| = k$, $\\mathbf{G}_{\\mathcal{S}} = \\mathbf{X}_{\\mathcal{S}}^\\top \\mathbf{X}_{\\mathcal{S}} \\in \\mathbb{R}^{k \\times k}$, and $\\mathbf{B}_{\\mathcal{S}} = \\mathbf{I}_k + \\frac{\\tau^2}{\\sigma^2} \\mathbf{G}_{\\mathcal{S}}$. The components of the log-likelihood are:\n$$\n\\log |\\boldsymbol{\\Sigma}_{\\mathcal{S}}| = N \\log \\sigma^2 + \\log |\\mathbf{B}_{\\mathcal{S}}|\n$$\n$$\n\\mathbf{y}_g^\\top \\boldsymbol{\\Sigma}_{\\mathcal{S}}^{-1} \\mathbf{y}_g = \\frac{1}{\\sigma^2}\\left(\\mathbf{y}_g^\\top \\mathbf{y}_g - \\frac{\\tau^2}{\\sigma^2} \\mathbf{y}_g^\\top \\mathbf{X}_{\\mathcal{S}} \\mathbf{B}_{\\mathcal{S}}^{-1} \\mathbf{X}_{\\mathcal{S}}^\\top \\mathbf{y}_g\\right)\n$$\nFor the special case where the parent set is empty ($\\mathcal{S} = \\emptyset$, $k=0$), $\\mathbf{X}_{\\mathcal{S}}$ is an empty matrix. The formulas simplify correctly: $\\log |\\mathbf{B}_{\\emptyset}| = \\log(1) = 0$ and the term involving $\\mathbf{X}_{\\mathcal{S}}$ in the quadratic form vanishes. Thus, for $\\mathcal{S}=\\emptyset$:\n$$\n\\log |\\boldsymbol{\\Sigma}_{\\emptyset}| = N \\log \\sigma^2 \\quad \\text{and} \\quad \\mathbf{y}_g^\\top \\boldsymbol{\\Sigma}_{\\emptyset}^{-1} \\mathbf{y}_g = \\frac{1}{\\sigma^2} \\mathbf{y}_g^\\top \\mathbf{y}_g\n$$\n\n### Step 4: Posterior Probability of Parent Sets\nUsing Bayes' rule, the posterior probability of a parent set $\\mathcal{S}$ is proportional to its prior probability times its marginal likelihood:\n$$\n\\Pr(\\mathcal{S} \\mid \\mathbf{y}_g, \\mathbf{X}, \\dots) \\propto p(\\mathcal{S}) \\cdot p(\\mathbf{y}_g \\mid \\mathbf{X}, \\mathcal{S}, \\dots)\n$$\nThe prior on the structure is defined by independent Bernoulli trials for each edge: $p(\\mathcal{S}) = p^{|\\mathcal{S}|} (1-p)^{G-|\\mathcal{S}|}$.\n\nTo maintain numerical stability, we work with logarithms. The unnormalized log-posterior score for each set $\\mathcal{S}$ is:\n$$\n\\log \\pi(\\mathcal{S}) = |\\mathcal{S}| \\log p + (G - |\\mathcal{S}|) \\log(1-p) + \\log p(\\mathbf{y}_g \\mid \\mathbf{X}, \\mathcal{S}, \\dots)\n$$\nWe compute this score for all $2^G=8$ parent sets. These scores are then normalized to sum to one by using the log-sum-exp trick. Let $\\{\\mathcal{S}_i\\}_{i=1}^{2^G}$ be the collection of all parent sets. The posterior probability of a specific set $\\mathcal{S}_j$ is:\n$$\n\\Pr(\\mathcal{S}_j \\mid \\cdot) = \\frac{\\exp(\\log \\pi(\\mathcal{S}_j))}{\\sum_{i=1}^{2^G} \\exp(\\log \\pi(\\mathcal{S}_i))}\n$$\nNumerically, this is computed as $\\exp(\\log \\pi(\\mathcal{S}_j) - C)$, where $C = \\log(\\sum_{i} \\exp(\\log \\pi(\\mathcal{S}_i)))$ is the log-normalizing constant, itself computed via log-sum-exp.\n\n### Step 5: Posterior Edge Inclusion Probabilities\nThe final quantity of interest is the posterior probability that a specific edge $(h \\to g)$ exists, which is represented by the indicator $z_{g,h}=1$. This is found by summing the posterior probabilities of all parent sets $\\mathcal{S}$ that include gene $h$ as a parent for gene $g$:\n$$\n\\Pr(z_{g,h}=1 \\mid \\mathbf{y}_g, \\mathbf{X}, \\dots) = \\sum_{\\mathcal{S} : h \\in \\mathcal{S}} \\Pr(\\mathcal{S} \\mid \\mathbf{y}_g, \\mathbf{X}, \\dots)\n$$\n\n### Step 6: Algorithmic Procedure\nThe overall algorithm proceeds as follows for each of the four hyperparameter test cases:\n1. Initialize a $G \\times G$ matrix, `posterior_edge_probs`, to store the results, where `posterior_edge_probs[h, g]` will hold $\\Pr(z_{g,h}=1 \\mid \\text{data})$.\n2. For each target gene $g$ from $1$ to $G$:\n    a. Create a list to store the unnormalized log-posterior scores for all $2^G$ parent sets.\n    b. For each parent set $\\mathcal{S} \\subseteq \\{1, \\dots, G\\}$:\n        i. Extract the corresponding predictor submatrix $\\mathbf{X}_{\\mathcal{S}}$.\n        ii. Calculate the log marginal likelihood $\\log p(\\mathbf{y}_g \\mid \\mathbf{X}, \\mathcal{S}, \\dots)$ using the stable formulas.\n        iii. Calculate the log-prior $\\log p(\\mathcal{S})$.\n        iv. Sum these to get the unnormalized log-posterior $\\log \\pi(\\mathcal{S})$ and add it to the list.\n    c. Normalize the list of log-posteriors to obtain the posterior probabilities $\\Pr(\\mathcal{S} \\mid \\text{data})$ for all $\\mathcal{S}$.\n    d. For each potential source gene $h$ from $1$ to $G$:\n        i. Sum the probabilities $\\Pr(\\mathcal{S} \\mid \\text{data})$ for all sets $\\mathcal{S}$ containing $h$.\n        ii. Store this sum in `posterior_edge_probs[h-1, g-1]`.\n3. After iterating through all target genes, the `posterior_edge_probs` matrix is complete. Flatten this matrix in row-major order to conform to the specified output format: source index $h$ as the outer loop, target index $g$ as the inner.\n4. Round each probability to six decimal places and format the output string as required.\n\nThis procedure provides an exact solution to the Bayesian inference problem for the given small-scale network.",
            "answer": "```\n[[0.999818,0.000000,0.999979,0.302302,0.999995,0.000000,0.000021,0.000000,0.999745],[0.993437,0.000000,0.998637,0.057393,0.999867,0.000000,0.000000,0.000000,0.985651],[0.528434,0.499692,0.575239,0.501538,0.612847,0.499632,0.499989,0.499709,0.548483],[1.000000,0.000000,1.000000,0.500000,1.000000,0.000000,0.000000,0.000000,1.000000]]\n```"
        }
    ]
}