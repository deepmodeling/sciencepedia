{
    "hands_on_practices": [
        {
            "introduction": "随机建模的许多应用都始于能够精确求解的基石模型。本练习的核心是 Ornstein-Uhlenbeck 过程，这是一个描述系统在随机扰动下如何回归稳态的线性随机微分方程（SDE）。通过解析求解该方程并推导其均值和方差，您将掌握处理 SDE 的核心分析技术，并将其与描述概率密度演化的 Fokker-Planck 方程建立联系 ()。",
            "id": "4357844",
            "problem": "在系统生物医学中，一个关于单一基因产物浓度的稳态调节的简化模型里，与标称水平的微小偏差可以被一个由等效分子噪声驱动的线性随机微分方程（SDE）所建模。考虑一维SDE\n$$\ndX_t \\;=\\; -\\theta\\,X_t\\,dt \\;+\\; \\sigma\\,dW_t,\\qquad X_0 \\;=\\; x_0,\n$$\n其中 $W_t$ 是一个标准维纳过程，$\\theta \\gt 0$ 和 $\\sigma \\gt 0$ 是常数，且 $x_0 \\in \\mathbb{R}$。\n\n任务：\n- 仅从Ito积分的基本性质和线性SDE的标准技巧出发，推导出唯一强解 $X_t$ 的闭式表达式。\n- 计算均值 $\\mathbb{E}[X_t]$ 和方差 $\\mathrm{Var}(X_t)$，并将其明确表示为 $t$、$\\theta$、$\\sigma$ 和 $x_0$ 的函数。\n- 使用与漂移项 $a(x) = -\\theta x$ 和扩散系数 $b(x) = \\sigma$ 相关联的Kolmogorov前向方程（Fokker–Planck方程），证明 $X_t$ 的概率密度保持高斯性，并且其均值和方差服从与从SDE解中获得的常微分方程相一致的方程。\n\n将您的最终答案以包含 $\\mathbb{E}[X_t]$ 和 $\\mathrm{Var}(X_t)$ 的单行矩阵形式给出。不需要数值近似，也不需要单位。所有指数函数请使用 $\\exp(\\cdot)$ 函数表示。",
            "solution": "用户提供了一个有效的问题陈述。该问题要求解一个线性随机微分方程（SDE）的解、均值和方差，并使用相关的Fokker-Planck方程进行一致性检验。该SDE是一个标准的Ornstein-Uhlenbeck过程，是随机微积分及其应用中的一个基本模型。\n\n问题分为三个部分。我们将依次解决每个部分。\n\n给定的SDE是：\n$$dX_t = -\\theta X_t dt + \\sigma dW_t, \\quad X_0 = x_0$$\n其中 $\\theta > 0$ 和 $\\sigma > 0$ 是常数。\n\n### 第一部分：闭式解的推导\n\n我们寻求强解 $X_t$。这是一个线性SDE，可以使用积分因子法求解，这类似于求解线性常微分方程（ODE）的方法。我们定义一个新过程 $Y_t = f(t) X_t$，其中 $f(t)$ 是某个合适的函数。我们将Ito引理应用于函数 $g(t, x) = f(t)x$。其偏导数为：\n$$\\frac{\\partial g}{\\partial t} = f'(t)x, \\quad \\frac{\\partial g}{\\partial x} = f(t), \\quad \\frac{\\partial^2 g}{\\partial x^2} = 0$$\n根据Ito引理，微分 $dY_t = d(f(t)X_t)$ 由下式给出：\n$$dY_t = \\frac{\\partial g}{\\partial t} dt + \\frac{\\partial g}{\\partial x} dX_t + \\frac{1}{2} \\frac{\\partial^2 g}{\\partial x^2} (dX_t)^2$$\n由于 $dX_t$ 的二次变分为 $(dX_t)^2 = (\\sigma dW_t)^2 = \\sigma^2 dt$，并且 $\\frac{\\partial^2 g}{\\partial x^2} = 0$，最后一项为零。代入偏导数和 $dX_t$ 的表达式：\n$$dY_t = f'(t)X_t dt + f(t)(-\\theta X_t dt + \\sigma dW_t)$$\n$$dY_t = (f'(t) - \\theta f(t))X_t dt + \\sigma f(t) dW_t$$\n为了简化方程，我们选择积分因子 $f(t)$ 使得 $X_t dt$ 项的系数为零。\n$$f'(t) - \\theta f(t) = 0$$\n这是一个关于 $f(t)$ 的一阶线性ODE。解为 $f(t) = C \\exp(\\theta t)$，其中 $C$ 是某个常数。我们可以设 $C=1$，因此我们的积分因子是 $f(t) = \\exp(\\theta t)$。\n\n做出这个选择后，$Y_t = \\exp(\\theta t) X_t$ 的SDE简化为：\n$$d(\\exp(\\theta t) X_t) = \\sigma \\exp(\\theta t) dW_t$$\n我们对两边从 $s=0$ 到 $s=t$ 进行积分：\n$$\\int_0^t d(\\exp(\\theta s) X_s) = \\int_0^t \\sigma \\exp(\\theta s) dW_s$$\n$$\\exp(\\theta t) X_t - \\exp(\\theta \\cdot 0) X_0 = \\sigma \\int_0^t \\exp(\\theta s) dW_s$$\n代入 $X_0 = x_0$：\n$$\\exp(\\theta t) X_t - x_0 = \\sigma \\int_0^t \\exp(\\theta s) dW_s$$\n对 $X_t$ 求解，我们得到唯一强解：\n$$X_t = x_0 \\exp(-\\theta t) + \\sigma \\exp(-\\theta t) \\int_0^t \\exp(\\theta s) dW_s$$\n另一种写法是通过改变随机积分中的积分变量：\n$$X_t = x_0 \\exp(-\\theta t) + \\sigma \\int_0^t \\exp(-\\theta(t-s)) dW_s$$\n\n### 第二部分：均值和方差的计算\n\n得到 $X_t$ 的显式解后，我们可以计算其统计矩。\n\n**均值 $\\mathbb{E}[X_t]$：**\n我们对解取期望。根据期望算子的线性性：\n$$\\mathbb{E}[X_t] = \\mathbb{E}\\left[ x_0 \\exp(-\\theta t) + \\sigma \\int_0^t \\exp(-\\theta(t-s)) dW_s \\right]$$\n$$\\mathbb{E}[X_t] = \\mathbb{E}[x_0 \\exp(-\\theta t)] + \\mathbb{E}\\left[ \\sigma \\int_0^t \\exp(-\\theta(t-s)) dW_s \\right]$$\n由于 $x_0$、$\\theta$ 和 $t$ 是确定性的，第一项是 $\\mathbb{E}[x_0 \\exp(-\\theta t)] = x_0 \\exp(-\\theta t)$。\n第二项涉及一个Ito积分的期望。对于确定性被积函数 $g(s) = \\sigma \\exp(-\\theta(t-s))$，Ito积分的均值为零：\n$$\\mathbb{E}\\left[ \\int_0^t g(s) dW_s \\right] = 0$$\n因此，第二项为零。\n$$\\mathbb{E}[X_t] = x_0 \\exp(-\\theta t)$$\n\n**方差 $\\mathrm{Var}(X_t)$：**\n方差定义为 $\\mathrm{Var}(X_t) = \\mathbb{E}[(X_t - \\mathbb{E}[X_t])^2]$。\n首先，我们计算与均值的偏差：\n$$X_t - \\mathbb{E}[X_t] = \\left( x_0 \\exp(-\\theta t) + \\sigma \\int_0^t \\exp(-\\theta(t-s)) dW_s \\right) - x_0 \\exp(-\\theta t)$$\n$$X_t - \\mathbb{E}[X_t] = \\sigma \\int_0^t \\exp(-\\theta(t-s)) dW_s$$\n现在，我们计算方差：\n$$\\mathrm{Var}(X_t) = \\mathbb{E}\\left[ \\left( \\sigma \\int_0^t \\exp(-\\theta(t-s)) dW_s \\right)^2 \\right]$$\n$$\\mathrm{Var}(X_t) = \\sigma^2 \\mathbb{E}\\left[ \\left( \\int_0^t \\exp(-\\theta(t-s)) dW_s \\right)^2 \\right]$$\n我们使用Ito等距性质，该性质表明，对于一个确定性的平方可积函数 $g(s)$，有 $\\mathbb{E}[(\\int_0^t g(s)dW_s)^2] = \\int_0^t \\mathbb{E}[g(s)^2]ds = \\int_0^t g(s)^2 ds$。\n在这里，我们的被积函数是 $g(s) = \\exp(-\\theta(t-s))$。\n$$\\mathrm{Var}(X_t) = \\sigma^2 \\int_0^t \\left( \\exp(-\\theta(t-s)) \\right)^2 ds = \\sigma^2 \\int_0^t \\exp(-2\\theta(t-s)) ds$$\n$$\\mathrm{Var}(X_t) = \\sigma^2 \\exp(-2\\theta t) \\int_0^t \\exp(2\\theta s) ds$$\n计算该积分：\n$$\\int_0^t \\exp(2\\theta s) ds = \\left[ \\frac{1}{2\\theta} \\exp(2\\theta s) \\right]_0^t = \\frac{1}{2\\theta}(\\exp(2\\theta t) - 1)$$\n将此结果代回方差的表达式中：\n$$\\mathrm{Var}(X_t) = \\sigma^2 \\exp(-2\\theta t) \\left( \\frac{\\exp(2\\theta t) - 1}{2\\theta} \\right)$$\n$$\\mathrm{Var}(X_t) = \\frac{\\sigma^2}{2\\theta} (1 - \\exp(-2\\theta t))$$\n\n### 第三部分：Fokker-Planck方程分析\n\nKolmogorov前向方程，或称Fokker-Planck方程（FPE），描述了过程 $X_t$ 的概率密度函数 $p(x,t)$ 的时间演化。对于一个一般的SDE $dX_t = a(X_t,t)dt + b(X_t,t)dW_t$，FPE为：\n$$\\frac{\\partial p}{\\partial t} = -\\frac{\\partial}{\\partial x}[a(x,t)p(x,t)] + \\frac{1}{2}\\frac{\\partial^2}{\\partial x^2}[b(x,t)^2 p(x,t)]$$\n在我们的例子中，漂移项是 $a(x) = -\\theta x$，扩散系数是 $b(x) = \\sigma$。FPE变为：\n$$\\frac{\\partial p}{\\partial t} = -\\frac{\\partial}{\\partial x}[-\\theta x p] + \\frac{1}{2}\\frac{\\partial^2}{\\partial x^2}[\\sigma^2 p] = \\theta \\frac{\\partial}{\\partial x}(x p) + \\frac{\\sigma^2}{2} \\frac{\\partial^2 p}{\\partial x^2}$$\n展开右边的第一项得到 $\\theta(p + x \\frac{\\partial p}{\\partial x})$。\n\n解 $X_t$ 是一个确定性项与一个高斯过程（Ito积分）之和。因此，$X_t$ 本身也是一个高斯过程。我们假设对于 $t > 0$，密度 $p(x,t)$ 是高斯分布的：\n$$p(x,t) = \\frac{1}{\\sqrt{2\\pi V(t)}} \\exp\\left( -\\frac{(x - \\mu(t))^2}{2V(t)} \\right)$$\n其中 $\\mu(t) = \\mathbb{E}[X_t]$ 且 $V(t) = \\mathrm{Var}(X_t)$。我们将此拟设代入FPE，以求得关于 $\\mu(t)$ 和 $V(t)$ 的ODE。处理 $\\ln p$ 会比较方便：\n$$\\ln p = -\\frac{1}{2}\\ln(2\\pi V) - \\frac{(x-\\mu)^2}{2V}$$\n由此，我们求出偏导数：\n$$\\frac{\\partial p}{\\partial x} = p \\frac{\\partial(\\ln p)}{\\partial x} = p \\left( -\\frac{x-\\mu}{V} \\right)$$\n$$\\frac{\\partial^2 p}{\\partial x^2} = \\frac{\\partial p}{\\partial x}\\left(-\\frac{x-\\mu}{V}\\right) + p\\left(-\\frac{1}{V}\\right) = p\\left(\\frac{(x-\\mu)^2}{V^2} - \\frac{1}{V}\\right)$$\n$$\\frac{\\partial p}{\\partial t} = p \\frac{\\partial(\\ln p)}{\\partial t} = p \\left( -\\frac{\\dot{V}}{2V} + \\frac{(x-\\mu)^2 \\dot{V}}{2V^2} + \\frac{(x-\\mu)\\dot{\\mu}}{V} \\right)$$\n现在我们将这些代入FPE，$\\frac{\\partial p}{\\partial t} = \\theta(p + x \\frac{\\partial p}{\\partial x}) + \\frac{\\sigma^2}{2} \\frac{\\partial^2 p}{\\partial x^2}$，然后两边同除以 $p(x,t) \\neq 0$：\n$$-\\frac{\\dot{V}}{2V} + \\frac{\\dot{V}}{2V^2}(x-\\mu)^2 + \\frac{\\dot{\\mu}}{V}(x-\\mu) = \\theta + \\theta x \\left(-\\frac{x-\\mu}{V}\\right) + \\frac{\\sigma^2}{2} \\left(\\frac{(x-\\mu)^2}{V^2} - \\frac{1}{V}\\right)$$\n为了按 $(x-\\mu)$ 的幂次对各项进行分组，我们将 $x$ 重写为 $(x-\\mu)+\\mu$：\n$$\\theta x \\left(-\\frac{x-\\mu}{V}\\right) = -\\frac{\\theta}{V}((x-\\mu)+\\mu)(x-\\mu) = -\\frac{\\theta}{V}(x-\\mu)^2 - \\frac{\\theta\\mu}{V}(x-\\mu)$$\nFPE变为：\n$$-\\frac{\\dot{V}}{2V} + \\frac{\\dot{\\mu}}{V}(x-\\mu) + \\frac{\\dot{V}}{2V^2}(x-\\mu)^2 = \\left(\\theta - \\frac{\\sigma^2}{2V}\\right) - \\frac{\\theta\\mu}{V}(x-\\mu) + \\left(\\frac{\\sigma^2}{2V^2}-\\frac{\\theta}{V}\\right)(x-\\mu)^2$$\n该等式必须对所有 $x \\in \\mathbb{R}$ 成立。因此，我们可以令 $(x-\\mu)$ 各次幂的系数相等：\n\\begin{itemize}\n    \\item $(x-\\mu)^1$的系数：$\\displaystyle \\frac{\\dot{\\mu}}{V} = -\\frac{\\theta\\mu}{V} \\implies \\dot{\\mu} = -\\theta\\mu$\n    \\item $(x-\\mu)^2$的系数：$\\displaystyle \\frac{\\dot{V}}{2V^2} = \\frac{\\sigma^2}{2V^2} - \\frac{\\theta}{V} \\implies \\dot{V} = \\sigma^2 - 2\\theta V$\n    \\item $(x-\\mu)^0$的系数：$\\displaystyle -\\frac{\\dot{V}}{2V} = \\theta - \\frac{\\sigma^2}{2V} \\implies \\dot{V} = -2\\theta V + \\sigma^2$\n\\end{itemize}\n从二次项和常数项得到的关于 $\\dot{V}$ 的方程是相同的，这表明高斯拟设是一致的。现在我们求解关于 $\\mu(t)$ 和 $V(t)$ 的ODE。\n\n对于均值 $\\mu(t)$，初始条件为 $\\mu(0) = x_0$：\n$$\\frac{d\\mu}{dt} = -\\theta\\mu \\implies \\mu(t) = \\mu(0)\\exp(-\\theta t) = x_0\\exp(-\\theta t)$$\n对于方差 $V(t)$，初始条件为 $V(0) = 0$（因为 $X_0=x_0$ 是一个确定性值）：\n$$\\frac{dV}{dt} + 2\\theta V = \\sigma^2$$\n这是一个一阶线性ODE。其解为：\n$$V(t) = V(0)\\exp(-2\\theta t) + \\frac{\\sigma^2}{2\\theta}(1 - \\exp(-2\\theta t))$$\n$$V(t) = 0 \\cdot \\exp(-2\\theta t) + \\frac{\\sigma^2}{2\\theta}(1 - \\exp(-2\\theta t)) = \\frac{\\sigma^2}{2\\theta}(1 - \\exp(-2\\theta t))$$\n从Fokker-Planck方程推导出的 $\\mu(t)$ 和 $V(t)$ 的结果，与直接从SDE解计算出的均值 $\\mathbb{E}[X_t]$ 和方差 $\\mathrm{Var}(X_t)$ 完全相同。这证实了 $X_t$ 的概率密度确实是具有这些矩的高斯分布。\n\n最终要求的量是作为时间函数的均值和方差。\n均值：$\\mathbb{E}[X_t] = x_0 \\exp(-\\theta t)$\n方差：$\\mathrm{Var}(X_t) = \\frac{\\sigma^2}{2\\theta}(1 - \\exp(-2\\theta t))$",
            "answer": "$$\\boxed{\\begin{pmatrix} x_0 \\exp(-\\theta t) & \\frac{\\sigma^2}{2\\theta} (1 - \\exp(-2\\theta t)) \\end{pmatrix}}$$"
        },
        {
            "introduction": "许多生物过程，如基因表达，本质上是离散和随机的，而随机微分方程通常是其近似。本练习将带您深入底层机制，通过实现精确的随机模拟算法（Gillespie 算法）来模拟单个 mRNA 分子的生成与降解过程 ()。通过将模拟结果与确定性常微分方程(ODE)的预测进行比较，您将直观地理解“内在噪音”的概念及其在生物系统中的重要性。",
            "id": "4357865",
            "problem": "考虑一个在系统生物医学中用于基因表达的典型信使核糖核酸（mRNA）生灭模型，该模型表示为非负整数上的连续时间马尔可夫链，状态为 $X(t) \\in \\{0,1,2,\\dots\\}$。该系统有两种反应：转录（迁入）和降解（死亡）。对于任何状态 $x \\in \\mathbb{N}_0$，反应及其倾向函数定义如下：\n- 转录：$\\varnothing \\rightarrow \\text{mRNA}$，倾向为 $a_1(x) = k$，其中 $k \\ge 0$ 是转录速率。\n- 降解：$\\text{mRNA} \\rightarrow \\varnothing$，倾向为 $a_2(x) = \\gamma x$，其中 $\\gamma \\ge 0$ 是单分子降解速率。\n\n你需要使用精确的随机模拟算法（SSA），也称为Gillespie直接法，从一个初始整数拷贝数 $X(0)=x_0$ 开始，模拟该系统直至一个固定的终止时间 $T > 0$。对于每组参数，执行 $R$ 次独立同分布的样本路径模拟，并计算在时间 $T$ 时的以下统计量：\n- $R$ 条路径上 $X(T)$ 的经验样本均值 $\\widehat{m}(T)$。\n- $R$ 条路径上 $X(T)$ 的经验无偏样本方差 $\\widehat{v}(T)$，计算时分母为 $R-1$。\n\n另外，计算由一阶矩方程支配的确定性平均场常微分方程（ODE）解 $x_{\\text{det}}(t)$\n$$\n\\frac{d}{dt} x_{\\text{det}}(t) = k - \\gamma \\, x_{\\text{det}}(t), \\quad x_{\\text{det}}(0)=x_0,\n$$\n并计算 $x_{\\text{det}}(T)$。该ODE提供了期望拷贝数的平均场预测，但预测的方差为零。\n\n为每组参数定义以下量化指标：\n- 时间 $T$ 时的平均绝对误差：$E_m = \\left| \\widehat{m}(T) - x_{\\text{det}}(T) \\right|$。\n- 时间 $T$ 时的解析方差 $v_{\\text{theory}}(T)$，由迁入-死亡过程的第一性原理定义：\n  - 如果 $\\gamma > 0$，\n    $$\n    v_{\\text{theory}}(T) = x_0 \\, e^{-\\gamma T} \\bigl(1 - e^{-\\gamma T}\\bigr) + \\frac{k}{\\gamma}\\bigl(1 - e^{-\\gamma T}\\bigr).\n    $$\n  - 如果 $\\gamma = 0$，\n    $$\n    v_{\\text{theory}}(T) = k\\,T.\n    $$\n- 时间 $T$ 时的方差绝对误差：$E_v = \\left| \\widehat{v}(T) - v_{\\text{theory}}(T) \\right|$。\n- 相对于平均场预测（其值为零）的方差差异，定义为 $D_v = \\widehat{v}(T)$。\n\n算法要求：\n- 实现精确的随机模拟算法（Gillespie直接法）。在任意状态 $x$ 和时间 $t$，计算总倾向 $a_0(x) = a_1(x)+a_2(x) = k + \\gamma x$。如果 $a_0(x) = 0$，则系统处于吸收态，不再发生反应。否则，从速率为 $a_0(x)$ 的指数分布中抽取反应间时间 $\\tau$，并将时间推进 $\\tau$。如果 $t+\\tau > T$，则停止并将当前状态记录为终止状态。否则，通过抽取一个均匀分布的随机变量 $u \\in (0,1)$ 来选择反应类型；如果 $u < a_1(x)/a_0(x)$，则选择转录，否则选择降解。相应地更新状态，并迭代直至 $t \\ge T$。\n- 对于确定性平均场，解析求解线性ODE：\n  - 如果 $\\gamma > 0$，$x_{\\text{det}}(T) = x_0 e^{-\\gamma T} + \\dfrac{k}{\\gamma}\\bigl(1 - e^{-\\gamma T}\\bigr)$。\n  - 如果 $\\gamma = 0$，$x_{\\text{det}}(T) = x_0 + k\\,T$。\n- 使用固定的伪随机数生成器种子 $1729$ 以确保可复现性。\n\n测试套件：\n为以下四组参数提供结果，每组由 $(k,\\gamma,x_0,T,R)$ 指定：\n- 情况 1：$(k=\\;5.0,\\;\\gamma=\\;1.0,\\;x_0=\\;10,\\;T=\\;2.0,\\;R=\\;3000)$。\n- 情况 2：$(k=\\;0.0,\\;\\gamma=\\;0.7,\\;x_0=\\;25,\\;T=\\;3.5,\\;R=\\;3000)$。\n- 情况 3：$(k=\\;12.0,\\;\\gamma=\\;4.0,\\;x_0=\\;0,\\;T=\\;1.5,\\;R=\\;3000)$。\n- 情况 4：$(k=\\;5.0,\\;\\gamma=\\;0.0,\\;x_0=\\;3,\\;T=\\;1.2,\\;R=\\;3000)$。\n\n你的程序必须：\n- 精确地按照规定实现随机模拟算法。\n- 对于每种情况，从 $R$ 条独立轨迹中估计 $\\widehat{m}(T)$ 和 $\\widehat{v}(T)$，计算 $x_{\\text{det}}(T)$ 和 $v_{\\text{theory}}(T)$，然后计算三元组 $(E_m, E_v, D_v)$。\n- 生成单行输出，其中包含一个逗号分隔的列表，所有结果按情况1到4的顺序连接，每种情况按 $(E_m, E_v, D_v)$ 的顺序贡献其三个数值。输出必须是以下形式的单个列表字符串\n  $$\n  [E_m^{(1)},E_v^{(1)},D_v^{(1)},E_m^{(2)},E_v^{(2)},D_v^{(2)},E_m^{(3)},E_v^{(3)},D_v^{(3)},E_m^{(4)},E_v^{(4)},D_v^{(4)}].\n  $$\n\n本问题中没有物理单位；拷贝数是无量纲的计数，速率是单位时间内的比率。角度和百分比不会出现。输出值必须是实数（浮点数）。不需要用户输入；程序必须按原样运行并按规定打印最后一行。",
            "solution": "该问题要求使用精确的随机模拟算法（SSA），也称为Gillespie直接法，来模拟一个典型的mRNA生灭过程。随机模拟的结果将与该过程均值和方差的解析解进行比较。\n\n该系统由一个连续时间马尔可夫链描述，其状态 $X(t)$ 代表在时间 $t$ 的mRNA分子数量。状态空间为非负整数集 $\\mathbb{N}_0 = \\{0, 1, 2, \\dots\\}$。控制系统动态的两个反应是：\n1.  **转录（出生/迁入）：** $\\varnothing \\rightarrow \\text{mRNA}$，以恒定的倾向 $a_1(x) = k$ 发生。此反应使分子数增加一。\n2.  **降解（死亡）：** $\\text{mRNA} \\rightarrow \\varnothing$，以与分子数成正比的倾向 $a_2(x) = \\gamma x$ 发生。此反应使分子数减少一。\n\n参数 $k \\ge 0$ 和 $\\gamma \\ge 0$ 分别是转录速率和单分子降解速率。\n\n### 随机模拟算法（Gillespie直接法）\n\nSSA提供了一种统计上精确的方法来模拟这类随机系统的时间演化。从时间 $t=0$ 时的初始状态 $X(0) = x_0$ 开始，算法按以下步骤进行：\n\n1.  **计算倾向：** 在给定状态 $x$下，任何反应发生的总倾向是各个倾向的总和：\n    $$\n    a_0(x) = a_1(x) + a_2(x) = k + \\gamma x.\n    $$\n    如果 $a_0(x) = 0$（当且仅当 $k=0$ 且 $x=0$ 时发生），系统处于吸收态，不再有进一步的反应发生。该轨迹的模拟终止。\n\n2.  **抽取到下一次反应的时间：** 到下一次反应的时间间隔 $\\tau$ 是一个从速率参数为 $a_0(x)$ 的指数分布中抽取的随机变量：\n    $$\n    \\tau \\sim \\text{Exponential}(a_0(x)).\n    $$\n    概率密度函数为 $p(\\tau) = a_0(x) e^{-a_0(x)\\tau}$。模拟时间按此间隔推进，$t \\leftarrow t + \\tau$。如果新时间 $t+\\tau$ 超过指定的终止时间 $T$，则模拟停止，并将系统的状态记录为其在时间 $T$ 的值。\n\n3.  **抽取发生哪种反应：** 根据其对总倾向的相对贡献来选择下一个发生的反应。从 $(0, 1)$ 上的均匀分布中抽取一个随机数 $u$。\n    -   如果 $u < \\frac{a_1(x)}{a_0(x)} = \\frac{k}{k+\\gamma x}$，则发生转录，状态更新为：$x \\leftarrow x+1$。\n    -   否则，发生降解，状态更新为：$x \\leftarrow x-1$。\n\n这个计算倾向、抽取时间以及抽取反应类型的过程会一直迭代，直到模拟时间 $t$ 达到或超过终止时间 $T$。\n\n为了获得稳健的统计数据，整个模拟过程会对 $R$ 条独立轨迹重复进行。从得到的最终状态样本 $\\{X_1(T), X_2(T), \\dots, X_R(T)\\}$ 中，我们计算：\n-   经验样本均值：$\\widehat{m}(T) = \\frac{1}{R} \\sum_{i=1}^{R} X_i(T)$。\n-   经验无偏样本方差：$\\widehat{v}(T) = \\frac{1}{R-1} \\sum_{i=1}^{R} (X_i(T) - \\widehat{m}(T))^2$。\n\n### 解析解\n\n倾向的线性性质使得我们可以推导出 $X(t)$ 分布矩的精确解析表达式。\n\n**平均场ODE解：**\n$X(t)$ 期望值的时间演化（记为 $\\mathbb{E}[X(t)]$）可以通过一个确定性常微分方程（ODE）来近似，该方程通常被称为平均场方程或速率方程。这是通过考虑平均变化率推导出来的：\n$$\n\\frac{d}{dt} \\mathbb{E}[X(t)] \\approx k - \\gamma \\mathbb{E}[X(t)].\n$$\n在初始条件 $x_{\\text{det}}(0) = x_0$下，求解这个关于 $x_{\\text{det}}(t) = \\mathbb{E}[X(t)]$ 的线性ODE，会根据 $\\gamma$ 的值得到不同的解：\n-   如果 $\\gamma > 0$：\n    $$\n    x_{\\text{det}}(T) = x_0 e^{-\\gamma T} + \\frac{k}{\\gamma}\\bigl(1 - e^{-\\gamma T}\\bigr).\n    $$\n-   如果 $\\gamma = 0$：\n    $$\n    x_{\\text{det}}(T) = x_0 + k T.\n    $$\n\n**精确解析方差：**\n过程的精确方差 $v_{\\text{theory}}(T) = \\text{Var}(X(T))$ 也可以从化学主方程推导出来。解为：\n-   如果 $\\gamma > 0$：\n    $$\n    v_{\\text{theory}}(T) = x_0 e^{-\\gamma T} \\bigl(1 - e^{-\\gamma T}\\bigr) + \\frac{k}{\\gamma}\\bigl(1 - e^{-\\gamma T}\\bigr).\n    $$\n    值得注意的是，对于这个特定的过程，方差可以通过 $v_{\\text{theory}}(T) = x_{\\text{det}}(T) - x_0 e^{-2\\gamma T}$ 与均值相关联。\n-   如果 $\\gamma = 0$（一个简单的出生泊松过程）：\n    $$\n    v_{\\text{theory}}(T) = k T.\n    $$\n\n### 指标计算\n\n对于每组参数，计算以下指标以量化随机模拟结果与理论预测之间的关系：\n1.  **平均绝对误差：** $E_m = \\left| \\widehat{m}(T) - x_{\\text{det}}(T) \\right|$。这衡量了经验估计均值与解析预测均值之间的偏差。\n2.  **方差绝对误差：** $E_v = \\left| \\widehat{v}(T) - v_{\\text{theory}}(T) \\right|$。这衡量了经验估计方差与精确解析方差之间的偏差。\n3.  **方差差异：** $D_v = \\widehat{v}(T)$。这就是估计的方差本身，用作随机波动的度量，这在确定性平均场模型中是完全不存在的（其中方差默认为零）。\n\n实现将首先定义一个运行单条SSA轨迹的函数，然后定义一个函数来为给定的参数集组织 $R$ 次这样的运行，计算经验统计量，计算解析解，最后推导出三个所需的指标。对所有四个测试用例重复此过程，并使用固定的随机数生成器种子以保证可复现性。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run simulations for all test cases and print results.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (k, gamma, x0, T, R)\n        (5.0, 1.0, 10, 2.0, 3000),  # Case 1\n        (0.0, 0.7, 25, 3.5, 3000),  # Case 2\n        (12.0, 4.0, 0, 1.5, 3000),  # Case 3\n        (5.0, 0.0, 3, 1.2, 3000),   # Case 4\n    ]\n\n    # Initialize a single random number generator with the specified seed for reproducibility.\n    rng = np.random.default_rng(1729)\n    \n    all_results = []\n    for case in test_cases:\n        k, gamma, x0, T, R = case\n        \n        # --- Stochastic Simulation (SSA) ---\n        final_states = np.empty(R, dtype=int)\n        for i in range(R):\n            final_states[i] = _run_single_ssa_trajectory(k, gamma, x0, T, rng)\n        \n        # Compute empirical statistics\n        m_hat_T = np.mean(final_states)\n        v_hat_T = np.var(final_states, ddof=1) if R > 1 else 0.0\n\n        # --- Analytical Solutions ---\n        # Deterministic mean-field solution\n        if gamma > 0:\n            x_det_T = x0 * np.exp(-gamma * T) + (k / gamma) * (1.0 - np.exp(-gamma * T))\n        else: # gamma == 0\n            x_det_T = x0 + k * T\n            \n        # Analytical variance\n        if gamma > 0:\n            v_theory_T = x0 * np.exp(-gamma * T) * (1.0 - np.exp(-gamma * T)) + (k / gamma) * (1.0 - np.exp(-gamma * T))\n        else: # gamma == 0\n            v_theory_T = k * T\n            \n        # --- Compute Metrics ---\n        E_m = np.abs(m_hat_T - x_det_T)\n        E_v = np.abs(v_hat_T - v_theory_T)\n        D_v = v_hat_T\n        \n        all_results.extend([E_m, E_v, D_v])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\ndef _run_single_ssa_trajectory(k, gamma, x0, T, rng):\n    \"\"\"\n    Implements the Gillespie direct method for a single trajectory of the mRNA birth-death model.\n    \n    Args:\n        k (float): Transcription rate.\n        gamma (float): Degradation rate.\n        x0 (int): Initial number of molecules.\n        T (float): Terminal time.\n        rng (numpy.random.Generator): The random number generator instance.\n        \n    Returns:\n        int: The number of molecules at time T.\n    \"\"\"\n    t = 0.0\n    x = x0\n    \n    while t < T:\n        # Calculate total propensity a0\n        a0 = k + gamma * x\n        \n        # If a0 is zero, no more reactions can occur (absorbing state)\n        if a0 <= 0:\n            break\n        \n        # Sample time to next reaction, tau\n        # tau ~ Exponential(rate=a0) which is equivalent to Exponential(scale=1/a0)\n        tau = rng.exponential(scale=1.0 / a0)\n        \n        # If the next reaction occurs after T, the simulation ends.\n        if t + tau > T:\n            break\n        \n        # Advance time\n        t += tau\n        \n        # Choose which reaction occurs\n        # Draw a uniform random number u\n        u = rng.random()\n        \n        # Probability of transcription is a1/a0 = k/a0\n        if u < k / a0:\n            # Transcription event\n            x += 1\n        else:\n            # Degradation event\n            # Ensure x does not go below 0 (though this is guaranteed by a2=gamma*x)\n            if x > 0:\n                x -= 1\n    \n    # The state at the end of the simulation is the current value of x\n    return x\n\n# Execute the main function\nsolve()\n```"
        },
        {
            "introduction": "理论模型和模拟的最终试金石是它们解释实验数据的能力。本练习将理论与实践相结合，要求您分析模拟的单细胞基因表达计数数据 ()。您将使用最大似然估计法为数据拟合两种常用模型（泊松分布和负二项分布），并应用赤池信息准则（$AIC$）和贝叶斯信息准则（$BIC$）来判断哪种模型更能描述观察到的变异性，这是系统生物学中一项至关重要的数据分析技能。",
            "id": "4357822",
            "problem": "考虑在独立的单细胞分析中测量的单个基因，产生非负整数计数 $\\{x_1, x_2, \\dots, x_n\\}$，其中每个 $x_i \\in \\{0,1,2,\\dots\\}$。您将比较两种针对这些计数的随机模型：一个参数为 $\\lambda$ 的泊松模型，以及一个由均值 $\\mu$ 和正离散（形状）参数 $r$ 参数化的负二项模型。假设计数是独立同分布的。您的任务是实现一个程序，对于每个提供的测试用例，计算每个模型下模型参数的最大似然估计（Maximum Likelihood Estimation, MLE），计算每个拟合模型的赤池信息准则（Akaike Information Criterion, AIC）和贝叶斯信息准则（Bayesian Information Criterion, BIC），并根据每个准则输出模型偏好。\n\n此问题的基本依据如下。从每个模型下独立观测值的似然定义，以及最大似然估计（MLE）原则出发，该原则选择使似然（等价于对数似然）最大化的参数。使用赤池信息准则（AIC）和贝叶斯信息准则（BIC）的公认定义，两者都通过估计的自由参数数量来惩罚模型的复杂性。决策将通过比较同一数据下不同模型的这些准则值来做出，优先选择值较小的模型；如果在小的数值容差内出现平局，则优先选择参数较少的更简单的模型。\n\n对于负二项模型，使用均值为 $\\mu$ 和离散度为 $r>0$ 的参数化，其对观测值 $x \\in \\{0,1,2,\\dots\\}$ 的概率质量函数为\n$$\n\\mathbb{P}(X=x \\mid r,\\mu) \\;=\\; \\binom{x+r-1}{x} \\left(\\frac{r}{r+\\mu}\\right)^r \\left(\\frac{\\mu}{r+\\mu}\\right)^x,\n$$\n其均值为 $\\mu$，方差为 $\\mu \\left(1 + \\frac{\\mu}{r}\\right)$。\n\n对于每个数据集，将泊松模型视为具有 $k_{\\text{Pois}} = 1$ 个自由参数，负二项模型视为具有 $k_{\\text{NB}} = 2$ 个自由参数。当基于 AIC 或 BIC 比较模型时，优先选择准则值较小的模型；如果两个值在 $\\varepsilon = 10^{-8}$ 的容差内相等，则选择泊松模型作为更简约的模型。\n\n不涉及物理单位。所有输出都应为纯整数。不需要百分比或角度单位。您的程序必须在没有封闭形式 MLE 的情况下实现数值优化，并必须确保在评估对数似然时的数值稳定性。\n\n使用以下包含五个不同数据集的测试套件，每个数据集代表来自不同数量细胞的单个基因的计数：\n- 测试用例 $1$：$\\{0,1,1,2,1,0,1,2,1,1,2,1,3,0\\}$。\n- 测试用例 $2$：$\\{0,0,5,10,0,3,20,0,7,15,0,1,12\\}$。\n- 测试用例 $3$：$\\{0,0,0,0,0,0,0,0,0\\}$。\n- 测试用例 $4$：$\\{4,5,6,5,4,5,6,5,5,4,6\\}$。\n- 测试用例 $5$：$\\{0,50,0,0,100,2,3,70,90,0,150,1,0\\}$。\n\n您的程序应生成单行输出，其中包含所有测试用例的汇总结果，形式为方括号括起来的逗号分隔列表。对于每个测试用例，输出一个双元素列表 $[a,b]$，其中 $a$ 是 AIC 下的首选模型， $b$ 是 BIC 下的首选模型，均编码为整数，0 代表泊松模型，1 代表负二项模型。因此，最终输出行的形式应为 $[[a_1,b_1],[a_2,b_2],[a_3,b_3],[a_4,b_4],[a_5,b_5]]$，其中每个 $a_i$ 和 $b_i$ 是 $\\{0,1\\}$ 中的整数。",
            "solution": "问题陈述要求对两种离散概率分布——泊松分布和负二项分布——进行比较分析，将它们作为给定单细胞基因表达计数集的模型。此比较将使用赤池信息准则（AIC）和贝叶斯信息准则（BIC）进行，这两个准则在模型拟合优度与模型复杂性之间进行权衡。对于每个准则，决策取决于选择准则值较低的模型。\n\n设观测数据为一组 $n$ 个独立同分布（i.i.d.）的非负整数计数 $\\mathbf{x} = \\{x_1, x_2, \\dots, x_n\\}$，其中每个 $x_i \\in \\{0, 1, 2, \\dots\\}$。任务的核心是找到每个模型参数的最大似然估计（MLE），然后计算相应的 AIC 和 BIC 值。\n\n### 模型 1：泊松分布\n泊松分布由单个率参数 $\\lambda > 0$ 表征，其对观测值 $x$ 的概率质量函数（PMF）为：\n$$ \\mathbb{P}(X=x \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^x}{x!} $$\n在独立同分布的假设下，数据集 $\\mathbf{x}$ 的似然函数是单个概率的乘积：\n$$ L(\\lambda; \\mathbf{x}) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{x_i}}{x_i!} $$\n为便于解析和数值计算，我们使用对数似然函数 $\\ell(\\lambda; \\mathbf{x}) = \\log L(\\lambda; \\mathbf{x})$：\n$$ \\ell(\\lambda; \\mathbf{x}) = \\sum_{i=1}^n \\left( x_i \\log \\lambda - \\lambda - \\log(x_i!) \\right) = (\\log \\lambda) \\sum_{i=1}^n x_i - n\\lambda - \\sum_{i=1}^n \\log(x_i!) $$\n为了找到 $\\lambda$ 的 MLE，我们将 $\\ell$ 关于 $\\lambda$ 的导数设为零：\n$$ \\frac{d\\ell}{d\\lambda} = \\frac{1}{\\lambda} \\sum_{i=1}^n x_i - n = 0 $$\n这得到了 $\\lambda$ 的著名闭式 MLE，即样本均值：\n$$ \\hat{\\lambda}_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^n x_i = \\bar{x} $$\n最大化对数似然 $\\ell_{\\text{Pois,max}}$ 是通过将 $\\hat{\\lambda}_{\\text{MLE}}$ 代回对数似然函数得到的。当所有 $x_i=0$ 时，会出现一个特殊情况，此时 $\\bar{x}=0$。在这种情况下，似然函数 $L(\\lambda; \\mathbf{x}) = e^{-n\\lambda}$ 在 $\\lambda \\to 0^+$ 时最大化，因此 $\\hat{\\lambda}_{\\text{MLE}} = 0$，最大化对数似然为 $\\ell_{\\text{Pois,max}} = 0$。\n\n### 模型 2：负二项分布\n负二项（NB）分布由均值 $\\mu \\ge 0$ 和正离散参数 $r > 0$ 指定。其 PMF 如下：\n$$ \\mathbb{P}(X=x \\mid r, \\mu) = \\binom{x+r-1}{x} \\left(\\frac{r}{r+\\mu}\\right)^r \\left(\\frac{\\mu}{r+\\mu}\\right)^x $$\n该分布的方差为 $\\mu + \\mu^2/r$。项 $\\mu^2/r$ 对过度离散（overdispersion）进行建模，这是与具有相同均值的泊松分布相比多出的方差。\n\n数据集 $\\mathbf{x}$ 的对数似然为 $\\ell(r, \\mu; \\mathbf{x}) = \\sum_{i=1}^n \\log \\mathbb{P}(X=x_i \\mid r, \\mu)$。使用恒等式 $\\binom{n}{k} = \\frac{\\Gamma(n+1)}{\\Gamma(k+1)\\Gamma(n-k+1)}$，单个观测值 $x_i$ 的对数似然可以使用对数伽马函数 $\\log \\Gamma(\\cdot)$ 写出：\n$$ \\ell_i(r, \\mu) = \\log\\Gamma(x_i+r) - \\log\\Gamma(r) - \\log\\Gamma(x_i+1) + r\\log r + x_i\\log\\mu - (x_i+r)\\log(r+\\mu) $$\n将总对数似然 $\\ell = \\sum_i \\ell_i$ 对 $\\mu$ 求导并令结果为零，可以发现均值参数的 MLE 也是样本均值：\n$$ \\hat{\\mu}_{\\text{MLE}} = \\bar{x} $$\n这简化了问题，因为我们只需要通过最大化剖面对数似然 $\\ell(r; \\mathbf{x}, \\hat{\\mu}_{\\text{MLE}})$ 来找到 $r$ 的 MLE。$\\hat{r}_{\\text{MLE}}$ 没有闭式解，因此必须通过数值优化找到。我们寻求最大化以下函数：\n$$ \\ell(r; \\mathbf{x}) = \\sum_{i=1}^n \\left[ \\log\\Gamma(x_i+r) - \\log\\Gamma(r) - \\log\\Gamma(x_i+1) + r\\log r + x_i\\log\\bar{x} - (x_i+r)\\log(r+\\bar{x}) \\right] $$\n一个重要的特殊情况是当样本方差 $\\hat{\\sigma}^2 = \\frac{1}{n}\\sum(x_i-\\bar{x})^2$ 小于或等于样本均值 $\\bar{x}$ 时。这种情况被称为低度离散（under-dispersion），意味着数据不支持 NB 模型的额外复杂性。在这种情况下，对数似然是 $r$ 的单调递增函数， $r$ 的 MLE 趋向于无穷大（$r \\to \\infty$）。当 $r \\to \\infty$ 时，NB 分布收敛于均值为 $\\mu$ 的泊松分布。因此，如果 $\\hat{\\sigma}^2 \\le \\bar{x}$，则本质上更倾向于泊松模型。\n\n### 模型选择准则\nAIC 和 BIC 定义如下：\n$$ \\text{AIC} = 2k - 2\\ell_{\\text{max}} $$\n$$ \\text{BIC} = k \\log(n) - 2\\ell_{\\text{max}} $$\n其中 $k$ 是估计参数的数量， $n$ 是样本大小， $\\ell_{\\text{max}}$ 是最大化的对数似然。对于此问题，参数数量为 $k_{\\text{Pois}} = 1$（用于 $\\lambda$）和 $k_{\\text{NB}} = 2$（用于 $\\mu$ 和 $r$）。\n\n### 决策过程\n对于每个数据集，我们遵循以下步骤：\n1.  计算样本大小 $n$ 和样本均值 $\\bar{x}$。\n2.  如果 $\\bar{x}=0$，则数据完全由零组成。$\\hat{\\lambda}=0$ 的泊松模型提供了完美拟合（$\\ell_{\\text{Pois,max}}=0$）。$\\hat{\\mu}=0$ 的 NB 模型也得到 $\\ell_{\\text{NB,max}}=0$，但更复杂（$k=2$ 对 $k=1$）。因此，对于 AIC 和 BIC，都优先选择泊松模型。\n3.  计算样本方差 $\\hat{\\sigma}^2$。如果 $\\hat{\\sigma}^2 \\le \\bar{x}$，数据没有过度离散。NB 模型的 MLE 收敛于泊松模型，因此由于 AIC 和 BIC 中的惩罚项，更简单的泊松模型被优先选择。\n4.  如果数据是过度离散的（$\\bar{x} > 0$ 且 $\\hat{\\sigma}^2 > \\bar{x}$），我们进行全面比较：\n    a.  使用 $\\hat{\\lambda} = \\bar{x}$ 计算 $\\ell_{\\text{Pois,max}}$。\n    b.  在保持 $\\mu = \\bar{x}$ 的情况下，对 $r > 0$ 进行 NB 对数似然的数值优化，以找到 $\\hat{r}$ 和相应的 $\\ell_{\\text{NB,max}}$。这是通过最小化负对数似然函数来实现的。\n    c.  计算 $\\text{AIC}_{\\text{Pois}}$、$\\text{BIC}_{\\text{Pois}}$、$\\text{AIC}_{\\text{NB}}$ 和 $\\text{BIC}_{\\text{NB}}$。\n    d.  对于每个准则，选择值较低的模型。如果值在 $\\varepsilon = 10^{-8}$ 的容差内，选择更简约的泊松模型。偏好编码为 0（泊松）和 1（负二项）。\n此过程为模型选择提供了一个严谨的框架，其基础是最大似然和信息论的原理。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize_scalar\nfrom scipy.special import gammaln\n\ndef _calculate_model_preference(counts: list[int]) -> list[int]:\n    \"\"\"\n    Calculates model preference for a single dataset.\n    \n    Compares Poisson and Negative Binomial models using AIC and BIC.\n\n    Args:\n        counts: A list of non-negative integer counts.\n\n    Returns:\n        A list [aic_preference, bic_preference] where 0 indicates a preference\n        for the Poisson model and 1 for the Negative Binomial model.\n    \"\"\"\n    x = np.array(counts)\n    n = len(x)\n    mu_mle = np.mean(x)\n    tol = 1e-8\n\n    # Case 1: Mean is zero. Data is all zeros.\n    # Poisson with lambda=0 is a perfect fit (LL=0).\n    # NB(mu=0, r) also has LL=0 but is more complex (k=2 vs k=1).\n    # The more parsimonious Poisson model is always preferred.\n    if mu_mle == 0:\n        return [0, 0]\n        \n    # Case 2: Data is under-dispersed or equi-dispersed.\n    # The MLE for NB dispersion 'r' tends to infinity, meaning the NB model\n    # converges to the Poisson model. The additional parameter is not justified.\n    var_mle = np.var(x, ddof=0)\n    if var_mle <= mu_mle:\n        return [0, 0]\n\n    # ----- Poisson Model Calculation -----\n    # The log-likelihood is sum(x_i*log(lambda) - lambda - log(x_i!)).\n    # We use gammaln(x+1) for log(x!).\n    log_lambda = np.log(mu_mle)\n    ll_pois = np.sum(x * log_lambda - mu_mle - gammaln(x + 1))\n\n    k_pois = 1\n    aic_pois = 2 * k_pois - 2 * ll_pois\n    bic_pois = k_pois * np.log(n) - 2 * ll_pois\n\n    # ----- Negative Binomial Model Calculation -----\n    \n    def neg_log_likelihood_nb_r(r: float, data: np.ndarray, mu: float) -> float:\n        \"\"\"Negative log-likelihood of NB model, to be minimized for r.\"\"\"\n        if r <= 0:\n            return np.inf\n        # The log-likelihood is sum(log(P(X=x_i | r, mu)))\n        log_likelihood = np.sum(\n            gammaln(data + r) - gammaln(r) - gammaln(data + 1) +\n            r * np.log(r) + data * np.log(mu) - (data + r) * np.log(r + mu)\n        )\n        return -log_likelihood\n\n    # Numerically optimize for r, given mu_mle.\n    # The 'bounded' method is used to ensure r > 0.\n    res = minimize_scalar(\n        neg_log_likelihood_nb_r,\n        args=(x, mu_mle),\n        bounds=(1e-8, 1e8),  # Search for r in a reasonable positive range.\n        method='bounded'\n    )\n    \n    # Maximized log-likelihood for NB is the negative of the minimized function value.\n    ll_nb = -res.fun\n    \n    k_nb = 2\n    aic_nb = 2 * k_nb - 2 * ll_nb\n    bic_nb = k_nb * np.log(n) - 2 * ll_nb\n\n    # ----- Model Comparison -----\n    # Prefer the model with the lower criterion value.\n    # If values are within tolerance, prefer the simpler Poisson model (output 0).\n    aic_pref = 1 if aic_nb < aic_pois - tol else 0\n    bic_pref = 1 if bic_nb < bic_pois - tol else 0\n    \n    return [aic_pref, bic_pref]\n\ndef solve():\n    \"\"\"\n    Main function to run the model comparison on all test cases and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        [0, 1, 1, 2, 1, 0, 1, 2, 1, 1, 2, 1, 3, 0],\n        [0, 0, 5, 10, 0, 3, 20, 0, 7, 15, 0, 1, 12],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [4, 5, 6, 5, 4, 5, 6, 5, 5, 4, 6],\n        [0, 50, 0, 0, 100, 2, 3, 70, 90, 0, 150, 1, 0]\n    ]\n\n    results = []\n    for case_data in test_cases:\n        a, b = _calculate_model_preference(case_data)\n        results.append(f\"[{a},{b}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}