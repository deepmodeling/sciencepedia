{
    "hands_on_practices": [
        {
            "introduction": "Parameter identifiability analysis begins with understanding the model's intrinsic structure. This exercise provides a foundational look at structural non-identifiability, where the mathematical form of a model prevents the unique determination of its parameters, even with perfect, noise-free data. By analyzing a simple exponential growth model , you will uncover a scaling symmetry and see how it leads to an entire family of parameter values producing identical outputs, thereby learning to identify the specific combinations of parameters that are, in fact, knowable.",
            "id": "4372101",
            "problem": "Consider the single-state linear system in a biochemical signaling motif where a measurable species $x(t)$ obeys the ordinary differential equation $\\dot{x}(t)=\\alpha \\beta x(t)$ with output $y(t)=x(t)$. The initial condition $x(0)=x_{0}>0$ is known, and the parameters $\\alpha>0$ and $\\beta>0$ are unknown. Assume continuous-time measurements $y(t)$ are available over a finite window $t\\in[0,T]$ under two regimes: (i) ideal, noiseless observation and (ii) additive measurement noise modeled as zero-mean Gaussian with constant variance $\\sigma^{2}>0$ independent of $t$. \n\nUsing foundational definitions: structural identifiability means that distinct parameter values generate distinct outputs for all admissible inputs and initial conditions, and a symmetry of the parameterized model is a transformation of parameters that leaves the input-output behavior invariant. Practical identifiability can be assessed in the small-noise limit by the Fisher Information Matrix (FIM), which quantifies local curvature of the likelihood and relates to estimator variance through the Cramér–Rao lower bound.\n\nTasks:\n- From first principles, show that the parameterized model admits a nontrivial one-parameter scaling symmetry in $(\\alpha,\\beta)$ that leaves $y(t)$ invariant for all $t$. Derive the infinitesimal generator of this symmetry as a vector field on the parameter manifold.\n- Use the invariants of this symmetry to determine the structurally identifiable combinations of $(\\alpha,\\beta)$ under noiseless observation and conclude which functions of $(\\alpha,\\beta)$ are structurally identifiable.\n- Construct an identifiable reparameterization by introducing a single parameter $\\phi=\\alpha\\beta$ and write the resulting model in terms of $\\phi$.\n- Under the Gaussian noise regime, write down the log-likelihood for continuous-time observation over $[0,T]$ and derive the Fisher Information Matrix for $(\\alpha,\\beta)$ and the Fisher Information scalar for the identifiable parameter $\\phi$. Show that the Fisher Information Matrix for $(\\alpha,\\beta)$ is rank-deficient (reflecting non-identifiability) while the Fisher Information for $\\phi$ is positive.\n- Provide, as your final calculated answer, the explicit expression for the reparameterized output $y(t)$ in terms of the identifiable parameter $\\phi$, the initial condition $x_{0}$, and time $t$. Your final answer must be a single closed-form analytic expression with no units. No rounding is required.",
            "solution": "The problem statement is first subjected to validation.\n\n### Step 1: Extract Givens\n-   **Model Equation**: $\\dot{x}(t)=\\alpha \\beta x(t)$\n-   **Output Equation**: $y(t)=x(t)$\n-   **Initial Condition**: $x(0)=x_{0}>0$, known.\n-   **Parameters**: $\\alpha>0$, $\\beta>0$, unknown.\n-   **Observation Interval**: $t \\in [0,T]$.\n-   **Observation Regimes**:\n    -   (i) Ideal, noiseless observation.\n    -   (ii) Additive, zero-mean Gaussian measurement noise with constant variance $\\sigma^{2}>0$.\n-   **Definitions**: Structural identifiability, model symmetry, practical identifiability via Fisher Information Matrix (FIM).\n-   **Tasks**:\n    1.  Derive the one-parameter scaling symmetry and its infinitesimal generator.\n    2.  Determine structurally identifiable combinations using symmetry invariants.\n    3.  Construct an identifiable reparameterization with $\\phi=\\alpha\\beta$.\n    4.  Derive the FIM for $(\\alpha,\\beta)$ and for $\\phi$, and analyze their rank/positivity.\n    5.  Provide the final expression for the reparameterized output $y(t)$.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded**: The problem is based on a standard linear ordinary differential equation, a fundamental tool in modeling dynamic systems. The concepts of structural and practical identifiability, symmetry analysis, and the Fisher Information Matrix are cornerstone principles in systems theory and parameter estimation, particularly within systems biomedicine. The problem is firmly rooted in established mathematical and statistical theory.\n-   **Well-Posed**: The problem is well-defined. The initial value problem for the ODE has a unique solution. Each task is a specific, answerable question based on the provided model and definitions.\n-   **Objective**: The problem is stated using precise mathematical language and is free from subjective or ambiguous terminology.\n-   **Completeness and Consistency**: The problem is self-contained, providing all necessary equations, conditions, and conceptual definitions to perform the required analysis. There are no internal contradictions.\n-   **Realism and Feasibility**: While the model is a simplification, it represents a valid theoretical framework for identifiability analysis. The assumptions (e.g., continuous-time observation, known initial condition) are standard idealizations in theoretical studies.\n-   **Structure and Triviality**: The problem is well-structured, guiding the analysis from structural (noiseless) to practical (noisy) identifiability. It correctly connects the abstract concept of model symmetries to the practical implications for parameter estimation, as reflected in the FIM. It is a non-trivial pedagogical problem that illustrates fundamental concepts.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is scientifically sound, well-posed, and complete. I will proceed with the detailed solution.\n\n### Solution Derivation\n\nThe system is described by the linear first-order ordinary differential equation (ODE):\n$$\n\\frac{dx}{dt} = (\\alpha\\beta) x(t)\n$$\nwith the initial condition $x(0) = x_0$. This is a separable ODE whose solution is found by integration:\n$$\n\\int_{x_0}^{x(t)} \\frac{dx}{x} = \\int_0^t \\alpha\\beta \\,d\\tau\n$$\n$$\n\\ln(x(t)) - \\ln(x_0) = \\alpha\\beta t\n$$\n$$\nx(t) = x_0 \\exp(\\alpha\\beta t)\n$$\nThe system output is $y(t) = x(t)$, so the input-output relationship is given by:\n$$\ny(t) = y(\\alpha, \\beta; t) = x_0 \\exp(\\alpha\\beta t)\n$$\n\n**Task 1: Symmetry Analysis**\n\nA symmetry of the model is a transformation of the parameters $(\\alpha, \\beta) \\to (\\tilde{\\alpha}, \\tilde{\\beta})$ that leaves the output $y(t)$ invariant for all $t \\in [0,T]$.\n$$\ny(\\tilde{\\alpha}, \\tilde{\\beta}; t) = y(\\alpha, \\beta; t)\n$$\n$$\nx_0 \\exp(\\tilde{\\alpha}\\tilde{\\beta} t) = x_0 \\exp(\\alpha\\beta t)\n$$\nSince $x_0 > 0$ and this must hold for all $t$, we must have $\\tilde{\\alpha}\\tilde{\\beta} = \\alpha\\beta$. This equation defines the symmetry. We can define a one-parameter family of transformations, indexed by a scalar $c>0$, as:\n$$\n\\tilde{\\alpha}(c) = c \\alpha\n$$\n$$\n\\tilde{\\beta}(c) = \\frac{1}{c} \\beta\n$$\nThis transformation maps a point $(\\alpha, \\beta)$ along a hyperbola in the parameter space, and for any $c$, the product remains invariant: $\\tilde{\\alpha}\\tilde{\\beta} = (c\\alpha)(\\frac{1}{c}\\beta) = \\alpha\\beta$. This is a non-trivial scaling symmetry.\n\nTo find the infinitesimal generator of this symmetry, we consider the transformation near the identity, which corresponds to $c \\approx 1$. Let $c = \\exp(\\epsilon)$ for an infinitesimal $\\epsilon$.\n$$\n\\tilde{\\alpha} = \\alpha \\exp(\\epsilon) \\approx \\alpha(1 + \\epsilon) = \\alpha + \\epsilon\\alpha\n$$\n$$\n\\tilde{\\beta} = \\beta \\exp(-\\epsilon) \\approx \\beta(1 - \\epsilon) = \\beta - \\epsilon\\beta\n$$\nThe infinitesimal change in the parameters is $(\\delta\\alpha, \\delta\\beta) = (\\tilde{\\alpha}-\\alpha, \\tilde{\\beta}-\\beta) = (\\epsilon\\alpha, -\\epsilon\\beta)$. The infinitesimal generator is the vector field $V$ on the parameter manifold defined by $V = (\\frac{\\delta\\alpha}{\\epsilon}, \\frac{\\delta\\beta}{\\epsilon})$.\n$$\nV = (\\alpha, -\\beta)\n$$\nIn differential operator form, this vector field is $V = \\alpha \\frac{\\partial}{\\partial \\alpha} - \\beta \\frac{\\partial}{\\partial \\beta}$.\n\n**Task 2: Structurally Identifiable Combinations**\n\nThe structurally identifiable combinations of parameters are the invariants of the symmetry group. A function $f(\\alpha, \\beta)$ is an invariant if it is constant along the flow generated by $V$. This is equivalent to the Lie derivative of $f$ with respect to $V$ being zero:\n$$\nL_V f = V(f) = \\alpha \\frac{\\partial f}{\\partial \\alpha} - \\beta \\frac{\\partial f}{\\partial \\beta} = 0\n$$\nThis is a first-order partial differential equation. The method of characteristics gives the characteristic equations:\n$$\n\\frac{d\\alpha}{\\alpha} = \\frac{d\\beta}{-\\beta}\n$$\nIntegrating both sides yields $\\ln|\\alpha| = -\\ln|\\beta| + K$ for some constant $K$. Since $\\alpha>0$ and $\\beta>0$, this simplifies to $\\ln(\\alpha) + \\ln(\\beta) = K$, or $\\ln(\\alpha\\beta) = K$. This implies that $\\alpha\\beta$ is constant along the characteristic curves (the flow of $V$). Therefore, any function of the product $\\alpha\\beta$ is an invariant of the symmetry. The fundamental invariant is the product $\\phi = \\alpha\\beta$.\nSince the output $y(t)$ depends only on this invariant combination, it is impossible to distinguish between different pairs $(\\alpha, \\beta)$ that share the same product. Consequently, only functions of $\\alpha\\beta$, denoted $g(\\alpha\\beta)$, are structurally identifiable. The parameters $\\alpha$ and $\\beta$ are not individually identifiable.\n\n**Task 3: Identifiable Reparameterization**\n\nBased on the symmetry analysis, the identifiable parameter is $\\phi = \\alpha\\beta$. We can reparameterize the model using only this new parameter.\nThe original ODE is $\\dot{x}(t) = (\\alpha\\beta) x(t)$. Substituting $\\phi = \\alpha\\beta$, we get the reparameterized model:\n$$\n\\dot{x}(t) = \\phi x(t)\n$$\nwith output $y(t) = x(t)$ and initial condition $x(0) = x_0$. This is a new model with a single parameter $\\phi > 0$. This model is structurally identifiable because distinct values of $\\phi$ will generate distinct output trajectories $y(t) = x_0 \\exp(\\phi t)$.\n\n**Task 4: Fisher Information Matrix (FIM) Analysis**\n\nUnder the additive Gaussian noise regime, the observed data is $y_{obs}(t) = y(t) + \\epsilon(t)$, where $\\epsilon(t)$ is continuous-time white noise. The log-likelihood functional is proportional to:\n$$\nL(\\boldsymbol{\\theta}) \\propto -\\frac{1}{2\\sigma^2} \\int_0^T [y_{obs}(t) - y(t; \\boldsymbol{\\theta})]^2 dt\n$$\nThe Fisher Information Matrix for a parameter vector $\\boldsymbol{\\theta}$ is given by:\n$$\nF_{ij} = \\frac{1}{\\sigma^2} \\int_0^T \\left( \\frac{\\partial y(t; \\boldsymbol{\\theta})}{\\partial \\theta_i} \\right) \\left( \\frac{\\partial y(t; \\boldsymbol{\\theta})}{\\partial \\theta_j} \\right) dt\n$$\nFirst, for the non-identifiable parameterization $\\boldsymbol{\\theta} = (\\alpha, \\beta)$, with $y(t) = x_0 \\exp(\\alpha\\beta t)$:\nThe partial derivatives (sensitivities) are:\n$$\n\\frac{\\partial y}{\\partial \\alpha} = x_0 (\\beta t) \\exp(\\alpha\\beta t)\n$$\n$$\n\\frac{\\partial y}{\\partial \\beta} = x_0 (\\alpha t) \\exp(\\alpha\\beta t)\n$$\nThe components of the FIM, $F_{(\\alpha,\\beta)}$, are:\n$$\nF_{\\alpha\\alpha} = \\frac{1}{\\sigma^2} \\int_0^T [x_0 \\beta t \\exp(\\alpha\\beta t)]^2 dt = \\frac{x_0^2 \\beta^2}{\\sigma^2} \\int_0^T t^2 \\exp(2\\alpha\\beta t) dt\n$$\n$$\nF_{\\beta\\beta} = \\frac{1}{\\sigma^2} \\int_0^T [x_0 \\alpha t \\exp(\\alpha\\beta t)]^2 dt = \\frac{x_0^2 \\alpha^2}{\\sigma^2} \\int_0^T t^2 \\exp(2\\alpha\\beta t) dt\n$$\n$$\nF_{\\alpha\\beta} = F_{\\beta\\alpha} = \\frac{1}{\\sigma^2} \\int_0^T [x_0 \\beta t \\exp(\\alpha\\beta t)][x_0 \\alpha t \\exp(\\alpha\\beta t)] dt = \\frac{x_0^2 \\alpha\\beta}{\\sigma^2} \\int_0^T t^2 \\exp(2\\alpha\\beta t) dt\n$$\nLet $I = \\int_0^T t^2 \\exp(2\\alpha\\beta t) dt$. Since the integrand is non-negative and not identically zero for $T>0$, we have $I>0$. The FIM is:\n$$\nF_{(\\alpha,\\beta)} = \\frac{x_0^2 I}{\\sigma^2} \\begin{pmatrix} \\beta^2 & \\alpha\\beta \\\\ \\alpha\\beta & \\alpha^2 \\end{pmatrix}\n$$\nThe determinant of this matrix is:\n$$\n\\det(F_{(\\alpha,\\beta)}) = \\left(\\frac{x_0^2 I}{\\sigma^2}\\right)^2 (\\beta^2 \\alpha^2 - (\\alpha\\beta)(\\alpha\\beta)) = 0\n$$\nSince $\\det(F_{(\\alpha,\\beta)}) = 0$, the matrix is singular (rank-deficient). This confirms that the parameters $(\\alpha, \\beta)$ are not practically identifiable, as the Cramér–Rao lower bound on the variance of any unbiased estimator would be infinite. The null space of this matrix is spanned by the vector $(\\alpha, -\\beta)^T$, which corresponds to the infinitesimal generator of the symmetry, confirming the connection between structural non-identifiability (symmetry) and practical non-identifiability (singular FIM).\n\nNext, for the identifiable parameterization with parameter $\\phi = \\alpha\\beta$, with $y(t) = x_0 \\exp(\\phi t)$:\nThe sensitivity is:\n$$\n\\frac{\\partial y}{\\partial \\phi} = x_0 t \\exp(\\phi t)\n$$\nThe FIM is a $1 \\times 1$ matrix (a scalar), $F_{\\phi}$:\n$$\nF_{\\phi} = \\frac{1}{\\sigma^2} \\int_0^T \\left(\\frac{\\partial y}{\\partial \\phi}\\right)^2 dt = \\frac{x_0^2}{\\sigma^2} \\int_0^T [t \\exp(\\phi t)]^2 dt = \\frac{x_0^2}{\\sigma^2} \\int_0^T t^2 \\exp(2\\phi t) dt\n$$\nSince $x_0^2 > 0$, $\\sigma^2 > 0$, and the integral $\\int_0^T t^2 \\exp(2\\phi t) dt$ is strictly positive for $T>0$ and $\\phi>0$, we have $F_{\\phi} > 0$. A positive (non-singular) FIM indicates that the parameter $\\phi$ is practically identifiable.\n\n**Task 5: Final Calculated Answer**\n\nThe final task is to provide the explicit expression for the reparameterized output $y(t)$ in terms of the identifiable parameter $\\phi$, the initial condition $x_0$, and time $t$. As derived from solving the reparameterized ODE $\\dot{x}(t) = \\phi x(t)$ with $x(0)=x_0$ and $y(t)=x(t)$, this expression is:\n$$\ny(t) = x_0 \\exp(\\phi t)\n$$\nThis is the required final answer.",
            "answer": "$$\n\\boxed{x_{0} \\exp(\\phi t)}\n$$"
        },
        {
            "introduction": "Building on the basic principles of structural identifiability, this problem explores a more complex, multi-exponential model commonly found in pharmacokinetics and systems biology. This practice is valuable because it contrasts a structurally identifiable model with a closely related non-identifiable variant, demonstrating that identifiability is highly sensitive to the specific parameterization. You will use analytical methods to prove identifiability from first principles and then compute the Fisher Information Matrix, connecting the theoretical concept of structural identifiability to the practical limits of parameter precision in the presence of noise .",
            "id": "4372069",
            "problem": "A dual-exponential pharmacokinetic signal in a controlled in vitro system is modeled as $y(t)=\\theta_{1}\\exp(-a t)+\\theta_{2}\\exp(-b t)$, where $a>0$ and $b>0$ are distinct and known rate constants set by the assay conditions, and $t\\in[0,T]$ denotes time. You are told that the measurement process is additive and that under noise-free measurements both coefficients $\\theta_{1}$ and $\\theta_{2}$ represent unknown abundances of two biochemically distinct species. In applications with instrumentation noise, the readout is $z(t)=y(t)+\\varepsilon(t)$, where $\\varepsilon(t)$ is zero-mean Gaussian white noise with constant spectral density $\\sigma^{2}$.\n\nUsing only fundamental definitions and well-tested facts from system identification and statistics:\n\n- Structural identifiability refers to injectivity of the input-output map with respect to parameters from idealized, noise-free data.\n- Under zero-mean Gaussian white noise with constant spectral density, the Fisher Information Matrix (FIM) for continuous-time observation of a parametric output is $J=(1/\\sigma^{2})\\int_{0}^{T} s(t)s(t)^{\\top}\\,dt$, where $s(t)$ is the sensitivity vector $s(t)=\\partial y(t)/\\partial \\theta$.\n\nTasks:\n\n1. Prove structural identifiability of $(\\theta_{1},\\theta_{2})$ by considering two noise-free measurements $y(t_{1})$ and $y(t_{2})$ at any two distinct times $t_{1}\\neq t_{2}$ with $t_{1},t_{2}\\in(0,T]$. Derive explicit expressions for $\\theta_{1}$ and $\\theta_{2}$ in terms of $y(t_{1})$, $y(t_{2})$, $a$, $b$, $t_{1}$, and $t_{2}$.\n\n2. For the noisy continuous-time observation on $[0,T]$, compute the Fisher Information Matrix $J$ for $\\theta=(\\theta_{1},\\theta_{2})$, and derive a closed-form expression for $\\det(J)$ in terms of $a$, $b$, $T$, and $\\sigma$.\n\n3. Consider a nonlinear variant in which two biochemical branches are modulated by a single catalytic efficiency parameter $\\kappa_{1}$ and two branch-specific factors $\\kappa_{2}$ and $\\kappa_{3}$, yielding $y(t)=\\kappa_{1}\\kappa_{2}\\exp(-a t)+\\kappa_{1}\\kappa_{3}\\exp(-b t)$ with the same known $a\\neq b>0$. Using the definition of structural identifiability, determine whether $(\\kappa_{1},\\kappa_{2},\\kappa_{3})$ are structurally identifiable from ideal, noise-free $y(t)$ on $[0,T]$ and justify your conclusion by explicitly constructing a nontrivial parameter transformation (if any) that leaves $y(t)$ invariant.\n\nProvide the final answer as the single closed-form analytic expression for $\\det(J)$ obtained in Task $2$. No numerical approximation is required. Do not include units in the final answer box.",
            "solution": "The problem statement is validated as follows.\n\n**Step 1: Extract Givens**\n- Model 1 (linear): $y(t)=\\theta_{1}\\exp(-a t)+\\theta_{2}\\exp(-b t)$\n- Model parameters: $\\theta_{1}$ and $\\theta_{2}$ are unknown abundances.\n- Model constants: $a>0$ and $b>0$ are distinct ($a \\neq b$) and known rate constants.\n- Time domain: $t \\in [0, T]$.\n- Noise model: $z(t)=y(t)+\\varepsilon(t)$, where $\\varepsilon(t)$ is zero-mean Gaussian white noise with constant spectral density $\\sigma^{2}$.\n- Definition of structural identifiability: Injectivity of the input-output map with respect to parameters from idealized, noise-free data.\n- Definition of Fisher Information Matrix (FIM) for continuous-time observation: $J=(1/\\sigma^{2})\\int_{0}^{T} s(t)s(t)^{\\top}\\,dt$, where $s(t)=\\partial y(t)/\\partial \\theta$.\n- Task 1: Prove structural identifiability of $(\\theta_{1},\\theta_{2})$ using two noise-free measurements $y(t_{1})$ and $y(t_{2})$ at any two distinct times $t_{1}\\neq t_{2}$ with $t_{1},t_{2}\\in(0,T]$. Derive explicit expressions for $\\theta_{1}$ and $\\theta_{2}$.\n- Task 2: Compute the FIM $J$ for $\\theta=(\\theta_{1},\\theta_{2})$ for noisy continuous-time observation on $[0,T]$, and derive a closed-form expression for $\\det(J)$.\n- Model 2 (nonlinear): $y(t)=\\kappa_{1}\\kappa_{2}\\exp(-a t)+\\kappa_{1}\\kappa_{3}\\exp(-b t)$ with known $a \\neq b > 0$.\n- Model parameters: $(\\kappa_{1}, \\kappa_{2}, \\kappa_{3})$ are unknown.\n- Task 3: Determine if $(\\kappa_{1},\\kappa_{2},\\kappa_{3})$ are structurally identifiable from noise-free $y(t)$ on $[0,T]$ and justify the conclusion.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, well-posed, and objective.\n- **Scientific and Factual Soundness:** The model $y(t)=\\theta_{1}\\exp(-a t)+\\theta_{2}\\exp(-b t)$ is a standard representation of a two-compartment pharmacokinetic model or parallel first-order reactions, fundamental in systems biology, chemistry, and engineering. The definitions of structural identifiability and the Fisher Information Matrix are standard and correct within system identification and statistical estimation theory.\n- **Well-Posed and Complete:** Each task is clearly specified. The problem provides all necessary equations, definitions, and constraints (e.g., $a \\neq b$) to perform the required derivations. The questions are unambiguous.\n- **Objectivity and Feasibility:** The problem is stated using precise mathematical language, free from subjective claims. The calculations required are standard integral calculus and linear algebra, making the tasks feasible.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\n**Solution Derivation**\n\n**Task 1: Structural Identifiability of $(\\theta_{1},\\theta_{2})$**\nTo prove structural identifiability for the parameters $\\theta = (\\theta_{1}, \\theta_{2})^{\\top}$, we must show that they can be uniquely determined from noise-free output $y(t)$. As instructed, we consider two measurements at two distinct time points, $t_{1} \\neq t_{2}$, where $t_{1}, t_{2} \\in (0, T]$. This yields a system of two linear equations in two unknowns, $\\theta_{1}$ and $\\theta_{2}$:\n$$\ny(t_{1}) = \\theta_{1}\\exp(-a t_{1}) + \\theta_{2}\\exp(-b t_{1})\n$$\n$$\ny(t_{2}) = \\theta_{1}\\exp(-a t_{2}) + \\theta_{2}\\exp(-b t_{2})\n$$\nThis system can be written in matrix form as:\n$$\n\\begin{pmatrix} y(t_{1}) \\\\ y(t_{2}) \\end{pmatrix} = \\begin{pmatrix} \\exp(-a t_{1}) & \\exp(-b t_{1}) \\\\ \\exp(-a t_{2}) & \\exp(-b t_{2}) \\end{pmatrix} \\begin{pmatrix} \\theta_{1} \\\\ \\theta_{2} \\end{pmatrix}\n$$\nA unique solution for $(\\theta_{1}, \\theta_{2})$ exists if and only if the determinant of the coefficient matrix is non-zero. Let the coefficient matrix be $M$. Its determinant is:\n$$\n\\det(M) = \\exp(-a t_{1}) \\exp(-b t_{2}) - \\exp(-b t_{1}) \\exp(-a t_{2}) = \\exp(-(a t_{1} + b t_{2})) - \\exp(-(b t_{1} + a t_{2}))\n$$\nThe determinant is zero if and only if $a t_{1} + b t_{2} = b t_{1} + a t_{2}$, which rearranges to $a(t_{1}-t_{2}) - b(t_{1}-t_{2}) = 0$, or $(a-b)(t_{1}-t_{2}) = 0$. By the problem statement, the rate constants are distinct ($a \\neq b$) and the measurement times are distinct ($t_{1} \\neq t_{2}$). Therefore, $(a-b) \\neq 0$ and $(t_{1}-t_{2}) \\neq 0$, which implies $\\det(M) \\neq 0$.\nSince the coefficient matrix is invertible for any choice of distinct $t_{1}, t_{2}$, the parameters $(\\theta_{1}, \\theta_{2})$ are uniquely determined. This proves they are structurally identifiable.\n\nThe explicit expressions for $\\theta_{1}$ and $\\theta_{2}$ can be found using Cramer's rule or by matrix inversion. Using Cramer's rule:\n$$\n\\theta_{1} = \\frac{\\begin{vmatrix} y(t_{1}) & \\exp(-b t_{1}) \\\\ y(t_{2}) & \\exp(-b t_{2}) \\end{vmatrix}}{\\det(M)} = \\frac{y(t_{1})\\exp(-b t_{2}) - y(t_{2})\\exp(-b t_{1})}{\\exp(-a t_{1}-b t_{2}) - \\exp(-a t_{2}-b t_{1})}\n$$\n$$\n\\theta_{2} = \\frac{\\begin{vmatrix} \\exp(-a t_{1}) & y(t_{1}) \\\\ \\exp(-a t_{2}) & y(t_{2}) \\end{vmatrix}}{\\det(M)} = \\frac{y(t_{2})\\exp(-a t_{1}) - y(t_{1})\\exp(-a t_{2})}{\\exp(-a t_{1}-b t_{2}) - \\exp(-a t_{2}-b t_{1})}\n$$\n\n**Task 2: Fisher Information Matrix for $\\theta=(\\theta_{1},\\theta_{2})$**\nThe parameter vector is $\\theta = (\\theta_{1}, \\theta_{2})^{\\top}$. The sensitivity vector $s(t)$ is the vector of partial derivatives of the output $y(t)$ with respect to the parameters:\n$$\ns(t) = \\frac{\\partial y(t)}{\\partial \\theta} = \\begin{pmatrix} \\frac{\\partial y(t)}{\\partial \\theta_{1}} \\\\ \\frac{\\partial y(t)}{\\partial \\theta_{2}} \\end{pmatrix} = \\begin{pmatrix} \\exp(-a t) \\\\ \\exp(-b t) \\end{pmatrix}\n$$\nThe Fisher Information Matrix $J$ is given by $J = (1/\\sigma^{2})\\int_{0}^{T} s(t)s(t)^{\\top}\\,dt$. First, we compute the outer product $s(t)s(t)^{\\top}$:\n$$\ns(t)s(t)^{\\top} = \\begin{pmatrix} \\exp(-a t) \\\\ \\exp(-b t) \\end{pmatrix} \\begin{pmatrix} \\exp(-a t) & \\exp(-b t) \\end{pmatrix} = \\begin{pmatrix} \\exp(-2at) & \\exp(-(a+b)t) \\\\ \\exp(-(a+b)t) & \\exp(-2bt) \\end{pmatrix}\n$$\nNow, we integrate each element of this matrix from $t=0$ to $t=T$:\n$$\nJ_{11} = \\frac{1}{\\sigma^{2}}\\int_{0}^{T} \\exp(-2at) \\,dt = \\frac{1}{\\sigma^{2}}\\left[-\\frac{1}{2a}\\exp(-2at)\\right]_{0}^{T} = \\frac{1-\\exp(-2aT)}{2a\\sigma^{2}}\n$$\n$$\nJ_{22} = \\frac{1}{\\sigma^{2}}\\int_{0}^{T} \\exp(-2bt) \\,dt = \\frac{1}{\\sigma^{2}}\\left[-\\frac{1}{2b}\\exp(-2bt)\\right]_{0}^{T} = \\frac{1-\\exp(-2bT)}{2b\\sigma^{2}}\n$$\n$$\nJ_{12} = J_{21} = \\frac{1}{\\sigma^{2}}\\int_{0}^{T} \\exp(-(a+b)t) \\,dt = \\frac{1}{\\sigma^{2}}\\left[-\\frac{1}{a+b}\\exp(-(a+b)t)\\right]_{0}^{T} = \\frac{1-\\exp(-(a+b)T)}{(a+b)\\sigma^{2}}\n$$\nThe full Fisher Information Matrix is:\n$$\nJ = \\frac{1}{\\sigma^{2}} \\begin{pmatrix} \\frac{1-\\exp(-2aT)}{2a} & \\frac{1-\\exp(-(a+b)T)}{a+b} \\\\ \\frac{1-\\exp(-(a+b)T)}{a+b} & \\frac{1-\\exp(-2bT)}{2b} \\end{pmatrix}\n$$\nFinally, we compute the determinant of $J$:\n$$\n\\det(J) = J_{11}J_{22} - J_{12}^{2}\n$$\n$$\n\\det(J) = \\left(\\frac{1-\\exp(-2aT)}{2a\\sigma^{2}}\\right)\\left(\\frac{1-\\exp(-2bT)}{2b\\sigma^{2}}\\right) - \\left(\\frac{1-\\exp(-(a+b)T)}{(a+b)\\sigma^{2}}\\right)^{2}\n$$\n$$\n\\det(J) = \\frac{1}{\\sigma^{4}} \\left( \\frac{(1-\\exp(-2aT))(1-\\exp(-2bT))}{4ab} - \\frac{(1-\\exp(-(a+b)T))^{2}}{(a+b)^{2}} \\right)\n$$\nThis is the required closed-form expression.\n\n**Task 3: Structural Identifiability of $(\\kappa_{1},\\kappa_{2},\\kappa_{3})$**\nThe nonlinear model is $y(t)=\\kappa_{1}\\kappa_{2}\\exp(-a t)+\\kappa_{1}\\kappa_{3}\\exp(-b t)$. This model can be mapped to the linear model by defining new parameters:\n$$\n\\theta_{1} = \\kappa_{1}\\kappa_{2}\n$$\n$$\n\\theta_{2} = \\kappa_{1}\\kappa_{3}\n$$\nFrom Task 1, we established that the parameters $(\\theta_{1}, \\theta_{2})$ are structurally identifiable. The question of whether $(\\kappa_{1}, \\kappa_{2}, \\kappa_{3})$ are identifiable is equivalent to asking if the mapping from $(\\kappa_{1}, \\kappa_{2}, \\kappa_{3})$ to $(\\theta_{1}, \\theta_{2})$ is injective. We have a system of $2$ equations for $3$ unknowns. This system is underdetermined, which suggests that the parameters are not identifiable.\n\nTo prove this formally, we must construct a nontrivial parameter transformation that leaves the output $y(t)$ invariant. Let $\\alpha$ be an arbitrary non-zero and non-unity constant ($\\alpha \\neq 0, \\alpha \\neq 1$). Consider a new set of parameters $(\\kappa'_{1}, \\kappa'_{2}, \\kappa'_{3})$ defined by the transformation:\n$$\n\\kappa'_{1} = \\alpha \\kappa_{1}\n$$\n$$\n\\kappa'_{2} = \\frac{1}{\\alpha} \\kappa_{2}\n$$\n$$\n\\kappa'_{3} = \\frac{1}{\\alpha} \\kappa_{3}\n$$\nThe original parameter set is $(\\kappa_{1}, \\kappa_{2}, \\kappa_{3})$. The new parameter set $(\\kappa'_{1}, \\kappa'_{2}, \\kappa'_{3})$ is different from the original set since $\\alpha \\neq 1$.\nNow we compute the values of $\\theta_{1}$ and $\\theta_{2}$ for this new parameter set, which we denote $\\theta'_{1}$ and $\\theta'_{2}$:\n$$\n\\theta'_{1} = \\kappa'_{1} \\kappa'_{2} = (\\alpha \\kappa_{1}) \\left(\\frac{1}{\\alpha} \\kappa_{2}\\right) = \\kappa_{1} \\kappa_{2} = \\theta_{1}\n$$\n$$\n\\theta'_{2} = \\kappa'_{1} \\kappa'_{3} = (\\alpha \\kappa_{1}) \\left(\\frac{1}{\\alpha} \\kappa_{3}\\right) = \\kappa_{1} \\kappa_{3} = \\theta_{2}\n$$\nSince $(\\theta'_{1}, \\theta'_{2}) = (\\theta_{1}, \\theta_{2})$, the output function $y(t)$ remains unchanged under this parameter transformation. For any given set of parameters $(\\kappa_{1}, \\kappa_{2}, \\kappa_{3})$, there exists an infinite family of parameter sets $(\\alpha\\kappa_{1}, \\kappa_{2}/\\alpha, \\kappa_{3}/\\alpha)$ that produce the exact same output $y(t)$. Therefore, it is impossible to uniquely determine $(\\kappa_{1}, \\kappa_{2}, \\kappa_{3})$ from measurements of $y(t)$.\nConclusion: The parameter set $(\\kappa_{1},\\kappa_{2},\\kappa_{3})$ is structurally non-identifiable.",
            "answer": "$$\n\\boxed{\\frac{1}{\\sigma^{4}} \\left[ \\frac{\\left(1-\\exp(-2aT)\\right)\\left(1-\\exp(-2bT)\\right)}{4ab} - \\frac{\\left(1-\\exp(-(a+b)T)\\right)^{2}}{(a+b)^{2}} \\right]}\n$$"
        },
        {
            "introduction": "While analytical methods are powerful, most real-world identifiability analysis relies on computational techniques applied to noisy data. This hands-on coding exercise introduces the profile likelihood method, a cornerstone of practical identifiability analysis, which is particularly useful for models with nuisance parameters. By simulating experiments with different data collection strategies , you will directly observe how the choice of measurement times can render a parameter either precisely estimable or practically non-identifiable, revealing the critical link between experimental design and model calibration.",
            "id": "4372027",
            "problem": "Consider the exponential decay model given by the Ordinary Differential Equation (ODE) $$\\frac{dx}{dt} = -\\theta x,$$ where the solution under an unknown initial condition is $$x(t) = x(0)\\,e^{-\\theta t}.$$ Suppose measurements are collected as $$y_i = x(t_i) + \\varepsilon_i,$$ where the measurement noise satisfies $$\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$$ independently across time points, and $\\sigma$ is known. The task is to assess parameter identifiability of $\\theta$ when $x(0)$ is unknown by computing the Profile Likelihood (PL) for $\\theta$ under Maximum Likelihood Estimation (MLE), and to illustrate flat directions when only steady-state or late-time data are available.\n\nYou must implement a program that, for a grid of $\\theta$ values, computes the profile negative log-likelihood $\\ell(\\theta)$ by minimizing the negative log-likelihood over the nuisance parameter $x(0)$ for each $\\theta$. Then, for each test case, you must report:\n- the MLE $\\hat{\\theta}$ in $\\mathrm{s}^{-1}$,\n- the width of the PL at likelihood-ratio threshold $\\Delta \\ell^\\star$ in $\\mathrm{s}^{-1}$,\n- a boolean flag indicating whether the PL exhibits a flat direction according to a specified criterion.\n\nDefinitions that must be used:\n- Profile Likelihood (PL): the likelihood as a function of $\\theta$ after optimizing over $x(0)$.\n- Maximum Likelihood Estimation (MLE): the estimates $\\hat{\\theta}$ obtained by minimizing the profile negative log-likelihood.\n- A flat direction is defined here as either of the following: the confidence set at threshold $\\Delta \\ell^\\star$ touches the $\\theta$-grid boundary, or its width is at least a fraction $w_{\\mathrm{flat}}$ of the total grid span.\n\nYou must use a fixed grid for $\\theta$ over the interval $[\\theta_{\\min},\\theta_{\\max}]$ with a uniform step $\\delta_\\theta$, and compute the PL at each grid point by optimizing out $x(0)$. Use the likelihood-ratio threshold $\\Delta \\ell^\\star$ to define the confidence set $\\{\\theta : \\ell(\\theta) - \\min_{\\theta} \\ell(\\theta) \\le \\Delta \\ell^\\star\\}$ and report its width as the difference between the largest and smallest $\\theta$ in this set. For the boolean flag, use the flatness criterion described above.\n\nYour program must implement the following test suite with synthetic data. In all cases, measurements are in arbitrary concentration units and time is in seconds; report $\\hat{\\theta}$ and widths in $\\mathrm{s}^{-1}$. For scenarios with noise, draw the Gaussian noise with the specified seed to ensure reproducibility.\n\n- Test Case $1$ (full time-course, identifiable):\n  - True parameters: $\\theta_{\\mathrm{true}} = 0.5$, $x(0)_{\\mathrm{true}} = 5.0$.\n  - Time points: $t = [0.0,\\,0.5,\\,1.0,\\,2.0,\\,3.5,\\,5.0,\\,6.0]$.\n  - Noise level: $\\sigma = 0.05$.\n  - Noise seed: $12345$.\n\n- Test Case $2$ (late-time-only, practically flat):\n  - True parameters: $\\theta_{\\mathrm{true}} = 0.5$, $x(0)_{\\mathrm{true}} = 5.0$.\n  - Time points: $t = [5.0,\\,6.0,\\,8.0,\\,10.0,\\,12.0]$.\n  - Noise level: $\\sigma = 0.05$.\n  - Noise seed: $23456$.\n\n- Test Case $3$ (steady-state-only, structurally flat):\n  - Time point: $t = [100.0]$.\n  - Observed measurement: $y = [0.0]$ exactly (no noise draw).\n  - Noise level used in the likelihood: $\\sigma = 0.01$.\n\nUse the following global analysis settings consistently across all test cases:\n- $\\theta_{\\min} = 0.001$,\n- $\\theta_{\\max} = 3.000$,\n- $\\delta_\\theta = 0.001$,\n- likelihood-ratio threshold $\\Delta \\ell^\\star = 2.0$,\n- flatness fraction threshold $w_{\\mathrm{flat}} = 0.7$.\n\nAlgorithmic tasks the program must perform for each test case:\n- Simulate or set the observed $y_i$ according to the case specification.\n- For each $\\theta$ on the grid, numerically optimize out $x(0)$ and compute the profile negative log-likelihood $\\ell(\\theta)$.\n- Compute $\\hat{\\theta}$ as the $\\theta$ that minimizes $\\ell(\\theta)$ on the grid.\n- Compute the PL width at $\\Delta \\ell^\\star$.\n- Compute the flatness flag using the specified criterion.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to a test case and is itself a sub-list of three entries $[\\hat{\\theta}, \\text{width}, \\text{is_flat}]$. The first two entries are floats in $\\mathrm{s}^{-1}$ and the third is a boolean. For example, the formatting should look like `[[0.500000,0.123000,False],[...],[...]]` with no spaces.\n\nNo external input is allowed; all values and data are defined within the program.",
            "solution": "The problem requires an assessment of parameter identifiability for an exponential decay model using the profile likelihood method. This is a fundamental technique in systems biology and other quantitative fields for analyzing the confidence in parameter estimates, particularly when some parameters (so-called nuisance parameters) are not of primary interest.\n\nThe model is described by the solution to an ordinary differential equation:\n$$x(t) = x(0)\\,e^{-\\theta t}$$\nwhere $$x(t)$$ is the state at time $$t$$, $$x(0)$$ is the initial state, and $$\\theta$$ is the decay rate constant. Measurements $$y_i$$ are taken at time points $$t_i$$ and are subject to independent, identically distributed Gaussian noise:\n$$y_i = x(t_i) + \\varepsilon_i, \\quad \\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$$\nThe standard deviation of the noise, $$\\sigma$$, is assumed to be known. The parameters to be estimated from the data $$\\{y_i, t_i\\}$$ are $$\\theta$$ and $$x(0)$$. Our interest is in the identifiability of $$\\theta$$, treating $$x(0)$$ as a nuisance parameter.\n\nThe analysis proceeds by constructing the likelihood function, which quantifies how probable the observed data are for a given set of parameters. For a single data point $$y_i$$, the probability density function is:\n$$p(y_i | \\theta, x(0)) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(y_i - x(0)e^{-\\theta t_i})^2}{2\\sigma^2} \\right)$$\nAssuming independence of measurements, the total likelihood $$\\mathcal{L}(\\theta, x(0))$$ for the entire dataset of $$N$$ points is the product of individual probabilities:\n$$\\mathcal{L}(\\theta, x(0)) = \\prod_{i=1}^{N} p(y_i | \\theta, x(0))$$\nIt is numerically more convenient to work with the negative log-likelihood, $$\\ell(\\theta, x(0)) = -\\log(\\mathcal{L}(\\theta, x(0)))$$. Taking the logarithm and negating gives:\n$$\\ell(\\theta, x(0)) = \\frac{N}{2}\\log(2\\pi\\sigma^2) + \\frac{1}{2\\sigma^2} \\sum_{i=1}^{N} (y_i - x(0)e^{-\\theta t_i})^2$$\nTo find the maximum likelihood estimates (MLE) of the parameters, one minimizes this function.\n\nThe profile likelihood for the parameter of interest, $$\\theta$$, is obtained by minimizing the negative log-likelihood with respect to the nuisance parameter $$x(0)$$ for each fixed value of $$\\theta$$. We define the profile negative log-likelihood as:\n$$\\ell_{PL}(\\theta) = \\min_{x(0)} \\ell(\\theta, x(0))$$\nSince the constant term $$\\frac{N}{2}\\log(2\\pi\\sigma^2)$$ does not depend on $$x(0)$$, this is equivalent to minimizing the sum of squared residuals, $$S(\\theta, x(0)) = \\sum_{i=1}^{N} (y_i - x(0)e^{-\\theta t_i})^2$$. This is a linear least-squares problem for $$x(0)$$. To find the optimal $$x(0)$$, which we denote $$\\hat{x}(0)(\\theta)$$, we set the partial derivative of $$S$$ with respect to $$x(0)$$ to zero:\n$$\\frac{\\partial S}{\\partial x(0)} = -2 \\sum_{i=1}^{N} (y_i - x(0)e^{-\\theta t_i})e^{-\\theta t_i} = 0$$\nSolving for $$x(0)$$ yields the analytical solution for its optimal value for a given $$\\theta$$:\n$$\\hat{x}(0)(\\theta) = \\frac{\\sum_{i=1}^{N} y_i e^{-\\theta t_i}}{\\sum_{i=1}^{N} e^{-2\\theta t_i}}$$\nThe second derivative, $$\\frac{\\partial^2 S}{\\partial x(0)^2} = 2 \\sum_{i=1}^{N} e^{-2\\theta t_i}$$, is always positive for real $$t_i$$ and $$\\theta$$, confirming that this is a minimum.\n\nSubstituting $$\\hat{x}(0)(\\theta)$$ back into the negative log-likelihood expression gives the profile negative log-likelihood for $$\\theta$$:\n$$\\ell_{PL}(\\theta) = \\frac{N}{2}\\log(2\\pi\\sigma^2) + \\frac{1}{2\\sigma^2} \\sum_{i=1}^{N} (y_i - \\hat{x}(0)(\\theta)e^{-\\theta t_i})^2$$\n\nThe algorithm to be implemented is as follows:\n1. For each test case, generate or define the measurement data $$\\{y_i\\}$$ at time points $$\\{t_i\\}$$.\n2. Create a discrete grid of $$\\theta$$ values from $$\\theta_{\\min}$$ to $$\\theta_{\\max}$$ with step $$\\delta_\\theta$$.\n3. For each $$\\theta_j$$ on the grid, calculate $$\\hat{x}(0)(\\theta_j)$$ using the analytical formula above. Then, compute the value of the profile negative log-likelihood, $$\\ell_{PL}(\\theta_j)$$.\n4. Find the global minimum of the computed profile, $$\\ell_{\\min} = \\min_j \\ell_{PL}(\\theta_j)$$. The corresponding parameter value, $$\\hat{\\theta} = \\arg\\min_j \\ell_{PL}(\\theta_j)$$, is the maximum likelihood estimate of $$\\theta$$.\n5. Determine the confidence set based on the likelihood-ratio test. The set of parameters consistent with the data at a certain confidence level is given by $$\\{\\theta : \\ell_{PL}(\\theta) - \\ell_{\\min} \\le \\Delta \\ell^\\star\\}$$. Here, $$\\Delta \\ell^\\star$$ is a predefined threshold. The width of this set is the difference between its maximum and minimum values.\n6. Assess whether the profile likelihood exhibits a \"flat direction\" according to the specified criteria: either the confidence set intersects the boundaries of the $$\\theta$$ grid, or its width exceeds a fraction $$w_{\\text{flat}}$$ of the total grid span.\n\nThis procedure is applied to three test cases designed to illustrate different identifiability scenarios:\n- **Case 1:** Data from a full time-course is expected to constrain both $$x(0)$$ and $$\\theta$$ well, leading to a well-defined minimum in the profile likelihood and a narrow confidence interval. This represents an identifiable parameter.\n- **Case 2:** Data from late time points only, where the signal $$x(t)$$ is close to zero. The information content is low, making it difficult to distinguish different (sufficiently large) values of $$\\theta$$. This leads to a \"practically non-identifiable\" parameter, characterized by a wide, flat-bottomed likelihood profile and a large confidence interval.\n- **Case 3:** A single data point at a very late time (steady state), where $$x(t \\to \\infty) \\to 0$$. A measurement of zero can be perfectly explained by setting $$\\hat{x}(0) = 0$$ for any positive decay rate $$\\theta$$. The likelihood will be constant for all $$\\theta > 0$$, making the parameter \"structurally non-identifiable\" from this data. The confidence interval will span the entire parameter range.\nThe implementation will numerically execute this analysis for each case and report the findings as requested.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for parameter identifiability in an exponential decay model\n    using profile likelihood for three test cases.\n    \"\"\"\n\n    # Global analysis settings\n    theta_min = 0.001\n    theta_max = 3.000\n    delta_theta = 0.001\n    delta_ell_star = 2.0\n    w_flat = 0.7\n\n    # Test case definitions\n    test_cases = [\n        {\n            \"name\": \"Case 1: Full time-course, identifiable\",\n            \"true_params\": {\"theta\": 0.5, \"x0\": 5.0},\n            \"time_points\": np.array([0.0, 0.5, 1.0, 2.0, 3.5, 5.0, 6.0]),\n            \"sigma\": 0.05,\n            \"noise_seed\": 12345,\n            \"y_data\": None\n        },\n        {\n            \"name\": \"Case 2: Late-time-only, practically flat\",\n            \"true_params\": {\"theta\": 0.5, \"x0\": 5.0},\n            \"time_points\": np.array([5.0, 6.0, 8.0, 10.0, 12.0]),\n            \"sigma\": 0.05,\n            \"noise_seed\": 23456,\n            \"y_data\": None\n        },\n        {\n            \"name\": \"Case 3: Steady-state-only, structurally flat\",\n            \"true_params\": None,\n            \"time_points\": np.array([100.0]),\n            \"sigma\": 0.01,\n            \"noise_seed\": None,\n            \"y_data\": np.array([0.0])\n        }\n    ]\n\n    # Generate theta grid\n    # Use linspace for robustness against floating point errors with arange\n    num_points = int(round((theta_max - theta_min) / delta_theta)) + 1\n    theta_grid = np.linspace(theta_min, theta_max, num_points)\n\n    all_results = []\n\n    for case in test_cases:\n        t = case[\"time_points\"]\n        sigma = case[\"sigma\"]\n        y = case[\"y_data\"]\n        N = len(t)\n        \n        # Generate synthetic data if not provided directly\n        if y is None:\n            rng = np.random.default_rng(case[\"noise_seed\"])\n            theta_true = case[\"true_params\"][\"theta\"]\n            x0_true = case[\"true_params\"][\"x0\"]\n            x_true = x0_true * np.exp(-theta_true * t)\n            noise = rng.normal(0, sigma, size=N)\n            y = x_true + noise\n\n        # Compute profile negative log-likelihood\n        profile_nll = np.zeros_like(theta_grid)\n        \n        for i, theta_val in enumerate(theta_grid):\n            exp_t = np.exp(-theta_val * t)\n            exp_2t = np.exp(-2 * theta_val * t)\n            \n            numerator = np.sum(y * exp_t)\n            denominator = np.sum(exp_2t)\n\n            # If denominator underflows to 0, it means all exp(-theta*t) are\n            # effectively 0. The model predicts 0, and x0 is indeterminate.\n            # The RSS is then simply sum of squares of y.\n            if denominator <= np.finfo(float).tiny:\n                rss = np.sum(y**2)\n            else:\n                x0_hat = numerator / denominator\n                rss = np.sum((y - x0_hat * exp_t)**2)\n            \n            # The full negative log-likelihood formula\n            nll = N / 2 * np.log(2 * np.pi * sigma**2) + rss / (2 * sigma**2)\n            profile_nll[i] = nll\n\n        # Find MLE and confidence interval\n        nll_min = np.min(profile_nll)\n        mle_idx = np.argmin(profile_nll)\n        theta_mle = theta_grid[mle_idx]\n        \n        threshold = nll_min + delta_ell_star\n        confidence_indices = np.where(profile_nll <= threshold)[0]\n        \n        # Calculate width of confidence interval\n        if len(confidence_indices) > 0:\n            ci_min_theta = theta_grid[confidence_indices[0]]\n            ci_max_theta = theta_grid[confidence_indices[-1]]\n            width = ci_max_theta - ci_min_theta\n        else: # Should not happen if logic is correct\n            width = 0.0\n\n        # Check for flatness\n        touches_boundary = False\n        if len(confidence_indices) > 0:\n            touches_boundary = (confidence_indices[0] == 0) or \\\n                               (confidence_indices[-1] == len(theta_grid) - 1)\n        \n        grid_span = theta_max - theta_min\n        width_is_large = (width >= w_flat * grid_span)\n        \n        is_flat = touches_boundary or width_is_large\n        \n        all_results.append([theta_mle, width, is_flat])\n\n    # Format output string\n    result_strings = []\n    for res in all_results:\n        theta_str = f\"{res[0]:.6f}\"\n        width_str = f\"{res[1]:.6f}\"\n        bool_str = str(res[2])\n        result_strings.append(f\"[{theta_str},{width_str},{bool_str}]\")\n    \n    final_output = f\"[{','.join(result_strings)}]\"\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}