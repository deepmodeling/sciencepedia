{
    "hands_on_practices": [
        {
            "introduction": "Hierarchical clustering is a cornerstone of pattern discovery, but its behavior is dictated by the chosen linkage criterion. This exercise focuses on Ward's method, which builds the hierarchy by merging clusters that cause the minimum increase in the total within-cluster variance. By working through the calculation from first principles, you will gain a concrete understanding of how Ward linkage operates to create compact, spherical clusters, a fundamental skill for interpreting dendrograms in heatmaps .",
            "id": "4328388",
            "problem": "A high-throughput transcriptomic study in systems biomedicine generates a standardized heatmap of gene expression across three biological axes representing aggregated pathway activity scores. The data are $z$-score standardized per gene, making all variables dimensionless. A preliminary hierarchical clustering has produced three non-overlapping clusters of samples, with cluster sizes and centroids (in the three-dimensional pathway space) given by $n_1 = 12$, $n_2 = 18$, $n_3 = 10$ and\n$$\n\\boldsymbol{m}_1 = (0.5,\\,-0.2,\\,1.1),\\quad \\boldsymbol{m}_2 = (0.4,\\,-0.1,\\,0.9),\\quad \\boldsymbol{m}_3 = (-0.6,\\,0.3,\\,0.2).\n$$\nConsider agglomerative hierarchical clustering under Ward linkage, which at each step merges the pair of clusters that leads to the smallest increase in the within-cluster sum of squared errors (SSE). Let $J$ denote the total within-cluster SSE,\n$$\nJ \\equiv \\sum_{c} \\sum_{i \\in c} \\|\\boldsymbol{x}_i - \\boldsymbol{m}_c\\|_2^{2},\n$$\nwhere $\\|\\cdot\\|_2$ is the Euclidean norm, $\\boldsymbol{x}_i$ is the feature vector of sample $i$, and $\\boldsymbol{m}_c$ is the centroid of cluster $c$. Assume Euclidean geometry in the given feature space and that centroids are defined by arithmetic means.\n\nUsing only the definitions above as the fundamental base, determine the single merge that Ward linkage would perform next and compute the corresponding increase in $J$ caused by this merge. Report only the minimal increase in $J$ as a real number. Round your answer to four significant figures. The quantity $J$ is dimensionless.",
            "solution": "The problem requires the determination of the next merge in an agglomerative hierarchical clustering process using Ward linkage, and the calculation of the associated increase in the total within-cluster sum of squared errors, $J$. The initial state consists of three clusters with given sizes and centroids.\n\nThe total within-cluster sum of squared errors, $J$, is defined as:\n$$\nJ \\equiv \\sum_{c} \\sum_{i \\in c} \\|\\boldsymbol{x}_i - \\boldsymbol{m}_c\\|_2^{2}\n$$\nwhere $c$ indexes the clusters, $\\boldsymbol{x}_i$ is the feature vector for sample $i$, and $\\boldsymbol{m}_c$ is the centroid of cluster $c$.\n\nWard linkage operates by merging the pair of clusters $(c_a, c_b)$ that results in the minimum increase in $J$. Let us derive the expression for this increase, $\\Delta J_{ab}$. Before the merge, the contribution of clusters $c_a$ and $c_b$ to the total SSE is:\n$$\nJ_{ab}^{\\text{before}} = \\sum_{i \\in c_a} \\|\\boldsymbol{x}_i - \\boldsymbol{m}_a\\|_2^{2} + \\sum_{j \\in c_b} \\|\\boldsymbol{x}_j - \\boldsymbol{m}_b\\|_2^{2}\n$$\nwhere $n_a$ and $n_b$ are the sizes of clusters $c_a$ and $c_b$, and $\\boldsymbol{m}_a$ and $\\boldsymbol{m}_b$ are their respective centroids.\n\nUpon merging $c_a$ and $c_b$ into a new cluster $c_{ab}$, the new cluster will have a size $n_{ab} = n_a + n_b$. Its centroid, $\\boldsymbol{m}_{ab}$, is the arithmetic mean of all points in the new cluster:\n$$\n\\boldsymbol{m}_{ab} = \\frac{1}{n_a + n_b} \\left( \\sum_{i \\in c_a} \\boldsymbol{x}_i + \\sum_{j \\in c_b} \\boldsymbol{x}_j \\right) = \\frac{n_a \\boldsymbol{m}_a + n_b \\boldsymbol{m}_b}{n_a + n_b}\n$$\nThe SSE of this new merged cluster is:\n$$\nJ_{ab}^{\\text{after}} = \\sum_{k \\in c_{ab}} \\|\\boldsymbol{x}_k - \\boldsymbol{m}_{ab}\\|_2^{2}\n$$\nThe increase in the total SSE is $\\Delta J_{ab} = J_{ab}^{\\text{after}} - J_{ab}^{\\text{before}}$. Using the parallel axis theorem, the SSE of the new cluster can be related to the sum of the original clusters' SSEs and the distance between their centroids. The final, simplified formula for the increase in SSE due to merging clusters $c_a$ and $c_b$ under Ward linkage is:\n$$\n\\Delta J_{ab} = \\frac{n_a n_b}{n_a + n_b} \\|\\boldsymbol{m}_a - \\boldsymbol{m}_b\\|_2^{2}\n$$\nThe problem provides the cluster sizes $n_1 = 12$, $n_2 = 18$, $n_3 = 10$ and centroids $\\boldsymbol{m}_1 = (0.5, -0.2, 1.1)$, $\\boldsymbol{m}_2 = (0.4, -0.1, 0.9)$, and $\\boldsymbol{m}_3 = (-0.6, 0.3, 0.2)$. We must calculate $\\Delta J$ for each of the three possible merges.\n\n1.  Merge of clusters $1$ and $2$:\n    The weighting factor is $\\frac{n_1 n_2}{n_1 + n_2} = \\frac{12 \\times 18}{12 + 18} = \\frac{216}{30} = 7.2$.\n    The squared Euclidean distance between centroids is:\n    $\\|\\boldsymbol{m}_1 - \\boldsymbol{m}_2\\|_2^{2} = \\|(0.5 - 0.4, -0.2 - (-0.1), 1.1 - 0.9)\\|_2^{2} = \\|(0.1, -0.1, 0.2)\\|_2^{2}$\n    $\\|\\boldsymbol{m}_1 - \\boldsymbol{m}_2\\|_2^{2} = (0.1)^2 + (-0.1)^2 + (0.2)^2 = 0.01 + 0.01 + 0.04 = 0.06$.\n    The increase in SSE is $\\Delta J_{12} = 7.2 \\times 0.06 = 0.432$.\n\n2.  Merge of clusters $1$ and $3$:\n    The weighting factor is $\\frac{n_1 n_3}{n_1 + n_3} = \\frac{12 \\times 10}{12 + 10} = \\frac{120}{22} = \\frac{60}{11}$.\n    The squared Euclidean distance between centroids is:\n    $\\|\\boldsymbol{m}_1 - \\boldsymbol{m}_3\\|_2^{2} = \\|(0.5 - (-0.6), -0.2 - 0.3, 1.1 - 0.2)\\|_2^{2} = \\|(1.1, -0.5, 0.9)\\|_2^{2}$\n    $\\|\\boldsymbol{m}_1 - \\boldsymbol{m}_3\\|_2^{2} = (1.1)^2 + (-0.5)^2 + (0.9)^2 = 1.21 + 0.25 + 0.81 = 2.27$.\n    The increase in SSE is $\\Delta J_{13} = \\frac{60}{11} \\times 2.27 = \\frac{136.2}{11} \\approx 12.3818$.\n\n3.  Merge of clusters $2$ and $3$:\n    The weighting factor is $\\frac{n_2 n_3}{n_2 + n_3} = \\frac{18 \\times 10}{18 + 10} = \\frac{180}{28} = \\frac{45}{7}$.\n    The squared Euclidean distance between centroids is:\n    $\\|\\boldsymbol{m}_2 - \\boldsymbol{m}_3\\|_2^{2} = \\|(0.4 - (-0.6), -0.1 - 0.3, 0.9 - 0.2)\\|_2^{2} = \\|(1.0, -0.4, 0.7)\\|_2^{2}$\n    $\\|\\boldsymbol{m}_2 - \\boldsymbol{m}_3\\|_2^{2} = (1.0)^2 + (-0.4)^2 + (0.7)^2 = 1.00 + 0.16 + 0.49 = 1.65$.\n    The increase in SSE is $\\Delta J_{23} = \\frac{45}{7} \\times 1.65 = \\frac{74.25}{7} \\approx 10.6071$.\n\nComparing the three possible increases in $J$:\n$\\Delta J_{12} = 0.432$\n$\\Delta J_{13} \\approx 12.3818$\n$\\Delta J_{23} \\approx 10.6071$\n\nThe minimum increase is $\\min(\\Delta J_{12}, \\Delta J_{13}, \\Delta J_{23}) = \\Delta J_{12} = 0.432$.\nTherefore, Ward linkage would perform the merge of cluster $1$ and cluster $2$. The question asks for the minimal increase in $J$ caused by this merge.\nThe value is $0.432$. Rounding to four significant figures, we get $0.4320$.",
            "answer": "$$\n\\boxed{0.4320}\n$$"
        },
        {
            "introduction": "Real-world biological data, especially from single-cell experiments, is often noisy and contains outlier populations that can distort clustering results. This practice contrasts two different clustering philosophies: the partitioning approach of $k$-means, which assigns every point to a cluster, and the density-based approach of DBSCAN, which can explicitly identify and separate noise. Analyzing this hypothetical dataset will clarify how algorithm choice impacts outlier handling and sharpen your ability to select the right tool to reveal true biological structure versus technical artifacts .",
            "id": "4328353",
            "problem": "A cohort of single-cell transcriptomes from a tumor microenvironment is embedded into a two-dimensional Principal Component Analysis (PCA) space for exploratory clustering and heatmap-based pattern discovery. Consider the following set of projected cell coordinates (each $p_i \\in \\mathbb{R}^2$ represents one cell), where distances are to be interpreted under the Euclidean metric:\n- $p_1 = (0.00, 0.00)$, $p_2 = (0.15, 0.05)$, $p_3 = (-0.10, -0.05)$, $p_4 = (0.05, -0.10)$, $p_5 = (0.00, 0.20)$\n- $p_6 = (5.00, 5.00)$, $p_7 = (5.10, 5.20)$, $p_8 = (4.90, 5.00)$, $p_9 = (5.00, 4.80)$, $p_{10} = (5.20, 5.10)$\n- $p_{11} = (10.00, 0.00)$, $p_{12} = (-8.00, 7.00)$\n\nAssume you apply $k$-means with $K=2$ (using Euclidean distance) and Density-Based Spatial Clustering of Applications with Noise (DBSCAN) with parameters $\\varepsilon = 0.25$ and $\\text{minPts} = 3$, using the convention that a point is a core point if its closed $\\varepsilon$-neighborhood contains at least $\\text{minPts}$ points including the point itself. In downstream analysis, you construct a heatmap of gene expression patterns by taking cluster-wise averages and then computing gene-wise $Z$-scores across all cells retained for visualization.\n\nWhich of the following statements are correct about the resulting cluster assignments and the implications for heatmap-based pattern discovery in this dataset?\n\nA. With $\\varepsilon = 0.25$ and $\\text{minPts} = 3$, DBSCAN identifies two dense clusters corresponding to the groups near $(0,0)$ and $(5,5)$, and labels $p_{11}$ and $p_{12}$ as noise; in contrast, $k$-means with $K=2$ assigns all points, including $p_{11}$ and $p_{12}$, to one of the two clusters, which can shift centroids toward the outliers and mildly dilute cluster-average expression in the heatmap.\n\nB. Increasing $\\varepsilon$ from $0.25$ to $0.35$ will necessarily merge the two dense groups into a single DBSCAN cluster in this dataset.\n\nC. Using $k$-means with $K=3$ will necessarily allocate a separate cluster to the two outliers together, thereby preventing dilution of the dense-cluster means in the heatmap.\n\nD. In DBSCAN, a point with fewer than $\\text{minPts}$ neighbors can still be assigned to a cluster if it lies within $\\varepsilon$ of a core point; such a point is a border point and is not labeled as noise.\n\nE. Masking DBSCAN-labeled noise cells prior to computing gene-wise $Z$-scores reduces variance inflation from outliers and tends to sharpen cluster-specific patterns in the heatmap relative to including outliers as in $k$-means assignments.\n\nSelect all that apply.",
            "solution": "This problem requires analyzing the behavior of DBSCAN and $k$-means clustering on a dataset with two dense clusters and two distinct outliers, and then evaluating their downstream effects on heatmap visualization.\n\nFirst, we analyze the DBSCAN results with parameters $\\varepsilon = 0.25$ and $\\text{minPts} = 3$. A point is a core point if it has at least 3 neighbors (including itself) within a radius of $0.25$.\n-   **Group 1 ($\\{p_1, ..., p_5\\}$):** The points are all close to the origin. For example, the distances from $p_1=(0,0)$ to the other four points in this group are approximately $0.158$, $0.112$, $0.112$, and $0.20$, all of which are less than $\\varepsilon=0.25$. Therefore, the neighborhood of $p_1$ contains all 5 points of this group. Since $5 \\ge \\text{minPts}$, $p_1$ is a core point. By extension, all points in this group are density-connected and form a single cluster.\n-   **Group 2 ($\\{p_6, ..., p_{10}\\}$):** These points are all close to $(5,5)$. The distances from $p_6=(5,5)$ to the other four points in this group are all less than $0.25$. For instance, $d(p_6, p_7) = \\sqrt{(-0.1)^2 + (-0.2)^2} \\approx 0.224 \\le 0.25$. The neighborhood of $p_6$ contains all 5 points of this group, making $p_6$ a core point. All points in this group form a second cluster.\n-   **Outliers ($\\{p_{11}, p_{12}\\}$):** The points $p_{11}=(10,0)$ and $p_{12}=(-8,7)$ are very far from all other points and from each other. Their distance to any other point is much greater than $\\varepsilon=0.25$. They have no neighbors within their $\\varepsilon$-radius and are not within the radius of any core point. Therefore, DBSCAN will classify them as noise.\n\nNext, we analyze $k$-means with $K=2$. $k$-means partitions all data points into $K$ clusters. Given the data's structure, the algorithm will converge with one cluster centered near $(0,0)$ and the other near $(5,5)$. The outliers $p_{11}$ and $p_{12}$ must be assigned to one of these two clusters. $p_{11}=(10,0)$ is closer to $(5,5)$ than to $(0,0)$, so it will join the second cluster. $p_{12}=(-8,7)$ is closer to $(0,0)$ than to $(5,5)$, so it will join the first cluster. Their inclusion will pull the final cluster centroids away from the dense regions.\n\nNow we evaluate each statement:\n**A:** This statement accurately describes the outcomes. DBSCAN finds two clusters and labels the outliers as noise. $k$-means forces the outliers into the two clusters, which shifts the centroids and will \"dilute\" the average expression profiles computed from these clusters for the heatmap. This statement is **correct**.\n\n**B:** This statement is incorrect. For the two dense clusters to merge, a point from one cluster must be within distance $\\varepsilon$ of a point in the other. The minimum distance between the two clusters is much larger than the proposed $\\varepsilon=0.35$ (e.g., the distance between $p_2$ and $p_8$ is approximately $6.86$). Increasing $\\varepsilon$ to $0.35$ will not connect them. This statement is **incorrect**.\n\n**C:** This statement is incorrect. The word \"necessarily\" is false, as $k$-means is sensitive to initialization. More importantly, the two outliers $p_{11}$ and $p_{12}$ are very far from each other ($\\approx 19.31$ units apart). It is highly unlikely they would be grouped into a single cluster. A more probable outcome for $K=3$ would be that one outlier forms its own singleton cluster, while the other gets grouped with one of the dense clusters. This statement is **incorrect**.\n\n**D:** This statement provides a correct definition of a \"border point\" (or boundary point) in the DBSCAN algorithm. Border points belong to a cluster because they are in the neighborhood of a core point, even if they themselves do not meet the `minPts` criterion. They are explicitly not considered noise. This statement is **correct**.\n\n**E:** This statement correctly describes a key benefit of DBSCAN's noise-handling capability for downstream analysis. Outliers can dramatically inflate the variance (and thus standard deviation, $\\sigma$) used in $Z$-scoring. By masking the noise points identified by DBSCAN, the $Z$-scores for the remaining cells are calculated relative to a more representative mean and a smaller standard deviation. This increases the magnitude of the resulting $Z$-scores, leading to sharper contrast and more distinct patterns in the final heatmap. This statement is **correct**.\n\nThe correct statements are A, D, and E.",
            "answer": "$$\\boxed{ADE}$$"
        },
        {
            "introduction": "Modern systems biomedicine relies on integrating diverse data types, such as transcriptomics and proteomics, which have vastly different scales and distributions. This problem addresses the critical importance of a well-designed preprocessing pipeline, exploring how transformations and scaling make features comparable before applying a distance-based method like Ward linkage . By evaluating a comprehensive, state-of-the-art pipeline, you will develop the strategic thinking required to move from raw multi-omics measurements to robust, interpretable patient subtypes and meaningful heatmap visualizations.",
            "id": "4328399",
            "problem": "In a systems biomedicine study integrating multi-omics profiles, you plan to discover patient subtypes via hierarchical clustering and visualize sample-by-feature heatmaps. You have $n=120$ patients and three data blocks: messenger ribonucleic acid (mRNA) expression for $p_{\\mathrm{RNA}}=500$ signature genes (log-counts per million, approximately continuous), deoxyribonucleic acid (DNA) methylation for $p_{\\mathrm{Meth}}=1000$ cytosine-phosphate-guanine sites (beta values in $[0,1]$), and proteomics for $p_{\\mathrm{Prot}}=200$ proteins (intensities spanning several orders of magnitude). You plan to cluster patients (rows) using hierarchical agglomerative clustering with Ward linkage to support heatmap-based pattern discovery.\n\nBase definitions:\n- Euclidean distance between two samples is the square root of the sum of squared feature-wise differences across $p$ features.\n- Within-cluster sum of squares is the sum, over features and samples in a cluster, of squared deviations from the cluster mean.\n- Ward linkage chooses the pair of clusters to merge at each step to produce the smallest increase in the total within-cluster sum of squares.\n\nWhich option best explains, from these bases, why Ward linkage is sensitive to variable scaling in this setting and proposes a scientifically sound preprocessing pipeline to mitigate this sensitivity and yield interpretable sample clusters and heatmaps?\n\nA. Ward linkage is sensitive to scaling because the increase in within-cluster sum of squares at a merge depends on squared Euclidean distances between cluster means, so linearly rescaling a feature by a factor multiplies its contribution by the square of that factor; high-variance or wide-range modalities can dominate. A suitable pipeline is: remove obvious outliers via initial Principal Component Analysis (PCA) screening on a robustly scaled version; apply modality-appropriate variance-stabilizing transforms (e.g., $\\log_2$ transform with a small offset for proteomics, arcsine-square-root or logit for methylation, and $\\log_2(1+x)$ for mRNA counts); correct batch effects within each modality using an Empirical Bayes (EB) approach if batches exist; impute sporadic missing values using $k$-nearest neighbors (KNN) within modality; standardize each feature across samples to zero mean and unit variance; optionally apply block scaling to equalize the total variance contribution of each modality (e.g., multiply all features in modality $m$ by $1/\\sqrt{p_m}$); compute Euclidean distances on the standardized data and apply Ward linkage for sample clustering; display the heatmap with features z-scored across samples to emphasize relative patterns.\n\nB. Ward linkage is insensitive to scaling because it clusters based on centroids, not raw features; to further ensure invariance, rescale each sample to the $[0,1]$ range (minâ€“max per row), compute correlation distance instead of Euclidean distance, and apply Ward linkage; in the heatmap, normalize each row to mean $1$ and variance $1$.\n\nC. Ward linkage is sensitive to scaling only if features are not centered; once features are mean-centered, differences in scale no longer matter because the clustering is driven by centroid differences. Therefore, the proper pipeline is to subtract the across-sample mean from each feature, leave variances as they are to preserve biological signal amplitude, compute Euclidean distances, and apply Ward linkage; the heatmap should display centered but unscaled features.\n\nD. Ward linkage is sensitive to scaling only because Euclidean distance over-penalizes large differences; switching to Manhattan distance removes the issue. The pipeline is to leave features on their original scales to preserve interpretability, compute Manhattan distances, and apply Ward linkage directly; the heatmap should display raw (untransformed) values to avoid distortion of absolute levels.\n\nChoose the single best option.",
            "solution": "The problem requires identifying the correct explanation for Ward linkage's sensitivity to feature scaling and proposing a sound preprocessing pipeline for a multi-omics dataset.\n\nFirst, let's analyze the sensitivity of Ward linkage. The method merges clusters $C_i$ and $C_j$ that minimize the increase in the total within-cluster sum of squares (WCSS). This increase, $\\Delta_{ij}$, is proportional to the squared Euclidean distance between the cluster centroids, $d^2(\\bar{\\mathbf{x}}_i, \\bar{\\mathbf{x}}_j)$. The squared Euclidean distance is the sum of squared differences across all features: $d^2(\\bar{\\mathbf{x}}_i, \\bar{\\mathbf{x}}_j) = \\sum_{k=1}^{p} (\\bar{x}_{ik} - \\bar{x}_{jk})^2$.\n\nIf a feature $k$ is rescaled by a factor $\\alpha$, its contribution to this sum changes by $\\alpha^2$. In the given multi-omics dataset, proteomics intensities span several orders of magnitude, meaning their variance and numerical range are vastly larger than those of methylation beta values (constrained to $[0,1]$). Without scaling, the features with large variance (proteomics) will contribute overwhelmingly to the distance calculations, effectively making the clustering algorithm ignore the information from the other data types. Therefore, Ward linkage is highly sensitive to feature scaling.\n\nNow we evaluate the proposed options:\n\n**A.** This option provides a correct explanation for the sensitivity, noting that rescaling multiplies a feature's contribution by the square of the scaling factor, allowing high-variance modalities to dominate. The proposed pipeline is comprehensive and aligns with best practices in bioinformatics. It includes:\n1.  **Modality-specific transformations** (e.g., log for proteomics, logit for methylation) to stabilize variance.\n2.  **Data cleaning and correction** (batch correction, imputation).\n3.  **Feature standardization** (z-scoring to zero mean and unit variance) to make all features comparable, which directly addresses the scaling sensitivity.\n4.  Optional **block scaling** to give each modality equal total influence.\n5.  Correct application of **Ward linkage on Euclidean distances** after preprocessing.\n6.  Standard **heatmap visualization** using z-scored data.\nThis is a state-of-the-art approach. This option is **correct**.\n\n**B.** This option's premise that Ward linkage is insensitive to scaling is fundamentally wrong. Centroids are directly dependent on the scale of the features. The proposed pipeline is also flawed: scaling per-sample (row-wise) is generally incorrect for making features comparable, and using Ward linkage with correlation distance is conceptually inconsistent, as Ward's method is specifically derived to minimize variance in Euclidean space. This option is **incorrect**.\n\n**C.** This option incorrectly claims that mean-centering alone solves the scaling issue. Centering does not affect Euclidean distances between points. The recommendation to \"leave variances as they are\" is precisely the error that a proper pipeline must correct. This option is **incorrect**.\n\n**D.** This option is incorrect on multiple levels. First, while Manhattan distance is less sensitive to extreme outliers than Euclidean distance, it is still sensitive to the overall scale of features. A feature with a 1000-fold larger range will still dominate the sum. Second, and more importantly, Ward linkage is mathematically defined in terms of minimizing the sum of squared errors, which is intrinsically tied to Euclidean distance. Using Ward linkage with Manhattan distance is a conceptual mismatch. This option is **incorrect**.\n\nBased on the analysis, option A is the only one that correctly explains the problem and proposes a scientifically valid and comprehensive solution.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}