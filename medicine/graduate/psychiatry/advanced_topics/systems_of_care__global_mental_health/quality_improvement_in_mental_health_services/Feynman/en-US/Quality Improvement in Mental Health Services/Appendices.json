{
    "hands_on_practices": [
        {
            "introduction": "Statistical Process Control (SPC) is a foundational QI methodology for monitoring, controlling, and improving processes over time. A key tool in SPC is the control chart, which helps distinguish between common cause variation (the natural noise in a process) and special cause variation (unexpected events that signal a process change). This exercise  guides you through the construction of a p-chart from first principles to monitor a critical safety outcome—medication error rates—and demonstrates how to adapt the chart for the real-world complication of varying sample sizes each week.",
            "id": "4752709",
            "problem": "A psychiatric hospital’s medication safety team is conducting Statistical Process Control (SPC) using a proportion chart (p-chart) to monitor weekly medication error rates in an inpatient service. In each week, the denominator varies because the number of medication administrations changes with census and acuity. Let each administration be a Bernoulli trial with probability $p$ of an error. Over a stable $52$-week baseline, the team estimated the process center as $\\hat{p}=0.06$. For a new audit week with $n=200$ administrations, construct the $3$-standard-deviation control limits for the p-chart for that week from first principles, starting from the Bernoulli and Binomial models and the Central Limit Theorem (CLT). Then compute the numerical values of the lower control limit (LCL) and upper control limit (UCL) for that week using the baseline estimate $\\hat{p}=0.06$ and the week’s denominator $n=200$. If a limit falls outside the interval $[0,1]$, truncate it to the nearest boundary of that interval. Express both limits as decimal fractions (not percentages), and round each to four significant figures. Provide your final result as a two-entry row matrix of the form $\\begin{pmatrix} \\text{LCL}  \\text{UCL} \\end{pmatrix}$.",
            "solution": "The problem is subjected to validation.\n\n### Step 1: Extract Givens\n-   **Topic**: Statistical Process Control (SPC) using a proportion chart (p-chart).\n-   **Application**: Monitoring weekly medication error rates.\n-   **Data Structure**: The number of medication administrations ($n$) varies weekly.\n-   **Stochastic Model**: Each administration is a Bernoulli trial with probability $p$ of an error.\n-   **Baseline Data**: A stable $52$-week period is used to estimate the process center.\n-   **Process Center Estimate**: $\\hat{p}=0.06$.\n-   **Audit Week Sample Size**: $n=200$ administrations.\n-   **Task 1**: Construct $3$-standard-deviation control limits for the p-chart for this week.\n-   **Derivation Requirement**: The construction must be from first principles, starting from Bernoulli and Binomial models and the Central Limit Theorem (CLT).\n-   **Task 2**: Compute the numerical values of the Lower Control Limit (LCL) and Upper Control Limit (UCL).\n-   **Truncation Rule**: If a limit falls outside the interval $[0,1]$, it must be truncated to the nearest boundary of that interval.\n-   **Formatting**: Express limits as decimal fractions, rounded to four significant figures.\n-   **Final Answer Format**: A two-entry row matrix $\\begin{pmatrix} \\text{LCL}  \\text{UCL} \\end{pmatrix}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed against the validation criteria.\n-   **Scientifically Grounded**: The problem is based on the standard and widely accepted statistical theory of Statistical Process Control, specifically the construction of p-charts. The underlying probabilistic models (Bernoulli, Binomial) and the use of the Central Limit Theorem are the correct theoretical foundations. The application to medication error monitoring is a classic example of quality improvement in healthcare. The provided numerical values, $\\hat{p}=0.06$ and $n=200$, are realistic.\n-   **Well-Posed**: The problem is clearly defined and provides all necessary information to derive and calculate the control limits: the baseline proportion estimate ($\\hat{p}$), the sample size for the week in question ($n$), and the multiple for the standard deviation (3-sigma limits). The objective is unambiguous.\n-   **Objective**: The problem is stated in precise, objective language, free of any subjective or opinion-based assertions.\n\nThe problem is free of the listed flaws: it is scientifically sound, formalizable, complete, realistic, and well-posed.\n\n### Step 3: Verdict and Action\nThe problem is valid. The solution will now be provided.\n\n### Solution Derivation and Calculation\n\nThe construction of control limits for a p-chart from first principles begins with the underlying stochastic process.\n\n1.  **Bernoulli Trial**: Each medication administration is modeled as an independent Bernoulli trial. Let $X_i$ be a random variable representing the outcome of the $i$-th administration, for $i \\in \\{1, 2, \\dots, n\\}$. We define $X_i = 1$ if a medication error occurs (a \"nonconforming\" event) and $X_i = 0$ otherwise. The probability of an error is $P(X_i=1) = p$. The mean and variance of this trial are:\n    $$E[X_i] = p$$\n    $$\\text{Var}(X_i) = p(1-p)$$\n\n2.  **Binomial Distribution**: The total number of errors, $Y$, in a sample of $n$ administrations is the sum of these independent Bernoulli trials: $Y = \\sum_{i=1}^{n} X_i$. Thus, $Y$ follows a Binomial distribution, $Y \\sim \\text{Bin}(n, p)$. The mean and variance of $Y$ are:\n    $$E[Y] = E\\left[\\sum_{i=1}^{n} X_i\\right] = \\sum_{i=1}^{n} E[X_i] = np$$\n    $$\\text{Var}(Y) = \\text{Var}\\left[\\sum_{i=1}^{n} X_i\\right] = \\sum_{i=1}^{n} \\text{Var}(X_i) = np(1-p)$$\n\n3.  **Sample Proportion**: The p-chart tracks the sample proportion of errors, which we denote as $\\hat{p}_{sample}$ for a given week. This is calculated as the number of errors divided by the total number of administrations:\n    $$\\hat{p}_{sample} = \\frac{Y}{n}$$\n    The expected value (mean) of the sample proportion is:\n    $$E[\\hat{p}_{sample}] = E\\left[\\frac{Y}{n}\\right] = \\frac{1}{n}E[Y] = \\frac{np}{n} = p$$\n    The variance of the sample proportion is:\n    $$\\text{Var}(\\hat{p}_{sample}) = \\text{Var}\\left[\\frac{Y}{n}\\right] = \\frac{1}{n^2}\\text{Var}(Y) = \\frac{np(1-p)}{n^2} = \\frac{p(1-p)}{n}$$\n    The standard deviation of the sample proportion, known as its standard error, is therefore:\n    $$\\sigma_{\\hat{p}_{sample}} = \\sqrt{\\frac{p(1-p)}{n}}$$\n\n4.  **Central Limit Theorem (CLT)**: For a sufficiently large sample size $n$, the Central Limit Theorem states that the distribution of the sample proportion $\\hat{p}_{sample}$ is approximately Normal, with the mean and variance derived above.\n    $$\\hat{p}_{sample} \\approx N\\left(\\mu = p, \\sigma^2 = \\frac{p(1-p)}{n}\\right)$$\n    The approximation is generally considered valid when $np \\geq 5$ and $n(1-p) \\geq 5$.\n\n5.  **Control Limits Construction**: The $3$-standard-deviation ($3$-sigma) control limits for any process are defined as $\\mu \\pm 3\\sigma$. For the p-chart, the process mean is $p$ and the process standard deviation is $\\sigma_{\\hat{p}_{sample}}$. In practice, the true process proportion $p$ is unknown and is estimated by the center line $\\hat{p}$ derived from stable baseline data.\n    The formulas for the control limits are:\n    -   Center Line (CL): $\\hat{p}$\n    -   Upper Control Limit (UCL): $\\hat{p} + 3\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}$\n    -   Lower Control Limit (LCL): $\\hat{p} - 3\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}$\n\nNow, we will compute the numerical values for the specific audit week.\n\n**Given values**:\n-   Baseline process proportion estimate: $\\hat{p} = 0.06$\n-   Sample size for the week: $n = 200$\n\nFirst, we verify the conditions for the Normal approximation using our estimates:\n-   $n\\hat{p} = 200 \\times 0.06 = 12$. Since $12 \\geq 5$, this condition is met.\n-   $n(1-\\hat{p}) = 200 \\times (1 - 0.06) = 200 \\times 0.94 = 188$. Since $188 \\geq 5$, this condition is also met.\nThe use of the CLT and the Normal approximation is justified.\n\nNext, we calculate the standard error for the sample proportion for this week:\n$$\\sigma_{\\hat{p}_{sample}} = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} = \\sqrt{\\frac{0.06(1 - 0.06)}{200}} = \\sqrt{\\frac{0.06 \\times 0.94}{200}} = \\sqrt{\\frac{0.0564}{200}} = \\sqrt{0.000282}$$\n$$\\sigma_{\\hat{p}_{sample}} \\approx 0.0167928556$$\n\nNow we calculate the $3$-sigma limits:\n-   **UCL**:\n    $$\\text{UCL} = \\hat{p} + 3\\sigma_{\\hat{p}_{sample}} = 0.06 + 3 \\times 0.0167928556 = 0.06 + 0.0503785668 = 0.1103785668$$\n-   **LCL**:\n    $$\\text{LCL} = \\hat{p} - 3\\sigma_{\\hat{p}_{sample}} = 0.06 - 3 \\times 0.0167928556 = 0.06 - 0.0503785668 = 0.0096214332$$\n\nThe calculated limits are $\\text{LCL} \\approx 0.00962$ and $\\text{UCL} \\approx 0.1104$. Both limits are within the valid interval for a proportion, $[0, 1]$. Therefore, no truncation is necessary.\n\nFinally, we round the results to four significant figures as required:\n-   $\\text{LCL} = 0.0096214332 \\rightarrow 0.009621$\n-   $\\text{UCL} = 0.1103785668 \\rightarrow 0.1104$\n\nThe control limits for the audit week with $n=200$ are $\\text{LCL} = 0.009621$ and $\\text{UCL} = 0.1104$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.009621  0.1104\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Long waiting times and system congestion are significant barriers to accessing mental healthcare. Queueing theory provides a powerful mathematical framework for analyzing and improving patient flow. This practice problem  introduces Little's Law, a remarkably simple yet profound relationship ($L = \\lambda W$) that connects the average number of patients in a system ($L$), their arrival rate ($\\lambda$), and the average time they spend there ($W$). By deriving and applying this law, you will gain a fundamental tool for diagnosing bottlenecks and quantifying the impact of changes aimed at improving system efficiency.",
            "id": "4752744",
            "problem": "A mental health intake clinic seeks to improve patient flow using principles from Quality Improvement (QI). Consider a single-class, first-come-first-served intake system where patients arrive according to a stationary, ergodic process with long-run average arrival rate $\\lambda$ (patients per day), and the system is stable in the sense that the long-run throughput rate equals the long-run arrival rate and all expectations are finite. Let $N(t)$ denote the number of patients in the intake system at time $t$, let $A(T)$ denote the total number of arrivals in the interval $[0,T]$, and let $W$ denote the long-run average total time a patient spends in the system (sojourn time), including waiting and assessment. Define $L$ to be the long-run time-average number of patients in the system, that is, the limit of $\\frac{1}{T}\\int_{0}^{T} N(t)\\,\\mathrm{d}t$ as $T \\to \\infty$, assuming this limit exists.\n\nStarting only from conservation of flow and time-averaging principles applicable to stable service systems, derive the canonical relationship between $L$, $\\lambda$, and $W$ that links the time-average patient count, the long-run arrival rate, and the average sojourn time. Then, for an intake clinic with long-run average arrival rate $\\lambda = 10$ patients per day and average sojourn time $W = 3$ days, compute the expected long-run number of patients in the intake system.\n\nExpress your final numerical answer as a count of patients (unit: patients). No rounding is required; provide the exact value.",
            "solution": "The problem statement is evaluated as scientifically grounded, well-posed, objective, and complete. It presents a classic problem in queueing theory, asking for the derivation of Little's Law from first principles and its application to a specific scenario. The provided definitions and assumptions—stationary, ergodic arrival process; a stable system where throughput equals arrival rate; the existence of long-run averages—are standard and sufficient for a rigorous solution. The problem is therefore deemed valid.\n\nThe problem asks for two parts: first, to derive the canonical relationship between the long-run time-average number of patients in the system, $L$, the long-run average arrival rate, $\\lambda$, and the long-run average sojourn time, $W$. Second, to compute $L$ given specific values for $\\lambda$ and $W$.\n\nThe given definitions are:\n1.  The long-run time-average number of patients in the system, $L$:\n    $$L = \\lim_{T \\to \\infty} \\frac{1}{T}\\int_{0}^{T} N(t)\\,\\mathrm{d}t$$\n    where $N(t)$ is the number of patients in the system at time $t$.\n\n2.  The long-run average arrival rate, $\\lambda$:\n    $$\\lambda = \\lim_{T \\to \\infty} \\frac{A(T)}{T}$$\n    where $A(T)$ is the total number of arrivals in the interval $[0,T]$.\n\n3.  The long-run average sojourn time, $W$. This is the average time a patient spends in the system. If $W_i$ is the sojourn time of the $i$-th patient, then $W$ is the long-run average of these values.\n\nThe core of the derivation is to account for the total \"patient-time\" accumulated in the system over a long time interval $[0, T]$ in two different ways. The total accumulated patient-time, which we can denote as $C(T)$, is represented by the integral of the number of patients over time:\n$$C(T) = \\int_{0}^{T} N(t)\\,\\mathrm{d}t$$\nBy definition, dividing this quantity by $T$ and taking the limit as $T \\to \\infty$ gives $L$.\n\nNow, let's consider this total accumulated patient-time from the perspective of individual patients. The total time $C(T)$ is the sum of the durations that each patient spends in the system during the interval $[0, T]$. For a stable system and a sufficiently large $T$, we can approximate this sum by considering all the patients who arrive within the interval $[0, T]$. The total time spent in the system by these $A(T)$ patients is $\\sum_{i=1}^{A(T)} W_i$, where $W_i$ represents the sojourn time for the $i$-th patient who arrived.\n\nThe key insight, formalized by John Little, is that for a stable system operating under ergodic assumptions, these two quantities are asymptotically equal. That is:\n$$\\int_{0}^{T} N(t)\\,\\mathrm{d}t \\approx \\sum_{i=1}^{A(T)} W_i$$\nThe approximation arises from neglecting the initial conditions (patients present at $t=0$) and edge effects (patients who have arrived but not yet departed by time $T$). In the limit as $T \\to \\infty$, the effects of these boundary conditions become negligible relative to the total accumulated time.\n\nTo formalize the relationship, we divide both sides by $T$ and take the limit as $T \\to \\infty$:\n$$\\lim_{T \\to \\infty} \\frac{1}{T}\\int_{0}^{T} N(t)\\,\\mathrm{d}t = \\lim_{T \\to \\infty} \\frac{1}{T} \\sum_{i=1}^{A(T)} W_i$$\nThe left side is, by definition, $L$. We can manipulate the right side by multiplying and dividing by $A(T)$:\n$$L = \\lim_{T \\to \\infty} \\left( \\frac{A(T)}{T} \\cdot \\frac{1}{A(T)} \\sum_{i=1}^{A(T)} W_i \\right)$$\nSince the limits of the individual factors exist (as per the problem statement's stability and ergodicity assumptions), the limit of the product is the product of the limits:\n$$L = \\left( \\lim_{T \\to \\infty} \\frac{A(T)}{T} \\right) \\cdot \\left( \\lim_{T \\to \\infty} \\frac{1}{A(T)} \\sum_{i=1}^{A(T)} W_i \\right)$$\nWe identify the terms based on their definitions:\n- The first term is the long-run average arrival rate: $\\lim_{T \\to \\infty} \\frac{A(T)}{T} = \\lambda$.\n- The second term is the long-run average sojourn time. As $T \\to \\infty$, the number of arrivals $A(T) \\to \\infty$. The expression is the average sojourn time over an increasing number of patients, which converges to $W$.\n\nTherefore, we arrive at the canonical relationship known as Little's Law:\n$$L = \\lambda W$$\nThis formula elegantly links the time-average number of items in a system ($L$) to the long-run average arrival rate ($\\lambda$) and the long-run average time an item spends in that system ($W$).\n\nNow, for the second part of the problem, we must compute the expected long-run number of patients, $L$, for the specific clinic. The given values are:\n-   Long-run average arrival rate, $\\lambda = 10$ patients per day.\n-   Average sojourn time, $W = 3$ days.\n\nSubstituting these values into the derived formula:\n$$L = \\lambda W = (10 \\text{ patients/day}) \\times (3 \\text{ days})$$\n$$L = 30 \\text{ patients}$$\nThe units are consistent, as (patients/day) $\\times$ (days) yields a dimensionless quantity representing a count of patients. The expected long-run number of patients in the intake system is $30$.",
            "answer": "$$\\boxed{30}$$"
        },
        {
            "introduction": "Comparing performance across different healthcare providers is essential for accountability and quality improvement, but it is fraught with methodological challenges. Unadjusted outcome metrics, such as raw readmission rates, can be deeply misleading if one hospital treats a sicker or more complex patient population than another. This exercise  delves into the critical concepts of case-mix and risk adjustment, teaching you how to use expected outcomes to create fairer, more insightful comparisons. Mastering this skill is crucial for avoiding incorrect conclusions about provider quality and for identifying true opportunities for improvement.",
            "id": "4752707",
            "problem": "A public mental health authority compares psychiatric hospitals using $30$-day all-cause readmission rates after inpatient discharge. Hospital Alpha and Hospital Beta each had $N = 400$ discharges last quarter. The authority also publishes reference, stratum-specific $30$-day readmission probabilities based on a statewide cohort, defined by four mutually exclusive patient strata that jointly capture clinical severity, comorbidity burden, substance use disorder (SUD), and social instability (e.g., housing insecurity). The strata and their reference baseline probabilities are: Stratum $1$: low severity, no SUD, stable housing, $p_1 = 0.08$; Stratum $2$: moderate severity or SUD, stable housing, $p_2 = 0.15$; Stratum $3$: high severity with SUD or unstable housing, $p_3 = 0.25$; Stratum $4$: very high severity with multimorbidity and unstable housing, $p_4 = 0.35$. Hospital Alpha’s case-mix counts are $(n_1, n_2, n_3, n_4) = (60, 100, 180, 60)$ with observed readmissions $Y_\\text{Alpha} = 72$. Hospital Beta’s case-mix counts are $(n_1, n_2, n_3, n_4) = (200, 140, 50, 10)$ with observed readmissions $Y_\\text{Beta} = 64$. Using first principles from probability and epidemiology—specifically conditional probability, expected value, and the definition of confounding—reason about what “case-mix” and “risk adjustment” mean in this context and whether unadjusted hospital-level proportions $\\hat r = Y/N$ are appropriate for comparing performance. Then, determine which statements are correct. Select all that apply.\n\nA. “Case-mix” is the joint distribution of patient-level covariates that causally or predictively influence readmission risk; “risk adjustment” estimates $E[\\text{readmissions} \\mid \\text{covariates}]$ to separate performance from patient risk. Using the stratum-specific baseline probabilities, Hospital Alpha’s standardized readmission ratio is less than $1$ while Hospital Beta’s is greater than $1$, so unadjusted rates mislead by favoring the easier case-mix hospital.\n\nB. “Case-mix” refers to hospital-level process measures (e.g., length of stay and staffing) rather than patient-level characteristics; if $N_\\text{Alpha} = N_\\text{Beta}$, risk adjustment is unnecessary for fair comparison.\n\nC. After risk adjustment based on the given strata, both hospitals perform equivalently because their observed rates are within $2\\%$ of each other.\n\nD. Unadjusted $30$-day readmission rates can mislead because they conflate differences in patient risk with differences in hospital performance; when expected readmissions differ across hospitals due to case-mix, comparing observed proportions alone introduces confounding.",
            "solution": "The problem statement is a well-posed exercise in applied epidemiology and biostatistics concerning risk adjustment in health services research. It is scientifically grounded, internally consistent, and contains all necessary information. Therefore, a full analysis is warranted.\n\nThe core task is to compare the performance of two psychiatric hospitals, Alpha and Beta, based on their $30$-day readmission rates, while accounting for differences in their patient populations, a concept known as \"case-mix.\"\n\nFirst, we define the key terms based on first principles in epidemiology.\n- **Unadjusted Readmission Rate ($\\hat{r}$)**: This is the overall proportion of patients readmitted, calculated as the total number of observed readmissions ($Y$) divided by the total number of discharges ($N$). It is a crude measure that does not account for patient-level risk factors.\n- **Case-Mix**: This refers to the distribution of patient characteristics that influence the outcome of interest (readmission). In this problem, the case-mix is the distribution of patients across the four risk strata, given by the counts $(n_1, n_2, n_3, n_4)$ for each hospital. A hospital with a higher proportion of patients in high-risk strata (e.g., Strata $3$ and $4$) is said to have a more challenging or \"sicker\" case-mix.\n- **Risk Adjustment**: This is the process of accounting for differences in case-mix to allow for a fairer comparison of outcomes between providers. A common method is to compare the observed number of events to an expected number.\n- **Expected Readmissions ($E$)**: This is the number of readmissions that would be expected in a hospital if its patients experienced the reference, stratum-specific readmission probabilities ($p_i$). Using the law of total expectation, it is calculated as the sum of the products of the number of patients in each stratum ($n_i$) and the reference probability for that stratum ($p_i$): $E = \\sum_{i=1}^{4} n_i p_i$.\n- **Standardized Readmission Ratio (SRR)**: This is a risk-adjusted measure of performance, defined as the ratio of observed readmissions ($Y$) to expected readmissions ($E$). $SRR = Y/E$. An $SRR  1$ indicates that the hospital had more readmissions than expected for its case-mix (poorer performance), while an $SRR  1$ indicates fewer readmissions than expected (better performance).\n- **Confounding**: In this context, case-mix acts as a confounder. It is a variable that is associated with both the exposure (the hospital) and the outcome (readmission), but it is not on the causal pathway from hospital quality to readmission. If one hospital has a sicker case-mix than another, its unadjusted readmission rate may be higher simply due to the higher intrinsic risk of its patients, not necessarily due to lower quality of care. Comparing unadjusted rates without accounting for this confounding can lead to incorrect conclusions about hospital performance.\n\nWe now apply these principles to the data provided.\n\n**Hospital Alpha**\n- Total Discharges: $N_\\text{Alpha} = 400$\n- Observed Readmissions: $Y_\\text{Alpha} = 72$\n- Case-Mix Counts: $(n_{\\text{Alpha},1}, n_{\\text{Alpha},2}, n_{\\text{Alpha},3}, n_{\\text{Alpha},4}) = (60, 100, 180, 60)$\n- Reference Probabilities: $(p_1, p_2, p_3, p_4) = (0.08, 0.15, 0.25, 0.35)$\n\nUnadjusted Readmission Rate for Alpha:\n$$ \\hat{r}_\\text{Alpha} = \\frac{Y_\\text{Alpha}}{N_\\text{Alpha}} = \\frac{72}{400} = 0.18 $$\n\nExpected Readmissions for Alpha:\n$$ E_\\text{Alpha} = \\sum_{i=1}^{4} n_{\\text{Alpha},i} p_i = (60 \\times 0.08) + (100 \\times 0.15) + (180 \\times 0.25) + (60 \\times 0.35) $$\n$$ E_\\text{Alpha} = 4.8 + 15 + 45 + 21 = 85.8 $$\n\nStandardized Readmission Ratio for Alpha:\n$$ SRR_\\text{Alpha} = \\frac{Y_\\text{Alpha}}{E_\\text{Alpha}} = \\frac{72}{85.8} \\approx 0.839 $$\n\n**Hospital Beta**\n- Total Discharges: $N_\\text{Beta} = 400$\n- Observed Readmissions: $Y_\\text{Beta} = 64$\n- Case-Mix Counts: $(n_{\\text{Beta},1}, n_{\\text{Beta},2}, n_{\\text{Beta},3}, n_{\\text{Beta},4}) = (200, 140, 50, 10)$\n- Reference Probabilities: $(p_1, p_2, p_3, p_4) = (0.08, 0.15, 0.25, 0.35)$\n\nUnadjusted Readmission Rate for Beta:\n$$ \\hat{r}_\\text{Beta} = \\frac{Y_\\text{Beta}}{N_\\text{Beta}} = \\frac{64}{400} = 0.16 $$\n\nExpected Readmissions for Beta:\n$$ E_\\text{Beta} = \\sum_{i=1}^{4} n_{\\text{Beta},i} p_i = (200 \\times 0.08) + (140 \\times 0.15) + (50 \\times 0.25) + (10 \\times 0.35) $$\n$$ E_\\text{Beta} = 16 + 21 + 12.5 + 3.5 = 53.0 $$\n\nStandardized Readmission Ratio for Beta:\n$$ SRR_\\text{Beta} = \\frac{Y_\\text{Beta}}{E_\\text{Beta}} = \\frac{64}{53.0} \\approx 1.208 $$\n\n**Summary of Findings:**\n- Unadjusted rates: $\\hat{r}_\\text{Alpha} = 18\\%$, $\\hat{r}_\\text{Beta} = 16\\%$. On this crude basis, Hospital Beta appears to perform better.\n- Case-Mix: Hospital Alpha has a much higher-risk patient population (a total of $180+60=240$ patients in the two highest risk strata) compared to Hospital Beta (a total of $50+10=60$ patients in those strata).\n- Risk-Adjusted Ratios: $SRR_\\text{Alpha} \\approx 0.839$ (less than $1$) and $SRR_\\text{Beta} \\approx 1.208$ (greater than $1$). After adjusting for case-mix, Hospital Alpha performed better than expected, while Hospital Beta performed worse than expected. The conclusion is reversed from the unadjusted comparison. The difference in case-mix confounded the initial comparison.\n\nNow we evaluate each option.\n\nA. “Case-mix” is the joint distribution of patient-level covariates that causally or predictively influence readmission risk; “risk adjustment” estimates $E[\\text{readmissions} \\mid \\text{covariates}]$ to separate performance from patient risk. Using the stratum-specific baseline probabilities, Hospital Alpha’s standardized readmission ratio is less than $1$ while Hospital Beta’s is greater than $1$, so unadjusted rates mislead by favoring the easier case-mix hospital.\n- The definitions of \"case-mix\" and \"risk adjustment\" are conceptually correct and standard in the field. Estimating expected readmissions conditional on covariates is the essence of risk adjustment.\n- Our calculation shows $SRR_\\text{Alpha} \\approx 0.839  1$ and $SRR_\\text{Beta} \\approx 1.208  1$. This part of the statement is correct.\n- Hospital Beta has the easier case-mix (more patients in low-risk strata) and a lower unadjusted rate. The unadjusted rates do indeed mislead by making Beta look better. This statement is fully consistent with our analysis.\n**Verdict: Correct.**\n\nB. “Case-mix” refers to hospital-level process measures (e.g., length of stay and staffing) rather than patient-level characteristics; if $N_\\text{Alpha} = N_\\text{Beta}$, risk adjustment is unnecessary for fair comparison.\n- The definition of \"case-mix\" is incorrect. Case-mix refers to patient-level characteristics, not hospital-level processes. Hospital processes are often considered indicators of quality, the very thing being measured, not confounders to be adjusted for in this manner.\n- The claim that equal sample sizes ($N_\\text{Alpha} = N_\\text{Beta}$) obviate the need for risk adjustment is false. Equal total numbers of patients do not imply an equal distribution of risk factors within those patient populations, as demonstrated by the data in this problem.\n**Verdict: Incorrect.**\n\nC. After risk adjustment based on the given strata, both hospitals perform equivalently because their observed rates are within $2\\%$ of each other.\n- The conclusion that \"both hospitals perform equivalently\" after risk adjustment is false. Our analysis shows that $SRR_\\text{Alpha}$ is substantially less than $1$ and $SRR_\\text{Beta}$ is substantially greater than $1$, indicating a clear difference in performance after adjustment.\n- The justification, \"because their observed rates are within $2\\%$ of each other\" ($\\hat{r}_\\text{Alpha}=18\\%$, $\\hat{r}_\\text{Beta}=16\\%$), refers to the unadjusted rates. This small unadjusted difference is irrelevant to the post-adjustment conclusion and, in fact, masks the true performance difference. The logic is flawed.\n**Verdict: Incorrect.**\n\nD. Unadjusted $30$-day readmission rates can mislead because they conflate differences in patient risk with differences in hospital performance; when expected readmissions differ across hospitals due to case-mix, comparing observed proportions alone introduces confounding.\n- This statement provides a precise and fundamental explanation of confounding in the context of hospital quality comparison. The unadjusted rate is a function of both patient risk and hospital performance.\n- The condition \"when expected readmissions differ across hospitals due to case-mix\" is met in our problem ($E_\\text{Alpha} = 85.8 \\neq E_\\text{Beta} = 53.0$).\n- The conclusion that \"comparing observed proportions alone introduces confounding\" is the correct epidemiological principle. This statement accurately diagnoses the methodological flaw of using crude rates for comparison in this scenario.\n**Verdict: Correct.**",
            "answer": "$$\\boxed{AD}$$"
        }
    ]
}