{
    "hands_on_practices": [
        {
            "introduction": "The long-term safety of Deep Brain Stimulation (DBS) hinges on fundamental electrochemical principles at the electrode-tissue interface. To prevent irreversible reactions that can damage neural tissue and corrode the electrode, stimulation pulses must be meticulously charge-balanced. This exercise  grounds your understanding in this critical safety requirement by asking you to first explain the biophysical rationale and then derive and calculate the net DC offset from an imperfectly balanced pulse, directly linking theory to a quantitative safety assessment.",
            "id": "4704925",
            "problem": "A Deep Brain Stimulation (DBS) system delivers periodic current pulses through a platinum-iridium macroelectrode into subcortical tissue. In practice, DBS stimulators use biphasic pulses designed to be charge-balanced to avoid net Direct Current (DC) passing through the electrode-tissue interface. Start from the following fundamental bases: the definition of electric charge as the time integral of current, $Q=\\int i(t)\\,dt$, Faraday’s laws of electrolysis relating net charge transfer to the extent of electrode reactions, and the definition of the time-average of a periodic function, $\\overline{i}=\\frac{1}{T}\\int_{0}^{T}i(t)\\,dt$ for period $T$.\n\nYou are asked to address both the biophysical rationale and the quantitative consequences of asymmetry:\n\n1. Explain, using the above bases and the concept of the electrochemical “water window” for noble metals, why DBS pulses must be charge-balanced (i.e., have net $\\int i(t)\\,dt$ per cycle equal to zero) to minimize irreversible Faradaic reactions that cause local pH shifts, gas evolution, tissue injury, and electrode corrosion.\n\n2. Consider an asymmetric biphasic waveform with a cathodic first phase of constant current $I_{c}<0$ for duration $t_{c}>0$, an anodic second phase of constant current $I_{a}>0$ for duration $t_{a}>0$, and an interphase gap of duration $t_{g}>0$ with zero current. Outside the pulses, the current is zero. Derive, from first principles, a closed-form expression for the net DC offset (the time-averaged current over one period) $\\overline{I}_{\\mathrm{DC}}$ in terms of $I_{c}$, $t_{c}$, $I_{a}$, $t_{a}$, and the stimulation frequency $f$ (so $T=\\frac{1}{f}$).\n\n3. A DBS device operates at frequency $f=130\\ \\mathrm{Hz}$ with $t_{c}=70\\ \\mu\\mathrm{s}$, $t_{a}=90\\ \\mu\\mathrm{s}$, $t_{g}=100\\ \\mu\\mathrm{s}$, $I_{c}=-2.5\\ \\mathrm{mA}$, and $I_{a}=+2.45\\ \\mathrm{mA}$. Using your derived expression, compute the numerical value of $\\overline{I}_{\\mathrm{DC}}$. Express the final answer in microamperes and round your answer to four significant figures. Assume zero current during the interphase gap and between pulses.",
            "solution": "The problem will be addressed in three parts as requested: first, a biophysical explanation for charge-balanced pulses; second, the derivation of an expression for the DC offset; and third, the calculation of this offset for the given parameters.\n\n### Part 1: Rationale for Charge-Balanced Pulses\n\nThe requirement for charge-balanced pulses in Deep Brain Stimulation (DBS) is a direct consequence of fundamental electrochemical principles, specifically Faraday's laws of electrolysis, applied to the electrode-tissue interface.\n\n1.  **Charge and Faradaic Reactions:** Faraday's laws state that the amount of a substance produced or consumed in an electrochemical reaction is directly proportional to the total electric charge passed through the electrode. The net charge, $Q_{\\mathrm{net}}$, delivered over one stimulation period, $T$, is given by the time integral of the current, $i(t)$, over that period:\n    $$ Q_{\\mathrm{net}} = \\int_{0}^{T} i(t) \\, dt $$\n    If $Q_{\\mathrm{net}}$ is non-zero, it signifies a net flow of electrons in one direction, which must be balanced by a net chemical transformation (a Faradaic reaction) at the electrode-tissue interface. A non-zero time-averaged current, $\\overline{I}_{\\mathrm{DC}} = \\frac{1}{T}\\int_{0}^{T}i(t)\\,dt = \\frac{Q_{\\mathrm{net}}}{T}$, is thus a direct measure of the rate of net Faradaic reactions.\n\n2.  **The Electrochemical Water Window:** The brain's interstitial fluid is an aqueous electrolyte solution. For a given electrode material, such as the platinum-iridium alloy used in DBS, there exists a range of electrode potentials within which water is thermodynamically stable and does not undergo oxidation or reduction. This range is known as the electrochemical \"water window\". Within this window, charge can be transferred safely through primarily non-Faradaic mechanisms (charging and discharging the capacitive double layer at the interface) and reversible Faradaic reactions (e.g., formation of a monolayer of platinum oxide). These processes do not cause cumulative damage.\n\n3.  **Consequences of Charge Imbalance:** If a pulse is not charge-balanced, $Q_{\\mathrm{net}} \\neq 0$. This continuous injection of net charge drives the electrode's potential towards and eventually beyond the limits of the water window.\n    *   **Excessive Cathodic Charge ($Q_{\\mathrm{net}} < 0$):** The electrode potential becomes highly negative, leading to the reduction of water:\n        $$ 2H_2O + 2e^- \\rightarrow H_2(g) + 2OH^- $$\n        This reaction produces hydrogen gas ($H_2$), which can cause mechanical damage to tissue, and hydroxide ions ($OH^-$), which cause a local increase in pH (alkalosis), leading to neurotoxicity.\n    *   **Excessive Anodic Charge ($Q_{\\mathrm{net}} > 0$):** The electrode potential becomes highly positive, leading to the oxidation of water or other species like chloride ions ($Cl^-$):\n        $$ 2H_2O \\rightarrow O_2(g) + 4H^+ + 4e^- $$\n        $$ 2Cl^- \\rightarrow Cl_2(g) + 2e^- $$\n        These reactions generate oxygen ($O_2$) or highly toxic chlorine gas ($Cl_2$), and protons ($H^+$), which cause a local decrease in pH (acidosis). Furthermore, these oxidative processes can corrode the noble metal electrode itself, degrading the device and releasing potentially toxic metal ions into the tissue.\n\nIn summary, to ensure the long-term safety and stability of DBS, the net charge injected per cycle must be zero ($Q_{\\mathrm{net}} = 0$). This practice, known as charge-balancing, confines the electrode potential within the safe water window, thereby preventing irreversible and damaging Faradaic reactions, tissue injury, and electrode corrosion.\n\n### Part 2: Derivation of the Net DC Offset\n\nThe net DC offset, $\\overline{I}_{\\mathrm{DC}}$, is defined as the time-averaged current over one full period, $T$. The period is related to the stimulation frequency $f$ by $T = \\frac{1}{f}$.\n$$ \\overline{I}_{\\mathrm{DC}} = \\frac{1}{T} \\int_{0}^{T} i(t) \\, dt $$\nThe current waveform $i(t)$ over one period from $t=0$ to $t=T$ is defined piecewise based on the problem description:\n- A cathodic phase: $i(t) = I_c$ for $0 \\le t < t_c$.\n- An interphase gap: $i(t) = 0$ for $t_c \\le t < t_c + t_g$.\n- An anodic phase: $i(t) = I_a$ for $t_c + t_g \\le t < t_c + t_g + t_a$.\n- The remainder of the period: $i(t) = 0$ for $t_c + t_g + t_a \\le t < T$.\n\nThe integral for $\\overline{I}_{\\mathrm{DC}}$ can be broken down according to these intervals:\n$$ \\overline{I}_{\\mathrm{DC}} = \\frac{1}{T} \\left[ \\int_{0}^{t_c} i(t) \\, dt + \\int_{t_c}^{t_c+t_g} i(t) \\, dt + \\int_{t_c+t_g}^{t_c+t_g+t_a} i(t) \\, dt + \\int_{t_c+t_g+t_a}^{T} i(t) \\, dt \\right] $$\nSubstituting the constant current values for each interval:\n$$ \\overline{I}_{\\mathrm{DC}} = \\frac{1}{T} \\left[ \\int_{0}^{t_c} I_c \\, dt + \\int_{t_c}^{t_c+t_g} 0 \\, dt + \\int_{t_c+t_g}^{t_c+t_g+t_a} I_a \\, dt + \\int_{t_c+t_g+t_a}^{T} 0 \\, dt \\right] $$\nEvaluating the integrals:\nThe first integral is $\\int_{0}^{t_c} I_c \\, dt = I_c [t]_{0}^{t_c} = I_c (t_c - 0) = I_c t_c$.\nThe third integral is $\\int_{t_c+t_g}^{t_c+t_g+t_a} I_a \\, dt = I_a [t]_{t_c+t_g}^{t_c+t_g+t_a} = I_a((t_c+t_g+t_a) - (t_c+t_g)) = I_a t_a$.\nThe other integrals are zero.\n\nSubstituting these results back into the expression for $\\overline{I}_{\\mathrm{DC}}$:\n$$ \\overline{I}_{\\mathrm{DC}} = \\frac{1}{T} \\left[ I_c t_c + 0 + I_a t_a + 0 \\right] = \\frac{I_c t_c + I_a t_a}{T} $$\nThe term $I_c t_c + I_a t_a$ represents the net charge delivered per pulse, $Q_{\\mathrm{pulse}}$.\nSince $T = \\frac{1}{f}$, we can express the DC offset in terms of the frequency $f$:\n$$ \\overline{I}_{\\mathrm{DC}} = (I_c t_c + I_a t_a) f $$\nThis is the required closed-form expression.\n\n### Part 3: Numerical Calculation\n\nWe use the derived expression $\\overline{I}_{\\mathrm{DC}} = (I_c t_c + I_a t_a) f$ and substitute the given numerical values. It is imperative to maintain consistent units (SI units: Amperes, Seconds, Hertz).\n\nGiven values:\n$f = 130 \\ \\mathrm{Hz}$\n$t_{c} = 70 \\ \\mu\\mathrm{s} = 70 \\times 10^{-6} \\ \\mathrm{s}$\n$t_{a} = 90 \\ \\mu\\mathrm{s} = 90 \\times 10^{-6} \\ \\mathrm{s}$\n$I_{c} = -2.5 \\ \\mathrm{mA} = -2.5 \\times 10^{-3} \\ \\mathrm{A}$\n$I_{a} = +2.45 \\ \\mathrm{mA} = 2.45 \\times 10^{-3} \\ \\mathrm{A}$\nThe value for $t_g$ is not required for this calculation.\n\nFirst, calculate the net charge per pulse, $Q_{\\mathrm{pulse}} = I_c t_c + I_a t_a$:\n$$ Q_{\\mathrm{pulse}} = (-2.5 \\times 10^{-3} \\ \\mathrm{A}) \\times (70 \\times 10^{-6} \\ \\mathrm{s}) + (2.45 \\times 10^{-3} \\ \\mathrm{A}) \\times (90 \\times 10^{-6} \\ \\mathrm{s}) $$\n$$ Q_{\\mathrm{pulse}} = -175 \\times 10^{-9} \\ \\mathrm{C} + 220.5 \\times 10^{-9} \\ \\mathrm{C} $$\n$$ Q_{\\mathrm{pulse}} = 45.5 \\times 10^{-9} \\ \\mathrm{C} $$\nNow, calculate the time-averaged DC offset, $\\overline{I}_{\\mathrm{DC}}$:\n$$ \\overline{I}_{\\mathrm{DC}} = Q_{\\mathrm{pulse}} \\times f = (45.5 \\times 10^{-9} \\ \\mathrm{C}) \\times (130 \\ \\mathrm{s}^{-1}) $$\n$$ \\overline{I}_{\\mathrm{DC}} = 5915 \\times 10^{-9} \\ \\mathrm{A} $$\n$$ \\overline{I}_{\\mathrm{DC}} = 5.915 \\times 10^{-6} \\ \\mathrm{A} $$\nThe problem requires the answer in microamperes ($\\mu\\mathrm{A}$). Since $1 \\ \\mu\\mathrm{A} = 10^{-6} \\ \\mathrm{A}$:\n$$ \\overline{I}_{\\mathrm{DC}} = 5.915 \\ \\mu\\mathrm{A} $$\nThis value is already expressed to four significant figures as required.",
            "answer": "$$\\boxed{5.915}$$"
        },
        {
            "introduction": "Establishing the efficacy of DBS requires rigorously designed clinical trials. The crossover trial, where each participant serves as their own control by receiving both active stimulation and a sham condition, is a powerful design, but it must account for \"carryover\" effects. This practice problem  places you in the role of a trial designer, tasking you with calculating the necessary washout period to ensure scientific validity. You will apply a first-order mathematical model of therapeutic effect decay, a practical skill that bridges physiological understanding with the statistical demands of clinical research.",
            "id": "4704939",
            "problem": "A two-period, two-sequence crossover clinical trial is planned to evaluate Deep Brain Stimulation (DBS) for treatment-resistant obsessive-compulsive disorder. In this trial, participants are randomized to sequences where the stimulation is either ON during period $1$ then OFF during period $2$ (sequence $\\mathrm{AB}$), or OFF during period $1$ then ON during period $2$ (sequence $\\mathrm{BA}$). The goal is to isolate the direct effect of stimulation while ensuring that carryover effects from the prior period do not confound outcomes in the subsequent period. To do so, a washout interval of duration $W$ is inserted between periods. To maintain scientific validity, residual carryover from the prior period entering the subsequent period must be controlled to be negligibly small relative to clinically meaningful changes.\n\nAssume the symptomatic effect magnitude $E(t)$ of DBS can be modeled as a first-order linear system with distinct characteristic time constants for turning stimulation ON and OFF, consistent with physiologically plausible temporal dynamics in psychiatric DBS. Specifically, when stimulation is turned ON, $E(t)$ relaxes toward a steady-state improvement $E_{\\infty}$ with a characteristic time constant $\\tau_{\\text{on}}$; when stimulation is turned OFF, $E(t)$ relaxes toward $0$ with a characteristic time constant $\\tau_{\\text{off}}$. Each period under the ON condition is designed to be sufficiently long that the system effectively reaches steady state before switching OFF, i.e., the end-of-period effect $E(0^{+})$ at the instant of turning OFF satisfies $E(0^{+}) \\approx E_{\\infty}$.\n\nUse the following scientifically plausible parameters for psychiatric DBS in obsessive-compulsive disorder: steady-state improvement $E_{\\infty} = 12$ points on the Yale-Brown Obsessive Compulsive Scale (Y-BOCS), minimal clinically important difference (MCID) $\\mathrm{MCID} = 3$ points, ON time constant $\\tau_{\\text{on}} = 24 \\,\\text{h}$, and OFF time constant $\\tau_{\\text{off}} = 48 \\,\\text{h}$. To ensure that carryover is negligible, require that the residual effect at the start of the next period after washout be less than $0.1 \\times \\mathrm{MCID}$.\n\nDerive, from first principles of first-order linear relaxation dynamics and crossover trial design considerations, the minimal washout duration $W$ such that the residual effect entering the next period is strictly below the specified threshold. Round your answer to three significant figures and express the final duration in hours.",
            "solution": "The problem requires determining the minimal washout duration $W$ such that the carryover effect from a preceding treatment period is rendered negligible. The decay of the therapeutic effect during the washout period (when stimulation is OFF) is governed by first-order linear relaxation dynamics.\n\nThe differential equation describing the effect magnitude $E(t)$ during the OFF period is:\n$$\n\\frac{dE(t)}{dt} = -\\frac{E(t)}{\\tau_{\\text{off}}}\n$$\nwhere $\\tau_{\\text{off}}$ is the characteristic time constant for the effect to dissipate. This is a first-order linear ordinary differential equation. We can solve it by separation of variables:\n$$\n\\frac{dE}{E} = -\\frac{1}{\\tau_{\\text{off}}} dt\n$$\nIntegrating both sides yields:\n$$\n\\int \\frac{dE}{E} = \\int -\\frac{1}{\\tau_{\\text{off}}} dt\n$$\n$$\n\\ln(E(t)) = -\\frac{t}{\\tau_{\\text{off}}} + C\n$$\nwhere $C$ is the constant of integration. Exponentiating both sides gives the general solution for $E(t)$:\n$$\nE(t) = \\exp\\left(-\\frac{t}{\\tau_{\\text{off}}} + C\\right) = E_0 \\exp\\left(-\\frac{t}{\\tau_{\\text{off}}}\\right)\n$$\nwhere $E_0 = \\exp(C)$ is the effect magnitude at the beginning of the washout period, i.e., at $t=0$.\n\nThe problem states that the ON period is sufficiently long for the effect to reach its steady-state value $E_{\\infty}$. Therefore, the initial condition for the washout period is $E(0) = E_{\\infty}$. Substituting this into the general solution, we find $E_0 = E_{\\infty}$. The equation describing the residual effect during washout is:\n$$\nE(t) = E_{\\infty} \\exp\\left(-\\frac{t}{\\tau_{\\text{off}}}\\right)\n$$\nThe washout period has a duration of $W$. The residual effect at the end of this period, which becomes the carryover effect at the start of the next period, is $E(W)$.\n$$\nE(W) = E_{\\infty} \\exp\\left(-\\frac{W}{\\tau_{\\text{off}}}\\right)\n$$\nThe problem imposes the constraint that this residual effect must be strictly less than a fraction ($0.1$) of the minimal clinically important difference (MCID):\n$$\nE(W) < 0.1 \\times \\mathrm{MCID}\n$$\nSubstituting the expression for $E(W)$, we obtain the inequality:\n$$\nE_{\\infty} \\exp\\left(-\\frac{W}{\\tau_{\\text{off}}}\\right) < 0.1 \\times \\mathrm{MCID}\n$$\nWe must now solve this inequality for the minimal duration $W$. First, isolate the exponential term:\n$$\n\\exp\\left(-\\frac{W}{\\tau_{\\text{off}}}\\right) < \\frac{0.1 \\times \\mathrm{MCID}}{E_{\\infty}}\n$$\nNext, take the natural logarithm of both sides. Since the natural logarithm is a monotonically increasing function, the direction of the inequality is preserved.\n$$\n\\ln\\left(\\exp\\left(-\\frac{W}{\\tau_{\\text{off}}}\\right)\\right) < \\ln\\left(\\frac{0.1 \\times \\mathrm{MCID}}{E_{\\infty}}\\right)\n$$\n$$\n-\\frac{W}{\\tau_{\\text{off}}} < \\ln\\left(\\frac{0.1 \\times \\mathrm{MCID}}{E_{\\infty}}\\right)\n$$\nTo solve for $W$, we multiply both sides by $-\\tau_{\\text{off}}$. Since $\\tau_{\\text{off}}$ is a positive constant ($48 \\,\\text{h}$), multiplying by a negative value reverses the inequality sign:\n$$\nW > -\\tau_{\\text{off}} \\ln\\left(\\frac{0.1 \\times \\mathrm{MCID}}{E_{\\infty}}\\right)\n$$\nUsing the logarithmic property $-\\ln(x) = \\ln(1/x)$, we can write this as:\n$$\nW > \\tau_{\\text{off}} \\ln\\left(\\left(\\frac{0.1 \\times \\mathrm{MCID}}{E_{\\infty}}\\right)^{-1}\\right)\n$$\n$$\nW > \\tau_{\\text{off}} \\ln\\left(\\frac{E_{\\infty}}{0.1 \\times \\mathrm{MCID}}\\right)\n$$\nThe minimal washout duration, $W_{\\min}$, is the value at the lower bound of this inequality. Now we substitute the given numerical values: $E_{\\infty} = 12$, $\\mathrm{MCID} = 3$, and $\\tau_{\\text{off}} = 48 \\,\\text{h}$.\n$$\nW > (48 \\,\\text{h}) \\times \\ln\\left(\\frac{12}{0.1 \\times 3}\\right)\n$$\n$$\nW > (48 \\,\\text{h}) \\times \\ln\\left(\\frac{12}{0.3}\\right)\n$$\n$$\nW > (48 \\,\\text{h}) \\times \\ln(40)\n$$\nCalculating the numerical value:\n$$\n\\ln(40) \\approx 3.68887945411\n$$\n$$\nW > 48 \\times 3.68887945411 \\,\\text{h}\n$$\n$$\nW > 177.066213797 \\,\\text{h}\n$$\nThe problem asks for the minimal duration, rounded to three significant figures. The calculated value is approximately $177.066\\,\\text{h}$. Rounding to three significant figures gives $177\\,\\text{h}$. Thus, the minimal duration $W$ must be $177$ hours to ensure the carryover effect is below the specified threshold.",
            "answer": "$$\n\\boxed{177}\n$$"
        },
        {
            "introduction": "The next frontier in DBS is personalization: moving beyond one-size-fits-all targets to predict clinical outcomes based on how stimulation interacts with an individual's unique brain wiring. This is the domain of connectome-based predictive modeling. This advanced exercise  provides hands-on experience with the core computational techniques involved, from deriving the statistical estimators for a linear model to validating its predictive power on held-out data. Mastering these methods is essential for anyone aspiring to contribute to the cutting-edge of computational psychiatry and neuromodulation.",
            "id": "4705016",
            "problem": "You are tasked with implementing and validating a connectome-based predictive model of symptom change for Deep Brain Stimulation (DBS) in psychiatry. The model assumes that stimulation engages specific white matter tracts and that the net clinical effect arises from the additive contributions of these tracts. Formulate the problem in terms of linear algebra and statistics, derive the estimators from first principles, and implement a program to evaluate the model on held-out patient outcomes.\n\nAssumptions and base principles:\n- Each patient is represented by a vector of tract engagements obtained from a normative structural connectome. Let $X \\in \\mathbb{R}^{n \\times p}$ denote the design matrix with $n$ patients and $p$ tracts, where entry $x_{ij}$ quantifies the engagement of tract $j$ by the patient-specific stimulation field. Let $y \\in \\mathbb{R}^{n}$ denote observed symptom changes in standardized scale units (unitless).\n- The clinical effect from stimulation is modeled as the linear superposition of small tract-specific effects: $y \\approx X w + \\varepsilon$, where $w \\in \\mathbb{R}^{p}$ are tract weights, and $\\varepsilon \\in \\mathbb{R}^{n}$ is an error term that aggregates unmodeled variation.\n- Training and testing are performed on disjoint sets of patients to simulate held-out validation. The training set specifies $X_{\\mathrm{train}} \\in \\mathbb{R}^{n_{\\mathrm{train}} \\times p}$ and $y_{\\mathrm{train}} \\in \\mathbb{R}^{n_{\\mathrm{train}}}$, and the test set specifies $X_{\\mathrm{test}} \\in \\mathbb{R}^{n_{\\mathrm{test}} \\times p}$ and $y_{\\mathrm{test}} \\in \\mathbb{R}^{n_{\\mathrm{test}}}$.\n\nYour tasks:\n1. Starting from the principle of minimizing mean squared error under the linear superposition assumption, derive the estimator for $w$ for two regimes:\n   - Unregularized estimation when the Gram matrix is invertible.\n   - Tikhonov-regularized estimation with a nonnegative regularization parameter $ \\lambda $, suitable for collinearity or underdetermined systems.\n2. Compute predictions $\\hat{y}_{\\mathrm{test}} = X_{\\mathrm{test}} \\hat{w}$ for the held-out test set.\n3. Quantify validation performance using:\n   - Root Mean Squared Error (RMSE), defined as $$\\mathrm{RMSE} = \\sqrt{\\frac{1}{n_{\\mathrm{test}}} \\sum_{i=1}^{n_{\\mathrm{test}}} \\left(y_{\\mathrm{test}, i} - \\hat{y}_{\\mathrm{test}, i}\\right)^2}.$$\n   - Pearson correlation coefficient $r$ between $y_{\\mathrm{test}}$ and $\\hat{y}_{\\mathrm{test}}$, expressed as a decimal.\n\nTest suite:\nImplement your solution for the following three cases. All symptom changes are in standardized scale units; report $\\mathrm{RMSE}$ in the same unit and $r$ as a decimal. For each case, use the specified $ \\lambda $.\n\n- Case $1$ (full-rank training, unregularized):\n  $$X_{\\mathrm{train}}^{(1)} = \\begin{bmatrix}\n  0.2 & 0.5 & 0.3 \\\\\n  0.1 & 0.4 & 0.2 \\\\\n  0.7 & 0.1 & 0.4 \\\\\n  0.3 & 0.6 & 0.5\n  \\end{bmatrix}, \\quad\n  y_{\\mathrm{train}}^{(1)} = \\begin{bmatrix}\n  -3.0 \\\\ -1.5 \\\\ -4.2 \\\\ -3.3\n  \\end{bmatrix},$$\n  $$X_{\\mathrm{test}}^{(1)} = \\begin{bmatrix}\n  0.25 & 0.45 & 0.35 \\\\\n  0.6 & 0.2 & 0.5 \\\\\n  0.05 & 0.4 & 0.1\n  \\end{bmatrix}, \\quad\n  y_{\\mathrm{test}}^{(1)} = \\begin{bmatrix}\n  -2.8 \\\\ -4.0 \\\\ -1.2\n  \\end{bmatrix}, \\quad \\lambda^{(1)} = 0.0.$$\n\n- Case $2$ (strong collinearity, ridge regularization):\n  $$X_{\\mathrm{train}}^{(2)} = \\begin{bmatrix}\n  0.3 & 0.31 & 0.2 \\\\\n  0.6 & 0.61 & 0.4 \\\\\n  0.1 & 0.11 & 0.05 \\\\\n  0.8 & 0.81 & 0.7\n  \\end{bmatrix}, \\quad\n  y_{\\mathrm{train}}^{(2)} = \\begin{bmatrix}\n  -2.5 \\\\ -5.0 \\\\ -1.0 \\\\ -6.0\n  \\end{bmatrix},$$\n  $$X_{\\mathrm{test}}^{(2)} = \\begin{bmatrix}\n  0.5 & 0.51 & 0.3 \\\\\n  0.2 & 0.21 & 0.1 \\\\\n  0.9 & 0.91 & 0.6\n  \\end{bmatrix}, \\quad\n  y_{\\mathrm{test}}^{(2)} = \\begin{bmatrix}\n  -4.0 \\\\ -1.4 \\\\ -6.5\n  \\end{bmatrix}, \\quad \\lambda^{(2)} = 0.1.$$\n\n- Case $3$ (underdetermined training, ridge regularization, tract with zero engagement):\n  $$X_{\\mathrm{train}}^{(3)} = \\begin{bmatrix}\n  0.2 & 0.0 & 0.5 & 0.1 \\\\\n  0.1 & 0.0 & 0.7 & 0.2 \\\\\n  0.3 & 0.0 & 0.2 & 0.4\n  \\end{bmatrix}, \\quad\n  y_{\\mathrm{train}}^{(3)} = \\begin{bmatrix}\n  -2.0 \\\\ -2.5 \\\\ -3.0\n  \\end{bmatrix},$$\n  $$X_{\\mathrm{test}}^{(3)} = \\begin{bmatrix}\n  0.25 & 0.0 & 0.6 & 0.15 \\\\\n  0.05 & 0.0 & 0.4 & 0.05\n  \\end{bmatrix}, \\quad\n  y_{\\mathrm{test}}^{(3)} = \\begin{bmatrix}\n  -2.3 \\\\ -1.4\n  \\end{bmatrix}, \\quad \\lambda^{(3)} = 1.0.$$\n\nProgram requirements:\n- Implement a program that, for each case, estimates $ \\hat{w} $, computes $\\hat{y}_{\\mathrm{test}}$, and calculates $\\mathrm{RMSE}$ and $r$.\n- Round each metric to $4$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain, in order, for Case $1$, Case $2$, and Case $3$: $[\\mathrm{RMSE}^{(1)}, r^{(1)}, \\mathrm{RMSE}^{(2)}, r^{(2)}, \\mathrm{RMSE}^{(3)}, r^{(3)}]$.",
            "solution": "### Derivation of the Estimator for Tract Weights $w$\n\nThe problem is to find the vector of tract weights $w$ that best explains the observed symptom changes $y$ given the tract engagement matrix $X$. The linear model is $y = Xw + \\varepsilon$. We seek to find an estimate $\\hat{w}$ that minimizes the discrepancy between the observed outcomes $y$ and the model predictions $\\hat{y} = Xw$.\n\nThe guiding principle is the minimization of the sum of squared errors (SSE), which is proportional to the mean squared error. The SSE, denoted as $L(w)$, is the squared Euclidean norm of the residual vector $y - Xw$:\n$$L(w) = \\| y - Xw \\|_2^2 = (y - Xw)^T (y - Xw)$$\nExpanding this expression, we get:\n$$L(w) = y^T y - y^T(Xw) - (Xw)^T y + (Xw)^T(Xw)$$\n$$L(w) = y^T y - w^T X^T y - w^T X^T y + w^T X^T X w$$\n$$L(w) = y^T y - 2w^T X^T y + w^T X^T X w$$\nTo find the minimum, we compute the gradient of $L(w)$ with respect to $w$ and set it to zero.\n$$\\nabla_w L(w) = \\frac{\\partial}{\\partial w} \\left( y^T y - 2w^T X^T y + w^T X^T X w \\right) = -2X^T y + 2X^T X w$$\nSetting the gradient to zero:\n$$-2X^T y + 2X^T X w = 0 \\implies X^T X w = X^T y$$\nThis set of linear equations is known as the normal equations. The derivation of the estimator $\\hat{w}$ depends on the properties of the matrix $X^T X$.\n\n**1. Unregularized Estimation (Ordinary Least Squares)**\nThis regime applies when the Gram matrix $G = X^T X$ is invertible. This is typically true when the number of patients $n$ is greater than or equal to the number of tracts $p$ ($n \\ge p$) and the columns of $X$ are linearly independent. In this case, we can left-multiply by the inverse of $X^T X$ to solve for $w$. The estimator is denoted $\\hat{w}_{\\mathrm{OLS}}$.\n$$\\hat{w}_{\\mathrm{OLS}} = (X^T X)^{-1} X^T y$$\nThis corresponds to the case where the regularization parameter $\\lambda = 0$.\n\n**2. Tikhonov-Regularized Estimation (Ridge Regression)**\nWhen $X^T X$ is singular or ill-conditioned (i.e., when $p > n$ or the columns of $X$ are highly correlated), the unregularized solution is not unique or is numerically unstable. Tikhonov regularization addresses this by adding a penalty term to the loss function. The new objective is to minimize:\n$$L(w, \\lambda) = \\| y - Xw \\|_2^2 + \\lambda \\| w \\|_2^2$$\nwhere $\\lambda \\ge 0$ is the regularization parameter. The penalty term $\\lambda \\| w \\|_2^2 = \\lambda w^T w$ penalizes large weights, which helps to prevent overfitting and stabilize the solution.\n\nThe loss function is:\n$$L(w, \\lambda) = y^T y - 2w^T X^T y + w^T X^T X w + \\lambda w^T w$$\n$$L(w, \\lambda) = y^T y - 2w^T X^T y + w^T (X^T X + \\lambda I) w$$\nwhere $I$ is the $p \\times p$ identity matrix. Taking the gradient with respect to $w$:\n$$\\nabla_w L(w, \\lambda) = -2X^T y + 2(X^T X + \\lambda I) w$$\nSetting the gradient to zero gives the regularized normal equations:\n$$(X^T X + \\lambda I) w = X^T y$$\nFor any $\\lambda > 0$, the matrix $(X^T X + \\lambda I)$ is invertible. This is because $X^T X$ is positive semi-definite, and adding a positive multiple of the identity matrix makes it positive definite, hence invertible. The resulting estimator is the Ridge Regression estimator, $\\hat{w}_{\\mathrm{Ridge}}$:\n$$\\hat{w}_{\\mathrm{Ridge}} = (X^T X + \\lambda I)^{-1} X^T y$$\nNote that this formula generalizes the OLS estimator, as setting $\\lambda = 0$ recovers the unregularized solution. In practice, solving the linear system $(X^T X + \\lambda I) \\hat{w} = X^T y$ is numerically more stable than computing the matrix inverse directly.\n\n### Prediction and Performance Evaluation\n\nOnce the estimator $\\hat{w}$ is computed from the training data ($X_{\\mathrm{train}}$, $y_{\\mathrm{train}}$), it is used to predict outcomes for the held-out test data, $X_{\\mathrm{test}}$.\n$$\\hat{y}_{\\mathrm{test}} = X_{\\mathrm{test}} \\hat{w}$$\nThe performance of the model is then quantified by comparing the predictions $\\hat{y}_{\\mathrm{test}}$ with the actual observed outcomes $y_{\\mathrm{test}}$.\n\n**1. Root Mean Squared Error (RMSE)**\nThe RMSE measures the average magnitude of the prediction errors.\n$$\\mathrm{RMSE} = \\sqrt{\\frac{1}{n_{\\mathrm{test}}} \\sum_{i=1}^{n_{\\mathrm{test}}} \\left(y_{\\mathrm{test}, i} - \\hat{y}_{\\mathrm{test}, i}\\right)^2} = \\sqrt{\\frac{1}{n_{\\mathrm{test}}} \\| y_{\\mathrm{test}} - \\hat{y}_{\\mathrm{test}} \\|_2^2}$$\n\n**2. Pearson Correlation Coefficient ($r$)**\nThe Pearson correlation coefficient measures the linear correlation between the predicted and observed outcomes. It is defined as the covariance of the two variables divided by the product of their standard deviations.\n$$r = \\frac{\\sum_{i=1}^{n_{\\mathrm{test}}} (y_{\\mathrm{test}, i} - \\bar{y}_{\\mathrm{test}})(\\hat{y}_{\\mathrm{test}, i} - \\bar{\\hat{y}}_{\\mathrm{test}})}{\\sqrt{\\sum_{i=1}^{n_{\\mathrm{test}}} (y_{\\mathrm{test}, i} - \\bar{y}_{\\mathrm{test}})^2} \\sqrt{\\sum_{i=1}^{n_{\\mathrm{test}}} (\\hat{y}_{\\mathrm{test}, i} - \\bar{\\hat{y}}_{\\mathrm{test}})^2}}$$\nwhere $\\bar{y}_{\\mathrm{test}}$ and $\\bar{\\hat{y}}_{\\mathrm{test}}$ are the mean values of the observed and predicted outcomes, respectively.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# No other libraries are permitted.\n\ndef solve():\n    \"\"\"\n    Main function to solve the connectome-based predictive modeling problem.\n    It processes three test cases, computes model performance, and prints the results.\n    \"\"\"\n\n    def evaluate_model(X_train, y_train, X_test, y_test, lambda_val):\n        \"\"\"\n        Estimates tract weights, predicts on test data, and computes performance.\n\n        Args:\n            X_train (np.ndarray): Training data design matrix (n_train x p).\n            y_train (np.ndarray): Training data outcome vector (n_train,).\n            X_test (np.ndarray): Test data design matrix (n_test x p).\n            y_test (np.ndarray): Test data outcome vector (n_test,).\n            lambda_val (float): Tikhonov regularization parameter.\n\n        Returns:\n            tuple: A tuple containing the calculated RMSE and Pearson correlation (r).\n        \"\"\"\n        # Ensure y vectors are 1D arrays\n        y_train = y_train.flatten()\n        y_test = y_test.flatten()\n        \n        n_train, p = X_train.shape\n        n_test = X_test.shape[0]\n\n        # 1. Estimate weights w_hat using Tikhonov-regularized least squares.\n        # This is robust and covers both OLS (lambda=0) and Ridge cases.\n        # The equation is (X_train.T @ X_train + lambda * I) @ w = X_train.T @ y_train\n        A = X_train.T @ X_train + lambda_val * np.identity(p)\n        b = X_train.T @ y_train\n        \n        # We solve the linear system A @ w = b, which is more stable than\n        # computing the inverse of A.\n        w_hat = np.linalg.solve(A, b)\n\n        # 2. Compute predictions on the test set.\n        y_hat_test = X_test @ w_hat\n\n        # 3. Quantify validation performance.\n        # Root Mean Squared Error (RMSE)\n        rmse = np.sqrt(np.mean((y_test - y_hat_test)**2))\n\n        # Pearson correlation coefficient (r)\n        # Handle cases where correlation is undefined (zero variance in either vector).\n        if np.std(y_test) == 0 or np.std(y_hat_test) == 0:\n            r = 0.0\n        else:\n            r = np.corrcoef(y_test, y_hat_test)[0, 1]\n        \n        return rmse, r\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (full-rank training, unregularized)\n        {\n            \"X_train\": np.array([\n                [0.2, 0.5, 0.3], [0.1, 0.4, 0.2], [0.7, 0.1, 0.4], [0.3, 0.6, 0.5]\n            ]),\n            \"y_train\": np.array([-3.0, -1.5, -4.2, -3.3]),\n            \"X_test\": np.array([\n                [0.25, 0.45, 0.35], [0.6, 0.2, 0.5], [0.05, 0.4, 0.1]\n            ]),\n            \"y_test\": np.array([-2.8, -4.0, -1.2]),\n            \"lambda\": 0.0\n        },\n        # Case 2 (strong collinearity, ridge regularization)\n        {\n            \"X_train\": np.array([\n                [0.3, 0.31, 0.2], [0.6, 0.61, 0.4], [0.1, 0.11, 0.05], [0.8, 0.81, 0.7]\n            ]),\n            \"y_train\": np.array([-2.5, -5.0, -1.0, -6.0]),\n            \"X_test\": np.array([\n                [0.5, 0.51, 0.3], [0.2, 0.21, 0.1], [0.9, 0.91, 0.6]\n            ]),\n            \"y_test\": np.array([-4.0, -1.4, -6.5]),\n            \"lambda\": 0.1\n        },\n        # Case 3 (underdetermined training, ridge regularization)\n        {\n            \"X_train\": np.array([\n                [0.2, 0.0, 0.5, 0.1], [0.1, 0.0, 0.7, 0.2], [0.3, 0.0, 0.2, 0.4]\n            ]),\n            \"y_train\": np.array([-2.0, -2.5, -3.0]),\n            \"X_test\": np.array([\n                [0.25, 0.0, 0.6, 0.15], [0.05, 0.0, 0.4, 0.05]\n            ]),\n            \"y_test\": np.array([-2.3, -1.4]),\n            \"lambda\": 1.0\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        rmse, r = evaluate_model(\n            case[\"X_train\"],\n            case[\"y_train\"],\n            case[\"X_test\"],\n            case[\"y_test\"],\n            case[\"lambda\"]\n        )\n        results.extend([rmse, r])\n\n    # Final print statement in the exact required format.\n    # Round each metric to 4 decimal places.\n    print(f\"[{','.join(f'{x:.4f}' for x in results)}]\")\n\nsolve()\n```"
        }
    ]
}