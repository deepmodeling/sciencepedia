{
    "hands_on_practices": [
        {
            "introduction": "The performance of any supervised AI model is fundamentally limited by the quality of its training data. This exercise explores how to quantify the consistency of expert annotations, a critical step in creating a reliable \"gold standard\" for training and evaluation. By calculating Cohen’s Kappa, you will gain a practical understanding of interrater reliability and its implications for developing diagnostic AI in dentistry .",
            "id": "4694101",
            "problem": "A teledentistry program is building an Artificial Intelligence (AI) system to triage dental radiographs for remote care in stomatology. Three board-certified experts independently annotate a shared set of $N=120$ tooth sites on intraoral images into three nominal categories: sound enamel ($S$), cavitated caries ($C$), and periapical pathology ($P$). To assess the quality of the gold-standard annotations, interrater reliability is quantified using the Cohen’s $\\kappa$ definition derived from observed agreement and chance agreement. For a given rater pair $(X,Y)$ over $K$ categories with total $N$ items, the observed agreement is the empirical proportion of identical labels across items, and the expected chance agreement is the probability of identical labels if $X$ and $Y$ label items independently following their respective marginal category frequencies.\n\nYou are given the pairwise $3 \\times 3$ contingency matrices of counts (rows are the categories chosen by the first rater in the pair, columns are the categories chosen by the second rater in the pair):\n\n$$M_{AB}=\\begin{pmatrix}\n50 & 6 & 4\\\\\n3 & 30 & 7\\\\\n2 & 9 & 9\n\\end{pmatrix},\\quad\nM_{AC}=\\begin{pmatrix}\n48 & 8 & 4\\\\\n6 & 28 & 6\\\\\n4 & 6 & 10\n\\end{pmatrix},\\quad\nM_{BC}=\\begin{pmatrix}\n47 & 6 & 2\\\\\n7 & 26 & 12\\\\\n4 & 10 & 6\n\\end{pmatrix}.$$\n\nAll matrices are computed over the same $N=120$ items and share marginals consistent with each rater’s category distribution across the three pairings. Using only the core definition of Cohen’s $\\kappa$, compute the aggregated interrater reliability by macro-averaging the pairwise $\\kappa$ values across the three rater pairs $(A,B)$, $(A,C)$, and $(B,C)$. Express your final answer as a decimal number, and round to four significant figures. Do not append a percentage sign.",
            "solution": "The problem requires the calculation of the aggregated interrater reliability by macro-averaging the pairwise Cohen's $\\kappa$ values for three rater pairs $(A,B)$, $(A,C)$, and $(B,C)$. The analysis is based on three given $3 \\times 3$ contingency matrices over a total of $N=120$ items and $K=3$ categories.\n\nThe core definition of Cohen's $\\kappa$ is given by the formula:\n$$ \\kappa = \\frac{p_o - p_e}{1 - p_e} $$\nwhere $p_o$ is the observed proportional agreement and $p_e$ is the expected proportional agreement by chance.\n\nFor a contingency matrix $M = (m_{ij})$ of size $K \\times K$, where $m_{ij}$ is the number of items assigned to category $i$ by the first rater and category $j$ by the second rater, the total number of items is $N = \\sum_{i=1}^K \\sum_{j=1}^K m_{ij}$.\n\nThe observed agreement, $p_o$, is the proportion of items on which the raters agree. It is calculated as the sum of the diagonal elements of the contingency matrix divided by the total number of items:\n$$ p_o = \\frac{\\sum_{i=1}^K m_{ii}}{N} = \\frac{\\text{Tr}(M)}{N} $$\n\nThe expected chance agreement, $p_e$, is the probability of agreement if the raters made their judgments independently. Let $r_i = \\sum_{j=1}^K m_{ij}$ be the total number of items the first rater assigned to category $i$ (row $i$ sum), and $c_j = \\sum_{i=1}^K m_{ij}$ be the total number of items the second rater assigned to category $j$ (column $j$ sum). The probability of both raters independently choosing category $k$ is the product of their marginal probabilities for that category. Summing over all categories:\n$$ p_e = \\sum_{k=1}^K \\frac{r_k}{N} \\cdot \\frac{c_k}{N} = \\frac{1}{N^2} \\sum_{k=1}^K r_k c_k $$\n\nWe will compute $\\kappa$ for each of the three rater pairs and then find their arithmetic mean. The total number of items is $N=120$.\n\n**1. Calculation for Rater Pair (A, B)**\nThe contingency matrix is $M_{AB} = \\begin{pmatrix} 50 & 6 & 4\\\\ 3 & 30 & 7\\\\ 2 & 9 & 9 \\end{pmatrix}$.\nThe observed agreement is:\n$$ p_{o,AB} = \\frac{50 + 30 + 9}{120} = \\frac{89}{120} $$\nThe marginal totals for rater A (rows) are $r_1 = 60$, $r_2 = 40$, $r_3 = 20$.\nThe marginal totals for rater B (columns) are $c_1 = 55$, $c_2 = 45$, $c_3 = 20$.\nThe expected chance agreement is:\n$$ p_{e,AB} = \\frac{(60 \\times 55) + (40 \\times 45) + (20 \\times 20)}{120^2} = \\frac{3300 + 1800 + 400}{14400} = \\frac{5500}{14400} = \\frac{55}{144} $$\nNow, we compute $\\kappa_{AB}$:\n$$ \\kappa_{AB} = \\frac{\\frac{89}{120} - \\frac{55}{144}}{1 - \\frac{55}{144}} = \\frac{\\frac{89 \\times 6 - 55 \\times 5}{720}}{\\frac{144 - 55}{144}} = \\frac{\\frac{534-275}{720}}{\\frac{89}{144}} = \\frac{\\frac{259}{720}}{\\frac{89}{144}} = \\frac{259}{720} \\times \\frac{144}{89} = \\frac{259}{5 \\times 89} = \\frac{259}{445} $$\n\n**2. Calculation for Rater Pair (A, C)**\nThe contingency matrix is $M_{AC} = \\begin{pmatrix} 48 & 8 & 4\\\\ 6 & 28 & 6\\\\ 4 & 6 & 10 \\end{pmatrix}$.\nThe observed agreement is:\n$$ p_{o,AC} = \\frac{48 + 28 + 10}{120} = \\frac{86}{120} = \\frac{43}{60} $$\nThe marginal totals for rater A (rows) are $r_1=60$, $r_2=40$, $r_3=20$.\nThe marginal totals for rater C (columns) are $c_1 = 58$, $c_2 = 42$, $c_3 = 20$.\nThe expected chance agreement is:\n$$ p_{e,AC} = \\frac{(60 \\times 58) + (40 \\times 42) + (20 \\times 20)}{120^2} = \\frac{3480 + 1680 + 400}{14400} = \\frac{5560}{14400} = \\frac{139}{360} $$\nNow, we compute $\\kappa_{AC}$:\n$$ \\kappa_{AC} = \\frac{\\frac{43}{60} - \\frac{139}{360}}{1 - \\frac{139}{360}} = \\frac{\\frac{43 \\times 6 - 139}{360}}{\\frac{360 - 139}{360}} = \\frac{\\frac{258 - 139}{360}}{\\frac{221}{360}} = \\frac{119}{221} = \\frac{7 \\times 17}{13 \\times 17} = \\frac{7}{13} $$\n\n**3. Calculation for Rater Pair (B, C)**\nThe contingency matrix is $M_{BC} = \\begin{pmatrix} 47 & 6 & 2\\\\ 7 & 26 & 12\\\\ 4 & 10 & 6 \\end{pmatrix}$.\nThe observed agreement is:\n$$ p_{o,BC} = \\frac{47 + 26 + 6}{120} = \\frac{79}{120} $$\nThe marginal totals for rater B (rows) are $r_1=55$, $r_2=45$, $r_3=20$.\nThe marginal totals for rater C (columns) are $c_1=58$, $c_2=42$, $c_3=20$.\nThe expected chance agreement is:\n$$ p_{e,BC} = \\frac{(55 \\times 58) + (45 \\times 42) + (20 \\times 20)}{120^2} = \\frac{3190 + 1890 + 400}{14400} = \\frac{5480}{14400} = \\frac{137}{360} $$\nNow, we compute $\\kappa_{BC}$:\n$$ \\kappa_{BC} = \\frac{\\frac{79}{120} - \\frac{137}{360}}{1 - \\frac{137}{360}} = \\frac{\\frac{79 \\times 3 - 137}{360}}{\\frac{360 - 137}{360}} = \\frac{\\frac{237 - 137}{360}}{\\frac{223}{360}} = \\frac{100}{223} $$\n\n**4. Macro-Averaging the Pairwise $\\kappa$ Values**\nThe final step is to compute the arithmetic mean (macro-average) of the three pairwise $\\kappa$ values:\n$$ \\bar{\\kappa} = \\frac{\\kappa_{AB} + \\kappa_{AC} + \\kappa_{BC}}{3} = \\frac{1}{3} \\left( \\frac{259}{445} + \\frac{7}{13} + \\frac{100}{223} \\right) $$\nTo obtain the final numerical answer, we convert the fractions to decimals:\n$$ \\kappa_{AB} = \\frac{259}{445} \\approx 0.58202247 $$\n$$ \\kappa_{AC} = \\frac{7}{13} \\approx 0.53846154 $$\n$$ \\kappa_{BC} = \\frac{100}{223} \\approx 0.44843049 $$\n$$ \\bar{\\kappa} \\approx \\frac{0.58202247 + 0.53846154 + 0.44843049}{3} = \\frac{1.5689145}{3} \\approx 0.5229715 $$\nRounding the result to four significant figures gives $0.5230$.",
            "answer": "$$\\boxed{0.5230}$$"
        },
        {
            "introduction": "To train a neural network for tasks like segmenting dental lesions, we must define a loss function that guides the learning process. This practice delves into the calculus behind the Sørensen–Dice loss, a standard choice for medical image segmentation due to its effectiveness with imbalanced data. Deriving its gradient will reveal how the model's updates are influenced by the global overlap between prediction and ground truth, a key concept for building robust segmentation models .",
            "id": "4694116",
            "problem": "A teledentistry platform integrates Convolutional Neural Networks (CNN) for automated segmentation of periapical lesions in cone-beam computed tomography (CBCT) images to guide a robotic endodontic access system. For a single two-dimensional slice, the segmentation network outputs per-pixel predictions $\\{p_{i}\\}_{i=1}^{N}$ with $p_{i} \\in [0,1]$, and the ground-truth labels are $\\{g_{i}\\}_{i=1}^{N}$ with $g_{i} \\in \\{0,1\\}$, where $N$ is the total number of pixels in the slice. The training objective uses the Sørensen–Dice loss\n$$\nL \\;=\\; 1 \\;-\\; \\frac{2 \\sum_{j=1}^{N} p_{j} g_{j}}{\\sum_{j=1}^{N} p_{j} \\;+\\; \\sum_{j=1}^{N} g_{j}}.\n$$\nStarting from the fundamental definitions of partial differentiation and the quotient rule from elementary calculus, derive the analytic expression for the partial derivative $\\frac{\\partial L}{\\partial p_{i}}$ with respect to the prediction $p_{i}$ for a single pixel index $i$. Then, using the derived expression, explain in scientific terms how the Dice loss inherently addresses class imbalance between lesion and background pixels in dental images (for example, when the number of lesion pixels is small relative to background). Your final answer must be the closed-form analytic expression for $\\frac{\\partial L}{\\partial p_{i}}$; no numerical approximation is required and no units apply.",
            "solution": "The Sørensen–Dice loss function, $L$, is given by:\n$$\nL = 1 - \\frac{2 \\sum_{j=1}^{N} p_{j} g_{j}}{\\sum_{j=1}^{N} p_{j} + \\sum_{j=1}^{N} g_{j}}\n$$\nwhere $\\{p_j\\}_{j=1}^N$ are the predicted probabilities, $\\{g_j\\}_{j=1}^N$ are the ground-truth binary labels, and $N$ is the total number of pixels. We are tasked with finding the partial derivative of $L$ with respect to the prediction $p_{i}$ for a single arbitrary pixel $i$.\n\nLet us define the Dice coefficient, $D$, as the fraction term:\n$$\nD = \\frac{2 \\sum_{j=1}^{N} p_{j} g_{j}}{\\sum_{j=1}^{N} p_{j} + \\sum_{j=1}^{N} g_{j}}\n$$\nThus, the loss is $L = 1 - D$. The partial derivative of $L$ with respect to $p_i$ is:\n$$\n\\frac{\\partial L}{\\partial p_{i}} = \\frac{\\partial}{\\partial p_{i}}(1 - D) = - \\frac{\\partial D}{\\partial p_{i}}\n$$\nTo calculate $\\frac{\\partial D}{\\partial p_{i}}$, we apply the quotient rule from elementary calculus. Let $f(x) = \\frac{u(x)}{v(x)}$, then $f'(x) = \\frac{v(x)u'(x) - u(x)v'(x)}{[v(x)]^2}$.\nFor our function $D$, the variable is $p_i$. Let us define the numerator and denominator functions:\n$$\nU(p_1, \\dots, p_N) = 2 \\sum_{j=1}^{N} p_{j} g_{j}\n$$\n$$\nV(p_1, \\dots, p_N) = \\sum_{j=1}^{N} p_{j} + \\sum_{j=1}^{N} g_{j}\n$$\nNext, we find the partial derivatives of $U$ and $V$ with respect to $p_i$.\n\nFor the numerator $U$, the summation $\\sum_{j=1}^{N} p_{j} g_{j}$ is a sum of terms $p_1 g_1 + p_2 g_2 + \\dots + p_i g_i + \\dots + p_N g_N$. When we differentiate this sum with respect to $p_i$, only the term containing $p_i$ will yield a non-zero result. The derivative of $p_i g_i$ with respect to $p_i$ is $g_i$, as $g_i$ is a constant. Therefore:\n$$\n\\frac{\\partial U}{\\partial p_{i}} = \\frac{\\partial}{\\partial p_{i}} \\left( 2 \\sum_{j=1}^{N} p_{j} g_{j} \\right) = 2 g_{i}\n$$\nFor the denominator $V$, the term $\\sum_{j=1}^{N} g_{j}$ is a constant with respect to any $p_k$. The term $\\sum_{j=1}^{N} p_{j}$ is the sum $p_1 + p_2 + \\dots + p_i + \\dots + p_N$. Its partial derivative with respect to $p_i$ is $1$. Thus:\n$$\n\\frac{\\partial V}{\\partial p_{i}} = \\frac{\\partial}{\\partial p_{i}} \\left( \\sum_{j=1}^{N} p_{j} + \\sum_{j=1}^{N} g_{j} \\right) = 1 + 0 = 1\n$$\nNow we can substitute these results into the quotient rule to find $\\frac{\\partial D}{\\partial p_{i}}$:\n$$\n\\frac{\\partial D}{\\partial p_{i}} = \\frac{V \\frac{\\partial U}{\\partial p_{i}} - U \\frac{\\partial V}{\\partial p_{i}}}{V^2} = \\frac{\\left( \\sum_{j=1}^{N} p_{j} + \\sum_{j=1}^{N} g_{j} \\right)(2 g_{i}) - \\left( 2 \\sum_{j=1}^{N} p_{j} g_{j} \\right)(1)}{\\left( \\sum_{j=1}^{N} p_{j} + \\sum_{j=1}^{N} g_{j} \\right)^2}\n$$\nFinally, we compute $\\frac{\\partial L}{\\partial p_{i}} = - \\frac{\\partial D}{\\partial p_{i}}$:\n$$\n\\frac{\\partial L}{\\partial p_{i}} = - \\frac{2 g_{i} \\left( \\sum_{j=1}^{N} p_{j} + \\sum_{j=1}^{N} g_{j} \\right) - 2 \\sum_{j=1}^{N} p_{j} g_{j}}{\\left( \\sum_{j=1}^{N} p_{j} + \\sum_{j=1}^{N} g_{j} \\right)^2}\n$$\nRearranging the terms in the numerator gives the final analytic expression:\n$$\n\\frac{\\partial L}{\\partial p_{i}} = \\frac{2 \\sum_{j=1}^{N} p_{j} g_{j} - 2 g_{i} \\left( \\sum_{j=1}^{N} p_{j} + \\sum_{j=1}^{N} g_{j} \\right)}{\\left( \\sum_{j=1}^{N} p_{j} + \\sum_{j=1}^{N} g_{j} \\right)^2}\n$$\nThis expression is the gradient component used in backpropagation to update the network weights based on the prediction for pixel $i$.\n\nThe scientific explanation for how the Dice loss addresses class imbalance lies in the structure of this gradient. In typical dental imaging, the number of lesion pixels (foreground, $g_j=1$) is far smaller than the number of healthy tissue pixels (background, $g_j=0$).\n\nLet's compare this to a standard per-pixel loss like binary cross-entropy (BCE), where the gradient for pixel $i$ depends only on the local prediction $p_i$ and label $g_i$. In a highly imbalanced dataset, the total loss and the overall gradient are dominated by the vast number of background pixels. Even if these pixels are classified correctly with high confidence (e.g., $p_i \\approx 0$ for $g_i=0$), their sheer quantity can overwhelm the learning signal from the few foreground pixels.\n\nThe Dice loss gradient, however, is fundamentally different. The numerator and denominator contain sums over *all* pixels in the image. This means the gradient for any single pixel $p_i$ is a function of the global state of the segmentation. It is not an independent, local quantity.\n\nLet's analyze the gradient for a background pixel ($g_i=0$):\n$$\n\\frac{\\partial L}{\\partial p_{i}} \\bigg|_{g_i=0} = \\frac{2 \\sum_{j=1}^{N} p_{j} g_{j}}{\\left( \\sum_{j=1}^{N} p_{j} + \\sum_{j=1}^{N} g_{j} \\right)^2}\n$$\nThis gradient is a function of the \"soft\" true positive count in the numerator, normalized by the squared sum of the \"soft\" predicted positive area and the true positive area. The contribution of each background pixel to the total gradient is modulated by the model's overall performance on the foreground class. If the foreground is small and poorly detected ($\\sum p_j g_j$ is small), the gradient updates for background pixels are also small. This prevents the easy-to-classify background pixels from generating large, distracting gradients.\n\nConversely, for a foreground pixel ($g_i=1$), the gradient has a more complex numerator: $2(\\sum p_j g_j - \\sum p_j - \\sum g_j)$. This term will be strongly negative (pushing $p_i$ towards $1$) if the overall lesion is being missed.\n\nIn essence, the Dice loss frames the optimization problem not as an aggregation of independent pixel-wise classification tasks, but as a holistic task of maximizing the overlap between the predicted and ground-truth foreground regions. The gradient for each pixel reflects how a change in its prediction would affect this global overlap score. By normalizing with respect to the sizes of the predicted and true positive sets, the loss function inherently balances the learning process, ensuring that the rare foreground class contributes a substantial training signal, thereby mitigating the class imbalance problem.",
            "answer": "$$\n\\boxed{\\frac{2 \\sum_{j=1}^{N} p_{j} g_{j} - 2 g_{i} \\left( \\sum_{j=1}^{N} p_{j} + \\sum_{j=1}^{N} g_{j} \\right)}{\\left( \\sum_{j=1}^{N} p_{j} + \\sum_{j=1}^{N} g_{j} \\right)^{2}}}\n$$"
        },
        {
            "introduction": "An AI model's output, such as a probability of disease, is not a decision in itself. This final practice bridges the gap between probabilistic prediction and clinical action by incorporating the real-world costs of diagnostic errors. By deriving the optimal decision boundary for a teledentistry screening system, you will learn how to tailor an AI's behavior to minimize expected costs, a crucial aspect of translating AI research into responsible clinical deployment .",
            "id": "4694110",
            "problem": "Consider a teledentistry screening system that evaluates occlusal surfaces using smartphone intraoral images streamed to a clinician-assisted Artificial Intelligence (AI) model. The AI uses a generalized linear model with a binary outcome (caries present: $y=1$, caries absent: $y=0$) and a canonical link to map a linear predictor into the unit interval, consistent with the Bernoulli model for $y$. Features $x \\in \\mathbb{R}^{d}$ summarize image-derived reflectance and texture metrics, patient-reported dietary frequency, and plaque indices. Let the linear predictor be $z=\\beta_{0}+\\beta^{\\top}x$, where $\\beta \\in \\mathbb{R}^{d}$ are model parameters. The remote decision to flag a lesion ($\\hat{y}=1$) triggers a Remote Operation Dental Assistance Robot (RODAR) visit for confirmatory examination; a false positive induces unnecessary mobilization and patient inconvenience with cost $C_{\\mathrm{FP}}>0$, while a false negative induces delayed treatment and potential lesion progression with cost $C_{\\mathrm{FN}}>0$. Assume correct decisions have zero cost.\n\nFrom first principles, starting with the Bernoulli likelihood for $y$ and the definition of odds and log-odds, derive the optimal decision boundary in the linear predictor space that minimizes expected cost under the above $0$–$1$ loss asymmetry. Express this boundary as $z=\\gamma$, where $\\gamma$ is a constant depending only on $C_{\\mathrm{FP}}$ and $C_{\\mathrm{FN}}$, and provide the closed-form analytic expression for $\\gamma$.\n\nSeparately, interpret the odds ratio $\\exp(\\beta_{j})$ in clinical terms for a single feature $x_{j}$, holding all other covariates fixed, in the context of caries detection via teledentistry.\n\nReport the final answer as the expression for $\\gamma$. No rounding is required, and no physical units are involved.",
            "solution": "The problem asks for two items: first, the derivation of the optimal decision boundary for a teledentistry classification system that minimizes expected cost, and second, the clinical interpretation of an odds ratio from the model.\n\nThe model is specified as a generalized linear model (GLM) for a binary outcome $y \\in \\{0, 1\\}$ (caries absent/present) with a canonical link function for the underlying Bernoulli distribution. This uniquely identifies the model as a logistic regression model.\n\nLet $p(x) = P(y=1|x)$ be the probability of caries given the feature vector $x \\in \\mathbb{R}^{d}$. In a logistic regression model, the log-odds (the canonical link function) of the outcome is a linear function of the features:\n$$ \\ln\\left(\\frac{p(x)}{1-p(x)}\\right) = z $$\nwhere $z = \\beta_{0} + \\beta^{\\top}x$ is the linear predictor. From this, the odds of caries are given by:\n$$ \\text{Odds}(x) = \\frac{p(x)}{1-p(x)} = \\exp(z) $$\n\nThe problem specifies an asymmetric cost structure. The cost of a decision $\\hat{y}$ given a true outcome $y$ is denoted $C(\\hat{y}, y)$. The provided costs are:\n-   $C(1, 0) = C_{\\mathrm{FP}}$ (False Positive)\n-   $C(0, 1) = C_{\\mathrm{FN}}$ (False Negative)\n-   $C(1, 1) = C(0, 0) = 0$ (True Positive and True Negative)\n\nThe optimal decision rule minimizes the expected cost for a given observation $x$. We must compare the expected cost of deciding $\\hat{y}=1$ versus $\\hat{y}=0$.\n\nThe expected cost of deciding $\\hat{y}=1$ (to flag a lesion) is:\n$$ E[\\text{Cost}|\\hat{y}=1, x] = C(1, 1)P(y=1|x) + C(1, 0)P(y=0|x) $$\n$$ E[\\text{Cost}|\\hat{y}=1, x] = 0 \\cdot p(x) + C_{\\mathrm{FP}} \\cdot (1-p(x)) = C_{\\mathrm{FP}}(1-p(x)) $$\n\nThe expected cost of deciding $\\hat{y}=0$ (not to flag) is:\n$$ E[\\text{Cost}|\\hat{y}=0, x] = C(0, 1)P(y=1|x) + C(0, 0)P(y=0|x) $$\n$$ E[\\text{Cost}|\\hat{y}=0, x] = C_{\\mathrm{FN}} \\cdot p(x) + 0 \\cdot (1-p(x)) = C_{\\mathrm{FN}}p(x) $$\n\nThe optimal decision is to flag the lesion ($\\hat{y}=1$) if the expected cost of doing so is less than the expected cost of not flagging. That is, we choose $\\hat{y}=1$ if:\n$$ E[\\text{Cost}|\\hat{y}=1, x] < E[\\text{Cost}|\\hat{y}=0, x] $$\n$$ C_{\\mathrm{FP}}(1-p(x)) < C_{\\mathrm{FN}}p(x) $$\n\nThe decision boundary is the threshold where the expected costs are equal. Let $p_{\\text{thresh}}$ be the probability at this boundary.\n$$ C_{\\mathrm{FP}}(1-p_{\\text{thresh}}) = C_{\\mathrm{FN}}p_{\\text{thresh}} $$\nSolving for $p_{\\text{thresh}}$:\n$$ C_{\\mathrm{FP}} - C_{\\mathrm{FP}}p_{\\text{thresh}} = C_{\\mathrm{FN}}p_{\\text{thresh}} $$\n$$ C_{\\mathrm{FP}} = (C_{\\mathrm{FN}} + C_{\\mathrm{FP}})p_{\\text{thresh}} $$\n$$ p_{\\text{thresh}} = \\frac{C_{\\mathrm{FP}}}{C_{\\mathrm{FN}} + C_{\\mathrm{FP}}} $$\n\nThe problem asks for the decision boundary in the linear predictor space, $z = \\gamma$. The linear predictor $z$ is the log-odds. We must find the log-odds corresponding to the probability threshold $p_{\\text{thresh}}$.\n\nFirst, we calculate the odds at the boundary:\n$$ \\text{Odds}_{\\text{thresh}} = \\frac{p_{\\text{thresh}}}{1 - p_{\\text{thresh}}} $$\nSubstituting the expression for $p_{\\text{thresh}}$:\n$$ \\text{Odds}_{\\text{thresh}} = \\frac{\\frac{C_{\\mathrm{FP}}}{C_{\\mathrm{FN}} + C_{\\mathrm{FP}}}}{1 - \\frac{C_{\\mathrm{FP}}}{C_{\\mathrm{FN}} + C_{\\mathrm{FP}}}} = \\frac{\\frac{C_{\\mathrm{FP}}}{C_{\\mathrm{FN}} + C_{\\mathrm{FP}}}}{\\frac{(C_{\\mathrm{FN}} + C_{\\mathrm{FP}}) - C_{\\mathrm{FP}}}{C_{\\mathrm{FN}} + C_{\\mathrm{FP}}}} = \\frac{\\frac{C_{\\mathrm{FP}}}{C_{\\mathrm{FN}} + C_{\\mathrm{FP}}}}{\\frac{C_{\\mathrm{FN}}}{C_{\\mathrm{FN}} + C_{\\mathrm{FP}}}} = \\frac{C_{\\mathrm{FP}}}{C_{\\mathrm{FN}}} $$\n\nThe decision boundary $\\gamma$ is the value of the linear predictor $z$ that corresponds to these odds. Since $z = \\ln(\\text{Odds})$, we have:\n$$ \\gamma = \\ln\\left(\\frac{C_{\\mathrm{FP}}}{C_{\\mathrm{FN}}}\\right) $$\nThe decision rule is to predict $\\hat{y}=1$ if $z > \\gamma$.\n\nThe second part of the problem asks for the interpretation of the odds ratio $\\exp(\\beta_j)$ for a single feature $x_j$.\nThe odds of caries are $\\text{Odds} = \\exp(z) = \\exp(\\beta_{0} + \\sum_{i=1}^{d} \\beta_{i}x_{i})$.\nConsider a one-unit increase in a specific feature $x_j$, while holding all other features $x_k$ (for $k \\neq j$) constant.\nThe original odds are $\\text{Odds}_1 = \\exp(\\beta_0 + \\sum_{k \\neq j}\\beta_k x_k + \\beta_j x_j)$.\nThe new odds after increasing $x_j$ by $1$ are $\\text{Odds}_2 = \\exp(\\beta_0 + \\sum_{k \\neq j}\\beta_k x_k + \\beta_j (x_j+1))$.\nThe ratio of these odds, known as the odds ratio (OR), is:\n$$ \\text{OR} = \\frac{\\text{Odds}_2}{\\text{Odds}_1} = \\frac{\\exp(\\beta_0 + \\sum_{k \\neq j}\\beta_k x_k + \\beta_j x_j + \\beta_j)}{\\exp(\\beta_0 + \\sum_{k \\neq j}\\beta_k x_k + \\beta_j x_j)} = \\frac{\\exp(\\beta_0 + \\sum_{i=1}^{d}\\beta_i x_i) \\exp(\\beta_j)}{\\exp(\\beta_0 + \\sum_{i=1}^{d}\\beta_i x_i)} = \\exp(\\beta_j) $$\nIn clinical terms, $\\exp(\\beta_j)$ represents the multiplicative factor by which the odds of having caries change for every one-unit increase in the feature $x_j$, assuming all other covariates in the model (such as other image metrics or patient-reported data) are held constant. For example, if $x_j$ is a dietary frequency score and $\\exp(\\beta_j) = 1.2$, then a one-point increase in this score is associated with a $20\\%$ increase in the odds of having caries, ceteris paribus.\n\nThe final answer requested is the expression for $\\gamma$.",
            "answer": "$$ \\boxed{\\ln\\left(\\frac{C_{\\mathrm{FP}}}{C_{\\mathrm{FN}}}\\right)} $$"
        }
    ]
}