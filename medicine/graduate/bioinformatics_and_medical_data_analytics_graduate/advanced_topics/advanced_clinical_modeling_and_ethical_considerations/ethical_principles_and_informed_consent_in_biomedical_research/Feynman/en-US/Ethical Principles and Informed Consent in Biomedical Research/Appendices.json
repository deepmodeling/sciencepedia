{
    "hands_on_practices": [
        {
            "introduction": "The principle of $k$-anonymity is a foundational technique in data de-identification, ensuring that any individual's record is indistinguishable from at least $k-1$ others. However, this protection is not absolute. This exercise guides you through a first-principles analysis to quantify the two primary residual risks: identity disclosure (correctly re-identifying a record) and attribute disclosure (correctly inferring a sensitive property). Quantifying these baseline risks is the essential first step in evaluating the ethical acceptability of a data release and motivating the need for stronger privacy models .",
            "id": "4560898",
            "problem": "A biomedical research team plans to release a de-identified Electronic Health Records (EHR) dataset for secondary analysis under Institutional Review Board (IRB) oversight. The dataset applies the definition of $k$-anonymity: every released record belongs to an equivalence class of at least $k$ records that share the same quasi-identifier pattern. Consider an adversary who knows that a particular individual participated in the study and knows their quasi-identifier pattern, but has no additional tie-breaking auxiliary information beyond what is consistent with the released data.\n\nStarting from the basic definitions of probability and Bayes’ theorem, derive the adversary’s worst-case posterior probability of correctly re-identifying the individual’s record within an equivalence class of size $k$. Next, consider a sensitive attribute $S$ (for example, diagnosis categories) with $m$ mutually exclusive categories. Within the individual’s equivalence class $E$, let the empirical distribution of $S$ be $\\mathbf{q} = (q_{1}, q_{2}, \\dots, q_{m})$ with $\\sum_{j=1}^{m} q_{j} = 1$. Under the same adversarial knowledge, derive the adversary’s maximum posterior probability of correctly inferring the individual’s sensitive attribute value.\n\nTo formalize ethical risk for consent evaluation, suppose the research team quantifies expected harm as a weighted sum of identity and attribute disclosure risks,\n$$\nR = \\lambda_{I} \\, P(\\text{identity correct}) + \\lambda_{A} \\, P(\\text{attribute correct}),\n$$\nwhere $\\lambda_{I} \\ge 0$ and $\\lambda_{A} \\ge 0$ are institutional weights representing relative harm severity. For a particular equivalence class with $k = 15$, a sensitive attribute distribution $\\mathbf{q} = (0.90, 0.06, 0.04)$, and weights $\\lambda_{I} = 0.50$ and $\\lambda_{A} = 0.50$, compute $R$. Express your final $R$ as a decimal and round to four significant figures.",
            "solution": "The problem as stated is scientifically grounded, well-posed, and objective. It is based on standard, formal definitions from the field of data privacy, specifically $k$-anonymity, and uses fundamental principles of probability theory to model adversarial risk. All necessary parameters for calculation are provided, and no contradictions exist. Therefore, the problem is valid, and we may proceed with a formal solution.\n\nThe solution is divided into three parts: first, the derivation of the identity disclosure risk; second, the derivation of the attribute disclosure risk; and third, the calculation of the total expected harm for the given parameters.\n\n**Part 1: Derivation of Identity Disclosure Risk**\n\nLet $E$ denote the equivalence class identified by the adversary, which contains the record of a specific individual of interest. The size of this equivalence class is given as $|E| = k$. By the definition of $k$-anonymity, all $k$ records within $E$ are indistinguishable based on their quasi-identifiers.\n\nThe adversary knows the individual is in the study and has identified their equivalence class $E$. The adversary's goal is to re-identify the specific record belonging to the individual from the $k$ records in $E$. The problem statement specifies that the adversary \"has no additional tie-breaking auxiliary information beyond what is consistent with the released data.\"\n\nLet the set of records in the equivalence class be $\\{r_1, r_2, \\dots, r_k\\}$. Let $H_i$ be the hypothesis that record $r_i$ is the record of the target individual. The adversary's state of knowledge implies that, a priori, each record is equally likely to be the target's record. This is an application of the principle of indifference. The prior probability for each hypothesis is:\n$$\nP(H_i) = \\frac{1}{k} \\quad \\text{for } i = 1, 2, \\dots, k\n$$\nThe problem asks for the adversary's worst-case posterior probability. Since the released data (the equivalence class itself) provides no information to distinguish among the $k$ records, the posterior probability does not update from the prior. An adversary's best guess is to select any one of the $k$ records. The probability of this guess being correct is $\\frac{1}{k}$. There is no strategy that can improve upon this, so this represents the maximum success probability for the adversary. This is the worst-case scenario from a privacy perspective.\n\nTherefore, the worst-case posterior probability of correct re-identification is:\n$$\nP(\\text{identity correct}) = \\frac{1}{k}\n$$\n\n**Part 2: Derivation of Attribute Disclosure Risk**\n\nLet $S$ be a sensitive attribute with $m$ mutually exclusive categories, denoted $\\{v_1, v_2, \\dots, v_m\\}$. Within the individual's equivalence class $E$, the empirical distribution of these attribute values is given by the vector $\\mathbf{q} = (q_1, q_2, \\dots, q_m)$, where $q_j$ is the fraction of records in $E$ that have the sensitive attribute value $v_j$. We are given that $\\sum_{j=1}^{m} q_j = 1$. The number of records in $E$ with attribute value $v_j$ is $k \\cdot q_j$.\n\nThe adversary's goal is to correctly infer the individual's true sensitive attribute value. Let $S_I$ be the true attribute value of the individual. As established in Part 1, from the adversary's perspective, the individual's record is a random draw from the $k$ records in $E$. Therefore, the probability that the individual's true attribute value is $v_j$ is equal to the proportion of records in $E$ having that value:\n$$\nP(S_I = v_j) = q_j\n$$\nA rational adversary will seek to maximize their chance of being correct. Their optimal strategy is to guess the most frequent attribute value within the equivalence class. The probability of success for this strategy is the probability of the most likely attribute.\n\nTherefore, the adversary's maximum posterior probability of correctly inferring the sensitive attribute value is:\n$$\nP(\\text{attribute correct}) = \\max_{j \\in \\{1, \\dots, m\\}} \\{q_j\\}\n$$\n\n**Part 3: Calculation of Total Expected Harm**\n\nThe total expected harm, $R$, is defined as a weighted sum of the identity and attribute disclosure risks:\n$$\nR = \\lambda_{I} \\, P(\\text{identity correct}) + \\lambda_{A} \\, P(\\text{attribute correct})\n$$\nSubstituting the expressions derived in the previous parts:\n$$\nR = \\lambda_{I} \\left(\\frac{1}{k}\\right) + \\lambda_{A} \\left(\\max_{j} \\{q_j\\}\\right)\n$$\nWe are given the following specific values:\n- Equivalence class size: $k = 15$\n- Sensitive attribute distribution: $\\mathbf{q} = (0.90, 0.06, 0.04)$\n- Institutional weights: $\\lambda_{I} = 0.50$ and $\\lambda_{A} = 0.50$\n\nFirst, we compute the component probabilities:\nThe identity disclosure risk is:\n$$\nP(\\text{identity correct}) = \\frac{1}{15}\n$$\nThe attribute disclosure risk is the maximum value in the distribution $\\mathbf{q}$:\n$$\nP(\\text{attribute correct}) = \\max\\{0.90, 0.06, 0.04\\} = 0.90\n$$\nNow, substitute these probabilities and the given weights into the formula for $R$:\n$$\nR = (0.50) \\cdot \\left(\\frac{1}{15}\\right) + (0.50) \\cdot (0.90)\n$$\n$$\nR = \\frac{0.50}{15} + 0.45\n$$\n$$\nR = \\frac{1/2}{15} + 0.45 = \\frac{1}{30} + 0.45\n$$\nTo perform the final calculation, we can convert the fraction to a decimal. $\\frac{1}{30} = 0.033333...$\n$$\nR = 0.033333... + 0.45 = 0.483333...\n$$\nThe problem requires the final answer to be rounded to four significant figures. The first four significant figures are $4$, $8$, $3$, and $3$. The fifth digit is $3$, which is less than $5$, so we round down.\n$$\nR \\approx 0.4833\n$$",
            "answer": "$$\n\\boxed{0.4833}\n$$"
        },
        {
            "introduction": "As the previous exercise demonstrates, $k$-anonymity alone can fail to prevent attribute disclosure, particularly if a sensitive value is common within an equivalence class. To address this, the principle of $l$-diversity requires that each group contains at least $l$ \"well-represented\" sensitive attribute values. This practice challenges you to work with a more statistically robust variant, entropy $l$-diversity, which uses Shannon entropy to measure the effective diversity of sensitive attributes . By calculating the maximum level of entropy $l$-diversity a dataset satisfies, you will gain hands-on experience with the formal methods used to ensure stronger privacy against attribute inference.",
            "id": "4560890",
            "problem": "A hospital consortium plans to share a de-identified research dataset under the principle of ethical data stewardship consistent with informed consent obligations. To mitigate attribute disclosure risk beyond $k$-anonymity, the consortium requires that the sensitive attribute (International Classification of Diseases, Tenth Revision (ICD-10) diagnosis code) satisfy entropy $l$-diversity across all $k$-anonymity equivalence classes induced by quasi-identifiers (age band, sex, and three-digit postal prefix). The entropy must be computed with the natural logarithm (base $e$).\n\nThree equivalence classes have the following distributions of diagnosis codes, where each count indicates how many records in the class carry that diagnosis code. Let the diagnosis codes be denoted by $d_1$, $d_2$, $d_3$, and $d_4$.\n\nClass $E_1$ ($30$ records): $d_1 = 12$, $d_2 = 10$, $d_3 = 8$.\n\nClass $E_2$ ($25$ records): $d_1 = 20$, $d_2 = 5$.\n\nClass $E_3$ ($45$ records): $d_1 = 15$, $d_2 = 15$, $d_3 = 10$, $d_4 = 5$.\n\nUsing the foundational definition of Shannon entropy for a finite categorical distribution and the entropy $l$-diversity criterion defined in terms of this entropy, derive from first principles and compute the largest real number $l$ such that entropy $l$-diversity holds simultaneously for all equivalence classes listed above. Round your final numerical answer to four significant figures. Express $l$ as a pure number with no units.",
            "solution": "The problem requires the determination of the largest real number $l$ such that a dataset satisfies entropy $l$-diversity for three given equivalence classes. This involves calculating the Shannon entropy for the distribution of sensitive attributes within each class and then applying the definition of entropy $l$-diversity.\n\nFirst, let us state the foundational principles. The Shannon entropy, $H$, of a discrete probability distribution $\\{p_1, p_2, \\ldots, p_n\\}$ is defined as:\n$$H = -\\sum_{i=1}^{n} p_i \\log_b(p_i)$$\nThe problem specifies the use of the natural logarithm, so the base $b$ is Euler's number, $e$. The formula becomes:\n$$H = -\\sum_{i=1}^{n} p_i \\ln(p_i)$$\nwhere $p_i$ is the probability of the $i$-th sensitive attribute value within an equivalence class. The probability $p_i$ is calculated as the ratio of the number of records with the $i$-th attribute, $c_i$, to the total number of records in the equivalence class, $N$, so $p_i = c_i / N$.\n\nThe criterion for entropy $l$-diversity is that for any equivalence class, the entropy $H$ of its sensitive attribute distribution must be greater than or equal to $\\ln(l)$:\n$$H \\ge \\ln(l)$$\nTo find the largest value of $l$ that holds true simultaneously for all given equivalence classes ($E_1$, $E_2$, and $E_3$), we must satisfy the condition for each class:\n$$H(E_1) \\ge \\ln(l)$$\n$$H(E_2) \\ge \\ln(l)$$\n$$H(E_3) \\ge \\ln(l)$$\nThis system of inequalities implies that $\\ln(l)$ must be less than or equal to the minimum of the entropies of all classes:\n$$\\ln(l) \\le \\min\\{H(E_1), H(E_2), H(E_3)\\}$$\nTo find the largest possible value of $l$, we take the equality:\n$$\\ln(l) = \\min\\{H(E_1), H(E_2), H(E_3)\\}$$\nThus, the task reduces to calculating the Shannon entropy for each class, identifying the minimum value, and then calculating $l$ as:\n$$l = \\exp(\\min\\{H(E_1), H(E_2), H(E_3)\\})$$\n\nNow, we compute the entropy for each class based on the provided data.\n\n**Equivalence Class $E_1$:**\nTotal records $N_1 = 30$.\nCounts for diagnosis codes: $c_{1,1} = 12$, $c_{1,2} = 10$, $c_{1,3} = 8$.\nThe corresponding probabilities are:\n$p_{1,1} = \\frac{12}{30} = \\frac{2}{5} = 0.4$\n$p_{1,2} = \\frac{10}{30} = \\frac{1}{3}$\n$p_{1,3} = \\frac{8}{30} = \\frac{4}{15}$\nThe entropy $H(E_1)$ is:\n$$H(E_1) = -\\left[ \\frac{12}{30} \\ln\\left(\\frac{12}{30}\\right) + \\frac{10}{30} \\ln\\left(\\frac{10}{30}\\right) + \\frac{8}{30} \\ln\\left(\\frac{8}{30}\\right) \\right]$$\n$$H(E_1) = -\\left[ 0.4 \\ln(0.4) + \\frac{1}{3} \\ln\\left(\\frac{1}{3}\\right) + \\frac{4}{15} \\ln\\left(\\frac{4}{15}\\right) \\right]$$\nNumerically, this evaluates to:\n$$H(E_1) \\approx -[0.4(-0.91629) + 0.33333(-1.09861) + 0.26667(-1.32176)]$$\n$$H(E_1) \\approx -[-0.36652 - 0.36620 - 0.35247] \\approx 1.08519$$\n\n**Equivalence Class $E_2$:**\nTotal records $N_2 = 25$.\nCounts for diagnosis codes: $c_{2,1} = 20$, $c_{2,2} = 5$.\nThe corresponding probabilities are:\n$p_{2,1} = \\frac{20}{25} = \\frac{4}{5} = 0.8$\n$p_{2,2} = \\frac{5}{25} = \\frac{1}{5} = 0.2$\nThe entropy $H(E_2)$ is:\n$$H(E_2) = -\\left[ \\frac{20}{25} \\ln\\left(\\frac{20}{25}\\right) + \\frac{5}{25} \\ln\\left(\\frac{5}{25}\\right) \\right]$$\n$$H(E_2) = -\\left[ 0.8 \\ln(0.8) + 0.2 \\ln(0.2) \\right]$$\nNumerically, this evaluates to:\n$$H(E_2) \\approx -[0.8(-0.22314) + 0.2(-1.60944)]$$\n$$H(E_2) \\approx -[-0.17851 - 0.32189] \\approx 0.50040$$\n\n**Equivalence Class $E_3$:**\nTotal records $N_3 = 45$.\nCounts for diagnosis codes: $c_{3,1} = 15$, $c_{3,2} = 15$, $c_{3,3} = 10$, $c_{3,4} = 5$.\nThe corresponding probabilities are:\n$p_{3,1} = \\frac{15}{45} = \\frac{1}{3}$\n$p_{3,2} = \\frac{15}{45} = \\frac{1}{3}$\n$p_{3,3} = \\frac{10}{45} = \\frac{2}{9}$\n$p_{3,4} = \\frac{5}{45} = \\frac{1}{9}$\nThe entropy $H(E_3)$ is:\n$$H(E_3) = -\\left[ 2 \\cdot \\frac{15}{45} \\ln\\left(\\frac{15}{45}\\right) + \\frac{10}{45} \\ln\\left(\\frac{10}{45}\\right) + \\frac{5}{45} \\ln\\left(\\frac{5}{45}\\right) \\right]$$\n$$H(E_3) = -\\left[ \\frac{2}{3} \\ln\\left(\\frac{1}{3}\\right) + \\frac{2}{9} \\ln\\left(\\frac{2}{9}\\right) + \\frac{1}{9} \\ln\\left(\\frac{1}{9}\\right) \\right]$$\nNumerically, this evaluates to:\n$$H(E_3) \\approx -[0.66667(-1.09861) + 0.22222(-1.50408) + 0.11111(-2.19722)]$$\n$$H(E_3) \\approx -[-0.73241 - 0.33424 - 0.24414] \\approx 1.31079$$\n\nComparing the computed entropies:\n$H(E_1) \\approx 1.08519$\n$H(E_2) \\approx 0.50040$\n$H(E_3) \\approx 1.31079$\nThe minimum entropy is $H_{min} = H(E_2) \\approx 0.50040$.\n\nTo find the largest $l$, we set $\\ln(l) = H_{min}$:\n$$\\ln(l) = H(E_2) = -\\left[ 0.8 \\ln(0.8) + 0.2 \\ln(0.2) \\right] \\approx 0.5004024$$\nNow, we solve for $l$:\n$$l = \\exp\\left(-\\left[ 0.8 \\ln(0.8) + 0.2 \\ln(0.2) \\right]\\right)$$\n$$l \\approx \\exp(0.5004024) \\approx 1.64947$$\nThe problem requires the answer to be rounded to four significant figures.\n$1.64947 \\to 1.649$.\n\nTherefore, the largest real number $l$ such that entropy $l$-diversity holds for all three equivalence classes is approximately $1.649$.",
            "answer": "$$\\boxed{1.649}$$"
        },
        {
            "introduction": "Deterministic models like $k$-anonymity and $l$-diversity are vulnerable to attacks that use auxiliary information. Differential Privacy (DP) offers a more robust, probabilistic framework that provides mathematically provable guarantees against a wide range of attacks. The core ethical and practical challenge of DP lies in the privacy-utility trade-off, controlled by the privacy parameter $\\epsilon$. This exercise moves beyond simple risk calculation to model this fundamental balance, allowing you to derive an expression for the total expected harm reduction achieved by deploying a DP system . This practice will help you develop the analytical skills to reason about how choices in privacy technology directly impact the ethical principles of beneficence and respect for persons.",
            "id": "4560960",
            "problem": "A biomedical data analytics team must decide whether to release a de-identified research dataset from a precision oncology biobank. They will only release under a formal Differential Privacy (DP) mechanism, Differential Privacy (DP), with privacy parameter $\\epsilon$. In the context of ethical principles for biomedical research, particularly beneficence and respect for persons, the team models harm along two channels: privacy harm due to possible re-identification and downstream clinical harm due to utility loss in models trained on privacy-perturbed data. The expected harm is defined as probability of a harmful event times its severity, averaged over participants.\n\nAssume there are $N=25000$ participants. For each participant, the severity of harm conditional on re-identification is $c_{p}=2.8$ standardized harm units, scaled by an individual sensitivity weight $w \\in [0,1]$ reflecting variability in privacy preferences and vulnerability. The sensitivity weights are independent and identically distributed as a Beta distribution $\\text{Beta}(a,b)$ with shape parameters $a=3$ and $b=4$. The non-DP baseline probability of re-identification for a released dataset is $r_{\\text{ND}}=0.09$. Under DP with parameter $\\epsilon$, the re-identification risk is modeled as\n$$\nr(\\epsilon)=r_{\\text{ND}}\\cdot \\frac{\\epsilon}{\\epsilon+\\lambda},\n$$\nwhere $\\lambda=0.8$ is a calibration constant that encodes the attenuation of attack success probabilities as $\\epsilon$ decreases. Utility loss under DP is modeled as\n$$\nu(\\epsilon)=\\frac{\\lambda}{\\epsilon+\\lambda},\n$$\nwhich captures the reduction in predictive model accuracy attributable to added noise. The downstream clinical harm per participant attributable to utility loss is $c_{u}=0.05$ standardized harm units per unit utility loss. In the non-DP baseline, there is no utility loss, so the downstream harm term is zero.\n\nUsing the foundational definitions of expected value and risk, and the ethical requirement to balance beneficence (minimizing expected harm) against scientific utility, derive the total expected harm reduction function $H_{\\text{red}}(\\epsilon)$, defined as the difference between the baseline expected total harm without DP and the expected total harm with DP, aggregated over all $N$ participants. Express the final $H_{\\text{red}}(\\epsilon)$ as a single simplified analytic expression in terms of $\\epsilon$, with no physical units, and do not round or approximate.",
            "solution": "The objective is to find the total expected harm reduction function, $H_{\\text{red}}(\\epsilon)$, which is the difference between the baseline total expected harm (without Differential Privacy) and the total expected harm with Differential Privacy.\n\n$H_{\\text{red}}(\\epsilon) = H_{\\text{total, baseline}} - H_{\\text{total, DP}}(\\epsilon)$\n\nWe will first formulate expressions for the total expected harm in both the baseline and DP scenarios. The total harm is the sum of privacy harm and clinical harm, aggregated over all $N$ participants.\n\n**1. Expected Harm per Participant**\n\nThe total expected harm for a single participant is the sum of the expected privacy harm and the expected clinical harm.\n$H_{\\text{participant}} = E[\\text{Privacy Harm}] + E[\\text{Clinical Harm}]$\n\n**Expected Privacy Harm:**\nThe privacy harm for a participant with sensitivity weight $w$ is the probability of re-identification multiplied by the severity of that event, which is $w \\cdot c_p$. The expected privacy harm is found by taking the expectation over the random variable $w$.\n\n$E[\\text{Privacy Harm}] = E[\\text{Probability of Re-ID} \\times w \\cdot c_p] = (\\text{Probability of Re-ID}) \\cdot c_p \\cdot E[w]$\n\nThe sensitivity weight $w$ follows a Beta distribution, $w \\sim \\text{Beta}(a,b)$. The expected value of a random variable following this distribution is:\n$E[w] = \\frac{a}{a+b}$\nSubstituting the given values $a=3$ and $b=4$:\n$E[w] = \\frac{3}{3+4} = \\frac{3}{7}$\n\n**Expected Clinical Harm:**\nThe clinical harm is due to utility loss $u(\\epsilon)$. The problem states the harm is $c_u$ per unit of utility loss.\n$E[\\text{Clinical Harm}] = u(\\epsilon) \\cdot c_u$\nThis component is deterministic for a given $\\epsilon$.\n\n**2. Total Expected Harm under DP, $H_{\\text{total, DP}}(\\epsilon)$**\n\nFor the case with Differential Privacy, the probability of re-identification is $r(\\epsilon) = r_{\\text{ND}} \\frac{\\epsilon}{\\epsilon+\\lambda}$.\nThe expected privacy harm per participant is:\n$H_{\\text{priv, DP}}(\\epsilon) = r(\\epsilon) \\cdot c_p \\cdot E[w] = \\left(r_{\\text{ND}} \\frac{\\epsilon}{\\epsilon+\\lambda}\\right) \\cdot c_p \\cdot \\frac{a}{a+b}$\n\nThe utility loss is $u(\\epsilon) = \\frac{\\lambda}{\\epsilon+\\lambda}$.\nThe clinical harm per participant is:\n$H_{\\text{clin, DP}}(\\epsilon) = u(\\epsilon) \\cdot c_u = \\left(\\frac{\\lambda}{\\epsilon+\\lambda}\\right) \\cdot c_u$\n\nThe total expected harm per participant under DP is the sum of these two components:\n$H_{\\text{participant, DP}}(\\epsilon) = H_{\\text{priv, DP}}(\\epsilon) + H_{\\text{clin, DP}}(\\epsilon) = \\left(r_{\\text{ND}} \\frac{\\epsilon}{\\epsilon+\\lambda}\\right) c_p \\frac{a}{a+b} + \\left(\\frac{\\lambda}{\\epsilon+\\lambda}\\right) c_u$\nAggregating over all $N$ participants:\n$H_{\\text{total, DP}}(\\epsilon) = N \\cdot H_{\\text{participant, DP}}(\\epsilon) = N \\left[ \\frac{\\epsilon}{\\epsilon+\\lambda} r_{\\text{ND}} c_p \\frac{a}{a+b} + \\frac{\\lambda}{\\epsilon+\\lambda} c_u \\right]$\n$H_{\\text{total, DP}}(\\epsilon) = \\frac{N}{\\epsilon+\\lambda} \\left[ \\left( r_{\\text{ND}} c_p \\frac{a}{a+b} \\right) \\epsilon + c_u \\lambda \\right]$\n\n**3. Total Expected Harm at Baseline, $H_{\\text{total, baseline}}$**\n\nIn the non-DP baseline case:\nThe probability of re-identification is given as $r_{\\text{ND}}$.\nThe expected privacy harm per participant is:\n$H_{\\text{priv, baseline}} = r_{\\text{ND}} \\cdot c_p \\cdot E[w] = r_{\\text{ND}} c_p \\frac{a}{a+b}$\n\nThe problem states that in the non-DP baseline, there is no utility loss, so the downstream clinical harm is zero.\n$H_{\\text{clin, baseline}} = 0$\n\nThe total expected harm per participant at baseline is:\n$H_{\\text{participant, baseline}} = r_{\\text{ND}} c_p \\frac{a}{a+b}$\nAggregating over all $N$ participants:\n$H_{\\text{total, baseline}} = N \\cdot H_{\\text{participant, baseline}} = N r_{\\text{ND}} c_p \\frac{a}{a+b}$\n\n**4. Harm Reduction Function, $H_{\\text{red}}(\\epsilon)$**\n\nNow we compute the harm reduction by subtracting the DP harm from the baseline harm.\n$H_{\\text{red}}(\\epsilon) = H_{\\text{total, baseline}} - H_{\\text{total, DP}}(\\epsilon)$\n$H_{\\text{red}}(\\epsilon) = N r_{\\text{ND}} c_p \\frac{a}{a+b} - \\frac{N}{\\epsilon+\\lambda} \\left[ \\left( r_{\\text{ND}} c_p \\fraca}{a+b} \\right) \\epsilon + c_u \\lambda \\right]$\n\nLet's simplify by factoring out common terms. Let $C = N r_{\\text{ND}} c_p \\frac{a}{a+b}$.\n$H_{\\text{red}}(\\epsilon) = C - \\frac{1}{\\epsilon+\\lambda} [C \\epsilon + N c_u \\lambda]$\n$H_{\\text{red}}(\\epsilon) = \\frac{C(\\epsilon+\\lambda) - (C\\epsilon + N c_u \\lambda)}{\\epsilon+\\lambda}$\n$H_{\\text{red}}(\\epsilon) = \\frac{C\\epsilon + C\\lambda - C\\epsilon - N c_u \\lambda}{\\epsilon+\\lambda}$\n$H_{\\text{red}}(\\epsilon) = \\frac{C\\lambda - N c_u \\lambda}{\\epsilon+\\lambda}$\n$H_{\\text{red}}(\\epsilon) = \\frac{\\lambda(C - N c_u)}{\\epsilon+\\lambda}$\n\nSubstitute back the expression for $C$:\n$H_{\\text{red}}(\\epsilon) = \\frac{\\lambda \\left( N r_{\\text{ND}} c_p \\frac{a}{a+b} - N c_u \\right)}{\\epsilon+\\lambda}$\n$H_{\\text{red}}(\\epsilon) = \\frac{N\\lambda \\left( r_{\\text{ND}} c_p \\frac{a}{a+b} - c_u \\right)}{\\epsilon+\\lambda}$\n\n**5. Substitution of Numerical Values**\n\nWe now substitute the given numerical values into the simplified expression.\n$N=25000$, $\\lambda=0.8$, $r_{\\text{ND}}=0.09$, $c_p=2.8$, $a=3$, $b=4$, $c_u=0.05$.\n\nFirst, calculate the term in the parentheses:\n$r_{\\text{ND}} c_p \\frac{a}{a+b} - c_u = (0.09) \\cdot (2.8) \\cdot \\frac{3}{3+4} - 0.05$\n$= (0.09) \\cdot (2.8) \\cdot \\frac{3}{7}$\nSince $2.8/7 = 0.4$:\n$= (0.09) \\cdot (0.4) \\cdot 3 - 0.05$\n$= (0.09) \\cdot (1.2) - 0.05$\n$= 0.108 - 0.05 = 0.058$\n\nNow calculate the numerator of the main expression, $N\\lambda \\times (\\text{term above})$:\nNumerator = $25000 \\cdot 0.8 \\cdot 0.058$\n$= 20000 \\cdot 0.058$\n$= 2 \\times 10^4 \\cdot 5.8 \\times 10^{-2}$\n$= 11.6 \\times 10^2 = 1160$\n\nThe denominator is $\\epsilon+\\lambda = \\epsilon+0.8$.\nCombining these results, the final expression for the harm reduction function is:\n$H_{\\text{red}}(\\epsilon) = \\frac{1160}{\\epsilon+0.8}$",
            "answer": "$$\\boxed{\\frac{1160}{\\epsilon + 0.8}}$$"
        }
    ]
}