{
    "hands_on_practices": [
        {
            "introduction": "将原始DNA序列转化为模型可以处理的输入，是构建基因组自监督学习模型的第一步。与直接使用单个核苷酸作为输入不同，现代模型通常依赖于“分词”（tokenization）技术，将序列分割成可变长度的子序列（即“词元”或token）。本练习将引导您从零开始实现两种主流的分词算法——字节对编码（Byte-Pair Encoding, BPE）和一元语言模型（Unigram Language Model, ULM），并使用信息论指标和与生物重复序列的对齐度来评估它们的性能。通过这项实践，您将深入理解不同分词策略在基因组数据预处理中的权衡，以及它们如何影响模型捕捉生物学上有意义的单元（如密码子或基序）的能力。",
            "id": "4606969",
            "problem": "给定一组脱氧核糖核酸（DNA）序列，请您评估字节对编码（BPE）和一元语言模型（ULM）作为基于分词的压缩器，在基因组学自监督学习环境中的适用性。使用基本编码理论，将压缩定义为由概率分词器引起的期望编码长度，并量化重复区域附近的分词错误。您需要从第一性原理出发实现这两种方法，并在一套测试用例上计算比较指标。\n\n从以下基本概念开始：\n- 香农信源编码原理指出，对于词元 $w$ 上的一个离散概率质量函数 $p(w)$，一个词元 $w$ 的最优期望编码长度（以比特为单位）由 $-\\log_2 p(w)$ 给出。对于一个被分割为词元 $\\{w_1, w_2, \\dots, w_T\\}$ 的序列，其总编码长度为 $$L = \\sum_{t=1}^{T} -\\log_2 p(w_t).$$\n- 对于一个长度为 $N$ 的 DNA 序列，每个核苷酸的平均编码长度为 $$\\bar{\\ell} = \\frac{L}{N}.$$\n- 重复区域被定义为一个固定长度的基序（motif）连续重复的区域。设 $s$ 是该区域在序列中的起始索引（从零开始），$m$ 是基序长度，$r$ 是基序重复的次数，因此该区域覆盖的索引范围是 $[s, s + r \\cdot m)$。\n\n您必须实现以下方法：\n1. 字节对编码（BPE），其定义为从一个字符级词汇表 $\\{A, C, G, T\\}$ 开始，迭代地合并最频繁的相邻词元对以形成新的词元，直到达到指定的词汇量大小 $V_{\\mathrm{BPE}}$。在每一步合并时，通过选择表示为连接字符串 $(\\text{left} + \\text{right})$ 时字典序最小的对来打破平局。通过对训练序列重复应用学习到的合并规则来获得训练分词结果。概率是根据训练分词结果中的词元频率，使用加法平滑来估计的：$$p_{\\mathrm{BPE}}(w) = \\frac{c(w) + \\lambda}{\\sum_{u} c(u) + \\lambda \\cdot |V|},$$ 其中 $c(w)$ 是词元 $w$ 的计数， $|V|$ 是最终的词汇量大小，$\\lambda$ 是平滑参数。测试序列通过按顺序应用相同的合并操作进行分词。\n\n2. 一元语言模型（ULM）分词：构建一个候选词汇表 $V_{\\mathrm{ULM}}$，该词汇表包含训练序列中长度从 $1$ 到 $L_{\\max}$ 的所有子串，并计算其出现次数（允许重叠）。使用加法平滑估计概率：$$p_{\\mathrm{ULM}}(w) = \\frac{c(w) + \\alpha}{\\sum_{u \\in V_{\\mathrm{ULM}}} c(u) + \\alpha \\cdot |V_{\\mathrm{ULM}}|},$$ 其中 $\\alpha$ 是平滑参数。通过动态规划对测试序列进行分词，以找到最小化总负对数概率的分割方式：$$\\min_{\\text{segmentations}} \\sum_{t} -\\log_2 p_{\\mathrm{ULM}}(w_t),$$ 约束条件是每个词元 $w_t \\in V_{\\mathrm{ULM}}$ 并且词元长度最多为 $L_{\\max}$。\n\n定义并计算重复区域附近的以下误差度量：\n- 设内部基序边界集合为 $$B = \\{s + m, s + 2m, \\dots, s + (r - 1)m\\},$$ 对于 $r \\geq 2$ 其基数为 $|B| = r - 1$。设 $T$ 是区域内的词元边界位置集合，其中每个边界是区域内第一个词元之后任意词元的起始索引。定义边界错位误差为 $$e = \\begin{cases}1 - \\frac{|B \\cap T|}{|B|},  \\text{if } |B| > 0, \\\\ 0,  \\text{if } |B| = 0. \\end{cases}$$ 这个指标衡量了未被词元边界捕获的基序边界的比例。\n\n对于每个测试用例，计算：\n- BPE 每个核苷酸的平均比特数 $\\bar{\\ell}_{\\mathrm{BPE}}$。\n- ULM 每个核苷酸的平均比特数 $\\bar{\\ell}_{\\mathrm{ULM}}$。\n- 边界错位误差 $e_{\\mathrm{BPE}}$。\n- 边界错位误差 $e_{\\mathrm{ULM}}$。\n\n对于两种方法，都使用给定的完整序列同时作为训练序列和测试序列。所有对数必须以 2 为底。对于概率估计，使用指定的 $\\lambda$ 和 $\\alpha$ 值进行加法平滑。\n\n实现程序以解决以下测试用例集，其中索引是基于零的，并且序列已明确提供：\n\n- 测试用例 1（正常情况）：序列 $S_1 =$ \"ACGTAC\" + \"ATG\" 重复 $10$ 次 + \"GGAAC\"。参数：$V_{\\mathrm{BPE}} = 12$, $L_{\\max} = 5$, $\\lambda = 1$, $\\alpha = 1$；重复区域起始位置 $s = 6$，基序长度 $m = 3$，重复次数 $r = 10$。\n\n- 测试用例 2（边界，同聚物）：序列 $S_2 =$ \"CG\" + \"A\" 重复 $20$ 次 + \"TGC\"。参数：$V_{\\mathrm{BPE}} = 8$, $L_{\\max} = 10$, $\\lambda = 1$, $\\alpha = 0.5$；重复区域起始位置 $s = 2$，基序长度 $m = 1$，重复次数 $r = 20$。\n\n- 测试用例 3（边缘，稀疏重复）：序列 $S_3 =$ \"GCTATCGAGT\" + \"CGT\" 重复 $3$ 次 + \"ACGTA\"。参数：$V_{\\mathrm{BPE}} = 6$, $L_{\\max} = 4$, $\\lambda = 1$, $\\alpha = 1$；重复区域起始位置 $s = 10$，基序长度 $m = 3$，重复次数 $r = 3$。\n\n- 测试用例 4（混合富含 GC 的重复）：序列 $S_4 =$ \"AT\" + \"GC\" 重复 $12$ 次 + \"TAAC\"。参数：$V_{\\mathrm{BPE}} = 10$, $L_{\\max} = 4$, $\\lambda = 1$, $\\alpha = 0.1$；重复区域起始位置 $s = 2$，基序长度 $m = 2$，重复次数 $r = 12$。\n\n您的程序必须：\n- 完全按照描述实现两种分词器。\n- 为每个测试用例计算四个量 $(\\bar{\\ell}_{\\mathrm{BPE}}, \\bar{\\ell}_{\\mathrm{ULM}}, e_{\\mathrm{BPE}}, e_{\\mathrm{ULM}})$。\n- 生成单行输出，包含一个用方括号括起来的逗号分隔列表。每个测试用例表示为一个四元列表，顺序为 $[\\bar{\\ell}_{\\mathrm{BPE}}, \\bar{\\ell}_{\\mathrm{ULM}}, e_{\\mathrm{BPE}}, e_{\\mathrm{ULM}}]$。将每个测试用例的所有四个值表示为保留 $6$ 位小数的十进制数。例如，最终输出必须看起来像 $$[[x_1,y_1,z_1,w_1],[x_2,y_2,z_2,w_2],[x_3,y_3,z_3,w_3],[x_4,y_4,z_4,w_4]].$$",
            "solution": "该问题要求对两种分词算法——字节对编码（BPE）和一元语言模型（ULM）——进行比较分析，以评估它们在压缩和分割基因组序列（尤其是在重复区域附近）方面的功效。此分析将通过从第一性原理实现这两种算法，并使用源于信息论和序列分析的指标，在一组给定的DNA序列上对它们进行评估来完成。\n\n### 基本原理\n\n压缩指标的核心是香农的信源编码定理。对于一个具有概率分布 $p(w)$ 的词元集合 $W$，一个词元 $w$ 的理论上最优的编码长度由其自信息 $I(w) = -\\log_2 p(w)$ 给出。对于一个词元序列 $w_1, w_2, \\dots, w_T$，总编码长度是各个编码长度之和：\n$$L = \\sum_{t=1}^{T} -\\log_2 p(w_t)$$\n将此值除以原始DNA序列中的核苷酸总数 $N$，得到每个核苷酸的平均比特数 $\\bar{\\ell} = L/N$，这是衡量压缩效率的指标。值越低表示压缩效果越好。\n\n第二个指标，边界错位误差 $e$，量化了词元边界与重复基序的自然边界的对齐程度。对于一个从索引 $s$ 开始、由长度为 $m$ 的基序重复 $r$ 次构成的区域，其内部基序边界位于位置 $B = \\{s+m, s+2m, \\dots, s+(r-1)m\\}$。给定一个在同一区域内产生词元边界 $T$ 的分词方法，误差是分词器错过的基序边界的比例：\n$$e = 1 - \\frac{|B \\cap T|}{|B|}$$\n对于 $|B| > 0$。误差 $e=0$ 表示完美对齐。\n\n### 方法 1：字节对编码（BPE）\n\nBPE 是一种贪婪数据压缩算法，它迭代地将最常见的相邻符号（词元）对替换为一个新符号。\n\n**1. 训练与分词：**\n该过程始于将 DNA 序列表示为其组成核苷酸（字符）的序列，这些核苷酸构成了初始词汇表 $V = \\{'A', 'C', 'G', 'T'\\}$。然后，算法执行指定数量的合并操作（$V_{\\mathrm{BPE}} - |V_{\\text{initial}}|$）。在每一步中：\na. 对当前序列表示中的所有相邻词元对进行计数。\nb. 选择频率最高的对进行合并。频率的平局通过选择连接字符串 $t_1+t_2$ 字典序最小的对 $(t_1, t_2)$ 来打破。\nc. 创建一个代表该对的新词元，并将其添加到词汇表中。\nd. 将序列中所有出现的被选中对替换为新词元。这种贪婪替换从左到右执行。\n\n由于问题规定训练和测试序列相同，训练过程产生的最终词元序列既用于概率估计，也用于指标计算。\n\n**2. 概率估计和编码长度：**\n在获得最终分词 $\\{w_1, w_2, \\dots, w_T\\}$ 后，确定词汇表中每个唯一词元 $w$ 的计数 $c(w)$。使用带有参数 $\\lambda$ 的加法（拉普拉斯）平滑来估计概率 $p_{\\mathrm{BPE}}(w)$：\n$$p_{\\mathrm{BPE}}(w) = \\frac{c(w) + \\lambda}{\\sum_{u \\in V} c(u) + \\lambda \\cdot |V|}$$\n其中 $|V| = V_{\\mathrm{BPE}}$ 是最终词汇量大小，$\\sum_{u \\in V} c(u)$ 是最终分词中的词元总数。总编码长度 $L_{\\mathrm{BPE}}$ 随后计算为 $\\sum_{t=1}^{T} -\\log_2 p_{\\mathrm{BPE}}(w_t)$。每个核苷酸的平均比特数为 $\\bar{\\ell}_{\\mathrm{BPE}} = L_{\\mathrm{BPE}} / N$。\n\n### 方法 2：一元语言模型（ULM）\n\nULM 分词将问题构建为寻找序列的最可能分词，其中可能性基于预先计算的词元概率模型。\n\n**1. 词汇表和概率估计：**\n首先，构建一个候选词汇表 $V_{\\mathrm{ULM}}$。它包含训练序列中长度从 $1$ 到最大值 $L_{\\max}$ 的所有子串。计算 $V_{\\mathrm{ULM}}$ 中每个子串 $w$ 的频率 $c(w)$（允许重叠）。然后使用带有参数 $\\alpha$ 的加法平滑来估计概率 $p_{\\mathrm{ULM}}(w)$：\n$$p_{\\mathrm{ULM}}(w) = \\frac{c(w) + \\alpha}{\\sum_{u \\in V_{\\mathrm{ULM}}} c(u) + \\alpha \\cdot |V_{\\mathrm{ULM}}|}$$\n为方便计算，我们使用负对数概率，它代表每个词元的“成本”：$\\text{cost}(w) = -\\log_2 p_{\\mathrm{ULM}}(w)$。\n\n**2. 通过动态规划进行最优分词：**\n测试序列（与训练序列相同）的最优分词是总成本最小的分词。这是一个经典的在有向无环图（DAG）上的最短路径问题，可以使用动态规划（维特比算法）高效解决。\n设 $dp[i]$ 为分割长度为 $i$ 的序列前缀 $S[0 \\dots i-1]$ 的最小成本。递推关系为：\n$$dp[i] = \\min_{1 \\le j \\le \\min(i, L_{\\max})} \\left( dp[i-j] + \\text{cost}(S[i-j:i]) \\right)$$\n基准情况为 $dp[0] = 0$。整个长度为 $N$ 的序列的最小成本即为 $dp[N]$，这恰好是总编码长度 $L_{\\mathrm{ULM}}$。每个核苷酸的平均比特数为 $\\bar{\\ell}_{\\mathrm{ULM}} = L_{\\mathrm{ULM}} / N$。\n通过在动态规划过程中存储回溯指针（即在每一步 $i$ 中是哪个 $j$ 产生了最小值），可以重构出最优的词元序列。\n\n### 指标计算\n\n对于 BPE 和 ULM，一旦确定了最终分词及其对应的起始索引，就计算边界错位误差 $e$。\n\n1.  **确定真实边界 ($B$)**：对于由 $(s, m, r)$ 定义的重复区域，内部基序边界集合为 $B = \\{s + k \\cdot m \\mid k = 1, 2, \\dots, r-1\\}$。如果 $r  2$，$B$ 为空。\n2.  **确定词元边界 ($T$)**：\n    a. 设完整序列分词中词元的起始索引为 $p_1, p_2, \\dots, p_K$。\n    b. 找到与指定重复区域 $[s, s + r \\cdot m)$ 重叠的第一个词元。设此词元在分词中的索引为 $j$。\n    c. 后续词元起始索引的集合是 $\\{p_{j+1}, p_{j+2}, \\dots, p_K\\}$。\n    d. 区域内的词元边界集合 $T$ 由这些落在区域范围内的后续起始索引构成：$T = \\{ p_k \\mid k > j \\text{ and } s \\le p_k  s+r \\cdot m \\}$。\n3.  **计算误差 ($e$)**：误差计算为 $e = 1 - \\frac{|B \\cap T|}{|B|}$，如果 $|B|=0$ 则为 $0$。\n\n这种结构化的方法能够对所提供的测试用例上的两种分词策略进行直接和定量的比较。",
            "answer": "```python\nimport numpy as np\nimport collections\nfrom math import log2\n\ndef solve():\n    \"\"\"\n    Main solver function to run all test cases and produce the final output.\n    \"\"\"\n\n    def get_bpe_pairs(tokens):\n        \"\"\"Counts adjacent token pairs.\"\"\"\n        return collections.Counter(zip(tokens, tokens[1:]))\n\n    def merge_bpe_tokens(tokens, pair, new_token):\n        \"\"\"Merges a specific pair in a token list.\"\"\"\n        new_tokens = []\n        i = 0\n        while i  len(tokens):\n            if i  len(tokens) - 1 and (tokens[i], tokens[i+1]) == pair:\n                new_tokens.append(new_token)\n                i += 2\n            else:\n                new_tokens.append(tokens[i])\n                i += 1\n        return new_tokens\n\n    def bpe_trainer(sequence, vocab_size):\n        \"\"\"Trains BPE and returns the final tokenization and merges.\"\"\"\n        if not sequence:\n            return [], []\n        \n        tokens = list(sequence)\n        initial_vocab_size = len(set(tokens))\n        num_merges = vocab_size - initial_vocab_size\n        \n        merges = []\n        if num_merges = 0:\n            return tokens, merges\n\n        for _ in range(num_merges):\n            pairs = get_bpe_pairs(tokens)\n            if not pairs:\n                break\n            \n            max_freq = max(pairs.values())\n            # Tie-breaking with lexicographical order\n            best_pairs_candidates = [p for p, freq in pairs.items() if freq == max_freq]\n            best_pairs_candidates.sort(key=lambda p: p[0] + p[1])\n            best_pair = best_pairs_candidates[0]\n            \n            new_token = best_pair[0] + best_pair[1]\n            tokens = merge_bpe_tokens(tokens, best_pair, new_token)\n            merges.append((best_pair, new_token))\n        \n        return tokens, merges\n\n    def ulm_trainer(sequence, l_max):\n        \"\"\"Builds ULM vocabulary and probabilities from substrings.\"\"\"\n        if not sequence:\n            return {}, {}\n            \n        vocab_counts = collections.Counter()\n        total_substrings = 0\n        for length in range(1, l_max + 1):\n            for i in range(len(sequence) - length + 1):\n                substring = sequence[i:i+length]\n                vocab_counts[substring] += 1\n                total_substrings += 1\n        return vocab_counts, total_substrings\n\n    def ulm_tokenizer(sequence, probs, vocab, l_max):\n        \"\"\"Segments a sequence using ULM via dynamic programming.\"\"\"\n        n = len(sequence)\n        costs = {token: -log2(p) for token, p in probs.items()}\n        \n        dp = [np.inf] * (n + 1)\n        backpointers = [-1] * (n + 1)\n        dp[0] = 0\n        \n        for i in range(1, n + 1):\n            for j in range(1, min(i, l_max) + 1):\n                sub = sequence[i-j:i]\n                if sub in costs:\n                    cost = dp[i-j] + costs[sub]\n                    if cost  dp[i]:\n                        dp[i] = cost\n                        backpointers[i] = i-j\n\n        if np.isinf(dp[n]):\n             return [], [], np.inf\n        \n        tokens = []\n        indices = []\n        end = n\n        while end > 0:\n            start = backpointers[end]\n            tokens.append(sequence[start:end])\n            indices.append(start)\n            end = start\n        \n        tokens.reverse()\n        indices.reverse()\n        return tokens, indices, dp[n]\n\n    def calculate_error(tokenization, start_indices, s, m, r, seq_len):\n        \"\"\"Calculates the boundary misalignment error.\"\"\"\n        if r  2:\n            return 0.0\n        \n        motif_boundaries = {s + k * m for k in range(1, r)}\n        \n        first_token_idx_in_seg = -1\n        region_start, region_end = s, s + r * m\n        \n        for i, token_start in enumerate(start_indices):\n            token_len = len(tokenization[i])\n            if token_start  region_end and token_start + token_len > region_start:\n                first_token_idx_in_seg = i\n                break\n        \n        token_boundaries = set()\n        if first_token_idx_in_seg != -1:\n            for i in range(first_token_idx_in_seg + 1, len(start_indices)):\n                token_start = start_indices[i]\n                if region_start = token_start  region_end:\n                    token_boundaries.add(token_start)\n                    \n        intersection_size = len(motif_boundaries.intersection(token_boundaries))\n        \n        return 1.0 - (intersection_size / len(motif_boundaries))\n\n\n    test_cases = [\n        {\"seq\": \"ACGTAC\" + \"ATG\" * 10 + \"GGAAC\", \"V_BPE\": 12, \"L_max\": 5, \"lambda\": 1, \"alpha\": 1, \"s\": 6, \"m\": 3, \"r\": 10},\n        {\"seq\": \"CG\" + \"A\" * 20 + \"TGC\", \"V_BPE\": 8, \"L_max\": 10, \"lambda\": 1, \"alpha\": 0.5, \"s\": 2, \"m\": 1, \"r\": 20},\n        {\"seq\": \"GCTATCGAGT\" + \"CGT\" * 3 + \"ACGTA\", \"V_BPE\": 6, \"L_max\": 4, \"lambda\": 1, \"alpha\": 1, \"s\": 10, \"m\": 3, \"r\": 3},\n        {\"seq\": \"AT\" + \"GC\" * 12 + \"TAAC\", \"V_BPE\": 10, \"L_max\": 4, \"lambda\": 1, \"alpha\": 0.1, \"s\": 2, \"m\": 2, \"r\": 12},\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        seq = case[\"seq\"]\n        n = len(seq)\n        v_bpe, l_max, lam, alpha = case[\"V_BPE\"], case[\"L_max\"], case[\"lambda\"], case[\"alpha\"]\n        s, m, r = case[\"s\"], case[\"m\"], case[\"r\"]\n\n        # --- BPE ---\n        bpe_tokens, _ = bpe_trainer(seq, v_bpe)\n        bpe_vocab = set(bpe_tokens) | set(list('ACGT')) # Ensure base vocab is included\n        \n        # In case merging reduces vocab size below V_BPE, use actual final size.\n        final_bpe_vocab_size = len(set(bpe_trainer(seq, v_bpe)[0]))\n        \n        bpe_counts = collections.Counter(bpe_tokens)\n        total_bpe_tokens = len(bpe_tokens)\n        \n        bpe_probs = {\n            token: (bpe_counts.get(token, 0) + lam) / (total_bpe_tokens + lam * final_bpe_vocab_size)\n            for token in set(bpe_tokens)\n        }\n        \n        l_bpe = sum(-log2(bpe_probs[token]) for token in bpe_tokens)\n        l_bar_bpe = l_bpe / n if n > 0 else 0\n        \n        bpe_start_indices = [0]\n        pos = 0\n        for token in bpe_tokens[:-1]:\n            pos += len(token)\n            bpe_start_indices.append(pos)\n        \n        e_bpe = calculate_error(bpe_tokens, bpe_start_indices, s, m, r, n)\n\n        # --- ULM ---\n        ulm_vocab_counts, total_substrings = ulm_trainer(seq, l_max)\n        ulm_vocab_size = len(ulm_vocab_counts)\n        denominator = total_substrings + alpha * ulm_vocab_size\n        ulm_probs = {\n            token: (count + alpha) / denominator\n            for token, count in ulm_vocab_counts.items()\n        }\n        \n        ulm_tokens, ulm_indices, l_ulm = ulm_tokenizer(seq, ulm_probs, ulm_vocab_counts, l_max)\n        l_bar_ulm = l_ulm / n if n > 0 else 0\n        \n        e_ulm = calculate_error(ulm_tokens, ulm_indices, s, m, r, n)\n        \n        # Round and append results\n        all_results.append([\n            round(l_bar_bpe, 6),\n            round(l_bar_ulm, 6),\n            round(e_bpe, 6),\n            round(e_ulm, 6)\n        ])\n\n    # Format final output string\n    result_str = \",\".join([str(res) for res in all_results])\n    print(f\"[{result_str.replace(' ', '')}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "双链DNA的正向链和其反向互补链编码着相同的生物信息，这是分子生物学的一个基本原则。因此，一个强大的基因组序列表示模型应当对序列的方向不敏感，即一个序列及其反向互补序列应产生相同或非常相似的嵌入向量。这种特性被称为“反向互补不变性”（reverse-complement invariance），是基因组模型一个至关重要的归纳偏置。本练习旨在让您设计并实现一个定量指标，用于检验和衡量嵌入函数在多大程度上满足这一不变性。通过将您设计的指标应用于几种假设的嵌入函数，您将学会如何在一个机器学习框架内，对基本的生物学原理进行形式化定义和验证。",
            "id": "4606981",
            "problem": "您正在自监督学习（SSL）的设定下，分析脱氧核糖核酸（DNA）序列的学习表示。在双链DNA中，碱基配对是明确定义的：沃森-克里克互补关系将 $A \\leftrightarrow T$ 和 $C \\leftrightarrow G$ 进行映射。给定一个有限字母表 $\\Sigma = \\{A,C,G,T\\}$，定义互补算子 $c:\\Sigma \\to \\Sigma$ 为 $c(A)=T$、$c(T)=A$、$c(C)=G$ 和 $c(G)=C$。对于一个序列 $s = (s_1,s_2,\\dots,s_L) \\in \\Sigma^L$，定义反向互补算子 $R$ 为 $R(s) = (c(s_L), c(s_{L-1}), \\dots, c(s_1))$。在许多基因组学任务中，一个学习到的嵌入函数 $f:\\Sigma^* \\to \\mathbb{R}^d$ 应当具有反向互补不变性，即对于内容等效的序列，无论其方向如何，$f(s) = f(R(s))$ 都成立。您必须提出一个能够定量验证反向互补不变性的测试，并设计一个度量标准来惩罚序列翻转下的不对称性。\n\n从双链DNA通过碱基互补性在两个方向上编码相同生物信息，以及互补碱基对为 $A \\leftrightarrow T$ 和 $C \\leftrightarrow G$ 这一基本原理出发，推导一个有原则的算法测试和一个适用于通过自监督目标学习到的 $\\mathbb{R}^d$ 嵌入的定量度量。您的度量必须对任意有限集合 $S \\subset \\Sigma^*$ 和任意嵌入函数 $f$ 满足以下性质：\n- 非负性：$M(f,S) \\ge 0$。\n- 关于反向互补不变性的不可辨识者同一性：$M(f,S) = 0$ 当且仅当对于所有 $s \\in S$，$f(s) = f(R(s))$。\n- 尺度不变性：对于任意标量 $\\alpha  0$，将 $f$ 替换为 $\\alpha f$ 不会改变该度量的值。\n- 跨异构序列长度的稳健聚合，除了度量设计中隐含的缩放外，不需要对每个输入序列进行显式重新缩放。\n\n您还必须定义一个决策规则，在给定一个非负阈值 $\\tau$ 的情况下，判定嵌入函数 $f$ 在集合 $S$ 上是否具有反向互补不变性。\n\n为以下测试套件实现您的度量和测试。每个测试用例包含一个特定的嵌入函数、一个序列集和一个阈值。嵌入函数的数学定义如下；令 $L$ 表示序列长度，位置由 $p=1,\\dots,L$ 索引：\n1. 嵌入函数 $f_{\\mathrm{inv}}:\\Sigma^* \\to \\mathbb{R}^2$（设计为具有反向互补不变性）：\n   - 定义 $\\phi_{\\mathrm{inv}}:\\Sigma \\to \\mathbb{R}^2$ 为 $\\phi_{\\mathrm{inv}}(A)=(1,0)$、$\\phi_{\\mathrm{inv}}(T)=(1,0)$、$\\phi_{\\mathrm{inv}}(C)=(0,1)$ 和 $\\phi_{\\mathrm{inv}}(G)=(0,1)$。\n   - 对于 $s=(s_1,\\dots,s_L)$，定义 $f_{\\mathrm{inv}}(s) = \\frac{1}{L}\\sum_{p=1}^{L} \\phi_{\\mathrm{inv}}(s_p)$。\n2. 嵌入函数 $f_{\\mathrm{sens}}:\\Sigma^* \\to \\mathbb{R}^3$（设计为方向敏感）：\n   - 定义 $\\phi_{\\mathrm{sens}}:\\Sigma \\to \\mathbb{R}^3$ 为 $\\phi_{\\mathrm{sens}}(A)=(1.0,0.5,0.2)$、$\\phi_{\\mathrm{sens}}(C)=(0.1,1.0,0.3)$、$\\phi_{\\mathrm{sens}}(G)=(0.3,0.2,1.0)$ 和 $\\phi_{\\mathrm{sens}}(T)=(1.2,0.8,0.4)$。\n   - 定义一个位置依赖权重 $w(p)=1+0.4(p-1)$ 和一个位置编码向量 $\\psi(p,L)=(0.05p,\\,0.02(L+1-p),\\,0.03)$。\n   - 对于 $s=(s_1,\\dots,s_L)$，定义 $f_{\\mathrm{sens}}(s) = \\frac{1}{L}\\sum_{p=1}^{L} \\big(w(p)\\,\\phi_{\\mathrm{sens}}(s_p) + \\psi(p,L)\\big)$。\n3. 嵌入函数 $f_{\\mathrm{part}}:\\Sigma^* \\to \\mathbb{R}^2$（设计为部分不变但具有微小方向敏感性）：\n   - 定义 $\\phi_{\\mathrm{part}}:\\Sigma \\to \\mathbb{R}^2$ 为 $\\phi_{\\mathrm{part}}(A)=(1,0)$、$\\phi_{\\mathrm{part}}(T)=(1,0)$、$\\phi_{\\mathrm{part}}(C)=(0,1)$ 和 $\\phi_{\\mathrm{part}}(G)=(0,1)$。\n   - 定义标量系数 $c:\\Sigma \\to \\mathbb{R}$ 为 $c(A)=1.0$、$c(T)=-1.0$、$c(C)=0.5$ 和 $c(G)=-0.5$，并令 $\\varepsilon=0.02$。\n   - 对于 $s=(s_1,\\dots,s_L)$，定义 $g(s)=\\sum_{p=1}^{L} c(s_p)\\,\\frac{p}{L}$ 和 $v_{\\mathrm{noise}}(s)=\\varepsilon\\,\\frac{g(s)}{L}\\,(1,-1)$，然后 $f_{\\mathrm{part}}(s)=\\frac{1}{L}\\sum_{p=1}^{L}\\phi_{\\mathrm{part}}(s_p) + v_{\\mathrm{noise}}(s)$。\n\n使用以下序列集：\n- $S_{\\mathrm{gen}} = \\{\\text{\"ACGTAC\"}, \\text{\"TTGCA\"}, \\text{\"CGAT\"}, \\text{\"GATTACA\"}, \\text{\"CCGGTTAA\"}\\}$。\n- $S_{\\mathrm{pal}} = \\{\\text{\"AT\"}, \\text{\"GC\"}, \\text{\"AGCT\"}, \\text{\"AATT\"}, \\text{\"CGCG\"}\\}$。\n- $S_{\\mathrm{sing}} = \\{\\text{\"A\"}, \\text{\"C\"}, \\text{\"G\"}, \\text{\"T\"}\\}$。\n- $S_{\\mathrm{long}} = \\{\\text{\"ACGTACGTACGTACGTACGT\"}\\}$。\n\n按以下顺序将测试用例定义为有序三元组 $(f,S,\\tau)$：\n- 用例 1: $(f_{\\mathrm{inv}}, S_{\\mathrm{gen}}, 10^{-12})$。\n- 用例 2: $(f_{\\mathrm{sens}}, S_{\\mathrm{gen}}, 10^{-1})$。\n- 用例 3: $(f_{\\mathrm{part}}, S_{\\mathrm{long}}, 5\\times 10^{-2})$。\n- 用例 4: $(f_{\\mathrm{inv}}, S_{\\mathrm{pal}}, 10^{-12})$。\n- 用例 5: $(f_{\\mathrm{sens}}, S_{\\mathrm{sing}}, 2\\times 10^{-1})$。\n\n您的程序必须：\n- 实现一个与上述定义一致的反向互补算子 $R$。\n- 完全按照规定实现三个嵌入函数。\n- 设计并计算一个满足上述四个性质的定量不对称性度量 $M(f,S)$。\n- 实现一个决策规则，返回一个布尔值，指示是否满足 $M(f,S) \\le \\tau$。\n- 按指定顺序生成五个用例的结果。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。每个结果必须是一个包含两项的列表，一项是度量值（四舍五入到六位小数），另一项是布尔决策，例如 $[m,\\mathrm{True}]$ 或 $[m,\\mathrm{False}]$。因此，最终输出行必须具有格式 $[[m_1,d_1],[m_2,d_2],[m_3,d_3],[m_4,d_4],[m_5,d_5]]$，其中每个 $m_i$ 是一个四舍五-入到六位小数的浮点数，每个 $d_i$ 是一个布尔值。",
            "solution": "该问题要求设计一个定量测试，以验证一个学习到的脱氧核糖核酸（DNA）序列嵌入函数是否表现出反向互补不变性。这是基因组学中的一个关键属性，因为双链DNA分子在其正向链和反向互补链中携带相同的生物信息。我们必须为嵌入函数 $f$ 在序列集 $S$ 上推导出一个有原则的度量 $M(f,S)$，以及一个相关的决策规则。\n\n对于嵌入函数 $f:\\Sigma^* \\to \\mathbb{R}^d$ 而言，反向互补不变性的基本原则是，对于任何序列 $s \\in \\Sigma^*$，其嵌入应与其反向互补序列（表示为 $R(s)$）的嵌入相同。在数学上，这表示为等式 $f(s) = f(R(s))$。任务是量化与此等式的偏差。\n\n我们的第一步是为单个序列 $s$ 定义一个不对称性度量。一个自然的起点是向量差 $v_{\\mathrm{diff}}(s) = f(s) - f(R(s))$。在一个不变的嵌入中，$v_{\\mathrm{diff}}(s) = \\vec{0}$。这个差值的大小可以通过向量范数来衡量，例如欧几里得范数 $\\|v_{\\mathrm{diff}}(s)\\|_2 = \\|f(s) - f(R(s))\\|_2$。这个量是非负的，并且当且仅当 $f(s) = f(R(s))$ 时为零，满足了针对单个序列所需性质中的两个。\n\n然而，问题要求该度量具有尺度不变性。如果我们将嵌入函数按因子 $\\alpha  0$ 缩放，得到一个新函数 $f' = \\alpha f$，则差值的范数变为 $\\|\\alpha f(s) - \\alpha f(R(s))\\|_2 = \\alpha \\|f(s) - f(R(s))\\|_2$。这不具有尺度不变性。为了解决这个问题，我们必须对差值进行归一化。创建两个向量 $u$ 和 $v$ 之间尺度不变的相对差异度量的一种标准方法是，用它们差值的范数除以它们范数的总和。这引出了我们对每条序列的不对称性度量 $m(s, f)$ 的定义：\n$$\nm(s, f) = \\frac{\\|f(s) - f(R(s))\\|_2}{\\|f(s)\\|_2 + \\|f(R(s))\\|_2}\n$$\n在 $f(s)$ 和 $f(R(s))$ 都是零向量的特殊情况下，它们是相等的，表达式变为 $\\frac{0}{0}$。在这种情况下，我们定义 $m(s, f) = 0$，这与完全不变性的条件一致。\n\n这个每序列度量 $m(s, f)$ 满足针对单个序列所需的性质：\n1.  **非负性**：由于范数是非负的，$m(s, f) \\ge 0$。\n2.  **不可辨识者同一性**：$m(s,f) = 0$ 当且仅当 $\\|f(s) - f(R(s))\\|_2 = 0$，这当且仅当 $f(s) = f(R(s))$ 时成立（假设分母不为零，或按定义处理零向量情况）。\n3.  **尺度不变性**：对于 $\\alpha  0$ 和 $f' = \\alpha f$，\n    $$\n    m(s, f') = \\frac{\\|\\alpha f(s) - \\alpha f(R(s))\\|_2}{\\|\\alpha f(s)\\|_2 + \\|\\alpha f(R(s))\\|_2} = \\frac{\\alpha \\|f(s) - f(R(s))\\|_2}{\\alpha \\left( \\|f(s)\\|_2 + \\|f(R(s))\\|_2 \\right)} = m(s, f)\n    $$\n    该性质成立。\n\n接下来，我们必须将这些每序列分数在一个有限序列集 $S \\subset \\Sigma^*$ 上进行聚合，以形成最终的度量 $M(f,S)$。问题要求在异构长度的序列上进行稳健的聚合。对归一化后的每序列分数求平均是一种合适的方法，因为每个分数 $m(s,f)$ 已经是介于 0 和 1 之间的无量纲量（与余弦距离相关）。这可以防止产生大范数嵌入的序列对总分产生不成比例的影响。因此，我们将聚合度量定义为每序列分数的算术平均值：\n$$\nM(f,S) = \\frac{1}{|S|} \\sum_{s \\in S} m(s, f) = \\frac{1}{|S|} \\sum_{s \\in S} \\frac{\\|f(s) - f(R(s))\\|_2}{\\|f(s)\\|_2 + \\|f(R(s))\\|_2}\n$$\n这个聚合度量 $M(f,S)$ 继承了所需的性质：它非负、尺度不变，并且当且仅当对所有 $s \\in S$ 都有 $f(s) = f(R(s))$ 时等于零。通过平均进行聚合提供了所要求的稳健性。\n\n最后，我们定义算法测试和决策规则。该测试包括为给定的函数 $f$ 和序列集 $S$ 计算 $M(f,S)$ 的值。给定一个非负阈值 $\\tau$，决策规则如下：\n- 如果 $M(f,S) \\le \\tau$，则声明嵌入函数 $f$ 在集合 $S$ 上是**反向互补不变的**。\n- 否则，声明它在 $S$ 上**不是反向互补不变的**。\n\n这个框架提供了一个完整的、有原则的、定量的方法，用于评估问题所指定的DNA序列嵌入的反向互补不变性。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run the analysis, and print results.\n    \"\"\"\n    \n    # ------------------ Operators and Helper Maps ------------------\n\n    def reverse_complement(s: str) -> str:\n        \"\"\"Computes the reverse complement of a DNA sequence.\"\"\"\n        complement_map = str.maketrans('ACGT', 'TGCA')\n        return s.translate(complement_map)[::-1]\n\n    # Maps for embedding function definitions\n    PHI_INV_MAP = {\n        'A': np.array([1.0, 0.0]), 'T': np.array([1.0, 0.0]),\n        'C': np.array([0.0, 1.0]), 'G': np.array([0.0, 1.0]),\n    }\n    PHI_SENS_MAP = {\n        'A': np.array([1.0, 0.5, 0.2]), 'C': np.array([0.1, 1.0, 0.3]),\n        'G': np.array([0.3, 0.2, 1.0]), 'T': np.array([1.2, 0.8, 0.4]),\n    }\n    PHI_PART_MAP = PHI_INV_MAP # Base is the same as f_inv\n    COEFF_PART_MAP = {'A': 1.0, 'T': -1.0, 'C': 0.5, 'G': -0.5}\n\n    # ------------------ Embedding Functions ------------------\n\n    def f_inv(s: str) -> np.ndarray:\n        \"\"\"Invariant embedding function.\"\"\"\n        L = len(s)\n        if L == 0:\n            return np.zeros(2)\n        \n        total = np.zeros(2)\n        for base in s:\n            total += PHI_INV_MAP[base]\n        \n        return total / L\n\n    def f_sens(s: str) -> np.ndarray:\n        \"\"\"Orientation-sensitive embedding function.\"\"\"\n        L = len(s)\n        if L == 0:\n            return np.zeros(3)\n\n        total = np.zeros(3)\n        for p_one_based in range(1, L + 1):\n            idx = p_one_based - 1\n            base = s[idx]\n            \n            w_p = 1.0 + 0.4 * (p_one_based - 1)\n            psi_p = np.array([0.05 * p_one_based, 0.02 * (L + 1 - p_one_based), 0.03])\n            \n            term = w_p * PHI_SENS_MAP[base] + psi_p\n            total += term\n\n        return total / L\n\n    def f_part(s: str) -> np.ndarray:\n        \"\"\"Partially invariant embedding function.\"\"\"\n        L = len(s)\n        if L == 0:\n            return np.zeros(2)\n\n        # Invariant part\n        f_base_total = np.zeros(2)\n        for base in s:\n            f_base_total += PHI_PART_MAP[base]\n        f_base = f_base_total / L\n\n        # Noise part\n        epsilon = 0.02\n        g_s = 0.0\n        for p_one_based in range(1, L + 1):\n            idx = p_one_based - 1\n            base = s[idx]\n            g_s += COEFF_PART_MAP[base] * (p_one_based / L)\n        \n        v_noise_vec = np.array([1.0, -1.0])\n        v_noise = epsilon * (g_s / L) * v_noise_vec\n        \n        return f_base + v_noise\n\n    # ------------------ Metric and Test Implementation ------------------\n\n    def calculate_metric(f, S: list[str]) -> float:\n        \"\"\"Calculates the asymmetry metric M(f, S).\"\"\"\n        if not S:\n            return 0.0\n        \n        total_metric = 0.0\n        for s in S:\n            rs = reverse_complement(s)\n            \n            # For palindromic sequences, s == rs, so metric contribution is 0\n            if s == rs:\n                continue\n\n            v_s = f(s)\n            v_rs = f(rs)\n            \n            norm_s = np.linalg.norm(v_s)\n            norm_rs = np.linalg.norm(v_rs)\n            \n            denominator = norm_s + norm_rs\n            \n            if denominator == 0.0:\n                # If both norms are 0, vectors are equal (both zero), so diff is 0\n                seq_metric = 0.0\n            else:\n                diff_norm = np.linalg.norm(v_s - v_rs)\n                seq_metric = diff_norm / denominator\n            \n            total_metric += seq_metric\n            \n        return total_metric / len(S)\n\n    # ------------------ Test Case Setup and Execution ------------------\n\n    # Sequence Sets\n    S_gen = [\"ACGTAC\", \"TTGCA\", \"CGAT\", \"GATTACA\", \"CCGGTTAA\"]\n    S_pal = [\"AT\", \"GC\", \"AGCT\", \"AATT\", \"CGCG\"]\n    S_sing = [\"A\", \"C\", \"G\", \"T\"]\n    S_long = [\"ACGTACGTACGTACGTACGT\"]\n\n    # Embedding function map\n    func_map = {\n        'f_inv': f_inv,\n        'f_sens': f_sens,\n        'f_part': f_part,\n    }\n    \n    # Sequence set map\n    set_map = {\n        'S_gen': S_gen,\n        'S_pal': S_pal,\n        'S_sing': S_sing,\n        'S_long': S_long,\n    }\n\n    # Test Cases: (function_name, set_name, threshold)\n    test_cases = [\n        ('f_inv', 'S_gen', 1e-12),\n        ('f_sens', 'S_gen', 1e-1),\n        ('f_part', 'S_long', 5e-2),\n        ('f_inv', 'S_pal', 1e-12),\n        ('f_sens', 'S_sing', 2e-1),\n    ]\n\n    results = []\n    for f_name, s_name, tau in test_cases:\n        f = func_map[f_name]\n        S = set_map[s_name]\n        \n        metric_value = calculate_metric(f, S)\n        decision = metric_value = tau\n        \n        rounded_metric = round(metric_value, 6)\n        results.append(f\"[{rounded_metric},{'True' if decision else 'False'}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "自监督模型能够从海量基因组数据中学习到复杂的模式，但其决策过程往往像一个“黑箱”，难以解释。为了信任并利用这些模型进行科学发现，我们需要理解它们做出预测的依据。特征归因方法，如积分梯度（Integrated Gradients, IG），能够将模型的特定输出追溯到对其贡献最大的输入特征——在这里，即单个的DNA碱基。这项实践将指导您从头开始实现积分梯度算法，以计算模型预测对输入核苷酸的贡献度。通过这种方式，您可以定量评估模型是否学会了识别生物学上相关的基序（motif），从而在模型性能和生物学可解释性之间建立起一座桥梁。",
            "id": "4606960",
            "problem": "给定一个简化的脱氧核糖核酸（DNA）序列掩码词元预测设置，其中长度为 $L$ 的序列（字母表为 $\\{A,C,G,T\\}$）在索引 $m$（从零开始）处有一个被掩盖的位置。考虑一个线性-softmax模型，它根据剩余的上下文来预测被掩盖的词元（即索引 $m$ 处的碱基）。该模型对输入序列的独热编码进行操作，其中被掩盖位置的向量被设置为零向量。设输入表示为一个向量 $x \\in \\mathbb{R}^{4L}$，它是通过按每个位置上固定的碱基顺序 $\\{A,C,G,T\\}$ 拼接独热向量得到的。该模型为对应于类别 $\\{A,C,G,T\\}$ 的 $k \\in \\{0,1,2,3\\}$ 定义了类别 logits 值 $z_k(x) = w_k^\\top x$，以及预测概率 $p_k(x) = \\exp(z_k(x)) / \\sum_{j=0}^{3} \\exp(z_j(x))$。权重的构造方式使得对应于碱基 A 的真实类别 $k=0$ 对掩码上游一个已知的生物基序有强烈的响应，而在其他地方响应较弱。\n\n您必须计算模型预测的掩码词元类别对输入碱基的积分梯度（Integrated Gradients, IG）归因，并评估归因是否与已知基序对齐。请使用以下定义和约束。\n\n1. 独热编码和基线：\n   - 对于每个位置 $j \\in \\{0,\\dots,L-1\\}$ 和碱基 $b \\in \\{A,C,G,T\\}$，令 $x_{j,b} \\in \\{0,1\\}$ 表示相应的独热分量，但在被掩盖的索引 $j=m$ 处，对所有 $b$ 设置 $x_{m,b} = 0$（掩码）。\n   - 基线是 $x' = \\mathbf{0} \\in \\mathbb{R}^{4L}$，直线路径为 $\\gamma(\\alpha) = x' + \\alpha (x-x') = \\alpha x$，其中 $\\alpha \\in [0,1]$。\n\n2. 模型构造：\n   - 在掩码紧邻上游的相对偏移量 $r \\in \\{-4,-3,-2,-1\\}$ 处固定一个长度为 4 的基序，其碱基分别为 $\\{T, A, T, A\\}$。对于任何在边界 $0 \\le j  L$ 内的位置 $j = m + r$，且碱基 $b$ 等于该偏移量处的基序碱基，为类别 A（类别索引 0）的权重向量中相应的特征分配一个大权重 $w$。对于所有类别的所有其他特征，分配一个小的正背景权重 $\\epsilon$。形式上，设 $M \\in \\mathbb{R}^{4 \\times 4L}$ 是权重矩阵，其行为 $M_k^\\top = w_k^\\top$：\n     - 对于类别 $k=0$（碱基 A），如果 $j=m+r$ 有效且 $b$ 等于偏移量 $r$ 处指定的基序碱基，则设置 $M_{0, (4j + \\text{idx}(b))} = w$；否则设置 $M_{0, (4j + \\text{idx}(b))} = \\epsilon$。\n     - 对于类别 $k \\in \\{1,2,3\\}$（碱基 C,G,T），对所有特征 $i$ 设置 $M_{k, i} = \\epsilon$。\n   - 使用碱基索引映射 $\\text{idx}(A)=0$，$\\text{idx}(C)=1$，$\\text{idx}(G)=2$，$\\text{idx}(T)=3$。\n\n3. 归因的预测目标：\n   - 设 $p(x) = \\text{softmax}(M x)$。确定预测类别 $c^\\star = \\arg\\max_{k \\in \\{0,1,2,3\\}} p_k(x)$。计算关于标量函数 $F(x) = p_{c^\\star}(x)$ 的积分梯度。\n\n4. 积分梯度定义：\n   - 对于每个特征 $i \\in \\{1,\\dots,4L\\}$，积分梯度定义为\n     $$\\mathrm{IG}_i(x) = (x_i - x_i') \\int_{0}^{1} \\frac{\\partial F(\\gamma(\\alpha))}{\\partial x_i} \\, d\\alpha,$$\n     您必须使用具有 $S$ 个步骤的黎曼和近似：\n     $$\\widehat{\\mathrm{IG}}_i(x) = (x_i - x_i') \\cdot \\frac{1}{S} \\sum_{s=1}^{S} \\left. \\frac{\\partial F(\\gamma(\\alpha))}{\\partial x_i} \\right|_{\\alpha = s/S}.$$\n\n5. 按位置归因和基序对齐分数：\n   - 通过对每个位置的四个通道的绝对值求和，将特征级归因聚合到位置级，即对于位置 $j$，\n     $$A_j = \\sum_{b \\in \\{A,C,G,T\\}} \\left| \\widehat{\\mathrm{IG}}_{4j + \\text{idx}(b)}(x) \\right|.$$\n   - 从总数中排除被掩盖的位置，因为其独热编码为零，并计算总归因\n     $$T = \\sum_{\\substack{j=0 \\\\ j \\ne m}}^{L-1} A_j.$$\n   - 给定一个以绝对位置表示的已标注基序区间列表 $[s, e)$（起始包含，结束不包含），将对齐分数定义为这些区间并集内的总归因分数：\n     $$\\text{score} = \\frac{\\sum_{[s,e)} \\sum_{j=s}^{e-1} A_j}{T}.$$\n\n6. 使用的数学和算法基础：\n   - 您必须从核心定义开始：独热编码、线性映射、softmax 函数以及上述积分梯度的定义。不要使用任何外部机器学习包或自动微分；从这些定义中解析地计算梯度。\n\n7. 所需数值：\n   - 使用 $w = 3.0$ 和 $\\epsilon = 0.05$。\n   - 使用指定的碱基顺序 $\\{A,C,G,T\\}$ 和索引映射。\n   - 对于每个案例，使用提供的 $S$作为黎曼和的步数。\n\n8. 测试套件：\n   实现程序以处理以下案例，每个案例由元组 $(\\text{sequence}, m, \\text{intervals}, S)$ 定义，其中序列长度 $L=20$：\n   - 案例 1（上游有清晰的基序）：\n     - 序列：\"GCCGACTATANGTCCAAGTT\"，其中索引 $m$ 处的字符 'N' 表示掩码输入。\n     - 掩码索引 $m = 10$。\n     - 标注区间：$[(6,10)]$。\n     - 步数 $S = 50$。\n   - 案例 2（出现两次基序，仅标注了近端的一个；离散化中使用最小步数的边界情况）：\n     - 序列：\"TATAGCTATANGCGTCAAGT\"。\n     - 掩码索引 $m = 10$。\n     - 标注区间：$[(6,10)]$。\n     - 步数 $S = 1$。\n   - 案例 3（没有近端基序）：\n     - 序列：\"GACGTCGCGCNATGCATAGC\"。\n     - 掩码索引 $m = 10$。\n     - 标注区间：$[(6,10)]$。\n     - 步数 $S = 50$。\n\n9. 程序输出：\n   - 您的程序应产生单行输出，其中包含三个案例的对齐分数，格式为方括号括起来的逗号分隔列表（例如，\"[r1,r2,r3]\"）。每个 $r_i$ 必须是浮点数。\n\n此问题不要求任何物理单位、角度或百分比。所有数学计算必须符合上述定义，所有输出必须是符合指定格式的数值浮点数。",
            "solution": "该问题要求为 DNA 序列上的线性-softmax 模型计算积分梯度（Integrated Gradients, IG），并根据得到的归因评估一个对齐分数。我将首先验证问题陈述，然后基于第一性原理提供一个分步解决方案。\n\n### 问题验证\n\n**步骤 1：提取给定信息**\n- **字母表**：$\\{A,C,G,T\\}$\n- **序列长度**：$L=20$\n- **输入**：独热编码向量 $x \\in \\mathbb{R}^{4L}$，被掩盖的位置 $m$ 表示为零向量。碱基顺序 $\\{A,C,G,T\\}$ 给出索引映射 $\\text{idx}(A)=0, \\text{idx}(C)=1, \\text{idx}(G)=2, \\text{idx}(T)=3$。\n- **模型**：线性-softmax 模型，logits 值为 $z_k(x) = w_k^\\top x$，概率为 $p_k(x) = \\text{softmax}(z)_k$。\n- **权重矩阵（$M$）**：$M_{k,i}$ 是类别 $k$ 和特征 $i$ 的权重。\n  - 基序：$\\{T,A,T,A\\}$，在相对于掩码的偏移量 $\\{-4,-3,-2,-1\\}$ 处。\n  - 类别 $k=0$ ('A')：如果 $j=m+r$（对于一个有效的基序偏移量 $r$）且 $b$ 是该偏移处的基序碱基，则 $M_{0, (4j + \\text{idx}(b))} = w$。否则，$M_{0,i} = \\epsilon$。\n  - 类别 $k \\in \\{1,2,3\\}$ ('C','G','T')：对所有特征 $i$，有 $M_{k,i} = \\epsilon$。\n- **数值常量**：$w=3.0$, $\\epsilon=0.05$。\n- **积分梯度（IG）**：\n  - 基线：$x' = \\mathbf{0}$。\n  - 路径：$\\gamma(\\alpha) = \\alpha x$，其中 $\\alpha \\in [0,1]$。\n  - 归因目标：$F(x) = p_{c^\\star}(x)$，其中 $c^\\star = \\arg\\max_k p_k(x)$。\n  - 黎曼和近似：$\\widehat{\\mathrm{IG}}_i(x) = x_i \\cdot \\frac{1}{S} \\sum_{s=1}^{S} \\left. \\frac{\\partial F(\\gamma(\\alpha))}{\\partial y_i} \\right|_{\\alpha = s/S}$。\n- **归因聚合与评分**：\n  - 按位置归因：$A_j = \\sum_{b \\in \\{A,C,G,T\\}} \\left| \\widehat{\\mathrm{IG}}_{4j + \\text{idx}(b)}(x) \\right|$。\n  - 总归因（不包括掩码）：$T = \\sum_{j=0, j \\ne m}^{L-1} A_j$。\n  - 对齐分数：$\\text{score} = (\\sum_{[s,e)} \\sum_{j=s}^{e-1} A_j) / T$。\n- **测试案例**：\n  - 案例 1：$(\\text{sequence} = \\text{\"GCCGACTATANGTCCAAGTT\"}, m=10, \\text{intervals}=[(6,10)], S=50)$\n  - 案例 2：$(\\text{sequence} = \\text{\"TATAGCTATANGCGTCAAGT\"}, m=10, \\text{intervals}=[(6,10)], S=1)$\n  - 案例 3：$(\\text{sequence} = \\text{\"GACGTCGCGCNATGCATAGC\"}, m=10, \\text{intervals}=[(6,10)], S=50)$\n\n**步骤 2：使用提取的给定信息进行验证**\n该问题在科学上是合理的，它使用了机器学习中已建立的概念（线性模型、softmax、积分梯度），并将其应用于标准的生物信息学任务（基序分析）。问题陈述良好，所有变量、函数和常量都有明确定义，从而为每个案例导出一个唯一的、可计算的解。语言客观而精确。不存在与科学合理性、可形式化性、完整性、可行性或结构相关的问题。该问题是一个非平凡的计算任务，需要仔细实现解析梯度和数值积分。\n\n**步骤 3：结论和行动**\n问题有效。我将继续提供完整解决方案。\n\n### 解决方案\n\n解决方案分为五个阶段：形式化模型和输入、推导 IG 所需的解析梯度、指定 IG 计算、定义归因聚合以及分析每个测试案例。\n\n**1. 模型和输入的形式化**\n输入是一个长度为 $L=20$ 的 DNA 序列，它被转换为一个独热向量 $x \\in \\mathbb{R}^{80}$。对于一个非掩码位置 $j$ 处的碱基 $b$，分量 $x_{4j + \\text{idx}(b)} = 1$，而该位置的所有其他分量为 $0$。在掩码位置 $m$ 处，所有四个分量都为 $0$。\n\n模型由权重矩阵 $M \\in \\mathbb{R}^{4 \\times 80}$ 定义。问题规定了其构造方式：\n- 对于行 $k \\in \\{1,2,3\\}$，对应于类别 'C', 'G', 'T'，所有权重均为 $\\epsilon = 0.05$。因此，对于 $k>0$，有 $M_{k,i} = \\epsilon$。\n- 对于行 $k=0$，对应于类别 'A'，对于匹配基序 $\\{T,A,T,A\\}$（位于位置 $\\{m-4, m-3, m-2, m-1\\}$）的特征，权重为 $w=3.0$。对于此行中的所有其他特征，权重为 $\\epsilon$。设 $I_{motif}$ 是对应于此基序的特征索引集。那么，如果 $i \\in I_{motif}$，则 $M_{0,i} = w$；如果 $i \\notin I_{motif}$，则 $M_{0,i} = \\epsilon$。\n\nlogits 值为 $z = Mx$，概率为 $p = \\text{softmax}(z)$。\n\n**2. 解析梯度推导**\nIG 计算的核心是目标函数 $F(y) = p_{c^\\star}(y)$ 相对于其输入向量 $y$ 的梯度。这里 $y$ 将是插值输入 $\\gamma(\\alpha)$。预测类别 $c^\\star = \\arg\\max_k p_k(x)$ 是根据原始输入 $x$ 确定的，并在整个 IG 计算过程中保持不变。\n\n使用链式法则，概率 $p_k$ 对输入特征 $y_i$ 的导数为：\n$$ \\frac{\\partial p_k(y)}{\\partial y_i} = \\sum_{l=0}^{3} \\frac{\\partial p_k(y)}{\\partial z_l(y)} \\frac{\\partial z_l(y)}{\\partial y_i} $$\nsoftmax 函数的雅可比矩阵为 $\\frac{\\partial p_k}{\\partial z_l} = p_k (\\delta_{kl} - p_l)$，其中 $\\delta_{kl}$ 是克罗内克 delta。线性层的导数为 $\\frac{\\partial z_l}{\\partial y_i} = M_{li}$。\n代入这些得到：\n$$ \\frac{\\partial p_k(y)}{\\partial y_i} = \\sum_{l=0}^{3} p_k(y)(\\delta_{kl} - p_l(y)) M_{li} = p_k(y) \\left( M_{ki} - \\sum_{l=0}^{3} p_l(y) M_{li} \\right) $$\n我们来分析一下 $E_i(y) = \\sum_{l=0}^{3} p_l(y) M_{li}$ 这一项。这是向量 $p(y)^\\top M$ 的第 $i$ 个分量。考虑到我们特定的权重矩阵 $M$，其中对于 $l>0$ 有 $M_{li} = \\epsilon$，我们可以简化 $E_i(y)$：\n$$ E_i(y) = p_0(y)M_{0i} + p_1(y)M_{1i} + p_2(y)M_{2i} + p_3(y)M_{3i} = p_0(y)M_{0i} + (p_1(y)+p_2(y)+p_3(y))\\epsilon $$\n因为 $\\sum_l p_l(y) = 1$，我们有 $p_1+p_2+p_3 = 1-p_0$。\n$$ E_i(y) = p_0(y)M_{0i} + (1-p_0(y))\\epsilon $$\n我们感兴趣的是类别 $c^\\star$ 的梯度。如果 $c^\\star=0$，梯度为：\n$$ \\frac{\\partial p_0(y)}{\\partial y_i} = p_0(y) (M_{0i} - E_i(y)) = p_0(y)(M_{0i} - [p_0(y)M_{0i} + (1-p_0(y))\\epsilon]) $$\n$$ \\frac{\\partial p_0(y)}{\\partial y_i} = p_0(y)(M_{0i}(1-p_0(y)) - \\epsilon(1-p_0(y))) = p_0(y)(1-p_0(y))(M_{0i} - \\epsilon) $$\n这是一个关键的简化。类别 'A' 关于特征 $i$ 的梯度与 $(M_{0i} - \\epsilon)$ 成正比。\n如果特征 $i$ 不属于 'A' 类基序的一部分，则 $M_{0i}=\\epsilon$，梯度为零。\n如果特征 $i$ 是基序的一部分，则 $M_{0i}=w$，梯度为 $p_0(y)(1-p_0(y))(w - \\epsilon)$。\n\n**3. 积分梯度计算**\nIG 的黎曼和近似如下：\n$$ \\widehat{\\mathrm{IG}}_i(x) = x_i \\cdot \\frac{1}{S} \\sum_{s=1}^{S} \\left. \\frac{\\partial p_{c^\\star}(y)}{\\partial y_i} \\right|_{y = \\gamma(\\alpha_s)} \\quad \\text{其中 } \\alpha_s = s/S $$\n梯度项在插值输入 $y = \\gamma(\\alpha_s) = \\alpha_s x$ 处求值。此时的 logits 值为 $z(\\alpha_s) = M(\\alpha_s x) = \\alpha_s z$，其中 $z=Mx$ 是原始输入的 logits 值。概率为 $p(\\alpha_s) = \\text{softmax}(\\alpha_s z)$。\n\n**4. 归因聚合与评分**\n特征级归因 $\\widehat{\\mathrm{IG}}_i$ 通过对每个位置 $j$ 处四个特征归因的绝对值求和，聚合成位置级归因 $A_j$：\n$$ A_j = \\sum_{b=0}^{3} |\\widehat{\\mathrm{IG}}_{4j+b}| $$\n总归因 $T$ 是所有非掩码位置上 $A_j$ 的和。最终的对齐分数是在标注的基序区间内的归因占总归因的比例。\n\n**5. 测试案例分析**\n\n- **案例 1：`seq=\"GCCGACTATANGTCCAAGTT\"`，`m=10`，`intervals=[(6,10)]`，`S=50`**\n  相对于掩码 $m=10$ 的基序位置是 $\\{6,7,8,9\\}$。序列在这些位置上有 `TATA`，与模型 'A' 类权重敏感的基序 $\\{T,A,T,A\\}$ 完全匹配。\n  输入向量 $x$ 对于序列中的碱基，将有 $x_{4j+\\text{idx}(b)}=1$。具体来说，对于基序位置，我们有 $x_{27}=1$ (`T` 在 6), $x_{28}=1$ (`A` 在 7), $x_{35}=1$ (`T` 在 8), $x_{36}=1$ (`A` 在 9)。\n  logits 值为 $z=Mx$。对于 $k=0$，$z_0 = 4w + (19-4)\\epsilon = 4(3.0) + 15(0.05) = 12.75$。对于 $k>0$，$z_k=19\\epsilon=0.95$。\n  显然，$z_0$ 是最大的，所以预测类别是 $c^\\star=0$。\n  我们使用 $p_0$ 的梯度公式。梯度 $\\frac{\\partial p_0}{\\partial y_i}$ 仅在 $M_{0i} \\ne \\epsilon$ 时非零，这只发生在四个基序特征 $i \\in I_{motif}$ 上。\n  IG 归因 $\\widehat{\\mathrm{IG}}_i$ 仅在 $x_i=1$ 且梯度非零时才非零。这两个条件对于四个基序特征都满足，因为序列与基序匹配。对于所有其他特征，要么梯度为零，要么 $x_i$ 为零。\n  因此，归因仅在对应于位置 $\\{6,7,8,9\\}$ 的特征上非零。位置级归因 $A_j$ 仅在 $j \\in \\{6,7,8,9\\}$ 时非零。\n  总归因是 $T = A_6+A_7+A_8+A_9$。\n  标注的区间是 $[6,10)$，覆盖了位置 $\\{6,7,8,9\\}$。此区间内的归因也是 $A_6+A_7+A_8+A_9$。\n  分数是 $(A_6+A_7+A_8+A_9) / (A_6+A_7+A_8+A_9) = 1.0$。\n\n- **案例 2：`seq=\"TATAGCTATANGCGTCAAGT\"`，`m=10`，`intervals=[(6,10)]`，`S=1`**\n  该序列在位置 $\\{6,7,8,9\\}$ 处同样有一个完美的 `TATA` 基序。分析与案例 1 相同。预测类别是 $c^\\star=0$。所有归因都局限于位置 $\\{6,7,8,9\\}$。标注的区间是 $[6,10)$。因此分数为 $1.0$。在这个高度结构化的问题中，步数 $S$ 会影响归因值的大小，但不会影响它们的比率。\n\n- **案例 3：`seq=\"GACGTCGCGCNATGCATAGC\"`，`m=10`，`intervals=[(6,10)]`，`S=50`**\n  在基序敏感位置 $\\{6,7,8,9\\}$ 的序列是 `GCGC`。这与 `TATA` 基序不匹配。\n  在计算 logits 值 $z=Mx$ 时，没有输入特征 $x_i=1$ 与特殊权重 $M_{0,i}=w$ 对齐。\n  因此，对于所有 $k \\in \\{0,1,2,3\\}$，logits 值是 $19$ 个 $\\epsilon \\cdot 1$ 项的和，所以 $z_k=19\\epsilon=0.95$。\n  所有 logits 值都相等。概率为 $p_k=0.25$ 对所有 $k$。按照惯例（例如 `numpy.argmax`），预测类别是第一个，即 $c^\\star=0$。\n  我们再次计算 $p_0$ 的 IG。梯度公式 $\\frac{\\partial p_0}{\\partial y_i} = p_0(y)(1-p_0(y))(M_{0i} - \\epsilon)$ 仍然有效。\n  归因 $\\widehat{\\mathrm{IG}}_i$ 要求 $x_i=1$ 和 $M_{0i} \\ne \\epsilon$ 同时成立。$M_{0i}=w$ 的特征集与输入中激活的特征集（$x_i=1$）是不相交的。对于输入中激活的任何特征 $i$（$x_i=1$），其对应的权重是 $M_{0i}=\\epsilon$，这使得梯度项 $(M_{0i}-\\epsilon)$ 为零。\n  因此，对所有 $i$，$\\widehat{\\mathrm{IG}}_i(x) = 0$。所有位置级归因 $A_j$ 都为零。总归因 $T=0$。\n  分数是 $\\frac{0}{0}$，逻辑上解释为 $0.0$，因为基序区域内的归因为零。\n\n最终预测分数：$[1.0, 1.0, 0.0]$。",
            "answer": "```python\nimport numpy as np\nfrom scipy.special import softmax\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    \n    # Test cases as defined in the problem statement\n    test_cases = [\n        # (sequence, mask_index, annotated_intervals, num_steps)\n        (\"GCCGACTATANGTCCAAGTT\", 10, [(6, 10)], 50),\n        (\"TATAGCTATANGCGTCAAGT\", 10, [(6, 10)], 1),\n        (\"GACGTCGCGCNATGCATAGC\", 10, [(6, 10)], 50),\n    ]\n\n    results = []\n    for case in test_cases:\n        score = calculate_score(*case)\n        results.append(score)\n\n    # Format the output as a comma-separated list in brackets\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef calculate_score(sequence, m, intervals, S):\n    \"\"\"\n    Calculates the motif alignment score for a single test case.\n    \"\"\"\n    # 1. Define constants and mappings\n    L = 20\n    w = 3.0\n    epsilon = 0.05\n    char_to_idx = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n    num_features = 4 * L\n    \n    # 2. Build one-hot encoded input vector x\n    x = np.zeros(num_features)\n    for j, char in enumerate(sequence):\n        if j != m and char in char_to_idx:\n            base_idx = char_to_idx[char]\n            x[4 * j + base_idx] = 1\n            \n    # 3. Build weight matrix M\n    M = np.full((4, num_features), epsilon)\n    motif_bases = {'T': -4, 'A': -3, 'T': -2, 'A': -1}\n    for base_char, offset in motif_bases.items():\n        pos = m + offset\n        if 0 = pos  L:\n            base_idx = char_to_idx[base_char]\n            feature_idx = 4 * pos + base_idx\n            M[0, feature_idx] = w  # Class 0 is for 'A'\n            \n    # 4. Determine the predicted class c_star for the input x\n    z = M @ x\n    p = softmax(z)\n    c_star = np.argmax(p)\n    \n    # 5. Compute Integrated Gradients using Riemann sum\n    sum_of_grads = np.zeros(num_features)\n    for s in range(1, S + 1):\n        alpha = s / S\n        \n        # Calculate logits and probabilities for the interpolated input\n        z_alpha = alpha * z\n        p_alpha = softmax(z_alpha)\n        \n        # Calculate the gradient of the predicted class probability w.r.t. the interpolated input\n        # This is the vector form of the analytical gradient:\n        # grad_i = p_alpha[c_star] * (M[c_star, i] - sum_l(p_alpha[l] * M[l, i]))\n        expected_weights = p_alpha @ M\n        grad_at_alpha = p_alpha[c_star] * (M[c_star, :] - expected_weights)\n        \n        sum_of_grads += grad_at_alpha\n\n    # Final IG is the average gradient multiplied by the input feature value\n    ig = x * (sum_of_grads / S)\n    \n    # 6. Aggregate feature attributions to position-wise attributions A_j\n    # Reshape to (L, 4) and sum absolute values over the base channels\n    A = np.sum(np.abs(ig.reshape((L, 4))), axis=1)\n\n    # 7. Compute the final alignment score\n    total_attribution = 0.0\n    motif_attribution = 0.0\n    \n    # Create a set of indices for annotated regions for efficient lookup\n    annotated_indices = set()\n    for start, end in intervals:\n        annotated_indices.update(range(start, end))\n\n    for j in range(L):\n        if j != m: # Exclude masked position from total attribution\n            total_attribution += A[j]\n            if j in annotated_indices:\n                motif_attribution += A[j]\n                \n    if total_attribution == 0:\n        score = 0.0\n    else:\n        score = motif_attribution / total_attribution\n        \n    return score\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}