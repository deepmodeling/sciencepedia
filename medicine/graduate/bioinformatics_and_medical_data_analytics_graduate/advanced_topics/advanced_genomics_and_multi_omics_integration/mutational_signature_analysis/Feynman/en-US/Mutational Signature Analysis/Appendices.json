{
    "hands_on_practices": [
        {
            "introduction": "At the core of mutational signature analysis lies the mathematical framework of Non-negative Matrix Factorization (NMF). This technique deconstructs a complex mutation catalogue, represented as a matrix $V$, into two simpler matrices: $W$, containing the fundamental mutational signatures, and $H$, describing the activity or \"exposure\" of these signatures in each sample. This exercise provides a foundational, hands-on opportunity to perform the core NMF calculation, reconstructing an approximate catalogue $WH$ from given signatures and exposures, and to quantify the goodness-of-fit using the Frobenius norm—a standard measure of reconstruction error .",
            "id": "4587886",
            "problem": "In mutational signature analysis within bioinformatics and medical data analytics, a non-negative mutation catalogue matrix $V \\in \\mathbb{R}_{\\ge 0}^{m \\times n}$ is approximated as the product $W H$ using Non-negative Matrix Factorization (NMF), where $W \\in \\mathbb{R}_{\\ge 0}^{m \\times r}$ encodes $r$ latent mutational signatures as probability distributions over $m$ mutation channels and $H \\in \\mathbb{R}_{\\ge 0}^{r \\times n}$ encodes the exposures of these signatures across $n$ samples. Consider a synthetic catalogue with $m = 6$ mutation channels and $n = 3$ samples, and a rank-$2$ approximation ($r = 2$). The columns of $W$ are normalized to sum to $1$ (each signature is a proper discrete distribution over channels), and $H$ contains non-negative exposures.\n\nYou are given the following matrices:\n$$\nV = \\begin{pmatrix}\n8 & 5 & 12 \\\\\n9 & 3 & 11 \\\\\n8 & 4 & 12 \\\\\n7 & 5 & 13 \\\\\n8 & 4 & 12 \\\\\n4 & 2 & 5\n\\end{pmatrix}, \\quad\nW = \\begin{pmatrix}\n\\frac{3}{12} & \\frac{1}{10} \\\\\n\\frac{2}{12} & \\frac{2}{10} \\\\\n\\frac{1}{12} & \\frac{3}{10} \\\\\n\\frac{3}{12} & \\frac{1}{10} \\\\\n\\frac{2}{12} & \\frac{2}{10} \\\\\n\\frac{1}{12} & \\frac{1}{10}\n\\end{pmatrix}, \\quad\nH = \\begin{pmatrix}\n24 & 12 & 36 \\\\\n20 & 10 & 30\n\\end{pmatrix}.\n$$\n\nCompute the reconstructed catalogue $W H$ and then compute the reconstruction error quantified by the Frobenius norm of the residual, namely the single scalar $\\|V - W H\\|_{F}$. Express your final error as an exact value; do not round. No units are required.",
            "solution": "The problem is validated as scientifically grounded, well-posed, and self-contained. The dimensions of the matrices are consistent for the required operations, and the contextual framing within mutational signature analysis is appropriate. All provided data and conditions are valid. We may proceed with the solution.\n\nThe objective is to compute the Frobenius norm of the residual matrix, $\\|V - W H\\|_{F}$, where $V$, $W$, and $H$ are given matrices. The Frobenius norm of a matrix $A \\in \\mathbb{R}^{m \\times n}$ is defined as $\\|A\\|_{F} = \\sqrt{\\sum_{i=1}^{m} \\sum_{j=1}^{n} |a_{ij}|^2}$.\n\nFirst, we must compute the reconstructed catalogue, which is the product of the signature matrix $W$ and the exposure matrix $H$.\nThe given matrices are:\n$$\nV = \\begin{pmatrix}\n8 & 5 & 12 \\\\\n9 & 3 & 11 \\\\\n8 & 4 & 12 \\\\\n7 & 5 & 13 \\\\\n8 & 4 & 12 \\\\\n4 & 2 & 5\n\\end{pmatrix}, \\quad\nW = \\begin{pmatrix}\n\\frac{3}{12} & \\frac{1}{10} \\\\\n\\frac{2}{12} & \\frac{2}{10} \\\\\n\\frac{1}{12} & \\frac{3}{10} \\\\\n\\frac{3}{12} & \\frac{1}{10} \\\\\n\\frac{2}{12} & \\frac{2}{10} \\\\\n\\frac{1}{12} & \\frac{1}{10}\n\\end{pmatrix}, \\quad\nH = \\begin{pmatrix}\n24 & 12 & 36 \\\\\n20 & 10 & 30\n\\end{pmatrix}.\n$$\nWe can simplify the fractions in matrix $W$:\n$$\nW = \\begin{pmatrix}\n\\frac{1}{4} & \\frac{1}{10} \\\\\n\\frac{1}{6} & \\frac{1}{5} \\\\\n\\frac{1}{12} & \\frac{3}{10} \\\\\n\\frac{1}{4} & \\frac{1}{10} \\\\\n\\frac{1}{6} & \\frac{1}{5} \\\\\n\\frac{1}{12} & \\frac{1}{10}\n\\end{pmatrix}\n$$\nNow, we compute the matrix product $WH$. Let the resulting matrix be $V' = WH$.\n$$\nV' = WH = \\begin{pmatrix}\n\\frac{1}{4} & \\frac{1}{10} \\\\\n\\frac{1}{6} & \\frac{1}{5} \\\\\n\\frac{1}{12} & \\frac{3}{10} \\\\\n\\frac{1}{4} & \\frac{1}{10} \\\\\n\\frac{1}{6} & \\frac{1}{5} \\\\\n\\frac{1}{12} & \\frac{1}{10}\n\\end{pmatrix}\n\\begin{pmatrix}\n24 & 12 & 36 \\\\\n20 & 10 & 30\n\\end{pmatrix}\n$$\nWe compute each element of $V'$:\n$V'_{11} = (\\frac{1}{4})(24) + (\\frac{1}{10})(20) = 6 + 2 = 8$\n$V'_{12} = (\\frac{1}{4})(12) + (\\frac{1}{10})(10) = 3 + 1 = 4$\n$V'_{13} = (\\frac{1}{4})(36) + (\\frac{1}{10})(30) = 9 + 3 = 12$\n\n$V'_{21} = (\\frac{1}{6})(24) + (\\frac{1}{5})(20) = 4 + 4 = 8$\n$V'_{22} = (\\frac{1}{6})(12) + (\\frac{1}{5})(10) = 2 + 2 = 4$\n$V'_{23} = (\\frac{1}{6})(36) + (\\frac{1}{5})(30) = 6 + 6 = 12$\n\n$V'_{31} = (\\frac{1}{12})(24) + (\\frac{3}{10})(20) = 2 + 6 = 8$\n$V'_{32} = (\\frac{1}{12})(12) + (\\frac{3}{10})(10) = 1 + 3 = 4$\n$V'_{33} = (\\frac{1}{12})(36) + (\\frac{3}{10})(30) = 3 + 9 = 12$\n\n$V'_{41} = (\\frac{1}{4})(24) + (\\frac{1}{10})(20) = 6 + 2 = 8$\n$V'_{42} = (\\frac{1}{4})(12) + (\\frac{1}{10})(10) = 3 + 1 = 4$\n$V'_{43} = (\\frac{1}{4})(36) + (\\frac{1}{10})(30) = 9 + 3 = 12$\n\n$V'_{51} = (\\frac{1}{6})(24) + (\\frac{1}{5})(20) = 4 + 4 = 8$\n$V'_{52} = (\\frac{1}{6})(12) + (\\frac{1}{5})(10) = 2 + 2 = 4$\n$V'_{53} = (\\frac{1}{6})(36) + (\\frac{1}{5})(30) = 6 + 6 = 12$\n\n$V'_{61} = (\\frac{1}{12})(24) + (\\frac{1}{10})(20) = 2 + 2 = 4$\n$V'_{62} = (\\frac{1}{12})(12) + (\\frac{1}{10})(10) = 1 + 1 = 2$\n$V'_{63} = (\\frac{1}{12})(36) + (\\frac{1}{10})(30) = 3 + 3 = 6$\n\nThus, the reconstructed matrix is:\n$$\nWH = \\begin{pmatrix}\n8 & 4 & 12 \\\\\n8 & 4 & 12 \\\\\n8 & 4 & 12 \\\\\n8 & 4 & 12 \\\\\n8 & 4 & 12 \\\\\n4 & 2 & 6\n\\end{pmatrix}\n$$\nNext, we compute the residual matrix, $R = V - WH$.\n$$\nR = \\begin{pmatrix}\n8 & 5 & 12 \\\\\n9 & 3 & 11 \\\\\n8 & 4 & 12 \\\\\n7 & 5 & 13 \\\\\n8 & 4 & 12 \\\\\n4 & 2 & 5\n\\end{pmatrix} - \\begin{pmatrix}\n8 & 4 & 12 \\\\\n8 & 4 & 12 \\\\\n8 & 4 & 12 \\\\\n8 & 4 & 12 \\\\\n8 & 4 & 12 \\\\\n4 & 2 & 6\n\\end{pmatrix} = \\begin{pmatrix}\n8-8 & 5-4 & 12-12 \\\\\n9-8 & 3-4 & 11-12 \\\\\n8-8 & 4-4 & 12-12 \\\\\n7-8 & 5-4 & 13-12 \\\\\n8-8 & 4-4 & 12-12 \\\\\n4-4 & 2-2 & 5-6\n\\end{pmatrix}\n$$\n$$\nR = \\begin{pmatrix}\n0 & 1 & 0 \\\\\n1 & -1 & -1 \\\\\n0 & 0 & 0 \\\\\n-1 & 1 & 1 \\\\\n0 & 0 & 0 \\\\\n0 & 0 & -1\n\\end{pmatrix}\n$$\nFinally, we compute the Frobenius norm of the residual matrix $R$.\n$$\n\\|R\\|_{F} = \\sqrt{\\sum_{i=1}^{6} \\sum_{j=1}^{3} R_{ij}^2}\n$$\nThe sum of the squares of the elements of $R$ is:\n$$\n\\sum R_{ij}^2 = (0^2 + 1^2 + 0^2) + (1^2 + (-1)^2 + (-1)^2) + (0^2 + 0^2 + 0^2) + ((-1)^2 + 1^2 + 1^2) + (0^2 + 0^2 + 0^2) + (0^2 + 0^2 + (-1)^2)\n$$\n$$\n\\sum R_{ij}^2 = (0+1+0) + (1+1+1) + (0) + (1+1+1) + (0) + (1)\n$$\n$$\n\\sum R_{ij}^2 = 1 + 3 + 3 + 1 = 8\n$$\nTherefore, the Frobenius norm is:\n$$\n\\|V - WH\\|_{F} = \\|R\\|_{F} = \\sqrt{8} = \\sqrt{4 \\times 2} = 2\\sqrt{2}\n$$\nThe reconstruction error is $2\\sqrt{2}$.",
            "answer": "$$\\boxed{2\\sqrt{2}}$$"
        },
        {
            "introduction": "Moving beyond the mechanics of matrix factorization, a critical question in any quantitative analysis is that of statistical power: how much data is required to obtain a reliable result? In mutational signature analysis, this translates to determining the minimum number of mutations needed to confidently estimate signature exposures. This practice problem delves into the statistical underpinnings of signature refitting, using the principles of maximum likelihood estimation and Fisher information to connect the precision of an exposure estimate to the total mutational burden, providing a crucial link between statistical theory and practical experimental design .",
            "id": "4587872",
            "problem": "Consider a mutational signature refitting scenario in which each tumor sample’s observed mutational catalog across $M$ categories is modeled as a multinomial random vector with total count $n$ and category probabilities $\\mathbf{p}(\\mathbf{w})$, where $\\mathbf{p}(\\mathbf{w})$ is a convex mixture of a fixed signature catalog. Specifically, let there be $K=2$ fixed signatures with normalized profiles $\\mathbf{s}^{(1)}$ and $\\mathbf{s}^{(2)}$ over $M=4$ categories, and let the exposure to signature $1$ be $w \\in (0,1)$ and to signature $2$ be $1-w$. The mixture model is\n$$\n\\mathbf{p}(w) \\;=\\; w\\,\\mathbf{s}^{(1)} \\;+\\; (1-w)\\,\\mathbf{s}^{(2)} \\;=\\; \\mathbf{s}^{(2)} + w\\big(\\mathbf{s}^{(1)} - \\mathbf{s}^{(2)}\\big).\n$$\nAssume large-sample maximum likelihood estimation and asymptotic normality for the exposure estimator to signature $1$, $\\hat{w}$, derived under the multinomial model. Reliability is defined in terms of a two-sided Wald confidence interval for $w$ at $95\\%$ confidence having half-width at most $\\delta$. Under the multinomial variance model and differentiable parameterization, the asymptotic variance of the maximum likelihood estimator (MLE) $\\hat{w}$ is determined by the Fisher information for $w$.\n\nYou are given the fixed signatures and the true exposure:\n- $\\mathbf{s}^{(1)} = \\big(0.4,\\,0.3,\\,0.2,\\,0.1\\big)$,\n- $\\mathbf{s}^{(2)} = \\big(0.1,\\,0.2,\\,0.3,\\,0.4\\big)$,\n- $w = 0.6$.\n\nLet the reliability threshold be $\\delta = 0.05$, and use the standard normal quantile $z_{0.975}$ for a $95\\%$ confidence level. Using first principles of likelihood-based inference for multinomial models (deriving the Fisher information for $w$ from the multinomial log-likelihood and the Jacobian of $\\mathbf{p}(w)$ with respect to $w$), determine the minimal number of mutations per sample, $n_{\\min}$, such that the Wald interval for $w$ has half-width at most $\\delta$.\n\nYour final answer must be the single minimal integer $n_{\\min}$ that satisfies this reliability criterion. Express the result as a whole number of mutations per sample. Do not include units inside the final answer box.",
            "solution": "The user wants to determine the minimal number of mutations, $n_{\\min}$, required to achieve a specified precision for the estimate of a mutational signature exposure, $w$. The precision is defined by the half-width of a $95\\%$ Wald confidence interval for $w$, which must be no larger than a threshold $\\delta$.\n\nFirst, I will perform the problem validation.\n\n### Step 1: Extract Givens\n- **Model**: The observed mutational catalog is a multinomial random vector.\n- **Number of categories**: $M=4$.\n- **Number of signatures**: $K=2$.\n- **Total mutation count**: $n$.\n- **Category probabilities**: $\\mathbf{p}(w) = w\\,\\mathbf{s}^{(1)} + (1-w)\\,\\mathbf{s}^{(2)}$.\n- **Signature 1 profile**: $\\mathbf{s}^{(1)} = (0.4,\\,0.3,\\,0.2,\\,0.1)$.\n- **Signature 2 profile**: $\\mathbf{s}^{(2)} = (0.1,\\,0.2,\\,0.3,\\,0.4)$.\n- **True exposure to signature 1**: $w = 0.6$.\n- **Reliability threshold (maximum half-width)**: $\\delta = 0.05$.\n- **Confidence level**: $95\\%$.\n- **Standard normal quantile**: $z_{0.975}$.\n- **Assumptions**: Large-sample maximum likelihood estimation (MLE) and asymptotic normality of the estimator $\\hat{w}$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is based on a standard statistical model (multinomial mixture model) widely used in bioinformatics for mutational signature analysis. The methods employed, namely maximum likelihood estimation, Fisher information, and Wald confidence intervals, are fundamental principles of statistical inference. The problem is scientifically sound.\n- **Well-Posed**: The problem is clearly stated and provides all necessary information to compute a unique integer value for $n_{\\min}$. The question is mathematically and statistically well-defined.\n- **Objective**: The problem is formulated using precise, quantitative, and unbiased language.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. I will now proceed with the solution.\n\nThe solution is derived from the principles of likelihood-based inference for multinomial models. The goal is to find the minimum sample size, $n$, such that the half-width of the $95\\%$ Wald confidence interval for the exposure parameter $w$ is at most $\\delta$.\n\nThe $95\\%$ Wald confidence interval for $w$ is given by:\n$$ \\hat{w} \\pm z_{0.975} \\sqrt{\\text{Var}(\\hat{w})} $$\nwhere $\\hat{w}$ is the MLE of $w$, and $z_{0.975}$ is the quantile of the standard normal distribution corresponding to a cumulative probability of $0.975$. The standard value is $z_{0.975} \\approx 1.96$.\n\nThe half-width of this interval is $h = z_{0.975} \\sqrt{\\text{Var}(\\hat{w})}$. The reliability criterion is $h \\le \\delta$.\n$$ z_{0.975} \\sqrt{\\text{Var}(\\hat{w})} \\le \\delta $$\nUnder the assumption of large-sample MLE, the variance of $\\hat{w}$ is approximated by the inverse of the Fisher information, $I_n(w)$:\n$$ \\text{Var}(\\hat{w}) \\approx [I_n(w)]^{-1} $$\nFor a sample of $n$ independent observations (mutations), the total Fisher information is $I_n(w) = n I_1(w)$, where $I_1(w)$ is the Fisher information for a single observation. The inequality becomes:\n$$ z_{0.975} \\sqrt{\\frac{1}{n I_1(w)}} \\le \\delta $$\nSquaring both sides and rearranging to solve for $n$, we get:\n$$ \\frac{z_{0.975}^2}{n I_1(w)} \\le \\delta^2 \\implies n \\ge \\frac{z_{0.975}^2}{\\delta^2 I_1(w)} $$\nThe minimal integer number of mutations, $n_{\\min}$, is the smallest integer satisfying this condition, which is the ceiling of the right-hand side.\n$$ n_{\\min} = \\left\\lceil \\frac{z_{0.975}^2}{\\delta^2 I_1(w)} \\right\\rceil $$\nNow, we must derive the expression for the Fisher information, $I_1(w)$. For a single trial from a multinomial distribution with probabilities $\\mathbf{p}(w) = (p_1(w), \\dots, p_M(w))$ that depend on a scalar parameter $w$, the Fisher information is given by:\n$$ I_1(w) = \\sum_{j=1}^{M} \\frac{1}{p_j(w)} \\left( \\frac{dp_j(w)}{dw} \\right)^2 $$\nThe problem defines the probability mixture model as $\\mathbf{p}(w) = w\\,\\mathbf{s}^{(1)} + (1-w)\\,\\mathbf{s}^{(2)}$.\nThe derivative of the vector of probabilities $\\mathbf{p}(w)$ with respect to $w$ is:\n$$ \\frac{d\\mathbf{p}(w)}{dw} = \\mathbf{s}^{(1)} - \\mathbf{s}^{(2)} $$\nLet's compute this vector difference using the given signature profiles:\n$\\mathbf{s}^{(1)} = (0.4, 0.3, 0.2, 0.1)$\n$\\mathbf{s}^{(2)} = (0.1, 0.2, 0.3, 0.4)$\n$$ \\frac{d\\mathbf{p}(w)}{dw} = (0.4-0.1, 0.3-0.2, 0.2-0.3, 0.1-0.4) = (0.3, 0.1, -0.1, -0.3) $$\nLet $\\mathbf{d} = \\frac{d\\mathbf{p}(w)}{dw}$. The components are $d_j = \\frac{dp_j(w)}{dw}$.\n\nNext, we compute the probability vector $\\mathbf{p}(w)$ at the true exposure value $w=0.6$:\n$$ \\mathbf{p}(0.6) = 0.6\\,\\mathbf{s}^{(1)} + (1-0.6)\\,\\mathbf{s}^{(2)} = 0.6\\,\\mathbf{s}^{(1)} + 0.4\\,\\mathbf{s}^{(2)} $$\n$$ p_1(0.6) = 0.6(0.4) + 0.4(0.1) = 0.24 + 0.04 = 0.28 $$\n$$ p_2(0.6) = 0.6(0.3) + 0.4(0.2) = 0.18 + 0.08 = 0.26 $$\n$$ p_3(0.6) = 0.6(0.2) + 0.4(0.3) = 0.12 + 0.12 = 0.24 $$\n$$ p_4(0.6) = 0.6(0.1) + 0.4(0.4) = 0.06 + 0.16 = 0.22 $$\nSo, $\\mathbf{p}(0.6) = (0.28, 0.26, 0.24, 0.22)$.\n\nNow, we can calculate the Fisher information for a single trial, $I_1(w)$, at $w=0.6$:\n$$ I_1(0.6) = \\sum_{j=1}^{4} \\frac{d_j^2}{p_j(0.6)} = \\frac{(0.3)^2}{0.28} + \\frac{(0.1)^2}{0.26} + \\frac{(-0.1)^2}{0.24} + \\frac{(-0.3)^2}{0.22} $$\n$$ I_1(0.6) = \\frac{0.09}{0.28} + \\frac{0.01}{0.26} + \\frac{0.01}{0.24} + \\frac{0.09}{0.22} $$\n$$ I_1(0.6) = \\frac{9}{28} + \\frac{1}{26} + \\frac{1}{24} + \\frac{9}{22} $$\nNumerically, this is:\n$$ I_1(0.6) \\approx 0.32142857 + 0.03846154 + 0.04166667 + 0.40909091 \\approx 0.81064769 $$\nFinally, we can compute $n_{\\min}$ using the given values $\\delta=0.05$ and $z_{0.975} = 1.96$:\n$$ n_{\\min} = \\left\\lceil \\frac{(1.96)^2}{(0.05)^2 \\times I_1(0.6)} \\right\\rceil $$\n$$ n_{\\min} = \\left\\lceil \\frac{3.8416}{0.0025 \\times 0.81064769} \\right\\rceil $$\n$$ n_{\\min} = \\left\\lceil \\frac{3.8416}{0.0020266192} \\right\\rceil $$\n$$ n_{\\min} = \\lceil 1895.5506 \\dots \\rceil $$\nSince the number of mutations must be an integer, we take the ceiling of this value.\n$$ n_{\\min} = 1896 $$\nTherefore, a minimum of $1896$ mutations per sample is required to ensure the $95\\%$ confidence interval for the exposure to signature $1$ has a half-width of at most $0.05$.",
            "answer": "$$\\boxed{1896}$$"
        },
        {
            "introduction": "A model is only as robust as the data it is built upon, and real-world genomic data is rarely perfect. Preprocessing errors, such as the misclassification of complex mutation types, can have subtle but significant downstream consequences. This final exercise challenges you to think critically about the stability of the mutational signature model in the face of such plausible data corruption. By analyzing how the misclassification of a doublet base substitution (DBS) propagates through the refitting process, you will develop a deeper understanding of how model structure, such as the separation of SBS and DBS signatures, interacts with data quality to influence the final results .",
            "id": "4587933",
            "problem": "Consider a tumor sample whose mutational catalog is represented by a concatenated count vector $x \\in \\mathbb{N}^{K+L}$, where the first $K$ entries are Single Base Substitution (SBS) category counts and the last $L$ entries are Doublet Base Substitution (DBS) category counts. Let $S \\in \\mathbb{R}^{(K+L) \\times M}$ be a signature matrix whose first $m$ columns are SBS signatures with support only on SBS categories and whose last $n$ columns are DBS signatures with support only on DBS categories (so that $M = m + n$ and $S$ is block-diagonal with respect to SBS versus DBS). Each signature column is a nonnegative probability vector over its supported mutation categories, and exposures $w \\in \\mathbb{R}_{\\ge 0}^{M}$ scale these columns to produce expected counts. Downstream refitting proceeds by solving the Non-negative Least Squares (NNLS) problem\n$$\n\\min_{w \\ge 0} \\left\\| x - S w \\right\\|_2^2,\n$$\nwhich is equivalent (under a homogeneous Poisson generative model with mean vector $\\mu = S w$) to Maximum Likelihood Estimation (MLE) of $w$ up to a monotone transformation of the objective.\n\nSuppose that, due to a calling or preprocessing error, a single true DBS event in DBS category index $j$ is misclassified as two adjacent SBS events in SBS category indices $r$ and $s$. Then the observed catalog is perturbed to $x' = x + \\Delta$, where $\\Delta \\in \\mathbb{Z}^{K+L}$ satisfies $\\Delta_{r} = 1$, $\\Delta_{s} = 1$, $\\Delta_{K+j} = -1$, and all other entries are zero. No other counts are changed, and the signature matrix $S$ remains fixed.\n\nBased solely on first principles of the block-diagonal structure of $S$, the NNLS refitting objective, and the Poisson generative interpretation, which of the following statements best characterizes the consequences of this misclassification for downstream signature refitting, including expected bias in exposures, the behavior of the Karush–Kuhn–Tucker (KKT) optimality conditions, and commonly reported fit metrics such as cosine similarity computed on SBS-only profiles?\n\nA. Under block-separated NNLS refitting with column-normalized signatures, misclassifying a single Doublet Base Substitution (DBS) as two Single Base Substitution (SBS) counts will shift the optimal exposure vector by increasing SBS exposures toward signatures that assign high probability to categories $r$ and $s$ and decreasing DBS exposures; moreover, the Karush–Kuhn–Tucker (KKT) conditions imply that if the DBS block loses sufficient informative counts, its optimal exposure can be driven to zero, and cosine similarity computed on SBS-only profiles can increase even though the underlying etiology has not changed.\n\nB. The misclassification does not affect refitting because SBS and DBS signatures are estimated jointly; the algorithm depends only on the total number of mutated bases and not on whether events are singletons or doublets, so DBS exposure estimates remain unbiased.\n\nC. Misclassification typically reduces SBS exposures because turning one DBS event into two SBS events inflates variance, and regularization in standard refitting penalizes larger SBS weights; the algorithm therefore shrinks SBS weights and increases DBS weights, improving detection of DBS signatures.\n\nD. The only measurable consequence is an increase in reconstruction error, since moving counts across SBS and DBS blocks always worsens the least-squares objective when the signature matrix is fixed; exposure estimates are otherwise unchanged.",
            "solution": "The user wants a critical analysis of a problem in mutational signature analysis, followed by a solution.\n\n### Step 1: Extract Givens\n-   **Data Vector**: A mutational catalog `x \\in \\mathbb{N}^{K+L}`.\n-   **Structure**: The first `K` entries are Single Base Substitution (SBS) counts; the last `L` entries are Doublet Base Substitution (DBS) counts.\n-   **Signature Matrix**: `S \\in \\mathbb{R}^{(K+L) \\times M}`.\n-   **Matrix Structure**: `S` is block-diagonal.\n    -   The first `m` columns are SBS signatures, with support only on the first `K` (SBS) categories.\n    -   The last `n` columns are DBS signatures, with support only on the last `L` (DBS) categories.\n    -   Total signatures `M = m + n`.\n-   **Signature Properties**: Each column of `S` is a non-negative probability vector over its supported categories.\n-   **Exposure Vector**: `w \\in \\mathbb{R}_{\\ge 0}^{M}`.\n-   **Generative Model**: Expected counts are `\\mu = S w`. The problem notes this corresponds to a homogeneous Poisson model.\n-   **Refitting Objective**: Non-negative Least Squares (NNLS), `\\min_{w \\ge 0} \\left\\| x - S w \\right\\|_2^2`.\n-   **The Perturbation**: A single true DBS event (category `j`) is misclassified as two SBS events (categories `r` and `s`).\n-   **Perturbed Data Vector**: `x' = x + \\Delta`, where `\\Delta \\in \\mathbb{Z}^{K+L}` has `\\Delta_{r} = 1`, `\\Delta_{s} = 1`, `\\Delta_{K+j} = -1`, and all other entries are `0$.\n-   **Constraint**: The signature matrix `S` is fixed during refitting.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is subjected to validation.\n\n-   **Scientific Grounding**: The problem is well-grounded in the established theory and practice of mutational signature analysis, a core topic in bioinformatics. The concepts of SBS/DBS catalogs, signature matrices, exposures, NNLS refitting, and Poisson models are all standard. The specified error—a DBS being miscalled as two adjacent SBS events—is a known and plausible artifact in sequencing data analysis. The problem is scientifically sound.\n-   **Well-Posedness**: The problem is mathematically well-posed. It describes a standard convex optimization problem (NNLS) and a specific perturbation to its input data. The question asks for a qualitative analysis of how the solution `w` changes in response to the perturbation. A unique optimal solution for the NNLS problem exists, and its properties can be analyzed.\n-   **Objectivity**: The problem is stated in precise, objective, mathematical language. It is free from ambiguity, subjectivity, or non-scientific claims.\n\n**Verdict**: The problem is valid. It is a well-defined question about the properties of a constrained optimization problem under a specific data perturbation, set within a standard bioinformatics framework.\n\n### Step 3: Derivation and Option Analysis\n\nWe proceed to solve the problem based on the validated statement.\n\n#### Principle-Based Derivation\n\nThe core of the problem lies in the block-diagonal structure of the signature matrix `S`. Let `x` be partitioned into its SBS and DBS parts, `x = \\begin{pmatrix} x_{SBS} \\\\ x_{DBS} \\end{pmatrix}`, where `x_{SBS} \\in \\mathbb{N}^K` and `x_{DBS} \\in \\mathbb{N}^L`. Similarly, the exposure vector `w` is partitioned as `w = \\begin{pmatrix} w_{SBS} \\\\ w_{DBS} \\end{pmatrix}`, where `w_{SBS} \\in \\mathbb{R}_{\\ge 0}^m` and `w_{DBS} \\in \\mathbb{R}_{\\ge 0}^n`.\n\nThe block-diagonal signature matrix can be written as `S = \\begin{pmatrix} S_{SBS} & 0 \\\\ 0 & S_{DBS} \\end{pmatrix}`, where `S_{SBS} \\in \\mathbb{R}^{K \\times m}` and `S_{DBS} \\in \\mathbb{R}^{L \\times n}`.\n\nThe NNLS objective function is:\n$$\n\\left\\| x - S w \\right\\|_2^2 = \\left\\| \\begin{pmatrix} x_{SBS} \\\\ x_{DBS} \\end{pmatrix} - \\begin{pmatrix} S_{SBS} & 0 \\\\ 0 & S_{DBS} \\end{pmatrix} \\begin{pmatrix} w_{SBS} \\\\ w_{DBS} \\end{pmatrix} \\right\\|_2^2 = \\left\\| \\begin{pmatrix} x_{SBS} - S_{SBS} w_{SBS} \\\\ x_{DBS} - S_{DBS} w_{DBS} \\end{pmatrix} \\right\\|_2^2\n$$\nBy the definition of the L2-norm, this is equivalent to:\n$$\n\\left\\| x_{SBS} - S_{SBS} w_{SBS} \\right\\|_2^2 + \\left\\| x_{DBS} - S_{DBS} w_{DBS} \\right\\|_2^2\n$$\nSince the objective function is a sum of two independent terms, and the non-negativity constraints `w_{SBS} \\ge 0` and `w_{DBS} \\ge 0` apply independently to the two parts of the exposure vector, the overall optimization problem decouples into two separate NNLS problems:\n1.  **SBS Refitting**: `\\min_{w_{SBS} \\ge 0} \\left\\| x_{SBS} - S_{SBS} w_{SBS} \\right\\|_2^2`\n2.  **DBS Refitting**: `\\min_{w_{DBS} \\ge 0} \\left\\| x_{DBS} - S_{DBS} w_{DBS} \\right\\|_2^2`\n\nThe perturbation `\\Delta` also acts separately on the two blocks. The new, perturbed data vector `x' = x + \\Delta` has components:\n-   `x'_{SBS} = x_{SBS} + \\Delta_{SBS}`, where `\\Delta_{SBS}` is a vector with `1` at indices `r` and `s`, and `0` otherwise.\n-   `x'_{DBS} = x_{DBS} + \\Delta_{DBS}`, where `\\Delta_{DBS}` is a vector with `-1` at index `j`, and `0` otherwise.\n\nWe analyze the effect on each subproblem:\n\n**Effect on SBS Exposures**: The SBS data vector `x'_{SBS}` now contains two additional counts. To explain these counts, the NNLS algorithm must find a new optimal exposure vector `w'_{SBS}`. The optimization will favor increasing the exposures of signatures (columns in `S_{SBS}`) that have high probabilities (large values) in rows `r` and `s`, as this will reduce the residual term `\\left\\| x'_{SBS} - S_{SBS} w'_{SBS} \\right\\|_2^2`. Consequently, the overall magnitude of the SBS exposure vector is expected to increase.\n\n**Effect on DBS Exposures**: The DBS data vector `x'_{DBS}` has one fewer count at category `j`. Since there is less mutational signal to explain, the NNLS algorithm will find a new optimal `w'_{DBS}` that is generally smaller in magnitude than the original `w_{DBS}`. The exposures of signatures that contribute to category `j` are particularly likely to decrease.\n\n**Karush–Kuhn–Tucker (KKT) Conditions and Zero Exposures**: For the DBS subproblem, the KKT optimality conditions for `w'_{DBS}` must hold. If the removal of the count at index `j` significantly weakens the evidence for one or more DBS signatures (i.e., the \"DBS block loses sufficient informative counts\"), the optimal solution may drive the corresponding exposures to zero. In an extreme case, if the original `x_{DBS}` was very sparse and the removed count was the primary evidence for any DBS activity, the new optimal solution could even become `w'_{DBS} = 0`. This is a characteristic behavior of NNLS where, below a certain signal threshold, attributing counts to a signature may increase the squared error more than leaving them as unexplained residual.\n\n**Effect on Cosine Similarity**: Cosine similarity is a common metric for goodness-of-fit, calculated between the observed counts (e.g., `x'_{SBS}`) and the reconstructed counts (`\\hat{x}'_{SBS} = S_{SBS} w'_{SBS}`). The misclassified counts `\\Delta_r=1` and `\\Delta_s=1` are now part of the data being fitted. The NNLS procedure will find `w'_{SBS}` to make `\\hat{x}'_{SBS}` as close as possible to `x'_{SBS}`. By \"explaining\" these two new counts, the model is fitting to the noise/error. This can lead to an artifactual increase in the alignment between the observed and reconstructed vectors, resulting in a higher cosine similarity. The fit to the *corrupted* data improves, even though the fit to the *true* underlying biological reality has worsened. Therefore, it is plausible that the cosine similarity *can* increase.\n\n#### Option-by-Option Analysis\n\n**A. Under block-separated NNLS refitting with column-normalized signatures, misclassifying a single Doublet Base Substitution (DBS) as two Single Base Substitution (SBS) counts will shift the optimal exposure vector by increasing SBS exposures toward signatures that assign high probability to categories $r$ and $s$ and decreasing DBS exposures; moreover, the Karush–Kuhn–Tucker (KKT) conditions imply that if the DBS block loses sufficient informative counts, its optimal exposure can be driven to zero, and cosine similarity computed on SBS-only profiles can increase even though the underlying etiology has not changed.**\nThis statement aligns perfectly with our derivation. It correctly identifies:\n1.  The block-separated nature of the refitting.\n2.  The increase in SBS exposures, targeted at explaining the new counts.\n3.  The decrease in DBS exposures due to the removal of a count.\n4.  The possibility of DBS exposures being driven to zero if the data signal becomes too weak.\n5.  The potential for an artifactual increase in the cosine similarity goodness-of-fit metric.\n**Verdict: Correct.**\n\n**B. The misclassification does not affect refitting because SBS and DBS signatures are estimated jointly; the algorithm depends only on the total number of mutated bases and not on whether events are singletons or doublets, so DBS exposure estimates remain unbiased.**\nThis statement is flawed on multiple grounds.\n1.  While the full vector `w` is found in one optimization, the block-diagonal structure of `S` means the SBS and DBS components are solved independently. They are not \"joint\" in a way that mixes their estimation.\n2.  The least-squares objective `\\|x - Sw\\|_2^2` depends on the count in each specific category, not just the total sum of mutations.\n3.  Removing a count from the DBS catalog introduces a systematic change, biasing the DBS exposure estimates downwards. They do not remain unbiased.\n**Verdict: Incorrect.**\n\n**C. Misclassification typically reduces SBS exposures because turning one DBS event into two SBS events inflates variance, and regularization in standard refitting penalizes larger SBS weights; the algorithm therefore shrinks SBS weights and increases DBS weights, improving detection of DBS signatures.**\nThis statement makes several incorrect claims.\n1.  Adding counts to the SBS block will necessitate *larger*, not smaller, SBS exposures to explain them.\n2.  The problem specifies an unregularized NNLS objective. Even if regularization were present, it would compete with, not dominate, the need to explain the additional counts.\n3.  DBS weights are expected to *decrease*, not increase, as a count has been removed from the DBS catalog.\n4.  Removing true signal degrades, rather than improves, the detection of DBS signatures.\n**Verdict: Incorrect.**\n\n**D. The only measurable consequence is an increase in reconstruction error, since moving counts across SBS and DBS blocks always worsens the least-squares objective when the signature matrix is fixed; exposure estimates are otherwise unchanged.**\nThis statement is fundamentally incorrect.\n1.  Changing the input data `x` to `x'` will change the optimal solution `w`. Claiming exposure estimates are \"unchanged\" is false.\n2.  The \"reconstruction error\" is the value of the objective function at the minimum, `\\|x' - S w'\\|_2^2`, where `w'` is the new optimal exposure. There is no guarantee that this value will be larger than the original error `\\|x - S w\\|_2^2`. The algorithm finds the best possible fit to the *new* data, and that fit could have a smaller or larger residual error.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}