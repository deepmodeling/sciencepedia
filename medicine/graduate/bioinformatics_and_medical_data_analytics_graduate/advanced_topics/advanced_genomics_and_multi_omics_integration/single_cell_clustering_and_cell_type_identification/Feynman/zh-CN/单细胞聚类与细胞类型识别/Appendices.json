{
    "hands_on_practices": [
        {
            "introduction": "在进行任何下游分析（如聚类或轨迹推断）之前，识别并移除低质量细胞是单细胞分析流程中至关重要的一步。低质量的细胞，例如那些由于细胞膜破裂而泄露了胞浆RNA的细胞，会引入技术噪音，从而扭曲真实的生物学信号。本练习将指导您构建一个稳健的工作流程，使用多维特征空间中的马氏距离（Mahalanobis distance）来识别异常细胞，这种方法相比于简单的单变量阈值设定更为强大和有原则性 。",
            "id": "4607386",
            "problem": "给定单细胞核糖核酸测序（scRNA-seq）中每个细胞的摘要特征。对于每个细胞索引 $i \\in \\{1,\\dots,N\\}$，您拥有三个实值特征：唯一分子标识符（UMI）或读数 $n_i$（正整数计数）、检测到的基因数量 $G_i$（正整数计数）以及线粒体分数 $m_i$（一个在 $[0,1]$ 区间内的实数）。任务是构建一个稳健且完全指定的工作流程，在三维特征空间 $(\\log n_i, \\log G_i, m_i)$ 中使用稳健马氏距离来识别低质量细胞，并在假设稳健马氏距离平方服从卡方分布的情况下，推导出排除阈值。\n\n使用以下基本依据：\n- 中心极限定理和乘性噪声启发我们，在高通量测序中，对数转换后的计数通常近似于高斯分布，这使得多变量高斯模型成为质量控制的一个合理且经过充分检验的起点。\n- 对于一个均值为 $\\boldsymbol{\\mu}$、协方差为 $\\boldsymbol{\\Sigma}$ 的多变量高斯向量 $\\mathbf{X} \\in \\mathbb{R}^p$，其马氏距离平方 $D^2 = (\\mathbf{X}-\\boldsymbol{\\mu})^\\top \\boldsymbol{\\Sigma}^{-1} (\\mathbf{X}-\\boldsymbol{\\mu})$ 服从自由度为 $p$ 的卡方分布。\n- 稳健统计学用高崩溃点的替代方法替换经典的均值和协方差，以减轻离群值的影响。\n\n构建一个从此类基础出发的算法，除了下述输入数组外，不假设任何未知数。您的算法必须对任何有限的 $N \\ge 3$ 精确且确定地实现以下步骤：\n\n1) 特征转换：\n- 对于每个细胞 $i$，使用自然对数计算一个三维向量 $\\mathbf{x}_i = (\\log n_i, \\log G_i, m_i)$。在测试套件中，输入 $n_i$ 和 $G_i$ 将严格为正。\n\n2) 稳健位置和尺度：\n- 计算稳健位置估计 $\\widehat{\\boldsymbol{\\mu}}$，作为 $\\{\\mathbf{x}_i\\}_{i=1}^N$ 的分量中位数。\n- 对于单变量样本 $\\{z_j\\}_{j=1}^N$，将稳健尺度 $s(z)$ 定义为 $s(z) = 1.4826 \\cdot \\mathrm{MAD}(z)$，其中 $\\mathrm{MAD}(z) = \\mathrm{median}_j \\left|z_j - \\mathrm{median}_k z_k\\right|$。如果 $s(z) = 0$，则用无偏样本标准差 $\\sqrt{\\frac{1}{N-1} \\sum_j (z_j - \\bar{z})^2}$ 替换它；如果该值也为 $0$，则将其设置为一个正常数 $10^{-8}$。\n\n3) 稳健Gnanadesikan–Kettenring协方差：\n- 令 $p=3$，并令 $X$ 是一个 $N \\times 3$ 的矩阵，其行为 $\\mathbf{x}_i^\\top$。通过从每行中减去 $\\widehat{\\boldsymbol{\\mu}}$ 来对 $X$ 进行中心化。\n- 对于每个分量 $j \\in \\{1,2,3\\}$，对中心化的列 $X_{\\cdot j}$ 计算 $s_j = s(X_{\\cdot j})$；将协方差的对角线设置为 $s_j^2$。\n- 对于每对 $j \\ne k$ 的 $(j,k)$，计算稳健相关性\n  $$r_{jk} = \\frac{s(X_{\\cdot j} + X_{\\cdot k})^2 - s(X_{\\cdot j} - X_{\\cdot k})^2}{s(X_{\\cdot j} + X_{\\cdot k})^2 + s(X_{\\cdot j} - X_{\\cdot k})^2},$$\n  如果分母为 $0$，则将该比率解释为 $0$。然后将非对角线稳健协方差项设置为 $\\widehat{\\Sigma}_{jk} = r_{jk} \\, s_j \\, s_k$。\n- 通过用 $\\frac{1}{2}(\\widehat{\\boldsymbol{\\Sigma}} + \\widehat{\\boldsymbol{\\Sigma}}^\\top)$ 替换 $\\widehat{\\boldsymbol{\\Sigma}}$ 来使其对称化。\n- 为保证数值稳定性进行正则化，设置 $\\widehat{\\boldsymbol{\\Sigma}} \\leftarrow \\widehat{\\boldsymbol{\\Sigma}} + \\gamma \\, \\bar{v} \\, \\mathbf{I}_3$，其中 $\\bar{v}$ 是 $\\widehat{\\boldsymbol{\\Sigma}}$ 对角线元素的平均值，$\\mathbf{I}_3$ 是 $3 \\times 3$ 单位矩阵，$\\gamma = 10^{-6}$。\n\n4) 稳健马氏距离平方：\n- 对于每个细胞 $i$，计算\n  $$D_i^2 = (\\mathbf{x}_i - \\widehat{\\boldsymbol{\\mu}})^\\top \\widehat{\\boldsymbol{\\Sigma}}^{-1} (\\mathbf{x}_i - \\widehat{\\boldsymbol{\\mu}}),$$\n  如有必要，使用数值稳定的伪逆。\n\n5) 卡方阈值：\n- 在 $p=3$ 维的工作多变量高斯模型下，采用卡方决策规则：如果 $D_i^2 > c_\\alpha$，则将细胞 $i$ 标记为低质量，其中 $c_\\alpha$ 是自由度为 $3$ 的卡方分布的 $(1-\\alpha)$ 分位数。也就是说，$c_\\alpha$ 满足 $\\mathbb{P}[\\chi^2_3 \\le c_\\alpha] = 1 - \\alpha$。程序必须使用标准的卡方分位数函数数值计算 $c_\\alpha$。\n\n边缘情况处理：\n- 如果任何稳健尺度 $s(\\cdot)$ 为 $0$，请遵循步骤2中的回退规则。\n- 如果 $\\widehat{\\boldsymbol{\\Sigma}}$ 是奇异或近奇异的，使用Moore–Penrose伪逆来计算 $D_i^2$。\n\n测试套件和要求输出：\n为以下三个测试用例实现您的程序。在每个用例中，将数组视为 $i=1,\\dots,N$ 的有序列表。\n\n- 测试用例A（正常路径）：\n  - $n = [\\,8200,\\,9100,\\,7600,\\,10400,\\,5000,\\,4500,\\,9800,\\,8700,\\,9200,\\,3000,\\,2800,\\,2600\\,]$\n  - $G = [\\,2300,\\,2500,\\,2100,\\,2700,\\,1600,\\,1500,\\,2400,\\,2350,\\,2450,\\,900,\\,850,\\,800\\,]$\n  - $m = [\\,0.08,\\,0.07,\\,0.09,\\,0.06,\\,0.12,\\,0.11,\\,0.07,\\,0.08,\\,0.07,\\,0.35,\\,0.40,\\,0.45\\,]$\n  - $\\alpha = 0.01$\n\n- 测试用例B（单个极端离群值）：\n  - $n = [\\,6000,\\,6100,\\,5900,\\,6050,\\,5800,\\,1000\\,]$\n  - $G = [\\,1800,\\,1750,\\,1850,\\,1780,\\,1820,\\,400\\,]$\n  - $m = [\\,0.10,\\,0.11,\\,0.09,\\,0.10,\\,0.10,\\,0.30\\,]$\n  - $\\alpha = 0.05$\n\n- 测试用例C（近乎恒定的线粒体分数与一个离群值；退化尺度边缘情况）：\n  - $n = [\\,7000,\\,7100,\\,7200,\\,7300,\\,6900,\\,6800,\\,2000\\,]$\n  - $G = [\\,2000,\\,2050,\\,1980,\\,2020,\\,2010,\\,1990,\\,700\\,]$\n  - $m = [\\,0.10,\\,0.10,\\,0.10,\\,0.10,\\,0.10,\\,0.10,\\,0.35\\,]$\n  - $\\alpha = 0.05$\n\n最终输出格式：\n- 对于每个测试用例，输出一个长度为 $N$ 的布尔值列表，其中第 $i$ 个条目如果细胞 $i$ 被标记为低质量则为 $\\mathrm{True}$，否则为 $\\mathrm{False}$。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果，每个元素是对应一个测试用例的布尔值列表。例如，输出格式应精确如 $[[b_{A,1},\\dots,b_{A,N_A}],[b_{B,1},\\dots,b_{B,N_B}],[b_{C,1},\\dots,b_{C,N_C}]]$，不含多余空格或文本。",
            "solution": "所呈现的问题是一个定义明确且科学上合理的练习，旨在将稳健统计方法应用于一项典型的生物信息学任务：单细胞RNA测序（scRNA-seq）数据的质量控制。所提出的工作流程在方法论上是严谨的，它利用既定原则来识别可能破坏下游分析的离群细胞。对数转换、用于位置和协方差的稳健估计量（中位数、中位数绝对偏差和Gnanadesikan-Kettenring估计量）以及基于卡方分布的决策规则的运用，构成了一个完整且可形式化的程序。该问题是自洽的、客观的，并基于标准的统计学和生物信息学实践，因此得到了验证。\n\n该解决方案是通过直接且精确地实现问题陈述中指定的五步算法来构建的。\n\n1) 特征转换：\n初始步骤是将原始细胞特征转换到一个更适合高斯建模的空间。每个细胞 $i=1, \\dots, N$ 的输入特征是UMI计数 $n_i$、检测到的基因数量 $G_i$ 和线粒体分数 $m_i$。$n_i$ 和 $G_i$ 都是计数数据，通常表现出右偏分布和均值-方差关系，其中方差随均值伸缩。对数转换是此类数据的标准方差稳定化转换，其动机是中心极限定理应用于乘性而非加性噪声过程。因此，我们为每个细胞构建一个三维特征向量 $\\mathbf{x}_i$：\n$$\n\\mathbf{x}_i = (\\log n_i, \\log G_i, m_i)^\\top\n$$\n这里，$\\log$ 表示自然对数。线粒体分数 $m_i$ 已经是一个比率，通常不进行转换。这些向量的集合构成一个 $N \\times 3$ 的数据矩阵 $X$，其中第 $i$ 行为 $\\mathbf{x}_i^\\top$。\n\n2) 稳健位置和尺度估计：\n经典估计量，如样本均值和样本协方差，对离群值高度敏感。为减轻此问题，我们采用稳健估计量。\n数据中心位置的稳健估计 $\\widehat{\\boldsymbol{\\mu}}$ 是 $N$ 个特征向量 $\\{\\mathbf{x}_i\\}_{i=1}^N$ 的分量中位数。\n单变量样本 $\\{z_j\\}_{j=1}^N$ 的稳健尺度估计定义为 $s(z)$。这基于中位数绝对偏差（MAD），一种高崩溃点的离散度估计量。MAD 定义为：\n$$\n\\mathrm{MAD}(z) = \\mathrm{median}_j \\left|z_j - \\mathrm{median}_k z_k\\right|\n$$\n尺度估计 $s(z)$ 则是MAD的缩放版本：\n$$\ns(z) = 1.4826 \\cdot \\mathrm{MAD}(z)\n$$\n常数 $1.4826$ 是一个校正因子，约等于 $1/\\Phi^{-1}(0.75)$，其中 $\\Phi^{-1}$ 是标准正态分布的分位数函数。这种缩放使 $s(z)$ 成为正态分布数据标准差的一致估计量。\n在 $\\mathrm{MAD}(z) = 0$ 的边缘情况下，表明变异性较低，我们回退到使用无偏样本标准差 $\\sqrt{\\frac{1}{N-1} \\sum_j (z_j - \\bar{z})^2}$。如果此值也为 $0$，则使用一个小的正常数 $10^{-8}$ 来确保尺度非零。\n\n3) 稳健Gnanadesikan–Kettenring（G-K）协方差估计：\n有了位置和尺度的稳健估计，我们构建一个稳健协方差矩阵 $\\widehat{\\boldsymbol{\\Sigma}}$。首先，通过从每行减去稳健位置估计 $\\widehat{\\boldsymbol{\\mu}}$ 来中心化数据矩阵 $X$。设此中心化矩阵的列为 $X_{\\cdot j}$，$j=1, 2, 3$。\n$\\widehat{\\boldsymbol{\\Sigma}}$ 的对角线元素是相应中心化特征列的稳健尺度的平方：\n$$\n\\widehat{\\Sigma}_{jj} = s_j^2 = s(X_{\\cdot j})^2\n$$\n非对角线元素源于稳健相关性估计 $r_{jk}$。G-K估计量利用了恒等式 $\\mathrm{Var}(U+V) - \\mathrm{Var}(U-V) = 4\\mathrm{Cov}(U,V)$。用稳健尺度估计量平方 $s(\\cdot)^2$ 替换方差，得到一个稳健的模拟。稳健相关性为：\n$$\nr_{jk} = \\frac{s(X_{\\cdot j} + X_{\\cdot k})^2 - s(X_{\\cdot j} - X_{\\cdot k})^2}{s(X_{\\cdot j} + X_{\\cdot k})^2 + s(X_{\\cdot j} - X_{\\cdot k})^2}\n$$\n分母是一个归一化因子。如果分母为 $0$，则相关性 $r_{jk}$ 取为 $0$。然后，非对角线协方差为 $\\widehat{\\Sigma}_{jk} = r_{jk} s_j s_k$。\n由于估计的可变性，所得矩阵可能不完全对称，因此对其进行对称化：$\\widehat{\\boldsymbol{\\Sigma}} \\leftarrow \\frac{1}{2}(\\widehat{\\boldsymbol{\\Sigma}} + \\widehat{\\boldsymbol{\\Sigma}}^\\top)$。\n最后，为确保矩阵是正定的且在求逆时条件良好，添加一个小的正则化项：\n$$\n\\widehat{\\boldsymbol{\\Sigma}} \\leftarrow \\widehat{\\boldsymbol{\\Sigma}} + \\gamma \\, \\bar{v} \\, \\mathbf{I}_3\n$$\n其中 $\\gamma = 10^{-6}$，$\\bar{v}$ 是 $\\widehat{\\boldsymbol{\\Sigma}}$ 对角线元素的平均值，$\\mathbf{I}_3$ 是 $3 \\times 3$ 单位矩阵。\n\n4) 稳健马氏距离平方：\n使用稳健位置 $\\widehat{\\boldsymbol{\\mu}}$ 和协方差 $\\widehat{\\boldsymbol{\\Sigma}}$ 估计，我们计算每个细胞 $i$ 的马氏距离平方：\n$$\nD_i^2 = (\\mathbf{x}_i - \\widehat{\\boldsymbol{\\mu}})^\\top \\widehat{\\boldsymbol{\\Sigma}}^{-1} (\\mathbf{x}_i - \\widehat{\\boldsymbol{\\mu}})\n$$\n该距离测量一个点离数据中心有多少个标准差，同时考虑了数据的相关结构。为保证数值稳定性，逆矩阵 $\\widehat{\\boldsymbol{\\Sigma}}^{-1}$ 使用Moore-Penrose伪逆计算。\n\n5) 用于离群值检测的卡方阈值：\n在假设大部分数据服从多变量高斯分布的前提下，马氏距离平方 $D_i^2$ 预期服从自由度为 $p=3$ 的卡方（$\\chi^2$）分布。离群值是那些显著偏离此分布的细胞。我们通过从卡方分布中定义一个阈值 $c_\\alpha$ 来建立决策规则。如果一个细胞 $i$ 的距离超过此阈值，则被标记为低质量离群值：\n$$\nD_i^2 > c_\\alpha\n$$\n阈值 $c_\\alpha$ 是 $\\chi^2_3$ 分布的 $(1-\\alpha)$-分位数，满足 $\\mathbb{P}[\\chi^2_3 \\le c_\\alpha] = 1 - \\alpha$，其中 $\\alpha$ 是显著性水平。该值使用一个标准的科学计算库函数进行数值计算。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef solve():\n    \"\"\"\n    Main function to run the outlier detection algorithm on all test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"n\": [8200, 9100, 7600, 10400, 5000, 4500, 9800, 8700, 9200, 3000, 2800, 2600],\n            \"G\": [2300, 2500, 2100, 2700, 1600, 1500, 2400, 2350, 2450, 900, 850, 800],\n            \"m\": [0.08, 0.07, 0.09, 0.06, 0.12, 0.11, 0.07, 0.08, 0.07, 0.35, 0.40, 0.45],\n            \"alpha\": 0.01\n        },\n        {\n            \"n\": [6000, 6100, 5900, 6050, 5800, 1000],\n            \"G\": [1800, 1750, 1850, 1780, 1820, 400],\n            \"m\": [0.10, 0.11, 0.09, 0.10, 0.10, 0.30],\n            \"alpha\": 0.05\n        },\n        {\n            \"n\": [7000, 7100, 7200, 7300, 6900, 6800, 2000],\n            \"G\": [2000, 2050, 1980, 2020, 2010, 1990, 700],\n            \"m\": [0.10, 0.10, 0.10, 0.10, 0.10, 0.10, 0.35],\n            \"alpha\": 0.05\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        outliers = detect_outliers(case[\"n\"], case[\"G\"], case[\"m\"], case[\"alpha\"])\n        results.append(outliers)\n\n    # Final print statement in the exact required format.\n    # str() adds spaces, so we remove them to match the required 'no extra spaces' format.\n    print(str(results).replace(\" \", \"\"))\n\ndef robust_scale(z: np.ndarray) -> float:\n    \"\"\"\n    Computes the robust scale s(z) = 1.4826 * MAD(z).\n    Handles edge cases where the scale is zero.\n    \"\"\"\n    n_z = len(z)\n    if n_z == 0:\n        return 0.0\n    \n    med = np.median(z)\n    mad = np.median(np.abs(z - med))\n    s = 1.4826 * mad\n\n    if s == 0:\n        if n_z  2:\n            return 1e-8\n        \n        # Fallback to unbiased sample standard deviation\n        s_std = np.std(z, ddof=1)\n        # If std dev is also zero or NaN, use the small constant\n        s = s_std if s_std > 0 else 1e-8\n        \n    return s\n\ndef detect_outliers(n: list, G: list, m: list, alpha: float) -> list:\n    \"\"\"\n    Implements the full robust outlier detection workflow.\n    \"\"\"\n    n_arr, g_arr, m_arr = np.array(n, dtype=float), np.array(G, dtype=float), np.array(m, dtype=float)\n    N = len(n)\n\n    # 1) Feature transformation\n    log_n = np.log(n_arr)\n    log_G = np.log(g_arr)\n    X = np.vstack((log_n, log_G, m_arr)).T\n    p = X.shape[1]\n\n    # 2) Robust location\n    mu_hat = np.median(X, axis=0)\n    \n    # Center the data for covariance calculation\n    X_centered = X - mu_hat\n\n    # 3) Robust Gnanadesikan–Kettenring covariance\n    s = np.array([robust_scale(X_centered[:, j]) for j in range(p)])\n    \n    Sigma_hat = np.zeros((p, p))\n\n    # Diagonal elements\n    for j in range(p):\n        Sigma_hat[j, j] = s[j]**2\n\n    # Off-diagonal elements\n    for j in range(p):\n        for k in range(j + 1, p):\n            s_jk_plus_2 = robust_scale(X_centered[:, j] + X_centered[:, k])**2\n            s_jk_minus_2 = robust_scale(X_centered[:, j] - X_centered[:, k])**2\n            \n            numerator = s_jk_plus_2 - s_jk_minus_2\n            denominator = s_jk_plus_2 + s_jk_minus_2\n            \n            r_jk = 0.0 if denominator == 0 else numerator / denominator\n            Sigma_hat[j, k] = r_jk * s[j] * s[k]\n    \n    # Symmetrize\n    Sigma_hat = 0.5 * (Sigma_hat + Sigma_hat.T)\n\n    # Regularize\n    gamma = 1e-6\n    v_bar = np.mean(np.diag(Sigma_hat))\n    Sigma_hat += gamma * v_bar * np.identity(p)\n\n    # 4) Robust squared Mahalanobis distances\n    # Use Moore-Penrose pseudoinverse for numerical stability\n    try:\n        Sigma_inv = np.linalg.pinv(Sigma_hat)\n    except np.linalg.LinAlgError:\n        # Fallback in case of a non-invertible matrix even after regularization\n        Sigma_inv = np.identity(p)\n\n    mahal_sq_dists = np.zeros(N)\n    for i in range(N):\n        delta = X[i, :] - mu_hat\n        mahal_sq_dists[i] = delta.T @ Sigma_inv @ delta\n\n    # 5) Chi-square cutoff\n    # Quantile function (inverse of CDF) is chi2.ppf in scipy\n    cutoff = chi2.ppf(1 - alpha, df=p)\n    \n    is_outlier = mahal_sq_dists > cutoff\n    \n    return is_outlier.tolist()\n\nsolve()\n```"
        },
        {
            "introduction": "在对高质量细胞进行聚类分析时，一个核心挑战是如何确定数据中存在的最优聚类数量$k$。错误地选择$k$可能会导致将不同的细胞类型合并或将均质的细胞群体分裂。本练习将指导您实现一种基于坚实统计学原理的方法——间隙统计（Gap Statistic），该方法通过将观测数据的聚类紧密度与无结构参考分布下的期望紧密度进行比较，来估计最佳的$k$值 。",
            "id": "4607414",
            "problem": "您的任务是设计并实现一个有原则的算法，该算法在主成分分析（PCA）降维后的空间上使用间隙统计（gap statistic）来选择单细胞数据的聚类数量。解决方案必须以一个完整的、可运行的程序形式呈现。该程序必须能够生成合成的类单细胞数据集，使用主成分分析（PCA）进行降维，对一系列 $k$ 值使用 $k$-均值算法对细胞进行聚类，并计算间隙统计量以选择最佳聚类数 $\\hat{k}$。最终输出必须将预定义测试套件中选定的所有 $\\hat{k}$ 值汇总为单行。\n\n推导和实现必须基于以下基础且经过充分检验的理论：\n\n- 分子生物学的中心法则支持将基因表达水平视为每个细胞的可测量数值特征。对数归一化和方差稳定化是公认的做法，可使聚合特征近似于连续的、类高斯分布。\n- 主成分分析（PCA）被定义为一种正交线性变换，它通过在连续的正交轴上最大化投影方差，将数据映射到低维空间。对于一个中心化的矩阵 $X \\in \\mathbb{R}^{n \\times p}$，PCA可以从奇异值分解（SVD）$X = U \\Sigma V^{\\top}$ 推导得出。\n- $k$-均值算法的目标是最小化簇内离散度。给定 $k$ 个簇 $\\{C_r\\}_{r=1}^k$ 及其质心 $\\{\\mu_r\\}_{r=1}^k$，簇内离散度为 $W_k = \\sum_{r=1}^{k} \\sum_{i \\in C_r} \\lVert x_i - \\mu_r \\rVert_2^2$。\n- 间隙统计将 $\\log(W_k)$ 与其在参考分布下的期望值 $\\mathbb{E}^\\star[\\log(W_k^\\star)]$进行比较，该参考分布是通过在相同范围内的无聚类基线上采样生成的。$k$ 处的间隙定义为 $\\mathrm{Gap}(k) = \\mathbb{E}^\\star[\\log(W_k^\\star)] - \\log(W_k)$。标准选择法则是选择满足 $\\mathrm{Gap}(k) \\ge \\mathrm{Gap}(k+1) - s_{k+1}$ 的最小 $k$，其中 $s_k$ 是 $\\log(W_k^\\star)$ 标准差的估计值，并按 $\\sqrt{1 + 1/B}$ 进行缩放，其中 $B$ 是蒙特卡洛采样的次数。\n\n您的方法应按以下步骤进行：\n\n- 数据生成：应在 $\\mathbb{R}^{p}$ 空间中生成合成数据集，其中包含 $p$ 个基因，其中一个 $s$ 维的信号子集编码了聚类结构，其余 $p - s$ 个基因为噪声。对于每个簇 $r$，从均值为 $\\mu_r \\in \\mathbb{R}^p$ 且协方差为对角矩阵的高斯分布中独立抽取 $n_r$ 个细胞。具体而言：\n  - 信号基因索引是前 $s$ 个坐标。对于簇 $r$，根据一个分离参数 $\\delta$ 和一个固定的几何模式设置其在这些坐标上的均值，并将其余坐标的均值设为 $0$。\n  - 信号坐标上的各基因方差设为 $\\sigma_{\\mathrm{signal}}^2$，噪声坐标上的方差设为 $\\sigma_{\\mathrm{noise}}^2$，各分量独立。\n  - 最终数据集包含 $n = \\sum_{r=1}^{k_{\\text{true}}} n_r$ 个细胞和 $p$ 个基因。\n- 预处理和 PCA：对基因特征进行中心化和z-score标准化（零均值和单位方差）。使用奇异值分解计算PCA，并保留前 $d$ 个主成分。将PCA得分表示为 $Y \\in \\mathbb{R}^{n \\times d}$。\n- 聚类：对于每个候选的 $k \\in \\{k_{\\min}, k_{\\min}+1, \\dots, k_{\\max}\\}$，对 $Y$ 执行 $k$-均值聚类以获得簇分配，并计算 $W_k$ 作为到簇质心的欧几里得距离平方和。\n- 参考分布和间隙统计：对于每个 $k$，通过在 $\\mathbb{R}^{d}$ 空间中 $Y$ 的轴对齐边界框内均匀抽取 $B$ 个独立的参考数据集来估计 $\\mathbb{E}^\\star[\\log(W_k^\\star)]$。对于每个参考数据集，计算 $k$-均值和 $W_k^\\star$。定义\n  $$ \\mathrm{Gap}(k) = \\frac{1}{B}\\sum_{b=1}^{B} \\log\\left(W_{k,b}^\\star\\right) - \\log\\left(W_k\\right), $$\n  并通过以下方式估计标准差\n  $$ s_k = \\sqrt{1 + \\frac{1}{B}} \\cdot \\operatorname{sd}\\left( \\left\\{ \\log\\left(W_{k,b}^\\star\\right) \\right\\}_{b=1}^{B} \\right). $$\n  选择 $\\hat{k}$ 为满足 $\\mathrm{Gap}(k) \\ge \\mathrm{Gap}(k+1) - s_{k+1}$ 的最小 $k$。如果不存在这样的 $k$，则设 $\\hat{k} = k_{\\max}$。如果 $k_{\\min} = k_{\\max}$，则設 $\\hat{k} = k_{\\min}$。\n- 需要证明的假设：解释为什么对PCA得分使用欧几里得 $k$-均值可以近似单细胞的细胞类型簇，为什么PCA空间上的轴对齐均匀参考分布是合适的零假设，为什么在间隙统计中使用对数变换，以及为什么该选择规则可以控制 $k$ 值的过拟合。\n\n您的程序必须实现上述方法，并在以下测试套件上进行评估。对于每个测试案例，数据生成参数都有精确规定。在所有案例中，信号维度固定为 $s = 3$。对于每个簇 $r \\in \\{1,\\dots,k_{\\text{true}}\\}$，在 $\\mathbb{R}^{p}$ 的前 $s$ 个坐标上按如下方式定义其均值向量：\n- 对于 $k_{\\text{true}} = 1$，将均值设为 $(0, 0, 0)$。\n- 对于 $k_{\\text{true}} = 2$，将簇均值设为 $(-\\delta, 0, 0)$ 和 $(+\\delta, 0, 0)$。\n- 对于 $k_{\\text{true}} = 3$，将簇均值设为 $(-\\delta, 0, 0)$、$(+\\delta, 0, 0)$ 和 $(0, +\\delta, 0)$。\n对于坐标 $j  s$，所有簇的均值都设为 $0$。使用对角协方差 $\\operatorname{diag}(\\sigma_{\\mathrm{signal}}^2, \\sigma_{\\mathrm{signal}}^2, \\sigma_{\\mathrm{signal}}^2, \\sigma_{\\mathrm{noise}}^2, \\dots, \\sigma_{\\mathrm{noise}}^2)$ 独立抽取样本。\n\n测试套件包含五个案例。每个案例是一个元组，指定了 $(\\text{seed}, n, p, \\text{cluster\\_sizes}, \\delta, \\sigma_{\\mathrm{signal}}, \\sigma_{\\mathrm{noise}}, d, k_{\\min}, k_{\\max}, B)$，其中所有实体均为整数或适当的实数：\n- 案例 1 (平衡、分离良好的簇，理想路径)：$(7, 600, 50, [200, 200, 200], 4.5, 0.6, 0.4, 10, 1, 6, 20)$。\n- 案例 2 (无簇，单一高斯分布)：$(13, 400, 50, [400], 0.0, 1.0, 1.0, 10, 1, 6, 20)$。\n- 案例 3 (不平衡的簇)：$(23, 500, 50, [50, 150, 300], 4.0, 0.7, 0.4, 10, 1, 6, 20)$。\n- 案例 4 (小样本量，边界行为)：$(31, 60, 30, [30, 30], 3.5, 0.6, 0.5, 5, 1, 4, 12)$。\n- 案例 5 (重叠的簇，具有挑战性)：$(41, 600, 50, [120, 180, 300], 2.6, 1.0, 0.6, 10, 1, 6, 20)$。\n\n实现要求：\n- 使用标准化PCA：在PCA之前，对每个基因在所有细胞中的表达值进行中心化和z-score标准化。通过奇异值分解计算PCA，并保留前 $d$ 个主成分。\n- 执行 $k$-均值算法，使用 $k$-means++ 初始化、多次随机重启，并执行 Lloyd 迭代直至收敛或达到最大迭代次数。\n- 在相同的 $d$ 维空间中，通过在PCA得分的轴对齐边界框上进行 $B$ 次均匀参考复制来计算间隙统计量。\n- 对于每个案例，返回一个整数 $\\hat{k}$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含按顺序排列的每个案例所选的 $\\hat{k}$，格式为逗号分隔的列表并用方括号括起（例如：$[\\hat{k}_1,\\hat{k}_2,\\hat{k}_3,\\hat{k}_4,\\hat{k}_5]$）。程序不得打印任何其他文本。\n\n本问题中的所有量都是无量纲的，不涉及物理单位。不涉及角度。唯一可接受的输出是指定的整数。算法必须仅使用允许的库从头开始实现，并且必须能够根据给定的种子完全复现，无需任何外部输入。",
            "solution": "该问题要求设计并实现一种计算方法，用于确定合成单细胞数据集中最佳的聚类数量 $\\hat{k}$。所指定的方法论整合了数据分析中的几项基础技术：用于降维的主成分分析（PCA），用于聚类的 $k$-均值算法，以及用于模型选择的间隙统计。本文提出的解决方案提供了算法的有原则的推导，证明了关键假设的合理性，并详细说明了实现步骤。\n\n### 理论框架与方法论 justifications\n\n总体目标是从高维基因表达数据中识别不同的细胞群体。我们将其建模为一个聚类问题。聚类的数量 $k$ 是未知的，必须从数据本身进行估计。\n\n**1. 数据模型与生物学基础**\n\n本问题的数据生成过程基于分子生物学的中心法则，该法则允许我们通过基因表达谱来量化细胞状态。每个细胞表示为一个向量 $x \\in \\mathbb{R}^p$，其中 $p$ 是基因数量，向量的分量是它们的表达水平。将不同的细胞类型建模为该高维空间中的独立簇是一种常见且有效的简化方法。假设这些簇遵循高斯分布是一个务实的选择，这反映了许多生物过程在聚合时，由于中心极限定理，会近似于正态分布。因此，整个数据集被建模为高斯混合模型。少数驱动细胞身份的“信号”基因与大量“噪声”基因之间的区别也是单细胞数据的一个现实特征，其中只有一小部分转录组定义了主要的细胞类型。\n\n**2. 通过主成分分析（PCA）进行降维**\n\n单细胞表达数据是出了名的高维（$p$ 值大）和稀疏。PCA是一种线性变换，它沿着一组新的正交轴（称为主成分）重新定向数据，这些主成分按其解释的方差量排序。对于一个已为每个特征中心化至零均值的数据矩阵 $X \\in \\mathbb{R}^{n \\times p}$（其中 $n$ 是细胞数，$p$ 是基因数），PCA可以通过其奇异值分解（SVD）计算得出：\n$$ X_c = U \\Sigma V^{\\top} $$\n其中 $V$ 的列包含主成分。数据在前 $d$ 个主成分上的投影，即主成分得分，由 $Y = X_c V_d = U_d \\Sigma_d$ 给出，其中 $V_d$、$U_d$ 和 $\\Sigma_d$ 是对应于前 $d$ 个成分的截断矩阵。\n\n问题指定在PCA之前对特征进行z-score标准化。这一点至关重要，因为PCA对特征的尺度很敏感。Z-score标准化确保每个基因对初始方差结构的贡献相等，防止高方差基因仅仅因为其尺度而主导主成分。\n\n*使用PCA的理由*：通过保留前 $d$ 个主成分，我们创建了一个低维表示 $Y \\in \\mathbb{R}^{n \\times d}$，它捕捉了数据中最重要的变异轴。其基本假设是，细胞类型簇之间的分离是这种变异的主要来源。因此，簇结构不仅在低维PCA空间中得以保留，而且常常得到增强，同时高维噪声被过滤掉。\n\n**3. 在PCA空间中进行聚类**\n\n聚类在PCA得分 $Y$ 上执行。选择 $k$-均值算法，该算法旨在将 $n$ 个数据点划分为 $k$ 个集合 $C_1, \\dots, C_k$，以最小化簇内平方和（WCSS），也称为离散度：\n$$ W_k = \\sum_{r=1}^{k} \\sum_{y_i \\in C_r} \\lVert y_i - \\mu_r \\rVert_2^2 $$\n其中 $\\mu_r$ 是簇 $C_r$ 的质心。\n\n*对PCA得分使用欧几里得k-均值算法的理由*：到主成分的投影是一个正交变换，如果保留所有成分，它将保留欧几里得距离。当我们降维到 $d$ 个成分时，这个空间中的欧几里得距离 $\\lVert y_i - y_j \\rVert_2$ 成为原始距离的近似值，并由最能分离数据的特征（主成分）的重要性加权。由于簇被建模为径向对称的高斯分布，因此最小化到质心的平方欧几里得距离是识别其中心的自然而有效的方法。\n\n**4. 用于估计 $\\hat{k}$ 的间隙统计**\n\n随着 $k$ 的增加，$W_k$ 的值总是会减小。因此，我们不能简单地选择最小化 $W_k$ 的 $k$。间隙统计提供了一个形式化的统计框架，通过将观察到的 $W_k$ 与在无聚类的零假设下的期望值进行比较来选择 $k$。\n\n其关键组成部分是：\n- **对数变换**：该统计量使用 $\\log(W_k)$ 计算。\n    *理由*：对于缺乏簇的数据（例如，均匀分布），$\\log(W_k)$ 与 $k$ 的关系曲线近似是线性的。取对数将 $W_k$ 的乘性缩放转换为加性效应，这使得偏离“无结构”趋势的偏差更容易被识别。它还有助于稳定方差并处理 $W_k$ 值可能存在的大动态范围。\n\n- **参考分布**：为了估计期望值 $\\mathbb{E}^\\star[\\log(W_k^\\star)]$，我们生成 $B$ 个参考数据集。每个参考数据集都是从一个体现零假设的概率分布中抽取的。\n    *理由*：问题指定了在PCA投影数据 $Y$ 的轴对齐边界框上的均匀分布。这是一个合适的零模型，因为它使用了与观测数据相同的值范围，但破坏了任何潜在的基于密度的结构或分组。它代表了聚类的“最坏情况”，为判断实际数据的聚类趋势提供了一个基准。\n\n- **间隙函数**：间隙定义为零假设下期望的对数离散度与观察到的对数离散度之间的差。\n$$ \\mathrm{Gap}(k) = \\mathbb{E}^\\star[\\log(W_k^\\star)] - \\log(W_k) = \\left( \\frac{1}{B}\\sum_{b=1}^{B} \\log(W_{k,b}^\\star) \\right) - \\log(W_k) $$\n更大的间隙值表明，观察到的数据比随机预期的具有更多的结构（即更具聚类性）。\n\n- **选择法则**：最优的 $\\hat{k}$ 被选为满足以下条件的最小 $k \\in \\{k_{\\min}, \\dots, k_{\\max}-1\\}$：\n$$ \\mathrm{Gap}(k) \\ge \\mathrm{Gap}(k+1) - s_{k+1} $$\n其中 $s_{k+1}$ 是 $\\mathbb{E}^\\star[\\log(W_{k+1}^\\star)]$ 的蒙特卡洛估计的标准误差，并根据样本量 $B$ 进行了调整。\n$$ s_k = \\operatorname{sd}(\\log(W_k^\\star)) \\cdot \\sqrt{1 + 1/B} $$\n*理由*：我们寻求 $\\mathrm{Gap}(k)$ 曲线中的“肘部”——即增加 $k$ 带来的收益递减的点。该法则通过寻找第一个 $k$ 来形式化这一点，使得该 $k$ 的间隙值不显著小于 $k+1$ 处的间隙值。$s_{k+1}$ 项考虑了我们模拟中的统计不确定性。通过要求从 $\\mathrm{Gap}(k)$ 到 $\\mathrm{Gap}(k+1)$ 的改进大于此不确定性，该规则可以防止过拟合，即防止因随机模拟噪声而非真实数据结构而选择更大的 $k$。\n\n### 算法实现\n\n对于每个测试案例，总体算法按以下步骤进行：\n1.  **生成数据**：根据指定的参数（$k_{\\text{true}}$、簇大小、$\\delta$、$\\sigma_{\\text{signal}}$、$\\sigma_{\\text{noise}}$）创建一个大小为 $n \\times p$ 的合成数据集。这涉及从几个具有规定均值和共享对角协方差矩阵的多元正态分布中抽样。\n2.  **预处理和降维**：对生成的数据矩阵 $X$ 的 $p$ 个特征（基因）进行标准化，使其均值为零，方差为一。然后通过计算其SVD对该标准化矩阵执行PCA。保留前 $d$ 个主成分得分，形成矩阵 $Y \\in \\mathbb{R}^{n \\times d}$。\n3.  **计算间隙统计**：对于从 $k_{\\min}$ 到 $k_{\\max}$ 的每个候选聚类数 $k$：\n    a. 使用 $k$-均值算法对实际数据 $Y$ 进行聚类，以获得质心和分配。计算簇内离散度 $W_k$ 并存储 $\\log(W_k)$。使用具有多次初始化（例如，$k$-means++）的稳健 $k$-均值实现。\n    b. 生成 $B$ 个参考数据集。每个数据集都是一个 $n \\times d$ 的矩阵，其中每个点都是从包围 $Y$ 的超矩形中均匀采样的。\n    c. 对于每个参考数据集，使用 $k$ 个簇运行 $k$-均值算法，计算其离散度 $W_{k,b}^\\star$，并存储 $\\log(W_{k,b}^\\star)$。\n    d. 从 $B$ 个对数离散度的集合中计算 $\\mathrm{Gap}(k)$ 和标准误差项 $s_k$。\n4.  **选择最优 $\\hat{k}$**：将选择规则 $\\mathrm{Gap}(k) \\ge \\mathrm{Gap}(k+1) - s_{k+1}$ 应用于计算出的间隙值，以找到估计的聚类数量 $\\hat{k}$。如果条件从未满足，$\\hat{k}$ 默认为 $k_{\\max}$。还处理了 $k_{\\min}=k_{\\max}$ 的边缘情况。记录最终的 $\\hat{k}$。\n此过程对问题陈述中指定的所有测试案例重复进行。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.cluster.vq import kmeans2\nfrom scipy.linalg import svd\n\ndef solve():\n    \"\"\"\n    Main function to run the entire pipeline for all test cases.\n    \"\"\"\n    # Test cases as specified in the problem statement.\n    # (seed, n, p, cluster_sizes, delta, sigma_signal, sigma_noise, d, k_min, k_max, B)\n    test_cases = [\n        (7, 600, 50, [200, 200, 200], 4.5, 0.6, 0.4, 10, 1, 6, 20),\n        (13, 400, 50, [400], 0.0, 1.0, 1.0, 10, 1, 6, 20),\n        (23, 500, 50, [50, 150, 300], 4.0, 0.7, 0.4, 10, 1, 6, 20),\n        (31, 60, 30, [30, 30], 3.5, 0.6, 0.5, 5, 1, 4, 12),\n        (41, 600, 50, [120, 180, 300], 2.6, 1.0, 0.6, 10, 1, 6, 20),\n    ]\n\n    results = []\n    for params in test_cases:\n        seed, n, p, cluster_sizes, delta, sigma_signal, sigma_noise, d, k_min, k_max, B = params\n        \n        # Set seed for reproducibility for this case\n        np.random.seed(seed)\n        \n        # 1. Generate synthetic data\n        X = generate_synthetic_data(n, p, cluster_sizes, delta, sigma_signal, sigma_noise)\n        \n        # 2. Preprocessing and PCA\n        Y = perform_pca(X, d)\n        \n        # 3. Compute Gap Statistic and select k\n        k_hat = find_optimal_k(Y, k_min, k_max, B)\n        results.append(k_hat)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef generate_synthetic_data(n, p, cluster_sizes, delta, sigma_signal, sigma_noise, s=3):\n    \"\"\"\n    Generates synthetic single-cell data based on a Gaussian mixture model.\n    \"\"\"\n    k_true = len(cluster_sizes)\n    \n    # Define cluster means\n    means = np.zeros((k_true, p))\n    if k_true == 1:\n        pass # Mean is already [0, 0, ..., 0]\n    elif k_true == 2:\n        means[0, 0] = -delta\n        means[1, 0] = +delta\n    elif k_true == 3:\n        means[0, 0] = -delta\n        means[1, 0] = +delta\n        means[2, 1] = +delta\n    \n    # Define diagonal covariance matrix\n    variances = np.array([sigma_signal**2] * s + [sigma_noise**2] * (p - s))\n    cov = np.diag(variances)\n    \n    # Generate data for each cluster\n    data_parts = []\n    for i in range(k_true):\n        n_r = cluster_sizes[i]\n        mean_r = means[i, :]\n        cluster_data = np.random.multivariate_normal(mean_r, cov, n_r)\n        data_parts.append(cluster_data)\n        \n    return np.vstack(data_parts)\n\ndef perform_pca(X, d):\n    \"\"\"\n    Performs standardization and PCA on the data matrix X.\n    \"\"\"\n    # Z-score normalization\n    mean = np.mean(X, axis=0)\n    std = np.std(X, axis=0)\n    # Avoid division by zero for constant features\n    std[std == 0] = 1.0\n    X_std = (X - mean) / std\n    \n    # PCA using SVD\n    U, s, Vt = svd(X_std, full_matrices=False)\n    \n    # Project data onto the top d principal components (get scores)\n    Y = U[:, :d] * s[:d]\n    return Y\n\ndef compute_dispersion(Y, k):\n    \"\"\"\n    Performs k-means and computes the within-cluster dispersion W_k.\n    \"\"\"\n    # k-means++-like initialization and 10 restarts\n    centroids, labels = kmeans2(Y, k, iter=10, minit='points')\n    \n    W_k = 0.0\n    for i in range(k):\n        cluster_points = Y[labels == i, :]\n        if cluster_points.shape[0] > 0:\n            W_k += np.sum((cluster_points - centroids[i, :])**2)\n            \n    return W_k\n\ndef find_optimal_k(Y, k_min, k_max, B):\n    \"\"\"\n    Computes the gap statistic and selects the optimal k.\n    \"\"\"\n    if k_min == k_max:\n        return k_min\n        \n    n, d = Y.shape\n    \n    log_W = np.zeros(k_max - k_min + 1)\n    gap_stats = np.zeros(k_max - k_min + 1)\n    s_k_vals = np.zeros(k_max - k_min + 1)\n    \n    # Bounding box for reference data generation\n    min_coords = np.min(Y, axis=0)\n    max_coords = np.max(Y, axis=0)\n    \n    k_range = range(k_min, k_max + 1)\n    \n    for i, k in enumerate(k_range):\n        # Dispersion for the actual data\n        log_W[i] = np.log(compute_dispersion(Y, k))\n        \n        # Dispersions for reference data\n        log_W_star_b = np.zeros(B)\n        for b in range(B):\n            Y_star = np.random.uniform(low=min_coords, high=max_coords, size=(n, d))\n            # Handle potential failure of k-means on uniform data for small k\n            try:\n                log_W_star_b[b] = np.log(compute_dispersion(Y_star, k))\n            except Exception:\n                # If k-means fails, e.g., creates an empty cluster, this run is invalid.\n                # A robust but complex solution would be to retry. A simpler one is to ignore it.\n                # Given the context, we can assume it's rare and fill with a neighbor or mean.\n                # For simplicity, we re-use the last valid value or 0 if none.\n                if b > 0:\n                    log_W_star_b[b] = log_W_star_b[b-1]\n                else:\n                    log_W_star_b[b] = 0 # should not happen often\n        \n        E_log_W_star = np.mean(log_W_star_b)\n        sd_log_W_star = np.std(log_W_star_b, ddof=0) # ddof=0 for population std\n        \n        gap_stats[i] = E_log_W_star - log_W[i]\n        s_k_vals[i] = sd_log_W_star * np.sqrt(1.0 + 1.0 / B)\n\n    # Selection rule\n    k_hat = k_max\n    for i in range(len(k_range) - 1):\n        k = k_range[i]\n        if gap_stats[i] >= gap_stats[i+1] - s_k_vals[i+1]:\n            k_hat = k\n            break\n            \n    return k_hat\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "一旦细胞被划分为不同的聚类，下一步就是解释这些聚类的生物学意义，即鉴定细胞类型。这一步通常通过寻找在特定聚类中特异性高表达的“标记基因”（marker genes）来完成。本练习将向您展示如何使用受试者工作特征曲线下面积（Area Under the ROC Curve, AUC）来量化一个基因区分两个细胞群体的能力，从而为鉴定标记基因提供一个可靠且定量的指标 。",
            "id": "4607367",
            "problem": "您正在分析一个单细胞核糖核酸测序 (scRNA-seq) 数据集，其中无监督聚类识别出两个簇，表示为 $\\mathcal{C}_{1}$ 和 $\\mathcal{C}_{0}$。一个候选标记基因 $g$ 被假设在 $\\mathcal{C}_{1}$ 中相对于 $\\mathcal{C}_{0}$ 是上调的，您希望使用受试者工作特征 (ROC) 曲线下面积来量化其区分能力。受试者工作特征 (ROC) 曲线比较了当一个标量分数的阈值变化时，真阳性率和假阳性率的变化情况；在这里，标量分数是基因 $g$ 的对数归一化表达量。将 $\\mathcal{C}_{1}$ 视为阳性类别，$\\mathcal{C}_{0}$ 视为阴性类别。一部分细胞中基因 $g$ 的对数归一化表达值如下：\n- $\\mathcal{C}_{1}$ (推定的标记阳性): $2.5$, $3.1$, $1.8$, $2.2$, $3.7$, $2.9$.\n- $\\mathcal{C}_{0}$ (推定的标记阴性): $0.6$, $1.1$, $1.4$, $1.9$, $2.3$.\n从 ROC 曲线关于真阳性率和假阳性率作为应用于标量分数的阈值的函数的基本定义出发，推导在这种有限样本情况下 ROC 曲线下面积 (AUC) 的表达式，并为上述数据计算其值。使用基于定义的概率解释，简要说明这个值对基因 $g$ 在 $\\mathcal{C}_{1}$ 和 $\\mathcal{C}_{0}$ 之间的区分能力意味着什么。将 AUC 报告为 $[0,1]$ 内的一个标量，并将您的最终数值答案四舍五入到四位有效数字。",
            "solution": "该问题要求计算和解释候选标记基因 $g$ 的受试者工作特征曲线下面积 (AUC)，给定其在两个细胞簇 $\\mathcal{C}_{1}$ (阳性类别) 和 $\\mathcal{C}_{0}$ (阴性类别) 中的对数归一化表达值。\n\n首先，让我们形式化受试者工作特征 (ROC) 曲线及其下面积的概念。ROC 曲线是一个图形图，它展示了当一个二元分类器系统的判别阈值变化时其诊断能力。它是通过在不同的阈值设置下，绘制真阳性率 ($TPR$) 相对于假阳性率 ($FPR$) 的曲线来创建的。\n\n令 $S$ 为用于分类的标量分数，在本问题中即为基因 $g$ 的对数归一化表达量。令 $\\tau$ 为分类阈值。如果一个样本的分数 $S  \\tau$，则它被分类为阳性。\n阳性类别是 $\\mathcal{C}_{1}$，包含 $N_1$ 个样本。阴性类别是 $\\mathcal{C}_{0}$，包含 $N_0$ 个样本。\n真阳性率，或称灵敏度，是被正确分类为阳性的阳性样本的比例：\n$$TPR(\\tau) = \\frac{\\text{Number of samples in } \\mathcal{C}_{1} \\text{ with score }  \\tau}{N_1}$$\n假阳性率，或称 ($1$ - 特异性)，是被错误分类为阳性的阴性样本的比例：\n$$FPR(\\tau) = \\frac{\\text{Number of samples in } \\mathcal{C}_{0} \\text{ with score }  \\tau}{N_0}$$\nROC 曲线是对于所有可能的 $\\tau \\in (-\\infty, \\infty)$ 值，点集 $(FPR(\\tau), TPR(\\tau))$。此曲线下面积，即 AUC，由以下积分给出：\n$$AUC = \\int_0^1 TPR(x) dx$$\n其中积分变量 $x$ 代表 $FPR$。\n\n一个基本结果为 AUC 提供了一个概率解释。AUC 等于从阳性类别中随机抽取的一个样本的分数高于从阴性类别中随机抽取的一个样本的分数的概率。令 $S_1$ 为从 $\\mathcal{C}_1$ 中随机抽取的一个样本的分数， $S_0$ 为从 $\\mathcal{C}_0$ 中随机抽取的一个样本的分数。那么，\n$$AUC = P(S_1  S_0)$$\n在可能出现平局（即 $S_1 = S_0$）的情况下，标准定义将其调整为 $AUC = P(S_1  S_0) + \\frac{1}{2} P(S_1 = S_0)$。\n\n对于没有平局的有限样本集，这个概率可以通过 Wilcoxon-Mann-Whitney U 统计量进行非参数估计，这涉及到计算所有可能的样本对（每类一个）的数量。令来自 $\\mathcal{C}_1$ 的分数组为 $X_1 = \\{x_{1,i}\\}_{i=1}^{N_1}$，来自 $\\mathcal{C}_0$ 的分数组为 $X_0 = \\{x_{0,j}\\}_{j=1}^{N_0}$。AUC 的计算公式如下：\n$$AUC = \\frac{1}{N_1 N_0} \\sum_{i=1}^{N_1} \\sum_{j=1}^{N_0} \\mathbb{I}(x_{1,i}, x_{0,j})$$\n其中\n$$ \\mathbb{I}(a, b) = \\begin{cases} 1  \\text{if } a  b \\\\ 0.5  \\text{if } a = b \\\\ 0  \\text{if } a  b \\end{cases} $$\n这个表达式为计算给定有限数据集的 AUC 提供了最直接的方法。\n\n现在，我们将此方法应用于所提供的数据。\n阳性类别是 $\\mathcal{C}_{1}$，有 $N_1 = 6$ 个样本。表达值为 $X_1 = \\{2.5, 3.1, 1.8, 2.2, 3.7, 2.9\\}$。\n阴性类别是 $\\mathcal{C}_{0}$，有 $N_0 = 5$ 个样本。表达值为 $X_0 = \\{0.6, 1.1, 1.4, 1.9, 2.3\\}$。\n\n一个来自 $\\mathcal{C}_{1}$ 的细胞和一个来自 $\\mathcal{C}_{0}$ 的细胞组成的对的总数为 $N_1 \\times N_0 = 6 \\times 5 = 30$。\n两个集合之间的表达值没有相同的值，因此 $\\mathbb{I}(a,b)$ 函数只会取 $1$ 或 $0$。我们需要计算满足 $x_{1,i}  x_{0,j}$ 的对 $(x_{1,i}, x_{0,j})$ 的数量。\n\n让我们系统地进行成对比较：\n对于每个 $x_{1,i} \\in X_1$，我们计算有多少个 $x_{0,j} \\in X_0$ 比它小。\n\\begin{itemize}\n    \\item 对于 $x_{1,i} = 2.5$：它大于 $X_0 = \\{0.6, 1.1, 1.4, 1.9, 2.3\\}$ 中的所有 $5$ 个值。计数为 $5$。\n    \\item 对于 $x_{1,i} = 3.1$：它大于 $X_0$ 中的所有 $5$ 个值。计数为 $5$。\n    \\item 对于 $x_{1,i} = 1.8$：它大于 $\\{0.6, 1.1, 1.4\\}$。计数为 $3$。\n    \\item 对于 $x_{1,i} = 2.2$：它大于 $\\{0.6, 1.1, 1.4, 1.9\\}$。计数为 $4$。\n    \\item 对于 $x_{1,i} = 3.7$：它大于 $X_0$ 中的所有 $5$ 个值。计数为 $5$。\n    \\item 对于 $x_{1,i} = 2.9$：它大于 $X_0$ 中的所有 $5$ 个值。计数为 $5$。\n\\end{itemize}\n\n$\\mathcal{C}_1$ 的分数大于 $\\mathcal{C}_0$ 的分数的对的总数是这些计数的总和：\n$$ \\text{Sum} = 5 + 5 + 3 + 4 + 5 + 5 = 27 $$\n现在，我们可以计算 AUC：\n$$ AUC = \\frac{27}{30} = \\frac{9}{10} = 0.9 $$\n问题要求答案四舍五入到四位有效数字，所以 $0.9$ 写为 $0.9000$。\n\n解释：AUC 值是衡量分类器性能的指标，范围从 $0$ 到 $1$。AUC 为 $0.5$ 对应于没有区分能力的随机分类器。AUC 为 $1.0$ 对应于完美分类器。计算出的 AUC 为 $0.9$，非常接近 $1.0$，这表明基因 $g$ 的表达水平在区分簇 $\\mathcal{C}_{1}$ 和 $\\mathcal{C}_{0}$ 的细胞方面具有出色的区分能力。\n根据概率解释，AUC 为 $0.9$ 意味着如果我们从 $\\mathcal{C}_{1}$ 中随机选择一个细胞，并从 $\\mathcal{C}_{0}$ 中随机选择一个细胞，那么有 $90\\%$ 的概率 $\\mathcal{C}_{1}$ 的细胞将具有更高的基因 $g$ 对数归一化表达量。这有力地支持了 $g$ 是在 $\\mathcal{C}_{1}$ 中上调的标记基因的假设。",
            "answer": "$$\\boxed{0.9000}$$"
        }
    ]
}