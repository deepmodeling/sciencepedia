{
    "hands_on_practices": [
        {
            "introduction": "Integrating omics data with clinical phenotypes begins with rigorous data processing to remove technical artifacts. In RNA-sequencing, the total number of reads per sample, or library size, can vary significantly for reasons unrelated to biology, confounding comparisons. This exercise will guide you through deriving and applying the robust median-of-ratios normalization method, a cornerstone of modern differential expression analysis, to ensure that downstream modeling reflects true biological variation rather than technical noise. ",
            "id": "4574649",
            "problem": "You are integrating Ribonucleic Acid sequencing (RNA-seq) transcript abundance with a binary clinical phenotype to enable downstream association modeling using a Generalized Linear Model (GLM). You observe an RNA-seq count matrix $X \\in \\mathbb{R}^{n \\times p}$ with $n$ genes and $p$ samples, where the $g$-th row corresponds to gene $g$ and the $j$-th column corresponds to sample $j$. Suppose $n = 4$ and $p = 3$, and the observed counts are\n$$\nX \\;=\\; \\begin{pmatrix}\n100 & 200 & 50 \\\\\n50 & 100 & 25 \\\\\n20 & 40 & 10 \\\\\n80 & 160 & 40\n\\end{pmatrix}.\n$$\nYou also record a binary phenotype vector $y \\in \\{0,1\\}^{p}$ indicating disease status for the samples, with $y = (0, 1, 0)$, and plan to fit a log-link GLM after between-sample normalization to remove multiplicative library size effects.\n\nStarting only from the fundamental modeling principle that the observed counts for gene $g$ in sample $j$ arise from a multiplicative decomposition of a sample-specific library size factor $s_{j} > 0$ and a gene- and sample-specific underlying expression level $\\theta_{g,j} > 0$, and that a robust normalizing procedure should be invariant to per-gene scaling across samples, derive a robust estimator of the library size factors $\\{s_{j}\\}_{j=1}^{p}$ consistent with these principles. Then, using this estimator, compute the normalized counts $\\tilde{X}$ via element-wise division of $X$ by the corresponding library size factors.\n\nFinally, report the single normalized count value $\\tilde{x}_{4,2}$, that is, the normalized count for gene $g=4$ in sample $j=2$. Express your final answer as a pure number with no units. No rounding is required.",
            "solution": "The problem requires the derivation and application of a robust estimator for library size factors in RNA-seq data analysis, based on fundamental principles.\n\nLet the observed count for gene $g$ in sample $j$ be denoted by $x_{g,j}$, where $g \\in \\{1, \\dots, n\\}$ and $j \\in \\{1, \\dots, p\\}$. The problem provides a count matrix $X \\in \\mathbb{R}^{n \\times p}$ with $n=4$ genes and $p=3$ samples.\n\nThe fundamental modeling principle states a multiplicative decomposition for the expected count:\n$$ E[x_{g,j}] \\approx s_{j} \\theta_{g,j} $$\nwhere $s_{j} > 0$ is the sample-specific library size factor and $\\theta_{g,j} > 0$ is the underlying expression level for gene $g$ in sample $j$.\n\nThe second principle is that the estimation procedure for the set of size factors $\\{s_j\\}_{j=1}^p$ must be invariant to per-gene scaling. That is, if we construct a new count matrix $X'$ where each row $g$ is scaled by a constant $c_g > 0$ such that $x'_{g,j} = c_g x_{g,j}$, the resulting size factor estimates $s'_j$ should be identical to the original estimates $s_j$.\n\nTo derive an estimator satisfying these properties, we construct a pseudo-reference sample that serves as a common baseline. A robust choice for the expression level of gene $g$ in this pseudo-reference sample, which we denote $\\mu_g$, is the geometric mean of the counts for gene $g$ across all samples:\n$$ \\mu_g = \\left( \\prod_{k=1}^{p} x_{g,k} \\right)^{1/p} $$\nLet's verify how this term behaves under per-gene scaling. If $x'_{g,j} = c_g x_{g,j}$, the new geometric mean $\\mu'_g$ is:\n$$ \\mu'_g = \\left( \\prod_{k=1}^{p} x'_{g,k} \\right)^{1/p} = \\left( \\prod_{k=1}^{p} c_g x_{g,k} \\right)^{1/p} = \\left( c_g^p \\prod_{k=1}^{p} x_{g,k} \\right)^{1/p} = c_g \\left( \\prod_{k=1}^{p} x_{g,k} \\right)^{1/p} = c_g \\mu_g $$\nSo, the pseudo-reference value for gene $g$ scales by the same factor $c_g$.\n\nNow, for each gene $g$ in each sample $j$, we compute the ratio of its observed count to the pseudo-reference count for that gene:\n$$ r_{g,j} = \\frac{x_{g,j}}{\\mu_g} $$\nIf we consider the multiplicative model, this ratio is approximately:\n$$ r_{g,j} \\approx \\frac{s_{j} \\theta_{g,j}}{\\mu_g} $$\nFor many genes, their expression levels might not vary dramatically across the biological conditions represented by the samples, meaning $\\theta_{g,j}$ is relatively constant for a given $g$. In this scenario, $\\mu_g$ can be seen as an estimate of a quantity proportional to this common expression level. The ratio $r_{g,j}$ would then be approximately proportional to $s_j$.\n\nCrucially, let's examine the behavior of this ratio under per-gene scaling:\n$$ r'_{g,j} = \\frac{x'_{g,j}}{\\mu'_g} = \\frac{c_g x_{g,j}}{c_g \\mu_g} = \\frac{x_{g,j}}{\\mu_g} = r_{g,j} $$\nThe ratios are invariant to the per-gene scaling factors $c_g$.\n\nFor a given sample $j$, we now have a set of $n$ ratios, $\\{r_{1,j}, r_{2,j}, \\dots, r_{n,j}\\}$. Each of these ratios is an estimate of the size factor $s_j$. To obtain a single, robust estimate for $s_j$, we take the median of this set of ratios. The median is robust to outliers, such as genes that are strongly differentially expressed, for which the assumption $\\theta_{g,j} \\approx \\theta_{g,k}$ might not hold.\nTherefore, our robust estimator for the size factor of sample $j$ is:\n$$ s_j = \\underset{g}{\\text{median}} \\left\\{ \\frac{x_{g,j}}{\\left( \\prod_{k=1}^{p} x_{g,k} \\right)^{1/p}} \\right\\} $$\nThis is known as the median-of-ratios method.\n\nWe now apply this procedure to the given data. The count matrix is:\n$$\nX = \\begin{pmatrix}\n100 & 200 & 50 \\\\\n50 & 100 & 25 \\\\\n20 & 40 & 10 \\\\\n80 & 160 & 40\n\\end{pmatrix}\n$$\nwith $n=4$ and $p=3$.\n\nFirst, we compute the geometric mean $\\mu_g$ for each gene (row):\n$$ \\mu_1 = (100 \\times 200 \\times 50)^{1/3} = (1000000)^{1/3} = 100 $$\n$$ \\mu_2 = (50 \\times 100 \\times 25)^{1/3} = (125000)^{1/3} = 50 $$\n$$ \\mu_3 = (20 \\times 40 \\times 10)^{1/3} = (8000)^{1/3} = 20 $$\n$$ \\mu_4 = (80 \\times 160 \\times 40)^{1/3} = (512000)^{1/3} = 80 $$\n\nNext, we compute the matrix of ratios $R$, where $r_{g,j} = x_{g,j} / \\mu_g$:\n$$\nR = \\begin{pmatrix}\n100/100 & 200/100 & 50/100 \\\\\n50/50 & 100/50 & 25/50 \\\\\n20/20 & 40/20 & 10/20 \\\\\n80/80 & 160/80 & 40/80\n\\end{pmatrix}\n= \\begin{pmatrix}\n1.0 & 2.0 & 0.5 \\\\\n1.0 & 2.0 & 0.5 \\\\\n1.0 & 2.0 & 0.5 \\\\\n1.0 & 2.0 & 0.5\n\\end{pmatrix}\n$$\n\nNow, we estimate the size factor $s_j$ for each sample $j$ by taking the median of the corresponding column in $R$:\n$$ s_1 = \\text{median}\\{1.0, 1.0, 1.0, 1.0\\} = 1.0 $$\n$$ s_2 = \\text{median}\\{2.0, 2.0, 2.0, 2.0\\} = 2.0 $$\n$$ s_3 = \\text{median}\\{0.5, 0.5, 0.5, 0.5\\} = 0.5 $$\nThe vector of size factors is $(s_1, s_2, s_3) = (1.0, 2.0, 0.5)$.\n\nFinally, we compute the normalized count matrix $\\tilde{X}$ by dividing each element $x_{g,j}$ by its corresponding sample's size factor $s_j$: $\\tilde{x}_{g,j} = x_{g,j} / s_j$. This is equivalent to dividing each column of $X$ by the corresponding size factor.\n$$\n\\tilde{X} = \\begin{pmatrix}\n100/1.0 & 200/2.0 & 50/0.5 \\\\\n50/1.0 & 100/2.0 & 25/0.5 \\\\\n20/1.0 & 40/2.0 & 10/0.5 \\\\\n80/1.0 & 160/2.0 & 40/0.5\n\\end{pmatrix}\n= \\begin{pmatrix}\n100 & 100 & 100 \\\\\n50 & 50 & 50 \\\\\n20 & 20 & 20 \\\\\n80 & 80 & 80\n\\end{pmatrix}\n$$\n\nThe problem asks for the normalized count value $\\tilde{x}_{4,2}$, which is the element in the 4th row and 2nd column of $\\tilde{X}$.\n$$ \\tilde{x}_{4,2} = \\frac{x_{4,2}}{s_2} = \\frac{160}{2.0} = 80 $$\nFrom the computed matrix $\\tilde{X}$, we can directly read this value.\n\nThe phenotype vector $y=(0, 1, 0)$ is provided to establish the context of a downstream analysis (a GLM), but it is not used in the normalization calculation itself, which is a prerequisite step.\nThe final required value is $\\tilde{x}_{4,2}$.",
            "answer": "$$\n\\boxed{80}\n$$"
        },
        {
            "introduction": "Bulk tissue expression profiles represent an average signal from a complex mixture of different cell types, which can obscure cell-specific responses to disease. This practice demonstrates how to computationally \"deconvolve\" this mixed signal using a single-cell reference matrix to estimate the underlying cell-type proportions. By implementing a deconvolution pipeline based on nonnegative least squares, you will learn to derive biologically interpretable features that can serve as powerful biomarkers for association with clinical phenotypes. ",
            "id": "4574619",
            "problem": "You are given a framework for integrating Ribonucleic Acid sequencing (RNA-seq) bulk gene expression data with clinical phenotypes by estimating cell-type proportions from a single-cell reference using the linear mixing model and nonnegative least squares. The principle is that bulk expression is a mixture of cell-type-specific expression profiles, consistent with the Central Dogma of Molecular Biology and additivity of transcript counts across constituent cell types within a tissue sample. This yields a linear model: for $G$ genes and $C$ cell types, the bulk expression vector $\\mathbf{y} \\in \\mathbb{R}^{G}$ is modeled as\n$$\n\\mathbf{y} = X \\mathbf{p} + \\boldsymbol{\\varepsilon},\n$$\nwhere $X \\in \\mathbb{R}^{G \\times C}$ is the reference matrix of mean expression for each gene in each cell type derived from single-cell measurements, $\\mathbf{p} \\in \\mathbb{R}^{C}$ is the unknown nonnegative vector of cell-type proportions, and $\\boldsymbol{\\varepsilon} \\in \\mathbb{R}^{G}$ is measurement noise. The biological constraints imply $p_c \\geq 0$ for all $c$ and, under the interpretation of $\\mathbf{p}$ as fractions, $\\sum_{c=1}^{C} p_c = 1$. In practice, one can estimate $\\mathbf{p}$ by solving the nonnegative least squares problem\n$$\n\\min_{\\mathbf{p} \\in \\mathbb{R}^{C}} \\|\\;X \\mathbf{p} - \\mathbf{y}\\;\\|_2 \\quad \\text{subject to} \\quad p_c \\geq 0 \\text{ for all } c,\n$$\nand then renormalize $\\mathbf{p}$ to have unit sum, because least squares does not enforce the sum-to-one constraint. Accuracy can be evaluated against simulated mixtures with known proportions.\n\nImplement a program that does the following for the provided test suite:\n- For each test case, construct the specified single-cell reference matrix $X$, true proportions $\\mathbf{p}^{\\star}$ (or its variant), and bulk vector $\\mathbf{y}$ according to the case description.\n- Estimate $\\widehat{\\mathbf{p}}$ by solving the nonnegative least squares problem and renormalize it to satisfy $\\sum_{c=1}^{C} \\widehat{p}_c = 1$ when $\\sum_{c=1}^{C} \\widehat{p}_c > 0$, otherwise set $\\widehat{p}_c = \\frac{1}{C}$ for all $c$.\n- Compute the root mean squared error (RMSE) between $\\widehat{\\mathbf{p}}$ and the specified ground-truth proportions for each case, using\n$$\n\\mathrm{RMSE}(\\widehat{\\mathbf{p}}, \\mathbf{p}^{\\dagger}) = \\sqrt{\\frac{1}{C} \\sum_{c=1}^{C} \\left(\\widehat{p}_c - p^{\\dagger}_c\\right)^2}.\n$$\n- For the clinical phenotype case, simulate a small cohort of bulk samples with known proportions and compute the Pearson correlation coefficient between the estimated fraction for a specified cell type and a clinical severity score vector $\\mathbf{s}$. Use the formula\n$$\nr = \\frac{\\sum_{t=1}^{T} \\left(\\widehat{p}_{j}^{(t)} - \\overline{\\widehat{p}_{j}}\\right)\\left(s^{(t)} - \\overline{s}\\right)}{\\sqrt{\\sum_{t=1}^{T}\\left(\\widehat{p}_{j}^{(t)} - \\overline{\\widehat{p}_{j}}\\right)^2}\\sqrt{\\sum_{t=1}^{T}\\left(s^{(t)} - \\overline{s}\\right)^2}},\n$$\nwhere $\\widehat{p}_{j}^{(t)}$ is the estimated fraction of cell type $j$ for sample $t$, $\\overline{\\widehat{p}_{j}}$ is its mean across samples, $s^{(t)}$ is the clinical severity for sample $t$, and $\\overline{s}$ is its mean. Express proportions as decimals, not percentages.\n\nTest Suite Specification:\n- All matrices and vectors are given explicitly. Use them exactly as specified without any additional normalization or transformation beyond what is described above. All numerical values are decimals, and any derived values should also be treated as decimals.\n\nCase $1$ (well-conditioned reference, noiseless mixture):\n- Genes $G = 5$, cell types $C = 3$.\n- Reference matrix\n$$\nX_1 = \\begin{bmatrix}\n8 & 2 & 1 \\\\\n4 & 1.5 & 2 \\\\\n3 & 3 & 6 \\\\\n5 & 1 & 0.5 \\\\\n7 & 2.5 & 1\n\\end{bmatrix}.\n$$\n- True proportions\n$$\n\\mathbf{p}^{\\star}_1 = \\begin{bmatrix} 0.5 \\\\ 0.3 \\\\ 0.2 \\end{bmatrix}.\n$$\n- Bulk vector\n$$\n\\mathbf{y}_1 = X_1 \\mathbf{p}^{\\star}_1 = \\begin{bmatrix} 4.8 \\\\ 2.85 \\\\ 3.6 \\\\ 2.9 \\\\ 4.45 \\end{bmatrix}.\n$$\n- Ground-truth for RMSE: $\\mathbf{p}^{\\dagger} = \\mathbf{p}^{\\star}_1$.\n\nCase $2$ (near-collinearity between cell-type profiles):\n- Reference matrix\n$$\nX_2 = \\begin{bmatrix}\n6 & 6.2 & 1 \\\\\n3 & 3.1 & 2 \\\\\n5 & 5.1 & 6 \\\\\n4 & 4.05 & 0.5 \\\\\n2 & 2.02 & 1\n\\end{bmatrix}.\n$$\n- True proportions\n$$\n\\mathbf{p}^{\\star}_2 = \\begin{bmatrix} 0.4 \\\\ 0.4 \\\\ 0.2 \\end{bmatrix}.\n$$\n- Bulk vector\n$$\n\\mathbf{y}_2 = X_2 \\mathbf{p}^{\\star}_2 = \\begin{bmatrix} 5.08 \\\\ 2.84 \\\\ 5.24 \\\\ 3.32 \\\\ 1.808 \\end{bmatrix}.\n$$\n- Ground-truth for RMSE: $\\mathbf{p}^{\\dagger} = \\mathbf{p}^{\\star}_2$.\n\nCase $3$ (unmodeled cell type present in bulk):\n- Use $X_3 = X_1$.\n- Known cell-type proportions (do not sum to one)\n$$\n\\mathbf{p}^{\\text{known}}_3 = \\begin{bmatrix} 0.5 \\\\ 0.25 \\\\ 0.15 \\end{bmatrix}, \\quad \\text{so} \\quad \\sum_{c=1}^{3} p^{\\text{known}}_{3,c} = 0.9.\n$$\n- Unknown cell type fraction is $0.1$ with expression vector\n$$\n\\mathbf{u} = \\begin{bmatrix} 4 \\\\ 1 \\\\ 3 \\\\ 2 \\\\ 5 \\end{bmatrix}.\n$$\n- Bulk vector\n$$\n\\mathbf{y}_3 = X_3 \\mathbf{p}^{\\text{known}}_3 + 0.1 \\cdot \\mathbf{u} = \\begin{bmatrix} 5.05 \\\\ 2.775 \\\\ 3.45 \\\\ 3.025 \\\\ 4.775 \\end{bmatrix}.\n$$\n- Ground-truth for RMSE: normalize the known proportions to sum to one,\n$$\n\\mathbf{p}^{\\dagger} = \\frac{\\mathbf{p}^{\\text{known}}_3}{0.9} = \\begin{bmatrix} 0.5555555556 \\\\ 0.2777777778 \\\\ 0.1666666667 \\end{bmatrix}.\n$$\n\nCase $4$ (measurement noise):\n- Use $X_4 = X_1$.\n- True proportions\n$$\n\\mathbf{p}^{\\star}_4 = \\begin{bmatrix} 0.3 \\\\ 0.5 \\\\ 0.2 \\end{bmatrix}.\n$$\n- Noiseless bulk\n$$\nX_4 \\mathbf{p}^{\\star}_4 = \\begin{bmatrix} 3.6 \\\\ 2.35 \\\\ 3.6 \\\\ 2.1 \\\\ 3.55 \\end{bmatrix}.\n$$\n- Additive noise vector\n$$\n\\boldsymbol{\\eta} = \\begin{bmatrix} 0.05 \\\\ -0.02 \\\\ 0.03 \\\\ -0.04 \\\\ 0.01 \\end{bmatrix}.\n$$\n- Bulk vector\n$$\n\\mathbf{y}_4 = X_4 \\mathbf{p}^{\\star}_4 + \\boldsymbol{\\eta} = \\begin{bmatrix} 3.65 \\\\ 2.33 \\\\ 3.63 \\\\ 2.06 \\\\ 3.56 \\end{bmatrix}.\n$$\n- Ground-truth for RMSE: $\\mathbf{p}^{\\dagger} = \\mathbf{p}^{\\star}_4$.\n\nCase $5$ (library-size scaling mismatch):\n- Use $X_5 = X_1$.\n- True proportions\n$$\n\\mathbf{p}^{\\star}_5 = \\mathbf{p}^{\\star}_1 = \\begin{bmatrix} 0.5 \\\\ 0.3 \\\\ 0.2 \\end{bmatrix}.\n$$\n- Bulk vector is a scaled version of Case $1$,\n$$\n\\mathbf{y}_5 = 2 \\cdot \\mathbf{y}_1 = \\begin{bmatrix} 9.6 \\\\ 5.7 \\\\ 7.2 \\\\ 5.8 \\\\ 8.9 \\end{bmatrix}.\n$$\n- Ground-truth for RMSE: $\\mathbf{p}^{\\dagger} = \\mathbf{p}^{\\star}_5$.\n\nCase $6$ (integration with a clinical severity phenotype):\n- Use $X_6 = X_1$.\n- Number of samples $T = 6$.\n- True proportions for each sample (columns indicate cell types $1,2,3$):\n$$\n\\mathbf{p}^{\\star (1)} = \\begin{bmatrix} 0.2 \\\\ 0.5 \\\\ 0.3 \\end{bmatrix},\\quad\n\\mathbf{p}^{\\star (2)} = \\begin{bmatrix} 0.1 \\\\ 0.5 \\\\ 0.4 \\end{bmatrix},\\quad\n\\mathbf{p}^{\\star (3)} = \\begin{bmatrix} 0.05 \\\\ 0.45 \\\\ 0.5 \\end{bmatrix},\\quad\n\\mathbf{p}^{\\star (4)} = \\begin{bmatrix} 0.3 \\\\ 0.6 \\\\ 0.1 \\end{bmatrix},\\quad\n\\mathbf{p}^{\\star (5)} = \\begin{bmatrix} 0.05 \\\\ 0.35 \\\\ 0.6 \\end{bmatrix},\\quad\n\\mathbf{p}^{\\star (6)} = \\begin{bmatrix} 0.15 \\\\ 0.55 \\\\ 0.3 \\end{bmatrix}.\n$$\n- Clinical severity scores vector\n$$\n\\mathbf{s} = \\begin{bmatrix} 0.3 \\\\ 0.5 \\\\ 0.7 \\\\ 0.2 \\\\ 0.9 \\\\ 0.4 \\end{bmatrix}.\n$$\n- For each sample $t \\in \\{1,\\dots,6\\}$, bulk vector is\n$$\n\\mathbf{y}_6^{(t)} = X_6 \\mathbf{p}^{\\star (t)} + \\boldsymbol{\\delta},\n$$\nwith a small fixed additive noise\n$$\n\\boldsymbol{\\delta} = \\begin{bmatrix} 0.01 \\\\ -0.01 \\\\ 0.0 \\\\ 0.02 \\\\ -0.02 \\end{bmatrix}.\n$$\n- Estimate $\\widehat{\\mathbf{p}}^{(t)}$ for each sample and compute the Pearson correlation coefficient $r$ between the estimated fractions of cell type $3$ (the third component of $\\widehat{\\mathbf{p}}^{(t)}$) and the vector $\\mathbf{s}$. Report $r$ as a decimal.\n\nOutput specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order: $\\mathrm{RMSE}$ for Case $1$, $\\mathrm{RMSE}$ for Case $2$, $\\mathrm{RMSE}$ for Case $3$, $\\mathrm{RMSE}$ for Case $4$, $\\mathrm{RMSE}$ for Case $5$, and the Pearson correlation $r$ for Case $6$. Each value must be rounded to $6$ decimal places. For example, the output should look like\n$$\n[\\text{value}_1,\\text{value}_2,\\text{value}_3,\\text{value}_4,\\text{value}_5,\\text{value}_6].\n$$\nAll proportions are to be interpreted as decimals (fractions between $0$ and $1$), not percentages.",
            "solution": "The starting point is the linear mixing model for bulk gene expression. Since transcript counts from different cell types add, a bulk measurement for gene $g$ can be expressed as\n$$\ny_g = \\sum_{c=1}^{C} X_{gc} p_c + \\varepsilon_g,\n$$\nwhere $X_{gc}$ is the mean expression of gene $g$ in cell type $c$, $p_c$ is the fraction of cell type $c$ in the sample, and $\\varepsilon_g$ captures measurement noise and model mismatch. As these quantities are counts or fractions, we have $X_{gc} \\geq 0$, $p_c \\geq 0$, and under the fraction interpretation $\\sum_{c=1}^{C} p_c = 1$. The model across all genes is succinctly written as\n$$\n\\mathbf{y} = X \\mathbf{p} + \\boldsymbol{\\varepsilon}.\n$$\n\nTo estimate $\\mathbf{p}$, we convert this into a convex optimization problem. Without noise, the least squares estimator solves\n$$\n\\min_{\\mathbf{p}} \\left\\| X \\mathbf{p} - \\mathbf{y} \\right\\|_2^2.\n$$\nBiological constraints imply nonnegativity of $\\mathbf{p}$, yielding the nonnegative least squares (NNLS) problem:\n$$\n\\min_{\\mathbf{p} \\in \\mathbb{R}^{C}} \\left\\| X \\mathbf{p} - \\mathbf{y} \\right\\|_2 \\quad \\text{subject to} \\quad p_c \\geq 0 \\text{ for all } c.\n$$\nThis is a convex problem with a unique minimizer when $X$ has full column rank and stable behavior under near-collinearity due to the nonnegativity and the quadratic objective. The Karush–Kuhn–Tucker (KKT) conditions ensure that at the optimum, either $\\widehat{p}_c = 0$ or the corresponding gradient condition is satisfied for active variables. Computationally, we solve NNLS via an active-set method (as implemented in standard libraries), which iteratively identifies a subset of variables constrained at zero and solves unconstrained least squares on the remainder until optimality.\n\nThe NNLS solution $\\widehat{\\mathbf{p}}$ does not enforce the sum-to-one constraint. However, when interpreting $\\widehat{\\mathbf{p}}$ as fractions, renormalization is straightforward:\n$$\n\\widehat{\\mathbf{p}} \\leftarrow \\frac{\\widehat{\\mathbf{p}}}{\\sum_{c=1}^{C} \\widehat{p}_c} \\quad \\text{if} \\quad \\sum_{c=1}^{C} \\widehat{p}_c > 0,\n$$\nensuring $\\sum_{c=1}^{C} \\widehat{p}_c = 1$. If $\\sum_{c=1}^{C} \\widehat{p}_c = 0$ due to degeneracy (all-zero estimate), a reasonable fallback is the uniform distribution\n$$\n\\widehat{p}_c = \\frac{1}{C} \\quad \\text{for all } c.\n$$\n\nAccuracy is quantified by the root mean squared error (RMSE) between $\\widehat{\\mathbf{p}}$ and the ground-truth $\\mathbf{p}^{\\dagger}$:\n$$\n\\mathrm{RMSE}(\\widehat{\\mathbf{p}}, \\mathbf{p}^{\\dagger}) = \\sqrt{\\frac{1}{C} \\sum_{c=1}^{C} \\left(\\widehat{p}_c - p^{\\dagger}_c\\right)^2}.\n$$\nIn Case $1$, the reference is well-conditioned and the mixture is noiseless, so NNLS should recover $\\mathbf{p}^{\\star}_1$ with negligible error after renormalization. In Case $2$, columns of $X_2$ are nearly collinear; NNLS remains applicable but the solution may be less precise due to ill-conditioning, yet renormalization will still enforce interpretability. In Case $3$, the bulk contains an unmodeled cell type. The correct evaluation compares $\\widehat{\\mathbf{p}}$ to the normalized known proportions $\\mathbf{p}^{\\dagger} = \\mathbf{p}^{\\text{known}}_3 / 0.9$ because the unknown fraction is not represented in $X$. In Case $4$, additive noise perturbs $\\mathbf{y}$, and NNLS trades off residual error to fit noisy observations, resulting in a small but nonzero RMSE. In Case $5$, a global scaling of $\\mathbf{y}$ corresponds to library-size differences; NNLS followed by renormalization is invariant to such scaling, thus RMSE should be near zero. \n\nFor integration with a clinical phenotype (Case $6$), we estimate $\\widehat{\\mathbf{p}}^{(t)}$ for each sample $t$ and focus on the fraction of a specific cell type, here cell type $3$. The Pearson correlation coefficient between the estimated fractions $\\{\\widehat{p}_3^{(t)}\\}_{t=1}^{T}$ and severity scores $\\{s^{(t)}\\}_{t=1}^{T}$ is\n$$\nr = \\frac{\\sum_{t=1}^{T} \\left(\\widehat{p}_{3}^{(t)} - \\overline{\\widehat{p}_{3}}\\right)\\left(s^{(t)} - \\overline{s}\\right)}{\\sqrt{\\sum_{t=1}^{T}\\left(\\widehat{p}_{3}^{(t)} - \\overline{\\widehat{p}_{3}}\\right)^2}\\sqrt{\\sum_{t=1}^{T}\\left(s^{(t)} - \\overline{s}\\right)^2}},\n$$\nwhich measures a linear association between estimated cell-type $3$ abundance and clinical severity, a standard approach for integrating omics-derived features with clinical phenotypes.\n\nAlgorithmic steps for each test case:\n- Construct $X$ and $\\mathbf{y}$ exactly as specified.\n- Solve NNLS to obtain $\\widehat{\\mathbf{p}}$.\n- Renormalize $\\widehat{\\mathbf{p}}$ to unit sum if possible, else set it to uniform fractions.\n- Compute $\\mathrm{RMSE}$ against the appropriate $\\mathbf{p}^{\\dagger}$ for Cases $1$–$5$ and Pearson correlation $r$ for Case $6$.\n\nFinally, aggregate the six results into one line formatted as $[\\text{value}_1,\\dots,\\text{value}_6]$, with each value rounded to $6$ decimal places and expressed in decimal form (no percentage signs). This provides a compact summary of deconvolution accuracy across varying conditions and its relevance to clinical phenotype integration.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import nnls\n\ndef nnls_deconvolution(X, y):\n    \"\"\"\n    Solve nonnegative least squares for p in y ≈ X p.\n    Returns p_hat renormalized to sum to 1 (if possible).\n    \"\"\"\n    p_hat, _ = nnls(X, y)\n    s = p_hat.sum()\n    if s > 0:\n        p_hat = p_hat / s\n    else:\n        # Fallback to uniform distribution if degenerate (sum == 0)\n        C = X.shape[1]\n        p_hat = np.full(C, 1.0 / C)\n    return p_hat\n\ndef rmse(p_hat, p_true):\n    \"\"\"\n    Root Mean Squared Error between estimated and true proportions.\n    \"\"\"\n    diff = p_hat - p_true\n    return np.sqrt(np.mean(diff * diff))\n\ndef pearson_correlation(x, y):\n    \"\"\"\n    Pearson correlation coefficient between two 1D arrays.\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    x_mean = np.mean(x)\n    y_mean = np.mean(y)\n    x_centered = x - x_mean\n    y_centered = y - y_mean\n    denom = np.sqrt(np.sum(x_centered**2) * np.sum(y_centered**2))\n    if denom == 0:\n        return 0.0\n    return float(np.sum(x_centered * y_centered) / denom)\n\ndef solve():\n    results = []\n\n    # Case 1\n    X1 = np.array([\n        [8.0, 2.0, 1.0],\n        [4.0, 1.5, 2.0],\n        [3.0, 3.0, 6.0],\n        [5.0, 1.0, 0.5],\n        [7.0, 2.5, 1.0]\n    ], dtype=float)\n    p1_true = np.array([0.5, 0.3, 0.2], dtype=float)\n    y1 = np.array([4.8, 2.85, 3.6, 2.9, 4.45], dtype=float)\n    p1_hat = nnls_deconvolution(X1, y1)\n    results.append(rmse(p1_hat, p1_true))\n\n    # Case 2\n    X2 = np.array([\n        [6.0, 6.2, 1.0],\n        [3.0, 3.1, 2.0],\n        [5.0, 5.1, 6.0],\n        [4.0, 4.05, 0.5],\n        [2.0, 2.02, 1.0]\n    ], dtype=float)\n    p2_true = np.array([0.4, 0.4, 0.2], dtype=float)\n    y2 = np.array([5.08, 2.84, 5.24, 3.32, 1.808], dtype=float)\n    p2_hat = nnls_deconvolution(X2, y2)\n    results.append(rmse(p2_hat, p2_true))\n\n    # Case 3 (unmodeled cell type)\n    X3 = X1.copy()\n    p3_known = np.array([0.5, 0.25, 0.15], dtype=float)  # sums to 0.9\n    u = np.array([4.0, 1.0, 3.0, 2.0, 5.0], dtype=float)\n    y3 = X3 @ p3_known + 0.1 * u\n    p3_true_norm = p3_known / 0.9\n    p3_hat = nnls_deconvolution(X3, y3)\n    results.append(rmse(p3_hat, p3_true_norm))\n\n    # Case 4 (measurement noise)\n    X4 = X1.copy()\n    p4_true = np.array([0.3, 0.5, 0.2], dtype=float)\n    y4_noiseless = X4 @ p4_true\n    eta = np.array([0.05, -0.02, 0.03, -0.04, 0.01], dtype=float)\n    y4 = y4_noiseless + eta\n    p4_hat = nnls_deconvolution(X4, y4)\n    results.append(rmse(p4_hat, p4_true))\n\n    # Case 5 (scaling mismatch)\n    X5 = X1.copy()\n    p5_true = p1_true.copy()\n    y5 = 2.0 * y1\n    p5_hat = nnls_deconvolution(X5, y5)\n    results.append(rmse(p5_hat, p5_true))\n\n    # Case 6 (clinical phenotype correlation)\n    X6 = X1.copy()\n    p6_list = [\n        np.array([0.2, 0.5, 0.3], dtype=float),\n        np.array([0.1, 0.5, 0.4], dtype=float),\n        np.array([0.05, 0.45, 0.5], dtype=float),\n        np.array([0.3, 0.6, 0.1], dtype=float),\n        np.array([0.05, 0.35, 0.6], dtype=float),\n        np.array([0.15, 0.55, 0.3], dtype=float),\n    ]\n    delta = np.array([0.01, -0.01, 0.0, 0.02, -0.02], dtype=float)\n    s = np.array([0.3, 0.5, 0.7, 0.2, 0.9, 0.4], dtype=float)\n    est_cell3 = []\n    for p_true in p6_list:\n        y6 = X6 @ p_true + delta\n        p_hat6 = nnls_deconvolution(X6, y6)\n        est_cell3.append(p_hat6[2])\n    r = pearson_correlation(np.array(est_cell3, dtype=float), s)\n    results.append(r)\n\n    # Round each result to 6 decimal places and print in specified format\n    rounded = [f\"{x:.6f}\" for x in results]\n    print(f\"[{','.join(rounded)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Once features are prepared, the next step is to build a model that links the high-dimensional omics data to a clinical phenotype. This exercise contrasts two powerful multivariate methods, Partial Least Squares (PLS) and Canonical Correlation Analysis (CCA), for building predictive models in a typical high-dimensional, low-sample-size scenario. By simulating data and implementing both algorithms from first principles, you will gain a deeper understanding of their distinct optimization goals and practical trade-offs, particularly regarding their robustness to noise and feature collinearity. ",
            "id": "4574633",
            "problem": "You are asked to write a complete, runnable program that simulates an omics-to-phenotype integration task and compares Partial Least Squares (PLS) and Canonical Correlation Analysis (CCA) for predicting a clinical phenotype from high-dimensional omics measurements under weak cross-correlation and high noise. The program must implement both methods from first principles using linear algebra and well-tested statistical definitions. The problem is framed around a linear Gaussian generative model that is widely used to model the relationship between omics features and clinical outcomes. The goal is to demonstrate and quantify which method yields better phenotype prediction and explain the reasoning from fundamental principles.\n\nFundamental base. Use the following foundational definitions and facts:\n- The Central Dogma of molecular biology states that information flows from deoxyribonucleic acid (DNA) to ribonucleic acid (RNA) to protein; in practice, high-dimensional omics measurements are treated as quantitative features related to latent biological processes that may be associated with clinical phenotypes. For the purposes of this problem, you will use a linear Gaussian latent-factor model to capture shared variability between omics features and a phenotype.\n- The sample covariance between two centered random vectors is defined as $$\\mathrm{Cov}(U,V)=\\frac{1}{n}\\sum_{i=1}^{n} U_i V_i,$$ and the sample correlation is the covariance normalized by the product of standard deviations. Linear prediction of a scalar response from features uses a coefficient vector to form a prediction by a linear combination.\n- Partial Least Squares (PLS) seeks linear combinations of features that maximize the sample covariance with the response, and Canonical Correlation Analysis (CCA) seeks linear combinations that maximize the sample correlation under normalization constraints. Both methods can be implemented via linear algebraic optimization principles without relying on domain-specific shortcuts.\n\nData-generating model. For each test case, simulate training and testing data according to a shared latent-factor model:\n- Let $p$ be the number of omics features. Construct a loadings vector $a \\in \\mathbb{R}^{p}$ with exactly $k_{\\mathrm{signal}}$ nonzero entries, chosen uniformly at random positions, and then normalized to unit Euclidean norm. This represents a latent omics module.\n- For each sample $i$, draw a latent scalar $g_i \\sim \\mathcal{N}(0,1)$, draw feature noise $\\epsilon_i \\sim \\mathcal{N}(0,\\sigma_x^2 I_p)$, and form omics features as $$X_i = g_i a + \\epsilon_i.$$\n- Draw phenotype noise $\\eta_i \\sim \\mathcal{N}(0,\\sigma_y^2)$ and form the clinical phenotype as $$y_i = c \\cdot g_i + \\eta_i,$$ where $c$ controls the strength of the true shared signal between omics and the phenotype. All draws are independent across samples and variables.\n- Use the same $a$ for the training and testing sets within each test case to represent a consistent underlying biology.\n\nPreprocessing. For each test case, center and scale each feature in the training set to zero mean and unit variance, and center and scale the training phenotype to zero mean and unit variance. Apply the same centering and scaling to the testing set using training-set statistics only.\n\nMethods to implement.\n- One-component PLS for a single response: derive the first predictive direction and the corresponding regression coefficient by maximizing sample covariance between the latent score $t$ formed from the features and the centered phenotype. Use a single latent component to form a coefficient vector $b_{\\mathrm{pls}} \\in \\mathbb{R}^{p}$ and produce test predictions $\\hat{y} = X_{\\mathrm{test}} b_{\\mathrm{pls}}$ mapped back to the original (unstandardized) phenotype scale.\n- One-component regularized CCA-based predictor: derive a canonical direction $u \\in \\mathbb{R}^{p}$ for the features by maximizing the sample correlation between $u^\\top X$ and the centered phenotype under a normalization constraint on the feature-side variance, and stabilize the solution with a ridge parameter $\\lambda>0$ added to the feature covariance matrix. Then perform least squares regression of the centered phenotype on the canonical score $z = X u$ to obtain a scalar coefficient $\\alpha$, yielding test predictions $\\hat{y} = \\alpha (X_{\\mathrm{test}} u)$ mapped back to the original phenotype scale.\n\nEvaluation. For each test case, compute the root mean squared error (RMSE) on the testing set,\n$$\\mathrm{RMSE}=\\sqrt{\\frac{1}{n_{\\mathrm{test}}} \\sum_{i=1}^{n_{\\mathrm{test}}} \\left(y_i - \\hat{y}_i\\right)^2},$$\nfor both PLS and CCA. Define an indicator for which method performs better: output $1$ if PLS has strictly lower RMSE than CCA by more than a small tolerance, output $-1$ if CCA has strictly lower RMSE than PLS by more than a small tolerance, and output $0$ otherwise. Use a tolerance of $10^{-6}$.\n\nDeterministic randomness. Use a fixed base random seed $s_0=12345$ and, for the $j$-th test case (starting at $j=0$), use seed $s_j = s_0 + j$ to ensure reproducibility and variation across cases.\n\nTest suite. Your program must run the following four test cases, which together probe a happy path, a very high-noise boundary condition, a high-dimensional edge case with $p \\gg n$, and a moderately stronger shared signal:\n- Case A (happy path, weak correlation, high noise): $n_{\\mathrm{train}}=120$, $n_{\\mathrm{test}}=280$, $p=200$, $k_{\\mathrm{signal}}=15$, $c=0.15$, $\\sigma_x=1.5$, $\\sigma_y=1.5$, $\\lambda=0.5$.\n- Case B (boundary, extremely high noise): $n_{\\mathrm{train}}=120$, $n_{\\mathrm{test}}=280$, $p=200$, $k_{\\mathrm{signal}}=15$, $c=0.10$, $\\sigma_x=3.0$, $\\sigma_y=3.0$, $\\lambda=1.0$.\n- Case C (edge, $p \\gg n$): $n_{\\mathrm{train}}=80$, $n_{\\mathrm{test}}=320$, $p=500$, $k_{\\mathrm{signal}}=25$, $c=0.12$, $\\sigma_x=2.0$, $\\sigma_y=2.0$, $\\lambda=2.0$.\n- Case D (moderately stronger shared signal, still noisy): $n_{\\mathrm{train}}=150$, $n_{\\mathrm{test}}=350$, $p=200$, $k_{\\mathrm{signal}}=20$, $c=0.30$, $\\sigma_x=1.8$, $\\sigma_y=1.8$, $\\lambda=0.8$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Flatten the results by concatenating, for each test case in order A, B, C, D, the triple $[\\mathrm{RMSE}_{\\mathrm{PLS}}, \\mathrm{RMSE}_{\\mathrm{CCA}}, \\mathrm{indicator}]$ into one list. For example, the output must be of the form\n$$[\\mathrm{RMSE}_A^{\\mathrm{PLS}},\\mathrm{RMSE}_A^{\\mathrm{CCA}},I_A,\\mathrm{RMSE}_B^{\\mathrm{PLS}},\\mathrm{RMSE}_B^{\\mathrm{CCA}},I_B,\\mathrm{RMSE}_C^{\\mathrm{PLS}},\\mathrm{RMSE}_C^{\\mathrm{CCA}},I_C,\\mathrm{RMSE}_D^{\\mathrm{PLS}},\\mathrm{RMSE}_D^{\\mathrm{CCA}},I_D].$$\nAll values must be numeric. No other text should be printed.",
            "solution": "The problem requires a comparative analysis of Partial Least Squares (PLS) and regularized Canonical Correlation Analysis (CCA) for predicting a clinical phenotype from high-dimensional omics data. The analysis is to be performed within a simulated environment defined by a linear Gaussian latent-factor model, which is a standard approach for modeling such relationships in bioinformatics. The solution involves deriving and implementing both methods from first principles and evaluating their predictive performance under conditions of weak signal and high noise.\n\nFirst, we establish the mathematical framework. Let $X_{\\text{train}} \\in \\mathbb{R}^{n_{\\text{train}} \\times p}$ and $y_{\\text{train}} \\in \\mathbb{R}^{n_{\\text{train}}}$ represent the training set of $n_{\\text{train}}$ samples, with $p$ omics features and a single phenotype, respectively. Similarly, let $X_{\\text{test}} \\in \\mathbb{R}^{n_{\\text{test}} \\times p}$ and $y_{\\text{test}} \\in \\mathbb{R}^{n_{\\text{test}}}$ be the corresponding testing data.\n\n### Data Preprocessing\nBefore applying the methods, the data must be preprocessed. We compute the mean $\\mu_{X_j}$ and standard deviation $\\sigma_{X_j}$ for each feature $j=1, \\dots, p$ from the training data $X_{\\text{train}}$. The standard deviation is computed using a denominator of $n_{\\text{train}}$, consistent with the problem's definition of sample covariance. Similarly, we compute the mean $\\mu_y$ and standard deviation $\\sigma_y$ for the training phenotype $y_{\\text{train}}$.\nThe training data are then standardized:\n$$ X_{s, \\text{train}} = (X_{\\text{train}} - \\mu_X) / \\sigma_X $$\n$$ y_{s, \\text{train}} = (y_{\\text{train}} - \\mu_y) / \\sigma_y $$\nwhere the subtraction and division operations are performed element-wise. The testing data are standardized using the statistics from the training data:\n$$ X_{s, \\text{test}} = (X_{\\text{test}} - \\mu_X) / \\sigma_X $$\nThis ensures that no information from the test set leaks into the training process. Predictions will be made on the standardized scale and then transformed back to the original phenotype scale for evaluation.\n\n### Method 1: Partial Least Squares (PLS) Regression\nThe objective of single-component PLS is to find a linear combination of features, called a latent score $t = X_{s, \\text{train}} w$, that maximally covaries with the phenotype $y_{s, \\text{train}}$. The weight vector $w \\in \\mathbb{R}^p$ defines the direction of projection. The optimization problem is:\n$$ \\max_{w} \\mathrm{Cov}(X_{s, \\text{train}} w, y_{s, \\text{train}}) \\quad \\text{subject to} \\quad \\|w\\|_2 = 1 $$\nThe sample covariance, for centered data of length $n_{\\text{train}}$, is given by:\n$$ \\mathrm{Cov}(t, y_{s, \\text{train}}) = \\frac{1}{n_{\\text{train}}} t^\\top y_{s, \\text{train}} = \\frac{1}{n_{\\text{train}}} (X_{s, \\text{train}} w)^\\top y_{s, \\text{train}} = \\frac{1}{n_{\\text{train}}} w^\\top X_{s, \\text{train}}^\\top y_{s, \\text{train}} $$\nMaximizing this quantity with respect to $w$ under the unit-norm constraint is a classic optimization problem whose solution is that $w$ must be aligned with the gradient, i.e., $w \\propto X_{s, \\text{train}}^\\top y_{s, \\text{train}}$. We can therefore define the PLS weight vector (unnormalized) as:\n$$ w_{\\text{pls}} = X_{s, \\text{train}}^\\top y_{s, \\text{train}} $$\nThe vector $X_{s, \\text{train}}^\\top y_{s, \\text{train}}$ contains the covariances between each feature and the response. Thus, PLS up-weights features that have a strong marginal covariance with the phenotype.\n\nThe latent scores for the training data are computed as $t_{\\text{pls}} = X_{s, \\text{train}} w_{\\text{pls}}$. To form a predictive model, we perform a simple linear regression of the standardized phenotype $y_{s, \\text{train}}$ onto these scores:\n$$ y_{s, \\text{train}} = \\beta t_{\\text{pls}} + \\text{error} $$\nThe ordinary least squares estimate for the scalar coefficient $\\beta$ is:\n$$ \\hat{\\beta} = (t_{\\text{pls}}^\\top t_{\\text{pls}})^{-1} t_{\\text{pls}}^\\top y_{s, \\text{train}} $$\nThe final regression coefficient vector in the space of the standardized features is $b_{s, \\text{pls}} = w_{\\text{pls}} \\hat{\\beta}$. This vector combines the covariance-maximizing direction with the regression scaling factor.\n\n### Method 2: Regularized Canonical Correlation Analysis (CCA)-based Regression\nCCA seeks a feature projection $z = X_{s, \\text{train}} u$ that maximizes the *correlation* with the phenotype $y_{s, \\text{train}}$. The optimization problem is:\n$$ \\max_{u} \\mathrm{Corr}(X_{s, \\text{train}} u, y_{s, \\text{train}}) = \\max_{u} \\frac{\\mathrm{Cov}(X_{s, \\text{train}} u, y_{s, \\text{train}})}{\\sqrt{\\mathrm{Var}(X_{s, \\text{train}} u) \\mathrm{Var}(y_{s, \\text{train}})}} $$\nSince $y_{s, \\text{train}}$ is standardized, its variance is $1$. The expression simplifies to:\n$$ \\max_{u} \\frac{u^\\top S_{xy}}{\\sqrt{u^\\top S_{xx} u}} $$\nwhere $S_{xy} = \\frac{1}{n_{\\text{train}}} X_{s, \\text{train}}^\\top y_{s, \\text{train}}$ is the vector of feature-phenotype covariances and $S_{xx} = \\frac{1}{n_{\\text{train}}} X_{s, \\text{train}}^\\top X_{s, \\text{train}}$ is the feature covariance matrix. In high-dimensional settings ($p > n_{\\text{train}}$), $S_{xx}$ is singular. To stabilize the problem, a ridge regularization term $\\lambda > 0$ is added to the feature covariance matrix, as specified. The problem becomes maximizing:\n$$ \\frac{u^\\top S_{xy}}{\\sqrt{u^\\top (S_{xx} + \\lambda I) u}} $$\nThe solution to this maximization problem is given by $u \\propto (S_{xx} + \\lambda I)^{-1} S_{xy}$. Substituting the data matrices, we get:\n$$ u \\propto \\left(\\frac{1}{n_{\\text{train}}} X_{s, \\text{train}}^\\top X_{s, \\text{train}} + \\lambda I\\right)^{-1} \\left(\\frac{1}{n_{\\text{train}}} X_{s, \\text{train}}^\\top y_{s, \\text{train}}\\right) $$\nThe factor of $1/n_{\\text{train}}$ can be cleared, leading to the weight vector for the canonical projection:\n$$ u_{\\text{cca}} \\propto (X_{s, \\text{train}}^\\top X_{s, \\text{train}} + n_{\\text{train}}\\lambda I)^{-1} X_{s, \\text{train}}^\\top y_{s, \\text{train}} $$\nThis is notably the solution to Ridge Regression. The problem specifies a two-step procedure: first find this direction $u_{\\text{cca}}$, then build a regression model on the resulting scores. We calculate the canonical scores $z_{\\text{cca}} = X_{s, \\text{train}} u_{\\text{cca}}$. Next, we perform a simple linear regression of $y_{s, \\text{train}}$ on $z_{\\text{cca}}$ to find a scalar coefficient $\\alpha$:\n$$ \\hat{\\alpha} = (z_{\\text{cca}}^\\top z_{\\text{cca}})^{-1} z_{\\text{cca}}^\\top y_{s, \\text{train}} $$\nThe final coefficient vector for prediction is $b_{s, \\text{cca}} = u_{\\text{cca}} \\hat{\\alpha}$.\n\n### Conceptual Comparison and Prediction\nThe fundamental difference lies in how feature correlations are handled. PLS, with $w_{\\text{pls}} \\propto X_{s, \\text{train}}^\\top y_{s, \\text{train}}$, ignores feature-feature correlations and greedily selects a direction based on direct feature-phenotype covariance. CCA, with $u_{\\text{cca}} \\propto (X_{s, \\text{train}}^\\top X_{s, \\text{train}} + \\dots)^{-1} X_{s, \\text{train}}^\\top y_{s, \\text{train}}$, explicitly incorporates the feature covariance structure $X_{s, \\text{train}}^\\top X_{s, \\text{train}}$. The matrix inversion effectively \"whitens\" the feature space, down-weighting redundant information.\nHowever, in high-noise settings, the sample covariance matrix $X_{s, \\text{train}}^\\top X_{s, \\text{train}}$ is a very noisy estimate of the true covariance. Its inversion can amplify noise, potentially leading to poor generalization. PLS, by avoiding this inversion, may prove more robust, even if it is a theoretically less sophisticated model. This is a common trade-off in high-dimensional statistics.\n\n### Prediction and Evaluation\nFor each method $m \\in \\{\\text{pls}, \\text{cca}\\}$, we have a coefficient vector $b_{s,m}$. Predictions for the test set are generated by:\n$1$. Standardize test features: $X_{s, \\text{test}} = (X_{\\text{test}} - \\mu_X) / \\sigma_X$.\n$2$. Predict on the standardized scale: $\\hat{y}_{s, \\text{test}} = X_{s, \\text{test}} b_{s,m}$.\n$3$. Un-standardize the prediction: $\\hat{y}_{\\text{test}} = \\hat{y}_{s, \\text{test}} \\sigma_y + \\mu_y$.\n\nThe performance is evaluated using the Root Mean Squared Error (RMSE) on the test set:\n$$ \\mathrm{RMSE} = \\sqrt{\\frac{1}{n_{\\text{test}}} \\sum_{i=1}^{n_{\\text{test}}} (y_{i, \\text{test}} - \\hat{y}_{i, \\text{test}})^2} $$\nAn indicator variable is computed to determine which method performs better based on a tolerance of $10^{-6}$.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Simulates omics-to-phenotype integration, comparing PLS and regularized CCA.\n    \"\"\"\n\n    def _solve_single_case(params):\n        \"\"\"\n        Executes a single test case for data generation, analysis, and evaluation.\n        \"\"\"\n        n_train, n_test, p, k_signal, c, sigma_x, sigma_y, lambda_val, seed = params\n        \n        rng = np.random.default_rng(seed)\n        \n        # Data Generation\n        # 1. Construct loading vector a\n        a = np.zeros(p)\n        nonzero_indices = rng.choice(p, k_signal, replace=False)\n        a[nonzero_indices] = rng.standard_normal(k_signal)\n        a /= np.linalg.norm(a)\n        \n        # 2. Generate training data\n        g_train = rng.standard_normal((n_train, 1))\n        epsilon_train = rng.standard_normal((n_train, p)) * sigma_x\n        X_train = g_train @ a.reshape(1, p) + epsilon_train\n        eta_train = rng.standard_normal(n_train) * sigma_y\n        y_train = c * g_train.flatten() + eta_train\n        \n        # 3. Generate testing data\n        g_test = rng.standard_normal((n_test, 1))\n        epsilon_test = rng.standard_normal((n_test, p)) * sigma_x\n        X_test = g_test @ a.reshape(1, p) + epsilon_test\n        eta_test = rng.standard_normal(n_test) * sigma_y\n        y_test = c * g_test.flatten() + eta_test\n\n        # Preprocessing using training set statistics\n        # Use ddof=0 for variance/std to align with the 1/n definition in problem statement.\n        mu_X = np.mean(X_train, axis=0)\n        std_X = np.std(X_train, axis=0, ddof=0)\n        std_X[std_X  1e-8] = 1.0  # Prevent division by zero for constant features\n\n        mu_y = np.mean(y_train)\n        std_y = np.std(y_train, ddof=0)\n        if std_y  1e-8:\n            std_y = 1.0\n            \n        X_train_s = (X_train - mu_X) / std_X\n        y_train_s = (y_train - mu_y) / std_y\n        X_test_s = (X_test - mu_X) / std_X\n        \n        # --- PLS Implementation ---\n        # Weight vector is proportional to covariance between features and response\n        w_pls = X_train_s.T @ y_train_s\n        \n        # PLS scores (projection of data onto the weight vector)\n        t_pls = X_train_s @ w_pls\n        \n        # Regress response onto scores to find the scalar coefficient beta\n        t_dot_t = t_pls.T @ t_pls\n        beta = (t_pls.T @ y_train_s) / t_dot_t if t_dot_t > 1e-12 else 0.0\n            \n        # Full PLS coefficient vector for standardized data\n        b_pls_s = w_pls * beta\n        \n        # Predict on test set and un-standardize\n        y_hat_pls_s = X_test_s @ b_pls_s\n        y_hat_pls = y_hat_pls_s * std_y + mu_y\n        \n        rmse_pls = np.sqrt(np.mean((y_test - y_hat_pls)**2))\n        \n        # --- Regularized CCA-based Regression Implementation ---\n        # Covariance matrices from standardized data\n        cov_xx = X_train_s.T @ X_train_s\n        cov_xy = X_train_s.T @ y_train_s\n        \n        # Regularization term is n_train * lambda * I\n        reg_term = n_train * lambda_val * np.eye(p)\n        \n        # Solve (cov_xx + reg_term) u = cov_xy for the canonical direction u\n        u_cca_raw = np.linalg.solve(cov_xx + reg_term, cov_xy)\n        \n        # Canonical scores\n        z_cca = X_train_s @ u_cca_raw\n        \n        # Regress response onto scores to get the scalar coefficient alpha\n        z_dot_z = z_cca.T @ z_cca\n        alpha = (z_cca.T @ y_train_s) / z_dot_z if z_dot_z > 1e-12 else 0.0\n            \n        # Full CCA coefficient vector for standardized data\n        b_cca_s = u_cca_raw * alpha\n        \n        # Predict on test set and un-standardize\n        y_hat_cca_s = X_test_s @ b_cca_s\n        y_hat_cca = y_hat_cca_s * std_y + mu_y\n        \n        rmse_cca = np.sqrt(np.mean((y_test - y_hat_cca)**2))\n        \n        # Evaluation\n        tol = 1e-6\n        if rmse_pls  rmse_cca - tol:\n            indicator = 1\n        elif rmse_cca  rmse_pls - tol:\n            indicator = -1\n        else:\n            indicator = 0\n            \n        return rmse_pls, rmse_cca, indicator\n\n    # --- Test Suite ---\n    s0 = 12345\n    test_cases = [\n        # Case A: Happy path\n        (120, 280, 200, 15, 0.15, 1.5, 1.5, 0.5, s0 + 0),\n        # Case B: Extremely high noise\n        (120, 280, 200, 15, 0.10, 3.0, 3.0, 1.0, s0 + 1),\n        # Case C: Edge case, p >> n\n        (80, 320, 500, 25, 0.12, 2.0, 2.0, 2.0, s0 + 2),\n        # Case D: Moderately stronger signal\n        (150, 350, 200, 20, 0.30, 1.8, 1.8, 0.8, s0 + 3),\n    ]\n\n    all_results = []\n    for params in test_cases:\n        rmse_pls, rmse_cca, indicator = _solve_single_case(params)\n        all_results.extend([rmse_pls, rmse_cca, indicator])\n\n    # Final output formatting\n    # Example format: [RMSE_A_PLS,RMSE_A_CCA,I_A,RMSE_B_PLS,RMSE_B_CCA,I_B,...]\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n\n```"
        }
    ]
}