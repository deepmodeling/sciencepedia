{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of phylodynamics is the molecular clock, which allows us to convert genetic divergence into time. This practice provides a foundational exercise in Bayesian parameter estimation by asking you to calibrate a strict molecular clock rate for a virus. You will apply Bayes' theorem with conjugate priors to analytically derive the posterior distribution for the substitution rate $\\mu$ and compute its posterior mean, offering a clear illustration of how prior knowledge is updated by observed data .",
            "id": "4542077",
            "problem": "A laboratory is calibrating a strict molecular clock for a rapidly evolving RNA virus to support downstream medical analytics. An alignment of length $L$ sites was constructed from $n$ tip-dated sequences sampled during a single, well-characterized outbreak whose origin time is known and set to $t=0$. For each tip $i \\in \\{1,\\dots,n\\}$ collected at time $t_i$ (in years since $t=0$), a maximum likelihood phylogeny with a known root was inferred, and the number of substitutions mapped along the root-to-tip path was recorded as $K_i$. Assume a strict molecular clock, site independence, and that along each root-to-tip path the substitution process is a time-homogeneous Poisson process with rate $\\mu$ substitutions per site per year, so that conditional on $\\mu$ the count $K_i$ is distributed as a Poisson random variable with mean $\\mu L t_i$. Assume that the $K_i$ are conditionally independent given $\\mu$. To complete the Bayesian calibration, place a Gamma prior (shape-rate parameterization) on the substitution rate $\\mu$: $\\mu \\sim \\mathrm{Gamma}(a_0,b_0)$.\n\nUsing Bayes’ theorem and these modeling assumptions only, derive the posterior distribution for $\\mu$ and determine the Bayes estimator under squared-error loss (the posterior mean). Then compute its numerical value for the following dataset:\n- $n=5$ tips, alignment length $L=10000$ sites;\n- sampling times (years since $t=0$): $(t_1,t_2,t_3,t_4,t_5) = (0.8, 1.6, 2.7, 3.9, 5.2)$;\n- root-to-tip substitution counts: $(K_1,K_2,K_3,K_4,K_5) = (8, 20, 28, 39, 53)$;\n- prior hyperparameters: $(a_0,b_0) = (2,2000)$.\n\nRound your final numerical answer for the substitution rate to four significant figures and express it in substitutions per site per year. The final answer must be a single real number.",
            "solution": "The problem requires the derivation of the posterior distribution for the substitution rate $\\mu$ and the computation of its posterior mean, which serves as the Bayes estimator under squared-error loss. We begin with Bayes’ theorem, which states that the posterior distribution is proportional to the product of the likelihood and the prior distribution:\n$$ P(\\mu | \\mathcal{D}) \\propto P(\\mathcal{D} | \\mu) P(\\mu) $$\nwhere $\\mathcal{D} = \\{K_1, \\dots, K_n\\}$ represents the observed data, specifically the substitution counts.\n\nThe likelihood function $P(\\mathcal{D} | \\mu)$ is constructed from the problem's assumptions. Given that each substitution count $K_i$ is a draw from an independent Poisson distribution with mean $\\lambda_i = \\mu L t_i$, the total likelihood is the product of the individual Poisson probability mass functions:\n$$ P(\\mathcal{D} | \\mu) = \\prod_{i=1}^{n} P(K_i | \\mu) = \\prod_{i=1}^{n} \\frac{(\\mu L t_i)^{K_i} \\exp(-\\mu L t_i)}{K_i!} $$\n\nThe prior distribution for $\\mu$ is given as a Gamma distribution with a shape parameter $a_0$ and a rate parameter $b_0$. The probability density function is:\n$$ P(\\mu) = \\mathrm{Gamma}(\\mu | a_0, b_0) = \\frac{b_0^{a_0}}{\\Gamma(a_0)} \\mu^{a_0-1} \\exp(-b_0 \\mu) $$\n\nTo find the posterior distribution $P(\\mu | \\mathcal{D})$, we combine the likelihood and the prior. We can disregard any terms that do not depend on $\\mu$, as these will be incorporated into the normalization constant of the posterior distribution.\n$$ P(\\mu | \\mathcal{D}) \\propto \\left( \\prod_{i=1}^{n} (\\mu L t_i)^{K_i} \\exp(-\\mu L t_i) \\right) \\left( \\mu^{a_0-1} \\exp(-b_0 \\mu) \\right) $$\nWe expand this expression and group the terms that involve $\\mu$:\n$$ P(\\mu | \\mathcal{D}) \\propto \\left( \\prod_{i=1}^{n} \\mu^{K_i} \\right) \\left( \\prod_{i=1}^{n} \\exp(-\\mu L t_i) \\right) \\mu^{a_0-1} \\exp(-b_0 \\mu) $$\n$$ P(\\mu | \\mathcal{D}) \\propto \\mu^{\\sum_{i=1}^{n} K_i} \\exp\\left(-\\mu L \\sum_{i=1}^{n} t_i\\right) \\mu^{a_0-1} \\exp(-b_0 \\mu) $$\nCombining the exponents of $\\mu$ and the arguments of the exponential function yields the kernel of the posterior distribution:\n$$ P(\\mu | \\mathcal{D}) \\propto \\mu^{(a_0 + \\sum_{i=1}^{n} K_i) - 1} \\exp\\left( - \\mu \\left(b_0 + L \\sum_{i=1}^{n} t_i\\right) \\right) $$\n\nThis expression is the kernel of a Gamma distribution. This demonstrates that the Gamma distribution is a conjugate prior for the rate parameter of the Poisson likelihood. The posterior distribution of $\\mu$ is therefore also a Gamma distribution, $\\mu | \\mathcal{D} \\sim \\mathrm{Gamma}(a_n, b_n)$, with the following updated parameters:\n$$ a_n = a_0 + \\sum_{i=1}^{n} K_i $$\n$$ b_n = b_0 + L \\sum_{i=1}^{n} t_i $$\n\nThe Bayes estimator of $\\mu$ under squared-error loss is the mean of the posterior distribution. For a Gamma distribution $\\mathrm{Gamma}(a, b)$ with shape-rate parameterization, the mean is given by the ratio of its parameters, $\\frac{a}{b}$. Thus, the estimator $\\hat{\\mu}_{\\text{Bayes}}$ is:\n$$ \\hat{\\mu}_{\\text{Bayes}} = E[\\mu | \\mathcal{D}] = \\frac{a_n}{b_n} = \\frac{a_0 + \\sum_{i=1}^{n} K_i}{b_0 + L \\sum_{i=1}^{n} t_i} $$\n\nWe now substitute the numerical values provided in the problem statement. The dataset consists of an alignment length $L=10000$ sites, $n=5$ tips with sampling times $(t_1,t_2,t_3,t_4,t_5) = (0.8, 1.6, 2.7, 3.9, 5.2)$ years and root-to-tip substitution counts $(K_1,K_2,K_3,K_4,K_5) = (8, 20, 28, 39, 53)$. The prior hyperparameters are $(a_0,b_0) = (2,2000)$.\n\nFirst, we compute the sum of the substitution counts and the sum of the sampling times:\n$$ \\sum_{i=1}^{5} K_i = 8 + 20 + 28 + 39 + 53 = 148 $$\n$$ \\sum_{i=1}^{5} t_i = 0.8 + 1.6 + 2.7 + 3.9 + 5.2 = 14.2 $$\n\nNext, we compute the posterior parameters $a_n$ and $b_n$:\n$$ a_n = a_0 + \\sum_{i=1}^{5} K_i = 2 + 148 = 150 $$\n$$ b_n = b_0 + L \\sum_{i=1}^{5} t_i = 2000 + 10000 \\times 14.2 = 2000 + 142000 = 144000 $$\n\nFinally, we compute the posterior mean, which is the Bayes estimator for $\\mu$:\n$$ \\hat{\\mu}_{\\text{Bayes}} = \\frac{a_n}{b_n} = \\frac{150}{144000} = \\frac{1}{960} \\approx 0.001041666... $$\n\nThe problem requires the result to be rounded to four significant figures. The numerical value is $1.041666... \\times 10^{-3}$. Rounding to four significant figures gives $1.042 \\times 10^{-3}$ substitutions per site per year.",
            "answer": "$$\\boxed{1.042 \\times 10^{-3}}$$"
        },
        {
            "introduction": "Real-world datasets often have complexities that must be explicitly modeled, such as the ascertainment bias common in SNP data. This exercise advances from parameter estimation to model comparison, a central task in Bayesian inference. You will compute a Bayes factor to weigh the evidence for two competing topologies, learning to derive site pattern probabilities under the Cavender-Farris-Neyman model and apply a formal correction for ascertainment bias, thereby ensuring a rigorous comparison .",
            "id": "4542022",
            "problem": "A research team is conducting Bayesian phylogenetic inference from Single Nucleotide Polymorphism (SNP) data on three taxa $\\{A,B,C\\}$ under biallelic states. The ascertainment protocol included only sites that are variable across the three taxa (i.e., all sites invariant across the sample were excluded at discovery). To explicitly account for this ascertainment, they use an ascertainment-corrected likelihood that conditions on observing a variable site. Assume the following modeling choices, which the team regards as the fundamental base for inference:\n\n- The sequence evolution across sites follows the Cavender–Farris–Neyman (CFN) model (two-state symmetric continuous-time Markov chain) on a rooted $3$-taxon tree, with independent and identically distributed sites, and stationary base frequencies $P(0)=P(1)=\\tfrac{1}{2}$.\n- Along an edge $e$, the probability of a state change is denoted by $s_{e} \\in [0, \\tfrac{1}{2}]$, and the probability of no change is $1 - s_{e}$. Different edges evolve independently.\n- The rooted model tree for topology $\\mathcal{T}_{1}$ is $((A,B),C)$ with the branch leading to $A$ having change probability $s$, the branch leading to $B$ having change probability $s$, the branch leading to $C$ having change probability $s$, and the internal branch between the $\\{A,B\\}$ ancestor and $C$ having change probability $r$. The alternative topology $\\mathcal{T}_{2}$ is $((A,C),B)$ with the same edge-specific change probabilities attached to corresponding pendant and internal edges.\n- The ascertainment-corrected (Lewis-type) likelihood for each site is the ordinary site likelihood divided by the probability that the site is variable under the model, so that inference is performed conditional on variability.\n\nFrom $N$ variable SNPs, the team tallies parsimony-informative pattern counts, where $n_{AB|C}$ denotes the number of variable sites in which $A$ and $B$ share the same state and differ from $C$ (collapsing the two complementary assignments), and analogously for $n_{AC|B}$ and $n_{BC|A}$. They observe\n$$\nn_{AB|C} = 120,\\quad n_{AC|B} = 80,\\quad n_{BC|A} = 100.\n$$\nAssume $s = 0.1$ on each pendant branch and $r = 0.05$ on the internal branch for both topologies. Treat sites as independent and adopt equal prior probability on the two topologies.\n\nUsing only the foundational definitions above and basic probability, derive the ascertainment-corrected pattern probabilities under the CFN model for each topology and compute the Bayes factor (ratio of marginal likelihoods, which here equals the ratio of ascertainment-corrected likelihoods due to equal priors and fixed branch change probabilities) in favor of $\\mathcal{T}_{1}$ over $\\mathcal{T}_{2}$. Round your final Bayes factor to four significant figures. The final answer must be a single real number without units.",
            "solution": "To compute the Bayes factor in favor of topology $\\mathcal{T}_1$ over $\\mathcal{T}_2$, we need to find the ratio of their marginal likelihoods, $BF_{12} = P(D|\\mathcal{T}_1)/P(D|\\mathcal{T}_2)$. Since the model parameters ($s$, $r$) are fixed and the prior probabilities on the topologies are equal, this simplifies to the ratio of the likelihoods evaluated for the given data $D=\\{n_{AB|C}, n_{AC|B}, n_{BC|A}\\}$.\n\nThe likelihood for a given topology $\\mathcal{T}$ is based on a multinomial distribution. However, because the data includes only variable sites, we must use an ascertainment-corrected likelihood. The probability of observing a specific variable site pattern is its raw probability divided by the total probability of observing any variable site.\n$$ P(\\text{pattern}|\\mathcal{T}, \\text{var}) = \\frac{P(\\text{pattern}|\\mathcal{T})}{P(\\text{site is variable}|\\mathcal{T})} $$\n\n**1. Pattern Probabilities for Topology $\\mathcal{T}_1 = ((A,B),C)$**\n\nUnder the CFN model with the given tree structure and parameters, we can derive the probabilities for the three possible variable site patterns. A pattern is congruent with the topology if it groups the sister taxa (A and B). The other two patterns are incongruent.\n- The probability of the congruent pattern, $\\{A,B\\} \\neq C$, is:\n  $$ p_{cong} = P(AB|C|\\mathcal{T}_1) = s(1-s) + r(1-2s)^2 $$\n- The probabilities of the two incongruent patterns, $\\{A,C\\} \\neq B$ and $\\{B,C\\} \\neq A$, are identical due to the model's symmetry:\n  $$ p_{incong} = P(AC|B|\\mathcal{T}_1) = P(BC|A|\\mathcal{T}_1) = s(1-s) $$\n\nThe total probability of a site being variable is the sum of these probabilities:\n$$ P(\\text{var}|\\mathcal{T}) = p_{cong} + 2p_{incong} = 3s(1-s) + r(1-2s)^2 $$\n\nThe ascertainment-corrected probabilities for $\\mathcal{T}_1$ are:\n- $\\pi_{AB|C} = P(AB|C|\\mathcal{T}_1, \\text{var}) = \\frac{p_{cong}}{P(\\text{var}|\\mathcal{T})}$\n- $\\pi_{AC|B} = P(AC|B|\\mathcal{T}_1, \\text{var}) = \\frac{p_{incong}}{P(\\text{var}|\\mathcal{T})}$\n- $\\pi_{BC|A} = P(BC|A|\\mathcal{T}_1, \\text{var}) = \\frac{p_{incong}}{P(\\text{var}|\\mathcal{T})}$\n\n**2. Pattern Probabilities for Topology $\\mathcal{T}_2 = ((A,C),B)$**\n\nBy symmetry, we can find the probabilities for $\\mathcal{T}_2$ by swapping the roles of taxa B and C. The congruent pattern is now $\\{A,C\\} \\neq B$.\n- $P(AC|B|\\mathcal{T}_2) = p_{cong}$\n- $P(AB|C|\\mathcal{T}_2) = p_{incong}$\n- $P(BC|A|\\mathcal{T}_2) = p_{incong}$\n\nThe total probability of variability $P(\\text{var}|\\mathcal{T})$ is the same for both topologies.\n\n**3. Computing the Bayes Factor**\n\nThe likelihood for each topology is proportional to the product of the corrected pattern probabilities raised to the power of their observed counts.\n$$ L(D|\\mathcal{T}_1) \\propto (\\pi_{AB|C})^{n_{AB|C}} (\\pi_{AC|B})^{n_{AC|B}} (\\pi_{BC|A})^{n_{BC|A}} = \\left(\\frac{p_{cong}}{P_{var}}\\right)^{120} \\left(\\frac{p_{incong}}{P_{var}}\\right)^{80+100} $$\n$$ L(D|\\mathcal{T}_2) \\propto (P(AB|C|\\mathcal{T}_2, \\text{var}))^{n_{AB|C}} (P(AC|B|\\mathcal{T}_2, \\text{var}))^{n_{AC|B}} (P(BC|A|\\mathcal{T}_2, \\text{var}))^{n_{BC|A}} = \\left(\\frac{p_{incong}}{P_{var}}\\right)^{120} \\left(\\frac{p_{cong}}{P_{var}}\\right)^{80} \\left(\\frac{p_{incong}}{P_{var}}\\right)^{100} $$\nThe Bayes factor is the ratio of these likelihoods:\n$$ BF_{12} = \\frac{L(D|\\mathcal{T}_1)}{L(D|\\mathcal{T}_2)} = \\frac{(p_{cong})^{120} (p_{incong})^{180}}{(p_{cong})^{80} (p_{incong})^{220}} = \\frac{(p_{cong})^{40}}{(p_{incong})^{40}} = \\left(\\frac{p_{cong}}{p_{incong}}\\right)^{40} $$\n\n**4. Numerical Calculation**\n\nWe substitute the given values $s=0.1$ and $r=0.05$ into the ratio:\n$$ \\frac{p_{cong}}{p_{incong}} = \\frac{s(1-s) + r(1-2s)^2}{s(1-s)} = 1 + \\frac{r(1-2s)^2}{s(1-s)} $$\n$$ \\frac{p_{cong}}{p_{incong}} = 1 + \\frac{0.05 \\times (1 - 2 \\times 0.1)^2}{0.1 \\times (1 - 0.1)} = 1 + \\frac{0.05 \\times (0.8)^2}{0.09} = 1 + \\frac{0.05 \\times 0.64}{0.09} = 1 + \\frac{0.032}{0.09} = 1 + \\frac{16}{45} = \\frac{61}{45} $$\nThe Bayes factor is:\n$$ BF_{12} = \\left(\\frac{61}{45}\\right)^{40} \\approx (1.3555...)^{40} \\approx 192618.39 $$\nRounding to four significant figures, the result is $1.926 \\times 10^5$.",
            "answer": "$$\\boxed{1.926 \\times 10^5}$$"
        },
        {
            "introduction": "While analytical solutions are instructive, much of modern Bayesian phylogenetics relies on computation to handle complex models. This capstone practice challenges you to build a phylogenetic inference engine from first principles, bridging the gap between theory and application. You will implement the Felsenstein pruning algorithm to calculate the likelihood $p(D \\mid T, \\mathbf{t})$ and then use numerical integration to obtain the marginal likelihood $p(D \\mid T)$, the key ingredient for comparing topologies .",
            "id": "4542079",
            "problem": "Construct a complete, runnable program that, given fixed Deoxyribonucleic Acid (DNA) alignments for four taxa labeled $A$, $B$, $C$, and $D$, computes the posterior probability of the unrooted split $\\{A,B\\} \\mid \\{C,D\\}$ under Bayesian inference in phylogenetics, by marginalizing over branch lengths with a specified prior and using the Jukes–Cantor 1969 (JC69) substitution model. The intended audience is at the advanced graduate level, and the task is to apply first principles of Bayesian inference, continuous parameter marginalization, and likelihood computation via the Felsenstein pruning algorithm.\n\nThe foundational base must be the following well-tested facts and definitions:\n- Bayes' theorem: for a discrete topology $T$ and continuous branch length vector $\\mathbf{t}$ with prior $p(\\mathbf{t} \\mid T)$, the posterior $p(T \\mid D)$ given data $D$ satisfies\n$$\np(T \\mid D) \\propto p(T) \\int p(D \\mid T, \\mathbf{t}) \\, p(\\mathbf{t} \\mid T) \\, d\\mathbf{t}.\n$$\n- Time-reversible continuous-time Markov process for the JC69 model: for branch length $t \\ge 0$, with rate normalized to $1$, the transition probability from nucleotide $i$ to nucleotide $j$ over time $t$ is\n$$\nP_{ij}(t) = \n\\begin{cases}\n\\frac{1}{4} + \\frac{3}{4} e^{-4t/3}, & \\text{if } i = j, \\\\\n\\frac{1}{4} - \\frac{1}{4} e^{-4t/3}, & \\text{if } i \\ne j,\n\\end{cases}\n$$\nand the stationary distribution is uniform, $\\pi_i = \\frac{1}{4}$ for all $i$.\n- The Felsenstein pruning algorithm: the likelihood $p(D \\mid T, \\mathbf{t})$ for a fixed tree $T$ and branch lengths $\\mathbf{t}$ factorizes across sites under independence, and can be computed by dynamic programming that propagates conditional likelihood vectors from leaves to the root along the tree.\n\nYou must not introduce any shortcut formulas beyond these foundations. Use only these principles to derive what to compute, why it is correct, and how to structure the algorithm.\n\nAssume the following modeling choices, which must be implemented exactly:\n- There are three possible unrooted binary topologies on the four taxa: $T_1 = \\{A,B\\} \\mid \\{C,D\\}$, $T_2 = \\{A,C\\} \\mid \\{B,D\\}$, and $T_3 = \\{A,D\\} \\mid \\{B,C\\}$, with prior $p(T_k) = \\frac{1}{3}$ for $k \\in \\{1,2,3\\}$.\n- For each topology $T$, the branch lengths are $\\mathbf{t} = (t_A, t_B, t_{\\text{int}}, t_C, t_D)$, corresponding to pendant branches to each leaf and a single internal branch. The prior over branch lengths is independent exponential with rate $\\lambda$, that is,\n$$\np(\\mathbf{t}\\mid T) = \\prod_{x \\in \\{A,B,\\text{int},C,D\\}} \\lambda e^{-\\lambda t_x}.\n$$\n- Use the JC69 model for nucleotide substitutions with uniform stationary distribution $\\pi_i = \\frac{1}{4}$.\n- To compute $p(D \\mid T)$, approximate the integral over $\\mathbf{t}$ by discrete quadrature over a supplied grid $\\mathcal{G} = \\{g_1, g_2, \\dots, g_m\\}$ for each branch, forming the $m^5$ Cartesian product. Use weights proportional to the prior density, i.e., approximate\n$$\n\\int p(D \\mid T, \\mathbf{t}) p(\\mathbf{t} \\mid T) \\, d\\mathbf{t} \\approx \\sum_{\\mathbf{t} \\in \\mathcal{G}^5} p(D \\mid T, \\mathbf{t}) \\, e^{-\\lambda \\sum_x t_x},\n$$\nwhere any factors constant across topologies cancel in the final posterior normalization. Perform all summations in a numerically stable way.\n\nDefine the site-likelihood by rooting the tree at one internal node compatible with the specified split and applying the Felsenstein pruning algorithm:\n- For $T_1 = \\{A,B\\} \\mid \\{C,D\\}$, root at the internal node connecting $A$, $B$, and the internal edge to node $R$; node $R$ connects to $C$ and $D$. The branch lengths are $(t_A, t_B, t_{\\text{int}}, t_C, t_D)$ along edges $(A, B, \\text{internal}, C, D)$.\n- For $T_2 = \\{A,C\\} \\mid \\{B,D\\}$, root at the internal node connecting $A$, $C$, and the internal edge to node $R$; node $R$ connects to $B$ and $D$.\n- For $T_3 = \\{A,D\\} \\mid \\{B,C\\}$, root at the internal node connecting $A$, $D$, and the internal edge to node $R$; node $R$ connects to $B$ and $C$.\n\nAt a leaf with observed nucleotide $b \\in \\{A,C,G,T\\}$ and branch length $t$, the conditional likelihood vector to its parent, indexed by parent nucleotide $i$, is $P_{i b}(t)$. At an internal node $R$ with two child messages $M^{(1)}(k)$ and $M^{(2)}(k)$ for nucleotide $k$, the node likelihood vector is $L_R(k) = M^{(1)}(k) M^{(2)}(k)$, and the message to its parent along branch length $t_{\\text{int}}$ is $M_R(i) = \\sum_{k} P_{i k}(t_{\\text{int}}) L_R(k)$. At the root, the site likelihood is $\\sum_i \\pi_i \\prod_{\\text{incident edges}} M_{\\text{edge}}(i)$.\n\nYour program must implement the above and report, for a set of given test cases, the posterior probability $p(T_1 \\mid D)$ of the split $\\{A,B\\} \\mid \\{C,D\\}$.\n\nTest Suite:\nFor each test case, you are given the DNA sequences for taxa $A$, $B$, $C$, and $D$, the exponential rate $\\lambda$, and the branch length grid $\\mathcal{G}$.\n\n- Test case $1$ (happy path, strong clade signal):\n  - $A$: \"ACGTACGTACGTACGTACGTACGT\"\n  - $B$: \"ACGTACGTACGTACGTACGTACGT\"\n  - $C$: \"TGCATGCATGCATGCATGCATGCA\"\n  - $D$: \"TGCATGCATGCATGCATGCATGCA\"\n  - $\\lambda = 2.0$\n  - $\\mathcal{G} = \\{0.02, 0.15, 0.5\\}$\n\n- Test case $2$ (near-ambiguous signal):\n  - $A$: \"ACGTACGTACGTACGTACGTACGT\"\n  - $B$: \"CGTACGTACGTACGTACGTACGTA\"\n  - $C$: \"GTACGTACGTACGTACGTACGTAC\"\n  - $D$: \"TACGTACGTACGTACGTACGTACG\"\n  - $\\lambda = 2.0$\n  - $\\mathcal{G} = \\{0.02, 0.15, 0.5\\}$\n\n- Test case $3$ (edge case, minimal data):\n  - $A$: \"A\"\n  - $B$: \"A\"\n  - $C$: \"C\"\n  - $D$: \"C\"\n  - $\\lambda = 2.0$\n  - $\\mathcal{G} = \\{0.02, 0.15, 0.5\\}$\n\nRequired final output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\"). Each result must be the posterior probability for the split $\\{A,B\\} \\mid \\{C,D\\}$ for the corresponding test case, expressed as a decimal number rounded to $6$ digits after the decimal point. No other text may be printed.\n\nAll angles are irrelevant in this problem. There are no physical units in this problem. Percentages must not be used; express probabilities as decimals.",
            "solution": "The problem requires implementing a Bayesian phylogenetic inference engine from first principles. The goal is to compute the posterior probability of a specific tree topology by marginalizing over all possible branch lengths.\n\n**Methodology**\n\nAccording to Bayes' theorem, the posterior probability of a topology $T_k$ given data $D$ is:\n$$p(T_k \\mid D) = \\frac{p(D \\mid T_k) p(T_k)}{\\sum_{j=1}^{3} p(D \\mid T_j) p(T_j)}$$\nWith a uniform prior on topologies ($p(T_k) = 1/3$), this simplifies to the ratio of marginal likelihoods:\n$$p(T_1 \\mid D) = \\frac{p(D \\mid T_1)}{\\sum_{j=1}^{3} p(D \\mid T_j)}$$\nThe core task is computing the marginal likelihood, $p(D \\mid T_k)$, for each of the three possible four-taxon topologies. This is done by integrating the likelihood over the prior distribution of branch lengths $\\mathbf{t}$:\n$$p(D \\mid T_k) = \\int p(D \\mid T_k, \\mathbf{t}) \\, p(\\mathbf{t} \\mid T_k) \\, d\\mathbf{t}$$\nThe problem specifies approximating this integral with a discrete sum (quadrature) over a grid $\\mathcal{G}$ for each of the five branch lengths. Ignoring constants that cancel out, we compute:\n$$M_k = \\sum_{\\mathbf{t} \\in \\mathcal{G}^5} p(D \\mid T_k, \\mathbf{t}) \\, e^{-\\lambda \\sum_{x} t_x}$$\nThe final posterior probability for topology $T_1$ is then $M_1 / (M_1 + M_2 + M_3)$.\n\n**Likelihood Calculation via Felsenstein's Pruning Algorithm**\n\nFor any given topology $T_k$ and branch length vector $\\mathbf{t}$, the likelihood of the alignment, $p(D \\mid T_k, \\mathbf{t})$, is the product of the likelihoods for each site, assuming independence. The likelihood for a single site is computed using Felsenstein's pruning algorithm. This algorithm works by propagating \"conditional likelihood vectors\" from the leaves of the tree to an arbitrarily chosen root.\n\n1.  **Leaves**: For each leaf, the conditional likelihood vector gives the probability of the observed nucleotide given each possible state of its parent node. This vector is a column from the JC69 transition probability matrix $P(t)$.\n2.  **Internal Nodes**: At an internal node, the vectors from its children are combined via element-wise multiplication.\n3.  **Propagation**: The resulting vector is then propagated up to its parent by multiplying it with the transition matrix of the connecting branch.\n4.  **Root**: At the root, all incoming vectors are combined. The final site likelihood is the sum of the elements of this final vector, weighted by the stationary probabilities of the root states ($\\pi_i = 1/4$ for all nucleotides).\n\nAll computations are performed using log-likelihoods and the log-sum-exp trick to ensure numerical stability.\n\n**Implementation**\n\nThe following Python program implements this logic to solve the problem for the given test cases.\n\n```python\nimport numpy as np\nfrom itertools import product\n\ndef solve():\n    \"\"\"\n    Main function to solve the phylogenetic inference problem for the given test cases.\n    \"\"\"\n\n    NUC_MAP = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n    \n    def get_jc69_matrix(t):\n        \"\"\"\n        Computes the JC69 transition probability matrix for a given branch length t.\n        \"\"\"\n        if t < 0:\n            raise ValueError(\"Branch length t cannot be negative.\")\n        \n        p_diag = 0.25 + 0.75 * np.exp(-4.0 * t / 3.0)\n        p_off_diag = 0.25 - 0.25 * np.exp(-4.0 * t / 3.0)\n        \n        mat = np.full((4, 4), p_off_diag)\n        np.fill_diagonal(mat, p_diag)\n        return mat\n\n    def compute_site_likelihood(site_data, taxa_split, branch_lengths, p_matrices, nuc_map):\n        \"\"\"\n        Computes the likelihood for a single site using Felsenstein's pruning algorithm.\n        \"\"\"\n        taxa_u = taxa_split[0]\n        taxa_v = taxa_split[1]\n        \n        t_map = {\n            taxa_u[0]: branch_lengths[0],\n            taxa_u[1]: branch_lengths[1],\n            taxa_v[0]: branch_lengths[2],\n            taxa_v[1]: branch_lengths[3],\n            'int': branch_lengths[4],\n        }\n\n        # Get conditional likelihood vectors from leaves.\n        L_leaf_u0 = p_matrices[t_map[taxa_u[0]]][:, nuc_map[site_data[taxa_u[0]]]]\n        L_leaf_u1 = p_matrices[t_map[taxa_u[1]]][:, nuc_map[site_data[taxa_u[1]]]]\n        L_leaf_v0 = p_matrices[t_map[taxa_v[0]]][:, nuc_map[site_data[taxa_v[0]]]]\n        L_leaf_v1 = p_matrices[t_map[taxa_v[1]]][:, nuc_map[site_data[taxa_v[1]]]]\n        \n        # Compute conditional likelihood at internal node V\n        L_V = L_leaf_v0 * L_leaf_v1\n        \n        # Pass message from V to U up the internal branch\n        P_int = p_matrices[t_map['int']]\n        M_V = P_int @ L_V\n        \n        # Compute conditional likelihood at root node U\n        L_U = L_leaf_u0 * L_leaf_u1 * M_V\n        \n        # Compute final site likelihood by averaging over root states\n        return np.sum(L_U) * 0.25\n\n    def compute_log_marginal_likelihood(sequences, taxa_split, grid, lmbda, nuc_map):\n        \"\"\"\n        Computes the log of the marginal likelihood for a given topology.\n        \"\"\"\n        num_sites = len(sequences['A'])\n        \n        p_matrices = {t: get_jc69_matrix(t) for t in grid}\n\n        grid_size = len(grid)\n        num_combinations = grid_size ** 5\n        log_terms = np.zeros(num_combinations)\n        \n        # Iterate over all 5-tuples of branch lengths from the grid\n        for i, bl_tuple in enumerate(product(grid, repeat=5)):\n            \n            # The order of bl_tuple corresponds to the 5 branches.\n            total_log_likelihood = 0.0\n            for s in range(num_sites):\n                site_data = {taxon: sequences[taxon][s] for taxon in 'ABCD'}\n                site_likelihood = compute_site_likelihood(site_data, taxa_split, bl_tuple, p_matrices, nuc_map)\n                \n                if site_likelihood > 0:\n                    total_log_likelihood += np.log(site_likelihood)\n                else:\n                    total_log_likelihood += -np.inf\n\n            log_prior_term = -lmbda * sum(bl_tuple)\n            \n            log_terms[i] = total_log_likelihood + log_prior_term\n        \n        max_log_term = np.max(log_terms)\n        if max_log_term == -np.inf:\n            return -np.inf\n            \n        log_marginal_lik = max_log_term + np.log(np.sum(np.exp(log_terms - max_log_term)))\n        \n        return log_marginal_lik\n\n    test_cases = [\n        {\n            \"sequences\": {\n                'A': \"ACGTACGTACGTACGTACGTACGT\",\n                'B': \"ACGTACGTACGTACGTACGTACGT\",\n                'C': \"TGCATGCATGCATGCATGCATGCA\",\n                'D': \"TGCATGCATGCATGCATGCATGCA\",\n            },\n            \"lambda\": 2.0,\n            \"grid\": [0.02, 0.15, 0.5]\n        },\n        {\n            \"sequences\": {\n                'A': \"ACGTACGTACGTACGTACGTACGT\",\n                'B': \"CGTACGTACGTACGTACGTACGTA\",\n                'C': \"GTACGTACGTACGTACGTACGTAC\",\n                'D': \"TACGTACGTACGTACGTACGTACG\",\n            },\n            \"lambda\": 2.0,\n            \"grid\": [0.02, 0.15, 0.5]\n        },\n        {\n            \"sequences\": {\n                'A': \"A\",\n                'B': \"A\",\n                'C': \"C\",\n                'D': \"C\",\n            },\n            \"lambda\": 2.0,\n            \"grid\": [0.02, 0.15, 0.5]\n        }\n    ]\n\n    results = []\n    \n    topologies = [\n        (('A', 'B'), ('C', 'D')),  # T1\n        (('A', 'C'), ('B', 'D')),  # T2\n        (('A', 'D'), ('B', 'C')),  # T3\n    ]\n\n    for case in test_cases:\n        sequences = case[\"sequences\"]\n        lmbda = case[\"lambda\"]\n        grid = tuple(sorted(case[\"grid\"]))\n\n        log_marginal_likelihoods = []\n        for taxa_split in topologies:\n            log_ml = compute_log_marginal_likelihood(sequences, taxa_split, grid, lmbda, NUC_MAP)\n            log_marginal_likelihoods.append(log_ml)\n        \n        log_ml_arr = np.array(log_marginal_likelihoods)\n        \n        max_log_ml = np.max(log_ml_arr)\n        \n        if max_log_ml == -np.inf:\n             posteriors = np.full(3, 1.0/3.0)\n        else:\n            exp_terms = np.exp(log_ml_arr - max_log_ml)\n            sum_exp_terms = np.sum(exp_terms)\n            posteriors = exp_terms / sum_exp_terms\n        \n        result_t1 = posteriors[0]\n        results.append(f\"{result_t1:.6f}\")\n\n    # The required output is a single line print.\n    print(f\"[{','.join(results)}]\")\n\n# The function is defined but not called in this context,\n# as it's intended to be the solution logic.\n# solve()\n```",
            "answer": "[1.000000,0.333333,0.999997]"
        }
    ]
}