{
    "hands_on_practices": [
        {
            "introduction": "图卷积网络（GCNs）的核心在于其能够在图结构上平滑地传播和聚合信息。归一化拉普拉斯算子是实现这一目标的关键数学工具，它能确保信息聚合过程的稳定性，特别是在处理节点重要性（度）差异巨大的网络时。通过一个具体的计算练习，您将亲手构建这一算子，从而深刻理解GCN滤波操作的谱图理论基础。",
            "id": "4570165",
            "problem": "考虑一个分子图表示，其中原子是顶点，共价键是无向边。在谱图理论中，将归一化拉普拉斯算子定义为 $L = I - D^{-1/2} A D^{-1/2}$，其中 $A$ 是邻接矩阵，$D$ 是对角度矩阵，$I$ 是单位矩阵。在图神经网络（GNN）中，特别是在图卷积网络（GCN）中，图滤波使用由 $L$ 构建的算子作用于图信号，以实现尊重交互网络拓扑的局部平滑。从谱域中的图卷积对应于在 $L$ 的特征值上乘以一个谱滤波器，以及 $L$ 的多项式实现局部空间滤波器这一基本出发点，解释为什么对称归一化 $L = I - D^{-1/2} A D^{-1/2}$ 被用于在生物分子交互网络中稳定跨异构度的滤波。\n\n现在考虑水分子的无权无向分子图，其中顶点2是氧原子，顶点1和3是氢原子，化学键为 $(1,2)$ 和 $(2,3)$。构建 $A$、$D$ 并计算归一化拉普拉斯算子 $L$。最后，计算该图的 $L$ 的第二小特征值（代数连通度）$\\lambda_{2}$。将 $\\lambda_{2}$ 作为你的最终答案。无需四舍五入，该量是无量纲的。",
            "solution": "该问题被评估为有效。它在科学上基于谱图理论和图神经网络（GNN）的原理，问题设定良好，具有唯一定义的计算任务，并且语言客观。问题为解释和计算两个部分都提供了所有必要的信息。\n\n该问题包含两部分。第一部分要求解释在GNN中使用对称归一化拉普拉斯算子的原因。第二部分要求计算水分子图的归一化拉普拉斯算子及其第二小特征值。\n\n\\subsection*{第一部分：对称归一化的解释}\n\n在图信号处理和GNN中，图卷积操作可以被解释为一个滤波器，它平滑定义在图顶点上的信号。图信号是一个向量 $x \\in \\mathbb{R}^N$，其中 $x_i$ 是信号在顶点 $i$ 处的值（例如，一个特征向量）。最简单的滤波操作涉及从相邻节点聚合信息。这可以表示为信号 $x$ 与图的邻接矩阵 $A$ 的乘法。新的信号 $y$ 由 $y = Ax$ 给出，其中 $y_i = \\sum_{j \\in \\mathcal{N}(i)} x_j$。\n\n在具有异构度分布的网络（例如生物分子交互网络）中使用原始邻接矩阵 $A$ 会出现一个重要问题。度数高的节点（中心节点）将比度数低的节点具有大得多的 $y_i$ 值，这仅仅是因为它们对更多的邻居进行求和。这可能导致特征向量的尺度高度依赖于节点度，从而在深度神经网络的训练过程中引起数值不稳定和梯度爆炸。\n\n为了解决这个问题，引入了归一化。一个常见的初始方法是行归一化，使用算子 $D^{-1}A$，其中 $D$ 是对角度矩阵。滤波操作变为 $y = D^{-1}Ax$，它计算条目 $y_i = \\frac{1}{\\deg(i)} \\sum_{j \\in \\mathcal{N}(i)} x_j$。这个操作用邻居节点特征的平均值取代了求和。虽然这解决了接收节点 $i$ 的特征尺度爆炸问题，但它引入了另一种偏差：它没有考虑发送节点 $j$ 的度。来自中心节点的消息与来自度为1的节点的消息被同等对待。\n\n对称归一化，即归一化拉普拉斯算子 $L = I - D^{-1/2} A D^{-1/2}$ 的基础，解决了这个局限性。在图卷积网络（GCN）传播规则中使用的算子实际上是 $\\tilde{A} = D^{-1/2} A D^{-1/2}$（通常会添加自环，但我们在这里考虑其基本形式）。滤波操作 $y = \\tilde{A}x$ 的元素形式为：\n$$y_i = \\sum_{j=1}^N (\\tilde{A})_{ij} x_j = \\sum_{j=1}^N \\frac{A_{ij}}{\\sqrt{\\deg(i)\\deg(j)}} x_j = \\frac{1}{\\sqrt{\\deg(i)}} \\sum_{j \\in \\mathcal{N}(i)} \\frac{x_j}{\\sqrt{\\deg(j)}}$$\n这种形式通过源节点和目标节点的度来归一化信息流。它防止了高度节点在聚合过程中占主导地位，从而形成一个更稳定和平衡的更新规则。\n\n从谱的角度来看，使用归一化拉普拉斯算子 $L$ 至关重要。$L$ 的特征值保证位于区间 $[0, 2]$ 内。这个性质对于深度GNN的稳定性至关重要。一个GCN层对图信号应用一个滤波器。堆叠多层对应于应用图滤波算子的一个多项式。如果算子的特征值量级大于1，重复应用可能导致信号（和梯度）的爆炸或消失。通过确保 $L$ 的特征值被限制在一个小的、稳定的范围内，对称归一化允许构建行为良好的多项式滤波器，从而能够创建深度且有效的GNN模型。总之，使用对称归一化 $D^{-1/2} A D^{-1/2}$ 是因为它 (1) 在空间上平衡了不同度数节点之间的影响，以及 (2) 在谱上确保了深度网络中图滤波操作的稳定性。\n\n\\subsection*{第二部分：水分子的计算}\n\n水分子 H-O-H 表示为一个无权无向图，顶点为 $V = \\{1, 2, 3\\}$，其中顶点2是氧（O），顶点1和3是氢（H）。化学键作为边给出，即 $E = \\{(1, 2), (2, 3)\\}$。\n\n首先，我们构建邻接矩阵 $A$。这是一个 $3 \\times 3$ 的对称矩阵，如果顶点 $i$ 和 $j$ 之间存在边，则 $A_{ij} = 1$，否则 $A_{ij} = 0$。\n$$A = \\begin{pmatrix} 0 & 1 & 0 \\\\ 1 & 0 & 1 \\\\ 0 & 1 & 0 \\end{pmatrix}$$\n\n接下来，我们构建对角度矩阵 $D$。顶点的度是连接到它的边的数量。\n$\\deg(1) = 1$\n$\\deg(2) = 2$\n$\\deg(3) = 1$\n度矩阵 $D$ 在其对角线上有这些度值，其他位置为零。\n$$D = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}$$\n\n然后我们计算矩阵 $D^{-1/2}$，它是通过对 $D$ 的每个对角元素取逆平方根得到的。\n$$D^{-1/2} = \\begin{pmatrix} 1^{-1/2} & 0 & 0 \\\\ 0 & 2^{-1/2} & 0 \\\\ 0 & 0 & 1^{-1/2} \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & \\frac{1}{\\sqrt{2}} & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}$$\n\n现在，我们计算对称归一化的邻接矩阵 $\\tilde{A} = D^{-1/2} A D^{-1/2}$。\n$$\\tilde{A} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & \\frac{1}{\\sqrt{2}} & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 0 & 1 & 0 \\\\ 1 & 0 & 1 \\\\ 0 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & \\frac{1}{\\sqrt{2}} & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}$$\n$$\\tilde{A} = \\begin{pmatrix} 0 & 1 & 0 \\\\ \\frac{1}{\\sqrt{2}} & 0 & \\frac{1}{\\sqrt{2}} \\\\ 0 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & \\frac{1}{\\sqrt{2}} & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 0 & \\frac{1}{\\sqrt{2}} & 0 \\\\ \\frac{1}{\\sqrt{2}} & 0 & \\frac{1}{\\sqrt{2}} \\\\ 0 & \\frac{1}{\\sqrt{2}} & 0 \\end{pmatrix}$$\n\n归一化拉普拉斯算子 $L$ 定义为 $L = I - \\tilde{A}$。\n$$L = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} - \\begin{pmatrix} 0 & \\frac{1}{\\sqrt{2}} & 0 \\\\ \\frac{1}{\\sqrt{2}} & 0 & \\frac{1}{\\sqrt{2}} \\\\ 0 & \\frac{1}{\\sqrt{2}} & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & -\\frac{1}{\\sqrt{2}} & 0 \\\\ -\\frac{1}{\\sqrt{2}} & 1 & -\\frac{1}{\\sqrt{2}} \\\\ 0 & -\\frac{1}{\\sqrt{2}} & 1 \\end{pmatrix}$$\n\n最后，我们通过求解特征方程 $\\det(L - \\lambda I) = 0$ 来计算 $L$ 的特征值。\n$$\\det \\begin{pmatrix} 1-\\lambda & -\\frac{1}{\\sqrt{2}} & 0 \\\\ -\\frac{1}{\\sqrt{2}} & 1-\\lambda & -\\frac{1}{\\sqrt{2}} \\\\ 0 & -\\frac{1}{\\sqrt{2}} & 1-\\lambda \\end{pmatrix} = 0$$\n沿第一行展开行列式：\n$$(1-\\lambda) \\left| \\begin{matrix} 1-\\lambda & -\\frac{1}{\\sqrt{2}} \\\\ -\\frac{1}{\\sqrt{2}} & 1-\\lambda \\end{matrix} \\right| - \\left(-\\frac{1}{\\sqrt{2}}\\right) \\left| \\begin{matrix} -\\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}} \\\\ 0 & 1-\\lambda \\end{matrix} \\right| = 0$$\n$$(1-\\lambda) \\left[ (1-\\lambda)^2 - \\left(-\\frac{1}{\\sqrt{2}}\\right)^2 \\right] + \\frac{1}{\\sqrt{2}} \\left[ \\left(-\\frac{1}{\\sqrt{2}}\\right)(1-\\lambda) - 0 \\right] = 0$$\n$$(1-\\lambda) \\left[ (1-\\lambda)^2 - \\frac{1}{2} \\right] - \\frac{1}{2}(1-\\lambda) = 0$$\n我们可以提出公因式 $(1-\\lambda)$：\n$$(1-\\lambda) \\left[ (1-\\lambda)^2 - \\frac{1}{2} - \\frac{1}{2} \\right] = 0$$\n$$(1-\\lambda) \\left[ (1-\\lambda)^2 - 1 \\right] = 0$$\n这个方程产生三个关于 $\\lambda$ 的解：\n1. $1-\\lambda = 0 \\implies \\lambda = 1$\n2. $(1-\\lambda)^2 - 1 = 0 \\implies (1-\\lambda)^2 = 1 \\implies 1-\\lambda = \\pm 1$\n   - $1-\\lambda = 1 \\implies \\lambda = 0$\n   - $1-\\lambda = -1 \\implies \\lambda = 2$\n\n$L$ 的特征值按非递减顺序排列为 $\\lambda_1 = 0$，$\\lambda_2 = 1$ 和 $\\lambda_3 = 2$。最小的特征值是 $\\lambda_1 = 0$，这对于一个连通图是符合预期的。第二小的特征值（代数连通度）是 $\\lambda_2$。\n\n因此，$\\lambda_2 = 1$。",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "真实的生物网络，如药物-靶点相互作用网络，通常是异构的，包含多种不同类型的关系（例如，“抑制”、“激活”）。关系图卷积网络（R-GCNs）是一种强大的GCN变体，它能为每种关系类型学习独特的变换。本练习将指导您从第一性原理出发，推导并应用R-GCN的核心更新规则，这是处理此类复杂网络的关键技能。",
            "id": "4570117",
            "problem": "考虑一个生物信息学和医学数据分析中的类型化分子相互作用网络，其中节点代表药物和蛋白质，有向边用相互作用类型进行标注。图神经网络（GNN）需要对邻域多重集具有置换不变性，而关系图卷积网络（R-GCN）通过对类型化边使用特定于关系的线性变换以及一个可选的自环贡献来扩展这一思想。从以下原则出发：(i) 图上的消息传递以置换不变的方式聚合来自邻域的信息；(ii) 类型化邻域引出每个关系各自的聚合器；(iii) 线性变换是向量特征上最简单的等变映射。请推导使用特定于关系的权重矩阵和自环项的关系图卷积网络（R-GCN）层的每节点更新方程。假设归一化因子是类型化邻域的大小，$c_{i,r} = |\\mathcal{N}_i^r|$，并且激活函数是恒等映射，$\\sigma(x) = x$。\n\n然后，将您推导的方程应用于以下小型类型化子图。节点集为 $\\mathcal{V} = \\{ d_1, d_2, p_1, p_2 \\}$，其在 $\\mathbb{R}^2$ 中的初始特征向量由下式给出\n$h_{d_1}^{(0)} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$，$h_{d_2}^{(0)} = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}$，$h_{p_1}^{(0)} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$，以及 $h_{p_2}^{(0)} = \\begin{pmatrix} 3 \\\\ -1 \\end{pmatrix}$。\n关系类型集为 $\\mathcal{R} = \\{ \\text{binds}, \\text{inhibits}, \\text{interacts} \\}$。$d_1$ 的入向类型化邻域为\n$\\mathcal{N}_{d_1}^{\\text{binds}} = \\{ p_1 \\}$，$\\mathcal{N}_{d_1}^{\\text{inhibits}} = \\{ p_2 \\}$，以及 $\\mathcal{N}_{d_1}^{\\text{interacts}} = \\{ d_2 \\}$，\n因此 $c_{d_1,\\text{binds}} = c_{d_1,\\text{inhibits}} = c_{d_1,\\text{interacts}} = 1$。\n\n设特定于关系的权重矩阵和自环矩阵为\n$W_{\\text{binds}} = \\begin{pmatrix} 2 & 1 \\\\ 0 & 1 \\end{pmatrix}$，$W_{\\text{inhibits}} = \\begin{pmatrix} -1 & 0 \\\\ 0 & 2 \\end{pmatrix}$，$W_{\\text{interacts}} = \\begin{pmatrix} 1 & -1 \\\\ 3 & 0 \\end{pmatrix}$，以及 $W_{0} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$。\n\n使用您推导的 R-GCN 更新规则，计算单层更新后的特征 $h_{d_1}^{(1)}$，并仅报告其第一个坐标，结果应为精确实数。不要四舍五入；最终答案无需单位。",
            "solution": "推导始于图神经网络（GNN）的核心消息传递原则：层 $l+1$ 的节点级更新是通过使用一个置换不变的算子聚合来自层 $l$ 的节点邻域的消息来构建的。在一个类型化或关系图中，每条边都带有一个关系标签 $r \\in \\mathcal{R}$，因此节点 $i$ 的邻域分解为类型化邻域 $\\mathcal{N}_i^r = \\{ j \\in \\mathcal{V} \\mid (j \\xrightarrow{r} i) \\text{ is an edge} \\}$。为了尊重每个关系的不同语义，聚合器应针对每个关系进行操作。与特征的线性表示相一致的最简单选择是在聚合前对邻居特征 $h_j^{(l)}$ 使用特定于关系的线性映射 $W_r$。为了防止因类型化邻域大小变化而导致的尺度爆炸，引入一个取决于 $\\mathcal{N}_i^r$ 大小的归一化因子 $c_{i,r}$。自环项允许节点通过矩阵 $W_0$ 保留并变换其自身的表示。使用激活函数 $\\sigma$，每个节点的更新因此呈现以下形式\n$$\nh_i^{(l+1)} \\;=\\; \\sigma\\!\\left( \\sum_{r \\in \\mathcal{R}} \\sum_{j \\in \\mathcal{N}_i^r} \\frac{1}{c_{i,r}} \\, W_r \\, h_j^{(l)} \\;+\\; W_0 \\, h_i^{(l)} \\right).\n$$\n根据问题中的假设，$c_{i,r} = |\\mathcal{N}_i^r|$ 且 $\\sigma(x) = x$ (恒等函数)。因此，更新方程简化为\n$$\nh_i^{(l+1)} \\;=\\; \\sum_{r \\in \\mathcal{R}} \\sum_{j \\in \\mathcal{N}_i^r} \\frac{1}{|\\mathcal{N}_i^r|} \\, W_r \\, h_j^{(l)} \\;+\\; W_0 \\, h_i^{(l)}.\n$$\n\n我们现在使用所提供的数据将此方程应用于节点 $d_1$。类型化邻域为\n$\\mathcal{N}_{d_1}^{\\text{binds}} = \\{ p_1 \\}$，\n$\\mathcal{N}_{d_1}^{\\text{inhibits}} = \\{ p_2 \\}$，\n$\\mathcal{N}_{d_1}^{\\text{interacts}} = \\{ d_2 \\}$，\n并且每个邻域的基数都为 $1$，因此每个归一化因子都是 $1$。\n\n计算每种类型的贡献：\n1. binds 关系的贡献：\n$$\nW_{\\text{binds}} \\, h_{p_1}^{(0)} \\;=\\; \\begin{pmatrix} 2 & 1 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n\\;=\\; \\begin{pmatrix} 2 \\cdot 0 + 1 \\cdot 1 \\\\ 0 \\cdot 0 + 1 \\cdot 1 \\end{pmatrix}\n\\;=\\; \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}.\n$$\n\n2. inhibits 关系的贡献：\n$$\nW_{\\text{inhibits}} \\, h_{p_2}^{(0)} \\;=\\; \\begin{pmatrix} -1 & 0 \\\\ 0 & 2 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ -1 \\end{pmatrix}\n\\;=\\; \\begin{pmatrix} -1 \\cdot 3 + 0 \\cdot (-1) \\\\ 0 \\cdot 3 + 2 \\cdot (-1) \\end{pmatrix}\n\\;=\\; \\begin{pmatrix} -3 \\\\ -2 \\end{pmatrix}.\n$$\n\n3. interacts 关系的贡献：\n$$\nW_{\\text{interacts}} \\, h_{d_2}^{(0)} \\;=\\; \\begin{pmatrix} 1 & -1 \\\\ 3 & 0 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}\n\\;=\\; \\begin{pmatrix} 1 \\cdot 2 + (-1) \\cdot 0 \\\\ 3 \\cdot 2 + 0 \\cdot 0 \\end{pmatrix}\n\\;=\\; \\begin{pmatrix} 2 \\\\ 6 \\end{pmatrix}.\n$$\n\n4. 自环贡献：\n$$\nW_0 \\, h_{d_1}^{(0)} \\;=\\; \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}\n\\;=\\; \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}.\n$$\n\n聚合所有贡献以获得 $h_{d_1}^{(1)}$：\n$$\nh_{d_1}^{(1)} \\;=\\; \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n\\;+\\; \\begin{pmatrix} -3 \\\\ -2 \\end{pmatrix}\n\\;+\\; \\begin{pmatrix} 2 \\\\ 6 \\end{pmatrix}\n\\;+\\; \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}\n\\;=\\; \\begin{pmatrix} 1 - 3 + 2 + 1 \\\\ 1 - 2 + 6 + 2 \\end{pmatrix}\n\\;=\\; \\begin{pmatrix} 1 \\\\ 7 \\end{pmatrix}.\n$$\n\n所要求的量是 $h_{d_1}^{(1)}$ 的第一个坐标，其值为 $1$。",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "在药物发现中，GNN的一个主要应用是链接预测，即识别潜在的药物-靶点相互作用。这类任务面临一个严峻的挑战：类别极度不平衡，已知的相互作用远少于非相互作用。本练习将引导您推导并应用如焦点损失（focal loss）等高级损失函数，这些函数专为解决不平衡问题而设计，对于有效训练模型至关重要。",
            "id": "4570205",
            "problem": "考虑一个二分药物-靶点相互作用 (DTI) 网络，其中一个图神经网络 (GNN) 为每个候选的药物-靶点对（边）生成一个实值 logit $z \\in \\mathbb{R}$，该 logit 通过 logistic sigmoid 函数 $p = \\sigma(z) = \\frac{1}{1 + \\exp(-z)}$ 映射到一个伯努利参数 $p$。假设对于一个小批量的边，在给定 $p$ 的情况下，观测到的二元边标签是条件独立的。该问题的基础是二元结果的伯努利似然和最大似然估计 (MLE) 原理，以及初等微积分中的链式法则。\n\n从伯努利似然 $p(y \\mid p) = p^{y} (1-p)^{1-y}$ 出发，并应用最大似然估计原理和负对数似然构造，完成以下任务：\n\n1. 推导适用于 DTI 链接预测的逐边二元交叉熵 (BCE) 损失 $L_{\\mathrm{BCE}}(y, p)$，并求出其关于 logit $z$ 的梯度 $\\frac{\\partial L_{\\mathrm{BCE}}}{\\partial z}$。\n\n2. 推导一个类别加权的 BCE 损失 $L_{\\mathrm{wBCE}}(y, p)$，该损失使用不同的正类和负类权重 $w_{1} > 0$ 和 $w_{0} > 0$ 来处理类别不平衡问题。求出其梯度 $\\frac{\\partial L_{\\mathrm{wBCE}}}{\\partial z}$。\n\n3. 从伯努利似然出发，推导一个带有聚焦参数 $\\gamma \\ge 0$ 和平衡参数 $\\alpha \\in (0,1)$ 的二元 focal loss $L_{\\mathrm{focal}}(y, p)$，该损失适用于 DTI 类别不平衡问题，并解释调制因子在衰减简单样本中的作用。求出其梯度 $\\frac{\\partial L_{\\mathrm{focal}}}{\\partial z}$。\n\n4. 对于一个包含2条边的小批量，其标签和 logit 分别为 $(y_{p}, z_{p}) = (1, 1.2)$ 和 $(y_{n}, z_{n}) = (0, -0.7)$，使用 focal loss，其中 $\\gamma = 2$ 且 $\\alpha = 0.25$。计算比率\n$$\nR \\;=\\; \\frac{\\left| \\frac{\\partial L_{\\mathrm{focal}}}{\\partial z_{p}} \\right|}{\\left| \\frac{\\partial L_{\\mathrm{focal}}}{\\partial z_{n}} \\right|}.\n$$\n将 $R$ 的最终数值四舍五入到五位有效数字。将最终答案表示为一个无量纲标量。",
            "solution": "问题陈述已经过验证，被认为是可靠、良构的，并且在统计机器学习原理上具有科学依据。我们可以继续进行推导和计算。\n\n连接 logit $z$ 和伯努利概率 $p$ 的核心关系是 logistic sigmoid 函数，$p = \\sigma(z) = \\frac{1}{1 + \\exp(-z)}$。该函数的一个关键性质（将被反复使用）是它关于 $z$ 的导数：\n$$\n\\frac{dp}{dz} = \\frac{d}{dz} (1 + \\exp(-z))^{-1} = -1 \\cdot (1 + \\exp(-z))^{-2} \\cdot (-\\exp(-z)) = \\frac{\\exp(-z)}{(1 + \\exp(-z))^{2}}\n$$\n这可以很简洁地用 $p$ 本身来表示：\n$$\n\\frac{dp}{dz} = \\frac{1}{1 + \\exp(-z)} \\cdot \\frac{\\exp(-z)}{1 + \\exp(-z)} = p \\cdot (1-p)\n$$\n所有后续的梯度计算都将依赖于链式法则，$\\frac{\\partial L}{\\partial z} = \\frac{\\partial L}{\\partial p} \\frac{dp}{dz}$。\n\n**1. 二元交叉熵 (BCE) 损失**\n\n最大似然估计 (MLE) 原理要求我们最大化观测数据的似然。对于单个二元观测 $y \\in \\{0, 1\\}$，伯努利似然为 $P(y \\mid p) = p^y(1-p)^{1-y}$。在计算上，最大化对数似然更为方便：\n$$\n\\ell(p \\mid y) = \\ln(P(y \\mid p)) = y \\ln(p) + (1-y)\\ln(1-p)\n$$\n在机器学习中，优化通常被构建为最小化一个损失函数。标准的选择是负对数似然 (NLL)。因此，逐边二元交叉熵损失被定义为伯努利分布的 NLL。\n$$\nL_{\\mathrm{BCE}}(y, p) = -\\ell(p \\mid y) = -[y \\ln(p) + (1-y)\\ln(1-p)]\n$$\n为了求出关于 logit $z$ 的梯度，我们应用链式法则：\n$$\n\\frac{\\partial L_{\\mathrm{BCE}}}{\\partial z} = \\frac{\\partial L_{\\mathrm{BCE}}}{\\partial p} \\frac{dp}{dz}\n$$\n首先，我们计算关于 $p$ 的偏导数：\n$$\n\\frac{\\partial L_{\\mathrm{BCE}}}{\\partial p} = - \\left[ y \\frac{1}{p} + (1-y) \\frac{-1}{1-p} \\right] = - \\left[ \\frac{y}{p} - \\frac{1-y}{1-p} \\right] = \\frac{1-y}{1-p} - \\frac{y}{p} = \\frac{p(1-y) - y(1-p)}{p(1-p)} = \\frac{p-y}{p(1-p)}\n$$\n现在，乘以 sigmoid 函数的导数：\n$$\n\\frac{\\partial L_{\\mathrm{BCE}}}{\\partial z} = \\left( \\frac{p-y}{p(1-p)} \\right) \\cdot (p(1-p)) = p - y\n$$\n代入 $p=\\sigma(z)$，梯度为 $\\frac{\\partial L_{\\mathrm{BCE}}}{\\partial z} = \\sigma(z) - y$。\n\n**2. 类别加权的二元交叉熵 (wBCE) 损失**\n\n为了解决类别不平衡问题，可以通过为正类 ($y=1$) 和负类 ($y=0$) 引入权重来修改标准的 BCE 损失，分别表示为 $w_1 > 0$ 和 $w_0 > 0$。损失函数变为：\n$$\nL_{\\mathrm{wBCE}}(y, p) = -[w_1 y \\ln(p) + w_0 (1-y)\\ln(1-p)]\n$$\n我们再次使用链式法则来求出关于 $z$ 的梯度。关于 $p$ 的偏导数为：\n$$\n\\frac{\\partial L_{\\mathrm{wBCE}}}{\\partial p} = - \\left[ w_1 y \\frac{1}{p} + w_0 (1-y) \\frac{-1}{1-p} \\right] = \\frac{w_0(1-y)}{1-p} - \\frac{w_1 y}{p}\n$$\n乘以 $\\frac{dp}{dz} = p(1-p)$：\n$$\n\\frac{\\partial L_{\\mathrm{wBCE}}}{\\partial z} = \\left( \\frac{w_0(1-y)}{1-p} - \\frac{w_1 y}{p} \\right) \\cdot p(1-p) = w_0(1-y)p - w_1 y(1-p)\n$$\n该梯度可以表示为 $z$ 的函数形式 $\\frac{\\partial L_{\\mathrm{wBCE}}}{\\partial z} = w_0(1-y)\\sigma(z) - w_1 y(1-\\sigma(z))$。\n\n**3. 二元 Focal Loss**\n\n二元 focal loss 是在加权 BCE 基础上的一个改进，它通过根据模型的置信度动态缩放交叉熵损失来解决类别不平衡问题。它引入了一个调制因子，该因子降低了“简单”样本（即以高置信度分类的样本）的损失贡献，从而使训练专注于“困难”的错分样本。\n\n令 $p_t$ 为模型对真实类别的估计概率的简写：\n$$\np_t = \\begin{cases} p  \\text{if } y=1 \\\\ 1-p  \\text{if } y=0 \\end{cases}\n$$\n标准的交叉熵损失是 $-\\ln(p_t)$。focal loss 引入了两个部分：一个静态的平衡参数 $\\alpha \\in (0,1)$（类似于 $w_0, w_1$）和一个动态的调制因子 $(1-p_t)^{\\gamma}$，其中聚焦参数 $\\gamma \\ge 0$。该损失定义为：\n$$\nL_{\\mathrm{focal}}(y, p) = -y \\alpha (1-p)^\\gamma \\ln(p) - (1-y)(1-\\alpha) p^\\gamma \\ln(1-p)\n$$\n对于正样本 ($y=1$)，当 $p \\to 1$（一个简单的正样本）时，项 $(1-p)^\\gamma$ 趋近于 $0$，从而减小了损失。类似地，对于负样本 ($y=0$)，当 $p \\to 0$（一个简单的负样本）时，项 $p^\\gamma$ 趋近于 $0$，也减小了损失。当 $\\gamma=0$ 时，focal loss 退化为 $\\alpha$ 平衡的交叉熵损失。\n\n为了求出梯度 $\\frac{\\partial L_{\\mathrm{focal}}}{\\partial z}$，我们再次使用链式法则，通过对 $p$ 求导然后乘以 $p(1-p)$。我们分别推导 $y=1$ 和 $y=0$ 情况下的梯度。\n\n对于 $y=1$：$L(p) = -\\alpha(1-p)^\\gamma \\ln(p)$。\n$$\n\\frac{\\partial L}{\\partial p} = -\\alpha \\left[ \\frac{d}{dp}((1-p)^\\gamma) \\ln(p) + (1-p)^\\gamma \\frac{d}{dp}(\\ln p) \\right] = -\\alpha \\left[ -\\gamma(1-p)^{\\gamma-1} \\ln(p) + \\frac{(1-p)^\\gamma}{p} \\right]\n$$\n乘以 $\\frac{dp}{dz} = p(1-p)$：\n$$\n\\frac{\\partial L}{\\partial z} = -\\alpha \\left[ -\\gamma(1-p)^{\\gamma-1} \\ln(p) + \\frac{(1-p)^\\gamma}{p} \\right] p(1-p) = -\\alpha \\left[ -\\gamma p(1-p)^\\gamma \\ln(p) + (1-p)^{\\gamma+1} \\right]\n$$\n$$\n\\implies \\frac{\\partial L}{\\partial z} = \\alpha(1-p)^\\gamma (\\gamma p \\ln(p) - (1-p)) = \\alpha(1-p)^\\gamma (\\gamma p \\ln(p) + p-1)\n$$\n对于 $y=0$：$L(p) = -(1-\\alpha)p^\\gamma \\ln(1-p)$。\n$$\n\\frac{\\partial L}{\\partial p} = -(1-\\alpha) \\left[ \\gamma p^{\\gamma-1} \\ln(1-p) + p^\\gamma \\frac{-1}{1-p} \\right]\n$$\n乘以 $\\frac{dp}{dz} = p(1-p)$：\n$$\n\\frac{\\partial L}{\\partial z} = -(1-\\alpha) \\left[ \\gamma p^{\\gamma-1} \\ln(1-p) - \\frac{p^\\gamma}{1-p} \\right] p(1-p) = -(1-\\alpha) \\left[ \\gamma p^\\gamma(1-p) \\ln(1-p) - p^{\\gamma+1} \\right]\n$$\n$$\n\\implies \\frac{\\partial L}{\\partial z} = (1-\\alpha)p^\\gamma (p - \\gamma(1-p)\\ln(1-p))\n$$\n结合这些结果，得到完整的梯度表达式：\n$$\n\\frac{\\partial L_{\\mathrm{focal}}}{\\partial z} = y\\alpha(1-p)^\\gamma(p-1+\\gamma p\\ln p) + (1-y)(1-\\alpha)p^\\gamma(p-\\gamma(1-p)\\ln(1-p))\n$$\n\n**4. 数值计算**\n\n给定一个正样本 $(y_p, z_p) = (1, 1.2)$ 和一个负样本 $(y_n, z_n) = (0, -0.7)$，参数为 $\\gamma=2$ 和 $\\alpha=0.25$。我们需要计算 $R = \\frac{\\left| \\frac{\\partial L_{\\mathrm{focal}}}{\\partial z_{p}} \\right|}{\\left| \\frac{\\partial L_{\\mathrm{focal}}}{\\partial z_{n}} \\right|}$。\n\n首先，我们计算概率：\n$p_p = \\sigma(z_p) = \\sigma(1.2) = \\frac{1}{1 + \\exp(-1.2)} \\approx 0.76852479$\n$p_n = \\sigma(z_n) = \\sigma(-0.7) = \\frac{1}{1 + \\exp(0.7)} \\approx 0.33181222$\n\n接下来，我们计算正样本 ($y_p=1$) 的梯度：\n$$\n\\frac{\\partial L_{\\mathrm{focal}}}{\\partial z_{p}} = \\alpha(1-p_p)^\\gamma(\\gamma p_p \\ln p_p + p_p - 1)\n$$\n代入数值：\n$1-p_p \\approx 1 - 0.76852479 = 0.23147521$\n$\\ln(p_p) \\approx \\ln(0.76852479) = -0.26328242$\n$$\n\\frac{\\partial L_{\\mathrm{focal}}}{\\partial z_{p}} \\approx 0.25 \\cdot (0.23147521)^2 \\cdot (2 \\cdot 0.76852479 \\cdot (-0.26328242) + 0.76852479 - 1)\n$$\n$$\n\\approx 0.25 \\cdot (0.05358087) \\cdot (-0.40467566 - 0.23147521) = 0.01339522 \\cdot (-0.63615087) \\approx -0.00852033\n$$\n所以，$\\left| \\frac{\\partial L_{\\mathrm{focal}}}{\\partial z_{p}} \\right| \\approx 0.00852033$。\n\n接下来，我们计算负样本 ($y_n=0$) 的梯度：\n$$\n\\frac{\\partial L_{\\mathrm{focal}}}{\\partial z_{n}} = (1-\\alpha)p_n^\\gamma(p_n - \\gamma(1-p_n)\\ln(1-p_n))\n$$\n代入数值：\n$1-\\alpha = 0.75$\n$1-p_n \\approx 1 - 0.33181222 = 0.66818778$\n$\\ln(1-p_n) \\approx \\ln(0.66818778) = -0.40318043$\n$$\n\\frac{\\partial L_{\\mathrm{focal}}}{\\partial z_{n}} \\approx 0.75 \\cdot (0.33181222)^2 \\cdot (0.33181222 - 2 \\cdot (0.66818778) \\cdot (-0.40318043))\n$$\n$$\n\\approx 0.75 \\cdot (0.11009972) \\cdot (0.33181222 + 0.53880447) = 0.08257479 \\cdot (0.87061669) \\approx 0.0718797\n$$\n所以，$\\left| \\frac{\\partial L_{\\mathrm{focal}}}{\\partial z_{n}} \\right| \\approx 0.0718797$。\n\n最后，我们计算比率 $R$：\n$$\nR = \\frac{\\left| \\frac{\\partial L_{\\mathrm{focal}}}{\\partial z_{p}} \\right|}{\\left| \\frac{\\partial L_{\\mathrm{focal}}}{\\partial z_{n}} \\right|} \\approx \\frac{0.00852033}{0.0718797} \\approx 0.1185361\n$$\n四舍五入到五位有效数字，我们得到 $R \\approx 0.11854$。$R$ 的小值证实了分类良好的正样本 ($p_p \\approx 0.77$) 的梯度幅度明显小于更模糊的负样本 ($p_n \\approx 0.33$) 的梯度幅度，这展示了 focal loss 的效果。",
            "answer": "$$\n\\boxed{0.11854}\n$$"
        }
    ]
}