{
    "hands_on_practices": [
        {
            "introduction": "我们从可解释机器学习的基石——SHAP公理——开始。这个练习将通过一个简单的线性模型，要求您直接从公理出发推导特征贡献度，从而揭开SHAP的神秘面纱 。这项实践将为您构建坚实的理论基础，并展示为何在线性模型中，SHAP值会呈现出一种直观简洁的形式。",
            "id": "4575319",
            "problem": "在一个心血管事件的临床风险分层情景中，考虑一个线性风险模型，该模型聚合了三个标准化的生物标志物测量值，记为特征向量 $x = (x_1, x_2, x_3)$。该模型输出一个标量风险评分，由 $f(x) = x_1 + 2 x_2 + 3 x_3$ 给出，其中学习到的权重 $1$、$2$ 和 $3$ 反映了每个生物标志物的相对重要性。在 Shapley 可加性解释 (SHAP) 框架下，可解释性被构建为一个在特征集合 $F = \\{1, 2, 3\\}$ 上的合作博弈，其中基线代表信息缺失的状态。假设基线是零向量，因此 $f(0,0,0) = 0$，并且缺失的特征被填补为其基线值。\n\n对于患者实例 $x = (1, 1, 1)$，使用合作博弈的公式计算三个生物标志物的 Shapley 值（特征归因），其中对于一个联盟 $S \\subseteq F$，其价值函数是当联盟 $S$ 中的特征存在，而 $F \\setminus S$ 中的特征被设置为基线值时的模型输出。请从 Shapley 值的基本公理（效率性、对称性、虚拟性和线性性）推导您的结果，而不是借助快捷公式。然后通过证明三个 Shapley 值的总和等于差值 $f(x) - f(0,0,0)$ 来验证效率性属性。\n\n将三个 Shapley 值以单行向量的形式报告。无需四舍五入，且不适用任何物理单位。",
            "solution": "对用户提供的问题陈述进行验证。\n\n### 步骤 1：提取已知信息\n-   **模型：** 一个用于心血管事件的线性风险模型。\n-   **特征向量：** $x = (x_1, x_2, x_3)$ 代表三个标准化的生物标志物测量值。\n-   **风险评分函数：** $f(x) = x_1 + 2 x_2 + 3 x_3$。权重为 $w_1 = 1$, $w_2 = 2$, $w_3 = 3$。\n-   **可解释性框架：** Shapley 可加性解释 (SHAP)。\n-   **合作博弈：** 该博弈定义在特征集合 $F = \\{1, 2, 3\\}$ 上。\n-   **基线：** 零向量 $(0,0,0)$。模型在基线处的输出为 $f(0,0,0) = 0$。\n-   **填补：** 缺失的特征（即不在联盟中的特征）被填补为其基线值 $0$。\n-   **用于解释的实例：** 特定的患者实例是 $x = (1, 1, 1)$。\n-   **价值函数：** 对于特征联盟 $S \\subseteq F$，价值函数 $v(S)$ 是当 $S$ 中的特征存在（即取自实例 $x$ 的值）且 $F \\setminus S$ 中的特征缺失（即取其基线值）时的模型输出。\n-   **任务 1：** 计算实例 $x = (1, 1, 1)$ 中三个生物标志物的 Shapley 值（$\\phi_1, \\phi_2, \\phi_3$）。\n-   **约束：** 推导必须基于 Shapley 值的基本公理（效率性、对称性、虚拟性和线性性），而不能使用快捷公式。\n-   **任务 2：** 通过证明 Shapley 值的总和等于 $f(x) - f(0,0,0)$ 来验证效率性属性。\n-   **报告格式：** 将三个 Shapley 值以单行向量的形式报告。\n\n### 步骤 2：使用提取的已知信息进行验证\n-   **科学依据：** 该问题基础扎实。SHAP 是可解释机器学习中一个标准的、严格定义的方法。线性模型是统计学和机器学习的基础。将其应用于临床风险分层是一个常见且恰当的用例。\n-   **适定性：** 该问题是适定的。模型、特征集、实例和基线都已明确定义。合作博弈的价值函数由这些定义明确地确定。对于给定的合作博弈，Shapley 值是唯一的。使用公理的约束指导了解题方法，但并未使问题变得不适定。\n-   **客观性：** 问题以精确、客观的数学语言陈述。所有术语都具有标准的、无歧义的定义。\n\n该问题没有任何科学不健全、不完整、矛盾或任何其他列出的缺陷。它是在应用数学和机器学习理论中一个有效且表述良好的问题。\n\n### 步骤 3：结论与行动\n问题有效。将提供完整的解答。\n\n### 解答推导\n问题的核心是为特定实例 $x=(1,1,1)$ 计算由模型 $f(x)$ 定义的合作博弈的 Shapley 值。Shapley 值 $(\\phi_1, \\phi_2, \\phi_3)$ 量化了每个特征相对于基线对模型预测的贡献。\n\n首先，我们为任何特征联盟 $S \\subseteq F = \\{1, 2, 3\\}$ 正式定义价值函数 $v(S)$。价值 $v(S)$ 是当 $S$ 中的特征取自实例 $x=(1, 1, 1)$ 的值，而不在 $S$ 中的特征取其基线值 $0$ 时模型的预测值。设 $x_S$ 为此输入向量。\n模型为 $f(z_1, z_2, z_3) = w_1 z_1 + w_2 z_2 + w_3 z_3$，权重为 $w_1=1, w_2=2, w_3=3$。\n那么价值函数为 $v(S) = f(x_S) = \\sum_{i=1}^3 w_i (x_S)_i$。\n$x_S$ 的分量为：如果 $i \\in S$，则 $(x_S)_i = x_i$；如果 $i \\notin S$，则 $(x_S)_i = 0$。\n给定实例 $x=(1,1,1)$，我们有对于所有 $i \\in F$，$x_i=1$。\n因此，价值函数简化为：\n$$v(S) = \\sum_{i \\in S} w_i x_i = \\sum_{i \\in S} w_i(1) = \\sum_{i \\in S} w_i$$\n这定义了我们必须分析的博弈。问题要求使用 Shapley 值的公理。这个问题的关键公理是线性性。\n\n**线性性公理** (Linearity Axiom) 指出，如果一个博弈的价值函数 $v$ 可以表示为其他价值函数 $u_k$ 的线性组合，$v = \\sum_k c_k u_k$，那么博弈 $v$ 中任何参与者 $i$ 的 Shapley 值是其在博弈 $u_k$ 中 Shapley 值的相同线性组合：$\\phi_i(v) = \\sum_k c_k \\phi_i(u_k)$。\n\n我们可以将我们的博弈 $v$ 分解为更简单的博弈之和。让我们根据每个权重的贡献定义三个博弈 $v_1, v_2, v_3$：\n-   如果 $1 \\in S$，则 $v_1(S) = w_1$，否则为 $0$。\n-   如果 $2 \\in S$，则 $v_2(S) = w_2$，否则为 $0$。\n-   如果 $3 \\in S$，则 $v_3(S) = w_3$，否则为 $0$。\n\n很明显，对于任何联盟 $S$，$v(S) = \\sum_{i \\in S} w_i = v_1(S) + v_2(S) + v_3(S)$。\n根据线性性公理，任何特征 $j \\in F$ 的 Shapley 值是其在每个子博弈中 Shapley 值的总和：\n$$\\phi_j(v) = \\phi_j(v_1) + \\phi_j(v_2) + \\phi_j(v_3)$$\n\n现在，我们使用其他公理来计算每个简单博弈 $v_k$ 的 Shapley 值。让我们分析博弈 $v_1$。\n参与者是 $\\{1, 2, 3\\}$。价值函数是：如果 $1 \\in S$，则 $v_1(S) = w_1$，否则为 $0$。\n考虑参与者 $2$。它对任何联盟 $T \\subseteq F \\setminus \\{2\\}$ 的边际贡献是 $v_1(T \\cup \\{2\\}) - v_1(T)$。由于价值 $v_1(S)$ 仅取决于参与者 $1$ 是否存在，因此将参与者 $2$ 添加到任何联盟 $T$ 中都不会改变价值。因此，对于所有不包含 $2$ 的 $T$，$v_1(T \\cup \\{2\\}) = v_1(T)$。参与者 $2$ 是一个**虚拟参与者** (dummy player)。\n**虚拟性公理** (Dummy Axiom) 指出，如果一个参与者对任何联盟都没有贡献，其 Shapley 值为 $0$。因此，$\\phi_2(v_1) = 0$。通过相同的推理，参与者 $3$ 在博弈 $v_1$ 中也是一个虚拟参与者，所以 $\\phi_3(v_1) = 0$。\n\n现在我们求 $\\phi_1(v_1)$。我们使用**效率性公理** (Efficiency Axiom)，该公理指出，所有参与者的 Shapley 值之和必须等于大联盟 $F$ 产生的总价值减去空联盟 $\\emptyset$ 的价值。\n对于博弈 $v_1$：$\\phi_1(v_1) + \\phi_2(v_1) + \\phi_3(v_1) = v_1(F) - v_1(\\emptyset)$。\n代入我们找到的值：$\\phi_1(v_1) + 0 + 0 = v_1(\\{1,2,3\\}) - v_1(\\emptyset)$。\n根据 $v_1$ 的定义，$v_1(\\{1,2,3\\}) = w_1$（因为 $1 \\in \\{1,2,3\\}$）且 $v_1(\\emptyset) = 0$（因为 $1 \\notin \\emptyset$）。\n所以，$\\phi_1(v_1) = w_1 - 0 = w_1$。\n\n我们可以对博弈 $v_2$ 和 $v_3$ 应用完全相同的逻辑：\n-   对于博弈 $v_2$：参与者 $1$ 和 $3$ 是虚拟参与者，所以 $\\phi_1(v_2)=0$ 和 $\\phi_3(v_2)=0$。根据效率性公理，$\\phi_2(v_2) = v_2(F) - v_2(\\emptyset) = w_2 - 0 = w_2$。\n-   对于博弈 $v_3$：参与者 $1$ 和 $2$ 是虚拟参与者，所以 $\\phi_1(v_3)=0$ 和 $\\phi_2(v_3)=0$。根据效率性公理，$\\phi_3(v_3) = v_3(F) - v_3(\\emptyset) = w_3 - 0 = w_3$。\n\n现在我们使用线性性属性来组合这些结果，以得到原始博弈 $v$ 的结果：\n-   $\\phi_1(v) = \\phi_1(v_1) + \\phi_1(v_2) + \\phi_1(v_3) = w_1 + 0 + 0 = w_1$。\n-   $\\phi_2(v) = \\phi_2(v_1) + \\phi_2(v_2) + \\phi_2(v_3) = 0 + w_2 + 0 = w_2$。\n-   $\\phi_3(v) = \\phi_3(v_1) + \\phi_3(v_2) + \\phi_3(v_3) = 0 + 0 + w_3 = w_3$。\n\n在这个所有特征值为 $1$ 且基线为 $0$ 的特定情景中，Shapley 值就是线性模型的权重。\n代入给定的权重 $w_1=1, w_2=2, w_3=3$：\n-   $\\phi_1 = 1$\n-   $\\phi_2 = 2$\n-   $\\phi_3 = 3$\n\n三个生物标志物的 Shapley 值为 $(1, 2, 3)$。\n\n最后，我们按照要求验证整个博弈的效率性属性。该属性指出，Shapley 值的总和必须等于模型对实例 $x$ 的预测值与对基线的预测值之间的差值。\n计算出的 Shapley 值的总和是：\n$$\\sum_{i=1}^3 \\phi_i = 1 + 2 + 3 = 6$$\n对实例 $x=(1,1,1)$ 的预测值是：\n$$f(x) = f(1,1,1) = 1(1) + 2(1) + 3(1) = 6$$\n对基线 $(0,0,0)$ 的预测值是：\n$$f(0,0,0) = 1(0) + 2(0) + 3(0) = 0$$\n差值为 $f(x) - f(0,0,0) = 6 - 0 = 6$。\n由于 $\\sum \\phi_i = 6$ 且 $f(x) - f(0,0,0) = 6$，效率性属性得到验证。\n\n最终答案是 Shapley 值的向量。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1  & 2 & 3\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "现实世界中的模型通常是非线性的。这项实践将我们的目光转向决策树——一种在临床环境中广泛应用的非线性模型，并引入了专门为此优化的TreeSHAP算法 。您将学习如何通过追踪树的路径，并利用背景数据来处理“缺失”的特征以计算SHAP值，从而一窥SHAP处理更复杂函数的方法。",
            "id": "4575307",
            "problem": "考虑一个用于临床生物标志物场景中风险评分的二元决策树，该决策树具有两个特征 $x_{1}$ 和 $x_{2}$。目标是使用专门针对决策树的 SHapley 加性解释 (SHapley Additive exPlanations, SHAP) 方法（即 TreeSHAP）来计算特征归因。为完整起见，请注意局部可解释模型无关解释 (Local Interpretable Model-agnostic Explanations, LIME) 是另一种用于局部可解释性的方法；然而，在本任务中，请使用基于合作博弈论沙普利值 (Shapley value) 的 TreeSHAP 框架。\n\n该树的深度为 $2$，具有以下结构和参数，这些是从一个训练背景数据集中学习得到的：\n\n- 根节点在阈值 $t_{1} = 3$ 处对 $x_{1}$ 进行分裂，如果 $x_{1} < 3$ 则进入左子节点，否则进入右子节点。\n- 根节点的左子节点在阈值 $t_{2} = 5$ 处对 $x_{2}$ 进行分裂，如果 $x_{2} < 5$ 则进入左叶节点，否则进入右叶节点，叶节点输出分别为 $v_{\\text{LL}} = 2$ 和 $v_{\\text{LR}} = 6$。\n- 根节点的右子节点在阈值 $t_{3} = 4$ 处对 $x_{2}$ 进行分裂，如果 $x_{2} < 4$ 则进入左叶节点，否则进入右叶节点，叶节点输出分别为 $v_{\\text{RL}} = 9$ 和 $v_{\\text{RR}} = -1$。\n\n背景数据提供了节点的覆盖计数（到达每个节点的训练样本数），当 TreeSHAP 中某个特征被视为“缺失”时，这些计数决定了分支概率：\n- 在根节点：总覆盖数 $N = 100$，其中左分支 $N_{\\text{L}} = 60$，右分支 $N_{\\text{R}} = 40$。\n- 在左子树中：$N_{\\text{LL}} = 45$，$N_{\\text{LR}} = 15$。\n- 在右子树中：$N_{\\text{RL}} = 10$，$N_{\\text{RR}} = 30$。\n\n对于实例 $\\mathbf{x} = (x_{1}, x_{2}) = (2, 6)$，执行以下操作：\n1. 使用专门针对决策树的沙普利值定义，分别计算特征 $x_{1}$ 和 $x_{2}$ 的 TreeSHAP 值 $\\phi_{x_{1}}$ 和 $\\phi_{x_{2}}$。当一个特征“缺失”时，通过在该缺失特征的分裂点上按所提供的训练覆盖计数成比例地分配质量来计算期望预测。\n2. 通过证明 $\\phi_{0} + \\phi_{x_{1}} + \\phi_{x_{2}} = f(\\mathbf{x})$ 来验证加性属性，其中 $\\phi_{0}$ 是模型在背景分布上的期望输出（偏置项），$f(\\mathbf{x})$ 是树在 $\\mathbf{x}$ 处的预测。\n\n将 $\\phi_{x_{2}}$ 作为你的最终答案。最终答案以简化有理数形式表示。无需四舍五入，也不包含任何单位。",
            "solution": "用户想要为给定实例和指定的决策树计算 TreeSHAP 值。解决方案要求在基于树的模型背景下应用沙普利值的定义，其中模型输出在特征子集上的条件期望是通过对背景数据样本的预测进行平均来计算的。\n\n首先，我们验证问题陈述。\n\n### 步骤 1：提取已知条件\n- **特征**：$x_{1}$，$x_{2}$。\n- **决策树结构**：\n  - 根节点：在 $t_{1} = 3$ 处对 $x_{1}$ 进行分裂。\n  - 左子节点 (若 $x_{1} < 3$)：在 $t_{2} = 5$ 处对 $x_{2}$ 进行分裂。\n    - 左左叶节点 (若 $x_{2} < 5$)：输出 $v_{\\text{LL}} = 2$。\n    - 左右叶节点 (若 $x_{2} \\ge 5$)：输出 $v_{\\text{LR}} = 6$。\n  - 右子节点 (若 $x_{1} \\ge 3$)：在 $t_{3} = 4$ 处对 $x_{2}$ 进行分裂。\n    - 右左叶节点 (若 $x_{2} < 4$)：输出 $v_{\\text{RL}} = 9$。\n    - 右右叶节点 (若 $x_{2} \\ge 4$)：输出 $v_{\\text{RR}} = -1$。\n- **覆盖计数**：\n  - 根节点：$N = 100$。\n  - 根节点分支：$N_{\\text{L}} = 60$ ($x_{1} < 3$)，$N_{\\text{R}} = 40$ ($x_{1} \\ge 3$)。\n  - 左子树分支：$N_{\\text{LL}} = 45$，$N_{\\text{LR}} = 15$。\n  - 右子树分支：$N_{\\text{RL}} = 10$，$N_{\\text{RR}} = 30$。\n- **待解释的实例**：$\\mathbf{x} = (x_{1}, x_{2}) = (2, 6)$。\n- **任务**：\n  1. 计算 TreeSHAP 值 $\\phi_{x_{1}}$ 和 $\\phi_{x_{2}}$。\n  2. 验证加性属性：$\\phi_{0} + \\phi_{x_{1}} + \\phi_{x_{2}} = f(\\mathbf{x})$。\n- **要求的最终答案**：$\\phi_{x_{2}}$ 以简化有理数形式表示。\n\n### 步骤 2：使用提取的已知条件进行验证\n该问题在科学上基于成熟的沙普利值理论及其在机器学习可解释性 (TreeSHAP) 中的应用。所提供的数据是完整且内部一致的：每个节点的覆盖计数总和正确 ($N_L+N_R = 60+40=100=N$; $N_{LL}+N_{LR} = 45+15=60=N_L$; $N_{RL}+N_{RR} = 10+30=40=N_R$)。该问题定义明确、客观，并且需要应用一个标准（尽管不简单）的算法。不存在歧义或矛盾。\n\n### 步骤 3：结论和行动\n问题有效。我们开始进行解答。\n\n特征 $i$ 的沙普利值是其在所有可能的特征排序中对预测的平均边际贡献。对于特征集合 $F = \\{x_1, x_2\\}$，特征 $i$ 的沙普利值由以下公式给出：\n$$ \\phi_i = \\sum_{S \\subseteq F \\setminus \\{i\\}} \\frac{|S|! (|F| - |S| - 1)!}{|F|!} \\left( E[f(X) | X_S \\cup \\{x_i\\}] - E[f(X) | X_S] \\right) $$\n其中 $E[f(X) | X_S]$ 是在子集 $S$ 中特征值已知的条件下的期望预测。对于 TreeSHAP，这个期望是在背景数据集上计算的。\n\n对于两个特征 $F=\\{x_1, x_2\\}$，可能的排序是 $(x_1, x_2)$ 和 $(x_2, x_1)$，每个排序的权重为 $\\frac{1}{2}$。$\\phi_{x_1}$ 和 $\\phi_{x_2}$ 的公式简化为：\n$$ \\phi_{x_1} = \\frac{1}{2} \\left( \\left(E[f(X)|x_1] - E[f(X)]\\right) + \\left(E[f(X)|x_1, x_2] - E[f(X)|x_2]\\right) \\right) $$\n$$ \\phi_{x_2} = \\frac{1}{2} \\left( \\left(E[f(X)|x_2] - E[f(X)]\\right) + \\left(E[f(X)|x_1, x_2] - E[f(X)|x_1]\\right) \\right) $$\n此处，条件期望是针对特定实例 $\\mathbf{x} = (2, 6)$ 进行评估的。\n\n**1. 计算模型预测值 $f(\\mathbf{x})$ 和基准值 $\\phi_0$。**\n\n需要解释的实例是 $\\mathbf{x} = (x_1, x_2) = (2, 6)$。\n- 在根节点，$x_1 = 2 < 3$，因此我们遍历到左子节点。\n- 在左子节点，$x_2 = 6 \\ge 5$，因此我们遍历到右叶节点。\n这条路径通向叶节点 LR。模型的预测值就是这个叶节点的值。\n$$ f(\\mathbf{x}) = v_{\\text{LR}} = 6 $$\n这等价于 $E[f(X) | x_1=2, x_2=6]$。\n\n基准值 $\\phi_0$ 是模型在背景数据分布上的无条件期望输出，即所有叶节点值的加权平均。\n$$ \\phi_0 = E[f(X)] = \\frac{N_{\\text{LL}}}{N} v_{\\text{LL}} + \\frac{N_{\\text{LR}}}{N} v_{\\text{LR}} + \\frac{N_{\\text{RL}}}{N} v_{\\text{RL}} + \\frac{N_{\\text{RR}}}{N} v_{\\text{RR}} $$\n$$ \\phi_0 = \\left(\\frac{45}{100}\\right) (2) + \\left(\\frac{15}{100}\\right) (6) + \\left(\\frac{10}{100}\\right) (9) + \\left(\\frac{30}{100}\\right) (-1) $$\n$$ \\phi_0 = \\frac{90}{100} + \\frac{90}{100} + \\frac{90}{100} - \\frac{30}{100} = \\frac{240}{100} = 2.4 $$\n\n**2. 计算条件期望。**\n\n**a) $E[f(X) | x_1=2]$**\n我们已知 $x_1=2$。在根节点，由于 $2 < 3$，我们被强制进入左分支。特征 $x_2$ 被认为是“缺失”的，因此其影响根据该分支内的数据分布进行平均。期望值是左子树中叶节点值的加权平均。\n$$ E[f(X) | x_1=2] = \\frac{N_{\\text{LL}}}{N_{\\text{L}}} v_{\\text{LL}} + \\frac{N_{\\text{LR}}}{N_{\\text{L}}} v_{\\text{LR}} = \\left(\\frac{45}{60}\\right) (2) + \\left(\\frac{15}{60}\\right) (6) $$\n$$ E[f(X) | x_1=2] = \\left(\\frac{3}{4}\\right) (2) + \\left(\\frac{1}{4}\\right) (6) = \\frac{6}{4} + \\frac{6}{4} = \\frac{12}{4} = 3 $$\n\n**b) $E[f(X) | x_2=6]$**\n我们已知 $x_2=6$。在根节点，分裂是基于 $x_1$ 的，而该特征是“缺失”的。我们考虑两个分支，并按其覆盖比例加权。\n- 路径 1 (左分支，概率 $\\frac{N_{\\text{L}}}{N} = \\frac{60}{100}$)：在 $t_2=5$ 处对 $x_2$ 进行分裂。由于 $x_2=6 \\ge 5$，该路径通向叶节点 LR，其值为 $v_{\\text{LR}} = 6$。\n- 路径 2 (右分支，概率 $\\frac{N_{\\text{R}}}{N} = \\frac{40}{100}$)：在 $t_3=4$ 处对 $x_2$ 进行分裂。由于 $x_2=6 \\ge 4$，该路径通向叶节点 RR，其值为 $v_{\\text{RR}} = -1$。\n期望值是这些结果的加权和。\n$$ E[f(X) | x_2=6] = \\left(\\frac{60}{100}\\right) (6) + \\left(\\frac{40}{100}\\right) (-1) $$\n$$ E[f(X) | x_2=6] = \\frac{360}{100} - \\frac{40}{100} = \\frac{320}{100} = 3.2 $$\n\n**3. 计算 SHAP 值 $\\phi_{x_1}$ 和 $\\phi_{x_2}$。**\n\n使用两个特征的简化公式：\n\n**a) 对于特征 $x_1$**：\n$$ \\phi_{x_1} = \\frac{1}{2} \\left[ \\left(E[f(X)|x_1=2] - E[f(X)]\\right) + \\left(E[f(X)|x_1=2, x_2=6] - E[f(X)|x_2=6]\\right) \\right] $$\n$$ \\phi_{x_1} = \\frac{1}{2} \\left[ (3 - 2.4) + (6 - 3.2) \\right] = \\frac{1}{2} [0.6 + 2.8] = \\frac{1}{2} [3.4] = 1.7 $$\n\n**b) 对于特征 $x_2$**：\n$$ \\phi_{x_2} = \\frac{1}{2} \\left[ \\left(E[f(X)|x_2=6] - E[f(X)]\\right) + \\left(E[f(X)|x_1=2, x_2=6] - E[f(X)|x_1=2]\\right) \\right] $$\n$$ \\phi_{x_2} = \\frac{1}{2} \\left[ (3.2 - 2.4) + (6 - 3) \\right] = \\frac{1}{2} [0.8 + 3] = \\frac{1}{2} [3.8] = 1.9 $$\n\n**4. 验证加性属性。**\n\nSHAP 值的加性属性表明，基准值与所有特征的 SHAP 值之和等于模型对该实例的预测值。\n$$ \\phi_0 + \\phi_{x_1} + \\phi_{x_2} = f(\\mathbf{x}) $$\n代入计算出的值：\n$$ 2.4 + 1.7 + 1.9 = 6.0 $$\n由于 $f(\\mathbf{x})=6$，该属性成立：$6.0 = 6$。这证实了我们计算的正确性。\n\n问题要求以简化有理数形式给出 $\\phi_{x_2}$ 的值。\n$$ \\phi_{x_2} = 1.9 = \\frac{19}{10} $$\n这个分数已经是最小整数比形式（最简形式）。",
            "answer": "$$\\boxed{\\frac{19}{10}}$$"
        },
        {
            "introduction": "SHAP值是在与某个基线（baseline）进行比较下，对模型预测进行的解释。这最后一个高级实践探讨了基线选择的深远影响 。通过分析同一个病人的特征贡献度在对比不同医院人群时如何变化，您将理解到模型解释并非绝对，而是与情境相关的——这是医疗数据分析中至关重要的一课。",
            "id": "4575283",
            "problem": "给定一个使用逻辑斯谛模型进行患者风险分层的二元分类情境，目标是量化当实验室特征的背景（医院）分布发生变化时，患者级别的SHapley Additive exPlanations (SHAP)值如何变化。一个标记为$B$的患者，其特征向量为 $x^{B} \\in \\mathbb{R}^{d}$，由一个参数为 $w \\in \\mathbb{R}^{d}$ 和 $b \\in \\mathbb{R}$ 的逻辑斯谛模型进行评估。该模型输出概率 $p(x) = \\sigma(w^{\\top} x + b)$，其中 $\\sigma(t) = \\frac{1}{1 + e^{-t}}$。对于SHapley Additive exPlanations (SHAP)，考虑在由 $g(x) = w^{\\top} x + b$ 定义的对数几率空间中计算的解释（即链接函数为logit）。每家医院特征的背景分布被建模为独立的、均值为 $\\mu \\in \\mathbb{R}^{d}$ 且方差有限的高斯随机变量。假设特征独立，并使用背景分布的期望值作为SHAP基线。\n\n任务：\n1) 从合作博弈论中Shapley值的定义以及线性、对称性和虚拟人公理出发，基于第一性原理推导在线性对数几率模型 $g(x) = w^{\\top} x + b$ 和独立性假设下的闭式SHAP值。明确证明特征 $j$ 的SHAP值为 $\\phi_{j}(x;\\mu) = w_{j}\\,(x_{j} - \\mu_{j})$，且基准值为 $\\phi_{0}(\\mu) = b + \\sum_{j=1}^{d} w_{j}\\,\\mu_{j}$。不要使用任何未从这些原理推导出的快捷公式。\n2) 使用第1部分的结果，推导当背景分布从均值为 $\\mu^{A}$ 的医院 $A$ 变为均值为 $\\mu^{B}$ 的医院 $B$ 时，固定患者 $x^{B}$ 的SHAP值的变化。证明对于每个特征 $j$，其变化为 $\\Delta \\phi_{j}(x^{B}; \\mu^{A} \\rightarrow \\mu^{B}) = \\phi_{j}(x^{B}; \\mu^{B}) - \\phi_{j}(x^{B}; \\mu^{A}) = w_{j}\\,(\\mu^{A}_{j} - \\mu^{B}_{j})$。\n3) 实现一个完整的、可运行的程序，为以下测试套件计算SHAP值的变化量 $\\Delta \\phi(x^{B}; \\mu^{A} \\rightarrow \\mu^{B}) \\in \\mathbb{R}^{d}$。您的实现不得依赖任何外部文件或输入。它必须直接使用第1部分和第2部分中的定义。\n\n模型和患者：\n- 维度 $d = 3$。\n- 权重 $w = (1.0,\\,-0.5,\\,0.0)$。\n- 截距 $b = -0.3$。\n- 患者 $B$ 的特征 $x^{B} = (1.2,\\,0.7,\\,-0.3)$。\n\n医院和场景（特征均值是在对数几率空间中计算SHAP所需的唯一量；方差是有限但未指定的）：\n- 场景 1（普遍漂移）：$\\mu^{A} = (0.0,\\,0.0,\\,0.0)$ 和 $\\mu^{B} = (0.8,\\,-0.4,\\,0.3)$。\n- 场景 2（无漂移）：$\\mu^{A} = (0.8,\\,-0.4,\\,0.3)$ 和 $\\mu^{B} = (0.8,\\,-0.4,\\,0.3)$。\n- 场景 3（仅不相关特征发生漂移）：$\\mu^{A} = (0.8,\\,-0.4,\\,0.3)$ 和 $\\mu^{B} = (0.8,\\,-0.4,\\,1.3)$。\n- 场景 4（单特征漂移）：$\\mu^{A} = (0.8,\\,-0.4,\\,0.3)$ 和 $\\mu^{B} = (1.8,\\,-0.4,\\,0.3)$。\n\n所需计算和输出：\n- 对于每个场景，计算向量 $\\Delta \\phi(x^{B}; \\mu^{A} \\rightarrow \\mu^{B}) = \\big(w_{1}(\\mu^{A}_{1} - \\mu^{B}_{1}),\\, w_{2}(\\mu^{A}_{2} - \\mu^{B}_{2}),\\, w_{3}(\\mu^{A}_{3} - \\mu^{B}_{3})\\big)$。\n- 您的程序应生成单行输出，其中包含一个方括号括起来的逗号分隔列表。每个场景的结果必须按特征1到3的顺序列印为Python风格的浮点数列表。例如，输出格式必须严格为：$[\\,[\\Delta\\phi^{(1)}_{1},\\Delta\\phi^{(1)}_{2},\\Delta\\phi^{(1)}_{3}],\\,[\\Delta\\phi^{(2)}_{1},\\ldots],\\,[\\Delta\\phi^{(3)}_{1},\\ldots],\\,[\\Delta\\phi^{(4)}_{1},\\ldots]\\,]$，在单行上，没有任何附加文本。\n\n注意：\n- 本任务不需要Light Interpretable Model-agnostic Explanations (LIME)；提及它仅为在生物信息学和医学数据分析的可解释机器学习中提供相关背景。\n- 没有物理单位，也不涉及角度。\n- 所有返回的数值必须是标准的浮点数（而不是分数）。",
            "solution": "该问题是有效的，因为它在科学上基于合作博弈论及其在机器学习可解释性（SHAP）中的应用，问题定义明确（well-posed）并提供了所有必要信息，且表述客观。我们开始解答。\n\n### 第1部分：线性模型SHAP值的推导\n\n目标是推导线性模型 $g(x) = w^{\\top}x + b = \\sum_{i=1}^d w_i x_i + b$ 中特征 $j$ 的SHAP值。推导将基于Shapley值的基本公理：线性（Linearity）、虚拟人（Dummy）、对称性（Symmetry）和有效性（Efficiency）。\n\n令 $N = \\{1, 2, \\dots, d\\}$ 为所有特征（参与者）的集合。特征联盟是子集 $S \\subseteq N$。在带有背景分布的SHAP情境中，联盟 $S$ 的值函数 $v(S)$ 表示模型的期望输出，其条件是观测到特定实例的特征值 $\\{x_i\\}_{i \\in S}$，而其余特征 $\\{X_j\\}_{j \\notin S}$ 是从其背景分布中抽取的随机变量。\n\n问题陈述特征是独立的，并且它们的背景分布是高斯分布，每个特征 $j$ 的均值为 $\\mu_j$。在独立性假设下，条件期望简化为对未观测特征的期望：\n$$v(S) = E[g(X) | X_S = x_S] = E\\left[\\sum_{i=1}^d w_i X_i + b \\right]$$\n其中，如果 $i \\in S$，则 $X_i = x_i$；如果 $i \\notin S$，则 $X_i$ 是一个均值为 $\\mu_i$ 的随机变量。\n\n利用期望的线性性质：\n$$v(S) = E\\left[\\sum_{i \\in S} w_i x_i + \\sum_{j \\notin S} w_j X_j + b\\right] = \\sum_{i \\in S} w_i x_i + \\sum_{j \\notin S} w_j E[X_j] + b$$\n由于 $E[X_j] = \\mu_j$，值函数为：\n$$v(S) = \\sum_{i \\in S} w_i x_i + \\sum_{j \\notin S} w_j \\mu_j + b$$\n\nShapley值的公式很复杂。一个更直接的方法是使用公理，特别是线性（或可加性）公理。模型 $g(x)$ 是可加的：\n$$g(x) = b + \\sum_{k=1}^d w_k x_k$$\n我们可以将博弈 $g$ 分解为 $d+1$ 个更简单的博弈之和：一个常数博弈 $g_0(x) = b$ 和 $d$ 个单特征博弈 $g_k(x) = w_k x_k$（对于 $k \\in \\{1, \\dots, d\\}$）。\n$$g(x) = g_0(x) + \\sum_{k=1}^d g_k(x)$$\n线性公理指出，博弈之和的SHAP值等于每个博弈的SHAP值之和。对于任何特征 $j$：\n$$\\phi_j(g) = \\phi_j(g_0) + \\sum_{k=1}^d \\phi_j(g_k)$$\n\n让我们分析每个组成博弈的SHAP值。\n\n1.  **常数博弈 $g_0(x) = b$**：\n    对于这个博弈，任何联盟 $S$ 的值函数 $v_0(S)$ 都是 $E[b] = b$。任何特征 $j$ 对于不包含它的任何联盟 $S$ 的边际贡献是 $v_0(S \\cup \\{j\\}) - v_0(S) = b - b = 0$。这意味着在这个博弈中，每个特征都是一个“虚拟”参与者。根据虚拟人公理，任何特征的SHAP值都为零：\n    $$\\phi_j(g_0) = 0 \\quad \\text{for all } j \\in \\{1, \\dots, d\\}$$\n\n2.  **对于固定的 $k \\in \\{1, \\dots, d\\}$ 的单特征博弈 $g_k(x) = w_k x_k$**：\n    设此博弈的值函数为 $v_k(S)$。\n    -   如果特征 $k$ 在联盟中（$k \\in S$），$v_k(S) = E[w_k X_k | X_S=x_S] = w_k x_k$。\n    -   如果特征 $k$ 不在联盟中（$k \\notin S$），$v_k(S) = E[w_k X_k | X_S=x_S] = w_k E[X_k] = w_k \\mu_k$。\n\n    现在我们计算任意特征 $j$ 的SHAP值 $\\phi_j(g_k)$。\n    -   **情况 $j \\neq k$**：考虑特征 $j$ 对不包含它的联盟 $S$ 的边际贡献。\n        -   如果 $k \\in S$，则 $k \\in S \\cup \\{j\\}$。因此，$v_k(S \\cup \\{j\\}) = w_k x_k$ 且 $v_k(S) = w_k x_k$。差值为 $0$。\n        -   如果 $k \\notin S$，则 $k \\notin S \\cup \\{j\\}$。因此，$v_k(S \\cup \\{j\\}) = w_k \\mu_k$ 且 $v_k(S) = w_k \\mu_k$。差值为 $0$。\n        由于特征 $j$ 的边际贡献总是零，所以在博弈 $g_k$ 中 $j$ 是一个虚拟参与者。根据虚拟人公理：\n        $$\\phi_j(g_k) = 0 \\quad \\text{for } j \\neq k$$\n    -   **情况 $j = k$**：我们需要找到 $\\phi_k(g_k)$。我们知道，对于博弈 $g_k$，所有 $j \\neq k$ 的参与者都是虚拟参与者。因此，博弈 $g_k$ 的SHAP值之和就是 $\\phi_k(g_k)$。根据有效性公理，SHAP值的总和等于大联盟 ($N$) 和空联盟 ($\\emptyset$) 之间的总支付差额。\n        $$\\sum_{j=1}^d \\phi_j(g_k) = \\phi_k(g_k) = v_k(N) - v_k(\\emptyset)$$\n        -   $v_k(N)$：对于大联盟 $N$，特征 $k$ 被包含在内，所以 $v_k(N) = w_k x_k$。\n        -   $v_k(\\emptyset)$：对于空联盟 $\\emptyset$，特征 $k$ 未被包含在内，所以 $v_k(\\emptyset) = w_k \\mu_k$。\n        因此，参与者 $k$ 在博弈 $g_k$ 中的SHAP值为：\n        $$\\phi_k(g_k) = w_k x_k - w_k \\mu_k = w_k(x_k - \\mu_k)$$\n\n3.  **组合结果**：\n    使用线性公理，原始博弈 $g$ 中特征 $j$ 的SHAP值为：\n    $$\\phi_j(g; x, \\mu) = \\phi_j(g_0) + \\sum_{k=1}^d \\phi_j(g_k)$$\n    和式中唯一的非零项是当 $k=j$ 时。\n    $$\\phi_j(g; x, \\mu) = 0 + \\phi_j(g_j) = w_j(x_j - \\mu_j)$$\n    这就完成了特征SHAP值的推导。\n\n**基准值 $\\phi_0$ 的推导**：\n基准值 $\\phi_0$ 是没有任何特征信息时的期望预测值，它对应于空联盟的值 $v(\\emptyset)$。\n$$\\phi_0(\\mu) = v(\\emptyset) = E[g(X)]$$\n其中期望是针对整个背景分布计算的。\n$$\\phi_0(\\mu) = E\\left[\\sum_{j=1}^d w_j X_j + b\\right] = \\sum_{j=1}^d w_j E[X_j] + b$$\n代入 $E[X_j] = \\mu_j$：\n$$\\phi_0(\\mu) = \\sum_{j=1}^d w_j \\mu_j + b$$\n这就完成了基准值的推导。\n\n### 第2部分：SHAP值变化的推导\n\n我们需要计算当背景分布从医院 $A$（均值 $\\mu^A$）变为医院 $B$（均值 $\\mu^B$）时，固定患者 $x^B$ 的SHAP值的变化。\n\n在医院 $A$ 时特征 $j$ 的SHAP值由第1部分的公式给出，使用 $\\mu^A$ 作为背景均值向量：\n$$\\phi_j(x^{B}; \\mu^{A}) = w_j (x^{B}_j - \\mu^{A}_j)$$\n\n同样地，在医院 $B$ 时特征 $j$ 的SHAP值使用 $\\mu^B$：\n$$\\phi_j(x^{B}; \\mu^{B}) = w_j (x^{B}_j - \\mu^{B}_j)$$\n\n变化量 $\\Delta \\phi_j$ 是这两个值之间的差：\n$$\\Delta \\phi_{j}(x^{B}; \\mu^{A} \\rightarrow \\mu^{B}) = \\phi_{j}(x^{B}; \\mu^{B}) - \\phi_{j}(x^{B}; \\mu^{A})$$\n代入表达式：\n$$\\Delta \\phi_j = w_j (x^{B}_j - \\mu^{B}_j) - w_j (x^{B}_j - \\mu^{A}_j)$$\n分配权重 $w_j$：\n$$\\Delta \\phi_j = (w_j x^{B}_j - w_j \\mu^{B}_j) - (w_j x^{B}_j - w_j \\mu^{A}_j)$$\n$$\\Delta \\phi_j = w_j x^{B}_j - w_j \\mu^{B}_j - w_j x^{B}_j + w_j \\mu^{A}_j$$\n与患者相关的项 $w_j x^{B}_j$ 被消掉了：\n$$\\Delta \\phi_j = w_j \\mu^{A}_j - w_j \\mu^{B}_j$$\n提出因子 $w_j$：\n$$\\Delta \\phi_j(x^{B}; \\mu^{A} \\rightarrow \\mu^{B}) = w_j (\\mu^{A}_j - \\mu^{B}_j)$$\n这证明了特征 $j$ 的SHAP值的变化仅取决于该特征的背景均值变化及其对应的模型权重。\n\n### 第3部分：实现与计算\n\n我们实现一个程序来为每个场景计算SHAP值变化向量 $\\Delta \\phi$。该向量的公式为：\n$$\\Delta \\phi(x^{B}; \\mu^{A} \\rightarrow \\mu^{B}) = \\left(w_1(\\mu^{A}_1 - \\mu^{B}_1), w_2(\\mu^{A}_2 - \\mu^{B}_2), \\dots, w_d(\\mu^{A}_d - \\mu^{B}_d)\\right)$$\n这可以通过逐元素乘积 $w \\odot (\\mu^A - \\mu^B)$ 来计算，其中 $\\odot$ 是逐元素乘积。\n\n给定数据：\n- $d = 3$\n- $w = (1.0, -0.5, 0.0)$\n- 如第2部分所示，此计算不需要患者特征 $x^B$ 和截距 $b$。\n\n**场景1：** $\\mu^{A} = (0.0, 0.0, 0.0)$, $\\mu^{B} = (0.8, -0.4, 0.3)$\n- $\\mu^{A} - \\mu^{B} = (-0.8, 0.4, -0.3)$\n- $\\Delta\\phi = (1.0 \\times -0.8, -0.5 \\times 0.4, 0.0 \\times -0.3) = (-0.8, -0.2, 0.0)$\n\n**场景2：** $\\mu^{A} = (0.8, -0.4, 0.3)$, $\\mu^{B} = (0.8, -0.4, 0.3)$\n- $\\mu^{A} - \\mu^{B} = (0.0, 0.0, 0.0)$\n- $\\Delta\\phi = (1.0 \\times 0.0, -0.5 \\times 0.0, 0.0 \\times 0.0) = (0.0, 0.0, 0.0)$\n\n**场景3：** $\\mu^{A} = (0.8, -0.4, 0.3)$, $\\mu^{B} = (0.8, -0.4, 1.3)$\n- $\\mu^{A} - \\mu^{B} = (0.0, 0.0, -1.0)$\n- $\\Delta\\phi = (1.0 \\times 0.0, -0.5 \\times 0.0, 0.0 \\times -1.0) = (0.0, 0.0, 0.0)$\n（变化为零，因为漂移仅发生在权重 $w_3=0.0$ 的特征3上。）\n\n**场景4：** $\\mu^{A} = (0.8, -0.4, 0.3)$, $\\mu^{B} = (1.8, -0.4, 0.3)$\n- $\\mu^{A} - \\mu^{B} = (-1.0, 0.0, 0.0)$\n- $\\Delta\\phi = (1.0 \\times -1.0, -0.5 \\times 0.0, 0.0 \\times 0.0) = (-1.0, 0.0, 0.0)$\n\n以下程序将执行这些计算并按指定格式输出结果。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the change in SHAP values for a logistic model when the\n    background distribution of features changes.\n    \"\"\"\n\n    # Model parameters\n    w = np.array([1.0, -0.5, 0.0])\n\n    # Test cases: Sets of background means for hospital A and hospital B\n    # Each case is a tuple (mu_A, mu_B)\n    test_cases = [\n        # Scenario 1 (general shift)\n        (np.array([0.0, 0.0, 0.0]), np.array([0.8, -0.4, 0.3])),\n        # Scenario 2 (no shift)\n        (np.array([0.8, -0.4, 0.3]), np.array([0.8, -0.4, 0.3])),\n        # Scenario 3 (shift only in an irrelevant feature)\n        (np.array([0.8, -0.4, 0.3]), np.array([0.8, -0.4, 1.3])),\n        # Scenario 4 (single-feature shift)\n        (np.array([0.8, -0.4, 0.3]), np.array([1.8, -0.4, 0.3])),\n    ]\n\n    results = []\n    for mu_A, mu_B in test_cases:\n        # The change in SHAP value for feature j is w_j * (mu_A_j - mu_B_j).\n        # This can be computed for all features at once using an element-wise\n        # vector product.\n        delta_phi_vector = w * (mu_A - mu_B)\n\n        # Convert numpy array to a list of standard Python floats\n        results.append(delta_phi_vector.tolist())\n\n    # Format the final output string to be a list of lists.\n    def format_list(lst):\n        # Creates a string representation of a list like '[v1,v2,v3]'\n        return f\"[{','.join(f'{v:.1f}' for v in lst)}]\"\n    \n    # Using python's default str() adds spaces, so we create the string manually\n    # to match the specified format precisely.\n    result_strings = [format_list(res) for res in results]\n    final_output_string = f\"[{','.join(result_strings)}]\"\n    \n    # Print the single-line result as required\n    print(final_output_string)\n\nsolve()\n```"
        }
    ]
}