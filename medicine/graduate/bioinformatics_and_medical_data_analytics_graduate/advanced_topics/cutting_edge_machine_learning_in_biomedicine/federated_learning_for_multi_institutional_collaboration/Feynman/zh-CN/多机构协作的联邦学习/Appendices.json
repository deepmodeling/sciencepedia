{
    "hands_on_practices": [
        {
            "introduction": "在部署复杂的联邦学习系统之前，对其性能进行建模至关重要。本练习将引导你计算通信开销并估算总训练时长，重点揭示网络瓶颈（即“掉队者”）对同步训练效率的关键影响。通过这个实践，你将从第一性原理出发，深入理解联邦学习中的系统级权衡。",
            "id": "4318598",
            "problem": "一个医院生物样本库联盟正在使用联邦学习（FL），特别是同步联邦平均（FedAvg）算法，来训练一个疾病风险预测模型，以遵守其主管机构审查委员会（IRB）强制规定的数据最小化和非传输策略。该联盟共有 $N=20$ 个生物样本库站点。在每个联邦轮次中，协调服务器向所有站点广播当前的模型参数，每个站点执行本地训练，然后每个站点将一组压缩后的更新模型参数上传到服务器。治理要求每轮都包含所有站点，以保持代表性和公平性。\n\n假设存在以下科学上成立且现实的条件：\n- 未压缩的模型大小为 $M=50$ 兆字节（MB）。使用十进制兆字节，其中 $1\\,\\mathrm{MB}=10^{6}$ 字节。\n- 应用于下行链路和上行链路负载的无损压缩方案产生的压缩比为 $r=0.2$，因此每个站点每个方向传输的负载大小为 $rM$。\n- 网络传输使用点对点链路；向 $N$ 个站点广播模型会导致 $N$ 次大小为 $rM$ 的独立传输（无多播聚合）。\n- 在每轮中，所有站点必须在轮次完成前结束下行链路、本地计算和上行链路阶段（同步屏障）。\n- 十九个站点的下行链路带宽为 $100\\,\\mathrm{Mb/s}$，上行链路带宽为 $100\\,\\mathrm{Mb/s}$，单向延迟分别为 $0.05\\,\\mathrm{s}$（下行链路）和 $0.10\\,\\mathrm{s}$（上行链路）。一个掉队站点的下行链路带宽为 $20\\,\\mathrm{Mb/s}$，上行链路带宽为 $20\\,\\mathrm{Mb/s}$，延迟与其它站点相同。使用十进制兆比特，其中 $1\\,\\mathrm{Mb}=10^{6}$ 比特。\n- 每个站点每轮的本地训练时间为 $30\\,\\mathrm{s}$，假设所有站点相同且与网络条件无关。\n- 忽略服务器端的聚合时间以及超出所述负载大小和延迟的加密协议开销。\n\n从同步FL轮次执行的核心定义以及数据大小、带宽和延迟之间的关系出发，完成以下任务：\n1. 计算每轮的通信开销，即所有链路（双向合计）上以兆字节（MB）为单位传输的总数据量。\n2. 估算在 $T$ 轮训练中的总挂钟训练时间（以小时为单位），表示为 $T$ 的函数。估算时需使用同步屏障假设，即轮次持续时间由下行链路最慢的站点和上行链路最慢的站点决定。\n\n将您的答案四舍五入到四位有效数字。对于第1部分，以兆字节表示数据量。对于第2部分，以小时表示总时间。",
            "solution": "**问题验证**\n\n根据有效性所需标准对所提供的问题陈述进行分析。\n\n**步骤1：提取给定条件**\n- 站点数量：$N=20$\n- 联邦学习模型：同步联邦平均（FedAvg）\n- 治理要求：每轮包含所有站点\n- 未压缩模型大小：$M=50$ 兆字节（$1\\,\\mathrm{MB}=10^6$ 字节）\n- 压缩比：$r=0.2$\n- 网络传输：点对点链路\n- 轮次完成条件：下行链路、本地计算和上行链路阶段的同步屏障\n- 正常站点数量：$19$\n- 正常站点带宽：$B_{down,normal} = B_{up,normal} = 100\\,\\mathrm{Mb/s}$（$1\\,\\mathrm{Mb}=10^6$ 比特）\n- 正常站点单向延迟：$L_{down,normal} = 0.05\\,\\mathrm{s}$，$L_{up,normal} = 0.10\\,\\mathrm{s}$\n- 掉队站点数量：$1$\n- 掉队站点带宽：$B_{down,straggler} = B_{up,straggler} = 20\\,\\mathrm{Mb/s}$\n- 掉队站点单向延迟：$L_{down,straggler} = 0.05\\,\\mathrm{s}$，$L_{up,straggler} = 0.10\\,\\mathrm{s}$\n- 每个站点每轮的本地训练时间：$T_{local}=30\\,\\mathrm{s}$\n- 总轮次数：$T$\n- 忽略因素：服务器端聚合时间、加密协议开销\n\n**步骤2：使用提取的给定条件进行验证**\n问题评估如下：\n- **科学上成立**：问题建立在分布式机器学习（联邦学习）和计算机网络的既定原则之上。诸如同步轮次、由掉队节点引起的通信瓶颈，以及使用带宽和延迟对数据传输时间进行建模等概念，都是标准且科学合理的。所提供的数值对于多机构研究环境是现实的。\n- **良态问题（Well-Posed）**：问题规范清晰，提供了所有必要的参数和约束，可以为所要求的两个量得出唯一且有意义的解。\n- **客观性**：问题以精确、量化且无偏见的技术语言陈述，不含主观或推测性主张。\n- **完整性与一致性**：问题是自洽的，没有矛盾。正常站点数量（$19$）和掉队站点数量（$1$）正确地总计为站点总数（$N=20$）。\n\n**步骤3：结论与操作**\n问题被认定为有效。完整的解答如下。\n\n---\n\n**第1部分：每轮通信开销**\n\n每轮的总通信开销 $V_{total}$ 是从服务器传输到所有站点（下行链路）和从所有站点传输到服务器（上行链路）的总数据量之和。\n\n首先，我们确定通过每个链路传输的压缩模型负载的大小 $S$。\n$$S = rM$$\n给定未压缩模型大小 $M=50\\,\\mathrm{MB}$ 和压缩比 $r=0.2$：\n$$S = 0.2 \\times 50\\,\\mathrm{MB} = 10\\,\\mathrm{MB}$$\n在下行链路阶段，服务器将此负载发送到 $N=20$ 个站点中的每一个。由于链路是点对点的，总下行链路数据量 $V_{down}$ 为：\n$$V_{down} = N \\times S = 20 \\times 10\\,\\mathrm{MB} = 200\\,\\mathrm{MB}$$\n在上行链路阶段，每个站点（共 $N=20$ 个）将其更新的（并压缩的）模型参数发送回服务器。因此，总上行链路数据量 $V_{up}$ 是相同的：\n$$V_{up} = N \\times S = 20 \\times 10\\,\\mathrm{MB} = 200\\,\\mathrm{MB}$$\n每轮的总通信开销是下行链路和上行链路数据量之和：\n$$V_{total} = V_{down} + V_{up} = 200\\,\\mathrm{MB} + 200\\,\\mathrm{MB} = 400\\,\\mathrm{MB}$$\n问题要求答案保留四位有效数字。因此，总开销为 $400.0\\,\\mathrm{MB}$。\n\n**第2部分：T轮的总挂钟训练时间**\n\n$T$ 轮的总训练时间 $T_{total}$ 是轮次数与单轮持续时间 $T_{round}$ 的乘积。\n$$T_{total}(T) = T \\times T_{round}$$\n单个同步轮次的持续时间是其顺序阶段（下行链路、本地计算和上行链路）的持续时间之和。由于同步屏障的存在，每个通信阶段的持续时间由最慢的（掉队）站点决定。\n\n首先，我们将负载大小 $S$ 从兆字节（megabytes）转换为兆比特（megabits），以与带宽单位（$1\\,\\mathrm{Mb/s} = 10^6\\,\\mathrm{bits/s}$）保持一致。使用转换关系 $1\\,\\mathrm{byte} = 8\\,\\mathrm{bits}$：\n$$S = 10\\,\\mathrm{MB} = 10 \\times 10^6\\,\\mathrm{bytes} = 10 \\times 10^6 \\times 8\\,\\mathrm{bits} = 80 \\times 10^6\\,\\mathrm{bits} = 80\\,\\mathrm{Mb}$$\n单次数据传输的时间模型为单向延迟（$L$）和序列化时间（$S/B$，其中 $B$ 是带宽）之和。\n\n**下行链路阶段持续时间 ($T_{down\\_phase}$)**\n这是最后一个站点接收到模型更新所需的时间。我们必须比较正常站点和掉队站点的总下行链路时间。\n- 对于正常站点：\n$$T_{down,normal} = L_{down,normal} + \\frac{S}{B_{down,normal}} = 0.05\\,\\mathrm{s} + \\frac{80\\,\\mathrm{Mb}}{100\\,\\mathrm{Mb/s}} = 0.05\\,\\mathrm{s} + 0.8\\,\\mathrm{s} = 0.85\\,\\mathrm{s}$$\n- 对于掉队站点：\n$$T_{down,straggler} = L_{down,straggler} + \\frac{S}{B_{down,straggler}} = 0.05\\,\\mathrm{s} + \\frac{80\\,\\mathrm{Mb}}{20\\,\\mathrm{Mb/s}} = 0.05\\,\\mathrm{s} + 4.0\\,\\mathrm{s} = 4.05\\,\\mathrm{s}$$\n该阶段的持续时间是这些时间中的最大值：\n$$T_{down\\_phase} = \\max(T_{down,normal}, T_{down,straggler}) = 4.05\\,\\mathrm{s}$$\n\n**本地计算阶段持续时间 ($T_{local}$)**\n给定此时间对所有站点都相同：\n$$T_{local} = 30\\,\\mathrm{s}$$\n\n**上行链路阶段持续时间 ($T_{up\\_phase}$)**\n这是服务器接收到最后一个站点的更新所需的时间。\n- 对于正常站点：\n$$T_{up,normal} = L_{up,normal} + \\frac{S}{B_{up,normal}} = 0.10\\,\\mathrm{s} + \\frac{80\\,\\mathrm{Mb}}{100\\,\\mathrm{Mb/s}} = 0.10\\,\\mathrm{s} + 0.8\\,\\mathrm{s} = 0.90\\,\\mathrm{s}$$\n- 对于掉队站点：\n$$T_{up,straggler} = L_{up,straggler} + \\frac{S}{B_{up,straggler}} = 0.10\\,\\mathrm{s} + \\frac{80\\,\\mathrm{Mb}}{20\\,\\mathrm{Mb/s}} = 0.10\\,\\mathrm{s} + 4.0\\,\\mathrm{s} = 4.10\\,\\mathrm{s}$$\n该阶段的持续时间是这些时间中的最大值：\n$$T_{up\\_phase} = \\max(T_{up,normal}, T_{up,straggler}) = 4.10\\,\\mathrm{s}$$\n\n**总轮次持续时间 ($T_{round}$)**\n一个同步轮次的总时间是三个顺序阶段的持续时间之和：\n$$T_{round} = T_{down\\_phase} + T_{local} + T_{up\\_phase} = 4.05\\,\\mathrm{s} + 30\\,\\mathrm{s} + 4.10\\,\\mathrm{s} = 38.15\\,\\mathrm{s}$$\n\n**$T$ 轮的总训练时间**\n$T$ 轮的总时间以小时表示。一小时有 $3600\\,\\mathrm{s}$。\n$$T_{total}(T) = T \\times T_{round} = T \\times 38.15\\,\\mathrm{s} \\times \\frac{1\\,\\mathrm{hour}}{3600\\,\\mathrm{s}} = T \\times \\frac{38.15}{3600}\\,\\mathrm{hours}$$\n$$T_{total}(T) \\approx T \\times 0.01059722...\\,\\mathrm{hours}$$\n将系数四舍五入到四位有效数字，得到 $0.01060$。\n$$T_{total}(T) \\approx 0.01060\\,T\\,\\mathrm{hours}$$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n400.0  & 0.01060T\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "联邦学习中最重大的挑战之一是处理跨机构的非独立同分布（Non-IID）数据，这会导致局部模型更新出现分歧。本练习将探讨如何通过优化客户端贡献的权重来最小化全局更新的误差，从而提供一个具体的方法来缓解统计异质性带来的负面影响。这个问题揭示了在局部梯度方差和由数据分布差异引入的偏差之间的权衡。",
            "id": "4563886",
            "problem": "考虑两家医院 $A$ 和 $B$ 通过联邦平均 (FedAvg) 协作，使用电子健康记录 (EHR) 训练一个用于二元疾病预测任务的逻辑回归模型。由于患者人口统计特征和合并症概况不同，各医院的数据呈现非独立同分布 (Non-IID) 特性，导致局部梯度存在统计异质性。\n\n在某一训练轮次中，服务器针对单个模型参数，使用权重 $w$ (用于医院 $A$) 和 $1-w$ (用于医院 $B$) 来聚合来自两家医院的一维随机梯度。设此轮的局部随机梯度建模为 $g_A = \\mu_A + \\epsilon_A$ 和 $g_B = \\mu_B + \\epsilon_B$，其中 $\\epsilon_A$ 和 $\\epsilon_B$ 是代表随机梯度噪声的零均值随机变量，在医院间相互独立，其方差分别为 $\\mathrm{Var}(\\epsilon_A) = \\sigma_A^2$ 和 $\\mathrm{Var}(\\epsilon_B) = \\sigma_B^2$。此轮的预期局部梯度均值分别为 $\\mu_A$ 和 $\\mu_B$。如果所有数据被汇集起来，将得到的中心化梯度是按数据比例加权的平均值 $T = p_A \\mu_A + p_B \\mu_B$，其中 $p_A$ 和 $p_B$ 分别是医院 $A$ 和 $B$ 持有的总样本比例，且满足 $p_A + p_B = 1$。\n\n服务器试图选择 $w$ 以最小化此轮中聚合的联邦梯度与中心化梯度之间的期望平方偏差，即最小化\n$$\n\\mathbb{E}\\left[\\left(w g_A + (1-w) g_B - T\\right)^{2}\\right].\n$$\n\n假设 $\\epsilon_A$ 和 $\\epsilon_B$ 相互独立，局部随机梯度是无偏的，并且 $w$ 是满足 $w + (1-w) = 1$ 的实数。使用从当前轮次的局部梯度统计数据中获得的以下科学上合理的值：\n- $\\mu_A = 0.14$, $\\mu_B = -0.02$,\n- $\\sigma_A^2 = 0.0025$, $\\sigma_B^2 = 0.0064$,\n- $p_A = 0.65$, $p_B = 0.35$.\n\n从上述定义和假设出发，推导期望平方偏差的表达式，并找出使其最小化的 $w$ 值。将最终答案表示为单个实数。答案保留四位有效数字。",
            "solution": "该问题是适定的，其科学基础在于联邦学习和优化的原理，并包含了获得唯一解所需的所有信息。所提供的数值在训练机器学习模型的背景下是合理的。因此，我将开始推导。\n\n目标是找到权重因子 $w$，以最小化聚合的联邦梯度与中心化梯度之间的期望平方偏差。目标函数由下式给出：\n$$\nL(w) = \\mathbb{E}\\left[\\left(w g_A + (1-w) g_B - T\\right)^{2}\\right]\n$$\n其中 $g_A$ 和 $g_B$ 是随机梯度，$T$ 是常数中心化梯度目标。\n\n我们可以使用偏差-方差分解来分解期望平方误差。对于任意随机变量 $X$ 和常数 $c$，我们有 $\\mathbb{E}[(X-c)^2] = \\mathrm{Var}(X) + (\\mathbb{E}[X]-c)^2$。令 $X = w g_A + (1-w) g_B$ 且 $c = T$。目标函数变为：\n$$\nL(w) = \\mathrm{Var}\\left(w g_A + (1-w) g_B\\right) + \\left(\\mathbb{E}\\left[w g_A + (1-w) g_B\\right] - T\\right)^2\n$$\n我们将分别分析方差项和偏差项。\n\n首先，我们计算方差项。局部随机梯度由 $g_A = \\mu_A + \\epsilon_A$ 和 $g_B = \\mu_B + \\epsilon_B$ 给出。由于 $\\mu_A$ 和 $\\mu_B$ 是常数，局部梯度的方差为：\n$$\n\\mathrm{Var}(g_A) = \\mathrm{Var}(\\mu_A + \\epsilon_A) = \\mathrm{Var}(\\epsilon_A) = \\sigma_A^2\n$$\n$$\n\\mathrm{Var}(g_B) = \\mathrm{Var}(\\mu_B + \\epsilon_B) = \\mathrm{Var}(\\epsilon_B) = \\sigma_B^2\n$$\n问题指出随机噪声项 $\\epsilon_A$ 和 $\\epsilon_B$ 是独立的，这意味着随机梯度 $g_A$ 和 $g_B$ 也是独立的。对于独立的随机变量，其加权和的方差是：\n$$\n\\mathrm{Var}\\left(w g_A + (1-w) g_B\\right) = w^2 \\mathrm{Var}(g_A) + (1-w)^2 \\mathrm{Var}(g_B) = w^2 \\sigma_A^2 + (1-w)^2 \\sigma_B^2\n$$\n\n接下来，我们计算偏差平方项 $(\\mathbb{E}[w g_A + (1-w) g_B] - T)^2$。我们首先求出聚合梯度的期望。利用期望的线性性质以及 $\\mathbb{E}[\\epsilon_A] = 0$ 和 $\\mathbb{E}[\\epsilon_B] = 0$：\n$$\n\\mathbb{E}[g_A] = \\mathbb{E}[\\mu_A + \\epsilon_A] = \\mu_A\n$$\n$$\n\\mathbb{E}[g_B] = \\mathbb{E}[\\mu_B + \\epsilon_B] = \\mu_B\n$$\n聚合梯度的期望是：\n$$\n\\mathbb{E}[w g_A + (1-w) g_B] = w \\mathbb{E}[g_A] + (1-w) \\mathbb{E}[g_B] = w \\mu_A + (1-w) \\mu_B\n$$\n中心化梯度为 $T = p_A \\mu_A + p_B \\mu_B$。由于 $p_A + p_B = 1$，我们有 $p_B = 1 - p_A$。与目标的偏差为：\n$$\n\\mathbb{E}[w g_A + (1-w) g_B] - T = (w \\mu_A + (1-w) \\mu_B) - (p_A \\mu_A + (1-p_A) \\mu_B)\n$$\n$$\n= (w - p_A)\\mu_A + ((1-w) - (1-p_A))\\mu_B = (w - p_A)\\mu_A - (w - p_A)\\mu_B = (w - p_A)(\\mu_A - \\mu_B)\n$$\n因此，偏差平方项为：\n$$\n\\left((w-p_A)(\\mu_A - \\mu_B)\\right)^2 = (w-p_A)^2 (\\mu_A - \\mu_B)^2\n$$\n\n结合方差项和偏差平方项，我们得到完整的目标函数 $L(w)$：\n$$\nL(w) = w^2 \\sigma_A^2 + (1-w)^2 \\sigma_B^2 + (w-p_A)^2 (\\mu_A - \\mu_B)^2\n$$\n为了找到使 $L(w)$ 最小化的 $w$ 值，我们对 $L(w)$ 关于 $w$ 求导，并令其等于 $0$。\n$$\n\\frac{dL}{dw} = \\frac{d}{dw} \\left[ w^2 \\sigma_A^2 + (1-w)^2 \\sigma_B^2 + (w-p_A)^2 (\\mu_A - \\mu_B)^2 \\right]\n$$\n$$\n\\frac{dL}{dw} = 2w \\sigma_A^2 + 2(1-w)(-1) \\sigma_B^2 + 2(w-p_A) (\\mu_A - \\mu_B)^2 = 0\n$$\n两边除以 2：\n$$\nw \\sigma_A^2 - (1-w) \\sigma_B^2 + (w-p_A) (\\mu_A - \\mu_B)^2 = 0\n$$\n我们展开并合并含有 $w$ 的项：\n$$\nw \\sigma_A^2 - \\sigma_B^2 + w \\sigma_B^2 + w (\\mu_A - \\mu_B)^2 - p_A (\\mu_A - \\mu_B)^2 = 0\n$$\n$$\nw (\\sigma_A^2 + \\sigma_B^2 + (\\mu_A - \\mu_B)^2) = \\sigma_B^2 + p_A (\\mu_A - \\mu_B)^2\n$$\n解出 $w$：\n$$\nw = \\frac{\\sigma_B^2 + p_A (\\mu_A - \\mu_B)^2}{\\sigma_A^2 + \\sigma_B^2 + (\\mu_A - \\mu_B)^2}\n$$\n二阶导数为 $\\frac{d^2L}{dw^2} = 2\\sigma_A^2 + 2\\sigma_B^2 + 2(\\mu_A - \\mu_B)^2$，由于方差为正，该二阶导数严格为正，这证实了求得的 $w$ 值对应一个最小值。\n\n现在，我们代入给定的数值：\n- $\\mu_A = 0.14$\n- $\\mu_B = -0.02$\n- $\\sigma_A^2 = 0.0025$\n- $\\sigma_B^2 = 0.0064$\n- $p_A = 0.65$\n\n首先，我们计算与预期梯度差异相关的项：\n$$\n\\mu_A - \\mu_B = 0.14 - (-0.02) = 0.16\n$$\n$$\n(\\mu_A - \\mu_B)^2 = (0.16)^2 = 0.0256\n$$\n现在，将这些值代入 $w$ 的表达式中。\n分子为：\n$$\n\\sigma_B^2 + p_A (\\mu_A - \\mu_B)^2 = 0.0064 + (0.65)(0.0256) = 0.0064 + 0.01664 = 0.02304\n$$\n分母为：\n$$\n\\sigma_A^2 + \\sigma_B^2 + (\\mu_A - \\mu_B)^2 = 0.0025 + 0.0064 + 0.0256 = 0.0345\n$$\n所以，$w$ 的最优值为：\n$$\nw = \\frac{0.02304}{0.0345} \\approx 0.66782608...\n$$\n将结果四舍五入到四位有效数字，我们得到 $w = 0.6678$。",
            "answer": "$$\n\\boxed{0.6678}\n$$"
        },
        {
            "introduction": "本练习将理论模型与实际执行联系起来，要求你为一个联邦学习编排框架设计一个确定性的模拟器。通过实现客户端选择、本地更新、梯度裁剪、量化和服务器聚合等核心逻辑，你将从代码层面深入理解这些组件如何相互作用，共同驱动联邦训练过程。这项练习在一个简化且受控的环境中，模拟了如Flower或NVFLARE等框架的底层机制。",
            "id": "4563870",
            "problem": "您的任务是为联邦学习（FL）实现一个最小化的、确定性的编排模拟器，该模拟器模拟现代编排框架（如 Flower 和 NVIDIA Federated Learning Application Runtime Environment (NVFLARE)）的核心语义，专注于生物信息学和医疗数据分析中的多机构协作场景。该模拟器必须执行基于轮次的同步协调，其中多个机构（客户端）在特定站点的数据上进行本地训练，并向中央协调器（服务器）提交更新，后者聚合已接受的更新以生成新的全局模型。您的程序必须是完全确定性的，并且必须完全按照规定使用所提供的测试套件。\n\n从以下基本概念开始：\n- 经验风险最小化（ERM）和基于梯度的优化：对于一个拥有二次可微、强凸的本地目标函数 $f_i(\\mathbf{m})$ 的客户端，其梯度下降步骤为 $\\mathbf{m} \\leftarrow \\mathbf{m} - \\eta \\nabla f_i(\\mathbf{m})$，其中步长 $\\eta > 0$。\n- 局部最小值 $\\mathbf{m}_i^\\star$ 附近的二次近似：对于客户端 $i$，在 $\\mathbf{m}_i^\\star$ 附近，梯度可以建模为 $\\nabla f_i(\\mathbf{m}) \\approx h_i \\left(\\mathbf{m} - \\mathbf{m}_i^\\star\\right)$，其中标量曲率 $h_i > 0$（为简单起见，假设曲率是各向同性的）。\n- 联邦平均（FedAvg）：给定在轮次 $t$ 中一组已接受的客户端 $A^t$，其客户端数据量为 $n_i$，全局模型更新计算为客户端增量的数据量加权平均值。\n\n为该问题定义模型维度 $d \\in \\{2\\}$ 和在轮次 $t \\in \\{0,1,\\dots,R\\}$ 的全局模型 $\\mathbf{m}^t \\in \\mathbb{R}^d$。对于每个客户端 $i \\in \\{1,2,\\dots,K\\}$，定义：\n- 本地数据量 $n_i \\in \\mathbb{N}$，\n- 曲率标量 $h_i \\in \\mathbb{R}_{>0}$，\n- 局部最优解 $\\mathbf{m}_i^\\star \\in \\mathbb{R}^d$，\n- 参与指示符 $s_i^t \\in \\{0,1\\}$，\n- 通信延迟 $d_i^t$（秒），以及轮次超时 $\\tau$（秒）。\n\n在轮次 $t$ 中，每个 $s_i^t = 1$ 的参与客户端从广播的全局模型 $\\mathbf{m}^t$ 开始，计算一个单步的本地梯度下降：\n$$\n\\Delta_i^t \\equiv \\mathbf{m}_i^{t,\\text{local}} - \\mathbf{m}^t = -\\eta \\nabla f_i(\\mathbf{m}^t) \\approx -\\eta\\, h_i \\left(\\mathbf{m}^t - \\mathbf{m}_i^\\star\\right),\n$$\n然后在将更新发送到服务器之前，应用逐客户端的更新裁剪和量化：\n1. L2 裁剪，界限为 $C > 0$：\n$$\n\\widetilde{\\Delta}_i^t = \n\\begin{cases}\n\\Delta_i^t  &\\text{如果 } \\lVert \\Delta_i^t \\rVert_2 \\le C, \\\\\n\\dfrac{C}{\\lVert \\Delta_i^t \\rVert_2}\\, \\Delta_i^t  &\\text{其他情况。}\n\\end{cases}\n$$\n2. 使用 $b \\in \\mathbb{N}$ 比特的均匀逐坐标量化，步长为 $2^{-b}$：\n$Q_b(x) = \\dfrac{\\operatorname{round}\\!\\left(x \\cdot 2^b\\right)}{2^b}$， $\\widehat{\\Delta}_i^t = \\left(Q_b\\left(\\widetilde{\\Delta}_{i,1}^t\\right), Q_b\\left(\\widetilde{\\Delta}_{i,2}^t\\right)\\right)$。\n服务器当且仅当 $s_i^t = 1$ 且 $d_i^t \\le \\tau$ 时接受更新。将已接受的集合表示为 $A^t = \\left\\{ i \\mid s_i^t = 1 \\text{ and } d_i^t \\le \\tau \\right\\}$。服务器的更新规则是：\n$$\n\\mathbf{m}^{t+1} =\n\\begin{cases}\n\\mathbf{m}^{t} + \\displaystyle\\sum_{i \\in A^t} \\left(\\dfrac{n_i}{\\sum_{j \\in A^t} n_j}\\right) \\widehat{\\Delta}_i^t,  &\\text{如果 } \\left|A^t\\right| \\ge 1, \\\\\n\\mathbf{m}^{t},  &\\text{如果 } \\left|A^t\\right| = 0.\n\\end{cases}\n$$\n\n经过 $R$ 轮后，评估全局模型与全局数据量加权目标\n$$\n\\overline{\\mathbf{m}}^\\star = \\dfrac{\\sum_{i=1}^K n_i \\mathbf{m}_i^\\star}{\\sum_{i=1}^K n_i}\n$$\n的接近程度。报告欧几里得距离的平方\n$$\nD = \\left\\lVert \\mathbf{m}^R - \\overline{\\mathbf{m}}^\\star \\right\\rVert_2^2.\n$$\n\n您的程序必须精确实现上述过程，并为以下测试套件生成结果。除非另有说明，所有数字均为实值，延迟和超时以秒为单位，向量为二维。不存在随机性；您必须按原样使用指定的值。\n\n测试用例 1（一般情况，有超时和量化，无裁剪）：\n- $K = 4$, $d = 2$, $R = 3$。\n- $\\eta = 0.2$, $C = 10^9$, $b = 8$, $\\tau = 0.9$。\n- $\\mathbf{m}^0 = [0, 0]$。\n- 数据量 $[n_1,n_2,n_3,n_4] = [50, 30, 10, 10]$。\n- 曲率 $[h_1,h_2,h_3,h_4] = [1.0, 0.5, 1.5, 1.0]$。\n- 最优解 $\\mathbf{m}_1^\\star = [1, -1]$, $\\mathbf{m}_2^\\star = [0.5, -0.5]$, $\\mathbf{m}_3^\\star = [2, -2]$, $\\mathbf{m}_4^\\star = [1, -1]$。\n- 参与矩阵：对于所有 $i \\in \\{1,2,3,4\\}$ 和 $t \\in \\{0,1,2\\}$，$s_i^t = 1$。\n- 延迟矩阵（行为轮次 $t = 0,1,2$；列为客户端 $i = 1,2,3,4$）：\n  - 轮次 0：$[0.2, 0.5, 1.2, 0.1]$，\n  - 轮次 1：$[0.3, 1.1, 0.2, 0.4]$，\n  - 轮次 2：$[0.8, 0.7, 0.6, 1.5]$。\n\n测试用例 2（边界情况：由于超时为零，没有被接受的更新）：\n- $K = 3$, $d = 2$, $R = 5$。\n- $\\eta = 0.1$, $C = 10^9$, $b = 8$, $\\tau = 0.0$。\n- $\\mathbf{m}^0 = [-1, -1]$。\n- 数据量 $[n_1,n_2,n_3] = [20, 40, 40]$。\n- 曲率 $[h_1,h_2,h_3] = [1.0, 1.0, 1.0]$。\n- 最优解 $\\mathbf{m}_1^\\star = [1, 0]$, $\\mathbf{m}_2^\\star = [0, 1]$, $\\mathbf{m}_3^\\star = [1, 1]$。\n- 参与矩阵：对于所有 $i \\in \\{1,2,3\\}$ 和 $t \\in \\{0,1,2,3,4\\}$，$s_i^t = 1$。\n- 延迟矩阵：每个条目都等于 $0.1$。\n\n测试用例 3（边缘情况：强裁剪和粗量化）：\n- $K = 3$, $d = 2$, $R = 2$。\n- $\\eta = 0.5$, $C = 0.4$, $b = 2$, $\\tau = 10.0$。\n- $\\mathbf{m}^0 = [0, 0]$。\n- 数据量 $[n_1,n_2,n_3] = [100, 50, 50]$。\n- 曲率 $[h_1,h_2,h_3] = [5.0, 2.0, 1.0]$。\n- 最优解 $\\mathbf{m}_1^\\star = [1, 1]$, $\\mathbf{m}_2^\\star = [-1, -1]$, $\\mathbf{m}_3^\\star = [2, 0]$。\n- 参与矩阵：对于所有 $i \\in \\{1,2,3\\}$ 和 $t \\in \\{0,1\\}$，$s_i^t = 1$。\n- 延迟矩阵：每个条目都等于 $0.0$。\n\n要求的最终输出格式：\n- 对于每个测试用例，计算 $D = \\left\\lVert \\mathbf{m}^R - \\overline{\\mathbf{m}}^\\star \\right\\rVert_2^2$。\n- 您的程序应生成单行输出，包含三个结果，以逗号分隔列表的形式置于方括号中，每个浮点数精确到小数点后六位，例如：$[0.123456,0.000000,1.500000]$。\n\n所有计算必须严格遵循上述定义。不使用角度。时间单位必须视为秒。不使用百分比；任何小数部分都必须表示为小数。",
            "solution": "该问题要求实现一个用于同步、基于轮次的联邦学习（FL）编排协议的确定性模拟器。该模拟遵循联邦优化的原则，即中央服务器协调多个客户端（例如，医疗机构）的训练过程，而无需访问其原始数据。问题的核心是追踪一个全局模型向量 $\\mathbf{m}^t \\in \\mathbb{R}^d$ 在一系列 $R$ 个通信轮次中的演变过程。\n\n模拟遵循以下操作序列，对每个轮次 $t \\in \\{0, 1, \\dots, R-1\\}$ 执行。\n\n**1. 客户端选择与参与**\n\n在每个轮次 $t$ 中，会选择一个客户端子集参与训练。问题指定了一个参与矩阵 $s_i^t$，其中 $s_i^t=1$ 表示客户端 $i$ 被选中参与该轮次。服务器向这些参与的客户端广播当前的全局模型 $\\mathbf{m}^t$。在执行本地计算后，每个客户端发回其更新。然而，由于网络延迟，更新可能无法及时到达。一个轮次有严格的超时时间 $\\tau$。只有当客户端 $i$ 的通信延迟 $d_i^t$ 不超过超时时间 $\\tau$ 时，其更新才会被考虑用于聚合。因此，在轮次 $t$ 中被服务器接受更新的客户端集合，记作 $A^t$，正式定义为：\n$$\nA^t = \\left\\{ i \\mid s_i^t = 1 \\text{ and } d_i^t \\le \\tau \\right\\}\n$$\n\n**2. 本地客户端更新**\n\n每个参与的客户端 $i \\in A^t$ 执行本地计算以生成一个更新。问题将本地训练过程简化为在客户端的本地目标函数 $f_i(\\mathbf{m})$ 上执行单步梯度下降。目标函数 $f_i$ 假定为二次可微和强凸的。在其局部最小值 $\\mathbf{m}_i^\\star$ 附近，梯度 $\\nabla f_i(\\mathbf{m})$ 由一个线性函数近似：\n$$\n\\nabla f_i(\\mathbf{m}) \\approx h_i \\left(\\mathbf{m} - \\mathbf{m}_i^\\star\\right)\n$$\n其中 $h_i > 0$ 是一个标量，表示损失曲面的（各向同性）曲率。使用这个近似，从全局模型 $\\mathbf{m}^t$ 开始的客户端按如下方式计算其本地模型更新 $\\Delta_i^t$：\n$$\n\\Delta_i^t = -\\eta \\nabla f_i(\\mathbf{m}^t) = -\\eta\\, h_i \\left(\\mathbf{m}^t - \\mathbf{m}_i^\\star\\right)\n$$\n这里，$\\eta > 0$ 是学习率或步长。\n\n**3. 更新裁剪与量化**\n\n在传输之前，每个客户端处理其计算出的更新 $\\Delta_i^t$，以管理通信带宽并提高鲁棒性。\n\n首先，应用 L2 范数裁剪。更新向量的大小被限制在一个阈值 $C > 0$ 以内。这可以防止来自单个客户端的过大更新破坏全局模型的稳定性。裁剪后的更新 $\\widetilde{\\Delta}_i^t$ 为：\n$$\n\\widetilde{\\Delta}_i^t = \n\\begin{cases}\n\\Delta_i^t  &\\text{如果 } \\lVert \\Delta_i^t \\rVert_2 \\le C \\\\\n\\dfrac{C}{\\lVert \\Delta_i^t \\rVert_2}\\, \\Delta_i^t  &\\text{如果 } \\lVert \\Delta_i^t \\rVert_2 > C\n\\end{cases}\n$$\n\n其次，对裁剪后的更新进行量化，以减小其传输大小。一个具有 $b$ 比特的均匀量化方案应用于向量的每个坐标。量化函数 $Q_b(x)$ 将实数 $x$ 映射到一个离散值：\n$$\nQ_b(x) = \\dfrac{\\operatorname{round}\\!\\left(x \\cdot 2^b\\right)}{2^b}\n$$\n客户端 $i$ 准备传输的最终更新为 $\\widehat{\\Delta}_i^t = \\left(Q_b\\left(\\widetilde{\\Delta}_{i,1}^t\\right), Q_b\\left(\\widetilde{\\Delta}_{i,2}^t\\right), \\dots, Q_b\\left(\\widetilde{\\Delta}_{i,d}^t\\right)\\right)$。\n\n**4. 服务器端聚合**\n\n服务器从所有在已接受集合 $A^t$ 中的客户端收集处理后的更新 $\\widehat{\\Delta}_i^t$。如果该集合非空 ($|A^t| \\ge 1$)，服务器将聚合这些更新以形成下一个全局模型 $\\mathbf{m}^{t+1}$。聚合使用联邦平均（FedAvg）算法进行，其中每个客户端的更新按其相对数据量 $n_i$ 加权。已接受客户端的总数据量为 $N_t = \\sum_{j \\in A^t} n_j$。更新规则为：\n$$\n\\mathbf{m}^{t+1} = \\mathbf{m}^{t} + \\sum_{i \\in A^t} \\left(\\dfrac{n_i}{N_t}\\right) \\widehat{\\Delta}_i^t\n$$\n如果已接受集合 $A^t$ 为空 ($|A^t| = 0$)，即没有收到更新或超时时间过严，则全局模型保持不变：\n$$\n\\mathbf{m}^{t+1} = \\mathbf{m}^{t}\n$$\n这个迭代过程从初始模型 $\\mathbf{m}^0$ 开始，重复进行 $R$ 轮。\n\n**5. 最终评估**\n\n经过 $R$ 轮后，评估最终全局模型 $\\mathbf{m}^R$ 的质量。比较的基准是理想的全局最优解 $\\overline{\\mathbf{m}}^\\star$，即所有客户端局部最优解的数据量加权平均值：\n$$\n\\overline{\\mathbf{m}}^\\star = \\dfrac{\\sum_{i=1}^K n_i \\mathbf{m}_i^\\star}{\\sum_{i=1}^K n_i}\n$$\n性能指标是最终模型与此理想目标之间的欧几里得距离的平方 $D$：\n$$\nD = \\left\\lVert \\mathbf{m}^R - \\overline{\\mathbf{m}}^\\star \\right\\rVert_2^2\n$$\n对每个提供的测试用例执行模拟，并报告得到的 $D$ 值。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a deterministic Federated Learning orchestration simulator\n    and computes the final model's squared distance to the ideal optimum\n    for a given set of test cases.\n    \"\"\"\n\n    test_cases = [\n        # Test Case 1\n        {\n            \"K\": 4, \"R\": 3, \"eta\": 0.2, \"C\": 1e9, \"b\": 8, \"tau\": 0.9,\n            \"m0\": np.array([0.0, 0.0]),\n            \"n\": np.array([50, 30, 10, 10]),\n            \"h\": np.array([1.0, 0.5, 1.5, 1.0]),\n            \"m_star\": np.array([[1.0, -1.0], [0.5, -0.5], [2.0, -2.0], [1.0, -1.0]]),\n            \"s\": np.ones((3, 4), dtype=int),\n            \"delays\": np.array([[0.2, 0.5, 1.2, 0.1], [0.3, 1.1, 0.2, 0.4], [0.8, 0.7, 0.6, 1.5]])\n        },\n        # Test Case 2\n        {\n            \"K\": 3, \"R\": 5, \"eta\": 0.1, \"C\": 1e9, \"b\": 8, \"tau\": 0.0,\n            \"m0\": np.array([-1.0, -1.0]),\n            \"n\": np.array([20, 40, 40]),\n            \"h\": np.array([1.0, 1.0, 1.0]),\n            \"m_star\": np.array([[1.0, 0.0], [0.0, 1.0], [1.0, 1.0]]),\n            \"s\": np.ones((5, 3), dtype=int),\n            \"delays\": np.full((5, 3), 0.1)\n        },\n        # Test Case 3\n        {\n            \"K\": 3, \"R\": 2, \"eta\": 0.5, \"C\": 0.4, \"b\": 2, \"tau\": 10.0,\n            \"m0\": np.array([0.0, 0.0]),\n            \"n\": np.array([100, 50, 50]),\n            \"h\": np.array([5.0, 2.0, 1.0]),\n            \"m_star\": np.array([[1.0, 1.0], [-1.0, -1.0], [2.0, 0.0]]),\n            \"s\": np.ones((2, 3), dtype=int),\n            \"delays\": np.zeros((2, 3))\n        }\n    ]\n\n    results = []\n\n    for case in test_cases:\n        K, R, eta, C, b, tau = case[\"K\"], case[\"R\"], case[\"eta\"], case[\"C\"], case[\"b\"], case[\"tau\"]\n        m_global = case[\"m0\"].copy()\n        n, h, m_star = case[\"n\"], case[\"h\"], case[\"m_star\"]\n        s, delays = case[\"s\"], case[\"delays\"]\n        \n        quantization_factor = 2**b\n\n        for t in range(R):\n            # 1. Determine accepted clients\n            accepted_indices = [i for i in range(K) if s[t, i] == 1 and delays[t, i] = tau]\n            \n            if not accepted_indices:\n                # No updates, model does not change\n                continue\n\n            # 2. Server-side preparation\n            accepted_n = n[accepted_indices]\n            total_n_accepted = np.sum(accepted_n)\n            weights = accepted_n / total_n_accepted\n            \n            aggregated_delta = np.zeros_like(m_global)\n\n            for idx, client_idx in enumerate(accepted_indices):\n                # 3. Client-side computation\n                \n                # Calculate local update delta\n                delta = -eta * h[client_idx] * (m_global - m_star[client_idx])\n\n                # L2 clipping\n                norm = np.linalg.norm(delta)\n                if norm > C:\n                    clipped_delta = (C / norm) * delta\n                else:\n                    clipped_delta = delta\n                \n                # Quantization\n                quantized_delta = np.round(clipped_delta * quantization_factor) / quantization_factor\n                \n                # 4. Contribute to aggregation\n                aggregated_delta += weights[idx] * quantized_delta\n            \n            # 5. Update global model\n            m_global += aggregated_delta\n\n        # Final evaluation\n        m_final = m_global\n        \n        # Calculate global data-size-weighted target\n        total_n_global = np.sum(n)\n        m_star_bar = np.sum(n[:, np.newaxis] * m_star, axis=0) / total_n_global\n        \n        # Calculate squared Euclidean distance\n        distance_sq = np.sum((m_final - m_star_bar)**2)\n        results.append(distance_sq)\n\n    # Format output as required\n    output_str = f\"[{','.join([f'{r:.6f}' for r in results])}]\"\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}