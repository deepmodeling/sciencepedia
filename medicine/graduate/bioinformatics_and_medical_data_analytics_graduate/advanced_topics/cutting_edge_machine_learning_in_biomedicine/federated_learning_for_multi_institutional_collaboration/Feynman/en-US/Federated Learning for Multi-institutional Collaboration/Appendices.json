{
    "hands_on_practices": [
        {
            "introduction": "Deploying a federated learning system across multiple institutions requires a practical understanding of its performance characteristics. This first exercise provides a quantitative framework for estimating total training time and communication overhead in a synchronous FL setting. By modeling a realistic scenario that includes a 'straggler' site with limited network bandwidth, you will learn to identify and calculate the impact of the most significant bottlenecks in distributed machine learning .",
            "id": "4318598",
            "problem": "A consortium of hospital biobanks is training a disease risk prediction model using Federated Learning (FL), specifically synchronous Federated Averaging (FedAvg), to comply with data minimization and non-transfer policies mandated by the governing Institutional Review Board (IRB). There are $N=20$ biobank sites. In each federated round, the coordinating server broadcasts the current model parameters to all sites, each site performs local training, and then each site uploads a compressed set of updated model parameters to the server. Governance requires inclusion of all sites per round to preserve representativeness and fairness.\n\nAssume the following scientifically grounded and realistic conditions:\n- The uncompressed model size is $M=50$ megabytes. Use decimal megabytes where $1\\,\\mathrm{MB}=10^{6}$ bytes.\n- A lossless compression scheme applied to both downlink and uplink payloads yields a compression ratio $r=0.2$, so each transmitted payload per site per direction is $rM$.\n- Network transport uses point-to-point links; broadcasting the model to $N$ sites results in $N$ separate transfers of size $rM$ (no multicast aggregation).\n- Per round, all sites must finish the downlink, local computation, and uplink phases before the round completes (synchronous barrier).\n- Nineteen sites have downlink bandwidth $100\\,\\mathrm{Mb/s}$ and uplink bandwidth $100\\,\\mathrm{Mb/s}$ with one-way latencies of $0.05\\,\\mathrm{s}$ (downlink) and $0.10\\,\\mathrm{s}$ (uplink). One straggler site has downlink bandwidth $20\\,\\mathrm{Mb/s}$ and uplink bandwidth $20\\,\\mathrm{Mb/s}$ with the same latencies. Use decimal megabits where $1\\,\\mathrm{Mb}=10^{6}$ bits.\n- Each site’s local training time per round is $30\\,\\mathrm{s}$, assumed equal across sites and independent of network conditions.\n- Ignore server-side aggregation time and cryptographic protocol overhead beyond the stated payload sizes and latencies.\n\nStarting from core definitions of synchronous FL round execution and the relation between data size, bandwidth, and latency, do the following:\n1. Calculate the per-round communication overhead as the total data volume transmitted across all links (both directions combined) in megabytes.\n2. Estimate the total wall-clock training time over $T$ rounds in hours, expressed as a function of $T$, using the synchronous barrier assumption that the round duration is set by the slowest site for the downlink and the slowest site for the uplink.\n\nRound your answers to four significant figures. For part 1, express the data volume in megabytes. For part 2, express the total time in hours.",
            "solution": "**Problem Validation**\n\nThe provided problem statement is analyzed against the required criteria for validity.\n\n**Step 1: Extract Givens**\n- Number of sites: $N=20$\n- Federated Learning model: Synchronous Federated Averaging (FedAvg)\n- Governance requirement: Inclusion of all sites per round\n- Uncompressed model size: $M=50\\,\\mathrm{MB}$ ($1\\,\\mathrm{MB}=10^6$ bytes)\n- Compression ratio: $r=0.2$\n- Network transport: Point-to-point links\n- Round completion condition: Synchronous barrier for downlink, local computation, and uplink phases\n- Number of normal sites: $19$\n- Normal site bandwidths: $B_{\\text{down,normal}} = B_{\\text{up,normal}} = 100\\,\\mathrm{Mb/s}$ ($1\\,\\mathrm{Mb}=10^6$ bits)\n- Normal site one-way latencies: $L_{\\text{down,normal}} = 0.05\\,\\mathrm{s}$, $L_{\\text{up,normal}} = 0.10\\,\\mathrm{s}$\n- Number of straggler sites: $1$\n- Straggler site bandwidths: $B_{\\text{down,straggler}} = B_{\\text{up,straggler}} = 20\\,\\mathrm{Mb/s}$\n- Straggler site one-way latencies: $L_{\\text{down,straggler}} = 0.05\\,\\mathrm{s}$, $L_{\\text{up,straggler}} = 0.10\\,\\mathrm{s}$\n- Local training time per round for each site: $T_{\\text{local}}=30\\,\\mathrm{s}$\n- Total number of rounds: $T$\n- Ignored factors: Server-side aggregation time, cryptographic protocol overhead\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is evaluated as follows:\n- **Scientifically Grounded**: The problem is built upon established principles of distributed machine learning (Federated Learning) and computer networking. Concepts such as synchronous rounds, communication bottlenecks caused by straggler nodes, and the modeling of data transfer time using bandwidth and latency are standard and scientifically sound. The provided numerical values are realistic for a multi-institutional research setting.\n- **Well-Posed**: The problem is clearly specified, providing all necessary parameters and constraints to arrive at a unique, meaningful solution for the two requested quantities.\n- **Objective**: The problem is stated in precise, quantitative, and unbiased technical language, free from subjective or speculative claims.\n- **Completeness and Consistency**: The problem is self-contained and free of contradictions. The number of normal sites ($19$) and straggler sites ($1$) correctly sums to the total number of sites ($N=20$).\n\n**Step 3: Verdict and Action**\nThe problem is deemed valid. A complete solution is provided below.\n\n---\n\n**Part 1: Per-round communication overhead**\n\nThe total communication overhead per round, $V_{\\text{total}}$, is the sum of the total data volume transmitted from the server to all sites (downlink) and from all sites to the server (uplink).\n\nFirst, we determine the size of the compressed model payload, $S$, which is transmitted over each link.\n$$S = rM$$\nGiven the uncompressed model size $M=50\\,\\mathrm{MB}$ and the compression ratio $r=0.2$:\n$$S = 0.2 \\times 50\\,\\mathrm{MB} = 10\\,\\mathrm{MB}$$\nIn the downlink phase, the server sends this payload to each of the $N=20$ sites. Since the links are point-to-point, the total downlink volume, $V_{\\text{down}}$, is:\n$$V_{\\text{down}} = N \\times S = 20 \\times 10\\,\\mathrm{MB} = 200\\,\\mathrm{MB}$$\nIn the uplink phase, each of the $N=20$ sites sends its updated (and compressed) model parameters back to the server. The total uplink volume, $V_{\\text{up}}$, is therefore identical:\n$$V_{\\text{up}} = N \\times S = 20 \\times 10\\,\\mathrm{MB} = 200\\,\\mathrm{MB}$$\nThe total per-round communication overhead is the sum of the downlink and uplink volumes:\n$$V_{\\text{total}} = V_{\\text{down}} + V_{\\text{up}} = 200\\,\\mathrm{MB} + 200\\,\\mathrm{MB} = 400\\,\\mathrm{MB}$$\nThe problem asks for the answer to four significant figures. Thus, the total overhead is $400.0\\,\\mathrm{MB}$.\n\n**Part 2: Total wall-clock training time over $T$ rounds**\n\nThe total training time, $T_{\\text{total}}$, for $T$ rounds is the product of the number of rounds and the duration of a single round, $T_{\\text{round}}$.\n$$T_{\\text{total}}(T) = T \\times T_{\\text{round}}$$\nThe duration of a single synchronous round is the sum of the durations of its sequential phases: downlink, local computation, and uplink. The duration of each communication phase is determined by the slowest (straggler) site due to the synchronous barrier.\n\nFirst, we convert the payload size $S$ from megabytes to megabits to be consistent with the bandwidth units ($1\\,\\mathrm{Mb/s} = 10^6\\,\\mathrm{bits/s}$). Using the conversion $1\\,\\mathrm{byte} = 8\\,\\mathrm{bits}$:\n$$S = 10\\,\\mathrm{MB} = 10 \\times 10^6\\,\\mathrm{bytes} = 10 \\times 10^6 \\times 8\\,\\mathrm{bits} = 80 \\times 10^6\\,\\mathrm{bits} = 80\\,\\mathrm{Mb}$$\nThe time for a single data transfer is modeled as the sum of one-way latency ($L$) and serialization time ($S/B$, where $B$ is bandwidth).\n\n**Downlink Phase Duration ($T_{\\text{down\\_phase}}$)**\nThis is the time until the last site receives the model update. We must compare the total downlink time for a normal site and the straggler.\n- For a normal site:\n$$T_{\\text{down,normal}} = L_{\\text{down,normal}} + \\frac{S}{B_{\\text{down,normal}}} = 0.05\\,\\mathrm{s} + \\frac{80\\,\\mathrm{Mb}}{100\\,\\mathrm{Mb/s}} = 0.05\\,\\mathrm{s} + 0.8\\,\\mathrm{s} = 0.85\\,\\mathrm{s}$$\n- For the straggler site:\n$$T_{\\text{down,straggler}} = L_{\\text{down,straggler}} + \\frac{S}{B_{\\text{down,straggler}}} = 0.05\\,\\mathrm{s} + \\frac{80\\,\\mathrm{Mb}}{20\\,\\mathrm{Mb/s}} = 0.05\\,\\mathrm{s} + 4.0\\,\\mathrm{s} = 4.05\\,\\mathrm{s}$$\nThe phase duration is the maximum of these times:\n$$T_{\\text{down\\_phase}} = \\max(T_{\\text{down,normal}}, T_{\\text{down,straggler}}) = 4.05\\,\\mathrm{s}$$\n\n**Local Computation Phase Duration ($T_{\\text{local}}$)**\nThis is given as identical for all sites:\n$$T_{\\text{local}} = 30\\,\\mathrm{s}$$\n\n**Uplink Phase Duration ($T_{\\text{up\\_phase}}$)**\nThis is the time until the server receives the update from the last site.\n- For a normal site:\n$$T_{\\text{up,normal}} = L_{\\text{up,normal}} + \\frac{S}{B_{\\text{up,normal}}} = 0.10\\,\\mathrm{s} + \\frac{80\\,\\mathrm{Mb}}{100\\,\\mathrm{Mb/s}} = 0.10\\,\\mathrm{s} + 0.8\\,\\mathrm{s} = 0.90\\,\\mathrm{s}$$\n- For the straggler site:\n$$T_{\\text{up,straggler}} = L_{\\text{up,straggler}} + \\frac{S}{B_{\\text{up,straggler}}} = 0.10\\,\\mathrm{s} + \\frac{80\\,\\mathrm{Mb}}{20\\,\\mathrm{Mb/s}} = 0.10\\,\\mathrm{s} + 4.0\\,\\mathrm{s} = 4.10\\,\\mathrm{s}$$\nThe phase duration is the maximum of these times:\n$$T_{\\text{up\\_phase}} = \\max(T_{\\text{up,normal}}, T_{\\text{up,straggler}}) = 4.10\\,\\mathrm{s}$$\n\n**Total Round Duration ($T_{\\text{round}}$)**\nThe total time for one synchronous round is the sum of the durations of the three sequential phases:\n$$T_{\\text{round}} = T_{\\text{down\\_phase}} + T_{\\text{local}} + T_{\\text{up\\_phase}} = 4.05\\,\\mathrm{s} + 30\\,\\mathrm{s} + 4.10\\,\\mathrm{s} = 38.15\\,\\mathrm{s}$$\n\n**Total Training Time for $T$ Rounds**\nThe total time over $T$ rounds is expressed in hours. There are $3600\\,\\mathrm{s}$ in one hour.\n$$T_{\\text{total}}(T) = T \\times T_{\\text{round}} = T \\times 38.15\\,\\mathrm{s} \\times \\frac{1\\,\\text{hour}}{3600\\,\\mathrm{s}} = T \\times \\frac{38.15}{3600}\\,\\mathrm{hours}$$\n$$T_{\\text{total}}(T) \\approx T \\times 0.01059722...\\,\\mathrm{hours}$$\nRounding the coefficient to four significant figures yields $0.01060$.\n$$T_{\\text{total}}(T) \\approx 0.01060\\,T\\,\\mathrm{hours}$$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n400.0 & 0.01060T\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "One of the most critical challenges in federated learning is statistical heterogeneity, where data distributions differ significantly across collaborating institutions. Standard Federated Averaging (FedAvg) can struggle in such Non-Independent and Identically Distributed (Non-IID) settings, potentially leading to slower convergence or reduced model accuracy. This practice challenges you to move beyond simple data-size-based weighting by deriving an optimal aggregation strategy that minimizes the error between the federated and ideal centralized gradients, providing deep insight into the bias-variance trade-offs in FL .",
            "id": "4563886",
            "problem": "Consider two hospitals, $A$ and $B$, collaborating via Federated Averaging (FedAvg) to train a logistic regression model for a binary disease prediction task using Electronic Health Records (EHR). The data are Non-Independent and Identically Distributed (Non-IID) across hospitals due to differing patient demographics and comorbidity profiles, leading to statistical heterogeneity in local gradients.\n\nAt a particular training round, the server aggregates one-dimensional stochastic gradients from the two hospitals for a single model parameter using weights $w$ for hospital $A$ and $1-w$ for hospital $B$. Let the local stochastic gradients at this round be modeled as $g_A = \\mu_A + \\epsilon_A$ and $g_B = \\mu_B + \\epsilon_B$, where $\\epsilon_A$ and $\\epsilon_B$ are zero-mean random variables representing stochastic gradient noise, independent across hospitals, with variances $\\mathrm{Var}(\\epsilon_A) = \\sigma_A^2$ and $\\mathrm{Var}(\\epsilon_B) = \\sigma_B^2$. The expected local gradient means at this round are $\\mu_A$ and $\\mu_B$, respectively. The centralized gradient that would be obtained if all data were pooled is the data-proportion weighted mean $T = p_A \\mu_A + p_B \\mu_B$, where $p_A$ and $p_B$ are the fractions of total samples held by hospitals $A$ and $B$, respectively, and satisfy $p_A + p_B = 1$.\n\nThe server seeks to choose $w$ to minimize the expected squared deviation between the aggregated federated gradient and the centralized gradient at this round, that is, to minimize\n$$\n\\mathbb{E}\\left[\\left(w g_A + (1-w) g_B - T\\right)^{2}\\right].\n$$\n\nAssume independence of $\\epsilon_A$ and $\\epsilon_B$, unbiasedness of local stochastic gradients, and that $w$ is a real number satisfying $w + (1-w) = 1$. Use the following scientifically plausible values obtained at the current round from local gradient statistics:\n- $\\mu_A = 0.14$, $\\mu_B = -0.02$,\n- $\\sigma_A^2 = 0.0025$, $\\sigma_B^2 = 0.0064$,\n- $p_A = 0.65$, $p_B = 0.35$.\n\nStarting from the above definitions and assumptions, derive the expression for the expected squared deviation, and find the value of $w$ that minimizes it. Express the final answer as a single real number. Round your answer to four significant figures.",
            "solution": "The problem is well-posed, scientifically grounded in the principles of federated learning and optimization, and contains all necessary information for a unique solution. The numerical values provided are plausible within the context of training a machine learning model. Therefore, I will proceed with the derivation.\n\nThe objective is to find the weighting factor $w$ that minimizes the expected squared deviation between the aggregated federated gradient and the centralized gradient. The objective function is given by:\n$$\nL(w) = \\mathbb{E}\\left[\\left(w g_A + (1-w) g_B - T\\right)^{2}\\right]\n$$\nwhere $g_A$ and $g_B$ are stochastic gradients, and $T$ is the constant centralized gradient target.\n\nWe can decompose the expected squared error using the bias-variance decomposition. For any random variable $X$ and constant $c$, we have $\\mathbb{E}[(X-c)^2] = \\mathrm{Var}(X) + (\\mathbb{E}[X]-c)^2$. Let $X = w g_A + (1-w) g_B$ and $c = T$. The objective function becomes:\n$$\nL(w) = \\mathrm{Var}\\left(w g_A + (1-w) g_B\\right) + \\left(\\mathbb{E}\\left[w g_A + (1-w) g_B\\right] - T\\right)^2\n$$\nWe will analyze the variance and bias terms separately.\n\nFirst, let's compute the variance term. The local stochastic gradients are given as $g_A = \\mu_A + \\epsilon_A$ and $g_B = \\mu_B + \\epsilon_B$. Since $\\mu_A$ and $\\mu_B$ are constants, the variance of the local gradients is:\n$$\n\\mathrm{Var}(g_A) = \\mathrm{Var}(\\mu_A + \\epsilon_A) = \\mathrm{Var}(\\epsilon_A) = \\sigma_A^2\n$$\n$$\n\\mathrm{Var}(g_B) = \\mathrm{Var}(\\mu_B + \\epsilon_B) = \\mathrm{Var}(\\epsilon_B) = \\sigma_B^2\n$$\nThe problem states that the stochastic noise terms $\\epsilon_A$ and $\\epsilon_B$ are independent, which implies that the stochastic gradients $g_A$ and $g_B$ are also independent. For independent random variables, the variance of a weighted sum is the weighted sum of their variances:\n$$\n\\mathrm{Var}\\left(w g_A + (1-w) g_B\\right) = w^2 \\mathrm{Var}(g_A) + (1-w)^2 \\mathrm{Var}(g_B) = w^2 \\sigma_A^2 + (1-w)^2 \\sigma_B^2\n$$\n\nNext, let's compute the squared bias term, $(\\mathbb{E}[w g_A + (1-w) g_B] - T)^2$. We first find the expectation of the aggregated gradient. Using the linearity of expectation and that $\\mathbb{E}[\\epsilon_A] = 0$ and $\\mathbb{E}[\\epsilon_B] = 0$:\n$$\n\\mathbb{E}[g_A] = \\mathbb{E}[\\mu_A + \\epsilon_A] = \\mu_A\n$$\n$$\n\\mathbb{E}[g_B] = \\mathbb{E}[\\mu_B + \\epsilon_B] = \\mu_B\n$$\nThe expectation of the aggregated gradient is:\n$$\n\\mathbb{E}[w g_A + (1-w) g_B] = w \\mathbb{E}[g_A] + (1-w) \\mathbb{E}[g_B] = w \\mu_A + (1-w) \\mu_B\n$$\nThe centralized gradient is $T = p_A \\mu_A + p_B \\mu_B$. Since $p_A + p_B = 1$, we have $p_B = 1 - p_A$. The deviation from the target is:\n$$\n\\mathbb{E}[w g_A + (1-w) g_B] - T = (w \\mu_A + (1-w) \\mu_B) - (p_A \\mu_A + (1-p_A) \\mu_B)\n$$\n$$\n= (w - p_A)\\mu_A + ((1-w) - (1-p_A))\\mu_B = (w - p_A)\\mu_A - (w - p_A)\\mu_B = (w - p_A)(\\mu_A - \\mu_B)\n$$\nThe squared bias term is therefore:\n$$\n\\left((w-p_A)(\\mu_A - \\mu_B)\\right)^2 = (w-p_A)^2 (\\mu_A - \\mu_B)^2\n$$\n\nCombining the variance and squared bias terms, we get the full objective function $L(w)$:\n$$\nL(w) = w^2 \\sigma_A^2 + (1-w)^2 \\sigma_B^2 + (w-p_A)^2 (\\mu_A - \\mu_B)^2\n$$\nTo find the value of $w$ that minimizes $L(w)$, we compute the derivative with respect to $w$ and set it to $0$.\n$$\n\\frac{dL}{dw} = \\frac{d}{dw} \\left[ w^2 \\sigma_A^2 + (1-w)^2 \\sigma_B^2 + (w-p_A)^2 (\\mu_A - \\mu_B)^2 \\right]\n$$\n$$\n\\frac{dL}{dw} = 2w \\sigma_A^2 + 2(1-w)(-1) \\sigma_B^2 + 2(w-p_A) (\\mu_A - \\mu_B)^2 = 0\n$$\nDividing by $2$:\n$$\nw \\sigma_A^2 - (1-w) \\sigma_B^2 + (w-p_A) (\\mu_A - \\mu_B)^2 = 0\n$$\nWe expand and collect terms containing $w$:\n$$\nw \\sigma_A^2 - \\sigma_B^2 + w \\sigma_B^2 + w (\\mu_A - \\mu_B)^2 - p_A (\\mu_A - \\mu_B)^2 = 0\n$$\n$$\nw (\\sigma_A^2 + \\sigma_B^2 + (\\mu_A - \\mu_B)^2) = \\sigma_B^2 + p_A (\\mu_A - \\mu_B)^2\n$$\nSolving for $w$:\n$$\nw = \\frac{\\sigma_B^2 + p_A (\\mu_A - \\mu_B)^2}{\\sigma_A^2 + \\sigma_B^2 + (\\mu_A - \\mu_B)^2}\n$$\nThe second derivative is $\\frac{d^2L}{dw^2} = 2\\sigma_A^2 + 2\\sigma_B^2 + 2(\\mu_A - \\mu_B)^2$, which is strictly positive as the variances are positive, confirming that this value of $w$ corresponds to a minimum.\n\nNow, we substitute the given numerical values:\n- $\\mu_A = 0.14$\n- $\\mu_B = -0.02$\n- $\\sigma_A^2 = 0.0025$\n- $\\sigma_B^2 = 0.0064$\n- $p_A = 0.65$\n\nFirst, we calculate the term related to the divergence of the expected gradients:\n$$\n\\mu_A - \\mu_B = 0.14 - (-0.02) = 0.16\n$$\n$$\n(\\mu_A - \\mu_B)^2 = (0.16)^2 = 0.0256\n$$\nNow, substitute these values into the expression for $w$.\nThe numerator is:\n$$\n\\sigma_B^2 + p_A (\\mu_A - \\mu_B)^2 = 0.0064 + (0.65)(0.0256) = 0.0064 + 0.01664 = 0.02304\n$$\nThe denominator is:\n$$\n\\sigma_A^2 + \\sigma_B^2 + (\\mu_A - \\mu_B)^2 = 0.0025 + 0.0064 + 0.0256 = 0.0345\n$$\nSo, the optimal value of $w$ is:\n$$\nw = \\frac{0.02304}{0.0345} \\approx 0.66782608...\n$$\nRounding the result to four significant figures, we get $w = 0.6678$.",
            "answer": "$$\n\\boxed{0.6678}\n$$"
        },
        {
            "introduction": "While theoretical analysis provides valuable insights, a complete understanding of federated learning comes from implementing the orchestration logic that governs the system. This final hands-on practice guides you through building a deterministic simulator for a synchronous FL workflow, emulating modern frameworks like Flower or NVFLARE. You will implement the full round-trip of a model update, from client-side gradient calculation and processing—including clipping and quantization—to server-side aggregation, thereby solidifying your grasp of the end-to-end mechanics of a federated system .",
            "id": "4563870",
            "problem": "You are tasked with implementing a minimal, deterministic orchestration simulator for Federated Learning (FL) that emulates core semantics of modern orchestration frameworks such as Flower and NVIDIA Federated Learning Application Runtime Environment (NVFLARE), focusing on multi-institutional collaboration scenarios in bioinformatics and medical data analytics. The simulator must execute round-based synchronous coordination, where multiple institutions (clients) conduct local training on site-specific data and submit updates to a central coordinator (server), which then aggregates accepted updates to produce a new global model. Your program must be fully deterministic and must use the provided test suite exactly as specified.\n\nStart from the following fundamental base:\n- Empirical risk minimization (ERM) and gradient-based optimization: a client with a twice-differentiable, strongly convex local objective $f_i(\\mathbf{m})$ has gradient descent step $\\mathbf{m} \\leftarrow \\mathbf{m} - \\eta \\nabla f_i(\\mathbf{m})$ for step size $\\eta > 0$.\n- Quadratic approximation near a local minimum $\\mathbf{m}_i^\\star$: for client $i$, near $\\mathbf{m}_i^\\star$, the gradient can be modeled as $\\nabla f_i(\\mathbf{m}) \\approx h_i \\left(\\mathbf{m} - \\mathbf{m}_i^\\star\\right)$ for scalar curvature $h_i > 0$ (assuming isotropic curvature for simplicity).\n- Federated Averaging (FedAvg): given a set of accepted clients $A^t$ in round $t$, with client data sizes $n_i$, the global model update is computed as a data-size-weighted average of client deltas.\n\nDefine the model dimension $d \\in \\{2\\}$ for this problem and the global model $\\mathbf{m}^t \\in \\mathbb{R}^d$ at round $t \\in \\{0,1,\\dots,R\\}$. For each client $i \\in \\{1,2,\\dots,K\\}$, define:\n- Local data size $n_i \\in \\mathbb{N}$,\n- Curvature scalar $h_i \\in \\mathbb{R}_{>0}$,\n- Local optimum $\\mathbf{m}_i^\\star \\in \\mathbb{R}^d$,\n- Participation indicator $s_i^t \\in \\{0,1\\}$,\n- Communication delay $d_i^t$ in seconds, with a round timeout $\\tau$ in seconds.\n\nIn round $t$, each participating client with $s_i^t = 1$ computes a single local gradient descent step from the broadcasted global model $\\mathbf{m}^t$:\n$$\n\\Delta_i^t \\equiv \\mathbf{m}_i^{t,\\text{local}} - \\mathbf{m}^t = -\\eta \\nabla f_i(\\mathbf{m}^t) \\approx -\\eta\\, h_i \\left(\\mathbf{m}^t - \\mathbf{m}_i^\\star\\right),\n$$\nthen applies per-client update clipping and quantization before sending the update to the server:\n1. L2 clipping with bound $C > 0$:\n$$\n\\widetilde{\\Delta}_i^t = \n\\begin{cases}\n\\Delta_i^t & \\text{if } \\lVert \\Delta_i^t \\rVert_2 \\le C, \\\\\n\\dfrac{C}{\\lVert \\Delta_i^t \\rVert_2}\\, \\Delta_i^t & \\text{otherwise.}\n\\end{cases}\n$$\n2. Uniform per-coordinate quantization with $b \\in \\mathbb{N}$ bits, using step size $2^{-b}$:\n$$\nQ_b(x) = \\dfrac{\\operatorname{round}\\!\\left(x \\cdot 2^b\\right)}{2^b}, \\quad \\widehat{\\Delta}_i^t = \\left(Q_b\\left(\\widetilde{\\Delta}_{i,1}^t\\right), Q_b\\left(\\widetilde{\\Delta}_{i,2}^t\\right)\\right).\n$$\nThe server accepts an update if and only if $s_i^t = 1$ and $d_i^t \\le \\tau$. Denote the accepted set $A^t = \\left\\{ i \\mid s_i^t = 1 \\text{ and } d_i^t \\le \\tau \\right\\}$. The server update is:\n$$\n\\mathbf{m}^{t+1} =\n\\begin{cases}\n\\mathbf{m}^{t} + \\displaystyle\\sum_{i \\in A^t} \\left(\\dfrac{n_i}{\\sum_{j \\in A^t} n_j}\\right) \\widehat{\\Delta}_i^t, & \\text{if } \\left|A^t\\right| \\ge 1, \\\\\n\\mathbf{m}^{t}, & \\text{if } \\left|A^t\\right| = 0.\n\\end{cases}\n$$\n\nAfter $R$ rounds, evaluate how close the global model is to the global data-size-weighted target\n$$\n\\overline{\\mathbf{m}}^\\star = \\dfrac{\\sum_{i=1}^K n_i \\mathbf{m}_i^\\star}{\\sum_{i=1}^K n_i}.\n$$\nReport the squared Euclidean distance\n$$\nD = \\left\\lVert \\mathbf{m}^R - \\overline{\\mathbf{m}}^\\star \\right\\rVert_2^2.\n$$\n\nYour program must implement the above exactly and produce results for the following test suite. All numbers are real-valued unless otherwise stated, delays and timeout are in seconds, and vectors are two-dimensional. There is no randomness; you must use the specified values as-is.\n\nTest Case $1$ (general case with timeouts and quantization, no clipping):\n- $K = 4$, $d = 2$, $R = 3$.\n- $\\eta = 0.2$, $C = 10^9$, $b = 8$, $\\tau = 0.9$.\n- $\\mathbf{m}^0 = [0, 0]$.\n- Data sizes $[n_1,n_2,n_3,n_4] = [50, 30, 10, 10]$.\n- Curvatures $[h_1,h_2,h_3,h_4] = [1.0, 0.5, 1.5, 1.0]$.\n- Optima $\\mathbf{m}_1^\\star = [1, -1]$, $\\mathbf{m}_2^\\star = [0.5, -0.5]$, $\\mathbf{m}_3^\\star = [2, -2]$, $\\mathbf{m}_4^\\star = [1, -1]$.\n- Participation matrix $s_i^t = 1$ for all $i \\in \\{1,2,3,4\\}$ and $t \\in \\{0,1,2\\}$.\n- Delays matrix (rows are rounds $t = 0,1,2$; columns are clients $i = 1,2,3,4$):\n  - Round $0$: $[0.2, 0.5, 1.2, 0.1]$,\n  - Round $1$: $[0.3, 1.1, 0.2, 0.4]$,\n  - Round $2$: $[0.8, 0.7, 0.6, 1.5]$.\n\nTest Case $2$ (boundary case: no accepted updates due to zero timeout):\n- $K = 3$, $d = 2$, $R = 5$.\n- $\\eta = 0.1$, $C = 10^9$, $b = 8$, $\\tau = 0.0$.\n- $\\mathbf{m}^0 = [-1, -1]$.\n- Data sizes $[n_1,n_2,n_3] = [20, 40, 40]$.\n- Curvatures $[h_1,h_2,h_3] = [1.0, 1.0, 1.0]$.\n- Optima $\\mathbf{m}_1^\\star = [1, 0]$, $\\mathbf{m}_2^\\star = [0, 1]$, $\\mathbf{m}_3^\\star = [1, 1]$.\n- Participation matrix $s_i^t = 1$ for all $i \\in \\{1,2,3\\}$ and $t \\in \\{0,1,2,3,4\\}$.\n- Delays matrix: every entry equals $0.1$.\n\nTest Case $3$ (edge case: strong clipping and coarse quantization):\n- $K = 3$, $d = 2$, $R = 2$.\n- $\\eta = 0.5$, $C = 0.4$, $b = 2$, $\\tau = 10.0$.\n- $\\mathbf{m}^0 = [0, 0]$.\n- Data sizes $[n_1,n_2,n_3] = [100, 50, 50]$.\n- Curvatures $[h_1,h_2,h_3] = [5.0, 2.0, 1.0]$.\n- Optima $\\mathbf{m}_1^\\star = [1, 1]$, $\\mathbf{m}_2^\\star = [-1, -1]$, $\\mathbf{m}_3^\\star = [2, 0]$.\n- Participation matrix $s_i^t = 1$ for all $i \\in \\{1,2,3\\}$ and $t \\in \\{0,1\\}$.\n- Delays matrix: every entry equals $0.0$.\n\nRequired final output format:\n- For each test case, compute $D = \\left\\lVert \\mathbf{m}^R - \\overline{\\mathbf{m}}^\\star \\right\\rVert_2^2$.\n- Your program should produce a single line of output containing the three results as a comma-separated list enclosed in square brackets, with each float rounded to exactly six digits after the decimal point, for example: $[0.123456,0.000000,1.500000]$.\n\nAll computations must follow the definitions above exactly. Angles are not used. Time units must be treated as seconds. Percentages are not used; any fractional quantities must be represented as decimals.",
            "solution": "The problem requires the implementation of a deterministic simulator for a synchronous, round-based Federated Learning (FL) orchestration protocol. The simulation follows the principles of federated optimization, where a central server coordinates the training process across multiple clients (e.g., medical institutions) without accessing their raw data. The core of the problem is to trace the evolution of a global model vector, $\\mathbf{m}^t \\in \\mathbb{R}^d$, over a series of $R$ communication rounds.\n\nThe simulation adheres to the following sequence of operations for each round $t \\in \\{0, 1, \\dots, R-1\\}$.\n\n**1. Client Selection and Participation**\n\nIn each round $t$, a subset of clients is selected to participate in training. The problem specifies a participation matrix $s_i^t$, where $s_i^t=1$ indicates that client $i$ is chosen for the round. The server broadcasts the current global model $\\mathbf{m}^t$ to these participating clients. After performing local computations, each client sends its update back. However, due to network latency, updates might not arrive in time. A round has a strict timeout $\\tau$. An update from client $i$ is only considered for aggregation if its communication delay $d_i^t$ does not exceed the timeout $\\tau$. Thus, the set of clients whose updates are accepted by the server in round $t$, denoted as $A^t$, is formally defined as:\n$$\nA^t = \\left\\{ i \\mid s_i^t = 1 \\text{ and } d_i^t \\le \\tau \\right\\}\n$$\n\n**2. Local Client Update**\n\nEach participating client $i \\in A^t$ performs a local computation to generate an update. The problem simplifies the local training process to a single step of gradient descent on the client's local objective function $f_i(\\mathbf{m})$. The objective function $f_i$ is assumed to be twice-differentiable and strongly convex. Near its local minimum $\\mathbf{m}_i^\\star$, the gradient $\\nabla f_i(\\mathbf{m})$ is approximated by a linear function:\n$$\n\\nabla f_i(\\mathbf{m}) \\approx h_i \\left(\\mathbf{m} - \\mathbf{m}_i^\\star\\right)\n$$\nwhere $h_i > 0$ is a scalar representing the (isotropic) curvature of the loss surface. Using this approximation, a client starting with the global model $\\mathbf{m}^t$ calculates its local model update, $\\Delta_i^t$, as follows:\n$$\n\\Delta_i^t = -\\eta \\nabla f_i(\\mathbf{m}^t) = -\\eta\\, h_i \\left(\\mathbf{m}^t - \\mathbf{m}_i^\\star\\right)\n$$\nHere, $\\eta > 0$ is the learning rate or step size.\n\n**3. Update Clipping and Quantization**\n\nBefore transmission, each client processes its calculated update $\\Delta_i^t$ to manage communication bandwidth and improve robustness.\n\nFirst, L2 norm clipping is applied. The magnitude of the update vector is capped at a threshold $C > 0$. This prevents excessively large updates from a single client from destabilizing the global model. The clipped update, $\\widetilde{\\Delta}_i^t$, is:\n$$\n\\widetilde{\\Delta}_i^t = \n\\begin{cases}\n\\Delta_i^t & \\text{if } \\lVert \\Delta_i^t \\rVert_2 \\le C \\\\\n\\dfrac{C}{\\lVert \\Delta_i^t \\rVert_2}\\, \\Delta_i^t & \\text{if } \\lVert \\Delta_i^t \\rVert_2 > C\n\\end{cases}\n$$\n\nSecond, the clipped update is quantized to reduce its size for transmission. A uniform quantization scheme with $b$ bits is applied to each coordinate of the vector. The quantization function $Q_b(x)$ maps a real number $x$ to a discrete value:\n$$\nQ_b(x) = \\dfrac{\\operatorname{round}\\!\\left(x \\cdot 2^b\\right)}{2^b}\n$$\nThe final update prepared for transmission by client $i$ is $\\widehat{\\Delta}_i^t = \\left(Q_b\\left(\\widetilde{\\Delta}_{i,1}^t\\right), Q_b\\left(\\widetilde{\\Delta}_{i,2}^t\\right), \\dots, Q_b\\left(\\widetilde{\\Delta}_{i,d}^t\\right)\\right)$.\n\n**4. Server-side Aggregation**\n\nThe server collects the processed updates $\\widehat{\\Delta}_i^t$ from all clients in the accepted set $A^t$. If this set is not empty ($|A^t| \\ge 1$), the server aggregates these updates to form the next global model, $\\mathbf{m}^{t+1}$. The aggregation is performed using the Federated Averaging (FedAvg) algorithm, where each client's update is weighted by its relative data size $n_i$. The total data size of accepted clients is $N_t = \\sum_{j \\in A^t} n_j$. The update rule is:\n$$\n\\mathbf{m}^{t+1} = \\mathbf{m}^{t} + \\sum_{i \\in A^t} \\left(\\dfrac{n_i}{N_t}\\right) \\widehat{\\Delta}_i^t\n$$\nIf the accepted set $A^t$ is empty ($|A^t| = 0$), no updates are received or the timeout is too strict, and the global model remains unchanged:\n$$\n\\mathbf{m}^{t+1} = \\mathbf{m}^{t}\n$$\nThis iterative process is repeated for $R$ rounds, starting with an initial model $\\mathbf{m}^0$.\n\n**5. Final Evaluation**\n\nAfter $R$ rounds, the quality of the final global model, $\\mathbf{m}^R$, is evaluated. The benchmark for comparison is the ideal global optimum, $\\overline{\\mathbf{m}}^\\star$, which is the data-size-weighted average of all clients' local optima:\n$$\n\\overline{\\mathbf{m}}^\\star = \\dfrac{\\sum_{i=1}^K n_i \\mathbf{m}_i^\\star}{\\sum_{i=1}^K n_i}\n$$\nThe performance metric is the squared Euclidean distance, $D$, between the final model and this ideal target:\n$$\nD = \\left\\lVert \\mathbf{m}^R - \\overline{\\mathbf{m}}^\\star \\right\\rVert_2^2\n$$\nThe simulation is executed for each provided test case, and the resulting value of $D$ is reported.",
            "answer": "[0.407759,5.890000,0.281875]"
        }
    ]
}