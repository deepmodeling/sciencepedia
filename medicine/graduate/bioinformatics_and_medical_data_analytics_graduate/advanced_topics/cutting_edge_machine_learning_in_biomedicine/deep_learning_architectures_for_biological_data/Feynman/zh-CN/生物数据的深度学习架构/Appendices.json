{
    "hands_on_practices": [
        {
            "introduction": "卷积神经网络（CNNs）是识别如DNA等序列数据中模式的强大工具。本练习旨在建立抽象的CNN参数（如卷积核大小和步幅）与具体的生物学任务（检测转录因子结合基序）之间的桥梁。通过这个实践 ，您将学习如何评估架构选择对基因组数据分析模型性能的实际影响，从而培养对模型设计的直觉。",
            "id": "4553876",
            "problem": "您正在设计卷积神经网络（CNN, Convolutional Neural Network）中的第一层一维卷积，用于检测沿通道进行 one-hot 编码的脱氧核糖核酸（DNA, Deoxyribonucleic Acid）序列中的转录因子结合基序。考虑一个长度为 $L_{\\mathrm{in}} = 1{,}000$ 个碱基的输入序列。该卷积层使用长度为 $k = 15$ 的卷积核，步幅 $s = 3$，在两端各有 $p = 2$ 个碱基的对称零填充，以及膨胀率 $d = 1$。\n\n从带步幅和填充的离散一维卷积在填充后输入上的定义出发，推导出一个通用表达式，该表达式通过计算有效卷积核对齐的数量来确定输出位置的数量。利用此推理过程，计算给定参数下的输出序列长度 $L_{\\mathrm{out}}$。然后，基于感受野和步幅引起的采样概念，分析此配置在检测跨度为 $M = 20$ 个碱基的基序方面的能力，特别是在单次滤波器应用中的完全覆盖、位置敏感性以及序列边界附近的检测方面。\n\n将输出长度 $L_{\\mathrm{out}}$ 报告为单个整数，不带单位。无需四舍五入。您的定性分析应包含在推理过程中，但最终答案必须仅为计算出的 $L_{\\mathrm{out}}$。",
            "solution": "用户要求推导一维卷积层的输出长度，并对该层在生物信息学特定应用中的特性进行定性分析。本解答将首先建立输出维度的一般公式，然后将其应用于给定参数，最后分析该配置的意义。\n\n**问题验证**\n\n在继续之前，对问题陈述进行严格验证。\n1.  **提取的已知条件**：\n    -   输入序列长度：$L_{\\mathrm{in}} = 1{,}000$ 个碱基。\n    -   卷积核长度：$k = 15$。\n    -   步幅：$s = 3$。\n    -   对称零填充：两端各 $p = 2$。\n    -   膨胀率：$d = 1$。\n    -   用于分析的基序长度：$M = 20$ 个碱基。\n\n2.  **验证结论**：该问题是**有效的**。它在科学上基于深度学习（特别是卷积神经网络）的标准原理，并应用于成熟的生物信息学领域进行基因组分析。该问题提法恰当，提供了一套完整且一致的参数（$L_{\\mathrm{in}}, k, s, p, d$），可以从中确定输出长度（$L_{\\mathrm{out}}$）的唯一且有意义的解。语言客观，并采用了标准术语。参数对于所描述的任务是符合实际的。\n\n**输出长度通用表达式的推导**\n\n一维卷积层的输出位置数量，即输出序列长度 $L_{\\mathrm{out}}$，由卷积核沿输入序列的有效放置次数决定。这可以从第一性原理推导得出。\n\n1.  一个长度为 $L_{\\mathrm{in}}$ 的输入序列首先经过对称填充，在其两端各添加 $p$ 个元素。这个新的、填充后的序列的长度，表示为 $L_{\\mathrm{padded}}$，是：\n    $$L_{\\mathrm{padded}} = L_{\\mathrm{in}} + 2p$$\n\n2.  长度为 $k$ 的卷积核可能会被膨胀。膨胀因子 $d$ 在连续的卷积核元素之间引入 $d-1$ 个间隙。因此，卷积核在输入上的有效空间范围，称为有效卷积核大小 $k_{\\mathrm{eff}}$，是：\n    $$k_{\\mathrm{eff}} = k + (k-1)(d-1)$$\n\n3.  卷积核在填充后的序列上移动，或称“步进”。每个位置都会计算一个输出。卷积核的第一次放置从填充后序列的索引 $0$ 开始。覆盖的范围是从索引 $0$ 到 $k_{\\mathrm{eff}}-1$。最后一次可能的放置也必须完全包含在填充后的序列内。卷积核的最后一个元素必须对齐到不大于 $L_{\\mathrm{padded}}-1$ 的索引。这意味着卷积核的最后一个起始位置是索引 $L_{\\mathrm{padded}} - k_{\\mathrm{eff}}$。\n\n4.  因此，可能的起始位置的总范围是从 $0$ 到 $L_{\\mathrm{padded}} - k_{\\mathrm{eff}}$，如果步幅为 $1$，这将包含 $L_{\\mathrm{padded}} - k_{\\mathrm{eff}} + 1$ 个可能的位置。\n\n5.  当步幅为 $s$ 时，卷积核不会占据每一个可能的位置。相反，它占据位置 $0, s, 2s, \\dots, Ns$，其中 $Ns$ 是最后一个有效的起始位置。这个最后的位置必须满足 $Ns \\le L_{\\mathrm{padded}} - k_{\\mathrm{eff}}$。因此，步数 $N$ 是满足此条件的最大整数，使用向下取整函数（floor function）求得：\n    $$N = \\left\\lfloor \\frac{L_{\\mathrm{padded}} - k_{\\mathrm{eff}}}{s} \\right\\rfloor$$\n    总输出位置数 $L_{\\mathrm{out}}$ 是这些放置的数量，即 $N+1$（包括在位置 $0$ 的初始放置）。\n\n6.  代入 $L_{\\mathrm{padded}}$ 和 $k_{\\mathrm{eff}}$ 的表达式，得到输出长度的通用公式：\n    $$L_{\\mathrm{out}} = \\left\\lfloor \\frac{(L_{\\mathrm{in}} + 2p) - (k + (k-1)(d-1))}{s} \\right\\rfloor + 1$$\n\n**根据给定参数计算 $L_{\\mathrm{out}}$**\n\n问题提供了以下参数：$L_{\\mathrm{in}} = 1{,}000$，$k = 15$，$s = 3$，$p = 2$，以及 $d = 1$。\n\n首先，我们将这些值代入推导出的通用表达式中。由于膨胀率 $d=1$，有效卷积核长度 $k_{\\mathrm{eff}}$ 就是 $k$：\n$$k_{\\mathrm{eff}} = 15 + (15-1)(1-1) = 15 + (14)(0) = 15$$\n$L_{\\mathrm{out}}$ 的公式简化为：\n$$L_{\\mathrm{out}} = \\left\\lfloor \\frac{L_{\\mathrm{in}} + 2p - k}{s} \\right\\rfloor + 1$$\n代入数值：\n$$L_{\\mathrm{out}} = \\left\\lfloor \\frac{1{,}000 + 2(2) - 15}{3} \\right\\rfloor + 1$$\n$$L_{\\mathrm{out}} = \\left\\lfloor \\frac{1{,}000 + 4 - 15}{3} \\right\\rfloor + 1$$\n$$L_{\\mathrm{out}} = \\left\\lfloor \\frac{989}{3} \\right\\rfloor + 1$$\n除法得到一个非整数值：\n$$\\frac{989}{3} \\approx 329.667$$\n应用向下取整函数，得到小于或等于此值的最大整数：\n$$\\lfloor 329.667 \\rfloor = 329$$\n最后，加 1 得到输出长度：\n$$L_{\\mathrm{out}} = 329 + 1 = 330$$\n\n**配置的定性分析**\n\n问题进一步要求分析此配置在检测长度为 $M = 20$ 个碱基的基序方面的能力。\n\n1.  **完全覆盖与感受野**：该卷积层中任何单个神经元的感受野是其有效卷积核大小，即 $k_{\\mathrm{eff}} = 15$ 个碱基。感兴趣的基序长度为 $M=20$ 个碱基。由于 $M > k_{\\mathrm{eff}}$，**没有任何单个滤波器应用能够看到整个基序**。滤波器最多只能学会识别不超过 15 个碱基的基序片段。要识别完整的 20 碱基基序，网络必须学会组合来自输出特征图中空间上相邻神经元的信息，这是后续层（例如，另一个卷积层或全连接层）的任务。\n\n2.  **位置敏感性与步幅**：步幅 $s=3$ 引入了对输入的下采样。这意味着卷积核每移动一步会“跳过”2 个碱基。因此，该层对于单碱基平移不具有平移等变性。一个从输入位置 $i$ 开始的基序，与一个从位置 $i+1$ 或 $i+2$ 开始的基序，将被卷积核以不同的方式看待。这种对模 $s$ 的位置移位的高度敏感性可能是有害的，因为基序位置的微小偏移可能导致其与卷积核的步幅错位，从而导致激活减弱并可能检测失败。\n\n3.  **序列边界附近的检测**：该层使用 $p=2$ 的对称填充。对于大小为 $k=15$ 的卷积核，这种填充不足以使卷积核在序列的极端两端对准基序中心。要将大小为 $k$ 的卷积核在第一个碱基（索引 $0$）上居中，需要 $p_{\\mathrm{center}} = \\lfloor k/2 \\rfloor = \\lfloor 15/2 \\rfloor = 7$ 的填充。而当 $p=2$ 时，第一个输出神经元的感受野中心位于原始序列位置 $7-2 = 5$ 处。这意味着序列最前面约 5 个和最后约 5 个碱基内的基序永远无法被置于卷积核视野的中心，这会削弱网络检测它们的能力。\n\n总之，由于卷积核尺寸过小，这个特定的层配置在结构上对于检测 20 碱基的基序存在局限性，并且其性能因大步幅和不充分的填充而进一步受损，这些因素影响了位置敏感性和边界检测。",
            "answer": "$$\\boxed{330}$$"
        },
        {
            "introduction": "生物学数据集，例如基因功能家族分类，常常存在严重的类别不平衡问题。本练习将探讨一种应对此挑战的系统性方法：通过重加权损失函数来调整模型的训练过程。您将从第一性原理出发，推导出如何设置类别权重，以确保在不平衡数据上计算的经验风险能够无偏地估计模型在均衡数据分布下的真实风险，这是一种源于重要性采样的强大技术 。",
            "id": "4553872",
            "problem": "一个研究团队正在使用Transformer架构训练一个深度分类器，用于从氨基酸序列中将基因分配到功能家族。训练集在不同功能家族之间表现出严重的类别不平衡。假设有 $C$ 个家族，索引为 $c \\in \\{1,\\dots,C\\}$，其类别计数为 $\\{n_c\\}$，总样本量为 $N = \\sum_{c=1}^{C} n_c$。该模型通过最小化经验交叉熵（CE）损失进行训练，该损失是每个样本CE项的样本均值。由于该团队的目标是在评估时模拟一个在所有家族上的均匀类别先验，他们决定通过一个仅取决于类别标签的恒定权重来对每个样本的CE项进行重新加权。\n\n从目标标签分布下的期望风险的定义出发，并根据一个要求——即在从观测标签分布中抽取的数据上计算的、依赖于标签的经验CE项重加权应是目标风险的无偏估计量——推导出每个类别的权重 $w_c$ 关于 $C$、$N$ 和 $n_c$ 的显式表达式。施加归一化条件，使得整个训练集上每个样本的平均权重等于 $1$，以保持损失的总体规模。\n\n然后，对于一个基因家族分类数据集，其中 $C = 5$，类别计数为\n$$\n\\{n_c\\}_{c=1}^{5} = \\{8000,\\ 2000,\\ 500,\\ 100,\\ 25\\},\n$$\n计算权重 $\\{w_c\\}_{c=1}^{5}$ 的数值。将最终答案表示为一个包含五个权重的单行矩阵，四舍五入到四位有效数字。不需要单位。",
            "solution": "该问题要求推导用于交叉熵损失函数的每类别权重，以纠正类别不平衡。其目标是使在具有非均匀类别分布的训练集上计算的经验风险，成为在均匀目标类别分布下的真实风险的无偏估计量。\n\n### 步骤1：形式化问题定义与权重表达式推导\n\n设 $(x, y)$ 是一个随机数据样本，其中 $x$ 是输入特征向量（氨基酸序列表示），$y \\in \\{1,\\dots,C\\}$ 是类别标签（功能家族）。由 $\\theta$ 参数化的模型输出类别上的概率分布 $p_\\theta(c|x)$，其中 $c \\in \\{1,\\dots,C\\}$。每个样本的交叉熵（CE）损失由 $L(x, y; \\theta) = -\\ln(p_\\theta(y|x))$ 给出。为简洁起见，我们将其表示为 $L(x, y)$。\n\n训练数据从一个观测数据分布 $P_{obs}(x, y)$ 中抽取。观测到类别 $c$ 的边际概率可以从类别计数 $\\{n_c\\}$ 和总样本量 $N = \\sum_{c=1}^{C} n_c$ 估计如下：\n$$\nP_{obs}(y=c) = \\frac{n_c}{N}\n$$\n\n评估时的目标是模拟一个均匀的类别先验。这定义了一个目标数据分布 $P_{target}(x, y)$，其每个类别的边际概率是均匀的：\n$$\nP_{target}(y=c) = \\frac{1}{C}\n$$\n在这种情况下（通过重要性加权进行领域自适应）的一个标准假设是，特征的类别条件分布 $P(x|y)$ 在观测域和目标域之间保持不变。因此，我们有：\n$$\nP_{obs}(x, y) = P(x|y) P_{obs}(y) \\quad \\text{和} \\quad P_{target}(x, y) = P(x|y) P_{target}(y)\n$$\n\n目标风险是关于目标分布的期望损失：\n$$\nR_{target} = \\mathbb{E}_{(x,y) \\sim P_{target}}[L(x,y)] = \\sum_{c=1}^{C} \\int_x L(x,c) P_{target}(x,c) dx\n$$\n代入 $P_{target}(x,c) = P(x|c) P_{target}(c)$，得到：\n$$\nR_{target} = \\sum_{c=1}^{C} P_{target}(c) \\int_x L(x,c) P(x|c) dx\n$$\n\n训练过程最小化一个经验风险 $\\hat{R}_{reweighted}$，该风险是CE项的样本均值，并由一个依赖于类别的常数 $w_c$ 进行重加权。对于一个从 $P_{obs}$ 中独立同分布抽取的训练集 $\\{(x_i, y_i)\\}_{i=1}^N$：\n$$\n\\hat{R}_{reweighted} = \\frac{1}{N} \\sum_{i=1}^{N} w_{y_i} L(x_i, y_i)\n$$\n\n核心要求是这个经验风险必须是目标风险的无偏估计量。这意味着 $\\hat{R}_{reweighted}$ 在训练集分布（即关于 $P_{obs}$）上的期望必须等于 $R_{target}$：\n$$\n\\mathbb{E}_{(x_i, y_i) \\sim P_{obs}}[\\hat{R}_{reweighted}] = R_{target}\n$$\n根据期望的线性和独立同分布假设，样本均值的期望等于单个项期望的均值：\n$$\n\\mathbb{E}[\\hat{R}_{reweighted}] = \\mathbb{E}_{(x,y) \\sim P_{obs}}[w_y L(x,y)]\n$$\n展开这个期望得到：\n$$\n\\mathbb{E}_{(x,y) \\sim P_{obs}}[w_y L(x,y)] = \\sum_{c=1}^{C} \\int_x w_c L(x,c) P_{obs}(x,c) dx\n$$\n代入 $P_{obs}(x,c) = P(x|c) P_{obs}(c)$：\n$$\n\\mathbb{E}_{(x,y) \\sim P_{obs}}[w_y L(x,y)] = \\sum_{c=1}^{C} w_c P_{obs}(c) \\int_x L(x,c) P(x|c) dx\n$$\n现在，我们将此表达式设为等于目标风险 $R_{target}$：\n$$\n\\sum_{c=1}^{C} w_c P_{obs}(c) \\int_x L(x,c) P(x|c) dx = \\sum_{c=1}^{C} P_{target}(c) \\int_x L(x,c) P(x|c) dx\n$$\n为使此等式对任何模型（即对任何函数 $L(x,c)$）都成立，每个类别 $c$ 的积分项系数必须相同。这导出了以下条件：\n$$\nw_c P_{obs}(c) = P_{target}(c)\n$$\n求解权重 $w_c$，我们得到基本的重要性权重表达式：\n$$\nw_c = \\frac{P_{target}(c)}{P_{obs}(c)}\n$$\n代入边际概率的表达式，我们发现：\n$$\nw_c = \\frac{1/C}{n_c/N} = \\frac{N}{C \\cdot n_c}\n$$\n这就是确保重加权经验风险是目标风险的无偏估计量的权重表达式。\n\n问题施加了一个额外的归一化约束：整个训练集上每个样本的平均权重必须等于 $1$。应用于 $N$ 个训练样本的总权重和为 $\\sum_{i=1}^N w_{y_i}$。由于类别 $c$ 有 $n_c$ 个样本，此和为 $\\sum_{c=1}^C n_c w_c$。平均权重为：\n$$\n\\text{Average Weight} = \\frac{1}{N} \\sum_{c=1}^C n_c w_c\n$$\n我们必须验证我们推导出的 $w_c$ 表达式是否满足此平均值等于 $1$ 的约束：\n$$\n\\frac{1}{N} \\sum_{c=1}^{C} n_c \\left(\\frac{N}{C \\cdot n_c}\\right) = \\frac{1}{N} \\sum_{c=1}^{C} \\frac{N}{C} = \\frac{1}{N} \\cdot C \\cdot \\frac{N}{C} = 1\n$$\n该约束无需任何进一步缩放即可满足。因此，每个类别的权重的最终表达式为：\n$$\nw_c = \\frac{N}{C \\cdot n_c}\n$$\n\n### 步骤2：数值计算\n\n给定数据集参数：\n- 类别数，$C = 5$。\n- 类别计数，$\\{n_c\\}_{c=1}^{5} = \\{8000, 2000, 500, 100, 25\\}$。\n\n首先，我们计算总样本数 $N$：\n$$\nN = \\sum_{c=1}^{5} n_c = 8000 + 2000 + 500 + 100 + 25 = 10625\n$$\n现在，我们对每个类别应用推导出的公式 $w_c = \\frac{N}{C \\cdot n_c}$，其中 $N = 10625$，$C=5$。\n\n对于 $c=1$ ($n_1 = 8000$)：\n$$\nw_1 = \\frac{10625}{5 \\times 8000} = \\frac{10625}{40000} = 0.265625\n$$\n四舍五入到四位有效数字：$w_1 \\approx 0.2656$。\n\n对于 $c=2$ ($n_2 = 2000$)：\n$$\nw_2 = \\frac{10625}{5 \\times 2000} = \\frac{10625}{10000} = 1.0625\n$$\n四舍五入到四位有效数字：$w_2 \\approx 1.063$。\n\n对于 $c=3$ ($n_3 = 500$)：\n$$\nw_3 = \\frac{10625}{5 \\times 500} = \\frac{10625}{2500} = 4.25\n$$\n四舍五入到四位有效数字：$w_3 = 4.250$。\n\n对于 $c=4$ ($n_4 = 100$)：\n$$\nw_4 = \\frac{10625}{5 \\times 100} = \\frac{10625}{500} = 21.25\n$$\n四舍五入到四位有效数字：$w_4 = 21.25$。\n\n对于 $c=5$ ($n_5 = 25$)：\n$$\nw_5 = \\frac{10625}{5 \\times 25} = \\frac{10625}{125} = 85\n$$\n四舍五入到四位有效数字：$w_5 = 85.00$。\n\n计算出的权重为 $\\{0.2656, 1.063, 4.250, 21.25, 85.00\\}$。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 0.2656  1.063  4.250  21.25  85.00 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "在医疗诊断等高风险应用中，了解模型对其预测的“置信度”与预测本身同样重要。本练习  介绍了一种在深度学习中量化预测不确定性的实用贝叶斯方法——蒙特卡洛丢弃（Monte Carlo Dropout）。您将推导并计算如何将模型的总预测不确定性分解为两个关键部分：代表数据固有噪声的偶然不确定性（aleatoric uncertainty）和反映模型知识局限的认知不确定性（epistemic uncertainty）。",
            "id": "4553839",
            "problem": "您正在使用一个在训练和推理过程中都带有 dropout 的深度神经网络，对单一细胞类型的对数转换基因表达进行建模。假设在推理时保持 dropout 可以实现对贝叶斯推理的变分近似，其中网络权重的近似后验分布是由随机伯努利掩码引出的。设新输入 $x$ 的预测后验由权重 $W$ 上的积分定义为 $p(y \\mid x, \\mathcal{D}) = \\int p(y \\mid x, W) \\, p(W \\mid \\mathcal{D}) \\, dW$，其中 $\\mathcal{D}$ 表示训练数据。\n\n在推理时，您通过执行 $T$ 次带有独立 dropout 掩码的随机前向传播来实现蒙特卡洛（MC）dropout，对于每次传播 $t \\in \\{1,\\dots,T\\}$，都获得一对 $(\\mu_t, \\sigma_t^{2})$，代表高斯似然 $y \\mid x, W^{(t)} \\sim \\mathcal{N}(\\mu_t, \\sigma_t^{2})$ 的条件均值和偶然方差。您可以假设这些前向传播是从权重的变分后验分布中进行的独立同分布抽样。任务是从预测后验的定义以及标准的全期望定律和全方差定律出发，推导出此设置所隐含的预测均值和预测方差的蒙特卡洛估计量。然后，使用以下 $T$ 个实现值计算预测均值和预测方差：\n- $T = 5$,\n- $(\\mu_1, \\sigma_1^{2}) = (1.10, 0.09)$,\n- $(\\mu_2, \\sigma_2^{2}) = (0.95, 0.16)$,\n- $(\\mu_3, \\sigma_3^{2}) = (1.20, 0.04)$,\n- $(\\mu_4, \\sigma_4^{2}) = (1.05, 0.09)$,\n- $(\\mu_5, \\sigma_5^{2}) = (1.15, 0.04)$.\n\n按顺序以预测均值、预测方差两个量提供您的最终数值结果。将您的数值答案四舍五入到四位有效数字。将最终答案表示为行矩阵。无需单位。",
            "solution": "该问题要求在一个使用 MC dropout 的贝叶斯深度学习框架中，推导预测均值和预测方差的蒙特卡洛估计量，然后使用所提供的数据进行数值计算。推导过程基于全期望定律和全方差定律。\n\n**1. 估计量的推导**\n\n设 $y$ 是模型对新输入 $x$ 的预测的随机变量，设 $W$ 是模型权重的随机变量，从近似后验分布 $p(W \\mid \\mathcal{D})$ 中抽取。问题将给定权重集 $W$ 的似然定义为高斯分布：\n$$y \\mid x, W \\sim \\mathcal{N}(\\mu(W), \\sigma^2(W))$$\n这里，$\\mu(W)$ 是条件均值，$\\sigma^2(W)$ 是条件方差，称为偶然不确定性。蒙特卡洛（MC）dropout 过程从近似后验中生成 $T$ 个样本 $\\{W^{(t)}\\}_{t=1}^T$，产生一组独立同分布的对 $\\{(\\mu_t, \\sigma_t^2)\\}_{t=1}^T$，其中 $\\mu_t = \\mu(W^{(t)})$ 且 $\\sigma_t^2 = \\sigma^2(W^{(t)})$。\n\n**1.1. 预测均值**\n\n预测均值 $\\mu_{pred}$ 是 $y$ 在预测后验分布 $p(y \\mid x, \\mathcal{D})$ 上的期望。使用全期望定律（或迭代期望定律），我们可以将其写成在权重 $W$ 分布上的期望：\n$$\\mu_{pred} = E[y \\mid x, \\mathcal{D}] = E_{W \\sim p(W \\mid \\mathcal{D})} \\left[ E[y \\mid x, W] \\right]$$\n根据我们高斯似然的定义，内层期望是高斯的均值，即 $E[y \\mid x, W] = \\mu(W)$。因此，预测均值是模型均值输出在权重分布上的期望：\n$$\\mu_{pred} = E_{W \\sim p(W \\mid \\mathcal{D})} [\\mu(W)]$$\n这个期望的蒙特卡洛估计量，记作 $\\hat{\\mu}_{pred}$，是从 $T$ 次随机前向传播中获得的均值 $\\mu_t$ 的样本平均值：\n$$\\hat{\\mu}_{pred} = \\frac{1}{T} \\sum_{t=1}^{T} \\mu_t$$\n\n**1.2. 预测方差**\n\n预测方差 $\\sigma_{pred}^2$ 是 $y$ 在预测后验上的方差。我们使用全方差定律：\n$$Var(y \\mid x, \\mathcal{D}) = E_{W \\sim p(W \\mid \\mathcal{D})} [Var(y \\mid x, W)] + Var_{W \\sim p(W \\mid \\mathcal{D})} [E[y \\mid x, W]]$$\n从我们的高斯似然中，我们确定各项：\n- 内层方差是偶然方差：$Var(y \\mid x, W) = \\sigma^2(W)$。\n- 内层期望是条件均值：$E[y \\mid x, W] = \\mu(W)$。\n\n将这些代入全方差定律，得到预测方差的分解：\n$$\\sigma_{pred}^2 = \\underbrace{E_{W \\sim p(W \\mid \\mathcal{D})} [\\sigma^2(W)]}_{\\text{偶然不确定性的期望}} + \\underbrace{Var_{W \\sim p(W \\mid \\mathcal{D})} [\\mu(W)]}_{\\text{认知不确定性}}$$\n预测方差是两个分量的和：\n1.  **偶然不确定性**：模型在不同权重配置下预测的方差的平均值。这代表了数据中模型无法减少的内在噪声。\n2.  **认知不确定性**：模型均值预测的方差。这代表了模型对其参数 $W$ 自身的不确定性，可以通过更多的数据来减少。\n\n$\\sigma_{pred}^2$ 的蒙特卡洛估计量，记作 $\\hat{\\sigma}_{pred}^2$，是通过用各自的样本估计来近似每一项得到的：\n- 偶然不确定性的期望的估计量是方差 $\\sigma_t^2$ 的样本均值：\n$$ \\hat{E}[\\sigma^2(W)] = \\frac{1}{T} \\sum_{t=1}^{T} \\sigma_t^2 $$\n- 认知不确定性的估计量是均值 $\\mu_t$ 的样本方差：\n$$ \\widehat{Var}[\\mu(W)] = \\frac{1}{T} \\sum_{t=1}^{T} (\\mu_t - \\hat{\\mu}_{pred})^2 $$\n结合这些，预测方差的蒙特卡洛估计量是：\n$$ \\hat{\\sigma}_{pred}^2 = \\left( \\frac{1}{T} \\sum_{t=1}^{T} \\sigma_t^2 \\right) + \\left( \\frac{1}{T} \\sum_{t=1}^{T} \\mu_t^2 - \\hat{\\mu}_{pred}^2 \\right) $$\n第二项是样本方差的一个代数等价形式。表达总方差估计量的另一种方式是：\n$$ \\hat{\\sigma}_{pred}^2 = \\frac{1}{T} \\sum_{t=1}^{T} (\\mu_t^2 + \\sigma_t^2) - \\hat{\\mu}_{pred}^2 $$\n\n**2. 数值计算**\n\n所提供的数据是：\n- $T = 5$\n- $(\\mu_t, \\sigma_t^2)$ 的实现值：\n  - $t=1$: $(\\mu_1, \\sigma_1^2) = (1.10, 0.09)$\n  - $t=2$: $(\\mu_2, \\sigma_2^2) = (0.95, 0.16)$\n  - $t=3$: $(\\mu_3, \\sigma_3^2) = (1.20, 0.04)$\n  - $t=4$: $(\\mu_4, \\sigma_4^2) = (1.05, 0.09)$\n  - $t=5$: $(\\mu_5, \\sigma_5^2) = (1.15, 0.04)$\n\n**2.1. 计算预测均值**\n使用推导出的估计量：\n$$ \\hat{\\mu}_{pred} = \\frac{1}{5} \\sum_{t=1}^{5} \\mu_t = \\frac{1}{5} (1.10 + 0.95 + 1.20 + 1.05 + 1.15) $$\n$$ \\hat{\\mu}_{pred} = \\frac{1}{5} (5.45) = 1.09 $$\n\n**2.2. 计算预测方差**\n我们将分别计算方差的两个分量。\n\n首先，是平均偶然不确定性：\n$$ \\hat{E}[\\sigma^2(W)] = \\frac{1}{5} \\sum_{t=1}^{5} \\sigma_t^2 = \\frac{1}{5} (0.09 + 0.16 + 0.04 + 0.09 + 0.04) $$\n$$ \\hat{E}[\\sigma^2(W)] = \\frac{1}{5} (0.42) = 0.084 $$\n\n接下来，是认知不确定性（均值的样本方差）：\n$$ \\widehat{Var}[\\mu(W)] = \\frac{1}{5} \\sum_{t=1}^{5} (\\mu_t - \\hat{\\mu}_{pred})^2 $$\n$$ \\widehat{Var}[\\mu(W)] = \\frac{1}{5} \\left( (1.10 - 1.09)^2 + (0.95 - 1.09)^2 + (1.20 - 1.09)^2 + (1.05 - 1.09)^2 + (1.15 - 1.09)^2 \\right) $$\n$$ \\widehat{Var}[\\mu(W)] = \\frac{1}{5} \\left( (0.01)^2 + (-0.14)^2 + (0.11)^2 + (-0.04)^2 + (0.06)^2 \\right) $$\n$$ \\widehat{Var}[\\mu(W)] = \\frac{1}{5} (0.0001 + 0.0196 + 0.0121 + 0.0016 + 0.0036) $$\n$$ \\widehat{Var}[\\mu(W)] = \\frac{1}{5} (0.037) = 0.0074 $$\n\n总预测方差是这两个分量的和：\n$$ \\hat{\\sigma}_{pred}^2 = \\hat{E}[\\sigma^2(W)] + \\widehat{Var}[\\mu(W)] = 0.084 + 0.0074 = 0.0914 $$\n\n**3. 最终结果**\n\n计算出的预测均值为 $1.09$，预测方差为 $0.0914$。按要求四舍五入到四位有效数字：\n- 预测均值: $1.090$\n- 预测方差: $0.09140$",
            "answer": "$$\\boxed{\\begin{pmatrix} 1.090  0.09140 \\end{pmatrix}}$$"
        }
    ]
}