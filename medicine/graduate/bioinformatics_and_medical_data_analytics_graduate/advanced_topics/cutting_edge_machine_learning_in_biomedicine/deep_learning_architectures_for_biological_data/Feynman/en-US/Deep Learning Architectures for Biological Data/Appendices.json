{
    "hands_on_practices": [
        {
            "introduction": "The design of a Convolutional Neural Network (CNN) architecture is a critical first step in analyzing biological sequence data, such as DNA. The geometric parameters of a convolutional layer—kernel size, stride, and padding—directly determine the model's receptive field and its ability to detect patterns of interest, like transcription factor binding motifs. This exercise provides fundamental practice in calculating the output dimensions of a convolutional layer and prompts a deeper analysis of how these architectural choices impact the model's capacity to learn biologically relevant features .",
            "id": "4553876",
            "problem": "You are designing a first-layer one-dimensional convolution in a Convolutional Neural Network (CNN; Convolutional Neural Network) to detect transcription factor binding motifs in Deoxyribonucleic Acid (DNA; Deoxyribonucleic Acid) sequences encoded as one-hot along channels. Consider an input sequence of length $L_{\\mathrm{in}} = 1{,}000$ bases. The convolutional layer uses a kernel of length $k = 15$, stride $s = 3$, symmetric zero-padding of $p = 2$ bases on each end, and dilation $d = 1$.\n\nStarting from the definition of discrete one-dimensional convolution with stride and padding on a padded input, derive the general expression that determines the number of output positions as the count of valid kernel alignments. Use this reasoning to compute the output sequence length $L_{\\mathrm{out}}$ for the given parameters. Then, based on the concept of receptive field and the sampling induced by stride, analyze how this configuration affects the ability to detect motifs that span $M = 20$ bases, particularly with respect to full coverage within a single filter application, positional sensitivity, and detection near sequence boundaries.\n\nReport the output length $L_{\\mathrm{out}}$ as a single integer with no units. No rounding is necessary. Your qualitative analysis should be included in your reasoning, but the final answer must be only the computed $L_{\\mathrm{out}}$.",
            "solution": "The user requires the derivation of the output length of a one-dimensional convolutional layer and a qualitative analysis of its properties for a specific application in bioinformatics. The solution will first establish the general formula for the output dimension, then apply it to the given parameters, and finally analyze the configuration's implications.\n\n**Problem Validation**\n\nThe problem statement is critically validated before proceeding.\n1.  **Givens Extracted**:\n    -   Input sequence length: $L_{\\mathrm{in}} = 1{,}000$ bases.\n    -   Convolutional kernel length: $k = 15$.\n    -   Stride: $s = 3$.\n    -   Symmetric zero-padding: $p = 2$ on each end.\n    -   Dilation: $d = 1$.\n    -   Subject motif length for analysis: $M = 20$ bases.\n\n2.  **Validation Verdict**: The problem is **valid**. It is scientifically grounded in the standard principles of deep learning (specifically, Convolutional Neural Networks) as applied to the well-established field of bioinformatics for genomic analysis. The problem is well-posed, providing a complete and consistent set of parameters ($L_{\\mathrm{in}}, k, s, p, d$) from which a unique, meaningful solution for the output length ($L_{\\mathrm{out}}$) can be determined. The language is objective and employs standard terminology. The parameters are realistic for the described task.\n\n**Derivation of the General Expression for Output Length**\n\nThe number of output positions, or the output sequence length $L_{\\mathrm{out}}$, of a one-dimensional convolutional layer is determined by the number of valid placements of the kernel along the input sequence. This can be derived from first principles.\n\n1.  An input sequence of length $L_{\\mathrm{in}}$ is first subjected to symmetric padding, where $p$ elements are added to each of its two ends. The length of this new, padded sequence, denoted $L_{\\mathrm{padded}}$, is:\n    $$L_{\\mathrm{padded}} = L_{\\mathrm{in}} + 2p$$\n\n2.  The kernel, of length $k$, may be dilated. The dilation factor $d$ introduces $d-1$ gaps between consecutive kernel elements. The effective spatial extent of the kernel on the input, known as the effective kernel size $k_{\\mathrm{eff}}$, is thus:\n    $$k_{\\mathrm{eff}} = k + (k-1)(d-1)$$\n\n3.  The kernel is moved, or 'strided', across the padded sequence. An output is computed for each position. The first placement of the kernel starts at index $0$ of the padded sequence. The extent covered is from index $0$ to $k_{\\mathrm{eff}}-1$. The last possible placement must also fit entirely within the padded sequence. The last element of the kernel must align with an index no greater than $L_{\\mathrm{padded}}-1$. This means the last starting position for the kernel is at index $L_{\\mathrm{padded}} - k_{\\mathrm{eff}}$.\n\n4.  The total span of possible starting positions is therefore from $0$ to $L_{\\mathrm{padded}} - k_{\\mathrm{eff}}$, which comprises $L_{\\mathrm{padded}} - k_{\\mathrm{eff}} + 1$ possible positions if the stride were $1$.\n\n5.  With a stride of $s$, the kernel does not occupy every possible position. Instead, it occupies positions $0, s, 2s, \\dots, Ns$, where $Ns$ is the last valid starting position. This last position must satisfy $Ns \\le L_{\\mathrm{padded}} - k_{\\mathrm{eff}}$. The number of steps, $N$, is therefore the largest integer satisfying this condition, which is found using the floor function:\n    $$N = \\left\\lfloor \\frac{L_{\\mathrm{padded}} - k_{\\mathrm{eff}}}{s} \\right\\rfloor$$\n    The total number of output positions, $L_{\\mathrm{out}}$, is the number of these placements, which is $N+1$ (including the initial placement at position $0$).\n\n6.  Substituting the expressions for $L_{\\mathrm{padded}}$ and $k_{\\mathrm{eff}}$, the general formula for the output length is obtained:\n    $$L_{\\mathrm{out}} = \\left\\lfloor \\frac{(L_{\\mathrm{in}} + 2p) - (k + (k-1)(d-1))}{s} \\right\\rfloor + 1$$\n\n**Computation of $L_{\\mathrm{out}}$ for the Given Parameters**\n\nThe problem provides the following parameters: $L_{\\mathrm{in}} = 1{,}000$, $k = 15$, $s = 3$, $p = 2$, and $d = 1$.\n\nFirst, we substitute these values into the derived general expression. Since the dilation is $d=1$, the effective kernel length $k_{\\mathrm{eff}}$ is simply $k$:\n$$k_{\\text{eff}} = 15 + (15-1)(1-1) = 15 + (14)(0) = 15$$\nThe formula for $L_{\\mathrm{out}}$ simplifies to:\n$$L_{\\mathrm{out}} = \\left\\lfloor \\frac{L_{\\mathrm{in}} + 2p - k}{s} \\right\\rfloor + 1$$\nSubstituting the numerical values:\n$$L_{\\mathrm{out}} = \\left\\lfloor \\frac{1{,}000 + 2(2) - 15}{3} \\right\\rfloor + 1$$\n$$L_{\\mathrm{out}} = \\left\\lfloor \\frac{1{,}000 + 4 - 15}{3} \\right\\rfloor + 1$$\n$$L_{\\mathrm{out}} = \\left\\lfloor \\frac{989}{3} \\right\\rfloor + 1$$\nThe division yields a non-integer value:\n$$\\frac{989}{3} \\approx 329.667$$\nApplying the floor function gives the greatest integer less than or equal to this value:\n$$\\lfloor 329.667 \\rfloor = 329$$\nFinally, adding $1$ yields the output length:\n$$L_{\\mathrm{out}} = 329 + 1 = 330$$\n\n**Qualitative Analysis of the Configuration**\n\nThe problem further requires an analysis of this configuration's ability to detect motifs of length $M = 20$ bases.\n\n1.  **Full Coverage and Receptive Field**: The receptive field of any single neuron in this convolutional layer is its effective kernel size, $k_{\\mathrm{eff}} = 15$ bases. The motif of interest is $M=20$ bases long. Since $M > k_{\\mathrm{eff}}$, **no single filter application can see the entire motif**. The filter can, at best, learn to recognize fragments of the motif that are no longer than $15$ bases. To identify the full $20$-base motif, the network must learn to combine information from spatially adjacent neurons in the output feature map, a task for subsequent layers (e.g., another convolutional layer or a fully-connected layer).\n\n2.  **Positional Sensitivity and Stride**: The stride of $s=3$ introduces a downsampling of the input. This means the kernel \"jumps\" over $2$ bases for each step it takes. Consequently, the layer is not translationally equivariant to single-base shifts. A motif that starts at input position $i$ will be viewed differently by the kernel than one starting at position $i+1$ or $i+2$. This high sensitivity to positional shifts modulo $s$ can be detrimental, as a small shift in a motif's location might cause it to be misaligned with the kernel's stride, leading to a diminished activation and a possible failed detection.\n\n3.  **Detection Near Sequence Boundaries**: The layer uses symmetric padding of $p=2$. With a kernel of size $k=15$, this padding is insufficient to allow the kernel to be centered over motifs at the extreme ends of the sequence. To center a kernel of size $k$ over the very first base (index $0$), a padding of $p_{\\mathrm{center}} = \\lfloor k/2 \\rfloor = \\lfloor 15/2 \\rfloor = 7$ would be required. With $p=2$, the receptive field of the very first output neuron is centered on original sequence position $7-2 = 5$. This implies that motifs within the first $\\approx 5$ and last $\\approx 5$ bases of the sequence can never be centered in the kernel's view, which can impair the network's ability to detect them.\n\nIn conclusion, this specific layer configuration is structurally limited for the task of detecting a $20$-base motif due to its small kernel size, and its performance is further compromised by a large stride and insufficient padding, which affect positional sensitivity and boundary detection.",
            "answer": "$$\\boxed{330}$$"
        },
        {
            "introduction": "Beyond architecture, the choice of loss function is paramount to training a successful deep learning model, especially for specialized tasks like semantic segmentation in medical imaging. While standard losses like cross-entropy are common, they may not directly optimize the evaluation metrics used in a domain, such as the Dice coefficient for measuring overlap. This practice guides you through the crucial process of deriving a differentiable \"soft\" Dice loss from its set-based definition, a technique that bridges the gap between evaluation and optimization, and solidifies your understanding of its relationship with the Jaccard index .",
            "id": "4553851",
            "problem": "In binary semantic segmentation for medical imaging (for example, tumor region delineation in histopathology), a model outputs per-pixel probabilities to indicate whether a pixel belongs to the positive class. Let there be $N$ pixels indexed by $i \\in \\{1,\\dots,N\\}$ with ground-truth labels $y_i \\in \\{0,1\\}$ and model outputs $p_i \\in [0,1]$. Define the positive set of ground truth as the index set where $y_i=1$, and the positive set of a thresholded prediction $\\hat{y}_i \\in \\{0,1\\}$ as the index set where $\\hat{y}_i=1$. The Dice coefficient and the Jaccard index (Intersection over Union, IoU) are widely used to evaluate and optimize segmentation architectures such as Convolutional Neural Networks (CNNs) in bioinformatics and medical data analytics.\n\nStarting only from the core definitions of set cardinality, the union, and the intersection, and from the definitions of precision and recall in binary classification, do the following:\n\n1. Derive a differentiable Dice-based loss suitable for training a deep network from per-pixel probabilities $p_i$, by relaxing set cardinalities with sums over $p_i$ and $y_i$. Clearly state the resulting loss in terms of $p_i$ and $y_i$ and any smoothing constant you introduce, and justify each relaxation step from first principles so that the loss is differentiable with respect to $p_i$.\n\n2. Using only the definitions of the intersection and union of two sets, derive an exact algebraic relationship that expresses the Dice coefficient as a function of the Jaccard index, and also express the Jaccard index as a function of the Dice coefficient.\n\n3. Consider a predicted binary mask with $1{,}200$ positive pixels, a ground-truth binary mask with $1{,}000$ positive pixels, and an intersection of $800$ positive pixels between them. Compute the Dice coefficient using only these counts.\n\nExpress the final Dice coefficient for part 3 as a reduced fraction. Do not use percentages or include any units. No rounding is required for the final reported value.",
            "solution": "This problem consists of three parts concerning the Dice coefficient and Jaccard index in the context of medical image segmentation. We will address each part sequentially. The validation of the problem statement confirms its scientific grounding, well-posedness, and objectivity, allowing for a complete solution.\n\n### Part 1: Derivation of a Differentiable Dice Loss Function\n\nThe objective is to derive a differentiable loss function based on the Dice coefficient, suitable for training a deep neural network on a per-pixel binary segmentation task.\n\nLet $Y$ be the set of pixel indices belonging to the positive class in the ground truth, and let $\\hat{Y}$ be the set of pixel indices predicted as the positive class. The Dice coefficient, $D$, is defined as a measure of overlap between these two sets:\n$$D = \\frac{2 |Y \\cap \\hat{Y}|}{|Y| + |\\hat{Y}|}$$\nHere, $|S|$ denotes the cardinality of a set $S$.\n\nThe Dice coefficient can also be expressed as the harmonic mean of precision ($P$) and recall ($R$). In binary classification, these are defined as:\n-   True Positives ($TP$): $|Y \\cap \\hat{Y}|$\n-   False Positives ($FP$): $|\\hat{Y} \\setminus Y| = |\\hat{Y}| - |Y \\cap \\hat{Y}|$\n-   False Negatives ($FN$): $|Y \\setminus \\hat{Y}| = |Y| - |Y \\cap \\hat{Y}|$\n\nUsing these, precision and recall are:\n$$P = \\frac{TP}{TP + FP} = \\frac{|Y \\cap \\hat{Y}|}{|\\hat{Y}|}$$\n$$R = \\frac{TP}{TP + FN} = \\frac{|Y \\cap \\hat{Y}|}{|Y|}$$\n\nThe F1-score, which is equivalent to the Dice coefficient in this context, is the harmonic mean of $P$ and $R$:\n$$D = \\frac{2PR}{P+R} = \\frac{2 \\left( \\frac{|Y \\cap \\hat{Y}|}{|\\hat{Y}|} \\right) \\left( \\frac{|Y \\cap \\hat{Y}|}{|Y|} \\right)}{\\frac{|Y \\cap \\hat{Y}|}{|\\hat{Y}|} + \\frac{|Y \\cap \\hat{Y}|}{|Y|}} = \\frac{\\frac{2|Y \\cap \\hat{Y}|^2}{|\\hat{Y}||Y|}}{\\frac{|Y \\cap \\hat{Y}|(|Y|+|\\hat{Y}|)}{|\\hat{Y}||Y|}} = \\frac{2|Y \\cap \\hat{Y}|}{|Y| + |\\hat{Y}|}$$\nThis confirms the initial definition.\n\nFor a segmentation problem with $N$ pixels, we can express the set cardinalities as sums over the per-pixel labels. Let $y_i \\in \\{0, 1\\}$ be the ground-truth label for a pixel $i$ and $\\hat{y}_i \\in \\{0, 1\\}$ be the predicted binary label.\n-   The cardinality of the ground-truth positive set is $|Y| = \\sum_{i=1}^{N} y_i$.\n-   The cardinality of the predicted positive set is $|\\hat{Y}| = \\sum_{i=1}^{N} \\hat{y}_i$.\n-   The cardinality of the intersection is $|Y \\cap \\hat{Y}| = \\sum_{i=1}^{N} y_i \\hat{y}_i$.\n\nThe Dice coefficient becomes:\n$$D = \\frac{2 \\sum_{i=1}^{N} y_i \\hat{y}_i}{\\sum_{i=1}^{N} y_i + \\sum_{i=1}^{N} \\hat{y}_i}$$\n\nA deep learning model outputs probabilities $p_i \\in [0, 1]$ for each pixel. The binary predictions $\\hat{y}_i$ are typically obtained by thresholding these probabilities, e.g., $\\hat{y}_i = 1$ if $p_i > 0.5$ and $\\hat{y}_i = 0$ otherwise. This thresholding function is non-differentiable (specifically, its derivative is zero almost everywhere and undefined at the threshold), which prevents the use of gradient-based optimization methods like backpropagation.\n\nTo create a differentiable loss, we must relax the discrete metric. The core principle is to replace the binary predicted labels $\\hat{y}_i$ with the continuous model outputs $p_i$. This is interpreted as a \"soft\" or probabilistic version of the metric.\n-   The intersection term $\\sum y_i \\hat{y}_i$ is relaxed to $\\sum y_i p_i$. Since $y_i$ is a binary constant ($1$ for positive class, $0$ for negative), this sum measures the model's confidence in the correct pixels. It can be viewed as the expected number of true positives.\n-   The predicted set cardinality $\\sum \\hat{y}_i$ is relaxed to $\\sum p_i$. This can be viewed as the expected number of positive predictions.\n\nThe resulting soft Dice score, $D_{\\text{soft}}$, is:\n$$D_{\\text{soft}} = \\frac{2 \\sum_{i=1}^{N} y_i p_i}{\\sum_{i=1}^{N} y_i + \\sum_{i=1}^{N} p_i}$$\n\nThis expression is differentiable with respect to each $p_i$, as it is composed of sums and division. However, one issue remains: if the ground truth is an empty mask ($\\sum y_i = 0$) and the model correctly predicts an empty mask (all $p_i$ are close to $0$), the denominator and numerator both approach $0$, leading to numerical instability and a potentially undefined gradient. To remedy this, a small smoothing constant, $\\epsilon > 0$, is typically added to both the numerator and the denominator.\n\nThe final, smoothed, differentiable Dice score is:\n$$D_{\\text{smooth}} = \\frac{2 \\sum_{i=1}^{N} y_i p_i + \\epsilon}{\\sum_{i=1}^{N} y_i + \\sum_{i=1}^{N} p_i + \\epsilon}$$\n\nSince optimization algorithms minimize a loss function, and the Dice score is a similarity metric to be maximized, the Dice loss, $L_{\\text{Dice}}$, is commonly defined as $1 - D_{\\text{smooth}}$:\n$$L_{\\text{Dice}} = 1 - \\frac{2 \\sum_{i=1}^{N} y_i p_i + \\epsilon}{\\sum_{i=1}^{N} y_i + \\sum_{i=1}^{N} p_i + \\epsilon}$$\nThis loss function is differentiable with respect to the model parameters (via the chain rule through $p_i$) and is suitable for training a deep network.\n\n### Part 2: Algebraic Relationship between Dice and Jaccard\n\nThe objective is to derive expressions relating the Dice coefficient ($D$) and the Jaccard index ($J$). Let $A$ and $B$ be two arbitrary sets.\n\nThe Jaccard index (or Intersection over Union, IoU) is defined as:\n$$J = \\frac{|A \\cap B|}{|A \\cup B|}$$\nThe Dice coefficient is defined as:\n$$D = \\frac{2 |A \\cap B|}{|A| + |B|}$$\n\nThe relationship between the union and intersection of two sets is given by the principle of inclusion-exclusion:\n$$|A \\cup B| = |A| + |B| - |A \\cap B|$$\n\nFirst, we express the Dice coefficient $D$ as a function of the Jaccard index $J$.\nFrom the definition of $J$, we can write:\n$$|A \\cup B| = \\frac{|A \\cap B|}{J}$$\nSubstitute this into the inclusion-exclusion formula:\n$$\\frac{|A \\cap B|}{J} = |A| + |B| - |A \\cap B|$$\nRearranging to solve for $|A| + |B|$:\n$$|A| + |B| = \\frac{|A \\cap B|}{J} + |A \\cap B| = |A \\cap B| \\left(\\frac{1}{J} + 1\\right) = |A \\cap B| \\left(\\frac{1+J}{J}\\right)$$\nNow, substitute this expression for $|A| + |B|$ into the definition of the Dice coefficient $D$:\n$$D = \\frac{2 |A \\cap B|}{|A| + |B|} = \\frac{2 |A \\cap B|}{|A \\cap B| \\left(\\frac{1+J}{J}\\right)} = \\frac{2}{\\frac{1+J}{J}}$$\nSimplifying this gives the relationship:\n$$D = \\frac{2J}{1+J}$$\n\nNext, we express the Jaccard index $J$ as a function of the Dice coefficient $D$. We start from the relationship just derived:\n$$D = \\frac{2J}{1+J}$$\nWe solve this equation for $J$:\n$$D(1+J) = 2J$$\n$$D + DJ = 2J$$\n$$D = 2J - DJ$$\n$$D = J(2 - D)$$\n$$J = \\frac{D}{2 - D}$$\n\nThese two equations provide the exact algebraic relationship between the Dice coefficient and the Jaccard index.\n\n### Part 3: Computation of the Dice Coefficient\n\nThe problem provides the following counts for a binary segmentation task:\n-   Number of positive pixels in the predicted mask, $|\\hat{Y}| = 1,200$.\n-   Number of positive pixels in the ground-truth mask, $|Y| = 1,000$.\n-   Number of positive pixels in the intersection of the two masks, $|Y \\cap \\hat{Y}| = 800$.\n\nThe task is to compute the Dice coefficient using these values. We use the standard definition of the Dice coefficient:\n$$D = \\frac{2 |Y \\cap \\hat{Y}|}{|Y| + |\\hat{Y}|}$$\nSubstituting the given numerical values:\n$$D = \\frac{2 \\times 800}{1,000 + 1,200}$$\n$$D = \\frac{1,600}{2,200}$$\nTo express this as a reduced fraction, we can simplify the expression by dividing the numerator and denominator by common factors.\n$$D = \\frac{1600}{2200} = \\frac{16}{22}$$\nDividing both the numerator and the denominator by their greatest common divisor, which is $2$:\n$$D = \\frac{16 \\div 2}{22 \\div 2} = \\frac{8}{11}$$\nThe Dice coefficient for the given counts is $\\frac{8}{11}$.",
            "answer": "$$\\boxed{\\frac{8}{11}}$$"
        },
        {
            "introduction": "In high-stakes applications like clinical diagnostics, a model's point prediction is often insufficient; we also need to quantify its uncertainty. Bayesian deep learning offers a principled framework for this, and Monte Carlo (MC) dropout has emerged as a practical method for approximating a Bayesian model's predictive distribution. This exercise explores how to derive and compute the predictive mean and variance from multiple stochastic forward passes, demonstrating how to decompose total uncertainty into its aleatoric (data) and epistemic (model) components .",
            "id": "4553839",
            "problem": "You are modeling log-transformed gene expression for a single cell type using a deep neural network with dropout during both training and inference. Assume that maintaining dropout at inference realizes a variational approximation to Bayesian inference, where the approximate posterior over network weights is induced by random Bernoulli masks. Let the predictive posterior for a new input $x$ be defined by the integral over weights $W$ as $p(y \\mid x, \\mathcal{D}) = \\int p(y \\mid x, W) \\, p(W \\mid \\mathcal{D}) \\, dW$, where $\\mathcal{D}$ denotes the training data.\n\nAt inference, you implement Monte Carlo (MC) dropout by performing $T$ stochastic forward passes with independent dropout masks, obtaining for each pass $t \\in \\{1,\\dots,T\\}$ a pair $(\\mu_t, \\sigma_t^2)$ representing the conditional mean and the aleatoric variance of a Gaussian likelihood $y \\mid x, W^{(t)} \\sim \\mathcal{N}(\\mu_t, \\sigma_t^2)$. You may assume that the forward passes are independent and identically distributed draws from the variational posterior over weights. The task is to derive the Monte Carlo estimators for the predictive mean and the predictive variance implied by this setup, starting from the definition of the predictive posterior and standard laws of total expectation and total variance. Then, compute the predictive mean and predictive variance using the following $T$ realizations:\n- $T = 5$,\n- $(\\mu_1, \\sigma_1^2) = (1.10, 0.09)$,\n- $(\\mu_2, \\sigma_2^2) = (0.95, 0.16)$,\n- $(\\mu_3, \\sigma_3^2) = (1.20, 0.04)$,\n- $(\\mu_4, \\sigma_4^2) = (1.05, 0.09)$,\n- $(\\mu_5, \\sigma_5^2) = (1.15, 0.04)$.\n\nProvide your final numerical results as two quantities in the order: predictive mean, predictive variance. Round your numerical answers to four significant figures. Express the final answer as a row matrix. No units are required.",
            "solution": "The problem requires the derivation of the Monte Carlo estimators for the predictive mean and predictive variance in a Bayesian deep learning framework using MC dropout, followed by a numerical computation using provided data. The derivation hinges on the laws of total expectation and total variance.\n\n**1. Derivation of Estimators**\n\nLet $y$ be the random variable for the model's prediction for a new input $x$, and let $W$ be the random variable for the model's weights, drawn from the approximate posterior distribution $p(W \\mid \\mathcal{D})$. The problem defines the likelihood for a given set of weights $W$ as a Gaussian distribution:\n$$y \\mid x, W \\sim \\mathcal{N}(\\mu(W), \\sigma^2(W))$$\nHere, $\\mu(W)$ is the conditional mean and $\\sigma^2(W)$ is the conditional variance, known as the aleatoric uncertainty. The Monte Carlo (MC) dropout procedure generates $T$ samples from the approximate posterior, $\\{W^{(t)}\\}_{t=1}^T$, yielding a set of i.i.d. pairs $\\{(\\mu_t, \\sigma_t^2)\\}_{t=1}^T$, where $\\mu_t = \\mu(W^{(t)})$ and $\\sigma_t^2 = \\sigma^2(W^{(t)})$.\n\n**1.1. Predictive Mean**\n\nThe predictive mean, $\\mu_{pred}$, is the expectation of $y$ over the predictive posterior distribution $p(y \\mid x, \\mathcal{D})$. Using the law of total expectation (or law of iterated expectations), we can write this as an expectation over the distribution of the weights $W$:\n$$\\mu_{pred} = E[y \\mid x, \\mathcal{D}] = E_{W \\sim p(W \\mid \\mathcal{D})} \\left[ E[y \\mid x, W] \\right]$$\nFrom the definition of our Gaussian likelihood, the inner expectation is the mean of the Gaussian, $E[y \\mid x, W] = \\mu(W)$. Therefore, the predictive mean is the expectation of the model's mean output over the distribution of weights:\n$$\\mu_{pred} = E_{W \\sim p(W \\mid \\mathcal{D})} [\\mu(W)]$$\nThe Monte Carlo estimator for this expectation, denoted $\\hat{\\mu}_{pred}$, is the sample average of the means $\\mu_t$ obtained from the $T$ stochastic forward passes:\n$$\\hat{\\mu}_{pred} = \\frac{1}{T} \\sum_{t=1}^{T} \\mu_t$$\n\n**1.2. Predictive Variance**\n\nThe predictive variance, $\\sigma_{pred}^2$, is the variance of $y$ over the predictive posterior. We use the law of total variance:\n$$Var(y \\mid x, \\mathcal{D}) = E_{W \\sim p(W \\mid \\mathcal{D})} [Var(y \\mid x, W)] + Var_{W \\sim p(W \\mid \\mathcal{D})} [E[y \\mid x, W]]$$\nFrom our Gaussian likelihood, we identify the terms:\n- The inner variance is the aleatoric variance: $Var(y \\mid x, W) = \\sigma^2(W)$.\n- The inner expectation is the conditional mean: $E[y \\mid x, W] = \\mu(W)$.\n\nSubstituting these into the law of total variance gives the decomposition of the predictive variance:\n$$\\sigma_{pred}^2 = \\underbrace{E_{W \\sim p(W \\mid \\mathcal{D})} [\\sigma^2(W)]}_{\\text{Expected aleatoric uncertainty}} + \\underbrace{Var_{W \\sim p(W \\mid \\mathcal{D})} [\\mu(W)]}_{\\text{Epistemic uncertainty}}$$\nThe predictive variance is the sum of two components:\n1.  **Aleatoric uncertainty**: The average of the variance predicted by the model across different weight configurations. This represents inherent noise in the data that the model cannot reduce.\n2.  **Epistemic uncertainty**: The variance in the model's mean predictions. This represents the model's own uncertainty about its parameters $W$ and can be reduced with more data.\n\nThe Monte Carlo estimator for $\\sigma_{pred}^2$, denoted $\\hat{\\sigma}_{pred}^2$, is obtained by approximating each term with its respective sample estimate:\n- The estimator for the expected aleatoric uncertainty is the sample mean of the variances $\\sigma_t^2$:\n$$ \\widehat{E}[\\sigma^2(W)] = \\frac{1}{T} \\sum_{t=1}^{T} \\sigma_t^2 $$\n- The estimator for the epistemic uncertainty is the sample variance of the means $\\mu_t$:\n$$ \\widehat{Var}[\\mu(W)] = \\frac{1}{T} \\sum_{t=1}^{T} (\\mu_t - \\hat{\\mu}_{pred})^2 $$\nCombining these, the Monte Carlo estimator for the predictive variance is:\n$$ \\hat{\\sigma}_{pred}^2 = \\left( \\frac{1}{T} \\sum_{t=1}^{T} \\sigma_t^2 \\right) + \\left( \\frac{1}{T} \\sum_{t=1}^{T} \\mu_t^2 - \\hat{\\mu}_{pred}^2 \\right) $$\nAn alternative way to express the total variance estimator is:\n$$ \\hat{\\sigma}_{pred}^2 = \\frac{1}{T} \\sum_{t=1}^{T} (\\mu_t^2 + \\sigma_t^2) - \\hat{\\mu}_{pred}^2 $$\n\n**2. Numerical Computation**\n\nThe provided data are:\n- $T = 5$\n- Realizations of $(\\mu_t, \\sigma_t^2)$:\n  - $t=1$: $(\\mu_1, \\sigma_1^2) = (1.10, 0.09)$\n  - $t=2$: $(\\mu_2, \\sigma_2^2) = (0.95, 0.16)$\n  - $t=3$: $(\\mu_3, \\sigma_3^2) = (1.20, 0.04)$\n  - $t=4$: $(\\mu_4, \\sigma_4^2) = (1.05, 0.09)$\n  - $t=5$: $(\\mu_5, \\sigma_5^2) = (1.15, 0.04)$\n\n**2.1. Compute Predictive Mean**\nUsing the derived estimator:\n$$ \\hat{\\mu}_{pred} = \\frac{1}{5} \\sum_{t=1}^{5} \\mu_t = \\frac{1}{5} (1.10 + 0.95 + 1.20 + 1.05 + 1.15) $$\n$$ \\hat{\\mu}_{pred} = \\frac{1}{5} (5.45) = 1.09 $$\n\n**2.2. Compute Predictive Variance**\nWe will compute the two components of variance separately.\n\nFirst, the average aleatoric uncertainty:\n$$ \\widehat{E}[\\sigma^2(W)] = \\frac{1}{5} \\sum_{t=1}^{5} \\sigma_t^2 = \\frac{1}{5} (0.09 + 0.16 + 0.04 + 0.09 + 0.04) $$\n$$ \\widehat{E}[\\sigma^2(W)] = \\frac{1}{5} (0.42) = 0.084 $$\n\nNext, the epistemic uncertainty (sample variance of the means):\n$$ \\widehat{Var}[\\mu(W)] = \\frac{1}{5} \\sum_{t=1}^{5} (\\mu_t - \\hat{\\mu}_{pred})^2 $$\n$$ \\widehat{Var}[\\mu(W)] = \\frac{1}{5} \\left( (1.10 - 1.09)^2 + (0.95 - 1.09)^2 + (1.20 - 1.09)^2 + (1.05 - 1.09)^2 + (1.15 - 1.09)^2 \\right) $$\n$$ \\widehat{Var}[\\mu(W)] = \\frac{1}{5} \\left( (0.01)^2 + (-0.14)^2 + (0.11)^2 + (-0.04)^2 + (0.06)^2 \\right) $$\n$$ \\widehat{Var}[\\mu(W)] = \\frac{1}{5} (0.0001 + 0.0196 + 0.0121 + 0.0016 + 0.0036) $$\n$$ \\widehat{Var}[\\mu(W)] = \\frac{1}{5} (0.037) = 0.0074 $$\n\nThe total predictive variance is the sum of these two components:\n$$ \\hat{\\sigma}_{pred}^2 = \\widehat{E}[\\sigma^2(W)] + \\widehat{Var}[\\mu(W)] = 0.084 + 0.0074 = 0.0914 $$\n\n**3. Final Results**\n\nThe computed predictive mean is $1.09$ and the predictive variance is $0.0914$. Rounding to four significant figures as requested:\n- Predictive mean: $1.090$\n- Predictive variance: $0.09140$",
            "answer": "$$\\boxed{\\begin{pmatrix} 1.090 & 0.09140 \\end{pmatrix}}$$"
        }
    ]
}