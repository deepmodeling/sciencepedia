{
    "hands_on_practices": [
        {
            "introduction": "Modern neural networks for medical image segmentation, such as the U-Net, rely on skip connections to fuse low-level and high-level feature maps. However, not all features passed through these connections are equally relevant. The following practice explores how to implement an additive attention gate, a powerful mechanism that learns to selectively focus on salient features while suppressing irrelevant information from the encoder path before it is merged with the decoder path. By working through this exercise , you will gain hands-on experience with a key architectural innovation and learn to quantify its effectiveness in modulating feature relevance.",
            "id": "4554527",
            "problem": "You are tasked with implementing and analyzing an additive attention gate for skip connections in a U-shaped Convolutional Neural Network (CNN) architecture for medical image segmentation, as featured in the 'Attention U-Net' architecture. The attention gate fuses encoder skip features with a decoder gating signal to suppress irrelevant activations before concatenation. The gate acts elementwise on feature vectors. Given skip features $x \\in \\mathbb{R}^d$, a decoder gating signal $s \\in \\mathbb{R}^d$, learnable linear maps $W_x \\in \\mathbb{R}^{d \\times d}$ and $W_g \\in \\mathbb{R}^{d \\times d}$, and bias $b \\in \\mathbb{R}^d$, compute the pre-activation $z = W_x x + W_g s + b$, the elementwise logistic sigmoid $g = \\sigma(z)$, and the attention-weighted skip features $y = g \\odot x$, where $\\sigma(u) = \\frac{1}{1 + e^{-u}}$ is applied elementwise and $\\odot$ denotes elementwise multiplication. Your program must implement this gate and quantify its effectiveness at suppressing irrelevant activations while preserving relevant ones.\n\nEffectiveness will be measured using the following quantities. Let $R \\subset \\{0,1,\\dots,d-1\\}$ be the index set of relevant feature positions and $I \\subset \\{0,1,\\dots,d-1\\}$ be the index set of irrelevant feature positions, with $R \\cap I = \\varnothing$ and $R \\cup I = \\{0,1,\\dots,d-1\\}$. Define the pre-gate energy in the relevant subset as $\\|x_R\\|_1 = \\sum_{i \\in R} |x_i|$, the post-gate energy as $\\|y_R\\|_1 = \\sum_{i \\in R} |y_i|$, and similarly for irrelevant subset $\\|x_I\\|_1$ and $\\|y_I\\|_1$. Define the preservation ratio $\\rho_R = \\frac{\\|y_R\\|_1}{\\|x_R\\|_1}$ and the suppression ratio $\\rho_I = \\frac{\\|y_I\\|_1}{\\|x_I\\|_1}$, both expressed as decimals (not percentages). If a denominator is zero, define the corresponding ratio to be $0$. The gate is deemed effective if $\\rho_I \\le \\tau_I$ and $\\rho_R \\ge \\tau_R$, where $\\tau_I$ is the suppression threshold and $\\tau_R$ is the preservation threshold.\n\nYour program must:\n- Implement the additive attention gate as described.\n- For each test case, compute $\\rho_I$, $\\rho_R$, and a boolean effectiveness decision according to thresholds $\\tau_I$ and $\\tau_R$.\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is the list $[\\rho_I,\\rho_R,\\text{decision}]$.\n\nUse the following test suite, each specified with dimension $d$, vectors $x$ and $s$, matrices $W_x$ and $W_g$, bias $b$, relevant indices $R$, irrelevant indices $I$, and thresholds $\\tau_I$ and $\\tau_R$.\n\nTest case $1$ (happy path, correlated gate signal aligns with relevant features):\n- $d = 8$\n- $x = [\\,1.2,\\,0.8,\\,1.0,\\,0.5,\\,0.3,\\,0.2,\\,0.1,\\,0.05\\,]$\n- $s = [\\,0.9,\\,0.7,\\,1.1,\\,-0.4,\\,-0.6,\\,-0.5,\\,-0.2,\\,-0.1\\,]$\n- $W_x = \\mathrm{diag}([\\,1.0,\\,1.0,\\,1.0,\\,1.0,\\,1.0,\\,1.0,\\,1.0,\\,1.0\\,])$\n- $W_g = \\mathrm{diag}([\\,1.0,\\,1.0,\\,1.0,\\,1.0,\\,1.0,\\,1.0,\\,1.0,\\,1.0\\,])$\n- $b = [\\,-0.1,\\,-0.1,\\,-0.1,\\,-0.1,\\,-0.1,\\,-0.1,\\,-0.1,\\,-0.1\\,]$\n- $R = \\{\\,0,\\,1,\\,2\\,\\}$, $I = \\{\\,3,\\,4,\\,5,\\,6,\\,7\\,\\}$\n- $\\tau_I = 0.6$, $\\tau_R = 0.7$\n\nTest case $2$ (boundary case, zero decoder signal and sign-inverting $W_x$ on some irrelevant features):\n- $d = 6$\n- $x = [\\,0.9,\\,0.7,\\,0.6,\\,0.4,\\,0.3,\\,0.2\\,]$\n- $s = [\\,0.0,\\,0.0,\\,0.0,\\,0.0,\\,0.0,\\,0.0\\,]$\n- $W_x = \\mathrm{diag}([\\,1.0,\\,1.0,\\,1.0,\\,1.0,\\,-1.0,\\,-1.0\\,])$\n- $W_g = \\mathrm{diag}([\\,0.0,\\,0.0,\\,0.0,\\,0.0,\\,0.0,\\,0.0\\,])$\n- $b = [\\,-0.3,\\,-0.3,\\,-0.3,\\,-0.3,\\,-0.3,\\,-0.3\\,]$\n- $R = \\{\\,0,\\,1,\\,2\\,\\}$, $I = \\{\\,3,\\,4,\\,5\\,\\}$\n- $\\tau_I = 0.6$, $\\tau_R = 0.7$\n\nTest case $3$ (saturation regime, strong positive gating on relevant and strong negative gating on irrelevant):\n- $d = 5$\n- $x = [\\,1.0,\\,1.2,\\,0.9,\\,0.4,\\,0.3\\,]$\n- $s = [\\,3.0,\\,2.5,\\,2.8,\\,-3.5,\\,-2.8\\,]$\n- $W_x = \\mathrm{diag}([\\,2.0,\\,2.0,\\,2.0,\\,2.0,\\,2.0\\,])$\n- $W_g = \\mathrm{diag}([\\,2.0,\\,2.0,\\,2.0,\\,2.0,\\,2.0\\,])$\n- $b = [\\,0.0,\\,0.0,\\,0.0,\\,0.0,\\,0.0\\,]$\n- $R = \\{\\,0,\\,1,\\,2\\,\\}$, $I = \\{\\,3,\\,4\\,\\}$\n- $\\tau_I = 0.6$, $\\tau_R = 0.7$\n\nTest case $4$ (degenerate gate relying solely on decoder signal and bias):\n- $d = 7$\n- $x = [\\,0.5,\\,0.6,\\,0.7,\\,0.2,\\,0.1,\\,0.05,\\,0.05\\,]$\n- $s = [\\,0.8,\\,0.9,\\,0.7,\\,-0.8,\\,-0.6,\\,-0.4,\\,-0.3\\,]$\n- $W_x = \\mathbf{0}_{7 \\times 7}$ (the $7 \\times 7$ zero matrix)\n- $W_g = \\mathrm{diag}([\\,1.0,\\,1.0,\\,1.0,\\,1.0,\\,1.0,\\,1.0,\\,1.0\\,])$\n- $b = [\\,-0.05,\\,-0.05,\\,-0.05,\\,-0.05,\\,-0.05,\\,-0.05,\\,-0.05\\,]$\n- $R = \\{\\,0,\\,1,\\,2\\,\\}$, $I = \\{\\,3,\\,4,\\,5,\\,6\\,\\}$\n- $\\tau_I = 0.6$, $\\tau_R = 0.7$\n\nTest case $5$ (conflicting signals, negative skip features partially aligned with positive decoder signal):\n- $d = 6$\n- $x = [\\,0.8,\\,0.9,\\,0.7,\\,-0.5,\\,-0.6,\\,0.4\\,]$\n- $s = [\\,0.7,\\,0.8,\\,0.6,\\,0.9,\\,-0.9,\\,-0.2\\,]$\n- $W_x = \\mathrm{diag}([\\,1.0,\\,1.0,\\,1.0,\\,1.0,\\,1.0,\\,1.0\\,])$\n- $W_g = \\mathrm{diag}([\\,1.0,\\,1.0,\\,1.0,\\,1.0,\\,1.0,\\,1.0\\,])$\n- $b = [\\,-0.1,\\,-0.1,\\,-0.1,\\,-0.1,\\,-0.1,\\,-0.1\\,]$\n- $R = \\{\\,0,\\,1,\\,2\\,\\}$, $I = \\{\\,3,\\,4,\\,5\\,\\}$\n- $\\tau_I = 0.6$, $\\tau_R = 0.7$\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element of the outer list corresponds to a test case and must be the inner list $[\\rho_I,\\rho_R,\\text{decision}]$. For example, if there are $3$ test cases, the output must look like $[[\\rho_I^{(1)},\\rho_R^{(1)},\\text{decision}^{(1)}],[\\rho_I^{(2)},\\rho_R^{(2)},\\text{decision}^{(2)}],[\\rho_I^{(3)},\\rho_R^{(3)},\\text{decision}^{(3)}]]$, with decimals and booleans expressed in standard programming notation.",
            "solution": "The problem requires the implementation and analysis of an additive attention gate, a mechanism commonly employed in U-shaped neural network architectures for medical image segmentation. The purpose of this gate is to modulate the feature maps passed through skip connections from the encoder to the decoder path. By learning to focus on salient features and suppress irrelevant ones, the attention gate improves the model's performance. The problem provides a precise mathematical formulation for the gate's operation and a set of metrics to quantify its effectiveness.\n\nThe core of the attention gate is defined by the following sequence of operations. Given a vector of skip features $x \\in \\mathbb{R}^d$ from the encoder and a gating signal vector $s \\in \\mathbb{R}^d$ from the corresponding decoder layer, the gate computes an intermediate pre-activation vector $z \\in \\mathbb{R}^d$. This is achieved through a linear combination of transformed versions of $x$ and $s$, plus a bias term $b \\in \\mathbb{R}^d$. The transformations are defined by learnable weight matrices $W_x \\in \\mathbb{R}^{d \\times d}$ and $W_g \\in \\mathbb{R}^{d \\times d}$. The equation is:\n$$z = W_x x + W_g s + b$$\nThe pre-activation vector $z$ is then passed through an element-wise logistic sigmoid function, $\\sigma(u) = \\frac{1}{1 + e^{-u}}$, to produce a vector of attention coefficients $g \\in \\mathbb{R}^d$. Each coefficient $g_i$ is in the range $(0, 1)$.\n$$g = \\sigma(z)$$\nFinally, the original skip features $x$ are modulated by these attention coefficients through element-wise multiplication (the Hadamard product, denoted by $\\odot$), yielding the attention-weighted output vector $y \\in \\mathbb{R}^d$:\n$$y = g \\odot x$$\nThis output $y$ is then typically concatenated with the decoder features.\n\nTo evaluate the gate's effectiveness, the feature vector's indices are partitioned into a set of relevant indices, $R$, and a set of irrelevant indices, $I$, such that $R \\cap I = \\varnothing$ and $R \\cup I = \\{0, 1, \\dots, d-1\\}$. The effectiveness is measured by comparing the \"energy\" of the features before and after gating, quantified by the $L_1$ norm.\n\nThe pre-gate energy for the irrelevant subset is $\\|x_I\\|_1 = \\sum_{i \\in I} |x_i|$, and the post-gate energy is $\\|y_I\\|_1 = \\sum_{i \\in I} |y_i|$. The suppression ratio $\\rho_I$ is the ratio of these energies:\n$$\\rho_I = \\frac{\\|y_I\\|_1}{\\|x_I\\|_1}$$\nA lower $\\rho_I$ indicates better suppression of irrelevant information.\n\nSimilarly, for the relevant subset, the pre-gate and post-gate energies are $\\|x_R\\|_1 = \\sum_{i \\in R} |x_i|$ and $\\|y_R\\|_1 = \\sum_{i \\in R} |y_i|$, respectively. The preservation ratio $\\rho_R$ is defined as:\n$$\\rho_R = \\frac{\\|y_R\\|_1}{\\|x_R\\|_1}$$\nA higher $\\rho_R$ signifies better preservation of salient features.\n\nThe problem specifies that if a denominator (i.e., $\\|x_I\\|_1$ or $\\|x_R\\|_1$) is zero, the corresponding ratio is defined as $0$. The gate is deemed effective if both $\\rho_I \\le \\tau_I$ and $\\rho_R \\ge \\tau_R$, where $\\tau_I$ and $\\tau_R$ are given thresholds.\n\nThe algorithmic procedure for each test case is as follows:\n1.  Represent all input vectors ($x$, $s$, $b$) and matrices ($W_x$, $W_g$) as numerical arrays. Note that since the weight matrices in the test cases are diagonal (or zero), the matrix-vector products $W_x x$ and $W_g s$ simplify to element-wise multiplication of the vectors $x$ and $s$ with the diagonal elements of $W_x$ and $W_g$, respectively.\n2.  Calculate the pre-activation vector $z$ according to the formula $z = W_x x + W_g s + b$.\n3.  Apply the element-wise sigmoid function to $z$ to obtain the attention coefficients $g$.\n4.  Compute the output vector $y$ by element-wise multiplication of $g$ and $x$.\n5.  Using the provided index sets $R$ and $I$, extract the relevant and irrelevant sub-vectors from both $x$ and $y$.\n6.  Calculate the four $L_1$ norms: $\\|x_R\\|_1$, $\\|y_R\\|_1$, $\\|x_I\\|_1$, and $\\|y_I\\|_1$.\n7.  Compute the ratios $\\rho_I$ and $\\rho_R$, correctly handling any potential division by zero as per the problem's rule.\n8.  Evaluate the boolean effectiveness condition by comparing $\\rho_I$ and $\\rho_R$ against their respective thresholds $\\tau_I$ and $\\tau_R$.\n9.  Format the resulting values $[\\rho_I, \\rho_R, \\text{decision}]$ into the final required output structure. This process is repeated for all test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and evaluates an additive attention gate for five test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1\n        {\n            \"d\": 8,\n            \"x\": np.array([1.2, 0.8, 1.0, 0.5, 0.3, 0.2, 0.1, 0.05]),\n            \"s\": np.array([0.9, 0.7, 1.1, -0.4, -0.6, -0.5, -0.2, -0.1]),\n            \"W_x_diag\": np.array([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]),\n            \"W_g_diag\": np.array([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]),\n            \"b\": np.array([-0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1]),\n            \"R\": [0, 1, 2],\n            \"I\": [3, 4, 5, 6, 7],\n            \"tau_I\": 0.6,\n            \"tau_R\": 0.7\n        },\n        # Test case 2\n        {\n            \"d\": 6,\n            \"x\": np.array([0.9, 0.7, 0.6, 0.4, 0.3, 0.2]),\n            \"s\": np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0]),\n            \"W_x_diag\": np.array([1.0, 1.0, 1.0, 1.0, -1.0, -1.0]),\n            \"W_g_diag\": np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0]),\n            \"b\": np.array([-0.3, -0.3, -0.3, -0.3, -0.3, -0.3]),\n            \"R\": [0, 1, 2],\n            \"I\": [3, 4, 5],\n            \"tau_I\": 0.6,\n            \"tau_R\": 0.7\n        },\n        # Test case 3\n        {\n            \"d\": 5,\n            \"x\": np.array([1.0, 1.2, 0.9, 0.4, 0.3]),\n            \"s\": np.array([3.0, 2.5, 2.8, -3.5, -2.8]),\n            \"W_x_diag\": np.array([2.0, 2.0, 2.0, 2.0, 2.0]),\n            \"W_g_diag\": np.array([2.0, 2.0, 2.0, 2.0, 2.0]),\n            \"b\": np.array([0.0, 0.0, 0.0, 0.0, 0.0]),\n            \"R\": [0, 1, 2],\n            \"I\": [3, 4],\n            \"tau_I\": 0.6,\n            \"tau_R\": 0.7\n        },\n        # Test case 4\n        {\n            \"d\": 7,\n            \"x\": np.array([0.5, 0.6, 0.7, 0.2, 0.1, 0.05, 0.05]),\n            \"s\": np.array([0.8, 0.9, 0.7, -0.8, -0.6, -0.4, -0.3]),\n            \"W_x_diag\": np.zeros(7), # Corresponds to a zero matrix\n            \"W_g_diag\": np.array([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]),\n            \"b\": np.array([-0.05, -0.05, -0.05, -0.05, -0.05, -0.05, -0.05]),\n            \"R\": [0, 1, 2],\n            \"I\": [3, 4, 5, 6],\n            \"tau_I\": 0.6,\n            \"tau_R\": 0.7\n        },\n        # Test case 5\n        {\n            \"d\": 6,\n            \"x\": np.array([0.8, 0.9, 0.7, -0.5, -0.6, 0.4]),\n            \"s\": np.array([0.7, 0.8, 0.6, 0.9, -0.9, -0.2]),\n            \"W_x_diag\": np.array([1.0, 1.0, 1.0, 1.0, 1.0, 1.0]),\n            \"W_g_diag\": np.array([1.0, 1.0, 1.0, 1.0, 1.0, 1.0]),\n            \"b\": np.array([-0.1, -0.1, -0.1, -0.1, -0.1, -0.1]),\n            \"R\": [0, 1, 2],\n            \"I\": [3, 4, 5],\n            \"tau_I\": 0.6,\n            \"tau_R\": 0.7\n        }\n    ]\n\n    results = []\n\n    def sigmoid(u):\n        return 1 / (1 + np.exp(-u))\n\n    for case in test_cases:\n        x, s, W_x_diag, W_g_diag, b = case[\"x\"], case[\"s\"], case[\"W_x_diag\"], case[\"W_g_diag\"], case[\"b\"]\n        R, I, tau_I, tau_R = case[\"R\"], case[\"I\"], case[\"tau_I\"], case[\"tau_R\"]\n\n        # 1. Compute pre-activation z = W_x*x + W_g*s + b\n        # Since W_x and W_g are diagonal, this simplifies to element-wise multiplication.\n        z = (W_x_diag * x) + (W_g_diag * s) + b\n\n        # 2. Compute attention coefficients g = sigma(z)\n        g = sigmoid(z)\n\n        # 3. Compute attention-weighted skip features y = g * x\n        y = g * x\n\n        # 4. Compute pre-gate and post-gate energies\n        x_R = x[R]\n        y_R = y[R]\n        x_I = x[I]\n        y_I = y[I]\n        \n        norm_x_R = np.sum(np.abs(x_R))\n        norm_y_R = np.sum(np.abs(y_R))\n        norm_x_I = np.sum(np.abs(x_I))\n        norm_y_I = np.sum(np.abs(y_I))\n\n        # 5. Compute ratios rho_R and rho_I\n        rho_R = norm_y_R / norm_x_R if norm_x_R != 0 else 0.0\n        rho_I = norm_y_I / norm_x_I if norm_x_I != 0 else 0.0\n\n        # 6. Determine effectiveness\n        is_effective = (rho_I = tau_I) and (rho_R >= tau_R)\n\n        results.append([rho_I, rho_R, is_effective])\n\n    # Final print statement in the exact required format.\n    # We construct the string manually to match the required format without spaces.\n    inner_results_str = [f\"[{res[0]},{res[1]},{str(res[2]).lower()}]\" for res in results]\n    final_output_str = f\"[{','.join(inner_results_str)}]\"\n    print(final_output_str.replace(\" \",\"\"))\n\nsolve()\n```"
        },
        {
            "introduction": "Transitioning from designing a single architectural component to training a complete, deep network introduces significant practical challenges, foremost among them being hardware limitations. For volumetric medical data, 3D architectures like the 3D U-Net are powerful but demand substantial GPU memory. This exercise  provides a crucial hands-on guide to a priori resource planning, walking you through the detailed calculation of a network's memory footprint—from model parameters and optimizer states to the activation maps retained for backpropagation—enabling you to determine the maximum feasible batch size for a given GPU budget.",
            "id": "4554578",
            "problem": "Consider a three-dimensional (3D) U-Net architecture for volumetric medical image segmentation, trained on a Graphics Processing Unit (GPU) with a total memory capacity of $12$ gigabytes (GB), where $1 \\, \\mathrm{GB} = 10^{9} \\, \\mathrm{bytes}$. The input patch has spatial dimensions $(128, 128, 128)$ voxels and a single input imaging modality channel. The U-Net uses the following structural assumptions:\n- The encoder consists of $4$ resolution levels followed by a bottleneck level, each encoder level having two $3 \\times 3 \\times 3$ convolutions with \"same\" padding and channels doubling at each downsampling: base channels $32$ at resolution level with spatial size $128^3$, then $64$ at $64^3$, $128$ at $32^3$, $256$ at $16^3$, and bottleneck $512$ at $8^3$.\n- Downsampling is by $2 \\times 2 \\times 2$ max pooling.\n- The decoder mirrors the encoder: each decoding stage first upsamples using a $2 \\times 2 \\times 2$ transposed convolution (stride $2$) to match the next higher spatial resolution, concatenates with the corresponding encoder feature map, and applies two $3 \\times 3 \\times 3$ convolutions. After concatenation, the two convolutions output the number of channels of that decoder stage ($256$ at $16^3$, $128$ at $32^3$, $64$ at $64^3$, $32$ at $128^3$).\n- The final output layer is a $1 \\times 1 \\times 1$ convolution mapping $32$ channels at $128^3$ to $K = 3$ segmentation classes.\n\nAssume training with Adaptive Moment Estimation (Adam) optimizer in single precision (float32). For memory accounting:\n- Each float32 tensor element occupies $4$ bytes.\n- During training, the output activations of every parameterized layer (all convolutions and transposed convolutions) are retained for backpropagation. Ignore the retained memory for pooling, concatenation, and any non-parameterized operations.\n- Count parameter tensors, their gradients, and both Adam first and second moment buffers, all in float32 and all resident on the GPU, so the per-parameter memory is $16$ bytes.\n- A fixed non-model overhead of $1 \\, \\mathrm{GB}$ is reserved for runtime, leaving the remainder of the $12 \\, \\mathrm{GB}$ budget for model parameters, optimizer states, and retained activations.\n\nLet the batch size be $B$. Using only the above assumptions and standard definitions of convolutional parameter counts and activation sizes, derive the total retained activation memory as a function of $B$, the total parameter plus optimizer memory, and determine the largest integer batch size $B$ that fits within the available GPU memory budget.\n\nAdditionally, justify at least two principled strategies to reduce memory consumption (for example, gradient checkpointing or mixed precision) and, for each, explicitly state how it asymptotically modifies the activation-memory term derived in this problem.\n\nExpress your final answer as the maximum integer $B$ that fits, with no units and no rounding instruction needed.",
            "solution": "The total available memory for the model and its training-related states is the total GPU memory minus the fixed overhead. Let $M_{GPU} = 12 \\, \\mathrm{GB} = 12 \\times 10^9 \\, \\mathrm{bytes}$ and the overhead $M_{OH} = 1 \\, \\mathrm{GB} = 1 \\times 10^9 \\, \\mathrm{bytes}$.\nThe available memory is $M_{avail} = M_{GPU} - M_{OH} = 11 \\times 10^9 \\, \\mathrm{bytes}$.\nThe total memory consumption during training comprises two main components:\n$1$. The memory for model parameters and optimizer states, which is independent of batch size. Let this be $M_{P}$.\n$2$. The memory for retained layer activations, which scales linearly with the batch size $B$. Let this be $M_{A}(B)$.\n\nThe constraint is $M_{P} + M_{A}(B) \\le M_{avail}$.\n\nWe will first calculate $M_{P}$, then derive the expression for $M_{A}(B)$, and finally solve for the maximum integer $B$.\n\n**1. Calculation of Parameter and Optimizer Memory ($M_{P}$)**\n\nThe memory cost per parameter is given as $16$ bytes, accounting for the parameter itself (float32, $4$ bytes), its gradient (float32, $4$ bytes), and the Adam optimizer's first and second moment estimates ( $2 \\times 4 = 8$ bytes).\nThe number of parameters for a convolutional layer with a kernel of size $k \\times k \\times k$, $C_{in}$ input channels, and $C_{out}$ output channels, including a bias term for each output channel, is given by $N_{params} = (k^3 \\cdot C_{in} + 1) \\cdot C_{out}$. For a transposed convolution, the formula is identical. We calculate the parameters for each layer.\n\nEncoder Path:\n- Level 1 ($128^3$): Two $3 \\times 3 \\times 3$ convolutions.\n  - Conv 1: $C_{in}=1$, $C_{out}=32$. $N_1 = (3^3 \\cdot 1 + 1) \\cdot 32 = 896$.\n  - Conv 2: $C_{in}=32$, $C_{out}=32$. $N_2 = (3^3 \\cdot 32 + 1) \\cdot 32 = 27,680$.\n- Level 2 ($64^3$): Two $3 \\times 3 \\times 3$ convolutions.\n  - Conv 1: $C_{in}=32$, $C_{out}=64$. $N_3 = (3^3 \\cdot 32 + 1) \\cdot 64 = 55,360$.\n  - Conv 2: $C_{in}=64$, $C_{out}=64$. $N_4 = (3^3 \\cdot 64 + 1) \\cdot 64 = 110,656$.\n- Level 3 ($32^3$): Two $3 \\times 3 \\times 3$ convolutions.\n  - Conv 1: $C_{in}=64$, $C_{out}=128$. $N_5 = (3^3 \\cdot 64 + 1) \\cdot 128 = 221,312$.\n  - Conv 2: $C_{in}=128$, $C_{out}=128$. $N_6 = (3^3 \\cdot 128 + 1) \\cdot 128 = 442,496$.\n- Level 4 ($16^3$): Two $3 \\times 3 \\times 3$ convolutions.\n  - Conv 1: $C_{in}=128$, $C_{out}=256$. $N_7 = (3^3 \\cdot 128 + 1) \\cdot 256 = 884,992$.\n  - Conv 2: $C_{in}=256$, $C_{out}=256$. $N_8 = (3^3 \\cdot 256 + 1) \\cdot 256 = 1,769,728$.\n- Bottleneck ($8^3$): Two $3 \\times 3 \\times 3$ convolutions.\n  - Conv 1: $C_{in}=256$, $C_{out}=512$. $N_9 = (3^3 \\cdot 256 + 1) \\cdot 512 = 3,539,456$.\n  - Conv 2: $C_{in}=512$, $C_{out}=512$. $N_{10} = (3^3 \\cdot 512 + 1) \\cdot 512 = 7,078,400$.\n\nDecoder Path:\n- Level 1 ($16^3$): One transposed conv, two regular convs.\n  - Transposed Conv: $C_{in}=512$, $C_{out}=256$ (standard U-Net practice to halve channels), kernel $2^3$. $N_{11} = (2^3 \\cdot 512 + 1) \\cdot 256 = 1,048,832$.\n  - Conv 1: Input is concatenation of upsampled map ($256$ ch) and encoder map ($256$ ch), so $C_{in}=512$. $C_{out}=256$. $N_{12} = (3^3 \\cdot 512 + 1) \\cdot 256 = 3,539,200$.\n  - Conv 2: $C_{in}=256$, $C_{out}=256$. $N_{13} = (3^3 \\cdot 256 + 1) \\cdot 256 = 1,769,728$.\n- Level 2 ($32^3$):\n  - Transposed Conv: $C_{in}=256$, $C_{out}=128$. $N_{14} = (2^3 \\cdot 256 + 1) \\cdot 128 = 262,272$.\n  - Conv 1: $C_{in}=128+128=256$, $C_{out}=128$. $N_{15} = (3^3 \\cdot 256 + 1) \\cdot 128 = 884,864$.\n  - Conv 2: $C_{in}=128$, $C_{out}=128$. $N_{16} = (3^3 \\cdot 128 + 1) \\cdot 128 = 442,496$.\n- Level 3 ($64^3$):\n  - Transposed Conv: $C_{in}=128$, $C_{out}=64$. $N_{17} = (2^3 \\cdot 128 + 1) \\cdot 64 = 65,600$.\n  - Conv 1: $C_{in}=64+64=128$, $C_{out}=64$. $N_{18} = (3^3 \\cdot 128 + 1) \\cdot 64 = 221,248$.\n  - Conv 2: $C_{in}=64$, $C_{out}=64$. $N_{19} = (3^3 \\cdot 64 + 1) \\cdot 64 = 110,656$.\n- Level 4 ($128^3$):\n  - Transposed Conv: $C_{in}=64$, $C_{out}=32$. $N_{20} = (2^3 \\cdot 64 + 1) \\cdot 32 = 16,416$.\n  - Conv 1: $C_{in}=32+32=64$, $C_{out}=32$. $N_{21} = (3^3 \\cdot 64 + 1) \\cdot 32 = 55,328$.\n  - Conv 2: $C_{in}=32$, $C_{out}=32$. $N_{22} = (3^3 \\cdot 32 + 1) \\cdot 32 = 27,680$.\n\nFinal Layer:\n- $1 \\times 1 \\times 1$ convolution: $C_{in}=32$, $C_{out}=3$. $N_{23} = (1^3 \\cdot 32 + 1) \\cdot 3 = 99$.\n\nTotal parameters $N_{P} = \\sum_{i=1}^{23} N_i = 22,575,299$.\nThe total memory for parameters and optimizer states is:\n$M_{P} = N_{P} \\times 16 \\, \\mathrm{bytes/param} = 22,575,299 \\times 16 = 361,204,784 \\, \\mathrm{bytes}$.\n\n**2. Calculation of Activation Memory ($M_{A}(B)$)**\n\nActivation memory is the sum of memory for the output tensors of all parameterized layers. Each float32 element occupies $4$ bytes. The size of an activation tensor is Batch $\\times$ Depth $\\times$ Height $\\times$ Width $\\times$ Channels. Let $V_d = d^3$ be the volume for a spatial dimension $d$. We sum the elements per sample across all layers.\n\n- At spatial size $128^3$:\n  - Encoder: 2 convs $\\times$ $32$ channels $\\implies 2 \\times V_{128} \\times 32$.\n  - Decoder: 1 T-conv $\\times$ $32$ ch, 2 convs $\\times$ $32$ ch $\\implies 3 \\times V_{128} \\times 32$.\n  - Final Layer: 1 conv $\\times$ $3$ channels $\\implies 1 \\times V_{128} \\times 3$.\n  - Total at $128^3$: $V_{128} \\times (2 \\cdot 32 + 3 \\cdot 32 + 3) = 2,097,152 \\times 163 = 341,835,776$ elements.\n\n- At spatial size $64^3$:\n  - Encoder: 2 convs $\\times$ $64$ channels $\\implies 2 \\times V_{64} \\times 64$.\n  - Decoder: 1 T-conv $\\times$ $64$ ch, 2 convs $\\times$ $64$ ch $\\implies 3 \\times V_{64} \\times 64$.\n  - Total at $64^3$: $V_{64} \\times (2 \\cdot 64 + 3 \\cdot 64) = 262,144 \\times 320 = 83,886,080$ elements.\n\n- At spatial size $32^3$:\n  - Encoder: 2 convs $\\times$ $128$ channels $\\implies 2 \\times V_{32} \\times 128$.\n  - Decoder: 1 T-conv $\\times$ $128$ ch, 2 convs $\\times$ $128$ ch $\\implies 3 \\times V_{32} \\times 128$.\n  - Total at $32^3$: $V_{32} \\times (2 \\cdot 128 + 3 \\cdot 128) = 32,768 \\times 640 = 20,971,520$ elements.\n\n- At spatial size $16^3$:\n  - Encoder: 2 convs $\\times$ $256$ channels $\\implies 2 \\times V_{16} \\times 256$.\n  - Decoder: 1 T-conv $\\times$ $256$ ch, 2 convs $\\times$ $256$ ch $\\implies 3 \\times V_{16} \\times 256$.\n  - Total at $16^3$: $V_{16} \\times (2 \\cdot 256 + 3 \\cdot 256) = 4,096 \\times 1280 = 5,242,880$ elements.\n\n- At spatial size $8^3$:\n  - Bottleneck: 2 convs $\\times$ $512$ channels $\\implies 2 \\times V_8 \\times 512$.\n  - Total at $8^3$: $V_8 \\times (2 \\cdot 512) = 512 \\times 1024 = 524,288$ elements.\n\nThe total number of activation elements stored per sample is the sum of these values:\n$E_{sample} = 341,835,776 + 83,886,080 + 20,971,520 + 5,242,880 + 524,288 = 452,460,544$.\n\nThe total activation memory for a batch of size $B$ is:\n$M_{A}(B) = B \\times E_{sample} \\times 4 \\, \\mathrm{bytes/element} = B \\times 452,460,544 \\times 4 = B \\times 1,809,842,176 \\, \\mathrm{bytes}$.\n\n**3. Determining the Maximum Batch Size ($B$)**\n\nWe use the memory constraint:\n$M_{P} + M_{A}(B) \\le M_{avail}$\n$361,204,784 + B \\times 1,809,842,176 \\le 11,000,000,000$\n$B \\times 1,809,842,176 \\le 11,000,000,000 - 361,204,784$\n$B \\times 1,809,842,176 \\le 10,638,795,216$\n$B \\le \\frac{10,638,795,216}{1,809,842,176} \\approx 5.8782$\n\nSince the batch size $B$ must be an integer, the largest possible value is $B=5$.\n\n**4. Strategies for Memory Reduction**\n\nTwo principled strategies to reduce memory consumption are gradient checkpointing and mixed-precision training.\n\n- **Gradient Checkpointing (Activation Recomputation)**: This technique reduces activation memory by not storing all intermediate activations during the forward pass. Instead, it stores only a subset of activations at designated \"checkpoint\" layers. During the backward pass, the activations for layers between checkpoints are recomputed on-the-fly, starting from the nearest checkpoint. This trades increased computation time for a significant reduction in memory.\n  - **Effect on Activation Memory**: The original activation memory term is $M_{A}(B) = B \\cdot C_{act}$, where $C_{act} = (\\sum_{i=1}^{L} |A_i|) \\cdot 4$ and $|A_i|$ is the number of elements in the activation tensor of layer $i$. With checkpointing, the new memory term becomes approximately $M'_{A}(B) = B \\cdot (C_{ckpt} + C_{seg}) \\cdot 4$, where $C_{ckpt}$ is the memory for the checkpointed activations themselves and $C_{seg}$ is the memory for the activations within the computationally heaviest segment between two checkpoints. If the network is divided into $k$ segments, the activation memory is roughly reduced by a factor proportional to $k$. Thus, instead of scaling with the total depth $L$ of the network, the activation memory scales with the depth of the largest segment, $\\max_k L_k$, plus the small overhead of the checkpoints themselves. This allows for a much larger batch size $B$.\n\n- **Mixed-Precision Training**: This involves using lower-precision numerical formats, typically 16-bit floating-point (FP16), for storing activations, gradients, and model weights, while maintaining a master copy of the weights in 32-bit floating-point (FP32) for stable weight updates.\n  - **Effect on Activation Memory**: This strategy directly modifies the memory cost per activation element. The original activation memory term is $M_{A}(B) = B \\cdot E_{sample} \\cdot 4$. By switching from FP32 ($4$ bytes) to FP16 ($2$ bytes) for activations, the per-element cost is halved. The new activation memory term becomes $M'_{A}(B) = B \\cdot E_{sample} \\cdot 2$. This represents a direct asymptotic modification: $M'_{A}(B) = \\frac{1}{2} M_{A}(B)$. This reduction by a factor of $2$ would theoretically allow for a batch size almost twice as large, assuming activation memory is the primary bottleneck after accounting for parameter and optimizer memory.",
            "answer": "$$\n\\boxed{5}\n$$"
        },
        {
            "introduction": "Once a model is trained, its clinical utility hinges on a rigorous and context-aware evaluation of its performance. In medical imaging, where tasks like lesion segmentation often involve extreme class imbalance (e.g., a few lesion pixels amidst millions of background pixels), standard metrics can be misleading. This final practice  delves into the construction and interpretation of two fundamental evaluation tools: the Receiver Operating Characteristic (ROC) curve and the Precision-Recall (PR) curve. You will learn to compute their respective areas under the curve (AUC) from raw model outputs and, critically, derive why the PR-AUC provides a more informative assessment of a model's performance in the face of the class imbalance prevalent in medical applications.",
            "id": "4554559",
            "problem": "A pixel-wise lesion segmentation model produces calibrated predicted probabilities $s_i \\in [0,1]$ for each pixel $i$ in a single image. Ground-truth labels are $y_i \\in \\{0,1\\}$, where $y_i = 1$ indicates lesion pixel and $y_i = 0$ indicates background. Consider the following $n = 20$ pixels, each given as $(s_i, y_i)$ in descending order of $s_i$:\n$(0.92, 1)$, $(0.90, 0)$, $(0.88, 0)$, $(0.85, 1)$, $(0.70, 0)$, $(0.60, 0)$, $(0.55, 0)$, $(0.40, 0)$, $(0.40, 0)$, $(0.35, 0)$, $(0.30, 0)$, $(0.25, 0)$, $(0.20, 0)$, $(0.18, 0)$, $(0.15, 0)$, $(0.12, 0)$, $(0.10, 0)$, $(0.08, 0)$, $(0.05, 0)$, $(0.02, 0)$. The total number of positives is $P = \\sum_i y_i$ and the total number of negatives is $N = n - P$.\n\nUsing only fundamental definitions that apply to binary decision rules induced by thresholding $s_i$ with a threshold $\\tau$, construct the Receiver Operating Characteristic - Area Under the Curve (ROC-AUC) and the Precision-Recall Area Under the Curve (PR-AUC) for this segmentation task. You must:\n- Define and use the True Positive Rate (TPR), False Positive Rate (FPR), Precision, and Recall in terms of true positives ($TP$), false positives ($FP$), true negatives ($TN$), and false negatives ($FN$).\n- Sweep the threshold $\\tau$ over the distinct score values to trace the ROC and PR curves.\n- Compute $ROC\\text{-}AUC$ as the area under the ROC curve and $PR\\text{-}AUC$ as the area under the PR curve, adhering to scientifically standard integration conventions for these curves.\n\nThen, starting from first principles about class prevalence and error normalization, provide a brief derivation-based explanation of why $PR\\text{-}AUC$ is more informative than $ROC\\text{-}AUC$ under extreme class imbalance in pixel-wise lesion segmentation.\n\nRound both areas to four significant figures. Report your final numerical answers as a single row matrix containing $ROC\\text{-}AUC$ followed by $PR\\text{-}AUC$.",
            "solution": "First, we determine the number of positive and negative samples from the provided data.\nThe positive labels ($y_i=1$) correspond to scores $s_i=0.92$ and $s_i=0.85$.\nThe total number of positive samples is $P = 2$.\nThe total number of negative samples is $N = n - P = 20 - 2 = 18$.\nThis dataset exhibits significant class imbalance, with $N \\gg P$.\n\nThe core metrics are defined as:\n- True Positives ($TP$): Number of positive samples correctly classified as positive ($y_i=1, s_i \\ge \\tau$).\n- False Positives ($FP$): Number of negative samples incorrectly classified as positive ($y_i=0, s_i \\ge \\tau$).\n- True Positive Rate ($TPR$) or Recall: $TPR = \\frac{TP}{P}$.\n- False Positive Rate ($FPR$): $FPR = \\frac{FP}{N}$.\n- Precision: $Precision = \\frac{TP}{TP+FP}$.\n\n**ROC Curve Construction and AUC Calculation**\nThe ROC curve plots $TPR$ against $FPR$ for varying thresholds $\\tau$. We construct the curve by processing the data points in descending order of their scores, which is already done. Each point represents a potential threshold.\nStarting with a threshold $\\tau > 0.92$, no pixels are classified as positive, so $TP=0$ and $FP=0$. This gives the starting point of the ROC curve: $(FPR, TPR) = (0, 0)$.\n\nWe iterate through the sorted data points:\n1. At $s_1=0.92$ ($y_1=1$): When the threshold drops below $0.92$, this pixel is now classified as positive. This is a true positive.\n   - $TP$ increases by $1$, $FP$ is unchanged. $TP=1, FP=0$.\n   - $TPR = TP/P = 1/2 = 0.5$.\n   - $FPR = FP/N = 0/18 = 0$.\n   - The curve moves from $(0,0)$ to $(0, 0.5)$.\n2. At $s_2=0.90$ ($y_2=0$): This is a false positive.\n   - $TP$ is unchanged, $FP$ increases by $1$. $TP=1, FP=1$.\n   - $TPR = 1/2 = 0.5$.\n   - $FPR = 1/18$.\n   - The curve moves from $(0, 0.5)$ to $(1/18, 0.5)$.\n3. At $s_3=0.88$ ($y_3=0$): This is a false positive.\n   - $TP$ is unchanged, $FP$ increases by $1$. $TP=1, FP=2$.\n   - $TPR = 1/2 = 0.5$.\n   - $FPR = 2/18 = 1/9$.\n   - The curve moves from $(1/18, 0.5)$ to $(2/18, 0.5)$.\n4. At $s_4=0.85$ ($y_4=1$): This is a true positive.\n   - $TP$ increases by $1$, $FP$ is unchanged. $TP=2, FP=2$.\n   - $TPR = 2/2 = 1$.\n   - $FPR = 2/18 = 1/9$.\n   - The curve moves from $(2/18, 0.5)$ to $(2/18, 1)$.\n5. For all subsequent points, $y_i=0$. Thus, as the threshold is lowered further, only $FP$ increases, from $2$ up to $18$. $TP$ remains fixed at $2$, so $TPR$ remains fixed at $1$.\n   - The curve moves horizontally from $(2/18, 1)$ to $(18/18, 1) = (1, 1)$.\n\nThe vertices of the ROC curve are $(0,0)$, $(0, 0.5)$, $(1/18, 0.5)$, $(2/18, 0.5)$, $(2/18, 1)$, and $(1, 1)$.\nThe $ROC\\text{-}AUC$ is the area under this curve, which is calculated using the trapezoidal rule. The area is the sum of the areas of trapezoids formed by consecutive points $(FPR_{k-1}, TPR_{k-1})$ and $(FPR_k, TPR_k)$:\n$ROC\\text{-}AUC = \\sum_{k} \\frac{TPR_{k-1} + TPR_k}{2} (FPR_k - FPR_{k-1})$.\nFor a curve composed of horizontal and vertical segments, this simplifies to summing the areas of rectangles under the horizontal segments.\n- Area under segment from $(0, 0.5)$ to $(1/18, 0.5)$: Area$_1 = (1/18 - 0) \\times 0.5 = 1/36$.\n- Area under segment from $(1/18, 0.5)$ to $(2/18, 0.5)$: Area$_2 = (2/18 - 1/18) \\times 0.5 = 1/36$.\n- Area under segment from $(2/18, 1)$ to $(1, 1)$: Area$_3 = (1 - 2/18) \\times 1 = 16/18$.\nTotal $ROC\\text{-}AUC = \\text{Area}_1 + \\text{Area}_2 + \\text{Area}_3 = \\frac{1}{36} + \\frac{1}{36} + \\frac{16}{18} = \\frac{2}{36} + \\frac{32}{36} = \\frac{34}{36} = \\frac{17}{18}$.\nNumerically, $ROC\\text{-}AUC = \\frac{17}{18} \\approx 0.94444...$. Rounded to four significant figures, this is $0.9444$.\n\n**PR Curve Construction and AUC Calculation**\nThe PR curve plots Precision against Recall ($=TPR$). A standard method for computing the area under the PR curve, especially in information retrieval and machine learning, is the Average Precision (AP) score. This method computes a weighted average of precisions, where each precision is calculated at each rank $k$ where a positive sample is found, and then averaged over the total number of positives. This is equivalent to a rectangular integration: $PR\\text{-}AUC = \\sum_{k} (R_k - R_{k-1}) P_k$, where $(R_k, P_k)$ are the recall and precision for the $k$-th point in the sorted list.\nThe formula simplifies to $PR\\text{-}AUC = \\frac{1}{P} \\sum_{k=1}^n \\text{Precision}(k) \\times I(y_k=1)$, where $I(y_k=1)$ is an indicator function that is $1$ if the sample at rank $k$ is positive and $0$ otherwise, and $\\text{Precision}(k)$ is the precision considering the top $k$ predictions.\n\nWe sum the precisions only at the ranks where a positive sample appears.\nThe positive samples are at rank $k=1$ and $k=4$.\n- At rank $k=1$ (for $s_1=0.92, y_1=1$):\n  - We have processed $1$ sample. $TP=1, FP=0$.\n  - Precision$(1) = \\frac{TP}{TP+FP} = \\frac{1}{1+0} = 1$.\n- At rank $k=4$ (for $s_4=0.85, y_4=1$):\n  - We have processed $4$ samples. At this point, we have encountered one positive ($s=0.92$) and two negatives ($s=0.90, s=0.88$). With the current sample, we have two positives total.\n  - $TP=2, FP=2$.\n  - Precision$(4) = \\frac{TP}{TP+FP} = \\frac{2}{2+2} = \\frac{2}{4} = 0.5$.\n\nNow, we compute the average precision:\n$PR\\text{-}AUC = \\frac{1}{P} (\\text{Precision}(1) + \\text{Precision}(4)) = \\frac{1}{2} (1 + 0.5) = \\frac{1.5}{2} = 0.75$.\nTo four significant figures, this is $0.7500$.\n\n**Part 2: Explanation of PR-AUC vs. ROC-AUC under Class Imbalance**\n\nIn pixel-wise lesion segmentation, the number of negative pixels (background, $N$) is typically orders of magnitude larger than the number of positive pixels (lesion, $P$), i.e., $N \\gg P$. We analyze why $PR\\text{-}AUC$ is more informative in this setting by examining the definitions of the metrics.\n\nThe ROC curve plots $TPR = \\frac{TP}{P}$ versus $FPR = \\frac{FP}{N}$. The key term here is $FPR$. Because it is normalized by the total number of negatives, $N$, the $FPR$ is insensitive to large changes in the absolute number of false positives ($FP$) when $N$ is very large. For a classifier to achieve a low $FPR$, it can afford to make a substantial number of false positive predictions. For example, if $N=10^6$, even with $1000$ false positives ($FP=1000$), the $FPR$ is only $\\frac{1000}{10^6} = 0.001$. An increase of $FP$ by a factor of $10$ to $10000$ would only increase $FPR$ to $0.01$. The ROC curve would remain close to the ideal point $(0,1)$, and the $ROC\\text{-}AUC$ would be high, suggesting excellent performance. This masks the fact that the model is generating thousands of false alarms, which would be unacceptable in a clinical setting.\n\nThe PR curve plots Precision $=\\frac{TP}{TP+FP}$ versus Recall ($TPR$). The Precision metric's denominator, $TP+FP$, is the total number of pixels predicted as positive. It does not involve the large number of true negatives, $TN$. Therefore, Precision is directly sensitive to the absolute number of false positives $FP$. Using the previous example, if we have $P=100$ and we achieve high recall ($TP \\approx 100$), a model generating $FP=1000$ would have a precision of $\\frac{100}{100+1000} \\approx 0.09$. A model with $FP=10000$ would have a precision of $\\frac{100}{100+10000} \\approx 0.01$. These large changes in $FP$ lead to dramatic drops in precision.\n\nConsequently, the PR curve and its area, $PR\\text{-}AUC$, directly reflect the trade-off between identifying all true lesions (Recall) and the fraction of predicted lesions that are correct (Precision). In a scenario with extreme class imbalance, a high $ROC\\text{-}AUC$ can be misleadingly optimistic, whereas a low $PR\\text{-}AUC$ would correctly indicate that the model's positive predictions are unreliable. Since the clinical utility of a segmentation model often depends on the reliability of its positive predictions (positive predictive value, which is Precision), the PR curve provides a more informative and realistic assessment of model performance under class imbalance.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 0.9444  0.7500 \\end{pmatrix}}\n$$"
        }
    ]
}