{
    "hands_on_practices": [
        {
            "introduction": "Evaluating a variant caller's performance requires more than a single, genome-wide accuracy score, as performance can vary dramatically across different genomic contexts. Regions containing features like homopolymers and tandem repeats are known to pose significant challenges to sequencing and alignment technologies. This exercise  provides a practical guide to implementing a stratified performance analysis, calculating a suite of key classification and calibration metrics from first principles to reveal how a model truly behaves in these difficult but important regions.",
            "id": "4554230",
            "problem": "You are analyzing a binary classifier for genomic variant calling that is trained with deep learning to predict whether a candidate locus contains a true variant. The classifier outputs, for each candidate locus indexed by $i$, a predicted probability $p_i \\in [0,1]$, a ground-truth binary label $y_i \\in \\{0,1\\}$, and a categorical region type $\\rho_i$ indicating the local genome context. The region types of interest are homopolymers, tandem repeats, and segmental duplications. Difficult regions such as homopolymers, tandem repeats, and segmental duplications are known to degrade the performance of sequencing and alignment, thereby stressing downstream variant callers, and performance should be stratified by region type.\n\nStarting from foundational statistical decision theory and the canonical binary classification framework, define a decision rule that maps each probability $p_i$ to a binary prediction $\\hat{y}_i \\in \\{0,1\\}$ using a single fixed threshold $t$ that minimizes $0$-$1$ decision loss under equal costs for false positives and false negatives. Then, using only the fundamental definitions of the confusion matrix counts true positives, false positives, true negatives, and false negatives, construct for each region type the following per-region quantities: positive predictive value (precision), sensitivity (recall), and the $F_1$ score. In addition, quantify probability quality via the mean squared error between $p_i$ and $y_i$ (the Brier score) and via the mean negative log-likelihood (the log-loss), both computed per region type. You must use a single common threshold $t$ across all regions.\n\nScientific realism constraint: use the canonical equal-cost threshold and standard per-region stratification. When computing log-loss, guard against undefined logarithms by applying an infinitesimal lower bound $\\epsilon$ to both $p_i$ and $1-p_i$ wherever needed.\n\nYour program must implement these computations exactly from first principles, without relying on pre-packaged metric functions. The threshold must be $t = 0.5$ and the logarithm must be the natural logarithm. Use $\\epsilon = 10^{-15}$ for numerical stability in log-loss.\n\nTest suite. For each test case, the data consist of three region-specific lists of $(p,y)$ pairs, one list per region type. The region types are written exactly as strings “homopolymer”, “tandem_repeat”, and “segmental_duplication”. All numbers are unitless probabilities and binary labels. The test cases are:\n\n- Test case $A$ (general “happy path” with moderate difficulty and near-boundary probabilities): \n  - homopolymer: $\\left[(0.9,1),(0.8,1),(0.2,0),(0.4,0),(0.6,1)\\right]$,\n  - tandem\\_repeat: $\\left[(0.7,1),(0.1,0),(0.3,0),(0.55,1),(0.49,0)\\right]$,\n  - segmental\\_duplication: $\\left[(0.6,0),(0.7,0),(0.2,0),(0.95,1),(0.4,1)\\right]$.\n- Test case $B$ (edge case with uniformly low probabilities and positives present, stressing recall): \n  - homopolymer: $\\left[(0.1,1),(0.2,1),(0.05,0)\\right]$,\n  - tandem\\_repeat: $\\left[(0.49,1),(0.49,0),(0.49,1)\\right]$,\n  - segmental\\_duplication: $\\left[(0.0,0),(0.0,1),(0.0,1)\\right]$.\n- Test case $C$ (boundary case with probabilities at $0$ and $1$, stressing calibration and numerical stability): \n  - homopolymer: $\\left[(1.0,1),(0.0,0)\\right]$,\n  - tandem\\_repeat: $\\left[(1.0,0),(0.0,1)\\right]$,\n  - segmental\\_duplication: $\\left[(1.0,1),(1.0,1),(0.0,0),(0.0,0)\\right]$.\n\nOutput specification. For each test case, compute for each region type, in the fixed order homopolymer, tandem\\_repeat, segmental\\_duplication, the following metrics in order: precision, recall, $F_1$, Brier score, and log-loss. If any metric has a zero denominator, define its value to be $0$. Round every metric to $6$ decimal places. Your program should produce a single line of output containing the results aggregated across all test cases and region types, as a comma-separated list of floats enclosed in square brackets, with the metrics ordered as described above. The output thus contains $45$ floating-point numbers: $3$ test cases times $3$ region types times $5$ metrics per region type. No other text should be printed.",
            "solution": "The problem requires a rigorous, first-principles-based evaluation of a binary classifier for genomic variant calling. The evaluation is to be stratified by three genomic region types: homopolymers, tandem repeats, and segmental duplications. We must first establish the decision rule from statistical first principles, then define and compute a set of standard performance metrics for given test data.\n\n### 1. Decision Rule from Statistical Decision Theory\n\nA binary classifier outputs a probability $p_i \\in [0,1]$ that a given locus $i$ belongs to the positive class (i.e., contains a true variant, $y_i=1$). Our task is to map this probability to a hard classification, $\\hat{y}_i \\in \\{0,1\\}$. This is a classic problem in statistical decision theory.\n\nThe goal is to find a decision rule that minimizes the expected loss. The problem specifies a $0$-$1$ loss function, where a correct classification has a loss of $0$ and an incorrect classification has a loss of $1$. It further specifies that the costs for false positives and false negatives are equal. Let this cost be $C > 0$. The cost of a true positive or a true negative is $0$.\n\nLet $p_i = P(y_i=1)$ be the probability that the true label is $1$. Consequently, $P(y_i=0) = 1-p_i$. We must choose an action $\\hat{y}_i$ from the set $\\{0,1\\}$. The expected loss, or risk, for each action is:\n\n1.  **Risk of predicting positive ($\\hat{y}_i=1$):** A loss occurs only if the true label is negative ($y_i=0$), which is a false positive. The expected loss is $R(\\hat{y}_i=1) = P(y_i=0) \\cdot C_{FP} = (1-p_i) \\cdot C$.\n2.  **Risk of predicting negative ($\\hat{y}_i=0$):** A loss occurs only if the true label is positive ($y_i=1$), which is a false negative. The expected loss is $R(\\hat{y}_i=0) = P(y_i=1) \\cdot C_{FN} = p_i \\cdot C$.\n\nThe optimal decision rule, known as the Bayes classifier for this loss function, is to choose the action that minimizes the expected loss. We predict $\\hat{y}_i=1$ if $R(\\hat{y}_i=1) < R(\\hat{y}_i=0)$:\n$$\n(1-p_i) \\cdot C < p_i \\cdot C\n$$\nSince $C > 0$, we can divide by $C$:\n$$\n1-p_i < p_i \\implies 1 < 2p_i \\implies p_i > 0.5\n$$\nThus, the decision rule that minimizes the $0$-$1$ loss with equal costs is to predict the positive class if its probability exceeds $0.5$. This corresponds to a threshold $t=0.5$. The problem statement provides this value, which is consistent with our derivation. The formal decision rule is:\n$$\n\\hat{y}_i = \\begin{cases} 1 & \\text{if } p_i > t \\\\ 0 & \\text{if } p_i \\le t \\end{cases}\n$$\nwhere $t=0.5$.\n\n### 2. Performance Metrics from First Principles\n\nFor each region type $\\rho$, we evaluate the classifier's performance by first calculating the counts of the four possible outcomes from the confusion matrix. Let $S_{\\rho}$ be the set of indices $i$ corresponding to loci in region type $\\rho$. Let $N_{\\rho} = |S_{\\rho}|$.\n\n-   **True Positives ($TP_{\\rho}$):** The number of correctly identified positive instances.\n    $$TP_{\\rho} = \\sum_{i \\in S_{\\rho}} \\mathbb{I}(y_i=1 \\text{ and } \\hat{y}_i=1)$$\n-   **False Positives ($FP_{\\rho}$):** The number of negative instances incorrectly identified as positive.\n    $$FP_{\\rho} = \\sum_{i \\in S_{\\rho}} \\mathbb{I}(y_i=0 \\text{ and } \\hat{y}_i=1)$$\n-   **True Negatives ($TN_{\\rho}$):** The number of correctly identified negative instances.\n    $$TN_{\\rho} = \\sum_{i \\in S_{\\rho}} \\mathbb{I}(y_i=0 \\text{ and } \\hat{y}_i=0)$$\n-   **False Negatives ($FN_{\\rho}$):** The number of positive instances incorrectly identified as negative.\n    $$FN_{\\rho} = \\sum_{i \\in S_{\\rho}} \\mathbb{I}(y_i=1 \\text{ and } \\hat{y}_i=0)$$\nwhere $\\mathbb{I}(\\cdot)$ is the indicator function.\n\nUsing these fundamental counts, we define the required metrics for each region $\\rho$.\n\n-   **Positive Predictive Value (Precision, $PPV_{\\rho}$):** The fraction of positive predictions that are correct.\n    $$PPV_{\\rho} = \\frac{TP_{\\rho}}{TP_{\\rho} + FP_{\\rho}}$$\n-   **Sensitivity (Recall, $Sens_{\\rho}$):** The fraction of actual positives that are correctly identified.\n    $$Sens_{\\rho} = \\frac{TP_{\\rho}}{TP_{\\rho} + FN_{\\rho}}$$\n-   **$F_1$ Score ($F_{1,\\rho}$):** The harmonic mean of precision and recall.\n    $$F_{1,\\rho} = 2 \\cdot \\frac{PPV_{\\rho} \\cdot Sens_{\\rho}}{PPV_{\\rho} + Sens_{\\rho}} = \\frac{2 TP_{\\rho}}{2 TP_{\\rho} + FP_{\\rho} + FN_{\\rho}}$$\nPer the problem specification, if the denominator of any of these metrics is zero, the value of the metric is defined as $0$.\n\nThe problem also requires metrics that assess the quality of the raw probability outputs, not just the hard classifications.\n\n-   **Brier Score ($BS_{\\rho}$):** The mean squared error between the predicted probabilities and the true binary labels. A lower score is better.\n    $$BS_{\\rho} = \\frac{1}{N_{\\rho}} \\sum_{i \\in S_{\\rho}} (p_i - y_i)^2$$\n-   **Log-Loss ($LL_{\\rho}$):** The mean negative log-likelihood of the true labels given the predicted probabilities. This metric heavily penalizes confident but incorrect predictions. A lower score is better. To avoid numerical instability from $\\log(0)$, we clip the probabilities to a small range $[\\epsilon, 1-\\epsilon]$. Let $\\tilde{p}_i = \\max(\\min(p_i, 1-\\epsilon), \\epsilon)$, where $\\epsilon=10^{-15}$.\n    $$LL_{\\rho} = -\\frac{1}{N_{\\rho}} \\sum_{i \\in S_{\\rho}} \\left[ y_i \\log(\\tilde{p}_i) + (1-y_i) \\log(1-\\tilde{p}_i) \\right]$$\n    The logarithm used is the natural logarithm ($\\log_e$ or $\\ln$).\n\n### 3. Computational Procedure\n\nThe solution is implemented by applying these definitions directly. For each of the three test cases, we iterate through the three specified region types. For each region's dataset of $(p_i, y_i)$ pairs:\n1.  The predicted labels $\\hat{y}_i$ are generated using the threshold $t=0.5$.\n2.  The counts $TP_{\\rho}$, $FP_{\\rho}$, $TN_{\\rho}$, and $FN_{\\rho}$ are computed.\n3.  The five metrics ($PPV_{\\rho}, Sens_{\\rho}, F_{1,\\rho}, BS_{\\rho}, LL_{\\rho}$) are calculated using the formulas above, with special handling for zero denominators as specified.\n4.  Each resulting metric value is rounded to $6$ decimal places.\n5.  All computed and rounded metrics from all test cases and regions are collected into a single list, maintaining the specified order. This list forms the final output.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_metrics(data, t, eps):\n    \"\"\"\n    Computes classification and calibration metrics from first principles.\n\n    Args:\n        data (list of tuples): A list of (probability, true_label) pairs.\n        t (float): The classification threshold.\n        eps (float): A small epsilon for clipping probabilities in log-loss.\n\n    Returns:\n        list of floats: A list containing [precision, recall, f1, brier, log_loss].\n    \"\"\"\n    if not data:\n        return [0.0] * 5\n\n    # Unpack probabilities and labels into numpy arrays for vectorized operations\n    p = np.array([item[0] for item in data], dtype=float)\n    y = np.array([item[1] for item in data], dtype=int)\n    N = len(y)\n\n    # 1. Apply decision rule to get predicted labels\n    y_hat = (p > t).astype(int)\n\n    # 2. Compute confusion matrix counts\n    TP = np.sum((y == 1) & (y_hat == 1))\n    FP = np.sum((y == 0) & (y_hat == 1))\n    TN = np.sum((y == 0) & (y_hat == 0))\n    FN = np.sum((y == 1) & (y_hat == 0))\n\n    # 3. Compute classification metrics\n    # Positive Predictive Value (Precision)\n    denom_prec = TP + FP\n    precision = TP / denom_prec if denom_prec > 0 else 0.0\n    \n    # Sensitivity (Recall)\n    denom_rec = TP + FN\n    recall = TP / denom_rec if denom_rec > 0 else 0.0\n\n    # F1 Score\n    denom_f1 = precision + recall\n    f1_score = 2 * (precision * recall) / denom_f1 if denom_f1 > 0 else 0.0\n\n    # 4. Compute probability quality metrics\n    # Brier Score (Mean Squared Error)\n    brier_score = np.mean((p - y)**2)\n\n    # Log-Loss (Mean Negative Log-Likelihood)\n    p_clipped = np.clip(p, eps, 1 - eps)\n    log_loss = -np.mean(y * np.log(p_clipped) + (1 - y) * np.log(1 - p_clipped))\n\n    return [precision, recall, f1_score, brier_score, log_loss]\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case A\n        {\n            \"homopolymer\": [(0.9, 1), (0.8, 1), (0.2, 0), (0.4, 0), (0.6, 1)],\n            \"tandem_repeat\": [(0.7, 1), (0.1, 0), (0.3, 0), (0.55, 1), (0.49, 0)],\n            \"segmental_duplication\": [(0.6, 0), (0.7, 0), (0.2, 0), (0.95, 1), (0.4, 1)],\n        },\n        # Test case B\n        {\n            \"homopolymer\": [(0.1, 1), (0.2, 1), (0.05, 0)],\n            \"tandem_repeat\": [(0.49, 1), (0.49, 0), (0.49, 1)],\n            \"segmental_duplication\": [(0.0, 0), (0.0, 1), (0.0, 1)],\n        },\n        # Test case C\n        {\n            \"homopolymer\": [(1.0, 1), (0.0, 0)],\n            \"tandem_repeat\": [(1.0, 0), (0.0, 1)],\n            \"segmental_duplication\": [(1.0, 1), (1.0, 1), (0.0, 0), (0.0, 0)],\n        }\n    ]\n\n    results = []\n    # Constants defined in the problem\n    threshold = 0.5\n    epsilon = 1e-15\n    region_order = [\"homopolymer\", \"tandem_repeat\", \"segmental_duplication\"]\n\n    # Iterate through all test cases and regions to compute metrics\n    for case_data in test_cases:\n        for region_name in region_order:\n            data_for_region = case_data[region_name]\n            metrics = calculate_metrics(data_for_region, threshold, epsilon)\n            \n            # Round each metric to 6 decimal places as required\n            rounded_metrics = [round(m, 6) for m in metrics]\n            results.extend(rounded_metrics)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "After establishing how to measure a model's performance, we turn to the fundamental source of errors: the sequencing data itself. This exercise  is a first-principles derivation that connects the ubiquitous Phred quality score, $Q$, to the expected rate of false positive variant calls. By working through this hypothetical scenario, you will develop a quantitative intuition for how sequencing noise at the per-base level aggregates into significant challenges at the genome-wide scale.",
            "id": "4554280",
            "problem": "A deep neural network (DNN) basecaller in a variant calling pipeline outputs well-calibrated Phred quality scores for base calls across a genome. Consider a dataset consisting of $L = 10^{8}$ distinct genomic positions, each covered by $c = 30$ independent reads from a sample with no true single-nucleotide variants (SNVs). Assume the basecaller assigns an average Phred quality of $Q = 35$ to all base calls, and that per-read errors are independent and identically distributed across reads and positions.\n\nFundamental definitions:\n- The Phred quality score $Q$ relates to the per-base error probability $p$ via $Q = -10 \\log_{10}(p)$.\n- A single-nucleotide variant (SNV) is declared by a naive thresholding rule if, at any position, the observed non-reference read count is at least $k = \\lceil 0.1 c \\rceil$.\n\nStarting from these definitions and assuming no true variation in the sample, derive from first principles the expected number of false positive SNV calls produced by the naive thresholding rule across the $L$ positions. Your derivation should be scientifically sound and must not assume any additional shortcut formulas. Round your final numerical answer to three significant figures. Express the final count as a unitless quantity.",
            "solution": "The user-provided problem has been assessed and is valid. It is scientifically grounded in the principles of genomics and probability theory, well-posed with all necessary parameters defined, and phrased objectively. The derivation proceeds from first principles as requested.\n\nThe derivation for the expected number of false positive single-nucleotide variant (SNV) calls is structured as follows:\nFirst, we calculate the per-base error probability ($p$) from the given Phred quality score ($Q$).\nSecond, we determine the minimum number of non-reference reads ($k$) required to call an SNV.\nThird, we model the number of erroneous reads at a single genomic position using a binomial distribution.\nFourth, we calculate the probability of a false positive call at a single site ($p_{FP}$), using a Poisson approximation to the binomial distribution, which is justified by the parameters.\nFinally, we calculate the expected total number of false positive calls across all ($L$) positions.\n\nStep 1: Derivation of the Per-Base Error Probability ($p$)\nThe Phred quality score $Q$ is defined in relation to the error probability $p$ by the formula:\n$$Q = -10 \\log_{10}(p)$$\nGiven $Q = 35$, we can solve for $p$:\n$$35 = -10 \\log_{10}(p)$$\n$$\\log_{10}(p) = -\\frac{35}{10} = -3.5$$\n$$p = 10^{-3.5}$$\n\nStep 2: Determination of the SNV Call Threshold ($k$)\nAn SNV is called if the number of non-reference reads at a position is at least $k$, where $k$ is given by the ceiling of $10\\%$ of the coverage $c$.\nGiven $c = 30$:\n$$k = \\lceil 0.1 \\times c \\rceil = \\lceil 0.1 \\times 30 \\rceil = \\lceil 3 \\rceil = 3$$\nThus, a false positive SNV is declared if $3$ or more reads at a given position contain a sequencing error, as the problem states there are no true variants.\n\nStep 3: Probabilistic Model for Errors at a Single Position\nAt any single genomic position, there are $c = 30$ independent reads. Since the sample has no true SNVs, any base that does not match the reference is an error. The probability of such an error in any single read is $p = 10^{-3.5}$.\nThe number of erroneous reads at a given position, which we denote by the random variable $X$, follows a binomial distribution. This is because each of the $c$ reads represents an independent Bernoulli trial with a success (error) probability of $p$.\nTherefore, $X \\sim \\text{Binomial}(n=c, p)$, where $n=30$. The probability mass function is:\n$$P(X=i) = \\binom{c}{i} p^i (1-p)^{c-i}$$\n\nStep 4: Probability of a False Positive at a Single Position ($p_{FP}$)\nA false positive occurs if the number of observed error reads $X$ is greater than or equal to the threshold $k=3$. The probability of this event, $p_{FP}$, is:\n$$p_{FP} = P(X \\ge 3) = \\sum_{i=3}^{30} \\binom{30}{i} p^i (1-p)^{30-i}$$\nDirect computation of this sum is cumbersome. However, since $p = 10^{-3.5}$ is very small ($p \\approx 3.16 \\times 10^{-4}$) and $c = 30$ is moderately large, the binomial distribution can be accurately approximated by a Poisson distribution. The parameter $\\lambda$ of the Poisson distribution is the mean of the binomial, $\\lambda = c \\times p$.\n$$\\lambda = 30 \\times 10^{-3.5}$$\nLet $Y$ be a Poisson-distributed random variable, $Y \\sim \\text{Poisson}(\\lambda)$. The probability of a false positive is approximated as:\n$$p_{FP} \\approx P(Y \\ge 3) = 1 - P(Y < 3) = 1 - \\sum_{i=0}^{2} P(Y=i)$$\n$$p_{FP} \\approx 1 - \\left( \\frac{e^{-\\lambda}\\lambda^0}{0!} + \\frac{e^{-\\lambda}\\lambda^1}{1!} + \\frac{e^{-\\lambda}\\lambda^2}{2!} \\right) = 1 - e^{-\\lambda} \\left( 1 + \\lambda + \\frac{\\lambda^2}{2} \\right)$$\nSince $\\lambda = 30 \\times 10^{-3.5} \\ll 1$, we can use the Taylor series expansion for $e^{-\\lambda}$ around $\\lambda=0$, which is $e^{-\\lambda} = 1 - \\lambda + \\frac{\\lambda^2}{2} - \\frac{\\lambda^3}{6} + O(\\lambda^4)$. Substituting this into the expression for $1 - p_{FP}$:\n$$1 - p_{FP} \\approx \\left(1 - \\lambda + \\frac{\\lambda^2}{2} - \\frac{\\lambda^3}{6} + \\dots\\right) \\left(1 + \\lambda + \\frac{\\lambda^2}{2}\\right)$$\nExpanding and keeping terms up to order $\\lambda^3$:\n$$1 - p_{FP} \\approx 1 - \\lambda + \\frac{\\lambda^2}{2} - \\frac{\\lambda^3}{6} + \\lambda - \\lambda^2 + \\frac{\\lambda^3}{2} + \\frac{\\lambda^2}{2} - \\frac{\\lambda^3}{2} + \\dots$$\n$$1 - p_{FP} \\approx 1 - \\frac{\\lambda^3}{6} + O(\\lambda^4)$$\nThus, for very small $\\lambda$, the probability of a false positive is well-approximated by the first term of the sum $P(Y \\ge 3)$, which is $P(Y=3)$:\n$$p_{FP} \\approx \\frac{\\lambda^3}{6}$$\n\nStep 5: Expected Total Number of False Positives\nThe total number of false positive calls, $N_{FP}$, across $L = 10^8$ independent genomic positions is a random variable. By the linearity of expectation, the expected number of false positives is the total number of positions multiplied by the probability of a false positive at any single position.\n$$E[N_{FP}] = L \\times p_{FP}$$\nSubstituting our approximation for $p_{FP}$:\n$$E[N_{FP}] \\approx L \\times \\frac{\\lambda^3}{6} = L \\times \\frac{(cp)^3}{6}$$\n\nStep 6: Numerical Calculation\nWe substitute the given values into the derived expression: $L=10^8$, $c=30$, $p=10^{-3.5}$, and $k=3$.\n$$E[N_{FP}] \\approx \\frac{10^8 \\times (30 \\times 10^{-3.5})^3}{3!} = \\frac{10^8 \\times 30^3 \\times (10^{-3.5})^3}{6}$$\n$$E[N_{FP}] \\approx \\frac{10^8 \\times 27000 \\times 10^{-10.5}}{6} = 10^8 \\times 4500 \\times 10^{-10.5}$$\n$$E[N_{FP}] \\approx (4.5 \\times 10^3) \\times 10^8 \\times 10^{-10.5} = 4.5 \\times 10^{3+8-10.5} = 4.5 \\times 10^{0.5}$$\n$$E[N_{FP}] = 4.5 \\sqrt{10}$$\nNow, we compute the numerical value:\n$$E[N_{FP}] \\approx 4.5 \\times 3.16227766 \\approx 14.230249...$$\nRounding the result to three significant figures gives $14.2$.",
            "answer": "$$ \\boxed{14.2} $$"
        },
        {
            "introduction": "In bioinformatics, correctly identifying a biological event is only half the battle; representing it accurately is the other. The same variant can often be described in multiple, computationally distinct but haplotype-equivalent ways, a nuance that can confound naive evaluation methods. This problem  explores the critical concepts of variant normalization and representation, demonstrating why a deep understanding of these data standards is essential for avoiding artificially inflated error rates and achieving fair benchmarking.",
            "id": "4554271",
            "problem": "A genomic variant can be modeled as a finite sequence of edit operations applied to a reference haplotype. Let the local reference substring be defined by $R[i..i+2] = \\text{\"GAT\"}$ at $1$-based coordinate $i$, embedded in a nonrepetitive context so that left-alignment is unique. A sequencing read pileup and assembled haplotype suggest that this locus differs from the reference by an adjacent Single Nucleotide Variant (SNV) and a deletion of length $2$ base pairs, yielding the alternate haplotype substring $\\text{\"T\"}$ at the same locus after all edits. Using the following bases:\n- The edit semantics are that a SNV replaces a single base at its coordinate, and a deletion removes a contiguous block of bases, each expressed in Variant Call Format (VCF) with an anchor base so that reference and alternate alleles share at least the leftmost base.\n- Two representations are considered equivalent if, after applying the edits to $R$, they produce the same alternate haplotype string.\n\nFrom these definitions and the Central Dogma of Molecular Biology (DNA to RNA to protein) as the biological basis for why sequence edits matter, and using widely accepted normalization practices (left-alignment and minimal representation of indels), select all statements that are correct about constructing equivalent representations and their implications for deep learning variant callers trained on pileup images with Convolutional Neural Networks (CNNs).\n\nOptions:\nA. A single-record complex allele with VCF fields $\\,\\text{POS}=i,\\ \\text{REF}=\\text{\"GAT\"},\\ \\text{ALT}=\\text{\"T\"}\\,$ is haplotype-equivalent to decomposing into primitive events consisting of an SNV at $i$ with $\\,\\text{REF}=\\text{\"G\"},\\ \\text{ALT}=\\text{\"T\"}\\,$ and a deletion of length $2$ bases starting at $i$ expressed with an anchor base as $\\,\\text{POS}=i,\\ \\text{REF}=\\text{\"GAT\"},\\ \\text{ALT}=\\text{\"G\"}\\,$.\n\nB. It is valid to represent this adjacent SNV plus $2$ base pair deletion as a single Multi-Nucleotide Polymorphism (MNP), because the anchor base guarantees the alternate allele can be expressed with the same length as the reference.\n\nC. If a deep learning caller is trained to emit primitive events (separate SNVs and indels) but evaluation truth sets encode such loci as single block substitutions, failure to normalize (e.g., left-align and merge into minimal block form) before comparison will inflate both false positives and false negatives due to representation mismatch, even when the predicted haplotype is correct.\n\nD. In repetitive sequence contexts, left-alignment can shift the start coordinate of the deletion, and merging the adjacent SNV with that deletion into a block substitution can move the effective change point, so naive per-locus classification can disagree across representations; haplotype-aware callers mitigate this by modeling the equivalence class of edits that yield the same alternate haplotype.\n\nE. Graph-based variant callers inherently produce a unique canonical representation for such complex events, making downstream normalization unnecessary for fair evaluation and training alignment across datasets.\n\nSelect all correct options.",
            "solution": "The problem statement is critically evaluated for validity prior to any attempt at a solution.\n\n**Step 1: Extract Givens**\n-   Local reference substring: $R[i..i+2] = \\text{\"GAT\"}$ at $1$-based coordinate $i$.\n-   Sequence context: Nonrepetitive, ensuring unique left-alignment.\n-   Observed event: An adjacent Single Nucleotide Variant (SNV) and a deletion of length $2$ base pairs.\n-   Resulting alternate haplotype substring: $\\text{\"T\"}$.\n-   Edit semantics:\n    -   An SNV replaces a single base.\n    -   A deletion removes a contiguous block of bases.\n    -   Variant Call Format (VCF) representation uses an anchor base such that reference (REF) and alternate (ALT) alleles share at least the leftmost base.\n-   Equivalence condition: Two representations are equivalent if applying the edits to the reference $R$ produces the same alternate haplotype string.\n-   Biological context: Central Dogma of Molecular Biology (DNA to RNA to protein).\n-   Standard practices: Left-alignment and minimal representation of indels.\n-   Technical context: Deep learning variant callers using Convolutional Neural Networks (CNNs) on pileup images.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientifically Grounded**: The problem is well-grounded in the established principles of bioinformatics, genomics, and computational biology. The concepts of SNVs, deletions, variant normalization (left-alignment, minimal representation), haplotype equivalence, VCF format, and the application of deep learning to variant calling are all standard and factual within the field. The provided example of a complex variant is realistic.\n-   **Well-Posed**: The problem is well-posed. It provides a clear reference state ($\\text{\"GAT\"}$), a clear final state ($\\text{\"T\"}$), and a set of rules (VCF semantics, equivalence) under which statements are to be evaluated. It asks for a qualitative analysis of these representations and their implications, which is a standard form of conceptual problem in science and engineering.\n-   **Objective**: The problem uses precise, standard, and objective terminology from genetics and bioinformatics. There is no subjective or ambiguous language in the problem setup.\n\n**Step 3: Verdict and Action**\n-   The problem statement is valid. It is scientifically sound, well-posed, objective, and presents a non-trivial, relevant scenario in modern bioinformatics. The solution process will now proceed.\n\n**Derivation of Principles**\n\nThe core of the problem is the representation of a genomic variant where the reference sequence $\\text{\"GAT\"}$ at a locus beginning at coordinate $i$ is altered to become the sequence $\\text{\"T\"}$. This net change can be described in multiple ways, whose equivalence is determined by whether they produce the same final haplotype.\n\n1.  **Block Substitution / Complex Allele Representation**:\n    The most direct and minimal representation of the net change is a single VCF record that describes the full alteration.\n    -   Reference allele (REF): $\\text{\"GAT\"}$\n    -   Alternate allele (ALT): $\\text{\"T\"}$\n    -   VCF record: $\\text{POS}=i, \\text{REF}=\\text{\"GAT\"}, \\text{ALT}=\\text{\"T\"}$.\n    This form is considered the canonical representation after normalization, as it parsimoniously describes the net effect. The length of REF is $3$, and the length of ALT is $1$.\n\n2.  **Decomposed Primitive Representation**:\n    The problem describes the event as an \"adjacent SNV and a deletion of length $2$ base pairs\". We can represent these as separate primitive events.\n    -   **SNV**: The first base, $\\text{'G'}$, is substituted by $\\text{'T'}$. The VCF record for this SNV at coordinate $i$ is $\\text{POS}=i, \\text{REF}=\\text{\"G\"}, \\text{ALT}=\\text{\"T\"}$.\n    -   **Deletion**: The subsequent $2$ bases, $\\text{\"AT\"}$, are deleted. This deletion begins at coordinate $i+1$. To represent this in VCF, which requires an anchor base, we must include the base preceding the deletion. The preceding base is $\\text{'G'}$ at coordinate $i$. Thus, the reference allele for the deletion record becomes $\\text{\"GAT\"}$ and the alternate allele, with $\\text{\"AT\"}$ removed, becomes $\\text{\"G\"}$.\n    -   VCF record for deletion: $\\text{POS}=i, \\text{REF}=\\text{\"GAT\"}, \\text{ALT}=\\text{\"G\"}$.\n    To check for haplotype equivalence, we apply both transformations to the reference. The SNV changes the base $\\text{'G'}$ at $i$ to $\\text{'T'}$. The deletion removes the bases $\\text{'A'}$ and $\\text{'T'}$ at $i+1$ and $i+2$. The combined effect is the replacement of $\\text{\"GAT\"}$ with $\\text{\"T\"}$. Thus, the set of two primitive records is haplotype-equivalent to the single complex record.\n\n3.  **Variant Normalization and its Implications for Machine Learning**:\n    Variant normalization is the process of converting different, but equivalent, variant representations into a single, canonical form. This typically involves left-alignment and merging adjacent primitive events into the most parsimonious (minimal) complex representation.\n    When training or evaluating a variant caller, especially a machine learning model, discrepancies in representation between the call set (predictions) and the truth set (labels) can lead to incorrect performance metrics. For example, if a model predicts two primitive variants and the truth set contains one complex variant, a simple VCF comparison would count $2$ false positives and $1$ false negative, even though the underlying predicted haplotype is correct. This necessitates a haplotype-aware comparison or normalization of both VCF files prior to evaluation.\n\n**Option-by-Option Analysis**\n\n**A. A single-record complex allele with VCF fields $\\,\\text{POS}=i,\\ \\text{REF}=\\text{\"GAT\"},\\ \\text{ALT}=\\text{\"T\"}\\,$ is haplotype-equivalent to decomposing into primitive events consisting of an SNV at $i$ with $\\,\\text{REF}=\\text{\"G\"},\\ \\text{ALT}=\\text{\"T\"}\\,$ and a deletion of length $2$ bases starting at $i$ expressed with an anchor base as $\\,\\text{POS}=i,\\ \\text{REF}=\\text{\"GAT\"},\\ \\text{ALT}=\\text{\"G\"}\\,$.**\n-   **Analysis**: As derived above, the single complex allele `(POS=i, REF=\"GAT\", ALT=\"T\")` transforms the reference substring $\\text{\"GAT\"}$ into $\\text{\"T\"}$. The pair of primitive events consists of an SNV `(POS=i, REF=\"G\", ALT=\"T\")` and a deletion `(POS=i, REF=\"GAT\", ALT=\"G\")`. Applying these two events to the reference results in the same outcome: the $\\text{'G'}$ at $i$ is replaced by $\\text{'T'}$, and the $\\text{\"AT\"}$ starting at $i+1$ is removed. The final haplotype substring is $\\text{\"T\"}$. The two representations are therefore haplotype-equivalent. The VCF representations given are also consistent with standard conventions.\n-   **Verdict**: Correct.\n\n**B. It is valid to represent this adjacent SNV plus $2$ base pair deletion as a single Multi-Nucleotide Polymorphism (MNP), because the anchor base guarantees the alternate allele can be expressed with the same length as the reference.**\n-   **Analysis**: A Multi-Nucleotide Polymorphism (MNP) is a type of variant where multiple nucleotides are substituted, meaning the length of the reference allele is equal to the length of the alternate allele, and this length is greater than $1$. In our case, the change is from $\\text{\"GAT\"}$ to $\\text{\"T\"}$, so $\\text{length(REF)} = 3$ and $\\text{length(ALT)} = 1$. Since $\\text{length(REF)} \\neq \\text{length(ALT)}$, this event is not an MNP. It is a complex indel or block substitution. The premise that the anchor base \"guarantees the alternate allele can be expressed with the same length as the reference\" is fundamentally incorrect. The VCF anchor base convention is specifically used to represent indels, where by definition, $\\text{length(REF)} \\neq \\text{length(ALT)}$.\n-   **Verdict**: Incorrect.\n\n**C. If a deep learning caller is trained to emit primitive events (separate SNVs and indels) but evaluation truth sets encode such loci as single block substitutions, failure to normalize (e.g., left-align and merge into minimal block form) before comparison will inflate both false positives and false negatives due to representation mismatch, even when the predicted haplotype is correct.**\n-   **Analysis**: This statement accurately describes a critical issue in bioinformatics benchmarking. If the caller outputs the two primitive events from option A, and the truth set contains the single complex event, a naive text-based or position-based comparison will find no matching records. The two primitive calls will be flagged as false positives (FPs), and the single complex call will be flagged as a false negative (FN). This happens despite the caller having correctly inferred the underlying biological sequence (the haplotype). Normalization of both the call set and the truth set into a canonical representation is the standard procedure required to prevent this artificial inflation of error rates and achieve a fair evaluation.\n-   **Verdict**: Correct.\n\n**D. In repetitive sequence contexts, left-alignment can shift the start coordinate of the deletion, and merging the adjacent SNV with that deletion into a block substitution can move the effective change point, so naive per-locus classification can disagree across representations; haplotype-aware callers mitigate this by modeling the equivalence class of edits that yield the same alternate haplotype.**\n-   **Analysis**: This statement correctly generalizes the issues of representation to repetitive contexts. In a repeat, an indel like $\\text{...TATATA...} \\to \\text{...TATA...}$ can be represented at multiple positions. Left-alignment is the convention to shift it to the leftmost possible coordinate. When an SNV is adjacent to such a mobile indel, the coordinates of the merged complex allele depend on the indel's aligned position. A naive classifier trained on pileup images centered at a specific coordinate (a specific representation) may fail if presented with an equivalent but shifted representation. Haplotype-aware methods, by assembling local sequences first and then calling variants based on the assembled haplotype, are intrinsically robust to these representational ambiguities. They compare the entire haplotype sequence, not the specific VCF encoding of the difference.\n-   **Verdict**: Correct.\n\n**E. Graph-based variant callers inherently produce a unique canonical representation for such complex events, making downstream normalization unnecessary for fair evaluation and training alignment across datasets.**\n-   **Analysis**: While graph-based methods are powerful in resolving haplotypes in complex regions, the final step of representing the difference between the chosen graph path (haplotype) and the reference path as a series of VCF records is not standardized across all tools. Different graph callers may employ different algorithms for this \"variant-unfurling\" step, potentially leading to different, though haplotype-equivalent, VCF representations. Therefore, asserting that they *inherently* produce a unique canonical form that obviates all downstream normalization is incorrect. For rigorous, tool-agnostic evaluation against a canonical truth set, a normalization step is still the best practice and often a necessity.\n-   **Verdict**: Incorrect.",
            "answer": "$$\\boxed{ACD}$$"
        }
    ]
}