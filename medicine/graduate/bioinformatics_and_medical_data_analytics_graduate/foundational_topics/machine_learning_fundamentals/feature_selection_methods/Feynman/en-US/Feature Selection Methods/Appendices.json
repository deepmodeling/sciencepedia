{
    "hands_on_practices": [
        {
            "introduction": "To build robust feature selection pipelines, we must first understand their limitations. This exercise explores the classic \"XOR\" problem, a scenario where features are powerfully predictive together but appear useless when examined individually . By demonstrating how a univariate metric like Mutual Information fails in this context, you will gain a crucial understanding of why multivariate interactions are a fundamental challenge in bioinformatics.",
            "id": "4563606",
            "problem": "You are analyzing a synthetic cohort in a genome-wide association study, where two binary Single Nucleotide Polymorphism (SNP) features $X_{1}$ and $X_{2}$ are coded as $X_{1} \\in \\{0,1\\}$ and $X_{2} \\in \\{0,1\\}$. The binary phenotype $Y \\in \\{0,1\\}$ is generated deterministically by the Exclusive OR (XOR) rule, $Y = X_{1} \\oplus X_{2}$, where $\\oplus$ denotes Exclusive OR (XOR). Assume $X_{1}$ and $X_{2}$ are independent and identically distributed Bernoulli random variables with parameter $1/2$, and that the synthetic cohort consists of $N = 4n$ individuals for some integer $n \\geq 1$, with each of the four $(X_{1}, X_{2})$ combinations occurring exactly $n$ times. In filter-based feature selection, features are ranked by univariate association with the target (for example, by Mutual Information (MI) or correlation), and selected accordingly.\n\nStarting from the foundational definitions of independence in probability theory and the definition of Mutual Information (MI),\n$$\nI(X;Y) = \\sum_{x} \\sum_{y} p(x,y) \\ln\\!\\left(\\frac{p(x,y)}{p(x)\\,p(y)}\\right),\n$$\nderive whether the marginal relationship between $X_{1}$ and $Y$ is informative for feature selection under the described data-generating mechanism. Then, using the specified synthetic cohort, compute the exact value of the Mutual Information $I(X_{1}; Y)$ implied by the cohort construction.\n\nExpress your final answer as the exact value. No rounding is required. No physical units are involved in this computation.",
            "solution": "The user has provided a problem to be solved. As per the instructions, the first step is to validate the problem statement.\n\n### Step 1: Extract Givens\n- **Features:** Two binary Single Nucleotide Polymorphism (SNP) features, coded as $X_{1} \\in \\{0,1\\}$ and $X_{2} \\in \\{0,1\\}$.\n- **Phenotype:** A binary phenotype, coded as $Y \\in \\{0,1\\}$.\n- **Data-Generating Rule:** $Y$ is generated deterministically by the Exclusive OR (XOR) rule, $Y = X_{1} \\oplus X_{2}$.\n- **Feature Distribution (Probabilistic):** $X_{1}$ and $X_{2}$ are independent and identically distributed (i.i.d.) Bernoulli random variables with parameter $1/2$.\n- **Cohort Construction (Deterministic):** The synthetic cohort consists of $N = 4n$ individuals for some integer $n \\geq 1$. Each of the four $(X_{1}, X_{2})$ combinations—$(0,0)$, $(0,1)$, $(1,0)$, $(1,1)$—occurs exactly $n$ times.\n- **Methodology:** Analysis is in the context of filter-based feature selection using univariate association.\n- **Task 1:** Starting from the foundational definitions of independence and Mutual Information, derive whether the marginal relationship between $X_{1}$ and $Y$ is informative.\n- **Task 2:** Compute the exact value of the Mutual Information $I(X_{1}; Y)$ implied by the cohort construction.\n- **Formula for Mutual Information (MI):** $I(X;Y) = \\sum_{x} \\sum_{y} p(x,y) \\ln\\!\\left(\\frac{p(x,y)}{p(x)\\,p(y)}\\right)$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is firmly rooted in information theory, probability theory, and machine learning. The XOR problem is a classic example used to illustrate the limitations of linear models and univariate feature selection methods. The concepts of Bernoulli variables, Mutual Information, and filter methods are standard in bioinformatics and data analytics. The problem setup is scientifically sound.\n- **Well-Posed:** All necessary information is provided. The probabilistic description of the features and the deterministic construction of the cohort are consistent with each other, as the cohort perfectly represents the underlying probability distribution. The task is clearly defined, and a unique, meaningful solution exists.\n- **Objective:** The problem is stated in precise, mathematical language. It is free of any subjectivity or ambiguity.\n- **Flaws Checklist:**\n    1.  **Scientific/Factual Unsoundness:** None. The premises are logically consistent and grounded in established theory.\n    2.  **Non-Formalizable/Irrelevant:** None. The problem is directly relevant to feature selection methods in bioinformatics.\n    3.  **Incomplete/Contradictory Setup:** None. The two descriptions of the data (probabilistic and cohort-based) are internally consistent and complete.\n    4.  **Unrealistic/Infeasible:** None. It describes a synthetic dataset, which is a common and valid tool for theoretical analysis.\n    5.  **Ill-Posed/Poorly Structured:** None. The question is specific and leads to a unique answer.\n    6.  **Pseudo-Profound/Trivial:** None. The problem addresses a conceptually important limitation of a common class of algorithms.\n    7.  **Outside Scientific Verifiability:** None. The derivation and calculation are mathematically verifiable.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. The solution process will now proceed.\n\n### Solution Derivation\n\nThe problem asks for two related results: first, to determine if the marginal relationship between $X_{1}$ and $Y$ is informative, and second, to compute the Mutual Information $I(X_{1}; Y)$.\n\n**Part 1: Informativeness of the Marginal Relationship**\n\nIn the context of feature selection, a marginal relationship is \"informative\" if the feature provides information about the target variable when considered alone. Statistically, this means the feature and the target variable are not independent. If they are statistically independent, knowing the value of the feature provides no information about the value of the target. We will test for the independence of $X_{1}$ and $Y$.\n\nTwo random variables are independent if and only if their joint probability distribution is the product of their marginal probability distributions, i.e., $p(x_{1}, y) = p(x_{1})p(y)$ for all possible values of $x_{1}$ and $y$. We will derive these distributions from the given data-generating process.\n\nFirst, we determine the marginal distributions for $X_{1}$ and $Y$.\nThe problem states that $X_{1}$ is a Bernoulli random variable with parameter $1/2$. Thus, its probability distribution is:\n$$ p(X_{1}=0) = \\frac{1}{2} $$\n$$ p(X_{1}=1) = \\frac{1}{2} $$\n\nNext, we find the distribution of $Y = X_{1} \\oplus X_{2}$. The variable $Y$ takes the value $0$ if $X_{1} = X_{2}$ and the value $1$ if $X_{1} \\neq X_{2}$.\nThe value $Y=0$ occurs for two mutually exclusive events: $(X_{1}=0, X_{2}=0)$ or $(X_{1}=1, X_{2}=1)$. Since $X_{1}$ and $X_{2}$ are independent, the probability is:\n$$ p(Y=0) = p(X_{1}=0, X_{2}=0) + p(X_{1}=1, X_{2}=1) $$\n$$ p(Y=0) = p(X_{1}=0)p(X_{2}=0) + p(X_{1}=1)p(X_{2}=1) = \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right) + \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right) = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2} $$\nThe value $Y=1$ occurs for two mutually exclusive events: $(X_{1}=0, X_{2}=1)$ or $(X_{1}=1, X_{2}=0)$. The probability is:\n$$ p(Y=1) = p(X_{1}=0, X_{2}=1) + p(X_{1}=1, X_{2}=0) $$\n$$ p(Y=1) = p(X_{1}=0)p(X_{2}=1) + p(X_{1}=1)p(X_{2}=0) = \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right) + \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right) = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2} $$\nSo, $Y$ is also a Bernoulli random variable with parameter $1/2$.\n\nNow, we determine the joint distribution $p(X_{1}, Y)$.\n-   $p(X_{1}=0, Y=0)$: This requires $X_{1}=0$ and $Y=0$. Since $Y=X_{1} \\oplus X_{2}$, we have $0 = 0 \\oplus X_{2}$, which implies $X_{2}=0$. Thus, this event corresponds to $(X_{1}=0, X_{2}=0)$.\n    $$ p(X_{1}=0, Y=0) = p(X_{1}=0, X_{2}=0) = p(X_{1}=0)p(X_{2}=0) = \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right) = \\frac{1}{4} $$\n-   $p(X_{1}=0, Y=1)$: This requires $X_{1}=0$ and $Y=1$. We have $1 = 0 \\oplus X_{2}$, which implies $X_{2}=1$. This event is $(X_{1}=0, X_{2}=1)$.\n    $$ p(X_{1}=0, Y=1) = p(X_{1}=0, X_{2}=1) = p(X_{1}=0)p(X_{2}=1) = \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right) = \\frac{1}{4} $$\n-   $p(X_{1}=1, Y=0)$: This requires $X_{1}=1$ and $Y=0$. We have $0 = 1 \\oplus X_{2}$, which implies $X_{2}=1$. This event is $(X_{1}=1, X_{2}=1)$.\n    $$ p(X_{1}=1, Y=0) = p(X_{1}=1, X_{2}=1) = p(X_{1}=1)p(X_{2}=1) = \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right) = \\frac{1}{4} $$\n-   $p(X_{1}=1, Y=1)$: This requires $X_{1}=1$ and $Y=1$. We have $1 = 1 \\oplus X_{2}$, which implies $X_{2}=0$. This event is $(X_{1}=1, X_{2}=0)$.\n    $$ p(X_{1}=1, Y=1) = p(X_{1}=1, X_{2}=0) = p(X_{1}=1)p(X_{2}=0) = \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right) = \\frac{1}{4} $$\n\nFinally, we test the independence condition $p(x_{1}, y) = p(x_{1})p(y)$:\n-   For $(x_{1}=0, y=0)$: $p(X_{1}=0, Y=0) = \\frac{1}{4}$. The product of marginals is $p(X_{1}=0)p(Y=0) = (\\frac{1}{2})(\\frac{1}{2}) = \\frac{1}{4}$. The condition holds.\n-   For $(x_{1}=0, y=1)$: $p(X_{1}=0, Y=1) = \\frac{1}{4}$. The product of marginals is $p(X_{1}=0)p(Y=1) = (\\frac{1}{2})(\\frac{1}{2}) = \\frac{1}{4}$. The condition holds.\n-   For $(x_{1}=1, y=0)$: $p(X_{1}=1, Y=0) = \\frac{1}{4}$. The product of marginals is $p(X_{1}=1)p(Y=0) = (\\frac{1}{2})(\\frac{1}{2}) = \\frac{1}{4}$. The condition holds.\n-   For $(x_{1}=1, y=1)$: $p(X_{1}=1, Y=1) = \\frac{1}{4}$. The product of marginals is $p(X_{1}=1)p(Y=1) = (\\frac{1}{2})(\\frac{1}{2}) = \\frac{1}{4}$. The condition holds.\n\nSince the independence condition holds for all possible outcomes, $X_{1}$ and $Y$ are statistically independent. Therefore, the marginal relationship between $X_{1}$ and $Y$ is uninformative for feature selection. A filter method based on univariate association would assign $X_1$ zero or near-zero importance and fail to select it, despite its critical role in the multivariate relationship that determines $Y$.\n\n**Part 2: Calculation of Mutual Information $I(X_{1}; Y)$**\n\nWe will now compute the exact value of the Mutual Information $I(X_{1}; Y)$ using the specified synthetic cohort. The cohort consists of $N = 4n$ individuals, with each of the four possible combinations of $(X_1, X_2)$ appearing exactly $n$ times. The value of $Y$ is determined by $Y=X_1 \\oplus X_2$.\n\nThe counts for each joint outcome of $(X_1, X_2, Y)$ are:\n-   $(X_1=0, X_2=0) \\implies Y=0$: count is $n$.\n-   $(X_1=0, X_2=1) \\implies Y=1$: count is $n$.\n-   $(X_1=1, X_2=0) \\implies Y=1$: count is $n$.\n-   $(X_1=1, X_2=1) \\implies Y=0$: count is $n$.\n\nFrom these counts, we can calculate the empirical probabilities required for the MI formula. The total number of individuals is $4n$.\n-   $p(X_{1}=0, Y=0)$: count for $(X_1=0, Y=0)$ comes from $(X_1=0, X_2=0)$, which is $n$. So, $p(X_{1}=0, Y=0) = \\frac{n}{4n} = \\frac{1}{4}$.\n-   $p(X_{1}=0, Y=1)$: count for $(X_1=0, Y=1)$ comes from $(X_1=0, X_2=1)$, which is $n$. So, $p(X_{1}=0, Y=1) = \\frac{n}{4n} = \\frac{1}{4}$.\n-   $p(X_{1}=1, Y=0)$: count for $(X_1=1, Y=0)$ comes from $(X_1=1, X_2=1)$, which is $n$. So, $p(X_{1}=1, Y=0) = \\frac{n}{4n} = \\frac{1}{4}$.\n-   $p(X_{1}=1, Y=1)$: count for $(X_1=1, Y=1)$ comes from $(X_1=1, X_2=0)$, which is $n$. So, $p(X_{1}=1, Y=1) = \\frac{n}{4n} = \\frac{1}{4}$.\n\nThe marginal probabilities are:\n-   $p(X_{1}=0) = p(X_{1}=0, Y=0) + p(X_{1}=0, Y=1) = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2}$.\n-   $p(X_{1}=1) = p(X_{1}=1, Y=0) + p(X_{1}=1, Y=1) = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2}$.\n-   $p(Y=0) = p(X_{1}=0, Y=0) + p(X_{1}=1, Y=0) = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2}$.\n-   $p(Y=1) = p(X_{1}=0, Y=1) + p(X_{1}=1, Y=1) = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2}$.\n\nThese empirical probabilities from the cohort are identical to the theoretical probabilities derived in Part 1.\n\nNow, we use the MI formula:\n$$ I(X_{1}; Y) = \\sum_{x_{1} \\in \\{0,1\\}} \\sum_{y \\in \\{0,1\\}} p(x_{1},y) \\ln\\!\\left(\\frac{p(x_{1},y)}{p(x_{1})\\,p(y)}\\right) $$\nFor each term in the summation, the ratio inside the logarithm is:\n$$ \\frac{p(x_{1},y)}{p(x_{1})\\,p(y)} = \\frac{1/4}{(1/2)(1/2)} = \\frac{1/4}{1/4} = 1 $$\nThis is true for all four combinations of $(x_1, y)$.\n\nTherefore, the MI calculation becomes:\n$$ I(X_1; Y) = p(0,0)\\ln(1) + p(0,1)\\ln(1) + p(1,0)\\ln(1) + p(1,1)\\ln(1) $$\n$$ I(X_1; Y) = \\frac{1}{4}\\ln(1) + \\frac{1}{4}\\ln(1) + \\frac{1}{4}\\ln(1) + \\frac{1}{4}\\ln(1) $$\nSince $\\ln(1) = 0$, every term in the sum is zero.\n$$ I(X_1; Y) = \\frac{1}{4}(0) + \\frac{1}{4}(0) + \\frac{1}{4}(0) + \\frac{1}{4}(0) = 0 $$\nThe Mutual Information between $X_{1}$ and $Y$ is exactly $0$. This confirms the conclusion from Part 1 that $X_1$ and $Y$ are statistically independent, and thus the marginal relationship is uninformative.",
            "answer": "$$\n\\boxed{0}\n$$"
        },
        {
            "introduction": "When analyzing thousands of features simultaneously, as is common in genomics, the risk of false discoveries skyrockets. This practice addresses the critical task of multiple hypothesis testing correction, contrasting the stringent Family-Wise Error Rate (FWER) with the more powerful False Discovery Rate (FDR) . Deriving the thresholds for Bonferroni and Benjamini-Hochberg procedures will equip you to make informed, defensible choices about statistical significance in high-dimensional data.",
            "id": "4563535",
            "problem": "A research team is performing a filter-based feature selection stage in a genomics pipeline prior to training a disease classifier. They test association for $m = 20{,}000$ transcript features against a binary disease phenotype, obtaining one independent $p$-value per feature. They wish to set a fixed $p$-value cutoff under two distinct error-control regimes at level $\\alpha = 0.05$:\n\n1. Family-Wise Error Rate (FWER), defined as the probability of making at least one false positive among all $m$ tests.\n\n2. False Discovery Rate (FDR), defined as the expected proportion of false positives among all rejected hypotheses.\n\nFrom first principles, using only the union bound for controlling FWER and the definition of FDR together with the known null $p$-value property that under true null hypotheses $p$-values are stochastically no smaller than a uniform distribution on $[0,1]$, derive:\n\n- The Bonferroni per-feature significance threshold that controls FWER at level $\\alpha$.\n\n- The Benjamini–Hochberg (BH) step-up critical value function $t(k)$, where $k \\in \\{1,2,\\ldots,m\\}$ indexes the rank of the sorted $p$-values in ascending order, that controls FDR at level $\\alpha$ under independence of $p$-values.\n\nCompute these quantities for $m = 20{,}000$ and $\\alpha = 0.05$. Express your final answer as a single row matrix whose first entry is the Bonferroni per-feature threshold and whose second entry is the closed-form expression for the BH critical value function $t(k)$ as a function of the rank index $k$. No rounding is required.\n\nFinally, justify in words the trade-off between controlling FWER and controlling FDR in the context of large-scale omics feature selection, and why a practitioner might prefer one over the other when the downstream objective is predictive performance rather than confirmatory inference. Your justification must proceed from the definitions above and the behavior of the corresponding thresholds without invoking unproven heuristics.",
            "solution": "The problem has been validated as scientifically grounded, well-posed, and objective. It is based on established principles of multiple hypothesis testing in statistics and bioinformatics. All necessary definitions and data are provided, and the problem is free of contradictions or ambiguities.\n\nLet $m = 20{,}000$ be the total number of features, which corresponds to $m$ independent hypothesis tests. Let $\\alpha = 0.05$ be the desired significance level for error control. For each hypothesis $H_i$, $i \\in \\{1, 2, \\ldots, m\\}$, we have a corresponding $p$-value, $p_i$. A hypothesis $H_i$ is rejected if its $p$-value $p_i$ is less than or equal to some threshold.\n\n### 1. Family-Wise Error Rate (FWER) and the Bonferroni Correction\n\nThe Family-Wise Error Rate (FWER) is defined as the probability of making at least one Type I error (a false positive) among all $m$ tests. Let $I_0$ be the set of indices corresponding to true null hypotheses, and let $m_0 = |I_0|$ be the number of true nulls. A Type I error for hypothesis $H_i$ occurs if $H_i$ is true ($i \\in I_0$) but is rejected. Let this event be $E_i$.\n\nThe FWER is then $P(\\cup_{i \\in I_0} E_i)$. The problem requires using the union bound (also known as Boole's inequality) to control the FWER. The union bound states that for any collection of events, the probability of their union is no greater than the sum of their individual probabilities.\n$$\n\\text{FWER} = P(\\cup_{i \\in I_0} E_i) \\le \\sum_{i \\in I_0} P(E_i)\n$$\nWe employ a single, fixed $p$-value cutoff, let's call it $t_{bonf}$. A hypothesis $H_i$ is rejected if $p_i \\le t_{bonf}$. The event $E_i$ is thus $\\{p_i \\le t_{bonf}\\}$. Under a true null hypothesis, the $p$-value $p_i$ is stochastically no smaller than a uniform random variable on $[0,1]$. For a continuous test statistic, which is standard, $p_i \\sim U(0,1)$. Therefore, for any $i \\in I_0$, the probability of a Type I error is $P(p_i \\le t_{bonf}) = t_{bonf}$.\n\nSubstituting this into the union bound inequality:\n$$\n\\text{FWER} \\le \\sum_{i \\in I_0} t_{bonf} = m_0 t_{bonf}\n$$\nSince the number of true nulls, $m_0$, is unknown, we use the most conservative case where all null hypotheses could be true, i.e., $m_0 \\le m$. This gives:\n$$\n\\text{FWER} \\le m_0 t_{bonf} \\le m t_{bonf}\n$$\nTo control the FWER at level $\\alpha$, we enforce the condition that this upper bound is no greater than $\\alpha$:\n$$\nm t_{bonf} \\le \\alpha\n$$\nSolving for the per-feature significance threshold $t_{bonf}$ gives the Bonferroni correction:\n$$\nt_{bonf} = \\frac{\\alpha}{m}\n$$\nFor the given values $m = 20{,}000$ and $\\alpha = 0.05$:\n$$\nt_{bonf} = \\frac{0.05}{20{,}000} = \\frac{5 \\times 10^{-2}}{2 \\times 10^4} = 2.5 \\times 10^{-6}\n$$\n\n### 2. False Discovery Rate (FDR) and the Benjamini-Hochberg (BH) Procedure\n\nThe False Discovery Rate (FDR) is defined as the expected proportion of false positives among all rejected hypotheses (discoveries). Let $R$ be the total number of rejected hypotheses and $V$ be the number of false positives (true null hypotheses that were rejected). The FDR is $E\\left[\\frac{V}{R}\\right]$ (with the fraction taken as $0$ if $R=0$).\n\nThe Benjamini-Hochberg (BH) procedure provides a less stringent control than FWER, which is often more powerful for exploratory analyses. We are asked to derive its critical value function, $t(k)$, from first principles.\n\nThe core idea is to find an adaptive threshold. Let us seek a threshold $t$ such that our estimate of the FDR is controlled at level $\\alpha$. For a given threshold $t$, the number of discoveries is $R(t) = \\sum_{i=1}^m \\mathbb{I}(p_i \\le t)$, where $\\mathbb{I}(\\cdot)$ is the indicator function. The expected number of false positives is $E[V(t)] = E[\\sum_{i \\in I_0} \\mathbb{I}(p_i \\le t)] = \\sum_{i \\in I_0} P(p_i \\le t) = m_0 t$.\n\nA simple, though not formally rigorous, plug-in estimator for the FDR for a given threshold $t$ is $\\widehat{\\text{FDR}}(t) = \\frac{E[V(t)]}{R(t)} = \\frac{m_0 t}{R(t)}$. Since $m_0$ is unknown, we can use the conservative upper bound $m_0 \\le m$, which yields the condition:\n$$\n\\frac{m t}{R(t)} \\le \\alpha \\implies t \\le \\frac{R(t)}{m} \\alpha\n$$\nThis inequality must hold for our rejection threshold $t$. However, the threshold $t$ itself determines the number of rejections $R(t)$. This circularity suggests a search procedure. The BH procedure provides a systematic way to perform this search.\n\nLet's order the $p$-values in ascending order: $p_{(1)} \\le p_{(2)} \\le \\ldots \\le p_{(m)}$.\nSuppose we consider the $k$-th ordered $p$-value, $p_{(k)}$, as our potential threshold. If we were to set our threshold $t = p_{(k)}$, then we would make exactly $k$ rejections. So, $R(t) = R(p_{(k)}) = k$.\nSubstituting $t=p_{(k)}$ and $R(t)=k$ into the condition above gives:\n$$\np_{(k)} \\le \\frac{k}{m} \\alpha\n$$\nThis gives us a condition for each ranked $p$-value. The BH procedure checks this condition for all $k \\in \\{1, 2, \\ldots, m\\}$. To maximize the number of discoveries (i.e., to have the highest power), we find the largest $k$ for which this condition holds. Let this be $k_{max} = \\max\\{k : p_{(k)} \\le \\frac{k\\alpha}{m}\\}$. Then, all hypotheses with $p$-values up to $p_{(k_{max})}$ are rejected.\n\nFrom this derivation, the critical value against which the $k$-th ranked $p$-value, $p_{(k)}$, is compared is:\n$$\nt(k) = \\frac{k \\alpha}{m}\n$$\nThis is the Benjamini-Hochberg step-up critical value function. For the given values, this becomes:\n$$\nt(k) = \\frac{k \\times 0.05}{20{,}000} = (2.5 \\times 10^{-6}) k\n$$\n\n### Justification of Trade-off for Predictive Performance\n\nThe choice between controlling FWER and FDR depends critically on the scientific goal. FWER is suited for confirmatory research where the cost of a single false claim is high. FDR is suited for exploratory research, such as feature selection for prediction, where the goal is to generate a promising set of candidates for a downstream task.\n\nThe Bonferroni threshold for FWER control is $t_{bonf} = \\alpha/m = 2.5 \\times 10^{-6}$. This is a single, extremely stringent threshold that does not adapt to the data. In a typical large-scale omics study where true effects may be modest, this high bar for significance often results in very few or no features being selected. This corresponds to a high rate of false negatives (truly predictive features being missed), which is detrimental to building a powerful predictive model. A model built on an impoverished feature set is likely to underfit and have poor predictive accuracy.\n\nThe BH critical values for FDR control are $t(k) = k\\alpha/m$. These values form an increasing ramp: $t(1) = \\alpha/m$, $t(2) = 2\\alpha/m$, and so on. Any selected feature under Bonferroni (where $p_{(1)} \\le \\alpha/m$) will also be selected by BH. However, BH allows for the selection of many more features, as the threshold becomes more lenient for less significant $p$-values. For example, the $100$-th ranked feature is tested against a threshold of $100\\alpha/m$, which is $100$ times larger than the Bonferroni threshold.\n\nThis more liberal approach of FDR control allows for a greater number of true positives to be included in the selected feature set. While this also increases the number of false positives, the FDR framework guarantees that the *expected proportion* of these \"noise\" features among all selected features is controlled at level $\\alpha$. For prediction, this is a highly desirable trade-off. Many modern machine learning algorithms (e.g., L1-regularized regression, random forests) are robust to the inclusion of a limited number of non-informative features; they have internal mechanisms to assign low importance or zero weight to them. The damage caused by including a few spurious features is often far less than the damage caused by excluding numerous truly predictive features.\n\nTherefore, when the objective is predictive performance rather than confirmatory inference, a practitioner would prefer FDR control over FWER control. FDR prioritizes higher power (sensitivity) to discover potentially predictive features, accepting a controlled proportion of false discoveries, which is a trade-off that aligns well with the goal of building robust and accurate predictive models. FWER's extreme conservatism (prioritizing specificity) is counterproductive in this context.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 2.5 \\times 10^{-6} & 2.5 \\times 10^{-6} k \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Moving beyond filters, embedded methods like the Least Absolute Shrinkage and Selection Operator (LASSO) integrate feature selection directly into model training. However, the $L_1$ penalty's behavior is not scale-invariant, a subtle but critical detail. This coding exercise provides a hands-on demonstration of how feature standardization dramatically alters which features are selected by LASSO, revealing the deep interplay between data preprocessing and model outcomes .",
            "id": "4563548",
            "problem": "You are given the task of quantitatively evaluating how scaling and normalization of features affect embedded penalties in the Least Absolute Shrinkage and Selection Operator (LASSO). In an embedded method, feature selection occurs during model training, and the $L_1$ penalty interacts with the feature scaling. Your goal is to construct a program that, for a scientifically plausible synthetic bioinformatics-style dataset, compares LASSO solutions computed on raw features versus standardized features (zero mean and unit variance) and quantifies the differences in the selected supports and penalty behavior.\n\nFundamental base. Consider the Least Absolute Shrinkage and Selection Operator (LASSO) with the objective\n$$\n\\min_{\\mathbf{w} \\in \\mathbb{R}^p} \\;\\; \\frac{1}{2n}\\left\\lVert \\mathbf{y} - \\mathbf{X}\\mathbf{w} \\right\\rVert_2^2 \\;+\\; \\lambda \\left\\lVert \\mathbf{w} \\right\\rVert_1,\n$$\nwhere $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$ is the feature matrix, $\\mathbf{y} \\in \\mathbb{R}^{n}$ is the response, $\\mathbf{w} \\in \\mathbb{R}^{p}$ are the coefficients, $n$ is the number of samples, $p$ is the number of features, and $\\lambda > 0$ is the regularization parameter. This formulation is a convex loss with an $L_1$ penalty that induces sparsity. In embedded methods, this sparsity mechanism performs feature selection during estimation. In bioinformatics and medical data analytics, feature scales frequently differ due to measurement units and assay protocols (for example, counts versus intensities), and these scale differences interact with the penalty to alter selection patterns. The dataset should be centered to remove the intercept; standardization further divides each feature by its sample standard deviation, yielding unit variance per feature.\n\nProgram requirements. Implement a solver that approximately minimizes the LASSO objective for a given $\\mathbf{X}$, $\\mathbf{y}$, and $\\lambda$ via an iterative method that is consistent with the subgradient optimality conditions for the above convex formulation. Then, for each test case below, compute:\n1. A solution on the raw (centered-only) features.\n2. A solution on standardized features (zero mean, unit variance).\nUse the same $\\lambda$ value for both fits within a given test case. Define the support of a solution as the set of indices $j \\in \\{0,1,\\dots,p-1\\}$ such that $|w_j| > \\tau$, where $\\tau = 10^{-6}$.\n\nQuantification metrics. For each test case, compute the following four quantities:\n- Support equality boolean $b$: $b$ is $true$ if and only if the raw-fit support equals the standardized-fit support; otherwise $false$.\n- Raw support match fraction $r$: the fraction in $[0,1]$ of feature indices where the raw-fit selection decision (selected or not selected) matches the ground-truth support.\n- Standardized support match fraction $s$: the fraction in $[0,1]$ of feature indices where the standardized-fit selection decision (selected or not selected) matches the ground-truth support.\n- Scale–selection correlation delta $c$: let $\\mathbf{u} \\in \\{0,1\\}^p$ be the indicator vector for selection under the raw fit and let $\\mathbf{v} \\in \\{0,1\\}^p$ be the indicator vector under the standardized fit; let $\\mathbf{q} \\in \\mathbb{R}^p$ be the per-feature scale multipliers used to construct raw features. Define the Pearson correlation coefficient (PCC) between a selection indicator and the scales as\n$$\n\\operatorname{corr}(\\mathbf{m}, \\mathbf{q}) = \\frac{ \\sum_{j=0}^{p-1} (m_j - \\bar{m})(q_j - \\bar{q}) }{ \\sqrt{ \\sum_{j=0}^{p-1} (m_j - \\bar{m})^2 } \\; \\sqrt{ \\sum_{j=0}^{p-1} (q_j - \\bar{q})^2 } },\n$$\nwhere $\\bar{m}$ and $\\bar{q}$ denote sample means. If either variance is zero, define the correlation to be $0$. The delta is $c = \\operatorname{corr}(\\mathbf{u}, \\mathbf{q}) - \\operatorname{corr}(\\mathbf{v}, \\mathbf{q})$. A positive $c$ indicates stronger scale bias in the raw fit compared to the standardized fit.\n\nData generation model. For each test case, generate $\\mathbf{Z} \\in \\mathbb{R}^{n \\times p}$ with entries drawn independently from a standard normal distribution. Construct $\\mathbf{X}_{\\text{raw}}$ by multiplying each column $j$ of $\\mathbf{Z}$ by a scale $q_j > 0$. When a collinearity constraint is specified, set $\\mathbf{Z}_{\\cdot,k} = \\mathbf{Z}_{\\cdot,j} + \\delta \\boldsymbol{\\eta}$ for indices $j,k$ with a small $\\delta > 0$ and independent standard normal noise vector $\\boldsymbol{\\eta}$. Let the ground-truth coefficient vector $\\mathbf{w}^{\\star} \\in \\mathbb{R}^p$ be sparse with nonzero entries at the specified indices. Generate responses via $\\mathbf{y} = \\mathbf{X}_{\\text{raw}} \\mathbf{w}^{\\star} + \\sigma \\boldsymbol{\\epsilon}$, where $\\boldsymbol{\\epsilon}$ has independent standard normal entries and $\\sigma > 0$ is the noise standard deviation. Use zero-based feature indexing throughout.\n\nTest suite. Use the following four test cases, each specified by $(n, p, \\mathbf{q}, \\text{support}, \\text{coeffs}, \\sigma, \\lambda, \\text{seed}, \\text{collinear})$:\n- Case A (happy path, heterogeneous scales): $n = 120$, $p = 12$, $\\mathbf{q} = [0.5, 2.0, 8.0, 1.0, 3.0, 0.2, 5.0, 1.5, 0.8, 10.0, 0.3, 4.0]$, support indices $\\{1,5,7,10\\}$, coefficients on support $[0.6, -1.2, 0.9, -0.5]$, $\\sigma = 0.30$, $\\lambda = 0.15$, seed $= 123$, no collinearity.\n- Case B (boundary, uniform scales): $n = 120$, $p = 12$, $\\mathbf{q} = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]$, support indices $\\{2,4,9\\}$, coefficients on support $[1.0, -0.8, 0.6]$, $\\sigma = 0.30$, $\\lambda = 0.15$, seed $= 124$, no collinearity.\n- Case C (edge, collinearity with different scales): $n = 100$, $p = 8$, $\\mathbf{q} = [6.0, 0.4, 12.0, 1.0, 0.2, 3.0, 0.5, 2.0]$, support indices $\\{0,5\\}$, coefficients on support $[1.0, -0.7]$, $\\sigma = 0.25$, $\\lambda = 0.20$, seed $= 125$, collinearity with $(j,k,\\delta) = (0,2,10^{-2})$.\n- Case D (edge, extreme scale disparity): $n = 150$, $p = 10$, $\\mathbf{q} = [10^{-3}, 30.0, 5 \\cdot 10^{-3}, 4.0, 8 \\cdot 10^{-4}, 15.0, 2 \\cdot 10^{-2}, 2.5, 0.5, 20.0]$, support indices $\\{0,2,6,9\\}$, coefficients on support $[1.5, -1.0, 0.8, 0.5]$, $\\sigma = 0.35$, $\\lambda = 0.25$, seed $= 126$, no collinearity.\n\nPreprocessing conventions. For the raw fit, center each feature column (subtract its sample mean) but do not scale by its standard deviation; for the standardized fit, center then divide each feature column by its sample standard deviation, ignoring any zero-standard-deviation columns by leaving them unscaled. Always center $\\mathbf{y}$ by subtracting its sample mean before fitting.\n\nFinal output requirements. For each of the above four cases, your program must output a list $[b, r, s, c]$ as defined above. Aggregate the four case results into a single line of output containing a comma-separated list enclosed in square brackets, with no spaces, in the order Case A, Case B, Case C, Case D. For example, the output must look like\n$[ [\\dots], [\\dots], [\\dots], [\\dots] ]$\nformatted without spaces as\n\"[[b_A,r_A,s_A,c_A],[b_B,r_B,s_B,c_B],[b_C,r_C,s_C,c_C],[b_D,r_D,s_D,c_D]]\".\nAll booleans must be printed as either \"True\" or \"False\". All floats must be printed in decimal notation. No external input is allowed; the program must run as is and produce the specified output.",
            "solution": "The user has provided a problem that requires a quantitative evaluation of the effect of feature scaling on the LASSO (Least Absolute Shrinkage and Selection Operator) method. This is a well-posed problem in the field of statistical machine learning and its application to bioinformatics.\n\n### Step 1: Extract Givens\n\n- **Objective Function (LASSO):**\n$$\n\\min_{\\mathbf{w} \\in \\mathbb{R}^p} \\;\\; \\frac{1}{2n}\\left\\lVert \\mathbf{y} - \\mathbf{X}\\mathbf{w} \\right\\rVert_2^2 \\;+\\; \\lambda \\left\\lVert \\mathbf{w} \\right\\rVert_1\n$$\n- **Variables:** $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$ (features), $\\mathbf{y} \\in \\mathbb{R}^{n}$ (response), $\\mathbf{w} \\in \\mathbb{R}^{p}$ (coefficients), $n$ (samples), $p$ (features), $\\lambda > 0$ (regularization parameter).\n- **Support Definition:** The support of a solution $\\mathbf{w}$ is the set of indices $j$ such that $|w_j| > \\tau$, where $\\tau = 10^{-6}$.\n- **Quantification Metrics:**\n    - $b$: Boolean, `true` if raw-fit support equals standardized-fit support.\n    - $r$: Fraction of features where the raw-fit selection decision matches the ground-truth support.\n    - $s$: Fraction of features where the standardized-fit selection decision matches the ground-truth support.\n    - $c$: $\\operatorname{corr}(\\mathbf{u}, \\mathbf{q}) - \\operatorname{corr}(\\mathbf{v}, \\mathbf{q})$, where $\\mathbf{u}$ and $\\mathbf{v}$ are selection indicator vectors for raw and standardized fits, and $\\mathbf{q}$ is the vector of feature scale multipliers. The Pearson correlation $\\operatorname{corr}(\\mathbf{m}, \\mathbf{q})$ is defined as:\n    $$\n    \\operatorname{corr}(\\mathbf{m}, \\mathbf{q}) = \\frac{ \\sum_{j=0}^{p-1} (m_j - \\bar{m})(q_j - \\bar{q}) }{ \\sqrt{ \\sum_{j=0}^{p-1} (m_j - \\bar{m})^2 } \\; \\sqrt{ \\sum_{j=0}^{p-1} (q_j - \\bar{q})^2 } }\n    $$\n    If either variance term in the denominator is zero, the correlation is defined to be $0$.\n- **Data Generation:**\n    - Generate $\\mathbf{Z} \\in \\mathbb{R}^{n \\times p}$ from i.i.d. standard normal entries.\n    - $\\mathbf{X}_{\\text{raw}, \\cdot, j} = \\mathbf{Z}_{\\cdot, j} \\times q_j$.\n    - For collinearity between features $j$ and $k$: $\\mathbf{Z}_{\\cdot,k} = \\mathbf{Z}_{\\cdot,j} + \\delta \\boldsymbol{\\eta}$, where $\\boldsymbol{\\eta}$ is an independent standard normal vector.\n    - $\\mathbf{y} = \\mathbf{X}_{\\text{raw}} \\mathbf{w}^{\\star} + \\sigma \\boldsymbol{\\epsilon}$, with $\\mathbf{w}^{\\star}$ being a sparse ground-truth vector and $\\boldsymbol{\\epsilon}$ being standard normal noise.\n- **Preprocessing:**\n    - Raw fit: Center feature columns of $\\mathbf{X}$ (subtract mean).\n    - Standardized fit: Center feature columns of $\\mathbf{X}$, then divide by their sample standard deviation. If a column's standard deviation is $0$, it is left unscaled (it becomes a zero vector after centering).\n    - Response $\\mathbf{y}$ is always centered.\n- **Test Cases:** Four cases (A, B, C, D) are specified with all necessary parameters: $(n, p, \\mathbf{q}, \\text{support}, \\text{coeffs}, \\sigma, \\lambda, \\text{seed}, \\text{collinear})$.\n\n### Step 2: Validate Using Extracted Givens\n\n- **Scientifically Grounded:** The problem is rooted in fundamental statistical learning theory. The LASSO is a canonical method for high-dimensional regression and feature selection. The investigation of its sensitivity to feature scaling is a standard and critical topic in both theory and practice, particularly in fields like bioinformatics where feature scales are often arbitrary and heterogeneous. The data generation model is a conventional approach for simulating data in such contexts. The problem is scientifically sound.\n- **Well-Posed:** The problem provides all necessary information to generate the datasets (including random seeds for reproducibility) and to perform the analysis. The LASSO objective function is convex, ensuring a global minimum exists. The specified iterative solver approach is standard for this non-differentiable objective. The metrics for evaluation are defined unambiguously. The problem is self-contained and sets up a solvable task.\n- **Objective:** The problem statement is formal and mathematical, avoiding any subjective or ambiguous language. The task is a quantitative comparison based on clearly defined metrics.\n\n### Step 3: Verdict and Action\n\nThe problem statement is **VALID**. It is scientifically sound, well-posed, and objective. I will proceed with formulating and implementing the solution.\n\n### Algorithmic Solution Design\n\nThe core of the problem is to solve the LASSO optimization problem. A widely used and efficient algorithm for this task is **Coordinate Descent**. This iterative method optimizes the objective function with respect to a single coefficient at a time, holding all other coefficients fixed. Cyclically iterating through all coefficients until convergence yields a solution to the full problem.\n\nFor the LASSO objective function $L(\\mathbf{w}) = \\frac{1}{2n} \\left\\lVert \\mathbf{y} - \\mathbf{X}\\mathbf{w} \\right\\rVert_2^2 + \\lambda \\left\\lVert \\mathbf{w} \\right\\rVert_1$, the update for a single coefficient $w_j$ is found by solving a one-dimensional LASSO problem. This yields a closed-form update rule based on soft-thresholding.\n\nLet's derive the update for $w_j$. We fix all $w_k$ for $k \\ne j$ and minimize $L$ with respect to $w_j$. This is equivalent to minimizing:\n$$\nf_j(w_j) = \\frac{1}{2n} \\sum_{i=1}^{n} \\left( (y_i - \\sum_{k \\neq j} X_{ik}w_k) - X_{ij}w_j \\right)^2 + \\lambda |w_j|\n$$\nThe partial derivative of the least-squares term with respect to $w_j$ is:\n$$\n\\frac{\\partial}{\\partial w_j}\\left(\\text{LS term}\\right) = \\frac{1}{n} \\sum_{i=1}^n \\left( -X_{ij} \\left( (y_i - \\sum_{k \\neq j} X_{ik}w_k) - X_{ij}w_j \\right) \\right) = \\frac{1}{n} \\left( -\\mathbf{X}_{\\cdot, j}^T \\mathbf{r}_{(-j)} + (\\mathbf{X}_{\\cdot, j}^T \\mathbf{X}_{\\cdot, j}) w_j \\right)\n$$\nwhere $\\mathbf{r}_{(-j)} = \\mathbf{y} - \\sum_{k \\neq j} \\mathbf{X}_{\\cdot, k}w_k$. The subgradient optimality condition is:\n$$\n0 \\in \\frac{1}{n} (\\mathbf{X}_{\\cdot, j}^T \\mathbf{X}_{\\cdot, j}) w_j - \\frac{1}{n} \\mathbf{X}_{\\cdot, j}^T \\mathbf{r}_{(-j)} + \\lambda \\partial|w_j|\n$$\nLet $\\rho_j = \\mathbf{X}_{\\cdot, j}^T \\mathbf{r}_{(-j)}$ and $d_j = \\mathbf{X}_{\\cdot, j}^T \\mathbf{X}_{\\cdot, j}$. The condition simplifies to $0 \\in \\frac{d_j}{n} w_j - \\frac{\\rho_j}{n} + \\lambda \\partial|w_j|$. The solution to this is given by soft-thresholding:\n$$\nw_j \\leftarrow \\frac{S_{n\\lambda}(\\rho_j)}{d_j}\n$$\nwhere $S_\\alpha(z) = \\operatorname{sign}(z) \\max(|z|-\\alpha, 0)$ is the soft-thresholding operator. The term $\\rho_j$ can be efficiently computed as $\\rho_j = \\mathbf{X}_{\\cdot, j}^T (\\mathbf{y} - \\mathbf{X}\\mathbf{w} + \\mathbf{X}_{\\cdot, j}w_j^{\\text{old}})$.\n\nThe overall algorithm proceeds as follows:\n1.  **Data Generation:** For each test case, generate data $(\\mathbf{X}_{\\text{raw}}, \\mathbf{y})$ using the specified parameters and random seed.\n2.  **Preprocessing:**\n    - Center $\\mathbf{y}$ to get $\\mathbf{y}_c$.\n    - For the \"raw\" fit, center the columns of $\\mathbf{X}_{\\text{raw}}$ to get $\\mathbf{X}_c$.\n    - For the \"standardized\" fit, center and scale the columns of $\\mathbf{X}_{\\text{raw}}$ to get $\\mathbf{X}_s$.\n3.  **Model Fitting:**\n    - Solve for $\\mathbf{w}_{\\text{raw}}$ by applying the coordinate descent algorithm to $(\\mathbf{X}_c, \\mathbf{y}_c, \\lambda)$.\n    - Solve for $\\mathbf{w}_{\\text{std}}$ by applying the coordinate descent algorithm to $(\\mathbf{X}_s, \\mathbf{y}_c, \\lambda)$.\n4.  **Metric Calculation:**\n    - Determine the supports for $\\mathbf{w}_{\\text{raw}}$ and $\\mathbf{w}_{\\text{std}}$ using the threshold $\\tau=10^{-6}$.\n    - Compute the ground-truth support from the test case specification.\n    - Calculate the four required metrics: $b$ (support equality), $r$ (raw match fraction), $s$ (standardized match fraction), and $c$ (scale-selection correlation delta). The Pearson correlation will be implemented directly from its definition.\n\nThis procedure will be followed for all four test cases, and the results will be aggregated into the specified output format.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef pearson_correlation(m, q):\n    \"\"\"\n    Computes the Pearson correlation coefficient between two 1D numpy arrays.\n    Returns 0 if either array has zero variance.\n    \"\"\"\n    mean_m = np.mean(m)\n    mean_q = np.mean(q)\n    \n    var_m = np.sum((m - mean_m)**2)\n    var_q = np.sum((q - mean_q)**2)\n    \n    if var_m == 0 or var_q == 0:\n        return 0.0\n        \n    cov_mq = np.sum((m - mean_m) * (q - mean_q))\n    \n    return cov_mq / np.sqrt(var_m * var_q)\n\ndef lasso_coordinate_descent(X, y, lambda_val, max_iter=2000, tol=1e-6):\n    \"\"\"\n    Solves the LASSO problem using coordinate descent.\n    Objective: (1/(2n)) * ||y - Xw||_2^2 + lambda * ||w||_1\n    \"\"\"\n    n, p = X.shape\n    w = np.zeros(p)\n    d = np.sum(X**2, axis=0)\n    \n    for i in range(max_iter):\n        w_old = np.copy(w)\n        \n        for j in range(p):\n            if d[j] == 0:\n                continue\n            \n            # Efficiently compute X_j^T * r_(-j)\n            rho_j = X[:, j].T @ (y - X @ w + X[:, j] * w[j])\n            \n            # Soft-thresholding\n            # Note: The penalty is lambda * ||w||_1, which corresponds to\n            # a threshold of n * lambda_val in the coordinate-wise update.\n            threshold = n * lambda_val\n            \n            if rho_j < -threshold:\n                w[j] = (rho_j + threshold) / d[j]\n            elif rho_j > threshold:\n                w[j] = (rho_j - threshold) / d[j]\n            else:\n                w[j] = 0.0\n                \n        # Convergence check\n        if np.linalg.norm(w - w_old, ord=np.inf) < tol:\n            break\n            \n    return w\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and generate final output.\n    \"\"\"\n    test_cases = [\n        {\n            \"n\": 120, \"p\": 12, \"q\": np.array([0.5, 2.0, 8.0, 1.0, 3.0, 0.2, 5.0, 1.5, 0.8, 10.0, 0.3, 4.0]),\n            \"support\": {1, 5, 7, 10}, \"coeffs\": [0.6, -1.2, 0.9, -0.5],\n            \"sigma\": 0.30, \"lambda\": 0.15, \"seed\": 123, \"collinear\": None\n        },\n        {\n            \"n\": 120, \"p\": 12, \"q\": np.array([1.0] * 12),\n            \"support\": {2, 4, 9}, \"coeffs\": [1.0, -0.8, 0.6],\n            \"sigma\": 0.30, \"lambda\": 0.15, \"seed\": 124, \"collinear\": None\n        },\n        {\n            \"n\": 100, \"p\": 8, \"q\": np.array([6.0, 0.4, 12.0, 1.0, 0.2, 3.0, 0.5, 2.0]),\n            \"support\": {0, 5}, \"coeffs\": [1.0, -0.7],\n            \"sigma\": 0.25, \"lambda\": 0.20, \"seed\": 125, \"collinear\": {\"j\": 0, \"k\": 2, \"delta\": 1e-2}\n        },\n        {\n            \"n\": 150, \"p\": 10, \"q\": np.array([1e-3, 30.0, 5e-3, 4.0, 8e-4, 15.0, 2e-2, 2.5, 0.5, 20.0]),\n            \"support\": {0, 2, 6, 9}, \"coeffs\": [1.5, -1.0, 0.8, 0.5],\n            \"sigma\": 0.35, \"lambda\": 0.25, \"seed\": 126, \"collinear\": None\n        }\n    ]\n\n    all_results = []\n    tau = 1e-6\n\n    for case in test_cases:\n        n, p, q_scales = case[\"n\"], case[\"p\"], case[\"q\"]\n        support_true_indices, coeffs_true = case[\"support\"], case[\"coeffs\"]\n        sigma, lambda_val, seed = case[\"sigma\"], case[\"lambda\"], case[\"seed\"]\n        collinear = case[\"collinear\"]\n\n        rng = np.random.default_rng(seed)\n\n        # Generate data\n        Z = rng.normal(size=(n, p))\n        if collinear:\n            j, k, delta = collinear[\"j\"], collinear[\"k\"], collinear[\"delta\"]\n            eta = rng.normal(size=n)\n            Z[:, k] = Z[:, j] + delta * eta\n\n        X_raw = Z * q_scales\n        \n        w_star = np.zeros(p)\n        for idx, val in zip(sorted(list(support_true_indices)), coeffs_true):\n            w_star[idx] = val\n        \n        epsilon = rng.normal(size=n)\n        y = X_raw @ w_star + sigma * epsilon\n\n        # Preprocessing\n        y_c = y - np.mean(y)\n\n        # Raw features (centered only)\n        X_raw_means = np.mean(X_raw, axis=0)\n        X_c = X_raw - X_raw_means\n\n        # Standardized features (centered and scaled)\n        X_raw_stds = np.std(X_raw, axis=0)\n        X_s = np.zeros_like(X_raw)\n        for j in range(p):\n            if X_raw_stds[j] > 1e-9: # Avoid division by zero\n                X_s[:, j] = (X_raw[:, j] - X_raw_means[j]) / X_raw_stds[j]\n            else:\n                X_s[:, j] = X_raw[:, j] - X_raw_means[j]\n\n        # LASSO fits\n        w_raw = lasso_coordinate_descent(X_c, y_c, lambda_val)\n        w_std = lasso_coordinate_descent(X_s, y_c, lambda_val)\n\n        # Calculate metrics\n        support_raw = {j for j, w_j in enumerate(w_raw) if abs(w_j) > tau}\n        support_std = {j for j, w_j in enumerate(w_std) if abs(w_j) > tau}\n\n        b = (support_raw == support_std)\n        \n        r_matches = sum(1 for j in range(p) if (j in support_raw) == (j in support_true_indices))\n        r = r_matches / p\n        \n        s_matches = sum(1 for j in range(p) if (j in support_std) == (j in support_true_indices))\n        s = s_matches / p\n        \n        u = np.array([1 if j in support_raw else 0 for j in range(p)])\n        v = np.array([1 if j in support_std else 0 for j in range(p)])\n        \n        corr_u_q = pearson_correlation(u, q_scales)\n        corr_v_q = pearson_correlation(v, q_scales)\n        c = corr_u_q - corr_v_q\n        \n        all_results.append([b, r, s, c])\n\n    # Format final output string\n    formatted_cases = []\n    for res_list in all_results:\n        # Convert bool to string 'True'/'False'\n        res_list[0] = str(res_list[0])\n        # Format list to string \"[val1,val2,...]\"\n        formatted_cases.append(f\"[{','.join(map(str, res_list))}]\")\n    \n    final_output = f\"[{','.join(formatted_cases)}]\"\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}