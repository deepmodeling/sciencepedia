{
    "hands_on_practices": [
        {
            "introduction": "In RNA-sequencing analysis, raw read counts are not directly comparable due to variations in gene length and total sequencing depth per sample, often called library size. The Transcripts Per Million (TPM) normalization is a standard method designed to address both issues, providing a more accurate estimate of relative gene abundance. This first practice will guide you through the calculation of TPM from first principles, solidifying your understanding of how it corrects for these technical confounders to make gene expression values comparable across a dataset .",
            "id": "4562784",
            "problem": "You are building gene expression features for downstream prognostic modeling in RNA sequencing data within clinical bioinformatics. A central normalization used in feature engineering is Transcripts Per Million (TPM), which adjusts for gene length and library size to yield comparable abundance estimates across samples. Consider a toy dataset comprising $3$ genes, labeled $G_1$, $G_2$, and $G_3$, with lengths $L_{G_1} = 1000$ base pairs (bp), $L_{G_2} = 2000$ bp, and $L_{G_3} = 500$ bp. Two patient samples, $S_1$ and $S_2$, have observed gene-level read counts $r_{g,s}$ as follows: for $S_1$, $(r_{G_1,S_1}, r_{G_2,S_1}, r_{G_3,S_1}) = (300, 200, 200)$; for $S_2$, $(r_{G_1,S_2}, r_{G_2,S_2}, r_{G_3,S_2}) = (200, 600, 250)$. Using first principles appropriate to RNA sequencing normalization, namely that expression estimation must correct for gene length (by scaling counts by gene length measured in kilobases) and for sample-specific library size (by rescaling within-sample abundance to a fixed constant of $10^6$), derive the Transcripts Per Million (TPM) values for each gene in each sample. Explicitly verify, as part of your derivation, that within each sample the TPM values sum to $10^6$. Finally, report the TPM of gene $G_3$ in sample $S_2$. Round your final numerical answer to four significant figures and express it as a pure number (no units).",
            "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded in the principles of RNA sequencing data normalization, well-posed with all necessary information provided, and objective in its language. The problem requires the calculation of Transcripts Per Million (TPM) values for a toy dataset, a standard procedure in bioinformatics.\n\nThe calculation of TPM is a two-step normalization process. First, read counts are normalized for gene length. Second, they are normalized for sequencing depth (library size) to make them comparable across samples.\n\nLet $r_{g,s}$ be the raw read count for gene $g$ in sample $s$.\nLet $L_g$ be the length of gene $g$ in base pairs (bp).\n\nThe given data are:\nGene lengths:\n$L_{G_1} = 1000$ bp\n$L_{G_2} = 2000$ bp\n$L_{G_3} = 500$ bp\n\nRead counts for sample $S_1$:\n$(r_{G_1,S_1}, r_{G_2,S_1}, r_{G_3,S_1}) = (300, 200, 200)$\n\nRead counts for sample $S_2$:\n$(r_{G_1,S_2}, r_{G_2,S_2}, r_{G_3,S_2}) = (200, 600, 250)$\n\nThe TPM calculation proceeds as follows:\n\n**Step 1: Normalize for gene length.**\nFirst, we convert the gene lengths from base pairs (bp) to kilobases (kb), as specified. Let $L'_g$ be the length of gene $g$ in kb.\n$$L'_g = \\frac{L_g}{1000}$$\nFor our $3$ genes:\n$L'_{G_1} = \\frac{1000}{1000} = 1.0 \\text{ kb}$\n$L'_{G_2} = \\frac{2000}{1000} = 2.0 \\text{ kb}$\n$L'_{G_3} = \\frac{500}{1000} = 0.5 \\text{ kb}$\n\nNext, we calculate the rate of Reads Per Kilobase (RPK) for each gene in each sample.\n$$RPK_{g,s} = \\frac{r_{g,s}}{L'_g}$$\n\n**Calculations for Sample $S_1$:**\n$RPK_{G_1,S_1} = \\frac{r_{G_1,S_1}}{L'_{G_1}} = \\frac{300}{1.0} = 300$\n$RPK_{G_2,S_1} = \\frac{r_{G_2,S_1}}{L'_{G_2}} = \\frac{200}{2.0} = 100$\n$RPK_{G_3,S_1} = \\frac{r_{G_3,S_1}}{L'_{G_3}} = \\frac{200}{0.5} = 400$\n\n**Calculations for Sample $S_2$:**\n$RPK_{G_1,S_2} = \\frac{r_{G_1,S_2}}{L'_{G_1}} = \\frac{200}{1.0} = 200$\n$RPK_{G_2,S_2} = \\frac{r_{G_2,S_2}}{L'_{G_2}} = \\frac{600}{2.0} = 300$\n$RPK_{G_3,S_2} = \\frac{r_{G_3,S_2}}{L'_{G_3}} = \\frac{250}{0.5} = 500$\n\n**Step 2: Normalize for library size and scale to $10^6$.**\nWe compute a per-sample normalization factor, $T_s$, by summing the RPK values across all genes within that sample.\n$$T_s = \\sum_{g} RPK_{g,s}$$\nThen, the TPM value for each gene is calculated by dividing its RPK by the normalization factor and multiplying by $10^6$.\n$$TPM_{g,s} = \\left(\\frac{RPK_{g,s}}{T_s}\\right) \\times 10^6$$\n\n**Calculations for Sample $S_1$:**\nFirst, calculate the normalization factor $T_{S_1}$:\n$T_{S_1} = RPK_{G_1,S_1} + RPK_{G_2,S_1} + RPK_{G_3,S_1} = 300 + 100 + 400 = 800$\n\nNow, calculate the TPM values for each gene in $S_1$:\n$TPM_{G_1,S_1} = \\left(\\frac{300}{800}\\right) \\times 10^6 = 0.375 \\times 10^6 = 375000$\n$TPM_{G_2,S_1} = \\left(\\frac{100}{800}\\right) \\times 10^6 = 0.125 \\times 10^6 = 125000$\n$TPM_{G_3,S_1} = \\left(\\frac{400}{800}\\right) \\times 10^6 = 0.5 \\times 10^6 = 500000$\n\nVerification for $S_1$: The sum of TPMs must be $10^6$.\n$\\sum_{g} TPM_{g,S_1} = 375000 + 125000 + 500000 = 1000000 = 10^6$. The condition is met.\n\n**Calculations for Sample $S_2$:**\nFirst, calculate the normalization factor $T_{S_2}$:\n$T_{S_2} = RPK_{G_1,S_2} + RPK_{G_2,S_2} + RPK_{G_3,S_2} = 200 + 300 + 500 = 1000$\n\nNow, calculate the TPM values for each gene in $S_2$:\n$TPM_{G_1,S_2} = \\left(\\frac{200}{1000}\\right) \\times 10^6 = 0.2 \\times 10^6 = 200000$\n$TPM_{G_2,S_2} = \\left(\\frac{300}{1000}\\right) \\times 10^6 = 0.3 \\times 10^6 = 300000$\n$TPM_{G_3,S_2} = \\left(\\frac{500}{1000}\\right) \\times 10^6 = 0.5 \\times 10^6 = 500000$\n\nVerification for $S_2$: The sum of TPMs must be $10^6$.\n$\\sum_{g} TPM_{g,S_2} = 200000 + 300000 + 500000 = 1000000 = 10^6$. The condition is met.\n\nThe problem asks for the TPM of gene $G_3$ in sample $S_2$. Based on the calculation above:\n$TPM_{G_3,S_2} = 500000$\n\nThe final answer must be rounded to four significant figures. The exact value is $500000$. To represent this with four significant figures, we use scientific notation: $5.000 \\times 10^5$.",
            "answer": "$$\\boxed{5.000 \\times 10^{5}}$$"
        },
        {
            "introduction": "While normalization is essential, no method is universally applicable, and choosing an inappropriate one can lead to erroneous conclusions. This exercise presents a critical thought experiment examining global scaling, a simple normalization method that assumes a stable total RNA output across samples. Through a carefully constructed hypothetical scenario, you will demonstrate how violating this assumption can systematically attenuate a true biological signal, highlighting the importance of scrutinizing the biological context before selecting a normalization strategy .",
            "id": "4562808",
            "problem": "A research team is conducting RNA sequencing (RNA-seq) to measure gene expression changes under a treatment. They consider a single gene of interest $G_{T}$ and a set of housekeeping genes $H_{1}, H_{2}, H_{3}$. Housekeeping genes are typically used to define the global expression baseline but, in this experiment, the housekeeping genes are biologically affected by the treatment. The team uses global scaling by total read counts, analogous to counts per million (CPM), as follows: for sample $s$, the normalized value of gene $g$ is $x_{g,s}/T_{s}$, where $x_{g,s}$ is the observed raw count and $T_{s}=\\sum_{g}x_{g,s}$ is the total count in sample $s$. The normalized fold change for $G_{T}$ is defined as the ratio of its normalized value in treatment to its normalized value in control, and the normalized log base $2$ fold change is the base-$2$ logarithm of that ratio.\n\nAssume the following observed raw counts are scientifically plausible and self-consistent with sequencing depth and expression levels:\n- Control sample counts: $x_{G_{T},\\mathrm{ctrl}}=100$, $x_{H_{1},\\mathrm{ctrl}}=100$, $x_{H_{2},\\mathrm{ctrl}}=100$, $x_{H_{3},\\mathrm{ctrl}}=100$.\n- Treatment sample counts: $x_{G_{T},\\mathrm{trt}}=200$, $x_{H_{1},\\mathrm{trt}}=150$, $x_{H_{2},\\mathrm{trt}}=150$, $x_{H_{3},\\mathrm{trt}}=150$.\n\nTreat the raw counts as proportional to true expression multiplied by sample-specific library size. You may assume no other genes contribute to the totals. From first principles, compute the normalized log base $2$ fold change for $G_{T}$ when using the defined global total-count scaling. Express your answer as an exact base-$2$ logarithmic expression. Do not approximate, and do not include any units.",
            "solution": "The foundational premise in RNA sequencing (RNA-seq) quantification is that an observed raw count $x_{g,s}$ for gene $g$ in sample $s$ scales proportionally with the true expression level of $g$ in $s$ and the sample-specific library size $L_{s}$. Global scaling by total counts corrects for differences in $L_{s}$ by dividing each $x_{g,s}$ by the sample total $T_{s}=\\sum_{g}x_{g,s}$, yielding a normalized value $x_{g,s}/T_{s}$ that is proportional to the true expression fraction of gene $g$ within sample $s$.\n\nThe fold change between treatment and control for a gene under a given normalization is defined as the ratio of the normalized treatment value to the normalized control value. The log base $2$ fold change is $\\log_{2}$ of that ratio.\n\nWe proceed step by step using these definitions.\n\nFirst, compute the total counts in each sample:\n- Control total:\n$$\nT_{\\mathrm{ctrl}}=x_{G_{T},\\mathrm{ctrl}}+x_{H_{1},\\mathrm{ctrl}}+x_{H_{2},\\mathrm{ctrl}}+x_{H_{3},\\mathrm{ctrl}}=100+100+100+100=400.\n$$\n- Treatment total:\n$$\nT_{\\mathrm{trt}}=x_{G_{T},\\mathrm{trt}}+x_{H_{1},\\mathrm{trt}}+x_{H_{2},\\mathrm{trt}}+x_{H_{3},\\mathrm{trt}}=200+150+150+150=650.\n$$\n\nNext, compute the normalized values for $G_{T}$ in each condition under global scaling:\n- Control normalized value for $G_{T}$:\n$$\n\\frac{x_{G_{T},\\mathrm{ctrl}}}{T_{\\mathrm{ctrl}}}=\\frac{100}{400}=\\frac{1}{4}.\n$$\n- Treatment normalized value for $G_{T}$:\n$$\n\\frac{x_{G_{T},\\mathrm{trt}}}{T_{\\mathrm{trt}}}=\\frac{200}{650}=\\frac{40}{130}=\\frac{8}{26}=\\frac{4}{13}.\n$$\n\nCompute the normalized fold change for $G_{T}$ as the ratio of these normalized values:\n$$\n\\text{FC}_{\\mathrm{norm}}=\\frac{\\frac{x_{G_{T},\\mathrm{trt}}}{T_{\\mathrm{trt}}}}{\\frac{x_{G_{T},\\mathrm{ctrl}}}{T_{\\mathrm{ctrl}}}}=\\frac{\\frac{200}{650}}{\\frac{100}{400}}=\\frac{200}{650}\\cdot\\frac{400}{100}=\\frac{800}{650}=\\frac{16}{13}.\n$$\n\nFinally, compute the normalized log base $2$ fold change:\n$$\n\\log_{2}\\left(\\text{FC}_{\\mathrm{norm}}\\right)=\\log_{2}\\left(\\frac{16}{13}\\right).\n$$\n\nTo interpret this as a counterexample demonstrating attenuation, note the true fold change in raw counts for $G_{T}$ is\n$$\n\\text{FC}_{\\mathrm{raw}}=\\frac{200}{100}=2,\n$$\nwith true log base $2$ fold change $\\log_{2}(2)=1$. Under global scaling by total counts, the normalized fold change becomes $\\frac{16}{13}$, which is strictly less than $2$, and the normalized log base $2$ fold change $\\log_{2}\\left(\\frac{16}{13}\\right)$ is strictly less than $1$. The attenuation arises because the housekeeping genes $H_{1},H_{2},H_{3}$ increase under treatment, inflating $T_{\\mathrm{trt}}$ relative to $T_{\\mathrm{ctrl}}$; dividing by the larger $T_{\\mathrm{trt}}$ reduces the normalized value of $G_{T}$ in treatment compared to control more than would occur if housekeeping genes were truly invariant. This demonstrates, from first principles, that global scaling can attenuate a true treatment effect when housekeeping genes are differentially expressed.\n\nThus, the requested exact base-$2$ logarithmic expression is $\\log_{2}\\left(\\frac{16}{13}\\right)$.",
            "answer": "$$\\boxed{\\log_{2}\\!\\left(\\frac{16}{13}\\right)}$$"
        },
        {
            "introduction": "Bioinformatics datasets, such as those from microbiome sequencing, are often compositional, meaning they represent parts of a whole and carry only relative information. Applying standard distance metrics in these cases can be misleading, as they fail to respect the data's inherent geometric constraints. This advanced practice involves a simulation study where you will compare naive, log-transformed, and the principled Centered Log-Ratio (CLR) transformation, allowing you to computationally verify why methods from Compositional Data Analysis are superior for recovering true biological relationships from such data .",
            "id": "4562735",
            "problem": "You are given a synthetic scenario modeling high-throughput sequencing data in bioinformatics and medical data analytics, where measured taxon counts per sample are compositional by construction. The downstream goal is to decide, by principled computation, which feature normalization and transformation pipeline yields pairwise distances that best align with the latent, data-generating process. Your program must be a complete, runnable program that constructs the data, computes pairwise distances under specified transforms, quantitatively assesses alignment to the ground truth, and outputs a final aggregated decision for a provided test suite.\n\nFundamental base and definitions:\n- Compositional data consist of positive vectors that are only informative up to scale. The closure operator maps any positive vector $\\mathbf{v} \\in \\mathbb{R}_{>0}^{K}$ to the simplex by $C(\\mathbf{v}) = \\mathbf{v} / \\left(\\sum_{k=1}^{K} v_k\\right)$.\n- The Centered Log-Ratio (CLR) transform of a composition $\\mathbf{x} \\in \\mathbb{R}_{>0}^{K}$ is defined as $\\operatorname{clr}(\\mathbf{x}) = \\log(\\mathbf{x}) - \\frac{1}{K}\\mathbf{1}\\mathbf{1}^{\\top}\\log(\\mathbf{x})$, where $\\log(\\cdot)$ is applied elementwise and $\\mathbf{1}$ is the all-ones vector in $\\mathbb{R}^{K}$.\n- The Aitchison distance between two compositions equals the Euclidean distance between their CLR transforms.\n- The Euclidean distance between two vectors $\\mathbf{u},\\mathbf{v} \\in \\mathbb{R}^{K}$ is $\\|\\mathbf{u} - \\mathbf{v}\\|_{2}$.\n- The Pearson correlation between two real vectors is defined as the covariance divided by the product of standard deviations.\n\nData-generating process:\n- There are $K$ taxa, $N$ samples, and $F$ latent factors. For sample $i \\in \\{1,\\ldots,N\\}$, the latent absolute abundance vector is\n$$\n\\log \\mathbf{A}_i = \\mathbf{b} + \\mathbf{W}\\,\\mathbf{s}_i,\n$$\nwhere $\\mathbf{b} \\in \\mathbb{R}^{K}$ is a baseline log-abundance vector, $\\mathbf{W} \\in \\mathbb{R}^{K \\times F}$ is a loadings matrix, and $\\mathbf{s}_i \\in \\mathbb{R}^{F}$ is the factor score vector for sample $i$. Absolute abundances are $\\mathbf{A}_i = \\exp(\\log \\mathbf{A}_i)$, applied elementwise.\n- Observed counts before rounding are $\\tilde{\\mathbf{c}}_i = L_i \\, C(\\mathbf{A}_i)$, where $L_i \\in \\mathbb{R}_{>0}$ is the library size (a strictly positive scalar) for sample $i$. If rounding is enabled for a test case, the observed counts become $\\mathbf{c}_i = \\operatorname{round}(\\tilde{\\mathbf{c}}_i)$, where $\\operatorname{round}(\\cdot)$ denotes rounding to the nearest integer, applied elementwise. If rounding is disabled, set $\\mathbf{c}_i = \\tilde{\\mathbf{c}}_i$.\n\nGround-truth distances:\n- The target distances induced by the data-generating process are defined between samples $i$ and $j$ as\n$$\nD_{\\mathrm{true}}(i,j) = \\left\\|\\log \\mathbf{A}_i - \\log \\mathbf{A}_j \\right\\|_{2}.\n$$\n\nCandidate pipelines and their pairwise distances:\n- Raw-count Euclidean: $D_{\\mathrm{raw}}(i,j) = \\left\\| \\mathbf{c}_i - \\mathbf{c}_j \\right\\|_{2}$.\n- Log-count Euclidean (with pseudocount): given a pseudocount $\\epsilon > 0$, define $\\mathbf{z}_i = \\log(\\mathbf{c}_i + \\epsilon)$ elementwise, and compute $D_{\\log}(i,j) = \\left\\|\\mathbf{z}_i - \\mathbf{z}_j\\right\\|_{2}$.\n- Aitchison via CLR (with pseudocount before closure): given a pseudocount $\\epsilon > 0$, define $\\mathbf{u}_i = \\mathbf{c}_i + \\epsilon$, then proportions $\\mathbf{p}_i = C(\\mathbf{u}_i)$, then $\\operatorname{clr}(\\mathbf{p}_i)$, and compute $D_{\\mathrm{clr}}(i,j) = \\left\\| \\operatorname{clr}(\\mathbf{p}_i) - \\operatorname{clr}(\\mathbf{p}_j) \\right\\|_{2}$.\n\nAlignment criterion:\n- For each candidate distance type $T \\in \\{\\mathrm{raw},\\log,\\mathrm{clr}\\}$, vectorize the upper-triangle (excluding the diagonal) of the pairwise distance matrix $D_T$ and also of $D_{\\mathrm{true}}$ into vectors $\\mathbf{d}_T$ and $\\mathbf{d}_{\\mathrm{true}}$. Compute the Pearson correlation $r_T$ between $\\mathbf{d}_T$ and $\\mathbf{d}_{\\mathrm{true}}$.\n- The best-aligned metric is the one with the largest $r_T$. In the event of a tie within a tolerance $\\gamma = 10^{-9}$, choose the metric with the smallest index according to the ordering $\\mathrm{raw} \\mapsto 0$, $\\log \\mapsto 1$, $\\mathrm{clr} \\mapsto 2$.\n\nTest suite:\nUse $K=5$ taxa, $N=4$ samples, and $F=2$ factors. Use the following shared, fixed parameters for all tests:\n- Baseline log-abundance vector $\\mathbf{b} = [\\,1.0,\\,0.5,\\,-0.2,\\,0.3,\\,-0.1\\,]$.\n- Loadings matrix with columns summing to zero across taxa to model compositional shifts unimpaired by constant offsets:\n$$\n\\mathbf{W} = \n\\begin{bmatrix}\n0.8 & -0.5 \\\\\n0.2 & 0.1 \\\\\n-0.4 & 0.2 \\\\\n-0.3 & 0.4 \\\\\n-0.3 & -0.2\n\\end{bmatrix}.\n$$\n- Sample factor scores\n$$\n\\mathbf{s}_1 = \\begin{bmatrix} -1.0 \\\\ 0.5 \\end{bmatrix},\\quad\n\\mathbf{s}_2 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix},\\quad\n\\mathbf{s}_3 = \\begin{bmatrix} 0.5 \\\\ -0.5 \\end{bmatrix},\\quad\n\\mathbf{s}_4 = \\begin{bmatrix} 1.2 \\\\ 0.8 \\end{bmatrix}.\n$$\n\nDefine three test cases that differ only in library sizes, rounding, and pseudocount:\n- Test case A (happy path, equal libraries; minimal compositional distortion):\n  - Library sizes $\\mathbf{L}^{(A)} = [\\,10000,\\,10000,\\,10000,\\,10000\\,]$.\n  - Rounding enabled.\n  - Pseudocount $\\epsilon^{(A)} = 1.0$.\n- Test case B (library confounding; strong differences in total counts):\n  - Library sizes $\\mathbf{L}^{(B)} = [\\,500,\\,20000,\\,200,\\,80000\\,]$.\n  - Rounding enabled.\n  - Pseudocount $\\epsilon^{(B)} = 1.0$.\n- Test case C (sparse regime with many small counts inducing zeros; robustness to pseudocount choice):\n  - Library sizes $\\mathbf{L}^{(C)} = [\\,100,\\,120,\\,80,\\,60\\,]$.\n  - Rounding enabled.\n  - Pseudocount $\\epsilon^{(C)} = 0.5$.\n\nRequired outputs:\n- For each test case, compute $r_{\\mathrm{raw}}$, $r_{\\log}$, and $r_{\\mathrm{clr}}$ as defined, determine the best-aligned metric using the tie-breaking rule with $\\gamma = 10^{-9}$, and encode it as an integer according to $\\mathrm{raw} \\mapsto 0$, $\\log \\mapsto 1$, $\\mathrm{clr} \\mapsto 2$.\n- Your program should produce a single line of output containing the results for the three test cases as a comma-separated list of integers enclosed in square brackets (e.g., \"[2,2,2]\"). No additional characters or whitespace are permitted in the output.\n\nAngle units, physical units, and percentages do not apply to this problem. Ensure all computations use the natural logarithm. The program must be self-contained, must not accept any input, and must rely only on the specified environment. All numbers must be handled and represented internally in standard floating-point arithmetic.",
            "solution": "The posed problem requires an evaluation of three distinct feature engineering and normalization pipelines for compositional data, which is characteristic of high-throughput sequencing outputs in bioinformatics. The goal is to determine which pipeline's resulting sample-to-sample distances best correlate with a known, ground-truth distance matrix derived from the latent data-generating process. This is a synthetic benchmark designed to quantitatively assess the efficacy of common data transformation techniques.\n\nFirst, we will construct the dataset according to the specified data-generating process. This process is founded on a log-linear model, a standard approach for simulating count-based biological data. For each of the $N=4$ samples, its latent state is defined by a factor score vector $\\mathbf{s}_i \\in \\mathbb{R}^{F}$ where $F=2$. These scores modulate a baseline log-abundance vector $\\mathbf{b} \\in \\mathbb{R}^{K}$ (with $K=5$ taxa) via a loadings matrix $\\mathbf{W} \\in \\mathbb{R}^{K \\times F}$. The latent log-absolute abundance vector for sample $i$ is given by:\n$$\n\\log \\mathbf{A}_i = \\mathbf{b} + \\mathbf{W}\\,\\mathbf{s}_i\n$$\nThe absolute abundances $\\mathbf{A}_i$ are obtained by elementwise exponentiation, $\\mathbf{A}_i = \\exp(\\log \\mathbf{A}_i)$. These represent the true, unobserved quantities of each taxon.\n\nIn sequencing experiments, the total number of reads per sample (library size, $L_i$) is a technical artifact and does not typically reflect total biomass. The measurement process captures relative, not absolute, abundances. This is modeled by applying the closure operator $C(\\mathbf{v}) = \\mathbf{v} / \\sum_{k=1}^{K} v_k$ to the absolute abundances, yielding a composition $\\mathbf{p}_i = C(\\mathbf{A}_i)$ on the simplex. The pre-rounded observed counts are then simulated by scaling this composition by the library size, $\\tilde{\\mathbf{c}}_i = L_i \\mathbf{p}_i$. Finally, these are rounded to the nearest integer to mimic discrete count data, $\\mathbf{c}_i = \\operatorname{round}(\\tilde{\\mathbf{c}}_i)$. This process creates a dataset where sample-to-sample differences are a mixture of true biological variation (from $\\mathbf{s}_i$) and technical artifacts (from $L_i$).\n\nThe ground-truth distance between any two samples, $i$ and $j$, is defined in the latent space of log-abundances:\n$$\nD_{\\mathrm{true}}(i,j) = \\left\\|\\log \\mathbf{A}_i - \\log \\mathbf{A}_j \\right\\|_{2}\n$$\nThis is the Euclidean distance between the unobserved log-abundance vectors. It represents the \"true\" dissimilarity that an ideal analysis pipeline should recover from the observed count data $\\mathbf{c}$.\n\nNext, we calculate sample-to-sample distances using three candidate pipelines applied to the observed counts $\\mathbf{c}_i$.\n\n$1$. **Raw-count Euclidean distance ($D_{\\mathrm{raw}}$)**:\n$D_{\\mathrm{raw}}(i,j) = \\left\\| \\mathbf{c}_i - \\mathbf{c}_j \\right\\|_{2}$. This is the most naive approach. It treats the counts as measurements in a standard Euclidean space. This metric is highly sensitive to differences in library sizes ($L_i$), which can dominate the distance calculation and obscure the underlying compositional differences. It is expected to perform poorly, especially in Test Case B where library sizes vary dramatically.\n\n$2$. **Log-count Euclidean distance ($D_{\\log}$)**:\nGiven a pseudocount $\\epsilon > 0$, we transform the counts to $\\mathbf{z}_i = \\log(\\mathbf{c}_i + \\epsilon)$ and compute $D_{\\log}(i,j) = \\left\\|\\mathbf{z}_i - \\mathbf{z}_j\\right\\|_{2}$. The logarithm helps to down-weight the influence of large counts and makes the data more symmetric. The pseudocount $\\epsilon$ is necessary to handle zero counts, which would otherwise be undefined under the logarithm. While often an improvement over raw counts, this method does not fully account for the unit-sum constraint of compositional data.\n\n$3$. **Aitchison distance via Centered Log-Ratio (CLR) transform ($D_{\\mathrm{clr}}$)**:\nThis is a principled approach from Compositional Data Analysis (CoDa). It correctly handles the relative nature of the data. First, a pseudocount $\\epsilon > 0$ is added to the raw counts to handle zeros: $\\mathbf{u}_i = \\mathbf{c}_i + \\epsilon$. Then, the data is closed to a composition, $\\mathbf{p}_i = C(\\mathbf{u}_i)$. The CLR transform is then applied:\n$$\n\\operatorname{clr}(\\mathbf{p}_i) = \\log(\\mathbf{p}_i) - \\frac{1}{K}\\sum_{k=1}^{K}\\log(p_{ik})\n$$\nThis transform maps the $K$-part compositional data from the simplex to a $(K-1)$-dimensional Euclidean space. The Aitchison distance is defined as the standard Euclidean distance in this CLR-space:\n$$\nD_{\\mathrm{clr}}(i,j) = \\left\\| \\operatorname{clr}(\\mathbf{p}_i) - \\operatorname{clr}(\\mathbf{p}_j) \\right\\|_{2}\n$$\nBy centering on the geometric mean of each composition's parts, the CLR transform effectively removes the influence of scale (and thus library size), focusing only on the relative changes between components. This method is theoretically best-suited for this type of data.\n\nTo evaluate which pipeline is superior, we compare the distances from each candidate pipeline to the ground-truth distances. For each distance type $T \\in \\{\\mathrm{raw}, \\log, \\mathrm{clr}\\}$, we form a vector $\\mathbf{d}_T$ containing all unique pairwise distances (the upper triangle of the distance matrix). We do the same for the true distances, creating vector $\\mathbf{d}_{\\mathrm{true}}$. The performance metric is the Pearson correlation coefficient, $r_T = \\operatorname{corr}(\\mathbf{d}_T, \\mathbf{d}_{\\mathrm{true}})$. The pipeline yielding the highest correlation is deemed the best. A tie-breaking rule, based on a tolerance of $\\gamma = 10^{-9}$ and a predefined ordering ($\\mathrm{raw} \\mapsto 0$, $\\log \\mapsto 1$, $\\mathrm{clr} \\mapsto 2$), ensures a unique winner.\n\nThe overall algorithm proceeds as follows for each of the three test cases:\n$1$. Generate the observed count matrix $\\{\\mathbf{c}_i\\}_{i=1..N}$ using the specified parameters for that test case.\n$2$. Compute the $N \\times N$ ground-truth distance matrix $D_{\\mathrm{true}}$ and vectorize its upper triangle into $\\mathbf{d}_{\\mathrm{true}}$.\n$3$. For each of the three pipelines, compute the corresponding $N \\times N$ distance matrix ($D_{\\mathrm{raw}}$, $D_{\\log}$, $D_{\\mathrm{clr}}$) and vectorize its upper triangle ($\\mathbf{d}_{\\mathrm{raw}}$, $\\mathbf{d}_{\\log}$, $\\mathbf{d}_{\\mathrm{clr}}$).\n$4$. Calculate the three Pearson correlations: $r_{\\mathrm{raw}}$, $r_{\\log}$, and $r_{\\mathrm{clr}}$.\n$5$. Identify the largest correlation value and use the tie-breaking rule to select the index of the winning pipeline.\nThis process will be repeated for all test cases, and the final list of winning indices will be reported.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by generating compositional data, computing distances\n    via three pipelines, and evaluating their alignment with ground-truth distances.\n    \"\"\"\n    # Shared parameters across all test cases\n    K = 5  # Number of taxa\n    N = 4  # Number of samples\n    F = 2  # Number of latent factors\n    gamma = 1e-9  # Tie-breaking tolerance\n\n    b = np.array([1.0, 0.5, -0.2, 0.3, -0.1])\n    W = np.array([\n        [0.8, -0.5],\n        [0.2, 0.1],\n        [-0.4, 0.2],\n        [-0.3, 0.4],\n        [-0.3, -0.2]\n    ])\n    s_vectors = [\n        np.array([-1.0, 0.5]),\n        np.array([0.0, 0.0]),\n        np.array([0.5, -0.5]),\n        np.array([1.2, 0.8])\n    ]\n\n    # Definition of the three test cases\n    test_cases = [\n        {\n            \"name\": \"A\",\n            \"L\": np.array([10000.0, 10000.0, 10000.0, 10000.0]),\n            \"rounding\": True,\n            \"epsilon\": 1.0\n        },\n        {\n            \"name\": \"B\",\n            \"L\": np.array([500.0, 20000.0, 200.0, 80000.0]),\n            \"rounding\": True,\n            \"epsilon\": 1.0\n        },\n        {\n            \"name\": \"C\",\n            \"L\": np.array([100.0, 120.0, 80.0, 60.0]),\n            \"rounding\": True,\n            \"epsilon\": 0.5\n        }\n    ]\n\n    # Mapping from metric name to index\n    metric_map = {\"raw\": 0, \"log\": 1, \"clr\": 2}\n    \n    final_results = []\n\n    for case in test_cases:\n        L = case[\"L\"]\n        rounding = case[\"rounding\"]\n        epsilon = case[\"epsilon\"]\n\n        # 1. Data Generation\n        log_A = []\n        for i in range(N):\n            log_A_i = b + W @ s_vectors[i]\n            log_A.append(log_A_i)\n        \n        A = [np.exp(la) for la in log_A]\n        \n        # Closure operator C(v)\n        compositions = [a / np.sum(a) for a in A]\n        \n        # Scale by library size to get counts\n        c_tilde = [L[i] * compositions[i] for i in range(N)]\n        \n        if rounding:\n            c = [np.round(ct) for ct in c_tilde]\n        else:\n            c = c_tilde\n\n        # 2. Distance Calculations and Vectorization\n        d_true = []\n        d_raw = []\n        d_log = []\n        d_clr = []\n        \n        # Helper for CLR transform\n        def clr_transform(vec):\n            # The problem applies pseudocount before closure and CLR\n            # The input vec to this function will be pre-processed (closed composition)\n            log_vec = np.log(vec)\n            return log_vec - np.mean(log_vec)\n\n        pairs = []\n        for i in range(N):\n            for j in range(i + 1, N):\n                pairs.append((i, j))\n\n        for i, j in pairs:\n            # Ground-truth distance\n            dist_true = np.linalg.norm(log_A[i] - log_A[j])\n            d_true.append(dist_true)\n            \n            # Raw-count Euclidean distance\n            dist_raw = np.linalg.norm(c[i] - c[j])\n            d_raw.append(dist_raw)\n\n            # Log-count Euclidean distance\n            z_i = np.log(c[i] + epsilon)\n            z_j = np.log(c[j] + epsilon)\n            dist_log = np.linalg.norm(z_i - z_j)\n            d_log.append(dist_log)\n            \n            # Aitchison distance via CLR\n            u_i = c[i] + epsilon\n            u_j = c[j] + epsilon\n            \n            p_i = u_i / np.sum(u_i)\n            p_j = u_j / np.sum(u_j)\n            \n            clr_i = clr_transform(p_i)\n            clr_j = clr_transform(p_j)\n            \n            dist_clr = np.linalg.norm(clr_i - clr_j)\n            d_clr.append(dist_clr)\n\n        d_true_vec = np.array(d_true)\n        all_d_vecs = {\n            \"raw\": np.array(d_raw),\n            \"log\": np.array(d_log),\n            \"clr\": np.array(d_clr)\n        }\n\n        # 3. Alignment Criterion\n        correlations = {}\n        for name, d_vec in all_d_vecs.items():\n            # Pearson correlation\n            # np.corrcoef returns a 2x2 matrix\n            corr_matrix = np.corrcoef(d_true_vec, d_vec)\n            correlations[name] = corr_matrix[0, 1]\n\n        # 4. Decision\n        corr_list = np.array([correlations[\"raw\"], correlations[\"log\"], correlations[\"clr\"]])\n        \n        # Replace NaN correlations with a very small number if a distance vector is constant\n        # This can happen if all differences are zero for a metric\n        corr_list = np.nan_to_num(corr_list, nan=-np.inf)\n\n        max_corr = np.max(corr_list)\n        \n        # Find indices of all metrics that are within gamma of the max\n        tied_indices = np.where(np.abs(corr_list - max_corr)  gamma)[0]\n        \n        # The winner is the one with the smallest index among those tied\n        best_index = np.min(tied_indices)\n        \n        final_results.append(best_index)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n```"
        }
    ]
}