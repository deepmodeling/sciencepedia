{
    "hands_on_practices": [
        {
            "introduction": "This first practice moves from theory to application by guiding you through the implementation of one of the most common algorithms for solving the LASSO: coordinate descent. You will build a solver for a synthetic genomics dataset and trace the 'LASSO path', which reveals the order in which features become non-zero as the regularization parameter $\\lambda$ decreases. This exercise provides a powerful, tangible intuition for how the $\\ell_1$ penalty enforces sparsity and performs feature selection in practice. ",
            "id": "4605278",
            "problem": "You are given a synthetic Single Nucleotide Polymorphism (SNP) genotype dataset and a continuous phenotype constructed from a sparse linear model. The features are coded as minor allele counts $0$, $1$, or $2$, which are then standardized to have zero mean and unit variance, and the phenotype is centered to zero mean. Consider the Least Absolute Shrinkage and Selection Operator (LASSO) problem for linear regression with coordinate descent. Your task is to implement a complete, runnable program that computes the LASSO solution along a decreasing grid of regularization strengths $\\lambda$ using coordinate descent and reports the path of coefficient entry: the order in which standardized feature coefficients become nonzero as $\\lambda$ decreases. You must strictly adhere to the final output formatting requirement stated below.\n\nFundamental base to use:\n- The linear model for a centered phenotype $y \\in \\mathbb{R}^n$ and standardized design matrix $X \\in \\mathbb{R}^{n \\times p}$, with columns centered and scaled so that $\\frac{1}{n}\\sum_{i=1}^n x_{ij}^2 = 1$ for each feature index $j$.\n- The LASSO objective, which penalizes the $\\ell_1$ norm of the coefficients: minimize\n$$\n\\frac{1}{2n}\\lVert y - X\\beta \\rVert_2^2 + \\lambda \\lVert \\beta \\rVert_1,\n$$\nwhere $\\beta \\in \\mathbb{R}^p$ are the coefficients and $\\lambda \\ge 0$ is the regularization parameter.\n- The coordinate-wise soft-threshold update for standardized features derived from subgradient/Karush–Kuhn–Tucker (KKT) stationarity:\n$$\n\\beta_j \\leftarrow \\frac{1}{\\frac{1}{n}\\lVert X_{\\cdot j} \\rVert_2^2} \\cdot S\\!\\left(\\frac{1}{n}X_{\\cdot j}^\\top \\left(y - \\sum_{k \\ne j} X_{\\cdot k}\\beta_k\\right), \\lambda \\right),\n$$\nwith the soft-threshold operator defined by $S(a,t) = \\operatorname{sign}(a)\\max(|a| - t, 0)$. Under the given standardization, $\\frac{1}{n}\\lVert X_{\\cdot j} \\rVert_2^2 = 1$ and the denominator simplifies to $1$.\n\nData construction:\n- Let $n = 60$ individuals and $p = 8$ SNPs. For reproducibility, draw each SNP column $X_{\\cdot j}$ independently as $\\operatorname{Binomial}(2, q_j)$, where the minor allele frequencies (MAF) are fixed as $(q_1,\\dots,q_8) = (0.05, 0.10, 0.20, 0.15, 0.30, 0.25, 0.40, 0.35)$ using a fixed random seed. After generation, standardize each feature to have zero mean and unit variance as above.\n- Construct the centered phenotype $y$ from a sparse linear model with fixed true coefficients $\\beta^\\star = (0.0, 1.2, 0.0, -0.8, 0.0, 0.0, 0.5, 0.0)$ and a deterministic perturbation $\\epsilon_i = 0.1\\sin(i)$ for index $i = 0, 1, \\dots, n-1$, and then center $y$:\n$$\ny \\leftarrow X\\beta^\\star + \\epsilon, \\quad \\bar{y} = \\frac{1}{n}\\sum_{i=1}^n y_i, \\quad y \\leftarrow y - \\bar{y}.\n$$\n\nAlgorithmic requirements:\n- Use coordinate descent with cyclic updates over $j = 1, \\dots, p$ to solve the LASSO objective at each $\\lambda$ until convergence. Use a warm-start across the decreasing $\\lambda$ path.\n- Use the stopping criterion $\\max_j |\\Delta\\beta_j| < 10^{-6}$ or a maximum of $1000$ full coordinate sweeps, whichever occurs first.\n- Let $\\lambda_{\\max} = \\max_j \\left|\\frac{1}{n} X_{\\cdot j}^\\top y\\right|$, for which the solution is $\\beta = 0$. Build decreasing $\\lambda$ grids as specified in the test suite below from $\\lambda_{\\max}$ to smaller values.\n\nPath of coefficient entry:\n- For each feature index $j \\in \\{0,1,\\dots,p-1\\}$, define its \"entry $\\lambda$\" as the first $\\lambda$ value (in the decreasing grid) at which the converged $\\beta_j$ becomes nonzero (numerically, treat $|\\beta_j| > 10^{-8}$ as nonzero). If a coefficient never becomes nonzero over the grid, it has no entry and should not appear in the order list. In the presence of ties (multiple features entering at the same grid $\\lambda$), list features in ascending index order. The path of coefficient entry is the ordered list of feature indices according to first entry as $\\lambda$ decreases.\n\nTest suite:\nImplement the following four test cases using the same base dataset, with explicit $\\lambda$ grids. The design ensures a happy path, boundary, extreme, and collinearity edge case coverage.\n\n- Test Case 1 (happy path): Use a geometric grid of $50$ values from $\\lambda_{\\max}$ down to $0.01\\lambda_{\\max}$, i.e., $\\lambda_k = \\lambda_{\\max} \\cdot r^{k-1}$ with common ratio $r$ chosen so that $\\lambda_{50} = 0.01\\lambda_{\\max}$. Report the ordered list of feature indices that first become nonzero along this path.\n- Test Case 2 (boundary case): Use the singleton grid $\\{\\lambda_{\\max}\\}$ only. Report the ordered list of feature indices (expected to be empty).\n- Test Case 3 (extreme small $\\lambda$): Use a geometric grid of $120$ values from $\\lambda_{\\max}$ down to $10^{-6}\\lambda_{\\max}$. Report the ordered list of feature indices that first become nonzero along this path.\n- Test Case 4 (collinearity edge case): Create an augmented dataset by duplicating SNP feature index $3$ to form a ninth feature (resulting in $p=9$). Standardize the augmented matrix column-wise using the same protocol, recompute $\\lambda_{\\max}$ for the augmented dataset, and use a geometric grid of $50$ values from this augmented $\\lambda_{\\max}$ down to $0.01$ times it. Report the ordered list of feature indices that first become nonzero along this path.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element should itself be a bracketed, comma-separated list of integers representing the feature indices (zero-based) in their order of first entry for the corresponding test case. For example, a valid output might look like [[1,3,2],[],[1,7,5,3,6],[1,3,8]]. No other text should be printed.\n\nNo physical units or angle units are involved. All numeric answers are pure numbers. Each test-case result must be a list of integers as specified.",
            "solution": "The user has provided a well-defined computational problem in the domain of statistical machine learning, specifically focusing on the implementation of the LASSO regression model. The task is to compute the coefficient entry path for a synthetic single nucleotide polymorphism (SNP) dataset using a coordinate descent algorithm. The problem statement is validated to be scientifically sound, well-posed, and complete.\n\n### Step 1: Problem Validation\n\n**1.1. Givens Extraction:**\n- **Model:** Linear regression with LASSO penalty: $\\min_{\\beta} \\frac{1}{2n}\\lVert y - X\\beta \\rVert_2^2 + \\lambda \\lVert \\beta \\rVert_1$.\n- **Data Properties:** $y$ is a centered phenotype vector in $\\mathbb{R}^n$. $X$ is a standardized design matrix in $\\mathbb{R}^{n \\times p}$ with columns centered to zero mean and scaled such that $\\frac{1}{n}\\sum_{i=1}^n x_{ij}^2 = 1$.\n- **Algorithm:** Coordinate descent with cyclic updates. The update rule for a standardized feature $j$ is $\\beta_j \\leftarrow S(\\rho_j, \\lambda)$, where $\\rho_j = \\frac{1}{n}X_{\\cdot j}^\\top (y - \\sum_{k \\ne j} X_{\\cdot k}\\beta_k)$ and $S(a,t) = \\operatorname{sign}(a)\\max(|a| - t, 0)$ is the soft-thresholding operator.\n- **Convergence Criteria:** Maximum change in coefficients $\\max_j |\\Delta\\beta_j| < 10^{-6}$ or a maximum of $1000$ sweeps.\n- **Regularization Path:** The path starts from $\\lambda_{\\max} = \\max_j |\\frac{1}{n} X_{\\cdot j}^\\top y|$ and decreases along a specified grid. Warm starts are used, where the solution for one $\\lambda$ initializes the solver for the next smaller $\\lambda$.\n- **Dataset Generation:** $n=60$, $p=8$. SNP features $X_{\\cdot j}$ are drawn from a $\\operatorname{Binomial}(2, q_j)$ distribution with specified minor allele frequencies (MAFs) $(q_j) = (0.05, 0.10, \\dots, 0.35)$ using a fixed random seed. The phenotype is $y = X\\beta^\\star + \\epsilon - \\operatorname{mean}(X\\beta^\\star + \\epsilon)$, with a fixed true coefficient vector $\\beta^\\star$ and a deterministic perturbation $\\epsilon_i = 0.1\\sin(i)$.\n- **Output Definition:** The \"path of coefficient entry\" is the ordered list of feature indices that become nonzero (defined as $|\\beta_j| > 10^{-8}$) for the first time as $\\lambda$ decreases along its grid. Ties at a given $\\lambda$ are broken by ascending feature index.\n- **Test Cases:** Four specific test cases are defined, varying the $\\lambda$ grid and including a case with induced collinearity.\n\n**1.2. Validation Verdict:**\nThe problem is **valid**. It is a clear, self-contained, and scientifically grounded task. It specifies a standard algorithm (coordinate descent for LASSO) applied to a synthetically generated dataset that mimics a real-world application in bioinformatics. All required parameters, constants, and procedures are defined, making the problem reproducible and verifiable. The one minor ambiguity of \"a fixed random seed\" is resolved by selecting a standard conventional value, which aligns with the intent of reproducibility.\n\n### Step 2: Solution Implementation\n\nThe solution proceeds by first implementing the necessary components: data generation, the coordinate descent algorithm, and the logic to trace the coefficient path. These components are then orchestrated to execute the four specified test cases.\n\n**2.1. Data Generation:**\nA function is defined to generate the synthetic dataset according to the problem's specifications. It uses a fixed random seed for reproducibility. It first generates the raw SNP data (allele counts $0, 1, 2$) from binomial distributions. Then, it standardizes this data matrix $X$ by centering each column to have a mean of $0$ and scaling it to have a unit empirical variance (i.e., $\\frac{1}{n}\\sum_i (x_{ij} - \\bar{x}_j)^2 = 1$). The phenotype $y$ is constructed using the standardized $X$, the true coefficient vector $\\beta^\\star$, and the specified deterministic perturbation $\\epsilon$, after which it is centered to have zero mean.\n\n**2.2. Coordinate Descent for LASSO:**\nThe core of the solution is the coordinate descent solver. For a given regularization parameter $\\lambda$, the algorithm iteratively updates each coefficient $\\beta_j$ until convergence. The implementation employs an efficient Gauss-Seidel style update, where the most recent values of other coefficients are used. To achieve this efficiently, we maintain and update the model's residual vector, $r = y - X\\beta$, iteratively.\n\nThe update for a single coefficient $\\beta_j$ proceeds as follows:\n1.  The argument for the soft-thresholding function, $\\rho_j = \\frac{1}{n}X_{\\cdot j}^\\top (y - \\sum_{k \\ne j} X_{\\cdot k}\\beta_k)$, is calculated efficiently. Recognizing that $y - \\sum_{k \\ne j} X_{\\cdot k}\\beta_k = r + X_{\\cdot j}\\beta_j^{\\text{old}}$, the argument becomes $\\rho_j = \\frac{1}{n}X_{\\cdot j}^\\top r + \\beta_j^{\\text{old}}$ because $\\frac{1}{n}X_{\\cdot j}^\\top X_{\\cdot j} = 1$ for standardized features. Here, $r$ is the residual calculated using the most up-to-date coefficient vector.\n2.  The coefficient is updated: $\\beta_j^{\\text{new}} \\leftarrow S(\\rho_j, \\lambda)$.\n3.  The change in the coefficient, $\\Delta\\beta_j = \\beta_j^{\\text{new}} - \\beta_j^{\\text{old}}$, is used to update the global residual vector efficiently: $r \\leftarrow r - X_{\\cdot j}\\Delta\\beta_j$. This avoids repeated expensive matrix-vector multiplications.\n\nThis cyclic process over all features constitutes one sweep. The algorithm terminates when the maximum change in any coefficient over a full sweep falls below the tolerance $10^{-6}$ or when $1000$ sweeps are completed.\n\n**2.3. Coefficient Path Calculation:**\nTo determine the entry path, we solve the LASSO problem for each $\\lambda$ in a decreasing grid. The solution for a larger $\\lambda$ serves as a \"warm start\" for the next smaller $\\lambda$, which accelerates convergence.\n\nAfter convergence at each $\\lambda$, we identify the set of features with nonzero coefficients (where $|\\beta_j| > 10^{-8}$). By comparing this set to the set of nonzero features from the previous (larger) $\\lambda$, we can identify any newly entered features. These new features, sorted by their index to handle ties, are appended to the overall entry path list.\n\n**2.4. Test Case Execution:**\nThe main program executes the four specified test cases:\n1.  **Happy Path:** A standard geometric grid of $50$ $\\lambda$ values is created for the base dataset ($p=8$).\n2.  **Boundary Case:** The grid consists of a single value, $\\lambda_{\\max}$. As theory predicts, no coefficients should become nonzero.\n3.  **Extreme Small $\\lambda$:** A longer grid of $120$ values reaching a much smaller minimum $\\lambda$ is used, allowing more coefficients to enter the model.\n4.  **Collinearity Case:** The design matrix is augmented by duplicating a feature column, creating perfect collinearity. The augmented matrix is re-standardized, a new $\\lambda_{\\max}$ is computed, and the path is traced on a new grid. This tests the algorithm's behavior and the tie-breaking rule under a known challenging condition.\n\nThe results from these four cases are collected and formatted into the precise string representation required by the problem statement.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the LASSO coefficient entry path for four test cases\n    using coordinate descent on a synthetic SNP dataset.\n    \"\"\"\n\n    # --- Problem Constants and Algorithm Parameters ---\n    N_SAMPLES = 60\n    N_FEATURES_BASE = 8\n    MAF_RATES = np.array([0.05, 0.10, 0.20, 0.15, 0.30, 0.25, 0.40, 0.35])\n    BETA_STAR = np.array([0.0, 1.2, 0.0, -0.8, 0.0, 0.0, 0.5, 0.0])\n    EPSILON = 0.1 * np.sin(np.arange(N_SAMPLES))\n    RANDOM_SEED = 0\n\n    CONV_TOL = 1e-6\n    NON_ZERO_TOL = 1e-8\n    MAX_SWEEPS = 1000\n\n    # --- Helper Functions ---\n\n    def soft_threshold(a, t):\n        \"\"\"Soft-thresholding operator S(a, t).\"\"\"\n        return np.sign(a) * np.maximum(np.abs(a) - t, 0)\n\n    def generate_base_data():\n        \"\"\"\n        Generates and standardizes the base dataset (X, y) for p=8\n        and also returns the raw, unstandardized X matrix.\n        \"\"\"\n        rng = np.random.default_rng(RANDOM_SEED)\n        X_raw = np.zeros((N_SAMPLES, N_FEATURES_BASE))\n        for j in range(N_FEATURES_BASE):\n            X_raw[:, j] = rng.binomial(2, MAF_RATES[j], size=N_SAMPLES)\n\n        # Standardize X: zero mean, unit variance (with 1/n normalization)\n        mean_X = np.mean(X_raw, axis=0)\n        std_X = np.std(X_raw, axis=0)\n        std_X[std_X == 0] = 1.0  # Avoid division by zero\n        X = (X_raw - mean_X) / std_X\n\n        # Generate y and center it\n        y_raw = X @ BETA_STAR + EPSILON\n        y = y_raw - np.mean(y_raw)\n\n        return X, y, X_raw\n\n    def coordinate_descent_lasso(X, y, lambda_val, beta_init):\n        \"\"\"\n        Solves the LASSO objective for a single lambda value.\n        Uses an efficient coordinate descent with Gauss-Seidel updates.\n        \"\"\"\n        n, p = X.shape\n        beta = np.copy(beta_init)\n        \n        for _ in range(MAX_SWEEPS):\n            beta_old_sweep = np.copy(beta)\n            for j in range(p):\n                # Calculate rho_j = (1/n) * X_j^T * (y - sum_{k!=j} X_k * beta_k)\n                # This uses the most recent beta values (Gauss-Seidel style).\n                # The calculation is done efficiently.\n                r_partial = y - (X @ beta - X[:, j] * beta[j])\n                rho_j = (X[:, j] @ r_partial) / n\n                beta[j] = soft_threshold(rho_j, lambda_val)\n            \n            if np.max(np.abs(beta - beta_old_sweep)) < CONV_TOL:\n                break\n        return beta\n\n    def solve_lasso_path(X, y, lambda_grid):\n        \"\"\"\n        Computes the coefficient entry path along a decreasing lambda grid.\n        \"\"\"\n        p = X.shape[1]\n        beta = np.zeros(p)\n        entry_path = []\n        entered_indices = set()\n\n        for lambda_val in lambda_grid:\n            # Use warm starts: previous solution initializes the next run\n            beta = coordinate_descent_lasso(X, y, lambda_val, beta_init=beta)\n\n            # Check for newly entered features\n            current_nonzero_indices = set(np.where(np.abs(beta) > NON_ZERO_TOL)[0])\n            newly_entered = sorted(list(current_nonzero_indices - entered_indices))\n\n            if newly_entered:\n                entry_path.extend(newly_entered)\n                entered_indices.update(newly_entered)\n        \n        return entry_path\n\n    # --- Main Execution Logic for All Test Cases ---\n    \n    results = []\n    \n    # Generate the base dataset once\n    X_base, y, X_raw_base = generate_base_data()\n    n_base, _ = X_base.shape\n\n    # -- Test Case 1: Happy Path --\n    lambda_max_1 = np.max(np.abs(X_base.T @ y / n_base))\n    lambda_grid_1 = np.geomspace(lambda_max_1, 0.01 * lambda_max_1, 50)\n    path_1 = solve_lasso_path(X_base, y, lambda_grid_1)\n    results.append(path_1)\n\n    # -- Test Case 2: Boundary Case --\n    lambda_grid_2 = np.array([lambda_max_1])\n    path_2 = solve_lasso_path(X_base, y, lambda_grid_2)\n    results.append(path_2)\n\n    # -- Test Case 3: Extreme Small Lambda --\n    lambda_grid_3 = np.geomspace(lambda_max_1, 1e-6 * lambda_max_1, 120)\n    path_3 = solve_lasso_path(X_base, y, lambda_grid_3)\n    results.append(path_3)\n    \n    # -- Test Case 4: Collinearity Edge Case --\n    X_aug_raw = np.hstack((X_raw_base, X_raw_base[:, 3:4]))\n    \n    # Re-standardize the augmented matrix\n    mean_aug = np.mean(X_aug_raw, axis=0)\n    std_aug = np.std(X_aug_raw, axis=0)\n    std_aug[std_aug == 0] = 1.0\n    X_aug = (X_aug_raw - mean_aug) / std_aug\n    n_aug, _ = X_aug.shape\n    \n    # Recompute lambda_max and grid for the augmented dataset\n    lambda_max_4 = np.max(np.abs(X_aug.T @ y / n_aug))\n    lambda_grid_4 = np.geomspace(lambda_max_4, 0.01 * lambda_max_4, 50)\n    \n    path_4 = solve_lasso_path(X_aug, y, lambda_grid_4)\n    results.append(path_4)\n\n    # --- Format and Print Final Output ---\n    output_str = \"[\" + \",\".join(f\"[{','.join(map(str, r))}]\" for r in results) + \"]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "After exploring the sparse solutions produced by the $\\ell_1$ penalty, we now turn to the distinct behavior of the $\\ell_2$ penalty in ridge regression. This exercise focuses on a key strength of ridge regression: its ability to handle highly correlated predictors, a frequent challenge in high-dimensional biological data. You will implement a numerically stable solver using Singular Value Decomposition (SVD) and analyze how ridge regression distributes coefficient weights among collinear features, while also learning to quantify model complexity using the concept of effective degrees of freedom. ",
            "id": "4605283",
            "problem": "You are given a linear modeling task motivated by gene expression analysis but framed purely as a numerical linear algebra problem. Consider a linear model with a design matrix $X \\in \\mathbb{R}^{n \\times p}$ and a response vector $y \\in \\mathbb{R}^{n}$. The ridge estimator is defined as the minimizer of the penalized least squares objective that adds an $\\ell_2$ penalty on the coefficients. You must analyze how the ridge estimator redistributes coefficients among collinear predictors and quantify its effective degrees of freedom by the trace of its linear smoother.\n\nYour program must implement the following steps for each test case:\n\n- Preprocessing:\n  - Standardize each column of $X$ to have zero mean and unit variance.\n  - Center $y$ to have zero mean.\n  - Do not add an intercept and do not standardize $y$ beyond centering.\n\n- Ridge estimator:\n  - Compute the ridge coefficient vector $\\hat{\\beta}_{\\lambda} \\in \\mathbb{R}^{p}$ for a specified penalty parameter $\\lambda \\in \\mathbb{R}_{\\ge 0}$ using a numerically stable method that does not explicitly invert matrices. You must use a singular value decomposition-based computation that remains valid when $X$ is rank-deficient.\n\n- Redistribution metric:\n  - For the first two predictors (the first two columns of $X$ after preprocessing), compute the ratio $r = \\hat{\\beta}_{\\lambda,1} / \\hat{\\beta}_{\\lambda,2}$ to quantify how coefficients are redistributed across collinear predictors.\n\n- Effective degrees of freedom:\n  - Compute the effective degrees of freedom as the trace of the linear operator that maps $y$ to the fitted values produced by the ridge estimator. This value must be computed in a numerically stable way that leverages singular value decomposition, without forming or inverting ill-conditioned matrices explicitly.\n\n- Rounding and output:\n  - Round both $r$ and the effective degrees of freedom to $6$ decimal places for reporting.\n\nFundamental base and constraints:\n- You must start from the definition of the ridge estimator as the minimizer of the penalized least squares objective and from the definition of the effective degrees of freedom as the trace of the linear operator that maps $y$ to its fitted values under ridge. You must derive any computational formulas you use from these foundations. Do not assume access to shortcut formulas not derived from these foundations in your implementation.\n- Your algorithm must be robust to multicollinearity, including exact collinearity.\n\nTest suite:\nImplement the above for the following three test cases. For clarity, vectors are column vectors.\n\n- Case A (high correlation, moderate regularization):\n  - Let $x_1 = [-3,-2,-1,0,1,2,3,4,5,6]^{\\top}$.\n  - Let $\\delta = [0.1, -0.2, 0.05, -0.05, 0.02, -0.01, 0.08, -0.03, 0.04, -0.02]^{\\top}$.\n  - Let $x_2 = 0.99\\,x_1 + \\delta$.\n  - Let $x_3 = [2,-1,3,-2,0,1,-3,2,-2,4]^{\\top}$.\n  - Form $X = [x_1, x_2, x_3]$ and $y = 1.5\\,x_1 + 1.5\\,x_2 + 0.2\\,x_3$.\n  - Use $\\lambda = 1.0$.\n\n- Case B (exact collinearity, near-unpenalized limit):\n  - Let $x_1' = [-4,-3,-2,-1,0,1,2,3]^{\\top}$.\n  - Let $x_2' = x_1'$.\n  - Let $x_3' = [1,-2,3,-4,4,-3,2,-1]^{\\top}$.\n  - Form $X' = [x_1', x_2', x_3']$ and $y' = 3\\,x_1' + 0.5\\,x_3'$.\n  - Use $\\lambda = 1\\times 10^{-8}$.\n\n- Case C (high correlation, heavy regularization):\n  - Use the same $X$ and $y$ from Case A.\n  - Use $\\lambda = 1\\times 10^{6}$.\n\nFinal output format:\n- Your program should produce a single line of output containing a list of results for the three cases in order A, B, C.\n- Each case result is a list $[r, \\mathrm{df}]$, where $r$ is the ratio and $\\mathrm{df}$ is the effective degrees of freedom, both rounded to $6$ decimal places.\n- The final output must be exactly a single line in the format: [[r_A,df_A],[r_B,df_B],[r_C,df_C]]\n- No additional text should be printed.",
            "solution": "The problem as stated is a well-posed numerical linear algebra exercise grounded in the principles of regularized linear models. All data, parameters, and procedures are explicitly defined, and the task is scientifically and mathematically sound. No inconsistencies, ambiguities, or factual errors are present. The problem is therefore deemed valid.\n\nThe core of the problem is to compute the ridge regression coefficient vector $\\hat{\\beta}_{\\lambda}$ and the effective degrees of freedom $\\mathrm{df}(\\lambda)$ using a numerically stable method based on Singular Value Decomposition (SVD).\n\n### Preprocessing\n\nLet the design matrix be $X \\in \\mathbb{R}^{n \\times p}$ and the response vector be $y \\in \\mathbb{R}^{n}$.\nFirst, each column $x_j$ of the matrix $X$ is standardized to have a mean of $0$ and a standard deviation of $1$. Let $\\mu_j = \\frac{1}{n} \\sum_{i=1}^{n} X_{ij}$ and $\\sigma_j = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (X_{ij} - \\mu_j)^2}$ be the mean and population standard deviation of the $j$-th column. The standardized matrix, which we will also denote as $X$ for simplicity in the following derivations, has columns $x'_j = (x_j - \\mu_j) / \\sigma_j$. If $\\sigma_j=0$, the standardized column is a vector of zeros.\nThe response vector $y$ is centered to have a mean of $0$. Let $\\mu_y = \\frac{1}{n} \\sum_{i=1}^{n} y_i$. The centered vector, also denoted as $y$, is $y' = y - \\mu_y$.\n\n### Ridge Regression Estimator\n\nThe ridge regression estimator $\\hat{\\beta}_{\\lambda}$ is the vector $\\beta \\in \\mathbb{R}^{p}$ that minimizes the penalized least squares objective function:\n$$\nL(\\beta) = \\|y - X\\beta\\|_2^2 + \\lambda \\|\\beta\\|_2^2\n$$\nwhere $\\lambda \\ge 0$ is the regularization parameter. To find the minimizer, we compute the gradient of $L(\\beta)$ with respect to $\\beta$ and set it to zero:\n$$\n\\nabla_{\\beta} L(\\beta) = \\nabla_{\\beta} \\left( (y - X\\beta)^T(y - X\\beta) + \\lambda\\beta^T\\beta \\right)\n$$\n$$\n\\nabla_{\\beta} L(\\beta) = \\nabla_{\\beta} \\left( y^Ty - 2y^TX\\beta + \\beta^TX^TX\\beta + \\lambda\\beta^T\\beta \\right)\n$$\n$$\n\\nabla_{\\beta} L(\\beta) = -2X^Ty + 2X^TX\\beta + 2\\lambda\\beta\n$$\nSetting the gradient to zero gives the normal equations for ridge regression:\n$$\n(X^TX + \\lambda I)\\hat{\\beta}_{\\lambda} = X^Ty\n$$\nThis leads to the formal solution $\\hat{\\beta}_{\\lambda} = (X^TX + \\lambda I)^{-1}X^Ty$. However, directly inverting the matrix $(X^TX + \\lambda I)$ can be numerically unstable, especially if $X$ is ill-conditioned (i.e., has highly correlated columns).\n\nA more stable approach utilizes the Singular Value Decomposition (SVD) of $X$. Let the SVD of the $n \\times p$ matrix $X$ be:\n$$\nX = U D V^T\n$$\nwhere $U$ is an $n \\times p$ matrix with orthonormal columns ($U^TU = I_p$), $D$ is a $p \\times p$ diagonal matrix with the singular values $d_1, d_2, \\ldots, d_p$ on its diagonal, and $V$ is a $p \\times p$ orthogonal matrix ($V^TV = VV^T = I_p$). Note that we are using the economy-size SVD as $n \\ge p$ in the test cases.\n\nSubstitute the SVD into the normal equations:\n$$\n\\hat{\\beta}_{\\lambda} = ( (VD^TU^T)(UDV^T) + \\lambda I )^{-1} (VD^TU^T)y\n$$\n$$\n\\hat{\\beta}_{\\lambda} = ( V D^T D V^T + \\lambda V I V^T )^{-1} V D^T U^T y\n$$\n$$\n\\hat{\\beta}_{\\lambda} = ( V(D^2 + \\lambda I)V^T )^{-1} V D^T U^T y\n$$\nUsing the property $(ABC)^{-1} = C^{-1}B^{-1}A^{-1}$:\n$$\n\\hat{\\beta}_{\\lambda} = (V^T)^{-1} (D^2 + \\lambda I)^{-1} V^{-1} V D^T U^T y\n$$\nSince $V$ is orthogonal, $(V^T)^{-1} = V$ and $V^{-1}=V^T$.\n$$\n\\hat{\\beta}_{\\lambda} = V (D^2 + \\lambda I)^{-1} D^T U^T y\n$$\nBecause $D$ (and thus $D^T$) is diagonal, the matrix $(D^2 + \\lambda I)^{-1}D^T$ is also diagonal. Its $j$-th diagonal element is $d_j / (d_j^2 + \\lambda)$. This formulation is numerically stable because it avoids matrix inversion and the term $d_j^2+\\lambda$ is strictly positive for $\\lambda>0$. The computation involves stable operations: SVD, matrix-vector products, and element-wise scaling.\n\nThe coefficient ratio is then computed as $r = \\hat{\\beta}_{\\lambda,1} / \\hat{\\beta}_{\\lambda,2}$.\n\n### Effective Degrees of Freedom\n\nThe fitted values from the ridge regression are given by $\\hat{y} = X\\hat{\\beta}_{\\lambda}$. We can express $\\hat{y}$ as a linear transformation of $y$:\n$$\n\\hat{y} = X \\left( (X^TX + \\lambda I)^{-1}X^T y \\right) = S_{\\lambda} y\n$$\nThe matrix $S_{\\lambda} = X(X^TX + \\lambda I)^{-1}X^T$ is known as the smoother matrix or hat matrix for ridge regression. The effective degrees of freedom, $\\mathrm{df}(\\lambda)$, is defined as the trace of this matrix:\n$$\n\\mathrm{df}(\\lambda) = \\mathrm{tr}(S_{\\lambda}) = \\mathrm{tr}(X(X^TX + \\lambda I)^{-1}X^T)\n$$\nUsing the cyclic property of the trace, $\\mathrm{tr}(ABC) = \\mathrm{tr}(CAB)$, we can write:\n$$\n\\mathrm{df}(\\lambda) = \\mathrm{tr}((X^TX + \\lambda I)^{-1}X^TX)\n$$\nNow, substitute the SVD of $X$ into this expression:\n$$\nX^TX = (UDV^T)^T(UDV^T) = VD^TU^TUDV^T = VD^2V^T\n$$\n$$\n(X^TX + \\lambda I)^{-1} = (VD^2V^T + \\lambda VIV^T)^{-1} = (V(D^2 + \\lambda I)V^T)^{-1} = V(D^2 + \\lambda I)^{-1}V^T\n$$\nThus,\n$$\n\\mathrm{df}(\\lambda) = \\mathrm{tr} \\left( V(D^2 + \\lambda I)^{-1}V^T \\cdot VD^2V^T \\right)\n$$\n$$\n\\mathrm{df}(\\lambda) = \\mathrm{tr} \\left( V(D^2 + \\lambda I)^{-1}D^2V^T \\right)\n$$\nAgain, using the cyclic property of a trace, $\\mathrm{tr}(V A V^T) = \\mathrm{tr}(A V^T V) = \\mathrm{tr}(A)$:\n$$\n\\mathrm{df}(\\lambda) = \\mathrm{tr} \\left( (D^2 + \\lambda I)^{-1}D^2 \\right)\n$$\nSince $(D^2 + \\lambda I)^{-1}D^2$ is a diagonal matrix with diagonal elements $d_j^2 / (d_j^2 + \\lambda)$, its trace is the sum of these elements:\n$$\n\\mathrm{df}(\\lambda) = \\sum_{j=1}^{p} \\frac{d_j^2}{d_j^2 + \\lambda}\n$$\nThis formula provides a direct, efficient, and numerically stable way to compute the effective degrees of freedom from the singular values of $X$.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the specified ridge regression analysis for three test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    x1_a = np.array([-3, -2, -1, 0, 1, 2, 3, 4, 5, 6])\n    delta_a = np.array([0.1, -0.2, 0.05, -0.05, 0.02, -0.01, 0.08, -0.03, 0.04, -0.02])\n    x2_a = 0.99 * x1_a + delta_a\n    x3_a = np.array([2, -1, 3, -2, 0, 1, -3, 2, -2, 4])\n    X_a = np.vstack([x1_a, x2_a, x3_a]).T\n    y_a = 1.5 * x1_a + 1.5 * x2_a + 0.2 * x3_a\n    \n    x1_b = np.array([-4, -3, -2, -1, 0, 1, 2, 3])\n    x2_b = x1_b.copy()\n    x3_b = np.array([1, -2, 3, -4, 4, -3, 2, -1])\n    X_b = np.vstack([x1_b, x2_b, x3_b]).T\n    y_b = 3 * x1_b + 0.5 * x3_b\n\n    test_cases = [\n        {\"X\": X_a, \"y\": y_a, \"lambda\": 1.0},\n        {\"X\": X_b, \"y\": y_b, \"lambda\": 1e-8},\n        {\"X\": X_a, \"y\": y_a, \"lambda\": 1e6},\n    ]\n\n    results = []\n\n    for case in test_cases:\n        X, y, lambda_val = case[\"X\"], case[\"y\"], case[\"lambda\"]\n        n, p = X.shape\n\n        # Preprocessing:\n        # 1. Center y\n        y_centered = y - np.mean(y)\n        \n        # 2. Standardize X\n        X_mean = np.mean(X, axis=0)\n        # Use ddof=0 for population standard deviation, as is default in numpy.\n        X_std = np.std(X, axis=0)\n        \n        # Standardize X, handling columns with zero standard deviation\n        X_scaled = np.zeros_like(X, dtype=float)\n        non_zero_std = X_std > 1e-12 # A small tolerance for floating point\n        X_scaled[:, non_zero_std] = (X[:, non_zero_std] - X_mean[non_zero_std]) / X_std[non_zero_std]\n\n        # Singular Value Decomposition of the standardized matrix\n        # Use economy SVD since n > p\n        U, s, Vt = np.linalg.svd(X_scaled, full_matrices=False)\n        V = Vt.T\n\n        # Compute the ridge coefficient vector beta_hat\n        # beta_hat = V @ diag(s / (s^2 + lambda)) @ U.T @ y_centered\n        # This is implemented efficiently without forming diagonal matrices\n        tmp = U.T @ y_centered\n        d_term = s / (s**2 + lambda_val)\n        beta_hat = V @ (d_term * tmp)\n\n        # Compute redistribution metric r\n        # The problem statement guarantees this won't be a division by zero.\n        if abs(beta_hat[1]) < 1e-12:\n            # Handle potential division by very small number, though not expected for these cases.\n            # If beta_hat[1] is effectively zero, the ratio is ill-defined.\n            # Given the problem's nature, we assume it's non-zero.\n            r = np.inf if beta_hat[0] > 0 else -np.inf if beta_hat[0] < 0 else np.nan\n        else:\n            r = beta_hat[0] / beta_hat[1]\n\n        # Compute effective degrees of freedom df\n        # df = sum(s_j^2 / (s_j^2 + lambda))\n        s_squared = s**2\n        df = np.sum(s_squared / (s_squared + lambda_val))\n\n        # Rounding and appending results\n        r_rounded = round(r, 6)\n        df_rounded = round(df, 6)\n        results.append([r_rounded, df_rounded])\n\n    # Format the final output string exactly as required\n    formatted_results = \",\".join([f\"[{res[0]},{res[1]}]\" for res in results])\n    print(f\"[{formatted_results}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Having implemented solvers, a crucial question for any practitioner is: 'How can I be sure my solution is optimal?' This final practice elevates your understanding from algorithmic implementation to rigorous verification by exploring the convex duality of the LASSO problem. You will derive the LASSO dual, implement a primal solver, and then construct a corresponding dual-feasible solution to compute the duality gap. This powerful technique provides a certificate of optimality, bridging the gap between practical computation and the deep theory of convex optimization. ",
            "id": "4605237",
            "problem": "You are modeling gene expression effects on a clinical response using the Least Absolute Shrinkage and Selection Operator (LASSO) to control model complexity via sparsity. Consider the primal LASSO problem: minimize over a coefficient vector $w \\in \\mathbb{R}^p$ the objective $0.5 \\lVert y - X w \\rVert_2^2 + \\lambda \\lVert w \\rVert_1$, where $X \\in \\mathbb{R}^{n \\times p}$ is a fixed design matrix of standardized features (e.g., gene expression levels), $y \\in \\mathbb{R}^n$ is the response vector (e.g., a clinical phenotype), and $\\lambda \\in \\mathbb{R}_{>0}$ is the regularization strength. Your task is to use convex duality to derive the dual problem, interpret the active dual constraints in terms of correlations between residuals and features, and then compute a duality gap to certify optimality.\n\nDerivation requirements:\n- Start from the fundamental base of convex analysis: the definition of the Fenchel conjugate and the Lagrangian dual for convex programs. Specifically, use only well-tested facts: convexity of the squared Euclidean norm, properties of the $\\ell_1$ norm, and the Fenchel conjugates of these functions. Do not assume any pre-derived LASSO dual form.\n- Derive the dual problem by introducing an equality constraint that expresses the residuals, forming the Lagrangian, and evaluating the infimum over the primal variables using the conjugates. Identify the dual feasible set and the dual objective explicitly.\n- Using the Karush–Kuhn–Tucker (KKT) conditions, interpret the dual feasibility constraints in terms of the correlations between residuals and features. Conclude how active constraints relate to nonzero coefficients in the primal.\n- Provide the explicit expression of the duality gap for any primal-dual pair and explain why it is nonnegative and equals zero at optimality.\n\nImplementation requirements:\n- Implement a solver for the primal problem using cyclic coordinate descent with soft-thresholding. For each coordinate update, use the fundamental step of minimizing a one-dimensional quadratic plus an $\\ell_1$ penalty. Use a stopping criterion based on the maximum absolute change in coordinates less than a tolerance of $10^{-10}$ or a maximum of $50000$ iterations, whichever comes first.\n- Construct a dual-feasible variable $\\theta \\in \\mathbb{R}^n$ from the residual $r = y - X w$ by scaling: set $\\theta = \\alpha r$ where $\\alpha = \\min\\{1, \\lambda / \\lVert X^\\top r \\rVert_\\infty\\}$ with the convention that if $\\lVert X^\\top r \\rVert_\\infty = 0$ then $\\alpha = 1$. This ensures dual feasibility.\n- Compute the primal objective $P(w) = 0.5 \\lVert y - X w \\rVert_2^2 + \\lambda \\lVert w \\rVert_1$, the dual objective $D(\\theta) = -0.5 \\lVert \\theta \\rVert_2^2 + y^\\top \\theta$, and the duality gap $G = P(w) - D(\\theta)$.\n- Identify the count of active dual constraints as the number of indices $j \\in \\{1,\\dots,p\\}$ for which $\\lvert X_{\\cdot j}^\\top \\theta \\rvert$ is within an absolute tolerance of $10^{-7}$ of $\\lambda$, that is, $\\lvert \\lvert X_{\\cdot j}^\\top \\theta \\rvert - \\lambda \\rvert \\le 10^{-7}$.\n\nTest suite:\nFor each of the following test cases, run your solver, construct the dual variable, compute the duality gap, and count the active constraints. For each case, the program should output a two-element list containing the duality gap and the active-constraint count. The duality gap must be rounded to $10$ decimal places. The final output must be a single line containing the list of all case results formatted as in the example below.\n\n- Test case $1$ (happy path, moderate regularization): $X \\in \\mathbb{R}^{5 \\times 3}$,\n  $$\n  X = \\begin{bmatrix}\n  1 & 0.5 & 0 \\\\\n  0 & 1.0 & 0.5 \\\\\n  1 & 0 & 1.0 \\\\\n  2 & 1.0 & 0 \\\\\n  0 & 0.5 & 1.0\n  \\end{bmatrix}, \\quad\n  y = \\begin{bmatrix} 2.0 \\\\ 0.0 \\\\ 1.0 \\\\ 3.0 \\\\ 1.0 \\end{bmatrix}, \\quad\n  \\lambda = 0.5.\n  $$\n- Test case $2$ (very large regularization; all-zero solution optimal): same $X$ and $y$ as in test case $1$, with $\\lambda = 10.0$.\n- Test case $3$ (boundary case; constraint just active at zero solution): same $X$ and $y$ as in test case $1$, with $\\lambda = 9.0$.\n- Test case $4$ (underdetermined system $p > n$): $X \\in \\mathbb{R}^{4 \\times 6}$,\n  $$\n  X = \\begin{bmatrix}\n  1 & 0 & 0 & 0.2 & 0 & 0.1 \\\\\n  0 & 1 & 0 & 0.1 & 0.2 & 0 \\\\\n  0 & 0 & 1 & 0 & 0.1 & 0.2 \\\\\n  1 & 1 & 1 & 0.1 & 0.1 & 0.1\n  \\end{bmatrix}, \\quad\n  y = \\begin{bmatrix} 0.5 \\\\ 1.2 \\\\ -0.7 \\\\ 0.3 \\end{bmatrix}, \\quad\n  \\lambda = 0.3.\n  $$\n- Test case $5$ (correlated features; multiple nearly active constraints): $X \\in \\mathbb{R}^{6 \\times 4}$,\n  $$\n  X = \\begin{bmatrix}\n  1 & 0.9 & 0 & 0 \\\\\n  0 & 0.1 & 1 & 0.9 \\\\\n  1 & 1.1 & 0 & 0.1 \\\\\n  0 & -0.1 & 1 & 1.1 \\\\\n  0.5 & 0.45 & 0.5 & 0.55 \\\\\n  1 & 0.95 & 0.2 & 0\n  \\end{bmatrix}, \\quad\n  y = \\begin{bmatrix} 1.0 \\\\ -1.0 \\\\ 0.5 \\\\ -0.5 \\\\ 0.2 \\\\ 0.8 \\end{bmatrix}, \\quad\n  \\lambda = 0.4.\n  $$\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case contributes a two-element list $[\\text{gap}, \\text{active\\_count}]$, where $\\text{gap}$ is a float rounded to $10$ decimal places and $\\text{active\\_count}$ is an integer. For example, a valid output for three test cases would look like $[[0.0000000000,1],[0.1234567890,0],[0.0000001000,2]]$.\n\nNo physical units or angle units are involved in this problem. All computations are dimensionless.",
            "solution": "The user-provided problem is a valid and well-posed question in the domain of computational statistics and machine learning, specifically concerning the analysis of the LASSO estimator. It is scientifically grounded, requires no unverifiable assumptions, and contains all necessary information for a complete solution. We will therefore proceed with the derivation and implementation as requested.\n\nThe solution is presented in three parts: the derivation of the LASSO dual problem, the interpretation of the optimality conditions, and the formulation and properties of the duality gap. These theoretical sections are followed by a description of the computational approach used in the final implementation.\n\n### Derivation of the LASSO Dual Problem\n\nThe primal LASSO problem is stated as:\n$$\n\\text{minimize}_{w \\in \\mathbb{R}^p} \\quad P(w) = \\frac{1}{2} \\lVert y - X w \\rVert_2^2 + \\lambda \\lVert w \\rVert_1\n$$\nwhere $X \\in \\mathbb{R}^{n \\times p}$ is the data matrix, $y \\in \\mathbb{R}^n$ is the response vector, $w \\in \\mathbb{R}^p$ is the coefficient vector, and $\\lambda > 0$ is a regularization parameter.\n\nTo derive the dual problem using the specified methodology, we first introduce an auxiliary variable for the residuals, $r \\in \\mathbb{R}^n$, rewriting the problem as an equivalent constrained optimization problem:\n$$\n\\text{minimize}_{w \\in \\mathbb{R}^p, r \\in \\mathbb{R}^n} \\quad \\frac{1}{2} \\lVert r \\rVert_2^2 + \\lambda \\lVert w \\rVert_1 \\quad \\text{subject to} \\quad r + Xw = y\n$$\nThe Lagrangian for this problem is formed by introducing a dual variable (Lagrange multiplier) $\\theta \\in \\mathbb{R}^n$ for the equality constraint:\n$$\nL(w, r, \\theta) = \\frac{1}{2} \\lVert r \\rVert_2^2 + \\lambda \\lVert w \\rVert_1 + \\theta^\\top (y - r - Xw)\n$$\nWe can rearrange the terms of the Lagrangian to separate the primal variables $w$ and $r$:\n$$\nL(w, r, \\theta) = \\left( \\lambda \\lVert w \\rVert_1 - \\theta^\\top Xw \\right) + \\left( \\frac{1}{2} \\lVert r \\rVert_2^2 - \\theta^\\top r \\right) + y^\\top \\theta\n$$\nThe Lagrange dual function, $g(\\theta)$, is the infimum of the Lagrangian over the primal variables:\n$$\ng(\\theta) = \\inf_{w \\in \\mathbb{R}^p, r \\in \\mathbb{R}^n} L(w, r, \\theta) = \\inf_{w} \\left( \\lambda \\lVert w \\rVert_1 - (X^\\top\\theta)^\\top w \\right) + \\inf_{r} \\left( \\frac{1}{2} \\lVert r \\rVert_2^2 - \\theta^\\top r \\right) + y^\\top \\theta\n$$\nThese two infima can be evaluated using the Fenchel conjugate. The Fenchel conjugate of a function $f(x)$ is $f^*(z) = \\sup_x (z^\\top x - f(x))$, which implies $\\inf_x (f(x) - z^\\top x) = -f^*(z)$.\n\nFor the term involving $w$, let $f_1(w) = \\lambda \\lVert w \\rVert_1$. Its conjugate is the indicator function of the dual-norm ball:\n$$\nf_1^*(z) = (\\lambda \\lVert \\cdot \\rVert_1)^*(z) =\n\\begin{cases}\n0 & \\text{if } \\lVert z \\rVert_\\infty \\le \\lambda \\\\\n\\infty & \\text{otherwise}\n\\end{cases}\n$$\nThus, the infimum over $w$ is:\n$$\n\\inf_{w} \\left( \\lambda \\lVert w \\rVert_1 - (X^\\top\\theta)^\\top w \\right) = -f_1^*(X^\\top\\theta) =\n\\begin{cases}\n0 & \\text{if } \\lVert X^\\top\\theta \\rVert_\\infty \\le \\lambda \\\\\n-\\infty & \\text{otherwise}\n\\end{cases}\n$$\nFor the term involving $r$, let $f_2(r) = \\frac{1}{2} \\lVert r \\rVert_2^2$. This function is its own Fenchel conjugate, $f_2^*(\\theta) = \\frac{1}{2} \\lVert \\theta \\rVert_2^2$.\nThus, the infimum over $r$ is:\n$$\n\\inf_{r} \\left( \\frac{1}{2} \\lVert r \\rVert_2^2 - \\theta^\\top r \\right) = -f_2^*(\\theta) = -\\frac{1}{2} \\lVert \\theta \\rVert_2^2\n$$\nSubstituting these results back into the expression for $g(\\theta)$, we find that the dual function is finite only when $\\lVert X^\\top\\theta \\rVert_\\infty \\le \\lambda$. The dual problem is to maximize $g(\\theta)$:\n$$\n\\text{maximize}_{\\theta \\in \\mathbb{R}^n} \\quad D(\\theta) = -\\frac{1}{2} \\lVert \\theta \\rVert_2^2 + y^\\top \\theta \\quad \\text{subject to} \\quad \\lVert X^\\top\\theta \\rVert_\\infty \\le \\lambda\n$$\nThe feasible set for the dual variable $\\theta$ is defined by the constraint $\\lVert X^\\top\\theta \\rVert_\\infty \\le \\lambda$, which is equivalent to the set of constraints $\\lvert X_{\\cdot j}^\\top \\theta \\rvert \\le \\lambda$ for all features $j=1, \\dots, p$.\n\n### Interpretation via Karush–Kuhn–Tucker (KKT) Conditions\n\nThe Karush–Kuhn–Tucker (KKT) conditions provide necessary conditions for optimality. For a primal-dual optimal pair $(w^*, r^*, \\theta^*)$, they are:\n$1$. **Primal feasibility**: $r^* + Xw^* = y$.\n$2$. **Dual feasibility**: $\\lVert X^\\top \\theta^* \\rVert_\\infty \\le \\lambda$.\n$3$. **Stationarity**: The gradient of the Lagrangian with respect to the primal variables must vanish at the optimum.\n    - $\\nabla_r L = r^* - \\theta^* = 0 \\implies r^* = \\theta^*$. At optimality, the dual variable equals the primal residual vector.\n    - $\\nabla_w L = 0 \\implies 0 \\in \\partial (\\lambda \\lVert w \\rVert_1)|_{w^*} - X^\\top \\theta^*$, which means $X^\\top \\theta^*$ must be a subgradient of $\\lambda \\lVert w \\rVert_1$ at $w^*$.\n\nThe subgradient of $\\lambda \\lVert w \\rVert_1$ at a vector $w$ is a vector $v$ with components:\n$$\nv_j =\n\\begin{cases}\n\\lambda \\cdot \\text{sign}(w_j) & \\text{if } w_j \\neq 0 \\\\\n\\text{any value in } [-\\lambda, \\lambda] & \\text{if } w_j = 0\n\\end{cases}\n$$\nCombining stationarity ($r^*=\\theta^*$) with the subgradient definition provides a profound interpretation. For each feature $j$:\n- If $w_j^* \\neq 0$ (feature $j$ is selected by the model), then $(X^\\top \\theta^*)_j = \\lambda \\cdot \\text{sign}(w_j^*)$. This implies $\\lvert (X^\\top r^*)_j \\rvert = \\lambda$. The magnitude of the correlation between feature $j$ and the optimal residual $r^*$ is exactly equal to the regularization strength $\\lambda$. The corresponding dual constraint is **active**.\n- If $w_j^* = 0$ (feature $j$ is not selected), then $\\lvert (X^\\top \\theta^*)_j \\rvert \\le \\lambda$. The magnitude of the correlation between feature $j$ and the optimal residual is less than or equal to $\\lambda$. The corresponding dual constraint is **inactive** or active.\n\n### Duality Gap and Optimality Certification\nFor any primal-feasible solution $w$ and any dual-feasible solution $\\theta$, the primal objective $P(w)$ and dual objective $D(\\theta)$ are defined as:\n$$\nP(w) = \\frac{1}{2} \\lVert y - X w \\rVert_2^2 + \\lambda \\lVert w \\rVert_1\n$$\n$$\nD(\\theta) = -\\frac{1}{2} \\lVert \\theta \\rVert_2^2 + y^\\top \\theta\n$$\nThe **duality gap**, $G(w, \\theta)$, is the difference between the primal and dual objective values:\n$$\nG(w, \\theta) = P(w) - D(\\theta)\n$$\nBy weak duality for convex problems, $P(w) \\ge D(\\theta)$ for any primal-feasible $w$ and dual-feasible $\\theta$. Therefore, the duality gap is always non-negative: $G(w, \\theta) \\ge 0$. Strong duality holds for the LASSO problem (as Slater's condition is satisfied), which means that at an optimal primal-dual pair $(w^*, \\theta^*)$, the duality gap is zero: $G(w^*, \\theta^*) = P(w^*) - D(\\theta^*) = 0$.\nThe duality gap serves as a certificate of sub-optimality. A small, non-negative gap computed from an approximate primal solution $w$ and a corresponding feasible dual solution $\\theta$ indicates that $w$ is close to the true optimal solution.\n\n### Computational Approach\nTo solve the problem numerically, we implement the following steps:\n$1$. The primal LASSO problem is solved for $w$ using a **cyclic coordinate descent** algorithm. The algorithm iteratively updates each coefficient $w_j$ to its optimal value while holding all other coefficients fixed. The update rule for $w_j$ is derived from minimizing a one-dimensional quadratic objective with an $\\ell_1$ penalty, which yields the soft-thresholding operator:\n$$\nw_j \\leftarrow \\frac{S_{\\lambda}(X_{\\cdot j}^\\top (y - \\sum_{k \\neq j} X_{\\cdot k}w_k))}{\\lVert X_{\\cdot j} \\rVert_2^2} \\quad \\text{where} \\quad S_{\\lambda}(z) = \\text{sign}(z) \\max(\\lvert z \\rvert - \\lambda, 0)\n$$\nIteration continues until the maximum absolute change in the coefficients falls below a tolerance of $10^{-10}$ or a maximum of $50000$ iterations is reached.\n$2$. Given the primal solution $w$ from the solver, we compute the residual vector $r = y - Xw$.\n$3$. A dual-feasible variable $\\theta$ is constructed by scaling the residual $r$. Let $c = X^\\top r$. We compute the scaling factor $\\alpha = \\min\\{1, \\lambda / \\lVert c \\rVert_\\infty\\}$ (with $\\alpha=1$ if $\\lVert c \\rVert_\\infty=0$). The dual variable is then $\\theta = \\alpha r$. This construction ensures that $\\lVert X^\\top\\theta \\rVert_\\infty \\le \\lambda$, satisfying dual feasibility.\n$4$. With the primal solution $w$ and the dual-feasible variable $\\theta$, we compute the primal objective $P(w)$, the dual objective $D(\\theta)$, and the duality gap $G(w, \\theta) = P(w) - D(\\theta)$.\n$5$. Finally, we count the number of active dual constraints by finding the number of indices $j$ for which the condition $\\lvert \\lvert (X^\\top \\theta)_j \\rvert - \\lambda \\rvert \\le 10^{-7}$ holds.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the LASSO problem for a series of test cases using coordinate descent,\n    computes the duality gap, and counts active dual constraints.\n    \"\"\"\n\n    def lasso_solver(X, y, lambda_val, tol=1e-10, max_iter=50000):\n        \"\"\"\n        Solves the LASSO primal problem using cyclic coordinate descent.\n        \"\"\"\n        n, p = X.shape\n        w = np.zeros(p)\n        \n        # Precompute squared L2-norms of columns of X, which do not change.\n        d = np.sum(X**2, axis=0)\n        # Handle case where a column is all zeros to avoid division by zero.\n        d[d == 0] = 1.0 \n\n        for i in range(max_iter):\n            w_old = w.copy()\n            for j in range(p):\n                # Calculate rho_j = X_j^T * (y - sum_{k!=j} X_k * w_k)\n                # This can be calculated efficiently as X_j^T * (y - Xw + X_j * w_j)\n                rho_j = X[:, j].T @ (y - X @ w) + d[j] * w[j]\n                \n                # Apply soft-thresholding operator\n                w_j_new = 0.0\n                if rho_j > lambda_val:\n                    w_j_new = (rho_j - lambda_val) / d[j]\n                elif rho_j < -lambda_val:\n                    w_j_new = (rho_j + lambda_val) / d[j]\n                \n                w[j] = w_j_new\n\n            # Check for convergence\n            if np.max(np.abs(w - w_old)) < tol:\n                break\n        \n        return w\n\n    test_cases = [\n        # Test case 1\n        (np.array([[1.0, 0.5, 0.0], [0.0, 1.0, 0.5], [1.0, 0.0, 1.0], [2.0, 1.0, 0.0], [0.0, 0.5, 1.0]]),\n         np.array([2.0, 0.0, 1.0, 3.0, 1.0]),\n         0.5),\n        # Test case 2\n        (np.array([[1.0, 0.5, 0.0], [0.0, 1.0, 0.5], [1.0, 0.0, 1.0], [2.0, 1.0, 0.0], [0.0, 0.5, 1.0]]),\n         np.array([2.0, 0.0, 1.0, 3.0, 1.0]),\n         10.0),\n        # Test case 3\n        (np.array([[1.0, 0.5, 0.0], [0.0, 1.0, 0.5], [1.0, 0.0, 1.0], [2.0, 1.0, 0.0], [0.0, 0.5, 1.0]]),\n         np.array([2.0, 0.0, 1.0, 3.0, 1.0]),\n         9.0),\n        # Test case 4\n        (np.array([[1.0, 0.0, 0.0, 0.2, 0.0, 0.1], [0.0, 1.0, 0.0, 0.1, 0.2, 0.0], [0.0, 0.0, 1.0, 0.0, 0.1, 0.2], [1.0, 1.0, 1.0, 0.1, 0.1, 0.1]]),\n         np.array([0.5, 1.2, -0.7, 0.3]),\n         0.3),\n        # Test case 5\n        (np.array([[1.0, 0.9, 0.0, 0.0], [0.0, 0.1, 1.0, 0.9], [1.0, 1.1, 0.0, 0.1], [0.0, -0.1, 1.0, 1.1], [0.5, 0.45, 0.5, 0.55], [1.0, 0.95, 0.2, 0.0]]),\n         np.array([1.0, -1.0, 0.5, -0.5, 0.2, 0.8]),\n         0.4)\n    ]\n\n    all_results = []\n    \n    for X, y, lambda_val in test_cases:\n        # 1. Solve for the primal variable w\n        w = lasso_solver(X, y, lambda_val)\n        \n        # 2. Construct the dual-feasible variable theta\n        r = y - X @ w\n        c = X.T @ r\n        norm_inf_c = np.max(np.abs(c))\n        \n        alpha = 1.0\n        if norm_inf_c > 1e-12: # Check against small tolerance for floating point\n            alpha = min(1.0, lambda_val / norm_inf_c)\n        \n        theta = alpha * r\n        \n        # 3. Compute primal and dual objectives, and the duality gap\n        primal_obj = 0.5 * np.sum(r**2) + lambda_val * np.sum(np.abs(w))\n        dual_obj = -0.5 * np.sum(theta**2) + y.T @ theta\n        duality_gap = primal_obj - dual_obj\n        \n        # 4. Count active dual constraints\n        dual_corr = X.T @ theta\n        active_tol = 1e-7\n        active_count = np.sum(np.abs(np.abs(dual_corr) - lambda_val) <= active_tol)\n        \n        all_results.append((duality_gap, active_count))\n        \n    # Format the final output string exactly as specified\n    case_strings = []\n    for gap, count in all_results:\n        case_strings.append(f\"[{gap:.10f},{count}]\")\n    final_output = f\"[{','.join(case_strings)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        }
    ]
}