{
    "hands_on_practices": [
        {
            "introduction": "Lasso 回归最引人注目的特性之一是它能够执行自动特征选择，这在处理基因组学等高维数据集时尤其有用。本实践将通过一个具体的编码练习，让您亲手实现坐标下降算法来求解 Lasso 问题。您将观察到，随着正则化参数 $\\lambda$ 的减小，模型的系数是如何从零开始逐一“进入”模型的，这一过程直观地揭示了 $\\ell_1$ 正则化产生稀疏性的内在机制 。",
            "id": "4605278",
            "problem": "给定一个合成的单核苷酸多态性 (SNP) 基因型数据集和一个由稀疏线性模型构建的连续表型。特征被编码为次要等位基因计数 $0$、$1$ 或 $2$，然后进行标准化以使其具有零均值和单位方差，表型也被中心化为零均值。考虑使用坐标下降法求解线性回归的最小绝对收缩和选择算子 (LASSO) 问题。您的任务是实现一个完整的、可运行的程序，该程序沿着一个递减的正则化强度 $\\lambda$ 网格计算 LASSO 解，并报告系数进入路径：即随着 $\\lambda$ 的减小，标准化特征系数变为非零的顺序。您必须严格遵守下面所述的最终输出格式要求。\n\n使用的基本原理：\n- 对于中心化的表型 $y \\in \\mathbb{R}^n$ 和标准化的设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 的线性模型，其中各列经过中心化和缩放，使得对于每个特征索引 $j$，$\\frac{1}{n}\\sum_{i=1}^n x_{ij}^2 = 1$。\n- LASSO 目标函数，它惩罚系数的 $\\ell_1$ 范数：最小化\n$$\n\\frac{1}{2n}\\lVert y - X\\beta \\rVert_2^2 + \\lambda \\lVert \\beta \\rVert_1\n$$\n其中 $\\beta \\in \\mathbb{R}^p$ 是系数，$\\lambda \\ge 0$ 是正则化参数。\n- 从次梯度/Karush–Kuhn–Tucker (KKT) 平稳性推导出的标准化特征的坐标向软阈值更新：\n$$\n\\beta_j \\leftarrow \\frac{1}{\\frac{1}{n}\\lVert X_{\\cdot j} \\rVert_2^2} \\cdot S\\!\\left(\\frac{1}{n}X_{\\cdot j}^\\top \\left(y - \\sum_{k \\ne j} X_{\\cdot k}\\beta_k\\right), \\lambda \\right)\n$$\n其中软阈值算子定义为 $S(a,t) = \\operatorname{sign}(a)\\max(|a| - t, 0)$。在给定的标准化条件下，$\\frac{1}{n}\\lVert X_{\\cdot j} \\rVert_2^2 = 1$，分母简化为 $1$。\n\n数据构建：\n- 设 $n = 60$ 个个体和 $p = 8$ 个 SNP。为保证可复现性，使用固定的随机种子，将每个 SNP 列 $X_{\\cdot j}$ 独立地抽样为 $\\operatorname{Binomial}(2, q_j)$，其中次要等位基因频率 (MAF) 固定为 $(q_1,\\dots,q_8) = (0.05, 0.10, 0.20, 0.15, 0.30, 0.25, 0.40, 0.35)$。生成后，如上所述将每个特征标准化为零均值和单位方差。\n- 使用固定的真实系数 $\\beta^\\star = (0.0, 1.2, 0.0, -0.8, 0.0, 0.0, 0.5, 0.0)$ 和一个确定性扰动 $\\epsilon_i = 0.1\\sin(i)$（对于索引 $i = 0, 1, \\dots, n-1$）构建中心化的表型 $y$，然后将 $y$ 中心化：\n$$\ny \\leftarrow X\\beta^\\star + \\epsilon, \\quad \\bar{y} = \\frac{1}{n}\\sum_{i=1}^n y_i, \\quad y \\leftarrow y - \\bar{y}.\n$$\n\n算法要求：\n- 使用坐标下降法，对 $j = 1, \\dots, p$ 进行循环更新，以在每个 $\\lambda$ 处求解 LASSO 目标函数，直至收敛。在递减的 $\\lambda$ 路径上使用热启动。\n- 使用停止准则 $\\max_j |\\Delta\\beta_j|  10^{-6}$ 或最多 $1000$ 次完整坐标轮扫，以先达到的为准。\n- 令 $\\lambda_{\\max} = \\max_j \\left|\\frac{1}{n} X_{\\cdot j}^\\top y\\right|$，此时解为 $\\beta = 0$。根据下面的测试套件中指定的，从 $\\lambda_{\\max}$ 到更小的值构建递减的 $\\lambda$ 网格。\n\n系数进入路径：\n- 对于每个特征索引 $j \\in \\{0,1,\\dots,p-1\\}$，将其“进入 $\\lambda$”定义为（在递减网格中）收敛的 $\\beta_j$ 首次变为非零的 $\\lambda$ 值（在数值上，将 $|\\beta_j| > 10^{-8}$ 视为非零）。如果一个系数在整个网格上从未变为非零，则它没有进入，不应出现在顺序列表中。在出现平局（即多个特征在同一网格 $\\lambda$ 值进入）的情况下，按索引升序排列特征。系数进入路径是根据 $\\lambda$ 递减时首次进入的顺序排列的特征索引列表。\n\n测试套件：\n使用相同的基础数据集，实现以下四个测试用例，并使用明确的 $\\lambda$ 网格。该设计确保了对正常路径、边界情况、极端情况和共线性边缘情况的覆盖。\n\n- 测试用例 1（正常路径）：使用一个从 $\\lambda_{\\max}$ 到 $0.01\\lambda_{\\max}$ 的包含 $50$ 个值的等比网格，即 $\\lambda_k = \\lambda_{\\max} \\cdot r^{k-1}$，其中公比 $r$ 的选择使得 $\\lambda_{50} = 0.01\\lambda_{\\max}$。报告沿此路径首次变为非零的特征索引的有序列表。\n- 测试用例 2（边界情况）：仅使用单点网格 $\\{\\lambda_{\\max}\\}$。报告特征索引的有序列表（预期为空）。\n- 测试用例 3（极端小 $\\lambda$）：使用一个从 $\\lambda_{\\max}$ 到 $10^{-6}\\lambda_{\\max}$ 的包含 $120$ 个值的等比网格。报告沿此路径首次变为非零的特征索引的有序列表。\n- 测试用例 4（共线性边缘情况）：通过复制 SNP 特征索引 $3$ 来创建一个增广数据集，形成第九个特征（导致 $p=9$）。使用相同的协议对增广矩阵进行列式标准化，为增广数据集重新计算 $\\lambda_{\\max}$，并使用一个从这个增广的 $\\lambda_{\\max}$ 到其 $0.01$ 倍的包含 $50$ 个值的等比网格。报告沿此路径首次变为非零的特征索引的有序列表。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含用方括号括起来的、以逗号分隔的结果列表。每个元素本身都应是一个用方括号括起来的、以逗号分隔的整数列表，表示相应测试用例中按首次进入顺序排列的特征索引（从零开始）。例如，一个有四个测试用例的输出可能看起来像“[[1,3,2],[],[1,7,5,3,6],[1,3,8]]”。不应打印任何其他文本。\n\n不涉及物理单位或角度单位。所有数值答案均为纯数字。每个测试用例的结果必须是按规定格式的整数列表。",
            "solution": "用户提供了一个在统计机器学习领域内定义明确的计算问题，特别关注 LASSO 回归模型的实现。任务是使用坐标下降算法为合成的单核苷酸多态性 (SNP) 数据集计算系数进入路径。该问题陈述经验证在科学上是合理的、适定的和完整的。\n\n### 步骤 1：问题验证\n\n**1.1. 提取给定信息：**\n- **模型：** 带有 LASSO 惩罚的线性回归：$\\min_{\\beta} \\frac{1}{2n}\\lVert y - X\\beta \\rVert_2^2 + \\lambda \\lVert \\beta \\rVert_1$。\n- **数据属性：** $y$ 是一个中心化的表型向量，位于 $\\mathbb{R}^n$。$X$ 是一个标准化的设计矩阵，位于 $\\mathbb{R}^{n \\times p}$，其列被中心化为零均值并缩放以使 $\\frac{1}{n}\\sum_{i=1}^n x_{ij}^2 = 1$。\n- **算法：** 带有循环更新的坐标下降法。对于标准化特征 $j$ 的更新规则是 $\\beta_j \\leftarrow S(\\rho_j, \\lambda)$，其中 $\\rho_j = \\frac{1}{n}X_{\\cdot j}^\\top (y - \\sum_{k \\ne j} X_{\\cdot k}\\beta_k)$ 且 $S(a,t) = \\operatorname{sign}(a)\\max(|a| - t, 0)$ 是软阈值算子。\n- **收敛准则：** 系数最大变化量 $\\max_j |\\Delta\\beta_j|  10^{-6}$ 或最多 $1000$ 次轮扫。\n- **正则化路径：** 路径从 $\\lambda_{\\max} = \\max_j |\\frac{1}{n} X_{\\cdot j}^\\top y|$ 开始，并沿着指定的递减网格进行。使用热启动，即一个 $\\lambda$ 的解作为下一个更小 $\\lambda$ 的求解器初始值。\n- **数据集生成：** $n=60$, $p=8$。SNP 特征 $X_{\\cdot j}$ 从 $\\operatorname{Binomial}(2, q_j)$ 分布中抽取，具有指定的次要等位基因频率 (MAF) $(q_j) = (0.05, 0.10, \\dots, 0.35)$，并使用固定的随机种子。表型为 $y = X\\beta^\\star + \\epsilon - \\operatorname{mean}(X\\beta^\\star + \\epsilon)$，具有固定的真实系数向量 $\\beta^\\star$ 和确定性扰动 $\\epsilon_i = 0.1\\sin(i)$。\n- **输出定义：** “系数进入路径”是随着 $\\lambda$ 沿其网格递减，首次变为非零（定义为 $|\\beta_j| > 10^{-8}$）的特征索引的有序列表。在给定的 $\\lambda$ 处出现平局时，按特征索引升序解决。\n- **测试用例：** 定义了四个具体的测试用例，它们改变了 $\\lambda$ 网格，并包括一个带有诱导共线性的情况。\n\n**1.2. 验证结论：**\n该问题是**有效的**。它是一个清晰、独立且具有科学依据的任务。它指定了一个标准算法（用于 LASSO 的坐标下降），并应用于一个模拟生物信息学中真实世界应用的合成生成数据集。所有必需的参数、常量和过程都已定义，使得问题可复现和可验证。“一个固定的随机种子”这一轻微的模糊性通过选择一个标准的常规值来解决，这符合可复现性的意图。\n\n### 步骤 2：解决方案实现\n\n解决方案首先实现必要的组件：数据生成、坐标下降算法以及追踪系数路径的逻辑。然后协调这些组件以执行四个指定的测试用例。\n\n**2.1. 数据生成：**\n定义一个函数，根据问题的规范生成合成数据集。它使用固定的随机种子以保证可复现性。它首先从二项分布中生成原始的 SNP 数据（等位基因计数 $0, 1, 2$）。然后，它通过将每列中心化为均值为 $0$ 并将其缩放为单位经验方差（即 $\\frac{1}{n}\\sum_i (x_{ij} - \\bar{x}_j)^2 = 1$）来标准化此数据矩阵 $X$。表型 $y$ 是使用标准化的 $X$、真实系数向量 $\\beta^\\star$ 和指定的确定性扰动 $\\epsilon$ 构建的，之后再将其中心化为零均值。\n\n**2.2. 用于 LASSO 的坐标下降法：**\n解决方案的核心是坐标下降求解器。对于给定的正则化参数 $\\lambda$，算法迭代更新每个系数 $\\beta_j$ 直至收敛。实现采用高效的 Gauss-Seidel 风格更新，其中使用其他系数的最新值。为了高效实现这一点，我们迭代地维护和更新模型的残差向量 $r = y - X\\beta$。\n\n单个系数 $\\beta_j$ 的更新过程如下：\n1.  软阈值函数的参数 $\\rho_j = \\frac{1}{n}X_{\\cdot j}^\\top (y - \\sum_{k \\ne j} X_{\\cdot k}\\beta_k)$ 被高效计算。注意到 $y - \\sum_{k \\ne j} X_{\\cdot k}\\beta_k = r + X_{\\cdot j}\\beta_j^{\\text{old}}$，参数变为 $\\rho_j = \\frac{1}{n}X_{\\cdot j}^\\top r + \\beta_j^{\\text{old}}$，因为对于标准化特征，$\\frac{1}{n}X_{\\cdot j}^\\top X_{\\cdot j} = 1$。这里，$r$ 是使用最新系数向量计算的残差。\n2.  系数被更新：$\\beta_j^{\\text{new}} \\leftarrow S(\\rho_j, \\lambda)$。\n3.  系数的变化量 $\\Delta\\beta_j = \\beta_j^{\\text{new}} - \\beta_j^{\\text{old}}$ 用于高效地更新全局残差向量：$r \\leftarrow r - X_{\\cdot j}\\Delta\\beta_j$。这避免了重复的昂贵矩阵向量乘法。\n\n这个对所有特征的循环过程构成一次轮扫。当一次完整轮扫中任何系数的最大变化低于容差 $10^{-6}$ 或完成 $1000$ 次轮扫时，算法终止。\n\n**2.3. 系数路径计算：**\n为了确定进入路径，我们为递减网格中的每个 $\\lambda$ 求解 LASSO 问题。较大 $\\lambda$ 的解作为下一个较小 $\\lambda$ 的“热启动”，这加速了收敛。\n\n在每个 $\\lambda$ 处收敛后，我们识别出具有非零系数（其中 $|\\beta_j| > 10^{-8}$）的特征集合。通过将此集合与前一个（较大的）$\\lambda$ 的非零特征集合进行比较，我们可以识别出任何新进入的特征。这些新特征按其索引排序以处理平局，然后附加到总的进入路径列表中。\n\n**2.4. 测试用例执行：**\n主程序执行四个指定的测试用例：\n1.  **正常路径：** 为基础数据集 ($p=8$) 创建一个标准的包含 $50$ 个 $\\lambda$ 值的等比网格。\n2.  **边界情况：** 网格仅包含单个值 $\\lambda_{\\max}$。正如理论预测，任何系数都不应变为非零。\n3.  **极端小 $\\lambda$：** 使用一个更长的包含 $120$ 个值的网格，达到一个更小的最小 $\\lambda$ 值，允许更多系数进入模型。\n4.  **共线性情况：** 通过复制一个特征列来增广设计矩阵，从而创造完美的共线性。对增广矩阵进行重新标准化，计算新的 $\\lambda_{\\max}$，并在新的网格上追踪路径。这测试了算法在已知挑战性条件下的行为以及平局处理规则。\n\n将这四个用例的结果收集起来，并格式化成问题陈述所要求的精确字符串表示。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the LASSO coefficient entry path for four test cases\n    using coordinate descent on a synthetic SNP dataset.\n    \"\"\"\n\n    # --- Problem Constants and Algorithm Parameters ---\n    N_SAMPLES = 60\n    N_FEATURES_BASE = 8\n    MAF_RATES = np.array([0.05, 0.10, 0.20, 0.15, 0.30, 0.25, 0.40, 0.35])\n    BETA_STAR = np.array([0.0, 1.2, 0.0, -0.8, 0.0, 0.0, 0.5, 0.0])\n    EPSILON = 0.1 * np.sin(np.arange(N_SAMPLES))\n    RANDOM_SEED = 0\n\n    CONV_TOL = 1e-6\n    NON_ZERO_TOL = 1e-8\n    MAX_SWEEPS = 1000\n\n    # --- Helper Functions ---\n\n    def soft_threshold(a, t):\n        \"\"\"Soft-thresholding operator S(a, t).\"\"\"\n        return np.sign(a) * np.maximum(np.abs(a) - t, 0)\n\n    def generate_base_data():\n        \"\"\"\n        Generates and standardizes the base dataset (X, y) for p=8\n        and also returns the raw, unstandardized X matrix.\n        \"\"\"\n        rng = np.random.default_rng(RANDOM_SEED)\n        X_raw = np.zeros((N_SAMPLES, N_FEATURES_BASE))\n        for j in range(N_FEATURES_BASE):\n            X_raw[:, j] = rng.binomial(2, MAF_RATES[j], size=N_SAMPLES)\n\n        # Standardize X: zero mean, unit variance (with 1/n normalization)\n        mean_X = np.mean(X_raw, axis=0)\n        std_X = np.std(X_raw, axis=0)\n        std_X[std_X == 0] = 1.0  # Avoid division by zero\n        X = (X_raw - mean_X) / std_X\n\n        # Generate y and center it\n        y_raw = X @ BETA_STAR + EPSILON\n        y = y_raw - np.mean(y_raw)\n\n        return X, y, X_raw\n\n    def coordinate_descent_lasso(X, y, lambda_val, beta_init):\n        \"\"\"\n        Solves the LASSO objective for a single lambda value.\n        Uses an efficient coordinate descent with Gauss-Seidel updates.\n        \"\"\"\n        n, p = X.shape\n        beta = np.copy(beta_init)\n        \n        for _ in range(MAX_SWEEPS):\n            beta_old_sweep = np.copy(beta)\n            for j in range(p):\n                # Calculate rho_j = (1/n) * X_j^T * (y - sum_{k!=j} X_k * beta_k)\n                # This uses the most recent beta values (Gauss-Seidel style).\n                # The calculation is done efficiently.\n                r_partial = y - (X @ beta - X[:, j] * beta[j])\n                rho_j = (X[:, j] @ r_partial) / n\n                beta[j] = soft_threshold(rho_j, lambda_val)\n            \n            if np.max(np.abs(beta - beta_old_sweep))  CONV_TOL:\n                break\n        return beta\n\n    def solve_lasso_path(X, y, lambda_grid):\n        \"\"\"\n        Computes the coefficient entry path along a decreasing lambda grid.\n        \"\"\"\n        p = X.shape[1]\n        beta = np.zeros(p)\n        entry_path = []\n        entered_indices = set()\n\n        for lambda_val in lambda_grid:\n            # Use warm starts: previous solution initializes the next run\n            beta = coordinate_descent_lasso(X, y, lambda_val, beta_init=beta)\n\n            # Check for newly entered features\n            current_nonzero_indices = set(np.where(np.abs(beta) > NON_ZERO_TOL)[0])\n            newly_entered = sorted(list(current_nonzero_indices - entered_indices))\n\n            if newly_entered:\n                entry_path.extend(newly_entered)\n                entered_indices.update(newly_entered)\n        \n        return entry_path\n\n    # --- Main Execution Logic for All Test Cases ---\n    \n    results = []\n    \n    # Generate the base dataset once\n    X_base, y, X_raw_base = generate_base_data()\n    n_base, _ = X_base.shape\n\n    # -- Test Case 1: Happy Path --\n    lambda_max_1 = np.max(np.abs(X_base.T @ y / n_base))\n    lambda_grid_1 = np.geomspace(lambda_max_1, 0.01 * lambda_max_1, 50)\n    path_1 = solve_lasso_path(X_base, y, lambda_grid_1)\n    results.append(path_1)\n\n    # -- Test Case 2: Boundary Case --\n    lambda_grid_2 = np.array([lambda_max_1])\n    path_2 = solve_lasso_path(X_base, y, lambda_grid_2)\n    results.append(path_2)\n\n    # -- Test Case 3: Extreme Small Lambda --\n    lambda_grid_3 = np.geomspace(lambda_max_1, 1e-6 * lambda_max_1, 120)\n    path_3 = solve_lasso_path(X_base, y, lambda_grid_3)\n    results.append(path_3)\n    \n    # -- Test Case 4: Collinearity Edge Case --\n    X_aug_raw = np.hstack((X_raw_base, X_raw_base[:, 3:4]))\n    \n    # Re-standardize the augmented matrix\n    mean_aug = np.mean(X_aug_raw, axis=0)\n    std_aug = np.std(X_aug_raw, axis=0)\n    std_aug[std_aug == 0] = 1.0\n    X_aug = (X_aug_raw - mean_aug) / std_aug\n    n_aug, _ = X_aug.shape\n    \n    # Recompute lambda_max and grid for the augmented dataset\n    lambda_max_4 = np.max(np.abs(X_aug.T @ y / n_aug))\n    lambda_grid_4 = np.geomspace(lambda_max_4, 0.01 * lambda_max_4, 50)\n    \n    path_4 = solve_lasso_path(X_aug, y, lambda_grid_4)\n    results.append(path_4)\n\n    # --- Format and Print Final Output ---\n    output_str = \"[\" + \",\".join(f\"[{','.join(map(str, r))}]\" for r in results) + \"]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "与 Lasso 不同，Ridge 回归（$\\ell_2$ 正则化）虽然也能控制模型复杂度，但它通常不会将系数精确地缩减至零。本练习旨在探讨 Ridge 回归在处理生物信息学中常见的共线性特征时的独特行为。我们将通过数值计算，观察 Ridge 如何在高度相关的预测变量之间“重新分配”系数的权重，并学习计算一个关键指标——有效自由度，它为我们提供了一种量化模型实际复杂度的有力工具 。",
            "id": "4605283",
            "problem": "您将处理一个受基因表达分析启发的线性建模任务，但该任务纯粹以数值线性代数问题的形式呈现。考虑一个线性模型，其设计矩阵为 $X \\in \\mathbb{R}^{n \\times p}$，响应向量为 $y \\in \\mathbb{R}^{n}$。岭估计量被定义为惩罚最小二乘目标的最小化器，该目标在系数上附加了一个 $\\ell_2$ 惩罚项。您必须分析岭估计量如何在共线性的预测变量之间重新分配系数，并通过其线性平滑算子的迹来量化其有效自由度。\n\n您的程序必须为每个测试用例实现以下步骤：\n\n- 预处理：\n  - 将 $X$ 的每一列进行标准化，使其均值为零，方差为一。\n  - 将 $y$ 中心化，使其均值为零。\n  - 不要添加截距项，并且除了中心化之外，不要对 $y$ 进行标准化。\n\n- 岭估计量：\n  - 对于给定的惩罚参数 $\\lambda \\in \\mathbb{R}_{\\ge 0}$，使用一种不显式求逆矩阵的数值稳定方法，计算岭系数向量 $\\hat{\\beta}_{\\lambda} \\in \\mathbb{R}^{p}$。您必须使用一种基于奇异值分解的计算方法，该方法在 $X$ 秩亏时仍然有效。\n\n- 重新分配度量：\n  - 对于前两个预测变量（预处理后 $X$ 的前两列），计算比率 $r = \\hat{\\beta}_{\\lambda,1} / \\hat{\\beta}_{\\lambda,2}$，以量化系数在共线性预测变量之间的重新分配情况。\n\n- 有效自由度：\n  - 计算有效自由度，其值为将 $y$ 映射到由岭估计量产生的拟合值的线性算子的迹。该值必须以数值稳定的方式计算，利用奇异值分解，而不显式地构造或求逆病态矩阵。\n\n- 舍入与输出：\n  - 将 $r$ 和有效自由度都四舍五入到 $6$ 位小数以便报告。\n\n基本依据与约束：\n- 您必须从岭估计量的定义（即惩罚最小二乘目标的最小化器）以及有效自由度的定义（即将 $y$ 映射到其在岭回归下的拟合值的线性算子的迹）出发。您使用的任何计算公式都必须从这些基础推导而来。在您的实现中，不要假设可以使用未从这些基础推导出的快捷公式。\n- 您的算法必须对多重共线性（包括完全共线性）具有鲁棒性。\n\n测试套件：\n为以下三个测试用例实现上述步骤。为清晰起见，所有向量均为列向量。\n\n- 案例 A（高相关性，中等正则化）：\n  - 令 $x_1 = [-3,-2,-1,0,1,2,3,4,5,6]^{\\top}$。\n  - 令 $\\delta = [0.1, -0.2, 0.05, -0.05, 0.02, -0.01, 0.08, -0.03, 0.04, -0.02]^{\\top}$。\n  - 令 $x_2 = 0.99\\,x_1 + \\delta$。\n  - 令 $x_3 = [2,-1,3,-2,0,1,-3,2,-2,4]^{\\top}$。\n  - 构造 $X = [x_1, x_2, x_3]$ 和 $y = 1.5\\,x_1 + 1.5\\,x_2 + 0.2\\,x_3$。\n  - 使用 $\\lambda = 1.0$。\n\n- 案例 B（完全共线性，接近无惩罚极限）：\n  - 令 $x_1' = [-4,-3,-2,-1,0,1,2,3]^{\\top}$。\n  - 令 $x_2' = x_1'$。\n  - 令 $x_3' = [1,-2,3,-4,4,-3,2,-1]^{\\top}$。\n  - 构造 $X' = [x_1', x_2', x_3']$ 和 $y' = 3\\,x_1' + 0.5\\,x_3'$。\n  - 使用 $\\lambda = 1\\times 10^{-8}$。\n\n- 案例 C（高相关性，强正则化）：\n  - 使用与案例 A 相同的 $X$ 和 $y$。\n  - 使用 $\\lambda = 1\\times 10^{6}$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含按 A、B、C 顺序排列的三个案例的结果列表。\n- 每个案例的结果是一个列表 $[r, \\mathrm{df}]$，其中 $r$ 是比率，$\\mathrm{df}$ 是有效自由度，两者都四舍五入到 $6$ 位小数。\n- 最终输出必须是严格符合以下格式的单行：\n  - $[[r_A,\\mathrm{df}_A],[r_B,\\mathrm{df}_B],[r_C,\\mathrm{df}_C]]$\n- 不应打印任何额外文本。",
            "solution": "所述问题是一个良构的数值线性代数练习，其基础是正则化线性模型的原理。所有数据、参数和流程都已明确定义，任务在科学上和数学上都是合理的。不存在不一致、歧义或事实错误。因此，该问题被认为是有效的。\n\n问题的核心是使用基于奇异值分解（SVD）的数值稳定方法来计算岭回归系数向量 $\\hat{\\beta}_{\\lambda}$ 和有效自由度 $\\mathrm{df}(\\lambda)$。\n\n### 预处理\n\n设设计矩阵为 $X \\in \\mathbb{R}^{n \\times p}$，响应向量为 $y \\in \\mathbb{R}^{n}$。\n首先，将矩阵 $X$ 的每一列 $x_j$ 进行标准化，使其均值为 $0$，标准差为 $1$。令 $\\mu_j = \\frac{1}{n} \\sum_{i=1}^{n} X_{ij}$ 和 $\\sigma_j = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (X_{ij} - \\mu_j)^2}$ 分别为第 $j$ 列的均值和总体标准差。标准化后的矩阵（在后续推导中为简化起见，我们仍将其记为 $X$）的列为 $x'_j = (x_j - \\mu_j) / \\sigma_j$。如果 $\\sigma_j=0$，则标准化后的列是零向量。\n响应向量 $y$ 被中心化，使其均值为 $0$。令 $\\mu_y = \\frac{1}{n} \\sum_{i=1}^{n} y_i$。中心化后的向量（同样记为 $y$）为 $y' = y - \\mu_y$。\n\n### 岭回归估计量\n\n岭回归估计量 $\\hat{\\beta}_{\\lambda}$ 是使惩罚最小二乘目标函数最小化的向量 $\\beta \\in \\mathbb{R}^{p}$：\n$$\nL(\\beta) = \\|y - X\\beta\\|_2^2 + \\lambda \\|\\beta\\|_2^2\n$$\n其中 $\\lambda \\ge 0$ 是正则化参数。为了找到最小化器，我们计算 $L(\\beta)$ 关于 $\\beta$ 的梯度，并将其设为零：\n$$\n\\nabla_{\\beta} L(\\beta) = \\nabla_{\\beta} \\left( (y - X\\beta)^T(y - X\\beta) + \\lambda\\beta^T\\beta \\right)\n$$\n$$\n\\nabla_{\\beta} L(\\beta) = \\nabla_{\\beta} \\left( y^Ty - 2y^TX\\beta + \\beta^TX^TX\\beta + \\lambda\\beta^T\\beta \\right)\n$$\n$$\n\\nabla_{\\beta} L(\\beta) = -2X^Ty + 2X^TX\\beta + 2\\lambda\\beta\n$$\n将梯度设为零，得到岭回归的正规方程：\n$$\n(X^TX + \\lambda I)\\hat{\\beta}_{\\lambda} = X^Ty\n$$\n这导出了形式解 $\\hat{\\beta}_{\\lambda} = (X^TX + \\lambda I)^{-1}X^Ty$。然而，直接对矩阵 $(X^TX + \\lambda I)$ 求逆可能导致数值不稳定，特别是当 $X$ 是病态的（即其列高度相关）时。\n\n一种更稳定的方法是利用 $X$ 的奇异值分解（SVD）。设 $n \\times p$ 矩阵 $X$ 的SVD为：\n$$\nX = U D V^T\n$$\n其中 $U$ 是一个具有标准正交列的 $n \\times p$ 矩阵（$U^TU = I_p$），$D$ 是一个 $p \\times p$ 对角矩阵，其对角线上是奇异值 $d_1, d_2, \\ldots, d_p$，而 $V$ 是一个 $p \\times p$ 正交矩阵（$V^TV = VV^T = I_p$）。注意，由于在测试用例中 $n \\ge p$，我们使用的是经济型SVD。\n\n将SVD代入正规方程：\n$$\n\\hat{\\beta}_{\\lambda} = ( (VD^TU^T)(UDV^T) + \\lambda I )^{-1} (VD^TU^T)y\n$$\n$$\n\\hat{\\beta}_{\\lambda} = ( V D^T D V^T + \\lambda V I V^T )^{-1} V D^T U^T y\n$$\n$$\n\\hat{\\beta}_{\\lambda} = ( V(D^2 + \\lambda I)V^T )^{-1} V D^T U^T y\n$$\n利用性质 $(ABC)^{-1} = C^{-1}B^{-1}A^{-1}$：\n$$\n\\hat{\\beta}_{\\lambda} = (V^T)^{-1} (D^2 + \\lambda I)^{-1} V^{-1} V D^T U^T y\n$$\n由于 $V$ 是正交的，所以 $(V^T)^{-1} = V$ 且 $V^{-1}=V^T$。\n$$\n\\hat{\\beta}_{\\lambda} = V (D^2 + \\lambda I)^{-1} D^T U^T y\n$$\n因为 $D$（以及 $D^T$）是对角矩阵，所以矩阵 $(D^2 + \\lambda I)^{-1}D^T$ 也是对角矩阵。其第 $j$ 个对角元素是 $d_j / (d_j^2 + \\lambda)$。这个表达式是数值稳定的，因为它避免了矩阵求逆，并且当 $\\lambda0$ 时，项 $d_j^2+\\lambda$ 严格为正。该计算涉及稳定的运算：SVD、矩阵向量乘积和逐元素缩放。\n\n然后计算系数比率 $r = \\hat{\\beta}_{\\lambda,1} / \\hat{\\beta}_{\\lambda,2}$。\n\n### 有效自由度\n\n岭回归的拟合值由 $\\hat{y} = X\\hat{\\beta}_{\\lambda}$ 给出。我们可以将 $\\hat{y}$ 表示为 $y$ 的一个线性变换：\n$$\n\\hat{y} = X \\left( (X^TX + \\lambda I)^{-1}X^T y \\right) = S_{\\lambda} y\n$$\n矩阵 $S_{\\lambda} = X(X^TX + \\lambda I)^{-1}X^T$ 被称为岭回归的平滑矩阵或帽子矩阵。有效自由度 $\\mathrm{df}(\\lambda)$ 定义为该矩阵的迹：\n$$\n\\mathrm{df}(\\lambda) = \\mathrm{tr}(S_{\\lambda}) = \\mathrm{tr}(X(X^TX + \\lambda I)^{-1}X^T)\n$$\n利用迹的循环性质 $\\mathrm{tr}(ABC) = \\mathrm{tr}(CAB)$，我们可以写出：\n$$\n\\mathrm{df}(\\lambda) = \\mathrm{tr}((X^TX + \\lambda I)^{-1}X^TX)\n$$\n现在，将 $X$ 的SVD代入此表达式：\n$$\nX^TX = (UDV^T)^T(UDV^T) = VD^TU^TUDV^T = VD^2V^T\n$$\n$$\n(X^TX + \\lambda I)^{-1} = (VD^2V^T + \\lambda VIV^T)^{-1} = (V(D^2 + \\lambda I)V^T)^{-1} = V(D^2 + \\lambda I)^{-1}V^T\n$$\n因此，\n$$\n\\mathrm{df}(\\lambda) = \\mathrm{tr} \\left( V(D^2 + \\lambda I)^{-1}V^T \\cdot VD^2V^T \\right)\n$$\n$$\n\\mathrm{df}(\\lambda) = \\mathrm{tr} \\left( V(D^2 + \\lambda I)^{-1}D^2V^T \\right)\n$$\n再次利用迹的循环性质 $\\mathrm{tr}(V A V^T) = \\mathrm{tr}(A V^T V) = \\mathrm{tr}(A)$：\n$$\n\\mathrm{df}(\\lambda) = \\mathrm{tr} \\left( (D^2 + \\lambda I)^{-1}D^2 \\right)\n$$\n由于 $(D^2 + \\lambda I)^{-1}D^2$ 是一个对角矩阵，其对角元素为 $d_j^2 / (d_j^2 + \\lambda)$，它的迹就是这些元素的和：\n$$\n\\mathrm{df}(\\lambda) = \\sum_{j=1}^{p} \\frac{d_j^2}{d_j^2 + \\lambda}\n$$\n这个公式提供了一种从 $X$ 的奇异值直接、高效且数值稳定地计算有效自由度的方法。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the specified ridge regression analysis for three test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    x1_a = np.array([-3, -2, -1, 0, 1, 2, 3, 4, 5, 6])\n    delta_a = np.array([0.1, -0.2, 0.05, -0.05, 0.02, -0.01, 0.08, -0.03, 0.04, -0.02])\n    x2_a = 0.99 * x1_a + delta_a\n    x3_a = np.array([2, -1, 3, -2, 0, 1, -3, 2, -2, 4])\n    X_a = np.vstack([x1_a, x2_a, x3_a]).T\n    y_a = 1.5 * x1_a + 1.5 * x2_a + 0.2 * x3_a\n    \n    x1_b = np.array([-4, -3, -2, -1, 0, 1, 2, 3])\n    x2_b = x1_b.copy()\n    x3_b = np.array([1, -2, 3, -4, 4, -3, 2, -1])\n    X_b = np.vstack([x1_b, x2_b, x3_b]).T\n    y_b = 3 * x1_b + 0.5 * x3_b\n\n    test_cases = [\n        {\"X\": X_a, \"y\": y_a, \"lambda\": 1.0},\n        {\"X\": X_b, \"y\": y_b, \"lambda\": 1e-8},\n        {\"X\": X_a, \"y\": y_a, \"lambda\": 1e6},\n    ]\n\n    results = []\n\n    for case in test_cases:\n        X, y, lambda_val = case[\"X\"], case[\"y\"], case[\"lambda\"]\n        n, p = X.shape\n\n        # Preprocessing:\n        # 1. Center y\n        y_centered = y - np.mean(y)\n        \n        # 2. Standardize X\n        X_mean = np.mean(X, axis=0)\n        # Use ddof=0 for population standard deviation, as is default in numpy.\n        X_std = np.std(X, axis=0)\n        \n        # Standardize X, handling columns with zero standard deviation\n        X_scaled = np.zeros_like(X, dtype=float)\n        non_zero_std = X_std > 1e-12 # A small tolerance for floating point\n        X_scaled[:, non_zero_std] = (X[:, non_zero_std] - X_mean[non_zero_std]) / X_std[non_zero_std]\n\n        # Singular Value Decomposition of the standardized matrix\n        # Use economy SVD since n >= p\n        U, s, Vt = np.linalg.svd(X_scaled, full_matrices=False)\n        V = Vt.T\n\n        # Compute the ridge coefficient vector beta_hat\n        # beta_hat = V @ diag(s / (s^2 + lambda)) @ U.T @ y_centered\n        # This is implemented efficiently without forming diagonal matrices\n        tmp = U.T @ y_centered\n        d_term = s / (s**2 + lambda_val)\n        \n        # Pad d_term with zeros if X was rank-deficient\n        d_term_padded = np.zeros(p)\n        d_term_padded[:len(d_term)] = d_term\n        \n        beta_hat = V @ (d_term_padded * tmp)\n\n        # Compute redistribution metric r\n        # The problem statement guarantees this won't be a division by zero for the given cases.\n        if abs(beta_hat[1])  1e-12:\n            # Handle potential division by very small number, though not expected for these cases.\n            # If beta_hat[1] is effectively zero, the ratio is ill-defined.\n            r = np.inf if beta_hat[0] > 0 else -np.inf if beta_hat[0]  0 else np.nan\n        else:\n            r = beta_hat[0] / beta_hat[1]\n\n        # Compute effective degrees of freedom df\n        # df = sum(s_j^2 / (s_j^2 + lambda))\n        s_squared = s**2\n        df = np.sum(s_squared / (s_squared + lambda_val))\n\n        # Rounding and appending results\n        r_rounded = round(r, 6)\n        df_rounded = round(df, 6)\n        results.append([r_rounded, df_rounded])\n\n    # Format the final output string exactly as required\n    formatted_results = \",\".join([f\"[{res[0]},{res[1]}]\" for res in results])\n    print(f\"[{formatted_results}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在实现了求解正则化模型的算法后，一个自然而然的问题是：我们如何确定算法找到的解确实是最优的？这个高级实践将带您深入了解 Lasso 背后的凸优化理论。通过推导并计算“对偶问题”的目标值，我们将学习如何利用“对偶间隙”这一强大工具来验证我们求解器所找到的解的最优性，这对于构建可靠的预测模型至关重要 。",
            "id": "4605237",
            "problem": "您正在使用最小绝对收缩和选择算子 (LASSO) 对基因表达对临床反应的影响进行建模，通过稀疏性来控制模型复杂度。考虑原始 LASSO 问题：对系数向量 $w \\in \\mathbb{R}^p$ 最小化目标函数 $0.5 \\lVert y - X w \\rVert_2^2 + \\lambda \\lVert w \\rVert_1$，其中 $X \\in \\mathbb{R}^{n \\times p}$ 是一个由标准化特征（例如，基因表达水平）组成的固定设计矩阵，$y \\in \\mathbb{R}^n$ 是响应向量（例如，临床表型），$\\lambda \\in \\mathbb{R}_{0}$ 是正则化强度。您的任务是使用凸对偶推导其对偶问题，根据残差和特征之间的相关性来解释激活的对偶约束，然后计算对偶间隙以验证最优性。\n\n推导要求：\n- 从凸分析的基础出发：凸规划的 Fenchel 共轭和拉格朗日对偶的定义。具体来说，只使用经过充分检验的事实：平方欧几里得范数的凸性、$\\ell_1$ 范数的性质，以及这些函数的 Fenchel 共轭。不要假设任何预先推导出的 LASSO 对偶形式。\n- 通过引入一个表示残差的等式约束来推导对偶问题，构建拉格朗日函数，并使用共轭函数求解其关于原始变量的下确界。明确指出对偶可行集和对偶目标函数。\n- 使用 Karush–Kuhn–Tucker (KKT) 条件，根据残差和特征之间的相关性来解释对偶可行性约束。总结激活约束如何与原始问题中的非零系数相关。\n- 提供任意原始-对偶对的对偶间隙的显式表达式，并解释为什么它在最优解处是非负的且等于零。\n\n实现要求：\n- 使用带有软阈值化的循环坐标下降法为原始问题实现一个求解器。对于每次坐标更新，使用最小化一维二次函数加 $\\ell_1$ 惩罚项的基本步骤。使用停止准则：坐标的最大绝对变化小于 $10^{-10}$ 的容差，或达到 $50000$ 次迭代的最大值，以先满足的条件为准。\n- 通过缩放从残差 $r = y - X w$ 构建一个对偶可行变量 $\\theta \\in \\mathbb{R}^n$：设 $\\theta = \\alpha r$，其中 $\\alpha = \\min\\{1, \\lambda / \\lVert X^\\top r \\rVert_\\infty\\}$，并约定如果 $\\lVert X^\\top r \\rVert_\\infty = 0$ 则 $\\alpha = 1$。这确保了对偶可行性。\n- 计算原始目标函数 $P(w) = 0.5 \\lVert y - X w \\rVert_2^2 + \\lambda \\lVert w \\rVert_1$，对偶目标函数 $D(\\theta) = -0.5 \\lVert \\theta \\rVert_2^2 + y^\\top \\theta$，以及对偶间隙 $G = P(w) - D(\\theta)$。\n- 将激活的对偶约束数量确定为满足 $\\lvert X_{\\cdot j}^\\top \\theta \\rvert$ 与 $\\lambda$ 的绝对容差在 $10^{-7}$ 之内的索引 $j \\in \\{1,\\dots,p\\}$ 的数量，即 $\\lvert \\lvert X_{\\cdot j}^\\top \\theta \\rvert - \\lambda \\rvert \\le 10^{-7}$。\n\n测试套件：\n对于以下每个测试用例，运行您的求解器，构建对偶变量，计算对偶间隙，并统计激活约束的数量。对于每个用例，程序应输出一个包含对偶间隙和激活约束计数的二元列表。对偶间隙必须四舍五入到 $10$ 位小数。最终输出必须是包含所有用例结果的单行列表，格式如以下示例所示。\n\n- 测试用例 1 (常规情况，中等正则化)：$X \\in \\mathbb{R}^{5 \\times 3}$,\n  $$\n  X = \\begin{bmatrix}\n  1  0.5  0 \\\\\n  0  1.0  0.5 \\\\\n  1  0  1.0 \\\\\n  2  1.0  0 \\\\\n  0  0.5  1.0\n  \\end{bmatrix}, \\quad\n  y = \\begin{bmatrix} 2.0 \\\\ 0.0 \\\\ 1.0 \\\\ 3.0 \\\\ 1.0 \\end{bmatrix}, \\quad\n  \\lambda = 0.5.\n  $$\n- 测试用例 2 (非常大的正则化；全零解为最优解)：与测试用例 1 相同的 $X$ 和 $y$，$\\lambda = 10.0$。\n- 测试用例 3 (边界情况；约束在零解处恰好激活)：与测试用例 1 相同的 $X$ 和 $y$，$\\lambda = 9.0$。\n- 测试用例 4 (欠定系统 $p  n$)：$X \\in \\mathbb{R}^{4 \\times 6}$,\n  $$\n  X = \\begin{bmatrix}\n  1  0  0  0.2  0  0.1 \\\\\n  0  1  0  0.1  0.2  0 \\\\\n  0  0  1  0  0.1  0.2 \\\\\n  1  1  1  0.1  0.1  0.1\n  \\end{bmatrix}, \\quad\n  y = \\begin{bmatrix} 0.5 \\\\ 1.2 \\\\ -0.7 \\\\ 0.3 \\end{bmatrix}, \\quad\n  \\lambda = 0.3.\n  $$\n- 测试用例 5 (相关特征；多个近似激活的约束)：$X \\in \\mathbb{R}^{6 \\times 4}$,\n  $$\n  X = \\begin{bmatrix}\n  1  0.9  0  0 \\\\\n  0  0.1  1  0.9 \\\\\n  1  1.1  0  0.1 \\\\\n  0  -0.1  1  1.1 \\\\\n  0.5  0.45  0.5  0.55 \\\\\n  1  0.95  0.2  0\n  \\end{bmatrix}, \\quad\n  y = \\begin{bmatrix} 1.0 \\\\ -1.0 \\\\ 0.5 \\\\ -0.5 \\\\ 0.2 \\\\ 0.8 \\end{bmatrix}, \\quad\n  \\lambda = 0.4.\n  $$\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表。每个测试用例贡献一个二元列表 $[\\text{gap}, \\text{active\\_count}]$，其中 $\\text{gap}$ 是一个四舍五入到 $10$ 位小数的浮点数，$\\text{active\\_count}$ 是一个整数。例如，三个测试用例的有效输出应如下所示：$[[0.0000000000,1],[0.1234567890,0],[0.0000001000,2]]$。\n\n此问题不涉及任何物理单位或角度单位。所有计算都是无量纲的。",
            "solution": "用户提供的问题是计算统计和机器学习领域中一个有效且适定的问题，具体涉及 LASSO 估计器的分析。该问题具有科学依据，无需不可验证的假设，并包含了完整解决方案所需的所有必要信息。因此，我们将按要求进行推导和实现。\n\n解决方案分为三个部分：LASSO 对偶问题的推导、最优性条件的解释，以及对偶间隙的公式化和性质。在这些理论部分之后，将描述最终实现中使用的计算方法。\n\n### LASSO 对偶问题的推导\n\n原始 LASSO 问题表述为：\n$$\n\\text{minimize}_{w \\in \\mathbb{R}^p} \\quad P(w) = \\frac{1}{2} \\lVert y - X w \\rVert_2^2 + \\lambda \\lVert w \\rVert_1\n$$\n其中 $X \\in \\mathbb{R}^{n \\times p}$ 是数据矩阵，$y \\in \\mathbb{R}^n$ 是响应向量，$w \\in \\mathbb{R}^p$ 是系数向量，$\\lambda  0$ 是正则化参数。\n\n为了使用指定的方法推导对偶问题，我们首先为残差引入一个辅助变量 $r \\in \\mathbb{R}^n$，将问题重写为一个等价的约束优化问题：\n$$\n\\text{minimize}_{w \\in \\mathbb{R}^p, r \\in \\mathbb{R}^n} \\quad \\frac{1}{2} \\lVert r \\rVert_2^2 + \\lambda \\lVert w \\rVert_1 \\quad \\text{subject to} \\quad r + Xw = y\n$$\n通过为等式约束引入一个对偶变量（拉格朗日乘子）$\\theta \\in \\mathbb{R}^n$ 来构建此问题的拉格朗日函数：\n$$\nL(w, r, \\theta) = \\frac{1}{2} \\lVert r \\rVert_2^2 + \\lambda \\lVert w \\rVert_1 + \\theta^\\top (y - r - Xw)\n$$\n我们可以重排拉格朗日函数的各项，以分离原始变量 $w$ 和 $r$：\n$$\nL(w, r, \\theta) = \\left( \\lambda \\lVert w \\rVert_1 - \\theta^\\top Xw \\right) + \\left( \\frac{1}{2} \\lVert r \\rVert_2^2 - \\theta^\\top r \\right) + y^\\top \\theta\n$$\n拉格朗日对偶函数 $g(\\theta)$ 是拉格朗日函数关于原始变量的下确界：\n$$\ng(\\theta) = \\inf_{w \\in \\mathbb{R}^p, r \\in \\mathbb{R}^n} L(w, r, \\theta) = \\inf_{w} \\left( \\lambda \\lVert w \\rVert_1 - (X^\\top\\theta)^\\top w \\right) + \\inf_{r} \\left( \\frac{1}{2} \\lVert r \\rVert_2^2 - \\theta^\\top r \\right) + y^\\top \\theta\n$$\n这两个下确界可以使用 Fenchel 共轭来评估。一个函数 $f(x)$ 的 Fenchel 共轭是 $f^*(z) = \\sup_x (z^\\top x - f(x))$，这意味着 $\\inf_x (f(x) - z^\\top x) = -f^*(z)$。\n\n对于涉及 $w$ 的项，令 $f_1(w) = \\lambda \\lVert w \\rVert_1$。其共轭函数是对偶范数球的指示函数：\n$$\nf_1^*(z) = (\\lambda \\lVert \\cdot \\rVert_1)^*(z) =\n\\begin{cases}\n0  \\text{if } \\lVert z \\rVert_\\infty \\le \\lambda \\\\\n\\infty  \\text{otherwise}\n\\end{cases}\n$$\n因此，关于 $w$ 的下确界是：\n$$\n\\inf_{w} \\left( \\lambda \\lVert w \\rVert_1 - (X^\\top\\theta)^\\top w \\right) = -f_1^*(X^\\top\\theta) =\n\\begin{cases}\n0  \\text{if } \\lVert X^\\top\\theta \\rVert_\\infty \\le \\lambda \\\\\n-\\infty  \\text{otherwise}\n\\end{cases}\n$$\n对于涉及 $r$ 的项，令 $f_2(r) = \\frac{1}{2} \\lVert r \\rVert_2^2$。这个函数是其自身的 Fenchel 共轭，即 $f_2^*(\\theta) = \\frac{1}{2} \\lVert \\theta \\rVert_2^2$。\n因此，关于 $r$ 的下确界是：\n$$\n\\inf_{r} \\left( \\frac{1}{2} \\lVert r \\rVert_2^2 - \\theta^\\top r \\right) = -f_2^*(\\theta) = -\\frac{1}{2} \\lVert \\theta \\rVert_2^2\n$$\n将这些结果代回 $g(\\theta)$ 的表达式，我们发现对偶函数仅在 $\\lVert X^\\top\\theta \\rVert_\\infty \\le \\lambda$ 时是有限的。对偶问题是最大化 $g(\\theta)$：\n$$\n\\text{maximize}_{\\theta \\in \\mathbb{R}^n} \\quad D(\\theta) = -\\frac{1}{2} \\lVert \\theta \\rVert_2^2 + y^\\top \\theta \\quad \\text{subject to} \\quad \\lVert X^\\top\\theta \\rVert_\\infty \\le \\lambda\n$$\n对偶变量 $\\theta$ 的可行集由约束 $\\lVert X^\\top\\theta \\rVert_\\infty \\le \\lambda$ 定义，这等价于对所有特征 $j=1, \\dots, p$ 的约束集 $\\lvert X_{\\cdot j}^\\top \\theta \\rvert \\le \\lambda$。\n\n### 通过 Karush–Kuhn–Tucker (KKT) 条件进行解释\n\nKarush–Kuhn–Tucker (KKT) 条件为最优性提供了必要条件。对于一个最优的原始-对偶对 $(w^*, r^*, \\theta^*)$，这些条件是：\n$1$. **原始可行性**：$r^* + Xw^* = y$。\n$2$. **对偶可行性**：$\\lVert X^\\top \\theta^* \\rVert_\\infty \\le \\lambda$。\n$3$. **平稳性**：拉格朗日函数关于原始变量的梯度在最优点必须为零。\n    - $\\nabla_r L = r^* - \\theta^* = 0 \\implies r^* = \\theta^*$。在最优解处，对偶变量等于原始残差向量。\n    - $\\nabla_w L = 0 \\implies 0 \\in \\partial (\\lambda \\lVert w \\rVert_1)|_{w^*} - X^\\top \\theta^*$，这意味着 $X^\\top \\theta^*$ 必须是 $\\lambda \\lVert w \\rVert_1$ 在 $w^*$ 处的次梯度。\n\n$\\lambda \\lVert w \\rVert_1$ 在向量 $w$ 处的次梯度是一个向量 $v$，其分量为：\n$$\nv_j =\n\\begin{cases}\n\\lambda \\cdot \\text{sign}(w_j)  \\text{if } w_j \\neq 0 \\\\\n\\text{any value in } [-\\lambda, \\lambda]  \\text{if } w_j = 0\n\\end{cases}\n$$\n将平稳性条件 ($r^*=\\theta^*$) 与次梯度的定义相结合，可以得出一个深刻的解释。对于每个特征 $j$：\n- 如果 $w_j^* \\neq 0$（特征 $j$ 被模型选中），则 $(X^\\top \\theta^*)_j = \\lambda \\cdot \\text{sign}(w_j^*)$。这意味着 $\\lvert (X^\\top r^*)_j \\rvert = \\lambda$。特征 $j$ 与最优残差 $r^*$ 之间相关性的绝对值恰好等于正则化强度 $\\lambda$。相应的对偶约束是**激活的** (active)。\n- 如果 $w_j^* = 0$（特征 $j$ 未被选中），则 $\\lvert (X^\\top \\theta^*)_j \\rvert \\le \\lambda$。特征 $j$ 与最优残差之间相关性的绝对值小于或等于 $\\lambda$。相应的对偶约束是**非激活的** (inactive) 或激活的。\n\n### 对偶间隙与最优性验证\n对于任何原始可行解 $w$ 和任何对偶可行解 $\\theta$，原始目标函数 $P(w)$ 和对偶目标函数 $D(\\theta)$ 定义为：\n$$\nP(w) = \\frac{1}{2} \\lVert y - X w \\rVert_2^2 + \\lambda \\lVert w \\rVert_1\n$$\n$$\nD(\\theta) = -\\frac{1}{2} \\lVert \\theta \\rVert_2^2 + y^\\top \\theta\n$$\n**对偶间隙** $G(w, \\theta)$ 是原始目标函数值与对偶目标函数值之间的差：\n$$\nG(w, \\theta) = P(w) - D(\\theta)\n$$\n根据凸问题的弱对偶性，对于任何原始可行解 $w$ 和对偶可行解 $\\theta$，都有 $P(w) \\ge D(\\theta)$。因此，对偶间隙总是非负的：$G(w, \\theta) \\ge 0$。对于 LASSO 问题，强对偶性成立（因为满足 Slater 条件），这意味着在最优的原始-对偶对 $(w^*, \\theta^*)$ 处，对偶间隙为零：$G(w^*, \\theta^*) = P(w^*) - D(\\theta^*) = 0$。\n对偶间隙可作为次优性的一个证明。从一个近似的原始解 $w$ 和一个对应的可行对偶解 $\\theta$ 计算出的一个小的非负间隙表明，$w$ 接近于真实的最优解。\n\n### 计算方法\n为了数值求解该问题，我们实现以下步骤：\n$1$. 使用**循环坐标下降**算法求解原始 LASSO 问题以得到 $w$。该算法迭代更新每个系数 $w_j$ 至其最优值，同时保持所有其他系数固定。$w_j$ 的更新规则源自最小化一个带 $\\ell_1$ 惩罚项的一维二次目标函数，从而得到软阈值算子：\n$$\nw_j \\leftarrow \\frac{S_{\\lambda}(X_{\\cdot j}^\\top (y - \\sum_{k \\neq j} X_{\\cdot k}w_k))}{\\lVert X_{\\cdot j} \\rVert_2^2} \\quad \\text{where} \\quad S_{\\lambda}(z) = \\text{sign}(z) \\max(\\lvert z \\rvert - \\lambda, 0)\n$$\n迭代持续进行，直到系数的最大绝对变化量低于 $10^{-10}$ 的容差，或达到 $50000$ 次的最大迭代次数。\n$2$. 根据求解器得到的原始解 $w$，我们计算残差向量 $r = y - Xw$。\n$3$. 通过缩放残差 $r$ 来构造一个对偶可行变量 $\\theta$。令 $c = X^\\top r$。我们计算缩放因子 $\\alpha = \\min\\{1, \\lambda / \\lVert c \\rVert_\\infty\\}$（如果 $\\lVert c \\rVert_\\infty=0$，则 $\\alpha=1$）。然后对偶变量为 $\\theta = \\alpha r$。这种构造确保了 $\\lVert X^\\top\\theta \\rVert_\\infty \\le \\lambda$，从而满足对偶可行性。\n$4$. 利用原始解 $w$ 和对偶可行变量 $\\theta$，我们计算原始目标函数 $P(w)$、对偶目标函数 $D(\\theta)$ 和对偶间隙 $G(w, \\theta) = P(w) - D(\\theta)$。\n$5$. 最后，我们通过找出满足条件 $\\lvert \\lvert (X^\\top \\theta)_j \\rvert - \\lambda \\rvert \\le 10^{-7}$ 的索引 $j$ 的数量，来统计激活的对偶约束的数量。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the LASSO problem for a series of test cases using coordinate descent,\n    computes the duality gap, and counts active dual constraints.\n    \"\"\"\n\n    def lasso_solver(X, y, lambda_val, tol=1e-10, max_iter=50000):\n        \"\"\"\n        Solves the LASSO primal problem using cyclic coordinate descent.\n        \"\"\"\n        n, p = X.shape\n        w = np.zeros(p)\n        \n        # Precompute squared L2-norms of columns of X, which do not change.\n        d = np.sum(X**2, axis=0)\n        # Handle case where a column is all zeros to avoid division by zero.\n        d[d == 0] = 1.0 \n\n        for i in range(max_iter):\n            w_old = w.copy()\n            for j in range(p):\n                # Calculate rho_j = X_j^T * (y - sum_{k!=j} X_k * w_k)\n                # This can be calculated efficiently as X_j^T * (y - Xw + X_j * w_j)\n                rho_j = X[:, j].T @ (y - X @ w) + d[j] * w[j]\n                \n                # Apply soft-thresholding operator\n                w_j_new = 0.0\n                if rho_j > lambda_val:\n                    w_j_new = (rho_j - lambda_val) / d[j]\n                elif rho_j  -lambda_val:\n                    w_j_new = (rho_j + lambda_val) / d[j]\n                \n                w[j] = w_j_new\n\n            # Check for convergence\n            if np.max(np.abs(w - w_old))  tol:\n                break\n        \n        return w\n\n    test_cases = [\n        # Test case 1\n        (np.array([[1.0, 0.5, 0.0], [0.0, 1.0, 0.5], [1.0, 0.0, 1.0], [2.0, 1.0, 0.0], [0.0, 0.5, 1.0]]),\n         np.array([2.0, 0.0, 1.0, 3.0, 1.0]),\n         0.5),\n        # Test case 2\n        (np.array([[1.0, 0.5, 0.0], [0.0, 1.0, 0.5], [1.0, 0.0, 1.0], [2.0, 1.0, 0.0], [0.0, 0.5, 1.0]]),\n         np.array([2.0, 0.0, 1.0, 3.0, 1.0]),\n         10.0),\n        # Test case 3\n        (np.array([[1.0, 0.5, 0.0], [0.0, 1.0, 0.5], [1.0, 0.0, 1.0], [2.0, 1.0, 0.0], [0.0, 0.5, 1.0]]),\n         np.array([2.0, 0.0, 1.0, 3.0, 1.0]),\n         9.0),\n        # Test case 4\n        (np.array([[1.0, 0.0, 0.0, 0.2, 0.0, 0.1], [0.0, 1.0, 0.0, 0.1, 0.2, 0.0], [0.0, 0.0, 1.0, 0.0, 0.1, 0.2], [1.0, 1.0, 1.0, 0.1, 0.1, 0.1]]),\n         np.array([0.5, 1.2, -0.7, 0.3]),\n         0.3),\n        # Test case 5\n        (np.array([[1.0, 0.9, 0.0, 0.0], [0.0, 0.1, 1.0, 0.9], [1.0, 1.1, 0.0, 0.1], [0.0, -0.1, 1.0, 1.1], [0.5, 0.45, 0.5, 0.55], [1.0, 0.95, 0.2, 0.0]]),\n         np.array([1.0, -1.0, 0.5, -0.5, 0.2, 0.8]),\n         0.4)\n    ]\n\n    all_results = []\n    \n    for X, y, lambda_val in test_cases:\n        # 1. Solve for the primal variable w\n        w = lasso_solver(X, y, lambda_val)\n        \n        # 2. Construct the dual-feasible variable theta\n        r = y - X @ w\n        c = X.T @ r\n        norm_inf_c = np.max(np.abs(c))\n        \n        alpha = 1.0\n        if norm_inf_c > 1e-12: # Check against small tolerance for floating point\n            alpha = min(1.0, lambda_val / norm_inf_c)\n        \n        theta = alpha * r\n        \n        # 3. Compute primal and dual objectives, and the duality gap\n        primal_obj = 0.5 * np.sum(r**2) + lambda_val * np.sum(np.abs(w))\n        dual_obj = -0.5 * np.sum(theta**2) + y.T @ theta\n        duality_gap = primal_obj - dual_obj\n        \n        # 4. Count active dual constraints\n        dual_corr = X.T @ theta\n        active_tol = 1e-7\n        active_count = np.sum(np.abs(np.abs(dual_corr) - lambda_val) = active_tol)\n        \n        all_results.append((duality_gap, active_count))\n        \n    # Format the final output string exactly as specified\n    case_strings = []\n    for gap, count in all_results:\n        case_strings.append(f\"[{gap:.10f},{count}]\")\n    final_output = f\"[{','.join(case_strings)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        }
    ]
}