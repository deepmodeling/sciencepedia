{
    "hands_on_practices": [
        {
            "introduction": "The t-SNE algorithm relies on several heuristics to produce compelling visualizations. This practice problem focuses on \"early exaggeration,\" a crucial initial step where attractive forces between similar data points are temporarily amplified. By analyzing the gradient of the cost function, you will develop a first-principles understanding of how this modification helps shape the global structure of the embedding and encourages the formation of distinct, well-separated clusters .",
            "id": "4590746",
            "problem": "A bioinformatics team is embedding single-cell RNA-sequencing profiles into a two-dimensional space using t-distributed Stochastic Neighbor Embedding (t-SNE). The algorithm minimizes the Kullback–Leibler divergence between a high-dimensional pairwise similarity distribution and a low-dimensional pairwise similarity distribution. Let the high-dimensional joint similarities be denoted by $p_{ij}$, constructed from Gaussian kernels over standardized gene-expression distances and symmetrized so that $\\sum_{i \\neq j} p_{ij} = 1$, and let the low-dimensional joint similarities be $q_{ij}$ constructed from a Student-$t$ kernel over the two-dimensional coordinates $\\{y_i\\}$ with $\\sum_{i \\neq j} q_{ij} = 1$. The loss is the Kullback–Leibler divergence $C = \\sum_{i \\neq j} p_{ij} \\log \\frac{p_{ij}}{q_{ij}}$, which is minimized by gradient descent with respect to the coordinates $\\{y_i\\}$.\n\nDuring the initial phase, the team applies an “early exaggeration” schedule: for a fixed number of iterations, they replace $p_{ij}$ by $\\tilde{p}_{ij} = \\alpha p_{ij}$ with $\\alpha > 1$ in the optimization while keeping the construction of $q_{ij}$ and its normalization unchanged. From first principles beginning with the definition of $C$ and the Student-$t$ kernel for $q_{ij}$, reason about how this modification alters the gradient of $C$ with respect to $y_i$ and, consequently, the dynamics of points belonging to the same cell type (which typically have larger $p_{ij}$ within that neighborhood). Then, determine which statement best captures the effect of increasing $\\alpha$ on the early trajectory of the embedding and the sharpening of clusters.\n\nChoose the single best option:\n\nA. Increasing $\\alpha$ scales the attractive component of the gradient associated with $p_{ij}$ by a factor of $\\alpha$ without directly scaling the repulsive component associated with $q_{ij}$, thereby causing points with large $p_{ij}$ to move together more rapidly and sharpening within-cluster cohesion early in optimization, while leaving the final optimum unchanged when $\\alpha$ is later returned to $1$.\n\nB. Increasing $\\alpha$ scales the low-dimensional similarities $q_{ij}$ by a factor of $\\alpha$, amplifying repulsive forces uniformly across all pairs and causing clusters to spread apart early, which sharpens boundaries by pushing points away.\n\nC. Increasing $\\alpha$ scales both the attractive and repulsive components of the gradient equally, so the net forces are unchanged and early exaggeration has negligible impact on how quickly clusters sharpen.\n\nD. Increasing $\\alpha$ effectively lowers the perplexity used to construct $p_{ij}$, shrinking inter-point distances in the high-dimensional space and causing a global contraction that merges nearby clusters rather than sharpening them.",
            "solution": "### 1. Principle-based Derivation\nThe t-SNE algorithm minimizes the Kullback-Leibler (KL) divergence $C = \\sum_{i \\neq j} p_{ij} \\log \\frac{p_{ij}}{q_{ij}}$. The gradient of this cost function with respect to the low-dimensional coordinate $y_i$ can be decomposed into an attractive and a repulsive component:\n$$ \\frac{\\partial C}{\\partial y_i} = \\underbrace{4 \\sum_{j \\neq i} p_{ij} (y_i - y_j) (1 + \\|y_i - y_j\\|^2)^{-1}}_{\\text{Attractive forces}} - \\underbrace{4 \\sum_{j \\neq i} q_{ij} (y_i - y_j) (1 + \\|y_i - y_j\\|^2)^{-1}}_{\\text{Repulsive forces}} $$\n\nThe \"early exaggeration\" phase modifies the optimization by replacing the high-dimensional similarities $p_{ij}$ with $\\tilde{p}_{ij} = \\alpha p_{ij}$ for $\\alpha > 1$. Note that this is a modification to the gradient calculation itself, not a re-optimization of a well-defined new KL divergence (which would require re-normalizing the modified $p_{ij}$ values). The practical implementation substitutes $\\alpha p_{ij}$ directly into the gradient formula:\n$$ \\left( \\frac{\\partial C}{\\partial y_i} \\right)_{\\text{exaggerated}} = 4 \\sum_{j \\neq i} (\\alpha p_{ij} - q_{ij}) (y_i - y_j) (1 + \\|y_i - y_j\\|^2)^{-1} $$\nWe can separate this into the scaled attractive and unscaled repulsive components:\n$$ \\left( \\frac{\\partial C}{\\partial y_i} \\right)_{\\text{exaggerated}} = \\alpha \\left( 4 \\sum_{j \\neq i} p_{ij} (y_i - y_j) (1 + \\|y_i - y_j\\|^2)^{-1} \\right) - \\left( 4 \\sum_{j \\neq i} q_{ij} (y_i - y_j) (1 + \\|y_i - y_j\\|^2)^{-1} \\right) $$\nThis shows that the attractive forces, proportional to $p_{ij}$, are magnified by the factor $\\alpha$, while the repulsive forces, proportional to $q_{ij}$, remain unchanged.\n\n### 2. Effect of the Modified Gradient\nSince $\\alpha > 1$, the attractive forces are significantly stronger than the repulsive forces during this initial phase. This has a profound effect on the embedding dynamics:\n-   Pairs of points with high similarity in the original space (large $p_{ij}$), which typically belong to the same cluster or cell type, experience a much stronger pull towards each other.\n-   This causes these groups of similar points to rapidly form tight, compact clusters in the low-dimensional space.\n-   This initial formation of well-separated clusters helps establish the global structure of the embedding, preventing clusters from getting \"stuck\" in poor local minima.\n-   Once the early exaggeration phase ends (i.e., $\\alpha$ is returned to $1$), the optimization proceeds with the standard gradient, fine-tuning the positions of points within and between the pre-formed clusters. The final objective function is the original one, so the exaggeration doesn't change the theoretical optimum, but it helps the optimization process find a better one.\n\n### 3. Option-by-Option Analysis\n-   **A. Correct.** This option accurately states that the attractive component (associated with $p_{ij}$) is scaled by $\\alpha$ while the repulsive component (associated with $q_{ij}$) is not. It correctly infers that this leads to faster movement of similar points and sharpens cluster cohesion early. It also correctly notes that the final optimum is defined by the original cost function.\n-   **B. Incorrect.** This confuses the roles of $p_{ij}$ and $q_{ij}$. The modification applies to the high-dimensional similarities $p_{ij}$, amplifying attractive forces, not the low-dimensional similarities $q_{ij}$ and repulsive forces.\n-   **C. Incorrect.** The attractive and repulsive components are not scaled equally. The core of the technique is the *imbalance* created by scaling only the attractive term.\n-   **D. Incorrect.** Perplexity is a hyperparameter used to construct the $p_{ij}$ values initially; early exaggeration is a subsequent modification to the optimization process and does not alter perplexity or the high-dimensional distances. Its goal is to sharpen clusters, not merge them.\n\nThe derivation confirms that option A provides the most accurate description of the mechanism and effect of early exaggeration in t-SNE.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Visualizing complex biological data, which may contain both tight cell clusters and diffuse transitional states, requires careful hyperparameter selection to avoid misleading artifacts. This exercise places you in a realistic scenario where you must devise a multi-scale strategy for both UMAP and t-SNE to faithfully represent these mixed structures. By reasoning about parameters like `n_neighbors` and `min_dist`, you will learn how to preserve different structural features and prevent the \"over-clumping\" that can obscure important biological signals .",
            "id": "4590753",
            "problem": "A single-cell ribonucleic acid sequencing (scRNA-seq) study of a heterogeneous tumor microenvironment yields a matrix of $N=8000$ cells by $p=20000$ genes, which is preprocessed by logarithmic normalization and projection onto the top $d=50$ principal components. Prior biological knowledge suggests two regimes in the latent structure: (i) tightly clustered immune cell subtypes with small within-cluster variance, and (ii) diffuse epithelial cell states spanning an epithelial-to-mesenchymal transition with broad within-cluster variance and transitional continua. You will construct a two-dimensional visualization using both t-Distributed Stochastic Neighbor Embedding (t-SNE) and Uniform Manifold Approximation and Projection (UMAP) to support downstream medical data analytics, such as overlaying clinical covariates and discovering putative subtypes. The aim is to avoid over-clumping, that is, artificially compact clusters that distort local densities and merge transitional continua into dense blobs, while still preserving meaningful local neighborhoods.\n\nAs fundamental base, recall that t-SNE defines high-dimensional conditional similarities $p_{j \\mid i} \\propto \\exp\\!\\left(-\\lVert x_i - x_j \\rVert^2 / (2 \\sigma_i^2)\\right)$ with $\\sigma_i$ chosen so that the Shannon entropy of $p_{\\cdot \\mid i}$ equals a target perplexity; the embedding minimizes a Kullback–Leibler divergence between high-dimensional $p_{ij}$ and low-dimensional $q_{ij}$, producing attractive forces weighted by $p_{ij}$ and repulsive forces weighted by $q_{ij}$. Uniform Manifold Approximation and Projection (UMAP) constructs a fuzzy simplicial set by defining neighbor radii $\\rho_i$ and smooth scales $\\sigma_i$ from $k=n_{\\text{neighbors}}$ nearest neighbors, symmetrizes memberships to form a high-dimensional fuzzy graph, and learns an embedding by minimizing cross-entropy between high- and low-dimensional fuzzy sets; the parameter $\\text{min\\_dist}$ shapes how strongly small low-dimensional distances increase membership, thereby controlling how tightly points are permitted to pack. Multi-scale constructions in each method combine neighborhoods at multiple scales to balance local and global structure.\n\nWhich combined hyperparameter strategy best mitigates over-clumping in this mixed-density dataset while justifying the use of different $n_{\\text{neighbors}}$ strata?\n\nA. Use UMAP with a single neighborhood scale $n_{\\text{neighbors}}=15$, $\\text{min\\_dist}=0.01$, and $\\text{set\\_op\\_mix\\_ratio}=0.0$. Use t-SNE with a single perplexity of $30$ and $\\text{early exaggeration}=12$.\n\nB. Use multi-scale UMAP by aggregating fuzzy graphs from $n_{\\text{neighbors}} \\in \\{15, 60, 200\\}$, set $\\text{min\\_dist}=0.3$, and $\\text{set\\_op\\_mix\\_ratio}=0.7$ to balance union and intersection. Use multi-scale t-SNE by averaging similarities from perplexities $\\{30, 120\\}$, set $\\text{early exaggeration}=8$, and learning rate $\\eta = N/12$.\n\nC. Use UMAP with $n_{\\text{neighbors}}=200$, $\\text{min\\_dist}=0.0$, and $\\text{set\\_op\\_mix\\_ratio}=1.0$. Use t-SNE with perplexity $5$ and $\\text{early exaggeration}=36$.\n\nD. Use UMAP with $n_{\\text{neighbors}}=5$, $\\text{min\\_dist}=0.5$, and $\\text{local\\_connectivity}=1$. Use t-SNE with perplexity $200$ and $\\text{early exaggeration}=4$.\n\nSelect the best option and be prepared to justify your choice from the first-principles behavior of neighborhood probability distributions and fuzzy memberships, including why multiple $n_{\\text{neighbors}}$ strata are appropriate in this scenario of tight and diffuse clusters.",
            "solution": "### 1. Core Problem Analysis\nThe dataset contains structures at different scales: small, dense immune cell clusters and a broad, diffuse epithelial-to-mesenchymal transition (EMT) continuum. The primary objective is to select hyperparameters that can faithfully represent both structures simultaneously, avoiding \"over-clumping\" where the diffuse continuum is artificially compressed into a dense blob.\n\n**Key Principles for a Solution:**\n1.  **Multi-Scale Information:** A single neighborhood size (perplexity for t-SNE, `n_neighbors` for UMAP) represents a trade-off. A small neighborhood resolves local detail (the immune clusters) but can fragment global structures (the EMT continuum). A large neighborhood preserves global structure but can merge local clusters. To capture both, a multi-scale approach is optimal. This involves combining information from graphs constructed with different neighborhood sizes.\n2.  **Controlling Embedding Density:** The tendency to create artificially dense clusters must be actively managed. In UMAP, the `min_dist` parameter directly controls this: a value close to 0 encourages tight packing, while a larger value (e.g., 0.3-0.5) enforces more space between points in the embedding, better representing diffuse structures. In t-SNE, reducing the `early_exaggeration` factor can lessen the aggressive initial clumping, helping to preserve the shape of broader structures.\n\n### 2. Option-by-Option Analysis\n\n**A. Incorrect.** This strategy uses single, standard-default neighborhood sizes for both algorithms ($n_{\\text{neighbors}}=15$, perplexity=30). This is a single-scale compromise that will struggle with the mixed-scale data. Critically, the UMAP `min_dist=0.01` actively promotes the over-clumping that the problem seeks to avoid, as it allows points to be packed almost on top of each other.\n\n**B. Correct.** This strategy directly implements the key principles identified above.\n*   **UMAP:** It employs a multi-scale approach by aggregating graphs from a range of neighborhood sizes ($n_{\\text{neighbors}} \\in \\{15, 60, 200\\}$). The small value helps define the tight immune clusters, while the larger values help capture the global EMT continuum. The `min_dist=0.3` is a moderate value that prevents over-clumping and allows the diffuse EMT structure to be represented more faithfully. The `set_op_mix_ratio=0.7` balances the fuzzy union (good for connecting continua) and intersection (good for defining crisp clusters).\n*   **t-SNE:** It also uses a multi-scale approach by averaging similarities from two different perplexities (30 and 120), capturing both local and more global relationships. The reduced `early_exaggeration=8` mitigates the aggressive initial packing phase, which is beneficial for preserving the diffuse EMT structure. The learning rate is a standard, robust choice.\nThis combined strategy is sophisticated and tailored precisely to the problem's requirements.\n\n**C. Incorrect.** The choices here are counterproductive. For UMAP, `min_dist=0.0` will cause maximum over-clumping. A single large `n_neighbors=200` will likely merge the small immune clusters. For t-SNE, a tiny perplexity of 5 will fragment the EMT continuum, while an extremely high `early_exaggeration=36` will force everything into unnaturally tight blobs. This strategy would produce a highly distorted visualization.\n\n**D. Incorrect.** This option proposes two opposing single-scale strategies. UMAP with `n_neighbors=5` is hyper-local and will fail to capture the global EMT continuum. t-SNE with perplexity 200 is highly global and will fail to resolve the fine-grained immune clusters. Neither method is configured to handle both scales simultaneously, making this approach inferior to the multi-scale strategy in B.\n\n### 3. Justification for Multiple Strata\nThe justification for using multiple `n_neighbors` or perplexity strata is that real-world data, like the described tumor microenvironment, is multiscale. Different biological phenomena occur over different \"distances\" in gene expression space. The tight immune clusters are defined by very local neighborhoods, while the EMT continuum is a global structure. By building and combining representations from multiple neighborhood sizes, the algorithm can create a single visualization that respects both the local details and the overarching global topology, which is impossible with any single-scale choice. Option B is the only one that correctly implements this principle for both UMAP and t-SNE.",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "While t-SNE and UMAP excel at visualization, using their 2-D output directly for clustering is a common and critical pitfall. These methods can significantly distort the relative densities and sizes of clusters, leading to incorrect partitioning by standard algorithms that assume uniform variance. This problem illuminates the reasons for this instability and challenges you to identify more robust clustering strategies that respect the data's intrinsic density variations .",
            "id": "4590830",
            "problem": "A single-cell ribonucleic acid sequencing dataset comprises $n$ cells with $p$ gene features, and the goal is to visualize and cluster discrete cell types. A practitioner uses t-Distributed Stochastic Neighbor Embedding (t-SNE) to obtain a $2$-D embedding $\\{y_i\\}_{i=1}^n$ of the high-dimensional points $\\{x_i\\}_{i=1}^n$, where t-SNE is defined by high-dimensional conditional probabilities $p_{j \\mid i} \\propto \\exp\\!\\left(-\\frac{\\|x_i - x_j\\|^2}{2 \\sigma_i^2}\\right)$ with $\\sigma_i$ chosen to match a target perplexity $\\mathcal{P}$ via $\\mathrm{Perp}\\!\\left(P_i\\right) = 2^{H(P_i)} = \\mathcal{P}$, and low-dimensional joint probabilities $q_{ij} \\propto \\left(1 + \\|y_i - y_j\\|^2\\right)^{-1}$; the embedding minimizes the Kullback–Leibler Divergence (KLD) between $\\{p_{ij}\\}$ and $\\{q_{ij}\\}$. Alternatively, the practitioner uses Uniform Manifold Approximation and Projection (UMAP), which constructs fuzzy simplicial set weights $w_{ij}$ using local connectivity $\\rho_i$ and scale $\\sigma_i$ and optimizes a cross-entropy objective to preserve these fuzzy memberships. The practitioner then clusters in the embedding space and observes that dense regions fracture into many micro-clusters across different random seeds, while sparse regions are lumped together, producing inconsistent biological annotations.\n\nFrom first principles of neighbor graphs and density-sensitive distances, explain how variable local sampling density $\\rho(x)$ drives instability in post-embedding clustering. In particular, note that for a fixed $k$-nearest neighbor graph, the local neighbor radius $r_i$ that captures $k$ neighbors satisfies $r_i \\approx \\left(\\frac{k}{\\rho(x_i)}\\right)^{1/d}$ for ambient dimension $d$, so high-density regions (large $\\rho(x_i)$) have small $r_i$ and sparse regions have large $r_i$. When clustering is performed in a $2$-D embedding with algorithms that assume uniform variance or a single global density threshold, density heterogeneity can induce over-splitting of dense regions and under-splitting of sparse regions. Propose methods that correct for density and reduce this instability without discarding meaningful boundaries between cell types.\n\nWhich of the following strategies most directly and justifiably address post-embedding clustering instability due to variable density and avoid over-splitting artifacts, while preserving biologically meaningful separations? Select all that apply.\n\nA. Construct a shared-nearest-neighbor graph in the original high-dimensional space using adaptive local scales $\\{\\sigma_i\\}$, and cluster this graph with the Leiden algorithm; use the $2$-D embedding strictly for visualization rather than for clustering.\n\nB. Use the density-preserving variant of Uniform Manifold Approximation and Projection (densMAP) to add a density-matching term to the objective so that relative densities are preserved in the $2$-D embedding, and then cluster with Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN) using mutual reachability distance to accommodate variable density.\n\nC. Increase the neighborhood size in UMAP to a very large value and then apply Density-Based Spatial Clustering of Applications with Noise (DBSCAN) on the $2$-D embedding with a single global $\\varepsilon$ parameter chosen from the knee of the $k$-distance plot.\n\nD. Apply $k$-means directly in the $2$-D embedding with $k$ selected by the silhouette index, and trust that the t-SNE heavy-tailed kernel has already equalized density sufficiently to prevent over-splitting.\n\nE. Replace Euclidean distances with diffusion distances computed from the high-dimensional Markov transition matrix at timescale $t$ chosen to smooth local stochasticity, then perform spectral clustering (for example, normalized cuts) on the resulting affinity; use the embedding only for visualization.\n\nF. Normalize embedded coordinates by dividing each point’s coordinates by a kernel density estimate at that location to equalize density and then run any off-the-shelf clustering method on the transformed $2$-D space.",
            "solution": "### 1. Problem Validation\nThe problem is scientifically valid, well-posed, and objective. It describes a common pitfall in bioinformatics: naively clustering the output of visualization algorithms like t-SNE or UMAP. These algorithms are designed to preserve local neighborhood structure for visualization but, in doing so, often distort the global structure, including the relative densities and sizes of clusters. This leads to instability when applying clustering algorithms that are sensitive to density, such as k-means or DBSCAN with a single global epsilon. The question asks for robust strategies to mitigate this issue.\n\n### 2. Principled Derivation of Solution\nThe core issue is a mismatch between the output of the embedding algorithm and the assumptions of the clustering algorithm.\n-   **t-SNE and UMAP distort density:** By design, both algorithms tend to expand dense regions and contract sparse regions to create a visually pleasing layout where clusters have roughly similar sizes and densities in the 2D plot. This is an artifact; it does not reflect the true data distribution.\n-   **Clustering algorithm assumptions:** Many common clustering algorithms are sensitive to these distortions. For example, k-means assumes clusters are spherical and have similar variance. DBSCAN uses a single global density threshold ($\\varepsilon$ and MinPts), making it fail on data with variable-density clusters. Applying these to a t-SNE/UMAP embedding leads to over-splitting of originally dense clusters (which are now expanded) and under-splitting/lumping of originally sparse clusters (which are now contracted).\n\nA robust strategy must therefore do one of two things:\n1.  **Cluster before embedding:** Perform clustering on a representation of the original high-dimensional data that is robust to noise and density variations, and use the 2D embedding for visualization only.\n2.  **Use density-aware methods:** Use a combination of an embedding algorithm that preserves density and a clustering algorithm that can handle variable density.\n\n### 3. Analysis of Options\n\n**A. Correct.** This is a state-of-the-art approach in single-cell analysis. A Shared Nearest Neighbor (SNN) graph is constructed in the high-dimensional (or PCA-reduced) space. This graph is robust to noise and captures the manifold structure. Graph-based clustering algorithms like Leiden or Louvain are then used to partition this graph. This approach completely bypasses the density distortions of the 2D embedding for the purpose of clustering, using the embedding only for its intended purpose: visualization.\n\n**B. Correct.** This strategy addresses the problem at both the embedding and clustering stages. **densMAP** is a variant of UMAP specifically designed to preserve the relative local densities of the original data in the embedding. **HDBSCAN** is a hierarchical density-based clustering algorithm that, unlike DBSCAN, does not require a single global density parameter and can successfully identify clusters of varying densities. Combining these two density-aware methods provides a powerful and robust solution.\n\n**C. Incorrect.** This strategy is flawed. Using a very large neighborhood size in UMAP will cause it to focus only on global structure, potentially merging biologically distinct clusters. Applying DBSCAN with a single global $\\varepsilon$ is precisely the wrong approach for data with variable density, even if the embedding were perfect, as it would still over-split dense regions and miss sparse ones.\n\n**D. Incorrect.** This describes the exact pitfall the problem is about. Applying $k$-means to the 2D embedding is highly unreliable because t-SNE's density equalization creates clusters of uniform size and separation in the plot, which violates the underlying data structure and misleads algorithms like $k$-means that are sensitive to cluster shape and variance. The heavy-tailed kernel does not solve this problem for clustering; it creates it.\n\n**E. Correct.** This is another robust, manifold-based approach. It involves building a Markov transition matrix on the high-dimensional data and computing diffusion distances. This process denoises the data and captures the intrinsic geometric relationships between points. Spectral clustering on the resulting affinity matrix is a powerful way to partition the data based on this intrinsic structure. As with option A, this method performs clustering on a high-dimensional representation and uses the 2D embedding only for visualization.\n\n**F. Incorrect.** While conceptually interesting, this is a heuristic post-processing step that is less robust and justifiable than options A, B, and E. It attempts to \"undo\" the density distortion in the 2D space. However, the distortion is nonlinear and complex. This normalization can introduce its own artifacts and is not a principled way to recover the original data structure. The strategies in A, B, and E are more direct and sound as they either work in the original high-dimensional space or use tools explicitly designed for density preservation.\n\nTherefore, strategies A, B, and E represent the most direct, justifiable, and robust solutions to the problem of post-embedding clustering instability.",
            "answer": "$$\\boxed{ABE}$$"
        }
    ]
}