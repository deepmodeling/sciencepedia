{
    "hands_on_practices": [
        {
            "introduction": "随机森林的核心是其基础学习器——决策树。要真正理解随机森林，我们必须首先掌握单个决策树是如何构建的。本练习将引导你手动完成分类树（CART）的最初两步分裂过程，通过亲手计算来体会算法如何在众多可能性中选择最佳的分裂特征和阈值。这项实践旨在揭示基尼不纯度降低的计算细节，将抽象的算法步骤转化为具体的、可操作的计算，从而巩固你对决策树生长机制的根本理解。",
            "id": "4603324",
            "problem": "考虑一个生物信息学和医疗数据分析中的二元疾病状态预测任务，其中随机森林（Random Forest）基学习器使用分类与回归树（Classification And Regression Trees, CART）。给定一个包含 $n = 12$ 名患者的玩具数据集，每名患者有两个连续的生物标志物和一个二元结果。生物标志物是标准化的且无量纲。对于患者 $i$，数据集表示为 $(x^{(i)}_1, x^{(i)}_2, y^{(i)})$，其中 $y^{(i)} \\in \\{0, 1\\}$。数据如下：\n$(2.0, 8.0, 0)$, $(2.5, 7.5, 0)$, $(3.0, 7.0, 0)$, $(3.5, 6.8, 1)$, $(4.0, 6.5, 0)$, $(4.5, 6.2, 1)$, $(5.0, 5.8, 1)$, $(5.5, 5.3, 1)$, $(6.0, 4.9, 1)$, $(6.5, 4.6, 1)$, $(7.0, 4.0, 0)$, $(7.5, 3.6, 0)$.\n\n从适用于 CART 的第一性原理出发，以不纯度降低作为选择标准，手动构建该树的前两个二元分裂。在根节点处，选择在两个生物标志物的所有候选阈值上最大化不纯度降低的分裂。然后，在下一步中，选择不纯度非零的单个子节点，并再次通过在限制于该节点的两个生物标志物的所有候选阈值上最大化不纯度降低来选择其最佳分裂。通过明确的计算和有原则的推理来证明你的选择。\n\n最后，计算这两个分裂实现的总不纯度降低，该降低是从根节点的不纯度到第二次分裂后的不纯度来衡量的。将最终的不纯度降低表示为小数，并将答案四舍五入到四位有效数字。",
            "solution": "该问题要求为给定的二元分类数据集手动构建一个分类与回归树（CART）的前两个层级。分裂标准是最大化不纯度降低。我们将遵循标准的 CART 分类算法，该算法使用基尼不纯度（Gini impurity）作为节点不纯度的度量。\n\n首先，我们定义数据集 $S$ 的基尼不纯度。如果有 $K$ 个类别，且 $p_k$ 是 $S$ 中属于类别 $k$ 的项目所占的比例，则基尼不纯度由下式给出：\n$$I_G(S) = 1 - \\sum_{k=1}^{K} p_k^2$$\n对于我们的二元分类问题，类别为 $y=0$ 和 $y=1$，设 $p_0$ 为类别 $0$ 的比例，$p_1$ 为类别 $1$ 的比例。公式简化为：\n$$I_G(S) = 1 - (p_0^2 + p_1^2)$$\n对于一个父节点 $P$ 分裂成两个子节点（左节点 $L$ 和右节点 $R$），其大小分别为 $N_L$ 和 $N_R$（$N_P = N_L + N_R$），分裂的不纯度是子节点不纯度的加权平均值：\n$$I_G(L, R) = \\frac{N_L}{N_P} I_G(L) + \\frac{N_R}{N_P} I_G(R)$$\n不纯度降低，或称基尼增益（Gini gain），是父节点不纯度与子节点加权不纯度之差：\n$$\\Delta I_G = I_G(P) - I_G(L, R)$$\n最佳分裂是使该增益最大化的分裂。对于连续特征，候选分裂阈值通常选择为该特征排序后的连续唯一值之间的中点。一种优化方法是只考虑不同类别数据点之间的中点。\n\n给定的数据集有 $n=12$ 名患者：\n$(2.0, 8.0, 0)$, $(2.5, 7.5, 0)$, $(3.0, 7.0, 0)$, $(3.5, 6.8, 1)$, $(4.0, 6.5, 0)$, $(4.5, 6.2, 1)$, $(5.0, 5.8, 1)$, $(5.5, 5.3, 1)$, $(6.0, 4.9, 1)$, $(6.5, 4.6, 1)$, $(7.0, 4.0, 0)$, $(7.5, 3.6, 0)$.\n\n**步骤1：根节点分析与第一次分裂**\n\n根节点包含所有 $12$ 个数据点。我们计算每个类别中的患者数量：$N_0 = 6$（类别 $0$）和 $N_1 = 6$（类别 $1$）。\n比例为 $p_0 = \\frac{6}{12} = 0.5$ 和 $p_1 = \\frac{6}{12} = 0.5$。\n根节点的基尼不纯度为：\n$$I_G(\\text{root}) = 1 - (0.5^2 + 0.5^2) = 1 - (0.25 + 0.25) = 0.5$$\n\n现在，我们评估两个生物标志物 $x_1$ 和 $x_2$ 的候选分裂。\n\n**对生物标志物 $x_1$ 的分析：**\n$x_1$ 的唯一排序值为 $2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0, 5.5, 6.0, 6.5, 7.0, 7.5$。对应的类别标签为 $0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0$。\n我们测试类别标签发生变化的阈值。\n1.  **阈值 $t_1 = \\frac{3.0+3.5}{2} = 3.25$：**\n    - 左侧 ($x_1 \\le 3.25$)：$3$ 个点，均为类别 $0$。（$N_L=3, N_{L,0}=3, N_{L,1}=0$）。$I_G(L) = 1 - (1^2 + 0^2) = 0$。\n    - 右侧 ($x_1 > 3.25$)：$9$ 个点，$3$ 个属于类别 $0$，$6$ 个属于类别 $1$。（$N_R=9, N_{R,0}=3, N_{R,1}=6$）。$p_{R,0}=\\frac{3}{9}, p_{R,1}=\\frac{6}{9}$。$I_G(R) = 1 - ((\\frac{1}{3})^2 + (\\frac{2}{3})^2) = 1 - \\frac{5}{9} = \\frac{4}{9}$。\n    - 不纯度降低 $\\Delta I_G = 0.5 - (\\frac{3}{12}(0) + \\frac{9}{12}(\\frac{4}{9})) = 0.5 - \\frac{1}{3} = \\frac{1}{6} \\approx 0.1667$。\n\n2.  **阈值 $t_2 = \\frac{4.0+4.5}{2} = 4.25$：**\n    - 左侧 ($x_1 \\le 4.25$)：$5$ 个点，$4$ 个属于类别 $0$，$1$ 个属于类别 $1$。（$N_L=5, N_{L,0}=4, N_{L,1}=1$）。$I_G(L) = 1 - ((\\frac{4}{5})^2 + (\\frac{1}{5})^2) = 1 - \\frac{17}{25} = \\frac{8}{25}$。\n    - 右侧 ($x_1 > 4.25$)：$7$ 个点，$2$ 个属于类别 $0$，$5$ 个属于类别 $1$。（$N_R=7, N_{R,0}=2, N_{R,1}=5$）。$I_G(R) = 1 - ((\\frac{2}{7})^2 + (\\frac{5}{7})^2) = 1 - \\frac{29}{49} = \\frac{20}{49}$。\n    - $\\Delta I_G = 0.5 - (\\frac{5}{12}(\\frac{8}{25}) + \\frac{7}{12}(\\frac{20}{49})) = 0.5 - (\\frac{2}{15} + \\frac{5}{21}) = 0.5 - \\frac{14+25}{105} = 0.5 - \\frac{39}{105} = \\frac{1}{2} - \\frac{13}{35} = \\frac{9}{70} \\approx 0.1286$。\n\n3.  **阈值 $t_3 = \\frac{6.5+7.0}{2} = 6.75$：**\n    - 左侧 ($x_1 \\le 6.75$)：$10$ 个点，$4$ 个属于类别 $0$，$6$ 个属于类别 $1$。（$N_L=10, N_{L,0}=4, N_{L,1}=6$）。$I_G(L) = 1 - ((\\frac{4}{10})^2 + (\\frac{6}{10})^2) = 1 - \\frac{52}{100} = \\frac{12}{25}$。\n    - 右侧 ($x_1 > 6.75$)：$2$ 个点，均为类别 $0$。（$N_R=2, N_{R,0}=2, N_{R,1}=0$）。$I_G(R) = 0$。\n    - $\\Delta I_G = 0.5 - (\\frac{10}{12}(\\frac{12}{25}) + \\frac{2}{12}(0)) = 0.5 - \\frac{2}{5} = 0.1$。\n\n$x_1$ 的最佳分裂增益为 $\\frac{1}{6}$。\n\n**对生物标志物 $x_2$ 的分析：**\n$x_2$ 的唯一排序值对应的类别标签为 $0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0$。\n我们测试类别标签发生变化的阈值。\n1.  **阈值 $t_1 = \\frac{4.0+4.6}{2} = 4.3$：**\n    - 左侧 ($x_2 \\le 4.3$)：$2$ 个点，均为类别 $0$。$I_G(L) = 0$。\n    - 右侧 ($x_2 > 4.3$)：$10$ 个点，$4$ 个属于类别 $0$，$6$ 个属于类别 $1$。$I_G(R) = \\frac{12}{25}$。\n    - $\\Delta I_G = 0.5 - (\\frac{2}{12}(0) + \\frac{10}{12}(\\frac{12}{25})) = 0.5 - \\frac{2}{5} = 0.1$。\n\n2.  **阈值 $t_2 = \\frac{6.2+6.5}{2} = 6.35$：**\n    - 左侧 ($x_2 \\le 6.35$)：$7$ 个点，$2$ 个属于类别 $0$，$5$ 个属于类别 $1$。$I_G(L) = \\frac{20}{49}$。\n    - 右侧 ($x_2 > 6.35$)：$5$ 个点，$4$ 个属于类别 $0$，$1$ 个属于类别 $1$。$I_G(R) = \\frac{8}{25}$。\n    - $\\Delta I_G = 0.5 - (\\frac{7}{12}(\\frac{20}{49}) + \\frac{5}{12}(\\frac{8}{25})) = 0.5 - (\\frac{5}{21} + \\frac{2}{15}) = \\frac{9}{70} \\approx 0.1286$。\n\n$x_2$ 的最佳分裂增益为 $\\frac{9}{70}$。\n\n**第一次分裂的结论：**\n比较每个特征的最佳增益：$\\Delta I_G(x_1, t=3.25) = \\frac{1}{6}$ 和 $\\Delta I_G(x_2, t=6.35) = \\frac{9}{70}$。\n由于 $\\frac{1}{6} \\approx 0.1667$ 且 $\\frac{9}{70} \\approx 0.1286$，我们有 $\\frac{1}{6} > \\frac{9}{70}$。\n第一次分裂是在生物标志物 $x_1$ 上，阈值为 $t=3.25$。\n- 节点 L1 (左子节点, $x_1 \\le 3.25$)：$3$ 个点，均为类别 $0$。这是一个纯节点，不纯度 $I_G(L1)=0$。它成为一个叶节点。\n- 节点 R1 (右子节点, $x_1 > 3.25$)：$9$ 个点，$3$ 个属于类别 $0$，$6$ 个属于类别 $1$。这是一个不纯节点，不纯度 $I_G(R1)=\\frac{4}{9}$。\n\n**步骤2：第二次分裂**\n\n我们现在分裂不纯节点 R1。需要降低的不纯度为 $I_G(R1) = \\frac{4}{9} \\approx 0.4444$。节点 R1 中的数据为：\n$(3.5, 6.8, 1)$, $(4.0, 6.5, 0)$, $(4.5, 6.2, 1)$, $(5.0, 5.8, 1)$, $(5.5, 5.3, 1)$, $(6.0, 4.9, 1)$, $(6.5, 4.6, 1)$, $(7.0, 4.0, 0)$, $(7.5, 3.6, 0)$.\n\n**对节点 R1 中生物标志物 $x_1$ 的分析：**\n按 $x_1$ 排序的标签：$1, 0, 1, 1, 1, 1, 1, 0, 0$。\n1.  **阈值 $t_1 = \\frac{4.0+4.5}{2} = 4.25$：**\n    - 左侧：$2$ 个点 $(3.5, 1), (4.0, 0)$。$N_L=2, N_{L,0}=1, N_{L,1}=1$。$I_G(L) = 1 - (0.5^2+0.5^2) = 0.5$。\n    - 右侧：$7$ 个点，$2$ 个属于类别 $0$，$5$ 个属于类别 $1$。$I_G(R) = \\frac{20}{49}$。\n    - $\\Delta I_G(\\text{R1}) = \\frac{4}{9} - (\\frac{2}{9}(0.5) + \\frac{7}{9}(\\frac{20}{49})) = \\frac{4}{9} - (\\frac{1}{9} + \\frac{20}{63}) = \\frac{4}{9} - \\frac{27}{63} = \\frac{4}{9} - \\frac{3}{7} = \\frac{1}{63}$。\n\n2.  **阈值 $t_2 = \\frac{6.5+7.0}{2} = 6.75$：**\n    - 左侧：$7$ 个点，$1$ 个属于类别 $0$，$6$ 个属于类别 $1$。$I_G(L) = 1 - ((\\frac{1}{7})^2 + (\\frac{6}{7})^2) = \\frac{12}{49}$。\n    - 右侧：$2$ 个点，均为类别 $0$。$I_G(R) = 0$。\n    - $\\Delta I_G(\\text{R1}) = \\frac{4}{9} - (\\frac{7}{9}(\\frac{12}{49}) + \\frac{2}{9}(0)) = \\frac{4}{9} - \\frac{12}{63} = \\frac{4}{9} - \\frac{4}{21} = \\frac{28-12}{63} = \\frac{16}{63}$。\n\n在节点 R1 上对 $x_1$ 的最佳分裂增益为 $\\frac{16}{63}$。\n\n**对节点 R1 中生物标志物 $x_2$ 的分析：**\n按 $x_2$ 排序的标签：$0, 0, 1, 1, 1, 1, 1, 0, 1$。\n1.  **阈值 $t_1 = \\frac{4.0+4.6}{2} = 4.3$：**\n    - 左侧 ($x_2 \\le 4.3$)：$2$ 个点，均为类别 $0$。$I_G(L)=0$。\n    - 右侧 ($x_2 > 4.3$)：$7$ 个点，$1$ 个属于类别 $0$，$6$ 个属于类别 $1$。$I_G(R) = \\frac{12}{49}$。\n    - $\\Delta I_G(\\text{R1}) = \\frac{4}{9} - (\\frac{2}{9}(0) + \\frac{7}{9}(\\frac{12}{49})) = \\frac{4}{9} - \\frac{12}{63} = \\frac{16}{63}$。\n\n2. **阈值 $t_2 = \\frac{6.5+6.8}{2} = 6.65$：**\n    - 左侧 ($x_2 \\le 6.65$)：$8$ 个点，$3$ 个类别 $0$，$5$ 个类别 $1$。$I_G(L) = 1-((\\frac{3}{8})^2+(\\frac{5}{8})^2) = \\frac{30}{64}=\\frac{15}{32}$。\n    - 右侧 ($x_2 > 6.65$)：$1$ 个点，类别 $1$。$I_G(R)=0$。\n    - $\\Delta I_G(\\text{R1}) = \\frac{4}{9} - (\\frac{8}{9}(\\frac{15}{32}) + \\frac{1}{9}(0)) = \\frac{4}{9} - \\frac{15}{36} = \\frac{4}{9} - \\frac{5}{12} = \\frac{16-15}{36} = \\frac{1}{36}$。\n\n**第二次分裂的结论：**\n节点 R1 的最大不纯度降低为 $\\frac{16}{63} \\approx 0.2540$。这个增益可以通过两个不同的分裂实现：($x_1 \\le 6.75$) 和 ($x_2 \\le 4.3$)。在出现平局的情况下，标准实现可能会根据预定义的顺序（例如，特征索引）进行选择。我们将任意选择对 $x_2$ 的分裂，因为它创建了一个纯节点。第二次分裂是在节点 R1 上，使用规则 $x_2 \\le 4.3$。\n\n**步骤3：总不纯度降低**\n\n总不纯度降低是根节点的初始不纯度与两次分裂后树的叶节点的最终加权不纯度之差。\n三个叶节点是：\n1.  第一次分裂产生的节点 L1 ($x_1 \\le 3.25$)：$N_{L1}=3$, $I_G(L1)=0$。\n2.  第二次分裂产生的节点 L2 ($x_1 > 3.25$ 且 $x_2 \\le 4.3$)：$N_{L2}=2$, $I_G(L2)=0$。\n3.  第二次分裂产生的节点 R2 ($x_1 > 3.25$ 且 $x_2 > 4.3$)：$N_{R2}=7$, $I_G(R2)=\\frac{12}{49}$。\n\n树的最终加权不纯度是：\n$$I_{final} = \\frac{N_{L1}}{N} I_G(L1) + \\frac{N_{L2}}{N} I_G(L2) + \\frac{N_{R2}}{N} I_G(R2)$$\n$$I_{final} = \\frac{3}{12}(0) + \\frac{2}{12}(0) + \\frac{7}{12}\\left(\\frac{12}{49}\\right) = \\frac{7 \\times 12}{12 \\times 49} = \\frac{7}{49} = \\frac{1}{7}$$\n总不纯度降低为：\n$$\\Delta I_{G, total} = I_G(\\text{root}) - I_{final} = 0.5 - \\frac{1}{7} = \\frac{1}{2} - \\frac{1}{7} = \\frac{7-2}{14} = \\frac{5}{14}$$\n将其转换为小数并四舍五入到四位有效数字：\n$$\\frac{5}{14} \\approx 0.3571428... \\approx 0.3571$$",
            "answer": "$$\\boxed{0.3571}$$"
        },
        {
            "introduction": "在掌握了分类树的构建后，我们转向随机森林的另一大应用领域：回归。回归树的分裂准则与分类树有所不同，它基于最小化平方误差损失。本练习要求你从第一性原理出发，推导一个关键结论：对于回归任务，最大化“方差缩减”等同于最小化子节点的残差平方和，并最终得出一个简洁的杂质度减少量的解析表达式。通过这个推导，你将深刻理解回归树分裂背后的数学原理，并认识到模型的优化目标如何直接转化为具体的算法实现。",
            "id": "4603260",
            "problem": "一项临床转化研究旨在通过随机森林框架中的集成决策树，利用多组学特征来预测连续的生物标志物浓度。每棵回归树选择分裂方式以最小化每个节点的平方误差损失。考虑一个包含从生物医学队列中抽取的生物标志物响应 $y_{1}, y_{2}, \\ldots, y_{n}$ 的单一节点。某个特征上的一个候选分裂将索引集划分为大小分别为 $n_{\\mathcal{L}}$ 和 $n_{\\mathcal{R}}$ 的不相交的左子集 $\\mathcal{L}$ 和右子集 $\\mathcal{R}$，使得 $n_{\\mathcal{L}} + n_{\\mathcal{R}} = n$。对于每个节点或子节点，预测值被限定为最小化平方误差和 (SSE) 的常数，且节点的不纯度被定义为该最小化的SSE。\n\n仅从平方误差损失的定义以及“对于给定的一组响应，最小化SSE的常数是样本均值”这一性质出发，推导以下两个结果：\n$\\,\\,\\,$(i) 对于使用平方误差损失的回归树，在所有候选分裂中，最优分裂是最大化总节点内方差减少量（等价于，最大化不纯度减少量）的分裂。\n$\\,\\,\\,$(ii) 由分裂引起的不纯度减少量 $\\Delta$ 的闭式解析表达式，该表达式用子节点均值 $\\bar{y}_{\\mathcal{L}}$ 和 $\\bar{y}_{\\mathcal{R}}$ 以及子节点大小 $n_{\\mathcal{L}}$ 和 $n_{\\mathcal{R}}$ 表示。\n\n你的最终答案必须是仅用 $n$、$n_{\\mathcal{L}}$、$n_{\\mathcal{R}}$、$\\bar{y}_{\\mathcal{L}}$ 和 $\\bar{y}_{\\mathcal{R}}$ 表示的关于 $\\Delta$ 的单一闭式表达式，其中 $n = n_{\\mathcal{L}} + n_{\\mathcal{R}}$。不需要进行数值计算或四舍五入。不要用百分比表示，也不要使用物理单位。",
            "solution": "该问题是有效的，因为它提出了一个统计学习理论中的标准、适定的推导，该推导基于回归树算法的既定原则。它是自洽的、客观的且在科学上是合理的。\n\n任务是推导关于使用平方误差损失的回归树的分裂准则的两个结果。设父节点 $P$ 中的 $n$ 个生物标志物响应的集合表示为 $\\{y_i\\}_{i=1}^n$。一次分裂将该集合划分为一个包含 $n_{\\mathcal{L}}$ 个响应的左子节点 $\\mathcal{L}$ 和一个包含 $n_{\\mathcal{R}}$ 个响应的右子节点 $\\mathcal{R}$，使得 $n_{\\mathcal{L}} + n_{\\mathcal{R}} = n$。\n\n首先，我们形式化平方误差和 (SSE) 和节点不纯度的概念。对于任意一个包含 $k$ 个响应的集合 $\\{z_j\\}_{j=1}^k$，常数预测值 $c$ 的SSE由下式给出：\n$$\nSSE(c) = \\sum_{j=1}^{k} (z_j - c)^2\n$$\n问题陈述，节点的预测值是最小化此SSE的常数 $c$。为找到这个最优常数，我们将SSE对 $c$ 求导，并令导数为零：\n$$\n\\frac{d(SSE)}{dc} = \\frac{d}{dc} \\sum_{j=1}^{k} (z_j - c)^2 = \\sum_{j=1}^{k} 2(z_j - c)(-1) = -2 \\sum_{j=1}^{k} (z_j - c) = 0\n$$\n$$\n\\sum_{j=1}^{k} z_j - \\sum_{j=1}^{k} c = 0 \\implies \\sum_{j=1}^{k} z_j - k c = 0 \\implies c = \\frac{1}{k} \\sum_{j=1}^{k} z_j = \\bar{z}\n$$\n这证实了给定的性质：样本均值 $\\bar{z}$ 是最小化SSE的常数预测值。\n\n节点的不纯度被定义为这个最小化的SSE。\n对于父节点 $P$，最优预测值是其样本均值 $\\bar{y}_P = \\frac{1}{n} \\sum_{i=1}^n y_i$。父节点的不纯度是：\n$$\nI(P) = \\sum_{i=1}^{n} (y_i - \\bar{y}_P)^2\n$$\n类似地，对于子节点 $\\mathcal{L}$ 和 $\\mathcal{R}$，它们各自的不纯度是：\n$$\nI(\\mathcal{L}) = \\sum_{i \\in \\mathcal{L}} (y_i - \\bar{y}_{\\mathcal{L}})^2 \\quad \\text{其中} \\quad \\bar{y}_{\\mathcal{L}} = \\frac{1}{n_{\\mathcal{L}}} \\sum_{i \\in \\mathcal{L}} y_i\n$$\n$$\nI(\\mathcal{R}) = \\sum_{i \\in \\mathcal{R}} (y_i - \\bar{y}_{\\mathcal{R}})^2 \\quad \\text{其中} \\quad \\bar{y}_{\\mathcal{R}} = \\frac{1}{n_{\\mathcal{R}}} \\sum_{i \\in \\mathcal{R}} y_i\n$$\n\n(i) 最优分裂准则的推导\n\n分裂的目标是创建比父节点更同质（即不纯度更低）的子节点。分裂后的总不纯度是所产生的子节点不纯度的总和：$I_{children} = I(\\mathcal{L}) + I(\\mathcal{R})$。回归树算法寻找将数据划分为集合 $\\mathcal{L}$ 和 $\\mathcal{R}$ 的分裂，以使这个总子节点不纯度最小化。\n\n不纯度的减少量，或称不纯度下降量 $\\Delta$，定义为父节点的不纯度减去子节点的总不纯度：\n$$\n\\Delta = I(P) - (I(\\mathcal{L}) + I(\\mathcal{R}))\n$$\n对于任何被评估的候选分裂，父节点 $P$ 及其不纯度 $I(P)$ 是固定的。因此，要最小化分裂后的不纯度 $I(\\mathcal{L}) + I(\\mathcal{R})$，就必须等价地最大化不纯度减少量 $\\Delta$。\n\n在此背景下，“总节点内方差”一词常与离均差平方和（Sum of Squared Errors from the mean）同义使用，因为 $I(P) = (n-1)s_P^2$，其中 $s_P^2$ 是样本方差。因此，$I(P)$ 是父节点的总节点内方差，而 $I(\\mathcal{L}) + I(\\mathcal{R})$ 是子节点的总节点内方差。所以最大化 $\\Delta$ 等价于最大化总节点内方差的减少量。这完成了第(i)部分的推导。\n\n(ii) 不纯度减少量 $\\Delta$ 的闭式表达式\n\n为了推导 $\\Delta$ 的闭式表达式，我们利用全方差公式，该公式在方差分析 (ANOVA) 中通常用平方和的形式表示。总平方和 ($SST$) 可以分解为组内平方和 ($SSW$) 与组间平方和 ($SSB$) 之和。\n在我们的符号体系中：\n$SST = I(P) = \\sum_{i=1}^{n} (y_i - \\bar{y}_P)^2$\n$SSW = I(\\mathcal{L}) + I(\\mathcal{R}) = \\sum_{i \\in \\mathcal{L}} (y_i - \\bar{y}_{\\mathcal{L}})^2 + \\sum_{i \\in \\mathcal{R}} (y_i - \\bar{y}_{\\mathcal{R}})^2$\n不纯度减少量为 $\\Delta = SST - SSW$。根据分解恒等式，这意味着 $\\Delta = SSB$。\n\n让我们推导 $SSB$。其一般形式是各组均值与总均值之差的平方和，并按组的大小进行加权。\n$$\nSSB = n_{\\mathcal{L}}(\\bar{y}_{\\mathcal{L}} - \\bar{y}_P)^2 + n_{\\mathcal{R}}(\\bar{y}_{\\mathcal{R}} - \\bar{y}_P)^2\n$$\n所以，我们有 $\\Delta = n_{\\mathcal{L}}(\\bar{y}_{\\mathcal{L}} - \\bar{y}_P)^2 + n_{\\mathcal{R}}(\\bar{y}_{\\mathcal{R}} - \\bar{y}_P)^2$。问题要求最终表达式用 $n, n_{\\mathcal{L}}, n_{\\mathcal{R}}, \\bar{y}_{\\mathcal{L}}$ 和 $\\bar{y}_{\\mathcal{R}}$ 表示，因此我们必须消去 $\\bar{y}_P$。\n\n总均值 $\\bar{y}_P$ 是子节点均值的加权平均：\n$$\n\\bar{y}_P = \\frac{1}{n} \\sum_{i=1}^n y_i = \\frac{1}{n} \\left(\\sum_{i \\in \\mathcal{L}} y_i + \\sum_{i \\in \\mathcal{R}} y_i\\right) = \\frac{n_{\\mathcal{L}}\\bar{y}_{\\mathcal{L}} + n_{\\mathcal{R}}\\bar{y}_{\\mathcal{R}}}{n}\n$$\n其中 $n = n_{\\mathcal{L}} + n_{\\mathcal{R}}$。\n\n在代入 $\\bar{y}_P$ 之前，我们可以先简化 $SSB$ 的表达式。\n$$\n\\Delta = n_{\\mathcal{L}}(\\bar{y}_{\\mathcal{L}}^2 - 2\\bar{y}_{\\mathcal{L}}\\bar{y}_P + \\bar{y}_P^2) + n_{\\mathcal{R}}(\\bar{y}_{\\mathcal{R}}^2 - 2\\bar{y}_{\\mathcal{R}}\\bar{y}_P + \\bar{y}_P^2)\n$$\n$$\n\\Delta = n_{\\mathcal{L}}\\bar{y}_{\\mathcal{L}}^2 + n_{\\mathcal{R}}\\bar{y}_{\\mathcal{R}}^2 - 2\\bar{y}_P(n_{\\mathcal{L}}\\bar{y}_{\\mathcal{L}} + n_{\\mathcal{R}}\\bar{y}_{\\mathcal{R}}) + (n_{\\mathcal{L}} + n_{\\mathcal{R}})\\bar{y}_P^2\n$$\n使用 $n\\bar{y}_P = n_{\\mathcal{L}}\\bar{y}_{\\mathcal{L}} + n_{\\mathcal{R}}\\bar{y}_{\\mathcal{R}}$ 和 $n = n_{\\mathcal{L}} + n_{\\mathcal{R}}$，上式变为：\n$$\n\\Delta = n_{\\mathcal{L}}\\bar{y}_{\\mathcal{L}}^2 + n_{\\mathcal{R}}\\bar{y}_{\\mathcal{R}}^2 - 2\\bar{y}_P(n\\bar{y}_P) + n\\bar{y}_P^2\n$$\n$$\n\\Delta = n_{\\mathcal{L}}\\bar{y}_{\\mathcal{L}}^2 + n_{\\mathcal{R}}\\bar{y}_{\\mathcal{R}}^2 - 2n\\bar{y}_P^2 + n\\bar{y}_P^2 = n_{\\mathcal{L}}\\bar{y}_{\\mathcal{L}}^2 + n_{\\mathcal{R}}\\bar{y}_{\\mathcal{R}}^2 - n\\bar{y}_P^2\n$$\n现在，我们代入 $\\bar{y}_P$ 的表达式：\n$$\n\\Delta = n_{\\mathcal{L}}\\bar{y}_{\\mathcal{L}}^2 + n_{\\mathcal{R}}\\bar{y}_{\\mathcal{R}}^2 - n \\left( \\frac{n_{\\mathcal{L}}\\bar{y}_{\\mathcal{L}} + n_{\\mathcal{R}}\\bar{y}_{\\mathcal{R}}}{n} \\right)^2\n$$\n$$\n\\Delta = n_{\\mathcal{L}}\\bar{y}_{\\mathcal{L}}^2 + n_{\\mathcal{R}}\\bar{y}_{\\mathcal{R}}^2 - \\frac{1}{n} (n_{\\mathcal{L}}\\bar{y}_{\\mathcal{L}} + n_{\\mathcal{R}}\\bar{y}_{\\mathcal{R}})^2\n$$\n$$\n\\Delta = n_{\\mathcal{L}}\\bar{y}_{\\mathcal{L}}^2 + n_{\\mathcal{R}}\\bar{y}_{\\mathcal{R}}^2 - \\frac{1}{n} (n_{\\mathcal{L}}^2\\bar{y}_{\\mathcal{L}}^2 + 2n_{\\mathcal{L}}n_{\\mathcal{R}}\\bar{y}_{\\mathcal{L}}\\bar{y}_{\\mathcal{R}} + n_{\\mathcal{R}}^2\\bar{y}_{\\mathcal{R}}^2)\n$$\n将所有项通分到共同分母 $n = n_{\\mathcal{L}} + n_{\\mathcal{R}}$ 下：\n$$\n\\Delta = \\frac{1}{n} \\left[ n(n_{\\mathcal{L}}\\bar{y}_{\\mathcal{L}}^2 + n_{\\mathcal{R}}\\bar{y}_{\\mathcal{R}}^2) - (n_{\\mathcal{L}}^2\\bar{y}_{\\mathcal{L}}^2 + 2n_{\\mathcal{L}}n_{\\mathcal{R}}\\bar{y}_{\\mathcal{L}}\\bar{y}_{\\mathcal{R}} + n_{\\mathcal{R}}^2\\bar{y}_{\\mathcal{R}}^2) \\right]\n$$\n$$\n\\Delta = \\frac{1}{n} \\left[ (n_{\\mathcal{L}}+n_{\\mathcal{R}})n_{\\mathcal{L}}\\bar{y}_{\\mathcal{L}}^2 + (n_{\\mathcal{L}}+n_{\\mathcal{R}})n_{\\mathcal{R}}\\bar{y}_{\\mathcal{R}}^2 - n_{\\mathcal{L}}^2\\bar{y}_{\\mathcal{L}}^2 - 2n_{\\mathcal{L}}n_{\\mathcal{R}}\\bar{y}_{\\mathcal{L}}\\bar{y}_{\\mathcal{R}} - n_{\\mathcal{R}}^2\\bar{y}_{\\mathcal{R}}^2 \\right]\n$$\n$$\n\\Delta = \\frac{1}{n} \\left[ (n_{\\mathcal{L}}^2\\bar{y}_{\\mathcal{L}}^2 + n_{\\mathcal{L}}n_{\\mathcal{R}}\\bar{y}_{\\mathcal{L}}^2) + (n_{\\mathcal{L}}n_{\\mathcal{R}}\\bar{y}_{\\mathcal{R}}^2 + n_{\\mathcal{R}}^2\\bar{y}_{\\mathcal{R}}^2) - n_{\\mathcal{L}}^2\\bar{y}_{\\mathcal{L}}^2 - 2n_{\\mathcal{L}}n_{\\mathcal{R}}\\bar{y}_{\\mathcal{L}}\\bar{y}_{\\mathcal{R}} - n_{\\mathcal{R}}^2\\bar{y}_{\\mathcal{R}}^2 \\right]\n$$\n消去项 ($n_{\\mathcal{L}}^2\\bar{y}_{\\mathcal{L}}^2$ 和 $n_{\\mathcal{R}}^2\\bar{y}_{\\mathcal{R}}^2$)，我们得到：\n$$\n\\Delta = \\frac{1}{n} \\left[ n_{\\mathcal{L}}n_{\\mathcal{R}}\\bar{y}_{\\mathcal{L}}^2 + n_{\\mathcal{L}}n_{\\mathcal{R}}\\bar{y}_{\\mathcal{R}}^2 - 2n_{\\mathcal{L}}n_{\\mathcal{R}}\\bar{y}_{\\mathcal{L}}\\bar{y}_{\\mathcal{R}} \\right]\n$$\n从分子中提出公因式 $n_{\\mathcal{L}}n_{\\mathcal{R}}$：\n$$\n\\Delta = \\frac{n_{\\mathcal{L}}n_{\\mathcal{R}}}{n} \\left[ \\bar{y}_{\\mathcal{L}}^2 - 2\\bar{y}_{\\mathcal{L}}\\bar{y}_{\\mathcal{R}} + \\bar{y}_{\\mathcal{R}}^2 \\right]\n$$\n括号中的项是一个完全平方式 $( \\bar{y}_{\\mathcal{L}} -  \\bar{y}_{\\mathcal{R}})^2$。这就得出了不纯度减少量的最终闭式表达式：\n$$\n\\Delta = \\frac{n_{\\mathcal{L}}n_{\\mathcal{R}}}{n} (\\bar{y}_{\\mathcal{L}} - \\bar{y}_{\\mathcal{R}})^2\n$$\n该表达式仅依赖于所要求的变量，并代表父节点总方差中可由该分裂解释的部分。",
            "answer": "$$\\boxed{\\frac{n_{\\mathcal{L}}n_{\\mathcal{R}}}{n}(\\bar{y}_{\\mathcal{L}} - \\bar{y}_{\\mathcal{R}})^{2}}$$"
        },
        {
            "introduction": "随机森林的威力远不止于分类和回归。其内部结构蕴含了样本之间丰富的“邻近”关系，可用于无监督学习任务，如异常检测。本练习将指导你实现一个基于随机森林邻近度的异常得分计算方法，用于在临床化学数据中识别非典型的患者画像。你将从零开始构建一个完整的分析流程，包括数据生成、模型训练、邻近度矩阵计算以及最终的异常检测，从而将理论知识转化为解决实际生物信息学问题的强大工具。",
            "id": "4603276",
            "problem": "您的任务是形式化并实现一种基于随机森林（RF）分类器所诱导的邻近结构的异常值检测方法，用于检测临床化学数据中的异常患者画像。您必须仅从基本定义出发，推导出一个基于邻近度的异常值度量，将其论证为训练集上的密度代理，然后设计一个评估方案，根据该度量将测试患者标记为异常。\n\n推导基础和定义：\n- 随机森林（RF）是在训练数据的自助法（bootstrap）复制样本上训练的一组决策树的集合。每棵树将特征空间划分为不相交的区域（叶节点）。在 RF 下，如果两个样本在多棵树中频繁落入同一个叶节点，则认为它们是相似的。\n- 定义任意两个样本之间的 RF 邻近度为它们落入同一终端叶节点的树所占的比例。\n- 某一点的邻近密度定义为该点到所有训练样本的邻近度平方的聚合，反映了训练数据在该点周围的集中程度。\n- 异常值得分定义为该邻近密度的倒数，因此稀疏区域对应于更大的异常值得分。\n\n数学公式：\n- 令 $T$ 为树的数量，令 $L_t(\\mathbf{x})$ 表示树 $t \\in \\{1,\\dots,T\\}$ 为特征向量为 $\\mathbf{x} \\in \\mathbb{R}^p$ 的样本分配的叶节点索引。\n- 对于训练样本 $i$ 和 $j$，定义邻近度\n$$\nP_{ij} \\;=\\; \\frac{1}{T}\\sum_{t=1}^T \\mathbf{1}\\!\\left\\{ L_t(\\mathbf{x}_i) \\,=\\, L_t(\\mathbf{x}_j)\\right\\},\n$$\n其中 $\\mathbf{1}\\{\\cdot\\}$ 是指示函数。\n- 对于一个样本 $\\mathbf{x}$（训练或测试样本），定义其相对于训练集 $\\{\\mathbf{x}_j\\}_{j=1}^n$ 的邻近密度为\n$$\nD(\\mathbf{x}) \\;=\\; \\sum_{j=1}^n \\left( \\frac{1}{T}\\sum_{t=1}^T \\mathbf{1}\\!\\left\\{ L_t(\\mathbf{x}) \\,=\\, L_t(\\mathbf{x}_j)\\right\\} \\right)^2.\n$$\n- 定义异常值得分\n$$\nO(\\mathbf{x}) \\;=\\; \\frac{1}{\\varepsilon + D(\\mathbf{x})},\n$$\n其中 $\\varepsilon>0$ 是一个用于保证数值稳定性的很小的数。较大的 $O(\\mathbf{x})$ 值表示相对于训练分布而言，该画像是异常的。\n\n用于评估的训练数据生成：\n- 考虑 $p = 8$ 个临床化学分析物，顺序如下：钠 $\\mathrm{[mmol/L]}$、钾 $\\mathrm{[mmol/L]}$、氯化物 $\\mathrm{[mmol/L]}$、碳酸氢盐 $\\mathrm{[mmol/L]}$、葡萄糖 $\\mathrm{[mg/dL]}$、肌酐 $\\mathrm{[mg/dL]}$、天冬氨酸转氨酶 (AST) $\\mathrm{[U/L]}$、丙氨酸转氨酶 (ALT) $\\mathrm{[U/L]}$。\n- 使用以下参数，从具有对角协方差的多元正态分布中独立生成 $n_\\text{healthy} = 300$ 个健康画像和 $n_\\text{disease} = 200$ 个疾病画像。所有随机抽样必须使用固定的种子 $s = 12345$ 以确保可复现性。\n- 健康组均值向量 $\\boldsymbol{\\mu}_H$ 和标准差 $\\boldsymbol{\\sigma}_H$：\n  - $\\boldsymbol{\\mu}_H = [140,\\, 4.2,\\, 103,\\, 24,\\, 90,\\, 0.9,\\, 22,\\, 21]$\n  - $\\boldsymbol{\\sigma}_H = [2,\\, 0.3,\\, 2,\\, 2,\\, 10,\\, 0.2,\\, 5,\\, 5]$\n- 疾病组均值向量 $\\boldsymbol{\\mu}_D$ 和标准差 $\\boldsymbol{\\sigma}_D$：\n  - $\\boldsymbol{\\mu}_D = [134,\\, 4.8,\\, 100,\\, 20,\\, 140,\\, 1.8,\\, 80,\\, 90]$\n  - $\\boldsymbol{\\sigma}_D = [4,\\, 0.5,\\, 3,\\, 3,\\, 25,\\, 0.5,\\, 30,\\, 35]$\n\n随机森林和邻近度规范：\n- 训练一个二元 RF 分类器以区分健康（$0$）与疾病（$1$）画像，使用以下固定的超参数：\n  - 树的数量 $T = 64$，\n  - 最大深度 $d_{\\max} = 8$，\n  - 每片叶子的最小样本数 $m_{\\min} = 5$，\n  - 每次分裂时考虑的特征数量 $m_{\\text{try}} = 3$，\n  - 在树构建阶段的节点处进行有放回的自助法抽样，使用相同的固定种子 $s = 12345$ 为整个训练流程提供种子。\n- 训练后，使用所有树和所有训练样本（不限于袋外样本），计算 $n \\times n$ 的训练邻近度矩阵 $P$，其元素 $P_{ij}$ 如上文所定义。\n\n异常值阈值：\n- 计算所有训练样本的异常值得分 $O(\\mathbf{x}_i)$，在密度计算中排除自身邻近度（即，在求和前将邻近度矩阵的对角线置零，以忽略 $D(\\mathbf{x}_i)$ 中的 $j=i$ 项）。\n- 令 $\\theta$ 为训练异常值得分在水平 $q = 0.95$ 处的经验分位数。当且仅当 $O(\\mathbf{x}) > \\theta$ 时，一个画像 $\\mathbf{x}$ 被标记为异常。\n\n测试套件：\n您必须评估以下 $4$ 个明确的测试患者（以与上述特征顺序相同的向量形式提供）。这些是确定性输入，而非抽样所得：\n\n- 测试 1（典型的类健康）：\n  - $\\mathbf{x}^{(1)} = [140,\\, 4.2,\\, 103,\\, 24,\\, 90,\\, 0.9,\\, 20,\\, 22]$。\n- 测试 2（临界代谢紊乱）：\n  - $\\mathbf{x}^{(2)} = [133,\\, 4.5,\\, 101,\\, 22,\\, 120,\\, 1.3,\\, 30,\\, 35]$。\n- 测试 3（显著的电解质失衡和类酸中毒）：\n  - $\\mathbf{x}^{(3)} = [120,\\, 2.5,\\, 85,\\, 12,\\, 180,\\, 1.0,\\, 25,\\, 25]$。\n- 测试 4（严重的类肝细胞损伤）：\n  - $\\mathbf{x}^{(4)} = [138,\\, 4.0,\\, 102,\\, 24,\\, 100,\\, 0.8,\\, 600,\\, 800]$。\n\n要求的输出：\n- 对于每个测试患者 $\\mathbf{x}^{(k)}$（$k \\in \\{1,2,3,4\\}$），计算其相对于训练集的异常值得分 $O(\\mathbf{x}^{(k)})$ 并与 $\\theta$ 进行比较。对每个患者，输出一个布尔值以指示其是否异常，即如果 $O(\\mathbf{x}^{(k)}) > \\theta$，则输出 $\\text{True}$，否则输出 $\\text{False}$。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，按 $k=1,2,3,4$ 的顺序排列结果，例如：$[\\text{True},\\text{False},\\text{True},\\text{True}]$。\n\n实现约束：\n- 您必须仅使用上述指定定义，从基本原理出发实现 RF 和邻近度计算。不允许使用外部机器学习库。\n- 您的代码必须是一个完整的、可运行的程序，确定性地执行以下步骤：数据生成、RF 训练、邻近度计算、阈值计算、测试患者评分以及按指定格式输出最终的布尔值。",
            "solution": "该问题要求使用随机森林（RF）邻近度为临床数据制定并实现一种异常值检测算法。从数据生成到最终评估的整个流程，都必须按照问题陈述中的定义从基本原理构建。\n\n### 问题陈述的验证\n问题陈述已经过严格验证，并被认为是有效的。它在科学上基于成熟的机器学习概念（基于随机森林的非参数密度估计），具有精确的数学定义，是适定的，并且其规范足够详细，可以得到唯一的、确定性的解。所有必需的参数、数据生成过程和评估指标都已提供。单一的随机种子（$s = 12345$）确保了可复现性。唯一轻微的模糊之处在于决策树分裂标准的选择，可以合理地假设其为分类任务中标准的基尼不纯度。数值稳定性常数 $\\varepsilon$ 的值也未指定；将使用一个标准的小值，如 $10^{-8}$。\n\n### 方法论框架\n\n核心原则是使用训练好的随机森林，不是为了分类，而是为了定义数据点之间的相似性度量，即“邻近度”。在 RF 中，如果两个数据点在多棵树中一致地落入同一个终端叶节点，则认为它们在模型划分的特征空间中非常接近。这个邻近度度量可以用来构建训练数据概率密度的一个代理。\n\n位于训练数据分布密集区域的点将与许多其他训练点具有高邻近度。相反，位于稀疏区域的异常值将与大多数训练点具有低邻近度。该算法将这种直觉形式化如下：\n\n1.  **邻近密度：** 对于任意点 $\\mathbf{x}$（无论来自训练集还是新的测试点），我们定义一个“邻近密度” $D(\\mathbf{x})$。通过对 $\\mathbf{x}$ 与训练集中所有点的邻近度的平方求和来计算。平方操作赋予了非常接近（高邻近度）的点更多的权重。\n2.  **异常值得分：** 异常值得分 $O(\\mathbf{x})$ 定义为邻近密度的倒数，即 $O(\\mathbf{x}) = 1 / (\\varepsilon + D(\\mathbf{x}))$。这种形式确保了稀疏区域中的点（低密度 $D(\\mathbf{x})$）会获得高的异常值得分。\n3.  **阈值设定：** 通过检查训练数据本身的异常值得分分布来确定一个异常值阈值 $\\theta$。选择这些得分的一个高分位数（具体为第 $95$ 百分位数）作为截止值。任何得分超过此阈值的新点都将被标记为“异常”画像。\n\n### 算法实现步骤\n\n该解决方案通过遵循以下步骤来确定性地实现：\n\n1.  **数据生成：**\n    *   用指定的种子 $s=12345$ 初始化一个随机数生成器。\n    *   从多元正态分布 $\\mathcal{N}(\\boldsymbol{\\mu}_H, \\text{diag}(\\boldsymbol{\\sigma}_H^2))$ 中抽取 $n_\\text{healthy} = 300$ 个“健康”画像，并标记为类别 $0$。\n    *   从 $\\mathcal{N}(\\boldsymbol{\\mu}_D, \\text{diag}(\\boldsymbol{\\sigma}_D^2))$ 中抽取 $n_\\text{disease} = 200$ 个“疾病”画像，并标记为类别 $1$。\n    *   这 $n = 500$ 个样本构成了训练数据集 $(\\mathbf{X}_{\\text{train}}, \\mathbf{y}_{\\text{train}})$。\n\n2.  **随机森林训练（从头开始）：**\n    *   构建一个包含 $T=64$ 棵决策树的随机森林。训练过程由单一的、带种子的随机数生成器控制。\n    *   对于 $T$ 棵树中的每一棵：\n        *   从训练集中有放回地抽取一个自助法样本。\n        *   在该自助法样本上生长一棵决策树，遵循超参数：最大深度 $d_{\\max}=8$，每片叶子的最小样本数 $m_{\\min}=5$，以及每次分裂时考虑的特征数量 $m_{\\text{try}}=3$。\n        *   每个节点使用的分裂标准是基尼不纯度的减少量，这是分类树的标准选择。通过评估 $m_{\\text{try}}$ 个随机选择的特征并为每个特征搜索最优阈值来找到最佳分裂点。\n    *   为每棵树中的每个唯一叶节点分配一个唯一的标识符。\n\n3.  **邻近度矩阵计算：**\n    *   训练后，将所有 $n=500$ 个训练样本穿过所有 $T=64$ 棵树，以确定它们落入的叶节点。这将产生一个 $500 \\times 64$ 的叶节点标识符矩阵。\n    *   计算 $n \\times n$ 的邻近度矩阵 $\\mathbf{P}$。元素 $P_{ij}$ 是样本 $\\mathbf{x}_i$ 和 $\\mathbf{x}_j$ 落入同一叶节点的树所占的比例：\n    $$ P_{ij} = \\frac{1}{T}\\sum_{t=1}^T \\mathbf{1}\\{ L_t(\\mathbf{x}_i) = L_t(\\mathbf{x}_j)\\} $$\n\n4.  **异常值阈值确定：**\n    *   计算每个训练样本 $\\mathbf{x}_i$ 的异常值得分。为防止样本被认为与自身平凡地接近，排除了自身邻近度项。这是通过将密度计算为 $D(\\mathbf{x}_i) = \\sum_{j \\neq i} P_{ij}^2$ 来实现的。\n    *   训练异常值得分计算为 $O(\\mathbf{x}_i) = 1/(\\varepsilon + D(\\mathbf{x}_i))$。\n    *   阈值 $\\theta$ 设置为这 $n$ 个训练异常值得分在 $q=0.95$ 处的经验分位数。\n\n5.  **评估测试患者：**\n    *   对于 $4$ 个指定的测试患者 $\\mathbf{x}^{(k)}$ 中的每一个：\n        *   将患者的特征向量穿过 $T$ 棵树，以获得其叶节点分配。\n        *   计算其与每个训练样本 $\\mathbf{x}_j$ 的邻近度，形成一个邻近度向量 $P(\\mathbf{x}^{(k)}, \\mathbf{x}_j)$。\n        *   通过对这些邻近度的平方求和来计算邻近密度：\n          $$ D(\\mathbf{x}^{(k)}) = \\sum_{j=1}^n \\left( P(\\mathbf{x}^{(k)}, \\mathbf{x}_j) \\right)^2 $$\n        *   计算异常值得分 $O(\\mathbf{x}^{(k)})$。\n        *   将该得分与阈值 $\\theta$进行比较。如果 $O(\\mathbf{x}^{(k)}) > \\theta$ 则结果为 `True`，否则为 `False`。\n\n这一系列操作产生一个确定性的布尔标志列表，指示哪些测试画像相对于学习到的训练数据分布被认为是异常的。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and evaluates a Random Forest proximity-based outlier detection method.\n    \"\"\"\n    # -------------------\n    # Configuration\n    # -------------------\n    # Data generation parameters\n    P_FEATURES = 8\n    N_HEALTHY = 300\n    N_DISEASE = 200\n    MU_H = np.array([140., 4.2, 103., 24., 90., 0.9, 22., 21.])\n    SIGMA_H = np.array([2., 0.3, 2., 2., 10., 0.2, 5., 5.])\n    MU_D = np.array([134., 4.8, 100., 20., 140., 1.8, 80., 90.])\n    SIGMA_D = np.array([4., 0.5, 3., 3., 25., 0.5, 30., 35.])\n    \n    # RF hyperparameters\n    T_TREES = 64\n    D_MAX = 8\n    M_MIN = 5\n    M_TRY = 3\n    \n    # Outlier detection parameters\n    Q_THRESHOLD = 0.95\n    EPSILON = 1e-8\n    \n    # Random seed\n    SEED = 12345\n    \n    # Test cases\n    test_cases = [\n        np.array([140., 4.2, 103., 24., 90., 0.9, 20., 22.]), # Test 1\n        np.array([133., 4.5, 101., 22., 120., 1.3, 30., 35.]), # Test 2\n        np.array([120., 2.5, 85., 12., 180., 1.0, 25., 25.]), # Test 3\n        np.array([138., 4.0, 102., 24., 100., 0.8, 600., 800.]),# Test 4\n    ]\n    \n    # Initialize random number generator for reproducibility\n    rng = np.random.default_rng(SEED)\n\n    # -------------------\n    # Helper Classes for RF\n    # -------------------\n    class Node:\n        def __init__(self, feature_index=None, threshold=None, left=None, right=None, *, value=None, leaf_id=None):\n            self.feature_index = feature_index\n            self.threshold = threshold\n            self.left = left\n            self.right = right\n            self.value = value  # Majority class if leaf node\n            self.leaf_id = leaf_id # Unique ID if leaf node\n\n    class DecisionTree:\n        def __init__(self, rng_instance, max_depth, min_samples_leaf, m_try):\n            self.rng = rng_instance\n            self.max_depth = max_depth\n            self.min_samples_leaf = min_samples_leaf\n            self.m_try = m_try\n            self.root = None\n            self.leaf_counter = 0\n\n        def _gini(self, y):\n            if y.size == 0:\n                return 0\n            _, counts = np.unique(y, return_counts=True)\n            probas = counts / y.size\n            return 1 - np.sum(probas**2)\n\n        def _find_best_split(self, X, y):\n            n_samples, n_features = X.shape\n            current_gini = self._gini(y)\n            best_gain = -1.0\n            best_feat, best_thresh = None, None\n            \n            feat_idxs = self.rng.choice(n_features, self.m_try, replace=False)\n\n            for feat_idx in feat_idxs:\n                thresholds = np.unique(X[:, feat_idx])\n                if thresholds.size > 1:\n                    thresholds = (thresholds[:-1] + thresholds[1:]) / 2.0\n                else: \n                    continue # Cannot split on a single value\n                \n                for thresh in thresholds:\n                    left_idxs = np.where(X[:, feat_idx] = thresh)[0]\n                    right_idxs = np.where(X[:, feat_idx] > thresh)[0]\n                    \n                    if left_idxs.size == 0 or right_idxs.size == 0:\n                        continue\n                    \n                    y_left, y_right = y[left_idxs], y[right_idxs]\n                    \n                    p_left = left_idxs.size / n_samples\n                    p_right = right_idxs.size / n_samples\n                    \n                    weighted_gini = p_left * self._gini(y_left) + p_right * self._gini(y_right)\n                    gain = current_gini - weighted_gini\n                    \n                    if gain > best_gain:\n                        best_gain = gain\n                        best_feat = feat_idx\n                        best_thresh = thresh\n            \n            return best_feat, best_thresh\n\n        def _build_tree(self, X, y, depth):\n            n_samples = y.size\n            \n            # Stopping criteria\n            if (depth >= self.max_depth or\n                n_samples  self.min_samples_leaf or\n                np.unique(y).size == 1):\n                leaf_value = np.bincount(y).argmax()\n                self.leaf_counter += 1\n                return Node(value=leaf_value, leaf_id=self.leaf_counter)\n\n            feat_idx, threshold = self._find_best_split(X, y)\n            \n            if feat_idx is None: # No beneficial split found\n                leaf_value = np.bincount(y).argmax()\n                self.leaf_counter += 1\n                return Node(value=leaf_value, leaf_id=self.leaf_counter)\n\n            left_idxs = np.where(X[:, feat_idx] = threshold)[0]\n            right_idxs = np.where(X[:, feat_idx] > threshold)[0]\n            \n            left_child = self._build_tree(X[left_idxs, :], y[left_idxs], depth + 1)\n            right_child = self._build_tree(X[right_idxs, :], y[right_idxs], depth + 1)\n            \n            return Node(feature_index=feat_idx, threshold=threshold, left=left_child, right=right_child)\n\n        def fit(self, X, y):\n            self.root = self._build_tree(X, y, depth=0)\n        \n        def _get_leaf_id(self, x, node):\n            if node.leaf_id is not None:\n                return node.leaf_id\n            if x[node.feature_index] = node.threshold:\n                return self._get_leaf_id(x, node.left)\n            return self._get_leaf_id(x, node.right)\n            \n        def get_leaf_for_sample(self, x):\n            return self._get_leaf_id(x, self.root)\n\n    class RandomForest:\n        def __init__(self, rng_instance, n_trees, max_depth, min_samples_leaf, m_try):\n            self.rng = rng_instance\n            self.n_trees = n_trees\n            self.max_depth = max_depth\n            self.min_samples_leaf = min_samples_leaf\n            self.m_try = m_try\n            self.trees = []\n\n        def fit(self, X, y):\n            n_samples = X.shape[0]\n            total_leaf_offset = 0\n            for _ in range(self.n_trees):\n                idxs = self.rng.choice(n_samples, n_samples, replace=True)\n                X_boot, y_boot = X[idxs], y[idxs]\n                \n                tree = DecisionTree(self.rng, self.max_depth, self.min_samples_leaf, self.m_try)\n                tree.fit(X_boot, y_boot)\n                \n                # Make leaf IDs unique across all trees in the forest\n                tree.leaf_counter += total_leaf_offset\n                \n                # Python passes objects by reference, so we need to traverse and update\n                nodes_to_visit = [tree.root]\n                while nodes_to_visit:\n                    node = nodes_to_visit.pop(0)\n                    if node:\n                        if node.leaf_id is not None:\n                            node.leaf_id += total_leaf_offset\n                        nodes_to_visit.append(node.left)\n                        nodes_to_visit.append(node.right)\n\n                self.trees.append(tree)\n                total_leaf_offset = tree.leaf_counter\n\n        def get_leaf_indices(self, X_eval):\n            n_eval = X_eval.shape[0]\n            leaf_indices = np.zeros((n_eval, self.n_trees), dtype=int)\n            for i, x in enumerate(X_eval):\n                for t_idx, tree in enumerate(self.trees):\n                    leaf_indices[i, t_idx] = tree.get_leaf_for_sample(x)\n            return leaf_indices\n\n    # -------------------\n    # Main Logic\n    # -------------------\n    # 1. Generate Data\n    X_healthy = rng.normal(loc=MU_H, scale=SIGMA_H, size=(N_HEALTHY, P_FEATURES))\n    X_disease = rng.normal(loc=MU_D, scale=SIGMA_D, size=(N_DISEASE, P_FEATURES))\n    X_train = np.vstack((X_healthy, X_disease))\n    y_train = np.hstack((np.zeros(N_HEALTHY, dtype=int), np.ones(N_DISEASE, dtype=int)))\n    n_train = X_train.shape[0]\n\n    # 2. Train Random Forest\n    rf = RandomForest(rng, T_TREES, D_MAX, M_MIN, M_TRY)\n    rf.fit(X_train, y_train)\n\n    # 3. Compute training proximity matrix\n    train_leaf_indices = rf.get_leaf_indices(X_train)\n    prox_matrix = np.zeros((n_train, n_train))\n    for i in range(n_train):\n        for j in range(i, n_train):\n            proximity = np.sum(train_leaf_indices[i, :] == train_leaf_indices[j, :]) / T_TREES\n            prox_matrix[i, j] = proximity\n            prox_matrix[j, i] = proximity\n            \n    # 4. Calculate outlier threshold\n    prox_matrix_no_diag = prox_matrix.copy()\n    np.fill_diagonal(prox_matrix_no_diag, 0)\n    \n    D_train = np.sum(np.square(prox_matrix_no_diag), axis=1)\n    O_train = 1 / (EPSILON + D_train)\n    theta = np.quantile(O_train, Q_THRESHOLD)\n\n    # 5. Evaluate test patients\n    results = []\n    X_test = np.array(test_cases)\n    test_leaf_indices = rf.get_leaf_indices(X_test)\n    \n    for k in range(len(test_cases)):\n        proximities_to_train = np.sum(test_leaf_indices[k, :] == train_leaf_indices, axis=1) / T_TREES\n        D_test = np.sum(np.square(proximities_to_train))\n        O_test = 1 / (EPSILON + D_test)\n        results.append(O_test > theta)\n\n    # Final print statement\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}