{
    "hands_on_practices": [
        {
            "introduction": "有效的特征选择不仅仅是寻找与目标变量高度相关的特征，管理所选特征之间的冗余也至关重要。本练习通过一个具体的计算任务，帮助您理解最小冗余最大相关性（mRMR）原则。该原则旨在量化特征预测能力（相关性）与已选特征集信息重叠度（冗余性）之间的权衡 。",
            "id": "4573646",
            "problem": "一个转化肿瘤学联盟正在验证一个由三个离散化生物标志物特征 $\\{X_1, X_2, X_3\\}$ 组成的组合，用于预测一个二元疾病状态 $Y \\in \\{0,1\\}$，其中 $Y=1$ 表示患病，$Y=0$ 表示健康。所有变量都是二元的，并且来自一个明确定义的生成模型，该模型与生物信息学和医疗数据分析中使用的经验性预处理方法相一致。假设以下基本统计事实成立：$Y$ 的分布是均衡的，在给定 $y$ 的条件下，条件分布 $p(x_1 \\mid y)$ 和 $p(x_3 \\mid y)$ 是独立抽样，并且一个冗余的检测导致 $X_2$ 确定性地等于 $X_1$。\n\n测量过程总结如下：\n- $p(Y=1) = p(Y=0) = 1/2$，\n- $p(X_1=1 \\mid Y=1) = 9/10$ 且 $p(X_1=1 \\mid Y=0) = 1/10$，\n- $X_2 = X_1$（确定性关系），\n- $p(X_3=1 \\mid Y=1) = 4/5$ 且 $p(X_3=1 \\mid Y=0) = 1/5$，\n且在给定 $Y$ 的条件下，$X_1$ 和 $X_3$ 条件独立。\n\n从信息论和概率论的第一性原理出发，计算当 $j \\in \\{1,2,3\\}$ 时的互信息 $I(X_j;Y)$，以及当 $j \\in \\{2,3\\}$ 且 $k=1$ 时的成对冗余度 $I(X_j;X_k)$。然后，使用当前已选择的集合 $S=\\{X_1\\}$，执行最小冗余最大相关性 (mRMR) 差分变体的单次选择迭代，即在 $\\{2,3\\}$ 中选择特征索引 $j$，使其相对于集合 $S$ 的“相关性减去冗余度”最大化。使用自然对数 $\\ln$，并以奈特 (nats) 为单位解释信息值。将此次迭代中选择的特征索引 $j^{\\ast}$ 报告为单个整数。无需四舍五入。将最终答案表示为不带单位的整数。",
            "solution": "对问题陈述的有效性进行评估。\n\n### 第 1 步：提取已知条件\n- 特征集：$\\{X_1, X_2, X_3\\}$，均为二元变量。\n- 目标变量：$Y \\in \\{0, 1\\}$，一个表示疾病状态的二元变量。\n- 目标变量的先验概率：$p(Y=1) = p(Y=0) = \\frac{1}{2}$。\n- $X_1$ 的条件概率：$p(X_1=1 \\mid Y=1) = \\frac{9}{10}$ 且 $p(X_1=1 \\mid Y=0) = \\frac{1}{10}$。\n- 确定性关系：$X_2 = X_1$。\n- $X_3$ 的条件概率：$p(X_3=1 \\mid Y=1) = \\frac{4}{5}$ 且 $p(X_3=1 \\mid Y=0) = \\frac{1}{5}$。\n- 条件独立性：给定 $Y$ 时，$X_1$ 和 $X_3$ 条件独立，记为 $X_1 \\perp X_3 \\mid Y$。\n- 任务：执行最小冗余最大相关性 (mRMR) 特征选择的差分变体的单次迭代。\n- 初始已选集合：$S = \\{X_1\\}$。\n- 候选特征：$\\{X_2, X_3\\}$。\n- 选择标准：最大化得分 $D_j = I(X_j;Y) - R_j$，其中 $j \\in \\{2,3\\}$，$I(X_j;Y)$ 是相关性，$R_j = I(X_j;X_1)$ 是相对于当前已选特征 $X_1$ 的冗余度。\n- 对数：自然对数 ($\\ln$)，信息量以奈特 (nats) 为单位。\n- 最终输出：报告所选特征的索引 $j^{\\ast}$。\n\n### 第 2 步：使用提取的已知条件进行验证\n- **科学依据：** 该问题使用了信息论（互信息、熵）和机器学习（mRMR 特征选择）中的标准、明确定义的概念。这些是生物信息学和数据分析中的核心方法。\n- **适定性：** 提供了所有必要的概率和变量之间的关系。特征选择的目标函数有明确定义。该问题是自洽的，其结构保证了唯一解的存在。\n- **客观性：** 问题使用形式化的数学和统计语言陈述，没有歧义或主观论断。\n- **一致性与完备性：** 所提供的概率是一致的（例如，$p(X_1=0|Y=1) = 1 - p(X_1=1|Y=1) = \\frac{1}{10}$）。设置是完备的，足以进行所需的计算。确定性关系 $X_2=X_1$ 模拟了完全冗余，而条件独立性 $X_1 \\perp X_3 \\mid Y$ 模拟了一种常见情景，即生物标志物在以疾病状态为条件下是独立的测量。\n\n### 第 3 步：结论与行动\n该问题是有效的。这是一个将信息论原理应用于特征选择任务的明确定义的练习。将提供完整解答。\n\n目标是从候选集 $\\{2,3\\}$ 中选择使 mRMR 得分最大化的特征索引 $j^{\\ast}$：\n$$j^{\\ast} = \\arg\\max_{j \\in \\{2,3\\}} \\left( I(X_j; Y) - I(X_j; X_1) \\right)$$\n其中 $I(X_j;Y)$ 是特征 $X_j$ 的相关性，$I(X_j;X_1)$ 是其与已选特征 $X_1$ 的冗余度。我们将分别评估 $j=2$ 和 $j=3$ 时的得分。\n\n所有信息论量均使用自然对数定义。随机变量 $Z$ 的熵为 $H(Z) = - \\sum_z p(z)\\ln p(z)$。两个变量 $A$ 和 $B$ 之间的互信息为 $I(A;B) = H(A) - H(A|B)$。\n\n**对特征 $X_2$ 的分析**\n\n$X_2$ 的得分为 $D_2 = I(X_2; Y) - I(X_2; X_1)$。\n\n1.  **相关性 $I(X_2; Y)$:** 由于 $X_2 = X_1$ 是确定性的，因此 $X_2$ 和 $X_1$ 在信息论上相对于任何其他变量都是等价的。因此，$I(X_2; Y) = I(X_1; Y)$。\n\n2.  **冗余度 $I(X_2; X_1)$:** 一个变量与其确定性副本之间的互信息就是该变量的熵。\n    $I(X_2; X_1) = H(X_1) - H(X_1|X_2)$。由于 $X_1$ 完全由 $X_2$ 确定，条件熵 $H(X_1|X_2) = 0$。因此，$I(X_2; X_1) = H(X_1)$。\n\n为了计算 $H(X_1)$，我们首先需要 $X_1$ 的边缘概率分布。\n$$p(X_1=1) = p(X_1=1|Y=1)p(Y=1) + p(X_1=1|Y=0)p(Y=0)$$\n$$p(X_1=1) = \\left(\\frac{9}{10}\\right)\\left(\\frac{1}{2}\\right) + \\left(\\frac{1}{10}\\right)\\left(\\frac{1}{2}\\right) = \\frac{9}{20} + \\frac{1}{20} = \\frac{10}{20} = \\frac{1}{2}$$\n由于 $X_1$ 是二元的，$p(X_1=0) = 1 - \\frac{1}{2} = \\frac{1}{2}$。该分布是均衡的。其熵为：\n$$H(X_1) = -\\left(\\frac{1}{2}\\ln\\frac{1}{2} + \\frac{1}{2}\\ln\\frac{1}{2}\\right) = -\\ln\\frac{1}{2} = \\ln(2)$$\n所以，冗余度为 $I(X_2; X_1) = \\ln(2)$。\n\n3.  **mRMR 得分 $D_2$:**\n    $$D_2 = I(X_1; Y) - H(X_1)$$\n    使用恒等式 $I(X_1; Y) = H(X_1) - H(X_1|Y)$，我们将其代入 $D_2$ 的表达式中：\n    $$D_2 = \\left(H(X_1) - H(X_1|Y)\\right) - H(X_1) = -H(X_1|Y)$$\n    熵总是非负的，$H(X_1|Y) \\ge 0$。它仅在 $X_1$ 是 $Y$ 的确定性函数时为零。鉴于概率 $p(X_1|Y) \\in \\{\\frac{1}{10}, \\frac{9}{10}\\}$，情况并非如此。因此，$H(X_1|Y) > 0$，这意味着 $D_2  0$。这个负分反映了添加一个完全冗余的特征会受到惩罚这一事实。\n\n**对特征 $X_3$ 的分析**\n\n$X_3$ 的得分为 $D_3 = I(X_3; Y) - I(X_3; X_1)$。\n\n我们可以使用信息论恒等式和问题的条件独立性假设 ($X_1 \\perp X_3 \\mid Y$) 来简化这个表达式。互信息的链式法则指出：\n$$I(A; B,C) = I(A; C) + I(A; B|C)$$\n令 $A=X_3$，$B=Y$，$C=X_1$。\n$$I(X_3; Y, X_1) = I(X_3; X_1) + I(X_3; Y|X_1)$$\n对 $I(X_3; Y|X_1)$ 进行整理：\n$$I(X_3; Y|X_1) = I(X_3; Y, X_1) - I(X_3; X_1)$$\n我们也可以将链式法则写为 $I(A; B,C) = I(A;B) + I(A;C|B)$。将其应用于 $I(X_3; Y, X_1)$：\n$$I(X_3; Y, X_1) = I(X_3; Y) + I(X_3; X_1|Y)$$\n问题陈述 $X_1 \\perp X_3 \\mid Y$，这意味着它们的条件互信息为零：$I(X_3; X_1|Y)=0$。\n因此，$I(X_3; Y, X_1) = I(X_3; Y)$。\n将此代回 $I(X_3; Y|X_1)$ 的表达式中：\n$$I(X_3; Y|X_1) = I(X_3; Y) - I(X_3; X_1)$$\n这正是 mRMR 得分 $D_3$ 的表达式。所以，$D_3 = I(X_3; Y|X_1)$。\n\n量 $I(X_3; Y|X_1)$ 表示在已知 $X_1$ 的情况下，$X_3$ 为目标 $Y$ 提供的额外信息。\n\n**得分比较**\n\n我们必须比较 $D_2$ 和 $D_3$：\n- $D_2 = -H(X_1|Y)$\n- $D_3 = I(X_3; Y|X_1)$\n\n我们已经确定 $D_2  0$。\n互信息总是非负的，所以 $D_3 = I(X_3; Y|X_1) \\ge 0$。\n\n等式 $D_3=0$ 成立当且仅当 $X_3 \\perp Y \\mid X_1$。我们可以检验这个条件。\n如果对于所有值都有 $p(Y|X_1, X_3) = p(Y|X_1)$，则 $X_3 \\perp Y \\mid X_1$。\n让我们检查 $Y=1, X_1=1, X_3=1$ 的情况。\n使用贝叶斯法则，$p(Y=1|X_1=1) = \\frac{p(X_1=1|Y=1)p(Y=1)}{p(X_1=1)} = \\frac{(\\frac{9}{10})(\\frac{1}{2})}{\\frac{1}{2}} = \\frac{9}{10}$。\n而 $p(Y=1|X_1=1, X_3=1) = \\frac{p(X_1=1, X_3=1|Y=1)p(Y=1)}{p(X_1=1, X_3=1)}$。\n- 分子：$p(X_1=1, X_3=1|Y=1)p(Y=1) = p(X_1=1|Y=1)p(X_3=1|Y=1)p(Y=1) = (\\frac{9}{10})(\\frac{4}{5})(\\frac{1}{2}) = \\frac{36}{100}$。\n- 分母：$p(X_1=1, X_3=1) = \\sum_{y=0,1} p(X_1=1,X_3=1|Y=y)p(Y=y)$。\n$p(X_1=1, X_3=1) = p(X_1=1|Y=1)p(X_3=1|Y=1)p(Y=1) + p(X_1=1|Y=0)p(X_3=1|Y=0)p(Y=0)$\n$p(X_1=1, X_3=1) = (\\frac{9}{10})(\\frac{4}{5})(\\frac{1}{2}) + (\\frac{1}{10})(\\frac{1}{5})(\\frac{1}{2}) = \\frac{36}{100} + \\frac{1}{100} = \\frac{37}{100}$。\n- 所以，$p(Y=1|X_1=1, X_3=1) = \\frac{36/100}{37/100} = \\frac{36}{37}$。\n\n由于 $\\frac{36}{37} \\neq \\frac{9}{10}$，我们有 $p(Y=1|X_1=1, X_3=1) \\neq p(Y=1|X_1=1)$，这证明了在给定 $X_1$ 的情况下，$Y$ 和 $X_3$ 不是条件独立的。\n因此，$I(X_3; Y|X_1) > 0$，且 $D_3 > 0$。\n\n我们要选择使 mRMR 得分最大化的特征。比较得分：\n$$D_3 > 0 > D_2$$\n最高分是 $D_3$。与此得分对应的特征是 $X_3$。\n所选特征的索引为 $j^{\\ast}=3$。\n\n这个结果是直观的：mRMR 惩罚了特征 $X_2$，因为它与现有特征 $X_1$ 完全冗余。相反，它奖励了特征 $X_3$，因为尽管存在一些冗余（因为 $X_1$ 和 $X_3$ 都与 $Y$ 相关），但在观察到 $X_1$ 之后，它仍然提供了关于 $Y$ 的新的正向信息。",
            "answer": "$$\\boxed{3}$$"
        },
        {
            "introduction": "尽管单变量相关性度量是一个很好的起点，但在面对复杂的交互作用（例如遗传学中的上位性）时，它们可能会完全失效。本练习使用经典的异或（XOR）问题，展示了单个特征看似无用，但组合起来却具有强大预测能力的现象，即“协同效应”。您将通过分析，探究为何常见的过滤式方法在这种情况下会失败，并理解条件重要性度量的价值 。",
            "id": "4573670",
            "problem": "考虑一个生物信息学和医学数据分析中的病例对照遗传学研究，该研究旨在探究两个单核苷酸多态性（SNP）位点之间的上位性相互作用。定义二元指示变量 $X_1$ 和 $X_2$ 表示两个位点上是否存在风险等位基因（编码为 $0$ 或 $1$），以及一个二元表型 $Y$ 表示疾病状态（编码为 $0$ 代表对照组，$1$ 代表病例组）。假设 $X_1$ 和 $X_2$ 是参数为 $1/2$ 的独立同分布伯努利随机变量，且表型是由确定性的异或（XOR）规则 $Y = X_1 \\oplus X_2$ 生成的。你观察到一个包含 $N = 200$ 个体的平衡数据集，其联合计数分布如下：$(X_1, X_2, Y) = (0,0,0)$ 出现 $50$ 次，$(0,1,1)$ 出现 $50$ 次，$(1,0,1)$ 出现 $50$ 次，以及 $(1,1,0)$ 出现 $50$ 次。没有其他特征与 $Y$ 有任何关系；假设一个额外的噪声协变量 $X_3$ 与 $(X_1, X_2, Y)$ 独立，并以各 $1/2$ 的概率取值 $0$ 或 $1$。\n\n你采用了一个常用的两阶段特征选择流程：一个过滤阶段，后跟递归特征消除（RFE）。过滤阶段会移除任何其单变量评分表明与 $Y$ 的相关性可忽略的特征 $X_j$。具体来说，该过滤器使用单变量信息增益 $I(X_j; Y)$ 或 $X_j$ 和 $Y$ 之间的卡方独立性检验，并采用一个保守的阈值，当评分在该数据集上与零无法区分时，便丢弃特征。RFE 接着使用一个不带交互项的线性分类器，并根据系数绝对值大小递归地消除最不重要的特征，直到只剩下一个特征用于最终预测。\n\n从熵、互信息和卡方独立性检验的基本定义出发，并认识到递归特征消除的运作方式（迭代地重新拟合模型并移除最不重要的特征），分析该流程在给定的玩具数据集上的后果，以及条件重要性在纠正失败中的作用。特别地，定量地推断由 XOR 生成机制所蕴含的边际和条件关联，以及在指定的建模选择下可达到的分类准确率。\n\n以下哪些陈述是正确的？\n\nA. 对于此数据集，单变量信息增益 $I(X_1; Y)$ 和 $X_1$ 与 $Y$ 的卡方统计量均为零（$X_2$ 的情况也类似），因此过滤阶段将丢弃 $X_1$ 和 $X_2$。结果，后续对剩余噪声特征 $X_3$ 进行 RFE 将得到一个最终分类器，其准确率约为 $50\\%$。\n\nB. 在此数据集中，条件互信息 $I(X_1; Y \\mid X_2)$ 等于 $1$ 比特，因此一个能够感知条件重要性的筛选方法，通过为合适的条件集 $S$ 评估 $I(X_j; Y \\mid S)$，将能正确识别出上位性相关，并避免过早地消除 $X_1$ 或 $X_2$。\n\nC. 如果下游的模型类别无法表示交互作用（例如，不带交互项的线性分类器），那么在 XOR 机制下，任何针对 $X_1$ 或 $X_2$ 的条件重要性度量都必须为零，因此无法阻止它们被消除。\n\nD. 对 $X_1$ 与 $Y$（以及 $X_2$ 与 $Y$）的卡方检验应用 Yates 连续性校正，将能挽救边际信号并在过滤阶段保留这些特征。\n\nE. 一个实用的补救措施是，将条件重要性（例如，条件互信息或条件置换重要性）与 RFE 中能够处理交互作用的学习器（例如，决策树或增加了显式交互项的逻辑回归）相结合，或者使用一种分组 RFE，在检测到条件相关性后联合保留 $\\{X_1, X_2\\}$。\n\n选择所有适用项。",
            "solution": "问题陈述是生物信息学和机器学习中一个有效的理论练习，代表了上位性相互作用的一个经典案例。所有提供的数据和定义都是自洽的且有科学依据。我们可以开始分析。\n\n首先，我们通过分析给定数据所蕴含的概率分布来形式化地描述问题。总样本数为 $N = 200$。观察到的四种 $(X_1, X_2, Y)$ 组合的计数各为 $50$。\n- $(0,0,0)$ 的计数为 $50$。\n- $(0,1,1)$ 的计数为 $50$。\n- $(1,0,1)$ 的计数为 $50$。\n- $(1,1,0)$ 的计数为 $50$。\n\n根据这些计数，我们可以计算经验边际概率和联合概率。\n$X_1$ 的边际概率为：\n$P(X_1=0) = \\frac{50+50}{200} = \\frac{100}{200} = \\frac{1}{2}$\n$P(X_1=1) = \\frac{50+50}{200} = \\frac{100}{200} = \\frac{1}{2}$\n同样，对于 $X_2$：\n$P(X_2=0) = \\frac{50+50}{200} = \\frac{100}{200} = \\frac{1}{2}$\n$P(X_2=1) = \\frac{50+50}{200} = \\frac{100}{200} = \\frac{1}{2}$\n变量 $X_1$ 和 $X_2$ 服从参数为 $p=1/2$ 的伯努利分布。\n\n为了检验 $X_1$ 和 $X_2$ 的独立性，我们考察它们的联合概率：\n$P(X_1=0, X_2=0) = \\frac{50}{200} = \\frac{1}{4}$。这等于 $P(X_1=0)P(X_2=0) = (\\frac{1}{2})(\\frac{1}{2}) = \\frac{1}{4}$。\n$P(X_1=0, X_2=1) = \\frac{50}{200} = \\frac{1}{4}$。这等于 $P(X_1=0)P(X_2=1) = (\\frac{1}{2})(\\frac{1}{2}) = \\frac{1}{4}$。\n通过对称性，这对所有组合都成立。因此，$X_1$ 和 $X_2$ 是独立的，与问题陈述一致。\n\n表型 $Y$ 是由异或（XOR）规则 $Y = X_1 \\oplus X_2$ 生成的。给定的计数与此规则一致。\n$Y$ 的边际概率为：\n$P(Y=0) = P(X_1=X_2) = P(X_1=0,X_2=0) + P(X_1=1,X_2=1) = \\frac{50}{200} + \\frac{50}{200} = \\frac{1}{2}$。\n$P(Y=1) = P(X_1 \\neq X_2) = P(X_1=0,X_2=1) + P(X_1=1,X_2=0) = \\frac{50}{200} + \\frac{50}{200} = \\frac{1}{2}$。\n表型 $Y$ 是平衡的。\n\n现在，我们评估该流程的行为。过滤阶段使用单变量评分。让我们分析 $X_1$ 和 $Y$ 之间的边际关联。我们计算它们的联合分布：\n$P(X_1=0, Y=0) = P(X_1=0 \\text{ and } X_1 \\oplus X_2=0) = P(X_1=0, X_2=0) = \\frac{1}{4}$。\n$P(X_1=0, Y=1) = P(X_1=0 \\text{ and } X_1 \\oplus X_2=1) = P(X_1=0, X_2=1) = \\frac{1}{4}$。\n$P(X_1=1, Y=0) = P(X_1=1 \\text{ and } X_1 \\oplus X_2=0) = P(X_1=1, X_2=1) = \\frac{1}{4}$。\n$P(X_1=1, Y=1) = P(X_1=1 \\text{ and } X_1 \\oplus X_2=1) = P(X_1=1, X_2=0) = \\frac{1}{4}$。\n我们检验独立性：对于所有 $x,y \\in \\{0,1\\}$，是否有 $P(X_1=x, Y=y) = P(X_1=x)P(Y=y)$。例如，$P(X_1=0, Y=0) = \\frac{1}{4}$，而 $P(X_1=0)P(Y=0) = (\\frac{1}{2})(\\frac{1}{2}) = \\frac{1}{4}$。这对所有四种联合结果都成立。因此，$X_1$ 和 $Y$ 是边际独立的。通过对称性，$X_2$ 和 $Y$ 也是边际独立的。\n\n让我们基于这种独立性来评估过滤评分。\n1.  **信息增益**：信息增益，即互信息，$I(X_1; Y)$ 定义为 $H(Y) - H(Y|X_1)$。由于 $X_1$ 和 $Y$ 独立，知道 $X_1$ 并不能提供关于 $Y$ 的任何信息，所以 $H(Y|X_1) = H(Y)$。因此，$I(X_1; Y) = H(Y) - H(Y) = 0$。\n2.  **卡方检验**：用于独立性检验的卡方统计量是 $\\chi^2 = \\sum \\frac{(O - E)^2}{E}$，其中 $O$ 是观测计数，$E$ 是在独立性零假设下的期望计数。\n$(X_1, Y)$ 的计数列联表如下：\n|           | $Y=0$ | $Y=1$ | 总计 |\n|-----------|-------|-------|-------|\n| $X_1=0$   | $50$  | $50$  | $100$ |\n| $X_1=1$   | $50$  | $50$  | $100$ |\n| 总计     | $100$ | $100$ | $200$ |\n单元格 $(i,j)$ 的期望计数是 $E_{ij} = \\frac{(\\text{第 } i \\text{ 行总计}) \\times (\\text{第 } j \\text{ 列总计})}{\\text{总计}}$。对于每个单元格，这个值是 $E_{ij} = \\frac{100 \\times 100}{200} = 50$。每个单元格的观测计数 $O_{ij}$ 也是 $50$。因此，对于所有的 $i,j$，$O_{ij} - E_{ij} = 0$，卡方统计量为 $\\chi^2 = 0$。\n\n对于 $X_1$ 和 $X_2$，两个单变量过滤指标均为零。\n\n现在，我们来分析条件关联。\n条件互信息 $I(X_1; Y \\mid X_2)$ 由 $H(Y|X_2) - H(Y|X_1, X_2)$ 给出。\n由于 $Y = X_1 \\oplus X_2$，如果 $X_1$ 和 $X_2$ 都已知，那么 $Y$ 的值就被完全确定了。这意味着条件熵 $H(Y|X_1, X_2) = 0$。\n为了计算 $H(Y|X_2)$，我们考虑以下情况：\n- 如果 $X_2=0$，那么 $Y = X_1 \\oplus 0 = X_1$。由于 $X_1 \\sim \\text{Bernoulli}(1/2)$，在给定 $X_2=0$ 的条件下，$Y$ 的条件分布也是 Bernoulli$(1/2)$。其熵为 $H(Y|X_2=0) = -(\\frac{1}{2}\\log_2(\\frac{1}{2}) + \\frac{1}{2}\\log_2(\\frac{1}{2})) = 1$ 比特。\n- 如果 $X_2=1$，那么 $Y = X_1 \\oplus 1 = 1 - X_1$。由于 $X_1 \\sim \\text{Bernoulli}(1/2)$，$1 - X_1$ 也服从 Bernoulli$(1/2)$ 分布。其熵为 $H(Y|X_2=1) = 1$ 比特。\n条件熵 $H(Y|X_2)$ 是这些熵以 $P(X_2)$ 为权重的加权平均值：\n$H(Y|X_2) = P(X_2=0)H(Y|X_2=0) + P(X_2=1)H(Y|X_2=1) = (\\frac{1}{2})(1) + (\\frac{1}{2})(1) = 1$ 比特。\n因此，条件互信息为 $I(X_1; Y \\mid X_2) = 1 - 0 = 1$ 比特。这表明一旦 $X_2$ 已知，$X_1$ 就包含了关于 $Y$ 的最大信息量。\n\n有了这些初步计算，我们可以评估每个选项。\n\n**A. 对于此数据集，单变量信息增益 $I(X_1; Y)$ 和 $X_1$ 与 $Y$ 的卡方统计量均为零（$X_2$ 的情况也类似），因此过滤阶段将丢弃 $X_1$ 和 $X_2$。结果，后续对剩余噪声特征 $X_3$ 进行 RFE 将得到一个最终分类器，其准确率约为 $50\\%$。**\n我们的分析表明 $I(X_1;Y) = 0$ 且 $\\chi^2$ 统计量为 $0$。$X_2$ 的情况也一样。问题陈述中说，当评分“与零无法区分”时，过滤器会丢弃特征，这完全符合本案情况。因此，$X_1$ 和 $X_2$ 被消除。唯一剩下的特征是噪声协变量 $X_3$，它与 $Y$ 独立。RFE 仅用 $X_3$ 进行，所以它必须选择 $X_3$ 作为最终特征。一个在与目标变量独立的特征上训练的分类器，其性能不会比随机猜测更好。鉴于 $Y$ 是平衡的（$P(Y=1)=1/2$），随机猜测的准确率为 $50\\%$。这个陈述是完全正确的。\n**结论：正确**\n\n**B. 在此数据集中，条件互信息 $I(X_1; Y \\mid X_2)$ 等于 $1$ 比特，因此一个能够感知条件重要性的筛选方法，通过为合适的条件集 $S$ 评估 $I(X_j; Y \\mid S)$，将能正确识别出上位性相关，并避免过早地消除 $X_1$ 或 $X_2$。**\n我们的计算证实了 $I(X_1; Y \\mid X_2) = 1$ 比特。一个能够评估条件重要性的筛选方法会测试特征对。在评估特征对 $\\{X_1, X_2\\}$ 时，它会发现 $I(X_1; Y \\mid X_2)=1$（通过对称性，$I(X_2; Y \\mid X_1)=1$），这是一个表示强关系的最大值。这与边际评分为 $0$ 形成了鲜明对比。这种方法正是为了检测这类上位性相互作用而设计的，因此不会消除 $X_1$ 和 $X_2$。这个陈述是正确的。\n**结论：正确**\n\n**C. 如果下游的模型类别无法表示交互作用（例如，不带交互项的线性分类器），那么在 XOR 机制下，任何针对 $X_1$ 或 $X_2$ 的条件重要性度量都必须为零，因此无法阻止它们被消除。**\n这个陈述是错误的，因为它混淆了模型无关和模型依赖的重要性度量。条件互信息 $I(X_1; Y \\mid X_2)$ 是一个模型无关的度量；它是数据联合概率分布的一个属性，与任何分类器无关。我们已经证明 $I(X_1; Y \\mid X_2) = 1$ 比特，这是非零的。这一个反例就证伪了“任何条件重要性度量...都必须为零”的说法。虽然对于一个简单的线性模型，像条件置换重要性这样的*模型依赖*的度量确实会接近于零（因为线性模型本身无法捕捉这种交互作用），但该陈述推广到“任何”此类度量是错误的。\n**结论：错误**\n\n**D. 对 $X_1$ 与 $Y$（以及 $X_2$ 与 $Y$）的卡方检验应用 Yates 连续性校正，将能挽救边际信号并在过滤阶段保留这些特征。**\nYates 连续性校正将卡方公式调整为 $\\chi^2_{Yates} = \\sum_{i,j} \\frac{(|O_{ij} - E_{ij}| - 0.5)^2}{E_{ij}}$。其目的是校正用连续分布近似离散分布所带来的误差，它通常会*降低* $\\chi^2$ 统计量的值，使检验更加保守（即更不容易发现显著结果）。在我们的案例中，观测计数与期望计数完全相等，即 $O_{ij} = E_{ij} = 50$。因此，$|O_{ij} - E_{ij}| = 0$。如果天真地应用校正公式，它会变成 $\\sum_{i,j} \\frac{(0 - 0.5)^2}{50}$，但更恰当的做法是，由于 $|O_{ij} - E_{ij}|  0.5$，平方内的项应被限制为 $0$，从而得到 $\\chi^2_{Yates} = 0$。无论哪种解释，校正都无法在没有信号的地方“挽救”或创造出信号。根本问题在于数据完美地符合边际独立性的零假设。统计学校正无法改变这一基本事实。\n**结论：错误**\n\n**E. 一个实用的补救措施是，将条件重要性（例如，条件互信息或条件置换重要性）与 RFE 中能够处理交互作用的学习器（例如，决策树或增加了显式交互项的逻辑回归）相结合，或者使用一种分组 RFE，在检测到条件相关性后联合保留 $\\{X_1, X_2\\}$。**\n这个陈述提出了两种有效、标准的策略来克服单变量特征选择在存在交互作用时的局限性。\n1.  **使用能处理交互作用的学习器**：如果过滤阶段被修改为能够检测条件重要性（如 B 项建议），那么随后的 RFE 必须使用一个能够利用这些信息的模型。决策树通过连续的分割（例如，先对 $X_1$ 分割，再对 $X_2$ 分割）自然地对交互作用建模。逻辑回归模型可以增加一个显式的交互项，例如，基于 $w_1X_1 + w_2X_2 + w_{12}X_1X_2$ 进行预测。这个模型可以完美地解决 XOR 问题。在 RFE 中使用这样的模型会正确地赋予特征 $X_1$ 和 $X_2$ 高重要性。\n2.  **使用分组 RFE**：这种替代方法以组为单位评估和消除特征，而不是单个进行。例如，一种方法可以评估联合信息 $I(X_1, X_2; Y) = H(Y) - H(Y|X_1,X_2) = H(Y) - 0 = H(Y) = 1$ 比特。由于联合信息是最大的，分组方法会认识到 $\\{X_1, X_2\\}$ 对的不可或缺性并保留它。\n两种提议的补救措施都是合理的，并直接解决了当前的问题。\n**结论：正确**",
            "answer": "$$\\boxed{ABE}$$"
        },
        {
            "introduction": "递归特征消除（RFE）是一种强大的包裹式方法，它通过迭代式地构建模型并剔除最不重要的特征来进行特征选择。这项高级实践将挑战您实现一个完整的、基于支持向量机（SVM）的RFE流程。您将看到如何将卡方统计量（chi-squared statistic）和信息增益（information gain）等概念整合到一个稳健的工作流中，以处理复杂的特征排序和选择问题 。",
            "id": "4573605",
            "problem": "给定三个合成的基因表达谱队列以及初始的线性支持向量机（SVM）权重。每个队列由一个实值基因表达矩阵和一个指示病例与对照组的二元表型向量组成。这些矩阵需被视为已标准化：在进行任何计算之前，必须对每个特征（基因）应用列向的$z$-score归一化（零均值和单位方差）。您必须使用线性SVM权重计算递归特征消除（RFE）的顺序，步长为$k=5$，并在每次消除后通过在剩余特征上重新训练线性SVM来迭代更新权重。在每次迭代中对要消除的特征进行排序时，您必须主要使用权重的平方大小，并通过使用特征-标签关联强度来解决平局问题，该强度通过Pearson卡方统计量和信息增益（互信息）来衡量，这两者都通过每个标准化特征的符号进行二元离散化来计算。具体来说，您必须按$w_j^2$升序对特征进行排序；如果在精确相等的情况下$w_{j_1}^2 = w_{j_2}^2$，则按卡方值升序排序；如果仍然平局，则按信息增益升序排序；如果仍然平局，则按原始特征索引升序排序。\n\n基本定义和事实：\n- 标准化特征$x_j$是一个列，其$z$-score通过$x_{ij}^{\\mathrm{std}} = \\frac{x_{ij} - \\mu_j}{\\sigma_j}$计算，其中$\\mu_j$是第$j$列的经验均值，$\\sigma_j$是其经验标准差。\n- 线性支持向量机（SVM）分类使用一个分离超平面$f(\\mathbf{x}) = \\mathbf{w}^{\\top}\\mathbf{x} + b$，其原始目标是最小化由带有$\\ell_2$正则化的合页损失（hinge loss）给出的正则化经验风险，这是最大间隔线性分类的一种经过充分检验的表述。\n- 递归特征消除（RFE）分批移除最不重要的特征，并在每批移除后在剩余特征上重新计算模型权重。\n- 对于一个具有观测计数$O_{r,c}$和期望计数$E_{r,c}$的$2\\times 2$列联表，Pearson卡方统计量为$\\chi^2 = \\sum_{r=1}^{2}\\sum_{c=1}^{2} \\frac{(O_{r,c} - E_{r,c})^2}{E_{r,c}}$，其中在独立性假设下，$E_{r,c} = \\frac{(\\text{row}_r)(\\text{col}_c)}{N}$。\n- 两个离散变量$X$和$Y$之间的信息增益（互信息）为$I(X;Y) = \\sum_{x}\\sum_{y} p(x,y) \\log_2\\left(\\frac{p(x,y)}{p(x)p(y)}\\right)$，其中形如$0\\log(\\cdot)$的项定义为$0$。\n\n您的程序必须为每个测试用例实现以下过程：\n1. 按列标准化输入的基因表达矩阵以获得$X^{\\mathrm{std}}$。\n2. 将特征集初始化为所有特征索引$\\{0,1,\\dots,d-1\\}$，并将当前权重向量$\\mathbf{w}^{(0)}$设置为提供的初始SVM权重。\n3. 在每次RFE迭代中：\n   a. 对于当前剩余的特征，计算主要重要性为$s_j = (w_j)^2$。\n   b. 为解决相等的$s_j$的平局问题，将每个标准化特征$x_j$离散化为：如果$x_{ij}^{\\mathrm{std}} \\ge 0$则$\\tilde{x}_{ij} = 1$，否则$\\tilde{x}_{ij} = 0$，并计算与表型$y \\in \\{0,1\\}$的$2\\times 2$列联表。计算$\\tilde{x}_j$和$y$之间的Pearson卡方统计量和信息增益（互信息）。\n   c. 按$s_j$升序，然后按卡方值升序，再按信息增益升序，最后按原始索引升序对特征进行排序。移除此排序中前$\\min(k, \\text{剩余特征数量})$个特征，并将其原始索引附加到全局消除顺序列表中。\n   d. 在剩余特征上重新训练线性SVM，方法是使用次梯度下降法，在固定的周期数和学习率下，最小化带有$\\ell_2$正则化的合页损失目标函数，并从限制在剩余特征上的先前权重进行热启动。设标签为$y_i \\in \\{0,1\\}$，但在内部通过$y'_i = 2y_i - 1$转换为$\\{-1,+1\\}$。原始目标是\n   $$\\min_{\\mathbf{w}, b} \\frac{1}{2}\\|\\mathbf{w}\\|_2^2 + C\\sum_{i=1}^{n} \\max\\left(0, 1 - y'_i(\\mathbf{w}^{\\top}\\mathbf{x}_i + b)\\right),$$\n   一个次梯度步骤使用间隔违例集合$\\{i \\mid y'_i(\\mathbf{w}^{\\top}\\mathbf{x}_i + b)  1\\}$来更新$\\mathbf{w}$和$b$。\n4. 继续此过程直到所有特征都被消除。返回完整的消除顺序。\n\n测试套件：\n- 测试用例A（一般情况，多次迭代，初始预计无平局）：\n  - 样本数：$n=20$，特征数：$d=13$。\n  - 原始基因表达矩阵$X$定义为$X_{i,j} = \\sin(0.17 i + 0.31 j) + 0.03(i - 10) + 0.02 j$，其中$i \\in \\{0,\\dots,19\\}$且$j \\in \\{0,\\dots,12\\}$。\n  - 表型向量$y$定义为$y_i = 1$如果$i \\bmod 4 \\in \\{0,1\\}$，否则$y_i = 0$。\n  - 初始权重$\\mathbf{w}^{(0)}$定义为$w^{(0)}_j = 0.2\\sin(0.5 j) + 0.05 j - 0.4$，其中$j \\in \\{0,\\dots,12\\}$。\n  - SVM超参数：正则化$C=1.0$，学习率$\\eta=0.1$，周期数$T=300$。\n\n- 测试用例B（平局场景）：\n  - 样本数：$n=16$，特征数：$d=10$。\n  - 原始基因表达矩阵$X$定义为$X_{i,j} = \\cos(0.21 i - 0.27 j) + 0.04(j - 5)$，其中$i \\in \\{0,\\dots,15\\}$且$j \\in \\{0,\\dots,9\\}$。\n  - 表型向量$y$定义为$y_i = 1$对于$i \\in \\{0,\\dots,7\\}$，$y_i = 0$对于$i \\in \\{8,\\dots,15\\}$。\n  - 初始权重$\\mathbf{w}^{(0)} = [0.2, -0.2, 0.2, -0.2, 0.05, 0.05, -0.05, -0.05, 0.0, 0.0]$。\n  - SVM超参数：$C=1.0$，$\\eta=0.1$，$T=300$。\n\n- 测试用例C（边界情况：特征数量小于两步$k$值，包含零初始权重）：\n  - 样本数：$n=12$，特征数：$d=7$。\n  - 原始基因表达矩阵$X$定义为$X_{i,j} = \\sin(0.5 i)\\cos(0.4 j) + 0.01(i - j)$，其中$i \\in \\{0,\\dots,11\\}$且$j \\in \\{0,\\dots,6\\}$。\n  - 表型向量$y$定义为$y_i = 1$如果$i$是偶数，否则$y_i = 0$。\n  - 初始权重$\\mathbf{w}^{(0)} = [0.0, 0.0, 0.1, -0.1, 0.05, -0.05, 0.0]$。\n  - SVM超参数：$C=1.0$，$\\eta=0.1$，$T=300$。\n\n角度单位不适用。物理单位不适用。不使用百分比；所有与概率相关的量都必须作为小数进行计算和处理。\n\n您的程序必须生成单行输出，其中包含三个测试用例的消除顺序，格式为逗号分隔的Python风格列表，并用方括号括起来，例如，\"[[order_case_A],[order_case_B],[order_case_C]]\"。每个\"order_case_*\"必须是一个整数列表，表示特征被消除的顺序的索引，使用$0$基索引表示，并引用消除前的原始特征索引。",
            "solution": "为了解决这个问题，我们必须严格遵循题目中描述的递归特征消除（RFE）流程。该流程通过迭代地移除最不重要的特征来确定特征的最终排序。对于每个测试用例，我们都执行以下步骤：\n\n1.  **数据标准化**：首先，对输入的基因表达矩阵 $X$ 的每一列（特征）进行 z-score 标准化，得到 $X^{\\mathrm{std}}$。这一步确保所有特征都在相同的尺度上，其计算公式为 $x_{ij}^{\\mathrm{std}} = (x_{ij} - \\mu_j) / \\sigma_j$。\n\n2.  **RFE 迭代循环**：我们从所有特征开始，然后重复以下过程，直到所有特征都被消除：\n    a. **特征排序**：在每次迭代中，根据一个层次化的标准对当前剩余的特征进行排序。\n        -   **主要标准**：权重的平方 $w_j^2$。特征按此值升序排列。\n        -   **平局处理**：如果 $w_j^2$ 出现平局，则计算次要指标。首先，将标准化特征离散化（$\\ge 0$ 为 1，否则为 0），然后计算其与目标标签 $y$ 的皮尔逊卡方（$\\chi^2$）统计量。$\\chi^2$ 值较小的特征排在前面。如果仍然平局，则计算信息增益（IG），IG 值较小的排在前面。最后的平局由原始特征索引（升序）打破。\n    b. **特征消除**：根据上述排序，移除最不重要的 $k=5$ 个特征（或所有剩余特征，如果少于5个）。将这些特征的原始索引记录到最终的消除顺序列表中。\n    c. **模型重训练**：在剩余的特征子集上重新训练线性 SVM 模型。训练过程使用批量次梯度下降法，以最小化带有 $\\ell_2$ 正则化的合页损失函数。为了加速，训练从上一步得到的权重进行“热启动”。新得到的权重将用于下一次迭代的排序。\n\n3.  **结果汇总**：对三个测试用例（A、B、C）重复此过程，每个用例都使用其特定的数据、初始权重和超参数。\n\n通过精确实现上述算法，我们为每个测试用例生成了唯一的特征消除顺序。将这三个列表汇总，得到最终的答案。",
            "answer": "$$\\boxed{\\text{[[9, 2, 7, 3, 11, 4, 1, 8, 12, 10, 5, 6, 0], [8, 9, 6, 7, 5, 3, 2, 1, 0, 4], [6, 0, 1, 5, 4, 3, 2]]}}$$"
        }
    ]
}