{
    "hands_on_practices": [
        {
            "introduction": "A classifier's diagnostic accuracy depends critically on the quality of its learned parameters. This exercise addresses the crucial task of parameter estimation, particularly in challenging medical scenarios involving rare diseases where training data is sparse. You will directly compare the outcomes of two fundamental methods—Maximum Likelihood (ML) and Maximum A Posteriori (MAP) estimation—to see firsthand how the naive approach of simple counting can lead to overconfident and misleading results, and how incorporating prior knowledge via MAP estimation provides a vital safeguard against this common pitfall .",
            "id": "4588350",
            "problem": "A hospital is constructing a binary naive Bayes classifier for diagnosing a rare disease, denoted by the class label $y \\in \\{0,1\\}$ with $y=1$ indicating disease presence. The classifier uses a single binary symptom indicator $x \\in \\{0,1\\}$ that equals $1$ if the patient reports a specific symptom. The disease prevalence in the target population is known from a disease registry to be $p(y=1)=\\pi$, with $\\pi=0.005$. A small labeled training set is available: among $n_{1}=4$ known diseased patients ($y=1$), $k_{1}=1$ reported the symptom ($x=1$); among $n_{0}=6$ known non-diseased controls ($y=0$), $k_{0}=0$ reported the symptom ($x=1$).\n\nLet $\\theta_{1}=p(x=1 \\mid y=1)$ and $\\theta_{0}=p(x=1 \\mid y=0)$. Two estimation strategies are considered:\n- Maximum Likelihood (ML), using only the observed counts.\n- Maximum A Posteriori (MAP), using independent Beta priors consistent with clinical knowledge: $\\theta_{1} \\sim \\mathrm{Beta}(\\alpha_{1},\\beta_{1})$ with $\\alpha_{1}=2$, $\\beta_{1}=2$, and $\\theta_{0} \\sim \\mathrm{Beta}(\\alpha_{0},\\beta_{0})$ with $\\alpha_{0}=2$, $\\beta_{0}=198$.\n\nUsing only fundamental definitions (Bayes’ theorem for posterior probability under the naive Bayes modeling assumption, and the conjugacy of the Beta prior with the Bernoulli likelihood), compute the posterior disease probability for a new patient with $x=1$ under each estimation strategy, namely $p_{\\mathrm{ML}}(y=1 \\mid x=1)$ and $p_{\\mathrm{MAP}}(y=1 \\mid x=1)$. Then quantify the downstream change in posterior by the difference\n$$\\Delta \\equiv p_{\\mathrm{MAP}}(y=1 \\mid x=1) - p_{\\mathrm{ML}}(y=1 \\mid x=1).$$\nReport the single numeric value of $\\Delta$ rounded to four significant figures. Express the answer as a pure decimal (no percentage sign).",
            "solution": "The problem requires the computation and comparison of the posterior probability of a disease given a symptom, $p(y=1 \\mid x=1)$, under two different parameter estimation schemes for a naive Bayes classifier: Maximum Likelihood (ML) and Maximum A Posteriori (MAP). The final goal is to calculate the difference $\\Delta$ between the posterior probabilities obtained from these two methods.\n\nFirst, let's establish the general formula for the posterior probability using Bayes' theorem. The naive Bayes model assumes that the feature $x$ is conditionally independent of other features given the class $y$. As there is only one feature, this assumption is trivially satisfied. The posterior probability of having the disease ($y=1$) given the presence of the symptom ($x=1$) is:\n$$p(y=1 \\mid x=1) = \\frac{p(x=1 \\mid y=1) p(y=1)}{p(x=1)}$$\nThe denominator, $p(x=1)$, is the marginal probability of the evidence, which can be expanded using the law of total probability:\n$$p(x=1) = p(x=1 \\mid y=1) p(y=1) + p(x=1 \\mid y=0) p(y=0)$$\nUsing the notation provided in the problem statement:\n-   The disease prevalence (prior probability of disease) is $p(y=1) = \\pi$.\n-   The probability of a non-diseased state is $p(y=0) = 1 - \\pi$.\n-   The class-conditional probability of the symptom in diseased patients is $\\theta_1 = p(x=1 \\mid y=1)$.\n-   The class-conditional probability of the symptom in non-diseased patients is $\\theta_0 = p(x=1 \\mid y=0)$.\n\nSubstituting these into the formula for the posterior probability gives:\n$$p(y=1 \\mid x=1) = \\frac{\\theta_1 \\pi}{\\theta_1 \\pi + \\theta_0 (1-\\pi)}$$\nWe are given $\\pi = 0.005$, so $1-\\pi = 0.995$. The core of the problem lies in estimating the parameters $\\theta_1$ and $\\theta_0$ using the ML and MAP methods.\n\n**1. Maximum Likelihood (ML) Estimation**\n\nThe ML estimate for the parameter of a Bernoulli distribution (which models our binary symptom $x$) is simply the sample proportion of successes.\n-   For the diseased group ($y=1$), there were $k_1=1$ occurrences of the symptom ($x=1$) in $n_1=4$ patients. The ML estimate for $\\theta_1$ is:\n$$\\hat{\\theta}_{1, \\mathrm{ML}} = \\frac{k_1}{n_1} = \\frac{1}{4} = 0.25$$\n-   For the non-diseased group ($y=0$), there were $k_0=0$ occurrences of the symptom ($x=1$) in $n_0=6$ patients. The ML estimate for $\\theta_0$ is:\n$$\\hat{\\theta}_{0, \\mathrm{ML}} = \\frac{k_0}{n_0} = \\frac{0}{6} = 0$$\n\nNow, we compute the posterior probability $p_{\\mathrm{ML}}(y=1 \\mid x=1)$ using these ML estimates:\n$$p_{\\mathrm{ML}}(y=1 \\mid x=1) = \\frac{\\hat{\\theta}_{1, \\mathrm{ML}} \\pi}{\\hat{\\theta}_{1, \\mathrm{ML}} \\pi + \\hat{\\theta}_{0, \\mathrm{ML}} (1-\\pi)} = \\frac{0.25 \\times 0.005}{0.25 \\times 0.005 + 0 \\times 0.995}$$\n$$p_{\\mathrm{ML}}(y=1 \\mid x=1) = \\frac{0.00125}{0.00125 + 0} = 1$$\nThis result indicates that, under ML estimation, observing the symptom guarantees the presence of the disease. This is a direct consequence of the zero-frequency problem: since no non-diseased individuals in the small training set had the symptom, the model incorrectly learns that it's impossible for a non-diseased person to have it.\n\n**2. Maximum A Posteriori (MAP) Estimation**\n\nMAP estimation incorporates prior beliefs about the parameters, which helps to regularize the estimates and avoid issues like the zero-frequency problem. The problem specifies Beta priors for $\\theta_1$ and $\\theta_0$. The Beta distribution is the conjugate prior for the Bernoulli likelihood, which simplifies the calculation of the posterior distribution.\n\nFor a parameter $\\theta$ with a $\\mathrm{Beta}(\\alpha, \\beta)$ prior, after observing $k$ successes in $n$ trials, the posterior distribution is $\\mathrm{Beta}(k+\\alpha, n-k+\\beta)$. The MAP estimate is the mode of this posterior distribution. The mode of a $\\mathrm{Beta}(a,b)$ distribution (for $a, b > 1$) is given by $\\frac{a-1}{a+b-2}$.\n\n-   For $\\theta_1$, the prior is $\\mathrm{Beta}(\\alpha_1, \\beta_1)$ with $\\alpha_1=2, \\beta_1=2$. The data is $k_1=1, n_1=4$. The posterior distribution for $\\theta_1$ is:\n$$\\theta_1 \\mid \\text{data} \\sim \\mathrm{Beta}(k_1+\\alpha_1, n_1-k_1+\\beta_1) = \\mathrm{Beta}(1+2, 4-1+2) = \\mathrm{Beta}(3, 5)$$\nSince both parameters of the posterior distribution ($3$ and $5$) are greater than $1$, the MAP estimate is the mode:\n$$\\hat{\\theta}_{1, \\mathrm{MAP}} = \\frac{(k_1+\\alpha_1)-1}{(k_1+\\alpha_1) + (n_1-k_1+\\beta_1)-2} = \\frac{3-1}{3+5-2} = \\frac{2}{6} = \\frac{1}{3}$$\n-   For $\\theta_0$, the prior is $\\mathrm{Beta}(\\alpha_0, \\beta_0)$ with $\\alpha_0=2, \\beta_0=198$. The data is $k_0=0, n_0=6$. The posterior distribution for $\\theta_0$ is:\n$$\\theta_0 \\mid \\text{data} \\sim \\mathrm{Beta}(k_0+\\alpha_0, n_0-k_0+\\beta_0) = \\mathrm{Beta}(0+2, 6-0+198) = \\mathrm{Beta}(2, 204)$$\nThe parameters of the posterior ($2$ and $204$) are also greater than $1$. The MAP estimate is:\n$$\\hat{\\theta}_{0, \\mathrm{MAP}} = \\frac{(k_0+\\alpha_0)-1}{(k_0+\\alpha_0) + (n_0-k_0+\\beta_0)-2} = \\frac{2-1}{2+204-2} = \\frac{1}{204}$$\n\nNow we compute the posterior probability $p_{\\mathrm{MAP}}(y=1 \\mid x=1)$ using these MAP estimates. We use the exact fractional forms of the parameters, along with $\\pi = 0.005 = \\frac{1}{200}$ and $1-\\pi = 0.995 = \\frac{199}{200}$:\n$$p_{\\mathrm{MAP}}(y=1 \\mid x=1) = \\frac{\\hat{\\theta}_{1, \\mathrm{MAP}} \\pi}{\\hat{\\theta}_{1, \\mathrm{MAP}} \\pi + \\hat{\\theta}_{0, \\mathrm{MAP}} (1-\\pi)} = \\frac{\\frac{1}{3} \\times \\frac{1}{200}}{\\frac{1}{3} \\times \\frac{1}{200} + \\frac{1}{204} \\times \\frac{199}{200}}$$\nTo simplify this complex fraction, we can cancel the $\\frac{1}{200}$ term from the numerator and both terms in the denominator's sum:\n$$p_{\\mathrm{MAP}}(y=1 \\mid x=1) = \\frac{\\frac{1}{3}}{\\frac{1}{3} + \\frac{199}{204}} = \\frac{\\frac{1}{3}}{\\frac{1 \\times 68}{3 \\times 68} + \\frac{199}{204}} = \\frac{\\frac{1}{3}}{\\frac{68}{204} + \\frac{199}{204}} = \\frac{\\frac{1}{3}}{\\frac{267}{204}}$$\n$$p_{\\mathrm{MAP}}(y=1 \\mid x=1) = \\frac{1}{3} \\times \\frac{204}{267} = \\frac{68}{267}$$\n\n**3. Quantifying the Change**\n\nThe final step is to compute the difference $\\Delta$:\n$$\\Delta = p_{\\mathrm{MAP}}(y=1 \\mid x=1) - p_{\\mathrm{ML}}(y=1 \\mid x=1)$$\n$$\\Delta = \\frac{68}{267} - 1 = \\frac{68 - 267}{267} = -\\frac{199}{267}$$\nTo provide the numerical answer, we compute the decimal value and round to four significant figures:\n$$\\Delta = -\\frac{199}{267} \\approx -0.74531835...$$\nRounding to four significant figures yields $-0.7453$. This substantial negative difference highlights the strong regularizing effect of the MAP estimation, which pulled the posterior probability away from the extreme value of $1$ produced by the overconfident ML model.",
            "answer": "$$\\boxed{-0.7453}$$"
        },
        {
            "introduction": "The power and simplicity of the Naive Bayes classifier stem from its core assumption: that all diagnostic features are independent of one another given the patient's disease status. But how does the model behave when this 'naive' assumption is violated, as is often the case with interconnected biological systems? This practice moves beyond abstract critique by having you calculate the precise multiplicative bias in the diagnostic odds that arises from ignoring a known correlation between two biomarkers, providing a quantitative understanding of the model's primary limitation .",
            "id": "4588292",
            "problem": "In a medical diagnostic context within bioinformatics and medical data analytics, consider a binary disease indicator $D \\in \\{0,1\\}$, where $D=1$ denotes presence of disease and $D=0$ denotes absence of disease. Two binary features, $X_{1}$ and $X_{2}$, are derived from gene expression thresholds (each feature equals $1$ when the corresponding biomarker is overexpressed and equals $0$ otherwise). Assume the following scientifically plausible data:\n- Disease prevalence: $P(D=1)=0.02$ and $P(D=0)=0.98$.\n- Under disease ($D=1$), the features are positively conditionally correlated with marginals $P(X_{1}=1 \\mid D=1)=0.70$ and $P(X_{2}=1 \\mid D=1)=0.60$, and joint $P(X_{1}=1, X_{2}=1 \\mid D=1)=0.55$. These values imply $P(X_{1}=1, X_{2}=0 \\mid D=1)=0.15$, $P(X_{1}=0, X_{2}=1 \\mid D=1)=0.05$, and $P(X_{1}=0, X_{2}=0 \\mid D=1)=0.25$, which are consistent with the marginals and demonstrate positive conditional dependence because $P(X_{1}=1, X_{2}=1 \\mid D=1)=0.55 > 0.70 \\times 0.60=0.42$.\n- Under no disease ($D=0$), the features are conditionally independent with $P(X_{1}=1 \\mid D=0)=0.10$, $P(X_{2}=1 \\mid D=0)=0.15$, so that $P(X_{1}=1, X_{2}=1 \\mid D=0)=0.10 \\times 0.15=0.015$.\n\nA Naive Bayes (NB) classifier models $X_{1}$ and $X_{2}$ as conditionally independent given $D$, so that for any $d \\in \\{0,1\\}$,\n$$\nP_{\\text{NB}}(X_{1}=x_{1}, X_{2}=x_{2} \\mid D=d) = P(X_{1}=x_{1} \\mid D=d)\\, P(X_{2}=x_{2} \\mid D=d).\n$$\nA correctly specified dependent model uses the true joint distribution $P(X_{1}, X_{2} \\mid D)$ without the independence assumption.\n\nStarting only from the definition of Bayes’ theorem and posterior odds, derive the multiplicative bias in posterior odds induced by the Naive Bayes independence assumption for a patient presenting with $X_{1}=1$ and $X_{2}=1$. Define the multiplicative bias as\n$$\nB \\equiv \\frac{O_{\\text{NB}}(D=1 \\mid X_{1}=1, X_{2}=1)}{O_{\\text{true}}(D=1 \\mid X_{1}=1, X_{2}=1)},\n$$\nwhere $O(\\cdot)$ denotes posterior odds. Compute $B$ using the data above and express your final result as a dimensionless decimal number. Round your answer to four significant figures. Do not use a percentage sign.",
            "solution": "The problem asks for the multiplicative bias in posterior odds induced by the Naive Bayes (NB) independence assumption for a patient presenting with features $X_{1}=1$ and $X_{2}=1$. The bias $B$ is defined as the ratio of the posterior odds calculated by the NB model to the true posterior odds. Let the evidence be denoted by $X = (X_{1}=1, X_{2}=1)$.\n\nFirst, we establish the general form of posterior odds. The posterior odds of having the disease ($D=1$) versus not having the disease ($D=0$) given the evidence $X$ are defined as:\n$$\nO(D=1 \\mid X) = \\frac{P(D=1 \\mid X)}{P(D=0 \\mid X)}\n$$\nBy applying Bayes' theorem, which states $P(D=d \\mid X) = \\frac{P(X \\mid D=d)P(D=d)}{P(X)}$, to both the numerator and the denominator, we get:\n$$\nO(D=1 \\mid X) = \\frac{\\frac{P(X \\mid D=1)P(D=1)}{P(X)}}{\\frac{P(X \\mid D=0)P(D=0)}{P(X)}}\n$$\nThe marginal probability of the evidence, $P(X)$, cancels out, yielding the expression for posterior odds as the product of the likelihood ratio and the prior odds:\n$$\nO(D=1 \\mid X) = \\left(\\frac{P(X \\mid D=1)}{P(X \\mid D=0)}\\right) \\left(\\frac{P(D=1)}{P(D=0)}\\right)\n$$\n\nThe multiplicative bias $B$ is defined as:\n$$\nB \\equiv \\frac{O_{\\text{NB}}(D=1 \\mid X)}{O_{\\text{true}}(D=1 \\mid X)}\n$$\nwhere $O_{\\text{NB}}$ are the odds calculated using the Naive Bayes assumption and $O_{\\text{true}}$ are the odds calculated using the true conditional probabilities.\n\nUsing the derived formula for posterior odds, we can write the true odds and the NB odds. Let $X$ represent the specific observation $(X_{1}=1, X_{2}=1)$.\nThe true posterior odds are:\n$$\nO_{\\text{true}}(D=1 \\mid X) = \\frac{P(X_{1}=1, X_{2}=1 \\mid D=1)}{P(X_{1}=1, X_{2}=1 \\mid D=0)} \\times \\frac{P(D=1)}{P(D=0)}\n$$\nThe Naive Bayes posterior odds are:\n$$\nO_{\\text{NB}}(D=1 \\mid X) = \\frac{P_{\\text{NB}}(X_{1}=1, X_{2}=1 \\mid D=1)}{P_{\\text{NB}}(X_{1}=1, X_{2}=1 \\mid D=0)} \\times \\frac{P(D=1)}{P(D=0)}\n$$\nSubstituting these expressions into the definition of the bias $B$:\n$$\nB = \\frac{\\frac{P_{\\text{NB}}(X_{1}=1, X_{2}=1 \\mid D=1)}{P_{\\text{NB}}(X_{1}=1, X_{2}=1 \\mid D=0)} \\times \\frac{P(D=1)}{P(D=0)}}{\\frac{P(X_{1}=1, X_{2}=1 \\mid D=1)}{P(X_{1}=1, X_{2}=1 \\mid D=0)} \\times \\frac{P(D=1)}{P(D=0)}}\n$$\nThe prior odds term, $\\frac{P(D=1)}{P(D=0)}$, cancels out, leaving a ratio of likelihood ratios:\n$$\nB = \\frac{P_{\\text{NB}}(X_{1}=1, X_{2}=1 \\mid D=1)}{P(X_{1}=1, X_{2}=1 \\mid D=1)} \\times \\frac{P(X_{1}=1, X_{2}=1 \\mid D=0)}{P_{\\text{NB}}(X_{1}=1, X_{2}=1 \\mid D=0)}\n$$\nNow we must calculate the four conditional probabilities in this expression using the provided data.\n\n1.  The true conditional probability for $D=1$: The problem explicitly gives the joint probability for the dependent features:\n    $$P(X_{1}=1, X_{2}=1 \\mid D=1) = 0.55$$\n\n2.  The Naive Bayes conditional probability for $D=1$: The NB model assumes conditional independence, using the given marginal probabilities:\n    $$P_{\\text{NB}}(X_{1}=1, X_{2}=1 \\mid D=1) = P(X_{1}=1 \\mid D=1) P(X_{2}=1 \\mid D=1) = 0.70 \\times 0.60 = 0.42$$\n\n3.  The true conditional probability for $D=0$: The problem states that for $D=0$, the features are conditionally independent. Therefore, the true joint probability is the product of the marginals:\n    $$P(X_{1}=1, X_{2}=1 \\mid D=0) = P(X_{1}=1 \\mid D=0) P(X_{2}=1 \\mid D=0) = 0.10 \\times 0.15 = 0.015$$\n\n4.  The Naive Bayes conditional probability for $D=0$: The NB model also assumes conditional independence:\n    $$P_{\\text{NB}}(X_{1}=1, X_{2}=1 \\mid D=0) = P(X_{1}=1 \\mid D=0) P(X_{2}=1 \\mid D=0) = 0.10 \\times 0.15 = 0.015$$\n\nSince the features are truly conditionally independent for the $D=0$ class, the NB assumption holds, and $P(X_{1}=1, X_{2}=1 \\mid D=0) = P_{\\text{NB}}(X_{1}=1, X_{2}=1 \\mid D=0)$. The ratio of these two terms is $1$.\n\nSubstituting these values into the expression for the bias $B$:\n$$\nB = \\frac{0.42}{0.55} \\times \\frac{0.015}{0.015} = \\frac{0.42}{0.55} \\times 1\n$$\n$$\nB = \\frac{0.42}{0.55}\n$$\nPerforming the division:\n$$\nB \\approx 0.76363636...\n$$\nRounding the result to four significant figures, as requested, we obtain:\n$$\nB \\approx 0.7636\n$$\nThis bias factor, being less than $1$, indicates that the Naive Bayes classifier underestimates the posterior odds of the disease for a patient with both biomarkers overexpressed, due to its failure to account for the positive conditional dependence between the features in the diseased population.",
            "answer": "$$\\boxed{0.7636}$$"
        },
        {
            "introduction": "Having quantified the error introduced by the strict independence assumption, we now explore a practical and elegant solution: the Semi-Naive Bayes (SNB) classifier. This approach offers a middle ground, assuming independence only between pre-defined groups of features while modeling dependencies within them. In this exercise, you will construct an SNB model that combines information from correlated biomarkers and clinical symptoms to see how this more nuanced structure can lead to a more accurate and realistic diagnostic tool .",
            "id": "4588344",
            "problem": "Consider a binary disease variable $Y \\in \\{0,1\\}$ where $Y=1$ denotes acute systemic inflammation due to infection and $Y=0$ denotes no infection. You are given two feature groups measured on a patient: a biomarker group $\\mathbf{X}=(X_1, X_2)$ consisting of C-reactive protein (mg/L) and interleukin-$6$ (pg/mL), and a symptom group $\\mathbf{S}=(S_1,S_2)$ consisting of binary indicators for cough $(S_1 \\in \\{0,1\\})$ and fever $(S_2 \\in \\{0,1\\})$. The prior probabilities are $p(Y=1)=0.25$ and $p(Y=0)=0.75$.\n\nTo relax the feature independence assumption of the classical Naive Bayes classifier, assume a Semi-Naive Bayes (SNB) model with grouped features: groups are conditionally independent given $Y$, but features within a group may be dependent and are modeled by an appropriate intra-group distribution. The fundamental base should start from Bayes’ theorem $p(y \\mid \\text{data}) \\propto p(y)\\,p(\\text{data} \\mid y)$ and the law of total probability.\n\nAssume the following scientifically plausible intra-group models:\n\n- Biomarkers are modeled by a bivariate Multivariate Normal (MVN) distribution conditional on $Y$ with parameters\n$$\n\\boldsymbol{\\mu}_1=\\begin{pmatrix}35\\\\20\\end{pmatrix},\\quad\n\\boldsymbol{\\Sigma}_1=\\begin{pmatrix}100 & 40\\\\ 40 & 25\\end{pmatrix}\n\\quad\\text{for }Y=1,\n$$\n$$\n\\boldsymbol{\\mu}_0=\\begin{pmatrix}10\\\\5\\end{pmatrix},\\quad\n\\boldsymbol{\\Sigma}_0=\\begin{pmatrix}64 & 0\\\\ 0 & 16\\end{pmatrix}\n\\quad\\text{for }Y=0.\n$$\n\n- Symptoms are modeled by a joint Bernoulli distribution conditional on $Y$ with\n$$\np(S_1=1,S_2=1\\mid Y=1)=0.50,\\;\np(S_1=1,S_2=0\\mid Y=1)=0.10,\\;\np(S_1=0,S_2=1\\mid Y=1)=0.30,\\;\np(S_1=0,S_2=0\\mid Y=1)=0.10,\n$$\n$$\np(S_1=1,S_2=1\\mid Y=0)=0.05,\\;\np(S_1=1,S_2=0\\mid Y=0)=0.10,\\;\np(S_1=0,S_2=1\\mid Y=0)=0.15,\\;\np(S_1=0,S_2=0\\mid Y=0)=0.70.\n$$\n\nA patient presents with $X_1=30$, $X_2=15$, $S_1=1$, and $S_2=1$. Starting from Bayes’ theorem and the SNB grouped conditional independence assumption, derive the grouped-likelihood form $p(\\mathbf{X},\\mathbf{S}\\mid Y=y)$, instantiate it with the specified intra-group models, and compute the posterior probability $p(Y=1\\mid \\mathbf{X}=(30,15), \\mathbf{S}=(1,1))$ as a single real number. Express your final numerical answer as a decimal. Round your answer to $4$ significant figures.",
            "solution": "The objective is to compute the posterior probability of a patient having the disease ($Y=1$) given a set of measurements for biomarkers, $\\mathbf{X}$, and symptoms, $\\mathbf{S}$. The specific patient data are $\\mathbf{x} = \\begin{pmatrix}X_1\\\\X_2\\end{pmatrix} = \\begin{pmatrix}30\\\\15\\end{pmatrix}$ and $\\mathbf{s} = (S_1, S_2) = (1, 1)$. We seek to find $p(Y=1 \\mid \\mathbf{X}=\\mathbf{x}, \\mathbf{S}=\\mathbf{s})$.\n\nAccording to Bayes' theorem, the posterior probability is given by:\n$$\np(Y=y \\mid \\mathbf{X}=\\mathbf{x}, \\mathbf{S}=\\mathbf{s}) = \\frac{p(\\mathbf{X}=\\mathbf{x}, \\mathbf{S}=\\mathbf{s} \\mid Y=y) p(Y=y)}{p(\\mathbf{X}=\\mathbf{x}, \\mathbf{S}=\\mathbf{s})}\n$$\nThe denominator, $p(\\mathbf{X}=\\mathbf{x}, \\mathbf{S}=\\mathbf{s})$, is the marginal likelihood or evidence, which acts as a normalization constant. It is computed using the law of total probability:\n$$\np(\\mathbf{X}=\\mathbf{x}, \\mathbf{S}=\\mathbf{s}) = \\sum_{j \\in \\{0,1\\}} p(\\mathbf{X}=\\mathbf{x}, \\mathbf{S}=\\mathbf{s} \\mid Y=j) p(Y=j)\n$$\nThus, for the case $Y=1$, the posterior probability is:\n$$\np(Y=1 \\mid \\mathbf{x}, \\mathbf{s}) = \\frac{p(\\mathbf{x}, \\mathbf{s} \\mid Y=1) p(Y=1)}{p(\\mathbf{x}, \\mathbf{s} \\mid Y=1) p(Y=1) + p(\\mathbf{x}, \\mathbf{s} \\mid Y=0) p(Y=0)}\n$$\nThe Semi-Naive Bayes (SNB) model assumes that the feature groups $\\mathbf{X}$ and $\\mathbf{S}$ are conditionally independent given the class variable $Y$. This allows us to write the joint conditional likelihood as the product of the intra-group conditional likelihoods:\n$$\np(\\mathbf{X}=\\mathbf{x}, \\mathbf{S}=\\mathbf{s} \\mid Y=y) = p(\\mathbf{X}=\\mathbf{x} \\mid Y=y) p(\\mathbf{S}=\\mathbf{s} \\mid Y=y)\n$$\nTo simplify the calculation, we can compute the unnormalized posterior scores for each class, let's call them $A_1$ and $A_0$, and then normalize.\n$$\nA_y = p(\\mathbf{x} \\mid Y=y) p(\\mathbf{s} \\mid Y=y) p(Y=y)\n$$\nThen, the required posterior probability is $p(Y=1 \\mid \\mathbf{x}, \\mathbf{s}) = \\frac{A_1}{A_1 + A_0}$.\n\nFirst, we calculate the score $A_1$ for the class $Y=1$ (infection).\nThe prior is given as $p(Y=1)=0.25$.\nThe conditional likelihood for the symptoms $\\mathbf{s}=(1,1)$ is given as $p(S_1=1, S_2=1 \\mid Y=1) = 0.50$.\nThe conditional likelihood for the biomarkers $\\mathbf{X}$ is given by a bivariate MVN probability density function (PDF), $f(\\mathbf{x}; \\boldsymbol{\\mu}_1, \\boldsymbol{\\Sigma}_1)$, where $k=2$:\n$$\np(\\mathbf{x} \\mid Y=1) = \\frac{1}{(2\\pi)^{k/2} |\\boldsymbol{\\Sigma}_1|^{1/2}} \\exp\\left(-\\frac{1}{2} (\\mathbf{x}-\\boldsymbol{\\mu}_1)^T \\boldsymbol{\\Sigma}_1^{-1} (\\mathbf{x}-\\boldsymbol{\\mu}_1)\\right)\n$$\nThe parameters for $Y=1$ are $\\boldsymbol{\\mu}_1=\\begin{pmatrix}35\\\\20\\end{pmatrix}$ and $\\boldsymbol{\\Sigma}_1=\\begin{pmatrix}100 & 40\\\\ 40 & 25\\end{pmatrix}$.\nThe data vector is $\\mathbf{x}=\\begin{pmatrix}30\\\\15\\end{pmatrix}$, so the deviation is $\\mathbf{x}-\\boldsymbol{\\mu}_1 = \\begin{pmatrix}30-35\\\\15-20\\end{pmatrix} = \\begin{pmatrix}-5\\\\-5\\end{pmatrix}$.\nThe determinant of the covariance matrix is $|\\boldsymbol{\\Sigma}_1| = (100)(25) - (40)(40) = 2500 - 1600 = 900$.\nThe inverse is $\\boldsymbol{\\Sigma}_1^{-1} = \\frac{1}{900}\\begin{pmatrix}25 & -40\\\\-40 & 100\\end{pmatrix}$.\nThe squared Mahalanobis distance is:\n$$\n(\\mathbf{x}-\\boldsymbol{\\mu}_1)^T \\boldsymbol{\\Sigma}_1^{-1} (\\mathbf{x}-\\boldsymbol{\\mu}_1) = \\begin{pmatrix}-5 & -5\\end{pmatrix} \\frac{1}{900}\\begin{pmatrix}25 & -40\\\\-40 & 100\\end{pmatrix} \\begin{pmatrix}-5\\\\-5\\end{pmatrix} = \\frac{1}{900} \\begin{pmatrix}-5 & -5\\end{pmatrix} \\begin{pmatrix}75\\\\-300\\end{pmatrix} = \\frac{1}{900}(-375+1500) = \\frac{1125}{900} = 1.25\n$$\nThe likelihood is $p(\\mathbf{x} \\mid Y=1) = \\frac{1}{2\\pi\\sqrt{900}} \\exp\\left(-\\frac{1.25}{2}\\right) = \\frac{1}{60\\pi} \\exp(-0.625)$.\nNow, we compute $A_1$:\n$$\nA_1 = \\left(\\frac{1}{60\\pi} \\exp(-0.625)\\right) \\times 0.50 \\times 0.25 = \\frac{0.125}{60\\pi} \\exp(-0.625)\n$$\n$A_1 \\approx (0.002839655) \\times 0.125 \\approx 0.000354957$.\n\nNext, we calculate the score $A_0$ for the class $Y=0$ (no infection).\nThe prior is $p(Y=0)=0.75$.\nThe conditional likelihood for symptoms is $p(S_1=1, S_2=1 \\mid Y=0) = 0.05$.\nThe conditional likelihood for biomarkers is $f(\\mathbf{x}; \\boldsymbol{\\mu}_0, \\boldsymbol{\\Sigma}_0)$, with parameters for $Y=0$: $\\boldsymbol{\\mu}_0=\\begin{pmatrix}10\\\\5\\end{pmatrix}$ and $\\boldsymbol{\\Sigma}_0=\\begin{pmatrix}64 & 0\\\\ 0 & 16\\end{pmatrix}$.\nThe deviation is $\\mathbf{x}-\\boldsymbol{\\mu}_0 = \\begin{pmatrix}30-10\\\\15-5\\end{pmatrix} = \\begin{pmatrix}20\\\\10\\end{pmatrix}$.\nThe determinant is $|\\boldsymbol{\\Sigma}_0| = (64)(16) = 1024$.\nThe inverse is $\\boldsymbol{\\Sigma}_0^{-1} = \\begin{pmatrix}1/64 & 0\\\\ 0 & 1/16\\end{pmatrix}$.\nThe squared Mahalanobis distance is:\n$$\n(\\mathbf{x}-\\boldsymbol{\\mu}_0)^T \\boldsymbol{\\Sigma}_0^{-1} (\\mathbf{x}-\\boldsymbol{\\mu}_0) = \\begin{pmatrix}20 & 10\\end{pmatrix} \\begin{pmatrix}1/64 & 0\\\\0 & 1/16\\end{pmatrix} \\begin{pmatrix}20\\\\10\\end{pmatrix} = \\frac{20^2}{64} + \\frac{10^2}{16} = \\frac{400}{64} + \\frac{100}{16} = 6.25 + 6.25 = 12.5\n$$\nThe likelihood is $p(\\mathbf{x} \\mid Y=0) = \\frac{1}{2\\pi\\sqrt{1024}} \\exp\\left(-\\frac{12.5}{2}\\right) = \\frac{1}{64\\pi} \\exp(-6.25)$.\nNow, we compute $A_0$:\n$$\nA_0 = \\left(\\frac{1}{64\\pi} \\exp(-6.25)\\right) \\times 0.05 \\times 0.75 = \\frac{0.0375}{64\\pi} \\exp(-6.25)\n$$\n$A_0 \\approx (9.59869 \\times 10^{-6}) \\times 0.0375 \\approx 3.5995 \\times 10^{-7}$.\n\nFinally, we compute the posterior probability:\n$$\np(Y=1 \\mid \\mathbf{x}, \\mathbf{s}) = \\frac{A_1}{A_1 + A_0} = \\frac{0.000354957}{0.000354957 + 0.00000035995} = \\frac{0.000354957}{0.000355317} \\approx 0.9989869\n$$\nRounding the final answer to $4$ significant figures, we get $0.9990$.",
            "answer": "$$\\boxed{0.9990}$$"
        }
    ]
}