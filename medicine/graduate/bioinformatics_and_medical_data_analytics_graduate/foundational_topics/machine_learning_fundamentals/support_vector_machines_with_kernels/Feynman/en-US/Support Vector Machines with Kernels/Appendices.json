{
    "hands_on_practices": [
        {
            "introduction": "This first practice grounds the abstract theory of kernel Support Vector Machines (SVMs) in concrete calculation. By working through the decision function of a pre-trained model, you will see exactly how support vectors, their corresponding weights ($\\alpha_i$), and the kernel function collaborate to classify new data points. This exercise demystifies the prediction process and reinforces the geometric interpretation of the SVM in the high-dimensional feature space .",
            "id": "5227041",
            "problem": "A hospital deploys a binary Support Vector Machine (SVM) to triage suspected sepsis versus non-sepsis from standardized laboratory features. Each patient is represented by a feature vector $\\mathbf{x} \\in \\mathbb{R}^{2}$ consisting of normalized lactate and C-reactive protein measurements. The trained model uses a second-degree inhomogeneous polynomial kernel $K(\\mathbf{u},\\mathbf{v}) = \\left(1 + \\mathbf{u}^{\\top}\\mathbf{v}\\right)^{2}$ and has exactly $3$ support vectors with labels $y_{i} \\in \\{-1,+1\\}$ and Lagrange multipliers $\\alpha_{i} > 0$ given by:\n- $\\mathbf{x}_{1} = (1,0)$ with $y_{1} = +1$ and $\\alpha_{1} = \\tfrac{1}{2}$,\n- $\\mathbf{x}_{2} = (0,1)$ with $y_{2} = +1$ and $\\alpha_{2} = \\tfrac{1}{2}$,\n- $\\mathbf{x}_{3} = (1,1)$ with $y_{3} = -1$ and $\\alpha_{3} = 1$.\n\nThe learned offset is $b = 5$.\n\nThree new clinical samples arrive with features:\n- $\\mathbf{z}^{(1)} = (2,0)$,\n- $\\mathbf{z}^{(2)} = (0,0)$,\n- $\\mathbf{z}^{(3)} = (1,2)$.\n\nStarting only from the core definitions of the SVM classifier in a reproducing kernel Hilbert space and general geometric distance in an inner-product space, derive the decision function and the expression for the geometric distance from a point to the decision boundary hyperplane induced by this SVM. Then, for each $\\mathbf{z}^{(j)}$, compute:\n1. the signed decision value, \n2. the geometric distance to the decision boundary hyperplane,\n3. the predicted class label using the sign convention $\\operatorname{sign}(f(\\mathbf{z})) \\in \\{-1,+1\\}$.\n\nReport your final result as the ordered triples for $j \\in \\{1,2,3\\}$ in the sequence $\\left(f(\\mathbf{z}^{(1)}), d(\\mathbf{z}^{(1)}), \\hat{y}^{(1)}, f(\\mathbf{z}^{(2)}), d(\\mathbf{z}^{(2)}), \\hat{y}^{(2)}, f(\\mathbf{z}^{(3)}), d(\\mathbf{z}^{(3)}), \\hat{y}^{(3)}\\right)$. Provide exact analytic values; do not round or approximate. No physical units are required.",
            "solution": "The problem is valid. It presents a well-defined task within the standard mathematical framework of Support Vector Machines (SVMs) and provides all necessary parameters to compute a unique solution.\n\nThe core of the task is to use a given kernel SVM model to classify new data points. The model is defined by its kernel function, its support vectors $\\mathbf{x}_{i}$ with their labels $y_{i}$ and Lagrange multipliers $\\alpha_{i}$, and its offset term $b$.\n\nFirst, we derive the explicit form of the decision function, $f(\\mathbf{z})$. The general form of the SVM decision function for a point $\\mathbf{z}$ is given by:\n$$f(\\mathbf{z}) = \\sum_{i \\in \\text{SV}} \\alpha_{i} y_{i} K(\\mathbf{x}_{i}, \\mathbf{z}) + b$$\nwhere $\\text{SV}$ is the set of indices of the support vectors.\n\nThe problem provides the following parameters:\n- Kernel function: $K(\\mathbf{u}, \\mathbf{v}) = \\left(1 + \\mathbf{u}^{\\top}\\mathbf{v}\\right)^{2}$.\n- Support vectors and associated parameters:\n  - $\\mathbf{x}_{1} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$, $y_{1} = +1$, $\\alpha_{1} = \\frac{1}{2}$\n  - $\\mathbf{x}_{2} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$, $y_{2} = +1$, $\\alpha_{2} = \\frac{1}{2}$\n  - $\\mathbf{x}_{3} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$, $y_{3} = -1$, $\\alpha_{3} = 1$\n- Offset: $b = 5$.\n\nLet $\\mathbf{z} = \\begin{pmatrix} z_{1} \\\\ z_{2} \\end{pmatrix}$. We first compute the dot products $\\mathbf{x}_{i}^{\\top}\\mathbf{z}$:\n$\\mathbf{x}_{1}^{\\top}\\mathbf{z} = \\begin{pmatrix} 1  0 \\end{pmatrix} \\begin{pmatrix} z_{1} \\\\ z_{2} \\end{pmatrix} = z_{1}$\n$\\mathbf{x}_{2}^{\\top}\\mathbf{z} = \\begin{pmatrix} 0  1 \\end{pmatrix} \\begin{pmatrix} z_{1} \\\\ z_{2} \\end{pmatrix} = z_{2}$\n$\\mathbf{x}_{3}^{\\top}\\mathbf{z} = \\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} z_{1} \\\\ z_{2} \\end{pmatrix} = z_{1} + z_{2}$\n\nNow, we can write the kernel evaluations:\n$K(\\mathbf{x}_{1}, \\mathbf{z}) = (1 + z_{1})^{2}$\n$K(\\mathbf{x}_{2}, \\mathbf{z}) = (1 + z_{2})^{2}$\n$K(\\mathbf{x}_{3}, \\mathbf{z}) = (1 + z_{1} + z_{2})^{2}$\n\nSubstituting these into the decision function formula:\n$f(\\mathbf{z}) = \\alpha_{1} y_{1} K(\\mathbf{x}_{1}, \\mathbf{z}) + \\alpha_{2} y_{2} K(\\mathbf{x}_{2}, \\mathbf{z}) + \\alpha_{3} y_{3} K(\\mathbf{x}_{3}, \\mathbf{z}) + b$\n$f(\\mathbf{z}) = \\left(\\frac{1}{2}\\right)(+1)(1 + z_{1})^{2} + \\left(\\frac{1}{2}\\right)(+1)(1 + z_{2})^{2} + (1)(-1)(1 + z_{1} + z_{2})^{2} + 5$\n$$f(\\mathbf{z}) = \\frac{1}{2}(1 + z_{1})^{2} + \\frac{1}{2}(1 + z_{2})^{2} - (1 + z_{1} + z_{2})^{2} + 5$$\nThis is the explicit decision function for any point $\\mathbf{z} \\in \\mathbb{R}^{2}$.\n\nNext, we derive the expression for the geometric distance from a point to the decision boundary. The decision boundary is a hyperplane in the reproducing kernel Hilbert space (RKHS) defined by $\\mathbf{w}^{\\top}\\phi(\\mathbf{z}) + b = 0$, where $\\phi$ is the feature map associated with the kernel $K$, and $\\mathbf{w} = \\sum_{i \\in \\text{SV}} \\alpha_{i} y_{i} \\phi(\\mathbf{x}_{i})$. The decision function can be written as $f(\\mathbf{z}) = \\mathbf{w}^{\\top}\\phi(\\mathbf{z}) + b$.\n\nThe geometric distance $d(\\mathbf{z})$ from a point $\\phi(\\mathbf{z})$ to this hyperplane is given by:\n$$d(\\mathbf{z}) = \\frac{|\\mathbf{w}^{\\top}\\phi(\\mathbf{z}) + b|}{\\|\\mathbf{w}\\|} = \\frac{|f(\\mathbf{z})|}{\\|\\mathbf{w}\\|}$$\nWe need to compute the squared norm of the weight vector, $\\|\\mathbf{w}\\|^{2}$. Using the kernel trick:\n$\\|\\mathbf{w}\\|^{2} = \\mathbf{w}^{\\top}\\mathbf{w} = \\left(\\sum_{i} \\alpha_{i} y_{i} \\phi(\\mathbf{x}_{i})\\right)^{\\top} \\left(\\sum_{j} \\alpha_{j} y_{j} \\phi(\\mathbf{x}_{j})\\right)$\n$= \\sum_{i,j} \\alpha_{i} \\alpha_{j} y_{i} y_{j} (\\phi(\\mathbf{x}_{i})^{\\top} \\phi(\\mathbf{x}_{j})) = \\sum_{i,j} \\alpha_{i} \\alpha_{j} y_{i} y_{j} K(\\mathbf{x}_{i}, \\mathbf{x}_{j})$\n\nWe compute the necessary kernel values between support vectors:\n$K(\\mathbf{x}_{1},\\mathbf{x}_{1}) = (1 + 1)^{2} = 4$\n$K(\\mathbf{x}_{1},\\mathbf{x}_{2}) = (1 + 0)^{2} = 1$\n$K(\\mathbf{x}_{1},\\mathbf{x}_{3}) = (1 + 1)^{2} = 4$\n$K(\\mathbf{x}_{2},\\mathbf{x}_{2}) = (1 + 1)^{2} = 4$\n$K(\\mathbf{x}_{2},\\mathbf{x}_{3}) = (1 + 1)^{2} = 4$\n$K(\\mathbf{x}_{3},\\mathbf{x}_{3}) = (1 + 2)^{2} = 9$\n\nThe sum for $\\|\\mathbf{w}\\|^{2}$ expands to $9$ terms:\n$\\|\\mathbf{w}\\|^{2} = \\alpha_{1}^{2}y_{1}^{2}K_{11} + \\alpha_{2}^{2}y_{2}^{2}K_{22} + \\alpha_{3}^{2}y_{3}^{2}K_{33} + 2\\alpha_{1}\\alpha_{2}y_{1}y_{2}K_{12} + 2\\alpha_{1}\\alpha_{3}y_{1}y_{3}K_{13} + 2\\alpha_{2}\\alpha_{3}y_{2}y_{3}K_{23}$\nPlugging in the values:\n$\\alpha_{1}y_{1} = \\frac{1}{2}$, $\\alpha_{2}y_{2} = \\frac{1}{2}$, $\\alpha_{3}y_{3} = -1$.\n$\\|\\mathbf{w}\\|^{2} = (\\frac{1}{2})^{2}(4) + (\\frac{1}{2})^{2}(4) + (-1)^{2}(9) + 2(\\frac{1}{2})(\\frac{1}{2})(1) + 2(\\frac{1}{2})(-1)(4) + 2(\\frac{1}{2})(-1)(4)$\n$\\|\\mathbf{w}\\|^{2} = (\\frac{1}{4})(4) + (\\frac{1}{4})(4) + (1)(9) + \\frac{1}{2} - 4 - 4$\n$\\|\\mathbf{w}\\|^{2} = 1 + 1 + 9 + \\frac{1}{2} - 8 = 11 + \\frac{1}{2} - 8 = 3 + \\frac{1}{2} = \\frac{7}{2}$.\n\nSo, $\\|\\mathbf{w}\\| = \\sqrt{\\frac{7}{2}}$. The geometric distance is:\n$$d(\\mathbf{z}) = \\frac{|f(\\mathbf{z})|}{\\sqrt{7/2}} = |f(\\mathbf{z})|\\sqrt{\\frac{2}{7}}$$\n\nNow we compute the required values for each new sample $\\mathbf{z}^{(j)}$.\n\nFor $\\mathbf{z}^{(1)} = (2,0)$:\n$f(\\mathbf{z}^{(1)}) = \\frac{1}{2}(1+2)^{2} + \\frac{1}{2}(1+0)^{2} - (1+2+0)^{2} + 5 = \\frac{1}{2}(9) + \\frac{1}{2}(1) - 9 + 5 = \\frac{10}{2} - 4 = 5 - 4 = 1$.\n$d(\\mathbf{z}^{(1)}) = \\frac{|1|}{\\sqrt{7/2}} = \\sqrt{\\frac{2}{7}} = \\frac{\\sqrt{14}}{7}$.\n$\\hat{y}^{(1)} = \\operatorname{sign}(1) = +1$.\n\nFor $\\mathbf{z}^{(2)} = (0,0)$:\n$f(\\mathbf{z}^{(2)}) = \\frac{1}{2}(1+0)^{2} + \\frac{1}{2}(1+0)^{2} - (1+0+0)^{2} + 5 = \\frac{1}{2} + \\frac{1}{2} - 1 + 5 = 1 - 1 + 5 = 5$.\n$d(\\mathbf{z}^{(2)}) = \\frac{|5|}{\\sqrt{7/2}} = 5\\sqrt{\\frac{2}{7}} = \\frac{5\\sqrt{14}}{7}$.\n$\\hat{y}^{(2)} = \\operatorname{sign}(5) = +1$.\n\nFor $\\mathbf{z}^{(3)} = (1,2)$:\n$f(\\mathbf{z}^{(3)}) = \\frac{1}{2}(1+1)^{2} + \\frac{1}{2}(1+2)^{2} - (1+1+2)^{2} + 5 = \\frac{1}{2}(4) + \\frac{1}{2}(9) - 16 + 5 = 2 + \\frac{9}{2} - 11 = \\frac{13}{2} - 11 = \\frac{13-22}{2} = -\\frac{9}{2}$.\n$d(\\mathbf{z}^{(3)}) = \\frac{|-9/2|}{\\sqrt{7/2}} = \\frac{9}{2} \\sqrt{\\frac{2}{7}} = \\frac{9\\sqrt{2}}{2\\sqrt{7}} = \\frac{9\\sqrt{14}}{14}$.\n$\\hat{y}^{(3)} = \\operatorname{sign}(-9/2) = -1$.\n\nThe final results are the ordered triples $(f(\\mathbf{z}^{(j)}), d(\\mathbf{z}^{(j)}), \\hat{y}^{(j)})$ assembled into a single sequence for $j = 1, 2, 3$.",
            "answer": "$$ \\boxed{ \\begin{pmatrix} 1  \\frac{\\sqrt{14}}{7}  1  5  \\frac{5\\sqrt{14}}{7}  1  -\\frac{9}{2}  \\frac{9\\sqrt{14}}{14}  -1 \\end{pmatrix} } $$"
        },
        {
            "introduction": "Effective machine learning practice is not just about applying algorithms but also about making informed choices when tuning them. This exercise presents a classic scenario in bioinformatics: training a model on noisy data . By reasoning through the effect of the cost parameter $C$, you will develop a deeper intuition for the fundamental trade-off between fitting the training data and building a model that generalizes well to new, unseen samples.",
            "id": "2433208",
            "problem": "You are building a binary tumor-versus-normal classifier from microarray gene expression profiles using a Support Vector Machine (SVM) with the kernel trick, for example with a Radial Basis Function (RBF) kernel. The measurements are known to be noisy due to batch effects and hybridization variability, and some samples are suspected outliers. A collaborator proposes setting the SVM cost parameter $C$ to be very large, arguing that the model should “respect every measured profile,” akin to believing every observed deviation is biologically significant.\n\nWhich outcome is most consistent with first principles of margin maximization and empirical error penalization under this choice of $C$ on such noisy microarray data?\n\nA. A very large $C$ will maximize the geometric margin, effectively ignoring noise and outliers, yielding better generalization and fewer support vectors.\n\nB. A very large $C$ will heavily penalize any margin violations, forcing the decision boundary to contort to fit even noisy or outlying samples as if all were biologically meaningful, shrinking the margin, increasing variance, often increasing the number of support vectors, and degrading generalization to new patients.\n\nC. A very large $C$ will make the classifier equivalent to an unsupervised clustering method that ignores labels and groups samples purely by expression similarity.\n\nD. A very large $C$ will render the choice of kernel irrelevant, producing essentially the same decision boundary regardless of the kernel.\n\nE. A very large $C$ increases regularization strength, smoothing the decision boundary and increasing bias while decreasing variance.",
            "solution": "The problem statement is subjected to rigorous validation.\n\nStep 1: Extract Givens\n- **Problem Domain**: Binary classification (tumor-versus-normal).\n- **Data Source**: Microarray gene expression profiles.\n- **Data Characteristics**: The data are known to be \"noisy\" and contain \"suspected outliers\".\n- **Algorithm**: Support Vector Machine (SVM) with the kernel trick.\n- **Specific Kernel Example**: Radial Basis Function (RBF) kernel.\n- **Proposed Action**: Setting the SVM cost parameter, denoted as $C$, to a \"very large\" value.\n- **Rationale for Action**: A collaborator's argument is to \"respect every measured profile,\" implying a belief that all observed data points, including noise and outliers, are significant.\n- **Question**: To determine the most likely outcome of this action based on the \"first principles of margin maximization and empirical error penalization.\"\n\nStep 2: Validate Using Extracted Givens\n- **Scientific Grounding**: The problem is well-grounded in the principles of statistical learning theory and its application in computational biology. The concepts of SVMs, the cost parameter $C$, the kernel trick, noise, outliers, and the bias-variance trade-off are all standard and fundamental. The scenario described is a classic textbook example of model fitting challenges.\n- **Well-Posedness**: The problem is well-posed. It asks for the consequence of tuning a specific, well-defined hyperparameter ($C$) in a clearly described context (noisy data). The relationship between $C$ and the SVM's behavior is mathematically defined, allowing for a unique and logical conclusion.\n- **Objectivity**: The problem is stated objectively. It presents a scenario and a collaborator's flawed hypothesis, asking for a scientifically correct analysis of the outcome.\n\nThe problem statement is scientifically sound, well-posed, and objective. It contains no contradictions, ambiguities, or factual errors. Therefore, it is valid, and we may proceed with the derivation of the solution.\n\nThe core of the Support Vector Machine is an optimization problem. For the soft-margin linear SVM, the primal problem is formulated as follows:\n$$ \\min_{\\mathbf{w}, b, \\boldsymbol{\\xi}} \\frac{1}{2} \\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^{N} \\xi_i $$\nsubject to the constraints:\n$$ y_i (\\mathbf{w}^T \\phi(\\mathbf{x}_i) + b) \\ge 1 - \\xi_i, \\quad \\text{for } i = 1, \\dots, N $$\n$$ \\xi_i \\ge 0, \\quad \\text{for } i = 1, \\dots, N $$\nHere, $\\mathbf{x}_i$ are the input feature vectors (gene expression profiles), $y_i \\in \\{-1, +1\\}$ are the class labels (normal or tumor), $\\mathbf{w}$ and $b$ define the separating hyperplane, and $\\phi$ is the mapping to a higher-dimensional feature space, which is handled implicitly by the kernel trick (for example, with an RBF kernel). The variables $\\xi_i$ are slack variables that allow for margin violations.\n\nThe two terms in the objective function represent a fundamental trade-off:\n1.  **$\\frac{1}{2} \\|\\mathbf{w}\\|^2$**: This is the regularization term. Minimizing this term is equivalent to maximizing the geometric margin, which is $2/\\|\\mathbf{w}\\|$. A larger margin generally leads to better generalization and a simpler decision boundary. This term promotes a low-complexity model.\n2.  **$C \\sum_{i=1}^{N} \\xi_i$**: This is the empirical error or penalty term. It penalizes data points that violate the margin (i.e., for which $\\xi_i > 0$). The parameter $C > 0$ is a hyperparameter that controls the weight of this penalty.\n\nThe parameter $C$ thus balances the trade-off between maximizing the margin (finding a simple model) and minimizing the classification error on the training set.\n- A **small** value of $C$ places a low penalty on margin violations. The optimizer prioritizes a large margin, even if it means misclassifying some training points. This leads to a \"soft margin,\" a simpler decision boundary, higher bias, and lower variance (stronger regularization).\n- A **large** value of $C$ places a high penalty on margin violations. The optimizer will go to great lengths to classify every training point correctly, even at the cost of shrinking the margin. This leads to a \"hard margin\" behavior, a more complex decision boundary that closely fits the training data, lower bias, and higher variance (weaker regularization).\n\nIn the given problem, the data is known to be noisy and contains outliers. Setting $C$ to a \"very large\" value means there is an enormous penalty for any point that is misclassified or falls within the margin. The SVM algorithm is thus forced to treat every single data point—including the noise and outliers—as crucially important. To accommodate these noisy and outlying points, the decision boundary will have to become highly complex and contorted. This is the definition of **overfitting**.\n\nThe specific consequences are:\n- **Margin**: The margin will be very small, as it must shrink to allow the complex boundary to snake around the data points.\n- **Variance**: The model will have high variance. It has learned the specific noise of the training set, not the underlying true separation between tumor and normal profiles.\n- **Generalization**: Its performance on new, unseen data (new patients) will be poor. This is degraded generalization.\n- **Support Vectors**: The number of support vectors will typically increase. In a highly overfit model, a large fraction of the training data points can become support vectors, as they are all needed to define the intricate decision boundary.\n\nNow, we evaluate the provided options against this correct physical and mathematical understanding.\n\n**A. A very large $C$ will maximize the geometric margin, effectively ignoring noise and outliers, yielding better generalization and fewer support vectors.**\nThis is a direct contradiction of the principles. A large $C$ *minimizes* the margin in the presence of noise, *pays extreme attention* to noise and outliers, *degrades* generalization, and typically *increases* the number of support vectors. This describes the behavior of a small $C$, not a large one.\nVerdict: **Incorrect**.\n\n**B. A very large $C$ will heavily penalize any margin violations, forcing the decision boundary to contort to fit even noisy or outlying samples as if all were biologically meaningful, shrinking the margin, increasing variance, often increasing the number of support vectors, and degrading generalization to new patients.**\nThis statement is perfectly aligned with our derivation. A very large $C$ imposes a heavy penalty on errors ($\\sum \\xi_i$), forcing the model to overfit the noisy training data. This leads to a contorted, low-margin boundary. Such a model has high variance and does not generalize well. The complexity of the boundary requires more data points to define it, thus increasing the number of support vectors.\nVerdict: **Correct**.\n\n**C. A very large $C$ will make the classifier equivalent to an unsupervised clustering method that ignores labels and groups samples purely by expression similarity.**\nThis is fundamentally incorrect. SVM is a supervised learning algorithm. The penalty term $C \\sum \\xi_i$ is computed with respect to the given labels $y_i$ via the constraint $y_i(\\mathbf{w}^T \\phi(\\mathbf{x}_i) + b) \\ge 1 - \\xi_i$. A large $C$ makes the model *more* sensitive to the labels, not less. It will never become an unsupervised method, which by definition does not use labels.\nVerdict: **Incorrect**.\n\n**D. A very large $C$ will render the choice of kernel irrelevant, producing essentially the same decision boundary regardless of the kernel.**\nThis is false. The kernel defines the feature space where the separation occurs. A large $C$ forces the model to find a complex separating boundary *within the chosen feature space*. If one uses a linear kernel, the boundary will still be a hyperplane (in the original space), though one that is poorly chosen to accommodate outliers. If one uses an RBF kernel, the boundary will be a highly non-linear, Gaussian-based surface. The resulting decision boundaries will be drastically different. The combination of a powerful kernel (like RBF) and a very large $C$ on noisy data is a classic recipe for severe overfitting.\nVerdict: **Incorrect**.\n\n**E. A very large $C$ increases regularization strength, smoothing the decision boundary and increasing bias while decreasing variance.**\nThis statement reverses the role of $C$. The parameter $C$ is inversely proportional to the strength of the regularization. A large $C$ means *weak* regularization. Weak regularization leads to a *less smooth* (more complex) boundary, *lower* bias (on the training set), and *higher* variance. This option incorrectly describes the effect of a small $C$.\nVerdict: **Incorrect**.",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "The true power of kernel methods lies in their ability to incorporate domain-specific knowledge directly into the model. This advanced practice challenges you to move from being a user of standard kernels to a designer of custom ones . You will construct a weighted string kernel for a splice-site prediction task, embedding biological intuition into the feature space, and then verify its mathematical validity by computing the Gram matrix.",
            "id": "2433200",
            "problem": "You are given a binary classification setting relevant to splice-site prediction in computational biology, modeled for a Support Vector Machine (SVM) using the kernel trick. Consider DNA sequences over the alphabet $\\{\\text{A}, \\text{C}, \\text{G}, \\text{T}\\}$ together with a same-length annotation mask over the alphabet $\\{\\text{E}, \\text{I}\\}$ indicating exon and intron positions, respectively. Define a weighted $k$-spectrum string kernel that upweights matches occurring fully within exon regions, as follows.\n\nLet $k \\in \\mathbb{N}$ be fixed. For a sequence $s$ of length $n$ and its mask $m$ of length $n$, for each starting position $p \\in \\{0,1,\\dots,n-k\\}$ define the $k$-mer window $s[p:p+k]$ and its window mask $m[p:p+k]$. Define a positional window weight\n$$\ng_{(s,m)}(p) \\;=\\;\n\\begin{cases}\nw_E  \\text{if all symbols of } m[p:p+k] \\text{ are } \\text{E},\\\\\nw_I  \\text{if all symbols of } m[p:p+k] \\text{ are } \\text{I},\\\\\n0  \\text{otherwise}.\n\\end{cases}\n$$\nFor any $k$-mer $u \\in \\{\\text{A},\\text{C},\\text{G},\\text{T}\\}^k$, define the weighted feature map component\n$$\n\\phi_u(s,m) \\;=\\; \\sum_{p=0}^{n-k} g_{(s,m)}(p)\\,\\mathbf{1}\\!\\left[s[p:p+k] = u\\right],\n$$\nwhere $\\mathbf{1}[\\cdot]$ is the indicator function. The kernel between two annotated sequences $(s,m)$ and $(t,n)$ is the inner product\n$$\nK\\big((s,m),(t,n)\\big) \\;=\\; \\sum_{u \\in \\{\\text{A},\\text{C},\\text{G},\\text{T}\\}^k} \\phi_u(s,m)\\,\\phi_u(t,n).\n$$\n\nUse the fixed parameter values $k=2$, $w_E=2$, and $w_I=1$. Consider the following annotated DNA sequences of equal length, each given as a pair $(\\text{sequence}, \\text{mask})$:\n- $A$: $($\"ACGTAC\"$,$ \"EEEIII\"$)$,\n- $B$: $($\"ACGTTC\"$,$ \"EEIIII\"$)$,\n- $C$: $($\"TTGTAC\"$,$ \"IIIIEE\"$)$,\n- $D$: $($\"AAAAAA\"$,$ \"IIIIII\"$)$,\n- $E$: $($\"AAAAAA\"$,$ \"EEEEEE\"$)$.\n\nYour tasks are:\n- Compute the kernel values $K(A,A)$, $K(A,B)$, $K(B,C)$, and $K(D,E)$.\n- Form the Gram matrix $G$ for the set $\\{A,B,C\\}$ with entries $G_{ij} = K(S_i,S_j)$ for $S_1=A$, $S_2=B$, $S_3=C$, and determine whether $G$ is positive semidefinite in the sense that all its eigenvalues are greater than or equal to $0$ (within standard floating-point rounding).\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the exact order:\n$[$$K(A,A)$$,$$K(A,B)$$,$$K(B,C)$$,$$K(D,E)$$,$$\\text{is\\_PSD}$$$]$, where $\\text{is\\_PSD}$ is a boolean. For example, the output format must look like $[$$x$$,$$y$$,$$z$$,$$u$$,$$\\text{True}$$]$ with no spaces. All numerical answers are unitless. The test suite consists of the four kernel evaluations specified above and the positive semidefinite check on the Gram matrix for $\\{A,B,C\\}$, covering a typical case, cross-exon/intron interactions, an all-intron versus all-exon contrast, and a matrix-level validity check.",
            "solution": "The problem is scientifically grounded, well-posed, and objective. It presents a valid computational task based on established principles of machine learning and bioinformatics. All necessary data and definitions are provided, and there are no contradictions or ambiguities. We shall proceed with the solution.\n\nThe core of the problem is the computation of a weighted $k$-spectrum string kernel. The kernel $K$ between two annotated sequences $(s, m)$ and $(t, n')$ is defined as the inner product of their feature vectors, $K\\big((s,m),(t,n')\\big) = \\langle \\phi(s,m), \\phi(t,n') \\rangle$. This can be written as:\n$$\nK\\big((s,m),(t,n')\\big) = \\sum_{u \\in \\{\\text{A},\\text{C},\\text{G},\\text{T}\\}^k} \\phi_u(s,m)\\,\\phi_u(t,n')\n$$\nwhere $\\phi_u(s, m)$ is the weighted count of the $k$-mer $u$ in sequence $s$ according to its mask $m$. A more direct, computationally efficient formulation, which avoids an explicit enumeration of all possible $k$-mers $u$, is given by:\n$$\nK\\big((s,m),(t,n')\\big) = \\sum_{p=0}^{|s|-k} \\sum_{q=0}^{|t|-k} g_{(s,m)}(p) g_{(t,n')}(q) \\mathbf{1}\\!\\left[s[p:p+k] = t[q:q+k]\\right]\n$$\nWe will utilize the feature map summation approach, as it is conceptually clear and equivalent. The fixed parameters are given as $k=2$, $w_E=2$, and $w_I=1$. All sequences have length $n=6$, so the number of $2$-mers in each is $n-k+1 = 6-2+1=5$, with starting positions $p \\in \\{0, 1, 2, 3, 4\\}$.\n\nFirst, we must compute the feature maps $\\phi(S)$ for each annotated sequence $S \\in \\{A, B, C, D, E\\}$. The feature map component $\\phi_u(S)$ for a $2$-mer $u$ is the sum of its positional weights $g(p)$ over all occurrences in the sequence.\n\n**1. Feature Map Calculations**\n\nFor each sequence, we list the $2$-mers, their mask windows, and the resulting positional weights $g(p)$.\n\n- **Sequence A**: $s_A = \\text{\"ACGTAC\"}$, $m_A = \\text{\"EEEIII\"}$\n  - $p=0$: $s_A[0:2]$=\"AC\", $m_A[0:2]$=\"EE\" (all 'E') $\\implies g_A(0) = w_E = 2$.\n  - $p=1$: $s_A[1:3]$=\"CG\", $m_A[1:3]$=\"EE\" (all 'E') $\\implies g_A(1) = w_E = 2$.\n  - $p=2$: $s_A[2:4]$=\"GT\", $m_A[2:4]$=\"EI\" (mixed) $\\implies g_A(2) = 0$.\n  - $p=3$: $s_A[3:5]$=\"TA\", $m_A[3:5]$=\"II\" (all 'I') $\\implies g_A(3) = w_I = 1$.\n  - $p=4$: $s_A[4:6]$=\"AC\", $m_A[4:6]$=\"II\" (all 'I') $\\implies g_A(4) = w_I = 1$.\n  The non-zero components of the feature map $\\phi(A)$ are:\n  $\\phi_{\\text{AC}}(A) = g_A(0) + g_A(4) = 2 + 1 = 3$.\n  $\\phi_{\\text{CG}}(A) = g_A(1) = 2$.\n  $\\phi_{\\text{TA}}(A) = g_A(3) = 1$.\n\n- **Sequence B**: $s_B = \\text{\"ACGTTC\"}$, $m_B = \\text{\"EEIIII\"}$\n  - $p=0$: \"AC\", \"EE\" (all 'E') $\\implies g_B(0) = w_E = 2$.\n  - $p=1$: \"CG\", \"EI\" (mixed) $\\implies g_B(1) = 0$.\n  - $p=2$: \"GT\", \"II\" (all 'I') $\\implies g_B(2) = w_I = 1$.\n  - $p=3$: \"TT\", \"II\" (all 'I') $\\implies g_B(3) = w_I = 1$.\n  - $p=4$: \"TC\", \"II\" (all 'I') $\\implies g_B(4) = w_I = 1$.\n  The non-zero components of $\\phi(B)$ are:\n  $\\phi_{\\text{AC}}(B) = 2$, $\\phi_{\\text{GT}}(B) = 1$, $\\phi_{\\text{TT}}(B) = 1$, $\\phi_{\\text{TC}}(B) = 1$.\n\n- **Sequence C**: $s_C = \\text{\"TTGTAC\"}$, $m_C = \\text{\"IIIIEE\"}$\n  - $p=0$: \"TT\", \"II\" (all 'I') $\\implies g_C(0) = w_I = 1$.\n  - $p=1$: \"TG\", \"II\" (all 'I') $\\implies g_C(1) = w_I = 1$.\n  - $p=2$: \"GT\", \"II\" (all 'I') $\\implies g_C(2) = w_I = 1$.\n  - $p=3$: \"TA\", \"IE\" (mixed) $\\implies g_C(3) = 0$.\n  - $p=4$: \"AC\", \"EE\" (all 'E') $\\implies g_C(4) = w_E = 2$.\n  The non-zero components of $\\phi(C)$ are:\n  $\\phi_{\\text{TT}}(C) = 1$, $\\phi_{\\text{TG}}(C) = 1$, $\\phi_{\\text{GT}}(C) = 1$, $\\phi_{\\text{AC}}(C) = 2$.\n\n- **Sequence D**: $s_D = \\text{\"AAAAAA\"}$, $m_D = \\text{\"IIIIII\"}$\n  - For all $p \\in \\{0, 1, 2, 3, 4\\}$, the $2$-mer is \"AA\" and the mask window is \"II\".\n  - Thus, $g_D(p) = w_I = 1$ for all $p$.\n  The only non-zero component of $\\phi(D)$ is:\n  $\\phi_{\\text{AA}}(D) = \\sum_{p=0}^4 1 = 5$.\n\n- **Sequence E**: $s_E = \\text{\"AAAAAA\"}$, $m_E = \\text{\"EEEEEE\"}$\n  - For all $p \\in \\{0, 1, 2, 3, 4\\}$, the $2$-mer is \"AA\" and the mask window is \"EE\".\n  - Thus, $g_E(p) = w_E = 2$ for all $p$.\n  The only non-zero component of $\\phi(E)$ is:\n  $\\phi_{\\text{AA}}(E) = \\sum_{p=0}^4 2 = 10$.\n\n**2. Kernel Value Computations**\n\nWe now compute the specified kernel values.\n\n- $K(A,A) = \\langle\\phi(A), \\phi(A)\\rangle = \\sum_u (\\phi_u(A))^2 = (\\phi_{\\text{AC}}(A))^2 + (\\phi_{\\text{CG}}(A))^2 + (\\phi_{\\text{TA}}(A))^2 = 3^2 + 2^2 + 1^2 = 9 + 4 + 1 = 14$.\n\n- $K(A,B) = \\langle\\phi(A), \\phi(B)\\rangle = \\sum_u \\phi_u(A)\\phi_u(B)$. The only common $2$-mer with non-zero weights is \"AC\".\n  $K(A,B) = \\phi_{\\text{AC}}(A)\\phi_{\\text{AC}}(B) = 3 \\times 2 = 6$.\n\n- $K(B,C) = \\langle\\phi(B), \\phi(C)\\rangle$. The common $2$-mers are \"AC\", \"GT\", and \"TT\".\n  $K(B,C) = \\phi_{\\text{AC}}(B)\\phi_{\\text{AC}}(C) + \\phi_{\\text{GT}}(B)\\phi_{\\text{GT}}(C) + \\phi_{\\text{TT}}(B)\\phi_{\\text{TT}}(C) = (2 \\times 2) + (1 \\times 1) + (1 \\times 1) = 4 + 1 + 1 = 6$.\n\n- $K(D,E) = \\langle\\phi(D), \\phi(E)\\rangle$. The only common $2$-mer is \"AA\".\n  $K(D,E) = \\phi_{\\text{AA}}(D)\\phi_{\\text{AA}}(E) = 5 \\times 10 = 50$.\n\n**3. Gram Matrix and Positive Semidefinite Check**\n\nThe Gram matrix $G$ for the set $\\{A, B, C\\}$ is a $3 \\times 3$ symmetric matrix with entries $G_{ij} = K(S_i, S_j)$, where $S_1=A, S_2=B, S_3=C$. We have already computed the off-diagonal entries $K(A,B)=6$ and $K(B,C)=6$. We need $K(A,C)$, $K(B,B)$, and $K(C,C)$.\n\n- $K(A,C) = \\langle\\phi(A), \\phi(C)\\rangle$. The only common $2$-mer is \"AC\".\n  $K(A,C) = \\phi_{\\text{AC}}(A)\\phi_{\\text{AC}}(C) = 3 \\times 2 = 6$.\n\n- $K(B,B) = \\langle\\phi(B), \\phi(B)\\rangle = \\sum_u (\\phi_u(B))^2 = (\\phi_{\\text{AC}}(B))^2 + (\\phi_{\\text{GT}}(B))^2 + (\\phi_{\\text{TT}}(B))^2 + (\\phi_{\\text{TC}}(B))^2 = 2^2 + 1^2 + 1^2 + 1^2 = 4 + 1 + 1 + 1 = 7$.\n\n- $K(C,C) = \\langle\\phi(C), \\phi(C)\\rangle = \\sum_u (\\phi_u(C))^2 = (\\phi_{\\text{AC}}(C))^2 + (\\phi_{\\text{GT}}(C))^2 + (\\phi_{\\text{TT}}(C))^2 + (\\phi_{\\text{TG}}(C))^2 = 2^2 + 1^2 + 1^2 + 1^2 = 4 + 1 + 1 + 1 = 7$.\n\nThe Gram matrix is therefore:\n$$\nG = \\begin{pmatrix} K(A,A)  K(A,B)  K(A,C) \\\\ K(B,A)  K(B,B)  K(B,C) \\\\ K(C,A)  K(C,B)  K(C,C) \\end{pmatrix} = \\begin{pmatrix} 14  6  6 \\\\ 6  7  6 \\\\ 6  6  7 \\end{pmatrix}\n$$\nTo determine if $G$ is positive semidefinite (PSD), we must check if all its eigenvalues $\\lambda$ are non-negative. We solve the characteristic equation $\\det(G - \\lambda I) = 0$.\nThe characteristic polynomial is $(\\lambda-1)(\\lambda^2 - 27\\lambda + 110) = 0$, which simplifies to $(\\lambda-1)(\\lambda-5)(\\lambda-22)=0$.\nThe eigenvalues are $\\lambda_1 = 1$, $\\lambda_2 = 5$, and $\\lambda_3 = 22$. Since all eigenvalues are strictly positive, G is not only positive semidefinite but also positive definite. This is expected, as any kernel defined as an inner product in a feature space is a valid Mercer kernel and will always produce a positive semidefinite Gram matrix. The result for `is_PSD` is `True`.\n\nSummary of results:\n- $K(A,A) = 14$\n- $K(A,B) = 6$\n- $K(B,C) = 6$\n- $K(D,E) = 50$\n- `is_PSD` = `True`",
            "answer": "[14,6,6,50,True]"
        }
    ]
}