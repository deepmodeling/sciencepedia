{
    "hands_on_practices": [
        {
            "introduction": "Cost-complexity pruning creates a sequence of optimally pruned trees, each corresponding to a specific value of the complexity parameter $\\alpha$. This exercise  guides you through the fundamental calculation of the critical $\\alpha$ value, or 'weakest link,' at which a specific subtree becomes a candidate for pruning. Mastering this derivation is key to understanding how the entire pruning path is constructed from first principles.",
            "id": "4615673",
            "problem": "A hospital system develops a predictive model for $30$-day readmission using Classification and Regression Trees (CART). The training labels are binary outcomes $y_i \\in \\{0,1\\}$, and each terminal node (leaf) $\\ell$ of a tree $T$ fits a Bernoulli mean via Maximum Likelihood Estimation (MLE). The empirical risk of a leaf $\\ell$ is defined as the negative log-likelihood\n$$\nR(\\ell) \\equiv -\\sum_{i \\in \\ell} \\big( y_i \\ln(\\hat{p}_\\ell) + (1-y_i)\\ln(1-\\hat{p}_\\ell) \\big),\n$$\nwhere $\\hat{p}_\\ell$ is the MLE for the Bernoulli mean in $\\ell$, and the risk of a tree $T$ is $R(T) \\equiv \\sum_{\\ell \\in \\mathcal{L}(T)} R(\\ell)$, with $\\mathcal{L}(T)$ the set of leaves. In cost-complexity pruning, one evaluates the objective\n$$\nC_\\alpha(T) \\equiv R(T) + \\alpha |\\mathcal{L}(T)|,\n$$\nwhere $\\alpha \\ge 0$ is a complexity parameter and $|\\mathcal{L}(T)|$ is the leaf count. Consider an internal node $t$ whose subtree $T_t$ has $|\\mathcal{L}(T_t)|$ leaves. Collapsing $T_t$ to a single leaf $t$ reduces the leaf count by $|\\mathcal{L}(T_t)|-1$ and changes the risk from $R(T_t)$ to $R(t)$, where $R(t)$ is computed as the negative log-likelihood of a single Bernoulli leaf fitted by MLE to all samples under $T_t$.\n\nStarting only from the above definitions, derive the weakest-link value $\\alpha_t$ for node $t$ by equating the increase in risk to the decrease in leaf count when collapsing $T_t$. Then, in a realistic medical cohort under $T_t$, suppose the subtree $T_t$ has $|\\mathcal{L}(T_t)| = 3$ leaves with per-leaf counts $(n_\\ell,k_\\ell)$ of total patients $n_\\ell$ and readmissions $k_\\ell$ given by\n- leaf $1$: $(n_1,k_1) = (50,5)$,\n- leaf $2$: $(n_2,k_2) = (30,9)$,\n- leaf $3$: $(n_3,k_3) = (20,4)$.\nUse the MLE $\\hat{p}_\\ell = k_\\ell/n_\\ell$ within each leaf and $\\hat{p}_t = (\\sum_{\\ell} k_\\ell)/(\\sum_{\\ell} n_\\ell)$ in the collapsed node $t$. Express the final answer as a single closed-form analytic expression for $\\alpha_t$ in terms of natural logarithms, with no numerical approximation. Do not include any units. No rounding is required.",
            "solution": "The principles of cost-complexity pruning for Classification and Regression Trees (CART) define a critical value of the complexity parameter, $\\alpha_t$, for each internal node $t$. This value, known as the weakest-link value, represents the threshold at which pruning the subtree $T_t$ rooted at $t$ becomes as costly as not pruning it. We find $\\alpha_t$ by equating the cost-complexity of the unpruned subtree, $C_\\alpha(T_t)$, with the cost-complexity of the pruned node $t$, $C_\\alpha(t)$.\n\nThe cost-complexity is defined as $C_\\alpha(T) \\equiv R(T) + \\alpha |\\mathcal{L}(T)|$, where $R(T)$ is the tree's risk and $|\\mathcal{L}(T)|$ is the number of leaves.\nFor the unpruned subtree $T_t$, the cost is $C_\\alpha(T_t) = R(T_t) + \\alpha |\\mathcal{L}(T_t)|$.\nIf we prune the subtree $T_t$, node $t$ becomes a leaf. The resulting tree has a cost associated with this new leaf, which is $C_\\alpha(t) = R(t) + \\alpha \\cdot 1$. Note that $|\\mathcal{L}(t)| = 1$ since the collapsed node is a single leaf.\n\nWe set these two costs equal to find the value of $\\alpha_t$ at which the trade-off is balanced:\n$$R(T_t) + \\alpha_t |\\mathcal{L}(T_t)| = R(t) + \\alpha_t$$\nRearranging the terms to solve for $\\alpha_t$:\n$$\\alpha_t \\big( |\\mathcal{L}(T_t)| - 1 \\big) = R(t) - R(T_t)$$\n$$\\alpha_t = \\frac{R(t) - R(T_t)}{|\\mathcal{L}(T_t)| - 1}$$\nThe numerator, $R(t) - R(T_t)$, is the increase in empirical risk due to pruning. The denominator, $|\\mathcal{L}(T_t)| - 1$, is the decrease in the number of leaves.\n\nThe risk of a single leaf $\\ell$ is the negative log-likelihood of the Bernoulli model fitted to the data within that leaf. For a leaf with $n_\\ell$ total samples and $k_\\ell$ positive outcomes (readmissions), the Maximum Likelihood Estimate (MLE) for the Bernoulli mean is $\\hat{p}_\\ell = k_\\ell / n_\\ell$. The risk is:\n$$R(\\ell) \\equiv -\\sum_{i \\in \\ell} \\big( y_i \\ln(\\hat{p}_\\ell) + (1-y_i)\\ln(1-\\hat{p}_\\ell) \\big)$$\nThis sum can be simplified by counting the number of positive ($y_i=1$) and negative ($y_i=0$) outcomes:\n$$R(\\ell) = - \\big[ k_\\ell \\ln(\\hat{p}_\\ell) + (n_\\ell - k_\\ell) \\ln(1 - \\hat{p}_\\ell) \\big]$$\n$$R(\\ell) = - \\left[ k_\\ell \\ln\\left(\\frac{k_\\ell}{n_\\ell}\\right) + (n_\\ell - k_\\ell) \\ln\\left(\\frac{n_\\ell - k_\\ell}{n_\\ell}\\right) \\right]$$\n\nThe problem provides data for a subtree $T_t$ with $|\\mathcal{L}(T_t)| = 3$ leaves.\nThe risk of the unpruned subtree $T_t$ is the sum of the risks of its leaves: $R(T_t) = R(\\ell_1) + R(\\ell_2) + R(\\ell_3)$.\nThe data for the three leaves are:\n- Leaf $1$: $(n_1, k_1) = (50, 5)$. So, $\\hat{p}_1 = 5/50 = 1/10$.\n  $$R(\\ell_1) = - \\left[ 5 \\ln\\left(\\frac{5}{50}\\right) + (50 - 5) \\ln\\left(1 - \\frac{5}{50}\\right) \\right] = - \\left[ 5 \\ln\\left(\\frac{1}{10}\\right) + 45 \\ln\\left(\\frac{9}{10}\\right) \\right]$$\n- Leaf $2$: $(n_2, k_2) = (30, 9)$. So, $\\hat{p}_2 = 9/30 = 3/10$.\n  $$R(\\ell_2) = - \\left[ 9 \\ln\\left(\\frac{9}{30}\\right) + (30 - 9) \\ln\\left(1 - \\frac{9}{30}\\right) \\right] = - \\left[ 9 \\ln\\left(\\frac{3}{10}\\right) + 21 \\ln\\left(\\frac{7}{10}\\right) \\right]$$\n- Leaf $3$: $(n_3, k_3) = (20, 4)$. So, $\\hat{p}_3 = 4/20 = 1/5$.\n  $$R(\\ell_3) = - \\left[ 4 \\ln\\left(\\frac{4}{20}\\right) + (20 - 4) \\ln\\left(1 - \\frac{4}{20}\\right) \\right] = - \\left[ 4 \\ln\\left(\\frac{1}{5}\\right) + 16 \\ln\\left(\\frac{4}{5}\\right) \\right]$$\n\nThe total risk of the subtree is:\n$$R(T_t) = - \\left[ 5 \\ln\\left(\\frac{1}{10}\\right) + 45 \\ln\\left(\\frac{9}{10}\\right) \\right] - \\left[ 9 \\ln\\left(\\frac{3}{10}\\right) + 21 \\ln\\left(\\frac{7}{10}\\right) \\right] - \\left[ 4 \\ln\\left(\\frac{1}{5}\\right) + 16 \\ln\\left(\\frac{4}{5}\\right) \\right]$$\n\nFor the pruned node $t$, we aggregate all samples from the subtree. The total sample count is $n_t = n_1 + n_2 + n_3 = 50 + 30 + 20 = 100$. The total readmission count is $k_t = k_1 + k_2 + k_3 = 5 + 9 + 4 = 18$.\nThe MLE for the pruned node is $\\hat{p}_t = k_t / n_t = 18 / 100 = 9/50$.\nThe risk of the pruned node $t$ is:\n$$R(t) = - \\left[ k_t \\ln(\\hat{p}_t) + (n_t - k_t) \\ln(1 - \\hat{p}_t) \\right]$$\n$$R(t) = - \\left[ 18 \\ln\\left(\\frac{18}{100}\\right) + (100 - 18) \\ln\\left(1 - \\frac{18}{100}\\right) \\right]$$\n$$R(t) = - \\left[ 18 \\ln\\left(\\frac{9}{50}\\right) + 82 \\ln\\left(\\frac{41}{50}\\right) \\right]$$\n\nNow, we can compute $\\alpha_t$. The decrease in leaf count is $|\\mathcal{L}(T_t)| - 1 = 3 - 1 = 2$.\n$$\\alpha_t = \\frac{R(t) - R(T_t)}{2} = \\frac{1}{2} \\big(R(t) - (R(\\ell_1) + R(\\ell_2) + R(\\ell_3))\\big)$$\nSubstituting the expressions for the risk terms:\n$$\\alpha_t = \\frac{1}{2} \\left( - \\left[ 18 \\ln\\left(\\frac{9}{50}\\right) + 82 \\ln\\left(\\frac{41}{50}\\right) \\right] - \\left( - \\left[ 5 \\ln\\left(\\frac{1}{10}\\right) + 45 \\ln\\left(\\frac{9}{10}\\right) \\right] - \\left[ 9 \\ln\\left(\\frac{3}{10}\\right) + 21 \\ln\\left(\\frac{7}{10}\\right) \\right] - \\left[ 4 \\ln\\left(\\frac{1}{5}\\right) + 16 \\ln\\left(\\frac{4}{5}\\right) \\right] \\right) \\right)$$\nSimplifying the signs yields the final closed-form expression for $\\alpha_t$:\n$$\\alpha_t = \\frac{1}{2} \\left[ \\left( 5 \\ln\\left(\\frac{1}{10}\\right) + 45 \\ln\\left(\\frac{9}{10}\\right) \\right) + \\left( 9 \\ln\\left(\\frac{3}{10}\\right) + 21 \\ln\\left(\\frac{7}{10}\\right) \\right) + \\left( 4 \\ln\\left(\\frac{1}{5}\\right) + 16 \\ln\\left(\\frac{4}{5}\\right) \\right) - \\left( 18 \\ln\\left(\\frac{9}{50}\\right) + 82 \\ln\\left(\\frac{41}{50}\\right) \\right) \\right]$$\nThis expression is the exact analytical value for $\\alpha_t$ as required.",
            "answer": "$$\\boxed{\\frac{1}{2} \\left[ \\left( 5 \\ln\\left(\\frac{1}{10}\\right) + 45 \\ln\\left(\\frac{9}{10}\\right) \\right) + \\left( 9 \\ln\\left(\\frac{3}{10}\\right) + 21 \\ln\\left(\\frac{7}{10}\\right) \\right) + \\left( 4 \\ln\\left(\\frac{1}{5}\\right) + 16 \\ln\\left(\\frac{4}{5}\\right) \\right) - \\left( 18 \\ln\\left(\\frac{9}{50}\\right) + 82 \\ln\\left(\\frac{41}{50}\\right) \\right) \\right]}$$"
        },
        {
            "introduction": "Once a pruning path is generated, the practical challenge is to select the single best tree that balances complexity and accuracy. This practice  simulates this process using K-fold cross-validation results from a clinical genomics study. You will apply the widely-used 'one-standard-error rule' to navigate the bias-variance trade-off and choose a final model, a core skill for building generalizable predictors.",
            "id": "4615670",
            "problem": "In a clinical genomics study that integrates Electronic Health Records (EHR) and whole-exome variants to predict $30$-day readmission ($0$ for no readmission, $1$ for readmission), a Classification and Regression Tree (CART) classifier is trained and pruned using cost-complexity pruning. The cost-complexity pruning parameter is denoted by $\\alpha \\geq 0$, and increasing $\\alpha$ yields strictly simpler subtrees along a nested pruning path. To estimate generalization error, $K$-fold cross-validation (CV) is performed with $K = 10$, and the fold-wise misclassification error (expressed as a decimal fraction) is recorded for each $\\alpha$ on the pruning path. The following $\\alpha$ values and corresponding subtree sizes (number of terminal nodes) are considered:\n- $\\alpha$: $0$, $0.005$, $0.01$, $0.02$, $0.03$, $0.05$.\n- Subtree sizes: $22$, $16$, $10$, $8$, $6$, $4$.\n\nFor each $\\alpha$, the ten fold-wise misclassification errors are:\n- $\\alpha = 0$, subtree size $22$: $(0.150,\\;0.130,\\;0.148,\\;0.132,\\;0.146,\\;0.134,\\;0.144,\\;0.136,\\;0.142,\\;0.138)$.\n- $\\alpha = 0.005$, subtree size $16$: $(0.145,\\;0.125,\\;0.143,\\;0.127,\\;0.141,\\;0.129,\\;0.139,\\;0.131,\\;0.137,\\;0.133)$.\n- $\\alpha = 0.01$, subtree size $10$: $(0.150,\\;0.118,\\;0.146,\\;0.122,\\;0.142,\\;0.126,\\;0.138,\\;0.130,\\;0.134,\\;0.134)$.\n- $\\alpha = 0.02$, subtree size $8$: $(0.148,\\;0.126,\\;0.146,\\;0.128,\\;0.144,\\;0.130,\\;0.142,\\;0.132,\\;0.140,\\;0.134)$.\n- $\\alpha = 0.03$, subtree size $6$: $(0.153,\\;0.129,\\;0.151,\\;0.131,\\;0.149,\\;0.133,\\;0.147,\\;0.135,\\;0.145,\\;0.137)$.\n- $\\alpha = 0.05$, subtree size $4$: $(0.159,\\;0.129,\\;0.157,\\;0.131,\\;0.155,\\;0.133,\\;0.153,\\;0.135,\\;0.151,\\;0.137)$.\n\nStarting from the foundational definitions of cross-validation error estimation and cost-complexity pruning, use the one-standard-error rule to select the pruning parameter $\\alpha$ and report the corresponding subtree size. Specifically:\n1. For each $\\alpha$, compute the mean cross-validated misclassification error $\\mu(\\alpha)$ across the $10$ folds.\n2. Identify the $\\alpha^{\\star}$ that minimizes $\\mu(\\alpha)$ and compute the standard error of $\\mu(\\alpha^{\\star})$ as $SE(\\alpha^{\\star}) = s(\\alpha^{\\star}) / \\sqrt{K}$, where $s(\\alpha^{\\star})$ is the sample standard deviation of the fold-wise errors at $\\alpha^{\\star}$ and $K = 10$.\n3. Form the one-standard-error threshold $\\tau = \\mu(\\alpha^{\\star}) + SE(\\alpha^{\\star})$.\n4. Select the largest $\\alpha$ (i.e., the simplest subtree on the path) whose mean error $\\mu(\\alpha)$ is less than or equal to $\\tau$.\nReport your final answer as a row vector containing the selected $\\alpha$ and the corresponding subtree size. Express exact values; no rounding is required. No units should be included in the final answer.",
            "solution": "The problem requires the selection of an optimal cost-complexity pruning parameter, $\\alpha$, and its corresponding subtree size for a CART classifier using the one-standard-error rule. The process involves analyzing the $K$-fold cross-validation results provided for a set of $\\alpha$ values. Here, $K=10$.\n\nThe one-standard-error rule is a heuristic for model selection that aims to find the simplest model whose performance is statistically comparable to the best-performing model. The procedure is as follows:\n\n1.  For each value of the tuning parameter $\\alpha$, compute the mean cross-validated misclassification error, $\\mu(\\alpha)$.\n2.  Identify the parameter $\\alpha^{\\star}$ that results in the minimum mean cross-validated error, $\\mu(\\alpha^{\\star})$.\n3.  Calculate the standard error of the mean error at $\\alpha^{\\star}$, denoted as $SE(\\alpha^{\\star})$.\n4.  Find the simplest model (in this context, the one corresponding to the largest $\\alpha$) whose mean error is within one standard error of the minimum. That is, select the largest $\\alpha$ such that $\\mu(\\alpha) \\leq \\mu(\\alpha^{\\star}) + SE(\\alpha^{\\star})$.\n\nLet's execute these steps with the given data.\n\n**Step 1: Compute the mean cross-validated error $\\mu(\\alpha)$ for each $\\alpha$.**\n\nThe mean error $\\mu(\\alpha)$ is the average of the $K=10$ fold-wise misclassification errors for each $\\alpha$.\n\nFor $\\alpha = 0$:\n$\\mu(0) = \\frac{1}{10}(0.150+0.130+0.148+0.132+0.146+0.134+0.144+0.136+0.142+0.138) = \\frac{1.400}{10} = 0.140$\n\nFor $\\alpha = 0.005$:\n$\\mu(0.005) = \\frac{1}{10}(0.145+0.125+0.143+0.127+0.141+0.129+0.139+0.131+0.137+0.133) = \\frac{1.350}{10} = 0.135$\n\nFor $\\alpha = 0.01$:\n$\\mu(0.01) = \\frac{1}{10}(0.150+0.118+0.146+0.122+0.142+0.126+0.138+0.130+0.134+0.134) = \\frac{1.340}{10} = 0.134$\n\nFor $\\alpha = 0.02$:\n$\\mu(0.02) = \\frac{1}{10}(0.148+0.126+0.146+0.128+0.144+0.130+0.142+0.132+0.140+0.134) = \\frac{1.370}{10} = 0.137$\n\nFor $\\alpha = 0.03$:\n$\\mu(0.03) = \\frac{1}{10}(0.153+0.129+0.151+0.131+0.149+0.133+0.147+0.135+0.145+0.137) = \\frac{1.410}{10} = 0.141$\n\nFor $\\alpha = 0.05$:\n$\\mu(0.05) = \\frac{1}{10}(0.159+0.129+0.157+0.131+0.155+0.133+0.153+0.135+0.151+0.137) = \\frac{1.480}{10} = 0.148$\n\nSummary of mean errors $\\mu(\\alpha)$ and subtree sizes $|T_{\\alpha}|$:\n- $\\alpha=0, |T_{\\alpha}|=22: \\mu(0) = 0.140$\n- $\\alpha=0.005, |T_{\\alpha}|=16: \\mu(0.005) = 0.135$\n- $\\alpha=0.01, |T_{\\alpha}|=10: \\mu(0.01) = 0.134$\n- $\\alpha=0.02, |T_{\\alpha}|=8: \\mu(0.02) = 0.137$\n- $\\alpha=0.03, |T_{\\alpha}|=6: \\mu(0.03) = 0.141$\n- $\\alpha=0.05, |T_{\\alpha}|=4: \\mu(0.05) = 0.148$\n\n**Step 2: Identify $\\alpha^{\\star}$ and compute $SE(\\alpha^{\\star})$.**\n\nThe minimum mean error is $\\mu_{\\min} = 0.134$, which occurs at $\\alpha = 0.01$. Therefore, $\\alpha^{\\star} = 0.01$.\n\nNext, we compute the standard error of the mean for this $\\alpha^{\\star}$. The standard error is given by $SE(\\alpha^{\\star}) = \\frac{s(\\alpha^{\\star})}{\\sqrt{K}}$, where $s(\\alpha^{\\star})$ is the sample standard deviation of the fold-wise errors for $\\alpha^{\\star}$. The sample variance $s^2(\\alpha^{\\star})$ is calculated as:\n$$s^2(\\alpha^{\\star}) = \\frac{1}{K-1} \\sum_{k=1}^{K} (e_k - \\mu(\\alpha^{\\star}))^2$$\nFor $\\alpha^{\\star}=0.01$, the errors are $e_k = (0.150, 0.118, 0.146, 0.122, 0.142, 0.126, 0.138, 0.130, 0.134, 0.134)$ and the mean is $\\mu(0.01) = 0.134$. The number of folds is $K=10$.\n\nThe deviations from the mean $(e_k - \\mu)$ are:\n$(0.016, -0.016, 0.012, -0.012, 0.008, -0.008, 0.004, -0.004, 0, 0)$.\n\nThe sum of squared deviations is:\n$\\sum (e_k - \\mu)^2 = (0.016)^2 + (-0.016)^2 + (0.012)^2 + (-0.012)^2 + (0.008)^2 + (-0.008)^2 + (0.004)^2 + (-0.004)^2 + 0^2 + 0^2$\n$\\sum (e_k - \\mu)^2 = 2 \\times [ (0.016)^2 + (0.012)^2 + (0.008)^2 + (0.004)^2 ]$\n$\\sum (e_k - \\mu)^2 = 2 \\times [ 0.000256 + 0.000144 + 0.000064 + 0.000016 ]$\n$\\sum (e_k - \\mu)^2 = 2 \\times [ 0.000480 ] = 0.00096$\n\nThe sample variance is:\n$s^2(0.01) = \\frac{0.00096}{10-1} = \\frac{0.00096}{9}$\n\nThe standard error of the mean is:\n$SE(0.01) = \\sqrt{\\frac{s^2(0.01)}{K}} = \\sqrt{\\frac{0.00096/9}{10}} = \\sqrt{\\frac{0.00096}{90}}$\n\nTo express this as an exact value, we convert $0.00096$ to a fraction: $0.00096 = \\frac{96}{100000} = \\frac{6}{6250} = \\frac{3}{3125}$.\n$SE(0.01) = \\sqrt{\\frac{3/3125}{90}} = \\sqrt{\\frac{3}{3125 \\times 90}} = \\sqrt{\\frac{1}{3125 \\times 30}} = \\sqrt{\\frac{1}{93750}}$.\nTo simplify the radical, we factor the denominator: $93750 = 10 \\times 9375 = (2 \\times 5) \\times (3 \\times 5^5) = 2 \\times 3 \\times 5^6$.\n$SE(0.01) = \\sqrt{\\frac{1}{2 \\times 3 \\times 5^6}} = \\frac{1}{\\sqrt{5^6}\\sqrt{6}} = \\frac{1}{5^3\\sqrt{6}} = \\frac{1}{125\\sqrt{6}} = \\frac{\\sqrt{6}}{125 \\times 6} = \\frac{\\sqrt{6}}{750}$.\n\n**Step 3: Form the one-standard-error threshold $\\tau$.**\n\nThe threshold is $\\tau = \\mu(\\alpha^{\\star}) + SE(\\alpha^{\\star})$.\n$\\tau = 0.134 + \\frac{\\sqrt{6}}{750}$.\n\n**Step 4: Select the largest $\\alpha$ such that $\\mu(\\alpha) \\leq \\tau$.**\n\nWe compare each $\\mu(\\alpha)$ to the threshold $\\tau$.\n$\\tau \\approx 0.134 + \\frac{2.44949}{750} \\approx 0.134 + 0.003266 \\approx 0.137266$.\n\n- $\\mu(0) = 0.140$. Is $0.140 \\leq 0.134 + \\frac{\\sqrt{6}}{750}$? This is equivalent to $0.006 \\leq \\frac{\\sqrt{6}}{750}$, or $0.006 \\times 750 \\leq \\sqrt{6}$, which is $4.5 \\leq \\sqrt{6}$. Squaring both sides gives $20.25 \\leq 6$, which is false.\n\n- $\\mu(0.005) = 0.135$. Is $0.135 \\leq 0.134 + \\frac{\\sqrt{6}}{750}$? This is equivalent to $0.001 \\leq \\frac{\\sqrt{6}}{750}$, or $0.75 \\leq \\sqrt{6}$. Squaring both sides gives $0.5625 \\leq 6$, which is true.\n\n- $\\mu(0.01) = 0.134$. Is $0.134 \\leq 0.134 + \\frac{\\sqrt{6}}{750}$? This is equivalent to $0 \\leq \\frac{\\sqrt{6}}{750}$, which is true.\n\n- $\\mu(0.02) = 0.137$. Is $0.137 \\leq 0.134 + \\frac{\\sqrt{6}}{750}$? This is equivalent to $0.003 \\leq \\frac{\\sqrt{6}}{750}$, or $2.25 \\leq \\sqrt{6}$. Squaring both sides gives $5.0625 \\leq 6$, which is true.\n\n- $\\mu(0.03) = 0.141$. Is $0.141 \\leq 0.134 + \\frac{\\sqrt{6}}{750}$? This is equivalent to $0.007 \\leq \\frac{\\sqrt{6}}{750}$, or $5.25 \\leq \\sqrt{6}$. Squaring both sides gives $27.5625 \\leq 6$, which is false.\n\n- $\\mu(0.05) = 0.148$. Is $0.148 \\leq 0.134 + \\frac{\\sqrt{6}}{750}$? This is equivalent to $0.014 \\leq \\frac{\\sqrt{6}}{750}$, or $10.5 \\leq \\sqrt{6}$. Squaring both sides gives $110.25 \\leq 6$, which is false.\n\nThe values of $\\alpha$ that satisfy the condition $\\mu(\\alpha) \\leq \\tau$ are $0.005$, $0.01$, and $0.02$. The rule is to select the largest $\\alpha$ from this set, which corresponds to the simplest (most pruned) model. The largest $\\alpha$ is $0.02$.\n\nThe subtree size corresponding to $\\alpha = 0.02$ is given as $8$.\n\nThus, the selected parameter is $\\alpha=0.02$ and the corresponding subtree size is $8$. The final answer is to be reported as a row vector.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.02 & 8\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "The cross-validation process itself is not immune to challenges, especially when dealing with the imbalanced datasets common in medical analytics. This problem  asks you to reason about how rare events can destabilize error estimation and, consequently, the pruning process. Understanding this failure mode from a theoretical standpoint is crucial for designing robust validation pipelines, like those using stratified sampling, that yield reliable and stable models.",
            "id": "4615706",
            "problem": "A clinical decision support system uses a binary decision tree to predict the presence of a disease in a cohort of $n$ patients, where the positive (diseased) class prevalence is $p$ and the negative (non-diseased) class prevalence is $1-p$. Cost-complexity pruning is applied to the fitted tree by selecting the pruning parameter $\\alpha$ that minimizes a Cross-Validation (CV) estimate of misclassification risk. The CV estimator aggregates per-fold misclassification risk at a fixed $\\alpha$ by summing class-specific misclassification rates weighted by the population prevalences, that is, the true risk is $E(\\alpha) = p \\,\\theta_{+}(\\alpha) + (1-p) \\,\\theta_{-}(\\alpha)$, where $\\theta_{+}(\\alpha)$ and $\\theta_{-}(\\alpha)$ are the class-conditional error probabilities on the positive and negative classes, respectively.\n\nAssume $p \\ll 1$, as is typical for rare diseases in bioinformatics and medical data analytics. In $k$-fold CV with non-stratified random partitioning, the number of positive examples $n_{+,i}$ in fold $i$ is random and can vary widely across folds, while the number of negative examples $n_{-,i}$ is comparatively large and stable. The per-fold risk estimator at a fixed $\\alpha$ can be written as $\\hat{E}_{i}(\\alpha) = p \\,\\hat{\\theta}_{+,i}(\\alpha) + (1-p)\\,\\hat{\\theta}_{-,i}(\\alpha)$, where $\\hat{\\theta}_{+,i}(\\alpha)$ and $\\hat{\\theta}_{-,i}(\\alpha)$ are empirical error rates computed on the held-out data in fold $i$. For a given fold $i$, conditional on the counts $n_{+,i}$ and $n_{-,i}$, the empirical class-specific errors can be treated as binomial proportions with variances that depend on $n_{+,i}$ and $n_{-,i}$.\n\nFrom first principles, starting with the definition of binomial proportion variance and the law of total variance, reason about how class imbalance ($p \\ll 1$) affects the across-fold variance of $\\hat{E}_{i}(\\alpha)$ and, consequently, the stability of the selected pruning parameter $\\alpha$ when minimizing the average CV risk. Then consider the effect of enforcing stratified $k$-fold CV with balanced folds, in which each fold has approximately $n_{+,i} \\approx n_{+}/k$ positives and $n_{-,i} \\approx n_{-}/k$ negatives, where $n_{+} = p n$ and $n_{-} = (1-p)n$.\n\nChoose the option that most accurately characterizes the effect of class imbalance on CV error variance and a scientifically sound procedure to improve the stability of pruning selection:\n\nA. With $p \\ll 1$, random $k$-fold Cross-Validation yields high across-fold variance of the estimated misclassification risk at fixed $\\alpha$ because the minority-class error estimate has variance that inflates when $n_{+,i}$ is small and random; stratified CV that fixes $n_{+,i} \\approx n_{+}/k$ in every fold reduces this variance and stabilizes pruning selection.\n\nB. Leave-One-Out Cross-Validation eliminates variance in the CV error estimate regardless of class imbalance, making pruning selection perfectly stable.\n\nC. Oversampling the majority class within each fold is sufficient to stabilize pruning selection because reducing randomness in $n_{-,i}$ addresses the dominant source of CV error variance under imbalance.\n\nD. Using stratified $k$-fold CV with per-class weighting $w_{+} = 1/p$ and $w_{-} = 1/(1-p)$ in the loss reduces both variance and bias in the CV error estimator, leading to more stable and fair pruning selection under class imbalance.\n\nE. Increasing $k$ to a large value without stratification always reduces the across-fold variance of CV error under class imbalance, ensuring stability.",
            "solution": "We begin from the definition of the misclassification risk under population prevalences, $E(\\alpha) = p\\,\\theta_{+}(\\alpha) + (1-p)\\,\\theta_{-}(\\alpha)$, where $\\theta_{+}(\\alpha)$ and $\\theta_{-}(\\alpha)$ are class-conditional error probabilities. In $k$-fold Cross-Validation (CV), the per-fold estimator at fixed $\\alpha$ is $\\hat{E}_{i}(\\alpha) = p \\,\\hat{\\theta}_{+,i}(\\alpha) + (1-p)\\,\\hat{\\theta}_{-,i}(\\alpha)$, where $\\hat{\\theta}_{+,i}(\\alpha)$ is the empirical error proportion on the positive class in fold $i$ and $\\hat{\\theta}_{-,i}(\\alpha)$ is the empirical error proportion on the negative class.\n\nFor a given fold $i$, conditional on $n_{+,i}$ positives and $n_{-,i}$ negatives in the held-out set, each $\\hat{\\theta}_{+,i}(\\alpha)$ is a binomial proportion with variance\n$$\n\\operatorname{Var}\\big(\\hat{\\theta}_{+,i}(\\alpha) \\mid n_{+,i}\\big) = \\frac{\\theta_{+}(\\alpha) \\big(1-\\theta_{+}(\\alpha)\\big)}{n_{+,i}},\n$$\nand similarly\n$$\n\\operatorname{Var}\\big(\\hat{\\theta}_{-,i}(\\alpha) \\mid n_{-,i}\\big) = \\frac{\\theta_{-}(\\alpha) \\big(1-\\theta_{-}(\\alpha)\\big)}{n_{-,i}}.\n$$\nAssuming the class-specific error counts are conditionally independent given the counts, the conditional variance of the per-fold risk estimator is\n$$\n\\operatorname{Var}\\big(\\hat{E}_{i}(\\alpha) \\mid n_{+,i}, n_{-,i}\\big) = p^{2} \\operatorname{Var}\\big(\\hat{\\theta}_{+,i}(\\alpha) \\mid n_{+,i}\\big) + (1-p)^{2} \\operatorname{Var}\\big(\\hat{\\theta}_{-,i}(\\alpha) \\mid n_{-,i}\\big),\n$$\nwhich yields\n$$\n\\operatorname{Var}\\big(\\hat{E}_{i}(\\alpha) \\mid n_{+,i}, n_{-,i}\\big) = p^{2} \\frac{\\theta_{+}(\\alpha)\\big(1-\\theta_{+}(\\alpha)\\big)}{n_{+,i}} + (1-p)^{2} \\frac{\\theta_{-}(\\alpha)\\big(1-\\theta_{-}(\\alpha)\\big)}{n_{-,i}}.\n$$\nBy the law of total variance,\n$$\n\\operatorname{Var}\\big(\\hat{E}_{i}(\\alpha)\\big) = \\mathbb{E}\\big[\\operatorname{Var}\\big(\\hat{E}_{i}(\\alpha)\\mid n_{+,i}, n_{-,i}\\big)\\big] + \\operatorname{Var}\\big(\\mathbb{E}\\big[\\hat{E}_{i}(\\alpha)\\mid n_{+,i}, n_{-,i}\\big]\\big).\n$$\nGiven that $\\mathbb{E}\\big[\\hat{\\theta}_{+,i}(\\alpha)\\mid n_{+,i}\\big] = \\theta_{+}(\\alpha)$ and $\\mathbb{E}\\big[\\hat{\\theta}_{-,i}(\\alpha)\\mid n_{-,i}\\big] = \\theta_{-}(\\alpha)$, the second term simplifies to zero for a fixed $\\alpha$ because $\\mathbb{E}\\big[\\hat{E}_{i}(\\alpha)\\mid n_{+,i}, n_{-,i}\\big] = p \\,\\theta_{+}(\\alpha) + (1-p)\\,\\theta_{-}(\\alpha)$ does not depend on $n_{+,i}$ or $n_{-,i}$. Therefore,\n$$\n\\operatorname{Var}\\big(\\hat{E}_{i}(\\alpha)\\big) = \\mathbb{E}\\left[p^{2} \\frac{\\theta_{+}(\\alpha)\\big(1-\\theta_{+}(\\alpha)\\big)}{n_{+,i}} + (1-p)^{2} \\frac{\\theta_{-}(\\alpha)\\big(1-\\theta_{-}(\\alpha)\\big)}{n_{-,i}}\\right].\n$$\nUnder non-stratified random partitioning, $n_{+,i}$ varies across folds. For large $n$ and moderate $k$, we can approximate $n_{+,i}$ as a binomial random variable $n_{+,i} \\sim \\mathrm{Bin}\\big(n/k, p\\big)$, so that the expectation $\\mathbb{E}\\big[1/n_{+,i}\\big]$ is dominated by the right tail near small $n_{+,i}$ when $p \\ll 1$. In practice, folds can have very few or even zero positives, and the term $p^{2} \\theta_{+}(\\alpha)\\big(1-\\theta_{+}(\\alpha)\\big)/n_{+,i}$ can become extremely large or ill-defined, inflating the across-fold variance of $\\hat{E}_{i}(\\alpha)$. Meanwhile, the negative-class term $(1-p)^{2} \\theta_{-}(\\alpha)\\big(1-\\theta_{-}(\\alpha)\\big)/n_{-,i}$ is relatively small because $n_{-,i} \\approx (1-p) n/k$ is large and more stable.\n\nThis heteroscedasticity directly impacts pruning selection. The selected $\\alpha$ minimizes the average CV risk across folds, $\\bar{E}(\\alpha) = \\frac{1}{k}\\sum_{i=1}^{k} \\hat{E}_{i}(\\alpha)$. If $\\hat{E}_{i}(\\alpha)$ exhibits high variance due to the minority-class term, the minimizer of $\\bar{E}(\\alpha)$ fluctuates substantially across repeated CV runs, and pruning selection is unstable.\n\nBy enforcing stratified $k$-fold CV with balanced folds, we fix $n_{+,i} \\approx n_{+}/k$ and $n_{-,i} \\approx n_{-}/k$ deterministically or with negligible variation. Then\n$$\n\\operatorname{Var}\\big(\\hat{E}_{i}(\\alpha)\\big) \\approx p^{2} \\frac{\\theta_{+}(\\alpha)\\big(1-\\theta_{+}(\\alpha)\\big)}{n_{+}/k} + (1-p)^{2} \\frac{\\theta_{-}(\\alpha)\\big(1-\\theta_{-}(\\alpha)\\big)}{n_{-}/k},\n$$\nwhich is minimized relative to the non-stratified case because the harmful variability in $1/n_{+,i}$ is removed, and the minority-class contribution now scales as $O\\big(k/n_{+}\\big)$. Consequently, the average CV risk $\\bar{E}(\\alpha)$ has smaller variance, and pruning selection is more stable.\n\nWe now evaluate each option:\n\nA. This option correctly identifies the mechanism: with $p \\ll 1$, the minority-class sample size $n_{+,i}$ is small and random across folds under non-stratified CV, inflating $\\operatorname{Var}\\big(\\hat{\\theta}_{+,i}(\\alpha)\\big)$ and, via the weighting by $p$, the variance of $\\hat{E}_{i}(\\alpha)$. It correctly states that stratified CV that approximately fixes $n_{+,i}$ across folds reduces this variance and stabilizes pruning selection. Verdict: Correct.\n\nB. Leave-One-Out Cross-Validation (LOOCV) does not eliminate variance; in fact, LOOCV typically has high variance in model selection settings. Under class imbalance, LOOCV yields test folds of size $1$ in which the presence of minority-class examples is very rare, exacerbating instability and producing highly variable per-fold estimates. Pruning selection is not perfectly stable. Verdict: Incorrect.\n\nC. Oversampling the majority class reduces randomness in $n_{-,i}$, which is already large and stable; it does not address the dominant source of variance arising from the small and variable $n_{+,i}$. Oversampling the majority would further skew class counts and can worsen imbalance effects. Verdict: Incorrect.\n\nD. Per-class weighting $w_{+} = 1/p$ and $w_{-} = 1/(1-p)$ changes the target loss to emphasize minority errors, which can reduce bias in the sense of aligning the estimator with a balanced objective. However, large weights on the minority class generally increase the variance contribution from $\\hat{\\theta}_{+,i}(\\alpha)$ unless minority sample sizes are stabilized and sufficiently large. Claiming that this procedure reduces both variance and bias is not generally valid; the variance may increase. Verdict: Incorrect.\n\nE. Increasing $k$ without stratification does not guarantee variance reduction. As $k$ grows, test folds shrink and the probability of very small $n_{+,i}$ increases under imbalance, often increasing the variance of per-fold estimates and destabilizing selection (extreme case: LOOCV). Verdict: Incorrect.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}