{
    "hands_on_practices": [
        {
            "introduction": "In many bioinformatics applications, our goal is to predict a continuous value, such as the concentration of a biomarker. To assess the performance of such a regression model, we need metrics that go beyond simple error. This exercise will guide you through calculating two fundamental evaluation metrics: the Root Mean Squared Error ($RMSE$), which quantifies the average magnitude of prediction error in the original units of the outcome, and the coefficient of determination ($R^2$), which measures the proportion of variance in the outcome that our model successfully explains. ",
            "id": "4579954",
            "problem": "A translational bioinformatics team trains a supervised learning model to predict serum creatinine (in mg/dL) from multi-omics features. The model is evaluated on a held-out validation cohort of $100$ adult patients. Let the true creatinine values be $\\{y_i\\}_{i=1}^{n}$ and the model predictions be $\\{\\hat{y}_i\\}_{i=1}^{n}$ with $n=100$. From the matched true and predicted values on this validation cohort, the following empirical summary statistics are obtained:\n- The residual sum of squares (sum of squared prediction errors): $\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = 19.36$.\n- The total sum of squares around the empirical mean $\\bar{y} = \\frac{1}{n}\\sum_{i=1}^{n} y_i$: $\\sum_{i=1}^{n} (y_i - \\bar{y})^2 = 48.40$.\n\nA clinical evaluation plan deems a model adequate on this cohort if and only if both of the following hold simultaneously: the Root Mean Squared Error (RMSE) is less than or equal to $0.5$ mg/dL and the coefficient of determination (denoted $R^2$) is at least $0.55$.\n\nStarting from the core definitions of empirical risk under squared loss and variance decomposition relative to the baseline predictor $y=\\bar{y}$, compute the RMSE (expressed in mg/dL) and $R^2$ for this model on the validation cohort, and determine whether the model meets the adequacy criterion. Report only the numerical values of RMSE and $R^2$ as your final answer, in that order, rounded to four significant figures. Express the RMSE in mg/dL. Do not include any units in the final boxed answer.",
            "solution": "The task is to compute the Root Mean Squared Error (RMSE) and the coefficient of determination ($R^2$) for a supervised learning model, and to assess if the model meets a predefined adequacy criterion.\n\nLet $n$ be the number of patients in the validation cohort, where $n=100$.\nLet $\\{y_i\\}_{i=1}^{n}$ be the set of true serum creatinine values and $\\{\\hat{y}_i\\}_{i=1}^{n}$ be the set of model predictions.\n\nThe problem provides two key summary statistics:\n1.  The Residual Sum of Squares (RSS), which is the sum of squared prediction errors:\n    $$ \\text{RSS} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = 19.36 $$\n2.  The Total Sum of Squares (TSS), which is the sum of squared differences between the true values and their empirical mean $\\bar{y}$:\n    $$ \\text{TSS} = \\sum_{i=1}^{n} (y_i - \\bar{y})^2 = 48.40 $$\n\nFirst, we compute the RMSE. The RMSE is defined as the square root of the Mean Squared Error (MSE). The MSE is the empirical risk under squared loss, calculated as the average of the squared errors:\n$$ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\frac{\\text{RSS}}{n} $$\nSubstituting the given values for RSS and $n$:\n$$ \\text{MSE} = \\frac{19.36}{100} = 0.1936 $$\nThe units of MSE are the square of the units of the target variable, i.e., $(\\text{mg/dL})^2$.\n\nThe RMSE is the square root of the MSE, which returns the error metric to the original units of the target variable:\n$$ \\text{RMSE} = \\sqrt{\\text{MSE}} $$\nSubstituting the calculated value of MSE:\n$$ \\text{RMSE} = \\sqrt{0.1936} = 0.44 $$\nSo, the RMSE is $0.44$ mg/dL.\n\nNext, we compute the coefficient of determination, $R^2$. This metric measures the proportion of the variance in the dependent variable that is predictable from the model. It is defined as:\n$$ R^2 = 1 - \\frac{\\text{RSS}}{\\text{TSS}} $$\nThis formula compares the error of the model (RSS) to the error of a baseline model that constantly predicts the mean of the true values (TSS).\n\nSubstituting the given values for RSS and TSS:\n$$ R^2 = 1 - \\frac{19.36}{48.40} $$\nThe ratio can be simplified:\n$$ \\frac{19.36}{48.40} = 0.4 $$\nTherefore, the $R^2$ value is:\n$$ R^2 = 1 - 0.4 = 0.6 $$\nThe $R^2$ is a dimensionless quantity.\n\nThe problem states the adequacy criterion as a two-part condition:\n1. RMSE $\\le 0.5$ mg/dL\n2. $R^2 \\ge 0.55$\n\nWe check our computed values against these criteria:\n1. For RMSE: $0.44 \\le 0.5$. This condition is met.\n2. For $R^2$: $0.6 \\ge 0.55$. This condition is also met.\nSince both conditions hold simultaneously, the model is deemed adequate on this validation cohort.\n\nThe final step is to report the numerical values of RMSE and $R^2$, rounded to four significant figures.\n- RMSE = $0.44$. To four significant figures, this is $0.4400$.\n- $R^2$ = $0.6$. To four significant figures, this is $0.6000$.\n\nThe final answer will present these two values in the specified order.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.4400 & 0.6000\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "When a machine learning model outputs a probability of disease, translating this into a clinical decision requires selecting a threshold. This choice creates a critical trade-off: a low threshold might catch more true cases (high recall) but also generate more false alarms (low precision). This practice demonstrates how to find the optimal decision threshold that maximizes the $F_1$-score, a metric that balances precision and recall, ensuring the model is tuned for the best overall performance in a classification task. ",
            "id": "4579975",
            "problem": "A probabilistic classifier is applied to a cohort of $200$ patients to predict the presence of a specific disease. For each patient $i \\in \\{1,\\dots,200\\}$, the model outputs a calibrated predicted probability $p_i \\in [0,1]$ of disease presence. Predictions are converted to binary decisions using a threshold $t \\in [0,1]$ with the rule: predict disease present if and only if $p_i \\geq t$. The patients can be grouped by identical predicted probabilities as follows; each bullet gives the predicted probability $p$ (in decimal form), together with the number of patients with disease present (positives) and disease absent (negatives) at that $p$:\n\n- $p=0.97$: $9$ positives, $1$ negative\n- $p=0.90$: $10$ positives, $6$ negatives\n- $p=0.82$: $8$ positives, $10$ negatives\n- $p=0.74$: $7$ positives, $13$ negatives\n- $p=0.66$: $6$ positives, $10$ negatives\n- $p=0.58$: $5$ positives, $13$ negatives\n- $p=0.50$: $5$ positives, $15$ negatives\n- $p=0.42$: $4$ positives, $18$ negatives\n- $p=0.34$: $3$ positives, $17$ negatives\n- $p=0.26$: $2$ positives, $12$ negatives\n- $p=0.18$: $1$ positive, $9$ negatives\n- $p=0.10$: $0$ positives, $16$ negatives\n\nAssume the model outputs are calibrated and the group counts and labels are accurate. Using the standard definitions of True Positive (TP), False Positive (FP), False Negative (FN), Precision, Recall, and the $F_1$-score, determine the single threshold $t$ that maximizes the $F_1$-score over all $t \\in [0,1]$. If there is an interval of thresholds that achieves the same maximal $F_1$-score, report the largest threshold in that interval. Round your threshold to four significant figures. In addition, explain, from first principles, how the class prevalence (the proportion of positives in the population) generically affects the location of the threshold that maximizes the $F_1$-score, holding the score distributions otherwise comparable.",
            "solution": "The solution requires two parts: finding the optimal threshold $t$ to maximize the $F_1$-score, and explaining the effect of prevalence on this optimal threshold.\n\n**Part 1: Finding the Optimal Threshold**\n\nThe $F_1$-score is the harmonic mean of Precision ($P$) and Recall ($R$):\n$$ F_1 = 2 \\cdot \\frac{P \\cdot R}{P + R} $$\nwhere Precision and Recall are defined in terms of True Positives ($TP$), False Positives ($FP$), and False Negatives ($FN$):\n$$ P = \\frac{TP}{TP + FP} \\quad \\text{and} \\quad R = \\frac{TP}{TP + FN} $$\nFrom the data, the total number of actual positive cases is $P_{total} = \\sum \\text{positives} = 9+10+8+7+6+5+5+4+3+2+1+0 = 60$. The total number of actual negative cases is $N_{total} = \\sum \\text{negatives} = 1+6+10+13+10+13+15+18+17+12+9+16 = 140$.\nThe $F_1$-score can be expressed directly in terms of $TP$, $FP$, and $FN$:\n$$ F_1 = \\frac{2 \\cdot TP}{2 \\cdot TP + FP + FN} $$\nA patient is predicted to have the disease if their predicted probability $p_i$ is greater than or equal to a threshold $t$, i.e., $p_i \\geq t$. The set of predicted positives changes only when the threshold $t$ crosses one of the unique probability values given in the problem. Therefore, we only need to evaluate the $F_1$-score at thresholds corresponding to these unique probability values.\n\nWe can compute the cumulative counts of $TP$ and $FP$ by processing the probability groups in descending order. For a given threshold $t$, all patients with probability $p \\ge t$ are classified as positive.\nLet $TP(t)$ and $FP(t)$ be the number of true and false positives for a threshold $t$.\n$TP(t) = \\sum_{p_k \\ge t} \\text{positives}(p_k)$\n$FP(t) = \\sum_{p_k \\ge t} \\text{negatives}(p_k)$\n$FN(t) = P_{total} - TP(t) = 60 - TP(t)$.\n\nThe calculations are summarized in the following table. The threshold $t$ corresponds to the probability value in the first column, meaning any patient with a score greater than or equal to $t$ is classified as positive.\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|}\n\\hline\n\\text{Threshold } t & \\text{Cumulative } TP & \\text{Cumulative } FP & FN & \\text{Precision} & \\text{Recall} & F_1\\text{-score} \\\\\n\\hline\n0.97 & 9 & 1 & 51 & 9/10 = 0.9000 & 9/60 = 0.1500 & 0.2571 \\\\\n0.90 & 19 & 7 & 41 & 19/26 \\approx 0.7308 & 19/60 \\approx 0.3167 & 0.4419 \\\\\n0.82 & 27 & 17 & 33 & 27/44 \\approx 0.6136 & 27/60 = 0.4500 & 0.5192 \\\\\n0.74 & 34 & 30 & 26 & 34/64 \\approx 0.5313 & 34/60 \\approx 0.5667 & 0.5484 \\\\\n\\mathbf{0.66} & \\mathbf{40} & \\mathbf{40} & \\mathbf{20} & \\mathbf{40/80 = 0.5000} & \\mathbf{40/60 \\approx 0.6667} & \\mathbf{0.5714} \\\\\n0.58 & 45 & 53 & 15 & 45/98 \\approx 0.4592 & 45/60 = 0.7500 & 0.5696 \\\\\n0.50 & 50 & 68 & 10 & 50/118 \\approx 0.4237 & 50/60 \\approx 0.8333 & 0.5618 \\\\\n0.42 & 54 & 86 & 6 & 54/140 \\approx 0.3857 & 54/60 = 0.9000 & 0.5400 \\\\\n0.34 & 57 & 103 & 3 & 57/160 \\approx 0.3563 & 57/60 = 0.9500 & 0.5182 \\\\\n0.26 & 59 & 115 & 1 & 59/174 \\approx 0.3391 & 59/60 \\approx 0.9833 & 0.5021 \\\\\n0.18 & 60 & 124 & 0 & 60/184 \\approx 0.3261 & 60/60 = 1.0000 & 0.4918 \\\\\n0.10 & 60 & 140 & 0 & 60/200 = 0.3000 & 60/60 = 1.0000 & 0.4615 \\\\\n\\hline\n\\end{array}\n$$\n\nThe maximum $F_1$-score is approximately $0.5714$ (which is exactly $4/7$). This score is achieved when the threshold $t$ is set such that all patients with score $p \\ge 0.66$ are classified as positive. This corresponds to any threshold $t$ in the interval $(0.58, 0.66]$. For any $t$ in this interval, the set of patients classified as positive remains the same, yielding the same $F_1$-score. The problem asks for the largest threshold in this interval, which is $t = 0.66$. Rounded to four significant figures, the threshold is $0.6600$.\n\n**Part 2: Effect of Class Prevalence on the Optimal Threshold**\n\nThe class prevalence, $\\pi$, is the proportion of positives in the population: $\\pi = P_{total} / (P_{total} + N_{total})$. We are asked to explain how $\\pi$ affects the threshold $t^*$ that maximizes the $F_1$-score, assuming the score distributions for the positive and negative classes are held constant.\n\nLet $f_P(s)$ and $f_N(s)$ be the probability density functions of the classifier scores for the positive and negative populations, respectively. For a given threshold $t$:\nRecall, or True Positive Rate ($TPR$), is the probability that a positive case is correctly identified:\n$$ R(t) = \\text{Recall}(t) = P(\\text{score} \\ge t | \\text{positive}) = \\int_t^1 f_P(s) ds $$\nThis definition shows that Recall is an intrinsic property of the classifier's score distribution for the positive class and is independent of prevalence $\\pi$.\n\nPrecision, on the other hand, is the probability that a case predicted as positive is truly positive. Using Bayes' theorem:\n$$ P(t) = \\text{Precision}(t) = P(\\text{positive} | \\text{score} \\ge t) = \\frac{P(\\text{score} \\ge t | \\text{positive}) P(\\text{positive})}{P(\\text{score} \\ge t)} $$\nThe denominator can be expanded using the law of total probability:\n$$ P(\\text{score} \\ge t) = P(\\text{score} \\ge t | \\text{positive})P(\\text{positive}) + P(\\text{score} \\ge t | \\text{negative})P(\\text{negative}) $$\nLetting $P(\\text{positive}) = \\pi$, $P(\\text{negative}) = 1-\\pi$, and $P(\\text{score} \\ge t | \\text{negative}) = \\int_t^1 f_N(s) ds$ (the False Positive Rate, $FPR(t)$), we get:\n$$ P(t) = \\frac{R(t) \\cdot \\pi}{R(t) \\cdot \\pi + FPR(t) \\cdot (1-\\pi)} $$\nThis expression explicitly shows that Precision is a function of prevalence $\\pi$. For a fixed threshold $t$, as $\\pi$ increases, the term $(1-\\pi)$ decreases, causing the denominator to decrease and thus Precision to increase.\n\nThe $F_1$-score is a balance between Precision and Recall. The optimal threshold $t^*$ is the one that achieves the best trade-off.\n1.  **Low Prevalence ($\\pi \\to 0$):** In a population with very few positives, the number of negatives $N_{total}$ is much larger than the number of positives $P_{total}$. Even a small False Positive Rate ($FPR$) can lead to a large number of false positives ($FP = FPR \\cdot N_{total}$), overwhelming the true positives ($TP = R \\cdot P_{total}$). This severely depresses Precision. To maximize the $F_1$-score, the primary challenge is to increase Precision. This is achieved by making the classifier more restrictive, i.e., by **increasing the threshold $t$**. A higher threshold reduces the $FPR$, thus reducing $FP$ and boosting Precision, albeit at the cost of lower Recall.\n2.  **High Prevalence ($\\pi \\to 1$):** In a population with many positives, the number of positives $P_{total}$ is much larger than the number of negatives $N_{total}$. Precision tends to be high for a wide range of thresholds because the number of false positives $FP$ will be small relative to the number of true positives $TP$. The limiting factor for the $F_1$-score becomes Recall. To maximize the $F_1$-score, the focus shifts to improving Recall. This is achieved by making the classifier more inclusive, i.e., by **decreasing the threshold $t$**. A lower threshold captures more true positives, increasing Recall.\n\nIn summary, as class prevalence $\\pi$ increases, the optimal threshold $t^*$ that maximizes the $F_1$-score generally decreases. Conversely, a decrease in prevalence requires an increase in the optimal threshold to maintain a high $F_1$-score.",
            "answer": "$$\n\\boxed{0.6600}\n$$"
        },
        {
            "introduction": "Unsupervised learning is a powerful tool for discovering hidden structures in complex biological data, such as identifying novel cancer subtypes from gene expression profiles. Evaluating the quality of the resulting clusters is a non-trivial task. This hands-on practice will introduce you to two key evaluation strategies: calculating the silhouette score to measure the internal coherence and separation of clusters, and using the Adjusted Rand Index ($ARI$) to quantify the agreement between the algorithm's clusters and a known ground-truth classification. ",
            "id": "4579918",
            "problem": "A cohort of $500$ cancer patients was profiled with high-throughput transcriptomics, and an unsupervised clustering algorithm was applied to z-score standardized gene expression profiles, producing $3$ clusters that are hypothesized to correspond to biological subtypes. For every patient in cluster $k \\in \\{1,2,3\\}$, the average dissimilarity to all other patients in the same cluster (computed as Euclidean distance on standardized expression) is empirically uniform and equals $A_{k}$, and the average dissimilarity to patients in a different cluster $\\ell \\neq k$ is empirically uniform and equals $D_{k,\\ell}$. The cluster-level averages are:\n$$\nA_{1} = 4.0,\\quad A_{2} = 3.5,\\quad A_{3} = 2.8,\n$$\n$$\nD_{1,2} = 6.2,\\quad D_{1,3} = 5.1,\\quad D_{2,1} = 6.2,\\quad D_{2,3} = 5.9,\\quad D_{3,1} = 5.1,\\quad D_{3,2} = 5.9.\n$$\nThe cluster sizes are $|C_{1}| = 220$, $|C_{2}| = 180$, and $|C_{3}| = 100$.\n\nA gold-standard ground truth partition of the patients into $3$ molecular subtypes was curated from clinical and multi-omics evidence, with subtype sizes $|G_{1}| = 200$, $|G_{2}| = 150$, and $|G_{3}| = 150$. The cross-tabulation of the clustering assignments (rows) against the ground truth subtypes (columns) is:\n$$\n\\begin{pmatrix}\n160 & 40 & 20 \\\\\n30 & 90 & 60 \\\\\n10 & 20 & 70\n\\end{pmatrix}.\n$$\n\nUsing the fundamental definitions of the silhouette coefficient for clustering validity and the Adjusted Rand Index (ARI) for partition agreement, compute:\n- the overall silhouette score over all $500$ patients, treating each patientâ€™s silhouette value as determined by its cluster-level $A_{k}$ and the minimum across $\\ell \\neq k$ of $D_{k,\\ell}$, and\n- the Adjusted Rand Index (ARI) comparing the clustering to the ground truth partition, derived from the combinatoric counts in the cross-tabulation.\n\nExpress both quantities as decimals, and round each to four significant figures. Present your final answer as a row matrix whose first entry is the silhouette score and whose second entry is the Adjusted Rand Index, with no units.",
            "solution": "### Solution Derivation\n\n#### Part 1: Overall Silhouette Score\n\nThe silhouette score for a single sample $i$ is defined as $s(i) = \\frac{b(i) - a(i)}{\\max\\{a(i), b(i)\\}}$, where $a(i)$ is the average dissimilarity of $i$ to other samples in its own cluster, and $b(i)$ is the minimum average dissimilarity of $i$ to samples in any other cluster.\n\nThe problem specifies that for any patient in cluster $C_k$, we should use the cluster-level average $a(i) = A_k$ and the cluster-level average for $b(i)$, which is $b_k = \\min_{\\ell \\neq k} D_{k,\\ell}$. Therefore, the silhouette score for any patient in cluster $C_k$, denoted $s_k$, is constant for that cluster:\n$$s_k = \\frac{b_k - A_k}{\\max\\{A_k, b_k\\}}$$\n\n- For cluster $C_1$:\n  $A_1 = 4.0$.\n  $b_1 = \\min\\{D_{1,2}, D_{1,3}\\} = \\min\\{6.2, 5.1\\} = 5.1$.\n  $s_1 = \\frac{5.1 - 4.0}{\\max\\{4.0, 5.1\\}} = \\frac{1.1}{5.1}$.\n\n- For cluster $C_2$:\n  $A_2 = 3.5$.\n  $b_2 = \\min\\{D_{2,1}, D_{2,3}\\} = \\min\\{6.2, 5.9\\} = 5.9$.\n  $s_2 = \\frac{5.9 - 3.5}{\\max\\{3.5, 5.9\\}} = \\frac{2.4}{5.9}$.\n\n- For cluster $C_3$:\n  $A_3 = 2.8$.\n  $b_3 = \\min\\{D_{3,1}, D_{3,2}\\} = \\min\\{5.1, 5.9\\} = 5.1$.\n  $s_3 = \\frac{5.1 - 2.8}{\\max\\{2.8, 5.1\\}} = \\frac{2.3}{5.1}$.\n\nThe overall silhouette score $S$ is the average of the individual scores over all $N=500$ patients. This is the weighted average of the cluster-specific scores, with weights equal to the cluster sizes:\n$$S = \\frac{1}{N} \\sum_{k=1}^{3} |C_k| s_k = \\frac{1}{500} \\left( |C_1|s_1 + |C_2|s_2 + |C_3|s_3 \\right)$$\n$$S = \\frac{1}{500} \\left( 220 \\cdot \\frac{1.1}{5.1} + 180 \\cdot \\frac{2.4}{5.9} + 100 \\cdot \\frac{2.3}{5.1} \\right)$$\n$$S = \\frac{1}{500} \\left( \\frac{242}{5.1} + \\frac{432}{5.9} + \\frac{230}{5.1} \\right) = \\frac{1}{500} \\left( \\frac{472}{5.1} + \\frac{432}{5.9} \\right)$$\n$$S \\approx \\frac{1}{500} \\left( 92.5490196... + 73.2203389... \\right) = \\frac{165.769358...}{500} \\approx 0.3315387...$$\nRounding to four significant figures, the overall silhouette score is $0.3315$.\n\n#### Part 2: Adjusted Rand Index (ARI)\n\nThe Adjusted Rand Index (ARI) measures the similarity between two data clusterings. Its general form is:\n$$\\text{ARI} = \\frac{\\text{Index} - \\text{Expected Index}}{\\text{Max Index} - \\text{Expected Index}}$$\nUsing the contingency table $n_{ij}$, where $i$ indexes the algorithm's clusters and $j$ indexes the ground truth classes, the ARI is calculated as:\n$$\\text{ARI} = \\frac{\\sum_{ij} \\binom{n_{ij}}{2} - \\frac{\\left[\\sum_i \\binom{a_i}{2}\\right] \\left[\\sum_j \\binom{b_j}{2}\\right]}{\\binom{N}{2}}}{\\frac{1}{2}\\left[\\sum_i \\binom{a_i}{2} + \\sum_j \\binom{b_j}{2}\\right] - \\frac{\\left[\\sum_i \\binom{a_i}{2}\\right] \\left[\\sum_j \\binom{b_j}{2}\\right]}{\\binom{N}{2}}}$$\nwhere $a_i$ are the row sums and $b_j$ are the column sums of the contingency table.\n\nFirst, we calculate the required combinatorial terms:\n- The sum of $\\binom{n_{ij}}{2}$ over all cells (Index):\n  $\\sum_{ij}\\binom{n_{ij}}{2} = \\binom{160}{2} + \\binom{40}{2} + \\binom{20}{2} + \\binom{30}{2} + \\binom{90}{2} + \\binom{60}{2} + \\binom{10}{2} + \\binom{20}{2} + \\binom{70}{2}$\n  $= 12720 + 780 + 190 + 435 + 4005 + 1770 + 45 + 190 + 2415 = 22550$.\n\n- The sum of $\\binom{a_i}{2}$ over rows:\n  The row sums are $a_1=220$, $a_2=180$, $a_3=100$.\n  $\\sum_{i}\\binom{a_{i}}{2} = \\binom{220}{2} + \\binom{180}{2} + \\binom{100}{2} = 24090 + 16110 + 4950 = 45150$.\n\n- The sum of $\\binom{b_j}{2}$ over columns:\n  The column sums are $b_1=200$, $b_2=150$, $b_3=150$.\n  $\\sum_{j}\\binom{b_{j}}{2} = \\binom{200}{2} + \\binom{150}{2} + \\binom{150}{2} = 19900 + 11175 + 11175 = 42250$.\n\n- The total number of pairs $\\binom{N}{2}$:\n  $\\binom{500}{2} = \\frac{500 \\times 499}{2} = 124750$.\n\nNow, we calculate the components of the ARI formula:\n- Expected Index:\n  $\\frac{\\left[\\sum_i \\binom{a_i}{2}\\right] \\left[\\sum_j \\binom{b_j}{2}\\right]}{\\binom{N}{2}} = \\frac{45150 \\times 42250}{124750} = \\frac{1907587500}{124750} \\approx 15291.28256...$\n\n- Numerator of ARI = Index - Expected Index:\n  $22550 - 15291.28256... = 7258.71743...$\n\n- Denominator of ARI:\n  $\\frac{1}{2}\\left(\\sum_i \\binom{a_i}{2} + \\sum_j \\binom{b_j}{2}\\right) - \\text{Expected Index}$\n  $= \\frac{1}{2}(45150 + 42250) - 15291.28256...$\n  $= \\frac{1}{2}(87400) - 15291.28256... = 43700 - 15291.28256... = 28408.71743...$\n\n- Finally, the ARI:\n  $\\text{ARI} = \\frac{7258.71743...}{28408.71743...} \\approx 0.255511...$\nRounding to four significant figures, the ARI is $0.2555$.\n\nThe requested quantities are the silhouette score and the ARI, rounded to four significant figures.\nSilhouette Score $\\approx 0.3315$.\nAdjusted Rand Index $\\approx 0.2555$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.3315 & 0.2555\n\\end{pmatrix}\n}\n$$"
        }
    ]
}