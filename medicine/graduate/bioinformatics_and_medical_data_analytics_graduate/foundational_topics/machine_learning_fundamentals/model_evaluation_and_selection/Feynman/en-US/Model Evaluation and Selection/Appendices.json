{
    "hands_on_practices": [
        {
            "introduction": "Probabilistic classifiers are central to modern medical analytics, but their continuous outputs must often be translated into discrete clinical actions. This is achieved by applying a decision threshold, a critical choice that balances the risks of false positives and false negatives. This exercise  provides hands-on practice in calculating fundamental performance metrics like sensitivity and specificity at different thresholds, illustrating the explicit trade-off involved in setting an operating point for a diagnostic model.",
            "id": "4585247",
            "problem": "A single-stage binary disease classifier outputs predicted probabilities $p_i \\in [0,1]$ for each patient $i \\in \\{1,\\dots,100\\}$. A decision rule declares a patient predicted positive if $p_i \\ge t$ and predicted negative otherwise. You are given $100$ patients grouped by identical predicted probabilities and their realized binary labels. The grouping is as follows:\n- $30$ patients with $p_i = 0.05$, among whom $0$ are truly positive.\n- $20$ patients with $p_i = 0.15$, among whom $0$ are truly positive.\n- $20$ patients with $p_i = 0.25$, among whom $0$ are truly positive.\n- $10$ patients with $p_i = 0.35$, among whom $1$ is truly positive.\n- $10$ patients with $p_i = 0.60$, among whom $3$ are truly positive.\n- $10$ patients with $p_i = 0.80$, among whom $6$ are truly positive.\n\nThe disease prevalence in this cohort is $\\pi = 0.1$.\n\nUsing the standard definitions of sensitivity and specificity in terms of the confusion matrix, compute these two metrics at threshold $t=0.2$ and at threshold $t=0.5$. Then, form the contrast defined as the change in Youden’s $J$ statistic across these thresholds, where Youden’s $J$ statistic is the sum of sensitivity and specificity minus $1$. Report the value of $J(0.2)-J(0.5)$ as a single simplified exact number. No rounding is necessary, and no units should be included in the final answer.",
            "solution": "The user-provided problem has been rigorously validated and found to be self-contained, consistent, and scientifically sound. All necessary data and definitions are provided, and there are no contradictions. The problem is well-posed and objective, falling squarely within the domain of model evaluation in data analytics.\n\nThe objective is to compute the change in Youden's $J$ statistic between two decision thresholds, $t=0.2$ and $t=0.5$. Youden's $J$ statistic is defined as $J = \\text{Sensitivity} + \\text{Specificity} - 1$.\n\nFirst, we must establish the total number of condition positive and condition negative cases in the cohort of $100$ patients. The data is provided in groups based on predicted probabilities. Let's aggregate the number of truly positive patients.\n- Group with $p_i = 0.05$: $0$ positive cases.\n- Group with $p_i = 0.15$: $0$ positive cases.\n- Group with $p_i = 0.25$: $0$ positive cases.\n- Group with $p_i = 0.35$: $1$ positive case.\n- Group with $p_i = 0.60$: $3$ positive cases.\n- Group with $p_i = 0.80$: $6$ positive cases.\n\nThe total number of truly positive patients, denoted by $P$, is the sum:\n$$P = 0 + 0 + 0 + 1 + 3 + 6 = 10$$\nThe total number of patients in the cohort is given by the sum of patients in each group: $30 + 20 + 20 + 10 + 10 + 10 = 100$.\nThe total number of truly negative patients, denoted by $N$, is the total number of patients minus the total number of truly positive patients:\n$$N = 100 - P = 100 - 10 = 90$$\nThe prevalence $\\pi$ is $\\frac{P}{100} = \\frac{10}{100} = 0.1$, which matches the value provided in the problem statement, confirming data consistency.\n\nThe definitions for sensitivity and specificity are:\n$$ \\text{Sensitivity} = \\frac{\\text{True Positives (TP)}}{\\text{Total Positives (P)}} $$\n$$ \\text{Specificity} = \\frac{\\text{True Negatives (TN)}}{\\text{Total Negatives (N)}} $$\n\nWe will now compute these metrics for each threshold.\n\n**Case 1: Threshold $t = 0.2$**\n\nA patient is predicted positive if their predicted probability $p_i$ is greater than or equal to $t=0.2$. The groups meeting this criterion are those with $p_i$ of $0.25$, $0.35$, $0.60$, and $0.80$.\n\nThe number of True Positives at this threshold, $TP(0.2)$, is the sum of truly positive patients in these predicted positive groups:\n$$TP(0.2) = 0 + 1 + 3 + 6 = 10$$\nThe sensitivity at $t=0.2$ is:\n$$\\text{Sensitivity}(0.2) = \\frac{TP(0.2)}{P} = \\frac{10}{10} = 1$$\n\nA patient is predicted negative if $p_i < 0.2$. The groups meeting this criterion are those with $p_i$ of $0.05$ and $0.15$.\nThe number of True Negatives, $TN(0.2)$, is the sum of truly negative patients in these predicted negative groups.\n- For the group with $p_i = 0.05$: There are $30$ patients, $0$ of whom are positive, so all $30$ are negative.\n- For the group with $p_i = 0.15$: There are $20$ patients, $0$ of whom are positive, so all $20$ are negative.\n$$TN(0.2) = 30 + 20 = 50$$\nThe specificity at $t=0.2$ is:\n$$\\text{Specificity}(0.2) = \\frac{TN(0.2)}{N} = \\frac{50}{90} = \\frac{5}{9}$$\nYouden's $J$ statistic at $t=0.2$ is:\n$$J(0.2) = \\text{Sensitivity}(0.2) + \\text{Specificity}(0.2) - 1 = 1 + \\frac{5}{9} - 1 = \\frac{5}{9}$$\n\n**Case 2: Threshold $t = 0.5$**\n\nA patient is predicted positive if $p_i \\ge 0.5$. The groups meeting this criterion are those with $p_i$ of $0.60$ and $0.80$.\nThe number of True Positives at this threshold, $TP(0.5)$, is the sum of truly positive patients in these groups:\n$$TP(0.5) = 3 + 6 = 9$$\nThe sensitivity at $t=0.5$ is:\n$$\\text{Sensitivity}(0.5) = \\frac{TP(0.5)}{P} = \\frac{9}{10}$$\n\nA patient is predicted negative if $p_i < 0.5$. The groups meeting this criterion are those with $p_i$ of $0.05$, $0.15$, $0.25$, and $0.35$.\nThe number of True Negatives, $TN(0.5)$, is the sum of truly negative patients in these predicted negative groups.\n- For the group with $p_i = 0.05$: $30$ negative patients.\n- For the group with $p_i = 0.15$: $20$ negative patients.\n- For the group with $p_i = 0.25$: $20$ negative patients.\n- For the group with $p_i = 0.35$: There are $10$ patients, $1$ of whom is positive, so $10 - 1 = 9$ are negative.\n$$TN(0.5) = 30 + 20 + 20 + 9 = 79$$\nThe specificity at $t=0.5$ is:\n$$\\text{Specificity}(0.5) = \\frac{TN(0.5)}{N} = \\frac{79}{90}$$\nYouden's $J$ statistic at $t=0.5$ is:\n$$J(0.5) = \\text{Sensitivity}(0.5) + \\text{Specificity}(0.5) - 1 = \\frac{9}{10} + \\frac{79}{90} - 1$$\nTo combine these terms, we find a common denominator, which is $90$:\n$$J(0.5) = \\frac{9 \\times 9}{10 \\times 9} + \\frac{79}{90} - \\frac{90}{90} = \\frac{81}{90} + \\frac{79}{90} - \\frac{90}{90} = \\frac{81 + 79 - 90}{90} = \\frac{160 - 90}{90} = \\frac{70}{90} = \\frac{7}{9}$$\n\n**Final Calculation**\n\nThe problem requires the computation of the contrast $J(0.2) - J(0.5)$.\n$$J(0.2) - J(0.5) = \\frac{5}{9} - \\frac{7}{9} = \\frac{5-7}{9} = -\\frac{2}{9}$$\nThe result is an exact, simplified fraction.",
            "answer": "$$\\boxed{-\\frac{2}{9}}$$"
        },
        {
            "introduction": "The Area Under the Receiver Operating Characteristic (AUROC) curve is a ubiquitous metric for evaluating a model's ability to discriminate between classes. However, a high AUROC does not guarantee optimal performance for all practical applications, especially those requiring a single, well-chosen decision threshold. This problem  challenges the notion that identical AUROC implies identical utility by constructing a scenario where two models with the same ranking power exhibit different maximum achievable $F_1$ scores, highlighting the often-overlooked importance of score calibration and distribution.",
            "id": "4585277",
            "problem": "A clinical informatics team is evaluating two probabilistic binary classifiers for early septic shock detection from electronic health records. The ground-truth labels are fixed across the cohort of $N=10$ adult intensive care unit patients, with $Y_i \\in \\{0,1\\}$ indicating septic shock ($1$) or non-shock ($0$) status for patient $i$. The labels are: $Y_1=1$, $Y_2=1$, $Y_3=0$, $Y_4=1$, $Y_5=0$, $Y_6=1$, $Y_7=0$, $Y_8=0$, $Y_9=0$, $Y_{10}=1$, so there are $m=5$ positives and $n=5$ negatives.\n\nTwo models produce risk scores interpreted as predicted probabilities:\n- Model $\\mathrm{A}$ is overconfident and outputs $s^{(\\mathrm{A})}_1=0.98$, $s^{(\\mathrm{A})}_2=0.92$, $s^{(\\mathrm{A})}_3=0.83$, $s^{(\\mathrm{A})}_4=0.79$, $s^{(\\mathrm{A})}_5=0.72$, $s^{(\\mathrm{A})}_6=0.68$, $s^{(\\mathrm{A})}_7=0.55$, $s^{(\\mathrm{A})}_8=0.51$, $s^{(\\mathrm{A})}_9=0.44$, $s^{(\\mathrm{A})}_{10}=0.37$, reflecting sharp score separation.\n- Model $\\mathrm{B}$ is conservative and better calibrated to prevalence, with compressed scores: $s^{(\\mathrm{B})}_1=0.88$, $s^{(\\mathrm{B})}_2=0.78$, $s^{(\\mathrm{B})}_3=0.81$, $s^{(\\mathrm{B})}_4=0.80$, $s^{(\\mathrm{B})}_5=0.79$, $s^{(\\mathrm{B})}_6=0.77$, $s^{(\\mathrm{B})}_7=0.76$, $s^{(\\mathrm{B})}_8=0.74$, $s^{(\\mathrm{B})}_9=0.73$, $s^{(\\mathrm{B})}_{10}=0.75$, so the descending score order is $\\{1,3,4,5,2,6,7,10,8,9\\}$.\n\nUse the following foundational definitions:\n- The Area Under the Receiver Operating Characteristic (AUROC) equals the probability that a randomly drawn positive has a strictly higher score than a randomly drawn negative, plus one-half the probability of ties. For finite samples with $m$ positives and $n$ negatives, it can be computed as $\\mathrm{AUROC} = \\frac{U}{mn}$, where $U$ is the Mann–Whitney $U$ statistic counting positive–negative concordant pairs (with one-half weight for ties).\n- The $F_1$ score for a thresholded classifier is $F_1 = \\frac{2\\,\\mathrm{TP}}{2\\,\\mathrm{TP}+\\mathrm{FP}+\\mathrm{FN}}$, where $\\mathrm{TP}$, $\\mathrm{FP}$, and $\\mathrm{FN}$ denote true positives, false positives, and false negatives. A threshold on scores yields a ranking-based selection of the top $k$ predicted positives, $k \\in \\{1,2,\\dots,N\\}$, and determines $(\\mathrm{TP},\\mathrm{FP},\\mathrm{FN})$ from the prefix of the descending score order.\n\nTask: Verify that Models $\\mathrm{A}$ and $\\mathrm{B}$ have identical AUROC using the ranking interpretation of the Mann–Whitney $U$ statistic, then determine the maximum achievable $F_1$ for each model over all thresholds, and finally compute the difference $F_{1,\\max}^{(\\mathrm{B})} - F_{1,\\max}^{(\\mathrm{A})}$. Express your final answer as a single simplified fraction. No rounding is required.",
            "solution": "The problem requires a three-part analysis: first, to verify that two models, $\\mathrm{A}$ and $\\mathrm{B}$, have identical Area Under the Receiver Operating Characteristic (AUROC) values; second, to determine the maximum achievable $F_1$ score for each model across all possible classification thresholds; and third, to compute the difference between these maximum $F_1$ scores.\n\nThe ground-truth data consists of $N=10$ patients. The set of true labels is $\\{Y_1=1, Y_2=1, Y_3=0, Y_4=1, Y_5=0, Y_6=1, Y_7=0, Y_8=0, Y_9=0, Y_{10}=1\\}$.\nThis gives $m=5$ positive instances (patients $\\{1, 2, 4, 6, 10\\}$) and $n=5$ negative instances (patients $\\{3, 5, 7, 8, 9\\}$).\n\n## Part 1: Verification of Identical AUROC\n\nThe AUROC is defined as $\\frac{U}{mn}$, where $U$ is the Mann–Whitney $U$ statistic. For samples without score ties, $U$ is the number of pairs of one positive and one negative instance where the positive instance has a higher score. Since AUROC depends only on the rank ordering of scores, we first establish the rank of each patient for both models, with rank $1$ being the highest score and rank $10$ being the lowest.\n\n**Model A:**\nThe scores $s_i^{(\\mathrm{A})}$ are provided in descending order corresponding to patient indices $i=1, 2, \\dots, 10$.\nThe rank-ordered list of patient indices is $\\{1, 2, 3, 4, 5, 6, 7, 8, 9, 10\\}$.\nThe corresponding sequence of true labels is $\\{Y_1, Y_2, \\dots, Y_{10}\\}$, which is $\\{1, 1, 0, 1, 0, 1, 0, 0, 0, 1\\}$.\nThe positive samples (label $1$) are patients $\\{1, 2, 4, 6, 10\\}$. Their ranks are $\\{1, 2, 4, 6, 10\\}$.\nThe negative samples (label $0$) are patients $\\{3, 5, 7, 8, 9\\}$. Their ranks are $\\{3, 5, 7, 8, 9\\}$.\n\nWe calculate $U_A$ by summing, for each positive instance, the number of negative instances with a lower score (i.e., a higher rank number).\n- Patient $1$ (rank $1$): ranked higher than all $5$ negatives (ranks $3, 5, 7, 8, 9$). Contribution: $5$.\n- Patient $2$ (rank $2$): ranked higher than all $5$ negatives. Contribution: $5$.\n- Patient $4$ (rank $4$): ranked higher than $4$ negatives (ranks $5, 7, 8, 9$). Contribution: $4$.\n- Patient $6$ (rank $6$): ranked higher than $3$ negatives (ranks $7, 8, 9$). Contribution: $3$.\n- Patient $10$ (rank $10$): ranked higher than $0$ negatives. Contribution: $0$.\nThe $U$ statistic for Model A is $U_A = 5+5+4+3+0=17$.\nThe AUROC for Model A is $\\mathrm{AUROC}_A = \\frac{U_A}{mn} = \\frac{17}{5 \\times 5} = \\frac{17}{25}$.\n\n**Model B:**\nThe problem provides the rank-ordered list of patient indices: $\\{1, 3, 4, 5, 2, 6, 7, 10, 8, 9\\}$.\nThe corresponding sequence of true labels is $\\{Y_1, Y_3, Y_4, Y_5, Y_2, Y_6, Y_7, Y_{10}, Y_8, Y_9\\}$, which is $\\{1, 0, 1, 0, 1, 1, 0, 1, 0, 0\\}$.\nThe positive samples are patients $\\{1, 4, 2, 6, 10\\}$. Their ranks are $\\{1, 3, 5, 6, 8\\}$.\nThe negative samples are patients $\\{3, 5, 7, 8, 9\\}$. Their ranks are $\\{2, 4, 7, 9, 10\\}$.\n\nWe calculate $U_B$ similarly:\n- Patient $1$ (rank $1$): ranked higher than all $5$ negatives (ranks $2, 4, 7, 9, 10$). Contribution: $5$.\n- Patient $4$ (rank $3$): ranked higher than $4$ negatives (ranks $4, 7, 9, 10$). Contribution: $4$.\n- Patient $2$ (rank $5$): ranked higher than $3$ negatives (ranks $7, 9, 10$). Contribution: $3$.\n- Patient $6$ (rank $6$): ranked higher than $3$ negatives (ranks $7, 9, 10$). Contribution: $3$.\n- Patient $10$ (rank $8$): ranked higher than $2$ negatives (ranks $9, 10$). Contribution: $2$.\nThe $U$ statistic for Model B is $U_B = 5+4+3+3+2=17$.\nThe AUROC for Model B is $\\mathrm{AUROC}_B = \\frac{U_B}{mn} = \\frac{17}{5 \\times 5} = \\frac{17}{25}$.\n\nThus, we have verified that $\\mathrm{AUROC}_A = \\mathrm{AUROC}_B = \\frac{17}{25}$.\n\n## Part 2: Maximum Achievable $F_1$ Score\n\nThe $F_1$ score is given by $F_1 = \\frac{2\\,\\mathrm{TP}}{2\\,\\mathrm{TP}+\\mathrm{FP}+\\mathrm{FN}}$. A classification threshold corresponds to selecting the top $k$ patients from the rank-ordered list, for $k \\in \\{1, \\dots, 10\\}$. For such a selection, the number of predicted positives is $k = \\mathrm{TP}+\\mathrm{FP}$. The total number of actual positives is fixed at $m = \\mathrm{TP}+\\mathrm{FN} = 5$.\nWe can rewrite the $F_1$ score as a function of $k$:\n$$F_1(k) = \\frac{2\\,\\mathrm{TP}(k)}{(\\mathrm{TP}(k)+\\mathrm{FP}(k)) + (\\mathrm{TP}(k)+\\mathrm{FN}(k))} = \\frac{2\\,\\mathrm{TP}(k)}{k + m} = \\frac{2\\,\\mathrm{TP}(k)}{k+5}$$\nwhere $\\mathrm{TP}(k)$ is the number of true positives among the top $k$ ranked patients. We now compute $F_1(k)$ for each model for $k=1, \\dots, 10$.\n\n**Model A:**\nThe rank-ordered label sequence is $\\{1, 1, 0, 1, 0, 1, 0, 0, 0, 1\\}$.\n- $k=1$: $\\mathrm{TP}(1)=1$, $F_1(1) = \\frac{2(1)}{1+5} = \\frac{2}{6} = \\frac{1}{3}$.\n- $k=2$: $\\mathrm{TP}(2)=2$, $F_1(2) = \\frac{2(2)}{2+5} = \\frac{4}{7}$.\n- $k=3$: $\\mathrm{TP}(3)=2$, $F_1(3) = \\frac{2(2)}{3+5} = \\frac{4}{8} = \\frac{1}{2}$.\n- $k=4$: $\\mathrm{TP}(4)=3$, $F_1(4) = \\frac{2(3)}{4+5} = \\frac{6}{9} = \\frac{2}{3}$.\n- $k=5$: $\\mathrm{TP}(5)=3$, $F_1(5) = \\frac{2(3)}{5+5} = \\frac{6}{10} = \\frac{3}{5}$.\n- $k=6$: $\\mathrm{TP}(6)=4$, $F_1(6) = \\frac{2(4)}{6+5} = \\frac{8}{11}$.\n- $k=7$: $\\mathrm{TP}(7)=4$, $F_1(7) = \\frac{2(4)}{7+5} = \\frac{8}{12} = \\frac{2}{3}$.\n- $k=8$: $\\mathrm{TP}(8)=4$, $F_1(8) = \\frac{2(4)}{8+5} = \\frac{8}{13}$.\n- $k=9$: $\\mathrm{TP}(9)=4$, $F_1(9) = \\frac{2(4)}{9+5} = \\frac{8}{14} = \\frac{4}{7}$.\n- $k=10$: $\\mathrm{TP}(10)=5$, $F_1(10) = \\frac{2(5)}{10+5} = \\frac{10}{15} = \\frac{2}{3}$.\nTo find the maximum, we compare the values. $\\frac{8}{11} \\approx 0.727$ is greater than $\\frac{2}{3} \\approx 0.667$ and $\\frac{8}{13} \\approx 0.615$. Thus, the maximum $F_1$ score for Model A is $F_{1,\\max}^{(\\mathrm{A})} = \\frac{8}{11}$.\n\n**Model B:**\nThe rank-ordered label sequence is $\\{1, 0, 1, 0, 1, 1, 0, 1, 0, 0\\}$.\n- $k=1$: $\\mathrm{TP}(1)=1$, $F_1(1) = \\frac{2(1)}{1+5} = \\frac{1}{3}$.\n- $k=2$: $\\mathrm{TP}(2)=1$, $F_1(2) = \\frac{2(1)}{2+5} = \\frac{2}{7}$.\n- $k=3$: $\\mathrm{TP}(3)=2$, $F_1(3) = \\frac{2(2)}{3+5} = \\frac{1}{2}$.\n- $k=4$: $\\mathrm{TP}(4)=2$, $F_1(4) = \\frac{2(2)}{4+5} = \\frac{4}{9}$.\n- $k=5$: $\\mathrm{TP}(5)=3$, $F_1(5) = \\frac{2(3)}{5+5} = \\frac{3}{5}$.\n- $k=6$: $\\mathrm{TP}(6)=4$, $F_1(6) = \\frac{2(4)}{6+5} = \\frac{8}{11}$.\n- $k=7$: $\\mathrm{TP}(7)=4$, $F_1(7) = \\frac{2(4)}{7+5} = \\frac{2}{3}$.\n- $k=8$: $\\mathrm{TP}(8)=5$, $F_1(8) = \\frac{2(5)}{8+5} = \\frac{10}{13}$.\n- $k=9$: $\\mathrm{TP}(9)=5$, $F_1(9) = \\frac{2(5)}{9+5} = \\frac{10}{14} = \\frac{5}{7}$.\n- $k=10$: $\\mathrm{TP}(10)=5$, $F_1(10) = \\frac{2(5)}{10+5} = \\frac{2}{3}$.\nTo find the maximum, we compare the larger values: $\\frac{8}{11} \\approx 0.727$, $\\frac{10}{13} \\approx 0.769$, and $\\frac{5}{7} \\approx 0.714$. The largest value is $\\frac{10}{13}$. Thus, the maximum $F_1$ score for Model B is $F_{1,\\max}^{(\\mathrm{B})} = \\frac{10}{13}$.\n\n## Part 3: Difference in Maximum $F_1$ Scores\n\nThe final task is to compute the difference $F_{1,\\max}^{(\\mathrm{B})} - F_{1,\\max}^{(\\mathrm{A})}$.\n$$F_{1,\\max}^{(\\mathrm{B})} - F_{1,\\max}^{(\\mathrm{A})} = \\frac{10}{13} - \\frac{8}{11}$$\nTo subtract the fractions, we find a common denominator, which is $13 \\times 11 = 143$.\n$$\\frac{10 \\times 11}{13 \\times 11} - \\frac{8 \\times 13}{11 \\times 13} = \\frac{110}{143} - \\frac{104}{143} = \\frac{110 - 104}{143} = \\frac{6}{143}$$\nThe fraction $\\frac{6}{143}$ is in simplest form, as $6 = 2 \\times 3$ and $143=11 \\times 13$ share no common factors.\nThis result demonstrates that even with identical discrimination power (AUROC), models can exhibit different optimal performances under threshold-based metrics like the $F_1$ score, due to differences in how they rank individual cases.",
            "answer": "$$\\boxed{\\frac{6}{143}}$$"
        },
        {
            "introduction": "When comparing two models, observing a difference in a performance metric on a single test set is not sufficient to declare a winner; this difference could be due to random chance. To make a rigorous conclusion, we must assess statistical significance, which is especially important in high-stakes medical applications. This practice  guides you through implementing a paired nonparametric bootstrap test for the Brier score, a powerful technique that respects the paired nature of data and provides a statistically sound basis for model selection without strong distributional assumptions.",
            "id": "4585270",
            "problem": "You are tasked with constructing a complete, runnable program that implements a paired nonparametric bootstrap hypothesis test for the difference in Brier score between two probabilistic classifiers evaluated on the same set of patients. The Brier score for a binary outcome is defined as follows. For patient-level outcomes $y_i \\in \\{0,1\\}$ and predicted probabilities $p_i^{(m)} \\in [0,1]$ from model $m \\in \\{1,2\\}$, and a dataset of size $n$, the empirical Brier score of model $m$ is\n$$\n\\mathrm{BS}_m = \\frac{1}{n} \\sum_{i=1}^{n} \\left( p_i^{(m)} - y_i \\right)^2.\n$$\nDefine the observed difference in Brier score as\n$$\n\\Delta_{\\mathrm{obs}} = \\mathrm{BS}_1 - \\mathrm{BS}_2.\n$$\nLower values of $\\mathrm{BS}_m$ indicate better probabilistic accuracy. The analysis must respect the pairing at the patient level, since both models are evaluated on the same patients, inducing within-patient correlation between $\\left(p_i^{(1)}, p_i^{(2)}, y_i\\right)$.\n\nYour task is to implement a hypothesis test for the null hypothesis $H_0: \\Delta = 0$ versus the two-sided alternative $H_1: \\Delta \\neq 0$ using a paired nonparametric bootstrap over patient-level units. The bootstrap must resample patient indices $i \\in \\{1,\\dots,n\\}$ with replacement and recompute the Brier score difference on each bootstrap sample to generate an empirical distribution. Let $\\Delta_b^{\\ast}$ denote the Brier difference computed on bootstrap replicate $b \\in \\{1,\\dots,B\\}$. Use the following principle-based procedures:\n\n1. Observed statistic. Compute $\\Delta_{\\mathrm{obs}}$ on the full dataset.\n\n2. Paired bootstrap resampling. For each bootstrap replicate $b$, draw $n$ indices with replacement from $\\{1,\\dots,n\\}$, apply these indices to the patient-level triplets $\\left(p_i^{(1)}, p_i^{(2)}, y_i\\right)$, and compute\n$$\n\\Delta_b^{\\ast} = \\frac{1}{n} \\sum_{j=1}^{n} \\left( p_{i_j}^{(1)} - y_{i_j} \\right)^2 - \\frac{1}{n} \\sum_{j=1}^{n} \\left( p_{i_j}^{(2)} - y_{i_j} \\right)^2,\n$$\nwhere $i_j$ are the resampled indices for replicate $b$.\n\n3. Two-sided bootstrap achieved significance level. Compute the two-sided bootstrap $p$-value using the centered bootstrap distribution to approximate the sampling distribution under $H_0$:\n$$\np_{\\mathrm{boot}} = \\frac{1 + \\sum_{b=1}^{B} \\mathbf{1}\\left( \\left| \\Delta_b^{\\ast} - \\Delta_{\\mathrm{obs}} \\right| \\ge \\left| \\Delta_{\\mathrm{obs}} \\right| \\right)}{B + 1}.\n$$\nThis formula includes the observed statistic to avoid a zero $p$-value for finite $B$.\n\n4. Bootstrap confidence interval. Construct a two-sided $95\\%$ confidence interval for $\\Delta$ by the percentile method using the empirical quantiles of $\\{\\Delta_b^{\\ast}\\}_{b=1}^{B}$:\n$$\n\\left[ \\mathrm{quantile}_{0.025}\\left(\\{\\Delta_b^{\\ast}\\}\\right),\\ \\mathrm{quantile}_{0.975}\\left(\\{\\Delta_b^{\\ast}\\}\\right) \\right].\n$$\n\nAssumptions you must rely on and not violate: patient-level observations $\\left(p_i^{(1)}, p_i^{(2)}, y_i\\right)$ are independent and identically distributed (i.i.d.) across $i$, with $y_i \\in \\{0,1\\}$ and $p_i^{(m)} \\in [0,1]$. The pairing of $\\left(p_i^{(1)}, p_i^{(2)}, y_i\\right)$ must be preserved in each bootstrap replicate.\n\nYour program must implement the above for a fixed test suite of scenarios. In each scenario, you must generate the outcomes and model predictions deterministically from the provided parameters, using the logistic function $\\sigma(t) = \\frac{1}{1 + e^{-t}}$ and a specified random seed for outcome generation. For each scenario, compute and report:\n- the observed Brier score difference $\\Delta_{\\mathrm{obs}}$,\n- the two-sided bootstrap $p$-value $p_{\\mathrm{boot}}$,\n- the lower and upper endpoints of the $95\\%$ percentile bootstrap confidence interval,\n- and a boolean decision $\\mathrm{reject}_{0.05}$ indicating whether $p_{\\mathrm{boot}} < 0.05$.\n\nAll numeric outputs must be reported as decimals rounded to six digits after the decimal point. Booleans must be reported as either True or False.\n\nTest suite specification. For each test case $k$, define $n$, the outcome-generating seed $s_y$, the bootstrap seed $s_b$, the number of bootstrap replicates $B$, and the recipe for constructing the patient-level covariates, outcomes, and predictions. Use the following four cases:\n\n- Case A (moderate sample, differing slopes):\n  - $n = 60$.\n  - Construct $x_i$ as a linearly spaced grid from $-2$ to $2$ with $n$ points.\n  - Define the true probability function $q_i = \\sigma(0.5 x_i - 0.3)$.\n  - Generate $y_i \\sim \\mathrm{Bernoulli}(q_i)$ using seed $s_y = 0$.\n  - Model $1$ predictions: $p_i^{(1)} = \\sigma(0.5 x_i - 0.3)$.\n  - Model $2$ predictions: $p_i^{(2)} = \\sigma(0.8 x_i - 0.3)$.\n  - Bootstrap: $B = 4000$, seed $s_b = 123$.\n\n- Case B (identical models, sanity check):\n  - $n = 40$.\n  - Construct $x_i$ as a linearly spaced grid from $-1$ to $1$ with $n$ points.\n  - Define $q_i = \\sigma(0.2 x_i + 0.1)$.\n  - Generate $y_i \\sim \\mathrm{Bernoulli}(q_i)$ using seed $s_y = 202$.\n  - Model $1$ predictions: $p_i^{(1)} = q_i$.\n  - Model $2$ predictions: $p_i^{(2)} = q_i$.\n  - Bootstrap: $B = 3000$, seed $s_b = 321$.\n\n- Case C (class imbalance, constant predictors):\n  - $n = 80$.\n  - Set $q_i \\equiv 0.08$ for all $i$.\n  - Generate $y_i \\sim \\mathrm{Bernoulli}(q_i)$ using seed $s_y = 404$.\n  - Model $1$ predictions: $p_i^{(1)} \\equiv 0.08$.\n  - Model $2$ predictions: $p_i^{(2)} \\equiv 0.15$.\n  - Bootstrap: $B = 5000$, seed $s_b = 999$.\n\n- Case D (tiny sample, different calibration strengths):\n  - $n = 10$.\n  - Define $x = [-4.0, -2.0, -1.0, -0.5, 0.0, 0.5, 1.0, 2.0, 3.0, 4.0]$.\n  - Define $q_i = \\sigma(1.2 x_i - 0.5)$.\n  - Generate $y_i \\sim \\mathrm{Bernoulli}(q_i)$ using seed $s_y = 77$.\n  - Model $1$ predictions: $p_i^{(1)} = \\sigma(1.2 x_i - 0.5)$.\n  - Model $2$ predictions: $p_i^{(2)} = \\sigma(0.6 x_i - 0.2)$.\n  - Bootstrap: $B = 2000$, seed $s_b = 42$.\n\nFinal output format. Your program should produce a single line of output containing a list of four sublists, one per test case, in the order A, B, C, D. Each sublist must contain five elements in the order $\\left[\\Delta_{\\mathrm{obs}}, p_{\\mathrm{boot}}, \\mathrm{CI}_{\\mathrm{low}}, \\mathrm{CI}_{\\mathrm{high}}, \\mathrm{reject}_{0.05}\\right]$. The entire output must be rendered in a single line as a bracketed, comma-separated list of these sublists with no spaces, for example:\n[[0.012345,0.502000,-0.010000,0.034000,False],[...],[...],[...]]",
            "solution": "The problem is valid. It presents a clear, well-defined, and scientifically sound task in the domain of statistical model comparison, which is a fundamental aspect of bioinformatics and medical data analytics. All necessary data, formulae, and computational procedures are specified without ambiguity, making the problem self-contained and well-posed. The methodology, employing a paired nonparametric bootstrap for the difference in Brier scores, is a standard and appropriate technique for comparing probabilistic classifiers on the same dataset.\n\nThe solution will be implemented by first defining a function to execute the complete pipeline for a single test case. This includes deterministic data generation, computation of the observed statistic, execution of the bootstrap resampling procedure, and calculation of the final results ($p$-value and confidence interval). A main function will then iterate through the four specified test cases, collect the results, and format them into the required single-line string output.\n\n**1. Problem Formulation**\n\nThe objective is to compare two probabilistic classifiers, denoted as model $1$ and model $2$, based on their predictive accuracy for a binary outcome. The models are evaluated on the same set of $n$ patients, for whom we have the true outcomes $y_i \\in \\{0, 1\\}$ and the predicted probabilities from each model, $p_i^{(1)}$ and $p_i^{(2)}$, for $i \\in \\{1, \\dots, n\\}$.\n\nThe Brier score ($\\mathrm{BS}$) is used as the evaluation metric. For a given model $m$, it is the mean squared error between the predicted probabilities and the actual outcomes:\n$$\n\\mathrm{BS}_m = \\frac{1}{n} \\sum_{i=1}^{n} \\left( p_i^{(m)} - y_i \\right)^2.\n$$\nA lower Brier score indicates better model calibration and accuracy. The statistic of interest is the difference in Brier scores between the two models:\n$$\n\\Delta = \\mathrm{BS}_1 - \\mathrm{BS}_2.\n$$\nA value of $\\Delta < 0$ suggests that model $1$ is superior to model $2$, while $\\Delta > 0$ suggests the opposite. The observed difference on the given dataset is denoted as $\\Delta_{\\mathrm{obs}}$.\n\n**2. Hypothesis Testing Framework**\n\nWe perform a hypothesis test to determine if the observed difference $\\Delta_{\\mathrm{obs}}$ is statistically significant. The null and alternative hypotheses are:\n- Null Hypothesis, $H_0: \\Delta = 0$. There is no difference in performance between the two models.\n- Alternative Hypothesis, $H_1: \\Delta \\neq 0$. There is a difference in performance.\n\nSince both models are evaluated on the same set of patients, the errors $(p_i^{(1)} - y_i)^2$ and $(p_i^{(2)} - y_i)^2$ are not independent. The patient-level triplets $(p_i^{(1)}, p_i^{(2)}, y_i)$ are the fundamental units of observation, and their paired structure must be preserved during statistical testing. A paired nonparametric bootstrap is an appropriate method for this scenario.\n\n**3. Paired Nonparametric Bootstrap Procedure**\n\nThe procedure consists of four main steps:\n\n**Step 1: Compute Observed Statistic**\nFirst, we compute the Brier scores for model $1$ and model $2$ on the original, complete dataset.\n$$\n\\mathrm{BS}_1 = \\frac{1}{n} \\sum_{i=1}^{n} \\left( p_i^{(1)} - y_i \\right)^2 \\quad \\text{and} \\quad \\mathrm{BS}_2 = \\frac{1}{n} \\sum_{i=1}^{n} \\left( p_i^{(2)} - y_i \\right)^2.\n$$\nThe observed difference is then calculated as $\\Delta_{\\mathrm{obs}} = \\mathrm{BS}_1 - \\mathrm{BS}_2$.\n\n**Step 2: Generate Bootstrap Distribution**\nWe generate an empirical sampling distribution for the statistic $\\Delta$ by resampling from the original data. This is done for $B$ bootstrap replicates. For each replicate $b \\in \\{1, \\dots, B\\}$:\n- A bootstrap sample is created by drawing $n$ indices, $\\{i_1, i_2, \\dots, i_n\\}$, from the original set of indices $\\{1, 2, \\dots, n\\}$ with replacement.\n- Using these resampled indices, we form a new dataset of patient triplets: $\\{(p_{i_j}^{(1)}, p_{i_j}^{(2)}, y_{i_j})\\}_{j=1}^n$.\n- The Brier score difference is computed for this bootstrap sample:\n$$\n\\Delta_b^{\\ast} = \\frac{1}{n} \\sum_{j=1}^{n} \\left( p_{i_j}^{(1)} - y_{i_j} \\right)^2 - \\frac{1}{n} \\sum_{j=1}^{n} \\left( p_{i_j}^{(2)} - y_{i_j} \\right)^2.\n$$\nThis process is repeated $B$ times to obtain a collection of bootstrap statistics, $\\{\\Delta_b^{\\ast}\\}_{b=1}^B$. This collection approximates the sampling distribution of $\\Delta$.\n\n**Step 3: Calculate Two-Sided p-value**\nTo test the null hypothesis $H_0: \\Delta = 0$, we assess how extreme the observed statistic $\\Delta_{\\mathrm{obs}}$ is relative to the distribution of $\\Delta$ under $H_0$. The bootstrap distribution $\\{\\Delta_b^{\\ast}\\}$ is centered around $\\Delta_{\\mathrm{obs}}$. To simulate the null distribution (which is centered at $0$), we can center the bootstrap distribution by subtracting $\\Delta_{\\mathrm{obs}}$ from each $\\Delta_b^{\\ast}$. The test measures the probability of observing a difference at least as large as $|\\Delta_{\\mathrm{obs}}|$ under this null distribution. This corresponds to counting the proportion of bootstrap replicates where the centered statistic, $\\Delta_b^{\\ast} - \\Delta_{\\mathrm{obs}}$, is at least as far from $0$ as $\\Delta_{\\mathrm{obs}}$ is. The two-sided $p$-value is formally computed as:\n$$\np_{\\mathrm{boot}} = \\frac{1 + \\sum_{b=1}^{B} \\mathbf{1}\\left( \\left| \\Delta_b^{\\ast} - \\Delta_{\\mathrm{obs}} \\right| \\ge \\left| \\Delta_{\\mathrm{obs}} \\right| \\right)}{B + 1}.\n$$\nThe indicator function $\\mathbf{1}(\\cdot)$ is $1$ if its argument is true and $0$ otherwise. Adding $1$ to both the numerator and denominator prevents a $p$-value of $0$ for a finite number of replicates $B$ and accounts for the observed sample itself.\n\n**Step 4: Construct Confidence Interval**\nA two-sided $95\\%$ confidence interval (CI) for the true difference $\\Delta$ can be constructed using the percentile method. This method uses the quantiles of the uncentered bootstrap distribution $\\{\\Delta_b^{\\ast}\\}$. The lower and upper bounds of the CI are given by the $2.5$-th and $97.5$-th percentiles of the empirical distribution of $\\Delta_b^{\\ast}$:\n$$\n\\mathrm{CI}_{95\\%} = \\left[ \\mathrm{quantile}_{0.025}\\left(\\{\\Delta_b^{\\ast}\\}\\right),\\ \\mathrm{quantile}_{0.975}\\left(\\{\\Delta_b^{\\ast}\\}\\right) \\right].\n$$\nIf this interval does not contain $0$, it provides evidence against the null hypothesis $H_0$ at the $\\alpha=0.05$ significance level.\n\n**4. Implementation Details**\n\nThe implementation will be carried out in Python using the `numpy` library. The use of `numpy.random.default_rng` with specified seeds ensures the reproducibility of both the synthetic data generation and the bootstrap resampling process. Vectorized operations in `numpy` will be leveraged for efficient computation of Brier scores and bootstrap statistics. Each test case will be processed by a dedicated function that encapsulates the logic described above, and the final results will be formatted to meet the precise output specification.\nThe logistic function, $\\sigma(t) = (1 + e^{-t})^{-1}$, is used for generating true probabilities and model predictions as specified in the test cases.\nThe final decision to reject $H_0$ is made by comparing the computed $p$-value to the significance level $\\alpha = 0.05$.\nThe decision is `True` if $p_{\\mathrm{boot}}  0.05$ and `False` otherwise.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n\n    # Logistic (sigmoid) function\n    def sigmoid(t):\n        return 1 / (1 + np.exp(-t))\n\n    def run_bootstrap_test(n, data_gen_params, B, s_y, s_b):\n        \"\"\"\n        Executes the paired nonparametric bootstrap test for a single scenario.\n\n        Args:\n            n (int): Sample size.\n            data_gen_params (dict): Parameters to generate data (x_vals, q_func, p1_func, p2_func).\n            B (int): Number of bootstrap replicates.\n            s_y (int): Random seed for outcome generation.\n            s_b (int): Random seed for bootstrap resampling.\n\n        Returns:\n            list: A list containing [delta_obs, p_boot, ci_low, ci_high, reject_0.05].\n        \"\"\"\n        # --- 1. Data Generation ---\n        rng_y = np.random.default_rng(s_y)\n        \n        x_vals = data_gen_params['x_vals']\n        q_func = data_gen_params['q_func']\n        p1_func = data_gen_params['p1_func']\n        p2_func = data_gen_params['p2_func']\n\n        q = q_func(x_vals)\n        p1 = p1_func(x_vals)\n        p2 = p2_func(x_vals)\n        \n        # Generate binary outcomes y_i ~ Bernoulli(q_i)\n        y = rng_y.binomial(1, q, size=n)\n\n        # --- 2. Observed Statistic ---\n        # Calculate individual squared errors\n        e1 = (p1 - y)**2\n        e2 = (p2 - y)**2\n        \n        # Calculate Brier Scores\n        bs1_obs = np.mean(e1)\n        bs2_obs = np.mean(e2)\n        \n        # Observed difference\n        delta_obs = bs1_obs - bs2_obs\n\n        # --- 3. Paired Bootstrap Resampling ---\n        rng_b = np.random.default_rng(s_b)\n        delta_star_values = np.zeros(B)\n        \n        for b in range(B):\n            # Draw n indices with replacement\n            indices = rng_b.integers(0, n, size=n)\n            \n            # Create bootstrap sample of squared errors\n            e1_b = e1[indices]\n            e2_b = e2[indices]\n            \n            # Compute Brier score difference on the bootstrap sample\n            # This is equivalent to np.mean(e1_b) - np.mean(e2_b)\n            delta_star_values[b] = np.mean(e1_b - e2_b)\n            \n        # --- 4. p-value and Confidence Interval ---\n        # Two-sided bootstrap p-value\n        abs_delta_obs = np.abs(delta_obs)\n        # Handle the case where delta_obs is numerically zero\n        if abs_delta_obs  1e-12:\n            p_boot = 1.0\n        else:\n            count = np.sum(np.abs(delta_star_values - delta_obs) >= abs_delta_obs)\n            p_boot = (1 + count) / (B + 1)\n        \n        # 95% percentile confidence interval\n        ci_low = np.quantile(delta_star_values, 0.025)\n        ci_high = np.quantile(delta_star_values, 0.975)\n        \n        # Decision rule\n        reject_0_05 = p_boot  0.05\n        \n        return [\n            round(delta_obs, 6),\n            round(p_boot, 6),\n            round(ci_low, 6),\n            round(ci_high, 6),\n            reject_0_05\n        ]\n\n    # Test suite specification\n    test_cases = [\n        {\n            \"name\": \"Case A\",\n            \"n\": 60,\n            \"data_gen_params\": {\n                \"x_vals\": np.linspace(-2, 2, 60),\n                \"q_func\": lambda x: sigmoid(0.5 * x - 0.3),\n                \"p1_func\": lambda x: sigmoid(0.5 * x - 0.3),\n                \"p2_func\": lambda x: sigmoid(0.8 * x - 0.3),\n            },\n            \"B\": 4000,\n            \"s_y\": 0,\n            \"s_b\": 123,\n        },\n        {\n            \"name\": \"Case B\",\n            \"n\": 40,\n            \"data_gen_params\": {\n                \"x_vals\": np.linspace(-1, 1, 40),\n                \"q_func\": lambda x: sigmoid(0.2 * x + 0.1),\n                \"p1_func\": lambda x: sigmoid(0.2 * x + 0.1),\n                \"p2_func\": lambda x: sigmoid(0.2 * x + 0.1),\n            },\n            \"B\": 3000,\n            \"s_y\": 202,\n            \"s_b\": 321,\n        },\n        {\n            \"name\": \"Case C\",\n            \"n\": 80,\n            \"data_gen_params\": {\n                \"x_vals\": None, # Not used\n                \"q_func\": lambda x: np.full(80, 0.08),\n                \"p1_func\": lambda x: np.full(80, 0.08),\n                \"p2_func\": lambda x: np.full(80, 0.15),\n            },\n            \"B\": 5000,\n            \"s_y\": 404,\n            \"s_b\": 999,\n        },\n        {\n            \"name\": \"Case D\",\n            \"n\": 10,\n            \"data_gen_params\": {\n                \"x_vals\": np.array([-4.0, -2.0, -1.0, -0.5, 0.0, 0.5, 1.0, 2.0, 3.0, 4.0]),\n                \"q_func\": lambda x: sigmoid(1.2 * x - 0.5),\n                \"p1_func\": lambda x: sigmoid(1.2 * x - 0.5),\n                \"p2_func\": lambda x: sigmoid(0.6 * x - 0.2),\n            },\n            \"B\": 2000,\n            \"s_y\": 77,\n            \"s_b\": 42,\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result = run_bootstrap_test(\n            case[\"n\"],\n            case[\"data_gen_params\"],\n            case[\"B\"],\n            case[\"s_y\"],\n            case[\"s_b\"],\n        )\n        all_results.append(result)\n    \n    # Format the final output string exactly as specified\n    outer_list = []\n    for inner_list in all_results:\n        # Format numbers to 6 decimal places, boolean as string\n        str_list = [f\"{v:.6f}\" for v in inner_list[:4]] + [str(inner_list[4])]\n        outer_list.append(f\"[{','.join(str_list)}]\")\n    \n    final_output = f\"[[{','.join(outer_list)}]]\"\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}