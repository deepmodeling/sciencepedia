{
    "hands_on_practices": [
        {
            "introduction": "This first exercise grounds your understanding in the fundamental principles of non-parametric hypothesis testing. You will implement a permutation-based paired sign test from scratch, a robust method for analyzing matched-pair data commonly found in genomics, such as tumor-normal comparisons . This practice not only reinforces the logic of permutation testing but also provides practical experience in controlling the False Discovery Rate (FDR) using the Benjamini-Hochberg procedure across multiple tests.",
            "id": "4609525",
            "problem": "You are given matched tumor–normal DNA methylation beta values for cohorts of patients. Each pair consists of a tumor and its matched normal for the same patient at the same cytosine-phosphate-guanine (CpG) locus. Assume that beta values are bounded in the unit interval $[0,1]$ and that pairs are exchangeable under the null hypothesis of no systematic shift between tumor and normal. Your task is to construct a permutation version of the paired sign test to detect a shift in the distribution of within-pair differences and to quantify error control via the Benjamini–Hochberg (BH) procedure.\n\nFundamental base to use:\n- Under the null hypothesis $H_0$ of no shift, the sign of the within-pair difference is equally likely to be positive or negative for any non-zero difference, and signs are independent across pairs. Formally, let $d_i = x_i^{\\mathrm{tumor}} - x_i^{\\mathrm{normal}}$. Under $H_0$, conditional on $\\{d_i \\neq 0\\}$, the signs are independent and identically distributed with probability $\\mathbb{P}(\\mathrm{sign}(d_i) = +1) = 1/2$. Pairs with $d_i = 0$ carry no information about the sign and are excluded from the effective sample size.\n- A permutation of tumor and normal within each pair is equivalent to independently flipping the sign of each non-zero $d_i$ with probability $1/2$, giving the exact permutation distribution for the sign count.\n- The Benjamini–Hochberg procedure at target false discovery rate (FDR) level $q$ rejects hypotheses by comparing ordered p-values $p_{(k)}$ to thresholds $(k/m)q$, where $m$ is the number of hypotheses tested in a family.\n\nDefinition of the test statistic and p-value:\n- Let $n$ be the number of non-zero differences and let $S$ denote the number of positive differences among these $n$. Define the two-sided test statistic as $T = \\lvert S - n/2 \\rvert$.\n- Under $H_0$ and permutation invariance, $S$ follows a binomial distribution with parameters $n$ and $1/2$, that is $S \\sim \\mathrm{Binomial}(n, 1/2)$, because the $n$ sign flips are independent and equally likely.\n- The exact permutation p-value (two-sided) is the proportion of all $2^n$ sign-flip assignments such that the resulting $S^\\ast$ satisfies $\\lvert S^\\ast - n/2 \\rvert \\ge \\lvert S - n/2 \\rvert$. Equivalently,\n$$\np = \\frac{1}{2^n} \\sum_{k=0}^{n} \\mathbf{1}\\left(\\left|k - \\frac{n}{2}\\right| \\ge \\left|S - \\frac{n}{2}\\right|\\right) \\binom{n}{k}.\n$$\n- Ties are defined by a tolerance $\\tau = 10^{-12}$: any $\\lvert d_i \\rvert  \\tau$ is treated as zero and excluded from $n$.\n\nYour program must:\n1. Implement the exact permutation paired sign test as above to compute two-sided p-values for one or multiple CpG features per dataset.\n2. Within each dataset, apply the Benjamini–Hochberg procedure at target FDR $q = 0.1$ to the vector of p-values across features in that dataset, producing a boolean decision vector indicating which features are rejected.\n\nTest suite:\n- Use the following three datasets as the complete test suite. All numbers are methylation beta values in $[0,1]$.\n\nDataset A (one feature, $n_{\\mathrm{pairs}} = 8$):\n- Normal: $\\,[0.65,\\,0.72,\\,0.48,\\,0.51,\\,0.40,\\,0.30,\\,0.55,\\,0.60]$\n- Tumor:  $\\,[0.70,\\,0.80,\\,0.55,\\,0.58,\\,0.50,\\,0.45,\\,0.62,\\,0.58]$\n\nDataset B (one feature with ties, $n_{\\mathrm{pairs}} = 10$; ties remove three pairs, leaving $n = 7$ non-zero differences):\n- Normal: $\\,[0.40,\\,0.50,\\,0.60,\\,0.55,\\,0.20,\\,0.80,\\,0.35,\\,0.35,\\,0.90,\\,0.10]$\n- Tumor:  $\\,[0.46,\\,0.48,\\,0.60,\\,0.59,\\,0.25,\\,0.80,\\,0.33,\\,0.30,\\,0.90,\\,0.13]$\n\nDataset C (four features, $n_{\\mathrm{pairs}} = 6$; columns are distinct CpG features):\n- Normal matrix ($6 \\times 4$):\n$\\begin{bmatrix}\n0.20  0.40  0.70  0.60 \\\\\n0.30  0.45  0.50  0.40 \\\\\n0.10  0.35  0.20  0.30 \\\\\n0.25  0.55  0.60  0.50 \\\\\n0.15  0.50  0.30  0.70 \\\\\n0.40  0.60  0.80  0.65 \\\\\n\\end{bmatrix}$\n- Tumor matrix ($6 \\times 4$):\n$\\begin{bmatrix}\n0.25  0.46  0.75  0.58 \\\\\n0.35  0.51  0.48  0.38 \\\\\n0.15  0.41  0.25  0.28 \\\\\n0.30  0.61  0.58  0.48 \\\\\n0.20  0.56  0.28  0.68 \\\\\n0.45  0.58  0.78  0.63 \\\\\n\\end{bmatrix}$\n\nAssumptions and conventions:\n- Use tolerance $\\tau = 10^{-12}$ when deciding whether $\\lvert d_i \\rvert$ is zero.\n- If $n = 0$, define the p-value to be $1$.\n- For the BH procedure, use $q = 0.1$ independently within each dataset’s family of features.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each dataset, output a two-element list: the first element is the list of two-sided p-values for all features in that dataset ordered as given, and the second element is the list of corresponding BH rejection decisions (booleans) in the same order. Floats must be rounded to eight decimal places and printed without trailing zeros or a trailing decimal point.\n- Concretely, the output must look like a nested list of the form\n\"[ [pvals_A, decisions_A], [pvals_B, decisions_B], [pvals_C, decisions_C] ]\"\nwhere each pvals_X is a list of floats and each decisions_X is a list of booleans.\n\nYour solution will be assessed for scientific correctness, algorithmic rigor, exact adherence to the definition of the permutation distribution, correct handling of ties, and proper implementation of the Benjamini–Hochberg procedure. No external input is required and no physical units are involved. Express answers as decimals rather than percentages or fractions in the printed output line.",
            "solution": "The problem requires the implementation of a permutation-based paired sign test to analyze matched tumor-normal DNA methylation data, followed by multiple testing correction using the Benjamini-Hochberg (BH) procedure. The solution is structured into three main parts: defining the statistical test for a single feature, defining the multiple testing correction procedure for a family of features, and applying these methods to the provided datasets.\n\n### 1. Permutation Paired Sign Test\n\nThe paired sign test is a non-parametric method to assess the consistency of the direction of differences within matched pairs. Its permutation version provides an exact p-value under the null hypothesis of exchangeability.\n\n#### 1.1. Test Formulation\n\nLet a dataset for a single CpG feature consist of $N$ matched pairs of beta values, $(x_i^{\\mathrm{normal}}, x_i^{\\mathrm{tumor}})$ for $i = 1, \\dots, N$.\n\nFirst, we compute the within-pair differences:\n$$\nd_i = x_i^{\\mathrm{tumor}} - x_i^{\\mathrm{normal}}\n$$\n\nThe null hypothesis, $H_0$, posits no systematic shift between tumor and normal values. Under $H_0$, the sign of any non-zero difference $d_i$ is a random outcome with equal probability of being positive or negative, i.e., $\\mathbb{P}(\\mathrm{sign}(d_i) = +1) = \\mathbb{P}(\\mathrm{sign}(d_i) = -1) = 1/2$. Differences where $d_i=0$ are uninformative regarding the direction of shift and are excluded. Due to floating-point representation, we treat any difference $d_i$ as zero if its absolute value is below a small tolerance $\\tau = 10^{-12}$:\n$$\nd_i \\text{ is considered zero if } \\lvert d_i \\rvert  \\tau\n$$\n\nLet $n$ be the number of pairs with non-zero differences. This is the effective sample size for the test. If $n=0$, all differences are zero, providing no evidence against $H_0$. In this case, the p-value is defined to be $1$.\n\nFor $n > 0$, we count the number of positive differences, denoted by $S$:\n$$\nS = \\sum_{i=1}^{N} \\mathbf{1}(d_i  \\tau)\n$$\nwhere $\\mathbf{1}(\\cdot)$ is the indicator function.\n\nUnder $H_0$, each of the $n$ non-zero differences has its sign determined by an independent Bernoulli trial with success probability $1/2$. Therefore, the total count of positive signs, $S$, follows a binomial distribution with parameters $n$ and $p=1/2$:\n$$\nS \\sim \\mathrm{Binomial}(n, 1/2)\n$$\n\n#### 1.2. P-value Calculation\n\nWe use a two-sided test to detect any systematic shift, whether positive or negative. The test statistic is defined based on the deviation of the observed count $S$ from its expected value under $H_0$, which is $n/2$. The statistic is $T = \\lvert S - n/2 \\rvert$.\n\nThe p-value is the probability of observing a deviation from the mean at least as extreme as the one observed. This corresponds to summing the probabilities of all outcomes $k \\in \\{0, \\dots, n\\}$ that are in the tails of the distribution, as defined by the observed deviation.\nThe two-sided p-value is given by:\n$$\np = \\mathbb{P}\\left(\\left|K - \\frac{n}{2}\\right| \\ge \\left|S - \\frac{n}{2}\\right|\\right) \\quad \\text{where } K \\sim \\mathrm{Binomial}(n, 1/2)\n$$\nUsing the probability mass function of the binomial distribution, $\\mathbb{P}(K=k) = \\binom{n}{k} (1/2)^k (1-1/2)^{n-k} = \\binom{n}{k} (1/2)^n$, the p-value can be computed as:\n$$\np = \\frac{1}{2^n} \\sum_{k=0}^{n} \\binom{n}{k} \\cdot \\mathbf{1}\\left(\\left|k - \\frac{n}{2}\\right| \\ge \\left|S - \\frac{n}{2}\\right|\\right)\n$$\nThis formula calculates the exact probability of obtaining a result as or more extreme than the observed result $S$, across all $2^n$ possible sign-flip combinations.\n\n### 2. Benjamini-Hochberg (BH) Procedure\n\nWhen performing multiple hypothesis tests simultaneously (e.g., one for each CpG feature), we must control for the increased risk of false positives. The Benjamini-Hochberg procedure controls the False Discovery Rate (FDR), which is the expected proportion of false rejections among all rejections.\n\nGiven a family of $m$ p-values, $\\{p_1, p_2, \\dots, p_m\\}$, and a target FDR level $q$ (here, $q=0.1$), the procedure is as follows:\n1.  Order the p-values from smallest to largest: $p_{(1)} \\le p_{(2)} \\le \\dots \\le p_{(m)}$.\n2.  Find the largest integer $k$ such that the $k$-th ordered p-value, $p_{(k)}$, satisfies the condition:\n    $$\n    p_{(k)} \\le \\frac{k}{m} q\n    $$\n3.  If such a $k$ exists, reject the null hypotheses corresponding to the p-values $p_{(1)}, p_{(2)}, \\dots, p_{(k)}$.\n4.  If no such $k$ exists, do not reject any of the null hypotheses.\n\nThis procedure is applied independently to each dataset's family of features.\n\n### 3. Application to Test Suite\n\n#### Dataset A (1 feature, 8 pairs)\n-   Normal: $\\,[0.65,\\,0.72,\\,0.48,\\,0.51,\\,0.40,\\,0.30,\\,0.55,\\,0.60]$\n-   Tumor:  $\\,[0.70,\\,0.80,\\,0.55,\\,0.58,\\,0.50,\\,0.45,\\,0.62,\\,0.58]$\n-   Differences $d_i$: $\\,[0.05, 0.08, 0.07, 0.07, 0.1, 0.15, 0.07, -0.02]$\n-   All $\\lvert d_i \\rvert \\ge 10^{-12}$, so the effective sample size is $n=8$.\n-   The number of positive differences is $S=7$.\n-   The deviation from the mean is $\\lvert S - n/2 \\rvert = \\lvert 7 - 8/2 \\rvert = 3$.\n-   We need to find the p-value for $K \\sim \\mathrm{Binomial}(8, 1/2)$, which is $\\mathbb{P}(\\lvert K - 4 \\rvert \\ge 3)$. This is equivalent to $\\mathbb{P}(K \\le 1 \\text{ or } K \\ge 7)$.\n-   $p = \\mathbb{P}(K=0) + \\mathbb{P}(K=1) + \\mathbb{P}(K=7) + \\mathbb{P}(K=8)$.\n-   $p = \\frac{1}{2^8} \\left[ \\binom{8}{0} + \\binom{8}{1} + \\binom{8}{7} + \\binom{8}{8} \\right] = \\frac{1+8+8+1}{256} = \\frac{18}{256} = 0.0703125$.\n-   **BH Procedure**: $m=1$, $q=0.1$. We reject if $p_{(1)} \\le (1/1)q$. Here, $p = 0.0703125 \\le 0.1$, so the null hypothesis is rejected. The decision is True.\n\n#### Dataset B (1 feature, 10 pairs)\n-   Normal: $\\,[0.40,\\,0.50,\\,0.60,\\,0.55,\\,0.20,\\,0.80,\\,0.35,\\,0.35,\\,0.90,\\,0.10]$\n-   Tumor:  $\\,[0.46,\\,0.48,\\,0.60,\\,0.59,\\,0.25,\\,0.80,\\,0.33,\\,0.30,\\,0.90,\\,0.13]$\n-   Differences $d_i$: $\\,[0.06, -0.02, 0, 0.04, 0.05, 0, -0.02, -0.05, 0, 0.03]$.\n-   Three pairs have $d_i=0$, so they are excluded. The effective sample size is $n=7$.\n-   The non-zero differences are $[0.06, -0.02, 0.04, 0.05, -0.02, -0.05, 0.03]$.\n-   The number of positive differences is $S=4$.\n-   The deviation from the mean is $\\lvert S - n/2 \\rvert = \\lvert 4 - 7/2 \\rvert = 0.5$.\n-   We need to find the p-value for $K \\sim \\mathrm{Binomial}(7, 1/2)$, which is $\\mathbb{P}(\\lvert K - 3.5 \\rvert \\ge 0.5)$. This is equivalent to $\\mathbb{P}(K \\le 3 \\text{ or } K \\ge 4)$. This inequality holds for all possible values of $K \\in \\{0, 1, \\dots, 7\\}$.\n-   Thus, the sum of probabilities is over the entire support of the distribution, and the p-value is $1$.\n-   **BH Procedure**: $m=1$, $q=0.1$. We reject if $p \\le 0.1$. Here, $p=1 \\not\\le 0.1$, so we fail to reject the null hypothesis. The decision is False.\n\n#### Dataset C (4 features, 6 pairs)\n-   This dataset has $m=4$ features, so we calculate four p-values and apply a single BH correction. For all features, the number of pairs is $N=6$.\n-   **Feature 1**: Diffs: $[0.05, 0.05, 0.05, 0.05, 0.05, 0.05]$. All non-zero, so $n=6$. $S=6$. Deviation is $\\lvert 6 - 3 \\rvert = 3$. The p-value is $\\mathbb{P}(\\lvert K-3 \\rvert \\ge 3)$ for $K \\sim \\mathrm{Binomial}(6, 1/2)$, so $\\mathbb{P}(K=0) + \\mathbb{P}(K=6) = (\\binom{6}{0} + \\binom{6}{6})/2^6 = (1+1)/64 = 2/64 = 0.03125$.\n-   **Feature 2**: Diffs: $[0.06, 0.06, 0.06, 0.06, 0.06, -0.02]$. All non-zero, so $n=6$. $S=5$. Deviation is $\\lvert 5 - 3 \\rvert = 2$. The p-value is $\\mathbb{P}(\\lvert K-3 \\rvert \\ge 2) \\implies \\mathbb{P}(K \\le 1 \\text{ or } K \\ge 5) = (\\binom{6}{0}+\\binom{6}{1}+\\binom{6}{5}+\\binom{6}{6})/64 = (1+6+6+1)/64 = 14/64 = 0.21875$.\n-   **Feature 3**: Diffs: $[0.05, -0.02, 0.05, -0.02, -0.02, -0.02]$. All non-zero, so $n=6$. $S=2$. Deviation is $\\lvert 2 - 3 \\rvert = 1$. The p-value is $\\mathbb{P}(\\lvert K-3 \\rvert \\ge 1) \\implies \\mathbb{P}(K \\le 2 \\text{ or } K \\ge 4) = (\\binom{6}{0}+\\binom{6}{1}+\\binom{6}{2}+\\binom{6}{4}+\\binom{6}{5}+\\binom{6}{6})/64 = (1+6+15+15+6+1)/64 = 44/64 = 0.6875$.\n-   **Feature 4**: Diffs: $[-0.02, -0.02, -0.02, -0.02, -0.02, -0.02]$. All non-zero, so $n=6$. $S=0$. Deviation is $\\lvert 0 - 3 \\rvert = 3$. This is the same deviation as Feature 1, so the p-value is also $0.03125$.\n-   P-value vector: $p = [0.03125, 0.21875, 0.6875, 0.03125]$.\n-   **BH Procedure**: $m=4$, $q=0.1$.\n    1.  Ordered p-values: $p_{(1)}=0.03125$ (Feat. 1 or 4), $p_{(2)}=0.03125$ (Feat. 4 or 1), $p_{(3)}=0.21875$ (Feat. 2), $p_{(4)}=0.6875$ (Feat. 3).\n    2.  BH thresholds $(k/m)q$:\n        -   $k=1: (1/4) \\times 0.1 = 0.025$\n        -   $k=2: (2/4) \\times 0.1 = 0.05$\n        -   $k=3: (3/4) \\times 0.1 = 0.075$\n        -   $k=4: (4/4) \\times 0.1 = 0.1$\n    3.  Find largest $k$ where $p_{(k)} \\le (k/m)q$:\n        -   $k=1: p_{(1)} = 0.03125 \\not\\le 0.025$.\n        -   $k=2: p_{(2)} = 0.03125 \\le 0.05$. This holds.\n        -   $k=3: p_{(3)} = 0.21875 \\not\\le 0.075$.\n        -   The largest $k$ satisfying the condition is $k=2$.\n    4.  Decision: Reject hypotheses for $p_{(1)}$ and $p_{(2)}$. These correspond to Feature 1 and Feature 4.\n-   The final decision vector, in original order, is [True, False, False, True].\n\nThis completes the manual validation of the expected results. The implementation will follow this logic.",
            "answer": "```python\nimport numpy as np\nfrom scipy.special import comb\nimport math\n\ndef format_item(item):\n    \"\"\"Formats a float or bool according to the problem's specifications.\"\"\"\n    if isinstance(item, bool):\n        return str(item)\n    if isinstance(item, (int, float)):\n        # Round to 8 decimal places as per the requirement\n        rounded_item = round(item, 8)\n        # If the rounded value is an integer, represent it as such\n        if rounded_item == int(rounded_item):\n            return str(int(rounded_item))\n        # Otherwise, format to 8 places and strip trailing zeros\n        return f'{rounded_item:.8f}'.rstrip('0')\n    return str(item)\n\ndef format_nested_list(lst):\n    \"\"\"Recursively builds the output string for a nested list.\"\"\"\n    if not isinstance(lst, list):\n         return format_item(lst)\n    \n    formatted_items = [format_nested_list(item) for item in lst]\n    return f\"[{','.join(formatted_items)}]\"\n\ndef permutation_sign_test(normal_vals, tumor_vals, tau=1e-12):\n    \"\"\"\n    Computes the two-sided p-value for the permutation paired sign test.\n    \"\"\"\n    normal_vals = np.asarray(normal_vals)\n    tumor_vals = np.asarray(tumor_vals)\n    differences = tumor_vals - normal_vals\n\n    # Filter out ties (differences close to zero)\n    non_zero_diffs = differences[np.abs(differences) = tau]\n    n = len(non_zero_diffs)\n\n    if n == 0:\n        return 1.0\n\n    # Count the number of positive differences\n    S = np.sum(non_zero_diffs  0)\n    \n    # Calculate the observed deviation from the mean under H0\n    obs_deviation = abs(S - n / 2)\n    \n    # Calculate the two-sided p-value by summing probabilities in the tails\n    # of the Binomial(n, 0.5) distribution.\n    p_value = 0.0\n    for k in range(n + 1):\n        if abs(k - n / 2) = obs_deviation:\n            p_value += comb(n, k, exact=True)\n            \n    p_value /= (2**n)\n    \n    return p_value\n\ndef benjamini_hochberg(p_values, q=0.1):\n    \"\"\"\n    Applies the Benjamini-Hochberg procedure to a list of p-values.\n    \"\"\"\n    p_values = np.asarray(p_values)\n    m = len(p_values)\n    \n    if m == 0:\n        return []\n\n    # Sort p-values and keep track of original indices\n    sorted_indices = np.argsort(p_values)\n    sorted_p_values = p_values[sorted_indices]\n    \n    # Calculate BH thresholds\n    ranks = np.arange(1, m + 1)\n    bh_thresholds = (ranks / m) * q\n    \n    # Find all p-values that are below their respective BH threshold\n    below_threshold = sorted_p_values = bh_thresholds\n    \n    # Find the largest k such that p_(k) = (k/m)q\n    significant_indices = np.where(below_threshold)[0]\n    if len(significant_indices) == 0:\n        max_k_idx = -1\n    else:\n        max_k_idx = np.max(significant_indices)\n\n    # All hypotheses up to the one at max_k_idx are rejected\n    rejections = np.zeros(m, dtype=bool)\n    if max_k_idx != -1:\n        rejection_indices = sorted_indices[:max_k_idx + 1]\n        rejections[rejection_indices] = True\n        \n    return rejections.tolist()\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis on all datasets and print the final result.\n    \"\"\"\n    # Define test cases\n    test_cases = {\n        \"A\": {\n            \"normal\": [[0.65, 0.72, 0.48, 0.51, 0.40, 0.30, 0.55, 0.60]],\n            \"tumor\":  [[0.70, 0.80, 0.55, 0.58, 0.50, 0.45, 0.62, 0.58]]\n        },\n        \"B\": {\n            \"normal\": [[0.40, 0.50, 0.60, 0.55, 0.20, 0.80, 0.35, 0.35, 0.90, 0.10]],\n            \"tumor\":  [[0.46, 0.48, 0.60, 0.59, 0.25, 0.80, 0.33, 0.30, 0.90, 0.13]]\n        },\n        \"C\": {\n            \"normal\": np.array([\n                [0.20, 0.40, 0.70, 0.60],\n                [0.30, 0.45, 0.50, 0.40],\n                [0.10, 0.35, 0.20, 0.30],\n                [0.25, 0.55, 0.60, 0.50],\n                [0.15, 0.50, 0.30, 0.70],\n                [0.40, 0.60, 0.80, 0.65]\n            ]).T.tolist(),\n            \"tumor\": np.array([\n                [0.25, 0.46, 0.75, 0.58],\n                [0.35, 0.51, 0.48, 0.38],\n                [0.15, 0.41, 0.25, 0.28],\n                [0.30, 0.61, 0.58, 0.48],\n                [0.20, 0.56, 0.28, 0.68],\n                [0.45, 0.58, 0.78, 0.63]\n            ]).T.tolist()\n        }\n    }\n\n    q_fdr = 0.1\n    all_results = []\n\n    for key in [\"A\", \"B\", \"C\"]:\n        case = test_cases[key]\n        p_vals = []\n        for i in range(len(case[\"normal\"])):\n            p = permutation_sign_test(case[\"normal\"][i], case[\"tumor\"][i])\n            p_vals.append(p)\n        \n        decisions = benjamini_hochberg(p_vals, q=q_fdr)\n        all_results.append([p_vals, decisions])\n\n    # Format the final output according to the specified format\n    formatted_results = [format_nested_list(res) for res in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Building on single-test execution, this practice shifts focus to the critical area of experimental design and power analysis within the context of RNA-sequencing. You will derive and implement a power calculation for detecting differential gene expression, based on the widely used negative binomial model . This hands-on problem demystifies power analysis by guiding you through an application of the Central Limit Theorem and the Delta Method, demonstrating how to assess the sensitivity of a study design while controlling the Family-Wise Error Rate (FWER).",
            "id": "4609496",
            "problem": "You are modeling differential gene expression in ribonucleic acid sequencing (RNA-seq) using a negative binomial sampling model with mean–variance relationship given by $\\mathrm{Var}(Y)=\\mu+\\phi \\mu^2$, where $Y$ is the read count for a gene in one sample, $\\mu$ is the mean count after library-size normalization, and $\\phi$ is the dispersion parameter. Consider a two-group comparison (treatment versus control) with $n_1$ independent replicates in group $1$ and $n_2$ independent replicates in group $2$. Let the baseline mean be $\\mu_1=\\mu$ in group $1$, and let the mean in group $2$ be $\\mu_2=\\mu \\cdot 2^{\\Delta}$, where $\\Delta$ is the log base $2$ fold-change. Assume independence across replicates and across genes, and that library-size normalization has been performed so that $\\mu$ is comparable across groups. You seek to compute the statistical power of a two-sided hypothesis test of no differential expression for a single gene while controlling the familywise error rate across $m$ independent genes using the Bonferroni correction at familywise level $\\alpha_{\\mathrm{FWER}}$.\n\nStarting from core statistical principles and definitions that are valid in this context, specifically: (i) the central limit theorem for averages of independent observations, (ii) the delta method for smooth transformations of asymptotically normal estimators, (iii) the Wald test for testing that a parameter equals zero using an asymptotically normal estimator and its standard error, and (iv) the Bonferroni correction to control the familywise error rate, derive an implementable procedure to approximate the power under the alternative when the true log base $2$ fold-change is $\\Delta$. Your derivation should be based only on the negative binomial mean–variance specification $\\mathrm{Var}(Y)=\\mu+\\phi \\mu^2$ and the definitions listed above, without assuming any ad hoc shortcut formulas not implied by those principles.\n\nYou must then implement this procedure in a program that, for each test case, computes the approximate power of the two-sided test at per-gene significance level $\\alpha_{\\mathrm{Bonf}}=\\alpha_{\\mathrm{FWER}}/m$ using a Wald statistic. The null hypothesis is that the log fold-change equals zero. Treat the fold-change on the natural logarithm scale as $\\beta=\\Delta \\cdot \\ln(2)$. The final answer for each test case must be a single real number in decimal form representing the power, rounded to $6$ decimal places.\n\nYour program must process the following test suite, where each test case is specified as a tuple $(\\mu,\\phi,\\Delta,n_1,n_2,m,\\alpha_{\\mathrm{FWER}})$:\n\n- Test case $1$: $(50,\\,0.1,\\,1,\\,3,\\,3,\\,20000,\\,0.05)$.\n- Test case $2$: $(5,\\,0.2,\\,1,\\,3,\\,3,\\,20000,\\,0.05)$.\n- Test case $3$: $(50,\\,0.1,\\,1,\\,6,\\,6,\\,20000,\\,0.05)$.\n- Test case $4$: $(20,\\,0.1,\\,0,\\,3,\\,3,\\,10,\\,0.1)$.\n- Test case $5$: $(100,\\,1.0,\\,1,\\,3,\\,3,\\,20000,\\,0.05)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the test suite, with each power rounded to $6$ decimal places (for example, $\\left[\\text{result}_1,\\text{result}_2,\\dots\\right]$). No other text should be printed. All angles, if any, should be in radians. All probabilities and significance levels must be provided and reported as decimals, not as percentages.",
            "solution": "The problem requires the derivation and implementation of a procedure to calculate the statistical power of a two-sided hypothesis test for differential gene expression in an RNA-seq experiment. The derivation must be based on first principles: the Central Limit Theorem (CLT), the Delta Method, the Wald test, and the Bonferroni correction.\n\nLet $Y_{ik}$ be the read count for a single gene in replicate $i$ of group $k$, where $k=1$ for the control group and $k=2$ for the treatment group. The number of replicates in each group are $n_1$ and $n_2$, respectively. The counts are assumed to follow a negative binomial distribution with mean $\\mathbb{E}[Y_{ik}] = \\mu_k$ and a specified mean-variance relationship $\\mathrm{Var}(Y_{ik}) = \\mu_k + \\phi \\mu_k^2$. The group means are related by $\\mu_1 = \\mu$ and $\\mu_2 = \\mu \\cdot 2^{\\Delta}$, where $\\Delta$ is the log base $2$ fold-change. We want to test the null hypothesis $H_0: \\Delta = 0$ against the alternative $H_A: \\Delta \\neq 0$.\n\nThe derivation proceeds as follows:\n\n**1. Asymptotic Distribution of Sample Means**\nThe sample mean for each group is the average of the read counts across replicates:\n$$ \\bar{Y}_1 = \\frac{1}{n_1} \\sum_{i=1}^{n_1} Y_{i1} \\quad \\text{and} \\quad \\bar{Y}_2 = \\frac{1}{n_2} \\sum_{i=1}^{n_2} Y_{i2} $$\nAccording to the Central Limit Theorem, for sufficiently large $n_1$ and $n_2$, the sample means are approximately normally distributed. Their means are $\\mathbb{E}[\\bar{Y}_k] = \\mu_k$ and their variances are $\\mathrm{Var}(\\bar{Y}_k) = \\frac{\\mathrm{Var}(Y_{ik})}{n_k} = \\frac{\\mu_k + \\phi \\mu_k^2}{n_k}$.\nThus, we have:\n$$ \\bar{Y}_1 \\stackrel{\\text{approx}}{\\sim} N\\left(\\mu_1, \\frac{\\mu_1 + \\phi \\mu_1^2}{n_1}\\right) $$\n$$ \\bar{Y}_2 \\stackrel{\\text{approx}}{\\sim} N\\left(\\mu_2, \\frac{\\mu_2 + \\phi \\mu_2^2}{n_2}\\right) $$\nSince replicates across groups are independent, $\\bar{Y}_1$ and $\\bar{Y}_2$ are independent random variables.\n\n**2. Asymptotic Distribution of the Log-Fold Change Estimator**\nThe parameter of interest is the natural log-fold change, $\\beta = \\ln(\\mu_2/\\mu_1) = \\ln(\\mu \\cdot 2^{\\Delta} / \\mu) = \\Delta \\ln(2)$. The null hypothesis is equivalent to $H_0: \\beta=0$. A natural estimator for $\\beta$ is obtained by substituting the sample means for the true means:\n$$ \\hat{\\beta} = \\ln(\\bar{Y}_2/\\bar{Y}_1) = \\ln(\\bar{Y}_2) - \\ln(\\bar{Y}_1) $$\nTo find the asymptotic distribution of $\\hat{\\beta}$, we apply the Delta Method. Let the function be $g(\\mu_1, \\mu_2) = \\ln(\\mu_2) - \\ln(\\mu_1)$. The variance of $\\hat{\\beta} = g(\\bar{Y}_1, \\bar{Y}_2)$ is approximated by:\n$$ \\mathrm{Var}(\\hat{\\beta}) \\approx \\left(\\frac{\\partial g}{\\partial \\mu_1}\\right)^2 \\mathrm{Var}(\\bar{Y}_1) + \\left(\\frac{\\partial g}{\\partial \\mu_2}\\right)^2 \\mathrm{Var}(\\bar{Y}_2) $$\nThe partial derivatives are $\\frac{\\partial g}{\\partial \\mu_1} = -1/\\mu_1$ and $\\frac{\\partial g}{\\partial \\mu_2} = 1/\\mu_2$. Substituting these and the variances of the sample means:\n$$ \\mathrm{Var}(\\hat{\\beta}) \\approx \\left(-\\frac{1}{\\mu_1}\\right)^2 \\left(\\frac{\\mu_1 + \\phi \\mu_1^2}{n_1}\\right) + \\left(\\frac{1}{\\mu_2}\\right)^2 \\left(\\frac{\\mu_2 + \\phi \\mu_2^2}{n_2}\\right) $$\n$$ \\sigma_{\\hat{\\beta}}^2 \\equiv \\mathrm{Var}(\\hat{\\beta}) \\approx \\frac{1+\\phi\\mu_1}{n_1\\mu_1} + \\frac{1+\\phi\\mu_2}{n_2\\mu_2} = \\left(\\frac{1}{n_1\\mu_1} + \\frac{\\phi}{n_1}\\right) + \\left(\\frac{1}{n_2\\mu_2} + \\frac{\\phi}{n_2}\\right) $$\nBy the Delta Method, $\\hat{\\beta}$ is asymptotically normally distributed with mean $\\beta$ and variance $\\sigma_{\\hat{\\beta}}^2$:\n$$ \\hat{\\beta} \\stackrel{\\text{approx}}{\\sim} N\\left(\\beta, \\sigma_{\\hat{\\beta}}^2\\right) $$\n\n**3. The Wald Test Statistic**\nThe Wald statistic for testing $H_0: \\beta = 0$ is constructed as $W = \\hat{\\beta} / \\mathrm{SE}(\\hat{\\beta})$, where $\\mathrm{SE}(\\hat{\\beta})$ is an estimate of the standard error of $\\hat{\\beta}$. For large samples, the estimated standard error converges to the true standard error, $\\mathrm{SE}(\\hat{\\beta}) \\approx \\sigma_{\\hat{\\beta}}$. Thus, the distribution of the test statistic under the alternative hypothesis (where $\\beta \\neq 0$) can be approximated as:\n$$ W = \\frac{\\hat{\\beta}}{\\mathrm{SE}(\\hat{\\beta})} \\approx \\frac{\\hat{\\beta}}{\\sigma_{\\hat{\\beta}}} \\sim N\\left(\\frac{\\beta}{\\sigma_{\\hat{\\beta}}}, 1\\right) $$\nUnder the null hypothesis ($\\beta=0$), $W \\sim N(0,1)$.\n\n**4. Power Calculation**\nWe are testing $m$ independent genes and controlling the familywise error rate (FWER) at level $\\alpha_{\\mathrm{FWER}}$ using the Bonferroni correction. The significance level for each individual gene test is therefore $\\alpha_{\\mathrm{Bonf}} = \\alpha_{\\mathrm{FWER}}/m$.\nFor a two-sided test, we reject $H_0$ if the absolute value of the Wald statistic exceeds the critical value from the standard normal distribution. The critical value $z_{\\alpha_{\\mathrm{Bonf}}/2}$ is defined such that $P(Z > z_{\\alpha_{\\mathrm{Bonf}}/2}) = \\alpha_{\\mathrm{Bonf}}/2$ for $Z \\sim N(0,1)$. This is equivalent to $z_{\\alpha_{\\mathrm{Bonf}}/2} = \\Phi^{-1}(1 - \\alpha_{\\mathrm{Bonf}}/2)$, where $\\Phi$ is the standard normal cumulative distribution function (CDF).\n\nThe power of the test is the probability of rejecting $H_0$ given that the alternative hypothesis is true:\n$$ \\text{Power} = P(|W| > z_{\\alpha_{\\mathrm{Bonf}}/2} \\mid H_A) = P(W > z_{\\alpha_{\\mathrm{Bonf}}/2} \\mid H_A) + P(W  -z_{\\alpha_{\\mathrm{Bonf}}/2} \\mid H_A) $$\nUnder $H_A$, the test statistic $W$ follows a normal distribution with mean $\\theta = \\beta / \\sigma_{\\hat{\\beta}}$ and variance $1$. Let $Z \\sim N(0,1)$, such that $W = Z + \\theta$. The power is:\n$$ \\text{Power} = P(Z + \\theta > z_{\\alpha_{\\mathrm{Bonf}}/2}) + P(Z + \\theta  -z_{\\alpha_{\\mathrm{Bonf}}/2}) $$\n$$ \\text{Power} = P(Z > z_{\\alpha_{\\mathrm{Bonf}}/2} - \\theta) + P(Z  -z_{\\alpha_{\\mathrm{Bonf}}/2} - \\theta) $$\nIn terms of the standard normal CDF $\\Phi$:\n$$ \\text{Power} = \\left(1 - \\Phi(z_{\\alpha_{\\mathrm{Bonf}}/2} - \\theta)\\right) + \\Phi(-z_{\\alpha_{\\mathrm{Bonf}}/2} - \\theta) $$\nThis provides the final formula for the power approximation.\n\n**Implementation Plan:**\nFor each test case $(\\mu, \\phi, \\Delta, n_1, n_2, m, \\alpha_{\\mathrm{FWER}})$:\n1.  Calculate $\\alpha_{\\mathrm{Bonf}} = \\alpha_{\\mathrm{FWER}} / m$.\n2.  Calculate the critical value $z_{\\alpha_{\\mathrm{Bonf}}/2} = \\Phi^{-1}(1 - \\alpha_{\\mathrm{Bonf}}/2)$.\n3.  Calculate the parameters under the alternative: $\\beta = \\Delta \\ln(2)$, $\\mu_1 = \\mu$, and $\\mu_2 = \\mu \\cdot 2^\\Delta$.\n4.  Calculate the variance $\\sigma_{\\hat{\\beta}}^2 = \\left(\\frac{1}{n_1\\mu_1} + \\frac{\\phi}{n_1}\\right) + \\left(\\frac{1}{n_2\\mu_2} + \\frac{\\phi}{n_2}\\right)$.\n5.  Calculate the non-centrality parameter $\\theta = \\beta / \\sqrt{\\sigma_{\\hat{\\beta}}^2}$.\n6.  Compute the power using the derived formula.\nIf $\\Delta=0$, then $\\beta=0$ and $\\theta=0$. The power formula simplifies to $\\alpha_{\\mathrm{Bonf}}$, which is the Type I error rate, as expected.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Derives and implements a procedure to calculate the statistical power\n    of a Wald test for differential gene expression analysis based on a\n    negative binomial model.\n    \"\"\"\n    # Test cases defined as tuples of (mu, phi, Delta, n1, n2, m, alpha_FWER)\n    test_cases = [\n        (50, 0.1, 1, 3, 3, 20000, 0.05),\n        (5, 0.2, 1, 3, 3, 20000, 0.05),\n        (50, 0.1, 1, 6, 6, 20000, 0.05),\n        (20, 0.1, 0, 3, 3, 10, 0.1),\n        (100, 1.0, 1, 3, 3, 20000, 0.05),\n    ]\n\n    results = []\n    for case in test_cases:\n        mu, phi, Delta, n1, n2, m, alpha_fwer = case\n\n        # Step 1: Calculate the per-gene significance level using Bonferroni correction.\n        # This is the alpha level for a single hypothesis test.\n        alpha_bonf = alpha_fwer / m\n\n        # Step 2: Find the critical value for the two-sided test.\n        # This is the z-score corresponding to the tail probability alpha_bonf / 2.\n        z_crit = norm.ppf(1 - alpha_bonf / 2)\n\n        # Step 3: Calculate parameters under the alternative hypothesis.\n        # beta is the natural log-fold change.\n        beta = Delta * np.log(2)\n        mu1 = mu\n        mu2 = mu * (2**Delta)\n\n        # Step 4: Calculate the variance of the log-fold change estimator (beta_hat).\n        # This is derived using the Delta method on the function ln(Y2_bar/Y1_bar).\n        # Var(beta_hat) = Var(ln(Y2_bar) - ln(Y1_bar))\n        #               ~ (1/mu1^2)*Var(Y1_bar) + (1/mu2^2)*Var(Y2_bar)\n        # Var(Yk_bar) = (mu_k + phi*mu_k^2) / n_k\n        # After simplification: Var(beta_hat) = (1+phi*mu1)/(n1*mu1) + (1+phi*mu2)/(n2*mu2)\n        var_beta_hat = (1 / (n1 * mu1) + phi / n1) + (1 / (n2 * mu2) + phi / n2)\n        se_beta_hat = np.sqrt(var_beta_hat)\n        \n        # Step 5: Calculate the non-centrality parameter (theta) of the Wald statistic distribution\n        # under the alternative hypothesis.\n        # The Wald statistic W = beta_hat / se_beta_hat is approx. N(beta/se_beta_hat, 1).\n        if se_beta_hat  0:\n            theta = beta / se_beta_hat\n        else: # Avoid division by zero, although not expected in these cases\n            theta = 0\n\n        # Step 6: Calculate the power of the test.\n        # Power = P(|W|  z_crit | H_A)\n        #       = P(W  z_crit) + P(W  -z_crit) where W ~ N(theta, 1)\n        #       = P(Z  z_crit - theta) + P(Z  -z_crit - theta) where Z ~ N(0, 1)\n        #       = (1 - cdf(z_crit - theta)) + cdf(-z_crit - theta)\n        power = 1 - norm.cdf(z_crit - theta) + norm.cdf(-z_crit - theta)\n        \n        # Append the formatted result.\n        results.append(f\"{power:.6f}\")\n\n    # Print the final output in the required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "This final practice delves into the theoretical underpinnings of the error control methods themselves, a vital topic for any researcher designing or interpreting high-throughput studies. You will compare the properties of the Holm and Hochberg multiple testing procedures, exploring why one is more powerful under independence . The core of the exercise is a hands-on calculation that reveals how certain p-value dependency structures, which can arise in real biological data, can cause a widely used procedure to fail, highlighting the critical importance of understanding a method's assumptions.",
            "id": "4609556",
            "problem": "In a differential expression study of $m=3$ microRNA biomarkers, you perform three hypothesis tests under a global null model. Let the $p$-values be denoted by $p_{1}, p_{2}, p_{3}$. You consider two multiple-testing procedures at nominal level $\\alpha$: the Holm step-down procedure and the Hochberg step-up procedure. Family-Wise Error Rate (FWER) is defined as the probability of at least one false rejection when all null hypotheses are true.\n\nTask A (independence baseline). Starting from the definitions of Holm’s and Hochberg’s procedures and the FWER, and using only well-tested foundational facts such as the Bonferroni inequality and the Simes inequality under independence, explain why, under independence of $(p_{1},p_{2},p_{3})$ with each marginal $p_{i} \\sim \\mathrm{Uniform}(0,1)$, both Holm and Hochberg control the FWER at level $\\alpha$, and why Hochberg is at least as powerful as Holm in this setting.\n\nTask B (dependence counterexample and explicit FWER calculation). Now suppose the $p$-values have the following dependent joint distribution under the global null:\n- With probability $1$, the multiset $\\{p_{1},p_{2},p_{3}\\}$ is obtained by drawing two independent values from $\\mathrm{Uniform}(0,\\frac{2}{3})$ and one independent value from $\\mathrm{Uniform}(\\frac{2}{3},1)$, and then assigning these three draws to $(p_{1},p_{2},p_{3})$ by a uniformly random permutation, independent of the draws.\n\nThis construction yields each $p_{i}$ marginally $\\mathrm{Uniform}(0,1)$ under the null while inducing a specific dependence structure.\n\n1. Verify that each marginal distribution is indeed $\\mathrm{Uniform}(0,1)$.\n2. For Hochberg’s step-up procedure at nominal level $\\alpha=\\frac{4}{5}$, compute the exact FWER under the above dependence. Provide your final result as an exact fraction. Do not round.",
            "solution": "The problem presents two tasks related to multiple hypothesis testing procedures, specifically the Holm step-down and Hochberg step-up methods.\n\n**Task A: Independence Baseline**\n\nThis task requires an explanation of why both Holm's and Hochberg's procedures control the Family-Wise Error Rate (FWER) at a nominal level $\\alpha$ when the $p$-values are independent and uniformly distributed on $(0,1)$, and why Hochberg's procedure is more powerful than Holm's. Let there be $m=3$ hypothesis tests with corresponding $p$-values $p_1, p_2, p_3$. The ordered $p$-values are denoted $p_{(1)} \\le p_{(2)} \\le p_{(3)}$. The global null hypothesis is that all individual null hypotheses are true, under which $p_i \\sim \\mathrm{Uniform}(0,1)$ for $i=1, 2, 3$.\n\nThe Holm-Bonferroni step-down procedure rejects hypotheses corresponding to $p_{(1)}, \\dots, p_{(k)}$ where $k$ is the largest index for which $p_{(i)} \\le \\frac{\\alpha}{m+1-i}$ for all $i=1, \\dots, k$. An equivalent, and simpler, statement is that it finds the smallest index $j$ such that $p_{(j)} > \\frac{\\alpha}{m+1-j}$ and rejects hypotheses corresponding to $p_{(1)}, \\dots, p_{(j-1)}$. If no such $j$ exists, all hypotheses are rejected.\n\nTo prove that Holm's procedure controls the FWER, we consider the event of making at least one false rejection. Under the global null, any rejection is a false rejection. A Type I error occurs if at least one hypothesis is rejected. This happens if and only if the most significant $p$-value, $p_{(1)}$, satisfies the condition $p_{(1)} \\le \\frac{\\alpha}{m}$. Therefore, the FWER is the probability of this event.\n$$\n\\mathrm{FWER} = P\\left(p_{(1)} \\le \\frac{\\alpha}{m}\\right) = P\\left(\\min_{i=1, \\dots, m} p_i \\le \\frac{\\alpha}{m}\\right)\n$$\nThis is the probability of a union of events: $P(\\cup_{i=1}^m \\{p_i \\le \\frac{\\alpha}{m}\\})$. By the Bonferroni inequality, the probability of a union of events is less than or equal to the sum of their individual probabilities:\n$$\nP\\left(\\bigcup_{i=1}^m \\left\\{p_i \\le \\frac{\\alpha}{m}\\right\\}\\right) \\le \\sum_{i=1}^m P\\left(p_i \\le \\frac{\\alpha}{m}\\right)\n$$\nSince each $p_i$ is uniformly distributed on $(0,1)$ under the null hypothesis, $P(p_i \\le x) = x$ for $x \\in [0,1]$. Thus,\n$$\n\\mathrm{FWER} \\le \\sum_{i=1}^m \\frac{\\alpha}{m} = m \\cdot \\frac{\\alpha}{m} = \\alpha\n$$\nThis demonstrates that the Holm procedure controls the FWER at level $\\alpha$. This proof relies only on the Bonferroni inequality and the marginal distribution of the $p$-values, so it holds regardless of their dependence structure.\n\nThe Hochberg step-up procedure rejects hypotheses corresponding to $p_{(1)}, \\dots, p_{(k)}$ where $k$ is the largest index for which $p_{(k)} \\le \\frac{\\alpha}{m+1-k}$. If no such $k$ exists, no hypotheses are rejected.\n\nThe proof that Hochberg's procedure controls the FWER under independence is more subtle and relies on the Simes inequality. The Simes test for a global null hypothesis rejects if $p_{(i)} \\le \\frac{i\\alpha}{m}$ for any $i \\in \\{1, \\dots, m\\}$. The Simes inequality states that for independent $p$-values from true null hypotheses, the probability of this event is less than or equal to $\\alpha$. A key result in multiple testing theory is that the Hochberg procedure is a closed testing procedure where the local test for any intersection hypothesis $H_I = \\cap_{i \\in I} H_i$ is the Simes test applied to the set of p-values $\\{p_i : i \\in I\\}$ at level $\\alpha$. Since the Simes test is a valid level-$\\alpha$ test under independence, the general theory of closed testing procedures guarantees that the Hochberg procedure strongly controls the FWER at level $\\alpha$.\n\nTo compare the power of the two procedures, let $R_H$ and $R_{HB}$ be the sets of indices of hypotheses rejected by the Holm and Hochberg procedures, respectively. Suppose a hypothesis $H_{(i)}$ is rejected by the Holm procedure. This implies that for all $j \\in \\{1, \\dots, i\\}$, we have $p_{(j)} \\le \\frac{\\alpha}{m+1-j}$. In particular, for $j=i$, we have $p_{(i)} \\le \\frac{\\alpha}{m+1-i}$. The Hochberg procedure's condition for rejecting at least $H_{(1)}, \\dots, H_{(i)}$ is met if we can find any index $k \\ge i$ such that $p_{(k)} \\le \\frac{\\alpha}{m+1-k}$. Since we already know $p_{(i)} \\le \\frac{\\alpha}{m+1-i}$, we can choose $k=i$, which satisfies the condition. Therefore, Hochberg will reject at least $H_{(1)}, \\dots, H_{(i)}$. This shows that any hypothesis rejected by Holm is also rejected by Hochberg, so $R_H \\subseteq R_{HB}$. Consequently, the Hochberg procedure is at least as powerful as the Holm procedure (and strictly more powerful if, for example, $p_{(1)} > \\frac{\\alpha}{m}$ but $p_{(2)} \\le \\frac{\\alpha}{m-1}$).\n\n**Task B: Dependence Counterexample and Explicit FWER Calculation**\n\n1. Verification of the marginal distribution:\nWe are given that the multiset $\\{p_1, p_2, p_3\\}$ is formed by two i.i.d. draws from $U_A \\sim \\mathrm{Uniform}(0, \\frac{2}{3})$ and one draw from $U_B \\sim \\mathrm{Uniform}(\\frac{2}{3}, 1)$, which are then randomly permuted. We must verify that the marginal distribution of any $p_i$ is $\\mathrm{Uniform}(0,1)$. By symmetry, we only need to check for $p_1$.\n\nWe use the law of total probability. For any $x \\in (0,1)$, the cumulative distribution function (CDF) of $p_1$ is:\n$$\nF_{p_1}(x) = P(p_1 \\le x) = P(p_1 \\le x | p_1 \\sim U_A)P(p_1 \\sim U_A) + P(p_1 \\le x | p_1 \\sim U_B)P(p_1 \\sim U_B)\n$$\nA specific draw is assigned to $p_1$ with probability $1/3$. Since there are two draws from $U_A$ and one from $U_B$, we have $P(p_1 \\sim U_A) = \\frac{2}{3}$ and $P(p_1 \\sim U_B) = \\frac{1}{3}$.\n\nThe CDF of $U_A$ is $F_A(x) = \\frac{x}{2/3} = \\frac{3x}{2}$ for $x \\in [0, \\frac{2}{3}]$ and $1$ for $x > \\frac{2}{3}$.\nThe CDF of $U_B$ is $F_B(x) = 0$ for $x \\le \\frac{2}{3}$ and $\\frac{x-2/3}{1-2/3} = 3x-2$ for $x \\in (\\frac{2}{3}, 1]$.\n\nCase 1: $x \\in (0, \\frac{2}{3}]$.\n$$\nF_{p_1}(x) = F_A(x) \\cdot \\frac{2}{3} + F_B(x) \\cdot \\frac{1}{3} = \\left(\\frac{3x}{2}\\right) \\cdot \\frac{2}{3} + (0) \\cdot \\frac{1}{3} = x\n$$\nCase 2: $x \\in (\\frac{2}{3}, 1)$.\n$$\nF_{p_1}(x) = F_A(x) \\cdot \\frac{2}{3} + F_B(x) \\cdot \\frac{1}{3} = (1) \\cdot \\frac{2}{3} + (3x-2) \\cdot \\frac{1}{3} = \\frac{2}{3} + x - \\frac{2}{3} = x\n$$\nCombining these cases, $F_{p_1}(x)=x$ for $x \\in (0,1)$, which is the CDF of a $\\mathrm{Uniform}(0,1)$ distribution. The verification is complete.\n\n2. FWER calculation for Hochberg's procedure:\nWe are given $m=3$ and $\\alpha = \\frac{4}{5}$. The Hochberg procedure rejects if any of the following conditions on the ordered $p$-values $p_{(1)} \\le p_{(2)} \\le p_{(3)}$ is met:\n- $p_{(1)} \\le \\frac{\\alpha}{3} = \\frac{4/5}{3} = \\frac{4}{15}$\n- $p_{(2)} \\le \\frac{\\alpha}{2} = \\frac{4/5}{2} = \\frac{2}{5}$\n- $p_{(3)} \\le \\frac{\\alpha}{1} = \\alpha = \\frac{4}{5}$\n\nThe FWER is the probability of the union of these three events. Let $V_A^{(1)}, V_A^{(2)}$ be the two i.i.d. draws from $\\mathrm{Uniform}(0, \\frac{2}{3})$ and $V_B$ be the draw from $\\mathrm{Uniform}(\\frac{2}{3}, 1)$. With probability $1$, we have $V_A^{(1)}  \\frac{2}{3}$, $V_A^{(2)}  \\frac{2}{3}$, and $V_B > \\frac{2}{3}$. Thus, when ordered, the three $p$-values will be such that $p_{(3)} = V_B$, and $\\{p_{(1)}, p_{(2)}\\}$ will be the ordered values of $\\{V_A^{(1)}, V_A^{(2)}\\}$. Let $V_{A,(1)} = \\min(V_A^{(1)}, V_A^{(2)})$ and $V_{A,(2)} = \\max(V_A^{(1)}, V_A^{(2)})$.\nSo, we have $p_{(1)} = V_{A,(1)}$, $p_{(2)} = V_{A,(2)}$, and $p_{(3)} = V_B$. The draws $V_A^{(1)}, V_A^{(2)}$ are independent of $V_B$, so events concerning $\\{p_{(1)}, p_{(2)}\\}$ are independent of events concerning $p_{(3)}$.\n\nThe rejection event $E$ is $E = \\{V_{A,(1)} \\le \\frac{4}{15}\\} \\cup \\{V_{A,(2)} \\le \\frac{2}{5}\\} \\cup \\{V_B \\le \\frac{4}{5}\\}$.\nLet $E_A = \\{V_{A,(1)} \\le \\frac{4}{15}\\} \\cup \\{V_{A,(2)} \\le \\frac{2}{5}\\}$ and $E_B = \\{V_B \\le \\frac{4}{5}\\}$.\nDue to independence, the FWER is $P(E) = P(E_A \\cup E_B) = P(E_A) + P(E_B) - P(E_A)P(E_B)$.\n\nFirst, we calculate $P(E_B)$:\n$V_B \\sim \\mathrm{Uniform}(\\frac{2}{3}, 1)$.\n$$\nP(E_B) = P\\left(V_B \\le \\frac{4}{5}\\right) = \\frac{4/5 - 2/3}{1 - 2/3} = \\frac{12/15 - 10/15}{1/3} = \\frac{2/15}{1/3} = \\frac{6}{15} = \\frac{2}{5}\n$$\n\nNext, we calculate $P(E_A)$. Let $X, Y$ be two i.i.d. variables from $\\mathrm{Uniform}(0, c)$ where $c=\\frac{2}{3}$. Then $V_{A,(1)}=\\min(X,Y)$ and $V_{A,(2)}=\\max(X,Y)$. Let $t_1 = \\frac{4}{15}$ and $t_2 = \\frac{2}{5}$. Note that $t_1  t_2  c$.\nWe calculate the probability of the complement, $E_A^c = \\{V_{A,(1)} > t_1\\} \\cap \\{V_{A,(2)} > t_2\\}$.\nThis is equivalent to $\\{\\min(X,Y) > t_1\\} \\cap \\{\\max(X,Y) > t_2\\}$.\nThe condition $\\min(X,Y) > t_1$ means $X > t_1$ and $Y > t_1$. The condition $\\max(X,Y) > t_2$ means $X > t_2$ or $Y > t_2$.\nSo, $E_A^c = \\{X > t_1, Y > t_1 \\text{ and } (X > t_2 \\text{ or } Y > t_2)\\}$.\nSince $t_1  t_2$, if $X>t_2$ or $Y>t_2$, then it is automatically true that $X>t_1$ or $Y>t_1$. The region for $E_A^c$ is the set of points $(X,Y)$ such that both are greater than $t_1$, and at least one is greater than $t_2$.\nThis can be calculated as $P(\\{X > t_1, Y > t_1\\}) - P(\\{t_1  X \\le t_2, t_1  Y \\le t_2\\})$.\nFor a variable $Z \\sim \\mathrm{Uniform}(0,c)$, $P(Z>z) = \\frac{c-z}{c}$ and $P(z_1  Z \\le z_2) = \\frac{z_2-z_1}{c}$.\n$$\nP(X > t_1) = \\frac{2/3 - 4/15}{2/3} = \\frac{10/15 - 4/15}{10/15} = \\frac{6/15}{10/15} = \\frac{6}{10} = \\frac{3}{5}\n$$\nSo, $P(X > t_1, Y > t_1) = P(X>t_1)P(Y>t_1) = \\left(\\frac{3}{5}\\right)^2 = \\frac{9}{25}$.\n$$\nP(t_1  X \\le t_2) = \\frac{2/5 - 4/15}{2/3} = \\frac{6/15 - 4/15}{10/15} = \\frac{2/15}{10/15} = \\frac{2}{10} = \\frac{1}{5}\n$$\nSo, $P(t_1  X \\le t_2, t_1  Y \\le t_2) = \\left(\\frac{1}{5}\\right)^2 = \\frac{1}{25}$.\nThus, $P(E_A^c) = \\frac{9}{25} - \\frac{1}{25} = \\frac{8}{25}$.\nThis gives $P(E_A) = 1 - P(E_A^c) = 1 - \\frac{8}{25} = \\frac{17}{25}$.\n\nFinally, we combine the results to find the FWER:\n$$\n\\mathrm{FWER} = P(E) = P(E_A) + P(E_B) - P(E_A)P(E_B) = \\frac{17}{25} + \\frac{2}{5} - \\left(\\frac{17}{25}\\right)\\left(\\frac{2}{5}\\right)\n$$\n$$\n\\mathrm{FWER} = \\frac{17}{25} + \\frac{10}{25} - \\frac{34}{125} = \\frac{27}{25} - \\frac{34}{125}\n$$\n$$\n\\mathrm{FWER} = \\frac{27 \\times 5}{125} - \\frac{34}{125} = \\frac{135}{125} - \\frac{34}{125} = \\frac{101}{125}\n$$\nThe nominal FWER level was $\\alpha=\\frac{4}{5}=\\frac{100}{125}$. The calculated FWER is $\\frac{101}{125}$, which is greater than $\\alpha$. This demonstrates that Hochberg's procedure fails to control the FWER under this specific dependence structure.",
            "answer": "$$\\boxed{\\frac{101}{125}}$$"
        }
    ]
}