{
    "hands_on_practices": [
        {
            "introduction": "本练习将指导您动手实现主成分分析 (PCA) 背后的核心思想。您将学习如何使用奇异值分解 (SVD) 来量化数据总方差中由最重要的几个维度所解释的比例 。通过这个实践，您将能够深入理解 SVD 如何揭示高维数据的内在结构，这对于有效的数据降维至关重要。",
            "id": "4578458",
            "problem": "您将获得用于生物信息学和医疗数据分析的面向列的基因表达矩阵，这些矩阵用于总结信使核糖核酸 (mRNA) 在各个样本中的丰度。计算任务是从线性代数和统计学的基础定义出发，量化通过截断奇异值分解 (SVD) 获得的前 $k$ 个奇异值捕获了多少总样本方差。您的实现必须严格遵循从第一性原理推导出的算法：计算列中心化数据矩阵，获取其 SVD，并确定由前 $k$ 个奇异值解释的方差比例。除了标准定义外，不要假设任何预先存在的公式。\n\n使用的基础理论：\n- 列中心化数据矩阵的样本协方差矩阵定义：对于一个数据矩阵 $X \\in \\mathbb{R}^{n \\times p}$，其中 $n$ 是样本数，$p$ 是基因数，通过从每列中减去其样本均值来定义列中心化矩阵 $X_c$。样本协方差矩阵为 $S = \\frac{1}{n - 1} X_c^\\top X_c$。\n- 实数矩阵的奇异值分解 (SVD)：对于 $X_c \\in \\mathbb{R}^{n \\times p}$，其 SVD 为 $X_c = U \\Sigma V^\\top$，其中 $U \\in \\mathbb{R}^{n \\times r}$ 和 $V \\in \\mathbb{R}^{p \\times r}$ 具有标准正交列，$\\Sigma = \\operatorname{diag}(\\sigma_1, \\dots, \\sigma_r)$，其中奇异值 $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_r \\ge 0$，且 $r = \\operatorname{rank}(X_c)$。\n- 弗罗贝尼乌斯范数恒等式：$\\|X_c\\|_F^2 = \\sum_{i=1}^{r} \\sigma_i^2$。\n\n您的程序必须：\n- 对于每个测试用例，对输入矩阵 $X$ 的列进行中心化以获得 $X_c$（减去每列的均值；不进行缩放）。\n- 计算 $X_c$ 的 SVD 以获得奇异值 $\\sigma_1, \\dots, \\sigma_r$。\n- 将级别为 $k$ 的截断 SVD 定义为保留前 $k$ 个奇异值和相应的奇异向量。为稳健地处理数值秩，设有效秩 $r$ 为满足 $\\sigma_i > \\tau$ 的奇异值的数量，其中 $\\tau$ 是基于维度和机器精度的容差。如果 $k > r$，则使用 $k_{\\text{eff}} = r$。如果 $k = 0$，则解释的比例必须为 $0$。\n- 仅使用上述基础恒等式，计算由前 $k_{\\text{eff}}$ 个奇异值解释的方差比例，以小数形式（而非百分比）表示，即前 $k_{\\text{eff}}$ 个奇异值的平方和与所有 $r$ 个奇异值的平方和之比。\n\n角度单位不适用。物理单位不适用。所有输出必须是小数。\n\n测试套件：\n对于下面的每个测试用例，$X$ 是原始表达矩阵，$k$ 是截断级别。在计算 SVD 之前，您必须对 $X$ 的列进行中心化。矩阵如下：\n\n- 情况 1（正常路径，高矩阵，中等 $k$）：\n$$\nX^{(1)} =\n\\begin{bmatrix}\n2  0  1 \\\\\n0  1  3 \\\\\n4  2  5 \\\\\n6  3  6\n\\end{bmatrix}, \\quad k^{(1)} = 2.\n$$\n\n- 情况 2（边界 $k = 0$）：\n$$\nX^{(2)} =\n\\begin{bmatrix}\n10  0  -2  3 \\\\\n5  1  0  0 \\\\\n0  -1  2  -3\n\\end{bmatrix}, \\quad k^{(2)} = 0.\n$$\n\n- 情况 3（宽矩阵，冗余列和常数列；$k$ 超过秩）：\n$$\nX^{(3)} =\n\\begin{bmatrix}\n1  1  5  0  -1 \\\\\n2  2  5  1  0 \\\\\n3  3  5  2  1\n\\end{bmatrix}, \\quad k^{(3)} = 3.\n$$\n\n- 情况 4（高矩阵，带有一个零列，非平凡秩）：\n$$\nX^{(4)} =\n\\begin{bmatrix}\n0  1  0  -1 \\\\\n0  2  1  0 \\\\\n0  3  2  1 \\\\\n0  4  3  2 \\\\\n0  5  4  3\n\\end{bmatrix}, \\quad k^{(4)} = 2.\n$$\n\n- 情况 5（列之间存在精确的线性相关性；$k$ 等于秩）：\n$$\nX^{(5)} =\n\\begin{bmatrix}\n1  2  5 \\\\\n2  4  10 \\\\\n3  6  15 \\\\\n4  8  20\n\\end{bmatrix}, \\quad k^{(5)} = 2.\n$$\n\n数值秩容差：\n使用一个数值稳定的容差 $\\tau$ 来决定哪些奇异值被视为非零。选择 $\\tau = \\max(n, p) \\cdot \\sigma_{\\max} \\cdot \\varepsilon$，其中 $n$ 和 $p$ 是 $X_c$ 的维度，$\\sigma_{\\max}$ 是 $X_c$ 的最大奇异值，$\\varepsilon$ 是 64 位浮点运算的机器ε。\n\n最终输出格式：\n您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个比例都四舍五入到 8 位小数，作为 $[0, 1]$ 区间内的小数。例如，一个有效的输出看起来像 $[r_a,r_b,r_c,r_d,r_e]$，其中每个 $r_\\cdot$ 都是一个小数。\n\n您的程序必须是自包含的，使用指定的测试套件，并产生上述单行输出。",
            "solution": "该问题要求我们计算列中心化数据矩阵的前 $k$ 个奇异值所捕获的总样本方差的比例。这个量在主成分分析 (PCA) 中是核心概念，用于评估降维质量。解决方案将从提供的基础定义推导得出。\n\n设给定的原始数据矩阵为 $X \\in \\mathbb{R}^{n \\times p}$，其中有 $n$ 个样本（行）和 $p$ 个特征或基因（列）。\n\n首先，我们通过从每列的元素中减去该列的均值来对数据进行中心化。设 $\\bar{x}_j = \\frac{1}{n} \\sum_{i=1}^{n} X_{ij}$ 为第 $j$ 列的均值。列中心化矩阵 $X_c$ 的元素为 $(X_c)_{ij} = X_{ij} - \\bar{x}_j$。\n\n总样本方差 $\\text{Var}_{\\text{total}}$ 是每个特征样本方差的总和。这等价于样本协方差矩阵 $S$ 的迹。问题将样本协方差矩阵定义为 $S = \\frac{1}{n - 1} X_c^\\top X_c$。\n利用迹算子的线性性质，总方差为：\n$$\n\\text{Var}_{\\text{total}} = \\operatorname{tr}(S) = \\operatorname{tr}\\left(\\frac{1}{n-1} X_c^\\top X_c\\right) = \\frac{1}{n - 1} \\operatorname{tr}(X_c^\\top X_c)\n$$\n迹的一个基本性质是 $\\operatorname{tr}(A^\\top A)$ 等于 $A$ 的弗罗贝尼乌斯范数的平方，即 $\\|A\\|_F^2 = \\sum_{i,j} A_{ij}^2$。因此，我们有：\n$$\n\\text{Var}_{\\text{total}} = \\frac{1}{n - 1} \\|X_c\\|_F^2\n$$\n问题提供了一个关键的恒等式，将弗罗贝尼乌斯范数与 $X_c$ 的奇异值联系起来。设 $X_c$ 的奇异值分解 (SVD) 为 $X_c = U \\Sigma V^\\top$，其中奇异值 $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_r > 0$ 是 $\\Sigma$ 的对角线元素，且 $r = \\operatorname{rank}(X_c)$。该恒等式为：\n$$\n\\|X_c\\|_F^2 = \\sum_{i=1}^{r} \\sigma_i^2\n$$\n将此代入我们的总方差表达式中，得到：\n$$\n\\text{Var}_{\\text{total}} = \\frac{1}{n - 1} \\sum_{i=1}^{r} \\sigma_i^2\n$$\n数据的主成分是矩阵 $Z = X_c V$ 的列。第 $i$ 个主成分的方差与第 $i$ 个奇异值的平方直接相关：$\\text{Var}(Z_i) = \\frac{\\sigma_i^2}{n - 1}$。前 $k$ 个主成分解释的总方差是它们各自方差的总和：\n$$\n\\text{Var}_k = \\sum_{i=1}^{k} \\text{Var}(Z_i) = \\sum_{i=1}^{k} \\frac{\\sigma_i^2}{n - 1} = \\frac{1}{n - 1} \\sum_{i=1}^{k} \\sigma_i^2\n$$\n由前 $k$ 个主成分解释的方差比例是它们捕获的方差 $\\text{Var}_k$ 与总方差 $\\text{Var}_{\\text{total}}$ 的比值。\n$$\nP_k = \\frac{\\text{Var}_k}{\\text{Var}_{\\text{total}}} = \\frac{\\frac{1}{n-1} \\sum_{i=1}^{k} \\sigma_i^2}{\\frac{1}{n-1} \\sum_{i=1}^{r} \\sigma_i^2} = \\frac{\\sum_{i=1}^{k} \\sigma_i^2}{\\sum_{i=1}^{r} \\sigma_i^2}\n$$\n因子 $\\frac{1}{n-1}$ 被约掉，表明方差比例可以直接从奇异值的平方计算得出。这个推导出的公式为我们的算法提供了基础。\n\n计算算法如下：\n1.  对于每个输入矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 和整数 $k$，计算列中心化矩阵 $X_c$。\n2.  使用 SVD 计算 $X_c$ 的奇异值 $\\sigma_i$。\n3.  为处理数值精度问题，确定有效秩 $r$。如果奇异值 $\\sigma_i > \\tau$，则认为其非零，其中容差 $\\tau$ 定义为 $\\tau = \\max(n, p) \\cdot \\sigma_{\\max} \\cdot \\varepsilon$。这里，$\\sigma_{\\max}$ 是最大奇异值，$\\varepsilon$ 是所用浮点精度的机器ε。有效秩 $r$ 是超过 $\\tau$ 的奇异值的数量。\n4.  需要考虑的主成分数量 $k_{\\text{eff}}$ 根据秩进行调整。如果 $k=0$，比例为 $0$。否则，$k_{\\text{eff}} = \\min(k, r)$。\n5.  最终比例使用推导出的公式计算：\n    $$\n    P_{k_{\\text{eff}}} = \\frac{\\sum_{i=1}^{k_{\\text{eff}}} \\sigma_i^2}{\\sum_{i=1}^{r} \\sigma_i^2}\n    $$\n    其中，求和是在步骤 3 中被视为非零的奇异值的平方上进行的。如果分母为零（即中心化矩阵的方差为零），则比例为 $0$。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the proportion of variance explained by the first k singular values\n    for a list of test cases according to the derived formula.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (happy path, tall matrix, moderate k)\n        (np.array([\n            [2, 0, 1],\n            [0, 1, 3],\n            [4, 2, 5],\n            [6, 3, 6]\n        ], dtype=float), 2),\n        \n        # Case 2 (boundary k = 0)\n        (np.array([\n            [10, 0, -2, 3],\n            [5, 1, 0, 0],\n            [0, -1, 2, -3]\n        ], dtype=float), 0),\n\n        # Case 3 (wide matrix, redundant and constant columns; k exceeds rank)\n        (np.array([\n            [1, 1, 5, 0, -1],\n            [2, 2, 5, 1, 0],\n            [3, 3, 5, 2, 1]\n        ], dtype=float), 3),\n\n        # Case 4 (tall matrix with a zero column, nontrivial rank)\n        (np.array([\n            [0, 1, 0, -1],\n            [0, 2, 1, 0],\n            [0, 3, 2, 1],\n            [0, 4, 3, 2],\n            [0, 5, 4, 3]\n        ], dtype=float), 2),\n\n        # Case 5 (exact linear dependence among columns; k equals rank)\n        (np.array([\n            [1, 2, 5],\n            [2, 4, 10],\n            [3, 6, 15],\n            [4, 8, 20]\n        ], dtype=float), 2)\n    ]\n\n    results = []\n    \n    for X, k in test_cases:\n        n, p = X.shape\n        \n        # Handle case with no rows, which implies no variance.\n        if n == 0:\n            results.append(0.0)\n            continue\n            \n        # Step 1: Center the columns of the input matrix X.\n        X_c = X - X.mean(axis=0)\n\n        # Step 2: Compute the SVD of the centered matrix X_c. Only singular values are needed.\n        try:\n            sigma = np.linalg.svd(X_c, compute_uv=False)\n        except np.linalg.LinAlgError:\n            # In case of a failure, assume zero variance.\n            results.append(0.0)\n            continue\n\n        # Handle matrices that result in no singular values (e.g., zero columns/rows).\n        if sigma.size == 0:\n            results.append(0.0)\n            continue\n\n        # Step 3: Determine the numerical rank r using the specified tolerance.\n        eps = np.finfo(X.dtype).eps\n        sigma_max = sigma[0]\n        tolerance = max(n, p) * sigma_max * eps\n        \n        # Filter for singular values greater than the tolerance.\n        sigma_effective = sigma[sigma > tolerance]\n        r = len(sigma_effective)\n        \n        # Step 4: Handle k and calculate the proportion of variance.\n        # If k is 0, the proportion must be 0.\n        if k == 0:\n            proportion = 0.0\n        else:\n            # Effective k cannot exceed the rank.\n            k_eff = min(k, r)\n            \n            # Use the squared effective singular values for variance calculations.\n            sigma_sq_effective = sigma_effective**2\n            \n            # The denominator is the sum of all effective squared singular values.\n            total_variance_proxy = np.sum(sigma_sq_effective)\n\n            if total_variance_proxy > 1e-15: # Check for near-zero total variance.\n                # If total variance is zero, no variance can be explained.\n                proportion = 0.0\n            else:\n                # The numerator is the sum of the first k_eff squared singular values.\n                explained_variance_proxy = np.sum(sigma_sq_effective[:k_eff])\n                proportion = explained_variance_proxy / total_variance_proxy\n                \n        results.append(round(proportion, 8))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在处理高维数据时，识别并移除冗余特征是提高模型性能和可解释性的关键一步。本练习将指导您实现一种基于数值秩的贪心算法，从一组可能存在线性依赖关系的临床预测因子中，系统地筛选出一个最小的线性无关子集 。这个实践将让您掌握如何利用 SVD 来稳健地评估列向量组的线性相关性，这是特征选择中的一个核心技能。",
            "id": "4578482",
            "problem": "给定多个实值设计矩阵，它们代表生物信息学和医疗数据分析中使用的冗余临床预测因子。任务是通过移除数值线性代数意义上线性相关的列，为每个矩阵提取一个最小预测因子生成集。您的程序必须依赖于第一性原理和一个考虑稳定性的数值秩判据来确定线性无关性。\n\n定义与基本依据：\n- $\\mathbb{R}^m$ 中的一组列向量 $\\{\\mathbf{a}_1,\\dots,\\mathbf{a}_k\\}$ 是线性无关的，如果 $x_1 \\mathbf{a}_1 + \\dots + x_k \\mathbf{a}_k = \\mathbf{0}$ 的唯一解是 $x_1 = \\dots = x_k = 0$。等价地，矩阵 $A = [\\mathbf{a}_1 \\ \\dots \\ \\mathbf{a}_k]$ 的秩为 $k$。\n- 矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 的秩是其列空间的维度，等于 $A$ 的严格正奇异值的数量。\n- 在数值计算中，精确的零被一个数值阈值所取代。我们将数值秩 $\\operatorname{rank}_{\\tau}(A)$ 定义为严格大于容差 $\\tau$ 的奇异值 $\\sigma_i(A)$ 的数量，其中容差是相对于谱范数定义的，即 $\\tau = 10^{-9} \\cdot \\|A\\|_2$。此处，$\\|A\\|_2$ 是 $A$ 的最大奇异值。\n\n要求的算法行为（基于原理）：\n- 从左到右（即，按从索引 $0$ 到 $n-1$ 的给定顺序）处理每个给定矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 的列。\n- 设 $\\tau = 10^{-9} \\cdot \\|A\\|_2$ 为该矩阵的固定阈值。初始化一个空索引集 $S$ 和一个当前秩 $r = 0$。对于每个按升序排列的列索引 $j$：\n  - 构建子矩阵 $A_S$，其列由 $S$ 中的索引指定（如果 $S$ 为空，则 $A_S$ 没有列）。计算 $r_{\\text{old}} = \\operatorname{rank}_{\\tau}(A_S)$。\n  - 通过将列 $j$ 附加到 $A_S$ 来构建增广子矩阵 $A_{S \\cup \\{j\\}}$。计算 $r_{\\text{new}} = \\operatorname{rank}_{\\tau}(A_{S \\cup \\{j\\}})$。\n  - 如果 $r_{\\text{new}} > r_{\\text{old}}$，则更新 $S \\leftarrow S \\cup \\{j\\}$ 和 $r \\leftarrow r_{\\text{new}}$；否则，跳过列 $j$，因为它在数值上与已选集合线性相关。\n- 继续此过程直到所有列都被处理完毕。最终得到的索引集 $S$（根据构造过程已按升序排列）是最小生成集，其基数等于数值秩 $\\operatorname{rank}_{\\tau}(A)$，并且它张成了 $A$ 的数值列空间。\n\n测试套件：\n对于下面的每个矩阵，按要求计算构成最小生成集 $S$ 的从零开始的列索引的升序列表。不涉及物理单位或角度。\n\n- 情况 1（冗余高矩阵）：\n  $$\n  A_1 =\n  \\begin{bmatrix}\n  1  0  1  0  1 \\\\\n  0  1  1  0  0 \\\\\n  2  1  3  0  2 \\\\\n  0  0  0  1  2 \\\\\n  1  1  2  1  3 \\\\\n  3  1  4  1  5\n  \\end{bmatrix}\n  $$\n  其中第 3 列等于第 1 列加第 2 列，第 5 列等于第 1 列加 2 倍的第 4 列。\n\n- 情况 2（零列和重复列）：\n  $$\n  A_2 =\n  \\begin{bmatrix}\n  1  0  1  0 \\\\\n  2  0  2  1 \\\\\n  3  0  3  0 \\\\\n  4  0  4  1\n  \\end{bmatrix}\n  $$\n  其中第 2 列是零向量，第 3 列是第 1 列的副本。\n\n- 情况 3（在容差下的近零列）：\n  $$\n  A_3 =\n  \\begin{bmatrix}\n  1  10^{-12}  0 \\\\\n  0  0          1 \\\\\n  0  0          0 \\\\\n  0  0          0 \\\\\n  0  0          0\n  \\end{bmatrix}\n  $$\n\n- 情况 4（包含组合的宽矩阵）：\n  $$\n  A_4 =\n  \\begin{bmatrix}\n  1  0  0  1  0  1 \\\\\n  0  1  0  1  1  0 \\\\\n  0  0  1  0  1  1\n  \\end{bmatrix}\n  $$\n  其中第 4、5、6 列是前三列的线性组合。\n\n- 情况 5（病态但在容差下无关）：\n  $$\n  A_5 =\n  \\begin{bmatrix}\n  1  1 \\\\\n  0  10^{-8}\n  \\end{bmatrix}\n  $$\n\n答案规格：\n- 对于每个矩阵 $A_i$，输出所选的从零开始的列索引的升序列表 $S_i$。\n- 将所有五个结果汇总到一行中，格式为无空格、方括号括起来的逗号分隔列表的列表，例如 $[[a_1,a_2],[b_1,\\dots],\\dots]$。\n- 您的程序必须只生成一行包含此汇总输出的内容。\n\n您的程序应生成单行输出，其中包含用方括号括起来的、无空格的、逗号分隔的列表的列表形式的结果（例如，$[[0,1,3],[0,3],[0,2],[0,1,2],[0,1]]$）。",
            "solution": "该问题要求从给定的设计矩阵 $A$ 中识别出一个最小预测因子生成集。这是数据分析中的一项基本任务，通常称为特征选择或降维，其目标是消除冗余信息。所规定的方法是一种贪心算法，它基于矩阵秩的数值鲁棒定义，迭代地构建一个线性无关的列集合。\n\n其核心原理基于数值线性代数的概念。一组向量 $\\{\\mathbf{a}_1, \\dots, \\mathbf{a}_k\\}$ 是线性无关的，如果由这些向量构成的矩阵 $A = [\\mathbf{a}_1 \\ \\dots \\ \\mathbf{a}_k]$ 的秩为 $k$。在浮点运算中，计算误差使得简单地检查奇异值是否为零变得不可靠。因此，我们使用数值秩的概念，即 $\\operatorname{rank}_{\\tau}(A)$，它被定义为严格大于容差 $\\tau$ 的奇异值 $\\sigma_i(A)$ 的数量。问题指定了一个相对容差：$\\tau = 10^{-9} \\cdot \\|A\\|_2$，其中 $\\|A\\|_2$ 是矩阵的谱范数，等同于其最大奇异值 $\\sigma_{\\max}(A)$。如果一个列的加入能够增加由已选列构成的矩阵的数值秩，则认为该列贡献了新信息。\n\n算法流程如下：\n1.  对于给定的矩阵 $A \\in \\mathbb{R}^{m \\times n}$，我们首先通过找到 $A$ 的最大奇异值来计算其谱范数 $\\|A\\|_2$。然后将数值容差固定为 $\\tau = 10^{-9} \\cdot \\|A\\|_2$。\n2.  我们初始化一个空索引集 $S$（用于存储所选列的索引）和一个变量 $r=0$ 来表示所选子矩阵的当前秩。\n3.  我们从左到右遍历 $A$ 的列，索引为 $j = 0, 1, \\dots, n-1$。\n4.  对于每一列 $\\mathbf{a}_j$，我们通过将其附加到已选列（由 $S$ 索引）来形成一个测试矩阵。设这个增广矩阵为 $A_{S \\cup \\{j\\}}$。\n5.  我们计算这个增广矩阵的数值秩，$r_{\\text{new}} = \\operatorname{rank}_{\\tau}(A_{S \\cup \\{j\\}})$。\n6.  如果 $r_{\\text{new}} > r$，这表示列 $\\mathbf{a}_j$ 相对于容差 $\\tau$ 与集合 $S$ 中的列是线性无关的。在这种情况下，我们通过添加 $j$ 来更新已选索引集，即 $S \\leftarrow S \\cup \\{j\\}$，并更新当前秩，$r \\leftarrow r_{\\text{new}}$。\n7.  如果 $r_{\\text{new}} \\le r$，则列 $\\mathbf{a}_j$ 在数值上与已在 $S$ 中的列线性相关，我们丢弃它。\n8.  遍历完所有列后，最终的集合 $S$ 包含了 $A$ 的列空间的一个最小生成集的从零开始的索引。\n\n让我们用情况 1 中的矩阵来说明这个过程：\n$$\nA_1 =\n\\begin{bmatrix}\n1  0  1  0  1 \\\\\n0  1  1  0  0 \\\\\n2  1  3  0  2 \\\\\n0  0  0  1  2 \\\\\n1  1  2  1  3 \\\\\n3  1  4  1  5\n\\end{bmatrix}\n$$\n首先，我们计算 $A_1$ 的奇异值，约等于 $\\{8.33, 2.50, 1.00, 0, 0\\}$。谱范数为 $\\|A_1\\|_2 \\approx 8.33$。容差为 $\\tau = 10^{-9} \\cdot 8.33 \\approx 8.33 \\times 10^{-9}$。我们初始化 $S = []$ 和 $r = 0$。\n\n- **对于 $j=0$**：我们测试列 $\\mathbf{a}_0$。矩阵 $[\\mathbf{a}_0]$ 的秩为 $1$。由于 $r_{\\text{new}}=1 > r=0$，我们将 $0$ 添加到我们的集合中。$S \\leftarrow [0]$，$r \\leftarrow 1$。\n- **对于 $j=1$**：我们测试列 $\\mathbf{a}_1$。矩阵 $[\\mathbf{a}_0 \\ \\mathbf{a}_1]$ 的秩为 $2$，因为 $\\mathbf{a}_0$ 和 $\\mathbf{a}_1$ 是线性无关的。由于 $r_{\\text{new}}=2 > r=1$，我们将 $1$ 添加到我们的集合中。$S \\leftarrow [0, 1]$，$r \\leftarrow 2$。\n- **对于 $j=2$**：我们测试列 $\\mathbf{a}_2$。已知 $\\mathbf{a}_2 = \\mathbf{a}_0 + \\mathbf{a}_1$。因此，$\\mathbf{a}_2$ 位于 $\\{\\mathbf{a}_0, \\mathbf{a}_1\\}$ 的张成空间中。矩阵 $[\\mathbf{a}_0 \\ \\mathbf{a}_1 \\ \\mathbf{a}_2]$ 的秩为 $2$。由于 $r_{\\text{new}}=2 \\ngtr r=2$，我们不将 $2$ 添加到我们的集合中。$S$ 保持为 $[0, 1]$。\n- **对于 $j=3$**：我们测试列 $\\mathbf{a}_3$。可以证明矩阵 $[\\mathbf{a}_0 \\ \\mathbf{a}_1 \\ \\mathbf{a}_3]$ 的秩为 $3$。由于 $r_{\\text{new}}=3 > r=2$，我们将 $3$ 添加到我们的集合中。$S \\leftarrow [0, 1, 3]$，$r \\leftarrow 3$。\n- **对于 $j=4$**：我们测试列 $\\mathbf{a}_4$。已知 $\\mathbf{a}_4 = \\mathbf{a}_0 + 2\\mathbf{a}_3$。因此，$\\mathbf{a}_4$ 位于 $\\{\\mathbf{a}_0, \\mathbf{a}_3\\}$ 的张成空间中，而这个空间是 $\\{\\mathbf{a}_0, \\mathbf{a}_1, \\mathbf{a}_3\\}$ 张成空间的子空间。矩阵 $[\\mathbf{a}_0 \\ \\mathbf{a}_1 \\ \\mathbf{a}_3 \\ \\mathbf{a}_4]$ 的秩为 $3$。由于 $r_{\\text{new}}=3 \\ngtr r=3$，我们不将 $4$ 添加到我们的集合中。\n\n处理完所有列后，$A_1$ 的最终最小生成集索引为 $S_1 = [0, 1, 3]$。将此相同过程应用于所有提供的测试矩阵。该实现使用 `numpy.linalg.svd` 来获取数值秩计算所需的奇异值。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef find_minimal_spanning_set(A: np.ndarray) -> list[int]:\n    \"\"\"\n    Extracts a minimal spanning set of column indices from matrix A.\n\n    The method iteratively builds a set of linearly independent columns based\n    on a numerically robust rank criterion. A column is added if its inclusion\n    increases the numerical rank of the matrix formed by the selected columns.\n    \"\"\"\n    m, n = A.shape\n    if n == 0:\n        return []\n\n    # Calculate the spectral norm of the full matrix A to determine the tolerance.\n    # The spectral norm is the largest singular value.\n    s_full = np.linalg.svd(A, compute_uv=False)\n    norm_A = s_full[0] if len(s_full) > 0 else 0.0\n    \n    # Define the tolerance for numerical rank calculation.\n    tau = 1e-9 * norm_A\n\n    def get_numerical_rank(M: np.ndarray, tolerance: float) -> int:\n        \"\"\"\n        Computes the numerical rank of matrix M based on a given tolerance.\n        The rank is the number of singular values greater than the tolerance.\n        \"\"\"\n        if M.shape[1] == 0:\n            return 0\n        s_sub = np.linalg.svd(M, compute_uv=False)\n        return np.sum(s_sub > tolerance)\n\n    selected_indices = []\n    current_rank = 0\n\n    # Iterate through each column index from left to right.\n    for j in range(n):\n        # Form a temporary matrix by augmenting the current set with the new column.\n        potential_indices = selected_indices + [j]\n        A_augmented = A[:, potential_indices]\n        \n        # Calculate the numerical rank of the augmented matrix.\n        new_rank = get_numerical_rank(A_augmented, tau)\n        \n        # If the rank increases, the new column is linearly independent.\n        if new_rank > current_rank:\n            selected_indices.append(j)\n            current_rank = new_rank\n            \n    return selected_indices\n\ndef solve():\n    \"\"\"\n    Defines the test cases, runs the algorithm, and prints the formatted result.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    A1 = np.array([\n        [1., 0., 1., 0., 1.],\n        [0., 1., 1., 0., 0.],\n        [2., 1., 3., 0., 2.],\n        [0., 0., 0., 1., 2.],\n        [1., 1., 2., 1., 3.],\n        [3., 1., 4., 1., 5.]\n    ], dtype=np.float64)\n\n    A2 = np.array([\n        [1., 0., 1., 0.],\n        [2., 0., 2., 1.],\n        [3., 0., 3., 0.],\n        [4., 0., 4., 1.]\n    ], dtype=np.float64)\n    \n    A3 = np.array([\n        [1., 1e-12, 0.],\n        [0., 0.,    1.],\n        [0., 0.,    0.],\n        [0., 0.,    0.],\n        [0., 0.,    0.]\n    ], dtype=np.float64)\n\n    A4 = np.array([\n        [1., 0., 0., 1., 0., 1.],\n        [0., 1., 0., 1., 1., 0.],\n        [0., 0., 1., 0., 1., 1.]\n    ], dtype=np.float64)\n\n    A5 = np.array([\n        [1., 1.],\n        [0., 1e-8]\n    ], dtype=np.float64)\n\n    test_cases = [A1, A2, A3, A4, A5]\n\n    results = []\n    for A in test_cases:\n        result = find_minimal_spanning_set(A)\n        results.append(result)\n\n    # The final print statement must match the specified format exactly:\n    # a bracketed, comma-separated list of lists with no spaces.\n    # The str() function on a list of lists includes spaces, e.g., '[[0, 1], [2]]'.\n    # We remove these spaces to match the required format '[[0,1],[2]]'.\n    print(str(results).replace(' ', ''))\n\nsolve()\n```"
        },
        {
            "introduction": "拟合线性模型是数据分析的一项基本任务，但教科书中的“正规方程”法在处理真实世界数据时常常因特征共线性而变得数值不稳定甚至失效。本练习将引导您利用 SVD 来计算稳健的摩尔-彭罗斯伪逆，从而为求解最小二乘问题提供一个普适的解决方案 。这个实践将加深您对 SVD 在构建稳定、可靠的预测模型中关键作用的理解。",
            "id": "4578463",
            "problem": "考虑一个线性模型，其中给定一个设计矩阵 $X \\in \\mathbb{R}^{m \\times p}$ 和一个结果向量 $y \\in \\mathbb{R}^{m}$。在生物信息学和医学数据分析中，当建模基因表达特征与表型测量之间的关系时，就会出现此类矩阵。从线性代数的基本定义出发，使用奇异值分解（SVD）构造 Moore–Penrose 伪逆 $X^{+}$ 并计算最小二乘解 $X^{+} y$。当 $X^{\\top} X$ 可逆时，还需求解唯一的正规方程解。设计您的程序以实现以下功能：\n\n- 使用 SVD 从基本原理推导 $X^{+}$，并采用一个数值上合理的阈值来判断哪些奇异值应被视为非零值。\n- 根据 $X$ 的列秩判断 $X^{\\top} X$ 是否可逆。\n- 如果 $X^{\\top} X$ 可逆，则计算正规方程解，并计算其与 $X^{+} y$ 之间差值的欧几里得范数。\n- 在所有情况下，计算残差 $X (X^{+} y) - y$ 的欧几里得范数。\n\n仅使用经过充分检验的理论：矩阵分解的定义、正交矩阵的性质以及线性系统的可解性条件。不要使用任何未经证实的捷径。\n\n您的程序必须实现并评估以下参数值的测试套件。每个 $X$ 和 $y$ 都已明确指定：\n\n1. 理想情况（高矩阵，满列秩）：\n   $$X_1 = \\begin{bmatrix}\n   2 & -1 & 3 \\\\\n   0 & 4 & 5 \\\\\n   1 & 2 & -2 \\\\\n   3 & -5 & 1 \\\\\n   4 & 0 & -1\n   \\end{bmatrix}, \\quad\n   y_1 = \\begin{bmatrix}\n   1 \\\\ 0 \\\\ -1 \\\\ 2 \\\\ 3\n   \\end{bmatrix}.$$\n\n2. 病态但可逆的 $X^{\\top} X$（近似共线性）：\n   $$X_2 = \\begin{bmatrix}\n   1 & 2.0001 & 0 \\\\\n   2 & 4.0002 & 1 \\\\\n   3 & 6.0003 & 0 \\\\\n   4 & 8.0004 & 1 \\\\\n   5 & 10.0005 & 0\n   \\end{bmatrix}, \\quad\n   y_2 = \\begin{bmatrix}\n   1 \\\\ 1 \\\\ 2 \\\\ 3 \\\\ 5\n   \\end{bmatrix}.$$\n\n3. 秩亏（正规方程不适用）：\n   $$X_3 = \\begin{bmatrix}\n   1 & 0 & 0 \\\\\n   0 & 1 & 1 \\\\\n   1 & 0 & 0 \\\\\n   0 & 1 & 1\n   \\end{bmatrix}, \\quad\n   y_3 = \\begin{bmatrix}\n   1 \\\\ 2 \\\\ 3 \\\\ 4\n   \\end{bmatrix}.$$\n\n4. 方形且可逆：\n   $$X_4 = \\begin{bmatrix}\n   3 & 0 & 1 \\\\\n   2 & -1 & 0 \\\\\n   1 & 2 & 4\n   \\end{bmatrix}, \\quad\n   y_4 = \\begin{bmatrix}\n   0 \\\\ 1 \\\\ 2\n   \\end{bmatrix}.$$\n\n为保证数值鲁棒性，基于 SVD 的伪逆应使用一个阈值 $\\tau = \\max(m, p) \\cdot \\varepsilon \\cdot \\sigma_{\\max}$，其中 $\\varepsilon$ 是双精度浮点数的机器精度，$\\sigma_{\\max}$ 是 $X$ 的最大奇异值。严格大于 $\\tau$ 的奇异值将被求倒数；其余的则被视为零。\n\n将每个测试用例的最终输出定义为一个包含三个条目的列表：\n- 一个布尔值，指示正规方程计算是否适用（即 $X^{\\top} X$ 是否可逆）。\n- 一个浮点数，等于基于 SVD 的解与正规方程解之差的欧几里得范数（如果适用），若不适用则为浮点数 $-1.0$。\n- 一个浮点数，等于残差 $X (X^{+} y) - y$ 的欧几里得范数。\n\n您的程序应生成单行输出，其中包含所有结果，格式为一个由各测试用例列表组成的逗号分隔列表，并用方括号括起来。例如，输出格式必须为：\n$$\\texttt{[[b\\_1,d\\_1,r\\_1],[b\\_2,d\\_2,r\\_2],[b\\_3,d\\_3,r\\_3],[b\\_4,d\\_4,r\\_4]]}.$$\n不涉及任何物理单位或角度单位；所有量均为无量纲实数。",
            "solution": "对所给问题的分析分两个阶段进行：首先，对问题陈述进行验证；其次，基于线性代数的基本原理详细推导求解方法。\n\n### 问题验证\n\n**1. 提取已知条件：**\n- **模型**：一个由设计矩阵 $X \\in \\mathbb{R}^{m \\times p}$ 和结果向量 $y \\in \\mathbb{R}^{m}$ 定义的线性系统。\n- **目标**：计算最小二乘解 $\\hat{\\beta}$，使 $\\|X\\beta - y\\|_2$ 最小化。\n- **方法1（SVD 伪逆）**：计算 $\\hat{\\beta}_{SVD} = X^{+}y$，其中 Moore-Penrose 伪逆 $X^{+}$ 是通过 $X$ 的奇异值分解（SVD）构造的。\n- **方法2（正规方程）**：当且仅当 $X^{\\top}X$ 可逆时，计算 $\\hat{\\beta}_{NE} = (X^{\\top}X)^{-1}X^{\\top}y$。\n- **SVD 的数值阈值**：如果奇异值 $\\sigma_i > \\tau$，则将其视为非零，其中 $\\tau = \\max(m, p) \\cdot \\varepsilon \\cdot \\sigma_{\\max}$。此处，$\\varepsilon$ 是双精度浮点数的机器精度，$\\sigma_{\\max}$ 是 $X$ 的最大奇异值。\n- **可逆性判据**：如果使用阈值 $\\tau$ 数值确定出的 $X$ 的列秩等于 $p$，则矩阵 $X^{\\top}X$ 被视为可逆。\n- **每个测试用例的所需输出**：一个包含三个值的列表：\n    1. 一个布尔值，指示正规方程方法是否适用（即 $X^{\\top}X$ 是否可逆）。\n    2. SVD 解与正规方程解之差的欧几里得范数 $\\|\\hat{\\beta}_{SVD} - \\hat{\\beta}_{NE}\\|_2$。如果正规方程方法不适用，此值为 $-1.0$。\n    3. SVD 解的残差向量的欧几里得范数 $\\|X\\hat{\\beta}_{SVD} - y\\|_2$。\n- **测试数据**：提供了四个特定的 $(X, y)$ 对用于评估。\n\n**2. 验证结论：**\n该问题是**有效的**。\n- 它**具有科学依据**，植根于线性代数和数值分析的基本原理，特别是关于线性最小二乘问题的求解。\n- 它是**良构的**，所有必要的数据、定义和数值标准都已明确提供，以便计算出唯一且有意义的解。\n- 它是**客观的**，以精确的数学术语陈述，没有歧义或主观论断。\n- 问题背景（生物信息学和医学数据分析）是恰当的，因为线性模型是这些领域的基石。所有指定的条件和数据在数学上和计算上都是合理的。\n\n### 解法推导\n\n该问题要求对求解线性最小二乘问题的两种方法进行比较分析，这在数据建模中至关重要。目标是找到一个参数向量 $\\beta \\in \\mathbb{R}^p$，它通过设计矩阵 $X \\in \\mathbb{R}^{m \\times p}$ 的线性变换，能够最好地解释观测到的结果 $y \\in \\mathbb{R}^m$。“最好”的定义是最小化误差平方和，即最小化残差向量的欧几里得范数平方。\n\n**最小二乘问题**\n目标是找到向量 $\\hat{\\beta}$，以解决以下最小化问题：\n$$ \\hat{\\beta} = \\arg\\min_{\\beta \\in \\mathbb{R}^p} \\| y - X\\beta \\|_2^2 $$\n该范数的平方可以表示为一个二次型：\n$$ L(\\beta) = (y - X\\beta)^{\\top}(y - X\\beta) = y^{\\top}y - 2\\beta^{\\top}X^{\\top}y + \\beta^{\\top}X^{\\top}X\\beta $$\n这是关于 $\\beta$ 的一个凸函数，其最小值可以通过将其关于 $\\beta$ 的梯度设置为零向量来找到。\n$$ \\nabla_{\\beta} L(\\beta) = -2X^{\\top}y + 2X^{\\top}X\\beta = 0 $$\n这就得到了著名的**正规方程**：\n$$ X^{\\top}X\\beta = X^{\\top}y $$\n\n**方法1：正规方程解**\n矩阵 $X^{\\top}X$ 是一个大小为 $p \\times p$ 的方阵。如果该矩阵可逆，则存在唯一的 $\\beta$ 解，由下式给出：\n$$ \\hat{\\beta}_{NE} = (X^{\\top}X)^{-1}X^{\\top}y $$\n线性代数中的一个核心定理指出，$X^{\\top}X$ 可逆当且仅当矩阵 $X$ 的列线性无关。这等价于说 $X$ 具有满列秩，即 $\\text{rank}(X) = p$。当这些条件不满足时（例如，当列向量共线或系统是欠定的），$X^{\\top}X$ 是奇异的，该方法会失效。此外，即使 $X^{\\top}X$ 理论上可逆，如果 $X$ 的列近似共线，它也可能是病态的。在这种情况下，显式计算逆矩阵会放大数值误差。\n\n**方法2：SVD 和 Moore-Penrose 伪逆解**\n奇异值分解（SVD）为分析线性系统提供了一个强大且数值鲁棒的框架。任何矩阵 $X \\in \\mathbb{R}^{m \\times p}$ 都可以分解为：\n$$ X = U \\Sigma V^{\\top} $$\n其中：\n- $U$ 是一个 $m \\times m$ 的正交矩阵，其列（$u_i$）是左奇异向量。\n- $V$ 是一个 $p \\times p$ 的正交矩阵，其列（$v_i$）是右奇异向量。\n- $\\Sigma$ 是一个 $m \\times p$ 的矩形对角矩阵，包含按降序排列的非负奇异值 $\\sigma_i$。非零奇异值的数量等于 $X$ 的秩。\n\n使用 SVD， $X$ 的 Moore-Penrose 伪逆 $X^{+}$ 被唯一地定义为：\n$$ X^{+} = V \\Sigma^{+} U^{\\top} $$\n其中 $\\Sigma^{+}$ 是一个由 $\\Sigma$ 导出的 $p \\times m$ 矩阵。它的构造方法是先取 $\\Sigma$ 的转置，然后对每个非零奇异值取倒数。在有限精度算术中，我们必须使用一个阈值 $\\tau = \\max(m, p) \\cdot \\varepsilon \\cdot \\sigma_{\\max}$ 来判断哪些奇异值在数值上是非零的。因此，$\\Sigma^{+}$ 对角线上的元素（表示为 $\\sigma_i^{+}$）为：\n$$ \\sigma_i^{+} = \\begin{cases} 1/\\sigma_i & \\text{if } \\sigma_i > \\tau \\\\ 0 & \\text{if } \\sigma_i \\le \\tau \\end{cases} $$\n矩阵的秩在数值上被估计为大于 $\\tau$ 的奇异值的数量。正规方程的适用条件 $\\text{rank}(X) = p$ 就是使用这个数值秩来检验的。\n\n基于 SVD 的最小二乘解由下式给出：\n$$ \\hat{\\beta}_{SVD} = X^{+}y = (V \\Sigma^{+} U^{\\top})y $$\n该解总是存在的。它为最小二乘问题提供了唯一的最小范数解，这意味着在所有使 $\\|X\\beta - y\\|_2$ 最小化的向量 $\\beta$ 中，$\\hat{\\beta}_{SVD}$ 是欧几里得范数 $\\|\\beta\\|_2$ 最小的那一个。\n\n**算法步骤**\n对于每个给定的 $(X, y)$ 对，执行以下步骤：\n1.  从 $X$ 获取维度 $m$ 和 $p$。\n2.  计算 $X$ 的 SVD，得到 $U$、奇异值向量 $s$ 和 $V^{\\top}$。为提高效率，使用经济型 SVD。\n3.  确定数值阈值 $\\tau = \\max(m, p) \\cdot \\varepsilon \\cdot \\sigma_{\\max}$，其中 $\\sigma_{\\max} = s[0]$。\n4.  通过计算大于 $\\tau$ 的奇异值 $s_i$ 的数量来确定数值秩 $r_{eff}$。\n5.  如果 $r_{eff} = p$，则将布尔值 `is_normal_eq_applicable` 设置为 `True`，否则设置为 `False`。\n6.  计算 SVD 解 $\\hat{\\beta}_{SVD}$。这可以通过 $\\hat{\\beta}_{SVD} = V(\\Sigma^{+}(U^{\\top}y))$ 高效完成，其中运算是逐向量进行的，无需显式构造矩阵 $\\Sigma^{+}$。\n7.  如果 `is_normal_eq_applicable` 为 `True`：\n    a. 计算正规方程解 $\\hat{\\beta}_{NE} = (X^{\\top}X)^{-1}X^{\\top}y$。\n    b. 计算 `difference_norm` 作为欧几里得范数 $\\|\\hat{\\beta}_{SVD} - \\hat{\\beta}_{NE}\\|_2$。\n8.  如果 `is_normal_eq_applicable` 为 `False`，则将 `difference_norm` 设置为 $-1.0$。\n9.  在所有情况下，计算残差向量的范数，`residual_norm` = $\\|X\\hat{\\beta}_{SVD} - y\\|_2$。\n10. 存储结果三元组 (`is_normal_eq_applicable`, `difference_norm`, `residual_norm`)。\n\n此过程将应用于所有四个测试用例，以生成最终输出。",
            "answer": "```python\nimport numpy as np\n\ndef solve_least_squares_case(X, y):\n    \"\"\"\n    Solves a linear least-squares problem using SVD and Normal Equations,\n    and returns the specified comparison metrics.\n\n    Args:\n        X (np.ndarray): The m x p design matrix.\n        y (np.ndarray): The m-dimensional outcome vector.\n\n    Returns:\n        list: A list containing [is_normal_eq_applicable, difference_norm, residual_norm].\n    \"\"\"\n    m, p = X.shape\n\n    # 1. Compute SVD and determine numerical rank\n    try:\n        U, s, Vt = np.linalg.svd(X, full_matrices=False)\n    except np.linalg.LinAlgError:\n        # Handle cases where SVD might fail, though unlikely for real matrices\n        return [False, -1.0, np.linalg.norm(y)]\n\n    # Get machine epsilon for double precision\n    eps = np.finfo(np.float64).eps\n    \n    # Set the threshold for singular values\n    sigma_max = s[0] if s.size > 0 else 0\n    tau = max(m, p) * eps * sigma_max\n\n    # Determine numerical rank\n    rank = np.sum(s > tau)\n    is_normal_eq_applicable = (rank == p)\n\n    # 2. Compute the SVD-based least-squares solution beta_svd = X_plus @ y\n    # X_plus = V @ np.diag(s_inv) @ U.T\n    # beta_svd = V @ np.diag(s_inv) @ U.T @ y\n    s_inv = np.zeros_like(s)\n    s_inv[s > tau] = 1.0 / s[s > tau]\n    \n    # Efficient computation of beta_svd\n    uty = U.T @ y\n    beta_svd = Vt.T @ (s_inv * uty)\n\n    # 3. Compute normal equation solution if applicable\n    difference_norm = -1.0\n    if is_normal_eq_applicable:\n        try:\n            XTX = X.T @ X\n            XTX_inv = np.linalg.inv(XTX)\n            XTy = X.T @ y\n            beta_ne = XTX_inv @ XTy\n            difference_norm = np.linalg.norm(beta_svd - beta_ne)\n        except np.linalg.LinAlgError:\n            # If XTX is singular despite rank check (extreme ill-conditioning),\n            # consider normal equations as not applicable in practice.\n            is_normal_eq_applicable = False\n            difference_norm = -1.0\n            # This logic branch correction ensures that if np.linalg.inv fails,\n            # we correctly report the normal equation method as inapplicable.\n            # Find a way to modify the result being built.\n            # The cleanest way is to just proceed with the `else` block logic.\n            pass # proceed to the default value\n\n    # In case of LinAlgError inside the if block, we need to ensure the values are correct.\n    # The boolean has to be re-set to False in the final list for consistency.\n    if difference_norm == -1.0:\n        is_normal_eq_applicable = False\n\n\n    # 4. Compute the residual norm for the SVD solution\n    residual_vec = X @ beta_svd - y\n    residual_norm = np.linalg.norm(residual_vec)\n    \n    return [is_normal_eq_applicable, difference_norm, residual_norm]\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    X1 = np.array([\n        [2, -1, 3],\n        [0, 4, 5],\n        [1, 2, -2],\n        [3, -5, 1],\n        [4, 0, -1]\n    ], dtype=np.float64)\n    y1 = np.array([1, 0, -1, 2, 3], dtype=np.float64)\n\n    X2 = np.array([\n        [1, 2.0001, 0],\n        [2, 4.0002, 1],\n        [3, 6.0003, 0],\n        [4, 8.0004, 1],\n        [5, 10.0005, 0]\n    ], dtype=np.float64)\n    y2 = np.array([1, 1, 2, 3, 5], dtype=np.float64)\n\n    X3 = np.array([\n        [1, 0, 0],\n        [0, 1, 1],\n        [1, 0, 0],\n        [0, 1, 1]\n    ], dtype=np.float64)\n    y3 = np.array([1, 2, 3, 4], dtype=np.float64)\n\n    X4 = np.array([\n        [3, 0, 1],\n        [2, -1, 0],\n        [1, 2, 4]\n    ], dtype=np.float64)\n    y4 = np.array([0, 1, 2], dtype=np.float64)\n\n    test_cases = [\n        (X1, y1),\n        (X2, y2),\n        (X3, y3),\n        (X4, y4),\n    ]\n\n    results = []\n    for X, y in test_cases:\n        result = solve_least_squares_case(X, y)\n        results.append(result)\n\n    # Format the output string as required.\n    # The default str() for a list adds spaces, e.g., '[True, 1.23, 4.56]'.\n    # The template `','.join(map(str, results))` joins these string representations with a comma.\n    # Enclosing this in brackets gives a string that looks like a list of lists.\n    # This precisely follows the structure provided in the skeleton code.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}