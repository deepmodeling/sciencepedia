{
    "hands_on_practices": [
        {
            "introduction": "A crucial first step in analyzing high-dimensional biological data is to understand its intrinsic structure and dimensionality. The Singular Value Decomposition (SVD) is the mathematical engine behind Principal Component Analysis (PCA), providing a way to identify the directions of greatest variance in the data. This practice guides you through deriving from first principles how to quantify the proportion of total variance captured by the leading singular components, a fundamental skill for assessing the potential for dimensionality reduction .",
            "id": "4578458",
            "problem": "You are given column-oriented gene expression matrices that are used in bioinformatics and medical data analytics to summarize messenger ribonucleic acid (mRNA) abundance across samples. The computational task is to quantify how much of the total sample-wise variance is captured by the first $k$ singular values obtained from the truncated Singular Value Decomposition (SVD), starting from foundational definitions in linear algebra and statistics. Your implementation must strictly follow an algorithm derived from first principles: compute the column-centered data matrix, obtain its SVD, and determine the proportion of variance explained by the leading $k$ singular values. Do not assume any pre-existing formulas beyond standard definitions.\n\nFoundational base to use:\n- The sample covariance matrix definition for a column-centered data matrix: for a data matrix $X \\in \\mathbb{R}^{n \\times p}$, where $n$ is the number of samples and $p$ is the number of genes, define the column-centered matrix $X_c$ by subtracting from each column its sample mean. The sample covariance matrix is $S = \\frac{1}{n - 1} X_c^\\top X_c$.\n- The Singular Value Decomposition (SVD) of a real matrix: for $X_c \\in \\mathbb{R}^{n \\times p}$, the SVD is $X_c = U \\Sigma V^\\top$ where $U \\in \\mathbb{R}^{n \\times r}$ and $V \\in \\mathbb{R}^{p \\times r}$ have orthonormal columns, $\\Sigma = \\operatorname{diag}(\\sigma_1, \\dots, \\sigma_r)$ with singular values $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_r \\ge 0$, and $r = \\operatorname{rank}(X_c)$.\n- The Frobenius norm identity: $\\|X_c\\|_F^2 = \\sum_{i=1}^{r} \\sigma_i^2$.\n\nYour program must:\n- For each test case, center the columns of the input matrix $X$ to obtain $X_c$ (subtract each column mean; no scaling).\n- Compute the SVD of $X_c$ to obtain singular values $\\sigma_1, \\dots, \\sigma_r$.\n- Define the truncated SVD at level $k$ as keeping the first $k$ singular values and corresponding singular vectors. To handle numerical rank robustly, let the effective rank $r$ be the count of singular values satisfying $\\sigma_i > \\tau$, where $\\tau$ is a tolerance based on dimensions and machine precision. If $k > r$, use $k_{\\text{eff}} = r$. If $k = 0$, the explained proportion must be $0$.\n- Compute the proportion of variance explained by the first $k_{\\text{eff}}$ singular values as a decimal fraction (not a percentage), namely the ratio of the sum of the squares of the first $k_{\\text{eff}}$ singular values to the sum of the squares of all $r$ singular values, using only the above foundational identities.\n\nAngle units do not apply. Physical units do not apply. All outputs must be decimal fractions.\n\nTest suite:\nFor each test case below, $X$ is the raw expression matrix and $k$ is the truncation level. You must center columns of $X$ before computing the SVD. The matrices are:\n\n- Case $1$ (happy path, tall matrix, moderate $k$):\n$$\nX^{(1)} =\n\\begin{bmatrix}\n2 & 0 & 1 \\\\\n0 & 1 & 3 \\\\\n4 & 2 & 5 \\\\\n6 & 3 & 6\n\\end{bmatrix}, \\quad k^{(1)} = 2.\n$$\n\n- Case $2$ (boundary $k = 0$):\n$$\nX^{(2)} =\n\\begin{bmatrix}\n10 & 0 & -2 & 3 \\\\\n5 & 1 & 0 & 0 \\\\\n0 & -1 & 2 & -3\n\\end{bmatrix}, \\quad k^{(2)} = 0.\n$$\n\n- Case $3$ (wide matrix, redundant and constant columns; $k$ exceeds rank):\n$$\nX^{(3)} =\n\\begin{bmatrix}\n1 & 1 & 5 & 0 & -1 \\\\\n2 & 2 & 5 & 1 & 0 \\\\\n3 & 3 & 5 & 2 & 1\n\\end{bmatrix}, \\quad k^{(3)} = 3.\n$$\n\n- Case $4$ (tall matrix with a zero column, nontrivial rank):\n$$\nX^{(4)} =\n\\begin{bmatrix}\n0 & 1 & 0 & -1 \\\\\n0 & 2 & 1 & 0 \\\\\n0 & 3 & 2 & 1 \\\\\n0 & 4 & 3 & 2 \\\\\n0 & 5 & 4 & 3\n\\end{bmatrix}, \\quad k^{(4)} = 2.\n$$\n\n- Case $5$ (exact linear dependence among columns; $k$ equals rank):\n$$\nX^{(5)} =\n\\begin{bmatrix}\n1 & 2 & 5 \\\\\n2 & 4 & 10 \\\\\n3 & 6 & 15 \\\\\n4 & 8 & 20\n\\end{bmatrix}, \\quad k^{(5)} = 2.\n$$\n\nNumerical rank tolerance:\nUse a numerically stable tolerance $\\tau$ to decide which singular values are treated as nonzero. Choose $\\tau = \\max(n, p) \\cdot \\sigma_{\\max} \\cdot \\varepsilon$, where $n$ and $p$ are the dimensions of $X_c$, $\\sigma_{\\max}$ is the largest singular value of $X_c$, and $\\varepsilon$ is machine epsilon for $64$-bit floating point arithmetic.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each proportion rounded to $8$ decimal places as a decimal fraction in $[0, 1]$. For example, a valid output looks like $[r_a,r_b,r_c,r_d,r_e]$ where each $r_\\cdot$ is a decimal fraction.\n\nYour program must be self-contained, use the specified test suite, and produce the single-line output described above.",
            "solution": "The problem requires us to compute the proportion of total sample-wise variance captured by the first $k$ singular values of a column-centered data matrix. This quantity is central to Principal Component Analysis (PCA), where it is used to assess the dimensionality reduction quality. The solution will be derived from the foundational definitions provided.\n\nLet the given raw data matrix be $X \\in \\mathbb{R}^{n \\times p}$, with $n$ samples (rows) and $p$ features or genes (columns).\n\nFirst, we center the data by subtracting the mean of each column from its elements. Let $\\bar{x}_j = \\frac{1}{n} \\sum_{i=1}^{n} X_{ij}$ be the mean of the $j$-th column. The column-centered matrix $X_c$ has elements $(X_c)_{ij} = X_{ij} - \\bar{x}_j$.\n\nThe total sample-wise variance, $\\text{Var}_{\\text{total}}$, is the sum of the sample variances of each feature. This is equivalent to the trace of the sample covariance matrix $S$. The problem defines the sample covariance matrix as $S = \\frac{1}{n - 1} X_c^\\top X_c$.\nUsing the linearity of the trace operator, the total variance is:\n$$\n\\text{Var}_{\\text{total}} = \\operatorname{tr}(S) = \\operatorname{tr}\\left(\\frac{1}{n-1} X_c^\\top X_c\\right) = \\frac{1}{n - 1} \\operatorname{tr}(X_c^\\top X_c)\n$$\nA fundamental property of the trace is that $\\operatorname{tr}(A^\\top A)$ is equal to the squared Frobenius norm of $A$, $\\|A\\|_F^2 = \\sum_{i,j} A_{ij}^2$. Therefore, we have:\n$$\n\\text{Var}_{\\text{total}} = \\frac{1}{n - 1} \\|X_c\\|_F^2\n$$\nThe problem provides a key identity connecting the Frobenius norm to the singular values of $X_c$. Let the Singular Value Decomposition (SVD) of $X_c$ be $X_c = U \\Sigma V^\\top$, where the singular values $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_r > 0$ are the diagonal entries of $\\Sigma$, and $r = \\operatorname{rank}(X_c)$. The identity is:\n$$\n\\|X_c\\|_F^2 = \\sum_{i=1}^{r} \\sigma_i^2\n$$\nSubstituting this into our expression for total variance gives:\n$$\n\\text{Var}_{\\text{total}} = \\frac{1}{n - 1} \\sum_{i=1}^{r} \\sigma_i^2\n$$\nThe principal components of the data are the columns of the matrix $Z = X_c V$. The variance of the $i$-th principal component is directly related to the $i$-th singular value squared: $\\text{Var}(Z_i) = \\frac{\\sigma_i^2}{n - 1}$. The total variance explained by the first $k$ principal components is the sum of their individual variances:\n$$\n\\text{Var}_k = \\sum_{i=1}^{k} \\text{Var}(Z_i) = \\sum_{i=1}^{k} \\frac{\\sigma_i^2}{n - 1} = \\frac{1}{n - 1} \\sum_{i=1}^{k} \\sigma_i^2\n$$\nThe proportion of variance explained by the first $k$ components is the ratio of the variance they capture, $\\text{Var}_k$, to the total variance, $\\text{Var}_{\\text{total}}$.\n$$\nP_k = \\frac{\\text{Var}_k}{\\text{Var}_{\\text{total}}} = \\frac{\\frac{1}{n-1} \\sum_{i=1}^{k} \\sigma_i^2}{\\frac{1}{n-1} \\sum_{i=1}^{r} \\sigma_i^2} = \\frac{\\sum_{i=1}^{k} \\sigma_i^2}{\\sum_{i=1}^{r} \\sigma_i^2}\n$$\nThe factor $\\frac{1}{n-1}$ cancels, showing that the proportion of variance can be computed directly from the squared singular values. This derived formula provides the basis for our algorithm.\n\nThe computational algorithm proceeds as follows:\n1.  For each input matrix $X \\in \\mathbb{R}^{n \\times p}$ and integer $k$, compute the column-centered matrix $X_c$.\n2.  Compute the singular values $\\sigma_i$ of $X_c$ using SVD.\n3.  To handle numerical precision, determine the effective rank $r$. A singular value $\\sigma_i$ is considered non-zero if $\\sigma_i > \\tau$, where the tolerance $\\tau$ is defined as $\\tau = \\max(n, p) \\cdot \\sigma_{\\max} \\cdot \\varepsilon$. Here, $\\sigma_{\\max}$ is the largest singular value and $\\varepsilon$ is machine epsilon for the floating-point precision used. The effective rank $r$ is the count of singular values exceeding $\\tau$.\n4.  The number of components to consider, $k_{\\text{eff}}$, is adjusted based on the rank. If $k=0$, the proportion is $0$. Otherwise, $k_{\\text{eff}} = \\min(k, r)$.\n5.  The final proportion is calculated using the derived formula:\n    $$\n    P_{k_{\\text{eff}}} = \\frac{\\sum_{i=1}^{k_{\\text{eff}}} \\sigma_i^2}{\\sum_{i=1}^{r} \\sigma_i^2}\n    $$\n    where the sums are taken over the squared singular values deemed non-zero in step $3$. If the denominator is zero (i.e., the centered matrix had zero variance), the proportion is $0$.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the proportion of variance explained by the first k singular values\n    for a list of test cases according to the derived formula.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (happy path, tall matrix, moderate k)\n        (np.array([\n            [2, 0, 1],\n            [0, 1, 3],\n            [4, 2, 5],\n            [6, 3, 6]\n        ], dtype=float), 2),\n        \n        # Case 2 (boundary k = 0)\n        (np.array([\n            [10, 0, -2, 3],\n            [5, 1, 0, 0],\n            [0, -1, 2, -3]\n        ], dtype=float), 0),\n\n        # Case 3 (wide matrix, redundant and constant columns; k exceeds rank)\n        (np.array([\n            [1, 1, 5, 0, -1],\n            [2, 2, 5, 1, 0],\n            [3, 3, 5, 2, 1]\n        ], dtype=float), 3),\n\n        # Case 4 (tall matrix with a zero column, nontrivial rank)\n        (np.array([\n            [0, 1, 0, -1],\n            [0, 2, 1, 0],\n            [0, 3, 2, 1],\n            [0, 4, 3, 2],\n            [0, 5, 4, 3]\n        ], dtype=float), 2),\n\n        # Case 5 (exact linear dependence among columns; k equals rank)\n        (np.array([\n            [1, 2, 5],\n            [2, 4, 10],\n            [3, 6, 15],\n            [4, 8, 20]\n        ], dtype=float), 2)\n    ]\n\n    results = []\n    \n    for X, k in test_cases:\n        n, p = X.shape\n        \n        # Handle case with no rows, which implies no variance.\n        if n == 0:\n            results.append(0.0)\n            continue\n            \n        # Step 1: Center the columns of the input matrix X.\n        X_c = X - X.mean(axis=0)\n\n        # Step 2: Compute the SVD of the centered matrix X_c. Only singular values are needed.\n        try:\n            sigma = np.linalg.svd(X_c, compute_uv=False)\n        except np.linalg.LinAlgError:\n            # In case of a failure, assume zero variance.\n            results.append(0.0)\n            continue\n\n        # Handle matrices that result in no singular values (e.g., zero columns/rows).\n        if sigma.size == 0:\n            results.append(0.0)\n            continue\n\n        # Step 3: Determine the numerical rank r using the specified tolerance.\n        eps = np.finfo(X.dtype).eps\n        sigma_max = sigma[0]\n        tolerance = max(n, p) * sigma_max * eps\n        \n        # Filter for singular values greater than the tolerance.\n        sigma_effective = sigma[sigma > tolerance]\n        r = len(sigma_effective)\n        \n        # Step 4: Handle k and calculate the proportion of variance.\n        # If k is 0, the proportion must be 0.\n        if k == 0:\n            proportion = 0.0\n        else:\n            # Effective k cannot exceed the rank.\n            k_eff = min(k, r)\n            \n            # Use the squared effective singular values for variance calculations.\n            sigma_sq_effective = sigma_effective**2\n            \n            # The denominator is the sum of all effective squared singular values.\n            total_variance_proxy = np.sum(sigma_sq_effective)\n\n            if total_variance_proxy  1e-15: # Check for near-zero total variance.\n                # If total variance is zero, no variance can be explained.\n                proportion = 0.0\n            else:\n                # The numerator is the sum of the first k_eff squared singular values.\n                explained_variance_proxy = np.sum(sigma_sq_effective[:k_eff])\n                proportion = explained_variance_proxy / total_variance_proxy\n                \n        results.append(round(proportion, 8))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Real-world datasets, particularly in genomics and clinical research, often suffer from multicollinearity, where predictors are highly correlated or redundant. This redundancy can destabilize statistical models and complicate interpretation. This hands-on exercise provides a concrete, algorithmic approach to identify and prune these dependent features, allowing you to construct a minimal, linearly independent spanning set of predictors for more robust downstream analysis . You will implement a greedy column selection procedure based on a numerically sound definition of matrix rank.",
            "id": "4578482",
            "problem": "You are given multiple real-valued design matrices representing redundant clinical predictors used in bioinformatics and medical data analytics. The task is to extract, for each matrix, a minimal spanning set of predictors by removing columns that are linearly dependent in the sense of numerical linear algebra. Your program must determine linear independence by relying on first principles and a stability-aware numerical rank criterion.\n\nDefinitions and fundamental base:\n- A set of column vectors $\\{\\mathbf{a}_1,\\dots,\\mathbf{a}_k\\}$ in $\\mathbb{R}^m$ is linearly independent if the only solution to $x_1 \\mathbf{a}_1 + \\dots + x_k \\mathbf{a}_k = \\mathbf{0}$ is $x_1 = \\dots = x_k = 0$. Equivalently, the rank of the matrix $A = [\\mathbf{a}_1 \\ \\dots \\ \\mathbf{a}_k]$ is $k$.\n- The rank of a matrix $A \\in \\mathbb{R}^{m \\times n}$ is the dimension of its column space and equals the number of strictly positive singular values of $A$.\n- In numerical computation, exact zero is replaced by a numerical threshold. We define the numerical rank $\\operatorname{rank}_{\\tau}(A)$ as the number of singular values $\\sigma_i(A)$ strictly greater than a tolerance $\\tau$, where the tolerance is defined relative to the spectral norm as $\\tau = 10^{-9} \\cdot \\|A\\|_2$. Here, $\\|A\\|_2$ is the largest singular value of $A$.\n\nRequired algorithmic behavior (principle-based):\n- Process the columns of each given matrix $A \\in \\mathbb{R}^{m \\times n}$ from left to right (that is, in their given order from index $0$ to $n-1$).\n- Let $\\tau = 10^{-9} \\cdot \\|A\\|_2$ be the fixed threshold for that matrix. Initialize an empty index set $S$ and a current rank $r = 0$. For each column index $j$ in ascending order:\n  - Form the submatrix $A_S$ whose columns are those indexed by $S$ (if $S$ is empty, $A_S$ has zero columns). Compute $r_{\\text{old}} = \\operatorname{rank}_{\\tau}(A_S)$.\n  - Form the augmented submatrix $A_{S \\cup \\{j\\}}$ by appending column $j$ to $A_S$. Compute $r_{\\text{new}} = \\operatorname{rank}_{\\tau}(A_{S \\cup \\{j\\}})$.\n  - If $r_{\\text{new}}  r_{\\text{old}}$, then update $S \\leftarrow S \\cup \\{j\\}$ and $r \\leftarrow r_{\\text{new}}$; otherwise, skip column $j$ as numerically dependent on the selected set.\n- Continue until all columns are processed. The resulting index set $S$ (sorted in ascending order by construction) is the minimal spanning set, with cardinality equal to the numerical rank $\\operatorname{rank}_{\\tau}(A)$, and it spans the numerical column space of $A$.\n\nTest suite:\nFor each matrix below, compute the ascending list of zero-based column indices forming the minimal spanning set $S$ as specified. There are no physical units or angles involved.\n\n- Case $1$ (redundant tall matrix):\n  $$\n  A_1 =\n  \\begin{bmatrix}\n  1  0  1  0  1 \\\\\n  0  1  1  0  0 \\\\\n  2  1  3  0  2 \\\\\n  0  0  0  1  2 \\\\\n  1  1  2  1  3 \\\\\n  3  1  4  1  5\n  \\end{bmatrix}\n  $$\n  where the column at index 2 is the sum of columns at indices 0 and 1, and the column at index 4 is the sum of the column at index 0 and twice the column at index 3.\n\n- Case $2$ (zero and duplicate columns):\n  $$\n  A_2 =\n  \\begin{bmatrix}\n  1  0  1  0 \\\\\n  2  0  2  1 \\\\\n  3  0  3  0 \\\\\n  4  0  4  1\n  \\end{bmatrix}\n  $$\n  where the column at index 1 is the zero vector and the column at index 2 duplicates the column at index 0.\n\n- Case $3$ (near-zero column under tolerance):\n  $$\n  A_3 =\n  \\begin{bmatrix}\n  1  10^{-12}  0 \\\\\n  0  0          1 \\\\\n  0  0          0 \\\\\n  0  0          0 \\\\\n  0  0          0\n  \\end{bmatrix}\n  $$\n\n- Case $4$ (wide matrix with combinations):\n  $$\n  A_4 =\n  \\begin{bmatrix}\n  1  0  0  1  0  1 \\\\\n  0  1  0  1  1  0 \\\\\n  0  0  1  0  1  1\n  \\end{bmatrix}\n  $$\n  where columns $4$, $5$, and $6$ are linear combinations of the first three columns.\n\n- Case $5$ (ill-conditioned but independent under tolerance):\n  $$\n  A_5 =\n  \\begin{bmatrix}\n  1  1 \\\\\n  0  10^{-8}\n  \\end{bmatrix}\n  $$\n\nAnswer specification:\n- For each matrix $A_i$, output the ascending list of selected zero-based column indices $S_i$.\n- Aggregate all five results into a single line as a bracketed, comma-separated list of lists with no spaces, for example, $[[a_1,a_2],[b_1,\\dots],\\dots]$.\n- Your program must produce exactly one line containing this aggregate output.\n\nYour program should produce a single line of output containing the results as a comma-separated list of lists enclosed in square brackets with no spaces (for example, $[[0,1,3],[0,3],[0,2],[0,1,2],[0,1]]$).",
            "solution": "The problem requires the identification of a minimal spanning set of predictors from a given design matrix $A$. This is a fundamental task in data analysis, often termed feature selection or dimensionality reduction, where the goal is to eliminate redundant information. The prescribed method is a greedy algorithm that iteratively builds a set of linearly independent columns based on a numerically robust definition of matrix rank.\n\nThe core principle is grounded in the concepts of numerical linear algebra. A set of vectors $\\{\\mathbf{a}_1, \\dots, \\mathbf{a}_k\\}$ is linearly independent if the matrix $A = [\\mathbf{a}_1 \\ \\dots \\ \\mathbf{a}_k]$ has rank $k$. In floating-point arithmetic, computational errors make a simple check for zero singular values unreliable. Therefore, we use the concept of numerical rank, $\\operatorname{rank}_{\\tau}(A)$, defined as the number of singular values $\\sigma_i(A)$ that are strictly greater than a tolerance $\\tau$. The problem specifies a relative tolerance: $\\tau = 10^{-9} \\cdot \\|A\\|_2$, where $\\|A\\|_2$ is the spectral norm of the matrix, equivalent to its largest singular value, $\\sigma_{\\max}(A)$. A column is deemed to contribute new information if its inclusion in the set of selected columns increases the numerical rank of the matrix formed by these columns.\n\nThe algorithmic procedure is as follows:\n1.  For a given matrix $A \\in \\mathbb{R}^{m \\times n}$, we first compute its spectral norm $\\|A\\|_2$ by finding the largest singular value of $A$. The numerical tolerance is then fixed as $\\tau = 10^{-9} \\cdot \\|A\\|_2$.\n2.  We initialize an empty set of indices, $S$, which will store the indices of the selected columns, and a variable for the current rank of the selected submatrix, $r=0$.\n3.  We iterate through the columns of $A$ from left to right, indexed by $j = 0, 1, \\dots, n-1$.\n4.  For each column $\\mathbf{a}_j$, we form a test matrix by appending it to the columns already selected, which are indexed by $S$. Let this augmented matrix be $A_{S \\cup \\{j\\}}$.\n5.  We compute the numerical rank of this augmented matrix, $r_{\\text{new}} = \\operatorname{rank}_{\\tau}(A_{S \\cup \\{j\\}})$.\n6.  If $r_{\\text{new}}  r$, it signifies that column $\\mathbf{a}_j$ is linearly independent of the columns in the set $S$ with respect to the tolerance $\\tau$. In this case, we update our set of selected indices by adding $j$, so $S \\leftarrow S \\cup \\{j\\}$, and update the current rank, $r \\leftarrow r_{\\text{new}}$.\n7.  If $r_{\\text{new}} \\le r$, the column $\\mathbf{a}_j$ is numerically dependent on the columns already in $S$, and we discard it.\n8.  After iterating through all columns, the final set $S$ contains the zero-based indices of a minimal spanning set for the column space of $A$.\n\nLet us illustrate this procedure with the matrix from Case $1$:\n$$\nA_1 =\n\\begin{bmatrix}\n1  0  1  0  1 \\\\\n0  1  1  0  0 \\\\\n2  1  3  0  2 \\\\\n0  0  0  1  2 \\\\\n1  1  2  1  3 \\\\\n3  1  4  1  5\n\\end{bmatrix}\n$$\nFirst, we compute the singular values of $A_1$, which are approximately $\\{8.33, 2.50, 1.00, 0, 0\\}$. The spectral norm is $\\|A_1\\|_2 \\approx 8.33$. The tolerance is $\\tau = 10^{-9} \\cdot 8.33 \\approx 8.33 \\times 10^{-9}$. We initialize $S = []$ and $r = 0$.\n\n- **For $j=0$**: We test column $\\mathbf{a}_0$. The matrix $[\\mathbf{a}_0]$ has rank $1$. Since $r_{\\text{new}}=1  r=0$, we add $0$ to our set. $S \\leftarrow [0]$, $r \\leftarrow 1$.\n- **For $j=1$**: We test column $\\mathbf{a}_1$. The matrix $[\\mathbf{a}_0 \\ \\mathbf{a}_1]$ has rank $2$ since $\\mathbf{a}_0$ and $\\mathbf{a}_1$ are linearly independent. Since $r_{\\text{new}}=2  r=1$, we add $1$ to our set. $S \\leftarrow [0, 1]$, $r \\leftarrow 2$.\n- **For $j=2$**: We test column $\\mathbf{a}_2$. We are given that $\\mathbf{a}_2 = \\mathbf{a}_0 + \\mathbf{a}_1$. Thus, $\\mathbf{a}_2$ is in the span of $\\{\\mathbf{a}_0, \\mathbf{a}_1\\}$. The matrix $[\\mathbf{a}_0 \\ \\mathbf{a}_1 \\ \\mathbf{a}_2]$ has rank $2$. Since $r_{\\text{new}}=2 \\ngtr r=2$, we do not add $2$ to our set. $S$ remains $[0, 1]$.\n- **For $j=3$**: We test column $\\mathbf{a}_3$. The matrix $[\\mathbf{a}_0 \\ \\mathbf{a}_1 \\ \\mathbf{a}_3]$ can be shown to have rank $3$. Since $r_{\\text{new}}=3  r=2$, we add $3$ to our set. $S \\leftarrow [0, 1, 3]$, $r \\leftarrow 3$.\n- **For $j=4$**: We test column $\\mathbf{a}_4$. We are given that $\\mathbf{a}_4 = \\mathbf{a}_0 + 2\\mathbf{a}_3$. Thus, $\\mathbf{a}_4$ is in the span of $\\{\\mathbf{a}_0, \\mathbf{a}_3\\}$, which is a subspace of the span of $\\{\\mathbf{a}_0, \\mathbf{a}_1, \\mathbf{a}_3\\}$. The matrix $[\\mathbf{a}_0 \\ \\mathbf{a}_1 \\ \\mathbf{a}_3 \\ \\mathbf{a}_4]$ has rank $3$. Since $r_{\\text{new}}=3 \\ngtr r=3$, we do not add $4$ to our set.\n\nAfter processing all columns, the final minimal spanning set of indices for $A_1$ is $S_1 = [0, 1, 3]$. This same procedure is applied to all provided test matrices. The implementation uses `numpy.linalg.svd` to obtain the singular values required for the numerical rank computations.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef find_minimal_spanning_set(A: np.ndarray) -> list[int]:\n    \"\"\"\n    Extracts a minimal spanning set of column indices from matrix A.\n\n    The method iteratively builds a set of linearly independent columns based\n    on a numerically robust rank criterion. A column is added if its inclusion\n    increases the numerical rank of the matrix formed by the selected columns.\n    \"\"\"\n    m, n = A.shape\n    if n == 0:\n        return []\n\n    # Calculate the spectral norm of the full matrix A to determine the tolerance.\n    # The spectral norm is the largest singular value.\n    s_full = np.linalg.svd(A, compute_uv=False)\n    norm_A = s_full[0] if len(s_full) > 0 else 0.0\n    \n    # Define the tolerance for numerical rank calculation.\n    tau = 1e-9 * norm_A\n\n    def get_numerical_rank(M: np.ndarray, tolerance: float) -> int:\n        \"\"\"\n        Computes the numerical rank of matrix M based on a given tolerance.\n        The rank is the number of singular values greater than the tolerance.\n        \"\"\"\n        if M.shape[1] == 0:\n            return 0\n        s_sub = np.linalg.svd(M, compute_uv=False)\n        return np.sum(s_sub > tolerance)\n\n    selected_indices = []\n    current_rank = 0\n\n    # Iterate through each column index from left to right.\n    for j in range(n):\n        # Form a temporary matrix by augmenting the current set with the new column.\n        potential_indices = selected_indices + [j]\n        A_augmented = A[:, potential_indices]\n        \n        # Calculate the numerical rank of the augmented matrix.\n        new_rank = get_numerical_rank(A_augmented, tau)\n        \n        # If the rank increases, the new column is linearly independent.\n        if new_rank > current_rank:\n            selected_indices.append(j)\n            current_rank = new_rank\n            \n    return selected_indices\n\ndef solve():\n    \"\"\"\n    Defines the test cases, runs the algorithm, and prints the formatted result.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    A1 = np.array([\n        [1., 0., 1., 0., 1.],\n        [0., 1., 1., 0., 0.],\n        [2., 1., 3., 0., 2.],\n        [0., 0., 0., 1., 2.],\n        [1., 1., 2., 1., 3.],\n        [3., 1., 4., 1., 5.]\n    ], dtype=np.float64)\n\n    A2 = np.array([\n        [1., 0., 1., 0.],\n        [2., 0., 2., 1.],\n        [3., 0., 3., 0.],\n        [4., 0., 4., 1.]\n    ], dtype=np.float64)\n    \n    A3 = np.array([\n        [1., 1e-12, 0.],\n        [0., 0.,    1.],\n        [0., 0.,    0.],\n        [0., 0.,    0.],\n        [0., 0.,    0.]\n    ], dtype=np.float64)\n\n    A4 = np.array([\n        [1., 0., 0., 1., 0., 1.],\n        [0., 1., 0., 1., 1., 0.],\n        [0., 0., 1., 0., 1., 1.]\n    ], dtype=np.float64)\n\n    A5 = np.array([\n        [1., 1.],\n        [0., 1e-8]\n    ], dtype=np.float64)\n\n    test_cases = [A1, A2, A3, A4, A5]\n\n    results = []\n    for A in test_cases:\n        result = find_minimal_spanning_set(A)\n        results.append(result)\n\n    # The final print statement must match the specified format exactly:\n    # a bracketed, comma-separated list of lists with no spaces.\n    # The str() function on a list of lists includes spaces, e.g., '[[0, 1], [2]]'.\n    # We remove these spaces to match the required format '[[0,1],[2]]'.\n    print(str(results).replace(' ', ''))\n\nsolve()\n```"
        },
        {
            "introduction": "Linear models are a cornerstone of data analysis, but solving them robustly requires careful numerical methods, especially when dealing with the non-ideal matrices found in real data. While the normal equations provide a textbook solution to the least-squares problem, they are sensitive to the collinearity issues we explored previously. This exercise demonstrates the superior stability and universality of the SVD-based Moore-Penrose pseudoinverse, providing a powerful tool for finding the optimal minimum-norm solution to any linear regression problem .",
            "id": "4578463",
            "problem": "Consider a linear model where a design matrix $X \\in \\mathbb{R}^{m \\times p}$ and an outcome vector $y \\in \\mathbb{R}^{m}$ are given. In bioinformatics and medical data analytics, such matrices arise when modeling relationships between gene expression features and phenotypic measurements. Starting from fundamental definitions in linear algebra, use the Singular Value Decomposition (SVD) to construct the Mooreâ€“Penrose pseudoinverse $X^{+}$ and compute the least-squares solution $X^{+} y$. When $X^{\\top} X$ is invertible, also compute the unique normal-equation solution. Design your program to:\n\n- Derive $X^{+}$ from first principles using SVD and a numerically justified threshold to decide which singular values are treated as nonzero.\n- Determine whether $X^{\\top} X$ is invertible based on the column rank of $X$.\n- If $X^{\\top} X$ is invertible, compute the normal-equation solution and compare it to $X^{+} y$ by the Euclidean norm of their difference.\n- In all cases, compute the Euclidean norm of the residual $X (X^{+} y) - y$.\n\nUse only well-tested facts: matrix decomposition definitions, properties of orthogonal matrices, and solvability conditions for linear systems. Do not use any unproven shortcuts.\n\nYour program must implement and evaluate the following test suite of parameter values. Each $X$ and $y$ is specified explicitly:\n\n1. Happy path (tall, full column rank):\n   $$X_1 = \\begin{bmatrix}\n   2  -1  3 \\\\\n   0  4  5 \\\\\n   1  2  -2 \\\\\n   3  -5  1 \\\\\n   4  0  -1\n   \\end{bmatrix}, \\quad\n   y_1 = \\begin{bmatrix}\n   1 \\\\ 0 \\\\ -1 \\\\ 2 \\\\ 3\n   \\end{bmatrix}.$$\n\n2. Ill-conditioned but invertible $X^{\\top} X$ (near-collinearity):\n   $$X_2 = \\begin{bmatrix}\n   1  2.0001  0 \\\\\n   2  4.0002  1 \\\\\n   3  6.0003  0 \\\\\n   4  8.0004  1 \\\\\n   5  10.0005  0\n   \\end{bmatrix}, \\quad\n   y_2 = \\begin{bmatrix}\n   1 \\\\ 1 \\\\ 2 \\\\ 3 \\\\ 5\n   \\end{bmatrix}.$$\n\n3. Rank-deficient (normal equations not applicable):\n   $$X_3 = \\begin{bmatrix}\n   1  0  0 \\\\\n   0  1  1 \\\\\n   1  0  0 \\\\\n   0  1  1\n   \\end{bmatrix}, \\quad\n   y_3 = \\begin{bmatrix}\n   1 \\\\ 2 \\\\ 3 \\\\ 4\n   \\end{bmatrix}.$$\n\n4. Square and invertible:\n   $$X_4 = \\begin{bmatrix}\n   3  0  1 \\\\\n   2  -1  0 \\\\\n   1  2  4\n   \\end{bmatrix}, \\quad\n   y_4 = \\begin{bmatrix}\n   0 \\\\ 1 \\\\ 2\n   \\end{bmatrix}.$$\n\nFor numerical robustness, the SVD-based pseudoinverse should use a threshold $\\tau = \\max(m, p) \\cdot \\varepsilon \\cdot \\sigma_{\\max}$, where $\\varepsilon$ is machine precision for double-precision floating point, and $\\sigma_{\\max}$ is the largest singular value of $X$. Singular values strictly greater than $\\tau$ are inverted; others are treated as zero.\n\nDefine the final output for each test case as a list with three entries:\n- The boolean indicating whether the normal-equation computation is applicable (that is, whether $X^{\\top} X$ is invertible).\n- The float equal to the Euclidean norm of the difference between the SVD-based solution and the normal-equation solution (if applicable), or the float $-1.0$ if not applicable.\n- The float equal to the Euclidean norm of the residual $X (X^{+} y) - y$.\n\nYour program should produce a single line of output containing the results as a comma-separated list of these per-test-case lists, enclosed in square brackets. For example, the output format must be:\n$$\\texttt{[[b\\_1,d\\_1,r\\_1],[b\\_2,d\\_2,r\\_2],[b\\_3,d\\_3,r\\_3],[b\\_4,d\\_4,r\\_4]]}.$$\nNo physical units or angle units are involved; all quantities are dimensionless real numbers.",
            "solution": "The problem requires a comparative analysis of two methods for solving the linear least-squares problem, which is fundamental to data modeling. The goal is to find a vector of parameters $\\beta \\in \\mathbb{R}^p$ that best explains the observed outcomes $y \\in \\mathbb{R}^m$ through a linear transformation by the design matrix $X \\in \\mathbb{R}^{m \\times p}$. \"Best\" is defined in the sense of minimizing the sum of squared errors, i.e., minimizing the squared Euclidean norm of the residual vector.\n\n**The Least-Squares Problem**\nThe objective is to find the vector $\\hat{\\beta}$ that solves the following minimization problem:\n$$ \\hat{\\beta} = \\arg\\min_{\\beta \\in \\mathbb{R}^p} \\| y - X\\beta \\|_2^2 $$\nThe squared norm can be expressed as a quadratic form:\n$$ L(\\beta) = (y - X\\beta)^{\\top}(y - X\\beta) = y^{\\top}y - 2\\beta^{\\top}X^{\\top}y + \\beta^{\\top}X^{\\top}X\\beta $$\nThis is a convex function of $\\beta$, and its minimum can be found by setting its gradient with respect to $\\beta$ to the zero vector.\n$$ \\nabla_{\\beta} L(\\beta) = -2X^{\\top}y + 2X^{\\top}X\\beta = 0 $$\nThis yields the celebrated **normal equations**:\n$$ X^{\\top}X\\beta = X^{\\top}y $$\n\n**Method 1: The Normal Equation Solution**\nThe matrix $X^{\\top}X$ is a square matrix of size $p \\times p$. If this matrix is invertible, a unique solution for $\\beta$ exists and is given by:\n$$ \\hat{\\beta}_{NE} = (X^{\\top}X)^{-1}X^{\\top}y $$\nA core theorem in linear algebra states that $X^{\\top}X$ is invertible if and only if the matrix $X$ has linearly independent columns. This is equivalent to stating that $X$ has full column rank, i.e., $\\text{rank}(X) = p$. When these conditions are not met (i.e., when columns are collinear or the system is underdetermined), $X^{\\top}X$ is singular, and this method fails. Furthermore, even if $X^{\\top}X$ is theoretically invertible, it may be ill-conditioned if $X$ has nearly collinear columns. The explicit computation of the inverse can amplify numerical errors in such cases.\n\n**Method 2: The SVD and Moore-Penrose Pseudoinverse Solution**\nThe Singular Value Decomposition (SVD) provides a powerful and numerically robust framework for analyzing linear systems. Any matrix $X \\in \\mathbb{R}^{m \\times p}$ can be decomposed as:\n$$ X = U \\Sigma V^{\\top} $$\nwhere:\n- $U$ is an $m \\times m$ orthogonal matrix whose columns ($u_i$) are the left-singular vectors.\n- $V$ is a $p \\times p$ orthogonal matrix whose columns ($v_i$) are the right-singular vectors.\n- $\\Sigma$ is an $m \\times p$ rectangular diagonal matrix containing the non-negative singular values $\\sigma_i$ in decreasing order. The number of non-zero singular values equals the rank of $X$.\n\nUsing the SVD, the Moore-Penrose pseudoinverse $X^{+}$ of $X$ is uniquely defined as:\n$$ X^{+} = V \\Sigma^{+} U^{\\top} $$\nwhere $\\Sigma^{+}$ is a $p \\times m$ matrix derived from $\\Sigma$. It is constructed by taking the transpose of $\\Sigma$ and then taking the reciprocal of each non-zero singular value. In finite-precision arithmetic, we must use a threshold $\\tau = \\max(m, p) \\cdot \\varepsilon \\cdot \\sigma_{\\max}$ to decide which singular values are numerically non-zero. The elements of the diagonal of $\\Sigma^{+}$, denoted $\\sigma_i^{+}$, are thus:\n$$ \\sigma_i^{+} = \\begin{cases} 1/\\sigma_i  \\text{if } \\sigma_i  \\tau \\\\ 0  \\text{if } \\sigma_i \\le \\tau \\end{cases} $$\nThe rank of the matrix is numerically estimated as the count of singular values greater than $\\tau$. The condition for the applicability of the normal equations, $\\text{rank}(X) = p$, is checked using this numerical rank.\n\nThe SVD-based least-squares solution is given by:\n$$ \\hat{\\beta}_{SVD} = X^{+}y = (V \\Sigma^{+} U^{\\top})y $$\nThis solution is always defined. It provides the unique minimum-norm solution to the least-squares problem, meaning that among all vectors $\\beta$ that minimize $\\|X\\beta - y\\|_2$, $\\hat{\\beta}_{SVD}$ is the one with the smallest Euclidean norm $\\|\\beta\\|_2$.\n\n**Algorithmic Procedure**\nFor each given pair $(X, y)$, the following steps are executed:\n1.  Obtain the dimensions $m$ and $p$ from $X$.\n2.  Compute the SVD of $X$ to get $U$, the singular values vector $s$, and $V^{\\top}$. For efficiency, the economy-sized SVD is used.\n3.  Determine the numerical threshold $\\tau = \\max(m, p) \\cdot \\varepsilon \\cdot \\sigma_{\\max}$, where $\\sigma_{\\max} = s[0]$.\n4.  Determine the numerical rank $r_{eff}$ by counting the singular values $s_i  \\tau$.\n5.  Set the boolean `is_normal_eq_applicable` to `True` if $r_{eff} = p$, and `False` otherwise.\n6.  Compute the SVD solution $\\hat{\\beta}_{SVD}$. This is done efficiently as $\\hat{\\beta}_{SVD} = V(\\Sigma^{+}(U^{\\top}y))$, where the operations are performed vector-wise without explicitly forming the matrix $\\Sigma^{+}$.\n7.  If `is_normal_eq_applicable` is `True`:\n    a. Compute the normal equation solution $\\hat{\\beta}_{NE} = (X^{\\top}X)^{-1}X^{\\top}y$.\n    b. Calculate `difference_norm` as the Euclidean norm $\\|\\hat{\\beta}_{SVD} - \\hat{\\beta}_{NE}\\|_2$.\n8.  If `is_normal_eq_applicable` is `False`, set `difference_norm` to $-1.0$.\n9.  In all cases, calculate the norm of the residual vector, `residual_norm` = $\\|X\\hat{\\beta}_{SVD} - y\\|_2$.\n10. Store the resulting triplet (`is_normal_eq_applicable`, `difference_norm`, `residual_norm`).\n\nThis procedure will be applied to all four test cases to generate the final output.",
            "answer": "```python\nimport numpy as np\n\ndef solve_least_squares_case(X, y):\n    \"\"\"\n    Solves a linear least-squares problem using SVD and Normal Equations,\n    and returns the specified comparison metrics.\n\n    Args:\n        X (np.ndarray): The m x p design matrix.\n        y (np.ndarray): The m-dimensional outcome vector.\n\n    Returns:\n        list: A list containing [is_normal_eq_applicable, difference_norm, residual_norm].\n    \"\"\"\n    m, p = X.shape\n\n    # 1. Compute SVD and determine numerical rank\n    try:\n        U, s, Vt = np.linalg.svd(X, full_matrices=False)\n    except np.linalg.LinAlgError:\n        # Handle cases where SVD might fail, though unlikely for real matrices\n        return [False, -1.0, np.linalg.norm(y)]\n\n    # Get machine epsilon for double precision\n    eps = np.finfo(np.float64).eps\n    \n    # Set the threshold for singular values\n    sigma_max = s[0] if s.size > 0 else 0\n    tau = max(m, p) * eps * sigma_max\n\n    # Determine numerical rank\n    rank = np.sum(s > tau)\n    is_normal_eq_applicable = (rank == p)\n\n    # 2. Compute the SVD-based least-squares solution beta_svd = X_plus @ y\n    # X_plus = V @ np.diag(s_inv) @ U.T\n    # beta_svd = V @ np.diag(s_inv) @ U.T @ y\n    s_inv = np.zeros_like(s)\n    s_inv[s > tau] = 1.0 / s[s > tau]\n    \n    # Efficient computation of beta_svd\n    uty = U.T @ y\n    beta_svd = Vt.T @ (s_inv * uty)\n\n    # 3. Compute normal equation solution if applicable\n    difference_norm = -1.0\n    if is_normal_eq_applicable:\n        try:\n            XTX = X.T @ X\n            XTX_inv = np.linalg.inv(XTX)\n            XTy = X.T @ y\n            beta_ne = XTX_inv @ XTy\n            difference_norm = np.linalg.norm(beta_svd - beta_ne)\n        except np.linalg.LinAlgError:\n            # If XTX is singular despite rank check (extreme ill-conditioning),\n            # consider normal equations as not applicable in practice.\n            is_normal_eq_applicable = False\n            difference_norm = -1.0\n            # This logic branch correction ensures that if np.linalg.inv fails,\n            # we correctly report the normal equation method as inapplicable.\n            # The list 'result' for this case should be updated.\n            # The cleanest way is to just proceed with the `else` block logic.\n            pass # proceed to the default value\n\n    # In case of LinAlgError inside the if block, we need to ensure the values are correct.\n    # The boolean has to be re-set to False in the final list for consistency.\n    if difference_norm == -1.0:\n        is_normal_eq_applicable = False\n\n\n    # 4. Compute the residual norm for the SVD solution\n    residual_vec = X @ beta_svd - y\n    residual_norm = np.linalg.norm(residual_vec)\n    \n    return [is_normal_eq_applicable, difference_norm, residual_norm]\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    X1 = np.array([\n        [2, -1, 3],\n        [0, 4, 5],\n        [1, 2, -2],\n        [3, -5, 1],\n        [4, 0, -1]\n    ], dtype=np.float64)\n    y1 = np.array([1, 0, -1, 2, 3], dtype=np.float64)\n\n    X2 = np.array([\n        [1, 2.0001, 0],\n        [2, 4.0002, 1],\n        [3, 6.0003, 0],\n        [4, 8.0004, 1],\n        [5, 10.0005, 0]\n    ], dtype=np.float64)\n    y2 = np.array([1, 1, 2, 3, 5], dtype=np.float64)\n\n    X3 = np.array([\n        [1, 0, 0],\n        [0, 1, 1],\n        [1, 0, 0],\n        [0, 1, 1]\n    ], dtype=np.float64)\n    y3 = np.array([1, 2, 3, 4], dtype=np.float64)\n\n    X4 = np.array([\n        [3, 0, 1],\n        [2, -1, 0],\n        [1, 2, 4]\n    ], dtype=np.float64)\n    y4 = np.array([0, 1, 2], dtype=np.float64)\n\n    test_cases = [\n        (X1, y1),\n        (X2, y2),\n        (X3, y3),\n        (X4, y4),\n    ]\n\n    results = []\n    for X, y in test_cases:\n        result = solve_least_squares_case(X, y)\n        results.append(result)\n\n    # Format the output string as required.\n    # The default str() for a list adds spaces, e.g., '[True, 1.23, 4.56]'.\n    # The template `','.join(map(str, results))` joins these string representations with a comma.\n    # Enclosing this in brackets gives a string that looks like a list of lists.\n    # This precisely follows the structure provided in the skeleton code.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}