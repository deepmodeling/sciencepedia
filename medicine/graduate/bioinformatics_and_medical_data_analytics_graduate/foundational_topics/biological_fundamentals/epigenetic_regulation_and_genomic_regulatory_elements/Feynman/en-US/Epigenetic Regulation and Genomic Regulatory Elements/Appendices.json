{
    "hands_on_practices": [
        {
            "introduction": "The regulation of gene expression begins with the sequence-specific binding of transcription factors to DNA. This exercise introduces the Position Weight Matrix (PWM), a cornerstone model in bioinformatics for representing these binding motifs. By deriving and applying a log-odds scoring system, you will practice the fundamental statistical principles used to distinguish meaningful biological signals from random background sequence, a critical skill for annotating regulatory elements across the genome .",
            "id": "4560143",
            "problem": "A candidate transcription factor binding site motif implicated in insulator function within epigenetic regulation of chromatin architecture is modeled from high-confidence peaks derived from Chromatin Immunoprecipitation followed by sequencing (ChIP-seq). The motif is represented by a Position Weight Matrix (PWM), formally the matrix of position-specific multinomial parameters $P=\\{p_{i,b}\\}$ of length $L$ over nucleotides $b \\in \\{\\mathrm{A},\\mathrm{C},\\mathrm{G},\\mathrm{T}\\}$, where each position $i \\in \\{1,\\dots,L\\}$ satisfies $\\sum_{b} p_{i,b} = 1$ and $p_{i,b} \\ge 0$. The genomic background is modeled as an independent and identically distributed multinomial with probabilities $q=\\{q_{\\mathrm{A}}, q_{\\mathrm{C}}, q_{\\mathrm{G}}, q_{\\mathrm{T}}\\}$ satisfying $\\sum_{b} q_{b} = 1$.\n\nFrom first principles, define the Position Weight Matrix (PWM) and the positional information content under a general background as the Kullback-Leibler divergence (KLD) between the motif position distribution and the background. Then, starting from the likelihood of a sequence under the PWM and the background models and invoking the standard position-independence assumption for transcription factor binding motifs, derive the log-likelihood ratio scoring function. Finally, compute the log-odds motif score for the following instance using the natural logarithm.\n\nUse the nucleotide order $(\\mathrm{A}, \\mathrm{C}, \\mathrm{G}, \\mathrm{T})$ for both the PWM and background. Let the motif length be $L=8$ and the PWM be\n$$\nP \\;=\\; \\begin{pmatrix}\n0.05 & 0.10 & 0.75 & 0.10 \\\\\n0.10 & 0.60 & 0.20 & 0.10 \\\\\n0.05 & 0.10 & 0.80 & 0.05 \\\\\n0.10 & 0.10 & 0.20 & 0.60 \\\\\n0.65 & 0.10 & 0.15 & 0.10 \\\\\n0.15 & 0.10 & 0.65 & 0.10 \\\\\n0.10 & 0.70 & 0.10 & 0.10 \\\\\n0.55 & 0.15 & 0.15 & 0.15\n\\end{pmatrix}.\n$$\nLet the genomic background be\n$$\nq \\;=\\; (0.30,\\, 0.20,\\, 0.22,\\, 0.28).\n$$\nConsider the length-$8$ sequence\n$$\n\\mathbf{b} \\;=\\; (\\mathrm{G},\\, \\mathrm{C},\\, \\mathrm{G},\\, \\mathrm{T},\\, \\mathrm{A},\\, \\mathrm{G},\\, \\mathrm{C},\\, \\mathrm{A}).\n$$\n\nCompute the natural-logarithm log-odds motif score of $\\mathbf{b}$ under $P$ relative to $q$, expressed in nats, and round your answer to $4$ significant figures. Your final reported value must be a single real number.",
            "solution": "The problem requires a three-part response: a theoretical definition of the Position Weight Matrix (PWM) and positional information content, a derivation of the log-likelihood ratio scoring function, and a specific calculation for a given DNA sequence, PWM, and background model.\n\n### Problem Validation\n\nFirst, the validity of the problem statement is assessed.\n\n1.  **Extract Givens**:\n    *   A Position Weight Matrix (PWM) is a matrix of position-specific multinomial parameters $P=\\{p_{i,b}\\}$ for a motif of length $L$ over nucleotides $b \\in \\{\\mathrm{A},\\mathrm{C},\\mathrm{G},\\mathrm{T}\\}$.\n    *   For each position $i \\in \\{1,\\dots,L\\}$, the probabilities sum to unity: $\\sum_{b} p_{i,b} = 1$ and are non-negative: $p_{i,b} \\ge 0$.\n    *   The genomic background is an independent and identically distributed (i.i.d.) multinomial model with probabilities $q=\\{q_{\\mathrm{A}}, q_{\\mathrm{C}}, q_{\\mathrm{G}}, q_{\\mathrm{T}}\\}$ where $\\sum_{b} q_{b} = 1$.\n    *   The nucleotide order is specified as $(\\mathrm{A}, \\mathrm{C}, \\mathrm{G}, \\mathrm{T})$.\n    *   The motif length is $L=8$.\n    *   The PWM is given as:\n        $$\n        P \\;=\\; \\begin{pmatrix}\n        0.05 & 0.10 & 0.75 & 0.10 \\\\\n        0.10 & 0.60 & 0.20 & 0.10 \\\\\n        0.05 & 0.10 & 0.80 & 0.05 \\\\\n        0.10 & 0.10 & 0.20 & 0.60 \\\\\n        0.65 & 0.10 & 0.15 & 0.10 \\\\\n        0.15 & 0.10 & 0.65 & 0.10 \\\\\n        0.10 & 0.70 & 0.10 & 0.10 \\\\\n        0.55 & 0.15 & 0.15 & 0.15\n        \\end{pmatrix}.\n        $$\n    *   The background nucleotide probabilities are $q = (0.30, 0.20, 0.22, 0.28)$.\n    *   The specific DNA sequence to be scored is $\\mathbf{b} = (\\mathrm{G}, \\mathrm{C}, \\mathrm{G}, \\mathrm{T}, \\mathrm{A}, \\mathrm{G}, \\mathrm{C}, \\mathrm{A})$.\n    *   The task is to compute the natural-logarithm log-odds motif score, rounded to $4$ significant figures.\n\n2.  **Validate Using Extracted Givens**:\n    *   **Scientific and Factual Soundness**: The problem is based on cornerstone concepts in bioinformatics for modeling transcription factor binding sites, including PWMs, background models, and log-odds scoring. These are standard and well-established methods.\n    *   **Completeness and Consistency**: The problem is self-contained. The PWM matrix rows correctly sum to $1$. For instance, for row $1$: $0.05 + 0.10 + 0.75 + 0.10 = 1.00$. This holds for all $8$ rows. The background probabilities also sum to $1$: $0.30 + 0.20 + 0.22 + 0.28 = 1.00$. The dimensions of the PWM ($8 \\times 4$) are consistent with the motif length ($L=8$) and the alphabet size ($4$ nucleotides). All data required for the calculation are provided.\n    *   **Well-Posedness**: The problem asks for a calculation based on well-defined mathematical formulas. A unique, stable, and meaningful solution exists.\n\n3.  **Verdict**: The problem is valid.\n\n### Part 1: Definitions from First Principles\n\nA **Position Weight Matrix (PWM)**, denoted as $P$, is a probabilistic model representing a sequence motif of a fixed length $L$. It is an $L \\times k$ matrix, where $k$ is the size of the alphabet (here, $k=4$ for DNA nucleotides $\\{\\mathrm{A},\\mathrm{C},\\mathrm{G},\\mathrm{T}\\}$). Each element $p_{i,b}$ of the matrix specifies the probability of observing nucleotide $b$ at position $i$ of the motif. This model assumes independence between positions. For each position $i \\in \\{1, \\dots, L\\}$, the set of probabilities $\\{p_{i,b}\\}_{b \\in \\{\\mathrm{A,C,G,T}\\}}$ forms a multinomial distribution, so $\\sum_{b} p_{i,b} = 1$ and $p_{i,b} \\ge 0$.\n\nThe **positional information content** at position $i$, denoted $I_i$, quantifies the conservation of that position. It measures the reduction in uncertainty about the nucleotide at position $i$ relative to the background distribution $q$. This is formally defined as the Kullback-Leibler divergence (KLD) from the background distribution $q$ to the motif's positional distribution $p_{i,\\cdot} = (p_{i,\\mathrm{A}}, p_{i,\\mathrm{C}}, p_{i,\\mathrm{G}}, p_{i,\\mathrm{T}})$. Using the natural logarithm, the information content in units of nats is:\n$$\nI_i = D_{\\mathrm{KL}}(p_{i,\\cdot} || q) = \\sum_{b \\in \\{\\mathrm{A,C,G,T}\\}} p_{i,b} \\ln\\left(\\frac{p_{i,b}}{q_b}\\right)\n$$\n\n### Part 2: Derivation of the Log-Likelihood Ratio Scoring Function\n\nTo distinguish true motif instances from background DNA, we formulate a hypothesis test. Let $\\mathbf{b} = (b_1, b_2, \\dots, b_L)$ be a DNA sequence of length $L$.\n\n*   **Hypothesis $M$ (Motif model)**: The sequence $\\mathbf{b}$ was generated by the motif model $P$.\n*   **Hypothesis $B$ (Background model)**: The sequence $\\mathbf{b}$ was generated by the background model $q$.\n\nUnder the assumption of positional independence inherent to the PWM model, the likelihood of observing sequence $\\mathbf{b}$ under hypothesis $M$ is the product of the probabilities of observing each base $b_i$ at its corresponding position $i$:\n$$\n\\mathbb{P}(\\mathbf{b} | M) = \\prod_{i=1}^{L} p_{i, b_i}\n$$\nUnder the i.i.d. background model, the likelihood of observing sequence $\\mathbf{b}$ under hypothesis $B$ is the product of the background probabilities for each base:\n$$\n\\mathbb{P}(\\mathbf{b} | B) = \\prod_{i=1}^{L} q_{b_i}\n$$\nThe **likelihood ratio** compares these two probabilities:\n$$\n\\frac{\\mathbb{P}(\\mathbf{b} | M)}{\\mathbb{P}(\\mathbf{b} | B)} = \\frac{\\prod_{i=1}^{L} p_{i, b_i}}{\\prod_{i=1}^{L} q_{b_i}} = \\prod_{i=1}^{L} \\frac{p_{i, b_i}}{q_{b_i}}\n$$\nFor numerical stability and additive properties, we take the logarithm of this ratio. This gives the **log-likelihood ratio** or **log-odds score**, $S(\\mathbf{b})$. Using the natural logarithm:\n$$\nS(\\mathbf{b}) = \\ln\\left( \\prod_{i=1}^{L} \\frac{p_{i, b_i}}{q_{b_i}} \\right) = \\sum_{i=1}^{L} \\ln\\left( \\frac{p_{i, b_i}}{q_{b_i}} \\right)\n$$\nThis function scores a sequence by summing the position-specific log-odds, where each term reflects how much more probable the observed nucleotide is under the motif model than the background model. This resulting sum is the log-odds motif score.\n\n### Part 3: Calculation for the Specific Instance\n\nWe are given the sequence $\\mathbf{b} = (\\mathrm{G}, \\mathrm{C}, \\mathrm{G}, \\mathrm{T}, \\mathrm{A}, \\mathrm{G}, \\mathrm{C}, \\mathrm{A})$. The nucleotide order for the PWM columns and background vector is $(\\mathrm{A}, \\mathrm{C}, \\mathrm{G}, \\mathrm{T})$. The background probabilities are $q = (q_{\\mathrm{A}}, q_{\\mathrm{C}}, q_{\\mathrm{G}}, q_{\\mathrm{T}}) = (0.30, 0.20, 0.22, 0.28)$.\n\nThe score $S(\\mathbf{b})$ is calculated by summing the log-odds for each position $i=1, \\dots, 8$:\n$$\nS(\\mathbf{b}) = \\sum_{i=1}^{8} \\ln\\left( \\frac{p_{i, b_i}}{q_{b_i}} \\right)\n$$\nLet's compute each term:\n*   Position $1$: base $b_1 = \\mathrm{G}$. $p_{1,\\mathrm{G}} = 0.75$, $q_{\\mathrm{G}} = 0.22$. Score = $\\ln(0.75 / 0.22)$.\n*   Position $2$: base $b_2 = \\mathrm{C}$. $p_{2,\\mathrm{C}} = 0.60$, $q_{\\mathrm{C}} = 0.20$. Score = $\\ln(0.60 / 0.20) = \\ln(3)$.\n*   Position $3$: base $b_3 = \\mathrm{G}$. $p_{3,\\mathrm{G}} = 0.80$, $q_{\\mathrm{G}} = 0.22$. Score = $\\ln(0.80 / 0.22)$.\n*   Position $4$: base $b_4 = \\mathrm{T}$. $p_{4,\\mathrm{T}} = 0.60$, $q_{\\mathrm{T}} = 0.28$. Score = $\\ln(0.60 / 0.28)$.\n*   Position $5$: base $b_5 = \\mathrm{A}$. $p_{5,\\mathrm{A}} = 0.65$, $q_{\\mathrm{A}} = 0.30$. Score = $\\ln(0.65 / 0.30)$.\n*   Position $6$: base $b_6 = \\mathrm{G}$. $p_{6,\\mathrm{G}} = 0.65$, $q_{\\mathrm{G}} = 0.22$. Score = $\\ln(0.65 / 0.22)$.\n*   Position $7$: base $b_7 = \\mathrm{C}$. $p_{7,\\mathrm{C}} = 0.70$, $q_{\\mathrm{C}} = 0.20$. Score = $\\ln(0.70 / 0.20) = \\ln(3.5)$.\n*   Position $8$: base $b_8 = \\mathrm{A}$. $p_{8,\\mathrm{A}} = 0.55$, $q_{\\mathrm{A}} = 0.30$. Score = $\\ln(0.55 / 0.30)$.\n\nSumming these values:\n$S(\\mathbf{b}) = \\ln(\\frac{0.75}{0.22}) + \\ln(\\frac{0.60}{0.20}) + \\ln(\\frac{0.80}{0.22}) + \\ln(\\frac{0.60}{0.28}) + \\ln(\\frac{0.65}{0.30}) + \\ln(\\frac{0.65}{0.22}) + \\ln(\\frac{0.70}{0.20}) + \\ln(\\frac{0.55}{0.30})$\n$S(\\mathbf{b}) \\approx 1.226402 + 1.098612 + 1.290944 + 0.762140 + 0.773190 + 1.083319 + 1.252763 + 0.606136$\n$S(\\mathbf{b}) \\approx 8.093506$\n\nThe problem requires the answer to be rounded to $4$ significant figures. The calculated value is $8.093506...$. The fifth significant digit is $5$, so we round up the fourth digit.\n$S(\\mathbf{b}) \\approx 8.094$.",
            "answer": "$$\n\\boxed{8.094}\n$$"
        },
        {
            "introduction": "While a DNA sequence motif indicates a potential regulatory site, its functional activity often depends on whether it is accessible to the cellular machinery. This practice simulates the analysis of ATAC-seq data, a powerful technique for mapping open chromatin regions genome-wide. You will implement a complete bioinformatic workflow, from identifying significant peaks of accessibility using a Poisson model to performing differential analysis with a Negative Binomial generalized linear model, learning how to derive functional insights from raw sequencing counts .",
            "id": "4560167",
            "problem": "A research group investigates open chromatin profiles using Assay for Transposase-Accessible Chromatin using sequencing (ATAC-seq) to infer epigenetic regulation at genomic regulatory elements. Under well-tested assumptions, an ATAC-seq read count within a fixed genomic window can be modeled as a realization of a Poisson process with background rate $\\lambda$, and differential accessibility across biological conditions can be modeled using a Negative Binomial generalized linear model (GLM) with a log link, arising from a Poisson-Gamma mixture. The Fraction of Reads in Peaks (FRiP) is defined as the ratio of reads falling in statistically supported peak windows to the total aligned reads for a sample. The Benjamini–Hochberg procedure provides a standard method to control the false discovery rate in multiple testing.\n\nStarting from these bases, design a single program that does the following for each provided test case:\n\n1. Peak calling task:\n   - For each genomic window with observed counts $k$ and background rate $\\lambda$, compute the one-sided upper-tail Poisson $p$-value $p = \\mathbb{P}(K \\ge k \\mid \\Lambda = \\lambda)$.\n   - Call a window a peak if its $p$-value is strictly less than a given cutoff $\\alpha$.\n   - Compute the FRiP as the ratio of the sum of observed counts in the called peak windows to the total aligned reads $N$ for that test case. Express FRiP as a decimal and round to $4$ decimal places.\n\n2. Differential accessibility task:\n   - For each region across samples, fit a Negative Binomial GLM with log link\n     $$\\log(\\mu_i) = \\log(s_i) + \\beta_0 + \\beta_{\\text{cond}} \\cdot x_i + \\beta_{\\text{batch}} \\cdot b_i,$$\n     where $\\mu_i$ is the expected count for sample $i$, $s_i$ is the size factor derived from the library size $L_i$ as $s_i = L_i / \\overline{L}$, $x_i \\in \\{0,1\\}$ encodes the biological condition, and $b_i \\in \\{0,1\\}$ encodes the batch. Use iteratively reweighted least squares (IRLS) to estimate the coefficients, with the Negative Binomial variance\n     $$\\mathrm{Var}(Y_i) = \\mu_i + \\alpha \\mu_i^2,$$\n     and with weights\n     $$w_i = \\frac{\\mu_i}{1 + \\alpha \\mu_i}.$$\n     Estimate a single pooled dispersion $\\alpha$ across regions by a method-of-moments on size-factor-normalized counts $y_i / s_i$, specifically\n     $$\\alpha = \\max\\left(\\epsilon, \\frac{1}{R} \\sum_{r=1}^{R} \\frac{\\widehat{v}_r - \\widehat{m}_r}{\\widehat{m}_r^2}\\right),$$\n     where $\\widehat{m}_r$ and $\\widehat{v}_r$ are the sample mean and sample variance for region $r$, and $\\epsilon$ is a small positive constant to ensure numerical stability. Perform a Wald test on $\\beta_{\\text{cond}}$ to obtain a two-sided $p$-value per region. Apply the Benjamini–Hochberg procedure to the set of $p$-values across regions and count the number of regions with adjusted $q$-values strictly less than the specified threshold.\n\n3. Output:\n   - For each test case, return a list containing three entries: the integer number of called peaks, the FRiP as a decimal rounded to $4$ decimals, and the integer number of differentially accessible regions after false discovery rate control. Aggregate the results across all test cases into a single list. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes its own list. For example, the output format must be similar to $[[a,b,c],[d,e,f],[g,h,i]]$, with no spaces.\n\nUse the following test suite. Each test case provides the peak-calling inputs $(k,\\lambda,\\alpha,N)$ and the differential accessibility inputs $(Y, x, b, L, q)$.\n\n- Test case $1$:\n  - Peak calling:\n    - Observed window counts $k = [12,3,0,10,25,2]$.\n    - Background rates $\\lambda = [5,2,1,3,10,1]$.\n    - Significance cutoff $\\alpha = 0.01$.\n    - Total aligned reads $N = 52$.\n  - Differential accessibility:\n    - Region-by-sample counts matrix $Y$ with regions as rows and samples as columns:\n      $$Y = \\begin{bmatrix}\n      30 & 20 & 60 & 40 \\\\\n      5 & 5 & 15 & 10 \\\\\n      10 & 12 & 11 & 9\n      \\end{bmatrix}.$$\n    - Condition vector $x = [0,0,1,1]$.\n    - Batch vector $b = [0,1,0,1]$.\n    - Library sizes $L = [1000000,800000,1100000,900000]$.\n    - False discovery rate threshold $q = 0.1$.\n\n- Test case $2$:\n  - Peak calling:\n    - Observed window counts $k = [5,4,6,3,3]$.\n    - Background rates $\\lambda = [4.5,4,5.5,3,2.9]$.\n    - Significance cutoff $\\alpha = 0.05$.\n    - Total aligned reads $N = 21$.\n  - Differential accessibility:\n    - Region-by-sample counts matrix:\n      $$Y = \\begin{bmatrix}\n      8 & 7 & 9 & 8 \\\\\n      2 & 1 & 6 & 5 \\\\\n      0 & 0 & 1 & 0\n      \\end{bmatrix}.$$\n    - Condition vector $x = [0,0,1,1]$.\n    - Batch vector $b = [0,1,0,1]$.\n    - Library sizes $L = [900000,900000,900000,900000]$.\n    - False discovery rate threshold $q = 0.1$.\n\n- Test case $3$:\n  - Peak calling:\n    - Observed window counts $k = [0,1,0,2,1,0,3]$.\n    - Background rates $\\lambda = [0.5,0.8,0.5,1.8,1,0.5,2]$.\n    - Significance cutoff $\\alpha = 0.1$.\n    - Total aligned reads $N = 7$.\n  - Differential accessibility:\n    - Region-by-sample counts matrix:\n      $$Y = \\begin{bmatrix}\n      10 & 50 & 12 & 55 \\\\\n      20 & 40 & 21 & 42 \\\\\n      5 & 10 & 5 & 10\n      \\end{bmatrix}.$$\n    - Condition vector $x = [0,0,1,1]$.\n    - Batch vector $b = [0,1,0,1]$.\n    - Library sizes $L = [900000,900000,900000,900000]$.\n    - False discovery rate threshold $q = 0.1$.\n\nImplementation constraints:\n- Use the one-sided upper-tail Poisson distribution for peak calling, and do not merge adjacent windows.\n- Use iteratively reweighted least squares for the Negative Binomial GLM with a single pooled dispersion estimated by method-of-moments on size-factor-normalized counts, and include the offset $\\log(s_i)$ derived from library sizes.\n- Use the Benjamini–Hochberg procedure to adjust $p$-values across regions within each test case.\n- Use $\\epsilon = 10^{-8}$ for numerical stability where needed.\n- Your program must be self-contained, require no input, and produce only the single-line output in the specified format. FRiP must be expressed as a decimal (not a percentage) and rounded to $4$ decimal places. Angles are not involved, and no physical units beyond counts are required.",
            "solution": "The problem is assessed to be valid. It is scientifically grounded in established principles of computational biology and statistics, well-posed with a clear and complete set of instructions, and uses objective, formal language. The provided data are consistent and sufficient to produce a unique, meaningful solution.\n\nThe solution is partitioned into two primary computational tasks: (1) peak calling from ATAC-seq data modeled by a Poisson process, and (2) identifying differentially accessible genomic regions using a Negative Binomial Generalized Linear Model (GLM).\n\n### Part 1: Peak Calling\n\nThis task identifies genomic windows with a significantly higher read count than expected from a background process.\n\n**Principle:** The number of reads $K$ in a genomic window is modeled as a Poisson random variable with a rate parameter $\\lambda$ representing the background signal. A significantly enriched window, or \"peak,\" is one where the observed count $k$ is highly unlikely to have occurred under this background model.\n\n**Methodology:**\n1.  **Hypothesis Test:** For each window with observed count $k$ and background rate $\\lambda$, we test the null hypothesis $H_0$ that the count is generated from a Poisson distribution with rate $\\lambda$. The alternative hypothesis is that the true rate is greater than $\\lambda$.\n2.  **p-Value Calculation:** The p-value is the probability of observing a count of at least $k$ under the null hypothesis. This corresponds to the one-sided upper-tail probability of the Poisson distribution:\n    $$p = \\mathbb{P}(K \\ge k | \\Lambda = \\lambda) = \\sum_{j=k}^{\\infty} \\frac{e^{-\\lambda}\\lambda^j}{j!}$$\n    For computational purposes, this is more efficiently and accurately calculated using the survival function (SF), which is $1$ minus the cumulative distribution function (CDF): $p = 1 - \\text{CDF}(k-1; \\lambda)$.\n3.  **Peak Identification:** A window is called a peak if its calculated $p$-value is strictly less than a predefined significance cutoff $\\alpha$, i.e., $p < \\alpha$.\n4.  **Fraction of Reads in Peaks (FRiP):** This quality metric is the ratio of the total number of reads falling within the called peak windows to the total number of aligned reads $N$ for the sample.\n    $$\\text{FRiP} = \\frac{\\sum_{i \\in \\text{peaks}} k_i}{N}$$\n    The result is rounded to $4$ decimal places as specified.\n\n### Part 2: Differential Accessibility Analysis\n\nThis task identifies genomic regions whose accessibility, measured by read counts, changes significantly between biological conditions, while controlling for batch effects and differences in sequencing depth.\n\n**Principle:** Read counts are modeled using a Negative Binomial (NB) distribution, which accommodates the overdispersion (variance greater than the mean) typically observed in sequencing data. A GLM framework allows us to model the expected count as a function of experimental variables.\n\n**Methodology:**\n1.  **Model Specification:** The expected count $\\mu_i$ for sample $i$ in a given region is modeled with a log link function:\n    $$\\log(\\mu_i) = \\log(s_i) + \\beta_0 + \\beta_{\\text{cond}} \\cdot x_i + \\beta_{\\text{batch}} \\cdot b_i$$\n    -   $\\log(s_i)$ is an offset term to account for library size differences. The size factor $s_i$ is calculated as $s_i = L_i / \\overline{L}$, where $L_i$ is the library size (total reads) for sample $i$ and $\\overline{L}$ is the mean library size across all samples.\n    -   $\\beta_0$ is the intercept, representing the baseline log-expected count.\n    -   $x_i \\in \\{0, 1\\}$ is an indicator variable for the biological condition. The coefficient $\\beta_{\\text{cond}}$ represents the log-fold change in accessibility between conditions.\n    -   $b_i \\in \\{0, 1\\}$ is an indicator for the batch. The coefficient $\\beta_{\\text{batch}}$ accounts for systematic variation due to batch effects.\n\n2.  **Parameter Estimation:**\n    -   **Dispersion Parameter ($\\alpha$):** The NB variance is given by $\\mathrm{Var}(Y_i) = \\mu_i + \\alpha \\mu_i^2$. A single, pooled dispersion parameter $\\alpha$ is estimated across all regions using the method-of-moments on size-factor-normalized counts ($y_{ir}/s_i$ for region $r$ and sample $i$). The formula is:\n        $$\\alpha = \\max\\left(\\epsilon, \\frac{1}{R} \\sum_{r=1}^{R} \\frac{\\widehat{v}_r - \\widehat{m}_r}{\\widehat{m}_r^2}\\right)$$\n        where $\\widehat{m}_r$ and $\\widehat{v}_r$ are the sample mean and variance of the normalized counts for region $r$, $R$ is the number of regions, and $\\epsilon=10^{-8}$ ensures positivity.\n    -   **GLM Coefficients ($\\boldsymbol{\\beta}$):** The coefficients $\\boldsymbol{\\beta} = [\\beta_0, \\beta_{\\text{cond}}, \\beta_{\\text{batch}}]^T$ are estimated for each region using Iteratively Reweighted Least Squares (IRLS). This algorithm iteratively solves a weighted least squares problem until the coefficient estimates converge. The update step at iteration $(t+1)$ is:\n        $$\\boldsymbol{\\beta}^{(t+1)} = (\\mathbf{X}^T \\mathbf{W}^{(t)} \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{W}^{(t)} \\mathbf{z}'^{(t)}$$\n        where:\n        -   $\\mathbf{X}$ is the design matrix, with columns for the intercept, condition ($x_i$), and batch ($b_i$).\n        -   $\\mathbf{W}^{(t)}$ is a diagonal matrix of weights, with $w_i = \\mu_i^{(t)} / (1 + \\alpha (\\mu_i^{(t)}))$, calculated using the means from the previous iteration.\n        -   $\\mathbf{z}'^{(t)}$ is the adjusted response vector, with elements $z'_i = (\\eta_i^{(t)} - \\log(s_i)) + (y_i - \\mu_i^{(t)})/\\mu_i^{(t)}$, where $\\eta^{(t)}$ is the linear predictor.\n\n3.  **Hypothesis Testing:**\n    -   To test for differential accessibility, we perform a Wald test on the coefficient of interest, $\\beta_{\\text{cond}}$. The null hypothesis is $H_0: \\beta_{\\text{cond}} = 0$, meaning no change between conditions.\n    -   The Wald test statistic $Z$ is calculated as the ratio of the estimated coefficient to its standard error:\n        $$Z = \\frac{\\hat{\\beta}_{\\text{cond}}}{\\text{SE}(\\hat{\\beta}_{\\text{cond}})}$$\n    -   The standard error is obtained from the diagonal of the coefficient covariance matrix, which is estimated as $\\text{Cov}(\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{X}^T\\hat{\\mathbf{W}}\\mathbf{X})^{-1}$, where $\\hat{\\mathbf{W}}$ uses the final converged mean estimates.\n    -   The two-sided $p$-value is computed from the standard normal distribution: $p = 2 \\cdot \\mathbb{P}(N(0,1) \\ge |Z|)$.\n\n4.  **Multiple Testing Correction:**\n    -   Since a test is performed for every region, we must correct for multiple comparisons to control the False Discovery Rate (FDR). The Benjamini-Hochberg (BH) procedure is applied to the set of $p$-values.\n    -   The BH procedure converts the $p$-values into $q$-values (adjusted $p$-values). A region is declared differentially accessible if its $q$-value is strictly less than the specified FDR threshold $q$.\n\nThe described algorithms are implemented in the final program to process each test case and generate the required output.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats\n\ndef solve():\n    \"\"\"\n    Solves the bioinformatics problem for a suite of test cases.\n    The problem involves two main tasks:\n    1. Peak calling based on a Poisson model.\n    2. Differential accessibility analysis using a Negative Binomial GLM.\n    \"\"\"\n    \n    test_cases = [\n        {\n            \"peak_calling\": {\n                \"k\": np.array([12, 3, 0, 10, 25, 2]),\n                \"lambda\": np.array([5, 2, 1, 3, 10, 1]),\n                \"alpha\": 0.01,\n                \"N\": 52\n            },\n            \"diff_access\": {\n                \"Y\": np.array([[30, 20, 60, 40], [5, 5, 15, 10], [10, 12, 11, 9]]),\n                \"x\": np.array([0, 0, 1, 1]),\n                \"b\": np.array([0, 1, 0, 1]),\n                \"L\": np.array([1000000, 800000, 1100000, 900000]),\n                \"q\": 0.1\n            }\n        },\n        {\n            \"peak_calling\": {\n                \"k\": np.array([5, 4, 6, 3, 3]),\n                \"lambda\": np.array([4.5, 4, 5.5, 3, 2.9]),\n                \"alpha\": 0.05,\n                \"N\": 21\n            },\n            \"diff_access\": {\n                \"Y\": np.array([[8, 7, 9, 8], [2, 1, 6, 5], [0, 0, 1, 0]]),\n                \"x\": np.array([0, 0, 1, 1]),\n                \"b\": np.array([0, 1, 0, 1]),\n                \"L\": np.array([900000, 900000, 900000, 900000]),\n                \"q\": 0.1\n            }\n        },\n        {\n            \"peak_calling\": {\n                \"k\": np.array([0, 1, 0, 2, 1, 0, 3]),\n                \"lambda\": np.array([0.5, 0.8, 0.5, 1.8, 1, 0.5, 2]),\n                \"alpha\": 0.1,\n                \"N\": 7\n            },\n            \"diff_access\": {\n                \"Y\": np.array([[10, 50, 12, 55], [20, 40, 21, 42], [5, 10, 5, 10]]),\n                \"x\": np.array([0, 0, 1, 1]),\n                \"b\": np.array([0, 1, 0, 1]),\n                \"L\": np.array([900000, 900000, 900000, 900000]),\n                \"q\": 0.1\n            }\n        }\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        # --- Task 1: Peak Calling ---\n        pc_data = case[\"peak_calling\"]\n        k, lam, alpha_peak, N = pc_data[\"k\"], pc_data[\"lambda\"], pc_data[\"alpha\"], pc_data[\"N\"]\n        \n        # p-value is P(K >= k), computed using survival function 1 - CDF(k-1)\n        p_values_peak = stats.poisson.sf(k - 1, lam)\n        is_peak = p_values_peak < alpha_peak\n        \n        num_peaks = np.sum(is_peak)\n        reads_in_peaks = np.sum(k[is_peak])\n        \n        frip = 0.0\n        if N > 0:\n            frip = reads_in_peaks / N\n        frip_rounded = round(frip, 4)\n\n        # --- Task 2: Differential Accessibility ---\n        da_data = case[\"diff_access\"]\n        Y, x, b, L, q_thresh = da_data[\"Y\"], da_data[\"x\"], da_data[\"b\"], da_data[\"L\"], da_data[\"q\"]\n        \n        epsilon = 1e-8\n        num_regions, num_samples = Y.shape\n\n        # Calculate size factors\n        mean_L = np.mean(L)\n        s = L / mean_L if mean_L > 0 else np.ones_like(L)\n\n        # Estimate pooled dispersion alpha\n        normalized_counts = Y / s\n        \n        disp_terms = []\n        for r in range(num_regions):\n            norm_counts_r = normalized_counts[r, :]\n            m_r = np.mean(norm_counts_r)\n            if m_r > epsilon:\n                v_r = np.var(norm_counts_r, ddof=1)\n                disp_terms.append((v_r - m_r) / (m_r**2))\n        \n        alpha_disp = np.mean(disp_terms) if disp_terms else 0\n        alpha_disp = max(epsilon, alpha_disp)\n        \n        # Design matrix\n        X = np.c_[np.ones(num_samples), x, b]\n        p_values_da = []\n\n        # Per-region GLM fit using IRLS\n        for r in range(num_regions):\n            y_r = Y[r, :]\n            \n            # IRLS\n            beta = np.zeros(X.shape[1])\n            num_iter = 25\n            \n            for _ in range(num_iter):\n                eta = X @ beta + np.log(s)\n                mu = np.exp(eta)\n                \n                # numerical stability\n                mu = np.clip(mu, epsilon, 1e10)\n                \n                weights = mu / (1 + alpha_disp * mu)\n                W = np.diag(weights)\n                \n                z_prime = (eta - np.log(s)) + (y_r - mu) / mu\n                \n                try:\n                    # Solve beta = (X^T W X)^-1 X^T W z'\n                    XTWX = X.T @ W @ X\n                    XTWz = X.T @ W @ z_prime\n                    beta = np.linalg.solve(XTWX, XTWz)\n                except np.linalg.LinAlgError:\n                    beta = np.full(X.shape[1], np.nan)\n                    break\n            \n            # Wald test for beta_cond (second coefficient)\n            if np.isnan(beta).any():\n                p_values_da.append(1.0)\n                continue\n\n            try:\n                # Recalculate W for final covariance\n                eta = X @ beta + np.log(s)\n                mu = np.exp(eta)\n                mu = np.clip(mu, epsilon, 1e10)\n                weights = mu / (1 + alpha_disp * mu)\n                W = np.diag(weights)\n\n                cov_beta = np.linalg.inv(X.T @ W @ X)\n                se_beta_cond = np.sqrt(cov_beta[1, 1])\n                \n                if se_beta_cond < epsilon:\n                    p_values_da.append(1.0)\n                else:\n                    wald_stat = beta[1] / se_beta_cond\n                    p_val = 2 * stats.norm.sf(np.abs(wald_stat))\n                    p_values_da.append(p_val)\n            except (np.linalg.LinAlgError, ValueError):\n                p_values_da.append(1.0)\n\n        p_values_da = np.array(p_values_da)\n\n        # Benjamini-Hochberg procedure\n        num_diff_regions = 0\n        if len(p_values_da) > 0:\n            sorted_indices = np.argsort(p_values_da)\n            sorted_pvals = p_values_da[sorted_indices]\n            \n            ranks = np.arange(1, num_regions + 1)\n            q_vals_sorted = sorted_pvals * num_regions / ranks\n            \n            # Enforce monotonicity\n            q_vals_sorted = np.minimum.accumulate(q_vals_sorted[::-1])[::-1]\n            q_vals_sorted = np.minimum(q_vals_sorted, 1.0)\n\n            # Map back to original order\n            q_vals = np.empty_like(p_values_da)\n            q_vals[sorted_indices] = q_vals_sorted\n            \n            num_diff_regions = np.sum(q_vals < q_thresh)\n\n        all_results.append([num_peaks, frip_rounded, num_diff_regions])\n    \n    # Format output as specified\n    output_str = str(all_results).replace(\" \", \"\")\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "Genomic regulatory elements like enhancers can influence genes over vast linear distances by looping through three-dimensional space. This practice delves into the analysis of Hi-C data, which maps these long-range interactions, by focusing on a critical preprocessing step: bias correction. You will implement the Iterative Correction and Eigenvector decomposition (ICE) algorithm to normalize a raw Hi-C contact matrix, gaining hands-on experience with the matrix balancing techniques essential for accurately studying 3D genome architecture .",
            "id": "4560081",
            "problem": "Implement an algorithm to perform Iterative Correction and Eigenvector decomposition (ICE) normalization of a High-throughput chromosome conformation capture (Hi-C) contact matrix via Iterative Proportional Fitting (IPF), targeting equalized row and column sums over a subset of valid (non-low-coverage) bins. Your program must compute a multiplicative bias vector that, when used to symmetrically scale the matrix, yields approximately constant row sums among valid bins, while excluding low-coverage bins from the balancing constraints.\n\nFundamental base and core definitions to be used:\n- A Hi-C contact matrix is a square, symmetric, nonnegative matrix $M \\in \\mathbb{R}_{\\ge 0}^{n \\times n}$ where entry $M_{ij}$ represents the number of contacts between genomic bins $i$ and $j$. The marginal (row or column) total of a bin is the sum of its contacts with all bins.\n- The ICE normalization problem seeks a diagonal scaling $D = \\mathrm{diag}(w)$ with $w \\in \\mathbb{R}_{\\ge 0}^{n}$ such that the scaled matrix $B = D M D$ has uniform row sums over a specified set of valid bins. The uniform row sum target is a positive constant $t \\in \\mathbb{R}_{>0}$.\n- Low-coverage bins are those whose raw row sum is below a threshold $\\tau \\in \\mathbb{R}_{>0}$ or equal to $0$, and these should be excluded from the equal-sum constraints. Let $S \\subset \\{1,\\dots,n\\}$ denote the index set of valid bins that pass the low-coverage filter.\n- For any vector $x \\in \\mathbb{R}^{n}$, define the row sum of the scaled matrix $B$ for bin $i$ as $r_i(B) = \\sum_{j=1}^{n} B_{ij}$. For symmetric scaling $B = D M D$, this satisfies $r_i(B) = w_i \\cdot (M w)_i$, where $(M w)_i$ denotes the $i$-th entry of the matrix-vector product $M w$.\n\nTask to implement:\n- Given a square symmetric nonnegative matrix $M$, a target row sum $t$, a low-coverage threshold $\\tau$, a tolerance $\\delta$, and a maximum iteration cap $K$, implement an IPF-style multiplicative update to compute a weight vector $w$ with the following goals:\n  - Valid bin set: define $S = \\{ i \\in \\{1,\\dots,n\\} \\mid \\sum_{j=1}^{n} M_{ij} >= \\tau \\ \\text{and} \\ \\sum_{j=1}^{n} M_{ij} > 0 \\}$.\n  - Initialization: set $w_i^{(0)} = 1$ for $i \\in S$ and $w_i^{(0)} = 0$ for $i \\notin S$.\n  - Symmetric IPF update for $k = 0,1,2,\\dots$:\n    - Compute $r_i^{(k)} = w_i^{(k)} \\cdot (M w^{(k)})_i$ for all $i$.\n    - For $i \\in S$ with $r_i^{(k)} > 0$, update $w_i^{(k+1)} = w_i^{(k)} \\cdot \\sqrt{\\frac{t}{r_i^{(k)}}}$. For $i \\notin S$, keep $w_i^{(k+1)} = 0$.\n  - Convergence criterion: define the maximum relative deviation among valid bins\n    $$\\varepsilon^{(k)} = \\max_{i \\in S} \\left| \\frac{r_i^{(k)}}{t} - 1 \\right|.$$\n    Stop when $\\varepsilon^{(k)} = \\delta$ or when $k$ reaches $K$.\n  - Low-coverage handling: bins $i \\notin S$ are excluded from the constraints and kept at $w_i = 0$; their corresponding rows and columns in the scaled matrix $B$ are effectively zeroed by the diagonal scaling.\n\nProgram requirements:\n- Input: hard-code the test suite specified below into your program. Do not read from files or standard input.\n- Output: for each test case, compute the tuple consisting of a boolean convergence flag (true if $\\varepsilon^{(k)} = \\delta$ before exceeding the iteration limit), the final maximum relative deviation $\\varepsilon^{(\\ast)}$ as a floating-point value, and the number of iterations performed as an integer. Aggregate all test case results into a single list and print exactly one line in the format of a single bracketed list with comma-separated entries and no spaces. Each entry must itself be a list of the form $[\\text{boolean},\\text{float},\\text{integer}]$.\n\nTest suite to implement and evaluate:\n- Use $t = 1$, $\\delta = 10^{-8}$, and $K = 10^4$ unless otherwise noted. For each case define $\\tau$ as specified.\n- Case A (general well-conditioned matrix):\n  $$M = \\begin{bmatrix}\n  20  5  3  0  2 \\\\\n  5  15  4  1  0 \\\\\n  3  4  18  2  1 \\\\\n  0  1  2  12  3 \\\\\n  2  0  1  3  10\n  \\end{bmatrix}, \\quad \\tau = 10.$$\n- Case B (matrix with a zero row and column; one bin must be excluded):\n  $$M = \\begin{bmatrix}\n  8  2  0  0 \\\\\n  2  7  1  0 \\\\\n  0  1  5  0 \\\\\n  0  0  0  0\n  \\end{bmatrix}, \\quad \\tau = 1.$$\n- Case C (sparse matrix with multiple low-coverage bins, thresholded out):\n  $$M = \\begin{bmatrix}\n  5  1  0  0  0  0 \\\\\n  1  4  1  0  0  0 \\\\\n  0  1  0  0  0  0 \\\\\n  0  0  0  3  1  0 \\\\\n  0  0  0  1  2  1 \\\\\n  0  0  0  0  1  0\n  \\end{bmatrix}, \\quad \\tau = 3.$$\n- Case D (already equal-sum structure; rapid convergence expected):\n  $$M = \\begin{bmatrix}\n  1  1  1 \\\\\n  1  1  1 \\\\\n  1  1  1\n  \\end{bmatrix}, \\quad \\tau = 1.$$\n\nAngle units are not applicable. There are no physical units to report. All numerical outputs must be in base real numbers.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with no spaces. For example, if there are four cases, the output must look like\n  $$[ [\\text{boolean}_1,\\text{float}_1,\\text{integer}_1], [\\text{boolean}_2,\\text{float}_2,\\text{integer}_2], [\\text{boolean}_3,\\text{float}_3,\\text{integer}_3], [\\text{boolean}_4,\\text{float}_4,\\text{integer}_4] ]$$\n  but without spaces, that is,\n  $$[[\\text{boolean}_1,\\text{float}_1,\\text{integer}_1],[\\text{boolean}_2,\\text{float}_2,\\text{integer}_2],[\\text{boolean}_3,\\text{float}_3,\\text{integer}_3],[\\text{boolean}_4,\\text{float}_4,\\text{integer}_4]].$$",
            "solution": "The user requests the implementation of the Iterative Correction and Eigenvector decomposition (ICE) normalization algorithm for Hi-C contact matrices, based on Iterative Proportional Fitting (IPF).\n\n### Step 1: Problem Validation\n\n**1.1. Extracted Givens**\n- **Hi-C Matrix ($M$)**: A square, symmetric, non-negative matrix $M \\in \\mathbb{R}_{\\ge 0}^{n \\times n}$.\n- **Scaling**: A diagonal scaling matrix $D = \\mathrm{diag}(w)$, where $w \\in \\mathbb{R}_{\\ge 0}^{n}$ is a bias vector. The scaled matrix is $B = DMD$.\n- **Normalization Goal**: For a given target sum $t \\in \\mathbb{R}_{0}$, the row sums of the scaled matrix, $r_i(B) = \\sum_{j=1}^{n} B_{ij}$, should be approximately equal to $t$ for all \"valid\" bins.\n- **Valid Bins ($S$)**: The set of bins to be normalized is defined as $S = \\{ i \\mid \\sum_{j=1}^{n} M_{ij} \\ge \\tau \\ \\text{and} \\ \\sum_{j=1}^{n} M_{ij}  0 \\}$, where $\\tau \\in \\mathbb{R}_{0}$ is a low-coverage threshold. Bins not in $S$ are excluded.\n- **Initialization**: The weight vector is initialized as $w_i^{(0)} = 1$ for $i \\in S$ and $w_i^{(0)} = 0$ for $i \\notin S$.\n- **IPF Update Rule**: For iteration $k=0, 1, 2, \\dots$:\n    - Current row sums: $r_i^{(k)} = w_i^{(k)} \\cdot (M w^{(k)})_i$.\n    - Weight update: For $i \\in S$ and $r_i^{(k)}  0$, update $w_i^{(k+1)} = w_i^{(k)} \\cdot \\sqrt{\\frac{t}{r_i^{(k)}}}$. For $i \\notin S$, $w_i^{(k+1)} = 0$.\n- **Convergence Criterion**: The process stops when the maximum relative deviation $\\varepsilon^{(k)} = \\max_{i \\in S} \\left| \\frac{r_i^{(k)}}{t} - 1 \\right|$ is less than or equal to a tolerance $\\delta$.\n- **Termination**: The process also stops if the number of iterations $k$ reaches a maximum cap $K$.\n- **Output**: For each test case, a tuple containing a boolean convergence flag, the final deviation $\\varepsilon^{(\\ast)}$, and the number of iterations. The final output is a single-line string representation of a list of these results.\n- **Test Cases**: Four specific matrices $M$ and thresholds $\\tau$ are provided, with global parameters $t=1$, $\\delta=10^{-8}$, and $K=10^4$.\n\n**1.2. Validation against Criteria**\n- **Scientifically Grounded**: The problem describes the standard ICE normalization algorithm, a widely accepted and fundamental technique in the bioinformatics field for correcting biases in Hi-C data. Its mathematical basis in Iterative Proportional Fitting is sound.\n- **Well-Posed**: The problem is well-posed. It provides all necessary inputs ($M, t, \\tau, \\delta, K$), a clear iterative procedure, and unambiguous termination conditions (convergence or iteration limit). The exclusion of low-coverage bins is a standard practice to handle sparse or disconnected matrices, ensuring the algorithm's stability.\n- **Objective**: All definitions and requirements are stated in precise, quantitative mathematical terms, leaving no room for subjective interpretation.\n\n**1.3. Verdict**\nThe problem is valid. It is scientifically sound, well-posed, objective, and complete. I will proceed to provide a solution.\n\n### Step 2: Algorithmic Design and Explanation\n\nThe core of the solution is an iterative algorithm that refines a weight vector $w$ to balance the row/column sums of a Hi-C matrix $M$. The algorithm proceeds as follows:\n\n1.  **Initialization**:\n    - Given the input matrix $M$ and the low-coverage threshold $\\tau$, we first compute the raw row sums for each bin (genomic region). Since $M$ is symmetric, row sums are equal to column sums.\n    - We define a set of \"valid\" bins $S$ as those whose raw sum is greater than or equal to $\\tau$. The condition that the sum must also be greater than $0$ is automatically satisfied since $\\tau$ is defined as a positive real number.\n    - A weight vector $w$ of size $n$ is initialized. For each valid bin $i \\in S$, $w_i$ is set to $1$. For all other bins $i \\notin S$, $w_i$ is set to $0$. This ensures that low-coverage or empty bins are excluded from the normalization process and will have zero signal in the final balanced matrix.\n\n2.  **Iterative Correction**:\n    - The algorithm enters a loop that continues until convergence is reached or the maximum number of iterations, $K$, is exceeded. Let the current iteration be $k$.\n    - **Compute Scaled Row Sums**: In each iteration, we calculate the current row sums of the implicitly scaled matrix $B = DMD$. The row sum for bin $i$ is $r_i = \\sum_j D_{ii} M_{ij} D_{jj} = w_i \\sum_j M_{ij} w_j = w_i (Mw)_i$. This is efficiently computed for all bins using a matrix-vector product followed by an element-wise product.\n    - **Check for Convergence**: We assess convergence by calculating the maximum relative deviation, $\\varepsilon^{(k)}$, among all valid bins: $\\varepsilon^{(k)} = \\max_{i \\in S} \\left| \\frac{r_i^{(k)}}{t} - 1 \\right|$. If $\\varepsilon^{(k)} \\le \\delta$, the matrix is considered balanced. The algorithm terminates, reporting success, the final $\\varepsilon^{(k)}$, and the current iteration count $k$.\n    - **Update Weights**: If not converged, the weight vector is updated for the next iteration. The update rule $w_i^{(k+1)} = w_i^{(k)} \\cdot \\sqrt{t/r_i^{(k)}}$ is applied only to valid bins ($i \\in S$) that also have a non-zero current row sum ($r_i^{(k)}  0$). This multiplicative update is designed to drive $r_i$ towards the target $t$. Bins with $r_i^{(k)}=0$ are not updated, as this would involve division by zero. Their weights remain unchanged, effectively freezing them out of further balancing if they become disconnected from the system.\n\n3.  **Termination**:\n    - If the loop completes $K$ iterations without achieving convergence, the process stops.\n    - The final values of the convergence flag (now `False`), the last calculated deviation $\\varepsilon$, and the total number of iterations ($K$) are returned.\n\nThis procedure, based on symmetric IPF, is guaranteed to converge for irreducible non-negative matrices. By filtering out low-coverage bins, we improve the likelihood that the remaining submatrix is well-behaved, making the normalization robust in practice. The implementation will use `numpy` for efficient array and matrix operations.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the ICE normalization test suite and print results.\n    \"\"\"\n\n    def ice_normalize(M, t, tau, delta, K):\n        \"\"\"\n        Performs ICE normalization on a Hi-C matrix using Iterative Proportional Fitting.\n\n        Args:\n            M (list of lists): The input square symmetric matrix.\n            t (float): The target row sum.\n            tau (float): The low-coverage threshold.\n            delta (float): The convergence tolerance.\n            K (int): The maximum number of iterations.\n\n        Returns:\n            tuple: A tuple containing (converged, final_epsilon, iterations).\n                   - converged (bool): True if the algorithm converged.\n                   - final_epsilon (float): The final maximum relative deviation.\n                   - iterations (int): The number of iterations performed.\n        \"\"\"\n        mat = np.array(M, dtype=float)\n        n = mat.shape[0]\n\n        # Step 1: Identify valid bins based on raw row sums and threshold tau.\n        # Since tau  0, the condition raw_sums = tau implies raw_sums  0.\n        raw_sums = mat.sum(axis=1)\n        valid_mask = raw_sums = tau\n\n        # Handle the edge case where no bins are considered valid.\n        if not np.any(valid_mask):\n            return (True, 0.0, 0)\n\n        # Step 2: Initialize the weight vector w.\n        w = np.zeros(n, dtype=float)\n        w[valid_mask] = 1.0\n        \n        k = 0\n        epsilon = np.inf\n\n        # Step 3: Begin the iterative correction loop.\n        while k  K:\n            # Calculate the current row sums of the scaled matrix: r_i = w_i * (M*w)_i\n            r = w * (mat @ w)\n            \n            # Isolate the row sums of valid bins to calculate epsilon.\n            r_valid = r[valid_mask]\n            \n            # Calculate the maximum relative deviation for valid bins.\n            epsilon = np.max(np.abs(r_valid / t - 1.0))\n\n            # Check for convergence.\n            if epsilon = delta:\n                return (True, epsilon, k)\n\n            # Prepare for the weight update. Only update valid bins with r  0.\n            update_mask = (r  0)  valid_mask\n\n            # If no bins can be updated, the algorithm is stuck. Break the loop.\n            if not np.any(update_mask):\n                break\n\n            # Apply the multiplicative update rule.\n            w[update_mask] *= np.sqrt(t / r[update_mask])\n            \n            k += 1\n\n        # After the loop, k = K or we broke out early. Recalculate the final epsilon.\n        r = w * (mat @ w)\n        r_valid = r[valid_mask]\n        epsilon = np.max(np.abs(r_valid / t - 1.0))\n        \n        # Determine final convergence status.\n        converged = epsilon = delta\n\n        # Return the final state. k is the number of iterations performed.\n        return (converged, epsilon, k)\n\n    # Define the test suite.\n    # Global parameters: t = 1, delta = 1e-8, K = 10000\n    common_params = {\"t\": 1.0, \"delta\": 1e-8, \"K\": 10000}\n    \n    test_cases = [\n        # Case A: General well-conditioned matrix\n        {\n            \"M\": [\n                [20, 5, 3, 0, 2],\n                [5, 15, 4, 1, 0],\n                [3, 4, 18, 2, 1],\n                [0, 1, 2, 12, 3],\n                [2, 0, 1, 3, 10]\n            ],\n            \"tau\": 10.0\n        },\n        # Case B: Matrix with a zero row and column\n        {\n            \"M\": [\n                [8, 2, 0, 0],\n                [2, 7, 1, 0],\n                [0, 1, 5, 0],\n                [0, 0, 0, 0]\n            ],\n            \"tau\": 1.0\n        },\n        # Case C: Sparse matrix with multiple low-coverage bins\n        {\n            \"M\": [\n                [5, 1, 0, 0, 0, 0],\n                [1, 4, 1, 0, 0, 0],\n                [0, 1, 0, 0, 0, 0],\n                [0, 0, 0, 3, 1, 0],\n                [0, 0, 0, 1, 2, 1],\n                [0, 0, 0, 0, 1, 0]\n            ],\n            \"tau\": 3.0\n        },\n        # Case D: Already equal-sum structure\n        {\n            \"M\": [\n                [1, 1, 1],\n                [1, 1, 1],\n                [1, 1, 1]\n            ],\n            \"tau\": 1.0\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        # Unpack parameters and run the normalization.\n        result_tuple = ice_normalize(\n            M=case[\"M\"],\n            tau=case[\"tau\"],\n            **common_params\n        )\n        # Convert tuple to list for the final string representation.\n        results.append(list(result_tuple))\n    \n    # Format the output string exactly as specified, without any spaces.\n    final_output = str(results).replace(' ', '')\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}