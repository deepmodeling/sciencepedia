{
    "hands_on_practices": [
        {
            "introduction": "The quality of raw data is the bedrock of any sound scientific analysis. In genomics, tools like FastQC provide a suite of quality control (QC) metrics, but their ultimate meaning lies in their impact on downstream biological discovery. This exercise  challenges you to move beyond heuristic thresholds by building a quantitative model from first principles, directly linking Next-Generation Sequencing (NGS) quality metrics to the statistical power for single-nucleotide variant (SNV) detection. By modeling how factors like base quality scores ($Q$) and read duplication affect the effective coverage and error probabilities, you will gain a deeper understanding of how data quality quantitatively gates analytical sensitivity.",
            "id": "4551926",
            "problem": "You are given a task in the context of data quality assessment in biological and clinical data for Next-Generation Sequencing (NGS). You must write a complete program that, for each dataset, performs the following: compute a FastQC-style summary, identify Quality Control (QC) failures, and quantify the expected loss in downstream single-nucleotide variant (SNV) calling sensitivity attributable to the observed QC metrics. Your derivation and implementation must be based on first principles: the Phred quality definition and a binomial test for variant detection.\n\nUse the following foundations and definitions:\n- Phred quality score is defined by $Q = -10 \\log_{10}(e)$, where $e$ is the base-calling error probability. Hence $e = 10^{-Q/10}$.\n- Assume a diploid heterozygous variant with true alternate allele fraction $f = 0.5$. Let $n$ be the effective number of independent observations (effective coverage) at a locus. Assume independence of reads at the locus after accounting for duplication.\n- Define the null hypothesis $H_0$ (no real variant): the probability that an alternate base call equals a specific non-reference allele is $p_0 = e/3$ (assuming equal distribution of substitution errors among the three non-reference bases).\n- Define the alternative hypothesis $H_1$ (a true heterozygous variant): the probability that a read is observed as the specific alternate allele is $p_1 = f \\cdot (1 - e) + (1 - f) \\cdot (e/3)$.\n- Set a per-site false positive tail probability budget of $\\alpha = 10^{-6}$. Define the minimum integer threshold $t$ for a variant call at depth $n$ as the smallest $t$ such that $\\Pr[X \\ge t \\mid X \\sim \\mathrm{Binomial}(n, p_0)] \\le \\alpha$. If no such $t \\le n$ exists, sensitivity is defined to be $0$.\n- The sensitivity under $H_1$ (power) is $S = \\Pr[X \\ge t \\mid X \\sim \\mathrm{Binomial}(n, p_1)]$.\n\nDefine the effective coverage $n$ from raw coverage and QC metrics as follows. Let $C_{\\text{raw}}$ be raw nominal coverage. The effective coverage is\n$$\nn = \\left\\lfloor C_{\\text{raw}} \\cdot m \\cdot (1 - d) \\cdot (1 - a) \\cdot (1 - n_f) \\cdot (1 - o) \\right\\rfloor,\n$$\nwhere $m$ is mapping rate (decimal), $d$ is duplication rate (decimal), $a$ is adapter contamination fraction (decimal), $o$ is overrepresented sequences fraction (decimal), and $n_f$ is per-base ambiguous call fraction (decimal). The floor function $\\lfloor \\cdot \\rfloor$ returns the greatest integer less than or equal to its argument. All fractions must be provided as decimals, not percentages.\n\nCompute a FastQC-style module summary with pass/warn/fail per dataset for the following $6$ modules using these thresholds:\n- Per-base sequence quality using mean Phred quality $Q_m$: pass if $Q_m \\ge 28$, warn if $23 \\le Q_m < 28$, fail if $Q_m < 23$.\n- Per-sequence GC content deviation $|g - g_0|$ where $g$ is observed GC fraction and $g_0$ is expected GC fraction: pass if $|g - g_0| \\le 0.05$, warn if $0.05 < |g - g_0| \\le 0.10$, fail if $|g - g_0| > 0.10$.\n- Sequence duplication levels using duplication rate $d$: pass if $d \\le 0.20$, warn if $0.20 < d \\le 0.50$, fail if $d > 0.50$.\n- Adapter content $a$: pass if $a \\le 0.01$, warn if $0.01 < a \\le 0.05$, fail if $a > 0.05$.\n- Overrepresented sequences fraction $o$: pass if $o \\le 0.01$, warn if $0.01 < o \\le 0.10$, fail if $o > 0.10$.\n- Per-base $N$ content $n_f$: pass if $n_f \\le 0.01$, warn if $0.01 < n_f \\le 0.03$, fail if $n_f > 0.03$.\n\nMap pass/warn/fail to integer codes as follows: pass $\\to +1$, warn $\\to 0$, fail $\\to -1$. The module order must be fixed as:\n$[$Per-base sequence quality, GC deviation, Duplication, Adapter, Overrepresented sequences, $N$ content$]$.\n\nDefine sensitivity loss relative to a per-dataset baseline with ideal QC at the same raw coverage: for the baseline, set $Q_m^{\\mathrm{base}} = 35$, $m^{\\mathrm{base}} = 1$, $d^{\\mathrm{base}} = 0$, $a^{\\mathrm{base}} = 0$, $o^{\\mathrm{base}} = 0$, $n_f^{\\mathrm{base}} = 0$, and compute $n_{\\mathrm{base}} = \\lfloor C_{\\text{raw}} \\rfloor$. Compute $S_{\\mathrm{base}}$ using $e_{\\mathrm{base}} = 10^{-Q_m^{\\mathrm{base}}/10}$ and the same $\\alpha$ and $f$. The sensitivity loss is $L = S_{\\mathrm{base}} - S$, which must be reported as a decimal in $[0,1]$.\n\nInput for each dataset consists of the following parameters, all dimensionless and provided as decimals where applicable:\n- Read length $L$ (bases) [used only for reporting consistency, not in the computation],\n- Raw coverage $C_{\\text{raw}}$,\n- Mean Phred quality $Q_m$,\n- Mapping rate $m$,\n- Duplication rate $d$,\n- Adapter contamination fraction $a$,\n- Overrepresented sequences fraction $o$,\n- Per-base $N$ fraction $n_f$,\n- Observed GC fraction $g$,\n- Expected GC fraction $g_0$.\n\nUse the following test suite of datasets, each specified as a tuple $(L, C_{\\text{raw}}, Q_m, m, d, a, o, n_f, g, g_0)$ with all numeric values written as decimals:\n- Test case $1$: $(150, 30, 32, 0.97, 0.12, 0.008, 0.006, 0.004, 0.41, 0.41)$.\n- Test case $2$: $(100, 20, 25, 0.95, 0.35, 0.02, 0.03, 0.015, 0.36, 0.41)$.\n- Test case $3$: $(150, 40, 20, 0.70, 0.70, 0.10, 0.15, 0.04, 0.50, 0.41)$.\n- Test case $4$: $(75, 10, 30, 0.20, 0.50, 0.02, 0.01, 0.01, 0.41, 0.41)$.\n\nYour program must, for each test case, compute:\n- The vector of module status codes in the fixed order specified above,\n- The integer count of failed modules (the number of entries equal to $-1$),\n- The sensitivity loss $L$ as a floating-point number.\n\nFinal output format:\nYour program should produce a single line of output containing a list of length $4$, where each element corresponds to a test case and is itself a list of the form $[$status\\_codes, fail\\_count, loss$]$. The $status\\_codes$ must be a list of $6$ integers in the fixed module order. For example, the overall structure must look like\n$[[[s_{11},\\dots,s_{16}], f_1, \\ell_1], [[s_{21},\\dots,s_{26}], f_2, \\ell_2], [[s_{31},\\dots,s_{36}], f_3, \\ell_3], [[s_{41},\\dots,s_{46}], f_4, \\ell_4]]$,\nwhere $s_{ij} \\in \\{-1,0,1\\}$, $f_i$ is an integer, and $\\ell_i$ is a decimal number in $[0,1]$. Print the list exactly in this single-line format using commas to separate values and square brackets to denote lists.",
            "solution": "We construct a principled solution grounded in the definitions of Phred quality and hypothesis testing with the binomial distribution. The aim is to translate observable Quality Control (QC) metrics into an effective coverage model and then quantify the impact of imperfect QC on variant detection sensitivity for single-nucleotide variants (SNVs).\n\nFirst, we use the Phred quality definition. The Phred score $Q$ and the error probability $e$ obey $Q = -10 \\log_{10}(e)$, so $e = 10^{-Q/10}$. For a dataset with mean Phred quality $Q_m$, we model the per-base error probability as $e = 10^{-Q_m/10}$. This is a well-tested relationship in sequencing technologies.\n\nSecond, we compute effective coverage $n$ as a product of raw coverage and multiplicative penalties from mapping, duplication, adapter contamination, overrepresented sequences, and ambiguous bases. Let $C_{\\text{raw}}$ be the nominal coverage. The effective coverage is\n$$\nn = \\left\\lfloor C_{\\text{raw}} \\cdot m \\cdot (1 - d) \\cdot (1 - a) \\cdot (1 - n_f) \\cdot (1 - o) \\right\\rfloor.\n$$\nThis formula is based on the rationale that:\n- Only a fraction $m$ of reads map to the reference and contribute to coverage.\n- Duplication rate $d$ indicates the proportion of non-independent reads; thus, a factor $(1-d)$ preserves unique observations.\n- Adapter contamination $a$ often leads to trimming or unusable bases; we penalize by $(1-a)$.\n- Per-base $N$ fraction $n_f$ indicates ambiguous calls that cannot inform SNV detection; we penalize by $(1-n_f)$.\n- Overrepresented sequences $o$ often reflect contamination or technical artifacts; we penalize by $(1-o)$.\nWe apply the floor $\\lfloor \\cdot \\rfloor$ to define $n$ as a non-negative integer count of independent Bernoulli trials.\n\nThird, we formalize the decision rule for variant detection using a hypothesis test on the binomial distribution. Under the null hypothesis $H_0$ (no true variant), any alternate observation arises from sequencing error. If the error probability is $e$, and assuming uniform miscall distribution among the three non-reference bases, the probability of observing a specific pre-specified alternate is\n$$\np_0 = \\frac{e}{3}.\n$$\nWe set a stringent false positive budget per site of $\\alpha = 10^{-6}$. The detection threshold $t$ is the smallest integer such that\n$$\n\\Pr\\!\\left[X \\ge t \\mid X \\sim \\mathrm{Binomial}(n, p_0)\\right] \\le \\alpha.\n$$\nIf such a $t$ exceeds $n$ (i.e., no $t \\le n$ satisfies the inequality), then no call is possible at the given $n$ and $e$, and we set the sensitivity to $0$.\n\nUnder the alternative hypothesis $H_1$ (true heterozygous variant with allele fraction $f = 0.5$), a read supports the specific alternate because either it comes from the alternate chromosome and is correctly called, or it comes from the reference chromosome and is miscalled as the specific alternate. Hence,\n$$\np_1 = f \\cdot (1 - e) + (1 - f) \\cdot \\frac{e}{3}.\n$$\nThe sensitivity (power) is then\n$$\nS = \\Pr\\!\\left[X \\ge t \\mid X \\sim \\mathrm{Binomial}(n, p_1)\\right].\n$$\nThis expression equals the survival function of the binomial distribution at $t-1$ with parameters $(n, p_1)$.\n\nFourth, to quantify sensitivity loss attributable to QC, we define a per-dataset baseline that fixes raw coverage but removes QC penalties and uses high quality. For the baseline, let $Q_m^{\\mathrm{base}} = 35$ (so $e_{\\mathrm{base}} = 10^{-3.5}$), $m^{\\mathrm{base}} = 1$, $d^{\\mathrm{base}} = 0$, $a^{\\mathrm{base}} = 0$, $o^{\\mathrm{base}} = 0$, $n_f^{\\mathrm{base}} = 0$, and\n$$\nn_{\\mathrm{base}} = \\left\\lfloor C_{\\text{raw}} \\right\\rfloor.\n$$\nWe compute the baseline threshold $t_{\\mathrm{base}}$ analogously using $p_0^{\\mathrm{base}} = e_{\\mathrm{base}}/3$ and $\\alpha$, and the baseline sensitivity $S_{\\mathrm{base}}$ using $p_1^{\\mathrm{base}} = f \\cdot (1 - e_{\\mathrm{base}}) + (1 - f) \\cdot e_{\\mathrm{base}}/3$. The sensitivity loss is\n$$\nL = S_{\\mathrm{base}} - S.\n$$\n\nFifth, we map QC to FastQC-style module statuses using thresholds:\n- Per-base sequence quality with $Q_m$: pass if $Q_m \\ge 28$, warn if $23 \\le Q_m < 28$, fail if $Q_m < 23$.\n- Per-sequence GC content deviation $|g - g_0|$: pass if $\\le 0.05$, warn if $\\in (0.05, 0.10]$, fail if > 0.10.\n- Sequence duplication levels $d$: pass if $\\le 0.20$, warn if $\\in (0.20, 0.50]$, fail if > 0.50.\n- Adapter content $a$: pass if $\\le 0.01$, warn if $\\in (0.01, 0.05]$, fail if > 0.05.\n- Overrepresented sequences $o$: pass if $\\le 0.01$, warn if $\\in (0.01, 0.10]$, fail if > 0.10.\n- Per-base $N$ content $n_f$: pass if $\\le 0.01$, warn if $\\in (0.01, 0.03]$, fail if > 0.03.\n\nWe map pass $\\to +1$, warn $\\to 0$, fail $\\to -1$, and compute the total number of failures by counting the number of $-1$ statuses across the six modules.\n\nAlgorithmic design:\n- For each dataset, compute $e = 10^{-Q_m/10}$, then $n$ via the multiplicative penalties and floor. Compute $p_0 = e/3$ and find the smallest threshold $t$ with binomial tail $\\le \\alpha$. If $t > n$, set $S = 0$; else compute $p_1$ and $S$ as the binomial survival function at $t-1$.\n- Compute the baseline values $e_{\\mathrm{base}}$, $n_{\\mathrm{base}}$, $p_0^{\\mathrm{base}}$, $t_{\\mathrm{base}}$, $p_1^{\\mathrm{base}}$, and $S_{\\mathrm{base}}$ analogously.\n- Output $L = S_{\\mathrm{base}} - S$ along with the module status vector and the count of failures.\n- The test suite consists of four datasets as specified, each with $(L, C_{\\text{raw}}, Q_m, m, d, a, o, n_f, g, g_0)$ given by:\n  - $(150, 30, 32, 0.97, 0.12, 0.008, 0.006, 0.004, 0.41, 0.41)$,\n  - $(100, 20, 25, 0.95, 0.35, 0.02, 0.03, 0.015, 0.36, 0.41)$,\n  - $(150, 40, 20, 0.70, 0.70, 0.10, 0.15, 0.04, 0.50, 0.41)$,\n  - $(75, 10, 30, 0.20, 0.50, 0.02, 0.01, 0.01, 0.41, 0.41)$.\nThese cover a high-quality case, boundary warnings without failures, severe failures, and low effective coverage.\n\nThe program computes the requested outputs and prints a single line in the specified nested list format:\n$[[[s_{11},\\dots,s_{16}], f_1, \\ell_1], [[s_{21},\\dots,s_{26}], f_2, \\ell_2], [[s_{31},\\dots,s_{36}], f_3, \\ell_3], [[s_{41},\\dots,s_{46}], f_4, \\ell_4]]$,\nwith $s_{ij} \\in \\{-1,0,1\\}$, $f_i$ integer, and $\\ell_i \\in [0,1]$ as a decimal (not a percentage).",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom math import floor\nfrom scipy.stats import binom\n\n# Fixed parameters for the detection model\nALPHA = 1e-6  # per-site false positive tail probability\nF = 0.5       # heterozygous allele fraction\nQ_BASE = 35.0 # baseline mean Phred quality\n\n# Module thresholds and order:\n# [Per-base quality, GC deviation, Duplication, Adapter, Overrepresented, N content]\ndef classify_modules(Qm, g, g0, d, a, o, nf):\n    statuses = []\n    # Per-base sequence quality\n    if Qm >= 28:\n        statuses.append(1)\n    elif Qm >= 23:\n        statuses.append(0)\n    else:\n        statuses.append(-1)\n    # GC deviation\n    gc_dev = abs(g - g0)\n    if gc_dev <= 0.05:\n        statuses.append(1)\n    elif gc_dev <= 0.10:\n        statuses.append(0)\n    else:\n        statuses.append(-1)\n    # Duplication\n    if d <= 0.20:\n        statuses.append(1)\n    elif d <= 0.50:\n        statuses.append(0)\n    else:\n        statuses.append(-1)\n    # Adapter\n    if a <= 0.01:\n        statuses.append(1)\n    elif a <= 0.05:\n        statuses.append(0)\n    else:\n        statuses.append(-1)\n    # Overrepresented sequences\n    if o <= 0.01:\n        statuses.append(1)\n    elif o <= 0.10:\n        statuses.append(0)\n    else:\n        statuses.append(-1)\n    # N content\n    if nf <= 0.01:\n        statuses.append(1)\n    elif nf <= 0.03:\n        statuses.append(0)\n    else:\n        statuses.append(-1)\n    return statuses\n\ndef phred_to_error(Q):\n    # e = 10^{-Q/10}\n    return 10.0 ** (-Q / 10.0)\n\ndef effective_coverage(C_raw, m, d, a, o, nf):\n    eff = C_raw * m * (1.0 - d) * (1.0 - a) * (1.0 - nf) * (1.0 - o)\n    n = int(np.floor(eff)) if eff > 0 else 0\n    return max(n, 0)\n\ndef find_threshold(n, p0, alpha):\n    # Find the smallest integer t such that P[X >= t | Bin(n, p0)] <= alpha\n    # We scan from t=0 to n+1; at t=n+1, the tail prob is 0 by definition.\n    # Use survival function for numerical stability.\n    for t in range(0, n + 2):\n        tail = binom.sf(t - 1, n, p0)  # P[X >= t]\n        if tail <= alpha:\n            return t\n    # Should never reach here due to n+1 guard\n    return n + 1\n\ndef sensitivity_at_params(n, Qm, alpha, f):\n    # Compute sensitivity given effective coverage n and mean Phred Qm\n    if n <= 0:\n        return 0.0\n    e = phred_to_error(Qm)\n    p0 = e / 3.0\n    t = find_threshold(n, p0, alpha)\n    if t > n:\n        return 0.0\n    p1 = f * (1.0 - e) + (1.0 - f) * (e / 3.0)\n    # Sensitivity = P[X >= t | Bin(n, p1)] = sf(t-1)\n    S = float(binom.sf(t - 1, n, p1))\n    # Clip to [0,1] to avoid minor numerical issues\n    if S < 0.0:\n        S = 0.0\n    elif S > 1.0:\n        S = 1.0\n    return S\n\ndef compute_loss(C_raw, Qm, m, d, a, o, nf):\n    # Current dataset sensitivity\n    n_eff = effective_coverage(C_raw, m, d, a, o, nf)\n    S_curr = sensitivity_at_params(n_eff, Qm, ALPHA, F)\n    # Baseline sensitivity\n    n_base = int(np.floor(C_raw)) if C_raw > 0 else 0\n    S_base = sensitivity_at_params(n_base, Q_BASE, ALPHA, F)\n    loss = S_base - S_curr\n    # Clip for safety\n    if loss < 0.0:\n        loss = 0.0\n    if loss > 1.0:\n        loss = 1.0\n    return loss\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each test case: (L, C_raw, Qm, m, d, a, o, nf, g, g0)\n    test_cases = [\n        (150, 30.0, 32.0, 0.97, 0.12, 0.008, 0.006, 0.004, 0.41, 0.41),\n        (100, 20.0, 25.0, 0.95, 0.35, 0.02, 0.03, 0.015, 0.36, 0.41),\n        (150, 40.0, 20.0, 0.70, 0.70, 0.10, 0.15, 0.04, 0.50, 0.41),\n        (75,  10.0, 30.0, 0.20, 0.50, 0.02, 0.01, 0.01, 0.41, 0.41),\n    ]\n\n    results = []\n    for case in test_cases:\n        L, C_raw, Qm, m, d, a, o, nf, g, g0 = case\n        status_codes = classify_modules(Qm, g, g0, d, a, o, nf)\n        fail_count = sum(1 for s in status_codes if s == -1)\n        loss = compute_loss(C_raw, Qm, m, d, a, o, nf)\n        # Round loss moderately for stable display\n        loss_rounded = float(np.round(loss, 6))\n        results.append([status_codes, fail_count, loss_rounded])\n\n    # Final print statement in the exact required format.\n    # Ensure no extra spaces for compact single-line output\n    def list_to_str(obj):\n        if isinstance(obj, list):\n            return \"[\" + \",\".join(list_to_str(x) for x in obj) + \"]\"\n        elif isinstance(obj, float):\n            # Use repr-like but controlled: avoid scientific for small numbers if possible\n            s = f\"{obj:.6f}\"\n            # Strip trailing zeros and possible trailing dot\n            s = s.rstrip('0').rstrip('.') if '.' in s else s\n            if s == \"\":\n                s = \"0\"\n            return s\n        else:\n            return str(obj)\n\n    print(list_to_str(results))\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "Clinical and biological databases are often fragmented and contain noisy records, making the correct identification of unique entities—such as patients—a formidable data quality challenge. This process, known as entity resolution, is fundamental to data integration and longitudinal analysis. In this practice , you will implement a complete entity resolution pipeline, employing techniques such as blocking for efficiency, fuzzy matching with Levenshtein distance, and graph-based clustering to enforce transitive linkage. Evaluating your system with metrics like precision, recall, and cluster-level merge/split errors will provide critical insight into the performance and failure modes of practical record linkage systems.",
            "id": "4551876",
            "problem": "Consider an entity resolution workflow for patient records in Electronic Health Records (EHR) where each record may contain noisy identifiers. The goal is to algorithmically determine which records refer to the same real-world patient (entity), construct predicted clusters via transitive closure of pairwise links, and then evaluate data quality using standard definitions from information retrieval and set theory, including an error analysis of merges and splits. The solution must be derived from first principles and implemented as a complete, runnable program.\n\nYou are provided with the following foundational base:\n- The notion of an entity as an equivalence class of records that refer to the same real-world patient, represented by a partition of the record set.\n- Pairwise evaluation based on set-theoretic definitions. Given a set of predicted clusters, define the set of unordered record pairs contained within the same predicted cluster. Similarly, define the set of unordered record pairs contained within the same ground-truth cluster.\n- The widely accepted definitions of precision and recall in information retrieval. Precision equals the fraction of predicted positive instances that are true positives. Recall equals the fraction of true positive instances that are correctly predicted.\n\nYou must implement the following workflow and computations without relying on prepackaged entity resolution routines:\n1. Record Representation: Each record contains fields \"id\", \"name\", \"dob\" (date of birth in \"YYYY-MM-DD\"), and \"zip\" (postal code).\n2. Blocking: A blocking function $b$ reduces the candidate space by grouping records that have the same pair $(\\text{last initial of the last name}, \\text{year of birth})$. Formally, for a record $r$, let $b(r) = (L(r), Y(r))$, where $L(r)$ is the first letter of the last token of the \"name\" field in uppercase, and $Y(r)$ is the first four characters of the \"dob\" field.\n3. String Normalization: Map the \"name\" field to uppercase and remove all non-alphabetic characters before computing similarity.\n4. Edit Distance: For two normalized strings $a$ and $b$, define the Levenshtein edit distance $d(a,b)$ as the minimum number of single-character insertions, deletions, or substitutions required to transform $a$ into $b$. The dynamic programming recurrence for $d(a,b)$ is constructed from the fundamental principle that optimal substructure holds for edit distance. Define the name similarity\n$$\ns_{\\text{name}}(a,b) = \n\\begin{cases}\n1 - \\dfrac{d(a,b)}{\\max(|a|,|b|)}, & \\text{if } \\max(|a|,|b|) > 0,\\\\\n1, & \\text{otherwise.}\n\\end{cases}\n$$\n5. Attribute Similarities: Define $s_{\\text{dob}}(r_i,r_j) = 1$ if the \"dob\" strings are exactly equal and $0$ otherwise. Define $s_{\\text{zip}}(r_i,r_j) = 1$ if the \"zip\" strings are exactly equal; if the first three characters of the \"zip\" strings are equal and the strings are at least three characters long, then $s_{\\text{zip}}(r_i,r_j) = 0.5$; otherwise, $s_{\\text{zip}}(r_i,r_j) = 0$.\n6. Weighted Score: For candidate pairs $(r_i,r_j)$ within the same block, compute a weighted score\n$$\nS(r_i,r_j) = w_{\\text{name}} \\cdot s_{\\text{name}}(r_i,r_j) + w_{\\text{dob}} \\cdot s_{\\text{dob}}(r_i,r_j) + w_{\\text{zip}} \\cdot s_{\\text{zip}}(r_i,r_j),\n$$\nwith nonnegative weights $w_{\\text{name}}, w_{\\text{dob}}, w_{\\text{zip}}$ that sum to $1$.\n7. Edge Construction and Clustering: Create an undirected link between $r_i$ and $r_j$ if $S(r_i,r_j) \\ge T$, where $T$ is a specified threshold. Predicted clusters are the connected components of this undirected graph over records.\n8. Pairwise Metrics: Let $P$ be the set of unordered pairs of records that lie within the same predicted cluster, and let $G$ be the set of unordered pairs of records that lie within the same ground-truth cluster. Define\n$$\n\\text{TP} = |P \\cap G|,\\quad \\text{FP} = |P \\setminus G|,\\quad \\text{FN} = |G \\setminus P|.\n$$\nPrecision is defined as\n$$\n\\text{precision} =\n\\begin{cases}\n\\dfrac{\\text{TP}}{\\text{TP} + \\text{FP}}, & \\text{if } \\text{TP} + \\text{FP} > 0,\\\\\n1, & \\text{if } \\text{TP} + \\text{FP} = 0,\n\\end{cases}\n$$\nand recall is defined as\n$$\n\\text{recall} =\n\\begin{cases}\n\\dfrac{\\text{TP}}{\\text{TP} + \\text{FN}}, & \\text{if } \\text{TP} + \\text{FN} > 0,\\\\\n1, & \\text{if } \\text{TP} + \\text{FN} = 0.\n\\end{cases}\n$$\nAll ratio outputs must be decimals, not percentages.\n9. Merge and Split Error Analysis: For each predicted cluster $C$, let $m_C$ be the number of distinct ground-truth entity identifiers present among records in $C$. The merge error count is\n$$\nM = \\sum_{C} \\max(0, m_C - 1).\n$$\nFor each ground-truth cluster $T$, let $s_T$ be the number of distinct predicted clusters that contain members of $T$. The split error count is\n$$\nS_{\\text{split}} = \\sum_{T} \\max(0, s_T - 1).\n$$\n\nYou must implement the above workflow and compute precision, recall, $M$, and $S_{\\text{split}}$ for each of the following test cases. Use the specified weights and thresholds.\n\nTest Case $1$ (happy path, moderate noise, correct merges):\n- Weights: $w_{\\text{name}} = 0.6$, $w_{\\text{dob}} = 0.3$, $w_{\\text{zip}} = 0.1$, threshold $T = 0.85$.\n- Records:\n  - $\\text{id}=$\"r1\", $\\text{name}=$\"John Smith\", $\\text{dob}=$\"1980-05-12\", $\\text{zip}=$\"02139\"\n  - $\\text{id}=$\"r2\", $\\text{name}=$\"JON SMITH\", $\\text{dob}=$\"1980-05-12\", $\\text{zip}=$\"02139\"\n  - $\\text{id}=$\"r3\", $\\text{name}=$\"Jane Doe\", $\\text{dob}=$\"1975-11-30\", $\\text{zip}=$\"02138\"\n  - $\\text{id}=$\"r4\", $\\text{name}=$\"Janet Doe\", $\\text{dob}=$\"1975-11-30\", $\\text{zip}=$\"02138\"\n  - $\\text{id}=$\"r5\", $\\text{name}=$\"Alice Johnson\", $\\text{dob}=$\"1990-02-01\", $\\text{zip}=$\"10001\"\n  - $\\text{id}=$\"r6\", $\\text{name}=$\"Alyce Johnson\", $\\text{dob}=$\"1990-02-01\", $\\text{zip}=$\"10001\"\n  - $\\text{id}=$\"r7\", $\\text{name}=$\"Bob Lee\", $\\text{dob}=$\"1980-05-12\", $\\text{zip}=$\"02139\"\n- Ground Truth Entity Identifiers:\n  - \"r1\" and \"r2\" belong to entity \"t1\".\n  - \"r3\" and \"r4\" belong to entity \"t2\".\n  - \"r5\" and \"r6\" belong to entity \"t3\".\n  - \"r7\" belongs to entity \"t4\".\n\nTest Case $2$ (boundary case, all unique, no merges expected):\n- Weights: $w_{\\text{name}} = 0.6$, $w_{\\text{dob}} = 0.3$, $w_{\\text{zip}} = 0.1$, threshold $T = 0.95$.\n- Records:\n  - $\\text{id}=$\"s1\", $\\text{name}=$\"Michael Brown\", $\\text{dob}=$\"1965-07-19\", $\\text{zip}=$\"90210\"\n  - $\\text{id}=$\"s2\", $\\text{name}=$\"Michelle Brown\", $\\text{dob}=$\"1970-07-19\", $\\text{zip}=$\"90210\"\n  - $\\text{id}=$\"s3\", $\\text{name}=$\"Carlos Garcia\", $\\text{dob}=$\"1988-12-12\", $\\text{zip}=$\"33101\"\n  - $\\text{id}=$\"s4\", $\\text{name}=$\"Li Wang\", $\\text{dob}=$\"1993-03-03\", $\\text{zip}=$\"10002\"\n- Ground Truth Entity Identifiers:\n  - Each of \"s1\", \"s2\", \"s3\", \"s4\" belongs to a distinct entity.\n\nTest Case $3$ (edge case, heavy noise causing erroneous merges and splits):\n- Weights: $w_{\\text{name}} = 0.6$, $w_{\\text{dob}} = 0.3$, $w_{\\text{zip}} = 0.1$, threshold $T = 0.7$.\n- Records:\n  - $\\text{id}=$\"u1\", $\\text{name}=$\"Sara Connor\", $\\text{dob}=$\"1979-01-01\", $\\text{zip}=$\"94110\"\n  - $\\text{id}=$\"u2\", $\\text{name}=$\"Sarah Conner\", $\\text{dob}=$\"1979-01-01\", $\\text{zip}=$\"94110\"\n  - $\\text{id}=$\"u3\", $\\text{name}=$\"Sera Conor\", $\\text{dob}=$\"1980-01-01\", $\\text{zip}=$\"94110\"\n  - $\\text{id}=$\"u4\", $\\text{name}=$\"Sam Conor\", $\\text{dob}=$\"1979-01-01\", $\\text{zip}=$\"94110\"\n  - $\\text{id}=$\"u5\", $\\text{name}=$\"Tom Hanks\", $\\text{dob}=$\"1956-07-09\", $\\text{zip}=$\"94110\"\n  - $\\text{id}=$\"u6\", $\\text{name}=$\"Thomas Hank\", $\\text{dob}=$\"1956-07-09\", $\\text{zip}=$\"94110\"\n  - $\\text{id}=$\"u7\", $\\text{name}=$\"Sara Connor\", $\\text{dob}=$\"1979-01-02\", $\\text{zip}=$\"94110\"\n- Ground Truth Entity Identifiers:\n  - \"u1\", \"u2\", \"u3\" belong to entity \"A\".\n  - \"u4\" belongs to entity \"B\".\n  - \"u5\", \"u6\" belong to entity \"C\".\n  - \"u7\" belongs to entity \"D\".\n\nFinal Output Specification:\n- For each test case in the order listed, output a four-element list $[\\text{precision}, \\text{recall}, M, S_{\\text{split}}]$ where precision and recall are rounded to four decimals (as decimals, not percentages), and $M$ and $S_{\\text{split}}$ are integers.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the four-element list for the corresponding test case. For example, the format must be like $[[p_1,r_1,M_1,S_1],[p_2,r_2,M_2,S_2],[p_3,r_3,M_3,S_3]]$ on a single line.",
            "solution": "The solution is systematically constructed by implementing the sequence of operations specified in the problem statement.\n\n**1. Foundational Data Representation and Pre-processing**\n\nEach patient record is treated as a structured data object with fields for \"id\", \"name\", \"dob\", and \"zip\". The ground truth is represented as a partition of the records, where each cell of the partition corresponds to a unique real-world entity.\n\n**2. Blocking for Computational Efficiency**\n\nTo avoid a computationally expensive all-pairs comparison, which would be of complexity $O(n^2)$ for $n$ records, a blocking strategy is employed. Records are partitioned into blocks based on a key derived from their attributes. The specified blocking function, $b(r) = (L(r), Y(r))$, groups records that share the same first initial of the last name, $L(r)$, and the same four-digit year of birth, $Y(r)$. All subsequent pairwise comparisons are restricted to records within the same block, significantly reducing the search space.\n\n**3. String Normalization and Similarity Metrics**\n\nRaw data is often noisy. To compare names robustly, a normalization function is first applied. This function converts the \"name\" field to uppercase and removes all non-alphabetic characters, ensuring that comparisons are case-insensitive and unaffected by punctuation.\n\nFollowing normalization, similarity scores between pairs of records $(r_i, r_j)$ are computed for each attribute:\n\n- **Name Similarity ($s_{\\text{name}}$)**: This is based on the Levenshtein edit distance, $d(a,b)$, a fundamental metric in string comparison that quantifies the difference between two sequences. The distance is calculated using a dynamic programming algorithm, which relies on the principle of optimal substructure. For two normalized name strings, $a$ and $b$, the matrix-based recurrence is:\n$$\nD[i][j] = \\min\n\\begin{cases}\nD[i-1][j] + 1 & \\text{(deletion)} \\\\\nD[i][j-1] + 1 & \\text{(insertion)} \\\\\nD[i-1][j-1] + \\mathbf{1}_{a[i] \\neq b[j]} & \\text{(substitution)}\n\\end{cases}\n$$\nThe distance $d(a,b)$ is the value $D[|a|][|b|]$. This distance is then normalized to produce a similarity score between $0$ and $1$:\n$$\ns_{\\text{name}}(a,b) = 1 - \\dfrac{d(a,b)}{\\max(|a|,|b|)}, \\text{ for } \\max(|a|,|b|) > 0\n$$\nIf both strings are empty, $s_{\\text{name}}=1$.\n\n- **Date of Birth Similarity ($s_{\\text{dob}}$)**: This is a strict binary comparison. $s_{\\text{dob}}(r_i,r_j) = 1$ if the \"dob\" strings are identical, and $s_{\\text{dob}}(r_i,r_j) = 0$ otherwise.\n\n- **ZIP Code Similarity ($s_{\\text{zip}}$)**: This metric allows for partial matches, reflecting geographical proximity. $s_{\\text{zip}}(r_i,r_j) = 1$ for an exact match, $s_{\\text{zip}}(r_i,r_j) = 0.5$ if the first three digits match (for ZIP codes of length at least $3$), and $s_{\\text{zip}}(r_i,r_j) = 0$ otherwise.\n\nThese individual similarities are combined into a single composite score using a weighted sum:\n$$\nS(r_i,r_j) = w_{\\text{name}} \\cdot s_{\\text{name}}(r_i,r_j) + w_{\\text{dob}} \\cdot s_{\\text{dob}}(r_i,r_j) + w_{\\text{zip}} \\cdot s_{\\text{zip}}(r_i,r_j)\n$$\nwhere the weights $w_{\\text{name}}, w_{\\text{dob}}, w_{\\text{zip}}$ are non-negative and sum to $1$.\n\n**4. Graph-Based Clustering**\n\nThe set of records is modeled as an undirected graph $G=(V, E)$, where the vertices $V$ are the records. An edge $(r_i, r_j)$ is added to the set of edges $E$ if the similarity score $S(r_i, r_j)$ meets or exceeds a specified threshold $T$. This step translates pairwise similarity into a network of relationships.\n\nThe predicted clusters of records are then determined by finding the connected components of this graph. A connected component is a maximal subgraph in which any two vertices are connected to each other by a path. This inherently enforces transitive closure: if record A is linked to B, and B is linked to C, all three are grouped into a single entity, even if A and C were not directly compared or did not meet the similarity threshold. A standard graph traversal algorithm, such as Depth-First Search (DFS) or Breadth-First Search (BFS), is used to identify these components.\n\n**5. Performance Evaluation from First Principles**\n\nThe quality of the predicted clusters is assessed against the ground truth using two families of metrics.\n\n- **Pairwise Metrics**: Precision and recall are adapted from information retrieval. The core idea is to evaluate the set of all unordered pairs of records that are predicted to be in the same cluster against the set of pairs that are truly in the same cluster.\n    - Let $G$ be the set of unordered pairs of records belonging to the same ground-truth entity. $|G| = \\sum_{T \\in \\text{Truth}} \\binom{|T|}{2}$.\n    - Let $P$ be the set of unordered pairs of records belonging to the same predicted cluster. $|P| = \\sum_{C \\in \\text{Predicted}} \\binom{|C|}{2}$.\n    - True Positives ($TP$): $|P \\cap G|$, the number of pairs correctly identified.\n    - False Positives ($FP$): $|P \\setminus G|$, the number of pairs incorrectly linked.\n    - False Negatives ($FN$): $|G \\setminus P|$, the number of true pairs that were missed.\n    - Precision and recall are then calculated as:\n    $$\n    \\text{precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}} = \\frac{|P \\cap G|}{|P|} \\quad \\text{and} \\quad \\text{recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}} = \\frac{|P \\cap G|}{|G|}\n    $$\n    The special case where the denominator is $0$ is handled by defining the metric as $1$, as specified.\n\n- **Cluster-Level Error Analysis**:\n    - **Merge Error ($M$)**: This quantifies how many distinct ground-truth entities are incorrectly merged into single predicted clusters. For each predicted cluster $C$, we count the number of distinct ground-truth entities represented, $m_C$. The total merge error is the sum of excesses over $1$: $M = \\sum_{C} \\max(0, m_C - 1)$.\n    - **Split Error ($S_{\\text{split}}$)**: This quantifies the fragmentation of ground-truth entities across multiple predicted clusters. For each ground-truth entity $T$, we count the number of distinct predicted clusters its members are scattered across, $s_T$. The total split error is $S_{\\text{split}} = \\sum_{T} \\max(0, s_T - 1)$.\n\nThese steps constitute a complete and rigorous methodology for entity resolution and its evaluation, which is implemented programmatically to solve the given test cases.",
            "answer": "```python\nimport numpy as np\nfrom collections import defaultdict\nfrom itertools import combinations\n\ndef solve():\n    \"\"\"\n    Main function to run the entity resolution workflow for all test cases.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"params\": {\"w_name\": 0.6, \"w_dob\": 0.3, \"w_zip\": 0.1, \"threshold\": 0.85},\n            \"records\": [\n                {\"id\": \"r1\", \"name\": \"John Smith\", \"dob\": \"1980-05-12\", \"zip\": \"02139\"},\n                {\"id\": \"r2\", \"name\": \"JON SMITH\", \"dob\": \"1980-05-12\", \"zip\": \"02139\"},\n                {\"id\": \"r3\", \"name\": \"Jane Doe\", \"dob\": \"1975-11-30\", \"zip\": \"02138\"},\n                {\"id\": \"r4\", \"name\": \"Janet Doe\", \"dob\": \"1975-11-30\", \"zip\": \"02138\"},\n                {\"id\": \"r5\", \"name\": \"Alice Johnson\", \"dob\": \"1990-02-01\", \"zip\": \"10001\"},\n                {\"id\": \"r6\", \"name\": \"Alyce Johnson\", \"dob\": \"1990-02-01\", \"zip\": \"10001\"},\n                {\"id\": \"r7\", \"name\": \"Bob Lee\", \"dob\": \"1980-05-12\", \"zip\": \"02139\"},\n            ],\n            \"ground_truth\": {\"t1\": [\"r1\", \"r2\"], \"t2\": [\"r3\", \"r4\"], \"t3\": [\"r5\", \"r6\"], \"t4\": [\"r7\"]},\n        },\n        {\n            \"params\": {\"w_name\": 0.6, \"w_dob\": 0.3, \"w_zip\": 0.1, \"threshold\": 0.95},\n            \"records\": [\n                {\"id\": \"s1\", \"name\": \"Michael Brown\", \"dob\": \"1965-07-19\", \"zip\": \"90210\"},\n                {\"id\": \"s2\", \"name\": \"Michelle Brown\", \"dob\": \"1970-07-19\", \"zip\": \"90210\"},\n                {\"id\": \"s3\", \"name\": \"Carlos Garcia\", \"dob\": \"1988-12-12\", \"zip\": \"33101\"},\n                {\"id\": \"s4\", \"name\": \"Li Wang\", \"dob\": \"1993-03-03\", \"zip\": \"10002\"},\n            ],\n            \"ground_truth\": {\"e1\": [\"s1\"], \"e2\": [\"s2\"], \"e3\": [\"s3\"], \"e4\": [\"s4\"]},\n        },\n        {\n            \"params\": {\"w_name\": 0.6, \"w_dob\": 0.3, \"w_zip\": 0.1, \"threshold\": 0.7},\n            \"records\": [\n                {\"id\": \"u1\", \"name\": \"Sara Connor\", \"dob\": \"1979-01-01\", \"zip\": \"94110\"},\n                {\"id\": \"u2\", \"name\": \"Sarah Conner\", \"dob\": \"1979-01-01\", \"zip\": \"94110\"},\n                {\"id\": \"u3\", \"name\": \"Sera Conor\", \"dob\": \"1980-01-01\", \"zip\": \"94110\"},\n                {\"id\": \"u4\", \"name\": \"Sam Conor\", \"dob\": \"1979-01-01\", \"zip\": \"94110\"},\n                {\"id\": \"u5\", \"name\": \"Tom Hanks\", \"dob\": \"1956-07-09\", \"zip\": \"94110\"},\n                {\"id\": \"u6\", \"name\": \"Thomas Hank\", \"dob\": \"1956-07-09\", \"zip\": \"94110\"},\n                {\"id\": \"u7\", \"name\": \"Sara Connor\", \"dob\": \"1979-01-02\", \"zip\": \"94110\"},\n            ],\n            \"ground_truth\": {\"A\": [\"u1\", \"u2\", \"u3\"], \"B\": [\"u4\"], \"C\": [\"u5\", \"u6\"], \"D\": [\"u7\"]},\n        },\n    ]\n\n    final_results = []\n    \n    for case in test_cases:\n        final_results.append(process_case(case))\n    \n    print(f\"[{','.join(map(str, final_results))}]\")\n\n\ndef normalize_name(name):\n    return \"\".join(filter(str.isalpha, name.upper()))\n\ndef levenshtein_distance(s1, s2):\n    m, n = len(s1), len(s2)\n    if m < n:\n        s1, s2 = s2, s1\n        m, n = n, m\n    \n    if n == 0:\n        return m\n    \n    dp = np.zeros((m + 1, n + 1), dtype=int)\n    for i in range(m + 1):\n        dp[i][0] = i\n    for j in range(n + 1):\n        dp[0][j] = j\n        \n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            cost = 0 if s1[i-1] == s2[j-1] else 1\n            dp[i][j] = min(dp[i-1][j] + 1,        # Deletion\n                           dp[i][j-1] + 1,        # Insertion\n                           dp[i-1][j-1] + cost)   # Substitution\n    return dp[m, n]\n\ndef get_pairs_from_clusters(clusters):\n    pairs = set()\n    for cluster in clusters:\n        # Sort to make pairs canonical\n        sorted_cluster = sorted(list(cluster))\n        for r1, r2 in combinations(sorted_cluster, 2):\n            pairs.add((r1, r2))\n    return pairs\n\ndef find_connected_components(adj, nodes):\n    visited = set()\n    components = []\n    for node in nodes:\n        if node not in visited:\n            component = set()\n            stack = [node]\n            visited.add(node)\n            while stack:\n                curr = stack.pop()\n                component.add(curr)\n                for neighbor in adj.get(curr, []):\n                    if neighbor not in visited:\n                        visited.add(neighbor)\n                        stack.append(neighbor)\n            components.append(component)\n    return components\n\ndef process_case(case_data):\n    records = case_data[\"records\"]\n    params = case_data[\"params\"]\n    ground_truth_clusters = case_data[\"ground_truth\"]\n\n    records_map = {rec['id']: rec for rec in records}\n    \n    # 1. Blocking\n    blocks = defaultdict(list)\n    for rec in records:\n        last_name_token = rec['name'].split()[-1]\n        last_initial = last_name_token[0].upper()\n        year_of_birth = rec['dob'][:4]\n        block_key = (last_initial, year_of_birth)\n        blocks[block_key].append(rec['id'])\n    \n    # 2. Edge Construction\n    adj = defaultdict(list)\n    for block in blocks.values():\n        for r_id1, r_id2 in combinations(block, 2):\n            rec1, rec2 = records_map[r_id1], records_map[r_id2]\n            \n            # Name similarity\n            norm_name1 = normalize_name(rec1['name'])\n            norm_name2 = normalize_name(rec2['name'])\n            max_len = max(len(norm_name1), len(norm_name2))\n            if max_len > 0:\n                s_name = 1 - levenshtein_distance(norm_name1, norm_name2) / max_len\n            else:\n                s_name = 1.0\n\n            # DOB similarity\n            s_dob = 1.0 if rec1['dob'] == rec2['dob'] else 0.0\n\n            # ZIP similarity\n            s_zip = 0.0\n            zip1, zip2 = rec1['zip'], rec2['zip']\n            if zip1 == zip2:\n                s_zip = 1.0\n            elif len(zip1) >= 3 and len(zip2) >= 3 and zip1[:3] == zip2[:3]:\n                s_zip = 0.5\n            \n            # Weighted score\n            score = (params['w_name'] * s_name +\n                     params['w_dob'] * s_dob +\n                     params['w_zip'] * s_zip)\n            \n            if score >= params['threshold']:\n                adj[r_id1].append(r_id2)\n                adj[r_id2].append(r_id1)\n    \n    # 3. Clustering\n    predicted_clusters = find_connected_components(adj, records_map.keys())\n\n    # 4. Pairwise Metrics\n    true_pairs = get_pairs_from_clusters(ground_truth_clusters.values())\n    predicted_pairs = get_pairs_from_clusters(predicted_clusters)\n\n    tp = len(true_pairs.intersection(predicted_pairs))\n    fp = len(predicted_pairs) - tp\n    fn = len(true_pairs) - tp\n    \n    precision = 1.0 if (tp + fp) == 0 else tp / (tp + fp)\n    recall = 1.0 if (tp + fn) == 0 else tp / (tp + fn)\n\n    # 5. Merge and Split Error Analysis\n    # Merge errors\n    record_to_gt = {rec_id: gt_id for gt_id, rec_ids in ground_truth_clusters.items() for rec_id in rec_ids}\n    merge_error = 0\n    for p_cluster in predicted_clusters:\n        gt_entities_in_cluster = {record_to_gt[rec_id] for rec_id in p_cluster}\n        merge_error += max(0, len(gt_entities_in_cluster) - 1)\n        \n    # Split errors\n    record_to_p_cluster = {}\n    for i, p_cluster in enumerate(predicted_clusters):\n        for rec_id in p_cluster:\n            record_to_p_cluster[rec_id] = i\n    \n    split_error = 0\n    for gt_cluster in ground_truth_clusters.values():\n        p_clusters_hit = {record_to_p_cluster[rec_id] for rec_id in gt_cluster}\n        split_error += max(0, len(p_clusters_hit) - 1)\n\n    return [round(precision, 4), round(recall, 4), merge_error, split_error]\n\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "When introducing a new measurement technique or comparing two clinical assays, it is not enough to show that their results are correlated; one must rigorously assess if they agree and can be used interchangeably. This exercise  provides hands-on practice with the gold-standard biostatistical methods for method comparison studies. You will implement Passing-Bablok regression, a robust non-parametric technique to estimate systematic bias, and the Bland-Altman analysis to calculate the Limits of Agreement ($LoA$), which define the expected range of differences between the two methods.",
            "id": "4552070",
            "problem": "You are given paired measurements from two clinical assays assessed on the same specimens. The goal is to evaluate method comparison and agreement using a robust linear estimator and a distribution-based agreement metric. Assume a linear relationship between the two assays expressed as $y = a + b x$, where $x$ denotes the measurement from Assay $X$ and $y$ denotes the measurement from Assay $Y$. You must implement a nonparametric robust regression to estimate the constant bias $a$ and proportional error $b$ under the following constraints: the estimator must be invariant under monotone transformations and must rely on order statistics rather than parametric distributional assumptions. Additionally, you must implement a distribution-based agreement assessment using the framework of differences, computing the mean difference and the Limits of Agreement (LoA), defined for the difference $d_i = y_i - x_i$ as the interval $[\\overline{d} - 1.96 s_d, \\overline{d} + 1.96 s_d]$, where $\\overline{d}$ is the sample mean of differences and $s_d$ is the sample standard deviation of differences. The LoA are to be interpreted on the same scale as the measurements and must be computed as real numbers.\n\nFundamental basis and definitions to be used for derivation and implementation:\n- The linear model $y = a + b x$ with unknown $a$ and $b$.\n- Robust estimation principles based on medians and order statistics to mitigate the influence of outliers and avoid reliance on normality.\n- Differences-based agreement metrics: for paired data $(x_i,y_i)$, the difference $d_i = y_i - x_i$, the mean $\\overline{d} = \\frac{1}{n}\\sum_{i=1}^{n} d_i$, and the sample standard deviation $s_d = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n} (d_i - \\overline{d})^2}$.\n- Limits of Agreement (LoA): $[\\overline{d} - 1.96 s_d, \\overline{d} + 1.96 s_d]$.\n\nYou must produce a program that, for each test case, returns a list $[b,a,\\overline{d},\\text{LoA}_{\\text{low}},\\text{LoA}_{\\text{high}}]$, where $b$ and $a$ are the robust slope and intercept estimates for the linear relationship $y = a + b x$, $\\overline{d}$ is the mean difference, and $\\text{LoA}_{\\text{low}}$ and $\\text{LoA}_{\\text{high}}$ are the lower and upper Limits of Agreement. All quantities are to be treated as dimensionless numeric values; no physical units are involved. The Limits of Agreement are to be computed using the constant $1.96$, reflecting two-sided coverage under the assumption that the distribution of differences is approximately symmetric.\n\nTest suite:\n- Case $1$ (general case; moderate proportional error and small bias): \n  $x = [\\,8,12,18,25,35,48,60,72,85,100\\,]$ and \n  $y = [\\,0.98\\cdot 8 + 1.7 + 0.3,\\;0.98\\cdot 12 + 1.7 - 0.4,\\;0.98\\cdot 18 + 1.7 + 0.2,\\;0.98\\cdot 25 + 1.7 - 0.1,\\;0.98\\cdot 35 + 1.7 + 0.5,\\;0.98\\cdot 48 + 1.7 - 0.2,\\;0.98\\cdot 60 + 1.7 + 0.1,\\;0.98\\cdot 72 + 1.7 + 0.0,\\;0.98\\cdot 85 + 1.7 - 0.3,\\;0.98\\cdot 100 + 1.7 + 0.4\\,]$.\n- Case $2$ (boundary with repeated $x$ values; tests handling of zero-denominator pairs): \n  $x = [\\,50,50,60,60,60,80,80,80,90,110\\,]$ and \n  $y = [\\,1.05\\cdot 50 - 3 + 1.0,\\;1.05\\cdot 50 - 3 - 2.0,\\;1.05\\cdot 60 - 3 + 0.5,\\;1.05\\cdot 60 - 3 - 1.5,\\;1.05\\cdot 60 - 3 + 0.4,\\;1.05\\cdot 80 - 3 + 1.2,\\;1.05\\cdot 80 - 3 - 0.8,\\;1.05\\cdot 80 - 3 + 0.3,\\;1.05\\cdot 90 - 3 + 0.0,\\;1.05\\cdot 110 - 3 - 0.6\\,]$.\n- Case $3$ (outliers present; tests robustness): \n  $x = [\\,5,15,25,35,45,55,65,75,85,95\\,]$ and \n  $y = [\\,1.2\\cdot 5 - 4 + 0.2,\\;1.2\\cdot 15 - 4 - 0.1,\\;1.2\\cdot 25 - 4 + 0.3,\\;1.2\\cdot 35 - 4 + 15.0,\\;1.2\\cdot 45 - 4 - 0.5,\\;1.2\\cdot 55 - 4 + 0.0,\\;1.2\\cdot 65 - 4 + 0.4,\\;1.2\\cdot 75 - 4 - 12.0,\\;1.2\\cdot 85 - 4 + 0.1,\\;1.2\\cdot 95 - 4 - 0.2\\,]$.\n- Case $4$ (near-agreement; small random deviations): \n  $x = [\\,10,20,30,40,50,60,70,80,90,100\\,]$ and \n  $y = [\\,10 + 0.05,\\;20 - 0.02,\\;30 + 0.03,\\;40 + 0.01,\\;50 - 0.04,\\;60 + 0.02,\\;70 - 0.03,\\;80 + 0.00,\\;90 + 0.04,\\;100 - 0.01\\,]$.\n\nYour program must:\n- Implement the robust estimator for $a$ and $b$ based on order statistics derived from pairwise relationships between points, avoiding zero-denominator computations when $x_j = x_i$.\n- Implement the Limits of Agreement calculation with the differences $d_i = y_i - x_i$, computing $\\overline{d}$ and $s_d$, then $\\overline{d} \\pm 1.96 s_d$.\n- For each test case, produce the list $[b,a,\\overline{d},\\text{LoA}_{\\text{low}},\\text{LoA}_{\\text{high}}]$ as real numbers.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each item is the list for one test case. For example, the output must be of the form $[[b_1,a_1,\\overline{d}_1,\\text{LoA}_{\\text{low},1},\\text{LoA}_{\\text{high},1}],[b_2,a_2,\\overline{d}_2,\\text{LoA}_{\\text{low},2},\\text{LoA}_{\\text{high},2}],\\ldots]$ with no additional text.",
            "solution": "The problem requires the implementation of a statistical methodology for comparing two clinical assays. This involves two main components: first, a robust linear regression to model the relationship between the two methods, and second, an agreement analysis to quantify the differences between their measurements. The solution will be derived and implemented based on standard, well-established techniques in biostatistics and clinical chemistry that match the problem's specifications.\n\n### Part 1: Robust Linear Regression using Passing-Bablok Estimator\n\nThe Passing-Bablok method is a nonparametric procedure for fitting a linear regression line that is robust to outliers. It aligns perfectly with the problem's requirements. The estimation proceeds in two steps: first the slope $b$, then the intercept $a$.\n\n**Step 1.1: Slope Estimation ($\\hat{b}$)**\n\nGiven $n$ paired data points $(x_i, y_i)$, the first step is to compute all possible pairwise slopes. For any two distinct points $i$ and $j$ ($1 \\le i < j \\le n$), a slope $s_{ij}$ is calculated, provided that the denominator is non-zero. The set of all such valid slopes, $S$, is given by:\n$$\nS = \\left\\{ s_{ij} = \\frac{y_j - y_i}{x_j - x_i} \\quad \\forall i < j \\text{ such that } x_i \\ne x_j \\right\\}\n$$\nThe problem explicitly states that pairs with $x_i = x_j$ should be ignored for slope calculation, which is a defining feature of this method. Let $N$ be the number of elements in the set $S$. The robust estimate of the slope, $\\hat{b}$, is the median of this set of pairwise slopes:\n$$\n\\hat{b} = \\text{median}(S)\n$$\nThe median is chosen for its robustness. For a sorted set of $N$ slopes, if $N$ is odd, the median is the value at position $(N+1)/2$. If $N$ is even, the median is conventionally the average of the two central values at positions $N/2$ and $N/2+1$. This estimator relies on order statistics (the median) and makes no assumptions about the distribution of the data or errors.\n\n**Step 1.2: Intercept Estimation ($\\hat{a}$)**\n\nOnce the slope estimate $\\hat{b}$ is obtained, the intercept $a$ can be estimated. For each data point $(x_i, y_i)$, an intercept candidate $c_i$ is calculated by rearranging the linear equation:\n$$\nc_i = y_i - \\hat{b} x_i\n$$\nThis results in a set of $n$ intercept candidates $\\{c_1, c_2, \\ldots, c_n\\}$. The robust estimate of the intercept, $\\hat{a}$, is the median of this set:\n$$\n\\hat{a} = \\text{median}(\\{c_1, c_2, \\ldots, c_n\\})\n$$\nThis two-step process yields the robust regression line $\\hat{y} = \\hat{a} + \\hat{b} x$.\n\n### Part 2: Agreement Analysis using Bland-Altman Method\n\nWhile regression analysis describes the systematic relationship between two methods, agreement analysis quantifies the random differences between them. The Bland-Altman approach is the standard for this task.\n\n**Step 2.1: Calculation of Differences ($d_i$)**\n\nThe fundamental quantity is the difference between the paired measurements:\n$$\nd_i = y_i - x_i\n$$\nThis represents the error or disagreement for the $i$-th specimen.\n\n**Step 2.2: Mean and Standard Deviation of Differences**\n\nThe average of these differences, $\\overline{d}$, represents the mean bias between the two assays. A value of $\\overline{d}$ close to $0$ indicates low systematic bias. It is calculated as:\n$$\n\\overline{d} = \\frac{1}{n} \\sum_{i=1}^{n} d_i\n$$\nThe scatter of the differences around the mean is quantified by the sample standard deviation, $s_d$. The problem provides the formula for the unbiased sample standard deviation, which divides by $n-1$:\n$$\ns_d = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} (d_i - \\overline{d})^2}\n$$\n\n**Step 2.3: Limits of Agreement (LoA)**\n\nThe Limits of Agreement define the range within which most differences between the two methods are expected to fall. Assuming the differences are approximately normally distributed, this range is typically a $95\\%$ prediction interval. The problem specifies using the constant $1.96$, which corresponds to the appropriate z-score from the standard normal distribution ($z_{0.025}$).\n\nThe lower and upper Limits of Agreement are calculated as:\n$$\n\\text{LoA}_{\\text{low}} = \\overline{d} - 1.96 s_d\n$$\n$$\n\\text{LoA}_{\\text{high}} = \\overline{d} + 1.96 s_d\n$$\nThese two values, along with the mean difference $\\overline{d}$, provide a complete summary of the agreement between the assays.\n\n### Summary of Calculations\nFor each test case, the program will execute these steps to compute the five required values: the robust slope $\\hat{b}$, the robust intercept $\\hat{a}$, the mean difference $\\overline{d}$, the lower limit of agreement $\\text{LoA}_{\\text{low}}$, and the upper limit of agreement $\\text{LoA}_{\\text{high}}$. These will be returned as a list $[\\hat{b}, \\hat{a}, \\overline{d}, \\text{LoA}_{\\text{low}}, \\text{LoA}_{\\text{high}}]$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the method comparison problem for all test cases.\n    Implements Passing-Bablok regression and Bland-Altman analysis.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (general case; moderate proportional error and small bias)\n        (\n            np.array([8, 12, 18, 25, 35, 48, 60, 72, 85, 100], dtype=float),\n            np.array([9.84, 13.06, 19.54, 26.1, 36.5, 48.54, 60.6, 72.26, 84.7, 100.1], dtype=float)\n        ),\n        # Case 2 (boundary with repeated x values; tests handling of zero-denominator pairs)\n        (\n            np.array([50, 50, 60, 60, 60, 80, 80, 80, 90, 110], dtype=float),\n            np.array([50.5, 47.5, 60.5, 58.5, 60.4, 82.2, 80.2, 81.3, 91.5, 111.9], dtype=float)\n        ),\n        # Case 3 (outliers present; tests robustness)\n        (\n            np.array([5, 15, 25, 35, 45, 55, 65, 75, 85, 95], dtype=float),\n            np.array([2.2, 13.9, 26.3, 53.0, 49.5, 62.0, 74.4, 74.0, 98.1, 109.8], dtype=float)\n        ),\n        # Case 4 (near-agreement; small random deviations)\n        (\n            np.array([10, 20, 30, 40, 50, 60, 70, 80, 90, 100], dtype=float),\n            np.array([10.05, 19.98, 30.03, 40.01, 49.96, 60.02, 69.97, 80.00, 90.04, 99.99], dtype=float)\n        )\n    ]\n\n    results = []\n    \n    for x, y in test_cases:\n        # Part 1: Passing-Bablok Regression for b and a\n        \n        # Step 1.1: Estimate slope b\n        n = len(x)\n        slopes = []\n        for i in range(n):\n            for j in range(i + 1, n):\n                if x[j] - x[i] != 0:\n                    slope_ij = (y[j] - y[i]) / (x[j] - x[i])\n                    slopes.append(slope_ij)\n        \n        b = np.median(slopes)\n        \n        # Step 1.2: Estimate intercept a\n        intercept_residuals = y - b * x\n        a = np.median(intercept_residuals)\n        \n        # Part 2: Bland-Altman Analysis for agreement\n        \n        # Step 2.1: Calculate differences\n        differences = y - x\n        \n        # Step 2.2: Calculate mean and std dev of differences\n        mean_diff = np.mean(differences)\n        # ddof=1 for sample standard deviation (division by n-1)\n        std_diff = np.std(differences, ddof=1)\n        \n        # Step 2.3: Calculate Limits of Agreement (LoA)\n        loa_low = mean_diff - 1.96 * std_diff\n        loa_high = mean_diff + 1.96 * std_diff\n        \n        # Compile results for the current test case\n        case_result = [b, a, mean_diff, loa_low, loa_high]\n        results.append(case_result)\n\n    # Final print statement in the exact required format.\n    # The default str() representation for a list is '[item1, item2, ...]', which is what's needed.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}