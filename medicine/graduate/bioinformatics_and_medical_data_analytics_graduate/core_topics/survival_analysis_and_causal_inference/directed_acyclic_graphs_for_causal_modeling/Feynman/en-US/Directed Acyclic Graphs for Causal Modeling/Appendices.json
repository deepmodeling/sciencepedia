{
    "hands_on_practices": [
        {
            "introduction": "The rules of d-separation provide a qualitative guide for identifying conditional independencies. This practice moves to a quantitative understanding by exploring the \"M-structure,\" a classic example of collider stratification bias. By working through a linear-Gaussian structural equation model, you will derive from first principles how conditioning on a common effect $C$ induces a non-causal association between two of its causes, $X$ and $Y$ . This exercise is fundamental to understanding why adjusting for colliders in statistical models can generate spurious findings.",
            "id": "4557747",
            "problem": "Consider a bioinformatics and medical data analytics pipeline where gene expression $X$ and a clinical outcome severity score $Y$ are analyzed in the presence of a measured quality control variable $C$ and two unobserved biological factors $U$ and $V$. Assume the following Directed Acyclic Graph (DAG): $X \\leftarrow U \\to C \\leftarrow V \\to Y$, with no direct causal edge from $X$ to $Y$ ($X \\not\\to Y$), and with $U$ and $V$ unobserved. The variable $U$ represents an unmeasured cell-type composition factor that influences both $X$ and $C$, while $V$ represents an unmeasured inflammatory state that influences both $C$ and $Y$. Let the system be governed by a linear-Gaussian Structural Equation Model (SEM) with independent exogenous components:\n- $U \\sim \\mathcal{N}(0,\\sigma_{U}^{2})$ and $V \\sim \\mathcal{N}(0,\\sigma_{V}^{2})$, with $U$ independent of $V$.\n- $\\varepsilon_{X} \\sim \\mathcal{N}(0,\\sigma_{X}^{2})$, $\\varepsilon_{C} \\sim \\mathcal{N}(0,\\sigma_{C}^{2})$, and $\\varepsilon_{Y} \\sim \\mathcal{N}(0,\\sigma_{Y}^{2})$, mutually independent and independent of $U$ and $V$.\n- Linear structural equations: $X = \\alpha U + \\varepsilon_{X}$, $C = \\beta U + \\gamma V + \\varepsilon_{C}$, and $Y = \\delta V + \\varepsilon_{Y}$.\n\nStarting from the core definitions of a Directed Acyclic Graph and the properties of multivariate Gaussian models, derive the unconditional covariance $\\operatorname{Cov}(X,Y)$ and then, by quantifying the effect of conditioning on the collider $C$, derive the conditional covariance $\\operatorname{Cov}(X,Y \\mid C)$ in terms of the parameters $\\alpha$, $\\beta$, $\\gamma$, $\\delta$, $\\sigma_{U}^{2}$, $\\sigma_{V}^{2}$, and $\\sigma_{C}^{2}$. Your derivation must be grounded in first principles (conditional distributions of multivariate normal variables and linear projection), without assuming any pre-stated shortcut formulas. Provide your final answer as a single closed-form analytic expression for $\\operatorname{Cov}(X,Y \\mid C)$. No numerical approximation is required, and no units are needed.",
            "solution": "The problem statement has been critically validated and is deemed valid. It is scientifically grounded in the established theory of structural equation models (SEMs) and multivariate Gaussian distributions, which are standard tools in causal inference and biostatistics. The problem is well-posed, with a complete and consistent set of definitions and parameters, ensuring that a unique, analytical solution can be derived. The language is objective and formal. Therefore, we proceed with a full derivation.\n\nThe problem describes a system of random variables $(X, Y, C, U, V)$. The variables $X$, $Y$, and $C$ are defined as linear combinations of the independent, zero-mean Gaussian random variables $U$, $V$, $\\varepsilon_{X}$, $\\varepsilon_{C}$, and $\\varepsilon_{Y}$. A linear combination of Gaussian variables is itself Gaussian. Consequently, the vector of observable variables $(X, Y, C)^T$ follows a multivariate normal distribution with a mean vector of zero, since all constituent variables have zero mean. We denote this vector by $\\mathbf{Z} = (X, Y, C)^T$. The properties of this system are entirely determined by its covariance matrix, $\\Sigma_{\\mathbf{Z}}$.\n\nFirst, we derive the unconditional covariance $\\operatorname{Cov}(X,Y)$ as requested. This is an element of the covariance matrix $\\Sigma_{\\mathbf{Z}}$. The elements of this matrix are computed using the linearity of the covariance operator and the specified independence conditions.\n\nThe structural equations are:\n$X = \\alpha U + \\varepsilon_{X}$\n$Y = \\delta V + \\varepsilon_{Y}$\n$C = \\beta U + \\gamma V + \\varepsilon_{C}$\n\nThe variances and covariances of the exogenous variables are given as:\n$\\operatorname{Var}(U) = \\sigma_{U}^{2}$, $\\operatorname{Var}(V) = \\sigma_{V}^{2}$, $\\operatorname{Var}(\\varepsilon_{X}) = \\sigma_{X}^{2}$, $\\operatorname{Var}(\\varepsilon_{C}) = \\sigma_{C}^{2}$, $\\operatorname{Var}(\\varepsilon_{Y}) = \\sigma_{Y}^{2}$.\nAll exogenous variables ($U, V, \\varepsilon_{X}, \\varepsilon_{C}, \\varepsilon_{Y}$) are mutually independent.\n\nWe compute the elements of the covariance matrix $\\Sigma_{\\mathbf{Z}}$:\nThe variances (diagonal elements):\n$\\operatorname{Var}(X) = \\operatorname{Cov}(\\alpha U + \\varepsilon_{X}, \\alpha U + \\varepsilon_{X}) = \\alpha^2 \\operatorname{Var}(U) + \\operatorname{Var}(\\varepsilon_{X}) = \\alpha^2\\sigma_{U}^{2} + \\sigma_{X}^{2}$.\n$\\operatorname{Var}(Y) = \\operatorname{Cov}(\\delta V + \\varepsilon_{Y}, \\delta V + \\varepsilon_{Y}) = \\delta^2 \\operatorname{Var}(V) + \\operatorname{Var}(\\varepsilon_{Y}) = \\delta^2\\sigma_{V}^{2} + \\sigma_{Y}^{2}$.\n$\\operatorname{Var}(C) = \\operatorname{Cov}(\\beta U + \\gamma V + \\varepsilon_{C}, \\beta U + \\gamma V + \\varepsilon_{C}) = \\beta^2 \\operatorname{Var}(U) + \\gamma^2 \\operatorname{Var}(V) + \\operatorname{Var}(\\varepsilon_{C}) = \\beta^2\\sigma_{U}^{2} + \\gamma^2\\sigma_{V}^{2} + \\sigma_{C}^{2}$.\n\nThe covariances (off-diagonal elements):\n$\\operatorname{Cov}(X, Y) = \\operatorname{Cov}(\\alpha U + \\varepsilon_{X}, \\delta V + \\varepsilon_{Y}) = \\alpha\\delta\\operatorname{Cov}(U,V) + \\alpha\\operatorname{Cov}(U,\\varepsilon_{Y}) + \\delta\\operatorname{Cov}(\\varepsilon_{X},V) + \\operatorname{Cov}(\\varepsilon_{X},\\varepsilon_{Y})$.\nDue to independence of all exogenous variables, all these covariance terms are zero.\n$$ \\operatorname{Cov}(X, Y) = 0 $$\nThis result is consistent with the d-separation criterion from the theory of Directed Acyclic Graphs. The only path between $X$ and $Y$ is $X \\leftarrow U \\to C \\leftarrow V \\to Y$, which is blocked by the collider node $C$. Thus, $X$ and $Y$ are unconditionally independent.\n\n$\\operatorname{Cov}(X, C) = \\operatorname{Cov}(\\alpha U + \\varepsilon_{X}, \\beta U + \\gamma V + \\varepsilon_{C}) = \\alpha\\beta\\operatorname{Var}(U) = \\alpha\\beta\\sigma_{U}^{2}$.\n$\\operatorname{Cov}(Y, C) = \\operatorname{Cov}(\\delta V + \\varepsilon_{Y}, \\beta U + \\gamma V + \\varepsilon_{C}) = \\gamma\\delta\\operatorname{Var}(V) = \\gamma\\delta\\sigma_{V}^{2}$.\n\nThe full covariance matrix for $\\mathbf{Z} = (X, Y, C)^T$ is therefore:\n$$\n\\Sigma_{\\mathbf{Z}} =\n\\begin{pmatrix}\n\\alpha^2\\sigma_{U}^{2} + \\sigma_{X}^{2} & 0 & \\alpha\\beta\\sigma_{U}^{2} \\\\\n0 & \\delta^2\\sigma_{V}^{2} + \\sigma_{Y}^{2} & \\gamma\\delta\\sigma_{V}^{2} \\\\\n\\alpha\\beta\\sigma_{U}^{2} & \\gamma\\delta\\sigma_{V}^{2} & \\beta^2\\sigma_{U}^{2} + \\gamma^2\\sigma_{V}^{2} + \\sigma_{C}^{2}\n\\end{pmatrix}\n$$\nTo find the conditional covariance $\\operatorname{Cov}(X,Y \\mid C)$, we use the formula for the conditional covariance matrix of a partitioned multivariate normal vector. Let us partition the vector $\\mathbf{Z}$ into $\\mathbf{Z}_1 = (X, Y)^T$ and $\\mathbf{Z}_2 = (C)$. The covariance matrix $\\Sigma_{\\mathbf{Z}}$ is partitioned conformably:\n$$\n\\Sigma_{\\mathbf{Z}} = \\begin{pmatrix} \\Sigma_{11} & \\Sigma_{12} \\\\ \\Sigma_{21} & \\Sigma_{22} \\end{pmatrix}\n$$\nwhere\n$\\Sigma_{11} = \\operatorname{Cov}(\\mathbf{Z}_1) = \\begin{pmatrix} \\operatorname{Var}(X) & \\operatorname{Cov}(X,Y) \\\\ \\operatorname{Cov}(Y,X) & \\operatorname{Var}(Y) \\end{pmatrix} = \\begin{pmatrix} \\alpha^2\\sigma_{U}^{2} + \\sigma_{X}^{2} & 0 \\\\ 0 & \\delta^2\\sigma_{V}^{2} + \\sigma_{Y}^{2} \\end{pmatrix}$\n\n$\\Sigma_{12} = \\operatorname{Cov}(\\mathbf{Z}_1, \\mathbf{Z}_2) = \\begin{pmatrix} \\operatorname{Cov}(X,C) \\\\ \\operatorname{Cov}(Y,C) \\end{pmatrix} = \\begin{pmatrix} \\alpha\\beta\\sigma_{U}^{2} \\\\ \\gamma\\delta\\sigma_{V}^{2} \\end{pmatrix}$\n\n$\\Sigma_{21} = \\operatorname{Cov}(\\mathbf{Z}_2, \\mathbf{Z}_1) = \\Sigma_{12}^T = \\begin{pmatrix} \\alpha\\beta\\sigma_{U}^{2} & \\gamma\\delta\\sigma_{V}^{2} \\end{pmatrix}$\n\n$\\Sigma_{22} = \\operatorname{Var}(\\mathbf{Z}_2) = \\operatorname{Var}(C) = \\beta^2\\sigma_{U}^{2} + \\gamma^2\\sigma_{V}^{2} + \\sigma_{C}^{2}$\n\nFor a multivariate normal distribution, the covariance matrix of $\\mathbf{Z}_1$ conditional on $\\mathbf{Z}_2$ is given by the Schur complement of $\\Sigma_{22}$ in $\\Sigma_{\\mathbf{Z}}$:\n$$\n\\operatorname{Cov}(\\mathbf{Z}_1 \\mid \\mathbf{Z}_2) = \\Sigma_{11} - \\Sigma_{12}\\Sigma_{22}^{-1}\\Sigma_{21}\n$$\nThe conditional covariance matrix for $(X,Y)$ given $C$ is $\\operatorname{Cov}((X,Y)^T \\mid C) = \\begin{pmatrix} \\operatorname{Var}(X \\mid C) & \\operatorname{Cov}(X,Y \\mid C) \\\\ \\operatorname{Cov}(Y,X \\mid C) & \\operatorname{Var}(Y \\mid C) \\end{pmatrix}$.\nSince $C$ is a scalar variable, $\\Sigma_{22}^{-1}$ is simply the reciprocal of the variance of $C$:\n$\\Sigma_{22}^{-1} = \\frac{1}{\\operatorname{Var}(C)} = \\frac{1}{\\beta^2\\sigma_{U}^{2} + \\gamma^2\\sigma_{V}^{2} + \\sigma_{C}^{2}}$.\n\nNow we compute the term $\\Sigma_{12}\\Sigma_{22}^{-1}\\Sigma_{21}$:\n$$\n\\Sigma_{12}\\Sigma_{22}^{-1}\\Sigma_{21} = \\frac{1}{\\operatorname{Var}(C)} \\begin{pmatrix} \\alpha\\beta\\sigma_{U}^{2} \\\\ \\gamma\\delta\\sigma_{V}^{2} \\end{pmatrix} \\begin{pmatrix} \\alpha\\beta\\sigma_{U}^{2} & \\gamma\\delta\\sigma_{V}^{2} \\end{pmatrix}\n$$\n$$\n= \\frac{1}{\\operatorname{Var}(C)} \\begin{pmatrix} (\\alpha\\beta\\sigma_{U}^{2})^2 & (\\alpha\\beta\\sigma_{U}^{2})(\\gamma\\delta\\sigma_{V}^{2}) \\\\ (\\gamma\\delta\\sigma_{V}^{2})(\\alpha\\beta\\sigma_{U}^{2}) & (\\gamma\\delta\\sigma_{V}^{2})^2 \\end{pmatrix}\n$$\n$$\n= \\frac{1}{\\beta^2\\sigma_{U}^{2} + \\gamma^2\\sigma_{V}^{2} + \\sigma_{C}^{2}} \\begin{pmatrix} \\alpha^2\\beta^2(\\sigma_{U}^{2})^2 & \\alpha\\beta\\gamma\\delta\\sigma_{U}^{2}\\sigma_{V}^{2} \\\\ \\alpha\\beta\\gamma\\delta\\sigma_{U}^{2}\\sigma_{V}^{2} & \\gamma^2\\delta^2(\\sigma_{V}^{2})^2 \\end{pmatrix}\n$$\nThe conditional covariance matrix is then:\n$$\n\\operatorname{Cov}((X,Y)^T \\mid C) = \\Sigma_{11} - \\Sigma_{12}\\Sigma_{22}^{-1}\\Sigma_{21}\n$$\n$$\n= \\begin{pmatrix} \\alpha^2\\sigma_{U}^{2} + \\sigma_{X}^{2} & 0 \\\\ 0 & \\delta^2\\sigma_{V}^{2} + \\sigma_{Y}^{2} \\end{pmatrix} - \\frac{1}{\\beta^2\\sigma_{U}^{2} + \\gamma^2\\sigma_{V}^{2} + \\sigma_{C}^{2}} \\begin{pmatrix} \\alpha^2\\beta^2(\\sigma_{U}^{2})^2 & \\alpha\\beta\\gamma\\delta\\sigma_{U}^{2}\\sigma_{V}^{2} \\\\ \\alpha\\beta\\gamma\\delta\\sigma_{U}^{2}\\sigma_{V}^{2} & \\gamma^2\\delta^2(\\sigma_{V}^{2})^2 \\end{pmatrix}\n$$\nWe are asked to find $\\operatorname{Cov}(X,Y \\mid C)$, which is the off-diagonal element of this resulting $2 \\times 2$ matrix.\n$$\n\\operatorname{Cov}(X,Y \\mid C) = 0 - \\frac{\\alpha\\beta\\gamma\\delta\\sigma_{U}^{2}\\sigma_{V}^{2}}{\\beta^2\\sigma_{U}^{2} + \\gamma^2\\sigma_{V}^{2} + \\sigma_{C}^{2}}\n$$\nThis expression quantifies the covariance between $X$ and $Y$ induced by conditioning on the common effect (collider) $C$. While $X$ and $Y$ are unconditionally independent, they become conditionally dependent. This phenomenon is known as collider stratification bias. The derived expression is non-zero as long as all path coefficients ($\\alpha, \\beta, \\gamma, \\delta$) and the variances of the unobserved common causes ($\\sigma_U^2, \\sigma_V^2$) are non-zero.",
            "answer": "$$\n\\boxed{- \\frac{\\alpha \\beta \\gamma \\delta \\sigma_{U}^{2} \\sigma_{V}^{2}}{\\beta^{2} \\sigma_{U}^{2} + \\gamma^{2} \\sigma_{V}^{2} + \\sigma_{C}^{2}}}\n$$"
        },
        {
            "introduction": "Once sources of bias are understood, the next step is to correctly estimate causal effects. This exercise introduces the g-computation formula (also known as standardization), a powerful method for estimating interventional quantities from observational data when a valid adjustment set is available. You will apply the formula in a hypothetical biomedical study, connecting the abstract $\\mathrm{do}$-operator to a concrete, model-based estimation procedure . This practice demonstrates how to simulate the effect of an intervention by averaging model-based predictions over the population's confounder distribution.",
            "id": "4557812",
            "problem": "Consider a bioinformatics and medical data analytics cohort study of a kinase inhibitor where the treatment dosage is represented by a continuous variable $X$ (log-dosage), the outcome $Y$ is a dimensionless standardized tumor growth index, and $Z$ is a binary indicator of baseline comorbidity ($Z=1$ for present, $Z=0$ for absent). The causal relationships are described by a Directed Acyclic Graph (DAG): $Z \\rightarrow X$, $Z \\rightarrow Y$, $X \\rightarrow Y$, and $X \\rightarrow M \\rightarrow Y$, where $M$ is a mediator capturing drug-induced signaling downstream of $X$. Assume $Z$ is a valid adjustment set for the causal effect of $X$ on $Y$ according to the back-door criterion, and that the standard identification conditions of consistency, conditional exchangeability given $Z$, and positivity hold.\n\nThe following statistical summaries are plausibly obtained from observational data:\n- The observational distribution of $Z$ is $\\mathbb{P}(Z=1)=\\frac{1}{3}$ and $\\mathbb{P}(Z=0)=\\frac{2}{3}$.\n- The conditional expected outcome model (a saturated linear model in $X$, $Z$, and their interaction, compatible with the DAG and not conditioning on the mediator $M$) is\n$$\n\\mathbb{E}(Y\\mid X,Z)=\\theta_{0}+\\theta_{1}X+\\theta_{2}Z+\\theta_{3}XZ,\n$$\nwith parameter values $\\theta_{0}=10$, $\\theta_{1}=2$, $\\theta_{2}=5$, and $\\theta_{3}=1$.\n\nStarting from the core definitions of the do-operator in causal modeling and the law of total expectation, derive the g-computation estimand for the interventional mean $\\mathbb{E}\\big(Y\\mid \\mathrm{do}(X=x)\\big)$ under the intervention $X=x$. Then, using the provided observational summaries, evaluate this estimand at $x=2$. Round your final numerical answer to four significant figures. Briefly explain, from first principles, how this estimand would be estimated from observational data without access to randomized experiments, ensuring that no mediator is included in the adjustment set and that the identification conditions are met.",
            "solution": "We begin by recalling the core definitions and assumptions required for identification of causal effects in Directed Acyclic Graphs (DAGs). A valid adjustment set $Z$ for the causal effect of $X$ on $Y$ satisfies the back-door criterion, meaning that $Z$ blocks all back-door paths from $X$ to $Y$ and does not include any descendants of $X$. Under the identification conditions of consistency, conditional exchangeability given $Z$, and positivity, the interventional distribution under $\\mathrm{do}(X=x)$ is identified via the g-computation formula, which is grounded in the do-operator and the law of total expectation. Specifically, conditioning on $Z$ suffices to remove confounding, yielding that the interventional mean is obtained by averaging the conditional expectation of $Y$ given $X=x$ over the observational distribution of $Z$.\n\nFormally, the target estimand for the interventional mean under the intervention $X=x$ is derived as follows. By consistency, $Y^{x}$ equals the observed $Y$ when $X$ is set to $x$. By conditional exchangeability given $Z$, we have $Y^{x}\\perp\\!\\!\\!\\perp X\\mid Z$. Positivity requires that $\\mathbb{P}(X=x\\mid Z=z)>0$ whenever $\\mathbb{P}(Z=z)>0$. Applying the law of total expectation to the interventional distribution, we obtain\n$$\n\\mathbb{E}\\big(Y\\mid \\mathrm{do}(X=x)\\big)=\\int \\mathbb{E}\\big(Y\\mid X=x,Z=z\\big)\\, \\mathrm{d}F_{Z}(z),\n$$\nwhere $F_{Z}$ denotes the observational distribution of $Z$. In the discrete case for $Z$, this reduces to a finite sum across the support of $Z$.\n\nGiven the provided model,\n$$\n\\mathbb{E}(Y\\mid X,Z)=\\theta_{0}+\\theta_{1}X+\\theta_{2}Z+\\theta_{3}XZ,\n$$\nwith $\\theta_{0}=10$, $\\theta_{1}=2$, $\\theta_{2}=5$, and $\\theta_{3}=1$, we compute the conditional expectations needed for the g-computation:\n\n1. For $Z=0$ and $X=x$,\n$$\n\\mathbb{E}(Y\\mid X=x,Z=0)=\\theta_{0}+\\theta_{1}x+\\theta_{2}\\cdot 0+\\theta_{3}\\cdot x\\cdot 0=10+2x.\n$$\n\n2. For $Z=1$ and $X=x$,\n$$\n\\mathbb{E}(Y\\mid X=x,Z=1)=\\theta_{0}+\\theta_{1}x+\\theta_{2}\\cdot 1+\\theta_{3}\\cdot x\\cdot 1=10+2x+5+x=15+3x.\n$$\n\nThe observational distribution of $Z$ is $\\mathbb{P}(Z=1)=\\frac{1}{3}$ and $\\mathbb{P}(Z=0)=\\frac{2}{3}$. Therefore, the interventional mean is\n$$\n\\mathbb{E}\\big(Y\\mid \\mathrm{do}(X=x)\\big)=\\mathbb{E}\\big(Y\\mid X=x,Z=0\\big)\\,\\mathbb{P}(Z=0)+\\mathbb{E}\\big(Y\\mid X=x,Z=1\\big)\\,\\mathbb{P}(Z=1).\n$$\nEvaluating at $x=2$:\n- $\\mathbb{E}(Y\\mid X=2,Z=0)=10+2\\cdot 2=14$,\n- $\\mathbb{E}(Y\\mid X=2,Z=1)=15+3\\cdot 2=21$.\n\nHence,\n$$\n\\mathbb{E}\\big(Y\\mid \\mathrm{do}(X=2)\\big)=14\\cdot \\frac{2}{3}+21\\cdot \\frac{1}{3}=\\frac{28}{3}+\\frac{21}{3}=\\frac{49}{3}.\n$$\nTo four significant figures, this is\n$$\n\\mathbb{E}\\big(Y\\mid \\mathrm{do}(X=2)\\big)\\approx 16.33.\n$$\n\nWe now explain how this estimand is estimated from observational data. Under the DAG with $Z$ as a valid adjustment set and with the identification assumptions satisfied, the parametric g-formula (g-computation) estimator proceeds by modeling the conditional expectation of $Y$ given $(X,Z)$ from observational data and then standardizing (i.e., averaging) the model-predicted outcomes over the empirical distribution of $Z$ at the intervention $X=x$. Let $\\hat{m}(x,z)$ denote an estimate of $\\mathbb{E}(Y\\mid X=x,Z=z)$, for example obtained via linear regression using the specified model, yielding estimated parameters $(\\hat{\\theta}_{0},\\hat{\\theta}_{1},\\hat{\\theta}_{2},\\hat{\\theta}_{3})$. Given an independent and identically distributed sample $\\{(Y_{i},X_{i},Z_{i}):i=1,\\dots,n\\}$, the plug-in estimator of the interventional mean is\n$$\n\\hat{\\psi}(x)=\\frac{1}{n}\\sum_{i=1}^{n}\\hat{m}(x,Z_{i}).\n$$\nThis estimator is a sample analog of the law of total expectation applied to the observational distribution of $Z$. Crucially, we do not condition on the mediator $M$ in the adjustment set to avoid biasing the total effect by blocking part of the causal pathway. Under correct model specification for $\\mathbb{E}(Y\\mid X,Z)$, consistency, conditional exchangeability given $Z$, and positivity, $\\hat{\\psi}(x)$ is a consistent estimator of $\\mathbb{E}\\big(Y\\mid \\mathrm{do}(X=x)\\big)$. In practice, flexible nonparametric or machine learning models can be used for $\\hat{m}(x,z)$ along with techniques such as cross-fitting, provided the same identification conditions are respected and the empirical distribution of $Z$ is used for standardization. Finally, diagnostics should confirm positivity, i.e., that for observed $Z=z$ values, treatment levels near $x$ have support in the data, ensuring meaningful extrapolation.",
            "answer": "$$\\boxed{16.33}$$"
        },
        {
            "introduction": "Inverse Probability Weighting (IPW) offers a distinct yet equally powerful approach to causal effect estimation. Instead of modeling the outcome, IPW works by creating a weighted pseudo-population that emulates a randomized experiment. This practice guides you through the derivation of these weights and highlights the crucial difference between standard and stabilized weights, a key technique for reducing the variance of IPW estimators . By calculating and comparing the variance of both types of weights, you will gain insight into the practical trade-offs inherent in this method.",
            "id": "4557720",
            "problem": "Consider a clinical bioinformatics and medical data analytics study of antibiotic assignment where the exposure is binary treatment $X \\in \\{0,1\\}$, the outcome is $Y$, and $Z$ is a measured baseline comorbidity score. A Directed Acyclic Graph (DAG) encodes the causal relations $Z \\to X$, $Z \\to Y$, and $X \\to Y$, with no other measured confounders. Assume $Z$ is a minimal sufficient adjustment set that satisfies the backdoor criterion, and the standard identification conditions of consistency, conditional exchangeability given $Z$, and positivity hold. In the observed data, the distribution of $Z$ is $P(Z=1)=0.4$ and $P(Z=0)=0.6$. The treatment mechanism is $P(X=1 \\mid Z=0)=0.3$, $P(X=1 \\mid Z=1)=0.7$, and by complement $P(X=0 \\mid Z=0)=0.7$, $P(X=0 \\mid Z=1)=0.3$.\n\nStarting from the DAG factorization and the principle of weighting to construct a pseudo-population in which treatment $X$ is independent of $Z$, derive the stabilized inverse probability weights for $X$ given $Z$. Then, using the provided distribution, compute the variance of the unstabilized inverse probability weights and the variance of the stabilized inverse probability weights under the joint distribution of $(X,Z)$, and report the ratio of these variances $\\mathrm{Var}(w_\\mathrm{un}) / \\mathrm{Var}(w_\\mathrm{st})$. Briefly discuss the bias-variance trade-off of stabilized inverse probability weighting relative to g-computation in this setting. Round your final numeric answer to four significant figures. No units are required in the final answer.",
            "solution": "The principle of Inverse Probability Weighting (IPW) is to re-weight the observed population to create a pseudo-population where the treatment assignment $X$ is independent of the confounder $Z$. The unstabilized weight for an individual is the inverse of their probability of receiving the treatment they actually received, conditional on their confounders: $w_{\\mathrm{un}} = 1/P(X|Z)$. The stabilized weight multiplies this by the marginal probability of receiving that treatment, $P(X)$, which reduces variance: $w_{\\mathrm{st}} = P(X)/P(X|Z)$.\n\n**1. Calculate Joint and Marginal Probabilities**\nFirst, we find the marginal probability of treatment, $P(X)$.\n$P(X=1) = P(X=1|Z=1)P(Z=1) + P(X=1|Z=0)P(Z=0) = (0.7)(0.4) + (0.3)(0.6) = 0.28 + 0.18 = 0.46$.\n$P(X=0) = 1 - P(X=1) = 1 - 0.46 = 0.54$.\n\nThe joint probabilities $P(X,Z) = P(X|Z)P(Z)$ are:\n$P(X=1, Z=1) = 0.28$, $P(X=0, Z=1) = 0.3 \\times 0.4 = 0.12$.\n$P(X=1, Z=0) = 0.18$, $P(X=0, Z=0) = 0.7 \\times 0.6 = 0.42$.\n\n**2. Compute Variance of Unstabilized Weights ($w_{\\mathrm{un}}$)**\nThe possible values of $w_{\\mathrm{un}}$ are $1/0.3$ and $1/0.7$.\nThe expectation is $E[w_{\\mathrm{un}}] = \\sum_{x,z} w_{\\mathrm{un}}(x,z) P(x,z) = \\sum_{x,z} \\frac{1}{P(x|z)} P(x|z)P(z) = \\sum_{x,z} P(z) = (P(Z=0)+P(Z=1)) \\times 2 = 2$.\nThe expectation of the square is $E[w_{\\mathrm{un}}^2] = \\sum_{x,z} w_{\\mathrm{un}}(x,z)^2 P(x,z) = \\sum_{x,z} \\frac{P(z)}{P(x|z)}$.\n$E[w_{\\mathrm{un}}^2] = \\frac{P(Z=0)}{P(X=1|Z=0)} + \\frac{P(Z=0)}{P(X=0|Z=0)} + \\frac{P(Z=1)}{P(X=1|Z=1)} + \\frac{P(Z=1)}{P(X=0|Z=1)}$\n$E[w_{\\mathrm{un}}^2] = \\frac{0.6}{0.3} + \\frac{0.6}{0.7} + \\frac{0.4}{0.7} + \\frac{0.4}{0.3} = 2 + \\frac{1.0}{0.7} + \\frac{4}{3} = 2 + \\frac{10}{7} + \\frac{4}{3} = \\frac{42+30+28}{21} = \\frac{100}{21}$.\nThe variance is $\\mathrm{Var}(w_{\\mathrm{un}}) = E[w_{\\mathrm{un}}^2] - (E[w_{\\mathrm{un}}])^2 = \\frac{100}{21} - 2^2 = \\frac{100 - 84}{21} = \\frac{16}{21}$.\n\n**3. Compute Variance of Stabilized Weights ($w_{\\mathrm{st}}$)**\nThe expectation is $E[w_{\\mathrm{st}}] = \\sum_{x,z} w_{\\mathrm{st}}(x,z) P(x,z) = \\sum_{x,z} \\frac{P(x)}{P(x|z)} P(x|z)P(z) = \\sum_{x,z} P(x)P(z) = (\\sum_x P(x))(\\sum_z P(z)) = 1 \\times 1 = 1$.\nThe expectation of the square is $E[w_{\\mathrm{st}}^2] = \\sum_{x,z} w_{\\mathrm{st}}(x,z)^2 P(x,z) = \\sum_{x,z} \\frac{P(x)^2}{P(x|z)}P(z)$.\n$E[w_{\\mathrm{st}}^2] = P(Z=0)\\left[\\frac{P(X=1)^2}{P(X=1|Z=0)} + \\frac{P(X=0)^2}{P(X=0|Z=0)}\\right] + P(Z=1)\\left[\\frac{P(X=1)^2}{P(X=1|Z=1)} + \\frac{P(X=0)^2}{P(X=0|Z=1)}\\right]$\n$E[w_{\\mathrm{st}}^2] = 0.6\\left[\\frac{0.46^2}{0.3} + \\frac{0.54^2}{0.7}\\right] + 0.4\\left[\\frac{0.46^2}{0.7} + \\frac{0.54^2}{0.3}\\right]$\n$E[w_{\\mathrm{st}}^2] = 0.6\\left[\\frac{0.2116}{0.3} + \\frac{0.2916}{0.7}\\right] + 0.4\\left[\\frac{0.2116}{0.7} + \\frac{0.2916}{0.3}\\right]$\n$E[w_{\\mathrm{st}}^2] \\approx 0.6(0.70533 + 0.41657) + 0.4(0.30229 + 0.972) \\approx 0.6(1.1219) + 0.4(1.27429) \\approx 0.67314 + 0.50972 = 1.18286$.\nIn fractions: $E[w_{\\mathrm{st}}^2] = \\frac{207}{175}$.\nThe variance is $\\mathrm{Var}(w_{\\mathrm{st}}) = E[w_{\\mathrm{st}}^2] - (E[w_{\\mathrm{st}}])^2 = \\frac{207}{175} - 1^2 = \\frac{207 - 175}{175} = \\frac{32}{175}$.\n\n**4. Compute the Ratio of Variances**\nThe ratio is:\n$$ \\frac{\\mathrm{Var}(w_{\\mathrm{un}})}{\\mathrm{Var}(w_{\\mathrm{st}})} = \\frac{16/21}{32/175} = \\frac{16}{21} \\times \\frac{175}{32} = \\frac{1}{21} \\times \\frac{175}{2} = \\frac{175}{42} = \\frac{25 \\times 7}{6 \\times 7} = \\frac{25}{6} $$\nAs a decimal, this is $4.1666...$. Rounded to four significant figures, the ratio is 4.167.\n\n**5. Discussion of Bias-Variance Trade-off**\nThe choice between IPW and g-computation centers on a trade-off related to modeling assumptions.\n- **IPW** requires correctly specifying the **propensity score model**, $P(X|Z)$. If this model is correct, IPW estimators are unbiased. However, they can have high variance, especially when propensity scores are near 0 or 1. Stabilized weights, as shown by our calculation where $\\mathrm{Var}(w_{\\mathrm{st}}) \\ll \\mathrm{Var}(w_{\\mathrm{un}})$, help reduce this variance but do not eliminate the method's sensitivity to positivity violations.\n- **G-computation** requires correctly specifying the **outcome model**, $E[Y|X,Z]$. If this model is correct, g-computation estimators are unbiased and typically have lower variance than IPW estimators.\n- **The Trade-off**: An analyst must decide which model—treatment or outcome—is easier to specify correctly. If the process of treatment assignment is well understood, IPW may be preferred despite higher variance. If the outcome mechanism is simpler to model, g-computation may offer more precise estimates. Misspecification of the chosen model leads to bias. Doubly robust methods, such as Augmented IPW, offer a compromise by remaining consistent if either (but not necessarily both) of the models is correctly specified, providing a safety net against model misspecification bias.",
            "answer": "$$\n\\boxed{4.167}\n$$"
        }
    ]
}