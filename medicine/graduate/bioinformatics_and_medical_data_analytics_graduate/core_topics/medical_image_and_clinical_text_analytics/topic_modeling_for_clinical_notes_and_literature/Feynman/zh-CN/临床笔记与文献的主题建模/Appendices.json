{
    "hands_on_practices": [
        {
            "introduction": "潜在狄利克雷分配（Latent Dirichlet Allocation, LDA）的推理过程中，折叠吉布斯采样（collapsed Gibbs sampling）是一种基石算法。本练习旨在深入剖析该过程的核心步骤：为单个词元（token）的隐主题分配计算完整的条件概率。通过完成这个基于具体数值的计算练习，您将牢固掌握模型如何巧妙地结合文档级别和主题级别的统计信息，从而为理解整个推理过程奠定坚实的数学基础。",
            "id": "4613930",
            "problem": "一家医院正在构建一个概率主题模型，用于挖掘临床笔记和文献摘要中的主题。考虑潜在狄利克雷分配（LDA）的生成过程，其中对于每个文档 $d$，文档-主题比例向量 $\\boldsymbol{\\theta}_{d}$ 从狄利克雷分布 $\\mathrm{Dir}(\\boldsymbol{\\alpha})$ 中抽取；对于每个主题 $k$，主题-词语分布 $\\boldsymbol{\\phi}_{k}$ 从狄利克雷分布 $\\mathrm{Dir}(\\beta \\mathbf{1}_{V})$ 中抽取，其中 $\\mathbf{1}_{V}$ 是 $V$ 维的全1向量。词语的生成过程是：首先为第 $i$ 个词元从多项分布 $\\mathrm{Multinomial}(\\boldsymbol{\\theta}_{d})$ 中抽取一个主题 $z_{i}$，然后从多项分布 $\\mathrm{Multinomial}(\\boldsymbol{\\phi}_{z_{i}})$ 中抽取一个词语 $w_{i}$。假设词汇表大小为 $V$，词语先验为对称的 $\\beta$，但文档-主题先验 $\\boldsymbol{\\alpha}$ 可能是不对称的。\n\n对于一个特定的临床笔记 $d$，考虑在坍缩吉布斯采样下对“fever”这个词元的一次出现进行主题分配的重采样，其中 $\\boldsymbol{\\theta}_{d}$ 和所有的 $\\boldsymbol{\\phi}_{k}$ 都被积分掉。给定以下不包含当前词元（用上标 $^{-i}$ 表示）的量：\n\n- 主题数量为 $K=3$。\n- 词汇表大小为 $V=15000$。\n- 对称词语先验为 $\\beta=0.002$。\n- 文档-主题狄利克雷先验为 $\\boldsymbol{\\alpha}=(\\alpha_{1},\\alpha_{2},\\alpha_{3})=(0.4,0.3,0.2)$。\n- 文档 $d$ 的文档-主题计数：$(n_{d,1}^{-i},n_{d,2}^{-i},n_{d,3}^{-i})=(15,3,6)$。\n- 词语“fever”的主题-词语计数：$(n_{1,\\mathrm{fever}}^{-i},n_{2,\\mathrm{fever}}^{-i},n_{3,\\mathrm{fever}}^{-i})=(120,5,10)$。\n- 主题词元总数：$(n_{1}^{-i},n_{2}^{-i},n_{3}^{-i})=(8000,6000,7000)$。\n\n从标准的LDA生成假设和狄利克雷-多项共轭性出发，推导将此词元分配给主题 $k$ 的坍缩吉布斯条件概率，并用它来计算 $k=1,2,3$ 的未归一化权重。将这些权重归一化，以获得此词元在主题上的一个合规分布。以小数形式报告将此词元分配给主题 $k=2$ 的归一化概率，并将最终答案四舍五入到五位有效数字。无需单位。",
            "solution": "该问题是有效的。它描述了潜在狄利克雷分配（LDA）中坍缩吉布斯采样的一个标准应用，这是概率主题建模中的一个核心算法。所有必要的数据和参数都已提供，问题在科学上是合理且定义明确的。\n\n任务是计算在给定所有其他主题分配的情况下，单个词元主题分配的条件概率。在LDA的坍缩吉布斯采样中，连续参数 $\\boldsymbol{\\theta}$（文档-主题分布）和 $\\boldsymbol{\\phi}$（主题-词语分布）被积分掉。这得益于狄利克雷分布和多项分布的共轭性。\n\n设 $z_i$ 是语料库中第 $i$ 个词元的主题分配。我们感兴趣的是对特定词元的主题进行重采样，该词元是文档 $d$ 中词语“fever”的一个实例。设该词元位于位置 $i$，其词语类型为 $w_i=v$（其中 $v$ 对应于“fever”）。在给定所有其他分配 $z_{\\neg i}$ 和所有词语 $W$ 的情况下，将此词元分配给主题 $k$ 的全条件概率由下式给出：\n$$ P(z_i = k | z_{\\neg i}, W, \\boldsymbol{\\alpha}, \\beta) \\propto P(\\text{word } v \\text{ from topic } k) \\times P(\\text{topic } k \\text{ in document } d) $$\n右侧的两项对应于主题和文档的坍缩狄利克雷-多项模型的预测分布。\n\n最终的公式为：\n$$ P(z_i = k | z_{\\neg i}, W, \\boldsymbol{\\alpha}, \\beta) \\propto \\frac{n_{k,v}^{-i} + \\beta}{n_k^{-i} + V\\beta} \\times (n_{d,k}^{-i} + \\alpha_k) $$\n此处，符号表示如下：\n- $n_{k,v}^{-i}$ 是词语类型 $v$ 被分配给主题 $k$ 的次数，不包括当前词元 $i$。\n- $n_k^{-i} = \\sum_{v'} n_{k,v'}^{-i}$ 是分配给主题 $k$ 的词元总数，不包括当前词元 $i$。\n- $n_{d,k}^{-i}$ 是文档 $d$ 中分配给主题 $k$ 的词元数，不包括当前词元 $i$。\n- $\\beta$ 是主题-词语狄利克雷先验的对称超参数。\n- $V$ 是词汇表大小。\n- $\\alpha_k$ 是文档-主题狄利克雷先验中主题 $k$ 的超参数。\n\n我们被赋予以下值：\n- 主题数量 $K=3$。\n- 词汇表大小 $V=15000$。\n- 对称词语先验 $\\beta=0.002$。\n- 文档-主题先验 $\\boldsymbol{\\alpha}=(\\alpha_{1}, \\alpha_{2}, \\alpha_{3})=(0.4, 0.3, 0.2)$。\n- 待重采样的词元对应词语 $v=\\text{\"fever\"}$。\n- 文档-主题计数：$(n_{d,1}^{-i}, n_{d,2}^{-i}, n_{d,3}^{-i})=(15, 3, 6)$。\n- “fever”的主题-词语计数：$(n_{1,v}^{-i}, n_{2,v}^{-i}, n_{3,v}^{-i})=(120, 5, 10)$。\n- 主题总计数：$(n_{1}^{-i}, n_{2}^{-i}, n_{3}^{-i})=(8000, 6000, 7000)$。\n\n首先，我们计算在所有主题中都为常数的项 $V\\beta$：\n$$ V\\beta = 15000 \\times 0.002 = 30 $$\n\n现在，我们计算将该词元分配给每个主题 $k \\in \\{1, 2, 3\\}$ 的未归一化权重 $w_k$。\n$$ w_k = (n_{d,k}^{-i} + \\alpha_k) \\times \\frac{n_{k,v}^{-i} + \\beta}{n_k^{-i} + V\\beta} $$\n\n对于主题 $k=1$：\n$$ w_1 = (n_{d,1}^{-i} + \\alpha_1) \\times \\frac{n_{1,v}^{-i} + \\beta}{n_1^{-i} + V\\beta} = (15 + 0.4) \\times \\frac{120 + 0.002}{8000 + 30} $$\n$$ w_1 = 15.4 \\times \\frac{120.002}{8030} $$\n$$ w_1 \\approx 0.23014082 $$\n\n对于主题 $k=2$：\n$$ w_2 = (n_{d,2}^{-i} + \\alpha_2) \\times \\frac{n_{2,v}^{-i} + \\beta}{n_2^{-i} + V\\beta} = (3 + 0.3) \\times \\frac{5 + 0.002}{6000 + 30} $$\n$$ w_2 = 3.3 \\times \\frac{5.002}{6030} $$\n$$ w_2 \\approx 0.00273741 $$\n\n对于主题 $k=3$：\n$$ w_3 = (n_{d,3}^{-i} + \\alpha_3) \\times \\frac{n_{3,v}^{-i} + \\beta}{n_3^{-i} + V\\beta} = (6 + 0.2) \\times \\frac{10 + 0.002}{7000 + 30} $$\n$$ w_3 = 6.2 \\times \\frac{10.002}{7030} $$\n$$ w_3 \\approx 0.00882111 $$\n\n为了获得归一化概率，我们将未归一化的权重相加：\n$$ S = w_1 + w_2 + w_3 \\approx 0.23014082 + 0.00273741 + 0.00882111 \\approx 0.24169934 $$\n\n将该词元分配给主题 $k=2$ 的归一化概率是：\n$$ P(z_i = 2 | \\dots) = \\frac{w_2}{S} = \\frac{w_2}{w_1 + w_2 + w_3} $$\n$$ P(z_i = 2 | \\dots) \\approx \\frac{0.00273741}{0.24169934} \\approx 0.0113256958 $$\n\n问题要求答案四舍五入到五位有效数字。\n$$ 0.0113256958 \\approx 0.011326 $$\n第五位有效数字是 $5$，其后的数字是 $6 \\ge 5$，所以我们向上取整。",
            "answer": "$$ \\boxed{0.011326} $$"
        },
        {
            "introduction": "在将理论模型应用于真实的临床文本时，我们必须处理数据中的实际挑战，例如罕见词或特殊缩写，这些都可能影响采样器的稳定性和混合速度。本练习探讨了一个关键的权衡：是保留全部数据以追求保真度，还是通过预处理（如剪枝稀有词元）来提高算法的稳定性。通过对剪枝方案造成的影响进行量化分析，您将学习如何评估数据预处理决策对模型行为的理论影响，这是在应用机器学习中一项至关重要的实践技能。",
            "id": "4613921",
            "problem": "给定一个应用于临床笔记和生物医学文献的隐狄利克雷分配（Latent Dirichlet Allocation, LDA）生成模型。对于每个文档 $d \\in \\{1,\\dots,D\\}$，文档-主题分布 $\\theta_d$ 服从参数向量为 $\\boldsymbol{\\alpha} \\in \\mathbb{R}_{>0}^K$ 的狄利克雷分布，其中 $K$ 是主题数量。对于每个主题 $k \\in \\{1,\\dots,K\\}$，主题-词项分布 $\\phi_k$ 服从参数向量为 $\\boldsymbol{\\beta} \\in \\mathbb{R}_{>0}^V$ 的狄利克雷分布，其中 $V$ 是词汇量大小。文档 $d$ 中的每个词符 $i$ 是通过首先采样一个主题分配 $z_i \\sim \\text{Multinomial}(\\theta_d)$，然后采样一个词项 $w_i \\sim \\text{Multinomial}(\\phi_{z_i})$ 来生成的。在LDA的折叠吉布斯采样器中，对于文档 $d$ 中词项为 $w$ 的一个词符，在排除当前词符后，为其分配主题 $k$ 的条件分布与下式成正比：\n$$\np(z_i = k \\mid \\text{rest}) \\propto \\left(n_{d,k}^{-i} + \\alpha_k\\right) \\cdot \\frac{n_{k,w}^{-i} + \\beta_w}{n_{k,\\cdot}^{-i} + \\beta_0},\n$$\n其中 $n_{d,k}^{-i}$ 是文档 $d$ 中分配给主题 $k$ 的词符计数（不包括词符 $i$），$n_{k,w}^{-i}$ 是词项 $w$ 被分配给主题 $k$ 的计数（不包括词符 $i$），$n_{k,\\cdot}^{-i}$ 是分配给主题 $k$ 的总词符数（不包括词符 $i$），$\\alpha_k$ 是文档的狄利克雷参数的第 $k$ 个分量，$\\beta_w$ 是词项的狄利克雷参数的第 $w$ 个分量，且 $\\beta_0 = \\sum_{v=1}^V \\beta_v$。\n\n罕见词符（例如，不常见的临床缩写或不频繁的生物医学术语）会使条件概率 $p(z_i \\mid \\text{rest})$ 高度集中，从而降低采样器的混合效率。考虑一个预采样剪枝方案，该方案移除一个根据语料库统计数据被认为是罕见的词符子集 $\\mathcal{R}$（例如，其语料库频率 $f_w$ 低于阈值 $\\tau$ 并且出现在少于 $m$ 个文档中的词符）。令 $r_d$ 表示从文档 $d$ 中移除的词符数量，令 $r_{\\mathrm{total}} = \\sum_{d=1}^{D} r_d$ 表示从整个语料库中移除的总词符数量。假设剪枝后的计数修改如下：对于任何文档 $d$ 和主题 $k$，文档-主题计数变为 $A_k' = A_k - r_d$，其中 $A_k = n_{d,k}^{-i} + \\alpha_k$；对于任何主题 $k$，主题总数变为 $C_k' = C_k - r_{\\mathrm{total}}$，其中 $C_k = n_{k,\\cdot}^{-i} + \\beta_0$。对于词项类型为 $w$ 的保留词符（即未被剪枝的词符），假设词项-主题计数因子不变，即 $B_k' = B_k$，其中 $B_k = n_{k,w}^{-i} + \\beta_w$，因为我们只剪枝罕见的词项类型，而 $w$ 不在其中。\n\n定义剪枝前的未归一化条件权重为 $u_k = A_k \\cdot \\frac{B_k}{C_k}$，剪枝后的为 $v_k = (A_k - r_d) \\cdot \\frac{B_k}{C_k - r_{\\mathrm{total}}}$。考虑分别从 $\\boldsymbol{u}$ 和 $\\boldsymbol{v}$ 通过除以其总和得到的归一化条件分布 $\\boldsymbol{p}$ 和 $\\boldsymbol{q}$。令比率\n$$\nR_{d,k} = \\frac{v_k}{u_k} = \\frac{A_k - r_d}{A_k} \\cdot \\frac{C_k}{C_k - r_{\\mathrm{total}}}\n$$\n捕捉乘性扰动。令\n$$\n\\varepsilon = \\max_{d,k} \\left( \\max\\left\\{ R_{d,k}, \\frac{1}{R_{d,k}} \\right\\} - 1 \\right).\n$$\n在对所有 $d$ 和 $k$ 均满足 $A_k - r_d > 0$ 和 $C_k - r_{\\mathrm{total}} > 0$ 的条件下，一个通用的乘性扰动论证给出了 $\\boldsymbol{p}$ 和 $\\boldsymbol{q}$ 之间全变分距离的一个界限：\n$$\n\\mathrm{TV}(\\boldsymbol{p}, \\boldsymbol{q}) \\le \n\\begin{cases}\n\\frac{\\varepsilon}{1 - \\varepsilon},  &\\text{if } \\varepsilon < 1, \\\\\n1,  &\\text{if } \\varepsilon \\ge 1.\n\\end{cases}\n$$\n在上述设定中，这个界限对于指定文档和主题中的所有保留词符是一致的，因为 $r_d$ 和 $r_{\\mathrm{total}}$ 的定义不依赖于特定的保留词符。\n\n任务：实现一个程序，根据给定的几个测试用例的计数和剪枝参数，计算保留词符的后验条件 $z$ 变化的最坏情况上界，该上界由上述 $\\mathrm{TV}(\\boldsymbol{p}, \\boldsymbol{q})$ 的界限量化。\n\n使用以下测试套件，其中指定了每个用例所需的所有参数：\n- 用例 1（中度剪枝，典型计数）：\n    - $K = 4$, $D = 3$, $V = 8$。\n    - $\\boldsymbol{\\alpha} = [0.3, 0.3, 0.3, 0.3]$。\n    - 均匀词项先验，所有 $v$ 的 $\\beta_v = 0.05$，因此 $\\beta_0 = 0.4$。\n    - 文档-主题计数 $n_{d,k}^{-i}$：\n      - 文档 1：$[20, 18, 22, 15]$，\n      - 文档 2：$[12, 16, 14, 10]$，\n      - 文档 3：$[25, 20, 30, 16]$。\n    - 主题总数 $n_{k,\\cdot}^{-i} = [250, 200, 300, 180]$。\n    - 每个文档的剪枝计数 $\\boldsymbol{r} = [2, 1, 3]$，且 $r_{\\mathrm{total}} = 6$。\n- 用例 2（重度剪枝，边界行为）：\n    - $K = 4$, $D = 3$, $V = 8$。\n    - $\\boldsymbol{\\alpha} = [0.3, 0.3, 0.3, 0.3]$。\n    - 均匀词项先验，所有 $v$ 的 $\\beta_v = 0.05$，$\\beta_0 = 0.4$。\n    - 文档-主题计数 $n_{d,k}^{-i}$：\n      - 文档 1：$[20, 18, 22, 15]$，\n      - 文档 2：$[12, 16, 14, 10]$，\n      - 文档 3：$[25, 20, 30, 16]$。\n    - 主题总数 $n_{k,\\cdot}^{-i} = [250, 200, 300, 180]$。\n    - 每个文档的剪枝计数 $\\boldsymbol{r} = [12, 9, 15]$，且 $r_{\\mathrm{total}} = 36$。\n- 用例 3（无剪枝，中性行为）：\n    - $K = 4$, $D = 3$, $V = 8$。\n    - $\\boldsymbol{\\alpha} = [0.3, 0.3, 0.3, 0.3]$。\n    - 均匀词项先验，所有 $v$ 的 $\\beta_v = 0.05$，$\\beta_0 = 0.4$。\n    - 文档-主题计数 $n_{d,k}^{-i}$：\n      - 文档 1：$[20, 18, 22, 15]$，\n      - 文档 2：$[12, 16, 14, 10]$，\n      - 文档 3：$[25, 20, 30, 16]$。\n    - 主题总数 $n_{k,\\cdot}^{-i} = [250, 200, 300, 180]$。\n    - 每个文档的剪枝计数 $\\boldsymbol{r} = [0, 0, 0]$，且 $r_{\\mathrm{total}} = 0$。\n- 用例 4（小文档边缘情况与罕见主题总数）：\n    - $K = 3$, $D = 1$, $V = 6$。\n    - $\\boldsymbol{\\alpha} = [0.1, 0.1, 0.1]$。\n    - 均匀词项先验，所有 $v$ 的 $\\beta_v = 0.01$，因此 $\\beta_0 = 0.06$。\n    - 文档-主题计数 $n_{d,k}^{-i}$：\n      - 文档 1：$[50, 5, 5]$。\n    - 主题总数 $n_{k,\\cdot}^{-i} = [40, 15, 8]$。\n    - 每个文档的剪枝计数 $\\boldsymbol{r} = [3]$，且 $r_{\\mathrm{total}} = 3$。\n\n你的程序必须为每个用例计算在所有文档和主题中 $\\mathrm{TV}(\\boldsymbol{p}, \\boldsymbol{q})$ 的最坏情况上界，方法是使用上述公式，取 $d$ 和 $k$ 上 $\\varepsilon$ 的最大值，然后如果 $\\varepsilon < 1$ 则映射到界限 $\\mathrm{TV} \\le \\frac{\\varepsilon}{1 - \\varepsilon}$，否则映射到 $\\mathrm{TV} \\le 1$。\n\n最终输出格式：你的程序应生成单行输出，包含一个用方括号括起来的逗号分隔列表的结果（例如，$[result_1,result_2,result_3,result_4]$）。每个 $result_j$ 必须是表示用例 $j$ 的全变分距离最坏情况上界的浮点数，四舍五入到六位小数。",
            "solution": "该问题陈述被评估为有效。它在科学上基于贝叶斯统计和主题建模的原理，特别是隐狄利克雷分配（LDA）。折叠吉布斯采样器更新规则的表述是标准的。对词符剪枝方案效果的分析被构建为扰动分析，这是一种成熟的数学技术。该问题是良构的，提供了所有必要的定义、数据和清晰、客观的计算任务。测试用例的所有参数和计数都已明确提供，问题设置是自洽的，没有矛盾或歧义。条件 $A_k - r_d > 0$ 和 $C_k - r_{\\mathrm{total}} > 0$ 已被陈述并在所有测试用例中成立，确保了量的良定性。因此，可以直接求解。\n\n目标是计算在词符剪枝方案前后，后验条件主题分布 $\\boldsymbol{p}$ 和 $\\boldsymbol{q}$ 之间全变分距离 $\\mathrm{TV}(\\boldsymbol{p}, \\boldsymbol{q})$ 的最坏情况上界。这个界限是作为最大扰动项 $\\varepsilon$ 的函数给出的。\n\n为一个给定文档 $d$ 中的词符分配主题 $k$ 的未归一化条件权重定义如下：\n剪枝前：$u_k = A_k \\cdot \\frac{B_k}{C_k}$\n剪枝后：$v_k = (A_k - r_d) \\cdot \\frac{B_k}{C_k - r_{\\mathrm{total}}}$\n\n量 $A_k$、$B_k$ 和 $C_k$ 基于LDA模型的计数定义：\n- $A_k = n_{d,k}^{-i} + \\alpha_k$，其中 $n_{d,k}^{-i}$ 是文档 $d$ 中分配给主题 $k$ 的词符计数（不包括当前词符），$\\alpha_k$ 是主题 $k$ 的狄利克雷先验。\n- $B_k = n_{k,w}^{-i} + \\beta_w$。问题陈述中提到，所考虑的词符不是罕见的，因此这个因子不受剪枝影响，在考虑后验概率的比率时会抵消掉。\n- $C_k = n_{k,\\cdot}^{-i} + \\beta_0$，其中 $n_{k,\\cdot}^{-i}$ 是分配给主题 $k$ 的总词符计数（不包括当前词符），$\\beta_0$ 是词汇表中所有词项的狄利克雷先验之和。\n\n对于给定的文档-主题对 $(d,k)$，乘性扰动是比率 $R_{d,k} = v_k / u_k$。由于假设项 $B_k$ 相同，它被抵消：\n$$\nR_{d,k} = \\frac{(A_k - r_d) \\cdot \\frac{B_k}{C_k - r_{\\mathrm{total}}}}{A_k \\cdot \\frac{B_k}{C_k}} = \\frac{A_k - r_d}{A_k} \\cdot \\frac{C_k}{C_k - r_{\\mathrm{total}}}\n$$\n其中 $r_d$ 是从文档 $d$ 中剪枝的词符数，$r_{\\mathrm{total}}$ 是从语料库中剪枝的总词符数。\n\n最大扰动 $\\varepsilon$ 定义为 $R_{d,k}$ 偏离 $1$ 的最坏情况偏差，取遍所有可能的文档 $d$ 和主题 $k$：\n$$\n\\varepsilon = \\max_{d \\in \\{1,\\dots,D\\}, k \\in \\{1,\\dots,K\\}} \\left( \\max\\left\\{ R_{d,k}, \\frac{1}{R_{d,k}} \\right\\} - 1 \\right)\n$$\n这捕捉了最大的乘性变化，无论是增加还是减少。\n\n最后，问题提供了一个全变分距离上界的公式：\n$$\n\\mathrm{TV}(\\boldsymbol{p}, \\boldsymbol{q}) \\le \n\\begin{cases}\n\\frac{\\varepsilon}{1 - \\varepsilon},  &\\text{if } \\varepsilon < 1, \\\\\n1,  &\\text{if } \\varepsilon \\ge 1.\n\\end{cases}\n$$\n\n每个测试用例的计算过程如下：\n1. 初始化一个变量 `max_epsilon` 为 $0$。\n2. 对于每个文档 $d$ (从 $1$ 到 $D$) 和每个主题 $k$ (从 $1$ 到 $K$)：\n   a. 检索相应的计数和参数：$n_{d,k}^{-i}$、$n_{k,\\cdot}^{-i}$、$\\alpha_k$、$r_d$、$r_{\\mathrm{total}}$ 和 $\\beta_0$。\n   b. 计算 $A_k = n_{d,k}^{-i} + \\alpha_k$。\n   c. 计算 $C_k = n_{k,\\cdot}^{-i} + \\beta_0$。\n   d. 如果 $r_{\\mathrm{total}} = 0$，这是一个特殊情况。此时，对所有 $d$ 都有 $r_d=0$，这使得对所有 $(d,k)$ 都有 $R_{d,k} = 1$。这意味着 $\\varepsilon=0$。\n   e. 如果 $r_{\\mathrm{total}} \\neq 0$，使用上述公式计算 $R_{d,k}$。条件 $A_k-r_d > 0$ 和 $C_k-r_{\\mathrm{total}} > 0$ 确保 $R_{d,k}$ 是良定义且为正的。\n   f. 计算当前 $(d,k)$ 对的扰动：$\\varepsilon_{d,k} = \\max\\left\\{ R_{d,k}, 1/R_{d,k} \\right\\} - 1$。\n   g. 如果当前扰动更大，则更新 `max_epsilon`：`max_epsilon = max(max_epsilon,` $\\varepsilon_{d,k})$。\n3. 遍历完所有 $(d,k)$ 对后，`max_epsilon` 将持有该测试用例的 $\\varepsilon$ 值。\n4. 使用分段公式计算全变分距离的最终界限。如果 $\\varepsilon < 1$，界限是 $\\varepsilon / (1 - \\varepsilon)$；否则，界限是 $1$。\n5. 结果按规定四舍五入到六位小数。\n\n此过程将应用于提供的四个测试用例中的每一个。\n\n用例4中对 $(d=1, k=2)$ 的计算示例：\n- 给定：$n_{1,2}^{-i} = 5$，$\\alpha_2 = 0.1$，$n_{2,\\cdot}^{-i} = 15$，$\\beta_0 = 0.06$，$r_1=3$，$r_{\\mathrm{total}}=3$。\n- $A_2 = 5 + 0.1 = 5.1$。\n- $C_2 = 15 + 0.06 = 15.06$。\n- $R_{1,2} = \\frac{5.1 - 3}{5.1} \\cdot \\frac{15.06}{15.06 - 3} = \\frac{2.1}{5.1} \\cdot \\frac{15.06}{12.06} \\approx 0.41176 \\times 1.24876 \\approx 0.51419$。\n- 扰动 $\\varepsilon_{1,2} = 1/0.51419 - 1 \\approx 1.9448 - 1 = 0.9448$。\n此过程对所有 $(d,k)$ 对重复，以找到最大的 $\\varepsilon$。\n对于用例4，最大 $\\varepsilon \\approx 0.94479$ 出现在 $(d=1,k=2)$。\n界限是 $\\frac{0.94479}{1 - 0.94479} \\approx \\frac{0.94479}{0.05521} \\approx 17.113402$。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the worst-case upper bound on the total variation distance \n    of LDA posterior conditionals due to a token pruning scheme.\n    \"\"\"\n    \n    test_cases = [\n        {\n            # Case 1 (moderate pruning, typical counts)\n            \"K\": 4, \"D\": 3,\n            \"alpha\": np.array([0.3, 0.3, 0.3, 0.3]),\n            \"beta0\": 0.4,\n            \"n_dk\": np.array([[20, 18, 22, 15], [12, 16, 14, 10], [25, 20, 30, 16]]),\n            \"n_k\": np.array([250, 200, 300, 180]),\n            \"r\": np.array([2, 1, 3]),\n            \"r_total\": 6,\n        },\n        {\n            # Case 2 (heavy pruning, boundary behavior)\n            \"K\": 4, \"D\": 3,\n            \"alpha\": np.array([0.3, 0.3, 0.3, 0.3]),\n            \"beta0\": 0.4,\n            \"n_dk\": np.array([[20, 18, 22, 15], [12, 16, 14, 10], [25, 20, 30, 16]]),\n            \"n_k\": np.array([250, 200, 300, 180]),\n            \"r\": np.array([12, 9, 15]),\n            \"r_total\": 36,\n        },\n        {\n            # Case 3 (no pruning, neutral behavior)\n            \"K\": 4, \"D\": 3,\n            \"alpha\": np.array([0.3, 0.3, 0.3, 0.3]),\n            \"beta0\": 0.4,\n            \"n_dk\": np.array([[20, 18, 22, 15], [12, 16, 14, 10], [25, 20, 30, 16]]),\n            \"n_k\": np.array([250, 200, 300, 180]),\n            \"r\": np.array([0, 0, 0]),\n            \"r_total\": 0,\n        },\n        {\n            # Case 4 (small-document edge case with rare-topic totals)\n            \"K\": 3, \"D\": 1,\n            \"alpha\": np.array([0.1, 0.1, 0.1]),\n            \"beta0\": 0.06,\n            \"n_dk\": np.array([[50, 5, 5]]),\n            \"n_k\": np.array([40, 15, 8]),\n            \"r\": np.array([3]),\n            \"r_total\": 3,\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        K = case[\"K\"]\n        D = case[\"D\"]\n        alpha = case[\"alpha\"]\n        beta0 = case[\"beta0\"]\n        n_dk_matrix = case[\"n_dk\"]\n        n_k_vector = case[\"n_k\"]\n        r_vector = case[\"r\"]\n        r_total = case[\"r_total\"]\n        \n        max_epsilon = 0.0\n        \n        # If no tokens are pruned, the perturbation is zero.\n        if r_total == 0:\n            epsilon = 0.0\n        else:\n            for d in range(D):\n                for k in range(K):\n                    n_dk = n_dk_matrix[d, k]\n                    n_k = n_k_vector[k]\n                    alpha_k = alpha[k]\n                    r_d = r_vector[d]\n                    \n                    A_k = n_dk + alpha_k\n                    C_k = n_k + beta0\n                    \n                    if A_k == 0 or (A_k - r_d) == 0 or (C_k - r_total) == 0:\n                       continue\n\n                    R_dk = ((A_k - r_d) / A_k) * (C_k / (C_k - r_total))\n                    \n                    current_epsilon = max(R_dk, 1.0 / R_dk) - 1.0\n                    \n                    if current_epsilon > max_epsilon:\n                        max_epsilon = current_epsilon\n            \n            epsilon = max_epsilon\n\n        if epsilon  1.0:\n            tv_bound = epsilon / (1.0 - epsilon)\n        else:\n            tv_bound = 1.0\n            \n        results.append(round(tv_bound, 6))\n\n    output_parts = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(output_parts)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在掌握了LDA的核心机制和数据处理策略之后，最后一个关键挑战是如何将其高效地应用于海量语料库。本练习将您的注意力从统计正确性转向计算效率，这是一个在处理真实世界大规模数据集（如整个医院的电子病历或大型医学文献库）时无法回避的问题。通过推导并实现朴素采样器与高级稀疏优化采样器的时间复杂度模型，您将深刻理解算法优化的原理及其带来的巨大性能提升，这对于将主题模型从理论研究推向实际应用至关重要。",
            "id": "4613954",
            "problem": "您正在分析用于主题建模的潜在狄利克雷分布（Latent Dirichlet Allocation, LDA）及其折叠吉布斯采样（collapsed Gibbs sampling, GS）方法。该主题建模应用于一个由临床笔记和生物医学文献摘要组成的语料库。设语料库中共有 $N$ 个词元（token），固定数量的 $K$ 个主题，以及大小为 $V$ 的词汇表。在 LDA 的折叠吉布斯采样中，每个词元的主题分配的条件分布是通过文档-主题计数和主题-词计数来计算的，然后从中采样一个新的主题。朴素实现方法在每次迭代中为语料库中的每个词元评估所有 $K$ 个主题的未归一化概率，这导致了每次迭代的计算成本与 $K$ 成正比。\n\n基本上，以下经过充分验证的事实适用：\n- 在 LDA 的折叠吉布斯采样中，更新每个词元的分配需要评估一个依赖于文档-主题计数和主题-词计数的未归一化概率，然后从生成的关于 $K$ 个主题的分类分布中抽取一个样本。\n- 朴素实现为每个词元评估所有 $K$ 个主题，这意味着每个词元的成本与 $K$ 呈线性关系。\n- 在临床笔记和生物医学摘要等典型语料库中，稀疏性之所以出现，是因为每个文档只使用一小部分主题，每个词也只出现在一小部分主题中。定义 $s_d$ 为每个词元所在文档遇到的非零文档-主题计数的平均数量，定义 $s_w$ 为每个词元对应的词遇到的非零主题-词计数的平均数量。根据经验，$s_d$ 和 $s_w$ 通常远小于 $K$。\n\n您的任务是：\n1. 从上述基本事实出发，根据第一性原理推导朴素折叠吉布斯采样器每次迭代操作计数的符号表达式，该表达式应以 $N$、$K$ 和每个候选评估常量 $c_{\\text{cand}}$ 表示。您的推导应仔细证明其与 $K$ 的线性依赖关系，并解释 $c_{\\text{cand}}$ 的作用。\n2. 提出一种利用稀疏性的优化方法，并从理论上证明其合理性。该方法通过将分布分解为稀疏分量和密集背景分量来实现。假设密集分量可以使用 Walker 别名方法（也称为别名表）在期望常数时间内采样，该方法每次迭代构建一次，成本与 $K$ 成线性关系。设 $c_{\\text{bucket}}$ 为每个词元组合稀疏分量的恒定操作数，$c_{\\text{alias}}$ 为每个词元从别名表中采样的期望恒定操作数，以及 $c_{\\text{build}}$ 为每次迭代中为每个主题构建别名表的恒定操作数。推导这种稀疏加别名优化方法每次迭代操作计数的符号表达式，该表达式应以 $N$、$K$、$s_d$、$s_w$、$c_{\\text{cand}}$、$c_{\\text{bucket}}$、$c_{\\text{alias}}$ 和 $c_{\\text{build}}$ 表示。\n3. 实现一个完整的、可运行的程序，为下面测试套件中的每个测试用例计算以下三个量：\n   - 朴素方法每次迭代的操作计数 $C_{\\text{naive}}$。\n   - 使用稀疏加别名方法时，优化后每次迭代的操作计数 $C_{\\text{opt}}$。\n   - 加速比 $R = \\frac{C_{\\text{naive}}}{C_{\\text{opt}}}$，以小数形式表示（而非百分比）。\n4. 在您的计算中使用以下固定常量值：$c_{\\text{cand}} = 1$，$c_{\\text{bucket}} = 3$，$c_{\\text{alias}} = 1$，以及 $c_{\\text{build}} = 2$。所有计数都是标量操作计数，因此是无量纲的。\n5. 测试套件包含以下参数集 $(N, K, s_d, s_w)$，旨在覆盖不同的情景：\n   - 案例 1（典型临床笔记规模）：$(10^6, 100, 5, 10)$。\n   - 案例 2（大主题库，稀疏使用）：$(10^5, 10^3, 3, 3)$。\n   - 案例 3（单主题边缘情况）：$(10^4, 1, 1, 1)$。\n   - 案例 4（无稀疏性边界条件）：$(5 \\cdot 10^5, 200, 200, 200)$。\n6. 您的程序必须为每个测试用例计算第 3 项中的三个量，并生成一行输出，将所有用例的结果聚合为一个用逗号分隔的列表，并用方括号括起来。每个用例的结果本身必须是一个包含三个小数值 $[C_{\\text{naive}}, C_{\\text{opt}}, R]$ 的列表，每个小数值四舍五入到六位小数。最终输出格式应为：\n   - 示例：$[[x_1,y_1,z_1],[x_2,y_2,z_2],[x_3,y_3,z_3],[x_4,y_4,z_4]]$。\n\n通过证明分解的合理性以及在支付每次迭代的一次性构建成本后使用别名方法作为常数时间采样器的合理性，来确保科学真实性。该问题必须在数学上进行框架设计，并且无需外部数据即可解决，其难度应符合生物信息学和医学数据分析领域中针对临床笔记和文献主题建模方向的高级研究生水平。",
            "solution": "该问题已经过验证，被确定为生物信息学和医学数据分析领域中一个定义明确、有科学依据的问题。它要求推导并应用潜在狄利克雷分布（LDA）采样器的计算复杂度模型。\n\n### 第 1 部分：朴素折叠吉布斯采样器操作计数的推导\n\nLDA 折叠吉布斯采样器的朴素实现为每次迭代对语料库进行一次完整遍历。单次迭代包括更新语料库中每个词元的主题分配。\n\n设 $N$ 为语料库中的词元总数，$K$ 为主题数量。\n对于 $N$ 个词元中的每一个，采样器必须计算该词元属于 $K$ 个可能主题中每一个的条件概率。这涉及到为 $K$ 个候选主题中的每一个计算未归一化的概率得分。\n问题将 $c_{\\text{cand}}$ 定义为一个常数，表示评估一个此类候选概率所需的基本操作数。该常数封装了诸如从内存中获取计数、执行乘法和除法等操作，这些操作由 LDA 条件概率公式指定：\n$$P(z_i=k | \\mathbf{z}_{\\neg i}, \\mathbf{w}) \\propto (n_{d,k}^{\\neg i} + \\alpha) \\frac{n_{k,w}^{\\neg i} + \\beta}{n_k^{\\neg i} + V\\beta}$$\n其中 $z_i$ 是第 $i$ 个词元的主题分配，$d$ 是其所在文档，$w$ 是其词类型，$n$ 是各种计数，而 $\\alpha, \\beta$ 是超参数。对单个主题 $k$ 计算此表达式的成本对应于 $c_{\\text{cand}}$。\n\n单个词元的总操作成本 $C_{\\text{token}}$ 是评估所有 $K$ 个主题的成本：\n$$C_{\\text{token}} = K \\cdot c_{\\text{cand}}$$\n\n采样器一次完整迭代的总操作成本 $C_{\\text{naive}}$ 是每个词元的成本乘以词元总数 $N$：\n$$C_{\\text{naive}} = N \\cdot C_{\\text{token}}$$\n代入 $C_{\\text{token}}$ 的表达式，我们得到朴素采样器每次迭代操作计数的符号表达式：\n$$C_{\\text{naive}} = N \\cdot K \\cdot c_{\\text{cand}}$$\n此推导清晰地表明了成本对 $N$ 和 $K$ 的线性依赖关系。\n\n### 第 2 部分：稀疏加别名优化采样器操作计数的推导\n\n该优化利用了文本语料库主题模型中的内在稀疏性：任何给定的文档通常只涉及少数几个主题，任何给定的词也只与少数几个主题强相关。这意味着对于一个词元 $i$（在文档 $d$ 中的词类型 $w$），计数 $n_{d,k}$ 和 $n_{k,w}$ 仅对 $K$ 个主题中的一小部分是非零的。完整的条件概率分布可以分解为稀疏分量（这些计数非零的部分）和一个密集的背景分量（由平滑超参数 $\\alpha$ 和 $\\beta$ 驱动）。\n\n优化采样器每次迭代的总成本 $C_{\\text{opt}}$ 是一次性设置成本与采样所有 $N$ 个词元的累积成本之和。\n\n**每次迭代的设置成本：**\n概率分布的密集背景分量与一个仅依赖于主题级别计数的项（例如，$(\\sum_v n_{k,v} + V\\beta)^{-1}$）成正比。如果准备好合适的数据结构，可以高效地对这 $K$ 个主题的分布进行采样。问题指定使用 Walker 别名方法。为具有 $K$ 个结果的分类分布构建别名表的计算成本与 $K$ 成线性关系。设 $c_{\\text{build}}$ 为构建该表时每个主题所需的恒定操作数。每次迭代的一次性总设置成本为：\n$$C_{\\text{build\\_iter}} = K \\cdot c_{\\text{build}}$$\n\n**每个词元的采样成本：**\n对于 $N$ 个词元中的每一个，采样器执行一系列操作。\n1.  **稀疏分量评估：**我们不再评估所有 $K$ 个主题，而是只显式评估那些文档-主题计数（$n_{d,k}$）或主题-词计数（$n_{k,w}$）为非零的主题。问题提供了 $s_d$ 作为每个词元遇到的非零文档-主题计数的平均数，以及 $s_w$ 作为每个词元遇到的非零主题-词计数的平均数。因此，平均需要评估的稀疏候选主题总数为 $s_d + s_w$。（为简单起见，我们假设可加性，这在此类复杂度分析中很常见，忽略了这两组主题之间可能存在的重叠）。每个候选的成本为 $c_{\\text{cand}}$，因此此步骤的成本是 $(s_d + s_w) \\cdot c_{\\text{cand}}$。\n\n2.  **分量组合：**在评估稀疏分量之后，必须计算它们对总概率质量的贡献，并与密集分量的质量相结合。问题将此逻辑的成本（例如，管理稀疏概率的数据结构、求和以及为最终采样决策做准备）抽象为单个恒定成本 $c_{\\text{bucket}}$。\n\n3.  **采样步骤：**最后一步是从组合（混合）分布中抽取一个新主题。这涉及决定是从稀疏集还是密集集中采样，然后执行采样。问题简化了这一复杂步骤，指出通过别名表从密集分量中采样的期望恒定成本为 $c_{\\text{alias}}$。此成本被理解为在此优化方案中每个词元的最终有效采样操作。\n\n将这些成本相加，得到每个词元的总平均操作成本 $C_{\\text{token\\_opt}}$：\n$$C_{\\text{token\\_opt}} = (s_d + s_w) \\cdot c_{\\text{cand}} + c_{\\text{bucket}} + c_{\\text{alias}}$$\n\n**总优化成本：**\n优化采样器每次迭代的总操作计数 $C_{\\text{opt}}$ 是初始构建成本与采样所有 $N$ 个词元的总成本之和：\n$$C_{\\text{opt}} = C_{\\text{build\\_iter}} + N \\cdot C_{\\text{token\\_opt}}$$\n代入推导出的表达式，我们得到最终的符号公式：\n$$C_{\\text{opt}} = K \\cdot c_{\\text{build}} + N \\cdot ((s_d + s_w) \\cdot c_{\\text{cand}} + c_{\\text{bucket}} + c_{\\text{alias}})$$\n当 $s_d, s_w \\ll K$ 时，此表达式有效，此时每个词元的成本近似为常数或增长远慢于 $K$，从而比朴素方法有显著的加速。如果不存在稀疏性（即 $s_d, s_w \\approx K$），则此方法的开销使其效率低于朴素方法。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and prints the operational costs and speedup for naive and \n    optimized LDA Gibbs samplers based on derived complexity formulas.\n    \"\"\"\n\n    c_cand = 1.0\n    c_bucket = 3.0\n    c_alias = 1.0\n    c_build = 2.0\n\n    test_cases = [\n        (10**6, 100, 5, 10),\n        (10**5, 10**3, 3, 3),\n        (10**4, 1, 1, 1),\n        (5 * 10**5, 200, 200, 200),\n    ]\n\n    results = []\n\n    for case in test_cases:\n        N, K, s_d, s_w = (float(c) for c in case)\n\n        c_naive = N * K * c_cand\n\n        per_token_cost_opt = (s_d + s_w) * c_cand + c_bucket + c_alias\n        c_opt = K * c_build + N * per_token_cost_opt\n        \n        if c_opt == 0:\n            speedup_ratio = float('inf')\n        else:\n            speedup_ratio = c_naive / c_opt\n\n        results.append([c_naive, c_opt, speedup_ratio])\n\n    output_parts = []\n    for res_list in results:\n        output_parts.append(f\"[{res_list[0]:.6f},{res_list[1]:.6f},{res_list[2]:.6f}]\")\n    \n    print(f\"[{','.join(output_parts)}]\")\n\n\nsolve()\n```"
        }
    ]
}