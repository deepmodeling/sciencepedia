{
    "hands_on_practices": [
        {
            "introduction": "现实世界中的医学影像，尤其是像三维磁共振成像（3D MRI）这样的容积扫描数据，其体素间距往往是各向异性的。这为通常假设数据各向同性的三维卷积神经网络（3D CNN）带来了挑战。本实践练习将指导您完成一个关键的设计任务：设计一个输入数据块（patch），使其能够在数据的物理现实与网络的架构需求之间找到平衡，这是任何三维分割流程中至关重要的一步。",
            "id": "4582642",
            "problem": "一个用于腹部器官分割的三维磁共振成像 (3D MRI) 数据集是以各向异性的体素间距采集的，其中平面内间距为 $s_{x} = 0.9\\,\\text{mm}$ 和 $s_{y} = 0.9\\,\\text{mm}$，平面间间距为 $s_{z} = 4.5\\,\\text{mm}$。一个基于 U-Net 分割架构 (U-Net) 的三维卷积神经网络 (3D CNN) 将使用从该三维数据中提取的区块进行训练。编码器使用三个下采样阶段，每个阶段沿 $(x,y,z)$ 轴的步幅为 $(2, 2, 1)$，所有卷积均使用步幅为 $1$ 和 \"same\" 填充。为确保所有金字塔层级的特征图维度一致，区块沿 $x$ 和 $y$ 轴的维度必须能被 $2^{3}$ 整除，而 $z$ 轴维度则没有来自下采样的可整除性约束。\n\n设计一个区块，使其物理视场 (FOV) 跨轴各向同性，并满足最小物理范围要求，以捕获足够的解剖学背景。具体来说，设以体素为单位的区块维度为 $(P_{x}, P_{y}, P_{z})$，并要求：\n- $P_{x} s_{x} = P_{y} s_{y} = P_{z} s_{z}$ (各向同性物理 FOV),\n- $P_{x} s_{x} \\geq 72\\,\\text{mm}$ (最小物理 FOV),\n- $P_{x}$ 和 $P_{y}$ 可被 $2^{3}$ 整除。\n\n在这些约束条件下，确定满足所有条件的最小整数三元组 $(P_{x}, P_{y}, P_{z})$（以体素为单位）。使用 LaTeX $\\texttt{pmatrix}$ 环境将最终答案表示为行矩阵。不需要四舍五入，最终答案框中不应包含任何单位。",
            "solution": "问题是确定用于训练三维卷积神经网络的区块的最小整数维度 $(P_{x}, P_{y}, P_{z})$（以体素为单位）。该设计必须满足与体素间距、网络架构和物理视场相关的几个约束条件。\n\n首先，我们将给定的信息和约束条件形式化。体素间距为 $s_{x} = 0.9\\,\\text{mm}$，$s_{y} = 0.9\\,\\text{mm}$ 和 $s_{z} = 4.5\\,\\text{mm}$。区块维度 $(P_{x}, P_{y}, P_{z})$ 必须是整数。\n\n约束条件如下：\n1.  **架构约束**：网络的编码器有三个下采样阶段，沿 $x$ 和 $y$ 轴的步幅为 $2$。因此，初始区块维度 $P_{x}$ 和 $P_{y}$ 必须能被 $2^{3} = 8$ 整除，以确保网络各层级的维度有效。由于沿 $z$ 轴的步幅为 $1$，因此对 $P_z$ 没有这样的约束。\n2.  **各向同性 FOV 约束**：区块的物理视场 (FOV) 必须是各向同性的。这意味着沿每个轴的物理尺寸必须相等。设这个公共物理尺寸为 $L$。\n    $$L = P_{x} s_{x} = P_{y} s_{y} = P_{z} s_{z}$$\n3.  **最小 FOV 约束**：物理 FOV 必须至少为 $72\\,\\text{mm}$。\n    $$L \\geq 72\\,\\text{mm}$$\n\n我们的目标是找到满足所有这些条件的最小整数三元组 $(P_{x}, P_{y}, P_{z})$。\n\n我们从分析各向同性 FOV 约束开始。使用等式的前两部分和给定的间距：\n$$P_{x} s_{x} = P_{y} s_{y}$$\n$$P_{x} (0.9) = P_{y} (0.9)$$\n这简化为 $P_{x} = P_{y}$。这意味着对 $P_{x}$ 的任何约束也适用于 $P_{y}$，这与两者都必须能被 $8$ 整除的架构约束是一致的。\n\n接下来，我们使用关联 $x$ 和 $z$ 维度的约束：\n$$P_{x} s_{x} = P_{z} s_{z}$$\n代入给定的体素间距：\n$$P_{x} (0.9) = P_{z} (4.5)$$\n我们可以用 $P_{z}$ 来求解 $P_{x}$：\n$$P_{x} = \\frac{4.5}{0.9} P_{z} = 5 P_{z}$$\n由于 $P_{x}$ 和 $P_{z}$ 都必须是整数，这个方程意味着 $P_{x}$ 必须是 $5$ 的整数倍。\n\n现在，我们结合所有对 $P_{x}$ 的约束：\na) $P_{x}$ 必须能被 $8$ 整除（来自架构约束）。\nb) $P_{x}$ 必须能被 $5$ 整除（来自各向同性 FOV 和整数体素约束）。\nc) 物理 FOV $L = P_{x} s_{x}$ 必须至少为 $72\\,\\text{mm}$。\n   $$P_{x} (0.9) \\geq 72$$\n   $$P_{x} \\geq \\frac{72}{0.9} = \\frac{720}{9} = 80$$\n\n为了同时满足 (a) 和 (b)，$P_{x}$ 必须是 $8$ 和 $5$ 的公倍数。由于 $8$ 和 $5$ 互质（它们的最大公约数为 $1$），$P_{x}$ 必须是它们的最小公倍数的倍数，即 $\\text{lcm}(8, 5) = 8 \\times 5 = 40$。\n\n所以，我们要寻找的是 $40$ 的倍数中大于或等于 $80$ 的最小整数 $P_{x}$。$40$ 的倍数有 $40, 80, 120, \\dots$。满足条件 $P_{x} \\geq 80$ 的最小 $40$ 的倍数是 $80$。\n\n因此，$P_{x}$ 的最小值为 $80$。\n\n有了 $P_{x}$ 的这个最小值，我们就可以找到 $P_{y}$ 和 $P_{z}$ 的相应最小值：\n-   由 $P_{x} = P_{y}$ 可得 $P_{y} = 80$。\n-   由 $P_{x} = 5 P_{z}$ 可得 $80 = 5 P_{z}$，从而得出 $P_{z} = \\frac{80}{5} = 16$。\n\n区块维度的最小整数三元组是 $(P_{x}, P_{y}, P_{z}) = (80, 80, 16)$。\n\n作为最后的检查，我们验证该解是否满足所有初始条件：\n-   $P_{x}=80$，$P_{y}=80$，$P_{z}=16$ 都是整数。\n-   $P_{x}=80$ 能被 $8$ 整除（$80 = 8 \\times 10$）。\n-   $P_{y}=80$ 能被 $8$ 整除（$80 = 8 \\times 10$）。\n-   物理 FOV 为 $L = P_{x} s_{x} = 80 \\times 0.9\\,\\text{mm} = 72\\,\\text{mm}$。这满足 $L \\geq 72\\,\\text{mm}$。\n-   FOV 是各向同性的：\n    -   $P_{x} s_{x} = 80 \\times 0.9\\,\\text{mm} = 72\\,\\text{mm}$。\n    -   $P_{y} s_{y} = 80 \\times 0.9\\,\\text{mm} = 72\\,\\text{mm}$。\n    -   $P_{z} s_{z} = 16 \\times 4.5\\,\\text{mm} = 72\\,\\text{mm}$。\n所有条件都已满足。由于我们是从找到 $P_x$ 的最小可能值开始的，所以这个三元组是最小的。",
            "answer": "$$\\boxed{\\begin{pmatrix} 80  80  16 \\end{pmatrix}}$$"
        },
        {
            "introduction": "在处理完几何挑战之后，我们转向统计学上的挑战。类别不平衡，即目标病灶远小于背景区域，是许多医学分割任务的决定性特征。本练习从三个基本角度解决这个问题：根据误分类成本调整决策阈值，对损失函数进行加权以给予稀有类别更多关注，以及通过数据重采样创建一个更平衡的训练集。这项实践为您提供了一套应对类别不平衡问题的原则性工具箱。",
            "id": "4582628",
            "problem": "您正在分析用于检测病变类别与背景的二元医学图像分割，使用一个逐体素分类器，该分类器输出经过校准的后验概率。核心挑战是在训练和决策过程中处理类别不平衡问题，同时保持基于统计决策理论和标准分割度量定义的原则性正确性。从基本原则出发，推导出一套可实现的规则，以解决决策阈值、损失加权和重采样问题。\n\n假设有一个经过校准的模型，它为病变类别输出逐体素的后验概率 $p = P(Y=1 \\mid x)$，其中 $Y \\in \\{0,1\\}$，$Y=1$ 表示病变，$Y=0$ 表示背景。设病变的流行率为 $\\pi = P(Y=1)$，背景的流行率为 $1 - \\pi$。设假阴性（false negative）的误分类成本为 $C_{\\mathrm{FN}}$，假阳性（false positive）的误分类成本为 $C_{\\mathrm{FP}}$。设 $\\epsilon > 0$ 是一个小的平滑常数。训练集规模大且独立同分布；模型经过良好校准，因此输出 $p$ 等于训练分布下的后验概率。\n\n基于这些基本原理，执行以下推导并实现相应的计算：\n\n- 使用贝叶斯决策理论，推导在后验概率 $p$ 上的贝叶斯最优阈值 $t$，该阈值在非对称误分类成本 $C_{\\mathrm{FN}}$ 和 $C_{\\mathrm{FP}}$ 下最小化期望条件风险。解释流行率 $\\pi$ 如何进入后验概率，以及当 $p$ 被校准时，它是否出现在最终决策规则中，然后为每个测试用例计算 $t$。\n\n- 从多类别设置中单个类别的 Sørensen–Dice 系数定义和广义 Dice 损失出发，为病变类别和背景类别构建类别权重，以在存在不平衡的情况下均衡每个类别的期望贡献。利用期望分母项与类别大小成比例的思想，推导出与类别流行率成反比的权重，并使用 $\\epsilon$进行平滑以避免奇异性。将权重归一化，使其总和为 $1$，从而形成一个凸组合。为每个测试用例计算病变类别的归一化权重 $w_1$ 和背景类别的归一化权重 $w_0$。\n\n- 假设您通过仅对病变（正）类别进行过采样，而不改变背景（负）类别，来实现目标有效流行率 $\\pi^\\star \\in (\\pi,1)$。推导所需的过采样因子 $r$，使得过采样后的新流行率等于 $\\pi^\\star$，用原始流行率 $\\pi$ 和目标 $\\pi^\\star$ 表示。为每个测试用例计算 $r$。\n\n所有输出必须是十进制表示的实数。不涉及物理单位或角度。实现一个程序，为每个测试用例计算一个包含四个浮点数的元组，顺序为 $[t, w_1, w_0, r]$。\n\n使用以下测试套件，它涵盖了一般情况、平衡情况、严重不平衡、接近零的流行率和高流行率：\n\n- 用例 1：$\\pi = 0.05$, $C_{\\mathrm{FN}} = 5$, $C_{\\mathrm{FP}} = 1$, $\\epsilon = 10^{-6}$, $\\pi^\\star = 0.5$。\n- 用例 2：$\\pi = 0.5$, $C_{\\mathrm{FN}} = 1$, $C_{\\mathrm{FP}} = 1$, $\\epsilon = 10^{-6}$, $\\pi^\\star = 0.5$。\n- 用例 3：$\\pi = 0.01$, $C_{\\mathrm{FN}} = 10$, $C_{\\mathrm{FP}} = 1$, $\\epsilon = 10^{-3}$, $\\pi^\\star = 0.2$。\n- 用例 4：$\\pi = 10^{-4}$, $C_{\\mathrm{FN}} = 100$, $C_{\\mathrm{FP}} = 1$, $\\epsilon = 10^{-2}$, $\\pi^\\star = 0.1$。\n- 用例 5：$\\pi = 0.9$, $C_{\\mathrm{FN}} = 1$, $C_{\\mathrm{FP}} = 5$, $\\epsilon = 10^{-6}$, $\\pi^\\star = 0.95$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，所有测试用例的值按顺序连接且不含空格。具体而言，输出必须是\n$[t_1, w_{1,1}, w_{0,1}, r_1, t_2, w_{1,2}, w_{0,2}, r_2, t_3, w_{1,3}, w_{0,3}, r_3, t_4, w_{1,4}, w_{0,4}, r_4, t_5, w_{1,5}, w_{0,5}, r_5]$。",
            "solution": "问题陈述已经过验证，被认为是科学上可靠、定义明确且客观的。它在医学图像分析和统计决策理论领域提出了一个可形式化的挑战，并建立在公认的原则之上。我们将对所要求的量进行严格、分步的推导。\n\n**1. 贝叶斯最优阈值 $t$ 的推导**\n\n目标是找到一个关于后验概率 $p = P(Y=1 \\mid x)$ 的决策阈值，以最小化期望条件风险。对每个体素的决策是预测其为病变类别（$\\hat{Y}=1$）或背景类别（$\\hat{Y}=0$）。真实类别由 $Y \\in \\{0, 1\\}$ 表示。我们已知误分类的成本：$C_{\\mathrm{FN}}$ 是假阴性（当 $Y=1$ 时预测 $\\hat{Y}=0$）的成本，$C_{\\mathrm{FP}}$ 是假阳性（当 $Y=0$ 时预测 $\\hat{Y}=1$）的成本。\n\n对于给定特征向量为 $x$ 的体素，校准后的模型提供了病变类别的后验概率 $p = P(Y=1 \\mid x)$。因此，背景类别的后验概率是 $P(Y=0 \\mid x) = 1 - p$。\n\n在给定观测值 $x$ 的情况下，做出决策 $\\hat{Y}$ 的条件风险是该决策的期望成本。\n\n决定为“病变”（$\\hat{Y}=1$）的条件风险是此决策出错的概率乘以该错误的成本：\n$$R(\\hat{Y}=1 \\mid x) = P(Y=0 \\mid x) \\cdot C_{\\mathrm{FP}} = (1 - p) C_{\\mathrm{FP}}$$\n\n类似地，决定为“背景”（$\\hat{Y}=0$）的条件风险是：\n$$R(\\hat{Y}=0 \\mid x) = P(Y=1 \\mid x) \\cdot C_{\\mathrm{FN}} = p C_{\\mathrm{FN}}$$\n\n贝叶斯决策理论规定，我们应该选择最小化此条件风险的行动。因此，我们当且仅当以下条件成立时，决定为“病变”（$\\hat{Y}=1$）：\n$$R(\\hat{Y}=1 \\mid x)  R(\\hat{Y}=0 \\mid x)$$\n$$(1 - p) C_{\\mathrm{FP}}  p C_{\\mathrm{FN}}$$\n\n为了找到决策阈值 $t$，我们对此不等式求解 $p$：\n$$C_{\\mathrm{FP}} - p C_{\\mathrm{FP}}  p C_{\\mathrm{FN}}$$\n$$C_{\\mathrm{FP}}  p (C_{\\mathrm{FN}} + C_{\\mathrm{FP}})$$\n$$p > \\frac{C_{\\mathrm{FP}}}{C_{\\mathrm{FN}} + C_{\\mathrm{FP}}}$$\n\n因此，最优策略是将后验概率 $p$ 超过某个阈值 $t$ 的体素分类为“病变”。贝叶斯最优阈值 $t$ 是两种风险相等的点：\n$$t = \\frac{C_{\\mathrm{FP}}}{C_{\\mathrm{FN}} + C_{\\mathrm{FP}}}$$\n\n问题询问流行率 $\\pi = P(Y=1)$ 如何进入此规则。根据贝叶斯定理，后验概率 $p$ 是流行率 $\\pi$、类别条件似然 $P(x \\mid Y=1)$ 和 $P(x \\mid Y=0)$ 以及证据 $P(x)$ 的函数：\n$$p = P(Y=1 \\mid x) = \\frac{P(x \\mid Y=1) P(Y=1)}{P(x)} = \\frac{P(x \\mid Y=1) \\pi}{P(x \\mid Y=1)\\pi + P(x \\mid Y=0)(1-\\pi)}$$\n问题指出，模型是经过良好校准的，这意味着其输出 $p$ 正确地表示了在训练分布下的真实后验概率 $P(Y=1 \\mid x)$。这表明模型已经学习并融合了训练数据流行率 $\\pi$ 的影响。因此，决策规则 $p > t$ 通过 $p$ 的值隐式地考虑了流行率。然而，决策阈值 $t$ 本身仅取决于误分类成本 $C_{\\mathrm{FN}}$ 和 $C_{\\mathrm{FP}}$，而不明文依赖于 $\\pi$。\n\n**2. 用于 Dice 损失的类别权重 ($w_1, w_0$) 的推导**\n\n任务是为基于 Dice 的损失函数构建类别权重，这些权重与类别流行率成反比，并带有一个平滑常数 $\\epsilon > 0$ 以防止除以零。权重必须归一化为总和为 $1$。\n\n设病变类别（$Y=1$）的流行率为 $\\pi_1 = \\pi$，背景类别（$Y=0$）的流行率为 $\\pi_0 = 1 - \\pi$。\n未归一化的权重 $\\tilde{w}_1$ 和 $\\tilde{w}_0$ 与平滑后的流行率成反比：\n$$\\tilde{w}_1 = \\frac{1}{\\pi_1 + \\epsilon} = \\frac{1}{\\pi + \\epsilon}$$\n$$\\tilde{w}_0 = \\frac{1}{\\pi_0 + \\epsilon} = \\frac{1}{1 - \\pi + \\epsilon}$$\n\n为了将这些权重归一化为总和为 $1$，我们将每个权重除以它们的总和 $S = \\tilde{w}_1 + \\tilde{w}_0$：\n$$S = \\frac{1}{\\pi + \\epsilon} + \\frac{1}{1 - \\pi + \\epsilon} = \\frac{(1 - \\pi + \\epsilon) + (\\pi + \\epsilon)}{(\\pi + \\epsilon)(1 - \\pi + \\epsilon)} = \\frac{1 + 2\\epsilon}{(\\pi + \\epsilon)(1 - \\pi + \\epsilon)}$$\n\n病变类别的归一化权重 $w_1$ 是：\n$$w_1 = \\frac{\\tilde{w}_1}{S} = \\frac{\\frac{1}{\\pi + \\epsilon}}{\\frac{1 + 2\\epsilon}{(\\pi + \\epsilon)(1 - \\pi + \\epsilon)}} = \\frac{1}{\\pi + \\epsilon} \\cdot \\frac{(\\pi + \\epsilon)(1 - \\pi + \\epsilon)}{1 + 2\\epsilon} = \\frac{1 - \\pi + \\epsilon}{1 + 2\\epsilon}$$\n\n背景类别的归一化权重 $w_0$ 是：\n$$w_0 = \\frac{\\tilde{w}_0}{S} = \\frac{\\frac{1}{1 - \\pi + \\epsilon}}{\\frac{1 + 2\\epsilon}{(\\pi + \\epsilon)(1 - \\pi + \\epsilon)}} = \\frac{1}{1 - \\pi + \\epsilon} \\cdot \\frac{(\\pi + \\epsilon)(1 - \\pi + \\epsilon)}{1 + 2\\epsilon} = \\frac{\\pi + \\epsilon}{1 + 2\\epsilon}$$\n\n作为检验，$w_1 + w_0 = \\frac{1 - \\pi + \\epsilon}{1 + 2\\epsilon} + \\frac{\\pi + \\epsilon}{1 + 2\\epsilon} = \\frac{1 - \\pi + \\epsilon + \\pi + \\epsilon}{1 + 2\\epsilon} = \\frac{1 + 2\\epsilon}{1 + 2\\epsilon} = 1$。推导是正确的。\n\n**3. 过采样因子 $r$ 的推导**\n\n我们的目标是找到因子 $r$，通过对病变（正）类别进行过采样，将有效流行率从初始值 $\\pi$ 改变为目标值 $\\pi^\\star$。\n\n设原始数据集中病变类别的样本数为 $N_1$，背景类别的样本数为 $N_0$。总样本数为 $N = N_0 + N_1$。原始流行率为：\n$$\\pi = \\frac{N_1}{N_0 + N_1}$$\n\n通过因子 $r$ 对病变类别进行过采样，会创建一个新的数据集，其中有 $N'_1 = r N_1$ 个病变样本，而背景样本数保持不变，$N'_0 = N_0$。新的总样本数为 $N' = N'_0 + N'_1 = N_0 + r N_1$。\n\n新的流行率 $\\pi^\\star$ 为：\n$$\\pi^\\star = \\frac{N'_1}{N'} = \\frac{r N_1}{N_0 + r N_1}$$\n\n为了用 $\\pi$ 和 $\\pi^\\star$ 表示 $r$，我们首先用 $\\pi$ 表示 $N_0$ 和 $N_1$。根据 $\\pi$ 的定义，我们有 $N_1 = \\pi(N_0 + N_1)$ 和 $N_0 = (1-\\pi)(N_0 + N_1)$。其比值为 $\\frac{N_0}{N_1} = \\frac{1-\\pi}{\\pi}$。所以，$N_0 = N_1 \\frac{1-\\pi}{\\pi}$。\n\n将 $N_0$ 的这个表达式代入 $\\pi^\\star$ 的方程中：\n$$\\pi^\\star = \\frac{r N_1}{N_1 \\frac{1-\\pi}{\\pi} + r N_1}$$\n由于 $N_1$ 必须为非零才能使此问题有意义，我们可以将分子和分母同除以 $N_1$：\n$$\\pi^\\star = \\frac{r}{\\frac{1-\\pi}{\\pi} + r}$$\n\n现在，我们求解 $r$：\n$$\\pi^\\star \\left(\\frac{1-\\pi}{\\pi} + r\\right) = r$$\n$$\\pi^\\star \\frac{1-\\pi}{\\pi} + \\pi^\\star r = r$$\n$$\\pi^\\star \\frac{1-\\pi}{\\pi} = r - \\pi^\\star r$$\n$$\\pi^\\star \\frac{1-\\pi}{\\pi} = r(1 - \\pi^\\star)$$\n$$r = \\frac{\\pi^\\star (1 - \\pi)}{\\pi (1 - \\pi^\\star)}$$\n这就是病变类别所需的过采样因子。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes segmentation-related parameters for a series of test cases based on\n    statistical decision theory and standard machine learning practices for class imbalance.\n    \"\"\"\n    test_cases = [\n        # Case 1: General case\n        {'pi': 0.05, 'cfn': 5.0, 'cfp': 1.0, 'eps': 1e-6, 'pi_star': 0.5},\n        # Case 2: Balanced case\n        {'pi': 0.5, 'cfn': 1.0, 'cfp': 1.0, 'eps': 1e-6, 'pi_star': 0.5},\n        # Case 3: Severe imbalance\n        {'pi': 0.01, 'cfn': 10.0, 'cfp': 1.0, 'eps': 1e-3, 'pi_star': 0.2},\n        # Case 4: Near-zero prevalence\n        {'pi': 1e-4, 'cfn': 100.0, 'cfp': 1.0, 'eps': 1e-2, 'pi_star': 0.1},\n        # Case 5: High prevalence\n        {'pi': 0.9, 'cfn': 1.0, 'cfp': 5.0, 'eps': 1e-6, 'pi_star': 0.95},\n    ]\n\n    results = []\n    for case in test_cases:\n        pi = case['pi']\n        cfn = case['cfn']\n        cfp = case['cfp']\n        eps = case['eps']\n        pi_star = case['pi_star']\n\n        # 1. Bayes-optimal threshold t\n        # t = C_FP / (C_FN + C_FP)\n        t = cfp / (cfn + cfp)\n\n        # 2. Normalized class weights w1 (lesion) and w0 (background)\n        # w1 = (1 - pi + eps) / (1 + 2*eps)\n        # w0 = (pi + eps) / (1 + 2*eps)\n        w_denominator = 1.0 + 2.0 * eps\n        w1 = (1.0 - pi + eps) / w_denominator\n        w0 = (pi + eps) / w_denominator\n\n        # 3. Oversampling factor r\n        # r = (pi_star * (1 - pi)) / (pi * (1 - pi_star))\n        # Note: The problem setup ensures pi > 0, pi_star  1, so no division by zero.\n        r = (pi_star * (1.0 - pi)) / (pi * (1.0 - pi_star))\n        \n        results.extend([t, w1, w0, r])\n\n    # Format the final output as a single comma-separated list in brackets, with no spaces.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "最后，我们将深入探讨训练过程本身的动态机制，学习超越基本训练循环的高级技术。本练习将探索如何实现现代化的训练策略，例如使用混合增强（mixup）以获得更好的泛化能力，如何将戴斯系数（Dice coefficient）等评估指标应用于概率性输出，以及如何利用余弦退火学习率调度来控制学习过程以实现更稳定的收敛。掌握这些技术是实现顶尖分割性能的关键。",
            "id": "4582643",
            "problem": "给定一个用于二元医学图像分割的训练设置，该设置使用线性标签混合进行数据增强，并采用基于余弦曲线的学习率调度。您的目标是，从第一性原理出发，推导并实现控制该协议下训练过程的数学量，然后为指定的测试用例计算这些量。您必须使用的基本原理是经验风险最小化（ERM）、伯努利标签的交叉熵（CE）和戴斯相似系数（DSC）的定义，以及学习率调度是一个平滑函数，在一个周期内从最大值开始到最小值结束的要求。所有角度都必须以弧度处理。\n\n任务 A（标签混合下的损失）：考虑两个二元标签向量 $y_1 \\in \\{0,1\\}^n$ 和 $y_2 \\in \\{0,1\\}^n$，以及一个包含 $n$ 个像素的图像块的预测正类别概率 $p \\in [0,1]^n$。数据增强是一种线性标签混合（通常称为“mixup”），对于一个混合系数 $\\lambda \\in [0,1]$，产生一个软标签 $y' = \\lambda y_1 + (1-\\lambda) y_2$。从伯努利目标的交叉熵（CE）定义和期望的线性性质出发，推导软标签 $y'$ 下的平均每像素交叉熵表达式，并为给定的 $p$、$y_1$、$y_2$ 和 $\\lambda$ 实现其计算。\n\n任务 B（概率性预测的软戴斯系数）：使用戴斯相似系数（DSC）对集合的标准定义，并通过将乘积之和解释为预期重叠，将其扩展到概率性预测，推导出 $p$ 和 $y'$ 之间的软戴斯系数表达式，并为给定的 $p$ 和 $y'$ 实现其计算。\n\n任务 C（一个周期内的余弦退火学习率）：一个训练协议使用带热重启的余弦退火（CAWR），其中在长度为 $T$ 个步骤的周期内，学习率在步骤 $t=0$ 时从最大值 $\\eta_0$ 开始，并在步骤 $t=T$ 时达到最小值，遵循一个以弧度计量的单个余弦半波。从调度是平滑的、在开始时达到 $\\eta_0$、在结束时达到最小值、并且在周期内单调递减的要求出发，推导在一个周期内整数步骤 $t \\in \\{0,1,\\dots,T\\}$ 的学习率公式，并为给定的 $\\eta_0$、$T$ 和 $t$ 实现其计算。\n\n测试套件：对于每个测试用例，给定 $p$、$y_1$、$y_2$、$\\lambda$、$\\eta_0$、$T$ 和 $t$。计算三个量：平均交叉熵损失（任务 A）、软戴斯系数（任务 B）和学习率（任务 C）。使用以下四个测试用例，其中 $n=4$：\n\n- 测试用例 1：\n  - $p = [0.9, 0.2, 0.8, 0.1]$\n  - $y_1 = [1, 0, 1, 0]$\n  - $y_2 = [0, 1, 0, 1]$\n  - $\\lambda = 0.3$\n  - $\\eta_0 = 0.01$\n  - $T = 10$\n  - $t = 3$\n\n- 测试用例 2（边界混合与周期开始）：\n  - $p = [0.5, 0.5, 0.5, 0.5]$\n  - $y_1 = [1, 1, 0, 0]$\n  - $y_2 = [0, 0, 1, 1]$\n  - $\\lambda = 1.0$\n  - $\\eta_0 = 0.001$\n  - $T = 20$\n  - $t = 0$\n\n- 测试用例 3（边界混合与周期结束）：\n  - $p = [0.99, 0.01, 0.99, 0.01]$\n  - $y_1 = [1, 0, 1, 0]$\n  - $y_2 = [1, 0, 0, 1]$\n  - $\\lambda = 0.0$\n  - $\\eta_0 = 0.02$\n  - $T = 5$\n  - $t = 5$\n\n- 测试用例 4（对称混合与周期结束）：\n  - $p = [0.8, 0.8, 0.2, 0.2]$\n  - $y_1 = [1, 1, 0, 0]$\n  - $y_2 = [0, 0, 1, 1]$\n  - $\\lambda = 0.5$\n  - $\\eta_0 = 0.005$\n  - $T = 7$\n  - $t = 7$\n\n最终输出格式：您的程序应生成单行输出，包含一个用方括号括起来的逗号分隔列表形式的结果。每个测试用例的结果本身应是一个形如 $[\\text{损失}, \\text{戴斯系数}, \\text{学习率}]$ 的三元组。例如，完整的输出应类似于 $[[\\text{L}_1,\\text{D}_1,\\text{LR}_1],[\\text{L}_2,\\text{D}_2,\\text{LR}_2],\\dots]$。如果您的计算中出现任何对数，请使用以弧度为单位的角度，并通过裁剪概率来处理数值稳定性，以避免未定义的对数。",
            "solution": "问题陈述已经过验证，被认为是科学上合理、自洽且定义明确的。所有任务都涉及机器学习和医学图像分析中标准原理的推导和应用。我们按顺序解决每个任务来继续解答。\n\n### 问题验证\n**步骤 1：提取已知信息**\n- **数据（每个图像块）：**\n  - 二元标签向量：$y_1 \\in \\{0,1\\}^n$, $y_2 \\in \\{0,1\\}^n$\n  - 预测的正类别概率：$p \\in [0,1]^n$\n  - 图像块大小：$n$ 像素\n- **增强和训练参数：**\n  - 软标签：$y' = \\lambda y_1 + (1-\\lambda) y_2$\n  - 混合系数：$\\lambda \\in [0,1]$\n  - 最大学习率：$\\eta_0$\n  - 周期长度：$T$ 步\n  - 周期内当前步骤：$t \\in \\{0,1,\\dots,T\\}$\n- **任务：**\n  - (A) 推导并计算预测 $p$ 相对于软标签 $y'$ 的平均每像素交叉熵（CE）损失。\n  - (B) 推导并计算 $p$ 和 $y'$ 之间的软戴斯相似系数（DSC）。\n  - (C) 推导并计算余弦退火调度下的学习率 $\\eta(t)$。\n- **基本原理：**\n  - 经验风险最小化（ERM）\n  - 伯努利标签的交叉熵（CE）\n  - 戴斯相似系数（DSC）\n  - 余弦学习率调度的性质（平滑、开始时最大、结束时最小、单调）。\n- **测试用例：**\n  - 提供了四个 $\\{p, y_1, y_2, \\lambda, \\eta_0, T, t\\}$ 的具体实例，其中 $n=4$。\n- **数值考虑：** 按标准使用自然对数计算 CE。裁剪概率以避免数值上未定义的结果（例如 $\\log(0)$）。\n\n**步骤 2：使用提取的已知信息进行验证**\n- **科学上合理：** 该问题基于机器学习中已建立的概念：交叉熵损失、戴斯分数、标签混合（mixup）和用于学习率调度的余弦退火。这些都是标准技术。\n- **定义明确：** 所有任务都定义清晰，并根据所提供的原理和数据具有唯一的解。\n- **客观性：** 问题以精确的数学语言陈述，没有主观性。\n\n**步骤 3：结论与行动**\n问题是有效的。我们现在将从第一性原理推导所需的表达式，然后将它们应用于测试用例。\n\n### 推导与解答\n\n**任务 A：标签混合下的平均交叉熵损失**\n\n交叉熵（CE）损失衡量真实概率分布与预测概率分布之间的不相似性。对于一个二元分类问题（例如，像素级分割），单个像素 $i$ 的真实标签是一个参数为 $y_i \\in \\{0,1\\}$ 的伯努利随机变量。正类别（$1$）的预测概率是 $p_i$。该单个像素的 CE 损失由下式给出：\n$$\n\\mathcal{L}_{CE,i}(p_i, y_i) = - [y_i \\log(p_i) + (1-y_i) \\log(1-p_i)]\n$$\n其中 $\\log$ 表示自然对数，这是信息论中的标准。在经验风险最小化（ERM）中，总损失是给定数据样本（图像块）中所有 $n$ 个像素的平均损失。\n\n问题引入了一个由两个硬标签 $y_1$ 和 $y_2$ 线性混合得到的软标签 $y'$：\n$$\ny' = \\lambda y_1 + (1-\\lambda) y_2\n$$\n对于每个像素 $i$，软标签为 $y'_i = \\lambda y_{1,i} + (1-\\lambda) y_{2,i}$。由于 $y_{1,i}, y_{2,i} \\in \\{0,1\\}$ 且 $\\lambda \\in [0,1]$，我们有 $y'_i \\in [0,1]$。我们可以将 $y'_i$ 解释为伯努利分布的参数，即真实标签为 $1$ 的概率。CE 损失是对此类概率性目标的有效度量。\n\n为了找到软标签 $y'$ 的 CE 损失，我们将 $y'_i$ 替换到每像素损失公式中的 $y_i$：\n$$\n\\mathcal{L}_{CE,i}(p_i, y'_i) = - [y'_i \\log(p_i) + (1-y'_i) \\log(1-p_i)]\n$$\n在 $n$ 个像素的图像块上的平均每像素 CE 损失是这些单个损失的均值：\n$$\n\\mathcal{L}_{CE}(p, y') = \\frac{1}{n} \\sum_{i=1}^n \\mathcal{L}_{CE,i}(p_i, y'_i) = -\\frac{1}{n} \\sum_{i=1}^n [y'_i \\log(p_i) + (1-y'_i) \\log(1-p_i)]\n$$\n这是损失的最终表达式。根据问题对数值稳定性的要求，为防止对数未定义，预测概率 $p_i$ 必须被裁剪到一个由小的 epsilon 界定的范围内，例如对于一个小的 $\\epsilon  0$，裁剪到 $[\\epsilon, 1-\\epsilon]$。\n\n**任务 B：软戴斯相似系数**\n\n戴斯相似系数（DSC）是一种用于衡量两个集合相似性的统计量。对于集合 $A$ 和 $B$，其定义为：\n$$\nDSC(A,B) = \\frac{2 |A \\cap B|}{|A| + |B|}\n$$\n在图像分割中，我们可以将真实标签掩码和预测掩码表示为长度为 $n$ 的二元向量 $y$ 和 $\\hat{y}$。集合运算可以转换为向量运算：\n- $|A| \\rightarrow \\sum_{i=1}^n y_i$（真实标签中的正像素数）\n- $|B| \\rightarrow \\sum_{i=1}^n \\hat{y}_i$（预测中的正像素数）\n- $|A \\cap B| \\rightarrow \\sum_{i=1}^n y_i \\hat{y}_i$（正确预测的正像素数，即真阳性）\n\n该任务要求将此扩展到概率性预测 $p$ 和软真实标签 $y'$。这是通过用它们的概率对应物替换二元指示符来实现的。乘积之和被解释为期望计数。\n- 预测 $\\hat{y}$ 被概率向量 $p$ 替换。\n- 真实标签 $y$ 被软标签向量 $y'$ 替换。\n\nDSC 公式中的各项随后被推广如下：\n- 预测集合的期望大小：$\\sum_{i=1}^n p_i$\n- 真实标签集合的期望大小：$\\sum_{i=1}^n y'_i$\n- 交集的期望大小：$\\sum_{i=1}^n p_i y'_i$\n\n将这些代入 DSC 定义，得到软戴斯系数：\n$$\nDSC_{soft}(p, y') = \\frac{2 \\sum_{i=1}^n p_i y'_i}{\\sum_{i=1}^n p_i + \\sum_{i=1}^n y'_i}\n$$\n该公式计算概率性输入的戴斯分数。为防止在预测和真实标签均为全零的情况下出现除以零，可以在分子和分母上添加一个小的平滑常数 $\\epsilon$，但我们将遵循直接推导，因为问题没有指定这一点。\n\n**任务 C：余弦退火学习率**\n\n学习率调度 $\\eta(t)$ 必须在一个周期 $T$ 步内，对于 $t \\in \\{0, 1, \\dots, T\\}$，遵循单个余弦半波。一个一般的余弦函数形式为 $f(x) = A \\cos(x) + C$。我们需要对其进行缩放和平移以满足给定的标准。设学习率为：\n$$\n\\eta(t) = C + A \\cos(\\omega t + \\phi)\n$$\n要求是：\n$1$. $\\eta(0) = \\eta_0$（最大值）。这意味着余弦项在 $t=0$ 时应为其最大值（$+1$）。选择 $\\phi=0$ 可以简化此问题。所以，$\\eta(t) = C + A \\cos(\\omega t)$。\n$2$. $\\eta(T) = \\eta_{min}$（最小值）。这意味着余弦项在 $t=T$ 时应为其最小值（$-1$）。\n$3$. 调度描述了一个单个半波。这意味着当 $t$ 从 $0$ 变为 $T$ 时，余弦的参数必须从 $0$ 变为 $\\pi$。\n\n根据要求 $3$，我们将余弦的参数设置为 $\\frac{t\\pi}{T}$。我们的函数变为：\n$$\n\\eta(t) = C + A \\cos\\left(\\frac{t\\pi}{T}\\right)\n$$\n现在我们应用边界条件：\n- 在 $t=0$ 时：$\\eta(0) = C + A \\cos(0) = C + A = \\eta_0$。\n- 在 $t=T$ 时：$\\eta(T) = C + A \\cos(\\pi) = C - A = \\eta_{min}$。\n\n问题陈述调度“在结束时达到最小值”，但没有为 $\\eta_{min}$ 指定一个值。标准的余弦退火调度通常会退火到最小值为 $\\eta_{min}=0$。这是对从最大值开始的“单个半波”最自然的解释。假设 $\\eta_{min}=0$：\n- $C + A = \\eta_0$\n- $C - A = 0 \\implies C = A$\n\n将 $C=A$ 代入第一个方程得到 $2A = \\eta_0$，所以 $A = \\frac{\\eta_0}{2}$。这也意味着 $C = \\frac{\\eta_0}{2}$。\n因此，步骤 $t$ 的学习率最终公式为：\n$$\n\\eta(t) = \\frac{\\eta_0}{2} + \\frac{\\eta_0}{2} \\cos\\left(\\frac{t\\pi}{T}\\right) = \\frac{\\eta_0}{2}\\left(1 + \\cos\\left(\\frac{t\\pi}{T}\\right)\\right)\n$$\n该函数正确地在 $\\eta(0)=\\eta_0$ 开始，在 $\\eta(T)=0$ 结束，是平滑的，并且在区间 $t \\in [0, T]$ 上单调递减，从而满足所有陈述的原理。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and computes quantities for a medical image segmentation training setup.\n    - Task A: Average Cross-Entropy loss with linear label mixing.\n    - Task B: Soft Dice Similarity Coefficient.\n    - Task C: Cosine-annealed learning rate.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case 1\n        {\n            \"p\": [0.9, 0.2, 0.8, 0.1], \"y1\": [1, 0, 1, 0], \"y2\": [0, 1, 0, 1],\n            \"lambda\": 0.3, \"eta0\": 0.01, \"T\": 10, \"t\": 3\n        },\n        # Test Case 2 (boundary mixing and cycle start)\n        {\n            \"p\": [0.5, 0.5, 0.5, 0.5], \"y1\": [1, 1, 0, 0], \"y2\": [0, 0, 1, 1],\n            \"lambda\": 1.0, \"eta0\": 0.001, \"T\": 20, \"t\": 0\n        },\n        # Test Case 3 (boundary mixing and cycle end)\n        {\n            \"p\": [0.99, 0.01, 0.99, 0.01], \"y1\": [1, 0, 1, 0], \"y2\": [1, 0, 0, 1],\n            \"lambda\": 0.0, \"eta0\": 0.02, \"T\": 5, \"t\": 5\n        },\n        # Test Case 4 (symmetric mixing and cycle end)\n        {\n            \"p\": [0.8, 0.8, 0.2, 0.2], \"y1\": [1, 1, 0, 0], \"y2\": [0, 0, 1, 1],\n            \"lambda\": 0.5, \"eta0\": 0.005, \"T\": 7, \"t\": 7\n        },\n    ]\n\n    results = []\n    \n    # Epsilon for numerical stability in log\n    epsilon = 1e-9\n\n    for case in test_cases:\n        # Unpack parameters\n        p_vec = np.array(case[\"p\"], dtype=float)\n        y1_vec = np.array(case[\"y1\"], dtype=float)\n        y2_vec = np.array(case[\"y2\"], dtype=float)\n        lambda_val = case[\"lambda\"]\n        eta0 = case[\"eta0\"]\n        T = case[\"T\"]\n        t = case[\"t\"]\n\n        # --- Task A: Average Cross-Entropy Loss with Label Mixing ---\n        # 1. Compute the soft label y'\n        y_prime = lambda_val * y1_vec + (1 - lambda_val) * y2_vec\n\n        # 2. Clip predictions for numerical stability\n        p_clipped = np.clip(p_vec, epsilon, 1 - epsilon)\n\n        # 3. Compute average cross-entropy loss\n        ce_loss = -np.mean(\n            y_prime * np.log(p_clipped) + (1 - y_prime) * np.log(1 - p_clipped)\n        )\n\n        # --- Task B: Soft Dice Similarity Coefficient ---\n        # 1. Compute numerator and denominator\n        numerator = 2 * np.sum(p_vec * y_prime)\n        denominator = np.sum(p_vec) + np.sum(y_prime)\n\n        # 2. Compute soft dice, handling potential division by zero\n        soft_dice = numerator / denominator if denominator > 0 else 0.0\n\n        # --- Task C: Cosine-Annealed Learning Rate ---\n        # Assumes eta_min = 0 as derived from problem statement principles.\n        # Handles T=0 case, though not present in tests.\n        if T == 0:\n            learning_rate = eta0\n        else:\n            learning_rate = (eta0 / 2.0) * (1 + np.cos(t * np.pi / T))\n        \n        results.append([ce_loss, soft_dice, learning_rate])\n\n    # Format the final output string exactly as required.\n    result_strings = [f\"[{res[0]},{res[1]},{res[2]}]\" for res in results]\n    final_output = f\"[{','.join(result_strings)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        }
    ]
}