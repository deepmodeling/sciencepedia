{
    "hands_on_practices": [
        {
            "introduction": "Medical imaging datasets, such as 3D MRI scans, often exhibit anisotropy, where the physical distance between voxels differs along different axes. This practice addresses the crucial first step of designing an effective data pipeline: extracting patches for a 3D CNN while accounting for this anisotropy. You will learn to determine the minimal patch dimensions in voxels that satisfy architectural constraints and produce an isotropic physical field of view, ensuring that the network learns features from a physically consistent spatial context .",
            "id": "4582642",
            "problem": "A Three-Dimensional Magnetic Resonance Imaging (3D MRI) dataset for abdominal organ segmentation is acquired with anisotropic voxel spacing, where the in-plane spacings are $s_{x} = 0.9\\,\\text{mm}$ and $s_{y} = 0.9\\,\\text{mm}$, and the through-plane spacing is $s_{z} = 4.5\\,\\text{mm}$. A Three-Dimensional Convolutional Neural Network (3D CNN) based on the U-Net segmentation architecture (U-Net) is to be trained with patches extracted from the volume. The encoder uses three downsampling stages, each with stride $(2, 2, 1)$ along $(x,y,z)$, and all convolutions use stride $1$ with \"same\" padding. To ensure consistent feature map dimensions at all pyramid levels, the patch dimensions along $x$ and $y$ must be divisible by $2^{3}$, while the $z$ dimension has no divisibility constraint from downsampling.\n\nDesign a patch such that the physical field of view (FOV) of the patch is isotropic across axes and meets a minimum physical extent requirement to capture sufficient anatomical context. Specifically, let the patch dimensions in voxels be $(P_{x}, P_{y}, P_{z})$, and require:\n- $P_{x} s_{x} = P_{y} s_{y} = P_{z} s_{z}$ (isotropic physical FOV),\n- $P_{x} s_{x} \\geq 72\\,\\text{mm}$ (minimum physical FOV),\n- $P_{x}$ and $P_{y}$ are divisible by $2^{3}$.\n\nUnder these constraints, determine the minimal integer triple $(P_{x}, P_{y}, P_{z})$ in voxels that satisfies all conditions. Express the final answer as a row matrix using the LaTeX pmatrix environment. No rounding is required, and no units should be included in the final answer box.",
            "solution": "The problem is to determine the minimal integer dimensions of a patch, $(P_{x}, P_{y}, P_{z})$, in voxels for training a 3D Convolutional Neural Network. The design must satisfy several constraints related to voxel spacing, network architecture, and physical field of view.\n\nFirst, we formalize the given information and constraints. The voxel spacings are $s_{x} = 0.9\\,\\text{mm}$, $s_{y} = 0.9\\,\\text{mm}$, and $s_{z} = 4.5\\,\\text{mm}$. The patch dimensions $(P_{x}, P_{y}, P_{z})$ must be integers.\n\nThe constraints are as follows:\n1.  **Architectural Constraint**: The encoder of the network has three downsampling stages with a stride of $2$ along the $x$ and $y$ axes. Therefore, the initial patch dimensions $P_{x}$ and $P_{y}$ must be divisible by $2^{3} = 8$ to ensure valid dimensions at all levels of the network. There is no such constraint on $P_z$ as the stride is $1$ along the $z$-axis.\n2.  **Isotropic FOV Constraint**: The physical field of view (FOV) of the patch must be isotropic. This means the physical size along each axis must be equal. Let this common physical size be $L$.\n    $$L = P_{x} s_{x} = P_{y} s_{y} = P_{z} s_{z}$$\n3.  **Minimum FOV Constraint**: The physical FOV must be at least $72\\,\\text{mm}$.\n    $$L \\geq 72\\,\\text{mm}$$\n\nOur objective is to find the minimal integer triple $(P_{x}, P_{y}, P_{z})$ that satisfies all these conditions.\n\nWe begin by analyzing the isotropic FOV constraint. Using the first two parts of the equality and the given spacings:\n$$P_{x} s_{x} = P_{y} s_{y}$$\n$$P_{x} (0.9) = P_{y} (0.9)$$\nThis simplifies to $P_{x} = P_{y}$. This means any constraint on $P_{x}$ also applies to $P_{y}$, which is consistent with the architectural constraint that both must be divisible by $8$.\n\nNext, we use the constraint relating the $x$ and $z$ dimensions:\n$$P_{x} s_{x} = P_{z} s_{z}$$\nSubstituting the given voxel spacings:\n$$P_{x} (0.9) = P_{z} (4.5)$$\nWe can solve for $P_{x}$ in terms of $P_{z}$:\n$$P_{x} = \\frac{4.5}{0.9} P_{z} = 5 P_{z}$$\nSince $P_{x}$ and $P_{z}$ must both be integers, this equation implies that $P_{x}$ must be an integer multiple of $5$.\n\nNow, we combine all the constraints on $P_{x}$:\na) $P_{x}$ must be divisible by $8$ (from the architectural constraint).\nb) $P_{x}$ must be divisible by $5$ (from the isotropic FOV and integer voxel constraints).\nc) The physical FOV $L = P_{x} s_{x}$ must be at least $72\\,\\text{mm}$.\n   $$P_{x} (0.9) \\geq 72$$\n   $$P_{x} \\geq \\frac{72}{0.9} = \\frac{720}{9} = 80$$\n\nTo satisfy both (a) and (b), $P_{x}$ must be a common multiple of $8$ and $5$. Since $8$ and $5$ are coprime (their greatest common divisor is $1$), $P_{x}$ must be a multiple of their least common multiple, which is $\\text{lcm}(8, 5) = 8 \\times 5 = 40$.\n\nSo, we are looking for the smallest integer $P_{x}$ that is a multiple of $40$ and is also greater than or equal to $80$. The multiples of $40$ are $40, 80, 120, \\dots$. The smallest multiple of $40$ that satisfies the condition $P_{x} \\geq 80$ is $80$.\n\nThus, the minimal value for $P_{x}$ is $80$.\n\nWith this minimal value for $P_{x}$, we can find the corresponding minimal values for $P_{y}$ and $P_{z}$:\n-   From $P_{x} = P_{y}$, we have $P_{y} = 80$.\n-   From $P_{x} = 5 P_{z}$, we have $80 = 5 P_{z}$, which gives $P_{z} = \\frac{80}{5} = 16$.\n\nThe minimal integer triple for the patch dimensions is $(P_{x}, P_{y}, P_{z}) = (80, 80, 16)$.\n\nAs a final check, we verify that this solution satisfies all initial conditions:\n-   $P_{x}=80$, $P_{y}=80$, $P_{z}=16$ are all integers.\n-   $P_{x}=80$ is divisible by $8$ ($80 = 8 \\times 10$).\n-   $P_{y}=80$ is divisible by $8$ ($80 = 8 \\times 10$).\n-   The physical FOV is $L = P_{x} s_{x} = 80 \\times 0.9\\,\\text{mm} = 72\\,\\text{mm}$. This satisfies $L \\geq 72\\,\\text{mm}$.\n-   The FOV is isotropic:\n    -   $P_{x} s_{x} = 80 \\times 0.9\\,\\text{mm} = 72\\,\\text{mm}$.\n    -   $P_{y} s_{y} = 80 \\times 0.9\\,\\text{mm} = 72\\,\\text{mm}$.\n    -   $P_{z} s_{z} = 16 \\times 4.5\\,\\text{mm} = 72\\,\\text{mm}$.\nAll conditions are met. Since we started by finding the smallest possible value for $P_x$, this triple is minimal.",
            "answer": "$$\\boxed{\\begin{pmatrix} 80 & 80 & 16 \\end{pmatrix}}$$"
        },
        {
            "introduction": "Class imbalance, where the target lesion occupies a tiny fraction of the image volume, is a defining challenge in medical segmentation. This exercise moves from data formatting to statistical handling, asking you to derive and implement three principled strategies to counteract imbalance from first principles. By working through Bayesian decision theory for optimal thresholding, inverse-prevalence weighting for loss functions, and calculating oversampling factors, you will build a robust toolkit for training models on skewed data distributions .",
            "id": "4582628",
            "problem": "You are analyzing binary medical image segmentation for detecting a lesion class against background in a voxel-wise classifier that outputs calibrated posterior probabilities. The core challenge is to handle class imbalance in training and decision-making while maintaining principled correctness grounded in statistical decision theory and the definitions of standard segmentation measures. Starting from fundamental principles, derive a set of implementable rules that address decision thresholds, loss weighting, and re-sampling.\n\nAssume a calibrated model that outputs a per-voxel posterior probability $p = P(Y=1 \\mid x)$ for the lesion class, where $Y \\in \\{0,1\\}$, $Y=1$ denotes lesion, and $Y=0$ denotes background. Let the lesion prevalence be $\\pi = P(Y=1)$ and the background prevalence be $1 - \\pi$. Let the misclassification cost for a false negative be $C_{\\mathrm{FN}}$ and for a false positive be $C_{\\mathrm{FP}}$. Let $\\epsilon > 0$ be a small smoothing constant. The training set size is large and independent and identically distributed; the model is well-calibrated, so the output $p$ equals the posterior under the training distribution.\n\nFrom these fundamentals, perform the following derivations and implement the corresponding computations:\n\n- Use Bayesian decision theory to derive the Bayes-optimal threshold $t$ on the posterior probability $p$ that minimizes the expected conditional risk under asymmetric misclassification costs $C_{\\mathrm{FN}}$ and $C_{\\mathrm{FP}}$. Explain how the prevalence $\\pi$ enters the posterior and whether it appears in the final decision rule when $p$ is calibrated, and then compute $t$ for each test case.\n\n- From the definition of the Sørensen–Dice coefficient for a single class in a multi-class setting and the generalized Dice loss, construct class weights for the lesion class and the background class that equalize the expected contribution of each class despite imbalance. Use the idea that the expected denominator terms scale with class sizes, and derive weights that are inversely proportional to class prevalence with smoothing by $\\epsilon$ to avoid singularities. Normalize the weights to sum to $1$ so that they form a convex combination. Compute the normalized weights $w_1$ for the lesion class and $w_0$ for the background class for each test case.\n\n- Suppose you perform re-sampling by oversampling the lesion (positive) class only, leaving the background (negative) class unchanged, to achieve a target effective prevalence $\\pi^\\star \\in (\\pi,1)$. Derive the oversampling factor $r$ needed so that the new prevalence after oversampling equals $\\pi^\\star$, in terms of the original prevalence $\\pi$ and the target $\\pi^\\star$. Compute $r$ for each test case.\n\nAll outputs must be real numbers expressed in decimal notation. No physical units or angles are involved. Implement a program that computes, for each test case, the tuple of four floats in the order $[t, w_1, w_0, r]$.\n\nUse the following test suite, which covers a general case, balanced case, severe imbalance, near-zero prevalence, and high prevalence:\n\n- Case $1$: $\\pi = 0.05$, $C_{\\mathrm{FN}} = 5$, $C_{\\mathrm{FP}} = 1$, $\\epsilon = 10^{-6}$, $\\pi^\\star = 0.5$.\n- Case $2$: $\\pi = 0.5$, $C_{\\mathrm{FN}} = 1$, $C_{\\mathrm{FP}} = 1$, $\\epsilon = 10^{-6}$, $\\pi^\\star = 0.5$.\n- Case $3$: $\\pi = 0.01$, $C_{\\mathrm{FN}} = 10$, $C_{\\mathrm{FP}} = 1$, $\\epsilon = 10^{-3}$, $\\pi^\\star = 0.2$.\n- Case $4$: $\\pi = 10^{-4}$, $C_{\\mathrm{FN}} = 100$, $C_{\\mathrm{FP}} = 1$, $\\epsilon = 10^{-2}$, $\\pi^\\star = 0.1$.\n- Case $5$: $\\pi = 0.9$, $C_{\\mathrm{FN}} = 1$, $C_{\\mathrm{FP}} = 5$, $\\epsilon = 10^{-6}$, $\\pi^\\star = 0.95$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with the values of all test cases concatenated in order and with no spaces. Concretely, the output must be\n$[t_1, w_{1,1}, w_{0,1}, r_1, t_2, w_{1,2}, w_{0,2}, r_2, t_3, w_{1,3}, w_{0,3}, r_3, t_4, w_{1,4}, w_{0,4}, r_4, t_5, w_{1,5}, w_{0,5}, r_5]$.",
            "solution": "The problem statement has been validated and is deemed scientifically grounded, well-posed, and objective. It presents a formalizable challenge within the domain of medical image analysis and statistical decision theory, founded on established principles. We shall proceed with a rigorous, step-by-step derivation of the requested quantities.\n\n**1. Derivation of the Bayes-Optimal Threshold $t$**\n\nThe objective is to find a decision threshold on the posterior probability $p = P(Y=1 \\mid x)$ that minimizes the expected conditional risk. The decision for each voxel is to predict either the lesion class ($\\hat{Y}=1$) or the background class ($\\hat{Y}=0$). The true class is denoted by $Y \\in \\{0, 1\\}$. We are given the costs for misclassification: $C_{\\mathrm{FN}}$ for a false negative (predicting $\\hat{Y}=0$ when $Y=1$) and $C_{\\mathrm{FP}}$ for a false positive (predicting $\\hat{Y}=1$ when $Y=0$).\n\nFor a given voxel with feature vector $x$, the calibrated model provides the posterior probability of the lesion class, $p = P(Y=1 \\mid x)$. The posterior probability of the background class is therefore $P(Y=0 \\mid x) = 1 - p$.\n\nThe conditional risk of making a decision $\\hat{Y}$ given the observation $x$ is the expected cost of that decision.\n\nThe conditional risk of deciding 'lesion' ($\\hat{Y}=1$) is the probability of this decision being wrong multiplied by the cost of that error:\n$$R(\\hat{Y}=1 \\mid x) = P(Y=0 \\mid x) \\cdot C_{\\mathrm{FP}} = (1 - p) C_{\\mathrm{FP}}$$\n\nThe conditional risk of deciding 'background' ($\\hat{Y}=0$) is similarly:\n$$R(\\hat{Y}=0 \\mid x) = P(Y=1 \\mid x) \\cdot C_{\\mathrm{FN}} = p C_{\\mathrm{FN}}$$\n\nBayesian decision theory dictates that we should choose the action that minimizes this conditional risk. Therefore, we decide 'lesion' ($\\hat{Y}=1$) if and only if:\n$$R(\\hat{Y}=1 \\mid x)  R(\\hat{Y}=0 \\mid x)$$\n$$(1 - p) C_{\\mathrm{FP}}  p C_{\\mathrm{FN}}$$\n\nTo find the decision threshold $t$, we solve this inequality for $p$:\n$$C_{\\mathrm{FP}} - p C_{\\mathrm{FP}}  p C_{\\mathrm{FN}}$$\n$$C_{\\mathrm{FP}}  p (C_{\\mathrm{FN}} + C_{\\mathrm{FP}})$$\n$$p > \\frac{C_{\\mathrm{FP}}}{C_{\\mathrm{FN}} + C_{\\mathrm{FP}}}$$\n\nThus, the optimal strategy is to classify a voxel as 'lesion' if its posterior probability $p$ exceeds a threshold $t$. The Bayes-optimal threshold $t$ is the value where the two risks are equal:\n$$t = \\frac{C_{\\mathrm{FP}}}{C_{\\mathrm{FN}} + C_{\\mathrm{FP}}}$$\n\nThe problem asks how the prevalence $\\pi = P(Y=1)$ enters this rule. According to Bayes' theorem, the posterior probability $p$ is a function of the prevalence $\\pi$, the class-conditional likelihoods $P(x \\mid Y=1)$ and $P(x \\mid Y=0)$, and the evidence $P(x)$:\n$$p = P(Y=1 \\mid x) = \\frac{P(x \\mid Y=1) P(Y=1)}{P(x)} = \\frac{P(x \\mid Y=1) \\pi}{P(x \\mid Y=1)\\pi + P(x \\mid Y=0)(1-\\pi)}$$\nThe problem states that the model is well-calibrated, meaning its output $p$ correctly represents the true posterior probability $P(Y=1 \\mid x)$ under the training distribution. This implies that the model has already learned and incorporated the influence of the training data's prevalence $\\pi$. Consequently, the decision rule $p > t$ implicitly accounts for prevalence through the value of $p$. The decision threshold $t$ itself, however, depends only on the misclassification costs $C_{\\mathrm{FN}}$ and $C_{\\mathrm{FP}}$, not explicitly on $\\pi$.\n\n**2. Derivation of Class Weights ($w_1, w_0$) for Dice Loss**\n\nThe task is to construct class weights for a Dice-based loss function that are inversely proportional to class prevalence, with a smoothing constant $\\epsilon > 0$ to prevent division by zero. The weights must be normalized to sum to $1$.\n\nLet the prevalence of the lesion class ($Y=1$) be $\\pi_1 = \\pi$ and the background class ($Y=0$) be $\\pi_0 = 1 - \\pi$.\nThe unnormalized weights, $\\tilde{w}_1$ and $\\tilde{w}_0$, are inversely proportional to the smoothed prevalences:\n$$\\tilde{w}_1 = \\frac{1}{\\pi_1 + \\epsilon} = \\frac{1}{\\pi + \\epsilon}$$\n$$\\tilde{w}_0 = \\frac{1}{\\pi_0 + \\epsilon} = \\frac{1}{1 - \\pi + \\epsilon}$$\n\nTo normalize these weights to sum to $1$, we divide each by their sum, $S = \\tilde{w}_1 + \\tilde{w}_0$:\n$$S = \\frac{1}{\\pi + \\epsilon} + \\frac{1}{1 - \\pi + \\epsilon} = \\frac{(1 - \\pi + \\epsilon) + (\\pi + \\epsilon)}{(\\pi + \\epsilon)(1 - \\pi + \\epsilon)} = \\frac{1 + 2\\epsilon}{(\\pi + \\epsilon)(1 - \\pi + \\epsilon)}$$\n\nThe normalized weight for the lesion class, $w_1$, is:\n$$w_1 = \\frac{\\tilde{w}_1}{S} = \\frac{\\frac{1}{\\pi + \\epsilon}}{\\frac{1 + 2\\epsilon}{(\\pi + \\epsilon)(1 - \\pi + \\epsilon)}} = \\frac{1}{\\pi + \\epsilon} \\cdot \\frac{(\\pi + \\epsilon)(1 - \\pi + \\epsilon)}{1 + 2\\epsilon} = \\frac{1 - \\pi + \\epsilon}{1 + 2\\epsilon}$$\n\nThe normalized weight for the background class, $w_0$, is:\n$$w_0 = \\frac{\\tilde{w}_0}{S} = \\frac{\\frac{1}{1 - \\pi + \\epsilon}}{\\frac{1 + 2\\epsilon}{(\\pi + \\epsilon)(1 - \\pi + \\epsilon)}} = \\frac{1}{1 - \\pi + \\epsilon} \\cdot \\frac{(\\pi + \\epsilon)(1 - \\pi + \\epsilon)}{1 + 2\\epsilon} = \\frac{\\pi + \\epsilon}{1 + 2\\epsilon}$$\n\nAs a check, $w_1 + w_0 = \\frac{1 - \\pi + \\epsilon}{1 + 2\\epsilon} + \\frac{\\pi + \\epsilon}{1 + 2\\epsilon} = \\frac{1 - \\pi + \\epsilon + \\pi + \\epsilon}{1 + 2\\epsilon} = \\frac{1 + 2\\epsilon}{1 + 2\\epsilon} = 1$. The derivation is correct.\n\n**3. Derivation of the Oversampling Factor $r$**\n\nWe aim to find the factor $r$ by which the lesion (positive) class must be oversampled to change the effective prevalence from an initial value $\\pi$ to a target value $\\pi^\\star$.\n\nLet the number of samples in the original dataset be $N_1$ for the lesion class and $N_0$ for the background class. The total number of samples is $N = N_0 + N_1$. The original prevalence is:\n$$\\pi = \\frac{N_1}{N_0 + N_1}$$\n\nOversampling the lesion class by a factor $r$ creates a new dataset with $N'_1 = r N_1$ lesion samples, while the number of background samples remains unchanged, $N'_0 = N_0$. The new total number of samples is $N' = N'_0 + N'_1 = N_0 + r N_1$.\n\nThe new prevalence, $\\pi^\\star$, is:\n$$\\pi^\\star = \\frac{N'_1}{N'} = \\frac{r N_1}{N_0 + r N_1}$$\n\nTo express $r$ in terms of $\\pi$ and $\\pi^\\star$, we first express $N_0$ and $N_1$ in terms of $\\pi$. From the definition of $\\pi$, we have $N_1 = \\pi(N_0 + N_1)$ and $N_0 = (1-\\pi)(N_0 + N_1)$. The ratio is $\\frac{N_0}{N_1} = \\frac{1-\\pi}{\\pi}$. So, $N_0 = N_1 \\frac{1-\\pi}{\\pi}$.\n\nSubstitute this expression for $N_0$ into the equation for $\\pi^\\star$:\n$$\\pi^\\star = \\frac{r N_1}{N_1 \\frac{1-\\pi}{\\pi} + r N_1}$$\nSince $N_1$ must be non-zero for this problem to be meaningful, we can divide the numerator and denominator by $N_1$:\n$$\\pi^\\star = \\frac{r}{\\frac{1-\\pi}{\\pi} + r}$$\n\nNow, we solve for $r$:\n$$\\pi^\\star \\left(\\frac{1-\\pi}{\\pi} + r\\right) = r$$\n$$\\pi^\\star \\frac{1-\\pi}{\\pi} + \\pi^\\star r = r$$\n$$\\pi^\\star \\frac{1-\\pi}{\\pi} = r - \\pi^\\star r$$\n$$\\pi^\\star \\frac{1-\\pi}{\\pi} = r(1 - \\pi^\\star)$$\n$$r = \\frac{\\pi^\\star (1 - \\pi)}{\\pi (1 - \\pi^\\star)}$$\nThis is the required oversampling factor for the lesion class.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes segmentation-related parameters for a series of test cases based on\n    statistical decision theory and standard machine learning practices for class imbalance.\n    \"\"\"\n    test_cases = [\n        # Case 1: General case\n        {'pi': 0.05, 'cfn': 5.0, 'cfp': 1.0, 'eps': 1e-6, 'pi_star': 0.5},\n        # Case 2: Balanced case\n        {'pi': 0.5, 'cfn': 1.0, 'cfp': 1.0, 'eps': 1e-6, 'pi_star': 0.5},\n        # Case 3: Severe imbalance\n        {'pi': 0.01, 'cfn': 10.0, 'cfp': 1.0, 'eps': 1e-3, 'pi_star': 0.2},\n        # Case 4: Near-zero prevalence\n        {'pi': 1e-4, 'cfn': 100.0, 'cfp': 1.0, 'eps': 1e-2, 'pi_star': 0.1},\n        # Case 5: High prevalence\n        {'pi': 0.9, 'cfn': 1.0, 'cfp': 5.0, 'eps': 1e-6, 'pi_star': 0.95},\n    ]\n\n    results = []\n    for case in test_cases:\n        pi = case['pi']\n        cfn = case['cfn']\n        cfp = case['cfp']\n        eps = case['eps']\n        pi_star = case['pi_star']\n\n        # 1. Bayes-optimal threshold t\n        # t = C_FP / (C_FN + C_FP)\n        t = cfp / (cfn + cfp)\n\n        # 2. Normalized class weights w1 (lesion) and w0 (background)\n        # w1 = (1 - pi + eps) / (1 + 2*eps)\n        # w0 = (pi + eps) / (1 + 2*eps)\n        w_denominator = 1.0 + 2.0 * eps\n        w1 = (1.0 - pi + eps) / w_denominator\n        w0 = (pi + eps) / w_denominator\n\n        # 3. Oversampling factor r\n        # r = (pi_star * (1 - pi)) / (pi * (1 - pi_star))\n        # Note: The problem setup ensures pi > 0, pi_star  1, so no division by zero.\n        r = (pi_star * (1.0 - pi)) / (pi * (1.0 - pi_star))\n        \n        results.extend([t, w1, w0, r])\n\n    # Format the final output as a single comma-separated list in brackets, with no spaces.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond static design choices, the dynamics of the training loop are paramount for achieving state-of-the-art performance in segmentation. This problem focuses on the practical implementation of a modern training protocol, translating core deep learning concepts into computable quantities. You will derive and apply expressions for data augmentation through label mixing, a region-based loss using the soft Dice coefficient, and a dynamic learning rate schedule based on cosine annealing, providing a hands-on experience with the engine of the training process .",
            "id": "4582643",
            "problem": "You are given a training setup for binary medical image segmentation that uses augmentation by linear label mixing and a learning rate schedule based on a cosine profile. The objective is to derive and implement, from first principles, the mathematical quantities that govern training under this protocol, and then compute them for specified test cases. The base principles you must use are the definitions of Empirical Risk Minimization (ERM), Cross-Entropy (CE) for Bernoulli labels, and the Dice Similarity Coefficient (DSC), together with the requirement that the learning rate schedule is a smooth function that starts at a maximum and ends at a minimum over a cycle. All angles must be treated in radians.\n\nTask A (Loss under label mixing): Consider two binary label vectors $y_1 \\in \\{0,1\\}^n$ and $y_2 \\in \\{0,1\\}^n$, and predicted positive class probabilities $p \\in [0,1]^n$ for a patch of $n$ pixels. The augmentation is a linear label mixing (commonly known as \"mixup\") yielding a soft label $y' = \\lambda y_1 + (1-\\lambda) y_2$ for a mixing coefficient $\\lambda \\in [0,1]$. Starting from the definition of the Cross-Entropy (CE) for Bernoulli targets and the linearity of expectation, derive the expression for the average per-pixel CE under the soft label $y'$ and implement its computation for given $p$, $y_1$, $y_2$, and $\\lambda$.\n\nTask B (Soft Dice for probabilistic predictions): Using the standard definition of the Dice Similarity Coefficient (DSC) for sets and extending it to probabilistic predictions by interpreting sums of products as expected overlap, derive the expression for the soft Dice between $p$ and $y'$ and implement its computation for given $p$, $y'$.\n\nTask C (Cosine-annealed learning rate within a cycle): A training protocol uses Cosine Annealing with Warm Restarts (CAWR) in which the learning rate over a cycle of length $T$ steps starts at a maximum $\\eta_0$ at step $t=0$ and ends at a minimum at step $t=T$, following a single half-wave of a cosine in radians. Starting from the requirements that the schedule is smooth, attains $\\eta_0$ at the start and a minimum at the end, and is monotonically decreasing over the cycle, derive a formula for the learning rate at an integer step $t \\in \\{0,1,\\dots,T\\}$ within a cycle and implement its computation for given $\\eta_0$, $T$, and $t$.\n\nTest Suite: For each test case, you are given $p$, $y_1$, $y_2$, $\\lambda$, $\\eta_0$, $T$, and $t$. Compute three quantities: the average CE loss (Task A), the soft Dice (Task B), and the learning rate (Task C). Use the following four test cases with $n=4$:\n\n- Test Case $1$:\n  - $p = [0.9, 0.2, 0.8, 0.1]$\n  - $y_1 = [1, 0, 1, 0]$\n  - $y_2 = [0, 1, 0, 1]$\n  - $\\lambda = 0.3$\n  - $\\eta_0 = 0.01$\n  - $T = 10$\n  - $t = 3$\n\n- Test Case $2$ (boundary mixing and cycle start):\n  - $p = [0.5, 0.5, 0.5, 0.5]$\n  - $y_1 = [1, 1, 0, 0]$\n  - $y_2 = [0, 0, 1, 1]$\n  - $\\lambda = 1.0$\n  - $\\eta_0 = 0.001$\n  - $T = 20$\n  - $t = 0$\n\n- Test Case $3$ (boundary mixing and cycle end):\n  - $p = [0.99, 0.01, 0.99, 0.01]$\n  - $y_1 = [1, 0, 1, 0]$\n  - $y_2 = [1, 0, 0, 1]$\n  - $\\lambda = 0.0$\n  - $\\eta_0 = 0.02$\n  - $T = 5$\n  - $t = 5$\n\n- Test Case $4$ (symmetric mixing and cycle end):\n  - $p = [0.8, 0.8, 0.2, 0.2]$\n  - $y_1 = [1, 1, 0, 0]$\n  - $y_2 = [0, 0, 1, 1]$\n  - $\\lambda = 0.5$\n  - $\\eta_0 = 0.005$\n  - $T = 7$\n  - $t = 7$\n\nFinal Output Format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case’s result should itself be a bracketed triple in the order $[\\text{loss}, \\text{dice}, \\text{learning\\ rate}]$. For example, the full output should look like $[[\\text{L}_1,\\text{D}_1,\\text{LR}_1],[\\text{L}_2,\\text{D}_2,\\text{LR}_2],\\dots]$. If any logarithm appears in your computation, use angles in radians and treat numerical stability by clipping probabilities to avoid undefined logarithms.",
            "solution": "The problem statement has been validated and found to be scientifically grounded, self-contained, and well-posed. All tasks involve the derivation and application of standard principles in machine learning and medical image analysis. We proceed with the solution by addressing each task in order.\n\n### Problem Validation\n**Step 1: Extract Givens**\n- **Data (per patch):**\n  - Binary label vectors: $y_1 \\in \\{0,1\\}^n$, $y_2 \\in \\{0,1\\}^n$\n  - Predicted positive class probabilities: $p \\in [0,1]^n$\n  - Patch size: $n$ pixels\n- **Augmentation and Training Parameters:**\n  - Soft label: $y' = \\lambda y_1 + (1-\\lambda) y_2$\n  - Mixing coefficient: $\\lambda \\in [0,1]$\n  - Maximum learning rate: $\\eta_0$\n  - Cycle length: $T$ steps\n  - Current step in cycle: $t \\in \\{0,1,\\dots,T\\}$\n- **Tasks:**\n  - (A) Derive and compute the average per-pixel Cross-Entropy (CE) loss for a prediction $p$ with respect to a soft label $y'$.\n  - (B) Derive and compute the soft Dice Similarity Coefficient (DSC) between $p$ and $y'$.\n  - (C) Derive and compute the learning rate $\\eta(t)$ for a Cosine Annealing schedule.\n- **Base Principles:**\n  - Empirical Risk Minimization (ERM)\n  - Cross-Entropy (CE) for Bernoulli labels\n  - Dice Similarity Coefficient (DSC)\n  - Cosine learning rate schedule properties (smooth, max at start, min at end, monotonic).\n- **Test Cases:**\n  - Four specific instances of $\\{p, y_1, y_2, \\lambda, \\eta_0, T, t\\}$ are provided with $n=4$.\n- **Numerical Considerations:** Use natural logarithms for CE, as is standard. Clip probabilities to avoid numerically undefined results (e.g., $\\log(0)$).\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is based on established concepts in machine learning: Cross-Entropy loss, Dice score, label mixing (mixup), and cosine annealing for learning rate scheduling. These are standard techniques.\n- **Well-Posed:** All tasks are clearly defined and have unique solutions based on the provided principles and data.\n- **Objective:** The problem is stated in precise, mathematical language, free from subjectivity.\n\n**Step 3: Verdict and Action**\nThe problem is valid. We will now derive the required expressions from first principles and then apply them to the test cases.\n\n### Derivations and Solution\n\n**Task A: Average Cross-Entropy Loss under Label Mixing**\n\nThe Cross-Entropy (CE) loss measures the dissimilarity between a true probability distribution and a predicted one. For a binary classification problem (e.g., pixel-wise segmentation), the ground truth for a single pixel $i$ is a Bernoulli random variable with parameter $y_i \\in \\{0,1\\}$. The predicted probability for the positive class ($1$) is $p_i$. The CE loss for this single pixel is given by:\n$$\n\\mathcal{L}_{CE,i}(p_i, y_i) = - [y_i \\log(p_i) + (1-y_i) \\log(1-p_i)]\n$$\nwhere $\\log$ denotes the natural logarithm, as is standard in information theory. In Empirical Risk Minimization (ERM), the total loss is the average loss over all $n$ pixels in the given data sample (image patch).\n\nThe problem introduces a soft label $y'$ derived from linear mixing of two hard labels, $y_1$ and $y_2$:\n$$\ny' = \\lambda y_1 + (1-\\lambda) y_2\n$$\nFor each pixel $i$, the soft label is $y'_i = \\lambda y_{1,i} + (1-\\lambda) y_{2,i}$. Since $y_{1,i}, y_{2,i} \\in \\{0,1\\}$ and $\\lambda \\in [0,1]$, we have $y'_i \\in [0,1]$. We can interpret $y'_i$ as the parameter of a Bernoulli distribution, i.e., the probability of the true label being $1$. The CE loss is a valid measure for such probabilistic targets.\n\nTo find the CE loss for the soft label $y'$, we substitute $y'_i$ for $y_i$ in the per-pixel loss formula:\n$$\n\\mathcal{L}_{CE,i}(p_i, y'_i) = - [y'_i \\log(p_i) + (1-y'_i) \\log(1-p_i)]\n$$\nThe average per-pixel CE loss over the patch of $n$ pixels is the mean of these individual losses:\n$$\n\\mathcal{L}_{CE}(p, y') = \\frac{1}{n} \\sum_{i=1}^n \\mathcal{L}_{CE,i}(p_i, y'_i) = -\\frac{1}{n} \\sum_{i=1}^n [y'_i \\log(p_i) + (1-y'_i) \\log(1-p_i)]\n$$\nThis is the final expression for the loss. As required by the problem for numerical stability, to prevent the logarithm from being undefined, the predicted probabilities $p_i$ must be clipped to a small-epsilon-bounded range, for instance $[\\epsilon, 1-\\epsilon]$ for a small $\\epsilon > 0$.\n\n**Task B: Soft Dice Similarity Coefficient**\n\nThe Dice Similarity Coefficient (DSC) is a statistic used to gauge the similarity of two sets. For sets $A$ and $B$, it is defined as:\n$$\nDSC(A,B) = \\frac{2 |A \\cap B|}{|A| + |B|}\n$$\nIn image segmentation, we can represent the ground truth mask and the predicted mask as binary vectors $y$ and $\\hat{y}$ of length $n$. The set operations translate to vector operations:\n- $|A| \\to \\sum_{i=1}^n y_i$ (the number of positive pixels in the ground truth)\n- $|B| \\to \\sum_{i=1}^n \\hat{y}_i$ (the number of positive pixels in the prediction)\n- $|A \\cap B| \\to \\sum_{i=1}^n y_i \\hat{y}_i$ (the number of correctly predicted positive pixels, i.e., true positives)\n\nThe task requires extending this to a probabilistic prediction $p$ and a soft ground truth $y'$. This is achieved by replacing the binary indicators with their probabilistic counterparts. The sums of products are interpreted as expected counts.\n- The prediction $\\hat{y}$ is replaced by the probability vector $p$.\n- The ground truth $y$ is replaced by the soft label vector $y'$.\n\nThe terms in the DSC formula are then generalized as follows:\n- Expected size of the predicted set: $\\sum_{i=1}^n p_i$\n- Expected size of the ground truth set: $\\sum_{i=1}^n y'_i$\n- Expected size of the intersection: $\\sum_{i=1}^n p_i y'_i$\n\nSubstituting these into the DSC definition yields the soft Dice coefficient:\n$$\nDSC_{soft}(p, y') = \\frac{2 \\sum_{i=1}^n p_i y'_i}{\\sum_{i=1}^n p_i + \\sum_{i=1}^n y'_i}\n$$\nThis formula computes the Dice score for probabilistic inputs. To prevent division by zero in cases where both the prediction and ground truth are all zeros, a small smoothing constant $\\epsilon$ can be added to the numerator and denominator, but we will adhere to the direct derivation as the problem does not specify this.\n\n**Task C: Cosine-Annealed Learning Rate**\n\nThe learning rate schedule $\\eta(t)$ must follow a single half-wave of a cosine over a cycle of $T$ steps, for $t \\in \\{0, 1, \\dots, T\\}$. A general cosine function is of the form $f(x) = A \\cos(x) + C$. We need to scale and shift this to meet the given criteria. Let the learning rate be given by:\n$$\n\\eta(t) = C + A \\cos(\\omega t + \\phi)\n$$\nThe requirements are:\n$1$. $\\eta(0) = \\eta_0$ (maximum value). This implies the cosine term should be at its maximum ($+1$) at $t=0$. Choosing $\\phi=0$ simplifies this. So, $\\eta(t) = C + A \\cos(\\omega t)$.\n$2$. $\\eta(T) = \\eta_{min}$ (minimum value). This implies the cosine term should be at its minimum ($-1$) at $t=T$.\n$3$. The schedule describes a single half-wave. This means the argument of the cosine must go from $0$ to $\\pi$ as $t$ goes from $0$ to $T$.\n\nFrom requirement $3$, we set the argument of the cosine to be $\\frac{t\\pi}{T}$. Our function becomes:\n$$\n\\eta(t) = C + A \\cos\\left(\\frac{t\\pi}{T}\\right)\n$$\nNow we apply the boundary conditions:\n- At $t=0$: $\\eta(0) = C + A \\cos(0) = C + A = \\eta_0$.\n- At $t=T$: $\\eta(T) = C + A \\cos(\\pi) = C - A = \\eta_{min}$.\n\nThe problem states the schedule \"ends at a minimum\", but does not specify a value for $\\eta_{min}$. Standard cosine annealing schedules often anneal to a minimum of $\\eta_{min}=0$. This is the most natural interpretation of a \"single half-wave\" that starts at a maximum. Assuming $\\eta_{min}=0$:\n- $C + A = \\eta_0$\n- $C - A = 0 \\implies C = A$\n\nSubstituting $C=A$ into the first equation gives $2A = \\eta_0$, so $A = \\frac{\\eta_0}{2}$. This also means $C = \\frac{\\eta_0}{2}$.\nThe final formula for the learning rate at step $t$ is therefore:\n$$\n\\eta(t) = \\frac{\\eta_0}{2} + \\frac{\\eta_0}{2} \\cos\\left(\\frac{t\\pi}{T}\\right) = \\frac{\\eta_0}{2}\\left(1 + \\cos\\left(\\frac{t\\pi}{T}\\right)\\right)\n$$\nThis function correctly starts at $\\eta(0)=\\eta_0$, ends at $\\eta(T)=0$, is smooth, and is monotonically decreasing over the interval $t \\in [0, T]$, thus satisfying all stated principles.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and computes quantities for a medical image segmentation training setup.\n    - Task A: Average Cross-Entropy loss with linear label mixing.\n    - Task B: Soft Dice Similarity Coefficient.\n    - Task C: Cosine-annealed learning rate.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case 1\n        {\n            \"p\": [0.9, 0.2, 0.8, 0.1], \"y1\": [1, 0, 1, 0], \"y2\": [0, 1, 0, 1],\n            \"lambda\": 0.3, \"eta0\": 0.01, \"T\": 10, \"t\": 3\n        },\n        # Test Case 2 (boundary mixing and cycle start)\n        {\n            \"p\": [0.5, 0.5, 0.5, 0.5], \"y1\": [1, 1, 0, 0], \"y2\": [0, 0, 1, 1],\n            \"lambda\": 1.0, \"eta0\": 0.001, \"T\": 20, \"t\": 0\n        },\n        # Test Case 3 (boundary mixing and cycle end)\n        {\n            \"p\": [0.99, 0.01, 0.99, 0.01], \"y1\": [1, 0, 1, 0], \"y2\": [1, 0, 0, 1],\n            \"lambda\": 0.0, \"eta0\": 0.02, \"T\": 5, \"t\": 5\n        },\n        # Test Case 4 (symmetric mixing and cycle end)\n        {\n            \"p\": [0.8, 0.8, 0.2, 0.2], \"y1\": [1, 1, 0, 0], \"y2\": [0, 0, 1, 1],\n            \"lambda\": 0.5, \"eta0\": 0.005, \"T\": 7, \"t\": 7\n        },\n    ]\n\n    results = []\n    \n    # Epsilon for numerical stability in log\n    epsilon = 1e-9\n\n    for case in test_cases:\n        # Unpack parameters\n        p_vec = np.array(case[\"p\"], dtype=float)\n        y1_vec = np.array(case[\"y1\"], dtype=float)\n        y2_vec = np.array(case[\"y2\"], dtype=float)\n        lambda_val = case[\"lambda\"]\n        eta0 = case[\"eta0\"]\n        T = case[\"T\"]\n        t = case[\"t\"]\n\n        # --- Task A: Average Cross-Entropy Loss with Label Mixing ---\n        # 1. Compute the soft label y'\n        y_prime = lambda_val * y1_vec + (1 - lambda_val) * y2_vec\n\n        # 2. Clip predictions for numerical stability\n        p_clipped = np.clip(p_vec, epsilon, 1 - epsilon)\n\n        # 3. Compute average cross-entropy loss\n        ce_loss = -np.mean(\n            y_prime * np.log(p_clipped) + (1 - y_prime) * np.log(1 - p_clipped)\n        )\n\n        # --- Task B: Soft Dice Similarity Coefficient ---\n        # 1. Compute numerator and denominator\n        numerator = 2 * np.sum(p_vec * y_prime)\n        denominator = np.sum(p_vec) + np.sum(y_prime)\n\n        # 2. Compute soft dice, handling potential division by zero\n        soft_dice = numerator / denominator if denominator > 0 else 0.0\n\n        # --- Task C: Cosine-Annealed Learning Rate ---\n        # Assumes eta_min = 0 as derived from problem statement principles.\n        # Handles T=0 case, though not present in tests.\n        if T == 0:\n            learning_rate = eta0\n        else:\n            learning_rate = (eta0 / 2.0) * (1 + np.cos(t * np.pi / T))\n        \n        results.append([ce_loss, soft_dice, learning_rate])\n\n    # Format the final output string exactly as required.\n    result_strings = [f\"[{res[0]},{res[1]},{res[2]}]\" for res in results]\n    final_output = f\"[{','.join(result_strings)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        }
    ]
}