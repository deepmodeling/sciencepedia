{
    "hands_on_practices": [
        {
            "introduction": "The power of Multiple Imputation (MI) lies not just in filling in missing values, but in correctly propagating the uncertainty caused by their absence. This practice delves into the cornerstone of MI analysis: Rubin's Rules for combining results. By working through the derivation and application of the total variance formula , you will gain a deep understanding of how to decompose uncertainty into its constituent parts—the 'within-imputation' variance (inherent sampling variability) and the 'between-imputation' variance (variability due to missingness).",
            "id": "4584865",
            "problem": "A clinical biomarker study examines the association between a gene-expression biomarker and a binary disease outcome using logistic regression. Due to missing biomarker measurements, the analysis uses Multiple Imputation (MI) under the Missing At Random (MAR) assumption with proper imputations. Let $Q$ denote a scalar regression coefficient (log-odds scale). Under the Bayesian posterior predictive framework for MI, each completed dataset $j \\in \\{1,\\dots,m\\}$ yields an estimate $Q^{(j)}$ and its corresponding complete-data variance estimate $U^{(j)}$. Assume large-sample approximate normality of $Q$ and independence of imputations. \n\nTasks:\n1) Starting from the law of total variance for $Q$ conditional on the observed data, derive a large-sample estimator for the total variance of the MI point estimate that combines the average complete-data variance and the across-imputation variability, and identify the finite-$m$ correction that accounts for Monte Carlo error introduced by using only $m$ imputations. Clearly explain the distinct roles of the within-imputation variance and the between-imputation variance in the decomposition, and analyze how the number of imputations $m$ affects the Monte Carlo error component.\n\n2) In a specific analysis with $m=10$ imputations, the scalar estimates and complete-data variances are:\n- $Q^{(1)}=0.72$, $U^{(1)}=0.038$\n- $Q^{(2)}=0.80$, $U^{(2)}=0.041$\n- $Q^{(3)}=0.85$, $U^{(3)}=0.040$\n- $Q^{(4)}=0.90$, $U^{(4)}=0.039$\n- $Q^{(5)}=0.88$, $U^{(5)}=0.042$\n- $Q^{(6)}=0.79$, $U^{(6)}=0.037$\n- $Q^{(7)}=0.92$, $U^{(7)}=0.040$\n- $Q^{(8)}=0.86$, $U^{(8)}=0.039$\n- $Q^{(9)}=0.81$, $U^{(9)}=0.040$\n- $Q^{(10)}=0.87$, $U^{(10)}=0.041$\n\nCompute the fraction of missing information, defined as the ratio of the extra variance attributable to missingness to the derived total variance from part (1). Express your final numerical answer as a decimal and round to $4$ significant figures. Do not use a percent sign in your final answer.",
            "solution": "This problem addresses the principles of combining results from multiple imputations (MI), a standard technique for handling missing data in statistical analysis. The solution is presented in two parts as requested.\n\n### Part 1: Derivation and Analysis of the Total Variance Estimator\n\nLet $Y$ represent the complete dataset, which can be partitioned into the observed data, $Y_{obs}$, and the missing data, $Y_{mis}$. Let $Q$ be the scalar quantity of interest, which in this case is a regression coefficient from a logistic regression model. The goal of MI is to make inferences about $Q$ based on $Y_{obs}$.\n\nIn the Bayesian framework for MI, we are interested in the posterior distribution of $Q$ given the observed data, $p(Q|Y_{obs})$. The mean of this distribution is the point estimate for $Q$, and its variance, $\\text{Var}(Q|Y_{obs})$, represents the total uncertainty about $Q$.\n\nThe multiple imputation process involves three steps:\n1.  Impute the missing values $m$ times, drawing from the posterior predictive distribution of the missing data, $p(Y_{mis}|Y_{obs})$, to create $m$ completed datasets, $\\{(Y_{obs}, Y_{mis}^{(1)}), ..., (Y_{obs}, Y_{mis}^{(m)})\\}$.\n2.  Analyze each of the $m$ completed datasets using the standard complete-data method. For each dataset $j$, this yields a point estimate $Q^{(j)}$ and its associated variance estimate $U^{(j)}$.\n3.  Combine the $m$ results into a single set of estimates and confidence intervals. The MI point estimate for $Q$ is the average of the individual estimates:\n    $$ \\bar{Q}_m = \\frac{1}{m} \\sum_{j=1}^{m} Q^{(j)} $$\n\nTo derive the total variance of this estimator, we start from the law of total variance, applied to the posterior distribution of $Q$ given $Y_{obs}$:\n$$ \\text{Var}(Q|Y_{obs}) = E[\\text{Var}(Q|Y_{obs}, Y_{mis})] + \\text{Var}(E[Q|Y_{obs}, Y_{mis}]) $$\nThe expectation and variance in this expression are taken over the posterior distribution of $Y_{mis}$ given $Y_{obs}$. Let us analyze each term:\n\n1.  **Within-Imputation Variance**: The first term, $E[\\text{Var}(Q|Y_{obs}, Y_{mis})]$, represents the average complete-data variance. The quantity $\\text{Var}(Q|Y_{obs}, Y_{mis})$ is the variance of $Q$ we would have if the data were complete (i.e., if we knew $Y_{mis}$). This is the source of uncertainty due to finite sampling, inherent even in the absence of missing data. The estimator for this variance in a single completed dataset $j$ is $U^{(j)}$. By averaging these across the $m$ imputations, we obtain an estimate for the expected complete-data variance:\n    $$ \\bar{U}_m = \\frac{1}{m} \\sum_{j=1}^{m} U^{(j)} $$\n    This is termed the **within-imputation variance**, as it captures the average sampling variance *within* each completed dataset.\n\n2.  **Between-Imputation Variance**: The second term, $\\text{Var}(E[Q|Y_{obs}, Y_{mis}])$, represents the additional variance that arises because the missing data $Y_{mis}$ are unknown. The point estimate $E[Q|Y_{obs}, Y_{mis}]$ would change depending on the specific values imputed for $Y_{mis}$. The variance of these point estimates across the posterior predictive distribution for $Y_{mis}$ thus quantifies the uncertainty due to missingness. We estimate this from the variability of the $m$ point estimates $Q^{(j)}$ around their mean $\\bar{Q}_m$:\n    $$ B_m = \\frac{1}{m-1} \\sum_{j=1}^{m} (Q^{(j)} - \\bar{Q}_m)^2 $$\n    This is the **between-imputation variance**. It directly reflects the extra uncertainty introduced by the fact that data are missing. If there were no missing data, all $Q^{(j)}$ would be identical, and $B_m$ would be $0$.\n\nCombining these components, the total posterior variance of $Q$ is estimated as the sum of the within- and between-imputation variances: $T \\approx \\bar{U}_m + B_m$. However, we use the MI estimator $\\bar{Q}_m$, which is itself a sample mean based on a finite number, $m$, of imputations. This introduces an additional source of Monte Carlo simulation error. The variance of the estimator $\\bar{Q}_m$ includes not only the posterior variance of $Q$ but also the variance from using a finite sample of imputations.\n\nThe variance estimator for $\\bar{Q}_m$, used for constructing confidence intervals and conducting hypothesis tests, must account for this. The total variance estimator, denoted $T_m$, is therefore given by Rubin's rule:\n$$ T_m = \\bar{U}_m + B_m + \\frac{B_m}{m} = \\bar{U}_m + \\left(1 + \\frac{1}{m}\\right)B_m $$\nHere, the term $\\frac{B_m}{m}$ is the **finite-$m$ correction**. It explicitly accounts for the Monte Carlo error introduced by using a finite number of imputations instead of an infinite number.\n\nThe number of imputations $m$ directly affects this Monte Carlo error component. As $m$ increases, the term $\\frac{B_m}{m}$ decreases and approaches $0$. For an infinitely large $m$, the total variance would be $T_\\infty = \\bar{U}_\\infty + B_\\infty$, which represents the true posterior variance of $Q$ given the observed data. A small value of $m$ results in a larger Monte Carlo error, leading to a less precise (i.e., higher variance) and less reproducible estimate of the total variance. A larger $m$ reduces this simulation error, yielding more stable and reliable inferences.\n\n### Part 2: Calculation of the Fraction of Missing Information\n\nWe are given data from $m=10$ imputations. We will first calculate the necessary components: the average within-imputation variance ($\\bar{U}_{10}$) and the between-imputation variance ($B_{10}$).\n\nThe MI point estimate is:\n$$ \\bar{Q}_{10} = \\frac{1}{10} \\sum_{j=1}^{10} Q^{(j)} = \\frac{1}{10}(0.72 + 0.80 + 0.85 + 0.90 + 0.88 + 0.79 + 0.92 + 0.86 + 0.81 + 0.87) = \\frac{8.40}{10} = 0.84 $$\n\nThe average within-imputation variance is:\n$$ \\bar{U}_{10} = \\frac{1}{10} \\sum_{j=1}^{10} U^{(j)} = \\frac{1}{10}(0.038 + 0.041 + 0.040 + 0.039 + 0.042 + 0.037 + 0.040 + 0.039 + 0.040 + 0.041) = \\frac{0.397}{10} = 0.0397 $$\n\nThe between-imputation variance is:\n$$ B_{10} = \\frac{1}{10-1} \\sum_{j=1}^{10} (Q^{(j)} - \\bar{Q}_{10})^2 $$\nThe sum of squared deviations is:\n$$ \\sum (Q^{(j)} - 0.84)^2 = (-0.12)^2 + (-0.04)^2 + (0.01)^2 + (0.06)^2 + (0.04)^2 + (-0.05)^2 + (0.08)^2 + (0.02)^2 + (-0.03)^2 + (0.03)^2 $$\n$$ \\sum (Q^{(j)} - 0.84)^2 = 0.0144 + 0.0016 + 0.0001 + 0.0036 + 0.0016 + 0.0025 + 0.0064 + 0.0004 + 0.0009 + 0.0009 = 0.0324 $$\nTherefore,\n$$ B_{10} = \\frac{0.0324}{9} = 0.0036 $$\n\nNext, we calculate the total variance using the derived formula from Part 1 with $m=10$:\n$$ T_{10} = \\bar{U}_{10} + \\left(1 + \\frac{1}{10}\\right)B_{10} = 0.0397 + (1.1)(0.0036) = 0.0397 + 0.00396 = 0.04366 $$\n\nThe problem defines the fraction of missing information (FMI) as the ratio of the extra variance attributable to missingness to the derived total variance. The total variance is $T_{10}$. The \"extra variance attributable to missingness\" is the part of the total variance that would disappear if the data were complete. This corresponds to the sum of the between-imputation variance and the Monte Carlo correction term, which is $(1 + 1/m)B_m$.\n\nThus, the fraction of missing information is calculated as:\n$$ \\text{FMI} = \\frac{(1 + 1/m)B_m}{T_m} = \\frac{(1 + 1/10)B_{10}}{T_{10}} $$\n$$ \\text{FMI} = \\frac{0.00396}{0.04366} \\approx 0.09070087036... $$\n\nRounding this result to $4$ significant figures, we get $0.09070$.",
            "answer": "$$\\boxed{0.09070}$$"
        },
        {
            "introduction": "Modern omics datasets often present a 'large $p$, small $n$' challenge, where standard regression-based imputation is ill-suited. This practice introduces matrix completion, a powerful technique that treats the entire dataset as a matrix with an underlying low-rank structure. You will formulate the problem using nuclear norm minimization, a convex relaxation of the rank, and perform a hands-on calculation using the soft-impute algorithm , revealing how global data structure can be exploited to fill in missing entries in high-dimensional settings.",
            "id": "4584852",
            "problem": "Consider a multi-omics data matrix $X \\in \\mathbb{R}^{n \\times p}$ whose entries represent standardized molecular measurements (for example, transcript abundances across $n$ samples and $p$ genes). The analyst observes a partially observed matrix $Y \\in \\mathbb{R}^{n \\times p}$, where entries are missing on an index set $\\Omega^{c}$, and the observed indices are collected in $\\Omega \\subset \\{1,\\dots,n\\} \\times \\{1,\\dots,p\\}$. The projection operator onto observed indices is defined by $P_{\\Omega}(Z)_{ij} = Z_{ij}$ if $(i,j) \\in \\Omega$ and $P_{\\Omega}(Z)_{ij} = 0$ otherwise. Assume the missingness mechanism is either Missing Completely At Random (MCAR) or Missing At Random (MAR) so that the likelihood factorization justifies consistent estimation from the observed entries. Under the standard latent factor model in omics ($X$ approximately low-rank due to a small number of biological and technical latent factors), and the identifiability condition of incoherence (singular vectors of $X$ not overly aligned with the coordinate axes), formulate from first principles a convex optimization problem that replaces direct rank minimization with a convex surrogate, penalizes model complexity, and fits only to the observed entries. Explicitly state the modeling assumptions (approximate low rank and incoherence) and the role they play in justifying the relaxation and identifiability for high-dimensional omics.\n\nThen, to concretize imputation via proximal methods, consider the toy omics matrix $Y \\in \\mathbb{R}^{2 \\times 2}$ with entries $Y_{11} = 4$, $Y_{12} = 2$, $Y_{21} = 2$, and $Y_{22}$ missing, with $\\Omega = \\{(1,1),(1,2),(2,1)\\}$. Perform one iteration of the soft-impute procedure starting from $X^{(0)} = 0$, which consists of forming $W^{(1)} = P_{\\Omega}(Y)$ and applying singular value thresholding at level $\\lambda = 1$ to obtain $X^{(1)}$. Report the imputed value $X^{(1)}_{22}$ as an exact algebraic expression (no rounding). In your derivation, clearly indicate any properties you use (for example, for symmetric matrices, the left and right singular vectors coincide with normalized eigenvectors up to signs). Your final answer must be the single expression for $X^{(1)}_{22}$.",
            "solution": "This problem has two parts. The first is to formulate the convex optimization problem for matrix completion. The second is to perform a single iteration of the soft-impute algorithm on a toy example.\n\n### Part 1: Formulation of the Convex Optimization Problem\n\nThe fundamental goal of matrix completion is to recover the full data matrix $X$ from its partially observed version $Y$. The core assumption is that the true matrix $X$ has a low-rank structure. The rank of a matrix is a measure of its complexity. Therefore, a natural starting point is to seek the matrix of the lowest possible rank that agrees with the observed data. This can be formulated as a constrained rank minimization problem:\n$$\n\\min_{Z \\in \\mathbb{R}^{n \\times p}} \\text{rank}(Z) \\quad \\text{subject to} \\quad P_{\\Omega}(Z) = P_{\\Omega}(Y)\n$$\nThis problem, however, is computationally intractable (NP-hard) because the rank function is non-convex. The standard approach is to relax the rank function to its convex envelope, the **nuclear norm**, denoted $\\|Z\\|_*$, which is the sum of the singular values of the matrix $Z$:\n$$\n\\|Z\\|_* = \\sum_{i=1}^{\\min(n,p)} \\sigma_i(Z)\n$$\nIn practice, observed data contains noise, so enforcing an exact match is undesirable. A more robust formulation minimizes a combination of a data fidelity term and the complexity penalty. This leads to the standard convex formulation for matrix completion, often called the soft-impute objective:\n$$\n\\min_{X \\in \\mathbb{R}^{n \\times p}} \\frac{1}{2} \\|P_{\\Omega}(Y - X)\\|_F^2 + \\lambda \\|X\\|_*\n$$\nHere, $\\|P_{\\Omega}(Y - X)\\|_F^2$ is the sum of squared errors on the observed entries, and the parameter $\\lambda > 0$ controls the trade-off between fitting the data and enforcing a low-rank structure.\n\n**Role of Modeling Assumptions:**\n\n1.  **Approximate Low Rank:** This is the foundational assumption. It posits that high-dimensional omics data is governed by a small number of latent factors, making the data matrix approximately low-rank. This structure is what allows for principled imputation; without it, the problem would be ill-posed. The nuclear norm penalty is the mathematical device used to enforce this assumed structure.\n\n2.  **Incoherence:** This is a technical condition ensuring that the singular vectors of $X$ are sufficiently \"spread out\" and not concentrated on a few coordinates. It ensures that the information about each latent factor is distributed across many matrix entries, so that observing a random subset is sufficient to recover the underlying structure.\n\n### Part 2: Calculation for the Toy Example\n\nWe perform one iteration of the soft-impute algorithm with initial guess $X^{(0)} = 0$ and $\\lambda=1$. The update is $X^{(1)} = S_{\\lambda}(P_{\\Omega}(Y) + P_{\\Omega^c}(X^{(0)}))$, where $S_{\\lambda}$ is the singular value thresholding operator. This simplifies to $X^{(1)} = S_{\\lambda}(P_{\\Omega}(Y))$.\n\n**Step 1: Form the matrix to be thresholded**\nGiven $Y_{11} = 4$, $Y_{12} = 2$, $Y_{21} = 2$, and $Y_{22}$ missing, the matrix $W = P_{\\Omega}(Y)$ is formed by setting the missing value to $0$:\n$$\nW = \\begin{pmatrix} 4 & 2 \\\\ 2 & 0 \\end{pmatrix}\n$$\n\n**Step 2: Compute the SVD of $W$**\nSince $W$ is symmetric, its singular values are the absolute values of its eigenvalues. We find the eigenvalues by solving the characteristic equation $\\det(W - \\mu I) = 0$:\n$$\n\\det\\begin{pmatrix} 4-\\mu & 2 \\\\ 2 & -\\mu \\end{pmatrix} = (4-\\mu)(-\\mu) - 4 = \\mu^2 - 4\\mu - 4 = 0\n$$\nUsing the quadratic formula, the eigenvalues are $\\mu = \\frac{4 \\pm \\sqrt{16 - 4(-4)}}{2} = 2 \\pm 2\\sqrt{2}$.\nThe singular values are $\\sigma_1 = |2+2\\sqrt{2}| = 2+2\\sqrt{2}$ and $\\sigma_2 = |2-2\\sqrt{2}| = 2\\sqrt{2}-2$.\n\n**Step 3: Apply Soft-Thresholding with $\\lambda=1$**\nThe new singular values, $\\sigma'_i$, are $\\sigma'_i = \\max(\\sigma_i - \\lambda, 0)$:\n$$\n\\sigma'_1 = (2 + 2\\sqrt{2}) - 1 = 1 + 2\\sqrt{2}\n$$\n$$\n\\sigma'_2 = \\max((2\\sqrt{2} - 2) - 1, 0) = \\max(2\\sqrt{2} - 3, 0) = 0 \\quad (\\text{since } 2\\sqrt{2} \\approx 2.828  3)\n$$\nThe resulting matrix $X^{(1)}$ will be a rank-1 matrix.\n\n**Step 4: Reconstruct the matrix $X^{(1)}$**\nThe reconstructed matrix is $X^{(1)} = \\sigma'_1 u_1 v_1^T$. Since $W$ is symmetric, we can set $u_1=v_1$, where $v_1$ is the normalized eigenvector corresponding to the eigenvalue $\\mu_1 = \\sigma_1$. We find the eigenvector for $\\mu_1 = 2 + 2\\sqrt{2}$ by solving $(W - \\mu_1 I)v=0$, which gives $(2-2\\sqrt{2})x + 2y = 0$. An unnormalized eigenvector is thus proportional to $\\begin{pmatrix} 1 \\\\ \\sqrt{2}-1 \\end{pmatrix}$.\nThe normalized eigenvector is $v_1 = \\frac{1}{\\sqrt{1^2 + (\\sqrt{2}-1)^2}} \\begin{pmatrix} 1 \\\\ \\sqrt{2}-1 \\end{pmatrix} = \\frac{1}{\\sqrt{4-2\\sqrt{2}}} \\begin{pmatrix} 1 \\\\ \\sqrt{2}-1 \\end{pmatrix}$.\n\nThe imputed value is the $(2,2)$ entry of $X^{(1)} = \\sigma'_1 v_1 v_1^T$:\n$$\nX^{(1)}_{22} = \\sigma'_1 \\times (v_1)_2^2 = (1 + 2\\sqrt{2}) \\left( \\frac{\\sqrt{2}-1}{\\sqrt{4-2\\sqrt{2}}} \\right)^2\n$$\n$$\nX^{(1)}_{22} = (1 + 2\\sqrt{2}) \\frac{(\\sqrt{2}-1)^2}{4-2\\sqrt{2}} = (1 + 2\\sqrt{2}) \\frac{3 - 2\\sqrt{2}}{4-2\\sqrt{2}}\n$$\nExpanding the numerator gives $(1+2\\sqrt{2})(3-2\\sqrt{2}) = 3 - 2\\sqrt{2} + 6\\sqrt{2} - 8 = 4\\sqrt{2} - 5$. So we have:\n$$\nX^{(1)}_{22} = \\frac{4\\sqrt{2} - 5}{4-2\\sqrt{2}}\n$$\nTo simplify, we rationalize the denominator:\n$$\nX^{(1)}_{22} = \\frac{4\\sqrt{2} - 5}{4-2\\sqrt{2}} \\times \\frac{4+2\\sqrt{2}}{4+2\\sqrt{2}} = \\frac{(4\\sqrt{2})(4) + (4\\sqrt{2})(2\\sqrt{2}) - 5(4) - 5(2\\sqrt{2})}{4^2 - (2\\sqrt{2})^2}\n$$\n$$\nX^{(1)}_{22} = \\frac{16\\sqrt{2} + 16 - 20 - 10\\sqrt{2}}{16 - 8} = \\frac{6\\sqrt{2} - 4}{8} = \\frac{3\\sqrt{2} - 2}{4}\n$$",
            "answer": "$$\\boxed{\\frac{3\\sqrt{2}-2}{4}}$$"
        },
        {
            "introduction": "Real-world datasets are messy, often mixing continuous, ordinal, and categorical data, which poses a significant challenge for standard imputation methods. This hands-on exercise introduces the Gaussian copula model, a sophisticated semiparametric framework designed to handle such mixed-type data. By implementing a copula-based imputation procedure , you will learn how to transform different data types into a unified latent Gaussian space, estimate their dependence structure, and perform principled imputation that respects the unique characteristics of each variable.",
            "id": "4584874",
            "problem": "Consider a semiparametric Gaussian copula model for mixed data with continuous expression features and an ordinal genotype. Let the latent vector be $\\mathbf{Z} = (Z_{1}, Z_{2}, Z_{G})^{\\top} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{R})$, where $\\mathbf{R}$ is a correlation matrix. The observed continuous expression variables $(X_{1}, X_{2})$ are linked to $(Z_{1}, Z_{2})$ via strictly increasing transformations $X_{j} = f_{j}(Z_{j})$ for $j \\in \\{1,2\\}$, and the observed genotype $G \\in \\{0,1,2\\}$ is ordinal, linked to $Z_{G}$ by thresholds $-\\infty = \\tau_{0}  \\tau_{1}  \\tau_{2}  \\tau_{3} = +\\infty$ so that $G = g$ if and only if $\\tau_{g}  Z_{G} \\le \\tau_{g+1}$. The missingness is assumed to be Missing at Random (MAR) in the sense of Rubin: the probability that an entry is missing may depend on the observed data but not on the missing values themselves. You will implement a copula-based Multiple Imputation (MI) procedure that estimates the latent correlation matrix $\\mathbf{R}$ using rank-based information and then computes imputed probabilities for missing genotypes by conditioning on observed expressions. You must express all probabilities as decimals, not percentages.\n\nYour program must carry out the following steps from first principles:\n\n1) Estimation of latent marginals and thresholds using rank information.\n   - For each continuous expression variable $X_{j}$, transform the observed values to latent normal scores via the empirical cumulative distribution function (empirical ranks with midranks for ties) and the probit link: for $n$ observed values of $X_{j}$, if $\\operatorname{rank}(x_{i}) \\in \\{1,\\ldots,n\\}$ is the midrank of $x_{i}$ among observed values, define $u_{i} = \\operatorname{rank}(x_{i})/(n+1)$ and $z_{i} = \\Phi^{-1}(u_{i})$, where $\\Phi^{-1}$ is the inverse standard normal cumulative distribution function. This yields pseudo-latent normal scores $Z_{j}$ for observed entries.\n   - For the ordinal genotype $G \\in \\{0,1,2\\}$, estimate the thresholds $\\tau_{1}$ and $\\tau_{2}$ using the observed category proportions with a small smoothing term to avoid boundary issues. Let the observed counts be $c_{0}, c_{1}, c_{2}$ and $n_{G} = c_{0} + c_{1} + c_{2}$ be the number of non-missing genotypes. With smoothing $\\epsilon = 10^{-3}$, set $p_{k} = (c_{k} + \\epsilon)/(n_{G} + 3\\epsilon)$ for $k \\in \\{0,1,2\\}$, and define\n   $$\\tau_{1} = \\Phi^{-1}(p_{0}), \\quad \\tau_{2} = \\Phi^{-1}(p_{0} + p_{1}).$$\n   To construct pseudo-latent values for observed genotypes, assign to each level $g \\in \\{0,1,2\\}$ the conditional mean of a truncated standard normal between its thresholds,\n   $$m_{g} = \\mathbb{E}[Z \\mid \\tau_{g}  Z \\le \\tau_{g+1}] = \\frac{\\varphi(\\tau_{g}) - \\varphi(\\tau_{g+1})}{\\Phi(\\tau_{g+1}) - \\Phi(\\tau_{g})},$$\n   where $\\varphi$ is the standard normal probability density function and $\\Phi$ is the standard normal cumulative distribution function, with the conventions $\\varphi(\\pm\\infty) = 0$, $\\Phi(-\\infty) = 0$, and $\\Phi(+\\infty) = 1$. Assign $Z_{G} = m_{g}$ for observed $G = g$ and $Z_{G} = \\text{NA}$ for missing $G$.\n\n2) Estimation of the latent correlation matrix.\n   - Assemble a matrix of pseudo-latent variables $(Z_{1}, Z_{2}, Z_{G})$ using the above transformations, leaving entries as missing where data are missing. Estimate the pairwise Pearson correlations using complete observations for each pair and assemble the symmetric matrix $\\widehat{\\mathbf{R}}$. If numerical issues result in a matrix that is not positive semidefinite, project $\\widehat{\\mathbf{R}}$ to the nearest valid correlation matrix by eigenvalue clipping and rescaling to unit diagonal. Denote the result by $\\widehat{\\mathbf{R}}^{+}$.\n\n3) Conditional genotype probabilities under the Gaussian copula.\n   - For each specified subject with missing $G$ and observed $(X_{1}, X_{2}) = \\mathbf{x}$, compute the subject’s latent normal scores $(z_{1}, z_{2})$ using the same rank-based normal-score transformation as in Step $1$ (computed on the full observed sample of each $X_{j}$). Partition $\\widehat{\\mathbf{R}}^{+}$ as\n   $$\\widehat{\\mathbf{R}}^{+} = \\begin{pmatrix} \\mathbf{R}_{xx}  \\mathbf{r}_{xG} \\\\ \\mathbf{r}_{Gx}^{\\top}  1 \\end{pmatrix},$$\n   where $\\mathbf{R}_{xx} \\in \\mathbb{R}^{2 \\times 2}$ and $\\mathbf{r}_{Gx} \\in \\mathbb{R}^{2}$. The conditional distribution of $Z_{G}$ given $\\mathbf{Z}_{x} = (Z_{1}, Z_{2})^{\\top} = \\mathbf{z}$ is univariate normal with mean\n   $$\\mu(\\mathbf{z}) = \\mathbf{r}_{Gx}^{\\top} \\mathbf{R}_{xx}^{-1} \\mathbf{z},$$\n   and variance\n   $$\\sigma^{2}(\\mathbf{z}) = 1 - \\mathbf{r}_{Gx}^{\\top} \\mathbf{R}_{xx}^{-1} \\mathbf{r}_{Gx}.$$\n   Using the thresholds $(\\tau_{1}, \\tau_{2})$, compute the conditional probabilities for $g \\in \\{0,1,2\\}$ as\n   $$\\mathbb{P}(G = g \\mid \\mathbf{X} = \\mathbf{x}) = \\Phi\\!\\left(\\frac{\\tau_{g+1} - \\mu(\\mathbf{z})}{\\sigma(\\mathbf{z})}\\right) - \\Phi\\!\\left(\\frac{\\tau_{g} - \\mu(\\mathbf{z})}{\\sigma(\\mathbf{z})}\\right).$$\n\nYour implementation should follow the above principles and compute the requested quantities for the test suite below. No physical units are involved. All probabilities must be output as decimals, not percentages.\n\nTest suite datasets and queries:\n\n- Test case $1$ (happy path, missing completely at random). Dataset $\\mathcal{D}_{1}$ with $n = 8$ subjects:\n  - $X_{1} = [\\,6.2,\\,5.9,\\,6.5,\\,7.0,\\,5.1,\\,4.8,\\,5.4,\\,6.8\\,]$,\n  - $X_{2} = [\\,2.1,\\,2.3,\\,2.0,\\,2.8,\\,1.9,\\,1.7,\\,2.2,\\,2.6\\,]$,\n  - $G = [\\,0,\\,1,\\,1,\\,2,\\,0,\\,1,\\,\\text{NA},\\,2\\,]$.\n  Compute $\\big(\\mathbb{P}(G=0\\mid \\mathbf{X}),\\,\\mathbb{P}(G=1\\mid \\mathbf{X}),\\,\\mathbb{P}(G=2\\mid \\mathbf{X})\\big)$ for the subject with $\\text{NA}$ genotype (the seventh entry).\n\n- Test case $2$ (missing at random with strong association). Dataset $\\mathcal{D}_{2}$ with $n = 9$ subjects:\n  - $X_{1} = [\\,4.5,\\,4.8,\\,5.5,\\,5.7,\\,5.9,\\,6.5,\\,6.7,\\,6.4,\\,6.9\\,]$,\n  - $X_{2} = [\\,1.2,\\,1.3,\\,1.6,\\,1.7,\\,1.5,\\,2.0,\\,2.1,\\,1.9,\\,2.2\\,]$,\n  - $G = [\\,0,\\,0,\\,1,\\,1,\\,1,\\,2,\\,2,\\,\\text{NA},\\,2\\,]$.\n  Compute $\\big(\\mathbb{P}(G=0\\mid \\mathbf{X}),\\,\\mathbb{P}(G=1\\mid \\mathbf{X}),\\,\\mathbb{P}(G=2\\mid \\mathbf{X})\\big)$ for the subject with $\\text{NA}$ genotype (the eighth entry).\n\n- Test case $3$ (edge case with ties and small counts). Dataset $\\mathcal{D}_{3}$ with $n = 6$ subjects:\n  - $X_{1} = [\\,5.0,\\,5.0,\\,5.1,\\,5.1,\\,5.2,\\,5.3\\,]$,\n  - $X_{2} = [\\,1.8,\\,1.8,\\,1.9,\\,1.9,\\,2.0,\\,2.1\\,]$,\n  - $G = [\\,0,\\,1,\\,\\text{NA},\\,1,\\,2,\\,\\text{NA}\\,]$.\n  Compute $\\big(\\mathbb{P}(G=0\\mid \\mathbf{X}),\\,\\mathbb{P}(G=1\\mid \\mathbf{X}),\\,\\mathbb{P}(G=2\\mid \\mathbf{X})\\big)$ for the last subject (the sixth entry) with $\\text{NA}$ genotype.\n\nImplementation requirements and output specification:\n\n- Use a smoothing constant $\\epsilon = 10^{-3}$ in the threshold estimation for $G$.\n- Estimate correlations by Pearson correlation of pseudo-latent normals as described, using pairwise complete observations, and project to the nearest correlation matrix when needed.\n- For each test case, return a list of three decimals corresponding to $\\big(\\mathbb{P}(G=0\\mid \\mathbf{X}),\\,\\mathbb{P}(G=1\\mid \\mathbf{X}),\\,\\mathbb{P}(G=2\\mid \\mathbf{X})\\big)$ for the specified subject, rounded to four decimal places.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with no spaces, where each element is itself a three-entry list in the same format. For example, an output with three test cases must look like \"[[a,b,c],[d,e,f],[g,h,i]]\" with each of $a,\\ldots,i$ being decimals rounded to four places.",
            "solution": "This solution follows the three main steps outlined in the problem: 1) estimation of latent variables and model parameters from observed data, 2) estimation of the latent correlation structure, and 3) computation of conditional probabilities for imputation.\n\n### Step 1: Estimation of Latent Marginals and Thresholds\n\nThe core principle of the semiparametric Gaussian copula model is that the dependence structure of a set of variables can be modeled by a multivariate normal distribution, even if the marginal distributions of the variables themselves are not normal. The link between the observed data and the latent normal variables is established through non-parametric, rank-based transformations for continuous variables and thresholding for ordinal variables.\n\n**Continuous Variables ($X_1, X_2$)**:\nFor each continuous variable $X_j$, we map the observed values to a standard normal distribution while preserving their ranks. This is a non-parametric estimation of the marginal cumulative distribution function (CDF). For a set of $n_j$ observed values of $X_j$, the rank of an observation $x_{ij}$, denoted $\\operatorname{rank}(x_{ij})$, provides its relative position. We use midranks to handle ties. The empirical CDF value for observation $i$ is calculated as:\n$$\nu_{ij} = \\frac{\\operatorname{rank}(x_{ij})}{n_j + 1}\n$$\nThese uniform scores are then transformed into standard normal scores using the inverse normal CDF, or probit function $\\Phi^{-1}$:\n$$\nz_{ij} = \\Phi^{-1}(u_{ij})\n$$\nThese $z_{ij}$ values serve as the pseudo-observations from the latent normal variable $Z_j$.\n\n**Ordinal Variable ($G$)**:\nFor the ordinal variable $G \\in \\{0,1,2\\}$, the model posits a latent standard normal variable $Z_G$ and a set of thresholds $\\tau_g$ such that $G=g$ if and only if $\\tau_g  Z_G \\le \\tau_{g+1}$. The thresholds are estimated from the empirical cumulative frequencies of the categories. Given observed counts $c_0, c_1, c_2$ for $n_G$ non-missing observations, we first calculate smoothed proportions:\n$$\np_k = \\frac{c_k + \\epsilon}{n_G + 3\\epsilon} \\quad \\text{for } k \\in \\{0,1,2\\}\n$$\nwhere $\\epsilon = 10^{-3}$ is a small smoothing constant. The thresholds are then estimated by applying the probit function to the cumulative proportions:\n$$\n\\tau_1 = \\Phi^{-1}(p_0), \\quad \\tau_2 = \\Phi^{-1}(p_0 + p_1)\n$$\nBy definition, we have $\\tau_0 = -\\infty$ and $\\tau_3 = +\\infty$.\n\nTo create pseudo-latent normal scores for the observed genotypes, we use the conditional expectation of $Z_G$ given that it falls within the estimated interval for a given category $g$. The conditional expectation of a standard normal variable $Z$ truncated to the interval $(a, b]$ is given by $(\\varphi(a) - \\varphi(b))/(\\Phi(b) - \\Phi(a))$, where $\\varphi$ and $\\Phi$ are the standard normal PDF and CDF, respectively.\n$$\nm_g = \\mathbb{E}[Z_G \\mid \\tau_g  Z_G \\le \\tau_{g+1}] = \\frac{\\varphi(\\tau_g) - \\varphi(\\tau_{g+1})}{\\Phi(\\tau_{g+1}) - \\Phi(\\tau_g)}\n$$\nFor each observed genotype $G_i = g$, we assign the pseudo-latent score $Z_{G,i} = m_g$.\n\n### Step 2: Estimation of the Latent Correlation Matrix\n\nWith the pseudo-latent variables $(Z_1, Z_2, Z_G)$ constructed, we estimate the latent correlation matrix $\\mathbf{R}$. The off-diagonal elements $\\widehat{R}_{jk}$ are estimated using the Pearson correlation coefficient between the vectors of pseudo-latent scores for variables $j$ and $k$, using only the subset of subjects for which both variables are observed (pairwise complete observations). This yields a symmetric matrix $\\widehat{\\mathbf{R}}$ with ones on the diagonal.\n\nDue to estimation from incomplete data, the resulting matrix $\\widehat{\\mathbf{R}}$ may not be positive semidefinite (PSD), a requirement for a correlation matrix. If so, it must be projected to the nearest valid correlation matrix. A standard procedure is eigenvalue clipping: compute the eigendecomposition of $\\widehat{\\mathbf{R}}$, clip any non-positive eigenvalues to a small positive value, reconstruct the matrix, and rescale the diagonal elements to be 1. The result is denoted $\\widehat{\\mathbf{R}}^{+}$.\n\n### Step 3: Conditional Genotype Probabilities\n\nFor a subject with a missing genotype $G$ but observed expression values $\\mathbf{X} = (X_1, X_2) = \\mathbf{x}$, we first find their corresponding latent scores $\\mathbf{z} = (z_1, z_2)$ using the rank-based transformation from Step 1. The imputation of $G$ is then framed as finding the conditional probability distribution of $G$ given $\\mathbf{Z}_x = \\mathbf{z}$ under the fitted multivariate normal model $\\mathcal{N}(\\mathbf{0}, \\widehat{\\mathbf{R}}^{+})$.\n\nLet the latent vector be partitioned as $\\mathbf{Z} = (\\mathbf{Z}_x^\\top, Z_G)^\\top$, where $\\mathbf{Z}_x = (Z_1, Z_2)^\\top$. The estimated correlation matrix $\\widehat{\\mathbf{R}}^{+}$ is partitioned conformably:\n$$\n\\widehat{\\mathbf{R}}^{+} = \\begin{pmatrix} \\mathbf{R}_{xx}  \\mathbf{r}_{Gx} \\\\ \\mathbf{r}_{Gx}^\\top  1 \\end{pmatrix}\n$$\nwhere $\\mathbf{R}_{xx}$ is the $2 \\times 2$ correlation matrix of $(Z_1, Z_2)$ and $\\mathbf{r}_{Gx}$ is the $2 \\times 1$ vector of correlations between $(Z_1, Z_2)$ and $Z_G$.\n\nAccording to the properties of the multivariate normal distribution, the conditional distribution of $Z_G$ given $\\mathbf{Z}_x = \\mathbf{z}$ is also normal, $Z_G \\mid \\mathbf{Z}_x=\\mathbf{z} \\sim \\mathcal{N}(\\mu(\\mathbf{z}), \\sigma^2(\\mathbf{z}))$, with conditional mean and variance:\n$$\n\\mu(\\mathbf{z}) = \\mathbf{r}_{Gx}^\\top \\mathbf{R}_{xx}^{-1} \\mathbf{z}\n$$\n$$\n\\sigma^2(\\mathbf{z}) = 1 - \\mathbf{r}_{Gx}^\\top \\mathbf{R}_{xx}^{-1} \\mathbf{r}_{Gx}\n$$\nWith the conditional distribution of $Z_G$ established, we can compute the probability of the missing genotype being $g \\in \\{0,1,2\\}$ by integrating the conditional normal density over the corresponding interval defined by the thresholds $(\\tau_g, \\tau_{g+1}]$:\n$$\n\\mathbb{P}(G=g \\mid \\mathbf{X}=\\mathbf{x}) = \\mathbb{P}(\\tau_g  Z_G \\le \\tau_{g+1} \\mid \\mathbf{Z}_x=\\mathbf{z}) = \\Phi\\left(\\frac{\\tau_{g+1} - \\mu(\\mathbf{z})}{\\sigma(\\mathbf{z})}\\right) - \\Phi\\left(\\frac{\\tau_{g} - \\mu(\\mathbf{z})}{\\sigma(\\mathbf{z})}\\right)\n$$\nwhere $\\sigma(\\mathbf{z}) = \\sqrt{\\sigma^2(\\mathbf{z})}$. This provides the imputed probabilities for the missing genotype.",
            "answer": "[[0.3475,0.4907,0.1618],[0.0465,0.4704,0.4831],[0.053,0.4952,0.4518]]"
        }
    ]
}