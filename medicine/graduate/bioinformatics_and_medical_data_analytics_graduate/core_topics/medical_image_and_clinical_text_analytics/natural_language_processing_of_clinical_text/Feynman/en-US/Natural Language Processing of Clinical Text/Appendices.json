{
    "hands_on_practices": [
        {
            "introduction": "The first crucial step in any clinical NLP pipeline is tokenization, the process of segmenting raw text into meaningful units, or tokens. This task is deceptively complex in the clinical domain due to its unique blend of abbreviations, numeric lab values with units, and dictation artifacts. This practice  provides a hands-on comparison of different tokenization strategies, from simple whitespace splitting to more sophisticated rule-based and subword methods, revealing how these initial choices can fundamentally alter the input to downstream models and impact metrics like vocabulary coverage.",
            "id": "4588759",
            "problem": "Consider the following clinical note snippet (verbatim, including punctuation and spacing):\nPt c/o chest pain, 2/10, onset ~3h ago; Na 142 mg/dL; uh... denies SOB.\n\nYou will analyze three tokenization schemes applied to this snippet and compute their token counts and out-of-vocabulary rates, then combine these quantities into a single scalar. Work from first principles of tokenization and out-of-vocabulary rate definitions as described below.\n\nTokenization schemes:\n1. Whitespace tokenization: Split the snippet on spaces only (that is, on space characters). No punctuation splitting is performed; punctuation remains attached to the surrounding text if present.\n2. Rule-based tokenization that preserves numeric units:\n   - First split on whitespace as above to obtain preliminary tokens.\n   - Then, for each preliminary token:\n     - If the token contains the exact substring \"...\", split it into the substring before \"...\" (if non-empty) and a separate token \"...\".\n     - Detach any single trailing character that is a comma, semicolon, or period (\",\", \";\", \".\") and emit it as its own token. Do not split inside slashes (\"/\") and do not detach punctuation when it is part of a unit (e.g., \"mg/dL\") or part of the three-character ellipsis \"...\".\n   - After punctuation handling, merge numeric-unit expressions: if a token that is exactly a numeral optionally prefixed by \"~\" (that is, \"~\" followed by digits, or digits alone) is immediately followed by a unit token from the unit list U = {\"mg/dL\", \"mmHg\", \"h\"}, merge the two into a single token by concatenation without any intervening space (e.g., \"142\" followed by \"mg/dL\" becomes \"142mg/dL\"; \"~3\" followed by \"h\" is equivalent to \"~3h\" which remains a single token).\n3. Subword Byte Pair Encoding (BPE)-style tokenization:\n   - Lowercase the snippet prior to subword segmentation.\n   - Split on whitespace to obtain words, then segment each word left-to-right using a greedy longest-match rule over the following subword vocabulary V_b:\n     {\"pt\", \"c\", \"/\", \"o\", \"chest\", \"pain\", \",\", \"2/10\", \"onset\", \"~\", \"3h\", \"ago\", \";\", \"na\", \"142\", \"mg/dl\", \"uh\", \"...\", \"denies\", \"sob\", \".\"}.\n   - If a word cannot be fully segmented using V_b, represent that entire word with the special symbol \"[UNK]\" as a single subword token. For this problem, treat only the literal token \"[UNK]\" as out-of-vocabulary at the subword level.\n\nLexicons for out-of-vocabulary (OOV) determination:\n- For whitespace tokenization, use the word-level lexicon V_w = {\"Pt\", \"c/o\", \"chest\", \"pain\", \"2/10\", \"onset\", \"~3h\", \"ago\", \"Na\", \"142\", \"mg/dL\", \"uh...\", \"denies\", \"SOB\"}.\n- For rule-based tokenization, use the token lexicon V_r = {\"Pt\", \"c/o\", \"chest\", \"pain\", \"2/10\", \"onset\", \"~3h\", \"ago\", \"Na\", \"142mg/dL\", \"uh\", \"...\", \"denies\", \"SOB\", \",\", \";\", \".\"}.\n- For BPE tokenization, use the subword vocabulary V_b above and define the OOV symbol exclusively as \"[UNK]\".\n\nDefinitions:\n- The token count under a tokenization scheme is the total number of tokens produced by that scheme on the snippet.\n- The out-of-vocabulary rate under a scheme is defined as the number of tokens not found in the scheme’s corresponding lexicon (for BPE: the number of subword tokens equal to \"[UNK]\") divided by the total number of tokens produced by that scheme.\n\nTasks:\n- Compute the number of tokens produced by whitespace tokenization, by the rule-based tokenization, and by the BPE tokenization. Denote these by $N_{\\mathrm{ws}}$, $N_{\\mathrm{rule}}$, and $N_{\\mathrm{bpe}}$, respectively.\n- Compute the out-of-vocabulary rates under each scheme. Denote these by $R_{\\mathrm{ws}}$, $R_{\\mathrm{rule}}$, and $R_{\\mathrm{bpe}}$, respectively.\n- Finally, compute the scalar\n$$S \\equiv \\left(N_{\\mathrm{bpe}} - N_{\\mathrm{rule}}\\right) + \\left(R_{\\mathrm{ws}} - R_{\\mathrm{rule}}\\right).$$\n\nProvide the final value of $S$ as a single reduced fraction. Do not round. No units are required.",
            "solution": "The problem posed is a well-defined exercise in computational linguistics applied to clinical text. All procedures and definitions are explicitly stated, allowing for a unique and verifiable solution. The problem is therefore valid. We shall proceed by systematically analyzing each specified tokenization scheme to compute the required quantities.\n\nThe clinical note snippet is: \"Pt c/o chest pain, 2/10, onset ~3h ago; Na 142 mg/dL; uh... denies SOB.\"\n\n**1. Whitespace Tokenization**\n\nThis scheme splits the text solely on space characters.\nApplying this rule to the snippet yields the following tokens:\n`\"Pt\"`, `\"c/o\"`, `\"chest\"`, `\"pain,\"`, `\"2/10,\"`, `\"onset\"`, `\"~3h\"`, `\"ago;\"`, `\"Na\"`, `\"142\"`, `\"mg/dL;\"`, `\"uh...\"`, `\"denies\"`, `\"SOB.\"`\n\nThe total number of tokens is the count of elements in this list.\n$$N_{\\mathrm{ws}} = 14$$\n\nNext, we calculate the out-of-vocabulary (OOV) rate. The given word-level lexicon is $V_w = \\{\"Pt\", \"c/o\", \"chest\", \"pain\", \"2/10\", \"onset\", \"\\~3h\", \"ago\", \"Na\", \"142\", \"mg/dL\", \"uh...\", \"denies\", \"SOB\"\\}$. A token is considered OOV if it is not present in $V_w$. We check each token:\n- `\"Pt\"`: in $V_w$\n- `\"c/o\"`: in $V_w$\n- `\"chest\"`: in $V_w$\n- `\"pain,\"`: **not** in $V_w$ (due to the comma)\n- `\"2/10,\"`: **not** in $V_w$ (due to the comma)\n- `\"onset\"`: in $V_w$\n- `\"~3h\"`: in $V_w$\n- `\"ago;\"`: **not** in $V_w$ (due to the semicolon)\n- `\"Na\"`: in $V_w$\n- `\"142\"`: in $V_w$\n- `\"mg/dL;\"`: **not** in $V_w$ (due to the semicolon)\n- `\"uh...\"`: in $V_w$\n- `\"denies\"`: in $V_w$\n- `\"SOB.\"`: **not** in $V_w$ (due to the period)\n\nThe number of OOV tokens is $5$.\nThe OOV rate, $R_{\\mathrm{ws}}$, is the ratio of OOV tokens to the total number of tokens.\n$$R_{\\mathrm{ws}} = \\frac{5}{14}$$\n\n**2. Rule-based Tokenization**\n\nThis scheme involves a multi-step process.\nStep (a): Initial split on whitespace, which gives the same $14$ preliminary tokens as in the first scheme.\nStep (b): Punctuation handling.\n- `\"pain,\"` $\\rightarrow$ `\"pain\"`, `\",\"`\n- `\"2/10,\"` $\\rightarrow$ `\"2/10\"`, `\",\"`\n- `\"ago;\"` $\\rightarrow$ `\"ago\"`, `\";\"`\n- `\"mg/dL;\"` $\\rightarrow$ `\"mg/dL\"`, `\";\"`\n- `\"uh...\"` $\\rightarrow$ `\"uh\"`, `\"...\"`\n- `\"SOB.\"` $\\rightarrow$ `\"SOB\"`, `\".\"`\nThe list of tokens after this step is:\n`\"Pt\"`, `\"c/o\"`, `\"chest\"`, `\"pain\"`, `\",\"`, `\"2/10\"`, `\",\"`, `\"onset\"`, `\"~3h\"`, `\"ago\"`, `\";\"`, `\"Na\"`, `\"142\"`, `\"mg/dL\"`, `\";\"`, `\"uh\"`, `\"...\"`, `\"denies\"`, `\"SOB\"`, `\".\"`\n\nStep (c): Merge numeric-unit expressions. The unit list is $U = \\{\"mg/dL\", \"mmHg\", \"h\"\\}$. We look for a sequence of a numeral token followed by a unit token from $U$. In the list above, the token `\"142\"` is a numeral, and it is immediately followed by `\"mg/dL\"`, which is in $U$. These two tokens are merged.\n- `\"142\"`, `\"mg/dL\"` $\\rightarrow$ `\"142mg/dL\"`\n\nThe final list of tokens for the rule-based scheme is:\n`\"Pt\"`, `\"c/o\"`, `\"chest\"`, `\"pain\"`, `\",\"`, `\"2/10\"`, `\",\"`, `\"onset\"`, `\"~3h\"`, `\"ago\"`, `\";\"`, `\"Na\"`, `\"142mg/dL\"`, `\";\"`, `\"uh\"`, `\"...\"`, `\"denies\"`, `\"SOB\"`, `\".\"`\n\nThe total count of these tokens is:\n$$N_{\\mathrm{rule}} = 19$$\n\nThe lexicon for this scheme is $V_r = \\{\"Pt\", \"c/o\", \"chest\", \"pain\", \"2/10\", \"onset\", \"\\~3h\", \"ago\", \"Na\", \"142mg/dL\", \"uh\", \"...\", \"denies\", \"SOB\", \",\", \";\", \".\"\\}$. By inspection, every single one of the $19$ tokens generated by our rule-based process is present in $V_r$. Therefore, the number of OOV tokens is $0$.\nThe OOV rate is:\n$$R_{\\mathrm{rule}} = \\frac{0}{19} = 0$$\n\n**3. BPE-style Tokenization**\n\nStep (a): The snippet is lowercased:\n`\"pt c/o chest pain, 2/10, onset ~3h ago; na 142 mg/dl; uh... denies sob.\"`\nStep (b): Splitting on whitespace gives the initial word list:\n`[\"pt\", \"c/o\", \"chest\", \"pain,\", \"2/10,\", \"onset\", \"~3h\", \"ago;\", \"na\", \"142\", \"mg/dl;\", \"uh...\", \"denies\", \"sob.\"]`\nStep (c): Each word is segmented using a greedy longest-match with the subword vocabulary $V_b = \\{\"pt\", \"c\", \"/\", \"o\", \"chest\", \"pain\", \",\", \"2/10\", \"onset\", \"\\~\", \"3h\", \"ago\", \";\", \"na\", \"142\", \"mg/dl\", \"uh\", \"...\", \"denies\", \"sob\", \".\"\\}$.\n- `\"pt\"` $\\rightarrow$ `[\"pt\"]` ($1$ token)\n- `\"c/o\"` $\\rightarrow$ `[\"c\", \"/\", \"o\"]` ($3$ tokens)\n- `\"chest\"` $\\rightarrow$ `[\"chest\"]` ($1$ token)\n- `\"pain,\"` $\\rightarrow$ `[\"pain\", \",\"]` ($2$ tokens)\n- `\"2/10,\"` $\\rightarrow$ `[\"2/10\", \",\"]` ($2$ tokens)\n- `\"onset\"` $\\rightarrow$ `[\"onset\"]` ($1$ token)\n- `\"~3h\"` $\\rightarrow$ `[\"~\", \"3h\"]` ($2$ tokens)\n- `\"ago;\"` $\\rightarrow$ `[\"ago\", \";\"]` ($2$ tokens)\n- `\"na\"` $\\rightarrow$ `[\"na\"]` ($1$ token)\n- `\"142\"` $\\rightarrow$ `[\"142\"]` ($1$ token)\n- `\"mg/dl;\"` $\\rightarrow$ `[\"mg/dl\", \";\"]` ($2$ tokens)\n- `\"uh...\"` $\\rightarrow$ `[\"uh\", \"...\"]` ($2$ tokens)\n- `\"denies\"` $\\rightarrow$ `[\"denies\"]` ($1$ token)\n- `\"sob.\"` $\\rightarrow$ `[\"sob\", \".\"]` ($2$ tokens)\n\nAll words were segmented completely. No `\"[UNK]\"` tokens were generated.\nThe total number of BPE tokens is the sum of the counts for each word:\n$$N_{\\mathrm{bpe}} = 1 + 3 + 1 + 2 + 2 + 1 + 2 + 2 + 1 + 1 + 2 + 2 + 1 + 2 = 23$$\nThe number of OOV tokens, defined as the count of `\"[UNK]\"`, is $0$.\nThe OOV rate is:\n$$R_{\\mathrm{bpe}} = \\frac{0}{23} = 0$$\n\n**Final Calculation**\n\nWe are asked to compute the scalar $S$:\n$$S \\equiv \\left(N_{\\mathrm{bpe}} - N_{\\mathrm{rule}}\\right) + \\left(R_{\\mathrm{ws}} - R_{\\mathrm{rule}}\\right)$$\nWe substitute the values calculated above:\n- $N_{\\mathrm{bpe}} = 23$\n- $N_{\\mathrm{rule}} = 19$\n- $R_{\\mathrm{ws}} = \\frac{5}{14}$\n- $R_{\\mathrm{rule}} = 0$\n\n$$S = (23 - 19) + \\left(\\frac{5}{14} - 0\\right)$$\n$$S = 4 + \\frac{5}{14}$$\nTo express this as a single fraction, we find a common denominator:\n$$S = \\frac{4 \\times 14}{14} + \\frac{5}{14} = \\frac{56}{14} + \\frac{5}{14} = \\frac{61}{14}$$\nThe number $61$ is prime, and $14 = 2 \\times 7$. Thus, the fraction $\\frac{61}{14}$ is in its simplest form.",
            "answer": "$$\\boxed{\\frac{61}{14}}$$"
        },
        {
            "introduction": "Once clinical text is tokenized, we can apply models to extract structured information, such as identifying symptoms or diseases through Named Entity Recognition (NER). State-of-the-art sequence tagging models often combine neural networks (like BiLSTMs) with a Conditional Random Field (CRF) layer to ensure the output labels are structurally valid. This exercise  demystifies the CRF layer by guiding you through the Viterbi algorithm, the dynamic programming method used to find the most likely sequence of labels, providing a concrete understanding of how models learn and enforce syntactic constraints like the BIO scheme.",
            "id": "4588740",
            "problem": "Consider a Bidirectional Long Short-Term Memory–Conditional Random Field (BiLSTM-CRF) sequence labeling model applied to a short snippet of clinical text: the tokenized sequence is [\"severe\", \"chest\", \"pain\", \"today\"]. The label set follows the Begin-Inside-Outside (BIO) scheme for a single entity type \"symptom\", with labels $\\mathrm{O}$, $\\mathrm{B}$, and $\\mathrm{I}$ denoting Outside, Begin-symptom, and Inside-symptom, respectively. In a linear-chain Conditional Random Field (CRF), the log-score of a labeled path is the sum of per-token emission log-potentials and label-to-label transition log-potentials, including the start and stop transitions. You will use this definition as the fundamental base.\n\nAssume the following log-potentials (all values are in log-space):\n\nStart-to-label transitions:\n$$A_{\\mathrm{START}\\rightarrow \\mathrm{O}} = 0.0,\\quad A_{\\mathrm{START}\\rightarrow \\mathrm{B}} = -0.5,\\quad A_{\\mathrm{START}\\rightarrow \\mathrm{I}} = -\\infty.$$\n\nLabel-to-label transitions (for $\\mathrm{O}$, $\\mathrm{B}$, $\\mathrm{I}$):\n$$A_{\\mathrm{O}\\rightarrow \\mathrm{O}} = 0.1,\\quad A_{\\mathrm{O}\\rightarrow \\mathrm{B}} = 0.0,\\quad A_{\\mathrm{O}\\rightarrow \\mathrm{I}} = -\\infty,$$\n$$A_{\\mathrm{B}\\rightarrow \\mathrm{O}} = -0.2,\\quad A_{\\mathrm{B}\\rightarrow \\mathrm{B}} = -1.0,\\quad A_{\\mathrm{B}\\rightarrow \\mathrm{I}} = 0.6,$$\n$$A_{\\mathrm{I}\\rightarrow \\mathrm{O}} = -0.2,\\quad A_{\\mathrm{I}\\rightarrow \\mathrm{B}} = -1.5,\\quad A_{\\mathrm{I}\\rightarrow \\mathrm{I}} = 0.5.$$\n\nLabel-to-stop transitions:\n$$A_{\\mathrm{O}\\rightarrow \\mathrm{STOP}} = 0.0,\\quad A_{\\mathrm{B}\\rightarrow \\mathrm{STOP}} = -0.3,\\quad A_{\\mathrm{I}\\rightarrow \\mathrm{STOP}} = -0.3.$$\n\nPer-token emission log-potentials $E_{t}(y)$ for token index $t \\in \\{1,2,3,4\\}$ and label $y \\in \\{\\mathrm{O},\\mathrm{B},\\mathrm{I}\\}$ are:\n$$E_{1}(\\mathrm{O}) = 0.4,\\quad E_{1}(\\mathrm{B}) = -0.3,\\quad E_{1}(\\mathrm{I}) = -1.0,$$\n$$E_{2}(\\mathrm{O}) = -0.2,\\quad E_{2}(\\mathrm{B}) = 1.2,\\quad E_{2}(\\mathrm{I}) = 0.5,$$\n$$E_{3}(\\mathrm{O}) = -0.5,\\quad E_{3}(\\mathrm{B}) = 0.8,\\quad E_{3}(\\mathrm{I}) = 1.5,$$\n$$E_{4}(\\mathrm{O}) = 0.6,\\quad E_{4}(\\mathrm{B}) = -0.6,\\quad E_{4}(\\mathrm{I}) = -0.8.$$\n\nAll $-\\infty$ entries encode invalid transitions mandated by the BIO constraints (for example, a transition $\\mathrm{O}\\rightarrow \\mathrm{I}$ is illegal).\n\nStarting from the core definition of a linear-chain CRF path log-score as a sum over start, transitions, emissions, and stop, derive the dynamic programming procedure consistent with Viterbi maximum-sum decoding and use it to compute the best tag sequence for the given token sequence. Show the intermediate score updates and backpointers used to recover the best path. Finally, compute the total best-path log-score including the stop transition. Express the final result as a single real-valued number; no rounding is required.",
            "solution": "The problem is well-posed, scientifically grounded, and provides all necessary data for a unique solution. It describes a standard application of the Viterbi algorithm for decoding in a linear-chain Conditional Random Field (CRF), a fundamental technique in natural language processing for sequence labeling tasks. All log-potential values are provided, and the constraints are consistent with the Begin-Inside-Outside (BIO) annotation scheme. Therefore, the problem is valid, and we may proceed with the solution.\n\nThe log-score $S$ for a given tag sequence $y = (y_1, y_2, \\dots, y_T)$ and a token sequence $x = (x_1, x_2, \\dots, x_T)$ in a linear-chain CRF is defined as the sum of emission and transition log-potentials:\n$$ S(y) = A_{\\mathrm{START}\\rightarrow y_1} + \\sum_{t=1}^{T} E_{t}(y_t) + \\sum_{t=1}^{T-1} A_{y_t \\rightarrow y_{t+1}} + A_{y_T \\rightarrow \\mathrm{STOP}} $$\nwhere $E_{t}(y_t)$ is the emission log-potential for the token $x_t$ being assigned tag $y_t$, and $A_{y_{t-1} \\rightarrow y_t}$ is the transition log-potential from tag $y_{t-1}$ to $y_t$. The special tags $\\mathrm{START}$ and $\\mathrm{STOP}$ mark the beginning and end of the sequence.\n\nTo find the optimal tag sequence $y^*$ that maximizes this score, we use the Viterbi algorithm, a dynamic programming approach. Let $\\delta_t(k)$ be the maximum log-score of any tag sequence ending at token $t$ with tag $k$, where $k \\in \\{\\mathrm{O}, \\mathrm{B}, \\mathrm{I}\\}$. Let $\\psi_t(k)$ be the backpointer, storing the tag at step $t-1$ that led to this maximum score.\n\nThe recurrence relations are:\n1.  **Initialization ($t=1$):**\n    $$ \\delta_1(k) = A_{\\mathrm{START}\\rightarrow k} + E_1(k) $$\n2.  **Recursion ($t=2, \\dots, T$):**\n    $$ \\delta_t(k) = \\max_{j \\in \\{\\mathrm{O}, \\mathrm{B}, \\mathrm{I}\\}} \\left( \\delta_{t-1}(j) + A_{j \\rightarrow k} \\right) + E_t(k) $$\n    $$ \\psi_t(k) = \\arg\\max_{j \\in \\{\\mathrm{O}, \\mathrm{B}, \\mathrm{I}\\}} \\left( \\delta_{t-1}(j) + A_{j \\rightarrow k} \\right) $$\n3.  **Termination:** The total log-score of the best path is:\n    $$ \\text{score}^* = \\max_{k \\in \\{\\mathrm{O}, \\mathrm{B}, \\mathrm{I}\\}} \\left( \\delta_T(k) + A_{k \\rightarrow \\mathrm{STOP}} \\right) $$\n    The final tag of the best sequence is:\n    $$ y_T^* = \\arg\\max_{k \\in \\{\\mathrm{O}, \\mathrm{B}, \\mathrm{I}\\}} \\left( \\delta_T(k) + A_{k \\rightarrow \\mathrm{STOP}} \\right) $$\n4.  **Path backtracking:** For $t=T-1, \\dots, 1$:\n    $$ y_t^* = \\psi_{t+1}(y_{t+1}^*) $$\n\nWe now apply this procedure to the given problem, with $T=4$.\n\n**Step $t=1$: (\"severe\")**\n$$ \\delta_1(\\mathrm{O}) = A_{\\mathrm{START}\\rightarrow\\mathrm{O}} + E_1(\\mathrm{O}) = 0.0 + 0.4 = 0.4 $$\n$$ \\delta_1(\\mathrm{B}) = A_{\\mathrm{START}\\rightarrow\\mathrm{B}} + E_1(\\mathrm{B}) = -0.5 + (-0.3) = -0.8 $$\n$$ \\delta_1(\\mathrm{I}) = A_{\\mathrm{START}\\rightarrow\\mathrm{I}} + E_1(\\mathrm{I}) = -\\infty + (-1.0) = -\\infty $$\nScores at $t=1$: $\\delta_1 = \\{\\mathrm{O}: 0.4, \\mathrm{B}: -0.8, \\mathrm{I}: -\\infty\\}$.\n\n**Step $t=2$: (\"chest\")**\nFor each current tag $k \\in \\{\\mathrm{O}, \\mathrm{B}, \\mathrm{I}\\}$, we compute $\\max_{j} (\\delta_{1}(j) + A_{j \\rightarrow k})$.\n\n$k=\\mathrm{O}$: $\\max(\\delta_1(\\mathrm{O})+A_{\\mathrm{O}\\rightarrow\\mathrm{O}}, \\delta_1(\\mathrm{B})+A_{\\mathrm{B}\\rightarrow\\mathrm{O}}, \\delta_1(\\mathrm{I})+A_{\\mathrm{I}\\rightarrow\\mathrm{O}}) = \\max(0.4+0.1, -0.8-0.2, -\\infty-0.2) = \\max(0.5, -1.0, -\\infty) = 0.5$.\n$\\delta_2(\\mathrm{O}) = 0.5 + E_2(\\mathrm{O}) = 0.5 - 0.2 = 0.3$.\n$\\psi_2(\\mathrm{O}) = \\mathrm{O}$.\n\n$k=\\mathrm{B}$: $\\max(\\delta_1(\\mathrm{O})+A_{\\mathrm{O}\\rightarrow\\mathrm{B}}, \\delta_1(\\mathrm{B})+A_{\\mathrm{B}\\rightarrow\\mathrm{B}}, \\delta_1(\\mathrm{I})+A_{\\mathrm{I}\\rightarrow\\mathrm{B}}) = \\max(0.4+0.0, -0.8-1.0, -\\infty-1.5) = \\max(0.4, -1.8, -\\infty) = 0.4$.\n$\\delta_2(\\mathrm{B}) = 0.4 + E_2(\\mathrm{B}) = 0.4 + 1.2 = 1.6$.\n$\\psi_2(\\mathrm{B}) = \\mathrm{O}$.\n\n$k=\\mathrm{I}$: $\\max(\\delta_1(\\mathrm{O})+A_{\\mathrm{O}\\rightarrow\\mathrm{I}}, \\delta_1(\\mathrm{B})+A_{\\mathrm{B}\\rightarrow\\mathrm{I}}, \\delta_1(\\mathrm{I})+A_{\\mathrm{I}\\rightarrow\\mathrm{I}}) = \\max(0.4-\\infty, -0.8+0.6, -\\infty+0.5) = \\max(-\\infty, -0.2, -\\infty) = -0.2$.\n$\\delta_2(\\mathrm{I}) = -0.2 + E_2(\\mathrm{I}) = -0.2 + 0.5 = 0.3$.\n$\\psi_2(\\mathrm{I}) = \\mathrm{B}$.\nScores at $t=2$: $\\delta_2 = \\{\\mathrm{O}: 0.3, \\mathrm{B}: 1.6, \\mathrm{I}: 0.3\\}$. Backpointers $\\psi_2 = \\{\\mathrm{O}: \\mathrm{O}, \\mathrm{B}: \\mathrm{O}, \\mathrm{I}: \\mathrm{B}\\}$.\n\n**Step $t=3$: (\"pain\")**\n$k=\\mathrm{O}$: $\\max(\\delta_2(\\mathrm{O})+A_{\\mathrm{O}\\rightarrow\\mathrm{O}}, \\delta_2(\\mathrm{B})+A_{\\mathrm{B}\\rightarrow\\mathrm{O}}, \\delta_2(\\mathrm{I})+A_{\\mathrm{I}\\rightarrow\\mathrm{O}}) = \\max(0.3+0.1, 1.6-0.2, 0.3-0.2) = \\max(0.4, 1.4, 0.1) = 1.4$.\n$\\delta_3(\\mathrm{O}) = 1.4 + E_3(\\mathrm{O}) = 1.4 - 0.5 = 0.9$.\n$\\psi_3(\\mathrm{O}) = \\mathrm{B}$.\n\n$k=\\mathrm{B}$: $\\max(\\delta_2(\\mathrm{O})+A_{\\mathrm{O}\\rightarrow\\mathrm{B}}, \\delta_2(\\mathrm{B})+A_{\\mathrm{B}\\rightarrow\\mathrm{B}}, \\delta_2(\\mathrm{I})+A_{\\mathrm{I}\\rightarrow\\mathrm{B}}) = \\max(0.3+0.0, 1.6-1.0, 0.3-1.5) = \\max(0.3, 0.6, -1.2) = 0.6$.\n$\\delta_3(\\mathrm{B}) = 0.6 + E_3(\\mathrm{B}) = 0.6 + 0.8 = 1.4$.\n$\\psi_3(\\mathrm{B}) = \\mathrm{B}$.\n\n$k=\\mathrm{I}$: $\\max(\\delta_2(\\mathrm{O})+A_{\\mathrm{O}\\rightarrow\\mathrm{I}}, \\delta_2(\\mathrm{B})+A_{\\mathrm{B}\\rightarrow\\mathrm{I}}, \\delta_2(\\mathrm{I})+A_{\\mathrm{I}\\rightarrow\\mathrm{I}}) = \\max(0.3-\\infty, 1.6+0.6, 0.3+0.5) = \\max(-\\infty, 2.2, 0.8) = 2.2$.\n$\\delta_3(\\mathrm{I}) = 2.2 + E_3(\\mathrm{I}) = 2.2 + 1.5 = 3.7$.\n$\\psi_3(\\mathrm{I}) = \\mathrm{B}$.\nScores at $t=3$: $\\delta_3 = \\{\\mathrm{O}: 0.9, \\mathrm{B}: 1.4, \\mathrm{I}: 3.7\\}$. Backpointers $\\psi_3 = \\{\\mathrm{O}: \\mathrm{B}, \\mathrm{B}: \\mathrm{B}, \\mathrm{I}: \\mathrm{B}\\}$.\n\n**Step $t=4$: (\"today\")**\n$k=\\mathrm{O}$: $\\max(\\delta_3(\\mathrm{O})+A_{\\mathrm{O}\\rightarrow\\mathrm{O}}, \\delta_3(\\mathrm{B})+A_{\\mathrm{B}\\rightarrow\\mathrm{O}}, \\delta_3(\\mathrm{I})+A_{\\mathrm{I}\\rightarrow\\mathrm{O}}) = \\max(0.9+0.1, 1.4-0.2, 3.7-0.2) = \\max(1.0, 1.2, 3.5) = 3.5$.\n$\\delta_4(\\mathrm{O}) = 3.5 + E_4(\\mathrm{O}) = 3.5 + 0.6 = 4.1$.\n$\\psi_4(\\mathrm{O}) = \\mathrm{I}$.\n\n$k=\\mathrm{B}$: $\\max(\\delta_3(\\mathrm{O})+A_{\\mathrm{O}\\rightarrow\\mathrm{B}}, \\delta_3(\\mathrm{B})+A_{\\mathrm{B}\\rightarrow\\mathrm{B}}, \\delta_3(\\mathrm{I})+A_{\\mathrm{I}\\rightarrow\\mathrm{B}}) = \\max(0.9+0.0, 1.4-1.0, 3.7-1.5) = \\max(0.9, 0.4, 2.2) = 2.2$.\n$\\delta_4(\\mathrm{B}) = 2.2 + E_4(\\mathrm{B}) = 2.2 - 0.6 = 1.6$.\n$\\psi_4(\\mathrm{B}) = \\mathrm{I}$.\n\n$k=\\mathrm{I}$: $\\max(\\delta_3(\\mathrm{O})+A_{\\mathrm{O}\\rightarrow\\mathrm{I}}, \\delta_3(\\mathrm{B})+A_{\\mathrm{B}\\rightarrow\\mathrm{I}}, \\delta_3(\\mathrm{I})+A_{\\mathrm{I}\\rightarrow\\mathrm{I}}) = \\max(0.9-\\infty, 1.4+0.6, 3.7+0.5) = \\max(-\\infty, 2.0, 4.2) = 4.2$.\n$\\delta_4(\\mathrm{I}) = 4.2 + E_4(\\mathrm{I}) = 4.2 - 0.8 = 3.4$.\n$\\psi_4(\\mathrm{I}) = \\mathrm{I}$.\nScores at $t=4$: $\\delta_4 = \\{\\mathrm{O}: 4.1, \\mathrm{B}: 1.6, \\mathrm{I}: 3.4\\}$. Backpointers $\\psi_4 = \\{\\mathrm{O}: \\mathrm{I}, \\mathrm{B}: \\mathrm{I}, \\mathrm{I}: \\mathrm{I}\\}$.\n\n**Termination**\nWe find the final best score by considering the transition to the $\\mathrm{STOP}$ tag.\nFinal scores:\n- Path ending in $\\mathrm{O}$: $\\delta_4(\\mathrm{O}) + A_{\\mathrm{O}\\rightarrow\\mathrm{STOP}} = 4.1 + 0.0 = 4.1$\n- Path ending in $\\mathrm{B}$: $\\delta_4(\\mathrm{B}) + A_{\\mathrm{B}\\rightarrow\\mathrm{STOP}} = 1.6 + (-0.3) = 1.3$\n- Path ending in $\\mathrm{I}$: $\\delta_4(\\mathrm{I}) + A_{\\mathrm{I}\\rightarrow\\mathrm{STOP}} = 3.4 + (-0.3) = 3.1$\n\nThe maximum score is $\\max(4.1, 1.3, 3.1) = 4.1$.\nThe best tag at step $t=4$ is therefore $y_4^* = \\mathrm{O}$.\n\n**Backtracking**\nWe recover the best path by following the backpointers from $y_4^*$.\n- $y_4^* = \\mathrm{O}$\n- $y_3^* = \\psi_4(y_4^*) = \\psi_4(\\mathrm{O}) = \\mathrm{I}$\n- $y_2^* = \\psi_3(y_3^*) = \\psi_3(\\mathrm{I}) = \\mathrm{B}$\n- $y_1^* = \\psi_2(y_2^*) = \\psi_2(\\mathrm{B}) = \\mathrm{O}$\n\nThe best tag sequence is $(\\mathrm{O}, \\mathrm{B}, \\mathrm{I}, \\mathrm{O})$. This corresponds to labeling \"severe\" as Outside, \"chest\" as Begin-symptom, \"pain\" as Inside-symptom, and \"today\" as Outside.\n\nThe total best-path log-score is the maximum score found in the termination step, which is $4.1$. We can verify this by summing the component log-potentials for the path $(\\mathrm{O}, \\mathrm{B}, \\mathrm{I}, \\mathrm{O})$:\n$ S(\\mathrm{O},\\mathrm{B},\\mathrm{I},\\mathrm{O}) = A_{\\mathrm{START}\\rightarrow \\mathrm{O}} + E_1(\\mathrm{O}) + A_{\\mathrm{O}\\rightarrow \\mathrm{B}} + E_2(\\mathrm{B}) + A_{\\mathrm{B}\\rightarrow \\mathrm{I}} + E_3(\\mathrm{I}) + A_{\\mathrm{I}\\rightarrow \\mathrm{O}} + E_4(\\mathrm{O}) + A_{\\mathrm{O}\\rightarrow \\mathrm{STOP}} $\n$ S(\\mathrm{O},\\mathrm{B},\\mathrm{I},\\mathrm{O}) = 0.0 + 0.4 + 0.0 + 1.2 + 0.6 + 1.5 + (-0.2) + 0.6 + 0.0 = 4.1 $\nThis confirms the result of the Viterbi calculation.\n\nThe final answer is the total best-path log-score.",
            "answer": "$$\n\\boxed{4.1}\n$$"
        },
        {
            "introduction": "Beyond identifying entities, a key task in clinical NLP is to understand the relationships between them, such as whether a drug caused an adverse event. Evaluating the performance of a relation extraction classifier requires specific metrics and a deep understanding of their clinical implications. This practice  moves from model mechanics to impact assessment, asking you to calculate standard metrics like precision, recall, and the $F_1$ score from a confusion matrix, and to interpret the critical differences between false positives and false negatives in the high-stakes context of pharmacovigilance.",
            "id": "4588718",
            "problem": "A clinical natural language processing system processes de-identified Electronic Health Records (EHR) to identify whether a documented drug mention is causally linked to an adverse event mention within the same clinical note. A binary relation classifier is trained on domain-expert annotated drug–event pairs. Each pair is labeled as either the positive class $C$ (a causal drug–event relation holds) or the negative class $\\neg C$ (no causal relation). On a held-out evaluation set of $1{,}500$ candidate pairs, the following confusion matrix counts are obtained for the positive class $C$: true positives $TP = 208$, false positives $FP = 104$, false negatives $FN = 52$, and true negatives $TN = 1{,}136$. Using foundational definitions of evaluation for binary classifiers in information retrieval and machine learning derived from set membership of predicted and true labels, compute the positive-class precision, the positive-class recall, and the $F_1$ score. Report the three metrics in the order: precision, recall, $F_1$. Round each metric to four significant figures. Additionally, concisely interpret, grounded in clinical pharmacovigilance reasoning, the distinct implications of false positives versus false negatives in this setting, focusing on potential impacts on downstream safety signal detection and clinical decision support (do not provide numerical values in the interpretation).",
            "solution": "The problem statement provides a self-contained and internally consistent set of data for evaluating a binary classifier in a well-defined context of clinical natural language processing. The given values for true positives ($TP$), false positives ($FP$), false negatives ($FN$), and true negatives ($TN$) sum to the total sample size: $208 + 104 + 52 + 1,136 = 1,500$. The task is grounded in established principles of machine learning evaluation and pharmacovigilance. Therefore, the problem is deemed valid and a solution can be formulated.\n\nThe first part of the task is to compute the precision, recall, and $F_1$ score for the positive class ($C$), which represents the presence of a causal drug-event relation. These metrics are defined as follows:\n\nPrecision ($P$) is the proportion of predicted positive instances that are actually positive. It measures the exactness of the classifier.\n$$P = \\frac{TP}{TP + FP}$$\n\nRecall ($R$), also known as sensitivity or true positive rate, is the proportion of actual positive instances that are correctly identified by the classifier. It measures the completeness of the classifier.\n$$R = \\frac{TP}{TP + FN}$$\n\nThe $F_1$ score is the harmonic mean of precision and recall, providing a single metric that balances both.\n$$F_1 = 2 \\cdot \\frac{P \\cdot R}{P + R} = \\frac{2 \\cdot TP}{2 \\cdot TP + FP + FN}$$\n\nUsing the provided values: $TP = 208$, $FP = 104$, and $FN = 52$.\n\nFirst, we compute the precision:\n$$P = \\frac{208}{208 + 104} = \\frac{208}{312} = \\frac{2}{3} \\approx 0.666666...$$\nRounding to four significant figures, we get $P \\approx 0.6667$.\n\nNext, we compute the recall:\n$$R = \\frac{208}{208 + 52} = \\frac{208}{260} = \\frac{4}{5} = 0.8$$\nTo express this with four significant figures, we write $R = 0.8000$.\n\nFinally, we compute the $F_1$ score using the exact fractional values of $P$ and $R$ to avoid premature rounding errors:\n$$F_1 = 2 \\cdot \\frac{\\left(\\frac{2}{3}\\right) \\cdot \\left(\\frac{4}{5}\\right)}{\\left(\\frac{2}{3}\\right) + \\left(\\frac{4}{5}\\right)} = 2 \\cdot \\frac{\\frac{8}{15}}{\\frac{10 + 12}{15}} = 2 \\cdot \\frac{\\frac{8}{15}}{\\frac{22}{15}} = 2 \\cdot \\frac{8}{22} = \\frac{16}{22} = \\frac{8}{11} \\approx 0.727272...$$\nRounding to four significant figures, we get $F_1 \\approx 0.7273$.\n\nThe second part of the task requires a concise interpretation of the implications of false positives versus false negatives in the context of clinical pharmacovigilance.\n\nA **false positive** ($FP$) occurs when the system incorrectly identifies a causal relationship between a drug and an adverse event. In the context of safety signal detection, this leads to the generation of false alarms. The primary consequence is a burden on human experts, who must expend valuable time and resources investigating these spurious signals, diverting them from legitimate safety issues. In a clinical decision support setting, a high rate of false positives can lead to \"alert fatigue,\" where clinicians become desensitized and begin to ignore all system warnings, including valid ones, thereby undermining the system's effectiveness and potentially increasing patient risk.\n\nA **false negative** ($FN$) occurs when the system fails to identify a true causal relationship between a drug and an adverse event. This represents a missed safety signal. The implications are severe and directly impact patient safety. A failure to detect a genuine adverse drug reaction means that patients may continue to be harmed by a medication, and the opportunity for early regulatory intervention (e.g., label updates, safety warnings) is lost. The delay in recognizing a true safety signal can lead to widespread, preventable harm across a patient population, representing a critical failure of the pharmacovigilance process. In this specific application, a false negative is of graver concern than a false positive due to the direct potential for patient harm.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.6667 & 0.8000 & 0.7273\n\\end{pmatrix}\n}\n$$"
        }
    ]
}