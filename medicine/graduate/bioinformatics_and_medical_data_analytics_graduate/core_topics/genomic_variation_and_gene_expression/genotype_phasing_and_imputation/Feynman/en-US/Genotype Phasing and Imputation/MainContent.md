## Introduction
The human genome, the blueprint of our biology, is inherited in two distinct copies—one from each parent. While modern technology allows us to easily read the [genetic variants](@entry_id:906564), or alleles, an individual possesses at millions of sites, this genotype information presents a fundamental puzzle: it doesn't tell us which alleles are grouped together on the same parental chromosome. This arrangement of alleles along a single chromosome is known as a haplotype. Disentangling the two parental [haplotypes](@entry_id:177949) from combined genotype data is the challenge of phasing, a problem of staggering complexity given the millions of ambiguous sites in a typical genome.

This article illuminates the statistical and computational methods developed to solve this puzzle and the related task of [genotype imputation](@entry_id:163993)—the inference of [genetic variants](@entry_id:906564) that were never directly measured. We will explore how patterns of [genetic inheritance](@entry_id:262521) shared across populations provide the [statistical power](@entry_id:197129) to reconstruct our individual genetic story with remarkable accuracy.

Across the following chapters, you will gain a comprehensive understanding of this cornerstone of modern genomics. In **Principles and Mechanisms**, we will delve into the core concepts of Linkage Disequilibrium and the elegant framework of Hidden Markov Models that form the algorithmic heart of phasing and [imputation](@entry_id:270805). Next, in **Applications and Interdisciplinary Connections**, we will journey through the transformative impact of these methods on [human genetics](@entry_id:261875), from boosting the power of disease association studies to enabling [personalized medicine](@entry_id:152668) and reconstructing ancient genomes. Finally, **Hands-On Practices** will provide an opportunity to solidify these concepts through targeted exercises. By navigating these topics, you will learn to see the genome not just as a string of letters, but as a rich, structured text revealing deep histories of ancestry, function, and disease.

## Principles and Mechanisms

Imagine you have two slightly different copies of a very long book—say, the complete works of Shakespeare. They are almost identical, but every few pages, there's a single word that differs. Your task is to reconstruct the original text of each of the two master copies. The problem is, all the pages have been torn out and shuffled. When you pick up a page and see a variant word, you know one copy has that word, but you don't know which one. And you don't know which variant words on other pages belong to that same copy. This, in essence, is the challenge of [genotype phasing](@entry_id:902971).

### The Problem of Phase: Two Books, One Story

In our own biological "books," our genomes, we inherit one complete copy of our chromosomes from our mother and another from our father. These two copies, or **homologs**, are lined up side-by-side. A **genotype** is what we typically measure at a specific location, or locus. For a common type of [genetic variant](@entry_id:906911) called a Single Nucleotide Polymorphism (SNP), where, say, the nucleotide could be an $A$ or a $G$, a person could have two $A$'s (genotype $AA$), two $G$'s (genotype $GG$), or one of each (genotype $AG$). The genotype tells us the set of alleles we possess, but it doesn't tell us which parent each [allele](@entry_id:906209) came from.

A **haplotype**, on the other hand, is the actual sequence of alleles as they are written along one single chromosome, one of our "books." The task of **phasing** is to determine which alleles travel together on the same chromosome. For a person with genotype $AA$ or $GG$—a **homozygote**—there is no ambiguity. Both chromosomes carry the same [allele](@entry_id:906209). But for a person with genotype $AG$—a **heterozygote**—we face a puzzle. Is the $A$ on the paternal chromosome and the $G$ on the maternal one? Or is it the other way around?

At a single heterozygous site, this seems like a simple coin flip. But the problem compounds exponentially. If an individual is heterozygous at $k$ different sites, there are $2^k$ possible [ordered pairs](@entry_id:269702) of [haplotypes](@entry_id:177949) that could explain their genotypes. Because swapping the "paternal" and "maternal" labels for the two complete haplotypes doesn't change the biological reality, there are $2^{k-1}$ fundamentally distinct phase configurations for $k \ge 1$ . Given that a typical human genome has millions of [heterozygous](@entry_id:276964) sites, the number of possibilities is astronomical. Brute force is not an option. We need to look for clues.

### The Population's Memory: Linkage Disequilibrium

The first, and perhaps most powerful, clue comes not from the individual alone, but from the shared history of the entire human population. Alleles at nearby loci on a chromosome are not shuffled independently during the creation of sperm and egg cells. They tend to be inherited together in blocks. This non-random association of alleles is called **Linkage Disequilibrium (LD)**. It is a form of genetic memory.

Let's make this concrete. Consider two nearby SNPs. At the first, the alleles are $A$ and $a$; at the second, $B$ and $b$. If these alleles were sorted independently, the frequency of the haplotype $AB$ in the population, $P_{AB}$, would simply be the product of the frequencies of [allele](@entry_id:906209) $A$ ($p_A$) and [allele](@entry_id:906209) $B$ ($p_B$). Linkage disequilibrium is the deviation from this expectation. We quantify it with a value $D = P_{AB} - p_A p_B$. If $D$ is not zero, the alleles are correlated. A more useful, normalized measure is the squared [correlation coefficient](@entry_id:147037), $r^2$, defined as:

$$
r^2 = \frac{D^2}{p_A(1-p_A)p_B(1-p_B)}
$$

This $r^2$ value ranges from $0$ (perfect independence) to $1$ (perfect correlation, where knowing the [allele](@entry_id:906209) at one locus tells you the [allele](@entry_id:906209) at the other).

How does this help us phase? Imagine an individual is a double heterozygote: their genotype is $Aa$ at the first locus and $Bb$ at the second. There are two possible phases: either their two [haplotypes](@entry_id:177949) are ($AB$, $ab$) or they are ($Ab$, $aB$). If we know the haplotype frequencies in the population, we can calculate which phase is more likely. In a population where the $AB$ and $ab$ haplotypes are much more common than $Ab$ and $aB$ (a situation of positive $D$), it is far more probable that our individual inherited the common haplotypes rather than the rare ones . The magnitude of $r^2$ tells us the strength of this statistical evidence. LD is the statistical wind that blows our inference in the right direction.

### Weaving a Haplotype: The Hidden Markov Model Mosaic

To harness the power of LD systematically across the entire genome, we need a robust mathematical framework. The key insight is to think of an individual's chromosome not as a unique, monolithic entity, but as a **mosaic** of short haplotype segments that are also present in the wider population. To do this, we use a large **reference panel** of pre-phased haplotypes from thousands of individuals.

The tool we use to build this mosaic is the **Hidden Markov Model (HMM)**, a beautiful piece of statistical machinery perfectly suited for this task. It works like this:

-   **Hidden States**: The HMM moves along the target individual's chromosome, one marker at a time. At each marker, the "hidden state" is the answer to the question: *which [haplotype](@entry_id:268358) from our reference panel is this part of the chromosome most likely a copy of?* If our panel has $K$ haplotypes, there are $K$ possible hidden states .

-   **Transitions**: As we move from one marker to the next, we might continue copying from the same reference haplotype, or we might "switch" to a new one. This switch in the model represents an ancestral **recombination** event, where the genetic shuffling of meiosis broke apart an ancient [haplotype block](@entry_id:270142). The probability of a switch depends on the **genetic distance** between the markers.
    -   A **genetic map** tells us the expected rate of recombination between any two points on a chromosome, measured in units called Morgans . Assuming recombination events occur randomly (the Haldane model), the probability of a switch, $r$, over a genetic distance $d$ is given by the famous mapping function $r = \frac{1}{2}(1 - \exp(-2d))$.
    -   The HMM elegantly captures this. The probability of staying on the same reference [haplotype](@entry_id:268358) decays exponentially with the genetic distance, while the probability of switching increases . This allows the model to favor copying long, unbroken segments from the reference panel, mirroring the biological reality of [haplotype blocks](@entry_id:166800).

-   **Emissions**: The HMM's states are hidden, but we have observed data: the individual's genotypes. The "emission probability" connects the hidden state to the observation. It answers: *given that we are copying from reference [haplotype](@entry_id:268358) $k$, what is the probability of observing the alleles we see in our individual?* Usually, this is a perfect match. But the model allows for a small mismatch probability, $\varepsilon$, to account for new mutations or the occasional genotyping error .

By running an efficient algorithm (like the Viterbi or Forward-Backward algorithm) on this HMM, we can infer the most likely sequence of hidden states—that is, the most probable mosaic of reference [haplotypes](@entry_id:177949) that explains our individual's genotype data. Once we have this mosaic, we have our phase.

### Looking Back in Time: The Coalescent View

One might ask: why is this mosaic model a valid way to think about genetics? The answer is profoundly beautiful and takes us backward in time. **Coalescent theory** is the study of how genetic lineages merge, or coalesce, as we trace them back to a common ancestor. Your genome and the genomes in the reference panel are all distant cousins, connected through a vast, branching family tree shaped by recombination, called the **Ancestral Recombination Graph (ARG)**.

The Li-Stephens HMM, the model we just described, is a brilliant and computationally tractable approximation of this immensely complex ARG . The process of "copying" from the reference panel is a proxy for tracing your ancestral lineage backward until it coalesces with one of the lineages in the panel. A "switch" in the HMM corresponds to a recombination event in your ancestry that caused your lineage to jump from descending from one ancestor to another. The model's parameters, like the recombination rate and mismatch probability, are not arbitrary; they are deeply connected to fundamental population genetic parameters like the [effective population size](@entry_id:146802) ($N_e$), [mutation rate](@entry_id:136737) ($\mu$), and [recombination rate](@entry_id:203271) ($r$) . This reveals a stunning unity, linking a practical algorithm to the deep-time processes that shape our genomes.

### From Phasing to Imputation: Filling in the Blanks

The HMM framework is so powerful that it allows us to do more than just determine the phase of measured genotypes. It allows us to perform **[genotype imputation](@entry_id:163993)**: the [statistical inference](@entry_id:172747) of genotypes at millions of sites that were *not* measured by our genotyping chip.

Once the HMM has determined the most likely mosaic of reference [haplotypes](@entry_id:177949) that explains an individual's observed genotypes, we have a template for the entire chromosome. To impute a missing genotype, we simply look at the corresponding position on the inferred reference haplotype segments and "read off" the alleles . This process allows us to take a sparse, cheap genotype dataset and make it dense and powerful, dramatically increasing the number of variants we can test for association with a disease in a [genome-wide association study](@entry_id:176222) (GWAS).

There are two main strategies for this: a fully probabilistic **joint imputation** that considers all possible phases, and a faster two-step approach called **pre-phasing**, which first determines the most likely phase and then imputes based on that single estimate. While computationally efficient, pre-phasing can sometimes be overconfident because it ignores the underlying uncertainty in the initial phasing step .

### The Other Clue: Listening to the Reads

So far, we have discussed **population-based phasing**, which relies on the statistical patterns of LD in a reference panel. But there is another, more direct source of information: the sequencing data from the individual themselves. This is the basis of **[read-backed phasing](@entry_id:897015)** .

Modern sequencing technologies work by reading short fragments of DNA. If a single DNA fragment—and the sequencing "read" that comes from it—is long enough to span two or more heterozygous sites, it physically links those alleles. The read tells us, with high certainty, that those specific alleles were on the same physical molecule of DNA. This provides a direct, local phase constraint.

Different sequencing technologies provide different types of read-based evidence:
-   **Paired-end short reads**: These provide two reads from opposite ends of a short DNA fragment (typically a few hundred base pairs). If the fragment spans two heterozygous sites, their phase is resolved. This is excellent for very local phasing.
-   **Long reads**: Technologies like PacBio and Oxford Nanopore produce reads thousands of base pairs long. These are a game-changer for [read-backed phasing](@entry_id:897015), as a single read can span dozens of heterozygous sites, creating long, contiguous phased blocks.
-   **Linked reads and Chromosome Conformation Capture (Hi-C)**: These are clever methods that provide sparse but very long-range information, linking alleles that can be megabases apart on the same chromosome . For instance, Hi-C captures segments of DNA that are physically close in the 3D space of the nucleus, which are overwhelmingly likely to be from the same chromosome.
-   **Strand-seq**: This ingenious single-cell technique separates reads originating from the two different strands of the DNA double helix (the Watson and Crick strands) after cell division, effectively partitioning all reads from a cell into two chromosome-wide sets, one for each homolog .

Read-backed phasing and population-based phasing have complementary strengths and weaknesses. Read-backed phasing is deterministic and highly accurate locally but struggles to connect distant phase blocks without long-range data. Population-based phasing excels at creating chromosome-length [haplotypes](@entry_id:177949) but can be less accurate for [rare variants](@entry_id:925903) not well-represented in the reference panel. The most powerful modern phasing methods are hybrids, using read-based evidence to build confident local blocks and population-based information to stitch them together into a complete picture.

### Putting It All Together: Ancestry and Accuracy

Two final points are crucial for any real-world application. First, the power of population-based methods depends entirely on the reference panel. LD patterns are not universal; they are a product of a population's specific demographic history—its size, bottlenecks, and migrations. A population that has been small for a long time will have more extensive LD than a large, diverse population . Therefore, for accurate phasing and imputation, it is critical that the **ancestry of the reference panel matches the ancestry of the target individual**. Using a mismatched panel is like trying to solve a jigsaw puzzle with pieces from a different box; it simply won't fit together correctly.

Second, how do we measure success? For phasing, the primary metric is the **switch error rate**. This is the rate at which our inferred [haplotype](@entry_id:268358) incorrectly jumps from, say, the true paternal chromosome to the true maternal one . We can estimate this with high confidence if we have genotype data from a family trio (father, mother, and child), as Mendelian inheritance rules allow us to trace the origin of each [allele](@entry_id:906209). For imputation, the gold-standard metric is the **imputation $R^2$**. This value can be interpreted as the squared correlation between the true, unknown genotypes and our imputed dosage values. An $R^2$ of 1.0 means perfect imputation, while an $R^2$ of 0.0 means our [imputation](@entry_id:270805) is no better than simply guessing based on the population's average [allele frequency](@entry_id:146872) .

From a simple question about ambiguous genotypes, we have journeyed through population genetics, [statistical modeling](@entry_id:272466), and cutting-edge sequencing technology. The principles of phasing and imputation reveal a deep and beautiful interplay between the history of our species, encoded in linkage disequilibrium, and the molecular reality of our own two chromosomes, read out by modern sequencers. By understanding these mechanisms, we can reconstruct the two "books" of our genome with remarkable fidelity, unlocking a wealth of information for understanding human health and disease.