{
    "hands_on_practices": [
        {
            "introduction": "Understanding genotype phasing begins with mastering the foundational concepts of haplotypes and the nature of phasing ambiguity. This exercise challenges you to define the possible phased haplotypes for a diploid individual heterozygous at two loci. By doing so, you will solidify your understanding of how unphased genotype data relates to the underlying haplotype pairs and establish a clear definition of a \"switch error,\" which is the most fundamental unit of error in phasing. ",
            "id": "4569491",
            "problem": "A diploid organism carries two homologous chromosomes. At a biallelic locus, a genotype records the unordered multiset of alleles present across the two homologs; a genotype is called heterozygous if the two alleles differ. A haplotype is the ordered sequence of alleles along a single homologous chromosome, and phasing assigns the observed alleles at each locus to one of the two haplotypes. Consider an individual with observed genotypes $A/G$ at locus $1$ and $T/C$ at locus $2$. Using only the core definitions of diploidy, heterozygosity, haplotypes, and phase consistency across loci, determine which option correctly enumerates all possible phased haplotype pairs consistent with the observed genotypes and also correctly defines what constitutes a switch error specifically for the segment spanning loci $1$ and $2$.\n  \nA. The only possible phased haplotype pairs consistent with the observed genotypes are $\\{(A,T),(G,C)\\}$ and $\\{(A,C),(G,T)\\}$. A switch error between loci $1$ and $2$ means that, relative to the true phasing, the inferred assignment of alleles at locus $2$ to chromosomes is reversed with respect to the assignment at locus $1$, producing the complementary phased pair; equivalently, the relative phase between the two loci is inverted.\n\nB. The possible phased haplotypes include $\\{(A,T),(G,C)\\}$, $\\{(A,C),(G,T)\\}$, and also $\\{(A,G),(T,C)\\}$ because each locus can be arbitrarily paired across chromosomes. A switch error is any case where the inferred alleles do not match the true alleles at either locus, regardless of heterozygosity.\n\nC. The possible haplotypes are exactly $\\{(A,T),(G,C)\\}$ and $\\{(A,C),(G,T)\\}$, but a switch error is defined as misassigning the alleles at locus $1$ while leaving locus $2$ correct; misassignments at homozygous sites also constitute switch errors.\n\nD. The possible haplotypes are $\\{(A,T),(G,C)\\}$ and $\\{(A,C),(G,T)\\}$, but a switch error occurs only when meiotic recombination between loci $1$ and $2$ changes the true haplotypes; therefore, inferring $\\{(A,C),(G,T)\\}$ when the truth is $\\{(A,T),(G,C)\\}$ is not a switch error.",
            "solution": "An individual with genotypes A/G at locus 1 and T/C at locus 2 can only have two possible pairs of haplotypes: either (AT, GC) or (AC, GT). This is because each haplotype must contain one allele from each locus. This eliminates option B. A switch error is defined as an incorrect inference of the relative phase between two heterozygous loci. If the true phase is (AT, GC), inferring (AC, GT) constitutes a switch error because the assignment of alleles at locus 2 is flipped relative to locus 1. This is an inversion of relative phase. Option A correctly states both the possible haplotypes and this definition. Option C's definition of a switch error is too narrow and incorrectly includes homozygous sites. Option D incorrectly confuses an inference error (a switch error) with a biological event (recombination).",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "After defining a switch error, the next step is to quantify its frequency and understand its consequences. This practice asks you to calculate the switch error rate from empirical data and, more importantly, to reason about its impact on downstream analyses. This exercise is crucial for developing an intuition for how phasing quality control metrics translate directly into the reliability of haplotype-based inference, such as genotype imputation. ",
            "id": "4569545",
            "problem": "In genotype phasing and imputation, a common quality metric is the rate at which inferred haplotypes incorrectly switch their parental assignment between consecutive heterozygous loci. Consider a phased chromosome segment evaluated against an external truth set derived from parent-offspring trios, where the segment contains $T$ adjacent heterozygous loci pairs that define potential phase switch positions (heterozygous transitions). Suppose curation identifies $E$ erroneous phase switches across this segment. Using foundational definitions that treat each heterozygous transition as a Bernoulli trial for whether a phase switch is correct or erroneous, and that define the switch error rate as the expected fraction of transitions that are erroneous, compute the switch error rate for $T = 1{,}000$ transitions and $E = 25$ erroneous switches. State your final answer as a decimal, rounded to four significant figures. Then, based on first principles of haplotype-based inference and the behavior of Hidden Markov Model (HMM)-based phasing in regions of varying linkage disequilibrium (LD) and Minor Allele Frequency (MAF), justify how a switch error rate at this level would be expected to impact downstream haplotype-based imputation accuracy and related analyses. Do not use a percentage sign in your numerical answer; express the rate as a unitless decimal.",
            "solution": "The problem asks for the calculation of the switch error rate from given data and a subsequent justification of its impact on downstream bioinformatic analyses. The validation process confirms that the problem is scientifically grounded, well-posed, and contains all necessary information for a complete solution.\n\nFirst, let us formalize the calculation of the switch error rate ($SER$). The problem states that each of the $T$ adjacent heterozygous loci pairs, or heterozygous transitions, can be treated as a Bernoulli trial. In this context, a trial represents the opportunity for a phase switch error to occur. The outcome of each trial is binary: either the phase is inferred correctly or it is erroneous. The problem provides the total number of such transitions, $T$, and the total number of observed erroneous phase switches, $E$.\n\nThe given values are:\n- The total number of heterozygous transitions, $T = 1,000$.\n- The number of erroneous phase switches, $E = 25$.\n\nThe problem defines the switch error rate as the expected fraction of transitions that are erroneous. For a series of Bernoulli trials, the best estimate for the probability of the event of interest (in this case, an error) is the relative frequency of its occurrence. Therefore, the switch error rate, $SER$, is computed as the ratio of the number of observed errors to the total number of opportunities for an error:\n$$\nSER = \\frac{E}{T}\n$$\nSubstituting the given numerical values into this equation yields:\n$$\nSER = \\frac{25}{1,000} = 0.025\n$$\nThe problem requires the final answer to be stated as a decimal rounded to four significant figures. The number $0.025$ has two significant figures ($2$ and $5$). To express this with four significant figures, we append two zeros, resulting in $0.02500$.\n\nNext, we must justify the impact of a switch error rate at this level on downstream haplotype-based analyses, such as imputation, based on first principles. An $SER$ of $0.025$ signifies that, on average, $2.5\\%$ of the phase connections between adjacent heterozygous sites are incorrect. This means one out of every $40$ such transitions is erroneous.\n\nThe fundamental principle of haplotype-based imputation is to infer ungenotyped variants in a study sample by leveraging a densely genotyped reference panel of known haplotypes. Algorithms, typically based on Hidden Markov Models (HMMs), work by identifying segments of reference haplotypes that match the sparse, phased haplotype framework of a study individual. The algorithm then \"copies\" the alleles from the best-matching reference haplotype(s) to fill in the missing genotypes.\n\nA phase switch error fundamentally corrupts this process. A switch error occurs when the algorithm incorrectly swaps the paternally-derived and maternally-derived chromosomal segments. For example, if the true paternal and maternal haplotypes at two consecutive heterozygous sites are $H_{pat} = \\dots A \\dots T \\dots$ and $H_{mat} = \\dots G \\dots C \\dots$, a switch error between these sites would result in an inferred haplotype of, for instance, $\\dots A \\dots C \\dots$. This creates a *chimeric* or *mosaic* haplotype that does not exist in the individual and is highly unlikely to exist in the reference panel, as it is an artificial construct of two different ancestral backgrounds.\n\nThe impact of this chimeric haplotype on imputation accuracy is severe, particularly in the region immediately following the switch error. When an HMM-based imputation algorithm encounters a switch error, the currently tracked reference haplotype suddenly ceases to match the individual's inferred haplotype. The model is forced to transition to a different reference haplotype to explain the observed data post-switch. This abrupt and artificial transition introduces a high probability of error. Essentially, the switch error breaks the integrity of the haplotype block, causing the imputation algorithm to lose its place and make mistakes as it tries to recover a match. This results in a localized drop in imputation quality (measured by metrics like $r^2$) around the site of the switch error.\n\nThe magnitude of this impact is modulated by local genomic context, specifically linkage disequilibrium ($LD$) and minor allele frequency ($MAF$).\n- In regions of high $LD$, where alleles over long distances are strongly correlated, haplotypes are long and less diverse. A single switch error is highly disruptive because it breaks a long, conserved haplotype block, creating a sequence with very low probability under the population model. This makes the imputation of any variants within that block, especially rare ones that depend on this long-range context, highly prone to error.\n- In regions of low $LD$ (e.g., recombination hotspots), haplotypes are shorter and more shuffled. Phasing algorithms naturally struggle more in these regions, often leading to a higher density of switch errors. While a single error might be seen as less damaging in a region that is already fragmented, the cumulative effect of multiple nearby errors can degrade imputation quality across the entire low-$LD$ segment.\n- The imputation of low-$MAF$ (rare) variants is disproportionately affected by phasing errors. Rare variants often reside on a single, long ancestral haplotype. The integrity of this haplotype is paramount for their accurate imputation. A switch error rate of $0.025$ is high enough to frequently disrupt these rare variant-carrying haplotypes, significantly reducing the power and accuracy of rare-variant association studies that rely on imputed data.\n\nIn summary, a switch error rate of $0.025$ introduces a significant burden of chimeric haplotypes into the data. This directly degrades the performance of downstream imputation by causing local mismatches against reference panels, forcing erroneous state transitions in the underlying HMMs, and ultimately leading to a measurable decrease in imputation accuracy, with a particularly severe impact on the inference of rare genetic variants.",
            "answer": "$$\n\\boxed{0.02500}\n$$"
        },
        {
            "introduction": "Genotype imputation algorithms typically yield a posterior probability distribution for each genotype rather than a single, definite call. This exercise demonstrates how to distill this probabilistic information into a single, practical value known as imputed dosage, which represents the expected count of a specific allele. Furthermore, it prompts you to think about model validation by considering how you would assess the calibration of these posterior probabilities, a critical step in evaluating the trustworthiness of any imputation model. ",
            "id": "4569492",
            "problem": "Consider a single bi-allelic single-nucleotide polymorphism (SNP) with a designated minor allele. Let the unobserved genotype be denoted by the random variable $G \\in \\{0,1,2\\}$, representing the count of minor alleles. An imputation algorithm based on a Hidden Markov Model (HMM) produces a posterior distribution over $G$ given observed data, encoded as $P(G=0 \\mid \\text{data}) = 0.1$, $P(G=1 \\mid \\text{data}) = 0.3$, and $P(G=2 \\mid \\text{data}) = 0.6$. Using only the core definition of conditional expectation from probability theory and the interpretation of imputed dosage as the expected minor-allele count under the posterior, compute the imputed dosage $\\hat{d}$ for this SNP.\n\nThen, using only first principles of probability calibration, briefly explain in words how you would assess whether such posterior genotype probabilities are calibrated against true genotypes across a large cohort with available ground-truth genotypes, without computing any numerical metric in this problem.\n\nProvide the imputed dosage as an exact value (no rounding). No units are required for the dosage.",
            "solution": "The problem is evaluated as valid. It is scientifically grounded in the principles of statistical genetics and probability theory, well-posed with all necessary information provided, and free from ambiguity or contradiction.\n\nThe problem consists of two parts. The first part requires the computation of the imputed dosage for a single-nucleotide polymorphism (SNP), and the second part asks for a conceptual explanation of how to assess the calibration of genotype probabilities.\n\n**Part 1: Computation of Imputed Dosage**\n\nThe problem defines the unobserved genotype as a discrete random variable $G$, which represents the count of the minor allele at a bi-allelic SNP. The possible values for $G$ are $g \\in \\{0, 1, 2\\}$, corresponding to zero, one, or two copies of the minor allele.\n\nAn imputation algorithm has produced a posterior probability distribution for $G$ conditional on observed data. These probabilities are given as:\n$P(G=0 \\mid \\text{data}) = 0.1$\n$P(G=1 \\mid \\text{data}) = 0.3$\n$P(G=2 \\mid \\text{data}) = 0.6$\n\nThe problem states that the imputed dosage, denoted $\\hat{d}$, is to be interpreted as the expected minor-allele count under this posterior distribution. This is equivalent to calculating the conditional expectation of the random variable $G$ given the data, which is written as $E[G \\mid \\text{data}]$.\n\nAccording to the core definition of expectation for a discrete random variable, the expected value is the sum of each possible value multiplied by its corresponding probability. Applying this definition to the conditional context, we have:\n$$ \\hat{d} = E[G \\mid \\text{data}] = \\sum_{g \\in \\{0,1,2\\}} g \\cdot P(G=g \\mid \\text{data}) $$\n\nWe can now substitute the given values into this formula:\n$$ \\hat{d} = (0 \\cdot P(G=0 \\mid \\text{data})) + (1 \\cdot P(G=1 \\mid \\text{data})) + (2 \\cdot P(G=2 \\mid \\text{data})) $$\n$$ \\hat{d} = (0 \\cdot 0.1) + (1 \\cdot 0.3) + (2 \\cdot 0.6) $$\n$$ \\hat{d} = 0 + 0.3 + 1.2 $$\n$$ \\hat{d} = 1.5 $$\n\nThe imputed dosage for this SNP is therefore $1.5$. This value represents the average number of minor alleles we expect this individual to have, given the observed data and the model.\n\n**Part 2: Assessing Probability Calibration**\n\nThe second part of the problem asks for an explanation of how to assess whether the posterior genotype probabilities are calibrated, using first principles and a large cohort with known true genotypes.\n\nProbability calibration, in its most fundamental sense, means that if a model predicts an outcome with probability $p$, that outcome should be observed with a frequency of approximately $p$ over many instances where that prediction was made.\n\nTo assess the calibration of the posterior genotype probabilities (e.g., $P(G=g \\mid \\text{data})$ for $g \\in \\{0, 1, 2\\}$) against ground-truth genotypes, one would perform the following conceptual procedure:\n1.  Take a large cohort of individuals for whom both the imputed posterior probabilities from the model and the true, experimentally determined genotypes are available.\n2.  To check the calibration for a specific genotype, say $G=2$, group individuals into bins based on their predicted posterior probability, $P(G=2 \\mid \\text{data})$. For instance, one bin could contain all individuals for whom the model predicted $P(G=2 \\mid \\text{data})$ to be in the range $[0.55, 0.65]$.\n3.  Within each bin, calculate the an empirical frequency: count the number of individuals whose true genotype is actually $G=2$ and divide this count by the total number of individuals in that bin.\n4.  If the model is well-calibrated, this calculated empirical frequency should be close to the average predicted probability for that bin (e.g., approximately $0.6$ for the $[0.55, 0.65]$ bin).\n5.  This process would be repeated for multiple probability bins covering the range from $0$ to $1$, and for all possible genotype states ($G=0$, $G=1$, and $G=2$), to form a complete assessment of the model's calibration. A well-calibrated model will exhibit close agreement between its assigned probabilities and the observed frequencies across all genotypes and all probability levels.",
            "answer": "$$\\boxed{1.5}$$"
        }
    ]
}