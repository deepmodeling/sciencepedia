{
    "hands_on_practices": [
        {
            "introduction": "A critical skill in genome annotation is the ability to quantitatively evaluate the performance of an annotation tool. Simply creating predictions is not enough; we must rigorously compare them against a \"gold standard\" or ground truth to measure accuracy. This exercise introduces the fundamental metrics of precision and recall, and explores the important distinction between evaluating predictions at the nucleotide level (base-wise) and at the structural feature level (feature-wise), providing a more nuanced understanding of an annotation's quality .",
            "id": "4611315",
            "problem": "A structural genome annotation system is evaluated against a curated ground truth on a single contiguous genomic locus. In structural annotation, exons are represented as closed genomic intervals with integer coordinates, and evaluation can be done at the base level (nucleotide-by-nucleotide) or at the feature level (whole exon objects). Start from first principles: sets, counting measure on integer coordinates, and the definition of a confusion matrix. Using these, formally define base-wise precision, base-wise recall, the base-wise harmonic mean of precision and recall (F1 score), and feature-wise counterparts based on an explicit matching rule. Then compute these metrics for the provided data.\n\nFundamental base and definitions to use:\n- Consider a locus with integer coordinate set $\\Omega \\subset \\mathbb{Z}$ and the counting measure $\\mu$ that assigns to any finite subset $A \\subset \\Omega$ the value $\\mu(A)$ equal to the number of elements in $A$.\n- Let the ground truth exon set be a finite collection of closed intervals on $\\Omega$, $T = \\{[s_i, e_i]\\}_{i=1}^{n}$ with $s_i, e_i \\in \\mathbb{Z}$ and $s_i \\leq e_i$. The union of bases covered by truth is $U_T = \\bigcup_{i=1}^{n} [s_i, e_i]$.\n- Let the predicted exon set be $P = \\{[a_j, b_j]\\}_{j=1}^{m}$, with union of bases covered by predictions $U_P = \\bigcup_{j=1}^{m} [a_j, b_j]$.\n- Base-wise confusion counts are defined from sets: true positive bases $TP_{\\text{b}} = \\mu(U_P \\cap U_T)$, false positive bases $FP_{\\text{b}} = \\mu(U_P \\setminus U_T)$, and false negative bases $FN_{\\text{b}} = \\mu(U_T \\setminus U_P)$.\n- Base-wise precision is $P_{\\text{b}} = \\frac{TP_{\\text{b}}}{TP_{\\text{b}} + FP_{\\text{b}}}$, base-wise recall is $R_{\\text{b}} = \\frac{TP_{\\text{b}}}{TP_{\\text{b}} + FN_{\\text{b}}}$, and base-wise F1 is $F1_{\\text{b}} = \\frac{2 P_{\\text{b}} R_{\\text{b}}}{P_{\\text{b}} + R_{\\text{b}}}$.\n- Feature-wise evaluation treats exons as atomic features. Define Intersection over Union (IoU), also known as Jaccard index, between a predicted interval $[a,b]$ and a truth interval $[s,e]$ by $\\mathrm{IoU}([a,b],[s,e]) = \\frac{\\mu([a,b] \\cap [s,e])}{\\mu([a,b] \\cup [s,e])}$. Use a one-to-one matching protocol: construct matches greedily in descending order of IoU, allowing at most one match per predicted exon and at most one match per truth exon, and declare a match valid if and only if its IoU is at least the threshold $\\tau = 0.5$. Under this protocol, the feature-level true positives $TP_{\\text{f}}$ are the number of valid matches, the feature-level false positives $FP_{\\text{f}}$ are the number of predicted exons left unmatched, and the feature-level false negatives $FN_{\\text{f}}$ are the number of truth exons left unmatched.\n- Feature-wise precision is $P_{\\text{f}} = \\frac{TP_{\\text{f}}}{TP_{\\text{f}} + FP_{\\text{f}}}$, feature-wise recall is $R_{\\text{f}} = \\frac{TP_{\\text{f}}}{TP_{\\text{f}} + FN_{\\text{f}}}$, and feature-wise F1 is $F1_{\\text{f}} = \\frac{2 P_{\\text{f}} R_{\\text{f}}}{P_{\\text{f}} + R_{\\text{f}}}$.\n\nData:\n- Truth exons $T$: $[1000,1199]$, $[1500,1699]$, $[2000,2199]$.\n- Predicted exons $P$: $[995,1205]$, $[1600,1799]$, $[1900,2099]$, $[3000,3099]$.\n- All lengths are computed inclusively: the length of an interval $[x,y]$ is $y - x + 1$.\n\nTasks:\n1. Using the above fundamental definitions, compute $TP_{\\text{b}}$, $FP_{\\text{b}}$, and $FN_{\\text{b}}$, then compute $P_{\\text{b}}$, $R_{\\text{b}}$, and $F1_{\\text{b}}$.\n2. Using the IoU-based one-to-one matching with threshold $\\tau = 0.5$, compute $TP_{\\text{f}}$, $FP_{\\text{f}}$, and $FN_{\\text{f}}$, then compute $P_{\\text{f}}$, $R_{\\text{f}}$, and $F1_{\\text{f}}$.\n\nRound each reported metric to four significant figures and express each as a decimal. Report your six metrics in the order: base-wise precision, base-wise recall, base-wise F1, feature-wise precision, feature-wise recall, feature-wise F1.",
            "solution": "The problem statement has been validated and is deemed sound. It is scientifically grounded in standard bioinformatics evaluation methodologies, is well-posed with a complete and consistent set of definitions and data, and is expressed in objective, formal language. A unique solution can be derived from the provided information. We will now proceed with the solution, which is divided into two parts as per the tasks.\n\nPart 1: Base-wise evaluation\n\nThe ground truth annotation consists of a set of $n=3$ exons, $T = \\{T_1, T_2, T_3\\}$, where $T_1 = [1000, 1199]$, $T_2 = [1500, 1699]$, and $T_3 = [2000, 2199]$. The predicted annotation consists of a set of $m=4$ exons, $P = \\{P_1, P_2, P_3, P_4\\}$, where $P_1 = [995, 1205]$, $P_2 = [1600, 1799]$, $P_3 = [1900, 2099]$, and $P_4 = [3000, 3099]$. The measure of a set of integer coordinates is its cardinality, and for an interval $[x,y]$, this is $\\mu([x,y]) = y-x+1$.\n\nFirst, we calculate the total number of bases in the ground truth, $\\mu(U_T)$, and in the prediction, $\\mu(U_P)$. The intervals in $T$ are disjoint, as are the intervals in $P$.\nThe lengths of the truth exons are:\n$\\mu(T_1) = 1199 - 1000 + 1 = 200$\n$\\mu(T_2) = 1699 - 1500 + 1 = 200$\n$\\mu(T_3) = 2199 - 2000 + 1 = 200$\nThe union of truth bases is $U_T = T_1 \\cup T_2 \\cup T_3$, and its measure is $\\mu(U_T) = \\mu(T_1) + \\mu(T_2) + \\mu(T_3) = 200 + 200 + 200 = 600$. This corresponds to the total number of positive instances in the ground truth ($TP_{\\text{b}} + FN_{\\text{b}}$).\n\nThe lengths of the predicted exons are:\n$\\mu(P_1) = 1205 - 995 + 1 = 211$\n$\\mu(P_2) = 1799 - 1600 + 1 = 200$\n$\\mu(P_3) = 2099 - 1900 + 1 = 200$\n$\\mu(P_4) = 3099 - 3000 + 1 = 100$\nThe union of predicted bases is $U_P = P_1 \\cup P_2 \\cup P_3 \\cup P_4$, and its measure is $\\mu(U_P) = \\mu(P_1) + \\mu(P_2) + \\mu(P_3) + \\mu(P_4) = 211 + 200 + 200 + 100 = 711$. This corresponds to the total number of predicted positive instances ($TP_{\\text{b}} + FP_{\\text{b}}$).\n\nNext, we identify the set of true positive bases, which is the intersection of the two union sets, $U_P \\cap U_T$. We find the intersections of the constituent intervals:\n$P_1 \\cap T_1 = [995, 1205] \\cap [1000, 1199] = [1000, 1199]$. Its measure is $\\mu([1000, 1199]) = 200$.\n$P_2 \\cap T_2 = [1600, 1799] \\cap [1500, 1699] = [1600, 1699]$. Its measure is $\\mu([1600, 1699]) = 100$.\n$P_3 \\cap T_3 = [1900, 2099] \\cap [2000, 2199] = [2000, 2099]$. Its measure is $\\mu([2000, 2099]) = 100$.\nAll other pairwise intersections of intervals from $P$ and $T$ are empty sets.\nThe measure of the true positive bases is the sum of the measures of these disjoint intersections:\n$TP_{\\text{b}} = \\mu(U_P \\cap U_T) = 200 + 100 + 100 = 400$.\n\nNow we can compute the number of false positive and false negative bases:\n$FP_{\\text{b}} = \\mu(U_P \\setminus U_T) = \\mu(U_P) - \\mu(U_P \\cap U_T) = 711 - 400 = 311$.\n$FN_{\\text{b}} = \\mu(U_T \\setminus U_P) = \\mu(U_T) - \\mu(U_P \\cap U_T) = 600 - 400 = 200$.\n\nUsing these counts, we compute the base-wise precision ($P_{\\text{b}}$), recall ($R_{\\text{b}}$), and F1 score ($F1_{\\text{b}}$):\n$P_{\\text{b}} = \\frac{TP_{\\text{b}}}{TP_{\\text{b}} + FP_{\\text{b}}} = \\frac{400}{400 + 311} = \\frac{400}{711} \\approx 0.5625879...$\n$R_{\\text{b}} = \\frac{TP_{\\text{b}}}{TP_{\\text{b}} + FN_{\\text{b}}} = \\frac{400}{400 + 200} = \\frac{400}{600} = \\frac{2}{3} \\approx 0.666666...$\n$F1_{\\text{b}} = \\frac{2 P_{\\text{b}} R_{\\text{b}}}{P_{\\text{b}} + R_{\\text{b}}} = \\frac{2 TP_{\\text{b}}}{2 TP_{\\text{b}} + FP_{\\text{b}} + FN_{\\text{b}}} = \\frac{2 \\cdot 400}{2 \\cdot 400 + 311 + 200} = \\frac{800}{1311} \\approx 0.6102212...$\n\nRounding to four significant figures, the base-wise metrics are: $P_{\\text{b}} = 0.5626$, $R_{\\text{b}} = 0.6667$, and $F1_{\\text{b}} = 0.6102$.\n\nPart 2: Feature-wise evaluation\n\nFor feature-wise evaluation, we pair predicted exons with truth exons based on Intersection over Union (IoU), with a threshold of $\\tau = 0.5$. The matching is one-to-one and greedy, proceeding in descending order of IoU.\n$\\mathrm{IoU}([a,b],[s,e]) = \\frac{\\mu([a,b] \\cap [s,e])}{\\mu([a,b] \\cup [s,e])} = \\frac{\\mu([a,b] \\cap [s,e])}{\\mu([a,b]) + \\mu([s,e]) - \\mu([a,b] \\cap [s,e])}$.\n\nWe calculate the IoU for all pairs $(P_j, T_i)$ that have a non-empty intersection:\n1.  For $(P_1, T_1)$: $\\mu(P_1 \\cap T_1) = 200$. $\\mu(P_1) = 211$, $\\mu(T_1) = 200$.\n    $\\mathrm{IoU}(P_1, T_1) = \\frac{200}{211 + 200 - 200} = \\frac{200}{211} \\approx 0.947867$.\n2.  For $(P_2, T_2)$: $\\mu(P_2 \\cap T_2) = 100$. $\\mu(P_2) = 200$, $\\mu(T_2) = 200$.\n    $\\mathrm{IoU}(P_2, T_2) = \\frac{100}{200 + 200 - 100} = \\frac{100}{300} = \\frac{1}{3} \\approx 0.333333$.\n3.  For $(P_3, T_3)$: $\\mu(P_3 \\cap T_3) = 100$. $\\mu(P_3) = 200$, $\\mu(T_3) = 200$.\n    $\\mathrm{IoU}(P_3, T_3) = \\frac{100}{200 + 200 - 100} = \\frac{100}{300} = \\frac{1}{3} \\approx 0.333333$.\n\nAll other pairs have an intersection of $\\emptyset$ and thus an IoU of $0$.\nThe list of non-zero IoU pairs in descending order is: $(\\mathrm{IoU}(P_1, T_1) \\approx 0.9479)$, $(\\mathrm{IoU}(P_2, T_2) \\approx 0.3333)$, $(\\mathrm{IoU}(P_3, T_3) \\approx 0.3333)$.\n\nApplying the greedy matching protocol:\n1.  The highest IoU is $\\mathrm{IoU}(P_1, T_1) \\approx 0.9479$, which is greater than the threshold $\\tau=0.5$. We accept this as a valid match. $P_1$ and $T_1$ are now considered matched.\n2.  The next highest IoU is $\\mathrm{IoU}(P_2, T_2) \\approx 0.3333$, which is less than $\\tau=0.5$. This is not a valid match.\n3.  The next highest IoU is $\\mathrm{IoU}(P_3, T_3) \\approx 0.3333$, which is also less than $\\tau=0.5$. This is not a valid match.\n\nTherefore, there is only one valid match: $(P_1, T_1)$. We now compute the feature-wise confusion counts:\n$TP_{\\text{f}}$ = number of valid matches = $1$.\n$FP_{\\text{f}}$ = number of unmatched predicted exons = $|\\{P_2, P_3, P_4\\}| = 3$.\n$FN_{\\text{f}}$ = number of unmatched truth exons = $|\\{T_2, T_3\\}| = 2$.\nThe total number of predictions is $m = TP_{\\text{f}} + FP_{\\text{f}} = 1 + 3 = 4$. The total number of truth features is $n = TP_{\\text{f}} + FN_{\\text{f}} = 1 + 2 = 3$. These counts are consistent.\n\nUsing these counts, we compute the feature-wise precision ($P_{\\text{f}}$), recall ($R_{\\text{f}}$), and F1 score ($F1_{\\text{f}}$):\n$P_{\\text{f}} = \\frac{TP_{\\text{f}}}{TP_{\\text{f}} + FP_{\\text{f}}} = \\frac{1}{1 + 3} = \\frac{1}{4} = 0.25$.\n$R_{\\text{f}} = \\frac{TP_{\\text{f}}}{TP_{\\text{f}} + FN_{\\text{f}}} = \\frac{1}{1 + 2} = \\frac{1}{3} \\approx 0.333333...$\n$F1_{\\text{f}} = \\frac{2 P_{\\text{f}} R_{\\text{f}}}{P_{\\text{f}} + R_{\\text{f}}} = \\frac{2 \\cdot (1/4) \\cdot (1/3)}{(1/4) + (1/3)} = \\frac{2/12}{7/12} = \\frac{2}{7} \\approx 0.285714...$\n\nRounding to four significant figures, the feature-wise metrics are: $P_{\\text{f}} = 0.2500$, $R_{\\text{f}} = 0.3333$, and $F1_{\\text{f}} = 0.2857$.\n\nFinal Summary of Metrics:\nBase-wise Precision ($P_{\\text{b}}$): $0.5626$\nBase-wise Recall ($R_{\\text{b}}$): $0.6667$\nBase-wise F1 ($F1_{\\text{b}}$): $0.6102$\nFeature-wise Precision ($P_{\\text{f}}$): $0.2500$\nFeature-wise Recall ($R_{\\text{f}}$): $0.3333$\nFeature-wise F1 ($F1_{\\text{f}}$): $0.2857$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.5626 & 0.6667 & 0.6102 & 0.2500 & 0.3333 & 0.2857\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Beyond annotating individual genes, a common task is to classify entire genomic contigs, such as distinguishing a host chromosome from mobile elements like plasmids or integrated prophages. These different genomic elements often have distinct statistical signatures in their sequence composition, copy number, and protein-coding content. This problem guides you through the construction of a naive Bayes classifier, a practical and widely used machine learning approach, to integrate these multiple lines of evidence and build a complete pipeline for classifying genomic sequences .",
            "id": "4611300",
            "problem": "You are tasked with formalizing and implementing a principled pipeline for structural and functional genome annotation to classify contigs from a bacterial genome assembly into one of three classes: chromosome, plasmid, or prophage. Your pipeline must be derived from foundational principles in genomics and statistics and must rely only on the following experimentally justified facts, formulas, and assumptions:\n\n- Deoxyribonucleic Acid (DNA) sequencing depth-of-coverage per locus is approximately proportional to the copy number of the underlying molecule in the sample; under stable library preparation, the per-contig mean coverage can be approximated by a Gaussian (Normal) distribution around a class-dependent mean with class-dependent standard deviation due to variation in mapping and local biases.\n- The Guanine-Cytosine (GC) fraction across a sufficiently long contig can be modeled, via the Central Limit Theorem (CLT), as approximately Gaussian around a class-dependent mean GC fraction, with class-dependent standard deviation reflecting compositional heterogeneity.\n- The number of hits to phage hallmark proteins per contig can be modeled as a Poisson process with a class-dependent rate per kilobase (kilobase abbreviated as kb), scaled by contig length in kilobases.\n\nConstruct a Bayesian classifier under the conditional independence assumption (naive Bayes): given a contig with features, treat coverage, GC fraction, and phage protein hits as conditionally independent given the class. For each class, compute the posterior score proportional to the prior probability multiplied by the likelihoods of the observed features under the class. Use log-posterior scores to avoid numerical underflow, and predict the class with the maximal log-posterior score.\n\nYour classifier must use the following class-conditional models and priors, which are fixed and known a priori from prior calibration on the host:\n\n- Classes: chromosome, plasmid, prophage. Encode these as integers: chromosome $\\to$ $0$, plasmid $\\to$ $1$, prophage $\\to$ $2$.\n- Priors: $P(\\text{chromosome}) = 0.75$, $P(\\text{plasmid}) = 0.20$, $P(\\text{prophage}) = 0.05$.\n- Coverage depth model (mean coverage across the contig, in $x$-fold; unitless): \n  - Chromosome: $\\mathcal{N}(\\mu = 60, \\sigma = 6)$.\n  - Plasmid: $\\mathcal{N}(\\mu = 180, \\sigma = 18)$.\n  - Prophage: $\\mathcal{N}(\\mu = 60, \\sigma = 6)$.\n- GC fraction model (fraction in $[0,1]$, expressed as a decimal):\n  - Chromosome: $\\mathcal{N}(\\mu = 0.51, \\sigma = 0.015)$.\n  - Plasmid: $\\mathcal{N}(\\mu = 0.44, \\sigma = 0.03)$.\n  - Prophage: $\\mathcal{N}(\\mu = 0.47, \\sigma = 0.02)$.\n- Phage protein hits model (count of hits, dimensionless), modeled as Poisson with mean equal to the class-specific rate per kilobase multiplied by the contig length in kilobases:\n  - Chromosome: rate $\\lambda = 0.01$ per kb.\n  - Plasmid: rate $\\lambda = 0.04$ per kb.\n  - Prophage: rate $\\lambda = 0.60$ per kb.\n\nMathematical definitions to be used:\n- If $X$ is Gaussian with mean $\\mu$ and standard deviation $\\sigma$, then the probability density function is \n  $$f_{X}(x \\mid \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi}\\,\\sigma} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right).$$\n- If $K$ is Poisson with mean $\\lambda$, then the probability mass function is \n  $$\\Pr(K = k \\mid \\lambda) = \\frac{\\lambda^{k} e^{-\\lambda}}{k!}.$$\n- Under the naive Bayes assumption, with prior $P(C)$ and independent features $F_{1},F_{2},F_{3}$ given class $C$, the posterior is proportional to \n  $$P(C \\mid F_{1},F_{2},F_{3}) \\propto P(C)\\, p(F_{1} \\mid C)\\, p(F_{2} \\mid C)\\, p(F_{3} \\mid C).$$\n- For numerical stability, use log-scores:\n  $$\\log \\text{score}(C) = \\log P(C) + \\log p(\\text{coverage} \\mid C) + \\log p(\\text{GC} \\mid C) + \\log p(\\text{hits} \\mid C),$$\n  where $\\log p(\\text{coverage} \\mid C)$ and $\\log p(\\text{GC} \\mid C)$ are Gaussian log-densities, and $\\log p(\\text{hits} \\mid C)$ is the Poisson log-mass. The class prediction is $\\arg \\max_{C} \\log \\text{score}(C)$.\n\nImplement a program that, for each contig in the test suite below, computes the predicted class code as an integer using the above model and prints all predictions as a single list on one line as specified under Final Output Format. No user input is required.\n\nUnits and constraints for the input features:\n- Contig length must be in kilobases (kb).\n- Mean coverage depth is in $x$-fold (unitless).\n- GC fraction must be a decimal in $[0,1]$.\n- Phage hits is a nonnegative integer.\n\nTest suite of contigs (each as a tuple $(L_{\\text{kb}}, \\text{coverage}, \\text{GC}, \\text{hits})$):\n- Case $1$: $(200, 62, 0.51, 1)$.\n- Case $2$: $(50, 170, 0.43, 3)$.\n- Case $3$: $(40, 58, 0.46, 28)$.\n- Case $4$: $(30, 65, 0.45, 1)$.\n- Case $5$: $(20, 185, 0.46, 10)$.\n- Case $6$: $(5, 135, 0.45, 0)$.\n\nDesign for coverage:\n- Include a standard “happy path” chromosome-like case (Case $1$).\n- Include a “happy path” plasmid-like case (Case $2$).\n- Include a “happy path” prophage-like case (Case $3$).\n- Include edge conditions: GC shift without coverage shift (Case $4$), conflict between high coverage and many phage hits (Case $5$), and a very short contig with zero phage hits (Case $6$).\n\nFinal Output Format:\n- Your program should produce a single line of output containing the results as a comma-separated list of integers enclosed in square brackets, for example, $[0,1,2]$.\n- Use the class encoding: chromosome $= 0$, plasmid $= 1$, prophage $= 2$.",
            "solution": "The user has provided a bioinformatics problem that requires the construction and implementation of a naive Bayes classifier for genomic contigs. I will first validate the problem statement according to the specified criteria and then, upon validation, provide a complete solution.\n\n### Step 1: Extract Givens\n\n- **Task**: Classify genomic contigs into one of three classes: chromosome, plasmid, or prophage.\n- **Classifier Model**: Naive Bayes, assuming conditional independence of features given the class.\n- **Classes and Encodings**: chromosome $\\to 0$, plasmid $\\to 1$, prophage $\\to 2$.\n- **Prior Probabilities**:\n    - $P(\\text{chromosome}) = 0.75$\n    - $P(\\text{plasmid}) = 0.20$\n    - $P(\\text{prophage}) = 0.05$\n- **Feature 1: Mean Coverage Depth ($x_{\\text{cov}}$)**: Modeled by a Gaussian distribution, $\\mathcal{N}(\\mu, \\sigma)$.\n    - Chromosome ($C=0$): $\\mathcal{N}(\\mu = 60, \\sigma = 6)$.\n    - Plasmid ($C=1$): $\\mathcal{N}(\\mu = 180, \\sigma = 18)$.\n    - Prophage ($C=2$): $\\mathcal{N}(\\mu = 60, \\sigma = 6)$.\n- **Feature 2: GC Fraction ($x_{\\text{GC}}$)**: Modeled by a Gaussian distribution, $\\mathcal{N}(\\mu, \\sigma)$.\n    - Chromosome ($C=0$): $\\mathcal{N}(\\mu = 0.51, \\sigma = 0.015)$.\n    - Plasmid ($C=1$): $\\mathcal{N}(\\mu = 0.44, \\sigma = 0.03)$.\n    - Prophage ($C=2$): $\\mathcal{N}(\\mu = 0.47, \\sigma = 0.02)$.\n- **Feature 3: Phage Protein Hits ($k_{\\text{hits}}$)**: Modeled by a Poisson distribution with mean $\\lambda_C \\cdot L_{\\text{kb}}$, where $L_{\\text{kb}}$ is contig length in kilobases.\n    - Chromosome ($C=0$): rate $\\lambda = 0.01$ per $kb$.\n    - Plasmid ($C=1$): rate $\\lambda = 0.04$ per $kb$.\n    - Prophage ($C=2$): rate $\\lambda = 0.60$ per $kb$.\n- **Mathematical Formulae**:\n    - Gaussian PDF: $f_{X}(x \\mid \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi}\\,\\sigma} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)$.\n    - Poisson PMF: $\\Pr(K = k \\mid \\lambda) = \\frac{\\lambda^{k} e^{-\\lambda}}{k!}$.\n    - Log-score for classification: $\\log \\text{score}(C) = \\log P(C) + \\log p(\\text{coverage} \\mid C) + \\log p(\\text{GC} \\mid C) + \\log p(\\text{hits} \\mid C)$.\n    - Prediction rule: $\\arg \\max_{C} \\log \\text{score}(C)$.\n- **Test Suite**: A set of $6$ contigs, each defined by a tuple $(L_{\\text{kb}}, \\text{coverage}, \\text{GC}, \\text{hits})$.\n    - Case $1$: ($200$, $62$, $0.51$, $1$)\n    - Case $2$: ($50$, $170$, $0.43$, $3$)\n    - Case $3$: ($40$, $58$, $0.46$, $28$)\n    - Case $4$: ($30$, $65$, $0.45$, $1$)\n    - Case $5$: ($20$, $185$, $0.46$, $10$)\n    - Case $6$: ($5$, $135$, $0.45$, $0$)\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem is subjected to rigorous validation against the specified criteria.\n\n- **Scientifically Grounded (Critical)**: The problem is scientifically sound. The use of coverage depth, GC content, and phage-specific protein counts are standard, empirically justified features in microbial genomics for differentiating chromosomes, plasmids, and prophages. The choice of Gaussian and Poisson distributions for these features is also well-founded, justified by the Central Limit Theorem and the nature of count data, respectively.\n- **Well-Posed**: The problem is well-posed. It provides a complete set of prior probabilities and class-conditional probability distributions for all features. The objective is clearly defined as maximizing the log-posterior score, which guarantees the existence and uniqueness of a solution for each test case.\n- **Objective (Critical)**: The problem is stated in precise, quantitative, and unbiased language. All parameters are given as fixed numerical values, leaving no room for subjective interpretation.\n- **Flaw Checklist**:\n    1.  **Scientific/Factual Unsoundness**: None. The premises are consistent with established principles of genomics and statistics.\n    2.  **Non-Formalizable/Irrelevant**: None. The problem is a formal and canonical application of Bayesian classification within the specified field of structural and functional genome annotation.\n    3.  **Incomplete/Contradictory Setup**: None. All necessary information (priors, model parameters, test data) is provided, and there are no contradictions.\n    4.  **Unrealistic/Infeasible**: None. The parameter values and feature values in the test suite are within plausible biological ranges.\n    5.  **Ill-Posed/Poorly Structured**: None. The problem structure is a classic classification task with a unique solution.\n    6.  **Pseudo-Profound/Trivial**: None. The problem requires a correct application of statistical principles and is not trivial. The test cases include non-obvious scenarios that test the robustness of the model.\n    7.  **Outside Scientific Verifiability**: None. The method is standard and the results are deterministic and verifiable.\n\n### Step 3: Verdict and Action\n\nThe problem is **valid**. It is a well-defined, scientifically grounded, and formalizable task in computational biology. I will now proceed to construct the solution.\n\n### Principled Solution\n\nThe objective is to classify a genomic contig, described by its features $(L_{\\text{kb}}, x_{\\text{cov}}, x_{\\text{GC}}, k_{\\text{hits}})$, into one of three classes $C \\in \\{0, 1, 2\\}$, corresponding to chromosome, plasmid, and prophage, respectively. We employ a naive Bayes classifier, which computes the posterior probability of each class given the observed features and selects the class with the highest probability.\n\nAccording to Bayes' theorem, the posterior probability is given by:\n$$P(C \\mid \\text{features}) = \\frac{P(\\text{features} \\mid C) P(C)}{P(\\text{features})}$$\nSince $P(\\text{features})$ is a normalization constant that is identical for all classes, we can ignore it for the purpose of maximization. The task reduces to finding the class $C$ that maximizes the product of the likelihood and the prior: $P(\\text{features} \\mid C) P(C)$.\n\nThe \"naive\" assumption of conditional independence allows us to express the likelihood as the product of the individual feature likelihoods:\n$$P(\\text{features} \\mid C) = p(x_{\\text{cov}} \\mid C) \\cdot p(x_{\\text{GC}} \\mid C) \\cdot p(k_{\\text{hits}} \\mid C)$$\nTo prevent numerical underflow from multiplying many small probabilities, and for computational convenience, we work with the logarithm of the score. The decision rule is thus:\n$$\\hat{C} = \\arg\\max_{C} \\left( \\log P(C) + \\log p(x_{\\text{cov}} \\mid C) + \\log p(x_{\\text{GC}} \\mid C) + \\log p(k_{\\text{hits}} \\mid C) \\right)$$\nwhere $\\hat{C}$ is the predicted class.\n\nThe individual log-likelihood components are derived from the specified distributions:\n\n1.  **Gaussian Log-Probability Density Function**: For a feature $x$ governed by $\\mathcal{N}(\\mu, \\sigma)$, the log-PDF is:\n    $$\\log p(x \\mid \\mu, \\sigma) = \\log\\left(\\frac{1}{\\sqrt{2\\pi}\\,\\sigma} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)\\right) = -\\frac{1}{2}\\log(2\\pi) - \\log(\\sigma) - \\frac{(x - \\mu)^2}{2\\sigma^2}$$\n    This formula is used for both the log-likelihood of coverage, $\\log p(x_{\\text{cov}} \\mid C)$, and GC fraction, $\\log p(x_{\\text{GC}} \\mid C)$, using the class-specific parameters $(\\mu_C, \\sigma_C)$.\n\n2.  **Poisson Log-Probability Mass Function**: For a feature $k$ governed by a Poisson distribution with mean $\\lambda$, the log-PMF is:\n    $$\\log \\Pr(K=k \\mid \\lambda) = \\log\\left(\\frac{\\lambda^{k} e^{-\\lambda}}{k!}\\right) = k \\log(\\lambda) - \\lambda - \\log(k!)$$\n    For the phage hits feature, the mean $\\lambda$ is class- and length-dependent: $\\lambda_C = \\text{rate}_C \\times L_{\\text{kb}}$. The term $\\log(k!)$ can be computed efficiently using the log-gamma function, as $\\log(k!) = \\texttt{lgamma}(k+1)$.\n\nThe algorithm proceeds as follows for each contig in the test suite:\n1.  Initialize a score for each of the $3$ classes.\n2.  For each class $C \\in \\{0, 1, 2\\}$:\n    a. Start with the log-prior probability, $\\log P(C)$.\n    b. Calculate the log-likelihood of the observed mean coverage, $x_{\\text{cov}}$, using the Gaussian log-PDF with parameters for class $C$. Add this to the score.\n    c. Calculate the log-likelihood of the observed GC fraction, $x_{\\text{GC}}$, using the Gaussian log-PDF with parameters for class $C$. Add this to the score.\n    d. Calculate the Poisson mean $\\lambda_C = \\text{rate}_C \\times L_{\\text{kb}}$.\n    e. Calculate the log-likelihood of the observed phage protein hits, $k_{\\text{hits}}$, using the Poisson log-PMF with mean $\\lambda_C$. Add this to the score.\n3.  Compare the final log-posterior scores for all three classes. The class with the maximum score is the predicted class for the contig.\n\nThis procedure will be implemented and applied to all $6$ test cases to generate the final list of predictions. The use of log-scores and direct calculation from first principles ensures a robust and accurate implementation faithful to the problem definition.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef solve():\n    \"\"\"\n    Implements a naive Bayes classifier for genomic contigs based on provided\n    statistical models and parameters.\n    \"\"\"\n\n    # Define the class-conditional models and priors as per the problem statement.\n    # Classes: 0: chromosome, 1: plasmid, 2: prophage\n    PARAMS = {\n        0: {\n            \"name\": \"chromosome\",\n            \"prior\": 0.75,\n            \"coverage\": {\"mu\": 60, \"sigma\": 6},\n            \"gc\": {\"mu\": 0.51, \"sigma\": 0.015},\n            \"phage_rate\": 0.01\n        },\n        1: {\n            \"name\": \"plasmid\",\n            \"prior\": 0.20,\n            \"coverage\": {\"mu\": 180, \"sigma\": 18},\n            \"gc\": {\"mu\": 0.44, \"sigma\": 0.03},\n            \"phage_rate\": 0.04\n        },\n        2: {\n            \"name\": \"prophage\",\n            \"prior\": 0.05,\n            \"coverage\": {\"mu\": 60, \"sigma\": 6},\n            \"gc\": {\"mu\": 0.47, \"sigma\": 0.02},\n            \"phage_rate\": 0.60\n        }\n    }\n\n    # Pre-calculate log priors for efficiency\n    LOG_PRIORS = {\n        class_id: np.log(params[\"prior\"]) for class_id, params in PARAMS.items()\n    }\n\n    # Define the test suite of contigs.\n    # Each tuple is (L_kb, coverage, GC_fraction, phage_hits).\n    test_cases = [\n        (200, 62, 0.51, 1),    # Case 1\n        (50, 170, 0.43, 3),    # Case 2\n        (40, 58, 0.46, 28),    # Case 3\n        (30, 65, 0.45, 1),     # Case 4\n        (20, 185, 0.46, 10),   # Case 5\n        (5, 135, 0.45, 0)      # Case 6\n    ]\n\n    def log_gaussian_pdf(x, mu, sigma):\n        \"\"\"\n        Calculates the log of the Gaussian Probability Density Function.\n        log p(x|mu, sigma) = -0.5*log(2*pi) - log(sigma) - (x - mu)^2 / (2*sigma^2)\n        \"\"\"\n        # The constant -0.5 * np.log(2 * np.pi) can be omitted for maximization,\n        # but is included here for formal correctness.\n        log_prob = -0.5 * np.log(2 * np.pi) - np.log(sigma) - 0.5 * ((x - mu) / sigma)**2\n        return log_prob\n\n    def log_poisson_pmf(k, rate, length_kb):\n        \"\"\"\n        Calculates the log of the Poisson Probability Mass Function.\n        log P(K=k|lambda) = k*log(lambda) - lambda - log(k!)\n        \"\"\"\n        lambda_val = rate * length_kb\n        \n        # Handle edge case where lambda is 0.\n        # If lambda = 0, P(k=0) = 1 (log(P) = 0), and P(k>0) = -inf.\n        if lambda_val == 0:\n            return 0.0 if k == 0 else -np.inf\n\n        # The term -gammaln(k + 1) is -log(k!). It is constant for a given k\n        # across classes, but included for formal correctness.\n        log_prob = k * np.log(lambda_val) - lambda_val - gammaln(k + 1)\n        return log_prob\n\n    results = []\n    for case in test_cases:\n        L_kb, coverage, gc, hits = case\n        log_posterior_scores = {}\n\n        for class_id, params in PARAMS.items():\n            # 1. Start with log prior\n            log_score = LOG_PRIORS[class_id]\n\n            # 2. Add log-likelihood for coverage\n            cov_params = params[\"coverage\"]\n            log_score += log_gaussian_pdf(coverage, cov_params[\"mu\"], cov_params[\"sigma\"])\n\n            # 3. Add log-likelihood for GC fraction\n            gc_params = params[\"gc\"]\n            log_score += log_gaussian_pdf(gc, gc_params[\"mu\"], gc_params[\"sigma\"])\n\n            # 4. Add log-likelihood for phage protein hits\n            phage_rate = params[\"phage_rate\"]\n            log_score += log_poisson_pmf(hits, phage_rate, L_kb)\n\n            log_posterior_scores[class_id] = log_score\n\n        # Find the class with the maximum log-posterior score\n        predicted_class = max(log_posterior_scores, key=log_posterior_scores.get)\n        results.append(predicted_class)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Automated annotation pipelines frequently encounter conflicting evidence from different data sources, such as transcriptomic data suggesting one exon boundary and protein homology suggesting another. Making a principled choice requires a framework that can formally weigh the evidence and consider the downstream consequences of a potential error. This practice problem demonstrates how Bayesian decision theory can be used to integrate conflicting data and make an optimal decision that minimizes expected loss, a powerful technique for building more intelligent and robust annotation systems .",
            "id": "4611389",
            "problem": "A gene model under structural and functional genome annotation has a single disputed exon boundary with two competing hypotheses: the boundary suggested by Ribonucleic Acid sequencing (RNA-seq), denoted $H_{R}$, and the boundary suggested by protein homology alignment, denoted $H_{P}$. The annotation pipeline must choose one boundary to maximize expected accuracy under a specified loss function, given observed evidence $d$ from both RNA-seq splice junctions and protein homology alignment scores.\n\nUse the following foundational base:\n- The Central Dogma of Molecular Biology states that genes encode messenger ribonucleic acid (mRNA) which translates into proteins. Exon boundaries determine the coding frame; misannotation can cause frameshifts or functional domain truncations.\n- Bayesian decision theory: posterior probability is given by Bayes’ theorem, and the optimal action minimizes expected loss. Formally, the posterior odds satisfy $$\\frac{P(H_{R}\\,|\\,d)}{P(H_{P}\\,|\\,d)}=\\frac{\\pi_{R}}{\\pi_{P}} \\cdot \\frac{p(d\\,|\\,H_{R})}{p(d\\,|\\,H_{P})},$$ where $\\pi_{R}$ and $\\pi_{P}$ are prior probabilities and $p(d\\,|\\,H)$ is the likelihood of the evidence under hypothesis $H$.\n\nAssume conditional independence of the RNA-seq and protein homology evidence given the true boundary, so that $$p(d\\,|\\,H)=p(d_{\\mathrm{RNA}}\\,|\\,H)\\cdot p(d_{\\mathrm{PROT}}\\,|\\,H).$$ Priors are informed by splice motif strength and comparative genomics: $$\\pi_{R}=0.6,\\quad \\pi_{P}=0.4.$$ Likelihoods of the observed data $d$ under each hypothesis, estimated from a calibrated generative model of junction read counts and alignment scores, are:\n- RNA-seq junction evidence: $$p(d_{\\mathrm{RNA}}\\,|\\,H_{R})=0.92,\\quad p(d_{\\mathrm{RNA}}\\,|\\,H_{P})=0.12.$$\n- Protein homology evidence: $$p(d_{\\mathrm{PROT}}\\,|\\,H_{R})=0.35,\\quad p(d_{\\mathrm{PROT}}\\,|\\,H_{P})=0.78.$$\n\nThe loss function penalizes incorrect choices asymmetrically due to potential frameshift versus in-frame truncation:\n- Choosing $H_{R}$ when $H_{P}$ is true incurs loss $$L(H_{R},H_{P})=9.$$\n- Choosing $H_{P}$ when $H_{R}$ is true incurs loss $$L(H_{P},H_{R})=3.$$\n- Choosing the true boundary incurs zero loss: $$L(H_{R},H_{R})=0,\\quad L(H_{P},H_{P})=0.$$\n\nTasks:\n1. Starting from Bayes’ theorem and the definition of expected loss, derive a decision rule that minimizes expected loss in terms of the posterior odds $\\frac{P(H_{R}\\,|\\,d)}{P(H_{P}\\,|\\,d)}$ and the loss ratio.\n2. Using the provided priors and likelihoods, compute the posterior odds and apply the decision rule to select the optimal boundary.\n3. Express your final choice as a single number with no units: output $0$ if the optimal action is to choose the RNA-seq-supported boundary $H_{R}$, and output $1$ if the optimal action is to choose the protein-homology-supported boundary $H_{P}$.\n\nNo rounding is required for the final choice. All steps must be justified from the stated base without invoking any unprovided shortcut formulas.",
            "solution": "The problem statement is first validated against the specified criteria.\n\n### Step 1: Extract Givens\n- **Hypotheses**: $H_{R}$ (boundary from RNA-seq), $H_{P}$ (boundary from protein homology).\n- **Evidence**: $d = (d_{\\mathrm{RNA}}, d_{\\mathrm{PROT}})$.\n- **Bayesian Framework**:\n  - Posterior Odds: $\\frac{P(H_{R}\\,|\\,d)}{P(H_{P}\\,|\\,d)}=\\frac{\\pi_{R}}{\\pi_{P}} \\cdot \\frac{p(d\\,|\\,H_{R})}{p(d\\,|\\,H_{P})}$.\n  - Conditional Independence: $p(d\\,|\\,H)=p(d_{\\mathrm{RNA}}\\,|\\,H)\\cdot p(d_{\\mathrm{PROT}}\\,|\\,H)$.\n- **Prior Probabilities**: $\\pi_{R} = P(H_{R}) = 0.6$, $\\pi_{P} = P(H_{P}) = 0.4$.\n- **Likelihoods for RNA-seq evidence ($d_{\\mathrm{RNA}}$)**:\n  - $p(d_{\\mathrm{RNA}}\\,|\\,H_{R}) = 0.92$.\n  - $p(d_{\\mathrm{RNA}}\\,|\\,H_{P}) = 0.12$.\n- **Likelihoods for Protein Homology evidence ($d_{\\mathrm{PROT}}$)**:\n  - $p(d_{\\mathrm{PROT}}\\,|\\,H_{R}) = 0.35$.\n  - $p(d_{\\mathrm{PROT}}\\,|\\,H_{P}) = 0.78$.\n- **Loss Function ($L(\\text{action}, \\text{truth})$)**:\n  - $L(H_{R}, H_{P}) = 9$.\n  - $L(H_{P}, H_{R}) = 3$.\n  - $L(H_{R}, H_{R}) = 0$.\n  - $L(H_{P}, H_{P}) = 0$.\n- **Tasks**:\n  1. Derive a decision rule to minimize expected loss.\n  2. Compute posterior odds and apply the rule.\n  3. Output $0$ for choosing $H_{R}$ or $1$ for choosing $H_{P}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated as follows:\n- **Scientifically Grounded**: The problem is grounded in the principles of bioinformatics and molecular biology, specifically genome annotation. The use of Bayesian decision theory to integrate conflicting evidence from RNA-seq and protein homology is a standard and scientifically sound approach.\n- **Well-Posed**: The problem is well-posed. It provides all necessary information: a complete set of hypotheses, prior probabilities that sum to $1$ ($0.6 + 0.4 = 1$), a full set of likelihoods for the evidence under each hypothesis, and a clearly defined loss function. The tasks are specific and lead to a unique, computable solution.\n- **Objective**: The problem is stated in precise, quantitative, and objective language, free of ambiguity or subjective claims.\n\nThe problem does not exhibit any of the enumerated invalidity flaws. It is scientifically sound, formally specified, complete, and logically consistent.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution is provided below.\n\n### Task 1: Derivation of the Decision Rule\n\nThe goal is to select an action, either choosing $H_R$ or choosing $H_P$, that minimizes the expected loss. The expectation is taken over the possible true states of nature ($H_R$ or $H_P$), conditioned on the observed evidence $d$. The posterior probabilities $P(H_{R}\\,|\\,d)$ and $P(H_{P}\\,|\\,d)$ are used for this purpose.\n\nThe expected loss of choosing action $H_R$, denoted $E[L|a=H_R]$, is:\n$$ E[L\\,|\\,a=H_{R}] = L(H_{R}, H_{R}) \\cdot P(H_{R}\\,|\\,d) + L(H_{R}, H_{P}) \\cdot P(H_{P}\\,|\\,d) $$\nGiven $L(H_{R}, H_{R}) = 0$, this simplifies to:\n$$ E[L\\,|\\,a=H_{R}] = L(H_{R}, H_{P}) \\cdot P(H_{P}\\,|\\,d) $$\n\nThe expected loss of choosing action $H_P$, denoted $E[L|a=H_P]$, is:\n$$ E[L\\,|\\,a=H_{P}] = L(H_{P}, H_{R}) \\cdot P(H_{R}\\,|\\,d) + L(H_{P}, H_{P}) \\cdot P(H_{P}\\,|\\,d) $$\nGiven $L(H_{P}, H_{P}) = 0$, this simplifies to:\n$$ E[L\\,|\\,a=H_{P}] = L(H_{P}, H_{R}) \\cdot P(H_{R}\\,|\\,d) $$\n\nTo minimize expected loss, we should choose action $H_R$ if its expected loss is less than the expected loss of choosing $H_P$. That is, we choose $H_R$ if:\n$$ E[L\\,|\\,a=H_{R}] < E[L\\,|\\,a=H_{P}] $$\nSubstituting the expressions for expected loss:\n$$ L(H_{R}, H_{P}) \\cdot P(H_{P}\\,|\\,d) < L(H_{P}, H_{R}) \\cdot P(H_{R}\\,|\\,d) $$\nTo express this in terms of the posterior odds, we rearrange the inequality. Since posterior probabilities and losses are non-negative, and assuming $P(H_{P}\\,|\\,d) > 0$ and $L(H_{P}, H_{R}) > 0$, we can divide without changing the inequality's direction:\n$$ \\frac{L(H_{R}, H_{P})}{L(H_{P}, H_{R})} < \\frac{P(H_{R}\\,|\\,d)}{P(H_{P}\\,|\\,d)} $$\nThis is the desired decision rule: Choose $H_R$ if the posterior odds in favor of $H_R$ are greater than the ratio of the losses for incorrect decisions. Conversely, choose $H_P$ if the posterior odds are less than the loss ratio.\n\n### Task 2: Computation and Application of the Decision Rule\n\nFirst, we compute the loss ratio from the given loss function:\n$$ \\text{Loss Ratio} = \\frac{L(H_{R}, H_{P})}{L(H_{P}, H_{R})} = \\frac{9}{3} = 3 $$\nSo, the decision rule is: Choose $H_R$ if $\\frac{P(H_{R}\\,|\\,d)}{P(H_{P}\\,|\\,d)} > 3$.\n\nNext, we compute the posterior odds using the provided formula and data. The posterior odds are the product of the prior odds and the Bayes factor (the ratio of likelihoods).\nThe prior odds are:\n$$ \\frac{\\pi_{R}}{\\pi_{P}} = \\frac{0.6}{0.4} = 1.5 $$\nThe Bayes factor, $BF$, is the likelihood ratio, which, due to the conditional independence of evidence, is:\n$$ BF = \\frac{p(d\\,|\\,H_{R})}{p(d\\,|\\,H_{P})} = \\frac{p(d_{\\mathrm{RNA}}\\,|\\,H_{R}) \\cdot p(d_{\\mathrm{PROT}}\\,|\\,H_{R})}{p(d_{\\mathrm{RNA}}\\,|\\,H_{P}) \\cdot p(d_{\\mathrm{PROT}}\\,|\\,H_{P})} $$\nSubstituting the given likelihood values:\n$$ BF = \\frac{0.92 \\cdot 0.35}{0.12 \\cdot 0.78} = \\frac{0.322}{0.0936} $$\nNow, we compute the posterior odds:\n$$ \\frac{P(H_{R}\\,|\\,d)}{P(H_{P}\\,|\\,d)} = \\frac{\\pi_{R}}{\\pi_{P}} \\cdot BF = 1.5 \\cdot \\frac{0.322}{0.0936} = \\frac{0.483}{0.0936} $$\nTo compare this value with the loss ratio of $3$, we can perform the division:\n$$ \\frac{0.483}{0.0936} \\approx 5.16025... $$\nAlternatively, we can perform the comparison without division:\nIs $\\frac{0.483}{0.0936} > 3$?\nThis is equivalent to asking if $0.483 > 3 \\cdot 0.0936$.\n$3 \\cdot 0.0936 = 0.2808$.\nSince $0.483 > 0.2808$, the inequality holds.\n\nThe posterior odds ($\\approx 5.16$) are greater than the loss ratio ($3$).\nAccording to our derived decision rule, the optimal action is to choose the RNA-seq-supported boundary, $H_R$.\n\n### Task 3: Final Choice\nThe problem specifies to output $0$ if the optimal action is to choose $H_R$ and $1$ if the optimal action is to choose $H_P$. Since our analysis concluded that choosing $H_R$ is the optimal action to minimize expected loss, the final output is $0$.",
            "answer": "$$\\boxed{0}$$"
        }
    ]
}