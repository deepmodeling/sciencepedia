## 引言
在[分子生物学](@entry_id:140331)的中心法则中，基因通过转录为信使RNA（mRNA），进而翻译成蛋[白质](@entry_id:919575)来行使其功能。因此，精确测量细胞内每种mRNA分子的数量——即转录本丰度——是理解[基因表达调控](@entry_id:185479)、细胞[状态和](@entry_id:193625)疾病机制的核心。然而，从数以百万计的短测序读段（reads）中准确推断出原始的分子计数，是一项充满挑战的计算难题。技术偏好、转录本长度差异以及由可变剪接和同源基因引起的读段模糊性，共同构成了一道道需要精妙统计思想才能跨越的障碍。

本文旨在系统性地揭示转录本丰度定量的原理、方法与应用。我们将带领读者开启一场从理论到实践的深度探索之旅。在“原理与机制”一章中，我们将建立起从读段计数到丰度估计的数学模型，并重点剖析[期望最大化](@entry_id:273892)（EM）算法如何巧妙地解决数据中的不确定性。接着，在“应用与[交叉](@entry_id:147634)学科联系”一章中，我们将见证这些理论如何在[差异表达分析](@entry_id:266370)、单细胞革命和[空间转录组学](@entry_id:270096)等前沿领域中转化为强大的洞察力。最后，通过“动手实践”部分，您将有机会亲手实现核心算法，将抽象概念转化为具体的计算技能。

## 原理与机制

在上一章中，我们了解了转录本丰度定量的目标：通过读取数以百万计的短RNA片段（我们称之为“读段”），来精确测量细胞内每种[信使RNA](@entry_id:262893)（mRNA）分子的数量。现在，让我们深入这场迷人的侦探游戏的核心，揭示其背后的原理和机制。我们将像物理学家一样，从一个理想化的简单世界出发，逐步引入现实世界的复杂性，并见证统计学思想如何以其惊人的优雅和力量，帮助我们应对挑战，最终揭示生命的蓝图。

### 理想世界：一台完美的分子复印机

想象一下，我们有一台完美的分子复印机。它的工作方式是：从细胞的RNA“汤”中随机抓取一个分子，复制它的一小段，然后把它放回去。我们重复这个过程数百万次。如果我们想知道每种RNA的丰度，最简单的方法似乎就是数一数每种RNA被复制了多少次。

然而，第一个复杂性很快就出现了：不同的RNA转录本长度不同。一条长两千个碱基的转录本，就像一本厚书；而一条长五百个碱基的转录本，则像一本小册子。即使细胞里“厚书”和“小册子”的数量完全相同，我们的复印机随机“翻页”时，显然更有可能翻到“厚书”里的页面。

这意味着，我们观察到的读段数（$c_t$）不仅仅与转录本的分子数（即摩尔丰度）成正比，还与其长度成正比。更精确地说，是与它的 **[有效长度](@entry_id:184361)**（**effective length**, $\ell_t^{\text{eff}}$）成正比。为什么是“有效”长度？因为一个长度为 $l$ 的片段，它的起始位点不能落在转录本的末端，否则片段就会“超出边界”。对于一个长度为 $\ell_t$ 的转录本，一个长度为 $l$ 的片段只有 $\ell_t - l + 1$ 个可能的起始位点。考虑到我们的测序文库中存在着各种不同长度的片段（其[分布](@entry_id:182848)为 $f(l)$），一个转录本的“[有效长度](@entry_id:184361)”就是它能容纳的所有可能片段起始位点的[期望值](@entry_id:153208)。这就像是每条转录本上为片段准备的“着陆跑道”的平均长度。

因此，我们得到了第一个基本关系式：

$$ \text{期望读段数} \propto \text{摩尔丰度} \times \text{有效长度} $$

这个简单的公式是我们整个量化之旅的基石。

### 破译密码：从读段到丰度

现在，让我们把这个公式倒过来。我们拥有的是读段计数，我们想要的是摩尔丰度。为了消除长度带来的偏差，我们需要进行“归一化”。这就是 **TPM (Transcripts Per Million，[每百万转录本](@entry_id:170576))** 这个优雅单位的由来。

计算[TPM](@entry_id:170576)分为两步：

1.  **长度归一化**：我们将每个转录本的原始读段数（$c_t$）除以它的[有效长度](@entry_id:184361)（以千碱基为单位，$\ell_t^{\text{kb}}$）。这个比值 $\frac{c_t}{\ell_t^{\text{kb}}}$ 不再受长度影响，它现在正比于我们真正关心的摩尔丰度。

2.  **文库大小归一化**：然后，我们将这些经过长度校正的“丰度率”相加，得到一个总和。再用每个转录本的丰度率除以这个总和，就得到了它在整个转录本群体中所占的相对比例。最后，我们将这个比例乘以一百万（$10^6$），便得到了[TPM](@entry_id:170576)值。

$$ \mathrm{TPM}_t = 10^6 \times \frac{c_t / \ell_t^{\text{kb}}}{\sum_s (c_s / \ell_s^{\text{kb}})} $$

[TPM](@entry_id:170576)的美妙之处在于它的 **[组合性](@entry_id:637804)（compositional property）**。由于其定义，一个样本中所有转录本的TPM值之和永远等于一百万。它清晰地告诉我们，在构成这个样本的所有转录本中，某一种转录本占据了百万分之几的“份额”。这对于描述和比较 **单个样本内部** 各转录本的相对组成至关重要。

### 迷雾重重：当读段身份不明

到目前为止，我们一直假设每个读段都能唯一地追溯到它的“母亲”——那个产生它的转录本。但生物学世界充满了“相貌相似的家庭成员”。由于可变剪接，一个基因可以产生多个不同的转录本（称为亚型，isoform）；或者，基因组中可能存在序列高度相似的[旁系同源基因](@entry_id:263736)。

这就导致了一个棘手的问题：**多重映射读段（multimapping reads）**。一个读段可能完美地比对到多个不同的转录本上。我们该怎么办？简单地丢弃这些读段，就像侦探扔掉了指向多个嫌疑人的关键证据一样，会造成巨大的信息损失，并严重低估那些来自相似[基因家族](@entry_id:266446)的转录本的丰度。

这时，统计学闪亮登场，它提供了一个无比优雅的解决方案：我们不必做出非黑即白的武断决定，我们可以进行 **概率性归属**。

让我们引入 **[似然函数](@entry_id:141927)（Likelihood function）** 的概念。[似然函数](@entry_id:141927) $\mathcal{L}(\pi)$ 衡量的是，在我们当前的丰度估计值 $\pi = (\pi_1, \pi_2, \ldots)$ 之下，我们观测到的这整套读段数据有多大的可能性会发生。对于一个可以映射到多个转录本（例如，集合 $E_i$）的模糊读段 $r_i$，它被观测到的总概率是它来自每个可能来源的概率之和，并由每个来源自身的丰度 $\pi_t$ 加权。

$$ P(r_i \mid \pi) = \sum_{t \in E_i} \pi_t \cdot w_{it} $$

这里的 $w_{it}$ 代表给定它来自转录本 $t$ 的条件下，观测到读段 $r_i$ 的概率，它包含了[比对质量](@entry_id:170584)、片段偏好等信息。由于我们假设每个读段的产生是独立的，整个数据集的总[似然](@entry_id:167119)就是所有单个读段似然的乘积。

$$ \mathcal{L}(\pi) = \prod_{i=1}^{N} P(r_i \mid \pi) = \prod_{i=1}^{N} \left( \sum_{t \in E_i} \pi_t \, w_{it} \right) $$

我们的目标，就是找到那一组能让这个总[似然](@entry_id:167119)值达到最大的丰度 $\pi$。这就是著名的 **最大似然估计（Maximum Likelihood Estimation）**。

### [EM算法](@entry_id:274778)：与[数据展开](@entry_id:139734)的一场对话

直接求解使 $\mathcal{L}(\pi)$ 最大化的 $\pi$ 是一个数学难题。然而，统计学中一个名为 **[期望最大化](@entry_id:273892)（Expectation-Maximization, EM）算法** 的思想为此提供了绝妙的迭代方案。我们可以把[EM算法](@entry_id:274778)想象成我们与数据之间的一场逐步深入的对话。

这场对话分为两步，周而复始：

1.  **期望步骤（E-step）**：我们从对丰度 $\pi$ 的一个初始猜测开始。基于这个猜测，我们计算每个多重映射读段来自其各个可能“母亲”转录本的后验概率。这个概率，我们称之为“责任”（responsibility）。一个我们当前认为更丰富的转录本，将在瓜分这个模糊读段的“功劳”时，获得更大的份额。对于一个读段 $r$，它被归属于转录本 $i$ 的责任（或分数计数）正比于当前丰度估计 $\theta_i$ 和[有效长度](@entry_id:184361) $L_i$ 的乘积。

    $$ \text{归属于转录本 } i \text{ 的分数} = \frac{\theta_i L_i}{\sum_{j \in S_r} \theta_j L_j} $$

2.  **最大化步骤（M-step）**：现在，我们收集所有这些“责任”份额。我们将唯一映射读段的完整计数，与多重映射读段的这些分数计数加总，从而得到每个转录本的一个新的、更新后的“有效总计数”。基于这个新的总计数，我们重新计算出一套更好的丰度估计值 $\pi^{\text{new}}$。

    $$ \pi_t^{\text{new}} = \frac{1}{N} \sum_{i=1}^{N} (\text{读段 } i \text{ 对转录本 } t \text{ 的责任}) $$

我们不断重复这场“期望-最大化”的舞蹈。每一次迭代，我们的猜测都会变得更准一些。[EM算法](@entry_id:274778)的优美之处在于，它保证了每一步都会让总[似然函数](@entry_id:141927)的值上升（或保持不变），最终收敛到一个最佳解。这是一个自我修正的过程，它让数据本身告诉我们如何以最合理的方式去揭开模糊性的面纱。

### 直面现实：偏见与噪声

我们的模型已经相当精巧，但真实世界的实验过程远非完美。切割RNA并进行测序的过程，并非完全均匀和随机，这会引入各种系统性的 **偏见（bias）**。

*   **位置偏见（Positional bias）**：片段更倾向于从转录本的中间产生，而非两端。
*   **序列偏见（Sequence bias）**：用于切割RNA或进行逆转录的酶，可能对特定的[核苷酸](@entry_id:275639)序列（例如，片段末端的几个碱基）有偏好。
*   **[GC含量](@entry_id:275315)偏见（GC-content bias）**：[GC含量](@entry_id:275315)过高或过低的片段，在PCR扩增步骤中的效率可能不同。

我们该如何应对？答案是：将这些偏见也构建到我们的[概率模型](@entry_id:265150)中！我们可以创建一个“偏见函数” $b_t$，根据片段的起始位置、局部序列和[GC含量](@entry_id:275315)来调整它被观测到的概率。关键在于，我们需要巧妙地设计和归一化这个函数，确保它只在转录本内部重新分配概率，而不会系统性地改变从整个转录本中抽样的总概率。这保证了我们仍然能够唯一地确定真实的丰度参数 $\theta_t$。 这充分展示了[概率建模](@entry_id:168598)的强大威力与灵活性。

除了技术偏见，还存在 **[生物学变异](@entry_id:897703)（biological variability）**。即使是两个来自同一处理组的、遗传背景相同的生物样本，其内部各种RNA分子的确切数目也不可能完全一样。这种固有的生物学“噪声”是真实存在的，它使得在生物学重复样本之间，基因的表达量计数呈现出比纯粹随机抽样所预期的更大波动。

这种现象被称为 **[过离散](@entry_id:263748)（overdispersion）**，即计数的[方差](@entry_id:200758)大于其均值。简单的泊松分布模型（其假设[方差](@entry_id:200758)等于均值）在这里就捉襟见肘了。为了捕捉这种[过离散](@entry_id:263748)现象，我们需要更强大的[统计模型](@entry_id:165873)：

*   **[负二项分布](@entry_id:894191)（Negative Binomial, NB）**：这是一个非常出色的模型，可以看作是[泊松分布](@entry_id:147769)的一种扩展。它假设[泊松分布](@entry_id:147769)的均值本身不是一个固定值，而是在不同生物学样本[间变](@entry_id:902015)化的（遵循伽马[分布](@entry_id:182848)）。这样一来，其[方差](@entry_id:200758)自然就大于均值，完美地模拟了[过离散](@entry_id:263748)现象。

*   **狄利克雷-[多项分布](@entry_id:189072)（Dirichlet-Multinomial, DM）**：这是一个更高级的模型，它同时考虑一个样本中所有的转录本。它不仅能捕捉[过离散](@entry_id:263748)，还能模拟不同转录本计数之间的 **负相关性**——因为它们都在争夺有限的测序“预算”，一个转录本的读段数增加了，其他转录本的读段数就必然会相对减少。

### 融会贯通：比较的艺术

最后，让我们回到[TPM](@entry_id:170576)这个单位。我们已经看到它在描述单个样本 **内部** 组分时的优越性。但如果我们想比较一个“处理组”样本和一个“对照组”样本，并判断哪些基因的表达发生了真实的变化时，[TPM](@entry_id:170576)就可能变成一个陷阱。

想象一下问题中的场景：一种药物处理导致细胞内少数几种原本就非常丰富的转录本（比如[核糖体RNA](@entry_id:149305)）的绝对数量急剧上升。由于TPM的总和是固定的（一百万），这几个“大户”的份额暴增，必然会挤压其他所有转录本的相对份额。结果就是，即使许多其他基因的绝对分子数根本没有变化，它们的TPM值也会看起来像是下降了。

这就是 **组[合数](@entry_id:263553)据（compositional data）** 的诅咒。在一个总量固定的系统中，各个组成部分并非[相互独立](@entry_id:273670)，它们通过分母彼此关联。用分析独立变量的标准统计方法来分析它们，就像试图只通过测量气球表面一个点的移动，来判断整个气球是在膨胀还是在缩小一样，得到的结果往往是误导性的。

正确的做法是什么呢？我们需要回归到最原始的 **计数数据**，并使用我们刚才讨论过的、能够处理[过离散](@entry_id:263748)的强大[统计模型](@entry_id:165873)（如[负二项分布](@entry_id:894191)模型）。这些模型会采用更稳健的归一化方法（例如，不被少数极端高表达基因所影响的“[中位数](@entry_id:264877)比例法”），从而实现样本间表达水平的公正比较。

这给了我们一个深刻的启示：为正确的任务选择正确的工具。TPM是审视样本 **内部** 景观的完美镜头；而基于计数的统计模型，则是严谨地比较 **样本之间** 差异的必备标尺。理解这些原理，我们才能真正从海量测序数据中，解读出生命活动的真实故事。