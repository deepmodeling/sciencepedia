{
    "hands_on_practices": [
        {
            "introduction": "Before comparing gene expression across samples, we must first address a fundamental technical artifact: variation in sequencing depth or library size. This exercise walks you through the foundational calculations of the median-of-ratios normalization method, a robust technique widely used in popular tools like DESeq2. By manually computing size factors for a small dataset, you will gain a concrete understanding of how raw counts are adjusted to make them comparable, a critical first step for any valid differential expression analysis .",
            "id": "4556260",
            "problem": "You are given a small, scientifically plausible RNA sequencing (RNA-seq) count matrix with four genes and three samples. Two samples belong to Condition X and one to Condition Y. To correct compositional differences arising from unequal library sizes, consider a normalization approach that constructs a pseudo-reference profile for each gene using a geometric mean over samples, and defines sample-specific scaling by a robust summary (median) of sample-to-reference ratios across genes. This is a widely used, well-tested strategy in differential gene expression analysis.\n\nThe observed gene-by-sample count matrix is:\n- Gene $1$: $[100, 200, 150]$ for samples $S_1$ (X), $S_2$ (X), $S_3$ (Y)\n- Gene $2$: $[50, 100, 75]$ for samples $S_1$ (X), $S_2$ (X), $S_3$ (Y)\n- Gene $3$: $[200, 400, 300]$ for samples $S_1$ (X), $S_2$ (X), $S_3$ (Y)\n- Gene $4$: $[80, 160, 320]$ for samples $S_1$ (X), $S_2$ (X), $S_3$ (Y)\n\nAssume that Genes $1$, $2$, and $3$ are not differentially expressed between conditions (their true expression is constant across samples), while Gene $4$ is upregulated in Condition Y. Let $g_i$ denote the geometric mean of counts for gene $i$ across the three samples, and let $s_j$ denote the length-normalizing size factor for sample $j$ derived from the median of the ratios of observed counts to the corresponding $g_i$ values across all genes with strictly positive counts.\n\nTasks:\n1. Compute $g_i$ for $i \\in \\{1,2,3,4\\}$.\n2. Compute $s_j$ for $j \\in \\{1,2,3\\}$ using the median-of-ratios normalization defined above.\n3. Consider a downstream differential expression comparison of $S_3$ (Y) versus $S_1$ (X) for a gene that is truly not differentially expressed. If normalization were mistakenly ignored, the naive log fold change would inherit a bias equal to the natural logarithm of the ratio of effective library sizes between $S_3$ and $S_1$. Using the $s_j$ you computed, provide this bias as a single real number. Round your final numerical answer to four significant figures and express it in natural logarithm units (no physical units).",
            "solution": "The user has provided a problem that requires critical validation before a solution is attempted.\n\n### Step 1: Extract Givens\n- **Count Matrix ($C$)**: A $4 \\times 3$ matrix for $4$ genes and $3$ samples.\n  - Gene $1$: $[100, 200, 150]$ for samples $S_1, S_2, S_3$.\n  - Gene $2$: $[50, 100, 75]$ for samples $S_1, S_2, S_3$.\n  - Gene $3$: $[200, 400, 300]$ for samples $S_1, S_2, S_3$.\n  - Gene $4$: $[80, 160, 320]$ for samples $S_1, S_2, S_3$.\n- **Sample Conditions**:\n  - Sample $S_1$: Condition X\n  - Sample $S_2$: Condition X\n  - Sample $S_3$: Condition Y\n- **Assumptions**:\n  - Genes $1$, $2$, and $3$ are not differentially expressed.\n  - Gene $4$ is upregulated in Condition Y.\n- **Definitions**:\n  - $g_i$: The geometric mean of counts for gene $i$ across all samples.\n  - $s_j$: The size factor for sample $j$, defined as the median of the ratios of observed counts to the corresponding $g_i$ values across all genes with strictly positive counts.\n- **Tasks**:\n  1. Compute $g_i$ for $i \\in \\{1, 2, 3, 4\\}$.\n  2. Compute $s_j$ for $j \\in \\{1, 2, 3\\}$.\n  3. Compute the bias in the naive log fold change for a non-differentially expressed gene when comparing $S_3$ vs. $S_1$. This bias is defined as the natural logarithm of the ratio of effective library sizes ($s_3/s_1$).\n- **Output Requirement**: Round the final numerical answer for the bias to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The problem describes the median-of-ratios normalization method, a core component of the widely used and well-validated DESeq/DESeq2 bioinformatics tools for RNA-seq differential expression analysis. The method is scientifically sound and established in the field.\n2.  **Well-Posed**: The problem provides a complete count matrix and unambiguous definitions for all quantities to be calculated ($g_i$ and $s_j$). The tasks are sequentially structured and lead to a unique, meaningful answer.\n3.  **Objective**: The problem is stated in precise, quantitative terms. There are no subjective or ambiguous statements.\n4.  **Incomplete or Contradictory Setup**: The problem is self-contained. All counts are positive, so the geometric mean ($g_i$) is well-defined and non-zero for all genes. There are no contradictions in the provided data or definitions.\n5.  **Unrealistic or Infeasible**: The count values are integers and scientifically plausible for a small-scale RNA-seq experiment. The scenario is realistic.\n6.  **Ill-Posed or Poorly Structured**: The use of the median makes the size factor calculation robust to the single differentially expressed gene (Gene $4$), which is a key design principle of the method. This structure ensures a stable and meaningful solution.\n7.  **Pseudo-Profound, Trivial, or Tautological**: The problem is not trivial. It requires a correct step-by-step application of a standard bioinformatics algorithm and an understanding of how normalization factors relate to systemic bias.\n8.  **Outside Scientific Verifiability**: All calculations are verifiable through standard mathematical procedures.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\nThe provided count matrix, denoted by $C_{ij}$ for gene $i$ and sample $j$, is:\n$$\nC = \\begin{pmatrix} 100  200  150 \\\\ 50  100  75 \\\\ 200  400  300 \\\\ 80  160  320 \\end{pmatrix}\n$$\nwhere rows correspond to genes $i \\in \\{1, 2, 3, 4\\}$ and columns correspond to samples $j \\in \\{1, 2, 3\\}$.\n\n**Task 1: Compute the geometric mean $g_i$ for each gene.**\nThe geometric mean $g_i$ for gene $i$ across the $N=3$ samples is defined as:\n$$\ng_i = \\left( \\prod_{j=1}^{3} C_{ij} \\right)^{1/3}\n$$\nFor gene $1$:\n$g_1 = (100 \\times 200 \\times 150)^{1/3} = (3,000,000)^{1/3} = (3 \\times 10^6)^{1/3} = 100 \\cdot 3^{1/3}$\nFor gene $2$:\n$g_2 = (50 \\times 100 \\times 75)^{1/3} = (375,000)^{1/3} = (0.375 \\times 10^6)^{1/3} = (125 \\times 3 \\times 10^3)^{1/3} = 50 \\cdot 3^{1/3}$\nFor gene $3$:\n$g_3 = (200 \\times 400 \\times 300)^{1/3} = (24,000,000)^{1/3} = (24 \\times 10^6)^{1/3} = 100 \\cdot (24)^{1/3} = 100 \\cdot (8 \\times 3)^{1/3} = 200 \\cdot 3^{1/3}$\nFor gene $4$:\n$g_4 = (80 \\times 160 \\times 320)^{1/3} = (4,096,000)^{1/3} = (4.096 \\times 10^6)^{1/3} = (1.6^3 \\times 10^6)^{1/3} = 1.6 \\times 100 = 160$\n\n**Task 2: Compute the size factor $s_j$ for each sample.**\nThe size factor $s_j$ for sample $j$ is the median of the ratios $C_{ij} / g_i$ across all genes $i$.\n$$\ns_j = \\underset{i}{\\text{median}} \\left( \\frac{C_{ij}}{g_i} \\right)\n$$\nFor sample $S_1$ ($j=1$):\nThe ratios are:\n$r_{11} = \\frac{C_{11}}{g_1} = \\frac{100}{100 \\cdot 3^{1/3}} = 3^{-1/3}$\n$r_{21} = \\frac{C_{21}}{g_2} = \\frac{50}{50 \\cdot 3^{1/3}} = 3^{-1/3}$\n$r_{31} = \\frac{C_{31}}{g_3} = \\frac{200}{200 \\cdot 3^{1/3}} = 3^{-1/3}$\n$r_{41} = \\frac{C_{41}}{g_4} = \\frac{80}{160} = \\frac{1}{2}$\nWe need the median of $\\{3^{-1/3}, 3^{-1/3}, 3^{-1/3}, 1/2\\}$. To order these values, we compare $3^{-1/3}$ and $1/2$. This is equivalent to comparing $2$ and $3^{1/3}$. Since $2^3=8$ and $(3^{1/3})^3=3$, and $8 > 3$, we have $2 > 3^{1/3}$. Therefore, $\\frac{1}{2}  \\frac{1}{3^{1/3}} = 3^{-1/3}$.\nThe sorted list of ratios is $\\{\\frac{1}{2}, 3^{-1/3}, 3^{-1/3}, 3^{-1/3}\\}$. The median of these four values is the mean of the two central values, which are both $3^{-1/3}$.\n$s_1 = 3^{-1/3}$\n\nFor sample $S_2$ ($j=2$):\nThe ratios are:\n$r_{12} = \\frac{C_{12}}{g_1} = \\frac{200}{100 \\cdot 3^{1/3}} = 2 \\cdot 3^{-1/3}$\n$r_{22} = \\frac{C_{22}}{g_2} = \\frac{100}{50 \\cdot 3^{1/3}} = 2 \\cdot 3^{-1/3}$\n$r_{32} = \\frac{C_{32}}{g_3} = \\frac{400}{200 \\cdot 3^{1/3}} = 2 \\cdot 3^{-1/3}$\n$r_{42} = \\frac{C_{42}}{g_4} = \\frac{160}{160} = 1$\nWe need the median of $\\{2 \\cdot 3^{-1/3}, 2 \\cdot 3^{-1/3}, 2 \\cdot 3^{-1/3}, 1\\}$. We compare $2 \\cdot 3^{-1/3}$ and $1$. This is equivalent to comparing $2$ and $3^{1/3}$. As established, $2 > 3^{1/3}$, so $2 \\cdot 3^{-1/3} > 1$.\nThe sorted list of ratios is $\\{1, 2 \\cdot 3^{-1/3}, 2 \\cdot 3^{-1/3}, 2 \\cdot 3^{-1/3}\\}$. The median is the mean of the two central values.\n$s_2 = 2 \\cdot 3^{-1/3}$\n\nFor sample $S_3$ ($j=3$):\nThe ratios are:\n$r_{13} = \\frac{C_{13}}{g_1} = \\frac{150}{100 \\cdot 3^{1/3}} = 1.5 \\cdot 3^{-1/3}$\n$r_{23} = \\frac{C_{23}}{g_2} = \\frac{75}{50 \\cdot 3^{1/3}} = 1.5 \\cdot 3^{-1/3}$\n$r_{33} = \\frac{C_{33}}{g_3} = \\frac{300}{200 \\cdot 3^{1/3}} = 1.5 \\cdot 3^{-1/3}$\n$r_{43} = \\frac{C_{43}}{g_4} = \\frac{320}{160} = 2$\nWe need the median of $\\{1.5 \\cdot 3^{-1/3}, 1.5 \\cdot 3^{-1/3}, 1.5 \\cdot 3^{-1/3}, 2\\}$. We compare $1.5 \\cdot 3^{-1/3}$ and $2$. This is equivalent to comparing $1.5 = 3/2$ and $2 \\cdot 3^{1/3}$. It is clear that $1.5  2 \\cdot 3^{1/3}$ since $3^{1/3} > 1$.\nThe sorted list of ratios is $\\{1.5 \\cdot 3^{-1/3}, 1.5 \\cdot 3^{-1/3}, 1.5 \\cdot 3^{-1/3}, 2\\}$. The median is the mean of the two central values.\n$s_3 = 1.5 \\cdot 3^{-1/3} = \\frac{3}{2} \\cdot 3^{-1/3}$\n\n**Task 3: Compute the bias term.**\nThe problem states that the bias in the naive log fold change is the natural logarithm of the ratio of effective library sizes. The size factors $s_j$ are the estimates for these effective library sizes. We are comparing $S_3$ versus $S_1$.\nThe bias is given by $\\ln(s_3/s_1)$.\nUsing the results from Task 2:\n$$\n\\frac{s_3}{s_1} = \\frac{1.5 \\cdot 3^{-1/3}}{3^{-1/3}} = 1.5\n$$\nThe bias is therefore:\n$$\n\\text{Bias} = \\ln(1.5)\n$$\nTo obtain the numerical value, we compute the natural logarithm of $1.5$:\n$$\n\\ln(1.5) \\approx 0.405465108...\n$$\nRounding to four significant figures gives $0.4055$. This bias represents the systematic error introduced into log-fold-change calculations when differences in library sequencing depth are not accounted for. Normalization by dividing counts by their respective size factors $s_j$ before computing the log ratio would remove this bias.",
            "answer": "$$\n\\boxed{0.4055}\n$$"
        },
        {
            "introduction": "Once counts are normalized, the next step is to fit a statistical model to estimate the magnitude and significance of expression changes between conditions. This practice delves into the core modeling machinery, contrasting two major strategies: the intuitive approach of log-transforming normalized counts and applying Ordinary Least Squares (OLS), and the more statistically direct method of using a Poisson Generalized Linear Model (GLM) on the raw counts. By implementing both, you will develop a practical understanding of how model choice impacts coefficient estimation and appreciate the theoretical underpinnings of modern DGE analysis .",
            "id": "4556329",
            "problem": "You are given a toy single-gene dataset measured across two biological conditions and two technical batches. Your task is to construct a design matrix that encodes condition and batch effects with reference levels, and then estimate the fitted coefficients in two modeling frameworks: a linear model on log-transformed Counts Per Million (CPM) using Ordinary Least Squares (OLS), and a Poisson Generalized Linear Model (GLM) with a logarithmic link using Maximum Likelihood Estimation (MLE). Your program must compute these estimates for a provided test suite and output the condition-effect coefficients for each test case in a single line, in the exact format specified below.\n\nStart from the following fundamental bases and core definitions:\n- Sequencing read counts for a gene in sample $i$, denoted $c_i$, are nonnegative integers and are commonly modeled as conditionally independent Poisson random variables given an expected mean $ \\mu_i $, satisfying $ y_i \\sim \\operatorname{Poisson}(\\mu_i) $ for observed counts $ y_i = c_i $. The Poisson mean $ \\mu_i $ scales with the sample-specific library size $ L_i $ through an offset, with the canonical log-link in Generalized Linear Models (GLMs) defined by $ \\eta_i = \\log(\\mu_i) = \\log(L_i) + \\mathbf{x}_i^\\top \\boldsymbol{\\beta} $, where $ \\mathbf{x}_i $ are the covariates and $ \\boldsymbol{\\beta} $ are regression coefficients.\n- Counts Per Million (CPM) is defined as $ \\operatorname{CPM}_i = 10^6 \\cdot \\frac{c_i}{L_i} $. A stabilized log-transformation is commonly used to mitigate heteroscedasticity and zero counts. Use the natural logarithm with an additive constant $\\delta$ to define $ y_i^{(\\log\\mathrm{CPM})} = \\log\\left(\\operatorname{CPM}_i + \\delta\\right) $, where $ \\delta = 1 $.\n- Ordinary Least Squares (OLS) estimates for linear models on the transformed response follow from minimizing the sum of squared residuals. For a design matrix $ \\mathbf{X} $ and response vector $ \\mathbf{y} $, the OLS estimator is the solution to the normal equations $ \\mathbf{X}^\\top \\mathbf{X} \\, \\widehat{\\boldsymbol{\\beta}} = \\mathbf{X}^\\top \\mathbf{y} $.\n- Maximum Likelihood Estimation (MLE) for Poisson GLM with logarithmic link and offset $ \\log(L_i) $ maximizes the log-likelihood $ \\ell(\\boldsymbol{\\beta}) = \\sum_i \\left( y_i \\eta_i - \\exp(\\eta_i) - \\log(y_i!) \\right) $, where $ \\eta_i = \\log(L_i) + \\mathbf{x}_i^\\top \\boldsymbol{\\beta} $. The Iteratively Reweighted Least Squares (IRLS) algorithm is used to compute the MLE, iterating weighted least squares updates with working weights $ w_i = \\mu_i $ and working response $ z_i = \\eta_i + \\frac{y_i - \\mu_i}{\\mu_i} $, where $ \\mu_i = \\exp(\\eta_i) $.\n\nDesign matrix specification:\n- There are two conditions, denoted $A$ and $B$, and two batches, denoted $1$ and $2$. Use reference coding with condition $A$ and batch $1$ as the reference levels.\n- For $n$ samples, construct $ \\mathbf{X} \\in \\mathbb{R}^{n \\times 3} $ with columns $ [\\text{intercept}, \\mathbb{1}\\{\\text{condition}=B\\}, \\mathbb{1}\\{\\text{batch}=2\\}] $. The intercept column is all ones $1$, the second column is an indicator of condition $B$, and the third column is an indicator of batch $2$.\n\nEstimation tasks per test case:\n1. Compute $ \\operatorname{CPM}_i = 10^6 \\cdot \\frac{c_i}{L_i} $ for each sample $i$, then compute $ y_i^{(\\log\\mathrm{CPM})} = \\log\\left(\\operatorname{CPM}_i + 1\\right) $. Fit the linear model $ y^{(\\log\\mathrm{CPM})} = \\mathbf{X}\\boldsymbol{\\beta} + \\varepsilon $ by OLS and report the condition effect coefficient $ \\widehat{\\beta}_{\\text{cond}} $ (the coefficient on the condition indicator).\n2. Fit the Poisson GLM with log-link and offset $ \\log(L_i) $ by IRLS to maximize the Poisson log-likelihood, and report the condition effect coefficient $ \\widehat{\\beta}_{\\text{cond}} $.\n\nTest suite:\nFor all cases, there are $ n = 8 $ samples indexed $ i = 1, \\dots, 8 $, with conditions $ A $ for $ i \\in \\{1,2,3,4\\} $ and $ B $ for $ i \\in \\{5,6,7,8\\} $, and batches $ 1 $ for $ i \\in \\{1,3,5,7\\} $ and $ 2 $ for $ i \\in \\{2,4,6,8\\} $. The design matrix $ \\mathbf{X} $ uses this encoding.\n\n- Case 1 (balanced, moderate counts):\n    - Library sizes $ L = [5{,}000{,}000, \\; 4{,}500{,}000, \\; 5{,}500{,}000, \\; 4{,}800{,}000, \\; 5{,}200{,}000, \\; 4{,}700{,}000, \\; 5{,}100{,}000, \\; 4{,}900{,}000] $.\n    - Counts $ c = [50, \\; 36, \\; 55, \\; 38, \\; 78, \\; 56, \\; 76, \\; 59] $.\n\n- Case 2 (edge case with zeros and low counts):\n    - Library sizes $ L = [3{,}000{,}000, \\; 3{,}500{,}000, \\; 4{,}000{,}000, \\; 2{,}500{,}000, \\; 3{,}000{,}000, \\; 3{,}500{,}000, \\; 4{,}000{,}000, \\; 2{,}500{,}000] $.\n    - Counts $ c = [0, \\; 1, \\; 0, \\; 2, \\; 3, \\; 0, \\; 4, \\; 1] $.\n\n- Case 3 (null condition effect, batch effect present):\n    - Library sizes $ L = [5{,}000{,}000, \\; 5{,}000{,}000, \\; 5{,}000{,}000, \\; 5{,}000{,}000, \\; 5{,}000{,}000, \\; 5{,}000{,}000, \\; 5{,}000{,}000, \\; 5{,}000{,}000] $.\n    - Counts $ c = [60, \\; 48, \\; 60, \\; 48, \\; 60, \\; 48, \\; 60, \\; 48] $.\n\nAnswer specification:\n- For each test case, return a pair of floats $ [\\widehat{\\beta}_{\\text{cond}}^{(\\mathrm{OLS})}, \\widehat{\\beta}_{\\text{cond}}^{(\\mathrm{MLE})}] $ corresponding to the condition $B$ coefficient from the OLS on $ \\log(\\mathrm{CPM}+1) $ and the Poisson GLM MLE, respectively.\n- Express the outputs as natural logarithm coefficients (unitless), rounded to six decimal places.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the pair for one test case, in order. For example: $ [[x_1,y_1],[x_2,y_2],[x_3,y_3]] $.",
            "solution": "The user-provided problem is assessed to be valid. It is scientifically grounded in established statistical models for RNA-sequencing data analysis, well-posed with a complete and consistent setup, and expressed in objective, formal language. The task is to estimate regression coefficients for a given dataset under two distinct modeling frameworks.\n\n### 1. Design Matrix Construction\nThe experimental design involves $n=8$ samples, categorized by two factors: condition (levels $A$ and $B$) and technical batch (levels $1$ and $2$). The problem specifies a reference coding scheme where condition $A$ and batch $1$ are the reference levels. The design matrix $\\mathbf{X} \\in \\mathbb{R}^{8 \\times 3}$ models the expected value of the response as a function of an intercept, the effect of being in condition $B$ relative to $A$, and the effect of being in batch $2$ relative to $1$.\n\nThe columns of $\\mathbf{X}$ are defined as $[\\text{intercept}, \\mathbb{1}\\{\\text{condition}=B\\}, \\mathbb{1}\\{\\text{batch}=2\\}]$. Given the sample annotations:\n- Samples $1, 2, 3, 4$: Condition $A$\n- Samples $5, 6, 7, 8$: Condition $B$\n- Samples $1, 3, 5, 7$: Batch $1$\n- Samples $2, 4, 6, 8$: Batch $2$\n\nThe resulting design matrix $\\mathbf{X}$ is:\n$$\n\\mathbf{X} = \\begin{pmatrix}\n1  0  0 \\\\\n1  0  1 \\\\\n1  0  0 \\\\\n1  0  1 \\\\\n1  1  0 \\\\\n1  1  1 \\\\\n1  1  0 \\\\\n1  1  1\n\\end{pmatrix}\n$$\nThe coefficient vector to be estimated is $\\boldsymbol{\\beta} = [\\beta_0, \\beta_{\\text{cond}}, \\beta_{\\text{batch}}]^\\top$, where $\\beta_0$ is the intercept, $\\beta_{\\text{cond}}$ is the coefficient for condition $B$, and $\\beta_{\\text{batch}}$ is the coefficient for batch $2$. Our objective is to estimate $\\beta_{\\text{cond}}$ under two different models.\n\n### 2. Model 1: Ordinary Least Squares (OLS) on Log-Transformed Counts\nThis approach models the data after a transformation aimed at stabilizing variance and making the data more amenable to linear modeling.\n\n**Data Transformation**:\nFirst, the raw counts $c_i$ for each sample $i$ are normalized by their respective library sizes $L_i$ to obtain Counts Per Million (CPM), mitigating biases from sequencing depth differences.\n$$\n\\operatorname{CPM}_i = 10^6 \\cdot \\frac{c_i}{L_i}\n$$\nSecond, a natural logarithm transformation is applied with an added offset (pseudocount) of $\\delta=1$. This step further reduces heteroscedasticity (the dependence of variance on the mean) and ensures that zero counts are handled without resulting in undefined values. The transformed response vector $\\mathbf{y}^{(\\log\\mathrm{CPM})}$ has elements:\n$$\ny_i^{(\\log\\mathrm{CPM})} = \\log(\\operatorname{CPM}_i + 1)\n$$\n**Linear Model and Estimation**:\nThe transformed data are then modeled with a standard linear model, assuming the errors $\\varepsilon_i$ are independent and identically distributed with a mean of $0$:\n$$\n\\mathbf{y}^{(\\log\\mathrm{CPM})} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\n$$\nThe OLS estimator $\\widehat{\\boldsymbol{\\beta}}_{\\text{OLS}}$ is found by minimizing the sum of squared residuals, which yields the well-known normal equations. The solution is:\n$$\n\\widehat{\\boldsymbol{\\beta}}_{\\text{OLS}} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}^{(\\log\\mathrm{CPM})}\n$$\nThe estimated condition effect, $\\widehat{\\beta}_{\\text{cond}}^{(\\mathrm{OLS})}$, is the second component of the vector $\\widehat{\\boldsymbol{\\beta}}_{\\text{OLS}}$.\n\n### 3. Model 2: Poisson Generalized Linear Model (GLM)\nThis approach models the raw counts directly, assuming they follow a Poisson distribution. This is a more principled approach as it respects the discrete, non-negative nature of count data.\n\n**Model Formulation**:\nThe counts $y_i = c_i$ are modeled as independent samples from a Poisson distribution, $y_i \\sim \\operatorname{Poisson}(\\mu_i)$. The mean count $\\mu_i$ is related to the predictors via a logarithmic link function. The library size $L_i$ is incorporated as an offset term, which is a known predictor with a fixed coefficient of $1$. The full model for the linear predictor $\\eta_i$ is:\n$$\n\\eta_i = \\log(\\mu_i) = \\log(L_i) + \\mathbf{x}_i^\\top \\boldsymbol{\\beta}\n$$\nwhere $\\mathbf{x}_i^\\top$ is the $i$-th row of the design matrix $\\mathbf{X}$.\n\n**Maximum Likelihood Estimation via IRLS**:\nThe coefficients $\\boldsymbol{\\beta}$ are estimated by maximizing the Poisson log-likelihood function:\n$$\n\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} \\left( y_i \\eta_i - e^{\\eta_i} - \\log(y_i!) \\right)\n$$\nThis maximization problem is solved numerically using the Iteratively Reweighted Least Squares (IRLS) algorithm. Starting with an initial guess $\\boldsymbol{\\beta}^{(0)}$ (e.g., a vector of zeros), the algorithm iteratively performs the following steps until the estimates for $\\boldsymbol{\\beta}$ converge:\n1.  **Compute Linear Predictor**: $\\boldsymbol{\\eta}^{(t)} = \\log(\\mathbf{L}) + \\mathbf{X}\\boldsymbol{\\beta}^{(t)}$.\n2.  **Compute Fitted Means**: $\\boldsymbol{\\mu}^{(t)} = \\exp(\\boldsymbol{\\eta}^{(t)})$.\n3.  **Compute Working Weights**: The weights are diagonal entries of a matrix $\\mathbf{W}^{(t)}$, with $w_i^{(t)} = \\mu_i^{(t)}$.\n4.  **Compute Working Response**: The adjusted response vector $\\mathbf{z}^{(t)}$ is computed as $z_i^{(t)} = \\eta_i^{(t)} + \\frac{y_i - \\mu_i^{(t)}}{\\mu_i^{(t)}}$.\n5.  **Update Coefficients**: The next estimate $\\boldsymbol{\\beta}^{(t+1)}$ is obtained by solving a weighted least squares problem. This involves fitting the model $(\\mathbf{z}^{(t)} - \\log(\\mathbf{L})) = \\mathbf{X}\\boldsymbol{\\beta}$ with weights $\\mathbf{W}^{(t)}$. The solution is given by:\n    $$\n    \\boldsymbol{\\beta}^{(t+1)} = \\left(\\mathbf{X}^\\top \\mathbf{W}^{(t)} \\mathbf{X}\\right)^{-1} \\mathbf{X}^\\top \\mathbf{W}^{(t)} \\left(\\mathbf{z}^{(t)} - \\log(\\mathbf{L})\\right)\n    $$\nUpon convergence, the resulting vector $\\widehat{\\boldsymbol{\\beta}}_{\\text{MLE}}$ contains the maximum likelihood estimates. The condition effect, $\\widehat{\\beta}_{\\text{cond}}^{(\\mathrm{MLE})}$, is the second component of this vector.\n\nThe final program implements these two methods for each of the three test cases provided.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_irls_poisson(X, y, L, max_iter=50, tol=1e-8):\n    \"\"\"\n    Fits a Poisson GLM with a log-link using Iteratively Reweighted Least Squares (IRLS).\n\n    Args:\n        X (np.ndarray): Design matrix of shape (n_samples, n_features).\n        y (np.ndarray): Count data vector of shape (n_samples,).\n        L (np.ndarray): Library sizes vector of shape (n_samples,).\n        max_iter (int): Maximum number of iterations.\n        tol (float): Convergence tolerance for the sum of absolute changes in coefficients.\n\n    Returns:\n        np.ndarray: Estimated coefficients (beta).\n    \"\"\"\n    offset = np.log(L)\n    # Initialize coefficients to zeros\n    beta = np.zeros(X.shape[1])\n\n    for i in range(max_iter):\n        # Linear predictor (eta)\n        eta = offset + X @ beta\n        \n        # Mean response (mu)\n        mu = np.exp(eta)\n        \n        # Add a small epsilon for numerical stability, to avoid division by zero\n        # if mu becomes extremely small.\n        mu = np.maximum(mu, np.finfo(float).eps)\n\n        # Working response (z)\n        z = eta + (y - mu) / mu\n        \n        # Working weights (w) are the means for a Poisson GLM with canonical link\n        w = mu\n        \n        # The response for the Weighted Least Squares (WLS) step is the\n        # working response minus the offset.\n        # y_wls = z - offset\n        y_wls = (X @ beta) + (y - mu) / mu\n\n        # Solve the WLS system: (X.T W X) beta = X.T W y_wls\n        # Use element-wise multiplication for efficiency instead of creating a diagonal matrix W.\n        # X_T_W is equivalent to X.T @ np.diag(w)\n        X_T_W = X.T * w  # Broadcasting w across rows of X.T\n        X_T_W_X = X_T_W @ X\n        X_T_W_y_wls = X_T_W @ y_wls\n        \n        try:\n            beta_new = np.linalg.solve(X_T_W_X, X_T_W_y_wls)\n        except np.linalg.LinAlgError:\n            # If the matrix is singular, use pseudo-inverse as a fallback\n            beta_new = np.linalg.pinv(X_T_W_X) @ X_T_W_y_wls\n\n        # Check for convergence\n        if np.sum(np.abs(beta_new - beta))  tol:\n            beta = beta_new\n            break\n            \n        beta = beta_new\n        \n    return beta\n\ndef solve():\n    \"\"\"\n    Main function to process test cases and compute OLS and MLE coefficients.\n    \"\"\"\n    # Design matrix is constant for all test cases\n    # Columns: Intercept, Condition B, Batch 2\n    X = np.array([\n        [1, 0, 0], [1, 0, 1], [1, 0, 0], [1, 0, 1],  # Condition A\n        [1, 1, 0], [1, 1, 1], [1, 1, 0], [1, 1, 1]   # Condition B\n    ], dtype=float)\n\n    # Test suite from the problem statement\n    test_cases = [\n        {\n            \"L\": [5_000_000, 4_500_000, 5_500_000, 4_800_000, 5_200_000, 4_700_000, 5_100_000, 4_900_000],\n            \"c\": [50, 36, 55, 38, 78, 56, 76, 59]\n        },\n        {\n            \"L\": [3_000_000, 3_500_000, 4_000_000, 2_500_000, 3_000_000, 3_500_000, 4_000_000, 2_500_000],\n            \"c\": [0, 1, 0, 2, 3, 0, 4, 1]\n        },\n        {\n            \"L\": [5_000_000, 5_000_000, 5_000_000, 5_000_000, 5_000_000, 5_000_000, 5_000_000, 5_000_000],\n            \"c\": [60, 48, 60, 48, 60, 48, 60, 48]\n        }\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        L = np.array(case[\"L\"], dtype=float)\n        c = np.array(case[\"c\"], dtype=float)\n\n        # Task 1: OLS on log(CPM+1)\n        cpm = 1e6 * c / L\n        y_logcpm = np.log(cpm + 1)\n        beta_ols = np.linalg.solve(X.T @ X, X.T @ y_logcpm)\n        beta_ols_cond = beta_ols[1]\n\n        # Task 2: Poisson GLM MLE via IRLS\n        beta_mle = run_irls_poisson(X, c, L)\n        beta_mle_cond = beta_mle[1]\n        \n        # Store the rounded results for the condition effect coefficient\n        all_results.append([round(beta_ols_cond, 6), round(beta_mle_cond, 6)])\n\n    # Format output string to be exactly [[x1,y1],[x2,y2],...] with no spaces\n    # Example: [[0.518861,0.510826],[-0.076961,0.405465],[0.0,0.0]]\n    output_str = str(all_results).replace(\" \", \"\")\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "A typical differential expression analysis involves testing thousands of hypotheses simultaneously, one for each gene. This massive multiplicity creates a high risk of false positives, making it essential to adjust p-values to control the error rate. This exercise guides you in implementing the Benjamini-Hochberg procedure, the standard method for controlling the False Discovery Rate (FDR). Mastering this algorithm is crucial for moving from a long list of raw p-values to a statistically sound and reliable set of differentially expressed genes .",
            "id": "2385494",
            "problem": "You are given collections of raw $p$-values arising from $m$ independent gene-level null hypotheses in a differential gene expression experiment comparing two conditions. Let the set of null hypotheses be $\\{H_1,\\dots,H_m\\}$ with corresponding raw $p$-values $p_1,\\dots,p_m \\in [0,1]$. For a target level $q \\in (0,1)$, the Benjaminiâ€“Hochberg (BH) procedure for controlling the False Discovery Rate (FDR) at level $q$ is defined as follows. Write the $p$-values in nondecreasing order as $p_{(1)} \\le \\dots \\le p_{(m)}$, where the subscript denotes order statistics. Define the index\n$$\nk \\;=\\; \\max\\left\\{ i \\in \\{1,\\dots,m\\} \\;:\\; p_{(i)} \\le \\frac{i}{m}\\,q \\right\\},\n$$\nwith the convention that if the set is empty then no hypotheses are rejected. The BH rejection set is then all hypotheses with raw $p$-values not exceeding the threshold $p_{(k)}$, that is,\n$$\n\\mathcal{R} \\;=\\; \\{ j \\in \\{1,\\dots,m\\} \\;:\\; p_j \\le p_{(k)} \\},\n$$\nwhen $k$ exists, and $\\mathcal{R}=\\varnothing$ otherwise. The BH adjusted $p$-values (also called BH $q$-values in some contexts) are defined for each hypothesis by first assigning to the $i$-th order statistic the value\n$$\n\\tilde{p}_{(i)} \\;=\\; \\min_{j \\in \\{i,\\dots,m\\}} \\left( \\frac{m}{j}\\,p_{(j)} \\right),\n$$\nthen truncating at $1$ by $\\min\\{\\tilde{p}_{(i)},1\\}$, and finally mapping back to the original hypothesis order. To make the rank assignment deterministic in the presence of ties among equal $p$-values, use a stable ordering that breaks ties by increasing original index, that is, if $p_a=p_b$ and $ab$ then $p_a$ precedes $p_b$ in the order. Index hypotheses by their original positions using $0$-based indices.\n\nTask. For each test case below, given a list of raw $p$-values and a target FDR level $q$, compute:\n- the list of BH adjusted $p$-values in the original hypothesis order, each rounded to $6$ decimal places (expressed as decimals, not as percentages), and\n- the list of rejected hypothesis indices (using $0$-based indexing) in increasing order under the BH procedure at level $q$.\n\nTest suite. Evaluate your implementation on the following four cases, which together cover a typical case, boundary values, ties, and a case with no rejections:\n- Case $1$: $p=(0.001,\\,0.04,\\,0.03,\\,0.2,\\,0.5,\\,0.0005,\\,0.07,\\,0.9)$, $q=0.1$.\n- Case $2$: $p=(0,\\,1,\\,0.5,\\,0.05,\\,0.2)$, $q=0.05$.\n- Case $3$: $p=(0.02,\\,0.02,\\,0.02,\\,0.5,\\,0.8,\\,0.9)$, $q=0.1$.\n- Case $4$: $p=(0.2,\\,0.3,\\,0.4,\\,0.6,\\,0.8)$, $q=0.01$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with one element per test case. Each element must itself be a two-element list whose first element is the list of rounded BH adjusted $p$-values (in original order) and whose second element is the list of rejected $0$-based indices in increasing order. No other text should be printed. For example, the overall structure must be of the form\n[[adj_case1,rej_case1],[adj_case2,rej_case2],[adj_case3,rej_case3],[adj_case4,rej_case4]]\nwhere adj_case$k$ and rej_case$k$ are lists for case $k$ as specified above.",
            "solution": "The problem statement is assessed to be valid. It presents a clear, well-defined computational task based on the standard and scientifically sound Benjamini-Hochberg procedure for controlling the False Discovery Rate, a fundamental method in computational biology and statistics. The definitions, conditions, and test cases are complete, consistent, and objective, permitting a unique and verifiable solution.\n\nThe task is to implement the Benjamini-Hochberg (BH) procedure. Given a set of $m$ raw $p$-values $\\{p_1, \\dots, p_m\\}$ from multiple hypothesis tests and a target False Discovery Rate (FDR) level $q$, we must compute the BH adjusted $p$-values and identify the set of rejected hypotheses. The procedure is deterministic and will be implemented through a sequence of principled steps derived directly from the provided mathematical definitions.\n\nLet $m$ be the total number of hypotheses. The algorithm proceeds as follows:\n\n1.  **Data Structuring and Sorting**: Each raw $p$-value $p_j$ is associated with its original $0$-based index $j$. The resulting pairs $(p_j, j)$ are then sorted in non-decreasing order of $p_j$. The problem specifies a stable sorting mechanism to handle ties: if $p_a=p_b$ for original indices $ab$, the pair corresponding to $a$ must precede the pair for $b$. This step yields the ordered $p$-values $p_{(1)} \\le p_{(2)} \\le \\dots \\le p_{(m)}$, along with their original indices. The rank of $p_{(i)}$ is $i$.\n\n2.  **Identification of the Rejection Threshold**: The core of the BH procedure is to find the largest rank $k \\in \\{1,\\dots,m\\}$ for which the ordered $p$-value $p_{(k)}$ satisfies the condition:\n    $$\n    p_{(k)} \\le \\frac{k}{m}q\n    $$\n    This condition compares the $k$-th smallest $p$-value to a threshold that becomes linearly more lenient with increasing rank $k$. If the set of ranks satisfying this inequality is empty, it is concluded that no hypotheses can be rejected at the specified FDR level $q$, and the rejection set $\\mathcal{R}$ is empty. Otherwise, the value $p_{(k)}$ corresponding to the largest such rank $k$ becomes the significance threshold.\n\n3.  **Determination of the Rejection Set**: The set of rejected hypotheses, $\\mathcal{R}$, consists of all hypotheses whose original, unadjusted $p$-value $p_j$ is less than or equal to the threshold $p_{(k)}$ found in the previous step. That is, $\\mathcal{R} = \\{ j \\in \\{1,\\dots,m\\} : p_j \\le p_{(k)} \\}$. The final output requires the $0$-based indices of these hypotheses, sorted in increasing order.\n\n4.  **Computation of Adjusted $p$-values**: The BH adjusted $p$-value for the hypothesis corresponding to the $i$-th ordered raw $p$-value, $p_{(i)}$, is defined as:\n    $$\n    \\tilde{p}_{(i)} = \\min_{j \\in \\{i,\\dots,m\\}} \\left( \\frac{m}{j}p_{(j)} \\right)\n    $$\n    This is then truncated at $1$ if necessary. This definition ensures that the adjusted $p$-values are monotonically non-decreasing with respect to their rank, i.e., $\\tilde{p}_{(1)} \\le \\tilde{p}_{(2)} \\le \\dots \\le \\tilde{p}_{(m)}$. An efficient computational strategy for this step involves first computing the scaled values $\\frac{m}{j}p_{(j)}$ for all ranks $j=1, \\dots, m$. Then, one iterates backwards from rank $m$ down to $1$, calculating the cumulative minimum at each step. Specifically, the adjusted $p$-value for rank $i$ is the minimum of its own scaled value and the adjusted $p$-value of rank $i+1$. The final adjusted $p$-values are then mapped back to their original hypothesis order and rounded to $6$ decimal places as required.\n\nThe implementation will apply this four-step logic to each of the provided test cases to generate the required outputs.",
            "answer": "```python\nimport numpy as np\n\ndef _format_output(data):\n    \"\"\"\n    Custom recursive function to format the final list into a string\n    without spaces and with floats formatted to 6 decimal places.\n    \"\"\"\n    if isinstance(data, list):\n        return f\"[{','.join(_format_output(item) for item in data)}]\"\n    if isinstance(data, float):\n        return f\"{data:.6f}\"\n    return str(data)\n\ndef benjamini_hochberg(p_values: np.ndarray, q: float):\n    \"\"\"\n    Performs the Benjamini-Hochberg procedure for FDR control.\n\n    Args:\n        p_values: A numpy array of raw p-values.\n        q: The target False Discovery Rate level.\n\n    Returns:\n        A tuple containing:\n        - A list of BH adjusted p-values, rounded to 6 decimal places, in original order.\n        - A sorted list of 0-based indices of rejected hypotheses.\n    \"\"\"\n    m = len(p_values)\n    if m == 0:\n        return [], []\n        \n    original_indices = np.arange(m)\n    \n    # Sort p-values while keeping track of original indices.\n    # The 'stable' kind ensures that for equal p-values, the original order is preserved,\n    # satisfying the problem's tie-breaking rule.\n    sorted_order_indices = np.argsort(p_values, kind='stable')\n    sorted_p_values = p_values[sorted_order_indices]\n    \n    # --- Step 1: Find rejection threshold ---\n    ranks = np.arange(1, m + 1)\n    bh_thresholds = (ranks / m) * q\n    \n    significant_mask = sorted_p_values = bh_thresholds\n    \n    rejected_indices = []\n    if np.any(significant_mask):\n        # Find the largest rank k satisfying the BH condition\n        k = np.max(ranks[significant_mask])\n        \n        # The rejection threshold is the p-value at this rank k\n        rejection_threshold = sorted_p_values[k - 1]\n        \n        # Identify all hypotheses with original p-values = threshold\n        rejected_mask = p_values = rejection_threshold\n        rejected_indices = original_indices[rejected_mask].tolist()\n\n    # --- Step 2: Calculate adjusted p-values ---\n    # Calculate raw scaled p-values: (m/i) * p_(i)\n    scaled_p_values = (m / ranks) * sorted_p_values\n    \n    # Enforce monotonicity by taking the cumulative minimum from the end (right-to-left)\n    adj_p_sorted = np.minimum.accumulate(scaled_p_values[::-1])[::-1]\n    \n    # Truncate values at 1.0\n    adj_p_sorted = np.minimum(adj_p_sorted, 1.0)\n    \n    # Unsort the adjusted p-values to match the original p-value order\n    adj_p_original = np.empty(m)\n    adj_p_original[sorted_order_indices] = adj_p_sorted\n    \n    # Round to 6 decimal places for final output\n    adj_p_rounded = [round(p, 6) for p in adj_p_original]\n\n    return adj_p_rounded, rejected_indices\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the final result.\n    \"\"\"\n    test_cases = [\n        ((0.001, 0.04, 0.03, 0.2, 0.5, 0.0005, 0.07, 0.9), 0.1),\n        ((0.0, 1.0, 0.5, 0.05, 0.2), 0.05),\n        ((0.02, 0.02, 0.02, 0.5, 0.8, 0.9), 0.1),\n        ((0.2, 0.3, 0.4, 0.6, 0.8), 0.01),\n    ]\n\n    results = []\n    for p_tuple, q_val in test_cases:\n        p_values_np = np.array(p_tuple)\n        adj_p, rej_idx = benjamini_hochberg(p_values_np, q_val)\n        results.append([adj_p, rej_idx])\n    \n    # Print the final result in the specified single-line format\n    print(_format_output(results))\n\nsolve()\n```"
        }
    ]
}