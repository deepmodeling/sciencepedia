{
    "hands_on_practices": [
        {
            "introduction": "在电喷雾电离质谱（ESI-MS）中，肽段的电荷状态是影响其检测和碎裂的关键参数。深入理解肽段的一级序列和周围溶液的$pH$值如何共同决定其电荷，对于解析质谱图和设计稳健的定量蛋白质组学实验至关重要。本练习（）将基础生物化学与实用的蛋白质组学联系起来，通过应用亨德森-哈塞尔巴尔赫方程来计算肽段的期望电荷状态，从而巩固支撑质谱生物标志物发现的物理化学原理。",
            "id": "4994707",
            "problem": "在使用液相色谱-质谱联用（LC-MS）的转化医学的生物标志物发现工作流程中，胰蛋白酶解肽段的电荷态分布对母离子选择和碎裂效率有关键性影响。考虑一个候选的血浆生物标志物肽段，其经过酶解后产生一个含有$n_{K} = 2$个赖氨酸残基、$n_{R} = 1$个精氨酸残基、$n_{H} = 3$个组氨酸残基和一个未修饰的游离N-末端（$n_{N} = 1$）的肽段。假设每个可质子化位点独立作用，最多能接受一个质子，无协同效应，且离子强度可忽略不计。电离前的溶剂是pH值为$\\mathrm{pH} = 7.40$的近生理条件缓冲液。对这些可质子化基团使用以下酸解离常数（以$\\mathrm{p}K_a$表示）：赖氨酸侧链$\\mathrm{p}K_{a,K} = 10.50$，精氨酸侧链$\\mathrm{p}K_{a,R} = 12.50$，组氨酸侧链$\\mathrm{p}K_{a,H} = 6.00$，以及肽段N-末端$\\mathrm{p}K_{a,N} = 8.00$。\n\n从酸碱性的定义以及$\\mathrm{p}H$、$\\mathrm{p}K_a$和碱与共轭酸浓度比之间的关系（亨德森-哈塞尔巴尔赫方程）出发，推导通用可质子化位点$i$的质子化分数$\\,\\alpha_i\\,$，然后通过对所有可质子化碱性位点上结合的预期质子数求和，计算该肽段在$\\mathrm{p}H = 7.40$时的预期正离子模式电荷态$q_{\\mathrm{avg}}$：\n$$q_{\\mathrm{avg}} = n_{K}\\,\\alpha_{K} + n_{R}\\,\\alpha_{R} + n_{H}\\,\\alpha_{H} + n_{N}\\,\\alpha_{N}.$$\n将最终的预期电荷态以无量纲数的形式给出。将您的答案四舍五入到四位有效数字。",
            "solution": "首先根据指定标准对问题进行严格验证。\n\n### 问题验证\n\n**第1步：提取给定信息**\n- 赖氨酸残基数：$n_{K} = 2$\n- 精氨酸残基数：$n_{R} = 1$\n- 组氨酸残基数：$n_{H} = 3$\n- 未修饰的游离N-末端数：$n_{N} = 1$\n- 溶剂pH值：$\\mathrm{pH} = 7.40$\n- 赖氨酸侧链pKa：$\\mathrm{p}K_{a,K} = 10.50$\n- 精氨酸侧链pKa：$\\mathrm{p}K_{a,R} = 12.50$\n- 组氨酸侧链pKa：$\\mathrm{p}K_{a,H} = 6.00$\n- 肽段N-末端pKa：$\\mathrm{p}K_{a,N} = 8.00$\n- 假设：可质子化位点独立，每个位点最多接受一个质子，无协同效应，离子强度可忽略。\n- 平均电荷公式：$q_{\\mathrm{avg}} = n_{K}\\,\\alpha_{K} + n_{R}\\,\\alpha_{R} + n_{H}\\,\\alpha_{H} + n_{N}\\,\\alpha_{N}$\n- 任务：推导质子化分数$\\alpha_i$并计算$q_{\\mathrm{avg}}$。\n\n**第2步：使用提取的给定信息进行验证**\n- **科学依据：** 该问题坚实地基于酸碱化学原理，特别是亨德森-哈塞尔巴尔赫方程，及其在计算肽段等电点和电荷态方面的应用，这是生物化学和蛋白质组学中的一个标准和基本概念。所提供的$\\mathrm{p}K_a$值对于肽链中的氨基酸残基是符合实际的。使用LC-MS进行生物标志物发现的背景是恰当且科学合理的。\n- **适定性：** 这是一个适定问题。它提供了所有必要的数据（每种残基的数量、它们各自的$\\mathrm{p}K_a$值以及溶剂的$\\mathrm{pH}$值）和一个明确的目标。每个位点独立且单次质子化的假设确保了可以确定唯一且稳定的解。\n- **客观性：** 问题以精确、客观的语言陈述，不含任何主观或基于观点的论断。\n\n**第3步：结论与行动**\n该问题是**有效的**。它科学合理、适定且客观。可以进行求解过程。\n\n### 求解推导\n\n该问题要求推导一个通用碱性位点的质子化分数$\\alpha_i$，然后计算该肽段的总预期电荷态$q_{\\mathrm{avg}}$。\n\n**1. 质子化分数（$\\alpha_i$）的推导**\n\n考虑一个通用的单质子碱性位点$B$，它与其共轭酸$BH^{+}$处于平衡状态。该平衡表示质子化形式的解离：\n$$BH^{+} \\rightleftharpoons B + H^{+}$$\n此平衡的酸解离常数$K_a$定义为：\n$$K_a = \\frac{[B][H^{+}]}{[BH^{+}]}$$\n根据定义，$\\mathrm{p}K_a = -\\log_{10}(K_a)$ 且 $\\mathrm{pH} = -\\log_{10}([H^{+}])$。对$K_a$表达式取以10为底的负对数，得到：\n$$-\\log_{10}(K_a) = -\\log_{10}\\left(\\frac{[B][H^{+}]}{[BH^{+}]}\\right)$$\n$$\\mathrm{p}K_a = -\\log_{10}([H^{+}]) - \\log_{10}\\left(\\frac{[B]}{[BH^{+}]}\\right)$$\n$$\\mathrm{p}K_a = \\mathrm{pH} - \\log_{10}\\left(\\frac{[B]}{[BH^{+}]}\\right)$$\n整理后得到亨德森-哈塞尔巴尔赫方程：\n$$\\mathrm{pH} = \\mathrm{p}K_a + \\log_{10}\\left(\\frac{[B]}{[BH^{+}]}\\right)$$\n位点$i$的质子化分数$\\alpha_i$是处于质子化状态$BH^{+}$的分子所占的比例：\n$$\\alpha_i = \\frac{[BH^{+}]}{[B] + [BH^{+}]}$$\n从亨德森-哈塞尔巴尔赫方程，我们可以表示出碱式与酸式的比率：\n$$\\log_{10}\\left(\\frac{[B]}{[BH^{+}]}\\right) = \\mathrm{pH} - \\mathrm{p}K_a$$\n$$ \\frac{[B]}{[BH^{+}]} = 10^{\\mathrm{pH} - \\mathrm{p}K_a} $$\n$$ [B] = [BH^{+}] \\cdot 10^{\\mathrm{pH} - \\mathrm{p}K_a} $$\n将$[B]$的这个表达式代入$\\alpha_i$的方程中：\n$$\\alpha_i = \\frac{[BH^{+}]}{([BH^{+}] \\cdot 10^{\\mathrm{pH} - \\mathrm{p}K_a}) + [BH^{+}]}$$\n从分母中提出$[BH^{+}]$公因子并消去该项，得到质子化分数的最终表达式：\n$$\\alpha_i = \\frac{1}{10^{\\mathrm{pH} - \\mathrm{p}K_a} + 1}$$\n\n**2. 具体质子化分数的计算**\n\n使用推导出的公式和给定值（$\\mathrm{pH} = 7.40$），我们计算每种碱性位点的质子化分数。\n\n- **赖氨酸 (K):** $\\mathrm{p}K_{a,K} = 10.50$\n$$\\alpha_{K} = \\frac{1}{10^{7.40 - 10.50} + 1} = \\frac{1}{10^{-3.10} + 1} \\approx \\frac{1}{0.0007943 + 1} \\approx 0.9992062$$\n\n- **精氨酸 (R):** $\\mathrm{p}K_{a,R} = 12.50$\n$$\\alpha_{R} = \\frac{1}{10^{7.40 - 12.50} + 1} = \\frac{1}{10^{-5.10} + 1} \\approx \\frac{1}{0.000007943 + 1} \\approx 0.9999921$$\n\n- **组氨酸 (H):** $\\mathrm{p}K_{a,H} = 6.00$\n$$\\alpha_{H} = \\frac{1}{10^{7.40 - 6.00} + 1} = \\frac{1}{10^{1.40} + 1} \\approx \\frac{1}{25.11886 + 1} \\approx 0.0382864$$\n\n- **N-末端 (N):** $\\mathrm{p}K_{a,N} = 8.00$\n$$\\alpha_{N} = \\frac{1}{10^{7.40 - 8.00} + 1} = \\frac{1}{10^{-0.60} + 1} \\approx \\frac{1}{0.2511886 + 1} \\approx 0.7992440$$\n\n**3. 预期电荷态（$q_{\\mathrm{avg}}$）的计算**\n\n总预期电荷态是所有可质子化位点上结合的预期质子数之和。这可使用问题陈述中提供的公式以及每种残基的数量进行计算：$n_{K} = 2$, $n_{R} = 1$, $n_{H} = 3$, $n_{N} = 1$。\n\n$$q_{\\mathrm{avg}} = n_{K}\\,\\alpha_{K} + n_{R}\\,\\alpha_{R} + n_{H}\\,\\alpha_{H} + n_{N}\\,\\alpha_{N}$$\n$$q_{\\mathrm{avg}} = (2)(\\alpha_{K}) + (1)(\\alpha_{R}) + (3)(\\alpha_{H}) + (1)(\\alpha_{N})$$\n代入计算出的每个$\\alpha_i$值：\n$$q_{\\mathrm{avg}} \\approx (2)(0.9992062) + (1)(0.9999921) + (3)(0.0382864) + (1)(0.7992440)$$\n$$q_{\\mathrm{avg}} \\approx 1.9984124 + 0.9999921 + 0.1148592 + 0.7992440$$\n$$q_{\\mathrm{avg}} \\approx 3.9125077$$\n问题要求将答案四舍五入到四位有效数字。\n$$q_{\\mathrm{avg}} \\approx 3.913$$\n该肽段在$\\mathrm{pH} = 7.40$时的预期正离子模式电荷态约为$+3.913$。",
            "answer": "$$\\boxed{3.913}$$"
        },
        {
            "introduction": "当一个蛋白质组学特征被开发成一个潜在的诊断或预后分类器后，必须对其性能进行严格的量化评估。受试者工作特征（ROC）曲线提供了一个全面的可视化工具，用于评估分类器在所有可能的决策阈值下区分两个组别（例如，疾病组与对照组）的能力。本练习（）提供了一个动手实践的机会，从原始分类器输出中构建ROC曲线并计算曲线下面积（AUC），这是评估区分能力的黄金标准。它还深入探讨了模型区分度（discrimination）和校准度（calibration）之间至关重要的概念差异，这对于临床决策至关重要。",
            "id": "4994751",
            "problem": "一个转化蛋白质组学团队正在验证一个通过液相色谱-串联质谱（LC-MS/MS）测量的候选生物标志物组合，以区分临床定义的疾病状态与对照组。一个基于肽段强度特征训练的概率分类器为盲法验证样本输出了排序后的预测概率。对于验证集，已知的临床状态和模型输出概率如下：\n\n- 存在疾病（阳性）：$\\{0.93,\\,0.85,\\,0.77,\\,0.52,\\,0.40,\\,0.30\\}$。\n- 不存在疾病（阴性）：$\\{0.88,\\,0.68,\\,0.35,\\,0.25,\\,0.15,\\,0.05\\}$。\n\n使用基本定义，即真阳性率（TPR，灵敏度）为 $\\mathrm{TPR} = \\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FN}}$，假阳性率（FPR，$1-$特异度）为 $\\mathrm{FPR} = \\frac{\\mathrm{FP}}{\\mathrm{FP}+\\mathrm{TN}}$，并且受试者工作特征（ROC）曲线是当预测概率的决策阈值 $\\tau$ 从高到低扫描时 $\\left(\\mathrm{FPR}(\\tau),\\,\\mathrm{TPR}(\\tau)\\right)$ 的参数图，请执行以下操作：\n\n- 通过在所有唯一的模型输出值上扫描 $\\tau$，从第一性原理推导 ROC 曲线，并明确列出 $\\left(\\mathrm{FPR},\\,\\mathrm{TPR}\\right)$ 点的序列。\n- 计算曲线下面积（AUC），即 $\\mathrm{TPR}$ 在 $\\mathrm{FPR}\\in[0,1]$ 上对 $\\mathrm{FPR}$ 的积分，并为给定数据精确求值。\n\n此外，针对临床决策，请简要解释在此背景下区分度与校准度的区别。假设通过单调映射 $p' = \\sigma\\!\\left(a + b\\,\\sigma^{-1}(p)\\right)$（其中 $\\sigma(x) = \\frac{1}{1+\\exp(-x)}$，$a = 0$，$b = 2$）对预测概率进行假设性重新校准。讨论（无需进行任何新的数值计算）这种重新校准是否会改变 ROC 曲线或 AUC，并解释其在错分成本不相等的情况下对阈值选择的影响。\n\n将最终的 AUC 表示为精确分数或小数。不包含任何单位，也不要使用百分号。如果选择小数，可以精确报告，无需四舍五入。",
            "solution": "该问题经验证具有科学依据、良定且客观。它提供了足够的数据和清晰的定义，可以为所要求的计算推导出唯一的解决方案，并为概念部分提供合理的讨论。该问题是评估二元分类器的标准练习，这是转化医学和生物标志物发现中的一项核心任务。\n\n### 第 1 部分：ROC 曲线的推导\n\n受试者工作特征（ROC）曲线是在不同决策阈值设置下，真阳性率（TPR）对假阳性率（FPR）的图。\n\n给定的数据是：\n- 存在疾病（阳性）类别的概率，$P$：$\\{0.93, 0.85, 0.77, 0.52, 0.40, 0.30\\}$。阳性样本总数为 $N_P = 6$。\n- 不存在疾病（阴性）类别的概率，$N$：$\\{0.88, 0.68, 0.35, 0.25, 0.15, 0.05\\}$。阴性样本总数为 $N_N = 6$。\n\nTPR 和 FPR 的定义如下：\n$$ \\mathrm{TPR} = \\frac{\\mathrm{TP}}{N_P} \\quad \\text{和} \\quad \\mathrm{FPR} = \\frac{\\mathrm{FP}}{N_N} $$\n其中 TP（真阳性）是分数大于或等于阈值 $\\tau$ 的阳性样本数量，FP（假阳性）是分数 $\\ge \\tau$ 的阴性样本数量。\n\n为了构建 ROC 曲线，我们将每个唯一的分数值视为一个潜在的阈值。我们按降序排列所有唯一的分数：\n$S = \\{0.93, 0.88, 0.85, 0.77, 0.68, 0.52, 0.40, 0.35, 0.30, 0.25, 0.15, 0.05\\}$。\n\n我们将阈值 $\\tau$ 从一个大于最大分数的值（例如 $\\tau=1$）扫到一个小于或等于最小分数的值。\n\n1.  当 $\\tau > 0.93$ 时：没有样本被分类为阳性。$\\mathrm{TP}=0$, $\\mathrm{FP}=0$。因此，$(\\mathrm{FPR}, \\mathrm{TPR}) = (0, 0)$。这是 ROC 曲线的起点。\n\n2.  当我们降低阈值越过每个分数时，我们更新 TP 或 FP。每当阈值越过一个阳性样本的分数时，TP 增加 $1$，TPR 增加 $1/N_P = 1/6$。每当它越过一个阴性样本的分数时，FP 增加 $1$，FPR 增加 $1/N_N = 1/6$。\n\n随着阈值的降低，生成 $(\\mathrm{FPR}, \\mathrm{TPR})$ 点的序列如下：\n-   $\\tau = 0.93$: 越过一个阳性样本的分数。TP 变为 $1$。点：$(\\frac{0}{6}, \\frac{1}{6}) = (0, \\frac{1}{6})$。\n-   $\\tau = 0.88$: 越过一个阴性样本的分数。FP 变为 $1$。点：$(\\frac{1}{6}, \\frac{1}{6})$。\n-   $\\tau = 0.85$: 越过一个阳性样本的分数。TP 变为 $2$。点：$(\\frac{1}{6}, \\frac{2}{6}) = (\\frac{1}{6}, \\frac{1}{3})$。\n-   $\\tau = 0.77$: 越过一个阳性样本的分数。TP 变为 $3$。点：$(\\frac{1}{6}, \\frac{3}{6}) = (\\frac{1}{6}, \\frac{1}{2})$。\n-   $\\tau = 0.68$: 越过一个阴性样本的分数。FP 变为 $2$。点：$(\\frac{2}{6}, \\frac{3}{6}) = (\\frac{1}{3}, \\frac{1}{2})$。\n-   $\\tau = 0.52$: 越过一个阳性样本的分数。TP 变为 $4$。点：$(\\frac{2}{6}, \\frac{4}{6}) = (\\frac{1}{3}, \\frac{2}{3})$。\n-   $\\tau = 0.40$: 越过一个阳性样本的分数。TP 变为 $5$。点：$(\\frac{2}{6}, \\frac{5}{6}) = (\\frac{1}{3}, \\frac{5}{6})$。\n-   $\\tau = 0.35$: 越过一个阴性样本的分数。FP 变为 $3$。点：$(\\frac{3}{6}, \\frac{5}{6}) = (\\frac{1}{2}, \\frac{5}{6})$。\n-   $\\tau = 0.30$: 越过一个阳性样本的分数。TP 变为 $6$。点：$(\\frac{3}{6}, \\frac{6}{6}) = (\\frac{1}{2}, 1)$。\n-   $\\tau = 0.25$: 越过一个阴性样本的分数。FP 变为 $4$。点：$(\\frac{4}{6}, \\frac{6}{6}) = (\\frac{2}{3}, 1)$。\n-   $\\tau = 0.15$: 越过一个阴性样本的分数。FP 变为 $5$。点：$(\\frac{5}{6}, \\frac{6}{6}) = (\\frac{5}{6}, 1)$。\n-   $\\tau = 0.05$: 越过一个阴性样本的分数。FP 变为 $6$。点：$(\\frac{6}{6}, \\frac{6}{6}) = (1, 1)$。这是最后一个点。\n\nROC 曲线的 $(\\mathrm{FPR}, \\mathrm{TPR})$ 顶点明确序列，从 $(0,0)$ 开始到 $(1,1)$ 结束，是：\n$$ (0,0) \\to (0, \\frac{1}{6}) \\to (\\frac{1}{6}, \\frac{1}{6}) \\to (\\frac{1}{6}, \\frac{1}{3}) \\to (\\frac{1}{6}, \\frac{1}{2}) \\to (\\frac{1}{3}, \\frac{1}{2}) \\to (\\frac{1}{3}, \\frac{2}{3}) \\to (\\frac{1}{3}, \\frac{5}{6}) \\to (\\frac{1}{2}, \\frac{5}{6}) \\to (\\frac{1}{2}, 1) \\to (\\frac{2}{3}, 1) \\to (\\frac{5}{6}, 1) \\to (1,1) $$\n\n### 第 2 部分：曲线下面积（AUC）的计算\n\nAUC 是 ROC 曲线的积分，$\\int_0^1 \\mathrm{TPR}(\\mathrm{FPR}) \\, d\\mathrm{FPR}$。对于由离散数据构建的经验 ROC 曲线，该积分通过对由顶点形成的梯形面积求和来计算。由点 $(x_{i-1}, y_{i-1})$ 和 $(x_i, y_i)$ 定义的梯形面积为 $\\frac{1}{2}(y_{i-1} + y_i)(x_i - x_{i-1})$。\n\nROC 曲线由水平段和垂直段组成。面积仅在 FPR 发生变化的水平段上累积。\n-   从 $\\mathrm{FPR}=0$ 到 $\\mathrm{FPR}=1/6$，$\\mathrm{TPR}$ 恒定为 $1/6$。面积 = $(\\frac{1}{6} - 0) \\times \\frac{1}{6} = \\frac{1}{36}$。（这假设了阶梯状行为，另一种方法是我们将使用的梯形法则）。\n\n对顶点使用梯形法则：\n-   点 $(\\frac{1}{6}, \\frac{1}{6})$ 到 $(\\frac{1}{6}, \\frac{1}{2})$：这些是垂直段，其中 $\\Delta\\mathrm{FPR}=0$，不贡献面积。\n-   点 $(\\frac{1}{6}, \\frac{1}{2})$ 到 $(\\frac{1}{3}, \\frac{1}{2})$：水平段。面积 = $\\frac{1}{2}(\\frac{1}{2}+\\frac{1}{2})(\\frac{1}{3}-\\frac{1}{6}) = \\frac{1}{2}(1)(\\frac{1}{6}) = \\frac{1}{12}$。\n-   点 $(\\frac{1}{3}, \\frac{1}{2})$ 到 $(\\frac{1}{3}, \\frac{5}{6})$：垂直段，零面积。\n-   以此类推。\n\n一个更直接的方法是计算曲线下矩形的面积之和。\n-   面积$_1$：宽 $\\frac{1}{6}$，高 $\\frac{1}{6}$。面积 = $\\frac{1}{36}$。\n-   面积$_2$：宽 $\\frac{1}{6}$，高 $\\frac{3}{6}$。面积 = $(\\frac{2}{6}-\\frac{1}{6}) \\times \\frac{3}{6} = \\frac{1}{6} \\times \\frac{3}{6} = \\frac{3}{36}$。\n-   面积$_3$：宽 $\\frac{1}{6}$，高 $\\frac{5}{6}$。面积 = $(\\frac{3}{6}-\\frac{2}{6}) \\times \\frac{5}{6} = \\frac{1}{6} \\times \\frac{5}{6} = \\frac{5}{36}$。\n-   面积$_4$：宽 $\\frac{3}{6}$，高 $1$。面积 = $(1-\\frac{3}{6}) \\times 1 = \\frac{3}{6} = \\frac{18}{36}$。\n总面积 = $\\frac{1}{36} + \\frac{3}{36} + \\frac{5}{36} + \\frac{18}{36} = \\frac{27}{36}$。\n\n一种等价且稳健的方法是使用 Wilcoxon-Mann-Whitney U 统计量公式，其中 AUC 是一个随机选择的阳性样本的分数高于一个随机选择的阴性样本的分数的概率，即 $P(S_P > S_N)$。共有 $N_P \\times N_N = 6 \\times 6 = 36$ 对可能的（阳性，阴性）样本配对。\n我们计算满足 $p \\in P$, $n \\in N$, 且 $p > n$ 的配对 $(p, n)$ 的数量。\n-   对于 $p=0.93$：$6$ 对（$0.93 >$ 所有 $6$ 个阴性分数）。\n-   对于 $p=0.85$：$5$ 对（$0.85 >$ 除 $0.88$ 外的所有阴性分数）。\n-   对于 $p=0.77$：$5$ 对（$0.77 >$ 除 $0.88$ 外的所有阴性分数）。\n-   对于 $p=0.52$：$4$ 对（$0.52 > \\{0.35, 0.25, 0.15, 0.05\\}$）。\n-   对于 $p=0.40$：$4$ 对（$0.40 > \\{0.35, 0.25, 0.15, 0.05\\}$）。\n-   对于 $p=0.30$：$3$ 对（$0.30 > \\{0.25, 0.15, 0.05\\}$）。\n一致对的总数为 $6 + 5 + 5 + 4 + 4 + 3 = 27$。\n阳性和阴性分数之间没有相同的值。\n因此，AUC 为：\n$$ \\mathrm{AUC} = \\frac{27}{36} = \\frac{3}{4} = 0.75 $$\n\n### 第 3 部分：区分度与校准度的解释\n\n-   **区分度**（Discrimination）指模型区分不同类别的能力。一个具有良好区分度的模型会给阳性实例赋予比阴性实例更高的分数。ROC 曲线及其相关的 AUC 是区分度的纯粹度量。它们是基于分数的排序构建的。AUC 为 $1.0$ 表示完美的区分能力，而 AUC 为 $0.5$ 表示模型的区分能力不优于随机猜测。\n\n-   **校准度**（Calibration）指模型的预测概率在多大程度上反映了事件的真实可能性。如果在一个模型预测概率为 $p$ 的所有实例中，阳性实例的真实比例也是 $p$，那么该模型就是完美校准的。校准度关注的是预测值的绝对意义，而不仅仅是它们的排序。一个模型可以有出色的区分度（高 AUC），但校准度很差（例如，系统性地过于自信或不自信）。\n\n总而言之，区分度是关于分离类别，而校准度是关于概率分数本身的可信度。\n\n### 第 4 部分：重新校准的效果\n\n所提出的重新校准是一个单调映射 $p' = \\sigma(a + b\\,\\sigma^{-1}(p))$。当 $a=0$ 且 $b=2$ 时，映射为 $p' = \\sigma(2\\,\\sigma^{-1}(p))$。sigmoid 函数 $\\sigma(x) = (1+\\exp(-x))^{-1}$ 及其反函数 logit 函数 $\\sigma^{-1}(p) = \\ln(p/(1-p))$ 都是严格单调递增的。由于 $b = 2 > 0$，变换 $x \\mapsto 2x$ 也是严格递增的。严格单调递增函数的复合函数本身也是严格单调递增的。\n\n-   **对 ROC 曲线和 AUC 的影响**：ROC 曲线和 AUC 是区分度的度量，仅依赖于分数的排序。由于重新校准函数 $p \\mapsto p'$ 是严格单调的，它保留了预测概率的排序。如果 $p_1 > p_2$，那么 $p'_1 > p'_2$。因此，ROC 曲线将与原始曲线**完全相同**，AUC 也将**保持不变**。重新校准在不影响区分度的情况下改善了校准度。\n\n-   **对阈值选择的启示**：临床决策通常涉及不同类型错分（假阳性 vs. 假阴性）的不相等成本。最优决策阈值 $\\tau^*$ 取决于这些成本。例如，为了最小化期望成本，如果预测概率 $p$ 超过一个与成本比率相关的阈值，就可能将其分类为阳性，例如 $\\tau^* = \\frac{C_{FP}}{C_{FP} + C_{FN}}$，其中 $C_{FP}$ 和 $C_{FN}$ 分别是假阳性和假阴性的成本。该公式假设概率是良好校准的。如果原始概率 $p$ 没有经过校准，应用理论上推导出的阈值是不正确的。重新校准旨在使概率 $p'$ 变得可靠。通过使用重新校准后的概率 $p'$，决策者可以更有信心地应用从成本效益分析中得出的阈值，相信它将导致最优结果。因此，尽管重新校准不改变整体区分能力（AUC），但通过确保用于阈值判定的概率是有意义的，它对于在个体层面做出信息充分、对成本敏感的决策至关重要。",
            "answer": "$$\\boxed{0.75}$$"
        },
        {
            "introduction": "一个成功的生物标志物发现实验通常会产生一长串候选蛋白质列表，但并非所有候选者都同样有希望进行下游验证或作为治疗靶点。因此，一个合理的优先级排序策略对于有效集中资源至关重要。本练习（）为此挑战引入了一种强大的系统生物学方法。通过将定量蛋白质组学数据与蛋白质-蛋白质相互作用网络相结合，您将学习计算各种网络中心性指标，以推断蛋白质的生物学重要性，并将这些见解整合成一个统一的“治疗杠杆指数”以进行候选物排序。",
            "id": "4994680",
            "problem": "您的任务是设计并实现一个完整的程序，该程序将一种原则性方法操作化，用于假设在转化医学中通过蛋白质组学发现的候选蛋白质生物标志物的因果作用和治疗杠杆点。从图论和定量蛋白质组学的核心定义开始，推导一种整合网络拓扑与肽级定量以优先排序候选物的方法。\n\n该程序必须将蛋白质相互作用网络建模为一个无向加权图，其中节点代表蛋白质，边权重代表从与质谱分析正交的独立来源推断出的相互作用置信度。候选生物标志物蛋白质通过液相色谱-串联质谱（LC-MS/MS）与无标记定量法鉴定，其表达效应大小以绝对log2倍数变化提供。每种蛋白质的成药性风险惩罚以一个介于$0$和$1$之间的无量纲数提供。\n\n使用以下基本基础和定义：\n\n- 考虑一个图 $G = (V, E)$，有 $|V| = n$ 个节点。每条无向边 $(i, j) \\in E$ 有一个置信度权重 $w_{ij} \\in (0, 1]$。更高的 $w_{ij}$ 表示更强的相互作用证据。通过规则 $l_{ij} = 1 / w_{ij}$ 定义有效边长 $l_{ij}$，因此更高的置信度意味着更短的有效距离。所有边长都严格为正。\n- 对于图中边长为 $l_{ij}$ 的最短路径计算，使用最短路径距离的标准定义：距离 $d(i,j)$ 等于从 $i$ 到 $j$ 的所有路径中 $l_{e}$ 总和的最小值。\n- 定义节点 $v$ 的介数中心性为所有有序源-目标对 $(s, t)$（其中 $s \\neq t \\neq v$）中，从 $s$ 到 $t$ 经过 $v$ 的最短路径所占比例的总和。这需要计算所有对之间的最短路径数量以及经过 $v$ 的子集。应用正确的无向图对计数处理方法。\n- 定义节点 $v$ 的调和紧密性中心性为从 $v$ 到所有其他可达节点的倒数最短路径距离的平均值，具体为 $\\sum_{u \\in V \\setminus \\{v\\}, u \\text{ reachable}} 1 / d(v, u)$ 除以 $(n - 1)$。\n- 定义特征向量中心性为非负邻接矩阵 $A$ 的主右特征向量，其中元素为 $A_{ij} = w_{ij}$，与非负矩阵的Perron–Frobenius定理一致。每个分量的大小反映了相应节点通过其邻居的影响力。\n- 定义节点 $v$ 的加权度中心性为与 $v$ 相邻的边的权重总和，即 $\\sum_{u: (v,u) \\in E} w_{vu}$。\n\n归一化要求：\n\n- 每个中心性向量在所有节点上必须使用节点集 $V$ 上的最小-最大缩放归一化到区间 $[0, 1]$。如果所有节点对于给定度量具有相同的值（即最小值等于最大值），则为该度量的所有节点分配归一化值 $0$。\n\n治疗杠杆指数：\n\n- 对于每个节点 $i$，令 $B_i$、$C_i$、$E_i$ 和 $D_i$ 分别表示归一化的介数、调和紧密性、特征向量和加权度中心性。令 $F_i$ 表示来自 LC-MS/MS 的绝对 log2 倍数变化，令 $p_i$ 表示成药性惩罚。节点 $i$ 的治疗杠杆指数是三个因子的乘积：绝对表达效应大小 $F_i$，一个由用户提供的非负权重 $(\\alpha, \\beta, \\gamma, \\delta)$ 的归一化中心性的类凸加权组合，以及一个成药性衰减因子 $(1 - p_i)$。具体来说，分数为 $S_i = F_i \\cdot (\\alpha B_i + \\beta C_i + \\gamma E_i + \\delta D_i) \\cdot (1 - p_i)$，其中 $(\\alpha, \\beta, \\gamma, \\delta)$ 针对每个测试用例提供，且不一定总和为 $1$。\n- 候选物优先排序规则：给定一个候选集 $K \\subseteq V$，选择使 $S_i$ 最大化的索引 $i^{\\star} \\in K$。如果在容差 $\\varepsilon = 10^{-12}$ 内出现平局，通过选择具有更大特征向量中心性 $E_i$ 的候选物来打破平局。如果在此容差内仍然存在平局，则选择最小的整数索引。\n\n算法约束与现实性：\n\n- 最短路径距离必须使用正权重算法计算，并采用指定的边长映射 $l_{ij} = 1 / w_{ij}$。最短路径数量的计算必须与加权距离和 $G$ 的无向性一致。\n- 对于不连通的图，按规定使用调和紧密性，并确保到不可达节点的距离不产生贡献。\n\n您的程序必须实现这些定义，并为以下测试套件生成结果。所有数值均为无量纲（无物理单位），具体的数值如下所示。\n\n测试套件：\n\n- 案例 1：\n  - 节点数 $n = 6$。\n  - 带置信度权重 $w_{ij}$ 的无向边：\n    - $(0, 1): w_{01} = 0.9$, $(0, 2): w_{02} = 0.6$, $(1, 2): w_{12} = 0.5$, $(1, 3): w_{13} = 0.7$, $(2, 3): w_{23} = 0.4$, $(3, 4): w_{34} = 0.8$, $(4, 5): w_{45} = 0.3$, $(2, 5): w_{25} = 0.2$。\n  - 候选集 $K = \\{1, 3, 4\\}$。\n  - 绝对log2倍数变化值 $F = [1.2, 0.8, 0.5, 1.1, 0.4, 0.9]$，按节点索引 $0$到$5$对齐。\n  - 成药性惩罚 $p = [0.2, 0.1, 0.3, 0.05, 0.4, 0.6]$，按节点索引对齐。\n  - 中心性组合权重 $(\\alpha, \\beta, \\gamma, \\delta) = (0.4, 0.3, 0.2, 0.1)$。\n\n- 案例 2：\n  - 节点数 $n = 5$。\n  - 带置信度权重 $w_{ij}$ 的无向边：\n    - $(0, 1): w_{01} = 0.5$, $(1, 2): w_{12} = 0.5$, $(2, 3): w_{23} = 0.5$, $(3, 4): w_{34} = 0.5$, $(0, 4): w_{04} = 0.1$。\n  - 候选集 $K = \\{0, 2, 4\\}$。\n  - 绝对log2倍数变化值 $F = [0.3, 1.0, 1.5, 0.2, 0.4]$，按节点索引 $0$到$4$对齐。\n  - 成药性惩罚 $p = [0.3, 0.1, 0.2, 0.5, 0.3]$，按节点索引对齐。\n  - 中心性组合权重 $(\\alpha, \\beta, \\gamma, \\delta) = (0.25, 0.25, 0.25, 0.25)$。\n\n- 案例 3：\n  - 节点数 $n = 6$。\n  - 带置信度权重 $w_{ij}$ 的无向边：\n    - 节点 $\\{0, 1, 2\\}$ 上的组件 A：$(0, 1): w_{01} = 0.7$, $(1, 2): w_{12} = 0.6$, $(0, 2): w_{02} = 0.5$。\n    - 节点 $\\{3, 4, 5\\}$ 上的组件 B：$(3, 4): w_{34} = 0.9$, $(4, 5): w_{45} = 0.8$。\n  - 候选集 $K = \\{2, 3, 5\\}$。\n  - 绝对log2倍数变化值 $F = [0.6, 0.1, 2.0, 0.5, 1.2, 1.4]$，按节点索引 $0$到$5$对齐。\n  - 成药性惩罚 $p = [0.5, 0.5, 0.1, 0.2, 0.3, 0.2]$，按节点索引对齐。\n  - 中心性组合权重 $(\\alpha, \\beta, \\gamma, \\delta) = (0.3, 0.4, 0.2, 0.1)$。\n\n输出规范：\n\n- 您的程序应生成单行输出，其中包含一个逗号分隔的列表，该列表包含三个案例中选定的候选节点索引，并用方括号括起来。例如，输出必须看起来完全像 $[i_1,i_2,i_3]$，其中每个 $i_k$ 是在案例 $k$ 中选为顶级治疗杠杆候选者的整数索引。",
            "solution": "用户提供了一个定义明确的计算问题，该问题位于转化医学和蛋白质组学领域。任务是基于一个自定义的“治疗杠杆指数”来优先排序候选蛋白质生物标志物。该指数整合了定量蛋白质组学数据（表达倍数变化）、网络拓扑属性（图中心性）和治疗可及性的启发式方法（成药性惩罚）。我将构建一个解决方案，该方案严格遵循所提供的数学定义和算法约束。\n\n### 第1步：问题分解与算法设计\n\n问题的核心是为一组候选蛋白质计算一个分数 $S_i$，并根据此分数和一系列平局打破规则选出最佳者。分数 $S_i$ 定义为：\n$$S_i = F_i \\cdot (\\alpha B_i + \\beta C_i + \\gamma E_i + \\delta D_i) \\cdot (1 - p_i)$$\n其中 $F_i$ 是绝对log倍数变化，$p_i$ 是成药性惩罚，$(\\alpha, \\beta, \\gamma, \\delta)$ 是权重，$B_i, C_i, E_i, D_i$ 是归一化的中心性度量。\n\n每个测试用例的总体算法将按以下步骤进行：\n1.  **图构建**：将蛋白质相互作用网络表示为一个加权无向图。邻接矩阵 $W$ 将存储置信度权重 $w_{ij}$，而邻接表将存储有效边长 $l_{ij} = 1/w_{ij}$，用于最短路径算法。\n2.  **中心性计算**：计算图中所有节点的四种所需中心性度量。\n    *   **加权度中心性 ($D$)**：这可以直接从加权邻接矩阵计算得出：$D_i = \\sum_j W_{ij}$。\n    *   **特征向量中心性 ($E$)**：通过找到加权邻接矩阵 $W$ 的主右特征向量来确定。Perron-Frobenius定理确保对于像 $W$ 这样的非负矩阵，存在一个与最大幅值特征值（谱半径）对应的非负特征向量。这可以使用标准数值库计算。对于不连通的图，此方法能正确识别出具有最大谱半径的连通分量中最中心的节点。\n    *   **所有对最短路径 (APSP)**：调和紧密性和介数中心性的计算依赖于最短路径距离。由于所有边长 $l_{ij} = 1/w_{ij}$ 均为正，Dijkstra算法是一个合适的选择。我们将从每个节点运行Dijkstra算法，以计算所有对最短路径距离的矩阵 $d(i, j)$。\n    *   **调和紧密性中心性 ($C$)**：使用计算出的最短路径距离 $d(i,j)$，节点 $i$ 的调和紧密性计算为 $C_i = \\frac{1}{n-1} \\sum_{j \\neq i, \\text{reachable}} \\frac{1}{d(i, j)}$。不可达节点的 $d(i,j) = \\infty$，因此它们的贡献为 $0$。\n    *   **介数中心性 ($B$)**：这是计算上最密集的度量。其形式化定义涉及对所有节点对 $(s, t)$ 的求和。直接实现效率会很低。Brandes算法提供了一种高效的计算方法。对于每个节点 $s$，它执行一次单源最短路径 (SSSP) 遍历（对于加权图使用修改版的Dijkstra算法），计算最短路径的数量 $\\sigma_{s*}$。然后它从最远的节点开始反向传播以计算依赖关系，将这些依赖关系相加得到最终的介数分数。问题指定了对有序对 $(s, t)$ 求和，这通过在无向图上标准应用Brandes算法（即从每个节点运行SSSP）即可实现。\n3.  **归一化**：四个计算出的中心性向量（$B, C, E, D$）中的每一个都必须使用最小-最大缩放归一化到 $[0, 1]$ 区间。对于某个度量，如果所有节点的值都相同（即最小值等于最大值），则按照规定，将所有节点的归一化值设为 $0$。\n4.  **评分与优先排序**：对于每个候选节点 $i \\in K$，使用提供的公式和归一化的中心性值计算治疗杠杆指数 $S_i$。\n5.  **带平局打破规则的选择**：通过识别 $K$ 中得分 $S_i$ 最高的节点来选择最终候选者。在容差 $\\varepsilon = 10^{-12}$ 内的平局通过选择具有更高归一化特征向量中心性 $E_i$ 的节点来解决。如果（同样在容差 $\\varepsilon$ 内）仍然存在平局，则选择最小的整数索引。\n\n这种结构化方法确保了问题陈述中的所有定义和约束都得到遵守，从而得到一个正确且可验证的解决方案。\n\n### 第2步：实施策略\n\n实现将是一个Python脚本，使用 `numpy` 库进行数值运算（特别是特征向量计算），并使用标准库中的 `heapq` 模块为Dijkstra算法实现一个高效的优先队列。\n\n一个主函数 `solve()` 将协调测试套件的处理。一个辅助函数 `_solve_case()` 将封装单个测试用例的逻辑，从图构建到最终候选选择。这种模块化设计增强了清晰度和正确性。将特别注意浮点数比较，使用指定的容差 $\\varepsilon$ 来处理潜在的不精确性，尤其是在最短路径和平局打破逻辑中。\n```python\nimport numpy as np\nimport heapq\n\ndef _solve_case(n, edges, K, F, p, centrality_weights, tol=1e-12):\n    \"\"\"\n    Solves a single instance of the biomarker prioritization problem.\n\n    This function implements the complete pipeline: graph construction, calculation of\n    four different network centralities (Weighted Degree, Eigenvector, Harmonic\n    Closeness, Betweenness), normalization of these centralities, calculation of\n    a therapeutic leverage index, and finally, prioritization of candidate\n    biomarkers based on this index with specified tie-breaking rules.\n    \"\"\"\n    if n == 0:\n        return None\n\n    # 1. Build graph representations\n    adj = [[] for _ in range(n)]\n    w_matrix = np.zeros((n, n))\n    for u, v, w in edges:\n        length = 1.0 / w\n        adj[u].append((v, length))\n        adj[v].append((u, length))\n        w_matrix[u, v] = w\n        w_matrix[v, u] = w\n\n    # 2. Calculate Centralities\n    # 2.1. Weighted Degree Centrality\n    degree_cent = np.sum(w_matrix, axis=1)\n\n    # 2.2. Eigenvector Centrality\n    try:\n        eigvals, eigvecs = np.linalg.eig(w_matrix)\n        dominant_idx = np.argmax(np.abs(eigvals))\n        eigen_cent_raw = eigvecs[:, dominant_idx].real\n        eigen_cent = np.abs(eigen_cent_raw)\n    except np.linalg.LinAlgError:\n        eigen_cent = np.zeros(n)\n\n    # 2.3. Betweenness Centrality (Brandes' Algorithm)\n    betweenness_cent = np.zeros(n)\n    for s in range(n):\n        dist = np.full(n, np.inf)\n        sigma = np.zeros(n, dtype=np.float64)\n        pred = [[] for _ in range(n)]\n        \n        dist[s] = 0\n        sigma[s] = 1.0\n        pq = [(0, s)]\n        stack = []\n\n        while pq:\n            d_u, u = heapq.heappop(pq)\n            if d_u > dist[u] + tol:\n                continue\n            stack.append(u)\n            for v, length in adj[u]:\n                new_dist_v = dist[u] + length\n                if new_dist_v < dist[v] - tol:\n                    dist[v] = new_dist_v\n                    sigma[v] = sigma[u]\n                    pred[v] = [u]\n                    heapq.heappush(pq, (new_dist_v, v))\n                elif abs(new_dist_v - dist[v]) <= tol:\n                    sigma[v] += sigma[u]\n                    pred[v].append(u)\n        \n        delta = np.zeros(n)\n        while stack:\n            w = stack.pop()\n            for v_pred in pred[w]:\n                if sigma[w] != 0:\n                    c = sigma[v_pred] / sigma[w]\n                    delta[v_pred] += c * (1 + delta[w])\n            if w != s:\n                betweenness_cent[w] += delta[w]\n\n    # 2.4. Harmonic Closeness Centrality (needs All-Pairs Shortest-Paths)\n    apsp_dists = np.full((n, n), np.inf)\n    for i in range(n):\n        source_dists = apsp_dists[i]\n        source_dists[i] = 0\n        pq = [(0, i)]\n        while pq:\n            d, u = heapq.heappop(pq)\n            if d > source_dists[u] + tol:\n                continue\n            for v, length in adj[u]:\n                if source_dists[v] > source_dists[u] + length + tol:\n                    source_dists[v] = source_dists[u] + length\n                    heapq.heappush(pq, (source_dists[v], v))\n\n    harmonic_cent = np.zeros(n)\n    if n > 1:\n        for i in range(n):\n            sum_inv_dist = 0\n            for j in range(n):\n                if i != j and np.isfinite(apsp_dists[i, j]) and apsp_dists[i, j] > tol:\n                    sum_inv_dist += 1.0 / apsp_dists[i, j]\n            harmonic_cent[i] = sum_inv_dist / (n - 1)\n\n    # 3. Normalize Centralities\n    def normalize(v):\n        min_v, max_v = np.min(v), np.max(v)\n        if abs(min_v - max_v) < tol:\n            return np.zeros_like(v, dtype=float)\n        return (v - min_v) / (max_v - min_v)\n\n    B_norm = normalize(betweenness_cent)\n    C_norm = normalize(harmonic_cent)\n    E_norm = normalize(eigen_cent)\n    D_norm = normalize(degree_cent)\n\n    # 4. Calculate Therapeutic Leverage Index\n    scores = {}\n    alpha, beta, gamma, delta = centrality_weights\n    for i in range(n):\n        centrality_mix = (alpha * B_norm[i] + beta * C_norm[i] + gamma * E_norm[i] + delta * D_norm[i])\n        score = F[i] * centrality_mix * (1 - p[i])\n        scores[i] = score\n\n    # 5. Prioritize Candidate\n    candidates = list(K)\n    if not candidates:\n        return -1 # Should not happen based on problem setup\n\n    candidates.sort(key=lambda i: scores[i], reverse=True)\n    top_score = scores[candidates[0]]\n    tied_on_score = [c for c in candidates if abs(scores[c] - top_score) <= tol]\n    \n    if len(tied_on_score) == 1:\n        return tied_on_score[0]\n    \n    tied_on_score.sort(key=lambda i: E_norm[i], reverse=True)\n    top_ec = E_norm[tied_on_score[0]]\n    tied_on_ec = [c for c in tied_on_score if abs(E_norm[c] - top_ec) <= tol]\n\n    if len(tied_on_ec) == 1:\n        return tied_on_ec[0]\n        \n    tied_on_ec.sort()\n    return tied_on_ec[0]\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and run the solver.\n    \"\"\"\n    test_cases = [\n        {\n            \"n\": 6,\n            \"edges\": [(0, 1, 0.9), (0, 2, 0.6), (1, 2, 0.5), (1, 3, 0.7), \n                      (2, 3, 0.4), (3, 4, 0.8), (4, 5, 0.3), (2, 5, 0.2)],\n            \"K\": {1, 3, 4},\n            \"F\": [1.2, 0.8, 0.5, 1.1, 0.4, 0.9],\n            \"p\": [0.2, 0.1, 0.3, 0.05, 0.4, 0.6],\n            \"weights\": (0.4, 0.3, 0.2, 0.1)\n        },\n        {\n            \"n\": 5,\n            \"edges\": [(0, 1, 0.5), (1, 2, 0.5), (2, 3, 0.5), (3, 4, 0.5), (0, 4, 0.1)],\n            \"K\": {0, 2, 4},\n            \"F\": [0.3, 1.0, 1.5, 0.2, 0.4],\n            \"p\": [0.3, 0.1, 0.2, 0.5, 0.3],\n            \"weights\": (0.25, 0.25, 0.25, 0.25)\n        },\n        {\n            \"n\": 6,\n            \"edges\": [(0, 1, 0.7), (1, 2, 0.6), (0, 2, 0.5), \n                      (3, 4, 0.9), (4, 5, 0.8)],\n            \"K\": {2, 3, 5},\n            \"F\": [0.6, 0.1, 2.0, 0.5, 1.2, 1.4],\n            \"p\": [0.5, 0.5, 0.1, 0.2, 0.3, 0.2],\n            \"weights\": (0.3, 0.4, 0.2, 0.1)\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = _solve_case(\n            case[\"n\"], case[\"edges\"], case[\"K\"], \n            case[\"F\"], case[\"p\"], case[\"weights\"]\n        )\n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```",
            "answer": "[3,2,2]"
        }
    ]
}