{
    "hands_on_practices": [
        {
            "introduction": "This practice explores the concept of \"equity by design,\" demonstrating how quantitative principles can be used proactively to build safety into clinical interventions. By developing an adaptive dosing algorithm , we move beyond simply observing disparities and begin to engineer solutions. This exercise will guide you through using pharmacokinetic models to set dose limits that explicitly protect underrepresented subgroups, turning abstract ethical principles into concrete computational rules.",
            "id": "4987556",
            "problem": "You are tasked with formalizing and implementing an adaptive dosing algorithm for a single oral drug with pharmacokinetics modeled by a one-compartment system with first-order elimination. The problem is framed to address health disparities by explicitly incorporating uncertainty in clearance associated with metabolizer status, and by applying a protective safety margin for underrepresented subgroups. Use purely mathematical derivations and logic based on the following widely accepted foundations.\n\nFundamental base:\n- One-compartment model with first-order elimination: the concentration-time profile after an instantaneous dose is given by $C(t) = \\dfrac{D}{V} e^{-k t}$, where $D$ is dose, $V$ is volume of distribution, and $k$ is the elimination rate constant defined by $k = \\dfrac{CL}{V}$.\n- Allometric scaling of clearance and linear scaling of volume of distribution:\n  - Clearance $CL$ scales as $CL = CL_0 \\cdot m \\cdot \\left(\\dfrac{BM}{70}\\right)^{0.75}$, where $CL_0$ is a baseline clearance constant, $m$ is a multiplicative factor for metabolizer status, and $BM$ is body mass in kilograms.\n  - Volume of distribution $V$ scales linearly as $V = V_0 \\cdot BM$, where $V_0$ is the baseline volume constant.\n- Repeated dosing every $\\tau$ hours achieves steady-state. The maximum (peak) steady-state concentration just after dosing must not exceed a toxicity threshold $C_{\\text{tox}}$ to preserve safety, and the steady-state trough concentration just before the next dose must not fall below an efficacy threshold $C_{\\text{eff}}$ to preserve effectiveness.\n\nHealth disparities and uncertainty:\n- Metabolizer status introduces uncertainty in $m$. For a given status, $m$ lies in an interval $[m_{\\min}, m_{\\max}]$ representing biological variability and limited data, which can be wider in underrepresented subgroups.\n- To preserve safety across underrepresented subgroups, a protective margin factor $\\gamma \\in (0, 1]$ is applied to the safety constraint for those flagged as underrepresented, while $\\gamma = 1$ otherwise.\n\nYour tasks:\n1. From the fundamental base, derive steady-state peak and trough expressions under repeated bolus dosing every $\\tau$ hours, and obtain inequality constraints on $D$ that ensure safety under the slowest elimination (smallest $k$) and efficacy under the fastest elimination (largest $k$).\n2. Construct an adaptive dosing rule $D^\\ast$ that:\n   - Satisfies the safety constraint for the worst-case within the metabolizer interval $[m_{\\min}, m_{\\max}]$, tightened by $\\gamma$ if the patient is underrepresented.\n   - Satisfies the efficacy constraint for the worst-case within $[m_{\\min}, m_{\\max}]$.\n   - If these constraints are incompatible, declare the case infeasible.\n3. Implement a program that computes $D^\\ast$ for the provided test suite and outputs doses in milligrams (mg) as floats with three decimal places. If a case is infeasible, output the integer $-1$ for that case.\n\nConstants (use these values and units exactly):\n- Baseline clearance $CL_0 = $ $3$ L/h.\n- Baseline volume $V_0 = $ $0.7$ L/kg.\n- Toxicity threshold $C_{\\text{tox}} = $ $5$ mg/L.\n- Efficacy threshold $C_{\\text{eff}} = $ $1$ mg/L.\n- Protective margin for underrepresented subgroups $\\gamma_{\\text{UR}} = $ $0.9$; otherwise $\\gamma = $ $1.0$.\n\nMetabolizer status intervals (for $m$):\n- Poor: $[0.5, 0.8]$.\n- Intermediate: $[0.8, 1.0]$.\n- Extensive: $[1.0, 1.2]$.\n- Ultrarapid: $[1.2, 1.6]$.\n\nTest suite (each case is $(BM, \\text{status}, \\tau, \\text{underrepresented flag})$; body mass in kilograms, dosing interval in hours):\n- Case $1$: $(70, \\text{extensive}, 12, \\text{False})$.\n- Case $2$: $(50, \\text{poor}, 12, \\text{True})$.\n- Case $3$: $(120, \\text{ultrarapid}, 12, \\text{False})$.\n- Case $4$: $(30, \\text{intermediate}, 12, \\text{False})$.\n- Case $5$: $(140, \\text{extensive}, 24, \\text{True})$.\n- Case $6$: $(90, \\text{poor}, 8, \\text{False})$.\n\nUnits:\n- Dose $D$ must be reported in mg.\n- Time $\\tau$ is in hours.\n- Concentrations are in mg/L.\n- Clearance is in L/h.\n- Volume is in L.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to a test case in order. Each feasible case must be the recommended dose in mg formatted to three decimal places (for example, $127.645$ mg printed as $127.645$), and each infeasible case must be the integer $-1$.\n- Example format: \"[127.645,51.807,225.523,54.184,-1,64.637]\".",
            "solution": "The problem is subjected to validation.\n\n### Step 1: Extract Givens\n- **Model**: One-compartment model with first-order elimination.\n- **Concentration profile after a single bolus dose $D$**: $C(t) = \\dfrac{D}{V} e^{-k t}$, where $V$ is the volume of distribution and $k$ is the elimination rate constant.\n- **Elimination rate constant**: $k = \\dfrac{CL}{V}$, where $CL$ is clearance.\n- **Clearance scaling**: $CL = CL_0 \\cdot m \\cdot \\left(\\dfrac{BM}{70}\\right)^{0.75}$, where $BM$ is body mass in kg.\n- **Volume of distribution scaling**: $V = V_0 \\cdot BM$.\n- **Repeated dosing interval**: $\\tau$ hours.\n- **Constraints**:\n    - Safety: Maximum steady-state concentration $C_{\\text{ss,max}} \\le \\gamma \\cdot C_{\\text{tox}}$.\n    - Efficacy: Trough steady-state concentration $C_{\\text{ss,min}} \\ge C_{\\text{eff}}$.\n- **Uncertainty**: Metabolizer status factor $m$ is in an interval $[m_{\\min}, m_{\\max}]$.\n- **Protective margin factor $\\gamma$**: $\\gamma = 0.9$ if underrepresented, $\\gamma = 1.0$ otherwise.\n- **Constants**:\n    - Baseline clearance $CL_0 = 3$ L/h.\n    - Baseline volume $V_0 = 0.7$ L/kg.\n    - Toxicity threshold $C_{\\text{tox}} = 5$ mg/L.\n    - Efficacy threshold $C_{\\text{eff}} = 1$ mg/L.\n- **Metabolizer status intervals for $m$**:\n    - Poor: $[0.5, 0.8]$.\n    - Intermediate: $[0.8, 1.0]$.\n    - Extensive: $[1.0, 1.2]$.\n    - Ultrarapid: $[1.2, 1.6]$.\n- **Test suite**:\n    - Case $1$: $(70, \\text{extensive}, 12, \\text{False})$\n    - Case $2$: $(50, \\text{poor}, 12, \\text{True})$\n    - Case $3$: $(120, \\text{ultrarapid}, 12, \\text{False})$\n    - Case $4$: $(30, \\text{intermediate}, 12, \\text{False})$\n    - Case $5$: $(140, \\text{extensive}, 24, \\text{True})$\n    - Case $6$: $(90, \\text{poor}, 8, \\text{False})$\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem utilizes a standard one-compartment pharmacokinetic model with first-order elimination and allometric scaling, which are fundamental and widely accepted principles in pharmacology and physiology.\n- **Well-Posed**: The problem is well-posed. It provides all necessary equations, parameters, and constraints to derive a dosing interval. The existence of a solution depends on the compatibility of these constraints for a given set of parameters, which is part of the problem to be determined (feasibility analysis).\n- **Objective**: All parameters and conditions are defined with mathematical precision. There are no subjective or ambiguous terms.\nThe problem is self-contained, internally consistent, and scientifically sound.\n\n### Step 3: Verdict and Action\nThe problem is valid. A principled solution will be constructed.\n\n### Solution Derivation\n\nThe solution requires a step-by-step derivation of the dosing algorithm from the fundamental principles provided.\n\n**1. Steady-State Concentration Expressions**\n\nFor a regimen of repeated bolus doses $D$ administered every $\\tau$ hours, the concentration accumulates. At steady-state, the concentration profile over one interval is periodic. The concentration just after a dose is the peak ($C_{\\text{ss,max}}$), and the concentration just before the next dose is the trough ($C_{\\text{ss,min}}$).\n\nA dose $D$ increases the concentration by $\\frac{D}{V}$. Over the interval $\\tau$, the concentration decays by a factor of $e^{-k\\tau}$. At steady-state, the concentration just before a dose ($C_{\\text{ss,min}}$) plus the increase from the dose, decayed over one interval, must equal the concentration just before the next dose:\n$$C_{\\text{ss,min}} = \\left(C_{\\text{ss,min}} + \\frac{D}{V}\\right) e^{-k\\tau}$$\nSolving for $C_{\\text{ss,min}}$:\n$$C_{\\text{ss,min}} (1 - e^{-k\\tau}) = \\frac{D}{V} e^{-k\\tau}$$\n$$C_{\\text{ss,min}} = \\frac{D}{V} \\frac{e^{-k\\tau}}{1 - e^{-k\\tau}} = \\frac{D}{V} \\frac{1}{e^{k\\tau} - 1}$$\nThe peak concentration is the trough plus the immediate increase from the dose:\n$$C_{\\text{ss,max}} = C_{\\text{ss,min}} + \\frac{D}{V} = \\frac{D}{V} \\frac{e^{-k\\tau}}{1 - e^{-k\\tau}} + \\frac{D}{V} = \\frac{D}{V} \\left(\\frac{e^{-k\\tau} + 1 - e^{-k\\tau}}{1 - e^{-k\\tau}}\\right)$$\n$$C_{\\text{ss,max}} = \\frac{D}{V} \\frac{1}{1 - e^{-k\\tau}}$$\n\n**2. Worst-Case Constraint Analysis**\n\nThe elimination rate constant $k$ depends on the metabolizer status factor $m$. The factor $m$ varies within an interval $[m_{\\min}, m_{\\max}]$. To guarantee safety and efficacy for any individual within that range, we must consider the worst-case scenarios for both constraints.\n\nFirst, we express $k$ in terms of the given parameters:\n$$k = \\frac{CL}{V} = \\frac{CL_0 \\cdot m \\cdot \\left(\\frac{BM}{70}\\right)^{0.75}}{V_0 \\cdot BM}$$\nSince $k$ is directly proportional to $m$, the minimum and maximum values of $k$ correspond to the minimum and maximum values of $m$:\n$$k_{\\min} = \\frac{CL_0 \\cdot m_{\\min} \\cdot \\left(\\frac{BM}{70}\\right)^{0.75}}{V_0 \\cdot BM}$$\n$$k_{\\max} = \\frac{CL_0 \\cdot m_{\\max} \\cdot \\left(\\frac{BM}{70}\\right)^{0.75}}{V_0 \\cdot BM}$$\n\n**Safety Constraint:**\nThe safety constraint is $C_{\\text{ss,max}} \\le \\gamma \\cdot C_{\\text{tox}}$.\n$$ \\frac{D}{V(1 - e^{-k\\tau})} \\le \\gamma \\cdot C_{\\text{tox}} $$\nThe term $C_{\\text{ss,max}}$ is a decreasing function of $k$. A slower elimination rate (smaller $k$) leads to higher drug accumulation and thus a higher peak concentration. To ensure safety across the entire range of $m$, the inequality must hold for the worst case, which is the smallest possible elimination rate, $k_{\\min}$.\n$$\\frac{D}{V(1 - e^{-k_{\\min}\\tau})} \\le \\gamma \\cdot C_{\\text{tox}}$$\nThis gives an upper bound on the dose $D$:\n$$D \\le \\gamma \\cdot C_{\\text{tox}} \\cdot V \\cdot (1 - e^{-k_{\\min}\\tau})$$\nWe define this upper bound as $D_{\\max}$:\n$$D_{\\max} = \\gamma \\cdot C_{\\text{tox}} \\cdot V \\cdot (1 - e^{-k_{\\min}\\tau})$$\n\n**Efficacy Constraint:**\nThe efficacy constraint is $C_{\\text{ss,min}} \\ge C_{\\text{eff}}$.\n$$\\frac{D}{V(e^{k\\tau} - 1)} \\ge C_{\\text{eff}}$$\nThe term $C_{\\text{ss,min}}$ is also a decreasing function of $k$. A faster elimination rate (larger $k$) leads to lower trough concentrations. To ensure efficacy, the inequality must hold for the worst case for efficacy, which is the largest possible elimination rate, $k_{\\max}$.\n$$\\frac{D}{V(e^{k_{\\max}\\tau} - 1)} \\ge C_{\\text{eff}}$$\nThis gives a lower bound on the dose $D$:\n$$D \\ge C_{\\text{eff}} \\cdot V \\cdot (e^{k_{\\max}\\tau} - 1)$$\nWe define this lower bound as $D_{\\min}$:\n$$D_{\\min} = C_{\\text{eff}} \\cdot V \\cdot (e^{k_{\\max}\\tau} - 1)$$\n\n**3. Adaptive Dosing Rule and Feasibility**\n\nA valid dose $D$ must exist in the interval $[D_{\\min}, D_{\\max}]$. This requires the interval to be non-empty, i.e., $D_{\\min} \\le D_{\\max}$. If $D_{\\min} > D_{\\max}$, no dose can simultaneously satisfy both safety and efficacy constraints for the given uncertainty, and the dosing regimen is deemed infeasible.\n\nThe problem asks for a single adaptive dosing rule, $D^\\ast$. A logical and common clinical choice is to use the highest possible dose that is guaranteed to be safe. This maximizes potential therapeutic benefit within the safety-constrained window. Therefore, we define the adaptive dose $D^\\ast$ as $D_{\\max}$.\n$$D^\\ast = D_{\\max} = \\gamma \\cdot C_{\\text{tox}} \\cdot V \\cdot (1 - e^{-k_{\\min}\\tau})$$\nThis choice is only valid if it also satisfies the efficacy constraint, i.e., $D^\\ast \\ge D_{\\min}$.\nThe condition for a feasible solution is:\n$$D_{\\max} \\ge D_{\\min}$$\nIf this condition holds, the recommended dose is $D^\\ast = D_{\\max}$. If not, the case is infeasible.\n\nThe volume of distribution $V$ is calculated as $V = V_0 \\cdot BM$. Substituting the given constants:\n- $V = 0.7 \\cdot BM$\n- $k = \\frac{3 \\cdot m \\cdot \\left(\\frac{BM}{70}\\right)^{0.75}}{0.7 \\cdot BM}$\nThe algorithm for each test case is as follows:\n1. Determine patient parameters: $BM$, $\\tau$, $m_{\\min}$, $m_{\\max}$, and $\\gamma$.\n2. Calculate $V$.\n3. Calculate $k_{\\min}$ and $k_{\\max}$ using their respective $m$ values.\n4. Calculate $D_{\\max}$ and $D_{\\min}$.\n5. If $D_{\\max} \\ge D_{\\min}$, the result is $D_{\\max}$.\n6. Otherwise, the result is infeasible (represented by $-1$).",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes adaptive drug doses based on a one-compartment pharmacokinetic model,\n    incorporating uncertainty and safety margins for health disparities.\n    \"\"\"\n\n    # --- Define constants and model parameters ---\n    CL0 = 3.0       # Baseline clearance constant (L/h)\n    V0 = 0.7        # Baseline volume constant (L/kg)\n    C_TOX = 5.0     # Toxicity threshold (mg/L)\n    C_EFF = 1.0     # Efficacy threshold (mg/L)\n    GAMMA_UR = 0.9  # Protective margin for underrepresented subgroups\n    GAMMA_DEF = 1.0 # Default protective margin\n\n    # Metabolizer status intervals for the factor 'm'\n    METABOLIZER_STATUS = {\n        \"poor\": (0.5, 0.8),\n        \"intermediate\": (0.8, 1.0),\n        \"extensive\": (1.0, 1.2),\n        \"ultrarapid\": (1.2, 1.6),\n    }\n\n    # --- Define the test suite ---\n    # Each case: (BM (kg), status_key, tau (h), is_underrepresented (bool))\n    test_cases = [\n        (70.0, \"extensive\", 12.0, False),\n        (50.0, \"poor\", 12.0, True),\n        (120.0, \"ultrarapid\", 12.0, False),\n        (30.0, \"intermediate\", 12.0, False),\n        (140.0, \"extensive\", 24.0, True),\n        (90.0, \"poor\", 8.0, False),\n    ]\n\n    results = []\n    for case in test_cases:\n        bm, status_key, tau, is_underrepresented = case\n\n        # --- Set patient-specific parameters ---\n        m_min, m_max = METABOLIZER_STATUS[status_key]\n        gamma = GAMMA_UR if is_underrepresented else GAMMA_DEF\n\n        # --- Calculate pharmacokinetic parameters ---\n        # Volume of distribution (V) in L\n        v = V0 * bm\n        \n        # Elimination rate constant (k) in h^-1\n        # Formula: k = (CL0 * m * (BM/70)**0.75) / (V0 * BM)\n        # We calculate k_min and k_max based on the metabolizer range [m_min, m_max]\n        def calculate_k(m_factor):\n            # Clearance (CL) in L/h\n            cl = CL0 * m_factor * (bm / 70.0)**0.75\n            # Elimination rate constant (k) in h^-1\n            return cl / v\n\n        k_min = calculate_k(m_min)  # Slowest elimination\n        k_max = calculate_k(m_max)  # Fastest elimination\n\n        # --- Apply worst-case analysis for dosing bounds ---\n        \n        # Safety constraint: C_ss_max <= gamma * C_tox\n        # Worst case for safety is slowest elimination (k_min), maximizing accumulation.\n        # This defines the maximum allowable dose, D_max.\n        # D_max = gamma * C_tox * V * (1 - exp(-k_min * tau))\n        d_max = gamma * C_TOX * v * (1.0 - np.exp(-k_min * tau))\n\n        # Efficacy constraint: C_ss_min >= C_eff\n        # Worst case for efficacy is fastest elimination (k_max), minimizing trough levels.\n        # This defines the minimum required dose, D_min.\n        # D_min = C_eff * V * (exp(k_max * tau) - 1)\n        d_min = C_EFF * v * (np.exp(k_max * tau) - 1.0)\n        \n        # --- Determine feasibility and final dose ---\n        # A solution is feasible if the safe dose range is not empty (D_max >= D_min).\n        if d_max >= d_min:\n            # The adaptive dose D* is chosen as the maximum safe dose.\n            recommended_dose = d_max\n            results.append(f\"{recommended_dose:.3f}\")\n        else:\n            # If no dose can satisfy both constraints, the case is infeasible.\n            results.append(\"-1\")\n\n    # --- Format and print the final output ---\n    print(f\"[{','.join(results)}]\")\n\n# Execute the solver\nsolve()\n```"
        },
        {
            "introduction": "A central goal of translational health equity research is to determine not just *if* an intervention works on average, but *for whom* it works and how its effects vary across different populations. This exercise  provides the formal statistical foundation for this analysis by examining heterogeneity of treatment effect (HTE). You will derive how an interaction term in a linear model serves as a direct estimate of HTE, providing a powerful tool for identifying and quantifying health disparities in the context of a randomized trial.",
            "id": "4987570",
            "problem": "A multi-site pragmatic randomized controlled trial (RCT) in translational medicine evaluates a community health navigator program intended to reduce disparities in hypertension control. Let $A \\in \\{0,1\\}$ denote assignment to the navigator program ($A=1$) versus usual care ($A=0$). Let $X \\in \\{0,1\\}$ be a baseline indicator of residing in a high Area Deprivation Index (ADI) neighborhood ($X=1$) versus a low ADI neighborhood ($X=0$), representing a social risk factor relevant to health disparities. Let $Y$ be the 6-month change in systolic blood pressure (in $\\mathrm{mmHg}$), defined so that higher values reflect greater blood pressure reduction.\n\nAdopt the potential outcomes framework with the Stable Unit Treatment Value Assumption (SUTVA), consistency, and positivity. Because of randomization, assume no unmeasured confounding given $X$, that is, $\\{Y(1),Y(0)\\} \\perp A \\mid X$. Suppose the conditional expectation of the observed outcome given treatment and the disparity indicator is linear in all first-order terms permitted by these binary variables.\n\nStarting only from these assumptions and definitions, and without invoking any additional modeling shortcuts, do the following:\n- Derive the most general linear form for $\\mathbb{E}[Y \\mid A, X]$ that includes all main effects and the first-order interaction between $A$ and $X$.\n- Use this derived form, together with the identification result implied by the stated assumptions, to express the conditional average treatment effect $\\tau(x) \\equiv \\mathbb{E}[Y(1)-Y(0) \\mid X=x]$ for $x \\in \\{0,1\\}$.\n- Compute the heterogeneity contrast $\\Delta_{\\mathrm{HTE}} \\equiv \\tau(1)-\\tau(0)$ as a closed-form analytic expression in terms of the coefficients of your derived linear form.\n\nYour final answer must be the single analytic expression for $\\Delta_{\\mathrm{HTE}}$. Do not include units. No rounding is required.",
            "solution": "The problem is first validated against the specified criteria.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   $A \\in \\{0,1\\}$: Assignment to navigator program ($A=1$) vs. usual care ($A=0$).\n-   $X \\in \\{0,1\\}$: Indicator of high Area Deprivation Index (ADI) neighborhood ($X=1$) vs. low ADI neighborhood ($X=0$).\n-   $Y$: 6-month change in systolic blood pressure (in $\\mathrm{mmHg}$), higher values are better.\n-   Potential Outcomes Framework: The existence of potential outcomes $Y(a)$ for $a \\in \\{0,1\\}$ is assumed.\n-   Stable Unit Treatment Value Assumption (SUTVA): This assumption is stated.\n-   Consistency: This assumption is stated, implying $Y = Y(A)$ if an individual received treatment $A$.\n-   Positivity: This assumption is stated, implying $0 < P(A=a|X=x) < 1$ for all $a, x$.\n-   Conditional Independence (Unconfoundedness): $\\{Y(1),Y(0)\\} \\perp A \\mid X$. This is given as a consequence of randomization.\n-   Functional Form Constraint: The conditional expectation $\\mathbb{E}[Y \\mid A, X]$ is linear in all first-order terms permitted by the binary variables $A$ and $X$, including the interaction.\n-   Definition of Conditional Average Treatment Effect (CATE): $\\tau(x) \\equiv \\mathbb{E}[Y(1)-Y(0) \\mid X=x]$ for $x \\in \\{0,1\\}$.\n-   Definition of Heterogeneity Contrast: $\\Delta_{\\mathrm{HTE}} \\equiv \\tau(1)-\\tau(0)$.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientifically Grounded:** The problem uses standard, well-established concepts from causal inference and biostatistics, specifically the potential outcomes framework (Rubin Causal Model), which is the foundation for analyzing randomized trials and observational studies. The concepts of effect modification (heterogeneity of treatment effect), conditional independence, and linear modeling are fundamental to this field. The setup models a realistic scenario in translational health equity research.\n-   **Well-Posed:** The problem provides a clear set of assumptions and definitions and asks for the derivation of a specific quantity ($\\Delta_{\\mathrm{HTE}}$) in terms of the parameters of a model whose form is also to be derived. The problem is self-contained and structured to lead to a unique analytical solution.\n-   **Objective:** The problem is stated using precise, formal, and objective mathematical and statistical language. There are no subjective or opinion-based elements.\n\nThe problem is a standard exercise in applying the principles of causal inference to quantify treatment effect heterogeneity. It is scientifically sound, well-posed, and objective. No flaws are identified.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A full solution will be provided.\n\n### Solution Derivation\n\nThe solution proceeds in three parts as requested by the problem statement.\n\n**Part 1: Derivation of the linear form for $\\mathbb{E}[Y \\mid A, X]$**\n\nThe problem states that the conditional expectation of the outcome $Y$ given the treatment assignment $A$ and the covariate $X$ is linear in all first-order terms permitted by these variables. Since both $A$ and $X$ are binary variables, taking values in $\\{0, 1\\}$, a general linear model for $\\mathbb{E}[Y \\mid A, X]$ includes an intercept term, main effects for $A$ and $X$, and a first-order interaction term, $A \\cdot X$. Let the coefficients for these terms be $\\beta_0, \\beta_1, \\beta_2,$ and $\\beta_3$, respectively. The most general form is therefore:\n$$\n\\mathbb{E}[Y \\mid A, X] = \\beta_0 + \\beta_1 A + \\beta_2 X + \\beta_3 (A \\cdot X)\n$$\nThis model is saturated, meaning it can perfectly represent the mean of $Y$ for each of the four possible combinations of $(A, X)$.\n\n**Part 2: Expression for the conditional average treatment effect $\\tau(x)$**\n\nThe conditional average treatment effect (CATE) for a given level of the covariate $X=x$ is defined as $\\tau(x) \\equiv \\mathbb{E}[Y(1)-Y(0) \\mid X=x]$. By the linearity of expectation, this can be written as:\n$$\n\\tau(x) = \\mathbb{E}[Y(1) \\mid X=x] - \\mathbb{E}[Y(0) \\mid X=x]\n$$\nThe problem states the assumption of conditional unconfoundedness, $\\{Y(1),Y(0)\\} \\perp A \\mid X$. This assumption allows us to identify the conditional means of the potential outcomes from the conditional means of the observed outcomes. Specifically, for any treatment level $a \\in \\{0, 1\\}$ and covariate level $x \\in \\{0, 1\\}$:\n$$\n\\mathbb{E}[Y(a) \\mid X=x] = \\mathbb{E}[Y(a) \\mid A=a, X=x]\n$$\nBy the consistency assumption ($Y=Y(A)$), we have $\\mathbb{E}[Y(a) \\mid A=a, X=x] = \\mathbb{E}[Y \\mid A=a, X=x]$.\nCombining these results gives the key identification formula:\n$$\n\\mathbb{E}[Y(a) \\mid X=x] = \\mathbb{E}[Y \\mid A=a, X=x]\n$$\nWe can now express $\\tau(x)$ using our linear model for the observed data:\n$$\n\\tau(x) = \\mathbb{E}[Y \\mid A=1, X=x] - \\mathbb{E}[Y \\mid A=0, X=x]\n$$\nWe compute this for each level of $x$:\n\nFor $x=0$ (low ADI neighborhood):\n$$\n\\tau(0) = \\mathbb{E}[Y \\mid A=1, X=0] - \\mathbb{E}[Y \\mid A=0, X=0]\n$$\nUsing our linear model:\n$$\n\\mathbb{E}[Y \\mid A=1, X=0] = \\beta_0 + \\beta_1(1) + \\beta_2(0) + \\beta_3(1 \\cdot 0) = \\beta_0 + \\beta_1\n$$\n$$\n\\mathbb{E}[Y \\mid A=0, X=0] = \\beta_0 + \\beta_1(0) + \\beta_2(0) + \\beta_3(0 \\cdot 0) = \\beta_0\n$$\nTherefore, the CATE for the $x=0$ group is:\n$$\n\\tau(0) = (\\beta_0 + \\beta_1) - \\beta_0 = \\beta_1\n$$\n\nFor $x=1$ (high ADI neighborhood):\n$$\n\\tau(1) = \\mathbb{E}[Y \\mid A=1, X=1] - \\mathbb{E}[Y \\mid A=0, X=1]\n$$\nUsing our linear model:\n$$\n\\mathbb{E}[Y \\mid A=1, X=1] = \\beta_0 + \\beta_1(1) + \\beta_2(1) + \\beta_3(1 \\cdot 1) = \\beta_0 + \\beta_1 + \\beta_2 + \\beta_3\n$$\n$$\n\\mathbb{E}[Y \\mid A=0, X=1] = \\beta_0 + \\beta_1(0) + \\beta_2(1) + \\beta_3(0 \\cdot 1) = \\beta_0 + \\beta_2\n$$\nTherefore, the CATE for the $x=1$ group is:\n$$\n\\tau(1) = (\\beta_0 + \\beta_1 + \\beta_2 + \\beta_3) - (\\beta_0 + \\beta_2) = \\beta_1 + \\beta_3\n$$\nThe coefficient $\\beta_1$ represents the average treatment effect in the reference group ($X=0$), and $\\beta_3$ represents the additional effect of the treatment in the $X=1$ group compared to the $X=0$ group.\n\n**Part 3: Computation of the heterogeneity contrast $\\Delta_{\\mathrm{HTE}}$**\n\nThe heterogeneity contrast, $\\Delta_{\\mathrm{HTE}}$, is defined as the difference between the conditional average treatment effects in the two strata of $X$:\n$$\n\\Delta_{\\mathrm{HTE}} \\equiv \\tau(1) - \\tau(0)\n$$\nSubstituting the expressions for $\\tau(1)$ and $\\tau(0)$ derived in Part 2:\n$$\n\\Delta_{\\mathrm{HTE}} = (\\beta_1 + \\beta_3) - (\\beta_1)\n$$\n$$\n\\Delta_{\\mathrm{HTE}} = \\beta_3\n$$\nThis result shows that the heterogeneity contrast, which measures how the treatment effect differs by ADI status, is captured precisely by the interaction coefficient $\\beta_3$ in the linear model.",
            "answer": "$$\\boxed{\\beta_3}$$"
        },
        {
            "introduction": "Translational research often relies on real-world data from healthcare systems, but unequal access to care can introduce significant selection bias, distorting our understanding of disease risk and health disparities. This practice  tackles this challenge head-on by exploring how differential referral patterns can bias study results. You will learn to apply inverse probability of selection weighting (IPSW), a key causal inference method, to correct for this bias and recover a more accurate estimate of a causal effect from biased case-control data.",
            "id": "4987625",
            "problem": "A translational medicine team is evaluating how unequal referral patterns by race affect estimation of the causal effect of a biomarker positivity on a disease in a case-control study drawn from specialty clinics. Let $E \\in \\{0,1\\}$ denote exposure (biomarker negative or positive), $D \\in \\{0,1\\}$ denote disease status (no disease or disease), $R \\in \\{\\text{African American}, \\text{White}\\}$ denote race, and $S \\in \\{0,1\\}$ denote selection into the clinic case-control sample. The target causal parameter is the causal risk ratio $\\mathrm{RR} = \\Pr(D=1 \\mid \\operatorname{do}(E=1)) / \\Pr(D=1 \\mid \\operatorname{do}(E=0))$ in the source population.\n\nThe following information is available and scientifically justified:\n- Clinic referral (selection) probabilities, estimated from health system data, satisfy $S \\perp E \\mid (D,R)$ and are:\n  $\\Pr(S=1 \\mid D=1, R=\\text{African American}) = 0.12$, $\\Pr(S=1 \\mid D=1, R=\\text{White}) = 0.20$, $\\Pr(S=1 \\mid D=0, R=\\text{African American}) = 0.06$, $\\Pr(S=1 \\mid D=0, R=\\text{White}) = 0.10$.\n- The observed case-control counts in the clinic sample are:\n  among cases ($D=1$): $E=1$ with $R=\\text{African American}$: $80$; $E=1$ with $R=\\text{White}$: $120$; $E=0$ with $R=\\text{African American}$: $40$; $E=0$ with $R=\\text{White}$: $60$.\n  among controls ($D=0$): $E=1$ with $R=\\text{African American}$: $50$; $E=1$ with $R=\\text{White}$: $100$; $E=0$ with $R=\\text{African American}$: $130$; $E=0$ with $R=\\text{White}$: $180$.\n\nStarting from core definitions of risk, odds, and selection, and assuming exchangeability conditional on race ($D(e) \\perp E \\mid R$ for $e \\in \\{0,1\\}$), consistency, and positivity, derive conditions under which the odds ratio among selected individuals, $\\mathrm{OR}_{S=1}$, is a biased estimate of the causal risk ratio in the population. Then, propose and apply an inverse probability of selection weighting strategy to correct the bias and recover an estimate of the causal risk ratio from the selected data.\n\nCompute the inverse-probability-of-selection-weighted risk ratio using weights $w(d,r) = 1/\\Pr(S=1 \\mid D=d, R=r)$ applied to the provided counts, treating weights as known. Express the final answer as a single real number. Round your answer to four significant figures.",
            "solution": "The user-provided problem has been assessed and is determined to be valid. It is scientifically grounded in the principles of causal inference and epidemiology, well-posed with sufficient and consistent data, and objective in its formulation. We may therefore proceed with a formal solution.\n\nThe problem requires us to first understand the conditions under which the observed odds ratio in a selected sample becomes a biased estimator of the causal risk ratio in the source population, and then to apply a correction method to obtain an unbiased estimate.\n\nFirst, let us define the causal parameter of interest, the causal risk ratio ($RR$):\n$$\n\\mathrm{RR} = \\frac{\\Pr(D=1 \\mid \\operatorname{do}(E=1))}{\\Pr(D=1 \\mid \\operatorname{do}(E=0))}\n$$\nHere, $D(e)$ represents the potential outcome for disease status had an individual, possibly contrary to fact, received exposure level $E=e$. The expression $\\Pr(D=1 \\mid \\operatorname{do}(E=e))$ is shorthand for $\\Pr(D(e)=1)$.\n\nThe problem provides the key assumption of exchangeability conditional on race ($R$): $D(e) \\perp E \\mid R$. Along with consistency ($D=D(E)$) and positivity, this assumption allows us to identify the causal risks with conditional probabilities in the source population, standardized over the distribution of race:\n$$\n\\Pr(D(e)=1) = \\sum_{r \\in \\{\\text{AA}, \\text{W}\\}} \\Pr(D(e)=1 \\mid R=r) \\Pr(R=r)\n$$\nBy exchangeability, $\\Pr(D(e)=1 \\mid R=r) = \\Pr(D(e)=1 \\mid E=e, R=r)$, and by consistency, this equals $\\Pr(D=1 \\mid E=e, R=r)$. Therefore, the causal risk ratio is equivalent to the standardized risk ratio in the source population:\n$$\n\\mathrm{RR} = \\frac{\\sum_{r} \\Pr(D=1 \\mid E=1, R=r) \\Pr(R=r)}{\\sum_{r} \\Pr(D=1 \\mid E=0, R=r) \\Pr(R=r)}\n$$\nThe data, however, are from a case-control sample drawn from specialty clinics ($S=1$). In such a study, one typically calculates an odds ratio. The odds ratio in the selected sample, $\\mathrm{OR}_{S=1}$, is given by:\n$$\n\\mathrm{OR}_{S=1} = \\frac{\\Pr(E=1|D=1,S=1)/\\Pr(E=0|D=1,S=1)}{\\Pr(E=1|D=0,S=1)/\\Pr(E=0|D=0,S=1)}\n$$\nThis measure is a biased estimate of the target causal risk ratio (or even the population odds ratio) due to selection bias. The bias arises because selection into the sample ($S=1$) is dependent on both disease status ($D$) and race ($R$). The provided conditional independence statement, $S \\perp E \\mid (D,R)$, indicates that within strata of disease and race, exposure status does not influence selection. However, the selection probabilities $\\Pr(S=1 \\mid D=d, R=r)$ are not uniform across these strata. Specifically, $\\Pr(S=1 \\mid D,R)$ differs by both $D$ and $R$. If race $R$ is also associated with the exposure $E$ in the population (a common scenario in health disparities research, where $R$ can be a common cause of both exposure patterns and disease risk), then conditioning on selection $S$ induces a spurious association between $E$ and $D$. This is a form of collider stratification bias, where $S$ is a collider (or descendant of a collider) on a non-causal path between $E$ and $D$. The condition for bias is precisely this differential selection based on joint values of $D$ and a common cause of $E$ and $D$ (like $R$).\n\nTo correct this bias, we use an inverse probability of selection weighting (IPSW) strategy. Each observation in the selected sample is weighted by the inverse of its probability of being selected. The weight for an individual with disease status $d$ and race $r$ is:\n$$\nw(d,r) = \\frac{1}{\\Pr(S=1 \\mid D=d, R=r)}\n$$\nApplying these weights to the observed counts $N_{edr}$ from the sample ($S=1$) allows us to reconstruct a pseudo-population whose composition is, in expectation, proportional to that of the original source population. The weighted count for each stratum is $N^*_{edr} = N_{edr} \\times w(d,r)$. We can then calculate the risk ratio directly from this pseudo-population.\n\nThe given selection probabilities and their corresponding weights are:\nLet $r_1$ denote African American and $r_2$ denote White.\n-   $\\Pr(S=1 \\mid D=1, R=r_1) = 0.12 \\implies w(1, r_1) = 1/0.12$\n-   $\\Pr(S=1 \\mid D=1, R=r_2) = 0.20 \\implies w(1, r_2) = 1/0.20 = 5$\n-   $\\Pr(S=1 \\mid D=0, R=r_1) = 0.06 \\implies w(0, r_1) = 1/0.06$\n-   $\\Pr(S=1 \\mid D=0, R=r_2) = 0.10 \\implies w(0, r_2) = 1/0.10 = 10$\n\nThe observed counts ($N_{edr}$) in the clinic sample ($S=1$) are:\n-   $N_{11r_1} = 80$ ($E=1, D=1, R=r_1$)\n-   $N_{11r_2} = 120$ ($E=1, D=1, R=r_2$)\n-   $N_{01r_1} = 40$ ($E=0, D=1, R=r_1$)\n-   $N_{01r_2} = 60$ ($E=0, D=1, R=r_2$)\n-   $N_{10r_1} = 50$ ($E=1, D=0, R=r_1$)\n-   $N_{10r_2} = 100$ ($E=1, D=0, R=r_2$)\n-   $N_{00r_1} = 130$ ($E=0, D=0, R=r_1$)\n-   $N_{00r_2} = 180$ ($E=0, D=0, R=r_2$)\n\nWe now compute the weighted counts ($N^*_{edr}$) for the pseudo-population:\n-   $N^*_{11r_1} = 80 \\times (1/0.12) = 8000/12 = 2000/3$\n-   $N^*_{11r_2} = 120 \\times 5 = 600$\n-   $N^*_{01r_1} = 40 \\times (1/0.12) = 4000/12 = 1000/3$\n-   $N^*_{01r_2} = 60 \\times 5 = 300$\n-   $N^*_{10r_1} = 50 \\times (1/0.06) = 5000/6 = 2500/3$\n-   $N^*_{10r_2} = 100 \\times 10 = 1000$\n-   $N^*_{00r_1} = 130 \\times (1/0.06) = 13000/6 = 6500/3$\n-   $N^*_{00r_2} = 180 \\times 10 = 1800$\n\nNext, we aggregate these weighted counts to estimate the total numbers of cases and non-cases for each exposure level in the source population.\nTotal weighted cases with exposure $E=1$:\n$$N^*_{11} = N^*_{11r_1} + N^*_{11r_2} = \\frac{2000}{3} + 600 = \\frac{2000+1800}{3} = \\frac{3800}{3}$$\nTotal weighted non-cases (controls) with exposure $E=1$:\n$$N^*_{10} = N^*_{10r_1} + N^*_{10r_2} = \\frac{2500}{3} + 1000 = \\frac{2500+3000}{3} = \\frac{5500}{3}$$\nTotal weighted cases with exposure $E=0$:\n$$N^*_{01} = N^*_{01r_1} + N^*_{01r_2} = \\frac{1000}{3} + 300 = \\frac{1000+900}{3} = \\frac{1900}{3}$$\nTotal weighted non-cases (controls) with exposure $E=0$:\n$$N^*_{00} = N^*_{00r_1} + N^*_{00r_2} = \\frac{6500}{3} + 1800 = \\frac{6500+5400}{3} = \\frac{11900}{3}$$\n\nNow, we calculate the estimated risks for each exposure group from the pseudo-population counts.\nTotal weighted individuals with $E=1$:\n$$N^*_{1 \\cdot} = N^*_{11} + N^*_{10} = \\frac{3800}{3} + \\frac{5500}{3} = \\frac{9300}{3} = 3100$$\nTotal weighted individuals with $E=0$:\n$$N^*_{0 \\cdot} = N^*_{01} + N^*_{00} = \\frac{1900}{3} + \\frac{11900}{3} = \\frac{13800}{3} = 4600$$\n\nThe estimated risk in the exposed group is:\n$$\\hat{\\Pr}(D=1 \\mid E=1) = \\frac{N^*_{11}}{N^*_{1 \\cdot}} = \\frac{3800/3}{3100} = \\frac{3800}{3 \\times 3100} = \\frac{38}{93}$$\nThe estimated risk in the unexposed group is:\n$$\\hat{\\Pr}(D=1 \\mid E=0) = \\frac{N^*_{01}}{N^*_{0 \\cdot}} = \\frac{1900/3}{4600} = \\frac{1900}{3 \\times 4600} = \\frac{19}{138}$$\nFinally, the IPSW-corrected causal risk ratio estimate is the ratio of these two risks:\n$$\\widehat{\\mathrm{RR}}_{\\mathrm{IPSW}} = \\frac{\\hat{\\Pr}(D=1 \\mid E=1)}{\\hat{\\Pr}(D=1 \\mid E=0)} = \\frac{38/93}{19/138} = \\frac{38}{93} \\times \\frac{138}{19} = \\frac{2 \\times 19}{3 \\times 31} \\times \\frac{3 \\times 46}{19} = \\frac{2 \\times 46}{31} = \\frac{92}{31}$$\nTo obtain the final numerical answer, we compute the value of this fraction and round to four significant figures:\n$$\\frac{92}{31} \\approx 2.9677419...$$\nRounding to four significant figures yields $2.968$.",
            "answer": "$$\n\\boxed{2.968}\n$$"
        }
    ]
}