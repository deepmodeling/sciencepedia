{
    "hands_on_practices": [
        {
            "introduction": "The first critical step in designing any pivotal trial is determining the required number of participants. This exercise guides you through deriving the sample size formula for a common binary endpoint from first principles, connecting the fundamental concepts of Type I and Type II errors to a concrete, practical calculation . Mastering this skill is essential for ensuring a trial is both ethically and scientifically sound, with adequate power to detect a clinically meaningful effect.",
            "id": "5044717",
            "problem": "A sponsor is designing a Phase III pivotal trial in translational medicine to evaluate a new analgesic in patients with chronic osteoarthritis pain. The primary endpoint is a responder analysis: a patient is defined as a responder if, at week $12$, the Numeric Pain Rating Scale (NPRS, $10$-point scale) improves by at least the Minimal Clinically Important Difference (MCID) of $2$ points. Based on Phase II data under similar inclusion criteria, the control arm responder probability is anticipated to be $p_{C} = 0.35$, while the treatment arm responder probability is anticipated to be $p_{T} = 0.50$. The clinically meaningful between-arm improvement to be detected is the difference $\\Delta = p_{T} - p_{C}$ implied by these planning values.\n\nThe trial will use a one-sided superiority hypothesis test at significance level $\\alpha = 0.025$ with power $1 - \\beta = 0.9$, and $1:1$ allocation between treatment and control. Assume the following standard large-sample framework: individual responses are independent and identically distributed within arms; the arm-level responder counts are binomial; and the difference in sample proportions is approximated by a normal distribution via the Central Limit Theorem (CLT). The test is a Wald-type $z$-test using the pooled variance under the null hypothesis $H_{0}: p_{T} - p_{C} \\leq 0$ and the unpooled variance under the alternative hypothesis $H_{1}: p_{T} - p_{C} > 0$. Let $\\Phi$ denote the cumulative distribution function (CDF) of the standard normal distribution, and write $z_{\\gamma} = \\Phi^{-1}(\\gamma)$ for the corresponding quantile.\n\nDerive from first principles the planning equation for the required equal per-arm sample size $n$ that achieves the specified $\\alpha$ and $1-\\beta$, starting from the binomial model, the CLT, and the definition of Type I and Type II error. Then, using the stated planning values and quantiles for $\\alpha$ and $1-\\beta$, compute the continuous-valued per-arm sample size. Express your final answer as a single real number representing the required per-arm sample size, rounded to four significant figures. No continuity correction or inflation for missing data is to be applied in this calculation.",
            "solution": "Let $n$ be the equal sample size per arm. Let $\\hat{p}_T$ and $\\hat{p}_C$ be the observed sample proportions of responders in the treatment and control arms. The difference in proportions is $\\hat{D} = \\hat{p}_T - \\hat{p}_C$. We are testing the one-sided superiority hypotheses:\n$$H_0: p_T - p_C \\le 0 \\quad \\text{vs.} \\quad H_1: p_T - p_C > 0$$\n      \nThe test is rejected if the observed difference $\\hat{D}$ exceeds a critical value $D_{crit}$. The Type I error rate $\\alpha$ is the probability of rejection under the null hypothesis boundary ($p_T = p_C$). The problem specifies using a pooled variance for the test statistic. We plan under a pooled probability $\\bar{p} = (p_T + p_C)/2$. The variance of the difference under the null is $\\text{Var}(\\hat{D}|H_0) = \\frac{2\\bar{p}(1-\\bar{p})}{n}$. The critical value is determined by the condition:\n$$ \\mathbb{P}(\\hat{D} > D_{crit} | H_0) = \\alpha $$\nStandardizing under the null, we get $\\frac{D_{crit}}{\\sqrt{2\\bar{p}(1-\\bar{p})/n}} = z_{1-\\alpha}$. This gives the first expression for the critical value:\n$$ D_{crit} = z_{1-\\alpha} \\sqrt{\\frac{2\\bar{p}(1-\\bar{p})}{n}} $$\n\nPower, $1-\\beta$, is the probability of rejection under the specific alternative hypothesis where the true probabilities are $p_T$ and $p_C$. The variance of the difference under the alternative is $\\text{Var}(\\hat{D}|H_1) = \\frac{p_T(1-p_T)}{n} + \\frac{p_C(1-p_C)}{n}$. The power condition is:\n$$ \\mathbb{P}(\\hat{D} > D_{crit} | H_1) = 1-\\beta $$\nStandardizing under the alternative, with true difference $\\Delta = p_T - p_C$, gives:\n$$ \\mathbb{P}\\left(\\frac{\\hat{D} - \\Delta}{\\sqrt{\\text{Var}(\\hat{D}|H_1)}} > \\frac{D_{crit} - \\Delta}{\\sqrt{\\text{Var}(\\hat{D}|H_1)}}\\right) = 1-\\beta $$\nThis implies $\\frac{D_{crit} - \\Delta}{\\sqrt{\\text{Var}(\\hat{D}|H_1)}} = z_{\\beta} = -z_{1-\\beta}$. This gives the second expression for the critical value:\n$$ D_{crit} = \\Delta - z_{1-\\beta} \\sqrt{\\frac{p_T(1-p_T) + p_C(1-p_C)}{n}} $$\n\nEquating the two expressions for $D_{crit}$ and solving for $n$ yields the sample size formula:\n$$ z_{1-\\alpha} \\sqrt{\\frac{2\\bar{p}(1-\\bar{p})}{n}} = \\Delta - z_{1-\\beta} \\sqrt{\\frac{p_T(1-p_T) + p_C(1-p_C)}{n}} $$\n$$ \\sqrt{n}\\Delta = z_{1-\\alpha} \\sqrt{2\\bar{p}(1-\\bar{p})} + z_{1-\\beta} \\sqrt{p_T(1-p_T) + p_C(1-p_C)} $$\n$$ n = \\frac{\\left( z_{1-\\alpha} \\sqrt{2\\bar{p}(1-\\bar{p})} + z_{1-\\beta} \\sqrt{p_T(1-p_T) + p_C(1-p_C)} \\right)^2}{\\Delta^2} $$\n\nNow, we substitute the planning values:\n-   $p_C = 0.35$, $p_T = 0.50$, so $\\Delta = 0.15$.\n-   $\\bar{p} = (0.35 + 0.50)/2 = 0.425$.\n-   One-sided $\\alpha = 0.025 \\implies z_{1-\\alpha} = z_{0.975} \\approx 1.95996$.\n-   Power $1-\\beta = 0.90 \\implies \\beta = 0.10 \\implies z_{1-\\beta} = z_{0.90} \\approx 1.28155$.\n-   Variance term under null: $2\\bar{p}(1-\\bar{p}) = 2(0.425)(0.575) = 0.48875$.\n-   Variance term under alternative: $p_T(1-p_T) + p_C(1-p_C) = 0.50(0.50) + 0.35(0.65) = 0.25 + 0.2275 = 0.4775$.\n\nPlugging these into the formula:\n$$ n = \\frac{\\left( 1.95996 \\sqrt{0.48875} + 1.28155 \\sqrt{0.4775} \\right)^2}{(0.15)^2} $$\n$$ n = \\frac{\\left( 1.95996 \\times 0.699107 + 1.28155 \\times 0.691014 \\right)^2}{0.0225} $$\n$$ n = \\frac{\\left( 1.37022 + 0.88554 \\right)^2}{0.0225} = \\frac{(2.25576)^2}{0.0225} \\approx \\frac{5.08846}{0.0225} \\approx 226.154 $$\n\nRounding to four significant figures, the required per-arm sample size is $226.2$.",
            "answer": "$$\\boxed{226.2}$$"
        },
        {
            "introduction": "While the proportional hazards assumption is convenient, it is often violated in practice, particularly when treatments have a delayed effect or early toxicity. This problem challenges you to reason about a scenario with non-proportional hazards, exploring why a single hazard ratio can be misleading and how alternative measures like Restricted Mean Survival Time (RMST) provide a more robust interpretation . This practice is crucial for correctly designing and interpreting modern survival trials where treatment effects may evolve over time.",
            "id": "5044569",
            "problem": "A Phase III pivotal superiority trial in metastatic disease is designed with overall survival as the primary endpoint. The control arm has a constant hazard of death of $0.05$ per month over time $t \\ge 0$. The experimental arm is hypothesized to exhibit delayed clinical effect with early toxicity: its hazard of death is $0.10$ per month for $0 \\le t \\le 6$ and $0.02$ per month for $t > 6$. Consider the standard Cox proportional hazards model analysis and the log-rank test, both of which presuppose time-constant hazard ratio. Investigators debate whether the hazard ratio (HR) is interpretable when hazards are not proportional and whether an alternative estimand, such as the restricted mean survival time (RMST), should be pre-specified.\n\nUsing only fundamental definitions of the hazard function, the survival function, and the meaning of proportional hazards, reason from first principles about this scenario. Without assuming any proportional hazards property that is not implied by the piecewise hazards above, determine which of the following statements are correct.\n\nA. In this setting, the instantaneous hazard ratio is $2$ for $0 \\le t \\le 6$ and $0.4$ for $t > 6$. Therefore, a single Cox model hazard ratio cannot represent a constant relative effect; the Cox estimate will reflect a weighted average dominated by periods with more events and can be greater than $1$ even if the late hazard is lower on treatment.\n\nB. The Cox model partial likelihood estimate of the hazard ratio equals the ratio of cumulative hazards up to any fixed time $\\tau$, that is, $\\widehat{\\mathrm{HR}} = \\Lambda_{\\text{treat}}(\\tau) / \\Lambda_{\\text{control}}(\\tau)$, providing a valid estimand under non-proportional hazards.\n\nC. The difference in restricted mean survival time up to $\\tau = 24$ months is approximately $-1.17$ months (experimental minus control), whereas up to $\\tau = 36$ months it is approximately $+0.20$ months, showing that RMST does not assume proportional hazards and that its interpretation depends on the truncation time $\\tau$.\n\nD. Stratifying the Cox model by randomization strata restores proportional hazards and makes the hazard ratio interpretable as a constant effect even when the true arm-specific hazards cross over time.\n\nE. A prespecified weighted log-rank test, for example with Fleming–Harrington late-event weighting, or a MaxCombo procedure, can increase power for delayed treatment effects relative to the standard log-rank test, but the corresponding effect-size estimand should be prespecified, such as a difference in RMST or a milestone survival probability.\n\nSelect all that apply.",
            "solution": "We begin by deriving the necessary survival analysis functions from the provided hazard rates. Let $h_C(t)$ and $h_E(t)$ be the hazard functions, $S_C(t)$ and $S_E(t)$ the survival functions, and $RMST(\\tau)$ the restricted mean survival time up to time $\\tau$.\n\n**Control Arm:**\n-   Hazard: $h_C(t) = 0.05$ for $t \\ge 0$.\n-   Cumulative Hazard: $\\Lambda_C(t) = \\int_0^t 0.05 du = 0.05t$.\n-   Survival: $S_C(t) = e^{-0.05t}$.\n\n**Experimental Arm:**\n-   Hazard: $h_E(t) = 0.10$ for $0 \\le t \\le 6$ and $h_E(t) = 0.02$ for $t > 6$.\n-   Cumulative Hazard: \n    -   For $0 \\le t \\le 6$: $\\Lambda_E(t) = 0.10t$.\n    -   For $t > 6$: $\\Lambda_E(t) = \\Lambda_E(6) + \\int_6^t 0.02 du = 0.6 + 0.02(t-6) = 0.48 + 0.02t$.\n-   Survival:\n    -   For $0 \\le t \\le 6$: $S_E(t) = e^{-0.10t}$.\n    -   For $t > 6$: $S_E(t) = e^{-(0.48 + 0.02t)}$.\n\nNow we evaluate each statement:\n\n**A. Correct.** The instantaneous hazard ratio, $HR(t) = h_E(t) / h_C(t)$, is $0.10 / 0.05 = 2.0$ for $t \\le 6$ and $0.02 / 0.05 = 0.4$ for $t > 6$. Since the hazard ratio is not constant, the proportional hazards (PH) assumption is violated. The single HR estimated by a Cox model is a weighted average of the time-varying log-HR, with weights determined by the number of events. Since the early hazard is high, many events will occur when the HR is 2.0, heavily influencing the estimate to be greater than 1, even though the long-term effect is beneficial.\n\n**B. Incorrect.** The Cox model produces a single estimate for the HR. The proposed quantity, the ratio of cumulative hazards $\\Lambda_E(\\tau) / \\Lambda_C(\\tau)$, is a function of time $\\tau$ and is not constant (e.g., at $\\tau=6$, it is 2.0; at $\\tau=24$, it is $(0.48 + 0.02 \\times 24)/(0.05 \\times 24) = 0.8$). The Cox estimate does not equal this time-dependent quantity.\n\n**C. Correct.** We calculate the RMST, defined as $\\int_0^\\tau S(t) dt$.\n-   $RMST_C(\\tau) = \\int_0^\\tau e^{-0.05t} dt = 20(1 - e^{-0.05\\tau})$.\n    -   $RMST_C(24) \\approx 13.976$ months.\n    -   $RMST_C(36) \\approx 16.694$ months.\n-   $RMST_E(\\tau) = \\int_0^6 S_E(t) dt + \\int_6^\\tau S_E(t) dt$ for $\\tau > 6$.\n    -   $RMST_E(24) = \\int_0^6 e^{-0.1t} dt + \\int_6^{24} e^{-(0.48+0.02t)} dt \\approx 4.512 + 8.296 = 12.808$ months.\n    -   $RMST_E(36) = \\int_0^6 e^{-0.1t} dt + \\int_6^{36} e^{-(0.48+0.02t)} dt \\approx 4.512 + 12.381 = 16.893$ months.\n-   The difference in RMST (Experimental - Control) is:\n    -   $\\Delta RMST(24) = 12.808 - 13.976 \\approx -1.17$ months.\n    -   $\\Delta RMST(36) = 16.893 - 16.694 \\approx +0.20$ months.\nThe calculations are correct. This demonstrates that RMST is calculable under non-proportional hazards and its value and interpretation depend strongly on the chosen time horizon $\\tau$.\n\n**D. Incorrect.** Stratification in a Cox model allows the baseline hazard to differ between strata but assumes a common, constant hazard ratio for the treatment effect *across time* within each stratum. It cannot fix non-proportionality of the treatment effect itself.\n\n**E. Correct.** The standard log-rank test is optimal under PH. For delayed effects (a form of non-PH), its power is reduced. Weighted log-rank tests (like Fleming-Harrington with late-weighting) or robust tests like MaxCombo are specifically designed to improve power in such scenarios. When using these tests, it is best practice to pre-specify an estimand that is interpretable under non-PH, such as the difference in RMST or a difference in milestone survival rates, as the test statistic itself doesn't correspond to a simple effect measure like an HR.",
            "answer": "$$\\boxed{ACE}$$"
        },
        {
            "introduction": "Missing data is an unavoidable reality in clinical trials, and regulatory agencies require a thorough assessment of its potential impact on study conclusions. This exercise introduces the 'tipping point' sensitivity analysis, a powerful tool for quantifying how much the primary Missing At Random (MAR) assumption must be violated to change the trial's outcome . By working through this calculation, you will gain hands-on experience in evaluating the robustness of trial results in the face of uncertainty.",
            "id": "5044646",
            "problem": "A Phase III superiority randomized controlled trial (RCT) compares an investigational therapy to control on a continuous primary endpoint at Week $24$. The primary analysis is prespecified as an intention-to-treat comparison of marginal arm means using a likelihood-based model that is valid under the Missing At Random (MAR) assumption. The following are observed from the primary analysis: the estimated treatment effect under MAR, denoted $\\hat{\\tau}^{\\mathrm{MAR}}$, is $\\hat{\\tau}^{\\mathrm{MAR}} = 0.30$ units with a large-sample standard error $s = 0.12$, and the two-sided significance level is $\\alpha = 0.05$. The proportions of missing primary outcomes are $\\pi_T = 0.10$ in the treatment arm and $\\pi_C = 0.05$ in the control arm.\n\nTo assess robustness to violations of MAR, the team plans a pattern-mixture model sensitivity analysis under Missing Not At Random (MNAR), using a symmetric $\\delta$-adjusted imputation scheme that is explicitly unfavorable to the investigational therapy: for missing outcomes in the treatment arm, imputed values are shifted downward by $\\delta$ relative to the MAR-based imputation, and for missing outcomes in the control arm, imputed values are shifted upward by $\\delta$ relative to the MAR-based imputation. Assume, for the purpose of this sensitivity analysis and in line with large-sample practice, that the standard error $s$ is approximately unchanged by small to moderate $\\delta$ shifts, and that two-sided $100(1-\\alpha)\\%$ confidence intervals are used to judge statistical significance.\n\nBased on first principles about missing data mechanisms—Missing Completely At Random (MCAR), Missing At Random (MAR), and Missing Not At Random (MNAR)—and mixture representations of marginal means within each arm, which option both correctly defines a tipping point analysis in this Phase III context and correctly identifies, under the described symmetric $\\delta$ adjustment, the minimal value of $\\delta$ (in endpoint units) that would lead to loss of statistical significance at the two-sided $5\\%$ level for the treatment effect estimate?\n\nA. A tipping point analysis is a systematic exploration over a grid of departures from MAR that maps combinations of MNAR shift parameters to qualitative conclusions (e.g., significant versus not significant), thereby identifying the boundary at which the conclusion “tips.” Under the described symmetric, unfavorable-to-treatment scheme, the smallest $\\delta$ that nullifies statistical significance is approximately $0.432$.\n\nB. A tipping point analysis determines the most likely MNAR mechanism from the data alone; here, because more data are missing in the treatment arm, only the difference in missingness proportions matters, so the minimal $\\delta$ that removes significance is approximately $1.296$.\n\nC. A tipping point analysis varies the number of imputations until results stabilize; under the symmetric $\\delta$ adjustment, using a one-sided $5\\%$ critical value is appropriate for superiority, yielding a minimal $\\delta$ of approximately $0.688$.\n\nD. A tipping point analysis assumes Missing Completely At Random, so the point estimate does not change with $\\delta$; thus the minimal $\\delta$ that would overturn statistical significance is $0$.\n\nE. A tipping point analysis is a pre-specified check that the result is robust if at least $50$ multiple imputations are used; because $50$ exceeds common practice, no $\\delta$ can change the conclusion, so the minimal $\\delta$ is undefined.",
            "solution": "First, we verify the initial claim of statistical significance from the primary analysis under the Missing At Random (MAR) assumption. The estimated treatment effect is $\\hat{\\tau}^{\\mathrm{MAR}} = 0.30$ with a standard error $s = 0.12$. The lower bound of the two-sided $95\\%$ confidence interval (CI) is:\n$$ \\text{CI}_{\\text{lower}} = \\hat{\\tau}^{\\mathrm{MAR}} - z_{0.975} \\cdot s = 0.30 - 1.96 \\times 0.12 = 0.30 - 0.2352 = 0.0648 $$\nSince the lower bound is greater than $0$, the result is statistically significant at the $\\alpha = 0.05$ level.\n\nNext, we evaluate the impact of the specified Missing Not At Random (MNAR) sensitivity analysis. A tipping point analysis systematically explores departures from the MAR assumption to find the point at which the study's conclusion changes. Here, the conclusion \"tips\" when the treatment effect is no longer statistically significant.\n\nThe proposed analysis uses a symmetric, unfavorable shift parameter $\\delta$. The MNAR-adjusted marginal mean in each arm ($\\hat{\\mu}^{\\mathrm{MNAR}}$) can be expressed as a function of the MAR-based mean ($\\hat{\\mu}^{\\mathrm{MAR}}$) and the proportion of missing data ($\\pi$):\n-   Treatment Arm: $\\hat{\\mu}_T^{\\mathrm{MNAR}}(\\delta) = (1-\\pi_T)\\hat{\\mu}_T^{\\mathrm{MAR}} + \\pi_T(\\hat{\\mu}_T^{\\mathrm{MAR}} - \\delta) = \\hat{\\mu}_T^{\\mathrm{MAR}} - \\pi_T \\delta$\n-   Control Arm: $\\hat{\\mu}_C^{\\mathrm{MNAR}}(\\delta) = (1-\\pi_C)\\hat{\\mu}_C^{\\mathrm{MAR}} + \\pi_C(\\hat{\\mu}_C^{\\mathrm{MAR}} + \\delta) = \\hat{\\mu}_C^{\\mathrm{MAR}} + \\pi_C \\delta$\n      \nThe MNAR-adjusted treatment effect, $\\hat{\\tau}^{\\mathrm{MNAR}}(\\delta)$, is the difference between these adjusted means:\n$$ \\hat{\\tau}^{\\mathrm{MNAR}}(\\delta) = \\hat{\\mu}_T^{\\mathrm{MNAR}}(\\delta) - \\hat{\\mu}_C^{\\mathrm{MNAR}}(\\delta) = (\\hat{\\mu}_T^{\\mathrm{MAR}} - \\hat{\\mu}_C^{\\mathrm{MAR}}) - (\\pi_T + \\pi_C)\\delta $$\n$$ \\hat{\\tau}^{\\mathrm{MNAR}}(\\delta) = \\hat{\\tau}^{\\mathrm{MAR}} - (\\pi_T + \\pi_C)\\delta $$\n      \nThe tipping point is the value of $\\delta$ where the result just ceases to be significant. This occurs when the lower bound of the new $95\\%$ CI equals $0$. Assuming the standard error $s$ remains constant:\n$$ \\hat{\\tau}^{\\mathrm{MNAR}}(\\delta) - z_{0.975} \\cdot s = 0 $$\nSubstituting our expression for the adjusted effect:\n$$ \\hat{\\tau}^{\\mathrm{MAR}} - (\\pi_T + \\pi_C)\\delta - z_{0.975} \\cdot s = 0 $$\n      \nNow we solve for the tipping point value of $\\delta$:\n$$ \\delta = \\frac{\\hat{\\tau}^{\\mathrm{MAR}} - z_{0.975} \\cdot s}{\\pi_T + \\pi_C} $$\nUsing the given values: $\\hat{\\tau}^{\\mathrm{MAR}} = 0.30$, $s = 0.12$, $\\pi_T = 0.10$, $\\pi_C = 0.05$, and $z_{0.975} \\approx 1.96$.\n$$ \\delta = \\frac{0.30 - 1.96 \\times 0.12}{0.10 + 0.05} = \\frac{0.30 - 0.2352}{0.15} = \\frac{0.0648}{0.15} = 0.432 $$\n\nLet's evaluate the options based on this result:\n- **A:** This option correctly defines a tipping point analysis and provides the calculated value of $\\delta \\approx 0.432$. This is the correct choice.\n- **B:** The definition is incorrect (it's a sensitivity analysis, it can't determine the true mechanism). The calculation is incorrect as it depends on the sum, not the difference, of missingness proportions.\n- **C:** The definition is incorrect (confuses sensitivity parameter with number of imputations). It also incorrectly uses a one-sided critical value when a two-sided level was specified.\n- **D:** The definition is incorrect. Tipping point analysis explores MNAR, not MCAR. The point estimate explicitly changes with $\\delta$.\n- **E:** The definition is incorrect, again confusing the sensitivity parameter $\\delta$ with the number of imputations. A tipping point can be calculated.\n\nThus, option A is the only one that is correct in both its definition and its calculation.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}