{
    "hands_on_practices": [
        {
            "introduction": "Translational studies often begin with small sample sizes, where assumptions of normality required for tests like the t-test may not be reliable. This practice challenges you to step back from standard parametric formulas and build a hypothesis test from the ground up using the Wilcoxon signed-rank test. By manually calculating an exact $p$-value from the ranks and their possible sign combinations, you will develop a deep intuition for how nonparametric methods achieve robust inference and what a $p$-value fundamentally represents under a null hypothesis. ",
            "id": "5022334",
            "problem": "A window-of-opportunity translational medicine study investigates whether a first-in-class kinase inhibitor acutely increases the phosphorylation ratio of a target pathway in tumor biopsies obtained immediately before and after a single therapeutic dose. Because the sample is small and the distribution of within-subject changes is unknown, a nonparametric paired analysis is required. For each subject, let the paired difference be $d_i = \\text{post}_i - \\text{pre}_i$, and assume the distribution of $d_i$ under the null hypothesis is continuous and symmetric about $0$. The pre- and post-dose phosphorylation ratios for $6$ subjects are:\n\nSubject $1$: pre $2.3$, post $2.8$; Subject $2$: pre $2.1$, post $1.8$; Subject $3$: pre $3.0$, post $4.2$; Subject $4$: pre $2.5$, post $3.3$; Subject $5$: pre $1.9$, post $1.5$; Subject $6$: pre $2.7$, post $3.6$.\n\nStarting only from the core definitions of the Wilcoxon signed-rank test, compute the exact two-sided $p$-value for testing the null hypothesis that the median paired difference is $0$. Your derivation must proceed from the assumptions stated above, treat all absolute differences with their exact integer ranks (no ties are present), and use combinatorial counting of sign allocations to obtain the exact null distribution of the signed-rank statistic. Express the final $p$-value as a reduced fraction. Do not report a confidence interval (CI) or effect size in your final answer. Your final answer must be a single number. If you choose to provide a decimal as an intermediate step, do not round in the final reported fraction.",
            "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded, well-posed, objective, and internally consistent. It presents a standard biostatistical problem based on realistic data from translational medicine and requests a calculation using a well-defined, appropriate statistical method. The problem provides all necessary information and contains no contradictions.\n\nThe task is to compute the exact two-sided $p$-value for a Wilcoxon signed-rank test on a paired dataset of size $n=6$. The null hypothesis, $H_0$, is that the median of the paired differences is $0$. The analysis is based on the assumption that the distribution of the paired differences is continuous and symmetric about $0$.\n\nFirst, we calculate the paired differences, $d_i = \\text{post}_i - \\text{pre}_i$, for each of the $n=6$ subjects.\n- Subject $1$: $d_1 = 2.8 - 2.3 = 0.5$\n- Subject $2$: $d_2 = 1.8 - 2.1 = -0.3$\n- Subject $3$: $d_3 = 4.2 - 3.0 = 1.2$\n- Subject $4$: $d_4 = 3.3 - 2.5 = 0.8$\n- Subject $5$: $d_5 = 1.5 - 1.9 = -0.4$\n- Subject $6$: $d_6 = 3.6 - 2.7 = 0.9$\n\nNext, we take the absolute values of these non-zero differences, $|d_i|$, and rank them from smallest ($1$) to largest ($6$).\n- $|d_2| = 0.3 \\implies \\text{Rank} = 1$\n- $|d_5| = 0.4 \\implies \\text{Rank} = 2$\n- $|d_1| = 0.5 \\implies \\text{Rank} = 3$\n- $|d_4| = 0.8 \\implies \\text{Rank} = 4$\n- $|d_6| = 0.9 \\implies \\text{Rank} = 5$\n- $|d_3| = 1.2 \\implies \\text{Rank} = 6$\nThere are no ties in the absolute differences, as stated in the problem. The set of ranks is $\\{1, 2, 3, 4, 5, 6\\}$.\n\nThe Wilcoxon signed-rank test statistic is based on the sum of the ranks corresponding to the positive or negative differences. Let $W^+$ be the sum of ranks of the positive differences and $W^-$ be the sum of ranks of the negative differences.\n- The differences $d_1, d_3, d_4, d_6$ are positive. Their corresponding ranks are $3, 6, 4, 5$.\n- The differences $d_2, d_5$ are negative. Their corresponding ranks are $1, 2$.\n\nWe calculate the observed values of $W^+$ and $W^-$:\n$$ W^+ = 3 + 6 + 4 + 5 = 18 $$\n$$ W^- = 1 + 2 = 3 $$\nAs a check, the sum of the statistics must equal the total sum of ranks: $W^+ + W^- = 18 + 3 = 21$.\nThe total sum of ranks is given by $\\frac{n(n+1)}{2} = \\frac{6(6+1)}{2} = \\frac{42}{2} = 21$. The calculation is consistent.\n\nFor a two-sided test, the test statistic, let's call it $W$, is the minimum of $W^+$ and $W^-$.\n$$ w_{obs} = \\min(W^+, W^-) = \\min(18, 3) = 3 $$\n\nUnder the null hypothesis ($H_0$), each difference $d_i$ has a $50\\%$ chance of being positive or negative, independently of its rank. Therefore, any of the $n=6$ ranks has a probability of $\\frac{1}{2}$ of being positive and $\\frac{1}{2}$ of being negative. With $6$ ranks, there are $2^6 = 64$ possible, equally likely combinations of signed ranks.\n\nThe two-sided $p$-value is the probability of observing a test statistic $W$ as extreme as or more extreme than $w_{obs}$. This is $P(W \\le w_{obs})$.\nThe event $W \\le 3$ is equivalent to the event $(W^+ \\le 3 \\text{ or } W^- \\le 3)$.\nThe events $W^+ \\le 3$ and $W^- \\le 3$ are mutually exclusive. If $W^+ \\le 3$, then $W^- = 21 - W^+ \\ge 21 - 3 = 18$, so $W^-$ cannot be less than or equal to $3$.\nThus, the $p$-value is $P(W^+ \\le 3) + P(W^- \\le 3)$.\nThe null distribution of $W^+$ is symmetric to that of $W^-$. Therefore, $P(W^+ \\le k) = P(W^- \\le k)$ for any $k$.\nThe $p$-value is $2 \\times P(W^+ \\le 3)$.\n\nWe need to count the number of ways to choose a subset of the ranks $\\{1, 2, 3, 4, 5, 6\\}$ whose sum is less than or equal to $3$. Let this count be $N_{extreme, 1-tail}$.\nThe possible sums for $W^+$ are:\n- Sum = $0$: This corresponds to the empty set of ranks (all differences are negative). There is $1$ way.\n- Sum = $1$: The set of one rank $\\{1\\}$. There is $1$ way.\n- Sum = $2$: The set of one rank $\\{2\\}$. There is $1$ way.\n- Sum = $3$: The set of one rank $\\{3\\}$, or the set of two ranks $\\{1, 2\\}$. There are $2$ ways.\nThe total number of ways to obtain a $W^+$ sum of $0, 1, 2,$ or $3$ is $N_{extreme, 1-tail} = 1 + 1 + 1 + 2 = 5$.\n\nThe one-sided probability is $P(W^+ \\le 3) = \\frac{N_{extreme, 1-tail}}{\\text{Total combinations}} = \\frac{5}{2^6} = \\frac{5}{64}$.\nThe two-sided $p$-value is:\n$$ p = 2 \\times P(W^+ \\le 3) = 2 \\times \\frac{5}{64} = \\frac{10}{64} $$\nThis fraction must be reduced to its simplest form.\n$$ p = \\frac{10 \\div 2}{64 \\div 2} = \\frac{5}{32} $$\nThe exact two-sided $p$-value for the Wilcoxon signed-rank test is $\\frac{5}{32}$.",
            "answer": "$$\\boxed{\\frac{5}{32}}$$"
        },
        {
            "introduction": "This exercise tackles the critical challenge of analyzing stratified data from multicenter trials, where site-to-site differences in patient populations can act as nuisance parameters. You will implement the Cochran-Mantel-Haenszel test, a cornerstone method that adjusts for these strata to estimate a common treatment effect, such as an odds ratio. This practice is essential for understanding how to validly pool evidence and control for confounding in complex study designs. ",
            "id": "5022322",
            "problem": "A multicenter Phase II translational medicine study evaluates whether a biomarker-guided therapy increases binary clinical response compared with standard care. Patients are enrolled across three hospitals (strata), with variable baseline response rates that are not of primary interest. Within each stratum, randomization assigns treatment. The primary estimand is a common log-odds effect across strata; stratum-specific baselines are treated as nuisance parameters. The observed $2 \\times 2$ tables are:\n\nStratum $1$ (Hospital $1$): Treatment group size $20$, responders $8$; Control group size $20$, responders $4$.\n\nStratum $2$ (Hospital $2$): Treatment group size $15$, responders $6$; Control group size $15$, responders $3$.\n\nStratum $3$ (Hospital $3$): Treatment group size $50$, responders $12$; Control group size $50$, responders $10$.\n\nLet $a_k$ denote the number of responders in the treatment group for stratum $k$, $b_k$ the number of nonresponders in treatment, $c_k$ the number of responders in control, and $d_k$ the number of nonresponders in control, with $n_k = a_k + b_k + c_k + d_k$. Consider the composite null hypothesis $H_0$ that the treatment has no effect across strata on the odds of response, while stratum-specific baseline response probabilities are arbitrary nuisance parameters. The alternative $H_1$ posits a positive common log-odds effect.\n\nUsing only first principles—namely, the definition of a $p$-value as a tail probability under $H_0$ and the fact that, under $H_0$ and conditioning on the fixed margins within each stratum, the distribution of $a_k$ is hypergeometric—perform the following:\n\n1. Derive the conditional expectation $\\mathbb{E}[a_k \\mid \\text{margins}]$ and the conditional variance $\\mathbb{V}[a_k \\mid \\text{margins}]$ under $H_0$ for each stratum, and compute their numerical values from the data.\n2. Form the efficient score $U = \\sum_{k=1}^3 (a_k - \\mathbb{E}[a_k \\mid \\text{margins}])$ and the corresponding information $I(0) = \\sum_{k=1}^3 \\mathbb{V}[a_k \\mid \\text{margins}]$ under $H_0$. Use these to construct the Cochran-Mantel-Haenszel (CMH) score test statistic $T = U^2 / I(0)$ and its large-sample $p$-value based on a chi-square distribution with $1$ degree of freedom.\n3. Use the score/information approximation to obtain the common log-odds effect estimate $\\hat{\\beta} \\approx U / I(0)$, the standard error $\\mathrm{se}(\\hat{\\beta}) \\approx \\sqrt{1 / I(0)}$, and a $95\\%$ confidence interval for the common odds ratio by exponentiating the interval for $\\hat{\\beta}$.\n\nWhich option correctly reports the validity of the testing procedure with nuisance parameters, the numerical $p$-value, the effect size estimate, and the confidence interval, along with the proper inferential conclusion at significance level $\\alpha = 0.05$?\n\nA. Conditioning on stratum margins eliminates nuisance parameters, yielding a valid level-$\\alpha$ test under the composite null. The CMH score statistic is $T \\approx 2.50$ with $p \\approx 0.11$; the estimated common log-odds effect is $\\hat{\\beta} \\approx 0.55$ so the common odds ratio is approximately $e^{0.55} \\approx 1.74$; the $95\\%$ confidence interval for the odds ratio is approximately $(0.88, 3.47)$. Therefore, there is no statistically significant benefit at $\\alpha = 0.05$.\n\nB. Plugging in pooled variance without stratification is valid under randomization and yields $p  0.05$, with a $95\\%$ confidence interval for the odds ratio approximately $(1.10, 2.82)$, supporting statistical significance.\n\nC. A permutation test that reassigns treatment labels across the entire sample (ignoring strata) is exact under the composite null and yields $p \\approx 0.11$, leading to a valid non-significant conclusion.\n\nD. The correct approach is to maximize the $p$-value over all values of the nuisance parameters; this gives $p \\approx 0.28$, implying no statistical significance at $\\alpha = 0.05$.",
            "solution": "The user wants me to solve a statistics problem related to a stratified clinical trial.\n\nHere's my plan:\n1.  **Validate the Problem Statement**: I need to go through my standard validation checklist.\n    *   **Extract Givens**: I'll list all the data and definitions provided in the problem.\n    *   **Validate**: I'll check for scientific soundness, well-posedness, completeness, etc.\n        *   The problem describes a multicenter Phase II trial, which is a standard setup in translational medicine.\n        *   It involves a binary outcome (response) and a comparison between a treatment and a control group.\n        *   The analysis is stratified by hospital, which is a common and recommended practice to handle center effects.\n        *   The primary estimand is a common log-odds effect, treating stratum-specific baselines as nuisance parameters. This is a very standard model (e.g., conditional logistic regression, Mantel-Haenszel methods).\n        *   The problem statement provides the data for three strata ($2 \\times 2$ tables).\n        *   It defines the notation for the contingency table cells ($a_k, b_k, c_k, d_k$).\n        *   It specifies the null hypothesis $H_0$ (no effect) and the alternative $H_1$ (positive effect).\n        *   Crucially, it directs me to use a specific method: a conditional test based on the hypergeometric distribution of $a_k$ under $H_0$ given the margins. This is the basis of the Cochran-Mantel-Haenszel (CMH) test.\n        *   It asks me to perform three specific calculations:\n            1.  Conditional expectation and variance of $a_k$.\n            2.  The CMH score statistic $T$ and its p-value.\n            3.  An estimate of the common log-odds ratio $\\hat{\\beta}$ and its confidence interval.\n        *   The problem seems scientifically grounded, well-posed, objective, and complete. It's a classic biostatistics problem. I don't see any immediate flaws. The instructions are very specific, which helps.\n\n2.  **Proceed to Solution**: Since the problem is valid, I'll execute the required calculations step-by-step. All mathematical entities will be in LaTeX.\n\n    *   **Step 0: Organize the data**. For each stratum $k=1, 2, 3$, I'll list the values of $a_k, b_k, c_k, d_k$ and the marginal totals.\n        *   $n_{1k} = a_k + b_k$ (treatment group size)\n        *   $n_{2k} = c_k + d_k$ (control group size)\n        *   $m_{1k} = a_k + c_k$ (total responders)\n        *   $m_{2k} = b_k + d_k$ (total non-responders)\n        *   $N_k = n_{1k} + n_{2k} = m_{1k} + m_{2k}$ (total size for stratum k)\n\n    *   **Step 1: Calculate Conditional Expectation and Variance**.\n        *   The problem states that under $H_0$ and conditioning on the margins, $a_k$ follows a hypergeometric distribution.\n        *   A random variable $X$ following a hypergeometric distribution with parameters (population size $N$, number of successes in population $K$, number of draws $n$) has:\n            *   $\\mathbb{E}[X] = n \\frac{K}{N}$\n            *   $\\mathbb{V}[X] = n \\frac{K}{N} \\left(1 - \\frac{K}{N}\\right) \\frac{N-n}{N-1}$\n        *   In my context, for stratum $k$:\n            *   The \"population\" is all patients in the stratum, size $N_k$.\n            *   The number of \"draws\" is the size of the treatment group, $n_{1k}$.\n            *   The number of \"successes\" in the population is the total number of responders, $m_{1k}$.\n        *   So, for $a_k$, we have:\n            *   $\\mathbb{E}[a_k \\mid \\text{margins}] = \\frac{n_{1k} m_{1k}}{N_k}$\n            *   $\\mathbb{V}[a_k \\mid \\text{margins}] = \\frac{n_{1k} n_{2k} m_{1k} m_{2k}}{N_k^2 (N_k - 1)}$\n        *   I will compute these values for $k=1, 2, 3$.\n\n    *   **Step 2: Calculate the CMH Score Statistic and p-value**.\n        *   $U = \\sum_{k=1}^3 (a_k - \\mathbb{E}[a_k \\mid \\text{margins}])$\n        *   $I(0) = \\sum_{k=1}^3 \\mathbb{V}[a_k \\mid \\text{margins}]$\n        *   $T = U^2 / I(0)$\n        *   The p-value is the probability that a $\\chi_1^2$ random variable is greater than $T$. $P(\\chi_1^2  T)$.\n        *   Note: The problem asks for a one-sided alternative ($H_1$: positive effect), but the score statistic $T=U^2/I(0)$ is for a two-sided test. A one-sided p-value would be based on the standardized score $Z=U/\\sqrt{I(0)}$. If $U0$, the one-sided p-value is $P(Z_{std\\_normal}  Z)$. If $U0$, it's $P(Z_{std\\_normal}  Z)$. Given the alternative is a *positive* common log-odds effect, we should check the sign of $U$. If $U0$, the one-sided p-value is half of the two-sided p-value from the $\\chi^2$ test. Let me double-check the question. Ah, it says \"The alternative $H_1$ posits a positive common log-odds effect.\" but then asks to compute $T = U^2/I(0)$ and its p-value based on a $\\chi_1^2$ distribution. This implies a two-sided test. Let's calculate the two-sided p-value as requested and see if it aligns with the options. The options seem to be discussing a two-sided test conclusion (\"no statistically significant benefit\", not \"no statistically significant positive benefit\"). Let's proceed with the two-sided test as implied by the construction of $T$.\n\n    *   **Step 3: Estimate Effect Size and Confidence Interval**.\n        *   The problem provides the approximations:\n            *   $\\hat{\\beta} \\approx U / I(0)$\n            *   $\\mathrm{se}(\\hat{\\beta}) \\approx \\sqrt{1 / I(0)}$\n        *   A $95\\%$ confidence interval for $\\beta$ is $\\hat{\\beta} \\pm 1.96 \\cdot \\mathrm{se}(\\hat{\\beta})$.\n        *   The confidence interval for the common odds ratio, $OR = e^{\\beta}$, is obtained by exponentiating the endpoints of the CI for $\\beta$.\n        *   CI for OR: $(\\exp(\\hat{\\beta} - 1.96 \\cdot \\mathrm{se}(\\hat{\\beta})), \\exp(\\hat{\\beta} + 1.96 \\cdot \\mathrm{se}(\\hat{\\beta})))$.\n\n3.  **Evaluate the Options**.\n    *   **Option A**: I will compare my calculated values for $T$, p-value, $\\hat{\\beta}$, and the CI for the OR with the values given in this option. I will also evaluate the initial statement about conditioning on margins eliminating nuisance parameters. This is a key feature of conditional likelihood methods and the CMH test. By conditioning on the sufficient statistics for the nuisance parameters (the row and column totals), the resulting likelihood and distribution for the parameter of interest (the common odds ratio) no longer depend on the nuisance parameters. This makes the test valid under the composite null. So the first sentence of A is correct in principle. I need to verify the numbers.\n    *   **Option B**: This suggests pooling the data, ignoring stratification. This is generally incorrect as it can lead to Simpson's paradox and biased estimates if the stratum-specific baseline rates and randomization ratios differ. This approach does not correctly handle the nuisance parameters. It's almost certainly incorrect as a method.\n    *   **Option C**: This suggests a permutation test that reassigns treatment labels across the entire sample (ignoring strata). Similar to option B, this is the wrong null hypothesis. The correct permutation scheme would be to permute treatment labels *within* each stratum to respect the study design. A permutation test that pools all subjects and re-randomizes ignores the stratified design and is therefore not an exact test for the specified composite null hypothesis. It tests a different, simpler null (complete homogeneity).\n    *   **Option D**: This suggests maximizing the p-value over the nuisance parameters. This is a generally invalid procedure for constructing a test. A valid frequentist test must have a controlled Type I error rate *for all* values of the nuisance parameters under the null. Maximizing the p-value is not a standard or correct way to achieve this. The standard methods are conditioning (like CMH) or using supremum-based tests (like a likelihood ratio test where you maximize the likelihood under $H_0$ w.r.t nuisance parameters). Maximizing the *p-value* is not a principle.\n\n4.  **Final Calculations**:\n\n    *   **Stratum 1**:\n        *   $a_1 = 8$, $c_1 = 4$ - $m_{11} = 12$ (responders)\n        *   $n_{11} = 20$, $n_{21} = 20$ - $N_1 = 40$\n        *   $b_1 = 20 - 8 = 12$, $d_1 = 20 - 4 = 16$ - $m_{21} = 28$ (non-responders)\n        *   Check margins: $a_1+b_1=20$, $c_1+d_1=20$, $a_1+c_1=12$, $b_1+d_1=28$. $N_1=20+20=40, m_{11}+m_{21}=12+28=40$. Correct.\n        *   $\\mathbb{E}[a_1] = \\frac{n_{11} m_{11}}{N_1} = \\frac{20 \\times 12}{40} = 6$.\n        *   $\\mathbb{V}[a_1] = \\frac{n_{11} n_{21} m_{11} m_{21}}{N_1^2 (N_1 - 1)} = \\frac{20 \\times 20 \\times 12 \\times 28}{40^2 \\times 39} = \\frac{134400}{1600 \\times 39} = \\frac{1344}{16 \\times 39} = \\frac{84}{39} = \\frac{28}{13} \\approx 2.1538$.\n\n    *   **Stratum 2**:\n        *   $a_2 = 6$, $c_2 = 3$ - $m_{12} = 9$ (responders)\n        *   $n_{12} = 15$, $n_{22} = 15$ - $N_2 = 30$\n        *   $b_2 = 15 - 6 = 9$, $d_2 = 15 - 3 = 12$ - $m_{22} = 21$ (non-responders)\n        *   Check margins: $a_2+b_2=15$, $c_2+d_2=15$, $a_2+c_2=9$, $b_2+d_2=21$. $N_2=15+15=30, m_{12}+m_{22}=9+21=30$. Correct.\n        *   $\\mathbb{E}[a_2] = \\frac{n_{12} m_{12}}{N_2} = \\frac{15 \\times 9}{30} = 4.5$.\n        *   $\\mathbb{V}[a_2] = \\frac{n_{12} n_{22} m_{12} m_{22}}{N_2^2 (N_2 - 1)} = \\frac{15 \\times 15 \\times 9 \\times 21}{30^2 \\times 29} = \\frac{42525}{900 \\times 29} = \\frac{47.25}{29} \\approx 1.6293$.\n\n    *   **Stratum 3**:\n        *   $a_3 = 12$, $c_3 = 10$ - $m_{13} = 22$ (responders)\n        *   $n_{13} = 50$, $n_{23} = 50$ - $N_3 = 100$\n        *   $b_3 = 50 - 12 = 38$, $d_3 = 50 - 10 = 40$ - $m_{23} = 78$ (non-responders)\n        *   Check margins: $a_3+b_3=50$, $c_3+d_3=50$, $a_3+c_3=22$, $b_3+d_3=78$. $N_3=50+50=100, m_{13}+m_{23}=22+78=100$. Correct.\n        *   $\\mathbb{E}[a_3] = \\frac{n_{13} m_{13}}{N_3} = \\frac{50 \\times 22}{100} = 11$.\n        *   $\\mathbb{V}[a_3] = \\frac{n_{13} n_{23} m_{13} m_{23}}{N_3^2 (N_3 - 1)} = \\frac{50 \\times 50 \\times 22 \\times 78}{100^2 \\times 99} = \\frac{4290000}{10000 \\times 99} = \\frac{429}{99} = \\frac{143}{33} = \\frac{13}{3} \\approx 4.3333$.\n\n    *   **Combine results**:\n        *   $U = (a_1 - \\mathbb{E}[a_1]) + (a_2 - \\mathbb{E}[a_2]) + (a_3 - \\mathbb{E}[a_3])$\n        *   $U = (8 - 6) + (6 - 4.5) + (12 - 11) = 2 + 1.5 + 1 = 4.5$.\n        *   $I(0) = \\mathbb{V}[a_1] + \\mathbb{V}[a_2] + \\mathbb{V}[a_3]$\n        *   $I(0) = \\frac{28}{13} + \\frac{189}{116} + \\frac{13}{3}$. ($47.25 / 29 = (189/4)/29 = 189/116$)\n        *   $I(0) \\approx 2.153846 + 1.629310 + 4.333333 \\approx 8.11649$.\n        *   $T = U^2 / I(0) = (4.5)^2 / 8.11649 = 20.25 / 8.11649 \\approx 2.49498$.\n        *   This seems to match $T \\approx 2.50$ in Option A.\n\n        *   **p-value**: $p = P(\\chi_1^2  2.49498)$.\n            *   The critical value for a $\\chi_1^2$ distribution at $\\alpha=0.05$ is approximately $3.84$. Since $T \\approx 2.50  3.84$, the result is not statistically significant at the $\\alpha = 0.05$ level.\n            *   Using a statistical calculator, the p-value is $1 - \\text{CDF}_{\\chi^2_1}(2.495) \\approx 0.1142$.\n            *   This matches $p \\approx 0.11$ in Option A.\n\n    *   **Effect Size and CI**:\n        *   $\\hat{\\beta} \\approx U / I(0) = 4.5 / 8.11649 \\approx 0.5544$.\n        *   This matches $\\hat{\\beta} \\approx 0.55$ in Option A.\n        *   The estimated common odds ratio is $e^{\\hat{\\beta}} \\approx e^{0.5544} \\approx 1.7409$.\n        *   This matches $e^{0.55} \\approx 1.74$ in Option A.\n        *   $\\mathrm{se}(\\hat{\\beta}) \\approx \\sqrt{1 / I(0)} = \\sqrt{1 / 8.11649} \\approx \\sqrt{0.12320} \\approx 0.3510$.\n        *   A $95\\%$ confidence interval for the common log-odds ratio $\\beta$ is given by $\\hat{\\beta} \\pm z_{0.975} \\cdot \\mathrm{se}(\\hat{\\beta})$. Using $z_{0.975} \\approx 1.96$:\n        *   $CI(\\beta) \\approx 0.5544 \\pm 1.96 \\times 0.3510 = 0.5544 \\pm 0.68796$.\n        *   $CI(\\beta) \\approx (-0.1336, 1.2424)$.\n        *   The $95\\%$ confidence interval for the common odds ratio is obtained by exponentiating the endpoints of the interval for $\\beta$:\n        *   $CI(OR) \\approx (\\exp(-0.1336), \\exp(1.2424))$.\n        *   $CI(OR) \\approx (0.8749, 3.4640)$.\n        *   This matches the interval $(0.88, 3.47)$ in Option A.\n\nThe results of the analysis fully support the claims made in Option A. Let us now dissect each option.\n\nOption A: \"Conditioning on stratum margins eliminates nuisance parameters, yielding a valid level-$\\alpha$ test under the composite null. The CMH score statistic is $T \\approx 2.50$ with $p \\approx 0.11$; the estimated common log-odds effect is $\\hat{\\beta} \\approx 0.55$ so the common odds ratio is approximately $e^{0.55} \\approx 1.74$; the $95\\%$ confidence interval for the odds ratio is approximately $(0.88, 3.47)$. Therefore, there is no statistically significant benefit at $\\alpha = 0.05$.\"\n- The initial premise regarding conditioning is a cornerstone of exact inference for discrete data and is the theoretical justification for the CMH test's validity. It is correct.\n- The calculated statistic $T \\approx 2.50$ matches our derivation.\n- The calculated p-value $p \\approx 0.11$ matches our derivation.\n- The estimated log-odds ratio $\\hat{\\beta} \\approx 0.55$ and odds ratio $OR \\approx 1.74$ match our derivation.\n- The $95\\%$ confidence interval for the odds ratio $(0.88, 3.47)$ matches our derivation.\n- The conclusion of no statistical significance at $\\alpha = 0.05$ is correct, as the p-value ($0.11$) is greater than $\\alpha$ and the confidence interval for the odds ratio includes the null value of $1$.\n- Verdict: **Correct**.\n\nOption B: \"Plugging in pooled variance without stratification is valid under randomization and yields $p  0.05$, with a $95\\%$ confidence interval for the odds ratio approximately $(1.10, 2.82)$, supporting statistical significance.\"\n- The premise that ignoring stratification is valid is incorrect. Stratification is a key feature of the study design intended to control for confounding by hospital site. Failing to account for it in the analysis invalidates the procedure, especially when baseline rates differ, and can lead to Simpson's Paradox. An analysis of the pooled data yields a test statistic of approx $2.52$ with a p-value of approx $0.11$, contradicting the claim of $p  0.05$. The method is inappropriate, and the claimed numerical results are false.\n- Verdict: **Incorrect**.\n\nOption C: \"A permutation test that reassigns treatment labels across the entire sample (ignoring strata) is exact under the composite null and yields $p \\approx 0.11$, leading to a valid non-significant conclusion.\"\n- The proposed permutation scheme is incorrect. A valid permutation test for a stratified design must respect the strata; that is, permutations of treatment labels must occur only *within* each stratum. Permuting across the entire sample tests a different null hypothesis (that of complete homogeneity) and is not an exact test for the composite null specified in the problem, where baseline rates can differ by stratum.\n- Verdict: **Incorrect**.\n\nOption D: \"The correct approach is to maximize the $p$-value over all values of the nuisance parameters; this gives $p \\approx 0.28$, implying no statistical significance at $\\alpha = 0.05$.\"\n- This describes a statistically invalid procedure. Maximizing a p-value over the space of nuisance parameters is not a recognized principle for constructing a hypothesis test. Valid frequentist tests must control the Type I error rate at level $\\alpha$ for all possible values of the nuisance parameters under the null hypothesis, often achieved by conditioning, as in the CMH test, or by other methods like using a supremum test statistic. The proposed method is not a valid approach.\n- Verdict: **Incorrect**.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "A statistically significant $p$-value is often a key milestone, yet its interpretation is notoriously subtle and prone to misconception. This advanced, computational exercise moves beyond the simple declaration of significance to quantify the credibility and reproducibility of a finding. By implementing a Bayesian framework, you will calibrate a conventional $p$-value to a local False Discovery Rate—the posterior probability that a significant result is actually a false positive—and predict the likelihood of successful replication.  This practice provides a modern, probabilistic lens for interpreting evidence, directly addressing the critical issue of reproducibility in science.",
            "id": "5022339",
            "problem": "You will implement a program that calibrates two-sided $p$-values to a probabilistic measure of reproducibility in a translational medicine setting and quantifies the posterior probability that an observed association is null. Consider an early-phase biomarker association study in translational medicine that reports an observed two-sided $p$-value $p$ for a single effect. Assume the standardized test statistic $Z$ used to compute $p$ is well-approximated by a Gaussian (Normal) distribution by the Central Limit Theorem (CLT), and that the observed study yields $Z_{\\text{obs}}$ whose magnitude is completely determined by $p$ through the definition of a two-sided $p$-value. Assume a two-component mixture model for the true biomarker effect $\\theta$: with prior probability $\\pi_0$ the effect is exactly null, and with prior probability $1 - \\pi_0$ the effect is drawn from a Normal distribution with mean $0$ and variance $\\tau^2$. Assume the study’s estimator is unbiased with sampling distribution that, after standardization, is $Z = \\theta/s + \\varepsilon$, where $s$ is the known standard error of the original study and $\\varepsilon \\sim \\mathcal{N}(0,1)$. A replication study is planned with standard error $s_{\\mathrm{rep}}$, and will use the same test (two-sided) with significance threshold $\\alpha$.\n\nStarting from core definitions, implement the following computations for each case in the test suite:\n- Calibrated $p$-value as a probability: the local False Discovery Rate (lFDR), defined as the posterior probability that the effect is null given the observed $Z_{\\text{obs}}$, written as $\\Pr(H_0 \\mid Z_{\\text{obs}})$ under the specified mixture model and Gaussian assumptions.\n- Predictive reproducibility probability: the posterior predictive probability that the replication study will achieve two-sided statistical significance at level $\\alpha$, namely $\\Pr\\left(|Z_{\\mathrm{rep}}| \\ge z_{\\alpha} \\mid Z_{\\text{obs}}\\right)$, where $z_{\\alpha}$ is the two-sided critical value determined by $\\alpha$ and $Z_{\\mathrm{rep}} = \\theta/s_{\\mathrm{rep}} + \\varepsilon'$ with $\\varepsilon' \\sim \\mathcal{N}(0,1)$.\n\nUse only fundamental laws and core definitions:\n- The two-sided $p$-value definition under a continuous symmetric null: $p = 2 \\cdot \\Pr(Z \\ge |Z_{\\text{obs}}| \\mid H_0)$.\n- The Gaussian distribution properties for linear transformations and sums of independent Normal random variables.\n- Bayes theorem for posterior and posterior predictive calculations under the stated mixture prior.\n\nYour program must produce results for the following test suite. Each test case is a tuple ($p, s, s_{\\mathrm{rep}}, \\pi_0, \\tau, \\alpha$) with all quantities unit-free and expressed as decimals:\n- Case $1$: ($0.02$, $0.2$, $0.2$, $0.9$, $0.3$, $0.05$).\n- Case $2$: ($0.05$, $0.15$, $0.25$, $0.9$, $0.3$, $0.05$).\n- Case $3$: ($0.5$, $0.1$, $0.1$, $0.9$, $0.3$, $0.05$).\n- Case $4$: ($0.000001$, $0.3$, $0.2$, $0.5$, $0.5$, $0.01$).\n- Case $5$: ($0.01$, $0.25$, $0.5$, $0.2$, $0.8$, $0.05$).\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the following order for each case: first the calibrated $p$-value as $\\Pr(H_0 \\mid Z_{\\text{obs}})$, then the predictive reproducibility probability $\\Pr\\left(|Z_{\\mathrm{rep}}| \\ge z_{\\alpha} \\mid Z_{\\text{obs}}\\right)$. For example, the output should be of the form $[r_{1,\\text{cal}}, r_{1,\\text{rep}}, r_{2,\\text{cal}}, r_{2,\\text{rep}}, \\dots, r_{5,\\text{cal}}, r_{5,\\text{rep}}]$ where each $r$ is a decimal in $[0,1]$.",
            "solution": "The objective is to compute two quantities for each test case: $1$) the calibrated $p$-value, defined as the local False Discovery Rate (lFDR), $\\Pr(H_0 \\mid Z_{\\text{obs}})$, and $2$) the predictive reproducibility probability, $\\Pr\\left(|Z_{\\mathrm{rep}}| \\ge z_{\\alpha} \\mid Z_{\\text{obs}}\\right)$. This will be done by applying Bayesian statistical principles to the specified two-component mixture model.\n\nThe problem defines a framework where a true, unobserved effect size $\\theta$ is either null (hypothesis $H_0: \\theta=0$) with prior probability $\\pi_0$, or drawn from a distribution of non-null effects (hypothesis $H_1$) with prior probability $1-\\pi_0$. The model specifics are:\n- Prior probabilities: $\\Pr(H_0) = \\pi_0$ and $\\Pr(H_1) = 1-\\pi_0$.\n- Prior distribution under $H_1$: $\\theta \\sim \\mathcal{N}(0, \\tau^2)$.\n- Sampling distribution of the test statistic $Z$: Given $\\theta$, $Z \\mid \\theta \\sim \\mathcal{N}(\\theta/s, 1)$, where $s$ is the standard error of the original study.\n- Replication test statistic: $Z_{\\mathrm{rep}} \\mid \\theta \\sim \\mathcal{N}(\\theta/s_{\\mathrm{rep}}, 1)$, where $s_{\\mathrm{rep}}$ is the standard error of the replication study.\n\nFrom the provided two-sided $p$-value $p$, the magnitude of the observed test statistic $|Z_{\\text{obs}}|$ is determined by $p = 2 \\cdot \\Pr(Z \\ge |Z_{\\text{obs}}| \\mid H_0)$. Since $Z \\mid H_0 \\sim \\mathcal{N}(0,1)$, we have $p = 2(1 - \\Phi(|Z_{\\text{obs}}|))$, where $\\Phi$ is the standard normal cumulative distribution function (CDF). This gives $|Z_{\\text{obs}}| = \\Phi^{-1}(1 - p/2)$. Due to the symmetry of the model (both the prior on $\\theta$ and the sampling error are centered at $0$), we can assume $Z_{\\text{obs}} = \\Phi^{-1}(1 - p/2)$ without loss of generality. Similarly, the critical value $z_{\\alpha}$ for the replication study at a two-sided significance level $\\alpha$ is $z_{\\alpha} = \\Phi^{-1}(1 - \\alpha/2)$.\n\n### Part 1: Derivation of the Local False Discovery Rate (lFDR)\n\nThe lFDR is the posterior probability of the null hypothesis given the observed data $Z_{\\text{obs}}$, $\\Pr(H_0 \\mid Z_{\\text{obs}})$. We apply Bayes' theorem:\n$$\n\\text{lFDR} = \\Pr(H_0 \\mid Z_{\\text{obs}}) = \\frac{f(Z_{\\text{obs}} \\mid H_0) \\Pr(H_0)}{f(Z_{\\text{obs}})}\n$$\nThe marginal density of the data, $f(Z_{\\text{obs}})$, is found using the law of total probability:\n$$\nf(Z_{\\text{obs}}) = f(Z_{\\text{obs}} \\mid H_0) \\Pr(H_0) + f(Z_{\\text{obs}} \\mid H_1) \\Pr(H_1)\n$$\nSubstituting this into the lFDR expression:\n$$\n\\text{lFDR} = \\frac{f(Z_{\\text{obs}} \\mid H_0) \\pi_0}{f(Z_{\\text{obs}} \\mid H_0) \\pi_0 + f(Z_{\\text{obs}} \\mid H_1) (1 - \\pi_0)}\n$$\nWe need the conditional densities $f(Z_{\\text{obs}} \\mid H_0)$ and $f(Z_{\\text{obs}} \\mid H_1)$.\n1.  **Under $H_0$**: $\\theta = 0$. The model $Z \\mid \\theta \\sim \\mathcal{N}(\\theta/s, 1)$ becomes $Z \\sim \\mathcal{N}(0, 1)$. Thus, the density is the standard normal probability density function (PDF), $\\phi(z)$:\n    $$f(z \\mid H_0) = \\phi(z) = \\frac{1}{\\sqrt{2\\pi}} e^{-z^2/2}$$\n2.  **Under $H_1$**: $\\theta \\sim \\mathcal{N}(0, \\tau^2)$ and $Z \\mid \\theta \\sim \\mathcal{N}(\\theta/s, 1)$. The marginal distribution of $Z$ under $H_1$ is found by integrating over all possible values of $\\theta$. This is a standard hierarchical model. The resulting distribution for $Z$ is also normal, with a mean $E[Z \\mid H_1] = E[E[Z \\mid \\theta]] = E[\\theta/s] = 0$ and variance $\\text{Var}(Z \\mid H_1) = E[\\text{Var}(Z \\mid \\theta)] + \\text{Var}(E[Z \\mid \\theta]) = E[1] + \\text{Var}(\\theta/s) = 1 + \\text{Var}(\\theta)/s^2 = 1 + \\tau^2/s^2$.\n    So, $Z \\mid H_1 \\sim \\mathcal{N}(0, 1 + \\tau^2/s^2)$. The density is:\n    $$f(z \\mid H_1) = \\frac{1}{\\sqrt{1 + \\tau^2/s^2}} \\phi\\left(\\frac{z}{\\sqrt{1 + \\tau^2/s^2}}\\right)$$\nBy substituting these densities into the lFDR formula, we obtain the calibrated $p$-value.\n\n### Part 2: Derivation of the Predictive Reproducibility Probability\n\nThe second quantity is the posterior predictive probability that the replication study will be statistically significant, $\\Pr(|Z_{\\mathrm{rep}}| \\ge z_{\\alpha} \\mid Z_{\\text{obs}})$. We use the law of total probability, conditioning over the hypotheses:\n$$\n\\Pr(|Z_{\\mathrm{rep}}| \\ge z_{\\alpha} \\mid Z_{\\text{obs}}) = \\sum_{i \\in \\{0, 1\\}} \\Pr(|Z_{\\mathrm{rep}}| \\ge z_{\\alpha} \\mid H_i, Z_{\\text{obs}}) \\Pr(H_i \\mid Z_{\\text{obs}})\n$$\nThis expands to:\n$$\n\\Pr(|Z_{\\mathrm{rep}}| \\ge z_{\\alpha} \\mid Z_{\\text{obs}}) = \\Pr(|Z_{\\mathrm{rep}}| \\ge z_{\\alpha} \\mid H_0, Z_{\\text{obs}})\\Pr(H_0 \\mid Z_{\\text{obs}}) + \\Pr(|Z_{\\mathrm{rep}}| \\ge z_{\\alpha} \\mid H_1, Z_{\\text{obs}})\\Pr(H_1 \\mid Z_{\\text{obs}})\n$$\nwhere $\\Pr(H_0 \\mid Z_{\\text{obs}})$ is the lFDR and $\\Pr(H_1 \\mid Z_{\\text{obs}}) = 1 - \\text{lFDR}$.\n\n1.  **Component under $H_0$**: If $H_0$ is true, $\\theta=0$. Then $Z_{\\mathrm{rep}} = 0/s_{\\mathrm{rep}} + \\varepsilon' \\sim \\mathcal{N}(0,1)$, where $\\varepsilon'$ is the replication sampling error. The distribution of $Z_{\\mathrm{rep}}$ is independent of the original observation $Z_{\\text{obs}}$. The probability of significance is, by definition of the two-sided critical value $z_{\\alpha}$:\n    $$ \\Pr(|Z_{\\mathrm{rep}}| \\ge z_{\\alpha} \\mid H_0, Z_{\\text{obs}}) = \\Pr(|Z| \\ge z_{\\alpha} \\text{ where } Z \\sim \\mathcal{N}(0,1)) = \\alpha $$\n2.  **Component under $H_1$**: To find $\\Pr(|Z_{\\mathrm{rep}}| \\ge z_{\\alpha} \\mid H_1, Z_{\\text{obs}})$, we first need the posterior distribution of $\\theta$ given $Z_{\\text{obs}}$ and $H_1$. This is a standard Bayesian update for a normal mean.\n    - Prior: $\\theta \\mid H_1 \\sim \\mathcal{N}(0, \\tau^2)$.\n    - Likelihood (from $Z_{\\text{obs}} \\mid \\theta \\sim \\mathcal{N}(\\theta/s, 1)$): $s Z_{\\text{obs}} \\mid \\theta \\sim \\mathcal{N}(\\theta, s^2)$.\n    - Posterior: $\\theta \\mid Z_{\\text{obs}}, H_1 \\sim \\mathcal{N}(\\mu_p, \\sigma_p^2)$, with\n      $$ \\mu_p = \\frac{s\\tau^2}{s^2+\\tau^2} Z_{\\text{obs}} \\quad \\text{and} \\quad \\sigma_p^2 = \\frac{s^2 \\tau^2}{s^2 + \\tau^2} $$\n    Next, we derive the posterior predictive distribution of $Z_{\\mathrm{rep}}$ given $Z_{\\text{obs}}$ and $H_1$. $Z_{\\mathrm{rep}} = \\theta/s_{\\mathrm{rep}} + \\varepsilon'$, where we average over the posterior distribution of $\\theta$. The resulting distribution is also normal, $Z_{\\mathrm{rep}} \\mid Z_{\\text{obs}}, H_1 \\sim \\mathcal{N}(\\mu_{\\text{pred}}, \\sigma_{\\text{pred}}^2)$, with:\n      $$ \\mu_{\\text{pred}} = E[\\theta/s_{\\mathrm{rep}}] = \\mu_p/s_{\\mathrm{rep}} = \\frac{s\\tau^2}{s_{\\mathrm{rep}}(s^2+\\tau^2)} Z_{\\text{obs}} $$\n      $$ \\sigma_{\\text{pred}}^2 = \\text{Var}(\\theta/s_{\\mathrm{rep}}) + E[\\text{Var}(\\varepsilon')] = \\sigma_p^2/s_{\\mathrm{rep}}^2 + 1 = \\frac{s^2 \\tau^2}{s_{\\mathrm{rep}}^2(s^2 + \\tau^2)} + 1 $$\n    The probability of significance under $H_1$ is then calculated from this distribution:\n    $$ \\Pr(|Z_{\\mathrm{rep}}| \\ge z_{\\alpha} \\mid H_1, Z_{\\text{obs}}) = \\Pr(Z_{\\mathrm{rep}} \\le -z_{\\alpha}) + \\Pr(Z_{\\mathrm{rep}} \\ge z_{\\alpha}) $$\n    $$ = \\Phi\\left(\\frac{-z_{\\alpha} - \\mu_{\\text{pred}}}{\\sigma_{\\text{pred}}}\\right) + 1 - \\Phi\\left(\\frac{z_{\\alpha} - \\mu_{\\text{pred}}}{\\sigma_{\\text{pred}}}\\right) $$\n\nCombining these components, the final predictive reproducibility probability is:\n$$\n\\Pr(|Z_{\\mathrm{rep}}| \\ge z_{\\alpha} \\mid Z_{\\text{obs}}) = \\alpha \\cdot \\text{lFDR} + \\Pr(|Z_{\\mathrm{rep}}| \\ge z_{\\alpha} \\mid H_1, Z_{\\text{obs}}) \\cdot (1 - \\text{lFDR})\n$$\nThe implementation will follow these derived formulas for each test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Calculates the calibrated p-value (lFDR) and the predictive reproducibility\n    probability for a series of test cases in a translational medicine context.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (p, s, s_rep, pi0, tau, alpha)\n    test_cases = [\n        (0.02, 0.2, 0.2, 0.9, 0.3, 0.05),\n        (0.05, 0.15, 0.25, 0.9, 0.3, 0.05),\n        (0.5, 0.1, 0.1, 0.9, 0.3, 0.05),\n        (0.000001, 0.3, 0.2, 0.5, 0.5, 0.01),\n        (0.01, 0.25, 0.5, 0.2, 0.8, 0.05),\n    ]\n\n    results = []\n    for p, s, s_rep, pi0, tau, alpha in test_cases:\n        # Step 1: Calculate initial statistics from input parameters.\n        \n        # Calculate the observed Z-statistic from the two-sided p-value.\n        # We assume Z_obs  0 by symmetry. p = 2 * (1 - CDF(Z_obs))\n        # = CDF(Z_obs) = 1 - p/2\n        z_obs = norm.ppf(1 - p / 2)\n        \n        # Calculate the critical Z-value for the replication study's significance level.\n        z_alpha = norm.ppf(1 - alpha / 2)\n\n        # Step 2: Calculate the local False Discovery Rate (lFDR).\n        # lFDR is the posterior probability of the null hypothesis, Pr(H0 | Z_obs).\n        \n        # Density of Z_obs under H0: Z|H0 ~ N(0, 1).\n        f_z_h0 = norm.pdf(z_obs, 0, 1)\n        \n        # Density of Z_obs under H1: Z|H1 ~ N(0, 1 + tau^2/s^2).\n        var_z_h1 = 1 + (tau**2) / (s**2)\n        f_z_h1 = norm.pdf(z_obs, 0, np.sqrt(var_z_h1))\n        \n        # Marginal density of Z_obs using the law of total probability.\n        f_z = f_z_h0 * pi0 + f_z_h1 * (1 - pi0)\n        \n        # Calculate lFDR using Bayes' theorem.\n        # lFDR = f(Z_obs|H0) * Pr(H0) / f(Z_obs)\n        if f_z  0:\n            lFDR = (f_z_h0 * pi0) / f_z\n        else:\n            # This case occurs if densities and priors result in zero marginal.\n            # For pi0=1, lFDR=1. For pi0=0, lFDR=0.\n            # With the given inputs, f_z should always be positive.\n            lFDR = 1.0 if pi0 == 1.0 else 0.0\n\n        results.append(lFDR)\n\n        # Step 3: Calculate the predictive reproducibility probability.\n        # Pr(|Z_rep| = z_alpha | Z_obs) is calculated using the law of total probability,\n        # conditional on H0 and H1.\n        \n        # Under H0, the probability of replication significance is alpha.\n        # Under H1, we must use the posterior predictive distribution of Z_rep.\n        \n        # First, find the posterior distribution of theta given Z_obs and H1.\n        # theta | Z_obs, H1 ~ N(mu_p, var_p)\n        var_p = (s**2 * tau**2) / (s**2 + tau**2)\n        mu_p = (s * tau**2 / (s**2 + tau**2)) * z_obs\n\n        # Then, find the posterior predictive distribution of Z_rep.\n        # Z_rep | Z_obs, H1 ~ N(mu_pred, var_pred)\n        mu_pred = mu_p / s_rep\n        var_pred = 1 + var_p / (s_rep**2)\n        sigma_pred = np.sqrt(var_pred)\n\n        # Probability of significance under H1 is the sum of the two tail probabilities\n        # of the posterior predictive distribution.\n        pr_h1_sig = norm.cdf(-z_alpha, loc=mu_pred, scale=sigma_pred) + \\\n                    (1 - norm.cdf(z_alpha, loc=mu_pred, scale=sigma_pred))\n        \n        # Combine the results using the law of total probability.\n        # Pr(sig|Z_obs) = Pr(sig|H0)*Pr(H0|Z_obs) + Pr(sig|H1)*Pr(H1|Z_obs)\n        reprod_prob = alpha * lFDR + pr_h1_sig * (1 - lFDR)\n        results.append(reprod_prob)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{x:.6f}' for x in results)}]\")\n\nsolve()\n```"
        }
    ]
}