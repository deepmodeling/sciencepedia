{
    "hands_on_practices": [
        {
            "introduction": "RNA-sequencing has revolutionized our ability to profile the transcriptome, but interpreting its raw output requires careful normalization. Raw read counts are biased by factors like transcript length and library sequencing depth, making direct comparisons misleading. This exercise  guides you through the first principles of calculating Transcripts Per Million (TPM), a robust normalization method that corrects for these biases to provide a more accurate and intuitive measure of a gene's relative abundance within a sample.",
            "id": "4569575",
            "problem": "A clinical pharmacology laboratory is quantifying hepatic messenger ribonucleic acid (mRNA) expression of drug-metabolizing enzymes and transporters using RNA sequencing (RNA-seq). The lab aims to compare expression levels across transcripts in a single sample without confounding by transcript length or sequencing depth. Starting from the central dogma of molecular biology (deoxyribonucleic acid transcribed to ribonucleic acid translated to protein) and the sampling nature of RNA-seq, define in words what Transcripts Per Million (TPM) and Fragments Per Kilobase per Million mapped fragments (FPKM) are, and explain conceptually how they differ from unnormalized fragment counts. Then, using only the core definitions, derive a computation that yields the TPM for a transcript from its observed fragment count and length together with the sample’s total length-normalized signal.\n\nYou are given paired-end RNA-seq data with total mapped fragments $N = 28 \\times 10^{6}$. For a hepatocyte sample used to inform dosing of a narrow-therapeutic-index drug, the following five transcripts are the only ones appreciably expressed for the purpose of this calculation:\n\n- CYP3A4: aligned fragments $r_{\\mathrm{A4}} = 380{,}000$, effective transcript length $\\ell_{\\mathrm{A4}} = 4.8 \\ \\mathrm{kb}$.\n- CYP2C9: aligned fragments $r_{\\mathrm{2C9}} = 140{,}000$, effective transcript length $\\ell_{\\mathrm{2C9}} = 3.2 \\ \\mathrm{kb}$.\n- CYP2D6: aligned fragments $r_{\\mathrm{2D6}} = 60{,}000$, effective transcript length $\\ell_{\\mathrm{2D6}} = 4.4 \\ \\mathrm{kb}$.\n- ABCB1: aligned fragments $r_{\\mathrm{ABCB1}} = 90{,}000$, effective transcript length $\\ell_{\\mathrm{ABCB1}} = 4.6 \\ \\mathrm{kb}$.\n- UGT1A1: aligned fragments $r_{\\mathrm{UGT1A1}} = 110{,}000$, effective transcript length $\\ell_{\\mathrm{UGT1A1}} = 2.0 \\ \\mathrm{kb}$.\n\nUsing a first-principles derivation consistent with the above definitions, compute the TPM for CYP3A4 in this sample, treating the five listed transcripts as constituting the entire expressed transcriptome for this calculation. Report the TPM as a dimensionless quantity and round your final numeric answer to four significant figures.",
            "solution": "The user has provided a problem that requires both conceptual explanation and quantitative calculation in the field of transcriptomics, specifically as applied to clinical pharmacology.\n\n### Step 1: Problem Validation\n\nFirst, I will extract the givens and validate the problem statement according to the specified criteria.\n\n#### Extracted Givens\n- **Context**: Quantification of hepatic messenger ribonucleic acid (mRNA) expression of drug-metabolizing enzymes and transporters using RNA sequencing (RNA-seq).\n- **Goal**: Compare expression levels across transcripts in a single sample, correcting for transcript length and sequencing depth.\n- **Conceptual Task**:\n    1.  Define Transcripts Per Million (TPM) and Fragments Per Kilobase per Million mapped fragments (FPKM).\n    2.  Explain their conceptual difference from unnormalized fragment counts.\n    3.  Derive the computation for TPM from first principles.\n- **Data (Paired-end RNA-seq)**:\n    - Total mapped fragments: $N = 28 \\times 10^{6}$.\n    - Assumption: The five listed transcripts constitute the entire expressed transcriptome for the calculation.\n    - Transcript-specific data:\n        - CYP3A4: aligned fragments $r_{\\mathrm{A4}} = 380{,}000$, effective transcript length $\\ell_{\\mathrm{A4}} = 4.8 \\ \\mathrm{kb}$.\n        - CYP2C9: aligned fragments $r_{\\mathrm{2C9}} = 140{,}000$, effective transcript length $\\ell_{\\mathrm{2C9}} = 3.2 \\ \\mathrm{kb}$.\n        - CYP2D6: aligned fragments $r_{\\mathrm{2D6}} = 60{,}000$, effective transcript length $\\ell_{\\mathrm{2D6}} = 4.4 \\ \\mathrm{kb}$.\n        - ABCB1: aligned fragments $r_{\\mathrm{ABCB1}} = 90{,}000$, effective transcript length $\\ell_{\\mathrm{ABCB1}} = 4.6 \\ \\mathrm{kb}$.\n        - UGT1A1: aligned fragments $r_{\\mathrm{UGT1A1}} = 110{,}000$, effective transcript length $\\ell_{\\mathrm{UGT1A1}} = 2.0 \\ \\mathrm{kb}$.\n- **Calculation Task**: Compute the TPM for CYP3A4.\n- **Reporting Requirement**: Report the final numeric answer as a dimensionless quantity rounded to four significant figures.\n\n#### Validation\n1.  **Scientifically Grounded**: The problem is based on established principles of molecular biology (the central dogma, RNA-seq) and bioinformatics. TPM and FPKM are standard, well-defined metrics in transcriptomics. The genes listed are authentic and of high relevance to clinical pharmacology. The data values are plausible for a simplified but realistic experimental scenario.\n2.  **Well-Posed**: The problem is self-contained and provides all necessary information for the calculation. The instruction to \"treat the five listed transcripts as constituting the entire expressed transcriptome for this calculation\" clarifies the computational scope, rendering the total mapped fragments value $N$ as extraneous but not contradictory information. The objective is clearly stated, and a unique solution exists.\n3.  **Objective**: The language is precise, technical, and free of bias or subjective claims.\n\nThe problem is deemed **valid** as it is scientifically sound, well-posed, and objective. It contains no inconsistencies or ambiguities that would prevent a rigorous solution.\n\n### Step 2: Solution\n\n#### Conceptual Definitions and Derivation\n\nThe central principle of RNA-seq is that it is a sampling process. The number of sequence fragments (or reads) that align to a particular transcript is a function of that transcript's abundance in the original cellular RNA pool and its length. Longer transcripts provide a larger target for the random fragmentation process that precedes sequencing. Therefore, raw fragment counts are not directly comparable.\n\n- **Unnormalized Fragment Counts**: This is the raw number of fragments that align to a specific transcript. This metric is biased by both transcript length (a long, moderately expressed transcript can yield more fragments than a short, highly expressed one) and sequencing depth (a sample sequenced to a greater depth will have higher counts for all transcripts).\n\n- **Fragments Per Kilobase per Million mapped fragments (FPKM)**: This metric normalizes for both length and sequencing depth. For a transcript $i$ with $r_i$ raw fragment counts and an effective length of $\\ell_i$ kilobases in a sample with a total of $N$ million mapped fragments, FPKM is calculated as:\n$$\n\\mathrm{FPKM}_i = \\frac{r_i}{\\ell_i \\times (N/10^6)}\n$$\nThe normalization is performed by first dividing the count by the transcript's length in kilobases (giving fragments per kilobase) and then dividing by the total number of mapped fragments in the library in millions (the sequencing depth factor). While FPKM corrects for both biases, it has a statistical deficiency: the sum of FPKM values across a sample is not constant and depends on the expression profile of the sample itself. This makes comparing the relative proportion of transcripts between samples problematic.\n\n- **Transcripts Per Million (TPM)**: This metric also normalizes for length and sequencing depth but in a different order, which results in a more stable and proportional measure.\n    1.  First, for every transcript $j$ in the sample, its raw fragment count $r_j$ is normalized by its length $\\ell_j$ (in kb). This value, $c_j = r_j/\\ell_j$, represents a length-normalized signal and is proportional to the molar abundance of the transcript.\n    2.  Next, these length-normalized signals are summed across all transcripts in the sample to obtain a total length-normalized signal: $S = \\sum_j c_j = \\sum_j \\frac{r_j}{\\ell_j}$.\n    3.  The proportion of each transcript is then calculated by dividing its individual length-normalized signal $c_i$ by the total signal $S$.\n    4.  Finally, this proportion is scaled by one million to give the TPM value.\n\nThis sequence of operations constitutes the first-principles derivation requested. The formula for the TPM of transcript $i$ is:\n$$\n\\mathrm{TPM}_i = \\left( \\frac{r_i / \\ell_i}{\\sum_j (r_j / \\ell_j)} \\right) \\times 10^6\n$$\nA key advantage of TPM is that the sum of the TPM values for all transcripts in a sample always equals $10^6$. This means that each TPM value represents a relative proportion of the total pool of transcripts, making it more suitable for comparing transcript proportions across different samples.\n\n#### Calculation of TPM for CYP3A4\n\nThe problem specifies that for this calculation, the transcriptome consists solely of the five listed transcripts. We will apply the derived TPM formula to compute the value for CYP3A4. Let the index $i$ refer to CYP3A4, and the index $j$ span the set of five transcripts: $\\{\\mathrm{A4}, \\mathrm{2C9}, \\mathrm{2D6}, \\mathrm{ABCB1}, \\mathrm{UGT1A1}\\}$.\n\nThe formula is:\n$$\n\\mathrm{TPM}_{\\mathrm{A4}} = \\frac{r_{\\mathrm{A4}} / \\ell_{\\mathrm{A4}}}{\\left( \\frac{r_{\\mathrm{A4}}}{\\ell_{\\mathrm{A4}}} + \\frac{r_{\\mathrm{2C9}}}{\\ell_{\\mathrm{2C9}}} + \\frac{r_{\\mathrm{2D6}}}{\\ell_{\\mathrm{2D6}}} + \\frac{r_{\\mathrm{ABCB1}}}{\\ell_{\\mathrm{ABCB1}}} + \\frac{r_{\\mathrm{UGT1A1}}}{\\ell_{\\mathrm{UGT1A1}}} \\right)} \\times 10^6\n$$\n\nFirst, we calculate the length-normalized signal for each transcript ($c_j = r_j / \\ell_j$):\n- $c_{\\mathrm{A4}} = \\frac{380000}{4.8} = 79166.666...$\n- $c_{\\mathrm{2C9}} = \\frac{140000}{3.2} = 43750$\n- $c_{\\mathrm{2D6}} = \\frac{60000}{4.4} \\approx 13636.363...$\n- $c_{\\mathrm{ABCB1}} = \\frac{90000}{4.6} \\approx 19565.217...$\n- $c_{\\mathrm{UGT1A1}} = \\frac{110000}{2.0} = 55000$\n\nNext, we sum these values to find the total length-normalized signal, $S$:\n$$\nS = 79166.666... + 43750 + 13636.363... + 19565.217... + 55000\n$$\n$$\nS \\approx 211118.24769\n$$\n\nNow, we can compute the TPM for CYP3A4:\n$$\n\\mathrm{TPM}_{\\mathrm{A4}} = \\left( \\frac{c_{\\mathrm{A4}}}{S} \\right) \\times 10^6\n$$\n$$\n\\mathrm{TPM}_{\\mathrm{A4}} = \\left( \\frac{79166.666...}{211118.24769} \\right) \\times 10^6\n$$\n$$\n\\mathrm{TPM}_{\\mathrm{A4}} \\approx 0.375000083 \\times 10^6\n$$\n$$\n\\mathrm{TPM}_{\\mathrm{A4}} \\approx 375000.083\n$$\n\nRounding the result to four significant figures as required:\n$$\n\\mathrm{TPM}_{\\mathrm{A4}} = 3.750 \\times 10^5\n$$\nThis value is dimensionless, as the units in the numerator and denominator of the proportion cancel out.",
            "answer": "$$\n\\boxed{3.750 \\times 10^5}\n$$"
        },
        {
            "introduction": "A central challenge in analyzing omics data is the sheer volume of simultaneous statistical tests, which inflates the risk of false positives. To confidently identify true biological signals, we must adjust for multiple comparisons. This practice problem  delves into the Benjamini-Hochberg procedure, a powerful method for controlling the False Discovery Rate (FDR), empowering you to derive adjusted $q$-values and make statistically sound conclusions from high-dimensional datasets.",
            "id": "4569608",
            "problem": "A randomized, placebo-controlled Phase II clinical pharmacology study evaluates the effect of a novel antihypertensive on the human plasma proteome to identify drug-perturbed proteins related to pharmacodynamic response. For each of $m$ candidate proteins, a two-sample statistical test is performed comparing post-dose abundance between drug and placebo arms, producing $p$-values. Assume the following foundational principles: under a true null hypothesis, the $p$-value is stochastically dominated by the uniform distribution on $[0,1]$ and is exactly uniform under the ideal test; and across proteins, $p$-values are independent. Define the False Discovery Rate (FDR) as $\\mathrm{FDR} = \\mathbb{E}\\!\\left[\\frac{V}{\\max(R,1)}\\right]$, where $V$ is the number of false rejections and $R$ is the total number of rejections.\n\nTask 1 (derivation): Starting from the above definitions and assumptions, derive the Benjamini–Hochberg (BH) step-up multiple testing rule that controls FDR at a target level $\\alpha$ under independence, making explicit the role of order statistics of the $p$-values, indicator variables for true nulls, and any inequalities you use. Then define the corresponding FDR-adjusted $q$-values for each ordered $p$-value as the minimal FDR level at which that hypothesis would be rejected by the derived rule.\n\nTask 2 (computation): In a targeted proteomics panel with $m = 12$ proteins, the sorted $p$-values are\n$$\n\\{\\,0.0004,\\;0.0012,\\;0.0025,\\;0.007,\\;0.011,\\;0.018,\\;0.032,\\;0.041,\\;0.056,\\;0.071,\\;0.12,\\;0.34\\,\\}.\n$$\nUsing your derivation, compute the BH $q$-values for these data. Report each $q$-value to four significant figures.\n\nTask 3 (decision): Using the computed $q$-values, identify discoveries at target FDR level $\\alpha = 0.05$ (that is, those proteins with $q \\leq 0.05$). Your final reported quantity must be the number of discoveries. Report this number as a pure count (no units). The final answer must be a single real number. If any intermediate numerical results require rounding, follow the specified four significant-figure rule.",
            "solution": "We begin from the formal definition of the False Discovery Rate (FDR) and the distributional properties of $p$-values. Let $m$ denote the total number of hypotheses, let $\\mathcal{H}_{0}$ denote the index set of true null hypotheses with cardinality $m_{0}$, and let $p_{1},\\dots,p_{m}$ denote the individual $p$-values. Under a true null hypothesis and an ideal valid test, $p$-values are independent and identically distributed as $\\mathrm{Uniform}(0,1)$, and under any alternative the $p$-values are stochastically smaller than uniform. The FDR is\n$$\n\\mathrm{FDR} \\equiv \\mathbb{E}\\!\\left[\\frac{V}{\\max(R,1)}\\right],\n$$\nwhere $V = \\sum_{j \\in \\mathcal{H}_{0}} \\mathbf{1}\\{\\text{hypothesis } j \\text{ is rejected}\\}$ and $R = \\sum_{j=1}^{m} \\mathbf{1}\\{\\text{hypothesis } j \\text{ is rejected}\\}$.\n\nDerivation of the Benjamini–Hochberg (BH) rule. Consider the order statistics of the $p$-values, denoted by $p_{(1)} \\leq p_{(2)} \\leq \\cdots \\leq p_{(m)}$, with corresponding hypotheses $H_{(1)},\\dots,H_{(m)}$. A step-up rule seeks a data-driven cutoff of the form $p_{(k)} \\leq \\tau_{k}$ with $\\tau_{k}$ increasing in $k$, so that if $H_{(k)}$ is rejected then so are all $H_{(i)}$ with $i \\leq k$.\n\nWe aim to construct $\\tau_{k}$ to control $\\mathrm{FDR}$ at a target $\\alpha \\in (0,1)$. A candidate is\n$$\n\\tau_{k} = \\frac{k}{m}\\,\\alpha.\n$$\nDefine the random index\n$$\n\\hat{k} \\equiv \\max\\{\\,k \\in \\{1,\\dots,m\\}: p_{(k)} \\leq \\tfrac{k}{m}\\alpha\\,\\},\n$$\nwith the convention that $\\hat{k} = 0$ if the set is empty. The BH step-up rule rejects $H_{(1)},\\dots,H_{(\\hat{k})}$ and accepts the rest.\n\nTo show FDR control under independence, use indicator variables. Let $I_{j} = \\mathbf{1}\\{j \\in \\mathcal{H}_{0}\\}$ and let $R$ denote the number of rejections under the BH rule. Then\n$$\n\\frac{V}{\\max(R,1)} = \\sum_{j=1}^{m} \\frac{I_{j}\\,\\mathbf{1}\\{p_{j} \\leq \\tfrac{R}{m}\\alpha\\}}{\\max(R,1)}.\n$$\nTaking expectations and using symmetry across true nulls, independence, and the fact that under a true null $p_{j} \\sim \\mathrm{Uniform}(0,1)$ and is independent of the $p$-values of false nulls, we have\n$$\n\\mathrm{FDR} = \\mathbb{E}\\!\\left[\\sum_{j \\in \\mathcal{H}_{0}} \\frac{\\mathbf{1}\\{p_{j} \\leq \\tfrac{R}{m}\\alpha\\}}{\\max(R,1)}\\right]\n= \\sum_{j \\in \\mathcal{H}_{0}} \\mathbb{E}\\!\\left[\\frac{\\mathbf{1}\\{p_{j} \\leq \\tfrac{R}{m}\\alpha\\}}{\\max(R,1)}\\right].\n$$\nConditioning on the $p$-values other than $p_{j}$, the threshold $\\tfrac{R}{m}\\alpha$ is measurable with respect to those, and by independence we obtain\n$$\n\\mathbb{E}\\!\\left[\\left.\\frac{\\mathbf{1}\\{p_{j} \\leq \\tfrac{R}{m}\\alpha\\}}{\\max(R,1)}\\right| \\{p_{\\ell}:\\ell \\neq j\\}\\right]\n= \\mathbb{E}\\!\\left[\\left.\\mathbf{1}\\{p_{j} \\leq \\tfrac{R}{m}\\alpha\\}\\right| \\{p_{\\ell}:\\ell \\neq j\\}\\right]\\cdot \\frac{1}{\\max(R,1)}\n= \\frac{\\tfrac{R}{m}\\alpha}{\\max(R,1)},\n$$\nbecause under the null $p_{j} \\sim \\mathrm{Uniform}(0,1)$ and independent of $R$. Therefore,\n$$\n\\mathrm{FDR} = \\sum_{j \\in \\mathcal{H}_{0}} \\mathbb{E}\\!\\left[\\frac{\\tfrac{R}{m}\\alpha}{\\max(R,1)}\\right]\n= m_{0}\\,\\mathbb{E}\\!\\left[\\frac{\\tfrac{R}{m}\\alpha}{\\max(R,1)}\\right]\n\\leq m_{0}\\,\\mathbb{E}\\!\\left[\\frac{\\alpha}{m}\\right]\n= \\frac{m_{0}}{m}\\,\\alpha\n\\leq \\alpha.\n$$\nThe inequality uses that $\\tfrac{R}{\\max(R,1)} \\leq 1$. Thus, under independence, the BH step-up rule with $\\tau_{k} = \\tfrac{k}{m}\\alpha$ controls $\\mathrm{FDR} \\leq \\alpha$.\n\nDefinition of BH $q$-values. For the ordered $p$-values, define the BH-adjusted $q$-values as\n$$\nq_{(i)} \\equiv \\min_{j \\geq i} \\left\\{\\frac{m}{j}\\,p_{(j)}\\right\\},\n$$\nfollowed by the monotone adjustment to enforce nondecreasing $q_{(1)} \\leq q_{(2)} \\leq \\cdots \\leq q_{(m)}$ via the running minimum from the largest index downwards, and optionally capping at $1$.\n\nComputation for the given data. Here $m = 12$ and the sorted $p$-values are\n$$\np_{(1)}=0.0004,\\; p_{(2)}=0.0012,\\; p_{(3)}=0.0025,\\; p_{(4)}=0.007,\\; p_{(5)}=0.011,\\; p_{(6)}=0.018,\n$$\n$$\np_{(7)}=0.032,\\; p_{(8)}=0.041,\\; p_{(9)}=0.056,\\; p_{(10)}=0.071,\\; p_{(11)}=0.12,\\; p_{(12)}=0.34.\n$$\nCompute the initial factors $\\frac{m}{i}p_{(i)}$:\n$$\n\\begin{aligned}\n&\\frac{12}{1}\\cdot 0.0004 = 0.0048,\\;\\; \\frac{12}{2}\\cdot 0.0012 = 0.0072,\\;\\; \\frac{12}{3}\\cdot 0.0025 = 0.0100,\\\\\n&\\frac{12}{4}\\cdot 0.007 = 0.0210,\\;\\; \\frac{12}{5}\\cdot 0.011 = 0.0264,\\;\\; \\frac{12}{6}\\cdot 0.018 = 0.0360,\\\\\n&\\frac{12}{7}\\cdot 0.032 \\approx 0.054857,\\;\\; \\frac{12}{8}\\cdot 0.041 = 0.0615,\\;\\; \\frac{12}{9}\\cdot 0.056 \\approx 0.0746667,\\\\\n&\\frac{12}{10}\\cdot 0.071 = 0.0852,\\;\\; \\frac{12}{11}\\cdot 0.12 \\approx 0.130909,\\;\\; \\frac{12}{12}\\cdot 0.34 = 0.34.\n\\end{aligned}\n$$\nApply the running minimum from $i=12$ down to $i=1$ (and cap at $1$ if needed). The sequence is already nondecreasing when read from $i=1$ to $i=12$, so the monotone adjustment leaves the values unchanged. Rounding each to four significant figures yields\n$$\n\\begin{aligned}\n&q_{(1)}=0.004800,\\;\\; q_{(2)}=0.007200,\\;\\; q_{(3)}=0.01000,\\;\\; q_{(4)}=0.02100,\\\\\n&q_{(5)}=0.02640,\\;\\; q_{(6)}=0.03600,\\;\\; q_{(7)}=0.05486,\\;\\; q_{(8)}=0.06150,\\\\\n&q_{(9)}=0.07467,\\;\\; q_{(10)}=0.08520,\\;\\; q_{(11)}=0.1309,\\;\\; q_{(12)}=0.3400.\n\\end{aligned}\n$$\n\nDecision at target FDR $\\alpha = 0.05$. A discovery corresponds to $q \\leq 0.05$. From the rounded values, the discoveries are for indices $i \\in \\{1,2,3,4,5,6\\}$, since $q_{(6)} = 0.03600 \\leq 0.05$ and $q_{(7)} = 0.05486 > 0.05$. Therefore, the number of discoveries is $6$.\n\nBecause the requested final result is the count of discoveries, it is a pure number without units.",
            "answer": "$$\\boxed{6}$$"
        },
        {
            "introduction": "The ultimate goal of many omics studies in pharmacology is to enable personalized medicine. This exercise  provides a direct application of this principle by translating a patient's pharmacogenomic profile into a concrete clinical action. By applying the fundamental steady-state pharmacokinetic equation, you will calculate a precise dose adjustment for a patient with genetically impaired drug metabolism, illustrating how omics can directly guide safer and more effective therapy.",
            "id": "4569593",
            "problem": "A lipophilic oral drug used in chronic therapy is eliminated predominantly by the hepatic enzyme Cytochrome P450 family 2 subfamily D member 6 (CYP2D6). In extensive metabolizers, clinical pharmacokinetic characterization indicates that dosing $D_{\\mathrm{EM}} = 250\\,\\mathrm{mg}$ every $\\tau = 12\\,\\mathrm{h}$ with oral bioavailability $F = 0.80$ achieves the desired average steady-state concentration $C_{ss,\\mathrm{avg}}^{*}$ under linear pharmacokinetics. A patient’s whole-genome sequencing reveals a loss-of-function genotype that reduces clearance by $50\\%$ relative to extensive metabolizers, so that $CL_{\\mathrm{PM}} = 0.50 \\times CL_{\\mathrm{EM}}$. Assume linear pharmacokinetics, that $\\tau$ and $F$ are unchanged by genotype, and that steady state is governed by a mass-balance in which the average input rate over a dosing interval equals the average elimination rate and elimination follows first-order kinetics with clearance.\n\nUsing these principles, determine the new maintenance dose $D_{\\mathrm{PM}}$ that will maintain the same target average steady-state concentration $C_{ss,\\mathrm{avg}}^{*}$ in the poor metabolizer. Round your answer to three significant figures and express the final dose in $\\mathrm{mg}$.",
            "solution": "The problem asks for the adjusted maintenance dose for a patient identified as a poor metabolizer of a drug, to achieve the same therapeutic target concentration as an extensive metabolizer. The solution is grounded in the fundamental principles of pharmacokinetics at steady state.\n\nFirst, the givens are extracted and validated.\n**Givens:**\n-   Extensive Metabolizer (EM) dose: $D_{\\mathrm{EM}} = 250\\,\\mathrm{mg}$\n-   Dosing interval: $\\tau = 12\\,\\mathrm{h}$\n-   Oral bioavailability: $F = 0.80$\n-   Target average steady-state concentration: $C_{ss,\\mathrm{avg}}^{*}$\n-   Relationship between clearances: $CL_{\\mathrm{PM}} = 0.50 \\times CL_{\\mathrm{EM}}$ for a Poor Metabolizer (PM).\n-   Assumptions: Linear pharmacokinetics, constant $F$ and $\\tau$, and steady-state is governed by mass balance where average input rate equals average elimination rate.\n\nThe problem is scientifically grounded, well-posed, and objective. It describes a standard clinical pharmacology scenario without inconsistencies or missing information. Therefore, the problem is valid, and a solution can be derived.\n\nThe core principle for determining the maintenance dose is the mass-balance equation at steady state. It states that the average rate of drug administration into the systemic circulation must equal the average rate of drug elimination from the body.\n\nThe average rate of drug administration is given by the total amount of drug that becomes systemically available over a dosing interval, divided by that interval. This is expressed as $\\frac{F \\times D}{\\tau}$, where $D$ is the maintenance dose, $F$ is the bioavailability, and $\\tau$ is the dosing interval.\n\nThe average rate of elimination for a drug following first-order kinetics is the product of the drug's total clearance ($CL$) and its average steady-state plasma concentration ($C_{ss,\\mathrm{avg}}$). This is expressed as $CL \\times C_{ss,\\mathrm{avg}}$.\n\nEquating these two rates at steady state gives the fundamental relationship:\n$$\n\\frac{F \\times D}{\\tau} = CL \\times C_{ss,\\mathrm{avg}}\n$$\nThis equation can be rearranged to solve for the average steady-state concentration:\n$$\nC_{ss,\\mathrm{avg}} = \\frac{F \\times D}{CL \\times \\tau}\n$$\n\nFor an extensive metabolizer (EM), the given dosing regimen achieves the target average steady-state concentration, $C_{ss,\\mathrm{avg}}^{*}$. We can write this relationship as:\n$$\nC_{ss,\\mathrm{avg}}^{*} = \\frac{F \\times D_{\\mathrm{EM}}}{CL_{\\mathrm{EM}} \\times \\tau}\n$$\nHere, $D_{\\mathrm{EM}} = 250\\,\\mathrm{mg}$, $F = 0.80$, $\\tau = 12\\,\\mathrm{h}$, and $CL_{\\mathrm{EM}}$ is the clearance in extensive metabolizers.\n\nFor the poor metabolizer (PM), the goal is to find a new dose, $D_{\\mathrm{PM}}$, that achieves the same target concentration, $C_{ss,\\mathrm{avg}}^{*}$. The bioavailability $F$ and dosing interval $\\tau$ are assumed to remain constant. The clearance, however, is reduced, as given by $CL_{\\mathrm{PM}}$. The equation for the poor metabolizer is:\n$$\nC_{ss,\\mathrm{avg}}^{*} = \\frac{F \\times D_{\\mathrm{PM}}}{CL_{\\mathrm{PM}} \\times \\tau}\n$$\n\nSince the target concentration $C_{ss,\\mathrm{avg}}^{*}$ is the same for both patient types, we can equate the two expressions:\n$$\n\\frac{F \\times D_{\\mathrm{EM}}}{CL_{\\mathrm{EM}} \\times \\tau} = \\frac{F \\times D_{\\mathrm{PM}}}{CL_{\\mathrm{PM}} \\times \\tau}\n$$\nThe terms $F$ and $\\tau$ are common on both sides of the equation and can be cancelled, simplifying the relationship to:\n$$\n\\frac{D_{\\mathrm{EM}}}{CL_{\\mathrm{EM}}} = \\frac{D_{\\mathrm{PM}}}{CL_{\\mathrm{PM}}}\n$$\nThis equation demonstrates that to maintain a constant average steady-state concentration, the dose must be adjusted in direct proportion to the clearance.\n\nWe can now solve for the new dose, $D_{\\mathrm{PM}}$:\n$$\nD_{\\mathrm{PM}} = D_{\\mathrm{EM}} \\times \\frac{CL_{\\mathrm{PM}}}{CL_{\\mathrm{EM}}}\n$$\n\nThe problem states that the patient's genotype reduces clearance by $50\\%$ relative to extensive metabolizers, which is mathematically expressed as $CL_{\\mathrm{PM}} = 0.50 \\times CL_{\\mathrm{EM}}$. From this, we can find the ratio of the clearances:\n$$\n\\frac{CL_{\\mathrm{PM}}}{CL_{\\mathrm{EM}}} = 0.50\n$$\n\nSubstituting this ratio and the value of $D_{\\mathrm{EM}}$ into the equation for $D_{\\mathrm{PM}}$:\n$$\nD_{\\mathrm{PM}} = 250\\,\\mathrm{mg} \\times 0.50\n$$\n$$\nD_{\\mathrm{PM}} = 125\\,\\mathrm{mg}\n$$\nThe problem specifies the answer should be rounded to three significant figures. The calculated value, $125\\,\\mathrm{mg}$, already has three significant figures. Therefore, the new maintenance dose for the poor metabolizer should be $125\\,\\mathrm{mg}$.",
            "answer": "$$\\boxed{125}$$"
        }
    ]
}