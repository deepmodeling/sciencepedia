{
    "hands_on_practices": [
        {
            "introduction": "A robust Visual Predictive Check (VPC) relies on stable estimates of data quantiles, but what happens when data is sparse within a given time interval? This exercise explores the fundamental statistical challenge of quantile estimation with small sample sizes, a common issue in clinical studies. By working through this problem (), you will learn to quantify the variability of quantile estimators and understand why underpowered VPCs can be misleadingly optimistic, a phenomenon known as \"apparent overcoverage\".",
            "id": "4601292",
            "problem": "A clinical pharmacology team is preparing a visual predictive check (VPC) for a population pharmacokinetic model. In each time bin, the VPC displays simulated prediction bands for the empirical $p$-quantile (for example, the $10\\%$ and $90\\%$ quantiles) of the concentration distribution across replicates that mimic the observed sample size in that bin. The observed data at a representative time bin $\\mathcal{B}$ are generated from a continuous distribution with cumulative distribution function $F(x)$ and density $f(x)$. Let $q_p$ denote the true $p$-quantile, so that $F(q_p) = p$, and let $\\hat{q}_p$ denote the empirical $p$-quantile computed from $n$ observations in bin $\\mathcal{B}$. The team notes that when $n$ is small per bin, the simulated quantile bands are conspicuously wide, and the observed quantile curve appears to lie inside the simulated bands more often than expected (apparent overcoverage), even when the model is not clearly misspecified.\n\nExplain, from first principles, how small $n$ increases the variance of $\\hat{q}_p$ and why this can lead to apparent overcoverage in VPCs. Then, propose quantitative criteria for minimum bin counts. In particular, consider the following design target for bin $\\mathcal{B}$: the simulated $90\\%$ prediction band for the empirical $10\\%$-quantile at $\\mathcal{B}$ has half-width $B = 0.50$ mg/L, and the team requires that the $95\\%$ confidence interval half-width for $\\hat{q}_{0.1}$ should not exceed a fraction $r = 0.5$ of $B$. At the true lower quantile, the density is $f(q_{0.1}) = 0.50$ (mg/L)$^{-1}$. Based on a principled large-sample approximation, derive the minimum $n$ that satisfies the team’s requirement. Additionally, suggest a rule-of-thumb criterion based on expected tail counts $n p$ and $n (1-p)$, and describe a bootstrap-based alternative that can be applied in practice to verify adequacy of $n$ within each bin.\n\nWhich option best provides the correct mechanism and quantitatively justified criteria?\n\nA. Small $n$ inflates the sampling variability of $\\hat{q}_p$ because the empirical cumulative distribution function near $q_p$ is noisy, and the inverse mapping to $\\hat{q}_p$ amplifies this noise by a factor related to $1/f(q_p)$. Consequently, simulated bands for empirical quantiles widen as $n$ decreases, so an observed $\\hat{q}_p$ will appear inside the band more often (apparent overcoverage). A principled large-sample approximation yields the requirement $n \\ge \\left(\\dfrac{z_{0.975}\\sqrt{p(1-p)}}{r\\,B\\,f(q_p)}\\right)^2$, which for $p=0.1$, $f(q_{0.1})=0.50$ (mg/L)$^{-1}$, $B=0.50$ mg/L, and $r=0.5$ gives $n \\ge 22.1$, so a minimum of $n=23$ per bin. A complementary rule-of-thumb requires $n p \\ge 10$ and $n(1-p) \\ge 10$, implying $n \\ge \\max\\{10/p,\\,10/(1-p)\\} = 100$ for $p=0.1$. In practice, a nonparametric bootstrap within each bin can estimate the $95\\%$ confidence interval width for $\\hat{q}_p$; one can require the bootstrap $95\\%$ interval half-width to be $\\le r B$ to confirm adequacy of $n$.\n\nB. Small $n$ reduces the variance of $\\hat{q}_p$ because fewer points mean less randomness in the order statistics, leading to undercoverage. Therefore, a simple criterion of $n \\ge 10$ per bin is sufficient for any quantile, and bootstrap is unnecessary if the mean concentration is stable.\n\nC. Apparent overcoverage is primarily caused by model misspecification, not by sampling variability. The number of observations per bin does not materially change quantile variance, so no quantitative minimum $n$ is needed; instead, one should fit a more flexible model and avoid bootstrap because it only amplifies model errors.\n\nD. The variability relevant for VPC quantile bands is the variance of the mean concentration, which is $\\sigma^2/n$ by the Central Limit Theorem; quantiles have the same variance scaling. Using this, one can set $n$ so that $z_{0.975}\\,\\sigma/\\sqrt{n} \\le r B$, and $n \\ge 22$ suffices with typical $\\sigma$. There is no need to consider $f(q_p)$ or tail counts, and bootstrapping should focus on the mean, not the quantiles.",
            "solution": "The problem statement asks for a multi-part analysis regarding the sample size ($n$) requirements for constructing reliable visual predictive checks (VPCs) in clinical pharmacology, focusing on the variability of empirical quantiles.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- **Context**: Visual predictive check (VPC) for a population pharmacokinetic model.\n- **Phenomenon**: With small sample size $n$ per time bin, simulated prediction bands are wide, and the observed quantile curve appears to lie inside the bands more often than expected (apparent overcoverage).\n- **Stochastic Model**: Observations in a time bin $\\mathcal{B}$ are drawn from a continuous distribution with cumulative distribution function (CDF) $F(x)$ and probability density function (PDF) $f(x)$.\n- **Definitions**: $q_p$ is the true $p$-quantile where $F(q_p) = p$. $\\hat{q}_p$ is the empirical $p$-quantile from $n$ observations.\n- **Design Target**: The analysis focuses on the empirical $10\\%$-quantile ($\\hat{q}_{0.1}$) in bin $\\mathcal{B}$.\n- **Design Parameters**:\n    - The half-width of the simulated $90\\%$ prediction band for $\\hat{q}_{0.1}$ is $B = 0.50$ mg/L.\n    - The required precision is that the $95\\%$ confidence interval (CI) half-width for $\\hat{q}_{0.1}$ must not exceed a fraction $r = 0.5$ of $B$.\n    - The density at the true quantile is given as $f(q_{0.1}) = 0.50$ (mg/L)$^{-1}$.\n- **Tasks**:\n    1. Explain the relationship between small $n$, the variance of $\\hat{q}_p$, and apparent overcoverage.\n    2. Derive the minimum sample size $n$ based on the given design target using a large-sample approximation.\n    3. Suggest a rule-of-thumb based on expected tail counts.\n    4. Describe a bootstrap-based alternative for verifying the adequacy of $n$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically and mathematically sound. The phenomenon of \"apparent overcoverage\" due to high sampling variability of summary statistics in VPCs with sparse data is a known issue in pharmacometrics. The problem utilizes the well-established asymptotic theory for sample quantiles. All necessary quantitative information ($p$, $r$, $B$, $f(q_{0.1})$, and confidence level for the CI) is provided to perform the required derivation. The definitions and tasks are clear, unambiguous, and self-contained. The problem is well-posed and relevant to the specified field.\n\n**Step 3: Verdict and Action**\nThe problem is valid. The solution will proceed with derivation and analysis.\n\n### Solution Derivation\n\n**1. Theoretical Explanation: Variance of $\\hat{q}_p$ and Apparent Overcoverage**\n\nThe empirical $p$-quantile, $\\hat{q}_p$, is an estimator for the true population quantile, $q_p$. Its value is determined from the order statistics of a sample of size $n$. According to large-sample theory in statistics, for a sample from a continuous distribution with PDF $f(x)$, the sample quantile $\\hat{q}_p$ is asymptotically normally distributed around the true quantile $q_p$ with a variance given by:\n$$ \\text{Var}(\\hat{q}_p) \\approx \\frac{p(1-p)}{n [f(q_p)]^2} $$\nThis formula reveals two key dependencies:\n- **Inverse dependence on $n$**: The variance of the quantile estimator is inversely proportional to the sample size $n$. As $n$ becomes smaller, $\\text{Var}(\\hat{q}_p)$ increases significantly.\n- **Inverse dependence on $f(q_p)^2$**: The variance is inversely proportional to the square of the probability density at the quantile. For tail quantiles (where $p$ is close to $0$ or $1$), the density $f(q_p)$ is often low. This amplifies the variance, meaning that tail quantiles are inherently more variable and harder to estimate accurately than central quantiles like the median.\n\nIn a VPC, the prediction interval for an empirical quantile is constructed by simulating many datasets (e.g., $1000$ replicates) of size $n$ from the proposed model and calculating the empirical quantile for each. The $90\\%$ prediction interval, for instance, would be the range covering the central $90\\%$ of these simulated quantiles (e.g., from the $5^{th}$ percentile to the $95^{th}$ percentile of the simulated values).\n\nIf $n$ is small, the variance of these simulated quantiles is large, as per the formula above. This results in a very wide prediction interval. The observed empirical quantile, calculated from the original data (which also has only $n$ points in that bin), is just one draw from this high-variance distribution. A wide prediction interval has a high probability of containing this single observed value, often exceeding the nominal coverage probability (e.g., $>90\\%$). This phenomenon is termed \"apparent overcoverage\". It does not indicate that the model is particularly good; rather, it indicates that the diagnostic test (the VPC) has low power to detect model misspecification due to the high statistical noise associated with small $n$.\n\n**2. Derivation of Minimum Sample Size $n$**\n\nWe need to find the minimum $n$ such that the half-width of the $95\\%$ confidence interval for $\\hat{q}_{0.1}$ does not exceed $r \\times B$.\n\nThe asymptotic distribution of $\\hat{q}_p$ is $N(q_p, \\frac{p(1-p)}{n [f(q_p)]^2})$.\nThe standard error of $\\hat{q}_p$ is $\\text{SE}(\\hat{q}_p) = \\sqrt{\\text{Var}(\\hat{q}_p)} \\approx \\frac{\\sqrt{p(1-p)}}{\\sqrt{n} f(q_p)}$.\n\nA $(1-\\alpha) \\times 100\\%$ confidence interval for $q_p$ is given by $\\hat{q}_p \\pm z_{1-\\alpha/2} \\cdot \\text{SE}(\\hat{q}_p)$.\nThe half-width ($H$) of this CI is:\n$$ H = z_{1-\\alpha/2} \\frac{\\sqrt{p(1-p)}}{\\sqrt{n} f(q_p)} $$\nThe problem specifies a $95\\%$ CI, so $\\alpha = 0.05$ and $z_{1-\\alpha/2} = z_{0.975} \\approx 1.960$.\n\nThe requirement is $H \\le r B$. Substituting the expression for $H$:\n$$ z_{0.975} \\frac{\\sqrt{p(1-p)}}{\\sqrt{n} f(q_p)} \\le r B $$\nWe solve this inequality for $n$:\n$$ \\sqrt{n} \\ge \\frac{z_{0.975} \\sqrt{p(1-p)}}{r B f(q_p)} $$\n$$ n \\ge \\left( \\frac{z_{0.975} \\sqrt{p(1-p)}}{r B f(q_p)} \\right)^2 $$\nNow, we substitute the given values:\n- $p = 0.1$, so $\\sqrt{p(1-p)} = \\sqrt{0.1 \\times 0.9} = \\sqrt{0.09} = 0.3$.\n- $z_{0.975} = 1.960$.\n- $r = 0.5$.\n- $B = 0.50$ mg/L.\n- $f(q_{0.1}) = 0.50$ (mg/L)$^{-1}$.\n\nPlugging these into the formula:\n$$ n \\ge \\left( \\frac{1.960 \\times 0.3}{0.5 \\times 0.50 \\times 0.50} \\right)^2 $$\n$$ n \\ge \\left( \\frac{0.588}{0.125} \\right)^2 $$\n$$ n \\ge (4.704)^2 $$\n$$ n \\ge 22.127... $$\nSince the sample size $n$ must be an integer, the minimum required number of observations per bin is $n=23$.\n\n**3. Rule-of-Thumb Criterion**\n\nThe normal approximation used for the distribution of $\\hat{q}_p$ is derived from the normal approximation to the binomial distribution. This approximation is generally considered valid when the expected number of counts on either side of the quantile is sufficiently large. A common rule-of-thumb is that the expected counts, $np$ and $n(1-p)$, should both be at least $5$ or, more conservatively, $10$.\n\nUsing the stricter criterion of $10$:\n$$ np \\ge 10 \\quad \\text{and} \\quad n(1-p) \\ge 10 $$\nThis implies:\n$$ n \\ge \\frac{10}{p} \\quad \\text{and} \\quad n \\ge \\frac{10}{1-p} $$\nSo, we must satisfy $n \\ge \\max\\left\\{\\frac{10}{p}, \\frac{10}{1-p}\\right\\}$.\nFor the $10\\%$-quantile ($p=0.1$):\n$$ n \\ge \\max\\left\\{\\frac{10}{0.1}, \\frac{10}{1-0.1}\\right\\} = \\max\\{100, 11.11...\\} $$\nThus, this rule-of-thumb suggests a minimum sample size of $n=100$ to ensure the validity of the normal approximation for the $10^{th}$ percentile. This is a general-purpose guideline for estimator validity, distinct from the precision-based calculation in Part 2.\n\n**4. Bootstrap-Based Alternative**\n\nThe asymptotic formula requires knowing the value of the PDF at the quantile, $f(q_p)$, which is rarely known in practice. A nonparametric bootstrap provides a practical, data-driven alternative to assess the adequacy of $n$. For a given bin with $n$ observations:\n1.  Generate a large number of bootstrap samples (e.g., $B_{boot} = 1000$) by resampling $n$ observations with replacement from the original $n$ data points in the bin.\n2.  For each bootstrap sample, compute the empirical $p$-quantile, $\\hat{q}_p^*$.\n3.  This process yields a distribution of $B_{boot}$ bootstrap quantiles, which approximates the sampling distribution of $\\hat{q}_p$.\n4.  A $95\\%$ confidence interval for $q_p$ can be constructed from this distribution, for example, by taking the $2.5^{th}$ and $97.5^{th}$ percentiles of the bootstrap quantile values (the percentile method).\n5.  The half-width of this bootstrap CI can then be calculated.\n6.  This empirical half-width can be compared directly against the design target: is the bootstrap CI half-width $\\le rB$? If so, the sample size $n$ in that bin is considered adequate for the desired level of precision.\n\n### Option Analysis\n\n**A. Small $n$ inflates the sampling variability of $\\hat{q}_p$...**\nThis option correctly states that small $n$ increases the variance of $\\hat{q}_p$, mentioning the role of $1/f(q_p)$. It correctly explains that this leads to wider simulated bands and apparent overcoverage. It presents the correct formula for the minimum $n$ based on the large-sample approximation. The calculation $n \\ge 22.1$, leading to a minimum of $n=23$, is correct. The rule-of-thumb calculation $n \\ge \\max\\{10/0.1, 10/0.9\\} = 100$ is also correct. Finally, it provides a correct description of using a nonparametric bootstrap to verify the adequacy of $n$ based on the CI width. This option aligns perfectly with the derived solution.\n**Verdict: Correct.**\n\n**B. Small $n$ reduces the variance of $\\hat{q}_p$...**\nThis option claims that small $n$ *reduces* variance, which is fundamentally incorrect. The variance of any standard statistical estimator, including the sample quantile, increases as sample size decreases. This false premise invalidates the entire option. The proposed criterion of $n \\ge 10$ is arbitrary and not generally sufficient.\n**Verdict: Incorrect.**\n\n**C. Apparent overcoverage is primarily caused by model misspecification...**\nThis option misattributes apparent overcoverage. While model misspecification is a primary concern in modeling, \"apparent overcoverage\" is a specific artifact of high sampling variability due to sparse data, which can mask misspecification. The claim that $n$ does not materially change quantile variance is a direct contradiction of statistical theory.\n**Verdict: Incorrect.**\n\n**D. The variability relevant for VPC quantile bands is the variance of the mean concentration...**\nThis option incorrectly equates the variance of a sample quantile with the variance of a sample mean ($\\sigma^2/n$). The variance of a sample quantile has a different formula, which critically depends on the PDF at the quantile, $f(q_p)$. Ignoring $f(q_p)$ and applying the Central Limit Theorem for the mean to a problem about quantiles is a major conceptual error.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "After establishing the need for a sufficient number of observations per bin, the next critical decision is how to define the bins themselves. This problem () challenges you to compare common binning strategies—equal-width, equal-count, and k-means—in the context of a realistic pharmacokinetic scenario with non-uniform sampling. Analyzing the trade-offs between bias and variance for each method will equip you to make informed, defensible choices when designing your own VPCs.",
            "id": "4601332",
            "problem": "In a Visual Predictive Check (VPC) for a nonlinear mixed effects pharmacokinetic model, you compute per-bin sample quantiles of observed concentration given time, denoted by $Q_{p}(t)$ for $p \\in \\{0.10, 0.50, 0.90\\}$, to compare with simulated prediction intervals. Observed sampling times $t$ are strongly right-skewed: approximately $80\\%$ of observations lie in $t \\in [0,8]$ hours, with dense early sampling and multiple protocol-driven time points, while the remaining $20\\%$ lie in $t \\in (8,48]$ hours with sparser, uneven sampling and gaps. The conditional distribution of concentration $C$ given $t$ is continuous, and its quantile function $Q_{p}(t)$ decreases smoothly in $t$, with a steeper decline for small $t$ and a gradual flattening for larger $t$. You must choose a binning strategy for $t$ to compute per-bin quantile estimates for the VPC and to generate nonparametric bootstrap confidence intervals within each bin using $B_{\\text{boot}}$ bootstrap resamples.\n\nConsider three ways to partition the $t$-axis into $B$ bins:\n- Equal-width binning: divide the observed range of $t$ into $B$ intervals of equal length.\n- Equal-count binning: divide the ordered $t$ values into $B$ adjacent bins each containing the same number $n_{b}$ of observations (up to rounding by at most $1$).\n- $k$-means binning on $t$: apply $k$-means clustering with Euclidean distance on the scalar $t$ using $k=B$, assigning each observation to a cluster and taking each cluster’s convex hull on $t$ as a bin.\n\nAssume within-bin nonparametric bootstrap resampling is used to quantify uncertainty in per-bin sample quantiles, with resample size equal to the original per-bin count $n_{b}$. Using only fundamental properties of quantiles, mixtures, and large-sample behavior of sample quantiles, compare these three binning methods in terms of bias and variance of the per-bin quantile estimators and the stability of the bootstrap confidence intervals, given the stated distributional properties of $t$ and $Q_{p}(t)$. Which choice is most appropriate for this VPC, and why?\n\nSelect the single best option.\n\nA. Equal-width binning is most appropriate because equal time span per bin guarantees both minimal bias of per-bin sample quantiles and approximately equal variance across bins, leading to uniformly narrow bootstrap confidence intervals even under right-skewed $t$.\n\nB. Equal-count binning is most appropriate because it equalizes per-bin sample size $n_{b}$ and therefore stabilizes the variance of per-bin quantile estimators and their bootstrap confidence intervals; with right-skewed $t$ and rapidly changing $Q_{p}(t)$ at small $t$, equal-count bins become narrower early (limiting bias where curvature is high) and wider late (where curvature is low, keeping bias small).\n\nC. $k$-means binning is most appropriate because it ensures both equal bin widths on $t$ and equal per-bin counts, simultaneously minimizing bias and variance of per-bin quantiles and equalizing bootstrap interval widths.\n\nD. $k$-means binning is most appropriate because minimizing within-bin squared distances on $t$ directly minimizes the bias of per-bin sample quantiles in the presence of trends in $Q_{p}(t)$, and therefore also yields uniform variance and bootstrap interval widths across bins regardless of the skewness of $t$.",
            "solution": "The goal is to compare binning strategies for $t$ in a Visual Predictive Check (VPC) based on their effects on bias and variance of per-bin quantile estimators and on the stability of bootstrap confidence intervals, given a strongly right-skewed distribution of $t$ and a smooth but nonstationary quantile function $Q_{p}(t)$ that changes rapidly for small $t$ and slowly for large $t$.\n\nWe proceed from fundamental definitions and well-tested asymptotic results:\n\n1. Definition of per-bin quantile targets and bias due to binning. For a bin indexed by $b$ with $t \\in I_{b}$ and empirical within-bin distribution $F_{t|b}$, the pooled data within the bin are draws from the conditional mixture distribution\n$$\nG_{b}(c) \\equiv \\int F_{C|t}(c \\mid t)\\,\\mathrm{d}F_{t|b}(t),\n$$\nwith density\n$$\ng_{b}(c) \\equiv \\int f_{C|t}(c \\mid t)\\,\\mathrm{d}F_{t|b}(t).\n$$\nThe per-bin sample quantile at level $p$, denoted $\\widehat{Q}_{p,b}$, is a consistent estimator of the mixture quantile $Q_{p,b}^{\\text{mix}}$, defined by $G_{b}\\!\\left(Q_{p,b}^{\\text{mix}}\\right)=p$. The scientifically relevant target for visualization is the pointwise conditional quantile $Q_{p}(t)$ as a function of $t$. Therefore, binning induces a bias if $Q_{p}(t)$ varies within $I_{b}$, because $Q_{p,b}^{\\text{mix}} \\neq Q_{p}(t_{0})$ for a generic $t_{0} \\in I_{b}$. For small bins and smooth $Q_{p}(t)$, a first-order approximation shows that the bias scales with within-bin variation of $t$ and the local slope or curvature of $Q_{p}(t)$. Hence, narrower bins where $|Q_{p}'(t)|$ or $|Q_{p}''(t)|$ is large reduce bias.\n\n2. Large-sample variance of sample quantiles. For independent and identically distributed samples of size $n_{b}$ from a continuous distribution with density $g_{b}$ positive at its $p$-th quantile $Q_{p,b}^{\\text{mix}}$, the asymptotic variance of the sample quantile satisfies the classical result\n$$\n\\mathrm{Var}\\!\\left(\\widehat{Q}_{p,b}\\right) \\approx \\frac{p(1-p)}{n_{b}\\,g_{b}\\!\\left(Q_{p,b}^{\\text{mix}}\\right)^{2}}.\n$$\nThus, for fixed $p$ and similar $g_{b}\\!\\left(Q_{p,b}^{\\text{mix}}\\right)$ across bins, the variance scales approximately as $1/n_{b}$. Consequently, per-bin sample size $n_{b}$ is the dominant driver of variance heterogeneity across bins. The nonparametric bootstrap within each bin resamples $n_{b}$ observations with replacement, so the bootstrap distribution and confidence interval widths largely reflect this variance; small $n_{b}$ yields unstable and wide intervals.\n\n3. Trade-off between within-bin width on $t$ (driving bias) and per-bin count $n_{b}$ (driving variance). Where $Q_{p}(t)$ changes rapidly (small $t$), bias control favors narrow bins; where $Q_{p}(t)$ changes slowly (large $t$), wider bins can be tolerated without incurring large bias. Where $t$ is sparse (large $t$), equal-width bins risk very small $n_{b}$, inflating variance and destabilizing the bootstrap.\n\nGiven the scenario: $t$ is strongly right-skewed with dense early sampling ($t \\in [0,8]$) and sparse late sampling ($t \\in (8,48]$). The quantile function $Q_{p}(t)$ decreases more steeply at small $t$ and flattens at larger $t$.\n\nWe now analyze each binning method.\n\nEqual-width binning. Equal-width bins impose the same time span per bin. In the dense early region, each bin will include many observations, so $n_{b}$ will be large and $\\mathrm{Var}\\!\\left(\\widehat{Q}_{p,b}\\right)$ will be small, producing narrow bootstrap intervals. In the sparse late region, some bins will have small $n_{b}$, inflating variance and widening bootstrap intervals, potentially to the point of instability if $n_{b}$ is too small for extreme quantiles. With respect to bias, equal-width bins control the maximum within-bin time span uniformly; however, since $Q_{p}(t)$ changes more rapidly at small $t$, the bias there can still be appreciable for a fixed bin width because bias is driven by the magnitude of $|Q_{p}'(t)|$ and $|Q_{p}''(t)|$. In contrast, at large $t$ where $Q_{p}(t)$ is flatter, equal-width bins incur less bias. Overall, equal-width binning yields pronounced variance heterogeneity across bins due to the right-skewed $t$, with high-variance, unstable late-time quantiles and bootstrap intervals. This violates the goal of stable, interpretable VPC intervals across $t$.\n\nEqual-count binning. Equal-count bins enforce $n_{b}$ nearly constant across bins, directly stabilizing $\\mathrm{Var}\\!\\left(\\widehat{Q}_{p,b}\\right)$ via the $1/n_{b}$ scaling and leading to bootstrap intervals with more uniform widths across $t$. Because $t$ is dense early, equal-count bins there will be narrow in $t$, which reduces bias where $Q_{p}(t)$ changes rapidly. Because $t$ is sparse late, equal-count bins will be wider, potentially increasing bias; however, in the stated scenario $Q_{p}(t)$ flattens at large $t$, so the bias penalty from wider late-time bins is mitigated. Net effect: reduced bias where it matters (small $t$), controlled bias where curvature is low (large $t$), and stabilized variance and bootstrap interval widths across bins.\n\n$k$-means binning on $t$. In one dimension with Euclidean distance, $k$-means partitions $t$ into contiguous intervals (Voronoi cells) with centroids located more densely where observations are dense. This tends to produce narrower bins in dense early regions and wider bins in sparse late regions, which qualitatively aligns with bias control similar to equal-count binning. However, $k$-means does not enforce equal per-bin counts $n_{b}$, nor does it directly account for $Q_{p}(t)$ or $f_{C|t}$. Consequently, per-bin sample sizes can still vary, particularly in the presence of gaps or clusters in $t$, leading to heterogeneous variances and bootstrap interval widths. Moreover, $k$-means minimizes within-bin squared distance on $t$ rather than directly minimizing the bias of mixture quantiles; while reduced within-bin spread in $t$ can reduce bias, there is no guarantee of variance stabilization or bias optimality relative to equal-count when $Q_{p}(t)$ curvature and sampling density vary in different ways.\n\nSynthesis. The fundamental variance relation $\\mathrm{Var}\\!\\left(\\widehat{Q}_{p,b}\\right) \\approx p(1-p)/\\left(n_{b}\\,g_{b}\\!\\left(Q_{p,b}^{\\text{mix}}\\right)^{2}\\right)$ highlights $n_{b}$ as the key driver of variance and bootstrap interval width. Equal-count binning directly stabilizes $n_{b}$, and its adaptive bin widths in $t$ naturally match the need for narrow bins where $Q_{p}(t)$ changes quickly and wider bins where it is flat. Equal-width binning fails to stabilize variance under skewed $t$, and $k$-means does not guarantee equal $n_{b}$ or uniform bootstrap stability. Therefore, equal-count best balances bias and variance for the stated $t$ distribution and $Q_{p}(t)$ behavior.\n\nOption-by-option analysis:\n\nA. Claims equal-width yields both minimal bias and approximately equal variance across bins under right-skewed $t$. This contradicts the variance scaling with $1/n_{b}$: equal-width produces large $n_{b}$ in early bins and small $n_{b}$ in late bins, hence heterogeneous variances and bootstrap widths. Bias is not uniformly minimal because $Q_{p}(t)$ changes rapidly at small $t$; fixed width can leave nontrivial bias early. Incorrect.\n\nB. Notes that equal-count stabilizes $n_{b}$, hence variance and bootstrap intervals, and that narrower early bins reduce bias where $Q_{p}(t)$ changes rapidly while wider late bins incur small bias where curvature is low. This aligns with the asymptotic variance relation and the bias considerations for mixtures. Correct.\n\nC. Asserts that $k$-means ensures both equal bin width and equal counts. In one-dimensional $k$-means, neither equal widths nor equal counts are guaranteed; counts and widths adapt to local density without equality constraints. Thus the stated reasons are false. Incorrect.\n\nD. Claims $k$-means directly minimizes quantile bias and yields uniform variance and bootstrap widths regardless of skewness. $k$-means minimizes within-bin squared deviations in $t$, not quantile bias, and does not ensure equal $n_{b}$ or uniform bootstrap widths, particularly with skewed $t$ and gaps. Incorrect.\n\nTherefore, the most appropriate choice is equal-count binning.",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "Moving from theory to application, this final exercise () asks you to implement a partial VPC and bootstrap analysis from the ground up. You will simulate pharmacokinetic data, apply mechanistically justified time windows, generate prediction intervals, and use bootstrapping to assess the uncertainty in model performance. This comprehensive coding task integrates the core concepts of the chapter into a practical workflow representative of real-world pharmacometric analysis.",
            "id": "4601305",
            "problem": "A single-dose oral pharmacokinetic scenario is considered under a one-compartment model with first-order absorption and first-order elimination. The structural concentration-time function for dose $D$ administered at time $t = 0$ is given by the well-tested formula derived from mass balance and linear disposition for a one-compartment model with first-order absorption:\n$$\nC(t) = \\begin{cases}\n\\frac{D K_a}{V (K_a - k)} \\left( e^{-k t} - e^{-K_a t} \\right), & K_a \\neq k, \\\\\n\\frac{D}{V} K_a t e^{-k t}, & K_a = k,\n\\end{cases}\n$$\nwhere $K_a$ is the first-order absorption rate constant in $\\mathrm{h}^{-1}$, $CL$ is the clearance in $\\mathrm{L}\\,\\mathrm{h}^{-1}$, $V$ is the volume of distribution in $\\mathrm{L}$, and $k = CL / V$ is the elimination rate constant in $\\mathrm{h}^{-1}$. The absorption half-life is $t_{1/2,a} = \\ln(2)/K_a$, and the elimination half-life is $t_{1/2,e} = \\ln(2)/k$. The time to maximum concentration under $K_a \\neq k$ is\n$$\nt_{\\max} = \\frac{\\ln(K_a/k)}{K_a - k}.\n$$\n\nA Visual Predictive Check (VPC) is a simulation-based diagnostic in which one generates multiple simulated datasets under a model and compares observed data to simulated quantiles or prediction intervals. Here, a partial VPC is conducted by restricting evaluation to pharmacologically-predefined early and late time windows to avoid cherry-picking. The windows are defined by mechanistic rationale:\n- Early window: $[0, n_a \\cdot t_{1/2,a}]$, where $n_a$ is a positive scalar.\n- Late window: $[t_{\\max} + n_e \\cdot t_{1/2,e}, \\infty)$, where $n_e$ is a positive scalar.\n\nTo account for variability, interindividual variability is applied using a log-normal model for parameters $K_a$, $CL$, and $V$, i.e., $K_a^{(i)} = K_a \\exp(\\eta_{K_a}^{(i)})$, $CL^{(i)} = CL \\exp(\\eta_{CL}^{(i)})$, and $V^{(i)} = V \\exp(\\eta_{V}^{(i)})$, with $\\eta$ terms sampled from a normal distribution with mean $0$ and specified standard deviations $\\omega_{K_a}$, $\\omega_{CL}$, and $\\omega_V$. Residual unexplained variability is modeled using a combined error model such that the simulated observation $\\tilde{C}(t)$ is\n$$\n\\tilde{C}(t) = C(t)\\left(1 + \\epsilon_{\\mathrm{prop}}\\right) + \\epsilon_{\\mathrm{add}},\n$$\nwith $\\epsilon_{\\mathrm{prop}} \\sim \\mathcal{N}(0, \\sigma_{\\mathrm{prop}}^2)$ and $\\epsilon_{\\mathrm{add}} \\sim \\mathcal{N}(0, \\sigma_{\\mathrm{add}}^2)$, independently.\n\nFor a two-sided prediction interval of nominal coverage $1 - \\alpha$ (e.g., $0.90$ when $\\alpha = 0.10$), let $q_{\\mathrm{low}}$ and $q_{\\mathrm{high}}$ be the lower and upper quantile limits of the simulated concentrations pooled across replicates within the defined window. The observed concentrations are generated once using the population parameters (i.e., without interindividual variability) plus residual error as above. The coverage proportion is computed as the fraction of observed windowed concentrations lying in $[q_{\\mathrm{low}}, q_{\\mathrm{high}}]$. To quantify uncertainty in the observed coverage, a nonparametric bootstrap resamples the observed windowed concentrations with replacement $B$ times; each bootstrap sample yields a coverage proportion, forming a bootstrap distribution from which the two-sided interval using quantiles at $0.025$ and $0.975$ is obtained. A test case is considered passing if the nominal target $1 - \\alpha$ lies within this bootstrap interval. If the window contains no observed time points, return the sentinel coverage value $-1.0$ and a pass indicator of $0$.\n\nImplement a program that performs the above partial VPC and bootstrap procedure for each of the specified test cases. All times are in $\\mathrm{h}$ and concentrations are in $\\mathrm{mg}\\,\\mathrm{L}^{-1}$. The program must use a fixed random seed for reproducibility. Use inclusive window boundaries.\n\nTest suite (each case provides parameters and specifies whether the early or late window is assessed):\n- Case $1$ (early window, happy path):\n  - $D = 100$ $\\mathrm{mg}$, $K_a = 1.0$ $\\mathrm{h}^{-1}$, $CL = 5.0$ $\\mathrm{L}\\,\\mathrm{h}^{-1}$, $V = 50.0$ $\\mathrm{L}$.\n  - Observation times $\\{0.25, 0.5, 1, 2, 4, 6, 8, 12, 24\\}$ $\\mathrm{h}$.\n  - $n_a = 3.0$, $n_e = 1.0$, window type early.\n  - Interindividual variability standard deviations $\\omega_{K_a} = 0.2$, $\\omega_{CL} = 0.2$, $\\omega_V = 0.2$.\n  - Residual error $\\sigma_{\\mathrm{prop}} = 0.2$, $\\sigma_{\\mathrm{add}} = 0.05$ $\\mathrm{mg}\\,\\mathrm{L}^{-1}$.\n  - Simulation replicates $N_{\\mathrm{sim}} = 1000$, bootstrap resamples $B = 400$, $\\alpha = 0.10$.\n\n- Case $2$ (late window, happy path):\n  - Same as Case $1$, window type late, $n_e = 1.0$.\n\n- Case $3$ (late window, edge case with no windowed observations):\n  - Same as Case $1$, window type late, $n_e = 100.0$.\n\n- Case $4$ (early window, boundary condition inclusion):\n  - $D = 100$ $\\mathrm{mg}$, $K_a = \\ln(2)$ $\\mathrm{h}^{-1}$, $CL = 5.0$ $\\mathrm{L}\\,\\mathrm{h}^{-1}$, $V = 50.0$ $\\mathrm{L}$.\n  - Observation times $\\{0.25, 0.5, 1, 2, 4, 6, 8, 12, 24\\}$ $\\mathrm{h}$.\n  - $n_a = 2.0$, $n_e = 1.0$, window type early.\n  - Interindividual variability and residual error as in Case $1$.\n  - $N_{\\mathrm{sim}} = 1000$, $B = 400$, $\\alpha = 0.10$.\n  - Note: $n_a \\cdot t_{1/2,a} = 2.0$ $\\mathrm{h}$, so the boundary time $t = 2.0$ $\\mathrm{h}$ must be included.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order:\n$[$coverage$_1$,pass$_1$,coverage$_2$,pass$_2$,coverage$_3$,pass$_3$,coverage$_4$,pass$_4]$,\nwhere coverage is a float in $\\left[0,1\\right]$ (or $-1.0$ if no windowed observations) and pass is an integer in $\\{0,1\\}$.",
            "solution": "The problem requires the implementation of a simulation and analysis workflow common in clinical pharmacology, specifically a partial Visual Predictive Check (VPC) with bootstrap analysis to assess the uncertainty of the model's predictive performance. The solution is designed by algorithmically formalizing the described pharmacokinetic model, statistical procedures, and evaluation criteria.\n\nThe core of the problem lies in a one-compartment pharmacokinetic model with first-order absorption and elimination. The concentration $C(t)$ of a drug at time $t$ following an oral dose $D$ is described by the Bateman function:\n$$\nC(t) = \\begin{cases}\n\\frac{D K_a}{V (K_a - k)} \\left( e^{-k t} - e^{-K_a t} \\right), & K_a \\neq k, \\\\\n\\frac{D}{V} K_a t e^{-k t}, & K_a = k,\n\\end{cases}\n$$\nwhere $K_a$ is the absorption rate constant, $V$ is the volume of distribution, and $k = CL / V$ is the elimination rate constant derived from clearance $CL$. For numerical stability, the case $K_a = k$ is handled separately, representing the mathematical limit of the first expression as $K_a \\to k$. This function is fundamental for all concentration predictions.\n\nThe analysis evaluates the model within specific time windows, mechanistically defined to separate the absorption and elimination phases of the drug's disposition.\n- The early window, focusing on absorption, is defined as $[0, n_a \\cdot t_{1/2,a}]$, where $t_{1/2,a} = \\ln(2)/K_a$ is the absorption half-life.\n- The late window, focusing on elimination, is defined as $[t_{\\max} + n_e \\cdot t_{1/2,e}, \\infty)$, where $t_{1/2,e} = \\ln(2)/k$ is the elimination half-life and $t_{\\max}$ is the time to maximum concentration. $t_{\\max}$ is computed as:\n$$\nt_{\\max} = \\begin{cases}\n\\frac{\\ln(K_a/k)}{K_a - k}, & K_a \\neq k, \\\\\n\\frac{1}{k}, & K_a = k.\n\\end{cases}\n$$\nThe use of population parameters for these calculations ensures consistent window definitions for a given model. All specified observation time points are filtered to retain only those falling within the inclusive boundaries of the calculated window for the given test case. If no time points fall within the window, the analysis for that case is terminated, returning sentinel values.\n\nThe simulation process introduces variability to reflect real-world populations. Interindividual variability (IIV) is modeled by assuming that individual-specific parameters are log-normally distributed around the population typical values. For a generic parameter $P$, an individual's parameter $P^{(i)}$ is simulated as $P^{(i)} = P \\exp(\\eta_P^{(i)})$, where $\\eta_P^{(i)}$ is a random deviate sampled from a normal distribution $\\mathcal{N}(0, \\omega_P^2)$, with $\\omega_P$ being the specified standard deviation for the IIV of that parameter. This is applied to $K_a$, $CL$, and $V$.\n\nResidual unexplained variability (RUV) accounts for measurement error and model misspecification. A combined proportional and additive error model is used. A simulated observation $\\tilde{C}(t)$ is generated from a true model-predicted concentration $C(t)$ by:\n$$\n\\tilde{C}(t) = C(t)\\left(1 + \\epsilon_{\\mathrm{prop}}\\right) + \\epsilon_{\\mathrm{add}},\n$$\nwhere $\\epsilon_{\\mathrm{prop}}$ and $\\epsilon_{\\mathrm{add}}$ are sampled independently from normal distributions $\\mathcal{N}(0, \\sigma_{\\mathrm{prop}}^2)$ and $\\mathcal{N}(0, \\sigma_{\\mathrm{add}}^2)$, respectively.\n\nThe core of the VPC is the creation of a prediction interval (PI). This is achieved by generating $N_{\\mathrm{sim}}$ sets of individual pharmacokinetic profiles. For each simulated individual $i=1, \\dots, N_{\\mathrm{sim}}$, we first sample their individual parameters $K_a^{(i)}, CL^{(i)}, V^{(i)}$ and then compute their concentration profile $C^{(i)}(t)$ at all windowed time points. RUV is then added to these predictions to yield simulated observations $\\tilde{C}^{(i)}(t)$. All simulated observations across all individuals and all time points within the window are pooled into a single large dataset. The lower and upper bounds of the PI, $q_{\\mathrm{low}}$ and $q_{\\mathrm{high}}$, are determined by calculating the $\\alpha/2$ and $1 - \\alpha/2$ quantiles of this pooled distribution, yielding a PI with a nominal coverage of $1 - \\alpha$.\n\nTo evaluate the model, a single \"observed\" dataset is generated. This is done by calculating the concentration profile using the population-typical parameters (i.e., no IIV) and adding a single realization of RUV at each windowed time point. The primary metric, the coverage proportion, is the fraction of these observed data points that fall within the PI, i.e., in the interval $[q_{\\mathrm{low}}, q_{\\mathrm{high}}]$.\n\nTo quantify the statistical uncertainty in this observed coverage proportion, a non-parametric bootstrap procedure is employed. The set of observed concentrations within the window is resampled with replacement $B$ times, creating $B$ bootstrap samples, each the same size as the original set of windowed observations. For each bootstrap sample, the coverage proportion is re-calculated using the original, fixed PI, $[q_{\\mathrm{low}}, q_{\\mathrm{high}}]$. This process generates a distribution of $B$ coverage proportions. A $95\\%$ confidence interval for the coverage proportion is then constructed by taking the $0.025$ and $0.975$ quantiles from this bootstrap distribution.\n\nFinally, a test case is deemed \"passing\" if the target nominal coverage, $1 - \\alpha$, lies within this $95\\%$ bootstrap confidence interval. This indicates that the observed coverage is statistically consistent with the expected coverage. The entire procedure is encapsulated in a program that iterates through each test case, reporting the calculated coverage proportion and the binary pass/fail indicator. A fixed random seed ensures the reproducibility of all stochastic elements of the simulation and analysis.",
            "answer": "```python\nimport numpy as np\n\ndef _concentration(t, D, Ka, V, k):\n    \"\"\"Calculates concentration using the one-compartment oral absorption model.\"\"\"\n    if V <= 0:\n        return np.inf\n    \n    # Use the appropriate formula based on whether Ka is close to k\n    # to maintain numerical stability.\n    if abs(Ka - k) < 1e-9:\n        concentration = (D / V) * Ka * t * np.exp(-k * t)\n    else:\n        # Pre-calculate common terms for efficiency.\n        factor = D * Ka / (V * (Ka - k))\n        concentration = factor * (np.exp(-k * t) - np.exp(-Ka * t))\n    \n    return concentration\n\ndef _tmax(Ka, k):\n    \"\"\"Calculates time to maximum concentration.\"\"\"\n    if abs(Ka - k) < 1e-9:\n        return 1.0 / k if k > 0 else 0.0\n    if Ka <= 0 or k <= 0 or Ka/k <= 0: # Ka/k can be negative if one is negative, guard log\n        return 0.0\n    return np.log(Ka / k) / (Ka - k)\n\ndef solve():\n    \"\"\"\n    Main function to run the VPC simulation and bootstrap analysis for all test cases.\n    \"\"\"\n    # Fixed random seed for reproducibility as required.\n    SEED = 0\n    rng = np.random.default_rng(SEED)\n\n    # Test suite definition as per the problem statement.\n    test_cases = [\n        # Case 1 (early window, happy path)\n        {'D': 100.0, 'Ka_pop': 1.0, 'CL_pop': 5.0, 'V_pop': 50.0,\n         'obs_times': np.array([0.25, 0.5, 1, 2, 4, 6, 8, 12, 24]),\n         'na': 3.0, 'ne': 1.0, 'window_type': 'early',\n         'omega_Ka': 0.2, 'omega_CL': 0.2, 'omega_V': 0.2,\n         'sigma_prop': 0.2, 'sigma_add': 0.05,\n         'N_sim': 1000, 'B': 400, 'alpha': 0.10},\n        # Case 2 (late window, happy path)\n        {'D': 100.0, 'Ka_pop': 1.0, 'CL_pop': 5.0, 'V_pop': 50.0,\n         'obs_times': np.array([0.25, 0.5, 1, 2, 4, 6, 8, 12, 24]),\n         'na': 3.0, 'ne': 1.0, 'window_type': 'late',\n         'omega_Ka': 0.2, 'omega_CL': 0.2, 'omega_V': 0.2,\n         'sigma_prop': 0.2, 'sigma_add': 0.05,\n         'N_sim': 1000, 'B': 400, 'alpha': 0.10},\n        # Case 3 (late window, edge case with no windowed observations)\n        {'D': 100.0, 'Ka_pop': 1.0, 'CL_pop': 5.0, 'V_pop': 50.0,\n         'obs_times': np.array([0.25, 0.5, 1, 2, 4, 6, 8, 12, 24]),\n         'na': 3.0, 'ne': 100.0, 'window_type': 'late',\n         'omega_Ka': 0.2, 'omega_CL': 0.2, 'omega_V': 0.2,\n         'sigma_prop': 0.2, 'sigma_add': 0.05,\n         'N_sim': 1000, 'B': 400, 'alpha': 0.10},\n        # Case 4 (early window, boundary condition inclusion)\n        {'D': 100.0, 'Ka_pop': np.log(2), 'CL_pop': 5.0, 'V_pop': 50.0,\n         'obs_times': np.array([0.25, 0.5, 1, 2, 4, 6, 8, 12, 24]),\n         'na': 2.0, 'ne': 1.0, 'window_type': 'early',\n         'omega_Ka': 0.2, 'omega_CL': 0.2, 'omega_V': 0.2,\n         'sigma_prop': 0.2, 'sigma_add': 0.05,\n         'N_sim': 1000, 'B': 400, 'alpha': 0.10},\n    ]\n\n    final_results = []\n\n    for case in test_cases:\n        # Unpack parameters for the current test case\n        k_pop = case['CL_pop'] / case['V_pop']\n        \n        # Calculate window boundaries based on population parameters\n        if case['window_type'] == 'early':\n            t_half_a = np.log(2) / case['Ka_pop'] if case['Ka_pop'] > 0 else 0\n            window_start, window_end = 0.0, case['na'] * t_half_a\n        else: # 'late'\n            tmax = _tmax(case['Ka_pop'], k_pop)\n            t_half_e = np.log(2) / k_pop if k_pop > 0 else np.inf\n            window_start, window_end = tmax + case['ne'] * t_half_e, np.inf\n\n        # Filter observation times to include only those within the window\n        obs_times_windowed = case['obs_times'][(case['obs_times'] >= window_start) & (case['obs_times'] <= window_end)]\n\n        # Handle the case where no observations are in the window\n        if len(obs_times_windowed) == 0:\n            final_results.extend([-1.0, 0])\n            continue\n            \n        # Generate \"observed\" data: population pred + residual error\n        c_pop_windowed = _concentration(obs_times_windowed, case['D'], case['Ka_pop'], case['V_pop'], k_pop)\n        eps_prop_obs = rng.normal(0, case['sigma_prop'], size=len(obs_times_windowed))\n        eps_add_obs = rng.normal(0, case['sigma_add'], size=len(obs_times_windowed))\n        c_obs_windowed = c_pop_windowed * (1 + eps_prop_obs) + eps_add_obs\n        c_obs_windowed = np.maximum(0, c_obs_windowed) # Concentrations cannot be negative\n\n        # Perform simulation for Prediction Interval (PI)\n        simulated_concentrations = []\n        for _ in range(case['N_sim']):\n            # Sample inter-individual variability (IIV)\n            eta_Ka, eta_CL, eta_V = rng.normal(0, [case['omega_Ka'], case['omega_CL'], case['omega_V']])\n            Ka_ind = case['Ka_pop'] * np.exp(eta_Ka)\n            CL_ind = case['CL_pop'] * np.exp(eta_CL)\n            V_ind = case['V_pop'] * np.exp(eta_V)\n            k_ind = CL_ind / V_ind if V_ind > 0 else np.inf\n            \n            # Calculate individual concentration predictions\n            c_ind_pred = _concentration(obs_times_windowed, case['D'], Ka_ind, V_ind, k_ind)\n            \n            # Add residual unexplained variability (RUV)\n            eps_prop = rng.normal(0, case['sigma_prop'], size=len(obs_times_windowed))\n            eps_add = rng.normal(0, case['sigma_add'], size=len(obs_times_windowed))\n            c_ind_sim = c_ind_pred * (1 + eps_prop) + eps_add\n            \n            simulated_concentrations.extend(np.maximum(0, c_ind_sim))\n\n        # Calculate Prediction Interval from pooled simulated data\n        q_low, q_high = np.quantile(simulated_concentrations, [case['alpha']/2, 1 - case['alpha']/2])\n        \n        # Calculate observed coverage\n        in_interval = (c_obs_windowed >= q_low) & (c_obs_windowed <= q_high)\n        observed_coverage = np.mean(in_interval)\n        \n        # Perform non-parametric bootstrap to find CI of coverage\n        n_obs_windowed = len(c_obs_windowed)\n        bootstrap_coverages = []\n        for _ in range(case['B']):\n            bootstrap_sample = rng.choice(c_obs_windowed, size=n_obs_windowed, replace=True)\n            bootstrap_coverage = np.mean((bootstrap_sample >= q_low) & (bootstrap_sample <= q_high))\n            bootstrap_coverages.append(bootstrap_coverage)\n            \n        ci_low, ci_high = np.quantile(bootstrap_coverages, [0.025, 0.975])\n        \n        # Determine pass/fail status\n        target_coverage = 1 - case['alpha']\n        is_pass = 1 if ci_low <= target_coverage <= ci_high else 0\n        \n        final_results.extend([observed_coverage, is_pass])\n\n    # Format and print the final output as a single line\n    print(f\"[{','.join(f'{x:.6f}' if isinstance(x, float) else str(x) for x in final_results)}]\")\n\nsolve()\n```"
        }
    ]
}