{
    "hands_on_practices": [
        {
            "introduction": "Nonlinear mixed-effects models can appear complex, but their components often have intuitive interpretations. This first practice focuses on a common structural model for drug clearance, demonstrating how a nonlinear, multiplicative relationship incorporating a covariate can be transformed into a linear equation. Mastering this fundamental skill of linearization is crucial for interpreting model parameters and understanding the ubiquitous log-normal assumption for inter-individual variability. ",
            "id": "4568900",
            "problem": "Consider a structural parameterization for individual clearance in a population pharmacokinetic study under the framework of Nonlinear Mixed-Effects Modeling (NLME). Let the individual clearance $CL_{i}$ be modeled as a multiplicative decomposition into a typical value and interindividual variability, together with an allometric covariate effect of body weight $WT_{i}$ referenced to $70$ kilograms. Specifically, assume the following scientifically grounded bases:\n\n- Allometric scaling from physiological theory: the typical metabolic rate scales with body mass as a power law with exponent $3/4$ for adult mammals.\n- Log-normal interindividual variability: multiplicative random effects on a positive parameter can be represented as an exponential of a mean-zero normal random variable, which implies additivity of noise on the logarithmic scale.\n- Independence of covariate and random effect: the body weight $WT_{i}$ is treated as an observed covariate independent of the random effect.\n\nStarting from these principles, let the parameterization be\n$$\nCL_{i} \\;=\\; \\theta_{CL} \\left(\\frac{WT_{i}}{70}\\right)^{\\beta} \\exp\\!\\big(\\eta_{CL,i}\\big),\n$$\nwhere $\\theta_{CL} \\!>\\! 0$ is the typical clearance at $WT=70$ kilograms, $\\beta$ is the allometric exponent, and $\\eta_{CL,i} \\sim \\mathcal{N}(0,\\omega_{CL}^{2})$ represents interindividual variability on the log scale.\n\nAssume the physiologically motivated allometric exponent $\\beta = 0.75$. Using only the bases above and general properties of the natural logarithm, derive a linear relationship between $\\ln(CL_{i})$ and $\\ln(WT_{i}/70)$ and identify the regression intercept and slope for the log-linear representation with additive error $\\eta_{CL,i}$. Provide your final answer as a row matrix containing, in order, the intercept and the slope. No numerical substitution is required, and no rounding is necessary.",
            "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded in established principles of pharmacokinetics and allometric scaling, well-posed with a unique solution, and free from ambiguity or contradiction. We may therefore proceed with the derivation.\n\nThe starting point is the given structural model for individual clearance, $CL_{i}$:\n$$\nCL_{i} = \\theta_{CL} \\left(\\frac{WT_{i}}{70}\\right)^{\\beta} \\exp(\\eta_{CL,i})\n$$\nwhere $CL_{i}$ is the clearance for the $i$-th individual, $\\theta_{CL}$ is the typical clearance for an individual with a body weight of $70$ kilograms, $WT_{i}$ is the body weight of the $i$-th individual, $\\beta$ is the allometric exponent, and $\\eta_{CL,i}$ is the random effect representing interindividual variability, with $\\eta_{CL,i} \\sim \\mathcal{N}(0, \\omega_{CL}^{2})$.\n\nThe objective is to derive a linear relationship between $\\ln(CL_{i})$ and $\\ln(WT_{i}/70)$. This process is achieved by applying the natural logarithm function to both sides of the equation.\n$$\n\\ln(CL_{i}) = \\ln\\left( \\theta_{CL} \\left(\\frac{WT_{i}}{70}\\right)^{\\beta} \\exp(\\eta_{CL,i}) \\right)\n$$\nUsing the property of logarithms that the logarithm of a product is the sum of the logarithms, $\\ln(a \\cdot b \\cdot c) = \\ln(a) + \\ln(b) + \\ln(c)$, we can expand the right-hand side:\n$$\n\\ln(CL_{i}) = \\ln(\\theta_{CL}) + \\ln\\left( \\left(\\frac{WT_{i}}{70}\\right)^{\\beta} \\right) + \\ln(\\exp(\\eta_{CL,i}))\n$$\nNext, we apply two further properties of logarithms. First, the power rule, $\\ln(x^{p}) = p \\ln(x)$, is applied to the second term. Second, the property that the natural logarithm is the inverse function of the exponential function, $\\ln(\\exp(x)) = x$, is applied to the third term. This yields:\n$$\n\\ln(CL_{i}) = \\ln(\\theta_{CL}) + \\beta \\ln\\left(\\frac{WT_{i}}{70}\\right) + \\eta_{CL,i}\n$$\nThis equation represents a linear model. A general linear regression model takes the form:\n$$\nY = \\text{Intercept} + (\\text{Slope}) \\cdot X + \\text{Error}\n$$\nBy comparing our derived equation to this general form, we can identify the corresponding components:\n- The dependent variable, $Y$, is $\\ln(CL_{i})$.\n- The independent variable, $X$, is $\\ln(WT_{i}/70)$.\n- The additive error term is $\\eta_{CL,i}$, which is consistent with the problem's premises as $\\eta_{CL,i}$ is a random variable with a mean of $0$.\n- The intercept is the constant term, which corresponds to $\\ln(\\theta_{CL})$.\n- The slope is the coefficient of the independent variable, which corresponds to $\\beta$.\n\nThe problem specifies to assume the physiologically motivated allometric exponent $\\beta = 0.75$. Substituting this value for the slope, we have:\nSlope $= \\beta = 0.75$.\nThe intercept remains in its symbolic form as $\\ln(\\theta_{CL})$, as no value for the typical clearance $\\theta_{CL}$ is provided.\n\nTherefore, the regression intercept and slope for the log-linear representation are $\\ln(\\theta_{CL})$ and $0.75$, respectively.",
            "answer": "$$\\boxed{\\begin{pmatrix} \\ln(\\theta_{CL}) & 0.75 \\end{pmatrix}}$$"
        },
        {
            "introduction": "A core strength of NLME is its ability to estimate parameters for an individual by \"borrowing strength\" from the entire population. This practice delves into the statistical engine that makes this possible, guiding you through the derivation of the objective function for Empirical Bayes Estimates (EBEs). By working through this, you will see precisely how information from an individual's data (the likelihood) is balanced against the population model (the prior), revealing the mathematical basis of mixed-effects estimation. ",
            "id": "4568879",
            "problem": "Consider a nonlinear mixed-effects model in clinical pharmacology where responses for individual $i$ at sampling times $\\{t_{ij}\\}_{j=1}^{n_i}$ follow the structural relationship $y_{ij} = f(t_{ij},\\phi_i) + \\epsilon_{ij}$, with measurement noise $\\epsilon_{ij}$ modeled as independent realizations of a Gaussian random variable with mean $0$ and variance $\\sigma^2$, that is, $\\epsilon_{ij} \\sim \\mathcal{N}(0,\\sigma^2)$ for all $j$. The individual-specific parameter vector $\\phi_i \\in \\mathbb{R}^{p}$ is determined by a differentiable link $\\Lambda(\\theta,\\eta_i)$, where $\\theta \\in \\mathbb{R}^{m}$ is a fixed-effects parameter vector and $\\eta_i \\in \\mathbb{R}^{q}$ is the individual random-effects vector modeled as $\\eta_i \\sim \\mathcal{N}(0,\\Omega)$ with a positive-definite covariance matrix $\\Omega \\in \\mathbb{R}^{q \\times q}$. Assume conditional independence of observations $y_{ij}$ given $\\phi_i$ and that $f(t,\\phi)$ is continuously differentiable in $\\phi$.\n\nStarting only from the definitions of the Gaussian probability density function and conditional independence, and invoking Bayes' theorem, derive the individual conditional log-likelihood $\\ell_i(\\theta,\\phi_i)$ of the observations $\\{y_{ij}\\}_{j=1}^{n_i}$ given $\\phi_i$ and the fixed-effects parameter $\\theta$. Then, using the prior for $\\eta_i$ and the link $\\phi_i=\\Lambda(\\theta,\\eta_i)$, derive the posterior objective in $\\eta_i$ whose minimizer yields the Empirical Bayes (EB) estimate of the random effects for individual $i$, and express this EB estimator as an optimization problem in $\\eta_i$.\n\nExpress your final answer as a single row matrix whose first entry is the derived analytic expression for the conditional log-likelihood $\\ell_i(\\theta,\\phi_i)$ (including additive constants), and whose second entry is the analytic expression of the EB estimator for $\\eta_i$ as an optimization problem. No numerical evaluation is required. Do not include any text or units in the final answer.",
            "solution": "The problem is valid as it is scientifically grounded in the established theory of nonlinear mixed-effects modeling, is well-posed with sufficient information for a unique derivation, and uses objective, formal language. We proceed with the solution.\n\nThe derivation consists of two main parts: first, the conditional log-likelihood of the observations for an individual, and second, the objective function for the Empirical Bayes (EB) estimation of the random effects for that individual.\n\n**Part 1: Derivation of the Conditional Log-Likelihood $\\ell_i(\\theta, \\phi_i)$**\n\nThe problem states that the structural model for the $j$-th observation for the $i$-th individual is $y_{ij} = f(t_{ij},\\phi_i) + \\epsilon_{ij}$. The measurement error, $\\epsilon_{ij}$, is given to be an independent realization from a Gaussian distribution with mean $0$ and variance $\\sigma^2$, denoted as $\\epsilon_{ij} \\sim \\mathcal{N}(0, \\sigma^2)$.\n\nFrom this definition, it follows that the conditional distribution of an observation $y_{ij}$ given the individual-specific parameter vector $\\phi_i$ is also Gaussian. The mean of this distribution is the predicted value from the structural model, $f(t_{ij}, \\phi_i)$, and the variance is $\\sigma^2$. Thus, we can write:\n$$y_{ij} | \\phi_i \\sim \\mathcal{N}(f(t_{ij}, \\phi_i), \\sigma^2)$$\n\nThe probability density function (PDF) for a single observation $y_{ij}$ conditional on $\\phi_i$ is given by the formula for a Gaussian PDF:\n$$p(y_{ij} | \\phi_i) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(y_{ij} - f(t_{ij}, \\phi_i))^2}{2\\sigma^2} \\right)$$\n\nThe problem states that the observations $\\{y_{ij}\\}_{j=1}^{n_i}$ for individual $i$ are conditionally independent given $\\phi_i$. Therefore, the joint conditional likelihood of all $n_i$ observations for individual $i$, denoted by $y_i = \\{y_{i1}, y_{i2}, \\dots, y_{in_i}\\}$, is the product of the individual PDFs:\n$$L_i(\\phi_i | y_i) = p(y_i | \\phi_i) = \\prod_{j=1}^{n_i} p(y_{ij} | \\phi_i)$$\nSubstituting the expression for the Gaussian PDF, we get:\n$$L_i(\\phi_i | y_i) = \\prod_{j=1}^{n_i} \\left[ \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(y_{ij} - f(t_{ij}, \\phi_i))^2}{2\\sigma^2} \\right) \\right]$$\nThis can be simplified by combining the terms:\n$$L_i(\\phi_i | y_i) = \\left( \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\right)^{n_i} \\exp\\left( -\\sum_{j=1}^{n_i} \\frac{(y_{ij} - f(t_{ij}, \\phi_i))^2}{2\\sigma^2} \\right)$$\n$$L_i(\\phi_i | y_i) = (2\\pi\\sigma^2)^{-n_i/2} \\exp\\left( -\\frac{1}{2\\sigma^2} \\sum_{j=1}^{n_i} (y_{ij} - f(t_{ij}, \\phi_i))^2 \\right)$$\n\nThe conditional log-likelihood, $\\ell_i$, is the natural logarithm of the conditional likelihood $L_i$. The problem asks for $\\ell_i(\\theta, \\phi_i)$. We note that conditioning on $\\phi_i$ makes the likelihood of the data $y_i$ independent of the fixed effects $\\theta$, so $p(y_i | \\phi_i, \\theta) = p(y_i | \\phi_i)$.\n$$\\ell_i(\\phi_i) = \\ln(L_i(\\phi_i | y_i)) = \\ln\\left[ (2\\pi\\sigma^2)^{-n_i/2} \\exp\\left( -\\frac{1}{2\\sigma^2} \\sum_{j=1}^{n_i} (y_{ij} - f(t_{ij}, \\phi_i))^2 \\right) \\right]$$\nUsing the properties of logarithms, $\\ln(ab) = \\ln(a) + \\ln(b)$ and $\\ln(e^x) = x$, we obtain:\n$$\\ell_i(\\phi_i) = \\ln((2\\pi\\sigma^2)^{-n_i/2}) - \\frac{1}{2\\sigma^2} \\sum_{j=1}^{n_i} (y_{ij} - f(t_{ij}, \\phi_i))^2$$\n$$\\ell_i(\\phi_i) = -\\frac{n_i}{2} \\ln(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{j=1}^{n_i} (y_{ij} - f(t_{ij}, \\phi_i))^2$$\nThis is the conditional log-likelihood for individual $i$, including all additive constants.\n\n**Part 2: Derivation of the Empirical Bayes (EB) Estimator Objective Function**\n\nThe Empirical Bayes estimate of the random effects vector $\\eta_i$ is its posterior mode, i.e., the value of $\\eta_i$ that maximizes the posterior probability density $p(\\eta_i | y_i, \\theta, \\Omega, \\sigma^2)$. The parameters $\\theta$, $\\Omega$, and $\\sigma^2$ are treated as fixed, known quantities for this estimation step.\n\nUsing Bayes' theorem, the posterior density is proportional to the product of the likelihood and the prior:\n$$p(\\eta_i | y_i, \\theta, \\Omega, \\sigma^2) \\propto p(y_i | \\eta_i, \\theta, \\sigma^2) \\cdot p(\\eta_i | \\Omega)$$\n\nThe first term, $p(y_i | \\eta_i, \\theta, \\sigma^2)$, is the likelihood of the data given the random effects. Since the individual parameters $\\phi_i$ are a deterministic function of $\\eta_i$ and $\\theta$ via the link $\\phi_i = \\Lambda(\\theta, \\eta_i)$, we have $p(y_i | \\eta_i, \\theta, \\sigma^2) = p(y_i | \\phi_i = \\Lambda(\\theta, \\eta_i), \\sigma^2)$. We can use the likelihood derived in Part 1 by substituting $\\phi_i = \\Lambda(\\theta, \\eta_i)$:\n$$p(y_i | \\eta_i, \\theta, \\sigma^2) = (2\\pi\\sigma^2)^{-n_i/2} \\exp\\left( -\\frac{1}{2\\sigma^2} \\sum_{j=1}^{n_i} (y_{ij} - f(t_{ij}, \\Lambda(\\theta, \\eta_i)))^2 \\right)$$\n\nThe second term, $p(\\eta_i | \\Omega)$, is the prior density of the random effects. We are given $\\eta_i \\sim \\mathcal{N}(0, \\Omega)$. For a $q$-dimensional multivariate normal distribution, the PDF is:\n$$p(\\eta_i | \\Omega) = \\frac{1}{\\sqrt{(2\\pi)^q \\det(\\Omega)}} \\exp\\left( -\\frac{1}{2}\\eta_i^T \\Omega^{-1} \\eta_i \\right)$$\nwhere $\\det(\\Omega)$ is the determinant of the covariance matrix $\\Omega$.\n\nMaximizing the posterior $p(\\eta_i | y_i, \\dots)$ is equivalent to maximizing its logarithm, $\\ln(p(\\eta_i | y_i, \\dots))$, which is in turn equivalent to minimizing its negative logarithm.\n$$\\ln(p(\\eta_i | y_i, \\dots)) = \\ln(p(y_i | \\eta_i, \\dots)) + \\ln(p(\\eta_i | \\dots)) + C$$\nwhere $C$ is a constant that does not depend on $\\eta_i$.\n$$-\\ln(p(\\eta_i | y_i, \\dots)) = -\\ln(p(y_i | \\eta_i, \\dots)) - \\ln(p(\\eta_i | \\dots)) - C$$\nSubstituting the expressions for the log-PDFs:\n$$-\\ln(p(\\eta_i | y_i, \\dots)) = \\left[ \\frac{n_i}{2} \\ln(2\\pi\\sigma^2) + \\frac{1}{2\\sigma^2} \\sum_{j=1}^{n_i} (y_{ij} - f(t_{ij}, \\Lambda(\\theta, \\eta_i)))^2 \\right] + \\left[ \\frac{1}{2}\\ln((2\\pi)^q \\det(\\Omega)) + \\frac{1}{2}\\eta_i^T \\Omega^{-1} \\eta_i \\right] - C$$\nTo find the value of $\\eta_i$ that minimizes this expression, we can drop all terms that are constant with respect to $\\eta_i$. This gives the objective function, $O_i(\\eta_i)$, to be minimized:\n$$O_i^{\\text{unscaled}}(\\eta_i) = \\frac{1}{2\\sigma^2} \\sum_{j=1}^{n_i} (y_{ij} - f(t_{ij}, \\Lambda(\\theta, \\eta_i)))^2 + \\frac{1}{2}\\eta_i^T \\Omega^{-1} \\eta_i$$\nMinimizing $O_i^{\\text{unscaled}}(\\eta_i)$ is equivalent to minimizing any positive multiple of it. It is conventional to work with an objective function that is scaled by a factor of $2$, which we will denote $O_i(\\eta_i)$:\n$$O_i(\\eta_i) = 2 \\cdot O_i^{\\text{unscaled}}(\\eta_i) = \\frac{1}{\\sigma^2} \\sum_{j=1}^{n_i} (y_{ij} - f(t_{ij}, \\Lambda(\\theta, \\eta_i)))^2 + \\eta_i^T \\Omega^{-1} \\eta_i$$\nThis is the posterior objective function. The first term is a weighted residual sum of squares, and the second is a penalty term shrinking the random effects towards their prior mean of zero.\n\nThe Empirical Bayes estimator for $\\eta_i$, denoted $\\hat{\\eta}_i^{EB}$, is the solution to the optimization problem of minimizing this objective function:\n$$\\hat{\\eta}_i^{EB} = \\arg\\min_{\\eta_i \\in \\mathbb{R}^q} O_i(\\eta_i)$$\nExplicitly, the estimator is expressed as the following optimization problem:\n$$\\hat{\\eta}_i^{EB} = \\arg\\min_{\\eta_i \\in \\mathbb{R}^q} \\left\\{ \\frac{1}{\\sigma^2} \\sum_{j=1}^{n_i} (y_{ij} - f(t_{ij}, \\Lambda(\\theta, \\eta_i)))^2 + \\eta_i^T \\Omega^{-1} \\eta_i \\right\\}$$\nThis completes the derivation.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n-\\frac{n_i}{2} \\ln(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{j=1}^{n_i} (y_{ij} - f(t_{ij},\\phi_i))^2 & \\arg\\min_{\\eta_i \\in \\mathbb{R}^q} \\left\\{ \\frac{1}{\\sigma^2} \\sum_{j=1}^{n_i} (y_{ij} - f(t_{ij}, \\Lambda(\\theta, \\eta_i)))^2 + \\eta_i^T \\Omega^{-1} \\eta_i \\right\\}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Clinical data are rarely perfect; observations often fall below the lower limit of quantification ($LLOQ$) of an assay. Instead of discarding or naively imputing these data, NLME models can incorporate this information correctly by treating it as \"censored.\" This exercise shows you how to derive the likelihood contribution for a censored observation, extending the principles from the previous practice to handle a common and important challenge in robust pharmacokinetic analysis. ",
            "id": "4568933",
            "problem": "In a Nonlinear Mixed-Effects (NLME) population pharmacokinetic model for clinical pharmacology, suppose the $i$-th individual has random effects $\\phi_i$ and the structural prediction at occasion $j$ is $f_{ij} = f(t_{ij}; \\phi_i)$. The observation model is defined by an additive residual error structure $y_{ij} = f_{ij} + \\epsilon_{ij}$, where $\\epsilon_{ij} \\sim \\mathcal{N}(0, \\sigma_{add}^{2})$ and is conditionally independent across occasions given $\\phi_i$. The assay has a Lower Limit of Quantification (LLOQ), and on occasion $j$ for individual $i$ the measurement is censored, meaning only the event $y_{ij} < LLOQ$ is observed. Starting from the fundamental definition that the likelihood contribution of a censored observation equals the probability of the observed event under the assumed conditional distribution of $y_{ij}$ given $\\phi_i$, derive a closed-form expression for the censored likelihood contribution $P(y_{ij} < LLOQ \\mid \\phi_i)$ in terms of the standard normal cumulative distribution function $\\Phi(\\cdot)$. Express your final answer solely in terms of $LLOQ$, $f_{ij}$, and $\\sigma_{add}$. Your final answer must be a single closed-form analytic expression.",
            "solution": "The problem is valid as it is scientifically grounded in the principles of pharmacokinetic modeling and statistics, is well-posed with a clear objective and sufficient information, and is expressed in objective, formal language. There are no contradictions, ambiguities, or factual inaccuracies. We may therefore proceed with the derivation.\n\nThe objective is to derive a closed-form expression for the likelihood contribution of a censored observation, which is defined as the probability of the observed event. The event is that the measurement $y_{ij}$ is below the Lower Limit of Quantification, $LLOQ$. The likelihood contribution for this censored observation on occasion $j$ for individual $i$ is therefore given by the conditional probability $P(y_{ij} < LLOQ \\mid \\phi_i)$.\n\nThe problem provides the structural model for the observation $y_{ij}$:\n$$\ny_{ij} = f_{ij} + \\epsilon_{ij}\n$$\nHere, $f_{ij}$ represents the model-predicted concentration for individual $i$ at occasion $j$, which is a function of the individual-specific random effects vector $\\phi_i$. The term $\\epsilon_{ij}$ is the additive residual error.\n\nThe distribution of the residual error is specified as a normal distribution with a mean of $0$ and a variance of $\\sigma_{add}^{2}$:\n$$\n\\epsilon_{ij} \\sim \\mathcal{N}(0, \\sigma_{add}^{2})\n$$\nThe residual errors $\\epsilon_{ij}$ are stated to be conditionally independent across occasions $j$ given $\\phi_i$.\n\nWe begin the derivation by substituting the observation model into the probability inequality:\n$$\nP(y_{ij} < LLOQ \\mid \\phi_i) = P(f_{ij} + \\epsilon_{ij} < LLOQ \\mid \\phi_i)\n$$\nThe conditioning is on the random effects $\\phi_i$. Since the structural prediction $f_{ij} = f(t_{ij}; \\phi_i)$ is a deterministic function of $\\phi_i$, within this conditional probability, $f_{ij}$ is treated as a fixed, known value. We can therefore rearrange the inequality to isolate the random variable $\\epsilon_{ij}$:\n$$\nP(f_{ij} + \\epsilon_{ij} < LLOQ \\mid \\phi_i) = P(\\epsilon_{ij} < LLOQ - f_{ij} \\mid \\phi_i)\n$$\nThe problem states that the residual errors $\\epsilon_{ij}$ are conditionally independent of each other given $\\phi_i$. The standard NLME model formulation further assumes that the residual error $\\epsilon_{ij}$ is independent of the random effects $\\phi_i$. Consequently, the distribution of $\\epsilon_{ij}$ does not depend on $\\phi_i$, and the conditioning on $\\phi_i$ can be removed from the probability statement involving only $\\epsilon_{ij}$:\n$$\nP(\\epsilon_{ij} < LLOQ - f_{ij} \\mid \\phi_i) = P(\\epsilon_{ij} < LLOQ - f_{ij})\n$$\nWe have a probability statement involving a normally distributed random variable, $\\epsilon_{ij} \\sim \\mathcal{N}(0, \\sigma_{add}^{2})$. To express this probability in terms of the standard normal cumulative distribution function (CDF), $\\Phi(\\cdot)$, we must standardize the random variable. A random variable $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$ can be standardized to a random variable $Z \\sim \\mathcal{N}(0, 1)$ by the transformation $Z = (X - \\mu) / \\sigma$.\n\nFor our variable $\\epsilon_{ij}$, the mean is $\\mu = 0$ and the standard deviation is $\\sigma = \\sqrt{\\sigma_{add}^{2}} = \\sigma_{add}$. The standardized variable is $\\frac{\\epsilon_{ij} - 0}{\\sigma_{add}} = \\frac{\\epsilon_{ij}}{\\sigma_{add}}$, which follows a standard normal distribution, $\\mathcal{N}(0, 1)$.\n\nWe apply this standardization to our inequality. Since the standard deviation $\\sigma_{add}$ is inherently positive ($\\sigma_{add} > 0$), dividing by it does not change the direction of the inequality:\n$$\nP(\\epsilon_{ij} < LLOQ - f_{ij}) = P\\left(\\frac{\\epsilon_{ij}}{\\sigma_{add}} < \\frac{LLOQ - f_{ij}}{\\sigma_{add}}\\right)\n$$\nThe probability $P(Z < z)$ for a standard normal variable $Z$ is, by definition, the value of the standard normal CDF at point $z$, denoted as $\\Phi(z)$. Therefore, we can write the expression as:\n$$\nP\\left(\\frac{\\epsilon_{ij}}{\\sigma_{add}} < \\frac{LLOQ - f_{ij}}{\\sigma_{add}}\\right) = \\Phi\\left(\\frac{LLOQ - f_{ij}}{\\sigma_{add}}\\right)\n$$\nThis expression represents the closed-form likelihood contribution for the censored observation. It is expressed in terms of the standard normal CDF $\\Phi(\\cdot)$ and depends only on the quantities specified: the lower limit of quantification $LLOQ$, the model prediction $f_{ij}$, and the additive residual error standard deviation $\\sigma_{add}$.",
            "answer": "$$\\boxed{\\Phi\\left(\\frac{LLOQ - f_{ij}}{\\sigma_{add}}\\right)}$$"
        }
    ]
}