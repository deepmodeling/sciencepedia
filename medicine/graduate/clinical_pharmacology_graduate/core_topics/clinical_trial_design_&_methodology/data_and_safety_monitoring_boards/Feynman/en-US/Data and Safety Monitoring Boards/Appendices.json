{
    "hands_on_practices": [
        {
            "introduction": "In early-phase, first-in-human studies, a Data and Safety Monitoring Board's (DSMB) primary role is to ensure participant safety, often by carefully managing dose escalation to avoid unacceptable toxicity. Bayesian inference offers a powerful and intuitive framework for this task, allowing prior knowledge from preclinical studies to be formally combined with emerging clinical data. This practice introduces the use of the Beta-Binomial conjugate model, a cornerstone of modern adaptive trial designs, to dynamically update the estimated probability of toxicity and guide decision-making .",
            "id": "4544913",
            "problem": "A Data and Safety Monitoring Board (DSMB) in a first-in-human clinical pharmacology dose-escalation study monitors the probability of dose-limiting toxicity (DLT) at the currently administered dose level. Let the true DLT probability at this dose be denoted by $p \\in (0,1)$. Prior to observing data, the DSMB encodes historical and mechanistic information into a Beta prior distribution for $p$, with parameters $a>0$ and $b>0$. After enrolling $n$ subjects at this dose, the DSMB observes $x$ DLTs. Assume DLT outcomes are independent and identically distributed Bernoulli random variables with parameter $p$, and the sampling model for $x$ is binomial.\n\nStarting from Bayes' theorem, using the Bernoulli/binomial likelihood and the Beta prior, derive the posterior distribution for $p$. Then, using the derived posterior distribution, compute the posterior mean $\\mathbb{E}[p \\mid x,n]$ for the following numerical specification: $a=0.7$, $b=1.6$, $x=3$, and $n=10$.\n\nExpress your final numerical answer for $\\mathbb{E}[p \\mid x,n]$ as a decimal (unitless) rounded to four significant figures.",
            "solution": "The problem requires the derivation of the posterior distribution for a binomial proportion $p$ using a Beta prior, and the subsequent calculation of the posterior mean for a given set of parameters. This is a classic application of Bayesian inference, specifically utilizing the concept of conjugate priors.\n\nFirst, we validate the problem statement.\n\nStep 1: Extract Givens\n-   The parameter of interest is the true dose-limiting toxicity (DLT) probability, $p \\in (0,1)$.\n-   The prior distribution for $p$ is a Beta distribution, $p \\sim \\text{Beta}(a, b)$, with parameters $a>0$ and $b>0$.\n-   The number of subjects is $n$.\n-   The number of observed DLTs is $x$.\n-   The sampling model for $x$ given $p$ is a binomial distribution, $x \\mid p \\sim \\text{Binomial}(n, p)$.\n-   Specific numerical values for a calculation are provided: $a=0.7$, $b=1.6$, $x=3$, $n=10$.\n-   The final numerical answer must be rounded to four significant figures.\n\nStep 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, employing the standard Beta-Binomial conjugate model, which is a fundamental tool in Bayesian statistics and widely used in clinical trial design and monitoring. The problem is well-posed, providing all necessary information ($a, b, n, x$) to derive a unique posterior distribution and compute its mean. The parameters are valid ($a>0$, $b>0$, $n=10$, $x=3$ where $0 \\le x \\le n$). The language is objective and formal. The problem is self-contained, consistent, and does not violate any scientific or mathematical principles.\n\nStep 3: Verdict and Action\nThe problem is deemed valid. We proceed with the solution.\n\nThe derivation begins with Bayes' theorem, which states that the posterior probability distribution is proportional to the product of the likelihood function and the prior probability distribution.\n\n$$\nf(p \\mid x, n) \\propto f(x \\mid p, n) \\cdot f(p; a, b)\n$$\n\nHere, $f(p \\mid x, n)$ is the posterior probability density function (PDF) of $p$ given the data $x$ and $n$. The term $f(x \\mid p, n)$ represents the probability of observing the data given the parameter $p$, which is the likelihood function $L(p \\mid x, n)$. The term $f(p; a, b)$ is the prior PDF of $p$.\n\nThe sampling model is binomial, so the probability mass function of $x$ is:\n$$\nf(x \\mid p, n) = \\binom{n}{x} p^x (1-p)^{n-x}\n$$\nWhen viewed as a function of $p$ for fixed data $(x,n)$, this is the likelihood function. In the context of Bayes' theorem, any factors that do not depend on the parameter $p$ can be absorbed into the proportionality constant. Thus, we can write the likelihood as:\n$$\nL(p \\mid x, n) \\propto p^x (1-p)^{n-x}\n$$\n\nThe prior distribution for $p$ is a Beta distribution with parameters $a$ and $b$:\n$$\np \\sim \\text{Beta}(a,b)\n$$\nThe PDF of the Beta distribution is:\n$$\nf(p; a, b) = \\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)} p^{a-1} (1-p)^{b-1}\n$$\nwhere $\\Gamma(\\cdot)$ is the gamma function. Again, we can ignore the normalizing constant, $\\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)}$, as it does not depend on $p$.\n$$\nf(p; a, b) \\propto p^{a-1} (1-p)^{b-1}\n$$\n\nNow, we multiply the likelihood and the prior to find the unnormalized posterior distribution:\n$$\nf(p \\mid x, n) \\propto \\left( p^x (1-p)^{n-x} \\right) \\cdot \\left( p^{a-1} (1-p)^{b-1} \\right)\n$$\nCombining the terms by adding the exponents:\n$$\nf(p \\mid x, n) \\propto p^{x+a-1} (1-p)^{n-x+b-1}\n$$\n$$\nf(p \\mid x, n) \\propto p^{(a+x)-1} (1-p)^{(b+n-x)-1}\n$$\n\nThis expression is the kernel of a Beta distribution with updated parameters. Let $a' = a+x$ and $b' = b+n-x$. The posterior PDF has the form:\n$$\nf(p \\mid x, n) \\propto p^{a'-1} (1-p)^{b'-1}\n$$\nTherefore, the posterior distribution of $p$ is a Beta distribution with parameters $a+x$ and $b+n-x$.\n$$\np \\mid x, n \\sim \\text{Beta}(a+x, b+n-x)\n$$\nThis demonstrates the property of conjugacy between the Beta prior and the binomial likelihood.\n\nNext, we must compute the posterior mean, $\\mathbb{E}[p \\mid x,n]$. The expected value (mean) of a random variable $Y$ that follows a Beta distribution, $Y \\sim \\text{Beta}(\\alpha, \\beta)$, is given by the formula:\n$$\n\\mathbb{E}[Y] = \\frac{\\alpha}{\\alpha+\\beta}\n$$\nApplying this formula to our posterior distribution, where the parameters are $\\alpha = a+x$ and $\\beta = b+n-x$, we obtain the posterior mean of $p$:\n$$\n\\mathbb{E}[p \\mid x, n] = \\frac{a+x}{(a+x) + (b+n-x)}\n$$\nSimplifying the denominator:\n$$\n\\mathbb{E}[p \\mid x, n] = \\frac{a+x}{a+b+n}\n$$\n\nFinally, we substitute the specified numerical values: $a=0.7$, $b=1.6$, $x=3$, and $n=10$.\n$$\n\\mathbb{E}[p \\mid x, n] = \\frac{0.7+3}{0.7+1.6+10}\n$$\n$$\n\\mathbb{E}[p \\mid x, n] = \\frac{3.7}{12.3}\n$$\nNow, we perform the division to obtain the numerical value:\n$$\n\\mathbb{E}[p \\mid x, n] = 0.30081300813...\n$$\nThe problem requires the answer to be rounded to four significant figures. The first four significant figures are $3$, $0$, $0$, and $8$. The fifth significant figure is $1$, which is less than $5$, so we round down (i.e., we truncate at the fourth significant figure).\nThe rounded posterior mean is $0.3008$.",
            "answer": "$$\\boxed{0.3008}$$"
        },
        {
            "introduction": "A DSMB rarely assesses a trial based on a single outcome; instead, it monitors a panel of multiple safety and efficacy endpoints. This multiplicity of tests increases the overall chance of a \"false alarm\"—that is, finding a statistically significant result purely by chance, which could lead to prematurely and incorrectly halting a promising trial. This exercise demonstrates the fundamental method of controlling this family-wise error rate by deriving the Bonferroni correction from first principles, a critical skill for maintaining statistical rigor in any interim analysis .",
            "id": "4544916",
            "problem": "A Data and Safety Monitoring Board (DSMB) overseeing a randomized clinical trial plans a single interim safety review across $m$ prespecified, independent safety endpoints. For each endpoint, the DSMB will conduct a one-sided hypothesis test focused on the direction of harm. The DSMB wishes to ensure that the overall probability of making at least one false positive conclusion among these $m$ tests, that is, the Family-Wise Error Rate (FWER), does not exceed a prespecified one-sided threshold $\\alpha$ across this interim look.\n\nStarting only from the definition of the Family-Wise Error Rate and elementary probability bounds, derive the equal per-endpoint one-sided significance level that guarantees $\\mathrm{FWER} \\le \\alpha$ when $m$ endpoints are each tested once at the interim. Then, using $m=13$ endpoints and an overall one-sided threshold $\\alpha=0.018$, compute the adjusted one-sided per-endpoint $\\alpha$ to be used for each test. Express the adjusted per-endpoint $\\alpha$ as a decimal number and round your final numeric answer to four significant figures.",
            "solution": "The problem as stated is subjected to validation.\n\n### Step 1: Extract Givens\n- A single interim safety review is planned.\n- There are $m$ prespecified, independent safety endpoints.\n- For each endpoint, a one-sided hypothesis test is conducted.\n- The Family-Wise Error Rate (FWER) is the overall probability of making at least one false positive conclusion.\n- The FWER must not exceed a prespecified one-sided threshold, $\\alpha$. This is expressed as $\\mathrm{FWER} \\le \\alpha$.\n- The task is to derive an equal per-endpoint one-sided significance level from the definition of FWER and elementary probability bounds.\n- The specific values provided for calculation are $m=13$ and $\\alpha=0.018$.\n- The final numerical answer must be rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is grounded in the established principles of biostatistics, specifically the theory of multiple comparisons in clinical trials. The concepts of FWER, hypothesis testing, and significance levels are fundamental to this field.\n- **Well-Posed**: The problem is well-posed. It requests a derivation based on specified principles and then a calculation using provided data. The assumption of independence, while not strictly necessary for the most common correction method (Bonferroni), provides a clear context. A unique solution exists.\n- **Objective**: The problem is stated using precise, objective scientific language.\n- **Completeness**: All necessary information for the derivation and calculation is provided. The prompt asks for a derivation from first principles, so no external formulae are needed at the outset.\n- **Consistency**: The givens are internally consistent and do not contain contradictions.\n\n### Step 3: Verdict and Action\nThe problem is deemed valid as it is scientifically sound, well-posed, and contains sufficient information for a unique solution.\n\n### Solution Derivation and Calculation\n\nThe problem requires the derivation of a per-endpoint significance level, which we shall denote as $\\alpha_{\\text{adj}}$, that controls the Family-Wise Error Rate (FWER) at a level $\\alpha$.\n\nLet there be $m$ independent hypothesis tests, one for each safety endpoint. Let $H_{0i}$ be the null hypothesis for the $i$-th endpoint, for $i = 1, 2, \\ldots, m$. The global null hypothesis, $H_0$, is the situation where all individual null hypotheses are simultaneously true, i.e., no safety issues exist across any of the endpoints.\n\nA Type I error for the $i$-th test occurs if we reject $H_{0i}$ when it is in fact true. Let $E_i$ be the event of making a Type I error for the $i$-th test. The significance level for this individual test is set to $\\alpha_{\\text{adj}}$, so under the null hypothesis $H_{0i}$, we have $P(E_i) = \\alpha_{\\text{adj}}$.\n\nThe Family-Wise Error Rate (FWER) is defined as the probability of making at least one Type I error among the $m$ tests, assuming the global null hypothesis holds. Mathematically, this is the probability of the union of the events $E_i$:\n$$\n\\mathrm{FWER} = P\\left(\\bigcup_{i=1}^{m} E_i\\right)\n$$\nThe problem states that we must start from \"elementary probability bounds\" to ensure that $\\mathrm{FWER} \\le \\alpha$. The most fundamental and widely applicable bound for the probability of a union of events is Boole's inequality, also known as the union bound. Boole's inequality states that for any finite collection of events, the probability that at least one of them occurs is no greater than the sum of their individual probabilities.\n$$\nP\\left(\\bigcup_{i=1}^{m} E_i\\right) \\le \\sum_{i=1}^{m} P(E_i)\n$$\nApplying this bound to the definition of FWER, we get:\n$$\n\\mathrm{FWER} \\le \\sum_{i=1}^{m} P(E_i)\n$$\nUnder the global null hypothesis, the probability of a Type I error for each test is $P(E_i) = \\alpha_{\\text{adj}}$. Since we are seeking an *equal* per-endpoint significance level, this value is the same for all $m$ tests. Substituting this into the inequality gives:\n$$\n\\mathrm{FWER} \\le \\sum_{i=1}^{m} \\alpha_{\\text{adj}} = m \\cdot \\alpha_{\\text{adj}}\n$$\nOur goal is to ensure that $\\mathrm{FWER} \\le \\alpha$. We can guarantee this by enforcing the stricter condition derived from the bound:\n$$\nm \\cdot \\alpha_{\\text{adj}} \\le \\alpha\n$$\nTo find the appropriate value for $\\alpha_{\\text{adj}}$, we can set the upper bound equal to the desired threshold $\\alpha$:\n$$\nm \\cdot \\alpha_{\\text{adj}} = \\alpha\n$$\nSolving for $\\alpha_{\\text{adj}}$ yields the required per-endpoint significance level:\n$$\n\\alpha_{\\text{adj}} = \\frac{\\alpha}{m}\n$$\nThis result is known as the Bonferroni correction. It guarantees that the FWER is controlled at or below the level $\\alpha$, regardless of the correlation structure between the endpoints. The problem's specification of \"independent\" endpoints is a condition under which this bound holds, although the bound itself is more general.\n\nNow, we apply this formula to the specific values given in the problem:\n- Number of endpoints, $m = 13$.\n- Overall FWER threshold, $\\alpha = 0.018$.\n\nSubstituting these values into our derived formula:\n$$\n\\alpha_{\\text{adj}} = \\frac{0.018}{13}\n$$\nPerforming the division gives:\n$$\n\\alpha_{\\text{adj}} \\approx 0.00138461538...\n$$\nThe problem requires this result to be rounded to four significant figures. The first significant figure is the first non-zero digit, which is $1$. The subsequent three significant figures are $3$, $8$, and $4$. The fifth significant digit is $6$. Since $6 \\ge 5$, we must round up the fourth significant digit ($4$) to $5$.\n\nTherefore, the adjusted one-sided per-endpoint significance level, rounded to four significant figures, is $0.001385$.",
            "answer": "$$\\boxed{0.001385}$$"
        },
        {
            "introduction": "For many clinical trials, particularly in oncology, the key outcomes are time-to-event measures like disease progression or survival, which are typically analyzed using the Cox proportional hazards model. A DSMB member must not only understand the model's output, such as the hazard ratio ($HR$), but also be able to critically evaluate its underlying assumptions. This advanced practice involves both the calculation of an $HR$ and, more importantly, the nuanced interpretation of what that result means when the crucial proportional hazards assumption may be violated, a scenario that has profound implications for assessing safety and efficacy over time .",
            "id": "4544930",
            "problem": "A randomized Phase II clinical pharmacology trial compares an investigational drug ($Z=1$) versus control ($Z=0$). The Data and Safety Monitoring Board (DSMB) requests an interim time-to-event analysis using the Cox proportional hazards model. Assume the hazard follows $h(t \\mid Z) = h_{0}(t)\\exp(\\beta Z)$, where $h_{0}(t)$ is an unspecified baseline hazard and $\\beta$ is the log hazard ratio parameter.\n\nAt five distinct ordered event times $t_{1},\\dots,t_{5}$, exactly one event occurs at each time. The sizes of the risk sets $R(t_{i})$ just prior to each event are:\n- $(n_{1i}, n_{0i}) = (50, 70)$ at $t_{1}$,\n- $(n_{1i}, n_{0i}) = (49, 70)$ at $t_{2}$,\n- $(n_{1i}, n_{0i}) = (49, 69)$ at $t_{3}$,\n- $(n_{1i}, n_{0i}) = (48, 69)$ at $t_{4}$,\n- $(n_{1i}, n_{0i}) = (47, 69)$ at $t_{5}$,\n\nwhere $n_{1i}$ is the number at risk with $Z=1$ and $n_{0i}$ is the number at risk with $Z=0$. The event occurs in the investigational drug group ($Z_{\\text{event}}=1$) at $t_{1}$, $t_{3}$, and $t_{4}$, and in the control group ($Z_{\\text{event}}=0$) at $t_{2}$ and $t_{5}$. Assume no other changes in risk set composition beyond those implied by these events between $t_{1}$ and $t_{5}$.\n\nStarting from the fundamental definition of the Cox model and its partial likelihood for distinct event times, derive the score function and observed information for $\\beta$ under a single binary covariate $Z \\in \\{0,1\\}$. Use a Newton–Raphson procedure initialized at $\\beta_{0}=0$ to obtain the maximum partial likelihood estimate $\\hat{\\beta}$ from these five event times and risk sets. Compute the hazard ratio estimate $\\widehat{\\mathrm{HR}}=\\exp(\\hat{\\beta})$. Round your final numerical answer to 4 significant figures.\n\nThen, explain—grounded in the remit of the Data and Safety Monitoring Board (DSMB)—how this interim result should be interpreted if diagnostics suggest that the proportional hazards assumption may be violated (for example, time-dependent trends in Schoenfeld residuals). In your explanation, address the implications for safety monitoring and group-sequential decision-making when hazards are non-proportional, including the role of alternative weighting in log-rank tests and time-varying effect modeling. Do not compute any additional numerical quantities beyond $\\widehat{\\mathrm{HR}}$.",
            "solution": "The problem is evaluated as valid. It is scientifically grounded in the principles of survival analysis and clinical trial monitoring, is well-posed with sufficient and consistent data, and is expressed in objective, formal language. We may, therefore, proceed with a solution.\n\nThe problem consists of two parts. The first is to compute the maximum partial likelihood estimate of the hazard ratio from the provided data using the Newton-Raphson method. The second is to provide a qualitative interpretation of this result in the context of a Data and Safety Monitoring Board (DSMB) review where the proportional hazards assumption is violated.\n\n**Part 1: Calculation of the Hazard Ratio Estimate**\n\nThe Cox proportional hazards model for a single binary covariate $Z \\in \\{0, 1\\}$ specifies the hazard for an individual as $h(t \\mid Z) = h_{0}(t)\\exp(\\beta Z)$, where $h_{0}(t)$ is the baseline hazard function and $\\beta$ is the log hazard ratio.\n\nThe partial likelihood for $D$ distinct event times $t_1, \\dots, t_D$ is given by:\n$$L(\\beta) = \\prod_{i=1}^{D} \\frac{\\exp(\\beta Z_i)}{\\sum_{j \\in R(t_i)} \\exp(\\beta Z_j)}$$\nwhere $Z_i$ is the covariate value for the subject who experiences an event at time $t_i$, and $R(t_i)$ is the risk set (the set of all subjects who are alive and uncensored just prior to $t_i$).\n\nLet $n_{1i}$ and $n_{0i}$ be the number of subjects at risk in the treatment group ($Z=1$) and control group ($Z=0$), respectively, at time $t_i$. The denominator can be simplified:\n$$\\sum_{j \\in R(t_i)} \\exp(\\beta Z_j) = n_{1i} \\exp(\\beta \\cdot 1) + n_{0i} \\exp(\\beta \\cdot 0) = n_{1i} \\exp(\\beta) + n_{0i}$$\n\nThe log-partial likelihood is:\n$$l(\\beta) = \\ln(L(\\beta)) = \\sum_{i=1}^{D} \\left[ \\beta Z_i - \\ln(n_{1i} \\exp(\\beta) + n_{0i}) \\right]$$\nTo find the maximum partial likelihood estimate $\\hat{\\beta}$, we use the Newton-Raphson algorithm, which iteratively updates an estimate for $\\beta$ using the formula:\n$$\\beta_{k+1} = \\beta_k + I(\\beta_k)^{-1} U(\\beta_k)$$\nwhere $U(\\beta)$ is the score function (first derivative of $l(\\beta)$) and $I(\\beta)$ is the observed information (negative second derivative of $l(\\beta)$).\n\nThe score function $U(\\beta) = \\frac{\\partial l(\\beta)}{\\partial \\beta}$ is:\n$$U(\\beta) = \\sum_{i=1}^{D} \\left[ Z_i - \\frac{n_{1i} \\exp(\\beta)}{n_{1i} \\exp(\\beta) + n_{0i}} \\right]$$\nLet's define $p_i(\\beta) = \\frac{n_{1i} \\exp(\\beta)}{n_{1i} \\exp(\\beta) + n_{0i}}$, which is the conditional probability of an event occurring in a subject from the treatment group at time $t_i$, given that an event occurred. Then $U(\\beta) = \\sum_{i=1}^{D} (Z_i - p_i(\\beta))$.\n\nThe observed information $I(\\beta) = -\\frac{\\partial^2 l(\\beta)}{\\partial \\beta^2} = -\\frac{\\partial U(\\beta)}{\\partial \\beta}$ is:\n$$I(\\beta) = \\sum_{i=1}^{D} \\left[ \\frac{(n_{1i} \\exp(\\beta))(n_{0i})}{(n_{1i} \\exp(\\beta) + n_{0i})^2} \\right] = \\sum_{i=1}^{D} p_i(\\beta)(1 - p_i(\\beta))$$\n\nWe are given $D=5$ event times. The event history is $Z_{\\text{event}} = (1, 0, 1, 1, 0)$. The risk set counts $(n_{1i}, n_{0i})$ are $(50, 70)$, $(49, 70)$, $(49, 69)$, $(48, 69)$, and $(47, 69)$ for $i=1, \\dots, 5$.\n\n**Iteration 1: Initialize with $\\beta_0 = 0$.**\nAt $\\beta_0=0$, we have $\\exp(\\beta_0) = 1$.\n$p_i(0) = \\frac{n_{1i}}{n_{1i} + n_{0i}}$.\n$p_1(0) = \\frac{50}{50+70} = \\frac{50}{120} \\approx 0.416667$\n$p_2(0) = \\frac{49}{49+70} = \\frac{49}{119} \\approx 0.411765$\n$p_3(0) = \\frac{49}{49+69} = \\frac{49}{118} \\approx 0.415254$\n$p_4(0) = \\frac{48}{48+69} = \\frac{48}{117} \\approx 0.410256$\n$p_5(0) = \\frac{47}{47+69} = \\frac{47}{116} \\approx 0.405172$\n\nThe score function at $\\beta_0=0$:\n$U(0) = \\sum_{i=1}^5 (Z_i - p_i(0)) = (1-p_1(0)) + (0-p_2(0)) + (1-p_3(0)) + (1-p_4(0)) + (0-p_5(0))$\n$U(0) = (1-0.416667) - 0.411765 + (1-0.415254) + (1-0.410256) - 0.405172$\n$U(0) \\approx 0.583333 - 0.411765 + 0.584746 + 0.589744 - 0.405172 = 0.940886$\n\nThe information at $\\beta_0=0$:\n$I(0) = \\sum_{i=1}^5 p_i(0)(1 - p_i(0))$\n$I(0) \\approx 0.416667(0.583333) + 0.411765(0.588235) + 0.415254(0.584746) + 0.410256(0.589744) + 0.405172(0.594828)$\n$I(0) \\approx 0.243056 + 0.242214 + 0.242818 + 0.241946 + 0.240993 = 1.211027$\n\nThe first update is:\n$\\beta_1 = \\beta_0 + I(0)^{-1} U(0) = 0 + \\frac{0.940886}{1.211027} \\approx 0.776934$\n\n**Iteration 2: Using $\\beta_1 = 0.776934$.**\nWe have $\\exp(\\beta_1) = \\exp(0.776934) \\approx 2.17478$.\n$p_1(\\beta_1) = \\frac{50 \\times 2.17478}{50 \\times 2.17478 + 70} \\approx 0.608346$\n$p_2(\\beta_1) = \\frac{49 \\times 2.17478}{49 \\times 2.17478 + 70} \\approx 0.603554$\n$p_3(\\beta_1) = \\frac{49 \\times 2.17478}{49 \\times 2.17478 + 69} \\approx 0.606982$\n$p_4(\\beta_1) = \\frac{48 \\times 2.17478}{48 \\times 2.17478 + 69} \\approx 0.602043$\n$p_5(\\beta_1) = \\frac{47 \\times 2.17478}{47 \\times 2.17478 + 69} \\approx 0.596996$\n\nThe score function at $\\beta_1$:\n$U(\\beta_1) = \\sum_{i=1}^5 (Z_i - p_i(\\beta_1)) = (1-0.608346) - 0.603554 + (1-0.606982) + (1-0.602043) - 0.596996$\n$U(\\beta_1) \\approx 0.391654 - 0.603554 + 0.393018 + 0.397957 - 0.596996 = -0.017921$\nSince $U(\\beta_1)$ is close to $0$, $\\beta_1$ is a good approximation of $\\hat{\\beta}$. We perform one more step for higher accuracy.\n\nThe information at $\\beta_1$:\n$I(\\beta_1) = \\sum p_i(\\beta_1)(1-p_i(\\beta_1))$\n$I(\\beta_1) \\approx 0.238319 + 0.239257 + 0.238555 + 0.239591 + 0.240586 = 1.196308$\n\nThe second update is:\n$\\beta_2 = \\beta_1 + I(\\beta_1)^{-1} U(\\beta_1) = 0.776934 + \\frac{-0.017921}{1.196308} \\approx 0.776934 - 0.014980 = 0.761954$\n\nWe can check the score function at $\\beta_2 = 0.761954$.\n$\\exp(\\beta_2) \\approx 2.14247$.\nA recalculation of $p_i(\\beta_2)$ and $U(\\beta_2)$ yields $U(\\beta_2) \\approx -0.000038$, which is negligibly different from $0$. Thus, we conclude that the maximum partial likelihood estimate is $\\hat{\\beta} \\approx 0.761954$.\n\nThe estimated hazard ratio is:\n$\\widehat{\\mathrm{HR}} = \\exp(\\hat{\\beta}) = \\exp(0.761954) \\approx 2.14247$\nRounding to 4 significant figures, we get $\\widehat{\\mathrm{HR}} = 2.142$.\n\n**Part 2: Interpretation for the DSMB**\n\nThe primary remit of a DSMB is to protect participant safety and ensure the trial's scientific integrity by monitoring accumulating data for signals of harm, overwhelming benefit, or futility. The calculated $\\widehat{\\mathrm{HR}} = 2.142$ suggests that, on average, the risk of an event in the investigational drug group ($Z=1$) is more than double the risk in the control group ($Z=0$). This serves as a potential safety alert.\n\nHowever, this result is predicated on the proportional hazards (PH) assumption, which posits that this hazard ratio is constant over time. If diagnostics (e.g., examination of Schoenfeld residuals) suggest this assumption is violated, the interpretation of this single, summary $\\widehat{\\mathrm{HR}}$ becomes problematic and potentially misleading for several critical reasons:\n\n1.  **Averaging Effect**: A single $\\widehat{\\mathrm{HR}}$ under non-proportional hazards represents a weighted average of the time-varying hazard ratio, $\\mathrm{HR}(t)=\\exp(\\beta(t))$, over the follow-up period. This average may not accurately reflect the true risk at any given time point. For a safety assessment, the pattern of risk is more important than the average. For instance, an increasing $\\mathrm{HR}(t)$ (e.g., from less than $1$ to substantially greater than $1$) indicates a delayed toxicity that an averaged $\\widehat{\\mathrm{HR}}$ might understate or obscure, particularly in an early interim analysis. Conversely, a decreasing $\\mathrm{HR}(t)$ (e.g., from very high initially to near $1$ later) may suggest an acute, manageable toxicity rather than a persistent danger.\n\n2.  **Implications for Safety Monitoring**: The DSMB's foremost concern is patient harm. The non-proportionality of hazards means the safety profile of the investigational drug changes over time. If the hazard ratio is increasing, the drug may appear safe early on, only for a serious safety signal to emerge later in the trial. A decision to continue the trial based on an early, benign-looking average HR could expose later participants to unacceptable risk. The current result of $\\widehat{\\mathrm{HR}} > 2$ is already concerning; knowing the trend of $\\mathrm{HR}(t)$ is paramount to understanding if this risk is immediate, delayed, escalating, or diminishing.\n\n3.  **Invalidation of Standard Group-Sequential Methods**: Group-sequential trial designs, which use pre-specified stopping boundaries (e.g., O'Brien-Fleming), rely on test statistics (like the score/log-rank statistic) whose distributional properties are derived under the PH assumption. When hazards are non-proportional, the statistical power of the standard log-rank test decreases, and the operating characteristics (type I error, power) of the sequential plan can be compromised. A trial might fail to stop for a true effect (either harm or benefit) simply because the chosen test statistic is insensitive to the pattern of non-proportionality.\n\nIn light of non-proportional hazards, the DSMB must not base its recommendation solely on the single $\\widehat{\\mathrm{HR}}$. Instead, it should request and deliberate on more appropriate analyses:\n\n-   **Modeling Time-Varying Effects**: The most direct approach is to abandon the simple Cox model and explicitly model the time-dependent log-hazard ratio, $\\beta(t)$. This can be done by including an interaction term between the treatment indicator and a function of time (e.g., $\\ln(t)$ or splines) in the Cox model, i.e., $h(t \\mid Z) = h_{0}(t) \\exp(\\beta_1 Z + \\beta_2 Z \\cdot g(t))$. A plot of the estimated $\\widehat{\\mathrm{HR}}(t) = \\exp(\\hat{\\beta}_1 + \\hat{\\beta}_2 g(t))$ would provide the DSMB with a dynamic picture of the relative risk, allowing for a much more nuanced safety assessment.\n\n-   **Using Weighted Log-Rank Tests**: The standard log-rank test gives equal weight to all events. The family of Harrington-Fleming weighted log-rank tests ($G^{\\rho, \\gamma}$) allows for differential weighting of early ($\\rho>0$) versus late ($\\gamma>0$) events. If, for example, a test that up-weights late events shows a stronger signal of harm than the standard log-rank test, this provides evidence of an emerging or delayed toxicity. This would be a grave concern for the DSMB.\n\n-   **Considering Alternative Endpoints**: If non-proportionality is severe, comparing survival curves via a hazard ratio may be inappropriate altogether. An alternative is to compare the Restricted Mean Survival Time (RMST) between arms. RMST is a robust measure of the average event-free time up to a specified time horizon and does not rely on the PH assumption.\n\n**Conclusion for the DSMB**: The interim finding of $\\widehat{\\mathrm{HR}} \\approx 2.14$ is a significant alert. The evidence of non-proportional hazards makes this single number an inadequate and potentially dangerous basis for a decision. The DSMB should immediately request analyses that characterize the time-dependency of the treatment effect. The board's recommendation—whether to continue, modify, or halt the trial for safety—should be based on a comprehensive understanding of how the hazard ratio evolves over the course of patient follow-up.",
            "answer": "$$\\boxed{2.142}$$"
        }
    ]
}