{
    "hands_on_practices": [
        {
            "introduction": "The core mechanism of a Marginal Structural Model (MSM) is the creation of a pseudo-population where the link between time-varying confounders and subsequent treatment is broken. This is achieved through Inverse Probability of Treatment Weighting (IPTW). This first exercise provides a foundational drill in this process, asking you to compute the weights for a single individual based on their specific history and a set of given probabilities. By working through this calculation , you will solidify your understanding of the formulas for both unstabilized and stabilized weights and see how they are constructed as a product over time.",
            "id": "4971107",
            "problem": "Consider an observational longitudinal cohort study in medicine with a binary, time-varying treatment process $\\{A_{t}: t=0,1\\}$ and time-varying covariates (potential confounders) $\\{L_{t}: t=0,1\\}$. The temporal ordering of measurements is $L_{0}$, then $A_{0}$, then $L_{1}$, then $A_{1}$. Assume the causal identification conditions of consistency, positivity, and sequential exchangeability hold, and that the data-generating process respects the indicated ordering, so that conditional treatment assignment probabilities are well-defined given histories. The goal is to fit a Marginal Structural Model (MSM), and weights are constructed using Inverse Probability of Treatment Weighting (IPTW) to remove confounding by $\\{L_{t}\\}$ while preserving the marginal treatment process.\n\nFor a specific individual with observed history $(L_0=1, A_0=1, L_1=1, A_1=1)$ and horizon $T=2$, suppose the following treatment assignment probabilities hold:\n- Baseline marginal treatment probability $P(A_{0}=1)=0.6$,\n- Baseline conditional treatment probability given baseline covariate $P(A_{0}=1 \\mid L_{0}=1)=0.8$,\n- Follow-up marginal treatment probability given past treatment $P(A_{1}=1 \\mid A_{0}=1)=0.7$,\n- Follow-up conditional treatment probability given past treatment and current covariate $P(A_{1}=1 \\mid A_{0}=1, L_{1}=1)=0.9$.\n\nUsing the core definitions of conditional probability and the purpose of IPTW for MSMs, derive from first principles the expressions for the unstabilized weight $w_{u}$ and the stabilized weight $w_{s}$ for this individual, and then compute their exact values. Express your final answer as simplified fractions in a single row matrix. No rounding is required; provide exact values.",
            "solution": "### Derivation of Solution\nThe objective is to calculate the unstabilized weight ($w_{u}$) and the stabilized weight ($w_{s}$) for a specific individual using Inverse Probability of Treatment Weighting (IPTW). IPTW creates a pseudo-population in which the time-varying confounders $L_t$ do not predict subsequent treatment $A_t$, thus removing time-dependent confounding.\n\nLet $\\bar{A}_k = (A_0, A_1, \\dots, A_k)$ denote the history of treatment up to time $k$, and $\\bar{L}_k = (L_0, L_1, \\dots, L_k)$ denote the history of the covariates. The general forms of the weights for an individual with observed history up to the final time point $K$ are given by the product of time-specific probabilities.\n\nThe unstabilized weight, $w_{u}$, is defined as the inverse of the product of conditional probabilities of receiving the observed treatment at each time point, given the past treatment and confounder history.\n$$w_{u} = \\prod_{k=0}^{K} \\frac{1}{P(A_{k}=a_{k} \\mid \\bar{A}_{k-1}=\\bar{a}_{k-1}, \\bar{L}_{k}=\\bar{l}_{k})}$$\n\nThe stabilized weight, $w_{s}$, modifies this by including a numerator term, which is the product of conditional probabilities of receiving the observed treatment given only the past treatment history. This stabilization reduces variance and typically results in weights with an expected value of $1$.\n$$w_{s} = \\prod_{k=0}^{K} \\frac{P(A_{k}=a_{k} \\mid \\bar{A}_{k-1}=\\bar{a}_{k-1})}{P(A_{k}=a_{k} \\mid \\bar{A}_{k-1}=\\bar{a}_{k-1}, \\bar{L}_{k}=\\bar{l}_{k})}$$\n\nIn this problem, the time horizon consists of two time points, $t=0$ and $t=1$, so we set $K=1$. The individual's observed history is $(L_0=1, A_0=1, L_1=1, A_1=1)$. Thus, $a_0=1, l_0=1, a_1=1, l_1=1$.\n\nFor $k=0$, the history $\\bar{A}_{-1}$ is empty. The formulas become:\n$$w_{u} = \\frac{1}{P(A_{0}=a_{0} \\mid L_{0}=l_{0}) \\times P(A_{1}=a_{1} \\mid A_{0}=a_{0}, \\bar{L}_{1}=\\bar{l}_{1})}$$\n$$w_{s} = \\frac{P(A_{0}=a_{0}) \\times P(A_{1}=a_{1} \\mid A_{0}=a_{0})}{P(A_{0}=a_{0} \\mid L_{0}=l_{0}) \\times P(A_{1}=a_{1} \\mid A_{0}=a_{0}, \\bar{L}_{1}=\\bar{l}_{1})}$$\n\nThe problem provides $P(A_{1}=1 \\mid A_{0}=1, L_{1}=1)=0.9$. This implies a specific model for treatment assignment where the probability of $A_1$ depends on the current confounder $L_1$ and past treatment $A_0$, but not on the baseline confounder $L_0$. That is, $P(A_{1}=a_{1} \\mid A_{0}=a_{0}, \\bar{L}_{1}=\\bar{l}_{1}) = P(A_{1}=a_{1} \\mid A_{0}=a_{0}, L_{1}=l_{1})$. This is a common and valid specification.\n\nWe can now substitute the given probabilities for the specific individual.\n\n**Calculation of the Unstabilized Weight ($w_{u}$)**\n\nFor the individual with history $(L_0=1, A_0=1, L_1=1, A_1=1)$, the denominator of the weight is the product of:\n1. $P(A_{0}=1 \\mid L_{0}=1) = 0.8$\n2. $P(A_{1}=1 \\mid A_{0}=1, L_{1}=1) = 0.9$\n\nTherefore, the unstabilized weight is:\n$$w_{u} = \\frac{1}{P(A_{0}=1 \\mid L_{0}=1) \\times P(A_{1}=1 \\mid A_{0}=1, L_{1}=1)}$$\n$$w_{u} = \\frac{1}{0.8 \\times 0.9} = \\frac{1}{0.72}$$\nTo express this as a simplified fraction:\n$$w_{u} = \\frac{1}{\\frac{72}{100}} = \\frac{100}{72} = \\frac{25 \\times 4}{18 \\times 4} = \\frac{25}{18}$$\n\n**Calculation of the Stabilized Weight ($w_{s}$)**\n\nThe denominator for $w_{s}$ is identical to the one used for $w_{u}$. The numerator is the product of:\n1. $P(A_{0}=1) = 0.6$\n2. $P(A_{1}=1 \\mid A_{0}=1) = 0.7$\n\nTherefore, the stabilized weight is:\n$$w_{s} = \\frac{P(A_{0}=1) \\times P(A_{1}=1 \\mid A_{0}=1)}{P(A_{0}=1 \\mid L_{0}=1) \\times P(A_{1}=1 \\mid A_{0}=1, L_{1}=1)}$$\n$$w_{s} = \\frac{0.6 \\times 0.7}{0.8 \\times 0.9} = \\frac{0.42}{0.72}$$\nTo express this as a simplified fraction:\n$$w_{s} = \\frac{42}{72} = \\frac{7 \\times 6}{12 \\times 6} = \\frac{7}{12}$$\n\nThe unstabilized weight for this individual is $w_{u} = \\frac{25}{18}$ and the stabilized weight is $w_{s} = \\frac{7}{12}$.",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{25}{18} & \\frac{7}{12} \\end{pmatrix}}$$"
        },
        {
            "introduction": "Having learned how to calculate weights, it is essential to understand precisely *why* this complex procedure is necessary. In longitudinal studies, it is common to encounter time-varying covariates that are both confounders for future treatment and are themselves affected by past treatment. This exercise demonstrates that naively adjusting for such a variable in a standard regression model can be misleading. By using a set of clear structural equations , you will quantify the bias that arises from conditioning on a collider, providing a concrete example of the critical problem that MSMs are designed to solve.",
            "id": "5209133",
            "problem": "A medical data science team is evaluating a time-varying antimicrobial dosing strategy using hospital electronic records. Let the baseline dose at admission be denoted by $A_0$ and the dose at day $t=1$ be denoted by $A_1$. Let $L_1$ be a day-$1$ physiologic biomarker that both affects the choice of the day-$1$ dose and affects recovery, and is itself affected by the baseline dose. Let $U$ be an unmeasured patient frailty factor that affects both the biomarker and recovery. The outcome $Y$ is a continuous measure of recovery at discharge. Consider the following linear structural equations with independent, mean-zero, unit-variance Gaussian errors:\n$$\nL_1 \\;=\\; \\gamma_0\\,A_0 \\;+\\; \\gamma_U\\,U \\;+\\; \\varepsilon_L,\\quad\nA_1 \\;=\\; \\delta_0\\,L_1 \\;+\\; \\varepsilon_A,\\quad\nY \\;=\\; \\beta_0\\,A_0 \\;+\\; \\beta_1\\,A_1 \\;+\\; \\beta_L\\,L_1 \\;+\\; \\beta_U\\,U \\;+\\; \\varepsilon_Y,\n$$\nwith $A_0 \\sim \\mathcal{N}(0,1)$, $U \\sim \\mathcal{N}(0,1)$, $\\varepsilon_L \\sim \\mathcal{N}(0,1)$, $\\varepsilon_A \\sim \\mathcal{N}(0,1)$, and $\\varepsilon_Y \\sim \\mathcal{N}(0,1)$ mutually independent. Let the parameters be fixed at\n$$\n\\gamma_0 = 1,\\quad \\gamma_U = 1,\\quad \\delta_0 = 1,\\quad \\beta_0 = 0,\\quad \\beta_1 = 1,\\quad \\beta_L = 0,\\quad \\beta_U = 1.\n$$\nThe causal target is the parameter for baseline treatment in a Marginal Structural Model (MSM), defined by the potential outcome regression $E\\!\\left[Y^{(a_0,a_1)}\\right] = \\theta_0 + \\theta_{a0}\\,a_0 + \\theta_{a1}\\,a_1$ under the standard potential outcomes framework (consistency, positivity, and sequential exchangeability), where $Y^{(a_0,a_1)}$ denotes the outcome that would be observed if $(A_0,A_1)$ were set to $(a_0,a_1)$.\n\nIn this scenario, clinicians might perform ordinary regression adjustment by fitting a linear model of $Y$ on $(A_0, A_1, L_1)$ using Ordinary Least Squares (OLS), which conditions on $L_1$ even though $L_1$ is influenced by $A_0$ and $U$. This conditioning can induce collider bias because $L_1$ receives arrows from both $A_0$ and $U$.\n\nStarting from the structural equations and the definitions of potential outcomes and linear OLS projections for jointly Gaussian variables, compute the OLS coefficient for $A_0$ in the regression of $Y$ on $(A_0, A_1, L_1)$, and then compute the bias of this OLS coefficient relative to the MSM parameter $\\theta_{a0}$. Provide the bias as a single real number. No rounding is required.",
            "solution": "The setting is a time-varying treatment problem where a time-varying covariate $L_1$ is affected by prior treatment $A_0$ and affects future treatment $A_1$ and the outcome $Y$. The Directed Acyclic Graph (DAG) has arrows $A_0 \\rightarrow L_1 \\rightarrow A_1 \\rightarrow Y$, $U \\rightarrow L_1$, and $U \\rightarrow Y$. Conditioning on $L_1$ creates a collider at $L_1$ between $A_0$ and $U$, yielding dependence between $A_0$ and $U$ given $L_1$; since $U$ also affects $Y$, this induces bias in regression coefficients for $A_0$.\n\nThe causal target is the baseline MSM parameter $\\theta_{a0}$ in the marginal structural model\n$$\nE\\!\\left[Y^{(a_0,a_1)}\\right] \\;=\\; \\theta_0 + \\theta_{a0}\\,a_0 + \\theta_{a1}\\,a_1.\n$$\nUnder the specified structural equations and consistency, setting $(A_0,A_1)=(a_0,a_1)$ yields\n$$\nY^{(a_0,a_1)} \\;=\\; \\beta_0\\,a_0 \\;+\\; \\beta_1\\,a_1 \\;+\\; \\beta_L\\,L_1^{(a_0)} \\;+\\; \\beta_U\\,U \\;+\\; \\varepsilon_Y,\n$$\nwith $L_1^{(a_0)} = \\gamma_0\\,a_0 + \\gamma_U\\,U + \\varepsilon_L$. Plugging in the given parameters $\\beta_0 = 0$ and $\\beta_L = 0$ gives\n$$\nY^{(a_0,a_1)} \\;=\\; \\beta_1\\,a_1 \\;+\\; \\beta_U\\,U \\;+\\; \\varepsilon_Y \\;=\\; a_1 \\;+\\; U \\;+\\; \\varepsilon_Y,\n$$\nso\n$$\nE\\!\\left[Y^{(a_0,a_1)}\\right] \\;=\\; a_1,\n$$\nand hence the true MSM parameters are $\\theta_{a0} = 0$ and $\\theta_{a1} = 1$ (with $\\theta_0 = 0$ under mean-zero exogenous errors).\n\nWe now compute the ordinary least squares (OLS) coefficient for $A_0$ in the regression of $Y$ on $(A_0, A_1, L_1)$. For jointly Gaussian, mean-zero variables, the OLS coefficient vector equals $\\Sigma_{X}^{-1}\\Sigma_{XY}$, where $X=(A_0, A_1, L_1)$, $\\Sigma_X$ is the covariance matrix of $X$, and $\\Sigma_{XY}$ is the vector of covariances of $X$ with $Y$.\n\nFirst, derive the implied linear forms from the parameters:\n$$\nL_1 \\;=\\; A_0 + U + \\varepsilon_L,\\quad\nA_1 \\;=\\; L_1 + \\varepsilon_A \\;=\\; A_0 + U + \\varepsilon_L + \\varepsilon_A,\\quad\nY \\;=\\; A_1 + U + \\varepsilon_Y \\;=\\; A_0 + 2U + \\varepsilon_L + \\varepsilon_A + \\varepsilon_Y.\n$$\nUsing independence and unit variances, compute variances and covariances:\n- Variances:\n$$\n\\operatorname{Var}(A_0) = 1,\\quad \\operatorname{Var}(U) = 1,\\quad \\operatorname{Var}(\\varepsilon_L) = 1,\\quad \\operatorname{Var}(\\varepsilon_A) = 1,\\quad \\operatorname{Var}(\\varepsilon_Y) = 1.\n$$\n$$\n\\operatorname{Var}(L_1) = \\operatorname{Var}(A_0)+\\operatorname{Var}(U)+\\operatorname{Var}(\\varepsilon_L) = 3,\n$$\n$$\n\\operatorname{Var}(A_1) = \\operatorname{Var}(L_1)+\\operatorname{Var}(\\varepsilon_A) = 4,\n$$\n$$\n\\operatorname{Var}(Y) = \\operatorname{Var}(A_0)+4\\operatorname{Var}(U)+\\operatorname{Var}(\\varepsilon_L)+\\operatorname{Var}(\\varepsilon_A)+\\operatorname{Var}(\\varepsilon_Y) = 8.\n$$\n- Covariances:\n$$\n\\operatorname{Cov}(A_0,L_1) = 1,\\quad \\operatorname{Cov}(A_0,A_1) = 1,\\quad \\operatorname{Cov}(A_0,Y) = 1,\n$$\n$$\n\\operatorname{Cov}(L_1,A_1) = \\operatorname{Var}(L_1) = 3,\\quad \\operatorname{Cov}(L_1,U) = 1,\n$$\n$$\n\\operatorname{Cov}(L_1,Y) = \\operatorname{Cov}(L_1,A_1)+\\operatorname{Cov}(L_1,U) = 3+1 = 4,\n$$\n$$\n\\operatorname{Cov}(A_1,Y) = \\operatorname{Var}(A_1) + \\operatorname{Cov}(A_1,U) = 4 + 1 = 5.\n$$\nConstruct the covariance matrix $\\Sigma_X$ and vector $\\Sigma_{XY}$:\n$$\n\\Sigma_X \\;=\\;\n\\begin{pmatrix}\n\\operatorname{Var}(A_0) & \\operatorname{Cov}(A_0,A_1) & \\operatorname{Cov}(A_0,L_1) \\\\\n\\operatorname{Cov}(A_0,A_1) & \\operatorname{Var}(A_1) & \\operatorname{Cov}(A_1,L_1) \\\\\n\\operatorname{Cov}(A_0,L_1) & \\operatorname{Cov}(A_1,L_1) & \\operatorname{Var}(L_1)\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n1 & 1 & 1 \\\\\n1 & 4 & 3 \\\\\n1 & 3 & 3\n\\end{pmatrix},\n$$\n$$\n\\Sigma_{XY} \\;=\\;\n\\begin{pmatrix}\n\\operatorname{Cov}(A_0,Y) \\\\\n\\operatorname{Cov}(A_1,Y) \\\\\n\\operatorname{Cov}(L_1,Y)\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n1 \\\\ 5 \\\\ 4\n\\end{pmatrix}.\n$$\nInvert $\\Sigma_X$. Its determinant is\n$$\n\\det(\\Sigma_X) \\;=\\; 1\\cdot(4\\cdot 3 - 3\\cdot 3) - 1\\cdot(1\\cdot 3 - 1\\cdot 3) + 1\\cdot(1\\cdot 3 - 4\\cdot 1) \\;=\\; 3 - 0 - 1 \\;=\\; 2.\n$$\nThe adjugate (cofactor transpose) is\n$$\n\\operatorname{Adj}(\\Sigma_X) \\;=\\;\n\\begin{pmatrix}\n3 & 0 & -1 \\\\\n0 & 2 & -2 \\\\\n-1 & -2 & 3\n\\end{pmatrix},\n$$\nso\n$$\n\\Sigma_X^{-1} \\;=\\; \\frac{1}{2}\n\\begin{pmatrix}\n3 & 0 & -1 \\\\\n0 & 2 & -2 \\\\\n-1 & -2 & 3\n\\end{pmatrix}.\n$$\nThe OLS coefficient vector $b$ in the regression of $Y$ on $(A_0, A_1, L_1)$ is\n$$\nb \\;=\\; \\Sigma_X^{-1}\\Sigma_{XY}\n\\;=\\;\n\\frac{1}{2}\n\\begin{pmatrix}\n3 & 0 & -1 \\\\\n0 & 2 & -2 \\\\\n-1 & -2 & 3\n\\end{pmatrix}\n\\begin{pmatrix}\n1 \\\\ 5 \\\\ 4\n\\end{pmatrix}\n\\;=\\;\n\\frac{1}{2}\n\\begin{pmatrix}\n3\\cdot 1 + 0\\cdot 5 - 1\\cdot 4 \\\\\n0\\cdot 1 + 2\\cdot 5 - 2\\cdot 4 \\\\\n-1\\cdot 1 - 2\\cdot 5 + 3\\cdot 4\n\\end{pmatrix}\n\\;=\\;\n\\frac{1}{2}\n\\begin{pmatrix}\n-1 \\\\ 2 \\\\ 1\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n-0.5 \\\\ 1 \\\\ 0.5\n\\end{pmatrix}.\n$$\nTherefore, the OLS coefficient for $A_0$ is $-0.5$. The true MSM parameter for baseline treatment is $\\theta_{a0} = 0$, so the bias of ordinary regression adjustment that conditions on $L_1$ is\n$$\nb_{A_0} - \\theta_{a0} \\;=\\; -0.5 - 0 \\;=\\; -0.5.\n$$\nThe direction of bias is negative in this explicit scenario, demonstrating collider bias induced by conditioning on $L_1$ which is affected by prior treatment $A_0$ and the unmeasured frailty $U$.",
            "answer": "$$\\boxed{-0.5}$$"
        },
        {
            "introduction": "While powerful, the validity of IPTW estimation hinges on the crucial assumption of positivity, which requires that every individual has a non-zero probability of receiving any treatment option at every point in time, given their observed history. This final practice explores the practical consequences of near-violations of this assumption. By analyzing a simplified treatment assignment model , you will derive a formal relationship between propensity scores close to $0$ or $1$ and the resulting variance of the IPTW estimator. This will provide a rigorous, first-principles understanding of how extreme weights can arise and severely compromise the precision and power of an MSM analysis.",
            "id": "5209136",
            "problem": "Consider a longitudinal Electronic Health Record (EHR) study in which a Marginal Structural Model (MSM) is to be fit using Inverse Probability of Treatment Weighting (IPTW) to adjust for time-varying confounding across $T \\geq 1$ visits. At each visit $t \\in \\{1,\\dots,T\\}$, let $A_{t} \\in \\{0,1\\}$ denote the binary treatment actually received, and let $\\pi_{t}$ denote the true propensity score defined as $\\pi_{t} = \\mathbb{P}(A_{t} = 1 \\mid H_{t})$, where $H_{t}$ is the observed treatment and covariate history up to time $t$. The unstabilized IPTW at visit $t$ is defined by the fundamental rule $W_{t} = \\frac{A_{t}}{\\pi_{t}} + \\frac{1-A_{t}}{1-\\pi_{t}}$, and the longitudinal weight is $W = \\prod_{t=1}^{T} W_{t}$.\n\nTo study how near-violations of positivity inflate the variability of the weights, consider the following simple, scientifically plausible assignment model that encodes rare-treatment strata while remaining internally consistent:\n- For each $t$, independently across $t$, the propensity score takes one of two values with equal probability: $\\pi_{t} \\in \\{\\varepsilon, 1-\\varepsilon\\}$ with $\\mathbb{P}(\\pi_{t}=\\varepsilon) = \\mathbb{P}(\\pi_{t}=1-\\varepsilon) = \\frac{1}{2}$, where $\\varepsilon \\in (0,\\frac{1}{2}]$ is a fixed constant.\n- Conditional on $\\pi_{t}$, treatment is assigned independently across $t$ as $A_{t} \\mid \\pi_{t} \\sim \\text{Bernoulli}(\\pi_{t})$.\n\nUsing only these assumptions and first principles of probability (such as the law of total expectation and independence), derive a closed-form analytic expression for the variance $\\operatorname{Var}(W)$ as a function of $\\varepsilon$ and $T$. Your final answer must be a single closed-form expression in terms of $\\varepsilon$ and $T$. No numerical approximation is required or permitted, and no units are involved. Express your final answer exactly; do not round.",
            "solution": "To find the variance of the longitudinal weight $W$, we use the definition $\\operatorname{Var}(W) = \\mathbb{E}[W^2] - (\\mathbb{E}[W])^2$. We must first compute the first and second moments of $W$.\n\nThe longitudinal weight is $W = \\prod_{t=1}^{T} W_t$. The problem states that for each visit $t$, the propensity score $\\pi_t$ is drawn independently from the same distribution. Furthermore, conditional on $\\pi_t$, the treatment $A_t$ is assigned independently. This implies that the pairs $(\\pi_t, A_t)$ are independent and identically distributed (i.i.d.) for $t=1, \\dots, T$. Since each $W_t$ is a function of only $(\\pi_t, A_t)$, the random variables $W_1, W_2, \\dots, W_T$ are also i.i.d.\n\nThis i.i.d. property allows us to simplify the expectations of the products:\n$$ \\mathbb{E}[W] = \\mathbb{E}\\left[\\prod_{t=1}^{T} W_t\\right] = \\prod_{t=1}^{T} \\mathbb{E}[W_t] = \\left(\\mathbb{E}[W_t]\\right)^T $$\n$$ \\mathbb{E}[W^2] = \\mathbb{E}\\left[\\left(\\prod_{t=1}^{T} W_t\\right)^2\\right] = \\mathbb{E}\\left[\\prod_{t=1}^{T} W_t^2\\right] = \\prod_{t=1}^{T} \\mathbb{E}[W_t^2] = \\left(\\mathbb{E}[W_t^2]\\right)^T $$\nOur task reduces to finding the first and second moments of a single weight $W_t$. For simplicity, let's drop the subscript $t$.\n\n**1. Calculation of $\\mathbb{E}[W]$**\nWe use the law of total expectation, $\\mathbb{E}[W] = \\mathbb{E}_{\\pi}[\\mathbb{E}_{A|\\pi}[W \\mid \\pi]]$. First, we compute the inner expectation, conditional on a given value of $\\pi$.\nThe weight is given by $W = \\frac{A}{\\pi} + \\frac{1-A}{1-\\pi}$. By the linearity of expectation and the fact that $\\mathbb{E}[A \\mid \\pi] = \\pi$ (since $A \\mid \\pi \\sim \\text{Bernoulli}(\\pi)$), we have:\n$$ \\mathbb{E}[W \\mid \\pi] = \\frac{\\mathbb{E}[A \\mid \\pi]}{\\pi} + \\frac{1 - \\mathbb{E}[A \\mid \\pi]}{1-\\pi} = \\frac{\\pi}{\\pi} + \\frac{1-\\pi}{1-\\pi} = 1 + 1 = 2 $$\nThe conditional expectation of $W$ given $\\pi$ is a constant, $2$. Therefore, the unconditional expectation is:\n$$ \\mathbb{E}[W] = \\mathbb{E}_{\\pi}[2] = 2 $$\n\n**2. Calculation of $\\mathbb{E}[W^2]$**\nAgain, we use the law of total expectation, $\\mathbb{E}[W^2] = \\mathbb{E}_{\\pi}[\\mathbb{E}_{A|\\pi}[W^2 \\mid \\pi]]$.\nFirst, we find the form of $W^2$. The variable $A$ is binary.\nIf $A=1$, then $W = \\frac{1}{\\pi}$, so $W^2 = \\frac{1}{\\pi^2}$.\nIf $A=0$, then $W = \\frac{1}{1-\\pi}$, so $W^2 = \\frac{1}{(1-\\pi)^2}$.\nNow we can compute the conditional expectation of $W^2$ given $\\pi$:\n$$ \\mathbb{E}[W^2 \\mid \\pi] = \\mathbb{P}(A=1 \\mid \\pi) \\cdot \\frac{1}{\\pi^2} + \\mathbb{P}(A=0 \\mid \\pi) \\cdot \\frac{1}{(1-\\pi)^2} $$\n$$ \\mathbb{E}[W^2 \\mid \\pi] = \\pi \\cdot \\frac{1}{\\pi^2} + (1-\\pi) \\cdot \\frac{1}{(1-\\pi)^2} = \\frac{1}{\\pi} + \\frac{1}{1-\\pi} = \\frac{1-\\pi + \\pi}{\\pi(1-\\pi)} = \\frac{1}{\\pi(1-\\pi)} $$\nNext, we take the expectation over the distribution of $\\pi$. The propensity score $\\pi$ takes values $\\varepsilon$ and $1-\\varepsilon$, each with probability $\\frac{1}{2}$.\n$$ \\mathbb{E}[W^2] = \\mathbb{E}_{\\pi}\\left[\\frac{1}{\\pi(1-\\pi)}\\right] = \\mathbb{P}(\\pi=\\varepsilon) \\cdot \\frac{1}{\\varepsilon(1-\\varepsilon)} + \\mathbb{P}(\\pi=1-\\varepsilon) \\cdot \\frac{1}{(1-\\varepsilon)(1-(1-\\varepsilon))} $$\n$$ \\mathbb{E}[W^2] = \\frac{1}{2} \\cdot \\frac{1}{\\varepsilon(1-\\varepsilon)} + \\frac{1}{2} \\cdot \\frac{1}{(1-\\varepsilon)\\varepsilon} = \\frac{1}{\\varepsilon(1-\\varepsilon)} $$\n\n**3. Final Calculation of $\\operatorname{Var}(W)$**\nNow we substitute the moments of a single weight, $\\mathbb{E}[W_t] = 2$ and $\\mathbb{E}[W_t^2] = \\frac{1}{\\varepsilon(1-\\varepsilon)}$, back into the expressions for the moments of the longitudinal weight $W$.\n$$ \\mathbb{E}[W] = (2)^T = 2^T $$\n$$ \\mathbb{E}[W^2] = \\left(\\frac{1}{\\varepsilon(1-\\varepsilon)}\\right)^T $$\nFinally, we compute the variance:\n$$ \\operatorname{Var}(W) = \\mathbb{E}[W^2] - (\\mathbb{E}[W])^2 = \\left(\\frac{1}{\\varepsilon(1-\\varepsilon)}\\right)^T - (2^T)^2 $$\n$$ \\operatorname{Var}(W) = \\left(\\frac{1}{\\varepsilon(1-\\varepsilon)}\\right)^T - 4^T $$\nThis expression is the closed-form variance of $W$ as a function of $\\varepsilon$ and $T$. The variance is non-negative, as required, since $(2\\varepsilon-1)^2 \\geq 0$ implies $\\frac{1}{\\varepsilon(1-\\varepsilon)} \\geq 4$. The variance explodes as $\\varepsilon \\to 0$, which correctly captures the inflation of variance due to near-violations of the positivity assumption.",
            "answer": "$$\\boxed{\\left(\\frac{1}{\\varepsilon(1-\\varepsilon)}\\right)^T - 4^T}$$"
        }
    ]
}