{
    "hands_on_practices": [
        {
            "introduction": "One of the most common and practical adaptations in clinical trials is blinded sample size re-estimation (SSR). Initial power calculations depend on assumptions about parameters that are unknown before the trial begins, such as the variance of the outcome. This exercise demonstrates how to use an interim, blinded estimate of the variance to recalculate the target sample size, ensuring the trial maintains its desired power without inflating the Type I error rate. Mastering this technique  is essential for managing uncertainty in trial planning and execution.",
            "id": "4772912",
            "problem": "A two-arm, equal-allocation, parallel-group superiority trial with a continuous primary endpoint is planned using a normal-theory framework justified by the Central Limit Theorem (CLT). Let the individual outcomes be independently and identically distributed with variance $\\sigma^{2}$ in each arm, and suppose the treatment effect is defined as a mean difference $\\delta = \\mu_{1} - \\mu_{2}$. The test is one-sided at type I error $\\alpha = 0.025$ and targets power $1 - \\beta = 0.9$ to detect $\\delta = 0.3$. The initial planning assumed $\\sigma^{2} = 1$. The design uses blinded sample size re-estimation based on the information function at an interim information time $t = 0.5$, where the pooled blinded variance estimate is $\\hat{\\sigma}^{2} = 1.44$.\n\nStarting from first principles—namely, the sampling distribution of the difference in sample means under the null and alternative hypotheses, the definition of type I error and power, and the behavior of the one-sided $z$-test—derive the continuous target for the total re-estimated sample size across both arms, $N_{\\text{new}}$, when the blinded interim estimate $\\hat{\\sigma}^{2}$ replaces the planning value $\\sigma^{2}$. Assume equal allocation, no other adaptations, and preservation of the originally planned critical value. Ignore integer constraints; report the real-valued $N_{\\text{new}}$ rounded to four significant figures.",
            "solution": "The problem requires the derivation of the re-estimated total sample size, $N_{\\text{new}}$, for a two-arm superiority clinical trial, based on an interim blinded variance estimate. We will begin from first principles, namely the statistical framework of the two-sample $z$-test for means.\n\nLet $X_{1j}$ and $X_{2j}$ be the continuous outcomes for the $j$-th subject in the treatment arm (arm $1$) and control arm (arm $2$), respectively. The outcomes are assumed to be independent and (approximately) normally distributed, $X_{ij} \\sim N(\\mu_i, \\sigma^2)$ for $i \\in \\{1, 2\\}$, with a common variance $\\sigma^2$. This is justified by the Central Limit Theorem. The treatment effect is the difference in means, $\\delta = \\mu_1 - \\mu_2$.\n\nThe trial uses equal allocation, so the sample sizes in each arm are $n_1 = n_2 = N/2$, where $N$ is the total sample size. The estimator for the treatment effect is the difference in sample means, $\\hat{\\delta} = \\bar{X}_1 - \\bar{X}_2$.\n\nThe sampling distribution of $\\hat{\\delta}$ is normal with mean $E[\\hat{\\delta}] = \\mu_1 - \\mu_2 = \\delta$ and variance $\\text{Var}(\\hat{\\delta}) = \\text{Var}(\\bar{X}_1) + \\text{Var}(\\bar{X}_2) = \\frac{\\sigma^2}{n_1} + \\frac{\\sigma^2}{n_2} = \\frac{\\sigma^2}{N/2} + \\frac{\\sigma^2}{N/2} = \\frac{4\\sigma^2}{N}$. Thus, $\\hat{\\delta} \\sim N(\\delta, \\frac{4\\sigma^2}{N})$.\n\nThe trial is designed to test the one-sided null hypothesis $H_0: \\delta \\le 0$ against the alternative $H_1: \\delta > 0$. For constructing the test, we consider the boundary case $H_0: \\delta = 0$. The standardized test statistic is:\n$$ Z = \\frac{\\hat{\\delta} - 0}{\\sqrt{\\text{Var}(\\hat{\\delta})}} = \\frac{\\bar{X}_1 - \\bar{X}_2}{\\sqrt{4\\sigma^2/N}} $$\nUnder $H_0$, $Z$ follows a standard normal distribution, $Z \\sim N(0, 1)$.\n\nThe one-sided type I error rate is specified as $\\alpha = 0.025$. The null hypothesis is rejected if the test statistic $Z$ exceeds a critical value, $c$. This critical value is determined by $P(Z > c | H_0) = \\alpha$. For a standard normal variable, this implies $c = z_{1-\\alpha}$, the $(1-\\alpha)$-quantile of the standard normal distribution. Given $\\alpha = 0.025$, the critical value is $c = z_{1-0.025} = z_{0.975}$.\n\nPower is the probability of correctly rejecting $H_0$ when a specific alternative hypothesis is true. The trial is powered to detect a treatment effect $\\delta_A = 0.3$ with power $1 - \\beta = 0.9$.\n$$ 1 - \\beta = P\\left(Z > z_{1-\\alpha} \\mid \\delta = \\delta_A\\right) $$\nSubstituting the definition of $Z$:\n$$ 1 - \\beta = P\\left(\\frac{\\bar{X}_1 - \\bar{X}_2}{\\sqrt{4\\sigma^2/N}} > z_{1-\\alpha} \\mid \\delta = \\delta_A\\right) $$\nTo evaluate this probability under the alternative hypothesis (where $E[\\bar{X}_1 - \\bar{X}_2] = \\delta_A$), we standardize the expression differently:\n$$ 1 - \\beta = P\\left(\\frac{(\\bar{X}_1 - \\bar{X}_2) - \\delta_A}{\\sqrt{4\\sigma^2/N}} > z_{1-\\alpha} - \\frac{\\delta_A}{\\sqrt{4\\sigma^2/N}}\\right) $$\nThe expression on the left inside the probability is a standard normal variable. Let $Z' \\sim N(0,1)$. Then:\n$$ P\\left(Z' > z_{1-\\alpha} - \\frac{\\delta_A \\sqrt{N}}{2\\sigma}\\right) = 1 - \\beta $$\nThis implies that the argument of the probability must be the $\\beta$-quantile of the standard normal distribution, $z_{\\beta}$, which is equal to $-z_{1-\\beta}$.\n$$ z_{1-\\alpha} - \\frac{\\delta_A \\sqrt{N}}{2\\sigma} = -z_{1-\\beta} $$\nSolving for the total sample size $N$ gives the general formula:\n$$ z_{1-\\alpha} + z_{1-\\beta} = \\frac{\\delta_A \\sqrt{N}}{2\\sigma} $$\n$$ \\sqrt{N} = \\frac{2\\sigma (z_{1-\\alpha} + z_{1-\\beta})}{\\delta_A} $$\n$$ N = \\frac{4\\sigma^2 (z_{1-\\alpha} + z_{1-\\beta})^2}{\\delta_A^2} $$\nThe problem describes a blinded sample size re-estimation procedure. This involves using an interim estimate of the variance, $\\hat{\\sigma}^2 = 1.44$, to recalculate the required sample size to achieve the target power. The re-estimation preserves the originally planned critical value ($z_{1-\\alpha}$), the target power ($1-\\beta$), and the effect size of interest ($\\delta_A$). Therefore, we can use the derived sample size formula, replacing the planning variance $\\sigma^2 = 1$ with the interim estimate $\\hat{\\sigma}^2 = 1.44$. All other parameters remain the same.\n\nThe re-estimated total sample size, $N_{\\text{new}}$, is given by:\n$$ N_{\\text{new}} = \\frac{4\\hat{\\sigma}^2 (z_{1-\\alpha} + z_{1-\\beta})^2}{\\delta_A^2} $$\nWe substitute the given values:\n$\\hat{\\sigma}^2 = 1.44$\n$\\delta_A = 0.3$\n$\\alpha = 0.025 \\implies z_{1-\\alpha} = z_{0.975}$\n$1-\\beta = 0.9 \\implies z_{1-\\beta} = z_{0.9}$\n\nThe required quantiles from the standard normal distribution are $z_{0.975} \\approx 1.959964$ and $z_{0.9} \\approx 1.281552$.\nPlugging these into the equation for $N_{\\text{new}}$:\n$$ N_{\\text{new}} = \\frac{4(1.44) (z_{0.975} + z_{0.9})^2}{(0.3)^2} $$\n$$ N_{\\text{new}} = \\frac{5.76}{0.09} (1.959964 + 1.281552)^2 $$\n$$ N_{\\text{new}} = 64 \\times (3.241516)^2 $$\n$$ N_{\\text{new}} = 64 \\times 10.507427... $$\n$$ N_{\\text{new}} = 672.4753... $$\nThe problem requires the result to be rounded to four significant figures.\n$$ N_{\\text{new}} \\approx 672.5 $$\nThis value represents the continuous target for the total sample size across both arms needed to achieve $90\\%$ power under the observed interim variance.",
            "answer": "$$\n\\boxed{672.5}\n$$"
        },
        {
            "introduction": "Confirmatory adaptive trials that modify design features based on interim data must employ rigorous statistical methods to combine results from different stages and control the overall Type I error. The inverse-normal method is a cornerstone of such designs, providing a flexible and powerful framework for combining stagewise test statistics. This practice  challenges you to derive this combination rule from first principles, illustrating how information from independent patient cohorts can be aggregated into a single, valid test statistic.",
            "id": "4950399",
            "problem": "Consider a two-stage adaptive clinical trial using a preplanned combination testing framework to preserve Type I error under data-dependent adaptations between stages. Let the stagewise standardized efficient score statistics be $z_1$ and $z_2$, each computed from independent patient cohorts at stages $1$ and $2$, respectively. Under the null hypothesis of no treatment effect, assume the standard asymptotic properties for generalized linear models: the efficient score $U$ satisfies $U \\approx \\mathcal{N}(0, I)$, where $I$ is the Fisher information, and Fisher information adds across independent stages. Specifically, let $U_j$ be the stage-$j$ efficient score with variance $I_j$, and define the stagewise standardized statistics by $z_j = U_j / \\sqrt{I_j}$, which are independent and identically distributed as $\\mathcal{N}(0,1)$ under the null. Let the total Fisher information be $I = I_1 + I_2$ and the information fractions be $w_1 = I_1 / I$ and $w_2 = I_2 / I$.\n\nUsing only these principles—the additivity of efficient scores and Fisher information under independent sampling, and standardization by the square root of Fisher information—derive the linear combination $Z_{comb}$ of $z_1$ and $z_2$ that is standardized to $\\mathcal{N}(0,1)$ under the null and respects information-based weighting. Then, for $w_1 = 0.4$, $w_2 = 0.6$, $z_1 = 1.0$, and $z_2 = 2.1$, compute the numerical value of $Z_{comb}$ and the one-sided $p$-value for superiority defined as $p = \\Pr(Z \\geq Z_{comb})$ for $Z \\sim \\mathcal{N}(0,1)$. Express the $p$-value as a decimal. Round both $Z_{comb}$ and $p$ to four significant figures.",
            "solution": "The first step is to derive the expression for the combined test statistic, $Z_{comb}$. The problem requires that this statistic be a linear combination of the stagewise standardized statistics, $z_1$ and $z_2$, that it be standardized to a standard normal distribution $\\mathcal{N}(0,1)$ under the null hypothesis, and that it respects information-based weighting.\n\nWe begin with the fundamental principles stated: the additivity of the efficient score $U$ and the Fisher information $I$. For a two-stage trial with independent cohorts, the total efficient score is the sum of the stagewise scores, and the total Fisher information is the sum of the stagewise information measures.\n$$U_{total} = U_1 + U_2$$\n$$I_{total} = I_1 + I_2$$\nThe overall standardized test statistic for the full dataset, if it were analyzed in a single step, is defined as the total efficient score divided by the square root of the total Fisher information. Let us call this statistic $Z$.\n$$Z = \\frac{U_{total}}{\\sqrt{I_{total}}} = \\frac{U_1 + U_2}{\\sqrt{I_1 + I_2}}$$\nUnder the null hypothesis and appropriate regularity conditions, this overall statistic $Z$ is asymptotically distributed as $\\mathcal{N}(0,1)$.\n\nThe problem defines the stagewise standardized statistics as $z_j = U_j / \\sqrt{I_j}$ for $j=1, 2$. We can rearrange this definition to express the stagewise efficient scores in terms of the standardized statistics:\n$$U_1 = z_1 \\sqrt{I_1}$$\n$$U_2 = z_2 \\sqrt{I_2}$$\nNow, we substitute these expressions back into the formula for the overall statistic $Z$. This will yield the desired combination statistic $Z_{comb}$.\n$$Z_{comb} = \\frac{z_1 \\sqrt{I_1} + z_2 \\sqrt{I_2}}{\\sqrt{I_1 + I_2}}$$\nTo express this in terms of the information fractions $w_1 = I_1 / I_{total}$ and $w_2 = I_2 / I_{total}$, we can rewrite the expression as:\n$$Z_{comb} = \\frac{\\sqrt{I_1}}{\\sqrt{I_1 + I_2}} z_1 + \\frac{\\sqrt{I_2}}{\\sqrt{I_1 + I_2}} z_2$$\nRecognizing that $\\sqrt{w_1} = \\sqrt{I_1 / (I_1+I_2)} = \\sqrt{I_1} / \\sqrt{I_1+I_2}$ and similarly for $w_2$, we arrive at the final form for the information-weighted combination statistic:\n$$Z_{comb} = \\sqrt{w_1} z_1 + \\sqrt{w_2} z_2$$\nThis derivation successfully uses the principle of information additivity to construct a combined statistic from the stagewise components. As a check, we can verify its distribution. Given that $z_1, z_2 \\sim \\mathcal{N}(0,1)$ and are independent, the mean of $Z_{comb}$ is $E[Z_{comb}] = \\sqrt{w_1} E[z_1] + \\sqrt{w_2} E[z_2] = \\sqrt{w_1}(0) + \\sqrt{w_2}(0) = 0$. The variance is $Var(Z_{comb}) = Var(\\sqrt{w_1} z_1 + \\sqrt{w_2} z_2) = (\\sqrt{w_1})^2 Var(z_1) + (\\sqrt{w_2})^2 Var(z_2) = w_1(1) + w_2(1) = w_1 + w_2$. By definition, $w_1 + w_2 = (I_1/I_{total}) + (I_2/I_{total}) = (I_1+I_2)/I_{total} = 1$. Thus, $Var(Z_{comb}) = 1$. Since $Z_{comb}$ is a linear combination of independent normal variables, it is also normally distributed. Therefore, $Z_{comb} \\sim \\mathcal{N}(0,1)$, as required.\n\nNext, we compute the numerical value of $Z_{comb}$ using the provided data: $w_1 = 0.4$, $w_2 = 0.6$, $z_1 = 1.0$, and $z_2 = 2.1$.\n$$Z_{comb} = \\sqrt{0.4} (1.0) + \\sqrt{0.6} (2.1)$$\n$$Z_{comb} \\approx (0.63245553) (1.0) + (0.77459667) (2.1)$$\n$$Z_{comb} \\approx 0.63245553 + 1.62665290$$\n$$Z_{comb} \\approx 2.25910843$$\nRounding to four significant figures, we get $Z_{comb} = 2.259$.\n\nFinally, we compute the one-sided $p$-value, defined as $p = \\Pr(Z \\geq Z_{comb})$ for a standard normal random variable $Z \\sim \\mathcal{N}(0,1)$. This is calculated as $1 - \\Phi(Z_{comb})$, where $\\Phi$ is the cumulative distribution function (CDF) of the standard normal distribution. Using the unrounded value of $Z_{comb}$ for precision:\n$$p = \\Pr(Z \\geq 2.25910843)$$\n$$p = 1 - \\Phi(2.25910843)$$\nUsing a standard normal distribution table or computational software, we find the value.\n$$p \\approx 1 - 0.98806392 = 0.01193608$$\nRounding this value to four significant figures gives $p = 0.01194$.\nThe two requested values are the combined test statistic $Z_{comb}$ and the corresponding one-sided $p$-value.",
            "answer": "$$\\boxed{\\begin{pmatrix} 2.259 & 0.01194 \\end{pmatrix}}$$"
        },
        {
            "introduction": "Bayesian adaptive designs offer a dynamic approach to learning and optimization during a trial, most notably through response-adaptive randomization (RAR). This strategy aims to solve the \"explore-exploit\" dilemma: balancing the need to learn about all treatments (explore) with the ethical imperative to assign more patients to the apparently better treatment (exploit). This computational exercise  delves into the multi-armed bandit framework and the Gittins index, which provides a theoretically optimal solution to this problem, offering hands-on experience in implementing a sophisticated method at the forefront of modern trial design.",
            "id": "4950423",
            "problem": "Consider a two-arm Bayesian adaptive clinical trial with binary outcomes per patient, modeled as a Multi-Armed Bandit (MAB). Each arm corresponds to a treatment whose success probability is unknown and modeled with a Beta prior. Let the prior for arm $i$ be $\\mathrm{Beta}(a_i,b_i)$, and let outcomes be modeled as $\\mathrm{Bernoulli}(p_i)$ with unknown $p_i$. After each patient allocation to arm $i$, the posterior updates by adding $1$ to $a_i$ upon success and $1$ to $b_i$ upon failure, consistent with Bayesian conjugacy for the Beta-Bernoulli model. Future rewards are geometrically discounted by factor $\\gamma \\in (0,1)$, where a reward of $1$ is received upon success and $0$ upon failure.\n\nThe Gittins index (GI) for an arm state $(a,b)$ under geometric discount factor $\\gamma$ is defined via an equivalent one-arm Markov Decision Process (MDP) with a constant per-period subsidy $c$ available if the decision is to stop sampling this arm. Let $V_c(a,b)$ denote the optimal discounted value starting from state $(a,b)$ when the stopping action yields an immediate and perpetual reward of $c$ each period, producing a stopping value of $c/(1-\\gamma)$. For the continuation action at state $(a,b)$, the expected immediate reward is the posterior mean $a/(a+b)$, and the next state is $(a+1,b)$ with probability $a/(a+b)$ and $(a,b+1)$ with probability $b/(a+b)$. Therefore, the Bellman optimality equation for a fixed subsidy $c$ is\n$$\nV_c(a,b) \\;=\\; \\max\\left\\{\\frac{c}{1-\\gamma},\\; \\frac{a}{a+b} + \\gamma\\left[\\frac{a}{a+b}V_c(a+1,b) + \\frac{b}{a+b}V_c(a,b+1)\\right]\\right\\}.\n$$\nThe Gittins index $\\mathrm{GI}(a,b)$ is the unique subsidy $c^*$ at which it is optimal to be indifferent between stopping and continuing at the state $(a,b)$, namely the threshold subsidy where stopping becomes optimal:\n$$\n\\mathrm{GI}(a,b) \\;=\\; \\inf\\left\\{c \\;:\\; \\frac{c}{1-\\gamma} \\;\\ge\\; \\frac{a}{a+b} + \\gamma\\left[\\frac{a}{a+b}V_c(a+1,b) + \\frac{b}{a+b}V_c(a,b+1)\\right]\\right\\}.\n$$\nThis index yields the myopically optimal arm to allocate next: at each decision point, allocate the next patient to the arm with the larger Gittins index.\n\nYour task is to implement a numerical algorithm that approximates $\\mathrm{GI}(a,b)$ for Beta-Bernoulli arms via dynamic programming and bisection on $c$. To make the computation tractable, use a finite truncation depth $N_{\\max}$ on the number of future samples per arm and impose the boundary condition $V_c(a',b')=\\frac{c}{1-\\gamma}$ for all states $(a',b')$ satisfying $(a'-a)+(b'-b)\\ge N_{\\max}$. Under this truncation, the maximal error in the discounted value is bounded above by $\\gamma^{N_{\\max}}/(1-\\gamma)$ because the per-period reward is at most $1$.\n\nAlgorithmic requirements:\n- Implement a function to compute $V_c(a,b)$ by backward induction over the truncated state space with boundary $V_c(a',b')=\\frac{c}{1-\\gamma}$ at depth $N_{\\max}$.\n- Implement a bisection search on $c\\in[0,1]$ to find the approximate Gittins index $\\mathrm{GI}(a,b)$ to within a tolerance $\\varepsilon$ specified in code. At each bisection midpoint, recompute $V_c$ and determine whether continuation is optimal at $(a,b)$ by comparing the stopping value $\\frac{c}{1-\\gamma}$ and the continuation value.\n- For two arms, compute their indices and choose the next allocation to the arm with the larger index. In case of equality up to the rounding specification, break ties by choosing arm $1$.\n\nUse the following test suite of parameter values to evaluate your implementation:\n1. Arm $1$: $\\mathrm{Beta}(10,6)$, Arm $2$: $\\mathrm{Beta}(8,8)$, discount $\\gamma=0.95$, truncation $N_{\\max}=200$.\n2. Arm $1$: $\\mathrm{Beta}(1,1)$, Arm $2$: $\\mathrm{Beta}(1,1)$, discount $\\gamma=0.95$, truncation $N_{\\max}=200$.\n3. Arm $1$: $\\mathrm{Beta}(100,1)$, Arm $2$: $\\mathrm{Beta}(10,20)$, discount $\\gamma=0.95$, truncation $N_{\\max}=200$.\n4. Arm $1$: $\\mathrm{Beta}(20,20)$, Arm $2$: $\\mathrm{Beta}(3,1)$, discount $\\gamma=0.90$, truncation $N_{\\max}=200$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must be a list of three items: the Gittins index for arm $1$, the Gittins index for arm $2$, and the chosen arm number ($1$ or $2$). Express indices as floats rounded to six decimal places, and the chosen arm as an integer. For example, the output format must be\n$$\n[\\,[\\text{GI}_1,\\text{GI}_2,\\text{arm}],\\,[\\text{GI}_1,\\text{GI}_2,\\text{arm}],\\,\\dots]\n$$\nwhere each $\\text{GI}_i$ is rounded to six decimal places. No additional text or whitespace beyond what is structurally necessary for a valid list should be printed.",
            "solution": "The problem requires the computation of Gittins indices for two-armed Bayesian bandits with Beta-Bernoulli posteriors to decide which arm to sample next. This is a classic problem in the domain of optimal sequential decision-making, and the Gittins index provides an optimal policy for maximizing the total discounted expected reward.\n\nThe core of the problem is to calculate the Gittins index, $\\mathrm{GI}(a,b)$, for an arm with a posterior success probability described by a $\\mathrm{Beta}(a,b)$ distribution. The Gittins index is defined through an auxiliary one-arm Markov Decision Process (MDP). In this MDP, at each step, we can either 'continue' sampling from the arm or 'stop' permanently.\n\nThe state of this MDP is given by the parameters $(a,b)$ of the Beta distribution.\nThe actions are:\n1.  **Continue**: Sample the arm once more. This action yields an immediate expected reward equal to the posterior mean of the success probability, which is $\\frac{a}{a+b}$. After observing the outcome (success or failure), the state transitions to $(a+1,b)$ with probability $\\frac{a}{a+b}$ or to $(a,b+1)$ with probability $\\frac{b}{a+b}$. The future rewards are discounted by a factor $\\gamma$.\n2.  **Stop**: Cease sampling from this arm. This action provides a constant \"subsidy\" reward of $c$ for all subsequent time periods. The total discounted value of this perpetual reward stream is $\\frac{c}{1-\\gamma}$.\n\nLet $V_c(a,b)$ be the maximum total discounted expected reward starting from state $(a,b)$ with a given subsidy $c$. The value function $V_c(a,b)$ must satisfy the Bellman optimality equation, which balances the value of stopping against the value of continuing:\n$$\nV_c(a,b) \\;=\\; \\max\\left\\{\\underbrace{\\frac{c}{1-\\gamma}}_{\\text{Value of Stopping}},\\; \\underbrace{\\frac{a}{a+b} + \\gamma\\left[\\frac{a}{a+b}V_c(a+1,b) + \\frac{b}{a+b}V_c(a,b+1)\\right]}_{\\text{Value of Continuing}}\\right\\}\n$$\n\nThe Gittins index, $\\mathrm{GI}(a,b)$, is defined as the specific value of the subsidy $c$ for which the values of stopping and continuing are equal at state $(a,b)$. It is the \"fair price\" for the option to continue sampling. Mathematically, it is the value $c^*$ satisfying:\n$$\n\\frac{c^*}{1-\\gamma} \\;=\\; \\frac{a}{a+b} + \\gamma\\left[\\frac{a}{a+b}V_{c^*}(a+1,b) + \\frac{b}{a+b}V_{c^*}(a,b+1)\\right]\n$$\n\nTo solve this problem numerically, we employ the specified two-part algorithm: dynamic programming and bisection search.\n\n**1. State Space Truncation and Dynamic Programming**\n\nThe state space $(a,b)$ is infinite. To make the computation tractable, we truncate it. Let the initial state of the arm be $(a_0, b_0)$. We consider a finite horizon of $N_{\\max}$ future samples. Any state $(a',b')$ reached after $N_{\\max}$ or more samples, i.e., where $(a'-a_0) + (b'-b_0) \\ge N_{\\max}$, is treated as a terminal state. At these boundary states, the problem specifies the boundary condition $V_c(a',b') = \\frac{c}{1-\\gamma}$. This implies that at the horizon, we are forced to take the stopping action.\n\nFor a fixed subsidy $c$, we can compute $V_c(a,b)$ for any state $(a,b)$ within this truncated horizon using dynamic programming (specifically, backward induction). Let the \"depth\" of a state $(a_0+i, b_0+j)$ be $n = i+j$. We compute the values $V_c$ for all states at depth $n=N_{\\max}-1$, then for $n=N_{\\max}-2$, and so on, down to $n=0$ (the initial state $(a_0,b_0)$).\n\nThe calculation for a state $(a,b)$ at depth $n$ requires the values $V_c(a+1,b)$ and $V_c(a,b+1)$, which are at depth $n+1$. The backward induction ensures these values are available from the previous step of the induction. The iterative process is as follows:\n- We use a 2D array, `V_table[i][j]`, to store the value $V_c(a_0+i, b_0+j)$.\n- We iterate $n$ from $N_{\\max}-1$ down to $0$.\n- For each $n$, we iterate through all valid states $(a_0+i, b_0+j)$ where $i+j=n$.\n- For each such state, we compute its continuation value using the known values for states at depth $n+1$ (either from `V_table` or the boundary value $\\frac{c}{1-\\gamma}$) and then apply the `max` operator from the Bellman equation to find `V_table[i][j]`.\n\n**2. Bisection Search for the Gittins Index**\n\nThe Gittins index is the value of $c$ that creates indifference between stopping and continuing at the root state $(a_0, b_0)$. We can find this value using a bisection search on $c$ over the interval $[0,1]$.\nLet's define a function of $c$:\n$$\nf(c) = \\left( \\frac{a_0}{a_0+b_0} + \\gamma\\left[\\frac{a_0}{a_0+b_0}V_c(a_0+1,b_0) + \\frac{b_0}{a_0+b_0}V_c(a_0,b_0+1)\\right] \\right) - \\frac{c}{1-\\gamma}\n$$\nWe are searching for the root $c^*$ where $f(c^*) = 0$. The function $f(c)$ can be shown to be monotonically decreasing in $c$, which makes bisection a suitable method.\n\nThe bisection algorithm proceeds as follows:\n- Initialize a search interval for $c$, for instance $[c_{low}, c_{high}] = [0,1]$.\n- Repeat until the interval width $(c_{high}-c_{low})$ is smaller than a specified tolerance $\\varepsilon$:\n    a. Choose the midpoint $c_{mid} = (c_{low} + c_{high})/2$.\n    b. Compute the necessary values of the value function $V_{c_{mid}}$ using the dynamic programming method described above.\n    c. Evaluate the continuation value at the root state $(a_0, b_0)$ using $V_{c_{mid}}$. Let this be $\\text{cont\\_val}$.\n    d. Compare $\\text{cont\\_val}$ with the stopping value $\\text{stop\\_val} = c_{mid}/(1-\\gamma)$.\n    e. If $\\text{cont\\_val} > \\text{stop\\_val}$, it means the subsidy $c_{mid}$ is too low to make stopping attractive. The true Gittins index must be higher. We update $c_{low} = c_{mid}$.\n    f. Otherwise, if $\\text{cont\\_val} \\le \\text{stop\\_val}$, the subsidy is high enough (or exactly right). The true Gittins index must be in the lower half. We update $c_{high} = c_{mid}$.\n- The approximation of the Gittins index is the final midpoint of the search interval.\n\n**Final Decision Rule**\n\nAfter computing the Gittins index for each of the two arms, $\\mathrm{GI}_1$ and $\\mathrm{GI}_2$, the next patient is allocated to the arm with the higher index. If $\\mathrm{GI}_1 \\ge \\mathrm{GI}_2$, we choose arm 1, as per the problem's tie-breaking rule.\n\nThis combined algorithm allows for a robust numerical approximation of the Gittins indices, providing a principled solution to the adaptive allocation problem.",
            "answer": "```python\nimport numpy as np\n\ndef compute_gittins_index(a0, b0, gamma, N_max, epsilon):\n    \"\"\"\n    Computes the Gittins index for a Beta-Bernoulli arm using dynamic programming\n    and bisection search.\n\n    Args:\n        a0 (float): Initial alpha parameter of the Beta prior.\n        b0 (float): Initial beta parameter of the Beta prior.\n        gamma (float): Discount factor.\n        N_max (int): Truncation depth for the state space.\n        epsilon (float): Tolerance for the bisection search.\n\n    Returns:\n        float: The approximated Gittins index.\n    \"\"\"\n    if a0 <= 0 or b0 <= 0:\n        raise ValueError(\"Beta parameters a and b must be positive.\")\n\n    c_low, c_high = 0.0, 1.0\n\n    # V_table[i][j] will store V_c(a0+i, b0+j)\n    # The maximum index required is N_max, so size is N_max+1.\n    V_table = np.zeros((N_max + 1, N_max + 1))\n\n    while c_high - c_low > epsilon:\n        c_mid = (c_low + c_high) / 2.0\n        stop_val = c_mid / (1.0 - gamma)\n\n        # Backward induction using dynamic programming to compute V_c_mid\n        # n is the number of future samples, from N_max-1 down to 0\n        for n in range(N_max - 1, -1, -1):\n            # i is the number of future successes\n            for i in range(n + 1):\n                j = n - i  # j is the number of future failures\n                \n                a = a0 + i\n                b = b0 + j\n\n                # Get V for next states (at depth n+1)\n                # Success state: a0+(i+1), b0+j\n                # Failure state: a0+i, b0+(j+1)\n                \n                # Check if the next states are on or past the boundary depth N_max\n                if (i + 1) + j >= N_max:\n                    V_succ = stop_val\n                else:\n                    # Value was computed in the (n+1) iteration of the outer loop\n                    V_succ = V_table[i + 1, j]\n                \n                if i + (j + 1) >= N_max:\n                    V_fail = stop_val\n                else:\n                    # Value was computed in the (n+1) iteration of the outer loop\n                    V_fail = V_table[i, j + 1]\n\n                p = a / (a + b)\n                \n                continuation_value = p + gamma * (p * V_succ + (1.0 - p) * V_fail)\n                \n                V_table[i, j] = max(stop_val, continuation_value)\n        \n        # After filling the V_table for c_mid, check the condition at the root state (a0, b0)\n        p0 = a0 / (a0 + b0)\n\n        # Get values for next states from root\n        if 1 >= N_max:\n            V_succ_root = stop_val\n            V_fail_root = stop_val\n        else:\n            V_succ_root = V_table[1, 0]\n            V_fail_root = V_table[0, 1]\n            \n        root_continuation_value = p0 + gamma * (p0 * V_succ_root + (1.0 - p0) * V_fail_root)\n\n        if root_continuation_value > stop_val:\n            c_low = c_mid\n        else:\n            c_high = c_mid\n            \n    return (c_low + c_high) / 2.0\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    \"\"\"\n    test_cases = [\n        {'arm1': (10, 6), 'arm2': (8, 8), 'gamma': 0.95, 'N_max': 200},\n        {'arm1': (1, 1), 'arm2': (1, 1), 'gamma': 0.95, 'N_max': 200},\n        {'arm1': (100, 1), 'arm2': (10, 20), 'gamma': 0.95, 'N_max': 200},\n        {'arm1': (20, 20), 'arm2': (3, 1), 'gamma': 0.90, 'N_max': 200},\n    ]\n\n    epsilon = 1e-9\n    results = []\n\n    for case in test_cases:\n        a1, b1 = case['arm1']\n        a2, b2 = case['arm2']\n        gamma = case['gamma']\n        N_max = case['N_max']\n\n        gi1 = compute_gittins_index(float(a1), float(b1), gamma, N_max, epsilon)\n        gi2 = compute_gittins_index(float(a2), float(b2), gamma, N_max, epsilon)\n\n        # Tie-breaking rule: choose arm 1 if indices are equal\n        chosen_arm = 1 if gi1 >= gi2 else 2\n        \n        results.append([round(gi1, 6), round(gi2, 6), chosen_arm])\n\n    # Format the final output string\n    # E.g., [[0.123456,0.234567,2],[...]]\n    output_str = \"[\" + \",\".join([f\"[{r[0]},{r[1]},{r[2]}]\" for r in results]) + \"]\"\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}