{
    "hands_on_practices": [
        {
            "introduction": "Understanding why working correlation matters begins with its impact on study design. This exercise  has you derive the 'design effect' ($DE$), a key metric showing how within-cluster dependence inflates variance and, consequently, sample size requirements. This fundamental calculation provides a concrete justification for carefully considering correlation in any GEE analysis.",
            "id": "4984736",
            "problem": "A multicenter clinical study evaluates a primary care intervention by recording the number of asthma exacerbations per patient at $m$ equally spaced follow-up visits. Investigators plan to analyze the marginal mean of these count outcomes using Generalized Estimating Equations (GEE), where GEE denotes Generalized Estimating Equations, with a log link and an exchangeable working correlation structure. In this setting, suppose each patient forms a cluster of size $m$, the marginal variance of each visit-specific count within a cluster is $\\sigma^2$, and the intra-cluster correlation coefficient (ICC), defined as the common correlation between any two distinct visits in the same cluster, is $\\rho$.\n\nLet $\\bar{Y}_{i} = \\frac{1}{m} \\sum_{j=1}^{m} Y_{ij}$ denote the within-patient mean count. The design effect $DE$ is defined as the ratio of the variance of $\\bar{Y}_{i}$ under the specified correlation structure to the variance of $\\bar{Y}_{i}$ under independence. Starting from the fundamental definition of an exchangeable working correlation structure and the variance of a linear combination of correlated outcomes, derive an analytic expression for $DE$ in terms of $m$ and $\\rho$. Then, for a planning scenario with $m = 6$ visits per patient and $\\rho = 0.15$, compute the numerical value of $DE$.\n\nRound your final numerical answer to four significant figures. Express the final answer as a pure number without units. In your solution, explain how this $DE$ informs sample size planning in GEE-based designs for clustered counts in medicine, and justify why the working correlation structure is central to this calculation.",
            "solution": "The problem posed is a well-defined exercise in statistical theory, grounded in the principles of analyzing correlated data using Generalized Estimating Equations (GEE). The provided information is self-contained, scientifically sound, and sufficient for a complete derivation and numerical calculation. We may therefore proceed with the solution.\n\nLet $Y_{ij}$ be the count outcome for patient $i$ at visit $j$, for $j \\in \\{1, 2, \\dots, m\\}$. Each patient represents a cluster of $m$ observations. The within-patient mean count is given by $\\bar{Y}_{i} = \\frac{1}{m} \\sum_{j=1}^{m} Y_{ij}$.\n\nThe problem specifies the following:\n1.  The marginal variance of each observation is constant: $\\text{Var}(Y_{ij}) = \\sigma^2$.\n2.  The working correlation structure is exchangeable, meaning the correlation between any two distinct observations within the same cluster is a constant intra-cluster correlation coefficient (ICC), $\\rho$. That is, $\\text{Corr}(Y_{ij}, Y_{ik}) = \\rho$ for any $j \\neq k$.\n\nFrom the definition of correlation, the covariance between two distinct observations within a cluster is:\n$$ \\text{Cov}(Y_{ij}, Y_{ik}) = \\text{Corr}(Y_{ij}, Y_{ik}) \\sqrt{\\text{Var}(Y_{ij})\\text{Var}(Y_{ik})} = \\rho \\sqrt{\\sigma^2 \\cdot \\sigma^2} = \\rho\\sigma^2 $$\n\nThe design effect, $DE$, is the ratio of the variance of $\\bar{Y}_{i}$ under this correlation structure to the variance of $\\bar{Y}_{i}$ assuming independence.\n\nFirst, we derive the variance of $\\bar{Y}_{i}$ under the specified exchangeable correlation structure. The variance of $\\bar{Y}_i$ is:\n$$ \\text{Var}(\\bar{Y}_{i}) = \\text{Var}\\left(\\frac{1}{m} \\sum_{j=1}^{m} Y_{ij}\\right) = \\frac{1}{m^2} \\text{Var}\\left(\\sum_{j=1}^{m} Y_{ij}\\right) $$\nThe variance of a sum of correlated random variables is given by the sum of all elements in the covariance matrix of those variables:\n$$ \\text{Var}\\left(\\sum_{j=1}^{m} Y_{ij}\\right) = \\sum_{j=1}^{m} \\text{Var}(Y_{ij}) + \\sum_{j \\neq k} \\text{Cov}(Y_{ij}, Y_{ik}) $$\nWithin a cluster of size $m$, there are $m$ variance terms and $m(m-1)$ off-diagonal covariance terms. Substituting the given values:\n$$ \\text{Var}\\left(\\sum_{j=1}^{m} Y_{ij}\\right) = \\sum_{j=1}^{m} \\sigma^2 + \\sum_{j \\neq k} \\rho\\sigma^2 = m\\sigma^2 + m(m-1)\\rho\\sigma^2 $$\nFactoring out the term $m\\sigma^2$:\n$$ \\text{Var}\\left(\\sum_{j=1}^{m} Y_{ij}\\right) = m\\sigma^2 \\left(1 + (m-1)\\rho\\right) $$\nSubstituting this back into the expression for $\\text{Var}(\\bar{Y}_{i})$:\n$$ \\text{Var}(\\bar{Y}_{i})_{\\text{correlated}} = \\frac{1}{m^2} \\left[m\\sigma^2(1 + (m-1)\\rho)\\right] = \\frac{\\sigma^2}{m}(1 + (m-1)\\rho) $$\n\nNext, we determine the variance of $\\bar{Y}_{i}$ under the assumption of independence. Independence is a special case of the exchangeable structure where $\\rho = 0$. Substituting $\\rho = 0$ into the formula above yields:\n$$ \\text{Var}(\\bar{Y}_{i})_{\\text{independent}} = \\frac{\\sigma^2}{m}(1 + (m-1)(0)) = \\frac{\\sigma^2}{m} $$\nThis is the standard formula for the variance of a sample mean of $m$ independent and identically distributed random variables.\n\nThe design effect, $DE$, is the ratio of these two variances:\n$$ DE = \\frac{\\text{Var}(\\bar{Y}_{i})_{\\text{correlated}}}{\\text{Var}(\\bar{Y}_{i})_{\\text{independent}}} = \\frac{\\frac{\\sigma^2}{m}(1 + (m-1)\\rho)}{\\frac{\\sigma^2}{m}} $$\nThe term $\\frac{\\sigma^2}{m}$ cancels, yielding the analytic expression for the design effect:\n$$ DE = 1 + (m-1)\\rho $$\n\nFor the specific planning scenario, we are given $m = 6$ visits and an ICC of $\\rho = 0.15$. We compute the numerical value of $DE$:\n$$ DE = 1 + (6-1)(0.15) = 1 + 5(0.15) = 1 + 0.75 = 1.75 $$\nRounded to four significant figures, the value is $1.750$.\n\nThe design effect is a crucial quantity for sample size planning in studies involving clustered data. Statistical power is directly related to the precision of parameter estimates, which is inversely related to their variance. In clustered designs, positive intra-cluster correlation ($\\rho > 0$) means that observations within a cluster are not independent and provide partially redundant information. This lack of independence inflates the variance of estimators compared to a scenario with fully independent data. The design effect $DE = 1 + (m-1)\\rho$ quantifies this variance inflation factor. To maintain the desired statistical power, a sample size calculation based on an assumption of independence ($N_{\\text{indep}}$) must be adjusted upward. The required number of clusters (patients) becomes $N_{\\text{cluster}} = N_{\\text{indep}} \\times DE$. In this specific problem, with $DE = 1.750$, the study requires $75\\%$ more patients than would be estimated if the within-patient correlation were ignored. Overlooking this effect would lead to a severely underpowered study.\n\nThe specification of the working correlation structure is central to this entire calculation because it defines the mathematical form of the design effect. The formula $DE = 1 + (m-1)\\rho$ is a direct consequence of the **exchangeable** assumption, where all pairwise correlations within a cluster are identical. If a different, more complex structure were assumed (e.g., an autoregressive AR($1$) structure, where $\\text{Corr}(Y_{ij}, Y_{ik})=\\rho^{|j-k|}$), the sum of covariance terms would change, leading to a different analytical expression for $DE$ and, consequently, a different sample size requirement. While GEE provides consistent estimates of the marginal mean parameters even when the working correlation structure is misspecified, the variance estimates (and thus standard errors, confidence intervals, and hypothesis tests) rely on this structure. Accurate sample size and power calculations are therefore critically dependent on a realistic a priori choice for the working correlation that reflects the true dependency in the data.",
            "answer": "$$\\boxed{\\pmatrix{1 + (m-1)\\rho & 1.750}}$$"
        },
        {
            "introduction": "After appreciating the impact of correlation, the next step is to learn how to estimate it from data. This practice  guides you through deriving the method-of-moments estimator for the common exchangeable correlation parameter, $\\rho$. It then introduces the important real-world concepts of small-sample bias and jackknife correction, providing you with tools to obtain more accurate correlation estimates.",
            "id": "4984712",
            "problem": "A biostatistics team is analyzing a longitudinal medical study using Generalized Estimating Equations (GEE). For subject $i$ with repeated responses $y_{ij}$ and mean model $E(y_{ij} \\mid \\boldsymbol{x}_{ij}) = \\mu_{ij}$, they define the Pearson residual $r_{ij}$ by $r_{ij} = (y_{ij} - \\mu_{ij})/\\sqrt{v(\\mu_{ij})}$, where $v(\\cdot)$ is the variance function appropriate for the chosen distributional family. The working correlation matrix is specified as exchangeable $R(\\alpha)$ with a single correlation parameter $\\rho$, meaning all off-diagonal correlations within a cluster equal $\\rho$. \n\nStarting from the definitions of the Pearson residual and exchangeable working correlation, and using a method-of-moments principle that matches empirical second-order moments to their model-based counterparts, derive a sample-based estimator for $\\rho$ that uses the Pearson residuals $\\{r_{ij}\\}$. \n\nThen, the team fits the mean model and obtains the following Pearson residuals for $I=4$ subjects with within-subject measurement counts $m_1=4$, $m_2=3$, $m_3=5$, and $m_4=2$:\n- Subject $1$ ($m_1=4$): $\\{0.9,\\; 0.6,\\; 0.5,\\; 0.4\\}$,\n- Subject $2$ ($m_2=3$): $\\{1.0,\\; 0.5,\\; 0.3\\}$,\n- Subject $3$ ($m_3=5$): $\\{0.7,\\; 0.6,\\; 0.4,\\; 0.2,\\; 0.1\\}$,\n- Subject $4$ ($m_4=2$): $\\{0.2,\\; 0.3\\}$.\n\nUsing your derived estimator, compute the estimate $\\hat{\\rho}$ from these residuals.\n\nBecause the mean model parameters are estimated, the estimator $\\hat{\\rho}$ can be biased in small samples. One widely applicable bias-reduction approach is the cluster jackknife. Define the leave-one-cluster-out estimates $\\hat{\\rho}_{(-i)}$ by recomputing the same moment estimator after removing all observations from cluster $i$, and then define the jackknife bias-corrected estimator by\n$$\n\\tilde{\\rho} \\equiv I\\,\\hat{\\rho} - (I-1)\\,\\bar{\\rho}_{(-\\cdot)}, \\quad \\text{where} \\quad \\bar{\\rho}_{(-\\cdot)} \\equiv \\frac{1}{I}\\sum_{i=1}^{I} \\hat{\\rho}_{(-i)}.\n$$\nCompute $\\tilde{\\rho}$ from the provided residuals. Round your final answer to four significant figures.\n\nFinally, explain, without using any computational shortcuts, why $\\hat{\\rho}$ can be biased in small samples when using estimated means, and outline conceptually how adjustments such as the jackknife, leverage-adjusted residuals, or bias-reduced estimating equations address this bias. Your final numeric answer should be the value of $\\tilde{\\rho}$, rounded to four significant figures, with no units.",
            "solution": "The problem asks for the derivation of a moment-based estimator for the exchangeable correlation parameter $\\rho$ in a Generalized Estimating Equations (GEE) framework, its computation on a given dataset, the calculation of a jackknife bias-corrected version of this estimate, and a conceptual explanation of the bias.\n\nFirst, we validate the problem statement.\n\n### Step 1: Extract Givens\n- **Model**: Longitudinal data analysis using GEE.\n- **Subjects**: $I$ subjects (clusters), indexed by $i$.\n- **Observations**: $m_i$ repeated responses per subject $i$, indexed by $j$.\n- **Response and Mean**: $y_{ij}$ is the response, with mean $E(y_{ij} \\mid \\boldsymbol{x}_{ij}) = \\mu_{ij}$.\n- **Pearson Residual**: $r_{ij} = (y_{ij} - \\mu_{ij})/\\sqrt{v(\\mu_{ij})}$, where $v(\\cdot)$ is the variance function.\n- **Working Correlation**: Exchangeable structure $R(\\alpha)$ with $\\text{Corr}(y_{ij}, y_{ik}) = \\rho$ for all $j \\neq k$.\n- **Data**: $I=4$ subjects.\n    - $m_1=4$, residuals $\\{0.9, 0.6, 0.5, 0.4\\}$.\n    - $m_2=3$, residuals $\\{1.0, 0.5, 0.3\\}$.\n    - $m_3=5$, residuals $\\{0.7, 0.6, 0.4, 0.2, 0.1\\}$.\n    - $m_4=2$, residuals $\\{0.2, 0.3\\}$.\n- **Jackknife Estimator**: $\\tilde{\\rho} \\equiv I\\,\\hat{\\rho} - (I-1)\\,\\bar{\\rho}_{(-\\cdot)}$, with $\\bar{\\rho}_{(-\\cdot)} \\equiv \\frac{1}{I}\\sum_{i=1}^{I} \\hat{\\rho}_{(-i)}$, where $\\hat{\\rho}_{(-i)}$ is the estimate computed after removing cluster $i$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded within the standard theory of statistical modeling (GEE). It is well-posed, with all necessary information provided for derivation and calculation. The language is objective and precise. The problem does not violate any fundamental principles, is not incomplete or contradictory, and is not trivial. It represents a standard procedure in biostatistical analysis.\n\n### Step 3: Verdict and Action\nThe problem is valid. We proceed with the solution.\n\n### Part 1: Derivation of the Estimator for $\\rho$\nThe estimator for the correlation parameter $\\rho$ is derived using a method-of-moments approach. We start by examining the properties of the Pearson residuals, $r_{ij}$.\n\nBy definition of the mean model, $E(y_{ij} - \\mu_{ij}) = 0$, which implies that the expected value of a Pearson residual is zero:\n$$ E(r_{ij}) = E\\left(\\frac{y_{ij} - \\mu_{ij}}{\\sqrt{v(\\mu_{ij})}}\\right) = \\frac{E(y_{ij} - \\mu_{ij})}{\\sqrt{v(\\mu_{ij})}} = 0 $$\nThe variance of the response $y_{ij}$ is typically modeled as $Var(y_{ij}) = \\phi \\, v(\\mu_{ij})$, where $\\phi$ is a scale parameter. For many common distributions (e.g., Poisson, binomial), $\\phi=1$. Assuming $\\phi=1$, the variance of a Pearson residual is:\n$$ Var(r_{ij}) = Var\\left(\\frac{y_{ij} - \\mu_{ij}}{\\sqrt{v(\\mu_{ij})}}\\right) = \\frac{Var(y_{ij})}{v(\\mu_{ij})} = \\frac{v(\\mu_{ij})}{v(\\mu_{ij})} = 1 $$\nNow, consider the covariance between two distinct residuals, $r_{ij}$ and $r_{ik}$, from the same subject $i$ ($j \\neq k$):\n$$ \\text{Cov}(r_{ij}, r_{ik}) = \\text{Cov}\\left(\\frac{y_{ij} - \\mu_{ij}}{\\sqrt{v(\\mu_{ij})}}, \\frac{y_{ik} - \\mu_{ik}}{\\sqrt{v(\\mu_{ik})}}\\right) = \\frac{\\text{Cov}(y_{ij}, y_{ik})}{\\sqrt{v(\\mu_{ij})v(\\mu_{ik})}} $$\nBy definition of correlation, $\\text{Cov}(y_{ij}, y_{ik}) = \\text{Corr}(y_{ij}, y_{ik})\\sqrt{Var(y_{ij})Var(y_{ik})}$. With an exchangeable working correlation, $\\text{Corr}(y_{ij}, y_{ik})=\\rho$, and with $Var(y_{ij}) = v(\\mu_{ij})$, this becomes:\n$$ \\text{Cov}(y_{ij}, y_{ik}) = \\rho \\sqrt{v(\\mu_{ij})v(\\mu_{ik})} $$\nSubstituting this back into the covariance expression for the residuals:\n$$ \\text{Cov}(r_{ij}, r_{ik}) = \\frac{\\rho \\sqrt{v(\\mu_{ij})v(\\mu_{ik})}}{\\sqrt{v(\\mu_{ij})v(\\mu_{ik})}} = \\rho $$\nSince the residuals have zero mean, their expected product is equal to their covariance:\n$$ E(r_{ij} r_{ik}) = \\text{Cov}(r_{ij}, r_{ik}) + E(r_{ij})E(r_{ik}) = \\rho + 0 \\cdot 0 = \\rho $$\nThe method of moments equates this theoretical expectation to the corresponding sample average. The sample average is taken over all possible distinct pairs of observations $(j,k)$ within each subject $i$, across all subjects. The number of such pairs for subject $i$ is $\\binom{m_i}{2}$, and the total number of pairs is $\\sum_{i=1}^{I} \\binom{m_i}{2}$.\nThe moment estimator $\\hat{\\rho}$ is therefore the average of the products $r_{ij}r_{ik}$:\n$$ \\hat{\\rho} = \\frac{\\sum_{i=1}^{I} \\sum_{j<k} r_{ij} r_{ik}}{\\sum_{i=1}^{I} \\binom{m_i}{2}} $$\nThis is the required sample-based estimator for $\\rho$.\n\n### Part 2: Computation of $\\hat{\\rho}$\nWe use the derived formula and the provided data.\nLet $S_i = \\sum_{j<k} r_{ij} r_{ik}$ be the sum of cross-products for subject $i$, and $D_i = \\binom{m_i}{2}$ be the number of pairs.\n\n- **Subject 1** ($m_1=4$): $r_1 = \\{0.9, 0.6, 0.5, 0.4\\}$\n$S_1 = (0.9)(0.6) + (0.9)(0.5) + (0.9)(0.4) + (0.6)(0.5) + (0.6)(0.4) + (0.5)(0.4) = 0.54 + 0.45 + 0.36 + 0.30 + 0.24 + 0.20 = 2.09$\n$D_1 = \\binom{4}{2} = 6$\n\n- **Subject 2** ($m_2=3$): $r_2 = \\{1.0, 0.5, 0.3\\}$\n$S_2 = (1.0)(0.5) + (1.0)(0.3) + (0.5)(0.3) = 0.50 + 0.30 + 0.15 = 0.95$\n$D_2 = \\binom{3}{2} = 3$\n\n- **Subject 3** ($m_3=5$): $r_3 = \\{0.7, 0.6, 0.4, 0.2, 0.1\\}$\n$S_3 = (0.7)(0.6+0.4+0.2+0.1) + (0.6)(0.4+0.2+0.1) + (0.4)(0.2+0.1) + (0.2)(0.1) = 0.91 + 0.42 + 0.12 + 0.02 = 1.47$\n$D_3 = \\binom{5}{2} = 10$\n\n- **Subject 4** ($m_4=2$): $r_4 = \\{0.2, 0.3\\}$\n$S_4 = (0.2)(0.3) = 0.06$\n$D_4 = \\binom{2}{2} = 1$\n\nThe total sum of cross-products is $S = \\sum_{i=1}^4 S_i = 2.09 + 0.95 + 1.47 + 0.06 = 4.57$.\nThe total number of pairs is $D = \\sum_{i=1}^4 D_i = 6 + 3 + 10 + 1 = 20$.\nThe estimate $\\hat{\\rho}$ is:\n$$ \\hat{\\rho} = \\frac{S}{D} = \\frac{4.57}{20} = 0.2285 $$\n\n### Part 3: Computation of the Jackknife Estimator $\\tilde{\\rho}$\nThe jackknife estimator is $\\tilde{\\rho} = I\\,\\hat{\\rho} - (I-1)\\,\\bar{\\rho}_{(-\\cdot)}$. With $I=4$, this is $\\tilde{\\rho} = 4\\,\\hat{\\rho} - 3\\,\\bar{\\rho}_{(-\\cdot)}$.\nWe need to calculate the leave-one-out estimates $\\hat{\\rho}_{(-i)}$ by removing subject $i$ one at a time. The formula for $\\hat{\\rho}_{(-i)}$ is $\\frac{S - S_i}{D - D_i}$.\n\n- **Leave out Subject 1**:\n$\\hat{\\rho}_{(-1)} = \\frac{S - S_1}{D - D_1} = \\frac{4.57 - 2.09}{20 - 6} = \\frac{2.48}{14} \\approx 0.177143$\n\n- **Leave out Subject 2**:\n$\\hat{\\rho}_{(-2)} = \\frac{S - S_2}{D - D_2} = \\frac{4.57 - 0.95}{20 - 3} = \\frac{3.62}{17} \\approx 0.212941$\n\n- **Leave out Subject 3**:\n$\\hat{\\rho}_{(-3)} = \\frac{S - S_3}{D - D_3} = \\frac{4.57 - 1.47}{20 - 10} = \\frac{3.10}{10} = 0.31$\n\n- **Leave out Subject 4**:\n$\\hat{\\rho}_{(-4)} = \\frac{S - S_4}{D - D_4} = \\frac{4.57 - 0.06}{20 - 1} = \\frac{4.51}{19} \\approx 0.237368$\n\nNow we compute the average of these leave-one-out estimates:\n$$ \\bar{\\rho}_{(-\\cdot)} = \\frac{1}{4} \\sum_{i=1}^4 \\hat{\\rho}_{(-i)} \\approx \\frac{1}{4}(0.177143 + 0.212941 + 0.31 + 0.237368) = \\frac{0.937452}{4} = 0.234363 $$\nFinally, we compute the jackknife bias-corrected estimate $\\tilde{\\rho}$:\n$$ \\tilde{\\rho} = 4\\,\\hat{\\rho} - 3\\,\\bar{\\rho}_{(-\\cdot)} \\approx 4(0.2285) - 3(0.234363) = 0.914 - 0.703089 = 0.210911 $$\nRounding to four significant figures, we get $\\tilde{\\rho} \\approx 0.2109$.\n\n### Part 4: Explanation of Bias and Correction Methods\nThe moment-based estimator $\\hat{\\rho}$ can be biased, particularly in samples with a small number of clusters ($I$). This bias arises because the estimator is calculated using Pearson residuals, $r_{ij} = (y_{ij} - \\hat{\\mu}_{ij})/\\sqrt{v(\\hat{\\mu}_{ij})}$, which are based on the *estimated* means $\\hat{\\mu}_{ij}$, not the true (and unknown) means $\\mu_{ij}$. The mean parameters $\\hat{\\beta}$ (upon which $\\hat{\\mu}_{ij}$ depend) are estimated from the same data. This estimation process tends to \"fit\" the data, making the estimated means $\\hat{\\mu}_{ij}$ closer to the observed responses $y_{ij}$ than the true means $\\mu_{ij}$ are. Consequently, the residuals $(y_{ij} - \\hat{\\mu}_{ij})$ are systematically smaller in magnitude than the \"true error\" terms $(y_{ij} - \\mu_{ij})$. This shrinkage of residuals towards zero causes the cross-products $r_{ij}r_{ik}$ to be smaller, on average, than their true counterparts. As a result, $\\hat{\\rho}$, which is an average of these cross-products, tends to be biased towards zero (underestimation). The bias is of order $O(1/I)$, making it more severe when the number of clusters $I$ is small.\n\nSeveral methods have been developed to address this small-sample bias:\n\n$1$. **Jackknife Resampling**: As demonstrated in this problem, the jackknife provides a general, non-parametric method for bias correction. It estimates the bias by observing how the statistic ($\\hat{\\rho}$) changes when each cluster is removed from the dataset. The difference between the original estimate and the average of the leave-one-out estimates provides an estimate of the bias, which is then subtracted from the original estimate. This procedure provably removes the leading term (e.g., $O(1/I)$) of the bias for many common estimators.\n\n$2$. **Leverage-Adjusted Residuals**: This method provides a more direct correction by adjusting the residuals themselves. In regression, points with high leverage have an outsized influence on the fitted model, pulling the fitted values closer to the observed values and resulting in smaller residuals. A similar concept applies in GEE. By calculating a generalized measure of leverage for each cluster, one can \"inflate\" the Pearson residuals to counteract the shrinkage caused by fitting the mean model. Common adjustments involve dividing the residuals by a factor related to leverage, such as $\\sqrt{1-h_{ii}}$ where $h_{ii}$ is a leverage-like quantity. Using these adjusted residuals in the moment estimator for $\\rho$ yields a less biased estimate.\n\n$3$. **Bias-Reduced or Modified Estimating Equations**: Instead of correcting the final estimate, this approach modifies the estimating equation used to find the estimate. The standard moment-based estimating equation for $\\rho$ is $\\sum_{i,j<k} (r_{ij} r_{ik} - \\rho) = 0$. Because the residuals use $\\hat{\\beta}$ instead of the true $\\beta$, the expected value of this estimating function is not zero. One can calculate an analytical approximation to this bias term (the non-zero expectation) and subtract it from the estimating equation, yielding a modified equation like $\\sum_{i,j<k} (r_{ij} r_{ik} - \\rho) - \\text{BiasTerm}(\\beta, \\rho) = 0$. Solving this new equation for $\\rho$ gives an estimate with reduced bias. This approach directly targets the source of the bias within the estimation machinery.",
            "answer": "$$\\boxed{0.2109}$$"
        },
        {
            "introduction": "Real-world analysis requires choosing among several plausible correlation structures. This capstone exercise  provides a complete workflow for this critical task. By implementing and comparing models using the Quasi-likelihood under the Independence model Criterion (QIC), you will learn how to make a data-driven, defensible choice for the working correlation in your GEE model.",
            "id": "4984695",
            "problem": "You are given a clinical longitudinal binary-outcome setting in which subjects are measured repeatedly over time. Your task is to build a program that implements a workflow for selecting among candidate working correlation structures in Generalized Estimating Equations (GEE) using the Quasi-likelihood under the Independence model Criterion (QIC) and the Correlation Information Criterion (CIC), and then re-estimating the regression coefficients after selecting the working correlation structure.\n\nThe foundational base is the following set of core definitions and well-tested facts:\n- Generalized Estimating Equations (GEE) specify regression for clustered or longitudinal data by solving the estimating equations that set the sum of cluster-wise contributions, formed from residuals weighted by the inverse of a working covariance, equal to zero.\n- For binary outcomes with the logistic link, the mean is $ \\mu_{ij} = \\mathrm{logit}^{-1}(x_{ij}^{\\top} \\beta) $ and the variance is $ \\mathrm{Var}(Y_{ij}) = \\mu_{ij} (1 - \\mu_{ij}) $.\n- Working correlation structures commonly used in practice include independence, exchangeable, and first-order autoregressive (AR(1)).\n- The quasi-likelihood for binary data under the independence working covariance coincides with the binomial log-likelihood up to an additive constant, so it can be evaluated directly from $ \\mu_{ij} $.\n- The model-based covariance of $ \\beta $ uses the working covariance, and the robust (sandwich) covariance uses the empirical residual variability; both are standard in GEE.\n- The Quasi-likelihood under the Independence model Criterion (QIC) and the Correlation Information Criterion (CIC) are standard, widely used indices for selecting the mean and correlation structures in GEE by balancing goodness-of-fit and complexity.\n\nYour program must implement the following, entirely from first principles, without any external files or user input:\n1. Fit GEE with a logistic link for each candidate working correlation structure in the set {independence, exchangeable, AR(1)}. Use the standard GEE estimating equations with a binary logistic variance function, and estimate the correlation parameter for exchangeable and AR(1) by method of moments based on Pearson residuals.\n2. For each candidate structure, compute:\n   - $ \\mathrm{QIC} = -2 \\, Q(\\hat{\\beta}; \\text{independence}) + 2 \\, \\mathrm{trace}(M_I \\, \\widehat{V}_\\beta) $ where $ Q(\\hat{\\beta}; \\text{independence}) $ is the quasi-likelihood under independence evaluated at $ \\hat{\\beta} $ from the candidate fit, $ M_I $ is the model-based information matrix under independence evaluated at $ \\hat{\\beta} $, and $ \\widehat{V}_\\beta $ is the robust sandwich covariance from the candidate fit.\n   - $ \\mathrm{CIC} = \\mathrm{trace}(M_I \\, \\widehat{V}_\\beta) $.\n3. Select the working correlation that minimizes QIC. If there is a tie in QIC up to numerical tolerance, break ties by choosing the smallest CIC. If there is still a tie, prefer independence over exchangeable over AR(1) in that order.\n4. Re-estimate $ \\beta $ under the selected working correlation structure and report the final parameter vector $ \\hat{\\beta} $.\n\nUse the following three test cases. Each test case is a complete dataset specified by subject identifiers, observation times, a time-invariant treatment indicator per subject, and a binary response. Data are provided sorted by subject and increasing time. The design matrix $ X $ must include an intercept, the treatment indicator, and the time variable in that order.\n\nTest case 1:\n- Subjects and sizes: $1,2,3,4,5,6$ (each with $3$ visits).\n- Times per subject: $[0,1,2]$ for each subject.\n- Treatment by subject: subject $1 \\rightarrow 0$, $2 \\rightarrow 0$, $3 \\rightarrow 1$, $4 \\rightarrow 1$, $5 \\rightarrow 0$, $6 \\rightarrow 1$; expand per visit.\n- Responses by subject:\n  - Subject $1$: $[0, 0, 1]$\n  - Subject $2$: $[0, 1, 1]$\n  - Subject $3$: $[1, 1, 1]$\n  - Subject $4$: $[0, 1, 1]$\n  - Subject $5$: $[0, 0, 0]$\n  - Subject $6$: $[1, 1, 0]$\n\nTest case 2:\n- Subjects and sizes: $10$ $(1$ visit$)$, $11$ $(2$ visits$)$, $12$ $(4$ visits$)$, $13$ $(3$ visits$)$.\n- Times per subject: subject $10$: $[0]$; $11$: $[0,1]$; $12$: $[0,1,2,3]$; $13$: $[0,1,2]$.\n- Treatment by subject: $10 \\rightarrow 0$, $11 \\rightarrow 1$, $12 \\rightarrow 0$, $13 \\rightarrow 1$; expand per visit.\n- Responses by subject:\n  - Subject $10$: $[0]$\n  - Subject $11$: $[1, 1]$\n  - Subject $12$: $[0, 0, 1, 0]$\n  - Subject $13$: $[1, 0, 1]$\n\nTest case 3:\n- Subjects and sizes: $20, 21, 22$ (each with $5$ visits).\n- Times per subject: $[0,1,2,3,4]$ for each subject.\n- Treatment by subject: $20 \\rightarrow 0$, $21 \\rightarrow 1$, $22 \\rightarrow 1$; expand per visit.\n- Responses by subject:\n  - Subject $20$: $[0, 0, 0, 1, 1]$\n  - Subject $21$: $[1, 1, 1, 1, 0]$\n  - Subject $22$: $[1, 1, 0, 0, 0]$\n\nAngle units are not involved. No physical units are involved.\n\nFinal output required format:\n- For each test case, output a list of four numbers: the selected working correlation encoded as an integer ($0$ for independence, $1$ for exchangeable, $2$ for AR(1)$)$, followed by the three entries of the re-estimated parameter vector $ \\hat{\\beta} $ corresponding to intercept, treatment, and time, respectively. Round each floating-point number to six decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list of the three per-case lists enclosed in square brackets. For example: \"[[sel,b0,b1,b2],[sel,b0,b1,b2],[sel,b0,b1,b2]]\".\n\nThis problem is purely mathematical and algorithmic; it does not require any domain-specific units. Implementations in any modern programming language should follow the logic above and the provided datasets exactly to produce deterministic results.",
            "solution": "The problem statement has been meticulously reviewed and is determined to be valid. It presents a clear, self-contained, and scientifically grounded task in the domain of statistical modeling. The problem is well-posed, objective, and provides all necessary information, definitions, and data to arrive at a unique, verifiable solution. The methodology specified—using Generalized Estimating Equations (GEE) with various working correlation structures and selecting the optimal structure via Quasi-likelihood under the Independence model Criterion (QIC) and Correlation Information Criterion (CIC)—is a standard and well-established statistical procedure.\n\nThe solution will be developed by first principles as requested. The core of the solution is an implementation of the GEE fitting procedure using an iterative Fisher scoring algorithm, followed by the calculation of model selection criteria and the application of the specified selection rules.\n\n**1. Theoretical Foundation of Generalized Estimating Equations (GEE)**\n\nLet $Y_i$ be the $n_i \\times 1$ vector of binary responses for subject $i$, where $i=1, \\dots, K$. Let $X_i$ be the corresponding $n_i \\times p$ matrix of covariates. The GEE model for the marginal mean of the response is specified via a link function $g(\\cdot)$. For this problem, the outcome is binary, so we use the logistic link function, $g(\\mu_{ij}) = \\mathrm{logit}(\\mu_{ij}) = x_{ij}^\\top \\beta$. The mean vector for subject $i$ is thus $\\mu_i(\\beta)$, with elements $\\mu_{ij} = E[Y_{ij}] = (1 + \\exp(-x_{ij}^\\top \\beta))^{-1}$. The variance of each response is given by the variance function for a Bernoulli random variable, $\\mathrm{Var}(Y_{ij}) = v(\\mu_{ij}) = \\mu_{ij}(1-\\mu_{ij})$.\n\nThe regression parameter vector $\\beta$ is estimated by solving the GEE:\n$$ \\mathcal{U}(\\beta) = \\sum_{i=1}^{K} D_i^\\top V_i^{-1} (Y_i - \\mu_i(\\beta)) = 0 $$\nwhere:\n- $D_i = \\frac{\\partial \\mu_i}{\\partial \\beta}$ is the $n_i \\times p$ matrix of derivatives of the mean with respect to the parameters. For the logistic link, $D_i = A_i X_i$, where $A_i$ is an $n_i \\times n_i$ diagonal matrix with diagonal elements $v(\\mu_{ij})$.\n- $V_i$ is the $n_i \\times n_i$ \"working\" covariance matrix for subject $i$. It is structured as $V_i = \\phi A_i^{1/2} R_i(\\alpha) A_i^{1/2}$. For binary data, the scale parameter $\\phi$ is fixed to $1$. $R_i(\\alpha)$ is the $n_i \\times n_i$ \"working\" correlation matrix, which depends on a parameter vector $\\alpha$.\n\nThe candidate working correlation structures are:\n- **Independence**: $R_i$ is the identity matrix, $I_{n_i}$. This assumes observations within a subject are uncorrelated.\n- **Exchangeable**: All off-diagonal elements of $R_i$ are equal to a single parameter $\\alpha$. $[R_i]_{jk} = \\alpha$ for $j \\neq k$ and $1$ for $j=k$.\n- **Autoregressive (AR(1))**: The correlation decays with time separation. $[R_i]_{jk} = \\alpha^{|t_{ij}-t_{ik}|}$, where $t_{ij}$ is the observation time.\n\nSince the estimating equations are non-linear in $\\beta$, the solution is found iteratively. A Fisher scoring algorithm is employed, with the update step at iteration $(t+1)$ given by:\n$$ \\beta^{(t+1)} = \\beta^{(t)} + \\left( \\sum_{i=1}^{K} D_i^{(t)\\top} V_i^{(t)-1} D_i^{(t)} \\right)^{-1} \\left( \\sum_{i=1}^{K} D_i^{(t)\\top} V_i^{(t)-1} (Y_i - \\mu_i^{(t)}) \\right) $$\nFor the exchangeable and AR(1) structures, the correlation parameter $\\alpha$ is also re-estimated at each iteration using the method of moments based on the current Pearson residuals, $e_{ij} = (Y_{ij}-\\mu_{ij})/\\sqrt{v(\\mu_{ij})}$.\n- For exchangeable correlation: $\\hat{\\alpha} = \\left(\\sum_i n_i(n_i-1)/2\\right)^{-1} \\sum_i \\sum_{j<k} e_{ij} e_{ik}$.\n- For AR(1) correlation: $\\hat{\\alpha} = \\left(\\sum_i (n_i-1)\\right)^{-1} \\sum_i \\sum_{j=2}^{n_i} e_{ij} e_{i,j-1}$ (assuming adjacent time points are separated by one unit, which holds true for estimation but not necessarily for constructing the full $R_i$ matrix).\n\nUpon convergence to $\\hat{\\beta}$, the robust \"sandwich\" covariance estimator for $\\hat{\\beta}$ is computed:\n$$ \\widehat{V}_\\beta = H_0^{-1} H_1 H_0^{-1} $$\nwhere $H_0 = \\sum_{i=1}^K D_i^\\top V_i^{-1} D_i$ is the \"naïve\" or model-based covariance inverse, and $H_1 = \\sum_{i=1}^K D_i^\\top V_i^{-1} (Y_i - \\hat{\\mu}_i)(Y_i - \\hat{\\mu}_i)^\\top V_i^{-1} D_i$ accounts for the empirical covariance. All terms are evaluated at $\\hat{\\beta}$.\n\n**2. Model Selection via QIC and CIC**\n\nTo select the best working correlation structure among the candidates, we use QIC and CIC. For each candidate model fit (e.g., AR(1)), we obtain an estimate $\\hat{\\beta}_{\\text{cand}}$ and its robust covariance $\\widehat{V}_\\beta(\\hat{\\beta}_{\\text{cand}})$. The QIC is then calculated as:\n$$ \\mathrm{QIC} = -2 \\, Q(\\hat{\\beta}_{\\text{cand}}; I) + 2 \\, \\mathrm{trace}(M_I(\\hat{\\beta}_{\\text{cand}}) \\, \\widehat{V}_\\beta(\\hat{\\beta}_{\\text{cand}})) $$\nwhere:\n- $Q(\\hat{\\beta}_{\\text{cand}}; I)$ is the log-quasi-likelihood from a model with an independence working correlation, but evaluated at the parameters $\\hat{\\beta}_{\\text{cand}}$ from the candidate model. For binary data, this is the binomial log-likelihood: $Q(\\beta; I) = \\sum_{i,j} [Y_{ij} \\log(\\mu_{ij}) + (1-Y_{ij})\\log(1-\\mu_{ij})]$.\n- $M_I(\\hat{\\beta}_{\\text{cand}})$ is the model-based information matrix under the independence assumption, $M_I = \\sum_i X_i^\\top A_i X_i$, evaluated at $\\hat{\\beta}_{\\text{cand}}$.\n- The trace term serves as a penalty for model complexity, analogous to the role of parameter count in AIC. The CIC is defined as this penalty term: $\\mathrm{CIC} = \\mathrm{trace}(M_I \\, \\widehat{V}_\\beta)$.\n\nThe selection procedure is as follows:\n1. For each candidate structure {independence, exchangeable, AR(1)}, fit the GEE model to get $\\hat{\\beta}$ and $\\widehat{V}_\\beta$.\n2. Compute QIC and CIC for each fit.\n3. Select the structure that yields the minimum QIC value.\n4. If QIC values are tied (within a small numerical tolerance), the tie is broken by selecting the model with the minimum CIC value.\n5. If a tie persists, the simplest model is preferred in the order: independence, then exchangeable, then AR(1).\n6. The final parameters to be reported are the $\\hat{\\beta}$ from the selected model.\n\n**3. Algorithmic Procedure**\n\nThe implementation will follow these steps for each test case provided:\n\n1.  **Data Preparation**: The subject identifiers, visit times, treatment indicators, and binary responses are organized into vectors and a design matrix $X$ (with columns for intercept, treatment, and time). Data are grouped by subject ID.\n2.  **Iterative Fitting**: For each of the three working correlation structures:\n    a. Initialize $\\beta$ (e.g., to zeros).\n    b. Iterate the Fisher scoring update for $\\beta$, re-estimating $\\alpha$ (for exchangeable and AR(1)) at each step, until the change in $\\beta$ is below a convergence threshold.\n    c. On convergence, compute the final robust covariance matrix $\\widehat{V}_\\beta$.\n3.  **Criteria Calculation**: Using the converged $\\hat{\\beta}$ and $\\widehat{V}_\\beta$ from each candidate fit, calculate the corresponding QIC and CIC values.\n4.  **Selection and Output**: Apply the specified selection rules to the computed (QIC, CIC) pairs to identify the optimal working correlation structure. The corresponding integer code ($0, 1,$ or $2$) and the three estimated regression coefficients ($\\hat{\\beta}_0, \\hat{\\beta}_1, \\hat{\\beta}_2$) are formatted and stored.\n\nThis comprehensive procedure ensures a robust and theoretically sound solution to the problem. The final program encapsulates this logic to process the test cases and generate the required output.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef _inv_logit(x):\n    \"\"\"Numerically stable inverse logit (sigmoid) function.\"\"\"\n    # Clip to avoid overflow in exp\n    x = np.clip(x, -500, 500)\n    return 1.0 / (1.0 + np.exp(-x))\n\nclass GEE_Binary:\n    \"\"\"\n    Implementation of GEE for binary outcomes from first principles.\n    \"\"\"\n    def __init__(self, y, X, ids, times, corr_type='independence', tol=1e-7, max_iter=50):\n        self.y = np.array(y, dtype=float)\n        self.X = np.array(X, dtype=float)\n        self.ids = np.array(ids)\n        self.times = np.array(times)\n        self.corr_type = corr_type\n        self.tol = tol\n        self.max_iter = max_iter\n        \n        self.n_obs, self.n_params = self.X.shape\n        self.unique_ids = np.unique(self.ids)\n        self.n_clusters = len(self.unique_ids)\n        \n        self.beta = np.zeros(self.n_params)\n        self.alpha = 0.0\n        self.V_robust = np.zeros((self.n_params, self.n_params))\n\n    def _get_cluster_data(self, subject_id):\n        mask = self.ids == subject_id\n        return self.y[mask], self.X[mask], self.times[mask]\n\n    def _estimate_alpha(self, mu):\n        if self.corr_type == 'independence':\n            return 0.0\n\n        pearson_res = (self.y - mu) / np.sqrt(np.clip(mu * (1 - mu), 1e-10, 1))\n        \n        num = 0.0\n        den = 0.0\n\n        for subject_id in self.unique_ids:\n            res_i = pearson_res[self.ids == subject_id]\n            n_i = len(res_i)\n            if n_i = 1:\n                continue\n\n            if self.corr_type == 'exchangeable':\n                # Sum over all unique pairs\n                for i in range(n_i):\n                    for j in range(i + 1, n_i):\n                        num += res_i[i] * res_i[j]\n                den += n_i * (n_i - 1) / 2.0\n            \n            elif self.corr_type == 'ar1':\n                # Sum over adjacent pairs\n                for i in range(n_i - 1):\n                    num += res_i[i] * res_i[i+1]\n                den += n_i - 1\n        \n        if den == 0:\n            return 0.0\n        \n        # Clip alpha to be within a valid range\n        return np.clip(num / den, -0.99, 0.99)\n\n    def _build_R_matrix(self, n_i, times_i, alpha):\n        R = np.identity(n_i)\n        if self.corr_type == 'exchangeable':\n            R[np.triu_indices(n_i, 1)] = alpha\n            R[np.tril_indices(n_i, -1)] = alpha\n        elif self.corr_type == 'ar1':\n            for i in range(n_i):\n                for j in range(i + 1, n_i):\n                    time_diff = abs(times_i[i] - times_i[j])\n                    corr = alpha ** time_diff\n                    R[i, j] = R[j, i] = corr\n        return R\n\n    def fit(self):\n        # Initial beta from a simple logistic regression (GEE with independence)\n        if self.corr_type != 'independence':\n            gee_indep = GEE_Binary(self.y, self.X, self.ids, self.times, 'independence')\n            gee_indep.fit()\n            self.beta = gee_indep.beta\n\n        for k in range(self.max_iter):\n            beta_old = self.beta.copy()\n            \n            eta = self.X @ self.beta\n            mu = _inv_logit(eta)\n            \n            # Clip mu to prevent variances of zero\n            mu = np.clip(mu, 1e-8, 1 - 1e-8)\n\n            if self.corr_type != 'independence':\n                self.alpha = self._estimate_alpha(mu)\n\n            sum_D_invV_D = np.zeros((self.n_params, self.n_params))\n            sum_D_invV_res = np.zeros(self.n_params)\n\n            for subject_id in self.unique_ids:\n                y_i, X_i, times_i = self._get_cluster_data(subject_id)\n                mu_i = mu[self.ids == subject_id]\n                n_i = len(y_i)\n                if n_i == 0: continue\n\n                res_i = y_i - mu_i\n                \n                A_i_diag = mu_i * (1 - mu_i)\n                A_i = np.diag(A_i_diag)\n                D_i = (X_i.T * A_i_diag).T # D_i = A_i @ X_i more explicit\n\n                R_i = self._build_R_matrix(n_i, times_i, self.alpha)\n                \n                # V_i = A_i^1/2 R_i A_i^1/2\n                A_i_sqrt = np.diag(np.sqrt(A_i_diag))\n                V_i = A_i_sqrt @ R_i @ A_i_sqrt\n                \n                try:\n                    V_i_inv = np.linalg.inv(V_i)\n                except np.linalg.LinAlgError:\n                    # Fallback to independence if matrix is singular\n                    V_i_inv = np.diag(1.0 / np.clip(A_i_diag, 1e-10, 1))\n                \n                sum_D_invV_D += X_i.T @ A_i @ V_i_inv @ A_i @ X_i\n                sum_D_invV_res += X_i.T @ A_i @ V_i_inv @ res_i\n\n            try:\n                update = np.linalg.inv(sum_D_invV_D) @ sum_D_invV_res\n                self.beta += update\n            except np.linalg.LinAlgError:\n                # If update fails, break the loop\n                break\n\n            if np.linalg.norm(self.beta - beta_old)  self.tol:\n                break\n        \n        # After convergence, compute robust covariance\n        self._compute_robust_covariance()\n\n    def _compute_robust_covariance(self):\n        eta = self.X @ self.beta\n        mu = _inv_logit(eta)\n        mu = np.clip(mu, 1e-8, 1 - 1e-8)\n        \n        H0 = np.zeros((self.n_params, self.n_params))\n        H1 = np.zeros((self.n_params, self.n_params))\n\n        for subject_id in self.unique_ids:\n            y_i, X_i, times_i = self._get_cluster_data(subject_id)\n            mu_i = mu[self.ids == subject_id]\n            n_i = len(y_i)\n            if n_i == 0: continue\n\n            res_i = y_i - mu_i\n            A_i_diag = mu_i * (1 - mu_i)\n            A_i = np.diag(A_i_diag)\n            \n            D_i = (X_i.T * A_i_diag).T\n\n            R_i = self._build_R_matrix(n_i, times_i, self.alpha)\n            A_i_sqrt = np.diag(np.sqrt(A_i_diag))\n            V_i = A_i_sqrt @ R_i @ A_i_sqrt\n            \n            try:\n                V_i_inv = np.linalg.inv(V_i)\n            except np.linalg.LinAlgError:\n                V_i_inv = np.diag(1.0 / np.clip(A_i_diag, 1e-10, 1))\n\n            C_i = X_i.T @ A_i @ V_i_inv\n            H0 += C_i @ A_i @ X_i\n            \n            outer_res = res_i.reshape(-1, 1) @ res_i.reshape(1, -1)\n            H1 += C_i @ outer_res @ C_i.T\n            \n        try:\n            H0_inv = np.linalg.inv(H0)\n            self.V_robust = H0_inv @ H1 @ H0_inv\n        except np.linalg.LinAlgError:\n            self.V_robust = np.full((self.n_params, self.n_params), np.nan)\n\ndef solve():\n    test_cases = [\n        {\n            \"ids_raw\": [(1, 3), (2, 3), (3, 3), (4, 3), (5, 3), (6, 3)],\n            \"times_raw\": [0, 1, 2],\n            \"treat_raw\": {1: 0, 2: 0, 3: 1, 4: 1, 5: 0, 6: 1},\n            \"y_raw\": {1: [0, 0, 1], 2: [0, 1, 1], 3: [1, 1, 1], 4: [0, 1, 1], 5: [0, 0, 0], 6: [1, 1, 0]}\n        },\n        {\n            \"ids_raw\": [(10, 1), (11, 2), (12, 4), (13, 3)],\n            \"times_raw\": {10: [0], 11: [0, 1], 12: [0, 1, 2, 3], 13: [0, 1, 2]},\n            \"treat_raw\": {10: 0, 11: 1, 12: 0, 13: 1},\n            \"y_raw\": {10: [0], 11: [1, 1], 12: [0, 0, 1, 0], 13: [1, 0, 1]}\n        },\n        {\n            \"ids_raw\": [(20, 5), (21, 5), (22, 5)],\n            \"times_raw\": [0, 1, 2, 3, 4],\n            \"treat_raw\": {20: 0, 21: 1, 22: 1},\n            \"y_raw\": {20: [0, 0, 0, 1, 1], 21: [1, 1, 1, 1, 0], 22: [1, 1, 0, 0, 0]}\n        }\n    ]\n\n    all_results = []\n    \n    for case_data in test_cases:\n        ids, times, treat, y = [], [], [], []\n        for subj_id, n_visits in case_data[\"ids_raw\"]:\n            ids.extend([subj_id] * n_visits)\n            if isinstance(case_data[\"times_raw\"], list):\n                times.extend(case_data[\"times_raw\"][:n_visits])\n            else:\n                times.extend(case_data[\"times_raw\"][subj_id])\n            treat.extend([case_data[\"treat_raw\"][subj_id]] * n_visits)\n            y.extend(case_data[\"y_raw\"][subj_id])\n\n        y, times, treat, ids = np.array(y), np.array(times), np.array(treat), np.array(ids)\n        X = np.c_[np.ones(len(y)), treat, times]\n        \n        candidate_fits = []\n        corr_types = ['independence', 'exchangeable', 'ar1']\n\n        for i, corr_type in enumerate(corr_types):\n            model = GEE_Binary(y, X, ids, times, corr_type=corr_type)\n            model.fit()\n            \n            beta_cand = model.beta\n            V_robust_cand = model.V_robust\n\n            eta_cand = X @ beta_cand\n            mu_cand = _inv_logit(eta_cand)\n            mu_cand = np.clip(mu_cand, 1e-8, 1 - 1e-8)\n\n            # Q(beta_cand; I)\n            quasi_likelihood = np.sum(y * np.log(mu_cand) + (1 - y) * np.log(1 - mu_cand))\n            \n            # M_I(beta_cand)\n            A_diag_cand = mu_cand * (1 - mu_cand)\n            M_I = X.T @ (A_diag_cand[:, np.newaxis] * X)\n\n            # QIC and CIC\n            trace_term = np.trace(M_I @ V_robust_cand)\n            qic = -2 * quasi_likelihood + 2 * trace_term\n            cic = trace_term\n            \n            candidate_fits.append({\n                'id': i, 'corr_type': corr_type, 'qic': qic, 'cic': cic, 'beta': beta_cand\n            })\n        \n        # Selection logic\n        # Sort by QIC, then by CIC, then by id (ind->exch->ar1)\n        best_fit = sorted(candidate_fits, key=lambda x: (x['qic'], x['cic'], x['id']))[0]\n        \n        result_row = [best_fit['id']] + [round(b, 6) for b in best_fit['beta']]\n        all_results.append(str(result_row).replace(\" \", \"\"))\n\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```"
        }
    ]
}