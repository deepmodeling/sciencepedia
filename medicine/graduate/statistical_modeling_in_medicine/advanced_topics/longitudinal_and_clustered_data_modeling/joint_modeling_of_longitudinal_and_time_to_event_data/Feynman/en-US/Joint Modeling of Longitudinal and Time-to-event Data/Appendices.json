{
    "hands_on_practices": [
        {
            "introduction": "Before delving into the full complexity of joint models, it is crucial to understand the behavior of their components. This first exercise focuses on the longitudinal submodel to build intuition for a fundamental statistical concept: Bayesian shrinkage . By temporarily setting the link to the survival outcome to zero ($\\alpha=0$), we can clearly see how the model judiciously combines information from a subject's individual measurements with information from the overall population to produce a more stable and realistic estimate of their underlying trajectory.",
            "id": "4968602",
            "problem": "In a joint model used in a medical study, the longitudinal submodel for a subject-specific biomarker is given by the random-intercept Gaussian model\n$$\ny_{ij} \\mid b_i \\sim \\mathcal{N}\\!\\left(\\mu + b_i,\\, \\sigma^{2}\\right), \\quad j = 1,\\dots,n_i,\n$$\nwith independent measurement errors across visits, and the subject-specific random intercept has a normal prior\n$$\nb_i \\sim \\mathcal{N}\\!\\left(0,\\, \\tau^{2}\\right).\n$$\nThe time-to-event submodel is a proportional hazards model with hazard\n$$\nh_i(t \\mid b_i) = h_0(t)\\,\\exp\\!\\left(\\alpha\\, b_i\\right).\n$$\nTo isolate the role of the longitudinal measurement error variance, suppose that the event-time contribution is conditionally uninformative for the random effect by taking $\\alpha = 0$, so that the likelihood factor from the event-time submodel does not involve $b_i$.\n\nStarting only from the definitions of the normal density and Bayes’s rule, derive the conditional posterior distribution of $b_i$ given the longitudinal measurements $\\{y_{ij}\\}_{j=1}^{n_i}$ and the hyperparameters $\\mu$, $\\sigma^{2}$, and $\\tau^{2}$. In particular, compute the posterior variance $\\operatorname{Var}(b_i \\mid y_{i1},\\dots,y_{in_i})$ in closed form as a function of $n_i$, $\\sigma^{2}$, and $\\tau^{2}$. Then, using the same derivation, identify how increasing $\\sigma^{2}$ affects the shrinkage of the posterior mean of $b_i$ toward zero, and explain why this occurs in terms of the posterior precision. Do not substitute numerical values. Your final reported answer must be the single closed-form analytic expression for the posterior variance $\\operatorname{Var}(b_i \\mid y_{i1},\\dots,y_{in_i})$. No rounding is required and no units are to be reported.",
            "solution": "The problem is valid as it presents a standard, well-posed problem in Bayesian statistical modeling, grounded in established probability theory. All necessary components (likelihood, prior, simplifying assumptions) are provided, and there are no scientific or logical contradictions. We will proceed with the derivation.\n\nThe objective is to derive the conditional posterior distribution of the random intercept $b_i$ for subject $i$, given the longitudinal measurements $\\{y_{ij}\\}_{j=1}^{n_i}$ and the hyperparameters $\\mu$, $\\sigma^{2}$, and $\\tau^{2}$. The problem specifies that the event-time submodel is uninformative for $b_i$ by setting the association parameter $\\alpha$ to $0$.\n\nAccording to Bayes’s rule, the posterior probability density function (PDF) for $b_i$ is proportional to the product of the likelihood of the data given $b_i$ and the prior PDF of $b_i$. We denote the set of measurements for subject $i$ as $Y_i = \\{y_{ij}\\}_{j=1}^{n_i}$.\n\n$$\np(b_i \\mid Y_i, \\mu, \\sigma^2, \\tau^2) \\propto p(Y_i \\mid b_i, \\mu, \\sigma^2) \\cdot p(b_i \\mid \\tau^2)\n$$\n\nThe hyperparameters are treated as fixed, known quantities. First, we define the likelihood function, $p(Y_i \\mid b_i, \\mu, \\sigma^2)$. The model states that, conditional on $b_i$, the measurements $y_{ij}$ are independent and identically distributed as $y_{ij} \\mid b_i \\sim \\mathcal{N}(\\mu + b_i, \\sigma^2)$. The PDF for a single measurement is:\n$$\np(y_{ij} \\mid b_i, \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_{ij} - (\\mu + b_i))^2}{2\\sigma^2}\\right)\n$$\nDue to independence, the likelihood for the set of $n_i$ measurements is the product of the individual PDFs:\n$$\np(Y_i \\mid b_i, \\mu, \\sigma^2) = \\prod_{j=1}^{n_i} p(y_{ij} \\mid b_i, \\mu, \\sigma^2) = (2\\pi\\sigma^2)^{-n_i/2} \\exp\\left( -\\frac{1}{2\\sigma^2} \\sum_{j=1}^{n_i} (y_{ij} - \\mu - b_i)^2 \\right)\n$$\n\nNext, we define the prior distribution for $b_i$, which is given as $b_i \\sim \\mathcal{N}(0, \\tau^2)$. The prior PDF is:\n$$\np(b_i \\mid \\tau^2) = \\frac{1}{\\sqrt{2\\pi\\tau^2}} \\exp\\left(-\\frac{b_i^2}{2\\tau^2}\\right)\n$$\n\nNow, we combine these to form the posterior, dropping any constant factors that do not depend on $b_i$:\n$$\np(b_i \\mid Y_i, \\dots) \\propto \\exp\\left( -\\frac{1}{2\\sigma^2} \\sum_{j=1}^{n_i} (y_{ij} - \\mu - b_i)^2 - \\frac{b_i^2}{2\\tau^2} \\right)\n$$\nTo identify the form of this distribution, we analyze the expression in the exponent, focusing on terms involving $b_i$. Let the exponent be denoted $Q(b_i)$:\n$$\nQ(b_i) = -\\frac{1}{2\\sigma^2} \\sum_{j=1}^{n_i} ( (y_{ij} - \\mu) - b_i )^2 - \\frac{b_i^2}{2\\tau^2}\n$$\nExpanding the squared term:\n$$\n\\sum_{j=1}^{n_i} ((y_{ij} - \\mu) - b_i)^2 = \\sum_{j=1}^{n_i} ( (y_{ij} - \\mu)^2 - 2b_i(y_{ij}-\\mu) + b_i^2 )\n$$\n$$\n= \\sum_{j=1}^{n_i} (y_{ij} - \\mu)^2 - 2b_i \\sum_{j=1}^{n_i} (y_{ij}-\\mu) + n_i b_i^2\n$$\nThe first term, $\\sum (y_{ij} - \\mu)^2$, does not depend on $b_i$ and can be absorbed into the proportionality constant. Let $\\bar{y}_i = \\frac{1}{n_i}\\sum_{j=1}^{n_i} y_{ij}$. Then $\\sum (y_{ij}-\\mu) = n_i(\\bar{y}_i - \\mu)$.\nSubstituting this back into $Q(b_i)$ and ignoring constant terms, we get:\n$$\nQ(b_i) \\propto -\\frac{1}{2\\sigma^2} (-2b_i n_i(\\bar{y}_i - \\mu) + n_i b_i^2) - \\frac{b_i^2}{2\\tau^2}\n$$\n$$\n= \\frac{n_i(\\bar{y}_i - \\mu)}{\\sigma^2} b_i - \\frac{n_i}{2\\sigma^2} b_i^2 - \\frac{1}{2\\tau^2} b_i^2\n$$\n$$\n= -\\frac{1}{2}\\left( \\left(\\frac{n_i}{\\sigma^2} + \\frac{1}{\\tau^2}\\right)b_i^2 - 2\\frac{n_i(\\bar{y}_i - \\mu)}{\\sigma^2} b_i \\right)\n$$\nThe expression for the posterior PDF is of the form $p(b_i \\mid \\dots) \\propto \\exp(-\\frac{1}{2}(\\cdot))$. The quadratic form in $b_i$ indicates that the posterior distribution is also Gaussian. A general normal PDF for a variable $x$ with mean $\\mu_{\\text{post}}$ and variance $\\sigma^2_{\\text{post}}$ has a kernel proportional to $\\exp(-\\frac{(x-\\mu_{\\text{post}})^2}{2\\sigma^2_{\\text{post}}}) \\propto \\exp(-\\frac{1}{2}(\\frac{1}{\\sigma^2_{\\text{post}}}x^2 - \\frac{2\\mu_{\\text{post}}}{\\sigma^2_{\\text{post}}}x))$.\n\nBy matching coefficients with our expression for $Q(b_i)$, we can identify the parameters of the posterior distribution of $b_i$.\nThe coefficient of $b_i^2$ gives the posterior precision (inverse variance):\n$$\n\\frac{1}{\\sigma^2_{\\text{post}}} = \\frac{1}{\\operatorname{Var}(b_i \\mid Y_i)} = \\frac{n_i}{\\sigma^2} + \\frac{1}{\\tau^2}\n$$\nFrom this, we directly compute the posterior variance as its reciprocal:\n$$\n\\operatorname{Var}(b_i \\mid Y_i) = \\left(\\frac{n_i}{\\sigma^2} + \\frac{1}{\\tau^2}\\right)^{-1} = \\frac{1}{\\frac{n_i\\tau^2 + \\sigma^2}{\\sigma^2\\tau^2}} = \\frac{\\sigma^2\\tau^2}{n_i\\tau^2 + \\sigma^2}\n$$\nThis provides the closed-form expression for the posterior variance as required.\n\nFor completeness and to address the second part of the question, we also derive the posterior mean. Matching the coefficient of the linear term in $b_i$:\n$$\n\\frac{\\mu_{\\text{post}}}{\\sigma^2_{\\text{post}}} = \\frac{n_i(\\bar{y}_i - \\mu)}{\\sigma^2}\n$$\n$$\n\\mathbb{E}[b_i \\mid Y_i] = \\mu_{\\text{post}} = \\sigma^2_{\\text{post}} \\left(\\frac{n_i(\\bar{y}_i - \\mu)}{\\sigma^2}\\right) = \\left(\\frac{\\sigma^2\\tau^2}{n_i\\tau^2 + \\sigma^2}\\right) \\left(\\frac{n_i(\\bar{y}_i - \\mu)}{\\sigma^2}\\right) = \\frac{n_i\\tau^2}{n_i\\tau^2 + \\sigma^2}(\\bar{y}_i - \\mu)\n$$\nThe posterior distribution is therefore $b_i \\mid Y_i \\sim \\mathcal{N}(\\mu_{\\text{post}}, \\sigma^2_{\\text{post}})$.\n\nNow we analyze the effect of increasing the measurement error variance, $\\sigma^2$, on the shrinkage of the posterior mean of $b_i$ toward its prior mean of zero. The posterior mean is:\n$$\n\\mathbb{E}[b_i \\mid Y_i] = \\left(\\frac{n_i\\tau^2}{n_i\\tau^2 + \\sigma^2}\\right)(\\bar{y}_i - \\mu)\n$$\nThe term $(\\bar{y}_i - \\mu)$ is an unbiased estimate of $b_i$ based only on the data for subject $i$. The term $S = \\frac{n_i\\tau^2}{n_i\\tau^2 + \\sigma^2}$ is a shrinkage factor, which is between $0$ and $1$. The posterior mean is a \"shrunken\" version of the data-based estimate, pulled towards the prior mean of $0$.\n\nTo see how increasing $\\sigma^2$ affects this shrinkage, we examine the derivative of $S$ with respect to $\\sigma^2$:\n$$\n\\frac{\\partial S}{\\partial(\\sigma^2)} = \\frac{\\partial}{\\partial(\\sigma^2)}\\left(\\frac{n_i\\tau^2}{n_i\\tau^2 + \\sigma^2}\\right) = -\\frac{n_i\\tau^2}{(n_i\\tau^2 + \\sigma^2)^2}\n$$\nSince $n_i > 0$ and $\\tau^2 > 0$, this derivative is strictly negative. This means that as $\\sigma^2$ increases, the shrinkage factor $S$ decreases. A smaller $S$ implies that the posterior mean $\\mathbb{E}[b_i \\mid Y_i]$ is pulled closer to $0$, which signifies stronger shrinkage.\n\nThis behavior is explained by considering the relative contributions of the data and the prior to the posterior precision. The posterior precision is the sum of the data precision and the prior precision:\n$$\n\\text{Posterior Precision} = \\underbrace{\\frac{n_i}{\\sigma^2}}_{\\text{Data Precision}} + \\underbrace{\\frac{1}{\\tau^2}}_{\\text{Prior Precision}}\n$$\nWhen $\\sigma^2$ increases, the data measurements become noisier and less reliable. This is reflected in the decrease of the data precision term, $n_i/\\sigma^2$. The Bayesian framework automatically accounts for this reduced information content from the data by giving more relative weight to the prior information. The posterior mean can be expressed as a precision-weighted average of the prior mean ($0$) and the data-based estimate ($\\bar{y}_i - \\mu$):\n$$\n\\mathbb{E}[b_i \\mid Y_i] = \\frac{n_i/\\sigma^2}{n_i/\\sigma^2 + 1/\\tau^2}(\\bar{y}_i - \\mu) + \\frac{1/\\tau^2}{n_i/\\sigma^2 + 1/\\tau^2}(0)\n$$\nAs $\\sigma^2$ increases, the weight on the data-based estimate, $\\frac{n_i/\\sigma^2}{n_i/\\sigma^2 + 1/\\tau^2}$, decreases, while the weight on the prior mean increases. Consequently, the posterior estimate of $b_i$ is shrunken more strongly toward the prior mean of $0$.",
            "answer": "$$\\boxed{\\frac{\\sigma^{2}\\tau^{2}}{n_i\\tau^{2} + \\sigma^{2}}}$$"
        },
        {
            "introduction": "Having explored the longitudinal submodel, we now turn to the core of the 'joint' specification: linking the biomarker's trajectory to the risk of an event. This practice focuses on calculating the subject-specific hazard function, which represents the instantaneous risk of the event at any given time . This calculation is central to understanding how a patient's evolving biomarker data dynamically updates their risk profile within the proportional hazards framework.",
            "id": "4968612",
            "problem": "A prospective cohort study in oncology tracks a subject-specific serum biomarker longitudinally and records time-to-progression. Consider a joint model with a shared parameter linking the longitudinal biomarker process to the time-to-event process. The longitudinal submodel yields a subject-specific posterior mean trajectory $\\hat m_i(t)$ at time $t$, where $i$ indexes the subject. The survival submodel adheres to the proportional hazards principle: the subject-specific hazard function $h_i(t)$ is proportional to a baseline hazard $h_0(t)$ with a log-linear predictor that depends on baseline covariates and the contemporaneous biomarker level. Specifically, suppose the log-hazard is additive in a baseline covariate contribution $\\gamma^{\\top} w_i$ and a current-value association $\\alpha \\hat m_i(t)$, where $\\gamma$ is the regression coefficient vector for the baseline covariate vector $w_i$, and $\\alpha$ quantifies the association between the expected biomarker level and the hazard. The baseline hazard $h_0(t)$ is a nonnegative function of $t$ reflecting the underlying event rate for a subject with $\\gamma^{\\top} w_i = 0$ and $\\hat m_i(t) = 0$.\n\nStarting from the definition of the hazard function and the proportional hazards assumption in the Cox Proportional Hazards (PH) model, derive an explicit expression for $h_i(t)$ in terms of $h_0(t)$, $\\gamma^{\\top} w_i$, $\\alpha$, and $\\hat m_i(t)$. Then, for a particular subject at a given time $t$, evaluate $h_i(t)$ numerically when $\\hat m_i(t) = 2.3$, $\\gamma^{\\top} w_i = 0.5$, $\\alpha = 0.8$, and $h_0(t) = 0.01$. Round your final numerical answer to four significant figures. Express the hazard in events per month. Finally, provide a brief numerical interpretation of the magnitude of the resulting hazard in terms of the instantaneous event probability over a small time interval.",
            "solution": "This problem concerns the joint modeling of longitudinal and time-to-event data. The task is to derive an expression for a subject-specific hazard function, evaluate it numerically, and provide an interpretation.\n\n### Derivation of the Hazard Function Expression\n\nThe problem states that the survival submodel adheres to the proportional hazards (PH) principle. In a PH model, the hazard function for subject $i$ at time $t$, denoted $h_i(t)$, is proportional to a baseline hazard function, $h_0(t)$. The factor of proportionality is a function of the subject's covariates. For a time-dependent model, this relationship is expressed as:\n$$\nh_i(t) = h_0(t) \\exp(\\eta_i(t))\n$$\nwhere $\\eta_i(t)$ is the linear predictor for subject $i$ at time $t$.\n\nThe problem specifies that the \"log-hazard is additive.\" This is standard terminology for the structure of the linear predictor in the log-hazard scale. Specifically, taking the natural logarithm of the PH equation gives:\n$$\n\\ln(h_i(t)) = \\ln(h_0(t)) + \\eta_i(t)\n$$\nor\n$$\n\\ln\\left(\\frac{h_i(t)}{h_0(t)}\\right) = \\eta_i(t)\n$$\nThe quantity $\\ln(h_i(t)) - \\ln(h_0(t))$ is the log-hazard ratio, which is equal to the linear predictor.\n\nThe problem defines the components of the linear predictor. It is additive in a baseline covariate contribution, $\\gamma^{\\top} w_i$, and a term for the current value of the biomarker, $\\alpha \\hat m_i(t)$. Therefore, the linear predictor $\\eta_i(t)$ is:\n$$\n\\eta_i(t) = \\gamma^{\\top} w_i + \\alpha \\hat m_i(t)\n$$\nSubstituting this expression for the linear predictor back into the first equation for $h_i(t)$ yields the explicit expression for the subject-specific hazard function:\n$$\nh_i(t) = h_0(t) \\exp\\left(\\gamma^{\\top} w_i + \\alpha \\hat m_i(t)\\right)\n$$\nThis is the derived expression for $h_i(t)$ in terms of the given components.\n\n### Numerical Evaluation\n\nThe next task is to evaluate $h_i(t)$ numerically for a particular subject at a given time $t$, using the provided values:\n-   $\\hat m_i(t) = 2.3$\n-   $\\gamma^{\\top} w_i = 0.5$\n-   $\\alpha = 0.8$\n-   $h_0(t) = 0.01$ events per month.\n\nWe substitute these values into the derived formula:\n$$\nh_i(t) = 0.01 \\times \\exp\\left(0.5 + (0.8)(2.3)\\right)\n$$\nFirst, we calculate the value of the linear predictor in the exponent:\n$$\n0.5 + (0.8)(2.3) = 0.5 + 1.84 = 2.34\n$$\nNow, substitute this back into the expression for the hazard:\n$$\nh_i(t) = 0.01 \\times \\exp(2.34)\n$$\nWe calculate the value of $\\exp(2.34)$:\n$$\n\\exp(2.34) \\approx 10.381606\n$$\nFinally, we compute $h_i(t)$:\n$$\nh_i(t) \\approx 0.01 \\times 10.381606 = 0.10381606\n$$\nThe problem requires rounding the final numerical answer to four significant figures. The first four significant figures are $1$, $0$, $3$, and $8$. The fifth significant figure is $1$, so we round down.\n$$\nh_i(t) \\approx 0.1038\n$$\nThe units are events per month.\n\n### Interpretation of the Hazard\n\nThe hazard function, $h(t)$, represents the instantaneous rate of event occurrence at time $t$, given survival up to time $t$. It is defined as:\n$$\nh(t) = \\lim_{\\Delta t \\to 0} \\frac{P(t \\le T  t + \\Delta t \\mid T \\ge t)}{\\Delta t}\n$$\nwhere $T$ is the time-to-event random variable. For a very small interval of time $\\Delta t$, the probability of the event occurring within that interval, conditional on survival to its start, can be approximated as:\n$$\nP(t \\le T  t + \\Delta t \\mid T \\ge t) \\approx h(t) \\Delta t\n$$\nIn our case, the calculated hazard is $h_i(t) \\approx 0.1038$ events per month. This means that for a subject who is progression-free at time $t$ and has the specified covariate and biomarker profile ($\\gamma^{\\top} w_i = 0.5$ and $\\hat m_i(t) = 2.3$), the instantaneous risk of progression is $0.1038$ per month. To make this more concrete, the approximate probability of this subject experiencing disease progression during the next small interval of time, $\\Delta t$, is $0.1038 \\times \\Delta t$, where $\\Delta t$ is measured in months. For example, over the course of one day (approximately $\\frac{1}{30}$ of a month), the probability of progression is about $0.1038 \\times \\frac{1}{30} \\approx 0.00346$, or approximately $0.35\\%$.",
            "answer": "$$\n\\boxed{0.1038}\n$$"
        },
        {
            "introduction": "One of the most powerful applications of joint models is in making dynamic predictions for individual subjects, a cornerstone of personalized medicine. This advanced exercise synthesizes the concepts from the longitudinal and survival submodels to forecast a patient's future survival probability based on their biomarker history up to the present moment . Furthermore, it introduces the critical step of quantifying the uncertainty around this prediction, reflecting the statistical confidence in our forecast and providing a more complete picture for clinical interpretation.",
            "id": "4968550",
            "problem": "A clinical study monitors a continuously observed biomarker through a Random Intercept and Random Slope longitudinal model and relates the instantaneous risk of a primary event to the current underlying biomarker level via a proportional hazards structure. Consider subject $i$ with history up to time $t^{\\star} = 2$ (years). The longitudinal submodel specifies the subject-specific latent biomarker trajectory as $m_i(t) = \\beta_0 + \\beta_1 t + b_{0i} + b_{1i} t$, where $b_{0i}$ and $b_{1i}$ are the random intercept and random slope, respectively. The time-to-event submodel specifies the hazard as $h_i(t \\mid b_i) = \\lambda_0 \\exp\\{\\alpha \\, m_i(t)\\}$, where $\\lambda_0  0$ is a constant baseline hazard and $\\alpha$ is a biomarker–event association parameter. Assume no additional baseline covariates in the hazard beyond $m_i(t)$.\n\nSuppose a joint model has been fit and yields the Empirical Bayes (EB) estimate $\\hat b_i = (\\hat b_{0i}, \\hat b_{1i})^{\\top}$ and a normal approximation to the posterior of $b_i$ given the subject’s data up to $t^{\\star}$: $b_i \\mid \\text{data up to } t^{\\star} \\approx \\mathcal{N}_2(\\hat b_i, V_i)$, where $V_i$ is the $2 \\times 2$ covariance matrix. For this subject, use the following parameter values estimated from the joint model fit:\n- $\\lambda_0 = 0.05$ (per year), $\\alpha = 0.4$, $\\beta_0 = 1.0$, $\\beta_1 = 0.3$,\n- $\\hat b_i = (\\hat b_{0i}, \\hat b_{1i})^{\\top} = (0.2, -0.05)^{\\top}$,\n- $V_i = \\begin{pmatrix} 0.04  0.006 \\\\ 0.006  0.01 \\end{pmatrix}$.\n\nStarting from the fundamental relationship between hazard and survival, $S_i(t) = \\exp\\!\\left(-\\int_0^t h_i(s \\mid b_i)\\, ds\\right)$, and conditioning on the subject’s information at $t^{\\star}$, derive from first principles the plug-in one-year dynamic survival prediction $S_i(1 \\mid \\hat b_i)$ for the window $[t^{\\star}, t^{\\star}+1]$ using the EB estimate $\\hat b_i$, and then quantify the uncertainty in $S_i(1 \\mid \\hat b_i)$ due solely to the estimation of $b_i$ via a first-order Delta method approximation based on $V_i$. Explicitly compute:\n1. The numerical value of $S_i(1 \\mid \\hat b_i)$.\n2. The numerical value of the first-order Delta method standard error for $S_i(1 \\mid \\hat b_i)$ induced by the uncertainty in $b_i$.\n\nExpress both the survival prediction and its standard error as unitless quantities. Round each to four significant figures. Your final answer must be the ordered pair of these two numbers.",
            "solution": "The problem requires the computation of a dynamic survival prediction and its associated standard error within the framework of a joint model for longitudinal and time-to-event data. The solution proceeds in two parts: first, the calculation of the plug-in survival estimate, and second, the quantification of its uncertainty using the first-order Delta method.\n\nThe problem notation $S_i(1 \\mid \\hat b_i)$ for the \"one-year dynamic survival prediction for the window $[t^{\\star}, t^{\\star}+1]$\" refers to the conditional probability of surviving past time $t^{\\star}+1$, given survival up to time $t^{\\star}$, evaluated using the Empirical Bayes estimate $\\hat b_i$. This conditional survival probability, which we denote as $S_{dyn}(1 \\mid b_i)$, is formally given by $P(T_i  t^{\\star}+1 \\mid T_i  t^{\\star}, b_i)$. Based on the fundamental relationship between the hazard function $h_i(t)$ and the survival function $S_i(t) = \\exp(-\\int_0^t h_i(s) ds)$, this conditional probability is:\n$$\nS_{dyn}(1 \\mid b_i) = \\frac{S_i(t^{\\star}+1 \\mid b_i)}{S_i(t^{\\star} \\mid b_i)} = \\frac{\\exp\\left(-\\int_0^{t^{\\star}+1} h_i(s \\mid b_i) \\, ds\\right)}{\\exp\\left(-\\int_0^{t^{\\star}} h_i(s \\mid b_i) \\, ds\\right)} = \\exp\\left(-\\int_{t^{\\star}}^{t^{\\star}+1} h_i(s \\mid b_i) \\, ds\\right)\n$$\nThe problem asks for the \"plug-in\" estimate, which means we substitute the random effects vector $b_i = (b_{0i}, b_{1i})^{\\top}$ with its estimate $\\hat b_i = (\\hat b_{0i}, \\hat b_{1i})^{\\top}$.\n\nThe subject-specific longitudinal trajectory, evaluated at the estimate $\\hat b_i$, is:\n$$\nm_i(t \\mid \\hat b_i) = \\beta_0 + \\beta_1 t + \\hat b_{0i} + \\hat b_{1i} t = (\\beta_0 + \\hat b_{0i}) + (\\beta_1 + \\hat b_{1i}) t\n$$\nLet's define the fixed and random components for subject $i$ as $B_{0i} = \\beta_0 + \\hat b_{0i}$ and $B_{1i} = \\beta_1 + \\hat b_{1i}$.\nThe hazard function is then $h_i(t \\mid \\hat b_i) = \\lambda_0 \\exp\\{\\alpha \\, m_i(t \\mid \\hat b_i)\\} = \\lambda_0 \\exp\\{\\alpha(B_{0i} + B_{1i} t)\\}$.\n\nFirst, we compute the cumulative hazard over the interval $[t^{\\star}, t^{\\star}+1]$:\n$$\n\\Lambda_i(t^{\\star}, t^{\\star}+1 \\mid \\hat b_i) = \\int_{t^{\\star}}^{t^{\\star}+1} h_i(s \\mid \\hat b_i) \\, ds = \\int_{t^{\\star}}^{t^{\\star}+1} \\lambda_0 \\exp(\\alpha B_{0i} + \\alpha B_{1i} s) \\, ds\n$$\n$$\n= \\lambda_0 \\exp(\\alpha B_{0i}) \\int_{t^{\\star}}^{t^{\\star}+1} \\exp(\\alpha B_{1i} s) \\, ds\n$$\nAssuming $B_{1i} \\neq 0$, the integral evaluates to:\n$$\n\\Lambda_i(t^{\\star}, t^{\\star}+1 \\mid \\hat b_i) = \\lambda_0 \\exp(\\alpha B_{0i}) \\left[ \\frac{\\exp(\\alpha B_{1i} s)}{\\alpha B_{1i}} \\right]_{s=t^{\\star}}^{t^{\\star}+1} = \\frac{\\lambda_0 \\exp(\\alpha B_{0i})}{\\alpha B_{1i}} \\left( \\exp(\\alpha B_{1i}(t^{\\star}+1)) - \\exp(\\alpha B_{1i} t^{\\star}) \\right)\n$$\nThe plug-in dynamic survival prediction is $S_{dyn}(1 \\mid \\hat b_i) = \\exp(-\\Lambda_i(t^{\\star}, t^{\\star}+1 \\mid \\hat b_i))$.\n\nUsing the given values:\n$t^{\\star} = 2$, $\\lambda_0 = 0.05$, $\\alpha = 0.4$, $\\beta_0 = 1.0$, $\\beta_1 = 0.3$, $\\hat b_{0i} = 0.2$, $\\hat b_{1i} = -0.05$.\n$B_{0i} = 1.0 + 0.2 = 1.2$\n$B_{1i} = 0.3 + (-0.05) = 0.25$\nSince $B_{1i} = 0.25 \\neq 0$, we can use the derived formula.\n$$\n\\Lambda_i(2, 3 \\mid \\hat b_i) = \\frac{0.05 \\exp(0.4 \\times 1.2)}{0.4 \\times 0.25} \\left( \\exp(0.4 \\times 0.25 \\times 3) - \\exp(0.4 \\times 0.25 \\times 2) \\right)\n$$\n$$\n= \\frac{0.05 \\exp(0.48)}{0.1} \\left( \\exp(0.3) - \\exp(0.2) \\right) = 0.5 \\exp(0.48) \\left( \\exp(0.3) - \\exp(0.2) \\right)\n$$\nNumerically, this is:\n$$\n\\Lambda_i(2, 3 \\mid \\hat b_i) \\approx 0.5 \\times 1.616074 \\times (1.349859 - 1.221403) \\approx 0.5 \\times 1.616074 \\times 0.128456 \\approx 0.10380\n$$\nThe survival prediction is:\n$$\nS_{dyn}(1 \\mid \\hat b_i) = \\exp(-0.10380) \\approx 0.90141\n$$\nRounding to four significant figures, the survival prediction is $0.9014$.\n\nNext, we compute the standard error using the first-order Delta method. Let $g(b_i) = S_{dyn}(1 \\mid b_i)$. The variance of $g(b_i)$ is approximated by:\n$$\n\\text{Var}(g(b_i)) \\approx \\left[ \\nabla_b g(b_i) \\right]^{\\top} V_i \\left[ \\nabla_b g(b_i) \\right] \\Big|_{b_i=\\hat b_i}\n$$\nwhere $\\nabla_b g(b_i)$ is the gradient of $g(b_i)$ with respect to $b_i = (b_{0i}, b_{1i})^{\\top}$.\nLet $\\Lambda(b_i) = \\int_{t^{\\star}}^{t^{\\star}+1} h_i(s \\mid b_i) \\, ds$. Then $g(b_i) = \\exp(-\\Lambda(b_i))$.\nUsing the chain rule, the gradient is:\n$$\n\\nabla_b g(b_i) = -\\exp(-\\Lambda(b_i)) \\nabla_b \\Lambda(b_i) = -g(b_i) \\nabla_b \\Lambda(b_i)\n$$\nWe need to compute the gradient of the cumulative hazard, $\\nabla_b \\Lambda(b_i) = (\\frac{\\partial \\Lambda}{\\partial b_{0i}}, \\frac{\\partial \\Lambda}{\\partial b_{1i}})^{\\top}$.\nThe first partial derivative is:\n$$\n\\frac{\\partial \\Lambda}{\\partial b_{0i}} = \\int_{t^{\\star}}^{t^{\\star}+1} \\frac{\\partial}{\\partial b_{0i}} h_i(s \\mid b_i) \\, ds = \\int_{t^{\\star}}^{t^{\\star}+1} h_i(s \\mid b_i) \\cdot \\alpha \\frac{\\partial m_i(s \\mid b_i)}{\\partial b_{0i}} \\, ds = \\int_{t^{\\star}}^{t^{\\star}+1} \\alpha h_i(s \\mid b_i) \\, ds = \\alpha \\Lambda(b_i)\n$$\nThe second partial derivative is:\n$$\n\\frac{\\partial \\Lambda}{\\partial b_{1i}} = \\int_{t^{\\star}}^{t^{\\star}+1} \\frac{\\partial}{\\partial b_{1i}} h_i(s \\mid b_i) \\, ds = \\int_{t^{\\star}}^{t^{\\star}+1} h_i(s \\mid b_i) \\cdot \\alpha \\frac{\\partial m_i(s \\mid b_i)}{\\partial b_{1i}} \\, ds = \\int_{t^{\\star}}^{t^{\\star}+1} \\alpha s h_i(s \\mid b_i) \\, ds\n$$\nWe evaluate these derivatives at $b_i = \\hat b_i$.\n$\\frac{\\partial \\Lambda}{\\partial b_{0i}} \\Big|_{\\hat b_i} = \\alpha \\Lambda(\\hat b_i) \\approx 0.4 \\times 0.10380 = 0.041520$.\nFor the second derivative:\n$$\n\\frac{\\partial \\Lambda}{\\partial b_{1i}} \\Big|_{\\hat b_i} = \\int_{2}^{3} \\alpha s \\, \\lambda_0 \\exp(\\alpha(B_{0i} + B_{1i} s)) \\, ds = \\alpha \\lambda_0 \\exp(\\alpha B_{0i}) \\int_{2}^{3} s \\exp(\\alpha B_{1i} s) \\, ds\n$$\nThe integral $\\int s \\exp(as) \\, ds$ is solved using integration by parts: $\\left(\\frac{s}{a} - \\frac{1}{a^2}\\right)\\exp(as)$.\nWith $a = \\alpha B_{1i} = 0.4 \\times 0.25 = 0.1$:\n$$\n\\int_{2}^{3} s \\exp(0.1 s) \\, ds = \\left[ \\left(\\frac{s}{0.1} - \\frac{1}{0.1^2}\\right) \\exp(0.1s) \\right]_{2}^{3} = \\left[ (10s - 100) \\exp(0.1s) \\right]_{2}^{3}\n$$\n$$\n= (10 \\times 3 - 100)\\exp(0.3) - (10 \\times 2 - 100)\\exp(0.2) = 80\\exp(0.2) - 70\\exp(0.3)\n$$\nNumerically, this is $80 \\times 1.221403 - 70 \\times 1.349859 \\approx 97.71224 - 94.49013 = 3.22211$.\nThen,\n$$\n\\frac{\\partial \\Lambda}{\\partial b_{1i}} \\Big|_{\\hat b_i} = (0.4)(0.05) \\exp(0.48) \\times 3.22211 = 0.02 \\exp(0.48) \\times 3.22211 \\approx 0.02 \\times 1.616074 \\times 3.22211 \\approx 0.10411\n$$\nSo, $\\nabla_b \\Lambda(\\hat b_i) \\approx (0.041520, 0.10411)^{\\top}$.\nThe gradient of $g$ is:\n$$\n\\nabla_b g(\\hat b_i) \\approx -0.90141 \\begin{pmatrix} 0.041520 \\\\ 0.10411 \\end{pmatrix} = \\begin{pmatrix} -0.037422 \\\\ -0.093845 \\end{pmatrix}\n$$\nNow we compute the variance using $V_i = \\begin{pmatrix} 0.04  0.006 \\\\ 0.006  0.01 \\end{pmatrix}$:\n$$\n\\text{Var}(g(\\hat b_i)) \\approx \\begin{pmatrix} -0.037422  -0.093845 \\end{pmatrix} \\begin{pmatrix} 0.04  0.006 \\\\ 0.006  0.01 \\end{pmatrix} \\begin{pmatrix} -0.037422 \\\\ -0.093845 \\end{pmatrix}\n$$\nFirst, the row-vector times matrix product:\n$$\n\\begin{pmatrix} -0.037422  -0.093845 \\end{pmatrix} \\begin{pmatrix} 0.04  0.006 \\\\ 0.006  0.01 \\end{pmatrix}\n$$\n$$\n= \\begin{pmatrix} (-0.037422)(0.04) + (-0.093845)(0.006)  (-0.037422)(0.006) + (-0.093845)(0.01) \\end{pmatrix}\n$$\n$$\n= \\begin{pmatrix} -0.00149688 - 0.00056307  -0.00022453 - 0.00093845 \\end{pmatrix} = \\begin{pmatrix} -0.00205995  -0.00116298 \\end{pmatrix}\n$$\nFinally, multiplying by the column vector:\n$$\n\\text{Var}(g(\\hat b_i)) \\approx \\begin{pmatrix} -0.00205995  -0.00116298 \\end{pmatrix} \\begin{pmatrix} -0.037422 \\\\ -0.093845 \\end{pmatrix}\n$$\n$$\n= (-0.00205995)(-0.037422) + (-0.00116298)(-0.093845) \\approx 0.00007709 + 0.00010915 \\approx 0.00018624\n$$\nThe standard error is the square root of the variance:\n$$\n\\text{SE}(g(\\hat b_i)) = \\sqrt{\\text{Var}(g(\\hat b_i))} \\approx \\sqrt{0.00018624} \\approx 0.013647\n$$\nRounding to four significant figures, the standard error is $0.01365$.\n\nThe required values are the survival prediction and its standard error.\n1. Survival prediction: $0.9014$\n2. Standard error: $0.01365$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 0.9014  0.01365 \\end{pmatrix}}\n$$"
        }
    ]
}