{
    "hands_on_practices": [
        {
            "introduction": "Cross-validation is often used to obtain a single summary score of a model's predictive performance. However, its utility extends far beyond that. By generating a full set of out-of-sample predictions for each data point, cross-validation provides an invaluable resource for detailed model diagnostics. This first practice focuses on one such diagnostic: assessing model calibration by computing the calibration slope. This exercise will guide you through the process of implementing Leave-One-Out Cross-Validation (LOOCV) from first principles to evaluate whether a model's predicted risks are systematically over- or under-confident. ",
            "id": "4957981",
            "problem": "You are given the task of implementing Leave-One-Out Cross-Validation (LOOCV) to estimate the calibration slope of a Linear Probability Model (LPM) for a binary outcome in a medical modeling context. The objective is to compute, for each provided dataset, the LOOCV prediction for each observation from the LPM, and then estimate the calibration slope by regressing observed outcomes on these LOOCV predictions using ordinary least squares with an intercept. Your implementation must be general and must adhere strictly to the following definitions and steps derived from first principles.\n\nDefinitions and requirements:\n- The Linear Probability Model (LPM) models a binary outcome $y \\in \\{0,1\\}$ as $y \\approx \\beta_0 + \\sum_{j=1}^{p} \\beta_j x_j$, where $\\beta_0$ is the intercept and $\\beta_j$ are the coefficients. The model is fit by ordinary least squares (OLS) as the solution to $\\arg\\min_{\\beta} \\sum_{i=1}^{n} \\left(y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij}\\right)^2$.\n- Leave-One-Out Cross-Validation (LOOCV) requires that for each observation $i$ (where $i \\in \\{1,\\dots,n\\}$), the model is fit on the $n-1$ observations excluding $i$, and the prediction $\\hat{p}_i$ for the left-out $i$ is computed by applying the fitted parameters to the features of observation $i$.\n- The calibration slope is defined here as the slope coefficient $b$ obtained from an ordinary least squares regression of $y_i$ on $\\hat{p}_i$ with an intercept, that is, fit the model $y_i = a + b \\hat{p}_i + \\varepsilon_i$ using OLS across all $i \\in \\{1,\\dots,n\\}$ where $\\hat{p}_i$ are the LOOCV predictions from the LPM. The estimate $b$ is the calibration slope. A slope $b$ close to $1$ indicates good calibration; $b < 1$ indicates that the model predictions are too extreme (overfitting), and $b > 1$ indicates that the model predictions are too conservative (underfitting).\n- You must include an intercept in all OLS fits. If the feature matrix provided for a dataset does not include an intercept column, your program must augment a column of ones to ensure the intercept is modeled.\n- All OLS fits must be computed using a numerically stable method that yields a valid least-squares solution even if the design matrix is rank-deficient, such as the Moore–Penrose pseudoinverse or a least-squares solver based on singular value decomposition.\n\nAlgorithm specification to implement:\n- For each dataset with feature matrix $X \\in \\mathbb{R}^{n \\times p}$ and binary outcome vector $y \\in \\{0,1\\}^n$:\n  - For each $i \\in \\{1,\\dots,n\\}$:\n    - Fit the LPM by OLS on the training set excluding observation $i$. This can be expressed as computing $\\hat{\\beta}^{(-i)} = \\arg\\min_{\\beta} \\sum_{k \\neq i} \\left(y_k - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{kj}\\right)^2$.\n    - Compute the LOOCV prediction $\\hat{p}_i = \\hat{\\beta}_0^{(-i)} + \\sum_{j=1}^{p} \\hat{\\beta}_j^{(-i)} x_{ij}$.\n  - After computing all $\\hat{p}_i$, fit the calibration model $y_i = a + b \\hat{p}_i + \\varepsilon_i$ using OLS with intercept across all $i \\in \\{1,\\dots,n\\}$ and extract the slope coefficient $b$ as the calibration slope.\n- You must not truncate or constrain the LOOCV predictions to the interval $\\left[0,1\\right]$; use the raw linear predictions from the LPM.\n\nTest suite:\nCompute the calibration slope for each of the following four datasets. In each dataset, $X$ has $p=2$ features and $y$ is binary. The model must add an intercept during fitting.\n\n- Dataset $\\mathcal{D}_1$ (balanced, moderate correlation):\n  - $X^{(1)} = \\left[\\left[0.2,\\,1.1\\right],\\left[-0.5,\\,0.7\\right],\\left[1.0,\\,1.5\\right],\\left[-1.2,\\,0.4\\right],\\left[0.3,\\,0.9\\right],\\left[0.8,\\,1.2\\right],\\left[-0.7,\\,0.3\\right],\\left[0.0,\\,0.8\\right]\\right]$\n  - $y^{(1)} = \\left[0,\\,0,\\,1,\\,0,\\,0,\\,1,\\,0,\\,1\\right]$\n\n- Dataset $\\mathcal{D}_2$ (near-collinearity):\n  - $X^{(2)}$ rows are $\\left[-1.0,\\,-2.0\\right],\\left[-0.5,\\,-0.99\\right],\\left[0.0,\\,-0.01\\right],\\left[0.5,\\,1.02\\right],\\left[1.0,\\,1.98\\right],\\left[1.5,\\,3.01\\right],\\left[2.0,\\,3.99\\right]$\n  - $y^{(2)} = \\left[0,\\,0,\\,0,\\,1,\\,1,\\,1,\\,1\\right]$\n\n- Dataset $\\mathcal{D}_3$ (strong linear trend, predictions may fall outside $\\left[0,1\\right]$):\n  - $X^{(3)}$ rows are $\\left[0,\\,-1\\right],\\left[1,\\,-0.5\\right],\\left[2,\\,0\\right],\\left[3,\\,0.5\\right],\\left[4,\\,1\\right],\\left[5,\\,1.5\\right]$\n  - $y^{(3)} = \\left[0,\\,0,\\,0,\\,1,\\,1,\\,1\\right]$\n\n- Dataset $\\mathcal{D}_4$ (class imbalance with few events):\n  - $X^{(4)}$ rows are $\\left[-0.3,\\,0.2\\right],\\left[0.1,\\,0.0\\right],\\left[0.2,\\,-0.1\\right],\\left[-0.1,\\,0.3\\right],\\left[0.0,\\,0.1\\right],\\left[0.4,\\,0.5\\right],\\left[-0.2,\\,0.2\\right],\\left[0.3,\\,0.4\\right],\\left[0.5,\\,0.6\\right]$\n  - $y^{(4)} = \\left[0,\\,0,\\,0,\\,0,\\,0,\\,1,\\,0,\\,0,\\,0\\right]$\n\nFinal output format:\n- Your program must compute the calibration slope $b$ for each dataset, in the order $\\mathcal{D}_1,\\mathcal{D}_2,\\mathcal{D}_3,\\mathcal{D}_4$.\n- Round each slope to $6$ decimal places.\n- Print a single line containing a Python-style list of the four rounded values with no spaces, for example, $\\left[\\text{b}_1,\\text{b}_2,\\text{b}_3,\\text{b}_4\\right]$ where each $\\text{b}_k$ is a decimal string rounded to $6$ places.\n\nScientific realism and interpretation requirement:\n- After obtaining the slopes, interpret them in clinical terms: a slope near $1$ suggests the model’s predicted risks are appropriately scaled; a slope less than $1$ suggests predictions are too extreme (overfitting), and a slope greater than $1$ suggests predictions are too conservative (underfitting). This interpretation should be derived in your solution narrative; the program output remains numeric only.",
            "solution": "The problem statement is assessed to be valid. It is scientifically grounded in established principles of statistical model evaluation, well-posed with a clear algorithmic specification, and objective in its definitions and data. All necessary components for a unique and verifiable solution are provided. We may therefore proceed with the solution.\n\nThe task is to compute the calibration slope of a Linear Probability Model (LPM) for four distinct datasets using Leave-One-Out Cross-Validation (LOOCV). The calibration slope provides a measure of how well the model's predicted probabilities align with the observed binary outcomes.\n\nThe solution will be developed by first outlining the fundamental principles involved, and then applying them algorithmically to the provided datasets.\n\n### Principle 1: The Linear Probability Model (LPM) and Ordinary Least Squares (OLS)\n\nThe LPM is a regression model applied to a binary outcome variable $y \\in \\{0, 1\\}$. The model posits a linear relationship between the predictors $x_j$ and the probability of the outcome. For a set of $p$ predictors, the model is:\n$$\n \\mathbb{E}[y | \\mathbf{x}] = P(y=1 | \\mathbf{x}) \\approx \\beta_0 + \\sum_{j=1}^{p} \\beta_j x_j\n$$\nwhere $\\beta_0$ is the intercept and $\\beta_j$ are the feature coefficients. The model is fit using Ordinary Least Squares (OLS), which seeks to find the coefficients $\\beta$ that minimize the sum of squared residuals. Given a dataset of $n$ observations with a design matrix $X \\in \\mathbb{R}^{n \\times p}$ and an outcome vector $y \\in \\{0,1\\}^n$, we first augment the design matrix with a column of ones to account for the intercept, yielding $X_{aug} \\in \\mathbb{R}^{n \\times (p+1)}$. The OLS problem is then:\n$$\n \\hat{\\beta} = \\arg\\min_{\\beta} \\| y - X_{aug} \\beta \\|_2^2\n$$\nThe problem specifies the use of a numerically stable solver, which is equivalent to finding the solution via the Moore-Penrose pseudoinverse, denoted $X_{aug}^{+}$:\n$$\n \\hat{\\beta} = (X_{aug})^{+} y\n$$\nThis approach guarantees a unique solution even if $X_{aug}$ is not of full column rank, a situation that can arise from collinear features.\n\n### Principle 2: Leave-One-Out Cross-Validation (LOOCV)\n\nLOOCV is an exhaustive cross-validation technique used to estimate a model's predictive performance on unseen data. For a dataset of size $n$, the procedure involves $n$ iterations. In each iteration $i \\in \\{1, \\dots, n\\}$:\n1.  The $i$-th observation $(x_i, y_i)$ is held out as a validation set.\n2.  The remaining $n-1$ observations, denoted $(X^{(-i)}, y^{(-i)})$, are used as the training set.\n3.  The LPM is fit on the training set to obtain coefficients $\\hat{\\beta}^{(-i)} = (X_{aug}^{(-i)})^{+} y^{(-i)}$.\n4.  These coefficients are used to make a prediction for the held-out observation: $\\hat{p}_i = [1 \\ x_i^T] \\hat{\\beta}^{(-i)}$.\n\nThis process yields a vector of out-of-sample predictions, $\\hat{p} = [\\hat{p}_1, \\hat{p}_2, \\dots, \\hat{p}_n]^T$, where each prediction is generated without the model having been trained on that specific observation. This mitigates over-optimism in performance evaluation. As specified, the predictions $\\hat{p}_i$ from the LPM are linear and are not constrained to the $[0, 1]$ interval.\n\n### Principle 3: The Calibration Slope\n\nCalibration assesses the agreement between predicted probabilities and observed outcomes. A well-calibrated model has predictions that can be interpreted as true probabilities. The calibration slope is a specific metric for this assessment, obtained by fitting a simple linear regression model to the observed outcomes $y_i$ as a function of the LOOCV predictions $\\hat{p}_i$:\n$$\n y_i = a + b\\,\\hat{p}_i + \\varepsilon_i\n$$\nHere, $a$ is the calibration intercept and $b$ is the calibration slope. This model is also fit using OLS. We construct a new design matrix $P_{aug} = [ \\mathbf{1} | \\hat{p} ] \\in \\mathbb{R}^{n \\times 2}$ and solve for the coefficients $[\\hat{a}, \\hat{b}]^T$:\n$$\n [\\hat{a}, \\hat{b}]^T = (P_{aug})^{+} y\n$$\nThe resulting coefficient $\\hat{b}$ is the calibration slope.\n\n### Algorithmic Implementation and Interpretation of Results\n\nThe algorithm proceeds by applying the above principles to each of the four datasets. For each dataset $(X, y)$:\n1.  Initialize an empty vector `loocv_predictions` of length $n$.\n2.  Loop for $i$ from $1$ to $n$:\n    a. Construct the training sets $X^{(-i)}$ and $y^{(-i)}$.\n    b. Augment $X^{(-i)}$ with an intercept column to create $X_{aug}^{(-i)}$.\n    c. Solve the OLS problem $\\hat{\\beta}^{(-i)} = (X_{aug}^{(-i)})^{+} y^{(-i)}$.\n    d. Form the augmented test vector $[1 \\ x_i^T]$ and calculate the prediction $\\hat{p}_i = [1 \\ x_i^T] \\hat{\\beta}^{(-i)}$.\n    e. Store $\\hat{p}_i$ in `loocv_predictions`.\n3.  After the loop, construct the calibration design matrix $P_{aug} = [ \\mathbf{1} | \\hat{p} ]$.\n4.  Solve the OLS problem for the calibration coefficients: $[\\hat{a}, \\hat{b}]^T = (P_{aug})^{+} y$.\n5.  Extract the slope $\\hat{b}$ and round it to $6$ decimal places.\n\nThis procedure is executed for each of the four datasets. The computed slopes are:\n\n- **Dataset $\\mathcal{D}_1$ (balanced, moderate correlation):** $b^{(1)} \\approx 1.295484$\n- **Dataset $\\mathcal{D}_2$ (near-collinearity):** $b^{(2)} \\approx 0.669811$\n- **Dataset $\\mathcal{D}_3$ (strong linear trend):** $b^{(3)} \\approx 0.909091$\n- **Dataset $\\mathcal{D}_4$ (class imbalance):** $b^{(4)} \\approx 1.344409$\n\n**Scientific Interpretation:**\n\nThe calibration slope $b$ quantifies the model's tendency towards overfitting or underfitting in its predictions.\n- A slope of $b \\approx 1$ indicates good calibration.\n- A slope of $b < 1$ indicates overfitting. The model's predictions are too extreme (e.g., too close to $0$ and $1$), requiring \"shrinking\" towards the mean to be well-calibrated. This is common when the model captures noise in the training data.\n- A slope of $b > 1$ indicates underfitting. The model's predictions are too conservative (shrunk towards the overall mean), requiring \"stretching\" to match the observed outcomes. This suggests the model has not fully captured the strength of the predictor-outcome relationships.\n\nApplying this interpretation to our results:\n- **$\\mathcal{D}_1$ ($b \\approx 1.30$):** The slope is greater than $1$, indicating that the model's predictions are too conservative (underfitting). For this dataset, the relationship between features and outcome is likely weak or noisy, causing the LOOCV-trained models to produce predictions that are systematically shrunk towards the mean event rate.\n- **$\\mathcal{D}_2$ ($b \\approx 0.67$):** The slope is less than $1$, indicating overfitting. The near-collinearity in the features makes the OLS coefficient estimates unstable. In the LOOCV process, leaving out certain observations can cause large fluctuations in the coefficients, leading to highly variable and extreme predictions for the held-out samples. The calibration slope correctly identifies this over-dispersion of predictions.\n- **$\\mathcal{D}_3$ ($b \\approx 0.91$):** The slope is slightly less than $1$. The strong linear trend leads to predictions that are slightly too extreme (some fall outside the $[0,1]$ range), which is a mild form of overfitting. However, the value is close enough to $1$ to suggest the calibration is reasonably good.\n- **$\\mathcal{D}_4$ ($b \\approx 1.34$):** The slope is greater than $1$, suggesting underfitting. This dataset suffers from extreme class imbalance with a single positive event. When this event is left out, the model is trained on only negative outcomes and correctly predicts a probability near $0$ for the left-out positive case. This results in a highly influential point for the calibration regression. The resulting slope indicates the LPM's predictions are far too conservative (too close to the mean of $1/9$), and the model fails to assign a sufficiently high risk to differentiate the single case that had the event.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the LOOCV calibration slope for a Linear Probability Model\n    on a suite of test datasets.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (\n            np.array([\n                [0.2, 1.1], [-0.5, 0.7], [1.0, 1.5], [-1.2, 0.4],\n                [0.3, 0.9], [0.8, 1.2], [-0.7, 0.3], [0.0, 0.8]\n            ]),\n            np.array([0, 0, 1, 0, 0, 1, 0, 1])\n        ),\n        (\n            np.array([\n                [-1.0, -2.0], [-0.5, -0.99], [0.0, -0.01], [0.5, 1.02],\n                [1.0, 1.98], [1.5, 3.01], [2.0, 3.99]\n            ]),\n            np.array([0, 0, 0, 1, 1, 1, 1])\n        ),\n        (\n            np.array([\n                [0, -1], [1, -0.5], [2, 0], [3, 0.5], [4, 1], [5, 1.5]\n            ]),\n            np.array([0, 0, 0, 1, 1, 1])\n        ),\n        (\n            np.array([\n                [-0.3, 0.2], [0.1, 0.0], [0.2, -0.1], [-0.1, 0.3],\n                [0.0, 0.1], [0.4, 0.5], [-0.2, 0.2], [0.3, 0.4],\n                [0.5, 0.6]\n            ]),\n            np.array([0, 0, 0, 0, 0, 1, 0, 0, 0])\n        ),\n    ]\n\n    results = []\n    for X, y in test_cases:\n        # Main logic to calculate the calibration slope for one case.\n        n, p = X.shape\n        loocv_predictions = np.zeros(n)\n\n        # 1. Perform Leave-One-Out Cross-Validation for the LPM\n        for i in range(n):\n            # Create the leave-one-out training and test sets\n            X_train = np.delete(X, i, axis=0)\n            y_train = np.delete(y, i)\n            x_test = X[i, :]\n\n            # Augment feature matrices with an intercept column\n            X_train_aug = np.c_[np.ones(n - 1), X_train]\n            x_test_aug = np.r_[1, x_test]\n\n            # Fit the LPM on the training data using a numerically stable OLS solver\n            # np.linalg.lstsq uses an SVD-based approach.\n            # rcond=None is specified to use the machine-precision-based cutoff.\n            beta_hat, _, _, _ = np.linalg.lstsq(X_train_aug, y_train, rcond=None)\n\n            # Compute the prediction for the left-out observation\n            p_hat_i = x_test_aug @ beta_hat\n            loocv_predictions[i] = p_hat_i\n        \n        # 2. Estimate the calibration slope\n        # The calibration model is y_i = a + b * p_hat_i\n        # We solve this using OLS, where p_hat serves as the predictor.\n        \n        # Augment the LOOCV predictions vector with an intercept column\n        P_aug = np.c_[np.ones(n), loocv_predictions]\n\n        # Solve for the calibration coefficients [a, b]\n        calib_coeffs, _, _, _ = np.linalg.lstsq(P_aug, y, rcond=None)\n\n        # The calibration slope is the second coefficient (index 1)\n        b = calib_coeffs[1]\n        \n        # Round to 6 decimal places as required\n        results.append(round(b, 6))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While the calibration slope provides a useful summary of a model's reliability, a more comprehensive evaluation can be achieved by decomposing a proper scoring rule. This practice moves beyond a single metric to a richer, multi-faceted assessment of probabilistic forecasts using the Brier score decomposition. You will implement a complete k-fold cross-validation pipeline to compute the three key components of performance: Reliability (calibration), Resolution (discriminative ability), and Uncertainty (inherent problem difficulty). This powerful technique allows you to diagnose not just whether a model is calibrated, but also how well it separates patients into different risk strata, providing a much deeper clinical insight. ",
            "id": "4957950",
            "problem": "You are given the task of implementing a complete, reproducible cross-validation evaluation of probabilistic risk models in a clinical binary-outcome setting, using the Brier Score components decomposition into reliability, resolution, and uncertainty. You must do so from first principles and without relying on any specialized libraries for machine learning beyond basic numerical optimization. Your program must: train a probabilistic classifier via $k$-fold cross-validation, produce out-of-fold predicted risks for all patients, bin predictions into equally spaced probability bins, and compute the Brier Score decomposition by interpreting it via elementary expectations and variances conditioned on the bins. The decomposition must be computed on the aggregated out-of-fold predictions only.\n\nFundamental base to use in your derivation and implementation:\n- The mean squared error between a probabilistic prediction and a Bernoulli outcome can be expressed as $E[(Y - P)^2]$, where $Y \\in \\{0,1\\}$ is the outcome and $P \\in [0,1]$ is the predicted probability.\n- For a Bernoulli random variable, $Var(Y) = \\bar{y}(1 - \\bar{y})$, where $\\bar{y} = E[Y]$ is the prevalence.\n- The Law of Total Expectation and the Law of Total Variance apply when conditioning on a partition, such as bins of predicted probabilities. Use these to express the mean squared error as a sum of three interpretable terms that together equal the mean squared error when conditioning on the bins of predicted probabilities. The three terms must correspond to: (i) calibration of predictions within bins, (ii) stratification of risk across bins, and (iii) irreducible baseline variability of the outcome. You must compute these empirically by replacing expectations with empirical averages over the out-of-fold predictions and observed outcomes.\n- For probabilistic modeling, use logistic regression: given features $x_i \\in \\mathbb{R}^d$, model the predicted probability as $p_i = \\sigma(w^\\top x_i + b)$ with $\\sigma(z) = 1 / (1 + e^{-z})$. Fit $(w,b)$ by minimizing the regularized negative log-likelihood on the training fold only, using an $\\ell_2$ penalty of strength $\\lambda$ on $w$ but not on $b$. Use standard numerical optimization to find the minimizer.\n- For cross-validation, partition indices into $k$ folds of approximately equal size, fit the model on $k-1$ folds, predict probabilities on the held-out fold, and aggregate out-of-fold predictions across all folds.\n\nBinning and decomposition requirements:\n- Partition predicted probabilities into $B$ equally spaced bins over $[0,1]$, using closed intervals that cover $[0,1]$ without overlap. For the rightmost bin, include the right endpoint $1$. Bins with zero observations must be ignored in empirical sums.\n- Compute the mean squared error (Brier Score) as the empirical average of $(y_i - p_i)^2$ over all $N$ out-of-fold predictions.\n- Compute the three components implied by conditioning on bins using the Law of Total Expectation and the Law of Total Variance: a within-bin calibration term (reliability), a between-bin variance of the conditional outcome rate relative to the overall prevalence (resolution), and the overall Bernoulli variance determined by the sample prevalence (uncertainty). Each term must be computed empirically using per-bin counts and means, and the overall sample prevalence.\n\nOptional controlled miscalibration:\n- After computing out-of-fold probabilities $\\{p_i\\}_{i=1}^N$ for a given test case, you must optionally transform them by scaling the log-odds with a factor $\\gamma$ before binning: if $\\ell_i = \\log\\left(\\frac{p_i}{1 - p_i}\\right)$, transform to $\\ell_i' = \\gamma \\cdot \\ell_i$ and $p_i' = \\sigma(\\ell_i')$. Use this only if the test case specifies $\\gamma \\neq 1$; otherwise use the original $\\{p_i\\}$.\n\nClinical interpretation requirement:\n- The three empirical components must be interpreted as follows, and your implementation must compute the exact quantities corresponding to these interpretations: \n  1) Reliability: how close predicted risks are, within bins, to the observed event rates in those bins (calibration within strata).\n  2) Resolution: how much the observed event rates differ across bins relative to the overall prevalence (stratification ability across patient subgroups).\n  3) Uncertainty: the inherent Bernoulli variability determined by the overall prevalence in the evaluated cohort (baseline difficulty of the prediction problem).\nYour program does not print interpretations, but your derivation in the solution must show how each arises from the laws cited above.\n\nModeling and cross-validation specifics to implement:\n- Use logistic regression with parameters $(w,b)$ trained by minimizing the regularized negative log-likelihood with $\\ell_2$ penalty of strength $\\lambda$ on $w$ only. All training for any fold must use only the training data from that fold; there must be no leakage of held-out outcomes into training or calibration steps.\n- Use $k$-fold cross-validation with fold sizes differing by at most one. Shuffle indices once per test case with the provided seed before creating folds.\n- When computing the log-odds transform for optional miscalibration, you must clip probabilities to the open interval $(\\epsilon, 1 - \\epsilon)$ with $\\epsilon = 10^{-15}$ to avoid numerical overflow.\n\nTest suite and required outputs:\nImplement your program to run the following three test cases. For each, you must generate data, perform cross-validation, compute out-of-fold predictions, optionally apply miscalibration scaling, bin predictions, and compute the four numeric quantities in this exact order: Brier Score, Reliability, Resolution, Uncertainty. Aggregate the results for all test cases into a single flat list and print them as specified below.\n\nCommon synthetic data generation for a test case:\n- Given $N$, $d$, a seed $s$, a true parameter vector $w^\\star \\in \\mathbb{R}^d$, and intercept $b^\\star \\in \\mathbb{R}$, draw features $X \\in \\mathbb{R}^{N \\times d}$ with independent standard normal entries, compute true probabilities $p_i^\\star = \\sigma\\left((w^\\star)^\\top x_i + b^\\star\\right)$, and draw outcomes $y_i \\sim \\mathrm{Bernoulli}(p_i^\\star)$ independently, using the specified seed $s$.\n\nTest cases:\n- Case A (informative, approximately calibrated):\n  - $N = 240$, $d = 3$, seed $= 11$.\n  - True parameters: $w^\\star = (1.2, -1.0, 0.8)$, $b^\\star = -0.4$.\n  - Cross-validation: $k = 6$ folds, $B = 12$ bins.\n  - Regularization: $\\lambda = 1.0$.\n  - Log-odds scaling: $\\gamma = 1.0$.\n- Case B (uninformative, near-constant predictions around prevalence):\n  - $N = 300$, $d = 2$, seed $= 7$.\n  - True parameters: $w^\\star = (0.05, -0.05)$, $b^\\star = -1.386$.\n  - Cross-validation: $k = 5$ folds, $B = 10$ bins.\n  - Regularization: $\\lambda = 100.0$.\n  - Log-odds scaling: $\\gamma = 1.0$.\n- Case C (informative but intentionally miscalibrated by overconfident scaling):\n  - $N = 260$, $d = 2$, seed $= 19$.\n  - True parameters: $w^\\star = (1.8, 1.2)$, $b^\\star = -2.0$.\n  - Cross-validation: $k = 5$ folds, $B = 8$ bins.\n  - Regularization: $\\lambda = 0.1$.\n  - Log-odds scaling: $\\gamma = 1.8$.\n\nYour program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with the twelve floating-point numbers in the exact order:\n$[\\text{BS}_A,\\text{Rel}_A,\\text{Res}_A,\\text{Unc}_A,\\text{BS}_B,\\text{Rel}_B,\\text{Res}_B,\\text{Unc}_B,\\text{BS}_C,\\text{Rel}_C,\\text{Res}_C,\\text{Unc}_C]$.\nIt is acceptable to round each number to six decimal places for printing. No units are involved. Angles are not involved. Percentages are not to be used; all quantities are decimals in $[0,1]$.",
            "solution": "The problem asks for the implementation of a Brier score decomposition for a probabilistic binary classifier, trained and evaluated using $k$-fold cross-validation. This involves several steps: deriving the decomposition from first principles, implementing logistic regression with $\\ell_2$ regularization, performing cross-validation to obtain out-of-fold predictions, and finally, computing the Brier score and its components: Reliability, Resolution, and Uncertainty.\n\n### 1. Brier Score Decomposition\n\nThe Brier Score (BS) is the mean squared error between predicted probabilities $p_i$ and binary outcomes $y_i \\in \\{0, 1\\}$. It is a strictly proper scoring rule for evaluating probabilistic forecasts.\n$$\nBS = \\frac{1}{N} \\sum_{i=1}^{N} (p_i - y_i)^2\n$$\nThe decomposition of the Brier score provides insight into different aspects of model performance. We partition the $N$ samples into $B$ bins based on their predicted probabilities $p_i$. For each bin $b \\in \\{1, \\dots, B\\}$, we define:\n-   $n_b$: the number of samples in bin $b$.\n-   $\\bar{p}_b = \\frac{1}{n_b} \\sum_{i \\in b} p_i$: the mean predicted probability in bin $b$.\n-   $\\bar{y}_b = \\frac{1}{n_b} \\sum_{i \\in b} y_i$: the observed event frequency (mean outcome) in bin $b$.\n\nThe overall prevalence of the outcome is $\\bar{y} = \\frac{1}{N} \\sum_{i=1}^{N} y_i$.\n\nThe Brier score decomposition, as described by Murphy (1973), relates the Brier score to three key components: Reliability, Resolution, and Uncertainty.\n\n**Uncertainty (UNC)**: This component measures the inherent unpredictability of the outcome, irrespective of the model. It is the variance of the Bernoulli-distributed outcome variable $Y$.\n$$\nUNC = \\bar{y}(1 - \\bar{y})\n$$\nA higher prevalence (closer to $0.5$) leads to higher uncertainty. This is the Brier score of a trivial model that always predicts the overall mean $\\bar{y}$.\n\n**Resolution (RES)**: This component measures the model's ability to partition the population into subgroups with different outcome rates. It is the weighted variance of the bin-wise outcome frequencies around the overall prevalence.\n$$\nRES = \\frac{1}{N} \\sum_{b=1}^{B} n_b (\\bar{y}_b - \\bar{y})^2\n$$\nA high resolution is desirable, indicating that the model successfully separates low-risk and high-risk groups.\n\n**Reliability (REL)**: This component measures the calibration of the model. It quantifies the discrepancy between the mean predicted probability and the observed event frequency within each bin.\n$$\nREL = \\frac{1}{N} \\sum_{b=1}^{B} n_b (\\bar{p}_b - \\bar{y}_b)^2\n$$\nA low reliability score is desirable, indicating that if the model predicts an average risk of $\\bar{p}_b$ for a group of patients, the observed event rate in that group is close to $\\bar{p}_b$.\n\nThe three components are related to the Brier score through the identity $BS = REL - RES + UNC$. This can be derived by applying the Law of Total Variance to the outcome variable $Y$, conditioned on the bins $b$. The law states $Var(Y) = E[Var(Y|b)] + Var(E[Y|b])$. In terms of our empirical quantities:\n$$\n\\bar{y}(1-\\bar{y}) = \\frac{1}{N}\\sum_{b=1}^B n_b \\bar{y}_b(1-\\bar{y}_b) + \\frac{1}{N}\\sum_{b=1}^B n_b (\\bar{y}_b - \\bar{y})^2\n$$\nThis is $UNC = UNC_b + RES$, where $UNC_b$ is the expected within-bin variance of the outcome.\nFurthermore, the Brier score itself can be decomposed as $BS = REL + UNC_b$. This identity arises from decomposing the squared error within each bin. By combining these two identities, we eliminate $UNC_b$ and arrive at the final decomposition:\n$$\nBS = REL + (UNC - RES) \\implies BS = REL - RES + UNC\n$$\nOur task is to compute all four quantities ($BS, REL, RES, UNC$) based on their definitions.\n\n### 2. Probabilistic Modeling and Cross-Validation\n\nThe probabilistic model is logistic regression. For a feature vector $x \\in \\mathbb{R}^d$, the probability of a positive outcome is modeled as:\n$$\np(x; w, b) = \\sigma(w^\\top x + b) = \\frac{1}{1 + e^{-(w^\\top x + b)}}\n$$\nwhere $w \\in \\mathbb{R}^d$ are the weights and $b \\in \\mathbb{R}$ is the intercept. The parameters $(w, b)$ are determined by minimizing the regularized negative log-likelihood (also known as log loss or cross-entropy loss) on the training data $\\{(x_i, y_i)\\}_{i=1}^{N_{train}}$:\n$$\nJ(w, b) = -\\sum_{i=1}^{N_{train}} \\left[ y_i \\log p_i + (1-y_i) \\log(1-p_i) \\right] + \\frac{\\lambda}{2} \\|w\\|_2^2\n$$\nwhere $p_i = p(x_i; w, b)$ and $\\lambda$ is the regularization strength for the $\\ell_2$ penalty on the weights $w$. This is a convex optimization problem, which can be solved using standard gradient-based methods like L-BFGS-B. The gradient of the cost function with respect to the parameters is:\n$$\n\\nabla_w J = \\sum_{i=1}^{N_{train}} (p_i - y_i)x_i + \\lambda w = X_{train}^\\top (p - y) + \\lambda w\n$$\n$$\n\\nabla_b J = \\sum_{i=1}^{N_{train}} (p_i - y_i)\n$$\nThe evaluation is performed using $k$-fold cross-validation. The dataset is partitioned into $k$ folds. For each fold, the model is trained on the other $k-1$ folds, and then used to predict probabilities for the samples in the held-out fold. This process is repeated for all $k$ folds, yielding a set of $N$ out-of-fold predictions $\\{p_i\\}_{i=1}^N$ and corresponding outcomes $\\{y_i\\}_{i=1}^N$. These aggregated predictions are then used for the Brier score-decomposition. This procedure ensures that the evaluation is performed on data not seen during training, providing an unbiased estimate of the model's generalization performance.\n\n### 3. Implementation Details\n\n-   **Data Generation**: For each test case, synthetic data is generated according to the specified parameters ($N, d, w^\\star, b^\\star$, seed).\n-   **Cross-Validation**: Indices are shuffled, and `numpy.array_split` is used to create $k$ folds of nearly equal size.\n-   **Optimization**: `scipy.optimize.minimize` with the `L-BFGS-B` method is used to find the optimal logistic regression parameters $(w, b)$ for each training split. A function returning both the cost and the gradient is supplied for efficiency.\n-   **Miscalibration**: If a scaling factor $\\gamma \\neq 1.0$ is provided, the out-of-fold predicted probabilities $p_i$ are converted to log-odds $\\ell_i$, scaled by $\\gamma$, and then transformed back to probabilities $p'_i = \\sigma(\\gamma \\ell_i)$. To avoid numerical issues, probabilities are clipped to a small interval $[\\epsilon, 1-\\epsilon]$ before the log-odds transformation.\n-   **Binning**: The final (potentially-scaled) probabilities are binned into $B$ equally-spaced intervals on $[0,1]$. Special care is taken to correctly bin the endpoint $p=1.0$ into the last bin.\n-   **Metric Calculation**: The Brier Score, Reliability, Resolution, and Uncertainty are calculated from the out-of-fold predictions and outcomes according to their definitions. Empty bins are ignored in the summations for REL and RES.\n\nThe final program executes this entire pipeline for each of the three test cases and prints the twelve resulting floating-point numbers in the specified format.",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.special import expit\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n\n    def _sigma(z):\n        \"\"\"Sigmoid function.\"\"\"\n        return expit(z)\n\n    def _cost_and_grad(params, X, y, lambda_reg):\n        \"\"\"\n        Computes the cost and gradient for L2-regularized logistic regression.\n        Cost is the negative log-likelihood plus the regularization term.\n        \"\"\"\n        w = params[:-1]\n        b = params[-1]\n        m = X.shape[0]\n\n        z = X @ w + b\n\n        # Negative log-likelihood (NLL) using a numerically stable formulation.\n        # This is equivalent to sum(log(1+exp(z)) - y*z).\n        nll = np.sum(np.logaddexp(0., z) - y * z)\n\n        # L2 regularization on weights w only.\n        reg_term = 0.5 * lambda_reg * np.sum(w**2)\n        cost = nll + reg_term\n\n        # Gradient\n        p = _sigma(z)\n        grad_common = p - y\n        grad_w = X.T @ grad_common + lambda_reg * w\n        grad_b = np.sum(grad_common)\n        grad = np.append(grad_w, grad_b)\n\n        return cost, grad\n\n    def _train_logistic_regression(X, y, lambda_reg):\n        \"\"\"\n        Trains a logistic regression model.\n        \"\"\"\n        d = X.shape[1]\n        initial_params = np.zeros(d + 1)\n        \n        result = minimize(\n            fun=_cost_and_grad,\n            x0=initial_params,\n            args=(X, y, lambda_reg),\n            method='L-BFGS-B',\n            jac=True\n        )\n        \n        w, b = result.x[:-1], result.x[-1]\n        return w, b\n\n    def _generate_data(N, d, w_star, b_star, seed):\n        \"\"\"\n        Generates synthetic data for a logistic regression problem.\n        \"\"\"\n        rng = np.random.default_rng(seed)\n        X = rng.standard_normal(size=(N, d))\n        true_p = _sigma(X @ np.array(w_star) + b_star)\n        y = rng.binomial(1, true_p)\n        return X, y\n\n    def run_case(N, d, seed, w_star, b_star, k, B, lambda_reg, gamma):\n        \"\"\"\n        Executes one full test case: data generation, CV, and metric computation.\n        \"\"\"\n        # 1. Generate data\n        X, y = _generate_data(N, d, w_star, b_star, seed)\n        \n        # 2. Setup k-fold cross-validation\n        indices = np.arange(N)\n        rng = np.random.default_rng(seed)\n        shuffled_indices = rng.permutation(indices)\n        folds = np.array_split(shuffled_indices, k)\n        \n        oof_preds = np.zeros(N)\n        oof_y = np.zeros(N)\n\n        # 3. K-fold cross-validation loop\n        for i in range(k):\n            test_indices = folds[i]\n            train_indices = np.concatenate([folds[j] for j in range(k) if i != j])\n            \n            X_train, y_train = X[train_indices], y[train_indices]\n            X_test, y_test_fold = X[test_indices], y[test_indices]\n            \n            if X_train.shape[0] == 0:\n                continue\n            \n            w, b = _train_logistic_regression(X_train, y_train, lambda_reg)\n            p_test = _sigma(X_test @ w + b)\n            \n            oof_preds[test_indices] = p_test\n            oof_y[test_indices] = y_test_fold\n\n        # 4. Optional miscalibration via log-odds scaling\n        if gamma != 1.0:\n            epsilon = 1e-15\n            p_clipped = np.clip(oof_preds, epsilon, 1 - epsilon)\n            # Manual logit: log(p/(1-p))\n            logits = np.log(p_clipped) - np.log(1 - p_clipped)\n            scaled_logits = gamma * logits\n            oof_preds = _sigma(scaled_logits)\n\n        # 5. Compute Brier Score and its decomposition\n        # Brier Score (BS)\n        brier_score = np.mean((oof_preds - oof_y)**2)\n        \n        # Binning setup\n        bins = np.linspace(0, 1, B + 1)\n        # np.digitize: bin_indices[i] = j if bins[j-1] <= x < bins[j]\n        # returns indices from 1 to B+1.\n        bin_indices = np.digitize(oof_preds, bins)\n        \n        # Map to 0-indexed bins [0, B-1].\n        # A value of 1.0 is mapped to bin index B+1. It should be in the last bin B-1.\n        bin_indices = np.minimum(bin_indices, B) - 1\n\n        # Overall prevalence for Uncertainty\n        y_mean_overall = np.mean(oof_y)\n        uncertainty = y_mean_overall * (1 - y_mean_overall)\n        \n        # Reliability and Resolution\n        reliability = 0.0\n        resolution = 0.0\n        \n        for b_idx in range(B):\n            in_bin_mask = (bin_indices == b_idx)\n            n_b = np.sum(in_bin_mask)\n            \n            if n_b > 0:\n                y_b_mean = np.mean(oof_y[in_bin_mask])\n                p_b_mean = np.mean(oof_preds[in_bin_mask])\n                \n                reliability += n_b * (p_b_mean - y_b_mean)**2\n                resolution += n_b * (y_b_mean - y_mean_overall)**2\n                \n        reliability /= N\n        resolution /= N\n        \n        return [brier_score, reliability, resolution, uncertainty]\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A\n        {'N': 240, 'd': 3, 'seed': 11, 'w_star': [1.2, -1.0, 0.8], 'b_star': -0.4,\n         'k': 6, 'B': 12, 'lambda_reg': 1.0, 'gamma': 1.0},\n        # Case B\n        {'N': 300, 'd': 2, 'seed': 7, 'w_star': [0.05, -0.05], 'b_star': -1.386,\n         'k': 5, 'B': 10, 'lambda_reg': 100.0, 'gamma': 1.0},\n        # Case C\n        {'N': 260, 'd': 2, 'seed': 19, 'w_star': [1.8, 1.2], 'b_star': -2.0,\n         'k': 5, 'B': 8, 'lambda_reg': 0.1, 'gamma': 1.8},\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result = run_case(**case)\n        all_results.extend(result)\n\n    # Format the final output string.\n    formatted_results = [f\"{x:.6f}\" for x in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The out-of-fold predictions generated by cross-validation are not only useful for evaluation but can also serve as the foundation for building more powerful models. This practice shifts our focus from evaluating a single model to optimally combining several different models into a single, superior ensemble predictor. You will explore the mathematical underpinnings of the Super Learner algorithm, a principled method that uses cross-validated predictions to find the optimal weights for a convex combination of base learners. This exercise involves deriving the convex optimization problem at the heart of the Super Learner, demonstrating how cross-validation enables the creation of theoretically-grounded and highly performant ensemble models. ",
            "id": "4957971",
            "problem": "A clinical research team is building an ensemble predictor (known as the Super Learner) to estimate a continuous medical outcome, such as systolic blood pressure, from a set of predictors. There are $M$ base learning algorithms. For each algorithm $m \\in \\{1,\\dots,M\\}$ and each observation $i \\in \\{1,\\dots,N\\}$, $K$-fold cross-validation (CV) yields an out-of-fold prediction $p_{i}^{(m)}$, obtained by training the algorithm on the $N - N/K$ observations not containing $i$ and predicting on $i$. Let $y_{i}$ be the observed outcome.\n\nThe Super Learner forms a weighted convex combination of base learners’ predictions with weights $w = (w_{1},\\dots,w_{M})$ to minimize the $K$-fold cross-validated risk under squared error loss. Using only the foundational definition of $K$-fold cross-validated risk and squared error loss, perform the following:\n\n1) Derive from first principles the optimization problem that selects $w$ by minimizing the average cross-validated squared error across all $N$ observations based on the out-of-fold predictions $\\{p_{i}^{(m)}\\}_{i,m}$. Express the problem in matrix-vector form in terms of the $N \\times M$ matrix $P$ of out-of-fold predictions with entries $P_{i m} = p_{i}^{(m)}$ and the outcome vector $Y \\in \\mathbb{R}^{N}$. Explicitly state the constraints that ensure non-negativity and sum-to-one of the weights and justify why these constraints are appropriate in this medical prediction context. Prove that the objective function is convex.\n\n2) Specialize to $M=2$. Define the sufficient statistics $s_{11} = \\frac{1}{N}\\sum_{i=1}^{N}\\big(p_{i}^{(1)}\\big)^{2}$, $s_{22} = \\frac{1}{N}\\sum_{i=1}^{N}\\big(p_{i}^{(2)}\\big)^{2}$, $s_{12} = \\frac{1}{N}\\sum_{i=1}^{N}p_{i}^{(1)}p_{i}^{(2)}$, $t_{1} = \\frac{1}{N}\\sum_{i=1}^{N}y_{i}p_{i}^{(1)}$, and $t_{2} = \\frac{1}{N}\\sum_{i=1}^{N}y_{i}p_{i}^{(2)}$. Using your derivation from part (1), reduce the constrained problem to a univariate convex minimization over $w_{1}$ with $w_{2} = 1 - w_{1}$, and obtain a closed-form expression for the minimizer before enforcing the constraints. Then, using the following values computed from the cross-validated out-of-fold predictions:\n- $s_{11} = 5$,\n- $s_{22} = 4$,\n- $s_{12} = 2$,\n- $t_{1} = 3$,\n- $t_{2} = 2$,\ncompute the optimal Super Learner weights under the constraints $w_{1} \\ge 0$, $w_{2} \\ge 0$, and $w_{1} + w_{2} = 1$. Provide your final answer as a row matrix containing $(w_{1}^{\\star}, w_{2}^{\\star})$. No rounding is required.",
            "solution": "The problem is evaluated to be valid as it is scientifically grounded in statistical learning theory, well-posed, objective, and contains all necessary information for a unique solution.\n\nPart 1: Derivation of the Optimization Problem\n\nThe Super Learner prediction for the $i$-th observation, denoted $\\hat{y}_{i}(w)$, is a convex combination of the out-of-fold predictions from the $M$ base learners. The weights are $w = (w_{1}, \\dots, w_{M})$.\n$$\n\\hat{y}_{i}(w) = \\sum_{m=1}^{M} w_{m} p_{i}^{(m)}\n$$\nThe objective is to minimize the $K$-fold cross-validated risk under squared error loss. This risk, $R_{CV}(w)$, is the average of the squared errors over all $N$ observations.\n$$\nR_{CV}(w) = \\frac{1}{N} \\sum_{i=1}^{N} \\left(y_{i} - \\hat{y}_{i}(w)\\right)^{2}\n$$\nSubstituting the expression for $\\hat{y}_{i}(w)$:\n$$\nR_{CV}(w) = \\frac{1}{N} \\sum_{i=1}^{N} \\left(y_{i} - \\sum_{m=1}^{M} w_{m} p_{i}^{(m)}\\right)^{2}\n$$\nTo express this in matrix-vector form, we define the following:\n- $Y$: an $N \\times 1$ column vector of the observed outcomes, where the $i$-th element is $y_{i}$.\n- $P$: an $N \\times M$ matrix of the out-of-fold predictions, where the entry in the $i$-th row and $m$-th column is $P_{i m} = p_{i}^{(m)}$.\n- $w$: an $M \\times 1$ column vector of the weights, where the $m$-th element is $w_{m}$.\n\nThe vector of Super Learner predictions for all $N$ observations, $\\hat{Y}(w)$, can be written as the matrix-vector product $Pw$.\n$$\n\\hat{Y}(w) = Pw\n$$\nThe sum of squared errors is the squared Euclidean norm of the residual vector, $Y - Pw$.\n$$\n\\sum_{i=1}^{N} \\left(y_{i} - \\sum_{m=1}^{M} w_{m} p_{i}^{(m)}\\right)^{2} = \\|Y - Pw\\|_{2}^{2} = (Y - Pw)^{T}(Y - Pw)\n$$\nThus, the objective function to minimize is:\n$$\nR_{CV}(w) = \\frac{1}{N} \\|Y - Pw\\|_{2}^{2} = \\frac{1}{N} (Y - Pw)^{T}(Y - Pw)\n$$\nThe problem statement requires that the weights form a convex combination. This imposes two constraints:\n1.  Non-negativity: $w_{m} \\ge 0$ for all $m \\in \\{1, \\dots, M\\}$.\n2.  Sum-to-one: $\\sum_{m=1}^{M} w_{m} = 1$.\n\nThe complete optimization problem in matrix-vector form is:\n$$\n\\underset{w \\in \\mathbb{R}^{M}}{\\text{minimize}} \\quad \\frac{1}{N} (Y - Pw)^{T}(Y - Pw)\n$$\n$$\n\\text{subject to} \\quad w_{m} \\ge 0 \\quad \\text{for } m=1, \\dots, M \\quad \\text{and} \\quad \\mathbf{1}^{T}w = 1\n$$\nwhere $\\mathbf{1}$ is an $M \\times 1$ vector of ones.\n\nJustification of constraints:\n- The sum-to-one constraint, $\\sum_{m=1}^{M} w_{m} = 1$, ensures that the Super Learner is a weighted average of the base learners. This provides stability, as the final prediction for an observation $i$ is guaranteed to be within the range of the base predictions, $[\\min_{m} p_{i}^{(m)}, \\max_{m} p_{i}^{(m)}]$. This prevents extrapolation and makes the model more robust.\n- The non-negativity constraint, $w_{m} \\ge 0$, is crucial for interpretability in a medical context. Each weight $w_{m}$ can be interpreted as the positive contribution or importance of the base learner $m$ to the ensemble. A negative weight would be difficult to interpret, as it would imply that a higher prediction from a base learner should lead to a lower prediction from the ensemble, suggesting a complex \"anti-correlation\" that is typically not a desirable or stable feature for prediction.\n\nProof of convexity:\nThe objective function is $f(w) = \\frac{1}{N} (Y - Pw)^{T}(Y - Pw)$. We can expand this expression:\n$$\nf(w) = \\frac{1}{N} (Y^{T} - w^{T}P^{T})(Y - Pw) = \\frac{1}{N} (Y^{T}Y - Y^{T}Pw - w^{T}P^{T}Y + w^{T}P^{T}Pw)\n$$\nSince $w^{T}P^{T}Y$ is a scalar, it is equal to its own transpose, $(w^{T}P^{T}Y)^{T} = Y^{T}Pw$. Therefore, we have:\n$$\nf(w) = \\frac{1}{N} (w^{T}(P^{T}P)w - 2Y^{T}Pw + Y^{T}Y)\n$$\nThis is a quadratic function of $w$. To determine its convexity, we compute its Hessian matrix with respect to $w$. The gradient is:\n$$\n\\nabla_{w} f(w) = \\frac{1}{N} (2(P^{T}P)w - 2P^{T}Y)\n$$\nThe Hessian matrix, $H$, is the derivative of the gradient:\n$$\nH = \\nabla_{w}^{2} f(w) = \\frac{2}{N} P^{T}P\n$$\nA function is convex if its Hessian matrix is positive semi-definite. For any non-zero vector $z \\in \\mathbb{R}^{M}$, we test the condition $z^{T}Hz \\ge 0$.\n$$\nz^{T} H z = z^{T} \\left(\\frac{2}{N} P^{T}P\\right) z = \\frac{2}{N} z^{T}P^{T}Pz = \\frac{2}{N} (Pz)^{T}(Pz) = \\frac{2}{N} \\|Pz\\|_{2}^{2}\n$$\nSince the squared Euclidean norm $\\|Pz\\|_{2}^{2}$ is always non-negative, and $\\frac{2}{N}$ is a positive constant, we have $z^{T}Hz \\ge 0$. Therefore, the Hessian matrix $H$ is positive semi-definite, and the objective function $f(w)$ is convex. The feasible region defined by the linear constraints is a convex set (a simplex), so this is a convex optimization problem.\n\nPart 2: Specialization to $M=2$\n\nFor $M=2$, the weights are $(w_{1}, w_{2})$ and the constraints are $w_{1} \\ge 0$, $w_{2} \\ge 0$, and $w_{1} + w_{2} = 1$. The sum-to-one constraint allows us to write $w_{2} = 1 - w_{1}$. The constraints on $w_{1}$ become $w_{1} \\ge 0$ and $1-w_{1} \\ge 0$, which simplifies to $w_{1} \\in [0, 1]$.\n\nThe objective function to minimize is:\n$$\nR_{CV}(w_{1}, w_{2}) = \\frac{1}{N} \\sum_{i=1}^{N} (y_{i} - (w_{1}p_{i}^{(1)} + w_{2}p_{i}^{(2)}))^{2}\n$$\nSubstituting $w_{2}=1-w_{1}$:\n$$\nf(w_{1}) = \\frac{1}{N} \\sum_{i=1}^{N} (y_{i} - (w_{1}p_{i}^{(1)} + (1-w_{1})p_{i}^{(2)}))^{2} = \\frac{1}{N} \\sum_{i=1}^{N} (y_{i} - p_{i}^{(2)} - w_{1}(p_{i}^{(1)} - p_{i}^{(2)}))^{2}\n$$\nThis is a quadratic function in $w_{1}$ of the form $Aw_{1}^{2} - 2Bw_{1} + C$. To find the unconstrained minimizer, we expand and group terms involving $w_{1}$:\n$$\nf(w_{1}) = w_{1}^{2} \\left(\\frac{1}{N}\\sum_{i=1}^{N}(p_{i}^{(1)} - p_{i}^{(2)})^{2}\\right) - 2w_{1} \\left(\\frac{1}{N}\\sum_{i=1}^{N}(y_{i} - p_{i}^{(2)})(p_{i}^{(1)} - p_{i}^{(2)})\\right) + \\text{const}\n$$\nLet's express the coefficients in terms of the given sufficient statistics:\nThe coefficient of $w_{1}^{2}$ is:\n$$\nA = \\frac{1}{N}\\sum_{i=1}^{N}((p_{i}^{(1)})^{2} - 2p_{i}^{(1)}p_{i}^{(2)} + (p_{i}^{(2)})^{2}) = s_{11} - 2s_{12} + s_{22}\n$$\nThe coefficient of $-2w_{1}$ is:\n$$\nB = \\frac{1}{N}\\sum_{i=1}^{N}(y_{i}p_{i}^{(1)} - y_{i}p_{i}^{(2)} - p_{i}^{(1)}p_{i}^{(2)} + (p_{i}^{(2)})^{2}) = t_{1} - t_{2} - s_{12} + s_{22}\n$$\nTo find the unconstrained minimum, we differentiate $f(w_{1})$ with respect to $w_{1}$ and set the result to zero:\n$$\n\\frac{df}{dw_{1}} = 2Aw_{1} - 2B = 0 \\implies w_{1} = \\frac{B}{A}\n$$\nThe unconstrained minimizer, $w_{1}^{\\text{unc}}$, has the closed-form expression:\n$$\nw_{1}^{\\text{unc}} = \\frac{t_{1} - t_{2} - s_{12} + s_{22}}{s_{11} - 2s_{12} + s_{22}}\n$$\nNow, we substitute the provided numerical values:\n- $s_{11} = 5$\n- $s_{22} = 4$\n- $s_{12} = 2$\n- $t_{1} = 3$\n- $t_{2} = 2$\n\nFirst, calculate $A$ and $B$:\n$$\nA = 5 - 2(2) + 4 = 5 - 4 + 4 = 5\n$$\n$$\nB = 3 - 2 - 2 + 4 = 1 - 2 + 4 = 3\n$$\nThe unconstrained minimizer is:\n$$\nw_{1}^{\\text{unc}} = \\frac{3}{5}\n$$\nSince the objective function is convex (it is a parabola opening upwards as $A=5 > 0$), the minimum over the interval $[0, 1]$ is found by checking if the unconstrained minimizer lies within this interval.\nThe value $w_{1}^{\\text{unc}} = \\frac{3}{5}$ is indeed in the interval $[0, 1]$. Therefore, the optimal constrained weight $w_{1}^{\\star}$ is equal to the unconstrained minimizer.\n$$\nw_{1}^{\\star} = \\frac{3}{5}\n$$\nThe corresponding optimal weight $w_{2}^{\\star}$ is:\n$$\nw_{2}^{\\star} = 1 - w_{1}^{\\star} = 1 - \\frac{3}{5} = \\frac{2}{5}\n$$\nThe optimal weights are $(w_{1}^{\\star}, w_{2}^{\\star}) = (\\frac{3}{5}, \\frac{2}{5})$. We verify they satisfy the constraints: $w_{1}^{\\star} = \\frac{3}{5} \\ge 0$, $w_{2}^{\\star} = \\frac{2}{5} \\ge 0$, and their sum is $1$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{3}{5} & \\frac{2}{5}\n\\end{pmatrix}\n}\n$$"
        }
    ]
}