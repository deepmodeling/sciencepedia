{
    "hands_on_practices": [
        {
            "introduction": "在现代统计实践中，惩罚回归是构建预测模型同时防止过拟合的关键工具。然而，评估和比较这些模型需要一种更精细的方法。本练习  超越了经典赤池信息准则（Akaike Information Criterion, AIC）中简单的参数计数，挑战您将更复杂的“有效自由度”概念应用于一个惩罚性Cox比例风险模型。通过完成此计算，您将对如何在存在正则化的情况下量化模型复杂性获得更深入的实践理解。",
            "id": "4953129",
            "problem": "一项基于医院的队列研究使用Cox比例风险模型对右删失的事件发生时间结果进行建模，其中对于协变量向量为 $x \\in \\mathbb{R}^{p}$ 的患者，其在时间 $t$ 的风险为 $h(t \\mid x) = h_{0}(t) \\exp(x^{\\top} \\beta)$，其中 $h_{0}(t)$ 是一个未指明的基线风险。通过在惩罚偏似然拟合中最小化不同惩罚强度下的信息准则来进行自动化变量选择。考虑一个带有二次惩罚矩阵和调整参数的惩罚拟合，该参数仅对系数的一个子集进行收缩，从而产生一个具有以下特征的最大惩罚偏似然估计 $\\hat{\\beta}_{\\lambda}$：\n\n- 在 $\\hat{\\beta}_{\\lambda}$ 处的偏对数似然为 $\\ell_{p}(\\hat{\\beta}_{\\lambda}) = -85.3$。\n- 在 $\\hat{\\beta}_{\\lambda}$ 处的偏似然的观测信息矩阵为\n$$\nJ(\\hat{\\beta}_{\\lambda}) \\;=\\;\n\\begin{pmatrix}\n50  2  1 \\\\\n2  30  0 \\\\\n1  0  20\n\\end{pmatrix}.\n$$\n- 二次惩罚项为 $\\frac{\\lambda}{2}\\,\\beta^{\\top} P \\beta$，调整参数 $\\lambda = 10$，惩罚矩阵为\n$$\nP \\;=\\;\n\\begin{pmatrix}\n1  0  0 \\\\\n0  1  0 \\\\\n0  0  0\n\\end{pmatrix},\n$$\n因此第三个系数未受惩罚。\n\n从作为最大似然拟合的期望Kullback–Leibler散度的渐近无偏估计量的赤池信息准则 (AIC) 的信息论基础出发，并考虑到Cox模型的半参数结构以及由惩罚引入的偏差，推导出适用于通过带有二次惩罚的偏似然拟合的Cox模型的AIC表达式。特别地，论证用基于局部曲率的有效自由度项替换朴素参数计数的合理性，然后使用给定的矩阵计算给定拟合的AIC。\n\n将最终的AIC数值四舍五入至四位有效数字。无需单位。",
            "solution": "首先验证问题以确保其科学上可靠、提法恰当，并且所有必要信息都已提供。\n\n**步骤1：提取已知信息**\n- 模型：Cox比例风险模型，$h(t \\mid x) = h_{0}(t) \\exp(x^{\\top} \\beta)$。\n- 估计方法：最大化惩罚偏似然。\n- 估计处的偏对数似然：$\\ell_{p}(\\hat{\\beta}_{\\lambda}) = -85.3$。\n- 在 $\\hat{\\beta}_{\\lambda}$ 处评估的偏似然的观测信息矩阵：\n$$\nJ(\\hat{\\beta}_{\\lambda}) =\n\\begin{pmatrix}\n50  2  1 \\\\\n2  30  0 \\\\\n1  0  20\n\\end{pmatrix}.\n$$\n- 惩罚项：$\\frac{\\lambda}{2}\\,\\beta^{\\top} P \\beta$。\n- 调整参数：$\\lambda = 10$。\n- 惩罚矩阵：\n$$\nP =\n\\begin{pmatrix}\n1  0  0 \\\\\n0  1  0 \\\\\n0  0  0\n\\end{pmatrix}.\n$$\n\n**步骤2：使用提取的已知信息进行验证**\n该问题是有效的。它被置于生存分析和惩罚回归的标准、完善的框架内。Cox模型、偏似然、二次惩罚（岭回归）和信息准则都是现代统计学中的基本概念。所有必需的数值和矩阵都已提供，并且它们的维度是一致的。惩罚矩阵 $P$ 的结构正确反映了第三个系数未受惩罚的陈述。该问题是客观、独立且科学合理的。\n\n**步骤3：推导与求解**\n赤池信息准则 (AIC) 是一种广泛用于模型选择的度量标准，其基础原理是最小化拟合模型与真实潜在数据生成过程之间的期望Kullback–Leibler散度。对于通过最大似然估计的模型，标准的AIC定义为：\n$$\n\\text{AIC} = -2\\ell(\\hat{\\theta}) + 2k\n$$\n其中 $\\ell(\\hat{\\theta})$ 是最大化后的对数似然， $k$ 是估计的参数数量。$2k$ 项作为模型复杂度的惩罚，用于校正使用相同数据进行模型拟合和评估所产生的乐观偏差。\n\n在Cox比例风险模型的背景下，回归系数 $\\beta$ 是通过最大化偏对数似然 $\\ell_p(\\beta)$ 而非完整似然来估计的。对于一个未惩罚的Cox模型，AIC被类似地定义为：\n$$\n\\text{AIC} = -2\\ell_p(\\hat{\\beta}) + 2p\n$$\n其中 $\\hat{\\beta}$ 是最大偏似然估计， $p$ 是协变量的数量，即 $\\beta$ 的维度。\n\n问题指明系数是通过最大化一个*惩罚*偏对数似然来估计的：\n$$\n\\ell_{\\text{pen}}(\\beta; \\lambda) = \\ell_p(\\beta) - \\frac{\\lambda}{2} \\beta^{\\top} P \\beta\n$$\n所得的估计 $\\hat{\\beta}_{\\lambda}$ 不再是最大偏似然估计。惩罚项在系数估计中引入了偏差（将受惩罚的分量向零收缩），但可以减少其方差，从而带来更好的预测性能。因此，朴素的参数计数 $p$ 不再是模型复杂性或“自由度”的准确度量。惩罚限制了参数空间，所以拟合的有效参数数量小于 $p$。\n\n为了将AIC推广到此类惩罚模型，参数计数 $p$ 被一个称为“有效自由度”的项所取代，记为 $df_{\\lambda}$。该项量化了拟合的惩罚模型的复杂性。对于二次惩罚，统计理论中一个公认的结果给出了有效自由度为：\n$$\ndf_{\\lambda} = \\text{tr}\\left( \\left( J(\\hat{\\beta}_{\\lambda}) + \\lambda P \\right)^{-1} J(\\hat{\\beta}_{\\lambda}) \\right)\n$$\n这里，$J(\\hat{\\beta}_{\\lambda}) = -\\frac{\\partial^2 \\ell_p(\\beta)}{\\partial \\beta \\partial \\beta^{\\top}} \\big|_{\\beta=\\hat{\\beta}_{\\lambda}}$ 是在惩罚估计 $\\hat{\\beta}_{\\lambda}$ 处评估的未惩罚偏似然的观测信息矩阵。矩阵 $\\left( J(\\hat{\\beta}_{\\lambda}) + \\lambda P \\right)$ 是惩罚偏对数似然的海森矩阵的负数，表示解处目标函数的总曲率。\n\n因此，惩罚Cox模型的广义AIC为：\n$$\n\\text{AIC}_{\\lambda} = -2\\ell_p(\\hat{\\beta}_{\\lambda}) + 2df_{\\lambda}\n$$\n至关重要的是，此公式中的似然项是在惩罚估计处评估的*未惩罚*偏对数似然。\n\n我们现在使用所提供的数据计算该值。\n首先，我们构建矩阵 $H = J(\\hat{\\beta}_{\\lambda}) + \\lambda P$：\n$$\n\\lambda P = 10 \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  0 \\end{pmatrix} = \\begin{pmatrix} 10  0  0 \\\\ 0  10  0 \\\\ 0  0  0 \\end{pmatrix}\n$$\n$$\nH = J(\\hat{\\beta}_{\\lambda}) + \\lambda P = \\begin{pmatrix} 50  2  1 \\\\ 2  30  0 \\\\ 1  0  20 \\end{pmatrix} + \\begin{pmatrix} 10  0  0 \\\\ 0  10  0 \\\\ 0  0  0 \\end{pmatrix} = \\begin{pmatrix} 60  2  1 \\\\ 2  40  0 \\\\ 1  0  20 \\end{pmatrix}\n$$\n接下来，我们必须求 $H$ 的逆矩阵。$H$ 的行列式是：\n$$\n\\det(H) = 60(40 \\cdot 20 - 0 \\cdot 0) - 2(2 \\cdot 20 - 0 \\cdot 1) + 1(2 \\cdot 0 - 40 \\cdot 1)\n$$\n$$\n\\det(H) = 60(800) - 2(40) - 40 = 48000 - 80 - 40 = 47880\n$$\n$H$ 的伴随矩阵，即其余子式矩阵的转置，是：\n$$\n\\text{adj}(H) = \\begin{pmatrix}\n+(40 \\cdot 20 - 0 \\cdot 0)  -(2 \\cdot 20 - 1 \\cdot 0)  +(2 \\cdot 0 - 40 \\cdot 1) \\\\\n-(2 \\cdot 20 - 0 \\cdot 1)  +(60 \\cdot 20 - 1 \\cdot 1)  -(60 \\cdot 0 - 2 \\cdot 1) \\\\\n+(2 \\cdot 0 - 40 \\cdot 1)  -(60 \\cdot 0 - 2 \\cdot 1)  +(60 \\cdot 40 - 2 \\cdot 2)\n\\end{pmatrix}^{\\top}\n$$\n$$\n\\text{adj}(H) = \\begin{pmatrix} 800  -40  -40 \\\\ -40  1199  2 \\\\ -40  2  2396 \\end{pmatrix}^{\\top} = \\begin{pmatrix} 800  -40  -40 \\\\ -40  1199  2 \\\\ -40  2  2396 \\end{pmatrix}\n$$\n逆矩阵是 $H^{-1} = \\frac{1}{47880} \\text{adj}(H)$。\n现在我们计算矩阵乘积 $M = H^{-1} J(\\hat{\\beta}_{\\lambda})$：\n$$\nM = \\frac{1}{47880} \\begin{pmatrix} 800  -40  -40 \\\\ -40  1199  2 \\\\ -40  2  2396 \\end{pmatrix} \\begin{pmatrix} 50  2  1 \\\\ 2  30  0 \\\\ 1  0  20 \\end{pmatrix}\n$$\n有效自由度是该矩阵的迹，$df_{\\lambda} = \\text{tr}(M) = M_{11} + M_{22} + M_{33}$。我们只需要计算对角线元素：\n$$\nM_{11} = \\frac{1}{47880} (800 \\cdot 50 - 40 \\cdot 2 - 40 \\cdot 1) = \\frac{40000 - 80 - 40}{47880} = \\frac{39880}{47880}\n$$\n$$\nM_{22} = \\frac{1}{47880} (-40 \\cdot 2 + 1199 \\cdot 30 + 2 \\cdot 0) = \\frac{-80 + 35970}{47880} = \\frac{35890}{47880}\n$$\n$$\nM_{33} = \\frac{1}{47880} (-40 \\cdot 1 + 2 \\cdot 0 + 2396 \\cdot 20) = \\frac{-40 + 47920}{47880} = \\frac{47880}{47880} = 1\n$$\n值 $M_{33} = 1$ 是符合预期的，因为第三个系数未受惩罚，因此贡献了整整一个自由度。\n总有效自由度是这些对角线元素之和：\n$$\ndf_{\\lambda} = \\frac{39880}{47880} + \\frac{35890}{47880} + \\frac{47880}{47880} = \\frac{39880 + 35890 + 47880}{47880} = \\frac{123650}{47880}\n$$\n数值上，$df_{\\lambda} \\approx 2.582665$。正如预期的那样，这小于朴素参数计数 $p=3$。\n\n最后，我们计算AIC：\n$$\n\\text{AIC}_{\\lambda} = -2\\ell_p(\\hat{\\beta}_{\\lambda}) + 2df_{\\lambda} = -2(-85.3) + 2\\left(\\frac{123650}{47880}\\right)\n$$\n$$\n\\text{AIC}_{\\lambda} = 170.6 + 2(2.582665...) = 170.6 + 5.16533... = 175.76533...\n$$\n四舍五入到四位有效数字，我们得到 $175.8$。",
            "answer": "$$\\boxed{175.8}$$"
        },
        {
            "introduction": "医学领域的预测建模常常会遇到一些导致标准方法失效的实际挑战。本问题  就提出了这样一个情景：逻辑回归中的完全分离，这是在处理罕见事件或强预测性生物标志物时常见的问题。您将批判性地评估三种不同的估计策略——无惩罚最大似然估计、Firth惩罚似然和 $\\ell_1$ 正则化（LASSO）——以理解它们在解决这种不稳定性时的理论特性和实用价值。",
            "id": "4953076",
            "problem": "一位临床医生正在对一个包含 $n$ 名患者的队列中的二元结果 $Y \\in \\{0,1\\}$ 进行建模，该结果表示一种罕见的术后并发症的发生，其患病率约为 $0.02$。考虑一个使用标准 logit 链接的逻辑斯蒂回归模型，其中线性预测器为 $\\eta_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2}$，且 $P(Y_i=1 \\mid X_{i1}, X_{i2}) = \\frac{\\exp(\\eta_i)}{1+\\exp(\\eta_i)}$。假设 $X_{i1} \\in \\{0,1\\}$ 是一个二元指示变量，代表一种极端的实验室异常。在观测数据中，对于每个 $Y_i=1$ 的病例，它都等于 $1$；对于每个 $Y_i=0$ 的对照，它都等于 $0$（这构成了由 $X_{1}$ 引起的完全分离）。假设 $X_{i2}$ 是一个在整个队列中均值为 $0$、方差为 $1$ 的标准化连续风险评分。\n\n考虑三种估计策略：\n\n- 无惩罚最大似然估计，它最大化对数似然函数 $l(\\beta) = \\sum_{i=1}^n \\left\\{ y_i \\eta_i - \\log\\left(1+\\exp(\\eta_i)\\right) \\right\\}$。\n\n- Firth 偏倚减小惩罚似然，通过最大化 $l^*(\\beta) = l(\\beta) + \\frac{1}{2} \\log |I(\\beta)|$ 获得，其中 $I(\\beta)$ 是期望 Fisher 信息矩阵，其动机源于 Jeffreys 不变先验。\n\n- $\\ell_1$ 惩罚逻辑斯蒂回归（最小绝对收缩和选择算子，也称为 LASSO），它通过最小化 $\\{-l(\\beta)\\} + \\lambda \\|\\beta\\|_1$ 来估计 $\\beta$，其中调整参数 $\\lambda > 0$ 通过 $K$ 折交叉验证选择。在标准实践中，截距 $\\beta_0$ 不被惩罚。\n\n在此情景下，选择所有正确的陈述：\n\nA. 在由 $X_1$ 引起的完全分离情况下，无惩罚最大似然估计量不以有限向量的形式存在；Firth 惩罚似然对所有系数都产生有限的估计值；对于任何 $\\lambda>0$，$\\ell_1$ 惩罚也产生有限的解，并且与 Firth 的方法不同，随着 $\\lambda$ 的增加，它可以将某些系数精确地设置为零。\n\nB. Firth 的惩罚等价于一个带有数据自适应权重的 $\\ell_2$ (岭) 惩罚；因为这种二次惩罚会引起强烈的收缩，即使没有调整，它也会将真正不相关的系数精确地设置为零，而 $\\ell_2$ 惩罚无法处理分离问题。\n\nC. 在稀有事件情景中，无惩罚的截距 $\\hat{\\beta}_0$ 在 logit 尺度上存在向下偏倚，导致对事件概率的低估；Firth 的方法减少了这种小样本偏倚，并倾向于增加估计的概率。相比之下，典型的 $\\ell_1$ 实现不惩罚 $\\beta_0$，但会将斜率向零收缩，这可能将高风险概况的拟合 logit 值拉回到稀有事件基线，并且如果 $\\lambda$ 过大，会加剧对真正高风险患者风险的低估。\n\nD. 在 $p>n$ 且没有分离的高维问题中，Firth 的方法通过将小系数精确设置为零来自动执行变量选择，无需任何调整，而当 $p>n$ 时，$\\ell_1$ 惩罚逻辑斯蒂回归未能收敛。\n\nE. 在完全分离的情况下，对于任何固定的 $\\lambda>0$，$\\ell_1$ 惩罚解是有限的；随着 $\\lambda \\to 0^+$，分离预测变量的系数在量级上发散，反映了在这种情况下无惩罚最大似然估计不存在。\n\n选择所有适用的选项。",
            "solution": "我们从逻辑斯蒂回归的典范形式开始。条件概率模型为 $P(Y_i=1 \\mid X_i) = \\pi_i = \\frac{\\exp(\\eta_i)}{1+\\exp(\\eta_i)}$，其中 $\\eta_i = \\beta_0 + \\sum_{j} \\beta_j X_{ij}$。对数似然为\n$$\nl(\\beta) = \\sum_{i=1}^n \\left\\{ y_i \\eta_i - \\log\\left(1+\\exp(\\eta_i)\\right) \\right\\}。\n$$\n得分函数为 $U(\\beta) = \\frac{\\partial l(\\beta)}{\\partial \\beta} = \\sum_{i=1}^n (y_i - \\pi_i) x_i$，其中 $x_i$ 是包含截距的协变量向量，期望 Fisher 信息是 $I(\\beta) = X^\\top W(\\beta) X$，其中 $W(\\beta)$ 是对角矩阵，其对角线元素为 $w_i = \\pi_i(1-\\pi_i)$，$X$ 是设计矩阵。\n\n完全分离与无惩罚最大似然估计的不存在性：完全分离意味着存在一个向量 $b$，使得对于所有 $y_i=1$ 的 $i$，$x_i^\\top b > 0$；对于所有 $y_i=0$ 的 $i$，$x_i^\\top b  0$。在我们的设定中，$X_{i1}$ 本身就完美地分离了数据，所以取一个在 $X_1$ 上有正分量而在其他地方为零的向量 $b$。对于任何 $t>0$，考虑 $\\beta(t) = t b$。那么对于 $y_i=1$，$\\eta_i(t) = x_i^\\top \\beta(t)$ 趋向于 $+\\infty$；对于 $y_i=0$，则趋向于 $-\\infty$。因此，当 $y_i=1$ 时 $\\pi_i(t) \\to 1$，当 $y_i=0$ 时 $\\pi_i(t) \\to 0$，并且对数似然满足\n$$\nl(\\beta(t)) = \\sum_{i=1}^n \\left\\{ y_i \\eta_i(t) - \\log\\left(1+\\exp(\\eta_i(t))\\right) \\right\\} \\to 0,\n$$\n因为对于每个 $i$，当 $y_i=1$ 时，随着 $\\eta_i \\to +\\infty$，$y_i \\eta_i - \\log(1+e^{\\eta_i}) \\to 0$；当 $y_i=0$ 时，随着 $\\eta_i \\to -\\infty$，$-\\log(1+e^{\\eta_i}) \\to 0$。因此，$l(\\beta)$ 的上确界是 $0$，但不存在有限的最大化点；最大似然估计量不以有限向量的形式存在。\n\nFirth 偏倚减小惩罚似然：Firth 的方法修改了得分函数，以消除渐近偏倚展开中的一阶项。等价地，它最大化惩罚对数似然函数\n$$\nl^*(\\beta) = l(\\beta) + \\frac{1}{2} \\log |I(\\beta)|,\n$$\n其中 $I(\\beta) = X^\\top W(\\beta) X$。沿着任何分离方向，当 $t \\to \\infty$ 时，我们有对所有 $i$，$\\pi_i(t) \\to 0$ 或 $\\pi_i(t) \\to 1$，所以对所有 $i$，$w_i(t) = \\pi_i(t)\\{1-\\pi_i(t)\\} \\to 0$。因此，$I(\\beta(t)) = X^\\top W(\\beta(t)) X \\to 0$（从其特征值趋于 $0$ 的意义上），从而 $\\log|I(\\beta(t))| \\to -\\infty$。由于 $l(\\beta(t)) \\to 0$，可以推断出沿着分离射线 $l^*(\\beta(t)) \\to -\\infty$，所以 $l^*(\\beta)$ 有严格上界并能达到一个有限的最大化点。因此，Firth 的方法在存在分离的情况下能产生有限的估计值，并且其构造本身就减少了小样本偏倚。\n\n$\\ell_1$ 惩罚逻辑斯蒂回归：$\\ell_1$ 惩罚估计量最小化\n$$\nQ_\\lambda(\\beta) = \\{-l(\\beta)\\} + \\lambda \\|\\beta\\|_1,\n$$\n其中 $\\lambda>0$。在分离情况下，如上文所述，沿着 $\\beta(t)=t b$，我们有 $-l(\\beta(t)) \\to 0$，而 $\\lambda \\|\\beta(t)\\|_1 \\to +\\infty$（与 $t$ 呈线性关系），因此 $Q_\\lambda(\\beta(t)) \\to +\\infty$。由于 $Q_\\lambda$ 是强制的 (coercive) 且是凸的，对于每个 $\\lambda>0$ 都存在一个有限的最小化点。Karush–Kuhn–Tucker 次梯度最优性条件意味着，对于每个系数 $\\beta_j$，如果在最优点（固定其他坐标时）满足 $\\left|\\frac{\\partial}{\\partial \\beta_j} \\{-l(\\beta)\\}\\big|_{\\beta_j=0}\\right| \\le \\lambda$，那么 $\\hat{\\beta}_j=0$。因此，随着 $\\lambda$ 的增加，$\\ell_1$ 惩罚会产生精确的零值，从而实现自动变量选择。随着 $\\lambda \\to 0^+$，$Q_\\lambda$ 趋近于无惩罚的目标函数，其最小化点可能沿着一个分离方向发散，这反映了有限的无惩罚最大似然估计不存在。\n\n稀有事件偏倚和惩罚的作用：当事件稀有时，截距 $\\beta_0$ 的最大似然估计量在 logit 尺度上存在朝向更负值的一阶小样本偏倚，这转化为对 $P(Y=1)$ 的低估（这在广义线性模型的偏倚展开和稀有事件逻辑斯蒂回归的文献中有记载）。Firth 的方法明确地移除了 $O(n^{-1})$ 阶的偏倚项，在稀有事件中，相对于无惩罚的拟合，它通常会增加估计的概率。在大多数实现中，$\\ell_1$ 惩罚逻辑斯蒂回归不惩罚截距 $\\beta_0$，而斜率系数则被向零收缩。对于稀有事件，收缩斜率会将拟合的 logit 值拉向无惩罚的截距，该截距接近稀有事件的基线；对于真正高风险概况的个体，如果 $\\lambda$ 过大，过度的收缩会降低预测的风险，从而加剧低估。\n\n我们现在评估每个选项：\n\nA. 正确。在完全分离情况下，不存在有限的无惩罚最大似然估计；$l(\\beta)$ 的上确界为 $0$，仅在无穷远处达到。Firth 的惩罚似然 $l^*(\\beta)$ 能达到一个有限的最大值，因为沿着分离方向 $\\frac{1}{2}\\log|I(\\beta)| \\to -\\infty$。对于任何 $\\lambda>0$，$\\ell_1$ 惩罚问题是凸且强制的，产生有限的解，并且随着 $\\lambda$ 的增加，可以将系数精确地设置为零，从而提供自动变量选择。\n\nB. 错误。Firth 的惩罚是 $\\frac{1}{2}\\log|I(\\beta)|$，它不是二次的，并且通过 $I(\\beta)$ 依赖于数据；它不等价于 $\\ell_2$ 岭惩罚。此外，岭回归和 Firth 的方法通常都不会将系数精确设置为零，并且 Firth 的标准形式不涉及调整参数。最后，任何带有正惩罚项的岭回归也可以通过确保有限解来解决分离问题。\n\nC. 正确。稀有事件中的小样本偏倚导致截距向下偏倚（更负的 logit 值），从而低估了 $P(Y=1)$。Firth 的偏倚减小方法纠正了这一阶偏倚，并且通常会增加估计的概率。在标准的 $\\ell_1$ 实践中，截距不被惩罚，但斜率被收缩；这将拟合的 logit 值拉向基线，如果 $\\lambda$ 很大，可能会降低真正高风险个体的预测风险，加剧对此类概况的风险低估。\n\nD. 错误。Firth 的方法不执行自动变量选择，并且在没有明确约束的情况下不会将系数精确设置为零。相反，当 $p>n$ 时，$\\ell_1$ 惩罚逻辑斯蒂回归是适定的 (well-posed) 且被普遍使用；它在适当的条件下收敛，并可以产生稀疏解。\n\nE. 正确。对于任何固定的 $\\lambda>0$，在分离情况下，$\\ell_1$ 惩罚目标函数是强制的，并有一个有限的最小化点。随着 $\\lambda \\to 0^+$，最小化点沿着分离方向移动，使得 $|\\hat{\\beta}_1| \\to \\infty$，这反映了有限的无惩罚最大似然估计量不存在的事实。",
            "answer": "$$\\boxed{ACE}$$"
        },
        {
            "introduction": "虽然许多变量选择方法都致力于识别单一的“最佳”模型，但一种更强大的策略通常是整合来自多个不同候选模型的预测。本练习  探讨了堆叠（stacking）这一先进集成方法的核心机制。您的任务是推导出组合这些模型的最佳权重，从而深入研究支撑这种顶尖预测技术的凸优化基础。",
            "id": "4953094",
            "problem": "在一项临床结局研究中，一家医院通过对 $n$ 名患者的训练队列应用自动化变量选择程序，为连续终点 $Y$ 开发了一个包含 $m$ 个预后模型的库。具体来说，该库包括在多个惩罚水平下由最小绝对收缩和选择算子 (LASSO) 产生的模型，以及在多个模型大小下由前向逐步选择产生的模型。使用 $K$ 折交叉验证，医院为每个模型计算折外预测：对于每个患者 $i \\in \\{1,\\dots,n\\}$ 和模型 $j \\in \\{1,\\dots,m\\}$，量 $\\hat{y}_{ij}$ 是通过在不包含患者 $i$ 的折上拟合模型 $j$ 而产生的对 $Y_i$ 的预测。设 $Z \\in \\mathbb{R}^{n \\times m}$ 是元素为 $Z_{ij} = \\hat{y}_{ij}$ 的矩阵，设 $Y \\in \\mathbb{R}^{n}$ 是观测结局的向量。\n\n为了组合这个库，考虑一个堆叠预测器，它形成模型预测的线性组合，其权重为 $w \\in \\mathbb{R}^{m}$，并服从单纯形平均约束 $\\sum_{j=1}^{m} w_j = 1$。选择堆叠权重以最小化交叉验证的均方误差，即求解以下优化问题\n$$\n\\min_{w \\in \\mathbb{R}^{m}} \\; \\frac{1}{n} \\sum_{i=1}^{n} \\left(Y_i - \\sum_{j=1}^{m} w_j Z_{ij}\\right)^{2} \\quad \\text{subject to} \\quad \\sum_{j=1}^{m} w_j = 1.\n$$\n假设 $Z$ 的列是线性无关的，因此 $Z^{\\top} Z$ 是可逆的。\n\n从凸优化和最小二乘法的基本原理出发，在上述单一线性等式约束下，且不对权重施加任何非负性约束的情况下，推导出最优权重向量关于 $Z$ 和 $Y$ 的闭式表达式。此外，论证该优化问题是否为凸问题，并说明最小值点唯一的条件。请用标准的线性代数运算，将你的最终答案表示为最优权重向量关于 $Z$ 和 $Y$ 的单个解析表达式。不要引入任何数值近似。你的答案必须是无单位的单个表达式。",
            "solution": "该问题要求推导一个受约束的最小二乘问题中最优权重的闭式表达式，并论证问题的凸性及其解的唯一性。\n\n首先，我们使用矩阵表示法将问题形式化。设 $w \\in \\mathbb{R}^{m}$ 为权重向量，$Y \\in \\mathbb{R}^{n}$ 为观测结局向量，$Z \\in \\mathbb{R}^{n \\times m}$ 为折外预测矩阵。优化问题由下式给出：\n$$\n\\min_{w \\in \\mathbb{R}^{m}} \\; \\frac{1}{n} \\sum_{i=1}^{n} \\left(Y_i - \\sum_{j=1}^{m} Z_{ij} w_j\\right)^{2} \\quad \\text{subject to} \\quad \\sum_{j=1}^{m} w_j = 1.\n$$\n求和项 $\\sum_{i=1}^{n} \\left(Y_i - \\sum_{j=1}^{m} Z_{ij} w_j\\right)^{2}$ 是向量 $Y - Zw$ 的欧几里得范数的平方，记作 $\\|Y - Zw\\|_2^2$。约束可以用向量形式写成 $\\mathbf{1}^\\top w = 1$，其中 $\\mathbf{1}$ 是一个包含 $m$ 个 1 的列向量。常数因子 $\\frac{1}{n}$ 不影响最小值点的位置，因此我们可以等价地最小化目标函数 $f(w) = \\|Y - Zw\\|_2^2$。因此，问题是：\n$$ \\min_{w \\in \\mathbb{R}^{m}} f(w) = \\|Y - Zw\\|_2^2 \\quad \\text{subject to} \\quad \\mathbf{1}^\\top w = 1. $$\n\n接下来，我们讨论问题的凸性和解的唯一性。目标函数可以展开为：\n$$ f(w) = (Y - Zw)^\\top(Y - Zw) = Y^\\top Y - 2Y^\\top Z w + w^\\top Z^\\top Z w. $$\n这是关于权重向量 $w$ 的二次函数。为确定其凸性，我们计算其关于 $w$ 的海森矩阵。$f(w)$ 的梯度是：\n$$ \\nabla_w f(w) = -2Z^\\top Y + 2Z^\\top Z w. $$\n海森矩阵是梯度关于 $w^\\top$ 的导数：\n$$ \\nabla_w^2 f(w) = 2Z^\\top Z. $$\n问题陈述中指出矩阵 $Z$ 的列是线性无关的。根据定义，这意味着对于任何非零向量 $v \\in \\mathbb{R}^m$，乘积 $Zv$ 是 $\\mathbb{R}^n$ 中的一个非零向量。因此，二次型 $v^\\top (Z^\\top Z) v$ 可以写成 $(Zv)^\\top(Zv) = \\|Zv\\|_2^2$，对于任何 $v \\neq 0$ 都严格为正。这证明了矩阵 $Z^\\top Z$ 是正定的。\n由于海森矩阵 $2Z^\\top Z$ 对所有 $w$ 都是正定的，目标函数 $f(w)$ 是严格凸的。约束集 $\\{w \\in \\mathbb{R}^m \\mid \\mathbf{1}^\\top w = 1\\}$ 是一个仿射子空间，也是一个凸集。在一个非空凸集上最小化一个严格凸函数，其最小值点是唯一的。因此，所述问题有唯一解。此唯一性的条件是 $Z$ 的列线性无关，这已在题目中给出。\n\n为了找到最优权重向量 $w$，我们使用拉格朗日乘子法。拉格朗日函数 $\\mathcal{L}(w, \\lambda)$ 是通过将约束乘以一个拉格朗日乘子 $\\lambda$ 并加到目标函数上构造的：\n$$ \\mathcal{L}(w, \\lambda) = (Y - Zw)^\\top(Y - Zw) + \\lambda(\\mathbf{1}^\\top w - 1). $$\n最优解必须满足一阶 Karush-Kuhn-Tucker (KKT) 条件，这可以通过将拉格朗日函数对 $w$ 和 $\\lambda$ 的偏导数设为零来找到。\n\n关于向量 $w$ 的偏导数是：\n$$ \\frac{\\partial \\mathcal{L}}{\\partial w} = 2Z^\\top Z w - 2Z^\\top Y + \\lambda\\mathbf{1}. $$\n将其设为零可得：\n$$ 2Z^\\top Z w - 2Z^\\top Y + \\lambda\\mathbf{1} = 0 \\implies Z^\\top Z w = Z^\\top Y - \\frac{\\lambda}{2}\\mathbf{1}. $$\n问题陈述中指出 $Z^\\top Z$ 是可逆的。因此，我们可以用 $\\lambda$ 来表示 $w$：\n$$ w = (Z^\\top Z)^{-1} \\left(Z^\\top Y - \\frac{\\lambda}{2}\\mathbf{1}\\right) = (Z^\\top Z)^{-1}Z^\\top Y - \\frac{\\lambda}{2}(Z^\\top Z)^{-1}\\mathbf{1}. \\quad (1) $$\n\n关于标量 $\\lambda$ 的偏导数是：\n$$ \\frac{\\partial \\mathcal{L}}{\\partial \\lambda} = \\mathbf{1}^\\top w - 1. $$\n将其设为零再现了原始约束：\n$$ \\mathbf{1}^\\top w = 1. \\quad (2) $$\n\n现在，我们将方程 $(1)$ 中 $w$ 的表达式代入约束方程 $(2)$ 中以求解 $\\lambda$：\n$$ \\mathbf{1}^\\top \\left( (Z^\\top Z)^{-1}Z^\\top Y - \\frac{\\lambda}{2}(Z^\\top Z)^{-1}\\mathbf{1} \\right) = 1. $$\n利用矩阵乘法的分配律：\n$$ \\mathbf{1}^\\top (Z^\\top Z)^{-1}Z^\\top Y - \\frac{\\lambda}{2} \\mathbf{1}^\\top (Z^\\top Z)^{-1}\\mathbf{1} = 1. $$\n我们现在可以分离出含有 $\\lambda$ 的项：\n$$ \\frac{\\lambda}{2} \\mathbf{1}^\\top (Z^\\top Z)^{-1}\\mathbf{1} = \\mathbf{1}^\\top (Z^\\top Z)^{-1}Z^\\top Y - 1. $$\n解出 $\\frac{\\lambda}{2}$ 得：\n$$ \\frac{\\lambda}{2} = \\frac{\\mathbf{1}^\\top (Z^\\top Z)^{-1}Z^\\top Y - 1}{\\mathbf{1}^\\top (Z^\\top Z)^{-1}\\mathbf{1}}. $$\n注意，分母 $\\mathbf{1}^\\top (Z^\\top Z)^{-1}\\mathbf{1}$ 是一个非零标量，因为 $(Z^\\top Z)^{-1}$ 是正定的。\n\n最后，我们将 $\\frac{\\lambda}{2}$ 的这个表达式代回方程 $(1)$，以获得最优权重向量 $w$ 的闭式解：\n$$ w = (Z^\\top Z)^{-1}Z^\\top Y - \\left( \\frac{\\mathbf{1}^\\top (Z^\\top Z)^{-1}Z^\\top Y - 1}{\\mathbf{1}^\\top (Z^\\top Z)^{-1}\\mathbf{1}} \\right) (Z^\\top Z)^{-1}\\mathbf{1}. $$\n该表达式完全根据给定的量 $Z$ 和 $Y$ 以及全 1 向量 $\\mathbf{1}$，使用所要求的标准线性代数运算，给出了最优权重向量 $w$。",
            "answer": "$$\n\\boxed{(Z^\\top Z)^{-1}Z^\\top Y - \\frac{\\mathbf{1}^\\top(Z^\\top Z)^{-1}Z^\\top Y - 1}{\\mathbf{1}^\\top(Z^\\top Z)^{-1}\\mathbf{1}} (Z^\\top Z)^{-1}\\mathbf{1}}\n$$"
        }
    ]
}