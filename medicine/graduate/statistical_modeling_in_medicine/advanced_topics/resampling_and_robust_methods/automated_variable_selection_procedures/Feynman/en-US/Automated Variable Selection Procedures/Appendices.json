{
    "hands_on_practices": [
        {
            "introduction": "Automated variable selection procedures applied to binary outcomes, particularly with rare events, can often identify predictors that perfectly separate cases from controls. This scenario, known as complete separation, causes the standard maximum likelihood estimator to diverge, posing a significant practical challenge. This practice explores how penalization methods provide a robust solution by comparing the theoretical properties of Firth's bias-reducing penalty against the widely used $\\ell_1$ (Lasso) penalty, clarifying their distinct mechanisms for ensuring finite estimates and their impact on inference .",
            "id": "4953076",
            "problem": "A clinician is modeling a binary outcome $Y \\in \\{0,1\\}$ indicating occurrence of a rare post-operative complication with prevalence approximately $0.02$ in a cohort of $n$ patients. Consider a logistic regression model with canonical logit link, where the linear predictor is $\\eta_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2}$ and $P(Y_i=1 \\mid X_{i1}, X_{i2}) = \\frac{\\exp(\\eta_i)}{1+\\exp(\\eta_i)}$. Suppose $X_{i1} \\in \\{0,1\\}$ is a binary indicator for an extreme laboratory abnormality that, in the observed data, is equal to $1$ for every case with $Y_i=1$ and $0$ for every control with $Y_i=0$ (this constitutes complete separation by $X_{1}$). Suppose $X_{i2}$ is a standardized continuous risk score with mean $0$ and variance $1$ across the cohort.\n\nThree estimation strategies are considered:\n\n- Unpenalized maximum likelihood estimation, which maximizes the log-likelihood $l(\\beta) = \\sum_{i=1}^n \\left\\{ y_i \\eta_i - \\log\\left(1+\\exp(\\eta_i)\\right) \\right\\}$.\n\n- Firth’s bias-reducing penalized likelihood, obtained by maximizing $l^*(\\beta) = l(\\beta) + \\frac{1}{2} \\log |I(\\beta)|$, where $I(\\beta)$ is the expected Fisher information matrix, as motivated by the Jeffreys invariant prior.\n\n- $\\ell_1$-penalized logistic regression (the least absolute shrinkage and selection operator, also known as Lasso), which estimates $\\beta$ by minimizing $\\{-l(\\beta)\\} + \\lambda \\|\\beta\\|_1$, with tuning parameter $\\lambda > 0$ chosen by $K$-fold cross-validation. In standard practice, the intercept $\\beta_0$ is not penalized.\n\nSelect all statements that are correct in this setting:\n\nA. With complete separation by $X_1$, the unpenalized maximum likelihood estimator does not exist as a finite vector; Firth’s penalized likelihood yields finite estimates for all coefficients; an $\\ell_1$ penalty also yields finite solutions for any $\\lambda>0$ and, unlike Firth’s method, can set some coefficients exactly to zero as $\\lambda$ increases.\n\nB. Firth’s penalization is equivalent to an $\\ell_2$ (ridge) penalty with data-adaptive weights; because this quadratic penalty induces strong shrinkage, it will set truly irrelevant coefficients exactly to zero even without tuning, whereas $\\ell_2$ penalties cannot handle separation.\n\nC. In rare-event settings, the unpenalized intercept $\\hat{\\beta}_0$ is biased downward on the logit scale, leading to underestimation of event probabilities; Firth’s method reduces this small-sample bias and tends to increase estimated probabilities. In contrast, typical $\\ell_1$ implementations do not penalize $\\beta_0$ but shrink slopes toward zero, which can pull fitted logits for high-risk profiles back toward the rare-event baseline and, if $\\lambda$ is too large, exacerbate underestimation of risk for truly high-risk patients.\n\nD. In high-dimensional problems with $p>n$ and no separation, Firth’s method automatically performs variable selection by setting small coefficients to exactly zero without any tuning, whereas $\\ell_1$-penalized logistic regression fails to converge when $p>n$.\n\nE. Under complete separation, for any fixed $\\lambda>0$ the $\\ell_1$-penalized solution is finite; as $\\lambda \\to 0^+$, the coefficient of the separating predictor diverges in magnitude, reflecting the nonexistence of the unpenalized maximum likelihood estimate in this case.\n\nChoose all that apply.",
            "solution": "We begin from the canonical formulation of logistic regression. The conditional probability model is $P(Y_i=1 \\mid X_i) = \\pi_i = \\frac{\\exp(\\eta_i)}{1+\\exp(\\eta_i)}$ with $\\eta_i = \\beta_0 + \\sum_{j} \\beta_j X_{ij}$. The log-likelihood is\n$$\nl(\\beta) = \\sum_{i=1}^n \\left\\{ y_i \\eta_i - \\log\\left(1+\\exp(\\eta_i)\\right) \\right\\}.\n$$\nThe score is $U(\\beta) = \\frac{\\partial l(\\beta)}{\\partial \\beta} = \\sum_{i=1}^n (y_i - \\pi_i) x_i$, where $x_i$ is the covariate vector including an intercept, and the expected Fisher information is $I(\\beta) = X^\\top W(\\beta) X$, where $W(\\beta)$ is diagonal with entries $w_i = \\pi_i(1-\\pi_i)$ and $X$ is the design matrix.\n\nComplete separation and nonexistence of the unpenalized maximum likelihood estimate: Complete separation means that there exists a vector $b$ such that $x_i^\\top b > 0$ for all $i$ with $y_i=1$ and $x_i^\\top b < 0$ for all $i$ with $y_i=0$. In our setting, $X_{i1}$ itself perfectly separates, so take $b$ with a positive component on $X_1$ and zeros elsewhere. For any $t>0$, consider $\\beta(t) = t b$. Then $\\eta_i(t) = x_i^\\top \\beta(t)$ goes to $+\\infty$ for $y_i=1$ and $-\\infty$ for $y_i=0$. Hence $\\pi_i(t) \\to 1$ when $y_i=1$ and $\\pi_i(t) \\to 0$ when $y_i=0$, and the log-likelihood satisfies\n$$\nl(\\beta(t)) = \\sum_{i=1}^n \\left\\{ y_i \\eta_i(t) - \\log\\left(1+\\exp(\\eta_i(t))\\right) \\right\\} \\to 0,\n$$\nsince for each $i$, $y_i \\eta_i - \\log(1+e^{\\eta_i}) \\to 0$ as $\\eta_i \\to +\\infty$ when $y_i=1$, and $-\\log(1+e^{\\eta_i}) \\to 0$ as $\\eta_i \\to -\\infty$ when $y_i=0$. Therefore the supremum of $l(\\beta)$ is $0$, but there is no finite maximizer; the maximum likelihood estimator does not exist as a finite vector.\n\nFirth’s bias-reducing penalized likelihood: Firth’s method modifies the score to remove the first-order term in the asymptotic bias expansion. Equivalently, it maximizes the penalized log-likelihood\n$$\nl^*(\\beta) = l(\\beta) + \\frac{1}{2} \\log |I(\\beta)|,\n$$\nwhere $I(\\beta) = X^\\top W(\\beta) X$. Along any separating direction, as $t \\to \\infty$, we have $\\pi_i(t) \\to 0$ or $\\pi_i(t) \\to 1$ for all $i$, so $w_i(t) = \\pi_i(t)\\{1-\\pi_i(t)\\} \\to 0$ for all $i$. Consequently, $I(\\beta(t)) = X^\\top W(\\beta(t)) X \\to 0$ in the sense that its eigenvalues go to $0$, and thus $\\log|I(\\beta(t))| \\to -\\infty$. Since $l(\\beta(t)) \\to 0$, it follows that $l^*(\\beta(t)) \\to -\\infty$ along separating rays, so $l^*(\\beta)$ is strictly bounded above and attains a finite maximizer. Therefore, Firth’s method yields finite estimates in the presence of separation and reduces small-sample bias by construction.\n\n$\\ell_1$-penalized logistic regression: The $\\ell_1$-penalized estimator minimizes\n$$\nQ_\\lambda(\\beta) = \\{-l(\\beta)\\} + \\lambda \\|\\beta\\|_1,\n$$\nwith $\\lambda>0$. Under separation, along $\\beta(t)=t b$ as above, we have $-l(\\beta(t)) \\to 0$ while $\\lambda \\|\\beta(t)\\|_1 \\to +\\infty$ linearly in $t$, so $Q_\\lambda(\\beta(t)) \\to +\\infty$. Since $Q_\\lambda$ is coercive and convex, a finite minimizer exists for every $\\lambda>0$. The Karush–Kuhn–Tucker subgradient optimality conditions imply that for each coefficient $\\beta_j$, if $\\left|\\frac{\\partial}{\\partial \\beta_j} \\{-l(\\beta)\\}\\big|_{\\beta_j=0}\\right| \\le \\lambda$ at the optimum (with other coordinates fixed), then $\\hat{\\beta}_j=0$. Hence $\\ell_1$ penalization yields exact zeros and thus automatic variable selection as $\\lambda$ increases. As $\\lambda \\to 0^+$, $Q_\\lambda$ approaches the unpenalized objective, and the minimizer may diverge along a separating direction, reflecting the nonexistence of a finite unpenalized maximum likelihood estimate.\n\nRare-event bias and the role of penalties: When the event is rare, the maximum likelihood estimator of the intercept $\\beta_0$ has a first-order small-sample bias toward more negative values on the logit scale, which translates into underestimation of $P(Y=1)$ (this is documented in bias expansions for generalized linear models and in rare-events logistic regression). Firth’s method explicitly removes the $O(n^{-1})$ bias term, and in rare events will typically increase estimated probabilities relative to the unpenalized fit. In most implementations, the intercept $\\beta_0$ is not penalized in $\\ell_1$-penalized logistic regression, whereas slope coefficients are shrunk toward zero. With rare events, shrinking slopes pulls fitted logits toward the unpenalized intercept, which is near the rare-event baseline; for individuals with truly high-risk profiles, excessive shrinkage can lower predicted risks, exacerbating underestimation if $\\lambda$ is too large.\n\nWe now assess each option:\n\nA. Correct. Under complete separation, there is no finite unpenalized maximum likelihood estimate; $l(\\beta)$ has supremum $0$ achieved only at infinity. Firth’s penalized likelihood $l^*(\\beta)$ attains a finite maximum because $\\frac{1}{2}\\log|I(\\beta)| \\to -\\infty$ along separating directions. The $\\ell_1$-penalized problem is convex and coercive for any $\\lambda>0$, yielding finite solutions that can set coefficients exactly to zero, providing automatic variable selection as $\\lambda$ increases.\n\nB. Incorrect. Firth’s penalty is $\\frac{1}{2}\\log|I(\\beta)|$, which is not quadratic and depends on the data through $I(\\beta)$; it is not equivalent to an $\\ell_2$ ridge penalty. Moreover, neither ridge nor Firth’s method sets coefficients exactly to zero in general, and Firth’s standard formulation does not involve a tuning parameter. Finally, ridge with any positive penalty also resolves separation by ensuring a finite solution.\n\nC. Correct. The small-sample bias in rare events drives the intercept downward (more negative logits), underestimating $P(Y=1)$. Firth’s bias reduction corrects this first-order bias and typically increases estimated probabilities. In standard $\\ell_1$ practice, the intercept is unpenalized but slopes are shrunk; this pulls fitted logits toward the baseline and, if $\\lambda$ is large, can reduce predicted risks for truly high-risk individuals, aggravating underestimation for such profiles.\n\nD. Incorrect. Firth’s method does not perform automatic variable selection and does not set coefficients to exactly zero without explicit constraints. In contrast, $\\ell_1$-penalized logistic regression is well-posed and commonly used when $p>n$; it converges under appropriate conditions and can yield sparse solutions.\n\nE. Correct. For any fixed $\\lambda>0$, the $\\ell_1$-penalized objective is coercive under separation and has a finite minimizer. As $\\lambda \\to 0^+$, the minimizer follows the separating direction with $|\\hat{\\beta}_1| \\to \\infty$, mirroring the nonexistence of a finite unpenalized maximum likelihood estimator.",
            "answer": "$$\\boxed{ACE}$$"
        },
        {
            "introduction": "A core task in automated variable selection is to choose an optimal model from a family of candidates, such as those generated by varying the penalty parameter in a penalized regression. Classical tools like the Akaike Information Criterion (AIC) must be modified for this context, as a simple count of covariates no longer accurately reflects model complexity due to shrinkage. This exercise delves into the concept of \"effective degrees of freedom\" as the correct measure of complexity for penalized models, providing hands-on practice in deriving and computing the generalized AIC for a penalized Cox proportional hazards model .",
            "id": "4953129",
            "problem": "A hospital-based cohort study models right-censored time-to-event outcomes using the Cox proportional hazards model, where the hazard at time $t$ for a patient with covariate vector $x \\in \\mathbb{R}^{p}$ is $h(t \\mid x) = h_{0}(t) \\exp(x^{\\top} \\beta)$, with $h_{0}(t)$ an unspecified baseline hazard. Automated variable selection is performed by minimizing an information criterion across penalty strengths in a penalized partial likelihood fit. Consider a penalized fit with a quadratic penalty matrix and a tuning parameter that shrinks only a subset of coefficients, yielding a maximum penalized partial likelihood estimate $\\hat{\\beta}_{\\lambda}$ with the following characteristics:\n\n- The partial log-likelihood at $\\hat{\\beta}_{\\lambda}$ is $\\ell_{p}(\\hat{\\beta}_{\\lambda}) = -85.3$.\n- The observed information matrix for the partial likelihood at $\\hat{\\beta}_{\\lambda}$ is\n$$\nJ(\\hat{\\beta}_{\\lambda}) \\;=\\;\n\\begin{pmatrix}\n50 & 2 & 1 \\\\\n2 & 30 & 0 \\\\\n1 & 0 & 20\n\\end{pmatrix}.\n$$\n- The quadratic penalty is $\\frac{\\lambda}{2}\\,\\beta^{\\top} P \\beta$ with tuning parameter $\\lambda = 10$ and penalty matrix\n$$\nP \\;=\\;\n\\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 0\n\\end{pmatrix},\n$$\nso that the third coefficient is unpenalized.\n\nStarting from the information-theoretic basis of the Akaike Information Criterion (AIC) as an asymptotic unbiased estimator of the expected Kullback–Leibler discrepancy for maximum likelihood fits, and accounting for the semiparametric structure of the Cox model and the bias introduced by penalization, derive the appropriate AIC expression for a Cox model fitted by partial likelihood with a quadratic penalty. In particular, justify replacing the naive parameter count by an effective degrees of freedom term based on local curvature, and then compute the AIC for the given fit using the provided matrices.\n\nExpress your final numerical AIC rounded to four significant figures. No units are required.",
            "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and all necessary information is provided.\n\n**Step 1: Extract Givens**\n- Model: Cox proportional hazards model, $h(t \\mid x) = h_{0}(t) \\exp(x^{\\top} \\beta)$.\n- Estimation method: Penalized partial likelihood maximization.\n- Partial log-likelihood at the estimate: $\\ell_{p}(\\hat{\\beta}_{\\lambda}) = -85.3$.\n- Observed information matrix for the partial likelihood, evaluated at $\\hat{\\beta}_{\\lambda}$:\n$$\nJ(\\hat{\\beta}_{\\lambda}) =\n\\begin{pmatrix}\n50 & 2 & 1 \\\\\n2 & 30 & 0 \\\\\n1 & 0 & 20\n\\end{pmatrix}.\n$$\n- Penalty term: $\\frac{\\lambda}{2}\\,\\beta^{\\top} P \\beta$.\n- Tuning parameter: $\\lambda = 10$.\n- Penalty matrix:\n$$\nP =\n\\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 0\n\\end{pmatrix}.\n$$\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is valid. It is set within the standard, well-established framework of survival analysis and penalized regression. The Cox model, partial likelihood, quadratic penalties (ridge regression), and information criteria are all fundamental concepts in modern statistics. All necessary numerical values and matrices are provided, and their dimensions are consistent. The structure of the penalty matrix $P$ correctly reflects the statement that the third coefficient is unpenalized. The problem is objective, self-contained, and scientifically sound.\n\n**Step 3: Derivation and Solution**\nThe Akaike Information Criterion (AIC) is a widely used metric for model selection, founded on the principle of minimizing the expected Kullback–Leibler divergence between the fitted model and the true underlying data-generating process. For a model estimated via maximum likelihood, the standard AIC is defined as:\n$$\n\\text{AIC} = -2\\ell(\\hat{\\theta}) + 2k\n$$\nwhere $\\ell(\\hat{\\theta})$ is the maximized log-likelihood and $k$ is the number of estimated parameters. The term $2k$ serves as a penalty for model complexity, correcting for the optimistic bias of using the same data to both fit and evaluate the model.\n\nIn the context of the Cox proportional hazards model, the regression coefficients $\\beta$ are estimated by maximizing the partial log-likelihood, $\\ell_p(\\beta)$, rather than a full likelihood. For an unpenalized Cox model, the AIC is analogously defined as:\n$$\n\\text{AIC} = -2\\ell_p(\\hat{\\beta}) + 2p\n$$\nwhere $\\hat{\\beta}$ is the maximum partial likelihood estimate and $p$ is the number of covariates, which is the dimension of $\\beta$.\n\nThe problem specifies that the coefficients are estimated by maximizing a *penalized* partial log-likelihood:\n$$\n\\ell_{\\text{pen}}(\\beta; \\lambda) = \\ell_p(\\beta) - \\frac{\\lambda}{2} \\beta^{\\top} P \\beta\n$$\nThe resulting estimate, $\\hat{\\beta}_{\\lambda}$, is no longer the maximum partial likelihood estimate. The penalty term introduces bias in the coefficient estimates (shrinking them towards zero for the penalized components) but can reduce their variance, leading to better predictive performance. Consequently, the naive parameter count $p$ is no longer an accurate measure of the model's complexity or \"degrees of freedom.\" The penalty restricts the parameter space, so the effective number of parameters fitted is less than $p$.\n\nTo generalize the AIC for such penalized models, the parameter count $p$ is replaced by an \"effective degrees of freedom\" term, denoted $df_{\\lambda}$. This term quantifies the complexity of the fitted penalized model. For a quadratic penalty, a well-established result from statistical theory gives the effective degrees of freedom as:\n$$\ndf_{\\lambda} = \\text{tr}\\left( \\left( J(\\hat{\\beta}_{\\lambda}) + \\lambda P \\right)^{-1} J(\\hat{\\beta}_{\\lambda}) \\right)\n$$\nHere, $J(\\hat{\\beta}_{\\lambda}) = -\\frac{\\partial^2 \\ell_p(\\beta)}{\\partial \\beta \\partial \\beta^{\\top}} \\big|_{\\beta=\\hat{\\beta}_{\\lambda}}$ is the observed information matrix of the unpenalized partial likelihood, evaluated at the penalized estimate $\\hat{\\beta}_{\\lambda}$. The matrix $\\left( J(\\hat{\\beta}_{\\lambda}) + \\lambda P \\right)$ is the negative of the Hessian of the penalized partial log-likelihood, representing the total curvature of the objective function at the solution.\n\nThe generalized AIC for the penalized Cox model is therefore:\n$$\n\\text{AIC}_{\\lambda} = -2\\ell_p(\\hat{\\beta}_{\\lambda}) + 2df_{\\lambda}\n$$\nIt is crucial to note that the likelihood term in this formula is the *unpenalized* partial log-likelihood evaluated at the penalized estimate.\n\nWe now compute this value using the provided data.\nFirst, we construct the matrix $H = J(\\hat{\\beta}_{\\lambda}) + \\lambda P$:\n$$\n\\lambda P = 10 \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 10 & 0 & 0 \\\\ 0 & 10 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix}\n$$\n$$\nH = J(\\hat{\\beta}_{\\lambda}) + \\lambda P = \\begin{pmatrix} 50 & 2 & 1 \\\\ 2 & 30 & 0 \\\\ 1 & 0 & 20 \\end{pmatrix} + \\begin{pmatrix} 10 & 0 & 0 \\\\ 0 & 10 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 60 & 2 & 1 \\\\ 2 & 40 & 0 \\\\ 1 & 0 & 20 \\end{pmatrix}\n$$\nNext, we must find the inverse of $H$. The determinant of $H$ is:\n$$\n\\det(H) = 60(40 \\cdot 20 - 0 \\cdot 0) - 2(2 \\cdot 20 - 0 \\cdot 1) + 1(2 \\cdot 0 - 40 \\cdot 1)\n$$\n$$\n\\det(H) = 60(800) - 2(40) - 40 = 48000 - 80 - 40 = 47880\n$$\nThe adjugate of $H$, which is the transpose of its cofactor matrix, is:\n$$\n\\text{adj}(H) = \\begin{pmatrix}\n+(40 \\cdot 20 - 0 \\cdot 0) & -(2 \\cdot 20 - 1 \\cdot 0) & +(2 \\cdot 0 - 40 \\cdot 1) \\\\\n-(2 \\cdot 20 - 0 \\cdot 1) & +(60 \\cdot 20 - 1 \\cdot 1) & -(60 \\cdot 0 - 2 \\cdot 1) \\\\\n+(2 \\cdot 0 - 40 \\cdot 1) & -(60 \\cdot 0 - 2 \\cdot 1) & +(60 \\cdot 40 - 2 \\cdot 2)\n\\end{pmatrix}^{\\top}\n$$\n$$\n\\text{adj}(H) = \\begin{pmatrix} 800 & -40 & -40 \\\\ -40 & 1199 & 2 \\\\ -40 & 2 & 2396 \\end{pmatrix}^{\\top} = \\begin{pmatrix} 800 & -40 & -40 \\\\ -40 & 1199 & 2 \\\\ -40 & 2 & 2396 \\end{pmatrix}\n$$\nThe inverse is $H^{-1} = \\frac{1}{47880} \\text{adj}(H)$.\nNow we compute the matrix product $M = H^{-1} J(\\hat{\\beta}_{\\lambda})$:\n$$\nM = \\frac{1}{47880} \\begin{pmatrix} 800 & -40 & -40 \\\\ -40 & 1199 & 2 \\\\ -40 & 2 & 2396 \\end{pmatrix} \\begin{pmatrix} 50 & 2 & 1 \\\\ 2 & 30 & 0 \\\\ 1 & 0 & 20 \\end{pmatrix}\n$$\nThe effective degrees of freedom is the trace of this matrix, $df_{\\lambda} = \\text{tr}(M) = M_{11} + M_{22} + M_{33}$. We only need to compute the diagonal elements:\n$$\nM_{11} = \\frac{1}{47880} (800 \\cdot 50 - 40 \\cdot 2 - 40 \\cdot 1) = \\frac{40000 - 80 - 40}{47880} = \\frac{39880}{47880}\n$$\n$$\nM_{22} = \\frac{1}{47880} (-40 \\cdot 2 + 1199 \\cdot 30 + 2 \\cdot 0) = \\frac{-80 + 35970}{47880} = \\frac{35890}{47880}\n$$\n$$\nM_{33} = \\frac{1}{47880} (-40 \\cdot 1 + 2 \\cdot 0 + 2396 \\cdot 20) = \\frac{-40 + 47920}{47880} = \\frac{47880}{47880} = 1\n$$\nThe value $M_{33} = 1$ is expected, as the third coefficient is unpenalized and thus contributes exactly one full degree of freedom.\nThe total effective degrees of freedom is the sum of these diagonal elements:\n$$\ndf_{\\lambda} = \\frac{39880}{47880} + \\frac{35890}{47880} + \\frac{47880}{47880} = \\frac{39880 + 35890 + 47880}{47880} = \\frac{123650}{47880}\n$$\nNumerically, $df_{\\lambda} \\approx 2.582665$. As expected, this is less than the naive parameter count of $p=3$.\n\nFinally, we compute the AIC:\n$$\n\\text{AIC}_{\\lambda} = -2\\ell_p(\\hat{\\beta}_{\\lambda}) + 2df_{\\lambda} = -2(-85.3) + 2\\left(\\frac{123650}{47880}\\right)\n$$\n$$\n\\text{AIC}_{\\lambda} = 170.6 + 2(2.582665...) = 170.6 + 5.16533... = 175.76533...\n$$\nRounding to four significant figures, we get $175.8$.",
            "answer": "$$\\boxed{175.8}$$"
        },
        {
            "introduction": "Instead of relying on a single \"best\" model identified through a selection procedure, modern statistical practice often favors ensembling, which combines predictions from multiple models to improve accuracy and robustness. Stacking is a principled ensembling method that finds the optimal linear combination of predictions from a library of candidate models by minimizing cross-validated error. This practice guides you through the fundamental least-squares optimization required to derive these optimal stacking weights, offering a clear, first-principles look at the mechanics of this powerful technique .",
            "id": "4953094",
            "problem": "In a clinical outcomes study, a hospital develops a library of $m$ prognostic models for a continuous endpoint $Y$ by applying automated variable selection procedures on a training cohort of $n$ patients. Specifically, the library includes models produced by the Least Absolute Shrinkage and Selection Operator (Lasso) at several penalty levels and forward stepwise selection at several model sizes. Using $K$-fold cross-validation, the hospital computes out-of-fold predictions for each model: for each patient $i \\in \\{1,\\dots,n\\}$ and model $j \\in \\{1,\\dots,m\\}$, the quantity $\\hat{y}_{ij}$ is the prediction for $Y_i$ produced by fitting model $j$ on folds that do not include patient $i$. Let $Z \\in \\mathbb{R}^{n \\times m}$ be the matrix with entries $Z_{ij} = \\hat{y}_{ij}$, and let $Y \\in \\mathbb{R}^{n}$ be the vector of observed outcomes.\n\nTo combine the library, consider a stacked predictor that forms a linear combination of model predictions with weights $w \\in \\mathbb{R}^{m}$ subject to the simplex-averaging constraint $\\sum_{j=1}^{m} w_j = 1$. The stacking weights are chosen to minimize the cross-validated mean squared error, that is, to solve the optimization problem\n$$\n\\min_{w \\in \\mathbb{R}^{m}} \\; \\frac{1}{n} \\sum_{i=1}^{n} \\left(Y_i - \\sum_{j=1}^{m} w_j Z_{ij}\\right)^{2} \\quad \\text{subject to} \\quad \\sum_{j=1}^{m} w_j = 1.\n$$\nAssume that the columns of $Z$ are linearly independent so that $Z^{\\top} Z$ is invertible.\n\nStarting from first principles in convex optimization and least squares, derive a closed-form expression for the optimal weight vector in terms of $Z$ and $Y$, under the above single linear equality constraint and without imposing any nonnegativity constraints on the weights. In addition, justify whether the optimization problem is convex and state the condition under which the minimizer is unique. Express your final answer as a single analytic expression for the optimal weight vector in terms of $Z$ and $Y$ using standard linear algebra operations. Do not introduce any numerical approximations. Your answer must be a single expression with no units.",
            "solution": "The problem requires the derivation of a closed-form expression for the optimal weights in a constrained least squares problem, along with a justification of the convexity of the problem and the uniqueness of its solution.\n\nFirst, we formalize the problem using matrix notation. Let $w \\in \\mathbb{R}^{m}$ be the vector of weights, $Y \\in \\mathbb{R}^{n}$ be the vector of observed outcomes, and $Z \\in \\mathbb{R}^{n \\times m}$ be the matrix of out-of-fold predictions. The optimization problem is given by:\n$$\n\\min_{w \\in \\mathbb{R}^{m}} \\; \\frac{1}{n} \\sum_{i=1}^{n} \\left(Y_i - \\sum_{j=1}^{m} Z_{ij} w_j\\right)^{2} \\quad \\text{subject to} \\quad \\sum_{j=1}^{m} w_j = 1.\n$$\nThe summation term $\\sum_{i=1}^{n} \\left(Y_i - \\sum_{j=1}^{m} Z_{ij} w_j\\right)^{2}$ is the squared Euclidean norm of the vector $Y - Zw$, denoted as $\\|Y - Zw\\|_2^2$. The constraint can be written in vector form as $\\mathbf{1}^\\top w = 1$, where $\\mathbf{1}$ is a column vector of $m$ ones. The constant factor $\\frac{1}{n}$ does not affect the location of the minimum, so we can equivalently minimize the objective function $f(w) = \\|Y - Zw\\|_2^2$. The problem is thus:\n$$ \\min_{w \\in \\mathbb{R}^{m}} f(w) = \\|Y - Zw\\|_2^2 \\quad \\text{subject to} \\quad \\mathbf{1}^\\top w = 1. $$\n\nNext, we address the convexity of the problem and the uniqueness of the solution. The objective function can be expanded as:\n$$ f(w) = (Y - Zw)^\\top(Y - Zw) = Y^\\top Y - 2Y^\\top Z w + w^\\top Z^\\top Z w. $$\nThis is a quadratic function of the weight vector $w$. To determine its convexity, we compute its Hessian matrix with respect to $w$. The gradient of $f(w)$ is:\n$$ \\nabla_w f(w) = -2Z^\\top Y + 2Z^\\top Z w. $$\nThe Hessian matrix is the derivative of the gradient with respect to $w^\\top$:\n$$ \\nabla_w^2 f(w) = 2Z^\\top Z. $$\nThe problem states that the columns of the matrix $Z$ are linearly independent. By definition, this means that for any non-zero vector $v \\in \\mathbb{R}^m$, the product $Zv$ is a non-zero vector in $\\mathbb{R}^n$. Consequently, the quadratic form $v^\\top (Z^\\top Z) v$ can be written as $(Zv)^\\top(Zv) = \\|Zv\\|_2^2$, which is strictly positive for any $v \\neq 0$. This proves that the matrix $Z^\\top Z$ is positive definite.\nSince the Hessian matrix $2Z^\\top Z$ is positive definite for all $w$, the objective function $f(w)$ is strictly convex. The constraint set $\\{w \\in \\mathbb{R}^m \\mid \\mathbf{1}^\\top w = 1\\}$ is an affine subspace, which is a convex set. A strictly convex function minimized over a non-empty convex set has a unique minimum. Therefore, the stated problem has a unique solution. The condition for this uniqueness is the linear independence of the columns of $Z$, which was given.\n\nTo find the optimal weight vector $w$, we use the method of Lagrange multipliers. The Lagrangian function $\\mathcal{L}(w, \\lambda)$ is constructed by adding the constraint multiplied by a Lagrange multiplier $\\lambda$:\n$$ \\mathcal{L}(w, \\lambda) = (Y - Zw)^\\top(Y - Zw) + \\lambda(\\mathbf{1}^\\top w - 1). $$\nThe optimal solution must satisfy the first-order Karush-Kuhn-Tucker (KKT) conditions, which are found by setting the partial derivatives of the Lagrangian with respect to $w$ and $\\lambda$ to zero.\n\nThe partial derivative with respect to the vector $w$ is:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial w} = 2Z^\\top Z w - 2Z^\\top Y + \\lambda\\mathbf{1}. $$\nSetting this to zero yields:\n$$ 2Z^\\top Z w - 2Z^\\top Y + \\lambda\\mathbf{1} = 0 \\implies Z^\\top Z w = Z^\\top Y - \\frac{\\lambda}{2}\\mathbf{1}. $$\nThe problem states that $Z^\\top Z$ is invertible. Thus, we can solve for $w$ in terms of $\\lambda$:\n$$ w = (Z^\\top Z)^{-1} \\left(Z^\\top Y - \\frac{\\lambda}{2}\\mathbf{1}\\right) = (Z^\\top Z)^{-1}Z^\\top Y - \\frac{\\lambda}{2}(Z^\\top Z)^{-1}\\mathbf{1}. \\quad (1) $$\n\nThe partial derivative with respect to the scalar $\\lambda$ is:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial \\lambda} = \\mathbf{1}^\\top w - 1. $$\nSetting this to zero reproduces the original constraint:\n$$ \\mathbf{1}^\\top w = 1. \\quad (2) $$\n\nNow, we substitute the expression for $w$ from equation $(1)$ into the constraint equation $(2)$ to solve for $\\lambda$:\n$$ \\mathbf{1}^\\top \\left( (Z^\\top Z)^{-1}Z^\\top Y - \\frac{\\lambda}{2}(Z^\\top Z)^{-1}\\mathbf{1} \\right) = 1. $$\nUsing the distributive property of matrix multiplication:\n$$ \\mathbf{1}^\\top (Z^\\top Z)^{-1}Z^\\top Y - \\frac{\\lambda}{2} \\mathbf{1}^\\top (Z^\\top Z)^{-1}\\mathbf{1} = 1. $$\nWe can now isolate the term containing $\\lambda$:\n$$ \\frac{\\lambda}{2} \\mathbf{1}^\\top (Z^\\top Z)^{-1}\\mathbf{1} = \\mathbf{1}^\\top (Z^\\top Z)^{-1}Z^\\top Y - 1. $$\nSolving for $\\frac{\\lambda}{2}$ gives:\n$$ \\frac{\\lambda}{2} = \\frac{\\mathbf{1}^\\top (Z^\\top Z)^{-1}Z^\\top Y - 1}{\\mathbf{1}^\\top (Z^\\top Z)^{-1}\\mathbf{1}}. $$\nNote that the denominator $\\mathbf{1}^\\top (Z^\\top Z)^{-1}\\mathbf{1}$ is a non-zero scalar because $(Z^\\top Z)^{-1}$ is positive definite.\n\nFinally, we substitute this expression for $\\frac{\\lambda}{2}$ back into equation $(1)$ to obtain the closed-form solution for the optimal weight vector $w$:\n$$ w = (Z^\\top Z)^{-1}Z^\\top Y - \\left( \\frac{\\mathbf{1}^\\top (Z^\\top Z)^{-1}Z^\\top Y - 1}{\\mathbf{1}^\\top (Z^\\top Z)^{-1}\\mathbf{1}} \\right) (Z^\\top Z)^{-1}\\mathbf{1}. $$\nThis expression provides the optimal weight vector $w$ entirely in terms of the given quantities $Z$ and $Y$, and the vector of ones $\\mathbf{1}$, using standard linear algebra operations as required.",
            "answer": "$$\n\\boxed{(Z^\\top Z)^{-1}Z^\\top Y - \\frac{\\mathbf{1}^\\top(Z^\\top Z)^{-1}Z^\\top Y - 1}{\\mathbf{1}^\\top(Z^\\top Z)^{-1}\\mathbf{1}} (Z^\\top Z)^{-1}\\mathbf{1}}\n$$"
        }
    ]
}