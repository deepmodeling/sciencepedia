{
    "hands_on_practices": [
        {
            "introduction": "最大似然估计量 (MLE) 因其优良的渐近性质（如一致性和有效性）而备受推崇。然而，对于实践者而言，理解它们在有限样本下并不总是无偏的，这一点至关重要。本练习 () 通过推导正态分布方差 $\\sigma^2$ 的最大似然估计，并证明其会系统性地低估真实值，为这一事实提供了一个经典而基础的证明。通过完成此推导，您将更深刻地理解估计量性质的细微之处，以及贝塞尔校正等修正方法的原理。",
            "id": "4922788",
            "problem": "一个生物统计学团队在标准化条件下测量了 $n$ 个受试者的连续生物标志物。假设测量值 $X_{1}, X_{2}, \\dots, X_{n}$ 独立同分布于正态分布 $\\mathcal{N}(\\mu, \\sigma^{2})$，其中 $\\mu$ 和 $\\sigma^{2}$ 均为未知。使用最大似然估计（MLE）的原理，从样本的似然函数推导出 $\\sigma^{2}$ 的最大似然估计量，然后在该模型下确定其期望。使用相对于总体均值的总变异分解作为基本方法，并结合关于独立同分布正态随机变量的和与均值的期望和方差的核心性质。明确说明 $\\sigma^{2}$ 的最大似然估计量是否有偏，并将其偏差量化为 $n$ 和 $\\sigma^{2}$ 的函数。最后，在偏差方面，将此最大似然估计量与基于除以 $n-1$ 的估计量进行对比。以 $n$ 和 $\\sigma^{2}$ 的单一闭式表达式报告 $\\sigma^{2}$ 的最大似然估计量的偏差。不需要进行数值计算，也不需要四舍五入。你的最终答案必须是单一的解析表达式。",
            "solution": "该问题要求推导正态分布方差 $\\sigma^2$ 的最大似然估计量（MLE），计算其期望和偏差，并与标准无偏估计量进行比较。样本由来自正态分布 $\\mathcal{N}(\\mu, \\sigma^2)$ 的 $n$ 个独立同分布（i.i.d.）的随机变量 $X_1, X_2, \\dots, X_n$ 组成，其中均值 $\\mu$ 和方差 $\\sigma^2$ 均为未知。\n\n首先，我们建立似然函数。单个观测值 $X_i$ 的概率密度函数（PDF）由下式给出：\n$$f(x_i | \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right)$$\n由于观测值是独立同分布的，联合概率密度函数，即似然函数 $L(\\mu, \\sigma^2)$，是个体概率密度函数的乘积：\n$$L(\\mu, \\sigma^2) = \\prod_{i=1}^{n} f(x_i | \\mu, \\sigma^2) = \\prod_{i=1}^{n} \\frac{1}{(2\\pi\\sigma^2)^{1/2}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right)$$\n这可以简化为：\n$$L(\\mu, \\sigma^2) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(x_i - \\mu)^2\\right)$$\n为了找到最大似然估计量，处理似然函数的自然对数，即对数似然函数 $\\ell(\\mu, \\sigma^2)$，更为方便：\n$$\\ell(\\mu, \\sigma^2) = \\ln L(\\mu, \\sigma^2) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(x_i - \\mu)^2$$\n最大似然估计量是使 $\\ell(\\mu, \\sigma^2)$ 最大化的 $\\mu$ 和 $\\sigma^2$ 的值。我们通过求关于 $\\mu$ 和 $\\sigma^2$ 的偏导数，将它们设为 $0$，然后求解所得的方程组来找到这些值。\n\n关于 $\\mu$ 的偏导数是：\n$$\\frac{\\partial \\ell}{\\partial \\mu} = -\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n} 2(x_i - \\mu)(-1) = \\frac{1}{\\sigma^2}\\sum_{i=1}^{n}(x_i - \\mu)$$\n将其设为 $0$ 可得 $\\sum_{i=1}^{n}(x_i - \\mu) = 0$，这意味着 $n\\mu = \\sum_{i=1}^{n}x_i$。因此，$\\mu$ 的最大似然估计量是样本均值，$\\hat{\\mu} = \\frac{1}{n}\\sum_{i=1}^{n}X_i = \\bar{X}$。\n\n接下来，我们求关于 $\\sigma^2$ 的偏导数。为简单起见，令 $\\theta = \\sigma^2$。\n$$\\ell(\\mu, \\theta) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\theta) - \\frac{1}{2\\theta}\\sum_{i=1}^{n}(x_i - \\mu)^2$$\n$$\\frac{\\partial \\ell}{\\partial \\theta} = -\\frac{n}{2\\theta} + \\frac{1}{2\\theta^2}\\sum_{i=1}^{n}(x_i - \\mu)^2$$\n将偏导数设为 $0$ 并代入 $\\mu$ 的最大似然估计量 $\\hat{\\mu} = \\bar{X}$，我们得到：\n$$-\\frac{n}{2\\hat{\\theta}} + \\frac{1}{2\\hat{\\theta}^2}\\sum_{i=1}^{n}(x_i - \\bar{X})^2 = 0$$\n乘以 $2\\hat{\\theta}^2$（假设 $\\hat{\\theta} > 0$）得到：\n$$-n\\hat{\\theta} + \\sum_{i=1}^{n}(x_i - \\bar{X})^2 = 0$$\n解出 $\\hat{\\theta}$，我们得到 $\\sigma^2$ 的最大似然估计量：\n$$\\hat{\\sigma}^2_{MLE} = \\frac{1}{n}\\sum_{i=1}^{n}(X_i - \\bar{X})^2$$\n\n现在，我们必须确定这个估计量的期望 $E[\\hat{\\sigma}^2_{MLE}]$，以评估其偏差。题目提示使用总平方和的分解。我们可以将离均差平方和重写如下：\n$$\\sum_{i=1}^{n}(X_i - \\bar{X})^2 = \\sum_{i=1}^{n}((X_i - \\mu) - (\\bar{X} - \\mu))^2$$\n展开此表达式得到：\n$$= \\sum_{i=1}^{n}\\left[(X_i - \\mu)^2 - 2(X_i - \\mu)(\\bar{X} - \\mu) + (\\bar{X} - \\mu)^2\\right]$$\n$$= \\sum_{i=1}^{n}(X_i - \\mu)^2 - 2(\\bar{X} - \\mu)\\sum_{i=1}^{n}(X_i - \\mu) + \\sum_{i=1}^{n}(\\bar{X} - \\mu)^2$$\n认识到 $\\sum_{i=1}^{n}(X_i - \\mu) = n(\\bar{X} - \\mu)$，我们将其代入中间项：\n$$= \\sum_{i=1}^{n}(X_i - \\mu)^2 - 2(\\bar{X} - \\mu) \\cdot n(\\bar{X} - \\mu) + n(\\bar{X} - \\mu)^2$$\n$$= \\sum_{i=1}^{n}(X_i - \\mu)^2 - 2n(\\bar{X} - \\mu)^2 + n(\\bar{X} - \\mu)^2$$\n$$= \\sum_{i=1}^{n}(X_i - \\mu)^2 - n(\\bar{X} - \\mu)^2$$\n现在我们对这个表达式取期望：\n$$E\\left[\\sum_{i=1}^{n}(X_i - \\bar{X})^2\\right] = E\\left[\\sum_{i=1}^{n}(X_i - \\mu)^2\\right] - E\\left[n(\\bar{X} - \\mu)^2\\right]$$\n我们分别计算每一项的期望。对于第一项，根据方差的定义，$E[(X_i - \\mu)^2] = \\text{Var}(X_i) = \\sigma^2$。根据期望的线性性质：\n$$E\\left[\\sum_{i=1}^{n}(X_i - \\mu)^2\\right] = \\sum_{i=1}^{n}E[(X_i - \\mu)^2] = \\sum_{i=1}^{n}\\sigma^2 = n\\sigma^2$$\n对于第二项，我们首先求样本均值 $\\bar{X}$ 的方差。由于 $X_i$ 是独立的，$\\text{Var}(\\bar{X}) = \\text{Var}(\\frac{1}{n}\\sum X_i) = \\frac{1}{n^2}\\sum\\text{Var}(X_i) = \\frac{1}{n^2}(n\\sigma^2) = \\frac{\\sigma^2}{n}$。第二项的期望是：\n$$E\\left[n(\\bar{X} - \\mu)^2\\right] = nE[(\\bar{X} - E[\\bar{X}])^2] = n\\text{Var}(\\bar{X}) = n\\left(\\frac{\\sigma^2}{n}\\right) = \\sigma^2$$\n将这些结果代回期望平方和的方程中：\n$$E\\left[\\sum_{i=1}^{n}(X_i - \\bar{X})^2\\right] = n\\sigma^2 - \\sigma^2 = (n-1)\\sigma^2$$\n现在我们可以计算 $\\sigma^2$ 的最大似然估计量的期望：\n$$E[\\hat{\\sigma}^2_{MLE}] = E\\left[\\frac{1}{n}\\sum_{i=1}^{n}(X_i - \\bar{X})^2\\right] = \\frac{1}{n}E\\left[\\sum_{i=1}^{n}(X_i - \\bar{X})^2\\right] = \\frac{1}{n}(n-1)\\sigma^2 = \\frac{n-1}{n}\\sigma^2$$\n参数 $\\theta$ 的估计量 $\\hat{\\theta}$ 的偏差定义为 $\\text{Bias}(\\hat{\\theta}) = E[\\hat{\\theta}] - \\theta$。对于 $\\hat{\\sigma}^2_{MLE}$，偏差为：\n$$\\text{Bias}(\\hat{\\sigma}^2_{MLE}) = E[\\hat{\\sigma}^2_{MLE}] - \\sigma^2 = \\frac{n-1}{n}\\sigma^2 - \\sigma^2 = \\left(\\frac{n-1}{n} - 1\\right)\\sigma^2 = \\left(\\frac{n-1-n}{n}\\right)\\sigma^2 = -\\frac{1}{n}\\sigma^2$$\n由于对于任何有限的 $n$，偏差都不为零，所以 $\\sigma^2$ 的最大似然估计量是一个有偏估计量。它系统地低估了真实方差。\n\n相比之下，样本方差（通常表示为 $S^2$）使用 $n-1$ 作为分母：\n$$S^2 = \\frac{1}{n-1}\\sum_{i=1}^{n}(X_i - \\bar{X})^2$$\n让我们计算它的期望：\n$$E[S^2] = E\\left[\\frac{1}{n-1}\\sum_{i=1}^{n}(X_i - \\bar{X})^2\\right] = \\frac{1}{n-1}E\\left[\\sum_{i=1}^{n}(X_i - \\bar{X})^2\\right]$$\n使用我们之前的结果 $E[\\sum_{i=1}^{n}(X_i - \\bar{X})^2] = (n-1)\\sigma^2$：\n$$E[S^2] = \\frac{1}{n-1}(n-1)\\sigma^2 = \\sigma^2$$\n$S^2$ 的偏差是 $\\text{Bias}(S^2) = E[S^2] - \\sigma^2 = \\sigma^2 - \\sigma^2 = 0$。因此，$S^2$ 是 $\\sigma^2$ 的一个无偏估计量。将 $S^2$ 与 $\\hat{\\sigma}^2_{MLE}$ 联系起来的因子 $\\frac{n}{n-1}$ 被称为贝塞尔校正（Bessel's correction）。\n\n问题要求的是 $\\sigma^2$ 的最大似然估计量的偏差，我们已经推导出其值为 $-\\frac{1}{n}\\sigma^2$。",
            "answer": "$$\\boxed{-\\frac{\\sigma^2}{n}}$$"
        },
        {
            "introduction": "除了点估计，似然框架最强大的应用之一是在假设检验中。似然比检验 (LRT) 为比较嵌套统计模型提供了一种通用且有原则的方法。本练习 () 将引导您推导似然比检验统计量，并将其应用于生物统计学中的一个常见问题：检验泊松分布的率参数 $\\lambda$。这个练习巩固了最大似然估计与正式统计推断之间的联系，并最终应用了威尔克斯定理。",
            "id": "4922734",
            "problem": "一家医院的感染控制单位监测了连续 $15$ 周每周新增的医院获得性感染（院内感染）计数。假设每周的计数 $X_{1}, \\ldots, X_{n}$ 独立同分布于一个率参数为 $\\lambda \\in (0,\\infty)$ 的泊松分布。观测到的计数（按时间顺序）为 $2, 0, 3, 1, 4, 2, 5, 3, 1, 2, 2, 4, 3, 0, 1$。该单位希望评估基线率是否等于每周 $2$ 例感染的参考值 $\\lambda_{0} = 2$。\n\n任务：\n1) 仅从似然函数 $L(\\theta)$、对数似然函数 $\\ell(\\theta) = \\ln L(\\theta)$、最大似然估计量 $\\hat{\\theta}$、原假设参数子空间 $\\Theta_{0} \\subset \\Theta$ 下的约束最大似然估计量 $\\hat{\\theta}_{0}$ 以及似然比 $\\Lambda = \\dfrac{\\sup_{\\theta \\in \\Theta_{0}} L(\\theta)}{\\sup_{\\theta \\in \\Theta} L(\\theta)}$ 的定义出发，推导似然比检验统计量，用备择假设下和原假设下的最大化对数似然之差来表示。\n2) 陈述威尔克斯定理（Wilks’ theorem），包括原假设下似然比统计量的渐近分布以及其自由度是如何确定的。\n3) 将问题具体化到泊松模型，用于检验 $H_{0}: \\lambda = \\lambda_{0}$ 与 $H_{1}: \\lambda \\neq \\lambda_{0}$。推导似然比统计量的闭式表达式，用 $n$、样本均值 $\\bar{X}$ 和 $\\lambda_{0}$ 表示。然后使用 $\\lambda_{0} = 2$ 和 $n = 15$ 对给定数据计算其数值。使用自然对数，并将最终数值答案四舍五入到四位有效数字。最终数值答案不需要单位。",
            "solution": "### 任务1：似然比检验统计量的推导\n\n我们被要求从基本定义出发，推导似然比检验统计量的形式。\n\n令 $\\theta$ 为参数空间 $\\Theta$ 中的一个参数。令 $L(\\theta)$ 为给定数据集的似然函数。对数似然函数定义为 $\\ell(\\theta) = \\ln L(\\theta)$。\n\n对于原假设 $H_0: \\theta \\in \\Theta_0$ 和备择假设 $H_1: \\theta \\in \\Theta \\setminus \\Theta_0$（其中 $\\Theta_0$ 是 $\\Theta$ 的一个子集），似然比定义为：\n$$\n\\Lambda = \\frac{\\sup_{\\theta \\in \\Theta_{0}} L(\\theta)}{\\sup_{\\theta \\in \\Theta} L(\\theta)}\n$$\n在整个参数空间 $\\Theta$ 上使似然最大化的参数值是最大似然估计量（MLE），记为 $\\hat{\\theta}$。因此，分母是在 MLE 处求得的似然值：\n$$\n\\sup_{\\theta \\in \\Theta} L(\\theta) = L(\\hat{\\theta})\n$$\n在约束参数空间 $\\Theta_0$ 上使似然最大化的参数值是约束最大似然估计量，记为 $\\hat{\\theta}_0$。因此，分子是在约束 MLE 处求得的似然值：\n$$\n\\sup_{\\theta \\in \\Theta_{0}} L(\\theta) = L(\\hat{\\theta}_{0})\n$$\n将这些代入似然比的定义中，得到：\n$$\n\\Lambda = \\frac{L(\\hat{\\theta}_0)}{L(\\hat{\\theta})}\n$$\n似然比检验统计量，通常用 $D$ 或 $-2 \\ln \\Lambda$ 表示，定义为：\n$$\nD = -2 \\ln \\Lambda\n$$\n代入 $\\Lambda$ 的表达式：\n$$\nD = -2 \\ln \\left( \\frac{L(\\hat{\\theta}_0)}{L(\\hat{\\theta})} \\right)\n$$\n使用对数的性质 $\\ln(a/b) = \\ln(a) - \\ln(b)$，我们得到：\n$$\nD = -2 \\left[ \\ln(L(\\hat{\\theta}_0)) - \\ln(L(\\hat{\\theta})) \\right]\n$$\n回顾对数似然函数的定义 $\\ell(\\theta) = \\ln(L(\\theta))$，我们可以将其写为：\n$$\nD = -2 \\left[ \\ell(\\hat{\\theta}_0) - \\ell(\\hat{\\theta}) \\right]\n$$\n将因子 $-2$ 分配进去，我们得到所求的表达式：\n$$\nD = 2 \\left[ \\ell(\\hat{\\theta}) - \\ell(\\hat{\\theta}_0) \\right]\n$$\n这表明似然比检验统计量是在备择假设下（即在整个参数空间上）最大化的对数似然与在原假设下（即在约束参数空间上）最大化的对数似然之差的两倍。\n\n### 任务2：威尔克斯定理的陈述\n\n威尔克斯定理描述了似然比检验统计量的渐近分布。\n\n设 $X_1, \\ldots, X_n$ 是来自一个概率分布的独立同分布（i.i.d.）随机样本，其参数向量 $\\theta$ 属于参数空间 $\\Theta \\subseteq \\mathbb{R}^k$。考虑检验原假设 $H_0: \\theta \\in \\Theta_0$ 与备择假设 $H_1: \\theta \\in \\Theta \\setminus \\Theta_0$。假设原假设参数空间 $\\Theta_0$ 是 $\\Theta$ 的一个子空间，由对 $\\theta$ 的参数施加的 $r$ 个独立约束定义。这意味着 $\\Theta$ 的维度是 $k$，而 $\\Theta_0$ 的维度是 $k-r$。\n\n设 $D = -2 \\ln \\Lambda = 2(\\ell(\\hat{\\theta}) - \\ell(\\hat{\\theta}_0))$ 为似然比检验统计量，其中 $\\hat{\\theta}$ 是 $\\theta$ 在 $\\Theta$ 中的最大似然估计量，而 $\\hat{\\theta}_0$ 是 $\\theta$ 在 $\\Theta_0$ 中的最大似然估计量。\n\n在某些正则性条件下（这些条件对泊松模型和许多其他标准分布都成立），威尔克斯定理指出，随着样本量 $n$ 趋于无穷大，在原假设 $H_0$ 下，统计量 $D$ 的分布收敛于一个卡方（$\\chi^2$）分布。这个极限卡方分布的自由度等于参数空间 $\\Theta$ 和 $\\Theta_0$ 的维度之差。\n$$\n\\text{自由度, } \\nu = \\dim(\\Theta) - \\dim(\\Theta_0) = k - (k-r) = r\n$$\n因此，在 $H_0$ 下，当 $n \\to \\infty$ 时，我们有依分布收敛：\n$$\nD \\xrightarrow{d} \\chi^2_r\n$$\n\n### 任务3：具体化到泊松模型和数值计算\n\n我们给定一个来自泊松($\\lambda$)分布的独立同分布观测样本 $X_1, \\ldots, X_n$。单个观测值 $X_i=x_i$ 的概率质量函数（PMF）为：\n$$\nP(X_i = x_i; \\lambda) = \\frac{\\lambda^{x_i} \\exp(-\\lambda)}{x_i!}\n$$\n整个样本 $x = (x_1, \\ldots, x_n)$ 的似然函数是各个PMF的乘积：\n$$\nL(\\lambda; x) = \\prod_{i=1}^{n} \\frac{\\lambda^{x_i} \\exp(-\\lambda)}{x_i!} = \\frac{\\lambda^{\\sum_{i=1}^{n} x_i} \\exp(-n\\lambda)}{\\prod_{i=1}^{n} x_i!}\n$$\n对数似然函数是：\n$$\n\\ell(\\lambda) = \\ln L(\\lambda) = \\left(\\sum_{i=1}^{n} x_i\\right) \\ln(\\lambda) - n\\lambda - \\sum_{i=1}^{n} \\ln(x_i!)\n$$\n我们要检验 $H_0: \\lambda = \\lambda_0$ 与 $H_1: \\lambda \\neq \\lambda_0$。完整参数空间是 $\\Theta = (0, \\infty)$，是一维的。原假设参数空间是 $\\Theta_0 = \\{\\lambda_0\\}$，是零维的。维度之差为 $1-0=1$，因此检验统计量将渐近服从 $\\chi^2_1$ 分布。\n\n首先，我们求无约束 MLE $\\hat{\\lambda}$。我们将 $\\ell(\\lambda)$ 对 $\\lambda$ 求导，并令结果为零：\n$$\n\\frac{d\\ell}{d\\lambda} = \\frac{\\sum x_i}{\\lambda} - n = 0 \\implies \\hat{\\lambda} = \\frac{\\sum x_i}{n} = \\bar{X}\n$$\n备择假设下的最大化对数似然是 $\\ell(\\hat{\\lambda}) = \\ell(\\bar{X})$。\n\n接下来，我们求约束 MLE $\\hat{\\lambda}_0$。在 $H_0$ 下，参数空间 $\\Theta_0$ 只包含一个点 $\\lambda_0$。因此，似然在此值处达到最大：\n$$\n\\hat{\\lambda}_0 = \\lambda_0\n$$\n原假设下的最大化对数似然是 $\\ell(\\hat{\\lambda}_0) = \\ell(\\lambda_0)$。\n\n现在，我们构建似然比统计量 $D = 2[\\ell(\\hat{\\lambda}) - \\ell(\\hat{\\lambda}_0)]$：\n$$\n\\ell(\\hat{\\lambda}) = \\left(\\sum X_i\\right) \\ln(\\bar{X}) - n\\bar{X} - \\sum \\ln(X_i!)\n$$\n$$\n\\ell(\\hat{\\lambda}_0) = \\left(\\sum X_i\\right) \\ln(\\lambda_0) - n\\lambda_0 - \\sum \\ln(X_i!)\n$$\n代入 $\\sum X_i = n\\bar{X}$：\n$$\n\\ell(\\hat{\\lambda}) = n\\bar{X} \\ln(\\bar{X}) - n\\bar{X} - \\sum \\ln(X_i!)\n$$\n$$\n\\ell(\\hat{\\lambda}_0) = n\\bar{X} \\ln(\\lambda_0) - n\\lambda_0 - \\sum \\ln(X_i!)\n$$\n差值为：\n$$\n\\ell(\\hat{\\lambda}) - \\ell(\\hat{\\lambda}_0) = (n\\bar{X} \\ln(\\bar{X}) - n\\bar{X}) - (n\\bar{X} \\ln(\\lambda_0) - n\\lambda_0)\n$$\n$$\n= n\\bar{X} \\ln(\\bar{X}) - n\\bar{X} - n\\bar{X} \\ln(\\lambda_0) + n\\lambda_0\n$$\n$$\n= n \\left[ \\bar{X} (\\ln(\\bar{X}) - \\ln(\\lambda_0)) - (\\bar{X} - \\lambda_0) \\right]\n$$\n$$\n= n \\left[ \\bar{X} \\ln\\left(\\frac{\\bar{X}}{\\lambda_0}\\right) - \\bar{X} + \\lambda_0 \\right]\n$$\n似然比统计量是 $D = 2(\\ell(\\hat{\\lambda}) - \\ell(\\hat{\\lambda}_0))$：\n$$\nD = 2n \\left[ \\lambda_0 - \\bar{X} + \\bar{X} \\ln\\left(\\frac{\\bar{X}}{\\lambda_0}\\right) \\right]\n$$\n这是该统计量用 $n$、$\\bar{X}$ 和 $\\lambda_0$ 表示的闭式表达式。\n\n最后，我们为给定数据计算其数值。\n数据为：$2, 0, 3, 1, 4, 2, 5, 3, 1, 2, 2, 4, 3, 0, 1$。\n样本量为 $n=15$。\n观测值之和为 $\\sum x_i = 2+0+3+1+4+2+5+3+1+2+2+4+3+0+1 = 33$。\n样本均值为 $\\bar{x} = \\frac{33}{15} = 2.2$。\n原假设值为 $\\lambda_0 = 2$。\n\n将这些值代入 $D$ 的表达式：\n$$\nD = 2(15) \\left[ 2 - 2.2 + 2.2 \\ln\\left(\\frac{2.2}{2}\\right) \\right]\n$$\n$$\nD = 30 \\left[ -0.2 + 2.2 \\ln(1.1) \\right]\n$$\n使用自然对数：\n$$\n\\ln(1.1) \\approx 0.0953101798\n$$\n$$\nD \\approx 30 \\left[ -0.2 + 2.2 \\times 0.0953101798 \\right]\n$$\n$$\nD \\approx 30 \\left[ -0.2 + 0.20968239556 \\right]\n$$\n$$\nD \\approx 30 \\left[ 0.00968239556 \\right]\n$$\n$$\nD \\approx 0.2904718668\n$$\n将最终数值答案四舍五入到四位有效数字，得到 $0.2905$。",
            "answer": "$$\\boxed{0.2905}$$"
        },
        {
            "introduction": "对于许多医学研究中至关重要的统计模型（如逻辑斯谛回归），最大似然估计无法以简洁的封闭形式解出。在这些情况下，我们必须求助于数值优化算法。这个动手实践问题 () 介绍了牛顿-拉夫逊方法，这是一种通过求解得分方程来寻找最大似然估计的强大迭代技术。您将首先推导该通用算法，然后将其逐步应用于逻辑斯谛回归模型，从而填补最大似然估计的理论概念与实际计算之间的鸿沟。",
            "id": "4969348",
            "problem": "一项临床研究调查了术后并发症的概率与一个标准化生物标志物的函数关系。令 $y_i \\in \\{0,1\\}$ 表示患者 $i$ 是否出现并发症，令 $x_i \\in \\mathbb{R}$ 表示其生物标志物的值。假设一个参数向量为 $\\theta \\in \\mathbb{R}^{p}$ 的参数模型通过最大似然估计 (MLE) 原理进行拟合，该原理选择 $\\theta$ 以最大化观测数据的对数似然 $l(\\theta)$。从得分函数 $U(\\theta)$ (即 $l(\\theta)$ 的梯度) 和 $U(\\theta)$ 的雅可比矩阵 $J(\\theta)$ 的定义出发，推导用于迭代求解得分方程 $U(\\theta)=0$ 以获得最大似然估计的牛顿-拉弗森更新法则。\n\n然后，将问题具体化到带单个预测变量的二元逻辑回归，其中对于每位患者\n$$\np_i(\\beta_0,\\beta_1) \\equiv \\Pr(y_i=1 \\mid x_i) = \\frac{1}{1+\\exp\\!\\big(-\\eta_i\\big)}, \\quad \\text{其中 } \\eta_i = \\beta_0 + \\beta_1 x_i,\n$$\n且独立观测值的对数似然为\n$$\nl(\\beta_0,\\beta_1) = \\sum_{i=1}^{n} \\left[ y_i \\ln p_i + (1-y_i)\\ln(1-p_i) \\right].\n$$\n使用基本原理，不引入任何快捷公式，推导此模型的得分向量 $U(\\beta_0,\\beta_1)$ 和雅可比矩阵 $J(\\beta_0,\\beta_1)$。\n\n最后，从初始猜测值 $\\beta^{(0)} = (\\beta_0^{(0)}, \\beta_1^{(0)})^{\\top} = (0,0)^{\\top}$ 开始，对以下包含 $n=6$ 名患者的数据集应用一次牛顿-拉弗森迭代：\n- 生物标志物值 $x = (-2,\\, -1,\\, 0,\\, 1,\\, 2,\\, 0)$，\n- 结局 $y = (0,\\, 0,\\, 0,\\, 1,\\, 1,\\, 0)$。\n\n计算一次牛顿-拉弗森步骤后更新的参数向量 $\\beta^{(1)}$。将你的最终答案表示为一个 $1 \\times 2$ 的行矩阵，并将每个元素四舍五入到四位有效数字。",
            "solution": "问题分为三个部分：首先，对最大似然估计 (MLE) 的牛顿-拉弗森更新法则进行一般性推导；其次，对二元逻辑回归模型的得分向量和雅可比矩阵进行具体推导；最后，将一次牛顿-拉弗森步骤应用于给定的数据集进行数值计算。\n\n### 第一部分：牛顿-拉弗森更新法则的推导\n\n最大似然估计 (MLE) 的目标是找到使对数似然函数 $l(\\theta)$ 最大化的参数向量 $\\theta$。这个最大值在对数似然函数的梯度为零的临界点处找到。得分函数 $U(\\theta)$ 定义为该梯度：\n$$\nU(\\theta) = \\nabla l(\\theta)\n$$\n因此，寻找最大似然估计 $\\hat\\theta$ 等价于求解得分方程 $U(\\hat\\theta) = 0$。\n\n牛顿-拉弗森方法是一种迭代求根算法。为了求解 $U(\\theta) = 0$，我们从一个初始猜测值 $\\theta^{(k)}$ 开始，旨在找到一个更好的近似值 $\\theta^{(k+1)}$。我们通过在 $\\theta^{(k)}$ 附近的一阶泰勒级数展开来近似 $U(\\theta)$：\n$$\nU(\\theta) \\approx U(\\theta^{(k)}) + J(\\theta^{(k)}) (\\theta - \\theta^{(k)})\n$$\n其中 $J(\\theta^{(k)})$ 是得分函数 $U(\\theta)$ 在 $\\theta^{(k)}$ 处的雅可比矩阵。雅可比矩阵的元素由 $J_{ij} = \\frac{\\partial U_i}{\\partial \\theta_j} = \\frac{\\partial^2 l}{\\partial \\theta_j \\partial \\theta_i}$ 给出。注意，这是对数似然函数 $l(\\theta)$ 的海森矩阵。\n\n为了找到下一个迭代值 $\\theta^{(k+1)}$，我们在近似式中令 $U(\\theta^{(k+1)}) = 0$：\n$$\n0 \\approx U(\\theta^{(k)}) + J(\\theta^{(k)}) (\\theta^{(k+1)} - \\theta^{(k)})\n$$\n重新整理此方程以求解更新步长 $(\\theta^{(k+1)} - \\theta^{(k)})$，我们得到：\n$$\nJ(\\theta^{(k)}) (\\theta^{(k+1)} - \\theta^{(k)}) = -U(\\theta^{(k)})\n$$\n假设雅可比矩阵 $J(\\theta^{(k)})$ 是可逆的，我们可以乘以它的逆矩阵 $[J(\\theta^{(k)})]^{-1}$：\n$$\n\\theta^{(k+1)} - \\theta^{(k)} = -[J(\\theta^{(k)})]^{-1} U(\\theta^{(k)})\n$$\n这就导出了用于迭代寻找最大似然估计的牛顿-拉弗森更新法则：\n$$\n\\theta^{(k+1)} = \\theta^{(k)} - [J(\\theta^{(k)})]^{-1} U(\\theta^{(k)})\n$$\n\n### 第二部分：逻辑回归的得分和雅可比矩阵\n\n我们现在具体化到参数为 $\\beta = (\\beta_0, \\beta_1)^{\\top}$ 的二元逻辑回归模型。对于 $n$ 个独立观测值，其对数似然由下式给出：\n$$\nl(\\beta_0, \\beta_1) = \\sum_{i=1}^{n} \\left[ y_i \\ln p_i + (1-y_i)\\ln(1-p_i) \\right]\n$$\n其中 $p_i = \\Pr(y_i=1 \\mid x_i) = \\frac{1}{1+\\exp(-\\eta_i)}$ 且 $\\eta_i = \\beta_0 + \\beta_1 x_i$。\n\n首先，我们求出 $p_i$ 关于 $\\eta_i$ 的一个关键导数：\n$$\n\\frac{\\partial p_i}{\\partial \\eta_i} = \\frac{\\partial}{\\partial \\eta_i} \\left( (1+\\exp(-\\eta_i))^{-1} \\right) = -1 \\cdot (1+\\exp(-\\eta_i))^{-2} \\cdot (-\\exp(-\\eta_i)) = \\frac{\\exp(-\\eta_i)}{(1+\\exp(-\\eta_i))^2}\n$$\n这可以重写为：\n$$\n\\frac{\\partial p_i}{\\partial \\eta_i} = \\frac{1}{1+\\exp(-\\eta_i)} \\cdot \\frac{\\exp(-\\eta_i)}{1+\\exp(-\\eta_i)} = p_i \\cdot \\left( \\frac{1+\\exp(-\\eta_i)-1}{1+\\exp(-\\eta_i)} \\right) = p_i \\cdot \\left( 1 - \\frac{1}{1+\\exp(-\\eta_i)} \\right) = p_i(1-p_i)\n$$\n\n得分向量 $U(\\beta_0, \\beta_1)$ 的分量为 $\\frac{\\partial l}{\\partial \\beta_0}$ 和 $\\frac{\\partial l}{\\partial \\beta_1}$。我们使用链式法则：\n$$\n\\frac{\\partial l}{\\partial \\beta_j} = \\sum_{i=1}^{n} \\frac{\\partial l_i}{\\partial \\beta_j} = \\sum_{i=1}^{n} \\frac{\\partial l_i}{\\partial p_i} \\frac{\\partial p_i}{\\partial \\eta_i} \\frac{\\partial \\eta_i}{\\partial \\beta_j}\n$$\n第 $i$ 个对数似然项关于 $p_i$ 的导数是：\n$$\n\\frac{\\partial l_i}{\\partial p_i} = \\frac{y_i}{p_i} - \\frac{1-y_i}{1-p_i} = \\frac{y_i(1-p_i) - (1-y_i)p_i}{p_i(1-p_i)} = \\frac{y_i - p_i}{p_i(1-p_i)}\n$$\n结合这些结果：\n$$\n\\frac{\\partial l_i}{\\partial \\beta_j} = \\left( \\frac{y_i - p_i}{p_i(1-p_i)} \\right) \\cdot (p_i(1-p_i)) \\cdot \\frac{\\partial \\eta_i}{\\partial \\beta_j} = (y_i - p_i) \\frac{\\partial \\eta_i}{\\partial \\beta_j}\n$$\n$\\eta_i$ 的导数是 $\\frac{\\partial \\eta_i}{\\partial \\beta_0} = 1$ 和 $\\frac{\\partial \\eta_i}{\\partial \\beta_1} = x_i$。\n得分向量的分量是：\n$$\nU_0 = \\frac{\\partial l}{\\partial \\beta_0} = \\sum_{i=1}^{n} (y_i - p_i) \\cdot 1 = \\sum_{i=1}^{n} (y_i - p_i)\n$$\n$$\nU_1 = \\frac{\\partial l}{\\partial \\beta_1} = \\sum_{i=1}^{n} (y_i - p_i) \\cdot x_i = \\sum_{i=1}^{n} x_i(y_i - p_i)\n$$\n所以得分向量是 $U(\\beta_0, \\beta_1) = \\begin{pmatrix} \\sum_{i=1}^{n} (y_i - p_i) \\\\ \\sum_{i=1}^{n} x_i(y_i - p_i) \\end{pmatrix}$。\n\n接下来，我们推导雅可比矩阵 $J(\\beta_0, \\beta_1)$，其元素为 $J_{jk} = \\frac{\\partial U_j}{\\partial \\beta_k} = \\frac{\\partial^2 l}{\\partial \\beta_k \\partial \\beta_j}$。\n$$\n\\frac{\\partial^2 l}{\\partial \\beta_k \\partial \\beta_j} = \\frac{\\partial}{\\partial \\beta_k} \\left( \\sum_{i=1}^{n} (y_i - p_i) \\frac{\\partial \\eta_i}{\\partial \\beta_j} \\right) = \\sum_{i=1}^{n} \\left( -\\frac{\\partial p_i}{\\partial \\beta_k} \\right) \\frac{\\partial \\eta_i}{\\partial \\beta_j}\n$$\n再次使用链式法则，$\\frac{\\partial p_i}{\\partial \\beta_k} = \\frac{\\partial p_i}{\\partial \\eta_i} \\frac{\\partial \\eta_i}{\\partial \\beta_k} = p_i(1-p_i) \\frac{\\partial \\eta_i}{\\partial \\beta_k}$。\n代入此式可得雅可比矩阵元素的一般形式：\n$$\nJ_{jk} = -\\sum_{i=1}^{n} p_i(1-p_i) \\frac{\\partial \\eta_i}{\\partial \\beta_k} \\frac{\\partial \\eta_i}{\\partial \\beta_j}\n$$\n现在我们计算具体的元素：\n$$\nJ_{00} = \\frac{\\partial^2 l}{\\partial \\beta_0^2} = -\\sum_{i=1}^{n} p_i(1-p_i) (1)(1) = -\\sum_{i=1}^{n} p_i(1-p_i)\n$$\n$$\nJ_{01} = \\frac{\\partial^2 l}{\\partial \\beta_1 \\partial \\beta_0} = -\\sum_{i=1}^{n} p_i(1-p_i) (x_i)(1) = -\\sum_{i=1}^{n} x_i p_i(1-p_i)\n$$\n根据对称性（克莱罗定理），$J_{10} = J_{01}$。\n$$\nJ_{11} = \\frac{\\partial^2 l}{\\partial \\beta_1^2} = -\\sum_{i=1}^{n} p_i(1-p_i) (x_i)(x_i) = -\\sum_{i=1}^{n} x_i^2 p_i(1-p_i)\n$$\n因此，雅可比矩阵是 $J(\\beta_0, \\beta_1) = \\begin{pmatrix} -\\sum p_i(1-p_i) & -\\sum x_i p_i(1-p_i) \\\\ -\\sum x_i p_i(1-p_i) & -\\sum x_i^2 p_i(1-p_i) \\end{pmatrix}$。\n\n### 第三部分：数值应用\n\n我们从 $\\beta^{(0)} = (\\beta_0^{(0)}, \\beta_1^{(0)})^{\\top} = (0,0)^{\\top}$ 开始，应用一次牛顿-拉弗森步骤。\n数据集为 $n=6$ 时的 $x = (-2, -1, 0, 1, 2, 0)$ 和 $y = (0, 0, 0, 1, 1, 0)$。\n\n首先，在 $\\beta^{(0)}$ 处评估概率。对于任意 $x_i$，$\\eta_i^{(0)} = \\beta_0^{(0)} + \\beta_1^{(0)} x_i = 0 + 0 \\cdot x_i = 0$。\n每位患者的概率为 $p_i^{(0)} = \\frac{1}{1+\\exp(-0)} = \\frac{1}{1+1} = 0.5$。\n\n接下来，计算得分向量 $U(\\beta^{(0)})$：\n我们需要以下总和：\n$\\sum_{i=1}^6 y_i = 0+0+0+1+1+0 = 2$。\n$\\sum_{i=1}^6 x_i = -2-1+0+1+2+0 = 0$。\n$\\sum_{i=1}^6 x_i y_i = (-2)(0) + (-1)(0) + (0)(0) + (1)(1) + (2)(1) + (0)(0) = 3$。\n得分向量的分量是：\n$U_0^{(0)} = \\sum_{i=1}^6 (y_i - p_i^{(0)}) = \\sum y_i - \\sum p_i^{(0)} = 2 - 6 \\times 0.5 = 2 - 3 = -1$。\n$U_1^{(0)} = \\sum_{i=1}^6 x_i (y_i - p_i^{(0)}) = \\sum x_i y_i - p_i^{(0)} \\sum x_i = 3 - 0.5 \\times 0 = 3$。\n所以，$U(\\beta^{(0)}) = \\begin{pmatrix} -1 \\\\ 3 \\end{pmatrix}$。\n\n接下来，计算雅可比矩阵 $J(\\beta^{(0)})$：\n对于所有的 $i$，$p_i^{(0)}(1-p_i^{(0)}) = 0.5 \\times (1-0.5) = 0.25$。\n我们需要 $x_i$ 的平方和：\n$\\sum_{i=1}^6 x_i^2 = (-2)^2 + (-1)^2 + 0^2 + 1^2 + 2^2 + 0^2 = 4+1+0+1+4+0 = 10$。\n雅可比矩阵的元素是：\n$J_{00}^{(0)} = -\\sum p_i^{(0)}(1-p_i^{(0)}) = -6 \\times 0.25 = -1.5$。\n$J_{01}^{(0)} = -\\sum x_i p_i^{(0)}(1-p_i^{(0)}) = -0.25 \\sum x_i = -0.25 \\times 0 = 0$。\n$J_{11}^{(0)} = -\\sum x_i^2 p_i^{(0)}(1-p_i^{(0)}) = -0.25 \\sum x_i^2 = -0.25 \\times 10 = -2.5$。\n所以，$J(\\beta^{(0)}) = \\begin{pmatrix} -1.5 & 0 \\\\ 0 & -2.5 \\end{pmatrix}$。\n\n最后，执行更新以找到 $\\beta^{(1)} = (\\beta_0^{(1)}, \\beta_1^{(1)})^{\\top}$：\n$$\n\\beta^{(1)} = \\beta^{(0)} - [J(\\beta^{(0)})]^{-1} U(\\beta^{(0)})\n$$\n对角雅可比矩阵的逆矩阵是：\n$$\n[J(\\beta^{(0)})]^{-1} = \\begin{pmatrix} 1/(-1.5) & 0 \\\\ 0 & 1/(-2.5) \\end{pmatrix} = \\begin{pmatrix} -2/3 & 0 \\\\ 0 & -2/5 \\end{pmatrix}\n$$\n现在，我们计算 $\\beta^{(1)}$：\n$$\n\\beta^{(1)} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} -2/3 & 0 \\\\ 0 & -2/5 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ 3 \\end{pmatrix} = -\\begin{pmatrix} (-2/3)(-1) + (0)(3) \\\\ (0)(-1) + (-2/5)(3) \\end{pmatrix} = -\\begin{pmatrix} 2/3 \\\\ -6/5 \\end{pmatrix} = \\begin{pmatrix} -2/3 \\\\ 6/5 \\end{pmatrix}\n$$\n用小数形式表示，即为 $\\beta^{(1)} = \\begin{pmatrix} -0.6666... \\\\ 1.2 \\end{pmatrix}$。\n将每个元素四舍五入到四位有效数字，我们得到：\n$\\beta_0^{(1)} \\approx -0.6667$\n$\\beta_1^{(1)} = 1.200$\n\n更新后的参数向量为 $\\beta^{(1)} \\approx (-0.6667, 1.200)^{\\top}$。按要求表示为 $1 \\times 2$ 的行矩阵：\n$\\begin{pmatrix} -0.6667 & 1.200 \\end{pmatrix}$。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} -0.6667 & 1.200 \\end{pmatrix}}\n$$"
        }
    ]
}