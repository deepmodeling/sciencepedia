{
    "hands_on_practices": [
        {
            "introduction": "Estimating the variability of a measurement is as fundamental as estimating its central tendency. This exercise delves into two of the most common estimators for population variance, demonstrating that the \"unbiased\" estimator is not always the most \"efficient\". By deriving the bias of the Maximum Likelihood Estimator (MLE) and comparing the Mean Squared Error (MSE) of both estimators, you will gain a concrete understanding of the celebrated bias-variance tradeoff.",
            "id": "4831014",
            "problem": "A biostatistics team analyzing variability in a continuous biomarker across independent patients assumes that repeated measurements $X_{1},\\dots,X_{n}$ are independent and identically distributed draws from a normal distribution $\\mathcal{N}(\\mu,\\sigma^{2})$ with unknown mean $\\mu$ and variance $\\sigma^{2}$. Consider two estimators of the variance: the Maximum Likelihood Estimator (MLE), defined by $\\hat{\\sigma}^{2}_{\\mathrm{MLE}}=\\frac{1}{n}\\sum_{i=1}^{n}\\left(X_{i}-\\bar{X}\\right)^{2}$, and the unbiased sample variance with denominator $n-1$, defined by $\\hat{\\sigma}^{2}_{U}=\\frac{1}{n-1}\\sum_{i=1}^{n}\\left(X_{i}-\\bar{X}\\right)^{2}$, where $\\bar{X}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}$. Starting from the definitions of bias, variance, and mean squared error, and from standard distributional properties of the normal model (in particular, the chi-square distribution of scaled sums of squared centered normals), derive the exact finite-sample bias of $\\hat{\\sigma}^{2}_{\\mathrm{MLE}}$ as a function of $n$ and $\\sigma^{2}$. Then, compare the mean squared error of $\\hat{\\sigma}^{2}_{\\mathrm{MLE}}$ to that of $\\hat{\\sigma}^{2}_{U}$ for finite $n$, and discuss which estimator is more efficient in the mean squared error sense and whether both estimators are consistent as $n\\to\\infty$. Report only the exact finite-sample bias of $\\hat{\\sigma}^{2}_{\\mathrm{MLE}}$ as your final answer. No rounding is required.",
            "solution": "The problem is valid as it is scientifically grounded, well-posed, objective, and complete. It is a standard problem in mathematical statistics concerning the properties of estimators for the variance of a normal distribution. We proceed with the solution.\n\nLet the repeated measurements be $X_{1}, \\dots, X_{n}$, which are independent and identically distributed (i.i.d.) random variables from a normal distribution $\\mathcal{N}(\\mu, \\sigma^{2})$. The sample mean is $\\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n}X_{i}$.\n\nThe two estimators for the variance $\\sigma^{2}$ are given as:\n$1$. The Maximum Likelihood Estimator (MLE): $\\hat{\\sigma}^{2}_{\\mathrm{MLE}} = \\frac{1}{n}\\sum_{i=1}^{n}(X_{i} - \\bar{X})^{2}$.\n$2$. The unbiased sample variance: $\\hat{\\sigma}^{2}_{U} = \\frac{1}{n-1}\\sum_{i=1}^{n}(X_{i} - \\bar{X})^{2}$.\n\nWe will analyze these estimators based on their bias, variance, mean squared error (MSE), and consistency. The bias of an estimator $\\hat{\\theta}$ for a parameter $\\theta$ is defined as $\\text{Bias}(\\hat{\\theta}) = E[\\hat{\\theta}] - \\theta$. The MSE is defined as $\\text{MSE}(\\hat{\\theta}) = E[(\\hat{\\theta} - \\theta)^{2}]$, which can be decomposed as $\\text{MSE}(\\hat{\\theta}) = \\text{Var}(\\hat{\\theta}) + (\\text{Bias}(\\hat{\\theta}))^{2}$.\n\nA fundamental result from statistical theory (specifically, a consequence of Cochran's theorem) for samples from a normal distribution is that the quantity $\\frac{1}{\\sigma^{2}}\\sum_{i=1}^{n}(X_{i} - \\bar{X})^{2}$ follows a chi-square distribution with $n-1$ degrees of freedom. We denote this as:\n$$ \\frac{\\sum_{i=1}^{n}(X_{i} - \\bar{X})^{2}}{\\sigma^{2}} \\sim \\chi^{2}_{n-1} $$\nA random variable $Y$ that follows a chi-square distribution with $k$ degrees of freedom, $Y \\sim \\chi^{2}_{k}$, has an expected value $E[Y] = k$ and a variance $\\text{Var}(Y) = 2k$.\n\nFirst, we derive the exact finite-sample bias of $\\hat{\\sigma}^{2}_{\\mathrm{MLE}}$. To do this, we first calculate its expected value.\n$$ E[\\hat{\\sigma}^{2}_{\\mathrm{MLE}}] = E\\left[\\frac{1}{n}\\sum_{i=1}^{n}(X_{i} - \\bar{X})^{2}\\right] = \\frac{1}{n}E\\left[\\sum_{i=1}^{n}(X_{i} - \\bar{X})^{2}\\right] $$\nWe can rewrite the sum of squares in terms of the $\\chi^{2}_{n-1}$ distributed variable:\n$$ \\sum_{i=1}^{n}(X_{i} - \\bar{X})^{2} = \\sigma^{2} \\left( \\frac{\\sum_{i=1}^{n}(X_{i} - \\bar{X})^{2}}{\\sigma^{2}} \\right) $$\nThe expected value of this sum is:\n$$ E\\left[\\sum_{i=1}^{n}(X_{i} - \\bar{X})^{2}\\right] = \\sigma^{2} E\\left[ \\frac{\\sum_{i=1}^{n}(X_{i} - \\bar{X})^{2}}{\\sigma^{2}} \\right] = \\sigma^{2} (n-1) $$\nsince the expected value of a $\\chi^{2}_{n-1}$ variable is its degrees of freedom, $n-1$.\nSubstituting this result back into the expression for $E[\\hat{\\sigma}^{2}_{\\mathrm{MLE}}]$:\n$$ E[\\hat{\\sigma}^{2}_{\\mathrm{MLE}}] = \\frac{1}{n} (n-1)\\sigma^{2} = \\frac{n-1}{n}\\sigma^{2} $$\nThe bias of $\\hat{\\sigma}^{2}_{\\mathrm{MLE}}$ is therefore:\n$$ \\text{Bias}(\\hat{\\sigma}^{2}_{\\mathrm{MLE}}) = E[\\hat{\\sigma}^{2}_{\\mathrm{MLE}}] - \\sigma^{2} = \\frac{n-1}{n}\\sigma^{2} - \\sigma^{2} = \\left(\\frac{n-1}{n} - 1\\right)\\sigma^{2} = -\\frac{1}{n}\\sigma^{2} $$\nThis is the required finite-sample bias of the MLE. It is a biased estimator, underestimating the true variance on average.\n\nNext, we compare the mean squared error (MSE) of $\\hat{\\sigma}^{2}_{\\mathrm{MLE}}$ and $\\hat{\\sigma}^{2}_{U}$.\nFirst, let's confirm the unbiasedness of $\\hat{\\sigma}^{2}_{U}$:\n$$ E[\\hat{\\sigma}^{2}_{U}] = E\\left[\\frac{1}{n-1}\\sum_{i=1}^{n}(X_{i} - \\bar{X})^{2}\\right] = \\frac{1}{n-1}E\\left[\\sum_{i=1}^{n}(X_{i} - \\bar{X})^{2}\\right] = \\frac{1}{n-1}(n-1)\\sigma^{2} = \\sigma^{2} $$\nSince $E[\\hat{\\sigma}^{2}_{U}] = \\sigma^{2}$, its bias is $\\text{Bias}(\\hat{\\sigma}^{2}_{U}) = 0$.\n\nTo calculate the MSEs, we need the variances of the estimators. We use the fact that $\\text{Var}(\\chi^{2}_{k}) = 2k$.\nThe variance of $\\hat{\\sigma}^{2}_{U}$ is:\n$$ \\text{Var}(\\hat{\\sigma}^{2}_{U}) = \\text{Var}\\left(\\frac{1}{n-1}\\sum_{i=1}^{n}(X_{i} - \\bar{X})^{2}\\right) = \\text{Var}\\left(\\frac{\\sigma^{2}}{n-1} \\cdot \\frac{\\sum_{i=1}^{n}(X_{i} - \\bar{X})^{2}}{\\sigma^{2}}\\right) $$\n$$ \\text{Var}(\\hat{\\sigma}^{2}_{U}) = \\left(\\frac{\\sigma^{2}}{n-1}\\right)^{2} \\text{Var}(\\chi^{2}_{n-1}) = \\frac{\\sigma^{4}}{(n-1)^{2}} \\cdot 2(n-1) = \\frac{2\\sigma^{4}}{n-1} $$\nSince $\\hat{\\sigma}^{2}_{U}$ is unbiased, its MSE is equal to its variance:\n$$ \\text{MSE}(\\hat{\\sigma}^{2}_{U}) = \\text{Var}(\\hat{\\sigma}^{2}_{U}) + (\\text{Bias}(\\hat{\\sigma}^{2}_{U}))^{2} = \\frac{2\\sigma^{4}}{n-1} + 0^{2} = \\frac{2\\sigma^{4}}{n-1} $$\n\nNow for $\\hat{\\sigma}^{2}_{\\mathrm{MLE}}$. Note that $\\hat{\\sigma}^{2}_{\\mathrm{MLE}} = \\frac{n-1}{n}\\hat{\\sigma}^{2}_{U}$.\nThe variance of $\\hat{\\sigma}^{2}_{\\mathrm{MLE}}$ is:\n$$ \\text{Var}(\\hat{\\sigma}^{2}_{\\mathrm{MLE}}) = \\text{Var}\\left(\\frac{n-1}{n}\\hat{\\sigma}^{2}_{U}\\right) = \\left(\\frac{n-1}{n}\\right)^{2} \\text{Var}(\\hat{\\sigma}^{2}_{U}) = \\frac{(n-1)^{2}}{n^{2}} \\cdot \\frac{2\\sigma^{4}}{n-1} = \\frac{2(n-1)\\sigma^{4}}{n^{2}} $$\nThe MSE of $\\hat{\\sigma}^{2}_{\\mathrm{MLE}}$ is the sum of its variance and squared bias:\n$$ \\text{MSE}(\\hat{\\sigma}^{2}_{\\mathrm{MLE}}) = \\text{Var}(\\hat{\\sigma}^{2}_{\\mathrm{MLE}}) + (\\text{Bias}(\\hat{\\sigma}^{2}_{\\mathrm{MLE}}))^{2} = \\frac{2(n-1)\\sigma^{4}}{n^{2}} + \\left(-\\frac{\\sigma^{2}}{n}\\right)^{2} $$\n$$ \\text{MSE}(\\hat{\\sigma}^{2}_{\\mathrm{MLE}}) = \\frac{2(n-1)\\sigma^{4}}{n^{2}} + \\frac{\\sigma^{4}}{n^{2}} = \\frac{(2n - 2 + 1)\\sigma^{4}}{n^{2}} = \\frac{(2n-1)\\sigma^{4}}{n^{2}} $$\nTo compare the MSEs, we examine the difference $\\text{MSE}(\\hat{\\sigma}^{2}_{U}) - \\text{MSE}(\\hat{\\sigma}^{2}_{\\mathrm{MLE}})$ for $n \\ge 2$:\n$$ \\frac{2\\sigma^{4}}{n-1} - \\frac{(2n-1)\\sigma^{4}}{n^{2}} = \\sigma^{4}\\left(\\frac{2}{n-1} - \\frac{2n-1}{n^{2}}\\right) = \\sigma^{4}\\left(\\frac{2n^{2} - (2n-1)(n-1)}{n^{2}(n-1)}\\right) $$\n$$ = \\sigma^{4}\\left(\\frac{2n^{2} - (2n^{2} - 3n + 1)}{n^{2}(n-1)}\\right) = \\sigma^{4}\\left(\\frac{3n - 1}{n^{2}(n-1)}\\right) $$\nFor any sample size $n \\ge 2$, the term $\\frac{3n-1}{n^{2}(n-1)}$ is strictly positive. Therefore, $\\text{MSE}(\\hat{\\sigma}^{2}_{U}) > \\text{MSE}(\\hat{\\sigma}^{2}_{\\mathrm{MLE}})$. This shows that $\\hat{\\sigma}^{2}_{\\mathrm{MLE}}$ has a smaller mean squared error than $\\hat{\\sigma}^{2}_{U}$ for any finite sample size. In the MSE sense, $\\hat{\\sigma}^{2}_{\\mathrm{MLE}}$ is the more efficient estimator. This illustrates the classic bias-variance tradeoff: by accepting a small bias, the MLE achieves a lower variance, resulting in an overall smaller MSE.\n\nFinally, we discuss consistency. An estimator $\\hat{\\theta}_{n}$ is consistent for $\\theta$ if it converges in probability to $\\theta$ as $n \\to \\infty$. A sufficient condition for consistency is that both the bias and the variance tend to zero as $n \\to \\infty$.\nFor $\\hat{\\sigma}^{2}_{\\mathrm{MLE}}$:\n$$ \\lim_{n\\to\\infty} \\text{Bias}(\\hat{\\sigma}^{2}_{\\mathrm{MLE}}) = \\lim_{n\\to\\infty} \\left(-\\frac{\\sigma^{2}}{n}\\right) = 0 $$\n$$ \\lim_{n\\to\\infty} \\text{Var}(\\hat{\\sigma}^{2}_{\\mathrm{MLE}}) = \\lim_{n\\to\\infty} \\frac{2(n-1)\\sigma^{4}}{n^{2}} = \\lim_{n\\to\\infty} \\left(\\frac{2\\sigma^{4}}{n} - \\frac{2\\sigma^{4}}{n^{2}}\\right) = 0 $$\nSince both its bias and variance go to zero, $\\hat{\\sigma}^{2}_{\\mathrm{MLE}}$ is a consistent estimator for $\\sigma^{2}$.\n\nFor $\\hat{\\sigma}^{2}_{U}$:\n$$ \\lim_{n\\to\\infty} \\text{Bias}(\\hat{\\sigma}^{2}_{U}) = \\lim_{n\\to\\infty} 0 = 0 $$\n$$ \\lim_{n\\to\\infty} \\text{Var}(\\hat{\\sigma}^{2}_{U}) = \\lim_{n\\to\\infty} \\frac{2\\sigma^{4}}{n-1} = 0 $$\nSince its bias is always zero and its variance goes to zero, $\\hat{\\sigma}^{2}_{U}$ is also a consistent estimator for $\\sigma^{2}$.\n\nThus, both estimators are consistent. Asymptotically, as $n \\to \\infty$, the factor $\\frac{n-1}{n}$ relating the two estimators approaches $1$, so they become equivalent.",
            "answer": "$$\\boxed{-\\frac{\\sigma^{2}}{n}}$$"
        },
        {
            "introduction": "While having an unbiased estimator for a primary parameter like a probability $p$ is a good starting point, our ultimate interest often lies in a non-linear transformation of it, such as the log-odds. This practice reveals a subtle but critical property of estimators: unbiasedness is not preserved under non-linear maps. You will use a Taylor series approximation to derive the leading-order bias of the log-odds estimator, a technique essential for understanding the behavior of many advanced statistical models.",
            "id": "4981389",
            "problem": "A medical safety monitoring study tracks whether each of $n$ independent patients experiences a binary adverse event during a fixed follow-up window. Let $X_1,\\dots,X_n$ be independent and identically distributed with $X_i \\sim \\text{Bernoulli}(p)$, where $p \\in (0,1)$ is the true probability of the adverse event. The parameter of interest for downstream risk communication is the log-odds $\\theta = \\ln\\!\\big(p/(1-p)\\big)$. The Maximum Likelihood Estimator (MLE) is $\\hat{p} = \\bar{X} = n^{-1} \\sum_{i=1}^{n} X_i$, and the plug-in estimator for the log-odds is $\\hat{\\theta} = \\ln\\!\\big(\\hat{p}/(1-\\hat{p})\\big)$.\n\nTasks:\n1. Starting from the definitions of expectation and independence, show that $\\hat{p}$ is unbiased for $p$.\n2. Assume $p \\in (0,1)$ and consider only the event $\\{0<\\hat{p}<1\\}$ so that $\\hat{\\theta}$ is well-defined; note that $P(0<\\hat{p}<1) \\to 1$ as $n \\to \\infty$ for fixed $p \\in (0,1)$. By expanding the transformation $g(x) = \\ln\\!\\big(x/(1-x)\\big)$ around $x=p$ to second order and using the leading nonvanishing terms in the expectation, derive the leading-order asymptotic bias $E[\\hat{\\theta}] - \\theta$ up to order $1/n$, expressed explicitly as a symbolic function of $p$ and $n$.\n\nProvide as your final answer a single closed-form analytic expression in $p$ and $n$ for the leading-order asymptotic bias. No numerical rounding is required, and no units are involved.",
            "solution": "The problem statement has been validated and is deemed a valid, well-posed problem in statistical theory. It is scientifically grounded, self-contained, and objective. We may therefore proceed with a full solution.\n\nThe problem is divided into two tasks. We will address them sequentially.\n\n**Task 1: Show that $\\hat{p}$ is an unbiased estimator for $p$.**\n\nAn estimator is unbiased if its expected value is equal to the true parameter value. Here, we must show that $E[\\hat{p}] = p$.\n\nThe estimator for $p$ is given as the sample mean, $\\hat{p} = \\bar{X} = n^{-1} \\sum_{i=1}^{n} X_i$.\nThe expectation of $\\hat{p}$ is:\n$$\nE[\\hat{p}] = E\\left[\\frac{1}{n} \\sum_{i=1}^{n} X_i\\right]\n$$\nBy the linearity of the expectation operator, we can move the constant factor $1/n$ and the summation outside the expectation:\n$$\nE[\\hat{p}] = \\frac{1}{n} \\sum_{i=1}^{n} E[X_i]\n$$\nThe problem states that $X_1, \\dots, X_n$ are independent and identically distributed (i.i.d.) random variables from a Bernoulli distribution with parameter $p$, i.e., $X_i \\sim \\text{Bernoulli}(p)$. For a Bernoulli random variable, the probability mass function is $P(X_i = 1) = p$ and $P(X_i = 0) = 1-p$. The expected value of each $X_i$ is, by definition:\n$$\nE[X_i] = 1 \\cdot P(X_i = 1) + 0 \\cdot P(X_i = 0) = 1 \\cdot p + 0 \\cdot (1-p) = p\n$$\nSince all $X_i$ are identically distributed, $E[X_i] = p$ for all $i \\in \\{1, \\dots, n\\}$. Substituting this result back into the expression for $E[\\hat{p}]$:\n$$\nE[\\hat{p}] = \\frac{1}{n} \\sum_{i=1}^{n} p\n$$\nThe sum consists of $n$ identical terms of $p$, so $\\sum_{i=1}^{n} p = n p$.\n$$\nE[\\hat{p}] = \\frac{1}{n} (n p) = p\n$$\nSince $E[\\hat{p}] = p$, the estimator $\\hat{p}$ is, by definition, an unbiased estimator for the parameter $p$. This completes the first task.\n\n**Task 2: Derive the leading-order asymptotic bias of $\\hat{\\theta}$.**\n\nThe parameter of interest is the log-odds, $\\theta = \\ln(p/(1-p))$, and its estimator is $\\hat{\\theta} = \\ln(\\hat{p}/(1-\\hat{p}))$. This can be written as $\\hat{\\theta} = g(\\hat{p})$ where the transformation function is $g(x) = \\ln(x/(1-x))$.\n\nThe bias of $\\hat{\\theta}$ is defined as $\\text{Bias}(\\hat{\\theta}) = E[\\hat{\\theta}] - \\theta$. To find an asymptotic expression for this bias, we use a Taylor series expansion of the function $g(\\hat{p})$ around the point $\\hat{p} = p$. The problem specifies expanding to the second order.\n$$\ng(\\hat{p}) \\approx g(p) + g'(p)(\\hat{p}-p) + \\frac{1}{2}g''(p)(\\hat{p}-p)^2\n$$\nFirst, we must find the first and second derivatives of $g(x) = \\ln(x) - \\ln(1-x)$.\nThe first derivative is:\n$$\ng'(x) = \\frac{d}{dx} [\\ln(x) - \\ln(1-x)] = \\frac{1}{x} - \\frac{1}{1-x}(-1) = \\frac{1}{x} + \\frac{1}{1-x} = \\frac{(1-x)+x}{x(1-x)} = \\frac{1}{x(1-x)}\n$$\nThe second derivative is:\n$$\ng''(x) = \\frac{d}{dx} [x^{-1} + (1-x)^{-1}] = -x^{-2} + (-1)(1-x)^{-2}(-1) = -x^{-2} + (1-x)^{-2} = \\frac{1}{(1-x)^2} - \\frac{1}{x^2}\n$$\nCombining the terms for $g''(x)$:\n$$\ng''(x) = \\frac{x^2 - (1-x)^2}{x^2(1-x)^2} = \\frac{x^2 - (1 - 2x + x^2)}{x^2(1-x)^2} = \\frac{2x-1}{x^2(1-x)^2}\n$$\nNow, we evaluate these derivatives at $x=p$:\n- $g(p) = \\ln(p/(1-p)) = \\theta$\n- $g'(p) = \\frac{1}{p(1-p)}$\n- $g''(p) = \\frac{2p-1}{p^2(1-p)^2}$\n\nSubstituting these into the Taylor expansion gives:\n$$\n\\hat{\\theta} = g(\\hat{p}) \\approx \\theta + \\left(\\frac{1}{p(1-p)}\\right)(\\hat{p}-p) + \\frac{1}{2}\\left(\\frac{2p-1}{p^2(1-p)^2}\\right)(\\hat{p}-p)^2\n$$\nTo find the approximate expected value of $\\hat{\\theta}$, we take the expectation of this expression. The problem assumes we are on the event $\\{0 < \\hat{p} < 1\\}$, where $\\hat{\\theta}$ is well-defined.\n$$\nE[\\hat{\\theta}] \\approx E\\left[\\theta + \\frac{1}{p(1-p)}(\\hat{p}-p) + \\frac{2p-1}{2p^2(1-p)^2}(\\hat{p}-p)^2\\right]\n$$\nUsing the linearity of expectation:\n$$\nE[\\hat{\\theta}] \\approx \\theta + \\frac{1}{p(1-p)}E[\\hat{p}-p] + \\frac{2p-1}{2p^2(1-p)^2}E[(\\hat{p}-p)^2]\n$$\nWe need to evaluate the two expectation terms:\n- $E[\\hat{p}-p]$ is the bias of $\\hat{p}$. As shown in Task 1, $\\hat{p}$ is unbiased, so $E[\\hat{p}] = p$. Therefore, $E[\\hat{p}-p] = E[\\hat{p}] - p = p - p = 0$.\n- $E[(\\hat{p}-p)^2]$ is, by definition, the variance of $\\hat{p}$, denoted $\\text{Var}(\\hat{p})$, since its mean is $p$.\n\nWe now calculate $\\text{Var}(\\hat{p})$:\n$$\n\\text{Var}(\\hat{p}) = \\text{Var}\\left(\\frac{1}{n} \\sum_{i=1}^{n} X_i\\right) = \\frac{1}{n^2} \\text{Var}\\left(\\sum_{i=1}^{n} X_i\\right)\n$$\nSince the $X_i$ are independent, the variance of the sum is the sum of the variances:\n$$\n\\text{Var}(\\hat{p}) = \\frac{1}{n^2} \\sum_{i=1}^{n} \\text{Var}(X_i)\n$$\nFor a Bernoulli($p$) variable, the variance is $\\text{Var}(X_i) = p(1-p)$. Since the variables are identically distributed:\n$$\n\\text{Var}(\\hat{p}) = \\frac{1}{n^2} \\sum_{i=1}^{n} p(1-p) = \\frac{1}{n^2} [n p(1-p)] = \\frac{p(1-p)}{n}\n$$\nSubstituting these results back into the approximation for $E[\\hat{\\theta}]$:\n$$\nE[\\hat{\\theta}] \\approx \\theta + \\frac{1}{p(1-p)}(0) + \\frac{2p-1}{2p^2(1-p)^2}\\left(\\frac{p(1-p)}{n}\\right)\n$$\nSimplifying the second term:\n$$\nE[\\hat{\\theta}] \\approx \\theta + \\frac{(2p-1)p(1-p)}{2n p^2(1-p)^2} = \\theta + \\frac{2p-1}{2n p(1-p)}\n$$\nThe asymptotic bias is $E[\\hat{\\theta}] - \\theta$. From the expression above, we find the leading-order term for the bias:\n$$\n\\text{Bias}(\\hat{\\theta}) = E[\\hat{\\theta}] - \\theta \\approx \\frac{2p-1}{2np(1-p)}\n$$\nThis expression is of order $1/n$. Higher-order terms in the Taylor expansion, such as the term involving $E[(\\hat{p}-p)^3]$, would contribute terms of order $1/n^2$ or smaller, confirming that this is the leading-order asymptotic bias.\n\nThe required final answer is the closed-form analytic expression for this leading-order asymptotic bias.",
            "answer": "$$\n\\boxed{\\frac{2p-1}{2np(1-p)}}\n$$"
        },
        {
            "introduction": "Having established that even simple transformations can introduce bias, a natural question arises: how can we correct for it? This exercise introduces the jackknife, a powerful and broadly applicable resampling technique for estimating and reducing estimator bias. By deriving the jackknife bias-corrected estimator for a ratio of means—a statistic common in fields like health economics—you will learn a practical, computationally-driven approach to improving the accuracy of your estimates.",
            "id": "4981405",
            "problem": "A health economics team is analyzing patient-level data from a multi-center observational cohort to estimate the Incremental Cost-Effectiveness Ratio (ICER) of a new chronic disease management program. For each patient $i \\in \\{1,\\dots,n\\}$, let $C_{i} \\in (0,\\infty)$ denote the total annual medical cost and $Q_{i} \\in (0,\\infty)$ denote the corresponding annual effectiveness measured in Quality-Adjusted Life Year (QALY). Assume $\\{(C_{i},Q_{i})\\}_{i=1}^{n}$ are independent and identically distributed draws from a joint distribution with finite second moments and strictly positive means. Consider the smooth statistic\n$$\n\\hat{\\theta} \\;=\\; \\frac{\\bar{C}}{\\bar{Q}}, \\quad \\text{where} \\quad \\bar{C} \\;=\\; \\frac{1}{n}\\sum_{i=1}^{n} C_{i}, \\quad \\bar{Q} \\;=\\; \\frac{1}{n}\\sum_{i=1}^{n} Q_{i}.\n$$\nThe scientific goal is to reduce the leading-order bias of $\\hat{\\theta}$ while retaining its consistency and first-order efficiency under the above assumptions.\n\nDefine the leave-one-out means\n$$\n\\bar{C}_{(i)} \\;=\\; \\frac{1}{n-1}\\sum_{\\substack{j=1 \\\\ j \\neq i}}^{n} C_{j}, \n\\qquad\n\\bar{Q}_{(i)} \\;=\\; \\frac{1}{n-1}\\sum_{\\substack{j=1 \\\\ j \\neq i}}^{n} Q_{j},\n$$\nand the associated leave-one-out replicates\n$$\n\\hat{\\theta}_{(i)} \\;=\\; \\frac{\\bar{C}_{(i)}}{\\bar{Q}_{(i)}}, \n\\qquad\n\\bar{\\hat{\\theta}}_{(\\cdot)} \\;=\\; \\frac{1}{n}\\sum_{i=1}^{n} \\hat{\\theta}_{(i)}.\n$$\nStarting from the definition of bias and the smoothness of the statistic mapping $(\\bar{C},\\bar{Q}) \\mapsto \\bar{C}/\\bar{Q}$, derive the jackknife estimate of the $\\mathcal{O}(n^{-1})$ bias of $\\hat{\\theta}$ in terms of $n$, $\\hat{\\theta}$, and the $\\hat{\\theta}_{(i)}$. Then, use this bias estimate to construct the jackknife bias-corrected estimator $\\hat{\\theta}_{JK}$. Express your final estimator in a single closed-form analytic expression involving $n$, $\\hat{\\theta}$, and $\\{\\hat{\\theta}_{(i)}\\}_{i=1}^{n}$. No numerical approximation is required; provide the exact expression with no units inside your final answer box.",
            "solution": "The user wants to derive the jackknife bias-corrected estimator for a ratio statistic.\n\n### Step 1: Extract Givens\n- Data: $\\{(C_{i},Q_{i})\\}_{i=1}^{n}$ are independent and identically distributed (i.i.d.) random variables.\n- Domain: $C_{i} \\in (0,\\infty)$, $Q_{i} \\in (0,\\infty)$.\n- Moments: The joint distribution has finite second moments. The means, $\\mu_C = E[C_i]$ and $\\mu_Q = E[Q_i]$, are strictly positive.\n- Estimator of interest: $\\hat{\\theta} \\;=\\; \\frac{\\bar{C}}{\\bar{Q}}$, where $\\bar{C} \\;=\\; \\frac{1}{n}\\sum_{i=1}^{n} C_{i}$ and $\\bar{Q} \\;=\\; \\frac{1}{n}\\sum_{i=1}^{n} Q_{i}$.\n- Leave-one-out means: $\\bar{C}_{(i)} \\;=\\; \\frac{1}{n-1}\\sum_{j \\neq i} C_{j}$ and $\\bar{Q}_{(i)} \\;=\\; \\frac{1}{n-1}\\sum_{j \\neq i} Q_{j}$.\n- Leave-one-out replicates: $\\hat{\\theta}_{(i)} \\;=\\; \\frac{\\bar{C}_{(i)}}{\\bar{Q}_{(i)}}$.\n- Mean of replicates: $\\bar{\\hat{\\theta}}_{(\\cdot)} \\;=\\; \\frac{1}{n}\\sum_{i=1}^{n} \\hat{\\theta}_{(i)}$.\n- Task: Derive the jackknife estimate of the $\\mathcal{O}(n^{-1})$ bias of $\\hat{\\theta}$ and use it to construct the jackknife bias-corrected estimator, $\\hat{\\theta}_{JK}$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientific Grounding**: The problem is well-grounded in statistical theory, specifically in the area of resampling methods for bias-variance analysis of estimators. The jackknife is a standard, rigorously defined technique. The application context, estimating an Incremental Cost-Effectiveness Ratio (ICER) in health economics, is a common and appropriate use case for ratio estimators and their bias correction.\n- **Well-Posed**: The problem is well-posed. All terms are defined mathematically. The assumptions (i.i.d. draws, finite second moments, positive means) are standard and sufficient for the required theoretical development (e.g., ensuring validity of Taylor series expansions and the Law of Large Numbers). The goal is to derive a specific formula, which has a unique solution.\n- **Objective**: The problem is stated in precise, objective mathematical language, free from any subjective or ambiguous claims.\n\nThe problem does not violate any of the invalidity criteria. It is a standard and formal problem in mathematical statistics.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. The solution process will now proceed.\n\n### Derivation of the Jackknife Estimator\n\nThe parameter of interest is the ratio of the true means, $\\theta = \\frac{\\mu_C}{\\mu_Q} = \\frac{E[C_i]}{E[Q_i]}$. The estimator $\\hat{\\theta} = \\bar{C}/\\bar{Q}$ is a function of sample means. Due to the non-linearity of the ratio function $f(x,y)=x/y$, the expectation of the estimator is not equal to the function of the expectations, i.e., $E[\\hat{\\theta}] \\neq E[\\bar{C}]/E[\\bar{Q}] = \\mu_C/\\mu_Q$. This discrepancy leads to bias.\n\nFor a smooth statistic like $\\hat{\\theta}$, which is based on a sample of size $n$, the bias can be expressed via an asymptotic expansion in powers of $n^{-1}$:\n$$\n\\text{Bias}(\\hat{\\theta}) = E[\\hat{\\theta}] - \\theta = \\frac{b_1}{n} + \\frac{b_2}{n^2} + \\mathcal{O}(n^{-3})\n$$\nwhere $b_1, b_2, \\dots$ are constants that depend on the moments of the underlying distribution of $(C_i, Q_i)$ but not on $n$. The goal is to estimate and remove the leading-order bias term, $\\frac{b_1}{n}$.\n\nThe jackknife procedure generates replicates of the estimator, $\\hat{\\theta}_{(i)}$, by systematically leaving out one observation at a time. Each $\\hat{\\theta}_{(i)}$ is an estimator of $\\theta$ based on a sample of size $n-1$. Therefore, its bias has the same functional form but with $n$ replaced by $n-1$:\n$$\n\\text{Bias}(\\hat{\\theta}_{(i)}) = E[\\hat{\\theta}_{(i)}] - \\theta = \\frac{b_1}{n-1} + \\frac{b_2}{(n-1)^2} + \\mathcal{O}((n-1)^{-3})\n$$\nSince the original data $\\{ (C_i, Q_i) \\}_{i=1}^n$ are i.i.d., all leave-one-out statistics $\\hat{\\theta}_{(i)}$ are identically distributed. Thus, their expectations are all equal.\n\nLet's consider the expectation of the mean of these replicates, $\\bar{\\hat{\\theta}}_{(\\cdot)}$:\n$$\nE[\\bar{\\hat{\\theta}}_{(\\cdot)}] = E\\left[\\frac{1}{n}\\sum_{i=1}^{n} \\hat{\\theta}_{(i)}\\right] = \\frac{1}{n}\\sum_{i=1}^{n} E[\\hat{\\theta}_{(i)}] = E[\\hat{\\theta}_{(1)}]\n$$\nSubstituting the bias expansion, we get:\n$$\nE[\\bar{\\hat{\\theta}}_{(\\cdot)}] = \\theta + \\frac{b_1}{n-1} + \\mathcal{O}(n^{-2})\n$$\n\nThe jackknife method estimates the bias of $\\hat{\\theta}$ by considering the difference between the average of the leave-one-out estimates and the full-sample estimate. The jackknife estimator of the bias is defined as:\n$$\n\\widehat{\\text{Bias}}_{JK}(\\hat{\\theta}) = (n-1) \\left( \\bar{\\hat{\\theta}}_{(\\cdot)} - \\hat{\\theta} \\right)\n$$\nTo verify that this is a reasonable estimator for the bias, we can examine its expectation:\n$$\nE[\\widehat{\\text{Bias}}_{JK}(\\hat{\\theta})] = (n-1) \\left( E[\\bar{\\hat{\\theta}}_{(\\cdot)}] - E[\\hat{\\theta}] \\right)\n$$\nSubstituting the expansions for the expectations:\n$$\nE[\\widehat{\\text{Bias}}_{JK}(\\hat{\\theta})] = (n-1) \\left( \\left(\\theta + \\frac{b_1}{n-1} + \\mathcal{O}(n^{-2})\\right) - \\left(\\theta + \\frac{b_1}{n} + \\mathcal{O}(n^{-2})\\right) \\right)\n$$\n$$\nE[\\widehat{\\text{Bias}}_{JK}(\\hat{\\theta})] = (n-1) \\left( \\frac{b_1}{n-1} - \\frac{b_1}{n} \\right) + \\mathcal{O}(n^{-1})\n$$\n$$\nE[\\widehat{\\text{Bias}}_{JK}(\\hat{\\theta})] = (n-1) \\left( \\frac{b_1 n - b_1 (n-1)}{n(n-1)} \\right) + \\mathcal{O}(n^{-1})\n$$\n$$\nE[\\widehat{\\text{Bias}}_{JK}(\\hat{\\theta})] = (n-1) \\left( \\frac{b_1}{n(n-1)} \\right) + \\mathcal{O}(n^{-1}) = \\frac{b_1}{n} + \\mathcal{O}(n^{-1})\n$$\nSince $E[\\widehat{\\text{Bias}}_{JK}(\\hat{\\theta})] = \\text{Bias}(\\hat{\\theta}) + \\mathcal{O}(n^{-2})$, this confirms that $\\widehat{\\text{Bias}}_{JK}(\\hat{\\theta})$ is an estimator of the leading-order bias term with bias of its own on the order of $\\mathcal{O}(n^{-2})$.\n\nThe jackknife bias-corrected estimator, $\\hat{\\theta}_{JK}$, is constructed by subtracting the estimated bias from the original estimator:\n$$\n\\hat{\\theta}_{JK} = \\hat{\\theta} - \\widehat{\\text{Bias}}_{JK}(\\hat{\\theta})\n$$\n$$\n\\hat{\\theta}_{JK} = \\hat{\\theta} - (n-1) \\left( \\bar{\\hat{\\theta}}_{(\\cdot)} - \\hat{\\theta} \\right)\n$$\nWe now simplify this expression to obtain the final closed form.\n$$\n\\hat{\\theta}_{JK} = \\hat{\\theta} - (n-1)\\bar{\\hat{\\theta}}_{(\\cdot)} + (n-1)\\hat{\\theta}\n$$\n$$\n\\hat{\\theta}_{JK} = (1 + n - 1)\\hat{\\theta} - (n-1)\\bar{\\hat{\\theta}}_{(\\cdot)}\n$$\n$$\n\\hat{\\theta}_{JK} = n\\hat{\\theta} - (n-1)\\bar{\\hat{\\theta}}_{(\\cdot)}\n$$\nThe problem asks for the final answer in terms of $n$, $\\hat{\\theta}$, and the set of replicates $\\{\\hat{\\theta}_{(i)}\\}_{i=1}^{n}$. We substitute the definition of $\\bar{\\hat{\\theta}}_{(\\cdot)}$:\n$$\n\\bar{\\hat{\\theta}}_{(\\cdot)} = \\frac{1}{n}\\sum_{i=1}^{n} \\hat{\\theta}_{(i)}\n$$\nThis gives the final expression for the jackknife bias-corrected estimator:\n$$\n\\hat{\\theta}_{JK} = n\\hat{\\theta} - (n-1) \\left( \\frac{1}{n} \\sum_{i=1}^{n} \\hat{\\theta}_{(i)} \\right)\n$$\n$$\n\\hat{\\theta}_{JK} = n\\hat{\\theta} - \\frac{n-1}{n} \\sum_{i=1}^{n} \\hat{\\theta}_{(i)}\n$$\nThis estimator is also interpretable as the average of the \"pseudo-values\" $\\tilde{\\theta}_i = n\\hat{\\theta} - (n-1)\\hat{\\theta}_{(i)}$. By design, the bias of $\\hat{\\theta}_{JK}$ is of order $\\mathcal{O}(n^{-2})$, an improvement over the original estimator's $\\mathcal{O}(n^{-1})$ bias.",
            "answer": "$$\\boxed{n\\hat{\\theta} - \\frac{n-1}{n}\\sum_{i=1}^{n} \\hat{\\theta}_{(i)}}$$"
        }
    ]
}