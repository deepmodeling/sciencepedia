{
    "hands_on_practices": [
        {
            "introduction": "在应用回归分析中，不同的诊断指标有时会给出看似矛盾的信号。本练习  呈现了一个经典场景：一个数据点在提高模型整体精度的同时，却对某个特定的系数估计产生了强大的影响。通过解决这个问题，你将学会剖析杠杆值和残差各自的独特作用，从而化解这种表面上的矛盾，并更深入地理解全局性和系数特异性诊断工具如何运作。",
            "id": "4959084",
            "problem": "一项前瞻性队列研究使用普通最小二乘 (OLS) 线性回归对一个连续性炎症标志物结果 $y$（例如，$\\log$ C-反应蛋白）进行建模，使用的预测变量包括 $x_1$（年龄）、$x_2$（体重指数）、$x_3$（当前吸烟指示变量）和 $x_4$（治疗分配），以及一个截距项。设设计矩阵为 $X \\in \\mathbb{R}^{n \\times p}$，其中 $p=5$（包含截距项）。考虑一个特定的患者 $i$，其协变量向量 $x_i \\in \\mathbb{R}^p$ 位于观测到的预测变量空间的外围（年龄非常大且体重指数很低），因此来自帽子矩阵 $H = X (X^\\top X)^{-1} X^\\top$ 的杠杆值 $h_{ii}$ 很高。假设原始残差 $r_i = y_i - x_i^\\top \\hat{\\beta}$ 的绝对值很小，并且病例删除精度诊断指标 $\\mathrm{COVRATIO}_i$ 满足 $\\mathrm{COVRATIO}_i  1$，其中 $\\mathrm{COVRATIO}_i$ 定义为全样本 OLS 估计量 $\\hat{\\beta}$ 的方差-协方差矩阵的行列式与病例删除估计量 $\\hat{\\beta}_{(-i)}$ 的方差-协方差矩阵的行列式之比。根据经验观察，该患者对年龄系数有很大的系数特定病例删除效应，其中 $|\\mathrm{DFBETAS}_{i,\\text{age}}|$ 超过了常规阈值，而对吸烟和治疗系数的影响可以忽略不计。\n\n从医学统计建模的角度出发，从 OLS 估计量及其方差的基本定义开始，推断包含一个极端协变量向量 $x_i$ 会如何改变 $\\hat{\\beta}$ 的全局精度以及某些系数的参数特定敏感性。然后，选择能够正确解释为什么一个高杠杆点可以同时导致 $\\mathrm{COVRATIO}_i  1$（表明全局精度提高）却又对特定系数具有影响力，并推荐在临床回归建模背景下能够解决这种明显矛盾的、适当且科学上一致的诊断方法的选项。\n\n选项：\nA. 如果 $h_{ii}$ 很高但 $|r_i|$ 很小，那么该观测值不可能对任何系数产生影响，因为产生影响需要一个大的残差；因此，$\\mathrm{COVRATIO}_i  1$ 的发现证明该病例完全是良性的，不需要进行系数级别的诊断。\nB. 包含一个高杠杆的 $x_i$ 会将 $x_i x_i^\\top$ 加到 $X^\\top X$ 上，这会减小 $\\det\\{(X^\\top X)^{-1}\\}$ 并可能导致 $\\mathrm{COVRATIO}_i  1$（全局精度提高），而 $x_i$ 在预测变量空间中的几何方向仍然可以以一种改变特定系数的方式重新加权信息；为了解决这个问题，应检查系数特定的病例删除效应，如 $\\mathrm{DFBETAS}_{i,j}$、分量式库克距离、针对 $x_j$ 的增量变量图，并使用稳健回归进行敏感性分析。\nC. 因为 $\\mathrm{COVRATIO}_i$ 是全局比较全样本与病例删除后的精度，所以一个小于1的值保证了该观测值改善了模型的所有方面；因此，系数影响不可能发生，任何大的 $\\mathrm{DFBETAS}$ 值都必定是虚假的。\nD. 参数影响是方向性的：即使 $|r_i| \\approx 0$，一个大的 $h_{ii}$ 加上 $x_i$ 与 $X$ 的一个信息不足的轴对齐，也可能通过病例删除时 $(X^\\top X)^{-1}$ 的重新定向，导致某些系数发生显著变化；适当的诊断方法包括对每个系数计算 $\\mathrm{DFBETAS}_{i,j}$、检查条件指数和方差分解比例以评估共线性、进行局部影响分析，以及进行有针对性的留一法重拟合以量化敏感性。",
            "solution": "问题陈述描述了普通最小二乘 (OLS) 回归中的一个场景，这在应用统计学中很常见，尤其是在医学研究等领域。它提出了一个明显的悖论：单个数据点在提高估计系数向量 $\\hat{\\beta}$ 的*全局*精度的同时，又对某个特定系数具有高度*影响力*。我们必须验证问题的设定，然后利用线性回归分析的基本原理来解决这个矛盾。\n\n### 问题验证\n\n**第1步：提取已知条件**\n-   **模型：** OLS 线性回归，$y = X\\beta + \\epsilon$。\n-   **数据：** 一项前瞻性队列研究。\n-   **结果 $y$：** 一个连续性炎症标志物。\n-   **预测变量：** $x_1$（年龄）、$x_2$（体重指数）、$x_3$（吸烟指示变量）、$x_4$（治疗分配），外加一个截距项。设计矩阵为 $X \\in \\mathbb{R}^{n \\times p}$，其中 $p=5$。\n-   **观测值 $i$：** 具有协变量向量 $x_i$ 的特定患者。\n-   **观测值 $i$ 的属性：**\n    1.  协变量向量 $x_i$ 位于预测变量空间的边缘（例如，年龄非常大，BMI 很低）。\n    2.  杠杆值 $h_{ii}$ 很高。杠杆值是帽子矩阵 $H = X(X^\\top X)^{-1}X^\\top$ 的第 $i$ 个对角元素。因此，$h_{ii} = x_i^\\top(X^\\top X)^{-1}x_i$。\n    3.  原始残差 $r_i = y_i - x_i^\\top \\hat{\\beta}$ 的绝对值很小。\n    4.  病例删除精度诊断指标 $\\mathrm{COVRATIO}_i  1$。它被定义为全样本 OLS 估计量 $\\hat{\\beta}$ 的方差-协方差矩阵的行列式与病例删除估计量 $\\hat{\\beta}_{(-i)}$ 的方差-协方差矩阵的行列式之比。\n    5.  该观测值对年龄系数有很大的影响，即 $|\\mathrm{DFBETAS}_{i,\\text{age}}|$ 很大，但对其他系数的影响可以忽略不计。\n\n**第2步：使用提取的已知条件进行验证**\n该问题在科学上和统计上是合理的。所描述的场景是回归诊断中的一个经典案例研究。\n-   **科学依据：** OLS、杠杆值、残差以及诸如 $\\mathrm{COVRATIO}$ 和 $\\mathrm{DFBETAS}$ 之类的影响诊断概念是统计建模的基础。将其应用于一个包含合理变量的医学研究是现实的。\n-   **适定性：** 该问题要求基于一个明确定义的统计情况，给出概念性解释并推荐进一步的诊断方法。它的结构使得存在一个正确的、非唯一但一致的、基于统计理论的解释和建议集合。\n-   **客观性：** 该问题以精确、客观和标准的统计术语陈述。\n\n**第3步：结论与行动**\n问题陈述是有效的。它描述了应用回归分析中一个非凡且重要的现象。我们将着手推导解决方案。\n\n### 从第一性原理推导\n\n设 $\\hat{\\beta} = (X^\\top X)^{-1} X^\\top y$ 为大小为 $n$ 的完整数据集的 OLS 估计量。$\\hat{\\beta}$ 的估计方差-协方差矩阵为 $\\widehat{\\mathrm{Var}}(\\hat{\\beta}) = s^2(X^\\top X)^{-1}$，其中 $s^2$ 是估计的误差方差。设 $\\hat{\\beta}_{(-i)}$ 和 $s_{(-i)}^2$ 为删除观测值 $i$ 时的相应量。不含第 $i$ 行的设计矩阵为 $X_{(-i)}$。\n\n**1. $\\mathrm{COVRATIO}_i$ 分析**\n\n问题将 $\\mathrm{COVRATIO}_i$ 定义为 $\\frac{\\det(\\widehat{\\mathrm{Var}}(\\hat{\\beta}))}{\\det(\\widehat{\\mathrm{Var}}(\\hat{\\beta}_{(-i)}))}$。代入公式：\n$$ \\mathrm{COVRATIO}_i = \\frac{\\det(s^2(X^\\top X)^{-1})}{\\det(s_{(-i)}^2(X_{(-i)}^\\top X_{(-i)})^{-1})} = \\frac{s^{2p} \\det((X^\\top X)^{-1})}{s_{(-i)}^{2p} \\det((X_{(-i)}^\\top X_{(-i)})^{-1})} = \\left(\\frac{s}{s_{(-i)}}\\right)^{2p} \\frac{\\det(X_{(-i)}^\\top X_{(-i)})}{\\det(X^\\top X)} $$\n完整信息矩阵与病例删除后的信息矩阵之间的关系是 $X^\\top X = X_{(-i)}^\\top X_{(-i)} + x_i x_i^\\top$。使用矩阵行列式引理 $\\det(A + uv^\\top) = \\det(A)(1 + v^\\top A^{-1} u)$，我们有：\n$$ \\det(X^\\top X) = \\det(X_{(-i)}^\\top X_{(-i)}) (1 + x_i^\\top (X_{(-i)}^\\top X_{(-i)})^{-1} x_i) $$\n回归诊断中的一个标准恒等式是 $1 - h_{ii} = \\frac{1}{1 + x_i^\\top (X_{(-i)}^\\top X_{(-i)})^{-1} x_i}$。因此：\n$$ \\frac{\\det(X_{(-i)}^\\top X_{(-i)})}{\\det(X^\\top X)} = 1 - h_{ii} $$\n将此代回 $\\mathrm{COVRATIO}_i$ 的表达式：\n$$ \\mathrm{COVRATIO}_i = \\left(\\frac{s}{s_{(-i)}}\\right)^{2p} (1 - h_{ii}) $$\n问题陈述中说杠杆值 $h_{ii}$ 很高，所以 $h_{ii}$ 接近于 $1$。这使得项 $(1 - h_{ii})$ 成为一个很小的正数。此外，残差 $r_i$ 很小。一个小的残差意味着观测值 $i$ 很好地拟合了完整模型，移除它不太可能大幅改变估计的误差方差。因此，比率 $s/s_{(-i)}$ 预计接近于 $1$。主导项是 $(1-h_{ii})$，它将 $\\mathrm{COVRATIO}_i$ 的值推向小于 $1$。\n\n根本原因是 $\\hat{\\beta}$ 的精度与信息矩阵 $X^\\top X$ 的“大小”有关。添加一个高杠杆点 $x_i$（它远离数据中心）会将矩阵 $x_i x_i^\\top$ 加到 $X_{(-i)}^\\top X_{(-i)}$ 上，有效地扩大了数据点的凸包。这通常会增加信息矩阵的“大小”（具体来说，$\\det(X^\\top X)  \\det(X_{(-i)}^\\top X_{(-i)})$），从而减小方差-协方差矩阵 $(X^\\top X)^{-1}$ 的“大小”。方差-协方差矩阵的行列式，即广义方差，是衡量 $\\hat{\\beta}$ 置信椭球体积的指标。较小的体积意味着较高的*全局*精度。因此，一个高杠杆点可以提高估计的整体精度，导致 $\\mathrm{COVRATIO}_i  1$。\n\n**2. $\\mathrm{DFBETAS}_{i,j}$ 分析**\n\n$\\mathrm{DFBETAS}_{i,j}$ 衡量删除观测值 $i$ 对第 $j$ 个系数 $\\hat{\\beta}_j$ 的影响。系数向量的变化由下式给出：\n$$ \\hat{\\beta} - \\hat{\\beta}_{(-i)} = \\frac{(X^\\top X)^{-1} x_i r_i}{1 - h_{ii}} $$\n$\\mathrm{DFBETAS}_{i,j}$ 的分子是该向量的第 $j$ 个分量，即 $(\\hat{\\beta} - \\hat{\\beta}_{(-i)})_j$。完整表达式为：\n$$ \\mathrm{DFBETAS}_{i,j} = \\frac{(\\hat{\\beta}_j - \\hat{\\beta}_{j,(-i)})}{s_{(-i)}\\sqrt{((X^\\top X)^{-1})_{jj}}} = \\frac{((X^\\top X)^{-1} x_i)_j}{s_{(-i)}\\sqrt{((X^\\top X)^{-1})_{jj}}} \\frac{r_i}{1-h_{ii}} $$\n在这里，我们看到了杠杆值和残差的关键组合。尽管原始残差 $r_i$ 很小，但高杠杆值 $h_{ii}$ 使得分母 $(1-h_{ii})$ 也很小。它们的比率 $r_i / (1-h_{ii})$，即点 $i$ 相对于病例删除回归的残差，可能很大。这个被放大的残差效应随后通过向量 $((X^\\top X)^{-1} x_i)_j$ 分配给各个系数。\n\n这种影响是*方向性的*。观测值 $x_i$ 是 $p$ 维预测变量空间中的一个向量。它对系数的影响取决于其相对于由预测变量及其相关性定义的轴的方向。对于给定的年龄非常大且BMI很低的患者，向量 $x_i$ 在“年龄”和“BMI”维度上是极端的。如果“年龄”预测变量是设计矩阵 $X$ 的一个“信息不足的轴”（例如，由于共线性，或者仅仅因为在那个极端年龄下几乎没有其他点），那么信息矩阵 $X^\\top X$ 在该方向上相对较弱。包含/排除点 $i$ 可能会导致逆矩阵 $(X^\\top X)^{-1}$ 发生显著的“重新定向”，从而导致相应系数 $\\hat{\\beta}_{\\text{age}}$ 发生大的变化。而对“吸烟”或“治疗”等其他系数的影响可能可以忽略不计，如果 $x_i$ 的极端性在 $X$ 所张成的空间中与这些预测变量维度大致正交。\n\n**推导结论：** 这个明显的矛盾可以通过认识到一个标量的、全局的精度度量（$\\det(\\widehat{\\mathrm{Var}}(\\hat{\\beta}))$）与一个向量值的、分量特定的影响度量（$\\mathrm{DFBETAS}_i$）之间的区别来解决。一个高杠杆点可以稳定整个回归平面（减小联合置信椭球的体积），同时又可能以一种显著改变特定斜率的方式使其发生转动。\n\n### 逐项分析选项\n\n**A. 如果 $h_{ii}$ 很高但 $|r_i|$ 很小，那么该观测值不可能对任何系数产生影响，因为产生影响需要一个大的残差；因此，$\\mathrm{COVRATIO}_i  1$ 的发现证明该病例完全是良性的，不需要进行系数级别的诊断。**\n这个陈述根本上是错误的。影响是杠杆值和残差大小的共同函数。系数变化的公式 $\\hat{\\beta} - \\hat{\\beta}_{(-i)}$ 包含了项 $r_i / (1 - h_{ii})$。对于高杠杆值（$h_{ii} \\to 1$），即使 $r_i$ 很小，这一项也可能很大。声称一个高杠杆点因为其残差小而不可能有影响力是一个危险的误解。忽略系数级别诊断的结论是不正确的，并且在统计上是幼稚的。\n**结论：错误。**\n\n**B. 包含一个高杠杆的 $x_i$ 会将 $x_i x_i^\\top$ 加到 $X^\\top X$ 上，这会减小 $\\det\\{(X^\\top X)^{-1}\\}$ 并可能导致 $\\mathrm{COVRATIO}_i  1$（全局精度提高），而 $x_i$ 在预测变量空间中的几何方向仍然可以以一种改变特定系数的方式重新加权信息；为了解决这个问题，应检查系数特定的病例删除效应，如 $\\mathrm{DFBETAS}_{i,j}$、分量式库克距离、针对 $x_j$ 的增量变量图，并使用稳健回归进行敏感性分析。**\n该选项正确地提供了全局精度提高（$\\mathrm{COVRATIO}_i  1$）的机制，即基于向信息矩阵添加 $x_i x_i^\\top$ 及其逆矩阵行列式的减小。它也正确地认识到 $x_i$ 的几何方向可能导致对特定系数的影响。推荐的诊断方法列表——$\\mathrm{DFBETAS}$、分量式影响度量、增量变量图和稳健回归——是全面且完全适合调查和解决此问题的。\n**结论：正确。**\n\n**C. 因为 $\\mathrm{COVRATIO}_i$ 是全局比较全样本与病例删除后的精度，所以一个小于1的值保证了该观测值改善了模型的所有方面；因此，系数影响不可能发生，任何大的 $\\mathrm{DFBETAS}$ 值都必定是虚假的。**\n这个陈述做出了一个无效的概括。一个全局的精度度量并不能保证模型所有方面的改善，特别是单个参数估计的稳定性。问题陈述本身就是对此主张的一个直接反例。基于一个全局指标宣称观察到的大 $\\mathrm{DFBETAS}$ 值为“虚假的”，是统计推理中的一个严重错误。\n**结论：错误。**\n\n**D. 参数影响是方向性的：即使 $|r_i| \\approx 0$，一个大的 $h_{ii}$ 加上 $x_i$ 与 $X$ 的一个信息不足的轴对齐，也可能通过病例删除时 $(X^\\top X)^{-1}$ 的重新定向，导致某些系数发生显著变化；适当的诊断方法包括对每个系数计算 $\\mathrm{DFBETAS}_{i,j}$、检查条件指数和方差分解比例以评估共线性、进行局部影响分析，以及进行有针对性的留一法重拟合以量化敏感性。**\n该选项为参数特定的影响提供了极好且深入的解释。它正确地指出了影响的方向性，并精确描述了其潜在机制：极端点与设计矩阵的“信息不足的轴”对齐，导致 $(X^\\top X)^{-1}$ 的“重新定向”。推荐的诊断方法也同样非常合适，并且与选项B中的方法互补，侧重于诊断设计矩阵的结构（共线性诊断）和正式的敏感性分析（局部影响、留一法）。这个解释富有洞察力且技术上精确。\n**结论：正确。**\n\n由于选项 B 和 D 都提供了正确的解释并推荐了适当的行动，它们都是有效的答案。选项 B 对 $\\mathrm{COVRATIO}$ 和 $\\mathrm{DFBETAS}$ 现象给出了更均衡的解释，而选项 D 则对方向性影响提供了更深入、更专业的解释。它们共同构成了一幅全面的图景。",
            "answer": "$$\\boxed{BD}$$"
        },
        {
            "introduction": "识别离群点和强影响点需要设定恰当的阈值，但教科书中的“经验法则”在实践中往往并不可靠。这个动手编程练习  将指导你实现一种更严谨、更现代的替代方法：使用残差自助法（bootstrap）来生成针对特定数据的经验阈值。这项强大的技术使你能够有效控制族错误率（family-wise error rate），为判断一个观测点是否异常提供了更坚实的统计基础。",
            "id": "4959122",
            "problem": "考虑一个用于连续医学结果的标准线性模型，其中响应向量 $y \\in \\mathbb{R}^{n}$ 被建模为 $y = X\\beta + \\varepsilon$，其中 $X \\in \\mathbb{R}^{n \\times p}$ 是设计矩阵（包含一个截距列），$\\beta \\in \\mathbb{R}^{p}$ 是系数向量，$\\varepsilon \\in \\mathbb{R}^{n}$ 是噪声向量。普通最小二乘 (OLS) 估计量 $\\hat{\\beta}$ 最小化残差平方和，并得到拟合值 $\\hat{y} = X\\hat{\\beta}$ 和残差 $e = y - \\hat{y}$。OLS 的帽子矩阵为 $H = X(X^{\\top}X)^{-1}X^{\\top}$，杠杆值为 $H$ 的对角元素 $h_{ii}$。残差方差估计量为 $s^{2} = \\frac{1}{n - p}\\sum_{i=1}^{n} e_{i}^{2}$。\n\n将观测值 $i$ 的外学生化残差定义为\n$$\nr_{i} = \\frac{e_{i}}{s_{(i)}\\sqrt{1 - h_{ii}}}, \n$$\n其中 $s_{(i)}^{2}$ 是在移除观测值 $i$ 后重新拟合模型计算得到的残差方差估计量。同样，将观测值 $i$ 的 Cook 距离定义为\n$$\nD_{i} = \\frac{e_{i}^{2}}{p\\,s^{2}} \\cdot \\frac{h_{ii}}{(1 - h_{ii})^{2}}.\n$$\n\n您需要实现一种有原则的阈值方法，通过带重拟合的残差自助法获得的经验零分布来检测离群点和强影响数据点。经验零分布是在模型 $y = X\\hat{\\beta} + \\varepsilon^{*}$ 下定义的，其中 $\\varepsilon^{*}$ 是通过对中心化后的残差进行重抽样生成的。检验统计量族是用于离群点检测的绝对外学生化残差 $\\{|r_{i}|\\}_{i=1}^{n}$ 和用于强影响点检测的 Cook 距离 $\\{D_{i}\\}_{i=1}^{n}$。为了在水平 $\\alpha$ 上控制每个统计量族的族错误率 (FWER)，请使用在自助法重复中各观测值 $i$ 上的最大统计量的经验分布：\n- 对于 $b = 1, \\dots, B$，构建自助法响应 $y^{*(b)} = X\\hat{\\beta} + e^{*(b)}$，其中 $e^{*(b)}$ 是通过从 $\\{e_{i} - \\bar{e}\\}_{i=1}^{n}$ 中有放回地抽样得到的，$\\bar{e}$ 是平均残差。将模型重新拟合到 $y^{*(b)}$ 上，以计算两种统计量族，并记录 $M_{r}^{(b)} = \\max_{1 \\le i \\le n} |r_{i}^{*(b)}|$ 和 $M_{D}^{(b)} = \\max_{1 \\le i \\le n} D_{i}^{*(b)}$。\n- 将阈值 $t_{r}$ 和 $t_{D}$ 分别定义为 $\\{M_{r}^{(b)}\\}_{b=1}^{B}$ 和 $\\{M_{D}^{(b)}\\}_{b=1}^{B}$ 的经验 $(1 - \\alpha)$-分位数。\n- 如果 $|r_{i}|  t_{r}$，则将观测值 $i$ 标记为离群点；如果 $D_{i}  t_{D}$，则标记为强影响点。\n\n您的程序必须实现上述过程，并为每个测试用例返回两个已排序的、从零开始的索引列表：检测到的离群点集合和检测到的强影响点集合。\n\n不涉及物理单位，因此答案中不需要。不涉及角度。如果 $\\alpha$ 需要小数或分数，请以小数形式表示（例如，$0.05$）。最终输出必须是整数列表。\n\n测试套件：\n实现以下 $4$ 个测试用例。在每个用例中，通过将一个截距列（全为 1）与指定的单个预测变量列 $x$ 连接来构建 $X$。对于所有用例，根据给定的公式确定性地定义 $y$，不进行任何随机抽取。\n\n- 用例 1（基准线，理想路径）：\n  - 样本量 $n = 20$，参数数量 $p = 2$。\n  - 预测变量值 $x_{i} = \\frac{i}{19}$，对于 $i = 0, 1, \\dots, 19$。\n  - 噪声偏移 $\\varepsilon_{i}$ 由列表给出\n    $[0.3, -0.4, 0.8, -0.2, 0.1, -0.7, 0.9, -0.6, 0.2, -1.1, 0.3, 0.4, -0.2, 0.7, -0.5, 0.6, -0.8, 0.2, -0.4, 0.1]$。\n  - 响应值 $y_{i} = 100 + 5 x_{i} + \\varepsilon_{i}$。\n  - 自助法重复次数 $B = 300$，FWER 水平 $\\alpha = 0.05$。\n\n- 用例 2（单个垂直离群点）：\n  - 使用与用例 1 相同的 $x_{i}$ 和 $\\varepsilon_{i}$。\n  - 按用例 1 构建基准线 $y_{i}$，然后在索引 $i = 10$ 处添加一个大小为 $+8.0$ 的单一偏差；即，设置 $y_{10} \\leftarrow y_{10} + 8.0$。\n  - 自助法重复次数 $B = 300$，FWER 水平 $\\alpha = 0.05$。\n\n- 用例 3（高杠杆强影响点）：\n  - 从用例 1 的数据开始，并附加一个额外的观测值，得到 $n = 21$。\n  - 附加 $x_{20} = 5.0$。\n  - 定义附加的响应值为 $y_{20} = 100 + 5 \\cdot x_{20} + 8.5 = 133.5$（即，在高杠杆 $x$ 处的一个中等正偏差）。\n  - 自助法重复次数 $B = 300$，FWER 水平 $\\alpha = 0.05$。\n\n- 用例 4（小样本，较温和的偏差，不同水平）：\n  - 样本量 $n = 8$，参数数量 $p = 2$。\n  - 预测变量值 $x_{i} = \\frac{i}{7}$，对于 $i = 0, 1, \\dots, 7$。\n  - 噪声偏移 $\\varepsilon_{i}$ 由列表给出 $[0.1, -0.2, 0.0, 0.3, -0.4, 0.2, -0.1, 0.0]$。\n  - 响应值 $y_{i} = 100 + 5 x_{i} + \\varepsilon_{i}$，然后在索引 $i = 3$ 处添加一个较温和的大小为 $+3.0$ 的偏差：设置 $y_{3} \\leftarrow y_{3} + 3.0$。\n  - 自助法重复次数 $B = 300$，FWER 水平 $\\alpha = 0.10$。\n\n算法要求：\n- 通过矩阵代数实现 OLS，以获得 $\\hat{\\beta}$、$\\hat{y}$、$e$、$H$ 和 $h_{ii}$。\n- 使用 OLS 下有效的代数恒等式计算 $s^{2}$ 和 $s_{(i)}^{2}$，而无需重拟合 $n$ 次。通过在分母中应用安全防护措施，确保在 $1 - h_{ii}$ 非常小的情况下的数值稳定性。\n- 为 $B$ 次重复实现带重拟合的残差自助法，以获得经验最大值 $M_{r}^{(b)}$ 和 $M_{D}^{(b)}$ 以及在期望的 $\\alpha$ 水平下的阈值 $t_{r}$ 和 $t_{D}$。\n- 对于每个测试用例，返回两个已排序的列表：满足 $|r_{i}|  t_{r}$ 的索引 $i$ 和满足 $D_{i}  t_{D}$ 的索引 $i$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。此列表的每个元素对应一个测试用例，并且本身必须是一个双元素列表，其中第一个元素是检测到的离群点索引的已排序列表，第二个元素是检测到的强影响点索引的已排序列表。例如，输出必须看起来像\n$[[[i\\_1,i\\_2], [j\\_1]], [[],[k\\_1,k\\_2]], \\dots]$\n所有索引都表示为整数，且列表按升序排列。不得打印任何其他文本。",
            "solution": "在线性建模中，有原则地检测离群点和强影响观测值的基础始于普通最小二乘法 (OLS) 和帽子矩阵的几何学。给定 $y = X\\beta + \\varepsilon$ 且 $X \\in \\mathbb{R}^{n \\times p}$，OLS 估计量为 $\\hat{\\beta} = (X^{\\top}X)^{-1} X^{\\top} y$，拟合值为 $\\hat{y} = X \\hat{\\beta}$，残差为 $e = y - \\hat{y}$。帽子矩阵 $H = X (X^{\\top}X)^{-1} X^{\\top}$ 将观测到的响应投影到 $X$ 的列空间上，其对角线项 $h_{ii}$ 量化了杠杆作用，即第 $i$ 个协变量模式对其自身拟合值的影响。\n\n为了量化相对于模型在响应方向上的离群性，外学生化残差对特定于观测值的方差和留一法不确定性进行了调整。观测值 $i$ 的外学生化残差为\n$$\nr_{i} = \\frac{e_{i}}{s_{(i)}\\sqrt{1 - h_{ii}}},\n$$\n其中 $s_{(i)}^{2}$ 是从移除观测值 $i$ 后重新拟合的模型中得到的残差方差估计量。恒等式\n$$\ns_{(i)}^{2} = \\frac{(n - p)s^{2} - \\frac{e_{i}^{2}}{1 - h_{ii}}}{n - p - 1}\n$$\n是从 Sherman–Morrison–Woodbury 引理和残差平方和的留一法分解中得到的。这个恒等式使得可以高效地计算 $s_{(i)}^{2}$ 而无需显式地重拟合 $n$ 个模型。它要求 $n - p - 1  0$，并且在 $1 - h_{ii}$ 接近零时需要数值保护措施。\n\n对拟合模型的影响由 Cook 距离量化，\n$$\nD_{i} = \\frac{e_{i}^{2}}{p\\,s^{2}} \\cdot \\frac{h_{ii}}{(1 - h_{ii})^{2}},\n$$\n它衡量了当删除观测值 $i$ 时 $\\hat{\\beta}$ 的总体变化，并根据残差方差和维度进行了归一化。较大的 $D_{i}$ 值表示强影响点，这通常是显著的杠杆作用 $h_{ii}$ 和不可忽略的残差大小的结合。\n\n$|r_{i}|$ 或 $D_{i}$ 的经典参数化阈值依赖于精确的高斯假设和多重比较校正。然而，医学数据常常表现出对理想化假设的偏离。为了以一种与模型无关的方式在水平 $\\alpha$ 上控制所有观测值的族错误率 (FWER)，我们通过带重拟合的残差自助法使用经验零分布：\n- 拟合模型以获得 $\\hat{\\beta}$、残差 $e$ 和拟合值 $\\hat{y}$。\n- 将残差中心化以强制其均值为零，$\\tilde{e}_{i} = e_{i} - \\bar{e}$，其中 $\\bar{e} = \\frac{1}{n}\\sum_{i=1}^{n} e_{i}$。在带截距的标准 OLS 下，$\\bar{e} = 0$，但中心化可以提高数值稳定性并保持零分布的对称性。\n- 对于 $b = 1, \\dots, B$：\n  1. 从 $\\{\\tilde{e}_{i}\\}_{i=1}^{n}$ 中有放回地抽样得到 $\\{e_{i}^{*(b)}\\}_{i=1}^{n}$。\n  2. 形成自助法响应 $y^{*(b)} = X\\hat{\\beta} + e^{*(b)}$。\n  3. 将 OLS 重新拟合到 $(X, y^{*(b)})$ 上，以获得 $e^{*(b)}$、$h_{ii}^{*(b)}$、$s^{2*(b)}$，并计算外学生化残差 $\\{r_{i}^{*(b)}\\}$ 和 Cook 距离 $\\{D_{i}^{*(b)}\\}$。\n  4. 记录最大值 $M_{r}^{(b)} = \\max_{i} |r_{i}^{*(b)}|$ 和 $M_{D}^{(b)} = \\max_{i} D_{i}^{*(b)}$。\n- 经过 $B$ 次重复后，经验阈值是 $(1 - \\alpha)$-分位数 $t_{r} = Q_{1 - \\alpha}\\left(\\{M_{r}^{(b)}\\}_{b=1}^{B}\\right)$ 和 $t_{D} = Q_{1 - \\alpha}\\left(\\{M_{D}^{(b)}\\}_{b=1}^{B}\\right)$。\n\n这种方法控制了所有观测值的 FWER，因为阈值是在将设计矩阵 $X$ 视为固定的情况下，根据拟合模型的残差结构下的最大统计量分布推导出来的。这些阈值能适应由杠杆作用引起的相关性以及残差变化的尺度。如果观测值的统计量超过这些阈值，则被标记：如果 $|r_{i}|  t_{r}$，观测值 $i$ 被视为离群点；如果 $D_{i}  t_{D}$，则被视为强影响点。\n\n实现的算法步骤：\n1. 用一个截距和指定的预测变量向量 $x$ 构建 $X$。\n2. 通过 OLS 恒等式计算 $\\hat{\\beta}$、$\\hat{y}$、$e$、$H$、$h_{ii}$ 和 $s^{2}$。\n3. 使用留一法恒等式计算 $s_{(i)}^{2}$，应用非负性和小分母保护措施：当 $(1 - h_{ii})$ 非常小时，使用一个最小下限（如 $10^{-12}$）以避免除以零；类似地，将 $s_{(i)}^{2}$ 限制在至少 $10^{-12}$ 以避免数值不稳定性。\n4. 计算观测数据的 $|r_{i}|$ 和 $D_{i}$。\n5. 执行 $B$ 次带重拟合的残差自助法重复，记录两个统计量族的最大值；计算经验 $(1 - \\alpha)$-分位数以得到 $t_{r}$ 和 $t_{D}$。\n6. 为每个测试用例返回两个已排序的索引列表：满足 $|r_{i}|  t_{r}$ 的索引 $i$ 和满足 $D_{i}  t_{D}$ 的索引 $i$。\n\n测试套件覆盖范围讨论：\n- 用例 1 提供了一个基准，具有中等噪声且没有注入异常；经验阈值应适应残差的尺度和设计几何，通常不会检测到任何点。\n- 用例 2 注入了一个单一的垂直离群点（在典型 $x$ 处 $y$ 有大的偏差），这应导致一个大的外学生化残差，并被 $|r_{i}|$ 准则检测到；如果杠杆作用保持典型水平，Cook 距离可能不太敏感。\n- 用例 3 附加了一个高杠杆点，其 $y$ 值有中等偏差；Cook 距离应该能检测到强影响，因为 $D_{i}$ 与 $h_{ii}$ 和 $e_{i}^{2}$ 都成比例，特别是通过 $(1 - h_{ii})^{-2}$ 项惩罚大的杠杆作用。\n- 用例 4 使用小样本和较温和的偏差，在不同的 FWER 水平 $\\alpha = 0.10$ 上测试灵敏度，并确保算法在较小的 $n$（其中 $n - p - 1$ 仍为正）下行为正确。\n\n最终程序必须精确实现这些步骤，并打印一行包含四个用例的结果，结果为一个由成对的已排序索引列表组成的列表，使用从零开始的索引，且无额外文本。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef add_intercept(x):\n    \"\"\"\n    Construct design matrix X with intercept and single predictor column.\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    ones = np.ones_like(x)\n    X = np.column_stack([ones, x])\n    return X\n\ndef ols_fit(X, y):\n    \"\"\"\n    Compute OLS estimates, fitted values, residuals, residual variance, and hat matrix diagonal.\n    \"\"\"\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y, dtype=float)\n    XtX = X.T @ X\n    XtX_inv = np.linalg.inv(XtX)\n    beta = XtX_inv @ (X.T @ y)\n    fitted = X @ beta\n    resid = y - fitted\n    n, p = X.shape\n    rss = float(resid.T @ resid)\n    s2 = rss / max(n - p, 1)  # guard against division by zero, though test cases ensure n  p\n    # Efficient computation of hat matrix diagonal: diag(X @ XtX_inv @ X^T)\n    # Each diagonal element h_i = x_i^T (X^T X)^{-1} x_i\n    H_diag = np.sum(X * (X @ XtX_inv), axis=1)\n    return beta, fitted, resid, s2, H_diag\n\ndef externally_studentized_residuals_abs(X, y):\n    \"\"\"\n    Compute absolute externally studentized residuals |r_i|.\n    \"\"\"\n    beta, fitted, resid, s2, h = ols_fit(X, y)\n    n, p = X.shape\n    # Safeguards for numerical stability\n    one_minus_h = np.maximum(1.0 - h, 1e-12)\n    # Leave-one-out residual variance using OLS identity:\n    # s_(i)^2 = ((n - p) * s^2 - e_i^2 / (1 - h_ii)) / (n - p - 1)\n    denom_df = max(n - p - 1, 1)  # guard; test cases ensure n - p - 1  0\n    s_loo_sq = ((n - p) * s2 - (resid ** 2) / one_minus_h) / denom_df\n    s_loo_sq = np.maximum(s_loo_sq, 1e-12)\n    r_ext = resid / (np.sqrt(one_minus_h) * np.sqrt(s_loo_sq))\n    return np.abs(r_ext)\n\ndef cooks_distance(X, y):\n    \"\"\"\n    Compute Cook's distance D_i for each observation.\n    \"\"\"\n    beta, fitted, resid, s2, h = ols_fit(X, y)\n    n, p = X.shape\n    one_minus_h = np.maximum(1.0 - h, 1e-12)\n    denom = p * s2 * (one_minus_h ** 2)\n    denom = np.maximum(denom, 1e-15)\n    D = (resid ** 2) * h / denom\n    return D\n\ndef bootstrap_thresholds(X, y, B=300, alpha=0.05, rng_seed=12345):\n    \"\"\"\n    Residual bootstrap with refitting to compute empirical (1 - alpha)-quantile thresholds\n    for the maxima of |r_i| and D_i across observations.\n    \"\"\"\n    # Fit on the original data\n    beta, fitted, resid, s2, h = ols_fit(X, y)\n    # Center residuals\n    resid_centered = resid - np.mean(resid)\n    n = len(y)\n    rng = np.random.default_rng(rng_seed)\n    max_r_values = np.empty(B, dtype=float)\n    max_D_values = np.empty(B, dtype=float)\n    for b in range(B):\n        resampled = rng.choice(resid_centered, size=n, replace=True)\n        y_star = fitted + resampled\n        # Recompute statistics on bootstrap sample\n        r_abs_star = externally_studentized_residuals_abs(X, y_star)\n        D_star = cooks_distance(X, y_star)\n        max_r_values[b] = np.max(r_abs_star)\n        max_D_values[b] = np.max(D_star)\n    thr_r = float(np.quantile(max_r_values, 1.0 - alpha))\n    thr_D = float(np.quantile(max_D_values, 1.0 - alpha))\n    return thr_r, thr_D\n\ndef case_1():\n    # n = 20, x_i = i/19\n    n = 20\n    x = np.linspace(0.0, 1.0, n)\n    eps = np.array([0.3, -0.4, 0.8, -0.2, 0.1, -0.7, 0.9, -0.6, 0.2, -1.1,\n                    0.3, 0.4, -0.2, 0.7, -0.5, 0.6, -0.8, 0.2, -0.4, 0.1], dtype=float)\n    y = 100.0 + 5.0 * x + eps\n    B = 300\n    alpha = 0.05\n    return x, y, B, alpha\n\ndef case_2():\n    # Same as case 1, but add +8 at index 10\n    x, y, B, alpha = case_1()\n    y2 = y.copy()\n    y2[10] = y2[10] + 8.0\n    return x, y2, B, alpha\n\ndef case_3():\n    # Case 1 appended with high-leverage x=5.0 and y=133.5\n    x, y, B, alpha = case_1()\n    x3 = np.concatenate([x, np.array([5.0])])\n    y3 = np.concatenate([y, np.array([133.5])])\n    return x3, y3, B, alpha\n\ndef case_4():\n    # n = 8, x_i = i/7, mild deviation +3 at index 3, alpha = 0.10\n    n = 8\n    x = np.linspace(0.0, 1.0, n)\n    eps = np.array([0.1, -0.2, 0.0, 0.3, -0.4, 0.2, -0.1, 0.0], dtype=float)\n    y = 100.0 + 5.0 * x + eps\n    y[3] = y[3] + 3.0\n    B = 300\n    alpha = 0.10\n    return x, y, B, alpha\n\ndef analyze_case(x, y, B, alpha):\n    \"\"\"\n    Perform bootstrap-based thresholding and return lists of outlier and influential indices.\n    \"\"\"\n    X = add_intercept(x)\n    thr_r, thr_D = bootstrap_thresholds(X, y, B=B, alpha=alpha, rng_seed=12345)\n    r_abs = externally_studentized_residuals_abs(X, y)\n    D = cooks_distance(X, y)\n    outliers = np.where(r_abs  thr_r)[0]\n    influentials = np.where(D  thr_D)[0]\n    return sorted(outliers.tolist()), sorted(influentials.tolist())\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        case_1(),\n        case_2(),\n        case_3(),\n        case_4(),\n    ]\n\n    results = []\n    for x, y, B, alpha in test_cases:\n        out_idx, inf_idx = analyze_case(x, y, B, alpha)\n        results.append([out_idx, inf_idx])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "与其简单地检测并移除离群点，一个更稳健的策略是构建一个本身对它们不那么敏感的模型。这个高级练习  旨在探索这种思想，它通过对比标准正态线性模型和一个重尾贝叶斯模型来实现。你将实现一种能自动降低极端观测点权重的算法，从而学习到稳健建模如何为处理强影响数据点提供一种主动性方法，以及如何在该框架内诊断它们的影响。",
            "id": "4959208",
            "problem": "考虑医学中患者水平测量的线性回归模型：对于每个患者索引 $i \\in \\{1,\\dots,n\\}$，标量响应 $y_i$ 通过 $y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$ 与标量协变量 $x_i$ 相关。假设 $\\varepsilon_i$ 在参数条件下是独立的。我们比较两种贝叶斯设定，它们仅在对观测特定偏差的正则化方式上有所不同。\n\n设定 $S_{\\mathrm{N}}$（轻尾）：$\\varepsilon_i \\mid \\sigma^2 \\sim \\mathcal{N}(0,\\sigma^2)$，其中 $\\sigma^2$ 有一个未指定的正常先验，系数向量 $(\\beta_0,\\beta_1)$ 有一个未指定的正常先验；这些选择在标准条件下保证了正常的后验，但在其他方面不具规定性。\n\n设定 $S_{\\mathrm{HT}}$（通过尺度混合实现的重尾）：引入潜尺度 $\\lambda_i$，并设置 $\\varepsilon_i \\mid \\lambda_i,\\sigma^2 \\sim \\mathcal{N}(0,\\lambda_i \\sigma^2)$，其中潜尺度 $\\lambda_i \\sim \\mathrm{Inv\\text{-}Gamma}(\\alpha,\\beta)$ 独立，$(\\alpha,\\beta)$ 为固定的超参数，$(\\beta_0,\\beta_1,\\sigma^2)$ 有未指定的正常先验。这种分层先验为 $y_i \\mid x_i$ 引入了重尾的边际误差，并被广泛用于减轻离群值的影响，方法是允许大的残差通过大的 $\\lambda_i$ 值来容纳，而不是通过极端的参数偏移。\n\n使用贝叶斯定理 $p(\\theta \\mid y) \\propto p(y \\mid \\theta)\\,p(\\theta)$（其中 $\\theta$ 收集了所有未知量）和尺度混合表示 $\\mathcal{N}(0,\\lambda_i \\sigma^2)$ 与 $\\lambda_i \\sim \\mathrm{Inv\\text{-}Gamma}(\\alpha,\\beta)$，推导遵循最大后验 (MAP) 原则的算法估计器，从条件独立性和正态线性模型的基本定义出发，不假设任何可以简化推导的闭式后验。\n\n您的程序必须为每种设定实现：\n- 一个有原则的迭代方案，对于 $S_{\\mathrm{HT}}$ 基于期望最大化 (EM) 算法，对于 $S_{\\mathrm{N}}$ 基于标准正规方程，从而为 $(\\beta_0,\\beta_1)$ 和 $\\sigma^2$ 产生与 MAP 等价的不动点估计，以及一组反映局部降权的观测特定权重 $w_i$。对于 $S_{\\mathrm{N}}$，权重是平坦的，即对所有 $i$，$w_i = 1$；对于 $S_{\\mathrm{HT}}$，EM 算法生成的 $w_i$ 是残差的函数，当 $|y_i - \\beta_0 - \\beta_1 x_i|$ 较大时，$w_i$ 会减小。\n- 从拟合的加权最小二乘法的帽子矩阵 $H$ 推导出的影响诊断。令 $X$ 表示 $n \\times 2$ 的设计矩阵，第一列为 $1$，第二列为 $x_i$，$W$ 是对角线上为 $w_i$ 的对角矩阵。定义加权帽子矩阵 $H = W^{1/2} X (X^\\top W X)^{-1} X^\\top W^{1/2}$，残差 $r_i = y_i - \\hat{y}_i$（其中 $\\hat{y} = X \\hat{\\beta}$），并使用留一法 (LOO) 恒等式 $r_i^{(-i)} = r_i / (1 - h_{ii})$，其中 $h_{ii}$ 是 $H$ 的第 $i$ 个对角元素。对于设定 $S_{\\mathrm{N}}$，将 LOO 预测方差近似为 $\\hat{\\sigma}^2/(1 - h_{ii})$。对于设定 $S_{\\mathrm{HT}}$，将 LOO 预测方差近似为 $(\\hat{\\sigma}^2 / w_i)/(1 - h_{ii})$。这些近似必须从加权最小二乘法中的线性投影和条件高斯性的第一性原理推导出来。\n- 后验预测检查 (PPCs)：对于每个 $i$，在近似的 LOO 预测分布下，计算双边尾部概率 $|Y_i - \\mu_i^{(-i)}| \\ge |y_i - \\mu_i^{(-i)}|$，其中 $\\mu_i^{(-i)} = y_i - r_i^{(-i)}$，方差如上所述。在方差为 $v_i^{(-i)}$ 的正态近似下，双边尾部概率为 $p_i = 2 \\left( 1 - \\Phi\\left( \\left| r_i^{(-i)} \\right|/\\sqrt{v_i^{(-i)}} \\right) \\right)$，其中 $\\Phi$ 是标准正态累积分布函数。如果 $p_i  0.05$（表示为小数 $0.05$），则将观测值 $i$ 标记为离群值。\n- 库克距离类似物：定义参数维度 $p = 2$，并计算 $D_i = \\frac{r_i^2}{p \\,\\hat{\\sigma}^2} \\cdot \\frac{h_{ii}}{(1 - h_{ii})^2}$ 用于 $S_{\\mathrm{N}}$。对于 $S_{\\mathrm{HT}}$，计算加权类似物，使用 $r_i^{(w)} = \\sqrt{w_i}\\,r_i$，即 $D_i^{(w)} = \\frac{(r_i^{(w)})^2}{p \\,\\hat{\\sigma}^2} \\cdot \\frac{h_{ii}}{(1 - h_{ii})^2}$。报告每种设定的在所有 $i$ 上的最大库克距离。\n\n测试套件：\n为以下确定性测试用例实现上述内容，每个用例提供 $(n,\\{x_i\\},\\{y_i\\},\\alpha,\\beta)$：\n- 情况 1（具有中等离群值的一般情况）：$n = 24$，$x_i = i/(n-1)$ 对于 $i = 0,\\dots,23$，基础信号 $y_i^{(0)} = 5 + 2 x_i + 0.1 \\sin(10 x_i)$，在索引 $i = 20$ 处有一个大小为 $+8$ 的加性离群值。因此对于所有 $i \\ne 20$，$y_i = y_i^{(0)}$，且 $y_{20} = y_{20}^{(0)} + 8$。重尾超参数：$(\\alpha,\\beta) = (2,2)$。\n- 情况 2（具有高杠杆率强离群值的边缘情况）：相同的 $x_i$ 和基础信号，但在索引 $i = 23$ 处有一个大小为 $+12$ 的加性离群值。重尾超参数：$(\\alpha,\\beta) = (2,2)$。\n- 情况 3（没有离群值的边缘情况）：相同的 $x_i$ 和基础信号，对于所有 $i$，$y_i = y_i^{(0)}$，其中 $(\\alpha,\\beta) = (2,2)$。\n\n对于每种情况，您的程序必须输出一行，其中包含跨三种情况的结果列表。对于每种情况，按顺序报告一个包含 4 个值的列表：在 $S_{\\mathrm{N}}$ 下标记的离群值数量（整数），在 $S_{\\mathrm{HT}}$ 下标记的离群值数量（整数），在 $S_{\\mathrm{N}}$ 下的最大库克距离（浮点数，四舍五入到 6 位小数），以及在 $S_{\\mathrm{HT}}$ 下的最大库克距离（浮点数，四舍五入到 6 位小数）。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含结果，格式为逗号分隔的案例列表，并用方括号括起来。例如，一个包含三种情况的输出应如下所示：$[[a_1,b_1,c_1,d_1],[a_2,b_2,c_2,d_2],[a_3,b_3,c_3,d_3]]$，其中每个 $a_k$ 和 $b_k$ 是整数，每个 $c_k$ 和 $d_k$ 是四舍五入到 6 位小数的浮点数。",
            "solution": "该问题要求推导并实现用于线性回归中离群值检测的统计方法，比较标准正态模型（$S_{\\mathrm{N}}$）与稳健的重尾模型（$S_{\\mathrm{HT}}$）。这涉及从基本原理推导参数估计方案和影响诊断。\n\n### 问题验证\n**步骤 1：提取已知条件**\n- **线性模型**：$y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$，对于 $i \\in \\{1, \\dots, n\\}$。\n- **设定 $S_{\\mathrm{N}}$**：$\\varepsilon_i \\mid \\sigma^2 \\sim \\mathcal{N}(0,\\sigma^2)$。$(\\beta_0, \\beta_1, \\sigma^2)$ 具有正常先验。通过正规方程进行估计。权重 $w_i = 1$。\n- **设定 $S_{\\mathrm{HT}}$**：$\\varepsilon_i \\mid \\lambda_i, \\sigma^2 \\sim \\mathcal{N}(0,\\lambda_i \\sigma^2)$，其中潜尺度 $\\lambda_i \\sim \\mathrm{Inv\\text{-}Gamma}(\\alpha,\\beta)$。$(\\beta_0, \\beta_1, \\sigma^2)$ 具有正常先验。通过期望最大化 (EM) 算法进行估计。权重 $w_i$ 是残差的函数。\n- **影响诊断**：\n    - 设计矩阵 $X$ 是一个 $n \\times 2$ 的矩阵，其列分别为 $1$ 和 $x_i$。\n    - 加权帽子矩阵 $H = W^{1/2} X (X^\\top W X)^{-1} X^\\top W^{1/2}$，其中 $W = \\mathrm{diag}(w_i)$。\n    - 留一法 (LOO) 残差恒等式：$r_i^{(-i)} = r_i / (1 - h_{ii})$，其中 $r_i = y_i - x_i^\\top\\hat{\\beta}$。\n    - $S_{\\mathrm{N}}$ 的近似 LOO 预测方差：$v_i^{(-i)} = \\hat{\\sigma}^2/(1 - h_{ii})$。\n    - $S_{\\mathrm{HT}}$ 的近似 LOO 预测方差：$v_i^{(-i)} = (\\hat{\\sigma}^2 / w_i)/(1 - h_{ii})$。\n- **后验预测检查 (PPCs)**：如果 $p_i  0.05$，则将观测值 $i$ 标记为离群值，其中 $p_i = 2 \\left( 1 - \\Phi\\left( |r_i^{(-i)}|/\\sqrt{v_i^{(-i)}} \\right) \\right)$，$\\Phi$ 是标准正态累积分布函数。\n- **库克距离类似物**：参数维度 $p=2$。\n    - 对于 $S_{\\mathrm{N}}$：$D_i = \\frac{r_i^2}{p \\hat{\\sigma}^2} \\cdot \\frac{h_{ii}}{(1 - h_{ii})^2}$。\n    - 对于 $S_{\\mathrm{HT}}$：$D_i^{(w)} = \\frac{w_i r_i^2}{p \\hat{\\sigma}^2} \\cdot \\frac{h_{ii}}{(1 - h_{ii})^2}$。\n- **测试用例**：提供了三个确定性用例，指定了 $n, \\{x_i\\}, \\{y_i\\}, \\alpha, \\beta$。\n- **输出**：对于每个用例，报告 `[# outliers SN, # outliers SHT, max Cook's D SN, max Cook's D SHT]`。\n\n**步骤 2：使用提取的已知条件进行验证**\n- **科学依据**：该问题在标准统计理论中有坚实的基础。使用正态分布的尺度混合（导致学生t分布的边际误差）进行稳健回归是一种经典技术。EM 算法是潜变量模型估计的标准方法。所有诊断（帽子矩阵、LOO 交叉验证、库克距离）都是回归分析的基础。\n- **良定性**：该问题是良定的。虽然它提到了“最大后验 (MAP)”估计的“未指定的正常先验”，但它通过指定算法（正规方程和 EM）立即阐明了预期的过程。这将求解器引向实际上是最大似然估计 (MLE) 的方法，这是一种常见且有效的方法，可以看作是平坦先验下的 MAP 估计。在温和的条件下，迭代方案保证收敛。\n- **客观性**：问题以精确的数学定义和客观标准陈述。测试用例是确定性的。\n\n**步骤 3：结论和行动**\n该问题在科学上是合理的、良定的和客观的。它被认为是 **有效的**。我们继续进行求解。\n\n---\n\n### 估计器和诊断的推导\n\n令 $\\beta = [\\beta_0, \\beta_1]^\\top$ 为系数向量，$X$ 为 $n \\times 2$ 的设计矩阵，其第 $i$ 行为 $x_i^\\top = [1, x_i]$。\n\n#### 设定 $S_{\\mathrm{N}}$ (标准正态模型)\n\n**1. 参数估计**\n问题要求进行 MAP 估计。在先验未指定的情况下，我们寻找使数据似然最大化的参数值。正态模型的对数似然函数为：\n$$ \\mathcal{L}(\\beta, \\sigma^2; y, X) = \\log p(y | X, \\beta, \\sigma^2) = \\sum_{i=1}^n \\log \\mathcal{N}(y_i; x_i^\\top\\beta, \\sigma^2) $$\n$$ \\mathcal{L} = -\\frac{n}{2}\\log(2\\pi) - \\frac{n}{2}\\log(\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n (y_i - x_i^\\top\\beta)^2 $$\n为了找到 MAP/MLE 估计 $\\hat{\\beta}$，我们最小化平方误差和项 $\\sum_{i=1}^n (y_i - x_i^\\top\\beta)^2 = \\|y - X\\beta\\|^2$。这是普通最小二乘 (OLS) 准则。解由正规方程 $X^\\top X \\hat{\\beta} = X^\\top y$ 给出，从而得到：\n$$ \\hat{\\beta} = (X^\\top X)^{-1}X^\\top y $$\n为了找到 $\\hat{\\sigma}^2$，我们将 $\\mathcal{L}$ 对 $\\sigma^2$ 求导并令其为零，代入 $\\hat{\\beta}$：\n$$ \\frac{\\partial \\mathcal{L}}{\\partial \\sigma^2} = -\\frac{n}{2\\sigma^2} + \\frac{1}{2(\\sigma^2)^2}\\sum_{i=1}^n (y_i - x_i^\\top\\hat{\\beta})^2 = 0 \\implies \\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^n (y_i - x_i^\\top\\hat{\\beta})^2 = \\frac{1}{n} \\|y - X\\hat{\\beta}\\|^2 $$\n对于此模型，权重是均匀的，$w_i=1$ 对所有 $i$ 成立，所以 $W=I_n$。\n\n**2. 诊断**\n- **帽子矩阵和 LOO 残差**：帽子矩阵为 $H = X(X^\\top X)^{-1}X^\\top$。对角元素 $h_{ii}$ 是杠杆值。留一法 (LOO) 残差恒等式 $r_i^{(-i)} = r_i / (1 - h_{ii})$ 是线性投影代数中的一个标准结果。\n- **LOO 预测方差**：对 $y_i$ 的 LOO 预测是 $\\mu_i^{(-i)} = x_i^\\top \\hat{\\beta}^{(-i)}$。预测误差 $Y_i - \\mu_i^{(-i)}$ 的方差是观测方差和预测方差之和：$\\mathrm{Var}(Y_i - \\mu_i^{(-i)}) = \\mathrm{Var}(\\varepsilon_i) + \\mathrm{Var}(x_i^\\top \\hat{\\beta}^{(-i)})$。这可以计算为 $\\sigma^2 + \\sigma^2 x_i^\\top(X_{(-i)}^\\top X_{(-i)})^{-1}x_i$。使用 Sherman-Morrison-Woodbury 公式，可以证明 $x_i^\\top(X_{(-i)}^\\top X_{(-i)})^{-1}x_i = h_{ii}/(1-h_{ii})$。因此，精确的 LOO 预测方差是：\n$$ \\mathrm{Var}(Y_i - \\mu_i^{(-i)}) = \\sigma^2 \\left(1 + \\frac{h_{ii}}{1-h_{ii}}\\right) = \\frac{\\sigma^2}{1-h_{ii}} $$\n使用我们的估计 $\\hat{\\sigma}^2$，我们得到指定的近似 $v_i^{(-i)} = \\hat{\\sigma}^2 / (1-h_{ii})$。\n- **PPC p 值**：检验统计量是 $Z_i = (Y_i - \\mu_i^{(-i)}) / \\sqrt{v_i^{(-i)}}$，它近似服从 $\\mathcal{N}(0,1)$。该统计量变为 $|z_i| = |r_i^{(-i)}|/\\sqrt{v_i^{(-i)}} = |r_i| / \\sqrt{\\hat{\\sigma}^2(1-h_{ii})}$。双边 p 值为 $p_i = 2(1 - \\Phi(|z_i|))$。\n- **库克距离**：直接使用公式 $D_i = \\frac{r_i^2}{p \\hat{\\sigma}^2} \\cdot \\frac{h_{ii}}{(1 - h_{ii})^2}$，其中 $p=2$。\n\n#### 设定 $S_{\\mathrm{HT}}$ (重尾模型)\n\n**1. 通过 EM 算法进行参数估计**\n该模型引入了潜变量 $\\lambda_i \\sim \\mathrm{Inv\\text{-}Gamma}(\\alpha, \\beta)$，使得直接最大化似然函数变得困难。我们使用 EM 算法，它迭代地计算完整数据对数似然的期望（E 步），然后将其最大化（M 步）。\n完整数据对数似然（在 $\\beta, \\log \\sigma$ 的平坦先验下与对数后验成正比）是：\n$$ \\log p(y, \\lambda | X, \\beta, \\sigma^2) \\propto \\sum_{i=1}^n \\left( \\log p(y_i|\\lambda_i, \\beta, \\sigma^2) + \\log p(\\lambda_i|\\alpha, \\beta) \\right) $$\n$$ \\propto \\sum_{i=1}^n \\left( -\\frac{1}{2}\\log(\\lambda_i \\sigma^2) - \\frac{(y_i - x_i^\\top\\beta)^2}{2\\lambda_i\\sigma^2} - (\\alpha+1)\\log\\lambda_i - \\frac{\\beta}{\\lambda_i} \\right) $$\n\n- **E 步**：在第 $t$ 次迭代，我们计算 $\\lambda_i$ 的充分统计量（即 $1/\\lambda_i$ 和 $\\log\\lambda_i$）在给定数据 $y$ 和当前参数 $\\theta^{(t)} = \\{\\beta^{(t)}, (\\sigma^2)^{(t)}\\}$ 条件下的期望。$\\lambda_i$ 的条件后验是：\n$$ p(\\lambda_i | y_i, \\theta^{(t)}) \\propto p(y_i|\\lambda_i, \\theta^{(t)}) p(\\lambda_i) \\propto \\lambda_i^{-1/2} e^{-\\frac{(r_i^{(t)})^2}{2\\lambda_i(\\sigma^2)^{(t)}}} \\cdot \\lambda_i^{-(\\alpha+1)} e^{-\\frac{\\beta}{\\lambda_i}} $$\n其中 $r_i^{(t)} = y_i - x_i^\\top \\beta^{(t)}$。这是逆伽玛分布的核：\n$$ \\lambda_i | y_i, \\theta^{(t)} \\sim \\mathrm{Inv\\text{-}Gamma}\\left(\\alpha' = \\alpha + \\frac{1}{2}, \\beta' = \\beta + \\frac{(r_i^{(t)})^2}{2(\\sigma^2)^{(t)}}\\right) $$\nM 步所需的期望是 $w_i^{(t+1)} = E[1/\\lambda_i | y, \\theta^{(t)}]$。对于一个 $\\mathrm{Inv\\text{-}Gamma}(a, b)$ 分布，$E[1/X] = a/b$。因此：\n$$ w_i^{(t+1)} = \\frac{\\alpha + 1/2}{\\beta + \\frac{(r_i^{(t)})^2}{2(\\sigma^2)^{(t)}}} = \\frac{2\\alpha + 1}{2\\beta + (r_i^{(t)})^2/(\\sigma^2)^{(t)}} $$\n这些是观测特定的权重。大的残差 $|r_i^{(t)}|$ 会导致小的权重 $w_i^{(t+1)}$，从而降低潜在离群值的影响。\n\n- **M 步**：我们最大化期望的完整数据对数似然 $Q(\\theta|\\theta^{(t)}) = E_{\\lambda|y,\\theta^{(t)}}[\\log p(y, \\lambda|\\theta)]$：\n$$ Q(\\theta|\\theta^{(t)}) \\propto -\\frac{n}{2}\\log(\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n w_i^{(t+1)} (y_i - x_i^\\top\\beta)^2 $$\n对 $\\beta$ 进行最大化是一个加权最小二乘 (WLS) 问题：\n$$ \\hat{\\beta}^{(t+1)} = (X^\\top W^{(t+1)} X)^{-1} X^\\top W^{(t+1)} y $$\n其中 $W^{(t+1)} = \\mathrm{diag}(w_i^{(t+1)})$。对 $\\sigma^2$ 进行最大化得到：\n$$ (\\hat{\\sigma}^2)^{(t+1)} = \\frac{1}{n} \\sum_{i=1}^n w_i^{(t+1)} (y_i - x_i^\\top\\hat{\\beta}^{(t+1)})^2 $$\nE 步和 M 步被迭代直到参数收敛。\n\n**2. 诊断**\n- **帽子矩阵和 LOO 残差**：WLS 拟合等价于对变换后的变量 $y^* = W^{1/2}y$ 和 $X^* = W^{1/2}X$ 进行 OLS 拟合。相关的帽子矩阵是针对这个变换后问题的，$H = X^*((X^*)^\\top X^*)^{-1}(X^*)^\\top = W^{1/2}X(X^\\top WX)^{-1}X^\\top W^{1/2}$，如问题所述。其对角元素是 $h_{ii} = w_i x_i^\\top (X^\\top WX)^{-1} x_i$。对于 $h_{ii}$ 的这个定义，LOO 残差恒等式 $r_i^{(-i)} = r_i / (1 - h_{ii})$ 仍然成立。\n- **LOO 预测方差**：在 $S_{HT}$ 模型中，WLS 公式意味着观测 $i$ 的有效误差方差为 $\\sigma^2/w_i$。LOO 预测方差的推导与 OLS 情况类似，但结合了这个观测特定的方差。总方差为 $\\mathrm{Var}(Y_i) + \\mathrm{Var}(\\text{prediction}) = \\frac{\\sigma^2}{w_i} + \\mathrm{Var}(x_i^\\top \\hat{\\beta}^{(-i)})$。第二项计算为 $\\frac{\\sigma^2}{w_i} \\frac{h_{ii}}{1-h_{ii}}$。将它们相加得到：\n$$ \\mathrm{Var}(Y_i - \\mu_i^{(-i)}) = \\frac{\\sigma^2}{w_i} \\left(1 + \\frac{h_{ii}}{1-h_{ii}}\\right) = \\frac{\\sigma^2}{w_i(1-h_{ii})} $$\n使用 EM 算法的估计值，我们得到近似 $v_i^{(-i)} = (\\hat{\\sigma}^2/w_i)/(1 - h_{ii})$。\n- **PPC p 值**：标准化的 LOO 残差是 $|z_i| = |r_i^{(-i)}|/\\sqrt{v_i^{(-i)}} = |r_i|\\sqrt{w_i} / \\sqrt{\\hat{\\sigma}^2(1-h_{ii})}$。p 值为 $p_i = 2(1 - \\Phi(|z_i|))$。\n- **库克距离**：公式 $D_i^{(w)} = \\frac{w_i r_i^2}{p \\hat{\\sigma}^2} \\cdot \\frac{h_{ii}}{(1 - h_{ii})^2}$ 是加权回归中标准库克距离的直接类似物。",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis for all test cases.\n    \"\"\"\n    \n    test_cases_params = [\n        {'n': 24, 'outlier_idx': 20, 'outlier_mag': 8, 'alpha': 2.0, 'beta_p': 2.0},\n        {'n': 24, 'outlier_idx': 23, 'outlier_mag': 12, 'alpha': 2.0, 'beta_p': 2.0},\n        {'n': 24, 'outlier_idx': None, 'outlier_mag': 0, 'alpha': 2.0, 'beta_p': 2.0},\n    ]\n\n    all_results = []\n    \n    for params in test_cases_params:\n        n = params['n']\n        x_vals = np.linspace(0, 1, n)\n        y_base = 5.0 + 2.0 * x_vals + 0.1 * np.sin(10.0 * x_vals)\n        y_vals = y_base.copy()\n        if params['outlier_idx'] is not None:\n            y_vals[params['outlier_idx']] += params['outlier_mag']\n        \n        alpha = params['alpha']\n        beta_p = params['beta_p']\n        \n        case_results = process_case(n, x_vals, y_vals, alpha, beta_p)\n        all_results.append(case_results)\n\n    # Format output\n    output_str = \"[\" + \",\".join([f\"[{r[0]},{r[1]},{r[2]:.6f},{r[3]:.6f}]\" for r in all_results]) + \"]\"\n    print(output_str)\n\ndef process_case(n, x_vals, y_vals, alpha, beta_p):\n    \"\"\"\n    Processes a single test case, performing analysis for S_N and S_HT models.\n    \"\"\"\n    X = np.stack([np.ones(n), x_vals], axis=1)\n    y = y_vals\n    p_dim = 2.0  # float for calculations\n\n    # ----- Specification S_N (Normal Model) -----\n    w_n = np.ones(n)\n    W_n = np.diag(w_n)\n    \n    # Parameter estimation\n    try:\n        beta_n = np.linalg.solve(X.T @ X, X.T @ y)\n    except np.linalg.LinAlgError:\n        # Fallback to pseudo-inverse if singular\n        beta_n = np.linalg.pinv(X.T @ X) @ X.T @ y\n        \n    r_n = y - X @ beta_n\n    sigma_sq_n = np.mean(r_n**2)\n    \n    # Hat matrix diagonals\n    try:\n        C_n = np.linalg.inv(X.T @ W_n @ X)\n        h_n = np.einsum('ij,jk,ki-i', X, C_n, X.T) * w_n\n    except np.linalg.LinAlgError:\n         h_n = np.full(n, 1.0/n) # Fallback for singular matrix\n    \n    # PPC outlier detection\n    # Prevent division by zero or sqrt of negative for h_ii near 1\n    h_n_safe = np.minimum(h_n, 1.0 - 1e-9)\n    z_n_denom = np.sqrt(sigma_sq_n * (1.0 - h_n_safe))\n    z_n = np.divide(np.abs(r_n), z_n_denom, out=np.zeros_like(z_n_denom), where=z_n_denom!=0)\n    p_vals_n = 2 * (1 - norm.cdf(z_n))\n    outliers_n = np.sum(p_vals_n  0.05)\n    \n    # Cook's distance\n    cook_d_denom = (1.0 - h_n_safe)**2\n    cook_d_n_term = np.divide(h_n_safe, cook_d_denom, out=np.zeros_like(cook_d_denom), where=cook_d_denom!=0)\n    cook_d_n = (r_n**2 / (p_dim * sigma_sq_n)) * cook_d_n_term\n    max_cook_d_n = np.max(cook_d_n)\n    \n    # ----- Specification S_HT (Heavy-Tailed Model) -----\n    # Initialize with S_N results\n    beta_ht = beta_n.copy()\n    sigma_sq_ht = sigma_sq_n\n    \n    n_iter = 100\n    for _ in range(n_iter):\n        r_ht_iter = y - X @ beta_ht\n        \n        # E-step: calculate weights\n        w_denom = (2.0 * beta_p + r_ht_iter**2 / sigma_sq_ht)\n        w_ht = np.divide(2.0 * alpha + 1.0, w_denom, out=np.zeros_like(w_denom), where=w_denom!=0)\n        W_ht = np.diag(w_ht)\n        \n        # M-step: update parameters\n        X_T_W_ht = X.T @ W_ht\n        try:\n           beta_ht = np.linalg.solve(X_T_W_ht @ X, X_T_W_ht @ y)\n        except np.linalg.LinAlgError:\n           beta_ht = np.linalg.pinv(X_T_W_ht @ X) @ (X_T_W_ht @ y)\n\n        r_ht_updated = y - X @ beta_ht\n        sigma_sq_ht = np.mean(w_ht * r_ht_updated**2)\n        if sigma_sq_ht  1e-12: sigma_sq_ht = 1e-12\n        \n    # Final values after convergence\n    r_ht = y - X @ beta_ht\n    w_ht_final = (2.0 * alpha + 1.0) / (2.0 * beta_p + r_ht**2 / sigma_sq_ht)\n    W_ht_final = np.diag(w_ht_final)\n    \n    # Hat matrix diagonals\n    try:\n        C_ht = np.linalg.inv(X.T @ W_ht_final @ X)\n        h_ht = w_ht_final * np.einsum('ij,jk,ki-i', X, C_ht, X.T)\n    except np.linalg.LinAlgError:\n        h_ht = np.full(n, 1.0/n)\n        \n    # PPC outlier detection\n    h_ht_safe = np.minimum(h_ht, 1.0 - 1e-9)\n    z_ht_denom = np.sqrt(sigma_sq_ht * (1.0 - h_ht_safe))\n    z_ht_num = np.abs(r_ht) * np.sqrt(w_ht_final)\n    z_ht = np.divide(z_ht_num, z_ht_denom, out=np.zeros_like(z_ht_denom), where=z_ht_denom!=0)\n    p_vals_ht = 2 * (1 - norm.cdf(z_ht))\n    outliers_ht = np.sum(p_vals_ht  0.05)\n    \n    # Weighted Cook's distance\n    cook_d_ht_denom = (1.0 - h_ht_safe)**2\n    cook_d_ht_term = np.divide(h_ht_safe, cook_d_ht_denom, out=np.zeros_like(cook_d_ht_denom), where=cook_d_ht_denom!=0)\n    cook_d_ht_num = w_ht_final * r_ht**2\n    sigma_sq_ht_safe = max(sigma_sq_ht, 1e-12)\n    cook_d_ht = (cook_d_ht_num / (p_dim * sigma_sq_ht_safe)) * cook_d_ht_term\n    max_cook_d_ht = np.max(cook_d_ht)\n\n    return outliers_n, outliers_ht, max_cook_d_n, max_cook_d_ht\n\nif __name__ == '__main__':\n    solve()\n\n```"
        }
    ]
}