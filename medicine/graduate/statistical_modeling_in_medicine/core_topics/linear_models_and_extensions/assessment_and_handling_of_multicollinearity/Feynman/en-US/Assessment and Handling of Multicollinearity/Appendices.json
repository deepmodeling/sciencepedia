{
    "hands_on_practices": [
        {
            "introduction": "The first step in diagnosing multicollinearity is to quantify the extent to which each predictor in a model is redundant. This exercise focuses on the Variance Inflation Factor (VIF), a fundamental and widely used diagnostic. By calculating VIF from a given correlation matrix in a clinical scenario, you will gain a concrete understanding of how it measures the inflation in the variance of a coefficient's estimate due to its linear relationship with other predictors .",
            "id": "4952398",
            "problem": "A biomedical research team is building a multiple linear regression model to study a continuous renal risk score $Y$ in a cohort of patients with chronic kidney disease. Three clinically relevant predictors are included: serum creatinine $X_{1}$, estimated glomerular filtration rate $X_{2}$, and age $X_{3}$. Each predictor has been standardized to zero mean and unit variance, yielding the design matrix with columns $(Z_{1}, Z_{2}, Z_{3})$. The sample correlation matrix of $(Z_{1}, Z_{2}, Z_{3})$ is\n$$\n\\mathbf{R} \\;=\\;\n\\begin{pmatrix}\n1 & -0.82 & 0.45 \\\\\n-0.82 & 1 & -0.60 \\\\\n0.45 & -0.60 & 1\n\\end{pmatrix}.\n$$\nStarting from the ordinary least squares (OLS) estimator and the definition of the Variance Inflation Factor (VIF), derive from first principles how the variance inflation for each coefficient arises from the inverse of the predictor correlation structure. Then, explicitly invert the matrix $\\mathbf{R}$ and compute $\\operatorname{VIF}_{j}$ for $j \\in \\{1,2,3\\}$. Finally, interpret each value in terms of redundancy of predictor $Z_{j}$ with respect to the other two predictors, based on the coefficient of determination $R^{2}_{j}$ when regressing $Z_{j}$ on the remaining predictors.\n\nRound your final numerical answers for $(\\operatorname{VIF}_{1}, \\operatorname{VIF}_{2}, \\operatorname{VIF}_{3})$ to four significant figures. Express the final vector of VIFs as a row matrix.",
            "solution": "The problem requires a derivation of the Variance Inflation Factor (VIF) from first principles, followed by its calculation and interpretation for a given set of standardized predictors.\n\n### Part 1: Derivation of VIF from First Principles\n\nThe standard multiple linear regression model is expressed in matrix form as:\n$$ \\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon} $$\nwhere $\\mathbf{y}$ is an $n \\times 1$ vector of observations of the response variable, $\\mathbf{X}$ is an $n \\times (p+1)$ design matrix (including an intercept column), $\\boldsymbol{\\beta}$ is a $(p+1) \\times 1$ vector of coefficients, and $\\boldsymbol{\\epsilon}$ is an $n \\times 1$ vector of errors with $\\mathbb{E}[\\boldsymbol{\\epsilon}] = \\mathbf{0}$ and $\\text{Var}(\\boldsymbol{\\epsilon}) = \\sigma^2 \\mathbf{I}_n$.\n\nThe Ordinary Least Squares (OLS) estimator for $\\boldsymbol{\\beta}$ is:\n$$ \\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y} $$\nThe covariance matrix of this estimator is:\n$$ \\text{Var}(\\hat{\\boldsymbol{\\beta}}) = \\text{Var}((\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}) = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T \\text{Var}(\\mathbf{y}) \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1} $$\nSince $\\text{Var}(\\mathbf{y}) = \\text{Var}(\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}) = \\text{Var}(\\boldsymbol{\\epsilon}) = \\sigma^2 \\mathbf{I}_n$, this simplifies to:\n$$ \\text{Var}(\\hat{\\boldsymbol{\\beta}}) = \\sigma^2 (\\mathbf{X}^T\\mathbf{X})^{-1} $$\nThe variance of a single coefficient estimator, $\\hat{\\beta}_j$, is the $j$-th diagonal element of this matrix:\n$$ \\text{Var}(\\hat{\\beta}_j) = \\sigma^2 [(\\mathbf{X}^T\\mathbf{X})^{-1}]_{jj} $$\nThe problem states that the predictors have been standardized to have a mean of zero and a variance of one. Let the $n \\times p$ matrix of these standardized predictors be $\\mathbf{Z} = [Z_1, Z_2, \\dots, Z_p]$. Because the predictors are centered, the intercept term is orthogonal to them. We can therefore focus on the coefficients of the predictors, and the relevant matrix is $\\mathbf{Z}^T\\mathbf{Z}$.\n\nThe $(i,j)$-th element of the $\\mathbf{Z}^T\\mathbf{Z}$ matrix is $\\sum_{k=1}^{n} Z_{ki}Z_{kj}$. By the definition of sample correlation for centered variables, the correlation between $Z_i$ and $Z_j$ is $r_{ij} = \\frac{\\sum_k Z_{ki}Z_{kj}}{\\sqrt{\\sum_k Z_{ki}^2 \\sum_k Z_{kj}^2}}$. Since the variables are standardized, their variance is $1$, and $\\sum_k Z_{ki}^2 = \\sum_k Z_{kj}^2 = n-1$. Thus, $r_{ij} = \\frac{\\sum_k Z_{ki}Z_{kj}}{n-1}$.\nThis implies that the predictor correlation matrix $\\mathbf{R}$ is related to $\\mathbf{Z}^T\\mathbf{Z}$ by:\n$$ \\mathbf{Z}^T\\mathbf{Z} = (n-1)\\mathbf{R} $$\nSubstituting this into the variance formula for the coefficients $\\hat{\\boldsymbol{\\beta}}_\\text{std}$ of the standardized model:\n$$ \\text{Var}(\\hat{\\boldsymbol{\\beta}}_\\text{std}) = \\sigma^2 (\\mathbf{Z}^T\\mathbf{Z})^{-1} = \\sigma^2 ((n-1)\\mathbf{R})^{-1} = \\frac{\\sigma^2}{n-1}\\mathbf{R}^{-1} $$\nThe variance of the $j$-th coefficient estimator is the $j$-th diagonal element:\n$$ \\text{Var}(\\hat{\\beta}_j) = \\frac{\\sigma^2}{n-1} [\\mathbf{R}^{-1}]_{jj} $$\nThe Variance Inflation Factor ($\\text{VIF}_j$) is defined as the factor by which $\\text{Var}(\\hat{\\beta}_j)$ is increased due to collinearity, compared to a baseline scenario where predictor $Z_j$ is orthogonal to all other predictors. In such an orthogonal case, the correlation matrix $\\mathbf{R}$ would be the identity matrix $\\mathbf{I}$, and its inverse $\\mathbf{R}^{-1}$ would also be $\\mathbf{I}$. The $j$-th diagonal element would be $[\\mathbf{I}]_{jj}=1$. The variance in this idealized case would be:\n$$ \\text{Var}(\\hat{\\beta}_j)_{\\text{ortho}} = \\frac{\\sigma^2}{n-1}(1) $$\nThe VIF is the ratio of the actual variance to this orthogonal-case variance:\n$$ \\operatorname{VIF}_j = \\frac{\\text{Var}(\\hat{\\beta}_j)}{\\text{Var}(\\hat{\\beta}_j)_{\\text{ortho}}} = \\frac{\\frac{\\sigma^2}{n-1} [\\mathbf{R}^{-1}]_{jj}}{\\frac{\\sigma^2}{n-1}} = [\\mathbf{R}^{-1}]_{jj} $$\nThis derivation shows that the VIF for the $j$-th predictor is precisely the $j$-th diagonal element of the inverse of the predictor correlation matrix $\\mathbf{R}$.\n\nFurthermore, it is a known property of the inverse of a correlation matrix that its diagonal elements are related to the coefficient of determination, $R^2_j$, obtained from regressing the $j$-th predictor $Z_j$ on the remaining $p-1$ predictors. The relationship is:\n$$ \\operatorname{VIF}_j = [\\mathbf{R}^{-1}]_{jj} = \\frac{1}{1 - R^2_j} $$\n\n### Part 2: Calculation of VIFs\n\nThe given correlation matrix is:\n$$ \\mathbf{R} = \\begin{pmatrix} 1 & -0.82 & 0.45 \\\\ -0.82 & 1 & -0.60 \\\\ 0.45 & -0.60 & 1 \\end{pmatrix} $$\nTo find the VIFs, we must compute $\\mathbf{R}^{-1}$. We first calculate the determinant of $\\mathbf{R}$:\n$$ \\det(\\mathbf{R}) = 1(1 \\cdot 1 - (-0.60)^2) - (-0.82)((-0.82) \\cdot 1 - (-0.60) \\cdot 0.45) + 0.45((-0.82)(-0.60) - 1 \\cdot 0.45) $$\n$$ \\det(\\mathbf{R}) = 1(1 - 0.36) + 0.82(-0.82 + 0.27) + 0.45(0.492 - 0.45) $$\n$$ \\det(\\mathbf{R}) = 0.64 + 0.82(-0.55) + 0.45(0.042) $$\n$$ \\det(\\mathbf{R}) = 0.64 - 0.451 + 0.0189 = 0.2079 $$\nNext, we find the adjugate of $\\mathbf{R}$, which is the transpose of its cofactor matrix $\\mathbf{C}$.\n$$ C_{11} = 1 - (-0.60)^2 = 1 - 0.36 = 0.64 $$\n$$ C_{22} = 1 - (0.45)^2 = 1 - 0.2025 = 0.7975 $$\n$$ C_{33} = 1 - (-0.82)^2 = 1 - 0.6724 = 0.3276 $$\nSince $\\mathbf{R}$ is symmetric, its cofactor matrix is symmetric, and we only need the diagonal elements for the VIFs.\nThe inverse matrix is $\\mathbf{R}^{-1} = \\frac{1}{\\det(\\mathbf{R})} \\text{adj}(\\mathbf{R})$. The diagonal elements are:\n$$ [\\mathbf{R}^{-1}]_{11} = \\frac{C_{11}}{\\det(\\mathbf{R})} = \\frac{0.64}{0.2079} $$\n$$ [\\mathbf{R}^{-1}]_{22} = \\frac{C_{22}}{\\det(\\mathbf{R})} = \\frac{0.7975}{0.2079} $$\n$$ [\\mathbf{R}^{-1}]_{33} = \\frac{C_{33}}{\\det(\\mathbf{R})} = \\frac{0.3276}{0.2079} $$\nNow we compute the VIFs:\n$$ \\operatorname{VIF}_1 = [\\mathbf{R}^{-1}]_{11} \\approx 3.07840307... $$\n$$ \\operatorname{VIF}_2 = [\\mathbf{R}^{-1}]_{22} \\approx 3.83645983... $$\n$$ \\operatorname{VIF}_3 = [\\mathbf{R}^{-1}]_{33} \\approx 1.57575757... $$\nRounding to four significant figures, we get:\n$$ \\operatorname{VIF}_1 \\approx 3.078 $$\n$$ \\operatorname{VIF}_2 \\approx 3.836 $$\n$$ \\operatorname{VIF}_3 \\approx 1.576 $$\n\n### Part 3: Interpretation\n\nWe interpret these values using the relationship $\\operatorname{VIF}_j = 1/(1 - R^2_j)$, which implies $R^2_j = 1 - 1/\\operatorname{VIF}_j$.\n\nFor predictor $Z_1$ (serum creatinine):\n- $\\operatorname{VIF}_1 = 3.078$. This means the variance of the coefficient for serum creatinine is $3.078$ times larger than it would be if serum creatinine were uncorrelated with eGFR and age.\n- $R^2_1 = 1 - 1/3.0784... = 1 - 0.3248... = 0.6751...$. This indicates that about $67.5\\%$ of the variance in standardized serum creatinine ($Z_1$) is explained by a linear combination of standardized eGFR ($Z_2$) and age ($Z_3$). This level of collinearity is moderate.\n\nFor predictor $Z_2$ (eGFR):\n- $\\operatorname{VIF}_2 = 3.836$. The variance of the coefficient for eGFR is $3.836$ times larger than it would be in the absence of collinearity.\n- $R^2_2 = 1 - 1/3.8364... = 1 - 0.2606... = 0.7393...$. This indicates that about $73.9\\%$ of the variance in standardized eGFR ($Z_2$) is explained by standardized serum creatinine ($Z_1$) and age ($Z_3$). This represents a moderate to high level of redundancy, which is clinically expected since eGFR is often calculated from serum creatinine.\n\nFor predictor $Z_3$ (age):\n- $\\operatorname{VIF}_3 = 1.576$. The variance of the coefficient for age is $1.576$ times larger than it would be in the absence of collinearity.\n- $R^2_3 = 1 - 1/1.5757... = 1 - 0.6346... = 0.3653...$. This indicates that about $36.5\\%$ of the variance in standardized age ($Z_3$) is explained by standardized serum creatinine ($Z_1$) and eGFR ($Z_2$). This is a low level of collinearity, suggesting that age provides relatively unique information compared to the other two predictors.\n\nIn summary, the VIF values quantify the degree of multicollinearity. $Z_1$ and particularly $Z_2$ show moderate collinearity, with $Z_2$ (eGFR) being the most redundant predictor, largely because it is functionally related to $Z_1$ (serum creatinine). $Z_3$ (age) is the least redundant.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n3.078 & 3.836 & 1.576\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "While VIFs are excellent for assessing individual predictors, they may not reveal more complex dependencies involving three or more variables. This practice introduces a more powerful diagnostic based on the eigenvalues of the predictor correlation matrix, leading to the calculation of condition indices . This approach provides a global assessment of model stability, helping you identify the number and severity of near-linear relationships that could compromise your regression estimates.",
            "id": "4952344",
            "problem": "A clinical researcher fits a multiple linear regression to model the logarithm of one-year hospitalization cost using $p=6$ predictors derived from a registry of patients with hypertension: age, body mass index, systolic blood pressure, diastolic blood pressure, estimated glomerular filtration rate, and an antihypertensive medication burden score. Each predictor is centered to zero mean and scaled to unit variance prior to constructing the design matrix $X \\in \\mathbb{R}^{n \\times p}$. Let $X^{\\top}X$ denote the $p \\times p$ Gram matrix. Suppose that an eigendecomposition of $X^{\\top}X$ yields the eigenvalues (ordered from largest to smallest)\n$$\n\\{\\lambda_{1},\\lambda_{2},\\lambda_{3},\\lambda_{4},\\lambda_{5},\\lambda_{6}\\}=\\{220,95,25,3.2,0.50,0.020\\}.\n$$\nNote that whether $X^{\\top}X$ is divided by $n$ does not affect any dimensionless ratios used below.\n\nStarting only from the properties of the spectral decomposition of a real symmetric positive definite matrix and the definition of singular values of $X$, do the following:\n- Derive a dimensionless diagnostic, computed from $\\{\\lambda_{i}\\}$, that assesses the degree to which $X^{\\top}X$ is nearly singular in specific directions, and explain why this diagnostic is relevant for the stability of ordinary least squares (OLS) estimates.\n- Compute this diagnostic for each $\\lambda_{i}$ above.\n- State explicit numeric thresholds you adopt to flag moderate and severe multicollinearity, and justify these thresholds based on how near-singularity of $X^{\\top}X$ inflates components of the OLS covariance.\n- Identify which of the computed diagnostics indicate problematic multicollinearity under your thresholds.\n\nFinally, report the largest value of this diagnostic for the given eigenvalues. Round your final reported number to four significant figures.",
            "solution": "The primary task is to derive and apply a diagnostic for multicollinearity using the eigenvalues of the Gram matrix $X^{\\top}X$. Multicollinearity refers to the near-linear dependence among predictor variables in a regression model. Its presence can severely undermine the stability and interpretability of the ordinary least squares (OLS) coefficient estimates.\n\nThe OLS estimator for the coefficient vector $\\beta$ is given by:\n$$\n\\hat{\\beta} = (X^{\\top}X)^{-1}X^{\\top}y\n$$\nwhere $y$ is the vector of outcomes. The stability of $\\hat{\\beta}$ is directly related to the properties of the matrix $(X^{\\top}X)^{-1}$. The covariance matrix of the OLS estimator is:\n$$\n\\text{Cov}(\\hat{\\beta}) = \\sigma^2 (X^{\\top}X)^{-1}\n$$\nwhere $\\sigma^2$ is the variance of the model errors. The variance of the individual coefficient estimates, $\\text{Var}(\\hat{\\beta}_j)$, are the diagonal elements of this matrix. High variances imply that the estimates are imprecise and unstable.\n\nThe Gram matrix $X^{\\top}X$ is a real, symmetric, and positive semi-definite matrix. Assuming the columns of $X$ are linearly independent, $X^{\\top}X$ is positive definite. It admits a spectral decomposition:\n$$\nX^{\\top}X = V \\Lambda V^{\\top}\n$$\nwhere $V$ is a $p \\times p$ orthogonal matrix whose columns $v_k$ are the eigenvectors of $X^{\\top}X$, and $\\Lambda$ is a diagonal matrix containing the corresponding eigenvalues $\\lambda_k$, such that $\\Lambda = \\text{diag}(\\lambda_1, \\lambda_2, \\dots, \\lambda_p)$. The inverse is then:\n$$\n(X^{\\top}X)^{-1} = (V \\Lambda V^{\\top})^{-1} = V \\Lambda^{-1} V^{\\top}\n$$\nwhere $\\Lambda^{-1} = \\text{diag}(1/\\lambda_1, 1/\\lambda_2, \\dots, 1/\\lambda_p)$.\n\nThe total variance of the coefficient estimates is the trace of the covariance matrix:\n$$\n\\sum_{j=1}^{p} \\text{Var}(\\hat{\\beta}_j) = \\text{tr}(\\text{Cov}(\\hat{\\beta})) = \\text{tr}(\\sigma^2 V \\Lambda^{-1} V^{\\top}) = \\sigma^2 \\text{tr}(\\Lambda^{-1} V^{\\top}V) = \\sigma^2 \\text{tr}(\\Lambda^{-1}) = \\sigma^2 \\sum_{k=1}^{p} \\frac{1}{\\lambda_k}\n$$\nThis equation reveals that if any eigenvalue $\\lambda_k$ is close to zero, its reciprocal $1/\\lambda_k$ becomes very large, thus inflating the total variance of the coefficient estimates. This indicates that the OLS solution is unstable. A small eigenvalue $\\lambda_k$ signifies that the data exhibit very little variance in the direction of the corresponding eigenvector $v_k$, implying a near-linear relationship among the predictors.\n\nTo formalize a dimensionless diagnostic, we begin with the singular values of the design matrix $X$, which are defined as the square roots of the eigenvalues of $X^{\\top}X$. Let the singular values be $\\mu_k = \\sqrt{\\lambda_k}$. A standard measure of a matrix's conditioning and proximity to singularity is its condition number. For the design matrix $X$, the condition number is the ratio of its largest to smallest singular value, $\\kappa(X) = \\mu_{\\text{max}} / \\mu_{\\text{min}}$.\n\nA more detailed diagnostic, which assesses near-singularity in specific directions, is the set of **condition indices**. The $k$-th condition index, denoted $\\eta_k$, is defined as the ratio of the largest singular value to the $k$-th singular value:\n$$\n\\eta_k = \\frac{\\mu_{\\text{max}}}{\\mu_k} = \\frac{\\sqrt{\\lambda_{\\text{max}}}}{\\sqrt{\\lambda_k}}\n$$\nBy convention, the eigenvalues are ordered from largest to smallest, so $\\lambda_{\\text{max}} = \\lambda_1$. The diagnostic is therefore:\n$$\n\\eta_k = \\sqrt{\\frac{\\lambda_1}{\\lambda_k}} \\quad \\text{for } k = 1, 2, \\dots, p\n$$\nThis is a dimensionless diagnostic. A large value for $\\eta_k$ ($k > 1$) indicates that $\\lambda_k$ is much smaller than $\\lambda_1$, signaling a potential collinearity problem associated with the $k$-th principal component of the predictor space. This is the diagnostic required by the problem.\n\nNow, we compute this diagnostic for the given eigenvalues. The set of ordered eigenvalues is:\n$$\n\\{\\lambda_{1},\\lambda_{2},\\lambda_{3},\\lambda_{4},\\lambda_{5},\\lambda_{6}\\}=\\{220, 95, 25, 3.2, 0.50, 0.020\\}\n$$\nThe largest eigenvalue is $\\lambda_1 = 220$. The condition indices are calculated as follows:\n$$\n\\eta_1 = \\sqrt{\\frac{220}{220}} = \\sqrt{1} = 1\n$$\n$$\n\\eta_2 = \\sqrt{\\frac{220}{95}} \\approx \\sqrt{2.315789} \\approx 1.52177\n$$\n$$\n\\eta_3 = \\sqrt{\\frac{220}{25}} = \\sqrt{8.8} \\approx 2.96648\n$$\n$$\n\\eta_4 = \\sqrt{\\frac{220}{3.2}} = \\sqrt{68.75} \\approx 8.29156\n$$\n$$\n\\eta_5 = \\sqrt{\\frac{220}{0.50}} = \\sqrt{440} \\approx 20.97618\n$$\n$$\n\\eta_6 = \\sqrt{\\frac{220}{0.020}} = \\sqrt{11000} \\approx 104.88088\n$$\n\nNext, we establish and justify thresholds for identifying problematic multicollinearity. The following rules of thumb, based on the work of Belsley, Kuh, and Welsch, are widely adopted in practice:\n- **Moderate multicollinearity**: A condition index $\\eta_k$ in the range $[10, 30)$ is taken to indicate a moderate to strong linear dependency.\n- **Severe multicollinearity**: A condition index $\\eta_k \\ge 30$ is taken to indicate a severe linear dependency, which can seriously degrade the regression estimates.\n\nThe justification for these thresholds relates to the numerical stability of the linear system. A condition index $\\eta_k = 10$ implies that $\\lambda_1/\\lambda_k = 100$, and $\\eta_k = 30$ implies $\\lambda_1/\\lambda_k = 900$. An eigenvalue that is two to three orders of magnitude smaller than the largest eigenvalue represents a dimension in the predictor space that is nearly degenerate (i.e., the data cloud is nearly flat in that direction). This near-degeneracy creates instability, as small changes in the input data can lead to large swings in the coefficient estimates associated with that dimension. The thresholds serve as practical markers for when the variance inflation caused by small eigenvalues becomes a concern for statistical inference.\n\nFinally, we identify which of the computed diagnostics indicate problematic multicollinearity using these thresholds:\n- $\\eta_1, \\eta_2, \\eta_3, \\eta_4$: These are all less than $10$ and do not indicate a problem.\n- $\\eta_5 \\approx 20.98$: This value is between $10$ and $30$, indicating **moderate multicollinearity**.\n- $\\eta_6 \\approx 104.88$: This value is greater than $30$, indicating **severe multicollinearity**.\n\nThe two smallest eigenvalues, $\\lambda_5 = 0.50$ and particularly $\\lambda_6 = 0.020$, are the sources of the multicollinearity in this dataset. The largest value of this diagnostic is $\\eta_6$.\n\nThe problem asks for the largest value of this diagnostic, rounded to four significant figures:\n$$\n\\eta_6 = \\sqrt{11000} \\approx 104.88088...\n$$\nRounding to four significant figures yields $104.9$.",
            "answer": "$$\n\\boxed{104.9}\n$$"
        },
        {
            "introduction": "Multicollinearity doesn't always arise from naturally correlated predictors; sometimes it is an artifact of model specification, especially when including interaction terms. This practice explores the mathematical origin of the correlation between a main effect and an interaction term, such as the correlation between $X_1$ and $X_1 X_2$ . By deriving this relationship, you will see precisely why mean-centering predictors is a critical, and simple, first step to mitigate this \"non-essential\" collinearity, ensuring more stable and interpretable model coefficients.",
            "id": "4952441",
            "problem": "A clinical outcomes study models a continuous risk score as a linear function of two continuous biomarkers with an interaction term. To assess potential multicollinearity between a main effect and its interaction, consider a joint normal data-generating mechanism for the biomarkers. Let $(X_1, X_2)$ be jointly normal with mean vector $(\\mu_1, \\mu_2)$, variances $\\sigma_1^2$ and $\\sigma_2^2$, and correlation $\\rho \\in (-1,1)$. Define standardized variables $Z_1 = (X_1 - \\mu_1)/\\sigma_1$ and $Z_2 = (X_2 - \\mu_2)/\\sigma_2$, so that $Z_1$ and $Z_2$ have zero mean and unit variance with correlation $\\rho$.\n\nUsing only fundamental definitions of covariance, variance, and properties of moments of the multivariate normal distribution, do the following:\n\n1) Derive the closed-form analytic expression for the correlation $\\operatorname{Corr}(Z_1, Z_1 Z_2)$.\n\n2) Derive the closed-form analytic expression for the correlation $\\operatorname{Corr}(X_1, X_1 X_2)$ in terms of $\\mu_1$, $\\mu_2$, $\\sigma_1$, $\\sigma_2$, and $\\rho$.\n\n3) Explain, in the sense of a derivation grounded in the same fundamental definitions, how centering $X_1$ and $X_2$ (that is, replacing $X_j$ by $X_j - \\mu_j$) alters the correlation between the main effect and the interaction by giving the closed-form analytic expression for $\\operatorname{Corr}(X_1 - \\mu_1, (X_1 - \\mu_1)(X_2 - \\mu_2))$.\n\nProvide your final answer as a row matrix containing, in order, the expressions from items $1)$, $2)$, and $3)$. No numerical rounding is required. The answer must be a single closed-form analytic expression without units.",
            "solution": "The problem requires the derivation of three correlation expressions related to a pair of jointly normal random variables, their interaction product, and the effect of mean-centering. We will address each part systematically using fundamental definitions.\n\nThe correlation between two random variables $A$ and $B$ is defined as:\n$$\n\\operatorname{Corr}(A, B) = \\frac{\\operatorname{Cov}(A, B)}{\\sqrt{\\operatorname{Var}(A)\\operatorname{Var}(B)}}\n$$\nwhere $\\operatorname{Cov}(A, B) = \\mathbb{E}[AB] - \\mathbb{E}[A]\\mathbb{E}[B]$ and $\\operatorname{Var}(A) = \\mathbb{E}[A^2] - (\\mathbb{E}[A])^2$.\n\nThe variables of interest are derived from $(X_1, X_2)$, which are jointly normal with mean vector $(\\mu_1, \\mu_2)$, variances $\\sigma_1^2$ and $\\sigma_2^2$, and correlation $\\rho$. The standardized variables are $Z_1 = (X_1 - \\mu_1)/\\sigma_1$ and $Z_2 = (X_2 - \\mu_2)/\\sigma_2$. From the problem statement, we have the following fundamental moments for the standard bivariate normal variables $Z_1$ and $Z_2$:\n\\begin{itemize}\n    \\item $\\mathbb{E}[Z_1] = 0$, $\\mathbb{E}[Z_2] = 0$\n    \\item $\\mathbb{E}[Z_1^2] = \\operatorname{Var}(Z_1) = 1$, $\\mathbb{E}[Z_2^2] = \\operatorname{Var}(Z_2) = 1$\n    \\item $\\mathbb{E}[Z_1 Z_2] = \\operatorname{Cov}(Z_1, Z_2) = \\rho$\n\\end{itemize}\nA key property of the multivariate normal distribution is that any central moment of odd order is zero. For zero-mean variables like $Z_1$ and $Z_2$, this means any expectation of a product of an odd number of such variables is zero. For example, $\\mathbb{E}[Z_1^3]=0$, $\\mathbb{E}[Z_2^3]=0$, $\\mathbb{E}[Z_1^2 Z_2]=0$, $\\mathbb{E}[Z_1 Z_2^2]=0$.\n\nWe will also need some fourth-order moments, which can be derived using Isserlis' theorem (Wick's theorem for Gaussian variables). For zero-mean Gaussian variables $W_1, W_2, W_3, W_4$:\n$\\mathbb{E}[W_1 W_2 W_3 W_4] = \\mathbb{E}[W_1 W_2]\\mathbb{E}[W_3 W_4] + \\mathbb{E}[W_1 W_3]\\mathbb{E}[W_2 W_4] + \\mathbb{E}[W_1 W_4]\\mathbb{E}[W_2 W_3]$.\nUsing this, we find:\n\\begin{itemize}\n    \\item $\\mathbb{E}[Z_1^4] = 3(\\mathbb{E}[Z_1^2])^2 = 3(1)^2 = 3$.\n    \\item $\\mathbb{E}[Z_1^2 Z_2^2] = \\mathbb{E}[Z_1^2]\\mathbb{E}[Z_2^2] + 2(\\mathbb{E}[Z_1 Z_2])^2 = 1 \\cdot 1 + 2\\rho^2 = 1 + 2\\rho^2$.\n\\end{itemize}\n\n**1) Derivation of $\\operatorname{Corr}(Z_1, Z_1 Z_2)$**\n\nWe need to compute the covariance and variances.\nThe variance of $Z_1$ is given as $\\operatorname{Var}(Z_1) = 1$.\n\nThe covariance is $\\operatorname{Cov}(Z_1, Z_1 Z_2) = \\mathbb{E}[Z_1(Z_1 Z_2)] - \\mathbb{E}[Z_1]\\mathbb{E}[Z_1 Z_2] = \\mathbb{E}[Z_1^2 Z_2] - (0)\\mathbb{E}[Z_1 Z_2] = \\mathbb{E}[Z_1^2 Z_2]$.\nThis is a third-order moment of the standard bivariate normal distribution. As stated, all odd-order central moments of a normal distribution are zero. Thus, $\\mathbb{E}[Z_1^2 Z_2] = 0$.\n\nAlternatively, using the law of total expectation and the property of conditional expectation for bivariate normal variables, $\\mathbb{E}[Z_2|Z_1=z_1] = \\rho z_1$:\n$$\n\\mathbb{E}[Z_1^2 Z_2] = \\mathbb{E}[\\mathbb{E}[Z_1^2 Z_2 | Z_1]] = \\mathbb{E}[Z_1^2 \\mathbb{E}[Z_2 | Z_1]] = \\mathbb{E}[Z_1^2 (\\rho Z_1)] = \\rho \\mathbb{E}[Z_1^3]\n$$\nSince $Z_1 \\sim N(0,1)$, its third central moment $\\mathbb{E}[Z_1^3]$ is $0$. Therefore, $\\mathbb{E}[Z_1^2 Z_2] = 0$.\n\nWith the covariance being zero, the correlation is also zero:\n$$\n\\operatorname{Corr}(Z_1, Z_1 Z_2) = \\frac{0}{\\sqrt{\\operatorname{Var}(Z_1)\\operatorname{Var}(Z_1 Z_2)}} = 0\n$$\nThis holds provided $\\operatorname{Var}(Z_1 Z_2) \\neq 0$. Let's compute it for completeness:\n$\\operatorname{Var}(Z_1 Z_2) = \\mathbb{E}[(Z_1 Z_2)^2] - (\\mathbb{E}[Z_1 Z_2])^2 = \\mathbb{E}[Z_1^2 Z_2^2] - \\rho^2 = (1 + 2\\rho^2) - \\rho^2 = 1 + \\rho^2$.\nSince $\\rho \\in (-1,1)$, $\\rho^2 < 1$, so $1 + \\rho^2 > 1$. The variance is strictly positive, so the correlation is well-defined and is $0$.\n\n**3) Derivation of $\\operatorname{Corr}(X_1 - \\mu_1, (X_1 - \\mu_1)(X_2 - \\mu_2))$ and Explanation**\n\nThis part is addressed before part 2) because it is a direct generalization of part 1). Let $Y_1 = X_1 - \\mu_1$ and $Y_2 = X_2 - \\mu_2$. These are the mean-centered versions of $X_1$ and $X_2$.\nFrom the definition of the standardized variables, we have $Y_1 = \\sigma_1 Z_1$ and $Y_2 = \\sigma_2 Z_2$.\nWe are asked to compute $\\operatorname{Corr}(Y_1, Y_1 Y_2)$. Substituting the expressions in terms of $Z_1$ and $Z_2$:\n$$\n\\operatorname{Corr}(\\sigma_1 Z_1, (\\sigma_1 Z_1)(\\sigma_2 Z_2)) = \\operatorname{Corr}(\\sigma_1 Z_1, \\sigma_1 \\sigma_2 Z_1 Z_2)\n$$\nUsing the property that correlation is invariant to positive scaling, i.e., $\\operatorname{Corr}(aA, bB) = \\operatorname{Corr}(A, B)$ for constants $a > 0, b > 0$:\n$$\n\\operatorname{Corr}(\\sigma_1 Z_1, \\sigma_1 \\sigma_2 Z_1 Z_2) = \\operatorname{Corr}(Z_1, Z_1 Z_2)\n$$\nFrom part 1), this correlation is $0$.\n\n*Explanation*: The process of centering the variables $X_1$ and $X_2$ by subtracting their respective means $\\mu_1$ and $\\mu_2$ ensures that the resulting variables have a mean of zero. The covariance between the centered main effect $X_1 - \\mu_1$ and the centered interaction $(X_1 - \\mu_1)(X_2 - \\mu_2)$ reduces to the third-order central moment $\\mathbb{E}[(X_1-\\mu_1)^2(X_2-\\mu_2)]$. For any multivariate normal distribution, all odd-order central moments are zero. This forces the covariance, and therefore the correlation, to be exactly zero. In contrast, as shown in part 2, the correlation for uncentered variables is generally non-zero. This demonstrates that mean-centering is a fundamentally important technique in regression modeling to mitigate multicollinearity between main effects and their interaction terms.\n\n**2) Derivation of $\\operatorname{Corr}(X_1, X_1 X_2)$**\n\nWe calculate the terms for the correlation formula $\\operatorname{Corr}(X_1, X_1 X_2) = \\frac{\\operatorname{Cov}(X_1, X_1 X_2)}{\\sqrt{\\operatorname{Var}(X_1)\\operatorname{Var}(X_1 X_2)}}$.\nThe variance of $X_1$ is given as $\\operatorname{Var}(X_1) = \\sigma_1^2$.\n\nFirst, we find the numerator, $\\operatorname{Cov}(X_1, X_1 X_2) = \\mathbb{E}[X_1^2 X_2] - \\mathbb{E}[X_1]\\mathbb{E}[X_1 X_2]$. We express $X_1$ and $X_2$ in terms of $Z_1$ and $Z_2$: $X_1 = \\sigma_1 Z_1 + \\mu_1$ and $X_2 = \\sigma_2 Z_2 + \\mu_2$.\n\n\\textit{Expectation $\\mathbb{E}[X_1 X_2]$:}\n$$\n\\mathbb{E}[X_1 X_2] = \\mathbb{E}[(\\sigma_1 Z_1 + \\mu_1)(\\sigma_2 Z_2 + \\mu_2)] = \\mathbb{E}[\\sigma_1\\sigma_2 Z_1 Z_2 + \\sigma_1\\mu_2 Z_1 + \\sigma_2\\mu_1 Z_2 + \\mu_1\\mu_2]\n$$\n$$\n= \\sigma_1\\sigma_2 \\mathbb{E}[Z_1 Z_2] + \\sigma_1\\mu_2 \\mathbb{E}[Z_1] + \\sigma_2\\mu_1 \\mathbb{E}[Z_2] + \\mu_1\\mu_2 = \\sigma_1\\sigma_2\\rho + \\mu_1\\mu_2\n$$\n\n\\textit{Expectation $\\mathbb{E}[X_1^2 X_2]$:}\n$$\n\\mathbb{E}[X_1^2 X_2] = \\mathbb{E}[(\\sigma_1 Z_1 + \\mu_1)^2 (\\sigma_2 Z_2 + \\mu_2)] = \\mathbb{E}[(\\sigma_1^2 Z_1^2 + 2\\sigma_1\\mu_1 Z_1 + \\mu_1^2)(\\sigma_2 Z_2 + \\mu_2)]\n$$\nExpanding and taking the expectation of each term:\n$$\n\\mathbb{E}[X_1^2 X_2] = \\sigma_1^2\\sigma_2\\mathbb{E}[Z_1^2 Z_2] + \\sigma_1^2\\mu_2\\mathbb{E}[Z_1^2] + 2\\sigma_1\\mu_1\\sigma_2\\mathbb{E}[Z_1 Z_2] + 2\\sigma_1\\mu_1\\mu_2\\mathbb{E}[Z_1] + \\mu_1^2\\sigma_2\\mathbb{E}[Z_2] + \\mu_1^2\\mu_2\n$$\n$$\n= \\sigma_1^2\\sigma_2(0) + \\sigma_1^2\\mu_2(1) + 2\\sigma_1\\mu_1\\sigma_2(\\rho) + 2\\sigma_1\\mu_1\\mu_2(0) + \\mu_1^2\\sigma_2(0) + \\mu_1^2\\mu_2\n$$\n$$\n= \\sigma_1^2\\mu_2 + 2\\rho\\sigma_1\\sigma_2\\mu_1 + \\mu_1^2\\mu_2\n$$\n\n\\textit{Covariance $\\operatorname{Cov}(X_1, X_1 X_2)$:}\nSince $\\mathbb{E}[X_1] = \\mu_1$:\n$$\n\\operatorname{Cov}(X_1, X_1 X_2) = (\\sigma_1^2\\mu_2 + 2\\rho\\sigma_1\\sigma_2\\mu_1 + \\mu_1^2\\mu_2) - \\mu_1(\\sigma_1\\sigma_2\\rho + \\mu_1\\mu_2)\n$$\n$$\n= \\sigma_1^2\\mu_2 + 2\\rho\\sigma_1\\sigma_2\\mu_1 + \\mu_1^2\\mu_2 - \\rho\\sigma_1\\sigma_2\\mu_1 - \\mu_1^2\\mu_2 = \\sigma_1^2\\mu_2 + \\rho\\sigma_1\\sigma_2\\mu_1\n$$\n\nNext, we find the denominator term $\\operatorname{Var}(X_1 X_2) = \\mathbb{E}[(X_1 X_2)^2] - (\\mathbb{E}[X_1 X_2])^2$.\n\n\\textit{Expectation $\\mathbb{E}[X_1^2 X_2^2]$:}\n$$\n\\mathbb{E}[X_1^2 X_2^2] = \\mathbb{E}[(\\sigma_1 Z_1 + \\mu_1)^2 (\\sigma_2 Z_2 + \\mu_2)^2] = \\mathbb{E}[(\\sigma_1^2 Z_1^2 + 2\\sigma_1\\mu_1 Z_1 + \\mu_1^2)(\\sigma_2^2 Z_2^2 + 2\\sigma_2\\mu_2 Z_2 + \\mu_2^2)]\n$$\nExpanding and taking expectation, only terms with even powers of $Z_1$ and $Z_2$ (or products like $Z_1 Z_2$) will be non-zero:\n\\begin{align*}\n\\mathbb{E}[X_1^2 X_2^2] &= \\sigma_1^2\\sigma_2^2\\mathbb{E}[Z_1^2 Z_2^2] + \\sigma_1^2\\mu_2^2\\mathbb{E}[Z_1^2] + \\mu_1^2\\sigma_2^2\\mathbb{E}[Z_2^2] + 4\\sigma_1\\mu_1\\sigma_2\\mu_2\\mathbb{E}[Z_1 Z_2] + \\mu_1^2\\mu_2^2 \\\\\n&= \\sigma_1^2\\sigma_2^2(1+2\\rho^2) + \\sigma_1^2\\mu_2^2(1) + \\mu_1^2\\sigma_2^2(1) + 4\\rho\\sigma_1\\sigma_2\\mu_1\\mu_2 + \\mu_1^2\\mu_2^2\n\\end{align*}\n\n\\textit{Variance $\\operatorname{Var}(X_1 X_2)$:}\n$$\n(\\mathbb{E}[X_1 X_2])^2 = (\\sigma_1\\sigma_2\\rho + \\mu_1\\mu_2)^2 = \\rho^2\\sigma_1^2\\sigma_2^2 + 2\\rho\\sigma_1\\sigma_2\\mu_1\\mu_2 + \\mu_1^2\\mu_2^2\n$$\n$$\n\\operatorname{Var}(X_1 X_2) = \\mathbb{E}[X_1^2 X_2^2] - (\\mathbb{E}[X_1 X_2])^2\n$$\n$$\n= [\\sigma_1^2\\sigma_2^2 + 2\\rho^2\\sigma_1^2\\sigma_2^2 + \\sigma_1^2\\mu_2^2 + \\mu_1^2\\sigma_2^2 + 4\\rho\\sigma_1\\sigma_2\\mu_1\\mu_2 + \\mu_1^2\\mu_2^2] - [\\rho^2\\sigma_1^2\\sigma_2^2 + 2\\rho\\sigma_1\\sigma_2\\mu_1\\mu_2 + \\mu_1^2\\mu_2^2]\n$$\n$$\n= \\sigma_1^2\\sigma_2^2 + \\rho^2\\sigma_1^2\\sigma_2^2 + \\sigma_1^2\\mu_2^2 + \\mu_1^2\\sigma_2^2 + 2\\rho\\sigma_1\\sigma_2\\mu_1\\mu_2 = \\sigma_1^2\\sigma_2^2(1+\\rho^2) + \\sigma_1^2\\mu_2^2 + \\mu_1^2\\sigma_2^2 + 2\\rho\\sigma_1\\sigma_2\\mu_1\\mu_2\n$$\n\n\\textit{Assembling the correlation:}\n$$\n\\operatorname{Corr}(X_1, X_1 X_2) = \\frac{\\sigma_1^2\\mu_2 + \\rho\\sigma_1\\sigma_2\\mu_1}{\\sqrt{\\sigma_1^2 \\left( \\sigma_1^2\\sigma_2^2(1+\\rho^2) + \\sigma_1^2\\mu_2^2 + \\mu_1^2\\sigma_2^2 + 2\\rho\\sigma_1\\sigma_2\\mu_1\\mu_2 \\right)}}\n$$\n$$\n= \\frac{\\sigma_1(\\sigma_1\\mu_2 + \\rho\\sigma_2\\mu_1)}{\\sigma_1 \\sqrt{\\sigma_1^2\\sigma_2^2(1+\\rho^2) + \\sigma_1^2\\mu_2^2 + \\mu_1^2\\sigma_2^2 + 2\\rho\\sigma_1\\sigma_2\\mu_1\\mu_2}}\n$$\n$$\n= \\frac{\\sigma_1\\mu_2 + \\rho\\sigma_2\\mu_1}{\\sqrt{\\sigma_1^2\\sigma_2^2(1+\\rho^2) + \\sigma_1^2\\mu_2^2 + \\sigma_2^2\\mu_1^2 + 2\\rho\\sigma_1\\sigma_2\\mu_1\\mu_2}}\n$$\n\nThe final expressions for the three parts are now determined.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0 & \\frac{\\sigma_1\\mu_2 + \\rho\\sigma_2\\mu_1}{\\sqrt{\\sigma_1^2\\sigma_2^2(1+\\rho^2) + \\sigma_1^2\\mu_2^2 + \\sigma_2^2\\mu_1^2 + 2\\rho\\sigma_1\\sigma_2\\mu_1\\mu_2}} & 0\n\\end{pmatrix}\n}\n$$"
        }
    ]
}