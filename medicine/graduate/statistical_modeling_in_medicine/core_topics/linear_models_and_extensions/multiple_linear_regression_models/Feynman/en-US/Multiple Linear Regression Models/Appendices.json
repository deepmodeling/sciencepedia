{
    "hands_on_practices": [
        {
            "introduction": "A common challenge in medical modeling is interpreting models with interaction terms. The coefficient of a main effect is no longer a standalone 'average' effect but is conditional on the value of the interacting variable. This first practice will guide you through the essential skill of calculating a marginal treatment effect at a specific value of a covariate (e.g., a patient's age) and, just as importantly, quantifying its statistical uncertainty using the coefficient covariance matrix. ",
            "id": "4977020",
            "problem": "A randomized controlled trial (RCT) evaluates a new antihypertensive therapy on the short-term change in systolic blood pressure (SBP) measured in millimeters of mercury (mmHg) after $3$ months. Let $y$ denote the observed SBP change, with negative values indicating reductions. Patients are characterized by a binary treatment assignment $x_{\\text{treat}} \\in \\{0,1\\}$ and age in years $x_{\\text{age}} \\in \\mathbb{R}$. Consider the multiple linear regression model for the conditional expectation,\n$$\nE[y \\mid x] \\;=\\; \\beta_0 \\;+\\; \\beta_{\\text{treat}}\\, x_{\\text{treat}} \\;+\\; \\beta_{\\text{age}}\\, x_{\\text{age}} \\;+\\; \\beta_{\\text{treat}\\times \\text{age}}\\, \\big(x_{\\text{treat}} \\cdot x_{\\text{age}}\\big),\n$$\nfit by Ordinary Least Squares (OLS). Define the marginal treatment effect at a specified age $a_0$ as the rate of change in the conditional mean with respect to treatment at that age. You are given the OLS coefficient estimates\n$$\n\\hat{\\boldsymbol{\\beta}} \\;=\\; \\big(\\hat{\\beta}_0,\\hat{\\beta}_{\\text{treat}},\\hat{\\beta}_{\\text{age}},\\hat{\\beta}_{\\text{treat}\\times \\text{age}}\\big)^{\\top}\n\\;=\\;\n\\big(1.8,\\,-4.6,\\,0.12,\\,-0.045\\big)^{\\top},\n$$\nand their estimated variance窶田ovariance matrix (in $\\text{mmHg}^2$),\n$$\n\\hat{\\Sigma} \\;=\\;\n\\begin{pmatrix}\n4.0 & -0.12 & -0.02 & 0.001 \\\\\n-0.12 & 1.44 & 0.03 & 0.006 \\\\\n-0.02 & 0.03 & 0.0025 & -0.0002 \\\\\n0.001 & 0.006 & -0.0002 & 0.0009\n\\end{pmatrix}.\n$$\nTake the specified age as $a_0 = 65$. Using only the definitions of the linear predictor for $E[y \\mid x]$, the notion of a marginal effect as a partial derivative, and the property that the variance of any linear combination of OLS coefficient estimates is determined by the variance窶田ovariance matrix, compute the standard error of the estimated marginal treatment effect at age $a_0$. Round your answer to four significant figures and express it in $\\text{mmHg}$.",
            "solution": "The multiple linear regression model specifies a linear conditional mean,\n$$\nE[y \\mid x] \\;=\\; \\beta_0 \\;+\\; \\beta_{\\text{treat}}\\, x_{\\text{treat}} \\;+\\; \\beta_{\\text{age}}\\, x_{\\text{age}} \\;+\\; \\beta_{\\text{treat}\\times \\text{age}}\\, \\big(x_{\\text{treat}} \\cdot x_{\\text{age}}\\big).\n$$\nBy definition, the marginal treatment effect at age $x_{\\text{age}} = a_0$ is the partial derivative of the conditional mean with respect to $x_{\\text{treat}}$ evaluated at $x_{\\text{age}} = a_0$:\n$$\n\\frac{\\partial}{\\partial x_{\\text{treat}}} E[y \\mid x] \\bigg|_{x_{\\text{age}}=a_0}\n\\;=\\;\n\\beta_{\\text{treat}} \\;+\\; \\beta_{\\text{treat}\\times \\text{age}}\\, a_0.\n$$\nThis quantity measures the instantaneous change in $E[y \\mid x]$ per unit change in $x_{\\text{treat}}$ at age $a_0$. Because $x_{\\text{treat}}$ is an indicator, this is equivalently the difference in conditional means between treated and untreated at age $a_0$ under the linear model.\n\nThe corresponding estimator of the marginal treatment effect is\n$$\n\\hat{L}(a_0) \\;=\\; \\hat{\\beta}_{\\text{treat}} \\;+\\; \\hat{\\beta}_{\\text{treat}\\times \\text{age}}\\, a_0.\n$$\nTo obtain its standard error, we use the property that the variance of any linear combination of the estimated coefficients is determined by the variance窶田ovariance matrix. Let the coefficient vector be ordered as\n$$\n\\hat{\\boldsymbol{\\beta}} \\;=\\; \\big(\\hat{\\beta}_0,\\hat{\\beta}_{\\text{treat}},\\hat{\\beta}_{\\text{age}},\\hat{\\beta}_{\\text{treat}\\times \\text{age}}\\big)^{\\top}.\n$$\nThen $\\hat{L}(a_0)$ is a linear combination with weight vector\n$$\n\\boldsymbol{g}(a_0) \\;=\\; \\big(0,\\,1,\\,0,\\,a_0\\big)^{\\top}.\n$$\nTherefore,\n$$\n\\operatorname{Var}\\big(\\hat{L}(a_0)\\big) \\;=\\; \\boldsymbol{g}(a_0)^{\\top}\\, \\hat{\\Sigma}\\, \\boldsymbol{g}(a_0),\n\\quad\n\\text{and}\n\\quad\n\\operatorname{SE}\\big(\\hat{L}(a_0)\\big) \\;=\\; \\sqrt{\\,\\boldsymbol{g}(a_0)^{\\top}\\, \\hat{\\Sigma}\\, \\boldsymbol{g}(a_0)\\,}.\n$$\nSubstituting $a_0 = 65$ and the provided $\\hat{\\Sigma}$, only the entries involving $\\hat{\\beta}_{\\text{treat}}$ and $\\hat{\\beta}_{\\text{treat}\\times \\text{age}}$ contribute:\n$$\n\\operatorname{Var}\\big(\\hat{L}(65)\\big)\n\\;=\\;\n\\operatorname{Var}(\\hat{\\beta}_{\\text{treat}})\n\\;+\\;\n2 \\cdot 65 \\cdot \\operatorname{Cov}(\\hat{\\beta}_{\\text{treat}}, \\hat{\\beta}_{\\text{treat}\\times \\text{age}})\n\\;+\\;\n65^2 \\cdot \\operatorname{Var}(\\hat{\\beta}_{\\text{treat}\\times \\text{age}}).\n$$\nWith $\\hat{\\Sigma}_{22} = 1.44$, $\\hat{\\Sigma}_{24} = 0.006$, and $\\hat{\\Sigma}_{44} = 0.0009$, we obtain\n$$\n\\operatorname{Var}\\big(\\hat{L}(65)\\big)\n\\;=\\;\n1.44 \\;+\\; 2 \\cdot 65 \\cdot 0.006 \\;+\\; 65^2 \\cdot 0.0009\n\\;=\\;\n1.44 \\;+\\; 0.78 \\;+\\; 3.8025\n\\;=\\;\n6.0225.\n$$\nHence the standard error is\n$$\n\\operatorname{SE}\\big(\\hat{L}(65)\\big)\n\\;=\\;\n\\sqrt{6.0225}\n\\;\\approx\\;\n2.454077\\ldots\n$$\nRounded to four significant figures (and expressed in $\\text{mmHg}$ as requested), the standard error is $2.454$.\n\nFor interpretation, the estimated marginal treatment effect at age $65$ is\n$$\n\\hat{L}(65) \\;=\\; \\hat{\\beta}_{\\text{treat}} \\;+\\; \\hat{\\beta}_{\\text{treat}\\times \\text{age}} \\cdot 65\n\\;=\\; -4.6 \\;+\\; (-0.045)\\cdot 65\n\\;=\\; -7.525,\n$$\nwhich means that, at age $65$, treatment is associated with a reduction of the expected SBP change by approximately $7.525$ $\\text{mmHg}$ relative to control, with a standard error of about $2.454$ $\\text{mmHg}$. The computation of the standard error follows from the variance of a linear combination of OLS coefficient estimates via the variance窶田ovariance matrix.",
            "answer": "$$\\boxed{2.454}$$"
        },
        {
            "introduction": "Building on the theme of customized inference, we now turn to studies with multiple treatment arms. Instead of just one treatment effect, we may want to answer more nuanced questions, like comparing two specific treatments or contrasting a group of novel therapies against a standard one. This exercise demonstrates how to use linear contrasts to formally express such hypotheses and to derive the covariance matrix of their estimates, a critical step for constructing confidence intervals and performing hypothesis tests. ",
            "id": "4977017",
            "problem": "A multi-arm randomized clinical trial investigates the mean change in a continuous biomarker under $4$ treatments: $A$, $B$, $C$, and $D$. Let $Y$ denote the change from baseline to follow-up, and let $Z$ denote a baseline covariate that is mean-centered within each treatment arm. Consider the multiple linear regression model with cell-means parameterization:\n$$\nY_{ij} \\;=\\; \\beta_A \\, g_{A,ij} \\;+\\; \\beta_B \\, g_{B,ij} \\;+\\; \\beta_C \\, g_{C,ij} \\;+\\; \\beta_D \\, g_{D,ij} \\;+\\; \\gamma \\, Z_{ij} \\;+\\; \\varepsilon_{ij},\n$$\nwhere $g_{A,ij}, g_{B,ij}, g_{C,ij}, g_{D,ij}$ are treatment indicators that satisfy $g_{A,ij} + g_{B,ij} + g_{C,ij} + g_{D,ij} = 1$, and the error terms $\\varepsilon_{ij}$ are independent and identically distributed with mean $0$ and variance $\\sigma^2$. Let the sample sizes be $n_A, n_B, n_C, n_D$ in the $4$ arms, respectively. The within-arm centering means that for each treatment arm $t \\in \\{A,B,C,D\\}$, $\\sum_{i=1}^{n_t} Z_{it} = 0$. The coefficients $\\beta_A, \\beta_B, \\beta_C, \\beta_D$ represent adjusted arm means at $Z=0$. Let $s^2$ denote the unbiased estimator of $\\sigma^2$ from ordinary least squares.\n\nDefine the pairwise contrast $\\mu_A - \\mu_B$ and the general contrast $\\frac{1}{2}(\\mu_A + \\mu_B) - \\frac{1}{2}(\\mu_C + \\mu_D)$, where $\\mu_t$ denotes the adjusted mean in arm $t$, here equal to $\\beta_t$ because the model uses cell-means coding and $Z$ is centered.\n\nTasks:\n- Construct the matrix $L$ whose rows encode these two contrasts acting on the full parameter vector $(\\beta_A, \\beta_B, \\beta_C, \\beta_D, \\gamma)^{\\top}$.\n- Starting from the linear model assumptions and the ordinary least squares estimating equations, express the estimated covariance matrix of $L \\hat{\\beta}$ in terms of $s^2$ and the design information, using the orthogonality implied by within-arm centering of $Z$.\n- Finally, extract the off-diagonal entry of the resulting $2 \\times 2$ covariance matrix as a closed-form expression in terms of $s^2$ and the arm sample sizes $n_A, n_B, n_C, n_D$.\n\nExpress your final answer as a single simplified analytic expression in terms of $s^2$ and $n_A, n_B, n_C, n_D$. No rounding is required. Do not include units.",
            "solution": "The estimated covariance matrix of the OLS estimator $\\hat{\\boldsymbol{\\beta}}$ is $\\widehat{\\operatorname{Cov}}(\\hat{\\boldsymbol{\\beta}}) = s^2 (X^{\\prime}X)^{-1}$. The design matrix $X$ has columns for the treatment indicators $(g_A, g_B, g_C, g_D)$ and the covariate $Z$. Due to the cell-means parameterization, the indicator columns are mutually orthogonal. The problem states that the covariate $Z$ is mean-centered within each treatment arm, so $\\sum_{i \\in \\text{arm } t} Z_i = 0$ for each arm $t$. This makes the covariate column orthogonal to each treatment indicator column. Consequently, the matrix $X^{\\prime}X$ is diagonal:\n$$\nX^{\\prime}X = \\operatorname{diag}(n_A, n_B, n_C, n_D, \\sum Z_{ij}^2)\n$$\nThe inverse is also diagonal:\n$$\n(X^{\\prime}X)^{-1} = \\operatorname{diag}(1/n_A, 1/n_B, 1/n_C, 1/n_D, 1/(\\sum Z_{ij}^2))\n$$\nThe estimated covariance matrix for the parameter vector $\\hat{\\boldsymbol{\\beta}} = (\\hat{\\beta}_A, \\hat{\\beta}_B, \\hat{\\beta}_C, \\hat{\\beta}_D, \\hat{\\gamma})^{\\top}$ is therefore:\n$$\n\\widehat{\\operatorname{Cov}}(\\hat{\\boldsymbol{\\beta}}) = s^2 \\operatorname{diag}(1/n_A, 1/n_B, 1/n_C, 1/n_D, 1/(\\sum Z_{ij}^2))\n$$\nThis shows that the estimators $\\hat{\\beta}_A, \\hat{\\beta}_B, \\hat{\\beta}_C, \\hat{\\beta}_D,$ and $\\hat{\\gamma}$ are uncorrelated.\n\nThe two contrasts are $C_1 = \\beta_A - \\beta_B$ and $C_2 = \\frac{1}{2}(\\beta_A + \\beta_B) - \\frac{1}{2}(\\beta_C + \\beta_D)$. We can represent these as a linear transformation $L\\boldsymbol{\\beta}$ of the parameter vector, with the matrix $L$ given by:\n$$\nL = \\begin{pmatrix}\n1 & -1 & 0 & 0 & 0 \\\\\n\\frac{1}{2} & \\frac{1}{2} & -\\frac{1}{2} & -\\frac{1}{2} & 0\n\\end{pmatrix}\n$$\nThe covariance matrix of the estimated contrasts $L\\hat{\\boldsymbol{\\beta}}$ is given by the formula $\\widehat{\\operatorname{Cov}}(L \\hat{\\boldsymbol{\\beta}}) = L \\widehat{\\operatorname{Cov}}(\\hat{\\boldsymbol{\\beta}}) L^{\\top}$. The problem asks for the off-diagonal entry of this $2 \\times 2$ matrix, which is the covariance between the two estimated contrasts. Let the row vectors of $L$ be $\\mathbf{l}_1^{\\top}$ and $\\mathbf{l}_2^{\\top}$. The off-diagonal entry is $\\mathbf{l}_1^{\\top} \\widehat{\\operatorname{Cov}}(\\hat{\\boldsymbol{\\beta}}) \\mathbf{l}_2$.\n$$\n\\operatorname{Cov}(\\widehat{C}_1, \\widehat{C}_2) = \\mathbf{l}_1^{\\top} \\left( s^2 (X^{\\prime}X)^{-1} \\right) \\mathbf{l}_2\n= s^2 \\begin{pmatrix} 1 & -1 & 0 & 0 & 0 \\end{pmatrix} \\operatorname{diag}\\left(\\frac{1}{n_A}, \\frac{1}{n_B}, \\dots\\right) \\begin{pmatrix} 1/2 \\\\ 1/2 \\\\ -1/2 \\\\ -1/2 \\\\ 0 \\end{pmatrix}\n$$\n$$\n= s^2 \\begin{pmatrix} \\frac{1}{n_A} & -\\frac{1}{n_B} & 0 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 1/2 \\\\ 1/2 \\\\ -1/2 \\\\ -1/2 \\\\ 0 \\end{pmatrix}\n$$\n$$\n= s^2 \\left( \\frac{1}{n_A} \\cdot \\frac{1}{2} + \\left(-\\frac{1}{n_B}\\right) \\cdot \\frac{1}{2} \\right) = \\frac{s^2}{2} \\left( \\frac{1}{n_A} - \\frac{1}{n_B} \\right)\n$$\nThis is the required expression for the off-diagonal entry.",
            "answer": "$$\\boxed{\\frac{s^2}{2}\\left(\\frac{1}{n_A} - \\frac{1}{n_B}\\right)}$$"
        },
        {
            "introduction": "A robust statistical finding should not be overly dependent on any single observation. Therefore, an essential part of the modeling workflow is to diagnose the influence of individual data points on our conclusions, particularly on key parameters like a treatment effect. This final practice introduces the concept of leave-one-out diagnostics, guiding you through the derivation and calculation of how much the estimated treatment effect changes when each observation is sequentially removed from the dataset. ",
            "id": "4977029",
            "problem": "A clinical trial evaluates a new antihypertensive drug. The outcome is systolic blood pressure reduction after twelve weeks, measured in millimeters of mercury (mmHg). Let $y_i$ denote the reduction for patient $i$. A multiple linear regression model is fit with an intercept, a treatment indicator, and a centered baseline covariate:\n$$\ny_i = \\beta_0 + \\beta_T T_i + \\beta_B b_i + \\varepsilon_i,\n$$\nwhere $T_i$ is coded as $-1$ for standard care and $+1$ for the new drug, and $b_i$ is the baseline systolic blood pressure deviation from the cohort mean (in mmHg). The dataset for $i = 1,\\dots,6$ is:\n- $i=1$: $T_1=-1$, $b_1=-2$, $y_1=4$\n- $i=2$: $T_2=-1$, $b_2=-1$, $y_2=2$\n- $i=3$: $T_3=-1$, $b_3=3$, $y_3=11$\n- $i=4$: $T_4=+1$, $b_4=-3$, $y_4=11$\n- $i=5$: $T_5=+1$, $b_5=+1$, $y_5=16$\n- $i=6$: $T_6=+1$, $b_6=+2$, $y_6=16$\n\nStarting from the ordinary least squares normal equations and appropriate matrix identities, derive the leave-one-out expression for the change in the estimated treatment coefficient $\\hat{\\beta}_T$ when observation $i$ is deleted, and compute this change for each $i$. Quantify the sensitivity of $\\hat{\\beta}_T$ to the deletion of each observation by reporting the largest absolute leave-one-out change in $\\hat{\\beta}_T$ across $i=1,\\dots,6$.\n\nRound your final numeric answer to four significant figures. Express the magnitude in mmHg per unit of the treatment indicator.",
            "solution": "First, we derive the general expression for the change in an ordinary least squares (OLS) coefficient estimate upon deletion of a single observation. The multiple linear regression model is given in matrix form as:\n$$\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\n$$\nwhere $\\mathbf{y}$ is the $n \\times 1$ vector of outcomes, $\\mathbf{X}$ is the $n \\times p$ design matrix, $\\boldsymbol{\\beta}$ is the $p \\times 1$ vector of coefficients, and $\\boldsymbol{\\varepsilon}$ is the $n \\times 1$ vector of errors. The OLS estimator for $\\boldsymbol{\\beta}$ using all $n$ observations is $\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$.\nThe change in the coefficient vector upon deleting observation $i$, $\\hat{\\boldsymbol{\\beta}}_{(i)} - \\hat{\\boldsymbol{\\beta}}$, can be expressed as:\n$$\n\\hat{\\boldsymbol{\\beta}}_{(i)} - \\hat{\\boldsymbol{\\beta}} = - \\frac{(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{x}_i e_i}{1-h_{ii}}\n$$\nwhere $\\mathbf{x}_i^T$ is the $i$-th row of $\\mathbf{X}$, $e_i = y_i - \\mathbf{x}_i^T\\hat{\\boldsymbol{\\beta}}$ is the $i$-th OLS residual, and $h_{ii} = \\mathbf{x}_i^T(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{x}_i$ is the $i$-th leverage value.\n\nNow, we apply this to the given data. The model is $y_i = \\beta_0 + \\beta_T T_i + \\beta_B b_i + \\varepsilon_i$. The design matrix $\\mathbf{X}$ and response vector $\\mathbf{y}$ are:\n$$\n\\mathbf{X} = \\begin{pmatrix} 1 & -1 & -2 \\\\ 1 & -1 & -1 \\\\ 1 & -1 & 3 \\\\ 1 & 1 & -3 \\\\ 1 & 1 & 1 \\\\ 1 & 1 & 2 \\end{pmatrix}, \\quad \\mathbf{y} = \\begin{pmatrix} 4 \\\\ 2 \\\\ 11 \\\\ 11 \\\\ 16 \\\\ 16 \\end{pmatrix}\n$$\nDue to the problem's design (treatment indicator coded $\\pm 1$, baseline covariate centered), the columns of $\\mathbf{X}$ are mutually orthogonal. This makes $\\mathbf{X}^T\\mathbf{X}$ a diagonal matrix:\n$$\n\\mathbf{X}^T\\mathbf{X} = \\begin{pmatrix} \\sum 1 & 0 & 0 \\\\ 0 & \\sum T_i^2 & 0 \\\\ 0 & 0 & \\sum b_i^2 \\end{pmatrix} = \\begin{pmatrix} 6 & 0 & 0 \\\\ 0 & 6 & 0 \\\\ 0 & 0 & 28 \\end{pmatrix}\n$$\nThe inverse is $C = (\\mathbf{X}^T\\mathbf{X})^{-1} = \\operatorname{diag}(1/6, 1/6, 1/28)$.\nNext, we compute $\\mathbf{X}^T\\mathbf{y} = (\\sum y_i, \\sum T_i y_i, \\sum b_i y_i)^T = (60, 26, 38)^T$.\nThe OLS estimates are:\n$$\n\\hat{\\boldsymbol{\\beta}} = C \\mathbf{X}^T\\mathbf{y} = (10, 26/6, 38/28)^T = (10, 13/3, 19/14)^T\n$$\nSo, $\\hat{\\beta}_T=13/3$. The fitted model is $\\hat{y}_i = 10 + \\frac{13}{3}T_i + \\frac{19}{14}b_i$.\n\nThe change in $\\hat{\\beta}_T$ is the second component of the vector $\\hat{\\boldsymbol{\\beta}}_{(i)} - \\hat{\\boldsymbol{\\beta}}$. Let $C_{T,T}$ be the second diagonal element of $C$. Because of orthogonality, $(C\\mathbf{x}_i)_T = C_{T,T}T_i$.\n$$\n\\Delta\\hat{\\beta}_T^{(i)} = \\hat{\\beta}_{T,(i)} - \\hat{\\beta}_T = - \\frac{C_{T,T} T_i e_i}{1-h_{ii}} = - \\frac{(1/6) T_i e_i}{1-h_{ii}}\n$$\nwhere $h_{ii} = \\frac{1}{6} + \\frac{T_i^2}{6} + \\frac{b_i^2}{28} = \\frac{1}{3} + \\frac{b_i^2}{28}$.\n\nWe compute these quantities for each observation $i=1, \\dots, 6$:\n- $i=1$: $T_1=-1, b_1=-2, y_1=4$. $h_{11}=10/21, e_1=22/21 \\implies \\Delta\\hat{\\beta}_T^{(1)} = 1/3 \\approx 0.3333$\n- $i=2$: $T_2=-1, b_2=-1, y_2=2$. $h_{22}=31/84, e_2=-97/42 \\implies \\Delta\\hat{\\beta}_T^{(2)} = -97/159 \\approx -0.6101$\n- $i=3$: $T_3=-1, b_3=3, y_3=11$. $h_{33}=55/84, e_3=53/42 \\implies \\Delta\\hat{\\beta}_T^{(3)} = 53/87 \\approx 0.6092$\n- $i=4$: $T_4=1, b_4=-3, y_4=11$. $h_{44}=55/84, e_4=31/42 \\implies \\Delta\\hat{\\beta}_T^{(4)} = -31/87 \\approx -0.3563$\n- $i=5$: $T_5=1, b_5=1, y_5=16$. $h_{55}=31/84, e_5=13/42 \\implies \\Delta\\hat{\\beta}_T^{(5)} = -13/159 \\approx -0.0818$\n- $i=6$: $T_6=1, b_6=2, y_6=16$. $h_{66}=10/21, e_6=-22/21 \\implies \\Delta\\hat{\\beta}_T^{(6)} = 1/3 \\approx 0.3333$\n\nWe now find the largest absolute change, $\\max_i |\\Delta\\hat{\\beta}_T^{(i)}|$.\nComparing the absolute values:\n- $| \\Delta\\hat{\\beta}_T^{(1)} | \\approx 0.3333$\n- $| \\Delta\\hat{\\beta}_T^{(2)} | \\approx 0.6101$\n- $| \\Delta\\hat{\\beta}_T^{(3)} | \\approx 0.6092$\n- $| \\Delta\\hat{\\beta}_T^{(4)} | \\approx 0.3563$\n- $| \\Delta\\hat{\\beta}_T^{(5)} | \\approx 0.0818$\n- $| \\Delta\\hat{\\beta}_T^{(6)} | \\approx 0.3333$\n\nThe largest absolute change is $|-97/159| \\approx 0.61006...$.\nRounding to four significant figures, the result is $0.6101$.",
            "answer": "$$\n\\boxed{0.6101}\n$$"
        }
    ]
}