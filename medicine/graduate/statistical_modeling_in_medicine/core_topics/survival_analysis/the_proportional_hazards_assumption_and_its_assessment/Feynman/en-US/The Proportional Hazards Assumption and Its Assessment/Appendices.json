{
    "hands_on_practices": [
        {
            "introduction": "Assessing the proportional hazards assumption often begins with visual diagnostics. This practice grounds your understanding in first principles by requiring the manual computation of group-specific Nelson-Aalen cumulative hazard estimators, $\\widehat{H}_g(t)$, from raw data. By plotting transformations of these estimators, such as the difference in their logarithms, you will gain a concrete feel for how deviations from proportionality manifest visually .",
            "id": "4991166",
            "problem": "A randomized clinical study follows two groups, indexed by $g \\in \\{1,2\\}$, to compare time to a cardiovascular event. For subject $i$ in group $g$, let $T_{gi}$ be the time to event or censoring and $\\Delta_{gi} \\in \\{0,1\\}$ the event indicator. The hazard function is $h_g(t)$, and the cumulative hazard is $H_g(t) = \\int_0^t h_g(u)\\,du$. Survival is $S_g(t) = \\exp\\{-H_g(t)\\}$. Under the proportional hazards assumption (PH), there exists a constant hazard ratio $\\theta > 0$ such that $h_2(t) = \\theta \\, h_1(t)$ for all $t \\ge 0$. Your task is to use group-specific Nelson–Aalen estimators of $H_g(t)$ to assess PH by inspecting whether the difference of the logarithms of the group-specific cumulative hazards is approximately constant over $t$.\nData: group $1$ has $n_1 = 4$ subjects with observed $(T_{1i}, \\Delta_{1i})$ equal to $(1,1)$, $(3,1)$, $(5,0)$, $(7,1)$. Group $2$ has $n_2 = 4$ subjects with $(T_{2i}, \\Delta_{2i})$ equal to $(2,1)$, $(4,1)$, $(6,1)$, $(8,0)$. Assume no left truncation and independent, non-informative right-censoring. Consider the pooled set of distinct event times across both groups.\nStarting from the core definitions of $h_g(t)$ and $H_g(t)$ and using the well-tested Nelson–Aalen estimator for $H_g(t)$ computed within each group, carry out the following tasks conceptually and computationally:\n- For each group $g \\in \\{1,2\\}$, define the group-specific risk set size $r_g(t_j)$ just prior to each distinct event time $t_j$ in that group and the number of events $d_g(t_j)$ at $t_j$. Use these to compute the group-specific Nelson–Aalen estimator $\\widehat H_g(t)$ as a right-continuous step function over the pooled event times up to the largest event time observed.\n- Using $\\log \\widehat H_g(t)$ when defined, inspect whether $\\log \\widehat H_1(t) - \\log \\widehat H_2(t)$ is approximately constant in $t$ and state whether the PH assumption appears supported in these data.\n- Be explicit about any constraints needed to compute the logarithms and any tie handling.\nWhich of the following statements are correct? Select all that apply.\n\nA. For each group $g$, the Nelson–Aalen estimator is computed using only within-group risk sets: $\\widehat H_g(t) = \\sum_{t_{gj} \\le t} d_g(t_{gj}) / r_g(t_{gj})$, where $r_g(t_{gj})$ counts subjects at risk in group $g$ strictly before $t_{gj}$ and $d_g(t_{gj})$ counts events in group $g$ at $t_{gj}$. To assess proportional hazards, evaluate $\\log \\widehat H_1(t) - \\log \\widehat H_2(t)$ over pooled event times $t$ where both $\\widehat H_g(t) > 0$ and check for approximate constancy in $t$.\n\nB. A simpler and equivalent PH diagnostic is to compute Kaplan–Meier estimators $\\widehat S_g(t)$ for each group and check whether $\\widehat S_1(t) - \\widehat S_2(t)$ is approximately constant in $t$; if so, PH holds.\n\nC. To stabilize estimation, the denominators in the Nelson–Aalen increments for each group should be the pooled-at-risk counts across both groups, i.e., $r(t_j)$ from all subjects, rather than the within-group $r_g(t_j)$.\n\nD. Because under proportional hazards $H_2(t) = \\theta H_1(t)$ for some constant $\\theta$, an equivalent diagnostic to the log-difference check is to inspect whether $\\widehat H_2(t) / \\widehat H_1(t)$ is approximately constant in $t$ over times where both denominators are positive.\n\nE. If at a given time $t$ there are $d_g(t) \\ge 2$ tied events in group $g$, the Nelson–Aalen increment at $t$ is $d_g(t) / r_g(t)$ using the group-specific risk set size just prior to $t$; this tie handling is standard for Nelson–Aalen.\n\nF. Using the provided data, the computed values of $\\log \\widehat H_1(t) - \\log \\widehat H_2(t)$ over times where both are defined vary substantially with $t$, so the PH assumption is not supported in this sample.\n\nG. When $\\widehat H_g(t) = 0$ at early times, it is acceptable to define $\\log \\widehat H_g(t) = 0$ so that all pooled event times can be used in the log-difference plot without restriction.",
            "solution": "### Problem Validation\n\n#### Step 1: Extract Givens\n- **Study setup**: A randomized clinical study with two groups, indexed by $g \\in \\{1,2\\}$.\n- **Data per subject**: For subject $i$ in group $g$, the observed data is $(T_{gi}, \\Delta_{gi})$, where $T_{gi}$ is the time to event or censoring and $\\Delta_{gi} \\in \\{0,1\\}$ is the event indicator ($\\Delta_{gi}=1$ for event, $\\Delta_{gi}=0$ for censoring).\n- **Hazard functions**: $h_g(t)$ is the hazard function for group $g$.\n- **Cumulative hazard functions**: $H_g(t) = \\int_0^t h_g(u)\\,du$.\n- **Survival functions**: $S_g(t) = \\exp\\{-H_g(t)\\}$.\n- **Proportional Hazards (PH) Assumption**: There exists a constant hazard ratio $\\theta > 0$ such that $h_2(t) = \\theta \\, h_1(t)$ for all $t \\ge 0$.\n- **Task**: Assess the PH assumption by inspecting whether the difference of the logarithms of the group-specific Nelson-Aalen estimators, $\\log \\widehat H_1(t) - \\log \\widehat H_2(t)$, is approximately constant over time $t$.\n- **Data for Group 1**: $n_1 = 4$ subjects with $(T_{1i}, \\Delta_{1i})$ pairs: $(1,1)$, $(3,1)$, $(5,0)$, $(7,1)$.\n- **Data for Group 2**: $n_2 = 4$ subjects with $(T_{2i}, \\Delta_{2i})$ pairs: $(2,1)$, $(4,1)$, $(6,1)$, $(8,0)$.\n- **Assumptions**: No left truncation, independent and non-informative right-censoring.\n- **Computational instructions**: Use group-specific Nelson–Aalen estimator $\\widehat H_g(t) = \\sum_{t_{gj} \\le t} d_g(t_{gj}) / r_g(t_{gj})$, where $r_g(t_j)$ is the number of subjects at risk in group $g$ just prior to time $t_j$, and $d_g(t_j)$ is the number of events in group $g$ at time $t_j$. Evaluation is to be done over the pooled set of distinct event times.\n\n#### Step 2: Validate Using Extracted Givens\nThe problem statement is evaluated against the validation criteria:\n- **Scientifically Grounded**: The problem is set within the standard framework of survival analysis, a core topic in biostatistics and medical statistics. All definitions—hazard function, cumulative hazard function, survival function, proportional hazards assumption, and the Nelson-Aalen estimator—are standard and textbook-correct. The proposed method for assessing the PH assumption is a valid and widely used graphical technique. There are no scientific or factual errors.\n- **Well-Posed**: The problem is clearly structured. It provides specific data, a well-defined statistical model (PH model), a standard non-parametric estimator (Nelson-Aalen), and a precise task (compute the estimators and a diagnostic quantity, then evaluate specific statements). The data are complete and sufficient to perform the required calculations. A unique solution exists for the computations.\n- **Objective**: The language is precise, quantitative, and free of subjectivity or ambiguity. Terms like \"risk set,\" \"event indicator,\" and \"Nelson-Aalen estimator\" have unambiguous definitions in the field.\n\nThe problem does not violate any of the invalidity criteria. It is a standard, formal exercise in applied statistics.\n\n#### Step 3: Verdict and Action\nThe problem is valid. The solution process will proceed.\n\n### Solution Derivation\n\nThe proportional hazards (PH) assumption states $h_2(t) = \\theta h_1(t)$ for a constant $\\theta > 0$. Integrating both sides from $0$ to $t$ yields the relationship for the cumulative hazard functions:\n$$ H_2(t) = \\int_0^t h_2(u)\\,du = \\int_0^t \\theta h_1(u)\\,du = \\theta \\int_0^t h_1(u)\\,du = \\theta H_1(t) $$\nTaking the natural logarithm of both sides (for $t$ such that $H_1(t) > 0$ and $H_2(t) > 0$), we get:\n$$ \\log H_2(t) = \\log(\\theta H_1(t)) = \\log \\theta + \\log H_1(t) $$\nRearranging gives:\n$$ \\log H_2(t) - \\log H_1(t) = \\log \\theta $$\nThis shows that under the PH assumption, the difference between the log-cumulative hazards of the two groups is a constant, $\\log \\theta$. The problem asks to check if $\\log \\widehat H_1(t) - \\log \\widehat H_2(t)$ is constant, which is equivalent to checking if $\\log \\widehat H_2(t) - \\log \\widehat H_1(t)$ is constant (the constant would just be the negative of the other). We can assess this by replacing the theoretical functions $H_g(t)$ with their Nelson-Aalen estimates $\\widehat H_g(t)$.\n\nThe Nelson-Aalen estimator for the cumulative hazard in group $g$ is given by:\n$$ \\widehat H_g(t) = \\sum_{t_{gj} \\le t} \\frac{d_g(t_{gj})}{r_g(t_{gj})} $$\nwhere $t_{gj}$ are the distinct event times in group $g$, $d_g(t_{gj})$ is the number of events in group $g$ at time $t_{gj}$, and $r_g(t_{gj})$ is the number of subjects at risk in group $g$ just prior to time $t_{gj}$.\n\n**1. Calculation for Group 1:**\nData: $(1,1), (3,1), (5,0), (7,1)$. Initial risk set size $n_1=4$.\nDistinct event times are $t=1, 3, 7$.\n- At $t=1$: $d_1(1)=1$, $r_1(1)=4$. Increment is $1/4$. $\\widehat H_1(t) = 1/4$ for $1 \\le t < 3$.\n- At $t=3$: One subject has had an event. $r_1(3)=3$. $d_1(3)=1$. Increment is $1/3$. $\\widehat H_1(t) = 1/4 + 1/3 = 7/12$ for $3 \\le t < 5$.\n- At $t=5$: One subject is censored. The risk set size decreases from $2$ to $1$ for subsequent times. There is no increment to $\\widehat H_1(t)$. For $5 \\le t < 7$, $\\widehat H_1(t)$ remains $7/12$.\n- At $t=7$: One subject has had an event at $t=1$, one at $t=3$, and one was censored at $t=5$. The risk set just prior to $t=7$ is $r_1(7)=1$. $d_1(7)=1$. Increment is $1/1 = 1$. $\\widehat H_1(t) = 7/12 + 1 = 19/12$ for $t \\ge 7$.\n\n**2. Calculation for Group 2:**\nData: $(2,1), (4,1), (6,1), (8,0)$. Initial risk set size $n_2=4$.\nDistinct event times are $t=2, 4, 6$.\n- At $t=2$: $d_2(2)=1$, $r_2(2)=4$. Increment is $1/4$. $\\widehat H_2(t) = 1/4$ for $2 \\le t < 4$.\n- At $t=4$: One subject has had an event. $r_2(4)=3$. $d_2(4)=1$. Increment is $1/3$. $\\widehat H_2(t) = 1/4 + 1/3 = 7/12$ for $4 \\le t < 6$.\n- At $t=6$: Two subjects have had events. $r_2(6)=2$. $d_2(6)=1$. Increment is $1/2$. $\\widehat H_2(t) = 7/12 + 1/2 = 13/12$ for $t \\ge 6$.\n- At $t=8$: One subject is censored.\n\n**3. Assessing the PH assumption:**\nWe evaluate $\\log \\widehat H_1(t) - \\log \\widehat H_2(t)$ at the pooled distinct event times $t \\in \\{1, 2, 3, 4, 6, 7\\}$, restricted to times where both $\\widehat H_1(t)>0$ and $\\widehat H_2(t)>0$.\n\n| Time $t$ | $\\widehat H_1(t)$ | $\\widehat H_2(t)$ | $\\log \\widehat H_1(t)$ | $\\log \\widehat H_2(t)$ | $\\log \\widehat H_1(t) - \\log \\widehat H_2(t)$ |\n| :------: | :--------------: | :--------------: | :-------------------: | :-------------------: | :---------------------------------------------: |\n| 1 | $1/4$ | $0$ | $\\approx -1.386$ | Undefined | Undefined |\n| 2 | $1/4$ | $1/4$ | $\\approx -1.386$ | $\\approx -1.386$ | $0$ |\n| 3 | $7/12$ | $1/4$ | $\\approx -0.539$ | $\\approx -1.386$ | $\\approx 0.847$ |\n| 4 | $7/12$ | $7/12$ | $\\approx -0.539$ | $\\approx -0.539$ | $0$ |\n| 6 | $7/12$ | $13/12$ | $\\approx -0.539$ | $\\approx 0.080$ | $\\approx -0.619$ |\n| 7 | $19/12$ | $13/12$ | $\\approx 0.459$ | $\\approx 0.080$ | $\\approx 0.379$ |\n\nThe values of the difference, $\\{0, 0.847, 0, -0.619, 0.379\\}$, fluctuate significantly and do not appear to be constant. This suggests that the proportional hazards assumption is not supported by these data.\n\n### Option-by-Option Analysis\n\n**A. For each group $g$, the Nelson–Aalen estimator is computed using only within-group risk sets: $\\widehat H_g(t) = \\sum_{t_{gj} \\le t} d_g(t_{gj}) / r_g(t_{gj})$, where $r_g(t_{gj})$ counts subjects at risk in group $g$ strictly before $t_{gj}$ and $d_g(t_{gj})$ counts events in group $g$ at $t_{gj}$. To assess proportional hazards, evaluate $\\log \\widehat H_1(t) - \\log \\widehat H_2(t)$ over pooled event times $t$ where both $\\widehat H_g(t) > 0$ and check for approximate constancy in $t$.**\nThis statement accurately describes the standard procedure. To estimate group-specific cumulative hazards, one must use group-specific risk sets and event counts. The diagnostic test involves comparing these estimated functions, specifically looking for a constant difference on the logarithmic scale, which corresponds to a constant ratio on the original scale. The restriction to times where the estimators are positive is mathematically necessary for the logarithm.\n**Verdict: Correct**\n\n**B. A simpler and equivalent PH diagnostic is to compute Kaplan–Meier estimators $\\widehat S_g(t)$ for each group and check whether $\\widehat S_1(t) - \\widehat S_2(t)$ is approximately constant in $t$; if so, PH holds.**\nThis is incorrect. Under PH, $S_2(t) = [S_1(t)]^\\theta$. This is a power relationship, not an additive one. The difference $S_1(t) - S_2(t)$ will not be constant. A correct diagnostic using survival functions is to plot $\\log(-\\log \\widehat S_2(t))$ versus $\\log(-\\log \\widehat S_1(t))$, which should yield a straight line with slope $\\theta$. Alternatively, a plot of $\\log(-\\log \\widehat S_2(t)) - \\log(-\\log \\widehat S_1(t))$ versus $\\log(t)$ should be a horizontal line at height $\\log \\theta$.\n**Verdict: Incorrect**\n\n**C. To stabilize estimation, the denominators in the Nelson–Aalen increments for each group should be the pooled-at-risk counts across both groups, i.e., $r(t_j)$ from all subjects, rather than the within-group $r_g(t_j)$.**\nThis is incorrect for the purpose of creating a PH diagnostic plot. Using pooled risk sets is the method for estimating a *common* cumulative hazard function, which is a key component in constructing the log-rank test statistic for the null hypothesis $H_0: \\theta=1$. The goal here is different: to estimate $\\widehat H_1(t)$ and $\\widehat H_2(t)$ separately to see if their relationship is consistent with proportionality for any $\\theta$. This requires group-specific estimators.\n**Verdict: Incorrect**\n\n**D. Because under proportional hazards $H_2(t) = \\theta H_1(t)$ for some constant $\\theta$, an equivalent diagnostic to the log-difference check is to inspect whether $\\widehat H_2(t) / \\widehat H_1(t)$ is approximately constant in $t$ over times where both denominators are positive.**\nThis is correct. The relationship $H_2(t) = \\theta H_1(t)$ directly implies that the ratio $H_2(t) / H_1(t) = \\theta$ for all $t$ where $H_1(t) > 0$. Checking for constancy of the ratio of the estimators $\\widehat H_2(t) / \\widehat H_1(t)$ is therefore a direct and mathematically equivalent way to assess the PH assumption. Taking the logarithm linearizes this relationship, which is often preferred for visual inspection as it can make deviations from constancy easier to detect and stabilize variance, but the ratio plot is also a valid diagnostic.\n**Verdict: Correct**\n\n**E. If at a given time $t$ there are $d_g(t) \\ge 2$ tied events in group $g$, the Nelson–Aalen increment at $t$ is $d_g(t) / r_g(t)$ using the group-specific risk set size just prior to $t$; this tie handling is standard for Nelson–Aalen.**\nThis statement is correct. The Nelson-Aalen framework naturally accommodates tied event times. The hazard increment at time $t$ is estimated as the total number of events at that time divided by the number at risk just prior. This is the standard definition and handling of ties for the Nelson-Aalen estimator. While the provided data has no ties, the principle described is accurate.\n**Verdict: Correct**\n\n**F. Using the provided data, the computed values of $\\log \\widehat H_1(t) - \\log \\widehat H_2(t)$ over times where both are defined vary substantially with $t$, so the PH assumption is not supported in this sample.**\nAs shown in the calculations above, the computed difference $\\log \\widehat H_1(t) - \\log \\widehat H_2(t)$ takes on the values $\\{0, \\approx 0.847, 0, \\approx-0.619, \\approx 0.379\\}$ at the relevant time points. This sequence is clearly not constant. The variation is substantial relative to the values themselves. Therefore, a visual inspection of these data would lead one to question the validity of the PH assumption.\n**Verdict: Correct**\n\n**G. When $\\widehat H_g(t) = 0$ at early times, it is acceptable to define $\\log \\widehat H_g(t) = 0$ so that all pooled event times can be used in the log-difference plot without restriction.**\nThis is mathematically incorrect. The natural logarithm of $0$ is undefined (it tends to $-\\infty$). Assigning an arbitrary finite value like $0$ is a falsification of the mathematical properties of the logarithm function and would severely distort the diagnostic plot. The correct procedure is to restrict the domain of the plot to times $t$ where both $\\widehat H_1(t)$ and $\\widehat H_2(t)$ are strictly positive.\n**Verdict: Incorrect**",
            "answer": "$$\\boxed{ADEF}$$"
        },
        {
            "introduction": "While graphical checks are invaluable, formal testing and flexible modeling of non-proportionality offer greater rigor. This exercise introduces the varying-coefficient Cox model, where a coefficient $\\beta(t)$ is allowed to be a smooth function of time, estimated using penalized splines. This powerful approach allows you to formally test the null hypothesis of proportionality and to characterize the precise shape of a time-varying effect .",
            "id": "4991173",
            "problem": "A randomized clinical study follows $n = 1200$ patients after hospital discharge for acute myocardial infarction, with event time $T$ defined as days to first major adverse cardiac event and independent right-censoring. Let $X$ denote a baseline biomarker measured at discharge, and let $Z$ denote a vector of baseline confounders. The clinical question is whether the effect of $X$ on the hazard is constant over time, i.e., whether the proportional hazards assumption holds.\n\nFrom first principles, recall that the Cox proportional hazards model specifies the hazard function as $h(t \\mid X,Z) = h_{0}(t)\\exp\\{\\beta_{X} X + \\gamma^{\\top} Z\\}$, where $h_{0}(t)$ is an unspecified baseline hazard and $\\beta_{X}$ is a constant log-hazard ratio. A natural extension to allow non-proportionality replaces $\\beta_{X}$ by a smooth function of time, $\\beta_{X}(t)$, yielding a varying-coefficient Cox model $h(t \\mid X,Z) = h_{0}(t)\\exp\\{\\beta_{X}(t) X + \\gamma^{\\top} Z\\}$. The proportional hazards null for $X$ is equivalently $H_{0}:\\beta_{X}'(t)=0$ for all $t$, i.e., $\\beta_{X}(t)$ is constant in $t$.\n\nYou are asked to identify a coherent modeling-and-testing strategy that uses penalized splines to represent $\\beta_{X}(t)$ and provides a valid inferential test of $H_{0}:\\beta_{X}'(t)=0$ over time. Which of the following options describes a statistically sound approach? Select all that apply.\n\nA. Represent $\\beta_{X}(t)$ using a basis $\\{b_{0}(t),\\ldots,b_{K}(t)\\}$ where $b_{0}(t)\\equiv 1$ spans the constant component and $\\{b_{1}(t),\\ldots,b_{K}(t)\\}$ span smooth deviations, and estimate coefficients by maximizing a penalized partial log-likelihood with a roughness penalty proportional to $\\int\\{\\beta_{X}'(t)\\}^{2}\\,dt$, so that the penalty’s null space consists of constant functions. Use the mixed-model representation in which the coefficients of $\\{b_{1}(t),\\ldots,b_{K}(t)\\}$ are Gaussian with variance parameter $\\tau^{2}$, and test $H_{0}:\\tau^{2}=0$ via a restricted likelihood ratio test with a mixture of chi-square reference distribution due to the boundary at zero.\n\nB. Fit a standard Cox model with a time-dependent covariate $X(t)=X\\cdot t$ and test the coefficient of $X\\cdot t$ with a Wald test. If the coefficient is not statistically significant, conclude $H_{0}:\\beta_{X}'(t)=0$.\n\nC. Fit a varying-coefficient Cox model where $\\beta_{X}(t)$ is represented by a cubic regression spline with a penalty on $\\int\\{\\beta_{X}''(t)\\}^{2}\\,dt$, estimate the smoothing parameter by maximizing the (unpenalized) partial likelihood, and test proportionality by a likelihood ratio test comparing the penalized fit to a model without $X$.\n\nD. Fit the varying-coefficient Cox model using a penalized spline for $\\beta_{X}(t)$ as in option A, and, in addition or as an alternative diagnostic, compute scaled Schoenfeld residuals for $X$ from a standard Cox model and regress them on a suitable function of time with a smooth term; test for zero trend over time to assess $H_{0}:\\beta_{X}'(t)=0$.\n\nE. Fit an additive hazards model with a smoothed baseline hazard and test proportionality by a Kolmogorov–Smirnov test on martingale residuals against time; if the test is not significant, conclude $H_{0}:\\beta_{X}'(t)=0$ for the Cox model.",
            "solution": "The problem statement poses a valid and well-defined question in the field of statistical modeling for survival analysis. It asks to identify a statistically sound strategy to test the proportional hazards (PH) assumption for a covariate $X$ in a Cox model. The PH assumption states that the coefficient $\\beta_X$ is constant over time, $t$. The null hypothesis is thus $H_{0}: \\beta_{X}(t) = \\text{constant}$ for all $t$, which is equivalent to $H_{0}: \\beta_{X}'(t)=0$ for all $t$. The strategy should involve representing the potentially time-varying coefficient $\\beta_X(t)$ using penalized splines.\n\nA penalized spline is a flexible tool for modeling a smooth function like $\\beta_X(t)$. The function is represented as a linear combination of basis functions, $\\beta_X(t) = \\sum_{k=0}^{K} \\theta_k b_k(t)$. To prevent overfitting and induce smoothness, a penalty term is added to the partial log-likelihood function that is being maximized. This penalty is a measure of the \"roughness\" of the function $\\beta_X(t)$. A common form for the penalty is $\\lambda \\int [\\beta_X^{(m)}(t)]^2 dt$, where $\\beta_X^{(m)}(t)$ is the $m$-th derivative of the function, and $\\lambda \\ge 0$ is a smoothing parameter that controls the trade-off between fit to the data and smoothness.\n\nFor testing the null hypothesis that $\\beta_X(t)$ is constant, a judicious choice of penalty is one whose null space (the set of functions for which the penalty is zero) corresponds exactly to the set of functions under the null hypothesis. Since a function is constant if and only if its first derivative is zero, a penalty proportional to $\\int [\\beta_{X}'(t)]^{2}\\,dt$ is a natural choice.\n\nA powerful framework for inference with penalized splines is their representation as linear mixed-effects models. In this formulation, the spline basis is decomposed into components representing the null space of the penalty (fixed effects) and components representing deviations from the null space (random effects). The smoothing parameter $\\lambda$ is related to the variance of the random effects. Testing the null hypothesis then becomes equivalent to testing if this variance component is zero.\n\nWe now evaluate each option based on these principles.\n\n### Option-by-Option Analysis\n\n**A. Represent $\\beta_{X}(t)$ using a basis $\\{b_{0}(t),\\ldots,b_{K}(t)\\}$ where $b_{0}(t)\\equiv 1$ spans the constant component and $\\{b_{1}(t),\\ldots,b_{K}(t)\\}$ span smooth deviations, and estimate coefficients by maximizing a penalized partial log-likelihood with a roughness penalty proportional to $\\int\\{\\beta_{X}'(t)\\}^{2}\\,dt$, so that the penalty’s null space consists of constant functions. Use the mixed-model representation in which the coefficients of $\\{b_{1}(t),\\ldots,b_{K}(t)\\}$ are Gaussian with variance parameter $\\tau^{2}$, and test $H_{0}:\\tau^{2}=0$ via a restricted likelihood ratio test with a mixture of chi-square reference distribution due to the boundary at zero.**\n\nThis option describes a statistically sophisticated and correct procedure.\n1.  **Model Specification**: Representing $\\beta_X(t)$ with a basis and separating it into a constant part (fixed effect) and smooth deviations (random effects) is standard practice in modern smoothing.\n2.  **Penalty Choice**: The penalty $\\int\\{\\beta_{X}'(t)\\}^{2}\\,dt$ is perfectly suited for testing the null hypothesis $H_0: \\beta_{X}'(t) = 0$. The penalty is zero if and only if $\\beta_X(t)$ is a constant function, aligning the penalty's null space with the hypothesis being tested.\n3.  **Mixed Model Framework**: The re-expression of the penalized spline model as a mixed model is a key theoretical and computational device. The coefficients of the smooth deviation components are treated as random effects with mean zero and variance $\\tau^2$.\n4.  **Hypothesis Test**: In this framework, the null hypothesis an an absolutely continuous $\\beta_X(t)$ being constant is equivalent to the variance of the random effects being zero, i.e., $H_0: \\tau^2=0$. Since a variance cannot be negative, this is a test on the boundary of the parameter space. A standard likelihood ratio test is invalid. A restricted likelihood ratio test (RLRT) with a reference distribution that is a mixture of chi-square distributions (e.g., $0.5\\chi^2_0 + 0.5\\chi^2_1$) is the correct procedure for this boundary test.\n\nEvery step described is coherent and state-of-the-art.\n**Verdict: Correct**\n\n**B. Fit a standard Cox model with a time-dependent covariate $X(t)=X\\cdot t$ and test the coefficient of $X\\cdot t$ with a Wald test. If the coefficient is not statistically significant, conclude $H_{0}:\\beta_{X}'(t)=0$.**\n\nThis option describes a test for a specific form of non-proportionality. The model assumes $h(t \\mid X,Z) = h_{0}(t)\\exp\\{\\beta_{X} X + \\delta (X \\cdot t) + \\gamma^{\\top} Z\\}$, which implies $\\beta_X(t) = \\beta_X + \\delta t$. This test is sensitive only to linear departures from proportionality. It lacks the flexibility to detect more complex patterns, such as a quadratic or periodic time-varying effect, for which a spline-based approach is designed. The problem specifically asks for a strategy that *uses penalized splines*, which this option does not. While it's a valid, albeit simple, test for PH, it does not fulfill the requirements of the prompt.\n**Verdict: Incorrect**\n\n**C. Fit a varying-coefficient Cox model where $\\beta_{X}(t)$ is represented by a cubic regression spline with a penalty on $\\int\\{\\beta_{X}''(t)\\}^{2}\\,dt$, estimate the smoothing parameter by maximizing the (unpenalized) partial likelihood, and test proportionality by a likelihood ratio test comparing the penalized fit to a model without $X$.**\n\nThis option contains several critical statistical errors.\n1.  **Smoothing Parameter Estimation**: The proposition to estimate the smoothing parameter by maximizing the *unpenalized* partial likelihood is fundamentally flawed. The smoothing parameter's role is to balance the (unpenalized) likelihood against the penalty term. Maximizing the unpenalized likelihood alone would always lead to choosing the minimal penalty (smoothing parameter $\\lambda=0$), resulting in an unpenalized, interpolating spline fit that is typically severely overfitted. Correct methods involve cross-validation or maximizing a penalized (or restricted) likelihood.\n2.  **Hypothesis Test Target**: The proposed test compares the penalized fit (with $\\beta_X(t)X$) to a model *without* the covariate $X$ at all. This tests the overall significance of $X$ (i.e., $H_0: \\beta_X(t)=0$ for all $t$), not the proportional hazards assumption ($H_0: \\beta_X(t) = \\text{constant}$). The correct comparison for testing PH would be between the model with a time-varying coefficient $\\beta_X(t)$ and the model with a constant coefficient $\\beta_X$.\n3.  **Penalty Choice**: While a penalty on the second derivative, $\\int\\{\\beta_X''(t)\\}^2 dt$, is common for smoothing, its null space consists of linear functions ($a+bt$), not just constant functions. While it can be used to test for constancy, it requires a more careful decomposition of the model than is implied here, and is less direct than the first-derivative penalty in option A for this specific hypothesis.\n\nGiven the fundamental errors in smoothing parameter estimation and hypothesis test formulation, this option is invalid.\n**Verdict: Incorrect**\n\n**D. Fit the varying-coefficient Cox model using a penalized spline for $\\beta_{X}(t)$ as in option A, and, in addition or as an alternative diagnostic, compute scaled Schoenfeld residuals for $X$ from a standard Cox model and regress them on a suitable function of time with a smooth term; test for zero trend over time to assess $H_{0}:\\beta_{X}'(t)=0$.**\n\nThis option describes a comprehensive and practical strategy. It combines the formal, model-based test from option A with a standard, widely-used diagnostic procedure.\n1.  **Formal Test**: The first part references the valid method described in option A.\n2.  **Diagnostic Check**: The second part describes the use of Schoenfeld residuals. For a standard Cox model (assuming PH), the expected value of the Schoenfeld residuals for a covariate is constant over time. Plotting the (scaled) residuals against time and looking for a non-zero trend is a classical method to diagnose violations of the PH assumption.\n3.  **Testing the Trend**: Regressing the residuals on a function of time (e.g., using a linear model or, more flexibly, a smoother like a spline or LOESS) and testing if the trend is zero is a formalization of this diagnostic check. It is a valid and robust method for assessing the PH assumption.\n\nThe combination of a formal test based on a varying-coefficient model and a residual-based diagnostic constitutes a very strong and coherent strategy for assessing the PH assumption. They are complementary techniques that any careful analyst would consider.\n**Verdict: Correct**\n\n**E. Fit an additive hazards model with a smoothed baseline hazard and test proportionality by a Kolmogorov–Smirnov test on martingale residuals against time; if the test is not significant, conclude $H_{0}:\\beta_{X}'(t)=0$ for the Cox model.**\n\nThis option is incorrect for multiple reasons.\n1.  **Wrong Model Class**: It proposes fitting an additive hazards model ($h(t \\mid X) = h_0(t) + \\beta X$) to test a hypothesis about a proportional hazards model ($h(t \\mid X) = h_0(t)\\exp(\\beta X)$). These models represent fundamentally different assumptions about how covariates affect the hazard. One cannot, in general, test a hypothesis about one model's parameters by fitting a different class of model.\n2.  **Inappropriate Test Statistic**: A Kolmogorov–Smirnov test is designed to compare a sample distribution to a reference distribution or to compare two sample distributions. It is not the standard tool for detecting a trend in residuals over time. Tests based on correlation or cumulative sums of residuals are appropriate for that purpose.\n3.  **Residual Type**: While martingale residuals are useful for overall goodness-of-fit assessment, Schoenfeld residuals are specifically designed for checking the PH assumption for individual covariates.\n\nThe entire procedure is logically and statistically flawed.\n**Verdict: Incorrect**",
            "answer": "$$\\boxed{AD}$$"
        },
        {
            "introduction": "Ultimately, statistical diagnosis must inform a sound analysis strategy, especially in complex clinical trial settings with competing priorities. This final practice confronts a scenario with crossing hazards, where a single hazard ratio is misleading, forcing a decision that balances statistical validity with clinical interpretability. By weighing options like stratification and alternative estimands such as the Restricted Mean Survival Time (RMST), you will learn to construct a robust analysis plan that faithfully represents the data and meets the needs of a clinical audience .",
            "id": "4991174",
            "problem": "A randomized, multicenter oncology trial compares an investigational therapy versus control on overall survival. Patients were randomized $1:1$ within centers, yielding $n=800$ participants across $J=20$ centers. Administrative censoring occurs at $T_{\\max}=24$ months; the observed number of events is $E \\approx 480$. Centers differ substantially in baseline mortality, with empirically observed center-specific baseline hazards differing by up to a factor of $3$ relative to one another, but there is no clinical rationale for a treatment-by-center interaction.\n\nLet $T$ denote survival time, $X$ the vector of baseline covariates including treatment $A \\in \\{0,1\\}$, and $Z$ a categorical indicator of center. The hazard function is $h(t \\mid X)$. The proportional hazards assumption posits $h(t \\mid X)=h_{0}(t)\\exp\\{\\beta^{\\top}X\\}$ for some baseline hazard $h_{0}(t)$ and constant regression coefficients $\\beta$. In a stratified Cox model by $Z$, the baseline hazard is allowed to vary by stratum while the regression coefficients are constrained to be common across strata.\n\nExploratory diagnostics show strong evidence against proportional hazards for treatment: scaled Schoenfeld residuals exhibit a monotone trend with time for $A$, and a global test for proportional hazards for $A$ yields $p<0.001$. Visual inspection of estimated hazard ratio functions (using flexible, nonparametric smoothing of time-varying effects) suggests an early hazard ratio above $1$ that crosses below $1$ around $t \\approx 6$ months, with increasing separation thereafter, consistent with a delayed treatment effect. Weighted log-rank tests with late-event weights show more sensitivity than standard log-rank, corroborating delayed benefit. The clinical audience requests a single summary measure that is robust and easy to communicate for guideline development, but they also value a faithful depiction of time-varying effects. The investigators wish to adjust for center to address baseline hazard heterogeneity without estimating a high-dimensional set of nuisance parameters and to maintain efficiency.\n\nYou must select a decision framework that addresses the detected non-proportional hazards, balancing clinical interpretability, bias control under model misspecification, and statistical efficiency. Which option below best implements such a framework in this setting?\n\nA. Declare a stratified Cox model by center with a flexible time-varying coefficient for treatment using a penalized spline in $\\log t$ as the sole analysis. Report the time-varying hazard ratio curve with pointwise confidence intervals and summarize with a weighted average hazard ratio over time.\n\nB. Pre-specify the restricted mean survival time at $\\tau=24$ months as the primary estimand and report the adjusted difference in restricted mean survival time between treatment arms, using a covariate-adjusted method (for example, regression on pseudo-observations) that includes center to improve efficiency. Supplement this with a stratified Cox model by center with a flexible time-varying treatment coefficient to display and test time-varying effects. Commit to both the time horizon $\\tau$ and smoothing degrees of freedom before unblinding.\n\nC. Fit an unstratified Cox proportional hazards model with treatment as the only covariate and use a robust (sandwich) variance estimator to protect inferences; interpret the coefficient as an average causal hazard ratio. Do not adjust for center to avoid loss of efficiency from over-parameterization.\n\nD. To avoid bias from non-proportional hazards, stratify by treatment and present Kaplan–Meier curves for each arm, reporting the difference in medians if both are estimable. Rely on visual comparison for clinical interpretation.\n\nE. Replace the Cox model with a log-normal accelerated failure time model including a random intercept for center to capture heterogeneity. Report the acceleration factor as the primary estimand, which is a single summary number, and use likelihood-based inference for efficiency even if hazards cross.",
            "solution": "The user wants a meticulous and exacting professor in the STEM fields (mathematics, physics, chemistry, biology, engineering, and related disciplines) to critically validate the problem statement—excluding the options—before attempting a solution. Based on a valid statement, I will then derive the correct answer and evaluate each provided option.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n*   **Trial Design:** Randomized, multicenter oncology trial.\n*   **Comparison:** Investigational therapy versus control.\n*   **Endpoint:** Overall survival.\n*   **Randomization:** $1:1$ within centers.\n*   **Sample Size:** $n=800$ participants.\n*   **Number of Centers:** $J=20$.\n*   **Censoring:** Administrative censoring at $T_{\\max}=24$ months.\n*   **Events:** Observed number of events $E \\approx 480$.\n*   **Center Heterogeneity:** Center-specific baseline hazards differ by up to a factor of $3$.\n*   **Interaction Assumption:** No clinical rationale for a treatment-by-center interaction.\n*   **Variable Definitions:**\n    *   $T$: Survival time.\n    *   $X$: Vector of baseline covariates, including treatment $A \\in \\{0,1\\}$.\n    *   $Z$: Categorical indicator of center.\n*   **Model Definitions:**\n    *   Hazard function: $h(t \\mid X)$.\n    *   Proportional Hazards (PH) Model: $h(t \\mid X)=h_{0}(t)\\exp\\{\\beta^{\\top}X\\}$.\n    *   Stratified Cox Model by $Z$: Baseline hazard $h_{0j}(t)$ varies by stratum $j$, while coefficients $\\beta$ are common.\n*   **Diagnostic Findings (Violation of PH for Treatment A):**\n    *   Scaled Schoenfeld residuals show a monotone trend with time.\n    *   Global test for PH for $A$ gives $p<0.001$.\n    *   Estimated hazard ratio (HR) function is time-varying: HR $>1$ for early $t$, crosses below $1$ at $t \\approx 6$ months, and then decreases, indicating a delayed treatment effect (crossing hazards).\n    *   Weighted log-rank tests with late-event weights are more sensitive than standard log-rank.\n*   **Analysis Objectives:**\n    1.  Provide a single, robust, and easily communicable summary measure for clinical guidelines.\n    2.  Provide a faithful depiction of the time-varying effects.\n    3.  Adjust for center to address baseline hazard heterogeneity.\n    4.  Avoid estimating a high-dimensional set of nuisance parameters for centers.\n    5.  Maintain statistical efficiency.\n\n**Step 2: Validate Using Extracted Givens**\n\n*   **Scientifically Grounded:** The problem describes a standard clinical trial scenario in oncology. All statistical concepts (randomization, censoring, Cox model, proportional hazards, Schoenfeld residuals, stratified analysis, log-rank tests, restricted mean survival time, accelerated failure time models) are established principles in biostatistics. The phenomenon of non-proportional, crossing hazards is a well-documented and critical issue in modern oncology trials, particularly with immunotherapies. The problem is firmly grounded in scientific and statistical practice.\n*   **Well-Posed:** The task is to select the best statistical analysis framework from a set of options that balances multiple, clearly stated, and realistic objectives (robust summary, accurate depiction of effects, efficiency, etc.) in the face of a specific data challenge (crossing hazards). This is a well-posed decision-theory problem within the domain of applied statistics.\n*   **Objective:** The problem statement is presented in precise, objective, and technical language. The requirements of the stakeholders are listed as constraints on the solution, not as subjective opinions.\n*   **Completeness and Consistency:** The problem is self-contained and provides sufficient detail to evaluate the proposed statistical strategies. The various requirements, while representing competing priorities (e.g., simplicity vs. completeness), are not logically contradictory but reflect the real-world trade-offs inherent in statistical analysis.\n*   **Realism:** The parameters ($n=800$, $J=20$, $E \\approx 480$, $T_{\\max}=24$ months) are entirely realistic for a Phase III oncology trial. The described findings are plausible and present a common, non-trivial challenge.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. It is a well-formulated, realistic, and scientifically sound problem in applied biostatistics. I will proceed with the solution and evaluation of the options.\n\n### Solution Derivation\n\nThe central challenge is the definitive violation of the proportional hazards (PH) assumption for the treatment effect, characterized by crossing hazard curves. This implies that the hazard ratio, $\\text{HR}(t)$, is not constant. Specifically, the therapy appears initially harmful (or less effective than control, $\\text{HR}(t)>1$) and becomes beneficial later ($\\text{HR}(t)<1$). In such cases, a single hazard ratio from a standard Cox PH model is a poor summary; it represents a weighted average of $\\log(\\text{HR}(t))$ that can be close to $0$ (implying an HR near $1$), masking the true dynamics of early harm and late benefit.\n\nThe analysis framework must therefore satisfy several key criteria derived from the problem statement:\n1.  **Robustness to NPH:** The primary summary measure must be interpretable and not biased by the non-proportionality.\n2.  **Interpretability:** The summary measure must be a single number that is easy for clinicians to understand and use for guidelines (e.g., \"how much survival time is gained?\").\n3.  **Completeness:** The framework should also visualize or describe the time-varying nature of the effect, as requested.\n4.  **Center Adjustment:** It must account for the substantial heterogeneity in baseline hazards across the $J=20$ centers efficiently. A stratified analysis is an ideal approach for this, as it allows each center to have its own baseline hazard function, $h_{0j}(t)$, without estimating $J-1$ extra parameters, thus preserving power.\n\nWe will now evaluate each option against these criteria.\n\n**Option A: Declare a stratified Cox model by center with a flexible time-varying coefficient for treatment using a penalized spline in $\\log t$ as the sole analysis. Report the time-varying hazard ratio curve with pointwise confidence intervals and summarize with a weighted average hazard ratio over time.**\n\n*   This approach correctly uses stratification by center to handle baseline hazard heterogeneity efficiently. It also directly models the non-proportional hazards using a time-varying coefficient, $\\beta(t)$, which addresses the need to depict the effect over time.\n*   However, the proposed summary measure—a weighted average hazard ratio—is problematic. When $\\text{HR}(t)$ crosses $1$, any average can be highly misleading. For instance, an average HR could be $1.0$, suggesting no effect, while hiding significant early harm and late benefit. The choice of weights is also arbitrary and can influence the result. This fails the criterion for a robust and easily communicable summary measure.\n*   **Verdict: Incorrect.** While it correctly models the time-varying effect and stratifies by center, the proposed summary measure is non-robust and difficult to interpret in a crossing hazards scenario.\n\n**Option B: Pre-specify the restricted mean survival time at $\\tau=24$ months as the primary estimand and report the adjusted difference in restricted mean survival time between treatment arms, using a covariate-adjusted method (for example, regression on pseudo-observations) that includes center to improve efficiency. Supplement this with a stratified Cox model by center with a flexible time-varying treatment coefficient to display and test time-varying effects. Commit to both the time horizon $\\tau$ and smoothing degrees of freedom before unblinding.**\n\n*   This option proposes the difference in Restricted Mean Survival Time (RMST) as the primary summary measure. The RMST up to a time horizon $\\tau$ is the area under the survival curve $S(t)$ from $0$ to $\\tau$, i.e., $\\text{RMST}(\\tau) = \\int_0^\\tau S(t) dt$. The difference in RMST between arms, $\\Delta(\\tau)$, has a direct and highly intuitive clinical interpretation: the average survival time gained (or lost) on treatment up to time $\\tau$. This estimand does not rely on the PH assumption and is robust to crossing hazards.\n*   Setting the time horizon $\\tau=24$ months is appropriate as it corresponds to the maximum administrative follow-up time $T_{\\max}$, ensuring all observed data contribute to the estimate.\n*   It addresses center heterogeneity by including center as a covariate in an adjusted RMST analysis, which is known to improve statistical efficiency.\n*   Crucially, it also satisfies the second requirement of the stakeholders by supplementing the primary RMST analysis with a flexible, time-varying coefficient Cox model (stratified by center) to visualize the HR curve, thereby providing a faithful depiction of the delayed effect.\n*   This dual approach perfectly balances the need for a single robust summary measure with the need for a detailed characterization of the effect over time. The mention of pre-specification is a hallmark of a rigorous analysis plan.\n*   **Verdict: Correct.** This framework is comprehensive, robust, and directly addresses all specified requirements and constraints of the problem.\n\n**Option C: Fit an unstratified Cox proportional hazards model with treatment as the only covariate and use a robust (sandwich) variance estimator to protect inferences; interpret the coefficient as an average causal hazard ratio. Do not adjust for center to avoid loss of efficiency from over-parameterization.**\n\n*   This approach is fundamentally flawed. It knowingly applies a misspecified model (the PH model) to data with strong evidence of NPH. The resulting single HR is a misleading average, as discussed for Option A. A robust variance estimator corrects standard errors for model misspecification but does not fix the bias or poor interpretability of the coefficient estimate itself.\n*   It explicitly advises against adjusting for center. This contradicts the problem's statement of substantial center heterogeneity and the investigators' desire for adjustment. Stratified randomization was used, and failing to account for the stratification variable in the analysis can lead to a loss of efficiency and power. The justification (\"avoid loss of efficiency from over-parameterization\") is incorrect; stratification by center *avoids* over-parameterization by not estimating center-specific coefficients.\n*   **Verdict: Incorrect.** This option ignores the primary data phenomenon (NPH), fails to meet a key analysis objective (adjust for center), and provides a flawed justification for its choices.\n\n**Option D: To avoid bias from non-proportional hazards, stratify by treatment and present Kaplan–Meier curves for each arm, reporting the difference in medians if both are estimable. Rely on visual comparison for clinical interpretation.**\n\n*   Presenting Kaplan-Meier (K-M) curves is a good exploratory step, but as a final analysis, it is incomplete. \"Stratify by treatment\" is imprecise phrasing for \"generate K-M curves for each treatment group\".\n*   The proposed summary measure is the difference in median survival times. With an event rate of $E/n \\approx 480/800 = 60\\%$, the median ($50\\%$ quantile) might be estimable for the control arm but may not be reached in the treatment arm by $T_{\\max}=24$ months if there is a substantial late benefit. In such cases, the median is not estimable, and no summary measure can be reported. The RMST (Option B) avoids this problem as it is always estimable for a given $\\tau$.\n*   This option fails to adjust for center effects, thus ignoring a major source of heterogeneity and forgoing an opportunity to increase statistical power and precision, which was a stated goal.\n*   **Verdict: Incorrect.** This approach is insufficient. It fails to provide a guaranteed summary measure, and it does not perform the requested and necessary adjustment for center.\n\n**Option E: Replace the Cox model with a log-normal accelerated failure time model including a random intercept for center to capture heterogeneity. Report the acceleration factor as the primary estimand, which is a single summary number, and use likelihood-based inference for efficiency even if hazards cross.**\n\n*   An Accelerated Failure Time (AFT) model is a valid alternative to a PH model. It models the effect of covariates as a multiplicative factor on the time scale. The log-normal AFT model is one specific parametric choice. Using a random intercept for center is a valid way to handle clustering, analogous to stratification in the Cox model.\n*   The main issue is that this approach replaces one strong assumption (proportional hazards) with another (a specific parametric AFT relationship). There is no evidence in the problem statement that the AFT assumption is more appropriate than the PH assumption; in fact, the complex crossing hazard pattern might violate the AFT assumption as well. While some AFT models can produce crossing hazards, it is not their typical behavior, and the single \"acceleration factor\" summary would still fail to capture the qualitative change in the treatment effect over time.\n*   This option fails to provide a \"faithful depiction of time-varying effects\"; it instead summarizes the effect with a single number based on a strong, unverified modeling assumption. The RMST approach in Option B is superior because it is largely assumption-free.\n*   **Verdict: Incorrect.** This option is a reasonable but inferior alternative to Option B. It imposes a strong parametric assumption that may be violated and does not fulfill the requirement of depicting the time-varying nature of the effect.\n\n**Conclusion:**\n\nOption B provides the most statistically sound and complete framework. It uses the RMST difference as a primary estimand, which is robust, interpretable, and well-suited for NPH scenarios with crossing hazards. It complements this robust summary with a flexible graphical analysis to satisfy the clinical need to understand the timing of the treatment effect. Finally, it correctly and efficiently handles the multisite structure of the data.",
            "answer": "$$\\boxed{B}$$"
        }
    ]
}