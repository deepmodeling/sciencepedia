{
    "hands_on_practices": [
        {
            "introduction": "The core output of a Cox proportional hazards model is a set of estimated coefficients, denoted as $\\hat{\\beta}$, which represent log-hazard ratios. The first and most fundamental step in interpreting the model is to translate these coefficients into hazard ratios ($HR$), which quantify the multiplicative effect of a covariate on the hazard rate. This practice provides a foundational exercise in calculating and interpreting the hazard ratio, a key metric for communicating the results of survival analysis .",
            "id": "1911775",
            "problem": "In a clinical trial for a new cardiovascular drug, 'Cor-Stabil,' researchers use a Cox proportional hazards model to analyze the time until a major adverse cardiac event occurs. The model aims to assess the drug's effectiveness while controlling for patient age.\n\nThe hazard function for a patient is given by the model:\n$$h(t | X_1, X_2) = h_0(t) \\exp(\\beta_1 X_1 + \\beta_2 X_2)$$\nHere, $h_0(t)$ is the baseline hazard function. The covariates are:\n- $X_1$: Treatment group, a binary variable where $X_1 = 1$ for patients receiving Cor-Stabil and $X_1 = 0$ for patients in the placebo group.\n- $X_2$: Patient's age in years at the beginning of the study, a continuous variable.\n\nAfter analyzing the data, the estimated coefficients are found to be $\\beta_1 = \\ln(0.65)$ and $\\beta_2 = 0.03$.\n\nCalculate the hazard ratio for a patient taking Cor-Stabil compared to a patient taking the placebo, assuming the two patients being compared are of the same age. Express your answer as a decimal.",
            "solution": "The Cox model specifies the hazard at time $t$ as $h(t \\mid X_{1}, X_{2}) = h_{0}(t)\\exp(\\beta_{1}X_{1} + \\beta_{2}X_{2})$. The hazard ratio comparing a Cor-Stabil patient ($X_{1}=1$) to a placebo patient ($X_{1}=0$) of the same age (so $X_{2}$ equal in both) is\n$$\n\\text{HR} = \\frac{h(t \\mid X_{1}=1, X_{2}=a)}{h(t \\mid X_{1}=0, X_{2}=a)} = \\frac{h_{0}(t)\\exp(\\beta_{1}\\cdot 1 + \\beta_{2}a)}{h_{0}(t)\\exp(\\beta_{1}\\cdot 0 + \\beta_{2}a)}.\n$$\nCanceling $h_{0}(t)$ and the common $\\exp(\\beta_{2}a)$ factor yields\n$$\n\\text{HR} = \\exp(\\beta_{1}).\n$$\nWith $\\beta_{1} = \\ln(0.65)$, use $\\exp(\\ln(x)) = x$ to obtain\n$$\n\\text{HR} = \\exp(\\ln(0.65)) = 0.65.\n$$\nThus, the hazard for Cor-Stabil relative to placebo, at the same age, is $0.65$.",
            "answer": "$$\\boxed{0.65}$$"
        },
        {
            "introduction": "Beyond assessing relative risk, a key application of the Cox model is to predict an individual's survival probability at a specific point in time. This is accomplished by integrating the model's parametric component (the hazard ratio) with its non-parametric component (the estimated cumulative baseline hazard). This exercise demonstrates how to synthesize these two parts to generate concrete predictions, highlighting the unique semi-parametric structure of the Cox model .",
            "id": "1911715",
            "problem": "In a reliability study for a new type of electronic component, the time-to-failure is modeled using a Cox proportional hazards framework. The hazard rate for a component at time $t$ is given by the model $h(t | Z) = h_0(t) \\exp(\\beta Z)$, where $t$ is the operational time in years, $Z$ is a continuous covariate representing the component's manufacturing precision score, and $h_0(t)$ is an unspecified baseline hazard function.\n\nThe cumulative hazard function for a component with covariate $Z$ is $H(t | Z) = H_0(t) \\exp(\\beta Z)$, where $H_0(t) = \\int_0^t h_0(\\tau) d\\tau$ is the cumulative baseline hazard. The probability of a component surviving beyond time $t$, known as the survival probability, is given by the relationship $S(t | Z) = \\exp(-H(t | Z))$.\n\nFrom an analysis of failure data, the maximum likelihood estimate for the model's regression coefficient is found to be $\\hat{\\beta} = \\ln(3)$. The non-parametric estimate of the cumulative baseline hazard at the 5-year mark is $\\hat{H}_0(5) = 0.2$.\n\nUsing these estimates, calculate the predicted 5-year survival probability for a component with a manufacturing precision score of $Z=1$. Round your final answer to four significant figures.",
            "solution": "We use the Cox proportional hazards model. The cumulative hazard for covariate value $Z$ at time $t$ is $H(t \\mid Z) = H_{0}(t)\\exp(\\beta Z)$, and the survival function is $S(t \\mid Z) = \\exp(-H(t \\mid Z))$.\n\nAt $t=5$ and $Z=1$, with estimates $\\hat{\\beta} = \\ln(3)$ and $\\hat{H}_{0}(5)=0.2$, the predicted cumulative hazard is\n$$\n\\hat{H}(5 \\mid 1) = \\hat{H}_{0}(5)\\exp(\\hat{\\beta}\\cdot 1) = 0.2 \\exp(\\ln(3)) = 0.2 \\cdot 3 = 0.6.\n$$\nTherefore the predicted survival probability is\n$$\n\\hat{S}(5 \\mid 1) = \\exp\\!\\left(-\\hat{H}(5 \\mid 1)\\right) = \\exp(-0.6) = \\exp\\!\\left(-\\frac{3}{5}\\right).\n$$\nNumerically, $\\exp(-0.6) \\approx 0.5488116361$, which rounded to four significant figures is $0.5488$.",
            "answer": "$$\\boxed{0.5488}$$"
        },
        {
            "introduction": "To fully grasp the mechanics of the Cox model, it is crucial to understand how its parameters are estimated from data. This advanced practice moves beyond application to the underlying theory, focusing on the partial log-likelihood function that forms the basis of inference. By deriving the score function (the gradient) and the observed information matrix (the negative Hessian), you will unpack the mathematical engine of maximum likelihood estimation as it applies to the Cox model .",
            "id": "3181431",
            "problem": "Consider the Cox proportional hazards (CPH) model with covariate vectors $x_{i} \\in \\mathbb{R}^{p}$ and regression parameter $\\beta \\in \\mathbb{R}^{p}$. Let $D$ denote the set of indices of observed events (non-tied), and let $R(t_{i})$ denote the risk set just prior to time $t_{i}$ for $i \\in D$. The partial log-likelihood is\n$$\n\\ell(\\beta) \\;=\\; \\sum_{i \\in D} \\Big\\{ x_{i}^{\\top}\\beta \\;-\\; \\ln\\!\\Big( \\sum_{j \\in R(t_{i})} \\exp(x_{j}^{\\top}\\beta) \\Big) \\Big\\}.\n$$\nStarting only from this definition, and using standard rules of vector calculus, do the following:\n\n1) Derive the score function $U(\\beta) \\equiv \\nabla_{\\beta}\\ell(\\beta)$ as a function of $\\beta$, $x_{i}$, and the risk sets $R(t_{i})$.\n\n2) Derive the observed information matrix $J(\\beta) \\equiv -\\nabla_{\\beta}^{2}\\ell(\\beta)$ as a function of $\\beta$, $x_{i}$, and the risk sets $R(t_{i})$.\n\nThen, consider the following dataset with $p=2$ covariates:\n- Subject $1$: event time $t_{1}=2$, event indicator $\\delta_{1}=1$, covariate $x_{1}=\\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$.\n- Subject $2$: event time $t_{2}=3$, event indicator $\\delta_{2}=1$, covariate $x_{2}=\\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$.\n- Subject $3$: censoring time $t_{3}=4$, event indicator $\\delta_{3}=0$, covariate $x_{3}=\\begin{pmatrix}1 \\\\ 1\\end{pmatrix}$.\n- Subject $4$: event time $t_{4}=5$, event indicator $\\delta_{4}=1$, covariate $x_{4}=\\begin{pmatrix}2 \\\\ -1\\end{pmatrix}$.\n\nAssume no tied event times. Use your general formulas to evaluate the observed information at $\\beta=\\begin{pmatrix}0 \\\\ 0\\end{pmatrix}$ and compute the determinant of $J(\\beta)$ at this value. Round your final numerical answer to four significant figures. Express the final answer as a pure number without units.",
            "solution": "The problem asks for the derivation of the score function and observed information matrix for the Cox proportional hazards model, followed by a calculation for a specific dataset.\n\nThe partial log-likelihood for the Cox model with no tied event times is given by\n$$\n\\ell(\\beta) = \\sum_{i \\in D} \\left\\{ x_{i}^{\\top}\\beta - \\ln\\left( \\sum_{j \\in R(t_{i})} \\exp(x_{j}^{\\top}\\beta) \\right) \\right\\}\n$$\nwhere $D$ is the set of indices for subjects with observed events, and $R(t_i)$ is the risk set at event time $t_i$.\n\n**1) Derivation of the Score Function $U(\\beta)$**\n\nThe score function is the gradient of the log-likelihood with respect to the parameter vector $\\beta$:\n$$\nU(\\beta) \\equiv \\nabla_{\\beta}\\ell(\\beta) = \\nabla_{\\beta} \\sum_{i \\in D} \\left\\{ x_{i}^{\\top}\\beta - \\ln\\left( \\sum_{j \\in R(t_{i})} \\exp(x_{j}^{\\top}\\beta) \\right) \\right\\}\n$$\nWe can interchange differentiation and summation:\n$$\nU(\\beta) = \\sum_{i \\in D} \\nabla_{\\beta} \\left\\{ x_{i}^{\\top}\\beta - \\ln\\left( \\sum_{j \\in R(t_{i})} \\exp(x_{j}^{\\top}\\beta) \\right) \\right\\}\n$$\nLet's differentiate a single term in the sum. The gradient of the first part is $\\nabla_{\\beta}(x_{i}^{\\top}\\beta) = x_i$.\n\nFor the second part, we use the chain rule for vector calculus, $\\nabla_{\\beta} \\ln(f(\\beta)) = \\frac{1}{f(\\beta)}\\nabla_{\\beta}f(\\beta)$. Let $S_i(\\beta) = \\sum_{j \\in R(t_{i})} \\exp(x_{j}^{\\top}\\beta)$.\nThe gradient of $S_i(\\beta)$ is:\n$$\n\\nabla_{\\beta} S_i(\\beta) = \\nabla_{\\beta} \\left(\\sum_{j \\in R(t_{i})} \\exp(x_{j}^{\\top}\\beta)\\right) = \\sum_{j \\in R(t_{i})} \\nabla_{\\beta}\\exp(x_{j}^{\\top}\\beta)\n$$\nUsing the chain rule again, $\\nabla_{\\beta}\\exp(u) = \\exp(u)\\nabla_{\\beta}u$. Here $u=x_j^\\top\\beta$, so $\\nabla_\\beta(x_j^\\top\\beta) = x_j$.\n$$\n\\nabla_{\\beta} S_i(\\beta) = \\sum_{j \\in R(t_{i})} \\exp(x_{j}^{\\top}\\beta) x_j\n$$\nThus, the gradient of the logarithmic term is:\n$$\n\\nabla_{\\beta} \\ln(S_i(\\beta)) = \\frac{\\sum_{j \\in R(t_{i})} x_j \\exp(x_{j}^{\\top}\\beta)}{\\sum_{j \\in R(t_{i})} \\exp(x_{j}^{\\top}\\beta)}\n$$\nLet us denote this term as $E(\\beta, t_i)$, which represents the expected value of the covariate vector over the risk set $R(t_i)$ at time $t_i$, weighted by the hazard terms $\\exp(x_j^\\top\\beta)$.\n\nCombining the parts, the gradient for the $i$-th term is $x_i - E(\\beta, t_i)$. The total score function is the sum over all events:\n$$\nU(\\beta) = \\sum_{i \\in D} \\left( x_i - \\frac{\\sum_{j \\in R(t_{i})} x_j \\exp(x_{j}^{\\top}\\beta)}{\\sum_{j \\in R(t_{i})} \\exp(x_{j}^{\\top}\\beta)} \\right) = \\sum_{i \\in D} (x_i - E(\\beta, t_i))\n$$\n\n**2) Derivation of the Observed Information Matrix $J(\\beta)$**\n\nThe observed information matrix is the negative of the Hessian of the log-likelihood: $J(\\beta) \\equiv -\\nabla_{\\beta}^{2}\\ell(\\beta) = -\\nabla_{\\beta} (U(\\beta)^{\\top})$. We differentiate the score function $U(\\beta)$ with respect to $\\beta^{\\top}$:\n$$\n\\nabla_{\\beta}^{2}\\ell(\\beta) = \\nabla_{\\beta}^\\top U(\\beta) = \\sum_{i \\in D} \\nabla_{\\beta}^\\top \\left( x_i - E(\\beta, t_i) \\right) = -\\sum_{i \\in D} \\nabla_{\\beta}^\\top E(\\beta, t_i)\n$$\nWe apply the quotient rule for vector differentiation to $E(\\beta, t_i) = \\frac{N_i(\\beta)}{D_i(\\beta)}$, where $N_i(\\beta) = \\sum_{j \\in R(t_i)} x_j \\exp(x_j^\\top \\beta)$ and $D_i(\\beta) = \\sum_{j \\in R(t_i)} \\exp(x_j^\\top \\beta)$.\n$$\n\\nabla_{\\beta}^\\top E(\\beta, t_i) = \\frac{D_i(\\beta) (\\nabla_{\\beta}^\\top N_i(\\beta)) - N_i(\\beta) (\\nabla_{\\beta}^\\top D_i(\\beta))}{D_i(\\beta)^2}\n$$\nThe necessary derivatives are:\n$$\n\\nabla_{\\beta}^\\top N_i(\\beta) = \\nabla_{\\beta}^\\top \\left( \\sum_{j \\in R(t_i)} x_j \\exp(x_j^\\top \\beta) \\right) = \\sum_{j \\in R(t_i)} x_j \\left( \\nabla_{\\beta}^\\top \\exp(x_j^\\top \\beta) \\right) = \\sum_{j \\in R(t_i)} x_j \\left( \\exp(x_j^\\top \\beta) x_j^\\top \\right) = \\sum_{j \\in R(t_i)} x_j x_j^\\top \\exp(x_j^\\top \\beta)\n$$\n$$\n\\nabla_{\\beta}^\\top D_i(\\beta) = \\nabla_{\\beta}^\\top \\left( \\sum_{j \\in R(t_i)} \\exp(x_j^\\top \\beta) \\right) = \\sum_{j \\in R(t_i)} \\exp(x_j^\\top \\beta) x_j^\\top = N_i(\\beta)^\\top\n$$\nSubstituting these back into the quotient rule expression:\n$$\n\\nabla_{\\beta}^\\top E(\\beta, t_i) = \\frac{\\left(\\sum_k \\exp(x_k^\\top\\beta)\\right) \\left(\\sum_j x_j x_j^\\top \\exp(x_j^\\top\\beta)\\right) - \\left(\\sum_j x_j \\exp(x_j^\\top\\beta)\\right) \\left(\\sum_k x_k \\exp(x_k^\\top\\beta)\\right)^\\top}{\\left(\\sum_k \\exp(x_k^\\top\\beta)\\right)^2}\n$$\nwhere all sums are over indices in $R(t_i)$. This simplifies to:\n$$\n\\nabla_{\\beta}^\\top E(\\beta, t_i) = \\frac{\\sum_j x_j x_j^\\top \\exp(x_j^\\top\\beta)}{\\sum_k \\exp(x_k^\\top\\beta)} - \\left(\\frac{\\sum_j x_j \\exp(x_j^\\top\\beta)}{\\sum_k \\exp(x_k^\\top\\beta)}\\right) \\left(\\frac{\\sum_k x_k \\exp(x_k^\\top\\beta)}{\\sum_l \\exp(x_l^\\top\\beta)}\\right)^\\top\n$$\nThis is $E[XX^\\top] - E[X]E[X]^\\top$, the conditional covariance matrix of the covariates in the risk set, which we denote $V(\\beta, t_i)$.\nThe Hessian is $\\nabla_{\\beta}^{2}\\ell(\\beta) = -\\sum_{i \\in D} V(\\beta, t_i)$. The observed information matrix is therefore:\n$$\nJ(\\beta) = \\sum_{i \\in D} V(\\beta, t_i) = \\sum_{i \\in D} \\left\\{ \\frac{\\sum_{j \\in R(t_i)} x_j x_j^\\top \\exp(x_j^\\top \\beta)}{\\sum_{k \\in R(t_i)} \\exp(x_k^\\top \\beta)} - E(\\beta, t_i)E(\\beta, t_i)^\\top \\right\\}\n$$\n\n**3) Calculation for the Dataset at $\\beta=0$**\n\nFirst, we identify the set of event indices $D$ and the risk sets $R(t_i)$ for each $i \\in D$. The event indicators are $\\delta_1=1$, $\\delta_2=1$, $\\delta_3=0$, $\\delta_4=1$. Thus, the set of event indices is $D = \\{1, 2, 4\\}$. The event times are $t_1=2$, $t_2=3$, $t_4=5$.\n- Risk set at $t_1=2$: $R(t_1) = \\{j : t_j \\ge 2\\} = \\{1, 2, 3, 4\\}$.\n- Risk set at $t_2=3$: $R(t_2) = \\{j : t_j \\ge 3\\} = \\{2, 3, 4\\}$.\n- Risk set at $t_4=5$: $R(t_4) = \\{j : t_j \\ge 5\\} = \\{4\\}$.\n\nWe need to evaluate $J(\\beta)$ at $\\beta=0$. At $\\beta=0$, $\\exp(x_j^\\top \\beta) = \\exp(0) = 1$ for all $j$. Let $|R(t_i)|$ be the number of subjects in risk set $R(t_i)$.\nThe expected value $E(0, t_i)$ simplifies to the sample mean of covariates in the risk set:\n$$\nE(0, t_i) = \\frac{\\sum_{j \\in R(t_i)} x_j}{|R(t_i)|} \\equiv \\bar{x}_{R(t_i)}\n$$\nThe first term in $V(0, t_i)$ becomes the mean of the outer products:\n$$\n\\frac{\\sum_{j \\in R(t_i)} x_j x_j^\\top}{|R(t_i)|}\n$$\nSo, $V(0, t_i)$ is the sample covariance matrix of covariates in $R(t_i)$ (using $1/N$ normalization):\n$$\nV(0, t_i) = \\frac{1}{|R(t_i)|} \\sum_{j \\in R(t_i)} x_j x_j^\\top - \\bar{x}_{R(t_i)}\\bar{x}_{R(t_i)}^\\top\n$$\nThen $J(0) = \\sum_{i \\in D} V(0, t_i) = V(0, t_1) + V(0, t_2) + V(0, t_4)$.\n\n- **Contribution from $t_1=2$**: $R(t_1)=\\{1, 2, 3, 4\\}$, so $|R(t_1)|=4$.\nThe covariates are $x_1=\\begin{pmatrix}0\\\\1\\end{pmatrix}$, $x_2=\\begin{pmatrix}1\\\\0\\end{pmatrix}$, $x_3=\\begin{pmatrix}1\\\\1\\end{pmatrix}$, $x_4=\\begin{pmatrix}2\\\\-1\\end{pmatrix}$.\n$$\n\\bar{x}_{R(t_1)} = \\frac{1}{4} \\left( \\begin{pmatrix}0\\\\1\\end{pmatrix} + \\begin{pmatrix}1\\\\0\\end{pmatrix} + \\begin{pmatrix}1\\\\1\\end{pmatrix} + \\begin{pmatrix}2\\\\-1\\end{pmatrix} \\right) = \\frac{1}{4}\\begin{pmatrix}4\\\\1\\end{pmatrix} = \\begin{pmatrix}1\\\\1/4\\end{pmatrix}.\n$$\n$$\n\\sum_{j \\in R(t_1)} x_j x_j^\\top = \\begin{pmatrix}00\\\\01\\end{pmatrix} + \\begin{pmatrix}10\\\\00\\end{pmatrix} + \\begin{pmatrix}11\\\\11\\end{pmatrix} + \\begin{pmatrix}4-2\\\\-21\\end{pmatrix} = \\begin{pmatrix}6-1\\\\-13\\end{pmatrix}.\n$$\n$$\nV(0, t_1) = \\frac{1}{4} \\begin{pmatrix}6-1\\\\-13\\end{pmatrix} - \\begin{pmatrix}1\\\\1/4\\end{pmatrix}\\begin{pmatrix}11/4\\end{pmatrix} = \\begin{pmatrix}3/2  -1/4 \\\\ -1/4  3/4\\end{pmatrix} - \\begin{pmatrix}1  1/4 \\\\ 1/4  1/16\\end{pmatrix} = \\begin{pmatrix}1/2  -1/2 \\\\ -1/2  11/16\\end{pmatrix}.\n$$\n\n- **Contribution from $t_2=3$**: $R(t_2)=\\{2, 3, 4\\}$, so $|R(t_2)|=3$.\n$$\n\\bar{x}_{R(t_2)} = \\frac{1}{3} \\left( \\begin{pmatrix}1\\\\0\\end{pmatrix} + \\begin{pmatrix}1\\\\1\\end{pmatrix} + \\begin{pmatrix}2\\\\-1\\end{pmatrix} \\right) = \\frac{1}{3}\\begin{pmatrix}4\\\\0\\end{pmatrix} = \\begin{pmatrix}4/3\\\\0\\end{pmatrix}.\n$$\n$$\n\\sum_{j \\in R(t_2)} x_j x_j^\\top = \\begin{pmatrix}10\\\\00\\end{pmatrix} + \\begin{pmatrix}11\\\\11\\end{pmatrix} + \\begin{pmatrix}4-2\\\\-21\\end{pmatrix} = \\begin{pmatrix}6-1\\\\-12\\end{pmatrix}.\n$$\n$$\nV(0, t_2) = \\frac{1}{3} \\begin{pmatrix}6-1\\\\-12\\end{pmatrix} - \\begin{pmatrix}4/3\\\\0\\end{pmatrix}\\begin{pmatrix}4/30\\end{pmatrix} = \\begin{pmatrix}2  -1/3 \\\\ -1/3  2/3\\end{pmatrix} - \\begin{pmatrix}16/9  0 \\\\ 0  0\\end{pmatrix} = \\begin{pmatrix}2/9  -1/3 \\\\ -1/3  2/3\\end{pmatrix}.\n$$\n\n- **Contribution from $t_4=5$**: $R(t_4)=\\{4\\}$, so $|R(t_4)|=1$.\n$\\bar{x}_{R(t_4)} = x_4$. The covariance of a single data point is zero. So, $V(0, t_4) = \\begin{pmatrix}00\\\\00\\end{pmatrix}$.\n\nNow, we sum the contributions to get $J(0)$:\n$$\nJ(0) = \\begin{pmatrix}1/2  -1/2 \\\\ -1/2  11/16\\end{pmatrix} + \\begin{pmatrix}2/9  -1/3 \\\\ -1/3  2/3\\end{pmatrix} = \\begin{pmatrix} 1/2+2/9  -1/2-1/3 \\\\ -1/2-1/3  11/16+2/3 \\end{pmatrix}\n$$\n$$\nJ(0) = \\begin{pmatrix} 9/18+4/18  -3/6-2/6 \\\\ -3/6-2/6  33/48+32/48 \\end{pmatrix} = \\begin{pmatrix} 13/18  -5/6 \\\\ -5/6  65/48 \\end{pmatrix}\n$$\nFinally, we compute the determinant of $J(0)$:\n$$\n\\det(J(0)) = \\left(\\frac{13}{18}\\right)\\left(\\frac{65}{48}\\right) - \\left(-\\frac{5}{6}\\right)^2 = \\frac{845}{864} - \\frac{25}{36}\n$$\nThe common denominator is $864 = 36 \\times 24$.\n$$\n\\det(J(0)) = \\frac{845}{864} - \\frac{25 \\times 24}{36 \\times 24} = \\frac{845 - 600}{864} = \\frac{245}{864}\n$$\nAs a decimal, this is $245 \\div 864 \\approx 0.2835648148...$. Rounding to four significant figures gives $0.2836$.",
            "answer": "$$\\boxed{0.2836}$$"
        }
    ]
}