{
    "hands_on_practices": [
        {
            "introduction": "This first exercise takes you back to the foundational principles of survival analysis. By starting with the simple yet powerful assumption of a constant hazard rate, $h(t) = \\lambda$, you will derive the exponential survival and density functions from scratch . This practice reinforces the fundamental differential equation linking the hazard and survival functions and provides a concrete understanding of the famous \"lack-of-memory\" property.",
            "id": "4991520",
            "problem": "A clinical trial monitors time to onset of a specific adverse event under a regimen whose instantaneous risk is believed to be constant over calendar time for a homogeneous subgroup. Let $T \\geq 0$ denote the continuous time-to-event random variable for a patient from this subgroup. Assume absolute continuity of the distribution of $T$ with respect to Lebesgue measure and that the hazard function is constant in time and equal to a fixed rate $\\lambda  0$ for all $t \\geq 0$.\n\nWork from first principles using only the core definitions below:\n- The survival function is $S(t) = \\mathbb{P}(T  t)$ for $t \\geq 0$.\n- The probability density function is $f(t)$, with $f(t) = \\frac{d}{dt}\\mathbb{P}(T \\leq t)$ whenever the derivative exists.\n- The hazard function is the limit $h(t) = \\lim_{\\Delta t \\to 0^{+}} \\frac{\\mathbb{P}(t \\leq T  t + \\Delta t \\mid T \\geq t)}{\\Delta t}$ for $t \\geq 0$.\n\nDerive, step by step from these definitions and standard rules of calculus and conditional probability, the survival function $S(t)$ and the probability density function $f(t)$ implied by the constant hazard assumption $h(t) = \\lambda$. Then use your derived $S(t)$ to obtain the closed-form expression for the conditional survival probability $\\mathbb{P}(T  s + t \\mid T  s)$ for $s \\geq 0$ and $t \\geq 0$, and interpret this expression in terms of the lack-of-memory property.\n\nReport your final result as a single row matrix with three entries, in the following order: the closed-form expression for $S(t)$, then for $f(t)$, and then for $\\mathbb{P}(T  s + t \\mid T  s)$, each simplified as a function of $\\lambda$, $s$, and $t$ where appropriate. No numerical approximation is required; provide exact expressions. Do not include any units. If you find multiple equivalent forms, present the simplest exponential form.",
            "solution": "The task is to derive the survival function $S(t)$, the probability density function $f(t)$, and the conditional survival probability $\\mathbb{P}(T  s + t \\mid T  s)$ for a continuous time-to-event random variable $T \\geq 0$ under the assumption of a constant hazard rate $h(t) = \\lambda  0$.\n\nLet us begin by relating the hazard function $h(t)$ to the survival function $S(t)$ and the probability density function $f(t)$ using the provided definitions.\n\nThe hazard function is defined as:\n$$h(t) = \\lim_{\\Delta t \\to 0^{+}} \\frac{\\mathbb{P}(t \\leq T  t + \\Delta t \\mid T \\geq t)}{\\Delta t}$$\nUsing the definition of conditional probability, $\\mathbb{P}(A \\mid B) = \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(B)}$, we can rewrite the term in the limit:\n$$\\mathbb{P}(t \\leq T  t + \\Delta t \\mid T \\geq t) = \\frac{\\mathbb{P}((t \\leq T  t + \\Delta t) \\cap (T \\geq t))}{\\mathbb{P}(T \\geq t)}$$\nThe event $\\{t \\leq T  t + \\Delta t\\}$ is a subset of the event $\\{T \\geq t\\}$. Therefore, their intersection is simply $\\{t \\leq T  t + \\Delta t\\}$. The expression becomes:\n$$\\mathbb{P}(t \\leq T  t + \\Delta t \\mid T \\geq t) = \\frac{\\mathbb{P}(t \\leq T  t + \\Delta t)}{\\mathbb{P}(T \\geq t)}$$\nThe denominator is the survival function, $S(t) = \\mathbb{P}(T  t)$. Since $T$ is a continuous random variable, $\\mathbb{P}(T=t) = 0$, so $\\mathbb{P}(T \\geq t) = \\mathbb{P}(T  t) = S(t)$. The numerator can be expressed in terms of the cumulative distribution function (CDF), $F(t) = \\mathbb{P}(T \\leq t)$, as $\\mathbb{P}(t \\leq T  t + \\Delta t) = F(t + \\Delta t) - F(t)$.\n\nSubstituting these into the definition of $h(t)$:\n$$h(t) = \\lim_{\\Delta t \\to 0^{+}} \\frac{F(t + \\Delta t) - F(t)}{\\Delta t} \\cdot \\frac{1}{S(t)}$$\nThe limit term is the definition of the derivative of the CDF, which is the probability density function (PDF), $f(t) = \\frac{d}{dt}F(t)$. Thus, we establish the fundamental relationship:\n$$h(t) = \\frac{f(t)}{S(t)}$$\nNext, we find a relationship between $S(t)$ and $f(t)$. The survival function is $S(t) = \\mathbb{P}(T  t) = 1 - \\mathbb{P}(T \\leq t) = 1 - F(t)$. Differentiating $S(t)$ with respect to $t$ gives:\n$$\\frac{d}{dt}S(t) = \\frac{d}{dt}(1 - F(t)) = - \\frac{d}{dt}F(t) = -f(t)$$\nNow, we can substitute $f(t) = -S'(t)$ into the expression for $h(t)$:\n$$h(t) = \\frac{-S'(t)}{S(t)} = -\\frac{d}{dt} \\ln(S(t))$$\nThis is a first-order ordinary differential equation for $S(t)$. The problem states that the hazard rate is a constant, $h(t) = \\lambda$, for all $t \\geq 0$.\n$$\\lambda = -\\frac{d}{dt} \\ln(S(t))$$\nWe can solve this differential equation by integrating both sides with respect to time from $0$ to $t$:\n$$\\int_0^t \\lambda \\, du = -\\int_0^t \\frac{d}{du} \\ln(S(u)) \\, du$$\n$$\\lambda [u]_0^t = -[\\ln(S(u))]_0^t$$\n$$\\lambda t = -(\\ln(S(t)) - \\ln(S(0)))$$\nThe initial condition $S(0)$ is the probability of survival beyond time $0$, which is $S(0) = \\mathbb{P}(T  0)$. Since $T$ is a non-negative continuous random variable representing time, $\\mathbb{P}(T \\geq 0) = 1$, and $\\mathbb{P}(T=0)=0$. Thus, $S(0) = \\mathbb{P}(T  0) = 1$. The natural logarithm of $S(0)$ is $\\ln(1) = 0$.\nThe equation simplifies to:\n$$\\lambda t = -\\ln(S(t))$$\nSolving for $S(t)$, we obtain the survival function:\n$$S(t) = \\exp(-\\lambda t)$$\nThis is the first required expression.\n\nNow, we derive the probability density function $f(t)$ using the relationship $f(t) = -S'(t)$:\n$$f(t) = -\\frac{d}{dt} S(t) = -\\frac{d}{dt} \\exp(-\\lambda t)$$\nApplying the chain rule for differentiation:\n$$f(t) = -(-\\lambda \\exp(-\\lambda t)) = \\lambda \\exp(-\\lambda t)$$\nThis is the second required expression, which is the PDF of the exponential distribution.\n\nFinally, we derive the conditional survival probability $\\mathbb{P}(T  s + t \\mid T  s)$ for $s \\geq 0$ and $t \\geq 0$. Using the definition of conditional probability:\n$$\\mathbb{P}(T  s + t \\mid T  s) = \\frac{\\mathbb{P}((T  s + t) \\cap (T  s))}{\\mathbb{P}(T  s)}$$\nSince $s \\geq 0$ and $t \\geq 0$, the condition $T  s+t$ implies $T  s$. Therefore, the intersection of the two events is just $\\{T  s+t\\}$.\n$$\\mathbb{P}(T  s + t \\mid T  s) = \\frac{\\mathbb{P}(T  s + t)}{\\mathbb{P}(T  s)}$$\nUsing the definition of the survival function $S(x) = \\mathbb{P}(T  x)$, we can write this as:\n$$\\mathbb{P}(T  s + t \\mid T  s) = \\frac{S(s+t)}{S(s)}$$\nSubstituting the derived expression for $S(t)$:\n$$\\mathbb{P}(T  s + t \\mid T  s) = \\frac{\\exp(-\\lambda(s+t))}{\\exp(-\\lambda s)} = \\frac{\\exp(-\\lambda s)\\exp(-\\lambda t)}{\\exp(-\\lambda s)} = \\exp(-\\lambda t)$$\nThis is the third required expression.\n\nThe interpretation of this final result is that $\\mathbb{P}(T  s + t \\mid T  s) = \\exp(-\\lambda t)$, which is equal to $S(t) = \\mathbb{P}(T  t)$. This identity, $\\mathbb{P}(T  s + t \\mid T  s) = \\mathbb{P}(T  t)$, is known as the lack-of-memory property. It signifies that for an object whose lifetime follows an exponential distribution, the probability of surviving for an additional duration $t$ is independent of how long it has already survived ($s$). The system \"forgets\" its past. In the context of the clinical trial, a patient who has remained event-free for time $s$ has the same future survival prognosis as a new patient starting the trial. This is a direct consequence of the assumption of a constant hazard rate $\\lambda$.\n\nThe three derived expressions are:\n1. $S(t) = \\exp(-\\lambda t)$\n2. $f(t) = \\lambda \\exp(-\\lambda t)$\n3. $\\mathbb{P}(T  s + t \\mid T  s) = \\exp(-\\lambda t)$\nThese are to be reported in a single row matrix.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\exp(-\\lambda t)  \\lambda \\exp(-\\lambda t)  \\exp(-\\lambda t)\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Moving from parametric to semi-parametric models, this practice explores the engine of the celebrated Cox Proportional Hazards model. You will work through a concrete example to understand how the partial likelihood allows for the estimation of hazard ratios without making any assumptions about the baseline hazard function, $h_0(t)$ . This exercise will then guide you through recovering an estimate of the baseline survival using the Breslow estimator, completing the picture of how the Cox model is fitted and used in practice.",
            "id": "3187045",
            "problem": "Consider the Cox Proportional Hazards (PH) model, which posits that for a subject with covariate vector $X$, the hazard function satisfies $h(t \\mid X) = h_0(t)\\,\\exp(X^{\\top}\\beta)$, where $h_0(t)$ is the baseline hazard and $\\beta$ is a finite-dimensional parameter vector. Let there be a single scalar covariate $X \\in \\{0,1\\}$ recorded at baseline. At a single observed time $t^{\\star}$, exactly $d=2$ failures occur simultaneously. Just before $t^{\\star}$, the risk set contains $n_0=3$ individuals with $X=0$ and $n_1=2$ individuals with $X=1$. Among the $d=2$ failures at $t^{\\star}$, one has $X=0$ and one has $X=1$. There are no prior failures and no censoring before $t^{\\star}$.\n\nStarting from the core definitions of the hazard function $h(t \\mid X)$, the survival function $S(t \\mid X)$, and the likelihood principle for right-censored time-to-event data, derive the partial likelihood for $\\beta$ at the tied failure time $t^{\\star}$ using Breslow’s treatment of ties, and explain why $h_0(t)$ is not involved in the estimation of $\\beta$. Then, specialize the partial likelihood to the data described and obtain the maximum likelihood estimator $\\hat{\\beta}$. Next, use Breslow’s estimator to recover the baseline cumulative hazard at $t^{\\star}$, denoted $\\hat{\\Lambda}_0(t^{\\star})$, and the baseline survival $\\hat{S}_0(t^{\\star})$. Finally, compute the estimated survival at $t^{\\star}$ for a subject with $X=1$, namely $\\hat{S}(t^{\\star} \\mid X=1)$, and provide its numeric value. Round your final numeric answer to four significant figures. The answer is unitless and must be expressed as a decimal.",
            "solution": "In the Cox Proportional Hazards model, the hazard for an individual with a covariate vector $X$ at time $t$ is given by\n$$h(t \\mid X) = h_0(t)\\exp(X^{\\top}\\beta)$$\nwhere $h_0(t)$ is the baseline hazard function, which is independent of the covariates, and $\\beta$ is a vector of regression coefficients.\n\nThe survival function $S(t \\mid X)$ is the probability of surviving beyond time $t$:\n$$S(t \\mid X) = \\exp\\left(-\\int_0^t h(u \\mid X) du\\right) = \\exp\\left(-\\int_0^t h_0(u)\\exp(X^{\\top}\\beta) du\\right)$$\n$$S(t \\mid X) = \\exp\\left(-\\exp(X^{\\top}\\beta)\\int_0^t h_0(u) du\\right) = \\left[\\exp\\left(-\\Lambda_0(t)\\right)\\right]^{\\exp(X^{\\top}\\beta)} = S_0(t)^{\\exp(X^{\\top}\\beta)}$$\nwhere $\\Lambda_0(t) = \\int_0^t h_0(u) du$ is the cumulative baseline hazard and $S_0(t)$ is the baseline survival function.\n\nThe estimation of $\\beta$ in the Cox model is typically performed by maximizing the partial likelihood. At a given failure time $t_{(j)}$, let $D_j$ be the set of individuals who fail, and $R(t_{(j)})$ be the risk set. The partial likelihood is constructed from the product of conditional probabilities. For a single failure ($|D_j|=1$, say individual $i$), the conditional probability that individual $i$ is the one who fails, given that one failure occurs at $t_{(j)}$ from the risk set $R(t_{(j)})$, is:\n$$ \\frac{h(t_{(j)} \\mid X_i)}{\\sum_{l \\in R(t_{(j)})} h(t_{(j)} \\mid X_l)} = \\frac{h_0(t_{(j)})\\exp(X_i^{\\top}\\beta)}{\\sum_{l \\in R(t_{(j)})} h_0(t_{(j)})\\exp(X_l^{\\top}\\beta)} = \\frac{\\exp(X_i^{\\top}\\beta)}{\\sum_{l \\in R(t_{(j)})} \\exp(X_l^{\\top}\\beta)} $$\nAs shown, the baseline hazard term $h_0(t_{(j)})$ cancels out from the numerator and denominator. Since the partial likelihood is the product of these terms over all failure times, it allows for the estimation of $\\beta$ without specifying or estimating the baseline hazard function.\n\nWhen ties are present in failure times, Breslow's treatment of ties approximates the contribution to the partial likelihood at time $t_{(j)}$ where $d_j$ failures occur by:\n$$L_j(\\beta) = \\frac{\\prod_{i \\in D_j} \\exp(X_i^{\\top}\\beta)}{\\left(\\sum_{l \\in R(t_{(j)})} \\exp(X_l^{\\top}\\beta)\\right)^{d_j}}$$\n\nFor this problem, there is a single failure time $t^{\\star}$ where $d=2$ failures occur. The risk set $R(t^{\\star})$ has $n_0=3$ individuals with $X=0$ and $n_1=2$ with $X=1$. The failing individuals $D(t^{\\star})$ consist of one with $X=0$ and one with $X=1$.\n\nThe partial likelihood for the single scalar $\\beta$ is:\n$$L(\\beta) = \\frac{\\exp(0 \\cdot \\beta) \\cdot \\exp(1 \\cdot \\beta)}{\\left( 3 \\cdot \\exp(0 \\cdot \\beta) + 2 \\cdot \\exp(1 \\cdot \\beta) \\right)^2} = \\frac{\\exp(\\beta)}{(3 + 2\\exp(\\beta))^2}$$\nTo find the maximum likelihood estimator (MLE) $\\hat{\\beta}$, we maximize the log-partial likelihood, $\\ell(\\beta) = \\ln(L(\\beta))$:\n$$\\ell(\\beta) = \\beta - 2\\ln(3 + 2\\exp(\\beta))$$\nWe take the derivative with respect to $\\beta$ and set it to zero:\n$$\\frac{d\\ell}{d\\beta} = 1 - 2 \\cdot \\frac{2\\exp(\\beta)}{3 + 2\\exp(\\beta)} = 1 - \\frac{4\\exp(\\beta)}{3 + 2\\exp(\\beta)} = 0$$\n$$3 + 2\\exp(\\beta) = 4\\exp(\\beta) \\implies 3 = 2\\exp(\\beta)$$\nThis gives us $\\exp(\\hat{\\beta}) = \\frac{3}{2}$, and the MLE is $\\hat{\\beta} = \\ln\\left(\\frac{3}{2}\\right)$.\n\nNext, we use Breslow's estimator for the cumulative baseline hazard at $t^{\\star}$:\n$$\\hat{\\Lambda}_0(t^{\\star}) = \\frac{d}{\\sum_{l \\in R(t^{\\star})} \\exp(X_l \\hat{\\beta})} = \\frac{2}{3 \\exp(0) + 2 \\exp(\\hat{\\beta})}$$\nSubstituting our result for $\\exp(\\hat{\\beta})$:\n$$\\hat{\\Lambda}_0(t^{\\star}) = \\frac{2}{3 + 2\\left(\\frac{3}{2}\\right)} = \\frac{2}{3+3} = \\frac{2}{6} = \\frac{1}{3}$$\nThe estimated baseline survival at $t^{\\star}$ is:\n$$\\hat{S}_0(t^{\\star}) = \\exp(-\\hat{\\Lambda}_0(t^{\\star})) = \\exp\\left(-\\frac{1}{3}\\right)$$\nFinally, we compute the estimated survival at $t^{\\star}$ for a subject with $X=1$:\n$$\\hat{S}(t^{\\star} \\mid X=1) = [\\hat{S}_0(t^{\\star})]^{\\exp(1 \\cdot \\hat{\\beta})} = \\left(\\exp\\left(-\\frac{1}{3}\\right)\\right)^{\\exp(\\hat{\\beta})}$$\nSubstituting $\\exp(\\hat{\\beta}) = 3/2$:\n$$\\hat{S}(t^{\\star} \\mid X=1) = \\left(\\exp\\left(-\\frac{1}{3}\\right)\\right)^{3/2} = \\exp\\left(-\\frac{1}{3} \\cdot \\frac{3}{2}\\right) = \\exp\\left(-\\frac{1}{2}\\right)$$\nThe numeric value is $\\exp(-0.5) \\approx 0.6065306597...$, which rounds to $0.6065$.",
            "answer": "$$\\boxed{0.6065}$$"
        },
        {
            "introduction": "Our final practice addresses a critical and common complexity in clinical data: competing risks. When subjects can experience one of several mutually exclusive event types, naively analyzing the \"risk\" for one cause by censoring the others leads to biased and uninterpretable results. This exercise will guide you through the correct framework by deriving the cause-specific cumulative incidence function (CIF), which provides an unbiased estimate of the event probability in the presence of competing events .",
            "id": "3186957",
            "problem": "A technology firm follows a cohort of identical devices from deployment until failure. Devices can fail from two mutually exclusive causes: mechanical wear (cause $1$) or power-supply fault (cause $2$). Let $T$ denote the time to failure (in months), and let $X \\in \\{0,1\\}$ indicate whether the device is operated under high-stress conditions ($X=1$) or not ($X=0$). For a device with covariate value $X=x$, the cause-specific hazard functions are assumed constant in time and given by $h_1(t \\mid X=x) = \\alpha_1 \\exp(\\beta_1 x)$ and $h_2(t \\mid X=x) = \\alpha_2 \\exp(\\beta_2 x)$, where $\\alpha_1,\\alpha_2  0$ and $\\beta_1,\\beta_2 \\in \\mathbb{R}$ are parameters.\n\nStarting only from foundational definitions of survival analysis (the survival function and the hazard function) and standard probability laws, derive an explicit closed-form expression for the cumulative incidence function for cause $1$ up to time $t$, denoted $F_1(t \\mid X=x)$. Your derivation should make clear which assumptions justify each step, and should not rely on any prepackaged formulas beyond those definitions.\n\nThen, using your derived expression, evaluate $F_1(t \\mid X=1)$ at $t = $ $24$ months for the parameter values $\\alpha_1 = 0.04$ month$^{-1}$, $\\alpha_2 = 0.02$ month$^{-1}$, $\\beta_1 = 0.8$, and $\\beta_2 = -0.4$. Express your final numerical answer as a decimal proportion (not a percentage) and round to four significant figures.\n\nFinally, briefly explain two distinct interpretational pitfalls that can arise when reporting the “risk of cause $1$” using cause-specific hazards in the presence of competing risks.\n\nOnly the numerical value for $F_1(24 \\mid X=1)$ will be graded as the final answer; however, full credit requires the derivation and interpretational discussion to be correct and complete.",
            "solution": "### Derivation of the Cumulative Incidence Function\n\nLet $T$ be the time to failure and $C \\in \\{1, 2\\}$ be the cause of failure. The analysis is conditional on a fixed covariate value $X=x$.\n\n1.  **Fundamental Definitions**:\n    -   The cause-specific hazard function for cause $j$, $h_j(t \\mid x)$, is the instantaneous rate of failure due to cause $j$ at time $t$, given survival up to time $t$.\n    -   The overall hazard function, $h_{\\text{all}}(t \\mid x)$, is the instantaneous rate of failure from any cause. Since the causes are mutually exclusive, the overall hazard is the sum of the cause-specific hazards:\n        $$h_{\\text{all}}(t \\mid x) = h_1(t \\mid x) + h_2(t \\mid x)$$\n    -   The overall survival function, $S(t \\mid x)$, is the probability of not failing from any cause by time $t$, $P(T  t \\mid X=x)$. It is related to the overall hazard function by:\n        $$S(t \\mid x) = \\exp\\left( -\\int_0^t h_{\\text{all}}(u \\mid x) \\,du \\right)$$\n    -   The cumulative incidence function (CIF) for cause $j$, $F_j(t \\mid x)$, is the probability of failing from cause $j$ by time $t$: $F_j(t \\mid x) = P(T \\le t, C=j \\mid X=x)$.\n\n2.  **Derivation of the CIF from Definitions**:\n    The CIF is the integral of the sub-distribution density $f_j(u \\mid x) = h_j(u \\mid x) S(u \\mid x)$ from $0$ to $t$:\n    $$F_j(t \\mid x) = \\int_0^t h_j(u \\mid x) S(u \\mid x) \\,du$$\n\n3.  **Applying the Specifics of the Problem**:\n    The given cause-specific hazards are constant with respect to time $t$:\n    $h_1(t \\mid x) = \\alpha_1 \\exp(\\beta_1 x)$\n    $h_2(t \\mid x) = \\alpha_2 \\exp(\\beta_2 x)$\n    \n    The overall hazard is therefore also constant in time:\n    $$h_{\\text{all}}(t \\mid x) = \\alpha_1 \\exp(\\beta_1 x) + \\alpha_2 \\exp(\\beta_2 x)$$\n    For notational simplicity, let's denote the time-constant hazards as $h_1(x)$, $h_2(x)$, and $h_{\\text{all}}(x)$. The integral in the survival function's exponent simplifies to $\\int_0^t h_{\\text{all}}(u \\mid x) \\,du = t \\cdot h_{\\text{all}}(x)$.\n    So, the overall survival function is:\n    $$S(t \\mid x) = \\exp(-t \\cdot h_{\\text{all}}(x)) = \\exp\\left( -t \\left[ \\alpha_1 \\exp(\\beta_1 x) + \\alpha_2 \\exp(\\beta_2 x) \\right] \\right) $$\n\n4.  **Finalizing the CIF Expression for Cause 1**:\n    We now substitute these forms into the integral definition of $F_1(t \\mid x)$:\n    $$F_1(t \\mid x) = \\int_0^t h_1(x) \\exp(-u \\cdot h_{\\text{all}}(x)) \\,du$$\n    Since $h_1(x)$ and $h_{\\text{all}}(x)$ are constant with respect to the integration variable $u$, we can solve the integral:\n    $$F_1(t \\mid x) = h_1(x) \\left[ \\frac{\\exp(-u \\cdot h_{\\text{all}}(x))}{-h_{\\text{all}}(x)} \\right]_0^t = \\frac{h_1(x)}{h_{\\text{all}}(x)} \\left( 1 - \\exp(-t \\cdot h_{\\text{all}}(x)) \\right)$$\n    Substituting the full parameter-dependent forms:\n    $$F_1(t \\mid x) = \\frac{\\alpha_1 \\exp(\\beta_1 x)}{\\alpha_1 \\exp(\\beta_1 x) + \\alpha_2 \\exp(\\beta_2 x)} \\left( 1 - \\exp\\left( -t \\left[ \\alpha_1 \\exp(\\beta_1 x) + \\alpha_2 \\exp(\\beta_2 x) \\right] \\right) \\right)$$\n    This expression represents the probability of eventually failing from cause $1$, given that a failure occurs, multiplied by the probability of failing from any cause by time $t$.\n\n### Numerical Evaluation\n\nWe need to evaluate $F_1(t \\mid x)$ for $t=24$, $x=1$, $\\alpha_1=0.04$, $\\alpha_2=0.02$, $\\beta_1=0.8$, and $\\beta_2=-0.4$.\n\nFirst, calculate the cause-specific hazards for $x=1$:\n$$h_1(x=1) = 0.04 \\exp(0.8) \\approx 0.08902164$$\n$$h_2(x=1) = 0.02 \\exp(-0.4) \\approx 0.01340640$$\nThe overall hazard for $x=1$ is:\n$$h_{\\text{all}}(x=1) = h_1(x=1) + h_2(x=1) \\approx 0.10242804$$\n\nNow, substitute these into the CIF formula with $t=24$:\n$$F_1(24 \\mid 1) = \\frac{h_1(x=1)}{h_{\\text{all}}(x=1)} \\left( 1 - \\exp\\left( -24 \\cdot h_{\\text{all}}(x=1) \\right) \\right)$$\n$$F_1(24 \\mid 1) \\approx \\frac{0.08902164}{0.10242804} \\left( 1 - \\exp\\left( -24 \\cdot 0.10242804 \\right) \\right)$$\n$$F_1(24 \\mid 1) \\approx 0.8691206 \\left( 1 - \\exp\\left( -2.45827296 \\right) \\right)$$\n$$F_1(24 \\mid 1) \\approx 0.8691206 \\left( 1 - 0.0855835 \\right) \\approx 0.794716$$\n\nRounding to four significant figures, the value is $0.7947$.\n\n### Interpretational Pitfalls of Cause-Specific Hazards\n\n1.  **Confounding Hazard Ratios with Risk Ratios**: A primary pitfall is to interpret the ratio of cause-specific hazards directly as a ratio of cumulative risks. A covariate that doubles the cause-specific hazard for cause $1$ does not necessarily double the probability of experiencing event $1$ by time $t$. The CIF, $F_1(t \\mid x) = \\int_0^t h_1(u \\mid x) S(u \\mid x) du$, depends on both $h_1(u \\mid x)$ and the overall survival $S(u \\mid x)$. If a covariate increases $h_1(u \\mid x)$, it also increases the overall hazard, which decreases the overall survival probability $S(u \\mid x)$. The integral for $F_1(t \\mid x)$ therefore involves a higher hazard but over a shorter effective time span, as fewer subjects remain at risk. The net effect on the cumulative probability is generally not proportional to the change in the hazard.\n\n2.  **The \"Independent Risks\" Fallacy and Misuse of Kaplan-Meier Estimation**: Another common pitfall is to analyze the risk of one cause by treating failures from competing causes as simple non-informative censoring. This approach implicitly analyzes a hypothetical world where the competing risks are eliminated. The resulting estimate, typically from a Kaplan-Meier estimator, $1 - S_{KM, j}(t)$, does not represent the actual probability of failure from cause $j$ in the real world. This quantity, sometimes called net survival, is always greater than or equal to the true CIF, $F_j(t)$. Summing these quantities across all causes, $\\sum_j (1 - S_{KM, j}(t))$, can yield a total probability greater than $1$. The correct measure for the probability of an event in the presence of competing risks is the cumulative incidence function.",
            "answer": "$$\n\\boxed{0.7947}\n$$"
        }
    ]
}