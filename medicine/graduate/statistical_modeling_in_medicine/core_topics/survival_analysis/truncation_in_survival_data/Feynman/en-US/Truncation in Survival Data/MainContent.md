## Introduction
In the analysis of [time-to-event data](@entry_id:165675), researchers often grapple with incomplete information, most commonly in the form of [censoring](@entry_id:164473). However, a more insidious challenge arises from **truncation**, where the very inclusion of a subject in a study depends on them not having the event of interest yet. This creates a critical knowledge gap: failure to account for this selection mechanism leads to [survivor bias](@entry_id:913033), a systematic error that can drastically distort findings and render conclusions invalid. This article provides a comprehensive guide to understanding and correctly handling truncation in survival data. The first chapter, **Principles and Mechanisms**, will lay the theoretical groundwork, explaining what truncation is, how it creates bias like [length-biased sampling](@entry_id:264779), and the statistical principle of conditioning used to correct for it. The second chapter, **Applications and Interdisciplinary Connections**, will explore the real-world consequences of truncation in fields from [pharmacoepidemiology](@entry_id:907872) to causal inference, highlighting common pitfalls like [immortal time bias](@entry_id:914926). Finally, **Hands-On Practices** will offer the chance to solidify these concepts through practical exercises. By mastering these principles, you will be equipped to turn fragmented, biased data into a coherent and truthful narrative of survival.

## Principles and Mechanisms

In our journey to understand the story of time—how long a patient survives, a machine operates, or a star shines—we are rarely granted a complete manuscript. Our data are often fragmented, with beginnings, middles, or ends obscured from view. The most familiar of these imperfections is **[right censoring](@entry_id:634946)**, where the story is cut short. We follow a patient for five years, at which point the study ends. They are still alive. We know their survival story is *at least* five years long, but the final chapter remains unwritten. This is like finding a large fragment of an ancient pot; we know it was part of a larger whole, but we don't have all the pieces .

But there is a far more subtle and treacherous form of incomplete data, one that can fundamentally warp our perception of reality if we are not careful. This is **truncation**. Truncation isn't just an incomplete story; it's a story we might never hear at all because of a selective filtering process. Imagine again you are an archaeologist. If the small, delicate pots made by a civilization have all crumbled to dust over the millennia, you will only ever find the large, sturdy ones. You might conclude, erroneously, that this civilization only made large pots. You are blind to an entire class of objects because they were filtered out by time. This filtering is the essence of truncation.

### The Admission Ticket: Censoring vs. Truncation

The fundamental difference between [censoring](@entry_id:164473) and truncation lies in how an individual becomes part of our dataset. Censoring deals with incomplete observation of subjects who are *already in our study*. Truncation is a condition of *entry* into the study itself. It’s the admission ticket.

#### Left Truncation: The Latecomers

The most common form in medical studies is **[left truncation](@entry_id:909727)**, also known as **delayed entry**. A subject is included in a study only if their event has not yet occurred by the time they are enrolled. Imagine a study on survival after a [diabetes diagnosis](@entry_id:916715) that recruits patients when they first visit a specialty clinic. A patient diagnosed with diabetes who suffers a fatal heart attack a month later—before they ever have a chance to visit the clinic—will never be enrolled. Their story is not just incomplete; it is entirely absent from our data. Their survival time, $T$, was shorter than their potential entry time, $L$, into the study. We only observe individuals for whom $T \ge L$  .

This is distinct from **left [censoring](@entry_id:164473)**, where we know an event occurred *before* our first observation, but we don't know exactly when. For example, a test at a patient's first visit reveals a tumor that must have appeared at some point since their last check-up a year ago. We have the information that $T \le L$, which is different from being completely unaware of the patient's existence . Even more complex is **interval [censoring](@entry_id:164473)**, where we only know the event happened between two observation points, say, between a negative test at 6 months and a positive one at 9 months, so $6 \lt T \le 9$ .

#### Right Truncation: Looking Backwards

Less common but equally important is **right truncation**. This typically occurs in retrospective studies where we sample based on the occurrence of an event. For instance, if we analyze a disease registry that was closed in the year 2000, we can only include individuals whose event (e.g., death) occurred *before* 2000. Anyone who had the event in 2001 or later is invisible to us. The condition for inclusion is $T \le R$, where $R$ is the truncation time .

In all cases of truncation, a whole segment of the population is missing from our sample by design. If we analyze the data as if it were a complete, random sample, our conclusions can be profoundly wrong.

### The Peril of the Prevalent: Survivor and Length Bias

What happens when we naively analyze a truncated sample? We fall prey to a pernicious form of [selection bias](@entry_id:172119) known as **[survivor bias](@entry_id:913033)**. Consider a [cross-sectional study](@entry_id:911635) that enrolls all *currently living* (prevalent) cases of a chronic disease at a single point in time . By definition, this study excludes everyone who was diagnosed but died before the survey date. The cohort is systematically enriched with individuals who have longer survival times. Those with aggressive, short-duration forms of the disease are disproportionately absent.

This phenomenon has a beautiful mathematical structure known as **[length-biased sampling](@entry_id:264779)**. In a population where a disease occurs at a steady rate, the probability of a person being a prevalent case is proportional to their total survival duration, $T$. A person who lives 10 years with a disease is ten times more likely to be captured in a "snapshot" survey than a person who lives only one year.

If the true distribution of survival times has a density $f(t)$ and a mean $\mu = \mathbb{E}[T]$, the distribution we observe in the [prevalent cohort](@entry_id:895281) is not $f(t)$. Instead, it is a new, "length-biased" distribution with density $g(t) = \frac{t f(t)}{\mu}$. What is the average survival time in this biased sample? It is the expected value calculated with this new density:
$$ \mathbb{E}_g[T] = \int_0^\infty t \cdot g(t) dt = \int_0^\infty t \cdot \frac{t f(t)}{\mu} dt = \frac{1}{\mu} \int_0^\infty t^2 f(t) dt = \frac{\mathbb{E}[T^2]}{\mathbb{E}[T]} $$
Since the variance, $\text{Var}(T) = \mathbb{E}[T^2] - (\mathbb{E}[T])^2$, is always non-negative, we know that $\mathbb{E}[T^2] \ge (\mathbb{E}[T])^2$. This means the average survival time we'd measure in the [prevalent cohort](@entry_id:895281), $\mathbb{E}_g[T]$, is *always greater than or equal to* the true mean survival time, $\mu$ .

The magnitude of this bias can be staggering. For a disease whose survival time follows an [exponential distribution](@entry_id:273894) with rate $\theta$ (meaning the true mean survival is $\mu = 1/\theta$), the biased mean becomes $\mathbb{E}_g[T] = 2/\theta$. The naive analysis of the [prevalent cohort](@entry_id:895281) would lead us to believe the average survival time is *twice as long* as it really is! .

### The Un-biasing: The Power of Conditioning

How can we correct this distorted view? The solution is one of the most elegant ideas in statistics: we use [conditional probability](@entry_id:151013) to adjust our perspective. We must explicitly account for the "admission ticket."

When we construct the **[likelihood function](@entry_id:141927)**—the mathematical expression of the probability of observing our data—we cannot use the unconditional probability of an event. We must use the probability of the event *given* that the subject was eligible to be in our study.

For a subject with left-truncation time $L_i$ who is observed to have an event at time $x_i$ (where $x_i \ge L_i$), their contribution to the likelihood is not simply the probability density $f(x_i)$. It is the [conditional probability density](@entry_id:265457):
$$ \text{Likelihood contribution} = \Pr(\text{event at } x_i \mid T_i \ge L_i) = \frac{f(x_i)}{\Pr(T_i \ge L_i)} = \frac{f(x_i)}{S(L_i)} $$
where $S(L_i)$ is the true [survival probability](@entry_id:137919) at the time of entry. That simple division by $S(L_i)$ is the mathematical key that unlocks an unbiased view of the world. It adjusts for the fact that this individual had to survive the gauntlet until time $L_i$ just to be seen by us  . This principle applies across all statistical models, from simple parametric ones to complex semi-parametric frameworks.

### The Evolving Risk Set: Truncation in Practice

This principle of conditioning has a direct and intuitive effect on our most common [survival analysis](@entry_id:264012) tools. The key is to correctly define who is "at risk" of an event at any given time $t$.

In a standard analysis without truncation, the **[risk set](@entry_id:917426)** at time $t$ consists of all individuals who entered at time zero and have not yet had an event or been censored. The [risk set](@entry_id:917426) only shrinks over time.

With [left truncation](@entry_id:909727), the [risk set](@entry_id:917426) becomes dynamic. A person is not at risk *in our study* until they have been admitted. Therefore, the [risk set](@entry_id:917426) at time $t$ must include only those individuals $i$ who have both (1) entered the study before time $t$, and (2) not yet had an event or been censored. This gives us the beautifully simple rule: individual $i$ is in the [risk set](@entry_id:917426) $R(t)$ if and only if $L_i  t \le X_i$, where $X_i$ is their observed event or [censoring](@entry_id:164473) time  .

The [risk set](@entry_id:917426) now grows as new subjects with $L_i  t$ enter, and it shrinks as subjects with $X_i = t$ experience an event or are censored. By correctly defining this evolving [risk set](@entry_id:917426), both the **Kaplan-Meier estimator** and the **[log-rank test](@entry_id:168043)** can be adapted to provide unbiased estimates and comparisons, even with [truncated data](@entry_id:163004) . The same logic extends seamlessly to the **Cox [proportional hazards model](@entry_id:171806)**, where the [partial likelihood](@entry_id:165240) is constructed using these same properly defined risk sets .

### The Broader Vista: Causal Inference and Model Checking

A deep understanding of truncation illuminates some of the most critical challenges in modern medical research, particularly in the realm of causal inference. A classic pitfall is **[immortal time bias](@entry_id:914926)**. This occurs in [observational studies](@entry_id:188981) comparing a new drug to no treatment. If we define the "treated" group as anyone who *eventually* starts the drug, we create a problem. The time from their eligibility until they actually start the drug is a period during which they must, by definition, be alive and event-free to receive the treatment. This period is "immortal." Classifying this [person-time](@entry_id:907645) as belonging to the "treated" group artificially inflates their apparent survival and makes the drug look better than it is.

This is a subtle form of [left truncation](@entry_id:909727) and [survivor bias](@entry_id:913033)! The solution, again, lies in careful handling of time. Advanced methods like **[target trial emulation](@entry_id:921058)** or **[marginal structural models](@entry_id:915309)** avoid this by anchoring every patient at a common time-zero (eligibility) and treating the drug exposure as a time-varying state. A patient contributes to the "untreated" [risk set](@entry_id:917426) until the moment they initiate the drug, at which point they switch to the "treated" [risk set](@entry_id:917426). This correctly attributes [person-time](@entry_id:907645) and eliminates [immortal time bias](@entry_id:914926), showing how the principles of handling delayed entry are essential for drawing valid causal conclusions .

Of course, our entire framework rests on a key assumption: **independent truncation**. This doesn't mean the entry time $L$ and survival time $T$ are completely unrelated. It means that, for those who have survived to a certain time $t$, knowing their specific entry time $\ell  t$ gives no *additional* information about their immediate risk of failure. This assumption is what allows us to believe the hazard rate we estimate from our truncated sample is the true population hazard rate .

Finally, the principle of conditioning on the entry time permeates the entire analytical process, right down to [model diagnostics](@entry_id:136895). When we calculate residuals to check if our Cox model fits the data well, the formulas must also be adjusted. For example, the widely used **Cox-Snell** and **[martingale](@entry_id:146036) residuals** are not calculated from time zero, but are based on the cumulative hazard accumulated *only during the time the subject was at risk*: from $L_i$ to $\tilde{T}_i$ . This demonstrates the beautiful unity of the concept: from the foundational definition of the likelihood, through the mechanics of estimation, to the final checks of our assumptions, correcting for the biased lens of truncation requires a consistent and principled application of [conditional probability](@entry_id:151013). It is by mastering this principle that we can turn fragmented stories into a coherent and truthful understanding of time.