{
    "hands_on_practices": [
        {
            "introduction": "Once an ordinal logistic regression model is fitted, a crucial next step is to use it for prediction. This practice guides you through the process of applying a pre-specified proportional odds model to a new patient profile . By calculating the linear predictor and using the model's cutpoints, you will transform abstract coefficients into tangible outcome probabilities, a fundamental skill for interpreting and communicating model results in a clinical setting.",
            "id": "4976153",
            "problem": "A hospital has adopted a four-level ordinal severity scale for acute infection, coded as $Y \\in \\{1,2,3,4\\}$ with $1=$ none, $2=$ mild, $3=$ moderate, and $4=$ severe. A proportional odds (cumulative logit) model has been fitted to a cohort study, using covariates measured at admission: age in years, sex (male indicator), serum C-reactive protein in $\\mathrm{mg/L}$, and Charlson comorbidity index. The fitted model uses cutpoints (also called thresholds) $\\alpha_1$, $\\alpha_2$, $\\alpha_3$ and a common slope vector $\\boldsymbol{\\beta}$, reported below, with the covariate scaling indicated for interpretability:\n\n- Cutpoints: $\\alpha_1 = -1.5$, $\\alpha_2 = 0.0$, $\\alpha_3 = 1.5$.\n- Slopes: age per $10$ years: $\\beta_{\\text{age}} = 0.25$; male indicator: $\\beta_{\\text{male}} = 0.40$; C-reactive protein per $50$ $\\mathrm{mg/L}$: $\\beta_{\\text{CRP}} = 0.60$; Charlson comorbidity per point: $\\beta_{\\text{CCI}} = 0.15$.\n\nConsider a patient with covariates: age $45$ years, female (male indicator $0$), C-reactive protein $20$ $\\mathrm{mg/L}$, Charlson comorbidity index $0$.\n\nUsing the definitions of the logit transform and the logistic function, compute the predicted category probabilities $P(Y=1 \\mid \\mathbf{x})$, $P(Y=2 \\mid \\mathbf{x})$, $P(Y=3 \\mid \\mathbf{x})$, and $P(Y=4 \\mid \\mathbf{x})$ under the proportional odds model for this patient, and interpret the distribution across severity levels in terms of clinical plausibility. Then, compute the expected severity index $\\mathbb{E}[Y \\mid \\mathbf{x}]$ for this patient.\n\nProvide only the expected severity index as your final answer, rounded to four significant figures. No units are required.",
            "solution": "The problem requires computing the predicted category probabilities and the expected severity index for a new patient using a given proportional odds model. The model is defined by:\n$$\n\\text{logit}\\left(P(Y \\le j \\mid \\mathbf{x})\\right) = \\ln\\left(\\frac{P(Y \\le j \\mid \\mathbf{x})}{P(Y > j \\mid \\mathbf{x})}\\right) = \\alpha_j - \\boldsymbol{\\beta}^T\\mathbf{x}\n$$\nwhere $\\boldsymbol{\\beta}^T\\mathbf{x}$ is the linear predictor.\n\n**1. Calculate the linear predictor ($\\eta = \\boldsymbol{\\beta}^T\\mathbf{x}$):**\nThe patient's covariates are: Age = $45$ years, Sex = female (indicator=$0$), CRP = $20$ $\\mathrm{mg/L}$, CCI = $0$. The scaled covariates are:\n- $x_{\\text{age}} = 45 / 10 = 4.5$\n- $x_{\\text{male}} = 0$\n- $x_{\\text{CRP}} = 20 / 50 = 0.4$\n- $x_{\\text{CCI}} = 0$\n\nThe linear predictor is:\n$$\n\\eta = (0.25)(4.5) + (0.40)(0) + (0.60)(0.4) + (0.15)(0) = 1.125 + 0.24 = 1.365\n$$\n\n**2. Compute the cumulative probabilities:**\nThe cumulative probabilities $P(Y \\le j \\mid \\mathbf{x})$ are calculated using the inverse logit (logistic) function, $\\sigma(z) = 1/(1+e^{-z})$.\n- $P(Y \\le 1 \\mid \\mathbf{x}) = \\sigma(\\alpha_1 - \\eta) = \\sigma(-1.5 - 1.365) = \\sigma(-2.865) \\approx 0.053914$\n- $P(Y \\le 2 \\mid \\mathbf{x}) = \\sigma(\\alpha_2 - \\eta) = \\sigma(0.0 - 1.365) = \\sigma(-1.365) \\approx 0.203430$\n- $P(Y \\le 3 \\mid \\mathbf{x}) = \\sigma(\\alpha_3 - \\eta) = \\sigma(1.5 - 1.365) = \\sigma(0.135) \\approx 0.533700$\n\n**3. Compute the individual category probabilities:**\n- $P(Y=1 \\mid \\mathbf{x}) = P(Y \\le 1 \\mid \\mathbf{x}) \\approx 0.053914$\n- $P(Y=2 \\mid \\mathbf{x}) = P(Y \\le 2 \\mid \\mathbf{x}) - P(Y \\le 1 \\mid \\mathbf{x}) \\approx 0.203430 - 0.053914 = 0.149516$\n- $P(Y=3 \\mid \\mathbf{x}) = P(Y \\le 3 \\mid \\mathbf{x}) - P(Y \\le 2 \\mid \\mathbf{x}) \\approx 0.533700 - 0.203430 = 0.330270$\n- $P(Y=4 \\mid \\mathbf{x}) = 1 - P(Y \\le 3 \\mid \\mathbf{x}) \\approx 1 - 0.533700 = 0.466300$\n\nThe model predicts a high probability of severe disease ($46.6\\%$) for this patient, which is plausible given the age and elevated CRP.\n\n**4. Compute the expected severity index $\\mathbb{E}[Y \\mid \\mathbf{x}]$:**\nThe expected value is the weighted sum of the category values:\n$$\n\\mathbb{E}[Y \\mid \\mathbf{x}] = \\sum_{j=1}^{4} j \\cdot P(Y=j \\mid \\mathbf{x})\n$$\n$$\n\\mathbb{E}[Y \\mid \\mathbf{x}] = 1(0.053914) + 2(0.149516) + 3(0.330270) + 4(0.466300)\n$$\n$$\n\\mathbb{E}[Y \\mid \\mathbf{x}] = 0.053914 + 0.299032 + 0.990810 + 1.865200 = 3.208956\n$$\nRounding to four significant figures, the expected severity index is $3.209$.",
            "answer": "$$\n\\boxed{3.209}\n$$"
        },
        {
            "introduction": "Beyond prediction, a core task in statistical modeling is to determine whether a covariate has a meaningful effect on the outcome. This exercise focuses on hypothesis testing within a multinomial logistic regression framework . You will implement a likelihood ratio test to assess the overall significance of a biomarker, comparing a full model against a restricted, intercepts-only model, and thereby practice a foundational technique for statistical inference in multi-category settings.",
            "id": "4976107",
            "problem": "Consider a medical cohort where the categorical outcome represents disease status with $K$ mutually exclusive categories and a continuous biomarker measured for each patient. Assume the following baseline-category multinomial logistic regression representation: for categories indexed by $k \\in \\{1,\\dots,K-1\\}$ relative to a chosen baseline category $0$, the linear predictor for category $k$ is $\\eta_k = \\alpha_k + \\beta_k x$, where $x$ is the biomarker value for a patient, and the category probabilities are defined by the logistic link. The dataset consists of independent observations $\\{(x_i, y_i)\\}_{i=1}^N$ with $y_i \\in \\{0,1,\\dots,K-1\\}$ and biomarker values $x_i \\in \\mathbb{R}$. The modeling target is to assess whether the biomarker has any effect on the odds of assignment to any non-baseline category, that is, whether all slope parameters are zero, under the null hypothesis $H_0: \\beta_k = 0$ for all $k \\in \\{1,\\dots,K-1\\}$, versus the alternative $H_1: \\exists k$ such that $\\beta_k \\ne 0$.\n\nStarting exclusively from the following bases:\n- The definition of independent categorical outcomes with probabilities $(p_{0i}, p_{1i}, \\dots, p_{(K-1)i})$ for each observation $i$ with $p_{0i} + \\sum_{k=1}^{K-1} p_{ki} = 1$.\n- The principle of maximum likelihood estimation, whereby parameter estimates maximize the joint likelihood (equivalently minimize the negative log-likelihood) implied by the specified model.\n- The baseline-category multinomial logistic link that maps linear predictors to probabilities through the logistic transformation, ensuring valid probabilities that sum to one across categories.\n- The general likelihood ratio test principle, which compares nested models using their maximized likelihoods and evaluates statistical significance via the appropriate large-sample distribution determined by the difference in dimensionality between the models.\n\nYour task is to implement a program that:\n1. Constructs synthetic datasets under scientifically plausible medical scenarios using specified parameter sets and random seeds.\n2. Fits two models by maximizing the appropriate likelihoods: the unrestricted model allowing $\\beta_k$ to vary freely across $k \\in \\{1,\\dots,K-1\\}$, and the restricted model under $H_0$ where $\\beta_k = 0$ for all $k \\in \\{1,\\dots,K-1\\}$ (intercepts-only).\n3. Performs the likelihood ratio test for $H_0$ against $H_1$ for each dataset, reporting the test statistic, the degrees of freedom defined as the difference in the number of free parameters between the unrestricted and restricted models, and the corresponding large-sample $p$-value computed using the appropriate reference distribution. The $p$-value must be reported as a decimal, not a percentage.\n\nData generation and test suite:\n- For each test case, generate biomarker values $x_i$ independently from a normal distribution specified by a mean and standard deviation, and then generate outcomes $y_i$ independently according to the multinomial logistic model with the given true parameters. Use the baseline-category parameterization with the baseline category indexed by $0$.\n- Use the following test suite, where each tuple specifies $(N, K, \\boldsymbol{\\alpha}, \\boldsymbol{\\beta}, \\mu_x, \\sigma_x, \\text{seed})$ with $\\boldsymbol{\\alpha} = (\\alpha_1,\\dots,\\alpha_{K-1})$ and $\\boldsymbol{\\beta} = (\\beta_1,\\dots,\\beta_{K-1})$:\n    1. $(800, 4, (-1.2, 0.5, -0.3), (0.8, -0.5, 0.4), 2.0, 1.0, 42)$, representing a four-category disease status with a biomarker showing heterogeneous effects across categories.\n    2. $(600, 3, (-0.2, -1.0), (0.0, 0.0), 1.0, 0.8, 123)$, representing a null-effect scenario where the biomarker has no impact on category odds.\n    3. $(50, 3, (-1.5, -1.0), (1.5, 1.2), 0.0, 1.2, 999)$, representing a small-sample boundary case with substantial biomarker effects.\n    4. $(1200, 5, (-2.0, -1.0, -1.5, -3.0), (0.7, 0.3, -0.4, 0.9), 1.5, 0.7, 2023)$, representing five-category outcomes including a rare category scenario with varied biomarker effects.\n\nAlgorithmic and numerical requirements:\n- Implement maximum likelihood estimation for both the unrestricted and restricted models using a numerically stable computation for the log-likelihood under the baseline-category multinomial logistic link. The optimizer should be based on a standard quasi-Newton method and must not rely on any closed-form solution.\n- Ensure reproducibility by using the specified seeds.\n- The program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets. For each test case, output a list of three values in the order $[T, \\text{df}, p]$, where $T$ is the likelihood ratio test statistic as a float, $\\text{df}$ is the degrees of freedom as an integer, and $p$ is the $p$-value as a float. The final output should be of the form $[[T_1,\\text{df}_1,p_1],[T_2,\\text{df}_2,p_2],[T_3,\\text{df}_3,p_3],[T_4,\\text{df}_4,p_4]]$.\n\nNo physical units or angle units are involved in this problem; all outputs are dimensionless quantities.",
            "solution": "The problem requires implementing a likelihood ratio test (LRT) to determine if a biomarker has a statistically significant effect in a multinomial logistic regression model. The core methodology, as implemented in the provided Python code, is as follows:\n\n1.  **Model Specification**: The analysis uses a baseline-category multinomial logistic model. For a $K$-category outcome, this involves defining $K-1$ linear predictors for the non-baseline categories relative to a reference category. The probability of each category is then calculated using the softmax function.\n\n2.  **Nested Models**: Two nested models are compared:\n    *   **Unrestricted Model ($M_1$)**: This is the full model where the biomarker's effect is estimated separately for each of the $K-1$ logit comparisons. It has $d_1 = (K-1)$ intercept parameters and $(K-1)$ slope parameters, for a total of $2(K-1)$ parameters.\n    *   **Restricted Model ($M_0$)**: This model corresponds to the null hypothesis ($H_0: \\beta_1 = \\dots = \\beta_{K-1} = 0$), which states the biomarker has no effect. All slope parameters are fixed to zero, so the model only contains $d_0 = K-1$ intercept parameters.\n\n3.  **Maximum Likelihood Estimation (MLE)**: For both models, the parameters are estimated by maximizing the multinomial log-likelihood function. This is achieved by numerically minimizing the negative log-likelihood using a quasi-Newton optimization algorithm (such as L-BFGS-B). For numerical stability, the `log-sum-exp` trick is employed when computing the softmax probabilities.\n\n4.  **Likelihood Ratio Test (LRT)**: The test statistic is calculated by comparing the maximized log-likelihoods of the two models:\n    $$ T = 2 \\left( \\ell(\\hat{\\theta}_{1}) - \\ell(\\hat{\\theta}_{0}) \\right) $$\n    where $\\ell(\\hat{\\theta}_1)$ and $\\ell(\\hat{\\theta}_0)$ are the maximized log-likelihoods for the unrestricted and restricted models, respectively.\n\n5.  **Inference**: Under the null hypothesis, the test statistic $T$ asymptotically follows a chi-squared ($\\chi^2$) distribution. The degrees of freedom (df) are the difference in the number of parameters between the models:\n    $$ \\text{df} = d_1 - d_0 = 2(K-1) - (K-1) = K-1 $$\n    The $p$-value is computed from the survival function of this $\\chi^2_{\\text{df}}$ distribution. A small $p$-value indicates that the biomarker's effect is statistically significant.",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.special import softmax, logsumexp\nfrom scipy.stats import chi2\n\ndef solve():\n    \"\"\"\n    Main function to run the likelihood ratio test for multinomial logistic regression\n    on several synthetic datasets.\n    \"\"\"\n    test_cases = [\n        # (N, K, alpha_true, beta_true, mu_x, sigma_x, seed)\n        (800, 4, [-1.2, 0.5, -0.3], [0.8, -0.5, 0.4], 2.0, 1.0, 42),\n        (600, 3, [-0.2, -1.0], [0.0, 0.0], 1.0, 0.8, 123),\n        (50, 3, [-1.5, -1.0], [1.5, 1.2], 0.0, 1.2, 999),\n        (1200, 5, [-2.0, -1.0, -1.5, -3.0], [0.7, 0.3, -0.4, 0.9], 1.5, 0.7, 2023),\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        N, K, alphas_true, betas_true, mu_x, sigma_x, seed = case\n        \n        # 1. Generate synthetic data\n        rng = np.random.default_rng(seed)\n        x_data = rng.normal(loc=mu_x, scale=sigma_x, size=N)\n        \n        alphas_true_np = np.array(alphas_true)\n        betas_true_np = np.array(betas_true)\n\n        # Calculate true linear predictors (eta) for categories 1 to K-1\n        eta = alphas_true_np[None, :] + np.outer(x_data, betas_true_np)\n        \n        # Add baseline category's eta (which is 0)\n        eta_full = np.hstack([np.zeros((N, 1)), eta])\n        \n        # Convert to probabilities using softmax\n        probs = softmax(eta_full, axis=1)\n        \n        # Generate categorical outcomes\n        y_data = np.array([rng.choice(K, p=p_i) for p_i in probs])\n        \n        # One-hot encode the outcome variable for likelihood calculation\n        y_one_hot = np.zeros((N, K))\n        y_one_hot[np.arange(N), y_data] = 1\n\n        # 2. Fit models using MLE\n        \n        def nll_m0(params, x, y_oh, k_val):\n            \"\"\"Negative log-likelihood for the restricted model (M0: intercepts only).\"\"\"\n            n_obs = x.shape[0]\n            alphas = params\n            # Eta does not depend on x\n            eta_k = np.tile(alphas, (n_obs, 1))\n            eta_full = np.hstack([np.zeros((n_obs, 1)), eta_k])\n            \n            log_denominators = logsumexp(eta_full, axis=1)\n            log_probs = eta_full - log_denominators[:, None]\n            \n            nll = -np.sum(y_oh * log_probs)\n            return nll\n\n        def nll_m1(params, x, y_oh, k_val):\n            \"\"\"Negative log-likelihood for the unrestricted model (M1).\"\"\"\n            n_obs = x.shape[0]\n            alphas = params[:k_val - 1]\n            betas = params[k_val - 1:]\n            \n            eta_k = alphas[None, :] + np.outer(x, betas)\n            eta_full = np.hstack([np.zeros((n_obs, 1)), eta_k])\n\n            log_denominators = logsumexp(eta_full, axis=1)\n            log_probs = eta_full - log_denominators[:, None]\n\n            nll = -np.sum(y_oh * log_probs)\n            return nll\n\n        # Fit restricted model (M0)\n        initial_params_0 = np.zeros(K - 1)\n        res0 = minimize(nll_m0, initial_params_0, args=(x_data, y_one_hot, K), method='L-BFGS-B')\n        logL0 = -res0.fun\n\n        # Fit unrestricted model (M1)\n        initial_params_1 = np.zeros(2 * (K - 1))\n        res1 = minimize(nll_m1, initial_params_1, args=(x_data, y_one_hot, K), method='L-BFGS-B')\n        logL1 = -res1.fun\n\n        # 3. Perform Likelihood Ratio Test\n        # Test statistic T = 2 * (logL(M1) - logL(M0))\n        T = 2 * (logL1 - logL0)\n        \n        # Degrees of freedom = difference in number of parameters\n        df = (2 * (K - 1)) - (K - 1)\n        \n        # p-value from chi-squared distribution\n        p_value = chi2.sf(T, df)\n        \n        all_results.append([T, df, p_value])\n\n    # Format the final output string\n    # E.g., [[T1,df1,p1],[T2,df2,p2],...]\n    output_str = f\"[{','.join(f'[{t:.6f},{d},{p:.6f}]' for t, d, p in all_results)}]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "The proportional odds assumption is the key feature that distinguishes ordinal from multinomial logistic regression, but it must be critically evaluated. This practice introduces a sensitivity analysis to assess the robustness of this assumption and its impact on parameter estimates . By systematically collapsing adjacent outcome categories and comparing the proportional odds model to a more flexible baseline-category multinomial model using the Akaike Information Criterion (AIC), you will gain experience in advanced model diagnostics and learn how to justify your choice of model.",
            "id": "4976140",
            "problem": "A medical outcomes researcher is modeling an ordered clinical endpoint, such as pain severity recorded on an ordinal scale, as a function of two covariates: a binary treatment indicator and a continuous age variable. The goal is to evaluate the robustness of the proportional odds assumption in the cumulative logit model by performing a sensitivity analysis that collapses adjacent outcome categories and then examines how parameter estimates and goodness-of-fit diagnostics change, both under the proportional odds model and under a more flexible baseline-category multinomial model.\n\nYou must implement a complete program that performs the following end-to-end procedure using only deterministic inputs and computations.\n\nFundamental base and model definitions:\n- Let the response be an ordinal variable with categories $\\{1,2,\\dots,K\\}$, with $K \\ge 2$. The cumulative logit (proportional odds) model specifies\n$$\\Pr(Y \\le k \\mid \\mathbf{x}) = \\sigma(\\alpha_k - \\mathbf{x}^{\\top}\\boldsymbol{\\beta}), \\quad k=1,\\dots,K-1,$$\nwhere $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ is the logistic function, $\\{\\alpha_k\\}$ are strictly increasing category thresholds, $\\alpha_1 < \\cdots < \\alpha_{K-1}$, and $\\boldsymbol{\\beta}$ is a vector of slopes that does not depend on $k$ (the proportional odds assumption).\n- The probability mass function for category $k$ is\n$$\\Pr(Y=k \\mid \\mathbf{x}) =\n\\begin{cases}\n\\sigma(\\alpha_1 - \\eta), & k=1, \\\\\n\\sigma(\\alpha_k - \\eta) - \\sigma(\\alpha_{k-1} - \\eta), & k=2,\\dots,K-1, \\\\\n1 - \\sigma(\\alpha_{K-1} - \\eta), & k=K,\n\\end{cases}$$\nwhere $\\eta = \\mathbf{x}^{\\top}\\boldsymbol{\\beta}$.\n- For baseline-category multinomial logistic regression with $K$ categories, designate category $K$ as the baseline. For $k=1,\\dots,K-1$,\n$$\\log \\frac{\\Pr(Y=k \\mid \\mathbf{x})}{\\Pr(Y=K \\mid \\mathbf{x})} = c_k + \\mathbf{x}^{\\top}\\boldsymbol{b}_k,$$\nwith category-specific intercepts $c_k$ and slopes $\\boldsymbol{b}_k$. Then $p_k(\\mathbf{x}) = \\Pr(Y=k \\mid \\mathbf{x})$ is given by the softmax:\n$$p_k(\\mathbf{x}) = \\frac{\\exp(c_k + \\mathbf{x}^{\\top}\\boldsymbol{b}_k)}{1 + \\sum_{j=1}^{K-1} \\exp(c_j + \\mathbf{x}^{\\top}\\boldsymbol{b}_j)}, \\quad k=1,\\dots,K-1,$$\nand\n$$p_K(\\mathbf{x}) = \\frac{1}{1 + \\sum_{j=1}^{K-1} \\exp(c_j + \\mathbf{x}^{\\top}\\boldsymbol{b}_j)}.$$\n\nLikelihood and diagnostics:\n- Given independent observations $\\{(y_i,\\mathbf{x}_i)\\}_{i=1}^n$, the log-likelihood under any model with category probabilities $\\{p_{i,k}\\}$ is\n$$\\ell = \\sum_{i=1}^n \\log p_{i,y_i}.$$\n- The deviance is defined (for these fully observed categorical outcomes) as\n$$D = -2 \\,\\ell.$$\n- The Akaike Information Criterion (AIC) is\n$$\\mathrm{AIC} = 2\\,d - 2\\,\\ell,$$\nwhere $d$ is the model's number of free parameters. The Bayesian Information Criterion (BIC) is\n$$\\mathrm{BIC} = \\log(n)\\, d - 2\\,\\ell.$$\n\nDeterministic synthetic data construction:\n- Let $n = 200$. Index observations by $i \\in \\{1,\\dots,n\\}$.\n- Define two covariates:\n  - Treatment $T_i \\in \\{0,1\\}$ deterministically as $T_i = 1$ if $i$ is even and $T_i = 0$ if $i$ is odd.\n  - Age $A_i$ deterministically as $A_i = 30 + 0.1\\, i$.\n- Let the standardized age be $Z_i = \\frac{A_i - \\bar{A}}{s_A}$, where $\\bar{A}$ and $s_A$ are the sample mean and sample standard deviation of $\\{A_i\\}_{i=1}^n$.\n- Let $K=4$, true thresholds $\\boldsymbol{\\alpha}^{\\star} = (-0.5, 0.8, 1.8)$, and true slopes $\\boldsymbol{\\beta}^{\\star} = (\\beta_T^{\\star}, \\beta_Z^{\\star}) = (-0.8, 0.5)$. For each $i$, compute the linear predictor $\\eta_i^{\\star} = \\beta_T^{\\star} T_i + \\beta_Z^{\\star} Z_i$.\n- Compute the cumulative probabilities under the proportional odds data-generating mechanism: $c_{i,k} = \\sigma(\\alpha_k^{\\star} - \\eta_i^{\\star})$ for $k=1,2,3$. Then the category probabilities are $p_{i,1} = c_{i,1}$, $p_{i,2} = c_{i,2} - c_{i,1}$, $p_{i,3} = c_{i,3} - c_{i,2}$, and $p_{i,4} = 1 - c_{i,3}$.\n- To assign a deterministic category $y_i \\in \\{1,2,3,4\\}$, compute $u_i = \\{ i \\cdot \\varphi \\}$, the fractional part of $i \\cdot \\varphi$, where $\\varphi = \\frac{\\sqrt{5}-1}{2}$. Then set $y_i$ to the smallest $k$ such that $\\sum_{j=1}^k p_{i,j} \\ge u_i$.\n\nModel fitting and parameterization:\n- For the proportional odds model with $K$ categories and $p$ covariates, use the log-likelihood above and estimate by maximum likelihood. Enforce the ordering constraint $\\alpha_1 < \\cdots < \\alpha_{K-1}$ by reparameterizing thresholds via unconstrained $\\gamma_1,\\dots,\\gamma_{K-1}$:\n$$\\alpha_1 = \\gamma_1, \\quad \\alpha_k = \\alpha_{k-1} + \\exp(\\gamma_k) \\ \\text{for } k=2,\\dots,K-1.$$\n- For the baseline-category multinomial model, treat category $K$ as baseline, with $(K-1)$ category-specific intercepts and slopes. Estimate by maximum likelihood using the softmax probability model and the log-likelihood above.\n- For numerical stability, probabilities must be bounded away from $0$ and $1$ when taking logarithms by clamping to a small positive lower bound.\n\nSensitivity analysis by collapsing adjacent categories:\n- Construct three collapsed outcomes by merging adjacent original categories:\n  - Case $\\mathcal{C}_{12}$: merge categories $1$ and $2$ into a single first category, leaving categories $(1+2), 3, 4$.\n  - Case $\\mathcal{C}_{23}$: merge categories $2$ and $3$, leaving categories $1, (2+3), 4$.\n  - Case $\\mathcal{C}_{34}$: merge categories $3$ and $4$, leaving categories $1, 2, (3+4)$.\n- For each collapsed outcome (with new $K'$), refit both models and compute the proportional odds estimate of the treatment slope $\\widehat{\\beta}_T$ and the goodness-of-fit metrics $\\ell$, $D$, $\\mathrm{AIC}$, and $\\mathrm{BIC}$.\n- Also fit the proportional odds model to the uncollapsed outcome (baseline with $K=4$) and denote its treatment slope by $\\widehat{\\beta}_T^{\\mathrm{base}}$ and its goodness-of-fit metrics.\n\nQuantities to report and test suite:\n- Define the three test cases to be the collapse schemes $\\mathcal{C}_{12}$, $\\mathcal{C}_{23}$, and $\\mathcal{C}_{34}$, in that order.\n- For each case $\\mathcal{C}_{ab}$, compute:\n  - The change in the estimated treatment slope under proportional odds relative to the baseline fit on the uncollapsed $K=4$ outcome:\n  $$\\Delta \\beta_T^{(ab)} = \\widehat{\\beta}_T^{(ab)} - \\widehat{\\beta}_T^{\\mathrm{base}}.$$\n  - The within-dataset difference in Akaike Information Criterion between the multinomial and proportional odds fits on the collapsed categories:\n  $$\\Delta \\mathrm{AIC}_{\\mathrm{MN}-\\mathrm{PO}}^{(ab)} = \\mathrm{AIC}_{\\mathrm{MN}}^{(ab)} - \\mathrm{AIC}_{\\mathrm{PO}}^{(ab)}.$$\n- Assemble the final results as a list of $6$ floats in the order\n$$\\big[\\Delta \\beta_T^{(12)}, \\ \\Delta \\mathrm{AIC}_{\\mathrm{MN}-\\mathrm{PO}}^{(12)}, \\ \\Delta \\beta_T^{(23)}, \\ \\Delta \\mathrm{AIC}_{\\mathrm{MN}-\\mathrm{PO}}^{(23)}, \\ \\Delta \\beta_T^{(34)}, \\ \\Delta \\mathrm{AIC}_{\\mathrm{MN}-\\mathrm{PO}}^{(34)}\\big].$$\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[x_1,x_2,\\dots,x_6]$.\n- Each float must be expressed as a decimal number rounded to exactly $6$ decimal places, with no percentage signs.\n\nConstraints:\n- Implement maximum likelihood estimation using a numerical optimizer starting from reasonable initial values. For thresholds in the proportional odds model, use the reparameterization above to guarantee order. For initial thresholds, you may use the empirical cumulative logits of the marginal outcome distribution; for slopes, you may initialize at zero.\n- The program must be self-contained and must not require any input; it must generate the deterministic dataset and perform all computations internally.\n\nTest suite and coverage rationale:\n- The three collapse schemes $\\mathcal{C}_{12}$, $\\mathcal{C}_{23}$, and $\\mathcal{C}_{34}$ collectively probe low-end merging, central merging (which may most strongly perturb intermediate thresholds), and high-end merging, respectively. This tests sensitivity of the proportional odds slope estimate and the relative goodness-of-fit between the multinomial and proportional odds models across different adjacent-category collapses.",
            "solution": "The problem requires performing a sensitivity analysis to evaluate the robustness of the proportional odds (PO) assumption. The analysis involves collapsing adjacent categories of an ordinal outcome and comparing the performance of a constrained PO model against a more flexible baseline-category multinomial (MN) model. The methodology implemented in the code is as follows:\n\n1.  **Deterministic Data Generation**: A synthetic dataset with $n=200$ observations is created based on a true PO model with $K=4$ categories. The covariates (treatment and age) and the ordinal outcomes are generated deterministically to ensure reproducibility.\n\n2.  **Baseline Model Fit**: A proportional odds (PO) model is first fitted to the original, uncollapsed 4-category outcome data. This provides a baseline estimate of the treatment effect slope, $\\widehat{\\beta}_T^{\\mathrm{base}}$.\n\n3.  **Category Collapse and Refitting**: The sensitivity analysis is performed across three scenarios, each involving the merging of two adjacent categories to create a new 3-category outcome:\n    *   Case $\\mathcal{C}_{12}$: Merging categories 1 and 2.\n    *   Case $\\mathcal{C}_{23}$: Merging categories 2 and 3.\n    *   Case $\\mathcal{C}_{34}$: Merging categories 3 and 4.\n\n    For each of these collapsed datasets, two models are fitted using Maximum Likelihood Estimation:\n    *   A **Proportional Odds (PO) Model**: This model maintains the parallel-slopes assumption on the new 3-category outcome.\n    *   A **Multinomial (MN) Model**: This model treats the 3-category outcome as nominal, allowing slopes to differ for each category comparison, thus serving as a more flexible (unconstrained) alternative.\n\n4.  **Model Evaluation and Comparison**: For each collapse case $\\mathcal{C}_{ab}$, two key metrics are calculated:\n    *   **Change in Slope Estimate ($\\Delta \\beta_T^{(ab)}$)**: The treatment slope from the new PO fit ($\\widehat{\\beta}_T^{(ab)}$) is compared to the baseline estimate: $\\Delta \\beta_T^{(ab)} = \\widehat{\\beta}_T^{(ab)} - \\widehat{\\beta}_T^{\\mathrm{base}}$. This measures how sensitive the parameter estimate is to the way categories are grouped. A large change might suggest the PO assumption is not robust.\n    *   **Difference in Akaike Information Criterion ($\\Delta \\mathrm{AIC}_{\\mathrm{MN}-\\mathrm{PO}}^{(ab)}$)**: The AIC values of the two models fitted on the *same* collapsed dataset are compared: $\\Delta \\mathrm{AIC}_{\\mathrm{MN}-\\mathrm{PO}}^{(ab)} = \\mathrm{AIC}_{\\mathrm{MN}}^{(ab)} - \\mathrm{AIC}_{\\mathrm{PO}}^{(ab)}$. AIC balances model fit with complexity. A negative value suggests that the superior fit of the flexible MN model outweighs its higher number of parameters, casting doubt on the parsimonious PO model's adequacy for that particular data structure.\n\nThe implementation uses numerical optimization for MLE. To ensure the ordering of thresholds in the PO model, a standard reparameterization trick ($\\alpha_k = \\alpha_{k-1} + \\exp(\\gamma_k)$) is employed.",
            "answer": "```python\n# The final answer must be a single, complete, standalone program.\n# Execution Environment: Python 3.12, numpy 1.23.5, scipy 1.11.4\n\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.special import expit as logistic_sigmoid\n\n# Small constant for numerical stability\nEPSILON = 1e-15\n\ndef generate_data():\n    \"\"\"Generates the deterministic synthetic dataset as per the problem description.\"\"\"\n    n = 200\n    i = np.arange(1, n + 1)\n\n    T = (i % 2 == 0).astype(int)\n    A = 30 + 0.1 * i\n\n    mean_A = np.mean(A)\n    std_A = np.std(A, ddof=1)\n    Z = (A - mean_A) / std_A\n\n    X = np.stack([T, Z], axis=1)\n\n    alpha_star = np.array([-0.5, 0.8, 1.8])\n    beta_star = np.array([-0.8, 0.5])\n    K = 4\n\n    eta_star = X @ beta_star\n\n    cum_probs = np.zeros((n, K - 1))\n    for k in range(K - 1):\n        cum_probs[:, k] = logistic_sigmoid(alpha_star[k] - eta_star)\n    \n    cat_probs = np.zeros((n, K))\n    cat_probs[:, 0] = cum_probs[:, 0]\n    for k in range(1, K - 1):\n        cat_probs[:, k] = cum_probs[:, k] - cum_probs[:, k-1]\n    cat_probs[:, K - 1] = 1 - cum_probs[:, K - 2]\n    \n    phi = (np.sqrt(5) - 1) / 2\n    u = (i * phi) % 1\n    \n    y = np.zeros(n, dtype=int)\n    cumulative_cat_probs = cat_probs.cumsum(axis=1) \n    for i_obs in range(n):\n        y[i_obs] = np.searchsorted(cumulative_cat_probs[i_obs, :], u[i_obs], side='left') + 1\n        \n    return X, y\n\ndef get_po_initial_params(y, K, p):\n    \"\"\"Computes initial parameters for the PO model.\"\"\"\n    n = len(y)\n    freqs = np.bincount(y - 1, minlength=K) / n\n    cum_freqs = np.cumsum(freqs)\n    \n    clipped_cum_freqs = np.clip(cum_freqs[:-1], EPSILON, 1 - EPSILON)\n    \n    try:\n        alpha_init = np.log(clipped_cum_freqs / (1 - clipped_cum_freqs))\n        gamma_init = np.zeros(K - 1)\n        gamma_init[0] = alpha_init[0]\n        if K > 2:\n            diffs = np.diff(alpha_init)\n            gamma_init[1:] = np.log(np.maximum(diffs, EPSILON))\n        if np.any(np.isnan(gamma_init)) or np.any(np.isinf(gamma_init)):\n            raise ValueError\n    except (ValueError, FloatingPointError):\n        gamma_init = np.linspace(-1, 1, K-1) \n\n    beta_init = np.zeros(p)\n    return np.concatenate([gamma_init, beta_init])\n    \ndef neg_log_lik_po(params, X, y, K):\n    \"\"\"Negative log-likelihood for the Proportional Odds model.\"\"\"\n    n, p = X.shape\n    gammas = params[:K-1]\n    betas = params[K-1:]\n    \n    alphas = np.zeros(K-1)\n    alphas[0] = gammas[0]\n    for k in range(1, K-1):\n        alphas[k] = alphas[k-1] + np.exp(gammas[k])\n        \n    eta = X @ betas\n    \n    cum_probs = np.zeros((n, K-1))\n    for k in range(K-1):\n        cum_probs[:, k] = logistic_sigmoid(alphas[k] - eta)\n\n    cat_probs = np.zeros((n, K))\n    cat_probs[:, 0] = cum_probs[:, 0]\n    if K > 2:\n        for k in range(1, K-1):\n            cat_probs[:, k] = cum_probs[:, k] - cum_probs[:, k-1]\n    cat_probs[:, K-1] = 1 - cum_probs[:, K-2]\n    \n    y_idx = y - 1\n    probs_obs = cat_probs[np.arange(n), y_idx]\n    \n    log_probs = np.log(np.maximum(probs_obs, EPSILON))\n    return -np.sum(log_probs)\n\ndef fit_po_model(X, y, K):\n    \"\"\"Fits the Proportional Odds model and returns results.\"\"\"\n    p = X.shape[1]\n    d = K - 1 + p\n    \n    initial_params = get_po_initial_params(y, K, p)\n\n    res = minimize(neg_log_lik_po, initial_params, args=(X, y, K), method='BFGS')\n    \n    log_lik = -res.fun\n    aic = 2 * d - 2 * log_lik\n    \n    beta_T_hat = res.x[K-1]\n    \n    return beta_T_hat, aic\n\ndef neg_log_lik_mn(params, X, y, K):\n    \"\"\"Negative log-likelihood for the Baseline-Category Multinomial model.\"\"\"\n    n, p = X.shape\n    \n    param_matrix = params.reshape((K-1, p+1))\n    X_aug = np.c_[np.ones(n), X]\n\n    scores = X_aug @ param_matrix.T\n    \n    # Log-sum-exp trick for stability\n    max_score_among_all = np.max(np.c_[scores, np.zeros(n)], axis=1, keepdims=True)\n    scores_stable = scores - max_score_among_all\n    \n    log_den = max_score_among_all.flatten() + np.log(np.sum(np.exp(scores_stable), axis=1) + np.exp(0 - max_score_among_all.flatten()))\n    \n    log_probs_k_minus_1 = scores - log_den[:, np.newaxis]\n    log_prob_K = -log_den\n    \n    log_cat_probs = np.c_[log_probs_k_minus_1, log_prob_K]\n\n    y_idx = y - 1\n    log_probs_obs = log_cat_probs[np.arange(n), y_idx]\n    \n    return -np.sum(log_probs_obs)\n\ndef fit_mn_model(X, y, K):\n    \"\"\"Fits the Baseline-Category Multinomial model and returns results.\"\"\"\n    p = X.shape[1]\n    d = (K-1) * (p+1)\n    \n    initial_params = np.zeros(d)\n\n    res = minimize(neg_log_lik_mn, initial_params, args=(X, y, K), method='BFGS', options={'gtol':1e-6})\n    \n    log_lik = -res.fun\n    aic = 2 * d - 2 * log_lik\n    \n    return aic\n\ndef solve():\n    \"\"\"Main function to perform the analysis and print results.\"\"\"\n    X, y_base = generate_data()\n    K_base = 4\n    \n    beta_T_base, _ = fit_po_model(X, y_base, K_base)\n    \n    results = []\n    \n    collapse_cases = [(1, 2), (2, 3), (3, 4)]\n    \n    for case in collapse_cases:\n        c1, c2 = case\n        y_collapsed = y_base.copy()\n        \n        if c1 == 1 and c2 == 2:\n            y_collapsed[y_collapsed == 2] = 1\n            y_collapsed[y_collapsed == 3] = 2\n            y_collapsed[y_collapsed == 4] = 3\n        elif c1 == 2 and c2 == 3:\n            y_collapsed[y_collapsed == 3] = 2\n            y_collapsed[y_collapsed == 4] = 3\n        elif c1 == 3 and c2 == 4:\n            y_collapsed[y_collapsed == 4] = 3\n            \n        K_collapsed = 3\n        \n        beta_T_hat_ab, aic_po_ab = fit_po_model(X, y_collapsed, K_collapsed)\n        aic_mn_ab = fit_mn_model(X, y_collapsed, K_collapsed)\n        \n        delta_beta_T = beta_T_hat_ab - beta_T_base\n        delta_aic = aic_mn_ab - aic_po_ab\n        \n        results.extend([delta_beta_T, delta_aic])\n\n    print(f\"[{','.join(f'{x:.6f}' for x in results)}]\")\n\nsolve()\n\n```"
        }
    ]
}