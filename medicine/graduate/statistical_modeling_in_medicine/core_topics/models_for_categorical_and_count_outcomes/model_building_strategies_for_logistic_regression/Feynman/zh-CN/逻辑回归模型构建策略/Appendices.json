{
    "hands_on_practices": [
        {
            "introduction": "为了有效地构建逻辑回归模型，我们必须首先掌握其核心组成部分的解释。本练习将重点关注线性预测变量、比值（odds）、概率以及比值比（odds ratio）之间的基本关系，这对于解释模型结果至关重要。通过将模型系数转化为具体的度量指标，您将巩固对预测变量如何影响结果的理解。",
            "id": "4974080",
            "problem": "给定一个适用于医学事件数据的二元结果模型，其中患者的事件指标 $Y \\in \\{0,1\\}$ 在给定预测变量向量 $\\mathbf{x} = (x_1,\\dots,x_k)$ 的条件下的条件概率为 $p(\\mathbf{x}) = \\mathbb{P}(Y=1 \\mid \\mathbf{x})$。假设使用逻辑斯谛回归模型，其基本关系定义为事件的对数优势比（logit）与预测变量呈线性关系：$$\\log\\left(\\frac{p(\\mathbf{x})}{1 - p(\\mathbf{x})}\\right) = \\beta_0 + \\sum_{j=1}^{k} \\beta_j x_j,$$ 其中 $\\beta_0$ 是截距，$(\\beta_1,\\dots,\\beta_k)$ 是回归系数。请仅从该定义以及优势比（odds）是概率与其补数之比的定义出发，推导对于给定的患者预测变量和指定的系数向量，如何计算拟合优势比和预测概率。然后，考虑在保持所有其他预测变量固定的情况下，将单个预测变量 $x_j$ 增加一个单位，并推导其对优势比的隐含乘法变化（即优势比率，odds ratio），作为 $\\beta_j$ 的函数。\n\n您的任务是实现一个程序，为下面的每个测试用例计算：\n- 在给定的预测变量向量 $\\mathbf{x}$ 和系数下的拟合优势比 $O(\\mathbf{x})$，\n- 预测概率 $p(\\mathbf{x})$，\n- 在保持其他预测变量固定的情况下，将指定预测变量索引 $j$ 增加一个单位的优势比率。\n\n所有计算输出都必须是实数。此问题中没有物理单位。您的程序必须将每个浮点数输出四舍五入到恰好六位小数。\n\n测试套件：\n- 用例 1：$\\beta = (-2.0, 0.6, 0.4)$，$\\mathbf{x} = (2.0, 1.5)$，$j = 1$。\n- 用例 2：$\\beta = (1.0, 2.0, -0.5, 0.3)$，$\\mathbf{x} = (3.0, -2.0, 5.0)$，$j = 2$。\n- 用例 3：$\\beta = (0.0, -3.0)$，$\\mathbf{x} = (2.0)$，$j = 1$。\n- 用例 4：$\\beta = (0.0, 0.0, 0.0)$，$\\mathbf{x} = (10.0, -5.0)$，$j = 2$。\n\n解释和索引细节：\n- 对于每个用例，$\\beta_0$ 是截距，其后是与 $\\mathbf{x}$ 的分量按顺序对应的 $(\\beta_1,\\dots,\\beta_k)$。\n- 索引 $j$ 指的是与 $x_j$ 关联的预测变量系数 $\\beta_j$，对预测变量使用基于 1 的索引（不包括截距）。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含所有测试用例的结果，形式为用方括号括起来的、逗号分隔的列表的列表。\n- 对于每个用例，输出一个形式为 $[O(\\mathbf{x}), p(\\mathbf{x}), \\text{OR}_j]$ 的列表，其中 $\\text{OR}_j$ 是 $x_j$ 增加一个单位的优势比率。\n- 每个数字必须四舍五入到恰好六位小数。\n- 格式示例（仅为例示）：$[[a_{11},a_{12},a_{13}],[a_{21},a_{22},a_{23}],\\dots]$，其中每个 $a_{mn}$ 是一个有六位小数的浮点数。\n\n您的程序必须在没有任何用户输入或外部文件的情况下运行，并且必须仅计算并打印上述四个指定用例的结果，格式必须与上述描述完全一致。",
            "solution": "该问题陈述经评估为有效。它在科学上基于广义线性模型，特别是逻辑斯谛回归的既定理论。该问题是适定的，提供了所有必要的数据和定义，以便为每个测试用例推导出唯一且有意义的解。语言客观而精确。\n\n任务要求在逻辑斯谛回归模型中推导和计算三个关键量：拟合优势比、预测概率和优势比率。我们从模型的基本定义开始。\n\n逻辑斯谛回归模型假定预测变量 $\\mathbf{x} = (x_1, \\dots, x_k)$ 与结果概率 $p(\\mathbf{x}) = \\mathbb{P}(Y=1 \\mid \\mathbf{x})$ 的对数优势比之间存在线性关系。该关系表示为：\n$$\n\\log\\left(\\frac{p(\\mathbf{x})}{1 - p(\\mathbf{x})}\\right) = \\beta_0 + \\sum_{j=1}^{k} \\beta_j x_j\n$$\n其中 $\\beta_0$ 是截距，$\\boldsymbol{\\beta} = (\\beta_1, \\dots, \\beta_k)$ 是预测变量系数的向量。为方便起见，我们将线性预测器（通常表示为 $\\eta(\\mathbf{x})$）定义为此方程的右侧：\n$$\n\\eta(\\mathbf{x}) = \\beta_0 + \\sum_{j=1}^{k} \\beta_j x_j = \\beta_0 + \\boldsymbol{\\beta}^T \\mathbf{x}\n$$\n\n**1. 拟合优势比 $O(\\mathbf{x})$ 的推导**\n\n事件的优势比定义为事件发生的概率与不发生的概率之比。\n$$\nO(\\mathbf{x}) = \\frac{p(\\mathbf{x})}{1 - p(\\mathbf{x})}\n$$\n通过将此定义代入基本模型方程，我们得到：\n$$\n\\log(O(\\mathbf{x})) = \\eta(\\mathbf{x})\n$$\n为了求解优势比 $O(\\mathbf{x})$，我们对此方程的两边取指数，利用指数函数是自然对数的反函数这一事实：\n$$\nO(\\mathbf{x}) = \\exp(\\log(O(\\mathbf{x}))) = \\exp(\\eta(\\mathbf{x}))\n$$\n因此，拟合优势比是通过对线性预测器取指数来计算的。\n$$\nO(\\mathbf{x}) = \\exp\\left(\\beta_0 + \\sum_{j=1}^{k} \\beta_j x_j\\right)\n$$\n\n**2. 预测概率 $p(\\mathbf{x})$ 的推导**\n\n为了找到预测概率 $p(\\mathbf{x})$，我们从优势比的定义 $O(\\mathbf{x}) = \\frac{p(\\mathbf{x})}{1 - p(\\mathbf{x})}$ 开始，并通过代数方法求解 $p(\\mathbf{x})$。\n$$\n\\begin{aligned}\nO(\\mathbf{x}) (1 - p(\\mathbf{x})) = p(\\mathbf{x}) \\\\\nO(\\mathbf{x}) - O(\\mathbf{x}) p(\\mathbf{x}) = p(\\mathbf{x}) \\\\\nO(\\mathbf{x}) = p(\\mathbf{x}) + O(\\mathbf{x}) p(\\mathbf{x}) \\\\\nO(\\mathbf{x}) = p(\\mathbf{x}) (1 + O(\\mathbf{x})) \\\\\np(\\mathbf{x}) = \\frac{O(\\mathbf{x})}{1 + O(\\mathbf{x})}\n\\end{aligned}\n$$\n代入我们推导出的 $O(\\mathbf{x}) = \\exp(\\eta(\\mathbf{x}))$ 的表达式，我们得到了用线性预测器表示的预测概率：\n$$\np(\\mathbf{x}) = \\frac{\\exp(\\eta(\\mathbf{x}))}{1 + \\exp(\\eta(\\mathbf{x}))}\n$$\n这个函数通常被称为逻辑斯谛函数或S型函数。通过将分子和分母同除以 $\\exp(\\eta(\\mathbf{x}))$，可以得到一个等价且通常在数值上更稳定的形式：\n$$\np(\\mathbf{x}) = \\frac{1}{\\exp(-\\eta(\\mathbf{x})) + 1}\n$$\n\n**3. 优势比率 $\\text{OR}_j$ 的推导**\n\n预测变量 $x_j$ 的优势比率（$OR$）是在保持所有其他预测变量不变的情况下，$x_j$ 每增加一个单位，优势比变化的倍数。设 $\\mathbf{x}$ 为原始预测变量向量 $(x_1, \\dots, x_j, \\dots, x_k)$，设 $\\mathbf{x}'$ 为 $x_j$ 增加一的向量：$(x_1, \\dots, x_j + 1, \\dots, x_k)$。\n\n在 $\\mathbf{x}$ 处的优势比为：\n$$\nO(\\mathbf{x}) = \\exp\\left(\\beta_0 + \\sum_{i=1}^{k} \\beta_i x_i\\right)\n$$\n在 $\\mathbf{x}'$ 处的优势比为：\n$$\nO(\\mathbf{x}') = \\exp\\left(\\beta_0 + \\sum_{i \\neq j} \\beta_i x_i + \\beta_j(x_j + 1)\\right) = \\exp\\left(\\beta_0 + \\sum_{i=1}^{k} \\beta_i x_i + \\beta_j\\right)\n$$\n优势比率 $\\text{OR}_j$ 是这两个优势比的比值：\n$$\n\\text{OR}_j = \\frac{O(\\mathbf{x}')}{O(\\mathbf{x})} = \\frac{\\exp\\left(\\beta_0 + \\sum_{i=1}^{k} \\beta_i x_i + \\beta_j\\right)}{\\exp\\left(\\beta_0 + \\sum_{i=1}^{k} \\beta_i x_i\\right)}\n$$\n利用指数性质 $\\frac{e^{a+b}}{e^a} = e^b$，我们可以简化表达式：\n$$\n\\text{OR}_j = \\exp\\left(\\left(\\beta_0 + \\sum_{i=1}^{k} \\beta_i x_i + \\beta_j\\right) - \\left(\\beta_0 + \\sum_{i=1}^{k} \\beta_i x_i\\right)\\right) = \\exp(\\beta_j)\n$$\n这是一个关键结果：预测变量 $x_j$ 增加一个单位的优势比率就是其对应系数 $\\beta_j$ 的指数。它是一个常数，不依赖于任何预测变量 $\\mathbf{x}$ 的值。\n\n**计算算法**\n\n对于每个由系数向量 $\\boldsymbol{\\beta}_{\\text{full}} = (\\beta_0, \\beta_1, \\dots, \\beta_k)$、预测变量向量 $\\mathbf{x} = (x_1, \\dots, x_k)$ 和预测变量索引 $j$ 指定的测试用例，算法如下：\n1.  计算线性预测器：$\\eta(\\mathbf{x}) = \\beta_0 + \\sum_{i=1}^{k} \\beta_i x_i$。这可以通过计算预测变量系数 $(\\beta_1, \\dots, \\beta_k)$ 与 $\\mathbf{x}$ 的点积，再加上截距 $\\beta_0$ 来高效完成。\n2.  计算拟合优势比：$O(\\mathbf{x}) = \\exp(\\eta(\\mathbf{x}))$。\n3.  计算预测概率：$p(\\mathbf{x}) = \\frac{O(\\mathbf{x})}{1 + O(\\mathbf{x})}$。\n4.  计算优势比率：$\\text{OR}_j = \\exp(\\beta_j)$。问题使用基于 1 的索引 $j$，因此 $\\beta_j$ 指的是 $x_j$ 的系数。当使用基于 0 的数组时，这将是完整系数向量中索引为 $j$ 的元素。\n5.  将三个结果 $O(\\mathbf{x})$、$p(\\mathbf{x})$ 和 $\\text{OR}_j$ 中的每一个都四舍五入到恰好六位小数，作为最终输出。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes fitted odds, predicted probability, and an odds ratio for a logistic\n    regression model across several test cases.\n    \"\"\"\n\n    # Test cases defined as tuples of (beta_vector, x_vector, j_index).\n    # beta_vector is (beta_0, beta_1, ..., beta_k).\n    # x_vector is (x_1, ..., x_k).\n    # j_index is 1-based for the predictor of interest.\n    test_cases = [\n        # Case 1: beta = (-2.0, 0.6, 0.4), x = (2.0, 1.5), j = 1\n        ((-2.0, 0.6, 0.4), (2.0, 1.5), 1),\n        # Case 2: beta = (1.0, 2.0, -0.5, 0.3), x = (3.0, -2.0, 5.0), j = 2\n        ((1.0, 2.0, -0.5, 0.3), (3.0, -2.0, 5.0), 2),\n        # Case 3: beta = (0.0, -3.0), x = (2.0), j = 1\n        ((0.0, -3.0), (2.0,), 1),\n        # Case 4: beta = (0.0, 0.0, 0.0), x = (10.0, -5.0), j = 2\n        ((0.0, 0.0, 0.0), (10.0, -5.0), 2)\n    ]\n\n    all_results = []\n    for case in test_cases:\n        beta_tuple, x_tuple, j = case\n\n        # Convert tuples to numpy arrays for vectorized operations\n        beta = np.array(beta_tuple)\n        x = np.array(x_tuple)\n\n        # Decompose beta into intercept and predictor coefficients\n        beta_0 = beta[0]\n        beta_predictors = beta[1:]\n\n        # 1. Calculate the linear predictor (eta)\n        # eta = beta_0 + sum(beta_i * x_i)\n        eta = beta_0 + np.dot(beta_predictors, x)\n\n        # 2. Calculate the fitted odds\n        # O(x) = exp(eta)\n        odds = np.exp(eta)\n\n        # 3. Calculate the predicted probability\n        # p(x) = O(x) / (1 + O(x))\n        prob = odds / (1 + odds)\n\n        # 4. Calculate the odds ratio for a one-unit increase in x_j\n        # OR_j = exp(beta_j)\n        # The problem uses 1-based indexing for j, which corresponds to\n        # the coefficient beta[j] in our 0-indexed beta array.\n        odds_ratio = np.exp(beta[j])\n\n        # Store the results for this case. The problem requires rounding to\n        # exactly six decimal places, which we achieve using f-string formatting.\n        all_results.append([odds, prob, odds_ratio])\n\n    # Format the final output string as a list of lists of numbers\n    # with each number formatted to exactly six decimal places.\n    formatted_cases = []\n    for result_set in all_results:\n        o, p, or_j = result_set\n        formatted_cases.append(f\"[{o:.6f},{p:.6f},{or_j:.6f}]\")\n\n    print(f\"[{','.join(formatted_cases)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "模型构建的一个关键方面是决定包含哪些预测变量。似然比检验（likelihood ratio test）为比较嵌套模型提供了一种原则性的统计方法，例如比较一个简单模型与一个增加了额外预测变量的模型。本练习将指导您使用似然比检验来正式评估增加一个新预测变量是否能显著改善模型拟合度，这是系统化模型选择策略的基石。",
            "id": "4974063",
            "problem": "在一项包含 $n = 2{,}200$ 名因2019冠状病毒病（COVID-19）住院的成年人的多中心队列研究中，研究人员使用逻辑回归模型来模拟 $30$ 天院内死亡率的概率。设 $Y_{i} \\in \\{0,1\\}$ 表示患者 $i$ 的死亡情况，并设 $\\boldsymbol{X}_{i}$ 表示一个基线预测因子向量（包括年龄、性别、合并症指数和一个综合严重程度评分）。简化模型仅使用 $\\boldsymbol{X}_{i}$。一个提议的完整模型在简化模型的基础上增加了一个额外的连续预测因子 $x_{j}$，该因子代表入院时的血清乳酸水平（单位为 $\\text{mmol}/\\text{L}$），并以队列均值为中心进行了中心化。\n\n两个模型都在标准正则性条件（独立观测、正确指定的连接函数和均值结构、无分离以及有限的Fisher信息）下通过最大似然法进行拟合。简化模型的最大化对数似然为 $\\ell_{0} = -1{,}248.30$，完整模型的最大化对数似然为 $\\ell_{1} = -1{,}241.80$。假设完整模型与简化模型相比恰好多出一个自由参数（即 $x_{j}$ 的系数），并且渐近似然理论适用。\n\n使用嵌套模型的似然原理和适当的渐近参考分布，计算比较完整模型和简化模型的检验统计量，然后计算用于检验 $x_{j}$ 系数为 $0$ 这一原假设的相应 $p$ 值。简要说明比较中使用的渐近分布的理由，并在医学领域逻辑回归的原则性模型构建策略背景下解释所得 $p$ 值的含义。\n\n将 $p$ 值以小数形式报告，并四舍五入至三位有效数字，作为你的最终答案。不要报告不等式，请提供一个单一的数字。",
            "solution": "该问题要求使用似然比检验（LRT）比较两个嵌套的逻辑回归模型。简化模型（我们记为 $M_0$）使用一组基线预测因子 $\\boldsymbol{X}_{i}$ 来预测死亡率。完整模型 $M_1$ 包含 $M_0$ 中的所有预测因子，外加一个额外的预测因子，即入院血清乳酸 $x_{j}$。这两个模型是嵌套的，因为 $M_0$ 是 $M_1$ 的一个特例，其中 $x_{j}$ 的系数被约束为零。\n\n比较基于似然原理，该原理指出，数据中关于参数的所有证据都包含在似然函数中。对于嵌套模型，可以通过比较它们的最大化对数似然值来量化较大模型中额外参数所带来的拟合优度的提升。\n\n似然比检验的检验统计量用 $D$ 表示，其计算方法是完整模型最大化对数似然 ($\\ell_1$) 与简化模型最大化对数似然 ($\\ell_0$)之差的两倍。\n$$\nD = 2(\\ell_{1} - \\ell_{0})\n$$\n问题提供了所需的值：\n- 简化模型（$M_0$）的最大化对数似然：$\\ell_{0} = -1{,}248.30$\n- 完整模型（$M_1$）的最大化对数似然：$\\ell_{1} = -1{,}241.80$\n\n将这些值代入检验统计量的公式中：\n$$\nD = 2(-1{,}241.80 - (-1{,}248.30)) = 2(-1{,}241.80 + 1{,}248.30) = 2(6.50) = 13.00\n$$\n计算出的检验统计量为 $D = 13.00$。\n\n下一步是通过将此值与适当的参考分布进行比较来确定其统计显著性。原假设 $H_0$ 是额外预测因子 $x_{j}$ 的系数为零。这等价于陈述简化模型 $M_0$ 是正确的模型。根据 Wilks 定理，在原假设下，对于足够大的样本量（$n=2{,}200$ 很大），似然比检验统计量 $D$ 渐近服从卡方（$\\chi^2$）分布。\n\n这个 $\\chi^2$ 分布的自由度（$df$）等于完整模型和简化模型之间自由估计参数数量的差异。问题陈述完整模型比简化模型恰好多一个自由参数（$x_{j}$ 的系数）。因此，自由度为 $df = 1$。\n所以，在 $H_0$ 下，检验统计量 $D$ 服从自由度为1的卡方分布（$\\chi^2_1$）：\n$$\nD \\sim \\chi^2_1\n$$\n$p$ 值是在原假设为真的前提下，观测到与计算出的检验统计量一样极端或更极端的检验统计量的概率。这对应于 $\\chi^2_1$ 分布的上尾概率。\n$$\np = P(\\chi^2_1 \\ge 13.00)\n$$\n使用标准统计计算器或软件查找自由度为 $1$ 的 $\\chi^2$ 分布的累积概率，我们发现值大于或等于 $13.00$ 的概率约为 $0.0003115$。按要求将此值四舍五入至三位有效数字，得到 $0.000312$。\n\n使用 $\\chi^2_1$ 分布的理由是 Wilks 定理，该定理确立了在标准正则性条件下嵌套模型的似然比统计量的渐近分布。问题描述中已说明这些条件（大样本量、独立观测、模型设定正确等）均已满足。\n\n在医学模型构建的背景下，对这个 $p$ 值的解释如下。这个非常小的 $p$ 值（$p \\approx 0.000312$）远低于常规的显著性水平（例如 $\\alpha = 0.05$ 或 $\\alpha = 0.01$）。这为拒绝血清乳酸系数为零的原假设提供了强有力的统计证据。换句话说，在包含年龄、性别、合并症指数和严重程度评分的模型中加入血清乳酸，可以使模型对数据的拟合度产生统计学上显著的改善。从原则性模型构建的角度来看，这一结果表明血清乳酸是该患者人群中 $30$ 天死亡率的重要预测因子，数据强烈支持将其纳入最终的预测模型中。保留该变量的决定还应考虑临床合理性和简约原则，但这里的统计证据非常有说服力。",
            "answer": "$$\\boxed{0.000312}$$"
        },
        {
            "introduction": "在逻辑回归中，当数据出现分离（separation）时，标准的极大似然估计可能会失败，而这是医学数据集中常见的问题。惩罚回归方法（如岭回归）通过生成稳定且有限的系数估计值，为此提供了稳健的解决方案。这项高级练习将从第一性原理出发，实现一个惩罚逻辑回归模型，使您掌握即使面对具挑战性的数据结构也能构建可靠模型的技能。",
            "id": "4974092",
            "problem": "考虑一个使用逻辑回归的医疗事件发生二元结果模型，其中每个个体 $i$ 提供一个特征向量 $\\boldsymbol{x}_i \\in \\mathbb{R}^p$ 和一个结果 $y_i \\in \\{0,1\\}$。其基本基础包括：(i) 伯努利数据模型，概率为 $\\mathbb{P}(Y_i = 1 \\mid \\boldsymbol{x}_i) = \\mu_i$，(ii) 逻辑连接函数 $\\mu_i = \\left(1 + \\exp\\left(-\\eta_i\\right)\\right)^{-1}$ 和线性预测器 $\\eta_i = \\boldsymbol{x}_i^\\top \\boldsymbol{\\beta}$，以及 (iii) 通过独立伯努利观测的对数似然进行的最大似然估计。在医疗数据集中，当存在一个能够完美（或近乎完美）地对结果进行分类的预测变量线性组合时，就会发生完全或准完全分离，这使得未惩罚的最大似然估计不稳定或非有限。\n\n您的任务是为逻辑回归实现一种模型构建策略，通过对系数添加欧几里得范数（L2）惩罚，在存在分离的情况下产生有限、稳定的估计值。具体来说，通过最大化一个惩罚目标函数来估计 $\\boldsymbol{\\beta}$，该函数由伯努利对数似然加上系数的二次惩罚项构成，但不惩罚截距。使用广义线性模型的规范迭代加权框架，从上述基本基础中推导出算法步骤，并在每次迭代中求解惩罚正规方程。通过适当处理极端的拟合值来确保数值稳定性。使用 $x_{\\text{age}}^{*} = \\left(x_{\\text{age}} - \\bar{x}_{\\text{age}}\\right)/10$ 对连续预测变量 $x_{\\text{age}}$ 进行标准化以改善条件数，其中 $10$ 是以年为单位的缩放因子。\n\n实现一个完整的、可运行的程序，该程序针对以下测试套件，拟合惩罚逻辑回归模型并报告系数估计值和拟合概率。截距不得被惩罚。对于每个测试用例，输出一个列表，其中包含估计的截距和系数，然后是训练数据上的最小和最大拟合概率，最后是为两个指定的新患者计算的拟合概率。所有输出必须是实数。最终的程序输出必须是单行，包含一个由各测试用例列表组成的列表，用方括号括起来，元素之间用逗号分隔。\n\n测试套件规范（每个用例使用一个截距和两个预测变量：以年为单位的 $x_{\\text{age}}$ 和表示二元生物标志物的 $x_{\\text{bio}} \\in \\{0,1\\}$）：\n\n- 用例 1（准完全分离，中等惩罚）：\n  - 年龄（岁）：$[55,64,47,38,70,59,53,61,49,45,72,50]$。\n  - 生物标志物：$[1,1,1,0,1,0,1,1,0,0,1,0]$。\n  - 结果：$[1,1,1,0,1,0,1,0,0,0,1,0]$。\n  - 惩罚参数：$\\lambda = 1$。\n  - 需要预测的新患者：\n    - 患者 A：$x_{\\text{age}} = 60$， $x_{\\text{bio}} = 1$。\n    - 患者 B：$x_{\\text{age}} = 50$， $x_{\\text{bio}} = 0$。\n\n- 用例 2（完全分离，中等惩罚）：\n  - 年龄（岁）：$[40,42,45,50,52,55,60,62,65,70]$。\n  - 生物标志物：$[0,0,0,0,1,1,1,1,1,1]$。\n  - 结果：$[0,0,0,0,1,1,1,1,1,1]$。\n  - 惩罚参数：$\\lambda = 1$。\n  - 需要预测的新患者：\n    - 患者 A：$x_{\\text{age}} = 60$， $x_{\\text{bio}} = 1$。\n    - 患者 B：$x_{\\text{age}} = 50$， $x_{\\text{bio}} = 0$。\n\n- 用例 3（极端不平衡，接近分离，轻度惩罚）：\n  - 年龄（岁）：$[30,35,40,45,50,55,60,65,70,75,80,85]$。\n  - 生物标志物：$[0,0,0,0,0,1,0,1,0,1,0,1]$。\n  - 结果：$[0,0,0,0,0,0,0,0,0,1,0,0]$。\n  - 惩罚参数：$\\lambda = 0.1$。\n  - 需要预测的新患者：\n    - 患者 A：$x_{\\text{age}} = 60$， $x_{\\text{bio}} = 1$。\n    - 患者 B：$x_{\\text{age}} = 50$， $x_{\\text{bio}} = 0$。\n\n- 用例 4（高收缩以评估强惩罚下的稳定性）：\n  - 使用与用例 1 相同的年龄、生物标志物和结果。\n  - 惩罚参数：$\\lambda = 100$。\n  - 需要预测的新患者：\n    - 患者 A：$x_{\\text{age}} = 60$, $x_{\\text{bio}} = 1$。\n    - 患者 B：$x_{\\text{age}} = 50$, $x_{\\text{bio}} = 0$。\n\n对于每个用例，构建设计矩阵，包含截距项 $x_0 = 1$、标准化年龄预测变量 $x_{\\text{age}}^{*}$ 和生物标志物 $x_{\\text{bio}}$。拟合惩罚模型并计算：\n- 估计的截距和系数 ($\\hat{\\beta}_0, \\hat{\\beta}_{\\text{age}}, \\hat{\\beta}_{\\text{bio}})$，作为有限实数。\n- 训练数据上的最小和最大拟合概率，每个都表示为 $[0,1]$ 范围内的十进制数。\n- 患者 A 和患者 B 的拟合概率，每个都表示为 $[0,1]$ 范围内的十进制数。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个元素对应一个测试用例，本身是一个包含八个浮点数的列表：[$\\hat{\\beta}_0$, $\\hat{\\beta}_{\\text{age}}$, $\\hat{\\beta}_{\\text{bio}}$, $\\min(\\hat{\\mu})$, $\\max(\\hat{\\mu})$, $\\hat{\\mu}_{\\text{A}}$, $\\hat{\\mu}_{\\text{B}}$]。",
            "solution": "该问题要求实现一个惩罚逻辑回归模型，以确保在数据分离存在的情况下系数估计的稳定性。该解决方案是根据第一性原理，使用迭代重加权最小二乘法（Iteratively Reweighted Least Squares, IRLS）算法开发的，并针对 L2 惩罚（岭回归）进行了调整。\n\n### 1. 模型和目标函数\n\n对于给定的预测向量 $\\boldsymbol{x}_i \\in \\mathbb{R}^p$，二元结果 $y_i \\in \\{0, 1\\}$ 的模型由逻辑回归框架指定：\n1.  **数据模型**：结果 $Y_i$ 服从伯努利分布，其成功概率为 $\\mu_i = \\mathbb{P}(Y_i = 1 \\mid \\boldsymbol{x}_i)$。\n2.  **系统性部分**：预测变量通过线性预测器 $\\eta_i = \\boldsymbol{x}_i^\\top \\boldsymbol{\\beta}$ 与结果的对数几率呈线性关系，其中 $\\boldsymbol{\\beta} \\in \\mathbb{R}^p$ 是系数向量。设计矩阵 $\\boldsymbol{X}$ 是一个 $N \\times p$ 矩阵，其中第 $i$ 行是 $\\boldsymbol{x}_i^\\top$。对于此问题，$p=3$ 且 $\\boldsymbol{x}_i = [1, x_{\\text{age}, i}^{*}, x_{\\text{bio}, i}]^\\top$。\n3.  **连接函数**：逻辑连接函数将概率 $\\mu_i$ 与线性预测器 $\\eta_i$ 连接起来：$\\eta_i = \\log\\left(\\frac{\\mu_i}{1 - \\mu_i}\\right)$。其逆连接函数为 $\\mu_i = \\frac{1}{1 + \\exp(-\\eta_i)}$。\n\n$N$ 个独立观测的对数似然是各个伯努利对数似然之和：\n$$\n\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^N \\left[ y_i \\log(\\mu_i) + (1-y_i) \\log(1-\\mu_i) \\right]\n$$\n代入 $\\eta_i = \\boldsymbol{x}_i^\\top \\boldsymbol{\\beta}$ 以及 $\\mu_i$ 和 $\\eta_i$ 之间的关系，可以表示为：\n$$\n\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^N \\left[ y_i (\\boldsymbol{x}_i^\\top \\boldsymbol{\\beta}) - \\log(1 + \\exp(\\boldsymbol{x}_i^\\top \\boldsymbol{\\beta})) \\right]\n$$\n为了解决由分离引起的估计不稳定性，我们对除截距 $\\beta_0$ 之外的所有系数的对数似然添加一个 L2 惩罚。需要最大化的惩罚对数似然目标函数是：\n$$\nQ(\\boldsymbol{\\beta}) = \\ell(\\boldsymbol{\\beta}) - \\frac{\\lambda}{2} \\sum_{j=1}^{p-1} \\beta_j^2\n$$\n其中 $\\lambda > 0$ 是惩罚参数。系数 $\\frac{1}{2}$ 是为了数学上的方便而设定的惯例。\n\n### 2. 通过惩罚 IRLS 进行优化\n\n我们使用基于 Newton-Raphson 的方法来找到最大化 $Q(\\boldsymbol{\\beta})$ 的 $\\boldsymbol{\\beta}$。更新步骤为 $\\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} - [\\nabla^2 Q(\\boldsymbol{\\beta}^{(t)})]^{-1} \\nabla Q(\\boldsymbol{\\beta}^{(t)})$。\n\n**梯度向量（得分）：**\n对数似然的梯度是 $\\nabla \\ell(\\boldsymbol{\\beta}) = \\boldsymbol{X}^\\top (\\boldsymbol{y} - \\boldsymbol{\\mu})$，其中 $\\boldsymbol{y}$ 是结果向量，$\\boldsymbol{\\mu}$ 是概率向量。惩罚项的梯度是 $\\lambda \\boldsymbol{P} \\boldsymbol{\\beta}$，其中 $\\boldsymbol{P}$ 是一个对角矩阵，其对角元素为 $P_{00}=0$ 和 $P_{jj}=1$（对于 $j \\ge 1$）。\n惩罚目标的梯度是：\n$$\n\\nabla Q(\\boldsymbol{\\beta}) = \\boldsymbol{X}^\\top (\\boldsymbol{y} - \\boldsymbol{\\mu}) - \\lambda \\boldsymbol{P} \\boldsymbol{\\beta}\n$$\n\n**Hessian 矩阵：**\n对数似然的 Hessian 矩阵是 $\\nabla^2 \\ell(\\boldsymbol{\\beta}) = -\\boldsymbol{X}^\\top \\boldsymbol{W} \\boldsymbol{X}$，其中 $\\boldsymbol{W}$ 是一个对角矩阵，其元素为 $W_{ii} = \\mu_i(1-\\mu_i)$，即伯努利响应的方差。惩罚项的 Hessian 矩阵是 $-\\lambda \\boldsymbol{P}$。\n惩罚目标的 Hessian 矩阵是：\n$$\n\\nabla^2 Q(\\boldsymbol{\\beta}) = -\\boldsymbol{X}^\\top \\boldsymbol{W} \\boldsymbol{X} - \\lambda \\boldsymbol{P}\n$$\n\n**Newton-Raphson 更新：**\n将梯度和 Hessian 矩阵代入更新规则得到：\n$$\n\\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} - [-\\boldsymbol{X}^\\top \\boldsymbol{W}^{(t)} \\boldsymbol{X} - \\lambda \\boldsymbol{P}]^{-1} [\\boldsymbol{X}^\\top (\\boldsymbol{y} - \\boldsymbol{\\mu}^{(t)}) - \\lambda \\boldsymbol{P} \\boldsymbol{\\beta}^{(t)}]\n$$\n$$\n\\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} + [\\boldsymbol{X}^\\top \\boldsymbol{W}^{(t)} \\boldsymbol{X} + \\lambda \\boldsymbol{P}]^{-1} [\\boldsymbol{X}^\\top (\\boldsymbol{y} - \\boldsymbol{\\mu}^{(t)}) - \\lambda \\boldsymbol{P} \\boldsymbol{\\beta}^{(t)}]\n$$\n重新整理此方程揭示了 IRLS 结构。我们来求解 $\\boldsymbol{\\beta}^{(t+1)}$：\n$$\n(\\boldsymbol{X}^\\top \\boldsymbol{W}^{(t)} \\boldsymbol{X} + \\lambda \\boldsymbol{P}) \\boldsymbol{\\beta}^{(t+1)} = (\\boldsymbol{X}^\\top \\boldsymbol{W}^{(t)} \\boldsymbol{X} + \\lambda \\boldsymbol{P}) \\boldsymbol{\\beta}^{(t)} + \\boldsymbol{X}^\\top (\\boldsymbol{y} - \\boldsymbol{\\mu}^{(t)}) - \\lambda \\boldsymbol{P} \\boldsymbol{\\beta}^{(t)}\n$$\n$$\n(\\boldsymbol{X}^\\top \\boldsymbol{W}^{(t)} \\boldsymbol{X} + \\lambda \\boldsymbol{P}) \\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{X}^\\top \\boldsymbol{W}^{(t)} \\boldsymbol{X} \\boldsymbol{\\beta}^{(t)} + \\boldsymbol{X}^\\top (\\boldsymbol{y} - \\boldsymbol{\\mu}^{(t)})\n$$\n右侧可以写成 $\\boldsymbol{X}^\\top \\boldsymbol{W}^{(t)} \\boldsymbol{z}^{(t)}$，其中 $\\boldsymbol{z}^{(t)}$ 是工作响应向量：\n$$\n\\boldsymbol{z}^{(t)} = \\boldsymbol{X} \\boldsymbol{\\beta}^{(t)} + (\\boldsymbol{W}^{(t)})^{-1} (\\boldsymbol{y} - \\boldsymbol{\\mu}^{(t)}) = \\boldsymbol{\\eta}^{(t)} + \\frac{\\boldsymbol{y} - \\boldsymbol{\\mu}^{(t)}}{\\boldsymbol{\\mu}^{(t)}(1-\\boldsymbol{\\mu}^{(t)})}\n$$\n通过求解以下线性方程组可以得到 $\\boldsymbol{\\beta}^{(t+1)}$ 的更新，这些方程是加权岭回归的正规方程：\n$$\n(\\boldsymbol{X}^\\top \\boldsymbol{W}^{(t)} \\boldsymbol{X} + \\lambda \\boldsymbol{P}) \\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{X}^\\top \\boldsymbol{W}^{(t)} \\boldsymbol{z}^{(t)}\n$$\n\n### 3. 算法实现\n\n使用以下迭代算法拟合惩罚逻辑回归模型：\n\n1.  **数据标准化**：使用训练数据均值 $\\bar{x}_{\\text{age}}$ 对连续预测变量 $x_{\\text{age}}$ 进行标准化：$x_{\\text{age}}^{*} = (x_{\\text{age}} - \\bar{x}_{\\text{age}}) / 10$。构建设计矩阵 $\\boldsymbol{X}$，其列分别为截距（$1$）、标准化年龄（$x_{\\text{age}}^{*}$）和生物标志物（$x_{\\text{bio}}$）。\n\n2.  **初始化**：初始化系数向量 $\\boldsymbol{\\beta}^{(0)} = \\boldsymbol{0}$，并设置惩罚矩阵 $\\boldsymbol{P} = \\text{diag}(0, 1, ..., 1)$。选择一个收敛容差 $\\epsilon$（例如 $10^{-8}$）和最大迭代次数。\n\n3.  **迭代**：对于 $t=0, 1, 2, ...$，直到收敛：\n    a.  计算线性预测器：$\\boldsymbol{\\eta}^{(t)} = \\boldsymbol{X} \\boldsymbol{\\beta}^{(t)}$。\n    b.  计算拟合概率：$\\boldsymbol{\\mu}^{(t)} = (1 + \\exp(-\\boldsymbol{\\eta}^{(t)}))^{-1}$。为确保数值稳定性并防止权重变为零，将概率裁剪到距离 $0$ 和 $1$ 一个很小的范围内，例如 $\\mu_i \\in [10^{-10}, 1 - 10^{-10}]$。\n    c.  计算 IRLS 权重：构建对角矩阵 $\\boldsymbol{W}^{(t)}$，其对角元素为 $W_{ii}^{(t)} = \\mu_i^{(t)}(1-\\mu_i^{(t)})$。\n    d.  计算工作响应：$\\boldsymbol{z}^{(t)} = \\boldsymbol{\\eta}^{(t)} + (\\boldsymbol{y} - \\boldsymbol{\\mu}^{(t)}) / (\\mu_i^{(t)}(1-\\mu_i^{(t)}))$。\n    e.  从惩罚正规方程中求解更新后的系数 $\\boldsymbol{\\beta}^{(t+1)}$：\n        $$\n        \\boldsymbol{\\beta}^{(t+1)} = (\\boldsymbol{X}^\\top \\boldsymbol{W}^{(t)} \\boldsymbol{X} + \\lambda \\boldsymbol{P})^{-1} (\\boldsymbol{X}^\\top \\boldsymbol{W}^{(t)} \\boldsymbol{z}^{(t)})\n        $$\n        这个线性系统可以高效求解，无需显式求逆，例如使用 `numpy.linalg.solve`。\n    f.  检查收敛性：如果系数的变化小于容差，即 $\\sum_j |\\beta_j^{(t+1)} - \\beta_j^{(t)}|  \\epsilon$，则循环终止。\n\n4.  **最终估计**：收敛后，最终的系数向量为 $\\hat{\\boldsymbol{\\beta}} = \\boldsymbol{\\beta}^{(t+1)}$。训练数据和新患者的最终拟合概率使用 $\\hat{\\boldsymbol{\\mu}} = (1 + \\exp(-\\boldsymbol{x}^\\top \\hat{\\boldsymbol{\\beta}}))^{-1}$ 计算，其中对于新患者，预测变量 $x_{\\text{age}}$ 使用原始训练数据的均值进行标准化。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef fit_penalized_logistic(ages, biomarker, outcomes, lambda_val, new_patients):\n    \"\"\"\n    Fits a penalized logistic regression model using Iteratively Reweighted Least Squares (IRLS).\n\n    Args:\n        ages (list): List of patient ages.\n        biomarker (list): List of binary biomarker statuses.\n        outcomes (list): List of binary outcomes.\n        lambda_val (float): The L2 penalty parameter.\n        new_patients (list of tuples): Data for new patients to predict on.\n\n    Returns:\n        list: A list of 8 floats: [beta_0, beta_age, beta_bio, min_mu, max_mu, mu_A, mu_B].\n    \"\"\"\n    # 1. Preprocessing and design matrix construction\n    ages_arr = np.array(ages, dtype=float)\n    biomarker_arr = np.array(biomarker, dtype=float)\n    y = np.array(outcomes, dtype=float)\n    n_obs = len(ages_arr)\n\n    mean_age = np.mean(ages_arr)\n    # Standardize age predictor as specified\n    scaled_age = (ages_arr - mean_age) / 10.0\n\n    # Design matrix X with intercept, standardized age, and biomarker\n    X = np.c_[np.ones(n_obs), scaled_age, biomarker_arr]\n    p = X.shape[1]\n\n    # 2. IRLS algorithm\n    # Initialization\n    beta = np.zeros(p)\n    # Penalty matrix P (does not penalize intercept)\n    P = np.diag([0.0] + [1.0] * (p - 1))\n    \n    # Convergence parameters\n    max_iter = 50\n    tol = 1e-8\n    epsilon = 1e-10\n\n    for i in range(max_iter):\n        # Calculate linear predictor and probabilities\n        eta = X @ beta\n        mu = 1.0 / (1.0 + np.exp(-eta))\n        \n        # Clip probabilities for numerical stability\n        mu = np.clip(mu, epsilon, 1.0 - epsilon)\n        \n        # Calculate weights and working response\n        W_diag = mu * (1.0 - mu)\n        W = np.diag(W_diag)\n        z = eta + (y - mu) / W_diag\n        \n        # Solve the penalized normal equations\n        # (X^T W X + lambda * P) beta = X^T W z\n        A = X.T @ W @ X + lambda_val * P\n        b = X.T @ W @ z\n        \n        try:\n            beta_new = np.linalg.solve(A, b)\n        except np.linalg.LinAlgError:\n            # Fallback to pseudo-inverse if solve fails, though unlikely with penalty\n            beta_new = np.linalg.pinv(A) @ b\n\n        # Check for convergence\n        if np.sum(np.abs(beta_new - beta))  tol:\n            beta = beta_new\n            break\n        \n        beta = beta_new\n\n    # 3. Final calculations\n    beta_hat = beta\n    \n    # Fitted probabilities on training data\n    final_eta = X @ beta_hat\n    final_mu = 1.0 / (1.0 + np.exp(-final_eta))\n    min_mu = np.min(final_mu)\n    max_mu = np.max(final_mu)\n\n    # Predictions for new patients\n    mu_preds = []\n    for patient_data in new_patients:\n        age_new, bio_new = patient_data\n        # Standardize age using the training data mean\n        scaled_age_new = (age_new - mean_age) / 10.0\n        x_new = np.array([1.0, scaled_age_new, float(bio_new)])\n        eta_new = x_new @ beta_hat\n        mu_new = 1.0 / (1.0 + np.exp(-eta_new))\n        mu_preds.append(mu_new)\n        \n    # 4. Assemble results in the required format\n    result_list = list(beta_hat) + [min_mu, max_mu] + mu_preds\n    return result_list\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final output.\n    \"\"\"\n    test_cases = [\n        {\n            \"ages\": [55, 64, 47, 38, 70, 59, 53, 61, 49, 45, 72, 50],\n            \"biomarker\": [1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0],\n            \"outcomes\": [1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0],\n            \"lambda\": 1.0,\n            \"new_patients\": [(60, 1), (50, 0)],\n        },\n        {\n            \"ages\": [40, 42, 45, 50, 52, 55, 60, 62, 65, 70],\n            \"biomarker\": [0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n            \"outcomes\": [0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n            \"lambda\": 1.0,\n            \"new_patients\": [(60, 1), (50, 0)],\n        },\n        {\n            \"ages\": [30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85],\n            \"biomarker\": [0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1],\n            \"outcomes\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n            \"lambda\": 0.1,\n            \"new_patients\": [(60, 1), (50, 0)],\n        },\n        {\n            \"ages\": [55, 64, 47, 38, 70, 59, 53, 61, 49, 45, 72, 50],\n            \"biomarker\": [1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0],\n            \"outcomes\": [1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0],\n            \"lambda\": 100.0,\n            \"new_patients\": [(60, 1), (50, 0)],\n        },\n    ]\n\n    all_results = []\n    for case in test_cases:\n        results = fit_penalized_logistic(\n            ages=case[\"ages\"],\n            biomarker=case[\"biomarker\"],\n            outcomes=case[\"outcomes\"],\n            lambda_val=case[\"lambda\"],\n            new_patients=case[\"new_patients\"]\n        )\n        all_results.append(results)\n\n    # Format the final output string exactly as required\n    # Create string representations of inner lists, then join them\n    inner_lists_str = [f\"[{','.join(f'{x:.8f}' for x in res)}]\" for res in all_results]\n    print(f\"[{','.join(inner_lists_str)}]\")\n\nsolve()\n```"
        }
    ]
}