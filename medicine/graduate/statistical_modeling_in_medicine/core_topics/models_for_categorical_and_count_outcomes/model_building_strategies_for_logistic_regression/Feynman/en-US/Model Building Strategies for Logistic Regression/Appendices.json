{
    "hands_on_practices": [
        {
            "introduction": "A logistic regression model does not directly predict the probability of an event, but rather the logarithm of its odds. This exercise provides foundational practice in translating the model's raw output—the linear predictor or log-odds—into clinically and intuitively meaningful quantities: the odds, the predicted probability, and the odds ratio ($OR$). Mastering this translation is the first step toward correctly interpreting and communicating the results of any logistic regression analysis .",
            "id": "4974080",
            "problem": "You are given a binary outcome model appropriate for medical event data, where a patient’s event indicator $Y \\in \\{0,1\\}$ has conditional probability $p(\\mathbf{x}) = \\mathbb{P}(Y=1 \\mid \\mathbf{x})$ given a vector of predictors $\\mathbf{x} = (x_1,\\dots,x_k)$. Assume the logistic regression model, defined by the fundamental relation that the log-odds (logit) of the event is linear in the predictors: $$\\log\\left(\\frac{p(\\mathbf{x})}{1 - p(\\mathbf{x})}\\right) = \\beta_0 + \\sum_{j=1}^{k} \\beta_j x_j,$$ where $\\beta_0$ is an intercept and $(\\beta_1,\\dots,\\beta_k)$ are regression coefficients. Starting only from this definition and the definition of odds as the ratio of probability to its complement, derive how to compute both the fitted odds and the predicted probability for a given patient’s predictors and a specified coefficient vector. Then, consider a one-unit increase in a single predictor $x_j$ while holding all other predictors fixed and derive the implied multiplicative change in the odds (that is, the odds ratio), as a function of $\\beta_j$.\n\nYour task is to implement a program that, for each test case below, computes:\n- the fitted odds $O(\\mathbf{x})$ at the given predictor vector $\\mathbf{x}$ and coefficients,\n- the predicted probability $p(\\mathbf{x})$,\n- the odds ratio for a one-unit increase in the specified predictor index $j$ while holding other predictors fixed.\n\nAll computed outputs must be real numbers. There are no physical units in this problem. Your program must round each floating-point output to exactly six decimal places.\n\nTest suite:\n- Case 1: $\\beta = (-2.0, 0.6, 0.4)$, $\\mathbf{x} = (2.0, 1.5)$, $j = 1$.\n- Case 2: $\\beta = (1.0, 2.0, -0.5, 0.3)$, $\\mathbf{x} = (3.0, -2.0, 5.0)$, $j = 2$.\n- Case 3: $\\beta = (0.0, -3.0)$, $\\mathbf{x} = (2.0)$, $j = 1$.\n- Case 4: $\\beta = (0.0, 0.0, 0.0)$, $\\mathbf{x} = (10.0, -5.0)$, $j = 2$.\n\nInterpretation and indexing details:\n- For each case, $\\beta_0$ is the intercept, followed by $(\\beta_1,\\dots,\\beta_k)$ corresponding to the components of $\\mathbf{x}$ in order.\n- The index $j$ refers to the predictor coefficient $\\beta_j$ associated with $x_j$, using $1$-based indexing for predictors (not counting the intercept).\n\nFinal output format:\n- Your program should produce a single line of output containing the results for all test cases as a comma-separated list of lists, enclosed in square brackets.\n- For each case, output a list of the form $[O(\\mathbf{x}), p(\\mathbf{x}), \\text{OR}_j]$, where $\\text{OR}_j$ is the odds ratio for a one-unit increase in $x_j$.\n- Each number must be rounded to exactly six decimal places.\n- Example format (illustrative only): $[[a_{11},a_{12},a_{13}],[a_{21},a_{22},a_{23}],\\dots]$ where each $a_{mn}$ is a float with six decimal places.\n\nYour program must run without any user input or external files and must compute and print the results for the four specified cases only, in the exact format described above.",
            "solution": "The problem statement is evaluated as valid. It is scientifically grounded in the established theory of generalized linear models, specifically logistic regression. The problem is well-posed, with all necessary data and definitions provided to derive a unique, meaningful solution for each test case. The language is objective and precise.\n\nThe task requires the derivation and computation of three key quantities in a logistic regression model: the fitted odds, the predicted probability, and the odds ratio. We begin from the fundamental definition of the model.\n\nThe logistic regression model posits a linear relationship between the predictors $\\mathbf{x} = (x_1, \\dots, x_k)$ and the log-odds of the outcome probability $p(\\mathbf{x}) = \\mathbb{P}(Y=1 \\mid \\mathbf{x})$. This relationship is expressed as:\n$$\n\\log\\left(\\frac{p(\\mathbf{x})}{1 - p(\\mathbf{x})}\\right) = \\beta_0 + \\sum_{j=1}^{k} \\beta_j x_j\n$$\nwhere $\\beta_0$ is the intercept and $\\boldsymbol{\\beta} = (\\beta_1, \\dots, \\beta_k)$ is the vector of predictor coefficients. For convenience, we define the linear predictor, often denoted by $\\eta(\\mathbf{x})$, as the right-hand side of this equation:\n$$\n\\eta(\\mathbf{x}) = \\beta_0 + \\sum_{j=1}^{k} \\beta_j x_j = \\beta_0 + \\boldsymbol{\\beta}^T \\mathbf{x}\n$$\n\n**1. Derivation of Fitted Odds, $O(\\mathbf{x})$**\n\nThe odds of an event are defined as the ratio of the probability of the event occurring to the probability of it not occurring.\n$$\nO(\\mathbf{x}) = \\frac{p(\\mathbf{x})}{1 - p(\\mathbf{x})}\n$$\nBy substituting this definition into the fundamental model equation, we have:\n$$\n\\log(O(\\mathbf{x})) = \\eta(\\mathbf{x})\n$$\nTo solve for the odds, $O(\\mathbf{x})$, we exponentiate both sides of this equation, using the fact that the exponential function is the inverse of the natural logarithm:\n$$\nO(\\mathbf{x}) = \\exp(\\log(O(\\mathbf{x}))) = \\exp(\\eta(\\mathbf{x}))\n$$\nThus, the fitted odds are calculated by exponentiating the linear predictor.\n$$\nO(\\mathbf{x}) = \\exp\\left(\\beta_0 + \\sum_{j=1}^{k} \\beta_j x_j\\right)\n$$\n\n**2. Derivation of Predicted Probability, $p(\\mathbf{x})$**\n\nTo find the predicted probability $p(\\mathbf{x})$, we start with the definition of odds, $O(\\mathbf{x}) = \\frac{p(\\mathbf{x})}{1 - p(\\mathbf{x})}$, and solve for $p(\\mathbf{x})$ algebraically.\n$$\n\\begin{aligned}\nO(\\mathbf{x}) (1 - p(\\mathbf{x})) &= p(\\mathbf{x}) \\\\\nO(\\mathbf{x}) - O(\\mathbf{x}) p(\\mathbf{x}) &= p(\\mathbf{x}) \\\\\nO(\\mathbf{x}) &= p(\\mathbf{x}) + O(\\mathbf{x}) p(\\mathbf{x}) \\\\\nO(\\mathbf{x}) &= p(\\mathbf{x}) (1 + O(\\mathbf{x})) \\\\\np(\\mathbf{x}) &= \\frac{O(\\mathbf{x})}{1 + O(\\mathbf{x})}\n\\end{aligned}\n$$\nSubstituting our derived expression for $O(\\mathbf{x}) = \\exp(\\eta(\\mathbf{x}))$, we obtain the predicted probability in terms of the linear predictor:\n$$\np(\\mathbf{x}) = \\frac{\\exp(\\eta(\\mathbf{x}))}{1 + \\exp(\\eta(\\mathbf{x}))}\n$$\nThis function is commonly known as the logistic function or sigmoid function. An equivalent and often more numerically stable form is obtained by dividing the numerator and denominator by $\\exp(\\eta(\\mathbf{x}))$:\n$$\np(\\mathbf{x}) = \\frac{1}{\\exp(-\\eta(\\mathbf{x})) + 1}\n$$\n\n**3. Derivation of the Odds Ratio, $\\text{OR}_j$**\n\nThe odds ratio ($OR$) for a predictor $x_j$ is the factor by which the odds change for a one-unit increase in $x_j$, holding all other predictors constant. Let $\\mathbf{x}$ be the original predictor vector $(x_1, \\dots, x_j, \\dots, x_k)$, and let $\\mathbf{x}'$ be the vector with $x_j$ incremented by one: $(x_1, \\dots, x_j + 1, \\dots, x_k)$.\n\nThe odds at $\\mathbf{x}$ are:\n$$\nO(\\mathbf{x}) = \\exp\\left(\\beta_0 + \\sum_{i=1}^{k} \\beta_i x_i\\right)\n$$\nThe odds at $\\mathbf{x}'$ are:\n$$\nO(\\mathbf{x}') = \\exp\\left(\\beta_0 + \\sum_{i \\neq j} \\beta_i x_i + \\beta_j(x_j + 1)\\right) = \\exp\\left(\\beta_0 + \\sum_{i=1}^{k} \\beta_i x_i + \\beta_j\\right)\n$$\nThe odds ratio, $\\text{OR}_j$, is the ratio of these two odds:\n$$\n\\text{OR}_j = \\frac{O(\\mathbf{x}')}{O(\\mathbf{x})} = \\frac{\\exp\\left(\\beta_0 + \\sum_{i=1}^{k} \\beta_i x_i + \\beta_j\\right)}{\\exp\\left(\\beta_0 + \\sum_{i=1}^{k} \\beta_i x_i\\right)}\n$$\nUsing the property of exponents $\\frac{e^{a+b}}{e^a} = e^b$, we can simplify the expression:\n$$\n\\text{OR}_j = \\exp\\left(\\left(\\beta_0 + \\sum_{i=1}^{k} \\beta_i x_i + \\beta_j\\right) - \\left(\\beta_0 + \\sum_{i=1}^{k} \\beta_i x_i\\right)\\right) = \\exp(\\beta_j)\n$$\nThis is a key result: the odds ratio for a one-unit increase in predictor $x_j$ is simply the exponential of its corresponding coefficient, $\\beta_j$. It is constant and does not depend on the values of any of the predictors $\\mathbf{x}$.\n\n**Computational Algorithm**\n\nFor each test case, specified by a coefficient vector $\\boldsymbol{\\beta}_{\\text{full}} = (\\beta_0, \\beta_1, \\dots, \\beta_k)$, a predictor vector $\\mathbf{x} = (x_1, \\dots, x_k)$, and a predictor index $j$, the algorithm is as follows:\n1.  Calculate the linear predictor: $\\eta(\\mathbf{x}) = \\beta_0 + \\sum_{i=1}^{k} \\beta_i x_i$. This can be computed efficiently using a dot product of the predictor coefficients $(\\beta_1, \\dots, \\beta_k)$ with $\\mathbf{x}$, and adding the intercept $\\beta_0$.\n2.  Calculate the fitted odds: $O(\\mathbf{x}) = \\exp(\\eta(\\mathbf{x}))$.\n3.  Calculate the predicted probability: $p(\\mathbf{x}) = \\frac{O(\\mathbf{x})}{1 + O(\\mathbf{x})}$.\n4.  Calculate the odds ratio: $\\text{OR}_j = \\exp(\\beta_j)$. The problem uses $1$-based indexing for $j$, so $\\beta_j$ refers to the coefficient of $x_j$. When using $0$-indexed arrays, this would be the element at index $j$ of the full coefficient vector.\n5.  Round each of the three results, $O(\\mathbf{x})$, $p(\\mathbf{x})$, and $\\text{OR}_j$, to exactly six decimal places for the final output.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes fitted odds, predicted probability, and an odds ratio for a logistic\n    regression model across several test cases.\n    \"\"\"\n\n    # Test cases defined as tuples of (beta_vector, x_vector, j_index).\n    # beta_vector is (beta_0, beta_1, ..., beta_k).\n    # x_vector is (x_1, ..., x_k).\n    # j_index is 1-based for the predictor of interest.\n    test_cases = [\n        # Case 1: beta = (-2.0, 0.6, 0.4), x = (2.0, 1.5), j = 1\n        ((-2.0, 0.6, 0.4), (2.0, 1.5), 1),\n        # Case 2: beta = (1.0, 2.0, -0.5, 0.3), x = (3.0, -2.0, 5.0), j = 2\n        ((1.0, 2.0, -0.5, 0.3), (3.0, -2.0, 5.0), 2),\n        # Case 3: beta = (0.0, -3.0), x = (2.0), j = 1\n        ((0.0, -3.0), (2.0,), 1),\n        # Case 4: beta = (0.0, 0.0, 0.0), x = (10.0, -5.0), j = 2\n        ((0.0, 0.0, 0.0), (10.0, -5.0), 2)\n    ]\n\n    all_results = []\n    for case in test_cases:\n        beta_tuple, x_tuple, j = case\n\n        # Convert tuples to numpy arrays for vectorized operations\n        beta = np.array(beta_tuple)\n        x = np.array(x_tuple)\n\n        # Decompose beta into intercept and predictor coefficients\n        beta_0 = beta[0]\n        beta_predictors = beta[1:]\n\n        # 1. Calculate the linear predictor (eta)\n        # eta = beta_0 + sum(beta_i * x_i)\n        eta = beta_0 + np.dot(beta_predictors, x)\n\n        # 2. Calculate the fitted odds\n        # O(x) = exp(eta)\n        odds = np.exp(eta)\n\n        # 3. Calculate the predicted probability\n        # p(x) = O(x) / (1 + O(x))\n        prob = odds / (1 + odds)\n\n        # 4. Calculate the odds ratio for a one-unit increase in x_j\n        # OR_j = exp(beta_j)\n        # The problem uses 1-based indexing for j, which corresponds to\n        # the coefficient beta[j] in our 0-indexed beta array.\n        odds_ratio = np.exp(beta[j])\n\n        # Store the results for this case. The problem requires rounding to\n        # exactly six decimal places, which we achieve using f-string formatting.\n        all_results.append([odds, prob, odds_ratio])\n\n    # Format the final output string as a list of lists of numbers\n    # with each number formatted to exactly six decimal places.\n    formatted_cases = []\n    for result_set in all_results:\n        o, p, or_j = result_set\n        formatted_cases.append(f\"[{o:.6f},{p:.6f},{or_j:.6f}]\")\n\n    print(f\"[{','.join(formatted_cases)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While interpreting a model's coefficients is crucial, the validity of that interpretation hinges on the model being correctly specified. A common pitfall is assuming a linear relationship between a continuous predictor and the log-odds, which may not capture the true underlying biological effect. This practice introduces the Multivariable Fractional Polynomials (MFP) procedure, a powerful and systematic method for selecting an appropriate functional form for continuous variables from a flexible family of power transformations, thereby improving model fit and accuracy .",
            "id": "4974006",
            "problem": "You are tasked with implementing a search procedure for Multivariable Fractional Polynomials (MFP) applied to a single continuous medical predictor, serum creatinine, in a binary outcome logistic regression. The goal is to select the best Fractional Polynomial (FP) transformation by the deviance criterion and present the resulting functional form quantifiably. The modeling context is advanced statistical modeling in medicine. You must produce a complete, runnable program that implements the following specification.\n\nFundamental base and definitions to use:\n- Logistic regression with binary outcomes: given observations $\\{(y_i, x_i)\\}_{i=1}^n$ where $y_i \\in \\{0,1\\}$ and $x_i \\in \\mathbb{R}_{>0}$ represents serum creatinine measured in $\\mathrm{mg}/\\mathrm{dL}$, assume the model\n$$\\operatorname{logit}(\\pi_i) = \\eta_i = \\beta_0 + \\beta_1 f_1(x_i) + \\beta_2 f_2(x_i),$$\nwhere for degree $1$ models the coefficient $\\beta_2$ and the term $f_2(x_i)$ are omitted, $\\pi_i = \\Pr(y_i=1 \\mid x_i)$, and $\\operatorname{logit}(\\pi) = \\log\\left(\\frac{\\pi}{1-\\pi}\\right)$.\n- Maximum Likelihood Estimation (MLE): given a design matrix for a chosen transformation, estimate the coefficient vector $\\boldsymbol{\\beta}$ by maximizing the log-likelihood\n$$\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^n \\left(y_i \\eta_i - \\log\\left(1 + e^{\\eta_i}\\right)\\right),$$\nwith $\\eta_i$ as above.\n- Deviance: for a fitted model with MLE $\\hat{\\boldsymbol{\\beta}}$, compute the deviance\n$$D = -2\\,\\ell(\\hat{\\boldsymbol{\\beta}}).$$\nSelect the FP transformation that minimizes $D$.\n\nFractional Polynomial (FP) transformation family:\n- Candidate powers set\n$$\\mathcal{P} = \\{-2,-1,-0.5,0,0.5,1,2,3\\}.$$\n- Degree $1$ FP: for $p \\in \\mathcal{P}$,\n$$f_1(x;p) = \\begin{cases}\nx^p, & p \\neq 0, \\\\\n\\log(x), & p = 0.\n\\end{cases}$$\n- Degree $2$ FP: for $(p_1, p_2) \\in \\mathcal{P} \\times \\mathcal{P}$,\n$$\n(f_1, f_2) =\n\\begin{cases}\n\\left(x^{p_1}, x^{p_2}\\right), & p_1 \\neq 0, p_2 \\neq 0, p_1 \\neq p_2, \\\\\n\\left(\\log(x), x^{p_2}\\right), & p_1 = 0, p_2 \\neq 0, \\\\\n\\left(x^{p_1}, \\log(x)\\right), & p_1 \\neq 0, p_2 = 0, \\\\\n\\left(\\log(x), \\log(x)^2\\right), & p_1 = p_2 = 0, \\\\\n\\left(x^p, x^p \\log(x)\\right), & p_1 = p_2 = p \\neq 0.\n\\end{cases}\n$$\nTo improve numerical stability, center each transformation column before fitting: for any column $g(x_i)$ in the design, use the centered version $\\tilde{g}(x_i) = g(x_i) - \\frac{1}{n}\\sum_{j=1}^n g(x_j)$.\n\nYour program must:\n- For each test case, enumerate all candidate FP degree $1$ and degree $2$ transformations defined above, fit a logistic regression by MLE, compute the deviance $D$, and select the transformation that minimizes $D$.\n- Report the selected FP degree $d \\in \\{1,2\\}$, the selected powers $(p_1, p_2)$ where $p_2$ is set to the special floating value $\\mathrm{NaN}$ when $d=1$, and the minimized deviance $D$.\n\nTest suite specification (all random generation must be deterministic and reproducible using the stated seeds):\n- Test case $1$ (happy path, repeated power optimal): number of patients $n = 200$; generate $x_i \\sim \\mathrm{Uniform}(0.6, 2.0)$ in $\\mathrm{mg}/\\mathrm{dL}$; true data-generating logit\n$$\\operatorname{logit}(\\pi_i) = -1 + 2\\,\\log(x_i) - 1\\,\\log(x_i)^2;$$\nthen $y_i \\sim \\mathrm{Bernoulli}(\\pi_i)$. Use a pseudo-random number generator seed $123$.\n- Test case $2$ (linear optimal): number of patients $n = 150$; generate $x_i \\sim \\mathrm{Uniform}(0.5, 1.5)$ in $\\mathrm{mg}/\\mathrm{dL}$; true data-generating logit\n$$\\operatorname{logit}(\\pi_i) = -5 + 4\\,x_i;$$\nthen $y_i \\sim \\mathrm{Bernoulli}(\\pi_i)$. Use a pseudo-random number generator seed $321$.\n- Test case $3$ (inverse optimal, boundary of negative power): number of patients $n = 120$; generate $x_i \\sim \\mathrm{Uniform}(0.8, 2.5)$ in $\\mathrm{mg}/\\mathrm{dL}$; true data-generating logit\n$$\\operatorname{logit}(\\pi_i) = -2 + \\frac{3}{x_i};$$\nthen $y_i \\sim \\mathrm{Bernoulli}(\\pi_i)$. Use a pseudo-random number generator seed $222$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must be a sublist of the form $[d, p_1, p_2, D]$, where $d$ is an integer, $p_1$ and $p_2$ are real numbers drawn from $\\mathcal{P}$ (with $p_2$ equal to the floating $\\mathrm{NaN}$ when $d=1$), and $D$ is a real number. For example,\n$$\\texttt{[[2,0.0,0.0,123.456789],[1,1.0,NaN,234.567890],[1,-1.0,NaN,345.678901]]}.$$\n\nConstraints and scientific realism:\n- All $x_i$ must remain strictly positive due to the $\\log(x)$ terms; use the specified uniform ranges.\n- The predictor unit is $\\mathrm{mg}/\\mathrm{dL}$; no unit conversion is required in the output because the deviance $D$ is unitless.\n- No external input or files are permitted; the program must be fully self-contained.",
            "solution": "The user-provided problem statement has been critically validated and is determined to be **valid**. It is scientifically grounded, well-posed, objective, and presents a non-trivial computational statistics task that is representative of genuine research problems in biostatistics. The problem consists of implementing the Multivariable Fractional Polynomials (MFP) method to select the optimal functional form for a single continuous predictor in a logistic regression model. This will be accomplished by performing an exhaustive search over a predefined set of candidate transformations and selecting the one that minimizes the deviance.\n\n### 1. Theoretical Framework: Logistic Regression and Model Selection\n\nThe problem is situated within the framework of generalized linear models, specifically binary logistic regression. We are given a set of observations $\\{(y_i, x_i)\\}_{i=1}^n$, where $y_i \\in \\{0, 1\\}$ is a binary outcome and $x_i \\in \\mathbb{R}_{>0}$ is a continuous predictor (serum creatinine). The logistic regression model links the predictor to the probability of the outcome, $\\pi_i = \\Pr(y_i=1 \\mid x_i)$, via the logit link function:\n$$ \\operatorname{logit}(\\pi_i) = \\log\\left(\\frac{\\pi_i}{1-\\pi_i}\\right) = \\eta_i $$\nHere, $\\eta_i$ is the linear predictor. The core of the MFP procedure is to find the best-fitting functional form for $x_i$ in this linear predictor. The general form of the model we will fit is:\n$$ \\eta_i = \\beta_0 + \\beta_1 f_1(x_i) + \\beta_2 f_2(x_i) $$\nwhere $f_1$ and $f_2$ are transformations of $x_i$ drawn from the Fractional Polynomial family. For degree $d=1$ models, the term $\\beta_2 f_2(x_i)$ is omitted.\n\nThe model parameters, $\\boldsymbol{\\beta}$, are estimated using Maximum Likelihood Estimation (MLE). This involves maximizing the log-likelihood function for Bernoulli-distributed outcomes:\n$$ \\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^n \\left( y_i \\eta_i - \\log(1 + e^{\\eta_i}) \\right) $$\nThis function is globally concave, guaranteeing that numerical optimization methods can reliably find the unique maximum. The model's goodness-of-fit is assessed using the deviance, defined as $D = -2\\ell(\\hat{\\boldsymbol{\\beta}})$, where $\\hat{\\boldsymbol{\\beta}}$ is the maximum likelihood estimate of $\\boldsymbol{\\beta}$. The goal of the MFP search is to identify the functional form $(f_1, f_2)$ that results in the minimum deviance $D$.\n\n### 2. The Candidate Model Space: Fractional Polynomials\n\nFractional Polynomials provide a rich, yet parsimonious, class of functions for modeling nonlinear relationships. The search is conducted over a pre-specified set of candidate powers, $\\mathcal{P} = \\{-2, -1, -0.5, 0, 0.5, 1, 2, 3\\}$. The function associated with power $p=0$ is defined as $\\log(x)$ as it arises from the limit of $(x^p - 1)/p$ as $p \\to 0$.\n\nThe candidate models are categorized by their degree, $d$:\n\n**Degree 1 (FP1):** These models use a single transformation.\n$$ \\eta_i = \\beta_0 + \\beta_1 f_1(x_i; p) \\quad \\text{where} \\quad f_1(x;p) = \\begin{cases} x^p & p \\neq 0 \\\\ \\log(x) & p = 0 \\end{cases} $$\nWith $|\\mathcal{P}|=8$ powers, there are $8$ candidate FP1 models.\n\n**Degree 2 (FP2):** These models use two transformations, with powers $(p_1, p_2)$ from $\\mathcal{P}$. By convention, we take $p_1 \\le p_2$.\n$$ \\eta_i = \\beta_0 + \\beta_1 f_1(x_i; p_1, p_2) + \\beta_2 f_2(x_i; p_1, p_2) $$\nThe functional forms depend on whether the powers are distinct or repeated:\n- **Distinct Powers ($p_1 < p_2$):** The transformations are $(f_1, f_2) = (x^{p_1}, x^{p_2})$, with the $\\log(x)$ modification if a power is $0$. There are $\\binom{8}{2} = 28$ such models.\n- **Repeated Powers ($p_1 = p_2 = p$):** To avoid collinearity, the second term is modified. This is known as a \"repeated powers\" model.\n$$ (f_1, f_2) = \\begin{cases} (x^p, x^p \\log(x)) & p \\neq 0 \\\\ (\\log(x), \\log(x)^2) & p = 0 \\end{cases} $$\nThere are $8$ such models.\n\nIn total, the search space consists of $8$ (FP1) + $28$ (distinct FP2) + $8$ (repeated FP2) = $44$ candidate models for each test-case dataset.\n\n### 3. Algorithmic Procedure\n\nThe implementation will follow a systematic, exhaustive search algorithm for each test case.\n\n**Step 1: Data Generation:** For each test case, generate $n$ observations $(x_i, y_i)$ according to the specified data-generating process. The predictor $x_i$ is drawn from a uniform distribution, the linear predictor $\\eta_i$ is calculated using the true model, the probability $\\pi_i$ is found via the inverse logit function $\\pi_i = (1 + e^{-\\eta_i})^{-1}$, and the outcome $y_i$ is drawn from a Bernoulli distribution with parameter $\\pi_i$.\n\n**Step 2: Model Search and Evaluation:**\nThe core of the algorithm is a loop over all $44$ candidate FP models.\n- For each model, defined by its degree $d$ and power(s) $(p_1, p_2)$:\n    a. **Construct Design Matrix:** Generate the transformed predictor columns (e.g., $x_i^{-0.5}$, $\\log(x_i)$).\n    b. **Center Predictors:** For numerical stability, center each transformed column by subtracting its mean. This creates $\\tilde{f}_j(x_i) = f_j(x_i) - \\bar{f}_j$.\n    c. **Form Full Design Matrix:** Combine a column of ones (for the intercept $\\beta_0$) with the one or two centered transformed predictor columns to form the design matrix $X$.\n    d. **Fit Model:** Perform MLE to find the coefficients $\\hat{\\boldsymbol{\\beta}}$ that maximize the log-likelihood. This is a numerical optimization problem. We will minimize the negative log-likelihood function, $F(\\boldsymbol{\\beta}) = -\\ell(\\boldsymbol{\\beta})$, using a quasi-Newton method (L-BFGS-B) from `scipy.optimize`. This requires providing the objective function $F(\\boldsymbol{\\beta})$ and its gradient (Jacobian):\n    $$ \\nabla_{\\boldsymbol{\\beta}} F(\\boldsymbol{\\beta}) = X^T(\\boldsymbol{\\pi} - \\mathbf{y}), \\quad \\text{where } \\boldsymbol{\\pi} = (1 + e^{-X\\boldsymbol{\\beta}})^{-1} $$\n    e. **Compute Deviance:** The minimized value of the objective function is $-\\ell(\\hat{\\boldsymbol{\\beta}})$. The deviance is $D = -2\\ell(\\hat{\\boldsymbol{\\beta}}) = 2 \\times \\min(F(\\boldsymbol{\\beta}))$.\n    f. **Compare and Store:** Keep track of the model (degree, powers) that yields the minimum deviance found so far.\n\n**Step 3: Report Results:** After iterating through all $44$ models, the one with the lowest deviance is the \"best\" model according to the MFP criterion. For each test case, report its degree $d$, powers $(p_1, p_2)$ (with $p_2$ as NaN for $d=1$), and its minimized deviance $D$.\n\nThis comprehensive procedure ensures that we correctly implement the specified statistical methodology and arrive at a reproducible, verifiable result for each defined test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport scipy\nfrom scipy.optimize import minimize\nfrom scipy.special import expit\nimport itertools\n\ndef solve():\n    \"\"\"\n    Main function to run the MFP search procedure on all test cases and print the results.\n    \"\"\"\n    \n    POWERS = [-2.0, -1.0, -0.5, 0.0, 0.5, 1.0, 2.0, 3.0]\n\n    test_cases = [\n        {\n            \"n\": 200, \"x_range\": (0.6, 2.0), \"seed\": 123,\n            \"true_logit_func\": lambda x: -1 + 2 * np.log(x) - 1 * (np.log(x)**2)\n        },\n        {\n            \"n\": 150, \"x_range\": (0.5, 1.5), \"seed\": 321,\n            \"true_logit_func\": lambda x: -5 + 4 * x\n        },\n        {\n            \"n\": 120, \"x_range\": (0.8, 2.5), \"seed\": 222,\n            \"true_logit_func\": lambda x: -2 + 3 / x\n        }\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        # 1. Generate data\n        rng = np.random.default_rng(case[\"seed\"])\n        x = rng.uniform(case[\"x_range\"][0], case[\"x_range\"][1], case[\"n\"])\n        \n        true_eta = case[\"true_logit_func\"](x)\n        pi = expit(true_eta)\n        y = rng.binomial(1, pi, size=case[\"n\"])\n\n        best_model = {\n            \"d\": 0, \"p1\": np.nan, \"p2\": np.nan, \"deviance\": np.inf\n        }\n        \n        # 2. MFP Search\n        \n        # Null model (intercept only) for initial beta guess\n        p_avg = np.mean(y)\n        beta_init_null = [np.log(p_avg / (1 - p_avg))]\n\n        # Degree 1 models\n        for p1 in POWERS:\n            # Create transformed predictor\n            if p1 == 0:\n                t1 = np.log(x)\n            else:\n                t1 = x**p1\n            \n            # Center and form design matrix\n            t1_centered = t1 - np.mean(t1)\n            X = np.c_[np.ones(case[\"n\"]), t1_centered]\n            \n            # Fit model and get deviance\n            beta_init = np.append(beta_init_null, 0)\n            res = fit_logistic_mle(X, y, beta_init)\n            \n            if res is not None and res['deviance'] < best_model[\"deviance\"]:\n                best_model = {\"d\": 1, \"p1\": p1, \"p2\": np.nan, \"deviance\": res['deviance']}\n\n        # Degree 2 models\n        for p1, p2 in itertools.combinations_with_replacement(POWERS, 2):\n            # Create transformed predictors\n            if p1 == p2:\n                if p1 == 0: # (log(x), log(x)^2)\n                    t1 = np.log(x)\n                    t2 = np.log(x)**2\n                else: # (x^p, x^p*log(x))\n                    t1 = x**p1\n                    t2 = t1 * np.log(x)\n            else: # p1 < p2\n                if p1 == 0:\n                    t1 = np.log(x)\n                else:\n                    t1 = x**p1\n                \n                # p2 will be > p1. If p2 is 0, p1 must be negative.\n                if p2 == 0: \n                    t2 = np.log(x)\n                else:\n                    t2 = x**p2\n            \n            # Center and form design matrix\n            t1_centered = t1 - np.mean(t1)\n            t2_centered = t2 - np.mean(t2)\n            X = np.c_[np.ones(case[\"n\"]), t1_centered, t2_centered]\n\n            # Fit model and get deviance\n            beta_init = np.append(beta_init_null, [0, 0])\n            res = fit_logistic_mle(X, y, beta_init)\n            \n            if res is not None and res['deviance'] < best_model[\"deviance\"]:\n                best_model = {\"d\": 2, \"p1\": p1, \"p2\": p2, \"deviance\": res['deviance']}\n        \n        all_results.append([best_model['d'], best_model['p1'], best_model['p2'], best_model['deviance']])\n\n    # 3. Format and print output\n    print(format_results(all_results))\n\ndef fit_logistic_mle(X, y, beta_init):\n    \"\"\"\n    Fits a logistic regression model using MLE.\n    \n    Args:\n        X (np.ndarray): Design matrix (n_samples, n_features).\n        y (np.ndarray): Response vector (n_samples,).\n        beta_init (np.ndarray): Initial guess for coefficients.\n    \n    Returns:\n        A dictionary with 'beta' and 'deviance', or None on failure.\n    \"\"\"\n    def neg_log_likelihood(beta, X, y):\n        eta = X @ beta\n        # Add clipping for numerical stability to avoid overflow in exp\n        eta = np.clip(eta, -30, 30)\n        p = expit(eta)\n        # Add clipping for stability to avoid log(0)\n        p = np.clip(p, 1e-9, 1 - 1e-9)\n        return -np.sum(y * np.log(p) + (1 - y) * np.log(1 - p))\n\n    def jacobian(beta, X, y):\n        eta = X @ beta\n        eta = np.clip(eta, -30, 30)\n        p = expit(eta)\n        return X.T @ (p - y)\n\n    try:\n        opt_res = minimize(\n            fun=neg_log_likelihood,\n            x0=beta_init,\n            args=(X, y),\n            method='L-BFGS-B',\n            jac=jacobian,\n            options={'maxiter': 1000}\n        )\n        if opt_res.success:\n            deviance = 2 * opt_res.fun\n            return {\"beta\": opt_res.x, \"deviance\": deviance}\n        return None\n    except (ValueError, np.linalg.LinAlgError):\n        return None\n\ndef format_results(results_list):\n    \"\"\"\n    Formats the list of results into the required string format.\n    Example: [[2,0.0,0.0,123.456789],[1,1.0,NaN,234.567890]]\n    \"\"\"\n    def format_single_result(res):\n        d, p1, p2, D = res\n        p1_str = str(float(p1))\n        p2_str = 'NaN' if np.isnan(p2) else str(float(p2))\n        d_str = f\"{D:.15g}\" # Use 'g' for flexible precision without trailing zeros\n        return f\"[{d},{p1_str},{p2_str},{d_str}]\"\n    \n    return f\"[{','.join(format_single_result(r) for r in results_list)}]\"\n\n# Run the solver\nsolve()\n\n```"
        },
        {
            "introduction": "Even a well-specified model can fail if the data exhibits certain challenging structures. One such issue is 'separation', where a predictor or combination of predictors perfectly or nearly perfectly distinguishes between outcomes, causing standard maximum likelihood estimates to become unstable or infinite. This hands-on exercise demonstrates how to overcome this problem by implementing a penalized estimation method (ridge regression), which introduces a penalty term to stabilize the coefficients and produce robust, finite estimates even in the face of separation .",
            "id": "4974092",
            "problem": "Consider a binary outcome model for medical event occurrence using logistic regression, where each individual $i$ provides a feature vector $\\boldsymbol{x}_i \\in \\mathbb{R}^p$ and an outcome $y_i \\in \\{0,1\\}$. The fundamental base consists of: (i) the Bernoulli data model with probability $\\mathbb{P}(Y_i = 1 \\mid \\boldsymbol{x}_i) = \\mu_i$, (ii) the logistic link $\\mu_i = \\left(1 + \\exp\\left(-\\eta_i\\right)\\right)^{-1}$ with linear predictor $\\eta_i = \\boldsymbol{x}_i^\\top \\boldsymbol{\\beta}$, and (iii) maximum likelihood estimation via the log-likelihood of independent Bernoulli observations. In medical datasets, complete or quasi separation occurs when there exists a linear combination of predictors that perfectly (or almost perfectly) classifies outcomes, making unpenalized maximum likelihood estimates unstable or non-finite.\n\nYour task is to implement a model building strategy for logistic regression that yields finite, stable estimates in the presence of separation by adding a Euclidean norm (L2) penalty to the coefficients. Specifically, estimate $\\boldsymbol{\\beta}$ by maximizing the penalized objective formed by adding a quadratic penalty in the coefficients to the Bernoulli log-likelihood, while not penalizing the intercept. Derive the algorithmic steps from the above fundamental base using the canonical iterative weighted framework for generalized linear models and solve the penalized normal equations at each iteration. Ensure numerical stability by appropriately handling extreme fitted values. Standardize the continuous predictor $x_{\\text{age}}$ to improve conditioning using $x_{\\text{age}}^{*} = \\left(x_{\\text{age}} - \\bar{x}_{\\text{age}}\\right)/10$, where $10$ is a scaling factor measured in years.\n\nImplement a complete, runnable program that, for the following test suite, fits the penalized logistic regression model and reports the coefficient estimates and fitted probabilities. The intercept must not be penalized. For each test case, output a list containing the estimated intercept and coefficients, followed by the minimum and maximum fitted probabilities on the training data, and finally the fitted probabilities for two specified new patients. All outputs must be real numbers. The final program output must be a single line containing a list of the per-test-case lists, enclosed in square brackets, with elements comma-separated.\n\nTest suite specification (each case uses an intercept and two predictors: $x_{\\text{age}}$ measured in years and $x_{\\text{bio}} \\in \\{0,1\\}$ indicating a binary biomarker):\n\n- Case $1$ (quasi separation, moderate penalty):\n  - Ages (years): $[55,64,47,38,70,59,53,61,49,45,72,50]$.\n  - Biomarker: $[1,1,1,0,1,0,1,1,0,0,1,0]$.\n  - Outcomes: $[1,1,1,0,1,0,1,0,0,0,1,0]$.\n  - Penalty parameter: $\\lambda = 1$.\n  - New patients to predict:\n    - Patient A: $x_{\\text{age}} = 60$, $x_{\\text{bio}} = 1$.\n    - Patient B: $x_{\\text{age}} = 50$, $x_{\\text{bio}} = 0$.\n\n- Case $2$ (complete separation, moderate penalty):\n  - Ages (years): $[40,42,45,50,52,55,60,62,65,70]$.\n  - Biomarker: $[0,0,0,0,1,1,1,1,1,1]$.\n  - Outcomes: $[0,0,0,0,1,1,1,1,1,1]$.\n  - Penalty parameter: $\\lambda = 1$.\n  - New patients to predict:\n    - Patient A: $x_{\\text{age}} = 60$, $x_{\\text{bio}} = 1$.\n    - Patient B: $x_{\\text{age}} = 50$, $x_{\\text{bio}} = 0$.\n\n- Case $3$ (extreme imbalance, near separation, light penalty):\n  - Ages (years): $[30,35,40,45,50,55,60,65,70,75,80,85]$.\n  - Biomarker: $[0,0,0,0,0,1,0,1,0,1,0,1]$.\n  - Outcomes: $[0,0,0,0,0,0,0,0,0,1,0,0]$.\n  - Penalty parameter: $\\lambda = 0.1$.\n  - New patients to predict:\n    - Patient A: $x_{\\text{age}} = 60$, $x_{\\text{bio}} = 1$.\n    - Patient B: $x_{\\text{age}} = 50$, $x_{\\text{bio}} = 0$.\n\n- Case $4$ (high shrinkage to assess stability under strong penalty):\n  - Use the same ages, biomarker, and outcomes as Case $1$.\n  - Penalty parameter: $\\lambda = 100$.\n  - New patients to predict:\n    - Patient A: $x_{\\text{age}} = 60$, $x_{\\text{bio}} = 1$.\n    - Patient B: $x_{\\text{age}} = 50$, $x_{\\text{bio}} = 0$.\n\nFor each case, build the design matrix with an intercept term $x_0 = 1$, the standardized age predictor $x_{\\text{age}}^{*}$, and the biomarker $x_{\\text{bio}}$. Fit the penalized model and compute:\n- The estimated intercept and coefficients $(\\hat{\\beta}_0, \\hat{\\beta}_{\\text{age}}, \\hat{\\beta}_{\\text{bio}})$ as finite real numbers.\n- The minimum and maximum fitted probabilities on the training data, each expressed as a decimal in $[0,1]$.\n- The fitted probabilities for Patient A and Patient B, each expressed as a decimal in $[0,1]$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list of seven floats: $[\\hat{\\beta}_0, \\hat{\\beta}_{\\text{age}}, \\hat{\\beta}_{\\text{bio}}, \\min(\\hat{\\mu}), \\max(\\hat{\\mu}), \\hat{\\mu}_{\\text{A}}, \\hat{\\mu}_{\\text{B}}]$.",
            "solution": "The problem requires the implementation of a penalized logistic regression model to ensure stable coefficient estimation in the presence of data separation. The solution is developed from first principles using the Iteratively Reweighted Least Squares (IRLS) algorithm, adapted for an L2 penalty (ridge regression).\n\n### 1. Model and Objective Function\n\nThe model for a binary outcome $y_i \\in \\{0, 1\\}$ given a predictor vector $\\boldsymbol{x}_i \\in \\mathbb{R}^p$ is specified by the logistic regression framework:\n1.  **Data Model**: The outcome $Y_i$ follows a Bernoulli distribution with success probability $\\mu_i = \\mathbb{P}(Y_i = 1 \\mid \\boldsymbol{x}_i)$.\n2.  **Systematic Component**: The predictors are linearly related to the log-odds of the outcome via a linear predictor $\\eta_i = \\boldsymbol{x}_i^\\top \\boldsymbol{\\beta}$, where $\\boldsymbol{\\beta} \\in \\mathbb{R}^p$ is the vector of coefficients. The design matrix $\\boldsymbol{X}$ is an $N \\times p$ matrix where row $i$ is $\\boldsymbol{x}_i^\\top$. For this problem, $p=3$ and $\\boldsymbol{x}_i = [1, x_{\\text{age}, i}^{*}, x_{\\text{bio}, i}]^\\top$.\n3.  **Link Function**: The logistic link function connects the probability $\\mu_i$ to the linear predictor $\\eta_i$: $\\eta_i = \\log\\left(\\frac{\\mu_i}{1 - \\mu_i}\\right)$. The inverse link function is $\\mu_i = \\frac{1}{1 + \\exp(-\\eta_i)}$.\n\nThe log-likelihood for $N$ independent observations is the sum of individual Bernoulli log-likelihoods:\n$$\n\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^N \\left[ y_i \\log(\\mu_i) + (1-y_i) \\log(1-\\mu_i) \\right]\n$$\nSubstituting $\\eta_i = \\boldsymbol{x}_i^\\top \\boldsymbol{\\beta}$ and the relationship between $\\mu_i$ and $\\eta_i$, this can be expressed as:\n$$\n\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^N \\left[ y_i (\\boldsymbol{x}_i^\\top \\boldsymbol{\\beta}) - \\log(1 + \\exp(\\boldsymbol{x}_i^\\top \\boldsymbol{\\beta})) \\right]\n$$\nTo address estimation instability from separation, we add an L2 penalty to the log-likelihood for all coefficients except the intercept $\\beta_0$. The penalized log-likelihood objective function to be maximized is:\n$$\nQ(\\boldsymbol{\\beta}) = \\ell(\\boldsymbol{\\beta}) - \\frac{\\lambda}{2} \\sum_{j=1}^{p-1} \\beta_j^2\n$$\nwhere $\\lambda > 0$ is the penalty parameter. The term $\\frac{1}{2}$ is a convention for mathematical convenience.\n\n### 2. Optimization via Penalized IRLS\n\nWe use a Newton-Raphson-based method to find the $\\boldsymbol{\\beta}$ that maximizes $Q(\\boldsymbol{\\beta})$. The update step is $\\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} - [\\nabla^2 Q(\\boldsymbol{\\beta}^{(t)})]^{-1} \\nabla Q(\\boldsymbol{\\beta}^{(t)})$.\n\n**Gradient Vector (Score):**\nThe gradient of the log-likelihood is $\\nabla \\ell(\\boldsymbol{\\beta}) = \\boldsymbol{X}^\\top (\\boldsymbol{y} - \\boldsymbol{\\mu})$, where $\\boldsymbol{y}$ is the vector of outcomes and $\\boldsymbol{\\mu}$ is the vector of probabilities. The gradient of the penalty term is $\\lambda \\boldsymbol{P} \\boldsymbol{\\beta}$, where $\\boldsymbol{P}$ is a diagonal matrix with $P_{00}=0$ and $P_{jj}=1$ for $j \\ge 1$.\nThe gradient of the penalized objective is:\n$$\n\\nabla Q(\\boldsymbol{\\beta}) = \\boldsymbol{X}^\\top (\\boldsymbol{y} - \\boldsymbol{\\mu}) - \\lambda \\boldsymbol{P} \\boldsymbol{\\beta}\n$$\n\n**Hessian Matrix:**\nThe Hessian of the log-likelihood is $\\nabla^2 \\ell(\\boldsymbol{\\beta}) = -\\boldsymbol{X}^\\top \\boldsymbol{W} \\boldsymbol{X}$, where $\\boldsymbol{W}$ is a diagonal matrix with elements $W_{ii} = \\mu_i(1-\\mu_i)$, the variance of the Bernoulli response. The Hessian of the penalty term is $-\\lambda \\boldsymbol{P}$.\nThe Hessian of the penalized objective is:\n$$\n\\nabla^2 Q(\\boldsymbol{\\beta}) = -\\boldsymbol{X}^\\top \\boldsymbol{W} \\boldsymbol{X} - \\lambda \\boldsymbol{P}\n$$\n\n**Newton-Raphson Update:**\nSubstituting the gradient and Hessian into the update rule gives:\n$$\n\\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} - [-\\boldsymbol{X}^\\top \\boldsymbol{W}^{(t)} \\boldsymbol{X} - \\lambda \\boldsymbol{P}]^{-1} [\\boldsymbol{X}^\\top (\\boldsymbol{y} - \\boldsymbol{\\mu}^{(t)}) - \\lambda \\boldsymbol{P} \\boldsymbol{\\beta}^{(t)}]\n$$\n$$\n\\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} + [\\boldsymbol{X}^\\top \\boldsymbol{W}^{(t)} \\boldsymbol{X} + \\lambda \\boldsymbol{P}]^{-1} [\\boldsymbol{X}^\\top (\\boldsymbol{y} - \\boldsymbol{\\mu}^{(t)}) - \\lambda \\boldsymbol{P} \\boldsymbol{\\beta}^{(t)}]\n$$\nRearranging this equation reveals the IRLS structure. Let's solve for $\\boldsymbol{\\beta}^{(t+1)}$:\n$$\n(\\boldsymbol{X}^\\top \\boldsymbol{W}^{(t)} \\boldsymbol{X} + \\lambda \\boldsymbol{P}) \\boldsymbol{\\beta}^{(t+1)} = (\\boldsymbol{X}^\\top \\boldsymbol{W}^{(t)} \\boldsymbol{X} + \\lambda \\boldsymbol{P}) \\boldsymbol{\\beta}^{(t)} + \\boldsymbol{X}^\\top (\\boldsymbol{y} - \\boldsymbol{\\mu}^{(t)}) - \\lambda \\boldsymbol{P} \\boldsymbol{\\beta}^{(t)}\n$$\n$$\n(\\boldsymbol{X}^\\top \\boldsymbol{W}^{(t)} \\boldsymbol{X} + \\lambda \\boldsymbol{P}) \\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{X}^\\top \\boldsymbol{W}^{(t)} \\boldsymbol{X} \\boldsymbol{\\beta}^{(t)} + \\boldsymbol{X}^\\top (\\boldsymbol{y} - \\boldsymbol{\\mu}^{(t)})\n$$\nThe right-hand side can be written as $\\boldsymbol{X}^\\top \\boldsymbol{W}^{(t)} \\boldsymbol{z}^{(t)}$, where $\\boldsymbol{z}^{(t)}$ is the working response vector:\n$$\n\\boldsymbol{z}^{(t)} = \\boldsymbol{X} \\boldsymbol{\\beta}^{(t)} + (\\boldsymbol{W}^{(t)})^{-1} (\\boldsymbol{y} - \\boldsymbol{\\mu}^{(t)}) = \\boldsymbol{\\eta}^{(t)} + \\frac{\\boldsymbol{y} - \\boldsymbol{\\mu}^{(t)}}{\\boldsymbol{\\mu}^{(t)}(1-\\boldsymbol{\\mu}^{(t)})}\n$$\nThe update for $\\boldsymbol{\\beta}^{(t+1)}$ is obtained by solving the following system of linear equations, which are the normal equations for a weighted ridge regression:\n$$\n(\\boldsymbol{X}^\\top \\boldsymbol{W}^{(t)} \\boldsymbol{X} + \\lambda \\boldsymbol{P}) \\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{X}^\\top \\boldsymbol{W}^{(t)} \\boldsymbol{z}^{(t)}\n$$\n\n### 3. Algorithmic Implementation\n\nThe penalized logistic regression model is fitted using the following iterative algorithm:\n\n1.  **Data Standardization**: Standardize the continuous predictor $x_{\\text{age}}$ using the training data mean $\\bar{x}_{\\text{age}}$: $x_{\\text{age}}^{*} = (x_{\\text{age}} - \\bar{x}_{\\text{age}}) / 10$. Construct the design matrix $\\boldsymbol{X}$ with columns for the intercept ($1$), standardized age ($x_{\\text{age}}^{*}$), and biomarker ($x_{\\text{bio}}$).\n\n2.  **Initialization**: Initialize the coefficient vector $\\boldsymbol{\\beta}^{(0)} = \\boldsymbol{0}$ and set the penalty matrix $\\boldsymbol{P} = \\text{diag}(0, 1, ..., 1)$. Choose a convergence tolerance $\\epsilon$ (e.g., $10^{-8}$) and a maximum number of iterations.\n\n3.  **Iteration**: For $t=0, 1, 2, ...$ until convergence:\n    a.  Compute the linear predictor: $\\boldsymbol{\\eta}^{(t)} = \\boldsymbol{X} \\boldsymbol{\\beta}^{(t)}$.\n    b.  Compute the fitted probabilities: $\\boldsymbol{\\mu}^{(t)} = (1 + \\exp(-\\boldsymbol{\\eta}^{(t)}))^{-1}$. To ensure numerical stability and prevent weights from becoming zero, probabilities are clipped to a small distance from $0$ and $1$, e.g., $\\mu_i \\in [10^{-10}, 1 - 10^{-10}]$.\n    c.  Compute the IRLS weights: Construct the diagonal matrix $\\boldsymbol{W}^{(t)}$ with $W_{ii}^{(t)} = \\mu_i^{(t)}(1-\\mu_i^{(t)})$.\n    d.  Compute the working response: $\\boldsymbol{z}^{(t)} = \\boldsymbol{\\eta}^{(t)} + (\\boldsymbol{y} - \\boldsymbol{\\mu}^{(t)}) / (\\mu_i^{(t)}(1-\\mu_i^{(t)}))$.\n    e.  Solve for the updated coefficients $\\boldsymbol{\\beta}^{(t+1)}$ from the penalized normal equations:\n        $$\n        \\boldsymbol{\\beta}^{(t+1)} = (\\boldsymbol{X}^\\top \\boldsymbol{W}^{(t)} \\boldsymbol{X} + \\lambda \\boldsymbol{P})^{-1} (\\boldsymbol{X}^\\top \\boldsymbol{W}^{(t)} \\boldsymbol{z}^{(t)})\n        $$\n        This linear system is solved efficiently without explicit matrix inversion, for instance, using `numpy.linalg.solve`.\n    f.  Check for convergence: The loop terminates if the change in coefficients is below the tolerance, i.e., $\\sum_j |\\beta_j^{(t+1)} - \\beta_j^{(t)}| < \\epsilon$.\n\n4.  **Final Estimates**: Upon convergence, the final coefficient vector is $\\hat{\\boldsymbol{\\beta}} = \\boldsymbol{\\beta}^{(t+1)}$. The final fitted probabilities for the training data and new patients are calculated using $\\hat{\\boldsymbol{\\mu}} = (1 + \\exp(-\\boldsymbol{x}^\\top \\hat{\\boldsymbol{\\beta}}))^{-1}$, where for new patients, the predictor $x_{\\text{age}}$ is standardized using the mean from the original training data.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef fit_penalized_logistic(ages, biomarker, outcomes, lambda_val, new_patients):\n    \"\"\"\n    Fits a penalized logistic regression model using Iteratively Reweighted Least Squares (IRLS).\n\n    Args:\n        ages (list): List of patient ages.\n        biomarker (list): List of binary biomarker statuses.\n        outcomes (list): List of binary outcomes.\n        lambda_val (float): The L2 penalty parameter.\n        new_patients (list of tuples): Data for new patients to predict on.\n\n    Returns:\n        list: A list of 8 floats: [beta_0, beta_age, beta_bio, min_mu, max_mu, mu_A, mu_B].\n    \"\"\"\n    # 1. Preprocessing and design matrix construction\n    ages_arr = np.array(ages, dtype=float)\n    biomarker_arr = np.array(biomarker, dtype=float)\n    y = np.array(outcomes, dtype=float)\n    n_obs = len(ages_arr)\n\n    mean_age = np.mean(ages_arr)\n    # Standardize age predictor as specified\n    scaled_age = (ages_arr - mean_age) / 10.0\n\n    # Design matrix X with intercept, standardized age, and biomarker\n    X = np.c_[np.ones(n_obs), scaled_age, biomarker_arr]\n    p = X.shape[1]\n\n    # 2. IRLS algorithm\n    # Initialization\n    beta = np.zeros(p)\n    # Penalty matrix P (does not penalize intercept)\n    P = np.diag([0.0] + [1.0] * (p - 1))\n    \n    # Convergence parameters\n    max_iter = 50\n    tol = 1e-8\n    epsilon = 1e-10\n\n    for i in range(max_iter):\n        # Calculate linear predictor and probabilities\n        eta = X @ beta\n        mu = 1.0 / (1.0 + np.exp(-eta))\n        \n        # Clip probabilities for numerical stability\n        mu = np.clip(mu, epsilon, 1.0 - epsilon)\n        \n        # Calculate weights and working response\n        W_diag = mu * (1.0 - mu)\n        W = np.diag(W_diag)\n        z = eta + (y - mu) / W_diag\n        \n        # Solve the penalized normal equations\n        # (X^T W X + lambda * P) beta = X^T W z\n        A = X.T @ W @ X + lambda_val * P\n        b = X.T @ W @ z\n        \n        try:\n            beta_new = np.linalg.solve(A, b)\n        except np.linalg.LinAlgError:\n            # Fallback to pseudo-inverse if solve fails, though unlikely with penalty\n            beta_new = np.linalg.pinv(A) @ b\n\n        # Check for convergence\n        if np.sum(np.abs(beta_new - beta)) < tol:\n            beta = beta_new\n            break\n        \n        beta = beta_new\n\n    # 3. Final calculations\n    beta_hat = beta\n    \n    # Fitted probabilities on training data\n    final_eta = X @ beta_hat\n    final_mu = 1.0 / (1.0 + np.exp(-final_eta))\n    min_mu = np.min(final_mu)\n    max_mu = np.max(final_mu)\n\n    # Predictions for new patients\n    mu_preds = []\n    for patient_data in new_patients:\n        age_new, bio_new = patient_data\n        # Standardize age using the training data mean\n        scaled_age_new = (age_new - mean_age) / 10.0\n        x_new = np.array([1.0, scaled_age_new, float(bio_new)])\n        eta_new = x_new @ beta_hat\n        mu_new = 1.0 / (1.0 + np.exp(-eta_new))\n        mu_preds.append(mu_new)\n        \n    # 4. Assemble results in the required format\n    result_list = list(beta_hat) + [min_mu, max_mu] + mu_preds\n    return result_list\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final output.\n    \"\"\"\n    test_cases = [\n        {\n            \"ages\": [55, 64, 47, 38, 70, 59, 53, 61, 49, 45, 72, 50],\n            \"biomarker\": [1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0],\n            \"outcomes\": [1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0],\n            \"lambda\": 1.0,\n            \"new_patients\": [(60, 1), (50, 0)],\n        },\n        {\n            \"ages\": [40, 42, 45, 50, 52, 55, 60, 62, 65, 70],\n            \"biomarker\": [0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n            \"outcomes\": [0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n            \"lambda\": 1.0,\n            \"new_patients\": [(60, 1), (50, 0)],\n        },\n        {\n            \"ages\": [30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85],\n            \"biomarker\": [0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1],\n            \"outcomes\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n            \"lambda\": 0.1,\n            \"new_patients\": [(60, 1), (50, 0)],\n        },\n        {\n            \"ages\": [55, 64, 47, 38, 70, 59, 53, 61, 49, 45, 72, 50],\n            \"biomarker\": [1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0],\n            \"outcomes\": [1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0],\n            \"lambda\": 100.0,\n            \"new_patients\": [(60, 1), (50, 0)],\n        },\n    ]\n\n    all_results = []\n    for case in test_cases:\n        results = fit_penalized_logistic(\n            ages=case[\"ages\"],\n            biomarker=case[\"biomarker\"],\n            outcomes=case[\"outcomes\"],\n            lambda_val=case[\"lambda\"],\n            new_patients=case[\"new_patients\"]\n        )\n        all_results.append(results)\n\n    # Format the final output string exactly as required\n    # Create string representations of inner lists, then join them\n    inner_lists_str = [f\"[{','.join(f'{x:.8f}' for x in res)}]\" for res in all_results]\n    print(f\"[{','.join(inner_lists_str)}]\")\n\nsolve()\n```"
        }
    ]
}