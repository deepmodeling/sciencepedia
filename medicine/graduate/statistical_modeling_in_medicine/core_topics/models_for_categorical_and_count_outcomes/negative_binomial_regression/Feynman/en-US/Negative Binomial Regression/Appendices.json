{
    "hands_on_practices": [
        {
            "introduction": "Before fitting a complex model, it is crucial to diagnose whether it is necessary. This first exercise explores how to use basic sample statistics—the mean and variance—to detect and quantify overdispersion, a common feature in medical count data. By working through this problem , you will see firsthand how ignoring this phenomenon can lead to underestimated standard errors in a simpler Poisson model, highlighting the importance of selecting the appropriate statistical framework for robust inference.",
            "id": "4822241",
            "problem": "A hospital-based cohort study tracks the number of acute exacerbations of chronic obstructive pulmonary disease (COPD) per patient-year. Let $Y_{i}$ denote the count for patient $i$. The sample mean and sample variance across $n$ patients are observed to be $\\bar{y}=2.4$ and $s^{2}=7.8$, respectively. Starting from the fundamental definition of equidispersion under the Poisson model, where $\\operatorname{Var}(Y)=\\mathbb{E}[Y]$, and the well-tested variance structure of the Negative Binomial model with quadratic mean function (NB2), where $\\operatorname{Var}(Y)=\\mu+\\alpha \\mu^{2}$ with $\\mu=\\mathbb{E}[Y]$ and $\\alpha>0$ indicating overdispersion, proceed as follows:\n\n- Define a principled measure of departure from equidispersion based on $\\bar{y}$ and $s^{2}$, and derive the corresponding method-of-moments estimator for the NB2 overdispersion parameter $\\alpha$ using the NB2 variance structure.\n- Using the framework of Generalized Linear Models (GLM), deduce how misspecification of equidispersion affects the asymptotic standard errors of Poisson regression coefficients when the true variance at the observed mean follows the NB2 form, and identify the implied multiplicative inflation factor for those standard errors at $\\mu=\\bar{y}$.\n\nReport the final multiplicative inflation factor as a single real number. Round your final answer to four significant figures. Do not include any units in your final answer.",
            "solution": "The solution is developed in two parts as requested by the problem statement.\n\n**Part 1: Method-of-Moments Estimator for the Overdispersion Parameter $\\alpha$**\n\nThe Poisson model is characterized by equidispersion, where the variance of the random variable is equal to its expectation: $\\operatorname{Var}(Y) = \\mathbb{E}[Y]$. An immediate sign of departure from this assumption is when the sample variance $s^2$ is markedly different from the sample mean $\\bar{y}$. In this problem, we are given $\\bar{y} = 2.4$ and $s^2 = 7.8$. Since $s^2 > \\bar{y}$, the data exhibit overdispersion.\n\nThe Negative Binomial model with a quadratic variance function (NB2) provides a parametric form for this overdispersion. The variance is given by $\\operatorname{Var}(Y) = \\mu + \\alpha \\mu^2$, where $\\mu = \\mathbb{E}[Y]$ and $\\alpha$ is the overdispersion parameter. When $\\alpha=0$, the NB2 variance reduces to the Poisson variance. Thus, $\\alpha$ serves as a natural measure of departure from equidispersion.\n\nTo derive the method-of-moments (MoM) estimator for $\\alpha$, we equate the population moments to their corresponding sample-based estimators.\nThe first population moment is the mean, $\\mu = \\mathbb{E}[Y]$. Its MoM estimator is the sample mean, $\\hat{\\mu} = \\bar{y}$.\nThe second central population moment is the variance, $\\operatorname{Var}(Y)$. Its MoM estimator is the sample variance, $s^2$.\n\nWe substitute these estimators into the NB2 variance equation:\n$$s^2 = \\hat{\\mu} + \\hat{\\alpha} \\hat{\\mu}^2$$\nReplacing $\\hat{\\mu}$ with $\\bar{y}$, we get an equation for the MoM estimator $\\hat{\\alpha}$:\n$$s^2 = \\bar{y} + \\hat{\\alpha} \\bar{y}^2$$\nSolving for $\\hat{\\alpha}$:\n$$\\hat{\\alpha} \\bar{y}^2 = s^2 - \\bar{y}$$\n$$\\hat{\\alpha} = \\frac{s^2 - \\bar{y}}{\\bar{y}^2}$$\nThis is the method-of-moments estimator for the NB2 overdispersion parameter $\\alpha$.\n\n**Part 2: Multiplicative Inflation Factor for Standard Errors**\n\nWe consider a scenario where a Poisson regression model is incorrectly fitted to data that are truly generated from a Negative Binomial process. This is a case of a misspecified variance function in a Generalized Linear Model (GLM). We need to determine the effect on the standard errors of the estimated regression coefficients, $\\hat{\\boldsymbol{\\beta}}$.\n\nIn a GLM, the asymptotic covariance matrix of $\\hat{\\boldsymbol{\\beta}}$ is given by the sandwich estimator, which remains consistent even when the variance function is misspecified. The formula is:\n$$\\operatorname{Cov}_{\\text{true}}(\\hat{\\boldsymbol{\\beta}}) = [I(\\boldsymbol{\\beta})]^{-1} J(\\boldsymbol{\\beta}) [I(\\boldsymbol{\\beta})]^{-1}$$\nHere, $I(\\boldsymbol{\\beta})$ is the Fisher information matrix derived from the *assumed* model (Poisson), and $J(\\boldsymbol{\\beta})$ is the outer product of the gradients, calculated under the *true* data generating process (Negative Binomial).\n\nFor a Poisson model with the canonical log link, $\\ln(\\mu_i) = \\mathbf{x}_i^T \\boldsymbol{\\beta}$, the score function for $\\boldsymbol{\\beta}$ is $U(\\boldsymbol{\\beta}) = \\sum_i \\mathbf{x}_i (y_i - \\mu_i)$. The Fisher information matrix for the assumed Poisson model is:\n$$I(\\boldsymbol{\\beta}) = \\sum_i \\mathbf{x}_i \\mathbf{x}_i^T \\mu_i$$\nThe covariance matrix reported by standard software using a Poisson model (the \"naive\" or \"model-based\" estimator) is the inverse of this matrix:\n$$\\operatorname{Cov}_{\\text{naive}}(\\hat{\\boldsymbol{\\beta}}) = [I(\\hat{\\boldsymbol{\\beta}})]^{-1} = \\left(\\sum_i \\mathbf{x}_i \\mathbf{x}_i^T \\hat{\\mu}_i\\right)^{-1}$$\nThe middle part of the sandwich, $J(\\boldsymbol{\\beta})$, is given by:\n$$J(\\boldsymbol{\\beta}) = \\mathbb{E}[U(\\boldsymbol{\\beta}) U(\\boldsymbol{\\beta})^T] = \\sum_i \\mathbf{x}_i \\mathbf{x}_i^T \\operatorname{Var}_{\\text{true}}(Y_i)$$\nUnder the true NB2 model, $\\operatorname{Var}_{\\text{true}}(Y_i) = \\mu_i + \\alpha \\mu_i^2 = \\mu_i(1 + \\alpha \\mu_i)$.\nThus,\n$$J(\\boldsymbol{\\beta}) = \\sum_i \\mathbf{x}_i \\mathbf{x}_i^T \\mu_i(1 + \\alpha \\mu_i)$$\nThe ratio of the true variance to the assumed Poisson variance is the dispersion factor, $\\phi_i$:\n$$\\phi_i = \\frac{\\operatorname{Var}_{\\text{true}}(Y_i)}{\\operatorname{Var}_{\\text{Poisson}}(Y_i)} = \\frac{\\mu_i + \\alpha \\mu_i^2}{\\mu_i} = 1 + \\alpha \\mu_i$$\nIf we can assume this dispersion factor is approximately constant across all observations, such that $\\phi_i \\approx \\phi$ for all $i$, we can simplify $J(\\boldsymbol{\\beta})$:\n$$J(\\boldsymbol{\\beta}) \\approx \\sum_i \\mathbf{x}_i \\mathbf{x}_i^T (\\phi \\mu_i) = \\phi \\left(\\sum_i \\mathbf{x}_i \\mathbf{x}_i^T \\mu_i\\right) = \\phi I(\\boldsymbol{\\beta})$$\nSubstituting this into the sandwich estimator formula:\n$$\\operatorname{Cov}_{\\text{true}}(\\hat{\\boldsymbol{\\beta}}) \\approx [I(\\boldsymbol{\\beta})]^{-1} [\\phi I(\\boldsymbol{\\beta})] [I(\\boldsymbol{\\beta})]^{-1} = \\phi [I(\\boldsymbol{\\beta})]^{-1} = \\phi \\operatorname{Cov}_{\\text{naive}}(\\hat{\\boldsymbol{\\beta}})$$\nThis shows that the true variances of the regression coefficients are inflated by the factor $\\phi$ compared to the naively estimated variances from the misspecified Poisson model. Consequently, the standard errors, which are the square roots of the variances, are inflated by a factor of $\\sqrt{\\phi}$.\n\nThe problem asks for this inflation factor evaluated at the observed mean, $\\mu = \\bar{y}$. We can estimate the dispersion factor $\\phi$ using the sample moments:\n$$\\hat{\\phi} = 1 + \\hat{\\alpha} \\bar{y}$$\nSubstituting our MoM estimator for $\\hat{\\alpha}$:\n$$\\hat{\\phi} = 1 + \\left(\\frac{s^2 - \\bar{y}}{\\bar{y}^2}\\right) \\bar{y} = 1 + \\frac{s^2 - \\bar{y}}{\\bar{y}} = \\frac{\\bar{y} + s^2 - \\bar{y}}{\\bar{y}} = \\frac{s^2}{\\bar{y}}$$\nThis provides a direct and intuitive estimator for the dispersion parameter: the ratio of the sample variance to the sample mean.\n\nUsing the given data, we calculate the estimated dispersion factor:\n$$\\hat{\\phi} = \\frac{s^2}{\\bar{y}} = \\frac{7.8}{2.4} = 3.25$$\nThe multiplicative inflation factor for the standard errors is the square root of this value:\n$$\\text{Inflation Factor} = \\sqrt{\\hat{\\phi}} = \\sqrt{3.25}$$\nCalculating the numerical value:\n$$\\sqrt{3.25} \\approx 1.8027756...$$\nRounding to four significant figures, we get $1.803$. This is the factor by which the standard errors from a Poisson regression would need to be multiplied to correct for the observed level of overdispersion.",
            "answer": "$$\\boxed{1.803}$$"
        },
        {
            "introduction": "Once the need for a Negative Binomial model is established, the next step is to understand how its parameters are estimated. This practice  delves into the core mechanics of the fitting procedure by focusing on the principle of maximum likelihood. Deriving the log-likelihood function and its gradients, known as score functions, reveals the mathematical engine that statistical software uses to find the optimal regression coefficients ($\\beta$) and dispersion parameter ($\\alpha$).",
            "id": "4822323",
            "problem": "A randomized clinical trial records the count of asthma exacerbations $y_i$ over a fixed follow-up period for $n$ independent patients, along with a covariate vector $x_i \\in \\mathbb{R}^p$ that includes treatment assignment and relevant baseline prognostic factors. Due to overdispersion relative to the Poisson model observed in similar respiratory trials, the analysis adopts a Generalized Linear Model (GLM) with a negative binomial mean-variance relationship of the so-called NB2 type: for each patient $i$, the conditional mean is modeled by the log link $\\mu_i = \\exp(x_i^\\top \\beta)$ for a regression parameter vector $\\beta \\in \\mathbb{R}^p$, and the conditional variance is $\\operatorname{Var}(y_i \\mid x_i) = \\mu_i + \\alpha \\mu_i^2$ with dispersion parameter $\\alpha > 0$. Assume $y_i \\in \\{0,1,2,\\dots\\}$ are conditionally independent given $\\{x_i\\}_{i=1}^n$.\n\nStarting from a valid probability mass function for the negative binomial distribution consistent with the NB2 variance function and the above mean parameterization, write down the full sample log-likelihood for $(\\beta,\\alpha)$ and then derive the score functions with respect to $\\beta$ and $\\alpha$. Express your final answer as a single closed-form analytic expression comprising the log-likelihood, the $\\beta$-score vector, and the $\\alpha$-score scalar, using only standard special functions where necessary. Provide the final answer as a single row matrix. No numerical evaluation is required, and no units are involved.",
            "solution": "## Solution Derivation\n\nThe solution requires three main steps:\n1.  Identify the probability mass function (PMF) of the negative binomial distribution corresponding to the specified mean and variance functions.\n2.  Construct the total log-likelihood function $\\ell(\\beta, \\alpha)$ for the sample of $n$ observations.\n3.  Differentiate the log-likelihood function with respect to the parameter vector $\\beta$ and the scalar parameter $\\alpha$ to obtain the respective score functions, $U_\\beta$ and $U_\\alpha$.\n\n### 1. The Negative Binomial PMF\nA common parameterization of the negative binomial distribution for a random variable $Y$ is in terms of a size parameter $r$ and a probability parameter $p$. The PMF is:\n$$ P(Y=y) = \\binom{y+r-1}{y} p^r (1-p)^y = \\frac{\\Gamma(y+r)}{\\Gamma(y+1)\\Gamma(r)} p^r (1-p)^y $$\nThe mean and variance under this parameterization are:\n$$ E[Y] = \\mu = \\frac{r(1-p)}{p} $$\n$$ \\operatorname{Var}(Y) = \\frac{r(1-p)}{p^2} = \\mu \\frac{1}{p} $$\nThe problem specifies the variance function as $\\operatorname{Var}(y_i \\mid x_i) = \\mu_i + \\alpha \\mu_i^2$. Equating the two forms of the variance gives:\n$$ \\mu_i \\frac{1}{p_i} = \\mu_i + \\alpha \\mu_i^2 = \\mu_i(1 + \\alpha \\mu_i) $$\nFor $\\mu_i > 0$, we can divide by $\\mu_i$ to solve for $p_i$:\n$$ \\frac{1}{p_i} = 1 + \\alpha \\mu_i \\implies p_i = \\frac{1}{1 + \\alpha \\mu_i} $$\nThis also implies $1 - p_i = 1 - \\frac{1}{1 + \\alpha \\mu_i} = \\frac{\\alpha \\mu_i}{1 + \\alpha \\mu_i}$.\nNow, we use the mean equation to find the size parameter $r$:\n$$ \\mu_i = r \\frac{1-p_i}{p_i} = r \\frac{\\alpha \\mu_i / (1 + \\alpha \\mu_i)}{1 / (1 + \\alpha \\mu_i)} = r (\\alpha \\mu_i) $$\nAgain, for $\\mu_i > 0$, we divide by $\\alpha \\mu_i$ to find $r$:\n$$ r = \\frac{1}{\\alpha} $$\nThus, the dispersion parameter $\\alpha$ is the reciprocal of the size parameter $r$. Since $\\alpha > 0$, $r$ is also positive. The PMF for observation $y_i$ is:\n$$ f(y_i; \\mu_i, \\alpha) = \\frac{\\Gamma(y_i + 1/\\alpha)}{\\Gamma(y_i+1)\\Gamma(1/\\alpha)} \\left(\\frac{1}{1+\\alpha\\mu_i}\\right)^{1/\\alpha} \\left(\\frac{\\alpha\\mu_i}{1+\\alpha\\mu_i}\\right)^{y_i} $$\n\n### 2. The Log-Likelihood Function\nThe log-likelihood for a single observation $y_i$ is the natural logarithm of its PMF:\n$$ \\ell_i(\\beta, \\alpha) = \\ln f(y_i; \\mu_i, \\alpha) $$\n$$ \\ell_i(\\beta, \\alpha) = \\ln\\Gamma(y_i + 1/\\alpha) - \\ln\\Gamma(y_i+1) - \\ln\\Gamma(1/\\alpha) + \\frac{1}{\\alpha}\\ln\\left(\\frac{1}{1+\\alpha\\mu_i}\\right) + y_i\\ln\\left(\\frac{\\alpha\\mu_i}{1+\\alpha\\mu_i}\\right) $$\n$$ \\ell_i(\\beta, \\alpha) = \\ln\\Gamma(y_i + 1/\\alpha) - \\ln\\Gamma(y_i+1) - \\ln\\Gamma(1/\\alpha) - \\frac{1}{\\alpha}\\ln(1+\\alpha\\mu_i) + y_i\\ln(\\alpha) + y_i\\ln(\\mu_i) - y_i\\ln(1+\\alpha\\mu_i) $$\nCombining terms and substituting $\\ln(\\mu_i) = x_i^\\top\\beta$:\n$$ \\ell_i(\\beta, \\alpha) = \\ln\\Gamma\\left(y_i + \\frac{1}{\\alpha}\\right) - \\ln\\Gamma\\left(\\frac{1}{\\alpha}\\right) - \\ln\\Gamma(y_i+1) + y_i\\ln(\\alpha) + y_i(x_i^\\top\\beta) - \\left(y_i + \\frac{1}{\\alpha}\\right)\\ln(1+\\alpha\\exp(x_i^\\top\\beta)) $$\nThe total log-likelihood for the sample of $n$ independent observations is the sum of the individual log-likelihoods:\n$$ \\ell(\\beta, \\alpha) = \\sum_{i=1}^n \\ell_i(\\beta, \\alpha) $$\n$$ \\ell(\\beta, \\alpha) = \\sum_{i=1}^n \\left[ \\ln\\Gamma\\left(y_i + \\frac{1}{\\alpha}\\right) - \\ln\\Gamma\\left(\\frac{1}{\\alpha}\\right) - \\ln\\Gamma(y_i+1) + y_i\\ln(\\alpha) + y_i(x_i^\\top\\beta) - \\left(y_i + \\frac{1}{\\alpha}\\right)\\ln(1+\\alpha\\exp(x_i^\\top\\beta)) \\right] $$\n\n### 3. The Score Functions\nThe score functions are the first partial derivatives of the log-likelihood with respect to the parameters.\n\n#### Score for $\\beta$\nThe score vector for $\\beta$, $U_\\beta$, is a $p$-dimensional vector whose components are $\\frac{\\partial\\ell}{\\partial\\beta_j}$. We use the chain rule: $\\frac{\\partial\\ell}{\\partial\\beta_j} = \\sum_{i=1}^n \\frac{\\partial\\ell_i}{\\partial\\mu_i} \\frac{\\partial\\mu_i}{\\partial\\beta_j}$.\nFirst, we differentiate $\\ell_i$ with respect to $\\mu_i$:\n$$ \\frac{\\partial\\ell_i}{\\partial\\mu_i} = \\frac{\\partial}{\\partial\\mu_i} \\left[ y_i\\ln(\\mu_i) - \\left(y_i + \\frac{1}{\\alpha}\\right)\\ln(1+\\alpha\\mu_i) \\right] = \\frac{y_i}{\\mu_i} - \\left(y_i + \\frac{1}{\\alpha}\\right) \\frac{\\alpha}{1+\\alpha\\mu_i} $$\n$$ \\frac{\\partial\\ell_i}{\\partial\\mu_i} = \\frac{y_i(1+\\alpha\\mu_i) - \\mu_i(y_i\\alpha + 1)}{\\mu_i(1+\\alpha\\mu_i)} = \\frac{y_i + \\alpha y_i \\mu_i - \\alpha y_i \\mu_i - \\mu_i}{\\mu_i(1+\\alpha\\mu_i)} = \\frac{y_i - \\mu_i}{\\mu_i(1+\\alpha\\mu_i)} $$\nNext, we differentiate $\\mu_i$ with respect to the vector $\\beta$:\n$$ \\frac{\\partial\\mu_i}{\\partial\\beta} = \\frac{\\partial}{\\partial\\beta} \\exp(x_i^\\top\\beta) = \\exp(x_i^\\top\\beta) \\cdot x_i = \\mu_i x_i $$\nCombining these, the contribution of observation $i$ to the score vector $U_\\beta$ is:\n$$ \\frac{\\partial\\ell_i}{\\partial\\beta} = \\frac{\\partial\\ell_i}{\\partial\\mu_i} \\frac{\\partial\\mu_i}{\\partial\\beta} = \\frac{y_i - \\mu_i}{\\mu_i(1+\\alpha\\mu_i)} (\\mu_i x_i) = \\frac{y_i - \\mu_i}{1+\\alpha\\mu_i} x_i $$\nSumming over all observations gives the full score vector for $\\beta$:\n$$ U_\\beta(\\beta, \\alpha) = \\frac{\\partial\\ell}{\\partial\\beta} = \\sum_{i=1}^n \\frac{y_i - \\exp(x_i^\\top\\beta)}{1+\\alpha\\exp(x_i^\\top\\beta)} x_i $$\n\n#### Score for $\\alpha$\nThe score for $\\alpha$, $U_\\alpha$, is the partial derivative of $\\ell(\\beta, \\alpha)$ with respect to $\\alpha$. We will need the digamma function, $\\psi(z) = \\frac{d}{dz}\\ln\\Gamma(z)$.\n$$ \\frac{\\partial\\ell_i}{\\partial\\alpha} = \\frac{\\partial}{\\partial\\alpha} \\left[ \\ln\\Gamma\\left(y_i + \\frac{1}{\\alpha}\\right) - \\ln\\Gamma\\left(\\frac{1}{\\alpha}\\right) + y_i\\ln(\\alpha) - \\left(y_i + \\frac{1}{\\alpha}\\right)\\ln(1+\\alpha\\mu_i) \\right] $$\nDifferentiating term-by-term:\n$$ \\frac{\\partial\\ell_i}{\\partial\\alpha} = \\psi\\left(y_i + \\frac{1}{\\alpha}\\right)\\left(-\\frac{1}{\\alpha^2}\\right) - \\psi\\left(\\frac{1}{\\alpha}\\right)\\left(-\\frac{1}{\\alpha^2}\\right) + \\frac{y_i}{\\alpha} - \\left[ \\left(-\\frac{1}{\\alpha^2}\\right)\\ln(1+\\alpha\\mu_i) + \\left(y_i + \\frac{1}{\\alpha}\\right)\\frac{\\mu_i}{1+\\alpha\\mu_i} \\right] $$\n$$ \\frac{\\partial\\ell_i}{\\partial\\alpha} = \\frac{1}{\\alpha^2}\\left[ \\psi\\left(\\frac{1}{\\alpha}\\right) - \\psi\\left(y_i + \\frac{1}{\\alpha}\\right) \\right] + \\frac{y_i}{\\alpha} + \\frac{1}{\\alpha^2}\\ln(1+\\alpha\\mu_i) - \\frac{y_i\\mu_i + \\mu_i/\\alpha}{1+\\alpha\\mu_i} $$\nLet's group the terms:\n$$ \\frac{\\partial\\ell_i}{\\partial\\alpha} = \\frac{1}{\\alpha^2}\\left[ \\psi\\left(\\frac{1}{\\alpha}\\right) - \\psi\\left(y_i + \\frac{1}{\\alpha}\\right) + \\ln(1+\\alpha\\mu_i) \\right] + \\frac{y_i}{\\alpha} - \\frac{\\alpha y_i\\mu_i + \\mu_i}{\\alpha(1+\\alpha\\mu_i)} $$\nPutting the last two terms on a common denominator:\n$$ \\frac{y_i(1+\\alpha\\mu_i) - (\\alpha y_i\\mu_i + \\mu_i)}{\\alpha(1+\\alpha\\mu_i)} = \\frac{y_i + \\alpha y_i\\mu_i - \\alpha y_i\\mu_i - \\mu_i}{\\alpha(1+\\alpha\\mu_i)} = \\frac{y_i - \\mu_i}{\\alpha(1+\\alpha\\mu_i)} $$\nThus, the derivative for a single observation is:\n$$ \\frac{\\partial\\ell_i}{\\partial\\alpha} = \\frac{1}{\\alpha^2}\\left[ \\psi\\left(\\frac{1}{\\alpha}\\right) - \\psi\\left(y_i + \\frac{1}{\\alpha}\\right) + \\ln(1+\\alpha\\mu_i) \\right] + \\frac{y_i - \\mu_i}{\\alpha(1+\\alpha\\mu_i)} $$\nSumming over all observations gives the total score for $\\alpha$:\n$$ U_\\alpha(\\beta, \\alpha) = \\frac{\\partial\\ell}{\\partial\\alpha} = \\sum_{i=1}^n \\left\\{ \\frac{1}{\\alpha^2}\\left[ \\psi\\left(\\frac{1}{\\alpha}\\right) - \\psi\\left(y_i + \\frac{1}{\\alpha}\\right) + \\ln\\left(1+\\alpha\\exp(x_i^\\top\\beta)\\right) \\right] + \\frac{y_i - \\exp(x_i^\\top\\beta)}{\\alpha\\left(1+\\alpha\\exp(x_i^\\top\\beta)\\right)} \\right\\} $$",
            "answer": "$$ \\boxed{ \\begin{pmatrix} \\sum_{i=1}^n \\left[ \\ln\\Gamma\\left(y_i + \\frac{1}{\\alpha}\\right) - \\ln\\Gamma\\left(\\frac{1}{\\alpha}\\right) - \\ln\\Gamma(y_i+1) + y_i\\ln(\\alpha) + y_i(x_i^\\top\\beta) - \\left(y_i + \\frac{1}{\\alpha}\\right)\\ln\\left(1+\\alpha\\exp(x_i^\\top\\beta)\\right) \\right] & \\sum_{i=1}^n \\frac{y_i - \\exp(x_i^\\top\\beta)}{1+\\alpha\\exp(x_i^\\top\\beta)} x_i & \\sum_{i=1}^n \\left( \\frac{1}{\\alpha^2}\\left[ \\psi\\left(\\frac{1}{\\alpha}\\right) - \\psi\\left(y_i + \\frac{1}{\\alpha}\\right) + \\ln\\left(1+\\alpha\\exp(x_i^\\top\\beta)\\right) \\right] + \\frac{y_i - \\exp(x_i^\\top\\beta)}{\\alpha\\left(1+\\alpha\\exp(x_i^\\top\\beta)\\right)} \\right) \\end{pmatrix} } $$"
        },
        {
            "introduction": "The value of a statistical model ultimately lies in its interpretation and application to real-world questions. This final exercise  bridges the gap between statistical output and clinical insight by translating a relative effect measure, the Incidence Rate Ratio ($IRR$), into an absolute one, the Absolute Rate Reduction ($ARR$). It also challenges you to reason about propagating statistical uncertainty from the model's parameters to such derived quantities, a critical skill for communicating results with scientific rigor.",
            "id": "4822293",
            "problem": "A multicenter randomized trial in clinical infectious disease compared a prophylactic intervention with standard care. The primary outcome was the count of infection events per person-time, with variable follow-up across participants. Exploratory analysis showed overdispersion relative to the Poisson model, so the analysis used a Negative Binomial regression with a logarithmic link and an offset for person-time, reporting an incidence rate ratio (IRR) for the intervention versus control. Suppose the fitted model yielded an IRR of $0.75$ for the intervention. In the standard care group, the observed baseline rate of infection was $4$ events per $100$ person-years.\n\nUsing only the definitions of an incidence rate, the meaning of an incidence rate ratio, and the interpretation of a logarithmic-link Negative Binomial model with a person-time offset, translate the reported IRR into the absolute reduction in expected events per $100$ person-years at this baseline rate. Provide the point estimate only. Express your final numeric answer in units of events per $100$ person-years, and round to two significant figures.\n\nAdditionally, justify—starting from first principles—how one would obtain uncertainty on this absolute scale from the Negative Binomial fit under large-sample theory, identifying the relevant parameter transformation and variance propagation approach. You do not need to compute a numerical interval; provide only the reasoning steps and the mathematical form of the transformation involved. Your final numeric answer should consist solely of the point estimate requested above.",
            "solution": "The problem requires two parts: first, the calculation of a point estimate for the absolute reduction in expected infection events, and second, a justification of the method for obtaining an uncertainty interval for this estimate.\n\nPart 1: Calculation of the Point Estimate\n\nThe analysis employs a Negative Binomial regression model for count data. The model uses a logarithmic link function and an offset for person-time. Let $Y_i$ be the count of infection events for participant $i$ over a person-time of $T_i$. The expected count is denoted by $E[Y_i]$. The corresponding incidence rate, $\\lambda_i$, is the expected number of events per unit of time, defined as $\\lambda_i = E[Y_i] / T_i$.\n\nThe model structure is given by:\n$$ \\ln(E[Y_i]) = \\mathbf{X}_i \\boldsymbol{\\beta} + \\ln(T_i) $$\nwhere $\\mathbf{X}_i$ is the row vector of predictors for participant $i$, $\\boldsymbol{\\beta}$ is the column vector of regression coefficients, and $\\ln(T_i)$ is the offset term. The use of an offset for the logarithm of person-time is standard for rate modeling. This equation can be rearranged to model the logarithm of the rate directly:\n$$ \\ln(E[Y_i]) - \\ln(T_i) = \\mathbf{X}_i \\boldsymbol{\\beta} $$\n$$ \\ln\\left(\\frac{E[Y_i]}{T_i}\\right) = \\mathbf{X}_i \\boldsymbol{\\beta} $$\n$$ \\ln(\\lambda_i) = \\mathbf{X}_i \\boldsymbol{\\beta} $$\nIn this trial, the single predictor is a binary variable, let's call it $Z_i$, indicating the treatment group. Let $Z_i = 0$ for the standard care (control) group and $Z_i = 1$ for the intervention group. The model for the log-rate simplifies to:\n$$ \\ln(\\lambda_i) = \\beta_0 + \\beta_1 Z_i $$\nFor the control group ($Z_i = 0$), the expected incidence rate is:\n$$ \\ln(\\lambda_{\\text{control}}) = \\beta_0 \\implies \\lambda_{\\text{control}} = \\exp(\\beta_0) $$\nFor the intervention group ($Z_i = 1$), the expected incidence rate is:\n$$ \\ln(\\lambda_{\\text{int}}) = \\beta_0 + \\beta_1 \\implies \\lambda_{\\text{int}} = \\exp(\\beta_0 + \\beta_1) $$\nThe Incidence Rate Ratio (IRR) is the ratio of the incidence rate in the intervention group to that in the control group:\n$$ \\text{IRR} = \\frac{\\lambda_{\\text{int}}}{\\lambda_{\\text{control}}} = \\frac{\\exp(\\beta_0 + \\beta_1)}{\\exp(\\beta_0)} = \\frac{\\exp(\\beta_0)\\exp(\\beta_1)}{\\exp(\\beta_0)} = \\exp(\\beta_1) $$\nThe problem provides the following givens:\n1.  The observed baseline rate in the standard care group is $\\lambda_{\\text{control}} = 4$ events per $100$ person-years.\n2.  The fitted model yields an IRR of $\\exp(\\hat{\\beta}_1) = 0.75$.\n\nUsing these values, we can find the estimated expected rate in the intervention group:\n$$ \\hat{\\lambda}_{\\text{int}} = \\hat{\\lambda}_{\\text{control}} \\times \\text{IRR} $$\n$$ \\hat{\\lambda}_{\\text{int}} = (4 \\text{ events per } 100 \\text{ person-years}) \\times 0.75 = 3 \\text{ events per } 100 \\text{ person-years} $$\nThe absolute reduction in expected events is the Absolute Rate Reduction (ARR). It is the difference between the rates in the control and intervention groups:\n$$ \\text{ARR} = \\lambda_{\\text{control}} - \\lambda_{\\text{int}} $$\nSubstituting the estimated values:\n$$ \\widehat{\\text{ARR}} = \\hat{\\lambda}_{\\text{control}} - \\hat{\\lambda}_{\\text{int}} = 4 - 3 = 1 $$\nThe units are events per $100$ person-years. The problem requires rounding to two significant figures. The number $1$ is therefore expressed as $1.0$.\n\nPart 2: Justification for Uncertainty Estimation\n\nTo obtain the uncertainty (e.g., a confidence interval) for the ARR, one must propagate the uncertainty from the estimated model coefficients, $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$, to the derived quantity $\\widehat{\\text{ARR}}$. The ARR is a function of the model parameters:\n$$ \\text{ARR}(\\beta_0, \\beta_1) = \\lambda_{\\text{control}} - \\lambda_{\\text{int}} = \\exp(\\beta_0) - \\exp(\\beta_0 + \\beta_1) $$\nThe justification proceeds from the following first principles of large-sample theory and statistical inference:\n\n1.  **Asymptotic Normality of Estimators**: The Negative Binomial regression coefficients $(\\hat{\\beta}_0, \\hat{\\beta}_1)$ are obtained via maximum likelihood estimation. Under standard regularity conditions and for a large sample size, the vector of estimators $(\\hat{\\beta}_0, \\hat{\\beta}_1)^T$ is asymptotically multivariate normal. Its mean is the true parameter vector $(\\beta_0, \\beta_1)^T$, and its variance-covariance matrix, $\\boldsymbol{\\Sigma}_{\\hat{\\beta}}$, is estimated as the inverse of the observed Fisher information matrix. This matrix is a standard output from statistical software:\n    $$ \\hat{\\boldsymbol{\\Sigma}}_{\\hat{\\beta}} = \\begin{pmatrix} \\widehat{\\text{Var}}(\\hat{\\beta}_0) & \\widehat{\\text{Cov}}(\\hat{\\beta}_0, \\hat{\\beta}_1) \\\\ \\widehat{\\text{Cov}}(\\hat{\\beta}_0, \\hat{\\beta}_1) & \\widehat{\\text{Var}}(\\hat{\\beta}_1) \\end{pmatrix} $$\n\n2.  **Parameter Transformation and the Delta Method**: The ARR is a non-linear function of $\\beta_0$ and $\\beta_1$. To find the variance of the estimated ARR, $\\widehat{\\text{ARR}} = \\text{ARR}(\\hat{\\beta}_0, \\hat{\\beta}_1)$, we use the Delta method. The Delta method provides a first-order Taylor series approximation for the variance of a function of asymptotically normal random variables. The general formula for the variance of a transformed parameter $g(\\boldsymbol{\\beta})$ is $\\text{Var}(\\widehat{g(\\boldsymbol{\\beta})}) \\approx (\\nabla g)^T \\boldsymbol{\\Sigma}_{\\hat{\\beta}} (\\nabla g)$, where $\\nabla g$ is the gradient of the function $g$.\n\n3.  **Derivation of the Variance of ARR**: Let $g(\\beta_0, \\beta_1) = \\text{ARR}(\\beta_0, \\beta_1)$. We first compute the gradient of this function:\n    $$ \\nabla g = \\begin{pmatrix} \\frac{\\partial g}{\\partial \\beta_0} \\\\ \\frac{\\partial g}{\\partial \\beta_1} \\end{pmatrix} $$\n    The partial derivatives are:\n    $$ \\frac{\\partial g}{\\partial \\beta_0} = \\frac{\\partial}{\\partial \\beta_0} \\left( \\exp(\\beta_0) - \\exp(\\beta_0 + \\beta_1) \\right) = \\exp(\\beta_0) - \\exp(\\beta_0 + \\beta_1) = \\lambda_{\\text{control}} - \\lambda_{\\text{int}} = \\text{ARR} $$\n    $$ \\frac{\\partial g}{\\partial \\beta_1} = \\frac{\\partial}{\\partial \\beta_1} \\left( \\exp(\\beta_0) - \\exp(\\beta_0 + \\beta_1) \\right) = -\\exp(\\beta_0 + \\beta_1) = -\\lambda_{\\text{int}} $$\n    The approximate variance of $\\widehat{\\text{ARR}}$ is then given by the quadratic form:\n    $$ \\text{Var}(\\widehat{\\text{ARR}}) \\approx \\begin{pmatrix} \\text{ARR} & -\\lambda_{\\text{int}} \\end{pmatrix} \\begin{pmatrix} \\text{Var}(\\hat{\\beta}_0) & \\text{Cov}(\\hat{\\beta}_0, \\hat{\\beta}_1) \\\\ \\text{Cov}(\\hat{\\beta}_0, \\hat{\\beta}_1) & \\text{Var}(\\hat{\\beta}_1) \\end{pmatrix} \\begin{pmatrix} \\text{ARR} \\\\ -\\lambda_{\\text{int}} \\end{pmatrix} $$\n    Expanding this expression gives the explicit formula for the variance:\n    $$ \\text{Var}(\\widehat{\\text{ARR}}) \\approx (\\text{ARR})^2 \\text{Var}(\\hat{\\beta}_0) + (\\lambda_{\\text{int}})^2 \\text{Var}(\\hat{\\beta}_1) - 2(\\text{ARR})(\\lambda_{\\text{int}}) \\text{Cov}(\\hat{\\beta}_0, \\hat{\\beta}_1) $$\n\n4.  **Construction of the Confidence Interval**: All terms in the variance formula are evaluated at their estimated values (e.g., $\\widehat{\\text{ARR}}$, $\\hat{\\lambda}_{\\text{int}}$, and the elements of $\\hat{\\boldsymbol{\\Sigma}}_{\\hat{\\beta}}$). By the properties of the Delta method, the estimator $\\widehat{\\text{ARR}}$ is itself asymptotically normally distributed. Therefore, an approximate $(1-\\alpha) \\times 100\\%$ confidence interval for the true ARR can be constructed using the Wald method:\n    $$ \\widehat{\\text{ARR}} \\pm z_{1-\\alpha/2} \\sqrt{\\widehat{\\text{Var}}(\\widehat{\\text{ARR}})} $$\n    where $z_{1-\\alpha/2}$ is the appropriate quantile of the standard normal distribution (e.g., $1.96$ for a $95\\%$ confidence interval). This procedure provides the required measure of uncertainty for the absolute reduction on its natural scale.",
            "answer": "$$ \\boxed{1.0} $$"
        }
    ]
}