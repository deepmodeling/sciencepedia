{
    "hands_on_practices": [
        {
            "introduction": "在回归模型中，多重共线性是一个常见且棘手的问题，它会导致系数估计值不稳定且难以解释。岭回归通过在最小二乘法中加入一个 $L_2$ 惩罚项来解决此问题。这个练习 () 将引导你从数值稳定性的角度，深入探究岭回归的工作原理。你将通过分析设计矩阵的特征值，亲眼见证正则化项如何通过改善矩阵的条件数来稳定模型求解过程。",
            "id": "4983782",
            "problem": "一个生物统计团队正在拟合一个多变量线性模型，以使用 $p=4$ 个标准化的实验室测量指标（设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 的列）来预测一个与肾功能相关的连续结果。已知这些指标由于共享的生理通路而高度共线性。该团队考虑用岭回归（ridge regression）替换普通最小二乘法以稳定系数估计。假设 $X$ 的列已经中心化并缩放到单位方差，并且假设对称矩阵 $X^{\\top}X$ 的特征值为 $\\{2000, 300, 1.5, 0.02\\}$，这反映了由共线性引起的近似退化。记 $I_p$ 为 $p \\times p$ 的单位矩阵。\n\n从岭回归的惩罚最小二乘目标函数出发，\n$$\nL(\\beta) \\;=\\; \\|y - X\\beta\\|_2^2 \\;+\\; \\lambda \\|\\beta\\|_2^2,\n$$\n并利用关于对称矩阵的谱分解和矩阵2-范数条件数的基本事实，执行以下操作：\n\n1. 推导与最小化 $L(\\beta)$ 相关的正规方程，并利用谱分解 $X^{\\top}X = Q \\Lambda Q^{\\top}$（其中 $Q$ 是正交矩阵，$\\Lambda$ 是对角矩阵），确定线性系统矩阵的特征值如何被岭惩罚项 $\\lambda$ 改变。\n\n2. 对于对称半正定矩阵 $M$，使用其矩阵2-范数条件数的定义 $\\kappa_2(M) = \\sigma_{\\max}(M) / \\sigma_{\\min}(M)$，将岭正则化后的条件数 $\\kappa_2\\!\\left(X^{\\top}X + \\lambda I_p\\right)$ 用 $X^{\\top}X$ 的最大和最小特征值以及 $\\lambda$ 来表示。\n\n3. 计算最小的非负 $\\lambda$，使得 $X^{\\top}X + \\lambda I_p$ 的条件数最多为 $50$。\n\n将你最终的 $\\lambda$ 数值答案四舍五入到四位有效数字。只报告 $\\lambda$ 的值。",
            "solution": "该问题是有效的，因为它在统计学习理论中有科学依据，提法明确、客观，并且包含了得到唯一解所需的所有信息。\n\n解答过程根据问题陈述分为三个部分推导。\n\n### 第1部分：正规方程的推导与特征值分析\n\n岭回归的目标函数为：\n$$\nL(\\beta) \\;=\\; \\|y - X\\beta\\|_2^2 \\;+\\; \\lambda \\|\\beta\\|_2^2\n$$\n为了找到最小化该函数的系数向量 $\\beta$，我们必须求 $L(\\beta)$ 关于 $\\beta$ 的梯度并将其设为零。首先，我们展开平方欧几里得范数：\n$$\nL(\\beta) \\;=\\; (y - X\\beta)^{\\top}(y - X\\beta) \\;+\\; \\lambda \\beta^{\\top}\\beta\n$$\n$$\nL(\\beta) \\;=\\; (y^{\\top} - \\beta^{\\top}X^{\\top})(y - X\\beta) \\;+\\; \\lambda \\beta^{\\top}\\beta\n$$\n$$\nL(\\beta) \\;=\\; y^{\\top}y - y^{\\top}X\\beta - \\beta^{\\top}X^{\\top}y + \\beta^{\\top}X^{\\top}X\\beta \\;+\\; \\lambda \\beta^{\\top}\\beta\n$$\n由于 $\\beta^{\\top}X^{\\top}y$ 是一个标量，它等于其转置 $( \\beta^{\\top}X^{\\top}y )^{\\top} = y^{\\top}X\\beta$。因此，我们可以合并中间两项：\n$$\nL(\\beta) \\;=\\; y^{\\top}y - 2\\beta^{\\top}X^{\\top}y + \\beta^{\\top}(X^{\\top}X + \\lambda I_p)\\beta\n$$\n现在，我们计算 $L(\\beta)$ 关于向量 $\\beta$ 的梯度：\n$$\n\\frac{\\partial L(\\beta)}{\\partial \\beta} \\;=\\; -2X^{\\top}y + 2(X^{\\top}X + \\lambda I_p)\\beta\n$$\n将梯度设为零以求最小值：\n$$\n-2X^{\\top}y + 2(X^{\\top}X + \\lambda I_p)\\beta \\;=\\; 0\n$$\n$$\n(X^{\\top}X + \\lambda I_p)\\beta \\;=\\; X^{\\top}y\n$$\n这就是岭回归的正规方程。线性系统矩阵为 $M_{\\lambda} = X^{\\top}X + \\lambda I_p$。\n\n为了确定特征值如何被改变，我们使用给定的谱分解 $X^{\\top}X = Q \\Lambda Q^{\\top}$，其中 $Q$ 是一个正交矩阵（$Q Q^{\\top} = Q^{\\top}Q = I_p$），而 $\\Lambda$ 是一个对角矩阵，其对角线元素是 $X^{\\top}X$ 的特征值。设这些特征值为 $\\sigma_i$，$i=1, \\dots, p$。\n我们可以将系统矩阵 $M_{\\lambda}$ 重写为：\n$$\nM_{\\lambda} \\;=\\; X^{\\top}X + \\lambda I_p \\;=\\; Q \\Lambda Q^{\\top} + \\lambda Q Q^{\\top} \\;=\\; Q(\\Lambda + \\lambda I_p)Q^{\\top}\n$$\n这是 $M_{\\lambda}$ 的谱分解。$M_{\\lambda}$ 的特征值是矩阵 $\\Lambda + \\lambda I_p$ 的对角线元素。如果 $X^{\\top}X$ 的特征值是 $\\{\\sigma_1, \\sigma_2, \\dots, \\sigma_p\\}$，那么 $M_{\\lambda} = X^{\\top}X + \\lambda I_p$ 的特征值是 $\\{\\sigma_1 + \\lambda, \\sigma_2 + \\lambda, \\dots, \\sigma_p + \\lambda\\}$。因此，岭惩罚项 $\\lambda$ 对矩阵 $X^{\\top}X$ 的每个特征值都增加了一个常数位移。\n\n### 第2部分：岭正则化后的条件数\n\n问题将对称半正定矩阵 $M$ 的矩阵2-范数条件数定义为 $\\kappa_2(M) = \\sigma_{\\max}(M) / \\sigma_{\\min}(M)$，其中 $\\sigma_{\\max}(M)$ 和 $\\sigma_{\\min}(M)$ 分别是 $M$ 的最大和最小奇异值。对于对称半正定矩阵，奇异值与特征值相同。\n所讨论的矩阵是 $M_{\\lambda} = X^{\\top}X + \\lambda I_p$。根据第1部分，其特征值为 $\\sigma_i + \\lambda$，其中 $\\sigma_i$ 是 $X^{\\top}X$ 的特征值。\n设 $\\sigma_{\\max}(X^{\\top}X)$ 和 $\\sigma_{\\min}(X^{\\top}X)$ 分别为 $X^{\\top}X$ 的最大和最小特征值。由于 $\\lambda \\ge 0$，$X^{\\top}X + \\lambda I_p$ 的最大和最小特征值为：\n$$\n\\sigma_{\\max}(X^{\\top}X + \\lambda I_p) \\;=\\; \\max_{i}(\\sigma_i + \\lambda) \\;=\\; \\sigma_{\\max}(X^{\\top}X) + \\lambda\n$$\n$$\n\\sigma_{\\min}(X^{\\top}X + \\lambda I_p) \\;=\\; \\min_{i}(\\sigma_i + \\lambda) \\;=\\; \\sigma_{\\min}(X^{\\top}X) + \\lambda\n$$\n因此，岭正则化后的条件数为：\n$$\n\\kappa_2(X^{\\top}X + \\lambda I_p) \\;=\\; \\frac{\\sigma_{\\max}(X^{\\top}X + \\lambda I_p)}{\\sigma_{\\min}(X^{\\top}X + \\lambda I_p)} \\;=\\; \\frac{\\sigma_{\\max}(X^{\\top}X) + \\lambda}{\\sigma_{\\min}(X^{\\top}X) + \\lambda}\n$$\n\n### 第3部分：$\\lambda$ 的计算\n\n给定 $p=4$ 的矩阵 $X^{\\top}X$ 的特征值为 $\\{2000, 300, 1.5, 0.02\\}$。其中最大和最小的特征值是：\n$$\n\\sigma_{\\max}(X^{\\top}X) = 2000\n$$\n$$\n\\sigma_{\\min}(X^{\\top}X) = 0.02\n$$\n问题要求找到最小的非负 $\\lambda$，使得 $X^{\\top}X + \\lambda I_p$ 的条件数最多为 50。使用第2部分的公式：\n$$\n\\kappa_2(X^{\\top}X + \\lambda I_p) \\;=\\; \\frac{2000 + \\lambda}{0.02 + \\lambda} \\le 50\n$$\n为了解出 $\\lambda$，我们整理这个不等式。由于 $\\lambda \\ge 0$，分母 $(0.02 + \\lambda)$ 是严格为正的，所以我们可以用它乘以不等式两边而不改变不等号的方向：\n$$\n2000 + \\lambda \\le 50(0.02 + \\lambda)\n$$\n$$\n2000 + \\lambda \\le 1 + 50\\lambda\n$$\n现在，我们分离出 $\\lambda$：\n$$\n2000 - 1 \\le 50\\lambda - \\lambda\n$$\n$$\n1999 \\le 49\\lambda\n$$\n$$\n\\lambda \\ge \\frac{1999}{49}\n$$\n满足此条件的最小 $\\lambda$ 值为 $\\lambda = \\frac{1999}{49}$。为了提供数值答案，我们计算这个值：\n$$\n\\lambda = \\frac{1999}{49} \\approx 40.795918367...\n$$\n问题要求将结果四舍五入到四位有效数字。前四位有效数字是 $40.79$。第五位有效数字是 $5$，需要向上舍入最后一位数字。因此，$40.79$ 四舍五入为 $40.80$。",
            "answer": "$$\\boxed{40.80}$$"
        },
        {
            "introduction": "LASSO 不仅能够压缩系数，还能进行变量选择，但它在处理相关预测变量时可能表现不佳，例如会从一组相关变量中任意选择一个。弹性网（Elastic Net）通过结合 $L_1$ 和 $L_2$ 惩罚项，克服了这一缺点，并能产生“分组效应”。在这个练习 () 中，你将通过一个理想化的相关预测变量场景，从数学上推导出 LASSO 和弹性网的截然不同的行为。这将让你具体地理解弹性网的分组效应，并明白为何在基因组学等预测变量天然成组的领域中，它通常是更好的选择。",
            "id": "4983817",
            "problem": "一个用于医院再入院的临床风险模型使用了两个高通量测量值，由于预处理造成的人为结果（preprocessing artifact），这两个测量值是完全共线的：两个预测变量是相同的中心化和标准化的列。假设收集了 $n \\in \\mathbb{N}$ 个观测值，设计矩阵有两列 $x_{1} \\in \\mathbb{R}^{n}$ 和 $x_{2} \\in \\mathbb{R}^{n}$，满足 $x_{1} = x_{2} = x$，且 $(1/n)\\,x^{\\top} x = 1$。结果是一个中心化的连续响应 $y \\in \\mathbb{R}^{n}$，它与重复信号成正比，$y = \\theta\\,x$，其中信号水平 $\\theta \\in \\mathbb{R}$ 未知，并假设 $\\theta > 0$。考虑系数向量 $\\beta = (\\beta_{1},\\beta_{2})$ 的惩罚最小二乘估计量。\n\n将最小绝对收缩和选择算子（LASSO）估计量定义为下式的最小化子：\n$$\n\\frac{1}{2n}\\,\\|y - X\\beta\\|^{2} + \\lambda \\left(|\\beta_{1}| + |\\beta_{2}|\\right),\n$$\n其中惩罚水平 $\\lambda > 0$，且 $X = [x_{1}\\;x_{2}]$。\n\n将混合参数为 $\\alpha \\in (0,1)$ 的弹性网络（EN）估计量定义为下式的最小化子：\n$$\n\\frac{1}{2n}\\,\\|y - X\\beta\\|^{2} + \\lambda \\alpha \\left(|\\beta_{1}| + |\\beta_{2}|\\right) + \\frac{\\lambda(1-\\alpha)}{2}\\left(\\beta_{1}^{2} + \\beta_{2}^{2}\\right).\n$$\n\n对于系数向量 $\\beta$，定义分组效应度量\n$$\nG(\\beta) \\equiv \\frac{|\\beta_{1} - \\beta_{2}|}{|\\beta_{1} + \\beta_{2}|},\n$$\n仅解释 $|\\beta_{1} + \\beta_{2}| > 0$ 的情况。\n\n在条件 $\\theta > \\lambda$ 和 $\\theta > \\lambda \\alpha$ 下，执行以下操作：\n\n- 从两个惩罚估计量的定义出发，利用 $x_{1} = x_{2}$ 和 $(1/n)\\,x^{\\top}x = 1$ 的条件，将每个优化问题简化为关于和 $s \\equiv \\beta_{1} + \\beta_{2}$ 的一维问题，并确定相应的最优和 $s_{\\text{LASSO}}^{\\star}$ 和 $s_{\\text{EN}}^{\\star}$。\n\n- 对于 LASSO，论证为什么最小化子不是唯一的，并且存在一个系数设置为零的最小化解，例如 $(\\hat{\\beta}_{1},\\hat{\\beta}_{2}) = (s_{\\text{LASSO}}^{\\star},0)$。对于 EN，证明对于固定和 $s$ 的所有分割，当系数均等分配时惩罚最小，即 $(\\hat{\\beta}_{1},\\hat{\\beta}_{2}) = \\left(s_{\\text{EN}}^{\\star}/2,\\,s_{\\text{EN}}^{\\star}/2\\right)$。\n\n- 计算具有一个零系数的极端 LASSO 最小化子的 $G_{\\text{LASSO}} \\equiv G(\\hat{\\beta}_{\\text{LASSO}})$ 和具有相等系数的 EN 最小化子的 $G_{\\text{EN}} \\equiv G(\\hat{\\beta}_{\\text{EN}})$。\n\n报告单个标量差值\n$$\n\\Delta \\equiv G_{\\text{LASSO}} - G_{\\text{EN}}.\n$$\n给出您的最终答案，形式为精确值，无需四舍五入。不需要单位。",
            "solution": "首先验证问题，发现其提法恰当、有科学依据且内部一致。我们可以继续进行推导。\n\n问题的核心在于最小化两个不同的惩罚最小二乘目标函数。两者中的一个共同项是缩放后的残差平方和（RSS），我们首先对其进行简化。\n设计矩阵为 $X = [x_{1}\\;x_{2}]$。鉴于 $x_{1} = x_{2} = x$，预测值为 $X\\beta = x_{1}\\beta_{1} + x_{2}\\beta_{2} = x\\beta_{1} + x\\beta_{2} = x(\\beta_{1} + \\beta_{2})$。\n令 $s \\equiv \\beta_{1} + \\beta_{2}$。则预测值为 $xs$。\n响应由 $y = \\theta x$ 给出。\nRSS 项为 $\\|y - X\\beta\\|^{2} = \\|\\theta x - xs\\|^{2} = \\|(\\theta-s)x\\|^{2} = (\\theta-s)^{2}\\|x\\|^{2}$。\n问题提供了归一化条件 $(1/n)\\,x^{\\top} x = 1$。由于 $x^{\\top} x = \\|x\\|^{2}$，这意味着 $\\|x\\|^{2} = n$。\n将此代入缩放后的 RSS 项，得到：\n$$\n\\frac{1}{2n}\\,\\|y - X\\beta\\|^{2} = \\frac{1}{2n}(\\theta-s)^{2}n = \\frac{1}{2}(\\theta-s)^{2}\n$$\n这个简化的损失项仅通过它们的和 $s$ 来依赖于 $\\beta_{1}$ 和 $\\beta_{2}$。\n\n首先，我们分析最小绝对收缩和选择算子（LASSO）估计量。要最小化的目标函数是：\n$$\nL_{\\text{LASSO}}(\\beta_{1}, \\beta_{2}) = \\frac{1}{2}(\\theta-s)^{2} + \\lambda(|\\beta_{1}| + |\\beta_{2}|)\n$$\n为了最小化这个函数，我们可以采用一个两阶段的方法。对于一个固定的和 $s = \\beta_{1} + \\beta_{2}$，我们首先针对 $s$ 的所有可能分割来最小化惩罚项 $\\lambda (|\\beta_{1}| + |\\beta_{2}|)$。RSS 项 $\\frac{1}{2}(\\theta-s)^{2}$ 对于固定的 $s$ 是一个常数。\n在约束 $\\beta_{1} + \\beta_{2} = s$ 下，$L_{1}$-范数 $|\\beta_{1}| + |\\beta_{2}|$ 的最小值是 $|s|$。当且仅当 $\\beta_{1}$ 和 $\\beta_{2}$ 同号或其中一个为零时，才能达到这个最小值。这意味着对于给定的最优和 $s_{\\text{LASSO}}^{\\star}$，$(\\beta_{1}, \\beta_{2})$ 的解不是唯一的。\n因此，LASSO 的优化问题简化为关于 $s$ 的一维问题：\n$$\n\\min_{s \\in \\mathbb{R}} \\left\\{ \\frac{1}{2}(\\theta-s)^{2} + \\lambda|s| \\right\\}\n$$\n这是一个标准问题，其解由软阈值算子给出。最优和 $s_{\\text{LASSO}}^{\\star}$ 是：\n$$\ns_{\\text{LASSO}}^{\\star} = \\text{sign}(\\theta)(|\\theta|-\\lambda)_{+}\n$$\n根据问题条件 $\\theta > 0$ 和 $\\theta > \\lambda$，这可以简化为：\n$$\ns_{\\text{LASSO}}^{\\star} = (\\theta-\\lambda)_{+} = \\theta - \\lambda\n$$\n如前所述，任何满足 $\\hat{\\beta}_{1} \\ge 0$, $\\hat{\\beta}_{2} \\ge 0$ 且 $\\hat{\\beta}_{1} + \\hat{\\beta}_{2} = s_{\\text{LASSO}}^{\\star} = \\theta - \\lambda$ 的数对 $(\\hat{\\beta}_{1}, \\hat{\\beta}_{2})$ 都是一个有效的 LASSO 最小化子。问题要求我们考虑一个特定的“极端”最小化子 $\\hat{\\beta}_{\\text{LASSO}}$，其中一个系数为零，比如 $\\hat{\\beta}_{\\text{LASSO}} = (s_{\\text{LASSO}}^{\\star}, 0) = (\\theta - \\lambda, 0)$。\n我们计算此解的分组效应度量 $G_{\\text{LASSO}}$。由于 $\\theta>\\lambda$，我们有 $|\\hat{\\beta}_{1} + \\hat{\\beta}_{2}| = |\\theta-\\lambda| > 0$，所以 $G$ 是良定义的。\n$$\nG_{\\text{LASSO}} = G(\\hat{\\beta}_{\\text{LASSO}}) = \\frac{|\\hat{\\beta}_{1} - \\hat{\\beta}_{2}|}{|\\hat{\\beta}_{1} + \\hat{\\beta}_{2}|} = \\frac{|(\\theta-\\lambda) - 0|}{|(\\theta-\\lambda)+0|} = \\frac{|\\theta-\\lambda|}{|\\theta-\\lambda|} = 1\n$$\n\n接下来，我们分析弹性网络（EN）估计量。目标函数是：\n$$\nL_{\\text{EN}}(\\beta_{1}, \\beta_{2}) = \\frac{1}{2}(\\theta-s)^{2} + \\lambda \\alpha (|\\beta_{1}| + |\\beta_{2}|) + \\frac{\\lambda(1-\\alpha)}{2}(\\beta_{1}^{2} + \\beta_{2}^{2})\n$$\n同样，对于固定的和 $s = \\beta_{1}+\\beta_{2}$，我们最小化惩罚部分。假设最优的 $s > 0$（我们稍后会验证），任何最优解都将有 $\\beta_{1}, \\beta_{2} \\ge 0$。在这种情况下，$|\\beta_{1}| + |\\beta_{2}| = \\beta_{1} + \\beta_{2} = s$。惩罚的 $L_{1}$ 部分 $\\lambda \\alpha s$ 对于固定的 $s$ 成为常数。任务简化为在约束 $\\beta_{1}+\\beta_{2}=s$ 和 $\\beta_{1}, \\beta_{2} \\ge 0$ 下最小化惩罚的 $L_{2}$ 部分 $\\frac{\\lambda(1-\\alpha)}{2}(\\beta_{1}^{2} + \\beta_{2}^{2})$。\n我们需要最小化 $\\beta_{1}^{2} + \\beta_{2}^{2} = \\beta_{1}^{2} + (s-\\beta_{1})^{2}$。设 $f(\\beta_{1}) = 2\\beta_{1}^{2} - 2s\\beta_{1} + s^{2}$。这是一个凸抛物线，其最小值在其导数为零处取得：$f'(\\beta_{1}) = 4\\beta_{1} - 2s = 0$，这得出 $\\beta_{1} = s/2$。因此，$\\beta_{2}=s/2$。平方 $L_{2}$-范数的严格凸性强制系数相等，这展示了 EN 的分组效应。\n\n使用最优分割 $(\\beta_{1}, \\beta_{2}) = (s/2, s/2)$，EN 目标函数仅成为 $s$ 的函数：\n$$\nL_{\\text{EN}}(s) = \\frac{1}{2}(\\theta-s)^{2} + \\lambda \\alpha |s| + \\frac{\\lambda(1-\\alpha)}{2}\\left(\\left(\\frac{s}{2}\\right)^{2} + \\left(\\frac{s}{2}\\right)^{2}\\right) = \\frac{1}{2}(\\theta-s)^{2} + \\lambda \\alpha |s| + \\frac{\\lambda(1-\\alpha)}{4}s^{2}\n$$\n我们通过最小化这个函数来找到最优和 $s_{\\text{EN}}^{\\star}$。给定 $\\theta > \\lambda\\alpha > 0$，我们可以假设 $s > 0$。对 $s$ 求导并设为零：\n$$\n\\frac{d}{ds}L_{\\text{EN}}(s) = -(\\theta-s) + \\lambda\\alpha + \\frac{\\lambda(1-\\alpha)}{2}s = 0\n$$\n$$\ns\\left(1 + \\frac{\\lambda(1-\\alpha)}{2}\\right) = \\theta - \\lambda\\alpha \\implies s_{\\text{EN}}^{\\star} = \\frac{\\theta - \\lambda\\alpha}{1 + \\frac{\\lambda(1-\\alpha)}{2}}\n$$\n条件 $\\theta > \\lambda\\alpha$ 确保 $s_{\\text{EN}}^{\\star} > 0$，这与我们的假设一致。\nEN 估计的系数为 $\\hat{\\beta}_{\\text{EN}} = (\\hat{\\beta}_{1}, \\hat{\\beta}_{2}) = (s_{\\text{EN}}^{\\star}/2, s_{\\text{EN}}^{\\star}/2)$。\n我们计算此解的分组效应 $G_{\\text{EN}}$。分母 $|\\hat{\\beta}_{1} + \\hat{\\beta}_{2}| = |s_{\\text{EN}}^{\\star}|$ 不为零。\n$$\nG_{\\text{EN}} = G(\\hat{\\beta}_{\\text{EN}}) = \\frac{|\\hat{\\beta}_{1} - \\hat{\\beta}_{2}|}{|\\hat{\\beta}_{1} + \\hat{\\beta}_{2}|} = \\frac{|s_{\\text{EN}}^{\\star}/2 - s_{\\text{EN}}^{\\star}/2|}{|s_{\\text{EN}}^{\\star}/2 + s_{\\text{EN}}^{\\star}/2|} = \\frac{0}{|s_{\\text{EN}}^{\\star}|} = 0\n$$\n\n最后，我们计算所需的差值 $\\Delta = G_{\\text{LASSO}} - G_{\\text{EN}}$。\n$$\n\\Delta = 1 - 0 = 1\n$$\n这个结果量化了 LASSO 和弹性网络在存在完全相关预测变量时的典型差异：LASSO 选择一个而弃另一个（$G=1$），而弹性网络则通过赋予它们相等的系数来将它们分组（$G=0$）。",
            "answer": "$$\n\\boxed{1}\n$$"
        },
        {
            "introduction": "理解了不同正则化方法的功能后，下一个关键问题是如何计算它们的解，特别是当目标函数包含像 $L_1$ 范数这样不可微的惩罚项时。近端梯度下降法是解决此类复合优化问题的标准算法。这个动手实践 () 将要求你从零开始实现近端梯度算法来求解弹性网问题。你不仅会编写核心的迭代逻辑，还将通过计算李普希茨常数来确定合适的步长，从而深入探索算法的收敛理论，并亲身验证理论条件如何影响实际的算法性能。",
            "id": "4983841",
            "problem": "考虑使用形式为 $F(\\beta) = g(\\beta) + h(\\beta)$ 的复合凸目标函数，通过弹性网络正则化线性建模进行医疗风险预测，其中 $g(\\beta)$ 是光滑凸损失函数，$h(\\beta)$ 是凸但可能不可微的惩罚项。在此问题中，光滑部分是正则化的最小二乘损失 $g(\\beta) = \\frac{1}{2n}\\lVert X\\beta - y\\rVert_2^2 + \\frac{\\lambda_2}{2}\\lVert \\beta \\rVert_2^2$，非光滑部分是弹性网络的 $\\ell_1$ 项 $h(\\beta) = \\lambda_1 \\lVert \\beta \\rVert_1$，其中 $X \\in \\mathbb{R}^{n \\times p}$ 是设计矩阵，$y \\in \\mathbb{R}^{n}$ 是响应向量，$\\beta \\in \\mathbb{R}^p$ 是系数，$\\lambda_1, \\lambda_2 \\ge 0$ 是正则化参数。任务是实现使用固定步长的近端梯度法来最小化 $F(\\beta)$，并通过 $g(\\beta)$ 梯度的 Lipschitz 常数来分析步长选择以确保收敛。\n\n您必须从头开始实现以下内容，不依赖于预构建的优化器：\n- 按照上述规定构造 $g(\\beta)$ 和 $h(\\beta)$，并由此定义 $F(\\beta) = g(\\beta) + h(\\beta)$。\n- 推导 $g(\\beta)$ 的梯度并实现它。\n- 计算 $\\nabla g(\\beta)$ 的一个有效 Lipschitz 常数 $L$，即 $g(\\beta)$ 的 Hessian 矩阵的算子范数。\n- 实现 $h(\\beta)$ 的近端算子，即逐坐标应用的软阈值映射。\n- 以 $\\beta^0 = 0$ 为初值，用固定步长 $s$ 实现近端梯度迭代 $\\beta^{k+1} = \\text{prox}_{s h}\\!\\left(\\beta^k - s \\nabla g(\\beta^k)\\right)$。\n- 对于每个测试用例，根据应用于 $1/L$ 的给定乘法因子选择步长 $s$，并研究其对目标函数单调性和收敛性的影响。\n\n使用以下数据集和参数化测试用例。所有具体的数值数组和标量都在下面提供。每个数据集由一个标识符 $d \\in \\{1,2\\}$ 表示。\n\n数据集 $d=1$：\n- $n = 6$, $p = 3$。\n- $X = \\begin{bmatrix}\n1.0  0.0  1.0\\\\\n0.0  1.0  1.0\\\\\n1.0  1.0  0.0\\\\\n1.0  2.0  1.0\\\\\n2.0  1.0  1.0\\\\\n0.0  0.0  1.0\n\\end{bmatrix}$，其中所有条目都在 $\\mathbb{R}$ 中：具体来说，$1.0$, $0.0$, $1.0$, $0.0$, $1.0$, $1.0$, $1.0$, $2.0$, $1.0$, $2.0$, $1.0$, $1.0$, $0.0$, $0.0$, $1.0$ 对应于显示的位置。\n- $y = \\begin{bmatrix}1.2\\\\0.7\\\\1.0\\\\2.3\\\\2.1\\\\0.3\\end{bmatrix}$。\n\n数据集 $d=2$：\n- $n = 5$, $p = 4$。\n- $X = \\begin{bmatrix}\n1.0  2.1  0.1  3.1\\\\\n2.0  4.0  -0.2  6.0\\\\\n3.0  6.2  0.05  9.2\\\\\n4.0  8.1  -0.1  12.1\\\\\n5.0  10.2  0.0  15.2\n\\end{bmatrix}$。\n- $y = \\begin{bmatrix}0.6\\\\1.0\\\\1.5\\\\2.1\\\\2.6\\end{bmatrix}$。\n\n对于每个测试用例，执行以下操作：\n- 根据数据集标识符 $d$ 设置 $(X,y)$。\n- 计算 $\\nabla g(\\beta)$ 的 Lipschitz 常数 $L$。\n- 设置步长 $s = \\text{step\\_factor} \\times \\frac{1}{L}$。\n- 初始化 $\\beta^0 = \\mathbf{0} \\in \\mathbb{R}^p$ 并迭代 $\\beta^{k+1} = \\text{prox}_{s h}\\!\\left(\\beta^k - s \\nabla g(\\beta^k)\\right)$，最多进行 $\\text{max\\_iters}$ 次迭代。\n- 跟踪目标值序列 $F(\\beta^k)$。确定两个布尔值：\n    - $\\text{is\\_monotone}$：目标序列在终止前是否在小的数值公差内非递增（对于所有迭代，接受 $F(\\beta^{k+1}) \\le F(\\beta^k) + \\epsilon$，其中 $\\epsilon = 10^{-12}$）。\n    - $\\text{converged}$：迭代是否在达到 $\\text{max\\_iters}$ 之前满足相对不动点公差 $\\lVert \\beta^{k+1} - \\beta^k \\rVert_2 \\le \\text{tol} \\cdot \\max(1, \\lVert \\beta^k \\rVert_2)$。\n\n使用以下测试套件。每个测试用例是一个元组 $(d,\\lambda_1,\\lambda_2,\\text{step\\_factor},\\text{max\\_iters},\\text{tol})$，所有实数值条目都明确写出：\n- 测试 1：$(1, 0.2, 0.1, 0.9, 1000, 1\\times 10^{-8})$。\n- 测试 2：$(1, 0.2, 0.1, 1.0, 1000, 1\\times 10^{-8})$。\n- 测试 3：$(1, 0.2, 0.1, 1.5, 2000, 1\\times 10^{-8})$。\n- 测试 4：$(1, 0.2, 0.1, 2.1, 200, 1\\times 10^{-8})$。\n- 测试 5：$(2, 1.0, 0.0, 0.9, 1000, 1\\times 10^{-8})$。\n\n你的程序必须：\n- 按规定实现近端梯度法，从 $X$ 和 $\\lambda_2$ 计算 $L$，并为每个测试用例使用相应的 $s$ 运行迭代。\n- 对于每个测试，返回一个布尔值列表 $[\\text{is\\_monotone}, \\text{converged}]$。\n- 将所有测试的结果按上述顺序汇总到一个列表中。\n\n最终输出格式：\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个元素对应一个测试用例，并且本身是一个双元素列表 $[\\text{is\\_monotone}, \\text{converged}]$。例如，要求的格式类似于 $[[\\text{True},\\text{True}],[\\text{True},\\text{False}],\\ldots]$。不应打印其他任何文本。",
            "solution": "该问题要求实现并分析用于解决弹性网络正则化线性回归问题的近端梯度法。需要最小化的目标函数是一个复合凸函数，形式为 $F(\\beta) = g(\\beta) + h(\\beta)$，其中 $g(\\beta)$ 是一个可微的凸函数，而 $h(\\beta)$ 是一个凸但可能不可微的函数。\n\n具体组成部分定义如下：\n1.  光滑部分 $g(\\beta)$ 是正则化的最小二乘损失：\n    $$g(\\beta) = \\frac{1}{2n}\\lVert X\\beta - y\\rVert_2^2 + \\frac{\\lambda_2}{2}\\lVert \\beta \\rVert_2^2$$\n    其中 $X \\in \\mathbb{R}^{n \\times p}$ 是设计矩阵，$y \\in \\mathbb{R}^{n}$ 是响应向量，$\\beta \\in \\mathbb{R}^p$ 是模型系数，$n$ 是样本数量，$p$ 是特征数量，$\\lambda_2 \\ge 0$ 是 $\\ell_2$ 正则化参数。\n\n2.  非光滑部分 $h(\\beta)$ 是 $\\ell_1$ 惩罚项：\n    $$h(\\beta) = \\lambda_1 \\lVert \\beta \\rVert_1 = \\lambda_1 \\sum_{j=1}^p |\\beta_j|$$\n    其中 $\\lambda_1 \\ge 0$ 是 $\\ell_1$ 正则化参数。\n\n近端梯度法是一种非常适合此类复合优化问题的迭代算法。其核心思想是将在光滑部分 $g(\\beta)$ 上的梯度下降步与在非光滑部分 $h(\\beta)$ 上的近端步相结合。\n\n以下是该算法的逐步推导和设计。\n\n**步骤 1：光滑部分的梯度, $\\nabla g(\\beta)$**\n\n为了对 $g(\\beta)$ 执行梯度步，我们首先需要它关于 $\\beta$ 的梯度。我们可以展开 $g(\\beta)$ 中的各项：\n$$g(\\beta) = \\frac{1}{2n}(X\\beta - y)^T(X\\beta - y) + \\frac{\\lambda_2}{2}\\beta^T\\beta$$\n$$g(\\beta) = \\frac{1}{2n}(\\beta^T X^T X \\beta - 2y^T X \\beta + y^T y) + \\frac{\\lambda_2}{2}\\beta^T\\beta$$\n对每一项关于 $\\beta$ 求梯度，得到：\n$$\\nabla g(\\beta) = \\frac{1}{2n}(2X^T X \\beta - 2X^T y) + \\frac{\\lambda_2}{2}(2\\beta)$$\n$$\\nabla g(\\beta) = \\frac{1}{n}X^T(X\\beta - y) + \\lambda_2 \\beta$$\n这个公式计算效率高，并将在算法中使用。\n\n**步骤 2：梯度的 Lipschitz 常数, $L$**\n\n近端梯度法的收敛性取决于步长，而步长又由 $\\nabla g(\\beta)$ 的 Lipschitz 常数决定。如果对于任意 $\\beta_1, \\beta_2 \\in \\mathbb{R}^p$，都有 $\\lVert \\nabla g(\\beta_1) - \\nabla g(\\beta_2) \\rVert_2 \\le L \\lVert \\beta_1 - \\beta_2 \\rVert_2$，则函数 $\\nabla g$ 是 $L$-Lipschitz 连续的。一个有效的此类常数 $L$ 可以取为 $g(\\beta)$ 的 Hessian 矩阵的谱范数。\n\nHessian 矩阵 $\\nabla^2 g(\\beta)$ 是通过对 $\\nabla g(\\beta)$ 关于 $\\beta$ 求导得到的：\n$$\\nabla^2 g(\\beta) = \\nabla_\\beta \\left( \\frac{1}{n}X^T X \\beta - \\frac{1}{n}X^T y + \\lambda_2 \\beta \\right) = \\frac{1}{n}X^T X + \\lambda_2 I$$\n其中 $I$ 是 $p \\times p$ 的单位矩阵。由于 Hessian 矩阵相对于 $\\beta$ 是常数，最紧的 Lipschitz 常数是其谱范数（最大特征值的绝对值）：\n$$L = \\left\\lVert \\frac{1}{n}X^T X + \\lambda_2 I \\right\\rVert_2$$\n矩阵 $\\frac{1}{n}X^T X + \\lambda_2 I$ 是对称半正定的。其特征值为 $\\frac{1}{n}\\sigma_i^2(X) + \\lambda_2$，其中 $\\sigma_i(X)$ 是 $X$ 的奇异值。因此，谱范数是最大的特征值：\n$$L = \\frac{1}{n} \\max_i(\\sigma_i^2(X)) + \\lambda_2 = \\frac{1}{n} \\lVert X \\rVert_2^2 + \\lambda_2$$\n其中 $\\lVert X \\rVert_2$ 是 $X$ 的谱范数。我们将为每个数据集计算该值以确定步长。\n\n**步骤 3：非光滑部分的近端算子, $\\text{prox}_{sh}(\\cdot)$**\n\n带有参数 $\\alpha > 0$ 的函数 $\\phi$ 的近端算子定义为：\n$$\\text{prox}_{\\alpha\\phi}(v) = \\arg\\min_u \\left( \\phi(u) + \\frac{1}{2\\alpha} \\lVert u - v \\rVert_2^2 \\right)$$\n对于我们的问题，函数是 $h(\\beta) = \\lambda_1 \\lVert \\beta \\rVert_1$，参数是步长 $s$。近端算子是 $\\text{prox}_{s h}(\\cdot)$。由于 $\\ell_1$-范数是可分的，我们可以独立地求解每个分量 $\\beta_j$：\n$$\\arg\\min_{\\beta_j} \\left( \\lambda_1 |\\beta_j| + \\frac{1}{2s} (\\beta_j - v_j)^2 \\right)$$\n其解是软阈值算子 $\\mathcal{S}_{s\\lambda_1}(\\cdot)$：\n$$[\\text{prox}_{sh}(v)]_j = \\mathcal{S}_{s\\lambda_1}(v_j) = \\text{sign}(v_j) \\max(|v_j| - s\\lambda_1, 0)$$\n此算子逐元素地应用于梯度步产生的向量。\n\n**步骤 4：近端梯度迭代**\n\n近端梯度法将这些组件组合成一个单一的迭代更新规则。从初始猜测 $\\beta^0$ 开始，第 $(k+1)$ 次迭代计算如下：\n$$\\beta^{k+1} = \\text{prox}_{s h}\\!\\left(\\beta^k - s \\nabla g(\\beta^k)\\right)$$\n这可以分解为两个概念性步骤：\n1.  **梯度步：** $z^k = \\beta^k - s \\nabla g(\\beta^k)$\n2.  **近端步：** $\\beta^{k+1} = \\text{prox}_{sh}(z^k) = \\mathcal{S}_{s\\lambda_1}(z^k)$\n\n**步骤 5：算法实现与步长分析**\n\n该算法对每个测试用例的实现如下：\n1.  初始化 $\\beta^0 = \\mathbf{0} \\in \\mathbb{R}^p$。\n2.  计算 Lipschitz 常数 $L = \\frac{1}{n} \\lVert X \\rVert_2^2 + \\lambda_2$。\n3.  设置步长 $s = \\text{step\\_factor} \\times \\frac{1}{L}$。\n4.  迭代 $k = 0, 1, 2, \\ldots, \\text{max\\_iters}-1$：\n    a. 计算梯度 $\\nabla g(\\beta^k) = \\frac{1}{n}X^T(X\\beta^k - y) + \\lambda_2 \\beta^k$。\n    b. 更新 $\\beta^{k+1} = \\mathcal{S}_{s\\lambda_1}(\\beta^k - s \\nabla g(\\beta^k))$。\n    c. 跟踪目标函数值 $F(\\beta^k) = \\frac{1}{2n}\\lVert X\\beta^k - y\\rVert_2^2 + \\frac{\\lambda_2}{2}\\lVert \\beta^k \\rVert_2^2 + \\lambda_1 \\lVert \\beta^k \\rVert_1$。\n    d. 检查单调性：序列 $F(\\beta^k)$ 必须是非递增的，即对于一个小的公差 $\\epsilon=10^{-12}$，有 $F(\\beta^{k+1}) \\le F(\\beta^k) + \\epsilon$。\n    e. 检查收敛性：如果 $\\beta$ 的相对变化很小，即 $\\lVert \\beta^{k+1} - \\beta^k \\rVert_2 \\le \\text{tol} \\cdot \\max(1, \\lVert \\beta^k \\rVert_2)$，则终止。\n\n步长 $s$ 的选择至关重要。近端梯度法的收敛理论保证，如果步长满足 $0  s \\le 1/L$，目标函数 $F(\\beta^k)$ 会单调递减。\n- 对于 $\\text{step\\_factor} \\in \\{0.9, 1.0\\}$，我们有 $s \\le 1/L$，因此我们期望单调收敛（`is_monotone` = True）。\n- 对于 $\\text{step\\_factor} = 1.5$，我们有 $s = 1.5/L$。这里，$1/L  s  2/L$。仍然保证收敛到最小值，但不保证目标函数值在每次迭代中都单调递减。我们可能会观察到某些 $k$ 使得 $F(\\beta^{k+1}) > F(\\beta^k)$，从而导致 `is_monotone` = False。\n- 对于 $\\text{step\\_factor} = 2.1$，我们有 $s > 2/L$。算法不保证收敛，并且预计会发散。这将表现为非单调、递增的目标值，并且无法满足收敛容差，导致 `is_monotone` = False 和 `converged` = False。\n\n这个框架使我们能够系统地实现算法，并验证步长选择对收敛性和单调性的理论性质。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Solves the elastic net problem using the proximal gradient method for a suite of test cases.\n    \"\"\"\n    \n    # Define datasets\n    d1_X = np.array([\n        [1.0, 0.0, 1.0], [0.0, 1.0, 1.0], [1.0, 1.0, 0.0],\n        [1.0, 2.0, 1.0], [2.0, 1.0, 1.0], [0.0, 0.0, 1.0]\n    ])\n    d1_y = np.array([1.2, 0.7, 1.0, 2.3, 2.1, 0.3])\n\n    d2_X = np.array([\n        [1.0, 2.1, 0.1, 3.1], [2.0, 4.0, -0.2, 6.0], [3.0, 6.2, 0.05, 9.2],\n        [4.0, 8.1, -0.1, 12.1], [5.0, 10.2, 0.0, 15.2]\n    ])\n    d2_y = np.array([0.6, 1.0, 1.5, 2.1, 2.6])\n\n    datasets = {1: (d1_X, d1_y), 2: (d2_X, d2_y)}\n\n    # Define test suite\n    test_cases = [\n        # (d, lambda1, lambda2, step_factor, max_iters, tol)\n        (1, 0.2, 0.1, 0.9, 1000, 1e-8),\n        (1, 0.2, 0.1, 1.0, 1000, 1e-8),\n        (1, 0.2, 0.1, 1.5, 2000, 1e-8),\n        (1, 0.2, 0.1, 2.1, 200, 1e-8),\n        (2, 1.0, 0.0, 0.9, 1000, 1e-8)\n    ]\n\n    def soft_thresholding(v, t):\n        \"\"\"Soft-thresholding operator.\"\"\"\n        return np.sign(v) * np.maximum(np.abs(v) - t, 0)\n\n    def run_proximal_gradient(X, y, lambda1, lambda2, step_factor, max_iters, tol):\n        \"\"\"\n        Runs the proximal gradient algorithm for elastic net.\n        \"\"\"\n        n, p = X.shape\n        beta = np.zeros(p)\n        \n        # Compute Lipschitz constant L\n        # L = (1/n) * ||X'X||_2 + lambda2 = (1/n) * ||X||_2^2 + lambda2\n        norm_X_sq = np.linalg.norm(X, 2)**2\n        L = (1.0 / n) * norm_X_sq + lambda2\n        \n        # Set step size s\n        s = step_factor / L\n        \n        is_monotone = True\n        converged = False\n        \n        # Objective function F(beta)\n        def objective(b):\n            loss = (1.0 / (2.0 * n)) * np.sum((X @ b - y)**2)\n            l2_reg = (lambda2 / 2.0) * np.sum(b**2)\n            l1_reg = lambda1 * np.sum(np.abs(b))\n            return loss + l2_reg + l1_reg\n\n        obj_val_k = objective(beta)\n        \n        for k in range(max_iters):\n            # Gradient of the smooth part g(beta)\n            grad_g = (1.0 / n) * X.T @ (X @ beta - y) + lambda2 * beta\n            \n            # Proximal gradient update\n            beta_k_plus_1 = soft_thresholding(beta - s * grad_g, s * lambda1)\n            \n            # Check for monotonicity\n            obj_val_k_plus_1 = objective(beta_k_plus_1)\n            if obj_val_k_plus_1 > obj_val_k + 1e-12:\n                is_monotone = False\n            \n            # Check for convergence\n            diff_norm = np.linalg.norm(beta_k_plus_1 - beta)\n            beta_norm = np.linalg.norm(beta)\n            if diff_norm = tol * max(1.0, beta_norm):\n                converged = True\n                break\n            \n            # Update for next iteration\n            beta = beta_k_plus_1\n            obj_val_k = obj_val_k_plus_1\n            \n        return [is_monotone, converged]\n\n    results = []\n    for case in test_cases:\n        d, lambda1, lambda2, step_factor, max_iters, tol = case\n        X, y = datasets[d]\n        result = run_proximal_gradient(X, y, lambda1, lambda2, step_factor, max_iters, tol)\n        results.append(result)\n        \n    # The required format is [[True,True],[True,False],...]\n    # Using str() and replace() to match the exact format without extra spaces.\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```"
        }
    ]
}