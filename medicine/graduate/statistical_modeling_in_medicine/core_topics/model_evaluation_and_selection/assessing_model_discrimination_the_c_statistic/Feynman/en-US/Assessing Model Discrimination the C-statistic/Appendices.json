{
    "hands_on_practices": [
        {
            "introduction": "To truly grasp the c-statistic, it is essential to understand its definition as a probability of concordance. This first practice exercise  strips the concept down to its core by having you manually enumerate all possible case-control pairs from a small, hypothetical dataset. By counting concordant, discordant, and tied pairs, you will directly compute the c-statistic and gain an intuitive feel for what this metric represents.",
            "id": "4952017",
            "problem": "A clinical prediction model for $30$-day all-cause mortality in sepsis was developed and evaluated on an out-of-sample validation cohort. The model outputs predicted probabilities for each patient. For the purpose of assessing discrimination via the concordance statistic (also known as the Area Under the Receiver Operating Characteristic (ROC) curve), focus on a subset of patients comprised of $n_{1}=5$ observed deaths (cases) and $n_{0}=6$ survivors (controls). The predicted probabilities for the cases are $0.82$, $0.61$, $0.34$, $0.77$, and $0.50$. The predicted probabilities for the controls are $0.40$, $0.61$, $0.29$, $0.73$, $0.50$, and $0.15$.\n\nAssume the following principles:\n- Patients are independently sampled and scored, and the model produces valid probability scores in $[0,1]$ that can be used for ranking.\n- The concordance statistic is interpreted as the probability that a randomly selected case has a strictly higher predicted probability than a randomly selected control, with exact ties between a case and a control contributing symmetrically.\n\nCompute the concordance statistic for this subset by explicitly enumerating all case–control pairs and assessing whether each pair is concordant, discordant, or tied. Report the final value as a decimal in $[0,1]$ rounded to four significant figures.",
            "solution": "The problem requires the computation of the concordance statistic (c-statistic), also known as the Area Under the Receiver Operating Characteristic curve (AUC), for a given set of predicted probabilities for cases and controls.\n\nFirst, we identify the provided data.\nThe number of cases (deaths) is $n_{1} = 5$.\nThe number of controls (survivors) is $n_{0} = 6$.\nThe set of predicted probabilities for the cases is $P_{1} = \\{0.82, 0.61, 0.34, 0.77, 0.50\\}$.\nThe set of predicted probabilities for the controls is $P_{0} = \\{0.40, 0.61, 0.29, 0.73, 0.50, 0.15\\}$.\n\nThe c-statistic is defined as the probability that a randomly selected case will have a higher predicted probability than a randomly selected control. When ties occur, they are to be handled symmetrically. This means each tied pair contributes $0.5$ to the count of \"favorable\" outcomes.\n\nThe total number of possible case-control pairs is the product of the number of cases and the number of controls:\n$$ N_{\\text{total}} = n_{1} \\times n_{0} = 5 \\times 6 = 30 $$\n\nThe formula for the c-statistic is:\n$$ C = \\frac{N_{C} + (0.5 \\times N_{T})}{N_{\\text{total}}} $$\nwhere $N_{C}$ is the number of concordant pairs (case probability is strictly greater than control probability) and $N_{T}$ is the number of tied pairs (case probability is equal to control probability).\n\nTo find $N_{C}$ and $N_{T}$, we must systematically compare each of the $5$ case probabilities against each of the $6$ control probabilities.\n\nLet's denote a case probability as $p_{\\text{case}}$ and a control probability as $p_{\\text{control}}$. A pair is:\n- Concordant if $p_{\\text{case}} > p_{\\text{control}}$.\n- Tied if $p_{\\text{case}} = p_{\\text{control}}$.\n- Discordant if $p_{\\text{case}} < p_{\\text{control}}$.\n\nWe will enumerate the comparisons for each case:\n\n1.  For case probability $p_{\\text{case}} = 0.82$:\n    - vs $0.40$ (Concordant)\n    - vs $0.61$ (Concordant)\n    - vs $0.29$ (Concordant)\n    - vs $0.73$ (Concordant)\n    - vs $0.50$ (Concordant)\n    - vs $0.15$ (Concordant)\n    This case contributes $6$ concordant pairs and $0$ tied pairs.\n\n2.  For case probability $p_{\\text{case}} = 0.61$:\n    - vs $0.40$ (Concordant)\n    - vs $0.61$ (Tied)\n    - vs $0.29$ (Concordant)\n    - vs $0.73$ (Discordant)\n    - vs $0.50$ (Concordant)\n    - vs $0.15$ (Concordant)\n    This case contributes $4$ concordant pairs and $1$ tied pair.\n\n3.  For case probability $p_{\\text{case}} = 0.34$:\n    - vs $0.40$ (Discordant)\n    - vs $0.61$ (Discordant)\n    - vs $0.29$ (Concordant)\n    - vs $0.73$ (Discordant)\n    - vs $0.50$ (Discordant)\n    - vs $0.15$ (Concordant)\n    This case contributes $2$ concordant pairs and $0$ tied pairs.\n\n4.  For case probability $p_{\\text{case}} = 0.77$:\n    - vs $0.40$ (Concordant)\n    - vs $0.61$ (Concordant)\n    - vs $0.29$ (Concordant)\n    - vs $0.73$ (Concordant)\n    - vs $0.50$ (Concordant)\n    - vs $0.15$ (Concordant)\n    This case contributes $6$ concordant pairs and $0$ tied pairs.\n\n5.  For case probability $p_{\\text{case}} = 0.50$:\n    - vs $0.40$ (Concordant)\n    - vs $0.61$ (Discordant)\n    - vs $0.29$ (Concordant)\n    - vs $0.73$ (Discordant)\n    - vs $0.50$ (Tied)\n    - vs $0.15$ (Concordant)\n    This case contributes $3$ concordant pairs and $1$ tied pair.\n\nNow, we sum the counts from all comparisons:\nTotal number of concordant pairs, $N_{C} = 6 + 4 + 2 + 6 + 3 = 21$.\nTotal number of tied pairs, $N_{T} = 0 + 1 + 0 + 0 + 1 = 2$.\n\nThe number of discordant pairs is $30 - 21 - 2 = 7$, which is a useful check but not directly needed for the formula.\n\nWe can now substitute these values into the formula for the c-statistic:\n$$ C = \\frac{21 + (0.5 \\times 2)}{30} $$\n$$ C = \\frac{21 + 1}{30} $$\n$$ C = \\frac{22}{30} $$\n$$ C = \\frac{11}{15} $$\n\nTo provide the answer as a decimal rounded to four significant figures, we perform the division:\n$$ C = 0.733333... $$\nRounding to four significant figures gives $0.7333$.",
            "answer": "$$\n\\boxed{0.7333}\n$$"
        },
        {
            "introduction": "The c-statistic is numerically identical to the Area Under the Receiver Operating Characteristic (ROC) curve, a fundamental tool for visualizing model performance. This practice  bridges the gap between abstract scores and their graphical representation. You will construct an ROC curve step-by-step from raw prediction data and then calculate its area, solidifying the equivalence between the c-statistic and the AUC.",
            "id": "4951977",
            "problem": "A logistic regression model was developed to predict in-hospital sepsis mortality. For a hold-out set of $12$ patients, the model’s predicted probabilities $\\hat{p}$ and the observed outcomes $y \\in \\{0,1\\}$ (with $1$ indicating death) are given below. Construct the empirical Receiver Operating Characteristic (ROC) curve from these data using the fundamental definitions of the true positive rate and false positive rate, taking the classification rule “predict positive if $\\hat{p} \\ge t$” for a threshold $t$ that varies from above the maximum $\\hat{p}$ to below the minimum $\\hat{p}$. Treat all identical scores at a given threshold as entering simultaneously, and connect successive $(\\mathrm{FPR}, \\mathrm{TPR})$ points with straight line segments. Then compute the area under this empirical ROC curve using the trapezoidal rule.\n\nPatient-level data:\n- Patient $1$: $\\hat{p} = 0.80$, $y = 0$\n- Patient $2$: $\\hat{p} = 0.58$, $y = 0$\n- Patient $3$: $\\hat{p} = 0.95$, $y = 1$\n- Patient $4$: $\\hat{p} = 0.51$, $y = 0$\n- Patient $5$: $\\hat{p} = 0.72$, $y = 1$\n- Patient $6$: $\\hat{p} = 0.84$, $y = 0$\n- Patient $7$: $\\hat{p} = 0.66$, $y = 0$\n- Patient $8$: $\\hat{p} = 0.78$, $y = 1$\n- Patient $9$: $\\hat{p} = 0.89$, $y = 1$\n- Patient $10$: $\\hat{p} = 0.71$, $y = 0$\n- Patient $11$: $\\hat{p} = 0.51$, $y = 0$\n- Patient $12$: $\\hat{p} = 0.73$, $y = 1$\n\nUse only the definitions of the true positive rate $\\mathrm{TPR} = \\mathrm{TP}/P$ and false positive rate $\\mathrm{FPR} = \\mathrm{FP}/N$ (with $P$ the number of positives and $N$ the number of negatives) and the trapezoidal rule for numerical integration of a piecewise-linear curve. Round your final numeric answer to four significant figures. Express the final area as a pure decimal (dimensionless).",
            "solution": "The problem is to construct the empirical Receiver Operating Characteristic (ROC) curve from a given set of patient data and then compute the area under this curve (AUC).\n\n### Step 1: Data Preparation and ROC Curve Construction\n\nFirst, we count the total number of actual positive cases ($P$) and actual negative cases ($N$) in the dataset.\n- Positive cases ($y=1$): Patients $3, 5, 8, 9, 12$. Thus, $P=5$.\n- Negative cases ($y=0$): Patients $1, 2, 4, 6, 7, 10, 11$. Thus, $N=7$.\nThe total number of patients is $P+N = 5+7=12$, which matches the problem statement.\n\nTo construct the ROC curve, we sort the patients by their predicted probability $\\hat{p}$ in descending order, keeping track of their true outcome $y$.\n1.  P3: $\\hat{p} = 0.95, y = 1$\n2.  P9: $\\hat{p} = 0.89, y = 1$\n3.  P6: $\\hat{p} = 0.84, y = 0$\n4.  P1: $\\hat{p} = 0.80, y = 0$\n5.  P8: $\\hat{p} = 0.78, y = 1$\n6.  P12: $\\hat{p} = 0.73, y = 1$\n7.  P5: $\\hat{p} = 0.72, y = 1$\n8.  P10: $\\hat{p} = 0.71, y = 0$\n9.  P7: $\\hat{p} = 0.66, y = 0$\n10. P2: $\\hat{p} = 0.58, y = 0$\n11. P4: $\\hat{p} = 0.51, y = 0$ (Tied)\n12. P11: $\\hat{p} = 0.51, y = 0$ (Tied)\n\nThe ROC curve is a plot of $\\mathrm{TPR}$ (y-axis) versus $\\mathrm{FPR}$ (x-axis) for varying thresholds $t$. The curve starts at $(0,0)$ with a very high threshold (e.g., $t>0.95$), where no patients are classified as positive, so $\\mathrm{TP}=0$ and $\\mathrm{FP}=0$. As we lower the threshold, we classify more patients as positive.\n- When the threshold crosses the score of a true positive ($y=1$), $\\mathrm{TP}$ increases by $1$, and TPR increases by $\\frac{1}{P} = \\frac{1}{5}$. This corresponds to a vertical step up on the ROC plot.\n- When the threshold crosses the score of a true negative ($y=0$), $\\mathrm{FP}$ increases by $1$, and FPR increases by $\\frac{1}{N} = \\frac{1}{7}$. This corresponds to a horizontal step to the right.\n- For tied scores, we update the counts simultaneously. The tied scores at $\\hat{p}=0.51$ are both negatives, so FP increases by $2$, and FPR increases by $\\frac{2}{7}$ in a single step.\n\nFollowing the sorted list, we generate the vertices of the ROC curve:\n- Start at $P_0 = (0, 0)$.\n- P3 ($y=1$): $\\mathrm{TP}=1$. Point $P_1 = (0, \\frac{1}{5})$.\n- P9 ($y=1$): $\\mathrm{TP}=2$. Point $P_2 = (0, \\frac{2}{5})$.\n- P6 ($y=0$): $\\mathrm{FP}=1$. Point $P_3 = (\\frac{1}{7}, \\frac{2}{5})$.\n- P1 ($y=0$): $\\mathrm{FP}=2$. Point $P_4 = (\\frac{2}{7}, \\frac{2}{5})$.\n- P8 ($y=1$): $\\mathrm{TP}=3$. Point $P_5 = (\\frac{2}{7}, \\frac{3}{5})$.\n- P12 ($y=1$): $\\mathrm{TP}=4$. Point $P_6 = (\\frac{2}{7}, \\frac{4}{5})$.\n- P5 ($y=1$): $\\mathrm{TP}=5$. Point $P_7 = (\\frac{2}{7}, \\frac{5}{5}) = (\\frac{2}{7}, 1)$.\n- P10 ($y=0$): $\\mathrm{FP}=3$. Point $P_8 = (\\frac{3}{7}, 1)$.\n- P7 ($y=0$): $\\mathrm{FP}=4$. Point $P_9 = (\\frac{4}{7}, 1)$.\n- P2 ($y=0$): $\\mathrm{FP}=5$. Point $P_{10} = (\\frac{5}{7}, 1)$.\n- P4 & P11 ($y=0$): $\\mathrm{FP}=5+2=7$. Point $P_{11} = (\\frac{7}{7}, 1) = (1, 1)$.\n\nSo, the ROC curve is a piecewise-linear path connecting the following sequence of points $(\\mathrm{FPR}_i, \\mathrm{TPR}_i)$:\n$P_0 = (0, 0)$, $P_1 = (0, \\frac{1}{5})$, $P_2 = (0, \\frac{2}{5})$, $P_3 = (\\frac{1}{7}, \\frac{2}{5})$, $P_4 = (\\frac{2}{7}, \\frac{2}{5})$, $P_5 = (\\frac{2}{7}, \\frac{3}{5})$, $P_6 = (\\frac{2}{7}, \\frac{4}{5})$, $P_7 = (\\frac{2}{7}, 1)$, $P_8 = (\\frac{3}{7}, 1)$, $P_9 = (\\frac{4}{7}, 1)$, $P_{10} = (\\frac{5}{7}, 1)$, $P_{11} = (1, 1)$.\n\n### Step 2: Calculation of the Area Under the Curve (AUC)\n\nThe AUC is calculated using the trapezoidal rule for the area under the piecewise-linear curve defined by the points $(x_i, y_i) = (\\mathrm{FPR}_i, \\mathrm{TPR}_i)$. The formula for the area is:\n$$ \\mathrm{AUC} = \\sum_{i=1}^{k} \\frac{1}{2} (y_{i-1} + y_i) (x_i - x_{i-1}) $$\nwhere $k$ is the number of points after the origin ($k=11$).\n\nWe calculate the area of each trapezoid formed by successive points:\n- $P_0 \\to P_1$: $\\Delta x = 0 - 0 = 0$. Area = $0$.\n- $P_1 \\to P_2$: $\\Delta x = 0 - 0 = 0$. Area = $0$.\n- $P_2 \\to P_3$: $\\frac{1}{2}(\\frac{2}{5} + \\frac{2}{5})(\\frac{1}{7} - 0) = \\frac{2}{5} \\times \\frac{1}{7} = \\frac{2}{35}$.\n- $P_3 \\to P_4$: $\\frac{1}{2}(\\frac{2}{5} + \\frac{2}{5})(\\frac{2}{7} - \\frac{1}{7}) = \\frac{2}{5} \\times \\frac{1}{7} = \\frac{2}{35}$.\n- $P_4 \\to P_5$: $\\Delta x = \\frac{2}{7} - \\frac{2}{7} = 0$. Area = $0$.\n- $P_5 \\to P_6$: $\\Delta x = 0$. Area = $0$.\n- $P_6 \\to P_7$: $\\Delta x = 0$. Area = $0$.\n- $P_7 \\to P_8$: $\\frac{1}{2}(1 + 1)(\\frac{3}{7} - \\frac{2}{7}) = 1 \\times \\frac{1}{7} = \\frac{1}{7} = \\frac{5}{35}$.\n- $P_8 \\to P_9$: $\\frac{1}{2}(1 + 1)(\\frac{4}{7} - \\frac{3}{7}) = 1 \\times \\frac{1}{7} = \\frac{1}{7} = \\frac{5}{35}$.\n- $P_9 \\to P_{10}$: $\\frac{1}{2}(1 + 1)(\\frac{5}{7} - \\frac{4}{7}) = 1 \\times \\frac{1}{7} = \\frac{1}{7} = \\frac{5}{35}$.\n- $P_{10} \\to P_{11}$: $\\frac{1}{2}(1 + 1)(1 - \\frac{5}{7}) = 1 \\times \\frac{2}{7} = \\frac{2}{7} = \\frac{10}{35}$.\n\nThe total AUC is the sum of these areas:\n$$ \\mathrm{AUC} = 0 + 0 + \\frac{2}{35} + \\frac{2}{35} + 0 + 0 + 0 + \\frac{5}{35} + \\frac{5}{35} + \\frac{5}{35} + \\frac{10}{35} $$\n$$ \\mathrm{AUC} = \\frac{2+2+5+5+5+10}{35} = \\frac{29}{35} $$\n\nFinally, we convert this fraction to a decimal and round to four significant figures as requested.\n$$ \\mathrm{AUC} = \\frac{29}{35} \\approx 0.8285714... $$\nRounding to four significant figures gives $0.8286$.",
            "answer": "$$\\boxed{0.8286}$$"
        },
        {
            "introduction": "While the c-statistic is a premier measure of model discrimination, it is crucial to recognize what it does not measure: calibration, or the absolute accuracy of predicted probabilities. This exercise  explores this distinction through a carefully constructed counterexample where a model with superior discrimination can have worse overall accuracy. This highlights the importance of using a suite of metrics to fully evaluate a clinical prediction model.",
            "id": "4951982",
            "problem": "A clinical risk prediction study aims to compare two binary outcome models on a small test cohort. Let the observed outcomes be $y_i \\in \\{0,1\\}$, where $y_i=1$ denotes event occurrence and $y_i=0$ denotes no event. For each model, the predicted risk for patient $i$ is a probability $\\hat{p}_i \\in [0,1]$. The concordance statistic (c-statistic) is operationally assessed as the area under the receiver operating characteristic curve (AUC), interpreted as the probability that a randomly chosen event case receives a higher predicted risk than a randomly chosen non-event control, with ties contributing half-credit. The Brier score is defined as the mean squared error between predicted probabilities and observed outcomes.\n\nSelect the single option that presents a valid counterexample in which model $M_1$ has strictly higher discrimination, quantified by a strictly larger $\\mathrm{AUC}$, yet a strictly worse overall accuracy by a strictly larger Brier score than comparator model $M_2$. The correct option must also provide a correct explanation for why this divergence can occur in medical risk modeling.\n\nAll options below refer to the same test cohort of $n=4$ patients with outcomes $y=(1,1,0,0)$ in this order.\n\nA. Predictions: $M_1$: $(0.80,\\,0.80,\\,0.70,\\,0.70)$; $M_2$: $(0.50,\\,0.50,\\,0.50,\\,0.50)$. Explanation: $M_1$ ranks all cases above all controls, giving strictly higher $\\mathrm{AUC}$ than $M_2$, but $M_1$ is miscalibrated for controls (overconfident high probabilities for non-events), increasing squared error. This occurs because $\\mathrm{AUC}$ depends only on ranks whereas the Brier score penalizes probability magnitude and calibration.\n\nB. Predictions: $M_1$: $(0.90,\\,0.60,\\,0.30,\\,0.20)$; $M_2$: $(0.80,\\,0.75,\\,0.40,\\,0.35)$. Explanation: $M_1$ has higher $\\mathrm{AUC}$ but worse Brier due to overconfidence; this shows that $\\mathrm{AUC}$ and Brier necessarily trade off.\n\nC. Predictions: $M_1$: $(0.60,\\,0.55,\\,0.61,\\,0.58)$; $M_2$: $(0.62,\\,0.63,\\,0.40,\\,0.39)$. Explanation: This difference arises because the c-statistic is sensitive to calibration level.\n\nD. Predictions: $M_1$: $(0.95,\\,0.95,\\,0.60,\\,0.60)$; $M_2$: $(0.80,\\,0.80,\\,0.20,\\,0.20)$. Explanation: Any increase in $\\mathrm{AUC}$ must imply a decrease in the Brier score, so $M_1$ necessarily has better Brier than $M_2$.",
            "solution": "We begin from first principles. The c-statistic interpreted as area under the receiver operating characteristic curve (AUC) equals the probability of concordance between a case and a control: for a randomly chosen case with predicted risk $s^{+}$ and a randomly chosen control with predicted risk $s^{-}$, $\\mathrm{AUC} = \\mathbb{P}(s^{+} > s^{-}) + \\tfrac{1}{2}\\mathbb{P}(s^{+} = s^{-})$. On a finite dataset with $n_1$ cases and $n_0$ controls, $\\mathrm{AUC}$ is the fraction of the $n_1 n_0$ case–control pairs that are concordant, plus half the fraction that are tied. The Brier score for probabilistic predictions $\\hat{p}_i$ on binary outcomes $y_i$ is $n^{-1}\\sum_{i=1}^n (\\hat{p}_i - y_i)^2$, which is a strictly proper scoring rule that penalizes miscalibration and lack of accuracy in the magnitude of predicted probabilities.\n\nWe analyze each option by computing both metrics.\n\nOption A:\n- Outcomes are $y=(1,1,0,0)$.\n- $M_1$ predictions: $(0.80, 0.80, 0.70, 0.70)$.\n- $M_2$ predictions: $(0.50, 0.50, 0.50, 0.50)$.\nCompute $\\mathrm{AUC}$ for $M_1$: There are $n_1=2$ cases and $n_0=2$ controls, yielding $n_1 n_0 = 4$ case–control pairs. Every case prediction $0.80$ is greater than every control prediction $0.70$, so all $4$ pairs are concordant and there are no ties. Thus $\\mathrm{AUC}_{M_1} = 1$.\nCompute $\\mathrm{AUC}$ for $M_2$: Every prediction equals $0.50$, so all $4$ case–control pairs are ties. By the tie rule, $\\mathrm{AUC}_{M_2} = \\tfrac{1}{2}$.\nCompute Brier scores:\n- $M_1$ case errors: $(1-0.80)^2 = 0.04$ and $(1-0.80)^2 = 0.04$.\n- $M_1$ control errors: $(0-0.70)^2 = 0.49$ and $(0-0.70)^2 = 0.49$.\nTotal squared error sum for $M_1$ is $0.04+0.04+0.49+0.49 = 1.06$, so $\\mathrm{Brier}_{M_1} = \\tfrac{1.06}{4} = 0.265$.\n- $M_2$ errors: $(1-0.50)^2 = 0.25$ for each case and $(0-0.50)^2 = 0.25$ for each control, sum $1.00$, so $\\mathrm{Brier}_{M_2} = \\tfrac{1.00}{4} = 0.25$.\nConclusion for A: $M_1$ has strictly higher $\\mathrm{AUC}$ ($1 > 0.5$) and strictly worse Brier score ($0.265 > 0.25$). The explanation is correct: $\\mathrm{AUC}$ depends on ranks only, while the Brier score penalizes the magnitude of probabilities and calibration; here $M_1$ is overconfident for controls, increasing squared error despite perfect ranking.\n\nOption B:\n- $M_1$ predictions: $(0.90, 0.60, 0.30, 0.20)$; $M_2$ predictions: $(0.80, 0.75, 0.40, 0.35)$.\nCompute $\\mathrm{AUC}$ for $M_1$: All case predictions ($0.90, 0.60$) exceed all control predictions ($0.30, 0.20$), so $\\mathrm{AUC}_{M_1}=1$.\nCompute $\\mathrm{AUC}$ for $M_2$: All case predictions ($0.80, 0.75$) exceed all control predictions ($0.40, 0.35$), so $\\mathrm{AUC}_{M_2}=1$.\nCompute Brier scores:\n- $M_1$ case errors: $(1-0.90)^2=0.01$, $(1-0.60)^2=0.16$; control errors: $(0-0.30)^2=0.09$, $(0-0.20)^2=0.04$. Sum $0.01+0.16+0.09+0.04=0.30$, $\\mathrm{Brier}_{M_1} = 0.30/4 = 0.075$.\n- $M_2$ case errors: $(1-0.80)^2=0.04$, $(1-0.75)^2=0.0625$; control errors: $(0-0.40)^2=0.16$, $(0-0.35)^2=0.1225$. Sum $0.04+0.0625+0.16+0.1225=0.385$, $\\mathrm{Brier}_{M_2} = 0.385/4 \\approx 0.09625$.\nConclusion for B: $\\mathrm{AUC}$ values are equal (not strictly higher for $M_1$), and $M_1$ has a better (lower) Brier score, not worse. The explanation claiming a necessary trade-off is incorrect in general because the metrics assess different aspects (ranking versus probabilistic accuracy).\n\nOption C:\n- $M_1$ predictions: $(0.60, 0.55, 0.61, 0.58)$; $M_2$ predictions: $(0.62, 0.63, 0.40, 0.39)$.\nCompute $\\mathrm{AUC}$ for $M_1$: Case $0.60$ vs controls $0.61, 0.58$ yields $1$ concordant ($0.60>0.58$) and $1$ discordant ($0.60<0.61$). Case $0.55$ vs controls $0.61, 0.58$ yields $0$ concordant and $2$ discordant. Thus concordant $=1$, discordant $=3$, no ties, so $\\mathrm{AUC}_{M_1} = 1/4 = 0.25$.\nCompute $\\mathrm{AUC}$ for $M_2$: Cases $0.62, 0.63$ both exceed controls $0.40, 0.39$, giving all $4$ pairs concordant, so $\\mathrm{AUC}_{M_2}=1$.\nCompute Brier scores:\n- $M_1$ case errors: $(1-0.60)^2=0.16$, $(1-0.55)^2=0.2025$; control errors: $(0-0.61)^2=0.3721$, $(0-0.58)^2=0.3364$. Sum $0.16+0.2025+0.3721+0.3364=1.071$, $\\mathrm{Brier}_{M_1}=1.071/4=0.26775$.\n- $M_2$ case errors: $(1-0.62)^2=0.1444$, $(1-0.63)^2=0.1369$; control errors: $(0-0.40)^2=0.16$, $(0-0.39)^2=0.1521$. Sum $0.1444+0.1369+0.16+0.1521=0.5934$, $\\mathrm{Brier}_{M_2}=0.5934/4=0.14835$.\nConclusion for C: $M_1$ has strictly lower $\\mathrm{AUC}$ and strictly worse Brier score than $M_2$; it is not a counterexample. The explanation is also incorrect: the c-statistic is not sensitive to calibration level; it depends only on ranking.\n\nOption D:\n- $M_1$ predictions: $(0.95, 0.95, 0.60, 0.60)$; $M_2$ predictions: $(0.80, 0.80, 0.20, 0.20)$.\nCompute $\\mathrm{AUC}$: In both models, all case predictions exceed all control predictions, so $\\mathrm{AUC}_{M_1}=\\mathrm{AUC}_{M_2}=1$.\nCompute Brier scores:\n- $M_1$ case errors: $(1-0.95)^2=0.0025$ each, sum $0.005$; control errors: $(0-0.60)^2=0.36$ each, sum $0.72$. Total $0.725$, $\\mathrm{Brier}_{M_1}=0.725/4=0.18125$.\n- $M_2$ case errors: $(1-0.80)^2=0.04$ each, sum $0.08$; control errors: $(0-0.20)^2=0.04$ each, sum $0.08$. Total $0.16$, $\\mathrm{Brier}_{M_2}=0.16/4=0.04$.\nConclusion for D: $\\mathrm{AUC}$ values are equal, and $M_1$ has a worse Brier score than $M_2$, but the required condition of strictly higher $\\mathrm{AUC}$ for $M_1$ is not met. The explanation is also incorrect: increases in $\\mathrm{AUC}$ do not necessarily imply decreases in the Brier score.\n\nOverall conclusion: Only Option A presents a valid counterexample and a correct explanation. It concretely shows that $M_1$ can have strictly higher discrimination (perfect ranking) while suffering a worse Brier score due to miscalibration and overconfident predictions for non-events. This occurs because the c-statistic (AUC) is a rank-based measure of discrimination that is invariant to monotone transformations of scores, whereas the Brier score is a strictly proper scoring rule that evaluates the magnitude and calibration of predicted probabilities relative to the observed binary outcomes and the underlying event prevalence.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}