{
    "hands_on_practices": [
        {
            "introduction": "本次练习将巩固您对 AIC 基本计算的理解。我们将探讨一个因样本量较小而必须对标准 Akaike 信息准则 (AIC) 进行修正的场景，从而引出小样本修正的 AIC，即 AICc。这个实践将展示在给定的数据背景下，如何根据选择正确的准则来决定模型的取舍 ，并让您看到小样本修正为何至关重要，以及它如何能改变模型选择的最终结论。",
            "id": "4966101",
            "problem": "一个临床流行病学团队使用两个候选的高斯线性模型来模拟 $n=40$ 名成年人循环炎症生物标志物的对数。这些模型具有独立、同方差的误差。模型 $\\mathcal{M}_{1}$ 在似然函数中包含 $k_{1}=10$ 个自由估计的参数（回归系数、截距和未知的误差方差），而模型 $\\mathcal{M}_{2}$ 包含 $k_{2}=7$ 个此类参数。在模型 $\\mathcal{M}_{1}$ 下，观测到的残差平方和为 $\\mathrm{RSS}_{1}=270$，在模型 $\\mathcal{M}_{2}$ 下为 $\\mathrm{RSS}_{2}=320$。\n\n从一个基本出发点开始：通过信息准则进行模型选择旨在最小化由Kullback-Leibler散度度量的预期样本外差异；并且在高斯线性模型下，最大化对数似然来自于以最大似然法估计的方差的正态密度。请基于此推导出一个信息准则的表达式，该准则用于估计预期样本外偏差及其小样本偏差校正。使用这些推导，计算模型 $\\mathcal{M}_{1}$ 和 $\\mathcal{M}_{2}$ 的赤池信息准则（AIC）和小样本校正AIC（AICc）的数值，并确定每个准则偏好哪个模型（值越小表示越偏好）。\n\n将数量\n$$\\Delta_{\\mathrm{AICc}} \\equiv \\mathrm{AICc}(\\mathcal{M}_{1}) - \\mathrm{AICc}(\\mathcal{M}_{2}),$$\n四舍五入到四位有效数字，并以单个数字报告。不需要单位。",
            "solution": "用户的要求是解决一个统计建模问题。我将首先验证问题陈述。\n\n### 步骤1：提取已知条件\n- **样本量**：$n = 40$ 名成年人\n- **模型类型**：具有独立、同方差误差的高斯线性模型。\n- **模型 $\\mathcal{M}_{1}$**：\n    - 自由估计参数数量：$k_{1}=10$\n    - 残差平方和：$\\mathrm{RSS}_{1}=270$\n- **模型 $\\mathcal{M}_{2}$**：\n    - 自由估计参数数量：$k_{2}=7$\n    - 残差平方和：$\\mathrm{RSS}_{2}=320$\n- **目标**：\n    1. 推导一个信息准则的表达式，该准则用于估计预期样本外偏差及其小样本偏差校正。这对应于推导高斯模型的AIC和AICc形式。\n    2. 计算模型 $\\mathcal{M}_{1}$ 和 $\\mathcal{M}_{2}$ 的AIC和AICc。\n    3. 确定每个准则偏好哪个模型。\n    4. 计算最终数量 $\\Delta_{\\mathrm{AICc}} \\equiv \\mathrm{AICc}(\\mathcal{M}_{1}) - \\mathrm{AICc}(\\mathcal{M}_{2})$，并四舍五入到四位有效数字。\n\n### 步骤2：使用提取的已知条件进行验证\n- **科学依据**：该问题植根于基础统计理论，特别是应用于线性回归的信息论模型选择（Kullback-Leibler散度、AIC、AICc）。这是生物统计学和流行病学中一个标准且成熟的方法。其前提是科学合理的。\n- **适定性**：问题定义清晰，提供了进行所需推导和计算的所有必要数值（$n$、$k_1$、$k_2$、$\\mathrm{RSS}_1$、$\\mathrm{RSS}_2$）和理论背景。存在唯一解。\n- **客观性**：问题以精确、定量且无偏见的语言陈述。\n\n该问题没有表现出任何诸如科学不合理、不完整、矛盾或模糊的缺陷。这是一个在应用统计学中有效且适定的问题。\n\n### 步骤3：结论与行动\n问题是有效的。我将继续进行完整的推导和求解。\n\n***\n\n### 推导与求解\n\n信息论模型选择的目标是确定一个能最好地逼近未知的、真实的数据生成过程的模型。从真实过程到模型的先验指定的“距离”或“差异”通常用Kullback-Leibler（KL）散度来度量。对于一个真实的概率密度 $f(y)$ 和一个模型近似 $g(y|\\theta)$，KL散度由下式给出：\n$$D_{\\mathrm{KL}}(f || g) = \\int f(y) \\ln\\left(\\frac{f(y)}{g(y|\\theta)}\\right) dy$$\n最小化 $D_{\\mathrm{KL}}(f || g)$ 相对于模型等价于最大化 $\\int f(y) \\ln(g(y|\\theta)) dy$ 项，因为 $\\int f(y) \\ln(f(y)) dy$ 是一个仅依赖于真实（但未知）过程的常数。\n\n我们使用一个包含 $n$ 个观测值的数据集来估计模型参数，得到最大似然估计（MLE），$\\hat{\\theta}$。拟合模型的质量由其在来自同一过程的新的、独立数据上的预期表现来判断。这就是预期的样本外KL散度，或者等价地，预期的样本外对数似然。Akaike（1973）的一个基本结果表明，当前数据的最大化对数似然 $\\ln(L(\\hat{\\theta}|y))$ 是这个目标量的一个有偏估计量。Akaike 证明，通过校正这种偏差，可以获得预期相对KL散度的一个低偏差估计量。这就引出了赤池信息准则（AIC）。\n\nAIC是预期样本外对数似然乘以-2的估计量。其定义为：\n$$\\mathrm{AIC} = -2 \\ln(L(\\hat{\\theta})) + 2k$$\n其中 $L(\\hat{\\theta})$ 是最大化似然，$k$ 是模型中自由估计参数的数量。$2k$ 项是渐近偏差校正。\n\n对于指定的高斯线性模型，$n$ 个观测值的似然函数为：\n$$L(\\beta, \\sigma^2) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - x_i^T\\beta)^2\\right)$$\n对数似然为：\n$$\\ln(L(\\beta, \\sigma^2)) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{1}{2\\sigma^2} \\mathrm{RSS}(\\beta)$$\n其中 $\\mathrm{RSS}(\\beta) = \\sum_{i=1}^n (y_i - x_i^T\\beta)^2$。回归系数的MLE $\\hat{\\beta}$ 最小化残差平方和，得到观测到的 $\\mathrm{RSS}$。方差的MLE是 $\\hat{\\sigma}^2 = \\frac{\\mathrm{RSS}}{n}$。将这些代入对数似然函数，得到最大化对数似然：\n$$\\ln(L(\\hat{\\theta})) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln\\left(\\frac{\\mathrm{RSS}}{n}\\right) - \\frac{n}{2}$$\n那么AIC是：\n$$\\mathrm{AIC} = -2\\left(-\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln\\left(\\frac{\\mathrm{RSS}}{n}\\right) - \\frac{n}{2}\\right) + 2k$$\n$$\\mathrm{AIC} = n\\ln(2\\pi) + n\\ln\\left(\\frac{\\mathrm{RSS}}{n}\\right) + n + 2k$$\n为了进行模型比较，可以省略所有模型中都存在的常数项（如 $n\\ln(2\\pi)+n$）。高斯模型的AIC常规形式是：\n$$\\mathrm{AIC} = n\\ln\\left(\\frac{\\mathrm{RSS}}{n}\\right) + 2k$$\n\nAIC的推导依赖于大样本理论。当样本量 $n$ 相对于参数数量 $k$ 不够大时，惩罚项 $2k$ 是不足的。Sugiura（1978）以及 Hurvich 和 Tsai（1989）为线性模型推导出了一个更精确的二阶偏差校正，从而得到了小样本AIC，即AICc。该准则是通过寻找预期KL散度的一个更精确的估计量来推导的。结果是AIC中的偏差校正项 $2k$ 应被一个更精细的项所取代。AICc由下式给出：\n$$\\mathrm{AICc} = -2\\ln(L(\\hat{\\theta})) + 2k \\frac{n}{n-k-1}$$\n这可以表示为与AIC的关系：\n$$\\mathrm{AICc} = \\left( -2\\ln(L(\\hat{\\theta})) + 2k \\right) - 2k + 2k \\frac{n}{n-k-1} = \\mathrm{AIC} + 2k\\left(\\frac{n}{n-k-1} - 1\\right)$$\n$$\\mathrm{AICc} = \\mathrm{AIC} + 2k\\left(\\frac{n - (n-k-1)}{n-k-1}\\right) = \\mathrm{AIC} + \\frac{2k(k+1)}{n-k-1}$$\n使用AIC的常规形式，我们有：\n$$\\mathrm{AICc} = n\\ln\\left(\\frac{\\mathrm{RSS}}{n}\\right) + 2k + \\frac{2k(k+1)}{n-k-1}$$\n当 $n/k  40$ 时，推荐使用此校正。\n\n现在我们将这些公式应用于给定的模型。已知 $n=40$。\n\n**对于模型 $\\mathcal{M}_{1}$**：\n$k_{1}=10$, $\\mathrm{RSS}_{1}=270$。\n首先，我们计算 $\\mathcal{M}_{1}$ 的AIC：\n$$\\mathrm{AIC}(\\mathcal{M}_{1}) = n\\ln\\left(\\frac{\\mathrm{RSS}_{1}}{n}\\right) + 2k_1 = 40\\ln\\left(\\frac{270}{40}\\right) + 2(10)$$\n$$\\mathrm{AIC}(\\mathcal{M}_{1}) = 40\\ln(6.75) + 20 \\approx 40(1.90954) + 20 = 76.3817 + 20 = 96.3817$$\n由于 $n/k_1 = 40/10 = 4  40$，小样本校正是必要的。\n$$\\mathrm{AICc}(\\mathcal{M}_{1}) = \\mathrm{AIC}(\\mathcal{M}_{1}) + \\frac{2k_1(k_1+1)}{n-k_1-1} = 96.3817 + \\frac{2(10)(10+1)}{40-10-1}$$\n$$\\mathrm{AICc}(\\mathcal{M}_{1}) = 96.3817 + \\frac{2(10)(11)}{29} = 96.3817 + \\frac{220}{29} \\approx 96.3817 + 7.5862 = 103.9679$$\n\n**对于模型 $\\mathcal{M}_{2}$**：\n$k_{2}=7$, $\\mathrm{RSS}_{2}=320$。\n首先，我们计算 $\\mathcal{M}_{2}$ 的AIC：\n$$\\mathrm{AIC}(\\mathcal{M}_{2}) = n\\ln\\left(\\frac{\\mathrm{RSS}_{2}}{n}\\right) + 2k_2 = 40\\ln\\left(\\frac{320}{40}\\right) + 2(7)$$\n$$\\mathrm{AIC}(\\mathcal{M}_{2}) = 40\\ln(8) + 14 \\approx 40(2.07944) + 14 = 83.1777 + 14 = 97.1777$$\n由于 $n/k_2 = 40/7 \\approx 5.7  40$，小样本校正也是必要的。\n$$\\mathrm{AICc}(\\mathcal{M}_{2}) = \\mathrm{AIC}(\\mathcal{M}_{2}) + \\frac{2k_2(k_2+1)}{n-k_2-1} = 97.1777 + \\frac{2(7)(7+1)}{40-7-1}$$\n$$\\mathrm{AICc}(\\mathcal{M}_{2}) = 97.1777 + \\frac{2(7)(8)}{32} = 97.1777 + \\frac{112}{32} = 97.1777 + 3.5 = 100.6777$$\n\n### 模型偏好\n偏好的模型是信息准则值较低的模型。\n- **使用AIC**：$\\mathrm{AIC}(\\mathcal{M}_{1}) \\approx 96.38$ 且 $\\mathrm{AIC}(\\mathcal{M}_{2}) \\approx 97.18$。由于 $\\mathrm{AIC}(\\mathcal{M}_{1})  \\mathrm{AIC}(\\mathcal{M}_{2})$，AIC准则偏好更复杂的模型 $\\mathcal{M}_{1}$。\n- **使用AICc**：$\\mathrm{AICc}(\\mathcal{M}_{1}) \\approx 103.97$ 且 $\\mathrm{AICc}(\\mathcal{M}_{2}) \\approx 100.68$。由于 $\\mathrm{AICc}(\\mathcal{M}_{2})  \\mathrm{AICc}(\\mathcal{M}_{1})$，AICc准则偏好更简单的模型 $\\mathcal{M}_{2}$。在小样本情况下，对复杂度的更大惩罚逆转了模型的偏好。\n\n### 最终计算\n问题要求计算 $\\Delta_{\\mathrm{AICc}} \\equiv \\mathrm{AICc}(\\mathcal{M}_{1}) - \\mathrm{AICc}(\\mathcal{M}_{2})$ 的值。\n$$\\Delta_{\\mathrm{AICc}} = 103.9679... - 100.6777... = 3.2902...$$\n四舍五入到四位有效数字，我们得到 $3.290$。",
            "answer": "$$\\boxed{3.290}$$"
        },
        {
            "introduction": "真实世界的数据往往是复杂的，而共线性（即预测变量之间的高度相关性）是应用统计建模中一个常见的挑战。本次练习通过一个真实的临床场景，探讨了 AIC 和贝叶斯信息准则 (BIC) 如何应对冗余信息 。您将亲眼看到，在样本量较大时，BIC 更强的复杂性惩罚项（罚项为 $k \\ln(n)$）如何使其比 AIC（罚项为 $2k$）更倾向于选择简约的模型，这是两者在实际应用中的一个关键区别。",
            "id": "4966069",
            "problem": "一项基于医院的队列研究在线性回归框架下，使用高斯残差对一个连续的生物标志物结果进行建模。研究人员考虑了以下候选模型，这些模型通过最大似然法拟合到同一样本量为 $n=600$ 的数据集上：\n- $M_0$：截距项、年龄、性别和一个合并症指数（$p_0=4$ 个参数，包括截距项）。\n- $M_1$：$M_0$ 加上身体质量指数 (BMI)（$p_1=5$）。\n- $M_2$：$M_0$ 加上腰围 (WC)（$p_2=5$）。\n- $M_3$：$M_0$ 加上 BMI 和 WC（$p_3=6$）。\n\n在研究人群中，BMI和WC高度相关，皮尔逊相关系数为 $\\rho=0.96$，这反映了共线性。设每个拟合模型的残差平方和为：\n- $RSS_0=120000$，\n- $RSS_1=119400$，\n- $RSS_2=119380$，\n- $RSS_3=119360$。\n\n假设满足最大似然估计的标准正则性条件，并且所有模型都是可识别的、正确设定的，且具有同方差的正态误差。仅使用信息准则在数据拟合度（通过最大化似然函数）和模型复杂度（自由参数的数量）之间进行权衡这一基本思想，回答以下问题。关于共线性在赤池信息准则 (AIC) 和贝叶斯信息准则 (BIC) 下对模型选择的影响，以及在这种情况下会选择哪些模型，以下哪些陈述是正确的？\n\nA. 在 AIC 标准下，选择 $M_2$ 模型；在 BIC 标准下，选择 $M_0$ 模型。\n\nB. 在 AIC 标准下，$M_3$ 优于 $M_2$，因为增加参数时最大化对数似然值不会减小，且 $M_2$ 和 $M_3$ 的惩罚项相同。\n\nC. 从 $M_2$ 到 $M_3$ 的微小增量改进是 BMI 和 WC 之间共线性的直接体现；对于大样本量 $n$，BIC 对每个参数更强的惩罚使其比 AIC 更不可能包含此类冗余的预测变量。\n\nD. 如果样本量减少到 $n=60$ 而 $RSS$ 值保持不变，BIC 对附加参数的惩罚将小于 AIC，使其比 AIC 更愿意包含冗余的预测变量。",
            "solution": "在进行求解之前，对问题陈述的有效性进行严格评估。\n\n### 步骤 1：提取给定信息\n-   **研究设计**：基于医院的队列研究。\n-   **统计框架**：使用高斯残差对连续生物标志物进行线性回归。\n-   **估计方法**：最大似然估计 (MLE)。\n-   **样本量**：$n=600$。\n-   **候选模型**：\n    -   $M_0$：截距项、年龄、性别、合并症指数。回归参数数量 $p_0=4$。\n    -   $M_1$：$M_0$ + 身体质量指数 (BMI)。回归参数数量 $p_1=5$。\n    -   $M_2$：$M_0$ + 腰围 (WC)。回归参数数量 $p_2=5$。\n    -   $M_3$：$M_0$ + BMI + WC。回归参数数量 $p_3=6$。\n-   **预测变量相关性**：BMI 和 WC 之间的皮尔逊相关系数 $\\rho=0.96$。\n-   **残差平方和 (RSS)**：\n    -   $RSS_0=120000$\n    -   $RSS_1=119400$\n    -   $RSS_2=119380$\n    -   $RSS_3=119360$\n-   **假设**：MLE 的标准正则性条件、模型可识别、设定正确、同方差正态误差。\n-   **任务**：使用赤池信息准则 (AIC) 和贝叶斯信息准则 (BIC) 评估关于模型选择的陈述，重点关注共线性的影响。\n\n### 步骤 2：使用提取的给定信息进行验证\n1.  **科学依据**：该问题在标准统计理论中有充分的依据。线性回归、MLE、AIC、BIC 和共线性是统计建模的核心概念。该场景是医学研究中的一个现实应用。\n2.  **问题定义良好**：该问题提供了计算 AIC 和 BIC 值并比较模型所需的所有必要数据（每个模型的 $n$、$p_i$、$RSS_i$）。问题表述明确。\n3.  **客观性**：该问题使用精确、客观的语言和定量数据进行陈述。\n4.  **一致性**：参数数量随模型复杂度的增加而逻辑地增加：$p_0=4$，$p_1=5$，$p_2=5$，$p_3=6$。随着参数的增加，RSS 值减小（$RSS_0 > RSS_1, RSS_2 > RSS_3$），这对于嵌套模型来说是数学上的必然。在添加一个高度相关的预测变量时，RSS 的微小下降与共线性现象是一致的。该设置是内部一致的。\n\n### 步骤 3：结论与行动\n该问题是有效的，因为它科学合理、定义良好、客观且内部一致。可以推导出解决方案。\n\n### 推导与选项分析\n\n信息准则的基本原理是在拟合优度与模型复杂度之间取得平衡。其通用形式为：\n-   赤池信息准则 (AIC): $\\mathrm{AIC} = -2 \\ln(\\mathcal{L}_{max}) + 2k$\n-   贝叶斯信息准则 (BIC): $\\mathrm{BIC} = -2 \\ln(\\mathcal{L}_{max}) + k \\ln(n)$\n\n其中 $\\mathcal{L}_{max}$ 是最大化似然值，$k$ 是估计参数的总数，$n$ 是样本量。AIC 或 BIC 值最低的模型是首选模型。\n\n对于一个包含 $p$ 个回归系数（包括截距项）并假设误差为同方差高斯分布的线性回归模型，方差 $\\sigma^2$ 也需要被估计。因此，估计参数的总数为 $k = p+1$。使用方差的 MLE 估计 $\\hat{\\sigma}^2 = \\mathrm{RSS}/n$，最大化对数似然函数由下式给出：\n$$ \\ln(\\mathcal{L}_{max}) = -\\frac{n}{2}\\left(\\ln(2\\pi) + 1 + \\ln\\left(\\frac{\\mathrm{RSS}}{n}\\right)\\right) $$\n将此代入 AIC 和 BIC 公式可得：\n$$ \\mathrm{AIC} = n(\\ln(2\\pi) + 1) + n\\ln\\left(\\frac{\\mathrm{RSS}}{n}\\right) + 2(p+1) $$\n$$ \\mathrm{BIC} = n(\\ln(2\\pi) + 1) + n\\ln\\left(\\frac{\\mathrm{RSS}}{n}\\right) + (p+1)\\ln(n) $$\n对于拟合到相同数据的模型进行比较时，常数项 $n(\\ln(2\\pi) + 1)$ 可以忽略。因此，我们可以使用以下与 AIC 和 BIC 成比例的量来比较模型，其中 $p$ 是回归系数的数量（此处的$p_i$）：\n$\\mathrm{AIC}' \\propto n\\ln(\\mathrm{RSS}) + 2p$\n$\\mathrm{BIC}' \\propto n\\ln(\\mathrm{RSS}) + p\\ln(n)$\n请注意，在惩罚项中使用 $p$ 而不是 $k=p+1$ 会得到相同的模型排序，因为任意两个模型之间惩罚项的差异保持不变。\n\n给定 $n=600$，每个参数的 BIC 惩罚为 $\\ln(600) \\approx 6.397$。\n\n让我们为每个模型计算 $n\\ln(\\mathrm{RSS})$、AIC 惩罚项 ($2p$) 和 BIC 惩罚项 ($p\\ln(n)$) 的值。我们将比较它们的和。\n\n**模型 $M_0$**：$p_0=4$，$RSS_0=120000$\n-   $n\\ln(RSS_0) = 600 \\times \\ln(120000) \\approx 7017.15$\n-   $\\mathrm{AIC}'_0 \\propto 7017.15 + 2(4) = 7025.15$\n-   $\\mathrm{BIC}'_0 \\propto 7017.15 + 4 \\times \\ln(600) \\approx 7017.15 + 25.59 = 7042.74$\n\n**模型 $M_1$**：$p_1=5$，$RSS_1=119400$\n-   $n\\ln(RSS_1) = 600 \\times \\ln(119400) \\approx 7014.08$\n-   $\\mathrm{AIC}'_1 \\propto 7014.08 + 2(5) = 7024.08$\n-   $\\mathrm{BIC}'_1 \\propto 7014.08 + 5 \\times \\ln(600) \\approx 7014.08 + 31.98 = 7046.06$\n\n**模型 $M_2$**：$p_2=5$，$RSS_2=119380$\n-   $n\\ln(RSS_2) = 600 \\times \\ln(119380) \\approx 7013.98$\n-   $\\mathrm{AIC}'_2 \\propto 7013.98 + 2(5) = 7023.98$\n-   $\\mathrm{BIC}'_2 \\propto 7013.98 + 5 \\times \\ln(600) \\approx 7013.98 + 31.98 = 7045.96$\n\n**模型 $M_3$**：$p_3=6$，$RSS_3=119360$\n-   $n\\ln(RSS_3) = 600 \\times \\ln(119360) \\approx 7013.88$\n-   $\\mathrm{AIC}'_3 \\propto 7013.88 + 2(6) = 7025.88$\n-   $\\mathrm{BIC}'_3 \\propto 7013.88 + 6 \\times \\ln(600) \\approx 7013.88 + 38.38 = 7052.26$\n\n**模型选择总结**：\n-   **AIC 排名（越低越好）**：$\\mathrm{AIC}'_2 (7023.98)  \\mathrm{AIC}'_1 (7024.08)  \\mathrm{AIC}'_0 (7025.15)  \\mathrm{AIC}'_3 (7025.88)$。AIC 选择模型 $M_2$。\n-   **BIC 排名（越低越好）**：$\\mathrm{BIC}'_0 (7042.74)  \\mathrm{BIC}'_2 (7045.96)  \\mathrm{BIC}'_1 (7046.06)  \\mathrm{BIC}'_3 (7052.26)$。BIC 选择模型 $M_0$。\n\n根据这些结果，我们评估每个选项。\n\n**A. 在 AIC 标准下，选择 $M_2$ 模型；在 BIC 标准下，选择 $M_0$ 模型。**\n我们的计算表明，AIC 的最小值属于模型 $M_2$，BIC 的最小值属于模型 $M_0$。该陈述是对计算结果的直接且准确的报告。\n**结论：正确。**\n\n**B. 在 AIC 标准下，$M_3$ 优于 $M_2$，因为增加参数时最大化对数似然值不会减小，且 $M_2$ 和 $M_3$ 的惩罚项相同。**\n该陈述包含一个关键缺陷。$M_3$ 的最大化对数似然值确实高于 $M_2$（因为 $RSS_3  RSS_2$）。然而，“$M_2$ 和 $M_3$ 的惩罚项相同”这一说法是错误的。\n-   对于 $M_2$，参数数量为 $p_2=5$，所以 AIC 惩罚项是 $2p_2=10$。\n-   对于 $M_3$，参数数量为 $p_3=6$，所以 AIC 惩罚项是 $2p_3=12$。\n惩罚项是不同的。此外，我们的计算显示 $\\mathrm{AIC}'_3 \\approx 7025.88 > \\mathrm{AIC}'_2 \\approx 7023.98$，所以在 AIC 标准下 $M_2$ 优于 $M_3$，这与该陈述的主要主张相矛盾。\n**结论：不正确。**\n\n**C. 从 $M_2$ 到 $M_3$ 的微小增量改进是 BMI 和 WC 之间共线性的直接体现；对于大样本量 $n$，BIC 对每个参数更强的惩罚使其比 AIC 更不可能包含此类冗余的预测变量。**\n该陈述由两部分组成。\n1.  “从 $M_2$ 到 $M_3$ 的微小增量改进是 BMI 和 WC 之间共线性的直接体现。”模型 $M_3$ 在模型 $M_2$ 的基础上添加了 BMI。RSS 的减少量为 $RSS_2 - RSS_3 = 119380 - 119360 = 20$。这是一个非常小的拟合改进，特别是与将 WC 添加到 $M_0$ 所带来的改进（$RSS_0 - RSS_2 = 120000 - 119380 = 620$）相比。当添加一个与模型中已有的预测变量（WC）高度相关的预测变量（BMI）时，这种微小的增量收益正是我们所期望的。陈述的这一部分是对数据的正确解释。\n2.  “对于大样本量 $n$，BIC 对每个参数更强的惩罚使其比 AIC 更不可能包含此类冗余的预测变量。” AIC 对每个附加参数的惩罚是 $2$，而 BIC 是 $\\ln(n)$。给定 $n=600$，$\\ln(600) \\approx 6.397 > 2$。因此，BIC 的惩罚要强得多。这个更大的惩罚意味着 BIC 需要似然函数有更大的改进才能证明增加一个参数是合理的，这使得它更“简约”或保守。因此，它比 AIC 更不可能选择带有额外（尤其是冗余）参数的模型。这是关于 AIC 与 BIC 行为的一个基本且正确的概念要点。这个普遍原则在这个问题中得到了证明：AIC 增加了一个预测变量（选择 $M_2$ 而非 $M_0$），而 BIC 则没有（选择 $M_0$ 而非 $M_2$）。\n陈述的两个部分在概念上和事实上都是正确的。\n**结论：正确。**\n\n**D. 如果样本量减少到 $n=60$ 而 $RSS$ 值保持不变，BIC 对附加参数的惩罚将小于 AIC，使其比 AIC 更愿意包含冗余的预测变量。**\n该陈述提出了一个假设情景。其关键主张是“BIC 对附加参数的惩罚将小于 AIC”。让我们来验证一下。\n-   AIC 对每个参数的惩罚是一个常数，$2$。\n-   BIC 对每个参数的惩罚是 $\\ln(n)$。对于假设的 $n=60$，这将是 $\\ln(60) \\approx 4.094$。\n-   由于 $4.094 > 2$，BIC 的惩罚仍然大于 AIC 的惩罚。只有当 $\\ln(n)  2$ 时，即 $n  e^2 \\approx 7.39$ 时，BIC 对每个参数的惩罚才会小于 AIC。\n-   该陈述的前提是错误的。因此，BIC 会“更愿意”添加预测变量的结论也是错误的。\n**结论：不正确。**\n\n由于陈述 A 和 C 都是正确的，并且题目说明允许多个正确选项，因此它们都必须包含在最终答案中。",
            "answer": "$$\\boxed{AC}$$"
        },
        {
            "introduction": "信息准则的一大优势是能够比较非嵌套模型，这是传统假设检验方法（如似然比检验）通常无法做到的。最后的这个练习将超越简单地选择“最佳”模型，转向对证据强度的量化 。您将学习如何解释 AIC 差异（$\\Delta \\mathrm{AIC}$）并计算 Akaike 权重，从而对模型不确定性有更细致的认识，并理解模型比较并非总是非黑即白的选择。",
            "id": "4966121",
            "problem": "一个临床团队正在使用两个非嵌套的逻辑回归模型，为疑似脓毒症患者的院内死亡率建模。这两个模型都通过最大似然法拟合到大小为 $n=800$ 的同一数据集上。模型 $\\mathrm{M}_1$ 使用了 $12$ 个参数（包括截距、人口统计学信息和 $8$ 种生物标志物），其最大化对数似然为 $\\ell_1=-520.3$。模型 $\\mathrm{M}_2$ 使用了 $10$ 个参数（包括截距、人口统计学信息和 $6$ 种生物标志物），其最大化对数似然为 $\\ell_2=-521.8$。两个模型都对二元结果使用了相同的似然函数（带有伯努利抽样模型的逻辑回归）。\n\n选择所有正确解释了赤池信息准则（AIC）如何为 $\\mathrm{M}_1$ 和 $\\mathrm{M}_2$ 之间提供有原则的比较，并评估在这种情况下 $\\Delta \\mathrm{AIC}$ 差异何时有意义的陈述。\n\nA. 在 Akaike 的框架下，偏差 $-2\\ell$ 较小的模型更优；由于 $-2\\ell_1  -2\\ell_2$，因此應選擇 $\\mathrm{M}_1$。\n\nB. 两个模型之间的 AIC 差异约为 $1.0$，有利于 $\\mathrm{M}_2$，表明证据仅为弱证据；这种比较是有效的，因为两个模型都是通过最大似然法对相同的似然函数和相同的数据进行拟合的。\n\nC. 赤池信息准则（AIC）是有原则的，因为它的目标是最小化预期的样本外 Kullback–Leibler 散度；传统解释将 $0$–$2$ 范围内的 $\\Delta \\mathrm{AIC}$ 视为可忽略不计，$4$–$7$ 视为相当可观，$10$ 视为强有力。在本例中，观察到的差异表明，这两个模型在预期预测性能方面基本等效。\n\nD. 当 $n=800$ 时，贝叶斯信息准则（BIC）必然与 AIC 在模型偏好上达成一致；在实践中，惩罚项的差异不会改变偏好。\n\nE. AIC 的差异仅对嵌套模型有意义；对于非嵌套模型，AIC 不能用于比较。\n\nF. AIC 较低的模型的隐含赤池权重约为 $0.62$，量化了相对于另一个模型，该模型获得的适度支持。",
            "solution": "问题陈述经评估是有效的，因为它在科学上基于标准的统计建模原则，问题提出得当，客观，并为所需计算提供了完整的数据集。\n\n这个问题要求使用赤池信息准则（AIC）比较两个逻辑回归模型 $\\mathrm{M}_1$ 和 $\\mathrm{M}_2$。这两个模型是拟合到大小为 $n=800$ 的同一数据集上的。\n\n给定的数据是：\n-   模型 $\\mathrm{M}_1$：参数数量 $k_1 = 12$，最大化对数似然 $\\ell_1 = -520.3$。\n-   模型 $\\mathrm{M}_2$：参数数量 $k_2 = 10$，最大化对数似然 $\\ell_2 = -521.8$。\n\n赤池信息准则定义为：\n$$ \\mathrm{AIC} = -2\\ell + 2k $$\n其中 $\\ell$ 是最大化对数似然，$k$ 是模型中估计的参数数量。AIC 较低的模型被认为在拟合优度和简约性之间提供了更好的平衡，因此更受青睐。\n\n首先，我们计算每个模型的 AIC。\n对于模型 $\\mathrm{M}_1$：\n$$ \\mathrm{AIC}_1 = -2\\ell_1 + 2k_1 = -2(-520.3) + 2(12) = 1040.6 + 24 = 1064.6 $$\n对于模型 $\\mathrm{M}_2$：\n$$ \\mathrm{AIC}_2 = -2\\ell_2 + 2k_2 = -2(-521.8) + 2(10) = 1043.6 + 20 = 1063.6 $$\n\n比较这两个值，$\\mathrm{AIC}_2  \\mathrm{AIC}_1$。因此，根据 AIC，模型 $\\mathrm{M}_2$ 更优。AIC 值的差异是：\n$$ \\Delta \\mathrm{AIC} = \\mathrm{AIC}_1 - \\mathrm{AIC}_2 = 1064.6 - 1063.6 = 1.0 $$\n\n现在我们评估每个选项。\n\nA. 在 Akaike 的框架下，偏差 $-2\\ell$ 较小的模型更优；由于 $-2\\ell_1  -2\\ell_2$，因此应选择 $\\mathrm{M}_1$。\n模型 $\\mathrm{M}_1$ 的偏差是 $-2\\ell_1 = -2(-520.3) = 1040.6$。模型 $\\mathrm{M}_2$ 的偏差是 $-2\\ell_2 = -2(-521.8) = 1043.6$。确实，$-2\\ell_1  -2\\ell_2$。然而，Akaike 框架偏好偏差较小模型的前提是错误的。AIC 定义为 $\\mathrm{AIC} = -2\\ell + 2k$。它是模型拟合度（由偏差 $-2\\ell$ 表示）和模型复杂度（由惩罚项 $2k$ 表示）之间的一种权衡。仅仅选择偏差最小的模型将总是偏向于最复杂的模型，而这容易导致过拟合。AIC 的目的正是为了惩罚这种复杂性。在这种情况下，尽管 $\\mathrm{M}_2$ 的偏差更高，但由于其更简约，AIC 偏好 $\\mathrm{M}_2$。\n**结论：错误。**\n\nB. 两个模型之间的 AIC 差异约为 $1.0$，有利于 $\\mathrm{M}_2$，表明证据仅为弱证据；这种比较是有效的，因为两个模型都是通过最大似然法对相同的似然函数和相同的数据进行拟合的。\n如上计算，差异 $\\mathrm{AIC}_1 - \\mathrm{AIC}_2 = 1.0$。由于 $\\mathrm{AIC}_2$ 较低，该差异有利于 $\\mathrm{M}_2$。$\\Delta \\mathrm{AIC}$ 值为 $1.0$ 通常被认为代表了反对 AIC 较高模型（$\\mathrm{M}_1$）的弱证据或可忽略的证据。该陈述还正确指出了使用 AIC 比较模型的关键条件：它们必须使用相同的响应变量和似然函数族拟合到相同的数据上。问题陈述确认了这一条件已满足。\n**结论：正确。**\n\nC. 赤池信息准则（AIC）是有原则的，因为它的目标是最小化预期的样本外 Kullback–Leibler 散度；传统解释将 $0$–$2$ 范围内的 $\\Delta \\mathrm{AIC}$ 视为可忽略不计，$4$–$7$ 视为相当可观，$10$ 视为强有力。在本例中，观察到的差异表明，这两个模型在预期预测性能方面基本等效。\n这个陈述准确地描述了 AIC 的理论基础。AIC 是相对预期 KL 散度的一个估计，KL 散度衡量了当使用一个模型来近似真实数据生成过程时丢失的信息。它是预期样本外预测准确性的一个度量。所提供的解释 $\\Delta \\mathrm{AIC}$（一个模型的 AIC 与集合中最小 AIC 之间的差异）的经验法则是由 Burnham and Anderson 提出的标准指南。我们计算出的差异是 $\\Delta \\mathrm{AIC} = 1.0$，属于 $0$–$2$ 的范围。由此得出模型在“预期预测性能方面基本等效”的结论是对这些规则的正确应用。\n**结论：正确。**\n\nD. 当 $n=800$ 时，贝叶斯信息准则（BIC）必然与 AIC 在模型偏好上达成一致；在实践中，惩罚项的差异不会改变偏好。\nBIC 定义为 $\\mathrm{BIC} = -2\\ell + k\\ln(n)$。每个参数的惩罚项是 $\\ln(n)$，而对于 AIC 则是 $2$。给定 $n=800$，$\\ln(800) \\approx 6.68$。由于 $\\ln(800) > 2$，BIC 对复杂度的惩罚比 AIC 更重。它们不一定会达成一致。例如，如果一个更复杂的模型有足够好的拟合度（更低的 $-2\\ell$），AIC 可能偏好它，而 BIC 可能偏好更简单的模型。我们来检查一下这个问题：\n$\\mathrm{BIC}_1 = -2(-520.3) + 12\\ln(800) \\approx 1040.6 + 12(6.685) = 1040.6 + 80.22 = 1120.82$。\n$\\mathrm{BIC}_2 = -2(-521.8) + 10\\ln(800) \\approx 1043.6 + 10(6.685) = 1043.6 + 66.85 = 1110.45$。\n这里，$\\mathrm{BIC}_2  \\mathrm{BIC}_1$，所以 BIC 也偏好 $\\mathrm{M}_2$。然而，它们*必然*一致的说法是错误的。惩罚项的差异正是导致不同模型偏好的原因，尤其是在大样本量下，BIC 的惩罚要大得多。\n**结论：错误。**\n\nE. AIC 的差异仅对嵌套模型有意义；对于非嵌套模型，AIC 不能用于比较。\n这是错误的。像 AIC 和 BIC 这样的信息准则相对于似然比检验等方法的一个主要优点是它们可以用来比较非嵌套模型。主要要求是模型拟合到相同的数据并使用相同的似然函数，而不是一个模型是另一个模型的特例。问题明确指出模型是非嵌套的，这对于 AIC 比较是一个完全有效的场景。\n**结论：错误。**\n\nF. AIC 较低的模型的隐含赤池权重约为 $0.62$，量化了相对于另一个模型，该模型获得的适度支持。\nAIC 较低的模型是 $\\mathrm{M}_2$。为了计算其赤池权重 $w_2$，我们使用与最小 AIC 的差值。这里，$\\mathrm{AIC}_{\\min} = \\mathrm{AIC}_2 = 1063.6$。所以，$\\Delta_2 = \\mathrm{AIC}_2 - \\mathrm{AIC}_{\\min} = 0$，且 $\\Delta_1 = \\mathrm{AIC}_1 - \\mathrm{AIC}_{\\min} = 1.0$。\n模型 $\\mathrm{M}_2$ 的赤池权重是：\n$$ w_2 = \\frac{\\exp(-\\frac{1}{2}\\Delta_2)}{\\exp(-\\frac{1}{2}\\Delta_1) + \\exp(-\\frac{1}{2}\\Delta_2)} = \\frac{\\exp(-\\frac{1}{2}(0))}{\\exp(-\\frac{1}{2}(1.0)) + \\exp(-\\frac{1}{2}(0))} = \\frac{1}{\\exp(-0.5) + 1} $$\n使用 $\\exp(-0.5) \\approx 0.6065$ 的值，我们得到：\n$$ w_2 \\approx \\frac{1}{0.6065 + 1} = \\frac{1}{1.6065} \\approx 0.6225 $$\n这个值约等于 $0.62$。赤池权重可以解释为在候选模型集中，模型 $\\mathrm{M}_2$ 是最佳模型的概率。权重为 $0.62$（相对于 $w_1 = 1 - w_2 \\approx 0.38$）表明 $\\mathrm{M}_2$ 更有可能是更好的模型，但证据并非压倒性的。“适度支持”是一个恰当的描述。\n**结论：正确。**",
            "answer": "$$\\boxed{BCF}$$"
        }
    ]
}