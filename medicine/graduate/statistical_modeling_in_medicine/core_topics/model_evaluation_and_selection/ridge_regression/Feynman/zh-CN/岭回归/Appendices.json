{
    "hands_on_practices": [
        {
            "introduction": "为了掌握岭回归的精髓，最好从其基本目标入手：最小化带惩罚的残差平方和。第一个练习将引导你在最简单的场景下——即一个不含截距的单预测变量模型——完成这一过程。通过从第一性原理推导岭回归估计量 ，你将对正则化参数 $\\lambda$ 如何直接影响系数估计建立起扎实的直觉。",
            "id": "1951876",
            "problem": "在机器学习的背景下，我们的任务是为一个包含 $n$ 个数据点 $(x_i, y_i)$ 的集合拟合一个无截距的简单线性模型 $y = \\beta x$。为防止在小数据集上发生过拟合，我们采用岭回归。系数 $\\beta$ 的岭估计值是通过最小化惩罚平方和误差（也称为目标函数 $L(\\beta)$）得到的值：\n$$L(\\beta) = \\sum_{i=1}^{n} (y_i - \\beta x_i)^2 + \\lambda \\beta^2$$\n其中 $\\lambda > 0$ 是控制收缩量的正则化参数。\n\n您的任务分为两部分。首先，通过最小化目标函数 $L(\\beta)$，推导出岭估计 $\\hat{\\beta}_{\\text{ridge}}$ 关于数据点 $(x_i, y_i)$ 和参数 $\\lambda$ 的一般闭式表达式。\n\n其次，将此推导出的表达式应用于一个由两个点组成的特定数据集：$(x_1, y_1) = (1, 3)$ 和 $(x_2, y_2) = (2, 5)$。使用正则化参数 $\\lambda = 1$ 计算岭估计 $\\hat{\\beta}_{\\text{ridge}}$ 的数值。\n\n以精确分数形式给出最终的数值。",
            "solution": "我们最小化无截距线性模型 $y=\\beta x$ 的惩罚平方和误差，其目标函数为\n$$\nL(\\beta)=\\sum_{i=1}^{n}(y_{i}-\\beta x_{i})^{2}+\\lambda \\beta^{2}, \\quad \\lambda>0.\n$$\n展开平方项并合并 $\\beta$ 的同次幂：\n$$\nL(\\beta)=\\sum_{i=1}^{n}\\left(y_{i}^{2}-2\\beta x_{i}y_{i}+\\beta^{2}x_{i}^{2}\\right)+\\lambda \\beta^{2}\n= \\sum_{i=1}^{n}y_{i}^{2}-2\\beta \\sum_{i=1}^{n}x_{i}y_{i}+\\beta^{2}\\sum_{i=1}^{n}x_{i}^{2}+\\lambda \\beta^{2}.\n$$\n对 $\\beta$ 求导，并将导数设为零（一阶最优性条件）：\n$$\n\\frac{dL}{d\\beta}=-2\\sum_{i=1}^{n}x_{i}y_{i}+2\\beta \\sum_{i=1}^{n}x_{i}^{2}+2\\lambda \\beta=0.\n$$\n求解 $\\beta$：\n$$\n2\\beta\\left(\\sum_{i=1}^{n}x_{i}^{2}+\\lambda\\right)=2\\sum_{i=1}^{n}x_{i}y_{i}\n\\quad \\Longrightarrow \\quad\n\\hat{\\beta}_{\\text{ridge}}=\\frac{\\sum_{i=1}^{n}x_{i}y_{i}}{\\sum_{i=1}^{n}x_{i}^{2}+\\lambda}.\n$$\n二阶导数为\n$$\n\\frac{d^{2}L}{d\\beta^{2}}=2\\sum_{i=1}^{n}x_{i}^{2}+2\\lambda>0,\n$$\n因此该解是唯一的最小化子。\n\n将此应用于 $(x_{1},y_{1})=(1,3)$，$(x_{2},y_{2})=(2,5)$ 且 $\\lambda=1$ 的情况：\n$$\n\\sum_{i=1}^{2}x_{i}y_{i}=1\\cdot 3+2\\cdot 5=13,\\qquad\n\\sum_{i=1}^{2}x_{i}^{2}=1^{2}+2^{2}=5.\n$$\n因此，\n$$\n\\hat{\\beta}_{\\text{ridge}}=\\frac{13}{5+1}=\\frac{13}{6}.\n$$",
            "answer": "$$\\boxed{\\frac{13}{6}}$$"
        },
        {
            "introduction": "掌握了标量情况后，我们现在可以将其推广到适用于多预测变量模型的、功能更强大的矩阵表示法。这个练习直接应用了岭回归估计量的封闭式矩阵解。通过使用预先计算好的汇总统计量，如 $X^T X$ 和 $X^T y$ ，你可以专注于岭惩罚项如何修正标准正规方程的代数机制，而无需陷入繁琐的数据处理。",
            "id": "1951893",
            "problem": "在机器学习领域，岭回归是一种用于正则化线性回归模型的常用技术。这对于防止过拟合和处理预测变量之间的多重共线性特别有用。岭回归的系数向量估计量 $\\hat{\\beta}_{\\lambda}$ 由以下公式给出：\n$$ \\hat{\\beta}_{\\lambda} = (X^T X + \\lambda I)^{-1} X^T y $$\n此处，$X$ 是设计矩阵，$y$ 是观测结果向量，$I$ 是适当维度的单位矩阵，$\\lambda$ 是一个非负的正则化参数。\n\n假设对于一个包含两个预测变量的特定数据集，已经预先计算出以下量：\n$$ X^T X = \\begin{pmatrix} 10  5 \\\\ 5  10 \\end{pmatrix} \\quad \\text{and} \\quad X^T y = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} $$\n使用正则化参数 $\\lambda = 5$，确定岭回归系数向量 $\\hat{\\beta}_5$。",
            "solution": "岭回归估计量由以下公式定义\n$$\n\\hat{\\beta}_{\\lambda} = (X^{T}X + \\lambda I)^{-1} X^{T} y.\n$$\n根据给定的数据，\n$$\nX^{T}X = \\begin{pmatrix} 10  5 \\\\ 5  10 \\end{pmatrix}, \\quad X^{T} y = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}, \\quad \\lambda = 5.\n$$\n计算正则化矩阵：\n$$\nX^{T}X + \\lambda I = \\begin{pmatrix} 10  5 \\\\ 5  10 \\end{pmatrix} + 5 \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 15  5 \\\\ 5  15 \\end{pmatrix}.\n$$\n对于一个 $2 \\times 2$ 矩阵 $\\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}$，其逆矩阵由下式给出\n$$\n\\left(\\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}\\right)^{-1} = \\frac{1}{ad - bc} \\begin{pmatrix} d  -b \\\\ -c  a \\end{pmatrix}.\n$$\n应用此公式，\n$$\n\\det(X^{T}X + \\lambda I) = 15 \\cdot 15 - 5 \\cdot 5 = 225 - 25 = 200,\n$$\n所以\n$$\n(X^{T}X + \\lambda I)^{-1} = \\frac{1}{200} \\begin{pmatrix} 15  -5 \\\\ -5  15 \\end{pmatrix}.\n$$\n那么\n$$\n\\hat{\\beta}_{5} = \\frac{1}{200} \\begin{pmatrix} 15  -5 \\\\ -5  15 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} = \\frac{1}{200} \\begin{pmatrix} 15 \\cdot 3 - 5 \\cdot 1 \\\\ -5 \\cdot 3 + 15 \\cdot 1 \\end{pmatrix} = \\frac{1}{200} \\begin{pmatrix} 40 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{5} \\\\ 0 \\end{pmatrix}.\n$$",
            "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{5}\\\\0\\end{pmatrix}}$$"
        },
        {
            "introduction": "虽然矩阵公式提供了一个优雅的理论解，但对于高维数据，对大矩阵求逆在计算上可能成本过高。最后一个练习将探讨一种强大的替代方法：坐标下降法，这是一种在现代机器学习中广泛使用的迭代优化算法。通过推导单个系数的更新规则（同时保持其他系数不变），你将深入了解许多大规模回归软件包背后的计算引擎，并理解这些复杂模型在实践中是如何被高效拟合的。",
            "id": "1951864",
            "problem": "一位数据科学家正在构建一个线性模型，用于从一组 $p$ 个特征中预测目标变量 $y$。该数据集包含 $n$ 个观测值。对于第 $i$ 个观测值，特征由向量 $\\mathbf{x}_i \\in \\mathbb{R}^p$ 给出，观测到的结果是 $y_i$。线性模型的预测由 $f(\\mathbf{x}_i; \\beta) = \\sum_{j=1}^{p} x_{ij} \\beta_j$ 给出，其中 $\\beta = (\\beta_1, \\dots, \\beta_p)$ 是模型系数的向量，$x_{ij}$ 是第 $i$ 个观测值的第 $j$ 个特征的值。\n\n为了减轻过拟合并处理特征间的多元共线性，该数据科学家选择使用岭回归。必须最小化的相关目标函数 $L(\\beta)$ 由一个平方和误差项和一个L2正则化项组成：\n$$L(\\beta) = \\frac{1}{2} \\sum_{i=1}^{n} \\left(y_i - \\sum_{j=1}^{p} x_{ij} \\beta_j\\right)^2 + \\frac{\\lambda}{2} \\sum_{j=1}^{p} \\beta_j^2$$\n这里，$\\lambda > 0$ 是一个预先确定的正则化参数。\n\n该数据科学家决定实现一个一次更新一个系数的优化算法。在这个迭代过程的单一步骤中，一个系数，比如 $\\beta_j$，被更新到其最优值，而所有其他系数 $\\beta_k$ (对于所有 $k \\neq j$) 则保持不变。\n\n您的任务是，在假设所有其他系数 $\\beta_k$ (对于 $k \\neq j$) 固定的前提下，推导出最小化目标函数 $L(\\beta)$ 的 $\\beta_j$ 值的封闭形式表达式。您的最终答案应该是一个用 $y_i$、$x_{ik}$、$\\lambda$ 和固定的系数 $\\beta_k$ (对于 $k \\neq j$) 表示的 $\\beta_j$ 的表达式。",
            "solution": "我们给定的目标函数为\n$$L(\\beta) = \\frac{1}{2} \\sum_{i=1}^{n} \\left(y_i - \\sum_{k=1}^{p} x_{ik} \\beta_k\\right)^{2} + \\frac{\\lambda}{2} \\sum_{k=1}^{p} \\beta_k^{2},$$\n其中 $\\lambda>0$。在坐标更新中，所有 $k \\neq j$ 的 $\\beta_k$ 都被视为常数，我们关于 $\\beta_j$ 来最小化 $L$。\n\n使用链式法则对平方误差项求导，并对二次惩罚项求导，从而对 $L$ 关于 $\\beta_j$ 进行微分：\n- 对于误差项，记 $r_i(\\beta) = y_i - \\sum_{k=1}^{p} x_{ik} \\beta_k$。那么\n$$\\frac{\\partial}{\\partial \\beta_j} \\left(\\frac{1}{2} \\sum_{i=1}^{n} r_i(\\beta)^{2}\\right) = \\sum_{i=1}^{n} r_i(\\beta) \\frac{\\partial r_i(\\beta)}{\\partial \\beta_j} = \\sum_{i=1}^{n} r_i(\\beta)\\,(-x_{ij}) = - \\sum_{i=1}^{n} x_{ij} \\left(y_i - \\sum_{k=1}^{p} x_{ik} \\beta_k\\right).$$\n- 对于正则化项，\n$$\\frac{\\partial}{\\partial \\beta_j} \\left(\\frac{\\lambda}{2} \\sum_{k=1}^{p} \\beta_k^{2}\\right) = \\lambda \\beta_j.$$\n\n因此偏导数为\n$$\\frac{\\partial L}{\\partial \\beta_j} = - \\sum_{i=1}^{n} x_{ij} \\left(y_i - \\sum_{k=1}^{p} x_{ik} \\beta_k\\right) + \\lambda \\beta_j.$$\n\n将此导数设为零以获得一阶最优性条件：\n$$- \\sum_{i=1}^{n} x_{ij} \\left(y_i - \\sum_{k=1}^{p} x_{ik} \\beta_k\\right) + \\lambda \\beta_j = 0.$$\n等价地，\n$$\\sum_{i=1}^{n} x_{ij} \\left(y_i - \\sum_{k=1}^{p} x_{ik} \\beta_k\\right) = \\lambda \\beta_j.$$\n\n展开关于 $k$ 的内层求和，并将 $k=j$ 的项与 $k \\neq j$ 的项分开：\n$$\\sum_{i=1}^{n} x_{ij} y_i - \\sum_{i=1}^{n} \\sum_{k \\neq j} x_{ij} x_{ik} \\beta_k - \\sum_{i=1}^{n} x_{ij}^{2} \\beta_j = \\lambda \\beta_j.$$\n\n将包含 $\\beta_j$ 的项收集到右侧：\n$$\\sum_{i=1}^{n} x_{ij} y_i - \\sum_{k \\neq j} \\beta_k \\sum_{i=1}^{n} x_{ij} x_{ik} = \\beta_j \\left(\\lambda + \\sum_{i=1}^{n} x_{ij}^{2}\\right).$$\n\n求解 $\\beta_j$：\n$$\\beta_j = \\frac{\\sum_{i=1}^{n} x_{ij} y_i - \\sum_{k \\neq j} \\beta_k \\sum_{i=1}^{n} x_{ij} x_{ik}}{\\lambda + \\sum_{i=1}^{n} x_{ij}^{2}}.$$\n\n等价地，以一种通过部分残差明确显示对固定系数依赖关系的形式，\n$$\\beta_j = \\frac{\\sum_{i=1}^{n} x_{ij} \\left(y_i - \\sum_{k \\neq j} x_{ik} \\beta_k\\right)}{\\lambda + \\sum_{i=1}^{n} x_{ij}^{2}}.$$\n这是在所有其他系数保持固定的情况下，岭回归中 $\\beta_j$ 的封闭形式坐标级最小化解。",
            "answer": "$$\\boxed{\\frac{\\sum_{i=1}^{n} x_{ij} \\left(y_i - \\sum_{k \\neq j} x_{ik} \\beta_k\\right)}{\\lambda + \\sum_{i=1}^{n} x_{ij}^{2}}}$$"
        }
    ]
}