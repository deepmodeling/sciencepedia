{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of clinical medicine is the establishment of reference intervals, which define the range of typical values for a physiological measure in a healthy population. Many such measures, like triglyceride concentrations, are inherently positive and exhibit right-skewed distributions. This practice will guide you through deriving the formula for percentiles of a log-normal distribution from first principles, a process that is fundamental to calculating these reference intervals . By completing this exercise, you will solidify your understanding of the direct mathematical link between the familiar normal distribution and its log-transformed counterpart, and apply it to a tangible and important clinical context.",
            "id": "4990452",
            "problem": "A clinical laboratory is establishing a reference interval for fasting plasma triglyceride concentration, denoted by the random variable $X$ (measured in $\\mathrm{mg/dL}$), in an ostensibly healthy adult population. Empirical evidence suggests that $X$ is strictly positive and right-skewed, and that the natural logarithm of $X$ is approximately Gaussian. Accordingly, model $X$ as log-normal with parameters $(\\mu,\\sigma^{2})$, meaning that $Y=\\ln X$ follows a normal distribution with mean $\\mu$ and variance $\\sigma^{2}$. In this setting, reference limits are specified by population percentiles.\n\nUsing only the definitions of the cumulative distribution function and quantiles, the fact that $\\ln(\\cdot)$ is a strictly increasing (monotone) function on $(0,\\infty)$, and the distributional assumption for $Y$, derive from first principles a general expression for the $p$th percentile $q_{p}$ of $X$ in terms of $p$, $\\mu$, and $\\sigma$. Then, for $p=0.95$, compute the corresponding reference limit when $(\\mu,\\sigma^{2})=(\\ln 100,\\,0.25)$. Express your final value in $\\mathrm{mg/dL}$ and round your answer to four significant figures.",
            "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n**Step 1: Extract Givens**\n-   The random variable $X$ represents fasting plasma triglyceride concentration.\n-   $X$ is strictly positive, $X  0$.\n-   $X$ is modeled as a log-normal distribution with parameters $(\\mu, \\sigma^2)$.\n-   $Y = \\ln X$ follows a normal distribution with mean $\\mu$ and variance $\\sigma^2$, denoted $Y \\sim N(\\mu, \\sigma^2)$.\n-   Reference limits are defined as population percentiles.\n-   The task is to derive the $p$-th percentile of $X$, denoted $q_p$, from first principles.\n-   The derivation must use:\n    1.  The definition of the cumulative distribution function (CDF).\n    2.  The definition of quantiles (percentiles).\n    3.  The property that $\\ln(\\cdot)$ is a strictly increasing function on $(0, \\infty)$.\n    4.  The distributional assumption for $Y$.\n-   The expression for $q_p$ must be in terms of $p$, $\\mu$, and $\\sigma$.\n-   A specific calculation is required for $p=0.95$ with parameters $(\\mu, \\sigma^2) = (\\ln 100, 0.25)$.\n-   The final numerical answer must be rounded to four significant figures.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientifically Grounded:** The problem is sound. Modeling right-skewed, positive-valued physiological data (like triglyceride levels) with a log-normal distribution is a standard and well-accepted method in biostatistics and clinical medicine.\n-   **Well-Posed:** The problem is well-posed. It provides all necessary information and definitions to derive a unique analytical expression and compute a specific numerical value.\n-   **Objective:** The problem is stated using precise, objective, and formal mathematical and statistical language.\n\n**Step 3: Verdict and Action**\nThe problem is valid. It is a standard derivation and application problem in mathematical statistics. A solution will be provided.\n\n**Derivation of the $p$-th percentile of $X$**\n\nLet $X$ be a random variable following a log-normal distribution with parameters $(\\mu, \\sigma^2)$. This means that the random variable $Y = \\ln(X)$ follows a normal distribution with mean $\\mu$ and variance $\\sigma^2$, i.e., $Y \\sim N(\\mu, \\sigma^2)$. We wish to find the $p$-th percentile of $X$, which we denote as $q_p$.\n\nBy the definition of a percentile, $q_p$ is the value such that the probability of $X$ being less than or equal to $q_p$ is $p$. In terms of the cumulative distribution function (CDF) of $X$, denoted $F_X(x)$, this is expressed as:\n$$F_X(q_p) = P(X \\le q_p) = p$$\n\nSince the natural logarithm function, $\\ln(\\cdot)$, is strictly increasing on its domain $(0, \\infty)$, the inequality $X \\le q_p$ is equivalent to the inequality $\\ln(X) \\le \\ln(q_p)$. Therefore, we can write the probability as:\n$$P(X \\le q_p) = P(\\ln(X) \\le \\ln(q_p))$$\n\nSubstituting $Y = \\ln(X)$, we get:\n$$P(Y \\le \\ln(q_p)) = p$$\n\nThis expression is the CDF of the random variable $Y$, denoted $F_Y(y)$, evaluated at $y = \\ln(q_p)$:\n$$F_Y(\\ln(q_p)) = p$$\n\nTo evaluate $F_Y$, we standardize the normal variable $Y$. Let $Z$ be a standard normal random variable, $Z \\sim N(0, 1)$, defined as $Z = \\frac{Y - \\mu}{\\sigma}$. The CDF of $Z$ is denoted by $\\Phi(z) = P(Z \\le z)$.\n\nWe can express the CDF of $Y$ in terms of $\\Phi$:\n$$F_Y(y) = P(Y \\le y) = P\\left(\\frac{Y - \\mu}{\\sigma} \\le \\frac{y - \\mu}{\\sigma}\\right) = P\\left(Z \\le \\frac{y - \\mu}{\\sigma}\\right) = \\Phi\\left(\\frac{y - \\mu}{\\sigma}\\right)$$\n\nSubstituting $y = \\ln(q_p)$ into this relationship, we have:\n$$F_Y(\\ln(q_p)) = \\Phi\\left(\\frac{\\ln(q_p) - \\mu}{\\sigma}\\right)$$\n\nCombining our equations, we arrive at:\n$$p = \\Phi\\left(\\frac{\\ln(q_p) - \\mu}{\\sigma}\\right)$$\n\nTo solve for $q_p$, we first apply the inverse CDF of the standard normal distribution, which is the quantile function $\\Phi^{-1}(p)$, to both sides. Let $z_p = \\Phi^{-1}(p)$ be the $p$-th quantile of the standard normal distribution.\n$$\\Phi^{-1}(p) = \\frac{\\ln(q_p) - \\mu}{\\sigma}$$\n$$z_p = \\frac{\\ln(q_p) - \\mu}{\\sigma}$$\n\nNow, we solve for $\\ln(q_p)$:\n$$z_p \\sigma = \\ln(q_p) - \\mu$$\n$$\\ln(q_p) = \\mu + z_p \\sigma$$\n\nFinally, we exponentiate both sides to solve for $q_p$:\n$$q_p = \\exp(\\mu + z_p \\sigma)$$\n\nReplacing $z_p$ with its definition, $\\Phi^{-1}(p)$, we obtain the general expression for the $p$-th percentile of a log-normal distribution:\n$$q_p = \\exp(\\mu + \\sigma \\Phi^{-1}(p))$$\n\n**Calculation for the specified case**\n\nWe are asked to compute the $95$-th percentile, so $p=0.95$. The given parameters are $\\mu = \\ln 100$ and $\\sigma^2 = 0.25$.\nFirst, we find the standard deviation $\\sigma$:\n$$\\sigma = \\sqrt{\\sigma^2} = \\sqrt{0.25} = 0.5$$\n\nNext, we need the value of $\\Phi^{-1}(0.95)$, which is the $z$-score that corresponds to a cumulative probability of $0.95$. From standard normal tables or statistical software, this value is:\n$$z_{0.95} = \\Phi^{-1}(0.95) \\approx 1.64485$$\n\nNow, we substitute the values of $\\mu$, $\\sigma$, and $z_{0.95}$ into our derived formula for $q_p$:\n$$q_{0.95} = \\exp(\\ln 100 + 0.5 \\times \\Phi^{-1}(0.95))$$\n$$q_{0.95} \\approx \\exp(\\ln 100 + 0.5 \\times 1.64485)$$\n\nWe can simplify the expression using properties of exponents:\n$$q_{0.95} = \\exp(\\ln 100) \\times \\exp(0.5 \\times 1.64485)$$\n$$q_{0.95} = 100 \\times \\exp(0.822425)$$\n\nNow, we compute the numerical value:\n$$\\exp(0.822425) \\approx 2.27596$$\n$$q_{0.95} \\approx 100 \\times 2.27596 = 227.596$$\n\nThe problem requires rounding the final answer to four significant figures. The first four significant figures are $2$, $2$, $7$, and $5$. The fifth digit is $9$, so we round up the fourth digit.\n$$q_{0.95} \\approx 227.6$$\nThe unit is $\\mathrm{mg/dL}$.",
            "answer": "$$\n\\boxed{227.6}\n$$"
        },
        {
            "introduction": "Moving beyond describing a single population, a primary goal in medical research is to quantify the effect of a treatment or intervention. When analyzing log-normally distributed outcomes like the urinary albumin-to-creatinine ratio (UACR), we often fit linear models on the logarithmic scale. This exercise challenges you to translate the output from such a model—specifically, the estimated coefficient $\\hat{\\beta}$ and its standard error—into a confidence interval for the fold-change, $\\exp(\\beta)$, which is the clinically meaningful measure of effect . This is a core skill for interpreting the results of clinical studies and illustrates how inference performed on the log scale is transformed back into a readily interpretable result on the original scale.",
            "id": "4990433",
            "problem": "Consider a randomized clinical study of an antihypertensive therapy where the urinary albumin-to-creatinine ratio (UACR) is measured in $n$ patients before randomization and at a fixed follow-up time point. Empirical evidence and physiological considerations support that strictly positive concentration-type measures such as UACR are right-skewed and well-modeled by a log-normal distribution: if $Y$ denotes UACR, then $\\ln(Y)$ is modeled as Gaussian. A log-linear regression model is fitted for the follow-up UACR (adjusted for baseline UACR and age) on the natural logarithmic scale,\n$$\n\\ln(Y_{i}) = \\beta_0 + \\beta T_{i} + \\gamma \\ln(Y_{i,\\text{baseline}}) + \\delta \\ln(\\text{Age}_{i}) + \\varepsilon_{i},\n$$\nwhere $T_{i} \\in \\{0,1\\}$ is the treatment indicator, $\\varepsilon_{i}$ are independently and identically distributed with $\\varepsilon_{i} \\sim \\mathcal{N}(0,\\sigma^{2})$, and $\\beta_0$, $\\beta$, $\\gamma$, and $\\delta$ are regression coefficients. In this model, the parameter $\\beta$ represents the additive effect on the mean of $\\ln(Y)$ due to treatment, and the fold-change in the geometric mean of $Y$ comparing treatment versus control is $\\exp(\\beta)$.\n\nSuppose the maximum likelihood estimator (MLE) $\\hat{\\beta}$ and its estimated standard error $\\operatorname{SE}(\\hat{\\beta})$ from the fitted model are $\\hat{\\beta} = 0.300$ and $\\operatorname{SE}(\\hat{\\beta}) = 0.120$. Assume standard regularity conditions ensuring that the MLE is approximately normal for large samples. Let $\\alpha = 0.05$. Using only foundational definitions and well-tested facts about the log-normal model, the normal approximation for MLEs, and monotone transformations, compute the $100(1-\\alpha)\\%$ confidence interval for the fold-change $\\exp(\\beta)$.\n\nExpress the final confidence interval endpoints as dimensionless decimal numbers and round your answer to four significant figures. Report the two endpoints as your final answer.",
            "solution": "The user has provided a problem in statistical inference which, upon thorough review, is deemed valid. The problem is scientifically grounded in standard biostatistical modeling, is well-posed with a unique and stable solution, and is expressed in objective, formal language. It is self-contained and free of contradictions. I will now proceed with a complete solution.\n\nThe objective is to compute a $100(1-\\alpha)\\%$ confidence interval for the fold-change parameter, $\\exp(\\beta)$, based on estimates from a log-linear regression model.\n\nThe provided information consists of:\n1.  The maximum likelihood estimate (MLE) of the treatment effect coefficient, $\\hat{\\beta} = 0.300$.\n2.  The estimated standard error of this estimator, $\\operatorname{SE}(\\hat{\\beta}) = 0.120$.\n3.  The significance level for the confidence interval, $\\alpha = 0.05$, which corresponds to a $100(1-0.05)\\% = 95\\%$ confidence level.\n4.  The assumption that, for a large sample size $n$, the sampling distribution of the MLE $\\hat{\\beta}$ is approximately normal.\n\nThe solution proceeds in two main steps: first, we construct a confidence interval for the parameter $\\beta$ on the log scale; second, we transform this interval to obtain the confidence interval for the fold-change $\\exp(\\beta)$.\n\nStep 1: Construct the $95\\%$ confidence interval for $\\beta$.\n\nThe problem states that under standard regularity conditions, the MLE $\\hat{\\beta}$ is approximately normally distributed. Specifically, the distribution of $\\hat{\\beta}$ can be approximated as:\n$$ \\hat{\\beta} \\sim \\mathcal{N}\\left(\\beta, \\left[\\operatorname{SE}(\\hat{\\beta})\\right]^2\\right) $$\nFrom this, we can construct the standardized quantity $Z$:\n$$ Z = \\frac{\\hat{\\beta} - \\beta}{\\operatorname{SE}(\\hat{\\beta})} \\sim \\mathcal{N}(0, 1) $$\nA $100(1-\\alpha)\\%$ confidence interval for $\\beta$ is an interval $[\\beta_L, \\beta_U]$ such that $P(\\beta_L \\le \\beta \\le \\beta_U) = 1-\\alpha$. This can be constructed from the pivotal quantity $Z$. We find the critical value $z_{1-\\alpha/2}$ from the standard normal distribution such that $P(-z_{1-\\alpha/2} \\le Z \\le z_{1-\\alpha/2}) = 1-\\alpha$.\nFor a significance level of $\\alpha = 0.05$, we require the quantile $z_{1-0.05/2} = z_{0.975}$. This is a standard, well-established value:\n$$ z_{0.975} \\approx 1.96 $$\nUsing this critical value, the $95\\%$ confidence interval for $\\beta$ is given by the formula:\n$$ \\left[ \\hat{\\beta} - z_{1-\\alpha/2} \\operatorname{SE}(\\hat{\\beta}), \\; \\hat{\\beta} + z_{1-\\alpha/2} \\operatorname{SE}(\\hat{\\beta}) \\right] $$\nSubstituting the given numerical values:\n$$ \\left[ 0.300 - 1.96 \\times 0.120, \\; 0.300 + 1.96 \\times 0.120 \\right] $$\nFirst, we calculate the margin of error:\n$$ \\text{Margin of Error} = 1.96 \\times 0.120 = 0.2352 $$\nNow, we compute the lower and upper bounds for the confidence interval of $\\beta$:\n$$ \\beta_L = 0.300 - 0.2352 = 0.0648 $$\n$$ \\beta_U = 0.300 + 0.2352 = 0.5352 $$\nThus, the $95\\%$ confidence interval for $\\beta$ is $[0.0648, 0.5352]$.\n\nStep 2: Transform the confidence interval to the scale of the fold-change $\\exp(\\beta)$.\n\nThe parameter of interest is the fold-change in the geometric mean, which is given by $\\exp(\\beta)$. A fundamental property of confidence intervals is that they can be transformed by monotone functions. If $[L, U]$ is a $100(1-\\alpha)\\%$ confidence interval for a parameter $\\theta$, and $g(\\cdot)$ is a strictly increasing function, then $[g(L), g(U)]$ is a $100(1-\\alpha)\\%$ confidence interval for $g(\\theta)$.\n\nIn this problem, the function is the natural exponential function, $g(\\beta) = \\exp(\\beta)$. This function is strictly increasing for all real numbers. Therefore, we can obtain the confidence interval for the fold-change $\\exp(\\beta)$ by exponentiating the endpoints of the confidence interval for $\\beta$.\n\nThe lower bound of the confidence interval for $\\exp(\\beta)$ is:\n$$ \\exp(\\beta_L) = \\exp(0.0648) $$\nThe upper bound of the confidence interval for $\\exp(\\beta)$ is:\n$$ \\exp(\\beta_U) = \\exp(0.5352) $$\n\nNow, we compute the numerical values for these endpoints:\n$$ \\text{Lower bound} = \\exp(0.0648) \\approx 1.0669528 $$\n$$ \\text{Upper bound} = \\exp(0.5352) \\approx 1.7077836 $$\nThe problem requires the final answer to be rounded to four significant figures.\n- For the lower bound, $1.0669528...$, the first four significant figures are $1.066$. The fifth digit is $9$, so we round up the fourth digit. The rounded value is $1.067$.\n- For the upper bound, $1.7077836...$, the first four significant figures are $1.707$. The fifth digit is $7$, so we round up the fourth digit. The rounded value is $1.708$.\n\nTherefore, the $95\\%$ confidence interval for the fold-change $\\exp(\\beta)$ is approximately $[1.067, 1.708]$. These endpoints are presented as the final answer.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1.067  1.708\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "The validity of the models and intervals we construct rests on underlying assumptions, and a critical statistician must know how to evaluate them. This practice focuses on the crucial step of model diagnostics: assessing whether a dataset is consistent with the log-normal assumption . Through a series of conceptual questions centered on the Shapiro-Wilk test for normality, you will explore the power and pitfalls of formal goodness-of-fit tests, particularly in the context of small sample sizes common in pilot studies. This exercise is designed to sharpen your critical thinking and move you from a mechanical calculator of statistics to a nuanced interpreter of evidence.",
            "id": "4990445",
            "problem": "A clinical pharmacology pilot study measures a positive biomarker concentration $X$ (in appropriate units) in $n=12$ adult participants. The assay reports values to one decimal place, so repeated values (ties) occur. Exploratory plots suggest right-skew on the original scale. One observation is relatively large compared with the rest, with $X=42.0$. Investigators posit that $X$ may be approximately log-normal, that is, that $Y=\\ln X$ is approximately normally distributed.\n\nThey fit no model at this stage and instead perform a normality diagnostic on $Y=\\ln X$ using the Shapiro–Wilk test, which is a goodness-of-fit test for normality. The Shapiro–Wilk test on $X$ (untransformed) returns a $p$-value of $p=0.004$. The Shapiro–Wilk test on $Y=\\ln X$ returns a $p$-value of $p=0.18$. No observations are at or below zero.\n\nUse only the following foundational facts and definitions in your reasoning:\n\n- A variable $X$ is log-normal if and only if $Y=\\ln X$ is normal.\n- A statistical hypothesis test is defined by a null hypothesis $H_0$ and an alternative $H_1$. The $p$-value is the probability, under $H_0$, of obtaining a test statistic at least as extreme as the observed value. At significance level $\\alpha$, the rule “reject $H_0$ if $p \\le \\alpha$” achieves Type I error rate $\\alpha$.\n- The power of a test is the probability of correctly rejecting $H_0$ when $H_1$ is true; power generally increases with sample size for fixed departure from $H_0$.\n- The Shapiro–Wilk test is an omnibus test for normality known to be among the more powerful tests for detecting departures from normality, particularly in small to moderate samples, compared with many alternatives such as the Kolmogorov–Smirnov test with a normal reference.\n\nBased on these premises and the study description, select all statements that are correct.\n\nA. With $p=0.18$ on $Y=\\ln X$ at $\\alpha=0.05$, we have proven that $X$ is log-normal.\n\nB. Testing $Y=\\ln X$ for normality is the appropriate way to diagnose log-normality of $X$. However, with $n=12$, failure to reject at $\\alpha=0.05$ provides only weak evidence; the Shapiro–Wilk test can have limited power to detect moderate deviations, so results should be complemented by a quantile–quantile plot and subject-matter considerations.\n\nC. Because the assay reports to one decimal place, the resulting ties can distort the null distribution of the Shapiro–Wilk statistic on $Y=\\ln X$, making its $p$-value potentially unreliable; in such cases, methods that account for rounding or censoring, or sensitivity analyses (for example, jittering), may be warranted.\n\nD. The presence of one large $X=42.0$ observation cannot influence the Shapiro–Wilk test on $Y=\\ln X$, because the logarithm removes outliers by construction.\n\nE. If the Shapiro–Wilk test on $Y=\\ln X$ does not reject at $\\alpha=0.05$, then the usual back-transformed confidence interval for the geometric mean of $X$ constructed under normality of $Y$ has guaranteed coverage of at least $95\\%$ even in small samples.\n\nF. In small samples, the Shapiro–Wilk test on $Y=\\ln X$ typically has higher power than the Kolmogorov–Smirnov test with a normal reference to detect departures from normality due to skewness and tail-weight.\n\nG. If investigators test normality on both $X$ and $Y=\\ln X$ and then choose the scale with the larger $p$-value for subsequent parametric inference at $\\alpha=0.05$, the overall Type I error for those downstream inferences remains exactly $0.05$.",
            "solution": "The problem statement describes a common scenario in biostatistics: assessing the distributional properties of a biomarker, particularly whether a log-transformation is appropriate for achieving normality. The provided information is consistent with established statistical principles and practices. The problem is scientifically grounded, well-posed, and objective. There are no contradictions, missing pieces of critical information, or factual errors. Therefore, the problem statement is valid. We will proceed to evaluate each option.\n\nThe core of the problem involves interpreting the results of two Shapiro–Wilk tests for normality. The test on the original data, $X$, yielded a $p$-value of $p=0.004$. At a conventional significance level like $\\alpha=0.05$, this small $p$-value leads to the rejection of the null hypothesis that $X$ is normally distributed. This aligns with the exploratory observation of right-skew. The test on the log-transformed data, $Y = \\ln X$, yielded a $p$-value of $p=0.18$. Since $0.18 > 0.05$, we fail to reject the null hypothesis that $Y$ is normally distributed. This suggests that a log-normal distribution might be a plausible model for $X$.\n\n**A. With $p=0.18$ on $Y=\\ln X$ at $\\alpha=0.05$, we have proven that $X$ is log-normal.**\n\nThis statement is fundamentally incorrect. In hypothesis testing, failing to reject the null hypothesis ($H_0$) is not equivalent to proving that $H_0$ is true. A $p$-value of $p=1.8$ simply means that the observed data are reasonably consistent with the null hypothesis (that $Y=\\ln X$ is normal). It does not rule out the possibility that the data come from a different, non-normal distribution that the test, with its given sample size, did not have enough power to distinguish from normality. The phrase \"we have proven\" is an absolute claim that statistical hypothesis testing cannot support. This is a classic misinterpretation of a non-significant $p$-value, often summarized as \"absence of evidence is not evidence of absence\".\n\nVerdict: **Incorrect**.\n\n**B. Testing $Y=\\ln X$ for normality is the appropriate way to diagnose log-normality of $X$. However, with $n=12$, failure to reject at $\\alpha=0.05$ provides only weak evidence; the Shapiro–Wilk test can have limited power to detect moderate deviations, so results should be complemented by a quantile–quantile plot and subject-matter considerations.**\n\nThis statement consists of several parts, all of which are statistically sound.\n1.  \"Testing $Y=\\ln X$ for normality is the appropriate way to diagnose log-normality of $X$.\" This is true by the definition provided: \"$X$ is log-normal if and only if $Y=\\ln X$ is normal.\"\n2.  \"However, with $n=12$, failure to reject at $\\alpha=0.05$ provides only weak evidence\". This is correct. The sample size of $n=12$ is very small, and statistical tests generally have low power in such scenarios, as stated in the premises (\"power generally increases with sample size\"). A non-rejection provides little conclusive information.\n3.  \"the Shapiro–Wilk test can have limited power to detect moderate deviations\". This is a direct consequence of the small sample size.\n4.  \"so results should be complemented by a quantile–quantile plot and subject-matter considerations.\" This represents best practice in applied statistics. Formal hypothesis tests for model assumptions should not be used in isolation. Visual diagnostics like quantile–quantile (Q-Q) plots are crucial for assessing the nature of any potential deviation from normality, and subject-matter expertise can inform whether the chosen distribution is plausible for the biological process being measured. The statement as a whole is an accurate and nuanced summary of the situation.\n\nVerdict: **Correct**.\n\n**C. Because the assay reports to one decimal place, the resulting ties can distort the null distribution of the Shapiro–Wilk statistic on $Y=\\ln X$, making its $p$-value potentially unreliable; in such cases, methods that account for rounding or censoring, or sensitivity analyses (for example, jittering), may be warranted.**\n\nThis statement addresses a subtle but important practical issue. The Shapiro–Wilk test statistic is based on the order statistics of the sample and is formally derived under the assumption that the data come from a continuous distribution. In a truly continuous distribution, the probability of two observations being identical (a tie) is zero. The problem states that the assay's limited precision (\"reports to one decimal place\") causes ties to occur. If $X_i = X_j$, then $Y_i = \\ln(X_i) = \\ln(X_j) = Y_j$, so ties in $X$ will also be present in $Y$. The presence of ties violates the underlying assumption of the test. While some software implementations have adjustments for ties, these are often approximations. The standard null distribution of the test statistic may not be accurate, making the resulting $p$-value unreliable. The suggested remedies—using methods designed for rounded/censored data or performing sensitivity analyses like jittering (adding small random noise to break ties)—are appropriate ways to assess the robustness of the conclusion to this data feature.\n\nVerdict: **Correct**.\n\n**D. The presence of one large $X=42.0$ observation cannot influence the Shapiro–Wilk test on $Y=\\ln X$, because the logarithm removes outliers by construction.**\n\nThis statement is incorrect. While the natural logarithm transformation, $Y = \\ln X$, is a concave function that compresses the scale for large values of $X$, it does not \"remove outliers by construction\". An observation that is extreme on the original scale will typically remain an extreme observation (though less dramatically so) on the log scale. For instance, if other $X$ values were clustered around $3.0$, then $X=42.0$ is an outlier. The corresponding log-transformed values would be around $\\ln(3.0) \\approx 1.1$ and $Y = \\ln(42.0) \\approx 3.74$. The value $3.74$ might still be an outlier relative to the other values clustered around $1.1$. The Shapiro–Wilk test statistic is a function of the entire set of ordered sample values and is certainly sensitive to extreme observations (outliers), as these strongly affect the overall shape of the empirical distribution. The claim that the outlier \"cannot influence\" the test is false.\n\nVerdict: **Incorrect**.\n\n**E. If the Shapiro–Wilk test on $Y=\\ln X$ does not reject at $\\alpha=0.05$, then the usual back-transformed confidence interval for the geometric mean of $X$ constructed under normality of $Y$ has guaranteed coverage of at least $95\\%$ even in small samples.**\n\nThis statement makes a claim of \"guaranteed coverage\", which is false. The confidence interval for the mean of $Y$, $\\mu_Y$, is constructed as $\\bar{Y} \\pm t_{n-1, 1-\\alpha/2} (s_Y/\\sqrt{n})$, and its nominal coverage of $(1-\\alpha)100\\%$ (e.g., $95\\%$) depends on the assumption that $Y$ is normally distributed. Failing to reject the normality of $Y$ with a low-power test (due to $n=12$) does not prove that $Y$ is normal. If the true distribution of $Y$ deviates from normality, the actual coverage of this t-interval (and thus the back-transformed interval for the geometric mean of $X$) is not guaranteed to be at its nominal level. The Central Limit Theorem, which could provide robustness for the mean, has little effect at such a small sample size. Therefore, there is no guarantee of $95\\%$ coverage.\n\nVerdict: **Incorrect**.\n\n**F. In small samples, the Shapiro–Wilk test on $Y=\\ln X$ typically has higher power than the Kolmogorov–Smirnov test with a normal reference to detect departures from normality due to skewness and tail-weight.**\n\nThis statement is a well-established fact in statistical literature and is directly supported by the premises provided in the problem statement. The problem explicitly states that the Shapiro–Wilk test \"is known to be among the more powerful tests for detecting departures from normality, particularly in small to moderate samples, compared with many alternatives such as the Kolmogorov–Smirnov test\". Departures from normality are most commonly characterized by skewness (asymmetry) and kurtosis (tail-weight). The Shapiro–Wilk test is specifically designed to be sensitive to these types of departures, which is why it is broadly recommended for testing normality over more general goodness-of-fit tests like the Kolmogorov–Smirnov test (when the parameters of the normal distribution are estimated, this becomes the Lilliefors test).\n\nVerdict: **Correct**.\n\n**G. If investigators test normality on both $X$ and $Y=\\ln X$ and then choose the scale with the larger $p$-value for subsequent parametric inference at $\\alpha=0.05$, the overall Type I error for those downstream inferences remains exactly $0.05$.**\n\nThis statement is incorrect. It describes a two-stage procedure where the statistical model is chosen based on a preliminary test on the same data that will be used for the final inference. This practice is known to distort the properties of the subsequent statistical tests. The nominal significance level $\\alpha$ of a test is valid under the assumption that the model was specified *a priori* (before seeing the data). By choosing the transformation that makes the data appear \"more normal\" (i.e., has a larger $p$-value from the normality test), the investigator is effectively \"gaming\" the system. This data-dependent model selection process inflates the overall Type I error rate of the final inferential procedure to a level greater than the nominal $\\alpha=0.05$. The claim that the error rate \"remains exactly $0.05$\" is false.\n\nVerdict: **Incorrect**.",
            "answer": "$$\\boxed{BCF}$$"
        }
    ]
}