{
    "hands_on_practices": [
        {
            "introduction": "A foundational skill in statistical modeling is the ability to correctly calculate and interpret a $p$-value from a standard test statistic. This first exercise provides essential practice in this area, focusing on a common epidemiological scenario involving a large-sample Z-test. By working through this problem , you will reinforce your understanding of the formal definition of a two-sided $p$-value and the mechanics of computing it as a tail probability from the standard normal distribution.",
            "id": "4617807",
            "problem": "A cohort study investigates whether high long-term exposure to fine particulate matter in air pollution is associated with incident asthma. Let the logarithm of the relative risk be denoted by $\\beta$. The estimator $\\hat{\\beta}$ is approximately normal with mean $0$ and variance $s_{\\hat{\\beta}}^{2}$ under the null hypothesis $H_{0}:\\beta=0$, by the Central Limit Theorem (CLT). Consider the standardized test statistic $Z=\\hat{\\beta}/s_{\\hat{\\beta}}$, which under $H_{0}$ has a standard normal distribution with mean $0$ and variance $1$. In the study, the observed value is $Z_{\\text{obs}}=2.1$. Using the definition of a two-sided $p$-value as the probability, computed under $H_{0}$, of obtaining a test statistic at least as extreme as the observed value in either tail, derive an expression for the two-sided $p$-value in terms of the standard normal cumulative distribution function $\\Phi(\\cdot)$ and compute its numerical value for $Z_{\\text{obs}}=2.1$. Round your numerical answer to four significant figures. Finally, briefly interpret the meaning of this $p$-value in the epidemiological context given.",
            "solution": "The problem is subjected to validation.\n\n### Step 1: Extract Givens\n- A cohort study investigates the association between long-term exposure to fine particulate matter and incident asthma.\n- The logarithm of the relative risk is denoted by $\\beta$.\n- The null hypothesis is $H_{0}: \\beta=0$.\n- The estimator for $\\beta$ is $\\hat{\\beta}$.\n- Under $H_{0}$, the distribution of the estimator is approximately normal: $\\hat{\\beta} \\sim N(0, s_{\\hat{\\beta}}^{2})$.\n- The standardized test statistic is $Z=\\hat{\\beta}/s_{\\hat{\\beta}}$.\n- Under $H_{0}$, the distribution of the test statistic is standard normal: $Z \\sim N(0, 1)$.\n- The observed value of the test statistic is $Z_{\\text{obs}}=2.1$.\n- The definition of a two-sided $p$-value is the probability, computed under $H_{0}$, of obtaining a test statistic at least as extreme as the observed value in either tail.\n- The standard normal cumulative distribution function (CDF) is denoted by $\\Phi(\\cdot)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically and mathematically sound. It presents a standard scenario in biostatistics and epidemiology: hypothesis testing for an association parameter (log relative risk) derived from a cohort study.\n- **Scientifically Grounded:** The problem uses established statistical principles (Central Limit Theorem, hypothesis testing, Z-statistic, p-value) within a realistic epidemiological context. The relationship between air pollution and asthma is a valid area of scientific inquiry.\n- **Well-Posed:** All necessary information is provided to derive the expression for the $p$-value, calculate its value, and interpret the result. The task is clear and unambiguous.\n- **Objective:** The language is formal, precise, and free of subjective claims.\n\nThe problem does not violate any of the invalidity criteria. It is complete, consistent, realistic, and well-structured.\n\n### Step 3: Verdict and Action\nThe problem is valid. A full solution will be provided.\n\nThe solution consists of three parts as requested: deriving an expression for the two-sided $p$-value, computing its numerical value, and providing an interpretation in the given context.\n\n**Part 1: Derivation of the $p$-value expression**\n\nThe two-sided $p$-value is defined as the probability, under the null hypothesis $H_{0}$, of observing a test statistic at least as extreme as the one observed. The test statistic is $Z$, which follows a standard normal distribution, $Z \\sim N(0, 1)$, under $H_{0}$. The observed value is $Z_{\\text{obs}} = 2.1$.\n\nAn outcome \"at least as extreme\" as $Z_{\\text{obs}}$ means the value of the test statistic $Z$ is either greater than or equal to $Z_{\\text{obs}}$ or less than or equal to $-Z_{\\text{obs}}$. Since the observed value is positive, this is equivalent to the test statistic's absolute value being greater than or equal to the observed value's absolute value.\nThe $p$-value is therefore given by the probability:\n$$p = P(|Z| \\ge |Z_{\\text{obs}}| | H_{0})$$\nSubstituting the observed value $Z_{\\text{obs}} = 2.1$:\n$$p = P(|Z| \\ge 2.1)$$\nThis inequality can be split into two disjoint events, representing the two tails of the distribution:\n$$p = P(Z \\ge 2.1) + P(Z \\le -2.1)$$\nThe standard normal distribution is symmetric about its mean of $0$. Therefore, the probability in the left tail is equal to the probability in the right tail:\n$$P(Z \\le -2.1) = P(Z \\ge 2.1)$$\nThe expression for the $p$-value can thus be simplified to:\n$$p = 2 \\times P(Z \\ge 2.1)$$\nThe standard normal cumulative distribution function (CDF), $\\Phi(z)$, is defined as $\\Phi(z) = P(Z \\le z)$. The probability in the right tail can be expressed in terms of the CDF as:\n$$P(Z \\ge z) = 1 - P(Z < z)$$\nSince the normal distribution is a continuous distribution, $P(Z < z) = P(Z \\le z) = \\Phi(z)$. Thus:\n$$P(Z \\ge 2.1) = 1 - \\Phi(2.1)$$\nSubstituting this back into the expression for the $p$-value, we get the desired expression in terms of the standard normal CDF:\n$$p = 2 \\times (1 - \\Phi(2.1))$$\nFor a general observed test statistic $Z_{\\text{obs}}$, the expression is $p = 2(1 - \\Phi(|Z_{\\text{obs}}|))$.\n\n**Part 2: Numerical computation of the $p$-value**\n\nWe need to compute the value of $p = 2 \\times (1 - \\Phi(2.1))$. Using standard statistical tables or software, the value of the standard normal CDF at $z=2.1$ is approximately:\n$$\\Phi(2.1) \\approx 0.98213558$$\nNow, we compute the $p$-value:\n$$p \\approx 2 \\times (1 - 0.98213558)$$\n$$p \\approx 2 \\times 0.01786442$$\n$$p \\approx 0.03572884$$\nThe problem requires the answer to be rounded to four significant figures. The first significant figure is $3$. The next three are $5$, $7$, and $2$. The fifth significant figure is $8$, which is $5$ or greater, so we round up the fourth significant figure ($2$) to $3$.\n$$p \\approx 0.03573$$\n\n**Part 3: Interpretation of the $p$-value**\n\nThe calculated $p$-value is approximately $0.03573$. In the context of the epidemiological study, this value has a precise meaning. The null hypothesis, $H_{0}: \\beta=0$, corresponds to no association between high long-term exposure to fine particulate matter and the incidence of asthma (i.e., the relative risk is $1$). The $p$-value is the probability of observing data that suggest an association at least as strong as the one found in this study (represented by $Z_{\\text{obs}}=2.1$), *assuming that the null hypothesis is true*.\n\nTherefore, the interpretation is: If there were truly no association between long-term exposure to fine particulate matter and the incidence of asthma, there would be a $3.573\\%$ probability of observing an association as strong as, or stronger than, the one detected in this study purely due to random chance or sampling variability. A small $p$-value (typically less than a pre-specified significance level such as $0.05$) is often taken as evidence to reject the null hypothesis in favor of the alternative hypothesis that an association does exist.",
            "answer": "$$\n\\boxed{0.03573}\n$$"
        },
        {
            "introduction": "While large-sample approximations are powerful, much of medical research deals with small sample sizes or categorical outcomes where such methods are inappropriate. This practice  transitions to the setting of exact tests, using Fisher’s exact test for a $2 \\times 2$ contingency table. Here, you will discover the important concept of conservatism, a phenomenon where the actual probability of a Type I error is less than the nominal significance level $\\alpha$, a direct consequence of the discrete nature of the underlying hypergeometric distribution.",
            "id": "4617755",
            "problem": "In a cohort study in epidemiology, consider a binary exposure and a binary outcome summarized in a $2 \\times 2$ contingency table. Suppose the row totals (group sizes) are fixed at $n_{1} = 5$ exposed and $n_{2} = 5$ unexposed, and the column totals (outcome counts) are fixed at $K = 5$ outcome-present and $N - K = 5$ outcome-absent, where $N = n_{1} + n_{2} = 10$. Let $X$ denote the number of outcome-present individuals in the exposed group.\n\nYou wish to test the null hypothesis $H_{0}: p_{1} = p_{2}$ against the one-sided alternative $H_{A}: p_{1} > p_{2}$ (higher risk among the exposed) using Fisher’s exact test at nominal level $\\alpha = 0.05$. Under Fisher’s framework, conditioning on the fixed margins yields that $X$ follows the hypergeometric distribution with population size $N$, number of successes $K$, and draws $n_{1}$.\n\nStarting from the core definition of a $p$-value as the probability, under $H_{0}$, of observing a test statistic at least as extreme as the one realized, explain why, in discrete settings, exact tests yield conservative $p$-values (that is, the actual Type I error rate is less than or equal to the nominal $\\alpha$). Then, for the given table, derive the rejection threshold $c$ such that the Fisher exact test rejects $H_{0}$ when $X \\geq c$. Compute the exact Type I error rate $P_{0}(X \\geq c)$ under $H_{0}$ and quantify the conservatism by the difference $\\alpha - P_{0}(X \\geq c)$.\n\nExpress your final numerical answer for $\\alpha - P_{0}(X \\geq c)$ as a simplified fraction. No rounding is required.",
            "solution": "The problem statement is validated as scientifically grounded, well-posed, objective, and complete. It represents a standard application of Fisher's exact test in biostatistics and epidemiology. All necessary parameters and conditions are provided for a unique and meaningful solution.\n\nThe problem asks for three things: an explanation of why exact tests for discrete data are conservative, the derivation of the rejection threshold for a specific Fisher's exact test, and the quantification of this conservatism.\n\nFirst, we address the conservatism of exact tests with discrete test statistics. Let $T$ be a test statistic that can take on a discrete set of values $\\{t_1, t_2, \\ldots, t_m\\}$ under the null hypothesis $H_0$. The $p$-value for an observed value $t_{obs}$ is the probability of observing a result at least as extreme as $t_{obs}$, assuming $H_0$ is true. For a one-sided test against an alternative like $H_A: p_1 > p_2$, \"at least as extreme\" corresponds to values of the test statistic greater than or equal to the observed value. The $p$-value is thus $P_0(T \\ge t_{obs})$.\n\nA hypothesis test is conducted at a nominal significance level $\\alpha$. We reject $H_0$ if the $p$-value is less than or equal to $\\alpha$. The actual Type I error rate, let's call it $\\alpha_{actual}$, is the probability of rejecting $H_0$ when it is true. This can be expressed as:\n$$ \\alpha_{actual} = P_0(\\text{reject } H_0) $$\nRejection occurs for a set of outcomes, the rejection region $R$. For a one-sided test with large values of $T$ being extreme, we define a critical value $c$ and the rejection region is $R = \\{t | t \\ge c\\}$. The actual Type I error rate is then $\\alpha_{actual} = P_0(T \\ge c)$.\nTo maintain the nominal significance level $\\alpha$, we must choose the critical value $c$ such that $\\alpha_{actual} \\le \\alpha$. That is, we must find $c$ such that $P_0(T \\ge c) \\le \\alpha$. To maximize the power of the test, we want the largest possible rejection region (and thus the smallest $c$) that satisfies this condition.\n\nBecause the distribution of $T$ is discrete, the cumulative probability $P_0(T \\ge k)$ can only take on a finite number of values as $k$ varies. It is therefore highly unlikely that for a pre-specified $\\alpha$ (e.g., $\\alpha = 0.05$), we can find a critical value $c$ such that $P_0(T \\ge c)$ is exactly equal to $\\alpha$. Instead, we choose the smallest $c$ such that $P_0(T \\ge c) \\le \\alpha$. This choice typically results in a strict inequality, $P_0(T \\ge c) < \\alpha$. The actual probability of a Type I error, $\\alpha_{actual}$, is less than the nominal level $\\alpha$. This phenomenon is what makes the test \"conservative\".\n\nNext, we apply this to the specific problem. The test statistic is $X$, the number of outcome-present individuals in the exposed group. The problem states that under the null hypothesis of no association ($p_1 = p_2$), and conditioning on the marginal totals, $X$ follows a hypergeometric distribution. The parameters are:\n- Population size $N = 10$ (total individuals)\n- Number of \"successes\" in the population $K = 5$ (total outcome-present)\n- Number of draws $n_1 = 5$ (size of the exposed group)\n\nThe probability mass function (PMF) for $X$ is given by:\n$$ P(X=x) = \\frac{\\binom{K}{x} \\binom{N-K}{n_1-x}}{\\binom{N}{n_1}} $$\nSubstituting the given values:\n$$ P(X=x) = \\frac{\\binom{5}{x} \\binom{10-5}{5-x}}{\\binom{10}{5}} = \\frac{\\binom{5}{x} \\binom{5}{5-x}}{\\binom{10}{5}} $$\nThe denominator is $\\binom{10}{5} = \\frac{10 \\cdot 9 \\cdot 8 \\cdot 7 \\cdot 6}{5 \\cdot 4 \\cdot 3 \\cdot 2 \\cdot 1} = 252$. The possible values for $X$ are integers from $\\max(0, n_1+K-N)$ to $\\min(n_1, K)$, which is $\\max(0, 5+5-10) = 0$ to $\\min(5,5) = 5$. So, $x \\in \\{0, 1, 2, 3, 4, 5\\}$.\n\nWe need to find the rejection threshold $c$ for the one-sided test $H_A: p_1 > p_2$ at level $\\alpha = 0.05$. Large values of $X$ support the alternative hypothesis. We seek the smallest integer $c$ such that $P_0(X \\ge c) \\le 0.05$. To do this, we calculate the probabilities for the upper tail of the distribution.\n\n- For $x=5$: $P(X=5) = \\frac{\\binom{5}{5}\\binom{5}{0}}{252} = \\frac{1 \\cdot 1}{252} = \\frac{1}{252}$.\n- For $x=4$: $P(X=4) = \\frac{\\binom{5}{4}\\binom{5}{1}}{252} = \\frac{5 \\cdot 5}{252} = \\frac{25}{252}$.\n- For $x=3$: $P(X=3) = \\frac{\\binom{5}{3}\\binom{5}{2}}{252} = \\frac{10 \\cdot 10}{252} = \\frac{100}{252}$.\n\nNow we evaluate the cumulative tail probabilities:\n- $P(X \\ge 5) = P(X=5) = \\frac{1}{252}$. As a decimal, this is approximately $0.00397$.\n- $P(X \\ge 4) = P(X=4) + P(X=5) = \\frac{25}{252} + \\frac{1}{252} = \\frac{26}{252}$. As a decimal, this is approximately $0.1032$.\n\nThe nominal significance level is $\\alpha = 0.05$. We need to find the smallest $c$ such that $P(X \\ge c) \\le 0.05$.\n- For $c=5$, we have $P(X \\ge 5) = \\frac{1}{252} \\approx 0.00397$, which is $\\le 0.05$.\n- For $c=4$, we have $P(X \\ge 4) = \\frac{26}{252} \\approx 0.1032$, which is $> 0.05$.\n\nTherefore, to ensure the Type I error rate does not exceed $0.05$, we must choose the rejection region to be $\\{X=5\\}$. This corresponds to a rejection threshold of $c=5$. The rule is: reject $H_0$ if $X \\ge 5$, which is equivalent to rejecting $H_0$ only if $X=5$.\n\nThe exact Type I error rate is the probability of this rejection region under $H_0$:\n$$ \\alpha_{actual} = P_0(X \\ge c) = P_0(X \\ge 5) = \\frac{1}{252} $$\nFinally, we quantify the conservatism of the test by the difference between the nominal level $\\alpha$ and the actual Type I error rate $\\alpha_{actual}$.\n$$ \\alpha - P_0(X \\ge c) = 0.05 - \\frac{1}{252} $$\nWe express $0.05$ as a fraction: $0.05 = \\frac{5}{100} = \\frac{1}{20}$.\nThe difference is:\n$$ \\frac{1}{20} - \\frac{1}{252} $$\nTo subtract the fractions, we find a common denominator. The least common multiple of $20 = 2^2 \\cdot 5$ and $252 = 2^2 \\cdot 3^2 \\cdot 7$ is $2^2 \\cdot 3^2 \\cdot 5 \\cdot 7 = 4 \\cdot 9 \\cdot 5 \\cdot 7 = 1260$.\n$$ \\frac{1}{20} = \\frac{1 \\cdot 63}{20 \\cdot 63} = \\frac{63}{1260} $$\n$$ \\frac{1}{252} = \\frac{1 \\cdot 5}{252 \\cdot 5} = \\frac{5}{1260} $$\nThe difference is:\n$$ \\frac{63}{1260} - \\frac{5}{1260} = \\frac{58}{1260} $$\nThis fraction can be simplified by dividing the numerator and denominator by their greatest common divisor, which is $2$:\n$$ \\frac{58 \\div 2}{1260 \\div 2} = \\frac{29}{630} $$\nSince $29$ is a prime number and $630$ is not a multiple of $29$, this is the simplified fraction.",
            "answer": "$$\\boxed{\\frac{29}{630}}$$"
        },
        {
            "introduction": "Our final practice addresses a critical question: how can we perform a valid hypothesis test when we cannot justify the distributional assumptions of standard parametric tests? This exercise  introduces the permutation test, a powerful and intuitive computational method grounded in the principle of exchangeability. By implementing a permutation test for a difference in means, you will gain a first-principles understanding of how to generate a null distribution directly from the data, providing a robust and assumption-light approach to calculating $p$-values.",
            "id": "4617824",
            "problem": "Consider a two-group comparison commonly encountered in epidemiology, where a continuous outcome is measured in an exposed group and an unexposed group. When the assumption of normality is doubtful, a principled approach to obtaining a valid $p$-value is to use a permutation test under the assumption of exchangeability. Exchangeability asserts that, under the null hypothesis of no effect, the joint distribution of outcomes is invariant to permutations of exposure labels. \n\nYou are required to formalize the permutation $p$-value based on exchangeability and implement a program that computes two-sided permutation $p$-values for the difference in means between two groups. The test statistic must be the difference in sample means, defined as $T = \\bar{X}_E - \\bar{X}_U$, where $\\bar{X}_E$ and $\\bar{X}_U$ denote the sample means of the exposed and unexposed groups, respectively. The two-sided $p$-value is to be computed using the absolute value $|T|$ as the measure of extremeness. Under exchangeability, the null distribution of $T$ is generated by relabeling the observed outcomes across all assignments of $n_E$ exposed labels and $n_U$ unexposed labels across the $n = n_E + n_U$ observations, or approximated via Monte Carlo (MC) sampling when exact enumeration is computationally infeasible.\n\nYour program must:\n- Accept a hard-coded test suite of cases, each case consisting of two lists of observed outcomes (real numbers) corresponding to exposed and unexposed groups, and an integer $B$ indicating the number of Monte Carlo permutations. If $B = 0$, use exact enumeration over all $\\binom{n}{n_E}$ relabelings; if $B > 0$, use $B$ random permutations to approximate the $p$-value under the exchangeability assumption. Use a fixed random seed for Monte Carlo to ensure reproducibility.\n- For each case, compute the two-sided permutation $p$-value for $T = \\bar{X}_E - \\bar{X}_U$, defined as the proportion of permutations (exact or MC) for which $|T^{\\pi}| \\ge |T_{\\text{obs}}|$, where $T^{\\pi}$ is the test statistic computed under a given label permutation $\\pi$, and $T_{\\text{obs}}$ is the observed test statistic computed from the original labeling.\n- Express each $p$-value as a decimal rounded to $6$ decimal places.\n\nFundamental base to use:\n- The definition of exchangeability under the null of no effect in epidemiology: the joint distribution of outcomes is invariant under permutations of exposure labels.\n- The definition of a $p$-value as the probability, under the null hypothesis, of observing a test statistic at least as extreme as the realized one.\n\nTest suite:\n- Case $1$ (balanced, exact computation): exposed $[2.1, 1.9, 2.3, 2.0, 2.2]$, unexposed $[1.8, 1.7, 1.6, 1.9, 1.5]$, $B = 0$.\n- Case $2$ (boundary, exact computation with identical outcomes): exposed $[0.0, 0.0]$, unexposed $[0.0, 0.0]$, $B = 0$.\n- Case $3$ (unbalanced, Monte Carlo approximation): exposed $[1.04, 0.92, 1.10, 0.95, 1.00, 1.08, 0.97, 1.02]$, unexposed $[0.88, 0.79, 0.85, 0.80, 0.90, 0.84, 0.76, 0.83, 0.88, 0.82, 0.91, 0.86]$, $B = 50000$, fixed seed $42$.\n- Case $4$ (extreme difference, exact computation): exposed $[5.0, 5.1, 5.2, 4.9]$, unexposed $[1.0, 1.1, 0.9, 1.2]$, $B = 0$.\n\nFinal output format:\nYour program should produce a single line of output containing the permutation $p$-values for the test suite, rounded to $6$ decimal places, as a comma-separated list enclosed in square brackets. For example, the output should look like $[p_1,p_2,p_3,p_4]$, where each $p_i$ corresponds to the $i$-th case in the order given above.",
            "solution": "The problem requires the computation of permutation test $p$-values for a two-group comparison, a non-parametric method of fundamental importance in epidemiology and biostatistics when distributional assumptions like normality are not met. The cornerstone of this method is the principle of exchangeability under the null hypothesis.\n\n### Step 1: Problem Validation\n\n**1.1. Extraction of Givens**\n-   **Context**: Two-group comparison (exposed vs. unexposed) with a continuous outcome in epidemiology.\n-   **Null Hypothesis ($H_0$) Assumption**: Exchangeability, which posits that the joint distribution of outcomes is invariant to permutations of exposure labels.\n-   **Test Statistic**: The difference in sample means, $T = \\bar{X}_E - \\bar{X}_U$, where $\\bar{X}_E$ and $\\bar{X}_U$ are the sample means of the exposed and unexposed groups, respectively.\n-   **Measure of Extremeness**: For a two-sided test, the absolute value of the test statistic, $|T|$.\n-   **Null Distribution Generation**:\n    -   If an integer $B=0$, use exact enumeration of all $\\binom{n}{n_E}$ possible relabelings, where $n=n_E+n_U$.\n    -   If $B>0$, use a Monte Carlo (MC) approximation with $B$ random permutations. A fixed random seed must be used for reproducibility in MC cases.\n-   **$p$-value Definition**: The two-sided permutation $p$-value is the proportion of permutations, $\\pi$, for which the permuted test statistic, $T^{\\pi}$, is at least as extreme as the observed test statistic, $T_{\\text{obs}}$. This is expressed as the proportion of permutations where $|T^{\\pi}| \\ge |T_{\\text{obs}}|$.\n-   **Output Requirement**: $p$-values must be rounded to $6$ decimal places.\n-   **Test Suite**:\n    -   Case $1$: exposed $[2.1, 1.9, 2.3, 2.0, 2.2]$, unexposed $[1.8, 1.7, 1.6, 1.9, 1.5]$, $B = 0$.\n    -   Case $2$: exposed $[0.0, 0.0]$, unexposed $[0.0, 0.0]$, $B = 0$.\n    -   Case $3$: exposed $[1.04, 0.92, 1.10, 0.95, 1.00, 1.08, 0.97, 1.02]$, unexposed $[0.88, 0.79, 0.85, 0.80, 0.90, 0.84, 0.76, 0.83, 0.88, 0.82, 0.91, 0.86]$, $B = 50000$, seed $42$.\n    -   Case $4$: exposed $[5.0, 5.1, 5.2, 4.9]$, unexposed $[1.0, 1.1, 0.9, 1.2]$, $B = 0$.\n\n**1.2. Validation against Criteria**\nThe problem statement is evaluated based on the provided criteria.\n-   **Scientifically Grounded**: The problem is well-grounded in established principles of non-parametric statistics. Permutation testing under the assumption of exchangeability is a standard, robust method for hypothesis testing. The choice of the difference in means as the test statistic is common and valid.\n-   **Well-Posed**: The problem is well-posed. It provides all necessary data (outcomes for two groups, number of permutations $B$), a clear definition of the test statistic, a precise rule for generating the null distribution (exact vs. MC), and an unambiguous definition of the two-sided $p$-value. A unique, stable, and meaningful solution exists for each test case.\n-   **Objective**: The problem is stated in precise, objective language, free from subjectivity or ambiguity.\n\nThe problem does not violate any of the specified invalidity conditions. It is scientifically sound, formalizable, complete, and computationally feasible. The test cases include a standard comparison, a boundary condition (identical groups), a larger sample size requiring approximation, and a case with a very clear separation between groups, providing a comprehensive test of the required implementation.\n\n**1.3. Verdict**\nThe problem is **valid**. A solution will be provided.\n\n### Step 2: Solution Formulation\n\nThe solution proceeds by formalizing the statistical procedure and then designing an algorithm to implement it.\n\n**2.1. Theoretical Framework**\nLet the observed outcomes for the exposed group be the set $Y_E = \\{y_{E,1}, \\dots, y_{E,n_E}\\}$ and for the unexposed group be $Y_U = \\{y_{U,1}, \\dots, y_{U,n_U}\\}$. The respective sample sizes are $n_E$ and $n_U$. The total number of observations is $n = n_E + n_U$.\n\nThe null hypothesis, $H_0$, is that the exposure has no effect on the outcome. Under $H_0$, the assumption of exchangeability implies that any of the $n$ subjects was equally likely to have been in the exposed or unexposed group. Therefore, the partition of the pooled outcomes $Y = Y_E \\cup Y_U$ into groups of size $n_E$ and $n_U$ is arbitrary.\n\nThe observed test statistic is the difference between the sample means:\n$$T_{\\text{obs}} = \\bar{y}_E - \\bar{y}_U = \\frac{1}{n_E}\\sum_{i=1}^{n_E} y_{E,i} - \\frac{1}{n_U}\\sum_{j=1}^{n_U} y_{U,j}$$\nTo assess the significance of $T_{\\text{obs}}$, we compare it to a reference distribution of test statistics generated under the null hypothesis. This null distribution is created by considering all possible ways to relabel the $n$ observations in the pooled set $Y$.\n\nA permutation $\\pi$ corresponds to a re-partitioning of $Y$ into a new \"exposed\" group $Y_E^{\\pi}$ of size $n_E$ and a new \"unexposed\" group $Y_U^{\\pi}$ of size $n_U$. For each such permutation, we compute a test statistic:\n$$T^{\\pi} = \\bar{y}_E^{\\pi} - \\bar{y}_U^{\\pi}$$\nThe collection of all $T^{\\pi}$ values forms the exact null distribution. The total number of unique permutations (i.e., unique partitions) is $N_{total} = \\binom{n}{n_E}$.\n\nThe two-sided $p$-value is the probability of observing a test statistic at least as extreme as what was actually observed. Using the absolute value as the measure of extremeness, the $p$-value is:\n$$p = \\frac{\\text{count}(\\{ \\pi : |T^{\\pi}| \\ge |T_{\\text{obs}}|\\})}{N_{total}}$$\n\n**2.2. Computational Strategy**\nThe implementation requires two distinct procedures based on the value of $B$.\n\n**Case 1: Exact Enumeration ($B = 0$)**\nThis approach is used when $N_{total} = \\binom{n}{n_E}$ is computationally manageable.\n1.  Combine all observations into a single pooled array, $Y$.\n2.  Calculate and store the absolute value of the observed test statistic, $|T_{\\text{obs}}|$.\n3.  Generate all possible combinations of $n_E$ indices from the set $\\{0, 1, \\dots, n-1\\}$. Each combination represents the indices of observations forming a permuted exposed group.\n4.  Initialize a counter, $N_{\\text{extreme}}$, to $0$.\n5.  Iterate through each combination of indices:\n    a. Form the permuted exposed group, $Y_E^{\\pi}$, and the permuted unexposed group, $Y_U^{\\pi}$.\n    b. Calculate the permuted test statistic, $T^{\\pi}$.\n    c. If $|T^{\\pi}| \\ge |T_{\\text{obs}}|$, increment $N_{\\text{extreme}}$.\n6.  The exact $p$-value is $p = N_{\\text{extreme}} / N_{total}$, where $N_{total} = \\binom{n}{n_E}$.\n\n**Case 2: Monte Carlo Approximation ($B > 0$)**\nThis approach is used when $N_{total}$ is too large for exact enumeration.\n1.  Combine all observations into a single pooled array, $Y$.\n2.  Calculate and store $|T_{\\text{obs}}|$.\n3.  Set the random number generator seed to ensure reproducibility.\n4.  Initialize a counter, $N_{\\text{extreme}}$, to $0$.\n5.  Repeat $B$ times:\n    a. Randomly shuffle the pooled array $Y$.\n    b. The first $n_E$ elements form the permuted exposed group, $Y_E^{\\pi}$, and the remaining $n_U$ elements form the unexposed group, $Y_U^{\\pi}$.\n    c. Calculate the permuted test statistic, $T^{\\pi}$.\n    d. If $|T^{\\pi}| \\ge |T_{\\text{obs}}|$, increment $N_{\\text{extreme}}$.\n6.  The approximate $p$-value is $p \\approx N_{\\text{extreme}} / B$.\n\nThis computational framework directly implements the theoretical principles and adheres to the specifications of the problem statement. The final result for each case is rounded to $6$ decimal places.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import comb\nimport itertools\n\ndef calculate_p_value(exposed_obs, unexposed_obs, B, seed=None):\n    \"\"\"\n    Computes the two-sided permutation p-value for the difference in means.\n\n    Args:\n        exposed_obs (list): Observed outcomes for the exposed group.\n        unexposed_obs (list): Observed outcomes for the unexposed group.\n        B (int): Number of Monte Carlo permutations. If 0, performs exact enumeration.\n        seed (int, optional): Random seed for Monte Carlo simulation. Defaults to None.\n\n    Returns:\n        float: The computed p-value, rounded to 6 decimal places.\n    \"\"\"\n    exposed_arr = np.array(exposed_obs)\n    unexposed_arr = np.array(unexposed_obs)\n\n    n_e = len(exposed_arr)\n    n_u = len(unexposed_arr)\n    n = n_e + n_u\n\n    all_data = np.concatenate((exposed_arr, unexposed_arr))\n\n    # Calculate the observed test statistic\n    t_obs = np.mean(exposed_arr) - np.mean(unexposed_arr)\n    abs_t_obs = np.abs(t_obs)\n\n    extreme_count = 0\n\n    if B == 0:  # Exact enumeration\n        total_permutations = comb(n, n_e, exact=True)\n        indices = range(n)\n        \n        # Iterate over all combinations of indices for the exposed group\n        for exposed_indices in itertools.combinations(indices, n_e):\n            exposed_indices = np.array(exposed_indices)\n            unexposed_indices = np.array(list(set(indices) - set(exposed_indices)))\n            \n            perm_exposed = all_data[exposed_indices]\n            perm_unexposed = all_data[unexposed_indices]\n            \n            t_perm = np.mean(perm_exposed) - np.mean(perm_unexposed)\n            \n            if np.abs(t_perm) >= abs_t_obs:\n                extreme_count += 1\n        \n        p_value = extreme_count / total_permutations\n\n    else:  # Monte Carlo approximation\n        if seed is not None:\n            rng = np.random.default_rng(seed)\n        else:\n            rng = np.random.default_rng()\n\n        for _ in range(B):\n            permuted_data = rng.permutation(all_data)\n            \n            perm_exposed = permuted_data[:n_e]\n            perm_unexposed = permuted_data[n_e:]\n            \n            t_perm = np.mean(perm_exposed) - np.mean(perm_unexposed)\n            \n            if np.abs(t_perm) >= abs_t_obs:\n                extreme_count += 1\n        \n        p_value = extreme_count / B\n    \n    return round(p_value, 6)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (balanced, exact computation)\n        {'exposed': [2.1, 1.9, 2.3, 2.0, 2.2], 'unexposed': [1.8, 1.7, 1.6, 1.9, 1.5], 'B': 0, 'seed': None},\n        # Case 2 (boundary, exact computation with identical outcomes)\n        {'exposed': [0.0, 0.0], 'unexposed': [0.0, 0.0], 'B': 0, 'seed': None},\n        # Case 3 (unbalanced, Monte Carlo approximation)\n        {'exposed': [1.04, 0.92, 1.10, 0.95, 1.00, 1.08, 0.97, 1.02], 'unexposed': [0.88, 0.79, 0.85, 0.80, 0.90, 0.84, 0.76, 0.83, 0.88, 0.82, 0.91, 0.86], 'B': 50000, 'seed': 42},\n        # Case 4 (extreme difference, exact computation)\n        {'exposed': [5.0, 5.1, 5.2, 4.9], 'unexposed': [1.0, 1.1, 0.9, 1.2], 'B': 0, 'seed': None}\n    ]\n\n    results = []\n    for case in test_cases:\n        p_val = calculate_p_value(case['exposed'], case['unexposed'], case['B'], case['seed'])\n        results.append(p_val)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        }
    ]
}