{
    "hands_on_practices": [
        {
            "introduction": "Before any advanced processing can be performed, we must ensure the integrity of our data at the moment of acquisition. This exercise tackles the fundamental issue of aliasing, a form of distortion that occurs when a signal is sampled too slowly. You will apply the Nyquist-Shannon sampling theorem not only to the time domain but also to the spatial domain in the context of high-density EMG, learning to diagnose both temporal and spatial aliasing. ",
            "id": "4170135",
            "problem": "An investigator is acquiring Electromyography (EMG) time series from a single bipolar channel while simultaneously recording a one-dimensional High-Density (HD) surface EMG array aligned with the muscle fiber direction. The time series is uniformly sampled at rate $f_s = 2000\\,\\mathrm{Hz}$, and the HD array has uniform inter-electrode spacing $d = 10\\,\\mathrm{mm} = 0.01\\,\\mathrm{m}$. Motor unit action potentials are modeled as traveling waveforms along fibers with conduction velocity $v = 5\\,\\mathrm{m/s}$. Due to tissue and electrode filtering, assume the temporal spectrum of the surface EMG is effectively bandlimited to $f_{\\max} = 500\\,\\mathrm{Hz}$ in the sense that spectral magnitudes above $500\\,\\mathrm{Hz}$ are negligible for interpretation. No analog low-pass filter is applied before time sampling, and no spatial smoothing is applied before spatial sampling.\n\nStarting only from:\n- the definition of uniform sampling in time and space,\n- the relation between temporal frequency, spatial wavelength, and propagation speed ($v = f\\,\\lambda$ for a non-dispersive traveling waveform),\n- and the Shannon sampling paradigm that a uniformly sampled bandlimited signal can be reconstructed uniquely if the sampling supports at least two samples per period of the highest-frequency content,\n\nselect the single option that makes a correct and complete statement about:\n(i) what the Nyquist frequency is and what aliasing means, in conceptual terms without relying on pre-given formulas, and\n(ii) whether temporal aliasing and spatial aliasing occur in this specific setup, with correct physical reasoning tied to the given parameters.\n\nA. The Nyquist frequency is the largest frequency that can be represented without ambiguity under ideal uniform sampling when the signal is bandlimited, and aliasing is the distortion that results when components above that limit fold into lower frequencies and become indistinguishable from genuine content. In this setup, temporal aliasing is absent because the sampling rate supports at least two samples per period of the $f_{\\max} = 500\\,\\mathrm{Hz}$ component, while spatial aliasing is present because the smallest spatial wavelength implied by $v = 5\\,\\mathrm{m/s}$ at $f_{\\max} = 500\\,\\mathrm{Hz}$ is $\\lambda_{\\min} = v/f_{\\max} = 0.01\\,\\mathrm{m}$, which is smaller than twice the electrode spacing ($2d = 0.02\\,\\mathrm{m}$), violating the two-samples-per-wavelength requirement.\n\nB. The Nyquist frequency equals the sampling rate, and aliasing is caused primarily by quantization error. In this setup, both temporal aliasing and spatial aliasing are absent because $f_s = 2000\\,\\mathrm{Hz}$ is much larger than $f_{\\max} = 500\\,\\mathrm{Hz}$, and $d = 0.01\\,\\mathrm{m}$ is sufficiently fine to resolve the spatial details.\n\nC. The Nyquist frequency is the smallest resolvable frequency, and aliasing is any noise that appears in the spectrum. In this setup, temporal aliasing is present because $f_{\\max} = 500\\,\\mathrm{Hz}$ is close to $f_s = 2000\\,\\mathrm{Hz}$, but spatial aliasing is absent because the conduction velocity $v = 5\\,\\mathrm{m/s}$ ensures spatial smoothness across electrodes.\n\nD. The Nyquist frequency is the largest frequency that can be represented without ambiguity under ideal uniform sampling, and aliasing is the folding of higher frequencies into lower ones. In this setup, temporal aliasing is absent, and spatial aliasing is absent because the smallest spatial wavelength $\\lambda_{\\min}$ equals $2d$, so the two-samples-per-wavelength criterion is exactly met.\n\nE. The Nyquist frequency is the largest frequency representable without ambiguity, and aliasing is spectral folding. In this setup, temporal aliasing can be removed by reducing $f_s$ to $1000\\,\\mathrm{Hz}$ while keeping $d = 0.01\\,\\mathrm{m}$, which also removes spatial aliasing because temporal sampling changes the spatial sampling adequacy for a traveling waveform.",
            "solution": "### Problem Validation\n\nFirst, I will validate the problem statement according to the specified criteria.\n\n**Step 1: Extract Givens**\n- Temporal sampling rate: $f_s = 2000\\,\\mathrm{Hz}$\n- Inter-electrode spacing (spatial sampling interval): $d = 10\\,\\mathrm{mm} = 0.01\\,\\mathrm{m}$\n- Conduction velocity of traveling waveforms: $v = 5\\,\\mathrm{m/s}$\n- Effective maximum temporal frequency of the signal: $f_{\\max} = 500\\,\\mathrm{Hz}$\n- No analog low-pass filter (anti-aliasing filter) is applied before time sampling.\n- No spatial smoothing is applied before spatial sampling.\n- Governing principles explicitly stated:\n    1. Definition of uniform sampling in time and space.\n    2. Relation for a non-dispersive traveling waveform: $v = f\\,\\lambda$, where $f$ is temporal frequency and $\\lambda$ is spatial wavelength.\n    3. Shannon sampling paradigm: A uniformly sampled bandlimited signal can be reconstructed uniquely if the sampling supports at least two samples per period of the highest-frequency content.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding (Critical):** The problem is firmly grounded in the fundamental principles of digital signal processing (the Nyquist-Shannon sampling theorem) and its application to a standard problem in biomechanics (analysis of EMG signals). The representation of motor unit action potentials as traveling waves, the relationship $v = f\\,\\lambda$, and the parameter values ($f_s$, $d$, $v$, $f_{\\max}$) are all standard and scientifically realistic in the context of electromyography research. The problem is free of pseudoscience or factual errors.\n- **Well-Posed:** The problem provides a complete set of parameters and explicitly states the principles to be used. The question is precise and asks for a specific analysis (conceptual definitions and assessment of temporal/spatial aliasing), which leads to a unique, derivable solution.\n- **Objective (Critical):** The problem is stated in objective, technical language. There are no subjective or ambiguous terms.\n- **Other Flaws:** The problem is not non-formalizable, incomplete, contradictory, unrealistic, ill-posed, or trivial. The statement that no analog low-pass filter is used is an important detail, but it is rendered non-problematic by the explicit premise that the signal is *effectively bandlimited* to $f_{\\max} = 500\\,\\mathrm{Hz}$. This premise must be accepted for the analysis.\n\n**Step 3: Verdict and Action**\nThe problem statement is **valid**. I will proceed with the solution.\n\n### Derivation and Solution\n\nThe problem requires two parts: (i) a conceptual explanation of the Nyquist frequency and aliasing, and (ii) a determination of whether temporal and spatial aliasing occur in the given setup.\n\n**(i) Conceptual Definitions**\n\nThe problem provides the core of the sampling paradigm: to perfectly reconstruct a signal with maximum frequency component $f_{\\max}$, one must sample it at a rate $f_s$ that provides \"at least two samples per period of the highest-frequency content.\"\n\n- **The Nyquist Frequency:** The period of the highest-frequency component is $T_{\\min} = 1/f_{\\max}$. The time between samples is the sampling interval, $\\Delta t = 1/f_s$. The condition \"at least two samples per period\" means that the period must be at least twice the sampling interval, i.e., $T_{\\min} \\ge 2\\Delta t$. Substituting the definitions, we get $1/f_{\\max} \\ge 2/f_s$, which rearranges to $f_s \\ge 2f_{\\max}$. The Nyquist-Shannon theorem states that if a signal is strictly bandlimited to $f_{\\max}$ (i.e., it contains no frequencies above $f_{\\max}$), it can be perfectly reconstructed if sampled at a rate $f_s > 2f_{\\max}$. The critical frequency, $f_s/2$, is known as the **Nyquist frequency**. It represents the highest frequency in a signal that can be unambiguously captured by a given sampling rate $f_s$.\n\n- **Aliasing:** If the sampling condition is violated, meaning the signal contains frequencies $f$ greater than the Nyquist frequency ($f > f_s/2$), these higher frequencies are not correctly recorded. During the sampling process, they \"impersonate\" or \"fold back\" into the frequency range $[0, f_s/2]$. A frequency component at $f$ will appear in the sampled signal's spectrum at an alias frequency $f' = \\vert f - k \\cdot f_s \\vert$, where $k$ is an integer chosen such that $f'$ falls within the $[0, f_s/2]$ range. This phenomenon is called **aliasing**, and it is an irreversible distortion because the aliased components become indistinguishable from the true signal components in that lower frequency range.\n\n**(ii) Analysis of Aliasing in the Specific Setup**\n\n**Temporal Aliasing:**\n1.  The temporal sampling rate is given as $f_s = 2000\\,\\mathrm{Hz}$.\n2.  The temporal Nyquist frequency is $f_{\\text{N,temp}} = f_s/2 = 2000\\,\\mathrm{Hz} / 2 = 1000\\,\\mathrm{Hz}$.\n3.  The problem states that the signal is effectively bandlimited to a maximum frequency $f_{\\max} = 500\\,\\mathrm{Hz}$.\n4.  To avoid temporal aliasing, the maximum signal frequency must be less than or equal to the Nyquist frequency: $f_{\\max} \\le f_{\\text{N,temp}}$.\n5.  In this case, $500\\,\\mathrm{Hz} \\le 1000\\,\\mathrm{Hz}$. This condition is satisfied.\n_Alternatively, using the problem's phrasing:_ The period of the highest frequency is $T_{\\min} = 1/f_{\\max} = 1/500\\,\\mathrm{s} = 0.002\\,\\mathrm{s}$. The sampling interval is $\\Delta t = 1/f_s = 1/2000\\,\\mathrm{s} = 0.0005\\,\\mathrm{s}$. The number of samples per period is $T_{\\min}/\\Delta t = 0.002/0.0005 = 4$. Since $4 \\ge 2$, the sampling criterion is met.\nTherefore, **temporal aliasing does not occur**.\n\n**Spatial Aliasing:**\nThe same sampling principles apply to the spatial domain. The \"signal\" is the potential distribution along the muscle, the \"sampling\" is done by the discrete electrodes, and the \"sampling interval\" is the inter-electrode distance $d$.\n1.  The spatial sampling interval is $d = 10\\,\\mathrm{mm} = 0.01\\,\\mathrm{m}$.\n2.  The spatial Nyquist frequency, in units of spatial frequency (cycles per meter, or $\\mathrm{m}^{-1}$), is $k_{\\text{N,spatial}} = 1/(2d) = 1/(2 \\times 0.01\\,\\mathrm{m}) = 50\\,\\mathrm{m}^{-1}$.\n3.  We must find the maximum spatial frequency, $k_{\\max}$, present in the signal. The relationship $v = f\\,\\lambda$ connects temporal frequency $f$ and spatial wavelength $\\lambda$. Since spatial frequency is $k = 1/\\lambda$, we can write $v = f/k$, or $k = f/v$.\n4.  The maximum spatial frequency $k_{\\max}$ corresponds to the maximum temporal frequency $f_{\\max}$:\n    $$k_{\\max} = \\frac{f_{\\max}}{v} = \\frac{500\\,\\mathrm{Hz}}{5\\,\\mathrm{m/s}} = 100\\,\\mathrm{m}^{-1}$$\n5.  To avoid spatial aliasing, the maximum spatial frequency must be less than or equal to the spatial Nyquist frequency: $k_{\\max} \\le k_{\\text{N,spatial}}$.\n6.  Here, $100\\,\\mathrm{m}^{-1} \\le 50\\,\\mathrm{m}^{-1}$. This condition is **violated**.\n_Alternatively, using the problem's phrasing:_ The \"period\" in space is the wavelength, $\\lambda$. The highest spatial frequency corresponds to the shortest spatial wavelength, $\\lambda_{\\min}$.\n    $$\\lambda_{\\min} = \\frac{v}{f_{\\max}} = \\frac{5\\,\\mathrm{m/s}}{500\\,\\mathrm{Hz}} = 0.01\\,\\mathrm{m}$$\nThe sampling criterion requires \"at least two samples per period,\" which means the shortest wavelength must be at least twice the spatial sampling interval: $\\lambda_{\\min} \\ge 2d$.\n    $$0.01\\,\\mathrm{m} \\ge 2 \\times 0.01\\,\\mathrm{m}$$\n    $$0.01\\,\\mathrm{m} \\ge 0.02\\,\\mathrm{m}$$\nThis inequality is false. The condition is violated.\nTherefore, **spatial aliasing occurs**.\n\n### Evaluation of Options\n\n**A. The Nyquist frequency is the largest frequency that can be represented without ambiguity under ideal uniform sampling when the signal is bandlimited, and aliasing is the distortion that results when components above that limit fold into lower frequencies and become indistinguishable from genuine content. In this setup, temporal aliasing is absent because the sampling rate supports at least two samples per period of the $f_{\\max} = 500\\,\\mathrm{Hz}$ component, while spatial aliasing is present because the smallest spatial wavelength implied by $v = 5\\,\\mathrm{m/s}$ at $f_{\\max} = 500\\,\\mathrm{Hz}$ is $\\lambda_{\\min} = v/f_{\\max} = 0.01\\,\\mathrm{m}$, which is smaller than twice the electrode spacing ($2d = 0.02\\,\\mathrm{m}$), violating the two-samples-per-wavelength requirement.**\n- The conceptual definitions of Nyquist frequency and aliasing are correct and well-stated.\n- The conclusion that temporal aliasing is absent is correct, and the reasoning is sound.\n- The conclusion that spatial aliasing is present is correct. The reasoning compares $\\lambda_{\\min} = 0.01\\,\\mathrm{m}$ to $2d = 0.02\\,\\mathrm{m}$ and correctly notes that $\\lambda_{\\min} < 2d$ constitutes a violation of the sampling criterion. The statement is entirely consistent with the derivation.\n- **Verdict: Correct.**\n\n**B. The Nyquist frequency equals the sampling rate, and aliasing is caused primarily by quantization error. In this setup, both temporal aliasing and spatial aliasing are absent because $f_s = 2000\\,\\mathrm{Hz}$ is much larger than $f_{\\max} = 500\\,\\mathrm{Hz}$, and $d = 0.01\\,\\mathrm{m}$ is sufficiently fine to resolve the spatial details.**\n- The definition that the Nyquist frequency equals the sampling rate is incorrect ($f_{\\text{N}} = f_s/2$). The statement that aliasing is caused by quantization error is incorrect.\n- The conclusion that spatial aliasing is absent is incorrect.\n- **Verdict: Incorrect.**\n\n**C. The Nyquist frequency is the smallest resolvable frequency, and aliasing is any noise that appears in the spectrum. In this setup, temporal aliasing is present because $f_{\\max} = 500\\,\\mathrm{Hz}$ is close to $f_s = 2000\\,\\mathrm{Hz}$, but spatial aliasing is absent because the conduction velocity $v = 5\\,\\mathrm{m/s}$ ensures spatial smoothness across electrodes.**\n- The definition of the Nyquist frequency as the smallest resolvable frequency is incorrect. The definition of aliasing as \"any noise\" is imprecise and incorrect.\n- The conclusion that temporal aliasing is present is incorrect. $f_{\\max}$ is not \"close\" to $f_s$ relative to the Nyquist criterion; $f_s$ is $4$ times $f_{\\max}$.\n- The conclusion that spatial aliasing is absent is incorrect.\n- **Verdict: Incorrect.**\n\n**D. The Nyquist frequency is the largest frequency that can be represented without ambiguity under ideal uniform sampling, and aliasing is the folding of higher frequencies into lower ones. In this setup, temporal aliasing is absent, and spatial aliasing is absent because the smallest spatial wavelength $\\lambda_{\\min}$ equals $2d$, so the two-samples-per-wavelength criterion is exactly met.**\n- The conceptual definitions are correct, though brief.\n- The conclusion that spatial aliasing is absent is incorrect.\n- The reasoning is based on the false calculation that $\\lambda_{\\min} = 2d$. My derivation shows $\\lambda_{\\min} = 0.01\\,\\mathrm{m}$ and $2d = 0.02\\,\\mathrm{m}$, so $\\lambda_{\\min} = d$, not $2d$. Thus, the criterion is not met.\n- **Verdict: Incorrect.**\n\n**E. The Nyquist frequency is the largest frequency representable without ambiguity, and aliasing is spectral folding. In this setup, temporal aliasing can be removed by reducing $f_s$ to $1000\\,\\mathrm{Hz}$ while keeping $d = 0.01\\,\\mathrm{m}$, which also removes spatial aliasing because temporal sampling changes the spatial sampling adequacy for a traveling waveform.**\n- The conceptual definitions are correct but brief.\n- The statement that temporal aliasing can be \"removed\" is nonsensical, as it is not present in the first place. Reducing $f_s$ to $1000\\,\\mathrm{Hz}$ would bring the system to the Nyquist limit ($f_s = 2f_{\\max}$), increasing the risk of aliasing, not removing it.\n- The claim that changing the temporal sampling rate $f_s$ affects spatial aliasing is fundamentally incorrect. Spatial aliasing depends on $d$ and the signal's spatial bandwidth ($k_{\\max}$), which are independent of $f_s$.\n- **Verdict: Incorrect.**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "A common task in EMG analysis is to compute an amplitude envelope, typically by rectification and low-pass filtering. However, a naive choice of filter can introduce significant time delays, or phase distortion, corrupting the timing of crucial events like muscle onset. This practice explores the critical difference between IIR and linear-phase FIR filters and demonstrates how noncausal, zero-phase filtering is used to create an accurate envelope without introducing timing artifacts. ",
            "id": "4170148",
            "problem": "A biomechanics laboratory studies Electromyography (EMG) signals and computes EMG envelopes by full-wave rectification followed by low-pass filtering to suppress carrier fluctuations and retain motor-unit recruitment dynamics. Consider discrete-time, Linear Time-Invariant (LTI) filtering of the rectified EMG sampled at rate $f_s$ to form the envelope, and onset detection by threshold crossing on the filtered envelope. Two families of filters are used in practice: Infinite Impulse Response (IIR) low-pass designs (for example, Butterworth) and Finite Impulse Response (FIR) low-pass designs with symmetric coefficients. The laboratory is considering four implementations for the envelope stage: (i) single-pass causal IIR, (ii) single-pass causal linear-phase FIR, (iii) forward–backward application of the IIR, and (iv) forward–backward application of the FIR. In forward–backward filtering, the signal is filtered forward in time and then filtered backward in time by the same filter, producing a noncausal operation often termed zero-phase filtering.\n\nUsing only foundational definitions for LTI systems—namely, the discrete-time frequency response $H(e^{j\\omega})$, its phase $\\phi(\\omega) = \\arg H(e^{j\\omega})$, the group delay $\\tau_g(\\omega) = -\\frac{d\\phi(\\omega)}{d\\omega}$, and the impulse response symmetry condition for linear-phase FIR filters—reason about phase response and onset timing bias. Select all statements that are correct.\n\nA. A causal IIR low-pass filter with real coefficients generally has nonlinear phase, so $\\tau_g(\\omega)$ depends on $\\omega$; forward–backward application cancels phase because the net frequency response is $H(e^{j\\omega})H(e^{-j\\omega})$, which has zero net phase and magnitude equal to $\\lvert H(e^{j\\omega}) \\rvert^2$, thus removing onset timing bias due to phase delay.\n\nB. A symmetric FIR low-pass filter has zero group delay for all $\\omega$, hence single-pass causal application does not shift EMG onset times.\n\nC. Applying forward–backward filtering to a linear-phase FIR yields a noncausal, zero-phase filter whose magnitude response equals the original magnitude response, not its square.\n\nD. For any stable causal low-pass filter, the group delay is constant and equals half the filter order, so onset timing bias is independent of frequency content.\n\nE. In threshold-based onset detection using a single-pass causal linear-phase FIR of length $N$ samples at sampling rate $f_s$, the detected onset time is shifted by a constant $\\Delta t = \\frac{N-1}{2 f_s}$ relative to the same detection applied to the noncausal zero-phase version of the same filtering, independent of the waveform shape of the envelope, because linear phase implies a pure constant delay in addition to magnitude shaping.\n\nSelect all that apply.",
            "solution": "The problem statement has been validated and found to be scientifically sound, well-posed, and objective. It provides a correct and standard description of digital signal processing techniques used in biomechanics for EMG signal analysis. We may proceed to evaluate the given statements.\n\nThe analysis will be based on the fundamental principles of discrete-time Linear Time-Invariant (LTI) systems. Let an LTI filter be characterized by its impulse response $h[n]$ and its Discrete-Time Fourier Transform (DTFT), the frequency response $H(e^{j\\omega}) = \\sum_{n=-\\infty}^{\\infty} h[n]e^{-j\\omega n}$. The frequency response can be written in polar form as $H(e^{j\\omega}) = \\lvert H(e^{j\\omega}) \\rvert e^{j\\phi(\\omega)}$, where $\\lvert H(e^{j\\omega}) \\rvert$ is the magnitude response and $\\phi(\\omega) = \\arg H(e^{j\\omega})$ is the phase response. The group delay is defined as $\\tau_g(\\omega) = -\\frac{d\\phi(\\omega)}{d\\omega}$ and represents the delay experienced by a narrow band of frequencies centered at $\\omega$.\n\nFor any filter with a real-valued impulse response $h[n]$, the frequency response exhibits conjugate symmetry: $H(e^{j\\omega}) = H^*(e^{-j\\omega})$. This implies an even magnitude response, $\\lvert H(e^{j\\omega}) \\rvert = \\lvert H(e^{-j\\omega}) \\rvert$, and an odd phase response, $\\phi(\\omega) = -\\phi(-\\omega)$.\n\nForward-backward filtering consists of a forward pass followed by a time-reversed backward pass with the same filter $h[n]$. If the input is $x[n]$, the output of the forward pass is $y_f[n] = x[n]*h[n]$. In the frequency domain, $Y_f(e^{j\\omega}) = X(e^{j\\omega})H(e^{j\\omega})$. The backward pass filters the time-reversed signal $y_f[-n]$. The final output $y_{fb}[n]$ has a frequency-domain representation $Y_{fb}(e^{j\\omega}) = X(e^{j\\omega}) H_{fb}(e^{j\\omega})$, where the effective frequency response is $H_{fb}(e^{j\\omega}) = H(e^{j\\omega})H(e^{-j\\omega})$. For a filter with real $h[n]$, this becomes $H_{fb}(e^{j\\omega}) = H(e^{j\\omega})H^*(e^{j\\omega}) = \\lvert H(e^{j\\omega}) \\rvert^2$. This response is purely real and non-negative, meaning its phase is zero for all frequencies ($\\phi_{fb}(\\omega) = 0$).\n\nNow, we evaluate each statement.\n\n**A. A causal IIR low-pass filter with real coefficients generally has nonlinear phase, so $\\tau_g(\\omega)$ depends on $\\omega$; forward–backward application cancels phase because the net frequency response is $H(e^{j\\omega})H(e^{-j\\omega})$, which has zero net phase and magnitude equal to $\\lvert H(e^{j\\omega}) \\rvert^2$, thus removing onset timing bias due to phase delay.**\n\n- A stable, causal IIR filter with real coefficients cannot have exact linear phase. Its rational transfer function structure with poles not at the origin or infinity leads to a nonlinear phase response $\\phi(\\omega)$. Consequently, the group delay $\\tau_g(\\omega) = -\\frac{d\\phi(\\omega)}{d\\omega}$ is a function of frequency $\\omega$. This causes phase distortion or dispersion, where different frequency components of the signal are delayed by different amounts.\n- As derived above, forward-backward filtering results in an effective frequency response of $H_{fb}(e^{j\\omega}) = H(e^{j\\omega})H(e^{-j\\omega})$. For a real IIR filter, this simplifies to $H_{fb}(e^{j\\omega}) = \\lvert H(e^{j\\omega}) \\rvert^2$. This new transfer function is purely real, meaning its phase is zero for all $\\omega$.\n- A zero-phase filter has zero group delay for all frequencies. It does not introduce any time delay or phase distortion. Therefore, it completely eliminates the timing bias (phase-induced delay) that would have been introduced by the original IIR filter, including the frequency-dependent part. The statement accurately describes the properties of IIR filters and the outcome of the forward-backward filtering technique.\n- Verdict: **Correct**.\n\n**B. A symmetric FIR low-pass filter has zero group delay for all $\\omega$, hence single-pass causal application does not shift EMG onset times.**\n\n- This statement refers to a causal linear-phase FIR filter with symmetric coefficients (implementation (ii)). Let the filter have length $N$ and a symmetric impulse response $h[n] = h[N-1-n]$ for $n=0, 1, ..., N-1$. Such a filter has a generalized linear phase response given by $\\phi(\\omega) = -\\alpha \\omega$, where $\\alpha = \\frac{N-1}{2}$ is a constant.\n- The group delay is $\\tau_g(\\omega) = -\\frac{d}{d\\omega}(-\\alpha \\omega) = \\alpha = \\frac{N-1}{2}$ samples. This delay is constant for all frequencies, not zero (unless $N=1$, a trivial filter).\n- Because the group delay is a non-zero constant, the filter introduces a pure time shift of $\\frac{N-1}{2}$ samples to the signal. This shift will affect the detected onset time. The statement that the group delay is zero is false.\n- Verdict: **Incorrect**.\n\n**C. Applying forward–backward filtering to a linear-phase FIR yields a noncausal, zero-phase filter whose magnitude response equals the original magnitude response, not its square.**\n\n- The forward-backward filtering process is inherently noncausal because the backward pass requires access to future values of the forward-filtered signal. As shown in the general derivation, the resulting effective filter has zero phase.\n- The effective magnitude response is $\\lvert H_{fb}(e^{j\\omega}) \\rvert = \\lvert \\lvert H(e^{j\\omega}) \\rvert^2 \\rvert = \\lvert H(e^{j\\omega}) \\rvert^2$. This is the square of the original filter's magnitude response. The statement claims the magnitude response is equal to the original, not its square. This is incorrect. The resulting passband will be flatter and the stopband will have greater attenuation (if the original attenuation was, say, $-40$ dB, the new attenuation will be $-80$ dB), but the magnitude response function itself is squared.\n- Verdict: **Incorrect**.\n\n**D. For any stable causal low-pass filter, the group delay is constant and equals half the filter order, so onset timing bias is independent of frequency content.**\n\n- This statement makes a false generalization. The property of constant group delay is characteristic of linear-phase filters, which are typically implemented as FIR filters. As established in the analysis of option A, stable causal IIR filters generally do not have linear phase, and therefore have a group delay that varies with frequency.\n- The statement claims this property holds for \"any stable causal low-pass filter,\" which is false. For example, a Butterworth IIR filter is a stable causal low-pass filter, but its group delay is not constant. Therefore, the premise is invalid, and the conclusion that timing bias is independent of frequency content does not hold true for all such filters.\n- Verdict: **Incorrect**.\n\n**E. In threshold-based onset detection using a single-pass causal linear-phase FIR of length $N$ samples at sampling rate $f_s$, the detected onset time is shifted by a constant $\\Delta t = \\frac{N-1}{2 f_s}$ relative to the same detection applied to the noncausal zero-phase version of the same filtering, independent of the waveform shape of the envelope, because linear phase implies a pure constant delay in addition to magnitude shaping.**\n\n- A single-pass causal linear-phase FIR filter of length $N$ possesses a constant group delay of $\\tau_g = \\frac{N-1}{2}$ samples.\n- A constant group delay implies that all frequency components of the signal are delayed by the same amount of time. This results in a perfect time-shift of the entire waveform without any shape distortion (phase dispersion). The magnitude response still shapes the signal's spectrum, but this is decoupled from the time shift.\n- The total time delay in seconds is the group delay in samples divided by the sampling rate $f_s$, which is $\\Delta t = \\frac{\\tau_g}{f_s} = \\frac{(N-1)/2}{f_s} = \\frac{N-1}{2f_s}$.\n- A \"noncausal zero-phase version\" of the filtering implies an operation with zero group delay. The relative shift between the causal linear-phase output and the zero-phase output is therefore exactly this constant delay $\\Delta t$.\n- Because the delay is uniform for all frequencies, the shape of the EMG envelope is preserved, just shifted in time. Thus, a threshold crossing on the filtered signal will occur at a time that is delayed by $\\Delta t$ compared to the zero-phase filtered signal, regardless of the envelope's specific shape.\n- Verdict: **Correct**.",
            "answer": "$$\\boxed{AE}$$"
        },
        {
            "introduction": "Moving beyond a simple activity envelope, a primary goal of advanced EMG analysis is to decompose the composite signal into the firing trains of individual motor units. This practice introduces a powerful, model-based approach known as convolutional sparse coding, which reframes decomposition as a statistical estimation problem. By deriving the objective function from first principles, you will understand how combining a data-fidelity term with a sparsity-promoting $\\ell_{1}$ norm regularizer enables the separation of superimposed motor unit action potentials. ",
            "id": "4170100",
            "problem": "An Electromyography (EMG) time series $x[n]$ recorded from a single differential electrode over a muscle is modeled as the linear superposition of Motor Unit Action Potential (MUAP) template responses convolved with latent, unit-specific spike trains, corrupted by additive noise. Let $\\{h_i[n]\\}_{i=1}^{K}$ denote fixed MUAP templates (finite-length impulse responses) associated with $K$ distinct motor units, and let $\\{s_i[n]\\}_{i=1}^{K}$ denote their corresponding nonnegative latent spike trains. Assume the following generative model in discrete time:\n$$\nx[n] = \\sum_{i=1}^{K} (h_i * s_i)[n] + w[n],\n$$\nwhere $*$ denotes linear convolution and $w[n]$ is additive noise. Assume $w[n]$ is a realization of a zero-mean, independent and identically distributed Gaussian process with variance $\\sigma^{2}$, that is, $w[n] \\sim \\mathcal{N}(0,\\sigma^{2})$ independently across $n$. Further assume that the spike trains $\\{s_i[n]\\}$ are independent across $i$ and have a Laplace (double-exponential) prior promoting sparsity, that is,\n$$\np(s_i) \\propto \\exp\\!\\big(-\\alpha \\lVert s_i \\rVert_{1}\\big),\n$$\nwith $\\alpha>0$ a fixed hyperparameter and $\\lVert s_i \\rVert_{1} \\triangleq \\sum_{n} |s_i[n]|$.\n\nStarting only from these modeling assumptions and the core definitions of linear convolution, Gaussian likelihood, Laplace prior, and Maximum A Posteriori (MAP) estimation, perform the following:\n\n- Using the independence assumptions, write the posterior $p(\\{s_i\\}\\,|\\,x)$ up to a proportionality constant, then take the negative logarithm to obtain the MAP objective for $\\{s_i\\}$ in terms of $x$, the templates $\\{h_i\\}$, and the spike trains $\\{s_i\\}$. Eliminate additive and multiplicative constants that do not depend on $\\{s_i\\}$, and express the objective as a sum of a data fidelity term and a sparsity-inducing regularizer. Introduce a single regularization parameter $\\lambda>0$ that collects the dependence on $\\sigma^{2}$ and $\\alpha$ into one scalar weight.\n- Explain, using first principles from sparse representation theory and properties of the $\\ell_{1}$ norm, how sparsity in $\\{s_i\\}$ promotes separation of temporally overlapping MUAPs when the templates $\\{h_i\\}$ are not identical and exhibit limited mutual coherence. Your explanation should reference the role of the triangle inequality for $\\ell_{1}$, the reduced support of sparse solutions, and the qualitative effect of mutual coherence on identifiability, without relying on unproven shortcut results.\n\nAs your final answer, provide the analytic expression for the resulting convolutional sparse coding MAP objective in its simplest form, written as a function of $\\{s_i\\}$, $x$, $\\{h_i\\}$, and $\\lambda$. No numerical computation is required, and no units are to be reported in the final answer.",
            "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n### Step 1: Extract Givens\n- EMG time series: $x[n]$\n- Generative model: $x[n] = \\sum_{i=1}^{K} (h_i * s_i)[n] + w[n]$\n- MUAP templates: $\\{h_i[n]\\}_{i=1}^{K}$, fixed, finite-length impulse responses\n- Latent spike trains: $\\{s_i[n]\\}_{i=1}^{K}$, nonnegative\n- Convolution operator: $*$\n- Additive noise: $w[n] \\sim \\mathcal{N}(0,\\sigma^{2})$, zero-mean, i.i.d. Gaussian with variance $\\sigma^2$\n- Independence of spike trains: $\\{s_i[n]\\}$ are independent across index $i$\n- Prior on spike trains: $p(s_i) \\propto \\exp(-\\alpha \\lVert s_i \\rVert_{1})$ with hyperparameter $\\alpha > 0$\n- Definition of $\\ell_1$ norm: $\\lVert s_i \\rVert_{1} \\triangleq \\sum_{n} |s_i[n]|$\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a standard one in the field of biomedical signal processing, specifically in the domain of EMG decomposition. The generative model described is a well-established convolutional sparse coding (CSC) framework. The assumptions are physically and mathematically sound:\n- **Scientifically Grounded**: The model of an EMG signal as a superposition of convolved spike trains is a cornerstone of EMG analysis. The use of a Gaussian model for noise and a Laplace prior (which leads to $\\ell_1$ regularization) for promoting sparsity in the underlying neural drive is a standard and powerful technique in modern statistical signal processing and machine learning.\n- **Well-Posed**: The task is to derive an objective function and provide a conceptual explanation. The premises provided (likelihood and prior distributions) are sufficient and consistent for deriving the Maximum A Posteriori (MAP) objective.\n- **Objective**: The problem is stated using precise mathematical and technical language, with no subjective or ambiguous terminology.\n\nAll validation criteria are met. The problem is deemed valid.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete, reasoned solution will be provided.\n\n### Derivation of the MAP Objective Function\n\nThe goal is to find the Maximum A Posteriori (MAP) estimate of the latent spike trains $\\{s_i\\}_{i=1}^{K}$ given the observed EMG signal $x$. The MAP estimate, denoted $\\{\\hat{s}_i\\}$, maximizes the posterior probability $p(\\{s_i\\} | x)$. Using Bayes' theorem, the posterior is proportional to the product of the likelihood and the prior:\n$$\np(\\{s_i\\}_{i=1}^{K} \\,|\\, x) \\propto p(x \\,|\\, \\{s_i\\}_{i=1}^{K}) \\, p(\\{s_i\\}_{i=1}^{K})\n$$\n\nFirst, we define the likelihood term $p(x \\,|\\, \\{s_i\\})$. The generative model is $x[n] = \\sum_{i=1}^{K} (h_i * s_i)[n] + w[n]$. This can be rewritten for the noise as $w[n] = x[n] - \\sum_{i=1}^{K} (h_i * s_i)[n]$. The problem states that $w[n]$ is a realization of an independent and identically distributed (i.i.d.) Gaussian process with zero mean and variance $\\sigma^2$, i.e., $w[n] \\sim \\mathcal{N}(0, \\sigma^2)$. The probability density function for a single noise sample is $p(w[n]) = (2\\pi\\sigma^2)^{-1/2} \\exp\\left(-\\frac{w[n]^2}{2\\sigma^2}\\right)$. Since the noise samples are independent across time $n$, the joint probability of the entire noise sequence $w = \\{w[n]\\}$ is the product of the individual probabilities. This gives the likelihood of observing data $x$ given the spike trains $\\{s_i\\}$:\n$$\np(x \\,|\\, \\{s_i\\}) = \\prod_{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{\\left(x[n] - \\sum_{i=1}^{K} (h_i * s_i)[n]\\right)^2}{2\\sigma^2} \\right)\n$$\nThis expression can be written more compactly. Taking the product of exponentials is equivalent to summing their arguments:\n$$\np(x \\,|\\, \\{s_i\\}) \\propto \\exp\\left( -\\frac{1}{2\\sigma^2} \\sum_{n} \\left(x[n] - \\sum_{i=1}^{K} (h_i * s_i)[n]\\right)^2 \\right)\n$$\nThe sum is the squared Euclidean ($\\ell_2$) norm of the residual vector. Let the full signals be represented as vectors $x$ and $s_i$. The expression is:\n$$\np(x \\,|\\, \\{s_i\\}) \\propto \\exp\\left( -\\frac{1}{2\\sigma^2} \\left\\lVert x - \\sum_{i=1}^{K} h_i * s_i \\right\\rVert_{2}^{2} \\right)\n$$\nNext, we define the prior term $p(\\{s_i\\})$. The problem states that the spike trains $\\{s_i\\}$ are independent across motor units $i$, and each follows a Laplace prior $p(s_i) \\propto \\exp(-\\alpha \\lVert s_i \\rVert_1)$. The joint prior is the product of the individual priors:\n$$\np(\\{s_i\\}_{i=1}^{K}) = \\prod_{i=1}^{K} p(s_i) \\propto \\prod_{i=1}^{K} \\exp(-\\alpha \\lVert s_i \\rVert_1) = \\exp\\left(-\\alpha \\sum_{i=1}^{K} \\lVert s_i \\rVert_1\\right)\n$$\nCombining the likelihood and the prior, the posterior probability is:\n$$\np(\\{s_i\\} \\,|\\, x) \\propto \\exp\\left( -\\frac{1}{2\\sigma^2} \\left\\lVert x - \\sum_{i=1}^{K} h_i * s_i \\right\\rVert_{2}^{2} \\right) \\exp\\left(-\\alpha \\sum_{i=1}^{K} \\lVert s_i \\rVert_1\\right)\n$$\n$$\np(\\{s_i\\} \\,|\\, x) \\propto \\exp\\left( -\\frac{1}{2\\sigma^2} \\left\\lVert x - \\sum_{i=1}^{K} h_i * s_i \\right\\rVert_{2}^{2} - \\alpha \\sum_{i=1}^{K} \\lVert s_i \\rVert_1 \\right)\n$$\nMaximizing the posterior probability is equivalent to minimizing its negative logarithm. Let $J(\\{s_i\\})$ be this negative log-posterior objective function:\n$$\nJ(\\{s_i\\}) = -\\ln(p(\\{s_i\\} \\,|\\, x)) = C + \\frac{1}{2\\sigma^2} \\left\\lVert x - \\sum_{i=1}^{K} h_i * s_i \\right\\rVert_{2}^{2} + \\alpha \\sum_{i=1}^{K} \\lVert s_i \\rVert_1\n$$\nwhere $C$ is a constant that does not depend on $\\{s_i\\}$. To find the MAP estimate $\\{\\hat{s}_i\\}$, we minimize this objective with respect to $\\{s_i\\}$. We can discard the constant $C$ and multiply the entire objective by a positive constant, $2\\sigma^2$, without changing the location of the minimum. This yields the final form of the MAP optimization problem:\n$$\n\\{\\hat{s}_i\\} = \\arg\\min_{\\{s_i\\}, s_i \\ge 0} \\left\\{ \\left\\lVert x - \\sum_{i=1}^{K} h_i * s_i \\right\\rVert_{2}^{2} + 2\\sigma^2\\alpha \\sum_{i=1}^{K} \\lVert s_i \\rVert_1 \\right\\}\n$$\nThe problem also asks to introduce a single regularization parameter $\\lambda = 2\\sigma^2\\alpha$. Since $\\sigma^2 > 0$ and $\\alpha > 0$, we have $\\lambda > 0$. The resulting objective function is a sum of two terms: a data fidelity term and a sparsity-inducing regularizer. The fidelity term, $\\left\\lVert x - \\sum_{i=1}^{K} h_i * s_i \\right\\rVert_{2}^{2}$, penalizes deviations of the model from the observed data and arises from the Gaussian noise assumption. The regularizer, $\\lambda \\sum_{i=1}^{K} \\lVert s_i \\rVert_1$, penalizes non-sparse solutions and arises from the Laplace prior assumption. The nonnegativity constraint $s_i[n] \\ge 0$ is also imposed, as given.\n\n### Explanation of Sparsity for MUAP Separation\n\nThe MAP objective derived above is a form of convolutional sparse coding. The effectiveness of this framework in separating temporally overlapping MUAPs relies on the interplay between the properties of the $\\ell_1$-norm regularizer and the characteristics of the MUAP template dictionary $\\{h_i\\}$.\n\n1.  **Sparsity and Reduced Support**: The $\\ell_1$-norm, $\\lVert s_i \\rVert_1 = \\sum_n |s_i[n]|$, is a convex function that promotes sparsity. This means that minimizing an objective containing an $\\ell_1$ penalty encourages solutions for $\\{s_i\\}$ where many of the coefficients $s_i[n]$ are exactly zero. In the context of EMG, an event $s_i[n] > 0$ represents a firing of motor unit $i$ at time $n$. A sparse solution implies that the model explains the observed signal $x[n]$ using the fewest possible motor unit firings, which is consistent with the underlying physiology of neural control. The set of non-zero entries in a signal is its \"support\". The $\\ell_1$ penalty seeks solutions with reduced support.\n\n2.  **Role of Mutual Coherence**: The ability to distinguish between different MUAPs depends on how \"different\" their templates $\\{h_i\\}$ are. This is formalized by the concept of mutual coherence of the dictionary of templates. Low mutual coherence implies that any one template $h_i$ cannot be well-represented as a linear combination of other templates $\\{h_j\\}_{j\\neq i}$. When two or more MUAPs from different units (e.g., $i$ and $j$) fire nearly simultaneously, the observed signal is a superposition of their templates, e.g., $s_i[n_1]h_i[n-n_1] + s_j[n_2]h_j[n-n_2]$. If the templates $h_i$ and $h_j$ have low coherence with all other templates $h_k$, it is impossible to accurately represent this superposition using a single scaled template $s_k[n_k]h_k[n-n_k]$ without incurring a large error in the data fidelity term $\\lVert \\cdot \\rVert_2^2$. Therefore, the optimization process is forced to activate both $s_i$ and $s_j$ to achieve a good fit.\n\n3.  **Identifiability via $\\ell_1$ Minimization and Triangle Inequality**: Once the optimizer is forced to use a combination of templates to explain an overlapping event, the $\\ell_1$ penalty guides the selection towards the sparsest possible combination. Consider the true explanation of an overlapping segment $x_{seg}$ requires two spikes, represented by spike trains $s_i$ and $s_j$. The contribution to the regularization term is $\\lambda (\\lVert s_i \\rVert_1 + \\lVert s_j \\rVert_1)$. An incorrect, non-sparse explanation might involve small contributions from many different motor units, $\\{s'_k\\}_{k=1}^K$, to reconstruct the same segment $x_{seg}$. A fundamental property related to the $\\ell_1$ norm and the triangle inequality is that representing a signal with a few \"correct\" atomic elements is generally more efficient (i.e., has a smaller $\\ell_1$ norm on the coefficients) than representing it with a dense combination of many elements. The minimizer of the $\\ell_1$ norm is preferentially a sparse vector. Sparse representation theory guarantees that if the true underlying solution is sufficiently sparse and the template dictionary has sufficiently low mutual coherence, then solving the $\\ell_1$-regularized minimization problem is guaranteed to recover the correct sparse solution.\n\nIn summary, when MUAPs overlap, low template coherence creates a large reconstruction error for any overly simple (e.g., single-spike) explanation. The optimizer must therefore use a superposition of multiple templates. The $\\ell_1$ regularizer then ensures that this superposition is the sparsest one possible, favoring the true, physiologically meaningful set of distinct motor unit firings over a dense, un-interpretable alternative. This allows the algorithm to \"unmix\" the superimposed signals into their constituent spike trains.",
            "answer": "$$\n\\boxed{\\left\\lVert x - \\sum_{i=1}^{K} h_i * s_i \\right\\rVert_{2}^{2} + \\lambda \\sum_{i=1}^{K} \\left\\lVert s_i \\right\\rVert_{1}}\n$$"
        }
    ]
}