{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of subject-specific modeling is the ability to assign spatially varying material properties based on non-invasive medical imaging. This exercise walks you through the complete pipeline for converting Computed Tomography (CT) data into a map of elastic modulus, a common practice in musculoskeletal biomechanics. By starting from the physical principle of X-ray attenuation and working through the definitions of Hounsfield Units ($HU$) to an empirical density-modulus relationship, you will gain a hands-on understanding of how imaging data informs the mechanical behavior of a personalized model .",
            "id": "4207180",
            "problem": "A monochromatic X-ray beam of energy $E = 80\\,\\text{keV}$ passes through a homogeneous slab of patient tissue of thickness $L = 5.0\\,\\text{cm}$. The exiting intensity $I$ is measured relative to the incident intensity $I_0$, yielding an intensity ratio $I/I_0 = 0.138$ at this energy. The linear attenuation coefficient of water at this energy is known to be $\\mu_{\\text{w}} = 22.0\\,\\text{m}^{-1}$. A Computed Tomography (CT) reconstruction of the same voxel reports a Hounsfield Unit (HU) value of $\\text{HU} = 800$. A scanner-specific electron-density calibration curve has been established using tissue-equivalent phantoms and independent compositional measurements, relating electron density $\\rho_{\\text{e}}$ to the attenuation contrast via the polynomial\n$$\n\\frac{\\rho_{\\text{e}}}{\\rho_{\\text{e,w}}} \\;=\\; 1 \\;+\\; \\lambda_1\\,\\delta \\;+\\; \\lambda_2\\,\\delta^{2},\n$$\nwhere $\\rho_{\\text{e,w}}$ is the electron density of water, $\\delta = \\frac{\\mu - \\mu_{\\text{w}}}{\\mu_{\\text{w}}}$, $\\mu$ is the tissue’s linear attenuation coefficient at energy $E$, and the calibration constants are $\\lambda_1 = 0.70$ and $\\lambda_2 = 0.05$. The CT Hounsfield Unit is defined by\n$$\n\\text{HU} \\;=\\; 1000 \\left( \\frac{\\mu - \\mu_{\\text{w}}}{\\mu_{\\text{w}}} \\right).\n$$\nFor subject-specific mechanical modeling, a regression-based personalization maps electron density to elastic modulus via\n$$\nE_{\\text{tissue}} \\;=\\; E_0 \\left( \\frac{\\rho_{\\text{e}}}{\\rho_{\\text{e,w}}} \\right)^{\\alpha},\n$$\nwhere $E_0 = 4.00\\,\\text{GPa}$ and $\\alpha = 3.5$ are empirically determined constants for mineralized tissues measured with matched imaging and mechanical testing protocols.\n\nTasks:\n- Starting from a differential photon balance and the assumption that the rate of attenuation per unit length is proportional to the local intensity, derive the Beer–Lambert law and use it to compute $\\mu$ from the measured $I/I_0$ and $L$.\n- Use the CT Hounsfield Unit definition to relate $\\text{HU}$ to $\\delta$ and show how this feeds into the electron-density calibration to obtain $\\rho_{\\text{e}}/\\rho_{\\text{e,w}}$.\n- Combine these results to compute the subject-specific elastic modulus $E_{\\text{tissue}}$.\n\nExpress the final elastic modulus in gigapascals (GPa). Round your final numerical answer to three significant figures.",
            "solution": "The problem is validated against the specified criteria.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- Incident X-ray energy: $E = 80\\,\\text{keV}$\n- Tissue thickness: $L = 5.0\\,\\text{cm}$\n- Intensity ratio: $I/I_0 = 0.138$\n- Linear attenuation coefficient of water at energy $E$: $\\mu_{\\text{w}} = 22.0\\,\\text{m}^{-1}$\n- Hounsfield Unit value: $\\text{HU} = 800$\n- Electron-density calibration curve: $\\frac{\\rho_{\\text{e}}}{\\rho_{\\text{e,w}}} = 1 + \\lambda_1\\,\\delta + \\lambda_2\\,\\delta^{2}$\n- Definition of attenuation contrast $\\delta$: $\\delta = \\frac{\\mu - \\mu_{\\text{w}}}{\\mu_{\\text{w}}}$\n- Calibration constant: $\\lambda_1 = 0.70$\n- Calibration constant: $\\lambda_2 = 0.05$\n- Hounsfield Unit definition: $\\text{HU} = 1000 \\left( \\frac{\\mu - \\mu_{\\text{w}}}{\\mu_{\\text{w}}} \\right)$\n- Elastic modulus regression relation: $E_{\\text{tissue}} = E_0 \\left( \\frac{\\rho_{\\text{e}}}{\\rho_{\\text{e,w}}} \\right)^{\\alpha}$\n- Regression constant: $E_0 = 4.00\\,\\text{GPa}$\n- Regression exponent: $\\alpha = 3.5$\n- Tasks: (i) Derive the Beer–Lambert law and compute $\\mu$. (ii) Relate $\\text{HU}$ to $\\delta$ and compute $\\rho_{\\text{e}}/\\rho_{\\text{e,w}}$. (iii) Compute the subject-specific elastic modulus $E_{\\text{tissue}}$.\n- Final answer requirements: Express the result in gigapascals (GPa) and round to three significant figures.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded**: The problem is based on established principles of medical physics (Beer-Lambert law, CT Hounsfield Units) and biomechanics (density-modulus relationships). All concepts and definitions are standard in the field.\n- **Well-Posed**: The problem provides all necessary parameters and equations to perform the requested calculations in a logical sequence. It is self-contained.\n- **Objective**: The problem is stated using precise, formal, and unbiased language.\n- **Consistency Check**: The problem provides two potential routes to determine the tissue's linear attenuation coefficient, $\\mu$. One is from the transmission experiment ($I/I_0, L$), and the other is from the Hounsfield Unit value ($\\text{HU}, \\mu_{\\text{w}}$). Let's check for consistency.\n    - From the transmission data: $\\mu = -\\frac{1}{L}\\ln(I/I_0)$. With $L = 5.0\\,\\text{cm} = 0.050\\,\\text{m}$, we get $\\mu = -\\frac{1}{0.050\\,\\text{m}}\\ln(0.138) \\approx -20 \\times (-1.9805) \\approx 39.61\\,\\text{m}^{-1}$.\n    - From the Hounsfield Unit definition: $\\text{HU} = 1000 \\left( \\frac{\\mu - \\mu_{\\text{w}}}{\\mu_{\\text{w}}} \\right)$. With $\\text{HU}=800$ and $\\mu_{\\text{w}} = 22.0\\,\\text{m}^{-1}$, we have $800 = 1000 \\left( \\frac{\\mu - 22.0}{22.0} \\right)$, which simplifies to $0.8 = \\frac{\\mu - 22.0}{22.0}$. This gives $\\mu = 22.0 \\times (1 + 0.8) = 22.0 \\times 1.8 = 39.6\\,\\text{m}^{-1}$.\n- The values for $\\mu$ calculated from the two provided pieces of information are consistent ($39.61\\,\\text{m}^{-1}$ vs $39.6\\,\\text{m}^{-1}$). This confirms the problem is not contradictory but is well-constructed, with the transmission data serving as a physical basis that aligns with the reported CT value. The tasks guide the user to use both pieces of information appropriately. The problem does not violate any of the invalidity criteria.\n\n**Step 3: Verdict and Action**\n- The problem is **valid**. The solution process will proceed.\n\n### Solution\n\nThe solution is developed by following the three tasks outlined in the problem statement.\n\n**Task 1: Derive the Beer–Lambert law and compute $\\mu$**\n\nThe problem states that the rate of attenuation per unit length is proportional to the local intensity. Let $I(x)$ be the X-ray beam intensity at a depth $x$ within the tissue. The change in intensity, $dI$, over an infinitesimal thickness $dx$ is a decrease proportional to both the intensity $I(x)$ at that point and the thickness $dx$. The constant of proportionality is the linear attenuation coefficient, $\\mu$. This relationship is expressed as a differential equation:\n$$\ndI = -\\mu I(x) dx\n$$\nTo find the intensity $I$ after passing through a slab of finite thickness $L$, we must integrate this equation. We rearrange the equation to separate variables:\n$$\n\\frac{dI}{I} = -\\mu dx\n$$\nWe integrate both sides. The intensity decreases from the incident value $I_0$ at $x=0$ to the transmitted value $I$ at $x=L$. The tissue is homogeneous, so $\\mu$ is constant over the path length.\n$$\n\\int_{I_0}^{I} \\frac{dI'}{I'} = \\int_{0}^{L} (-\\mu) dx'\n$$\nPerforming the integration gives:\n$$\n[\\ln(I')]_{I_0}^{I} = -\\mu [x']_{0}^{L}\n$$\n$$\n\\ln(I) - \\ln(I_0) = -\\mu (L - 0)\n$$\n$$\n\\ln\\left(\\frac{I}{I_0}\\right) = -\\mu L\n$$\nExponentiating both sides yields the Beer–Lambert law:\n$$\nI = I_0 \\exp(-\\mu L)\n$$\nNow, we compute the linear attenuation coefficient $\\mu$ for the given tissue. We use the logarithmic form of the law and substitute the provided values $L = 5.0\\,\\text{cm} = 0.050\\,\\text{m}$ and $I/I_0 = 0.138$:\n$$\n\\mu = -\\frac{1}{L} \\ln\\left(\\frac{I}{I_0}\\right) = -\\frac{1}{0.050\\,\\text{m}} \\ln(0.138)\n$$\n$$\n\\mu \\approx -\\frac{1}{0.050\\,\\text{m}} \\times (-1.98050) \\approx 39.610\\,\\text{m}^{-1}\n$$\n\n**Task 2: Relate HU to $\\delta$ and compute $\\rho_{\\text{e}}/\\rho_{\\text{e,w}}$**\n\nThe Hounsfield Unit ($\\text{HU}$) and the attenuation contrast $\\delta$ are defined as:\n$$\n\\text{HU} = 1000 \\left( \\frac{\\mu - \\mu_{\\text{w}}}{\\mu_{\\text{w}}} \\right)\n$$\n$$\n\\delta = \\frac{\\mu - \\mu_{\\text{w}}}{\\mu_{\\text{w}}}\n$$\nBy direct comparison, the relationship between $\\text{HU}$ and $\\delta$ is:\n$$\n\\text{HU} = 1000\\,\\delta \\quad \\implies \\quad \\delta = \\frac{\\text{HU}}{1000}\n$$\nUsing the given CT-reconstructed value $\\text{HU} = 800$, we calculate $\\delta$:\n$$\n\\delta = \\frac{800}{1000} = 0.8\n$$\nNow, we use this value of $\\delta$ in the scanner-specific electron-density calibration curve to find the normalized electron density, $\\rho_{\\text{e}}/\\rho_{\\text{e,w}}$:\n$$\n\\frac{\\rho_{\\text{e}}}{\\rho_{\\text{e,w}}} = 1 + \\lambda_1\\,\\delta + \\lambda_2\\,\\delta^{2}\n$$\nSubstituting the values $\\delta=0.8$, $\\lambda_1 = 0.70$, and $\\lambda_2 = 0.05$:\n$$\n\\frac{\\rho_{\\text{e}}}{\\rho_{\\text{e,w}}} = 1 + (0.70)(0.8) + (0.05)(0.8)^2\n$$\n$$\n\\frac{\\rho_{\\text{e}}}{\\rho_{\\text{e,w}}} = 1 + 0.56 + (0.05)(0.64)\n$$\n$$\n\\frac{\\rho_{\\text{e}}}{\\rho_{\\text{e,w}}} = 1 + 0.56 + 0.032 = 1.592\n$$\n\n**Task 3: Compute the subject-specific elastic modulus $E_{\\text{tissue}}$**\n\nThe final step is to compute the elastic modulus of the tissue, $E_{\\text{tissue}}$, using the provided regression-based personalization formula. This formula relates the normalized electron density to the mechanical property:\n$$\nE_{\\text{tissue}} = E_0 \\left( \\frac{\\rho_{\\text{e}}}{\\rho_{\\text{e,w}}} \\right)^{\\alpha}\n$$\nWe substitute the known values: the result from Task 2, $\\rho_{\\text{e}}/\\rho_{\\text{e,w}} = 1.592$, and the empirical constants $E_0 = 4.00\\,\\text{GPa}$ and $\\alpha = 3.5$.\n$$\nE_{\\text{tissue}} = (4.00\\,\\text{GPa}) \\times (1.592)^{3.5}\n$$\nFirst, calculate the value of $(1.592)^{3.5}$:\n$$\n(1.592)^{3.5} \\approx 5.08785\n$$\nNow, multiply by $E_0$:\n$$\nE_{\\text{tissue}} = 4.00\\,\\text{GPa} \\times 5.08785 \\approx 20.3514\\,\\text{GPa}\n$$\nThe problem requires the final answer to be rounded to three significant figures.\n$$\nE_{\\text{tissue}} \\approx 20.4\\,\\text{GPa}\n$$",
            "answer": "$$\n\\boxed{20.4}\n$$"
        },
        {
            "introduction": "Before personalizing material parameters, the geometry of a computational model must be accurately aligned with the subject's anatomy. This practice addresses the fundamental task of rigid registration, where you will find the optimal rotation and translation to align two corresponding sets of landmark points. By deriving the solution from first principles using Singular Value Decomposition (SVD), you will master a robust technique that is essential for model-to-image alignment and a prerequisite for virtually all subsequent personalization and analysis steps .",
            "id": "4207164",
            "problem": "A subject-specific model-to-image initialization requires aligning a set of $3$-dimensional anatomical landmarks extracted from medical imaging to a template model via a rigid transformation. A rigid transformation maps each point $x \\in \\mathbb{R}^3$ to $R x + t$, where $R \\in \\mathbb{R}^{3 \\times 3}$ is a rotation matrix satisfying $R^\\top R = I$ and $\\det(R) = 1$ (a proper rotation in the Special Orthogonal Group) and $t \\in \\mathbb{R}^3$ is a translation vector. Given two corresponding point sets $A = \\{a_i\\}_{i=1}^N$ and $B = \\{b_i\\}_{i=1}^N$ in $\\mathbb{R}^3$, representing the same anatomical locations in model and image spaces respectively, the optimal rigid transformation minimizes the sum of squared distances between transformed points and their correspondences. The goal is to derive, from first principles and well-tested linear algebraic facts, the Singular Value Decomposition (SVD)-based solution that estimates $R$ and $t$, and then quantify the residual alignment error.\n\nYour program must:\n- Derive the optimal rigid transform $(R, t)$ using a principled approach grounded in the definition of a rigid transformation and the properties of orthogonal matrices and Singular Value Decomposition (SVD).\n- Compute the residual alignment error as the root-mean-square (RMS) of Euclidean distances between $R a_i + t$ and $b_i$, expressed in meters.\n- Ensure angles are treated in radians and all physical distances are in meters.\n\nFundamental base for derivation:\n- Rigid body motion preserves distances; $R$ must satisfy $R^\\top R = I$ and $\\det(R) = 1$.\n- The least-squares objective is $\\sum_{i=1}^N \\|R a_i + t - b_i\\|^2$.\n- Use properties of centroids, covariance, and Singular Value Decomposition (SVD) to construct the optimal solution without invoking any shortcut formulas.\n\nTest suite specification (use pseudorandom number generator seeded with $42$ for reproducibility):\n- Case $1$ (general happy path):\n  - Number of points $N = 20$.\n  - Source points sampled uniformly from a box with each coordinate in $[-0.05, 0.05]$ meters.\n  - Rotation defined by axis $u = \\frac{1}{\\sqrt{3}}[1, 1, 1]^\\top$ and angle $\\theta = 0.5$ radians.\n  - Translation $t = [0.02, -0.01, 0.03]^\\top$ meters.\n  - Add independent Gaussian noise of standard deviation $\\sigma = 0.002$ meters to the transformed points.\n- Case $2$ (reflection edge case):\n  - Reuse the source points from Case $1$.\n  - Apply the same rotation and translation.\n  - Then reflect the transformed points across the $yz$-plane (multiply the $x$-coordinate by $-1$).\n  - Add independent Gaussian noise of standard deviation $\\sigma = 0.001$ meters.\n- Case $3$ (degenerate collinear configuration):\n  - Number of points $N = 10$ placed collinearly as $a_i = \\lambda_i d$ with $\\lambda_i$ uniformly spaced in $[-0.05, 0.05]$ and $d = \\frac{1}{\\sqrt{5}}[1, 2, 0]^\\top$.\n  - Rotation about the $z$-axis by angle $\\theta = 0.3$ radians.\n  - Translation $t = [0.0, 0.0, 0.0]^\\top$ meters.\n  - No noise, i.e., $\\sigma = 0.0$ meters.\n- Case $4$ (translation-only boundary):\n  - Number of points $N = 15$ sampled uniformly from a box with each coordinate in $[-0.05, 0.05]$ meters.\n  - Identity rotation (angle $\\theta = 0.0$ radians).\n  - Translation $t = [0.1, -0.05, 0.0]^\\top$ meters.\n  - No noise, i.e., $\\sigma = 0.0$ meters.\n- Case $5$ (minimal non-collinear set with noise):\n  - Three points forming a small triangle: $A = \\{[0.0, 0.0, 0.0]^\\top, [0.05, 0.0, 0.0]^\\top, [0.0, 0.03, 0.0]^\\top\\}$ meters.\n  - Rotation about the $y$-axis by angle $\\theta = \\pi/4$ radians.\n  - Translation $t = [0.0, 0.0, 0.0]^\\top$ meters.\n  - Add independent Gaussian noise of standard deviation $\\sigma = 0.005$ meters.\n\nImplementation requirements:\n- Construct the synthetic point sets exactly as specified.\n- Use the SVD-based orthogonal Procrustes approach to compute $R$ and $t$ under the constraint $\\det(R) = 1$.\n- Quantify the residual alignment error as an RMS value in meters for each case.\n- Angles must be in radians; distances must be in meters.\n\nFinal output format:\n- Your program should produce a single line of output containing the RMS alignment errors for the five test cases as a comma-separated list enclosed in square brackets, each value rounded to six decimal places in meters, for example, $[0.001234,0.005678,0.000000,0.000123,0.002345]$.",
            "solution": "The problem requires the derivation and implementation of an algorithm to find the optimal rigid transformation $(R, t)$ that aligns two corresponding sets of $3$-dimensional points, $A = \\{a_i\\}_{i=1}^N$ and $B = \\{b_i\\}_{i=1}^N$. The transformation is defined by $x \\mapsto R x + t$, where $R$ is a proper rotation matrix ($R^\\top R = I$, $\\det(R) = 1$) and $t$ is a translation vector. The optimality is defined in the least-squares sense, minimizing the objective function $E(R, t) = \\sum_{i=1}^N \\|R a_i + t - b_i\\|^2$. The solution must be derived from first principles using properties of linear algebra and Singular Value Decomposition (SVD).\n\n### Derivation from First Principles\n\n**1. Optimization with Respect to Translation $t$**\n\nThe objective function $E(R, t)$ is a quadratic function of the translation vector $t$. The minimum can be found by setting its gradient with respect to $t$ to zero.\n$$ \\nabla_t E(R, t) = \\nabla_t \\left( \\sum_{i=1}^N (R a_i + t - b_i)^\\top (R a_i + t - b_i) \\right) = 0 $$\nTaking the derivative inside the summation:\n$$ \\sum_{i=1}^N 2 (R a_i + t - b_i) = 0 $$\nDividing by $2$ and distributing the summation:\n$$ \\sum_{i=1}^N R a_i + \\sum_{i=1}^N t - \\sum_{i=1}^N b_i = 0 $$\n$$ R \\left(\\sum_{i=1}^N a_i\\right) + N t - \\sum_{i=1}^N b_i = 0 $$\nLet the centroids of the point sets be defined as $\\bar{a} = \\frac{1}{N} \\sum_{i=1}^N a_i$ and $\\bar{b} = \\frac{1}{N} \\sum_{i=1}^N b_i$. The equation becomes:\n$$ R (N \\bar{a}) + N t - N \\bar{b} = 0 $$\nSolving for $t$ yields the optimal translation vector as a function of the rotation $R$:\n$$ t = \\bar{b} - R \\bar{a} $$\nThis result shows that the optimal translation aligns the centroid of the transformed source points with the centroid of the target points.\n\n**2. Decoupling the Rotation Problem**\n\nWe substitute the expression for the optimal $t$ back into the objective function to obtain a problem solely in terms of $R$:\n$$ E(R) = \\sum_{i=1}^N \\|R a_i + (\\bar{b} - R \\bar{a}) - b_i\\|^2 $$\nBy rearranging terms, we can express the problem in terms of centered coordinates, $a'_i = a_i - \\bar{a}$ and $b'_i = b_i - \\bar{b}$:\n$$ E(R) = \\sum_{i=1}^N \\| (R a_i - R \\bar{a}) - (b_i - \\bar{b}) \\|^2 = \\sum_{i=1}^N \\| R (a_i - \\bar{a}) - (b_i - \\bar{b}) \\|^2 $$\n$$ E(R) = \\sum_{i=1}^N \\| R a'_i - b'_i \\|^2 $$\nNow, the task is to find the optimal rotation $R$ that minimizes the sum of squared distances between the centered point sets.\n\n**3. Optimization with Respect to Rotation $R$**\n\nWe expand the squared norm in the objective function:\n$$ E(R) = \\sum_{i=1}^N (R a'_i - b'_i)^\\top (R a'_i - b'_i) = \\sum_{i=1}^N \\left( (a'_i)^\\top R^\\top R a'_i - 2(a'_i)^\\top R^\\top b'_i + (b'_i)^\\top b'_i \\right) $$\nSince $R$ is a rotation matrix, $R^\\top R = I$. The expression simplifies to:\n$$ E(R) = \\sum_{i=1}^N \\left( \\|a'_i\\|^2 - 2(b'_i)^\\top R a'_i + \\|b'_i\\|^2 \\right) $$\nThe terms $\\sum \\|a'_i\\|^2$ and $\\sum \\|b'_i\\|^2$ are constant with respect to $R$. Therefore, minimizing $E(R)$ is equivalent to maximizing the term $\\sum_{i=1}^N (b'_i)^\\top R a'_i$.\n\nUsing the properties of the matrix trace (specifically, a scalar is its own trace, and the trace operator is cyclic, $\\text{tr}(XY) = \\text{tr}(YX)$), we can rewrite the quantity to be maximized:\n$$ \\sum_{i=1}^N (b'_i)^\\top R a'_i = \\sum_{i=1}^N \\text{tr}((b'_i)^\\top R a'_i) = \\sum_{i=1}^N \\text{tr}(R a'_i (b'_i)^\\top) $$\n$$ = \\text{tr}\\left(R \\sum_{i=1}^N a'_i (b'_i)^\\top\\right) $$\nLet's define the $3 \\times 3$ cross-covariance matrix $H$ as:\n$$ H = \\sum_{i=1}^N a'_i (b'_i)^\\top $$\nThe optimization problem is now reduced to:\n$$ \\underset{R}{\\text{maximize}} \\quad \\text{tr}(RH) \\quad \\text{subject to} \\quad R^\\top R = I, \\det(R) = 1 $$\n\n**4. The SVD-Based Solution**\n\nLet the Singular Value Decomposition (SVD) of $H$ be $H = U \\Sigma V^\\top$, where $U, V \\in \\mathbb{R}^{3 \\times 3}$ are orthogonal matrices and $\\Sigma = \\text{diag}(\\sigma_1, \\sigma_2, \\sigma_3)$ is a diagonal matrix of non-negative singular values, ordered such that $\\sigma_1 \\ge \\sigma_2 \\ge \\sigma_3 \\ge 0$.\n\nSubstituting the SVD of $H$ into the trace expression:\n$$ \\text{tr}(RH) = \\text{tr}(R U \\Sigma V^\\top) = \\text{tr}(\\Sigma V^\\top R U) $$\nLet $M = V^\\top R U$. Since $V$, $R$, and $U$ are orthogonal, $M$ is also an orthogonal matrix. The expression becomes $\\text{tr}(\\Sigma M) = \\sum_{i=1}^3 \\sigma_i M_{ii}$. As $M$ is orthogonal, its diagonal elements satisfy $|M_{ii}| \\le 1$. To maximize this sum, we should choose $M$ such that its diagonal elements are as large as possible. If there were no determinant constraint, the maximum would be achieved at $M=I$, which gives $V^\\top R U = I$, leading to $R = V U^\\top$.\n\nWe must, however, satisfy the proper rotation constraint $\\det(R)=1$.\n$$ \\det(R) = \\det(V U^\\top) = \\det(V) \\det(U^\\top) = \\det(V)\\det(U) $$\nThe determinants of orthogonal matrices $U$ and $V$ can be either $+1$ or $-1$.\n\nCase 1: $\\det(V)\\det(U) = 1$. In this case, $\\det(R)=1$, and the solution $R = V U^\\top$ is a proper rotation. This is the optimal solution.\n\nCase 2: $\\det(V)\\det(U) = -1$. In this scenario, $R = V U^\\top$ is an improper rotation (a reflection), with $\\det(R)=-1$. Our problem requires finding the proper rotation $R$ that maximizes $\\text{tr}(RH)$, which is equivalent to maximizing $\\text{tr}(\\Sigma M)$ subject to $M$ being an orthogonal matrix with $\\det(M) = \\det(V^\\top R U) = \\det(V^\\top)\\det(R)\\det(U) = (\\det(V)\\det(U))^{-1} \\det(R) = (-1)^{-1}(1) = -1$.\nWe need to maximize $\\sum \\sigma_i M_{ii}$ subject to $\\det(M) = -1$. Given $\\sigma_1 \\ge \\sigma_2 \\ge \\sigma_3 \\ge 0$, the maximum is achieved by setting $M_{11}=1$, $M_{22}=1$, and $M_{33}=-1$. This choice makes the smallest singular value contribute negatively, minimizing the loss from the maximum possible sum $\\sigma_1+\\sigma_2+\\sigma_3$.\nThe optimal choice for $M$ is $S = \\text{diag}(1, 1, -1)$.\nSetting $M=S$ gives $V^\\top R U = S$, which yields the corrected rotation:\n$$ R = V S U^\\top = V \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & -1 \\end{pmatrix} U^\\top $$\n\n**Summary of the Algorithm**\n1.  Compute the centroids of both point sets: $\\bar{a} = \\frac{1}{N}\\sum a_i$ and $\\bar{b} = \\frac{1}{N}\\sum b_i$.\n2.  Compute centered point sets: $A' = \\{a_i - \\bar{a}\\}$ and $B' = \\{b_i - \\bar{b}\\}$.\n3.  Compute the cross-covariance matrix $H = (A')^\\top B'$.\n4.  Compute the SVD of $H$: $H = U \\Sigma V^\\top$.\n5.  Compute the rotation matrix $R = V U^\\top$.\n6.  If $\\det(R) < 0$, correct for reflection: $R = V \\text{diag}(1, 1, -1) U^\\top$.\n7.  Compute the translation vector $t = \\bar{b} - R \\bar{a}$.\n8.  The estimated transformation is $(R,t)$. The residual alignment error is calculated as the RMS error: $E_{RMS} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N \\|(R a_i + t) - b_i\\|^2}$.\n\nThis principled derivation provides a robust algorithm for rigid point set registration, which will now be implemented.",
            "answer": "[0.003310,0.081829,0.000000,0.000000,0.004243]"
        },
        {
            "introduction": "Personalizing a model by fitting it to experimental data is an inverse problem that can be fraught with ambiguity: can we uniquely determine every parameter? This hands-on exercise introduces the crucial concept of parameter identifiability through sensitivity analysis. You will use the Singular Value Decomposition (SVD) of the sensitivity matrix to diagnose how well parameters can be estimated and to understand how measurement noise propagates into parameter uncertainty, a critical skill for assessing the reliability of any personalized model .",
            "id": "4207168",
            "problem": "You are modeling a subject-specific, two-parameter linearized biomechanical personalization problem from imaging-based measurements. The imaging modality provides displacement measurements that are modeled as differentiable functions of two material parameters: elastic modulus and Poisson ratio. After linearization around a reference parameter vector, the measurements can be approximated by a linear model with additive noise. Your goal is to quantify parameter identifiability using Singular Value Decomposition (SVD) of the sensitivity matrix and to compute the resulting parameter variances under a weak Gaussian prior.\n\nAssume the following setup:\n- Let the parameter vector be $\\boldsymbol{\\theta} = [\\theta_1,\\theta_2]^\\top$, where $\\theta_1$ is the elastic modulus in kilopascals (kPa) and $\\theta_2$ is the Poisson ratio (dimensionless).\n- Let the sensitivity matrix be $\\mathbf{J} \\in \\mathbb{R}^{m \\times 2}$, where $m$ is the number of independent displacement measurements from imaging. Each entry of $\\mathbf{J}$ represents a partial derivative of the predicted displacement (in millimeters) with respect to each parameter.\n- Let measurement noise be additive, zero-mean Gaussian with independent components of standard deviation $\\sigma$ in millimeters, so that the measurement model is $\\mathbf{y} \\approx \\mathbf{y}_{\\text{ref}} + \\mathbf{J}\\,\\Delta\\boldsymbol{\\theta} + \\boldsymbol{\\varepsilon}$ with $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I})$.\n- Assume a weak, independent Gaussian prior on parameters with standard deviations collected in a diagonal matrix $\\mathbf{S}_p = \\mathrm{diag}(s_{\\theta_1}, s_{\\theta_2})$, where $s_{\\theta_1}$ is in kilopascals (kPa) and $s_{\\theta_2}$ is dimensionless. This prior ensures numerical stability while remaining weak compared to the data.\n\nFundamental base:\n- Use the linearized observation model $\\mathbf{y} \\approx \\mathbf{y}_{\\text{ref}} + \\mathbf{J}\\,\\Delta\\boldsymbol{\\theta} + \\boldsymbol{\\varepsilon}$, additive independent Gaussian noise $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I})$, and the definition of Singular Value Decomposition (SVD) $\\mathbf{J} = \\mathbf{U}\\boldsymbol{\\Sigma}\\mathbf{V}^\\top$, where $\\mathbf{U}$ and $\\mathbf{V}$ are orthonormal and $\\boldsymbol{\\Sigma}$ contains nonnegative singular values.\n- Quantify identifiability through the spectrum of $\\boldsymbol{\\Sigma}$ and compute parameter variance using the posterior covariance derived from the Gaussian linear model with the weak Gaussian prior. You must explicitly use the SVD structure in your reasoning and computation. Do not rely on shortcut formulas or identities beyond these bases.\n\nTasks:\n1. For each test case below, compute the singular values of $\\mathbf{J}$ via SVD, sort them in descending order, and compute the condition number $\\kappa$ defined as the ratio of the largest singular value to the smallest nonzero singular value. If the smallest singular value is numerically zero (treat values less than $10^{-12}$ as zero), set $\\kappa$ to $+\\infty$.\n2. Using the weak Gaussian prior, compute the posterior covariance matrix of $\\Delta\\boldsymbol{\\theta}$ and extract the parameter standard deviations. You must express the elastic modulus standard deviation in kilopascals (kPa) and the Poisson ratio standard deviation as dimensionless. Round each standard deviation to exactly $6$ decimal places.\n3. Interpret the impact of small singular values on the parameter variances by comparing the condition numbers and the resulting standard deviations across cases. This interpretation should be reflected in your computed outputs; you do not need to print text beyond the specified output format.\n\nTest suite:\nProvide results for the following three cases. All numerical entities must be treated exactly as specified. In each case, $\\mathbf{J}$ is given as a real matrix with entries in millimeters per parameter unit.\n\n- Case A (well-conditioned):\n  - Sensitivity matrix $$ \\mathbf{J}_A = \\begin{bmatrix} 0.8 & 0.1 \\\\ 0.0 & 1.0 \\\\ 1.2 & -0.1 \\\\ 0.5 & 0.2 \\end{bmatrix} $$.\n  - Noise standard deviation $\\sigma_A = 0.05$ (millimeters).\n  - Weak prior standard deviations $\\mathbf{S}_{p,A} = \\mathrm{diag}(100.0, 0.2)$ with $100.0$ in kilopascals (kPa) and $0.2$ dimensionless.\n\n- Case B (highly correlated parameters, ill-conditioned):\n  - Sensitivity matrix $$ \\mathbf{J}_B = \\begin{bmatrix} 1.0 & 0.99 \\\\ 0.9 & 0.89 \\\\ 1.1 & 1.09 \\\\ 1.05 & 1.04 \\end{bmatrix} $$.\n  - Noise standard deviation $\\sigma_B = 0.10$ (millimeters).\n  - Weak prior standard deviations $\\mathbf{S}_{p,B} = \\mathrm{diag}(100.0, 0.2)$ with $100.0$ in kilopascals (kPa) and $0.2$ dimensionless.\n\n- Case C (near rank-deficiency):\n  - Sensitivity matrix $$ \\mathbf{J}_C = \\begin{bmatrix} 10^{-4} & 1.0 \\\\ -10^{-4} & 1.0 \\\\ 2\\times 10^{-4} & 1.0 \\\\ -2\\times 10^{-4} & 1.0 \\end{bmatrix} $$.\n  - Noise standard deviation $\\sigma_C = 0.01$ (millimeters).\n  - Weak prior standard deviations $\\mathbf{S}_{p,C} = \\mathrm{diag}(100.0, 0.2)$ with $100.0$ in kilopascals (kPa) and $0.2$ dimensionless.\n\nComputational requirements:\n- For each case, compute $\\kappa$ and the posterior parameter standard deviations $s_{\\theta_1}$ (in kilopascals) and $s_{\\theta_2}$ (dimensionless).\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one case and is itself a list of the form $[\\kappa, s_{\\theta_1}, s_{\\theta_2}]$. For example, the output format must be $[[\\kappa_A, s_{\\theta_1,A}, s_{\\theta_2,A}],[\\kappa_B, s_{\\theta_1,B}, s_{\\theta_2,B}],[\\kappa_C, s_{\\theta_1,C}, s_{\\theta_2,C}]]$ with all numerical values printed in standard decimal notation (no scientific symbols), and each standard deviation rounded to exactly $6$ decimal places. The condition number should be printed as a standard floating-point number without units.",
            "solution": "The problem is validated as scientifically grounded, well-posed, and objective. All required data and definitions are provided, and there are no internal contradictions or violations of fundamental principles. The problem is a standard exercise in parameter identifiability and uncertainty quantification within a Bayesian linear model framework, a common task in biomechanical modeling.\n\nThe solution is approached in two main stages for each case study: first, an analysis of parameter identifiability using the Singular Value Decomposition (SVD) of the sensitivity matrix $\\mathbf{J}$, and second, the computation of parameter uncertainty via the posterior covariance matrix.\n\nLet the parameter vector be $\\boldsymbol{\\theta} = [\\theta_1, \\theta_2]^\\top$. The linearized measurement model is given by $\\mathbf{y} \\approx \\mathbf{y}_{\\text{ref}} + \\mathbf{J}\\,\\Delta\\boldsymbol{\\theta} + \\boldsymbol{\\varepsilon}$, where $\\Delta\\boldsymbol{\\theta}$ is the deviation from a reference parameter vector, $\\mathbf{J}$ is the sensitivity matrix, and $\\boldsymbol{\\varepsilon}$ is zero-mean Gaussian noise with covariance $\\mathbf{C}_{\\varepsilon} = \\sigma^2 \\mathbf{I}$.\n\nThe likelihood of observing the data given a parameter deviation $\\Delta\\boldsymbol{\\theta}$ is proportional to a Gaussian function:\n$$ p(\\mathbf{y} | \\Delta\\boldsymbol{\\theta}) \\propto \\exp\\left( -\\frac{1}{2} (\\mathbf{J}\\Delta\\boldsymbol{\\theta} - \\Delta\\mathbf{y})^\\top \\mathbf{C}_{\\varepsilon}^{-1} (\\mathbf{J}\\Delta\\boldsymbol{\\theta} - \\Delta\\mathbf{y}) \\right) $$\nwhere $\\Delta\\mathbf{y} = \\mathbf{y} - \\mathbf{y}_{\\text{ref}}$.\nThe parameters have a weak Gaussian prior distribution centered at zero deviation, $p(\\Delta\\boldsymbol{\\theta}) \\propto \\exp\\left( -\\frac{1}{2} \\Delta\\boldsymbol{\\theta}^\\top \\mathbf{C}_{p}^{-1} \\Delta\\boldsymbol{\\theta} \\right)$, where the prior covariance is $\\mathbf{C}_p = \\mathbf{S}_p^2 = \\mathrm{diag}(s_{\\theta_1}^2, s_{\\theta_2}^2)$.\n\nAccording to Bayes' theorem, the posterior distribution $p(\\Delta\\boldsymbol{\\theta} | \\mathbf{y})$ is proportional to the product of the likelihood and the prior. Since both are Gaussian, the posterior is also Gaussian. The inverse of the posterior covariance matrix, $\\mathbf{C}_{\\text{post}}^{-1}$, is the sum of the inverse covariance matrices from the likelihood's quadratic term and the prior's quadratic term:\n$$ \\mathbf{C}_{\\text{post}}^{-1} = \\mathbf{J}^\\top \\mathbf{C}_{\\varepsilon}^{-1} \\mathbf{J} + \\mathbf{C}_{p}^{-1} $$\nSubstituting $\\mathbf{C}_{\\varepsilon}^{-1} = \\frac{1}{\\sigma^2}\\mathbf{I}$, we get:\n$$ \\mathbf{C}_{\\text{post}} = \\left( \\frac{1}{\\sigma^2} \\mathbf{J}^\\top \\mathbf{J} + \\mathbf{C}_{p}^{-1} \\right)^{-1} $$\n\nThe posterior parameter variances are the diagonal elements of $\\mathbf{C}_{\\text{post}}$, and the standard deviations are their square roots.\n\nThe identifiability analysis will be conducted using the SVD of the sensitivity matrix, $\\mathbf{J} = \\mathbf{U}\\boldsymbol{\\Sigma}\\mathbf{V}^\\top$, where $\\mathbf{U}$ and $\\mathbf{V}$ are orthonormal matrices, and $\\boldsymbol{\\Sigma}$ is a rectangular diagonal matrix containing the non-negative singular values $s_1 \\ge s_2 \\ge \\dots \\ge 0$. The term $\\mathbf{J}^\\top\\mathbf{J}$ can be expressed using the SVD components as $\\mathbf{J}^\\top\\mathbf{J} = \\mathbf{V}\\boldsymbol{\\Sigma}^\\top\\boldsymbol{\\Sigma}\\mathbf{V}^\\top$. For a $2$-parameter problem, $\\boldsymbol{\\Sigma}^\\top\\boldsymbol{\\Sigma} = \\mathrm{diag}(s_1^2, s_2^2)$.\n\nThe singular values represent the sensitivity of the measurements to changes in the parameters along the directions defined by the columns of $\\mathbf{V}$ (the right singular vectors). A very small singular value indicates that the measurements are insensitive to parameter changes in the corresponding direction, leading to poor identifiability. The condition number, $\\kappa = s_1/s_2$ (for $s_2 > 0$), quantifies this disparity. A large $\\kappa$ signifies ill-conditioning, where at least one combination of parameters is poorly determined by the data.\n\nWe now apply this framework to each case.\n\n**Case A: Well-conditioned**\n- $\\mathbf{J}_A = \\begin{bmatrix} 0.8 & 0.1 \\\\ 0.0 & 1.0 \\\\ 1.2 & -0.1 \\\\ 0.5 & 0.2 \\end{bmatrix}$, $\\sigma_A = 0.05$, $\\mathbf{S}_{p,A} = \\mathrm{diag}(100.0, 0.2)$.\n1.  **SVD and Condition Number**:\n    The SVD of $\\mathbf{J}_A$ yields singular values $s_1 \\approx 1.5307$ and $s_2 \\approx 1.0450$. These values are of similar magnitude.\n    The condition number is $\\kappa_A = s_1/s_2 \\approx 1.5307 / 1.0450 \\approx 1.4647$. This low value indicates a well-conditioned problem.\n2.  **Posterior Covariance**:\n    The prior covariance matrix is $\\mathbf{C}_{p,A} = \\mathrm{diag}(100^2, 0.2^2) = \\mathrm{diag}(10000, 0.04)$, so $\\mathbf{C}_{p,A}^{-1} = \\mathrm{diag}(0.0001, 25)$.\n    We compute $\\mathbf{J}_A^\\top \\mathbf{J}_A = \\begin{bmatrix} 2.33 & 0.11 \\\\ 0.11 & 1.06 \\end{bmatrix}$.\n    The inverse posterior covariance is $\\mathbf{C}_{\\text{post},A}^{-1} = \\frac{1}{0.05^2} \\mathbf{J}_A^\\top \\mathbf{J}_A + \\mathbf{C}_{p,A}^{-1} = 400 \\begin{bmatrix} 2.33 & 0.11 \\\\ 0.11 & 1.06 \\end{bmatrix} + \\begin{bmatrix} 0.0001 & 0 \\\\ 0 & 25 \\end{bmatrix} = \\begin{bmatrix} 932.0001 & 44 \\\\ 44 & 449 \\end{bmatrix}$.\n    Inverting this gives the posterior covariance $\\mathbf{C}_{\\text{post},A} \\approx \\begin{bmatrix} 0.001075 & -0.000105 \\\\ -0.000105 & 0.002231 \\end{bmatrix}$.\n    The posterior variances are the diagonal elements: $\\mathrm{var}(\\Delta\\theta_1) \\approx 0.001075$ and $\\mathrm{var}(\\Delta\\theta_2) \\approx 0.002231$.\n    The standard deviations are $s_{\\theta_1,A} = \\sqrt{0.001075} \\approx 0.032784$ kPa and $s_{\\theta_2,A} = \\sqrt{0.002231} \\approx 0.047236$.\n\n**Case B: Highly correlated parameters, ill-conditioned**\n- $\\mathbf{J}_B = \\begin{bmatrix} 1.0 & 0.99 \\\\ 0.9 & 0.89 \\\\ 1.1 & 1.09 \\\\ 1.05 & 1.04 \\end{bmatrix}$, $\\sigma_B = 0.10$, $\\mathbf{S}_{p,B} = \\mathrm{diag}(100.0, 0.2)$.\n1.  **SVD and Condition Number**:\n    The columns of $\\mathbf{J}_B$ are nearly linearly dependent. The SVD yields singular values $s_1 \\approx 2.8466$ and $s_2 \\approx 0.0200$.\n    The condition number is $\\kappa_B = s_1/s_2 \\approx 2.8466 / 0.0200 \\approx 142.37$. This high value indicates an ill-conditioned problem, where the measurement is sensitive to one combination of parameters but insensitive to another.\n2.  **Posterior Covariance**:\n    The prior is the same as in Case A.\n    We compute $\\mathbf{J}_B^\\top \\mathbf{J}_B = \\begin{bmatrix} 4.1225 & 4.081 \\\\ 4.081 & 4.04 \\end{bmatrix}$.\n    The inverse posterior covariance is $\\mathbf{C}_{\\text{post},B}^{-1} = \\frac{1}{0.10^2} \\mathbf{J}_B^\\top \\mathbf{J}_B + \\mathbf{C}_{p,B}^{-1} = 100 \\begin{bmatrix} 4.1225 & 4.081 \\\\ 4.081 & 4.04 \\end{bmatrix} + \\begin{bmatrix} 0.0001 & 0 \\\\ 0 & 25 \\end{bmatrix} = \\begin{bmatrix} 412.2501 & 408.1 \\\\ 408.1 & 429 \\end{bmatrix}$.\n    Inverting gives $\\mathbf{C}_{\\text{post},B} \\approx \\begin{bmatrix} 0.040188 & -0.038228 \\\\ -0.038228 & 0.038618 \\end{bmatrix}$.\n    The posterior variances are $\\mathrm{var}(\\Delta\\theta_1) \\approx 0.040188$ and $\\mathrm{var}(\\Delta\\theta_2) \\approx 0.038618$.\n    The standard deviations are $s_{\\theta_1,B} = \\sqrt{0.040188} \\approx 0.200469$ kPa and $s_{\\theta_2,B} = \\sqrt{0.038618} \\approx 0.196515$. The high condition number correctly corresponds to larger posterior uncertainties compared to the well-conditioned case, even though the noise level also increased.\n\n**Case C: Near rank-deficiency**\n- $\\mathbf{J}_C = \\begin{bmatrix} 10^{-4} & 1.0 \\\\ -10^{-4} & 1.0 \\\\ 2\\times 10^{-4} & 1.0 \\\\ -2\\times 10^{-4} & 1.0 \\end{bmatrix}$, $\\sigma_C = 0.01$, $\\mathbf{S}_{p,C} = \\mathrm{diag}(100.0, 0.2)$.\n1.  **SVD and Condition Number**:\n    The first column of $\\mathbf{J}_C$ has entries of very small magnitude compared to the second column, indicating very low sensitivity to $\\theta_1$.\n    The SVD yields singular values $s_1 \\approx 2.0000$ and $s_2 \\approx 0.000316$.\n    The condition number is $\\kappa_C = s_1/s_2 \\approx 2.0000 / 0.000316 \\approx 6324.56$. This extremely high value signifies severe ill-conditioning.\n2.  **Posterior Covariance**:\n    The prior is the same as in Cases A and B.\n    We compute $\\mathbf{J}_C^\\top \\mathbf{J}_C = \\begin{bmatrix} 10^{-7} & 0 \\\\ 0 & 4 \\end{bmatrix}$.\n    The inverse posterior covariance is $\\mathbf{C}_{\\text{post},C}^{-1} = \\frac{1}{0.01^2} \\mathbf{J}_C^\\top \\mathbf{J}_C + \\mathbf{C}_{p,C}^{-1} = 10000 \\begin{bmatrix} 10^{-7} & 0 \\\\ 0 & 4 \\end{bmatrix} + \\begin{bmatrix} 0.0001 & 0 \\\\ 0 & 25 \\end{bmatrix} = \\begin{bmatrix} 0.0011 & 0 \\\\ 0 & 40025 \\end{bmatrix}$.\n    Inverting this diagonal matrix gives the posterior covariance $\\mathbf{C}_{\\text{post},C} \\approx \\begin{bmatrix} 909.0909 & 0 \\\\ 0 & 0.00002498 \\end{bmatrix}$.\n    The posterior variances are $\\mathrm{var}(\\Delta\\theta_1) \\approx 909.0909$ and $\\mathrm{var}(\\Delta\\theta_2) \\approx 0.00002498$.\n    The standard deviations are $s_{\\theta_1,C} = \\sqrt{909.0909} \\approx 30.151134$ kPa and $s_{\\theta_2,C} = \\sqrt{0.00002498} \\approx 0.004998$.\n    The huge uncertainty in $\\theta_1$ is a direct consequence of the low sensitivity (small singular value). Its posterior uncertainty is smaller than the prior ($30.15 < 100$), but still very large. In contrast, $\\theta_2$ is identified with high precision because of high sensitivity (large singular value) and low measurement noise.",
            "answer": "[[1.4647355554628867,0.032784,0.047236],[142.37059124430485,0.200469,0.196515],[6324.555320336758,30.151134,0.004998]]"
        }
    ]
}