{
    "hands_on_practices": [
        {
            "introduction": "A crucial first step in creating a subject-specific model is accurately aligning a generic template with the patient's unique anatomy captured in medical images. This process, known as registration, is often achieved by matching corresponding anatomical landmarks. This practice delves into the mathematical foundation of rigid registration by guiding you through the derivation and implementation of the Orthogonal Procrustes algorithm, a robust method for finding the optimal rotation and translation between two point sets. ",
            "id": "4207164",
            "problem": "A subject-specific model-to-image initialization requires aligning a set of $3$-dimensional anatomical landmarks extracted from medical imaging to a template model via a rigid transformation. A rigid transformation maps each point $x \\in \\mathbb{R}^3$ to $R x + t$, where $R \\in \\mathbb{R}^{3 \\times 3}$ is a rotation matrix satisfying $R^\\top R = I$ and $\\det(R) = 1$ (a proper rotation in the Special Orthogonal Group) and $t \\in \\mathbb{R}^3$ is a translation vector. Given two corresponding point sets $A = \\{a_i\\}_{i=1}^N$ and $B = \\{b_i\\}_{i=1}^N$ in $\\mathbb{R}^3$, representing the same anatomical locations in model and image spaces respectively, the optimal rigid transformation minimizes the sum of squared distances between transformed points and their correspondences. The goal is to derive, from first principles and well-tested linear algebraic facts, the Singular Value Decomposition (SVD)-based solution that estimates $R$ and $t$, and then quantify the residual alignment error.\n\nYour program must:\n- Derive the optimal rigid transform $(R, t)$ using a principled approach grounded in the definition of a rigid transformation and the properties of orthogonal matrices and Singular Value Decomposition (SVD).\n- Compute the residual alignment error as the root-mean-square (RMS) of Euclidean distances between $R a_i + t$ and $b_i$, expressed in meters.\n- Ensure angles are treated in radians and all physical distances are in meters.\n\nFundamental base for derivation:\n- Rigid body motion preserves distances; $R$ must satisfy $R^\\top R = I$ and $\\det(R) = 1$.\n- The least-squares objective is $\\sum_{i=1}^N \\|R a_i + t - b_i\\|^2$.\n- Use properties of centroids, covariance, and Singular Value Decomposition (SVD) to construct the optimal solution without invoking any shortcut formulas.\n\nTest suite specification (use pseudorandom number generator seeded with $42$ for reproducibility):\n- Case $1$ (general happy path):\n  - Number of points $N = 20$.\n  - Source points sampled uniformly from a box with each coordinate in $[-0.05, 0.05]$ meters.\n  - Rotation defined by axis $u = \\frac{1}{\\sqrt{3}}[1, 1, 1]^\\top$ and angle $\\theta = 0.5$ radians.\n  - Translation $t = [0.02, -0.01, 0.03]^\\top$ meters.\n  - Add independent Gaussian noise of standard deviation $\\sigma = 0.002$ meters to the transformed points.\n- Case $2$ (reflection edge case):\n  - Reuse the source points from Case $1$.\n  - Apply the same rotation and translation.\n  - Then reflect the transformed points across the $yz$-plane (multiply the $x$-coordinate by $-1$).\n  - Add independent Gaussian noise of standard deviation $\\sigma = 0.001$ meters.\n- Case $3$ (degenerate collinear configuration):\n  - Number of points $N = 10$ placed collinearly as $a_i = \\lambda_i d$ with $\\lambda_i$ uniformly spaced in $[-0.05, 0.05]$ and $d = \\frac{1}{\\sqrt{5}}[1, 2, 0]^\\top$.\n  - Rotation about the $z$-axis by angle $\\theta = 0.3$ radians.\n  - Translation $t = [0.0, 0.0, 0.0]^\\top$ meters.\n  - No noise, i.e., $\\sigma = 0.0$ meters.\n- Case $4$ (translation-only boundary):\n  - Number of points $N = 15$ sampled uniformly from a box with each coordinate in $[-0.05, 0.05]$ meters.\n  - Identity rotation (angle $\\theta = 0.0$ radians).\n  - Translation $t = [0.1, -0.05, 0.0]^\\top$ meters.\n  - No noise, i.e., $\\sigma = 0.0$ meters.\n- Case $5$ (minimal non-collinear set with noise):\n  - Three points forming a small triangle: $A = \\{[0.0, 0.0, 0.0]^\\top, [0.05, 0.0, 0.0]^\\top, [0.0, 0.03, 0.0]^\\top\\}$ meters.\n  - Rotation about the $y$-axis by angle $\\theta = \\pi/4$ radians.\n  - Translation $t = [0.0, 0.0, 0.0]^\\top$ meters.\n  - Add independent Gaussian noise of standard deviation $\\sigma = 0.005$ meters.\n\nImplementation requirements:\n- Construct the synthetic point sets exactly as specified.\n- Use the SVD-based orthogonal Procrustes approach to compute $R$ and $t$ under the constraint $\\det(R) = 1$.\n- Quantify the residual alignment error as an RMS value in meters for each case.\n- Angles must be in radians; distances must be in meters.\n\nFinal output format:\n- Your program should produce a single line of output containing the RMS alignment errors for the five test cases as a comma-separated list enclosed in square brackets, each value rounded to six decimal places in meters, for example, $[0.001234,0.005678,0.000000,0.000123,0.002345]$.",
            "solution": "The problem requires the derivation and implementation of an algorithm to find the optimal rigid transformation $(R, t)$ that aligns two corresponding sets of $3$-dimensional points, $A = \\{a_i\\}_{i=1}^N$ and $B = \\{b_i\\}_{i=1}^N$. The transformation is defined by $x \\mapsto R x + t$, where $R$ is a proper rotation matrix ($R^\\top R = I$, $\\det(R) = 1$) and $t$ is a translation vector. The optimality is defined in the least-squares sense, minimizing the objective function $E(R, t) = \\sum_{i=1}^N \\|R a_i + t - b_i\\|^2$. The solution must be derived from first principles using properties of linear algebra and Singular Value Decomposition (SVD).\n\n### Derivation from First Principles\n\n**1. Optimization with Respect to Translation $t$**\n\nThe objective function $E(R, t)$ is a quadratic function of the translation vector $t$. The minimum can be found by setting its gradient with respect to $t$ to zero.\n$$ \\nabla_t E(R, t) = \\nabla_t \\left( \\sum_{i=1}^N (R a_i + t - b_i)^\\top (R a_i + t - b_i) \\right) = 0 $$\nTaking the derivative inside the summation:\n$$ \\sum_{i=1}^N 2 (R a_i + t - b_i) = 0 $$\nDividing by $2$ and distributing the summation:\n$$ \\sum_{i=1}^N R a_i + \\sum_{i=1}^N t - \\sum_{i=1}^N b_i = 0 $$\n$$ R \\left(\\sum_{i=1}^N a_i\\right) + N t - \\sum_{i=1}^N b_i = 0 $$\nLet the centroids of the point sets be defined as $\\bar{a} = \\frac{1}{N} \\sum_{i=1}^N a_i$ and $\\bar{b} = \\frac{1}{N} \\sum_{i=1}^N b_i$. The equation becomes:\n$$ R (N \\bar{a}) + N t - N \\bar{b} = 0 $$\nSolving for $t$ yields the optimal translation vector as a function of the rotation $R$:\n$$ t = \\bar{b} - R \\bar{a} $$\nThis result shows that the optimal translation aligns the centroid of the transformed source points with the centroid of the target points.\n\n**2. Decoupling the Rotation Problem**\n\nWe substitute the expression for the optimal $t$ back into the objective function to obtain a problem solely in terms of $R$:\n$$ E(R) = \\sum_{i=1}^N \\|R a_i + (\\bar{b} - R \\bar{a}) - b_i\\|^2 $$\nBy rearranging terms, we can express the problem in terms of centered coordinates, $a'_i = a_i - \\bar{a}$ and $b'_i = b_i - \\bar{b}$:\n$$ E(R) = \\sum_{i=1}^N \\| (R a_i - R \\bar{a}) - (b_i - \\bar{b}) \\|^2 = \\sum_{i=1}^N \\| R (a_i - \\bar{a}) - (b_i - \\bar{b}) \\|^2 $$\n$$ E(R) = \\sum_{i=1}^N \\| R a'_i - b'_i \\|^2 $$\nNow, the task is to find the optimal rotation $R$ that minimizes the sum of squared distances between the centered point sets.\n\n**3. Optimization with Respect to Rotation $R$**\n\nWe expand the squared norm in the objective function:\n$$ E(R) = \\sum_{i=1}^N (R a'_i - b'_i)^\\top (R a'_i - b'_i) = \\sum_{i=1}^N \\left( (a'_i)^\\top R^\\top R a'_i - 2(a'_i)^\\top R^\\top b'_i + (b'_i)^\\top b'_i \\right) $$\nSince $R$ is a rotation matrix, $R^\\top R = I$. The expression simplifies to:\n$$ E(R) = \\sum_{i=1}^N \\left( \\|a'_i\\|^2 - 2(b'_i)^\\top R a'_i + \\|b'_i\\|^2 \\right) $$\nThe terms $\\sum \\|a'_i\\|^2$ and $\\sum \\|b'_i\\|^2$ are constant with respect to $R$. Therefore, minimizing $E(R)$ is equivalent to maximizing the term $\\sum_{i=1}^N (b'_i)^\\top R a'_i$.\n\nUsing the properties of the matrix trace (specifically, a scalar is its own trace, and the trace operator is cyclic, $\\text{tr}(XY) = \\text{tr}(YX)$), we can rewrite the quantity to be maximized:\n$$ \\sum_{i=1}^N (b'_i)^\\top R a'_i = \\sum_{i=1}^N \\text{tr}((b'_i)^\\top R a'_i) = \\sum_{i=1}^N \\text{tr}(R a'_i (b'_i)^\\top) $$\n$$ = \\text{tr}\\left(R \\sum_{i=1}^N a'_i (b'_i)^\\top\\right) $$\nLet's define the $3 \\times 3$ cross-covariance matrix $H$ as:\n$$ H = \\sum_{i=1}^N a'_i (b'_i)^\\top $$\nThe optimization problem is now reduced to:\n$$ \\underset{R}{\\text{maximize}} \\quad \\text{tr}(RH) \\quad \\text{subject to} \\quad R^\\top R = I, \\det(R) = 1 $$\n\n**4. The SVD-Based Solution**\n\nLet the Singular Value Decomposition (SVD) of $H$ be $H = U \\Sigma V^\\top$, where $U, V \\in \\mathbb{R}^{3 \\times 3}$ are orthogonal matrices and $\\Sigma = \\text{diag}(\\sigma_1, \\sigma_2, \\sigma_3)$ is a diagonal matrix of non-negative singular values, ordered such that $\\sigma_1 \\ge \\sigma_2 \\ge \\sigma_3 \\ge 0$.\n\nSubstituting the SVD of $H$ into the trace expression:\n$$ \\text{tr}(RH) = \\text{tr}(R U \\Sigma V^\\top) = \\text{tr}(\\Sigma V^\\top R U) $$\nLet $M = V^\\top R U$. Since $V$, $R$, and $U$ are orthogonal, $M$ is also an orthogonal matrix. The expression becomes $\\text{tr}(\\Sigma M) = \\sum_{i=1}^3 \\sigma_i M_{ii}$. As $M$ is orthogonal, its diagonal elements satisfy $|M_{ii}| \\le 1$. To maximize this sum, we should choose $M$ such that its diagonal elements are as large as possible. If there were no determinant constraint, the maximum would be achieved at $M=I$, which gives $V^\\top R U = I$, leading to $R = V U^\\top$.\n\nWe must, however, satisfy the proper rotation constraint $\\det(R)=1$.\n$$ \\det(R) = \\det(V U^\\top) = \\det(V) \\det(U^\\top) = \\det(V)\\det(U) $$\nThe determinants of orthogonal matrices $U$ and $V$ can be either $+1$ or $-1$.\n\nCase 1: $\\det(V)\\det(U) = 1$. In this case, $\\det(R)=1$, and the solution $R = V U^\\top$ is a proper rotation. This is the optimal solution.\n\nCase 2: $\\det(V)\\det(U) = -1$. In this scenario, $R = V U^\\top$ is an improper rotation (a reflection), with $\\det(R)=-1$. Our problem requires finding the proper rotation $R$ that maximizes $\\text{tr}(RH)$, which is equivalent to maximizing $\\text{tr}(\\Sigma M)$ subject to $M$ being an orthogonal matrix with $\\det(M) = \\det(V^\\top R U) = \\det(V^\\top)\\det(R)\\det(U) = (\\det(V)\\det(U))^{-1} \\det(R) = (-1)^{-1}(1) = -1$.\nWe need to maximize $\\sum \\sigma_i M_{ii}$ subject to $\\det(M) = -1$. Given $\\sigma_1 \\ge \\sigma_2 \\ge \\sigma_3 \\ge 0$, the maximum is achieved by setting $M_{11}=1$, $M_{22}=1$, and $M_{33}=-1$. This choice makes the smallest singular value contribute negatively, minimizing the loss from the maximum possible sum $\\sigma_1+\\sigma_2+\\sigma_3$.\nThe optimal choice for $M$ is $S = \\text{diag}(1, 1, -1)$.\nSetting $M=S$ gives $V^\\top R U = S$, which yields the corrected rotation:\n$$ R = V S U^\\top = V \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  -1 \\end{pmatrix} U^\\top $$\n\n**Summary of the Algorithm**\n1.  Compute the centroids of both point sets: $\\bar{a} = \\frac{1}{N}\\sum a_i$ and $\\bar{b} = \\frac{1}{N}\\sum b_i$.\n2.  Compute centered point sets: $A' = \\{a_i - \\bar{a}\\}$ and $B' = \\{b_i - \\bar{b}\\}$.\n3.  Compute the cross-covariance matrix $H = (A')^\\top B'$.\n4.  Compute the SVD of $H$: $H = U \\Sigma V^\\top$.\n5.  Compute the rotation matrix $R = V U^\\top$.\n6.  If $\\det(R)  0$, correct for reflection: $R = V \\text{diag}(1, 1, -1) U^\\top$.\n7.  Compute the translation vector $t = \\bar{b} - R \\bar{a}$.\n8.  The estimated transformation is $(R,t)$. The residual alignment error is calculated as the RMS error: $E_{RMS} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N \\|(R a_i + t) - b_i\\|^2}$.\n\nThis principled derivation provides a robust algorithm for rigid point set registration, which will now be implemented.",
            "answer": "```python\nimport numpy as np\n\ndef rodrigues_rotation_matrix(axis, angle):\n    \"\"\"Generates a 3x3 rotation matrix from an axis and an angle using Rodrigues' formula.\"\"\"\n    axis = np.asarray(axis)\n    axis = axis / np.linalg.norm(axis)\n    # Skew-symmetric matrix\n    K = np.array([[0, -axis[2], axis[1]],\n                      [axis[2], 0, -axis[0]],\n                      [-axis[1], axis[0], 0]])\n    R = np.identity(3) + np.sin(angle) * K + (1 - np.cos(angle)) * (K @ K)\n    return R\n\ndef rigid_align(A, B):\n    \"\"\"\n    Computes the optimal rigid transformation (R, t) to align point set A to B.\n    \n    Args:\n        A (np.ndarray): Source point set, shape (N, 3).\n        B (np.ndarray): Target point set, shape (N, 3).\n\n    Returns:\n        tuple: A tuple (R, t, rms_error) containing the rotation matrix,\n               translation vector, and the root-mean-square error.\n    \"\"\"\n    N = A.shape[0]\n    \n    # 1. Compute centroids\n    centroid_A = np.mean(A, axis=0)\n    centroid_B = np.mean(B, axis=0)\n\n    # 2. Compute centered point sets\n    A_centered = A - centroid_A\n    B_centered = B - centroid_B\n    \n    # 3. Compute cross-covariance matrix H\n    H = A_centered.T @ B_centered\n    \n    # 4. Compute SVD of H\n    U, s, Vh = np.linalg.svd(H)\n    V = Vh.T\n\n    # 5. Compute rotation matrix R\n    R = V @ U.T\n    \n    # 6. Check for reflection and correct if necessary\n    if np.linalg.det(R)  0:\n        # Correct R to be a proper rotation\n        V_corrected = V.copy()\n        V_corrected[:, -1] *= -1\n        R = V_corrected @ U.T\n\n    # 7. Compute translation vector t\n    t = centroid_B - R @ centroid_A\n    \n    # 8. Compute RMS error\n    A_transformed = (R @ A.T).T + t\n    diff = A_transformed - B\n    rms_error = np.sqrt(np.mean(np.sum(diff**2, axis=1)))\n    \n    return R, t, rms_error\n\n\ndef solve():\n    \"\"\"\n    Generates test cases, computes rigid alignment, and prints RMS errors.\n    \"\"\"\n    rng = np.random.default_rng(42)\n    results = []\n\n    # Case 1: General happy path\n    N1 = 20\n    sigma1 = 0.002\n    A1 = rng.uniform(-0.05, 0.05, (N1, 3))\n    R_true1 = rodrigues_rotation_matrix([1, 1, 1], 0.5)\n    t_true1 = np.array([0.02, -0.01, 0.03])\n    B1 = (R_true1 @ A1.T).T + t_true1 + rng.normal(0, sigma1, (N1, 3))\n    _, _, rms_error1 = rigid_align(A1, B1)\n    results.append(rms_error1)\n\n    # Case 2: Reflection edge case\n    # Reuse A1 and true transform from Case 1\n    N2 = N1\n    sigma2 = 0.001\n    B2_no_noise = (R_true1 @ A1.T).T + t_true1\n    # Reflect across the yz-plane (x - -x)\n    reflection_matrix = np.diag([-1, 1, 1])\n    B2_reflected = B2_no_noise @ reflection_matrix.T\n    B2 = B2_reflected + rng.normal(0, sigma2, (N2, 3))\n    _, _, rms_error2 = rigid_align(A1, B2)\n    results.append(rms_error2)\n    \n    # Case 3: Degenerate collinear configuration\n    N3 = 10\n    d3 = np.array([1, 2, 0]) / np.sqrt(5)\n    lambdas = np.linspace(-0.05, 0.05, N3)\n    A3 = np.outer(lambdas, d3)\n    R_true3 = rodrigues_rotation_matrix([0, 0, 1], 0.3)\n    t_true3 = np.array([0.0, 0.0, 0.0])\n    B3 = (R_true3 @ A3.T).T + t_true3\n    _, _, rms_error3 = rigid_align(A3, B3)\n    results.append(rms_error3)\n\n    # Case 4: Translation-only boundary\n    N4 = 15\n    A4 = rng.uniform(-0.05, 0.05, (N4, 3))\n    R_true4 = np.identity(3) # angle = 0.0\n    t_true4 = np.array([0.1, -0.05, 0.0])\n    B4 = (R_true4 @ A4.T).T + t_true4\n    _, _, rms_error4 = rigid_align(A4, B4)\n    results.append(rms_error4)\n\n    # Case 5: Minimal non-collinear set with noise\n    N5 = 3\n    sigma5 = 0.005\n    A5 = np.array([[0.0, 0.0, 0.0], [0.05, 0.0, 0.0], [0.0, 0.03, 0.0]])\n    R_true5 = rodrigues_rotation_matrix([0, 1, 0], np.pi/4)\n    t_true5 = np.array([0.0, 0.0, 0.0])\n    B5 = (R_true5 @ A5.T).T + t_true5 + rng.normal(0, sigma5, (N5, 3))\n    _, _, rms_error5 = rigid_align(A5, B5)\n    results.append(rms_error5)\n\n    # Format output\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Once a model is geometrically aligned, its material properties must be personalized to reflect the subject's specific tissue behavior. This is often an inverse problem where we tune parameters to match model outputs to measurements, but not all parameters may be uniquely determinable from the available data. This exercise introduces the concept of parameter identifiability, using Singular Value Decomposition (SVD) of the sensitivity matrix to diagnose which parameters or combinations of parameters can be confidently estimated from a given set of measurements. ",
            "id": "4207168",
            "problem": "You are modeling a subject-specific, two-parameter linearized biomechanical personalization problem from imaging-based measurements. The imaging modality provides displacement measurements that are modeled as differentiable functions of two material parameters: elastic modulus and Poisson ratio. After linearization around a reference parameter vector, the measurements can be approximated by a linear model with additive noise. Your goal is to quantify parameter identifiability using Singular Value Decomposition (SVD) of the sensitivity matrix and to compute the resulting parameter variances under a weak Gaussian prior.\n\nAssume the following setup:\n- Let the parameter vector be $\\boldsymbol{\\theta} = [\\theta_1,\\theta_2]^\\top$, where $\\theta_1$ is the elastic modulus in kilopascals (kPa) and $\\theta_2$ is the Poisson ratio (dimensionless).\n- Let the sensitivity matrix be $\\mathbf{J} \\in \\mathbb{R}^{m \\times 2}$, where $m$ is the number of independent displacement measurements from imaging. Each entry of $\\mathbf{J}$ represents a partial derivative of the predicted displacement (in millimeters) with respect to each parameter.\n- Let measurement noise be additive, zero-mean Gaussian with independent components of standard deviation $\\sigma$ in millimeters, so that the measurement model is $\\mathbf{y} \\approx \\mathbf{y}_{\\text{ref}} + \\mathbf{J}\\,\\Delta\\boldsymbol{\\theta} + \\boldsymbol{\\varepsilon}$ with $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I})$.\n- Assume a weak, independent Gaussian prior on parameters with standard deviations collected in a diagonal matrix $\\mathbf{S}_p = \\mathrm{diag}(s_{\\theta_1}, s_{\\theta_2})$, where $s_{\\theta_1}$ is in kilopascals (kPa) and $s_{\\theta_2}$ is dimensionless. This prior ensures numerical stability while remaining weak compared to the data.\n\nFundamental base:\n- Use the linearized observation model $\\mathbf{y} \\approx \\mathbf{y}_{\\text{ref}} + \\mathbf{J}\\,\\Delta\\boldsymbol{\\theta} + \\boldsymbol{\\varepsilon}$, additive independent Gaussian noise $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I})$, and the definition of Singular Value Decomposition (SVD) $\\mathbf{J} = \\mathbf{U}\\boldsymbol{\\Sigma}\\mathbf{V}^\\top$, where $\\mathbf{U}$ and $\\mathbf{V}$ are orthonormal and $\\boldsymbol{\\Sigma}$ contains nonnegative singular values.\n- Quantify identifiability through the spectrum of $\\boldsymbol{\\Sigma}$ and compute parameter variance using the posterior covariance derived from the Gaussian linear model with the weak Gaussian prior. You must explicitly use the SVD structure in your reasoning and computation. Do not rely on shortcut formulas or identities beyond these bases.\n\nTasks:\n1. For each test case below, compute the singular values of $\\mathbf{J}$ via SVD, sort them in descending order, and compute the condition number $\\kappa$ defined as the ratio of the largest singular value to the smallest nonzero singular value. If the smallest singular value is numerically zero (treat values less than $10^{-12}$ as zero), set $\\kappa$ to $+\\infty$.\n2. Using the weak Gaussian prior, compute the posterior covariance matrix of $\\Delta\\boldsymbol{\\theta}$ and extract the parameter standard deviations. You must express the elastic modulus standard deviation in kilopascals (kPa) and the Poisson ratio standard deviation as dimensionless. Round each standard deviation to exactly $6$ decimal places.\n3. Interpret the impact of small singular values on the parameter variances by comparing the condition numbers and the resulting standard deviations across cases. This interpretation should be reflected in your computed outputs; you do not need to print text beyond the specified output format.\n\nTest suite:\nProvide results for the following three cases. All numerical entities must be treated exactly as specified. In each case, $\\mathbf{J}$ is given as a real matrix with entries in millimeters per parameter unit.\n\n- Case A (well-conditioned):\n  - Sensitivity matrix $\\mathbf{J}_A = \\begin{bmatrix} 0.8  0.1 \\\\ 0.0  1.0 \\\\ 1.2  -0.1 \\\\ 0.5  0.2 \\end{bmatrix}$.\n  - Noise standard deviation $\\sigma_A = 0.05$ (millimeters).\n  - Weak prior standard deviations $\\mathbf{S}_{p,A} = \\mathrm{diag}(100.0, 0.2)$ with $100.0$ in kilopascals (kPa) and $0.2$ dimensionless.\n\n- Case B (highly correlated parameters, ill-conditioned):\n  - Sensitivity matrix $\\mathbf{J}_B = \\begin{bmatrix} 1.0  0.99 \\\\ 0.9  0.89 \\\\ 1.1  1.09 \\\\ 1.05  1.04 \\end{bmatrix}$.\n  - Noise standard deviation $\\sigma_B = 0.10$ (millimeters).\n  - Weak prior standard deviations $\\mathbf{S}_{p,B} = \\mathrm{diag}(100.0, 0.2)$ with $100.0$ in kilopascals (kPa) and $0.2$ dimensionless.\n\n- Case C (near rank-deficiency):\n  - Sensitivity matrix $\\mathbf{J}_C = \\begin{bmatrix} 10^{-4}  1.0 \\\\ -10^{-4}  1.0 \\\\ 2\\times 10^{-4}  1.0 \\\\ -2\\times 10^{-4}  1.0 \\end{bmatrix}$.\n  - Noise standard deviation $\\sigma_C = 0.01$ (millimeters).\n  - Weak prior standard deviations $\\mathbf{S}_{p,C} = \\mathrm{diag}(100.0, 0.2)$ with $100.0$ in kilopascals (kPa) and $0.2$ dimensionless.\n\nComputational requirements:\n- For each case, compute $\\kappa$ and the posterior parameter standard deviations $s_{\\theta_1}$ (in kilopascals) and $s_{\\theta_2}$ (dimensionless).\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one case and is itself a list of the form $[\\kappa, s_{\\theta_1}, s_{\\theta_2}]$. For example, the output format must be $[[\\kappa_A, s_{\\theta_1,A}, s_{\\theta_2,A}],[\\kappa_B, s_{\\theta_1,B}, s_{\\theta_2,B}],[\\kappa_C, s_{\\theta_1,C}, s_{\\theta_2,C}]]$ with all numerical values printed in standard decimal notation (no scientific symbols), and each standard deviation rounded to exactly $6$ decimal places. The condition number should be printed as a standard floating-point number without units.",
            "solution": "The problem is validated as scientifically grounded, well-posed, and objective. All required data and definitions are provided, and there are no internal contradictions or violations of fundamental principles. The problem is a standard exercise in parameter identifiability and uncertainty quantification within a Bayesian linear model framework, a common task in biomechanical modeling.\n\nThe solution is approached in two main stages for each case study: first, an analysis of parameter identifiability using the Singular Value Decomposition (SVD) of the sensitivity matrix $\\mathbf{J}$, and second, the computation of parameter uncertainty via the posterior covariance matrix.\n\nLet the parameter vector be $\\boldsymbol{\\theta} = [\\theta_1, \\theta_2]^\\top$. The linearized measurement model is given by $\\mathbf{y} \\approx \\mathbf{y}_{\\text{ref}} + \\mathbf{J}\\,\\Delta\\boldsymbol{\\theta} + \\boldsymbol{\\varepsilon}$, where $\\Delta\\boldsymbol{\\theta}$ is the deviation from a reference parameter vector, $\\mathbf{J}$ is the sensitivity matrix, and $\\boldsymbol{\\varepsilon}$ is zero-mean Gaussian noise with covariance $\\mathbf{C}_{\\varepsilon} = \\sigma^2 \\mathbf{I}$.\n\nThe likelihood of observing the data given a parameter deviation $\\Delta\\boldsymbol{\\theta}$ is proportional to a Gaussian function:\n$$ p(\\mathbf{y} | \\Delta\\boldsymbol{\\theta}) \\propto \\exp\\left( -\\frac{1}{2} (\\mathbf{J}\\Delta\\boldsymbol{\\theta} - \\Delta\\mathbf{y})^\\top \\mathbf{C}_{\\varepsilon}^{-1} (\\mathbf{J}\\Delta\\boldsymbol{\\theta} - \\Delta\\mathbf{y}) \\right) $$\nwhere $\\Delta\\mathbf{y} = \\mathbf{y} - \\mathbf{y}_{\\text{ref}}$.\nThe parameters have a weak Gaussian prior distribution centered at zero deviation, $p(\\Delta\\boldsymbol{\\theta}) \\propto \\exp\\left( -\\frac{1}{2} \\Delta\\boldsymbol{\\theta}^\\top \\mathbf{C}_{p}^{-1} \\Delta\\boldsymbol{\\theta} \\right)$, where the prior covariance is $\\mathbf{C}_p = \\mathbf{S}_p^2 = \\mathrm{diag}(s_{\\theta_1}^2, s_{\\theta_2}^2)$.\n\nAccording to Bayes' theorem, the posterior distribution $p(\\Delta\\boldsymbol{\\theta} | \\mathbf{y})$ is proportional to the product of the likelihood and the prior. Since both are Gaussian, the posterior is also Gaussian. The inverse of the posterior covariance matrix, $\\mathbf{C}_{\\text{post}}^{-1}$, is the sum of the inverse covariance matrices from the likelihood's quadratic term and the prior's quadratic term:\n$$ \\mathbf{C}_{\\text{post}}^{-1} = \\mathbf{J}^\\top \\mathbf{C}_{\\varepsilon}^{-1} \\mathbf{J} + \\mathbf{C}_{p}^{-1} $$\nSubstituting $\\mathbf{C}_{\\varepsilon}^{-1} = \\frac{1}{\\sigma^2}\\mathbf{I}$, we get:\n$$ \\mathbf{C}_{\\text{post}} = \\left( \\frac{1}{\\sigma^2} \\mathbf{J}^\\top \\mathbf{J} + \\mathbf{C}_{p}^{-1} \\right)^{-1} $$\n\nThe posterior parameter variances are the diagonal elements of $\\mathbf{C}_{\\text{post}}$, and the standard deviations are their square roots.\n\nThe identifiability analysis will be conducted using the SVD of the sensitivity matrix, $\\mathbf{J} = \\mathbf{U}\\boldsymbol{\\Sigma}\\mathbf{V}^\\top$, where $\\mathbf{U}$ and $\\mathbf{V}$ are orthonormal matrices, and $\\boldsymbol{\\Sigma}$ is a rectangular diagonal matrix containing the non-negative singular values $s_1 \\ge s_2 \\ge \\dots \\ge 0$. The term $\\mathbf{J}^\\top\\mathbf{J}$ can be expressed using the SVD components as $\\mathbf{J}^\\top\\mathbf{J} = \\mathbf{V}\\boldsymbol{\\Sigma}^\\top\\boldsymbol{\\Sigma}\\mathbf{V}^\\top$. For a $2$-parameter problem, $\\boldsymbol{\\Sigma}^\\top\\boldsymbol{\\Sigma} = \\mathrm{diag}(s_1^2, s_2^2)$.\n\nThe singular values represent the sensitivity of the measurements to changes in the parameters along the directions defined by the columns of $\\mathbf{V}$ (the right singular vectors). A very small singular value indicates that the measurements are insensitive to parameter changes in the corresponding direction, leading to poor identifiability. The condition number, $\\kappa = s_1/s_2$ (for $s_2  0$), quantifies this disparity. A large $\\kappa$ signifies ill-conditioning, where at least one combination of parameters is poorly determined by the data.\n\nWe now apply this framework to each case.\n\n**Case A: Well-conditioned**\n- $\\mathbf{J}_A = \\begin{bmatrix} 0.8  0.1 \\\\ 0.0  1.0 \\\\ 1.2  -0.1 \\\\ 0.5  0.2 \\end{bmatrix}$, $\\sigma_A = 0.05$, $\\mathbf{S}_{p,A} = \\mathrm{diag}(100.0, 0.2)$.\n1.  **SVD and Condition Number**:\n    The SVD of $\\mathbf{J}_A$ yields singular values $s_1 \\approx 1.5307$ and $s_2 \\approx 1.0450$. These values are of similar magnitude.\n    The condition number is $\\kappa_A = s_1/s_2 \\approx 1.5307 / 1.0450 \\approx 1.4647$. This low value indicates a well-conditioned problem.\n2.  **Posterior Covariance**:\n    The prior covariance matrix is $\\mathbf{C}_{p,A} = \\mathrm{diag}(100^2, 0.2^2) = \\mathrm{diag}(10000, 0.04)$, so $\\mathbf{C}_{p,A}^{-1} = \\mathrm{diag}(0.0001, 25)$.\n    We compute $\\mathbf{J}_A^\\top \\mathbf{J}_A = \\begin{bmatrix} 2.33  0.11 \\\\ 0.11  1.06 \\end{bmatrix}$.\n    The inverse posterior covariance is $\\mathbf{C}_{\\text{post},A}^{-1} = \\frac{1}{0.05^2} \\mathbf{J}_A^\\top \\mathbf{J}_A + \\mathbf{C}_{p,A}^{-1} = 400 \\begin{bmatrix} 2.33  0.11 \\\\ 0.11  1.06 \\end{bmatrix} + \\begin{bmatrix} 0.0001  0 \\\\ 0  25 \\end{bmatrix} = \\begin{bmatrix} 932.0001  44 \\\\ 44  449 \\end{bmatrix}$.\n    Inverting this gives the posterior covariance $\\mathbf{C}_{\\text{post},A} \\approx \\begin{bmatrix} 0.001075  -0.000105 \\\\ -0.000105  0.002231 \\end{bmatrix}$.\n    The posterior variances are the diagonal elements: $\\mathrm{var}(\\Delta\\theta_1) \\approx 0.001075$ and $\\mathrm{var}(\\Delta\\theta_2) \\approx 0.002231$.\n    The standard deviations are $s_{\\theta_1,A} = \\sqrt{0.001075} \\approx 0.032784$ kPa and $s_{\\theta_2,A} = \\sqrt{0.002231} \\approx 0.047236$.\n\n**Case B: Highly correlated parameters, ill-conditioned**\n- $\\mathbf{J}_B = \\begin{bmatrix} 1.0  0.99 \\\\ 0.9  0.89 \\\\ 1.1  1.09 \\\\ 1.05  1.04 \\end{bmatrix}$, $\\sigma_B = 0.10$, $\\mathbf{S}_{p,B} = \\mathrm{diag}(100.0, 0.2)$.\n1.  **SVD and Condition Number**:\n    The columns of $\\mathbf{J}_B$ are nearly linearly dependent. The SVD yields singular values $s_1 \\approx 2.8466$ and $s_2 \\approx 0.0200$.\n    The condition number is $\\kappa_B = s_1/s_2 \\approx 2.8466 / 0.0200 \\approx 142.37$. This high value indicates an ill-conditioned problem, where the measurement is sensitive to one combination of parameters but insensitive to another.\n2.  **Posterior Covariance**:\n    The prior is the same as in Case A.\n    We compute $\\mathbf{J}_B^\\top \\mathbf{J}_B = \\begin{bmatrix} 4.1225  4.081 \\\\ 4.081  4.04 \\end{bmatrix}$.\n    The inverse posterior covariance is $\\mathbf{C}_{\\text{post},B}^{-1} = \\frac{1}{0.10^2} \\mathbf{J}_B^\\top \\mathbf{J}_B + \\mathbf{C}_{p,B}^{-1} = 100 \\begin{bmatrix} 4.1225  4.081 \\\\ 4.081  4.04 \\end{bmatrix} + \\begin{bmatrix} 0.0001  0 \\\\ 0  25 \\end{bmatrix} = \\begin{bmatrix} 412.2501  408.1 \\\\ 408.1  429 \\end{bmatrix}$.\n    Inverting gives $\\mathbf{C}_{\\text{post},B} \\approx \\begin{bmatrix} 0.040188  -0.038228 \\\\ -0.038228  0.038618 \\end{bmatrix}$.\n    The posterior variances are $\\mathrm{var}(\\Delta\\theta_1) \\approx 0.040188$ and $\\mathrm{var}(\\Delta\\theta_2) \\approx 0.038618$.\n    The standard deviations are $s_{\\theta_1,B} = \\sqrt{0.040188} \\approx 0.200469$ kPa and $s_{\\theta_2,B} = \\sqrt{0.038618} \\approx 0.196515$. The high condition number correctly corresponds to larger posterior uncertainties compared to the well-conditioned case, even though the noise level also increased.\n\n**Case C: Near rank-deficiency**\n- $\\mathbf{J}_C = \\begin{bmatrix} 10^{-4}  1.0 \\\\ -10^{-4}  1.0 \\\\ 2\\times 10^{-4}  1.0 \\\\ -2\\times 10^{-4}  1.0 \\end{bmatrix}$, $\\sigma_C = 0.01$, $\\mathbf{S}_{p,C} = \\mathrm{diag}(100.0, 0.2)$.\n1.  **SVD and Condition Number**:\n    The first column of $\\mathbf{J}_C$ has entries of very small magnitude compared to the second column, indicating very low sensitivity to $\\theta_1$.\n    The SVD yields singular values $s_1 \\approx 2.0000$ and $s_2 \\approx 0.000316$.\n    The condition number is $\\kappa_C = s_1/s_2 \\approx 2.0000 / 0.000316 \\approx 6324.56$. This extremely high value signifies severe ill-conditioning.\n2.  **Posterior Covariance**:\n    The prior is the same as in Cases A and B. The columns of $\\mathbf{J}_C$ are orthogonal.\n    We compute $\\mathbf{J}_C^\\top \\mathbf{J}_C = \\begin{bmatrix} 10^{-7}  0 \\\\ 0  4 \\end{bmatrix}$.\n    The inverse posterior covariance is $\\mathbf{C}_{\\text{post},C}^{-1} = \\frac{1}{0.01^2} \\mathbf{J}_C^\\top \\mathbf{J}_C + \\mathbf{C}_{p,C}^{-1} = 10000 \\begin{bmatrix} 10^{-7}  0 \\\\ 0  4 \\end{bmatrix} + \\begin{bmatrix} 0.0001  0 \\\\ 0  25 \\end{bmatrix} = \\begin{bmatrix} 0.0011  0 \\\\ 0  40025 \\end{bmatrix}$.\n    Inverting gives the posterior covariance matrix: $\\mathbf{C}_{\\text{post},C} \\approx \\begin{bmatrix} 909.0909  0 \\\\ 0  0.00002498 \\end{bmatrix}$.\n    The posterior variances are $\\mathrm{var}(\\Delta\\theta_1) \\approx 909.0909$ and $\\mathrm{var}(\\Delta\\theta_2) \\approx 0.00002498$.\n    The standard deviations are $s_{\\theta_1,C} = \\sqrt{909.0909} \\approx 30.151134$ kPa and $s_{\\theta_2,C} = \\sqrt{0.00002498} \\approx 0.004998$.\n    The huge uncertainty in $\\theta_1$ is a direct consequence of the low sensitivity (small singular value). Its posterior uncertainty is smaller than the prior ($30.15  100$), but still very large. In contrast, $\\theta_2$ is identified with high precision because of high sensitivity (large singular value) and low measurement noise.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes parameter identifiability metrics (condition number) and posterior\n    parameter standard deviations for three biomechanical personalization cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"name\": \"Case A\",\n            \"J\": np.array([\n                [0.8, 0.1],\n                [0.0, 1.0],\n                [1.2, -0.1],\n                [0.5, 0.2]\n            ]),\n            \"sigma\": 0.05,\n            \"s_p_diag\": np.array([100.0, 0.2])\n        },\n        {\n            \"name\": \"Case B\",\n            \"J\": np.array([\n                [1.0, 0.99],\n                [0.9, 0.89],\n                [1.1, 1.09],\n                [1.05, 1.04]\n            ]),\n            \"sigma\": 0.10,\n            \"s_p_diag\": np.array([100.0, 0.2])\n        },\n        {\n            \"name\": \"Case C\",\n            \"J\": np.array([\n                [1e-4, 1.0],\n                [-1e-4, 1.0],\n                [2e-4, 1.0],\n                [-2e-4, 1.0]\n            ]),\n            \"sigma\": 0.01,\n            \"s_p_diag\": np.array([100.0, 0.2])\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        J = case[\"J\"]\n        sigma = case[\"sigma\"]\n        s_p_diag = case[\"s_p_diag\"]\n\n        # Task 1: SVD and Condition Number\n        # U, s, Vt are computed, but only s is needed here.\n        # numpy's svd function returns singular values sorted in descending order.\n        singular_values = np.linalg.svd(J, compute_uv=False)\n        \n        s_max = singular_values[0]\n        s_min = singular_values[-1] # Smallest singular value\n\n        # Per problem spec, treat values  1e-12 as zero.\n        if s_min  1e-12:\n            kappa = np.inf\n        else:\n            kappa = s_max / s_min\n\n        # Task 2: Posterior Covariance and Standard Deviations\n        # Prior covariance matrix C_p = diag(s_p^2)\n        # Its inverse C_p_inv = diag(1/s_p^2)\n        C_p_inv = np.diag(1.0 / (s_p_diag**2))\n\n        # Fisher Information from data: (1/sigma^2) * J^T * J\n        JtJ = J.T @ J\n        \n        # Inverse of posterior covariance: C_post_inv = (1/sigma^2)J^T*J + C_p_inv\n        C_post_inv = (1.0 / sigma**2) * JtJ + C_p_inv\n\n        # Posterior covariance\n        C_post = np.linalg.inv(C_post_inv)\n\n        # Variances are the diagonal elements of the posterior covariance matrix.\n        variances = np.diag(C_post)\n        \n        # Standard deviations are the square root of variances.\n        posterior_stdevs = np.sqrt(variances)\n        s_theta1 = posterior_stdevs[0]\n        s_theta2 = posterior_stdevs[1]\n        \n        results.append(f\"[{kappa},{s_theta1:.6f},{s_theta2:.6f}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The final step in the modeling pipeline is validation, where the personalized model's predictions are quantitatively compared against independent experimental data. This ensures the model is not just a good fit to the calibration data but also a reliable predictive tool. This practice introduces the Bland-Altman analysis, a standard statistical method used in the medical and engineering sciences to assess the agreement between two different methods of measurement—in this case, a computational model and an in vivo imaging technique. ",
            "id": "4207157",
            "problem": "A subject-specific finite element model of the left ventricle was constructed from imaging, combining cine magnetic resonance imaging and Displacement Encoding with Stimulated Echoes (DENSE) magnetic resonance imaging. The DENSE magnetic resonance imaging provides a voxelwise displacement field, from which the Lagrangian circumferential strain $E_{cc}$ at end-systole is obtained via the standard continuum mechanics definition based on the deformation gradient $F$, namely the Green–Lagrange strain $E = \\frac{1}{2}\\left(F^{\\top}F - I\\right)$, aggregated over American Heart Association segments. The model predicts $E_{cc}$ using a personalized constitutive parameter set fitted to the same subject’s imaging data.\n\nFor $i = 1, \\dots, 12$ segments, the measured and predicted Lagrangian circumferential strains at end-systole are given as follows, where $E^{\\mathrm{DENSE}}_{cc,i}$ denotes the DENSE magnetic resonance imaging measurement and $E^{\\mathrm{model}}_{cc,i}$ the model prediction:\n$(E^{\\mathrm{DENSE}}_{cc,1}, E^{\\mathrm{model}}_{cc,1}) = (-0.18, -0.17)$, $(E^{\\mathrm{DENSE}}_{cc,2}, E^{\\mathrm{model}}_{cc,2}) = (-0.22, -0.23)$, $(E^{\\mathrm{DENSE}}_{cc,3}, E^{\\mathrm{model}}_{cc,3}) = (-0.16, -0.15)$, $(E^{\\mathrm{DENSE}}_{cc,4}, E^{\\mathrm{model}}_{cc,4}) = (-0.20, -0.19)$, $(E^{\\mathrm{DENSE}}_{cc,5}, E^{\\mathrm{model}}_{cc,5}) = (-0.25, -0.27)$, $(E^{\\mathrm{DENSE}}_{cc,6}, E^{\\mathrm{model}}_{cc,6}) = (-0.12, -0.11)$, $(E^{\\mathrm{DENSE}}_{cc,7}, E^{\\mathrm{model}}_{cc,7}) = (-0.19, -0.20)$, $(E^{\\mathrm{DENSE}}_{cc,8}, E^{\\mathrm{model}}_{cc,8}) = (-0.21, -0.22)$, $(E^{\\mathrm{DENSE}}_{cc,9}, E^{\\mathrm{model}}_{cc,9}) = (-0.15, -0.14)$, $(E^{\\mathrm{DENSE}}_{cc,10}, E^{\\mathrm{model}}_{cc,10}) = (-0.23, -0.24)$, $(E^{\\mathrm{DENSE}}_{cc,11}, E^{\\mathrm{model}}_{cc,11}) = (-0.17, -0.16)$, $(E^{\\mathrm{DENSE}}_{cc,12}, E^{\\mathrm{model}}_{cc,12}) = (-0.24, -0.26)$.\n\nPerform a Bland–Altman analysis comparing the model predictions to the DENSE magnetic resonance imaging measurements by computing the bias (mean difference) and the $95\\%$ limits of agreement for the differences defined as $\\Delta_i = E^{\\mathrm{model}}_{cc,i} - E^{\\mathrm{DENSE}}_{cc,i}$. Assume the differences are approximately normally distributed and use the conventional $1.96$ multiplier to form the $95\\%$ limits of agreement. Report the bias and the lower and upper limits of agreement as three dimensionless decimal numbers. Round your results to four significant figures. Express all quantities as dimensionless decimals.",
            "solution": "The problem statement is a valid exercise in the statistical comparison of a computational model's predictions with experimental measurements, a common task in the field of biomechanics and specifically in subject-specific modeling. The problem provides all necessary data and methodological constraints to arrive at a unique, verifiable solution.\n\nThe objective is to perform a Bland–Altman analysis to compare the model-predicted Lagrangian circumferential strain, $E^{\\mathrm{model}}_{cc}$, with the DENSE MRI-measured strain, $E^{\\mathrm{DENSE}}_{cc}$. This analysis requires the computation of the bias (mean difference) and the $95\\%$ limits of agreement.\n\nThe difference for each of the $N=12$ segments is defined as:\n$$\n\\Delta_i = E^{\\mathrm{model}}_{cc,i} - E^{\\mathrm{DENSE}}_{cc,i} \\quad \\text{for } i = 1, \\dots, 12\n$$\n\nFirst, we compute the individual differences, $\\Delta_i$, for each of the $12$ data pairs provided:\n1.  $\\Delta_1 = -0.17 - (-0.18) = +0.01$\n2.  $\\Delta_2 = -0.23 - (-0.22) = -0.01$\n3.  $\\Delta_3 = -0.15 - (-0.16) = +0.01$\n4.  $\\Delta_4 = -0.19 - (-0.20) = +0.01$\n5.  $\\Delta_5 = -0.27 - (-0.25) = -0.02$\n6.  $\\Delta_6 = -0.11 - (-0.12) = +0.01$\n7.  $\\Delta_7 = -0.20 - (-0.19) = -0.01$\n8.  $\\Delta_8 = -0.22 - (-0.21) = -0.01$\n9.  $\\Delta_9 = -0.14 - (-0.15) = +0.01$\n10. $\\Delta_{10} = -0.24 - (-0.23) = -0.01$\n11. $\\Delta_{11} = -0.16 - (-0.17) = +0.01$\n12. $\\Delta_{12} = -0.26 - (-0.24) = -0.02$\n\nThe set of differences is $\\{0.01, -0.01, 0.01, 0.01, -0.02, 0.01, -0.01, -0.01, 0.01, -0.01, 0.01, -0.02\\}$.\n\nThe bias is the mean of these differences, denoted by $\\bar{\\Delta}$:\n$$\n\\bar{\\Delta} = \\frac{1}{N} \\sum_{i=1}^{N} \\Delta_i\n$$\nThe sum of the differences is:\n$$\n\\sum_{i=1}^{12} \\Delta_i = 6(0.01) + 4(-0.01) + 2(-0.02) = 0.06 - 0.04 - 0.04 = -0.02\n$$\nThe bias is therefore:\n$$\n\\bar{\\Delta} = \\frac{-0.02}{12} = -\\frac{1}{600} \\approx -0.001666\\dots\n$$\n\nNext, we calculate the sample standard deviation of the differences, $s_{\\Delta}$. The sample variance, $s_{\\Delta}^2$, is given by:\n$$\ns_{\\Delta}^2 = \\frac{1}{N-1} \\sum_{i=1}^{N} (\\Delta_i - \\bar{\\Delta})^2\n$$\nTo compute this, we first find the sum of the squares of the differences, $\\sum_{i=1}^{N} \\Delta_i^2$:\n$$\n\\sum_{i=1}^{12} \\Delta_i^2 = 6(0.01)^2 + 4(-0.01)^2 + 2(-0.02)^2 = 6(0.0001) + 4(0.0001) + 2(0.0004) = 0.0006 + 0.0004 + 0.0008 = 0.0018\n$$\nUsing the computational formula for the sum of squared deviations:\n$$\n\\sum_{i=1}^{N} (\\Delta_i - \\bar{\\Delta})^2 = \\sum_{i=1}^{N} \\Delta_i^2 - \\frac{\\left(\\sum_{i=1}^{N} \\Delta_i\\right)^2}{N} = 0.0018 - \\frac{(-0.02)^2}{12} = 0.0018 - \\frac{0.0004}{12} = 0.0018 - 0.0000333\\dots = 0.0017666\\dots\n$$\nThe sample variance is:\n$$\ns_{\\Delta}^2 = \\frac{0.0017666\\dots}{12-1} = \\frac{0.0017666\\dots}{11} \\approx 0.000160606\\dots\n$$\nThe sample standard deviation is the square root of the variance:\n$$\ns_{\\Delta} = \\sqrt{0.000160606\\dots} \\approx 0.01267304\n$$\n\nThe $95\\%$ limits of agreement are defined as $\\bar{\\Delta} \\pm 1.96 s_{\\Delta}$. The problem specifies using the multiplier $1.96$.\nFirst, calculate the margin of agreement:\n$$\n1.96 \\times s_{\\Delta} = 1.96 \\times 0.01267304 \\approx 0.02483916\n$$\nThe lower limit of agreement, $L_{lower}$, is:\n$$\nL_{lower} = \\bar{\\Delta} - 1.96 s_{\\Delta} \\approx -0.00166667 - 0.02483916 = -0.02650583\n$$\nThe upper limit of agreement, $L_{upper}$, is:\n$$\nL_{upper} = \\bar{\\Delta} + 1.96 s_{\\Delta} \\approx -0.00166667 + 0.02483916 = 0.02317249\n$$\n\nFinally, we round the required quantities—bias, lower limit, and upper limit—to four significant figures.\nBias: $\\bar{\\Delta} \\approx -0.00166667 \\rightarrow -0.001667$\nLower Limit: $L_{lower} \\approx -0.02650583 \\rightarrow -0.02651$\nUpper Limit: $L_{upper} \\approx 0.02317249 \\rightarrow 0.02317$\n\nThe results for the Bland-Altman analysis are a bias of $-0.001667$, and $95\\%$ limits of agreement from $-0.02651$ to $0.02317$. The small bias indicates that the model has low systematic error compared to the DENSE measurements. The limits of agreement define the range within which $95\\%$ of the differences between the model and the measurement are expected to lie.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} -0.001667  -0.02651  0.02317 \\end{pmatrix}}\n$$"
        }
    ]
}