{
    "hands_on_practices": [
        {
            "introduction": "The likelihood function is the cornerstone of Bayesian parameter estimation, forming the bridge between a deterministic model's predictions and noisy experimental data. This foundational exercise guides you through deriving the likelihood for a common scenario in biomechanics: fitting a dynamic model to motion capture data assuming Gaussian measurement errors . Mastering this derivation is crucial for understanding how data informs our parameter estimates and for building more complex statistical models.",
            "id": "4157236",
            "problem": "Consider a forward dynamics musculoskeletal model that deterministically maps a parameter vector $\\theta$ to a continuous-time kinematic trajectory $x(t;\\theta)$ for a single scalar joint angle over a finite observation window $t \\in [0,T]$. Motion capture provides discrete-time measurements at sample times $t_{1}, t_{2}, \\dots, t_{N}$, yielding data $\\tilde{x}(t_{i})$ for $i=1,\\dots,N$. Assume an additive measurement model $\\tilde{x}(t_{i}) = x(t_{i};\\theta) + \\varepsilon_{i}$, where the measurement errors $\\varepsilon_{i}$ are independent across time points and follow a zero-mean Gaussian distribution with common variance $\\sigma^{2}$. The forward dynamics trajectory $x(t;\\theta)$ is the output of the biomechanical model given $\\theta$ and is assumed to be known deterministically once $\\theta$ is specified.\n\nUsing only foundational definitions from probability and Bayesian inference (specifically, the definition of a likelihood as the joint probability of the observed data given parameters, the independence of measurement errors, and the form of a Gaussian probability density function), derive the likelihood function for the entire dataset $\\{\\tilde{x}(t_{i})\\}_{i=1}^{N}$ as a function of $\\theta$ and $\\sigma^{2}$. Express your final result as a single closed-form analytic expression in terms of $N$, $\\sigma^{2}$, and the residuals between the measured data and the model-predicted trajectory at the sampled times, without introducing any additional parameters. No numerical evaluation is required. The final answer must be a single analytic expression. Do not include any units in your final expression.",
            "solution": "We begin from the generative measurement model for each discrete time point $t_{i}$:\n$$\n\\tilde{x}(t_{i}) \\;=\\; x(t_{i};\\theta) + \\varepsilon_{i},\n$$\nwhere $\\varepsilon_{i}$ are independent Gaussian random variables with mean $0$ and variance $\\sigma^{2}$. The independence assumption across time points implies that the joint probability of the observed data across all $N$ times is the product of the individual probabilities at each time point.\n\nIn Bayesian inference, the likelihood function is defined as the joint probability density of the observed data given the parameters. Let $\\{\\tilde{x}(t_{i})\\}_{i=1}^{N}$ denote the collection of observed measurements. Define the residuals\n$$\nr_{i}(\\theta) \\;=\\; \\tilde{x}(t_{i}) - x(t_{i};\\theta).\n$$\nUnder the additive Gaussian noise model, each residual $r_{i}(\\theta)$ is a realization from a Gaussian distribution with mean $0$ and variance $\\sigma^{2}$. The probability density function (PDF) of a zero-mean Gaussian random variable with variance $\\sigma^{2}$ evaluated at $r_{i}(\\theta)$ is\n$$\n\\frac{1}{\\sqrt{2\\pi \\sigma^{2}}} \\exp\\!\\left( -\\frac{r_{i}(\\theta)^{2}}{2\\sigma^{2}} \\right).\n$$\nBecause the errors are independent across time points, the joint PDF of all residuals is the product of the individual PDFs. Therefore, the likelihood function for $\\theta$ and $\\sigma^{2}$ given the dataset $\\{\\tilde{x}(t_{i})\\}_{i=1}^{N}$ is\n$$\nL(\\theta,\\sigma^{2} \\mid \\{\\tilde{x}(t_{i})\\}_{i=1}^{N}) \\;=\\; \\prod_{i=1}^{N} \\frac{1}{\\sqrt{2\\pi \\sigma^{2}}} \\exp\\!\\left( -\\frac{(\\tilde{x}(t_{i}) - x(t_{i};\\theta))^{2}}{2\\sigma^{2}} \\right).\n$$\nWe can simplify this product by collecting terms. The product of the normalization constants becomes\n$$\n\\prod_{i=1}^{N} \\frac{1}{\\sqrt{2\\pi \\sigma^{2}}} \\;=\\; (2\\pi \\sigma^{2})^{-N/2}.\n$$\nThe product of the exponentials becomes the exponential of the sum of exponents:\n$$\n\\prod_{i=1}^{N} \\exp\\!\\left( -\\frac{(\\tilde{x}(t_{i}) - x(t_{i};\\theta))^{2}}{2\\sigma^{2}} \\right) \\;=\\; \\exp\\!\\left( -\\frac{1}{2\\sigma^{2}} \\sum_{i=1}^{N} (\\tilde{x}(t_{i}) - x(t_{i};\\theta))^{2} \\right).\n$$\nThus, the closed-form likelihood is\n$$\nL(\\theta,\\sigma^{2} \\mid \\{\\tilde{x}(t_{i})\\}_{i=1}^{N}) \\;=\\; (2\\pi \\sigma^{2})^{-N/2} \\exp\\!\\left( -\\frac{1}{2\\sigma^{2}} \\sum_{i=1}^{N} \\left(\\tilde{x}(t_{i}) - x(t_{i};\\theta)\\right)^{2} \\right).\n$$\nThis expression depends only on $N$, $\\sigma^{2}$, and the residuals between the motion capture data and the model-predicted trajectory evaluated at the sampled times, as required.",
            "answer": "$$\\boxed{(2\\pi\\sigma^{2})^{-N/2}\\,\\exp\\!\\left(-\\frac{1}{2\\sigma^{2}}\\sum_{i=1}^{N}\\bigl(\\tilde{x}(t_{i})-x(t_{i};\\theta)\\bigr)^{2}\\right)}$$"
        },
        {
            "introduction": "Successful parameter estimation involves not just computing a posterior distribution, but also interpreting it to understand what the data can and cannot tell us. This exercise explores the critical concept of parameter identifiability through the lens of the posterior covariance matrix . By analyzing a model with nearly collinear parameter effects, you will see how strong correlations in the posterior reveal fundamental limitations in the model or experimental design, a crucial skill for any modeling endeavor.",
            "id": "4157241",
            "problem": "A single-degree-of-freedom elbow flexion experiment is performed under quasi-static conditions so that dynamic effects are negligible. The net joint torque $y_i$ at trial $i$ is modeled as the sum of muscle contributions proportional to known functions of joint angle and muscle-tendon geometry, yielding a linearized observation model in two dimensionless parameters: a normalized activation gain $\\theta_1$ and a normalized series-elastic stiffness scaling $\\theta_2$. Under small variations about a fixed posture, the torque is modeled as\n$$\ny_i \\approx h_{i1}\\,\\theta_1 + h_{i2}\\,\\theta_2 + \\varepsilon_i,\n$$\nwhere $\\varepsilon_i$ represents unmodeled variability and sensor noise. The coefficients $h_{i1}$ and $h_{i2}$ are determined by moment arms and the local sensitivity of the muscle-tendon unit force to $\\theta_1$ and $\\theta_2$, respectively.\n\nAssume three trials with design matrix\n$$\nX=\\begin{pmatrix}\nh_{11} & h_{12} \\\\\nh_{21} & h_{22} \\\\\nh_{31} & h_{32}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1.00 & 0.98 \\\\\n0.50 & 0.49 \\\\\n0.20 & 0.21\n\\end{pmatrix},\n$$\nand observed torques\n$$\ny=\\begin{pmatrix}\n1.49 \\\\\n0.73 \\\\\n0.32\n\\end{pmatrix}.\n$$\nAssume independent and identically distributed additive noise with $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2 I)$, where $\\sigma^2=0.50$, and an independent Gaussian prior on the parameters $\\theta=\\begin{pmatrix}\\theta_1 \\\\ \\theta_2\\end{pmatrix}$ with $\\theta \\sim \\mathcal{N}(\\mu_0,\\Sigma_0)$, where $\\mu_0=\\begin{pmatrix}0 \\\\ 0\\end{pmatrix}$ and $\\Sigma_0=\\operatorname{diag}(10,10)$. Using Bayes’ theorem and standard properties of Gaussian distributions, derive the posterior distribution $p(\\theta \\mid y)$ and compute the posterior covariance between $\\theta_1$ and $\\theta_2$ (i.e., the off-diagonal entry of the $2\\times 2$ posterior covariance matrix). Explain how the sign and magnitude of this covariance reveal identifiability issues in the model given the near-collinearity in $X$.\n\nRound your final numerical answer for the posterior covariance to four significant figures. Report it as a pure number (dimensionless).",
            "solution": "The problem requires the derivation of the posterior distribution for the parameters of a linear biomechanical model and the computation of the posterior covariance between them. The model is given by the equation $y = X\\theta + \\varepsilon$, where $y$ is the vector of observed torques, $X$ is the design matrix, $\\theta$ is the vector of parameters, and $\\varepsilon$ is the noise vector.\n\nThis is a standard Bayesian linear regression problem. The components of the Bayesian model are:\n1.  **Likelihood**: The noise is assumed to be Gaussian, $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$. This implies that the likelihood of the data given the parameters is also Gaussian:\n    $$\n    p(y \\mid \\theta) = \\mathcal{N}(X\\theta, \\sigma^2 I) \\propto \\exp\\left( -\\frac{1}{2\\sigma^2}(y - X\\theta)^T (y - X\\theta) \\right)\n    $$\n2.  **Prior**: The parameters are assumed to have a Gaussian prior distribution:\n    $$\n    p(\\theta) = \\mathcal{N}(\\mu_0, \\Sigma_0) \\propto \\exp\\left( -\\frac{1}{2}(\\theta - \\mu_0)^T \\Sigma_0^{-1} (\\theta - \\mu_0) \\right)\n    $$\n\nAccording to Bayes' theorem, the posterior distribution is proportional to the product of the likelihood and the prior: $p(\\theta \\mid y) \\propto p(y \\mid \\theta)p(\\theta)$. Since the likelihood and prior are both Gaussian, the posterior distribution is also Gaussian, $p(\\theta \\mid y) = \\mathcal{N}(\\mu_N, \\Sigma_N)$.\n\nThe posterior covariance matrix $\\Sigma_N$ and posterior mean vector $\\mu_N$ are given by the standard formulas for Bayesian linear regression:\n$$\n\\Sigma_N = \\left(\\Sigma_0^{-1} + \\frac{1}{\\sigma^2} X^T X \\right)^{-1}\n$$\n$$\n\\mu_N = \\Sigma_N \\left(\\Sigma_0^{-1} \\mu_0 + \\frac{1}{\\sigma^2} X^T y \\right)\n$$\nThe problem asks for the posterior covariance between $\\theta_1$ and $\\theta_2$, which is the off-diagonal element of the posterior covariance matrix $\\Sigma_N$. We will now compute this matrix.\n\nThe given values are:\n- Design matrix: $X = \\begin{pmatrix} 1.00 & 0.98 \\\\ 0.50 & 0.49 \\\\ 0.20 & 0.21 \\end{pmatrix}$\n- Observed torques: $y = \\begin{pmatrix} 1.49 \\\\ 0.73 \\\\ 0.32 \\end{pmatrix}$ (This is not needed for the covariance calculation)\n- Noise variance: $\\sigma^2 = 0.50$\n- Prior mean: $\\mu_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$\n- Prior covariance: $\\Sigma_0 = \\begin{pmatrix} 10 & 0 \\\\ 0 & 10 \\end{pmatrix}$\n\nFirst, we calculate the inverse of the prior covariance matrix, $\\Sigma_0^{-1}$:\n$$\n\\Sigma_0^{-1} = \\begin{pmatrix} 10 & 0 \\\\ 0 & 10 \\end{pmatrix}^{-1} = \\begin{pmatrix} \\frac{1}{10} & 0 \\\\ 0 & \\frac{1}{10} \\end{pmatrix} = \\begin{pmatrix} 0.1 & 0 \\\\ 0 & 0.1 \\end{pmatrix}\n$$\n\nNext, we compute the matrix product $X^T X$:\n$$\nX^T X = \\begin{pmatrix} 1.00 & 0.50 & 0.20 \\\\ 0.98 & 0.49 & 0.21 \\end{pmatrix} \\begin{pmatrix} 1.00 & 0.98 \\\\ 0.50 & 0.49 \\\\ 0.20 & 0.21 \\end{pmatrix}\n$$\nThe elements are:\n- $(X^T X)_{11} = (1.00)^2 + (0.50)^2 + (0.20)^2 = 1.00 + 0.25 + 0.04 = 1.29$\n- $(X^T X)_{12} = (1.00)(0.98) + (0.50)(0.49) + (0.20)(0.21) = 0.98 + 0.245 + 0.042 = 1.267$\n- $(X^T X)_{21} = (X^T X)_{12} = 1.267$\n- $(X^T X)_{22} = (0.98)^2 + (0.49)^2 + (0.21)^2 = 0.9604 + 0.2401 + 0.0441 = 1.2446$\nSo,\n$$\nX^T X = \\begin{pmatrix} 1.29 & 1.267 \\\\ 1.267 & 1.2446 \\end{pmatrix}\n$$\n\nNow, we compute the term $\\frac{1}{\\sigma^2} X^T X$, which is the precision matrix of the likelihood:\n$$\n\\frac{1}{\\sigma^2} X^T X = \\frac{1}{0.50} \\begin{pmatrix} 1.29 & 1.267 \\\\ 1.267 & 1.2446 \\end{pmatrix} = 2 \\begin{pmatrix} 1.29 & 1.267 \\\\ 1.267 & 1.2446 \\end{pmatrix} = \\begin{pmatrix} 2.58 & 2.534 \\\\ 2.534 & 2.4892 \\end{pmatrix}\n$$\n\nThe posterior precision matrix, $\\Sigma_N^{-1}$, is the sum of the prior precision and the likelihood precision:\n$$\n\\Sigma_N^{-1} = \\Sigma_0^{-1} + \\frac{1}{\\sigma^2} X^T X = \\begin{pmatrix} 0.1 & 0 \\\\ 0 & 0.1 \\end{pmatrix} + \\begin{pmatrix} 2.58 & 2.534 \\\\ 2.534 & 2.4892 \\end{pmatrix} = \\begin{pmatrix} 2.68 & 2.534 \\\\ 2.534 & 2.5892 \\end{pmatrix}\n$$\n\nFinally, we find the posterior covariance matrix $\\Sigma_N$ by inverting the posterior precision matrix $\\Sigma_N^{-1}$. For a $2 \\times 2$ matrix $A = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$, its inverse is $A^{-1} = \\frac{1}{ad-bc} \\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix}$.\nThe determinant of $\\Sigma_N^{-1}$ is:\n$$\n\\det(\\Sigma_N^{-1}) = (2.68)(2.5892) - (2.534)^2 = 6.939056 - 6.421156 = 0.5179\n$$\nNow, we compute the inverse:\n$$\n\\Sigma_N = \\frac{1}{0.5179} \\begin{pmatrix} 2.5892 & -2.534 \\\\ -2.534 & 2.68 \\end{pmatrix}\n$$\nThe posterior covariance between $\\theta_1$ and $\\theta_2$ is the off-diagonal element of this matrix, $\\Sigma_{N,12}$:\n$$\n\\operatorname{Cov}(\\theta_1, \\theta_2 \\mid y) = \\Sigma_{N,12} = \\frac{-2.534}{0.5179} \\approx -4.8930295...\n$$\nRounding to four significant figures, the posterior covariance is $-4.893$.\n\n**Interpretation of the result:**\nThe design matrix $X$ exhibits near-collinearity, as its columns are almost linearly dependent. The second column is nearly a multiple of the first (e.g., $h_{12}=0.98 \\cdot h_{11}$ and $h_{22}=0.98 \\cdot h_{21}$). This means the model output $y_i \\approx h_{i1}\\theta_1 + h_{i2}\\theta_2$ is sensitive to a specific linear combination of the parameters, approximately $h_{i1}(\\theta_1 + 0.98\\theta_2)$, rather than to $\\theta_1$ and $\\theta_2$ individually.\n\nThis lack of independent influence on the output is a classic parameter identifiability problem. Different pairs of $(\\theta_1, \\theta_2)$ values that keep the sum $\\theta_1 + 0.98\\theta_2$ constant would produce very similar model predictions, and thus have similar likelihood values. This creates a long, narrow \"ridge\" in the likelihood function in the $(\\theta_1, \\theta_2)$ parameter space, oriented along the line $\\theta_1 + 0.98\\theta_2 = \\text{const}$.\n\nThe posterior distribution, $p(\\theta \\mid y)$, is the product of the likelihood and the prior. While the broad, uncorrelated prior $\\mathcal{N}(0, 10I)$ helps to regularize the problem and yield a proper posterior, the shape of this posterior is dominated by the ridge in the likelihood. The result is a posterior distribution shaped like a narrow, tilted ellipse.\n\nThe posterior covariance, $\\operatorname{Cov}(\\theta_1, \\theta_2 \\mid y)$, mathematically captures this relationship.\n- **Sign**: The covariance is large and negative ($-4.893$). This reflects the trade-off identified above: to keep the sum $\\theta_1 + 0.98\\theta_2$ constant, an increase in $\\theta_1$ must be compensated by a decrease in $\\theta_2$. This results in a strong negative correlation in their joint posterior distribution.\n- **Magnitude**: The magnitude of the covariance is large relative to the parameter variances. We can compute the posterior correlation coefficient $\\rho$:\n$$\n\\rho = \\frac{\\operatorname{Cov}(\\theta_1, \\theta_2 \\mid y)}{\\sqrt{\\operatorname{Var}(\\theta_1 \\mid y)\\operatorname{Var}(\\theta_2 \\mid y)}} = \\frac{-2.534/0.5179}{\\sqrt{(2.5892/0.5179)(2.68/0.5179)}} = \\frac{-2.534}{\\sqrt{2.5892 \\times 2.68}} \\approx -0.962\n$$\nA correlation coefficient of $\\approx -0.962$, which is very close to $-1$, confirms an extremely strong negative linear relationship between the parameters in the posterior. This high magnitude indicates that although the data (with the prior) can constrain the parameters to lie along a thin line in the parameter space, they cannot effectively distinguish between different points on that line. This is the hallmark of a severe identifiability issue driven by the experimental design (i.e., the near-collinearity of $X$).",
            "answer": "$$\\boxed{-4.893}$$"
        },
        {
            "introduction": "Beyond estimating parameters for a single model, a key task in science is to compare competing hypotheses. Bayesian inference provides a principled way to do this through model comparison, weighing the evidence for different model structures. This advanced practice guides you through the computation of the Bayes factor to compare two distinct biomechanical models of tissue contact: one with linear damping and one with nonlinear damping . This exercise provides practical experience with the marginal likelihood, the cornerstone of Bayesian model selection, and its role in adjudicating between scientific theories.",
            "id": "4157229",
            "problem": "You are given a quasi-static contact mechanics scenario modeling indentation into compliant biological tissue, where the measured contact force is assumed to arise from an elastic spring term and a velocity-dependent damping term, contaminated by measurement noise. Consider two competing parametric formulations for the damping contribution and construct a Bayesian model comparison framework to decide between them using Bayes factors.\n\nFundamental base and assumptions:\n- In the Kelvin–Voigt contact model, the force is the sum of an elastic term and a damping term. For a one-dimensional indentation depth $x(t)$ and indentation velocity $v(t) = \\frac{dx}{dt}$, the quasi-static contact force satisfies $F(t) = F_{\\text{elastic}}(t) + F_{\\text{damping}}(t) + \\varepsilon(t)$, where $\\varepsilon(t)$ represents measurement noise.\n- Let the elastic term be $F_{\\text{elastic}}(t) = k\\,x(t)$, with stiffness parameter $k$ in Newtons per meter ($\\mathrm{N/m}$).\n- Consider two damping formulations:\n  1. Linear damping: $F_{\\text{damping}}(t) = c\\, v(t)$, with $c$ in Newton-seconds per meter ($\\mathrm{N\\cdot s/m}$).\n  2. Nonlinear odd-quadratic damping: $F_{\\text{damping}}(t) = c_n\\, v(t)\\,\\lvert v(t)\\rvert$, with $c_n$ in Newton-seconds squared per meter squared ($\\mathrm{N\\cdot s^2/m^2}$).\n- Assume additive measurement noise $\\varepsilon(t)$ is independently and identically distributed Gaussian with zero mean and known standard deviation $\\sigma$ in Newtons ($\\mathrm{N}$). That is, $\\varepsilon(t) \\sim \\mathcal{N}(0,\\sigma^2)$.\n\nBayesian parameterization and priors:\n- For each model, collect $N$ samples at times $t_1,\\dots,t_N$. Denote the force observations as a vector $\\mathbf{y}\\in\\mathbb{R}^N$, and define the design matrix $\\boldsymbol{\\Phi}\\in\\mathbb{R}^{N\\times 2}$ such that the model is linear in parameters $\\boldsymbol{\\theta}\\in\\mathbb{R}^2$:\n  - For the linear damping model $M_{\\text{lin}}$: $\\boldsymbol{\\Phi} = [\\,\\mathbf{x}\\;\\; \\mathbf{v}\\,]$ and $\\boldsymbol{\\theta} = [\\,k\\;\\; c\\,]^\\top$.\n  - For the nonlinear damping model $M_{\\text{nonlin}}$: $\\boldsymbol{\\Phi} = [\\,\\mathbf{x}\\;\\; \\mathbf{r}\\,]$ and $\\boldsymbol{\\theta} = [\\,k\\;\\; c_n\\,]^\\top$, where $r(t) = v(t)\\lvert v(t)\\rvert$.\n- Place a zero-mean Gaussian prior on the parameters: $\\boldsymbol{\\theta} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{S}_0)$, with prior covariance $\\mathbf{S}_0=\\mathrm{diag}(s_k^2, s_d^2)$, where $s_k$ is the prior standard deviation for $k$ and $s_d$ is the prior standard deviation for $c$ (under $M_{\\text{lin}}$) or $c_n$ (under $M_{\\text{nonlin}}$).\n\nBayesian evidence and Bayes factor:\n- The Bayesian evidence (marginal likelihood) for a model $M$ is $p(\\mathbf{y}\\mid M) = \\int p(\\mathbf{y}\\mid \\boldsymbol{\\theta},M)\\,p(\\boldsymbol{\\theta}\\mid M)\\,d\\boldsymbol{\\theta}$.\n- The Bayes factor comparing the linear damping model $M_{\\text{lin}}$ to the nonlinear damping model $M_{\\text{nonlin}}$ is $B_{12} = \\dfrac{p(\\mathbf{y}\\mid M_{\\text{lin}})}{p(\\mathbf{y}\\mid M_{\\text{nonlin}})}$. Report its natural logarithm $\\ln B_{12}$, which is dimensionless.\n\nGiven that both models are linear in parameters and the noise is Gaussian with known variance, the marginal likelihood integral is analytically tractable via Gaussian integration. For a given model $M$ with design matrix $\\boldsymbol{\\Phi}$, prior covariance $\\mathbf{S}_0$, and noise standard deviation $\\sigma$, define\n$$\n\\mathbf{C} = \\sigma^2 \\mathbf{I}_N + \\boldsymbol{\\Phi}\\,\\mathbf{S}_0\\,\\boldsymbol{\\Phi}^\\top,\n$$\nwhere $\\mathbf{I}_N$ is the $N\\times N$ identity matrix. The Bayesian evidence is the multivariate Gaussian density of $\\mathbf{y}$ with mean $\\mathbf{0}$ and covariance $\\mathbf{C}$.\n\nYour task:\n- Derive from first principles the exact Bayesian evidence for each model $M_{\\text{lin}}$ and $M_{\\text{nonlin}}$ using the definitions above, and implement a numerically stable computation of $\\ln p(\\mathbf{y}\\mid M)$ using Cholesky factorization of $\\mathbf{C}$. Then compute the Bayes factor $B_{12}$ and report $\\ln B_{12}$ for the specified test suite.\n- Use the following test suite to generate $\\mathbf{x}$, $\\mathbf{v}$, and $\\mathbf{y}$ deterministically. In all cases, use a sampling duration of $T=1.0$ seconds and $N=50$ samples at equally spaced times $t_n = \\frac{n}{N}T$ for $n=0,\\dots, N-1$. Use a baseline indentation depth $x_0 = 4\\times 10^{-3}\\,\\mathrm{m}$ and a sinusoidal modulation of amplitude $A = 2\\times 10^{-3}\\,\\mathrm{m}$ and frequency $f=3\\,\\mathrm{Hz}$:\n  - Indentation depth: $x(t) = x_0 + A\\sin(2\\pi f t)$ in meters.\n  - Velocity: $v(t) = \\frac{dx}{dt} = A(2\\pi f)\\cos(2\\pi f t)$ in meters per second.\n  - Nonlinear regressor: $r(t) = v(t)\\lvert v(t)\\rvert$ in $(\\mathrm{m/s})^2$.\n\nFor each case, generate the force $\\mathbf{y}$ without adding random noise, using the specified generative model and parameters; the known noise standard deviation $\\sigma$ only enters the evidence computation.\n\nTest suite cases:\n1. Case 1 (linear damping generative, happy path):\n   - Generative model: $F(t) = k_{\\text{true}}\\,x(t) + c_{\\text{true}}\\,v(t)$.\n   - Parameters: $k_{\\text{true}} = 3.0\\times 10^4\\,\\mathrm{N/m}$, $c_{\\text{true}} = 80\\,\\mathrm{N\\cdot s/m}$, $\\sigma = 1.0\\,\\mathrm{N}$.\n2. Case 2 (nonlinear damping generative):\n   - Generative model: $F(t) = k_{\\text{true}}\\,x(t) + c_{n,\\text{true}}\\,v(t)\\lvert v(t)\\rvert$.\n   - Parameters: $k_{\\text{true}} = 3.0\\times 10^4\\,\\mathrm{N/m}$, $c_{n,\\text{true}} = 4.0\\times 10^3\\,\\mathrm{N\\cdot s^2/m^2}$, $\\sigma = 1.0\\,\\mathrm{N}$.\n3. Case 3 (boundary condition: zero velocities):\n   - Generative setup: same $x(t)$, but set $v(t)\\equiv 0$ deterministically; $F(t) = k_{\\text{true}}\\,x(t)$.\n   - Parameters: $k_{\\text{true}} = 3.0\\times 10^4\\,\\mathrm{N/m}$, $\\sigma = 1.0\\,\\mathrm{N}$.\n4. Case 4 (edge case: high measurement noise in evidence):\n   - Generative model: $F(t) = k_{\\text{true}}\\,x(t) + c_{\\text{true}}\\,v(t)$ as in Case 1.\n   - Parameters: $k_{\\text{true}} = 3.0\\times 10^4\\,\\mathrm{N/m}$, $c_{\\text{true}} = 80\\,\\mathrm{N\\cdot s/m}$, $\\sigma = 50.0\\,\\mathrm{N}$.\n\nPriors:\n- Use the same prior for the stiffness in both models: $s_k = 1.0\\times 10^5\\,\\mathrm{N/m}$.\n- For the damping parameters, use $s_c = 5.0\\times 10^2\\,\\mathrm{N\\cdot s/m}$ under $M_{\\text{lin}}$ and $s_{c_n} = 5.0\\times 10^3\\,\\mathrm{N\\cdot s^2/m^2}$ under $M_{\\text{nonlin}}$.\n\nNumerical implementation requirements:\n- Compute the covariance $\\mathbf{C}$ for each model and case, then compute\n  $$\n  \\ln p(\\mathbf{y}\\mid M) = -\\tfrac{1}{2}\\left( N\\ln(2\\pi) + \\ln\\det(\\mathbf{C}) + \\mathbf{y}^\\top \\mathbf{C}^{-1}\\mathbf{y} \\right).\n  $$\n- Use Cholesky factorization to evaluate $\\ln\\det(\\mathbf{C})$ and $\\mathbf{y}^\\top \\mathbf{C}^{-1}\\mathbf{y}$ stably: if $\\mathbf{C}=\\mathbf{L}\\mathbf{L}^\\top$ with lower-triangular $\\mathbf{L}$, then $\\ln\\det(\\mathbf{C}) = 2\\sum_i \\ln L_{ii}$ and $\\mathbf{y}^\\top \\mathbf{C}^{-1}\\mathbf{y}$ can be obtained by solving $\\mathbf{L}\\mathbf{w}=\\mathbf{y}$ and then $\\mathbf{L}^\\top \\mathbf{z}=\\mathbf{w}$ and computing $\\mathbf{y}^\\top \\mathbf{z}$.\n\nFinal output specification:\n- For each of the four cases, compute the natural logarithm of the Bayes factor $\\ln B_{12}$ that compares the linear damping model $M_{\\text{lin}}$ against the nonlinear damping model $M_{\\text{nonlin}}$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each $\\ln B_{12}$ rounded to three decimal places (e.g., \"[x1,x2,x3,x4]\"). The values are dimensionless and must be reported without any physical units.",
            "solution": "The user-provided problem statement has been analyzed and is determined to be valid. It is scientifically grounded in established principles of mechanics and Bayesian statistics, is well-posed, objective, and self-contained. The problem requires the computation of the log Bayes factor to compare two biomechanical models, a task for which all necessary theoretical background, data, and parameters are provided.\n\nThe objective is to compute the natural logarithm of the Bayes factor, $\\ln B_{12}$, which compares a linear damping model ($M_{\\text{lin}}$) to a nonlinear damping model ($M_{\\text{nonlin}}$). The Bayes factor is the ratio of the models' evidences (or marginal likelihoods):\n$$\nB_{12} = \\frac{p(\\mathbf{y} \\mid M_{\\text{lin}})}{p(\\mathbf{y} \\mid M_{\\text{nonlin}})}\n$$\nTaking the natural logarithm, we get $\\ln B_{12} = \\ln p(\\mathbf{y} \\mid M_{\\text{lin}}) - \\ln p(\\mathbf{y} \\mid M_{\\text{nonlin}})$. Our primary task is to derive and compute the log-evidence, $\\ln p(\\mathbf{y} \\mid M)$, for each model.\n\nA general model $M$ is defined by the linear relationship $\\mathbf{y} = \\boldsymbol{\\Phi}\\boldsymbol{\\theta} + \\boldsymbol{\\varepsilon}$, where $\\mathbf{y} \\in \\mathbb{R}^N$ is the vector of force observations, $\\boldsymbol{\\Phi} \\in \\mathbb{R}^{N \\times P}$ is the design matrix (with $P=2$ parameters), $\\boldsymbol{\\theta} \\in \\mathbb{R}^P$ is the parameter vector, and $\\boldsymbol{\\varepsilon} \\in \\mathbb{R}^N$ is the measurement noise.\n\nThe problem specifies the following distributions:\n1.  The likelihood, derived from the noise model $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I}_N)$, is a multivariate Gaussian:\n    $$\n    p(\\mathbf{y} \\mid \\boldsymbol{\\theta}, M) = \\mathcal{N}(\\mathbf{y} ; \\boldsymbol{\\Phi}\\boldsymbol{\\theta}, \\sigma^2 \\mathbf{I}_N)\n    $$\n2.  The prior on the parameters is a zero-mean multivariate Gaussian:\n    $$\n    p(\\boldsymbol{\\theta} \\mid M) = \\mathcal{N}(\\boldsymbol{\\theta} ; \\mathbf{0}, \\mathbf{S}_0)\n    $$\n\nThe evidence is the marginal likelihood, obtained by integrating the product of the likelihood and the prior over all possible parameter values:\n$$\np(\\mathbf{y} \\mid M) = \\int p(\\mathbf{y} \\mid \\boldsymbol{\\theta}, M) p(\\boldsymbol{\\theta} \\mid M) d\\boldsymbol{\\theta}\n$$\nThis is a standard convolution of Gaussian distributions. The resulting marginal distribution of $\\mathbf{y}$ is also a Gaussian. We can determine its parameters by considering the joint distribution of $\\mathbf{y}$ and $\\boldsymbol{\\theta}$. The mean of $\\mathbf{y}$ is:\n$$\n\\mathbb{E}[\\mathbf{y}] = \\mathbb{E}[\\boldsymbol{\\Phi}\\boldsymbol{\\theta} + \\boldsymbol{\\varepsilon}] = \\boldsymbol{\\Phi}\\mathbb{E}[\\boldsymbol{\\theta}] + \\mathbb{E}[\\boldsymbol{\\varepsilon}] = \\boldsymbol{\\Phi}\\mathbf{0} + \\mathbf{0} = \\mathbf{0}\n$$\nThe covariance of $\\mathbf{y}$, denoted $\\mathbf{C}$, is:\n$$\n\\mathbf{C} = \\mathrm{Cov}(\\mathbf{y}) = \\mathbb{E}[\\mathbf{y}\\mathbf{y}^\\top] = \\mathbb{E}[(\\boldsymbol{\\Phi}\\boldsymbol{\\theta} + \\boldsymbol{\\varepsilon})(\\boldsymbol{\\Phi}\\boldsymbol{\\theta} + \\boldsymbol{\\varepsilon})^\\top]\n$$\nExpanding this and using the independence of $\\boldsymbol{\\theta}$ and $\\boldsymbol{\\varepsilon}$ ($\\mathbb{E}[\\boldsymbol{\\theta}\\boldsymbol{\\varepsilon}^\\top] = \\mathbb{E}[\\boldsymbol{\\theta}]\\mathbb{E}[\\boldsymbol{\\varepsilon}^\\top] = \\mathbf{0}$), we get:\n$$\n\\mathbf{C} = \\boldsymbol{\\Phi} \\mathbb{E}[\\boldsymbol{\\theta}\\boldsymbol{\\theta}^\\top] \\boldsymbol{\\Phi}^\\top + \\mathbb{E}[\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}^\\top] = \\boldsymbol{\\Phi}\\mathbf{S}_0\\boldsymbol{\\Phi}^\\top + \\sigma^2\\mathbf{I}_N\n$$\nThus, the evidence is the probability density of the observation vector $\\mathbf{y}$ under a multivariate Gaussian distribution with zero mean and covariance $\\mathbf{C}$:\n$$\np(\\mathbf{y} \\mid M) = \\mathcal{N}(\\mathbf{y} ; \\mathbf{0}, \\mathbf{C}) = \\frac{1}{(2\\pi)^{N/2} \\det(\\mathbf{C})^{1/2}} \\exp\\left(-\\frac{1}{2} \\mathbf{y}^\\top \\mathbf{C}^{-1} \\mathbf{y}\\right)\n$$\nThe natural logarithm of the evidence is:\n$$\n\\ln p(\\mathbf{y} \\mid M) = -\\frac{N}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln\\det(\\mathbf{C}) - \\frac{1}{2}\\mathbf{y}^\\top \\mathbf{C}^{-1} \\mathbf{y} = -\\frac{1}{2}\\left(N\\ln(2\\pi) + \\ln\\det(\\mathbf{C}) + \\mathbf{y}^\\top \\mathbf{C}^{-1} \\mathbf{y}\\right)\n$$\nThis confirms the formula provided in the problem statement.\n\nFor numerical stability, we compute the terms involving $\\mathbf{C}$ using its Cholesky factorization, $\\mathbf{C} = \\mathbf{L}\\mathbf{L}^\\top$, where $\\mathbf{L}$ is a lower-triangular matrix.\n1.  The log-determinant is $\\ln\\det(\\mathbf{C}) = \\ln\\det(\\mathbf{L}\\mathbf{L}^\\top) = 2\\ln\\det(\\mathbf{L}) = 2\\sum_{i} \\ln(L_{ii})$, where $L_{ii}$ are the diagonal elements of $\\mathbf{L}$.\n2.  The quadratic form $\\mathbf{y}^\\top \\mathbf{C}^{-1} \\mathbf{y}$ is computed by solving the triangular system $\\mathbf{L}\\mathbf{w} = \\mathbf{y}$ for $\\mathbf{w}$, and then calculating $\\mathbf{w}^\\top\\mathbf{w}$. This is because $\\mathbf{y}^\\top \\mathbf{C}^{-1} \\mathbf{y} = \\mathbf{y}^\\top (\\mathbf{L}\\mathbf{L}^\\top)^{-1} \\mathbf{y} = \\mathbf{y}^\\top (\\mathbf{L}^\\top)^{-1}\\mathbf{L}^{-1}\\mathbf{y} = (\\mathbf{L}^{-1}\\mathbf{y})^\\top(\\mathbf{L}^{-1}\\mathbf{y}) = \\mathbf{w}^\\top\\mathbf{w}$.\n\nThe solution proceeds by applying this methodology to each of the four test cases. A function is implemented to calculate the log-evidence for a given model and dataset.\n\nFor each case, we first generate the kinematic data: time $t_n = \\frac{n}{N}T$ for $n = 0, \\ldots, N-1$ with $T=1.0\\,\\mathrm{s}$ and $N=50$. The indentation depth $x(t)$, velocity $v(t)$, and nonlinear regressor $r(t)$ are computed as functions of time.\n- $x(t) = x_0 + A\\sin(2\\pi f t)$ with $x_0 = 4 \\times 10^{-3}\\,\\mathrm{m}$, $A = 2 \\times 10^{-3}\\,\\mathrm{m}$, $f=3\\,\\mathrm{Hz}$.\n- $v(t) = A(2\\pi f)\\cos(2\\pi f t)$.\n- $r(t) = v(t)\\lvert v(t)\\rvert$.\n\nThen, for each case, the force data vector $\\mathbf{y}$ is synthesized according to the specified generative model. Subsequently, we compute $\\ln p(\\mathbf{y} \\mid M_{\\text{lin}})$ and $\\ln p(\\mathbf{y} \\mid M_{\\text{nonlin}})$ using the specified priors and noise standard deviation $\\sigma$. The priors are defined by $s_k = 1.0\\times 10^5\\,\\mathrm{N/m}$, $s_c = 5.0\\times 10^2\\,\\mathrm{N\\cdot s/m}$, and $s_{c_n} = 5.0\\times 10^3\\,\\mathrm{N\\cdot s^2/m^2}$.\n\n- **Model $M_{\\text{lin}}$**: $\\boldsymbol{\\Phi}_{\\text{lin}} = [\\mathbf{x}, \\mathbf{v}]$ and $\\mathbf{S}_{0, \\text{lin}} = \\mathrm{diag}(s_k^2, s_c^2)$.\n- **Model $M_{\\text{nonlin}}$**: $\\boldsymbol{\\Phi}_{\\text{nonlin}} = [\\mathbf{x}, \\mathbf{r}]$ and $\\mathbf{S}_{0, \\text{nonlin}} = \\mathrm{diag}(s_k^2, s_{c_n}^2)$.\n\nThe final result for each case is $\\ln B_{12} = \\ln p(\\mathbf{y} \\mid M_{\\text{lin}}) - \\ln p(\\mathbf{y} \\mid M_{\\text{nonlin}})$.\n\n- **Case 1**: Data generated from the linear model. We expect $\\ln B_{12} > 0$, favoring $M_{\\text{lin}}$.\n- **Case 2**: Data generated from the nonlinear model. We expect $\\ln B_{12}  0$, favoring $M_{\\text{nonlin}}$.\n- **Case 3**: Velocity is zero, so $v(t)=r(t)=0$. The columns corresponding to damping in both design matrices are zero vectors. The terms involving damping priors ($s_c^2$, $s_{c_n}^2$) vanish from the calculation of $\\mathbf{C} = \\boldsymbol{\\Phi}\\mathbf{S}_0\\boldsymbol{\\Phi}^\\top+\\sigma^2\\mathbf{I}_N$. Consequently, $\\mathbf{C}_{\\text{lin}} = \\mathbf{C}_{\\text{nonlin}}$. Since $\\mathbf{y}$ is also identical, the evidences will be equal, and we expect $\\ln B_{12} = 0$.\n- **Case 4**: Data generated from the linear model, but $\\sigma$ in the evidence calculation is large. The term $\\sigma^2\\mathbf{I}_N$ dominates the covariance matrices $\\mathbf{C}$, making them very similar for both models. This reduces the ability to distinguish between them, so we expect $\\ln B_{12}$ to be positive but significantly smaller in magnitude than in Case 1.",
            "answer": "$$\\boxed{[513.791,-260.672,0.000,1.968]}$$"
        }
    ]
}