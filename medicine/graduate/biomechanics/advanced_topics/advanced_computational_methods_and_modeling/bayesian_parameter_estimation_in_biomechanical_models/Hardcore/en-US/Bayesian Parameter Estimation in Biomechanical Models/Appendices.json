{
    "hands_on_practices": [
        {
            "introduction": "The cornerstone of connecting a deterministic model to real-world data is the likelihood function, which quantifies the probability of observing your experimental data for a given set of model parameters. In this first exercise, you will derive the most common likelihood used in biomechanics, which is based on the assumption of additive Gaussian noise. Understanding this derivation is crucial as it reveals the probabilistic foundation for the ubiquitous 'sum of squared errors' criterion used in many parameter fitting methods.",
            "id": "4157236",
            "problem": "Consider a forward dynamics musculoskeletal model that deterministically maps a parameter vector $\\theta$ to a continuous-time kinematic trajectory $x(t;\\theta)$ for a single scalar joint angle over a finite observation window $t \\in [0,T]$. Motion capture provides discrete-time measurements at sample times $t_{1}, t_{2}, \\dots, t_{N}$, yielding data $\\tilde{x}(t_{i})$ for $i=1,\\dots,N$. Assume an additive measurement model $\\tilde{x}(t_{i}) = x(t_{i};\\theta) + \\varepsilon_{i}$, where the measurement errors $\\varepsilon_{i}$ are independent across time points and follow a zero-mean Gaussian distribution with common variance $\\sigma^{2}$. The forward dynamics trajectory $x(t;\\theta)$ is the output of the biomechanical model given $\\theta$ and is assumed to be known deterministically once $\\theta$ is specified.\n\nUsing only foundational definitions from probability and Bayesian inference (specifically, the definition of a likelihood as the joint probability of the observed data given parameters, the independence of measurement errors, and the form of a Gaussian probability density function), derive the likelihood function for the entire dataset $\\{\\tilde{x}(t_{i})\\}_{i=1}^{N}$ as a function of $\\theta$ and $\\sigma^{2}$. Express your final result as a single closed-form analytic expression in terms of $N$, $\\sigma^{2}$, and the residuals between the measured data and the model-predicted trajectory at the sampled times, without introducing any additional parameters. No numerical evaluation is required. The final answer must be a single analytic expression. Do not include any units in your final expression.",
            "solution": "We begin from the generative measurement model for each discrete time point $t_{i}$:\n$$\n\\tilde{x}(t_{i}) \\;=\\; x(t_{i};\\theta) + \\varepsilon_{i},\n$$\nwhere $\\varepsilon_{i}$ are independent Gaussian random variables with mean $0$ and variance $\\sigma^{2}$. The independence assumption across time points implies that the joint probability of the observed data across all $N$ times is the product of the individual probabilities at each time point.\n\nIn Bayesian inference, the likelihood function is defined as the joint probability density of the observed data given the parameters. Let $\\{\\tilde{x}(t_{i})\\}_{i=1}^{N}$ denote the collection of observed measurements. Define the residuals\n$$\nr_{i}(\\theta) \\;=\\; \\tilde{x}(t_{i}) - x(t_{i};\\theta).\n$$\nUnder the additive Gaussian noise model, each residual $r_{i}(\\theta)$ is a realization from a Gaussian distribution with mean $0$ and variance $\\sigma^{2}$. The probability density function (PDF) of a zero-mean Gaussian random variable with variance $\\sigma^{2}$ evaluated at $r_{i}(\\theta)$ is\n$$\n\\frac{1}{\\sqrt{2\\pi \\sigma^{2}}} \\exp\\!\\left( -\\frac{r_{i}(\\theta)^{2}}{2\\sigma^{2}} \\right).\n$$\nBecause the errors are independent across time points, the joint PDF of all residuals is the product of the individual PDFs. Therefore, the likelihood function for $\\theta$ and $\\sigma^{2}$ given the dataset $\\{\\tilde{x}(t_{i})\\}_{i=1}^{N}$ is\n$$\nL(\\theta,\\sigma^{2} \\mid \\{\\tilde{x}(t_{i})\\}_{i=1}^{N}) \\;=\\; \\prod_{i=1}^{N} \\frac{1}{\\sqrt{2\\pi \\sigma^{2}}} \\exp\\!\\left( -\\frac{(\\tilde{x}(t_{i}) - x(t_{i};\\theta))^{2}}{2\\sigma^{2}} \\right).\n$$\nWe can simplify this product by collecting terms. The product of the normalization constants becomes\n$$\n\\prod_{i=1}^{N} \\frac{1}{\\sqrt{2\\pi \\sigma^{2}}} \\;=\\; (2\\pi \\sigma^{2})^{-N/2}.\n$$\nThe product of the exponentials becomes the exponential of the sum of exponents:\n$$\n\\prod_{i=1}^{N} \\exp\\!\\left( -\\frac{(\\tilde{x}(t_{i}) - x(t_{i};\\theta))^{2}}{2\\sigma^{2}} \\right) \\;=\\; \\exp\\!\\left( -\\frac{1}{2\\sigma^{2}} \\sum_{i=1}^{N} (\\tilde{x}(t_{i}) - x(t_{i};\\theta))^{2} \\right).\n$$\nThus, the closed-form likelihood is\n$$\nL(\\theta,\\sigma^{2} \\mid \\{\\tilde{x}(t_{i})\\}_{i=1}^{N}) \\;=\\; (2\\pi \\sigma^{2})^{-N/2} \\exp\\!\\left( -\\frac{1}{2\\sigma^{2}} \\sum_{i=1}^{N} \\left(\\tilde{x}(t_{i}) - x(t_{i};\\theta)\\right)^{2} \\right).\n$$\nThis expression depends only on $N$, $\\sigma^{2}$, and the residuals between the motion capture data and the model-predicted trajectory evaluated at the sampled times, as required.",
            "answer": "$$\\boxed{(2\\pi\\sigma^{2})^{-N/2}\\,\\exp\\!\\left(-\\frac{1}{2\\sigma^{2}}\\sum_{i=1}^{N}\\bigl(\\tilde{x}(t_{i})-x(t_{i};\\theta)\\bigr)^{2}\\right)}$$"
        },
        {
            "introduction": "A key advantage of the Bayesian framework is its ability to formally incorporate prior knowledge into the estimation process. For many biomechanical parameters, such as segment masses or muscle strengths, we have strong prior beliefs about their plausible ranges. This exercise will guide you through the practical steps of constructing a suitable prior distribution—the log-normal—to enforce positivity and capture this domain knowledge. You will then perform a complete Bayesian update, combining the prior with a new measurement to obtain the posterior distribution.",
            "id": "4157247",
            "problem": "A lower-limb multibody dynamics model requires estimating the mass of a single segment (the thigh) for a human subject of total body mass $75$ kilograms. To enforce positivity and biomechanically plausible ranges, you decide to place a log-normal prior on the unknown segment mass $m$, that is, $m \\sim \\text{LogNormal}(\\mu_{0}, \\sigma_{0}^{2})$, where $\\ln(m)$ is normally distributed. Empirical population studies suggest the thigh mass lies between $8\\%$ and $12\\%$ of total body mass for typical adults, so for this subject you will encode plausibility by choosing the prior such that the $10\\%$ quantile is $6$ kilograms and the $90\\%$ quantile is $9$ kilograms. The dynamic identification experiment you perform yields a single estimate $\\hat{m}$ whose multiplicative noise on mass is approximately log-normal. Equivalently, on the logarithmic scale the measurement model can be written as\n$$\nz \\equiv \\ln(\\hat{m}) = \\ln(m) + \\varepsilon,\n$$\nwhere $\\varepsilon \\sim \\mathcal{N}(0, s^{2})$ with known $s = 0.05$. You record $\\hat{m} = 7.5$ kilograms.\n\nUsing the definitions of the log-normal distribution, the cumulative distribution function (CDF) of the standard normal distribution, and the conjugate update for a normal prior with a normal likelihood on the log scale, do the following:\n\n1. Construct the prior by solving for $\\mu_{0}$ and $\\sigma_{0}$ that make the $10\\%$ and $90\\%$ prior quantiles equal to $6$ and $9$ kilograms, respectively.\n2. Derive the posterior distribution of $X \\equiv \\ln(m)$ given the observed $z = \\ln(7.5)$ under the measurement model above, and obtain the posterior parameters $\\mu_{\\text{post}}$ and $\\sigma_{\\text{post}}^{2}$.\n3. Transform back to mass units to obtain the posterior mean segment mass $E[m \\mid z]$.\n\nRound your final numerical answer to four significant figures. Express the final mass in kilograms (kg). The final answer must be a single real-valued number.",
            "solution": "The problem requires a three-part solution involving Bayesian parameter estimation for a biomechanical model. First, we determine the parameters of a log-normal prior distribution. Second, we update this prior with a measurement to find the posterior distribution. Third, we calculate the posterior mean of the estimated parameter.\n\nThe parameter of interest is the thigh segment mass, $m$, in kilograms. The total body mass of the subject is $75$ kg.\n\n**Part 1: Prior Distribution Construction**\n\nThe prior for the mass $m$ is a log-normal distribution, $m \\sim \\text{LogNormal}(\\mu_{0}, \\sigma_{0}^{2})$. This implies that the natural logarithm of the mass, let's call it $X = \\ln(m)$, follows a normal distribution, $X \\sim \\mathcal{N}(\\mu_{0}, \\sigma_{0}^{2})$.\n\nWe are given two quantiles for the mass $m$:\n1. The $10\\%$ quantile is $6$ kg, i.e., $P(m \\le 6) = 0.10$.\n2. The $90\\%$ quantile is $9$ kg, i.e., $P(m \\le 9) = 0.90$.\n\nSince the logarithm is a strictly increasing function, the quantiles of $X = \\ln(m)$ are the logarithms of the quantiles of $m$.\n$$Q_{0.10}(X) = \\ln(Q_{0.10}(m)) = \\ln(6)$$\n$$Q_{0.90}(X) = \\ln(Q_{0.90}(m)) = \\ln(9)$$\n\nFor a normally distributed random variable $X \\sim \\mathcal{N}(\\mu_{0}, \\sigma_{0}^{2})$, the $p$-th quantile is given by $Q_{p}(X) = \\mu_{0} + \\sigma_{0} \\Phi^{-1}(p)$, where $\\Phi^{-1}(p)$ is the quantile function (inverse CDF) of the standard normal distribution $\\mathcal{N}(0, 1)$.\n\nLet $q_{0.1} = \\Phi^{-1}(0.10)$ and $q_{0.9} = \\Phi^{-1}(0.90)$. From the symmetry of the normal distribution, we know that $q_{0.1} = -q_{0.9}$. A standard value from statistical tables or computation is $q_{0.9} \\approx 1.28155$.\n\nThis gives us a system of two linear equations for the two unknowns, $\\mu_{0}$ and $\\sigma_{0}$:\n1. $\\ln(6) = \\mu_{0} + \\sigma_{0} q_{0.1} = \\mu_{0} - \\sigma_{0} q_{0.9}$\n2. $\\ln(9) = \\mu_{0} + \\sigma_{0} q_{0.9}$\n\nTo solve for $\\mu_{0}$, we can add the two equations:\n$$\\ln(6) + \\ln(9) = 2\\mu_{0}$$\n$$\\ln(6 \\times 9) = 2\\mu_{0}$$\n$$\\mu_{0} = \\frac{1}{2}\\ln(54)$$\n\nTo solve for $\\sigma_{0}$, we subtract the first equation from the second:\n$$\\ln(9) - \\ln(6) = (\\mu_{0} + \\sigma_{0} q_{0.9}) - (\\mu_{0} - \\sigma_{0} q_{0.9}) = 2\\sigma_{0} q_{0.9}$$\n$$\\ln\\left(\\frac{9}{6}\\right) = 2\\sigma_{0} q_{0.9}$$\n$$\\sigma_{0} = \\frac{\\ln(1.5)}{2q_{0.9}} = \\frac{\\ln(1.5)}{2\\Phi^{-1}(0.9)}$$\n\n**Part 2: Posterior Distribution Derivation**\n\nWe are given a measurement $\\hat{m} = 7.5$ kg. The measurement model on the logarithmic scale is given by $z = \\ln(m) + \\varepsilon$, where $z = \\ln(\\hat{m}) = \\ln(7.5)$ and the noise $\\varepsilon$ is distributed as $\\varepsilon \\sim \\mathcal{N}(0, s^{2})$ with a known standard deviation $s = 0.05$.\n\nThis measurement model implies a likelihood function for the parameter $X = \\ln(m)$. Given a value of $X$, the observation $z$ is normally distributed around $X$:\n$$p(z \\mid X) = \\mathcal{N}(z \\mid X, s^2)$$\n\nWe use Bayes' theorem to find the posterior distribution of $X$ given the observation $z$:\n$$p(X \\mid z) \\propto p(z \\mid X) p(X)$$\nThe prior is $p(X) = \\mathcal{N}(X \\mid \\mu_{0}, \\sigma_{0}^{2})$, and the likelihood is $p(z \\mid X) = \\mathcal{N}(z \\mid X, s^{2})$. Since both are Gaussian, the posterior will also be Gaussian. This is a conjugate prior setup.\nThe posterior distribution is $p(X \\mid z) = \\mathcal{N}(X \\mid \\mu_{\\text{post}}, \\sigma_{\\text{post}}^{2})$.\n\nThe posterior precision (inverse variance) is the sum of the prior and likelihood precisions:\n$$\\frac{1}{\\sigma_{\\text{post}}^{2}} = \\frac{1}{\\sigma_{0}^{2}} + \\frac{1}{s^{2}}$$\nSo, the posterior variance is:\n$$\\sigma_{\\text{post}}^{2} = \\left(\\frac{1}{\\sigma_{0}^{2}} + \\frac{1}{s^{2}}\\right)^{-1} = \\frac{\\sigma_{0}^{2} s^{2}}{\\sigma_{0}^{2} + s^{2}}$$\n\nThe posterior mean is a precision-weighted average of the prior mean and the measurement:\n$$\\mu_{\\text{post}} = \\sigma_{\\text{post}}^{2} \\left( \\frac{\\mu_{0}}{\\sigma_{0}^{2}} + \\frac{z}{s^{2}} \\right) = \\frac{\\mu_{0} s^{2} + z \\sigma_{0}^{2}}{s^{2} + \\sigma_{0}^{2}}$$\n\n**Part 3: Posterior Mean Mass Calculation**\n\nThe posterior distribution of $X = \\ln(m)$ is $\\mathcal{N}(\\mu_{\\text{post}}, \\sigma_{\\text{post}}^{2})$. This means that the posterior distribution of the mass $m$ is a log-normal distribution, $m \\mid z \\sim \\text{LogNormal}(\\mu_{\\text{post}}, \\sigma_{\\text{post}}^{2})$.\n\nThe expected value (mean) of a log-normally distributed random variable with parameters $\\mu$ and $\\sigma^2$ is given by the formula $E = \\exp(\\mu + \\sigma^{2}/2)$.\nTherefore, the posterior mean of the mass $m$ is:\n$$E[m \\mid z] = \\exp\\left(\\mu_{\\text{post}} + \\frac{\\sigma_{\\text{post}}^{2}}{2}\\right)$$\n\nNow, we substitute the derived expressions and numerical values.\nFirst, compute the values for the prior parameters:\n- $\\mu_{0} = \\frac{1}{2}\\ln(54) \\approx 1.994492$\n- $\\sigma_{0} = \\frac{\\ln(1.5)}{2\\Phi^{-1}(0.9)} \\approx \\frac{0.405465}{2 \\times 1.28155} \\approx 0.158190$\n- $\\sigma_{0}^{2} \\approx (0.158190)^{2} \\approx 0.025024$\n\nNext, use the measurement information:\n- $z = \\ln(7.5) \\approx 2.014903$\n- $s = 0.05$, so $s^{2} = 0.0025$\n\nCalculate the posterior parameters:\n- $\\sigma_{\\text{post}}^{2} = \\frac{\\sigma_{0}^{2} s^{2}}{\\sigma_{0}^{2} + s^{2}} = \\frac{0.025024 \\times 0.0025}{0.025024 + 0.0025} = \\frac{6.256 \\times 10^{-5}}{0.027524} \\approx 0.002273$\n- $\\mu_{\\text{post}} = \\frac{\\mu_{0} s^{2} + z \\sigma_{0}^{2}}{s^{2} + \\sigma_{0}^{2}} = \\frac{(1.994492)(0.0025) + (2.014903)(0.025024)}{0.0025 + 0.025024} = \\frac{0.004986 + 0.050419}{0.027524} = \\frac{0.055405}{0.027524} \\approx 2.013035$\n\nFinally, compute the posterior mean mass:\n$$E[m \\mid z] = \\exp\\left(\\mu_{\\text{post}} + \\frac{\\sigma_{\\text{post}}^{2}}{2}\\right) \\approx \\exp\\left(2.013035 + \\frac{0.002273}{2}\\right)$$\n$$E[m \\mid z] \\approx \\exp(2.013035 + 0.0011365) = \\exp(2.0141715) \\approx 7.49503$$\n\nRounding the final answer to four significant figures, we get $7.495$. The mass is expressed in kilograms.",
            "answer": "$$\\boxed{7.495}$$"
        },
        {
            "introduction": "The goal of parameter estimation is not just to find a single 'best' value, but to characterize the uncertainty in our estimates. The posterior distribution provides this full picture, and its structure can reveal critical insights about our model and experimental design. In this final practice, you will analyze the posterior from a simplified muscle model and discover how strong correlation between parameters can signal an identifiability problem. This exercise demonstrates how Bayesian analysis serves as a powerful diagnostic tool, helping us understand when our data is insufficient to independently resolve different model parameters.",
            "id": "4157241",
            "problem": "A single-degree-of-freedom elbow flexion experiment is performed under quasi-static conditions so that dynamic effects are negligible. The net joint torque $y_i$ at trial $i$ is modeled as the sum of muscle contributions proportional to known functions of joint angle and muscle-tendon geometry, yielding a linearized observation model in two dimensionless parameters: a normalized activation gain $\\theta_1$ and a normalized series-elastic stiffness scaling $\\theta_2$. Under small variations about a fixed posture, the torque is modeled as\n$$\ny_i \\approx h_{i1}\\,\\theta_1 + h_{i2}\\,\\theta_2 + \\varepsilon_i,\n$$\nwhere $\\varepsilon_i$ represents unmodeled variability and sensor noise. The coefficients $h_{i1}$ and $h_{i2}$ are determined by moment arms and the local sensitivity of the muscle-tendon unit force to $\\theta_1$ and $\\theta_2$, respectively.\n\nAssume three trials with design matrix\n$$\nX=\\begin{pmatrix}\nh_{11} & h_{12} \\\\\nh_{21} & h_{22} \\\\\nh_{31} & h_{32}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1.00 & 0.98 \\\\\n0.50 & 0.49 \\\\\n0.20 & 0.21\n\\end{pmatrix},\n$$\nand observed torques\n$$\ny=\\begin{pmatrix}\n1.49 \\\\\n0.73 \\\\\n0.32\n\\end{pmatrix}.\n$$\nAssume independent and identically distributed additive noise with $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2 I)$, where $\\sigma^2=0.50$, and an independent Gaussian prior on the parameters $\\theta=\\begin{pmatrix}\\theta_1 \\\\ \\theta_2\\end{pmatrix}$ with $\\theta \\sim \\mathcal{N}(\\mu_0,\\Sigma_0)$, where $\\mu_0=\\begin{pmatrix}0 \\\\ 0\\end{pmatrix}$ and $\\Sigma_0=\\operatorname{diag}(10,10)$. Using Bayes’ theorem and standard properties of Gaussian distributions, derive the posterior distribution $p(\\theta \\mid y)$ and compute the posterior covariance between $\\theta_1$ and $\\theta_2$ (i.e., the off-diagonal entry of the $2\\times 2$ posterior covariance matrix). Explain how the sign and magnitude of this covariance reveal identifiability issues in the model given the near-collinearity in $X$.\n\nRound your final numerical answer for the posterior covariance to four significant figures. Report it as a pure number (dimensionless).",
            "solution": "The problem requires the derivation of the posterior distribution for the parameters of a linear biomechanical model and the computation of the posterior covariance between them. The model is given by the equation $y = X\\theta + \\varepsilon$, where $y$ is the vector of observed torques, $X$ is the design matrix, $\\theta$ is the vector of parameters, and $\\varepsilon$ is the noise vector.\n\nThis is a standard Bayesian linear regression problem. The components of the Bayesian model are:\n1.  **Likelihood**: The noise is assumed to be Gaussian, $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$. This implies that the likelihood of the data given the parameters is also Gaussian:\n    $$\n    p(y \\mid \\theta) = \\mathcal{N}(X\\theta, \\sigma^2 I) \\propto \\exp\\left( -\\frac{1}{2\\sigma^2}(y - X\\theta)^T (y - X\\theta) \\right)\n    $$\n2.  **Prior**: The parameters are assumed to have a Gaussian prior distribution:\n    $$\n    p(\\theta) = \\mathcal{N}(\\mu_0, \\Sigma_0) \\propto \\exp\\left( -\\frac{1}{2}(\\theta - \\mu_0)^T \\Sigma_0^{-1} (\\theta - \\mu_0) \\right)\n    $$\n\nAccording to Bayes' theorem, the posterior distribution is proportional to the product of the likelihood and the prior: $p(\\theta \\mid y) \\propto p(y \\mid \\theta)p(\\theta)$. Since the likelihood and prior are both Gaussian, the posterior distribution is also Gaussian, $p(\\theta \\mid y) = \\mathcal{N}(\\mu_N, \\Sigma_N)$.\n\nThe posterior covariance matrix $\\Sigma_N$ and posterior mean vector $\\mu_N$ are given by the standard formulas for Bayesian linear regression:\n$$\n\\Sigma_N = \\left(\\Sigma_0^{-1} + \\frac{1}{\\sigma^2} X^T X \\right)^{-1}\n$$\n$$\n\\mu_N = \\Sigma_N \\left(\\Sigma_0^{-1} \\mu_0 + \\frac{1}{\\sigma^2} X^T y \\right)\n$$\nThe problem asks for the posterior covariance between $\\theta_1$ and $\\theta_2$, which is the off-diagonal element of the posterior covariance matrix $\\Sigma_N$. We will now compute this matrix.\n\nThe given values are:\n- Design matrix: $X = \\begin{pmatrix} 1.00 & 0.98 \\\\ 0.50 & 0.49 \\\\ 0.20 & 0.21 \\end{pmatrix}$\n- Observed torques: $y = \\begin{pmatrix} 1.49 \\\\ 0.73 \\\\ 0.32 \\end{pmatrix}$ (This is not needed for the covariance calculation)\n- Noise variance: $\\sigma^2 = 0.50$\n- Prior mean: $\\mu_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$\n- Prior covariance: $\\Sigma_0 = \\begin{pmatrix} 10 & 0 \\\\ 0 & 10 \\end{pmatrix}$\n\nFirst, we calculate the inverse of the prior covariance matrix, $\\Sigma_0^{-1}$:\n$$\n\\Sigma_0^{-1} = \\begin{pmatrix} 10 & 0 \\\\ 0 & 10 \\end{pmatrix}^{-1} = \\begin{pmatrix} \\frac{1}{10} & 0 \\\\ 0 & \\frac{1}{10} \\end{pmatrix} = \\begin{pmatrix} 0.1 & 0 \\\\ 0 & 0.1 \\end{pmatrix}\n$$\n\nNext, we compute the matrix product $X^T X$:\n$$\nX^T X = \\begin{pmatrix} 1.00 & 0.50 & 0.20 \\\\ 0.98 & 0.49 & 0.21 \\end{pmatrix} \\begin{pmatrix} 1.00 & 0.98 \\\\ 0.50 & 0.49 \\\\ 0.20 & 0.21 \\end{pmatrix}\n$$\nThe elements are:\n- $(X^T X)_{11} = (1.00)^2 + (0.50)^2 + (0.20)^2 = 1.00 + 0.25 + 0.04 = 1.29$\n- $(X^T X)_{12} = (1.00)(0.98) + (0.50)(0.49) + (0.20)(0.21) = 0.98 + 0.245 + 0.042 = 1.267$\n- $(X^T X)_{21} = (X^T X)_{12} = 1.267$\n- $(X^T X)_{22} = (0.98)^2 + (0.49)^2 + (0.21)^2 = 0.9604 + 0.2401 + 0.0441 = 1.2446$\nSo,\n$$\nX^T X = \\begin{pmatrix} 1.29 & 1.267 \\\\ 1.267 & 1.2446 \\end{pmatrix}\n$$\n\nNow, we compute the term $\\frac{1}{\\sigma^2} X^T X$, which is the precision matrix of the likelihood:\n$$\n\\frac{1}{\\sigma^2} X^T X = \\frac{1}{0.50} \\begin{pmatrix} 1.29 & 1.267 \\\\ 1.267 & 1.2446 \\end{pmatrix} = 2 \\begin{pmatrix} 1.29 & 1.267 \\\\ 1.267 & 1.2446 \\end{pmatrix} = \\begin{pmatrix} 2.58 & 2.534 \\\\ 2.534 & 2.4892 \\end{pmatrix}\n$$\n\nThe posterior precision matrix, $\\Sigma_N^{-1}$, is the sum of the prior precision and the likelihood precision:\n$$\n\\Sigma_N^{-1} = \\Sigma_0^{-1} + \\frac{1}{\\sigma^2} X^T X = \\begin{pmatrix} 0.1 & 0 \\\\ 0 & 0.1 \\end{pmatrix} + \\begin{pmatrix} 2.58 & 2.534 \\\\ 2.534 & 2.4892 \\end{pmatrix} = \\begin{pmatrix} 2.68 & 2.534 \\\\ 2.534 & 2.5892 \\end{pmatrix}\n$$\n\nFinally, we find the posterior covariance matrix $\\Sigma_N$ by inverting the posterior precision matrix $\\Sigma_N^{-1}$. For a $2 \\times 2$ matrix $A = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$, its inverse is $A^{-1} = \\frac{1}{ad-bc} \\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix}$.\nThe determinant of $\\Sigma_N^{-1}$ is:\n$$\n\\det(\\Sigma_N^{-1}) = (2.68)(2.5892) - (2.534)^2 = 6.939056 - 6.421156 = 0.5179\n$$\nNow, we compute the inverse:\n$$\n\\Sigma_N = \\frac{1}{0.5179} \\begin{pmatrix} 2.5892 & -2.534 \\\\ -2.534 & 2.68 \\end{pmatrix}\n$$\nThe posterior covariance between $\\theta_1$ and $\\theta_2$ is the off-diagonal element of this matrix, $\\Sigma_{N,12}$:\n$$\n\\operatorname{Cov}(\\theta_1, \\theta_2 \\mid y) = \\Sigma_{N,12} = \\frac{-2.534}{0.5179} \\approx -4.8930295...\n$$\nRounding to four significant figures, the posterior covariance is $-4.893$.\n\n**Interpretation of the result:**\nThe design matrix $X$ exhibits near-collinearity, as its columns are almost linearly dependent. The second column is nearly a multiple of the first (e.g., $h_{12}=0.98 \\cdot h_{11}$ and $h_{22}=0.98 \\cdot h_{21}$). This means the model output $y_i \\approx h_{i1}\\theta_1 + h_{i2}\\theta_2$ is sensitive to a specific linear combination of the parameters, approximately $h_{i1}(\\theta_1 + 0.98\\theta_2)$, rather than to $\\theta_1$ and $\\theta_2$ individually.\n\nThis lack of independent influence on the output is a classic parameter identifiability problem. Different pairs of $(\\theta_1, \\theta_2)$ values that keep the sum $\\theta_1 + 0.98\\theta_2$ constant would produce very similar model predictions, and thus have similar likelihood values. This creates a long, narrow \"ridge\" in the likelihood function in the $(\\theta_1, \\theta_2)$ parameter space, oriented along the line $\\theta_1 + 0.98\\theta_2 = \\text{const}$.\n\nThe posterior distribution, $p(\\theta \\mid y)$, is the product of the likelihood and the prior. While the broad, uncorrelated prior $\\mathcal{N}(0, 10I)$ helps to regularize the problem and yield a proper posterior, the shape of this posterior is dominated by the ridge in the likelihood. The result is a posterior distribution shaped like a narrow, tilted ellipse.\n\nThe posterior covariance, $\\operatorname{Cov}(\\theta_1, \\theta_2 \\mid y)$, mathematically captures this relationship.\n- **Sign**: The covariance is large and negative ($-4.893$). This reflects the trade-off identified above: to keep the sum $\\theta_1 + 0.98\\theta_2$ constant, an increase in $\\theta_1$ must be compensated by a decrease in $\\theta_2$. This results in a strong negative correlation in their joint posterior distribution.\n- **Magnitude**: The magnitude of the covariance is large relative to the parameter variances. We can compute the posterior correlation coefficient $\\rho$:\n$$\n\\rho = \\frac{\\operatorname{Cov}(\\theta_1, \\theta_2 \\mid y)}{\\sqrt{\\operatorname{Var}(\\theta_1 \\mid y)\\operatorname{Var}(\\theta_2 \\mid y)}} = \\frac{-2.534/0.5179}{\\sqrt{(2.5892/0.5179)(2.68/0.5179)}} = \\frac{-2.534}{\\sqrt{2.5892 \\times 2.68}} \\approx -0.962\n$$\nA correlation coefficient of $\\approx -0.962$, which is very close to $-1$, confirms an extremely strong negative linear relationship between the parameters in the posterior. This high magnitude indicates that although the data (with the prior) can constrain the parameters to lie along a thin line in the parameter space, they cannot effectively distinguish between different points on that line. This is the hallmark of a severe identifiability issue driven by the experimental design (i.e., the near-collinearity of $X$).",
            "answer": "$$\\boxed{-4.893}$$"
        }
    ]
}