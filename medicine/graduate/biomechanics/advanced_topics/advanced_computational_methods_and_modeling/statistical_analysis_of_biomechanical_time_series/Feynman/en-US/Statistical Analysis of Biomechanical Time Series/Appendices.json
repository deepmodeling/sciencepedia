{
    "hands_on_practices": [
        {
            "introduction": "Before any sophisticated statistical analysis can be performed, we must ensure the integrity of our digital data at the most fundamental level: the moment of acquisition. The Nyquist-Shannon sampling theorem is not just a theoretical guideline; its violation leads to a phenomenon called aliasing, where high-frequency components in a signal masquerade as lower frequencies, irreversibly corrupting the data. This first exercise  provides a practical exploration of this pitfall, demonstrating how common electrical interference can be aliased into the frequency band of a biological signal like gait, and how to predict the exact frequency of this artifact.",
            "id": "4205658",
            "problem": "A laboratory force plate records vertical ground reaction force during steady treadmill walking and is known to be contaminated by an electrical mains interference component at $60$ Hz. The true gait fundamentals in vertical force are near $1$ Hz and its harmonics are well below $10$ Hz. Let the continuous-time signal be modeled as $x(t) = g(t) + a \\cos(2 \\pi f_{i} t)$, where $g(t)$ is a band-limited gait component with most energy below $10$ Hz, $a$ is a constant amplitude, and $f_{i} = 60$ Hz is the interference frequency. The signal is uniformly sampled at rate $f_{s}$ (samples per second) to produce a discrete-time sequence $x[n] = x(n / f_{s})$. Assume no anti-alias filtering is applied prior to sampling.\n\nUsing only the sampling theorem and the definition of the discrete-time Fourier representation of sampled sinusoids, derive from first principles how sub-Nyquist sampling of the $60$ Hz interference produces an aliased sinusoid in the sampled signal. Specifically, determine the frequency $f_{\\text{alias}}$ that appears in the discrete-time baseband when $f_{i} > f_{s}/2$, and explain how inappropriate choices of $f_{s}$ can move the aliased interference into the gait band near $1$ Hz, thereby biasing spectral estimates such as the Power Spectral Density (PSD). Finally, for the case $f_{s} = 100$ Hz, quantify the aliased frequency $f_{\\text{alias}}$ that will be observed in the sampled data. Express the final numerical value of $f_{\\text{alias}}$ in Hz. No rounding is required.",
            "solution": "The problem statement is scientifically grounded, well-posed, objective, and contains sufficient information for a unique solution. It is based on fundamental principles of the Nyquist-Shannon sampling theorem and the phenomenon of aliasing in digital signal processing, which is a common and practical issue in biomechanical data acquisition. The problem is therefore deemed valid.\n\nThe continuous-time signal is given by the model $x(t) = g(t) + a \\cos(2 \\pi f_{i} t)$, where $g(t)$ is the desired gait signal, band-limited to frequencies primarily below $10$ Hz, and the second term represents an interfering sinusoid of frequency $f_{i} = 60$ Hz. This signal is sampled uniformly at a rate of $f_s$ samples per second, without prior anti-alias filtering. The resulting discrete-time sequence is $x[n] = x(t_n)$ where $t_n = n/f_s$ for integer sample indices $n$.\n\nSubstituting $t = n/f_s$ into the interference term gives its discrete-time representation:\n$$ s[n] = a \\cos\\left(2 \\pi f_{i} \\frac{n}{f_s}\\right) = a \\cos\\left( \\left(\\frac{2 \\pi f_{i}}{f_s}\\right) n \\right) $$\nThis is a discrete-time sinusoid of the form $a \\cos(\\omega_0 n)$, where the normalized angular frequency is $\\omega_0 = 2 \\pi f_i / f_s$.\n\nA fundamental property of discrete-time sinusoids is that frequencies are only unique within a $2\\pi$ interval of normalized angular frequency, or equivalently, an $f_s$ interval of cyclical frequency. This is because for any integer $k$:\n$$ \\cos(\\omega_0 n) = \\cos(\\omega_0 n - 2 \\pi k n) = \\cos((\\omega_0 - 2 \\pi k)n) $$\nThe two signals $\\cos(\\omega_0 n)$ and $\\cos((\\omega_0 - 2 \\pi k)n)$ are identical for all integer values of $n$. In terms of cyclical frequencies, this means a frequency $f_i$ is indistinguishable from any frequency $f_i - k f_s$ after sampling.\n\nThe sampling theorem states that to perfectly reconstruct a continuous-time signal from its samples, the sampling frequency $f_s$ must be strictly greater than twice the highest frequency component in the signal, $f_{\\text{max}}$. This critical frequency, $f_s/2$, is known as the Nyquist frequency. The range of unique frequencies in the discrete domain, from $0$ to $f_s/2$, is called the baseband.\n\nWhen a signal is sampled at a rate $f_s$ that is less than $2 f_i$ (i.e., $f_i > f_s/2$), the condition of the sampling theorem is violated for the interference component. The frequency $f_i$ is \"aliased\" to a lower frequency $f_{\\text{alias}}$ that falls within the baseband $[0, f_s/2]$. We can find this aliased frequency by determining which frequency in the baseband is equivalent to $f_i$. This is accomplished by subtracting integer multiples of the sampling frequency $f_s$ from $f_i$ until the result lies within the principal range $[-f_s/2, f_s/2]$, and then taking its absolute value. This can be expressed by the general formula:\n$$ f_{\\text{alias}} = |f_i - k f_s| $$\nwhere the integer $k$ is chosen to map the frequency into the principal alias range. The appropriate value for $k$ is the integer closest to the ratio $f_i/f_s$, which can be written using the rounding function:\n$$ k = \\text{round}\\left(\\frac{f_i}{f_s}\\right) $$\nTherefore, the aliased frequency observed in the baseband is given by:\n$$ f_{\\text{alias}} = \\left|f_i - \\text{round}\\left(\\frac{f_i}{f_s}\\right) f_s\\right| $$\n\nInappropriate choices of $f_s$ can cause this aliased frequency to appear within the frequency band of the gait signal ($0$-$10$ Hz), corrupting the spectral analysis. For instance, the fundamental frequency of gait is often near $1$ Hz. If we want to find a sampling frequency that would alias the $60$ Hz interference to appear at or near $1$ Hz, we would set $f_{\\text{alias}} \\approx 1$ Hz. Using our derived relation:\n$$ |60 - k f_s| \\approx 1 $$\nIf we consider the case where $k=1$, we have $|60 - f_s| \\approx 1$. This implies that choosing a sampling frequency $f_s \\approx 59$ Hz or $f_s \\approx 61$ Hz would be disastrous. For example, if $f_s = 59$ Hz, then $k = \\text{round}(60/59) = \\text{round}(1.017) = 1$. The aliased frequency would be $f_{\\text{alias}} = |60 - 1 \\times 59| = 1$ Hz. A powerful sinusoidal component at $1$ Hz would be introduced into the sampled data, which would be indistinguishable from the true gait signal's fundamental harmonic. This would severely bias any Power Spectral Density (PSD) estimate by creating a large, artificial peak at $1$ Hz, leading to incorrect scientific conclusions about the neuromuscular control of walking.\n\nFinally, for the specific case where the interference frequency is $f_i = 60$ Hz and the sampling rate is $f_s = 100$ Hz, we can quantify the aliased frequency. The Nyquist frequency is $f_s/2 = 50$ Hz. Since $f_i > f_s/2$, aliasing occurs. We use the derived formula:\n$$ k = \\text{round}\\left(\\frac{f_i}{f_s}\\right) = \\text{round}\\left(\\frac{60}{100}\\right) = \\text{round}(0.6) = 1 $$\nThe aliased frequency is then:\n$$ f_{\\text{alias}} = |f_i - k f_s| = |60 - 1 \\times 100| = |-40| = 40 \\text{ Hz} $$\nThus, the $60$ Hz mains interference will appear as a spurious sinusoidal component at a frequency of $40$ Hz in the sampled data. Since $40$ Hz is well outside the gait band of interest (below $10$ Hz), in this specific case, the aliased artifact could potentially be removed with a digital low-pass filter without affecting the gait signal.",
            "answer": "$$\n\\boxed{40}\n$$"
        },
        {
            "introduction": "Once a time series is properly sampled, a key step is to characterize its internal structure, and the autocovariance function is a cornerstone of this process. For any finite-length recording, we can only *estimate* this function, and this exercise  delves into a crucial, and often counterintuitive, aspect of estimation theory: the bias-variance tradeoff. By deriving the variances of two common autocovariance estimators, you will uncover why an \"unbiased\" estimator is not always the best choice and how its variance can increase for large time lags, degrading the estimate's reliability.",
            "id": "4205595",
            "problem": "An investigator is analyzing the envelope of Electromyography (EMG) recordings, modeled as a discrete-time, zero-mean, weakly stationary process $\\{x_t\\}_{t=1}^{N}$ sampled at constant interval $\\Delta t$, with underlying autocovariance function $\\gamma(\\ell) = \\mathbb{E}[x_t x_{t+\\ell}]$ for integer lag $\\ell \\geq 0$. For a finite-length realization of length $N$, define the lag-$\\ell$ sample autocovariance using two conventional normalizations:\n- The \"unbiased\" estimator\n$$\n\\hat{\\gamma}_{u}(\\ell) = \\frac{1}{N - \\ell} \\sum_{t=1}^{N-\\ell} x_t x_{t+\\ell},\n$$\n- The \"biased\" estimator\n$$\n\\hat{\\gamma}_{b}(\\ell) = \\frac{1}{N} \\sum_{t=1}^{N-\\ell} x_t x_{t+\\ell}.\n$$\nStarting from the definitions of weak stationarity, the autocovariance function, and properties of zero-mean Gaussian processes, derive expressions for the variance of each estimator as a function of the lag $\\ell$ and the record length $N$. Use only first principles: linearity of expectation, the definition of covariance, weak stationarity, and the moment identities for zero-mean Gaussian variables.\n\nAssume the EMG envelope can be approximated by a zero-mean Gaussian process with autocovariance $\\gamma(\\ell)$ that depends only on the lag. You may express intermediate results using finite sums over index differences, but you must carry the derivation to a closed-form dependence on the normalizing factors of $\\hat{\\gamma}_{u}(\\ell)$ and $\\hat{\\gamma}_{b}(\\ell)$.\n\nProvide the final answer as a single analytic expression for the ratio of the variances of the unbiased to the biased estimators, expressed only in terms of $N$ and $\\ell$. The ratio is dimensionless. No numerical rounding is required. Your answer must be a calculation.",
            "solution": "The problem requires the derivation of the variance for two estimators of the autocovariance function, $\\hat{\\gamma}_{u}(\\ell)$ and $\\hat{\\gamma}_{b}(\\ell)$, and subsequently, the ratio of these variances. The derivation will be based on first principles for a zero-mean, weakly stationary Gaussian process.\n\nFirst, we state the definitions of the two estimators as provided:\nThe \"unbiased\" estimator is\n$$\n\\hat{\\gamma}_{u}(\\ell) = \\frac{1}{N - \\ell} \\sum_{t=1}^{N-\\ell} x_t x_{t+\\ell}\n$$\nThe \"biased\" estimator is\n$$\n\\hat{\\gamma}_{b}(\\ell) = \\frac{1}{N} \\sum_{t=1}^{N-\\ell} x_t x_{t+\\ell}\n$$\nwhere $\\{x_t\\}$ is a realization of a discrete-time, zero-mean, weakly stationary Gaussian process of length $N$, $\\ell$ is the integer lag, and $\\gamma(\\ell) = \\mathbb{E}[x_t x_{t+\\ell}]$ is the true autocovariance function. The process being zero-mean implies $\\mathbb{E}[x_t] = 0$ for all $t$.\n\nThe core of both estimators is the sum term. Let us define this sum as $S_{\\ell}$:\n$$\nS_{\\ell} = \\sum_{t=1}^{N-\\ell} x_t x_{t+\\ell}\n$$\nThe estimators can then be written as:\n$$\n\\hat{\\gamma}_{u}(\\ell) = \\frac{1}{N - \\ell} S_{\\ell}\n$$\n$$\n\\hat{\\gamma}_{b}(\\ell) = \\frac{1}{N} S_{\\ell}\n$$\nThe variance of a random variable $Y$ scaled by a constant $c$ is $\\text{Var}[cY] = c^2 \\text{Var}[Y]$. This is a fundamental property derived from the definition of variance. Applying this property, we can express the variances of the estimators in terms of the variance of $S_{\\ell}$:\n$$\n\\text{Var}[\\hat{\\gamma}_{u}(\\ell)] = \\text{Var}\\left[\\frac{1}{N - \\ell} S_{\\ell}\\right] = \\frac{1}{(N - \\ell)^2} \\text{Var}[S_{\\ell}]\n$$\n$$\n\\text{Var}[\\hat{\\gamma}_{b}(\\ell)] = \\text{Var}\\left[\\frac{1}{N} S_{\\ell}\\right] = \\frac{1}{N^2} \\text{Var}[S_{\\ell}]\n$$\nTo find the expressions for the variances of the estimators as requested, we must derive the variance of $S_{\\ell}$. The variance of $S_{\\ell}$ is given by $\\text{Var}[S_{\\ell}] = \\mathbb{E}[S_{\\ell}^2] - (\\mathbb{E}[S_{\\ell}])^2$.\n\nFirst, we calculate the expectation of $S_{\\ell}$, $\\mathbb{E}[S_{\\ell}]$. Using the linearity of the expectation operator:\n$$\n\\mathbb{E}[S_{\\ell}] = \\mathbb{E}\\left[\\sum_{t=1}^{N-\\ell} x_t x_{t+\\ell}\\right] = \\sum_{t=1}^{N-\\ell} \\mathbb{E}[x_t x_{t+\\ell}]\n$$\nDue to weak stationarity, $\\mathbb{E}[x_t x_{t+\\ell}] = \\gamma(\\ell)$ is independent of $t$.\n$$\n\\mathbb{E}[S_{\\ell}] = \\sum_{t=1}^{N-\\ell} \\gamma(\\ell) = (N-\\ell)\\gamma(\\ell)\n$$\n\nNext, we calculate the second moment of $S_{\\ell}$, $\\mathbb{E}[S_{\\ell}^2]$.\n$$\n\\mathbb{E}[S_{\\ell}^2] = \\mathbb{E}\\left[ \\left(\\sum_{t=1}^{N-\\ell} x_t x_{t+\\ell}\\right) \\left(\\sum_{s=1}^{N-\\ell} x_s x_{s+\\ell}\\right) \\right] = \\sum_{t=1}^{N-\\ell} \\sum_{s=1}^{N-\\ell} \\mathbb{E}[x_t x_{t+\\ell} x_s x_{s+\\ell}]\n$$\nThe term inside the sum is a fourth-order moment of the process variables. Since the process is assumed to be Gaussian and has zero mean, we can apply Isserlis' theorem (also known as Wick's theorem for Gaussian variables), which states that for any four zero-mean Gaussian random variables $Z_1, Z_2, Z_3, Z_4$:\n$$\n\\mathbb{E}[Z_1 Z_2 Z_3 Z_4] = \\mathbb{E}[Z_1 Z_2]\\mathbb{E}[Z_3 Z_4] + \\mathbb{E}[Z_1 Z_3]\\mathbb{E}[Z_2 Z_4] + \\mathbb{E}[Z_1 Z_4]\\mathbb{E}[Z_2 Z_3]\n$$\nApplying this to our variables $x_t, x_{t+\\ell}, x_s, x_{s+\\ell}$:\n$$\n\\mathbb{E}[x_t x_{t+\\ell} x_s x_{s+\\ell}] = \\mathbb{E}[x_t x_{t+\\ell}]\\mathbb{E}[x_s x_{s+\\ell}] + \\mathbb{E}[x_t x_s]\\mathbb{E}[x_{t+\\ell} x_{s+\\ell}] + \\mathbb{E}[x_t x_{s+\\ell}]\\mathbb{E}[x_{t+\\ell} x_s]\n$$\nUsing the definition of the autocovariance function $\\gamma(k) = \\mathbb{E}[x_i x_{i+k}]$, and noting that due to stationarity, $\\gamma(k)$ depends only on the lag $k$ and is an even function, i.e., $\\gamma(k) = \\gamma(-k)$:\n\\begin{itemize}\n    \\item $\\mathbb{E}[x_t x_{t+\\ell}] = \\gamma(\\ell)$\n    \\item $\\mathbb{E}[x_s x_{s+\\ell}] = \\gamma(\\ell)$\n    \\item $\\mathbb{E}[x_t x_s] = \\gamma(s-t)$\n    \\item $\\mathbb{E}[x_{t+\\ell} x_{s+\\ell}] = \\gamma((s+\\ell) - (t+\\ell)) = \\gamma(s-t)$\n    \\item $\\mathbb{E}[x_t x_{s+\\ell}] = \\gamma(s+\\ell-t)$\n    \\item $\\mathbb{E}[x_{t+\\ell} x_s] = \\gamma(s-(t+\\ell)) = \\gamma(s-t-\\ell)$\n\\end{itemize}\nSubstituting these into the moment expansion gives:\n$$\n\\mathbb{E}[x_t x_{t+\\ell} x_s x_{s+\\ell}] = \\gamma(\\ell)^2 + \\gamma(s-t)^2 + \\gamma(s-t+\\ell)\\gamma(s-t-\\ell)\n$$\nNow, substitute this back into the expression for $\\mathbb{E}[S_{\\ell}^2]$:\n$$\n\\mathbb{E}[S_{\\ell}^2] = \\sum_{t=1}^{N-\\ell} \\sum_{s=1}^{N-\\ell} \\left( \\gamma(\\ell)^2 + \\gamma(s-t)^2 + \\gamma(s-t+\\ell)\\gamma(s-t-\\ell) \\right)\n$$\nWe can separate the term $\\gamma(\\ell)^2$, which is constant with respect to the summation indices $s$ and $t$:\n$$\n\\mathbb{E}[S_{\\ell}^2] = \\sum_{t=1}^{N-\\ell} \\sum_{s=1}^{N-\\ell} \\gamma(\\ell)^2 + \\sum_{t=1}^{N-\\ell} \\sum_{s=1}^{N-\\ell} \\left( \\gamma(s-t)^2 + \\gamma(s-t+\\ell)\\gamma(s-t-\\ell) \\right)\n$$\n$$\n\\mathbb{E}[S_{\\ell}^2] = (N-\\ell)^2 \\gamma(\\ell)^2 + \\sum_{t=1}^{N-\\ell} \\sum_{s=1}^{N-\\ell} \\left( \\gamma(s-t)^2 + \\gamma(s-t+\\ell)\\gamma(s-t-\\ell) \\right)\n$$\nNow we can compute $\\text{Var}[S_{\\ell}]$:\n$$\n\\text{Var}[S_{\\ell}] = \\mathbb{E}[S_{\\ell}^2] - (\\mathbb{E}[S_{\\ell}])^2\n$$\n$$\n\\text{Var}[S_{\\ell}] = \\left[ (N-\\ell)^2 \\gamma(\\ell)^2 + \\sum_{t,s=1}^{N-\\ell} \\left( \\gamma(s-t)^2 + \\gamma(s-t+\\ell)\\gamma(s-t-\\ell) \\right) \\right] - \\left( (N-\\ell)\\gamma(\\ell) \\right)^2\n$$\n$$\n\\text{Var}[S_{\\ell}] = \\sum_{t=1}^{N-\\ell} \\sum_{s=1}^{N-\\ell} \\left( \\gamma(s-t)^2 + \\gamma(s-t+\\ell)\\gamma(s-t-\\ell) \\right)\n$$\nThis is the general expression for the variance of the unnormalized sum $S_{\\ell}$, often related to Bartlett's formula.\n\nWith this result, we can write the explicit expressions for the variance of each estimator, showing their dependence on their respective normalizing factors as required:\n$$\n\\text{Var}[\\hat{\\gamma}_{u}(\\ell)] = \\frac{1}{(N-\\ell)^2} \\text{Var}[S_{\\ell}] = \\frac{1}{(N-\\ell)^2} \\sum_{t=1}^{N-\\ell} \\sum_{s=1}^{N-\\ell} \\left( \\gamma(s-t)^2 + \\gamma(s-t+\\ell)\\gamma(s-t-\\ell) \\right)\n$$\n$$\n\\text{Var}[\\hat{\\gamma}_{b}(\\ell)] = \\frac{1}{N^2} \\text{Var}[S_{\\ell}] = \\frac{1}{N^2} \\sum_{t=1}^{N-\\ell} \\sum_{s=1}^{N-\\ell} \\left( \\gamma(s-t)^2 + \\gamma(s-t+\\ell)\\gamma(s-t-\\ell) \\right)\n$$\nThese expressions fulfill the requirement to derive the variance for each estimator using first principles.\n\nFinally, we are asked to find the ratio of these variances, $\\frac{\\text{Var}[\\hat{\\gamma}_{u}(\\ell)]}{\\text{Var}[\\hat{\\gamma}_{b}(\\ell)]}$.\n$$\n\\frac{\\text{Var}[\\hat{\\gamma}_{u}(\\ell)]}{\\text{Var}[\\hat{\\gamma}_{b}(\\ell)]} = \\frac{\\frac{1}{(N-\\ell)^2} \\sum_{t,s} \\left( \\gamma(s-t)^2 + \\dots \\right)}{\\frac{1}{N^2} \\sum_{t,s} \\left( \\gamma(s-t)^2 + \\dots \\right)}\n$$\nAssuming the process is not trivial, such that $\\text{Var}[S_{\\ell}] \\neq 0$, the double summation term, which is common to both the numerator and the denominator, cancels out.\n$$\n\\frac{\\text{Var}[\\hat{\\gamma}_{u}(\\ell)]}{\\text{Var}[\\hat{\\gamma}_{b}(\\ell)]} = \\frac{1/(N-\\ell)^2}{1/N^2} = \\frac{N^2}{(N-\\ell)^2}\n$$\nThis simplifies to the final expression, which depends only on the record length $N$ and the lag $\\ell$.\n$$\n\\frac{\\text{Var}[\\hat{\\gamma}_{u}(\\ell)]}{\\text{Var}[\\hat{\\gamma}_{b}(\\ell)]} = \\left(\\frac{N}{N-\\ell}\\right)^2\n$$",
            "answer": "$$\\boxed{\\left(\\frac{N}{N-\\ell}\\right)^{2}}$$"
        },
        {
            "introduction": "Characterizing a signal with its autocovariance is descriptive, but building a parametric model allows for prediction, simulation, and deeper insight into the underlying process. Autoregressive (AR) models provide a powerful framework for this, representing a signal's current value as a linear combination of its past values. This practice  bridges the gap between signal characterization and modeling by having you derive the famous Yule–Walker equations, which provide the direct mathematical link between a process's autocovariance and its AR coefficients, and then apply them to a realistic biomechanical signal.",
            "id": "4205673",
            "problem": "Consider a zero-mean ankle angle velocity time series $\\{v_t\\}$ recorded during steady treadmill walking at sampling frequency $f_s = 100 \\,\\mathrm{Hz}$. Assume $\\{v_t\\}$ is wide-sense stationary and can be modeled as an autoregressive process of order $p$ (AR($p$)) driven by a zero-mean white innovation process. Starting only from the definitions of a wide-sense stationary process, the linear autoregressive model, and the orthogonality of the one-step-ahead prediction error to past samples, derive the system of equations that relate the autoregressive coefficients to the autocovariance function (the Yule–Walker equations) for a general AR($p$) process.\n\nThen, treat the following empirically estimated autocovariances (computed from a long, stationary segment and expressed in $(\\mathrm{rad}/\\mathrm{s})^{2}$) as accurate surrogates of the true autocovariances: $\\hat{\\gamma}(0) = 0.80$, $\\hat{\\gamma}(1) = 0.50$, and $\\hat{\\gamma}(2) = 0.20$. Using your derived equations with $p=2$, compute the AR(2) coefficients $\\phi_1$ and $\\phi_2$ for the ankle angle velocity model. Express $\\phi_1$ and $\\phi_2$ as dimensionless numbers, and round your answers to four significant figures.",
            "solution": "The problem is assessed as valid. It is scientifically grounded in the principles of time series analysis, well-posed with sufficient and consistent information, and objectively stated. The task is to first derive the Yule–Walker equations for a general autoregressive process of order $p$, AR($p$), and then use these equations to compute the coefficients for a specific AR($2$) model based on given autocovariance values.\n\nFirst, we derive the Yule–Walker equations. We are given the following definitions and properties:\n1.  The time series $\\{v_t\\}$ is wide-sense stationary (WSS). For a zero-mean process as specified, this implies its autocovariance function, $\\gamma(k) = E[v_t v_{t-k}]$, depends only on the time lag $k$ and not on the time $t$. A property of the autocovariance function is that it is even, i.e., $\\gamma(k) = \\gamma(-k)$.\n2.  The process is modeled as a linear autoregressive model of order $p$, AR($p$):\n    $$v_t = \\sum_{i=1}^{p} \\phi_i v_{t-i} + \\epsilon_t$$\n    where $\\phi_1, \\phi_2, \\dots, \\phi_p$ are the autoregressive coefficients and $\\{\\epsilon_t\\}$ is a zero-mean white innovation process with variance $\\sigma_\\epsilon^2 = E[\\epsilon_t^2]$.\n3.  The one-step-ahead prediction error, which is the innovation term $\\epsilon_t$, is orthogonal to (uncorrelated with) the past samples used for prediction. Mathematically, this is expressed as $E[\\epsilon_t v_{s}] = 0$ for all $s < t$.\n\nTo derive the relationship between the coefficients $\\{\\phi_i\\}$ and the autocovariance function $\\gamma(k)$, we start with the AR($p$) model definition. We multiply both sides of the equation by $v_{t-k}$ for a lag $k > 0$:\n$$v_t v_{t-k} = \\left( \\sum_{i=1}^{p} \\phi_i v_{t-i} \\right) v_{t-k} + \\epsilon_t v_{t-k}$$\n\nNext, we take the expectation of both sides. By the linearity of the expectation operator, we have:\n$$E[v_t v_{t-k}] = E\\left[ \\sum_{i=1}^{p} \\phi_i v_{t-i} v_{t-k} \\right] + E[\\epsilon_t v_{t-k}]$$\n$$E[v_t v_{t-k}] = \\sum_{i=1}^{p} \\phi_i E[v_{t-i} v_{t-k}] + E[\\epsilon_t v_{t-k}]$$\n\nWe now substitute the definition of the autocovariance function. The left-hand side is $E[v_t v_{t-k}] = \\gamma(k)$. Since the process is stationary, the term inside the summation is $E[v_{t-i} v_{t-k}] = \\gamma((t-k) - (t-i)) = \\gamma(i-k)$. Because the autocovariance is an even function, $\\gamma(i-k) = \\gamma(k-i)$. The equation becomes:\n$$\\gamma(k) = \\sum_{i=1}^{p} \\phi_i \\gamma(k-i) + E[\\epsilon_t v_{t-k}]$$\n\nNow we analyze the term $E[\\epsilon_t v_{t-k}]$. Since we have chosen $k > 0$, the time index $t-k$ is always less than $t$. From the orthogonality principle, the innovation $\\epsilon_t$ is uncorrelated with all past values of the process. Therefore, for $k > 0$:\n$$E[\\epsilon_t v_{t-k}] = 0$$\n\nSubstituting this result back into our equation, we obtain the Yule–Walker equations:\n$$\\gamma(k) = \\sum_{i=1}^{p} \\phi_i \\gamma(k-i), \\quad \\text{for } k = 1, 2, \\dots, p$$\n\nThis represents a system of $p$ linear equations for the $p$ unknown coefficients $\\phi_1, \\dots, \\phi_p$. Explicitly, the system is:\n\\begin{align*}\n\\gamma(1) &= \\phi_1 \\gamma(0) + \\phi_2 \\gamma(1) + \\dots + \\phi_p \\gamma(p-1) \\\\\n\\gamma(2) &= \\phi_1 \\gamma(1) + \\phi_2 \\gamma(0) + \\dots + \\phi_p \\gamma(p-2) \\\\\n&\\vdots \\\\\n\\gamma(p) &= \\phi_1 \\gamma(p-1) + \\phi_2 \\gamma(p-2) + \\dots + \\phi_p \\gamma(0)\n\\end{align*}\n\nNow, we apply this general result to the specific case given in the problem, where the process is AR($2$) (i.e., $p=2$). The system of Yule–Walker equations for $p=2$ is:\n\\begin{align*}\n\\gamma(1) &= \\phi_1 \\gamma(0) + \\phi_2 \\gamma(1) \\\\\n\\gamma(2) &= \\phi_1 \\gamma(1) + \\phi_2 \\gamma(0)\n\\end{align*}\n\nThe problem provides the following autocovariance values, which are treated as true for this calculation:\n$\\gamma(0) = 0.80$\n$\\gamma(1) = 0.50$\n$\\gamma(2) = 0.20$\n\nSubstituting these values into the system of equations, we get:\n\\begin{align*}\n0.50 &= \\phi_1(0.80) + \\phi_2(0.50) \\\\\n0.20 &= \\phi_1(0.50) + \\phi_2(0.80)\n\\end{align*}\nThis is a system of two linear equations in two unknowns, $\\phi_1$ and $\\phi_2$. We can write this in matrix form $\\boldsymbol{\\gamma} = \\boldsymbol{\\Gamma} \\boldsymbol{\\phi}$:\n$$\n\\begin{pmatrix} 0.50 \\\\ 0.20 \\end{pmatrix} = \\begin{pmatrix} 0.80 & 0.50 \\\\ 0.50 & 0.80 \\end{pmatrix} \\begin{pmatrix} \\phi_1 \\\\ \\phi_2 \\end{pmatrix}\n$$\nTo solve for $\\phi_1$ and $\\phi_2$, we can use various methods, such as substitution or matrix inversion. Using substitution, we can solve the first equation for $\\phi_1$:\n$$0.80\\phi_1 = 0.50 - 0.50\\phi_2 \\implies \\phi_1 = \\frac{0.50 - 0.50\\phi_2}{0.80} = \\frac{5 - 5\\phi_2}{8}$$\nSubstitute this expression for $\\phi_1$ into the second equation:\n$$0.20 = 0.50 \\left( \\frac{5 - 5\\phi_2}{8} \\right) + 0.80\\phi_2$$\nTo simplify, multiply the entire equation by $8$:\n$$1.60 = 0.50(5 - 5\\phi_2) + 6.4\\phi_2$$\n$$1.60 = 2.5 - 2.5\\phi_2 + 6.4\\phi_2$$\n$$1.60 = 2.5 + 3.9\\phi_2$$\n$$3.9\\phi_2 = 1.60 - 2.5 = -0.90$$\n$$\\phi_2 = -\\frac{0.90}{3.9} = -\\frac{9}{39} = -\\frac{3}{13}$$\nNow, substitute the value of $\\phi_2$ back into the expression for $\\phi_1$:\n$$\\phi_1 = \\frac{5 - 5(-\\frac{3}{13})}{8} = \\frac{5 + \\frac{15}{13}}{8} = \\frac{\\frac{65+15}{13}}{8} = \\frac{\\frac{80}{13}}{8} = \\frac{10}{13}$$\n\nFinally, we compute the numerical values and round to four significant figures as requested.\n$$\\phi_1 = \\frac{10}{13} \\approx 0.7692307... \\approx 0.7692$$\n$$\\phi_2 = -\\frac{3}{13} \\approx -0.230769... \\approx -0.2308$$\nThe coefficients are dimensionless, which is consistent with the formulation of the AR model.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.7692 & -0.2308\n\\end{pmatrix}\n}\n$$"
        }
    ]
}