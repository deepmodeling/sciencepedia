{
    "hands_on_practices": [
        {
            "introduction": "Any digital analysis of biomechanical data begins with sampling a continuous, real-world signal. This exercise explores the critical concept of aliasing, a phenomenon where high-frequency components, such as electrical noise, can be misrepresented as low-frequency signals within the band of interest if the sampling rate is not sufficiently high. Mastering the principles of aliasing  is a foundational skill for ensuring data integrity and preventing serious measurement errors in experimental biomechanics.",
            "id": "4205658",
            "problem": "A laboratory force plate records vertical ground reaction force during steady treadmill walking and is known to be contaminated by an electrical mains interference component at $60$ Hz. The true gait fundamentals in vertical force are near $1$ Hz and its harmonics are well below $10$ Hz. Let the continuous-time signal be modeled as $x(t) = g(t) + a \\cos(2 \\pi f_{i} t)$, where $g(t)$ is a band-limited gait component with most energy below $10$ Hz, $a$ is a constant amplitude, and $f_{i} = 60$ Hz is the interference frequency. The signal is uniformly sampled at rate $f_{s}$ (samples per second) to produce a discrete-time sequence $x[n] = x(n / f_{s})$. Assume no anti-alias filtering is applied prior to sampling.\n\nUsing only the sampling theorem and the definition of the discrete-time Fourier representation of sampled sinusoids, derive from first principles how sub-Nyquist sampling of the $60$ Hz interference produces an aliased sinusoid in the sampled signal. Specifically, determine the frequency $f_{\\text{alias}}$ that appears in the discrete-time baseband when $f_{i}  f_{s}/2$, and explain how inappropriate choices of $f_{s}$ can move the aliased interference into the gait band near $1$ Hz, thereby biasing spectral estimates such as the Power Spectral Density (PSD). Finally, for the case $f_{s} = 100$ Hz, quantify the aliased frequency $f_{\\text{alias}}$ that will be observed in the sampled data. Express the final numerical value of $f_{\\text{alias}}$ in Hz. No rounding is required.",
            "solution": "The problem statement is scientifically grounded, well-posed, objective, and contains sufficient information for a unique solution. It is based on fundamental principles of the Nyquist-Shannon sampling theorem and the phenomenon of aliasing in digital signal processing, which is a common and practical issue in biomechanical data acquisition. The problem is therefore deemed valid.\n\nThe continuous-time signal is given by the model $x(t) = g(t) + a \\cos(2 \\pi f_{i} t)$, where $g(t)$ is the desired gait signal, band-limited to frequencies primarily below $10$ Hz, and the second term represents an interfering sinusoid of frequency $f_{i} = 60$ Hz. This signal is sampled uniformly at a rate of $f_s$ samples per second, without prior anti-alias filtering. The resulting discrete-time sequence is $x[n] = x(t_n)$ where $t_n = n/f_s$ for integer sample indices $n$.\n\nSubstituting $t = n/f_s$ into the interference term gives its discrete-time representation:\n$$ s[n] = a \\cos\\left(2 \\pi f_{i} \\frac{n}{f_s}\\right) = a \\cos\\left( \\left(\\frac{2 \\pi f_{i}}{f_s}\\right) n \\right) $$\nThis is a discrete-time sinusoid of the form $a \\cos(\\omega_0 n)$, where the normalized angular frequency is $\\omega_0 = 2 \\pi f_i / f_s$.\n\nA fundamental property of discrete-time sinusoids is that frequencies are only unique within a $2\\pi$ interval of normalized angular frequency, or equivalently, an $f_s$ interval of cyclical frequency. This is because for any integer $k$:\n$$ \\cos(\\omega_0 n) = \\cos(\\omega_0 n - 2 \\pi k n) = \\cos((\\omega_0 - 2 \\pi k)n) $$\nThe two signals $\\cos(\\omega_0 n)$ and $\\cos((\\omega_0 - 2 \\pi k)n)$ are identical for all integer values of $n$. In terms of cyclical frequencies, this means a frequency $f_i$ is indistinguishable from any frequency $f_i - k f_s$ after sampling.\n\nThe sampling theorem states that to perfectly reconstruct a continuous-time signal from its samples, the sampling frequency $f_s$ must be strictly greater than twice the highest frequency component in the signal, $f_{\\text{max}}$. This critical frequency, $f_s/2$, is known as the Nyquist frequency. The range of unique frequencies in the discrete domain, from $0$ to $f_s/2$, is called the baseband.\n\nWhen a signal is sampled at a rate $f_s$ that is less than $2 f_i$ (i.e., $f_i  f_s/2$), the condition of the sampling theorem is violated for the interference component. The frequency $f_i$ is \"aliased\" to a lower frequency $f_{\\text{alias}}$ that falls within the baseband $[0, f_s/2]$. We can find this aliased frequency by determining which frequency in the baseband is equivalent to $f_i$. This is accomplished by subtracting integer multiples of the sampling frequency $f_s$ from $f_i$ until the result lies within the principal range $[-f_s/2, f_s/2]$, and then taking its absolute value. This can be expressed by the general formula:\n$$ f_{\\text{alias}} = |f_i - k f_s| $$\nwhere the integer $k$ is chosen to map the frequency into the principal alias range. The appropriate value for $k$ is the integer closest to the ratio $f_i/f_s$, which can be written using the rounding function:\n$$ k = \\text{round}\\left(\\frac{f_i}{f_s}\\right) $$\nTherefore, the aliased frequency observed in the baseband is given by:\n$$ f_{\\text{alias}} = \\left|f_i - \\text{round}\\left(\\frac{f_i}{f_s}\\right) f_s\\right| $$\n\nInappropriate choices of $f_s$ can cause this aliased frequency to appear within the frequency band of the gait signal ($0$-$10$ Hz), corrupting the spectral analysis. For instance, the fundamental frequency of gait is often near $1$ Hz. If we want to find a sampling frequency that would alias the $60$ Hz interference to appear at or near $1$ Hz, we would set $f_{\\text{alias}} \\approx 1$ Hz. Using our derived relation:\n$$ |60 - k f_s| \\approx 1 $$\nIf we consider the case where $k=1$, we have $|60 - f_s| \\approx 1$. This implies that choosing a sampling frequency $f_s \\approx 59$ Hz or $f_s \\approx 61$ Hz would be disastrous. For example, if $f_s = 59$ Hz, then $k = \\text{round}(60/59) = \\text{round}(1.017) = 1$. The aliased frequency would be $f_{\\text{alias}} = |60 - 1 \\times 59| = 1$ Hz. A powerful sinusoidal component at $1$ Hz would be introduced into the sampled data, which would be indistinguishable from the true gait signal's fundamental harmonic. This would severely bias any Power Spectral Density (PSD) estimate by creating a large, artificial peak at $1$ Hz, leading to incorrect scientific conclusions about the neuromuscular control of walking.\n\nFinally, for the specific case where the interference frequency is $f_i = 60$ Hz and the sampling rate is $f_s = 100$ Hz, we can quantify the aliased frequency. The Nyquist frequency is $f_s/2 = 50$ Hz. Since $f_i  f_s/2$, aliasing occurs. We use the derived formula:\n$$ k = \\text{round}\\left(\\frac{f_i}{f_s}\\right) = \\text{round}\\left(\\frac{60}{100}\\right) = \\text{round}(0.6) = 1 $$\nThe aliased frequency is then:\n$$ f_{\\text{alias}} = |f_i - k f_s| = |60 - 1 \\times 100| = |-40| = 40 \\text{ Hz} $$\nThus, the $60$ Hz mains interference will appear as a spurious sinusoidal component at a frequency of $40$ Hz in the sampled data. Since $40$ Hz is well outside the gait band of interest (below $10$ Hz), in this specific case, the aliased artifact could potentially be removed with a digital low-pass filter without affecting the gait signal.",
            "answer": "$$\n\\boxed{40}\n$$"
        },
        {
            "introduction": "Once a clean time series has been acquired, a powerful next step is to create a mathematical model that parsimoniously captures its dynamics. This practice introduces autoregressive (AR) models, which represent a signal's current value based on a weighted sum of its past values, a technique frequently used for system identification and spectral analysis. By deriving the Yule-Walker equations from first principles and applying them to model ankle kinematics , you will bridge the gap between abstract statistical theory and practical biomechanical modeling.",
            "id": "4205673",
            "problem": "Consider a zero-mean ankle angle velocity time series $\\{v_t\\}$ recorded during steady treadmill walking at sampling frequency $f_s = 100 \\,\\mathrm{Hz}$. Assume $\\{v_t\\}$ is wide-sense stationary and can be modeled as an autoregressive process of order $p$ (AR($p$)) driven by a zero-mean white innovation process. Starting only from the definitions of a wide-sense stationary process, the linear autoregressive model, and the orthogonality of the one-step-ahead prediction error to past samples, derive the system of equations that relate the autoregressive coefficients to the autocovariance function (the Yule–Walker equations) for a general AR($p$) process.\n\nThen, treat the following empirically estimated autocovariances (computed from a long, stationary segment and expressed in $(\\mathrm{rad}/\\mathrm{s})^{2}$) as accurate surrogates of the true autocovariances: $\\hat{\\gamma}(0) = 0.80$, $\\hat{\\gamma}(1) = 0.50$, and $\\hat{\\gamma}(2) = 0.20$. Using your derived equations with $p=2$, compute the AR(2) coefficients $\\phi_1$ and $\\phi_2$ for the ankle angle velocity model. Express $\\phi_1$ and $\\phi_2$ as dimensionless numbers, and round your answers to four significant figures.",
            "solution": "The problem is assessed as valid. It is scientifically grounded in the principles of time series analysis, well-posed with sufficient and consistent information, and objectively stated. The task is to first derive the Yule–Walker equations for a general autoregressive process of order $p$, AR($p$), and then use these equations to compute the coefficients for a specific AR($2$) model based on given autocovariance values.\n\nFirst, we derive the Yule–Walker equations. We are given the following definitions and properties:\n1.  The time series $\\{v_t\\}$ is wide-sense stationary (WSS). For a zero-mean process as specified, this implies its autocovariance function, $\\gamma(k) = E[v_t v_{t-k}]$, depends only on the time lag $k$ and not on the time $t$. A property of the autocovariance function is that it is even, i.e., $\\gamma(k) = \\gamma(-k)$.\n2.  The process is modeled as a linear autoregressive model of order $p$, AR($p$):\n    $$v_t = \\sum_{i=1}^{p} \\phi_i v_{t-i} + \\epsilon_t$$\n    where $\\phi_1, \\phi_2, \\dots, \\phi_p$ are the autoregressive coefficients and $\\{\\epsilon_t\\}$ is a zero-mean white innovation process with variance $\\sigma_\\epsilon^2 = E[\\epsilon_t^2]$.\n3.  The one-step-ahead prediction error, which is the innovation term $\\epsilon_t$, is orthogonal to (uncorrelated with) the past samples used for prediction. Mathematically, this is expressed as $E[\\epsilon_t v_{s}] = 0$ for all $s  t$.\n\nTo derive the relationship between the coefficients $\\{\\phi_i\\}$ and the autocovariance function $\\gamma(k)$, we start with the AR($p$) model definition. We multiply both sides of the equation by $v_{t-k}$ for a lag $k  0$:\n$$v_t v_{t-k} = \\left( \\sum_{i=1}^{p} \\phi_i v_{t-i} \\right) v_{t-k} + \\epsilon_t v_{t-k}$$\n\nNext, we take the expectation of both sides. By the linearity of the expectation operator, we have:\n$$E[v_t v_{t-k}] = E\\left[ \\sum_{i=1}^{p} \\phi_i v_{t-i} v_{t-k} \\right] + E[\\epsilon_t v_{t-k}]$$\n$$E[v_t v_{t-k}] = \\sum_{i=1}^{p} \\phi_i E[v_{t-i} v_{t-k}] + E[\\epsilon_t v_{t-k}]$$\n\nWe now substitute the definition of the autocovariance function. The left-hand side is $E[v_t v_{t-k}] = \\gamma(k)$. Since the process is stationary, the term inside the summation is $E[v_{t-i} v_{t-k}] = \\gamma((t-k) - (t-i)) = \\gamma(i-k)$. Because the autocovariance is an even function, $\\gamma(i-k) = \\gamma(k-i)$. The equation becomes:\n$$\\gamma(k) = \\sum_{i=1}^{p} \\phi_i \\gamma(k-i) + E[\\epsilon_t v_{t-k}]$$\n\nNow we analyze the term $E[\\epsilon_t v_{t-k}]$. Since we have chosen $k  0$, the time index $t-k$ is always less than $t$. From the orthogonality principle, the innovation $\\epsilon_t$ is uncorrelated with all past values of the process. Therefore, for $k  0$:\n$$E[\\epsilon_t v_{t-k}] = 0$$\n\nSubstituting this result back into our equation, we obtain the Yule–Walker equations:\n$$\\gamma(k) = \\sum_{i=1}^{p} \\phi_i \\gamma(k-i), \\quad \\text{for } k = 1, 2, \\dots, p$$\n\nThis represents a system of $p$ linear equations for the $p$ unknown coefficients $\\phi_1, \\dots, \\phi_p$. Explicitly, the system is:\n\\begin{align*}\n\\gamma(1) = \\phi_1 \\gamma(0) + \\phi_2 \\gamma(1) + \\dots + \\phi_p \\gamma(p-1) \\\\\n\\gamma(2) = \\phi_1 \\gamma(1) + \\phi_2 \\gamma(0) + \\dots + \\phi_p \\gamma(p-2) \\\\\n\\vdots \\\\\n\\gamma(p) = \\phi_1 \\gamma(p-1) + \\phi_2 \\gamma(p-2) + \\dots + \\phi_p \\gamma(0)\n\\end{align*}\n\nNow, we apply this general result to the specific case given in the problem, where the process is AR($2$) (i.e., $p=2$). The system of Yule–Walker equations for $p=2$ is:\n\\begin{align*}\n\\gamma(1) = \\phi_1 \\gamma(0) + \\phi_2 \\gamma(1) \\\\\n\\gamma(2) = \\phi_1 \\gamma(1) + \\phi_2 \\gamma(0)\n\\end{align*}\n\nThe problem provides the following autocovariance values, which are treated as true for this calculation:\n$\\gamma(0) = 0.80$\n$\\gamma(1) = 0.50$\n$\\gamma(2) = 0.20$\n\nSubstituting these values into the system of equations, we get:\n\\begin{align*}\n0.50 = \\phi_1(0.80) + \\phi_2(0.50) \\\\\n0.20 = \\phi_1(0.50) + \\phi_2(0.80)\n\\end{align*}\nThis is a system of two linear equations in two unknowns, $\\phi_1$ and $\\phi_2$. We can write this in matrix form $\\boldsymbol{\\gamma} = \\boldsymbol{\\Gamma} \\boldsymbol{\\phi}$:\n$$\n\\begin{pmatrix} 0.50 \\\\ 0.20 \\end{pmatrix} = \\begin{pmatrix} 0.80  0.50 \\\\ 0.50  0.80 \\end{pmatrix} \\begin{pmatrix} \\phi_1 \\\\ \\phi_2 \\end{pmatrix}\n$$\nTo solve for $\\phi_1$ and $\\phi_2$, we can use various methods, such as substitution or matrix inversion. Using substitution, we can solve the first equation for $\\phi_1$:\n$$0.80\\phi_1 = 0.50 - 0.50\\phi_2 \\implies \\phi_1 = \\frac{0.50 - 0.50\\phi_2}{0.80} = \\frac{5 - 5\\phi_2}{8}$$\nSubstitute this expression for $\\phi_1$ into the second equation:\n$$0.20 = 0.50 \\left( \\frac{5 - 5\\phi_2}{8} \\right) + 0.80\\phi_2$$\nTo simplify, multiply the entire equation by $8$:\n$$1.60 = 0.50(5 - 5\\phi_2) + 6.4\\phi_2$$\n$$1.60 = 2.5 - 2.5\\phi_2 + 6.4\\phi_2$$\n$$1.60 = 2.5 + 3.9\\phi_2$$\n$$3.9\\phi_2 = 1.60 - 2.5 = -0.90$$\n$$\\phi_2 = -\\frac{0.90}{3.9} = -\\frac{9}{39} = -\\frac{3}{13}$$\nNow, substitute the value of $\\phi_2$ back into the expression for $\\phi_1$:\n$$\\phi_1 = \\frac{5 - 5(-\\frac{3}{13})}{8} = \\frac{5 + \\frac{15}{13}}{8} = \\frac{\\frac{65+15}{13}}{8} = \\frac{\\frac{80}{13}}{8} = \\frac{10}{13}$$\n\nFinally, we compute the numerical values and round to four significant figures as requested.\n$$\\phi_1 = \\frac{10}{13} \\approx 0.7692307... \\approx 0.7692$$\n$$\\phi_2 = -\\frac{3}{13} \\approx -0.230769... \\approx -0.2308$$\nThe coefficients are dimensionless, which is consistent with the formulation of the AR model.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.7692  -0.2308\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "To achieve the most accurate and robust measurements, modern biomechanics often relies on optimally fusing data from multiple sensors. This final practice delves into the Kalman filter, the optimal linear estimator for dynamic systems subject to Gaussian noise. You will derive the filter's update equations from fundamental principles and apply them to a classic sensor fusion problem: estimating knee joint angle by combining gyroscope and optical marker data . This exercise provides hands-on experience with one of the most powerful tools in modern time series analysis and control theory.",
            "id": "4205599",
            "problem": "A sagittal-plane knee joint angle time series is recorded during treadmill walking at sampling period $T$ with two synchronized sensors: an Inertial Measurement Unit (IMU) gyroscope mounted on the shank, and a motion capture optical marker cluster tracked by cameras. Let the knee flexion angle at discrete time $k$ be $\\theta_k$. The IMU gyroscope produces a measured angular velocity $u_k$ that is used as a known input, while the optical system produces a direct angle measurement $y_k$. Assume a first-principles discrete-time kinematic model for angle propagation that integrates angular velocity and accounts for unmodeled accelerations and integration errors as additive process noise:\n$$\n\\theta_{k+1} = \\theta_k + T\\,u_k + v_k,\n$$\nwith $v_k$ a zero-mean white Gaussian process noise with variance $Q$. The optical marker angle measurement is modeled as\n$$\ny_k = \\theta_k + r_k,\n$$\nwith $r_k$ a zero-mean white Gaussian measurement noise with variance $R$. Assume $v_k$ and $r_k$ are mutually independent and independent of the state and input, and that all random variables are jointly Gaussian. You may assume unbiased initial conditions with covariance $P_{0|0}$.\n\n1) Starting from the linear Gaussian state-space model and the definitions of prior (prediction) and posterior (update) estimates, derive the discrete-time Kalman filter time-update and measurement-update equations specialized to this scalar system, giving explicit formulas for the predicted mean $\\hat{\\theta}_{k|k-1}$, predicted covariance $P_{k|k-1}$, innovation $\\nu_k$, innovation covariance $S_k$, Kalman gain $K_k$, posterior mean $\\hat{\\theta}_{k|k}$, and posterior covariance $P_{k|k}$. Your derivation must proceed from the orthogonality principle and conditional Gaussian identities, not by quoting a pre-derived formula.\n\n2) For steady-state walking in which the noise statistics are time-invariant, compute the steady-state Kalman gain $K_{\\infty}$ that the recursion converges to under constant $Q$ and $R$. Use the following scientifically plausible parameters: sampling period $T = 0.01\\,\\mathrm{s}$ (i.e., $100\\,\\mathrm{Hz}$ sampling), gyroscope angular velocity measurement standard deviation $\\sigma_g = 0.03\\,\\mathrm{rad/s}$, and optical marker angle measurement standard deviation $\\sigma_m = 0.5^{\\circ}$, with the conversion $\\sigma_m = \\frac{\\pi}{360}\\,\\mathrm{rad}$. For the random-walk integrated model above, take $Q = (\\sigma_g T)^2$ and $R = \\sigma_m^2$. Round your final value of $K_{\\infty}$ to four significant figures. Express the final answer as a dimensionless number without units.",
            "solution": "The problem is found to be valid as it is scientifically grounded in kinematics and estimation theory, well-posed, objective, and internally consistent.\n\nThe problem describes a scalar linear Gaussian state-space model for a knee joint angle, $\\theta_k$. To align with standard Kalman filter notation where the state at time $k$ is predicted from the state at time $k-1$, we will adopt an index shift on the provided model. The state dynamics are given by $\\theta_{k+1} = \\theta_k + T u_k + v_k$. We will analyze the transition from time $k-1$ to $k$, which is equivalent in structure. The state-space model is thus defined as:\nState equation:\n$$\n\\theta_k = \\theta_{k-1} + T u_{k-1} + v_{k-1}\n$$\nMeasurement equation:\n$$\ny_k = \\theta_k + r_k\n$$\nHere, $\\theta_k$ is the state (knee angle), $u_{k-1}$ is the known input (measured angular velocity), and $y_k$ is the measurement (optical marker angle). The process noise $v_{k-1}$ and measurement noise $r_k$ are zero-mean, mutually independent, white Gaussian processes with variances $Q$ and $R$ respectively, i.e., $v_{k-1} \\sim \\mathcal{N}(0, Q)$ and $r_k \\sim \\mathcal{N}(0, R)$. The problem can be written in the general state-space form $x_k = A x_{k-1} + B u_{k-1} + v_{k-1}$ and $y_k = H x_k + r_k$ with scalar coefficients $A=1$, $B=T$, and $H=1$.\n\nThe derivation of the Kalman filter equations proceeds in two steps: a time update (prediction) and a measurement update (correction). We proceed from the definitions of the conditional mean and the orthogonality principle.\n\n**1) Derivation of the Discrete-Time Kalman Filter Equations**\n\nThe objective is to find the minimum mean squared error (MMSE) estimate of the state $\\theta_k$. Given the Gaussian assumption, this is equivalent to the conditional mean. Let $\\mathcal{Y}_k = \\{y_1, y_2, \\dots, y_k\\}$ be the set of all measurements up to time $k$.\n\n**Time Update (Prediction)**\nThe time update step predicts the state and its error covariance at time $k$ based on information available up to time $k-1$.\n\nPredicted Mean $\\hat{\\theta}_{k|k-1}$: The predicted mean is the conditional expectation of $\\theta_k$ given measurements up to $k-1$.\n$$\n\\hat{\\theta}_{k|k-1} = E[\\theta_k | \\mathcal{Y}_{k-1}] = E[\\theta_{k-1} + T u_{k-1} + v_{k-1} | \\mathcal{Y}_{k-1}]\n$$\nBy linearity of expectation, and noting that $u_{k-1}$ is a known deterministic input and $v_{k-1}$ is zero-mean and independent of past measurements $\\mathcal{Y}_{k-1}$:\n$$\n\\hat{\\theta}_{k|k-1} = E[\\theta_{k-1} | \\mathcal{Y}_{k-1}] + T E[u_{k-1} | \\mathcal{Y}_{k-1}] + E[v_{k-1} | \\mathcal{Y}_{k-1}] = \\hat{\\theta}_{k-1|k-1} + T u_{k-1} + 0\n$$\nThus, the predicted mean is:\n$$\n\\hat{\\theta}_{k|k-1} = \\hat{\\theta}_{k-1|k-1} + T u_{k-1}\n$$\n\nPredicted Covariance $P_{k|k-1}$: This is the variance of the prediction error, $e_{k|k-1} = \\theta_k - \\hat{\\theta}_{k|k-1}$.\n$$\ne_{k|k-1} = (\\theta_{k-1} + T u_{k-1} + v_{k-1}) - (\\hat{\\theta}_{k-1|k-1} + T u_{k-1}) = (\\theta_{k-1} - \\hat{\\theta}_{k-1|k-1}) + v_{k-1} = e_{k-1|k-1} + v_{k-1}\n$$\nThe predicted covariance is $P_{k|k-1} = E[e_{k|k-1}^2]$.\n$$\nP_{k|k-1} = E[(e_{k-1|k-1} + v_{k-1})^2] = E[e_{k-1|k-1}^2] + 2 E[e_{k-1|k-1} v_{k-1}] + E[v_{k-1}^2]\n$$\nThe posterior error at time $k-1$, $e_{k-1|k-1}$, is a function of random variables up to that time and is therefore uncorrelated with the future noise term $v_{k-1}$. The cross-term is $E[e_{k-1|k-1} v_{k-1}] = E[e_{k-1|k-1}]E[v_{k-1}] = 0$. By definition, $E[e_{k-1|k-1}^2] = P_{k-1|k-1}$ and $E[v_{k-1}^2] = Q$.\nThus, the predicted covariance is:\n$$\nP_{k|k-1} = P_{k-1|k-1} + Q\n$$\n\n**Measurement Update (Correction)**\nThe measurement update step corrects the predicted estimate using the new measurement $y_k$.\n\nInnovation $\\nu_k$: The innovation is the new information provided by the measurement $y_k$, defined as the difference between the actual measurement and its prediction.\n$$\n\\nu_k = y_k - E[y_k | \\mathcal{Y}_{k-1}] = y_k - E[\\theta_k + r_k | \\mathcal{Y}_{k-1}] = y_k - (\\hat{\\theta}_{k|k-1} + E[r_k|\\mathcal{Y}_{k-1}])\n$$\nSince $r_k$ is independent of past measurements, $E[r_k|\\mathcal{Y}_{k-1}]=0$.\n$$\n\\nu_k = y_k - \\hat{\\theta}_{k|k-1}\n$$\n\nInnovation Covariance $S_k$: This is the variance of the innovation.\n$$\n\\nu_k = (\\theta_k + r_k) - \\hat{\\theta}_{k|k-1} = (\\theta_k - \\hat{\\theta}_{k|k-1}) + r_k = e_{k|k-1} + r_k\n$$\n$$\nS_k = E[\\nu_k^2] = E[(e_{k|k-1} + r_k)^2] = E[e_{k|k-1}^2] + 2 E[e_{k|k-1} r_k] + E[r_k^2]\n$$\nThe prediction error $e_{k|k-1}$ is uncorrelated with the measurement noise $r_k$, so $E[e_{k|k-1} r_k]=0$.\n$$\nS_k = P_{k|k-1} + R\n$$\n\nKalman Gain $K_k$: The posterior estimate $\\hat{\\theta}_{k|k}$ is a linear update of the prior estimate: $\\hat{\\theta}_{k|k} = \\hat{\\theta}_{k|k-1} + K_k \\nu_k$. The gain $K_k$ is chosen to minimize the posterior error variance $P_{k|k} = E[(\\theta_k - \\hat{\\theta}_{k|k})^2]$. The orthogonality principle states that this is achieved when the posterior error $e_{k|k} = \\theta_k - \\hat{\\theta}_{k|k}$ is orthogonal to the innovation $\\nu_k$, i.e., $E[e_{k|k}\\nu_k] = 0$.\n$$\ne_{k|k} = \\theta_k - (\\hat{\\theta}_{k|k-1} + K_k \\nu_k) = e_{k|k-1} - K_k \\nu_k\n$$\nApplying the orthogonality condition:\n$$\nE[(e_{k|k-1} - K_k \\nu_k)\\nu_k] = E[e_{k|k-1}\\nu_k] - K_k E[\\nu_k^2] = 0\n$$\nSolving for $K_k$:\n$$\nK_k = \\frac{E[e_{k|k-1}\\nu_k]}{E[\\nu_k^2]} = \\frac{E[e_{k|k-1}(e_{k|k-1} + r_k)]}{S_k} = \\frac{E[e_{k|k-1}^2] + E[e_{k|k-1}r_k]}{S_k} = \\frac{P_{k|k-1} + 0}{S_k}\n$$\n$$\nK_k = \\frac{P_{k|k-1}}{P_{k|k-1} + R}\n$$\n\nPosterior Mean $\\hat{\\theta}_{k|k}$: As defined above, the updated estimate is:\n$$\n\\hat{\\theta}_{k|k} = \\hat{\\theta}_{k|k-1} + K_k \\nu_k = \\hat{\\theta}_{k|k-1} + K_k (y_k - \\hat{\\theta}_{k|k-1})\n$$\n\nPosterior Covariance $P_{k|k}$: This is the variance of the posterior error $e_{k|k} = (1-K_k)e_{k|k-1} - K_k r_k$.\n$$\nP_{k|k} = E[e_{k|k}^2] = E[((1-K_k)e_{k|k-1} - K_k r_k)^2] = (1-K_k)^2 E[e_{k|k-1}^2] + K_k^2 E[r_k^2] - 2(1-K_k)K_k E[e_{k|k-1}r_k]\n$$\nThe cross-term is zero.\n$$\nP_{k|k} = (1-K_k)^2 P_{k|k-1} + K_k^2 R\n$$\nSubstituting $K_k=P_{k|k-1}/(P_{k|k-1}+R)$ and $1-K_k=R/(P_{k|k-1}+R)$:\n$$\nP_{k|k} = \\left(\\frac{R}{P_{k|k-1}+R}\\right)^2 P_{k|k-1} + \\left(\\frac{P_{k|k-1}}{P_{k|k-1}+R}\\right)^2 R = \\frac{R^2 P_{k|k-1} + P_{k|k-1}^2 R}{(P_{k|k-1}+R)^2} = \\frac{P_{k|k-1}R(R+P_{k|k-1})}{(P_{k|k-1}+R)^2} = \\frac{P_{k|k-1}R}{P_{k|k-1}+R}\n$$\nThis is equivalent to the simpler form $P_{k|k} = (1-K_k)P_{k|k-1}$.\n$$\nP_{k|k} = (1-K_k)P_{k|k-1}\n$$\n\n**2) Computation of the Steady-State Kalman Gain**\n\nIn steady-state, as $k \\to \\infty$, the covariances $P_{k|k-1}$ and $P_{k|k}$ converge to constant values, denoted $P_p$ and $P_u$ respectively. Consequently, the Kalman gain $K_k$ converges to a steady-state value $K_{\\infty}$. The recursive covariance equations become a set of algebraic equations:\n1.  $P_p = P_u + Q$\n2.  $P_u = (1-K_{\\infty})P_p$\n3.  $K_{\\infty} = \\frac{P_p}{P_p + R}$\n\nWe can solve for $K_{\\infty}$. Substitute (2) into (1):\n$P_p = (1 - K_{\\infty})P_p + Q \\implies P_p - P_p + K_{\\infty}P_p = Q \\implies K_{\\infty}P_p = Q$.\nThis gives $P_p = Q/K_{\\infty}$, assuming $K_{\\infty} \\neq 0$. Substitute this into (3):\n$K_{\\infty} = \\frac{Q/K_{\\infty}}{Q/K_{\\infty} + R} = \\frac{Q}{Q + R K_{\\infty}}$\nRearranging this gives a quadratic equation for $K_{\\infty}$:\n$K_{\\infty}(Q + R K_{\\infty}) = Q \\implies R K_{\\infty}^2 + Q K_{\\infty} - Q = 0$\nThis is the algebraic Riccati equation for this system. Using the quadratic formula, the solutions for $K_{\\infty}$ are:\n$K_{\\infty} = \\frac{-Q \\pm \\sqrt{Q^2 - 4(R)(-Q)}}{2R} = \\frac{-Q \\pm \\sqrt{Q^2 + 4RQ}}{2R}$\nSince the Kalman gain relates variances, it must be non-negative ($K_k = P_{k|k-1}/(P_{k|k-1}+R) \\in [0,1]$ as $P_{k|k-1}, R \\ge 0$). We must take the positive root:\n$K_{\\infty} = \\frac{-Q + \\sqrt{Q^2 + 4RQ}}{2R}$\n\nNow, we substitute the given numerical values:\n$T = 0.01\\,\\mathrm{s}$\n$\\sigma_g = 0.03\\,\\mathrm{rad/s}$\n$\\sigma_m = 0.5^{\\circ} = \\frac{\\pi}{360}\\,\\mathrm{rad}$\nThe noise variances are:\n$Q = (\\sigma_g T)^2 = (0.03 \\times 0.01)^2 = (3 \\times 10^{-4})^2 = 9 \\times 10^{-8}\\,\\mathrm{rad}^2$\n$R = \\sigma_m^2 = \\left(\\frac{\\pi}{360}\\right)^2 \\approx (8.726646 \\times 10^{-3})^2 \\approx 7.61545 \\times 10^{-5}\\,\\mathrm{rad}^2$\n\nTo find $K_{\\infty}$, we can first compute the ratio $\\alpha = Q/R$:\n$\\alpha = \\frac{9 \\times 10^{-8}}{7.61545 \\times 10^{-5}} \\approx 0.001181806$\nThe quadratic equation for $K_{\\infty}$ can be written in terms of $\\alpha$ by dividing by $R$:\n$K_{\\infty}^2 + \\frac{Q}{R} K_{\\infty} - \\frac{Q}{R} = 0 \\implies K_{\\infty}^2 + \\alpha K_{\\infty} - \\alpha = 0$\nThe solution is:\n$K_{\\infty} = \\frac{-\\alpha + \\sqrt{\\alpha^2 + 4\\alpha}}{2}$\nSubstituting the value of $\\alpha$:\n$K_{\\infty} = \\frac{-0.001181806 + \\sqrt{(0.001181806)^2 + 4(0.001181806)}}{2}$\n$K_{\\infty} = \\frac{-0.001181806 + \\sqrt{1.39666 \\times 10^{-6} + 0.004727224}}{2}$\n$K_{\\infty} = \\frac{-0.001181806 + \\sqrt{0.00472862}}{2}$\n$K_{\\infty} = \\frac{-0.001181806 + 0.06876497}{2}$\n$K_{\\infty} = \\frac{0.067583164}{2} \\approx 0.033791582$\n\nRounding to four significant figures, the steady-state Kalman gain is $0.03379$.",
            "answer": "$$\\boxed{0.03379}$$"
        }
    ]
}