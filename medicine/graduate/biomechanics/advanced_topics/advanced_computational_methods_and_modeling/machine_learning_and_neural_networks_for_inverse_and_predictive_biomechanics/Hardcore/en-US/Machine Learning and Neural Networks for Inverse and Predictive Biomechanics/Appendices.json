{
    "hands_on_practices": [
        {
            "introduction": "A foundational challenge in biomechanics is the muscle redundancy problem, where an infinite number of muscle activation patterns can produce the same joint torque. This exercise tackles this ill-posed inverse problem using Tikhonov regularization, a classic and powerful technique for finding a unique and physiologically plausible solution. By implementing and analyzing the regularized solution (), you will develop a core understanding of how to stabilize ill-conditioned systems and appreciate the trade-offs involved in selecting a regularization parameter.",
            "id": "4186288",
            "problem": "You are given a static inverse biomechanics problem framed in purely mathematical terms. The mapping from muscle forces to joint torques can be represented by a moment arm matrix. Let $R \\in \\mathbb{R}^{m \\times n}$ denote the moment arm matrix, $f \\in \\mathbb{R}^{n}$ the unknown muscle force vector, and $\\tau \\in \\mathbb{R}^{m}$ the measured joint torque vector. Measurements of $\\tau$ are noisy. The static equilibrium relation for joint torques is modeled as $ \\tau = R f + \\varepsilon $, where $ \\varepsilon $ captures measurement noise. To obtain a robust estimate of $f$ for redundant systems (when $n > m$), use zero-order Tikhonov regularization and the normal equations.\n\nStarting from the definitions above and foundational laws of mechanics, derive the regularized normal equations and compute the regularized solution for the muscle forces $f$ for several test cases. Then analyze the sensitivity of $f$ to the regularization parameter $\\alpha$ through an analytic derivative with respect to $\\alpha$ and via conditioning of the normal matrix. All computations are to be expressed in the International System of Units (SI): forces in Newtons (N) and torques in Newton-meters (N·m). Angle units are not required for this problem.\n\nFundamental base on which you must build:\n- Newton's second law of motion and static equilibrium principles imply that the net joint torque equals the sum of moments generated by muscle forces acting through moment arms.\n- Linear algebra definitions and properties for least-squares and regularization.\n\nTask requirements:\n1. Derive, from first principles, the regularized normal equations for the zero-order Tikhonov problem, then implement the computation of the regularized solution $f$ using only the normal equations for each specified test case.\n2. For each test case and a provided set of three strictly positive regularization parameters $ \\alpha $, compute:\n   - The regularized solution $ f(\\alpha) $ for each provided $ \\alpha $.\n   - The condition number of the regularized normal matrix $ R^\\top R + \\alpha I $ for the smallest and largest $ \\alpha $ in the set, using the $2$-norm.\n   - The analytical sensitivity magnitude at the middle $ \\alpha $, defined as $ s = \\| \\frac{d f}{d \\alpha} \\|_2 / \\| f \\|_2 $, where $ \\frac{d f}{d \\alpha} $ is the derivative of the solution with respect to $ \\alpha $.\n   - A boolean indicating whether $ \\| f(\\alpha) \\|_2 $ is non-increasing as $ \\alpha $ increases across the three given values.\n3. Express floats rounded to six decimal places. The sensitivity $ s $ and condition numbers are dimensionless. Do not output the forces or torques themselves; only output the requested dimensionless metrics and the boolean monotonicity check.\n4. Use the following test suite. Each test case supplies $R$, a noisy torque vector $\\tau$, and a strictly increasing list of three regularization parameters $\\alpha$:\n   - Test Case $1$ (two joints, three muscles, moderate noise):\n     $$\n     R_1 = \\begin{bmatrix}\n     0.04 & 0.02 & -0.01 \\\\\n     0.00 & 0.03 & 0.05\n     \\end{bmatrix}, \\quad\n     \\tau_1 = \\begin{bmatrix}\n     6.6 \\\\\n     4.9\n     \\end{bmatrix}, \\quad\n     \\alpha_1 = \\left[ 10^{-4},\\, 10^{-2},\\, 1.0 \\right].\n     $$\n   - Test Case $2$ (one joint, four muscles, near collinearity and small noise):\n     $$\n     R_2 = \\begin{bmatrix}\n     0.03 & 0.029 & -0.03 & 0.0301\n     \\end{bmatrix}, \\quad\n     \\tau_2 = \\begin{bmatrix}\n     3.149\n     \\end{bmatrix}, \\quad\n     \\alpha_2 = \\left[ 10^{-6},\\, 10^{-3},\\, 10^{-1} \\right].\n     $$\n   - Test Case $3$ (three joints, six muscles, higher noise and mild dependencies):\n     $$\n     R_3 = \\begin{bmatrix}\n     0.05 & 0.02 & -0.01 & 0.00 & 0.03 & -0.02 \\\\\n     0.00 & 0.04 & 0.02 & -0.01 & 0.01 & 0.03 \\\\\n     0.01 & -0.01 & 0.03 & 0.02 & -0.03 & 0.00\n     \\end{bmatrix}, \\quad\n     \\tau_3 = \\begin{bmatrix}\n     7.7 \\\\\n     4.1 \\\\\n     2.4\n     \\end{bmatrix}, \\quad\n     \\alpha_3 = \\left[ 10^{-5},\\, 10^{-2},\\, 1.0 \\right].\n     $$\n5. Final output format:\n   - Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a sublist of the form $[\\text{cond\\_low}, \\text{cond\\_high}, \\text{sensitivity\\_mid}, \\text{monotone}]$. For example:\n     $$\n     \\left[ [c_{1,\\text{low}}, c_{1,\\text{high}}, s_1, \\text{True}], [c_{2,\\text{low}}, c_{2,\\text{high}}, s_2, \\text{False}], [c_{3,\\text{low}}, c_{3,\\text{high}}, s_3, \\text{True}] \\right].\n     $$\n   - All floating-point values must be rounded to six decimal places. The boolean must be written as either $\\text{True}$ or $\\text{False}$.",
            "solution": "The problem is assessed to be valid. It is scientifically grounded in established principles of static biomechanics and numerical linear algebra, is well-posed through the use of Tikhonov regularization, and is specified with objective, complete, and consistent mathematical definitions and data.\n\nThe core of the problem is to find a stable estimate for the muscle force vector $f \\in \\mathbb{R}^{n}$ from noisy measurements of a joint torque vector $\\tau \\in \\mathbb{R}^{m}$, given the linear relationship $\\tau \\approx R f$, where $R \\in \\mathbb{R}^{m \\times n}$ is the moment arm matrix. For a redundant system ($n > m$), the problem is underdetermined and ill-posed. Zero-order Tikhonov regularization addresses this by finding a vector $f$ that minimizes a composite cost function, balancing the data fidelity (squared error of the model) and the magnitude of the solution (squared $L_2$-norm of forces), controlled by a regularization parameter $\\alpha > 0$.\n\n### Derivation of the Regularized Normal Equations\nThe Tikhonov cost function, $J(f)$, is defined as:\n$$ J(f) = \\| R f - \\tau \\|_2^2 + \\alpha \\| f \\|_2^2 $$\nHere, $\\| \\cdot \\|_2$ denotes the Euclidean ($L_2$) norm. We seek the vector $f$ that minimizes $J(f)$. To derive the solution, we first expand the norm terms using the identity $\\|x\\|_2^2 = x^\\top x$:\n$$ J(f) = (R f - \\tau)^\\top (R f - \\tau) + \\alpha (f^\\top f) $$\nExpanding the first term yields:\n$$ J(f) = (f^\\top R^\\top - \\tau^\\top)(R f - \\tau) + \\alpha f^\\top f $$\n$$ J(f) = f^\\top R^\\top R f - f^\\top R^\\top \\tau - \\tau^\\top R f + \\tau^\\top \\tau + \\alpha f^\\top f $$\nSince the term $\\tau^\\top R f$ is a scalar, it is equal to its transpose $(f^\\top R^\\top \\tau)^\\top$, which is $f^\\top R^\\top \\tau$. This allows combining the two cross-terms. We can also write $\\alpha f^\\top f$ as $\\alpha f^\\top I f$, where $I$ is the identity matrix of size $n \\times n$.\n$$ J(f) = f^\\top (R^\\top R) f - 2 f^\\top R^\\top \\tau + \\tau^\\top \\tau + f^\\top (\\alpha I) f $$\n$$ J(f) = f^\\top (R^\\top R + \\alpha I) f - 2 (R^\\top \\tau)^\\top f + \\tau^\\top \\tau $$\nThis expression is a quadratic form in $f$. A minimum exists and can be found by taking the gradient of $J(f)$ with respect to $f$ and setting it to zero. Using the matrix calculus identities $\\frac{\\partial}{\\partial x}(x^\\top A x) = 2Ax$ for symmetric $A$ and $\\frac{\\partial}{\\partial x}(b^\\top x) = b$, we get:\n$$ \\nabla_f J(f) = 2(R^\\top R + \\alpha I) f - 2 R^\\top \\tau $$\nSetting the gradient to zero, $\\nabla_f J(f) = 0$:\n$$ 2(R^\\top R + \\alpha I) f - 2 R^\\top \\tau = 0 $$\n$$ (R^\\top R + \\alpha I) f = R^\\top \\tau $$\nThis is the **regularized normal equation**. The matrix $A(\\alpha) = R^\\top R + \\alpha I$ is symmetric. Since $R^\\top R$ is positive semi-definite and $\\alpha > 0$, $A(\\alpha)$ is positive definite and therefore invertible. The solution for the muscle force vector $f$ is unique and given by:\n$$ f(\\alpha) = (R^\\top R + \\alpha I)^{-1} R^\\top \\tau $$\n\n### Derivation of the Analytical Sensitivity\nThe sensitivity of the solution $f$ with respect to the regularization parameter $\\alpha$ is its derivative, $\\frac{df}{d\\alpha}$. This can be found by implicitly differentiating the normal equation with respect to $\\alpha$:\n$$ \\frac{d}{d\\alpha} \\left[ (R^\\top R + \\alpha I) f(\\alpha) \\right] = \\frac{d}{d\\alpha} (R^\\top \\tau) $$\nThe right-hand side is zero since $R$ and $\\tau$ are constant with respect to $\\alpha$. Applying the product rule for differentiation to the left-hand side gives:\n$$ \\left[ \\frac{d}{d\\alpha} (R^\\top R + \\alpha I) \\right] f(\\alpha) + (R^\\top R + \\alpha I) \\frac{df}{d\\alpha} = 0 $$\nThe derivative of $(R^\\top R + \\alpha I)$ with respect to $\\alpha$ is the identity matrix $I$.\n$$ I f(\\alpha) + (R^\\top R + \\alpha I) \\frac{df}{d\\alpha} = 0 $$\nSolving for the derivative $\\frac{df}{d\\alpha}$:\n$$ \\frac{df}{d\\alpha} = -(R^\\top R + \\alpha I)^{-1} f(\\alpha) $$\nThe problem requires the relative sensitivity magnitude, defined as:\n$$ s = \\frac{\\| \\frac{df}{d\\alpha} \\|_2}{\\| f(\\alpha) \\|_2} $$\n\n### Computation of Metrics\nFor each test case and its set of three strictly increasing regularization parameters $(\\alpha_{low}, \\alpha_{mid}, \\alpha_{high})$, the following metrics are computed:\n1.  **Condition Number ($cond_2$)**: The $2$-norm condition number of the regularized normal matrix $A(\\alpha) = R^\\top R + \\alpha I$ is computed for $\\alpha_{low}$ and $\\alpha_{high}$. It measures the numerical stability of the linear system, with higher values indicating greater ill-conditioning.\n2.  **Sensitivity Magnitude ($s$)**: The sensitivity $s$ is computed at $\\alpha = \\alpha_{mid}$ using the derived analytical expressions for $f(\\alpha_{mid})$ and $\\frac{df}{d\\alpha}|_{\\alpha_{mid}}$.\n3.  **Monotonicity Check**: The $2$-norms of the solution vectors, $\\|f(\\alpha_{low})\\|_2$, $\\|f(\\alpha_{mid})\\|_2$, and $\\|f(\\alpha_{high})\\|_2$, are calculated. The boolean `monotone` is `True` if $\\|f(\\alpha_{low})\\|_2 \\ge \\|f(\\alpha_{mid})\\|_2 \\ge \\|f(\\alpha_{high})\\|_2$, and `False` otherwise. It can be shown that $\\|f(\\alpha)\\|_2$ is a monotonically decreasing function for $\\alpha > 0$, so this is expected to be `True`.\n\nThese derivations provide the foundation for the numerical implementation.",
            "answer": "```python\nimport numpy as np\n\ndef calculate_metrics(R, tau, alphas):\n    \"\"\"\n    Computes Tikhonov regularization metrics for a given biomechanics problem.\n\n    Args:\n        R (np.ndarray): The moment arm matrix (m x n).\n        tau (np.ndarray): The measured torque vector (m,).\n        alphas (list): A list of three strictly positive regularization parameters.\n\n    Returns:\n        list: A list containing [cond_low, cond_high, sensitivity_mid, monotone].\n    \"\"\"\n    alpha_low, alpha_mid, alpha_high = alphas\n    n = R.shape[1]\n    I = np.identity(n)\n\n    Rt = R.T\n    RtR = Rt @ R\n    Rt_tau = Rt @ tau\n\n    # --- Condition Numbers ---\n    A_low = RtR + alpha_low * I\n    cond_low = np.linalg.cond(A_low, 2)\n\n    A_high = RtR + alpha_high * I\n    cond_high = np.linalg.cond(A_high, 2)\n\n    # --- Sensitivity at alpha_mid ---\n    A_mid = RtR + alpha_mid * I\n    A_mid_inv = np.linalg.inv(A_mid)\n    \n    f_mid = A_mid_inv @ Rt_tau\n    \n    df_dalpha_mid = -A_mid_inv @ f_mid\n    \n    norm_f_mid = np.linalg.norm(f_mid, 2)\n    norm_df_dalpha_mid = np.linalg.norm(df_dalpha_mid, 2)\n    \n    if norm_f_mid  1e-12:  # Avoid division by zero if f is essentially zero\n        s_mid = 0.0\n    else:\n        s_mid = norm_df_dalpha_mid / norm_f_mid\n\n    # --- Monotonicity Check ---\n    f_low = np.linalg.inv(A_low) @ Rt_tau\n    norm_f_low = np.linalg.norm(f_low, 2)\n\n    f_high = np.linalg.inv(A_high) @ Rt_tau\n    norm_f_high = np.linalg.norm(f_high, 2)\n    \n    is_monotone = (norm_f_low >= norm_f_mid) and (norm_f_mid >= norm_f_high)\n\n    return [cond_low, cond_high, s_mid, is_monotone]\n\ndef solve():\n    \"\"\"\n    Defines test cases and computes the required metrics for each,\n    then prints the results in the specified format.\n    \"\"\"\n    test_cases = [\n        {\n            \"R\": np.array([[0.04, 0.02, -0.01], [0.00, 0.03, 0.05]]),\n            \"tau\": np.array([6.6, 4.9]),\n            \"alphas\": [1e-4, 1e-2, 1.0],\n        },\n        {\n            \"R\": np.array([[0.03, 0.029, -0.03, 0.0301]]),\n            \"tau\": np.array([3.149]),\n            \"alphas\": [1e-6, 1e-3, 1e-1],\n        },\n        {\n            \"R\": np.array([\n                [0.05, 0.02, -0.01, 0.00, 0.03, -0.02],\n                [0.00, 0.04, 0.02, -0.01, 0.01, 0.03],\n                [0.01, -0.01, 0.03, 0.02, -0.03, 0.00]\n            ]),\n            \"tau\": np.array([7.7, 4.1, 2.4]),\n            \"alphas\": [1e-5, 1e-2, 1.0],\n        },\n    ]\n\n    all_results = []\n    for case in test_cases:\n        metrics = calculate_metrics(case[\"R\"], case[\"tau\"], case[\"alphas\"])\n        all_results.append(metrics)\n\n    # Format the final output string precisely as requested.\n    string_results = []\n    for res in all_results:\n        # res = [cond_low, cond_high, s_mid, monotone]\n        # Floats are formatted to six decimal places.\n        # str(bool) correctly produces 'True' or 'False'.\n        res_str = (\n            f\"[{res[0]:.6f},{res[1]:.6f},{res[2]:.6f},{str(res[3])}]\"\n        )\n        string_results.append(res_str)\n    \n    final_output = f\"[{','.join(string_results)}]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "To train physics-informed models that respect the laws of mechanics, we need to efficiently compute how a simulated trajectory changes with respect to model parameters. This practice introduces the adjoint method, a cornerstone of differentiable simulation that computes these gradients with remarkable efficiency. By implementing the adjoint method for a simple dynamic system and validating it against finite differences (), you will gain the fundamental skill required for gradient-based optimization of complex biomechanical models.",
            "id": "4186295",
            "problem": "Implement a self-contained program that computes adjoint-based gradients of a trajectory-matching loss with respect to inertial parameters of a single planar segment and validates these gradients against central finite differences. The core biomechanical simulator models a single-degree-of-freedom (DOF) planar segment (for example, a shank or forearm) rotating about a fixed pivot under external joint torque. The fundamental base is Newton’s second law for rotational motion. The state consists of the joint angle and angular velocity. The inertial parameters to be estimated are the mass, the moment of inertia about the segment center of mass, and the center-of-mass distance from the joint.\n\nFundamental laws and core definitions:\n- Newton’s second law for rotation gives $$I_{\\text{tot}} \\, \\ddot{\\theta} + b \\, \\dot{\\theta} + m g c \\sin(\\theta) = \\tau,$$ where $I_{\\text{tot}}$ is the total moment of inertia about the joint given by $$I_{\\text{tot}} = I + m c^2,$$ $m$ is mass, $I$ is moment of inertia about the center of mass, $c$ is the center-of-mass distance from the joint, $b$ is viscous damping about the joint, $g$ is gravitational acceleration, $\\theta$ is the joint angle, $\\dot{\\theta}$ is the angular velocity, $\\ddot{\\theta}$ is the angular acceleration, and $\\tau$ is the external joint torque. The gravitational constant is $g = 9.81\\,\\text{m/s}^2$.\n\nDiscretization and simulation:\n- Use semi-implicit Euler integration with time step $dt$ seconds. For discrete time index $k \\in \\{0,1,\\dots,N-1\\}$, define\n$$\\ddot{\\theta}_k = \\frac{\\tau_k - b \\, \\dot{\\theta}_k - m g c \\sin(\\theta_k)}{I + m c^2},$$\n$$\\dot{\\theta}_{k+1} = \\dot{\\theta}_k + dt \\, \\ddot{\\theta}_k,$$\n$$\\theta_{k+1} = \\theta_k + dt \\, \\dot{\\theta}_{k+1}.$$\nThe initial state is $(\\theta_0,\\dot{\\theta}_0)$.\n\nLoss and gradients:\n- Define the trajectory-matching loss over angles as\n$$\\mathcal{L}(m,I,c) = \\frac{1}{2} \\sum_{k=0}^{N} w_k \\left(\\theta_k - \\theta^{\\text{tar}}_k\\right)^2,$$\nwhere $\\theta^{\\text{tar}}_k$ is a target trajectory generated by the same simulator under a different set of inertial parameters, and $w_k$ are nonnegative weights (use $w_k = 1$ for all $k$). Angles must be in radians.\n- Implement the adjoint-based gradient computation for the parameter vector $(m,I,c)$ using the discrete-time adjoint method (which is equivalent to Backpropagation Through Time (BPTT)):\n    - Let $s_k = [\\theta_k, \\dot{\\theta}_k]^\\top$ be the state and $s_{k+1} = f_k(s_k; m,I,c)$ be the discrete update. Let $\\lambda_k = \\partial \\mathcal{L}/\\partial s_k$ be the adjoint.\n    - The per-step loss contribution is $\\ell_k = \\frac{1}{2} w_k (\\theta_k - \\theta^{\\text{tar}}_k)^2$, so $\\partial \\ell_k / \\partial s_k = [w_k (\\theta_k - \\theta^{\\text{tar}}_k), \\, 0]^\\top$.\n    - The adjoint recursion is\n    $$\\lambda_N = \\frac{\\partial \\ell_N}{\\partial s_N}, \\quad \\lambda_k = \\frac{\\partial \\ell_k}{\\partial s_k} + \\left(\\frac{\\partial f_k}{\\partial s_k}\\right)^\\top \\lambda_{k+1}, \\quad \\text{for } k = N-1,\\dots,0.$$\n    - The gradient with respect to parameters is\n    $$\\nabla_{(m,I,c)} \\mathcal{L} = \\sum_{k=0}^{N-1} \\left(\\frac{\\partial f_k}{\\partial (m,I,c)}\\right)^\\top \\lambda_{k+1}.$$\n- Derive $\\partial f_k/\\partial s_k$ and $\\partial f_k/\\partial (m,I,c)$ from the discrete update, and use these to implement the adjoint recursion and gradient accumulation.\n\nUnits:\n- Mass $m$ in kilograms ($\\text{kg}$).\n- Center-of-mass distance $c$ in meters ($\\text{m}$).\n- Moment of inertia $I$ in kilogram-meter squared ($\\text{kg}\\cdot\\text{m}^2$).\n- Damping $b$ in Newton-meter-second ($\\text{N}\\cdot\\text{m}\\cdot\\text{s}$).\n- Torque $\\tau$ in Newton-meter ($\\text{N}\\cdot\\text{m}$).\n- Time $dt$ in seconds ($\\text{s}$).\n- Angles $\\theta$ in radians.\n\nValidation against finite differences:\n- Implement central finite differences for $(m,I,c)$ with a small step $h_i$ per parameter, $h_i = \\epsilon \\max(1, |p_i|)$ where $p_i \\in \\{m,I,c\\}$ and $\\epsilon$ is a small positive decimal (for example, $10^{-8}$). Compute\n$$\\frac{\\partial \\mathcal{L}}{\\partial p_i} \\approx \\frac{\\mathcal{L}(p_i + h_i) - \\mathcal{L}(p_i - h_i)}{2 h_i}.$$\n- For each test case, compute the adjoint-based gradient and compare it to the finite-difference gradient for each parameter using both an absolute tolerance $a_{\\text{tol}}$ and a relative tolerance $r_{\\text{tol}}$: a parameter passes if\n$$|\\nabla^{\\text{adj}}_i - \\nabla^{\\text{fd}}_i| \\le a_{\\text{tol}} \\quad \\text{or} \\quad \\frac{|\\nabla^{\\text{adj}}_i - \\nabla^{\\text{fd}}_i|}{\\max(|\\nabla^{\\text{fd}}_i|, 10^{-12})} \\le r_{\\text{tol}}.$$\nUse $a_{\\text{tol}} = 10^{-9}$ and $r_{\\text{tol}} = 10^{-6}$. A test case passes if all three parameters pass.\n\nTest suite:\n- Gravity is set to $g = 9.81\\,\\text{m/s}^2$ in all cases. Damping $b$ is specified per case. Use the same external torque for both the predicted and the target trajectory of each case.\n- Case $1$ (general case):\n    - Parameters: $m = 5.0$ $\\text{kg}$, $I = 0.25$ $\\text{kg}\\cdot\\text{m}^2$, $c = 0.20$ $\\text{m}$.\n    - Target parameters: $m^{\\text{tar}} = 5.5$ $\\text{kg}$, $I^{\\text{tar}} = 0.28$ $\\text{kg}\\cdot\\text{m}^2$, $c^{\\text{tar}} = 0.22$ $\\text{m}$.\n    - Time step: $dt = 0.002$ $\\text{s}$, steps: $N = 500$.\n    - Damping: $b = 0.05$ $\\text{N}\\cdot\\text{m}\\cdot\\text{s}$.\n    - Initial state: $\\theta_0 = 0.0$ $\\text{rad}$, $\\dot{\\theta}_0 = 0.0$ $\\text{rad}/\\text{s}$.\n    - Torque: $\\tau_k = A \\sin(2 \\pi f t_k)$, with $A = 1.0$ $\\text{N}\\cdot\\text{m}$, $f = 1.0$ $\\text{Hz}$, $t_k = k \\, dt$.\n- Case $2$ (gravity-only boundary):\n    - Parameters: $m = 3.0$ $\\text{kg}$, $I = 0.15$ $\\text{kg}\\cdot\\text{m}^2$, $c = 0.18$ $\\text{m}$.\n    - Target parameters: $m^{\\text{tar}} = 3.1$ $\\text{kg}$, $I^{\\text{tar}} = 0.14$ $\\text{kg}\\cdot\\text{m}^2$, $c^{\\text{tar}} = 0.17$ $\\text{m}$.\n    - Time step: $dt = 0.00125$ $\\text{s}$, steps: $N = 800$.\n    - Damping: $b = 0.02$ $\\text{N}\\cdot\\text{m}\\cdot\\text{s}$.\n    - Initial state: $\\theta_0 = 0.30$ $\\text{rad}$, $\\dot{\\theta}_0 = 0.0$ $\\text{rad}/\\text{s}$.\n    - Torque: $\\tau_k = 0$ for all $k$.\n- Case $3$ (higher damping, higher frequency):\n    - Parameters: $m = 7.0$ $\\text{kg}$, $I = 0.40$ $\\text{kg}\\cdot\\text{m}^2$, $c = 0.25$ $\\text{m}$.\n    - Target parameters: $m^{\\text{tar}} = 7.2$ $\\text{kg}$, $I^{\\text{tar}} = 0.42$ $\\text{kg}\\cdot\\text{m}^2$, $c^{\\text{tar}} = 0.26$ $\\text{m}$.\n    - Time step: $dt = 0.002$ $\\text{s}$, steps: $N = 600$.\n    - Damping: $b = 0.20$ $\\text{N}\\cdot\\text{m}\\cdot\\text{s}$.\n    - Initial state: $\\theta_0 = -0.20$ $\\text{rad}$, $\\dot{\\theta}_0 = 0.0$ $\\text{rad}/\\text{s}$.\n    - Torque: $\\tau_k = A \\sin(2 \\pi f t_k)$, with $A = 0.8$ $\\text{N}\\cdot\\text{m}$, $f = 3.0$ $\\text{Hz}$, $t_k = k \\, dt$.\n- Case $4$ (small inertia and center-of-mass distance):\n    - Parameters: $m = 2.0$ $\\text{kg}$, $I = 0.05$ $\\text{kg}\\cdot\\text{m}^2$, $c = 0.05$ $\\text{m}$.\n    - Target parameters: $m^{\\text{tar}} = 2.1$ $\\text{kg}$, $I^{\\text{tar}} = 0.052$ $\\text{kg}\\cdot\\text{m}^2$, $c^{\\text{tar}} = 0.045$ $\\text{m}$.\n    - Time step: $dt = 0.0015$ $\\text{s}$, steps: $N = 700$.\n    - Damping: $b = 0.03$ $\\text{N}\\cdot\\text{m}\\cdot\\text{s}$.\n    - Initial state: $\\theta_0 = 0.10$ $\\text{rad}$, $\\dot{\\theta}_0 = 0.0$ $\\text{rad}/\\text{s}$.\n    - Torque: $\\tau_k = A \\sin(2 \\pi f t_k)$, with $A = 0.3$ $\\text{N}\\cdot\\text{m}$, $f = 1.5$ $\\text{Hz}$, $t_k = k \\, dt$.\n\nProgram requirements:\n- Implement the discrete simulator, the adjoint-based gradient, and central finite differences as specified above.\n- For each case, compute whether all three parameter gradients pass the tolerances. Aggregate these pass booleans.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[\\text{result1},\\text{result2},\\text{result3},\\text{result4}]$), where each result is a boolean indicating pass or fail for the corresponding test case.",
            "solution": "The posed problem requires the implementation and validation of an adjoint-based method for computing gradients of a trajectory-matching loss function with respect to the inertial parameters of a planar rigid body. The validation is to be performed against a central finite difference approximation. This problem is scientifically well-grounded in classical mechanics and numerical optimization, and is specified with sufficient detail to permit a unique and verifiable solution.\n\nThe process involves three main stages:\n1.  **Forward Simulation**: A numerical simulator, based on the provided equations of motion and integration scheme, is used to generate state trajectories.\n2.  **Adjoint Gradient Computation**: An adjoint-based method, which is computationally equivalent to Backpropagation Through Time (BPTT), is implemented to compute the exact gradients of the loss function with respect to the model parameters. This requires deriving the analytical Jacobians of the discrete-time state transition function.\n3.  **Gradient Validation**: The analytically derived adjoint gradients are compared against a numerical approximation using the central finite difference method to ensure correctness.\n\nHerein, we will formalize the necessary derivations and algorithmic steps.\n\n**1. Forward Dynamics and Discretization**\n\nThe dynamics of the single-degree-of-freedom planar segment are governed by Newton's second law for rotation:\n$$I_{\\text{tot}} \\, \\ddot{\\theta} + b \\, \\dot{\\theta} + m g c \\sin(\\theta) = \\tau$$\nwhere $I_{\\text{tot}} = I + m c^2$ is the total moment of inertia about the pivot, derived from the parallel axis theorem. The parameters are mass $m$, moment of inertia about the center of mass $I$, and center-of-mass distance from the joint $c$. Constants include viscous damping $b$ and gravitational acceleration $g = 9.81 \\, \\text{m/s}^2$. The state is defined by the angle $\\theta$ and angular velocity $\\dot{\\theta}$.\n\nThe angular acceleration at a discrete time step $k$, $\\ddot{\\theta}_k$, can be expressed as:\n$$\\ddot{\\theta}_k = f_{\\text{accel}}(\\theta_k, \\dot{\\theta}_k; m, I, c) = \\frac{\\tau_k - b \\dot{\\theta}_k - m g c \\sin(\\theta_k)}{I + m c^2}$$\nThe problem specifies a semi-implicit Euler integration scheme with a time step $dt$:\n\\begin{align*} \\label{eq:1} \\dot{\\theta}_{k+1} = \\dot{\\theta}_k + dt \\, \\ddot{\\theta}_k \\\\ \\theta_{k+1} = \\theta_k + dt \\, \\dot{\\theta}_{k+1} \\end{align*}\nBy substituting the expression for $\\dot{\\theta}_{k+1}$ into the update for $\\theta_{k+1}$, we can write the full state update for $s_k = [\\theta_k, \\dot{\\theta}_k]^\\top$ as $s_{k+1} = f_k(s_k; m, I, c)$:\n\\begin{align*} \\theta_{k+1} = \\theta_k + dt (\\dot{\\theta}_k + dt \\, \\ddot{\\theta}_k) = \\theta_k + dt \\dot{\\theta}_k + dt^2 \\ddot{\\theta}_k \\\\ \\dot{\\theta}_{k+1} = \\dot{\\theta}_k + dt \\, \\ddot{\\theta}_k \\end{align*}\nThis formulation explicitly relates the state at step $k+1$ to the state at step $k$.\n\n**2. Adjoint-Based Gradient Computation**\n\nThe goal is to compute the gradient of the loss function $\\mathcal{L}(m,I,c) = \\frac{1}{2} \\sum_{k=0}^{N} (\\theta_k - \\theta^{\\text{tar}}_k)^2$ with respect to the parameters $p = (m, I, c)$. The discrete adjoint method provides an efficient way to do this. We define an adjoint state vector $\\lambda_k = \\partial \\mathcal{L} / \\partial s_k$ and propagate it backward in time.\n\nThe backward recursion is initialized at the final time step $N$:\n$$\\lambda_N = \\frac{\\partial \\mathcal{L}}{\\partial s_N} = \\begin{bmatrix} \\partial \\mathcal{L} / \\partial \\theta_N \\\\ \\partial \\mathcal{L} / \\partial \\dot{\\theta}_N \\end{bmatrix} = \\begin{bmatrix} (\\theta_N - \\theta^{\\text{tar}}_N) \\\\ 0 \\end{bmatrix}$$\nFor $k = N-1, \\dots, 0$, the adjoint state is updated according to the chain rule:\n$$\\lambda_k = \\frac{\\partial \\mathcal{L}}{\\partial s_k} = \\frac{\\partial \\ell_k}{\\partial s_k} + \\left(\\frac{\\partial s_{k+1}}{\\partial s_k}\\right)^\\top \\frac{\\partial \\mathcal{L}}{\\partial s_{k+1}} = \\frac{\\partial \\ell_k}{\\partial s_k} + \\left(\\frac{\\partial f_k}{\\partial s_k}\\right)^\\top \\lambda_{k+1}$$\nwhere $\\frac{\\partial \\ell_k}{\\partial s_k} = [(\\theta_k - \\theta^{\\text{tar}}_k), \\, 0]^\\top$.\n\nThe total gradient with respect to the parameters $p$ is accumulated over all time steps:\n$$\\nabla_p \\mathcal{L} = \\sum_{k=0}^{N-1} \\left(\\frac{\\partial s_{k+1}}{\\partial p}\\right)^\\top \\frac{\\partial \\mathcal{L}}{\\partial s_{k+1}} = \\sum_{k=0}^{N-1} \\left(\\frac{\\partial f_k}{\\partial p}\\right)^\\top \\lambda_{k+1}$$\n\n**3. Derivation of State and Parameter Jacobians**\n\nTo implement the adjoint method, we must derive the Jacobians $\\frac{\\partial f_k}{\\partial s_k}$ and $\\frac{\\partial f_k}{\\partial p}$. Let $D = I + m c^2$.\n\n**State Jacobian $\\frac{\\partial f_k}{\\partial s_k}$:**\nFirst, we find the partial derivatives of $\\ddot{\\theta}_k$ with respect to the state variables $\\theta_k$ and $\\dot{\\theta}_k$:\n$$\\frac{\\partial \\ddot{\\theta}_k}{\\partial \\theta_k} = -\\frac{m g c \\cos(\\theta_k)}{D}, \\quad \\frac{\\partial \\ddot{\\theta}_k}{\\partial \\dot{\\theta}_k} = -\\frac{b}{D}$$\nUsing these, we form the $2 \\times 2$ Jacobian matrix of the state transition function $f_k$:\n$$\\frac{\\partial f_k}{\\partial s_k} = \\begin{bmatrix} \\frac{\\partial \\theta_{k+1}}{\\partial \\theta_k}  \\frac{\\partial \\theta_{k+1}}{\\partial \\dot{\\theta}_k} \\\\ \\frac{\\partial \\dot{\\theta}_{k+1}}{\\partial \\theta_k}  \\frac{\\partial \\dot{\\theta}_{k+1}}{\\partial \\dot{\\theta}_k} \\end{bmatrix} = \\begin{bmatrix} 1 + dt^2 \\frac{\\partial \\ddot{\\theta}_k}{\\partial \\theta_k}  dt + dt^2 \\frac{\\partial \\ddot{\\theta}_k}{\\partial \\dot{\\theta}_k} \\\\ dt \\frac{\\partial \\ddot{\\theta}_k}{\\partial \\theta_k}  1 + dt \\frac{\\partial \\ddot{\\theta}_k}{\\partial \\dot{\\theta}_k} \\end{bmatrix}$$\n\n**Parameter Jacobian $\\frac{\\partial f_k}{\\partial p}$:**\nThe partial derivatives of the state update with respect to a parameter $p_i \\in \\{m, I, c\\}$ are:\n$$\\frac{\\partial \\theta_{k+1}}{\\partial p_i} = dt^2 \\frac{\\partial \\ddot{\\theta}_k}{\\partial p_i}, \\quad \\frac{\\partial \\dot{\\theta}_{k+1}}{\\partial p_i} = dt \\frac{\\partial \\ddot{\\theta}_k}{\\partial p_i}$$\nThis can be written compactly as $\\frac{\\partial f_k}{\\partial p} = [dt^2, dt]^\\top \\frac{\\partial \\ddot{\\theta}_k}{\\partial p}$. We need the partials of $\\ddot{\\theta}_k$ w.r.t $m, I, c$:\n\n-   With respect to mass $m$:\n    $$\\frac{\\partial \\ddot{\\theta}_k}{\\partial m} = \\frac{\\partial}{\\partial m}\\left(\\frac{\\tau_k - b \\dot{\\theta}_k - m g c \\sin(\\theta_k)}{I + m c^2}\\right) = \\frac{-g c \\sin(\\theta_k) \\cdot D - (\\tau_k - b \\dot{\\theta}_k - m g c \\sin(\\theta_k)) \\cdot c^2}{D^2} = -\\frac{g c \\sin(\\theta_k)}{D} - \\ddot{\\theta}_k \\frac{c^2}{D}$$\n\n-   With respect to inertia $I$:\n    $$\\frac{\\partial \\ddot{\\theta}_k}{\\partial I} = \\frac{\\partial}{\\partial I}\\left(\\frac{\\tau_k - b \\dot{\\theta}_k - m g c \\sin(\\theta_k)}{I + m c^2}\\right) = \\frac{-(\\tau_k - b \\dot{\\theta}_k - m g c \\sin(\\theta_k))}{D^2} = -\\frac{\\ddot{\\theta}_k}{D}$$\n\n-   With respect to center-of-mass distance $c$:\n    $$\\frac{\\partial \\ddot{\\theta}_k}{\\partial c} = \\frac{\\partial}{\\partial c}\\left(\\frac{\\tau_k - b \\dot{\\theta}_k - m g c \\sin(\\theta_k)}{I + m c^2}\\right) = \\frac{-m g \\sin(\\theta_k) \\cdot D - (\\tau_k - b \\dot{\\theta}_k - m g c \\sin(\\theta_k)) \\cdot 2mc}{D^2} = -\\frac{m g \\sin(\\theta_k)}{D} - \\ddot{\\theta}_k \\frac{2mc}{D}$$\n\nThese Jacobians are computed at each step of a forward pass and stored. Then, a backward pass computes the adjoint states and accumulates the gradients.\n\n**4. Gradient Validation via Finite Differences**\n\nTo confirm the correctness of the analytical gradients, we compare them to numerical approximations from the central finite difference method:\n$$\\frac{\\partial \\mathcal{L}}{\\partial p_i} \\approx \\frac{\\mathcal{L}(p_i + h_i) - \\mathcal{L}(p_i - h_i)}{2 h_i}$$\nwhere $p_i \\in \\{m,I,c\\}$ and $h_i$ is a small perturbation, chosen as $h_i = \\epsilon \\max(1, |p_i|)$ with $\\epsilon=10^{-8}$ to handle parameters of varying scales. The agreement between the adjoint gradient $\\nabla^{\\text{adj}}$ and the finite difference gradient $\\nabla^{\\text{fd}}$ is assessed using both absolute ($a_{\\text{tol}} = 10^{-9}$) and relative ($r_{\\text{tol}} = 10^{-6}$) tolerances. A validation passes if for each parameter component $i$, either the absolute difference or the relative difference is within the specified tolerance.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the gradient validation for all test cases.\n    \"\"\"\n    g = 9.81\n    epsilon_fd = 1e-8\n    atol = 1e-9\n    rtol = 1e-6\n\n    test_cases = [\n        # Case 1 (general case)\n        {\n            \"params\":       {\"m\": 5.0, \"I\": 0.25, \"c\": 0.20},\n            \"params_tar\":   {\"m\": 5.5, \"I\": 0.28, \"c\": 0.22},\n            \"dt\": 0.002, \"N\": 500, \"b\": 0.05,\n            \"s0\": [0.0, 0.0],\n            \"torque_func\": lambda t: 1.0 * np.sin(2 * np.pi * 1.0 * t),\n        },\n        # Case 2 (gravity-only boundary)\n        {\n            \"params\":       {\"m\": 3.0, \"I\": 0.15, \"c\": 0.18},\n            \"params_tar\":   {\"m\": 3.1, \"I\": 0.14, \"c\": 0.17},\n            \"dt\": 0.00125, \"N\": 800, \"b\": 0.02,\n            \"s0\": [0.30, 0.0],\n            \"torque_func\": lambda t: 0.0,\n        },\n        # Case 3 (higher damping, higher frequency)\n        {\n            \"params\":       {\"m\": 7.0, \"I\": 0.40, \"c\": 0.25},\n            \"params_tar\":   {\"m\": 7.2, \"I\": 0.42, \"c\": 0.26},\n            \"dt\": 0.002, \"N\": 600, \"b\": 0.20,\n            \"s0\": [-0.20, 0.0],\n            \"torque_func\": lambda t: 0.8 * np.sin(2 * np.pi * 3.0 * t),\n        },\n        # Case 4 (small inertia and center-of-mass distance)\n        {\n            \"params\":       {\"m\": 2.0, \"I\": 0.05, \"c\": 0.05},\n            \"params_tar\":   {\"m\": 2.1, \"I\": 0.052, \"c\": 0.045},\n            \"dt\": 0.0015, \"N\": 700, \"b\": 0.03,\n            \"s0\": [0.10, 0.0],\n            \"torque_func\": lambda t: 0.3 * np.sin(2 * np.pi * 1.5 * t),\n        },\n    ]\n\n    def simulate(p, b, dt, N, s0, torque_func):\n        \"\"\"\n        Runs the forward simulation to generate a trajectory.\n        \"\"\"\n        m, I, c = p[\"m\"], p[\"I\"], p[\"c\"]\n        states = np.zeros((N + 1, 2))\n        states[0] = s0\n        \n        I_tot = I + m * c**2\n        \n        for k in range(N):\n            theta_k, theta_dot_k = states[k]\n            t_k = k * dt\n            tau_k = torque_func(t_k)\n            \n            theta_ddot_k = (tau_k - b * theta_dot_k - m * g * c * np.sin(theta_k)) / I_tot\n            \n            theta_dot_k_plus_1 = theta_dot_k + dt * theta_ddot_k\n            theta_k_plus_1 = theta_k + dt * theta_dot_k_plus_1\n            \n            states[k + 1] = [theta_k_plus_1, theta_dot_k_plus_1]\n            \n        return states[:, 0] # Return only angle trajectory\n\n    def compute_loss(theta, theta_tar):\n        \"\"\"\n        Computes the trajectory-matching loss.\n        \"\"\"\n        return 0.5 * np.sum((theta - theta_tar)**2)\n\n    def compute_adjoint_gradient(p, b, dt, N, s0, torque_func, theta_tar):\n        \"\"\"\n        Computes the gradient using the discrete adjoint method.\n        \"\"\"\n        m, I, c = p[\"m\"], p[\"I\"], p[\"c\"]\n\n        # Forward pass to store states and Jacobians\n        states = np.zeros((N + 1, 2))\n        states[0] = s0\n        dfds_list = []\n        dfdp_list = []\n\n        I_tot = I + m * c**2\n        D_inv = 1.0 / I_tot\n\n        for k in range(N):\n            theta_k, theta_dot_k = states[k]\n            t_k = k * dt\n            tau_k = torque_func(t_k)\n\n            # Acceleration and its derivatives\n            theta_ddot_k = (tau_k - b * theta_dot_k - m * g * c * np.sin(theta_k)) * D_inv\n            \n            d_ddot_d_theta = -m * g * c * np.cos(theta_k) * D_inv\n            d_ddot_d_dot_theta = -b * D_inv\n            \n            # State Jacobian df/ds\n            dfds = np.array([\n                [1 + dt**2 * d_ddot_d_theta, dt + dt**2 * d_ddot_d_dot_theta],\n                [dt * d_ddot_d_theta,      1 + dt * d_ddot_d_dot_theta]\n            ])\n            dfds_list.append(dfds)\n\n            # Parameter derivatives of acceleration\n            d_ddot_d_m = -g * c * np.sin(theta_k) * D_inv - theta_ddot_k * c**2 * D_inv\n            d_ddot_d_I = -theta_ddot_k * D_inv\n            d_ddot_d_c = -m * g * np.sin(theta_k) * D_inv - theta_ddot_k * 2 * m * c * D_inv\n            \n            # Parameter Jacobian df/dp\n            dfdp = np.outer(np.array([dt**2, dt]), np.array([d_ddot_d_m, d_ddot_d_I, d_ddot_d_c]))\n            dfdp_list.append(dfdp)\n\n            # State update\n            theta_dot_k_plus_1 = theta_dot_k + dt * theta_ddot_k\n            theta_k_plus_1 = theta_k + dt * theta_dot_k_plus_1\n            states[k+1] = [theta_k_plus_1, theta_dot_k_plus_1]\n\n        # Backward pass (Adjoint)\n        lambdas = np.zeros((N + 1, 2))\n        lambdas[N, 0] = states[N, 0] - theta_tar[N] \n        \n        grad = np.zeros(3)\n\n        for k in range(N - 1, -1, -1):\n            theta_k = states[k, 0]\n            \n            # Accumulate parameter gradient\n            grad += dfdp_list[k].T @ lambdas[k + 1]\n            \n            # Update adjoint state\n            d_loss_d_sk = np.array([theta_k - theta_tar[k], 0.0])\n            lambdas[k] = d_loss_d_sk + dfds_list[k].T @ lambdas[k + 1]\n            \n        return grad\n\n    def compute_fd_gradient(p, b, dt, N, s0, torque_func, theta_tar):\n        \"\"\"\n        Computes the gradient using central finite differences.\n        \"\"\"\n        grad = np.zeros(3)\n        param_keys = [\"m\", \"I\", \"c\"]\n        for i, key in enumerate(param_keys):\n            p_val = p[key]\n            h = epsilon_fd * max(1.0, abs(p_val))\n            \n            p_plus = p.copy()\n            p_plus[key] = p_val + h\n            theta_plus = simulate(p_plus, b, dt, N, s0, torque_func)\n            loss_plus = compute_loss(theta_plus, theta_tar)\n            \n            p_minus = p.copy()\n            p_minus[key] = p_val - h\n            theta_minus = simulate(p_minus, b, dt, N, s0, torque_func)\n            loss_minus = compute_loss(theta_minus, theta_tar)\n            \n            grad[i] = (loss_plus - loss_minus) / (2 * h)\n            \n        return grad\n\n    def validate_gradients(grad_adj, grad_fd):\n        \"\"\"\n        Compares adjoint and finite difference gradients.\n        \"\"\"\n        for i in range(3):\n            abs_err = abs(grad_adj[i] - grad_fd[i])\n            \n            if abs_err = atol:\n                continue\n\n            denom = max(abs(grad_fd[i]), 1e-12)\n            rel_err = abs_err / denom\n            \n            if rel_err > rtol:\n                return False\n        return True\n\n    results = []\n    for case in test_cases:\n        theta_tar = simulate(\n            case[\"params_tar\"], case[\"b\"], case[\"dt\"], case[\"N\"], \n            case[\"s0\"], case[\"torque_func\"]\n        )\n        \n        grad_adj = compute_adjoint_gradient(\n            case[\"params\"], case[\"b\"], case[\"dt\"], case[\"N\"], \n            case[\"s0\"], case[\"torque_func\"], theta_tar\n        )\n        \n        grad_fd = compute_fd_gradient(\n            case[\"params\"], case[\"b\"], case[\"dt\"], case[\"N\"], \n            case[\"s0\"], case[\"torque_func\"], theta_tar\n        )\n        \n        is_passed = validate_gradients(grad_adj, grad_fd)\n        results.append(is_passed)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A predictive model's output is incomplete without a measure of its confidence. This exercise moves beyond point estimates to the critical task of uncertainty quantification, decomposing a model's total predictive variance into its constituent parts. Based on the law of total variance, you will learn to distinguish between aleatoric (data) uncertainty and epistemic (model) uncertainty using a model ensemble (), providing deeper insight into your model's limitations and pathways for its improvement.",
            "id": "4186272",
            "problem": "You are given an ensemble of neural network models for predictive biomechanics, each providing a probabilistic estimate of knee flexion moment during gait. For a fixed input $x$ that describes kinematic and anthropometric features, model $m$ outputs a Gaussian predictive distribution with mean $\\mu_m(x)$ (in $\\mathrm{N}\\cdot\\mathrm{m}$) and variance $\\sigma_m^2(x)$ (in $(\\mathrm{N}\\cdot\\mathrm{m})^2$). Assume an ensemble of $M$ models and an equally weighted mixture across the ensemble members.\n\nStarting from the definition of variance and expectation, derive the decomposition of the predictive variance of the mixture distribution into a sum of two components: a data noise component (aleatoric uncertainty) and a model uncertainty component (epistemic uncertainty). Use only the following foundational base:\n\n- The definition of expectation and variance for a random variable $Y$: $\\mathbb{E}[Y]$ and $\\mathrm{Var}(Y) = \\mathbb{E}\\left[(Y - \\mathbb{E}[Y])^2\\right]$.\n- The law of total expectation: $\\mathbb{E}[Y] = \\mathbb{E}_{\\Theta}\\left[ \\mathbb{E}[Y \\mid \\Theta] \\right]$ where $\\Theta$ is a random element representing model identity or parameter variation.\n- The law of total variance: $\\mathrm{Var}(Y) = \\mathbb{E}_{\\Theta}\\left[ \\mathrm{Var}(Y \\mid \\Theta) \\right] + \\mathrm{Var}_{\\Theta}\\left( \\mathbb{E}[Y \\mid \\Theta] \\right)$.\n\nUse these principles to express the predictive variance $\\mathrm{Var}(Y \\mid x)$ as the sum of an aleatoric component and an epistemic component in terms of $\\{\\mu_m(x)\\}_{m=1}^M$ and $\\{\\sigma_m^2(x)\\}_{m=1}^M$ under equal weighting of ensemble members.\n\nThen, implement a program that, for each test case below, computes:\n- The total predictive variance $V_{\\text{total}}(x)$ in $(\\mathrm{N}\\cdot\\mathrm{m})^2$.\n- The aleatoric variance $V_{\\text{aleatoric}}(x)$ in $(\\mathrm{N}\\cdot\\mathrm{m})^2$.\n- The epistemic variance $V_{\\text{epistemic}}(x)$ in $(\\mathrm{N}\\cdot\\mathrm{m})^2$.\n\nUse the ensemble equal-weight mixture interpretation. For numerical stability, if the computed epistemic variance is negative due to floating-point rounding error, clamp it to zero.\n\nExpress every variance in $(\\mathrm{N}\\cdot\\mathrm{m})^2$, rounded to six decimal places.\n\nTest suite (each case is a pair of lists $(\\{\\mu_m\\},\\{\\sigma_m^2\\})$ for a single input $x$):\n\n1. Happy path, heteroscedastic ensemble with $M=5$:\n   - $\\{\\mu_m\\} = [52.0, 49.0, 55.0, 51.5, 53.0]$ (in $\\mathrm{N}\\cdot\\mathrm{m}$)\n   - $\\{\\sigma_m^2\\} = [30.25, 20.25, 36.0, 25.0, 28.09]$ (in $(\\mathrm{N}\\cdot\\mathrm{m})^2$)\n\n2. Boundary case, identical means with varying aleatoric variances, $M=4$:\n   - $\\{\\mu_m\\} = [50.0, 50.0, 50.0, 50.0]$ (in $\\mathrm{N}\\cdot\\mathrm{m}$)\n   - $\\{\\sigma_m^2\\} = [16.0, 25.0, 9.0, 4.0]$ (in $(\\mathrm{N}\\cdot\\mathrm{m})^2$)\n\n3. Edge case, single-model ensemble, $M=1$:\n   - $\\{\\mu_m\\} = [60.0]$ (in $\\mathrm{N}\\cdot\\mathrm{m}$)\n   - $\\{\\sigma_m^2\\} = [36.0]$ (in $(\\mathrm{N}\\cdot\\mathrm{m})^2$)\n\n4. Edge case, zero aleatoric variance with differing means, $M=2$:\n   - $\\{\\mu_m\\} = [40.0, 60.0]$ (in $\\mathrm{N}\\cdot\\mathrm{m}$)\n   - $\\{\\sigma_m^2\\} = [0.0, 0.0]$ (in $(\\mathrm{N}\\cdot\\mathrm{m})^2$)\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each item is itself a list $[V_{\\text{total}},V_{\\text{aleatoric}},V_{\\text{epistemic}}]$ corresponding to a test case, with all three values rounded to six decimal places, for example: \n$[[v_{1,\\text{total}},v_{1,\\text{aleatoric}},v_{1,\\text{epistemic}}],[v_{2,\\text{total}},v_{2,\\text{aleatoric}},v_{2,\\text{epistemic}}],\\dots]$.",
            "solution": "Let $Y$ denote the knee flexion moment (in $\\mathrm{N}\\cdot\\mathrm{m}$) predicted for a fixed input $x$. Consider an ensemble of $M$ neural network models, each providing a conditional Gaussian prediction $Y \\mid \\Theta=m, x \\sim \\mathcal{N}(\\mu_m(x), \\sigma_m^2(x))$, where $\\Theta$ is a discrete random variable that indexes the model identity. We assume an equally weighted mixture over $\\Theta$, that is $\\mathbb{P}(\\Theta=m)=\\frac{1}{M}$ for $m=1,\\dots,M$.\n\nWe start from the core definitions of expectation and variance and use the law of total expectation and the law of total variance. For any random variable $Y$ and conditioning variable $\\Theta$, the law of total expectation gives\n$$\n\\mathbb{E}[Y \\mid x] = \\mathbb{E}_{\\Theta}\\left[ \\mathbb{E}[Y \\mid x, \\Theta] \\right],\n$$\nand the law of total variance gives\n$$\n\\mathrm{Var}(Y \\mid x) = \\mathbb{E}_{\\Theta}\\left[ \\mathrm{Var}(Y \\mid x, \\Theta) \\right] + \\mathrm{Var}_{\\Theta}\\left( \\mathbb{E}[Y \\mid x, \\Theta] \\right).\n$$\n\nIn our setting, $\\mathbb{E}[Y \\mid x, \\Theta=m] = \\mu_m(x)$ and $\\mathrm{Var}(Y \\mid x, \\Theta=m) = \\sigma_m^2(x)$. Under equal weighting across $M$ models, the expectation over $\\Theta$ becomes a uniform average:\n$$\n\\mathbb{E}_{\\Theta}\\left[ \\mathrm{Var}(Y \\mid x, \\Theta) \\right] = \\frac{1}{M}\\sum_{m=1}^M \\sigma_m^2(x),\n$$\nand\n$$\n\\mathrm{Var}_{\\Theta}\\left( \\mathbb{E}[Y \\mid x, \\Theta] \\right) = \\frac{1}{M}\\sum_{m=1}^M \\left(\\mu_m(x)\\right)^2 - \\left( \\frac{1}{M}\\sum_{m=1}^M \\mu_m(x) \\right)^2.\n$$\n\nTherefore, the predictive variance decomposes into two interpretable components:\n- Aleatoric variance (data noise):\n$$\nV_{\\text{aleatoric}}(x) = \\frac{1}{M}\\sum_{m=1}^M \\sigma_m^2(x),\n$$\nwhich captures irreducible uncertainty due to measurement noise and inherent variability modeled by each network's conditional distribution.\n- Epistemic variance (model uncertainty):\n$$\nV_{\\text{epistemic}}(x) = \\frac{1}{M}\\sum_{m=1}^M \\left(\\mu_m(x)\\right)^2 - \\left( \\frac{1}{M}\\sum_{m=1}^M \\mu_m(x) \\right)^2,\n$$\nwhich captures variability in the predicted means across models and is reducible with more data or better modeling.\n\nThe total predictive variance is their sum:\n$$\nV_{\\text{total}}(x) = V_{\\text{aleatoric}}(x) + V_{\\text{epistemic}}(x).\n$$\n\nAlgorithmic design based on these principles:\n1. For each test case, store $\\{\\mu_m\\}_{m=1}^M$ and $\\{\\sigma_m^2\\}_{m=1}^M$.\n2. Compute the uniform average of $\\sigma_m^2$ to obtain $V_{\\text{aleatoric}}(x)$.\n3. Compute the uniform averages of $\\mu_m$ and $\\mu_m^2$:\n   - Let $\\bar{\\mu} = \\frac{1}{M}\\sum_{m=1}^M \\mu_m(x)$,\n   - Let $\\overline{\\mu^2} = \\frac{1}{M}\\sum_{m=1}^M \\left(\\mu_m(x)\\right)^2$,\n   then $V_{\\text{epistemic}}(x) = \\overline{\\mu^2} - \\bar{\\mu}^2$.\n4. Compute $V_{\\text{total}}(x) = V_{\\text{aleatoric}}(x) + V_{\\text{epistemic}}(x)$.\n5. For numerical robustness, if floating-point errors yield $V_{\\text{epistemic}}(x)  0$, set $V_{\\text{epistemic}}(x) = 0$ before summation.\n6. Express each variance in $(\\mathrm{N}\\cdot\\mathrm{m})^2$, rounded to six decimal places, and aggregate the results for all test cases in the specified output format.\n\nScientific realism and edge coverage:\n- The happy path case uses plausible knee moment magnitudes and heteroscedastic variances typical in gait kinetics.\n- Identical means isolate aleatoric uncertainty, yielding zero epistemic variance as expected.\n- The single-model case has zero epistemic variance by construction.\n- The zero-aleatoric case isolates epistemic uncertainty from disagreement in predicted means.\n\nThis approach adheres to the law of total variance and provides a principled decomposition consistent with ensemble-based predictive biomechanics modeling.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_variance_components(mu_list, var_list):\n    \"\"\"\n    Compute aleatoric, epistemic, and total predictive variance for an ensemble.\n    Inputs:\n        mu_list: list or 1D array of predicted means (N·m)\n        var_list: list or 1D array of predicted variances ((N·m)^2)\n    Returns:\n        (V_total, V_aleatoric, V_epistemic) in ((N·m)^2)\n    \"\"\"\n    mu = np.asarray(mu_list, dtype=float)\n    sigma2 = np.asarray(var_list, dtype=float)\n    M = mu.size\n    if M == 0:\n        raise ValueError(\"Empty ensemble.\")\n\n    # Aleatoric variance: mean of predicted variances across ensemble members\n    V_aleatoric = float(np.mean(sigma2))\n\n    # Epistemic variance: variance of predicted means under equal weights\n    mean_mu = float(np.mean(mu))\n    mean_mu_sq = float(np.mean(mu**2))\n    V_epistemic = mean_mu_sq - mean_mu**2\n\n    # Numerical stability: clamp tiny negative due to floating point\n    if V_epistemic  0 and V_epistemic > -1e-12:\n        V_epistemic = 0.0\n    elif V_epistemic  -1e-12:\n        # If significantly negative, input data may be inconsistent;\n        # still clamp to zero to satisfy non-negativity.\n        V_epistemic = 0.0\n\n    V_total = V_aleatoric + V_epistemic\n    return V_total, V_aleatoric, V_epistemic\n\ndef format_results(results, decimals=6):\n    \"\"\"\n    Format list of tuples (total, aleatoric, epistemic) into the required single-line string.\n    Rounds each float to specified decimals.\n    \"\"\"\n    def fmt_float(x):\n        return f\"{round(x, decimals):.{decimals}f}\"\n    inner_lists = []\n    for t, a, e in results:\n        inner = f\"[{fmt_float(t)},{fmt_float(a)},{fmt_float(e)}]\"\n        inner_lists.append(inner)\n    return f\"[{','.join(inner_lists)}]\"\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each test case is a tuple: (mu_list, var_list)\n    test_cases = [\n        # 1. Happy path, heteroscedastic ensemble with M=5\n        ([52.0, 49.0, 55.0, 51.5, 53.0], [30.25, 20.25, 36.0, 25.0, 28.09]),\n        # 2. Boundary case, identical means, varying variances, M=4\n        ([50.0, 50.0, 50.0, 50.0], [16.0, 25.0, 9.0, 4.0]),\n        # 3. Edge case, single-model ensemble, M=1\n        ([60.0], [36.0]),\n        # 4. Edge case, zero aleatoric variance, differing means, M=2\n        ([40.0, 60.0], [0.0, 0.0]),\n    ]\n\n    results = []\n    for mu_list, var_list in test_cases:\n        V_total, V_aleatoric, V_epistemic = compute_variance_components(mu_list, var_list)\n        results.append((V_total, V_aleatoric, V_epistemic))\n\n    # Final print statement in the exact required format.\n    print(format_results(results, decimals=6))\n\nsolve()\n```"
        }
    ]
}