{
    "hands_on_practices": [
        {
            "introduction": "生物力学中的逆动力学问题旨在从可测量的关节力矩推断肌肉力，这是一个经典的挑战。由于肌肉系统的冗余性，该问题通常是欠定的且具有病态性质，导致其解对测量噪声高度敏感。本实践练习将演示如何运用吉洪诺夫正则化 (Tikhonov regularization) 来获得稳定且符合物理直觉的解，它在数据保真度和解的简洁性之间取得平衡，是深入更复杂的机器学习模型之前必须掌握的基础技术 。",
            "id": "4186288",
            "problem": "给定一个以纯数学术语构建的静态逆向生物力学问题。从肌肉力到关节力矩的映射可以用一个力臂矩阵来表示。令 $R \\in \\mathbb{R}^{m \\times n}$ 表示力臂矩阵，$f \\in \\mathbb{R}^{n}$ 表示未知的肌肉力向量，$\\tau \\in \\mathbb{R}^{m}$ 表示测得的关节力矩向量。$\\tau$ 的测量值包含噪声。关节力矩的静态平衡关系建模为 $ \\tau = R f + \\varepsilon $，其中 $ \\varepsilon $ 代表测量噪声。为了给冗余系统（当 $n > m$ 时）获得一个鲁棒的 $f$ 估计，使用零阶 Tikhonov 正则化和正规方程。\n\n从上述定义和力学基本定律出发，推导正则化正规方程，并为几个测试用例计算肌肉力 $f$ 的正则化解。然后，通过关于正则化参数 $\\alpha$ 的解析导数以及通过正规矩阵的条件数来分析 $f$ 对 $\\alpha$ 的敏感性。所有计算均以国际单位制（SI）表示：力的单位是牛顿（N），力矩的单位是牛顿米（N·m）。本问题不需要角度单位。\n\n您必须建立在以下基本原理之上：\n- 牛顿第二运动定律和静态平衡原理意味着净关节力矩等于肌肉力通过力臂产生的力矩之和。\n- 用于最小二乘法和正则化的线性代数定义和性质。\n\n任务要求：\n1. 从第一性原理出发，推导零阶 Tikhonov 问题的正则化正规方程，然后仅使用正规方程为每个指定的测试用例实现正则化解 $f$ 的计算。\n2. 对于每个测试用例和一组给定的三个严格为正的正则化参数 $ \\alpha $，计算：\n   - 对每个给定的 $ \\alpha $，计算正则化解 $ f(\\alpha) $。\n   - 对集合中最小和最大的 $ \\alpha $，计算正则化正规矩阵 $ R^\\top R + \\alpha I $ 的条件数，使用 $2$-范数。\n   - 在中间的 $ \\alpha $ 值处，计算解析敏感性大小，定义为 $ s = \\| \\frac{d f}{d \\alpha} \\|_2 / \\| f \\|_2 $，其中 $ \\frac{d f}{d \\alpha} $ 是解关于 $ \\alpha $ 的导数。\n   - 一个布尔值，指示当 $ \\alpha $ 在三个给定值上增加时，$ \\| f(\\alpha) \\|_2 $ 是否非递增。\n3. 浮点数表示为四舍五入到六位小数。敏感性 $s$ 和条件数是无量纲的。不要输出力或力矩本身；只输出所要求的无量纲指标和布尔单调性检查。\n4. 使用以下测试套件。每个测试用例提供 $R$、一个带噪声的力矩向量 $\\tau$，以及一个由三个严格递增的正则化参数 $\\alpha$ 组成的列表：\n   - 测试用例 $1$（两个关节，三个肌肉，中等噪声）：\n     $$\n     R_1 = \\begin{bmatrix}\n     0.04  0.02  -0.01 \\\\\n     0.00  0.03  0.05\n     \\end{bmatrix}, \\quad\n     \\tau_1 = \\begin{bmatrix}\n     6.6 \\\\\n     4.9\n     \\end{bmatrix}, \\quad\n     \\alpha_1 = \\left[ 10^{-4},\\, 10^{-2},\\, 1.0 \\right].\n     $$\n   - 测试用例 $2$（一个关节，四个肌肉，接近共线性和小噪声）：\n     $$\n     R_2 = \\begin{bmatrix}\n     0.03  0.029  -0.03  0.0301\n     \\end{bmatrix}, \\quad\n     \\tau_2 = \\begin{bmatrix}\n     3.149\n     \\end{bmatrix}, \\quad\n     \\alpha_2 = \\left[ 10^{-6},\\, 10^{-3},\\, 10^{-1} \\right].\n     $$\n   - 测试用例 $3$（三个关节，六个肌肉，较高噪声和轻度依赖性）：\n     $$\n     R_3 = \\begin{bmatrix}\n     0.05  0.02  -0.01  0.00  0.03  -0.02 \\\\\n     0.00  0.04  0.02  -0.01  0.01  0.03 \\\\\n     0.01  -0.01  0.03  0.02  -0.03  0.00\n     \\end{bmatrix}, \\quad\n     \\tau_3 = \\begin{bmatrix}\n     7.7 \\\\\n     4.1 \\\\\n     2.4\n     \\end{bmatrix}, \\quad\n     \\alpha_3 = \\left[ 10^{-5},\\, 10^{-2},\\, 1.0 \\right].\n     $$\n5. 最终输出格式：\n   - 你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。每个测试用例对应一个形式为 $[\\text{cond\\_low}, \\text{cond\\_high}, \\text{sensitivity\\_mid}, \\text{monotone}]$ 的子列表。例如：\n     $$\n     \\left[ [c_{1,\\text{low}}, c_{1,\\text{high}}, s_1, \\text{True}], [c_{2,\\text{low}}, c_{2,\\text{high}}, s_2, \\text{False}], [c_{3,\\text{low}}, c_{3,\\text{high}}, s_3, \\text{True}] \\right].\n     $$\n   - 所有浮点值必须四舍五入到六位小数。布尔值必须写作 $\\text{True}$ 或 $\\text{False}$。",
            "solution": "该问题经评估是有效的。它科学上基于静态生物力学和数值线性代数的既定原理，通过使用 Tikhonov 正则化而变得适定，并且具有客观、完整和一致的数学定义和数据。\n\n问题的核心是从带噪声的关节力矩向量 $\\tau \\in \\mathbb{R}^{m}$ 的测量值中找到肌肉力向量 $f \\in \\mathbb{R}^{n}$ 的稳定估计，给定线性关系 $\\tau \\approx R f$，其中 $R \\in \\mathbb{R}^{m \\times n}$ 是力臂矩阵。对于冗余系统（$n > m$），该问题是欠定且不适定的。零阶 Tikhonov 正则化通过找到一个最小化复合成本函数的向量 $f$ 来解决这个问题，该函数平衡了数据保真度（模型的平方误差）和解的幅度（力的平方 $L_2$-范数），并由一个正则化参数 $\\alpha > 0$ 控制。\n\n### 正则化正规方程的推导\nTikhonov 成本函数 $J(f)$ 定义为：\n$$ J(f) = \\| R f - \\tau \\|_2^2 + \\alpha \\| f \\|_2^2 $$\n此处，$\\| \\cdot \\|_2$ 表示欧几里得（$L_2$）范数。我们寻求最小化 $J(f)$ 的向量 $f$。为了推导解，我们首先使用恒等式 $\\|x\\|_2^2 = x^\\top x$ 展开范数项：\n$$ J(f) = (R f - \\tau)^\\top (R f - \\tau) + \\alpha (f^\\top f) $$\n展开第一项得到：\n$$ J(f) = (f^\\top R^\\top - \\tau^\\top)(R f - \\tau) + \\alpha f^\\top f $$\n$$ J(f) = f^\\top R^\\top R f - f^\\top R^\\top \\tau - \\tau^\\top R f + \\tau^\\top \\tau + \\alpha f^\\top f $$\n由于项 $\\tau^\\top R f$ 是一个标量，它等于其转置 $(f^\\top R^\\top \\tau)^\\top$，即 $f^\\top R^\\top \\tau$。这使得可以合并两个交叉项。我们也可以将 $\\alpha f^\\top f$ 写成 $\\alpha f^\\top I f$，其中 $I$ 是大小为 $n \\times n$ 的单位矩阵。\n$$ J(f) = f^\\top (R^\\top R) f - 2 f^\\top R^\\top \\tau + \\tau^\\top \\tau + f^\\top (\\alpha I) f $$\n$$ J(f) = f^\\top (R^\\top R + \\alpha I) f - 2 (R^\\top \\tau)^\\top f + \\tau^\\top \\tau $$\n这个表达式是关于 $f$ 的二次型。存在最小值，可以通过计算 $J(f)$ 相对于 $f$ 的梯度并将其设为零来找到。使用矩阵微积分恒等式 $\\frac{\\partial}{\\partial x}(x^\\top A x) = 2Ax$（对于对称矩阵 $A$）和 $\\frac{\\partial}{\\partial x}(b^\\top x) = b$，我们得到：\n$$ \\nabla_f J(f) = 2(R^\\top R + \\alpha I) f - 2 R^\\top \\tau $$\n将梯度设为零，$\\nabla_f J(f) = 0$：\n$$ 2(R^\\top R + \\alpha I) f - 2 R^\\top \\tau = 0 $$\n$$ (R^\\top R + \\alpha I) f = R^\\top \\tau $$\n这就是**正则化正规方程**。矩阵 $A(\\alpha) = R^\\top R + \\alpha I$ 是对称的。由于 $R^\\top R$ 是半正定的且 $\\alpha > 0$，因此 $A(\\alpha)$ 是正定的，因而是可逆的。肌肉力向量 $f$ 的解是唯一的，由以下公式给出：\n$$ f(\\alpha) = (R^\\top R + \\alpha I)^{-1} R^\\top \\tau $$\n\n### 解析敏感性的推导\n解 $f$ 关于正则化参数 $\\alpha$ 的敏感性是其导数 $\\frac{df}{d\\alpha}$。这可以通过对正规方程关于 $\\alpha$ 进行隐式微分来找到：\n$$ \\frac{d}{d\\alpha} \\left[ (R^\\top R + \\alpha I) f(\\alpha) \\right] = \\frac{d}{d\\alpha} (R^\\top \\tau) $$\n右侧为零，因为 $R$ 和 $\\tau$ 相对于 $\\alpha$ 是常数。对左侧应用乘法法则进行微分得到：\n$$ \\left[ \\frac{d}{d\\alpha} (R^\\top R + \\alpha I) \\right] f(\\alpha) + (R^\\top R + \\alpha I) \\frac{df}{d\\alpha} = 0 $$\n$(R^\\top R + \\alpha I)$ 关于 $\\alpha$ 的导数是单位矩阵 $I$。\n$$ I f(\\alpha) + (R^\\top R + \\alpha I) \\frac{df}{d\\alpha} = 0 $$\n求解导数 $\\frac{df}{d\\alpha}$：\n$$ \\frac{df}{d\\alpha} = -(R^\\top R + \\alpha I)^{-1} f(\\alpha) $$\n问题要求相对敏感性大小，定义为：\n$$ s = \\frac{\\| \\frac{df}{d\\alpha} \\|_2}{\\| f(\\alpha) \\|_2} $$\n\n### 指标的计算\n对于每个测试用例及其包含三个严格递增的正则化参数 $(\\alpha_{low}, \\alpha_{mid}, \\alpha_{high})$ 的集合，计算以下指标：\n1.  **条件数 ($cond_2$)**：为 $\\alpha_{low}$ 和 $\\alpha_{high}$ 计算正则化正规矩阵 $A(\\alpha) = R^\\top R + \\alpha I$ 的 $2$-范数条件数。它衡量了线性系统的数值稳定性，值越高表示病态程度越严重。\n2.  **敏感性大小 ($s$)**：使用推导出的 $f(\\alpha_{mid})$ 和 $\\frac{df}{d\\alpha}|_{\\alpha_{mid}}$ 的解析表达式，在 $\\alpha = \\alpha_{mid}$ 处计算敏感性 $s$。\n3.  **单调性检查**：计算解向量的 $2$-范数 $\\|f(\\alpha_{low})\\|_2$、$\\|f(\\alpha_{mid})\\|_2$ 和 $\\|f(\\alpha_{high})\\|_2$。如果 $\\|f(\\alpha_{low})\\|_2 \\ge \\|f(\\alpha_{mid})\\|_2 \\ge \\|f(\\alpha_{high})\\|_2$ 成立，则布尔值 `monotone` 为 `True`，否则为 `False`。可以证明，对于 $\\alpha > 0$，$\\|f(\\alpha)\\|_2$ 是一个单调递减函数，因此预期结果为 `True`。\n\n这些推导为数值实现提供了基础。",
            "answer": "```python\nimport numpy as np\n\ndef calculate_metrics(R, tau, alphas):\n    \"\"\"\n    Computes Tikhonov regularization metrics for a given biomechanics problem.\n\n    Args:\n        R (np.ndarray): The moment arm matrix (m x n).\n        tau (np.ndarray): The measured torque vector (m,).\n        alphas (list): A list of three strictly positive regularization parameters.\n\n    Returns:\n        list: A list containing [cond_low, cond_high, sensitivity_mid, monotone].\n    \"\"\"\n    alpha_low, alpha_mid, alpha_high = alphas\n    n = R.shape[1]\n    I = np.identity(n)\n\n    Rt = R.T\n    RtR = Rt @ R\n    Rt_tau = Rt @ tau\n\n    # --- Condition Numbers ---\n    A_low = RtR + alpha_low * I\n    cond_low = np.linalg.cond(A_low, 2)\n\n    A_high = RtR + alpha_high * I\n    cond_high = np.linalg.cond(A_high, 2)\n\n    # --- Sensitivity at alpha_mid ---\n    A_mid = RtR + alpha_mid * I\n    A_mid_inv = np.linalg.inv(A_mid)\n    \n    f_mid = A_mid_inv @ Rt_tau\n    \n    df_dalpha_mid = -A_mid_inv @ f_mid\n    \n    norm_f_mid = np.linalg.norm(f_mid, 2)\n    norm_df_dalpha_mid = np.linalg.norm(df_dalpha_mid, 2)\n    \n    if norm_f_mid  1e-12:  # Avoid division by zero if f is essentially zero\n        s_mid = 0.0\n    else:\n        s_mid = norm_df_dalpha_mid / norm_f_mid\n\n    # --- Monotonicity Check ---\n    f_low = np.linalg.inv(A_low) @ Rt_tau\n    norm_f_low = np.linalg.norm(f_low, 2)\n\n    f_high = np.linalg.inv(A_high) @ Rt_tau\n    norm_f_high = np.linalg.norm(f_high, 2)\n    \n    is_monotone = (norm_f_low >= norm_f_mid) and (norm_f_mid >= norm_f_high)\n\n    return [cond_low, cond_high, s_mid, is_monotone]\n\ndef solve():\n    \"\"\"\n    Defines test cases and computes the required metrics for each,\n    then prints the results in the specified format.\n    \"\"\"\n    test_cases = [\n        {\n            \"R\": np.array([[0.04, 0.02, -0.01], [0.00, 0.03, 0.05]]),\n            \"tau\": np.array([6.6, 4.9]),\n            \"alphas\": [1e-4, 1e-2, 1.0],\n        },\n        {\n            \"R\": np.array([[0.03, 0.029, -0.03, 0.0301]]),\n            \"tau\": np.array([3.149]),\n            \"alphas\": [1e-6, 1e-3, 1e-1],\n        },\n        {\n            \"R\": np.array([\n                [0.05, 0.02, -0.01, 0.00, 0.03, -0.02],\n                [0.00, 0.04, 0.02, -0.01, 0.01, 0.03],\n                [0.01, -0.01, 0.03, 0.02, -0.03, 0.00]\n            ]),\n            \"tau\": np.array([7.7, 4.1, 2.4]),\n            \"alphas\": [1e-5, 1e-2, 1.0],\n        },\n    ]\n\n    all_results = []\n    for case in test_cases:\n        metrics = calculate_metrics(case[\"R\"], case[\"tau\"], case[\"alphas\"])\n        all_results.append(metrics)\n\n    # Format the final output string precisely as requested.\n    string_results = []\n    for res in all_results:\n        # res = [cond_low, cond_high, s_mid, monotone]\n        # Floats are formatted to six decimal places.\n        # str(bool) correctly produces 'True' or 'False'.\n        res_str = (\n            f\"[{res[0]:.6f},{res[1]:.6f},{res[2]:.6f},{str(res[3])}]\"\n        )\n        string_results.append(res_str)\n    \n    final_output = f\"[{','.join(string_results)}]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "从逆向问题转向预测模型，例如用于预测步态动力学的神经网络，我们不仅需要知道模型的预测值，更需要了解其预测的置信度。本练习引入了不确定性分解这一关键概念，它使我们能够区分可减少的模型不确定性 (认知不确定性) 和固有的数据噪声 (偶然不确定性)。掌握这项技能对于在生物力学领域负责任地部署机器学习模型至关重要 。",
            "id": "4186272",
            "problem": "给定一个用于预测生物力学的神经网络模型集成，其中每个模型都提供对步态中膝关节屈曲力矩的概率性估计。对于描述运动学和人体测量学特征的固定输入 $x$，模型 $m$ 输出一个高斯预测分布，其均值为 $\\mu_m(x)$ (单位：$\\mathrm{N}\\cdot\\mathrm{m}$)，方差为 $\\sigma_m^2(x)$ (单位：$(\\mathrm{N}\\cdot\\mathrm{m})^2$)。假设集成包含 $M$ 个模型，并且集成成员之间采用等权重混合。\n\n从方差和期望的定义出发，将混合分布的预测方差分解为两个分量的和：数据噪声分量（偶然不确定性）和模型不确定性分量（认知不确定性）。仅使用以下基础原理：\n\n- 随机变量 $Y$ 的期望和方差的定义：$\\mathbb{E}[Y]$ 和 $\\mathrm{Var}(Y) = \\mathbb{E}\\left[(Y - \\mathbb{E}[Y])^2\\right]$。\n- 全期望定律：$\\mathbb{E}[Y] = \\mathbb{E}_{\\Theta}\\left[ \\mathbb{E}[Y \\mid \\Theta] \\right]$，其中 $\\Theta$ 是一个表示模型身份或参数变化的随机元素。\n- 全方差定律：$\\mathrm{Var}(Y) = \\mathbb{E}_{\\Theta}\\left[ \\mathrm{Var}(Y \\mid \\Theta) \\right] + \\mathrm{Var}_{\\Theta}\\left( \\mathbb{E}[Y \\mid \\Theta] \\right)$。\n\n在集成成员等权重的情况下，使用这些原​​则，将预测方差 $\\mathrm{Var}(Y \\mid x)$ 表示为偶然分量和认知分量之和，并用 $\\{\\mu_m(x)\\}_{m=1}^M$ 和 $\\{\\sigma_m^2(x)\\}_{m=1}^M$ 来表示。\n\n然后，实现一个程序，为下面的每个测试用例计算：\n- 总预测方差 $V_{\\text{total}}(x)$，单位为 $(\\mathrm{N}\\cdot\\mathrm{m})^2$。\n- 偶然方差 $V_{\\text{aleatoric}}(x)$，单位为 $(\\mathrm{N}\\cdot\\mathrm{m})^2$。\n- 认知方差 $V_{\\text{epistemic}}(x)$，单位为 $(\\mathrm{N}\\cdot\\mathrm{m})^2$。\n\n使用集成等权重混合的解释。为了数值稳定性，如果计算出的认知方差因浮点舍入误差而为负，则将其限制为零。\n\n将每个方差以 $(\\mathrm{N}\\cdot\\mathrm{m})^2$ 为单位表示，并四舍五入到六位小数。\n\n测试套件（每个案例是针对单个输入 $x$ 的一对列表 $(\\{\\mu_m\\},\\{\\sigma_m^2\\})$）：\n\n1. 常规路径，异方差集成，$M=5$：\n   - $\\{\\mu_m\\} = [52.0, 49.0, 55.0, 51.5, 53.0]$ (单位：$\\mathrm{N}\\cdot\\mathrm{m}$)\n   - $\\{\\sigma_m^2\\} = [30.25, 20.25, 36.0, 25.0, 28.09]$ (单位：$(\\mathrm{N}\\cdot\\mathrm{m})^2$)\n\n2. 边界情况，均值相同，偶然方差不同，$M=4$：\n   - $\\{\\mu_m\\} = [50.0, 50.0, 50.0, 50.0]$ (单位：$\\mathrm{N}\\cdot\\mathrm{m}$)\n   - $\\{\\sigma_m^2\\} = [16.0, 25.0, 9.0, 4.0]$ (单位：$(\\mathrm{N}\\cdot\\mathrm{m})^2$)\n\n3. 边缘情况，单模型集成，$M=1$：\n   - $\\{\\mu_m\\} = [60.0]$ (单位：$\\mathrm{N}\\cdot\\mathrm{m}$)\n   - $\\{\\sigma_m^2\\} = [36.0]$ (单位：$(\\mathrm{N}\\cdot\\mathrm{m})^2$)\n\n4. 边缘情况，偶然方差为零，均值不同，$M=2$：\n   - $\\{\\mu_m\\} = [40.0, 60.0]$ (单位：$\\mathrm{N}\\cdot\\mathrm{m}$)\n   - $\\{\\sigma_m^2\\} = [0.0, 0.0]$ (单位：$(\\mathrm{N}\\cdot\\mathrm{m})^2$)\n\n您的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表。列表中的每个项目本身也是一个列表 $[V_{\\text{total}},V_{\\text{aleatoric}},V_{\\text{epistemic}}]$，对应一个测试用例，所有三个值都四舍五入到六位小数，例如：\n$[[v_{1,\\text{total}},v_{1,\\text{aleatoric}},v_{1,\\text{epistemic}}],[v_{2,\\text{total}},v_{2,\\text{aleatoric}},v_{2,\\text{epistemic}}],\\dots]$。",
            "solution": "令 $Y$ 表示对固定输入 $x$ 预测的膝关节屈曲力矩（单位：$\\mathrm{N}\\cdot\\mathrm{m}$）。考虑一个由 $M$ 个神经网络模型组成的集成，每个模型提供一个条件高斯预测 $Y \\mid \\Theta=m, x \\sim \\mathcal{N}(\\mu_m(x), \\sigma_m^2(x))$，其中 $\\Theta$ 是索引模型身份的离散随机变量。我们假设在 $\\Theta$ 上进行等权重混合，即对于 $m=1,\\dots,M$，有 $\\mathbb{P}(\\Theta=m)=\\frac{1}{M}$。\n\n我们从期望和方差的核心定义出发，使用全期望定律和全方差定律。对于任意随机变量 $Y$ 和条件变量 $\\Theta$，全期望定律给出\n$$\n\\mathbb{E}[Y \\mid x] = \\mathbb{E}_{\\Theta}\\left[ \\mathbb{E}[Y \\mid x, \\Theta] \\right],\n$$\n全方差定律给出\n$$\n\\mathrm{Var}(Y \\mid x) = \\mathbb{E}_{\\Theta}\\left[ \\mathrm{Var}(Y \\mid x, \\Theta) \\right] + \\mathrm{Var}_{\\Theta}\\left( \\mathbb{E}[Y \\mid x, \\Theta] \\right).\n$$\n\n在我们的设定中，$\\mathbb{E}[Y \\mid x, \\Theta=m] = \\mu_m(x)$ 且 $\\mathrm{Var}(Y \\mid x, \\Theta=m) = \\sigma_m^2(x)$。在 $M$ 个模型等权重的情况下，关于 $\\Theta$ 的期望变成一个均匀平均值：\n$$\n\\mathbb{E}_{\\Theta}\\left[ \\mathrm{Var}(Y \\mid x, \\Theta) \\right] = \\frac{1}{M}\\sum_{m=1}^M \\sigma_m^2(x),\n$$\n且\n$$\n\\mathrm{Var}_{\\Theta}\\left( \\mathbb{E}[Y \\mid x, \\Theta] \\right) = \\frac{1}{M}\\sum_{m=1}^M \\left(\\mu_m(x)\\right)^2 - \\left( \\frac{1}{M}\\sum_{m=1}^M \\mu_m(x) \\right)^2.\n$$\n\n因此，预测方差可分解为两个可解释的分量：\n- 偶然方差（数据噪声）：\n$$\nV_{\\text{aleatoric}}(x) = \\frac{1}{M}\\sum_{m=1}^M \\sigma_m^2(x),\n$$\n它捕捉了由测量噪声和每个网络条件分布所建模的内在变异性导致的不可约不确定性。\n- 认知方差（模型不确定性）：\n$$\nV_{\\text{epistemic}}(x) = \\frac{1}{M}\\sum_{m=1}^M \\left(\\mu_m(x)\\right)^2 - \\left( \\frac{1}{M}\\sum_{m=1}^M \\mu_m(x) \\right)^2,\n$$\n它捕捉了跨模型预测均值的变异性，并且可以通过更多数据或更好的建模来减少。\n\n总预测方差是它们的和：\n$$\nV_{\\text{total}}(x) = V_{\\text{aleatoric}}(x) + V_{\\text{epistemic}}(x).\n$$\n\n基于这些原则的算法设计：\n1. 对于每个测试用例，存储 $\\{\\mu_m\\}_{m=1}^M$ 和 $\\{\\sigma_m^2\\}_{m=1}^M$。\n2. 计算 $\\sigma_m^2$ 的均匀平均值以获得 $V_{\\text{aleatoric}}(x)$。\n3. 计算 $\\mu_m$ 和 $\\mu_m^2$ 的均匀平均值：\n   - 令 $\\bar{\\mu} = \\frac{1}{M}\\sum_{m=1}^M \\mu_m(x)$，\n   - 令 $\\overline{\\mu^2} = \\frac{1}{M}\\sum_{m=1}^M \\left(\\mu_m(x)\\right)^2$，\n   则 $V_{\\text{epistemic}}(x) = \\overline{\\mu^2} - \\bar{\\mu}^2$。\n4. 计算 $V_{\\text{total}}(x) = V_{\\text{aleatoric}}(x) + V_{\\text{epistemic}}(x)$。\n5. 为保证数值鲁棒性，如果浮点误差导致 $V_{\\text{epistemic}}(x)  0$，则在求和前将 $V_{\\text{epistemic}}(x)$ 设置为 0。\n6. 将每个方差以 $(\\mathrm{N}\\cdot\\mathrm{m})^2$ 为单位表示，四舍五入到六位小数，并按照指定的输出格式汇总所有测试用例的结果。\n\n科学真实性和边缘情况覆盖：\n- 常规路径案例使用了在步态动力学中典型的、合理的膝关节力矩大小和异方差方差。\n- 相同的均值分离出偶然不确定性，如预期一样产生零认知方差。\n- 单模型案例根据构造，其认知方差为零。\n- 零偶然方差案例从预测均值的差异中分离出认知不确定性。\n\n该方法遵循全方差定律，并提供了一种与基于集成的预测生物力学建模一致的、有原则的分解。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_variance_components(mu_list, var_list):\n    \"\"\"\n    Compute aleatoric, epistemic, and total predictive variance for an ensemble.\n    Inputs:\n        mu_list: list or 1D array of predicted means (N·m)\n        var_list: list or 1D array of predicted variances ((N·m)^2)\n    Returns:\n        (V_total, V_aleatoric, V_epistemic) in ((N·m)^2)\n    \"\"\"\n    mu = np.asarray(mu_list, dtype=float)\n    sigma2 = np.asarray(var_list, dtype=float)\n    M = mu.size\n    if M == 0:\n        raise ValueError(\"Empty ensemble.\")\n\n    # Aleatoric variance: mean of predicted variances across ensemble members\n    V_aleatoric = float(np.mean(sigma2))\n\n    # Epistemic variance: variance of predicted means under equal weights\n    mean_mu = float(np.mean(mu))\n    mean_mu_sq = float(np.mean(mu**2))\n    V_epistemic = mean_mu_sq - mean_mu**2\n\n    # Numerical stability: clamp tiny negative due to floating point\n    if V_epistemic  0 and V_epistemic > -1e-12:\n        V_epistemic = 0.0\n    elif V_epistemic  -1e-12:\n        # If significantly negative, input data may be inconsistent;\n        # still clamp to zero to satisfy non-negativity.\n        V_epistemic = 0.0\n\n    V_total = V_aleatoric + V_epistemic\n    return V_total, V_aleatoric, V_epistemic\n\ndef format_results(results, decimals=6):\n    \"\"\"\n    Format list of tuples (total, aleatoric, epistemic) into the required single-line string.\n    Rounds each float to specified decimals.\n    \"\"\"\n    def fmt_float(x):\n        return f\"{round(x, decimals):.{decimals}f}\"\n    inner_lists = []\n    for t, a, e in results:\n        inner = f\"[{fmt_float(t)},{fmt_float(a)},{fmt_float(e)}]\"\n        inner_lists.append(inner)\n    return f\"[{','.join(inner_lists)}]\"\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each test case is a tuple: (mu_list, var_list)\n    test_cases = [\n        # 1. Happy path, heteroscedastic ensemble with M=5\n        ([52.0, 49.0, 55.0, 51.5, 53.0], [30.25, 20.25, 36.0, 25.0, 28.09]),\n        # 2. Boundary case, identical means, varying variances, M=4\n        ([50.0, 50.0, 50.0, 50.0], [16.0, 25.0, 9.0, 4.0]),\n        # 3. Edge case, single-model ensemble, M=1\n        ([60.0], [36.0]),\n        # 4. Edge case, zero aleatoric variance, differing means, M=2\n        ([40.0, 60.0], [0.0, 0.0]),\n    ]\n\n    results = []\n    for mu_list, var_list in test_cases:\n        V_total, V_aleatoric, V_epistemic = compute_variance_components(mu_list, var_list)\n        results.append((V_total, V_aleatoric, V_epistemic))\n\n    # Final print statement in the exact required format.\n    print(format_results(results, decimals=6))\n\nsolve()\n```"
        },
        {
            "introduction": "为了构建能遵循物理定律 (如牛顿定律) 的机器学习模型，我们需要计算损失函数相对于模型参数的梯度，而这需要“穿透”物理模拟器本身。本实践练习通过实现伴随法 (adjoint method) 来揭示这一过程的奥秘，该方法是贯穿时间反向传播 (BPTT) 的核心引擎。掌握这一高效计算梯度的方法，是构建物理信息神经网络和解决轨迹优化问题的基石 。",
            "id": "4186295",
            "problem": "实现一个独立的程序，用于计算单个平面连杆关于其惯性参数的轨迹匹配损失的伴随法梯度，并通过与中心有限差分法对比来验证这些梯度。核心生物力学模拟器为一个单自由度（DOF）平面连杆（例如，小腿或前臂）在外部关节力矩作用下围绕固定枢轴旋转进行建模。其基本依据是牛顿转动第二定律。状态由关节角度和角速度组成。待估计的惯性参数为质量、绕连杆质心的转动惯量以及质心到关节的距离。\n\n基本定律与核心定义：\n- 牛顿转动第二定律为 $$I_{\\text{tot}} \\, \\ddot{\\theta} + b \\, \\dot{\\theta} + m g c \\sin(\\theta) = \\tau,$$ 其中 $I_{\\text{tot}}$ 是绕关节的总转动惯量，由 $$I_{\\text{tot}} = I + m c^2,$$ 给出，$m$ 是质量，$I$ 是绕质心的转动惯量，$c$ 是质心到关节的距离，$b$ 是绕关节的粘性阻尼，$g$ 是重力加速度，$\\theta$ 是关节角度，$\\dot{\\theta}$ 是角速度，$\\ddot{\\theta}$ 是角加速度，$\\tau$ 是外部关节力矩。重力常数 $g$ 为 $9.81$ $\\text{m}/\\text{s}^2$。\n\n离散化与仿真：\n- 使用时间步长为 $dt$ 秒的半隐式欧拉积分。对于离散时间索引 $k \\in \\{0,1,\\dots,N-1\\}$，定义\n$$\\ddot{\\theta}_k = \\frac{\\tau_k - b \\, \\dot{\\theta}_k - m g c \\sin(\\theta_k)}{I + m c^2},$$\n$$\\dot{\\theta}_{k+1} = \\dot{\\theta}_k + dt \\, \\ddot{\\theta}_k,$$\n$$\\theta_{k+1} = \\theta_k + dt \\, \\dot{\\theta}_{k+1}.$$\n初始状态为 $(\\theta_0,\\dot{\\theta}_0)$。\n\n损失与梯度：\n- 定义关于角度的轨迹匹配损失为\n$$\\mathcal{L}(m,I,c) = \\frac{1}{2} \\sum_{k=0}^{N} w_k \\left(\\theta_k - \\theta^{\\text{tar}}_k\\right)^2,$$\n其中 $\\theta^{\\text{tar}}_k$ 是由同一模拟器在另一组惯性参数下生成的目标轨迹，而 $w_k$ 是非负权重（对所有 $k$ 使用 $w_k = 1$）。角度必须以弧度为单位。\n- 使用离散时间伴随法（等效于随时间反向传播，BPTT）为参数向量 $(m,I,c)$ 实现基于伴随法的梯度计算：\n    - 令 $s_k = [\\theta_k, \\dot{\\theta}_k]^\\top$ 为状态，$s_{k+1} = f_k(s_k; m,I,c)$ 为离散更新。令 $\\lambda_k = \\partial \\mathcal{L}/\\partial s_k$ 为伴随变量。\n    - 每步的损失贡献为 $\\ell_k = \\frac{1}{2} w_k (\\theta_k - \\theta^{\\text{tar}}_k)^2$，所以 $\\partial \\ell_k / \\partial s_k = [w_k (\\theta_k - \\theta^{\\text{tar}}_k), \\, 0]^\\top$。\n    - 伴随递归为\n    $$\\lambda_N = \\frac{\\partial \\ell_N}{\\partial s_N}, \\quad \\lambda_k = \\frac{\\partial \\ell_k}{\\partial s_k} + \\left(\\frac{\\partial f_k}{\\partial s_k}\\right)^\\top \\lambda_{k+1}, \\quad \\text{对于 } k = N-1,\\dots,0.$$\n    - 关于参数的梯度为\n    $$\\nabla_{(m,I,c)} \\mathcal{L} = \\sum_{k=0}^{N-1} \\left(\\frac{\\partial f_k}{\\partial (m,I,c)}\\right)^\\top \\lambda_{k+1}.$$\n- 从离散更新中推导 $\\partial f_k/\\partial s_k$ 和 $\\partial f_k/\\partial (m,I,c)$，并使用它们来实现伴随递归和梯度累积。\n\n单位：\n- 质量 $m$，单位为千克（$\\text{kg}$）。\n- 质心距离 $c$，单位为米（$\\text{m}$）。\n- 转动惯量 $I$，单位为千克·米²（$\\text{kg}\\cdot\\text{m}^2$）。\n- 阻尼 $b$，单位为牛顿·米·秒（$\\text{N}\\cdot\\text{m}\\cdot\\text{s}$）。\n- 力矩 $\\tau$，单位为牛顿·米（$\\text{N}\\cdot\\text{m}$）。\n- 时间 $dt$，单位为秒（$\\text{s}$）。\n- 角度 $\\theta$，单位为弧度。\n\n与有限差分法的验证：\n- 为 $(m,I,c)$ 实现中心有限差分法，每个参数使用一个小的步长 $h_i$，$h_i = \\epsilon \\max(1, |p_i|)$，其中 $p_i \\in \\{m,I,c\\}$，$\\epsilon$ 是一个小的正小数（例如 $10^{-8}$）。计算\n$$\\frac{\\partial \\mathcal{L}}{\\partial p_i} \\approx \\frac{\\mathcal{L}(p_i + h_i) - \\mathcal{L}(p_i - h_i)}{2 h_i}.$$\n- 对于每个测试用例，计算基于伴随法的梯度，并使用绝对容差 $a_{\\text{tol}}$ 和相对容差 $r_{\\text{tol}}$ 将其与每个参数的有限差分梯度进行比较：如果满足\n$$|\\nabla^{\\text{adj}}_i - \\nabla^{\\text{fd}}_i| \\le a_{\\text{tol}} \\quad \\text{or} \\quad \\frac{|\\nabla^{\\text{adj}}_i - \\nabla^{\\text{fd}}_i|}{\\max(|\\nabla^{\\text{fd}}_i|, 10^{-12})} \\le r_{\\text{tol}}.$$\n则一个参数通过测试。使用 $a_{\\text{tol}} = 10^{-9}$ 和 $r_{\\text{tol}} = 10^{-6}$。如果所有三个参数都通过，则测试用例通过。\n\n测试套件：\n- 在所有情况下，重力设为 $g = 9.81$ $\\text{m}/\\text{s}^2$。阻尼 $b$ 按各用例指定。每个用例的预测轨迹和目标轨迹使用相同的外部力矩。\n- 用例1（一般情况）：\n    - 参数：$m = 5.0$ $\\text{kg}$，$I = 0.25$ $\\text{kg}\\cdot\\text{m}^2$，$c = 0.20$ $\\text{m}$。\n    - 目标参数：$m^{\\text{tar}} = 5.5$ $\\text{kg}$，$I^{\\text{tar}} = 0.28$ $\\text{kg}\\cdot\\text{m}^2$，$c^{\\text{tar}} = 0.22$ $\\text{m}$。\n    - 时间步长：$dt = 0.002$ $\\text{s}$，步数：$N = 500$。\n    - 阻尼：$b = 0.05$ $\\text{N}\\cdot\\text{m}\\cdot\\text{s}$。\n    - 初始状态：$\\theta_0 = 0.0$ $\\text{rad}$，$\\dot{\\theta}_0 = 0.0$ $\\text{rad}/\\text{s}$。\n    - 力矩：$\\tau_k = A \\sin(2 \\pi f t_k)$，其中 $A = 1.0$ $\\text{N}\\cdot\\text{m}$，$f = 1.0$ $\\text{Hz}$，$t_k = k \\, dt$。\n- 用例2（仅重力边界情况）：\n    - 参数：$m = 3.0$ $\\text{kg}$，$I = 0.15$ $\\text{kg}\\cdot\\text{m}^2$，$c = 0.18$ $\\text{m}$。\n    - 目标参数：$m^{\\text{tar}} = 3.1$ $\\text{kg}$，$I^{\\text{tar}} = 0.14$ $\\text{kg}\\cdot\\text{m}^2$，$c^{\\text{tar}} = 0.17$ $\\text{m}$。\n    - 时间步长：$dt = 0.00125$ $\\text{s}$，步数：$N = 800$。\n    - 阻尼：$b = 0.02$ $\\text{N}\\cdot\\text{m}\\cdot\\text{s}$。\n    - 初始状态：$\\theta_0 = 0.30$ $\\text{rad}$，$\\dot{\\theta}_0 = 0.0$ $\\text{rad}/\\text{s}$。\n    - 力矩：对所有 $k$ 都有 $\\tau_k = 0$。\n- 用例3（更高阻尼，更高频率）：\n    - 参数：$m = 7.0$ $\\text{kg}$，$I = 0.40$ $\\text{kg}\\cdot\\text{m}^2$，$c = 0.25$ $\\text{m}$。\n    - 目标参数：$m^{\\text{tar}} = 7.2$ $\\text{kg}$，$I^{\\text{tar}} = 0.42$ $\\text{kg}\\cdot\\text{m}^2$，$c^{\\text{tar}} = 0.26$ $\\text{m}$。\n    - 时间步长：$dt = 0.002$ $\\text{s}$，步数：$N = 600$。\n    - 阻尼：$b = 0.20$ $\\text{N}\\cdot\\text{m}\\cdot\\text{s}$。\n    - 初始状态：$\\theta_0 = -0.20$ $\\text{rad}$，$\\dot{\\theta}_0 = 0.0$ $\\text{rad}/\\text{s}$。\n    - 力矩：$\\tau_k = A \\sin(2 \\pi f t_k)$，其中 $A = 0.8$ $\\text{N}\\cdot\\text{m}$，$f = 3.0$ $\\text{Hz}$，$t_k = k \\, dt$。\n- 用例4（小转动惯量和小质心距离）：\n    - 参数：$m = 2.0$ $\\text{kg}$，$I = 0.05$ $\\text{kg}\\cdot\\text{m}^2$，$c = 0.05$ $\\text{m}$。\n    - 目标参数：$m^{\\text{tar}} = 2.1$ $\\text{kg}$，$I^{\\text{tar}} = 0.052$ $\\text{kg}\\cdot\\text{m}^2$，$c^{\\text{tar}} = 0.045$ $\\text{m}$。\n    - 时间步长：$dt = 0.0015$ $\\text{s}$，步数：$N = 700$。\n    - 阻尼：$b = 0.03$ $\\text{N}\\cdot\\text{m}\\cdot\\text{s}$。\n    - 初始状态：$\\theta_0 = 0.10$ $\\text{rad}$，$\\dot{\\theta}_0 = 0.0$ $\\text{rad}/\\text{s}$。\n    - 力矩：$\\tau_k = A \\sin(2 \\pi f t_k)$，其中 $A = 0.3$ $\\text{N}\\cdot\\text{m}$，$f = 1.5$ $\\text{Hz}$，$t_k = k \\, dt$。\n\n程序要求：\n- 按照上述说明实现离散模拟器、基于伴随法的梯度和中心有限差分法。\n- 对每个用例，计算所有三个参数的梯度是否都通过容差测试。汇总这些布尔值结果。\n- 最终输出格式：您的程序应生成单行输出，其中包含一个方括号内的逗号分隔列表（例如，$[\\text{result1},\\text{result2},\\text{result3},\\text{result4}]$），每个结果是一个布尔值，表示相应测试用例的通过或失败。",
            "solution": "所提出的问题要求实现并验证一种基于伴随法的方法，用于计算平面刚体关于其惯性参数的轨迹匹配损失函数的梯度。验证将通过与中心有限差分近似进行对比来完成。该问题在经典力学和数值优化方面有坚实的科学基础，并且其规范足够详细，可以得出唯一且可验证的解。\n\n该过程包括三个主要阶段：\n1.  **正向仿真**：使用基于所提供的运动方程和积分方案的数值模拟器来生成状态轨迹。\n2.  **伴随梯度计算**：实现一种基于伴随法的方法（在计算上等效于随时间反向传播，BPTT），以计算损失函数关于模型参数的精确梯度。这需要推导离散时间状态转移函数的解析雅可比矩阵。\n3.  **梯度验证**：将解析推导的伴随梯度与使用中心有限差分法得到的数值近似进行比较，以确保其正确性。\n\n在此，我们将形式化地描述必要的推导和算法步骤。\n\n**1. 正向动力学与离散化**\n\n单自由度平面连杆的动力学由牛顿转动第二定律控制：\n$$I_{\\text{tot}} \\, \\ddot{\\theta} + b \\, \\dot{\\theta} + m g c \\sin(\\theta) = \\tau$$\n其中 $I_{\\text{tot}} = I + m c^2$ 是绕枢轴的总转动惯量，根据平行轴定理推导得出。参数为质量 $m$、绕质心的转动惯量 $I$ 以及质心到关节的距离 $c$。常量包括粘性阻尼 $b$ 和重力加速度 $g = 9.81 \\, \\text{m/s}^2$。状态由角度 $\\theta$ 和角速度 $\\dot{\\theta}$ 定义。\n\n在离散时间步 $k$ 处的角加速度 $\\ddot{\\theta}_k$ 可表示为：\n$$\\ddot{\\theta}_k = f_{\\text{accel}}(\\theta_k, \\dot{\\theta}_k; m, I, c) = \\frac{\\tau_k - b \\dot{\\theta}_k - m g c \\sin(\\theta_k)}{I + m c^2}$$\n问题指定了时间步长为 $dt$ 的半隐式欧拉积分方案：\n\\begin{align*} \\label{eq:1} \\dot{\\theta}_{k+1} = \\dot{\\theta}_k + dt \\, \\ddot{\\theta}_k \\\\ \\theta_{k+1} = \\theta_k + dt \\, \\dot{\\theta}_{k+1} \\end{align*}\n通过将 $\\dot{\\theta}_{k+1}$ 的表达式代入 $\\theta_{k+1}$ 的更新公式，我们可以将状态 $s_k = [\\theta_k, \\dot{\\theta}_k]^\\top$ 的完整状态更新写为 $s_{k+1} = f_k(s_k; m, I, c)$：\n\\begin{align*} \\theta_{k+1} = \\theta_k + dt (\\dot{\\theta}_k + dt \\, \\ddot{\\theta}_k) = \\theta_k + dt \\dot{\\theta}_k + dt^2 \\ddot{\\theta}_k \\\\ \\dot{\\theta}_{k+1} = \\dot{\\theta}_k + dt \\, \\ddot{\\theta}_k \\end{align*}\n这个公式明确地将第 $k+1$ 步的状态与第 $k$ 步的状态联系起来。\n\n**2. 基于伴随法的梯度计算**\n\n目标是计算损失函数 $\\mathcal{L}(m,I,c) = \\frac{1}{2} \\sum_{k=0}^{N} (\\theta_k - \\theta^{\\text{tar}}_k)^2$ 关于参数 $p = (m, I, c)$ 的梯度。离散伴随法为此提供了一种高效的方法。我们定义一个伴随状态向量 $\\lambda_k = \\partial \\mathcal{L} / \\partial s_k$，并使其随时间反向传播。\n\n反向递归在最终时间步 $N$ 初始化：\n$$\\lambda_N = \\frac{\\partial \\mathcal{L}}{\\partial s_N} = \\begin{bmatrix} \\partial \\mathcal{L} / \\partial \\theta_N \\\\ \\partial \\mathcal{L} / \\partial \\dot{\\theta}_N \\end{bmatrix} = \\begin{bmatrix} (\\theta_N - \\theta^{\\text{tar}}_N) \\\\ 0 \\end{bmatrix}$$\n对于 $k = N-1, \\dots, 0$，伴随状态根据链式法则进行更新：\n$$\\lambda_k = \\frac{\\partial \\mathcal{L}}{\\partial s_k} = \\frac{\\partial \\ell_k}{\\partial s_k} + \\left(\\frac{\\partial s_{k+1}}{\\partial s_k}\\right)^\\top \\frac{\\partial \\mathcal{L}}{\\partial s_{k+1}} = \\frac{\\partial \\ell_k}{\\partial s_k} + \\left(\\frac{\\partial f_k}{\\partial s_k}\\right)^\\top \\lambda_{k+1}$$\n其中 $\\frac{\\partial \\ell_k}{\\partial s_k} = [(\\theta_k - \\theta^{\\text{tar}}_k), \\, 0]^\\top$。\n\n关于参数 $p$ 的总梯度是在所有时间步上累积得到的：\n$$\\nabla_p \\mathcal{L} = \\sum_{k=0}^{N-1} \\left(\\frac{\\partial s_{k+1}}{\\partial p}\\right)^\\top \\frac{\\partial \\mathcal{L}}{\\partial s_{k+1}} = \\sum_{k=0}^{N-1} \\left(\\frac{\\partial f_k}{\\partial p}\\right)^\\top \\lambda_{k+1}$$\n\n**3. 状态和参数雅可比矩阵的推导**\n\n为了实现伴随法，我们必须推导雅可比矩阵 $\\frac{\\partial f_k}{\\partial s_k}$ 和 $\\frac{\\partial f_k}{\\partial p}$。令 $D = I + m c^2$。\n\n**状态雅可比矩阵 $\\frac{\\partial f_k}{\\partial s_k}$：**\n首先，我们求出 $\\ddot{\\theta}_k$ 关于状态变量 $\\theta_k$ 和 $\\dot{\\theta}_k$ 的偏导数：\n$$\\frac{\\partial \\ddot{\\theta}_k}{\\partial \\theta_k} = -\\frac{m g c \\cos(\\theta_k)}{D}, \\quad \\frac{\\partial \\ddot{\\theta}_k}{\\partial \\dot{\\theta}_k} = -\\frac{b}{D}$$\n利用这些，我们构成状态转移函数 $f_k$ 的 $2 \\times 2$ 雅可比矩阵：\n$$\\frac{\\partial f_k}{\\partial s_k} = \\begin{bmatrix} \\frac{\\partial \\theta_{k+1}}{\\partial \\theta_k}  \\frac{\\partial \\theta_{k+1}}{\\partial \\dot{\\theta}_k} \\\\ \\frac{\\partial \\dot{\\theta}_{k+1}}{\\partial \\theta_k}  \\frac{\\partial \\dot{\\theta}_{k+1}}{\\partial \\dot{\\theta}_k} \\end{bmatrix} = \\begin{bmatrix} 1 + dt^2 \\frac{\\partial \\ddot{\\theta}_k}{\\partial \\theta_k}  dt + dt^2 \\frac{\\partial \\ddot{\\theta}_k}{\\partial \\dot{\\theta}_k} \\\\ dt \\frac{\\partial \\ddot{\\theta}_k}{\\partial \\theta_k}  1 + dt \\frac{\\partial \\ddot{\\theta}_k}{\\partial \\dot{\\theta}_k} \\end{bmatrix}$$\n\n**参数雅可比矩阵 $\\frac{\\partial f_k}{\\partial p}$：**\n状态更新关于参数 $p_i \\in \\{m, I, c\\}$ 的偏导数为：\n$$\\frac{\\partial \\theta_{k+1}}{\\partial p_i} = dt^2 \\frac{\\partial \\ddot{\\theta}_k}{\\partial p_i}, \\quad \\frac{\\partial \\dot{\\theta}_{k+1}}{\\partial p_i} = dt \\frac{\\partial \\ddot{\\theta}_k}{\\partial p_i}$$\n这可以紧凑地写成 $\\frac{\\partial f_k}{\\partial p} = [dt^2, dt]^\\top \\frac{\\partial \\ddot{\\theta}_k}{\\partial p}$。我们需要 $\\ddot{\\theta}_k$ 关于 $m, I, c$ 的偏导数：\n\n-   关于质量 $m$：\n    $$\\frac{\\partial \\ddot{\\theta}_k}{\\partial m} = \\frac{\\partial}{\\partial m}\\left(\\frac{\\tau_k - b \\dot{\\theta}_k - m g c \\sin(\\theta_k)}{I + m c^2}\\right) = \\frac{-g c \\sin(\\theta_k) \\cdot D - (\\tau_k - b \\dot{\\theta}_k - m g c \\sin(\\theta_k)) \\cdot c^2}{D^2} = -\\frac{g c \\sin(\\theta_k)}{D} - \\ddot{\\theta}_k \\frac{c^2}{D}$$\n\n-   关于转动惯量 $I$：\n    $$\\frac{\\partial \\ddot{\\theta}_k}{\\partial I} = \\frac{\\partial}{\\partial I}\\left(\\frac{\\tau_k - b \\dot{\\theta}_k - m g c \\sin(\\theta_k)}{I + m c^2}\\right) = \\frac{-(\\tau_k - b \\dot{\\theta}_k - m g c \\sin(\\theta_k))}{D^2} = -\\frac{\\ddot{\\theta}_k}{D}$$\n\n-   关于质心距离 $c$：\n    $$\\frac{\\partial \\ddot{\\theta}_k}{\\partial c} = \\frac{\\partial}{\\partial c}\\left(\\frac{\\tau_k - b \\dot{\\theta}_k - m g c \\sin(\\theta_k)}{I + m c^2}\\right) = \\frac{-m g \\sin(\\theta_k) \\cdot D - (\\tau_k - b \\dot{\\theta}_k - m g c \\sin(\\theta_k)) \\cdot 2mc}{D^2} = -\\frac{m g \\sin(\\theta_k)}{D} - \\ddot{\\theta}_k \\frac{2mc}{D}$$\n\n这些雅可比矩阵在前向传递的每一步中计算并存储。然后，一次反向传递计算伴随状态并累积梯度。\n\n**4. 通过有限差分进行梯度验证**\n\n为了确认解析梯度的正确性，我们将其与中心有限差分法的数值近似进行比较：\n$$\\frac{\\partial \\mathcal{L}}{\\partial p_i} \\approx \\frac{\\mathcal{L}(p_i + h_i) - \\mathcal{L}(p_i - h_i)}{2 h_i}$$\n其中 $p_i \\in \\{m,I,c\\}$，$h_i$ 是一个小扰动，选择为 $h_i = \\epsilon \\max(1, |p_i|)$ 且 $\\epsilon=10^{-8}$，以处理不同尺度的参数。伴随梯度 $\\nabla^{\\text{adj}}$ 和有限差分梯度 $\\nabla^{\\text{fd}}$ 之间的一致性通过绝对容差（$a_{\\text{tol}} = 10^{-9}$）和相对容差（$r_{\\text{tol}} = 10^{-6}$）进行评估。如果对于每个参数分量 $i$，其绝对差或相对差在指定的容差范围内，则验证通过。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the gradient validation for all test cases.\n    \"\"\"\n    g = 9.81\n    epsilon_fd = 1e-8\n    atol = 1e-9\n    rtol = 1e-6\n\n    test_cases = [\n        # Case 1 (general case)\n        {\n            \"params\":       {\"m\": 5.0, \"I\": 0.25, \"c\": 0.20},\n            \"params_tar\":   {\"m\": 5.5, \"I\": 0.28, \"c\": 0.22},\n            \"dt\": 0.002, \"N\": 500, \"b\": 0.05,\n            \"s0\": [0.0, 0.0],\n            \"torque_func\": lambda t: 1.0 * np.sin(2 * np.pi * 1.0 * t),\n        },\n        # Case 2 (gravity-only boundary)\n        {\n            \"params\":       {\"m\": 3.0, \"I\": 0.15, \"c\": 0.18},\n            \"params_tar\":   {\"m\": 3.1, \"I\": 0.14, \"c\": 0.17},\n            \"dt\": 0.00125, \"N\": 800, \"b\": 0.02,\n            \"s0\": [0.30, 0.0],\n            \"torque_func\": lambda t: 0.0,\n        },\n        # Case 3 (higher damping, higher frequency)\n        {\n            \"params\":       {\"m\": 7.0, \"I\": 0.40, \"c\": 0.25},\n            \"params_tar\":   {\"m\": 7.2, \"I\": 0.42, \"c\": 0.26},\n            \"dt\": 0.002, \"N\": 600, \"b\": 0.20,\n            \"s0\": [-0.20, 0.0],\n            \"torque_func\": lambda t: 0.8 * np.sin(2 * np.pi * 3.0 * t),\n        },\n        # Case 4 (small inertia and center-of-mass distance)\n        {\n            \"params\":       {\"m\": 2.0, \"I\": 0.05, \"c\": 0.05},\n            \"params_tar\":   {\"m\": 2.1, \"I\": 0.052, \"c\": 0.045},\n            \"dt\": 0.0015, \"N\": 700, \"b\": 0.03,\n            \"s0\": [0.10, 0.0],\n            \"torque_func\": lambda t: 0.3 * np.sin(2 * np.pi * 1.5 * t),\n        },\n    ]\n\n    def simulate(p, b, dt, N, s0, torque_func):\n        \"\"\"\n        Runs the forward simulation to generate a trajectory.\n        \"\"\"\n        m, I, c = p[\"m\"], p[\"I\"], p[\"c\"]\n        states = np.zeros((N + 1, 2))\n        states[0] = s0\n        \n        I_tot = I + m * c**2\n        if I_tot = 0: return np.full(N+1, np.nan) # Invalid parameters\n        \n        for k in range(N):\n            theta_k, theta_dot_k = states[k]\n            t_k = k * dt\n            tau_k = torque_func(t_k)\n            \n            theta_ddot_k = (tau_k - b * theta_dot_k - m * g * c * np.sin(theta_k)) / I_tot\n            \n            theta_dot_k_plus_1 = theta_dot_k + dt * theta_ddot_k\n            theta_k_plus_1 = theta_k + dt * theta_dot_k_plus_1\n            \n            states[k + 1] = [theta_k_plus_1, theta_dot_k_plus_1]\n            \n        return states[:, 0] # Return only angle trajectory\n\n    def compute_loss(theta, theta_tar):\n        \"\"\"\n        Computes the trajectory-matching loss.\n        \"\"\"\n        return 0.5 * np.sum((theta - theta_tar)**2)\n\n    def compute_adjoint_gradient(p, b, dt, N, s0, torque_func, theta_tar):\n        \"\"\"\n        Computes the gradient using the discrete adjoint method.\n        \"\"\"\n        m, I, c = p[\"m\"], p[\"I\"], p[\"c\"]\n\n        # Forward pass to store states and Jacobians\n        states = np.zeros((N + 1, 2))\n        states[0] = s0\n        dfds_list = []\n        dfdp_list = []\n\n        I_tot = I + m * c**2\n        if I_tot = 0: return np.full(3, np.nan)\n        D_inv = 1.0 / I_tot\n\n        for k in range(N):\n            theta_k, theta_dot_k = states[k]\n            t_k = k * dt\n            tau_k = torque_func(t_k)\n\n            # Acceleration and its derivatives\n            theta_ddot_k = (tau_k - b * theta_dot_k - m * g * c * np.sin(theta_k)) * D_inv\n            \n            d_ddot_d_theta = -m * g * c * np.cos(theta_k) * D_inv\n            d_ddot_d_dot_theta = -b * D_inv\n            \n            # State Jacobian df/ds\n            dfds = np.array([\n                [1 + dt**2 * d_ddot_d_theta, dt + dt**2 * d_ddot_d_dot_theta],\n                [dt * d_ddot_d_theta,      1 + dt * d_ddot_d_dot_theta]\n            ])\n            dfds_list.append(dfds)\n\n            # Parameter derivatives of acceleration\n            d_ddot_d_m = -g * c * np.sin(theta_k) * D_inv - theta_ddot_k * c**2 * D_inv\n            d_ddot_d_I = -theta_ddot_k * D_inv\n            d_ddot_d_c = -m * g * np.sin(theta_k) * D_inv - theta_ddot_k * 2 * m * c * D_inv\n            \n            # Parameter Jacobian df/dp\n            dfdp = np.outer(np.array([dt**2, dt]), np.array([d_ddot_d_m, d_ddot_d_I, d_ddot_d_c]))\n            dfdp_list.append(dfdp)\n\n            # State update\n            theta_dot_k_plus_1 = theta_dot_k + dt * theta_ddot_k\n            theta_k_plus_1 = theta_k + dt * theta_dot_k_plus_1\n            states[k+1] = [theta_k_plus_1, theta_dot_k_plus_1]\n\n        # Backward pass (Adjoint)\n        lambdas = np.zeros((N + 1, 2))\n        lambdas[N, 0] = states[N, 0] - theta_tar[N] \n        \n        grad = np.zeros(3)\n\n        for k in range(N - 1, -1, -1):\n            theta_k = states[k, 0]\n            \n            # Accumulate parameter gradient\n            grad += dfdp_list[k].T @ lambdas[k + 1]\n            \n            # Update adjoint state\n            d_loss_d_sk = np.array([theta_k - theta_tar[k], 0.0])\n            lambdas[k] = d_loss_d_sk + dfds_list[k].T @ lambdas[k + 1]\n            \n        return grad\n\n    def compute_fd_gradient(p, b, dt, N, s0, torque_func, theta_tar):\n        \"\"\"\n        Computes the gradient using central finite differences.\n        \"\"\"\n        grad = np.zeros(3)\n        param_keys = [\"m\", \"I\", \"c\"]\n        for i, key in enumerate(param_keys):\n            p_val = p[key]\n            h = epsilon_fd * max(1.0, abs(p_val))\n            \n            p_plus = p.copy()\n            p_plus[key] = p_val + h\n            theta_plus = simulate(p_plus, b, dt, N, s0, torque_func)\n            loss_plus = compute_loss(theta_plus, theta_tar)\n            \n            p_minus = p.copy()\n            p_minus[key] = p_val - h\n            theta_minus = simulate(p_minus, b, dt, N, s0, torque_func)\n            loss_minus = compute_loss(theta_minus, theta_tar)\n            \n            grad[i] = (loss_plus - loss_minus) / (2 * h)\n            \n        return grad\n\n    def validate_gradients(grad_adj, grad_fd):\n        \"\"\"\n        Compares adjoint and finite difference gradients.\n        \"\"\"\n        for i in range(3):\n            abs_err = abs(grad_adj[i] - grad_fd[i])\n            \n            if abs_err = atol:\n                continue\n\n            denom = max(abs(grad_fd[i]), 1e-12)\n            rel_err = abs_err / denom\n            \n            if rel_err > rtol:\n                return False\n        return True\n\n    results = []\n    for case in test_cases:\n        theta_tar = simulate(\n            case[\"params_tar\"], case[\"b\"], case[\"dt\"], case[\"N\"], \n            case[\"s0\"], case[\"torque_func\"]\n        )\n        \n        grad_adj = compute_adjoint_gradient(\n            case[\"params\"], case[\"b\"], case[\"dt\"], case[\"N\"], \n            case[\"s0\"], case[\"torque_func\"], theta_tar\n        )\n        \n        grad_fd = compute_fd_gradient(\n            case[\"params\"], case[\"b\"], case[\"dt\"], case[\"N\"], \n            case[\"s0\"], case[\"torque_func\"], theta_tar\n        )\n        \n        is_passed = validate_gradients(grad_adj, grad_fd)\n        results.append(is_passed)\n\n    print(f\"[{','.join(str(r) for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}