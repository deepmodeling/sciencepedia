{
    "hands_on_practices": [
        {
            "introduction": "逆向生物力学经常面临肌肉冗余的挑战，这是一个不适定问题 (ill-posed problem)，即多种肌肉激活模式可以产生相同的关节力矩。本练习介绍 Tikhonov 正则化，这是一种经典而强大的数学技术，用于寻找唯一且符合生理学特性的解。通过完成这项练习 ，您将亲身体验逆向问题中固有的数值挑战，并理解正则化在获得稳定肌肉力估计中的基本作用。",
            "id": "4186288",
            "problem": "给定一个纯粹用数学术语表述的静态逆向生物力学问题。从肌肉力到关节力矩的映射可以用一个力臂矩阵表示。令 $R \\in \\mathbb{R}^{m \\times n}$ 表示力臂矩阵，$f \\in \\mathbb{R}^{n}$ 为未知的肌肉力向量，$\\tau \\in \\mathbb{R}^{m}$ 为测得的关节力矩向量。$\\tau$ 的测量值带有噪声。关节力矩的静态平衡关系建模为 $ \\tau = R f + \\varepsilon $，其中 $ \\varepsilon $ 表示测量噪声。为了获得冗余系统（当 $n > m$ 时）中 $f$ 的稳健估计，使用零阶 Tikhonov 正则化和正规方程。\n\n从上述定义和力学基本定律出发，推导正则化正规方程，并为几个测试用例计算肌肉力 $f$ 的正则化解。然后，通过对正则化参数 $\\alpha$ 的解析导数和正规矩阵的条件数来分析 $f$ 对 $\\alpha$ 的灵敏度。所有计算均需使用国际单位制 (SI) 表示：力以牛顿 (N) 为单位，力矩以牛顿米 (N·m) 为单位。本问题不需要角度单位。\n\n您必须基于以下基本原理：\n- 牛顿第二运动定律和静态平衡原理意味着净关节力矩等于由肌肉力通过力臂产生的力矩之和。\n- 用于最小二乘法和正则化的线性代数定义与性质。\n\n任务要求：\n1. 从第一性原理出发，推导零阶 Tikhonov 问题的正则化正规方程，然后仅使用正规方程为每个指定的测试用例计算正则化解 $f$。\n2. 对于每个测试用例和一组给定的三个严格为正的正则化参数 $ \\alpha $，计算：\n   - 对每个给定的 $ \\alpha $ 计算正则化解 $ f(\\alpha) $。\n   - 使用 $2$-范数，计算集合中最小和最大 $ \\alpha $ 值对应的正则化正规矩阵 $ R^\\top R + \\alpha I $ 的条件数。\n   - 在中间的 $ \\alpha $ 值处计算解析灵敏度大小，定义为 $$ s = \\frac{\\| \\frac{d f}{d \\alpha} \\|_2}{\\| f \\|_2} $$ 其中 $ \\frac{d f}{d \\alpha} $ 是解关于 $ \\alpha $ 的导数。\n   - 一个布尔值，指示当 $ \\alpha $ 在给定的三个值上增加时，$ \\| f(\\alpha) \\|_2 $ 是否非增。\n3. 将浮点数四舍五入到六位小数。灵敏度 $ s $ 和条件数是无量纲的。不要输出力或力矩本身；仅输出所要求的无量纲度量和布尔单调性检查。\n4. 使用以下测试套件。每个测试用例提供 $R$、一个带噪声的力矩向量 $\\tau$ 和一个严格递增的包含三个正则化参数 $\\alpha$ 的列表：\n   - 测试用例 1（两个关节，三个肌肉，中等噪声）：\n     $$\n     R_1 = \\begin{bmatrix}\n     0.04  0.02  -0.01 \\\\\n     0.00  0.03  0.05\n     \\end{bmatrix}, \\quad\n     \\tau_1 = \\begin{bmatrix}\n     6.6 \\\\\n     4.9\n     \\end{bmatrix}, \\quad\n     \\alpha_1 = \\left[ 10^{-4},\\, 10^{-2},\\, 1.0 \\right].\n     $$\n   - 测试用例 2（一个关节，四个肌肉，接近共线性和小噪声）：\n     $$\n     R_2 = \\begin{bmatrix}\n     0.03  0.029  -0.03  0.0301\n     \\end{bmatrix}, \\quad\n     \\tau_2 = \\begin{bmatrix}\n     3.149\n     \\end{bmatrix}, \\quad\n     \\alpha_2 = \\left[ 10^{-6},\\, 10^{-3},\\, 10^{-1} \\right].\n     $$\n   - 测试用例 3（三个关节，六个肌肉，较高噪声和轻度依赖性）：\n     $$\n     R_3 = \\begin{bmatrix}\n     0.05  0.02  -0.01  0.00  0.03  -0.02 \\\\\n     0.00  0.04  0.02  -0.01  0.01  0.03 \\\\\n     0.01  -0.01  0.03  0.02  -0.03  0.00\n     \\end{bmatrix}, \\quad\n     \\tau_3 = \\begin{bmatrix}\n     7.7 \\\\\n     4.1 \\\\\n     2.4\n     \\end{bmatrix}, \\quad\n     \\alpha_3 = \\left[ 10^{-5},\\, 10^{-2},\\, 1.0 \\right].\n     $$\n5. 最终输出格式：\n   - 您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个测试用例贡献一个形如 $[\\text{cond\\_low}, \\text{cond\\_high}, \\text{sensitivity\\_mid}, \\text{monotone}]$ 的子列表。例如：\n     $$\n     \\left[ [c_{1,\\text{low}}, c_{1,\\text{high}}, s_1, \\text{True}], [c_{2,\\text{low}}, c_{2,\\text{high}}, s_2, \\text{False}], [c_{3,\\text{low}}, c_{3,\\text{high}}, s_3, \\text{True}] \\right].\n     $$\n   - 所有浮点值必须四舍五入到六位小数。布尔值必须写作 $\\text{True}$ 或 $\\text{False}$。",
            "solution": "该问题经评估有效。它在科学上基于静态生物力学和数值线性代数的既定原理，通过使用 Tikhonov 正则化得到了良好定义，并用客观、完整、一致的数学定义和数据进行了规定。\n\n问题的核心是根据关节力矩向量 $\\tau \\in \\mathbb{R}^{m}$ 的带噪声测量值，在给定线性关系 $\\tau \\approx R f$ 的情况下，为肌肉力向量 $f \\in \\mathbb{R}^{n}$ 找到一个稳定的估计，其中 $R \\in \\mathbb{R}^{m \\times n}$ 是力臂矩阵。对于冗余系统（$n > m$），该问题是欠定的和不适定的。零阶 Tikhonov 正则化通过寻找一个向量 $f$ 来最小化一个复合成本函数来解决此问题，该函数平衡了数据保真度（模型的平方误差）和解的大小（力的 $L_2$ 范数的平方），并由一个正则化参数 $\\alpha > 0$ 控制。\n\n### 正则化正规方程的推导\nTikhonov 成本函数 $J(f)$ 定义为：\n$$ J(f) = \\| R f - \\tau \\|_2^2 + \\alpha \\| f \\|_2^2 $$\n此处，$\\| \\cdot \\|_2$ 表示欧几里得 ($L_2$) 范数。我们寻求最小化 $J(f)$ 的向量 $f$。为了推导解，我们首先使用恒等式 $\\|x\\|_2^2 = x^\\top x$ 展开范数项：\n$$ J(f) = (R f - \\tau)^\\top (R f - \\tau) + \\alpha (f^\\top f) $$\n展开第一项得到：\n$$ J(f) = (f^\\top R^\\top - \\tau^\\top)(R f - \\tau) + \\alpha f^\\top f $$\n$$ J(f) = f^\\top R^\\top R f - f^\\top R^\\top \\tau - \\tau^\\top R f + \\tau^\\top \\tau + \\alpha f^\\top f $$\n由于项 $\\tau^\\top R f$ 是一个标量，它等于其转置 $(f^\\top R^\\top \\tau)^\\top$，即 $f^\\top R^\\top \\tau$。这使得两个交叉项可以合并。我们也可以将 $\\alpha f^\\top f$ 写成 $\\alpha f^\\top I f$，其中 $I$ 是大小为 $n \\times n$ 的单位矩阵。\n$$ J(f) = f^\\top (R^\\top R) f - 2 f^\\top R^\\top \\tau + \\tau^\\top \\tau + f^\\top (\\alpha I) f $$\n$$ J(f) = f^\\top (R^\\top R + \\alpha I) f - 2 (R^\\top \\tau)^\\top f + \\tau^\\top \\tau $$\n此表达式是关于 $f$ 的二次型。存在一个最小值，可以通过求 $J(f)$ 对 $f$ 的梯度并将其设为零来找到。使用矩阵微积分恒等式 $\\frac{\\partial}{\\partial x}(x^\\top A x) = 2Ax$（对于对称矩阵 $A$）和 $\\frac{\\partial}{\\partial x}(b^\\top x) = b$，我们得到：\n$$ \\nabla_f J(f) = 2(R^\\top R + \\alpha I) f - 2 R^\\top \\tau $$\n将梯度设为零，$\\nabla_f J(f) = 0$：\n$$ 2(R^\\top R + \\alpha I) f - 2 R^\\top \\tau = 0 $$\n$$ (R^\\top R + \\alpha I) f = R^\\top \\tau $$\n这就是**正则化正规方程**。矩阵 $A(\\alpha) = R^\\top R + \\alpha I$ 是对称的。由于 $R^\\top R$ 是半正定的且 $\\alpha > 0$，所以 $A(\\alpha)$ 是正定的，因此是可逆的。肌肉力向量 $f$ 的解是唯一的，由下式给出：\n$$ f(\\alpha) = (R^\\top R + \\alpha I)^{-1} R^\\top \\tau $$\n\n### 解析灵敏度的推导\n解 $f$ 关于正则化参数 $\\alpha$ 的灵敏度是其导数 $\\frac{df}{d\\alpha}$。这可以通过对正规方程关于 $\\alpha$ 进行隐式微分来找到：\n$$ \\frac{d}{d\\alpha} \\left[ (R^\\top R + \\alpha I) f(\\alpha) \\right] = \\frac{d}{d\\alpha} (R^\\top \\tau) $$\n右侧为零，因为 $R$ 和 $\\tau$ 相对于 $\\alpha$ 是常数。对左侧应用微分的乘法法则得到：\n$$ \\left[ \\frac{d}{d\\alpha} (R^\\top R + \\alpha I) \\right] f(\\alpha) + (R^\\top R + \\alpha I) \\frac{df}{d\\alpha} = 0 $$\n$(R^\\top R + \\alpha I)$ 关于 $\\alpha$ 的导数是单位矩阵 $I$。\n$$ I f(\\alpha) + (R^\\top R + \\alpha I) \\frac{df}{d\\alpha} = 0 $$\n求解导数 $\\frac{df}{d\\alpha}$：\n$$ \\frac{df}{d\\alpha} = -(R^\\top R + \\alpha I)^{-1} f(\\alpha) $$\n问题要求相对灵敏度大小，定义为：\n$$ s = \\frac{\\| \\frac{df}{d\\alpha} \\|_2}{\\| f(\\alpha) \\|_2} $$\n\n### 度量的计算\n对于每个测试用例及其包含三个严格递增的正则化参数 $(\\alpha_{low}, \\alpha_{mid}, \\alpha_{high})$ 的集合，计算以下度量：\n1.  **条件数 ($cond_2$)**：对于 $\\alpha_{low}$ 和 $\\alpha_{high}$，计算正则化正规矩阵 $A(\\alpha) = R^\\top R + \\alpha I$ 的 $2$-范数条件数。它衡量了线性系统的数值稳定性，值越高表示病态程度越严重。\n2.  **灵敏度大小 ($s$)**：使用推导出的 $f(\\alpha_{mid})$ 和 $\\frac{df}{d\\alpha}|_{\\alpha_{mid}}$ 的解析表达式，在 $\\alpha = \\alpha_{mid}$ 处计算灵敏度 $s$。\n3.  **单调性检查**：计算解向量的 $2$-范数 $\\|f(\\alpha_{low})\\|_2$、$\\|f(\\alpha_{mid})\\|_2$ 和 $\\|f(\\alpha_{high})\\|_2$。如果 $\\|f(\\alpha_{low})\\|_2 \\ge \\|f(\\alpha_{mid})\\|_2 \\ge \\|f(\\alpha_{high})\\|_2$，则布尔值 `monotone` 为 `True`，否则为 `False`。可以证明，对于 $\\alpha > 0$，$ \\|f(\\alpha)\\|_2 $ 是一个单调递减函数，所以预期结果应为 `True`。\n\n这些推导为数值实现提供了基础。",
            "answer": "```python\nimport numpy as np\n\ndef calculate_metrics(R, tau, alphas):\n    \"\"\"\n    Computes Tikhonov regularization metrics for a given biomechanics problem.\n\n    Args:\n        R (np.ndarray): The moment arm matrix (m x n).\n        tau (np.ndarray): The measured torque vector (m,).\n        alphas (list): A list of three strictly positive regularization parameters.\n\n    Returns:\n        list: A list containing [cond_low, cond_high, sensitivity_mid, monotone].\n    \"\"\"\n    alpha_low, alpha_mid, alpha_high = alphas\n    n = R.shape[1]\n    I = np.identity(n)\n\n    Rt = R.T\n    RtR = Rt @ R\n    Rt_tau = Rt @ tau\n\n    # --- Condition Numbers ---\n    A_low = RtR + alpha_low * I\n    cond_low = np.linalg.cond(A_low, 2)\n\n    A_high = RtR + alpha_high * I\n    cond_high = np.linalg.cond(A_high, 2)\n\n    # --- Sensitivity at alpha_mid ---\n    A_mid = RtR + alpha_mid * I\n    A_mid_inv = np.linalg.inv(A_mid)\n    \n    f_mid = A_mid_inv @ Rt_tau\n    \n    df_dalpha_mid = -A_mid_inv @ f_mid\n    \n    norm_f_mid = np.linalg.norm(f_mid, 2)\n    norm_df_dalpha_mid = np.linalg.norm(df_dalpha_mid, 2)\n    \n    if norm_f_mid  1e-12:  # Avoid division by zero if f is essentially zero\n        s_mid = 0.0\n    else:\n        s_mid = norm_df_dalpha_mid / norm_f_mid\n\n    # --- Monotonicity Check ---\n    f_low = np.linalg.inv(A_low) @ Rt_tau\n    norm_f_low = np.linalg.norm(f_low, 2)\n\n    f_high = np.linalg.inv(A_high) @ Rt_tau\n    norm_f_high = np.linalg.norm(f_high, 2)\n    \n    is_monotone = (norm_f_low >= norm_f_mid) and (norm_f_mid >= norm_f_high)\n\n    return [cond_low, cond_high, s_mid, is_monotone]\n\ndef solve():\n    \"\"\"\n    Defines test cases and computes the required metrics for each,\n    then prints the results in the specified format.\n    \"\"\"\n    test_cases = [\n        {\n            \"R\": np.array([[0.04, 0.02, -0.01], [0.00, 0.03, 0.05]]),\n            \"tau\": np.array([6.6, 4.9]),\n            \"alphas\": [1e-4, 1e-2, 1.0],\n        },\n        {\n            \"R\": np.array([[0.03, 0.029, -0.03, 0.0301]]),\n            \"tau\": np.array([3.149]),\n            \"alphas\": [1e-6, 1e-3, 1e-1],\n        },\n        {\n            \"R\": np.array([\n                [0.05, 0.02, -0.01, 0.00, 0.03, -0.02],\n                [0.00, 0.04, 0.02, -0.01, 0.01, 0.03],\n                [0.01, -0.01, 0.03, 0.02, -0.03, 0.00]\n            ]),\n            \"tau\": np.array([7.7, 4.1, 2.4]),\n            \"alphas\": [1e-5, 1e-2, 1.0],\n        },\n    ]\n\n    all_results = []\n    for case in test_cases:\n        metrics = calculate_metrics(case[\"R\"], case[\"tau\"], case[\"alphas\"])\n        all_results.append(metrics)\n\n    # Format the final output string precisely as requested.\n    string_results = []\n    for res in all_results:\n        # res = [cond_low, cond_high, s_mid, monotone]\n        # Floats are formatted to six decimal places.\n        # str(bool) correctly produces 'True' or 'False'.\n        res_str = (\n            f\"[{res[0]:.6f},{res[1]:.6f},{res[2]:.6f},{str(res[3])}]\"\n        )\n        string_results.append(res_str)\n    \n    final_output = f\"[{','.join(string_results)}]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "当使用神经网络进行预测时，仅获得单个点估计通常是不够的；我们还必须量化该预测的置信度。本练习深入探讨了不确定性分解，将数据中固有的随机性（偶然不确定性, aleatoric uncertainty）与模型自身的不确定性（认知不确定性, epistemic uncertainty）分离开来。通过分析一个模型集成  的输出，您将学到一种评估预测性生物力学模型可靠性的原则性方法。",
            "id": "4186272",
            "problem": "给定一个用于预测生物力学的神经网络模型集成，其中每个模型都提供步态过程中膝关节屈曲力矩的概率估计。对于描述运动学和人体测量学特征的固定输入 $x$，模型 $m$ 输出一个高斯预测分布，其均值为 $\\mu_m(x)$（单位为 $\\mathrm{N}\\cdot\\mathrm{m}$），方差为 $\\sigma_m^2(x)$（单位为 $(\\mathrm{N}\\cdot\\mathrm{m})^2$）。假设有一个包含 $M$ 个模型的集成，并且集成成员之间采用等权重混合。\n\n从方差和期望的定义出发，推导混合分布的预测方差分解为两个分量之和：数据噪声分量（偶然不确定性）和模型不确定性分量（认知不确定性）。仅使用以下基本原理：\n\n- 随机变量 $Y$ 的期望和方差定义：$\\mathbb{E}[Y]$ 和 $\\mathrm{Var}(Y) = \\mathbb{E}\\left[(Y - \\mathbb{E}[Y])^2\\right]$。\n- 全期望定律：$\\mathbb{E}[Y] = \\mathbb{E}_{\\Theta}\\left[ \\mathbb{E}[Y \\mid \\Theta] \\right]$，其中 $\\Theta$ 是一个表示模型标识或参数变化的随机元素。\n- 全方差定律：$\\mathrm{Var}(Y) = \\mathbb{E}_{\\Theta}\\left[ \\mathrm{Var}(Y \\mid \\Theta) \\right] + \\mathrm{Var}_{\\Theta}\\left( \\mathbb{E}[Y \\mid \\Theta] \\right)$。\n\n使用这些原理，将在集成成员等权重的情况下，用 $\\{\\mu_m(x)\\}_{m=1}^M$ 和 $\\{\\sigma_m^2(x)\\}_{m=1}^M$ 表示预测方差 $\\mathrm{Var}(Y \\mid x)$，并将其分解为偶然分量和认知分量之和。\n\n然后，实现一个程序，对下面的每个测试用例计算：\n- 总预测方差 $V_{\\text{total}}(x)$，单位为 $(\\mathrm{N}\\cdot\\mathrm{m})^2$。\n- 偶然方差 $V_{\\text{aleatoric}}(x)$，单位为 $(\\mathrm{N}\\cdot\\mathrm{m})^2$。\n- 认知方差 $V_{\\text{epistemic}}(x)$，单位为 $(\\mathrm{N}\\cdot\\mathrm{m})^2$。\n\n使用集成等权重混合的解释。为了数值稳定性，如果由于浮点舍入误差计算出的认知方差为负，则将其限制为零。\n\n将每个方差用 $(\\mathrm{N}\\cdot\\mathrm{m})^2$ 表示，并四舍五入到六位小数。\n\n测试套件（每个用例是针对单个输入 $x$ 的一对列表 $(\\{\\mu_m\\},\\{\\sigma_m^2\\})$）：\n\n1. 正常情况，包含 $M=5$ 的异方差集成：\n   - $\\{\\mu_m\\} = [52.0, 49.0, 55.0, 51.5, 53.0]$（单位：$\\mathrm{N}\\cdot\\mathrm{m}$）\n   - $\\{\\sigma_m^2\\} = [30.25, 20.25, 36.0, 25.0, 28.09]$（单位：$(\\mathrm{N}\\cdot\\mathrm{m})^2$）\n\n2. 边界情况，均值相同但偶然方差不同，$M=4$：\n   - $\\{\\mu_m\\} = [50.0, 50.0, 50.0, 50.0]$（单位：$\\mathrm{N}\\cdot\\mathrm{m}$）\n   - $\\{\\sigma_m^2\\} = [16.0, 25.0, 9.0, 4.0]$（单位：$(\\mathrm{N}\\cdot\\mathrm{m})^2$）\n\n3. 边缘情况，单模型集成，$M=1$：\n   - $\\{\\mu_m\\} = [60.0]$（单位：$\\mathrm{N}\\cdot\\mathrm{m}$）\n   - $\\{\\sigma_m^2\\} = [36.0]$（单位：$(\\mathrm{N}\\cdot\\mathrm{m})^2$）\n\n4. 边缘情况，偶然方差为零但均值不同，$M=2$：\n   - $\\{\\mu_m\\} = [40.0, 60.0]$（单位：$\\mathrm{N}\\cdot\\mathrm{m}$）\n   - $\\{\\sigma_m^2\\} = [0.0, 0.0]$（单位：$(\\mathrm{N}\\cdot\\mathrm{m})^2$）\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。列表中的每个项目本身都是一个对应于一个测试用例的列表 $[V_{\\text{total}},V_{\\text{aleatoric}},V_{\\text{epistemic}}]$，其中所有三个值都四舍五入到六位小数，例如：$[[v_{1,\\text{total}},v_{1,\\text{aleatoric}},v_{1,\\text{epistemic}}],[v_{2,\\text{total}},v_{2,\\text{aleatoric}},v_{2,\\text{epistemic}}],\\dots]$。",
            "solution": "令 $Y$ 表示针对固定输入 $x$ 预测的膝关节屈曲力矩（单位为 $\\mathrm{N}\\cdot\\mathrm{m}$）。考虑一个包含 $M$ 个神经网络模型的集成，每个模型提供一个条件高斯预测 $$Y \\mid \\Theta=m, x \\sim \\mathcal{N}(\\mu_m(x), \\sigma_m^2(x))$$ 其中 $\\Theta$ 是一个索引模型标识的离散随机变量。我们假设在 $\\Theta$ 上是等权重混合，即对于 $m=1,\\dots,M$，有 $\\mathbb{P}(\\Theta=m)=\\frac{1}{M}$。\n\n我们从期望和方差的核心定义出发，并使用全期望定律和全方差定律。对于任何随机变量 $Y$ 和条件变量 $\\Theta$，全期望定律给出\n$$\n\\mathbb{E}[Y \\mid x] = \\mathbb{E}_{\\Theta}\\left[ \\mathbb{E}[Y \\mid x, \\Theta] \\right],\n$$\n全方差定律给出\n$$\n\\mathrm{Var}(Y \\mid x) = \\mathbb{E}_{\\Theta}\\left[ \\mathrm{Var}(Y \\mid x, \\Theta) \\right] + \\mathrm{Var}_{\\Theta}\\left( \\mathbb{E}[Y \\mid x, \\Theta] \\right).\n$$\n\n在我们的设定中，$\\mathbb{E}[Y \\mid x, \\Theta=m] = \\mu_m(x)$ 且 $\\mathrm{Var}(Y \\mid x, \\Theta=m) = \\sigma_m^2(x)$。在 $M$ 个模型上等权重的情况下，对 $\\Theta$ 的期望变成一个均匀平均值：\n$$\n\\mathbb{E}_{\\Theta}\\left[ \\mathrm{Var}(Y \\mid x, \\Theta) \\right] = \\frac{1}{M}\\sum_{m=1}^M \\sigma_m^2(x),\n$$\n以及\n$$\n\\mathrm{Var}_{\\Theta}\\left( \\mathbb{E}[Y \\mid x, \\Theta] \\right) = \\frac{1}{M}\\sum_{m=1}^M \\left(\\mu_m(x)\\right)^2 - \\left( \\frac{1}{M}\\sum_{m=1}^M \\mu_m(x) \\right)^2.\n$$\n\n因此，预测方差分解为两个可解释的分量：\n- 偶然方差（数据噪声）：\n$$\nV_{\\text{aleatoric}}(x) = \\frac{1}{M}\\sum_{m=1}^M \\sigma_m^2(x),\n$$\n它捕捉了由测量噪声和每个网络条件分布建模的内在变异性所导致的不可约不确定性。\n- 认知方差（模型不确定性）：\n$$\nV_{\\text{epistemic}}(x) = \\frac{1}{M}\\sum_{m=1}^M \\left(\\mu_m(x)\\right)^2 - \\left( \\frac{1}{M}\\sum_{m=1}^M \\mu_m(x) \\right)^2,\n$$\n它捕捉了不同模型预测均值之间的变异性，并且可以通过更多数据或更好的建模来减少。\n\n总预测方差是它们的和：\n$$\nV_{\\text{total}}(x) = V_{\\text{aleatoric}}(x) + V_{\\text{epistemic}}(x).\n$$\n\n基于这些原理的算法设计：\n1. 对于每个测试用例，存储 $\\{\\mu_m\\}_{m=1}^M$ 和 $\\{\\sigma_m^2\\}_{m=1}^M$。\n2. 计算 $\\sigma_m^2$ 的均匀平均值以获得 $V_{\\text{aleatoric}}(x)$。\n3. 计算 $\\mu_m$ 和 $\\mu_m^2$ 的均匀平均值：\n   - 令 $\\bar{\\mu} = \\frac{1}{M}\\sum_{m=1}^M \\mu_m(x)$，\n   - 令 $\\overline{\\mu^2} = \\frac{1}{M}\\sum_{m=1}^M \\left(\\mu_m(x)\\right)^2$，\n   则 $V_{\\text{epistemic}}(x) = \\overline{\\mu^2} - \\bar{\\mu}^2$。\n4. 计算 $V_{\\text{total}}(x) = V_{\\text{aleatoric}}(x) + V_{\\text{epistemic}}(x)$。\n5. 为保证数值稳健性，如果浮点误差导致 $V_{\\text{epistemic}}(x)  0$，则在求和前将 $V_{\\text{epistemic}}(x)$ 设置为 0。\n6. 将每个方差用 $(\\mathrm{N}\\cdot\\mathrm{m})^2$ 表示，四舍五入到六位小数，并按指定的输出格式汇总所有测试用例的结果。\n\n科学真实性和边缘覆盖：\n- 正常情况用例使用了合理的膝关节力矩量级和步态动力学中典型的异方差。\n- 相同的均值隔离了偶然不确定性，如预期那样产生零认知方差。\n- 单模型用例根据其构造，认知方差为零。\n- 零偶然方差用例隔离了由预测均值不一致引起的认知不确定性。\n\n此方法遵循全方差定律，并提供了一个与基于集成的预测生物力学建模相一致的、有原则的分解。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_variance_components(mu_list, var_list):\n    \"\"\"\n    Compute aleatoric, epistemic, and total predictive variance for an ensemble.\n    Inputs:\n        mu_list: list or 1D array of predicted means (N·m)\n        var_list: list or 1D array of predicted variances ((N·m)^2)\n    Returns:\n        (V_total, V_aleatoric, V_epistemic) in ((N·m)^2)\n    \"\"\"\n    mu = np.asarray(mu_list, dtype=float)\n    sigma2 = np.asarray(var_list, dtype=float)\n    M = mu.size\n    if M == 0:\n        raise ValueError(\"Empty ensemble.\")\n\n    # Aleatoric variance: mean of predicted variances across ensemble members\n    V_aleatoric = float(np.mean(sigma2))\n\n    # Epistemic variance: variance of predicted means under equal weights\n    mean_mu = float(np.mean(mu))\n    mean_mu_sq = float(np.mean(mu**2))\n    V_epistemic = mean_mu_sq - mean_mu**2\n\n    # Numerical stability: clamp tiny negative due to floating point\n    if V_epistemic  0 and V_epistemic > -1e-12:\n        V_epistemic = 0.0\n    elif V_epistemic  -1e-12:\n        # If significantly negative, input data may be inconsistent;\n        # still clamp to zero to satisfy non-negativity.\n        V_epistemic = 0.0\n\n    V_total = V_aleatoric + V_epistemic\n    return V_total, V_aleatoric, V_epistemic\n\ndef format_results(results, decimals=6):\n    \"\"\"\n    Format list of tuples (total, aleatoric, epistemic) into the required single-line string.\n    Rounds each float to specified decimals.\n    \"\"\"\n    def fmt_float(x):\n        return f\"{round(x, decimals):.{decimals}f}\"\n    inner_lists = []\n    for t, a, e in results:\n        inner = f\"[{fmt_float(t)},{fmt_float(a)},{fmt_float(e)}]\"\n        inner_lists.append(inner)\n    return f\"[{','.join(inner_lists)}]\"\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each test case is a tuple: (mu_list, var_list)\n    test_cases = [\n        # 1. Happy path, heteroscedastic ensemble with M=5\n        ([52.0, 49.0, 55.0, 51.5, 53.0], [30.25, 20.25, 36.0, 25.0, 28.09]),\n        # 2. Boundary case, identical means, varying variances, M=4\n        ([50.0, 50.0, 50.0, 50.0], [16.0, 25.0, 9.0, 4.0]),\n        # 3. Edge case, single-model ensemble, M=1\n        ([60.0], [36.0]),\n        # 4. Edge case, zero aleatoric variance, differing means, M=2\n        ([40.0, 60.0], [0.0, 0.0]),\n    ]\n\n    results = []\n    for mu_list, var_list in test_cases:\n        V_total, V_aleatoric, V_epistemic = compute_variance_components(mu_list, var_list)\n        results.append((V_total, V_aleatoric, V_epistemic))\n\n    # Final print statement in the exact required format.\n    print(format_results(results, decimals=6))\n\nsolve()\n```"
        },
        {
            "introduction": "将物理学原理整合到机器学习中的一个关键是能够高效地计算通过复杂、时变仿真的梯度。本练习将指导您实现伴随法 (adjoint method)，这是可微物理 (differentiable physics) 和随时间反向传播 (backpropagation through time) 背后的核心技术。通过计算轨迹匹配损失函数相对于仿真器物理参数的精确梯度 ，您将掌握一项在生物力学中训练物理信息模型 (physics-informed models) 和执行基于梯度的优化的基础技术。",
            "id": "4186295",
            "problem": "实现一个自包含的程序，该程序计算轨迹匹配损失相对于单个平面分段惯性参数的伴随梯度，并使用中心有限差分对这些梯度进行验证。核心生物力学模拟器模拟一个单自由度 (DOF) 平面分段（例如，小腿或前臂），在外部关节扭矩作用下围绕固定枢轴旋转。其基本依据是 Newton 旋转运动第二定律。状态由关节角度和角速度组成。待估计的惯性参数是质量、绕分段质心的转动惯量以及质心到关节的距离。\n\n基本定律和核心定义：\n- Newton 旋转运动第二定律给出 $$I_{\\text{tot}} \\, \\ddot{\\theta} + b \\, \\dot{\\theta} + m g c \\sin(\\theta) = \\tau,$$ 其中 $I_{\\text{tot}}$ 是绕关节的总转动惯量，由下式给出 $$I_{\\text{tot}} = I + m c^2,$$ $m$ 是质量，$I$ 是绕质心的转动惯量，$c$ 是质心到关节的距离，$b$ 是绕关节的粘性阻尼，$g$ 是重力加速度，$\\theta$ 是关节角度，$\\dot{\\theta}$ 是角速度，$\\ddot{\\theta}$ 是角加速度，$\\tau$ 是外部关节扭矩。重力常数 $g$ 为 $9.81 \\, \\text{m/s}^2$。\n\n离散化与仿真：\n- 使用半隐式欧拉积分，时间步长为 $dt$ 秒。对于离散时间索引 $k \\in \\{0,1,\\dots,N-1\\}$，定义\n$$\\ddot{\\theta}_k = \\frac{\\tau_k - b \\, \\dot{\\theta}_k - m g c \\sin(\\theta_k)}{I + m c^2},$$\n$$\\dot{\\theta}_{k+1} = \\dot{\\theta}_k + dt \\, \\ddot{\\theta}_k,$$\n$$\\theta_{k+1} = \\theta_k + dt \\, \\dot{\\theta}_{k+1}.$$\n初始状态为 $(\\theta_0,\\dot{\\theta}_0)$。\n\n损失与梯度：\n- 将关于角度的轨迹匹配损失定义为\n$$\\mathcal{L}(m,I,c) = \\frac{1}{2} \\sum_{k=0}^{N} w_k \\left(\\theta_k - \\theta^{\\text{tar}}_k\\right)^2,$$\n其中 $\\theta^{\\text{tar}}_k$ 是由同一模拟器在不同惯性参数下生成的目标轨迹，$w_k$ 是非负权重（对所有 $k$ 使用 $w_k = 1$）。角度必须以弧度为单位。\n- 使用离散时间伴随方法（等同于随时间反向传播 (BPTT)）为参数向量 $(m,I,c)$ 实现基于伴随的梯度计算：\n    - 令 $s_k = [\\theta_k, \\dot{\\theta}_k]^\\top$ 为状态，$s_{k+1} = f_k(s_k; m,I,c)$ 为离散更新。令 $\\lambda_k = \\partial \\mathcal{L}/\\partial s_k$ 为伴随变量。\n    - 每一步的损失贡献为 $\\ell_k = \\frac{1}{2} w_k (\\theta_k - \\theta^{\\text{tar}}_k)^2$，因此 $\\partial \\ell_k / \\partial s_k = [w_k (\\theta_k - \\theta^{\\text{tar}}_k), \\, 0]^\\top$。\n    - 伴随递归为\n    $$\\lambda_N = \\frac{\\partial \\ell_N}{\\partial s_N}, \\quad \\lambda_k = \\frac{\\partial \\ell_k}{\\partial s_k} + \\left(\\frac{\\partial f_k}{\\partial s_k}\\right)^\\top \\lambda_{k+1}, \\quad \\text{for } k = N-1,\\dots,0.$$\n    - 关于参数的梯度为\n    $$\\nabla_{(m,I,c)} \\mathcal{L} = \\sum_{k=0}^{N-1} \\left(\\frac{\\partial f_k}{\\partial (m,I,c)}\\right)^\\top \\lambda_{k+1}.$$\n- 从离散更新中推导 $\\partial f_k/\\partial s_k$ 和 $\\partial f_k/\\partial (m,I,c)$，并使用它们来实现伴随递归和梯度累积。\n\n单位：\n- 质量 $m$ 单位为千克 ($\\text{kg}$)。\n- 质心距离 $c$ 单位为米 ($\\text{m}$)。\n- 转动惯量 $I$ 单位为千克米平方 ($\\text{kg}\\cdot\\text{m}^2$)。\n- 阻尼 $b$ 单位为牛顿米秒 ($\\text{N}\\cdot\\text{m}\\cdot\\text{s}$)。\n- 扭矩 $\\tau$ 单位为牛顿米 ($\\text{N}\\cdot\\text{m}$)。\n- 时间 $dt$ 单位为秒 ($\\text{s}$)。\n- 角度 $\\theta$ 单位为弧度。\n\n与有限差分的验证：\n- 为 $(m,I,c)$ 实现中心有限差分，每个参数使用一个小的步长 $h_i$，$h_i = \\epsilon \\max(1, |p_i|)$，其中 $p_i \\in \\{m,I,c\\}$，$\\epsilon$ 是一个小的正小数（例如，$10^{-8}$）。计算\n$$ \\frac{\\partial \\mathcal{L}}{\\partial p_i} \\approx \\frac{\\mathcal{L}(p_i + h_i) - \\mathcal{L}(p_i - h_i)}{2 h_i}. $$\n- 对于每个测试用例，计算基于伴随的梯度，并使用绝对容差 $a_{\\text{tol}}$ 和相对容差 $r_{\\text{tol}}$ 将其与每个参数的有限差分梯度进行比较：如果满足以下条件，则参数通过测试\n$$|\\nabla^{\\text{adj}}_i - \\nabla^{\\text{fd}}_i| \\le a_{\\text{tol}} \\quad \\text{或} \\quad \\frac{|\\nabla^{\\text{adj}}_i - \\nabla^{\\text{fd}}_i|}{\\max(|\\nabla^{\\text{fd}}_i|, 10^{-12})} \\le r_{\\text{tol}}.$$\n使用 $a_{\\text{tol}} = 10^{-9}$ 和 $r_{\\text{tol}} = 10^{-6}$。如果所有三个参数都通过，则测试用例通过。\n\n测试套件：\n- 在所有情况下，重力设定为 $g = 9.81 \\, \\text{m/s}^2$。阻尼 $b$ 根据具体情况指定。对每个用例的预测轨迹和目标轨迹使用相同的外部扭矩。\n- 用例 1 (一般情况)：\n    - 参数：$m = 5.0$ $\\text{kg}$，$I = 0.25$ $\\text{kg}\\cdot\\text{m}^2$，$c = 0.20$ $\\text{m}$。\n    - 目标参数：$m^{\\text{tar}} = 5.5$ $\\text{kg}$，$I^{\\text{tar}} = 0.28$ $\\text{kg}\\cdot\\text{m}^2$，$c^{\\text{tar}} = 0.22$ $\\text{m}$。\n    - 时间步长：$dt = 0.002$ $\\text{s}$，步数：$N = 500$。\n    - 阻尼：$b = 0.05$ $\\text{N}\\cdot\\text{m}\\cdot\\text{s}$。\n    - 初始状态：$\\theta_0 = 0.0$ $\\text{rad}$，$\\dot{\\theta}_0 = 0.0$ $\\text{rad}/\\text{s}$。\n    - 扭矩：$\\tau_k = A \\sin(2 \\pi f t_k)$，其中 $A = 1.0$ $\\text{N}\\cdot\\text{m}$，$f = 1.0$ $\\text{Hz}$，$t_k = k \\, dt$。\n- 用例 2 (仅重力边界)：\n    - 参数：$m = 3.0$ $\\text{kg}$，$I = 0.15$ $\\text{kg}\\cdot\\text{m}^2$，$c = 0.18$ $\\text{m}$。\n    - 目标参数：$m^{\\text{tar}} = 3.1$ $\\text{kg}$，$I^{\\text{tar}} = 0.14$ $\\text{kg}\\cdot\\text{m}^2$，$c^{\\text{tar}} = 0.17$ $\\text{m}$。\n    - 时间步长：$dt = 0.00125$ $\\text{s}$，步数：$N = 800$。\n    - 阻尼：$b = 0.02$ $\\text{N}\\cdot\\text{m}\\cdot\\text{s}$。\n    - 初始状态：$\\theta_0 = 0.30$ $\\text{rad}$，$\\dot{\\theta}_0 = 0.0$ $\\text{rad}/\\text{s}$。\n    - 扭矩：对所有 $k$，$\\tau_k = 0$。\n- 用例 3 (更高阻尼，更高频率)：\n    - 参数：$m = 7.0$ $\\text{kg}$，$I = 0.40$ $\\text{kg}\\cdot\\text{m}^2$，$c = 0.25$ $\\text{m}$。\n    - 目标参数：$m^{\\text{tar}} = 7.2$ $\\text{kg}$，$I^{\\text{tar}} = 0.42$ $\\text{kg}\\cdot\\text{m}^2$，$c^{\\text{tar}} = 0.26$ $\\text{m}$。\n    - 时间步长：$dt = 0.002$ $\\text{s}$，步数：$N = 600$。\n    - 阻尼：$b = 0.20$ $\\text{N}\\cdot\\text{m}\\cdot\\text{s}$。\n    - 初始状态：$\\theta_0 = -0.20$ $\\text{rad}$，$\\dot{\\theta}_0 = 0.0$ $\\text{rad}/\\text{s}$。\n    - 扭矩：$\\tau_k = A \\sin(2 \\pi f t_k)$，其中 $A = 0.8$ $\\text{N}\\cdot\\text{m}$，$f = 3.0$ $\\text{Hz}$，$t_k = k \\, dt$。\n- 用例 4 (小转动惯量和质心距离)：\n    - 参数：$m = 2.0$ $\\text{kg}$，$I = 0.05$ $\\text{kg}\\cdot\\text{m}^2$，$c = 0.05$ $\\text{m}$。\n    - 目标参数：$m^{\\text{tar}} = 2.1$ $\\text{kg}$，$I^{\\text{tar}} = 0.052$ $\\text{kg}\\cdot\\text{m}^2$，$c^{\\text{tar}} = 0.045$ $\\text{m}$。\n    - 时间步长：$dt = 0.0015$ $\\text{s}$，步数：$N = 700$。\n    - 阻尼：$b = 0.03$ $\\text{N}\\cdot\\text{m}\\cdot\\text{s}$。\n    - 初始状态：$\\theta_0 = 0.10$ $\\text{rad}$，$\\dot{\\theta}_0 = 0.0$ $\\text{rad}/\\text{s}$。\n    - 扭矩：$\\tau_k = A \\sin(2 \\pi f t_k)$，其中 $A = 0.3$ $\\text{N}\\cdot\\text{m}$，$f = 1.5$ $\\text{Hz}$，$t_k = k \\, dt$。\n\n程序要求：\n- 按照上述规定实现离散模拟器、基于伴随的梯度和中心有限差分。\n- 对于每个用例，计算所有三个参数梯度是否都通过容差测试。汇总这些通过的布尔值。\n- 最终输出格式：您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，$[\\text{result1},\\text{result2},\\text{result3},\\text{result4}]$），其中每个结果是一个布尔值，表示相应测试用例的通过或失败。",
            "solution": "所提出的问题要求实现并验证一种基于伴随的方法，用于计算轨迹匹配损失函数相对于平面刚体惯性参数的梯度。验证将通过与中心有限差分近似进行比较来执行。该问题在经典力学和数值优化方面有充分的科学依据，并且其规定足够详细，可以得到唯一且可验证的解。\n\n该过程涉及三个主要阶段：\n1.  **正向仿真**：使用基于所提供的运动方程和积分方案的数值模拟器来生成状态轨迹。\n2.  **基于伴随的梯度计算**：实现一种基于伴随的方法，该方法在计算上等同于随时间反向传播 (BPTT)，以计算损失函数相对于模型参数的精确梯度。这需要推导离散时间状态转移函数的解析雅可比矩阵。\n3.  **梯度验证**：将解析推导的伴随梯度与使用中心有限差分法的数值近似进行比较，以确保其正确性。\n\n在此，我们将形式化必要的推导和算法步骤。\n\n**1. 正向动力学和离散化**\n\n单自由度平面分段的动力学由 Newton 旋转运动第二定律决定：\n$$I_{\\text{tot}} \\, \\ddot{\\theta} + b \\, \\dot{\\theta} + m g c \\sin(\\theta) = \\tau$$\n其中 $I_{\\text{tot}} = I + m c^2$ 是绕枢轴的总转动惯量，由平行轴定理导出。参数为质量 $m$、绕质心的转动惯量 $I$ 以及质心到关节的距离 $c$。常数包括粘性阻尼 $b$ 和重力加速度 $g = 9.81 \\, \\text{m/s}^2$。状态由角度 $\\theta$ 和角速度 $\\dot{\\theta}$ 定义。\n\n在离散时间步 $k$ 的角加速度 $\\ddot{\\theta}_k$ 可表示为：\n$$\\ddot{\\theta}_k = f_{\\text{accel}}(\\theta_k, \\dot{\\theta}_k; m, I, c) = \\frac{\\tau_k - b \\dot{\\theta}_k - m g c \\sin(\\theta_k)}{I + m c^2}$$\n问题指定了时间步长为 $dt$ 的半隐式欧拉积分方案：\n$$\n\\begin{align*}\n\\dot{\\theta}_{k+1} = \\dot{\\theta}_k + dt \\, \\ddot{\\theta}_k \\\\\n\\theta_{k+1} = \\theta_k + dt \\, \\dot{\\theta}_{k+1}\n\\end{align*}\n$$\n通过将 $\\dot{\\theta}_{k+1}$ 的表达式代入 $\\theta_{k+1}$ 的更新公式，我们可以将状态 $s_k = [\\theta_k, \\dot{\\theta}_k]^\\top$ 的完整状态更新写为 $s_{k+1} = f_k(s_k; m, I, c)$：\n$$\n\\begin{align*}\n\\theta_{k+1} = \\theta_k + dt (\\dot{\\theta}_k + dt \\, \\ddot{\\theta}_k) = \\theta_k + dt \\dot{\\theta}_k + dt^2 \\ddot{\\theta}_k \\\\\n\\dot{\\theta}_{k+1} = \\dot{\\theta}_k + dt \\, \\ddot{\\theta}_k\n\\end{align*}\n$$\n该公式明确地将第 $k+1$ 步的状态与第 $k$ 步的状态关联起来。\n\n**2. 基于伴随的梯度计算**\n\n目标是计算损失函数 $\\mathcal{L}(m,I,c) = \\frac{1}{2} \\sum_{k=0}^{N} (\\theta_k - \\theta^{\\text{tar}}_k)^2$ 相对于参数 $p = (m, I, c)$ 的梯度。离散伴随方法为此提供了一种高效的方式。我们定义一个伴随状态向量 $\\lambda_k = \\partial \\mathcal{L} / \\partial s_k$ 并将其在时间上反向传播。\n\n反向递归在最终时间步 $N$ 初始化：\n$$ \\lambda_N = \\frac{\\partial \\mathcal{L}}{\\partial s_N} = \\begin{bmatrix} \\partial \\mathcal{L} / \\partial \\theta_N \\\\ \\partial \\mathcal{L} / \\partial \\dot{\\theta}_N \\end{bmatrix} = \\begin{bmatrix} (\\theta_N - \\theta^{\\text{tar}}_N) \\\\ 0 \\end{bmatrix} $$\n对于 $k = N-1, \\dots, 0$，伴随状态根据链式法则更新：\n$$ \\lambda_k = \\frac{\\partial \\mathcal{L}}{\\partial s_k} = \\frac{\\partial \\ell_k}{\\partial s_k} + \\left(\\frac{\\partial s_{k+1}}{\\partial s_k}\\right)^\\top \\frac{\\partial \\mathcal{L}}{\\partial s_{k+1}} = \\frac{\\partial \\ell_k}{\\partial s_k} + \\left(\\frac{\\partial f_k}{\\partial s_k}\\right)^\\top \\lambda_{k+1} $$\n其中 $\\frac{\\partial \\ell_k}{\\partial s_k} = [(\\theta_k - \\theta^{\\text{tar}}_k), \\, 0]^\\top$。\n\n相对于参数 $p$ 的总梯度是在所有时间步上累积的：\n$$ \\nabla_p \\mathcal{L} = \\sum_{k=0}^{N-1} \\left(\\frac{\\partial s_{k+1}}{\\partial p}\\right)^\\top \\frac{\\partial \\mathcal{L}}{\\partial s_{k+1}} = \\sum_{k=0}^{N-1} \\left(\\frac{\\partial f_k}{\\partial p}\\right)^\\top \\lambda_{k+1} $$\n\n**3. 状态和参数雅可比矩阵的推导**\n\n为了实现伴随方法，我们必须推导雅可比矩阵 $\\frac{\\partial f_k}{\\partial s_k}$ 和 $\\frac{\\partial f_k}{\\partial p}$。令 $D = I + m c^2$。\n\n**状态雅可比矩阵 $\\frac{\\partial f_k}{\\partial s_k}$：**\n首先，我们求 $\\ddot{\\theta}_k$ 对状态变量 $\\theta_k$ 和 $\\dot{\\theta}_k$ 的偏导数：\n$$\\frac{\\partial \\ddot{\\theta}_k}{\\partial \\theta_k} = -\\frac{m g c \\cos(\\theta_k)}{D}, \\quad \\frac{\\partial \\ddot{\\theta}_k}{\\partial \\dot{\\theta}_k} = -\\frac{b}{D}$$\n利用这些，我们构建状态转移函数 $f_k$ 的 $2 \\times 2$ 雅可比矩阵：\n$$\\frac{\\partial f_k}{\\partial s_k} = \\begin{bmatrix} \\frac{\\partial \\theta_{k+1}}{\\partial \\theta_k}  \\frac{\\partial \\theta_{k+1}}{\\partial \\dot{\\theta}_k} \\\\ \\frac{\\partial \\dot{\\theta}_{k+1}}{\\partial \\theta_k}  \\frac{\\partial \\dot{\\theta}_{k+1}}{\\partial \\dot{\\theta}_k} \\end{bmatrix} = \\begin{bmatrix} 1 + dt^2 \\frac{\\partial \\ddot{\\theta}_k}{\\partial \\theta_k}  dt + dt^2 \\frac{\\partial \\ddot{\\theta}_k}{\\partial \\dot{\\theta}_k} \\\\ dt \\frac{\\partial \\ddot{\\theta}_k}{\\partial \\theta_k}  1 + dt \\frac{\\partial \\ddot{\\theta}_k}{\\partial \\dot{\\theta}_k} \\end{bmatrix}$$\n\n**参数雅可比矩阵 $\\frac{\\partial f_k}{\\partial p}$：**\n状态更新对参数 $p_i \\in \\{m, I, c\\}$ 的偏导数为：\n$$\\frac{\\partial \\theta_{k+1}}{\\partial p_i} = dt^2 \\frac{\\partial \\ddot{\\theta}_k}{\\partial p_i}, \\quad \\frac{\\partial \\dot{\\theta}_{k+1}}{\\partial p_i} = dt \\frac{\\partial \\ddot{\\theta}_k}{\\partial p_i}$$\n这可以紧凑地写为 $\\frac{\\partial f_k}{\\partial p} = [dt^2, dt]^\\top \\frac{\\partial \\ddot{\\theta}_k}{\\partial p}$。我们需要 $\\ddot{\\theta}_k$ 对 $m, I, c$ 的偏导数：\n\n-   关于质量 $m$：\n    $$\\frac{\\partial \\ddot{\\theta}_k}{\\partial m} = \\frac{\\partial}{\\partial m}\\left(\\frac{\\tau_k - b \\dot{\\theta}_k - m g c \\sin(\\theta_k)}{I + m c^2}\\right) = \\frac{-g c \\sin(\\theta_k) \\cdot D - (\\tau_k - b \\dot{\\theta}_k - m g c \\sin(\\theta_k)) \\cdot c^2}{D^2} = -\\frac{g c \\sin(\\theta_k)}{D} - \\ddot{\\theta}_k \\frac{c^2}{D}$$\n\n-   关于转动惯量 $I$：\n    $$\\frac{\\partial \\ddot{\\theta}_k}{\\partial I} = \\frac{\\partial}{\\partial I}\\left(\\frac{\\tau_k - b \\dot{\\theta}_k - m g c \\sin(\\theta_k)}{I + m c^2}\\right) = \\frac{-(\\tau_k - b \\dot{\\theta}_k - m g c \\sin(\\theta_k))}{D^2} = -\\frac{\\ddot{\\theta}_k}{D}$$\n\n-   关于质心距离 $c$：\n    $$\\frac{\\partial \\ddot{\\theta}_k}{\\partial c} = \\frac{\\partial}{\\partial c}\\left(\\frac{\\tau_k - b \\dot{\\theta}_k - m g c \\sin(\\theta_k)}{I + m c^2}\\right) = \\frac{-m g \\sin(\\theta_k) \\cdot D - (\\tau_k - b \\dot{\\theta}_k - m g c \\sin(\\theta_k)) \\cdot 2mc}{D^2} = -\\frac{m g \\sin(\\theta_k)}{D} - \\ddot{\\theta}_k \\frac{2mc}{D}$$\n\n这些雅可比矩阵在前向传递的每一步中计算并存储。然后，后向传递计算伴随状态并累积梯度。\n\n**4. 通过有限差分进行梯度验证**\n\n为确认解析梯度的正确性，我们将其与中心有限差分法的数值近似进行比较：\n$$\\frac{\\partial \\mathcal{L}}{\\partial p_i} \\approx \\frac{\\mathcal{L}(p_i + h_i) - \\mathcal{L}(p_i - h_i)}{2 h_i}$$\n其中 $p_i \\in \\{m,I,c\\}$，$h_i$ 是一个小扰动，选择为 $h_i = \\epsilon \\max(1, |p_i|)$ 且 $\\epsilon=10^{-8}$，以处理不同尺度的参数。伴随梯度 $\\nabla^{\\text{adj}}$ 与有限差分梯度 $\\nabla^{\\text{fd}}$ 之间的一致性通过绝对容差 ($a_{\\text{tol}} = 10^{-9}$) 和相对容差 ($r_{\\text{tol}} = 10^{-6}$) 进行评估。如果对于每个参数分量 $i$，绝对差异或相对差异在指定容差范围内，则验证通过。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the gradient validation for all test cases.\n    \"\"\"\n    g = 9.81\n    epsilon_fd = 1e-8\n    atol = 1e-9\n    rtol = 1e-6\n\n    test_cases = [\n        # Case 1 (general case)\n        {\n            \"params\":       {\"m\": 5.0, \"I\": 0.25, \"c\": 0.20},\n            \"params_tar\":   {\"m\": 5.5, \"I\": 0.28, \"c\": 0.22},\n            \"dt\": 0.002, \"N\": 500, \"b\": 0.05,\n            \"s0\": [0.0, 0.0],\n            \"torque_func\": lambda t: 1.0 * np.sin(2 * np.pi * 1.0 * t),\n        },\n        # Case 2 (gravity-only boundary)\n        {\n            \"params\":       {\"m\": 3.0, \"I\": 0.15, \"c\": 0.18},\n            \"params_tar\":   {\"m\": 3.1, \"I\": 0.14, \"c\": 0.17},\n            \"dt\": 0.00125, \"N\": 800, \"b\": 0.02,\n            \"s0\": [0.30, 0.0],\n            \"torque_func\": lambda t: 0.0,\n        },\n        # Case 3 (higher damping, higher frequency)\n        {\n            \"params\":       {\"m\": 7.0, \"I\": 0.40, \"c\": 0.25},\n            \"params_tar\":   {\"m\": 7.2, \"I\": 0.42, \"c\": 0.26},\n            \"dt\": 0.002, \"N\": 600, \"b\": 0.20,\n            \"s0\": [-0.20, 0.0],\n            \"torque_func\": lambda t: 0.8 * np.sin(2 * np.pi * 3.0 * t),\n        },\n        # Case 4 (small inertia and center-of-mass distance)\n        {\n            \"params\":       {\"m\": 2.0, \"I\": 0.05, \"c\": 0.05},\n            \"params_tar\":   {\"m\": 2.1, \"I\": 0.052, \"c\": 0.045},\n            \"dt\": 0.0015, \"N\": 700, \"b\": 0.03,\n            \"s0\": [0.10, 0.0],\n            \"torque_func\": lambda t: 0.3 * np.sin(2 * np.pi * 1.5 * t),\n        },\n    ]\n\n    def simulate(p, b, dt, N, s0, torque_func):\n        \"\"\"\n        Runs the forward simulation to generate a trajectory.\n        \"\"\"\n        m, I, c = p[\"m\"], p[\"I\"], p[\"c\"]\n        states = np.zeros((N + 1, 2))\n        states[0] = s0\n        \n        I_tot = I + m * c**2\n        \n        for k in range(N):\n            theta_k, theta_dot_k = states[k]\n            t_k = k * dt\n            tau_k = torque_func(t_k)\n            \n            theta_ddot_k = (tau_k - b * theta_dot_k - m * g * c * np.sin(theta_k)) / I_tot\n            \n            theta_dot_k_plus_1 = theta_dot_k + dt * theta_ddot_k\n            theta_k_plus_1 = theta_k + dt * theta_dot_k_plus_1\n            \n            states[k + 1] = [theta_k_plus_1, theta_dot_k_plus_1]\n            \n        return states[:, 0] # Return only angle trajectory\n\n    def compute_loss(theta, theta_tar):\n        \"\"\"\n        Computes the trajectory-matching loss.\n        \"\"\"\n        return 0.5 * np.sum((theta - theta_tar)**2)\n\n    def compute_adjoint_gradient(p, b, dt, N, s0, torque_func, theta_tar):\n        \"\"\"\n        Computes the gradient using the discrete adjoint method.\n        \"\"\"\n        m, I, c = p[\"m\"], p[\"I\"], p[\"c\"]\n\n        # Forward pass to store states and Jacobians\n        states = np.zeros((N + 1, 2))\n        states[0] = s0\n        dfds_list = []\n        dfdp_list = []\n\n        I_tot = I + m * c**2\n        D_inv = 1.0 / I_tot\n\n        for k in range(N):\n            theta_k, theta_dot_k = states[k]\n            t_k = k * dt\n            tau_k = torque_func(t_k)\n\n            # Acceleration and its derivatives\n            theta_ddot_k = (tau_k - b * theta_dot_k - m * g * c * np.sin(theta_k)) * D_inv\n            \n            d_ddot_d_theta = -m * g * c * np.cos(theta_k) * D_inv\n            d_ddot_d_dot_theta = -b * D_inv\n            \n            # State Jacobian df/ds\n            dfds = np.array([\n                [1 + dt**2 * d_ddot_d_theta, dt + dt**2 * d_ddot_d_dot_theta],\n                [dt * d_ddot_d_theta,      1 + dt * d_ddot_d_dot_theta]\n            ])\n            dfds_list.append(dfds)\n\n            # Parameter derivatives of acceleration\n            d_ddot_d_m = -g * c * np.sin(theta_k) * D_inv - theta_ddot_k * c**2 * D_inv\n            d_ddot_d_I = -theta_ddot_k * D_inv\n            d_ddot_d_c = -m * g * np.sin(theta_k) * D_inv - theta_ddot_k * 2 * m * c * D_inv\n            \n            # Parameter Jacobian df/dp\n            dfdp = np.outer(np.array([dt**2, dt]), np.array([d_ddot_d_m, d_ddot_d_I, d_ddot_d_c]))\n            dfdp_list.append(dfdp)\n\n            # State update\n            theta_dot_k_plus_1 = theta_dot_k + dt * theta_ddot_k\n            theta_k_plus_1 = theta_k + dt * theta_dot_k_plus_1\n            states[k+1] = [theta_k_plus_1, theta_dot_k_plus_1]\n\n        # Backward pass (Adjoint)\n        lambdas = np.zeros((N + 1, 2))\n        lambdas[N, 0] = states[N, 0] - theta_tar[N] \n        \n        grad = np.zeros(3)\n\n        for k in range(N - 1, -1, -1):\n            theta_k = states[k, 0]\n            \n            # Accumulate parameter gradient\n            grad += dfdp_list[k].T @ lambdas[k + 1]\n            \n            # Update adjoint state\n            d_loss_d_sk = np.array([theta_k - theta_tar[k], 0.0])\n            lambdas[k] = d_loss_d_sk + dfds_list[k].T @ lambdas[k + 1]\n            \n        return grad\n\n    def compute_fd_gradient(p, b, dt, N, s0, torque_func, theta_tar):\n        \"\"\"\n        Computes the gradient using central finite differences.\n        \"\"\"\n        grad = np.zeros(3)\n        param_keys = [\"m\", \"I\", \"c\"]\n        for i, key in enumerate(param_keys):\n            p_val = p[key]\n            h = epsilon_fd * max(1.0, abs(p_val))\n            \n            p_plus = p.copy()\n            p_plus[key] = p_val + h\n            theta_plus = simulate(p_plus, b, dt, N, s0, torque_func)\n            loss_plus = compute_loss(theta_plus, theta_tar)\n            \n            p_minus = p.copy()\n            p_minus[key] = p_val - h\n            theta_minus = simulate(p_minus, b, dt, N, s0, torque_func)\n            loss_minus = compute_loss(theta_minus, theta_tar)\n            \n            grad[i] = (loss_plus - loss_minus) / (2 * h)\n            \n        return grad\n\n    def validate_gradients(grad_adj, grad_fd):\n        \"\"\"\n        Compares adjoint and finite difference gradients.\n        \"\"\"\n        for i in range(3):\n            abs_err = abs(grad_adj[i] - grad_fd[i])\n            \n            if abs_err = atol:\n                continue\n\n            denom = max(abs(grad_fd[i]), 1e-12)\n            rel_err = abs_err / denom\n            \n            if rel_err > rtol:\n                return False\n        return True\n\n    results = []\n    for case in test_cases:\n        theta_tar = simulate(\n            case[\"params_tar\"], case[\"b\"], case[\"dt\"], case[\"N\"], \n            case[\"s0\"], case[\"torque_func\"]\n        )\n        \n        grad_adj = compute_adjoint_gradient(\n            case[\"params\"], case[\"b\"], case[\"dt\"], case[\"N\"], \n            case[\"s0\"], case[\"torque_func\"], theta_tar\n        )\n        \n        grad_fd = compute_fd_gradient(\n            case[\"params\"], case[\"b\"], case[\"dt\"], case[\"N\"], \n            case[\"s0\"], case[\"torque_func\"], theta_tar\n        )\n        \n        is_passed = validate_gradients(grad_adj, grad_fd)\n        results.append(is_passed)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}