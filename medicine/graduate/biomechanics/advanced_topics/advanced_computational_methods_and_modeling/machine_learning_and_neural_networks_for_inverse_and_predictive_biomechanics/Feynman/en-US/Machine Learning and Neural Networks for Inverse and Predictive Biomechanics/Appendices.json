{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of modern machine learning is gradient-based optimization. To apply these powerful techniques to physics-based biomechanical models, we first need an efficient way to compute the gradient of a performance metric, such as a trajectory tracking error, with respect to model parameters. This exercise guides you through implementing the adjoint method—the computational workhorse behind backpropagation—to find the exact gradients of a simple musculoskeletal simulator, a foundational skill for building differentiable physics models.",
            "id": "4186295",
            "problem": "Implement a self-contained program that computes adjoint-based gradients of a trajectory-matching loss with respect to inertial parameters of a single planar segment and validates these gradients against central finite differences. The core biomechanical simulator models a single-degree-of-freedom (DOF) planar segment (for example, a shank or forearm) rotating about a fixed pivot under external joint torque. The fundamental base is Newton’s second law for rotational motion. The state consists of the joint angle and angular velocity. The inertial parameters to be estimated are the mass, the moment of inertia about the segment center of mass, and the center-of-mass distance from the joint.\n\nFundamental laws and core definitions:\n- Newton’s second law for rotation gives $$I_{\\text{tot}} \\, \\ddot{\\theta} + b \\, \\dot{\\theta} + m g c \\sin(\\theta) = \\tau,$$ where $I_{\\text{tot}}$ is the total moment of inertia about the joint given by $$I_{\\text{tot}} = I + m c^2,$$ $m$ is mass, $I$ is moment of inertia about the center of mass, $c$ is the center-of-mass distance from the joint, $b$ is viscous damping about the joint, $g$ is gravitational acceleration, $\\theta$ is the joint angle, $\\dot{\\theta}$ is the angular velocity, $\\ddot{\\theta}$ is the angular acceleration, and $\\tau$ is the external joint torque. The gravitational constant $g$ is $9.81$ $\\text{m}/\\text{s}^2$.\n\nDiscretization and simulation:\n- Use semi-implicit Euler integration with time step $dt$ seconds. For discrete time index $k \\in \\{0,1,\\dots,N-1\\}$, define\n$$\\ddot{\\theta}_k = \\frac{\\tau_k - b \\, \\dot{\\theta}_k - m g c \\sin(\\theta_k)}{I + m c^2},$$\n$$\\dot{\\theta}_{k+1} = \\dot{\\theta}_k + dt \\, \\ddot{\\theta}_k,$$\n$$\\theta_{k+1} = \\theta_k + dt \\, \\dot{\\theta}_{k+1}.$$\nThe initial state is $(\\theta_0,\\dot{\\theta}_0)$.\n\nLoss and gradients:\n- Define the trajectory-matching loss over angles as\n$$\\mathcal{L}(m,I,c) = \\frac{1}{2} \\sum_{k=0}^{N} w_k \\left(\\theta_k - \\theta^{\\text{tar}}_k\\right)^2,$$\nwhere $\\theta^{\\text{tar}}_k$ is a target trajectory generated by the same simulator under a different set of inertial parameters, and $w_k$ are nonnegative weights (use $w_k = 1$ for all $k$). Angles must be in radians.\n- Implement the adjoint-based gradient computation for the parameter vector $(m,I,c)$ using the discrete-time adjoint method (which is equivalent to Backpropagation Through Time (BPTT)):\n    - Let $s_k = [\\theta_k, \\dot{\\theta}_k]^\\top$ be the state and $s_{k+1} = f_k(s_k; m,I,c)$ be the discrete update. Let $\\lambda_k = \\partial \\mathcal{L}/\\partial s_k$ be the adjoint.\n    - The per-step loss contribution is $\\ell_k = \\frac{1}{2} w_k (\\theta_k - \\theta^{\\text{tar}}_k)^2$, so $\\partial \\ell_k / \\partial s_k = [w_k (\\theta_k - \\theta^{\\text{tar}}_k), \\, 0]^\\top$.\n    - The adjoint recursion is\n    $$\\lambda_N = \\frac{\\partial \\ell_N}{\\partial s_N}, \\quad \\lambda_k = \\frac{\\partial \\ell_k}{\\partial s_k} + \\left(\\frac{\\partial f_k}{\\partial s_k}\\right)^\\top \\lambda_{k+1}, \\quad \\text{for } k = N-1,\\dots,0.$$\n    - The gradient with respect to parameters is\n    $$\\nabla_{(m,I,c)} \\mathcal{L} = \\sum_{k=0}^{N-1} \\left(\\frac{\\partial f_k}{\\partial (m,I,c)}\\right)^\\top \\lambda_{k+1}.$$\n- Derive $\\partial f_k/\\partial s_k$ and $\\partial f_k/\\partial (m,I,c)$ from the discrete update, and use these to implement the adjoint recursion and gradient accumulation.\n\nUnits:\n- Mass $m$ in kilograms ($\\text{kg}$).\n- Center-of-mass distance $c$ in meters ($\\text{m}$).\n- Moment of inertia $I$ in kilogram-meter squared ($\\text{kg}\\cdot\\text{m}^2$).\n- Damping $b$ in Newton-meter-second ($\\text{N}\\cdot\\text{m}\\cdot\\text{s}$).\n- Torque $\\tau$ in Newton-meter ($\\text{N}\\cdot\\text{m}$).\n- Time $dt$ in seconds ($\\text{s}$).\n- Angles $\\theta$ in radians.\n\nValidation against finite differences:\n- Implement central finite differences for $(m,I,c)$ with a small step $h_i$ per parameter, $h_i = \\epsilon \\max(1, |p_i|)$ where $p_i \\in \\{m,I,c\\}$ and $\\epsilon$ is a small positive decimal (for example, $10^{-8}$). Compute\n$$\\frac{\\partial \\mathcal{L}}{\\partial p_i} \\approx \\frac{\\mathcal{L}(p_i + h_i) - \\mathcal{L}(p_i - h_i)}{2 h_i}.$$\n- For each test case, compute the adjoint-based gradient and compare it to the finite-difference gradient for each parameter using both an absolute tolerance $a_{\\text{tol}}$ and a relative tolerance $r_{\\text{tol}}$: a parameter passes if\n$$|\\nabla^{\\text{adj}}_i - \\nabla^{\\text{fd}}_i| \\le a_{\\text{tol}} \\quad \\text{or} \\quad \\frac{|\\nabla^{\\text{adj}}_i - \\nabla^{\\text{fd}}_i|}{\\max(|\\nabla^{\\text{fd}}_i|, 10^{-12})} \\le r_{\\text{tol}}.$$\nUse $a_{\\text{tol}} = 10^{-9}$ and $r_{\\text{tol}} = 10^{-6}$. A test case passes if all three parameters pass.\n\nTest suite:\n- Gravity is set to $g = 9.81$ $\\text{m}/\\text{s}^2$ in all cases. Damping $b$ is specified per case. Use the same external torque for both the predicted and the target trajectory of each case.\n- Case $1$ (general case):\n    - Parameters: $m = 5.0$ $\\text{kg}$, $I = 0.25$ $\\text{kg}\\cdot\\text{m}^2$, $c = 0.20$ $\\text{m}$.\n    - Target parameters: $m^{\\text{tar}} = 5.5$ $\\text{kg}$, $I^{\\text{tar}} = 0.28$ $\\text{kg}\\cdot\\text{m}^2$, $c^{\\text{tar}} = 0.22$ $\\text{m}$.\n    - Time step: $dt = 0.002$ $\\text{s}$, steps: $N = 500$.\n    - Damping: $b = 0.05$ $\\text{N}\\cdot\\text{m}\\cdot\\text{s}$.\n    - Initial state: $\\theta_0 = 0.0$ $\\text{rad}$, $\\dot{\\theta}_0 = 0.0$ $\\text{rad}/\\text{s}$.\n    - Torque: $\\tau_k = A \\sin(2 \\pi f t_k)$, with $A = 1.0$ $\\text{N}\\cdot\\text{m}$, $f = 1.0$ $\\text{Hz}$, $t_k = k \\, dt$.\n- Case $2$ (gravity-only boundary):\n    - Parameters: $m = 3.0$ $\\text{kg}$, $I = 0.15$ $\\text{kg}\\cdot\\text{m}^2$, $c = 0.18$ $\\text{m}$.\n    - Target parameters: $m^{\\text{tar}} = 3.1$ $\\text{kg}$, $I^{\\text{tar}} = 0.14$ $\\text{kg}\\cdot\\text{m}^2$, $c^{\\text{tar}} = 0.17$ $\\text{m}$.\n    - Time step: $dt = 0.00125$ $\\text{s}$, steps: $N = 800$.\n    - Damping: $b = 0.02$ $\\text{N}\\cdot\\text{m}\\cdot\\text{s}$.\n    - Initial state: $\\theta_0 = 0.30$ $\\text{rad}$, $\\dot{\\theta}_0 = 0.0$ $\\text{rad}/\\text{s}$.\n    - Torque: $\\tau_k = 0$ for all $k$.\n- Case $3$ (higher damping, higher frequency):\n    - Parameters: $m = 7.0$ $\\text{kg}$, $I = 0.40$ $\\text{kg}\\cdot\\text{m}^2$, $c = 0.25$ $\\text{m}$.\n    - Target parameters: $m^{\\text{tar}} = 7.2$ $\\text{kg}$, $I^{\\text{tar}} = 0.42$ $\\text{kg}\\cdot\\text{m}^2$, $c^{\\text{tar}} = 0.26$ $\\text{m}$.\n    - Time step: $dt = 0.002$ $\\text{s}$, steps: $N = 600$.\n    - Damping: $b = 0.20$ $\\text{N}\\cdot\\text{m}\\cdot\\text{s}$.\n    - Initial state: $\\theta_0 = -0.20$ $\\text{rad}$, $\\dot{\\theta}_0 = 0.0$ $\\text{rad}/\\text{s}$.\n    - Torque: $\\tau_k = A \\sin(2 \\pi f t_k)$, with $A = 0.8$ $\\text{N}\\cdot\\text{m}$, $f = 3.0$ $\\text{Hz}$, $t_k = k \\, dt$.\n- Case $4$ (small inertia and center-of-mass distance):\n    - Parameters: $m = 2.0$ $\\text{kg}$, $I = 0.05$ $\\text{kg}\\cdot\\text{m}^2$, $c = 0.05$ $\\text{m}$.\n    - Target parameters: $m^{\\text{tar}} = 2.1$ $\\text{kg}$, $I^{\\text{tar}} = 0.052$ $\\text{kg}\\cdot\\text{m}^2$, $c^{\\text{tar}} = 0.045$ $\\text{m}$.\n    - Time step: $dt = 0.0015$ $\\text{s}$, steps: $N = 700$.\n    - Damping: $b = 0.03$ $\\text{N}\\cdot\\text{m}\\cdot\\text{s}$.\n    - Initial state: $\\theta_0 = 0.10$ $\\text{rad}$, $\\dot{\\theta}_0 = 0.0$ $\\text{rad}/\\text{s}$.\n    - Torque: $\\tau_k = A \\sin(2 \\pi f t_k)$, with $A = 0.3$ $\\text{N}\\cdot\\text{m}$, $f = 1.5$ $\\text{Hz}$, $t_k = k \\, dt$.\n\nProgram requirements:\n- Implement the discrete simulator, the adjoint-based gradient, and central finite differences as specified above.\n- For each case, compute whether all three parameter gradients pass the tolerances. Aggregate these pass booleans.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[\\text{result1},\\text{result2},\\text{result3},\\text{result4}]$), where each result is a boolean indicating pass or fail for the corresponding test case.",
            "solution": "The posed problem requires the implementation and validation of an adjoint-based method for computing gradients of a trajectory-matching loss function with respect to the inertial parameters of a planar rigid body. The validation is to be performed against a central finite difference approximation. This problem is scientifically well-grounded in classical mechanics and numerical optimization, and is specified with sufficient detail to permit a unique and verifiable solution.\n\nThe process involves three main stages:\n1.  **Forward Simulation**: A numerical simulator, based on the provided equations of motion and integration scheme, is used to generate state trajectories.\n2.  **Adjoint Gradient Computation**: An adjoint-based method, which is computationally equivalent to Backpropagation Through Time (BPTT), is implemented to compute the exact gradients of the loss function with respect to the model parameters. This requires deriving the analytical Jacobians of the discrete-time state transition function.\n3.  **Gradient Validation**: The analytically derived adjoint gradients are compared against a numerical approximation using the central finite difference method to ensure correctness.\n\nHerein, we will formalize the necessary derivations and algorithmic steps.\n\n**1. Forward Dynamics and Discretization**\n\nThe dynamics of the single-degree-of-freedom planar segment are governed by Newton's second law for rotation:\n$$I_{\\text{tot}} \\, \\ddot{\\theta} + b \\, \\dot{\\theta} + m g c \\sin(\\theta) = \\tau$$\nwhere $I_{\\text{tot}} = I + m c^2$ is the total moment of inertia about the pivot, derived from the parallel axis theorem. The parameters are mass $m$, moment of inertia about the center of mass $I$, and center-of-mass distance from the joint $c$. Constants include viscous damping $b$ and gravitational acceleration $g = 9.81 \\, \\text{m/s}^2$. The state is defined by the angle $\\theta$ and angular velocity $\\dot{\\theta}$.\n\nThe angular acceleration at a discrete time step $k$, $\\ddot{\\theta}_k$, can be expressed as:\n$$\\ddot{\\theta}_k = f_{\\text{accel}}(\\theta_k, \\dot{\\theta}_k; m, I, c) = \\frac{\\tau_k - b \\dot{\\theta}_k - m g c \\sin(\\theta_k)}{I + m c^2}$$\nThe problem specifies a semi-implicit Euler integration scheme with a time step $dt$:\n\\begin{align*} \\dot{\\theta}_{k+1} &= \\dot{\\theta}_k + dt \\, \\ddot{\\theta}_k \\\\ \\theta_{k+1} &= \\theta_k + dt \\, \\dot{\\theta}_{k+1} \\end{align*}\nBy substituting the expression for $\\dot{\\theta}_{k+1}$ into the update for $\\theta_{k+1}$, we can write the full state update for $s_k = [\\theta_k, \\dot{\\theta}_k]^\\top$ as $s_{k+1} = f_k(s_k; m, I, c)$:\n\\begin{align*} \\theta_{k+1} &= \\theta_k + dt (\\dot{\\theta}_k + dt \\, \\ddot{\\theta}_k) = \\theta_k + dt \\dot{\\theta}_k + dt^2 \\ddot{\\theta}_k \\\\ \\dot{\\theta}_{k+1} &= \\dot{\\theta}_k + dt \\, \\ddot{\\theta}_k \\end{align*}\nThis formulation explicitly relates the state at step $k+1$ to the state at step $k$.\n\n**2. Adjoint-Based Gradient Computation**\n\nThe goal is to compute the gradient of the loss function $\\mathcal{L}(m,I,c) = \\frac{1}{2} \\sum_{k=0}^{N} (\\theta_k - \\theta^{\\text{tar}}_k)^2$ with respect to the parameters $p = (m, I, c)$. The discrete adjoint method provides an efficient way to do this. We define an adjoint state vector $\\lambda_k = \\partial \\mathcal{L} / \\partial s_k$ and propagate it backward in time.\n\nThe backward recursion is initialized at the final time step $N$:\n$$\\lambda_N = \\frac{\\partial \\mathcal{L}}{\\partial s_N} = \\begin{bmatrix} \\partial \\mathcal{L} / \\partial \\theta_N \\\\ \\partial \\mathcal{L} / \\partial \\dot{\\theta}_N \\end{bmatrix} = \\begin{bmatrix} (\\theta_N - \\theta^{\\text{tar}}_N) \\\\ 0 \\end{bmatrix}$$\nFor $k = N-1, \\dots, 0$, the adjoint state is updated according to the chain rule:\n$$\\lambda_k = \\frac{\\partial \\mathcal{L}}{\\partial s_k} = \\frac{\\partial \\ell_k}{\\partial s_k} + \\left(\\frac{\\partial s_{k+1}}{\\partial s_k}\\right)^\\top \\frac{\\partial \\mathcal{L}}{\\partial s_{k+1}} = \\frac{\\partial \\ell_k}{\\partial s_k} + \\left(\\frac{\\partial f_k}{\\partial s_k}\\right)^\\top \\lambda_{k+1}$$\nwhere $\\frac{\\partial \\ell_k}{\\partial s_k} = [(\\theta_k - \\theta^{\\text{tar}}_k), \\, 0]^\\top$.\n\nThe total gradient with respect to the parameters $p$ is accumulated over all time steps:\n$$\\nabla_p \\mathcal{L} = \\sum_{k=0}^{N-1} \\left(\\frac{\\partial s_{k+1}}{\\partial p}\\right)^\\top \\frac{\\partial \\mathcal{L}}{\\partial s_{k+1}} = \\sum_{k=0}^{N-1} \\left(\\frac{\\partial f_k}{\\partial p}\\right)^\\top \\lambda_{k+1}$$\n\n**3. Derivation of State and Parameter Jacobians**\n\nTo implement the adjoint method, we must derive the Jacobians $\\frac{\\partial f_k}{\\partial s_k}$ and $\\frac{\\partial f_k}{\\partial p}$. Let $D = I + m c^2$.\n\n**State Jacobian $\\frac{\\partial f_k}{\\partial s_k}$:**\nFirst, we find the partial derivatives of $\\ddot{\\theta}_k$ with respect to the state variables $\\theta_k$ and $\\dot{\\theta}_k$:\n$$\\frac{\\partial \\ddot{\\theta}_k}{\\partial \\theta_k} = -\\frac{m g c \\cos(\\theta_k)}{D}, \\quad \\frac{\\partial \\ddot{\\theta}_k}{\\partial \\dot{\\theta}_k} = -\\frac{b}{D}$$\nUsing these, we form the $2 \\times 2$ Jacobian matrix of the state transition function $f_k$:\n$$\\frac{\\partial f_k}{\\partial s_k} = \\begin{bmatrix} \\frac{\\partial \\theta_{k+1}}{\\partial \\theta_k} & \\frac{\\partial \\theta_{k+1}}{\\partial \\dot{\\theta}_k} \\\\ \\frac{\\partial \\dot{\\theta}_{k+1}}{\\partial \\theta_k} & \\frac{\\partial \\dot{\\theta}_{k+1}}{\\partial \\dot{\\theta}_k} \\end{bmatrix} = \\begin{bmatrix} 1 + dt^2 \\frac{\\partial \\ddot{\\theta}_k}{\\partial \\theta_k} & dt + dt^2 \\frac{\\partial \\ddot{\\theta}_k}{\\partial \\dot{\\theta}_k} \\\\ dt \\frac{\\partial \\ddot{\\theta}_k}{\\partial \\theta_k} & 1 + dt \\frac{\\partial \\ddot{\\theta}_k}{\\partial \\dot{\\theta}_k} \\end{bmatrix}$$\n\n**Parameter Jacobian $\\frac{\\partial f_k}{\\partial p}$:**\nThe partial derivatives of the state update with respect to a parameter $p_i \\in \\{m, I, c\\}$ are:\n$$\\frac{\\partial \\theta_{k+1}}{\\partial p_i} = dt^2 \\frac{\\partial \\ddot{\\theta}_k}{\\partial p_i}, \\quad \\frac{\\partial \\dot{\\theta}_{k+1}}{\\partial p_i} = dt \\frac{\\partial \\ddot{\\theta}_k}{\\partial p_i}$$\nThis can be written compactly as $\\frac{\\partial f_k}{\\partial p} = [dt^2, dt]^\\top \\frac{\\partial \\ddot{\\theta}_k}{\\partial p}$. We need the partials of $\\ddot{\\theta}_k$ w.r.t $m, I, c$:\n\n-   With respect to mass $m$:\n    $$\\frac{\\partial \\ddot{\\theta}_k}{\\partial m} = \\frac{\\partial}{\\partial m}\\left(\\frac{\\tau_k - b \\dot{\\theta}_k - m g c \\sin(\\theta_k)}{I + m c^2}\\right) = \\frac{-g c \\sin(\\theta_k) \\cdot D - (\\tau_k - b \\dot{\\theta}_k - m g c \\sin(\\theta_k)) \\cdot c^2}{D^2} = -\\frac{g c \\sin(\\theta_k)}{D} - \\ddot{\\theta}_k \\frac{c^2}{D}$$\n\n-   With respect to inertia $I$:\n    $$\\frac{\\partial \\ddot{\\theta}_k}{\\partial I} = \\frac{\\partial}{\\partial I}\\left(\\frac{\\tau_k - b \\dot{\\theta}_k - m g c \\sin(\\theta_k)}{I + m c^2}\\right) = \\frac{-(\\tau_k - b \\dot{\\theta}_k - m g c \\sin(\\theta_k))}{D^2} = -\\frac{\\ddot{\\theta}_k}{D}$$\n\n-   With respect to center-of-mass distance $c$:\n    $$\\frac{\\partial \\ddot{\\theta}_k}{\\partial c} = \\frac{\\partial}{\\partial c}\\left(\\frac{\\tau_k - b \\dot{\\theta}_k - m g c \\sin(\\theta_k)}{I + m c^2}\\right) = \\frac{-m g \\sin(\\theta_k) \\cdot D - (\\tau_k - b \\dot{\\theta}_k - m g c \\sin(\\theta_k)) \\cdot 2mc}{D^2} = -\\frac{m g \\sin(\\theta_k)}{D} - \\ddot{\\theta}_k \\frac{2mc}{D}$$\n\nThese Jacobians are computed at each step of a forward pass and stored. Then, a backward pass computes the adjoint states and accumulates the gradients.\n\n**4. Gradient Validation via Finite Differences**\n\nTo confirm the correctness of the analytical gradients, we compare them to numerical approximations from the central finite difference method:\n$$\\frac{\\partial \\mathcal{L}}{\\partial p_i} \\approx \\frac{\\mathcal{L}(p_i + h_i) - \\mathcal{L}(p_i - h_i)}{2 h_i}$$\nwhere $p_i \\in \\{m,I,c\\}$ and $h_i$ is a small perturbation, chosen as $h_i = \\epsilon \\max(1, |p_i|)$ with $\\epsilon=10^{-8}$ to handle parameters of varying scales. The agreement between the adjoint gradient $\\nabla^{\\text{adj}}$ and the finite difference gradient $\\nabla^{\\text{fd}}$ is assessed using both absolute ($a_{\\text{tol}} = 10^{-9}$) and relative ($r_{\\text{tol}} = 10^{-6}$) tolerances. A validation passes if for each parameter component $i$, either the absolute difference or the relative difference is within the specified tolerance.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the gradient validation for all test cases.\n    \"\"\"\n    g = 9.81\n    epsilon_fd = 1e-8\n    atol = 1e-9\n    rtol = 1e-6\n\n    test_cases = [\n        # Case 1 (general case)\n        {\n            \"params\":       {\"m\": 5.0, \"I\": 0.25, \"c\": 0.20},\n            \"params_tar\":   {\"m\": 5.5, \"I\": 0.28, \"c\": 0.22},\n            \"dt\": 0.002, \"N\": 500, \"b\": 0.05,\n            \"s0\": [0.0, 0.0],\n            \"torque_func\": lambda t: 1.0 * np.sin(2 * np.pi * 1.0 * t),\n        },\n        # Case 2 (gravity-only boundary)\n        {\n            \"params\":       {\"m\": 3.0, \"I\": 0.15, \"c\": 0.18},\n            \"params_tar\":   {\"m\": 3.1, \"I\": 0.14, \"c\": 0.17},\n            \"dt\": 0.00125, \"N\": 800, \"b\": 0.02,\n            \"s0\": [0.30, 0.0],\n            \"torque_func\": lambda t: 0.0,\n        },\n        # Case 3 (higher damping, higher frequency)\n        {\n            \"params\":       {\"m\": 7.0, \"I\": 0.40, \"c\": 0.25},\n            \"params_tar\":   {\"m\": 7.2, \"I\": 0.42, \"c\": 0.26},\n            \"dt\": 0.002, \"N\": 600, \"b\": 0.20,\n            \"s0\": [-0.20, 0.0],\n            \"torque_func\": lambda t: 0.8 * np.sin(2 * np.pi * 3.0 * t),\n        },\n        # Case 4 (small inertia and center-of-mass distance)\n        {\n            \"params\":       {\"m\": 2.0, \"I\": 0.05, \"c\": 0.05},\n            \"params_tar\":   {\"m\": 2.1, \"I\": 0.052, \"c\": 0.045},\n            \"dt\": 0.0015, \"N\": 700, \"b\": 0.03,\n            \"s0\": [0.10, 0.0],\n            \"torque_func\": lambda t: 0.3 * np.sin(2 * np.pi * 1.5 * t),\n        },\n    ]\n\n    def simulate(p, b, dt, N, s0, torque_func):\n        \"\"\"\n        Runs the forward simulation to generate a trajectory.\n        \"\"\"\n        m, I, c = p[\"m\"], p[\"I\"], p[\"c\"]\n        states = np.zeros((N + 1, 2))\n        states[0] = s0\n        \n        I_tot = I + m * c**2\n        \n        for k in range(N):\n            theta_k, theta_dot_k = states[k]\n            t_k = k * dt\n            tau_k = torque_func(t_k)\n            \n            theta_ddot_k = (tau_k - b * theta_dot_k - m * g * c * np.sin(theta_k)) / I_tot\n            \n            theta_dot_k_plus_1 = theta_dot_k + dt * theta_ddot_k\n            theta_k_plus_1 = theta_k + dt * theta_dot_k_plus_1\n            \n            states[k + 1] = [theta_k_plus_1, theta_dot_k_plus_1]\n            \n        return states[:, 0] # Return only angle trajectory\n\n    def compute_loss(theta, theta_tar):\n        \"\"\"\n        Computes the trajectory-matching loss.\n        \"\"\"\n        return 0.5 * np.sum((theta - theta_tar)**2)\n\n    def compute_adjoint_gradient(p, b, dt, N, s0, torque_func, theta_tar):\n        \"\"\"\n        Computes the gradient using the discrete adjoint method.\n        \"\"\"\n        m, I, c = p[\"m\"], p[\"I\"], p[\"c\"]\n\n        # Forward pass to store states and Jacobians\n        states = np.zeros((N + 1, 2))\n        states[0] = s0\n        dfds_list = []\n        dfdp_list = []\n\n        I_tot = I + m * c**2\n        D_inv = 1.0 / I_tot\n\n        for k in range(N):\n            theta_k, theta_dot_k = states[k]\n            t_k = k * dt\n            tau_k = torque_func(t_k)\n\n            # Acceleration and its derivatives\n            theta_ddot_k = (tau_k - b * theta_dot_k - m * g * c * np.sin(theta_k)) * D_inv\n            \n            d_ddot_d_theta = -m * g * c * np.cos(theta_k) * D_inv\n            d_ddot_d_dot_theta = -b * D_inv\n            \n            # State Jacobian df/ds\n            dfds = np.array([\n                [1 + dt**2 * d_ddot_d_theta, dt + dt**2 * d_ddot_d_dot_theta],\n                [dt * d_ddot_d_theta,      1 + dt * d_ddot_d_dot_theta]\n            ])\n            dfds_list.append(dfds)\n\n            # Parameter derivatives of acceleration\n            d_ddot_d_m = -g * c * np.sin(theta_k) * D_inv - theta_ddot_k * c**2 * D_inv\n            d_ddot_d_I = -theta_ddot_k * D_inv\n            d_ddot_d_c = -m * g * np.sin(theta_k) * D_inv - theta_ddot_k * 2 * m * c * D_inv\n            \n            # Parameter Jacobian df/dp\n            dfdp = np.outer(np.array([dt**2, dt]), np.array([d_ddot_d_m, d_ddot_d_I, d_ddot_d_c]))\n            dfdp_list.append(dfdp)\n\n            # State update\n            theta_dot_k_plus_1 = theta_dot_k + dt * theta_ddot_k\n            theta_k_plus_1 = theta_k + dt * theta_dot_k_plus_1\n            states[k+1] = [theta_k_plus_1, theta_dot_k_plus_1]\n\n        # Backward pass (Adjoint)\n        lambdas = np.zeros((N + 1, 2))\n        lambdas[N, 0] = states[N, 0] - theta_tar[N] \n        \n        grad = np.zeros(3)\n\n        for k in range(N - 1, -1, -1):\n            theta_k = states[k, 0]\n            \n            # Accumulate parameter gradient\n            grad += dfdp_list[k].T @ lambdas[k + 1]\n            \n            # Update adjoint state\n            d_loss_d_sk = np.array([theta_k - theta_tar[k], 0.0])\n            lambdas[k] = d_loss_d_sk + dfds_list[k].T @ lambdas[k + 1]\n            \n        return grad\n\n    def compute_fd_gradient(p, b, dt, N, s0, torque_func, theta_tar):\n        \"\"\"\n        Computes the gradient using central finite differences.\n        \"\"\"\n        grad = np.zeros(3)\n        param_keys = [\"m\", \"I\", \"c\"]\n        for i, key in enumerate(param_keys):\n            p_val = p[key]\n            h = epsilon_fd * max(1.0, abs(p_val))\n            \n            p_plus = p.copy()\n            p_plus[key] = p_val + h\n            theta_plus = simulate(p_plus, b, dt, N, s0, torque_func)\n            loss_plus = compute_loss(theta_plus, theta_tar)\n            \n            p_minus = p.copy()\n            p_minus[key] = p_val - h\n            theta_minus = simulate(p_minus, b, dt, N, s0, torque_func)\n            loss_minus = compute_loss(theta_minus, theta_tar)\n            \n            grad[i] = (loss_plus - loss_minus) / (2 * h)\n            \n        return grad\n\n    def validate_gradients(grad_adj, grad_fd):\n        \"\"\"\n        Compares adjoint and finite difference gradients.\n        \"\"\"\n        for i in range(3):\n            abs_err = abs(grad_adj[i] - grad_fd[i])\n            \n            if abs_err <= atol:\n                continue\n\n            denom = max(abs(grad_fd[i]), 1e-12)\n            rel_err = abs_err / denom\n            \n            if rel_err > rtol:\n                return False\n        return True\n\n    results = []\n    for case in test_cases:\n        theta_tar = simulate(\n            case[\"params_tar\"], case[\"b\"], case[\"dt\"], case[\"N\"], \n            case[\"s0\"], case[\"torque_func\"]\n        )\n        \n        grad_adj = compute_adjoint_gradient(\n            case[\"params\"], case[\"b\"], case[\"dt\"], case[\"N\"], \n            case[\"s0\"], case[\"torque_func\"], theta_tar\n        )\n        \n        grad_fd = compute_fd_gradient(\n            case[\"params\"], case[\"b\"], case[\"dt\"], case[\"N\"], \n            case[\"s0\"], case[\"torque_func\"], theta_tar\n        )\n        \n        is_passed = validate_gradients(grad_adj, grad_fd)\n        results.append(is_passed)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Before attempting to estimate parameters for a complex biomechanical model, it is crucial to ask: are these parameters even theoretically identifiable from the data we can measure? This practice explores the concept of parameter estimability using the Fisher Information Matrix ($I$), which leverages the model's Jacobian matrix ($J$) to determine which parameters have a unique and distinguishable effect on sensor outputs. By analyzing the rank of the FIM, you can diagnose issues like parameter redundancy and inform better experimental design, a critical step that builds on the sensitivity information explored in the previous practice .",
            "id": "4186299",
            "problem": "Consider a predictive biomechanics setting in which a neural network surrogate encodes a differentiable mapping from a parameter vector $\\theta \\in \\mathbb{R}^p$ to a vector of sensor outputs $y(\\theta) \\in \\mathbb{R}^m$. The physical outputs include joint angles and ground reaction forces generated by a forward musculoskeletal dynamics model rooted in Newton's laws (for example, Newton's second law $F = m a$ for segmental dynamics and torque balance equations for joints). Assume additive sensor noise modeled as zero-mean multivariate Gaussian with covariance matrix $\\Sigma \\in \\mathbb{R}^{m \\times m}$, that is $y_{\\text{measured}} = y(\\theta) + \\varepsilon$ with $\\varepsilon \\sim \\mathcal{N}(0, \\Sigma)$. Angles are measured in radians; forces are measured in newtons. Express any angle quantities in radians. No angles appear in the final output; the final outputs are indices of estimable parameters.\n\nYou are provided with local linearizations of the neural network mapping at a nominal parameter $\\theta_0$, represented by the Jacobian matrix $J = \\left.\\frac{\\partial y}{\\partial \\theta}\\right|_{\\theta_0} \\in \\mathbb{R}^{m \\times p}$ for three distinct sensor configurations (test cases). For each test case, compute the Fisher Information Matrix (FIM), defined under the Gaussian noise model by\n$$\nI(\\theta_0) = J^\\top \\Sigma^{-1} J,\n$$\nand use it to determine a maximal subset of parameter indices that are estimable from the given sensors. A parameter subset is deemed estimable if its corresponding columns in the Jacobian (under the metric induced by $\\Sigma^{-1}$) are linearly independent, meaning the Fisher Information Matrix has full rank with respect to those parameters. Operationally, to select a maximal estimable subset, perform a rank-revealing column-pivoted factorization on the whitened Jacobian $\\Sigma^{-1/2} J$ and retain the pivot indices up to the numerical rank. Sort the selected parameter indices in ascending order before returning them.\n\nUse the following tolerance convention for numerical rank determination. Let $R$ be the upper-triangular factor from a pivoted $\\mathrm{QR}$ factorization of $\\Sigma^{-1/2}J$. Define\n$$\n\\tau = \\max(m,p) \\cdot \\epsilon \\cdot \\max\\left( |R_{11}|, \\| \\Sigma^{-1/2}J \\|_2 \\right),\n$$\nwhere $\\epsilon$ is machine precision for double-precision floating-point arithmetic. The numerical rank is the number of diagonal entries of $R$ whose absolute values exceed $\\tau$.\n\nYour program should process the three test cases below. In each case, $m = 6$ outputs comprise three joint angles (hip, knee, ankle) in radians and three components of ground reaction force (in newtons), and $p = 4$ parameters correspond to $(k_{\\text{knee}}, b_{\\text{knee}}, F_{\\max,\\text{gas}}, L_{\\text{slack}})$.\n\nTest case $1$ (happy path, all parameters estimable):\n$$\nJ_1 =\n\\begin{bmatrix}\n0.01 & 0.02 & 0.00 & 0.00 \\\\\n-0.20 & -0.40 & 0.05 & -0.01 \\\\\n0.00 & -0.05 & -0.30 & 0.10 \\\\\n0.00 & 0.00 & 5.00 & -1.00 \\\\\n1.00 & 2.00 & 10.00 & -2.00 \\\\\n0.50 & 0.80 & 15.00 & -3.00 \\\\\n\\end{bmatrix},\n\\quad\n\\Sigma_1 = \\mathrm{diag}\\left(10^{-4}, 3\\cdot 10^{-4}, 2\\cdot 10^{-4}, 25^2, 40^2, 50^2\\right).\n$$\n\nTest case $2$ (boundary case: exact collinearity between two parameter directions):\nLet the first two columns be exactly proportional by a factor of $2$.\n$$\nJ_2 =\n\\begin{bmatrix}\n0.00 & 0.00 & 0.10 & 0.00 \\\\\n-0.50 & -1.00 & 0.00 & 0.00 \\\\\n0.20 & 0.40 & -0.30 & 0.20 \\\\\n0.00 & 0.00 & 4.00 & -1.00 \\\\\n1.00 & 2.00 & 8.00 & -2.00 \\\\\n0.30 & 0.60 & 12.00 & -3.00 \\\\\n\\end{bmatrix},\n\\quad\n\\Sigma_2 = \\mathrm{diag}\\left(10^{-4}, 3\\cdot 10^{-4}, 2\\cdot 10^{-4}, 25^2, 40^2, 50^2\\right).\n$$\n\nTest case $3$ (edge case: an unobservable parameter under the given sensors):\n$$\nJ_3 =\n\\begin{bmatrix}\n0.02 & 0.01 & 0.00 & 0.00 \\\\\n-0.30 & -0.10 & 0.00 & 0.00 \\\\\n0.00 & -0.20 & -0.10 & 0.00 \\\\\n0.00 & 0.00 & 5.00 & 0.00 \\\\\n2.00 & 4.00 & 10.00 & 0.00 \\\\\n0.00 & 0.20 & 15.00 & 0.00 \\\\\n\\end{bmatrix},\n\\quad\n\\Sigma_3 = \\mathrm{diag}\\left(10^{-4}, 3\\cdot 10^{-4}, 2\\cdot 10^{-4}, 25^2, 40^2, 50^2\\right).\n$$\n\nTasks for each test case:\n$1.$ Compute $\\Sigma^{-1}$ and the Fisher Information Matrix $I = J^\\top \\Sigma^{-1} J$.\n$2.$ Compute a symmetric matrix square root of $\\Sigma^{-1}$, denoted $\\Sigma^{-1/2}$, for example via eigenvalue decomposition.\n$3.$ Form the whitened Jacobian $W = \\Sigma^{-1/2} J$ and perform a column-pivoted $\\mathrm{QR}$ factorization to find a maximal set of linearly independent columns under the metric induced by $\\Sigma^{-1}$.\n$4.$ Determine the numerical rank using the tolerance rule above and extract the pivot indices up to the rank.\n$5.$ Sort the selected parameter indices in ascending order.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a bracket-enclosed, comma-separated list of the selected parameter indices for the corresponding test case. For example, the output should look like\n[[i_1,i_2,...],[j_1,j_2,...],[k_1,k_2,...]]\nwith indices expressed as zero-based integers.\n\nThe required final outputs are lists of integers, one list per test case. No units are required for the final outputs. Angles are in radians, forces are in newtons, and noise variances are in $\\text{radian}^2$ and $\\text{newton}^2$ respectively; use these units internally when forming $\\Sigma$ and its inverse.",
            "solution": "The central task is to determine the maximal subset of estimable parameters for a linearized biomechanical model, given a set of sensor configurations. This problem lies at the intersection of estimation theory, numerical linear algebra, and biomechanics. A parameter subset is deemed estimable if its components have a linearly independent influence on the sensor outputs, a condition that can be rigorously assessed using the Fisher Information Matrix (FIM).\n\nThe problem is predicated on a standard model for system identification where measurements $y_{\\text{measured}} \\in \\mathbb{R}^m$ are related to system parameters $\\theta \\in \\mathbb{R}^p$ via a mapping $y(\\theta)$, corrupted by additive Gaussian noise $\\varepsilon \\sim \\mathcal{N}(0, \\Sigma)$. The measurement model is thus $y_{\\text{measured}} = y(\\theta) + \\varepsilon$.\n\nIn a local neighborhood of a nominal parameter vector $\\theta_0$, the mapping can be approximated by its first-order Taylor expansion: $y(\\theta_0 + \\delta\\theta) \\approx y(\\theta_0) + J \\delta\\theta$, where $J = \\left.\\frac{\\partial y}{\\partial \\theta}\\right|_{\\theta_0}$ is the Jacobian matrix. The sensitivity of the measurements to changes in the parameters is entirely captured by this Jacobian.\n\nFor a Gaussian noise model, the Fisher Information Matrix (FIM) provides a measure of the information that the observable data carry about the unknown parameters. It is defined as $I(\\theta_0) = J^\\top \\Sigma^{-1} J$. The FIM is a $p \\times p$ symmetric, positive semi-definite matrix. The estimability of a set of parameters is directly related to the rank of the FIM. A set of parameters is locally identifiable or estimable if and only if the corresponding submatrix of the FIM is full rank (i.e., invertible). A rank deficiency in the FIM implies that there are linear combinations of parameter variations that have no effect on the measurements, making it impossible to distinguish their individual values.\n\nThe rank of the FIM, $\\text{rank}(I) = \\text{rank}(J^\\top \\Sigma^{-1} J)$, is equivalent to the rank of the whitened Jacobian matrix $W = \\Sigma^{-1/2} J$. The matrix $\\Sigma^{-1/2}$ is the symmetric square root of the inverse covariance matrix $\\Sigma^{-1}$ and acts as a \"whitening\" transformation. It transforms the problem into a space where the noise is identity-covariance, i.e., $\\varepsilon' = \\Sigma^{-1/2}\\varepsilon \\sim \\mathcal{N}(0, I)$, and the measurement model becomes $\\Sigma^{-1/2}y_{\\text{measured}} = W\\theta + \\varepsilon'$. In this whitened space, the columns of $W$ represent the sensitivity of the whitened measurements to each parameter, and standard Euclidean geometry can be used to assess their linear independence.\n\nTo find a maximal subset of estimable parameters, we must identify a maximal set of linearly independent columns in the whitened Jacobian $W$. A numerically robust method for this is the column-pivoted QR factorization, which decomposes the matrix $W$ such that $W P = Q R$. Here, $Q$ is an orthogonal matrix, $R$ is an upper-triangular matrix, and $P$ is a permutation matrix (or its corresponding index vector) that reorders the columns of $W$ according to a pivoting strategy. The pivoting strategy, typically based on selecting the column with the largest remaining norm at each step, tends to push linearly dependent columns to the right. The diagonal entries of $R$ reveal the rank: if a column is linearly dependent on the preceding (pivoted) columns, its corresponding diagonal entry in $R$ will be zero (or numerically close to zero).\n\nThe numerical rank is determined by comparing the absolute values of the diagonal entries of $R$ against a tolerance $\\tau$. The problem specifies the following tolerance for a matrix of size $m \\times p$:\n$$\n\\tau = \\max(m,p) \\cdot \\epsilon \\cdot \\max\\left( |R_{11}|, \\| \\Sigma^{-1/2}J \\|_2 \\right)\n$$\nwhere $\\epsilon$ is the machine precision for double-precision floating-point numbers. The numerical rank is the number of diagonal entries $|R_{ii}|$ that are greater than $\\tau$.\n\nThe maximal set of estimable parameters is identified by the first 'rank' indices from the permutation vector $P$. These indices correspond to the original (pre-pivoting) column numbers of $W$ (and thus $J$), which in turn correspond to the parameter indices. The final step is to sort these indices in ascending order.\n\nThe procedure for each test case is as follows:\n$1$. Given the Jacobian $J \\in \\mathbb{R}^{6 \\times 4}$ and the diagonal covariance matrix $\\Sigma \\in \\mathbb{R}^{6 \\times 6}$. The corresponding dimensions are $m=6$ and $p=4$.\n$2$. Construct the diagonal matrix $\\Sigma^{-1/2}$. If $\\Sigma = \\text{diag}(\\sigma_1^2, \\dots, \\sigma_6^2)$, then $\\Sigma^{-1/2} = \\text{diag}(1/\\sigma_1, \\dots, 1/\\sigma_6)$.\n$3$. Compute the whitened Jacobian $W = \\Sigma^{-1/2} J$. This is equivalent to scaling the $i$-th row of $J$ by $1/\\sigma_i$.\n$4$. Perform the column-pivoted QR factorization of $W$ to obtain $Q$, $R$, and the pivot index vector $P$.\n$5$. Compute the tolerance $\\tau$ using the provided formula with $m=6$ and $p=4$.\n$6$. Determine the numerical rank by counting the number of diagonal entries of $R$ such that $|R_{ii}| > \\tau$.\n$7. Select the first `rank` indices from the pivot vector $P$.\n$8$. Sort these indices in ascending order to obtain the final result for the test case.\n\nThis procedure is applied to the three distinct test cases provided.\n- **Test Case 1:** The Jacobian $J_1$ is constructed such that its columns are linearly independent. After whitening, they remain so. The numerical rank is expected to be $4$, and all parameter indices $\\{0, 1, 2, 3\\}$ will be selected.\n- **Test Case 2:** The second column of $J_2$ is exactly twice the first column ($J_{2,:,1} = 2 \\cdot J_{2,:,0}$). This linear dependence is preserved after whitening. The matrix $W_2$ has rank $3$. The QR factorization will identify a basis of three columns, and the resulting estimable set will contain three parameter indices.\n- **Test Case 3:** The fourth column of $J_3$ is a zero vector. This means the fourth parameter ($L_{\\text{slack}}$, index $3$) has no effect on any sensor measurement and is therefore unobservable. The rank of $W_3$ is at most $3$. The QR factorization will correctly identify this, resulting in an estimable set of three parameters.",
            "answer": "```python\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Solves the parameter estimability problem for the three given test cases.\n\n    For each case, it computes the maximal subset of estimable parameters\n    by performing a rank-revealing column-pivoted QR factorization on the\n    whitened Jacobian matrix, as detailed in the problem description.\n    \"\"\"\n\n    # Test cases defined in the problem statement\n    # Each case is a tuple (Jacobian, diagonal_of_Covariance_matrix)\n    test_cases = [\n        (\n            np.array([\n                [0.01, 0.02, 0.00, 0.00],\n                [-0.20, -0.40, 0.05, -0.01],\n                [0.00, -0.05, -0.30, 0.10],\n                [0.00, 0.00, 5.00, -1.00],\n                [1.00, 2.00, 10.00, -2.00],\n                [0.50, 0.80, 15.00, -3.00],\n            ]),\n            np.array([1e-4, 3e-4, 2e-4, 25**2, 40**2, 50**2]),\n        ),\n        (\n            np.array([\n                [0.00, 0.00, 0.10, 0.00],\n                [-0.50, -1.00, 0.00, 0.00],\n                [0.20, 0.40, -0.30, 0.20],\n                [0.00, 0.00, 4.00, -1.00],\n                [1.00, 2.00, 8.00, -2.00],\n                [0.30, 0.60, 12.00, -3.00],\n            ]),\n            np.array([1e-4, 3e-4, 2e-4, 25**2, 40**2, 50**2]),\n        ),\n        (\n            np.array([\n                [0.02, 0.01, 0.00, 0.00],\n                [-0.30, -0.10, 0.00, 0.00],\n                [0.00, -0.20, -0.10, 0.00],\n                [0.00, 0.00, 5.00, 0.00],\n                [2.00, 4.00, 10.00, 0.00],\n                [0.00, 0.20, 15.00, 0.00],\n            ]),\n            np.array([1e-4, 3e-4, 2e-4, 25**2, 40**2, 50**2]),\n        ),\n    ]\n\n    results = []\n    for J, sigma_diag in test_cases:\n        # Get matrix dimensions\n        m, p = J.shape\n\n        # Step 1: Compute the symmetric square root of the inverse covariance matrix\n        # Since Sigma is diagonal, Sigma_inv_sqrt is also diagonal.\n        sigma_inv_sqrt_diag = 1.0 / np.sqrt(sigma_diag)\n\n        # Step 2: Form the whitened Jacobian W = Sigma^{-1/2} * J\n        # This is equivalent to scaling each row of J.\n        W = sigma_inv_sqrt_diag[:, np.newaxis] * J\n\n        # Step 3: Perform column-pivoted QR factorization\n        # A[:, P] = Q @ R, where P is the permutation vector (pivot indices)\n        Q, R, P = linalg.qr(W, pivoting=True)\n\n        # Step 4: Determine the numerical rank using the specified tolerance\n        eps = np.finfo(float).eps\n        \n        # Handle the case where W might be a zero matrix, causing R to be zero.\n        if R.shape[0] > 0 and R.shape[1] > 0:\n            R11_abs = np.abs(R[0, 0])\n        else: # empty or 1-d matrix\n            R11_abs = 0.0\n\n        w_norm2 = linalg.norm(W, ord=2)\n        \n        tau = max(m, p) * eps * max(R11_abs, w_norm2)\n\n        # The rank is the number of diagonal entries of R whose magnitude exceeds tau\n        diag_R_abs = np.abs(np.diag(R))\n        rank = np.sum(diag_R_abs > tau)\n\n        # Step 5: Extract the pivot indices up to the determined rank\n        estimable_indices = P[:rank]\n\n        # Step 6: Sort the selected parameter indices in ascending order\n        estimable_indices.sort()\n        results.append(list(estimable_indices))\n\n    # Format the final output string as specified: [[i_1,i_2,...],[j_1,j_2,...],...]\n    formatted_results = [f\"[{','.join(map(str, r))}]\" for r in results]\n    final_output_string = f\"[{','.join(formatted_results)}]\"\n    \n    print(final_output_string)\n\nsolve()\n```"
        },
        {
            "introduction": "Many inverse problems in biomechanics, such as estimating individual muscle forces ($f$) from net joint torques ($\\tau$), are inherently ill-posed due to muscular redundancy. Even when parameters are theoretically estimable , the resulting system of equations may be ill-conditioned, making solutions highly sensitive to measurement noise. This exercise introduces Tikhonov regularization, a fundamental technique that finds a stable and physically plausible solution by adding a penalty term controlled by a parameter $\\alpha$, effectively balancing data fidelity with solution simplicity.",
            "id": "4186288",
            "problem": "You are given a static inverse biomechanics problem framed in purely mathematical terms. The mapping from muscle forces to joint torques can be represented by a moment arm matrix. Let $R \\in \\mathbb{R}^{m \\times n}$ denote the moment arm matrix, $f \\in \\mathbb{R}^{n}$ the unknown muscle force vector, and $\\tau \\in \\mathbb{R}^{m}$ the measured joint torque vector. Measurements of $\\tau$ are noisy. The static equilibrium relation for joint torques is modeled as $ \\tau = R f + \\varepsilon $, where $ \\varepsilon $ captures measurement noise. To obtain a robust estimate of $f$ for redundant systems (when $n > m$), use zero-order Tikhonov regularization and the normal equations.\n\nStarting from the definitions above and foundational laws of mechanics, derive the regularized normal equations and compute the regularized solution for the muscle forces $f$ for several test cases. Then analyze the sensitivity of $f$ to the regularization parameter $\\alpha$ through an analytic derivative with respect to $\\alpha$ and via conditioning of the normal matrix. All computations are to be expressed in the International System of Units (SI): forces in Newtons (N) and torques in Newton-meters (N·m). Angle units are not required for this problem.\n\nFundamental base on which you must build:\n- Newton's second law of motion and static equilibrium principles imply that the net joint torque equals the sum of moments generated by muscle forces acting through moment arms.\n- Linear algebra definitions and properties for least-squares and regularization.\n\nTask requirements:\n1. Derive, from first principles, the regularized normal equations for the zero-order Tikhonov problem, then implement the computation of the regularized solution $f$ using only the normal equations for each specified test case.\n2. For each test case and a provided set of three strictly positive regularization parameters $ \\alpha $, compute:\n   - The regularized solution $ f(\\alpha) $ for each provided $ \\alpha $.\n   - The condition number of the regularized normal matrix $ R^\\top R + \\alpha I $ for the smallest and largest $ \\alpha $ in the set, using the $2$-norm.\n   - The analytical sensitivity magnitude at the middle $ \\alpha $, defined as $ s = \\| \\frac{d f}{d \\alpha} \\|_2 / \\| f \\|_2 $, where $ \\frac{d f}{d \\alpha} $ is the derivative of the solution with respect to $ \\alpha $.\n   - A boolean indicating whether $ \\| f(\\alpha) \\|_2 $ is non-increasing as $ \\alpha $ increases across the three given values.\n3. Express floats rounded to six decimal places. The sensitivity $ s $ and condition numbers are dimensionless. Do not output the forces or torques themselves; only output the requested dimensionless metrics and the boolean monotonicity check.\n4. Use the following test suite. Each test case supplies $R$, a noisy torque vector $\\tau$, and a strictly increasing list of three regularization parameters $\\alpha$:\n   - Test Case $1$ (two joints, three muscles, moderate noise):\n     $$\n     R_1 = \\begin{bmatrix}\n     0.04 & 0.02 & -0.01 \\\\\n     0.00 & 0.03 & 0.05\n     \\end{bmatrix}, \\quad\n     \\tau_1 = \\begin{bmatrix}\n     6.6 \\\\\n     4.9\n     \\end{bmatrix}, \\quad\n     \\alpha_1 = \\left[ 10^{-4},\\, 10^{-2},\\, 1.0 \\right].\n     $$\n   - Test Case $2$ (one joint, four muscles, near collinearity and small noise):\n     $$\n     R_2 = \\begin{bmatrix}\n     0.03 & 0.029 & -0.03 & 0.0301\n     \\end{bmatrix}, \\quad\n     \\tau_2 = \\begin{bmatrix}\n     3.149\n     \\end{bmatrix}, \\quad\n     \\alpha_2 = \\left[ 10^{-6},\\, 10^{-3},\\, 10^{-1} \\right].\n     $$\n   - Test Case $3$ (three joints, six muscles, higher noise and mild dependencies):\n     $$\n     R_3 = \\begin{bmatrix}\n     0.05 & 0.02 & -0.01 & 0.00 & 0.03 & -0.02 \\\\\n     0.00 & 0.04 & 0.02 & -0.01 & 0.01 & 0.03 \\\\\n     0.01 & -0.01 & 0.03 & 0.02 & -0.03 & 0.00\n     \\end{bmatrix}, \\quad\n     \\tau_3 = \\begin{bmatrix}\n     7.7 \\\\\n     4.1 \\\\\n     2.4\n     \\end{bmatrix}, \\quad\n     \\alpha_3 = \\left[ 10^{-5},\\, 10^{-2},\\, 1.0 \\right].\n     $$\n5. Final output format:\n   - Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a sublist of the form $[\\text{cond\\_low}, \\text{cond\\_high}, \\text{sensitivity\\_mid}, \\text{monotone}]$. For example:\n     [ [c_1,low, c_1,high, s_1, True], [c_2,low, c_2,high, s_2, False], [c_3,low, c_3,high, s_3, True] ]\n   - All floating-point values must be rounded to six decimal places. The boolean must be written as either $\\text{True}$ or $\\text{False}$.",
            "solution": "The problem is assessed to be valid. It is scientifically grounded in established principles of static biomechanics and numerical linear algebra, is well-posed through the use of Tikhonov regularization, and is specified with objective, complete, and consistent mathematical definitions and data.\n\nThe core of the problem is to find a stable estimate for the muscle force vector $f \\in \\mathbb{R}^{n}$ from noisy measurements of a joint torque vector $\\tau \\in \\mathbb{R}^{m}$, given the linear relationship $\\tau \\approx R f$, where $R \\in \\mathbb{R}^{m \\times n}$ is the moment arm matrix. For a redundant system ($n > m$), the problem is underdetermined and ill-posed. Zero-order Tikhonov regularization addresses this by finding a vector $f$ that minimizes a composite cost function, balancing the data fidelity (squared error of the model) and the magnitude of the solution (squared $L_2$-norm of forces), controlled by a regularization parameter $\\alpha > 0$.\n\n### Derivation of the Regularized Normal Equations\nThe Tikhonov cost function, $J(f)$, is defined as:\n$$ J(f) = \\| R f - \\tau \\|_2^2 + \\alpha \\| f \\|_2^2 $$\nHere, $\\| \\cdot \\|_2$ denotes the Euclidean ($L_2$) norm. We seek the vector $f$ that minimizes $J(f)$. To derive the solution, we first expand the norm terms using the identity $\\|x\\|_2^2 = x^\\top x$:\n$$ J(f) = (R f - \\tau)^\\top (R f - \\tau) + \\alpha (f^\\top f) $$\nExpanding the first term yields:\n$$ J(f) = (f^\\top R^\\top - \\tau^\\top)(R f - \\tau) + \\alpha f^\\top f $$\n$$ J(f) = f^\\top R^\\top R f - f^\\top R^\\top \\tau - \\tau^\\top R f + \\tau^\\top \\tau + \\alpha f^\\top f $$\nSince the term $\\tau^\\top R f$ is a scalar, it is equal to its transpose $(f^\\top R^\\top \\tau)^\\top$, which is $f^\\top R^\\top \\tau$. This allows combining the two cross-terms. We can also write $\\alpha f^\\top f$ as $\\alpha f^\\top I f$, where $I$ is the identity matrix of size $n \\times n$.\n$$ J(f) = f^\\top (R^\\top R) f - 2 f^\\top R^\\top \\tau + \\tau^\\top \\tau + f^\\top (\\alpha I) f $$\n$$ J(f) = f^\\top (R^\\top R + \\alpha I) f - 2 (R^\\top \\tau)^\\top f + \\tau^\\top \\tau $$\nThis expression is a quadratic form in $f$. A minimum exists and can be found by taking the gradient of $J(f)$ with respect to $f$ and setting it to zero. Using the matrix calculus identities $\\frac{\\partial}{\\partial x}(x^\\top A x) = 2Ax$ for symmetric $A$ and $\\frac{\\partial}{\\partial x}(b^\\top x) = b$, we get:\n$$ \\nabla_f J(f) = 2(R^\\top R + \\alpha I) f - 2 R^\\top \\tau $$\nSetting the gradient to zero, $\\nabla_f J(f) = 0$:\n$$ 2(R^\\top R + \\alpha I) f - 2 R^\\top \\tau = 0 $$\n$$ (R^\\top R + \\alpha I) f = R^\\top \\tau $$\nThis is the **regularized normal equation**. The matrix $A(\\alpha) = R^\\top R + \\alpha I$ is symmetric. Since $R^\\top R$ is positive semi-definite and $\\alpha > 0$, $A(\\alpha)$ is positive definite and therefore invertible. The solution for the muscle force vector $f$ is unique and given by:\n$$ f(\\alpha) = (R^\\top R + \\alpha I)^{-1} R^\\top \\tau $$\n\n### Derivation of the Analytical Sensitivity\nThe sensitivity of the solution $f$ with respect to the regularization parameter $\\alpha$ is its derivative, $\\frac{df}{d\\alpha}$. This can be found by implicitly differentiating the normal equation with respect to $\\alpha$:\n$$ \\frac{d}{d\\alpha} \\left[ (R^\\top R + \\alpha I) f(\\alpha) \\right] = \\frac{d}{d\\alpha} (R^\\top \\tau) $$\nThe right-hand side is zero since $R$ and $\\tau$ are constant with respect to $\\alpha$. Applying the product rule for differentiation to the left-hand side gives:\n$$ \\left[ \\frac{d}{d\\alpha} (R^\\top R + \\alpha I) \\right] f(\\alpha) + (R^\\top R + \\alpha I) \\frac{df}{d\\alpha} = 0 $$\nThe derivative of $(R^\\top R + \\alpha I)$ with respect to $\\alpha$ is the identity matrix $I$.\n$$ I f(\\alpha) + (R^\\top R + \\alpha I) \\frac{df}{d\\alpha} = 0 $$\nSolving for the derivative $\\frac{df}{d\\alpha}$:\n$$ \\frac{df}{d\\alpha} = -(R^\\top R + \\alpha I)^{-1} f(\\alpha) $$\nThe problem requires the relative sensitivity magnitude, defined as:\n$$ s = \\frac{\\| \\frac{df}{d\\alpha} \\|_2}{\\| f(\\alpha) \\|_2} $$\n\n### Computation of Metrics\nFor each test case and its set of three strictly increasing regularization parameters $(\\alpha_{low}, \\alpha_{mid}, \\alpha_{high})$, the following metrics are computed:\n1.  **Condition Number ($cond_2$)**: The $2$-norm condition number of the regularized normal matrix $A(\\alpha) = R^\\top R + \\alpha I$ is computed for $\\alpha_{low}$ and $\\alpha_{high}$. It measures the numerical stability of the linear system, with higher values indicating greater ill-conditioning.\n2.  **Sensitivity Magnitude ($s$)**: The sensitivity $s$ is computed at $\\alpha = \\alpha_{mid}$ using the derived analytical expressions for $f(\\alpha_{mid})$ and $\\frac{df}{d\\alpha}|_{\\alpha_{mid}}$.\n3.  **Monotonicity Check**: The $2$-norms of the solution vectors, $\\|f(\\alpha_{low})\\|_2$, $\\|f(\\alpha_{mid})\\|_2$, and $\\|f(\\alpha_{high})\\|_2$, are calculated. The boolean `monotone` is `True` if $\\|f(\\alpha_{low})\\|_2 \\ge \\|f(\\alpha_{mid})\\|_2 \\ge \\|f(\\alpha_{high})\\|_2$, and `False` otherwise. It can be shown that $\\|f(\\alpha)\\|_2$ is a monotonically decreasing function for $\\alpha > 0$, so this is expected to be `True`.\n\nThese derivations provide the foundation for the numerical implementation.",
            "answer": "```python\nimport numpy as np\n\ndef calculate_metrics(R, tau, alphas):\n    \"\"\"\n    Computes Tikhonov regularization metrics for a given biomechanics problem.\n\n    Args:\n        R (np.ndarray): The moment arm matrix (m x n).\n        tau (np.ndarray): The measured torque vector (m,).\n        alphas (list): A list of three strictly positive regularization parameters.\n\n    Returns:\n        list: A list containing [cond_low, cond_high, sensitivity_mid, monotone].\n    \"\"\"\n    alpha_low, alpha_mid, alpha_high = alphas\n    n = R.shape[1]\n    I = np.identity(n)\n\n    Rt = R.T\n    RtR = Rt @ R\n    Rt_tau = Rt @ tau\n\n    # --- Condition Numbers ---\n    A_low = RtR + alpha_low * I\n    cond_low = np.linalg.cond(A_low, 2)\n\n    A_high = RtR + alpha_high * I\n    cond_high = np.linalg.cond(A_high, 2)\n\n    # --- Sensitivity at alpha_mid ---\n    A_mid = RtR + alpha_mid * I\n    A_mid_inv = np.linalg.inv(A_mid)\n    \n    f_mid = A_mid_inv @ Rt_tau\n    \n    df_dalpha_mid = -A_mid_inv @ f_mid\n    \n    norm_f_mid = np.linalg.norm(f_mid, 2)\n    norm_df_dalpha_mid = np.linalg.norm(df_dalpha_mid, 2)\n    \n    if norm_f_mid < 1e-12:  # Avoid division by zero if f is essentially zero\n        s_mid = 0.0\n    else:\n        s_mid = norm_df_dalpha_mid / norm_f_mid\n\n    # --- Monotonicity Check ---\n    f_low = np.linalg.inv(A_low) @ Rt_tau\n    norm_f_low = np.linalg.norm(f_low, 2)\n\n    f_high = np.linalg.inv(A_high) @ Rt_tau\n    norm_f_high = np.linalg.norm(f_high, 2)\n    \n    is_monotone = (norm_f_low >= norm_f_mid) and (norm_f_mid >= norm_f_high)\n\n    return [cond_low, cond_high, s_mid, is_monotone]\n\ndef solve():\n    \"\"\"\n    Defines test cases and computes the required metrics for each,\n    then prints the results in the specified format.\n    \"\"\"\n    test_cases = [\n        {\n            \"R\": np.array([[0.04, 0.02, -0.01], [0.00, 0.03, 0.05]]),\n            \"tau\": np.array([6.6, 4.9]),\n            \"alphas\": [1e-4, 1e-2, 1.0],\n        },\n        {\n            \"R\": np.array([[0.03, 0.029, -0.03, 0.0301]]),\n            \"tau\": np.array([3.149]),\n            \"alphas\": [1e-6, 1e-3, 1e-1],\n        },\n        {\n            \"R\": np.array([\n                [0.05, 0.02, -0.01, 0.00, 0.03, -0.02],\n                [0.00, 0.04, 0.02, -0.01, 0.01, 0.03],\n                [0.01, -0.01, 0.03, 0.02, -0.03, 0.00]\n            ]),\n            \"tau\": np.array([7.7, 4.1, 2.4]),\n            \"alphas\": [1e-5, 1e-2, 1.0],\n        },\n    ]\n\n    all_results = []\n    for case in test_cases:\n        metrics = calculate_metrics(case[\"R\"], case[\"tau\"], case[\"alphas\"])\n        all_results.append(metrics)\n\n    # Format the final output string precisely as requested.\n    string_results = []\n    for res in all_results:\n        # res = [cond_low, cond_high, s_mid, monotone]\n        # Floats are formatted to six decimal places.\n        # str(bool) correctly produces 'True' or 'False'.\n        res_str = (\n            f\"[{res[0]:.6f},{res[1]:.6f},{res[2]:.6f},{str(res[3])}]\"\n        )\n        string_results.append(res_str)\n    \n    final_output = f\"[{','.join(string_results)}]\"\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}