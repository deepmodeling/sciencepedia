{
    "hands_on_practices": [
        {
            "introduction": "Accurate movement requires a precise estimate of the body's state, yet all sensory feedback is corrupted by noise. This practice explores how the central nervous system can optimally combine information from multiple sensory modalities, such as vision and proprioception, using the principles of Bayesian inference. By deriving the fused estimate yourself , you will gain a foundational understanding of sensory reweighting, a key mechanism for robust state estimation in the face of uncertainty.",
            "id": "4192578",
            "problem": "A participant performs a one-dimensional reaching task in which the Central Nervous System (CNS) must estimate hand position along a single axis. Two independent sensory modalities provide unbiased measurements of the same underlying true position, each with additive Gaussian noise. Let the true position be denoted by $x$, and the observed measurements be $x_{1}$ (modality $1$) and $x_{2}$ (modality $2$). Assume $x_{1} \\sim \\mathcal{N}(x, \\sigma_{1}^{2})$ and $x_{2} \\sim \\mathcal{N}(x, \\sigma_{2}^{2})$, with independence between modalities and no systematic bias. Assume an uninformative prior on $x$.\n\nIn a particular trial, modality $1$ (e.g., proprioception) reports $x_{1} = 12$ mm and modality $2$ (e.g., vision) reports $x_{2} = 8$ mm. Their noise variances are $\\sigma_{1}^{2} = 25$ mm$^{2}$ and $\\sigma_{2}^{2} = 9$ mm$^{2}$, respectively.\n\nStarting from Bayes’ rule and the Gaussian likelihood model, derive the posterior distribution $p(x \\mid x_{1}, x_{2})$ and identify its mean (the Bayesian fused estimate) and variance. Then compute the fused estimate and its variance for the given numerical values. Finally, explain how sensory reweighting arises from relative reliability within this framework and how it manifests in the fused estimate.\n\nExpress the fused estimate in millimeters and the fused variance in square millimeters. Provide the final numerical values as exact simplified fractions; do not round.",
            "solution": "The problem requires the derivation of the posterior distribution for the true position $x$ given two independent sensory measurements, $x_{1}$ and $x_{2}$, and then the computation of its mean and variance for a specific trial. Finally, an explanation of sensory reweighting is required.\n\n**Step 1: Problem Validation**\nThe problem statement is scientifically grounded, well-posed, and objective. It describes a standard Bayesian Maximum Likelihood Estimation (MLE) model for sensory integration, a fundamental concept in computational neuroscience and motor control. All givens are clearly defined, consistent, and scientifically plausible. The problem is valid.\n\n**Step 2: Derivation of the Posterior Distribution**\nAccording to Bayes' rule, the posterior probability of the true position $x$ given the measurements $x_{1}$ and $x_{2}$ is:\n$$p(x \\mid x_{1}, x_{2}) = \\frac{p(x_{1}, x_{2} \\mid x) p(x)}{p(x_{1}, x_{2})}$$\nThe term $p(x_{1}, x_{2})$ is a normalization constant, so we can express this as a proportionality:\n$$p(x \\mid x_{1}, x_{2}) \\propto p(x_{1}, x_{2} \\mid x) p(x)$$\nThe problem states that the sensory modalities are independent. Therefore, the joint likelihood $p(x_{1}, x_{2} \\mid x)$ is the product of the individual likelihoods:\n$$p(x_{1}, x_{2} \\mid x) = p(x_{1} \\mid x) p(x_{2} \\mid x)$$\nThe prior on $x$, $p(x)$, is given as \"uninformative\". In this context, this is treated as a uniform prior, $p(x) \\propto \\text{constant}$. This improper prior is acceptable here because the posterior will be proper. Thus, the posterior is proportional to the product of the likelihoods:\n$$p(x \\mid x_{1}, x_{2}) \\propto p(x_{1} \\mid x) p(x_{2} \\mid x)$$\nThe likelihoods are given by the Gaussian probability density functions for $x_{1} \\sim \\mathcal{N}(x, \\sigma_{1}^{2})$ and $x_{2} \\sim \\mathcal{N}(x, \\sigma_{2}^{2})$:\n$$p(x_{1} \\mid x) = \\frac{1}{\\sqrt{2\\pi\\sigma_{1}^{2}}} \\exp\\left(-\\frac{(x_{1} - x)^{2}}{2\\sigma_{1}^{2}}\\right)$$\n$$p(x_{2} \\mid x) = \\frac{1}{\\sqrt{2\\pi\\sigma_{2}^{2}}} \\exp\\left(-\\frac{(x_{2} - x)^{2}}{2\\sigma_{2}^{2}}\\right)$$\nSubstituting these into the proportionality for the posterior and dropping the constant normalization factors:\n$$p(x \\mid x_{1}, x_{2}) \\propto \\exp\\left(-\\frac{(x_{1} - x)^{2}}{2\\sigma_{1}^{2}}\\right) \\exp\\left(-\\frac{(x_{2} - x)^{2}}{2\\sigma_{2}^{2}}\\right)$$\n$$p(x \\mid x_{1}, x_{2}) \\propto \\exp\\left(-\\frac{1}{2}\\left[\\frac{(x - x_{1})^{2}}{\\sigma_{1}^{2}} + \\frac{(x - x_{2})^{2}}{\\sigma_{2}^{2}}\\right]\\right)$$\nTo identify the form of this distribution, we analyze the exponent by expanding the quadratic terms in $x$:\n$$-\\frac{1}{2}\\left[ \\frac{x^{2} - 2xx_{1} + x_{1}^{2}}{\\sigma_{1}^{2}} + \\frac{x^{2} - 2xx_{2} + x_{2}^{2}}{\\sigma_{2}^{2}} \\right]$$\n$$= -\\frac{1}{2}\\left[ x^{2}\\left(\\frac{1}{\\sigma_{1}^{2}} + \\frac{1}{\\sigma_{2}^{2}}\\right) - 2x\\left(\\frac{x_{1}}{\\sigma_{1}^{2}} + \\frac{x_{2}}{\\sigma_{2}^{2}}\\right) + \\left(\\frac{x_{1}^{2}}{\\sigma_{1}^{2}} + \\frac{x_{2}^{2}}{\\sigma_{2}^{2}}\\right) \\right]$$\nThis is a quadratic function of $x$. The resulting posterior distribution is therefore also a Gaussian, $\\mathcal{N}(\\hat{x}, \\sigma_{\\text{fused}}^{2})$, whose exponent is of the form $-\\frac{(x - \\hat{x})^{2}}{2\\sigma_{\\text{fused}}^{2}} = -\\frac{1}{2\\sigma_{\\text{fused}}^{2}}(x^{2} - 2x\\hat{x} + \\hat{x}^{2})$.\n\nBy comparing the coefficients of the $x^{2}$ term, we find the inverse variance of the posterior:\n$$\\frac{1}{\\sigma_{\\text{fused}}^{2}} = \\frac{1}{\\sigma_{1}^{2}} + \\frac{1}{\\sigma_{2}^{2}}$$\nThis shows that the precision (inverse variance) of the fused estimate is the sum of the precisions of the individual sensory modalities. The variance of the fused estimate is:\n$$\\sigma_{\\text{fused}}^{2} = \\left(\\frac{1}{\\sigma_{1}^{2}} + \\frac{1}{\\sigma_{2}^{2}}\\right)^{-1} = \\frac{\\sigma_{1}^{2}\\sigma_{2}^{2}}{\\sigma_{1}^{2} + \\sigma_{2}^{2}}$$\nBy comparing the coefficients of the $x$ term, we find the mean of the posterior:\n$$\\frac{2\\hat{x}}{2\\sigma_{\\text{fused}}^{2}} = \\frac{x_{1}}{\\sigma_{1}^{2}} + \\frac{x_{2}}{\\sigma_{2}^{2}}$$\n$$\\hat{x} = \\sigma_{\\text{fused}}^{2} \\left(\\frac{x_{1}}{\\sigma_{1}^{2}} + \\frac{x_{2}}{\\sigma_{2}^{2}}\\right) = \\frac{\\frac{x_{1}}{\\sigma_{1}^{2}} + \\frac{x_{2}}{\\sigma_{2}^{2}}}{\\frac{1}{\\sigma_{1}^{2}} + \\frac{1}{\\sigma_{2}^{2}}}$$\nThis mean $\\hat{x}$ is the Bayesian fused estimate.\n\n**Step 3: Numerical Computation**\nThe given values are:\n$x_{1} = 12$ mm, $\\sigma_{1}^{2} = 25$ mm$^{2}$\n$x_{2} = 8$ mm, $\\sigma_{2}^{2} = 9$ mm$^{2}$\n\nWe compute the fused variance, $\\sigma_{\\text{fused}}^{2}$:\n$$\\frac{1}{\\sigma_{\\text{fused}}^{2}} = \\frac{1}{25} + \\frac{1}{9} = \\frac{9 + 25}{225} = \\frac{34}{225}$$\n$$\\sigma_{\\text{fused}}^{2} = \\frac{225}{34} \\text{ mm}^{2}$$\nNext, we compute the fused estimate, $\\hat{x}$:\n$$\\hat{x} = \\frac{\\frac{12}{25} + \\frac{8}{9}}{\\frac{1}{25} + \\frac{1}{9}} = \\frac{\\frac{12 \\times 9 + 8 \\times 25}{225}}{\\frac{34}{225}}$$\n$$\\hat{x} = \\frac{\\frac{108 + 200}{225}}{\\frac{34}{225}} = \\frac{308/225}{34/225} = \\frac{308}{34}$$\nSimplifying the fraction gives:\n$$\\hat{x} = \\frac{154}{17} \\text{ mm}$$\n\n**Step 4: Explanation of Sensory Reweighting**\nThe formula for the fused estimate $\\hat{x}$ can be expressed as a weighted average of the individual measurements $x_{1}$ and $x_{2}$:\n$$\\hat{x} = w_{1}x_{1} + w_{2}x_{2}$$\nwhere the weights $w_{1}$ and $w_{2}$ are given by:\n$$w_{1} = \\frac{1/\\sigma_{1}^{2}}{1/\\sigma_{1}^{2} + 1/\\sigma_{2}^{2}} \\quad \\text{and} \\quad w_{2} = \\frac{1/\\sigma_{2}^{2}}{1/\\sigma_{1}^{2} + 1/\\sigma_{2}^{2}}$$\nNote that $w_{1} + w_{2} = 1$. The weight $w_{i}$ assigned to a measurement $x_{i}$ is proportional to its precision, $1/\\sigma_{i}^{2}$. Precision is a measure of a sensory modality's reliability; a smaller variance (less noise) corresponds to higher precision and thus greater reliability.\n\n**Sensory reweighting** is the principle, derived from this Bayesian framework, that the central nervous system combines sensory signals by assigning more weight to more reliable (i.e., less noisy or more precise) sources of information.\n\nThis principle manifests in the fused estimate by pulling it closer to the measurement from the more reliable modality. For the given values:\n- Precision of modality $1$: $1/\\sigma_{1}^{2} = 1/25$\n- Precision of modality $2$: $1/\\sigma_{2}^{2} = 1/9$\nSince $1/9 > 1/25$, modality $2$ is more reliable.\n\nLet's compute the numerical weights:\n- Total precision: $1/25 + 1/9 = 34/225$\n- Weight for modality $1$: $w_{1} = \\frac{1/25}{34/225} = \\frac{1}{25} \\times \\frac{225}{34} = \\frac{9}{34}$\n- Weight for modality $2$: $w_{2} = \\frac{1/9}{34/225} = \\frac{1}{9} \\times \\frac{225}{34} = \\frac{25}{34}$\n\nThe weight for the more reliable modality $2$ ($w_{2} = 25/34 \\approx 0.735$) is significantly larger than the weight for modality $1$ ($w_{1} = 9/34 \\approx 0.265$). The individual measurements are $x_{1} = 12$ and $x_{2} = 8$. The fused estimate is $\\hat{x} = 154/17 \\approx 9.06$, which is much closer to $x_{2}=8$ than to $x_{1}=12$. This demonstrates how the brain optimally \"reweights\" its trust in sensory inputs based on their relative quality, a core strategy for robust motor control. Furthermore, the resulting estimate is more precise than either input alone: $\\sigma_{\\text{fused}}^{2} = 225/34 \\approx 6.62$, which is less than both $\\sigma_{1}^{2}=25$ and $\\sigma_{2}^{2}=9$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{154}{17} & \\frac{225}{34}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Human reaching movements are stereotypically smooth, suggesting they follow a pre-planned path that optimizes some criterion. This exercise delves into the influential minimum-jerk hypothesis, which proposes that the brain plans trajectories that are as smooth as kinematically possible. By applying the calculus of variations to define a trajectory between two points , you will see how a simple optimization principle can account for complex biological motion.",
            "id": "4192584",
            "problem": "A central hypothesis in human motor control is that voluntary reaching movements are planned to minimize the time integral of squared kinematic jerk. Consider a planar point-mass end-effector undergoing a reach in the horizontal plane from initial position $(0,0)$ in meters to final position $(0.25,0.15)$ in meters, completed in duration $T=0.8 \\ \\mathrm{s}$. The movement is constrained to have zero initial and final velocity and acceleration in both coordinates. Let the movement in each Cartesian coordinate be represented by a time-polynomial $x(t)=\\sum_{k=0}^{5} a_{k}^{x} t^{k}$ and $y(t)=\\sum_{k=0}^{5} a_{k}^{y} t^{k}$ for $t \\in [0,T]$, where the coefficients $a_{k}^{x}$ and $a_{k}^{y}$ are constant and to be determined.\n\nStarting from the definition of the minimum-jerk objective as the cost functional\n$$\n\\mathcal{J} = \\int_{0}^{T} \\left( \\left( \\frac{d^{3} x}{d t^{3}} \\right)^{2} + \\left( \\frac{d^{3} y}{d t^{3}} \\right)^{2} \\right) dt,\n$$\nand using only fundamental principles of calculus of variations and the stated kinematic boundary conditions at $t=0$ and $t=T$, derive the form of the optimal coordinate trajectories and compute the polynomial coefficients $a_{0}^{x}, a_{1}^{x}, a_{2}^{x}, a_{3}^{x}, a_{4}^{x}, a_{5}^{x}$ and $a_{0}^{y}, a_{1}^{y}, a_{2}^{y}, a_{3}^{y}, a_{4}^{y}, a_{5}^{y}$ for the specified reach. Report the coefficients for $x(t)$ and $y(t)$ in the order $(a_{0}^{x}, a_{1}^{x}, a_{2}^{x}, a_{3}^{x}, a_{4}^{x}, a_{5}^{x}, a_{0}^{y}, a_{1}^{y}, a_{2}^{y}, a_{3}^{y}, a_{4}^{y}, a_{5}^{y})$.\n\nExpress each coefficient in International System of Units (SI), where $a_{0}$ is in meters, $a_{1}$ is in meters per second, $a_{2}$ is in meters per second squared, $a_{3}$ is in meters per second cubed, $a_{4}$ is in meters per second to the fourth power, and $a_{5}$ is in meters per second to the fifth power. Round each nonzero coefficient to eight significant figures. In your final boxed answer, provide the twelve numbers as a single row vector and do not include units inside the box. Angle units are not applicable in this problem.",
            "solution": "The problem statement is first subjected to a rigorous validation process.\n\n### Step 1: Extract Givens\n- **Hypothesis**: Voluntary reaching movements are planned to minimize the time integral of squared kinematic jerk.\n- **System**: A planar point-mass end-effector.\n- **Initial Position**: $(x(0), y(0)) = (0,0)$ meters.\n- **Final Position**: $(x(T), y(T)) = (0.25, 0.15)$ meters.\n- **Movement Duration**: $T=0.8 \\ \\mathrm{s}$.\n- **Boundary Conditions at $t=0$**: Zero velocity and acceleration.\n  - $\\dot{x}(0) = 0$, $\\dot{y}(0) = 0$.\n  - $\\ddot{x}(0) = 0$, $\\ddot{y}(0) = 0$.\n- **Boundary Conditions at $t=T$**: Zero velocity and acceleration.\n  - $\\dot{x}(T) = 0$, $\\dot{y}(T) = 0$.\n  - $\\ddot{x}(T) = 0$, $\\ddot{y}(T) = 0$.\n- **Trajectory Model**:\n  - $x(t) = \\sum_{k=0}^{5} a_{k}^{x} t^{k}$ for $t \\in [0,T]$.\n  - $y(t) = \\sum_{k=0}^{5} a_{k}^{y} t^{k}$ for $t \\in [0,T]$.\n- **Cost Functional**: $\\mathcal{J} = \\int_{0}^{T} \\left( \\left( \\frac{d^{3} x}{d t^{3}} \\right)^{2} + \\left( \\frac{d^{3} y}{d t^{3}} \\right)^{2} \\right) dt$.\n- **Objective**: Derive the optimal trajectories and compute the twelve polynomial coefficients $(a_{0}^{x}, a_{1}^{x}, a_{2}^{x}, a_{3}^{x}, a_{4}^{x}, a_{5}^{x})$ and $(a_{0}^{y}, a_{1}^{y}, a_{2}^{y}, a_{3}^{y}, a_{4}^{y}, a_{5}^{y})$.\n- **Output Specification**: Report the twelve coefficients as a single row vector, with each nonzero coefficient rounded to eight significant figures.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientific Grounding**: The problem is based on the minimum-jerk hypothesis, a seminal and well-established principle in computational motor control, originally formulated by Flash and Hogan (1985). This is scientifically sound.\n- **Well-Posedness**: The problem seeks to find the coefficients of two $5$th-degree polynomials. Each polynomial has $6$ unknown coefficients. For each coordinate ($x$ and $y$), there are $6$ boundary conditions provided: position at $t=0$ and $t=T$, velocity at $t=0$ and $t=T$, and acceleration at $t=0$ and $t=T$. This constitutes a well-posed problem with a sufficient number of conditions to determine a unique solution for the coefficients.\n- **Objectivity**: The problem is stated using precise mathematical and physical language, with all quantities clearly defined. It is free of ambiguity or subjective claims.\n- **Conclusion**: The problem is scientifically grounded, well-posed, complete, and stated objectively. It does not violate any of the invalidity criteria.\n\n### Step 3: Verdict and Action\nThe problem is valid. The solution process will now commence.\n\nThe cost functional $\\mathcal{J}$ is separable for the $x$ and $y$ coordinates:\n$$\n\\mathcal{J} = \\int_{0}^{T} \\left( \\frac{d^{3} x}{d t^{3}} \\right)^{2} dt + \\int_{0}^{T} \\left( \\frac{d^{3} y}{d t^{3}} \\right)^{2} dt = \\mathcal{J}_x + \\mathcal{J}_y\n$$\nMinimizing $\\mathcal{J}$ is equivalent to independently minimizing $\\mathcal{J}_x$ and $\\mathcal{J}_y$. Let us consider the general problem for a coordinate $p(t)$, which can represent either $x(t)$ or $y(t)$. The functional to be minimized is $\\mathcal{J}_p = \\int_{0}^{T} (\\dddot{p}(t))^2 dt$. The integrand is $L(t, p, \\dot{p}, \\ddot{p}, \\dddot{p}) = (\\dddot{p})^2$.\n\nAccording to the fundamental principles of the calculus of variations, the function $p(t)$ that minimizes this functional must satisfy the Euler-Lagrange equation. For a functional with derivatives up to the $n$-th order, this equation is:\n$$\n\\frac{\\partial L}{\\partial p} - \\frac{d}{dt}\\frac{\\partial L}{\\partial \\dot{p}} + \\frac{d^2}{dt^2}\\frac{\\partial L}{\\partial \\ddot{p}} - \\dots + (-1)^n \\frac{d^n}{dt^n}\\frac{\\partial L}{\\partial p^{(n)}} = 0\n$$\nIn our case, $n=3$. The partial derivatives of $L$ with respect to $p$, $\\dot{p}$, and $\\ddot{p}$ are all zero. The only non-zero contribution comes from the term involving $\\dddot{p}$:\n$$\n\\frac{\\partial L}{\\partial \\dddot{p}} = 2\\dddot{p}\n$$\nThe Euler-Lagrange equation thus simplifies to:\n$$\n(-1)^3 \\frac{d^3}{dt^3} \\left( \\frac{\\partial L}{\\partial \\dddot{p}} \\right) = 0 \\implies -\\frac{d^3}{dt^3} (2\\dddot{p}) = 0 \\implies \\frac{d^6 p}{dt^6} = 0\n$$\nThe general solution to this $6$th-order ordinary differential equation is a $5$th-order polynomial, as given in the problem statement:\n$$\np(t) = a_5 t^5 + a_4 t^4 + a_3 t^3 + a_2 t^2 + a_1 t + a_0\n$$\nThe coefficients $a_k$ are determined by the six boundary conditions. We will now solve for these coefficients for $x(t)$ and $y(t)$ separately.\n\n**Derivation for $x$-coordinate coefficients:**\nThe trajectory is $x(t) = a_{5}^{x} t^5 + a_{4}^{x} t^4 + a_{3}^{x} t^3 + a_{2}^{x} t^2 + a_{1}^{x} t + a_{0}^{x}$.\nThe derivatives are:\n$\\dot{x}(t) = 5 a_{5}^{x} t^4 + 4 a_{4}^{x} t^3 + 3 a_{3}^{x} t^2 + 2 a_{2}^{x} t + a_{1}^{x}$\n$\\ddot{x}(t) = 20 a_{5}^{x} t^3 + 12 a_{4}^{x} t^2 + 6 a_{3}^{x} t + 2 a_{2}^{x}$\n\nThe boundary conditions are:\n$x(0)=0.0$, $x(T)=0.25$\n$\\dot{x}(0)=0$, $\\dot{x}(T)=0$\n$\\ddot{x}(0)=0$, $\\ddot{x}(T)=0$\nwhere $T=0.8$.\n\nApplying the conditions at $t=0$:\n$x(0) = a_{0}^{x} = 0$\n$\\dot{x}(0) = a_{1}^{x} = 0$\n$\\ddot{x}(0) = 2a_{2}^{x} = 0 \\implies a_{2}^{x} = 0$\n\nThus, the first three coefficients ($a_{0}^{x}, a_{1}^{x}, a_{2}^{x}$) are zero. The polynomial reduces to $x(t) = a_{5}^{x} t^5 + a_{4}^{x} t^4 + a_{3}^{x} t^3$.\nNow, applying the conditions at $t=T$:\n1. $x(T) = a_{5}^{x} T^5 + a_{4}^{x} T^4 + a_{3}^{x} T^3 = 0.25$\n2. $\\dot{x}(T) = 5 a_{5}^{x} T^4 + 4 a_{4}^{x} T^3 + 3 a_{3}^{x} T^2 = 0$\n3. $\\ddot{x}(T) = 20 a_{5}^{x} T^3 + 12 a_{4}^{x} T^2 + 6 a_{3}^{x} T = 0$\n\nThis is a system of three linear equations for $a_{3}^{x}, a_{4}^{x}, a_{5}^{x}$. We can simplify by dividing by powers of $T \\neq 0$:\n1. $a_{5}^{x} T^2 + a_{4}^{x} T + a_{3}^{x} = \\frac{0.25}{T^3}$\n2. $5 a_{5}^{x} T^2 + 4 a_{4}^{x} T + 3 a_{3}^{x} = 0$\n3. $10 a_{5}^{x} T^2 + 6 a_{4}^{x} T + 3 a_{3}^{x} = 0$\n\nSolving this system symbolically for a general displacement $\\Delta x = x(T) - x(0)$ yields:\n$a_{3}^{x} = \\frac{10 \\Delta x}{T^3}$\n$a_{4}^{x} = -\\frac{15 \\Delta x}{T^4}$\n$a_{5}^{x} = \\frac{6 \\Delta x}{T^5}$\n\nSubstituting $\\Delta x = 0.25$ and $T=0.8$:\n$a_{3}^{x} = \\frac{10(0.25)}{(0.8)^3} = \\frac{2.5}{0.512} = 4.8828125$\n$a_{4}^{x} = -\\frac{15(0.25)}{(0.8)^4} = -\\frac{3.75}{0.4096} = -9.1552734375$\n$a_{5}^{x} = \\frac{6(0.25)}{(0.8)^5} = \\frac{1.5}{0.32768} = 4.57763671875$\n\n**Derivation for $y$-coordinate coefficients:**\nThe procedure is identical. The boundary conditions are:\n$y(0)=0.0$, $y(T)=0.15$\n$\\dot{y}(0)=0$, $\\dot{y}(T)=0$\n$\\ddot{y}(0)=0$, $\\ddot{y}(T)=0$\n\nThe coefficients $a_{0}^{y}, a_{1}^{y}, a_{2}^{y}$ are zero for the same reasons as for the $x$-coordinate. For the remaining coefficients, we use the same symbolic solutions with $\\Delta y = y(T) - y(0) = 0.15$:\n$a_{3}^{y} = \\frac{10 \\Delta y}{T^3} = \\frac{10(0.15)}{(0.8)^3} = \\frac{1.5}{0.512} = 2.9296875$\n$a_{4}^{y} = -\\frac{15 \\Delta y}{T^4} = -\\frac{15(0.15)}{(0.8)^4} = -\\frac{2.25}{0.4096} = -5.4931640625$\n$a_{5}^{y} = \\frac{6 \\Delta y}{T^5} = \\frac{6(0.15)}{(0.8)^5} = \\frac{0.9}{0.32768} = 2.74658203125$\n\nFinally, we round the nonzero coefficients to eight significant figures as specified.\nFor $x(t)$:\n$a_{0}^{x} = 0$\n$a_{1}^{x} = 0$\n$a_{2}^{x} = 0$\n$a_{3}^{x} = 4.8828125$\n$a_{4}^{x} = -9.1552734375 \\approx -9.1552734$\n$a_{5}^{x} = 4.57763671875 \\approx 4.5776367$\n\nFor $y(t)$:\n$a_{0}^{y} = 0$\n$a_{1}^{y} = 0$\n$a_{2}^{y} = 0$\n$a_{3}^{y} = 2.9296875$\n$a_{4}^{y} = -5.4931640625 \\approx -5.4931641$\n$a_{5}^{y} = 2.74658203125 \\approx 2.7465820$\n\nThe final set of coefficients is assembled into a single row vector in the specified order $(a_{0}^{x}, \\dots, a_{5}^{x}, a_{0}^{y}, \\dots, a_{5}^{y})$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0 & 0 & 0 & 4.8828125 & -9.1552734 & 4.5776367 & 0 & 0 & 0 & 2.9296875 & -5.4931641 & 2.7465820\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Planning a trajectory is only half the battle; the nervous system must also generate motor commands to execute the plan and correct for any deviations. This practice introduces the powerful Linear-Quadratic Regulator (LQR) framework to model this process as an optimal feedback control problem. By solving the algebraic Riccati equation for a simplified limb model , you will learn how the CNS can derive a control policy that optimally balances the trade-off between minimizing movement error and conserving metabolic energy.",
            "id": "4192635",
            "problem": "In movement biomechanics, a common abstraction of endpoint control for limb movement treats the limb endpoint position as a point-mass whose kinematics are governed by Newton’s laws, with the central nervous system selecting control inputs to achieve stable, smooth movements while trading off accuracy and effort. Consider a simplified, dimensionless model for endpoint motor control where the state is $x(t)$ (position error) and $v(t)$ (velocity error), and the control input $u(t)$ represents a normalized neural drive that directly sets acceleration. The dynamics are the double-integrator\n$$\n\\dot{x}(t) = v(t), \\quad \\dot{v}(t) = u(t),\n$$\nwhich can be written in linear state-space form as $\\dot{\\mathbf{z}}(t) = A \\mathbf{z}(t) + B u(t)$, with $\\mathbf{z}(t) = \\begin{pmatrix} x(t) \\\\ v(t) \\end{pmatrix}$, $A = \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix}$, and $B = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$. Assume the central nervous system selects the control $u(t)$ to minimize the infinite-horizon quadratic performance index\n$$\nJ = \\int_{0}^{\\infty} \\left(x(t)^{2} + v(t)^{2} + 0.1\\,u(t)^{2}\\right) \\, dt,\n$$\nwhich encodes a trade-off between state regulation (movement accuracy and smoothness) and neural effort. The weighting matrices for this Linear Quadratic Regulator (LQR) problem are $Q = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$ and $R = 0.1$. All quantities are dimensionless.\n\nStarting from first principles of optimal control (dynamic programming via the stationary Hamilton–Jacobi–Bellman equation) applied to this linear-quadratic setting, derive the algebraic Riccati equation for the symmetric matrix $P \\in \\mathbb{R}^{2 \\times 2}$ that characterizes the optimal value function, determine the unique stabilizing solution $P$, and then compute the corresponding optimal state feedback gain $K \\in \\mathbb{R}^{1 \\times 2}$ for the law $u(t) = -K \\begin{pmatrix} x(t) \\\\ v(t) \\end{pmatrix}$. Provide $K$ in exact symbolic form; no numerical rounding is required. If you opt to present a numerical approximation, round the components of $K$ to four significant figures and state that you did so. Express the final $K$ without units (the model is dimensionless).",
            "solution": "The problem is a standard infinite-horizon, continuous-time Linear Quadratic Regulator (LQR) problem. We are given a linear time-invariant system and a quadratic cost functional to minimize. The task is to find the optimal state feedback gain matrix $K$. The solution proceeds from first principles using the Hamilton-Jacobi-Bellman (HJB) equation.\n\nThe system dynamics are given by $\\dot{\\mathbf{z}}(t) = A \\mathbf{z}(t) + B u(t)$, where the state is $\\mathbf{z}(t) = \\begin{pmatrix} x(t) \\\\ v(t) \\end{pmatrix}$, and the matrices are:\n$$\nA = \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix}, \\quad B = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n$$\nThe performance index to be minimized is:\n$$\nJ = \\int_{0}^{\\infty} \\left(x(t)^{2} + v(t)^{2} + 0.1\\,u(t)^{2}\\right) \\, dt = \\int_{0}^{\\infty} \\left(\\mathbf{z}(t)^T Q \\mathbf{z}(t) + u(t)^T R u(t)\\right) \\, dt\n$$\nwhere the weighting matrices are:\n$$\nQ = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}, \\quad R = 0.1\n$$\nThe goal is to find an optimal control law of the form $u(t) = -K\\mathbf{z}(t)$.\n\nFor an infinite-horizon LQR problem, the optimal value function, which represents the minimum cost-to-go from state $\\mathbf{z}$, is quadratic in the state:\n$$\nV(\\mathbf{z}) = \\mathbf{z}^T P \\mathbf{z}\n$$\nwhere $P \\in \\mathbb{R}^{2 \\times 2}$ is a constant, symmetric, positive-definite matrix.\n\nThe stationary Hamilton-Jacobi-Bellman (HJB) equation provides a necessary and sufficient condition for optimality:\n$$\n\\min_{u} \\left\\{ \\mathbf{z}^T Q \\mathbf{z} + u^T R u + \\left(\\frac{\\partial V}{\\partial \\mathbf{z}}\\right)^T \\dot{\\mathbf{z}} \\right\\} = 0\n$$\nThe gradient of the value function is $\\frac{\\partial V}{\\partial \\mathbf{z}} = 2P\\mathbf{z}$. Substituting this and the state dynamics $\\dot{\\mathbf{z}} = A\\mathbf{z} + Bu$ into the HJB equation yields:\n$$\n\\min_{u} \\left\\{ \\mathbf{z}^T Q \\mathbf{z} + u^T R u + (2P\\mathbf{z})^T (A\\mathbf{z} + Bu) \\right\\} = 0\n$$\n$$\n\\min_{u} \\left\\{ \\mathbf{z}^T Q \\mathbf{z} + u^T R u + 2\\mathbf{z}^T P A \\mathbf{z} + 2\\mathbf{z}^T P B u \\right\\} = 0\n$$\nTo find the control $u$ that minimizes the expression in the braces (the Hamiltonian), we take its derivative with respect to $u$ and set it to zero. Since $R$ is a scalar, we have:\n$$\n\\frac{\\partial}{\\partial u} \\left( \\dots \\right) = 2Ru + 2B^T P \\mathbf{z} = 0\n$$\nSolving for the optimal control $u^*(t)$:\n$$\nu^*(t) = -R^{-1} B^T P \\mathbf{z}(t)\n$$\nThis gives the optimal state feedback law, where the gain matrix is $K = R^{-1} B^T P$.\n\nSubstituting $u^*$ back into the HJB equation:\n$$\n\\mathbf{z}^T Q \\mathbf{z} + (-R^{-1} B^T P \\mathbf{z})^T R (-R^{-1} B^T P \\mathbf{z}) + 2\\mathbf{z}^T P A \\mathbf{z} + 2\\mathbf{z}^T P B (-R^{-1} B^T P \\mathbf{z}) = 0\n$$\n$$\n\\mathbf{z}^T Q \\mathbf{z} + \\mathbf{z}^T P B R^{-1} R R^{-1} B^T P \\mathbf{z} + 2\\mathbf{z}^T P A \\mathbf{z} - 2\\mathbf{z}^T P B R^{-1} B^T P \\mathbf{z} = 0\n$$\nSimplifying, we get:\n$$\n\\mathbf{z}^T Q \\mathbf{z} + 2\\mathbf{z}^T P A \\mathbf{z} - \\mathbf{z}^T P B R^{-1} B^T P \\mathbf{z} = 0\n$$\nSince $P$ is symmetric ($P=P^T$), we can write $2\\mathbf{z}^T P A \\mathbf{z} = \\mathbf{z}^T P A \\mathbf{z} + (\\mathbf{z}^T P A \\mathbf{z})^T = \\mathbf{z}^T P A \\mathbf{z} + \\mathbf{z}^T A^T P \\mathbf{z} = \\mathbf{z}^T(A^T P + PA)\\mathbf{z}$. The equation becomes:\n$$\n\\mathbf{z}^T \\left( A^T P + PA - P B R^{-1} B^T P + Q \\right) \\mathbf{z} = 0\n$$\nThis equation must hold for any state $\\mathbf{z}$. This is only possible if the matrix expression in the parentheses is the zero matrix. This leads to the continuous-time Algebraic Riccati Equation (ARE):\n$$\nA^T P + PA - P B R^{-1} B^T P + Q = 0\n$$\nWe now solve this equation for $P$. Let $P = \\begin{pmatrix} p_{11} & p_{12} \\\\ p_{12} & p_{22} \\end{pmatrix}$. We have $R^{-1} = 1/0.1 = 10$.\nThe terms in the ARE are:\n$$\nA^T P = \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix} \\begin{pmatrix} p_{11} & p_{12} \\\\ p_{12} & p_{22} \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\ p_{11} & p_{12} \\end{pmatrix}\n$$\n$$\nPA = \\begin{pmatrix} p_{11} & p_{12} \\\\ p_{12} & p_{22} \\end{pmatrix} \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 0 & p_{11} \\\\ 0 & p_{12} \\end{pmatrix}\n$$\n$$\nP B R^{-1} B^T P = P \\left( \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} (10) \\begin{pmatrix} 0 & 1 \\end{pmatrix} \\right) P = P \\begin{pmatrix} 0 & 0 \\\\ 0 & 10 \\end{pmatrix} P\n$$\n$$\n= \\begin{pmatrix} p_{11} & p_{12} \\\\ p_{12} & p_{22} \\end{pmatrix} \\begin{pmatrix} 0 & 0 \\\\ 0 & 10 \\end{pmatrix} \\begin{pmatrix} p_{11} & p_{12} \\\\ p_{12} & p_{22} \\end{pmatrix} = \\begin{pmatrix} 0 & 10p_{12} \\\\ 0 & 10p_{22} \\end{pmatrix} \\begin{pmatrix} p_{11} & p_{12} \\\\ p_{12} & p_{22} \\end{pmatrix} = \\begin{pmatrix} 10p_{12}^2 & 10p_{12}p_{22} \\\\ 10p_{12}p_{22} & 10p_{22}^2 \\end{pmatrix}\n$$\nSubstituting these into the ARE:\n$$\n\\begin{pmatrix} 0 & p_{11} \\\\ p_{11} & 2p_{12} \\end{pmatrix} - \\begin{pmatrix} 10p_{12}^2 & 10p_{12}p_{22} \\\\ 10p_{12}p_{22} & 10p_{22}^2 \\end{pmatrix} + \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix}\n$$\nThis matrix equation yields a system of three scalar algebraic equations:\n1. $1 - 10p_{12}^2 = 0$\n2. $p_{11} - 10p_{12}p_{22} = 0$\n3. $1 + 2p_{12} - 10p_{22}^2 = 0$\n\nFrom equation (1), $p_{12}^2 = 1/10$, so $p_{12} = \\pm 1/\\sqrt{10}$.\nThe closed-loop system dynamics are $\\dot{\\mathbf{z}} = (A-BK)\\mathbf{z}$. The matrix $A-BK$ must be Hurwitz (all eigenvalues have negative real parts) for stability.\n$K = R^{-1} B^T P = 10 \\begin{pmatrix} 0 & 1 \\end{pmatrix} \\begin{pmatrix} p_{11} & p_{12} \\\\ p_{12} & p_{22} \\end{pmatrix} = \\begin{pmatrix} 10p_{12} & 10p_{22} \\end{pmatrix} = \\begin{pmatrix} k_1 & k_2 \\end{pmatrix}$.\nThe closed-loop system matrix is:\n$$\nA-BK = \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} k_1 & k_2 \\end{pmatrix} = \\begin{pmatrix} 0 & 1 \\\\ -k_1 & -k_2 \\end{pmatrix}\n$$\nThe characteristic polynomial is $\\lambda^2 + k_2\\lambda + k_1 = 0$. For stability, the Routh-Hurwitz conditions require all coefficients to be positive, i.e., $k_1 > 0$ and $k_2 > 0$.\nSince $k_1 = 10p_{12}$ and $k_2 = 10p_{22}$, we must have $p_{12} > 0$ and $p_{22} > 0$.\nTherefore, we must choose the positive root for $p_{12}$:\n$$\np_{12} = \\frac{1}{\\sqrt{10}} = \\frac{\\sqrt{10}}{10}\n$$\nNow, substitute this into equation (3) to find $p_{22}$:\n$$\n1 + 2\\left(\\frac{1}{\\sqrt{10}}\\right) - 10p_{22}^2 = 0 \\implies 10p_{22}^2 = 1 + \\frac{2}{\\sqrt{10}} = 1 + \\frac{2\\sqrt{10}}{10} = \\frac{10+2\\sqrt{10}}{10}\n$$\n$$\np_{22}^2 = \\frac{10+2\\sqrt{10}}{100}\n$$\nSince we need $p_{22} > 0$, we take the positive square root:\n$$\np_{22} = \\sqrt{\\frac{10+2\\sqrt{10}}{100}} = \\frac{\\sqrt{10+2\\sqrt{10}}}{10}\n$$\nFinally, we find $p_{11}$ from equation (2):\n$$\np_{11} = 10p_{12}p_{22} = 10 \\left(\\frac{\\sqrt{10}}{10}\\right) \\left(\\frac{\\sqrt{10+2\\sqrt{10}}}{10}\\right) = \\frac{\\sqrt{10}\\sqrt{10+2\\sqrt{10}}}{10} = \\frac{\\sqrt{100+20\\sqrt{10}}}{10}\n$$\nHaving found the components of the stabilizing solution $P$, we can compute the optimal feedback gain $K$:\n$$\nK = \\begin{pmatrix} k_1 & k_2 \\end{pmatrix} = \\begin{pmatrix} 10p_{12} & 10p_{22} \\end{pmatrix}\n$$\n$$\nk_1 = 10p_{12} = 10 \\left(\\frac{\\sqrt{10}}{10}\\right) = \\sqrt{10}\n$$\n$$\nk_2 = 10p_{22} = 10 \\left(\\frac{\\sqrt{10+2\\sqrt{10}}}{10}\\right) = \\sqrt{10+2\\sqrt{10}}\n$$\nSo, the optimal feedback gain matrix is $K = \\begin{pmatrix} \\sqrt{10} & \\sqrt{10+2\\sqrt{10}} \\end{pmatrix}$.\nThe optimal control law is $u(t) = -k_1 x(t) - k_2 v(t) = -\\sqrt{10} x(t) - \\sqrt{10+2\\sqrt{10}} v(t)$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\sqrt{10} & \\sqrt{10+2\\sqrt{10}} \\end{pmatrix}}\n$$"
        }
    ]
}