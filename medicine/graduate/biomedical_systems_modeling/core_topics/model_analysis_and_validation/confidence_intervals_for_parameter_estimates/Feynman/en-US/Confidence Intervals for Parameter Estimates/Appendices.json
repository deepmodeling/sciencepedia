{
    "hands_on_practices": [
        {
            "introduction": "Fitting dynamic models, such as those in pharmacokinetics, to experimental data is a cornerstone of biomedical systems modeling. However, obtaining a point estimate for a parameter like an elimination rate is only half the story; we must also quantify our uncertainty. This practice  guides you through the fundamental mechanics of constructing a confidence interval for a parameter in a nonlinear model, emphasizing the crucial role of the Student's $t$-distribution when measurement error is estimated from the data.",
            "id": "3878488",
            "problem": "A pharmacokinetic one-compartment intravenous bolus model describes plasma concentration dynamics by the Ordinary Differential Equation (ODE) $dC/dt = -k\\,C$, where $k$ is the first-order elimination rate constant and $C(t)$ is the plasma concentration at time $t$. The analytic solution is $C(t) = C_{0}\\,\\exp(-k\\,t)$, where $C_{0}$ is the initial concentration immediately after bolus administration. Suppose $n$ distinct plasma concentration observations $\\{y_{i}\\}_{i=1}^{n}$ at times $\\{t_{i}\\}_{i=1}^{n}$ are modeled by $y_{i} = C_{0}\\,\\exp(-k\\,t_{i}) + \\varepsilon_{i}$, with independent and identically distributed measurement errors $\\varepsilon_{i} \\sim \\mathcal{N}(0,\\sigma^{2})$. You fit the nonlinear least squares estimator by minimizing the Sum of Squared Errors (SSE) $S(\\theta) = \\sum_{i=1}^{n} \\left(y_{i} - C_{0}\\,\\exp(-k\\,t_{i})\\right)^{2}$ over $\\theta = (k, C_{0})^{\\top}$ and obtain the Gauss–Newton solution $\\hat{\\theta} = (\\hat{k}, \\hat{C}_{0})^{\\top}$.\n\nAt the solution, the Jacobian matrix $J(\\hat{\\theta})$ of partial derivatives of the model predictions with respect to parameters has $p=2$ columns. Under the Gauss–Newton approximation, local linearization is used to characterize the uncertainty in $\\hat{\\theta}$. Let $n=30$, and suppose the following quantities are computed at $\\hat{\\theta}$:\n- The Sum of Squared Errors (SSE) is $S(\\hat{\\theta}) = 0.84$.\n- The $(1,1)$ element of $\\left(J(\\hat{\\theta})^{\\top}J(\\hat{\\theta})\\right)^{-1}$ is $\\left[\\left(J(\\hat{\\theta})^{\\top}J(\\hat{\\theta})\\right)^{-1}\\right]_{11} = 2.0 \\times 10^{-3}$.\n- The estimated elimination rate constant is $\\hat{k} = 0.35$ $\\mathrm{h}^{-1}$.\n\nStarting from the assumptions of independent Gaussian errors, the nonlinear least squares setup, and the Gauss–Newton local linearization, derive a two-sided confidence interval for $k$ at confidence level $0.95$ that justifies the use of the Student’s $t$ distribution with the appropriate Degrees of Freedom (DoF). Explicitly explain the origin of the degrees of freedom and why the $t$ distribution (rather than the normal distribution) is used.\n\nThen, compute the numerical endpoints of the derived confidence interval for $k$ using the provided quantities. Round your endpoints to four significant figures. Express the final endpoints in $\\mathrm{h}^{-1}$.",
            "solution": "The problem is first validated by extracting the given information and assessing its scientific validity and completeness.\n\n### Step 1: Extract Givens\n- Model ODE: $dC/dt = -k\\,C$\n- Analytic solution: $C(t) = C_{0}\\,\\exp(-k\\,t)$\n- Statistical model for observations: $y_{i} = C_{0}\\,\\exp(-k\\,t_{i}) + \\varepsilon_{i}$ for $i=1, \\dots, n$\n- Error distribution: $\\varepsilon_{i} \\sim \\mathcal{N}(0,\\sigma^{2})$, independent and identically distributed.\n- Parameter vector: $\\theta = (k, C_{0})^{\\top}$\n- Number of parameters: $p=2$\n- Objective function for nonlinear least squares: $S(\\theta) = \\sum_{i=1}^{n} \\left(y_{i} - C_{0}\\,\\exp(-k\\,t_{i})\\right)^{2}$\n- Estimator: $\\hat{\\theta} = (\\hat{k}, \\hat{C}_{0})^{\\top}$, the minimizer of $S(\\theta)$.\n- Number of observations: $n=30$\n- Sum of Squared Errors at the solution: $S(\\hat{\\theta}) = 0.84$\n- Component of the inverse approximate Hessian matrix: $\\left[\\left(J(\\hat{\\theta})^{\\top}J(\\hat{\\theta})\\right)^{-1}\\right]_{11} = 2.0 \\times 10^{-3}$\n- Estimated elimination rate constant: $\\hat{k} = 0.35 \\, \\mathrm{h}^{-1}$\n- Desired confidence level: $0.95$\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem describes a standard one-compartment pharmacokinetic model with intravenous bolus administration. The governing ODE, its solution, and the nonlinear least squares estimation framework with Gaussian error assumptions are fundamental and correct concepts in biomedical systems modeling and statistical parameter estimation. The use of the Gauss-Newton method and the resulting approximation for the parameter covariance matrix are standard in nonlinear regression analysis. All premises are scientifically sound.\n- **Well-Posed**: The problem is well-posed. It asks for the derivation and computation of a confidence interval for a specific parameter, $k$. All necessary numerical values ($n$, $p$, $S(\\hat{\\theta})$, $\\hat{k}$, and the relevant matrix element for the standard error calculation) and the confidence level are provided. A unique solution exists.\n- **Objective**: The problem is stated in precise, objective, and technical language, free from ambiguity or subjective content.\n- **Completeness**: The problem is self-contained and provides sufficient information for a solution.\n- **Other Flaws**: The problem is not trivial, tautological, or ill-posed. It represents a standard but conceptually important application of statistical theory in a scientific context.\n\n### Step 3: Verdict and Action\nThe problem is valid. A detailed solution will be provided.\n\n### Solution Derivation and Calculation\n\nThe goal is to derive and compute a $95\\%$ confidence interval for the parameter $k$. In nonlinear least squares, under the assumption that the errors $\\varepsilon_{i}$ are independent and identically distributed as $\\mathcal{N}(0,\\sigma^{2})$, the parameter estimator $\\hat{\\theta}$ is approximately normally distributed for large $n$, with mean equal to the true parameter vector $\\theta$ and a covariance matrix given by $\\text{Cov}(\\hat{\\theta}) \\approx \\sigma^{2} \\left(J(\\theta)^{\\top}J(\\theta)\\right)^{-1}$. The Jacobian matrix $J(\\theta)$ is an $n \\times p$ matrix of partial derivatives of the model function with respect to the parameters, evaluated at the true parameter values. In practice, we evaluate $J$ at the estimate $\\hat{\\theta}$.\n\nThe variance of the estimate for a single parameter, $\\hat{k}$, is the first diagonal element of this covariance matrix:\n$$ \\text{Var}(\\hat{k}) \\approx \\sigma^{2} \\left[\\left(J(\\hat{\\theta})^{\\top}J(\\hat{\\theta})\\right)^{-1}\\right]_{11} $$\nThe true error variance $\\sigma^{2}$ is generally unknown and must be estimated from the data. The unbiased estimator for $\\sigma^{2}$ is the mean squared error (MSE), denoted as $s^{2}$:\n$$ s^{2} = \\frac{S(\\hat{\\theta})}{n-p} = \\frac{\\text{SSE}}{\\text{DoF}} $$\nThe denominator, $n-p$, represents the degrees of freedom (DoF) for the error. This is because we start with $n$ independent data points, but we use them to estimate $p$ parameters. The estimation of these $p$ parameters imposes $p$ constraints on the residuals, leaving $n-p$ degrees of freedom for the estimation of the error variance. In this problem, $n=30$ and $p=2$ (for $k$ and $C_0$), so the degrees of freedom are $30 - 2 = 28$.\n\nSubstituting the estimate $s^{2}$ for the true variance $\\sigma^{2}$, we obtain the estimated variance of $\\hat{k}$:\n$$ \\widehat{\\text{Var}}(\\hat{k}) = s^{2} \\left[\\left(J(\\hat{\\theta})^{\\top}J(\\hat{\\theta})\\right)^{-1}\\right]_{11} $$\nThe standard error of the estimate $\\hat{k}$ is the square root of this estimated variance:\n$$ \\text{se}(\\hat{k}) = \\sqrt{\\widehat{\\text{Var}}(\\hat{k})} = \\sqrt{s^{2} \\left[\\left(J(\\hat{\\theta})^{\\top}J(\\hat{\\theta})\\right)^{-1}\\right]_{11}} $$\nNow, we justify the use of the Student's $t$-distribution. Consider the pivotal quantity:\n$$ T = \\frac{\\hat{k} - k}{\\text{se}(\\hat{k})} $$\nIf the true variance $\\sigma^{2}$ were known, the quantity $(\\hat{k}-k)/\\sqrt{\\text{Var}(\\hat{k})}$ would be approximately a standard normal variable, $Z \\sim \\mathcal{N}(0,1)$. However, because we use the estimate $s^2$, we introduce additional uncertainty. The statistical theory states that the quantity $\\frac{(n-p)s^{2}}{\\sigma^{2}}$ follows a chi-squared ($\\chi^{2}$) distribution with $n-p$ degrees of freedom.\n\nLet's rewrite the pivotal quantity $T$:\n$$ T = \\frac{\\hat{k} - k}{\\sqrt{s^{2} \\left[\\left(J^{\\top}J\\right)^{-1}\\right]_{11}}} = \\frac{\\frac{\\hat{k} - k}{\\sigma\\sqrt{\\left[\\left(J^{\\top}J\\right)^{-1}\\right]_{11}}}}{\\sqrt{\\frac{s^{2}}{\\sigma^{2}}}} = \\frac{Z}{\\sqrt{\\frac{(n-p)s^{2}/\\sigma^{2}}{n-p}}} $$\nThe numerator $Z$ is (approximately) a standard normal random variable. The term in the square root in the denominator is a $\\chi^{2}$-distributed random variable with $n-p$ degrees of freedom, divided by its degrees of freedom. By definition, a random variable formed by the ratio of a standard normal variable to the square root of an independent $\\chi^{2}$ variable divided by its degrees of freedom follows a Student's $t$-distribution with those same degrees of freedom.\nTherefore, $T = \\frac{\\hat{k} - k}{\\text{se}(\\hat{k})}$ follows a $t$-distribution with $n-p = 28$ degrees of freedom. This is why the $t$-distribution is used instead of the normal distribution: it correctly accounts for the additional uncertainty introduced by estimating $\\sigma^{2}$ from the data.\n\nA two-sided confidence interval for $k$ at confidence level $1-\\alpha$ is constructed as:\n$$ \\hat{k} \\pm t_{\\alpha/2, n-p} \\times \\text{se}(\\hat{k}) $$\nwhere $t_{\\alpha/2, n-p}$ is the critical value from the $t$-distribution that leaves an area of $\\alpha/2$ in the upper tail. For a $95\\%$ confidence level, $1-\\alpha = 0.95$, so $\\alpha = 0.05$ and $\\alpha/2 = 0.025$.\n\nNow we compute the numerical values.\n1.  Degrees of Freedom (DoF): $n-p = 30-2 = 28$.\n2.  Estimated error variance: $s^{2} = \\frac{S(\\hat{\\theta})}{n-p} = \\frac{0.84}{28} = 0.03$.\n3.  Standard error of $\\hat{k}$:\n    $$ \\text{se}(\\hat{k}) = \\sqrt{s^{2} \\left[\\left(J(\\hat{\\theta})^{\\top}J(\\hat{\\theta})\\right)^{-1}\\right]_{11}} = \\sqrt{0.03 \\times (2.0 \\times 10^{-3})} = \\sqrt{6.0 \\times 10^{-5}} $$\n    $$ \\text{se}(\\hat{k}) \\approx 0.0077459667 $$\n4.  Critical $t$-value: For a $95\\%$ confidence interval and $28$ degrees of freedom, the critical value is $t_{0.025, 28}$. From standard statistical tables or software, $t_{0.025, 28} = 2.0484$.\n5.  Margin of Error (ME):\n    $$ \\text{ME} = t_{0.025, 28} \\times \\text{se}(\\hat{k}) = 2.0484 \\times 0.0077459667 \\approx 0.0158673 $$\n6.  Confidence Interval (CI):\n    $$ \\text{CI} = \\hat{k} \\pm \\text{ME} = 0.35 \\pm 0.0158673 $$\n    -   Lower bound: $0.35 - 0.0158673 = 0.3341327$\n    -   Upper bound: $0.35 + 0.0158673 = 0.3658673$\n\nRounding the endpoints to four significant figures as requested:\n-   Lower bound: $0.3341$\n-   Upper bound: $0.3659$\n\nThe $95\\%$ confidence interval for the elimination rate constant $k$ is $(0.3341, 0.3659) \\, \\mathrm{h}^{-1}$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.3341 & 0.3659\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "While the Wald interval provides a straightforward method for uncertainty quantification, its performance can be sensitive to the specific parameterization of the model. This exercise  explores this critical concept by comparing two confidence intervals for an exponential rate constant: one constructed directly and another constructed on a logarithmic scale and transformed back. This comparison reveals the lack of invariance in Wald intervals and illustrates how a thoughtful reparameterization can yield more statistically sound and physically meaningful results.",
            "id": "3878457",
            "problem": "A biomedical systems model for receptor internalization posits that the waiting times for endocytosis events are independent and identically distributed exponential random variables with rate constant $k$ (in $\\text{min}^{-1}$). Specifically, the observed times $\\{t_{i}\\}_{i=1}^{n}$ are modeled as independent samples from a distribution with probability density function $f(t \\mid k) = k \\exp(-k t)$ for $t \\ge 0$. The rate constant $k$ quantifies the average frequency of these events and is positive.\n\nStarting from the fundamental definitions of likelihood, Maximum Likelihood Estimator (MLE), and Fisher information, proceed as follows:\n- Derive the MLE $\\hat{k}$ for $k$ and the total Fisher information $I(k)$ in $k$ under the exponential model.\n- Using the asymptotic normality of the MLE and Fisher information, construct a two-sided Wald interval for $k$ at nominal confidence level $1 - \\alpha$ (take $\\alpha = 0.05$ and use the standard normal quantile $z_{0.975} \\approx 1.96$).\n- Consider the reparameterization $\\phi = g(k) = \\ln(k)$. Derive the Fisher information $I_{\\phi}(\\phi)$ for $\\phi$ via the chain rule for scores, and construct the corresponding two-sided Wald interval for $\\phi$ at the same confidence level. Then map this interval back to the original parameter $k$ via the inverse transformation $k = \\exp(\\phi)$ to obtain a transformed-back interval for $k$.\n- Explain how reparameterization affects Wald intervals and discuss whether Wald intervals are invariant under nonlinear transformations, using the above derivations to support your reasoning.\n\nFor the concrete dataset with $n = 40$ and $\\sum_{i=1}^{n} t_{i} = S = 200$ minutes, compute numerically:\n1. The endpoints of the two-sided $95\\%$ Wald interval for $k$ constructed directly on $k$.\n2. The endpoints of the two-sided $95\\%$ Wald interval for $\\phi$ mapped back to $k$ via $k = \\exp(\\phi)$.\n\nRound all four endpoints to four significant figures. Express your final numeric endpoints for $k$ in $\\text{min}^{-1}$. Report the four numbers in a single row matrix in the order $\\big[$lower endpoint for $k$ (direct), upper endpoint for $k$ (direct), lower endpoint for $k$ (transformed-back), upper endpoint for $k$ (transformed-back)$\\big]$.",
            "solution": "The problem provides a complete, self-contained, and scientifically grounded scenario in statistical inference. All definitions and data required for a solution are present, and the problem adheres to established principles of maximum likelihood estimation and confidence interval construction. The problem is well-posed and objective. Therefore, the problem is valid.\n\nWe begin by deriving the Maximum Likelihood Estimator (MLE) and Fisher information for the rate constant $k$. The likelihood function for $n$ independent and identically distributed observations $\\{t_{i}\\}_{i=1}^{n}$ from an exponential distribution with rate $k$ is given by:\n$$L(k \\mid \\{t_{i}\\}) = \\prod_{i=1}^{n} f(t_i \\mid k) = \\prod_{i=1}^{n} k \\exp(-k t_i) = k^n \\exp\\left(-k \\sum_{i=1}^{n} t_i\\right)$$\nThe log-likelihood function, $\\ell(k)$, is:\n$$\\ell(k) = \\ln L(k \\mid \\{t_{i}\\}) = n \\ln(k) - k \\sum_{i=1}^{n} t_i$$\nTo find the MLE $\\hat{k}$, we compute the score function (the first derivative of $\\ell(k)$ with respect to $k$) and set it to zero:\n$$\\frac{d\\ell}{dk} = \\frac{n}{k} - \\sum_{i=1}^{n} t_i = 0$$\nSolving for $k$ gives the MLE:\n$$\\hat{k} = \\frac{n}{\\sum_{i=1}^{n} t_i} = \\frac{1}{\\bar{t}}$$\nwhere $\\bar{t}$ is the sample mean of the observed times.\n\nNext, we derive the Fisher information. The second derivative of the log-likelihood is:\n$$\\frac{d^2\\ell}{dk^2} = -\\frac{n}{k^2}$$\nThe Fisher information for the entire sample of size $n$, $I(k)$, is the negative expectation of the second derivative:\n$$I(k) = -E\\left[\\frac{d^2\\ell}{dk^2}\\right] = -E\\left[-\\frac{n}{k^2}\\right] = \\frac{n}{k^2}$$\nNote that since the second derivative does not depend on the data $\\{t_i\\}$, the expectation is simply the quantity itself. This is the observed Fisher information as well as the expected Fisher information.\n\nNow, we construct the two-sided Wald interval for $k$. The Wald interval is based on the asymptotic normality of the MLE, which states that for large $n$, $\\hat{k}$ is approximately normally distributed with mean $k$ and variance $I(k)^{-1}$. The variance is estimated by substituting $\\hat{k}$ for $k$, giving an estimated standard error, $\\text{SE}(\\hat{k})$:\n$$\\text{SE}(\\hat{k}) = \\sqrt{I(\\hat{k})^{-1}} = \\sqrt{\\frac{\\hat{k}^2}{n}} = \\frac{\\hat{k}}{\\sqrt{n}}$$\nThe $(1-\\alpha)$ Wald confidence interval for $k$ is given by:\n$$CI_k = \\hat{k} \\pm z_{1-\\alpha/2} \\cdot \\text{SE}(\\hat{k}) = \\hat{k} \\pm z_{1-\\alpha/2} \\frac{\\hat{k}}{\\sqrt{n}}$$\n\nNext, we consider the reparameterization $\\phi = g(k) = \\ln(k)$. By the invariance property of MLEs, the MLE for $\\phi$ is $\\hat{\\phi} = g(\\hat{k}) = \\ln(\\hat{k})$. The Fisher information for $\\phi$, denoted $I_{\\phi}(\\phi)$, can be found using the transformation formula:\n$$I_{\\phi}(\\phi) = I(k) \\left(\\frac{dk}{d\\phi}\\right)^2$$\nThe inverse transformation is $k = \\exp(\\phi)$, so the derivative is $\\frac{dk}{d\\phi} = \\exp(\\phi) = k$. Substituting this into the formula for $I_{\\phi}(\\phi)$:\n$$I_{\\phi}(\\phi) = \\left(\\frac{n}{k^2}\\right) \\cdot (k)^2 = n$$\nThe Fisher information for $\\phi$ is a constant, $n$. This means the asymptotic variance of $\\hat{\\phi}$ is also constant: $\\text{Var}(\\hat{\\phi}) \\approx I_{\\phi}(\\phi)^{-1} = \\frac{1}{n}$. The standard error of $\\hat{\\phi}$ is:\n$$\\text{SE}(\\hat{\\phi}) = \\frac{1}{\\sqrt{n}}$$\nThe $(1-\\alpha)$ Wald confidence interval for $\\phi$ is:\n$$CI_{\\phi} = \\hat{\\phi} \\pm z_{1-\\alpha/2} \\cdot \\text{SE}(\\hat{\\phi}) = \\hat{\\phi} \\pm \\frac{z_{1-\\alpha/2}}{\\sqrt{n}}$$\nLet the lower and upper bounds of this interval be $\\phi_L$ and $\\phi_U$. To obtain a confidence interval for $k$, we apply the inverse transformation $k = \\exp(\\phi)$ to these bounds:\n$$CI_{k, \\text{transformed}} = [\\exp(\\phi_L), \\exp(\\phi_U)] = \\left[\\exp\\left(\\hat{\\phi} - \\frac{z_{1-\\alpha/2}}{\\sqrt{n}}\\right), \\exp\\left(\\hat{\\phi} + \\frac{z_{1-\\alpha/2}}{\\sqrt{n}}\\right)\\right]$$\nSubstituting $\\hat{\\phi} = \\ln(\\hat{k})$:\n$$CI_{k, \\text{transformed}} = \\left[\\exp\\left(\\ln(\\hat{k})\\right) \\exp\\left(-\\frac{z_{1-\\alpha/2}}{\\sqrt{n}}\\right), \\exp\\left(\\ln(\\hat{k})\\right) \\exp\\left(\\frac{z_{1-\\alpha/2}}{\\sqrt{n}}\\right)\\right] = \\left[\\hat{k} \\exp\\left(-\\frac{z_{1-\\alpha/2}}{\\sqrt{n}}\\right), \\hat{k} \\exp\\left(\\frac{z_{1-\\alpha/2}}{\\sqrt{n}}\\right)\\right]$$\n\nThe Wald interval procedure is not invariant under nonlinear reparameterization. This is demonstrated by comparing the two intervals we derived. The first interval, $CI_k$, is symmetric about $\\hat{k}$ on the linear scale. The second interval, $CI_{k, \\text{transformed}}$, is asymmetric about $\\hat{k}$ (it is symmetric on the logarithmic scale). The transformation $\\phi = \\ln(k)$ is often preferred for rate parameters because it transforms the parameter space from $(0, \\infty)$ to $(-\\infty, \\infty)$. The resulting confidence interval for $k$, obtained by back-transforming the interval for $\\phi$, is guaranteed to contain only positive values, which is consistent with the definition of a rate constant. The direct interval for $k$ can, in principle, yield a negative lower bound, which is physically nonsensical. The logarithmic transformation also often acts as a variance-stabilizing transformation, improving the accuracy of the normal approximation used to construct the interval.\n\nFinally, we compute the numerical values for the given dataset: $n=40$, $\\sum_{i=1}^{n} t_i = S = 200$ minutes, $\\alpha=0.05$, and $z_{0.975} \\approx 1.96$.\nFirst, calculate the MLE for $k$:\n$$\\hat{k} = \\frac{n}{S} = \\frac{40}{200} = 0.2 \\, \\text{min}^{-1}$$\n\n1. Endpoints of the direct $95\\%$ Wald interval for $k$:\nThe interval is $\\hat{k} \\pm z_{0.975} \\frac{\\hat{k}}{\\sqrt{n}}$.\n$$0.2 \\pm 1.96 \\times \\frac{0.2}{\\sqrt{40}} = 0.2 \\pm 1.96 \\times \\frac{0.2}{6.324555...} = 0.2 \\pm 0.0619806...$$\nLower endpoint: $0.2 - 0.0619806... = 0.138019... \\approx 0.1380$\nUpper endpoint: $0.2 + 0.0619806... = 0.2619806... \\approx 0.2620$\n\n2. Endpoints of the $95\\%$ Wald interval for $\\phi$ mapped back to $k$:\nThe interval is $\\left[ \\hat{k} \\exp\\left(-\\frac{z_{0.975}}{\\sqrt{n}}\\right), \\hat{k} \\exp\\left(\\frac{z_{0.975}}{\\sqrt{n}}\\right) \\right]$.\nThe exponent term is $\\frac{1.96}{\\sqrt{40}} \\approx 0.309903...$\nLower endpoint: $0.2 \\times \\exp(-0.309903...) = 0.2 \\times 0.73352... = 0.146704... \\approx 0.1467$\nUpper endpoint: $0.2 \\times \\exp(0.309903...) = 0.2 \\times 1.36329... = 0.272658... \\approx 0.2727$\n\nThe four endpoints, rounded to four significant figures, are $0.1380$, $0.2620$, $0.1467$, and $0.2727$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.1380 & 0.2620 & 0.1467 & 0.2727\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Beyond analyzing data from a completed experiment, a key skill for a modeler is to design future experiments that will be maximally informative. This advanced practice  connects system dynamics with statistical precision by using sensitivity equations to derive the Fisher information. By optimizing this information metric, you will determine the single best time to take measurements to minimize the width of the confidence interval for a model parameter, transitioning from a reactive to a proactive role in the scientific process.",
            "id": "3878401",
            "problem": "A biomolecular pharmacokinetic system is described by a one-compartment intravenous bolus model. The plasma drug concentration $x(t)$ obeys the ordinary differential equation $dx/dt = -k\\,x$ with known initial condition $x(0) = x_{0} > 0$, where the elimination rate constant $k > 0$ is an unknown scalar parameter. No dosing or input occurs after $t=0$. Concentration measurements $y_{i}$ are collected at design times $t_{1},\\dots,t_{n}$ with independent additive measurement noise: $y_{i} = x(t_{i}) + \\varepsilon_{i}$, where $\\varepsilon_{i} \\sim \\mathcal{N}(0,\\sigma^{2})$ and $\\sigma^{2} > 0$ is known. The design is based on a nominal parameter value $k_{0} > 0$.\n\nUsing the sensitivity equation framework and standard regularity conditions for maximum likelihood estimation, carry out the following:\n\n1. Derive the sensitivity $s(t) = \\partial x(t)/\\partial k$ by formulating and solving the sensitivity differential equation consistent with $dx/dt = -k\\,x$ and $x(0)=x_{0}$.\n\n2. Using the derived sensitivity and the statistical model for $y_{i}$, derive the expected Fisher information for $k$ evaluated at the nominal value $k_{0}$ as a function of the design times $t_{1},\\dots,t_{n}$, the initial condition $x_{0}$, and the noise variance $\\sigma^{2}$.\n\n3. The large-sample Wald (named after Abraham Wald) confidence interval for $k$ centered at the maximum likelihood estimator has width proportional to the square root of the inverse of the expected Fisher information evaluated at $k_{0}$. Under a fixed budget that allows exactly $n$ measurements and identical cost per measurement independent of time, you may place multiple measurements at the same time point. Determine the set of time points $t_{1},\\dots,t_{n} \\in [0,\\infty)$ that minimizes the Wald interval width, and provide the analytic expression for the single optimal sampling time $t^{*}$ at which all measurements should be taken.\n\nExpress the final optimal sampling time $t^{*}$ in hours. You do not need to compute a numerical value.",
            "solution": "The problem is evaluated as valid, as it is scientifically grounded in pharmacokinetic modeling and estimation theory, well-posed, objective, and contains all necessary information for a symbolic solution. The solution proceeds in three parts as requested.\n\n### Part 1: Derivation of the Sensitivity Equation\n\nThe system is described by the ordinary differential equation (ODE) for the plasma drug concentration $x(t)$:\n$$\n\\frac{dx}{dt} = -k x(t)\n$$\nwith the initial condition $x(0) = x_0$. This is a first-order linear homogeneous ODE, which can be solved by separation of variables.\n$$\n\\frac{dx}{x} = -k dt\n$$\nIntegrating both sides gives $\\ln(x(t)) = -kt + C$, where $C$ is the integration constant. Exponentiating yields $x(t) = \\exp(-kt + C) = A\\exp(-kt)$, where $A = \\exp(C)$. Applying the initial condition $x(0) = x_0$, we find $A = x_0$. Thus, the solution to the ODE is:\n$$\nx(t) = x_0 \\exp(-kt)\n$$\nThe sensitivity of the state $x(t)$ with respect to the parameter $k$ is defined as $s(t) = \\frac{\\partial x(t)}{\\partial k}$. To find the differential equation governing $s(t)$, we differentiate the primary ODE with respect to $k$:\n$$\n\\frac{\\partial}{\\partial k}\\left(\\frac{dx}{dt}\\right) = \\frac{\\partial}{\\partial k}(-k x)\n$$\nAssuming sufficient regularity, we can interchange the order of differentiation:\n$$\n\\frac{d}{dt}\\left(\\frac{\\partial x}{\\partial k}\\right) = -\\frac{\\partial}{\\partial k}(k) \\cdot x - k \\cdot \\frac{\\partial x}{\\partial k}\n$$\nSubstituting $s(t) = \\frac{\\partial x}{\\partial k}$ and noting that $\\frac{\\partial k}{\\partial k} = 1$, we obtain the sensitivity ODE:\n$$\n\\frac{ds}{dt} = -x(t) - k s(t)\n$$\nThe initial condition for the sensitivity, $s(0)$, is found by differentiating the initial condition for $x(t)$ with respect to $k$. Since $x(0) = x_0$ is a given constant and does not depend on $k$, we have:\n$$\ns(0) = \\frac{\\partial x(0)}{\\partial k} = \\frac{\\partial x_0}{\\partial k} = 0\n$$\nWe now solve the sensitivity ODE, $\\frac{ds}{dt} + k s(t) = -x(t)$, with $s(0) = 0$. Substituting the expression for $x(t)$:\n$$\n\\frac{ds}{dt} + k s(t) = -x_0 \\exp(-kt)\n$$\nThis is a first-order linear non-homogeneous ODE. We use an integrating factor $I(t) = \\exp\\left(\\int k dt\\right) = \\exp(kt)$. Multiplying the ODE by $I(t)$:\n$$\n\\exp(kt)\\frac{ds}{dt} + k \\exp(kt) s(t) = -x_0 \\exp(-kt)\\exp(kt)\n$$\nThe left side is the derivative of the product $s(t)I(t)$:\n$$\n\\frac{d}{dt}\\left(s(t)\\exp(kt)\\right) = -x_0\n$$\nIntegrating both sides with respect to $t$:\n$$\ns(t)\\exp(kt) = \\int -x_0 dt = -x_0 t + C'\n$$\nwhere $C'$ is the integration constant. Solving for $s(t)$:\n$$\ns(t) = (-x_0 t + C')\\exp(-kt)\n$$\nApplying the initial condition $s(0) = 0$:\n$$\n0 = (-x_0 \\cdot 0 + C')\\exp(-k \\cdot 0) \\implies C' = 0\n$$\nTherefore, the sensitivity is:\n$$\ns(t) = -x_0 t \\exp(-kt)\n$$\n\n### Part 2: Derivation of the Expected Fisher Information\n\nThe measurement model is $y_i = x(t_i) + \\varepsilon_i$, where $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$ are independent and identically distributed noise terms. For such a model with additive Gaussian noise, the expected Fisher information for the parameter $k$ from a set of $n$ measurements is given by the sum of the information from each measurement. For a single measurement $y_i$ at time $t_i$, the Fisher information is:\n$$\nF_i(k) = \\frac{1}{\\sigma^2} \\left( \\frac{\\partial x(t_i)}{\\partial k} \\right)^2\n$$\nSince the measurements are independent, the total expected Fisher information is the sum:\n$$\nF = \\sum_{i=1}^{n} F_i(k) = \\sum_{i=1}^{n} \\frac{1}{\\sigma^2} \\left( \\frac{\\partial x(t_i)}{\\partial k} \\right)^2 = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} [s(t_i)]^2\n$$\nSubstituting the derived expression for the sensitivity $s(t) = -x_0 t \\exp(-kt)$:\n$$\n[s(t_i)]^2 = (-x_0 t_i \\exp(-kt_i))^2 = x_0^2 t_i^2 \\exp(-2kt_i)\n$$\nThe total Fisher information as a function of $k$ is:\n$$\nF(k) = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} x_0^2 t_i^2 \\exp(-2kt_i) = \\frac{x_0^2}{\\sigma^2} \\sum_{i=1}^{n} t_i^2 \\exp(-2kt_i)\n$$\nThe problem requires this to be evaluated at the nominal parameter value $k_0$.\n$$\nF(k_0) = \\frac{x_0^2}{\\sigma^2} \\sum_{i=1}^{n} t_i^2 \\exp(-2k_0 t_i)\n$$\nThis is the expected Fisher information for $k$ as a function of the design times $t_1, \\dots, t_n$, the initial condition $x_0$, and the noise variance $\\sigma^2$.\n\n### Part 3: Optimal Sampling Time\n\nThe width of the large-sample Wald confidence interval for $k$ is proportional to the square root of the inverse of the expected Fisher information, i.e., width $\\propto \\sqrt{1/F(k_0)}$. Minimizing the confidence interval width is therefore equivalent to maximizing the Fisher information $F(k_0)$. The optimization problem is to choose the measurement times $t_i \\geq 0$ for $i=1, \\dots, n$ to maximize:\n$$\nF(k_0) = \\frac{x_0^2}{\\sigma^2} \\sum_{i=1}^{n} t_i^2 \\exp(-2k_0 t_i)\n$$\nMaximizing $F(k_0)$ is equivalent to maximizing the sum $\\sum_{i=1}^{n} t_i^2 \\exp(-2k_0 t_i)$, since $\\frac{x_0^2}{\\sigma^2}$ is a positive constant. Let's define the function $f(t) = t^2 \\exp(-2k_0 t)$. We need to maximize $\\sum_{i=1}^{n} f(t_i)$.\nSince $f(t) \\geq 0$ for $t \\geq 0$, the sum is maximized when each term $f(t_i)$ is individually maximized. This means that all sampling times $t_i$ should be chosen to be the single time $t^*$ that maximizes $f(t)$ over the domain $t \\in [0, \\infty)$.\n\nTo find the maximum of $f(t)$, we compute its derivative with respect to $t$ and set it to zero.\n$$\nf'(t) = \\frac{d}{dt} \\left( t^2 \\exp(-2k_0 t) \\right)\n$$\nUsing the product rule $(uv)' = u'v + uv'$:\n$$\nf'(t) = (2t)\\exp(-2k_0 t) + t^2(-2k_0)\\exp(-2k_0 t)\n$$\n$$\nf'(t) = (2t - 2k_0 t^2)\\exp(-2k_0 t) = 2t(1 - k_0 t)\\exp(-2k_0 t)\n$$\nWe set $f'(t) = 0$ to find the critical points. Since $\\exp(-2k_0 t) > 0$ for all $t$, the solutions are given by:\n$$\n2t(1 - k_0 t) = 0\n$$\nThis yields two critical points: $t=0$ and $t = \\frac{1}{k_0}$.\n\nWe must now determine which of these points corresponds to the global maximum on $[0, \\infty)$.\n- At $t=0$: $f(0) = 0^2 \\exp(0) = 0$.\n- At $t=\\frac{1}{k_0}$: $f\\left(\\frac{1}{k_0}\\right) = \\left(\\frac{1}{k_0}\\right)^2 \\exp\\left(-2k_0 \\frac{1}{k_0}\\right) = \\frac{1}{k_0^2} \\exp(-2)$.\n- As $t \\to \\infty$: $f(t) = t^2 \\exp(-2k_0 t) \\to 0$, as the exponential decay dominates the polynomial growth.\n\nSince $k_0 > 0$, we have $f\\left(\\frac{1}{k_0}\\right) = \\frac{\\exp(-2)}{k_0^2} > 0$. Comparing the values, the function $f(t)$ attains its global maximum at $t = \\frac{1}{k_0}$.\n\nTherefore, to maximize the Fisher information, all $n$ measurements should be taken at this single optimal time point. The optimal sampling time $t^*$ is:\n$$\nt^* = \\frac{1}{k_0}\n$$\nThe problem statement implies that if $k_0$ has units of inverse hours, $t^*$ will have units of hours.",
            "answer": "$$\n\\boxed{\\frac{1}{k_0}}\n$$"
        }
    ]
}