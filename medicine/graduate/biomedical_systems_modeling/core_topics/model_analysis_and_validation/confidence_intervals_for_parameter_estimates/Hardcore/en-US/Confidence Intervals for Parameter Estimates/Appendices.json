{
    "hands_on_practices": [
        {
            "introduction": "Confidence intervals are frequently constructed using the asymptotic normality of Maximum Likelihood Estimators (MLEs). This practice grounds this theory by first asking you to derive a standard Wald interval for a rate parameter from an exponential model. The core of the exercise, however, lies in exploring the consequences of reparameterization—specifically, by comparing the standard interval to one derived on a logarithmic scale, you will gain firsthand insight into why the choice of parameterization is a critical and practical consideration in statistical modeling .",
            "id": "3878457",
            "problem": "A biomedical systems model for receptor internalization posits that the waiting times for endocytosis events are independent and identically distributed exponential random variables with rate constant $k$ (in $\\text{min}^{-1}$). Specifically, the observed times $\\{t_{i}\\}_{i=1}^{n}$ are modeled as independent samples from a distribution with probability density function $f(t \\mid k) = k \\exp(-k t)$ for $t \\ge 0$. The rate constant $k$ quantifies the average frequency of these events and is positive.\n\nStarting from the fundamental definitions of likelihood, Maximum Likelihood Estimator (MLE), and Fisher information, proceed as follows:\n- Derive the MLE $\\hat{k}$ for $k$ and the total Fisher information $I(k)$ in $k$ under the exponential model.\n- Using the asymptotic normality of the MLE and Fisher information, construct a two-sided Wald interval for $k$ at nominal confidence level $1 - \\alpha$ (take $\\alpha = 0.05$ and use the standard normal quantile $z_{0.975} \\approx 1.96$).\n- Consider the reparameterization $\\phi = g(k) = \\ln(k)$. Derive the Fisher information $I_{\\phi}(\\phi)$ for $\\phi$ via the chain rule for scores, and construct the corresponding two-sided Wald interval for $\\phi$ at the same confidence level. Then map this interval back to the original parameter $k$ via the inverse transformation $k = \\exp(\\phi)$ to obtain a transformed-back interval for $k$.\n- Explain how reparameterization affects Wald intervals and discuss whether Wald intervals are invariant under nonlinear transformations, using the above derivations to support your reasoning.\n\nFor the concrete dataset with $n = 40$ and $\\sum_{i=1}^{n} t_{i} = S = 200$ minutes, compute numerically:\n1. The endpoints of the two-sided $95\\%$ Wald interval for $k$ constructed directly on $k$.\n2. The endpoints of the two-sided $95\\%$ Wald interval for $\\phi$ mapped back to $k$ via $k = \\exp(\\phi)$.\n\nRound all four endpoints to four significant figures. Express your final numeric endpoints for $k$ in $\\text{min}^{-1}$. Report the four numbers in a single row matrix in the order $\\big[$lower endpoint for $k$ (direct), upper endpoint for $k$ (direct), lower endpoint for $k$ (transformed-back), upper endpoint for $k$ (transformed-back)$\\big]$.",
            "solution": "We begin by deriving the Maximum Likelihood Estimator (MLE) and Fisher information for the rate constant $k$. The likelihood function for $n$ independent and identically distributed observations $\\{t_{i}\\}_{i=1}^{n}$ from an exponential distribution with rate $k$ is given by:\n$$L(k \\mid \\{t_{i}\\}) = \\prod_{i=1}^{n} f(t_i \\mid k) = \\prod_{i=1}^{n} k \\exp(-k t_i) = k^n \\exp\\left(-k \\sum_{i=1}^{n} t_i\\right)$$\nThe log-likelihood function, $\\ell(k)$, is:\n$$\\ell(k) = \\ln L(k \\mid \\{t_{i}\\}) = n \\ln(k) - k \\sum_{i=1}^{n} t_i$$\nTo find the MLE $\\hat{k}$, we compute the score function (the first derivative of $\\ell(k)$ with respect to $k$) and set it to zero:\n$$\\frac{d\\ell}{dk} = \\frac{n}{k} - \\sum_{i=1}^{n} t_i = 0$$\nSolving for $k$ gives the MLE:\n$$\\hat{k} = \\frac{n}{\\sum_{i=1}^{n} t_i} = \\frac{1}{\\bar{t}}$$\nwhere $\\bar{t}$ is the sample mean of the observed times.\n\nNext, we derive the Fisher information. The second derivative of the log-likelihood is:\n$$\\frac{d^2\\ell}{dk^2} = -\\frac{n}{k^2}$$\nThe Fisher information for the entire sample of size $n$, $I(k)$, is the negative expectation of the second derivative:\n$$I(k) = -E\\left[\\frac{d^2\\ell}{dk^2}\\right] = -E\\left[-\\frac{n}{k^2}\\right] = \\frac{n}{k^2}$$\nNote that since the second derivative does not depend on the data $\\{t_i\\}$, the expectation is simply the quantity itself. This is the observed Fisher information as well as the expected Fisher information.\n\nNow, we construct the two-sided Wald interval for $k$. The Wald interval is based on the asymptotic normality of the MLE, which states that for large $n$, $\\hat{k}$ is approximately normally distributed with mean $k$ and variance $I(k)^{-1}$. The variance is estimated by substituting $\\hat{k}$ for $k$, giving an estimated standard error, $\\text{SE}(\\hat{k})$:\n$$\\text{SE}(\\hat{k}) = \\sqrt{I(\\hat{k})^{-1}} = \\sqrt{\\frac{\\hat{k}^2}{n}} = \\frac{\\hat{k}}{\\sqrt{n}}$$\nThe $(1-\\alpha)$ Wald confidence interval for $k$ is given by:\n$$CI_k = \\hat{k} \\pm z_{1-\\alpha/2} \\cdot \\text{SE}(\\hat{k}) = \\hat{k} \\pm z_{1-\\alpha/2} \\frac{\\hat{k}}{\\sqrt{n}}$$\n\nNext, we consider the reparameterization $\\phi = g(k) = \\ln(k)$. By the invariance property of MLEs, the MLE for $\\phi$ is $\\hat{\\phi} = g(\\hat{k}) = \\ln(\\hat{k})$. The Fisher information for $\\phi$, denoted $I_{\\phi}(\\phi)$, can be found using the transformation formula:\n$$I_{\\phi}(\\phi) = I(k) \\left(\\frac{dk}{d\\phi}\\right)^2$$\nThe inverse transformation is $k = \\exp(\\phi)$, so the derivative is $\\frac{dk}{d\\phi} = \\exp(\\phi) = k$. Substituting this into the formula for $I_{\\phi}(\\phi)$:\n$$I_{\\phi}(\\phi) = \\left(\\frac{n}{k^2}\\right) \\cdot (k)^2 = n$$\nThe Fisher information for $\\phi$ is a constant, $n$. This means the asymptotic variance of $\\hat{\\phi}$ is also constant: $Var(\\hat{\\phi}) \\approx I_{\\phi}(\\phi)^{-1} = \\frac{1}{n}$. The standard error of $\\hat{\\phi}$ is:\n$$\\text{SE}(\\hat{\\phi}) = \\frac{1}{\\sqrt{n}}$$\nThe $(1-\\alpha)$ Wald confidence interval for $\\phi$ is:\n$$CI_{\\phi} = \\hat{\\phi} \\pm z_{1-\\alpha/2} \\cdot \\text{SE}(\\hat{\\phi}) = \\hat{\\phi} \\pm \\frac{z_{1-\\alpha/2}}{\\sqrt{n}}$$\nLet the lower and upper bounds of this interval be $\\phi_L$ and $\\phi_U$. To obtain a confidence interval for $k$, we apply the inverse transformation $k = \\exp(\\phi)$ to these bounds:\n$$CI_{k, \\text{transformed}} = [\\exp(\\phi_L), \\exp(\\phi_U)] = \\left[\\exp\\left(\\hat{\\phi} - \\frac{z_{1-\\alpha/2}}{\\sqrt{n}}\\right), \\exp\\left(\\hat{\\phi} + \\frac{z_{1-\\alpha/2}}{\\sqrt{n}}\\right)\\right]$$\nSubstituting $\\hat{\\phi} = \\ln(\\hat{k})$:\n$$CI_{k, \\text{transformed}} = \\left[\\exp\\left(\\ln(\\hat{k})\\right) \\exp\\left(-\\frac{z_{1-\\alpha/2}}{\\sqrt{n}}\\right), \\exp\\left(\\ln(\\hat{k})\\right) \\exp\\left(\\frac{z_{1-\\alpha/2}}{\\sqrt{n}}\\right)\\right] = \\left[\\hat{k} \\exp\\left(-\\frac{z_{1-\\alpha/2}}{\\sqrt{n}}\\right), \\hat{k} \\exp\\left(\\frac{z_{1-\\alpha/2}}{\\sqrt{n}}\\right)\\right]$$\n\nThe Wald interval procedure is not invariant under nonlinear reparameterization. This is demonstrated by comparing the two intervals we derived. The first interval, $CI_k$, is symmetric about $\\hat{k}$ on the linear scale. The second interval, $CI_{k, \\text{transformed}}$, is asymmetric about $\\hat{k}$ (it is symmetric on the logarithmic scale). The transformation $\\phi = \\ln(k)$ is often preferred for rate parameters because it transforms the parameter space from $(0, \\infty)$ to $(-\\infty, \\infty)$. The resulting confidence interval for $k$, obtained by back-transforming the interval for $\\phi$, is guaranteed to contain only positive values, which is consistent with the definition of a rate constant. The direct interval for $k$ can, in principle, yield a negative lower bound, which is physically nonsensical. The logarithmic transformation also often acts as a variance-stabilizing transformation, improving the accuracy of the normal approximation used to construct the interval.\n\nFinally, we compute the numerical values for the given dataset: $n=40$, $\\sum_{i=1}^{n} t_i = S = 200$ minutes, $\\alpha=0.05$, and $z_{0.975} = 1.96$.\nFirst, calculate the MLE for $k$:\n$$\\hat{k} = \\frac{n}{S} = \\frac{40}{200} = 0.2 \\, \\text{min}^{-1}$$\n\n1. Endpoints of the direct $95\\%$ Wald interval for $k$:\nThe interval is $\\hat{k} \\pm z_{0.975} \\frac{\\hat{k}}{\\sqrt{n}}$.\n$$0.2 \\pm 1.96 \\times \\frac{0.2}{\\sqrt{40}} = 0.2 \\pm 1.96 \\times \\frac{0.2}{6.324555...} = 0.2 \\pm 0.0619806...$$\nLower endpoint: $0.2 - 0.0619806... = 0.138019... \\approx 0.1380$\nUpper endpoint: $0.2 + 0.0619806... = 0.2619806... \\approx 0.2620$\n\n2. Endpoints of the $95\\%$ Wald interval for $\\phi$ mapped back to $k$:\nThe interval is $\\left[ \\hat{k} \\exp\\left(-\\frac{z_{0.975}}{\\sqrt{n}}\\right), \\hat{k} \\exp\\left(\\frac{z_{0.975}}{\\sqrt{n}}\\right) \\right]$.\nThe exponent term is $\\frac{1.96}{\\sqrt{40}} \\approx 0.309903...$\nLower endpoint: $0.2 \\times \\exp(-0.309903...) = 0.2 \\times 0.73352... = 0.146704... \\approx 0.1467$\nUpper endpoint: $0.2 \\times \\exp(0.309903...) = 0.2 \\times 1.36329... = 0.272658... \\approx 0.2727$\n\nThe four endpoints, rounded to four significant figures, are $0.1380$, $0.2620$, $0.1467$, and $0.2727$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.1380 & 0.2620 & 0.1467 & 0.2727\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Moving from simple distributions to dynamic systems, this exercise applies the principles of confidence intervals to a parameter of a pharmacokinetic model described by an Ordinary Differential Equation (ODE). You will use the Gauss-Newton approximation within a nonlinear least squares framework to quantify the uncertainty of an estimated elimination rate. This practice reinforces the link between dynamic modeling and statistical inference and crucially, requires you to justify the use of the Student's $t$-distribution, a detail that correctly accounts for the uncertainty in the estimated measurement error .",
            "id": "3878488",
            "problem": "A pharmacokinetic one-compartment intravenous bolus model describes plasma concentration dynamics by the Ordinary Differential Equation (ODE) $dC/dt = -k\\,C$, where $k$ is the first-order elimination rate constant and $C(t)$ is the plasma concentration at time $t$. The analytic solution is $C(t) = C_{0}\\,\\exp(-k\\,t)$, where $C_{0}$ is the initial concentration immediately after bolus administration. Suppose $n$ distinct plasma concentration observations $\\{y_{i}\\}_{i=1}^{n}$ at times $\\{t_{i}\\}_{i=1}^{n}$ are modeled by $y_{i} = C_{0}\\,\\exp(-k\\,t_{i}) + \\varepsilon_{i}$, with independent and identically distributed measurement errors $\\varepsilon_{i} \\sim \\mathcal{N}(0,\\sigma^{2})$. You fit the nonlinear least squares estimator by minimizing the Sum of Squared Errors (SSE) $S(\\theta) = \\sum_{i=1}^{n} \\left(y_{i} - C_{0}\\,\\exp(-k\\,t_{i})\\right)^{2}$ over $\\theta = (k, C_{0})^{\\top}$ and obtain the Gauss–Newton solution $\\hat{\\theta} = (\\hat{k}, \\hat{C}_{0})^{\\top}$.\n\nAt the solution, the Jacobian matrix $J(\\hat{\\theta})$ of partial derivatives of the model predictions with respect to parameters has $p=2$ columns. Under the Gauss–Newton approximation, local linearization is used to characterize the uncertainty in $\\hat{\\theta}$. Let $n=30$, and suppose the following quantities are computed at $\\hat{\\theta}$:\n- The Sum of Squared Errors (SSE) is $S(\\hat{\\theta}) = 0.84$.\n- The $(1,1)$ element of $\\left(J(\\hat{\\theta})^{\\top}J(\\hat{\\theta})\\right)^{-1}$ is $\\left[\\left(J(\\hat{\\theta})^{\\top}J(\\hat{\\theta})\\right)^{-1}\\right]_{11} = 2.0 \\times 10^{-3}$.\n- The estimated elimination rate constant is $\\hat{k} = 0.35$ $\\mathrm{h}^{-1}$.\n\nStarting from the assumptions of independent Gaussian errors, the nonlinear least squares setup, and the Gauss–Newton local linearization, derive a two-sided confidence interval for $k$ at confidence level $0.95$ that justifies the use of the Student’s $t$ distribution with the appropriate Degrees of Freedom (DoF). Explicitly explain the origin of the degrees of freedom and why the $t$ distribution (rather than the normal distribution) is used.\n\nThen, compute the numerical endpoints of the derived confidence interval for $k$ using the provided quantities. Round your endpoints to four significant figures. Express the final endpoints in $\\mathrm{h}^{-1}$.",
            "solution": "The goal is to derive and compute a $95\\%$ confidence interval for the parameter $k$. In nonlinear least squares, under the assumption that the errors $\\varepsilon_{i}$ are independent and identically distributed as $\\mathcal{N}(0,\\sigma^{2})$, the parameter estimator $\\hat{\\theta}$ is approximately normally distributed for large $n$, with mean equal to the true parameter vector $\\theta$ and a covariance matrix given by $\\text{Cov}(\\hat{\\theta}) \\approx \\sigma^{2} \\left(J(\\theta)^{\\top}J(\\theta)\\right)^{-1}$. The Jacobian matrix $J(\\theta)$ is an $n \\times p$ matrix of partial derivatives of the model function with respect to the parameters, evaluated at the true parameter values. In practice, we evaluate $J$ at the estimate $\\hat{\\theta}$.\n\nThe variance of the estimate for a single parameter, $\\hat{k}$, is the first diagonal element of this covariance matrix:\n$$ \\text{Var}(\\hat{k}) \\approx \\sigma^{2} \\left[\\left(J(\\hat{\\theta})^{\\top}J(\\hat{\\theta})\\right)^{-1}\\right]_{11} $$\nThe true error variance $\\sigma^{2}$ is generally unknown and must be estimated from the data. The unbiased estimator for $\\sigma^{2}$ is the mean squared error (MSE), denoted as $s^{2}$:\n$$ s^{2} = \\frac{S(\\hat{\\theta})}{n-p} = \\frac{\\text{SSE}}{\\text{DoF}} $$\nThe denominator, $n-p$, represents the degrees of freedom (DoF) for the error. This is because we start with $n$ independent data points, but we use them to estimate $p$ parameters. The estimation of these $p$ parameters imposes $p$ constraints on the residuals, leaving $n-p$ degrees of freedom for the estimation of the error variance. In this problem, $n=30$ and $p=2$ (for $k$ and $C_0$), so the degrees of freedom are $30 - 2 = 28$.\n\nSubstituting the estimate $s^{2}$ for the true variance $\\sigma^{2}$, we obtain the estimated variance of $\\hat{k}$:\n$$ \\widehat{\\text{Var}}(\\hat{k}) = s^{2} \\left[\\left(J(\\hat{\\theta})^{\\top}J(\\hat{\\theta})\\right)^{-1}\\right]_{11} $$\nThe standard error of the estimate $\\hat{k}$ is the square root of this estimated variance:\n$$ \\text{se}(\\hat{k}) = \\sqrt{\\widehat{\\text{Var}}(\\hat{k})} = \\sqrt{s^{2} \\left[\\left(J(\\hat{\\theta})^{\\top}J(\\hat{\\theta})\\right)^{-1}\\right]_{11}} $$\nNow, we justify the use of the Student's $t$-distribution. Consider the pivotal quantity:\n$$ T = \\frac{\\hat{k} - k}{\\text{se}(\\hat{k})} $$\nIf the true variance $\\sigma^{2}$ were known, the quantity $(\\hat{k}-k)/\\sqrt{\\text{Var}(\\hat{k})}$ would be approximately a standard normal variable, $Z \\sim \\mathcal{N}(0,1)$. However, because we use the estimate $s^2$, we introduce additional uncertainty. The statistical theory states that the quantity $\\frac{(n-p)s^{2}}{\\sigma^{2}}$ follows a chi-squared ($\\chi^{2}$) distribution with $n-p$ degrees of freedom.\n\nLet's rewrite the pivotal quantity $T$:\n$$ T = \\frac{\\hat{k} - k}{\\sqrt{s^{2} \\left[\\left(J^{\\top}J\\right)^{-1}\\right]_{11}}} = \\frac{\\frac{\\hat{k} - k}{\\sigma\\sqrt{\\left[\\left(J^{\\top}J\\right)^{-1}\\right]_{11}}}}{\\sqrt{\\frac{s^{2}}{\\sigma^{2}}}} = \\frac{Z}{\\sqrt{\\frac{(n-p)s^{2}/\\sigma^{2}}{n-p}}} $$\nThe numerator $Z$ is (approximately) a standard normal random variable. The term in the square root in the denominator is a $\\chi^{2}$-distributed random variable with $n-p$ degrees of freedom, divided by its degrees of freedom. By definition, a random variable formed by the ratio of a standard normal variable to the square root of an independent $\\chi^{2}$ variable divided by its degrees of freedom follows a Student's $t$-distribution with those same degrees of freedom.\nTherefore, $T = \\frac{\\hat{k} - k}{\\text{se}(\\hat{k})}$ follows a $t$-distribution with $n-p = 28$ degrees of freedom. This is why the $t$-distribution is used instead of the normal distribution: it correctly accounts for the additional uncertainty introduced by estimating $\\sigma^{2}$ from the data.\n\nA two-sided confidence interval for $k$ at confidence level $1-\\alpha$ is constructed as:\n$$ \\hat{k} \\pm t_{\\alpha/2, n-p} \\times \\text{se}(\\hat{k}) $$\nwhere $t_{\\alpha/2, n-p}$ is the critical value from the $t$-distribution that leaves an area of $\\alpha/2$ in the upper tail. For a $95\\%$ confidence level, $1-\\alpha = 0.95$, so $\\alpha = 0.05$ and $\\alpha/2 = 0.025$.\n\nNow we compute the numerical values.\n1.  Degrees of Freedom (DoF): $n-p = 30-2 = 28$.\n2.  Estimated error variance: $s^{2} = \\frac{S(\\hat{\\theta})}{n-p} = \\frac{0.84}{28} = 0.03$.\n3.  Standard error of $\\hat{k}$:\n    $$ \\text{se}(\\hat{k}) = \\sqrt{s^{2} \\left[\\left(J(\\hat{\\theta})^{\\top}J(\\hat{\\theta})\\right)^{-1}\\right]_{11}} = \\sqrt{0.03 \\times (2.0 \\times 10^{-3})} = \\sqrt{6.0 \\times 10^{-5}} $$\n    $$ \\text{se}(\\hat{k}) \\approx 0.0077459667 $$\n4.  Critical $t$-value: For a $95\\%$ confidence interval and $28$ degrees of freedom, the critical value is $t_{0.025, 28}$. From standard statistical tables or software, $t_{0.025, 28} = 2.0484$.\n5.  Margin of Error (ME):\n    $$ \\text{ME} = t_{0.025, 28} \\times \\text{se}(\\hat{k}) = 2.0484 \\times 0.0077459667 \\approx 0.0158673 $$\n6.  Confidence Interval (CI):\n    $$ \\text{CI} = \\hat{k} \\pm \\text{ME} = 0.35 \\pm 0.0158673 $$\n    -   Lower bound: $0.35 - 0.0158673 = 0.3341327$\n    -   Upper bound: $0.35 + 0.0158673 = 0.3658673$\n\nRounding the endpoints to four significant figures as requested:\n-   Lower bound: $0.3341$\n-   Upper bound: $0.3659$\n\nThe $95\\%$ confidence interval for the elimination rate constant $k$ is $(0.3341, 0.3659) \\, \\mathrm{h}^{-1}$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.3341 & 0.3659\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "While asymptotic methods are powerful, they are not universally applicable, especially with discrete data or rare events. This practice challenges you to construct an 'exact' Clopper-Pearson confidence interval for a binomial proportion when zero events are observed—a common scenario in safety and pharmacovigilance studies. By deriving the interval from its first principles of inverting a hypothesis test, you will develop a deeper understanding of what a confidence interval represents and analyze its performance through the concept of coverage probability .",
            "id": "3878404",
            "problem": "A pharmacovigilance team is evaluating a rare adverse event associated with a new implantable cardiac assist device. Let the per-patient risk of the event be modeled as independent Bernoulli trials with success probability $p$ across $n$ device activations, so the total number of observed events $X$ is $\\operatorname{Binomial}(n,p)$. In a pilot study with $n=40$ activations, the team observes $x=0$ events. The team requests an exact two-sided confidence interval for $p$ with nominal coverage level $1-\\alpha$ where $\\alpha=0.05$ that remains valid in discrete and boundary cases (zero counts).\n\nUsing the inversion-of-exact-tests construction for the binomial model, derive from first principles the exact equal-tailed Clopper–Pearson (CP) interval for $p$ when $X=0$ and $n=40$, and obtain the closed-form expression for its upper endpoint. Then, analyze the interval’s conservatism at a rare event probability $p_{0}=10^{-4}$ by computing the exact coverage function $C(p_{0})=\\mathbb{P}_{p_{0}}\\!\\big(p_{0}\\in I(X)\\big)$, where $I(X)$ denotes the CP interval produced by the observation $X$. Provide a rigorous justification for any simplification of $C(p_{0})$ that relies on properties of the CP bounds at the boundary and on monotonicity with respect to $x$. Finally, quantify the absolute conservatism at $p_{0}$ as the difference $\\Delta=C(p_{0})-(1-\\alpha)$ and report the single numerical value of $\\Delta$ rounded to four significant figures. No units are required for the final answer.",
            "solution": "First, we derive the exact equal-tailed Clopper–Pearson (CP) interval for the binomial proportion $p$. The number of events $X$ follows a binomial distribution, $X \\sim \\operatorname{Binomial}(n,p)$, with probability mass function $\\mathbb{P}(X=k|p) = \\binom{n}{k} p^k (1-p)^{n-k}$. The CP interval $[p_L(x), p_U(x)]$ for an observed count $x$ is constructed by inverting two one-sided, exact hypothesis tests, each at significance level $\\alpha/2$. The endpoints $p_L$ and $p_U$ are the solutions to the following equations:\n$$ \\mathbb{P}(X \\ge x | p_L) = \\sum_{k=x}^{n} \\binom{n}{k} p_L^k (1-p_L)^{n-k} = \\frac{\\alpha}{2} $$\n$$ \\mathbb{P}(X \\le x | p_U) = \\sum_{k=0}^{x} \\binom{n}{k} p_U^k (1-p_U)^{n-k} = \\frac{\\alpha}{2} $$\nSpecial conventions apply for boundary observations $x=0$ and $x=n$.\n\nFor the given observation $x=0$:\nThe lower bound $p_L(0)$ is determined from $\\mathbb{P}(X \\ge 0 | p_L) = \\alpha/2$. Since $\\mathbb{P}(X \\ge 0 | p) = 1$ for any $p \\in [0,1]$, the equation $1 = \\alpha/2$ has no solution. By convention, for an observation of $x=0$, the lower bound is the physical boundary of the parameter space, so $p_L(0) = 0$.\n\nThe upper bound $p_U(0)$ is determined from $\\mathbb{P}(X \\le 0 | p_U) = \\alpha/2$. This simplifies to:\n$$ \\mathbb{P}(X = 0 | p_U) = \\frac{\\alpha}{2} $$\nSubstituting the binomial probability mass function:\n$$ \\binom{n}{0} p_U^0 (1-p_U)^{n-0} = (1-p_U)^n = \\frac{\\alpha}{2} $$\nSolving this equation for $p_U$ yields the closed-form expression for the upper endpoint:\n$$ 1-p_U = \\left(\\frac{\\alpha}{2}\\right)^{1/n} \\implies p_U(0) = 1 - \\left(\\frac{\\alpha}{2}\\right)^{1/n} $$\nWith $n=40$ and $\\alpha=0.05$, the specific CP interval for $x=0$ is $[0, 1-(0.025)^{1/40}]$.\n\nNext, we analyze the interval's conservatism by computing the exact coverage function $C(p_{0})$ at $p_{0}=10^{-4}$. The coverage function is the probability that the random interval $I(X)$ contains the true parameter $p_{0}$, where the randomness is over $X \\sim \\operatorname{Binomial}(n, p_{0})$.\n$$ C(p_{0}) = \\mathbb{P}_{p_{0}}(p_{0} \\in I(X)) = \\mathbb{P}_{p_{0}}(p_L(X) \\le p_{0} \\le p_U(X)) $$\nThis can be written as a sum over all possible outcomes $k \\in \\{0, 1, \\dots, n\\}$:\n$$ C(p_{0}) = \\sum_{k=0}^{n} \\mathbb{I}(p_L(k) \\le p_{0} \\le p_U(k)) \\cdot \\mathbb{P}(X=k | p_{0}) $$\nwhere $\\mathbb{I}(\\cdot)$ is the indicator function.\n\nWe now provide a rigorous justification for simplifying this sum. The simplification relies on determining for which values of $k$ the interval $I(k) = [p_L(k), p_U(k)]$ actually contains $p_{0}$. This requires satisfying two conditions: $p_L(k) \\le p_0$ and $p_U(k) \\ge p_0$.\n\nThe lower bound function $p_L(k)$ is a strictly increasing function of $k$ for $k>0$.\nFor $k=0$, we have $p_L(0)=0$. Since $p_0=10^{-4} > 0$, the condition $p_L(0) \\le p_0$ is satisfied.\nFor $k=1$, the lower bound $p_L(1)$ is the solution to $\\mathbb{P}(X \\ge 1 | p_L(1)) = \\alpha/2$.\n$$ 1 - \\mathbb{P}(X=0 | p_L(1)) = 1 - (1-p_L(1))^n = \\frac{\\alpha}{2} $$\n$$ (1-p_L(1))^n = 1-\\frac{\\alpha}{2} \\implies p_L(1) = 1 - \\left(1-\\frac{\\alpha}{2}\\right)^{1/n} $$\nSubstituting $n=40$ and $\\alpha=0.05$:\n$$ p_L(1) = 1 - (1-0.025)^{1/40} = 1 - (0.975)^{1/40} \\approx 1 - 0.9993673 = 0.0006327 $$\nWe compare this with $p_{0}=10^{-4}=0.0001$. We find that $p_L(1) \\approx 0.0006327 > 0.0001 = p_{0}$.\nSince $p_L(k)$ is a monotonically increasing function of $k$, it follows that for all $k \\ge 1$, $p_L(k) \\ge p_L(1) > p_0$. This means the condition $p_L(k) \\le p_0$ is false for all $k \\ge 1$.\nTherefore, the only possible value of $k$ for which the interval $I(k)$ can cover $p_0$ is $k=0$.\n\nWe must also verify that for $k=0$, the second condition, $p_U(0) \\ge p_0$, holds.\n$$ p_U(0) = 1 - \\left(\\frac{\\alpha}{2}\\right)^{1/n} = 1 - (0.025)^{1/40} \\approx 1 - 0.911905 = 0.088095 $$\nClearly, $p_U(0) \\approx 0.0881 > 0.0001 = p_0$.\nThus, the interval $I(k)$ covers $p_0$ if and only if $k=0$.\n\nThe sum for the coverage probability $C(p_0)$ collapses to a single term:\n$$ C(p_{0}) = \\mathbb{I}(p_L(0) \\le p_0 \\le p_U(0)) \\cdot \\mathbb{P}(X=0 | p_0) + \\sum_{k=1}^{n} (0) \\cdot \\mathbb{P}(X=k | p_0) $$\n$$ C(p_{0}) = 1 \\cdot \\mathbb{P}(X=0 | p_0) = \\binom{n}{0} p_0^0 (1-p_0)^{n-0} = (1-p_0)^n $$\nThis provides the simplified expression for the coverage at $p_0$.\n\nFinally, we quantify the absolute conservatism $\\Delta = C(p_0) - (1-\\alpha)$.\nSubstituting the expressions and numerical values for $p_0$, $n$, and $\\alpha$:\n$$ \\Delta = (1-p_0)^n - (1-\\alpha) = (1 - 10^{-4})^{40} - (1 - 0.05) $$\n$$ \\Delta = (0.9999)^{40} - 0.95 $$\nComputing the numerical value:\n$$ (0.9999)^{40} \\approx 0.996005996... $$\n$$ \\Delta \\approx 0.996005996 - 0.95 = 0.046005996... $$\nRounding this result to four significant figures, we get $0.04601$.",
            "answer": "$$\\boxed{0.04601}$$"
        }
    ]
}