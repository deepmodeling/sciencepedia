{
    "hands_on_practices": [
        {
            "introduction": "At the core of system identification lies the task of parameter estimation, where we seek to find the model parameters that best explain observed data. For the many biomedical systems that are inherently nonlinear, this requires specialized optimization techniques. This exercise guides you through the foundational derivation of the Gauss-Newton method, a cornerstone algorithm for solving nonlinear least-squares problems. By working from first principles, you will gain a deep understanding of how the algorithm linearizes the problem at each step and will analyze its convergence properties, a crucial aspect for its practical application .",
            "id": "3935343",
            "problem": "Consider a nonlinear inverse problem arising in parameter estimation for a two-compartment oxygen transport model in muscle tissue, where the measured venous oxygen concentration at sampling times is modeled by a differentiable forward map $f:\\mathbb{R}^{p}\\to\\mathbb{R}^{m}$ that depends on an unknown parameter vector $\\theta\\in\\mathbb{R}^{p}$. The data vector is $y\\in\\mathbb{R}^{m}$, and the residual is defined as $r(\\theta)=y-f(\\theta)$. The objective is a nonlinear least-squares criterion $\\phi(\\theta)=\\frac{1}{2}\\|r(\\theta)\\|_{2}^{2}$.\n\nStarting only from the definitions of the residual $r(\\theta)$, the objective $\\phi(\\theta)$, and first-order Taylor expansion for differentiable maps, derive the update direction that results from minimizing the norm of the linearized residual around a current iterate $\\theta^{(k)}$. Concretely:\n\n1. Use a first-order Taylor expansion of $r(\\theta)$ about $\\theta^{(k)}$ to obtain an approximation of $r(\\theta^{(k)}+\\Delta\\theta)$ in terms of the Jacobian $J(\\theta^{(k)})\\in\\mathbb{R}^{m\\times p}$ of $r$ at $\\theta^{(k)}$ and the increment $\\Delta\\theta$.\n2. Formulate the linearized least-squares problem in $\\Delta\\theta$ arising from this approximation, and derive the normal equations for this problem.\n3. Solve the normal equations to obtain a closed-form expression for the increment $\\Delta\\theta^{(k)}$ as a function of $J(\\theta^{(k)})$ and $r(\\theta^{(k)})$.\n4. Analyze the local convergence of the resulting iterative scheme in a neighborhood of a local minimizer $\\theta^{\\star}$ of $\\phi(\\theta)$ under the following assumptions appropriate for biomedical system identification: $r$ is twice continuously differentiable in a convex neighborhood of $\\theta^{\\star}$, the Jacobian $J(\\theta)$ is Lipschitz continuous in that neighborhood, and $J(\\theta^{\\star})$ has full column rank. Establish the qualitative convergence rates in the two cases $r(\\theta^{\\star})=0$ and $r(\\theta^{\\star})\\neq 0$, and state conditions under which the rate is quadratic or linear, respectively.\n\nYour final answer must be the single analytic expression for the increment $\\Delta\\theta^{(k)}$ obtained in part 3. No numerical evaluation is required, and no units are needed. Do not include an equality sign in the final answer. If you introduce any additional abbreviations, spell them out on first use (for example, Near-Infrared Spectroscopy (NIRS)).",
            "solution": "The problem asks for the derivation of an update step for a nonlinear least-squares problem and an analysis of the resulting algorithm's local convergence properties. The problem is well-posed, scientifically grounded in the field of biomedical system identification, and mathematically consistent. We proceed with the derivation.\n\nThe objective is to minimize the function $\\phi(\\theta) = \\frac{1}{2}\\|r(\\theta)\\|_{2}^{2}$, where $r(\\theta) = y - f(\\theta)$ is the residual vector. Here, $\\theta \\in \\mathbb{R}^{p}$ is the parameter vector, $y \\in \\mathbb{R}^{m}$ is the data vector, and $f:\\mathbb{R}^{p} \\to \\mathbb{R}^{m}$ is the forward map. We are seeking an iterative method to find a local minimum of $\\phi(\\theta)$. The method is constructed by successively solving a linearized version of the problem. Let $\\theta^{(k)}$ be the current estimate of the optimal parameter vector. We seek an increment $\\Delta\\theta$ such that $\\theta^{(k+1)} = \\theta^{(k)} + \\Delta\\theta$ is a better estimate.\n\n**1. First-Order Taylor Expansion of the Residual**\n\nThe residual function $r(\\theta)$ is a differentiable map from $\\mathbb{R}^{p}$ to $\\mathbb{R}^{m}$. We can approximate $r(\\theta)$ in the vicinity of the current iterate $\\theta^{(k)}$ using a first-order Taylor expansion. For a small increment $\\Delta\\theta$, the residual at the new point $\\theta^{(k)} + \\Delta\\theta$ is approximated as:\n$$r(\\theta^{(k)} + \\Delta\\theta) \\approx r(\\theta^{(k)}) + J_r(\\theta^{(k)}) \\Delta\\theta$$\nwhere $J_r(\\theta^{(k)}) \\in \\mathbb{R}^{m \\times p}$ is the Jacobian matrix of the residual function $r$ evaluated at $\\theta^{(k)}$. The problem statement provides the notation $J(\\theta^{(k)})$ for this Jacobian. The elements of this matrix are given by $(J_r(\\theta))_{ij} = \\frac{\\partial r_i}{\\partial \\theta_j}$.\nSince $r(\\theta) = y - f(\\theta)$ and the data vector $y$ is constant with respect to $\\theta$, the Jacobian of $r$ is the negative of the Jacobian of $f$:\n$$J(\\theta) \\equiv J_r(\\theta) = \\frac{\\partial r}{\\partial \\theta} = - \\frac{\\partial f}{\\partial \\theta}$$\nLet us denote $r_k = r(\\theta^{(k)})$ and $J_k = J(\\theta^{(k)})$. The linearized residual is then:\n$$\\tilde{r}(\\Delta\\theta) = r_k + J_k \\Delta\\theta$$\n\n**2. Linearized Least-Squares Problem and Normal Equations**\n\nThe core idea of the Gauss-Newton method is to find the increment $\\Delta\\theta$ that minimizes the squared norm of this linearized residual, $\\tilde{r}(\\Delta\\theta)$. This transforms the original nonlinear least-squares problem into a sequence of linear least-squares problems. The objective for the linearized problem is:\n$$\\min_{\\Delta\\theta} \\frac{1}{2} \\|\\tilde{r}(\\Delta\\theta)\\|_2^2 = \\min_{\\Delta\\theta} \\frac{1}{2} \\|r_k + J_k \\Delta\\theta\\|_2^2$$\nLet $\\Psi(\\Delta\\theta) = \\frac{1}{2} \\|r_k + J_k \\Delta\\theta\\|_2^2$. We can expand the squared norm:\n$$\\Psi(\\Delta\\theta) = \\frac{1}{2} (r_k + J_k \\Delta\\theta)^T (r_k + J_k \\Delta\\theta)$$\n$$\\Psi(\\Delta\\theta) = \\frac{1}{2} (r_k^T r_k + r_k^T J_k \\Delta\\theta + (\\Delta\\theta)^T J_k^T r_k + (\\Delta\\theta)^T J_k^T J_k \\Delta\\theta)$$\nSince $r_k^T J_k \\Delta\\theta$ is a scalar, it is equal to its transpose, $(\\Delta\\theta)^T J_k^T r_k$. Thus, we have:\n$$\\Psi(\\Delta\\theta) = \\frac{1}{2} (r_k^T r_k + 2 r_k^T J_k \\Delta\\theta + (\\Delta\\theta)^T J_k^T J_k \\Delta\\theta)$$\nThis is a quadratic function of $\\Delta\\theta$. To find the minimum, we differentiate $\\Psi(\\Delta\\theta)$ with respect to $\\Delta\\theta$ and set the gradient to zero. Using standard rules of matrix calculus:\n$$\\nabla_{\\Delta\\theta} \\Psi(\\Delta\\theta) = \\frac{1}{2} (0 + 2 J_k^T r_k + 2 J_k^T J_k \\Delta\\theta) = J_k^T r_k + J_k^T J_k \\Delta\\theta$$\nSetting the gradient to zero, $\\nabla_{\\Delta\\theta} \\Psi(\\Delta\\theta) = 0$, yields the **normal equations** for the linear least-squares problem:\n$$J_k^T J_k \\Delta\\theta = -J_k^T r_k$$\n\n**3. Solution for the Increment $\\Delta\\theta^{(k)}$**\n\nThe normal equations form a system of $p$ linear equations in the $p$ unknown components of $\\Delta\\theta$. The matrix $J_k^T J_k$ is a $p \\times p$ symmetric matrix. If this matrix is invertible, which is true if and only if the Jacobian $J_k$ has full column rank ($p$), we can solve for $\\Delta\\theta$ by multiplying by the inverse:\n$$(J_k^T J_k)^{-1} (J_k^T J_k) \\Delta\\theta = -(J_k^T J_k)^{-1} J_k^T r_k$$\nThis gives the closed-form expression for the update direction $\\Delta\\theta^{(k)}$ at iteration $k$:\n$$\\Delta\\theta^{(k)} = -(J(\\theta^{(k)})^T J(\\theta^{(k)}))^{-1} J(\\theta^{(k)})^T r(\\theta^{(k)})$$\n\n**4. Local Convergence Analysis**\n\nThe resulting iterative scheme is the Gauss-Newton algorithm: $\\theta^{(k+1)} = \\theta^{(k)} + \\Delta\\theta^{(k)}$. We now analyze its local convergence in a neighborhood of a local minimizer $\\theta^{\\star}$ of $\\phi(\\theta)$. A necessary condition for $\\theta^{\\star}$ to be a local minimizer is that the gradient of $\\phi(\\theta)$ vanishes at $\\theta^{\\star}$. The gradient of $\\phi(\\theta) = \\frac{1}{2}r(\\theta)^T r(\\theta)$ is $\\nabla \\phi(\\theta) = J(\\theta)^T r(\\theta)$. Thus, at the solution, $J(\\theta^{\\star})^T r(\\theta^{\\star}) = 0$.\n\nThe convergence rate of an iterative method depends on its relationship to Newton's method. Newton's method for minimizing $\\phi(\\theta)$ would use the update $\\Delta\\theta_N = -H_{\\phi}(\\theta)^{-1} \\nabla\\phi(\\theta)$, where $H_{\\phi}(\\theta)$ is the Hessian matrix of $\\phi(\\theta)$. The Hessian is:\n$$H_{\\phi}(\\theta) = \\frac{\\partial}{\\partial \\theta} (J(\\theta)^T r(\\theta)) = J(\\theta)^T J(\\theta) + \\sum_{i=1}^{m} r_i(\\theta) H_{r_i}(\\theta)$$\nwhere $r_i(\\theta)$ is the $i$-th component of the residual vector and $H_{r_i}(\\theta)$ is the Hessian matrix of $r_i(\\theta)$.\n\nComparing the Gauss-Newton update with the Newton update, we see that the Gauss-Newton method approximates the true Hessian $H_{\\phi}(\\theta)$ by the term $J(\\theta)^T J(\\theta)$, neglecting the second term involving the sum of residual components and their Hessians. The validity of this approximation determines the convergence rate. The assumptions are that $r$ is twice continuously differentiable, $J(\\theta)$ is Lipschitz continuous, and $J(\\theta^\\star)$ has full column rank. The full rank of $J(\\theta^\\star)$ ensures that $J(\\theta^{\\star})^T J(\\theta^{\\star})$ is positive definite, and by continuity, $J(\\theta^{(k)})^T J(\\theta^{(k)})$ is invertible for $\\theta^{(k)}$ sufficiently close to $\\theta^{\\star}$.\n\n**Case 1: Zero-residual problem ($r(\\theta^{\\star}) = 0$)**\nIf the model can perfectly fit the data, then at the solution, $r(\\theta^{\\star}) = 0$. In this case, the second term in the true Hessian vanishes at the solution:\n$$H_{\\phi}(\\theta^{\\star}) = J(\\theta^{\\star})^T J(\\theta^{\\star}) + \\sum_{i=1}^{m} 0 \\cdot H_{r_i}(\\theta^{\\star}) = J(\\theta^{\\star})^T J(\\theta^{\\star})$$\nThe Gauss-Newton approximation of the Hessian becomes exact at the solution. For $\\theta^{(k)}$ close to $\\theta^{\\star}$, $r(\\theta^{(k)})$ will be small, so the neglected term $\\sum r_i(\\theta^{(k)}) H_{r_i}(\\theta^{(k)})$ is also small. The Gauss-Newton method becomes asymptotically identical to Newton's method. Under the given assumptions, Newton's method exhibits local **quadratic convergence**. That is, the error $\\|\\theta^{(k+1)} - \\theta^{\\star}\\|$ is proportional to $\\|\\theta^{(k)} - \\theta^{\\star}\\|^2$ for $\\theta^{(k)}$ sufficiently close to $\\theta^{\\star}$.\n\n**Case 2: Non-zero-residual problem ($r(\\theta^{\\star}) \\neq 0$)**\nIn many practical scenarios, particularly with noisy biomedical data, the model cannot perfectly fit the data, and the residual at the minimum is non-zero, $r(\\theta^{\\star}) \\neq 0$. In this situation, the second term in the Hessian, $\\sum_{i=1}^{m} r_i(\\theta^{\\star}) H_{r_i}(\\theta^{\\star})$, is generally non-zero. The Gauss-Newton Hessian $J(\\theta)^T J(\\theta)$ is an incomplete approximation of the true Hessian $H_{\\phi}(\\theta)$ even at the solution. The method is no longer equivalent to Newton's method. The Jacobian of the iteration map $g(\\theta) = \\theta + \\Delta\\theta(\\theta)$ at the fixed point $\\theta^{\\star}$ can be shown to be non-zero. Provided that the spectral radius of this Jacobian is less than $1$, which holds if the neglected part of the Hessian is not too large compared to $J(\\theta^{\\star})^T J(\\theta^{\\star})$, the iteration will converge. However, because the Jacobian of the iteration map is non-zero, the convergence is **linear**. The rate of linear convergence depends on the magnitude of the residuals $r_i(\\theta^{\\star})$ and the nonlinearity of the model (captured by the Hessians $H_{r_i}$). For small residuals or nearly linear models, the convergence can be fast. For large residuals and highly nonlinear models, convergence can be slow or may fail.",
            "answer": "$$\\boxed{-(J(\\theta^{(k)})^T J(\\theta^{(k)}))^{-1} J(\\theta^{(k)})^T r(\\theta^{(k)})}$$"
        },
        {
            "introduction": "A frequent challenge in biomedical modeling is that the inverse problems we need to solve are ill-posed, meaning small measurement errors can cause large, unphysical variations in the estimated solution. Regularization is the essential technique for overcoming this by introducing additional information to stabilize the solution. This practice delves into the fundamental bias-variance trade-off by quantitatively comparing two classic regularization strategies: Tikhonov regularization and Truncated Singular Value Decomposition (TSVD). By deriving the expected estimation error for each method in the Singular Value Decomposition (SVD) basis, you will develop a clear, component-wise intuition for how regularization works to control noise amplification at the cost of introducing a manageable bias .",
            "id": "3935371",
            "problem": "Consider a linear inverse problem arising from dynamic tracer kinetic modeling of organ perfusion, where a known forward operator maps a discretized tissue impulse response to a measured concentration-time curve. After linearization around a baseline operating point, the model is $$y = A x + \\epsilon,$$ where $A \\in \\mathbb{R}^{m \\times n}$ is ill-conditioned, $x \\in \\mathbb{R}^{n}$ is the unknown parameter vector encoding the impulse response coefficients, $y \\in \\mathbb{R}^{m}$ is the measurement vector, and $\\epsilon \\sim \\mathcal{N}(0,\\sigma^{2} I_{m})$ is white Gaussian measurement noise with variance $\\sigma^{2}$. Let the Singular Value Decomposition (SVD) of $A$ be $A = U \\Sigma V^{\\top}$, where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ has diagonal entries $s_{1} \\ge s_{2} \\ge \\cdots \\ge s_{n}  0$, the singular values of $A$. Expand the true parameter vector in the right-singular vector basis as $$x = \\sum_{i=1}^{n} x_{i} v_{i},$$ where $v_{i}$ is the $i$-th column of $V$ and $x_{i} \\in \\mathbb{R}$ are the expansion coefficients.\n\nTwo regularized estimators are considered:\n- Tikhonov regularization with parameter $\\lambda  0$, defined by $$\\hat{x}_{\\mathrm{Tik}}(\\lambda) = \\arg\\min_{x \\in \\mathbb{R}^{n}} \\|A x - y\\|_{2}^{2} + \\lambda \\|x\\|_{2}^{2}.$$\n- Truncated Singular Value Decomposition (TSVD) with truncation index $k \\in \\{1,2,\\ldots,n\\}$, defined by projecting $y$ onto the span of the first $k$ left-singular vectors and inverting only the first $k$ singular components.\n\nStarting from the model $y = A x + \\epsilon$, the orthogonality of $U$ and $V$, and the properties of white Gaussian noise, derive the expected squared estimation error $$\\mathbb{E}\\big\\|\\hat{x}_{\\mathrm{Tik}}(\\lambda) - x\\big\\|_{2}^{2} \\quad \\text{and} \\quad \\mathbb{E}\\big\\|\\hat{x}_{\\mathrm{TSVD}}(k) - x\\big\\|_{2}^{2}$$ in terms of $\\{s_{i}\\}_{i=1}^{n}$, $\\{x_{i}\\}_{i=1}^{n}$, $\\sigma^{2}$, $\\lambda$, and $k$. Use a bias-variance decomposition that is consistent with the filtered SVD representation of linear estimators diagonalized by the Singular Value Decomposition (SVD). Then analyze, in terms of each singular component index $i$, how the bias and variance contributions differ between Tikhonov and Truncated Singular Value Decomposition (TSVD), and identify index-dependent regimes where TSVD yields smaller expected error than Tikhonov.\n\nFinally, define the difference $$\\Delta(\\lambda,k) = \\mathbb{E}\\big\\|\\hat{x}_{\\mathrm{Tik}}(\\lambda) - x\\big\\|_{2}^{2} - \\mathbb{E}\\big\\|\\hat{x}_{\\mathrm{TSVD}}(k) - x\\big\\|_{2}^{2}.$$ Provide a single closed-form analytic expression for $\\Delta(\\lambda,k)$ explicitly as a function of $\\{s_{i}\\}_{i=1}^{n}$, $\\{x_{i}\\}_{i=1}^{n}$, $\\sigma^{2}$, $\\lambda$, and $k$. Express the final answer as a symbolic expression without units.",
            "solution": "The problem as stated is scientifically sound, well-posed, objective, and self-contained. It presents a standard, non-trivial problem in the field of inverse problem theory and system identification. All necessary definitions, conditions, and data are provided for a rigorous mathematical derivation. Therefore, the problem is valid, and a solution can be derived.\n\nThe problem requires the derivation and comparison of the expected mean squared error (MSE) for two regularized estimators, Tikhonov and Truncated Singular Value Decomposition (TSVD), for a linear inverse problem. We begin by transforming the problem into the basis defined by the Singular Value Decomposition (SVD) of the matrix $A$.\n\nThe model is given by $y = Ax + \\epsilon$, where $A = U \\Sigma V^{\\top}$ and $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I_m)$. We multiply the model equation by $U^{\\top}$:\n$$U^{\\top} y = U^{\\top} (U \\Sigma V^{\\top}) x + U^{\\top} \\epsilon = \\Sigma (V^{\\top} x) + U^{\\top} \\epsilon$$\nLet us define the transformed vectors:\n- $\\tilde{y} = U^{\\top} y \\in \\mathbb{R}^{m}$\n- $\\tilde{x} = V^{\\top} x \\in \\mathbb{R}^{n}$\n- $\\tilde{\\epsilon} = U^{\\top} \\epsilon \\in \\mathbb{R}^{m}$\n\nThe components of $\\tilde{x}$ are given by $\\tilde{x}_i = v_i^{\\top} x$. The problem statement denotes these coefficients as $x_i$, so we will adopt this notation, i.e., $\\tilde{x} = (x_1, x_2, \\ldots, x_n)^{\\top}$. The transformed noise vector $\\tilde{\\epsilon}$ has mean $\\mathbb{E}[\\tilde{\\epsilon}] = U^{\\top}\\mathbb{E}[\\epsilon] = 0$ and covariance $\\mathbb{E}[\\tilde{\\epsilon}\\tilde{\\epsilon}^{\\top}] = U^{\\top}\\mathbb{E}[\\epsilon\\epsilon^{\\top}]U = U^{\\top}(\\sigma^2 I_m)U = \\sigma^2 I_m$. Thus, the components $\\tilde{\\epsilon}_i$ are uncorrelated with variance $\\sigma^2$.\n\nThe model in the SVD basis decomposes into a set of scalar equations:\n$$\\tilde{y}_i = s_i x_i + \\tilde{\\epsilon}_i \\quad \\text{for } i=1, \\ldots, n$$\n$$\\tilde{y}_i = \\tilde{\\epsilon}_i \\quad \\text{for } i=n+1, \\ldots, m$$\nThe squared estimation error is invariant under the orthogonal transformation by $V$, i.e., $\\|\\hat{x} - x\\|_2^2 = \\|V^{\\top}(\\hat{x} - x)\\|_2^2 = \\|\\hat{\\tilde{x}} - \\tilde{x}\\|_2^2 = \\sum_{i=1}^n (\\hat{x}_i - x_i)^2$. We can therefore compute the total MSE by summing the MSE for each component.\n\nThe MSE for an estimator $\\hat{x}_i$ of a single component $x_i$ is given by the sum of its squared bias and variance:\n$$\\mathbb{E}[(\\hat{x}_i - x_i)^2] = \\left(\\mathbb{E}[\\hat{x}_i] - x_i\\right)^2 + \\mathbb{E}\\left[(\\hat{x}_i - \\mathbb{E}[\\hat{x}_i])^2\\right]$$\n\nMost linear estimators in this context can be described by filter factors, $f_i$, applied to the naive solution. The estimated coefficients are given by $\\hat{x}_i = f_i \\frac{\\tilde{y}_i}{s_i} = f_i(x_i + \\frac{\\tilde{\\epsilon}_i}{s_i})$. The component-wise MSE is then:\n$$\\mathbb{E}[(\\hat{x}_i - x_i)^2] = \\mathbb{E}\\left[\\left(f_i x_i + f_i \\frac{\\tilde{\\epsilon}_i}{s_i} - x_i\\right)^2\\right] = \\mathbb{E}\\left[\\left((f_i - 1)x_i + f_i \\frac{\\tilde{\\epsilon}_i}{s_i}\\right)^2\\right]$$\nSince $\\mathbb{E}[\\tilde{\\epsilon}_i] = 0$, the cross-term vanishes, and we get the general expression for per-component MSE:\n$$\\mathrm{MSE}_i = \\underbrace{(f_i - 1)^2 x_i^2}_{\\text{squared bias}} + \\underbrace{\\frac{f_i^2 \\sigma^2}{s_i^2}}_{\\text{variance}}$$\n\n**Tikhonov Regularization**\nThe Tikhonov estimator minimizes $\\|Ax - y\\|_2^2 + \\lambda \\|x\\|_2^2$. In the SVD basis, this is equivalent to minimizing $\\sum_{i=1}^n (s_i x_i - \\tilde{y}_i)^2 + \\lambda \\sum_{i=1}^n x_i^2$. This yields the component-wise solution $\\hat{x}_{i, \\mathrm{Tik}} = \\frac{s_i}{s_i^2 + \\lambda} \\tilde{y}_i$. The corresponding filter factors are $f_i^{\\mathrm{Tik}} = \\frac{s_i^2}{s_i^2 + \\lambda}$.\n\nThe per-component MSE for Tikhonov regularization is:\n- Squared bias: $\\left(\\frac{s_i^2}{s_i^2 + \\lambda} - 1\\right)^2 x_i^2 = \\left(\\frac{-\\lambda}{s_i^2 + \\lambda}\\right)^2 x_i^2 = \\frac{\\lambda^2 x_i^2}{(s_i^2 + \\lambda)^2}$\n- Variance: $\\left(\\frac{s_i^2}{s_i^2 + \\lambda}\\right)^2 \\frac{\\sigma^2}{s_i^2} = \\frac{s_i^2 \\sigma^2}{(s_i^2 + \\lambda)^2}$\n\nThe total expected squared estimation error for the Tikhonov estimator is the sum over all components:\n$$\\mathbb{E}\\big\\|\\hat{x}_{\\mathrm{Tik}}(\\lambda) - x\\big\\|_{2}^{2} = \\sum_{i=1}^{n} \\left[ \\frac{\\lambda^2 x_i^2}{(s_i^2 + \\lambda)^2} + \\frac{s_i^2 \\sigma^2}{(s_i^2 + \\lambda)^2} \\right] = \\sum_{i=1}^{n} \\frac{\\lambda^2 x_i^2 + s_i^2 \\sigma^2}{(s_i^2 + \\lambda)^2}$$\n\n**Truncated Singular Value Decomposition (TSVD)**\nThe TSVD estimator with truncation index $k$ effectively sets the filter factors to $1$ for the first $k$ components and to $0$ for the remaining components.\n$$f_i^{\\mathrm{TSVD}} = \\begin{cases} 1  \\text{if } i \\le k \\\\ 0  \\text{if } i  k \\end{cases}$$\nWe analyze the MSE in two parts:\n\nFor $i \\in \\{1, \\ldots, k\\}$: $f_i = 1$.\n- Squared bias: $(1 - 1)^2 x_i^2 = 0$. The estimator is unbiased for these components.\n- Variance: $\\frac{1^2 \\sigma^2}{s_i^2} = \\frac{\\sigma^2}{s_i^2}$. The variance is amplified, especially for small $s_i$.\n\nFor $i \\in \\{k+1, \\ldots, n\\}$: $f_i = 0$.\n- Squared bias: $(0 - 1)^2 x_i^2 = x_i^2$. The error is purely due to the bias of truncating the component to zero.\n- Variance: $\\frac{0^2 \\sigma^2}{s_i^2} = 0$. There is no noise propagation for truncated components.\n\nThe total expected squared estimation error for the TSVD estimator is:\n$$\\mathbb{E}\\big\\|\\hat{x}_{\\mathrm{TSVD}}(k) - x\\big\\|_{2}^{2} = \\sum_{i=1}^{k} \\frac{\\sigma^2}{s_i^2} + \\sum_{i=k+1}^{n} x_i^2$$\n\n**Comparison and Analysis**\n\nFor each singular component $i$, the bias and variance contributions are as follows:\n- **For $i \\le k$ (included components):**\n  - Tikhonov: Introduces a bias $\\frac{-\\lambda x_i}{s_i^2 + \\lambda}$ but reduces the variance from $\\sigma^2/s_i^2$ to $\\frac{s_i^4 \\sigma^2}{(s_i^2 + \\lambda)^2 s_i^2}$. Tikhonov always has smaller variance than TSVD for these components.\n  - TSVD: Is unbiased but has a larger variance contribution of $\\sigma^2/s_i^2$.\n  - TSVD yields a smaller expected error for component $i \\le k$ if its MSE is less than Tikhonov's MSE: $\\frac{\\sigma^2}{s_i^2}  \\frac{\\lambda^2 x_i^2 + s_i^2 \\sigma^2}{(s_i^2 + \\lambda)^2}$. This simplifies to $s_i^2 \\lambda x_i^2  2\\sigma^2 s_i^2 + \\sigma^2 \\lambda$. This regime corresponds to components with a high signal-to-noise ratio ($x_i^2/\\sigma^2$), where the zero bias of TSVD outweighs its higher variance.\n\n- **For $i  k$ (filtered/truncated components):**\n  - Tikhonov: Smoothly filters the component, resulting in a bias of $\\frac{-\\lambda x_i}{s_i^2 + \\lambda}$ and a variance of $\\frac{s_i^2 \\sigma^2}{(s_i^2 + \\lambda)^2}$. The bias is smaller in magnitude than the TSVD bias, $|-\\lambda x_i / (s_i^2+\\lambda)|  |x_i|$.\n  - TSVD: Abruptly truncates the component to zero, leading to a large bias of $-x_i$ and zero variance.\n  - TSVD yields a smaller expected error for component $i  k$ if $x_i^2  \\frac{\\lambda^2 x_i^2 + s_i^2 \\sigma^2}{(s_i^2 + \\lambda)^2}$. This simplifies to $x_i^2(s_i^2 + 2\\lambda)  \\sigma^2$. This regime corresponds to components where the true signal power $x_i^2$ is negligible compared to the noise level $\\sigma^2$. In such cases, truncating to zero (TSVD) is a better strategy than estimating a small, noisy value (Tikhonov). Typically, for components with non-negligible $x_i$, Tikhonov is superior due to its much smaller bias error.\n\n**Derivation of $\\Delta(\\lambda, k)$**\nFinally, we compute the difference in the total expected squared errors as requested.\n$$\\Delta(\\lambda,k) = \\mathbb{E}\\big\\|\\hat{x}_{\\mathrm{Tik}}(\\lambda) - x\\big\\|_{2}^{2} - \\mathbb{E}\\big\\|\\hat{x}_{\\mathrm{TSVD}}(k) - x\\big\\|_{2}^{2}$$\nSubstituting the derived expressions for the total MSEs:\n$$\\Delta(\\lambda,k) = \\left( \\sum_{i=1}^{n} \\frac{\\lambda^2 x_i^2 + s_i^2 \\sigma^2}{(s_i^2 + \\lambda)^2} \\right) - \\left( \\sum_{i=1}^{k} \\frac{\\sigma^2}{s_i^2} + \\sum_{i=k+1}^{n} x_i^2 \\right)$$\nThis expression can be split according to the summation ranges of the TSVD error:\n$$\\Delta(\\lambda,k) = \\left( \\sum_{i=1}^{k} \\frac{\\lambda^2 x_i^2 + s_i^2 \\sigma^2}{(s_i^2 + \\lambda)^2} + \\sum_{i=k+1}^{n} \\frac{\\lambda^2 x_i^2 + s_i^2 \\sigma^2}{(s_i^2 + \\lambda)^2} \\right) - \\left( \\sum_{i=1}^{k} \\frac{\\sigma^2}{s_i^2} + \\sum_{i=k+1}^{n} x_i^2 \\right)$$\nCombining the sums over the same indices provides the final analytical expression:\n$$\\Delta(\\lambda,k) = \\sum_{i=1}^{k} \\left[ \\frac{\\lambda^2 x_i^2 + s_i^2 \\sigma^2}{(s_i^2 + \\lambda)^2} - \\frac{\\sigma^2}{s_i^2} \\right] + \\sum_{i=k+1}^{n} \\left[ \\frac{\\lambda^2 x_i^2 + s_i^2 \\sigma^2}{(s_i^2 + \\lambda)^2} - x_i^2 \\right]$$\nThis is the required closed-form expression for the difference in expected squared error between the Tikhonov and TSVD estimators.",
            "answer": "$$\\boxed{\\left(\\sum_{i=1}^{n} \\frac{\\lambda^{2} x_{i}^{2} + s_{i}^{2} \\sigma^{2}}{(s_{i}^{2} + \\lambda)^{2}}\\right) - \\left(\\sum_{i=1}^{k} \\frac{\\sigma^{2}}{s_{i}^{2}} + \\sum_{i=k+1}^{n} x_{i}^{2}\\right)}$$"
        },
        {
            "introduction": "After identifying a model and estimating its parameters, the crucial final step is model validation: how can we be sure our model has captured the essential dynamics of the system? The most powerful tool for this is residual analysis. If a model is adequate, the residuals—the discrepancies between the model's predictions and the actual data—should behave like random, unpredictable noise. This practical coding exercise will teach you how to implement and interpret key statistical diagnostics, such as the sample autocorrelation function and the Ljung-Box test, to formally check if the residuals from a model fit are \"white\". This skill is indispensable for rigorously assessing model quality and identifying opportunities for improvement .",
            "id": "3935335",
            "problem": "You are given a cardiovascular residual time-series analysis task in the setting of model validation for discrete-time system identification. Assume a linear, causal, discrete-time model is fitted to a measured cardiovascular signal (for example, arterial pressure in millimeters of mercury), and denote the residual sequence by $e_t = y_t - \\hat{y}_t$, where $y_t$ is the measured output and $\\hat{y}_t$ is the one-step-ahead prediction at discrete time $t$. Under correct model specification and standard assumptions for linear systems, the residuals should be a realization of a discrete-time, wide-sense stationary, zero-mean, independent white-noise process with constant variance. Your task is to implement residual diagnostics grounded in fundamental definitions and inferential testing to detect departures from this ideal.\n\nStarting from the following fundamental base:\n- The residual sequence $e_t$ is defined as the difference between the measured output and the model prediction, $e_t = y_t - \\hat{y}_t$.\n- Under the null hypothesis of correct model specification, residuals form a zero-mean, independent, identically distributed sequence with constant variance. Stationarity implies that the autocovariance $\\gamma_k = \\mathbb{E}[(e_t - \\mu)(e_{t-k} - \\mu)]$ depends only on lag $k$, with $\\mu = \\mathbb{E}[e_t]$.\n- The autocorrelation function is defined by $\\rho_k = \\gamma_k/\\gamma_0$ for lag $k$, with $\\gamma_0$ the variance.\n- The Ljung–Box portmanteau statistic aggregates information across multiple lags to test the joint hypothesis that $\\rho_1 = \\rho_2 = \\dots = \\rho_h = 0$.\n\nImplement a program that performs the following for each residual time series in a test suite:\n1. Compute the sample autocorrelation function up to a specified maximum lag $h$ using the canonical estimator based on the sample mean and sample variance of $e_t$.\n2. Compute the Ljung–Box portmanteau statistic at maximum lag $h$ and its associated $p$-value under the large-sample $\\chi^2$ approximation for the null hypothesis of whiteness.\n3. Decide whether the residuals indicate misspecification by combining two diagnostics:\n   - A joint-test decision based on the Ljung–Box $p$-value compared against a significance level $\\alpha$.\n   - A marginal diagnostic based on whether any sample autocorrelation magnitude exceeds its large-sample confidence bound derived from the standard normal approximation.\n4. Output, for each test case, a triple consisting of the Ljung–Box statistic, the associated $p$-value, and a boolean detection decision, where the boolean is true if misspecification is detected and false otherwise. All reported values are dimensionless.\n\nUse the following scientifically plausible, self-consistent residual-generating test suite that emulates distinct cardiovascular modeling contexts. In each case, residuals are specified in millimeters of mercury, but diagnostics are dimensionless. Let the significance level be $\\alpha = 0.05$, and define the maximum lag as $h = \\min(20, \\lfloor n/4 \\rfloor)$, where $n$ is the sample size for the residual sequence. For approximate single-lag confidence bounds, use the two-sided standard normal quantile $z_{0.975}$.\n\nTest suite (each case provides all parameters needed to generate residuals):\n- Case $1$ (well-specified, independent noise): $n = 512$, residuals are independent Gaussian with zero mean and standard deviation $\\sigma = 5$ $\\mathrm{mmHg}$, random seed $12345$.\n- Case $2$ (misspecification via first-order autoregression): $n = 512$, residuals follow an autoregressive process $e_t = \\phi e_{t-1} + \\epsilon_t$ with $\\phi = 0.6$, $\\epsilon_t$ independent Gaussian with zero mean and standard deviation $\\sigma = 5$ $\\mathrm{mmHg}$, random seed $54321$.\n- Case $3$ (misspecification via oscillatory component, e.g., respiratory sinus arrhythmia): $n = 500$, sampling frequency $f_s = 10$ $\\mathrm{Hz}$, residuals $e_t = A \\sin(2\\pi f_0 t/f_s) + \\epsilon_t$ with amplitude $A = 3$ $\\mathrm{mmHg}$ and frequency $f_0 = 0.25$ $\\mathrm{Hz}$, $\\epsilon_t$ independent Gaussian with zero mean and standard deviation $\\sigma = 2$ $\\mathrm{mmHg}$, random seed $24680$.\n- Case $4$ (small-sample misspecification): $n = 40$, autoregressive residuals $e_t = \\phi e_{t-1} + \\epsilon_t$ with $\\phi = 0.4$, $\\epsilon_t$ independent Gaussian with zero mean and standard deviation $\\sigma = 5$ $\\mathrm{mmHg}$, random seed $11111$.\n- Case $5$ (borderline weak correlation): $n = 200$, autoregressive residuals $e_t = \\phi e_{t-1} + \\epsilon_t$ with $\\phi = 0.2$, $\\epsilon_t$ independent Gaussian with zero mean and standard deviation $\\sigma = 5$ $\\mathrm{mmHg}$, random seed $22222$.\n\nFor each test case, compute:\n- The sample autocorrelation sequence $\\{r_k\\}_{k=1}^h$.\n- The Ljung–Box statistic at lag $h$ and its $p$-value under the null.\n- A detection decision that is true if either the Ljung–Box $p$-value is less than $\\alpha$ or any $|r_k|$ exceeds the approximate large-sample bound $z_{0.975}/\\sqrt{n}$, and false otherwise.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to a test case and is itself a list of three values in the order $[Q, p, decision]$. For example, the output format must be like $[[Q_1,p_1,\\mathrm{decision}_1],[Q_2,p_2,\\mathrm{decision}_2],\\dots]$, with all numeric values printed in standard decimal notation and booleans as either true or false.",
            "solution": "The problem statement has been critically validated and is deemed valid. It is scientifically grounded in the principles of time-series analysis and system identification, is well-posed with all necessary information provided, and is free of ambiguities or contradictions.\n\nThe task is to perform residual diagnostics on five distinct time series to assess a hypothetical underlying model for misspecification. The core principle is that if a linear model is correctly specified, the residuals $e_t = y_t - \\hat{y}_t$ should constitute a zero-mean white noise process. Departures from this ideal, such as significant autocorrelation, indicate that the model has failed to capture the full dynamics of the system. We will implement two standard statistical tests to detect such departures: the Ljung–Box portmanteau test for joint autocorrelation and a marginal test on individual autocorrelation coefficients.\n\nFirst, we generate the five residual sequences as specified. For each sequence of length $n$, the maximum lag for analysis is set to $h = \\min(20, \\lfloor n/4 \\rfloor)$.\n\n-   **Case 1 (Well-specified, independent noise)**: $n=512$, $h=20$. The residuals $e_t$ are a sequence of $512$ independent and identically distributed (i.i.d.) random variables from a normal distribution $\\mathcal{N}(0, 5^2)$. This represents the null hypothesis ($H_0$) scenario where the model is correctly specified.\n-   **Case 2 (Misspecification via autoregression)**: $n=512$, $h=20$. The residuals follow a first-order autoregressive (AR(1)) process $e_t = 0.6 e_{t-1} + \\epsilon_t$, where $\\epsilon_t \\sim \\mathcal{N}(0, 5^2)$. The non-zero coefficient $\\phi=0.6$ introduces serial correlation, which should be detected as a model misspecification.\n-   **Case 3 (Misspecification via oscillatory component)**: $n=500$, $h=20$. The residuals are generated by $e_t = 3 \\sin(2\\pi (0.25) t/10) + \\epsilon_t$, with $\\epsilon_t \\sim \\mathcal{N}(0, 2^2)$. This simulates an unmodeled periodic component, such as respiratory sinus arrhythmia influencing a cardiovascular signal, which will manifest as a periodic pattern in the autocorrelation function.\n-   **Case 4 (Small-sample misspecification)**: $n=40$, $h=10$. This is another AR(1) process, $e_t = 0.4 e_{t-1} + \\epsilon_t$, but with a very small sample size. This case tests the power of the diagnostics under less-than-ideal, small-sample conditions.\n-   **Case 5 (Borderline weak correlation)**: $n=200$, $h=20$. This AR(1) case, $e_t = 0.2 e_{t-1} + \\epsilon_t$, has a weak correlation coefficient $\\phi=0.2$. It tests the sensitivity of the diagnostics to subtle model misspecifications.\n\nFor each generated sequence $\\{e_t\\}_{t=1}^n$, we perform the following calculations:\n\n1.  **Sample Autocorrelation Function (ACF)**: The foundation of the analysis is the sample ACF, denoted $\\{r_k\\}$. It is an estimate of the true autocorrelation function $\\rho_k = \\mathrm{Corr}(e_t, e_{t-k})$. The calculation proceeds as follows:\n    -   Compute the sample mean: $\\bar{e} = \\frac{1}{n} \\sum_{t=1}^{n} e_t$.\n    -   Compute the sample autocovariance at lag $k$: $\\hat{\\gamma}_k = \\frac{1}{n} \\sum_{t=k+1}^{n} (e_t - \\bar{e})(e_{t-k} - \\bar{e})$, for $k = 0, 1, \\dots, h$. The term $\\hat{\\gamma}_0$ is the sample variance.\n    -   Compute the sample autocorrelation at lag $k$: $r_k = \\frac{\\hat{\\gamma}_k}{\\hat{\\gamma}_0}$.\n\n2.  **Marginal Significance Test**: Under the null hypothesis that the residuals are white noise, the sample autocorrelations $r_k$ for lags $k0$ are approximately normally distributed with mean $0$ and variance $1/n$. We can thus test the significance of each $r_k$ individually. A two-sided test at significance level $\\alpha=0.05$ rejects $H_0$ if $|r_k|$ exceeds a critical value. This value is derived from the standard normal distribution quantile $z_{1-\\alpha/2}$. For $\\alpha=0.05$, the critical value is $z_{0.975} \\approx 1.96$. A misspecification is flagged if for any $k \\in \\{1, \\dots, h\\}$:\n    $$ |r_k|  \\frac{z_{0.975}}{\\sqrt{n}} $$\n\n3.  **Ljung–Box Portmanteau Test**: This test provides a single, joint assessment of the autocorrelations up to lag $h$. The null hypothesis is $H_0: \\rho_1 = \\rho_2 = \\dots = \\rho_h = 0$. The Ljung–Box statistic, $Q$, is defined as:\n    $$ Q = n(n+2) \\sum_{k=1}^{h} \\frac{r_k^2}{n-k} $$\n    Under $H_0$, $Q$ follows approximately a chi-squared distribution with $h$ degrees of freedom, denoted $\\chi^2(h)$. The $p$-value is the probability of observing a statistic at least as extreme as the computed $Q$, i.e., $p = P(\\chi^2(h) \\geq Q)$. A small $p$-value (less than the significance level $\\alpha=0.05$) provides evidence against the null hypothesis, suggesting that at least one $\\rho_k$ is non-zero.\n\n4.  **Combined Decision**: The final decision on model misspecification is a boolean (`true`/`false`). A misspecification is detected if either the Ljung-Box test is significant or at least one individual autocorrelation coefficient is significant. That is, the decision is `true` if:\n    $$ (p\\text{-value}  0.05) \\quad \\lor \\quad (\\exists k \\in \\{1, \\dots, h\\} \\text{ s.t. } |r_k|  z_{0.975}/\\sqrt{n}) $$\n    Otherwise, the decision is `false`.\n\nThis procedure is systematically applied to each of the five test cases, and the resulting Ljung-Box statistic ($Q$), its $p$-value, and the final boolean decision are reported. The implementation uses `numpy` for numerical operations and time-series generation, and `scipy.stats` to access the cumulative distribution functions for the $\\chi^2$ and normal distributions required for calculating the $p$-value and critical values.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats\n\ndef generate_residuals(case_params):\n    \"\"\"Generates a residual time series based on the specified case parameters.\"\"\"\n    n, case_type, params, seed = case_params\n    rng = np.random.default_rng(seed)\n\n    if case_type == 'iid_gaussian':\n        sigma = params['sigma']\n        e = rng.normal(loc=0.0, scale=sigma, size=n)\n    elif case_type == 'ar1':\n        phi, sigma = params['phi'], params['sigma']\n        epsilon = rng.normal(loc=0.0, scale=sigma, size=n)\n        e = np.zeros(n)\n        e[0] = epsilon[0]\n        for t in range(1, n):\n            e[t] = phi * e[t-1] + epsilon[t]\n    elif case_type == 'sinusoid':\n        A, f0, fs, sigma = params['A'], params['f0'], params['fs'], params['sigma']\n        t_vec = np.arange(n)\n        epsilon = rng.normal(loc=0.0, scale=sigma, size=n)\n        e = A * np.sin(2 * np.pi * f0 * t_vec / fs) + epsilon\n    \n    return e\n\ndef perform_residual_diagnostics(e, alpha=0.05):\n    \"\"\"\n    Performs residual diagnostics on a time series e.\n    Returns the Ljung-Box statistic, its p-value, and a boolean decision.\n    \"\"\"\n    n = len(e)\n    h = min(20, n // 4)\n\n    # Compute sample mean\n    mean_e = np.mean(e)\n\n    # Compute sample autocovariances and autocorrelations\n    # Use a direct, formula-based implementation for clarity\n    centered_e = e - mean_e\n    acov = np.zeros(h + 1)\n    for k in range(h + 1):\n        acov[k] = np.dot(centered_e[k:], centered_e[:n-k]) / n\n    \n    # Avoid division by zero if variance is zero (highly unlikely for these cases)\n    if acov[0] == 0:\n        return [np.nan, np.nan, True]\n\n    acorr = acov / acov[0]\n    r_k = acorr[1:] # We need r_1, ..., r_h\n\n    # --- Ljung-Box Test ---\n    k_vals = np.arange(1, h + 1)\n    lb_stat = n * (n + 2) * np.sum(r_k**2 / (n - k_vals))\n    \n    # p-value from chi-squared distribution with h degrees of freedom\n    p_value = stats.chi2.sf(lb_stat, h)\n    \n    decision_ljung_box = p_value  alpha\n\n    # --- Individual Lag Test ---\n    z_crit = stats.norm.ppf(1 - alpha / 2)\n    conf_bound = z_crit / np.sqrt(n)\n    decision_individual_lags = np.any(np.abs(r_k)  conf_bound)\n\n    # --- Combined Decision ---\n    final_decision = decision_ljung_box or decision_individual_lags\n    \n    return [lb_stat, p_value, final_decision]\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis for all test cases and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, type, params, seed)\n        (512, 'iid_gaussian', {'sigma': 5.0}, 12345),\n        (512, 'ar1', {'phi': 0.6, 'sigma': 5.0}, 54321),\n        (500, 'sinusoid', {'A': 3.0, 'f0': 0.25, 'fs': 10.0, 'sigma': 2.0}, 24680),\n        (40, 'ar1', {'phi': 0.4, 'sigma': 5.0}, 11111),\n        (200, 'ar1', {'phi': 0.2, 'sigma': 5.0}, 22222)\n    ]\n\n    all_results = []\n    for case_params in test_cases:\n        residuals = generate_residuals(case_params)\n        result_triplet = perform_residual_diagnostics(residuals, alpha=0.05)\n        all_results.append(result_triplet)\n\n    # Format the output as specified in the problem statement\n    formatted_results = []\n    for res in all_results:\n        q_val_str = str(res[0])\n        p_val_str = str(res[1])\n        # The problem requires lowercase boolean strings 'true' or 'false'\n        decision_str = str(res[2]).lower()\n        formatted_results.append(f\"[{q_val_str},{p_val_str},{decision_str}]\")\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}