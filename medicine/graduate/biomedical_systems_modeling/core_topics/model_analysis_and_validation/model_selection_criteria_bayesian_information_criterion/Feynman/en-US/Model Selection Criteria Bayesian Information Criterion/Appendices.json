{
    "hands_on_practices": [
        {
            "introduction": "The Bayesian Information Criterion provides a principled way to balance a model's fit to the data against its complexity. This first exercise () grounds the abstract definition of BIC in a concrete calculation for two nested linear regression models. By working from the fundamental Gaussian likelihood, you will compute BIC values and explore analytically how the sample size $n$ plays a crucial role in the trade-off, a key property that distinguishes BIC from other criteria.",
            "id": "4928639",
            "problem": "A biostatistician is comparing two nested Gaussian linear regression models for a continuous biomarker $Y$ under the standard assumptions of independent Gaussian errors with unknown variance. Model $\\mathcal{M}_{0}$ includes an intercept, age, and sex (so the mean structure has $p_{0}=3$ regression parameters), while model $\\mathcal{M}_{1}$ augments $\\mathcal{M}_{0}$ by adding treatment and an age-by-treatment interaction (so the mean structure has $p_{1}=5$ regression parameters). Both models include a single residual variance parameter.\n\nOn a study with $n=180$ independent participants, the residual sums of squares after ordinary least squares fitting are $\\mathrm{RSS}_{0}=162.0$ for $\\mathcal{M}_{0}$ and $\\mathrm{RSS}_{1}=155.0$ for $\\mathcal{M}_{1}$.\n\n(a) Using only first principles for the Gaussian likelihood and the definition of the Bayesian Information Criterion (BIC), compute the BIC for each model, taking the total number of free parameters for $\\mathcal{M}_{j}$ to be $k_{j}=p_{j}+1$ (the $p_{j}$ regression coefficients including the intercept, plus the residual variance). Report the two BIC values rounded to four significant figures.\n\n(b) Now consider increasing the sample size by repeatedly sampling from the same data-generating process. Assume that, as $n$ grows, the per-observation residual sum of squares stabilizes so that $\\mathrm{RSS}_{0}\\approx n r_{0}$ and $\\mathrm{RSS}_{1}\\approx n r_{1}$ with constants $r_{0}=0.90$ and $r_{1}=0.86$. Let $\\Delta k = k_{1}-k_{0}$. Derive, from first principles, an exact closed-form expression (you may introduce special functions if needed) for the minimal sample size $n_{\\ast}$ such that for all $n \\ge n_{\\ast}$ the BIC of $\\mathcal{M}_{1}$ is strictly smaller than that of $\\mathcal{M}_{0}$. Express $n_{\\ast}$ in terms of $r_{0}$, $r_{1}$, and $\\Delta k$, and then specialize your expression to the numerical values above.\n\nRound any intermediate numerical BIC values in part (a) to four significant figures. For the final deliverable, provide the exact analytic expression for $n_{\\ast}$ from part (b); do not approximate or round the expression. The final answer must be a single closed-form analytic expression.",
            "solution": "We begin with the Gaussian linear model with independent errors. Under model $\\mathcal{M}_{j}$, we posit\n$$\nY \\sim \\mathcal{N}\\!\\left(X_{j}\\beta_{j},\\,\\sigma^{2} I_{n}\\right),\n$$\nwith $k_{j}$ free parameters consisting of $p_{j}$ regression coefficients (including the intercept) and the residual variance $\\sigma^{2}$. The maximized Gaussian log-likelihood for linear regression with unknown variance is obtained by substituting the maximum likelihood estimators $\\hat{\\beta}_{j}$ and $\\hat{\\sigma}^{2}_{j}=\\mathrm{RSS}_{j}/n$ into the log-likelihood. The resulting maximized log-likelihood is\n$$\n\\hat{\\ell}_{j}\n\\,=\\, -\\frac{n}{2}\\left[\\ln(2\\pi) + 1 + \\ln\\!\\left(\\frac{\\mathrm{RSS}_{j}}{n}\\right)\\right].\n$$\nThe Bayesian Information Criterion (BIC) is defined by\n$$\n\\mathrm{BIC}_{j} \\,=\\, -2\\,\\hat{\\ell}_{j} \\;+\\; k_{j}\\,\\ln n\n\\,=\\, n\\left[\\ln(2\\pi)+1+\\ln\\!\\left(\\frac{\\mathrm{RSS}_{j}}{n}\\right)\\right] \\;+\\; k_{j}\\,\\ln n.\n$$\nPart (a). For $\\mathcal{M}_{0}$, we have $p_{0}=3$, hence $k_{0}=p_{0}+1=4$. For $\\mathcal{M}_{1}$, $p_{1}=5$, hence $k_{1}=6$. With $n=180$, $\\mathrm{RSS}_{0}=162.0$, and $\\mathrm{RSS}_{1}=155.0$, compute\n$$\n\\ln n \\,=\\, \\ln(180),\n\\quad\n\\frac{\\mathrm{RSS}_{0}}{n} \\,=\\, \\frac{162.0}{180} \\,=\\, 0.90,\n\\quad\n\\frac{\\mathrm{RSS}_{1}}{n} \\,=\\, \\frac{155.0}{180} \\,=\\, \\frac{31}{36}.\n$$\nWe evaluate\n$$\n-2\\,\\hat{\\ell}_{0}\n= n\\left[\\ln(2\\pi)+1+\\ln\\!\\left(\\frac{\\mathrm{RSS}_{0}}{n}\\right)\\right]\n= 180\\left[\\ln(2\\pi)+1+\\ln(0.90)\\right],\n$$\nand\n$$\n-2\\,\\hat{\\ell}_{1}\n= 180\\left[\\ln(2\\pi)+1+\\ln\\!\\left(\\frac{31}{36}\\right)\\right].\n$$\nThen\n$$\n\\mathrm{BIC}_{0}\n= -2\\,\\hat{\\ell}_{0} + k_{0}\\ln n\n= 180\\left[\\ln(2\\pi)+1+\\ln(0.90)\\right] + 4\\,\\ln(180),\n$$\n$$\n\\mathrm{BIC}_{1}\n= -2\\,\\hat{\\ell}_{1} + k_{1}\\ln n\n= 180\\left[\\ln(2\\pi)+1+\\ln\\!\\left(\\frac{31}{36}\\right)\\right] + 6\\,\\ln(180).\n$$\nNumerically,\n$$\n\\ln(2\\pi)\\approx 1.8378770664,\\quad\n\\ln(180)\\approx 5.1929568509,\\quad\n\\ln(0.90)\\approx -0.1053605157,\\quad\n\\ln\\!\\left(\\frac{31}{36}\\right)\\approx -0.1495317340.\n$$\nThus,\n$$\n-2\\,\\hat{\\ell}_{0}\\approx 180\\left[1.8378770664+1-0.1053605157\\right]\\approx 491.8529791,\n$$\n$$\n\\mathrm{BIC}_{0}\\approx 491.8529791 + 4\\times 5.1929568509 \\approx 512.6248065,\n$$\nand\n$$\n-2\\,\\hat{\\ell}_{1}\\approx 180\\left[1.8378770664+1-0.1495317340\\right]\\approx 483.9021598,\n$$\n$$\n\\mathrm{BIC}_{1}\\approx 483.9021598 + 6\\times 5.1929568509 \\approx 515.0599009.\n$$\nRounded to four significant figures, $\\mathrm{BIC}_{0}\\approx 512.6$ and $\\mathrm{BIC}_{1}\\approx 515.1$. Therefore, for $n=180$, the simpler model $\\mathcal{M}_{0}$ is preferred by the Bayesian Information Criterion.\n\nPart (b). For large $n$ under the stated stabilization, write $\\mathrm{RSS}_{0}\\approx n r_{0}$ and $\\mathrm{RSS}_{1}\\approx n r_{1}$ with constants $r_{0}=0.90$ and $r_{1}=0.86$. The difference in BIC simplifies because the additive constant $n\\left[\\ln(2\\pi)+1\\right]$ cancels between models:\n$$\n\\Delta \\mathrm{BIC}(n)\n= \\mathrm{BIC}_{1}-\\mathrm{BIC}_{0}\n= n\\left[\\ln\\!\\left(\\frac{\\mathrm{RSS}_{1}}{n}\\right) - \\ln\\!\\left(\\frac{\\mathrm{RSS}_{0}}{n}\\right)\\right] + (k_{1}-k_{0})\\ln n\n\\approx n\\ln\\!\\left(\\frac{r_{1}}{r_{0}}\\right) + \\Delta k\\,\\ln n,\n$$\nwhere $\\Delta k = k_{1}-k_{0} = (p_{1}+1)-(p_{0}+1) = p_{1}-p_{0}$. The threshold $n_{\\ast}$ at which $\\mathrm{BIC}_{1}=\\mathrm{BIC}_{0}$ solves\n$$\nn\\,\\ln\\!\\left(\\frac{r_{1}}{r_{0}}\\right) + \\Delta k\\,\\ln n = 0.\n$$\nLet $a=\\ln(r_{1}/r_{0})$ (note that $a<0$ when $r_{1}<r_{0}$) and rewrite\n$$\n\\Delta k\\,\\ln n = -a\\,n,\\quad\\text{so}\\quad \\ln n = -\\frac{a}{\\Delta k}\\,n.\n$$\nExponentiating yields\n$$\nn = \\exp\\!\\left(-\\frac{a}{\\Delta k}\\,n\\right)\\quad\\Longleftrightarrow\\quad n\\,\\exp\\!\\left(\\frac{a}{\\Delta k}\\,n\\right)=1.\n$$\nDefine $c=\\frac{a}{\\Delta k}$ (so $c<0$ when $r_{1}<r_{0}$). Setting $u=cn$ gives $u\\,\\exp(u)=c$. The solutions are expressed in terms of the multivalued Lambert $W$ function, $u=W(c)$, so\n$$\nn \\,=\\, \\frac{W(c)}{c}\\,=\\, \\frac{\\Delta k}{\\ln(r_{1}/r_{0})}\\,W\\!\\left(\\frac{\\ln(r_{1}/r_{0})}{\\Delta k}\\right).\n$$\nFor $c\\in(-\\exp(-1),0)$ there are two real branches, $W_{0}(c)\\in(-1,0)$ and $W_{-1}(c)\\le -1$, yielding two positive solutions. As $n$ grows large, the linear term $a\\,n$ dominates, and when $a<0$ the difference $\\Delta\\mathrm{BIC}(n)$ becomes negative beyond the larger root. Therefore the minimal $n_{\\ast}$ such that for all $n\\ge n_{\\ast}$ we have $\\mathrm{BIC}_{1}<\\mathrm{BIC}_{0}$ is given by the larger root, which uses the $W_{-1}$ branch:\n$$\nn_{\\ast} \\,=\\, \\frac{\\Delta k}{\\ln(r_{1}/r_{0})}\\,W_{-1}\\!\\left(\\frac{\\ln(r_{1}/r_{0})}{\\Delta k}\\right).\n$$\nSpecializing to $r_{0}=0.90$, $r_{1}=0.86$, and $\\Delta k = k_{1}-k_{0} = 2$, we obtain\n$$\nn_{\\ast} \\,=\\, \\frac{2}{\\ln(0.86/0.90)}\\,W_{-1}\\!\\left(\\frac{\\ln(0.86/0.90)}{2}\\right)\n\\,=\\, \\frac{2}{\\ln(43/45)}\\,W_{-1}\\!\\left(\\frac{\\ln(43/45)}{2}\\right).\n$$\nQualitative discussion. When $r_{1}<r_{0}$, the larger model yields a strictly better per-observation fit, so the term $n\\,\\ln(r_{1}/r_{0})$ is negative and grows in magnitude linearly with $n$. The complexity penalty grows only as $\\Delta k\\,\\ln n$. Consequently, for sufficiently large $n$ (specifically $n\\ge n_{\\ast}$ as given above), the larger model $\\mathcal{M}_{1}$ is preferred by the Bayesian Information Criterion. Conversely, if there is no true improvement so that $r_{1}\\approx r_{0}$, then $\\ln(r_{1}/r_{0})\\approx 0$ and the penalty term dominates, leading the Bayesian Information Criterion to prefer the simpler model $\\mathcal{M}_{0}$ as $n$ increases.",
            "answer": "$$\\boxed{\\frac{2}{\\ln\\!\\left(\\frac{43}{45}\\right)}\\,W_{-1}\\!\\left(\\frac{\\ln\\!\\left(\\frac{43}{45}\\right)}{2}\\right)}$$"
        },
        {
            "introduction": "While the BIC formula, $-2\\ln(\\hat{L}) + k\\ln(n)$, appears straightforward, its correct application hinges on a nuanced and often tricky task: determining the number of free parameters, $k$. In realistic biomedical systems, models are rarely simple collections of independent parameters. This exercise () challenges you to carefully dissect a complex multi-tissue model, accounting for shared parameters, linear constraints, and fixed values to arrive at the true model complexity, a crucial step for valid model comparison.",
            "id": "3903943",
            "problem": "In a multi-tissue nonlinear regression used in biomedical systems modeling of tracer kinetics, you jointly fit data from $T = 5$ tissues indexed by $t \\in \\{1,2,3,4,5\\}$. For each tissue, you observe time-course measurements modeled as $y_{t,i} = f(t_{t,i}; \\boldsymbol{\\theta}_g, \\boldsymbol{\\phi}_t) + \\varepsilon_{t,i}$, where $f(\\cdot)$ is a nonlinear mechanistic model whose precise form is not required here. The joint fit is performed by maximum likelihood under the following scientifically motivated structure:\n- There is a vector of global kinetic parameters $\\boldsymbol{\\theta}_g$ of length $4$ that is shared across all tissues, representing common transport and enzymatic rates. One of these $4$ global kinetic parameters is fixed a priori from an external calibration experiment and not re-estimated in this fit.\n- Each tissue has an amplitude parameter $A_t$ that is constrained to represent a fractional contribution, with the constraint $\\sum_{t=1}^{5} A_t = 1$.\n- Each tissue has an effective delay parameter, but due to shared vascular pathways, tissues $1$ and $2$ share a common delay (denote it $\\tau_{12}$), whereas tissues $3$, $4$, and $5$ each have their own distinct delays ($\\tau_3$, $\\tau_4$, $\\tau_5$). There are no additional constraints among these delays.\n- The initial condition for the state driving $f(\\cdot)$ in tissue $t$ is not known. Rather than estimating $5$ unrelated initial conditions, you impose a parsimonious linear covariate structure $x_{0,t} = x_0 + \\delta q_t$, where $q_t$ are known, distinct tissue-level covariates, and $(x_0,\\delta)$ are parameters shared across all tissues.\n- Measurement noise is modeled as independent Gaussian with tissue-specific variances: $\\varepsilon_{t,i} \\sim \\mathcal{N}(0,\\sigma_t^2)$. However, imaging protocol symmetry implies $\\sigma_4^2 = \\sigma_5^2$, while $\\sigma_1^2$, $\\sigma_2^2$, and $\\sigma_3^2$ are all distinct and unrelated to each other.\n- A shared baseline trend $b_0 + b_1 g(t)$, with known scalar function $g(\\cdot)$, is added inside $f(\\cdot)$ and is parameterized by two coefficients $(b_0,b_1)$ that are common across all tissues.\n\nAll other aspects of the model are fully specified and known. When using the Bayesian Information Criterion (BIC) to compare this model to alternatives, you must supply the parameter count $k$ as the number of free, estimable parameters appearing in the joint likelihood for the $5$ tissues under the constraints given above.\n\nWhich value of $k$ is correct for this model?\n\nA. $k = 17$\n\nB. $k = 19$\n\nC. $k = 20$\n\nD. $k = 23$\n\nE. $k = 15$",
            "solution": "The problem requires the determination of the total number of free, estimable parameters, denoted by $k$, for a joint maximum likelihood fit of a multi-tissue nonlinear regression model. The value of $k$ is essential for calculating the Bayesian Information Criterion (BIC), which is defined as $\\text{BIC} = -2 \\ln(\\hat{L}) + k \\ln(N)$, where $\\hat{L}$ is the maximized likelihood and $N$ is the total number of observations. The task is to count $k$ based on the provided model structure and constraints. We will proceed by systematically enumerating the parameters from each component of the model.\n\n1.  **Global Kinetic Parameters ($\\boldsymbol{\\theta}_g$):**\n    The problem states there is a vector of global kinetic parameters $\\boldsymbol{\\theta}_g$ of length $4$. These are shared across all $T=5$ tissues. However, one of these $4$ parameters is specified to be fixed a priori from an external experiment. Therefore, it is not estimated in this fitting procedure. The number of free parameters from this group is the total number minus the number of fixed parameters.\n    Number of kinetic parameters = $4 - 1 = 3$.\n\n2.  **Amplitude Parameters ($A_t$):**\n    Each of the $5$ tissues has an amplitude parameter $A_t$. This gives a set of $5$ parameters $\\{A_1, A_2, A_3, A_4, A_5\\}$. These parameters are subject to the linear constraint $\\sum_{t=1}^{5} A_t = 1$. A single linear constraint on a set of $n$ parameters reduces the number of free parameters by $1$. Once $4$ of the amplitudes are estimated, the fifth is determined by the constraint (e.g., $A_5 = 1 - \\sum_{t=1}^{4} A_t$).\n    Number of free amplitude parameters = $5 - 1 = 4$.\n\n3.  **Delay Parameters ($\\tau_t$):**\n    The structure of the delay parameters is explicitly defined. Tissues $1$ and $2$ share a single common delay, which we denote $\\tau_{12}$. Tissues $3$, $4$, and $5$ each have their own distinct delays, denoted $\\tau_3$, $\\tau_4$, and $\\tau_5$. There are no further constraints mentioned among these delays. The set of parameters to be estimated is $\\{\\tau_{12}, \\tau_3, \\tau_4, \\tau_5\\}$.\n    Number of free delay parameters = $1 + 1 + 1 + 1 = 4$.\n\n4.  **Initial Condition Parameters ($x_{0,t}$):**\n    The initial conditions are not estimated independently for each tissue. Instead, they are modeled by a \"parsimonious linear covariate structure\": $x_{0,t} = x_0 + \\delta q_t$. The tissue-level covariates $q_t$ are known values. The parameters to be estimated are the intercept $x_0$ and the slope $\\delta$, which are shared across all tissues.\n    Number of free initial condition parameters = $2$ (namely, $x_0$ and $\\delta$).\n\n5.  **Measurement Noise Variance Parameters ($\\sigma_t^2$):**\n    The measurement noise is modeled as independent Gaussian with tissue-specific variances $\\sigma_t^2$. For $5$ tissues, this would normally imply $5$ parameters: $\\{\\sigma_1^2, \\sigma_2^2, \\sigma_3^2, \\sigma_4^2, \\sigma_5^2\\}$. However, there is a symmetry constraint given: $\\sigma_4^2 = \\sigma_5^2$. This means that a single parameter represents the variance for both tissue $4$ and tissue $5$. The other variances, $\\sigma_1^2$, $\\sigma_2^2$, and $\\sigma_3^2$, are distinct. Thus, the set of unique variance parameters to be estimated is $\\{\\sigma_1^2, \\sigma_2^2, \\sigma_3^2, \\sigma_{4,5}^2\\}$, where $\\sigma_{4,5}^2$ is the common variance for tissues $4$ and $5$.\n    Number of free variance parameters = $1 + 1 + 1 + 1 = 4$.\n\n6.  **Baseline Trend Parameters ($b_0, b_1$):**\n    A shared baseline trend, described by the function $b_0 + b_1 g(t)$, is included in the model. The function $g(\\cdot)$ is known. The parameters to be estimated are the coefficients $b_0$ and $b_1$. These are common across all tissues.\n    Number of free baseline parameters = $2$.\n\n**Total Parameter Count ($k$):**\nTo find the total number of free parameters $k$, we sum the counts from each category:\n$$ k = (\\text{kinetics}) + (\\text{amplitudes}) + (\\text{delays}) + (\\text{initial conditions}) + (\\text{variances}) + (\\text{baseline}) $$\n$$ k = 3 + 4 + 4 + 2 + 4 + 2 $$\n$$ k = 19 $$\n\nThe total number of free, estimable parameters for this model is $19$.\n\nNow, we evaluate the given options.\n\nA. $k = 17$: **Incorrect**. This value is less than the derived count of $19$. An error leading to $17$ might involve, for example, incorrectly counting only $2$ noise variance parameters instead of $4$.\n\nB. $k = 19$: **Correct**. This value matches the systematically derived total number of free parameters.\n\nC. $k = 20$: **Incorrect**. This value could be obtained by incorrectly failing to account for the constraint on the amplitude parameters. If one counted all $5$ amplitudes as free parameters, the total would be $3 + 5 + 4 + 2 + 4 + 2 = 20$. This neglects the fact that $\\sum A_t = 1$ removes one degree of freedom.\n\nD. $k = 23$: **Incorrect**. This value is significantly higher than the correct count. It likely results from multiple counting errors. For instance, if one ignored the amplitude constraint (adding $1$ parameter) and also ignored the parsimonious structure for initial conditions, instead counting $5$ distinct initial conditions (adding $5 - 2 = 3$ parameters), the total would be $19 + 1 + 3 = 23$. This demonstrates a failure to correctly interpret the model's constraints and parsimonious structures.\n\nE. $k = 15$: **Incorrect**. This count is too low. It could arise from erroneously imposing extra constraints. For instance, assuming all $5$ delays are shared (i.e., only $1$ delay parameter, a reduction of $3$) and that only a single initial condition parameter $x_0$ is estimated (a reduction of $1$), would lead to $19 - 3 - 1 = 15$. This contradicts the explicit description of the model's parameterization.",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "The utility of BIC extends far beyond the familiar territory of linear regression with Gaussian errors. Many biomedical phenomena, such as neural spike counts or disease incidence, are better described by other statistical distributions. This practice problem () moves into the domain of Generalized Linear Models (GLMs) by applying BIC to a Poisson regression model, a cornerstone for analyzing count data. You will calculate the maximized log-likelihood from first principles for this model class and use it to evaluate the model's performance.",
            "id": "3903998",
            "problem": "A systems neuroscientist models spike counts recorded from a single neuron in $10\\,\\text{ms}$ time bins using a Generalized Linear Model (GLM) with a Poisson observation model and the canonical logarithmic link. The covariates are stimulus amplitude $s_i \\in \\mathbb{R}$ and a binary indicator $h_i \\in \\{0,1\\}$ denoting whether a spike occurred in the immediately preceding bin. For observation $i$, the linear predictor is $x_i^{\\top}\\beta = \\beta_0 + \\beta_1 s_i + \\beta_2 h_i$ and the conditional mean of the Poisson count is $\\mu_i$.\n\nTwelve independent observations $(s_i, h_i, y_i)$ are recorded as follows:\n$(s_1, h_1, y_1) = (0.1, 0, 0)$,\n$(s_2, h_2, y_2) = (0.3, 0, 0)$,\n$(s_3, h_3, y_3) = (0.5, 0, 1)$,\n$(s_4, h_4, y_4) = (0.7, 0, 1)$,\n$(s_5, h_5, y_5) = (0.9, 0, 1)$,\n$(s_6, h_6, y_6) = (0.2, 1, 0)$,\n$(s_7, h_7, y_7) = (0.4, 1, 0)$,\n$(s_8, h_8, y_8) = (0.6, 1, 1)$,\n$(s_9, h_9, y_9) = (0.8, 1, 1)$,\n$(s_{10}, h_{10}, y_{10}) = (1.0, 1, 2)$,\n$(s_{11}, h_{11}, y_{11}) = (0.0, 0, 0)$,\n$(s_{12}, h_{12}, y_{12}) = (0.5, 1, 1)$.\n\nA maximum likelihood fit yields coefficient estimates $\\hat{\\beta} = (\\beta_0, \\beta_1, \\beta_2)$ with $\\beta_0 = -2.5$, $\\beta_1 = 1.2$, and $\\beta_2 = -0.6$. Starting from first principles, use the Poisson probability mass function and the GLM canonical logarithmic link to derive the maximized likelihood $\\hat{L}$ of the observed data under the fitted model. Then compute the Bayesian Information Criterion (BIC) for this fitted model using $n = 12$ observations and the number of free parameters equal to the number of estimated coefficients.\n\nUse the natural logarithm throughout. Express the final BIC value as a dimensionless real number. Round your answer to four significant figures.",
            "solution": "The problem asks for the calculation of the Bayesian Information Criterion (BIC) for a Generalized Linear Model (GLM) fitted to neuronal spike count data. This requires first deriving the maximized log-likelihood of the data under the given model and estimated parameters.\n\nFirst principles of the model are as follows. The observed spike count $y_i$ in bin $i$ is assumed to follow a Poisson distribution with mean $\\mu_i$. The probability mass function (PMF) is:\n$$ P(y_i | \\mu_i) = \\frac{\\mu_i^{y_i} \\exp(-\\mu_i)}{y_i!} $$\nThe model is a GLM with a canonical logarithmic link function, which connects the mean $\\mu_i$ to a linear predictor $\\eta_i$:\n$$ \\ln(\\mu_i) = \\eta_i $$\nThis implies $\\mu_i = \\exp(\\eta_i)$. The linear predictor is a function of the covariates $s_i$ and $h_i$:\n$$ \\eta_i = x_i^{\\top}\\beta = \\beta_0 + \\beta_1 s_i + \\beta_2 h_i $$\nFor a set of $n$ independent observations, the total likelihood $L$ is the product of the individual probabilities, $L = \\prod_{i=1}^{n} P(y_i|\\mu_i)$. It is more convenient to work with the log-likelihood, $\\ln(L)$:\n$$ \\ln(L) = \\sum_{i=1}^{n} \\ln(P(y_i|\\mu_i)) = \\sum_{i=1}^{n} \\left( y_i \\ln(\\mu_i) - \\mu_i - \\ln(y_i!) \\right) $$\nSubstituting the GLM equations, the log-likelihood becomes a function of the parameter vector $\\beta$:\n$$ \\ln(L(\\beta)) = \\sum_{i=1}^{n} \\left( y_i (x_i^{\\top}\\beta) - \\exp(x_i^{\\top}\\beta) - \\ln(y_i!) \\right) $$\nThe problem provides the maximum likelihood estimates (MLE) for the coefficients, $\\hat{\\beta} = (\\hat{\\beta}_0, \\hat{\\beta}_1, \\hat{\\beta}_2)^{\\top}$, where $\\hat{\\beta}_0 = -2.5$, $\\hat{\\beta}_1 = 1.2$, and $\\hat{\\beta}_2 = -0.6$. The maximized log-likelihood, denoted $\\ln(\\hat{L})$, is obtained by substituting these estimates into the log-likelihood function. Let $\\hat{\\eta}_i = x_i^{\\top}\\hat{\\beta}$ be the estimated linear predictor and $\\hat{\\mu}_i = \\exp(\\hat{\\eta}_i)$ be the estimated mean for observation $i$.\n$$ \\ln(\\hat{L}) = \\sum_{i=1}^{n} \\left( y_i \\hat{\\eta}_i - \\hat{\\mu}_i - \\ln(y_i!) \\right) $$\nWe have $n=12$ observations. We will now compute $\\hat{\\eta}_i$ for each observation $i=1, \\dots, 12$:\n$y = (0, 0, 1, 1, 1, 0, 0, 1, 1, 2, 0, 1)$.\n1.  $(s_1, h_1) = (0.1, 0) \\implies \\hat{\\eta}_1 = -2.5 + 1.2(0.1) - 0.6(0) = -2.38$\n2.  $(s_2, h_2) = (0.3, 0) \\implies \\hat{\\eta}_2 = -2.5 + 1.2(0.3) - 0.6(0) = -2.14$\n3.  $(s_3, h_3) = (0.5, 0) \\implies \\hat{\\eta}_3 = -2.5 + 1.2(0.5) - 0.6(0) = -1.90$\n4.  $(s_4, h_4) = (0.7, 0) \\implies \\hat{\\eta}_4 = -2.5 + 1.2(0.7) - 0.6(0) = -1.66$\n5.  $(s_5, h_5) = (0.9, 0) \\implies \\hat{\\eta}_5 = -2.5 + 1.2(0.9) - 0.6(0) = -1.42$\n6.  $(s_6, h_6) = (0.2, 1) \\implies \\hat{\\eta}_6 = -2.5 + 1.2(0.2) - 0.6(1) = -2.86$\n7.  $(s_7, h_7) = (0.4, 1) \\implies \\hat{\\eta}_7 = -2.5 + 1.2(0.4) - 0.6(1) = -2.62$\n8.  $(s_8, h_8) = (0.6, 1) \\implies \\hat{\\eta}_8 = -2.5 + 1.2(0.6) - 0.6(1) = -2.38$\n9.  $(s_9, h_9) = (0.8, 1) \\implies \\hat{\\eta}_9 = -2.5 + 1.2(0.8) - 0.6(1) = -2.14$\n10. $(s_{10}, h_{10}) = (1.0, 1) \\implies \\hat{\\eta}_{10} = -2.5 + 1.2(1.0) - 0.6(1) = -1.90$\n11. $(s_{11}, h_{11}) = (0.0, 0) \\implies \\hat{\\eta}_{11} = -2.5 + 1.2(0.0) - 0.6(0) = -2.50$\n12. $(s_{12}, h_{12}) = (0.5, 1) \\implies \\hat{\\eta}_{12} = -2.5 + 1.2(0.5) - 0.6(1) = -2.50$\n\nNext, we calculate the three components of the sum for $\\ln(\\hat{L})$:\nThe first component is $\\sum_{i=1}^{12} y_i \\hat{\\eta}_i$:\n$$ \\sum y_i \\hat{\\eta}_i = (0)\\hat{\\eta}_1 + (0)\\hat{\\eta}_2 + (1)(-1.90) + (1)(-1.66) + (1)(-1.42) + (0)\\hat{\\eta}_6 + (0)\\hat{\\eta}_7 + (1)(-2.38) + (1)(-2.14) + (2)(-1.90) + (0)\\hat{\\eta}_{11} + (1)(-2.50) $$\n$$ \\sum y_i \\hat{\\eta}_i = -1.90 - 1.66 - 1.42 - 2.38 - 2.14 - 3.80 - 2.50 = -15.80 $$\nThe second component is $\\sum_{i=1}^{12} \\hat{\\mu}_i = \\sum_{i=1}^{12} \\exp(\\hat{\\eta}_i)$:\n$$ \\sum \\hat{\\mu}_i = \\exp(-2.38) + \\exp(-2.14) + \\exp(-1.90) + \\exp(-1.66) + \\exp(-1.42) + \\exp(-2.86) + \\exp(-2.62) + \\exp(-2.38) + \\exp(-2.14) + \\exp(-1.90) + \\exp(-2.50) + \\exp(-2.50) $$\n$$ \\sum \\hat{\\mu}_i \\approx 0.09255 + 0.11766 + 0.14957 + 0.18992 + 0.24171 + 0.05727 + 0.07280 + 0.09255 + 0.11766 + 0.14957 + 0.08208 + 0.08208 \\approx 1.44542 $$\nThe third component is $\\sum_{i=1}^{12} \\ln(y_i!)$. Since $\\ln(0!) = \\ln(1) = 0$, we only need to consider non-zero $y_i$ where $y_i > 1$. The only such value is $y_{10}=2$.\n$$ \\sum \\ln(y_i!) = \\ln(y_{10}!) = \\ln(2!) = \\ln(2) \\approx 0.69315 $$\nNow, we combine these components to find $\\ln(\\hat{L})$:\n$$ \\ln(\\hat{L}) = \\sum y_i \\hat{\\eta}_i - \\sum \\hat{\\mu}_i - \\sum \\ln(y_i!) \\approx -15.80 - 1.44542 - 0.69315 = -17.93857 $$\nThe Bayesian Information Criterion (BIC) is defined as:\n$$ \\text{BIC} = k \\ln(n) - 2 \\ln(\\hat{L}) $$\nwhere $n$ is the number of observations and $k$ is the number of free parameters in the model.\nIn this problem, $n=12$. The model has $k=3$ free parameters, which are the coefficients $\\beta_0, \\beta_1, \\beta_2$.\nSubstituting the values:\n$$ \\text{BIC} = 3 \\ln(12) - 2(-17.93857) $$\nUsing $\\ln(12) \\approx 2.48491$:\n$$ \\text{BIC} \\approx 3(2.48491) + 2(17.93857) = 7.45473 + 35.87714 \\approx 43.33187 $$\nRounding the result to four significant figures, we get $43.33$.",
            "answer": "$$\\boxed{43.33}$$"
        }
    ]
}