{
    "hands_on_practices": [
        {
            "introduction": "This first practice exercise provides a foundational walkthrough of applying the Bayesian Information Criterion. We will compare two nested Gaussian linear models, a common task in biomedical research, to see how BIC adjudicates between a simpler model and a more complex one that offers a better fit to the data. By working from first principles, you will not only calculate the BIC for each model but also analytically explore how the sample size $n$ critically influences the trade-off between model fit and complexity, a key characteristic of this criterion. ",
            "id": "4928639",
            "problem": "A biostatistician is comparing two nested Gaussian linear regression models for a continuous biomarker $Y$ under the standard assumptions of independent Gaussian errors with unknown variance. Model $\\mathcal{M}_{0}$ includes an intercept, age, and sex (so the mean structure has $p_{0}=3$ regression parameters), while model $\\mathcal{M}_{1}$ augments $\\mathcal{M}_{0}$ by adding treatment and an age-by-treatment interaction (so the mean structure has $p_{1}=5$ regression parameters). Both models include a single residual variance parameter.\n\nOn a study with $n=180$ independent participants, the residual sums of squares after ordinary least squares fitting are $\\mathrm{RSS}_{0}=162.0$ for $\\mathcal{M}_{0}$ and $\\mathrm{RSS}_{1}=155.0$ for $\\mathcal{M}_{1}$.\n\n(a) Using only first principles for the Gaussian likelihood and the definition of the Bayesian Information Criterion (BIC), compute the BIC for each model, taking the total number of free parameters for $\\mathcal{M}_{j}$ to be $k_{j}=p_{j}+1$ (the $p_{j}$ regression coefficients including the intercept, plus the residual variance). Report the two BIC values rounded to four significant figures.\n\n(b) Now consider increasing the sample size by repeatedly sampling from the same data-generating process. Assume that, as $n$ grows, the per-observation residual sum of squares stabilizes so that $\\mathrm{RSS}_{0}\\approx n r_{0}$ and $\\mathrm{RSS}_{1}\\approx n r_{1}$ with constants $r_{0}=0.90$ and $r_{1}=0.86$. Let $\\Delta k = k_{1}-k_{0}$. Derive, from first principles, an exact closed-form expression (you may introduce special functions if needed) for the minimal sample size $n_{\\ast}$ such that for all $n \\ge n_{\\ast}$ the BIC of $\\mathcal{M}_{1}$ is strictly smaller than that of $\\mathcal{M}_{0}$. Express $n_{\\ast}$ in terms of $r_{0}$, $r_{1}$, and $\\Delta k$, and then specialize your expression to the numerical values above.\n\nRound any intermediate numerical BIC values in part (a) to four significant figures. For the final deliverable, provide the exact analytic expression for $n_{\\ast}$ from part (b); do not approximate or round the expression. The final answer must be a single closed-form analytic expression.",
            "solution": "We begin with the Gaussian linear model with independent errors. Under model $\\mathcal{M}_{j}$, we posit\n$$\nY \\sim \\mathcal{N}\\!\\left(X_{j}\\beta_{j},\\,\\sigma^{2} I_{n}\\right),\n$$\nwith $k_{j}$ free parameters consisting of $p_{j}$ regression coefficients (including the intercept) and the residual variance $\\sigma^{2}$. The maximized Gaussian log-likelihood for linear regression with unknown variance is obtained by substituting the maximum likelihood estimators $\\hat{\\beta}_{j}$ and $\\hat{\\sigma}^{2}_{j}=\\mathrm{RSS}_{j}/n$ into the log-likelihood. The resulting maximized log-likelihood is\n$$\n\\hat{\\ell}_{j}\n\\,=\\, -\\frac{n}{2}\\left[\\ln(2\\pi) + 1 + \\ln\\!\\left(\\frac{\\mathrm{RSS}_{j}}{n}\\right)\\right].\n$$\nThe Bayesian Information Criterion (BIC) is defined by\n$$\n\\mathrm{BIC}_{j} \\,=\\, -2\\,\\hat{\\ell}_{j} \\;+\\; k_{j}\\,\\ln n\n\\,=\\, n\\left[\\ln(2\\pi)+1+\\ln\\!\\left(\\frac{\\mathrm{RSS}_{j}}{n}\\right)\\right] \\;+\\; k_{j}\\,\\ln n.\n$$\nPart (a). For $\\mathcal{M}_{0}$, we have $p_{0}=3$, hence $k_{0}=p_{0}+1=4$. For $\\mathcal{M}_{1}$, $p_{1}=5$, hence $k_{1}=6$. With $n=180$, $\\mathrm{RSS}_{0}=162.0$, and $\\mathrm{RSS}_{1}=155.0$, compute\n$$\n\\ln n \\,=\\, \\ln(180),\n\\quad\n\\frac{\\mathrm{RSS}_{0}}{n} \\,=\\, \\frac{162.0}{180} \\,=\\, 0.90,\n\\quad\n\\frac{\\mathrm{RSS}_{1}}{n} \\,=\\, \\frac{155.0}{180} \\,=\\, \\frac{31}{36}.\n$$\nWe evaluate\n$$\n-2\\,\\hat{\\ell}_{0}\n= n\\left[\\ln(2\\pi)+1+\\ln\\!\\left(\\frac{\\mathrm{RSS}_{0}}{n}\\right)\\right]\n= 180\\left[\\ln(2\\pi)+1+\\ln(0.90)\\right],\n$$\nand\n$$\n-2\\,\\hat{\\ell}_{1}\n= 180\\left[\\ln(2\\pi)+1+\\ln\\!\\left(\\frac{31}{36}\\right)\\right].\n$$\nThen\n$$\n\\mathrm{BIC}_{0}\n= -2\\,\\hat{\\ell}_{0} + k_{0}\\ln n\n= 180\\left[\\ln(2\\pi)+1+\\ln(0.90)\\right] + 4\\,\\ln(180),\n$$\n$$\n\\mathrm{BIC}_{1}\n= -2\\,\\hat{\\ell}_{1} + k_{1}\\ln n\n= 180\\left[\\ln(2\\pi)+1+\\ln\\!\\left(\\frac{31}{36}\\right)\\right] + 6\\,\\ln(180).\n$$\nNumerically,\n$$\n\\ln(2\\pi)\\approx 1.8378770664,\\quad\n\\ln(180)\\approx 5.1929568509,\\quad\n\\ln(0.90)\\approx -0.1053605157,\\quad\n\\ln\\!\\left(\\frac{31}{36}\\right)\\approx -0.1495317340.\n$$\nThus,\n$$\n-2\\,\\hat{\\ell}_{0}\\approx 180\\left[1.8378770664+1-0.1053605157\\right]\\approx 491.8529791,\n$$\n$$\n\\mathrm{BIC}_{0}\\approx 491.8529791 + 4\\times 5.1929568509 \\approx 512.6248065,\n$$\nand\n$$\n-2\\,\\hat{\\ell}_{1}\\approx 180\\left[1.8378770664+1-0.1495317340\\right]\\approx 483.9021598,\n$$\n$$\n\\mathrm{BIC}_{1}\\approx 483.9021598 + 6\\times 5.1929568509 \\approx 515.0599009.\n$$\nRounded to four significant figures, $\\mathrm{BIC}_{0}\\approx 512.6$ and $\\mathrm{BIC}_{1}\\approx 515.1$. Therefore, for $n=180$, the simpler model $\\mathcal{M}_{0}$ is preferred by the Bayesian Information Criterion.\n\nPart (b). For large $n$ under the stated stabilization, write $\\mathrm{RSS}_{0}\\approx n r_{0}$ and $\\mathrm{RSS}_{1}\\approx n r_{1}$ with constants $r_{0}=0.90$ and $r_{1}=0.86$. The difference in BIC simplifies because the additive constant $n\\left[\\ln(2\\pi)+1\\right]$ cancels between models:\n$$\n\\Delta \\mathrm{BIC}(n)\n= \\mathrm{BIC}_{1}-\\mathrm{BIC}_{0}\n= n\\left[\\ln\\!\\left(\\frac{\\mathrm{RSS}_{1}}{n}\\right) - \\ln\\!\\left(\\frac{\\mathrm{RSS}_{0}}{n}\\right)\\right] + (k_{1}-k_{0})\\ln n\n\\approx n\\ln\\!\\left(\\frac{r_{1}}{r_{0}}\\right) + \\Delta k\\,\\ln n,\n$$\nwhere $\\Delta k = k_{1}-k_{0} = (p_{1}+1)-(p_{0}+1) = p_{1}-p_{0}$. The threshold $n_{\\ast}$ at which $\\mathrm{BIC}_{1}=\\mathrm{BIC}_{0}$ solves\n$$\nn\\,\\ln\\!\\left(\\frac{r_{1}}{r_{0}}\\right) + \\Delta k\\,\\ln n = 0.\n$$\nLet $a=\\ln(r_{1}/r_{0})$ (note that $a0$ when $r_{1}r_{0}$) and rewrite\n$$\n\\Delta k\\,\\ln n = -a\\,n,\\quad\\text{so}\\quad \\ln n = -\\frac{a}{\\Delta k}\\,n.\n$$\nExponentiating yields\n$$\nn = \\exp\\!\\left(-\\frac{a}{\\Delta k}\\,n\\right)\\quad\\Longleftrightarrow\\quad n\\,\\exp\\!\\left(\\frac{a}{\\Delta k}\\,n\\right)=1.\n$$\nDefine $c=\\frac{a}{\\Delta k}$ (so $c0$ when $r_{1}r_{0}$). Setting $u=cn$ gives $u\\,\\exp(u)=c$. The solutions are expressed in terms of the multivalued Lambert $W$ function, $u=W(c)$, so\n$$\nn \\,=\\, \\frac{W(c)}{c}\\,=\\, \\frac{\\Delta k}{\\ln(r_{1}/r_{0})}\\,W\\!\\left(\\frac{\\ln(r_{1}/r_{0})}{\\Delta k}\\right).\n$$\nFor $c\\in(-\\exp(-1),0)$ there are two real branches, $W_{0}(c)\\in(-1,0)$ and $W_{-1}(c)\\le -1$, yielding two positive solutions. As $n$ grows large, the linear term $a\\,n$ dominates, and when $a0$ the difference $\\Delta\\mathrm{BIC}(n)$ becomes negative beyond the larger root. Therefore the minimal $n_{\\ast}$ such that for all $n\\ge n_{\\ast}$ we have $\\mathrm{BIC}_{1}\\mathrm{BIC}_{0}$ is given by the larger root, which uses the $W_{-1}$ branch:\n$$\nn_{\\ast} \\,=\\, \\frac{\\Delta k}{\\ln(r_{1}/r_{0})}\\,W_{-1}\\!\\left(\\frac{\\ln(r_{1}/r_{0})}{\\Delta k}\\right).\n$$\nSpecializing to $r_{0}=0.90$, $r_{1}=0.86$, and $\\Delta k = k_{1}-k_{0} = 2$, we obtain\n$$\nn_{\\ast} \\,=\\, \\frac{2}{\\ln(0.86/0.90)}\\,W_{-1}\\!\\left(\\frac{\\ln(0.86/0.90)}{2}\\right)\n\\,=\\, \\frac{2}{\\ln(43/45)}\\,W_{-1}\\!\\left(\\frac{\\ln(43/45)}{2}\\right).\n$$\nQualitative discussion. When $r_{1}r_{0}$, the larger model yields a strictly better per-observation fit, so the term $n\\,\\ln(r_{1}/r_{0})$ is negative and grows in magnitude linearly with $n$. The complexity penalty grows only as $\\Delta k\\,\\ln n$. Consequently, for sufficiently large $n$ (specifically $n\\ge n_{\\ast}$ as given above), the larger model $\\mathcal{M}_{1}$ is preferred by the Bayesian Information Criterion. Conversely, if there is no true improvement so that $r_{1}\\approx r_{0}$, then $\\ln(r_{1}/r_{0})\\approx 0$ and the penalty term dominates, leading the Bayesian Information Criterion to prefer the simpler model $\\mathcal{M}_{0}$ as $n$ increases.",
            "answer": "$$\\boxed{\\frac{2}{\\ln\\!\\left(\\frac{43}{45}\\right)}\\,W_{-1}\\!\\left(\\frac{\\ln\\!\\left(\\frac{43}{45}\\right)}{2}\\right)}$$"
        },
        {
            "introduction": "Applying model selection criteria like BIC requires more than just plugging numbers into a formula; it demands a precise understanding of what constitutes model complexity. This exercise challenges you to determine the correct number of free parameters, $k$, for a sophisticated multi-tissue nonlinear regression model, a scenario representative of modern systems biology. By carefully accounting for shared parameters, linear constraints, and other structural assumptions, you will learn to accurately quantify model complexity, a crucial and often-misunderstood step in the process of model selection. ",
            "id": "3903943",
            "problem": "In a multi-tissue nonlinear regression used in biomedical systems modeling of tracer kinetics, you jointly fit data from $T = 5$ tissues indexed by $t \\in \\{1,2,3,4,5\\}$. For each tissue, you observe time-course measurements modeled as $y_{t,i} = f(t_{t,i}; \\boldsymbol{\\theta}_g, \\boldsymbol{\\phi}_t) + \\varepsilon_{t,i}$, where $f(\\cdot)$ is a nonlinear mechanistic model whose precise form is not required here. The joint fit is performed by maximum likelihood under the following scientifically motivated structure:\n- There is a vector of global kinetic parameters $\\boldsymbol{\\theta}_g$ of length $4$ that is shared across all tissues, representing common transport and enzymatic rates. One of these $4$ global kinetic parameters is fixed a priori from an external calibration experiment and not re-estimated in this fit.\n- Each tissue has an amplitude parameter $A_t$ that is constrained to represent a fractional contribution, with the constraint $\\sum_{t=1}^{5} A_t = 1$.\n- Each tissue has an effective delay parameter, but due to shared vascular pathways, tissues $1$ and $2$ share a common delay (denote it $\\tau_{12}$), whereas tissues $3$, $4$, and $5$ each have their own distinct delays ($\\tau_3$, $\\tau_4$, $\\tau_5$). There are no additional constraints among these delays.\n- The initial condition for the state driving $f(\\cdot)$ in tissue $t$ is not known. Rather than estimating $5$ unrelated initial conditions, you impose a parsimonious linear covariate structure $x_{0,t} = x_0 + \\delta q_t$, where $q_t$ are known, distinct tissue-level covariates, and $(x_0,\\delta)$ are parameters shared across all tissues.\n- Measurement noise is modeled as independent Gaussian with tissue-specific variances: $\\varepsilon_{t,i} \\sim \\mathcal{N}(0,\\sigma_t^2)$. However, imaging protocol symmetry implies $\\sigma_4^2 = \\sigma_5^2$, while $\\sigma_1^2$, $\\sigma_2^2$, and $\\sigma_3^2$ are all distinct and unrelated to each other.\n- A shared baseline trend $b_0 + b_1 g(t)$, with known scalar function $g(\\cdot)$, is added inside $f(\\cdot)$ and is parameterized by two coefficients $(b_0,b_1)$ that are common across all tissues.\n\nAll other aspects of the model are fully specified and known. When using the Bayesian Information Criterion (BIC) to compare this model to alternatives, you must supply the parameter count $k$ as the number of free, estimable parameters appearing in the joint likelihood for the $5$ tissues under the constraints given above.\n\nWhich value of $k$ is correct for this model?\n\nA. $k = 17$\n\nB. $k = 19$\n\nC. $k = 20$\n\nD. $k = 23$\n\nE. $k = 15$",
            "solution": "The problem requires the determination of the total number of free, estimable parameters, denoted by $k$, for a joint maximum likelihood fit of a multi-tissue nonlinear regression model. The value of $k$ is essential for calculating the Bayesian Information Criterion (BIC), which is defined as $\\text{BIC} = -2 \\ln(\\hat{L}) + k \\ln(N)$, where $\\hat{L}$ is the maximized likelihood and $N$ is the total number of observations. The task is to count $k$ based on the provided model structure and constraints. We will proceed by systematically enumerating the parameters from each component of the model.\n\n1.  **Global Kinetic Parameters ($\\boldsymbol{\\theta}_g$):**\n    The problem states there is a vector of global kinetic parameters $\\boldsymbol{\\theta}_g$ of length $4$. These are shared across all $T=5$ tissues. However, one of these $4$ parameters is specified to be fixed a priori from an external experiment. Therefore, it is not estimated in this fitting procedure. The number of free parameters from this group is the total number minus the number of fixed parameters.\n    Number of kinetic parameters = $4 - 1 = 3$.\n\n2.  **Amplitude Parameters ($A_t$):**\n    Each of the $5$ tissues has an amplitude parameter $A_t$. This gives a set of $5$ parameters $\\{A_1, A_2, A_3, A_4, A_5\\}$. These parameters are subject to the linear constraint $\\sum_{t=1}^{5} A_t = 1$. A single linear constraint on a set of $n$ parameters reduces the number of free parameters by $1$. Once $4$ of the amplitudes are estimated, the fifth is determined by the constraint (e.g., $A_5 = 1 - \\sum_{t=1}^{4} A_t$).\n    Number of free amplitude parameters = $5 - 1 = 4$.\n\n3.  **Delay Parameters ($\\tau_t$):**\n    The structure of the delay parameters is explicitly defined. Tissues $1$ and $2$ share a single common delay, which we denote $\\tau_{12}$. Tissues $3$, $4$, and $5$ each have their own distinct delays, denoted $\\tau_3$, $\\tau_4$, and $\\tau_5$. There are no further constraints mentioned among these delays. The set of parameters to be estimated is $\\{\\tau_{12}, \\tau_3, \\tau_4, \\tau_5\\}$.\n    Number of free delay parameters = $1 + 1 + 1 + 1 = 4$.\n\n4.  **Initial Condition Parameters ($x_{0,t}$):**\n    The initial conditions are not estimated independently for each tissue. Instead, they are modeled by a \"parsimonious linear covariate structure\": $x_{0,t} = x_0 + \\delta q_t$. The tissue-level covariates $q_t$ are known values. The parameters to be estimated are the intercept $x_0$ and the slope $\\delta$, which are shared across all tissues.\n    Number of free initial condition parameters = $2$ (namely, $x_0$ and $\\delta$).\n\n5.  **Measurement Noise Variance Parameters ($\\sigma_t^2$):**\n    The measurement noise is modeled as independent Gaussian with tissue-specific variances $\\sigma_t^2$. For $5$ tissues, this would normally imply $5$ parameters: $\\{\\sigma_1^2, \\sigma_2^2, \\sigma_3^2, \\sigma_4^2, \\sigma_5^2\\}$. However, there is a symmetry constraint given: $\\sigma_4^2 = \\sigma_5^2$. This means that a single parameter represents the variance for both tissue $4$ and tissue $5$. The other variances, $\\sigma_1^2$, $\\sigma_2^2$, and $\\sigma_3^2$, are distinct. Thus, the set of unique variance parameters to be estimated is $\\{\\sigma_1^2, \\sigma_2^2, \\sigma_3^2, \\sigma_{4,5}^2\\}$, where $\\sigma_{4,5}^2$ is the common variance for tissues $4$ and $5$.\n    Number of free variance parameters = $1 + 1 + 1 + 1 = 4$.\n\n6.  **Baseline Trend Parameters ($b_0, b_1$):**\n    A shared baseline trend, described by the function $b_0 + b_1 g(t)$, is included in the model. The function $g(\\cdot)$ is known. The parameters to be estimated are the coefficients $b_0$ and $b_1$. These are common across all tissues.\n    Number of free baseline parameters = $2$.\n\n**Total Parameter Count ($k$):**\nTo find the total number of free parameters $k$, we sum the counts from each category:\n$$ k = (\\text{kinetics}) + (\\text{amplitudes}) + (\\text{delays}) + (\\text{initial conditions}) + (\\text{variances}) + (\\text{baseline}) $$\n$$ k = 3 + 4 + 4 + 2 + 4 + 2 $$\n$$ k = 19 $$\n\nThe total number of free, estimable parameters for this model is $19$.\n\nNow, we evaluate the given options.\n\nA. $k = 17$: **Incorrect**. This value is less than the derived count of $19$. An error leading to $17$ might involve, for example, incorrectly counting only $2$ noise variance parameters instead of $4$.\n\nB. $k = 19$: **Correct**. This value matches the systematically derived total number of free parameters.\n\nC. $k = 20$: **Incorrect**. This value could be obtained by incorrectly failing to account for the constraint on the amplitude parameters. If one counted all $5$ amplitudes as free parameters, the total would be $3 + 5 + 4 + 2 + 4 + 2 = 20$. This neglects the fact that $\\sum A_t = 1$ removes one degree of freedom.\n\nD. $k = 23$: **Incorrect**. This value is significantly higher than the correct count. It likely results from multiple counting errors. For instance, if one ignored the amplitude constraint (adding $1$ parameter) and also ignored the parsimonious structure for initial conditions, instead counting $5$ distinct initial conditions (adding $5 - 2 = 3$ parameters), the total would be $19 + 1 + 3 = 23$. This demonstrates a failure to correctly interpret the model's constraints and parsimonious structures.\n\nE. $k = 15$: **Incorrect**. This count is too low. It could arise from erroneously imposing extra constraints. For instance, assuming all $5$ delays are shared (i.e., only $1$ delay parameter, a reduction of $3$) and that only a single initial condition parameter $x_0$ is estimated (a reduction of $1$), would lead to $19 - 3 - 1 = 15$. This contradicts the explicit description of the model's parameterization.",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "In the era of high-dimensional biomedical data, where the number of predictors can exceed the number of observations, classical model selection becomes intractable. This hands-on coding exercise demonstrates how BIC can be adapted to guide model selection in this modern context, specifically for choosing the optimal regularization parameter in a LASSO regression. You will implement the entire workflow from first principles, using BIC to navigate the path from a null model to a complex one, ultimately selecting a sparse model that balances predictive accuracy with parsimony. ",
            "id": "3904016",
            "problem": "You are tasked with implementing a self-contained program that performs model selection for a sparse linear model in the context of biomedical systems modeling using the Bayesian Information Criterion (BIC). Consider a standard linear model with Gaussian noise, where the observed response vector is $y \\in \\mathbb{R}^n$ and the design matrix is $X \\in \\mathbb{R}^{n \\times p}$. The data-generating process is assumed to be $y = X \\beta^\\star + \\varepsilon$, with $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$ and unknown variance $\\sigma^2$. You will fit Least Absolute Shrinkage and Selection Operator (LASSO)-penalized linear regression and select the regularization parameter $\\lambda$ by minimizing the Bayesian Information Criterion (BIC).\n\nYour implementation must adhere to the following principles and steps:\n\n- Begin from the maximum likelihood formulation of the Gaussian linear model and the definition of the Bayesian Information Criterion (BIC), where BIC is computed using the maximized Gaussian log-likelihood and a degrees-of-freedom equal to the number of nonzero regression coefficients. Do not assume any pre-derived shortcut expressions; your method must follow logically from these base definitions.\n- Implement LASSO via coordinate descent to compute solutions along a regularization path. Use a path of $L$ logarithmically spaced $\\lambda$ values from $\\lambda_{\\max}$ down to a small fraction $\\lambda_{\\min} = \\alpha \\lambda_{\\max}$, where $\\lambda_{\\max}$ is the smallest value at which all coefficients are zero at the minimizer. Warm-start successive fits along the path for computational stability.\n- Standardize the predictors and center the response prior to fitting: each column of $X$ must be centered to zero mean and scaled to unit standard deviation, and $y$ must be centered to zero mean. Treat the intercept as unpenalized; since centering is used, the intercept is $0$ during fitting.\n- For each fitted model along the path, compute the BIC using the maximized Gaussian log-likelihood and define the degrees-of-freedom as the count of nonzero coefficients (excluding the intercept, which is $0$ under centering).\n- Select the $\\lambda$ that minimizes BIC. If multiple $\\lambda$ values yield numerically indistinguishable BIC values within machine precision, choose the largest $\\lambda$ among them.\n\nAlgorithmic constraints and numerical details:\n\n- Use coordinate descent with soft-thresholding updates. Implement a convergence criterion based on the maximum absolute change across coefficient updates being below a tolerance $\\tau$.\n- Use an absolute threshold $\\delta$ to decide whether a coefficient is considered nonzero when counting degrees-of-freedom. This threshold must be positive and small relative to machine precision.\n- When computing the maximized Gaussian log-likelihood for BIC, guard against degenerate residual sums of squares by adding a small positive constant $\\epsilon$ inside any logarithm as needed to preserve numerical stability.\n- You must not use any external datasets. Generate synthetic datasets deterministically using provided seeds.\n\nTest suite and data generation:\n\nFor each test case, generate $X$ and $y$ as described. Use the following cases, all with a fixed coefficient amplitude $A$, nonzero coefficients located at the smallest $k$ indices (i.e., indices $0$ through $k-1$), and alternating signs $+A,-A,+A,\\dots$:\n\n- Case $1$ (independent features, moderate dimension): $n=120$, $p=30$, $k=5$, $\\sigma=1.0$, $\\rho=0.0$, $A=2.5$, seed $7$.\n- Case $2$ (null model edge case): $n=80$, $p=40$, $k=0$, $\\sigma=1.0$, $\\rho=0.0$, $A=2.0$, seed $13$.\n- Case $3$ (high dimensional, $pn$): $n=30$, $p=60$, $k=3$, $\\sigma=0.5$, $\\rho=0.0$, $A=2.0$, seed $42$.\n- Case $4$ (correlated block of predictors): $n=100$, $p=50$, $k=4$, $\\sigma=0.8$, $\\rho=0.8$, $A=2.0$, seed $11$. Construct the first $m=\\min(10,p)$ columns with correlation $\\rho$ by $X_{:,j} = \\rho g + \\sqrt{1-\\rho^2} \\,\\varepsilon_j$ for $j \\in \\{0,\\dots,m-1\\}$, where $g \\in \\mathbb{R}^n$ is shared across the block, and $\\varepsilon_j$ are independent standard normal vectors. Remaining columns are independent standard normal.\n\nFor all cases, form $y = X \\beta^\\star + \\sigma \\eta$ with $\\eta \\sim \\mathcal{N}(0, I_n)$. Use a regularization path length $L=60$, a path ratio $\\alpha=10^{-3}$, a coordinate descent tolerance $\\tau=10^{-6}$, a nonzero threshold $\\delta=10^{-8}$, and a logarithm guard $\\epsilon=10^{-12}$.\n\nFinal output format:\n\n- Your program must produce a single line of output containing the selected $\\lambda$ for each of the $4$ test cases, rounded to $6$ decimal places, as a comma-separated list enclosed in square brackets (e.g., $[0.123456,0.234567,0.345678,0.456789]$).\n- No additional text or lines are permitted in the output.\n\nAll quantities and symbols must be written in LaTeX within this problem statement. Angles are not used. Physical units do not apply.",
            "solution": "The user-provided problem statement has been analyzed and is determined to be valid. It is scientifically grounded, well-posed, objective, and contains sufficient information to derive a unique, verifiable solution.\n\nThe problem requires the implementation of a model selection procedure for sparse linear regression. The core of the task is to fit a series of LASSO-penalized linear models along a regularization path and select the optimal model using the Bayesian Information Criterion (BIC). The implementation must be derived from first principles, specifically the maximized Gaussian log-likelihood, and must employ a coordinate descent algorithm for fitting the LASSO models.\n\nHere is a step-by-step derivation of the complete solution.\n\n### 1. The Statistical Model and Data Preprocessing\n\nThe underlying model is a linear relationship between a design matrix $X \\in \\mathbb{R}^{n \\times p}$ and a response vector $y \\in \\mathbb{R}^n$, corrupted by Gaussian noise:\n$$\ny = X \\beta^\\star + \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)\n$$\nwhere $\\beta^\\star \\in \\mathbb{R}^p$ is the true sparse coefficient vector and $\\sigma^2$ is the unknown noise variance.\n\nAs per the problem specification, we first preprocess the data. The response vector $y$ is centered by subtracting its mean:\n$$\ny_c = y - \\bar{y}\n$$\nEach predictor column $X_j$ (for $j=1, \\dots, p$) of the design matrix is standardized to have a mean of $0$ and a standard deviation of $1$. This is achieved by:\n$$\nX_{s,j} = \\frac{X_j - \\bar{X}_j}{s_j}\n$$\nwhere $\\bar{X}_j$ is the mean of column $j$ and $s_j$ is its standard deviation, computed as $s_j = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (X_{ij} - \\bar{X}_j)^2}$. This standardization ensures that $\\frac{1}{n} \\sum_{i=1}^n X_{s,ij}^2 = 1$. With centered data, the intercept term is zero, so it is not included in the penalized regression.\n\n### 2. LASSO Regression via Coordinate Descent\n\nThe LASSO method estimates the coefficients $\\beta$ by minimizing a penalized least-squares objective function. For standardized predictors and a centered response, the objective is:\n$$\n\\hat{\\beta}(\\lambda) = \\arg\\min_{\\beta \\in \\mathbb{R}^p} \\left\\{ \\frac{1}{2n} \\|y_c - X_s \\beta\\|_2^2 + \\lambda \\|\\beta\\|_1 \\right\\}\n$$\nwhere $\\lambda \\ge 0$ is the regularization parameter that controls the sparsity of the solution, and $\\|\\beta\\|_1 = \\sum_{j=1}^p |\\beta_j|$ is the $\\ell_1$-norm of the coefficient vector.\n\nWe solve this optimization problem using coordinate descent. The algorithm iteratively updates each coefficient $\\beta_j$ while holding all other coefficients $\\beta_k$ ($k \\neq j$) fixed. The update rule for $\\beta_j$ is found by minimizing the objective with respect to only $\\beta_j$. This univariate problem has a closed-form solution given by the soft-thresholding operator $S_{\\lambda}(\\cdot)$:\n$$\n\\beta_j^{\\text{new}} \\leftarrow S_{\\lambda}(\\rho_j)\n$$\nwhere\n$$\nS_{\\lambda}(z) = \\text{sign}(z) \\max(|z| - \\lambda, 0)\n$$\nand $\\rho_j$ is the simple least-squares coefficient estimate on the partial residual. Given our data standardization ($X_{s,j}^T X_{s,j} = n$), $\\rho_j$ can be computed efficiently as:\n$$\n\\rho_j = \\frac{1}{n} X_{s,j}^T (y_c - \\sum_{k \\neq j} X_{s,k} \\beta_k^{\\text{old}}) = \\frac{1}{n} X_{s,j}^T (y_c - X_s \\beta^{\\text{old}}) + \\beta_j^{\\text{old}}\n$$\nWe cycle through all coefficients $j=1, \\dots, p$ repeatedly until the maximum absolute change in any coefficient between iterations is below a tolerance $\\tau$.\n\n### 3. The Regularization Path\n\nWe compute LASSO solutions for a sequence of $L$ regularization parameters, from a maximum value $\\lambda_{\\max}$ down to $\\lambda_{\\min}$.\n- $\\lambda_{\\max}$ is the smallest value of $\\lambda$ for which the estimated coefficient vector $\\hat{\\beta}(\\lambda)$ is entirely zero. From the Karush-Kuhn-Tucker (KKT) optimality conditions, this occurs when $\\lambda$ is equal to the maximum absolute correlation between the predictors and the response:\n$$\n\\lambda_{\\max} = \\max_{j} \\left| \\frac{1}{n} X_{s,j}^T y_c \\right|\n$$\n- The path ends at $\\lambda_{\\min} = \\alpha \\lambda_{\\max}$, with $\\alpha=10^{-3}$.\n- The $L=60$ values of $\\lambda$ are logarithmically spaced between $\\lambda_{\\max}$ and $\\lambda_{\\min}$.\nSolutions are computed sequentially from $\\lambda_{\\max}$ to $\\lambda_{\\min}$, using the solution for the previous $\\lambda$ as a \"warm start\" for the coordinate descent algorithm. This improves computational efficiency.\n\n### 4. Model Selection with Bayesian Information Criterion (BIC)\n\nFor each fitted model $\\hat{\\beta}(\\lambda)$ along the path, we compute its BIC value to assess its quality. The BIC is defined as:\n$$\n\\text{BIC} = d_{\\lambda} \\ln(n) - 2 \\mathcal{L}_{\\max}\n$$\nwhere $n$ is the number of samples, $d_{\\lambda}$ is the effective degrees of freedom of the model, and $\\mathcal{L}_{\\max}$ is the maximized log-likelihood.\n\nThe Gaussian log-likelihood function is:\n$$\n\\mathcal{L}(\\beta, \\sigma^2) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{1}{2\\sigma^2} \\|y_c - X_s\\beta\\|^2\n$$\nFor a given $\\hat{\\beta}(\\lambda)$ from LASSO, this likelihood is maximized with respect to $\\sigma^2$ by its maximum likelihood estimator (MLE):\n$$\n\\hat{\\sigma}^2 = \\frac{1}{n} \\|y_c - X_s\\hat{\\beta}(\\lambda)\\|^2 = \\frac{\\text{RSS}_{\\lambda}}{n}\n$$\nwhere $\\text{RSS}_{\\lambda}$ is the residual sum of squares for the model with parameter $\\lambda$. Plugging $\\hat{\\sigma}^2$ back into the log-likelihood gives the maximized value:\n$$\n\\mathcal{L}_{\\max} = -\\frac{n}{2}\\left( \\ln(2\\pi) + \\ln\\left(\\frac{\\text{RSS}_{\\lambda}}{n}\\right) + 1 \\right)\n$$\nSubstituting this into the BIC formula:\n$$\n\\text{BIC}(\\lambda) = d_{\\lambda} \\ln(n) + n \\left( \\ln(2\\pi) + \\ln\\left(\\frac{\\text{RSS}_{\\lambda}}{n}\\right) + 1 \\right)\n$$\nFor model comparison, terms constant across all models (i.e., those not dependent on $\\lambda$) can be dropped. This yields the more common computational form:\n$$\n\\text{BIC}(\\lambda) \\propto d_{\\lambda} \\ln(n) + n \\ln\\left(\\frac{\\text{RSS}_{\\lambda}}{n}\\right)\n$$\nThe degrees of freedom, $d_{\\lambda}$, for a LASSO model is the number of non-zero coefficients:\n$$\nd_{\\lambda} = \\sum_{j=1}^{p} \\mathbb{I}(|\\hat{\\beta}_j(\\lambda)|  \\delta)\n$$\nwhere $\\delta=10^{-8}$ is a small positive threshold to account for floating-point inaccuracies. For numerical stability, the logarithm is computed as $n(\\ln(\\text{RSS}_{\\lambda} + \\epsilon) - \\ln(n))$, where $\\epsilon=10^{-12}$ is a small guard value.\n\nThe optimal regularization parameter, $\\lambda^\\star$, is the one that minimizes the BIC. If multiple $\\lambda$ values yield the minimum BIC, the largest of these $\\lambda$ values is chosen, which corresponds to the most parsimonious model among the top candidates.\n\n### 5. Synthetic Data Generation\n\nThe procedure is tested on four synthetic datasets generated as specified:\n- A true coefficient vector $\\beta^\\star$ is constructed with its first $k$ components being non-zero with amplitude $A$ and alternating signs.\n- The design matrix $X$ is generated from a multivariate normal distribution. For cases 1-3, predictors are independent. For case 4, a block of predictors are correlated.\n- The response $y$ is generated as $y = X\\beta^\\star + \\sigma\\eta$, where $\\eta$ is a vector of standard normal noise.\n- Each case uses a deterministic seed for the random number generator to ensure reproducibility.\n\nThe final output is the selected $\\lambda^\\star$ for each of the four test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef soft_threshold(rho, lam):\n    \"\"\"Soft-thresholding operator.\"\"\"\n    if rho  lam:\n        return rho - lam\n    elif rho  -lam:\n        return rho + lam\n    else:\n        return 0.0\n\ndef coordinate_descent(X, y, lambda_val, beta_init, tau, max_iter=1000):\n    \"\"\"\n    LASSO regression using coordinate descent.\n    Assumes X is standardized (mean 0, variance 1) and y is centered.\n    The objective is: (1/2n) * ||y - X*beta||^2 + lambda * ||beta||_1\n    \"\"\"\n    n, p = X.shape\n    beta = beta_init.copy()\n\n    for _ in range(max_iter):\n        beta_old = beta.copy()\n        \n        for j in range(p):\n            # Calculate rho_j = (1/n) * X_j^T * r_j, where r_j is the partial residual\n            # This is equivalent to (1/n) * X_j^T * y' + beta_j for a simplified update\n            # where y' is the residual y - X*beta + X_j*beta_j\n            rho_j = (1/n) * X[:, j].T @ (y - X @ beta + X[:, j] * beta[j])\n            \n            # Update beta_j with soft thresholding\n            beta[j] = soft_threshold(rho_j, lambda_val)\n\n        # Check for convergence\n        if np.max(np.abs(beta - beta_old))  tau:\n            break\n            \n    return beta\n\ndef solve_case(n, p, k, sigma, rho, A, seed, L, alpha, tau, delta, epsilon):\n    \"\"\"\n    Runs a single test case: generates data, performs LASSO with BIC selection.\n    \"\"\"\n    # 1. Generate Data\n    rng = np.random.default_rng(seed)\n\n    # Generate true coefficients beta_star\n    beta_star = np.zeros(p)\n    beta_star[:k] = A * ((-1) ** np.arange(k))\n\n    # Generate design matrix X\n    if rho == 0.0:\n        X = rng.standard_normal((n, p))\n    else:\n        m = min(10, p)\n        X = np.zeros((n, p))\n        g = rng.standard_normal(n)\n        # Correlated block\n        for j in range(m):\n            eps_j = rng.standard_normal(n)\n            X[:, j] = rho * g + np.sqrt(1 - rho**2) * eps_j\n        # Independent block\n        if p  m:\n            X[:, m:] = rng.standard_normal((n, p - m))\n    \n    # Generate response y\n    noise = rng.standard_normal(n)\n    y = X @ beta_star + sigma * noise\n\n    # 2. Preprocess Data\n    y_mean = np.mean(y)\n    y_c = y - y_mean\n\n    X_mean = np.mean(X, axis=0)\n    X_std = np.std(X, axis=0)\n    # Guard against division by zero for constant columns\n    X_std[X_std  1e-12] = 1.0\n    X_s = (X - X_mean) / X_std\n\n    # 3. Define Regularization Path\n    lambda_max = np.max(np.abs(X_s.T @ y_c)) / n\n    lambda_min = alpha * lambda_max\n    lambdas = np.logspace(np.log10(lambda_max), np.log10(lambda_min), num=L)\n\n    # 4. Fit models and compute BIC along the path\n    bics = []\n    beta = np.zeros(p)  # Initial beta for warm start\n    log_n = np.log(n)\n\n    for lam in lambdas:\n        # Fit model using coordinate descent with warm start\n        beta = coordinate_descent(X_s, y_c, lam, beta, tau)\n\n        # Compute RSS\n        rss = np.sum((y_c - X_s @ beta)**2)\n        \n        # Compute degrees of freedom (dof)\n        dof = np.sum(np.abs(beta)  delta)\n        \n        # Compute BIC: dof*log(n) + n*log(RSS/n)\n        # Using n*(log(RSS + epsilon) - log(n)) for numerical stability\n        bic_val = dof * log_n + n * (np.log(rss + epsilon) - log_n)\n        bics.append(bic_val)\n    \n    # 5. Select best lambda based on minimum BIC\n    # np.argmin() returns the first index of the minimum, which corresponds to the\n    # largest lambda in case of a tie, as lambdas are sorted descendingly.\n    best_idx = np.argmin(bics)\n    best_lambda = lambdas[best_idx]\n    \n    return best_lambda\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # Case 1: n=120, p=30, k=5, sigma=1.0, rho=0.0, A=2.5, seed=7\n        {'n': 120, 'p': 30, 'k': 5, 'sigma': 1.0, 'rho': 0.0, 'A': 2.5, 'seed': 7},\n        # Case 2: n=80, p=40, k=0, sigma=1.0, rho=0.0, A=2.0, seed=13\n        {'n': 80, 'p': 40, 'k': 0, 'sigma': 1.0, 'rho': 0.0, 'A': 2.0, 'seed': 13},\n        # Case 3: n=30, p=60, k=3, sigma=0.5, rho=0.0, A=2.0, seed=42\n        {'n': 30, 'p': 60, 'k': 3, 'sigma': 0.5, 'rho': 0.0, 'A': 2.0, 'seed': 42},\n        # Case 4: n=100, p=50, k=4, sigma=0.8, rho=0.8, A=2.0, seed=11\n        {'n': 100, 'p': 50, 'k': 4, 'sigma': 0.8, 'rho': 0.8, 'A': 2.0, 'seed': 11},\n    ]\n\n    # Algorithmic parameters\n    L = 60\n    alpha = 1e-3\n    tau = 1e-6\n    delta = 1e-8\n    epsilon = 1e-12\n\n    results = []\n    for params in test_cases:\n        best_lambda = solve_case(\n            params['n'], params['p'], params['k'], params['sigma'], params['rho'],\n            params['A'], params['seed'], L, alpha, tau, delta, epsilon\n        )\n        results.append(best_lambda)\n        \n    # Format a string for each result, then join them\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}