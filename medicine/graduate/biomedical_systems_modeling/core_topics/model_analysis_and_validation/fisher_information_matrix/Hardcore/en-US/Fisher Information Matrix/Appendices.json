{
    "hands_on_practices": [
        {
            "introduction": "We begin with a foundational model for count-based measurements common in biophysics and molecular biology, the Poisson distribution. This exercise  challenges you to derive the Fisher information for the rate parameter $\\lambda$, a quantity central to modeling phenomena like photon counts from a biosensor or occurrences of genetic mutations. By working through this derivation from first principles, you will develop a concrete intuition for how information is quantified and see how an experiment's sensitivity to parameter changes depends on the underlying event rate.",
            "id": "3885946",
            "problem": "A single-molecule fluorescence biosensor counts photon emission events in fixed acquisition intervals. Under standard conditions, the photon counts in each interval are modeled as independent and identically distributed draws $y_{i} \\sim \\mathrm{Poisson}(\\lambda)$, where $\\lambda$ is the mean event rate per interval determined by molecular concentration and instrument gain. Suppose $n$ consecutive intervals are recorded, yielding counts $\\{y_{1}, y_{2}, \\dots, y_{n}\\}$.\n\nStarting from the modeling assumptions and first principles of likelihood-based inference, derive the scalar Fisher information (the $(1,1)$ element of the Fisher information matrix for this single-parameter model) about the parameter $\\lambda$ contained in the $n$ observations. Then, based on your derived expression, discuss how the local sensitivity to changes in $\\lambda$ differs between low-count and high-count regimes in this biosensing context.\n\nExpress your final answer as a single closed-form analytic expression in terms of $n$ and $\\lambda$. No rounding is required, and no physical units should be included in the final expression.",
            "solution": "The goal is to derive the scalar Fisher information for the parameter $\\lambda$ of a Poisson process, given $n$ independent and identically distributed (i.i.d.) observations $\\{y_1, y_2, \\dots, y_n\\}$, where each $y_i \\sim \\mathrm{Poisson}(\\lambda)$.\n\nThe Fisher information, $I(\\lambda)$, for a single parameter $\\lambda$ can be calculated as the negative of the expected value of the second derivative of the log-likelihood function with respect to the parameter:\n$$\nI(\\lambda) = -E\\left[ \\frac{\\partial^2}{\\partial \\lambda^2} \\ln L(\\lambda | \\mathbf{y}) \\right]\n$$\nwhere $L(\\lambda | \\mathbf{y})$ is the likelihood function for the set of observations $\\mathbf{y} = \\{y_1, y_2, \\dots, y_n\\}$.\n\nFirst, we establish the log-likelihood function. Since the $n$ observations are i.i.d. from a Poisson distribution, the total likelihood function $L(\\lambda | \\mathbf{y})$ is the product of the individual probability mass functions (PMFs):\n$$\nL(\\lambda | \\mathbf{y}) = \\prod_{i=1}^{n} P(y_i | \\lambda) = \\prod_{i=1}^{n} \\frac{\\lambda^{y_i} e^{-\\lambda}}{y_i!}\n$$\nThe log-likelihood function, $\\ell(\\lambda | \\mathbf{y}) = \\ln L(\\lambda | \\mathbf{y})$, is therefore:\n$$\n\\ell(\\lambda | \\mathbf{y}) = \\sum_{i=1}^{n} \\ln \\left( \\frac{\\lambda^{y_i} e^{-\\lambda}}{y_i!} \\right) = \\sum_{i=1}^{n} \\left( y_i \\ln(\\lambda) - \\lambda - \\ln(y_i!) \\right)\n$$\n$$\n\\ell(\\lambda | \\mathbf{y}) = \\left( \\sum_{i=1}^{n} y_i \\right) \\ln(\\lambda) - n\\lambda - \\sum_{i=1}^{n} \\ln(y_i!)\n$$\nNext, we find the first and second derivatives of the log-likelihood with respect to $\\lambda$:\nThe first derivative (the score function) is:\n$$\n\\frac{\\partial\\ell}{\\partial \\lambda} = \\frac{1}{\\lambda} \\sum_{i=1}^{n} y_i - n\n$$\nThe second derivative is:\n$$\n\\frac{\\partial^2\\ell}{\\partial \\lambda^2} = -\\frac{1}{\\lambda^2} \\sum_{i=1}^{n} y_i\n$$\nFinally, we compute the Fisher information by taking the negative of the expectation of this second derivative. The expectation is taken over the distribution of the data $\\mathbf{y}$.\n$$\nI(\\lambda) = -E\\left[ -\\frac{1}{\\lambda^2} \\sum_{i=1}^{n} y_i \\right] = \\frac{1}{\\lambda^2} E\\left[ \\sum_{i=1}^{n} y_i \\right]\n$$\nBy linearity of expectation, and since $E[y_i] = \\lambda$ for a Poisson random variable:\n$$\nE\\left[ \\sum_{i=1}^{n} y_i \\right] = \\sum_{i=1}^{n} E[y_i] = \\sum_{i=1}^{n} \\lambda = n\\lambda\n$$\nSubstituting this result back into the expression for $I(\\lambda)$:\n$$\nI(\\lambda) = \\frac{1}{\\lambda^2} (n\\lambda) = \\frac{n}{\\lambda}\n$$\nThis is the scalar Fisher information for the parameter $\\lambda$ based on $n$ i.i.d. Poisson observations.\n\nNow, we discuss how the local sensitivity to changes in $\\lambda$ differs between low-count and high-count regimes.\nThe Fisher information, $I(\\lambda) = n/\\lambda$, quantifies the amount of information the data provide about the parameter $\\lambda$. A higher value of $I(\\lambda)$ implies that the log-likelihood function is more sharply peaked, allowing for a more precise estimation of $\\lambda$. The CramÃ©r-Rao lower bound states that the variance of any unbiased estimator $\\hat{\\lambda}$ is bounded by $\\mathrm{Var}(\\hat{\\lambda}) \\ge 1/I(\\lambda) = \\lambda/n$.\n\nIn the **low-count regime** (where $\\lambda$ is small, approaching $0$):\nThe Fisher information $I(\\lambda) = n/\\lambda$ becomes very large. This indicates a high degree of local sensitivity to absolute changes in $\\lambda$. For example, distinguishing between a mean count rate of $\\lambda=0.1$ and $\\lambda=0.2$ is relatively easy because the probability of observing zero photons, $P(y=0) = e^{-\\lambda}$, changes significantly. The log-likelihood function is highly curved near small values of $\\lambda$, meaning that the parameter is well-localized by the data.\n\nIn the **high-count regime** (where $\\lambda$ is large):\nThe Fisher information $I(\\lambda) = n/\\lambda$ becomes small. This indicates that the experiment is less sensitive to small *absolute* changes in $\\lambda$. For instance, distinguishing between $\\lambda=100$ and $\\lambda=101$ is much more difficult than distinguishing between $\\lambda=1$ and $\\lambda=2$, even though the absolute difference is $1$ in both cases. The log-likelihood function is broader, implying that the parameter is less precisely localized for a given absolute change. While the absolute precision (proportional to $1/\\sqrt{I(\\lambda)} = \\sqrt{\\lambda/n}$) worsens as $\\lambda$ increases, the *relative* precision (proportional to $\\sqrt{\\lambda/n} / \\lambda = 1/\\sqrt{n\\lambda}$) improves. However, Fisher information itself relates to the curvature of the log-likelihood with respect to the parameter $\\lambda$, and this curvature decreases as $\\lambda$ increases, signifying lower local sensitivity to absolute parameter changes.",
            "answer": "$$\n\\boxed{\\frac{n}{\\lambda}}\n$$"
        },
        {
            "introduction": "Building on the single-parameter case, we now advance to a multi-parameter model that is arguably the most fundamental in all of statistics: the Gaussian distribution with unknown mean $\\mu$ and variance $\\sigma^2$. This practice  requires deriving the full $2 \\times 2$ Fisher Information Matrix, which involves calculating both diagonal and off-diagonal elements. The resulting diagonal structure is not an accident; it reveals the important property of parameter orthogonality, demonstrating that for a Gaussian sample, the information we gain about the mean is independent of the information we gain about the variance.",
            "id": "3886010",
            "problem": "A biomarker quantification system produces repeated measurements due to intrinsic biological variability and sensor noise. Let the measurements be modeled as independent and identically distributed (i.i.d.) samples $y_{1},y_{2},\\dots,y_{n}$ from a Gaussian distribution with unknown mean $\\mu$ and unknown variance $\\sigma^{2}$, that is $y_{i}\\sim \\mathcal{N}(\\mu,\\sigma^{2})$ for $i=1,\\dots,n$. In a parameter estimation study for this biomedical system, you are interested in the local sensitivity of the likelihood to the parameters, summarized by the Fisher information matrix for the parameter vector $(\\mu,\\sigma^{2})$.\n\nStarting from first principles, namely the joint probability density for i.i.d. Gaussian measurements, the log-likelihood for $(\\mu,\\sigma^{2})$, and the definition of the Fisher information matrix in terms of the score and its expected curvature, derive an explicit analytic expression for the Fisher information matrix $I(\\mu,\\sigma^{2})$ for a sample size $n$. Your derivation must explicitly account for expectations with respect to the data-generating model and must not contain any random sample realizations in the final result.\n\nGive your final answer as a single closed-form $2\\times 2$ matrix in terms of $n$ and $\\sigma^{2}$, ordered by the parameter vector $(\\mu,\\sigma^{2})$. No rounding is required. Do not include any units in your final matrix.",
            "solution": "The task is to derive the Fisher Information Matrix (FIM) for a set of $n$ i.i.d. Gaussian random variables $y_i \\sim \\mathcal{N}(\\mu, \\sigma^{2})$. The parameter vector is $\\theta = (\\mu, \\sigma^{2})$. The FIM is a $2 \\times 2$ matrix $I(\\theta)$ whose $(j, k)$-th element is given by $[I(\\theta)]_{jk} = -E\\left[\\frac{\\partial^2 \\ell(\\theta)}{\\partial\\theta_j \\partial\\theta_k}\\right]$, where $\\ell(\\theta)$ is the log-likelihood function.\n\nThe log-likelihood function for the sample $Y = (y_1, \\dots, y_n)$ is:\n$$\\ell(\\mu, \\sigma^2) = -\\frac{n}{2} \\ln(2\\pi) - \\frac{n}{2} \\ln(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - \\mu)^2$$\n\nNext, we compute the second partial derivatives of the log-likelihood function:\n\\begin{enumerate}\n    \\item $\\frac{\\partial^2\\ell}{\\partial\\mu^2} = \\frac{\\partial}{\\partial\\mu}\\left(\\frac{1}{\\sigma^2}\\sum_{i=1}^{n}(y_i - \\mu)\\right) = -\\frac{n}{\\sigma^2}$\n    \\item $\\frac{\\partial^2\\ell}{\\partial\\mu \\partial\\sigma^2} = \\frac{\\partial}{\\partial\\mu}\\left(-\\frac{n}{2\\sigma^2} + \\frac{1}{2(\\sigma^2)^2}\\sum_{i=1}^{n}(y_i - \\mu)^2\\right) = -\\frac{1}{(\\sigma^2)^2}\\sum_{i=1}^{n}(y_i - \\mu)$\n    \\item $\\frac{\\partial^2\\ell}{\\partial(\\sigma^2)^2} = \\frac{\\partial}{\\partial\\sigma^2}\\left(-\\frac{n}{2\\sigma^2} + \\frac{1}{2(\\sigma^2)^2}\\sum_{i=1}^{n}(y_i - \\mu)^2\\right) = \\frac{n}{2(\\sigma^2)^2} - \\frac{1}{(\\sigma^2)^3}\\sum_{i=1}^{n}(y_i - \\mu)^2$\n\\end{enumerate}\n\nNow, we compute the negative expectation of each second derivative to find the elements of the FIM:\n\\begin{enumerate}\n    \\item $I_{\\mu\\mu} = -E\\left[-\\frac{n}{\\sigma^2}\\right] = \\frac{n}{\\sigma^2}$ (since the term is constant).\n    \\item $I_{\\mu, \\sigma^2} = -E\\left[-\\frac{1}{(\\sigma^2)^2}\\sum_{i=1}^{n}(y_i - \\mu)\\right] = \\frac{1}{(\\sigma^2)^2}\\sum_{i=1}^{n}E[y_i - \\mu] = 0$, because $E[y_i] = \\mu$.\n    \\item $I_{\\sigma^2, \\sigma^2} = -E\\left[\\frac{n}{2(\\sigma^2)^2} - \\frac{1}{(\\sigma^2)^3}\\sum_{i=1}^{n}(y_i - \\mu)^2\\right]$. Using the linearity of expectation and the fact that $E[(y_i - \\mu)^2] = \\sigma^2$:\n    $$I_{\\sigma^2, \\sigma^2} = -\\left(\\frac{n}{2(\\sigma^2)^2} - \\frac{1}{(\\sigma^2)^3}\\sum_{i=1}^{n}E[(y_i - \\mu)^2]\\right) = -\\left(\\frac{n}{2(\\sigma^2)^2} - \\frac{n\\sigma^2}{(\\sigma^2)^3}\\right) = -\\left(\\frac{n}{2(\\sigma^2)^2} - \\frac{n}{(\\sigma^2)^2}\\right) = \\frac{n}{2(\\sigma^2)^2}$$\n\\end{enumerate}\n\nAssembling the elements into the FIM for the parameter vector $(\\mu, \\sigma^2)$:\n$$I(\\mu, \\sigma^2) = \\begin{pmatrix} I_{\\mu\\mu} & I_{\\mu, \\sigma^2} \\\\ I_{\\sigma^2, \\mu} & I_{\\sigma^2, \\sigma^2} \\end{pmatrix} = \\begin{pmatrix} \\frac{n}{\\sigma^2} & 0 \\\\ 0 & \\frac{n}{2(\\sigma^2)^2} \\end{pmatrix}$$\nThe off-diagonal terms are zero, indicating that for a Gaussian sample, the information about the mean $\\mu$ is orthogonal to the information about the variance $\\sigma^2$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{n}{\\sigma^{2}} & 0 \\\\\n0 & \\frac{n}{2 (\\sigma^{2})^{2}}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Our final exercise synthesizes the previous concepts within the framework of linear regression, a cornerstone of biomedical data analysis. This problem  asks for the Fisher Information Matrix for a general linear model, where the mean response is structured by a design matrix $X$ and a vector of coefficients $\\beta$. The derivation highlights one of the most powerful applications of Fisher information: it explicitly connects the structure of an experiment, as encoded in the matrix $X^{\\top}X$, to the precision with which we can estimate our parameters of interest.",
            "id": "3885914",
            "problem": "A widely used linear observation model in biomedical systems modeling posits that repeated measurements of a biomarker response across replicates can be described by a multivariate Gaussian distribution with a mean that is linear in known covariates and independent, homoscedastic noise. Consider an experiment with $n$ independent replicates, where the response vector is $y \\in \\mathbb{R}^{n}$, the design matrix is $X \\in \\mathbb{R}^{n \\times p}$ with full column rank $p$, and the parameter vector is $\\beta \\in \\mathbb{R}^{p}$. Assume the noise is independent across replicates and homoscedastic with variance $\\sigma^{2} > 0$, so that the probabilistic model is $y \\sim \\mathcal{N}(X \\beta, \\sigma^{2} I_{n})$, where $I_{n}$ is the $n \\times n$ identity matrix. Starting only from the fundamental definition of the log-likelihood for the multivariate Gaussian model and the definition of the Fisher information matrix, derive the Fisher information matrix $I(\\beta, \\sigma^{2})$ for the joint parameter $(\\beta, \\sigma^{2})$, and identify the block structure that links the matrix $X^{\\top} X$ to information about $\\beta$. Your final answer must be a single closed-form block matrix expression in terms of $X$, $n$, and $\\sigma^{2}$ only. No numerical rounding is required, and no physical units need be reported.",
            "solution": "The parameter vector for this model is $\\theta = (\\beta^{\\top}, \\sigma^{2})^{\\top}$, a vector of dimension $p+1$. The Fisher Information Matrix (FIM), $I(\\theta)$, is a $(p+1) \\times (p+1)$ block matrix whose elements are given by $I_{ij}(\\theta) = -E\\left[\\frac{\\partial^2 \\ell(\\theta)}{\\partial \\theta_i \\partial \\theta_j}\\right]$, where $\\ell(\\theta)$ is the log-likelihood function.\n\nThe log-likelihood function for the model $y \\sim \\mathcal{N}(X\\beta, \\sigma^{2}I_{n})$ is:\n$$\\ell(\\beta, \\sigma^{2}) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^{2}) - \\frac{1}{2\\sigma^{2}}(y - X\\beta)^{\\top}(y - X\\beta)$$\n\nWe will compute the components of the FIM in a block structure corresponding to parameters $\\beta$ and $\\sigma^2$:\n$$I(\\beta, \\sigma^{2}) = \\begin{pmatrix} I_{\\beta\\beta} & I_{\\beta, \\sigma^{2}} \\\\ I_{\\sigma^{2}, \\beta} & I_{\\sigma^{2}, \\sigma^{2}} \\end{pmatrix}$$\n\n1.  **Block $I_{\\beta\\beta}$:**\n    The second partial derivative of $\\ell$ with respect to $\\beta$ (the Hessian block) is:\n    $$\\frac{\\partial^2 \\ell}{\\partial \\beta \\partial \\beta^{\\top}} = \\frac{\\partial}{\\partial \\beta^{\\top}}\\left(\\frac{1}{\\sigma^{2}}X^{\\top}(y - X\\beta)\\right) = -\\frac{1}{\\sigma^{2}}X^{\\top}X$$\n    This expression is constant with respect to the data $y$, so its negative expectation is:\n    $$I_{\\beta\\beta} = -E\\left[-\\frac{1}{\\sigma^{2}}X^{\\top}X\\right] = \\frac{1}{\\sigma^{2}}X^{\\top}X$$\n\n2.  **Block $I_{\\sigma^{2}, \\sigma^{2}}$:**\n    The second partial derivative of $\\ell$ with respect to $\\sigma^2$ is:\n    $$\\frac{\\partial^2 \\ell}{\\partial (\\sigma^2)^2} = \\frac{n}{2(\\sigma^{2})^2} - \\frac{1}{(\\sigma^{2})^3}(y - X\\beta)^{\\top}(y - X\\beta)$$\n    To find the FIM component, we take the negative expectation. We use the fact that $E[(y - X\\beta)^{\\top}(y - X\\beta)] = E[\\text{tr}((y - X\\beta)(y - X\\beta)^{\\top})] = \\text{tr}(\\text{Cov}(y)) = \\text{tr}(\\sigma^2 I_n) = n\\sigma^2$.\n    $$I_{\\sigma^{2}, \\sigma^{2}} = -E\\left[\\frac{n}{2\\sigma^{4}} - \\frac{1}{\\sigma^{6}}(y - X\\beta)^{\\top}(y - X\\beta)\\right] = -\\frac{n}{2\\sigma^{4}} + \\frac{1}{\\sigma^{6}}E[(y - X\\beta)^{\\top}(y - X\\beta)]$$\n    $$I_{\\sigma^{2}, \\sigma^{2}} = -\\frac{n}{2\\sigma^{4}} + \\frac{n\\sigma^{2}}{\\sigma^{6}} = \\frac{n}{2\\sigma^{4}}$$\n\n3.  **Off-diagonal blocks $I_{\\beta, \\sigma^{2}}$ and $I_{\\sigma^{2}, \\beta}$:**\n    The mixed partial derivative is:\n    $$\\frac{\\partial^2 \\ell}{\\partial \\beta \\partial \\sigma^2} = \\frac{\\partial}{\\partial \\sigma^2}\\left(\\frac{1}{\\sigma^{2}}X^{\\top}(y - X\\beta)\\right) = -\\frac{1}{(\\sigma^{2})^2}X^{\\top}(y - X\\beta)$$\n    Taking the negative expectation:\n    $$I_{\\beta, \\sigma^{2}} = -E\\left[-\\frac{1}{\\sigma^{4}}X^{\\top}(y - X\\beta)\\right] = \\frac{1}{\\sigma^{4}}X^{\\top}E[y - X\\beta]$$\n    Since $E[y] = X\\beta$, we have $E[y - X\\beta] = 0$. Thus, the off-diagonal block is a zero matrix:\n    $$I_{\\beta, \\sigma^{2}} = 0_{p \\times 1}$$\n    By symmetry, $I_{\\sigma^{2}, \\beta} = I_{\\beta, \\sigma^{2}}^{\\top} = 0_{1 \\times p}$.\n\nAssembling the blocks gives the full Fisher Information Matrix:\n$$I(\\beta, \\sigma^{2}) = \\begin{pmatrix} \\frac{1}{\\sigma^{2}}X^{\\top}X & 0 \\\\ 0 & \\frac{n}{2\\sigma^{4}} \\end{pmatrix}$$\nThe block-diagonal structure shows that the information about the regression coefficients $\\beta$ is orthogonal to the information about the noise variance $\\sigma^2$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{\\sigma^{2}}X^{\\top}X & 0 \\\\\n0 & \\frac{n}{2\\sigma^{4}}\n\\end{pmatrix}\n}\n$$"
        }
    ]
}