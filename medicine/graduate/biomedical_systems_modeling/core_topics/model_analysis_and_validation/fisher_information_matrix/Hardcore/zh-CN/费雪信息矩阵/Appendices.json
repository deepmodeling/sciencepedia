{
    "hands_on_practices": [
        {
            "introduction": "理论学习的最佳方式是从一个基本模型开始。这个练习将引导您从第一性原理出发，为伯努利分布中的单个参数推导费雪信息。通过这个过程，您将深入理解费雪信息作为对数似然函数曲率期望值的核心概念，并探索在生物医学等领域中，观测稀有事件为何能提供如此丰富的信息。",
            "id": "3886000",
            "problem": "一种高通量单细胞检测方法被用于检测循环肿瘤细胞中是否存在特定的生物标志物。每个独立处理的细胞产生一个二元结果 $X_{i} \\in \\{0,1\\}$，指示检测到生物标志物不存在 ($X_{i}=0$) 或存在 ($X_{i}=1$)。假设结果是独立同分布的 (i.i.d.)，且 $X_{i} \\sim \\mathrm{Bernoulli}(p)$，其中 $p \\in (0,1)$ 是一个固定但未知的检测概率，它表征了对于给定患者的检测方法和内在生物学特性。您处理了 $n \\in \\mathbb{N}$ 个细胞。\n\n仅使用Fisher信息是在真实参数值下的抽样分布下，对数似然函数关于参数的负二阶导数的期望这一定义，推导基于$n$个独立同分布结果的$p$的Fisher信息。在您的推导中，将此Fisher信息与对数似然函数在真实$p$值周围的曲率联系起来，并评论在生物医学系统中稀有事件检测的背景下，当$p \\to 0^{+}$和$p \\to 1^{-}$时，该曲率的行为。\n\n以$n$和$p$的单个闭式解析表达式形式提供您的最终答案。无需数值取整。不表示单位。",
            "solution": "该问题陈述被认为是有效的。它在科学上基于已建立的统计理论，问题阐述清晰，目标明确，信息充分，并使用客观、正式的语言。该问题是统计推断中的一个标准推导，并具有生物医学系统建模中的相关应用背景。因此，我们可以进行推导。\n\n该问题要求基于$n$个独立同分布 (i.i.d.) 观测值的样本，推导伯努利分布参数$p$的Fisher信息。观测值为$X_1, X_2, \\dots, X_n$，其中每个$X_i \\in \\{0, 1\\}$遵循伯努利分布，$X_i \\sim \\mathrm{Bernoulli}(p)$。参数$p \\in (0,1)$代表成功的概率，即$P(X_i=1) = p$。\n\n单个观测值$X_i$的概率质量函数 (PMF) 为：\n$$P(X_i = x_i | p) = p^{x_i} (1-p)^{1-x_i} \\quad \\text{对于 } x_i \\in \\{0, 1\\}$$\n由于观测值是独立同分布的，整个样本$X = (X_1, \\dots, X_n)$的联合PMF是各个PMF的乘积。这个联合PMF，作为参数$p$的函数，就是似然函数$L(p | X)$：\n$$L(p | X) = \\prod_{i=1}^{n} p^{X_i} (1-p)^{1-X_i}$$\n这可以通过定义成功总次数$S_n = \\sum_{i=1}^{n} X_i$来简化。似然函数变为：\n$$L(p | S_n) = p^{S_n} (1-p)^{n-S_n}$$\n下一步是求对数似然函数$\\ell(p | S_n)$，即似然函数的自然对数：\n$$\\ell(p | S_n) = \\ln(L(p | S_n)) = \\ln\\left(p^{S_n} (1-p)^{n-S_n}\\right)$$\n利用对数的性质，我们得到：\n$$\\ell(p | S_n) = S_n \\ln(p) + (n-S_n) \\ln(1-p)$$\n问题要求我们使用Fisher信息$I(p)$的定义，即对数似然函数关于参数$p$的负二阶导数的期望。我们首先计算对数似然函数的一阶导数，即得分函数：\n$$\\frac{\\partial \\ell}{\\partial p} = \\frac{\\partial}{\\partial p} \\left[ S_n \\ln(p) + (n-S_n) \\ln(1-p) \\right] = \\frac{S_n}{p} - \\frac{n-S_n}{1-p}$$\n接下来，我们计算对数似然函数关于$p$的二阶导数：\n$$\\frac{\\partial^2 \\ell}{\\partial p^2} = \\frac{\\partial}{\\partial p} \\left[ \\frac{S_n}{p} - \\frac{n-S_n}{1-p} \\right] = -\\frac{S_n}{p^2} - \\left( -(n-S_n) \\frac{-1}{(1-p)^2} \\right) = -\\frac{S_n}{p^2} - \\frac{n-S_n}{(1-p)^2}$$\nFisher信息$I(p)$定义为该二阶导数的负期望，其中期望是关于由真实参数值$p$决定的数据抽样分布来计算的：\n$$I(p) = -E\\left[ \\frac{\\partial^2 \\ell}{\\partial p^2} \\right] = -E\\left[ -\\frac{S_n}{p^2} - \\frac{n-S_n}{(1-p)^2} \\right]$$\n根据期望算子的线性性质：\n$$I(p) = E\\left[ \\frac{S_n}{p^2} + \\frac{n-S_n}{(1-p)^2} \\right] = \\frac{1}{p^2} E[S_n] + \\frac{1}{(1-p)^2} E[n-S_n]$$\n统计量$S_n = \\sum_{i=1}^{n} X_i$是$n$个独立同分布的伯努利随机变量之和。单个伯努利变量$X_i$的期望是$E[X_i] = p$。因此，$S_n$的期望是：\n$$E[S_n] = E\\left[ \\sum_{i=1}^{n} X_i \\right] = \\sum_{i=1}^{n} E[X_i] = \\sum_{i=1}^{n} p = np$$\n此外，$E[n-S_n] = n - E[S_n] = n - np = n(1-p)$。\n将这些期望代入$I(p)$的表达式中：\n$$I(p) = \\frac{1}{p^2} (np) + \\frac{1}{(1-p)^2} (n(1-p))$$\n简化各项，我们得到：\n$$I(p) = \\frac{n}{p} + \\frac{n}{1-p}$$\n将这些项通分，得到Fisher信息的最终表达式：\n$$I(p) = n \\left( \\frac{1-p+p}{p(1-p)} \\right) = \\frac{n}{p(1-p)}$$\n这是基于$n$次独立同分布伯努利试验的参数$p$的Fisher信息。\n\n问题的第二部分要求评论Fisher信息与对数似然函数曲率之间的关系。二阶导数$\\frac{\\partial^2 \\ell}{\\partial p^2}$衡量了对数似然函数在给定点的局部曲率。Fisher信息$I(p) = E\\left[-\\frac{\\partial^2 \\ell}{\\partial p^2}\\right]$代表了对数似然函数在真实参数值$p$处的期望曲率。较大的$I(p)$值对应于一个在其最大值周围高度尖峰或急剧弯曲的对数似然函数。这表明数据为估计参数提供了大量信息，因为$p$值偏离最大似然估计的微小变化会导致对数似然值的迅速下降。相反，较小的$I(p)$值意味着一个平坦的对数似然函数，此时数据对于确定$p$的精确值所提供的信息较少。\n\n最后，我们分析$I(p) = \\frac{n}{p(1-p)}$在$p \\to 0^{+}$和$p \\to 1^{-}$时的行为。\n分母$p(1-p)$在区间$(0,1)$的两个极端都趋近于$0$。\n当$p \\to 0^{+}$时，我们有$p(1-p) \\to 0$，因此$I(p) \\to \\infty$。\n当$p \\to 1^{-}$时，我们也有$p(1-p) \\to 0$，因此$I(p) \\to \\infty$。\n在生物医学系统中稀有事件检测的背景下，生物标志物的存在通常是一个稀有事件，这对应于$p$非常接近$0$的情况。当$p \\to 0^{+}$时$I(p) \\to \\infty$这一事实表明，当一个事件本质上是稀有时，即使观察到它的单个实例（即$X_i=1$）也具有极高的信息量。对数似然函数变得极其尖锐，从而可以非常精确地估计微小的参数$p$。例如，当细胞数量很大（$n$很大）且真实$p$接近$0$时，预期观察到零次成功，但观察到一两次成功则为区分一个非常小的$p$和$p=0$提供了强有力的证据。高曲率（高Fisher信息）反映了这种高信息含量。对称地，如果一个事件几乎是确定的（$p \\approx 1$），观察到一个稀有的失败（即$X_i=0$）也同样具有很高的信息量。\n当分母$p(1-p)$最大化时，信息量最小化，这发生在$p=1/2$时，此时任何单次试验结果的不确定性最大。",
            "answer": "$$\\boxed{\\frac{n}{p(1-p)}}$$"
        },
        {
            "introduction": "在掌握了单参数模型之后，我们自然会想知道如何处理包含多个未知参数的系统。本练习将挑战您推导高斯分布的均值 ($\\mu$) 和方差 ($\\sigma^2$) 的 $2 \\times 2$ 费雪信息矩阵。 这个推导至关重要，因为它不仅展示了矩阵形式的应用，还将揭示参数正交性的重要概念——在这种特定参数化下，关于均值和方差的信息是如何解耦的。",
            "id": "3886010",
            "problem": "一个生物标志物定量系统由于内在的生物变异性和传感器噪声，会产生重复测量。设测量值被建模为来自未知均值为 $\\mu$、未知方差为 $\\sigma^{2}$ 的高斯分布的独立同分布 (i.i.d.) 样本 $y_{1},y_{2},\\dots,y_{n}$，即 $y_{i}\\sim \\mathcal{N}(\\mu,\\sigma^{2})$，其中 $i=1,\\dots,n$。在针对该生物医学系统的参数估计研究中，您感兴趣的是似然函数对参数的局部灵敏度，这由参数向量 $(\\mu,\\sigma^{2})$ 的费雪信息矩阵来概括。\n\n从第一性原理出发，即i.i.d.高斯测量的联合概率密度、$(\\mu,\\sigma^{2})$ 的对数似然，以及费雪信息矩阵根据得分函数及其期望曲率的定义，为样本量 $n$ 推导费雪信息矩阵 $I(\\mu,\\sigma^{2})$ 的显式解析表达式。\n\n您的推导过程必须明确考虑关于数据生成模型的期望，并且最终结果中不得包含任何随机样本的实现值。\n\n请以单个闭式 $2\\times 2$ 矩阵的形式给出最终答案，用 $n$ 和 $\\sigma^{2}$ 表示，并按参数向量 $(\\mu,\\sigma^{2})$ 的顺序排列。无需四舍五入。不要在最终矩阵中包含任何单位。",
            "solution": "该问题是良构的、有科学依据的，并包含了得出唯一解所需的所有信息。任务是为来自高斯分布 $\\mathcal{N}(\\mu, \\sigma^2)$ 的 $n$ 个独立同分布样本推导费雪信息矩阵 (FIM)，参数向量为 $\\theta = (\\mu, \\sigma^2)$。\n\n设 $Y = (y_1, y_2, \\dots, y_n)$ 是独立同分布样本的向量，其中每个 $y_i \\sim \\mathcal{N}(\\mu, \\sigma^2)$。单个样本 $y_i$ 的概率密度函数 (PDF) 是\n$$f(y_i | \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\mu)^2}{2\\sigma^2}\\right)$$\n由于样本的独立同分布特性，联合 PDF，即似然函数 $L(Y | \\mu, \\sigma^2)$，是个体 PDF 的乘积：\n$$L(Y | \\mu, \\sigma^2) = \\prod_{i=1}^{n} f(y_i | \\mu, \\sigma^2) = \\left(\\frac{1}{2\\pi\\sigma^2}\\right)^{n/2} \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - \\mu)^2\\right)$$\n对数似然函数，记为 $\\ell(\\mu, \\sigma^2) = \\ln L(Y | \\mu, \\sigma^2)$，是：\n$$\\ell(\\mu, \\sigma^2) = \\ln\\left[\\left(\\frac{1}{2\\pi\\sigma^2}\\right)^{n/2}\\right] + \\ln\\left[\\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - \\mu)^2\\right)\\right]$$\n$$\\ell(\\mu, \\sigma^2) = -\\frac{n}{2} \\ln(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - \\mu)^2$$\n$$\\ell(\\mu, \\sigma^2) = -\\frac{n}{2} \\ln(2\\pi) - \\frac{n}{2} \\ln(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - \\mu)^2$$\n对于参数向量 $\\theta = (\\theta_1, \\theta_2)^T = (\\mu, \\sigma^2)^T$，费雪信息矩阵 $I(\\theta)$ 是一个 $2 \\times 2$ 矩阵，其第 $(j, k)$ 个元素由下式给出：\n$$[I(\\theta)]_{jk} = -E\\left[\\frac{\\partial^2 \\ell(\\theta)}{\\partial\\theta_j \\partial\\theta_k}\\right]$$\n其中期望 $E[\\cdot]$ 是关于数据生成分布计算的。我们必须计算对数似然函数的二阶偏导数。\n\n首先，我们计算一阶偏导数（得分向量）：\n$$\\frac{\\partial\\ell}{\\partial\\mu} = \\frac{\\partial}{\\partial\\mu}\\left(-\\frac{n}{2} \\ln(2\\pi) - \\frac{n}{2} \\ln(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - \\mu)^2\\right) = -\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} 2(y_i - \\mu)(-1) = \\frac{1}{\\sigma^2}\\sum_{i=1}^{n}(y_i - \\mu)$$\n$$\\frac{\\partial\\ell}{\\partial\\sigma^2} = \\frac{\\partial}{\\partial\\sigma^2}\\left(-\\frac{n}{2} \\ln(2\\pi) - \\frac{n}{2} \\ln(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - \\mu)^2\\right) = -\\frac{n}{2\\sigma^2} + \\frac{1}{2(\\sigma^2)^2}\\sum_{i=1}^{n}(y_i - \\mu)^2$$\n接下来，我们计算二阶偏导数（海森矩阵）：\n\\begin{enumerate}\n    \\item 对于 $\\frac{\\partial^2\\ell}{\\partial\\mu^2}$：\n    $$\\frac{\\partial^2\\ell}{\\partial\\mu^2} = \\frac{\\partial}{\\partial\\mu}\\left(\\frac{1}{\\sigma^2}\\sum_{i=1}^{n}(y_i - \\mu)\\right) = \\frac{1}{\\sigma^2}\\sum_{i=1}^{n}(-1) = -\\frac{n}{\\sigma^2}$$\n    \\item 对于混合偏导数 $\\frac{\\partial^2\\ell}{\\partial\\mu \\partial\\sigma^2}$：\n    $$\\frac{\\partial^2\\ell}{\\partial\\mu \\partial\\sigma^2} = \\frac{\\partial}{\\partial\\mu}\\left(-\\frac{n}{2\\sigma^2} + \\frac{1}{2(\\sigma^2)^2}\\sum_{i=1}^{n}(y_i - \\mu)^2\\right) = \\frac{1}{2(\\sigma^2)^2}\\sum_{i=1}^{n}2(y_i - \\mu)(-1) = -\\frac{1}{(\\sigma^2)^2}\\sum_{i=1}^{n}(y_i - \\mu)$$\n    \\item 对于 $\\frac{\\partial^2\\ell}{\\partial(\\sigma^2)^2}$：\n    $$\\frac{\\partial^2\\ell}{\\partial(\\sigma^2)^2} = \\frac{\\partial}{\\partial\\sigma^2}\\left(-\\frac{n}{2\\sigma^2} + \\frac{1}{2(\\sigma^2)^2}\\sum_{i=1}^{n}(y_i - \\mu)^2\\right) = \\frac{n}{2(\\sigma^2)^2} - \\frac{1}{(\\sigma^2)^3}\\sum_{i=1}^{n}(y_i - \\mu)^2$$\n\\end{enumerate}\n现在，我们计算每个二阶导数的负期望，以求得 FIM 的元素。\n\\begin{enumerate}\n    \\item $I_{\\mu\\mu} = -E\\left[\\frac{\\partial^2\\ell}{\\partial\\mu^2}\\right] = -E\\left[-\\frac{n}{\\sigma^2}\\right]$。由于此项相对于 $y_i$ 是一个常数，所以期望就是该项本身。\n    $$I_{\\mu\\mu} = -\\left(-\\frac{n}{\\sigma^2}\\right) = \\frac{n}{\\sigma^2}$$\n    \\item $I_{\\mu, \\sigma^2} = I_{\\sigma^2, \\mu} = -E\\left[\\frac{\\partial^2\\ell}{\\partial\\mu \\partial\\sigma^2}\\right] = -E\\left[-\\frac{1}{(\\sigma^2)^2}\\sum_{i=1}^{n}(y_i - \\mu)\\right] = \\frac{1}{(\\sigma^2)^2}\\sum_{i=1}^{n}E[y_i - \\mu]$。根据定义，$E[y_i] = \\mu$，所以 $E[y_i - \\mu] = E[y_i] - \\mu = \\mu - \\mu = 0$。\n    $$I_{\\mu, \\sigma^2} = \\frac{1}{(\\sigma^2)^2}\\sum_{i=1}^{n}(0) = 0$$\n    \\item $I_{\\sigma^2, \\sigma^2} = -E\\left[\\frac{\\partial^2\\ell}{\\partial(\\sigma^2)^2}\\right] = -E\\left[\\frac{n}{2(\\sigma^2)^2} - \\frac{1}{(\\sigma^2)^3}\\sum_{i=1}^{n}(y_i - \\mu)^2\\right]$。利用期望的线性性质：\n    $$I_{\\sigma^2, \\sigma^2} = -\\left(\\frac{n}{2(\\sigma^2)^2} - \\frac{1}{(\\sigma^2)^3}\\sum_{i=1}^{n}E[(y_i - \\mu)^2]\\right)$$\n    项 $E[(y_i - \\mu)^2]$ 是 $y_i$ 方差的定义，即 $\\sigma^2$。\n    $$I_{\\sigma^2, \\sigma^2} = -\\left(\\frac{n}{2(\\sigma^2)^2} - \\frac{1}{(\\sigma^2)^3}\\sum_{i=1}^{n}\\sigma^2\\right) = -\\left(\\frac{n}{2(\\sigma^2)^2} - \\frac{n\\sigma^2}{(\\sigma^2)^3}\\right) = -\\left(\\frac{n}{2(\\sigma^2)^2} - \\frac{n}{(\\sigma^2)^2}\\right)$$\n    $$I_{\\sigma^2, \\sigma^2} = -\\left(-\\frac{n}{2(\\sigma^2)^2}\\right) = \\frac{n}{2(\\sigma^2)^2}$$\n\\end{enumerate}\n将各元素组装成参数向量 $(\\mu, \\sigma^2)$ 的费雪信息矩阵 $I(\\mu, \\sigma^2)$：\n$$I(\\mu, \\sigma^2) = \\begin{pmatrix} I_{\\mu\\mu}  I_{\\mu, \\sigma^2} \\\\ I_{\\sigma^2, \\mu}  I_{\\sigma^2, \\sigma^2} \\end{pmatrix} = \\begin{pmatrix} \\frac{n}{\\sigma^2}  0 \\\\ 0  \\frac{n}{2(\\sigma^2)^2} \\end{pmatrix}$$\n非对角线项为零，这表明 $\\mu$ 和 $\\sigma^2$ 的估计量是渐近正交的。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{n}{\\sigma^{2}} & 0 \\\\\n0 & \\frac{n}{2 (\\sigma^{2})^{2}}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "最后，我们将费雪信息矩阵的概念应用于生物医学研究中无处不在的分类工具——逻辑回归模型。这个高级练习要求您为一个广义线性模型推导费雪信息矩阵，您会发现信息量现在不仅取决于样本数量，还与协变量的设计以及参数自身的取值息息相关。 更重要的是，此练习将理论与实践相结合，探讨了当数据出现“分离”现象时，费雪信息矩阵如何变得奇异，从而导致模型估计失效的根本原因。",
            "id": "3885995",
            "problem": "建立一个生物医学分类模型，用于根据患者的协变量预测某种病理生理状态的存在。对于每个由 $i \\in \\{1,\\dots,n\\}$ 索引的个体，一个二元结果 $y_i \\in \\{0,1\\}$ 表示该状态的存在 ($y_i=1$) 或不存在 ($y_i=0$)。记录一个协变量向量 $x_i \\in \\mathbb{R}^{p}$（若适用，则包含截距项），其条件结果模型是具有标准 logit 链接的广义线性模型：$y_i \\mid x_i \\sim \\mathrm{Bernoulli}(\\pi_i)$，其中 $\\pi_i = \\mathrm{logit}^{-1}(x_i^{\\top}\\beta)$，$\\beta \\in \\mathbb{R}^{p}$ 是回归参数向量，且 $\\mathrm{logit}^{-1}(t) = 1/(1+\\exp(-t))$。假设在给定协变量和参数的情况下，各观测值是独立的，并将 $\\{x_i\\}_{i=1}^{n}$ 视为固定的（非随机的）设计点。\n\n从独立伯努利变量的似然和 logit 链接的基本定义出发，仅使用第一性原理推导此模型下 $\\beta$ 的费雪信息矩阵 (FIM) 的闭式表达式。接着，证明该表达式等于得分向量外积的期望值。最后，讨论协变量和结果中的完全分离与准完全分离如何影响 FIM 的秩和条件数，确定在何种关于 $\\{x_i,y_i\\}_{i=1}^{n}$ 的条件下 FIM 会变得奇异，并从对数似然的曲率角度解释其根本原因。\n\n你的最终答案必须是 FIM 的一个单一闭式解析表达式，表示为 $\\beta$ 的函数，并用 $\\pi_i$ 和 $x_i$ 写出。不需要进行数值计算或四舍五入。最终表达式不表示任何单位。",
            "solution": "所述问题是有效的。这是一个适定、有科学依据且客观的数理统计问题，具体涉及广义线性模型的性质。它要求从第一性原理出发，为逻辑斯蒂回归模型推导一个基本量——费雪信息矩阵 (FIM)，并在特定数据条件下分析其性质。所有定义和前提都是标准的，没有矛盾或歧义。因此，我们可以进行形式化的求解。\n\n解决方案的结构如下：\n1.  逻辑斯蒂回归模型的对数似然函数的推导。\n2.  得分向量的计算，即对数似然函数的梯度。\n3.  Hessian 矩阵的计算，即对数似然的二阶偏导数矩阵。\n4.  根据定义 $\\mathcal{I}(\\beta) = -E[H(\\beta)]$ 从 Hessian 矩阵推导费雪信息矩阵 (FIM)。\n5.  证明 FIM 的另一个等价形式 $\\mathcal{I}(\\beta) = E[S(\\beta)S(\\beta)^{\\top}]$。\n6.  在完全分离和准完全分离条件下讨论 FIM 的奇异性。\n\n设参数向量为 $\\beta \\in \\mathbb{R}^{p}$。数据包含 $n$ 个独立观测值 $\\{(y_i, x_i)\\}_{i=1}^{n}$，其中 $y_i \\in \\{0,1\\}$ 是二元结果，$x_i \\in \\mathbb{R}^{p}$ 是固定协变量的向量。\n\n模型指定 $y_i$ 服从成功概率为 $\\pi_i$ 的伯努利分布：\n$y_i \\mid x_i, \\beta \\sim \\mathrm{Bernoulli}(\\pi_i)$。\n单个观测值 $y_i$ 的概率质量函数 (PMF) 由下式给出：\n$$P(y_i \\mid x_i, \\beta) = \\pi_i^{y_i} (1-\\pi_i)^{1-y_i}$$\n概率 $\\pi_i$ 通过反 logit（或 logistic）函数与协变量和参数相关联：\n$$\\pi_i = \\frac{1}{1 + \\exp(-x_i^{\\top}\\beta)}$$\n这是伯努利分布的标准链接函数。将其表示为 $\\pi_i = \\frac{\\exp(x_i^{\\top}\\beta)}{1 + \\exp(x_i^{\\top}\\beta)}$ 会很方便。由此，我们也有 $1-\\pi_i = \\frac{1}{1 + \\exp(x_i^{\\top}\\beta)}$。\n\n$n$ 个独立观测值的总似然函数 $L(\\beta)$ 是各个概率质量函数的乘积：\n$$L(\\beta) = \\prod_{i=1}^{n} \\pi_i^{y_i} (1-\\pi_i)^{1-y_i}$$\n对数似然函数 $\\ell(\\beta)$ 是似然函数的自然对数：\n$$\\ell(\\beta) = \\ln(L(\\beta)) = \\sum_{i=1}^{n} \\left[ y_i \\ln(\\pi_i) + (1-y_i) \\ln(1-\\pi_i) \\right]$$\n设 $\\eta_i = x_i^{\\top}\\beta$。代入 $\\pi_i$ 和 $1-\\pi_i$ 的表达式：\n$$\\ln(\\pi_i) = \\ln\\left(\\frac{\\exp(\\eta_i)}{1 + \\exp(\\eta_i)}\\right) = \\eta_i - \\ln(1 + \\exp(\\eta_i))$$\n$$\\ln(1-\\pi_i) = \\ln\\left(\\frac{1}{1 + \\exp(\\eta_i)}\\right) = -\\ln(1 + \\exp(\\eta_i))$$\n将这些代入对数似然表达式：\n$$\\ell(\\beta) = \\sum_{i=1}^{n} \\left[ y_i (\\eta_i - \\ln(1 + \\exp(\\eta_i))) + (1-y_i)(-\\ln(1 + \\exp(\\eta_i))) \\right]$$\n$$\\ell(\\beta) = \\sum_{i=1}^{n} \\left[ y_i \\eta_i - y_i \\ln(1 + \\exp(\\eta_i)) - \\ln(1 + \\exp(\\eta_i)) + y_i \\ln(1 + \\exp(\\eta_i)) \\right]$$\n$$\\ell(\\beta) = \\sum_{i=1}^{n} \\left[ y_i \\eta_i - \\ln(1 + \\exp(\\eta_i)) \\right]$$\n$$\\ell(\\beta) = \\sum_{i=1}^{n} \\left[ y_i x_i^{\\top}\\beta - \\ln(1 + \\exp(x_i^{\\top}\\beta)) \\right]$$\n这是带有 logit 链接的伯努利广义线性模型的对数似然函数的标准形式。\n\n得分向量 $S(\\beta)$ 是对数似然函数关于参数向量 $\\beta$ 的梯度。其第 $j$ 个分量 $S_j(\\beta)$ 是关于 $\\beta_j$ 的偏导数：\n$$S_j(\\beta) = \\frac{\\partial \\ell(\\beta)}{\\partial \\beta_j} = \\frac{\\partial}{\\partial \\beta_j} \\sum_{i=1}^{n} \\left[ y_i x_i^{\\top}\\beta - \\ln(1 + \\exp(x_i^{\\top}\\beta)) \\right]$$\n$x_i^{\\top}\\beta = \\sum_{k=1}^p x_{ik}\\beta_k$ 关于 $\\beta_j$ 的导数是 $x_{ij}$。对第二项使用链式法则：\n$$S_j(\\beta) = \\sum_{i=1}^{n} \\left[ y_i x_{ij} - \\frac{1}{1 + \\exp(x_i^{\\top}\\beta)} \\cdot \\exp(x_i^{\\top}\\beta) \\cdot x_{ij} \\right]$$\n认识到 $\\frac{\\exp(x_i^{\\top}\\beta)}{1 + \\exp(x_i^{\\top}\\beta)} = \\pi_i$，我们得到：\n$$S_j(\\beta) = \\sum_{i=1}^{n} (y_i - \\pi_i) x_{ij}$$\n写成向量形式，得分向量为：\n$$S(\\beta) = \\nabla_{\\beta} \\ell(\\beta) = \\sum_{i=1}^{n} (y_i - \\pi_i) x_i$$\n这可以紧凑地写为 $S(\\beta) = X^{\\top}(y-\\pi)$，其中 $X$ 是行向量为 $x_i^{\\top}$ 的 $n \\times p$ 设计矩阵，$y$ 和 $\\pi$ 分别是结果和概率的 $n \\times 1$ 向量。\n\n接下来，我们推导 Hessian 矩阵 $H(\\beta)$，其元素是对数似然的二阶偏导数 $H_{jk}(\\beta) = \\frac{\\partial^2 \\ell(\\beta)}{\\partial \\beta_j \\partial \\beta_k}$。这可以通过将得分向量的第 $j$ 个分量对 $\\beta_k$ 求导得到：\n$$H_{jk}(\\beta) = \\frac{\\partial S_j(\\beta)}{\\partial \\beta_k} = \\frac{\\partial}{\\partial \\beta_k} \\sum_{i=1}^{n} (y_i - \\pi_i) x_{ij} = \\sum_{i=1}^{n} \\left( -\\frac{\\partial \\pi_i}{\\partial \\beta_k} \\right) x_{ij}$$\n我们需要 $\\pi_i$ 关于 $\\beta_k$ 的导数。使用链式法则和 $\\eta_i = x_i^{\\top}\\beta$：\n$$\\frac{\\partial \\pi_i}{\\partial \\beta_k} = \\frac{\\partial \\pi_i}{\\partial \\eta_i} \\frac{\\partial \\eta_i}{\\partial \\beta_k}$$\n我们有 $\\frac{\\partial \\eta_i}{\\partial \\beta_k} = x_{ik}$。对于第一部分：\n$$\\frac{\\partial \\pi_i}{\\partial \\eta_i} = \\frac{\\partial}{\\partial \\eta_i} \\left( \\frac{\\exp(\\eta_i)}{1+\\exp(\\eta_i)} \\right) = \\frac{\\exp(\\eta_i)(1+\\exp(\\eta_i)) - \\exp(\\eta_i)\\exp(\\eta_i)}{(1+\\exp(\\eta_i))^2} = \\frac{\\exp(\\eta_i)}{(1+\\exp(\\eta_i))^2}$$\n这可以重写为：\n$$\\frac{\\partial \\pi_i}{\\partial \\eta_i} = \\frac{\\exp(\\eta_i)}{1+\\exp(\\eta_i)} \\cdot \\frac{1}{1+\\exp(\\eta_i)} = \\pi_i (1-\\pi_i)$$\n因此，$\\frac{\\partial \\pi_i}{\\partial \\beta_k} = \\pi_i(1-\\pi_i)x_{ik}$。将此代入 $H_{jk}(\\beta)$ 的表达式中：\n$$H_{jk}(\\beta) = \\sum_{i=1}^{n} -(\\pi_i(1-\\pi_i)x_{ik}) x_{ij} = -\\sum_{i=1}^{n} \\pi_i(1-\\pi_i) x_{ij} x_{ik}$$\n因此，Hessian 矩阵可以写成外积之和：\n$$H(\\beta) = -\\sum_{i=1}^{n} \\pi_i(1-\\pi_i) x_i x_i^{\\top}$$\n用矩阵表示法，即为 $H(\\beta) = -X^{\\top}WX$，其中 $W$ 是一个 $n \\times n$ 的对角矩阵，其对角线元素为 $W_{ii} = \\pi_i(1-\\pi_i)$。\n\n费雪信息矩阵 (FIM) $\\mathcal{I}(\\beta)$ 定义为 Hessian 矩阵期望值的负数：\n$$\\mathcal{I}(\\beta) = -E[H(\\beta)]$$\n期望是关于数据 $Y$ 的分布计算的。协变量 $x_i$ 是固定的，参数 $\\beta$ 在求期望时也是固定的。项 $\\pi_i$ 仅是 $x_i$ 和 $\\beta$ 的函数，因此相对于 $Y$ 是非随机的。因此，Hessian 矩阵 $H(\\beta)$ 是非随机的，其期望就是其自身：\n$$E[H(\\beta)] = H(\\beta) = -\\sum_{i=1}^{n} \\pi_i(1-\\pi_i) x_i x_i^{\\top}$$\n因此，FIM 为：\n$$\\mathcal{I}(\\beta) = -H(\\beta) = \\sum_{i=1}^{n} \\pi_i(1-\\pi_i) x_i x_i^{\\top}$$\n这也可以写作 $\\mathcal{I}(\\beta) = X^{\\top}WX$。\n\n现在，我们证明它等于得分向量外积的期望 $E[S(\\beta)S(\\beta)^{\\top}]$。\n$$S(\\beta)S(\\beta)^{\\top} = \\left(\\sum_{i=1}^{n} (y_i - \\pi_i) x_i\\right) \\left(\\sum_{j=1}^{n} (y_j - \\pi_j) x_j\\right)^{\\top} = \\sum_{i=1}^{n}\\sum_{j=1}^{n} (y_i - \\pi_i)(y_j - \\pi_j) x_i x_j^{\\top}$$\n取期望：\n$$E[S(\\beta)S(\\beta)^{\\top}] = \\sum_{i=1}^{n}\\sum_{j=1}^{n} E[(y_i - \\pi_i)(y_j - \\pi_j)] x_i x_j^{\\top}$$\n我们考虑期望项 $E[(y_i - \\pi_i)(y_j - \\pi_j)]$ 的两种情况：\n1.  当 $i \\neq j$ 时，观测值 $y_i$ 和 $y_j$ 是独立的。因此：\n    $E[(y_i - \\pi_i)(y_j - \\pi_j)] = E[y_i - \\pi_i] E[y_j - \\pi_j]$。由于对任意 $k$ 都有 $E[y_k] = \\pi_k$，我们得到 $E[y_k - \\pi_k] = 0$。所以，当 $i \\neq j$ 时，该项为 $0$。\n2.  当 $i = j$ 时，该项变为：\n    $E[(y_i - \\pi_i)^2] = \\mathrm{Var}(y_i)$。对于一个伯努利变量，其方差为 $\\mathrm{Var}(y_i) = \\pi_i(1-\\pi_i)$。\n\n双重求和坍缩为关于 $i=j$ 的单个求和：\n$$E[S(\\beta)S(\\beta)^{\\top}] = \\sum_{i=1}^{n} E[(y_i - \\pi_i)^2] x_i x_i^{\\top} = \\sum_{i=1}^{n} \\pi_i(1-\\pi_i) x_i x_i^{\\top}$$\n这与从 Hessian 矩阵推导出的表达式相同，从而证实了该等式。\n\n最后，我们讨论数据分离对 FIM 的影响。FIM 是一系列半正定矩阵 $\\pi_i(1-\\pi_i) x_i x_i^{\\top}$ 的和。它为正定（因此非奇异且可逆）的充要条件是，对于那些满足 $\\pi_i(1-\\pi_i) > 0$ 的向量 $\\{x_i\\}$ 集合能够张成 $\\mathbb{R}^p$ 空间。\n权重项 $\\pi_i(1-\\pi_i)$ 非零，当且仅当 $0 < \\pi_i < 1$。这要求 $x_i^{\\top}\\beta$ 是有限的。\n- **完全分离**发生于存在一个向量 $\\beta$ 能够完美地分离结果：对于所有 $y_i=1$ 的 $i$，$x_i^{\\top}\\beta > 0$；对于所有 $y_i=0$ 的 $i$，$x_i^{\\top}\\beta < 0$。在这种情况下，似然函数是单调的，并且对于任何有限的 $\\beta$ 都无法达到最大值。当一个迭代优化算法在分离方向上增加 $\\beta$ 的模长，即 $\\|\\beta\\| \\to \\infty$ 时，预测概率 $\\pi_i$ 会收敛到观测结果 $y_i$。也就是说，对于所有观测值 $i$，$\\pi_i \\to 1$ 或 $\\pi_i \\to 0$。\n- **准完全分离**与此类似，但允许一些点位于分离边界上，即对于 $y_i=1$，$x_i^{\\top}\\beta \\ge 0$；对于 $y_i=0$，$x_i^{\\top}\\beta \\le 0$。$\\beta$ 的最大似然估计 (MLE) 也是无穷大。\n在任一种分离情况下，当 $\\|\\beta\\| \\to \\infty$ 时，权重 $\\pi_i(1-\\pi_i)$ 对所有 $i$ 都趋近于 $0$。因此，费雪信息矩阵 $\\mathcal{I}(\\beta) = \\sum_{i=1}^{n} \\pi_i(1-\\pi_i) x_i x_i^{\\top}$ 趋近于零矩阵，而零矩阵是奇异的。\nFIM 的奇异性与对数似然曲面的曲率直接相关，因为在此模型中 $\\mathcal{I}(\\beta)=-H(\\beta)$。一个奇异的 FIM 意味着 Hessian 矩阵至少有一个零特征值。Hessian 矩阵的特征值对应于参数空间某一主方向上对数似然函数的曲率。零特征值表示对数似然曲面在该方向上是平坦的。在数据分离的情况下，这个平坦方向对应于参数空间中分离向量的方向。这种平坦性表明参数无法从数据中被唯一识别，因为沿此方向移动不会改变似然值，也就不存在唯一的有限最大值。这种曲率的丧失意味着在分离方向上，关于参数向量模长的信息为零。",
            "answer": "$$\\boxed{\\sum_{i=1}^{n} \\pi_i(1-\\pi_i) x_i x_i^{\\top}}$$"
        }
    ]
}