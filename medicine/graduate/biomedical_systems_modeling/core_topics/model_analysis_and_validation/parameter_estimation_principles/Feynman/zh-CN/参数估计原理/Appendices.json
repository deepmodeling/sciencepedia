{
    "hands_on_practices": [
        {
            "introduction": "在我们尝试从数据中估计模型参数之前，必须先回答一个更基本的问题：在理想情况下（即没有噪声的数据），我们能否唯一地确定这些参数的值？这个概念被称为结构可辨识性（structural identifiability），它是任何有意义的参数估计工作的基石。下面的练习  将引导你从第一性原理出发，为一个简单的线性系统推导其参数可辨识的条件，从而牢固地建立起这一核心概念。",
            "id": "3916249",
            "problem": "在生物医学系统建模中，考虑一个简化但科学上合理的输注-响应子系统模型，其中，在有限的观测窗口内的所有时间 $t$，测量输出 $y(t)$（例如，与施加输入成比例的生物标志物浓度）满足 $y(t)=\\theta u(t)$，其中 $\\theta$ 是一个未知的实数标量参数，表示从输入 $u(t)$ 到输出 $y(t)$ 的增益。假设在不同的时间点 $t_1,\\dots,t_N$ 进行无噪声测量 $y(t_i)$，并且输入 $u(t)$ 是完全已知的，可以在这些时间点进行评估。根据给定输入下参数到输出映射的单射性这一定义，从第一性原理出发，推导使 $\\theta$ 结构可辨识的输入值 $u(t_1),\\dots,u(t_N)$ 的充要条件。然后，在这些条件下，不使用任何简便公式，推导出一个显式的闭式逆映射，以从测量数据 $\\{(t_i,u(t_i),y(t_i))\\}_{i=1}^{N}$ 中恢复 $\\theta$。您的最终答案必须是仅用 $\\{u(t_i),y(t_i)\\}_{i=1}^{N}$ 表示 $\\theta$ 的单一解析表达式。无需四舍五入；请精确表达您的答案，最终表达式中无需物理单位。",
            "solution": "对问题陈述进行验证。\n\n### 步骤 1：提取已知条件\n-   **模型方程：** 测量输出 $y(t)$ 通过方程 $y(t) = \\theta u(t)$ 与输入 $u(t)$ 相关，其中 $\\theta$ 是一个未知的实数标量参数。\n-   **测量方案：** 在有限的观测窗口内，于 $N$ 个不同的时间点 $t_1, \\dots, t_N$ 进行无噪声测量。\n-   **数据：** 可用数据集为 $\\{(t_i, u(t_i), y(t_i))\\}_{i=1}^{N}$。输入值 $u(t_i)$ 已知。\n-   **可辨识性定义：** 结构可辨识性定义为参数到输出映射的单射性。\n-   **目标 1：** 推导使 $\\theta$ 结构可辨识的输入值 $\\{u(t_i)\\}_{i=1}^{N}$ 的充要条件。\n-   **目标 2：** 在这些条件下，从第一性原理出发，推导一个显式的闭式逆映射以从数据中恢复 $\\theta$。\n-   **最终答案格式：** 最终答案必须是仅用 $\\{u(t_i), y(t_i)\\}_{i=1}^{N}$ 表示 $\\theta$ 的单一解析表达式。\n\n### 步骤 2：使用提取的已知条件进行验证\n-   **科学依据：** 模型 $y(t) = \\theta u(t)$ 代表一个简单的静态线性增益系统。这是许多领域中的一个基本且科学上合理的模型，包括作为生物医学系统中的简化模型（例如，稳态剂量-响应）。结构可辨识性的概念是系统理论和参数估计的核心原则。该问题具有科学依据。\n-   **适定性：** 问题是适定的。它要求找出 $\\theta$ 存在唯一解的条件，然后要求推导该解。这是可辨识性问题的标准结构。\n-   **客观性：** 问题以精确、客观的数学语言陈述。\n-   **完整性与一致性：** 问题是自洽的。它提供了模型、数据结构、可辨识性的定义和一个明确的目标。“无噪声测量”的假设是分析*结构*可辨识性的一个标准且必要的理想化，因为结构可辨识性是模型结构本身的属性，与噪声过程无关。\n-   **现实性与可行性：** 该模型是一个理想化的模型，但并非科学上不合理或物理上不可能。它构成了更复杂模型的基础。\n-   **其他缺陷：** 该问题并非微不足道，因为它需要从第一性原理进行严格的推导。它不是非适定的、隐喻性的或不可验证的。\n\n### 步骤 3：结论与行动\n该问题有效。将提供完整解答。\n\n问题要求两件事：首先，参数 $\\theta$ 结构可辨识的条件；其次，从数据中求出 $\\theta$ 的显式逆映射。\n\n让我们首先形式化参数到输出的映射。参数是标量 $\\theta \\in \\mathbb{R}$。实验的输出包含一组 $N$ 个测量值，我们可以将其排列成一个向量 $\\mathbf{y} \\in \\mathbb{R}^N$，其中 $\\mathbf{y} = \\begin{pmatrix} y(t_1) & y(t_2) & \\dots & y(t_N) \\end{pmatrix}^T$。模型方程 $y(t) = \\theta u(t)$ 为每个测量点提供了关系：\n$$y(t_i) = \\theta u(t_i) \\quad \\text{对于 } i = 1, 2, \\dots, N$$\n这个方程组可以写成向量形式。令 $\\mathbf{u} = \\begin{pmatrix} u(t_1) & u(t_2) & \\dots & u(t_N) \\end{pmatrix}^T$ 为已知输入值的向量。则该方程组为：\n$$\\mathbf{y} = \\theta \\mathbf{u}$$\n参数到输出的映射，我们记为 $\\mathcal{M}$，是一个函数，它接受参数 $\\theta$，并对于由 $\\mathbf{u}$ 定义的给定输入实验，产生输出向量 $\\mathbf{y}$。因此，$\\mathcal{M}: \\mathbb{R} \\to \\mathbb{R}^N$ 定义为：\n$$\\mathcal{M}(\\theta) = \\theta \\mathbf{u}$$\n问题指出，$\\theta$ 是结构可辨识的，当且仅当这个映射 $\\mathcal{M}$ 是单射的。如果 $f(x_1) = f(x_2)$ 意味着 $x_1 = x_2$，则函数 $f$ 是单射的。将此定义应用于我们的映射 $\\mathcal{M}$，我们必须有：\n$$\\mathcal{M}(\\theta_1) = \\mathcal{M}(\\theta_2) \\implies \\theta_1 = \\theta_2$$\n让我们分析前提 $\\mathcal{M}(\\theta_1) = \\mathcal{M}(\\theta_2)$。根据我们的映射定义，这意味着：\n$$\\theta_1 \\mathbf{u} = \\theta_2 \\mathbf{u}$$\n重新整理这个向量方程，我们得到：\n$$(\\theta_1 - \\theta_2) \\mathbf{u} = \\mathbf{0}$$\n其中 $\\mathbf{0}$ 是 $\\mathbb{R}^N$ 中的零向量。这一个向量方程等价于 $N$ 个标量方程组成的方程组：\n$$(\\theta_1 - \\theta_2) u(t_i) = 0 \\quad \\text{对于 } i = 1, 2, \\dots, N$$\n为了使映射是单射的，这个方程组必须能强制得出结论 $\\theta_1 - \\theta_2 = 0$，这等价于 $\\theta_1 = \\theta_2$。方程 $(\\theta_1 - \\theta_2) u(t_i) = 0$ 在 $\\theta_1 - \\theta_2 = 0$ 或 $u(t_i) = 0$ 时成立。如果对所有 $i=1, \\dots, N$ 都有 $u(t_i) = 0$，那么 $\\mathbf{u} = \\mathbf{0}$。在这种情况下，方程变为 $(\\theta_1 - \\theta_2) \\cdot 0 = 0$，这对任意选择的 $\\theta_1$ 和 $\\theta_2$ 都成立。我们无法得出 $\\theta_1 = \\theta_2$ 的结论，因此参数是不可辨识的。\n反之，如果存在至少一个索引 $j \\in \\{1, \\dots, N\\}$ 使得 $u(t_j) \\neq 0$，那么为了使方程 $(\\theta_1 - \\theta_2) u(t_j) = 0$ 成立，我们必须有 $\\theta_1 - \\theta_2 = 0$。这意味着 $\\theta_1 = \\theta_2$。\n因此，映射 $\\mathcal{M}$ 是单射的，从而 $\\theta$ 是结构可辨识的充要条件是输入向量 $\\mathbf{u}$ 不是零向量。这意味着必须至少存在一个测量时间 $t_i$，使得该时间的输入 $u(t_i)$ 不为零。\n\n接下来，我们必须推导从数据 $\\{(t_i, u(t_i), y(t_i))\\}_{i=1}^{N}$ 中恢复 $\\theta$ 的逆映射。这需要解关于 $\\theta$ 的方程组 $y(t_i) = \\theta u(t_i)$。由于问题指定了无噪声测量，所有这些方程都同时成立。我们需要找到一个能整合所有 $N$ 个测量信息的 $\\theta$ 的表达式。这是一个经典的超定系统（如果 $N>1$），一个稳健的求解方法是找到使模型预测与测量值之间误差平方和最小的 $\\theta$ 值。这就是最小二乘法原理。即使在无噪声的情况下，该方法也保证能返回精确的真实值。\n\n将成本函数 $J(\\theta)$ 定义为残差平方和：\n$$J(\\theta) = \\sum_{i=1}^{N} (y(t_i) - \\theta u(t_i))^2$$\n为了找到使 $J(\\theta)$ 最小化的 $\\theta$ 值，我们对 $J$ 关于 $\\theta$ 求导，并令其为零。\n$$\\frac{dJ}{d\\theta} = \\sum_{i=1}^{N} \\frac{d}{d\\theta} (y(t_i) - \\theta u(t_i))^2$$\n$$= \\sum_{i=1}^{N} 2(y(t_i) - \\theta u(t_i))(-u(t_i))$$\n$$= -2 \\sum_{i=1}^{N} (y(t_i)u(t_i) - \\theta u(t_i)^2)$$\n将导数设为零：\n$$-2 \\sum_{i=1}^{N} (y(t_i)u(t_i) - \\theta u(t_i)^2) = 0$$\n$$\\sum_{i=1}^{N} y(t_i)u(t_i) - \\sum_{i=1}^{N} \\theta u(t_i)^2 = 0$$\n$$\\sum_{i=1}^{N} y(t_i)u(t_i) = \\theta \\sum_{i=1}^{N} u(t_i)^2$$\n我们可以通过除以总和 $\\sum_{i=1}^{N} u(t_i)^2$ 来解出 $\\theta$。这个操作当且仅当该总和不为零时有效。平方和 $\\sum_{i=1}^{N} u(t_i)^2$ 为零，当且仅当每一项 $u(t_i)^2$ 都为零，这意味着对所有 $i=1, \\dots, N$ 都有 $u(t_i) = 0$。这恰恰是我们发现 $\\theta$ *不可*辨识的条件。因此，在推导出的可辨识性条件下（即至少有一个 $u(t_i) \\neq 0$），分母保证不为零，$\\theta$ 的唯一解存在。\n\n显式的闭式逆映射为：\n$$\\theta = \\frac{\\sum_{i=1}^{N} y(t_i) u(t_i)}{\\sum_{i=1}^{N} u(t_i)^2}$$\n此表达式根据已知的输入值 $\\{u(t_i)\\}$ 和测量的输出值 $\\{y(t_i)\\}$ 提供了 $\\theta$ 的值，符合要求。\n这也可以用向量点积表示为 $\\theta = (\\mathbf{y} \\cdot \\mathbf{u}) / (\\mathbf{u} \\cdot \\mathbf{u})$。这就是逆映射的最终表达式。",
            "answer": "$$\n\\boxed{\\frac{\\sum_{i=1}^{N} y(t_i) u(t_i)}{\\sum_{i=1}^{N} u(t_i)^2}}\n$$"
        },
        {
            "introduction": "确定了参数是可辨识的之后，我们便面临下一个问题：如何选择最佳的估计方法？对于同一个参数，我们往往可以构建出多种不同的估计量（estimator）。这个练习  深入探讨了评估估计量性能的关键概念——经典的偏差-方差权衡（bias-variance tradeoff）。通过分析一个“收缩估计量”（shrinkage estimator），你将发现，引入少量偏差有时可以显著降低估计量的方差，从而获得比传统无偏估计量更低的总均方误差（Mean Squared Error）。",
            "id": "3916253",
            "problem": "一个实验室正在一个单室静脉推注药代动力学模型中，量化肝清除率，该参数用 $\\theta$ 表示，单位为升/小时。对于单个受试者，重复的微透析校准产生独立的测量值 $y_{i}$，这些测量值遵循加性噪声模型 $y_{i} = \\theta + \\varepsilon_{i}$，其中 $\\varepsilon_{i}$ 是独立同分布的，服从高斯分布 $\\varepsilon_{i} \\sim \\mathcal{N}(0, \\sigma^{2})$。对相似队列的荟萃分析为缩减提供了基线清除率 $\\mu_{0}$。\n\n考虑$\\theta$的两个估计量：\n- 无偏估计量 $\\hat{\\theta}_{U} = \\bar{y}$，其中 $\\bar{y}$ 是 $\\{y_{i}\\}_{i=1}^{n}$ 的样本均值。\n- 缩减估计量 $\\hat{\\theta}_{\\alpha} = \\alpha \\bar{y} + (1 - \\alpha)\\mu_{0}$，其中缩减权重 $\\alpha \\in [0, 1]$ 为固定值。\n\n从偏差、方差和均方误差 (MSE) 的定义出发，其中均方误差 (MSE) 定义为 $\\operatorname{MSE}(\\hat{\\theta}) = \\mathbb{E}\\big[(\\hat{\\theta} - \\theta)^{2}\\big]$，在给定的测量模型下，推导两种估计量的 MSE 作为 $\\alpha$、$n$、$\\sigma^{2}$、$\\mu_{0}$ 和 $\\theta$ 的函数。然后确定使缩减估计量的 MSE 最小化的$\\alpha$值。使用该最小值点，计算在以下情景中，有偏估计量的最小化 MSE 与无偏估计量的 MSE 之比：\n- $n = 12$\n- $\\sigma = 0.9$ 升/小时\n- $\\mu_{0} = 1.2$ 升/小时\n- $\\theta = 0.8$ 升/小时\n\n将最终比率表示为无量纲的小数。将最终答案四舍五入至四位有效数字。",
            "solution": "本问题要求推导参数 $\\theta$ 的两个估计量的均方误差 (MSE)，优化缩减参数 $\\alpha$，并计算在特定情景下 MSE 的比率。该问题提法明确，具有估计理论的科学依据，并提供了所有必要信息。\n\n首先，我们来确定样本均值 $\\bar{y}$ 的统计性质。测量值由模型 $y_{i} = \\theta + \\varepsilon_{i}$ 给出，其中 $\\varepsilon_{i}$ 是来自高斯分布 $\\mathcal{N}(0, \\sigma^{2})$ 的独立同分布 (i.i.d.) 随机变量。\n单次测量的期望为 $\\mathbb{E}[y_i] = \\mathbb{E}[\\theta + \\varepsilon_i] = \\theta + \\mathbb{E}[\\varepsilon_i] = \\theta + 0 = \\theta$。\n单次测量的方差为 $\\operatorname{Var}(y_i) = \\operatorname{Var}(\\theta + \\varepsilon_i) = \\operatorname{Var}(\\varepsilon_i) = \\sigma^2$。\n\n样本均值为 $\\bar{y} = \\frac{1}{n} \\sum_{i=1}^{n} y_i$。\n其期望为 $\\mathbb{E}[\\bar{y}] = \\mathbb{E}\\left[\\frac{1}{n} \\sum_{i=1}^{n} y_i\\right] = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{E}[y_i] = \\frac{1}{n} (n\\theta) = \\theta$。\n因为测量值 $y_i$ 是独立的，所以样本均值的方差为 $\\operatorname{Var}(\\bar{y}) = \\operatorname{Var}\\left(\\frac{1}{n} \\sum_{i=1}^{n} y_i\\right) = \\frac{1}{n^2} \\sum_{i=1}^{n} \\operatorname{Var}(y_i) = \\frac{1}{n^2} (n\\sigma^2) = \\frac{\\sigma^2}{n}$。\n\n估计量 $\\hat{\\theta}$ 的均方误差定义为 $\\operatorname{MSE}(\\hat{\\theta}) = \\mathbb{E}[(\\hat{\\theta} - \\theta)^2]$。这可以分解为偏差的平方与估计量方差之和：$\\operatorname{MSE}(\\hat{\\theta}) = (\\operatorname{Bias}(\\hat{\\theta}))^2 + \\operatorname{Var}(\\hat{\\theta})$，其中 $\\operatorname{Bias}(\\hat{\\theta}) = \\mathbb{E}[\\hat{\\theta}] - \\theta$。\n\n**1. 无偏估计量 $\\hat{\\theta}_{U}$ 的 MSE**\n无偏估计量由 $\\hat{\\theta}_{U} = \\bar{y}$ 给出。\n该估计量的偏差为 $\\operatorname{Bias}(\\hat{\\theta}_{U}) = \\mathbb{E}[\\hat{\\theta}_{U}] - \\theta = \\mathbb{E}[\\bar{y}] - \\theta = \\theta - \\theta = 0$。\n该估计量的方差为 $\\operatorname{Var}(\\hat{\\theta}_{U}) = \\operatorname{Var}(\\bar{y}) = \\frac{\\sigma^2}{n}$。\n因此，无偏估计量的 MSE 为：\n$$ \\operatorname{MSE}(\\hat{\\theta}_{U}) = (0)^2 + \\frac{\\sigma^2}{n} = \\frac{\\sigma^2}{n} $$\n\n**2. 缩减估计量 $\\hat{\\theta}_{\\alpha}$ 的 MSE**\n缩减估计量由 $\\hat{\\theta}_{\\alpha} = \\alpha \\bar{y} + (1 - \\alpha)\\mu_{0}$ 给出，其中 $\\mu_{0}$ 是一个固定常数。\n首先，我们求其偏差。其期望为 $\\mathbb{E}[\\hat{\\theta}_{\\alpha}] = \\mathbb{E}[\\alpha \\bar{y} + (1 - \\alpha)\\mu_{0}] = \\alpha \\mathbb{E}[\\bar{y}] + (1 - \\alpha)\\mu_{0} = \\alpha\\theta + (1 - \\alpha)\\mu_{0}$。\n偏差为 $\\operatorname{Bias}(\\hat{\\theta}_{\\alpha}) = \\mathbb{E}[\\hat{\\theta}_{\\alpha}] - \\theta = (\\alpha\\theta + (1 - \\alpha)\\mu_{0}) - \\theta = \\theta(\\alpha - 1) + (1 - \\alpha)\\mu_{0} = (1 - \\alpha)(\\mu_{0} - \\theta)$。\n接下来，我们求其方差。由于 $\\mu_0$ 是一个常数，$\\operatorname{Var}(\\hat{\\theta}_{\\alpha}) = \\operatorname{Var}(\\alpha \\bar{y} + (1 - \\alpha)\\mu_{0}) = \\operatorname{Var}(\\alpha \\bar{y}) = \\alpha^2 \\operatorname{Var}(\\bar{y}) = \\alpha^2 \\frac{\\sigma^2}{n}$。\n缩减估计量的 MSE 是其偏差平方与方差之和：\n$$ \\operatorname{MSE}(\\hat{\\theta}_{\\alpha}) = ((1 - \\alpha)(\\mu_{0} - \\theta))^2 + \\left(\\alpha^2 \\frac{\\sigma^2}{n}\\right) = (1 - \\alpha)^2 (\\mu_{0} - \\theta)^2 + \\frac{\\alpha^2 \\sigma^2}{n} $$\n\n**3. $\\operatorname{MSE}(\\hat{\\theta}_{\\alpha})$ 的最小化**\n为了找到使 $\\operatorname{MSE}(\\hat{\\theta}_{\\alpha})$ 最小的 $\\alpha$ 值，我们对 $\\alpha$ 求导并令其为零。\n设 $M(\\alpha) = \\operatorname{MSE}(\\hat{\\theta}_{\\alpha})$。\n$$ \\frac{dM}{d\\alpha} = \\frac{d}{d\\alpha} \\left[ (1 - \\alpha)^2 (\\mu_{0} - \\theta)^2 + \\frac{\\alpha^2 \\sigma^2}{n} \\right] = -2(1 - \\alpha)(\\mu_{0} - \\theta)^2 + \\frac{2\\alpha \\sigma^2}{n} $$\n将导数设为零以找到最优的 $\\alpha$，我们记为 $\\alpha_{\\text{opt}}$：\n$$ -2(1 - \\alpha_{\\text{opt}})(\\mu_{0} - \\theta)^2 + \\frac{2\\alpha_{\\text{opt}} \\sigma^2}{n} = 0 $$\n$$ \\alpha_{\\text{opt}} \\frac{\\sigma^2}{n} = (1 - \\alpha_{\\text{opt}})(\\mu_{0} - \\theta)^2 $$\n$$ \\alpha_{\\text{opt}} \\frac{\\sigma^2}{n} = (\\mu_{0} - \\theta)^2 - \\alpha_{\\text{opt}}(\\mu_{0} - \\theta)^2 $$\n$$ \\alpha_{\\text{opt}} \\left( \\frac{\\sigma^2}{n} + (\\mu_{0} - \\theta)^2 \\right) = (\\mu_{0} - \\theta)^2 $$\n$$ \\alpha_{\\text{opt}} = \\frac{(\\mu_{0} - \\theta)^2}{(\\mu_{0} - \\theta)^2 + \\frac{\\sigma^2}{n}} $$\n二阶导数 $\\frac{d^2M}{d\\alpha^2} = 2(\\mu_0 - \\theta)^2 + \\frac{2\\sigma^2}{n}$ 恒为正（对于 $\\sigma^2 > 0$），证实了此 $\\alpha$ 值对应一个最小值。\n\n**4. 最小化 MSE 之比**\n所要求的比率为 $R = \\frac{\\operatorname{MSE}(\\hat{\\theta}_{\\alpha_{\\text{opt}}})}{\\operatorname{MSE}(\\hat{\\theta}_{U})}$。\n我们首先通过将 $\\alpha_{\\text{opt}}$ 代入 MSE 公式来找到最小化的 MSE，即 $\\operatorname{MSE}(\\hat{\\theta}_{\\alpha_{\\text{opt}}})$ 的表达式。一个更简单的方法是使用在最小化过程中的一个中间步骤推导出的关系：$\\alpha_{\\text{opt}} \\frac{\\sigma^2}{n} = (1 - \\alpha_{\\text{opt}})(\\mu_{0} - \\theta)^2$。\n$\\operatorname{MSE}(\\hat{\\theta}_{\\alpha_{\\text{opt}}}) = (1 - \\alpha_{\\text{opt}})^2 (\\mu_{0} - \\theta)^2 + \\alpha_{\\text{opt}}^2 \\frac{\\sigma^2}{n}$\n$= (1 - \\alpha_{\\text{opt}}) \\left[ (1 - \\alpha_{\\text{opt}})(\\mu_{0} - \\theta)^2 \\right] + \\alpha_{\\text{opt}} \\left[ \\alpha_{\\text{opt}}\\frac{\\sigma^2}{n}\\right]$\n利用该关系式，我们可以替换括号中的项：\n$= (1 - \\alpha_{\\text{opt}}) \\left[ \\alpha_{\\text{opt}}\\frac{\\sigma^2}{n}\\right] + \\alpha_{\\text{opt}} \\left[ (1 - \\alpha_{\\text{opt}})(\\mu_{0} - \\theta)^2 \\right]$\n$= \\alpha_{\\text{opt}}(1 - \\alpha_{\\text{opt}}) \\left[ \\frac{\\sigma^2}{n} + (\\mu_0 - \\theta)^2 \\right]$\n回想 $1 - \\alpha_{\\text{opt}} = \\frac{\\sigma^2/n}{(\\mu_0 - \\theta)^2 + \\sigma^2/n}$：\n$\\operatorname{MSE}(\\hat{\\theta}_{\\alpha_{\\text{opt}}}) = \\frac{(\\mu_{0} - \\theta)^2}{(\\mu_{0} - \\theta)^2 + \\frac{\\sigma^2}{n}} \\cdot \\frac{\\frac{\\sigma^2}{n}}{(\\mu_{0} - \\theta)^2 + \\frac{\\sigma^2}{n}} \\cdot \\left[ (\\mu_0 - \\theta)^2 + \\frac{\\sigma^2}{n} \\right]$\n$\\operatorname{MSE}(\\hat{\\theta}_{\\alpha_{\\text{opt}}}) = \\frac{\\frac{\\sigma^2}{n} (\\mu_{0} - \\theta)^2}{(\\mu_{0} - \\theta)^2 + \\frac{\\sigma^2}{n}}$\n\n现在我们可以计算比率 $R$：\n$$ R = \\frac{\\operatorname{MSE}(\\hat{\\theta}_{\\alpha_{\\text{opt}}})}{\\operatorname{MSE}(\\hat{\\theta}_{U})} = \\frac{\\frac{\\frac{\\sigma^2}{n} (\\mu_{0} - \\theta)^2}{(\\mu_{0} - \\theta)^2 + \\frac{\\sigma^2}{n}}}{\\frac{\\sigma^2}{n}} $$\n$$ R = \\frac{(\\mu_{0} - \\theta)^2}{(\\mu_{0} - \\theta)^2 + \\frac{\\sigma^2}{n}} $$\n我们观察到该比率恰好等于 $\\alpha_{\\text{opt}}$。\n\n**5. 数值计算**\n给定以下数值：\n$n = 12$\n$\\sigma = 0.9$ 升/小时，所以 $\\sigma^2 = 0.81$ (升/小时)$^2$\n$\\mu_{0} = 1.2$ 升/小时\n$\\theta = 0.8$ 升/小时\n\n首先，我们计算 $R$ 表达式中的各分量：\n分母中的偏差平方项是 $(\\mu_{0} - \\theta)^2 = (1.2 - 0.8)^2 = (0.4)^2 = 0.16$。\n样本均值的方差是 $\\frac{\\sigma^2}{n} = \\frac{0.81}{12} = 0.0675$。\n\n现在，我们计算比率 $R$：\n$$ R = \\frac{0.16}{0.16 + 0.0675} = \\frac{0.16}{0.2275} $$\n$$ R \\approx 0.703296703... $$\n四舍五入至四位有效数字，我们得到 $0.7033$。",
            "answer": "$$\\boxed{0.7033}$$"
        },
        {
            "introduction": "在生物医学建模中，我们经常遇到必须为正值的参数，例如反应速率常数或浓度。直接对这些参数进行优化可能会遇到数值不稳定的问题或违反其物理约束。这个练习  介绍了一种强大而优雅的解决方案：对数变换（log-transform）重参数化。你将学习到，这种变换不仅巧妙地处理了正值约束，还与贝叶斯建模中的尺度不变性（scale invariance）等深层原理紧密相连，是参数估计工具箱中必不可少的一项技术。",
            "id": "3916251",
            "problem": "考虑贝叶斯参数估计框架中，一个药代动力学消除模型里的严格为正的生化速率参数 $\\theta \\in (0,\\infty)$。为提高数值稳定性，并在无约束优化或采样过程中处理正值约束，一种常见的重参数化方法是定义对数变换参数 $\\phi = \\ln(\\theta)$，该变换将 $(0,\\infty)$ 映射到 $(-\\infty,\\infty)$。从概率密度的变量替换法则和一对一变换下的概率质量不变性出发，通过解释对数变换如何引出对 $\\theta$ 的尺度不变建模，来论证对正参数使用对数变换的合理性。然后，当 $\\theta$ 的先验为形状参数 $a>0$、速率参数 $b>0$ 的伽马分布，即 $\\pi_{\\theta}(\\theta) = \\frac{b^{a}}{\\Gamma(a)} \\,\\theta^{a-1} \\exp(-b\\theta)$ (对于 $\\theta>0$) 时，推导变换后 $\\phi$ 的先验密度。你的推导必须明确指出由变换 $\\phi=\\ln(\\theta)$ 产生的雅可比项。请将你的最终答案表示为变换后先验密度 $\\pi_{\\phi}(\\phi)$ 关于 $\\phi$、$a$ 和 $b$ 的单个、完全简化的解析表达式。无需进行数值计算。最终答案不应包含单位。",
            "solution": "该问题是有效的，因为它在科学上基于贝叶斯概率论及其在生物医学系统建模中的应用，问题陈述清晰，提供了所有必要信息，并使用客观、正式的语言表达。因此，我们可以着手求解。\n\n该问题要求两个主要部分：首先，基于尺度不变性原理，为正参数使用对数变换提供理由；其次，当原始参数 $\\theta$ 服从伽马分布时，推导对数变换后参数 $\\phi = \\ln(\\theta)$ 的概率密度函数。\n\n我们从论证开始。在贝叶斯建模中，先验分布的选择应尽可能不引入任意信息。对于一个表示尺度量的严格为正的参数 $\\theta$（例如速率常数，其中时间单位从秒变为分钟对应于将 $\\theta$ 乘以一个常数因子），我们希望推断不受单位选择的影响。此性质称为尺度不变性。\n\n让我们考虑一个尺度变换 $\\theta' = c\\theta$，其中 $c > 0$ 为某个常数。如果关于 $\\theta'$ 的先验 $\\pi'(\\theta')$ 与 $\\pi(\\theta)$ 具有相同的函数形式，则称无信息先验 $\\pi(\\theta)$ 是尺度不变的。根据变量替换法则，$\\pi'(\\theta') = \\pi(\\theta(\\theta')) \\left| \\frac{d\\theta}{d\\theta'} \\right|$。由于 $\\theta = \\theta'/c$，我们有 $\\left|\\frac{d\\theta}{d\\theta'}\\right| = 1/c$。因此，$\\pi'(\\theta') = \\pi(\\theta'/c) \\cdot \\frac{1}{c}$。\n一个朴素的无信息先验选择可能是均匀分布，$\\pi(\\theta) \\propto 1$。在尺度变换下，这变为 $\\pi'(\\theta') \\propto 1 \\cdot \\frac{1}{c}$，它不是均匀的，并且依赖于尺度因子 $c$。这是不理想的。\n\n对于正参数 $\\theta$，合适的尺度不变先验是 Jeffreys 先验，由 $\\pi_{\\theta}(\\theta) \\propto \\frac{1}{\\theta}$ 给出。我们来检验其不变性：$\\pi'(\\theta') \\propto \\pi(\\theta'/c) \\cdot \\frac{1}{c} = \\frac{1}{(\\theta'/c)} \\cdot \\frac{1}{c} = \\frac{c}{\\theta'} \\cdot \\frac{1}{c} = \\frac{1}{\\theta'}$。$\\theta'$ 的先验具有相同的形式 $\\pi'(\\theta') \\propto \\frac{1}{\\theta'}$，从而证实了其尺度不变性。\n\n现在我们将其与对数变换 $\\phi = \\ln(\\theta)$ 联系起来。我们想找到与 $\\theta$ 的尺度不变 Jeffreys 先验相对应的 $\\phi$ 的先验。概率密度函数的变换通用法则是基于概率质量守恒，即 $\\pi_{\\phi}(\\phi) |d\\phi| = \\pi_{\\theta}(\\theta) |d\\theta|$。这导出以下公式：\n$$ \\pi_{\\phi}(\\phi) = \\pi_{\\theta}(\\theta(\\phi)) \\left| \\frac{d\\theta}{d\\phi} \\right| $$\n逆变换为 $\\theta(\\phi) = \\exp(\\phi)$。雅可比项所需的导数为 $\\frac{d\\theta}{d\\phi} = \\exp(\\phi)$。雅可比为 $\\left| \\frac{d\\theta}{d\\phi} \\right| = |\\exp(\\phi)| = \\exp(\\phi)$，因为 $\\exp(\\phi) > 0$。\n将 Jeffreys 先验 $\\pi_{\\theta}(\\theta) \\propto 1/\\theta$ 和逆变换 $\\theta = \\exp(\\phi)$ 代入公式，得到：\n$$ \\pi_{\\phi}(\\phi) \\propto \\pi_{\\theta}(\\exp(\\phi)) \\left| \\exp(\\phi) \\right| = \\frac{1}{\\exp(\\phi)} \\cdot \\exp(\\phi) = 1 $$\n这表明，对数变换参数 $\\phi$ 上的均匀先验 $\\pi_{\\phi}(\\phi) \\propto 1$ 精确对应于原始参数 $\\theta$ 上的尺度不变 Jeffreys 先验。因此，用 $\\phi = \\ln(\\theta)$ 进行重参数化并对 $\\phi$ 使用均匀先验，是体现对 $\\theta$ 尺度无知的一种有原则的方法。这为对数变换提供了强有力的理论依据。\n\n接下来，我们推导当 $\\theta$ 服从伽马分布时 $\\phi$ 的特定变换后先验密度。\n给定的 $\\theta$ 的先验为：\n$$ \\pi_{\\theta}(\\theta) = \\frac{b^{a}}{\\Gamma(a)} \\theta^{a-1} \\exp(-b\\theta) \\quad \\text{对于 } \\theta > 0 $$\n变换为 $\\phi = \\ln(\\theta)$，这意味着逆变换为 $\\theta = \\exp(\\phi)$。$\\theta$ 的定义域是 $(0, \\infty)$，对应的 $\\phi$ 的定义域是 $(-\\infty, \\infty)$。\n\n推导需要变换的雅可比。如前所述，该项源于变量替换公式。雅可比 $J$ 是逆映射 $\\theta(\\phi)$ 导数的绝对值：\n$$ J = \\left| \\frac{d\\theta}{d\\phi} \\right| = \\left| \\frac{d}{d\\phi} \\left( \\exp(\\phi) \\right) \\right| = |\\exp(\\phi)| = \\exp(\\phi) $$\n这个雅可比项 $\\exp(\\phi)$ 必须包含在变换后密度的公式中。\n\n$\\phi$ 的密度公式为：\n$$ \\pi_{\\phi}(\\phi) = \\pi_{\\theta}(\\theta(\\phi)) \\cdot J $$\n我们将 $\\theta = \\exp(\\phi)$ 代入 $\\pi_{\\theta}(\\theta)$ 的表达式中：\n$$ \\pi_{\\theta}(\\theta(\\phi)) = \\frac{b^{a}}{\\Gamma(a)} \\left(\\exp(\\phi)\\right)^{a-1} \\exp\\left(-b\\exp(\\phi)\\right) $$\n简化第一项的指数，得到：\n$$ \\pi_{\\theta}(\\theta(\\phi)) = \\frac{b^{a}}{\\Gamma(a)} \\exp\\left((a-1)\\phi\\right) \\exp\\left(-b\\exp(\\phi)\\right) $$\n现在，我们乘以雅可比 $J = \\exp(\\phi)$：\n$$ \\pi_{\\phi}(\\phi) = \\left[ \\frac{b^{a}}{\\Gamma(a)} \\exp\\left((a-1)\\phi\\right) \\exp\\left(-b\\exp(\\phi)\\right) \\right] \\cdot \\exp(\\phi) $$\n为了简化，我们合并底数中包含 $\\phi$ 的指数项：\n$$ \\exp\\left((a-1)\\phi\\right) \\cdot \\exp(\\phi) = \\exp\\left((a-1)\\phi + \\phi\\right) = \\exp(a\\phi - \\phi + \\phi) = \\exp(a\\phi) $$\n将此代回 $\\pi_{\\phi}(\\phi)$ 的表达式，得到完全简化的变换后先验密度：\n$$ \\pi_{\\phi}(\\phi) = \\frac{b^{a}}{\\Gamma(a)} \\exp(a\\phi) \\exp(-b\\exp(\\phi)) $$\n通过合并指数函数的参数，可以更紧凑地写成：\n$$ \\pi_{\\phi}(\\phi) = \\frac{b^{a}}{\\Gamma(a)} \\exp(a\\phi - b\\exp(\\phi)) $$\n这是对数伽马分布的密度，对于 $\\phi \\in (-\\infty, \\infty)$ 成立。",
            "answer": "$$\\boxed{\\frac{b^{a}}{\\Gamma(a)} \\exp(a\\phi - b\\exp(\\phi))}$$"
        }
    ]
}