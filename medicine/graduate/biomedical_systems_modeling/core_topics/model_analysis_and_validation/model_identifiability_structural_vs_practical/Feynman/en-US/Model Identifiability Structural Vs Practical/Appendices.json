{
    "hands_on_practices": [
        {
            "introduction": "Understanding structural identifiability begins with recognizing model symmetries. This first exercise challenges you to analyze a minimal biomarker clearance model and demonstrate from first principles how a scaling symmetry between measurement gain and initial concentration makes them individually unidentifiable . By constructing this invariance explicitly, you will discover the identifiable parameter combination and see how fixing one parameter through calibration breaks the symmetry, a foundational concept in model building.",
            "id": "3902575",
            "problem": "Consider a minimal clearance model for a circulating biomarker in blood, where the biomarker concentration is denoted by $x(t)$ and evolves with no external input according to the first-order ordinary differential equation $dx/dt = -k\\,x$, with unknown clearance rate $k>0$. The measured output is a sensor reading $y(t)$ that is linearly proportional to the concentration via an unknown measurement gain $s>0$, so $y(t) = s\\,x(t)$. The initial concentration $x(0) = x_{0}>0$ is unknown. Assume ideal, continuous measurement of $y(t)$ for all $t \\geq 0$ and an error-free model-sensor system.\n\nStarting from the foundational definition of structural identifiability—namely, that a parameter (or parameter combination) is structurally identifiable if distinct parameter values do not produce identical outputs $y(t)$ for all $t$—you must analyze the identifiability of $k$, $s$, and $x_{0}$ for this model by constructing an exact symmetry that maps one parameter tuple to another while leaving the output trajectory $y(t)$ unchanged for all $t \\geq 0$. Use this symmetry to determine the maximal invariant combination of parameters that is structurally identifiable.\n\nNext, consider a calibration scenario in which the measurement gain $s$ is fixed to a known constant $s^{\\star}>0$. Show how this calibration breaks the exact symmetry and derive the uniquely identifiable initial concentration $x_{0}$ in terms of the observable $y(0)$ and the known $s^{\\star}$. Your reasoning must begin from the stated model equations and the structural identifiability definition, and must justify each step without appealing to any shortcut formulas that presume the result.\n\nProvide your final answer as the single, closed-form analytic expression for the structurally identifiable parameter combination under the exact symmetry of the uncalibrated model.",
            "solution": "The analysis begins with the mathematical definition of the model. The state variable, biomarker concentration $x(t)$, is governed by the first-order ordinary differential equation:\n$$ \\frac{dx}{dt} = -k\\,x(t) $$\nwith an unknown initial condition $x(0) = x_0 > 0$ and an unknown clearance rate $k > 0$. The measured output $y(t)$ is related to the state by a linear scaling with an unknown gain $s > 0$:\n$$ y(t) = s\\,x(t) $$\nThe parameter vector for this uncalibrated model is $\\theta = (k, s, x_0)$.\n\nFirst, we must find the analytical solution for the state variable $x(t)$. The differential equation is separable:\n$$ \\frac{dx}{x} = -k\\,dt $$\nIntegrating both sides from time $0$ to $t$ gives:\n$$ \\int_{x(0)}^{x(t)} \\frac{d\\xi}{\\xi} = \\int_0^t -k\\,d\\tau $$\n$$ \\ln(x(t)) - \\ln(x(0)) = -k\\,t $$\n$$ \\ln\\left(\\frac{x(t)}{x(0)}\\right) = -k\\,t $$\nGiven $x(0) = x_0$, we solve for $x(t)$:\n$$ x(t) = x_0 \\exp(-k\\,t) $$\n\nNext, we express the measurable output $y(t)$ as a function of time and the model parameters:\n$$ y(t) = s\\,x(t) = s \\left(x_0 \\exp(-k\\,t)\\right) = (s\\,x_0) \\exp(-k\\,t) $$\nThis expression represents the complete output trajectory of the system for a given parameter vector $\\theta = (k, s, x_0)$.\n\nTo assess structural identifiability, we use the definition provided: parameters are structurally identifiable if different parameter values lead to different outputs. Conversely, a model is structurally non-identifiable if there exist at least two distinct parameter vectors, $\\theta = (k, s, x_0)$ and $\\tilde{\\theta} = (\\tilde{k}, \\tilde{s}, \\tilde{x}_0)$, such that $\\theta \\neq \\tilde{\\theta}$ but the corresponding outputs $y(t; \\theta)$ and $y(t; \\tilde{\\theta})$ are identical for all $t \\geq 0$. We set the outputs equal to find the conditions for such an equivalence:\n$$ y(t; \\theta) = y(t; \\tilde{\\theta}) \\quad \\forall t \\ge 0 $$\n$$ (s\\,x_0) \\exp(-k\\,t) = (\\tilde{s}\\,\\tilde{x}_0) \\exp(-\\tilde{k}\\,t) $$\n\nFor this equality to hold for all $t \\ge 0$, the functions of time on both sides must be identical. An exponential decay function $A \\exp(-\\lambda t)$ is uniquely defined by its initial value $A$ and its decay rate $\\lambda$. Thus, we must equate the corresponding parts:\n1.  The decay rates must be equal: $k = \\tilde{k}$.\n2.  The pre-exponential factors (initial values of the output) must be equal: $s\\,x_0 = \\tilde{s}\\,\\tilde{x}_0$.\n\nFrom condition (1), the parameter $k$ must be the same in any two parameter vectors that produce the same output. This means that $k$ is uniquely determinable from the output data (e.g., from the slope of $\\ln(y(t))$ versus $t$) and is therefore structurally identifiable.\n\nCondition (2), however, reveals an ambiguity. The equality $s\\,x_0 = \\tilde{s}\\,\\tilde{x}_0$ does not require that $s=\\tilde{s}$ and $x_0=\\tilde{x}_0$. This allows for an exact symmetry in the model. Let $\\alpha$ be any positive constant, $\\alpha > 0$. We can define a transformation from one parameter set $(s, x_0)$ to another $(\\tilde{s}, \\tilde{x}_0)$ as follows:\n$$ \\tilde{s} = \\alpha s $$\n$$ \\tilde{x}_0 = \\frac{1}{\\alpha} x_0 $$\nIf we choose any $\\alpha \\neq 1$, the resulting parameter vector $\\tilde{\\theta} = (k, \\alpha s, x_0/\\alpha)$ is distinct from $\\theta = (k, s, x_0)$. However, this new vector produces an identical output because $\\tilde{k}=k$ and the product $\\tilde{s}\\,\\tilde{x}_0 = (\\alpha s)(\\frac{1}{\\alpha} x_0) = s\\,x_0$ remains invariant under the transformation. This proves that an infinite number of pairs $(s, x_0)$ can explain the same data, meaning $s$ and $x_0$ are not individually structurally identifiable.\n\nThe analysis shows that while $s$ and $x_0$ are not identifiable, their product $s\\,x_0$ is. The quantities that can be uniquely determined from ideal data are $k$ and the composite parameter $P = s\\,x_0$. The expression $s\\,x_0$ represents the maximal invariant combination of parameters under the described symmetry, as it is precisely this product that remains unchanged by the transformation that renders the individual parameters unidentifiable.\n\nNow, we analyze the calibrated scenario where the measurement gain $s$ is fixed to a known value $s^{\\star} > 0$. The unknown parameters are now only $(k, x_0)$. The output equation is:\n$$ y(t) = s^{\\star}\\,x_0 \\exp(-k\\,t) $$\nWe repeat the identifiability test. Let two parameter vectors $(k, x_0)$ and $(\\tilde{k}, \\tilde{x}_0)$ produce the same output:\n$$ s^{\\star}\\,x_0 \\exp(-k\\,t) = s^{\\star}\\,\\tilde{x}_0 \\exp(-\\tilde{k}\\,t) $$\nSince $s^{\\star}$ is a known non-zero constant, we can divide both sides by it:\n$$ x_0 \\exp(-k\\,t) = \\tilde{x}_0 \\exp(-\\tilde{k}\\,t) $$\nFor this equality to hold for all $t \\geq 0$, we must have $k=\\tilde{k}$ and $x_0=\\tilde{x}_0$. This means the parameter vector is uniquely determined. The prior knowledge of $s$ breaks the scaling symmetry, and both $k$ and $x_0$ become structurally identifiable. The uniquely identifiable value of $x_0$ can be derived from the observables. At $t=0$, the output is $y(0)=s^{\\star}x_0 \\exp(0) = s^{\\star}x_0$. Since $y(0)$ is measurable and $s^{\\star}$ is known, $x_0$ is given by:\n$$ x_0 = \\frac{y(0)}{s^{\\star}} $$\nThis confirms that the calibration renders $x_0$ identifiable.\n\nThe problem asks for the single structurally identifiable parameter combination from the uncalibrated model's symmetry. This is the product that remains invariant, which is $s\\,x_0$.",
            "answer": "$$\n\\boxed{s x_{0}}\n$$"
        },
        {
            "introduction": "Structural non-identifiability often arises from the inherent structure of a model, especially when only partial information, like steady-state data, is available. In this practice, you will explore a ligand-receptor binding model where individual association and dissociation rates cannot be determined from steady-state measurements alone . Your task is to derive the so-called \"gauge transformation\" that leaves the output invariant, revealing that only the ratio of the rates—the dissociation constant $K_d$—is structurally identifiable.",
            "id": "3902592",
            "problem": "Consider a well-mixed ligand-receptor system under mass-action kinetics with a single ligand species at concentration input $u(t)$ and a single receptor population with total concentration $R_{\\mathrm{tot}}$ known a priori. Let $B(t)$ denote the ligand-receptor complex concentration. The dynamics follow the law of mass action:\n$$\n\\frac{dB}{dt} \\;=\\; \\theta_{2}\\,u(t)\\,\\big(R_{\\mathrm{tot}} - B(t)\\big) \\;-\\; \\theta_{1}\\,B(t),\n$$\nwhere $\\theta_{2}$ is the association rate constant and $\\theta_{1}$ is the dissociation rate constant. The measured output is the steady-state complex concentration $y(u)$ attained after $u(t)$ is held constant at $u$ for sufficient time that $\\frac{dB}{dt}=0$. Assume experiments can be performed for a set of constant inputs $u$ spanning a nontrivial range, and the measurements are noise-free. No transient dynamics are recorded; only steady-state readouts $y(u)$ are available. \n\nStarting from the fundamental definitions of mass action and steady state, and without using any pre-packaged identifiability results, do the following:\n- Derive the input-output map $y(u)$ at steady state in terms of $\\theta_{1}$, $\\theta_{2}$, $R_{\\mathrm{tot}}$, and $u$.\n- By reasoning about invariances of the input-output map, determine a one-parameter family of parameter transformations (a gauge transformation) acting on $(\\theta_{1},\\theta_{2})$ that leaves $y(u)$ invariant for all constant $u$. Explain why this invariance is structural and not a consequence of limited data or experimental noise.\n- Identify the unique nontrivial combination of $\\theta_{1}$ and $\\theta_{2}$ that is structurally identifiable from steady-state data alone, and justify your conclusion from first principles.\n\nReport as your final answer the analytically simplified expression for this structurally identifiable parameter combination. Do not include any units. If you choose to define any intermediate symbols, eliminate them in your final expression so that your answer contains only $\\theta_{1}$ and $\\theta_{2}$.",
            "solution": "The starting point is the law of mass action for the binding and unbinding processes. The rate of formation of the complex $B(t)$ is proportional to the product of free ligand $u(t)$ and free receptor $R_{\\mathrm{tot}} - B(t)$ with proportionality constant $\\theta_{2}$. The rate of dissociation is proportional to $B(t)$ with proportionality constant $\\theta_{1}$. This yields the ordinary differential equation (ODE)\n$$\n\\frac{dB}{dt} \\;=\\; \\theta_{2}\\,u(t)\\,\\big(R_{\\mathrm{tot}} - B(t)\\big) \\;-\\; \\theta_{1}\\,B(t).\n$$\n\nAt steady state under a constant input $u(t)\\equiv u$, the time derivative vanishes by definition:\n$$\n0 \\;=\\; \\theta_{2}\\,u\\,\\big(R_{\\mathrm{tot}} - B_{\\mathrm{ss}}\\big) \\;-\\; \\theta_{1}\\,B_{\\mathrm{ss}},\n$$\nwhere $B_{\\mathrm{ss}}$ denotes the steady-state complex concentration. Solving algebraically for $B_{\\mathrm{ss}}$:\n\\begin{align*}\n\\theta_{2}\\,u\\,R_{\\mathrm{tot}} \\;-\\; \\theta_{2}\\,u\\,B_{\\mathrm{ss}} \\;-\\; \\theta_{1}\\,B_{\\mathrm{ss}} \\;&=\\; 0, \\\\\n\\theta_{2}\\,u\\,R_{\\mathrm{tot}} \\;&=\\; (\\theta_{2}\\,u + \\theta_{1})\\,B_{\\mathrm{ss}}, \\\\\nB_{\\mathrm{ss}} \\;&=\\; \\frac{\\theta_{2}\\,u}{\\theta_{1} + \\theta_{2}\\,u}\\,R_{\\mathrm{tot}}.\n\\end{align*}\nBy the problem definition, the measured steady-state output is $y(u) = B_{\\mathrm{ss}}$. Therefore, the input-output map is\n$$\ny(u) \\;=\\; \\frac{\\theta_{2}\\,u}{\\theta_{1} + \\theta_{2}\\,u}\\,R_{\\mathrm{tot}}.\n$$\n\nTo study invariances, rewrite this map to make parameter combinations explicit. Factor $\\theta_{2}$ from the denominator:\n\\begin{align*}\ny(u) \\;&=\\; \\frac{\\theta_{2}\\,u}{\\theta_{1} + \\theta_{2}\\,u}\\,R_{\\mathrm{tot}} \\;=\\; \\frac{u}{\\frac{\\theta_{1}}{\\theta_{2}} + u}\\,R_{\\mathrm{tot}}.\n\\end{align*}\nThis shows that $y(u)$ depends on $\\theta_{1}$ and $\\theta_{2}$ only through the ratio $\\theta_{1}/\\theta_{2}$. Consequently, any transformation of $(\\theta_{1},\\theta_{2})$ that preserves the ratio $\\theta_{1}/\\theta_{2}$ leaves $y(u)$ invariant for all $u$. A natural one-parameter family (Lie group) of such transformations is the multiplicative scaling\n$$\n(\\theta_{1},\\theta_{2}) \\;\\mapsto\\; (\\alpha\\,\\theta_{1},\\;\\alpha\\,\\theta_{2}), \\quad \\alpha \\in \\mathbb{R}_{>0}.\n$$\nUnder this transformation, the ratio is invariant:\n$$\n\\frac{\\alpha\\,\\theta_{1}}{\\alpha\\,\\theta_{2}} \\;=\\; \\frac{\\theta_{1}}{\\theta_{2}},\n$$\nand thus $y(u)$ is unchanged for every constant $u$. Because this invariance emerges directly from the algebraic structure of the steady-state solution and holds for all possible constant inputs $u$ and all positive scaling factors $\\alpha$, it is a structural property of the model-input-output triplet at steady state. It is not a practical identifiability issue arising from noise, finite sampling, or a restricted set of $u$ values; rather, no matter how many steady-state data points are collected across $u$, the absolute scales of $\\theta_{1}$ and $\\theta_{2}$ cannot be separated, only their ratio can.\n\nTo identify the structurally identifiable parameter combination, consider the observable $y(u)$ expressed in terms of a single composite parameter:\n$$\ny(u) \\;=\\; \\frac{u}{\\frac{\\theta_{1}}{\\theta_{2}} + u}\\,R_{\\mathrm{tot}}.\n$$\nDefine the dissociation constant (often denoted $K_{d}$ in biochemistry) as $K_{d} \\equiv \\theta_{1}/\\theta_{2}$. From the above expression, $y(u)$ depends on $(\\theta_{1},\\theta_{2})$ only through $K_{d}$. Therefore, $K_{d} = \\theta_{1}/\\theta_{2}$ is the unique nontrivial structurally identifiable combination from steady-state data alone, whereas the absolute values of $\\theta_{1}$ and $\\theta_{2}$ are structurally unidentifiable due to the scaling (gauge) symmetry.\n\nThus, the requested gauge transformation is multiplicative scaling of $(\\theta_{1},\\theta_{2})$ by a common positive factor, and the structurally identifiable combination is the ratio $\\theta_{1}/\\theta_{2}$, which is invariant under that transformation. The final requested expression is this invariant combination written in terms of $\\theta_{1}$ and $\\theta_{2}$ only.",
            "answer": "$$\\boxed{\\frac{\\theta_{1}}{\\theta_{2}}}$$"
        },
        {
            "introduction": "While structural identifiability is a prerequisite, it does not guarantee that parameters can be estimated from real, noisy data. This exercise bridges the gap to practical identifiability by using the Fisher Information Matrix (FIM) to analyze a classic pharmacokinetic model . You will derive the FIM to quantify how experimental design choices, such as sampling times and measurement noise, determine the precision of parameter estimates for a structurally identifiable model.",
            "id": "3902479",
            "problem": "Consider a one-compartment intravenous bolus model commonly used in pharmacokinetics, with unknown volume of distribution $V$ and first-order elimination rate constant $k$. A known dose $D$ is administered instantaneously at time $t=0$, and the plasma concentration for $t \\ge 0$ is \n$$\nC(t) \\;=\\; \\frac{D}{V}\\,\\exp(-k\\,t).\n$$\nYou collect $M$ discrete measurements $\\{y_m\\}_{m=1}^{M}$ at sampling times $\\{t_m\\}_{m=1}^{M}$, modeled as \n$$\ny_m \\;=\\; C(t_m) \\;+\\; \\varepsilon_m,\n$$\nwhere $\\varepsilon_m$ are independent, identically distributed Gaussian random variables with zero mean and variance $\\sigma^2$, i.e., $\\varepsilon_m \\sim \\mathcal{N}(0,\\sigma^2)$.\n\nUsing only fundamental definitions from statistical estimation theory and system modeling, proceed as follows:\n- First, assess structural identifiability of the parameter vector $\\theta = (V,k)$ from the input-output relation $u(t) \\mapsto C(t)$ with $u(t)$ being the bolus at $t=0$ and ideal, noise-free observations $C(t)$ at discrete times. Base your reasoning on invertibility of the input-output map without invoking numerical algorithms.\n- Next, starting from the definition of the Fisher Information Matrix (FIM) for independent Gaussian measurement noise,\n$$\n[\\mathcal{I}(\\theta)]_{ij} \\;=\\; \\mathbb{E}\\!\\left[-\\,\\frac{\\partial^2}{\\partial \\theta_i \\,\\partial \\theta_j}\\,\\ln L(y_1,\\ldots,y_M \\mid \\theta)\\right],\n$$\nderive the $2 \\times 2$ Fisher Information Matrix $\\mathcal{I}(\\theta)$ for $\\theta=(V,k)$ in terms of $D$, $V$, $k$, $\\sigma$, and the sampling times $\\{t_m\\}_{m=1}^{M}$.\n- Compute the determinant $\\det\\big(\\mathcal{I}(\\theta)\\big)$ in closed form and interpret how its magnitude reflects practical identifiability.\n- Using your expression, identify conditions on the sampling times $\\{t_m\\}_{m=1}^{M}$ and the noise level $\\sigma$ under which $\\mathcal{I}(\\theta)$ becomes ill-conditioned in the sense of near-singularity or large condition number. Frame these conditions in terms of the geometry of the sensitivity vectors and the dispersion of the sampling times under appropriate weights.\n\nYour final answer should be the closed-form analytic expression for $\\det\\big(\\mathcal{I}(\\theta)\\big)$ as a function of $D$, $V$, $k$, $\\sigma$, and $\\{t_m\\}_{m=1}^{M}$. No rounding is required. Do not include units in your final boxed answer.",
            "solution": "The problem asks for four distinct analyses: an assessment of structural identifiability, the derivation of the Fisher Information Matrix (FIM), the calculation of its determinant, and an interpretation of conditions leading to its ill-conditioning.\n\nFirst, we assess the structural identifiability of the parameter vector $\\theta = (V, k)$. Structural identifiability concerns the uniqueness of parameter values given ideal, noise-free observations of the model output over time. The model output is given by the function $C(t, \\theta) = \\frac{D}{V}\\,\\exp(-k\\,t)$. For the parameter vector $\\theta$ to be structurally identifiable, if two parameter vectors $\\theta_1 = (V_1, k_1)$ and $\\theta_2 = (V_2, k_2)$ produce the same output for all observation times $t \\ge 0$, then it must be that $\\theta_1 = \\theta_2$.\nLet's assume $C(t, \\theta_1) = C(t, \\theta_2)$ for all $t$ in the observation window.\n$$\n\\frac{D}{V_1}\\,\\exp(-k_1\\,t) = \\frac{D}{V_2}\\,\\exp(-k_2\\,t)\n$$\nAssuming the dose $D$ is non-zero, we can divide by $D$:\n$$\n\\frac{1}{V_1}\\,\\exp(-k_1\\,t) = \\frac{1}{V_2}\\,\\exp(-k_2\\,t)\n$$\nThis equality must hold for all $t \\ge 0$. Let's evaluate it at $t=0$:\n$$\n\\frac{1}{V_1}\\,\\exp(0) = \\frac{1}{V_2}\\,\\exp(0) \\implies \\frac{1}{V_1} = \\frac{1}{V_2}\n$$\nThis directly implies $V_1 = V_2$. Since $V_1 = V_2$, we can substitute this back into the equation, which simplifies to:\n$$\n\\exp(-k_1\\,t) = \\exp(-k_2\\,t)\n$$\nTaking the natural logarithm of both sides gives:\n$$\n-k_1\\,t = -k_2\\,t\n$$\nFor any time $t > 0$, we can divide by $-t$ to obtain $k_1 = k_2$. Therefore, the condition $C(t, \\theta_1) = C(t, \\theta_2)$ for all $t \\ge 0$ implies that $(V_1, k_1) = (V_2, k_2)$. The parameters $V$ and $k$ are thus structurally identifiable from noise-free measurements of $C(t)$.\n\nNext, we derive the $2 \\times 2$ Fisher Information Matrix (FIM), $\\mathcal{I}(\\theta)$. The problem states that the measurements $y_m$ are corrupted by independent, identically distributed (i.i.d.) Gaussian noise $\\varepsilon_m \\sim \\mathcal{N}(0,\\sigma^2)$. The log-likelihood function for $M$ such measurements is:\n$$\n\\ln L(y_1,\\ldots,y_M \\mid \\theta) = \\sum_{m=1}^{M} \\ln \\left( \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_m - C(t_m, \\theta))^2}{2\\sigma^2}\\right) \\right)\n$$\n$$\n\\ln L = -\\frac{M}{2}\\ln(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{m=1}^{M} (y_m - C(t_m, \\theta))^2\n$$\nThe FIM element $[\\mathcal{I}(\\theta)]_{ij}$ is defined as $\\mathbb{E}\\!\\left[-\\,\\frac{\\partial^2}{\\partial \\theta_i \\,\\partial \\theta_j}\\,\\ln L\\right]$. Let's compute the derivatives. The first partial derivative with respect to a parameter $\\theta_i$ is:\n$$\n\\frac{\\partial \\ln L}{\\partial \\theta_i} = -\\frac{1}{2\\sigma^2} \\sum_{m=1}^{M} 2(y_m - C(t_m, \\theta)) \\left(-\\frac{\\partial C(t_m, \\theta)}{\\partial \\theta_i}\\right) = \\frac{1}{\\sigma^2} \\sum_{m=1}^{M} (y_m - C(t_m, \\theta)) \\frac{\\partial C(t_m, \\theta)}{\\partial \\theta_i}\n$$\nThe second partial derivative with respect to $\\theta_j$ is:\n$$\n\\frac{\\partial^2 \\ln L}{\\partial \\theta_i \\partial \\theta_j} = \\frac{1}{\\sigma^2} \\sum_{m=1}^{M} \\left[ -\\frac{\\partial C(t_m, \\theta)}{\\partial \\theta_j} \\frac{\\partial C(t_m, \\theta)}{\\partial \\theta_i} + (y_m - C(t_m, \\theta)) \\frac{\\partial^2 C(t_m, \\theta)}{\\partial \\theta_i \\partial \\theta_j} \\right]\n$$\nTaking the expectation $\\mathbb{E}[\\cdot]$, we use the fact that $\\mathbb{E}[y_m] = C(t_m, \\theta)$, which means $\\mathbb{E}[y_m - C(t_m, \\theta)] = 0$. The second term in the sum vanishes.\n$$\n\\mathbb{E}\\left[\\frac{\\partial^2 \\ln L}{\\partial \\theta_i \\partial \\theta_j}\\right] = -\\frac{1}{\\sigma^2} \\sum_{m=1}^{M} \\frac{\\partial C(t_m, \\theta)}{\\partial \\theta_i} \\frac{\\partial C(t_m, \\theta)}{\\partial \\theta_j}\n$$\nTherefore, the FIM element simplifies to:\n$$\n[\\mathcal{I}(\\theta)]_{ij} = \\frac{1}{\\sigma^2} \\sum_{m=1}^{M} \\frac{\\partial C(t_m, \\theta)}{\\partial \\theta_i} \\frac{\\partial C(t_m, \\theta)}{\\partial \\theta_j}\n$$\nThe parameters are $\\theta_1 = V$ and $\\theta_2 = k$. We must compute the sensitivity functions (partial derivatives of $C(t)$ with respect to $V$ and $k$):\n$$\n\\frac{\\partial C(t)}{\\partial V} = \\frac{\\partial}{\\partial V} \\left(\\frac{D}{V} \\exp(-kt)\\right) = -\\frac{D}{V^2} \\exp(-kt)\n$$\n$$\n\\frac{\\partial C(t)}{\\partial k} = \\frac{\\partial}{\\partial k} \\left(\\frac{D}{V} \\exp(-kt)\\right) = \\frac{D}{V} \\exp(-kt)(-t) = -t \\frac{D}{V} \\exp(-kt)\n$$\nNow we construct the elements of the FIM, $\\mathcal{I}(\\theta) = \\begin{pmatrix} \\mathcal{I}_{VV} & \\mathcal{I}_{VK} \\\\ \\mathcal{I}_{KV} & \\mathcal{I}_{KK} \\end{pmatrix}$:\n$$\n\\mathcal{I}_{VV} = \\frac{1}{\\sigma^2} \\sum_{m=1}^{M} \\left( \\frac{\\partial C(t_m)}{\\partial V} \\right)^2 = \\frac{1}{\\sigma^2} \\sum_{m=1}^{M} \\left( -\\frac{D}{V^2} \\exp(-kt_m) \\right)^2 = \\frac{D^2}{\\sigma^2 V^4} \\sum_{m=1}^{M} \\exp(-2kt_m)\n$$\n$$\n\\mathcal{I}_{KK} = \\frac{1}{\\sigma^2} \\sum_{m=1}^{M} \\left( \\frac{\\partial C(t_m)}{\\partial k} \\right)^2 = \\frac{1}{\\sigma^2} \\sum_{m=1}^{M} \\left( -t_m \\frac{D}{V} \\exp(-kt_m) \\right)^2 = \\frac{D^2}{\\sigma^2 V^2} \\sum_{m=1}^{M} t_m^2 \\exp(-2kt_m)\n$$\n$$\n\\mathcal{I}_{VK} = \\mathcal{I}_{KV} = \\frac{1}{\\sigma^2} \\sum_{m=1}^{M} \\frac{\\partial C(t_m)}{\\partial V} \\frac{\\partial C(t_m)}{\\partial k} = \\frac{1}{\\sigma^2} \\sum_{m=1}^{M} \\left(-\\frac{D}{V^2} \\exp(-kt_m)\\right) \\left(-t_m \\frac{D}{V} \\exp(-kt_m)\\right) = \\frac{D^2}{\\sigma^2 V^3} \\sum_{m=1}^{M} t_m \\exp(-2kt_m)\n$$\nThird, we compute the determinant of $\\mathcal{I}(\\theta)$.\n$$\n\\det\\big(\\mathcal{I}(\\theta)\\big) = \\mathcal{I}_{VV} \\mathcal{I}_{KK} - \\mathcal{I}_{VK}^2\n$$\n$$\n\\det\\big(\\mathcal{I}(\\theta)\\big) = \\left(\\frac{D^2}{\\sigma^2 V^4} \\sum_{m=1}^{M} \\exp(-2kt_m)\\right) \\left(\\frac{D^2}{\\sigma^2 V^2} \\sum_{m=1}^{M} t_m^2 \\exp(-2kt_m)\\right) - \\left(\\frac{D^2}{\\sigma^2 V^3} \\sum_{m=1}^{M} t_m \\exp(-2kt_m)\\right)^2\n$$\n$$\n\\det\\big(\\mathcal{I}(\\theta)\\big) = \\frac{D^4}{\\sigma^4 V^6} \\left[ \\left(\\sum_{m=1}^{M} \\exp(-2kt_m)\\right) \\left(\\sum_{m=1}^{M} t_m^2 \\exp(-2kt_m)\\right) \\right] - \\frac{D^4}{\\sigma^4 V^6} \\left( \\sum_{m=1}^{M} t_m \\exp(-2kt_m) \\right)^2\n$$\nFactoring out the common term gives the final expression for the determinant:\n$$\n\\det\\big(\\mathcal{I}(\\theta)\\big) = \\frac{D^4}{\\sigma^4 V^6} \\left[ \\left(\\sum_{m=1}^{M} \\exp(-2kt_m)\\right) \\left(\\sum_{m=1}^{M} t_m^2 \\exp(-2kt_m)\\right) - \\left(\\sum_{m=1}^{M} t_m \\exp(-2kt_m)\\right)^2 \\right]\n$$\nThe magnitude of this determinant is a measure of practical identifiability. The Cramer-Rao Lower Bound states that the covariance matrix of any unbiased estimator $\\hat{\\theta}$ is bounded by the inverse of the FIM, i.e., $\\text{Cov}(\\hat{\\theta}) \\ge \\mathcal{I}(\\theta)^{-1}$. The volume of the confidence ellipsoid for the parameters is inversely proportional to $\\sqrt{\\det(\\mathcal{I}(\\theta))}$. A larger determinant implies a smaller inverse matrix, smaller variance bounds for the parameter estimates, and a smaller confidence region. Thus, a large $\\det(\\mathcal{I}(\\theta))$ corresponds to good practical identifiability.\n\nFinally, we analyze the conditions under which $\\mathcal{I}(\\theta)$ becomes ill-conditioned. Ill-conditioning implies the matrix is near-singular, which means its determinant is close to zero. The determinant expression is non-negative by the Cauchy-Schwarz inequality. It is zero if and only if the two sensitivity vectors, $\\vec{s}_V = (\\ldots, \\frac{\\partial C(t_m)}{\\partial V}, \\ldots)$ and $\\vec{s}_k = (\\ldots, \\frac{\\partial C(t_m)}{\\partial k}, \\ldots)$, are linearly dependent. This occurs if $\\vec{s}_k = c \\cdot \\vec{s}_V$ for a scalar constant $c$.\n$$\n-t_m \\frac{D}{V} \\exp(-kt_m) = c \\left( -\\frac{D}{V^2} \\exp(-kt_m) \\right) \\implies t_m \\frac{1}{V} = c \\frac{1}{V^2} \\implies t_m = \\frac{c}{V}\n$$\nThis must hold for all $m$. This is only possible if all sampling times are identical, $t_m = t_{const}$ for all $m=1, \\ldots, M$. In this case, the measurements provide no information to distinguish the rate of decay from the initial concentration, and the parameters are not practically identifiable. The FIM is exactly singular.\nMore generally, the FIM becomes ill-conditioned (near-singular, large condition number) if the sampling times $\\{t_m\\}$ are poorly chosen. The term in the brackets of the determinant, $\\left(\\sum w_m\\right) \\left(\\sum t_m^2 w_m\\right) - \\left(\\sum t_m w_m\\right)^2$ with weights $w_m = \\exp(-2kt_m)$, is proportional to the weighted variance of the sampling times. Ill-conditioning arises when:\n1.  The sampling times $\\{t_m\\}$ are clustered very tightly together. This makes the weighted variance of the times very small, so $\\det(\\mathcal{I}(\\theta)) \\approx 0$. The sensitivity vectors become nearly collinear.\n2.  The sampling times are chosen in a non-informative region. If all $t_m \\gg 1/k$, then $\\exp(-kt_m) \\approx 0$ for all $m$. All entries of the FIM become close to zero, leading to a near-zero determinant and poor identifiability.\nThe noise level $\\sigma$ also affects practical identifiability. The term $1/\\sigma^4$ in the determinant shows that as noise variance $\\sigma^2$ increases, $\\det(\\mathcal{I}(\\theta))$ decreases rapidly. High noise levels degrade the quality of information and thus worsen practical identifiability. However, $\\sigma$ is a scalar multiplier for the entire matrix, so it does not affect the matrix's condition number, which depends only on the ratio of eigenvalues. The primary cause of ill-conditioning (a high condition number) is a poor experimental design, i.e., the choice of sampling times $\\{t_m\\}$.",
            "answer": "$$\n\\boxed{\\frac{D^4}{\\sigma^4 V^6} \\left[ \\left(\\sum_{m=1}^{M} \\exp(-2kt_m)\\right) \\left(\\sum_{m=1}^{M} t_m^2 \\exp(-2kt_m)\\right) - \\left(\\sum_{m=1}^{M} t_m \\exp(-2kt_m)\\right)^2 \\right]}\n$$"
        }
    ]
}