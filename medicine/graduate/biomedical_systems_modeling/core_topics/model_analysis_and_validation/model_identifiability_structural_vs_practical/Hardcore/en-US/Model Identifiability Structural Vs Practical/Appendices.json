{
    "hands_on_practices": [
        {
            "introduction": "Structural non-identifiability is a fundamental property of a model's mathematical structure, where different parameter values can lead to identical predictions. This exercise provides a foundational look at this issue using a simple clearance model. By explicitly constructing a symmetry transformation, you will demonstrate how individual parameters can be inseparable while their combinations remain uniquely defined, a crucial first diagnostic for any modeling endeavor .",
            "id": "3902575",
            "problem": "Consider a minimal clearance model for a circulating biomarker in blood, where the biomarker concentration is denoted by $x(t)$ and evolves with no external input according to the first-order ordinary differential equation $dx/dt = -k\\,x$, with unknown clearance rate $k>0$. The measured output is a sensor reading $y(t)$ that is linearly proportional to the concentration via an unknown measurement gain $s>0$, so $y(t) = s\\,x(t)$. The initial concentration $x(0) = x_{0}>0$ is unknown. Assume ideal, continuous measurement of $y(t)$ for all $t \\geq 0$ and an error-free model-sensor system.\n\nStarting from the foundational definition of structural identifiability—namely, that a parameter (or parameter combination) is structurally identifiable if distinct parameter values do not produce identical outputs $y(t)$ for all $t$—you must analyze the identifiability of $k$, $s$, and $x_{0}$ for this model by constructing an exact symmetry that maps one parameter tuple to another while leaving the output trajectory $y(t)$ unchanged for all $t \\geq 0$. Use this symmetry to determine the maximal invariant combination of parameters that is structurally identifiable.\n\nNext, consider a calibration scenario in which the measurement gain $s$ is fixed to a known constant $s^{\\star}>0$. Show how this calibration breaks the exact symmetry and derive the uniquely identifiable initial concentration $x_{0}$ in terms of the observable $y(0)$ and the known $s^{\\star}$. Your reasoning must begin from the stated model equations and the structural identifiability definition, and must justify each step without appealing to any shortcut formulas that presume the result.\n\nProvide your final answer as the single, closed-form analytic expression for the structurally identifiable parameter combination under the exact symmetry of the uncalibrated model.",
            "solution": "The analysis begins with the mathematical definition of the model. The state variable, biomarker concentration $x(t)$, is governed by the first-order ordinary differential equation:\n$$ \\frac{dx}{dt} = -k\\,x(t) $$\nwith an unknown initial condition $x(0) = x_0 > 0$ and an unknown clearance rate $k > 0$. The measured output $y(t)$ is related to the state by a linear scaling with an unknown gain $s > 0$:\n$$ y(t) = s\\,x(t) $$\nThe parameter vector for this uncalibrated model is $\\theta = (k, s, x_0)$.\n\nFirst, we must find the analytical solution for the state variable $x(t)$. The differential equation is separable:\n$$ \\frac{dx}{x} = -k\\,dt $$\nIntegrating both sides from time $0$ to $t$ gives:\n$$ \\int_{x(0)}^{x(t)} \\frac{d\\xi}{\\xi} = \\int_0^t -k\\,d\\tau $$\n$$ \\ln(x(t)) - \\ln(x(0)) = -k\\,t $$\n$$ \\ln\\left(\\frac{x(t)}{x(0)}\\right) = -k\\,t $$\nGiven $x(0) = x_0$, we solve for $x(t)$:\n$$ x(t) = x_0 \\exp(-k\\,t) $$\n\nNext, we express the measurable output $y(t)$ as a function of time and the model parameters:\n$$ y(t) = s\\,x(t) = s \\left(x_0 \\exp(-k\\,t)\\right) = (s\\,x_0) \\exp(-k\\,t) $$\nThis expression represents the complete output trajectory of the system for a given parameter vector $\\theta = (k, s, x_0)$.\n\nTo assess structural identifiability, we use the definition provided: parameters are structurally identifiable if different parameter values lead to different outputs. Conversely, a model is structurally non-identifiable if there exist at least two distinct parameter vectors, $\\theta = (k, s, x_0)$ and $\\tilde{\\theta} = (\\tilde{k}, \\tilde{s}, \\tilde{x}_0)$, such that $\\theta \\neq \\tilde{\\theta}$ but the corresponding outputs $y(t; \\theta)$ and $y(t; \\tilde{\\theta})$ are identical for all $t \\geq 0$. We set the outputs equal to find the conditions for such an equivalence:\n$$ y(t; \\theta) = y(t; \\tilde{\\theta}) \\quad \\forall t \\ge 0 $$\n$$ (s\\,x_0) \\exp(-k\\,t) = (\\tilde{s}\\,\\tilde{x}_0) \\exp(-\\tilde{k}\\,t) $$\n\nFor this equality to hold for all $t \\ge 0$, the functions of time on both sides must be identical. An exponential decay function $A \\exp(-\\lambda t)$ is uniquely defined by its initial value $A$ and its decay rate $\\lambda$. Thus, we must equate the corresponding parts:\n1.  The decay rates must be equal: $k = \\tilde{k}$.\n2.  The pre-exponential factors (initial values of the output) must be equal: $s\\,x_0 = \\tilde{s}\\,\\tilde{x}_0$.\n\nFrom condition (1), the parameter $k$ must be the same in any two parameter vectors that produce the same output. This means that $k$ is uniquely determinable from the output data (e.g., from the slope of $\\ln(y(t))$ versus $t$) and is therefore structurally identifiable.\n\nCondition (2), however, reveals an ambiguity. The equality $s\\,x_0 = \\tilde{s}\\,\\tilde{x}_0$ does not require that $s=\\tilde{s}$ and $x_0=\\tilde{x}_0$. This allows for an exact symmetry in the model. Let $\\alpha$ be any positive constant, $\\alpha > 0$. We can define a transformation from one parameter set $(s, x_0)$ to another $(\\tilde{s}, \\tilde{x}_0)$ as follows:\n$$ \\tilde{s} = \\alpha s $$\n$$ \\tilde{x}_0 = \\frac{1}{\\alpha} x_0 $$\nIf we choose any $\\alpha \\neq 1$, the resulting parameter vector $\\tilde{\\theta} = (k, \\alpha s, x_0/\\alpha)$ is distinct from $\\theta = (k, s, x_0)$. However, this new vector produces an identical output because $\\tilde{k}=k$ and the product $\\tilde{s}\\,\\tilde{x}_0 = (\\alpha s)(\\frac{1}{\\alpha} x_0) = s\\,x_0$ remains invariant under the transformation. This proves that an infinite number of pairs $(s, x_0)$ can explain the same data, meaning $s$ and $x_0$ are not individually structurally identifiable.\n\nThe analysis shows that while $s$ and $x_0$ are not identifiable, their product $s\\,x_0$ is. The quantities that can be uniquely determined from ideal data are $k$ and the composite parameter $P = s\\,x_0$. The expression $s\\,x_0$ represents the maximal invariant combination of parameters under the described symmetry, as it is precisely this product that remains unchanged by the transformation that renders the individual parameters unidentifiable.\n\nNow, we analyze the calibrated scenario where the measurement gain $s$ is fixed to a known value $s^{\\star} > 0$. The unknown parameters are now only $(k, x_0)$. The output equation is:\n$$ y(t) = s^{\\star}\\,x_0 \\exp(-k\\,t) $$\nWe repeat the identifiability test. Let two parameter vectors $(k, x_0)$ and $(\\tilde{k}, \\tilde{x}_0)$ produce the same output:\n$$ s^{\\star}\\,x_0 \\exp(-k\\,t) = s^{\\star}\\,\\tilde{x}_0 \\exp(-\\tilde{k}\\,t) $$\nSince $s^{\\star}$ is a known non-zero constant, we can divide both sides by it:\n$$ x_0 \\exp(-k\\,t) = \\tilde{x}_0 \\exp(-\\tilde{k}\\,t) $$\nFor this equality to hold for all $t \\geq 0$, we must have $k=\\tilde{k}$ and $x_0=\\tilde{x}_0$. This means the parameter vector is uniquely determined. The prior knowledge of $s$ breaks the scaling symmetry, and both $k$ and $x_0$ become structurally identifiable. The uniquely identifiable value of $x_0$ can be derived from the observables. At $t=0$, the output is $y(0)=s^{\\star}x_0 \\exp(0) = s^{\\star}x_0$. Since $y(0)$ is measurable and $s^{\\star}$ is known, $x_0$ is given by:\n$$ x_0 = \\frac{y(0)}{s^{\\star}} $$\nThis confirms that the calibration renders $x_0$ identifiable.\n\nThe problem asks for the single structurally identifiable parameter combination from the uncalibrated model's symmetry. This is the product that remains invariant, which is $s\\,x_0$.",
            "answer": "$$\n\\boxed{s x_{0}}\n$$"
        },
        {
            "introduction": "While a model may be structurally identifiable, practical challenges like measurement noise and finite sampling can still make parameter estimation unreliable. The Fisher Information Matrix ($FIM$) is a central tool for quantifying this practical identifiability, linking experimental design to the expected precision of parameter estimates. This practice will guide you through deriving the $FIM$ for a standard pharmacokinetic model, allowing you to see precisely how choices like sampling times and noise levels impact the certainty of your conclusions .",
            "id": "3902479",
            "problem": "Consider a one-compartment intravenous bolus model commonly used in pharmacokinetics, with unknown volume of distribution $V$ and first-order elimination rate constant $k$. A known dose $D$ is administered instantaneously at time $t=0$, and the plasma concentration for $t \\ge 0$ is \n$$\nC(t) \\;=\\; \\frac{D}{V}\\,\\exp(-k\\,t).\n$$\nYou collect $M$ discrete measurements $\\{y_m\\}_{m=1}^{M}$ at sampling times $\\{t_m\\}_{m=1}^{M}$, modeled as \n$$\ny_m \\;=\\; C(t_m) \\;+\\; \\varepsilon_m,\n$$\nwhere $\\varepsilon_m$ are independent, identically distributed Gaussian random variables with zero mean and variance $\\sigma^2$, i.e., $\\varepsilon_m \\sim \\mathcal{N}(0,\\sigma^2)$.\n\nUsing only fundamental definitions from statistical estimation theory and system modeling, proceed as follows:\n- First, assess structural identifiability of the parameter vector $\\theta = (V,k)$ from the input-output relation $u(t) \\mapsto C(t)$ with $u(t)$ being the bolus at $t=0$ and ideal, noise-free observations $C(t)$ at discrete times. Base your reasoning on invertibility of the input-output map without invoking numerical algorithms.\n- Next, starting from the definition of the Fisher Information Matrix (FIM) for independent Gaussian measurement noise,\n$$\n[\\mathcal{I}(\\theta)]_{ij} \\;=\\; \\mathbb{E}\\!\\left[-\\,\\frac{\\partial^2}{\\partial \\theta_i \\,\\partial \\theta_j}\\,\\ln L(y_1,\\ldots,y_M \\mid \\theta)\\right],\n$$\nderive the $2 \\times 2$ Fisher Information Matrix $\\mathcal{I}(\\theta)$ for $\\theta=(V,k)$ in terms of $D$, $V$, $k$, $\\sigma$, and the sampling times $\\{t_m\\}_{m=1}^{M}$.\n- Compute the determinant $\\det\\big(\\mathcal{I}(\\theta)\\big)$ in closed form and interpret how its magnitude reflects practical identifiability.\n- Using your expression, identify conditions on the sampling times $\\{t_m\\}_{m=1}^{M}$ and the noise level $\\sigma$ under which $\\mathcal{I}(\\theta)$ becomes ill-conditioned in the sense of near-singularity or large condition number. Frame these conditions in terms of the geometry of the sensitivity vectors and the dispersion of the sampling times under appropriate weights.\n\nYour final answer should be the closed-form analytic expression for $\\det\\big(\\mathcal{I}(\\theta)\\big)$ as a function of $D$, $V$, $k$, $\\sigma$, and $\\{t_m\\}_{m=1}^{M}$. No rounding is required. Do not include units in your final boxed answer.",
            "solution": "The problem statement is evaluated to be valid. It is scientifically grounded in the principles of pharmacokinetic modeling and statistical estimation theory. It is well-posed, providing all necessary definitions and data to perform the requested derivations and analysis. The language is objective and precise, and the setup is self-contained and logically consistent. Therefore, I will proceed with a full solution.\n\nThe problem asks for four distinct analyses: an assessment of structural identifiability, the derivation of the Fisher Information Matrix (FIM), the calculation of its determinant, and an interpretation of conditions leading to its ill-conditioning.\n\nFirst, we assess the structural identifiability of the parameter vector $\\theta = (V, k)$. Structural identifiability concerns the uniqueness of parameter values given ideal, noise-free observations of the model output over time. The model output is given by the function $C(t, \\theta) = \\frac{D}{V}\\,\\exp(-k\\,t)$. For the parameter vector $\\theta$ to be structurally identifiable, if two parameter vectors $\\theta_1 = (V_1, k_1)$ and $\\theta_2 = (V_2, k_2)$ produce the same output for all observation times $t \\ge 0$, then it must be that $\\theta_1 = \\theta_2$.\nLet's assume $C(t, \\theta_1) = C(t, \\theta_2)$ for all $t$ in the observation window.\n$$\n\\frac{D}{V_1}\\,\\exp(-k_1\\,t) = \\frac{D}{V_2}\\,\\exp(-k_2\\,t)\n$$\nAssuming the dose $D$ is non-zero, we can divide by $D$:\n$$\n\\frac{1}{V_1}\\,\\exp(-k_1\\,t) = \\frac{1}{V_2}\\,\\exp(-k_2\\,t)\n$$\nThis equality must hold for all $t \\ge 0$. Let's evaluate it at $t=0$:\n$$\n\\frac{1}{V_1}\\,\\exp(0) = \\frac{1}{V_2}\\,\\exp(0) \\implies \\frac{1}{V_1} = \\frac{1}{V_2}\n$$\nThis directly implies $V_1 = V_2$. Since $V_1 = V_2$, we can substitute this back into the equation, which simplifies to:\n$$\n\\exp(-k_1\\,t) = \\exp(-k_2\\,t)\n$$\nTaking the natural logarithm of both sides gives:\n$$\n-k_1\\,t = -k_2\\,t\n$$\nFor any time $t > 0$, we can divide by $-t$ to obtain $k_1 = k_2$. Therefore, the condition $C(t, \\theta_1) = C(t, \\theta_2)$ for all $t \\ge 0$ implies that $(V_1, k_1) = (V_2, k_2)$. The parameters $V$ and $k$ are thus structurally identifiable from noise-free measurements of $C(t)$.\n\nNext, we derive the $2 \\times 2$ Fisher Information Matrix (FIM), $\\mathcal{I}(\\theta)$. The problem states that the measurements $y_m$ are corrupted by independent, identically distributed (i.i.d.) Gaussian noise $\\varepsilon_m \\sim \\mathcal{N}(0,\\sigma^2)$. The log-likelihood function for $M$ such measurements is:\n$$\n\\ln L(y_1,\\ldots,y_M \\mid \\theta) = \\sum_{m=1}^{M} \\ln \\left( \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_m - C(t_m, \\theta))^2}{2\\sigma^2}\\right) \\right)\n$$\n$$\n\\ln L = -\\frac{M}{2}\\ln(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{m=1}^{M} (y_m - C(t_m, \\theta))^2\n$$\nThe FIM element $[\\mathcal{I}(\\theta)]_{ij}$ is defined as $\\mathbb{E}\\!\\left[-\\,\\frac{\\partial^2}{\\partial \\theta_i \\,\\partial \\theta_j}\\,\\ln L\\right]$. Let's compute the derivatives. The first partial derivative with respect to a parameter $\\theta_i$ is:\n$$\n\\frac{\\partial \\ln L}{\\partial \\theta_i} = -\\frac{1}{2\\sigma^2} \\sum_{m=1}^{M} 2(y_m - C(t_m, \\theta)) \\left(-\\frac{\\partial C(t_m, \\theta)}{\\partial \\theta_i}\\right) = \\frac{1}{\\sigma^2} \\sum_{m=1}^{M} (y_m - C(t_m, \\theta)) \\frac{\\partial C(t_m, \\theta)}{\\partial \\theta_i}\n$$\nThe second partial derivative with respect to $\\theta_j$ is:\n$$\n\\frac{\\partial^2 \\ln L}{\\partial \\theta_i \\partial \\theta_j} = \\frac{1}{\\sigma^2} \\sum_{m=1}^{M} \\left[ -\\frac{\\partial C(t_m, \\theta)}{\\partial \\theta_j} \\frac{\\partial C(t_m, \\theta)}{\\partial \\theta_i} + (y_m - C(t_m, \\theta)) \\frac{\\partial^2 C(t_m, \\theta)}{\\partial \\theta_i \\partial \\theta_j} \\right]\n$$\nTaking the expectation $\\mathbb{E}[\\cdot]$, we use the fact that $\\mathbb{E}[y_m] = C(t_m, \\theta)$, which means $\\mathbb{E}[y_m - C(t_m, \\theta)] = 0$. The second term in the sum vanishes.\n$$\n\\mathbb{E}\\left[\\frac{\\partial^2 \\ln L}{\\partial \\theta_i \\partial \\theta_j}\\right] = -\\frac{1}{\\sigma^2} \\sum_{m=1}^{M} \\frac{\\partial C(t_m, \\theta)}{\\partial \\theta_i} \\frac{\\partial C(t_m, \\theta)}{\\partial \\theta_j}\n$$\nTherefore, the FIM element simplifies to:\n$$\n[\\mathcal{I}(\\theta)]_{ij} = \\frac{1}{\\sigma^2} \\sum_{m=1}^{M} \\frac{\\partial C(t_m, \\theta)}{\\partial \\theta_i} \\frac{\\partial C(t_m, \\theta)}{\\partial \\theta_j}\n$$\nThe parameters are $\\theta_1 = V$ and $\\theta_2 = k$. We must compute the sensitivity functions (partial derivatives of $C(t)$ with respect to $V$ and $k$):\n$$\n\\frac{\\partial C(t)}{\\partial V} = \\frac{\\partial}{\\partial V} \\left(\\frac{D}{V} \\exp(-kt)\\right) = -\\frac{D}{V^2} \\exp(-kt)\n$$\n$$\n\\frac{\\partial C(t)}{\\partial k} = \\frac{\\partial}{\\partial k} \\left(\\frac{D}{V} \\exp(-kt)\\right) = \\frac{D}{V} \\exp(-kt)(-t) = -t \\frac{D}{V} \\exp(-kt)\n$$\nNow we construct the elements of the FIM, $\\mathcal{I}(\\theta) = \\begin{pmatrix} \\mathcal{I}_{VV} & \\mathcal{I}_{VK} \\\\ \\mathcal{I}_{KV} & \\mathcal{I}_{KK} \\end{pmatrix}$:\n$$\n\\mathcal{I}_{VV} = \\frac{1}{\\sigma^2} \\sum_{m=1}^{M} \\left( \\frac{\\partial C(t_m)}{\\partial V} \\right)^2 = \\frac{1}{\\sigma^2} \\sum_{m=1}^{M} \\left( -\\frac{D}{V^2} \\exp(-kt_m) \\right)^2 = \\frac{D^2}{\\sigma^2 V^4} \\sum_{m=1}^{M} \\exp(-2kt_m)\n$$\n$$\n\\mathcal{I}_{KK} = \\frac{1}{\\sigma^2} \\sum_{m=1}^{M} \\left( \\frac{\\partial C(t_m)}{\\partial k} \\right)^2 = \\frac{1}{\\sigma^2} \\sum_{m=1}^{M} \\left( -t_m \\frac{D}{V} \\exp(-kt_m) \\right)^2 = \\frac{D^2}{\\sigma^2 V^2} \\sum_{m=1}^{M} t_m^2 \\exp(-2kt_m)\n$$\n$$\n\\mathcal{I}_{VK} = \\mathcal{I}_{KV} = \\frac{1}{\\sigma^2} \\sum_{m=1}^{M} \\frac{\\partial C(t_m)}{\\partial V} \\frac{\\partial C(t_m)}{\\partial k} = \\frac{1}{\\sigma^2} \\sum_{m=1}^{M} \\left(-\\frac{D}{V^2} \\exp(-kt_m)\\right) \\left(-t_m \\frac{D}{V} \\exp(-kt_m)\\right) = \\frac{D^2}{\\sigma^2 V^3} \\sum_{m=1}^{M} t_m \\exp(-2kt_m)\n$$\nThird, we compute the determinant of $\\mathcal{I}(\\theta)$.\n$$\n\\det\\big(\\mathcal{I}(\\theta)\\big) = \\mathcal{I}_{VV} \\mathcal{I}_{KK} - \\mathcal{I}_{VK}^2\n$$\n$$\n\\det\\big(\\mathcal{I}(\\theta)\\big) = \\left(\\frac{D^2}{\\sigma^2 V^4} \\sum_{m=1}^{M} \\exp(-2kt_m)\\right) \\left(\\frac{D^2}{\\sigma^2 V^2} \\sum_{m=1}^{M} t_m^2 \\exp(-2kt_m)\\right) - \\left(\\frac{D^2}{\\sigma^2 V^3} \\sum_{m=1}^{M} t_m \\exp(-2kt_m)\\right)^2\n$$\n$$\n\\det\\big(\\mathcal{I}(\\theta)\\big) = \\frac{D^4}{\\sigma^4 V^6} \\left[ \\left(\\sum_{m=1}^{M} \\exp(-2kt_m)\\right) \\left(\\sum_{m=1}^{M} t_m^2 \\exp(-2kt_m)\\right) \\right] - \\frac{D^4}{\\sigma^4 V^6} \\left( \\sum_{m=1}^{M} t_m \\exp(-2kt_m) \\right)^2\n$$\nFactoring out the common term gives the final expression for the determinant:\n$$\n\\det\\big(\\mathcal{I}(\\theta)\\big) = \\frac{D^4}{\\sigma^4 V^6} \\left[ \\left(\\sum_{m=1}^{M} \\exp(-2kt_m)\\right) \\left(\\sum_{m=1}^{M} t_m^2 \\exp(-2kt_m)\\right) - \\left(\\sum_{m=1}^{M} t_m \\exp(-2kt_m)\\right)^2 \\right]\n$$\nThe magnitude of this determinant is a measure of practical identifiability. The Cramer-Rao Lower Bound states that the covariance matrix of any unbiased estimator $\\hat{\\theta}$ is bounded by the inverse of the FIM, i.e., $\\text{Cov}(\\hat{\\theta}) \\ge \\mathcal{I}(\\theta)^{-1}$. The volume of the confidence ellipsoid for the parameters is inversely proportional to $\\sqrt{\\det(\\mathcal{I}(\\theta))}$. A larger determinant implies a smaller inverse matrix, smaller variance bounds for the parameter estimates, and a smaller confidence region. Thus, a large $\\det(\\mathcal{I}(\\theta))$ corresponds to good practical identifiability.\n\nFinally, we analyze the conditions under which $\\mathcal{I}(\\theta)$ becomes ill-conditioned. Ill-conditioning implies the matrix is near-singular, which means its determinant is close to zero. The determinant expression is non-negative by the Cauchy-Schwarz inequality. It is zero if and only if the two sensitivity vectors, $\\vec{s}_V = (\\ldots, \\frac{\\partial C(t_m)}{\\partial V}, \\ldots)$ and $\\vec{s}_k = (\\ldots, \\frac{\\partial C(t_m)}{\\partial k}, \\ldots)$, are linearly dependent. This occurs if $\\vec{s}_k = c \\cdot \\vec{s}_V$ for a scalar constant $c$.\n$$\n-t_m \\frac{D}{V} \\exp(-kt_m) = c \\left( -\\frac{D}{V^2} \\exp(-kt_m) \\right) \\implies t_m \\frac{1}{V} = c \\frac{1}{V^2} \\implies t_m = \\frac{c}{V}\n$$\nThis must hold for all $m$. This is only possible if all sampling times are identical, $t_m = t_{const}$ for all $m=1, \\ldots, M$. In this case, the measurements provide no information to distinguish the rate of decay from the initial concentration, and the parameters are not practically identifiable. The FIM is exactly singular.\nMore generally, the FIM becomes ill-conditioned (near-singular, large condition number) if the sampling times $\\{t_m\\}$ are poorly chosen. The term in the brackets of the determinant, $\\left(\\sum w_m\\right) \\left(\\sum t_m^2 w_m\\right) - \\left(\\sum t_m w_m\\right)^2$ with weights $w_m = \\exp(-2kt_m)$, is proportional to the weighted variance of the sampling times. Ill-conditioning arises when:\n1.  The sampling times $\\{t_m\\}$ are clustered very tightly together. This makes the weighted variance of the times very small, so $\\det(\\mathcal{I}(\\theta)) \\approx 0$. The sensitivity vectors become nearly collinear.\n2.  The sampling times are chosen in a non-informative region. If all $t_m \\gg 1/k$, then $\\exp(-kt_m) \\approx 0$ for all $m$. All entries of the FIM become close to zero, leading to a near-zero determinant and poor identifiability.\nThe noise level $\\sigma$ also affects practical identifiability. The term $1/\\sigma^4$ in the determinant shows that as noise variance $\\sigma^2$ increases, $\\det(\\mathcal{I}(\\theta))$ decreases rapidly. High noise levels degrade the quality of information and thus worsen practical identifiability. However, $\\sigma$ is a scalar multiplier for the entire matrix, so it does not affect the matrix's condition number, which depends only on the ratio of eigenvalues. The primary cause of ill-conditioning (a high condition number) is a poor experimental design, i.e., the choice of sampling times $\\{t_m\\}$.",
            "answer": "$$\n\\boxed{\\frac{D^4}{\\sigma^4 V^6} \\left[ \\left(\\sum_{m=1}^{M} \\exp(-2kt_m)\\right) \\left(\\sum_{m=1}^{M} t_m^2 \\exp(-2kt_m)\\right) - \\left(\\sum_{m=1}^{M} t_m \\exp(-2kt_m)\\right)^2 \\right]}\n$$"
        },
        {
            "introduction": "Practical identifiability issues often arise from a mismatch between the timescale of the system's dynamics and the duration of the experiment. This is particularly true for parameters that define slow, long-term behavior, which can be nearly impossible to estimate from short-term data. In this exercise, you will quantify the systematic error, or bias, introduced when estimating a terminal elimination rate from a finite data record, providing a concrete understanding of how data truncation impacts parameter accuracy .",
            "id": "3902443",
            "problem": "Consider a standard intravenous bolus two-compartment pharmacokinetic model for plasma concentration, whose measured output $y(t)$ at time $t \\ge 0$ is given (noise-free) by\n$$\ny(t) \\;=\\; A \\exp(-\\alpha t) \\;+\\; B \\exp(-\\beta t),\n$$\nwith $A>0$, $B>0$, and rate constants satisfying $0<\\beta<\\alpha$. The terminal elimination rate $\\beta$ is commonly interpreted as the asymptotic slope of the semilogarithmic concentration-time profile. Assume continuous sampling on the finite horizon $[0,T]$ for some finite $T>0$. Adopt the following definitions: structural identifiability means that distinct parameter values produce distinct outputs for all $t \\ge 0$ under ideal, noise-free measurement, whereas practical identifiability refers to the feasibility of parameter estimation given finite, noisy, and truncated data.\n\nUsing only foundational calculus and the stated model, do the following:\n\n- Justify, by first principles, that the parameter $\\beta$ manifests in the terminal (large-time) behavior through the identity\n$$\n\\beta \\;=\\; - \\lim_{t \\to \\infty} \\frac{d}{dt}\\,\\ln y(t).\n$$\nExplain why, for any finite $T$, replacing the limit by evaluation at $t=T$ induces a nonzero discrepancy from $\\beta$ whenever $A>0$.\n\n- Define the finite-time slope-based estimator\n$$\n\\hat{\\beta}(T) \\;=\\; -\\,\\left.\\frac{d}{dt}\\,\\ln y(t)\\right|_{t=T}.\n$$\nCompute the truncation bias\n$$\n\\operatorname{Bias}(T) \\;=\\; \\hat{\\beta}(T) \\;-\\; \\beta\n$$\nexactly, as a closed-form analytical expression in terms of $A$, $B$, $\\alpha$, $\\beta$, and $T$.\n\nExpress your final answer as a single symbolic expression for $\\operatorname{Bias}(T)$ (no units). Do not approximate or round.",
            "solution": "The problem is subjected to validation and is deemed valid. It is scientifically grounded in standard pharmacokinetic theory, mathematically well-posed, objective, and internally consistent. We may therefore proceed with the solution.\n\nThe problem asks for two main results based on the biexponential model for plasma concentration $y(t)$ following an intravenous bolus injection:\n$$\ny(t) = A \\exp(-\\alpha t) + B \\exp(-\\beta t)\n$$\nwhere $A>0$, $B>0$, and the rate constants satisfy $0 < \\beta < \\alpha$.\n\nFirst, we will justify the identity for the terminal elimination rate constant $\\beta$ and explain the discrepancy that arises from finite-time evaluation. The identity in question is:\n$$\n\\beta = - \\lim_{t \\to \\infty} \\frac{d}{dt}\\,\\ln y(t)\n$$\nTo begin, we compute the derivative of the natural logarithm of $y(t)$. Using the chain rule, $\\frac{d}{dt} \\ln(u) = \\frac{1}{u} \\frac{du}{dt}$, we have:\n$$\n\\frac{d}{dt}\\,\\ln y(t) = \\frac{1}{y(t)} \\frac{dy}{dt}\n$$\nFirst, we find the derivative of $y(t)$ with respect to time $t$:\n$$\n\\frac{dy}{dt} = \\frac{d}{dt} \\left( A \\exp(-\\alpha t) + B \\exp(-\\beta t) \\right) = -A\\alpha \\exp(-\\alpha t) - B\\beta \\exp(-\\beta t)\n$$\nSubstituting this and the expression for $y(t)$ into the equation for the derivative of $\\ln y(t)$:\n$$\n\\frac{d}{dt}\\,\\ln y(t) = \\frac{-A\\alpha \\exp(-\\alpha t) - B\\beta \\exp(-\\beta t)}{A \\exp(-\\alpha t) + B \\exp(-\\beta t)}\n$$\nNow, we evaluate the limit as $t \\to \\infty$. To resolve the indeterminate form $\\frac{0}{0}$, we factor out the term with the slowest-decaying exponential, which is $\\exp(-\\beta t)$ since we are given that $\\beta < \\alpha$.\n$$\n\\lim_{t \\to \\infty} \\frac{d}{dt}\\,\\ln y(t) = \\lim_{t \\to \\infty} \\frac{\\exp(-\\beta t) \\left( -A\\alpha \\exp(-(\\alpha-\\beta)t) - B\\beta \\right)}{\\exp(-\\beta t) \\left( A \\exp(-(\\alpha-\\beta)t) + B \\right)}\n$$\nCanceling the common factor $\\exp(-\\beta t)$:\n$$\n\\lim_{t \\to \\infty} \\frac{-A\\alpha \\exp(-(\\alpha-\\beta)t) - B\\beta}{A \\exp(-(\\alpha-\\beta)t) + B}\n$$\nSince $\\alpha > \\beta$, the term $\\alpha - \\beta$ is positive. Therefore, as $t \\to \\infty$, the exponential term $\\exp(-(\\alpha-\\beta)t)$ approaches $0$. The limit becomes:\n$$\n\\frac{-A\\alpha (0) - B\\beta}{A (0) + B} = \\frac{-B\\beta}{B} = -\\beta\n$$\nThus, we have shown that $\\lim_{t \\to \\infty} \\frac{d}{dt}\\,\\ln y(t) = -\\beta$, which confirms the identity $\\beta = - \\lim_{t \\to \\infty} \\frac{d}{dt}\\,\\ln y(t)$.\n\nNext, we explain why replacing the limit with evaluation at a finite time $t=T$ induces a nonzero discrepancy. The finite-time estimator is defined as:\n$$\n\\hat{\\beta}(T) = -\\left.\\frac{d}{dt}\\,\\ln y(t)\\right|_{t=T} = - \\frac{-A\\alpha \\exp(-\\alpha T) - B\\beta \\exp(-\\beta T)}{A \\exp(-\\alpha T) + B \\exp(-\\beta T)} = \\frac{A\\alpha \\exp(-\\alpha T) + B\\beta \\exp(-\\beta T)}{A \\exp(-\\alpha T) + B \\exp(-\\beta T)}\n$$\nA discrepancy exists if $\\hat{\\beta}(T) \\neq \\beta$. Let us assume, for the sake of contradiction, that $\\hat{\\beta}(T) = \\beta$ for some finite $T>0$.\n$$\n\\frac{A\\alpha \\exp(-\\alpha T) + B\\beta \\exp(-\\beta T)}{A \\exp(-\\alpha T) + B \\exp(-\\beta T)} = \\beta\n$$\n$$\nA\\alpha \\exp(-\\alpha T) + B\\beta \\exp(-\\beta T) = \\beta \\left( A \\exp(-\\alpha T) + B \\exp(-\\beta T) \\right)\n$$\n$$\nA\\alpha \\exp(-\\alpha T) + B\\beta \\exp(-\\beta T) = A\\beta \\exp(-\\alpha T) + B\\beta \\exp(-\\beta T)\n$$\nSubtracting $B\\beta \\exp(-\\beta T)$ from both sides gives:\n$$\nA\\alpha \\exp(-\\alpha T) = A\\beta \\exp(-\\alpha T)\n$$\nRearranging the terms:\n$$\nA(\\alpha - \\beta) \\exp(-\\alpha T) = 0\n$$\nThe problem states that $A > 0$ and $\\alpha > \\beta$, which implies $\\alpha - \\beta > 0$. For any finite $T$, the term $\\exp(-\\alpha T)$ is also positive. Therefore, the product $A(\\alpha - \\beta) \\exp(-\\alpha T)$ is strictly positive. This contradicts the result that the product must be $0$. The initial assumption that $\\hat{\\beta}(T) = \\beta$ must be false. Consequently, a nonzero discrepancy exists for any finite $T$ as long as $A>0$. This discrepancy arises because the faster-decaying exponential term, $A \\exp(-\\alpha t)$, has not fully vanished at any finite time $T$.\n\nSecond, we compute the truncation bias, $\\operatorname{Bias}(T) = \\hat{\\beta}(T) - \\beta$. Using the expression derived for $\\hat{\\beta}(T)$:\n$$\n\\operatorname{Bias}(T) = \\left( \\frac{A\\alpha \\exp(-\\alpha T) + B\\beta \\exp(-\\beta T)}{A \\exp(-\\alpha T) + B \\exp(-\\beta T)} \\right) - \\beta\n$$\nTo simplify, we combine the terms over a common denominator:\n$$\n\\operatorname{Bias}(T) = \\frac{(A\\alpha \\exp(-\\alpha T) + B\\beta \\exp(-\\beta T)) - \\beta(A \\exp(-\\alpha T) + B \\exp(-\\beta T))}{A \\exp(-\\alpha T) + B \\exp(-\\beta T)}\n$$\nDistributing $\\beta$ in the numerator:\n$$\n\\operatorname{Bias}(T) = \\frac{A\\alpha \\exp(-\\alpha T) + B\\beta \\exp(-\\beta T) - A\\beta \\exp(-\\alpha T) - B\\beta \\exp(-\\beta T)}{A \\exp(-\\alpha T) + B \\exp(-\\beta T)}\n$$\nThe terms involving $B$ cancel out:\n$$\n\\operatorname{Bias}(T) = \\frac{A\\alpha \\exp(-\\alpha T) - A\\beta \\exp(-\\alpha T)}{A \\exp(-\\alpha T) + B \\exp(-\\beta T)}\n$$\nFactoring the numerator gives the expression for the bias:\n$$\n\\operatorname{Bias}(T) = \\frac{A(\\alpha - \\beta) \\exp(-\\alpha T)}{A \\exp(-\\alpha T) + B \\exp(-\\beta T)}\n$$\nThis is a valid closed-form expression. For a more compact representation, we can multiply the numerator and the denominator by $\\exp(\\beta T)$:\n$$\n\\operatorname{Bias}(T) = \\frac{A(\\alpha - \\beta) \\exp(-\\alpha T)\\exp(\\beta T)}{(A \\exp(-\\alpha T) + B \\exp(-\\beta T))\\exp(\\beta T)} = \\frac{A(\\alpha - \\beta) \\exp(-(\\alpha - \\beta)T)}{A\\exp(-(\\alpha - \\beta)T) + B}\n$$\nThis final expression represents the exact analytical form for the truncation bias.",
            "answer": "$$\n\\boxed{\\frac{A (\\alpha - \\beta) \\exp(-(\\alpha - \\beta)T)}{A \\exp(-(\\alpha - \\beta)T) + B}}\n$$"
        }
    ]
}