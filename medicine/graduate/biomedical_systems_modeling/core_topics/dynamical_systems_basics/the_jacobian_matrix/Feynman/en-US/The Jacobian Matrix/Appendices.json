{
    "hands_on_practices": [
        {
            "introduction": "The first step in understanding the complex behavior of a nonlinear dynamical system is to analyze its behavior near equilibrium. This exercise provides fundamental practice in this core technique. By calculating the Jacobian matrix at a fixed point, you will construct the \"best linear approximation\" of the system, which allows you to predict its local stability and dynamics without solving the full nonlinear equations .",
            "id": "1717040",
            "problem": "Consider a simplified model for the interaction of two quantities, $x(t)$ and $y(t)$, evolving over time $t$. Their rates of change are described by the following system of coupled, nonlinear differential equations:\n$$\n\\begin{aligned}\n\\frac{dx}{dt} &= x - xy \\\\\n\\frac{dy}{dt} &= -y + xy\n\\end{aligned}\n$$\nAn equilibrium point of this system is a point $(x^*, y^*)$ where both rates of change are zero. This system possesses an equilibrium point where both $x^*$ and $y^*$ are positive constants. The dynamics of the system for states close to this positive equilibrium point can be approximated by a linear system of the form $\\frac{d\\vec{u}}{dt} = A \\vec{u}$, where $\\vec{u}$ represents small deviations from the equilibrium and $A$ is a constant 2x2 matrix.\n\nWhich of the following matrices corresponds to the matrix $A$ for the linearization around the positive equilibrium point?\n\nA. $\\begin{pmatrix} 1 & -1 \\\\ 1 & -1 \\end{pmatrix}$\n\nB. $\\begin{pmatrix} 0 & -1 \\\\ 1 & 0 \\end{pmatrix}$\n\nC. $\\begin{pmatrix} 1-y & -x \\\\ y & x-1 \\end{pmatrix}$\n\nD. $\\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix}$\n\nE. $\\begin{pmatrix} 0 & 1 \\\\ -1 & 0 \\end{pmatrix}$",
            "solution": "Set $f_{1}(x,y)=x-xy$ and $f_{2}(x,y)=-y+xy$. Equilibria satisfy $f_{1}=0$ and $f_{2}=0$:\n$$\nx(1-y)=0,\\quad y(x-1)=0.\n$$\nFor a positive equilibrium with $x^{*}>0$ and $y^{*}>0$, we must have $1-y^{*}=0$ and $x^{*}-1=0$, hence $(x^{*},y^{*})=(1,1)$.\n\nLinearization uses the Jacobian matrix $J(x,y)$ of $\\vec{f}(x,y)=(f_{1},f_{2})$:\n$$\nJ(x,y)=\\begin{pmatrix}\n\\frac{\\partial f_{1}}{\\partial x} & \\frac{\\partial f_{1}}{\\partial y} \\\\\n\\frac{\\partial f_{2}}{\\partial x} & \\frac{\\partial f_{2}}{\\partial y}\n\\end{pmatrix}\n=\\begin{pmatrix}\n1-y & -x \\\\\ny & x-1\n\\end{pmatrix}.\n$$\nEvaluating at $(x^{*},y^{*})=(1,1)$ gives\n$$\nA=J(1,1)=\\begin{pmatrix}\n1-1 & -1 \\\\\n1 & 1-1\n\\end{pmatrix}=\\begin{pmatrix}\n0 & -1 \\\\\n1 & 0\n\\end{pmatrix}.\n$$\nComparing with the options, this corresponds to option B.",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "Building on the foundational skill of linearization, this problem applies the Jacobian matrix to a classic biomedical context: a two-compartment pharmacokinetic (PK) model. Here, the Jacobian is not just a mathematical construct; its eigenvalues directly determine the stability of the drug concentrations in the body. This practice demonstrates how to use the Jacobian to derive conditions for system stability, linking abstract mathematical properties to tangible physiological outcomes .",
            "id": "3937760",
            "problem": "Consider a two-compartment pharmacokinetic (PK) model, where PK stands for pharmacokinetics, with state vector $\\mathbf{x} = \\begin{pmatrix} x_{1} \\\\ x_{2} \\end{pmatrix}$ and input $u$ representing a constant infusion rate. The mass balance equations are given by\n$$\n\\dot{x}_{1} = -\\left(k_{10} + k_{12}\\right) x_{1} + k_{21} x_{2} + u, \\qquad\n\\dot{x}_{2} = k_{12} x_{1} - k_{21} x_{2},\n$$\nwhere $k_{10}$ is the elimination rate from compartment $1$, and $k_{12}$ and $k_{21}$ are inter-compartmental transfer rates from compartment $1$ to $2$ and from compartment $2$ to $1$, respectively. Assume $k_{10} > 0$, $k_{12} \\ge 0$, and $k_{21} > 0$, consistent with physical realism.\n\nStarting from the core definition of the Jacobian matrix of a vector field, construct the Jacobian $A = \\partial f / \\partial x$ of the autonomous part of the system (i.e., treat $u$ as a constant and do not include it in the Jacobian). Then, using first principles (definitions of the characteristic polynomial and eigenvalues), derive the eigenvalues of $A$ in closed form as functions of $k_{10}$, $k_{12}$, and $k_{21}$, and determine the necessary and sufficient conditions under which all eigenvalues have strictly negative real parts.\n\nExpress your final answer as the ordered pair of eigenvalues of $A$ in closed form. No numerical rounding is required, and no units should be included in the final expression.",
            "solution": "The system of differential equations describing the two-compartment model can be written in state-space form as $\\dot{\\mathbf{x}} = \\mathbf{f}(\\mathbf{x}) + \\mathbf{g}(u)$, where $\\mathbf{x} = \\begin{pmatrix} x_{1} \\\\ x_{2} \\end{pmatrix}$. The autonomous part of the system is described by the vector field $\\mathbf{f}(\\mathbf{x})$:\n$$\n\\mathbf{f}(\\mathbf{x}) = \\begin{pmatrix} f_1(x_1, x_2) \\\\ f_2(x_1, x_2) \\end{pmatrix} = \\begin{pmatrix} -\\left(k_{10} + k_{12}\\right) x_{1} + k_{21} x_{2} \\\\ k_{12} x_{1} - k_{21} x_{2} \\end{pmatrix}\n$$\nThe Jacobian matrix $A$ of this vector field is defined as $A = \\frac{\\partial \\mathbf{f}}{\\partial \\mathbf{x}}$. We compute the partial derivatives of the components of $\\mathbf{f}(\\mathbf{x})$ with respect to the state variables $x_1$ and $x_2$.\n$$\n\\frac{\\partial f_1}{\\partial x_1} = -\\left(k_{10} + k_{12}\\right)\n$$\n$$\n\\frac{\\partial f_1}{\\partial x_2} = k_{21}\n$$\n$$\n\\frac{\\partial f_2}{\\partial x_1} = k_{12}\n$$\n$$\n\\frac{\\partial f_2}{\\partial x_2} = -k_{21}\n$$\nAssembling these derivatives into a matrix gives the Jacobian $A$:\n$$\nA = \\begin{pmatrix}\n\\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} \\\\\n\\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2}\n\\end{pmatrix}\n= \\begin{pmatrix}\n-(k_{10} + k_{12}) & k_{21} \\\\\nk_{12} & -k_{21}\n\\end{pmatrix}\n$$\nThe eigenvalues $\\lambda$ of the matrix $A$ are the roots of the characteristic equation, which is defined by $\\det(A - \\lambda I) = 0$, where $I$ is the $2 \\times 2$ identity matrix.\n$$\nA - \\lambda I = \\begin{pmatrix}\n-(k_{10} + k_{12}) - \\lambda & k_{21} \\\\\nk_{12} & -k_{21} - \\lambda\n\\end{pmatrix}\n$$\nThe determinant is:\n$$\n\\det(A - \\lambda I) = \\left(-(k_{10} + k_{12}) - \\lambda\\right)\\left(-k_{21} - \\lambda\\right) - (k_{12})(k_{21}) = 0\n$$\nExpanding this expression yields the characteristic polynomial:\n$$\n(\\lambda + k_{10} + k_{12})(\\lambda + k_{21}) - k_{12}k_{21} = 0\n$$\n$$\n\\lambda^2 + (k_{21})\\lambda + (k_{10} + k_{12})\\lambda + (k_{10} + k_{12})k_{21} - k_{12}k_{21} = 0\n$$\n$$\n\\lambda^2 + (k_{10} + k_{12} + k_{21})\\lambda + k_{10}k_{21} + k_{12}k_{21} - k_{12}k_{21} = 0\n$$\n$$\n\\lambda^2 + (k_{10} + k_{12} + k_{21})\\lambda + k_{10}k_{21} = 0\n$$\nThis is a quadratic equation of the form $a\\lambda^2 + b\\lambda + c = 0$, with coefficients $a=1$, $b = k_{10} + k_{12} + k_{21}$, and $c = k_{10}k_{21}$. The roots (eigenvalues) can be found using the quadratic formula, $\\lambda = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$:\n$$\n\\lambda_{1,2} = \\frac{-(k_{10} + k_{12} + k_{21}) \\pm \\sqrt{(k_{10} + k_{12} + k_{21})^2 - 4k_{10}k_{21}}}{2}\n$$\nThis is the closed-form expression for the two eigenvalues of the Jacobian matrix $A$.\n\nNext, we determine the necessary and sufficient conditions for all eigenvalues to have strictly negative real parts. For a second-order characteristic polynomial $\\lambda^2 + p_1\\lambda + p_0 = 0$, the Routh-Hurwitz stability criterion states that the real parts of both roots are strictly negative if and only if both coefficients $p_1$ and $p_0$ are strictly positive.\nIn our case, the coefficients are:\n$$\np_1 = k_{10} + k_{12} + k_{21}\n$$\n$$\np_0 = k_{10}k_{21}\n$$\nThe problem states the physical constraints on the rate constants: $k_{10} > 0$, $k_{12} \\ge 0$, and $k_{21} > 0$.\nLet's check the positivity of the coefficients based on these constraints.\n1.  For $p_1$: Since $k_{10}$ and $k_{21}$ are strictly positive and $k_{12}$ is non-negative, their sum $p_1 = k_{10} + k_{12} + k_{21}$ must be strictly positive.\n2.  For $p_0$: Since $k_{10}$ and $k_{21}$ are both strictly positive, their product $p_0 = k_{10}k_{21}$ must also be strictly positive.\nBoth Routh-Hurwitz conditions ($p_1 > 0$ and $p_0 > 0$) are satisfied if and only if the given physical constraints ($k_{10} > 0$ and $k_{21} > 0$, with $k_{12} \\ge 0$) hold. Therefore, the necessary and sufficient conditions for all eigenvalues to have strictly negative real parts are precisely the given physical constraints on the rate constants, i.e., $k_{10} > 0$, $k_{12} \\ge 0$, and $k_{21} > 0$.\n\nThe final answer requires the ordered pair of eigenvalues in closed form.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{-(k_{10} + k_{12} + k_{21}) + \\sqrt{(k_{10} + k_{12} + k_{21})^2 - 4k_{10}k_{21}}}{2} & \\frac{-(k_{10} + k_{12} + k_{21}) - \\sqrt{(k_{10} + k_{12} + k_{21})^2 - 4k_{10}k_{21}}}{2} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "The utility of the Jacobian extends beyond analyzing state stability to the practical challenges of building and validating models from data. This advanced problem introduces the concept of a parameter Jacobian, which measures how a model's output changes with respect to its parameters. You will explore how the rank of this matrix is crucial for determining parameter identifiability and diagnosing confounding, essential skills for any serious modeler in the biomedical sciences .",
            "id": "3937826",
            "problem": "Consider a receptor–ligand binding experiment in biomedical systems modeling in which the measured bound receptors are modeled by a static input–output map $y = B_{\\text{bound}} = p_1 x / (1 + p_2 x)$, where $x$ is the free ligand concentration, and $(p_1, p_2)$ are unknown parameters. Suppose $N \\geq 2$ measurements $\\{(x_i, y_i)\\}_{i=1}^N$ are taken at distinct concentrations $x_i$ in a saturating regime satisfying $x_i \\gg 1/p_2$ for all $i$, and parameter estimation is performed by nonlinear least squares. Let the parameter Jacobian matrix $J_p$ be defined as the $N \\times 2$ matrix with entries $[J_p]_{i j} = \\partial y(x_i; p_1, p_2)/\\partial p_j$. \n\nUsing core definitions of the Jacobian, local sensitivity, and identifiability, reason from first principles to explain why a singular $J_p$ indicates parameter confounding in this setting. Then, choose the option that both (i) correctly explains the confounding in terms of local sensitivity and identifiable parameter combinations under the saturating design described, and (ii) proposes a reparameterization that yields a model with a parameter Jacobian of full column rank for $N \\geq 2$ distinct saturating concentrations $\\{x_i\\}$.\n\nWhich option is correct?\n\nA. A singular $J_p$ means the two sensitivity columns are linearly dependent, so only a combination such as $p_1/p_2$ is locally identifiable in the saturating regime where $x_i \\gg 1/p_2$. Reparameterize to $\\theta_1 = p_1/p_2$ and $\\theta_2 = 1/p_2$, yielding $y = \\theta_1 x / (\\theta_2 + x)$; for distinct $x_i$ in saturation, the parameter Jacobian in $(\\theta_1, \\theta_2)$ has full column rank.\n\nB. A singular $J_p$ simply indicates too few samples; with enough data, the original $(p_1, p_2)$ parameterization will automatically yield full rank without any change. No reparameterization is necessary.\n\nC. A singular $J_p$ occurs only when $p_2 = 0$, because the denominator then loses parameter dependence. A logarithmic reparameterization $\\theta_1 = \\ln p_1$, $\\theta_2 = \\ln p_2$ resolves the singularity and guarantees full column rank.\n\nD. A singular $J_p$ means the parameters are orthogonal and thus independently estimable; to remove the singularity, reparameterize to $\\theta_1 = p_1 p_2$ and $\\theta_2 = p_2$, which yields $y = (\\theta_1/\\theta_2) x / (1 + \\theta_2 x)$ and a full-rank Jacobian in the saturating regime.",
            "solution": "The analysis of parameter identifiability hinges on the properties of the parameter Jacobian matrix $J_p$. In nonlinear least squares, the parameter covariance is related to $(J_p^T J_p)^{-1}$. If $J_p$ does not have full column rank, $J_p^T J_p$ is singular, its inverse does not exist, and the parameters are not locally identifiable. For a $N \\times 2$ matrix, this means its two columns (the sensitivity vectors) are linearly dependent.\n\nThe model is $y(x; p_1, p_2) = \\frac{p_1 x}{1 + p_2 x}$. The sensitivity vectors are the columns of the Jacobian, given by the partial derivatives with respect to $p_1$ and $p_2$:\n1.  Sensitivity to $p_1$: $\\frac{\\partial y}{\\partial p_1} = \\frac{x}{1 + p_2 x}$\n2.  Sensitivity to $p_2$: $\\frac{\\partial y}{\\partial p_2} = -\\frac{p_1 x^2}{(1 + p_2 x)^2}$\n\nThe problem states that measurements are in a saturating regime where $x_i \\gg 1/p_2$, or $p_2 x_i \\gg 1$. Under this approximation, the sensitivities at each data point $x_i$ become:\n$$ \\frac{\\partial y}{\\partial p_1} \\approx \\frac{x_i}{p_2 x_i} = \\frac{1}{p_2} $$\n$$ \\frac{\\partial y}{\\partial p_2} \\approx -\\frac{p_1 x_i^2}{(p_2 x_i)^2} = -\\frac{p_1}{p_2^2} $$\nThe Jacobian matrix $J_p$ is approximately:\n$$ J_p \\approx \\begin{pmatrix}\n1/p_2 & -p_1/p_2^2 \\\\\n\\vdots & \\vdots \\\\\n1/p_2 & -p_1/p_2^2\n\\end{pmatrix} $$\nThe second column is a constant multiple of the first: $(\\text{column } 2) \\approx (-p_1/p_2) \\times (\\text{column } 1)$. The columns are linearly dependent, so $J_p$ is singular. This demonstrates parameter confounding: the data cannot distinguish the individual effects of $p_1$ and $p_2$. The model output itself in this regime is $y \\approx p_1x / (p_2x) = p_1/p_2$, which shows that the data only contains information about the ratio $p_1/p_2$.\n\nTo resolve this, we reparameterize the model using identifiable combinations. Let $\\theta_1 = p_1/p_2$ (the saturation level, $B_{\\max}$) and $\\theta_2 = 1/p_2$ (the half-saturation constant, $K_d$). The model becomes:\n$$ y(x; \\theta_1, \\theta_2) = \\frac{\\theta_1 x}{\\theta_2 + x} $$\nThe new Jacobian $J_\\theta$ has columns corresponding to the sensitivities with respect to $\\theta_1$ and $\\theta_2$:\n1.  Sensitivity to $\\theta_1$: $\\frac{\\partial y}{\\partial \\theta_1} = \\frac{x}{\\theta_2 + x}$\n2.  Sensitivity to $\\theta_2$: $\\frac{\\partial y}{\\partial \\theta_2} = -\\frac{\\theta_1 x}{(\\theta_2 + x)^2}$\n\nIn the saturating regime ($x_i \\gg \\theta_2$), these sensitivities become:\n$$ \\frac{\\partial y}{\\partial \\theta_1} \\approx \\frac{x_i}{x_i} = 1 $$\n$$ \\frac{\\partial y}{\\partial \\theta_2} \\approx -\\frac{\\theta_1 x_i}{x_i^2} = -\\frac{\\theta_1}{x_i} $$\nThe new Jacobian is approximately:\n$$ J_\\theta \\approx \\begin{pmatrix}\n1 & -\\theta_1/x_1 \\\\\n1 & -\\theta_1/x_2 \\\\\n\\vdots & \\vdots \\\\\n1 & -\\theta_1/x_N\n\\end{pmatrix} $$\nSince all $x_i$ are distinct and $N \\ge 2$, the second column is not a multiple of the first (a vector of ones). Thus, the columns are linearly independent, and $J_\\theta$ has full column rank.\n\nBased on this analysis:\n*   **Option A** is correct. It correctly identifies the linear dependence, the identifiable combination $p_1/p_2$, and the valid reparameterization that yields a full-rank Jacobian.\n*   **Option B** is incorrect. The issue is the uninformative experimental design (quality of data), not the quantity. More data from the same saturating regime won't fix the problem.\n*   **Option C** is incorrect. The singularity arises from the experimental design ($x_i \\to \\infty$), not from a specific parameter value like $p_2=0$. A logarithmic transform does not fix the underlying confounding.\n*   **Option D** is incorrect. A singular Jacobian indicates confounding, the opposite of orthogonality. The proposed reparameterization is equivalent to the original and does not solve the problem.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}