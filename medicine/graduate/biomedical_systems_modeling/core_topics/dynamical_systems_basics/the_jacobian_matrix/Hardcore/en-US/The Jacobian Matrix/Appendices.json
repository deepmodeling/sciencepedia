{
    "hands_on_practices": [
        {
            "introduction": "The behavior of nonlinear systems can be incredibly complex, but a cornerstone of their analysis is to simplify the problem by examining the dynamics near an equilibrium point. This practice guides you through the fundamental process of linearization, using the Jacobian matrix to construct a linear system that approximates the behavior of a nonlinear model. Mastering this technique is the first step toward assessing local stability and predicting how a system will respond to small perturbations. ",
            "id": "1717040",
            "problem": "Consider a simplified model for the interaction of two quantities, $x(t)$ and $y(t)$, evolving over time $t$. Their rates of change are described by the following system of coupled, nonlinear differential equations:\n$$\n\\begin{aligned}\n\\frac{dx}{dt} = x - xy \\\\\n\\frac{dy}{dt} = -y + xy\n\\end{aligned}\n$$\nAn equilibrium point of this system is a point $(x^*, y^*)$ where both rates of change are zero. This system possesses an equilibrium point where both $x^*$ and $y^*$ are positive constants. The dynamics of the system for states close to this positive equilibrium point can be approximated by a linear system of the form $\\frac{d\\vec{u}}{dt} = A \\vec{u}$, where $\\vec{u}$ represents small deviations from the equilibrium and $A$ is a constant 2x2 matrix.\n\nWhich of the following matrices corresponds to the matrix $A$ for the linearization around the positive equilibrium point?\n\nA. $\\begin{pmatrix} 1  -1 \\\\ 1  -1 \\end{pmatrix}$\n\nB. $\\begin{pmatrix} 0  -1 \\\\ 1  0 \\end{pmatrix}$\n\nC. $\\begin{pmatrix} 1-y  -x \\\\ y  x-1 \\end{pmatrix}$\n\nD. $\\begin{pmatrix} 1  0 \\\\ 0  -1 \\end{pmatrix}$\n\nE. $\\begin{pmatrix} 0  1 \\\\ -1  0 \\end{pmatrix}$",
            "solution": "Set $f_{1}(x,y)=x-xy$ and $f_{2}(x,y)=-y+xy$. Equilibria satisfy $f_{1}=0$ and $f_{2}=0$:\n$$\nx(1-y)=0,\\quad y(x-1)=0.\n$$\nFor a positive equilibrium with $x^{*}0$ and $y^{*}0$, we must have $1-y^{*}=0$ and $x^{*}-1=0$, hence $(x^{*},y^{*})=(1,1)$.\n\nLinearization uses the Jacobian matrix $J(x,y)$ of $\\vec{f}(x,y)=(f_{1},f_{2})$:\n$$\nJ(x,y)=\\begin{pmatrix}\n\\frac{\\partial f_{1}}{\\partial x}  \\frac{\\partial f_{1}}{\\partial y} \\\\\n\\frac{\\partial f_{2}}{\\partial x}  \\frac{\\partial f_{2}}{\\partial y}\n\\end{pmatrix}\n=\\begin{pmatrix}\n1-y  -x \\\\\ny  x-1\n\\end{pmatrix}.\n$$\nEvaluating at $(x^{*},y^{*})=(1,1)$ gives\n$$\nA=J(1,1)=\\begin{pmatrix}\n1-1  -1 \\\\\n1  1-1\n\\end{pmatrix}=\\begin{pmatrix}\n0  -1 \\\\\n1  0\n\\end{pmatrix}.\n$$\nComparing with the options, this corresponds to option B.",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "Building on the concept of linearization, we now apply the Jacobian to a core problem in biomedical engineering: pharmacokinetic (PK) modeling. This exercise demonstrates how the Jacobian's eigenvalues directly determine the stability of a multi-compartment drug distribution model. By analyzing these eigenvalues, you can predict whether drug concentrations will reach a stable steady state, a critical consideration for ensuring safe and effective drug therapy. ",
            "id": "3937760",
            "problem": "Consider a two-compartment pharmacokinetic (PK) model with state vector $\\mathbf{x} = \\begin{pmatrix} x_{1} \\\\ x_{2} \\end{pmatrix}$ and input $u$ representing a constant infusion rate. The mass balance equations are given by\n$$\n\\dot{x}_{1} = -\\left(k_{10} + k_{12}\\right) x_{1} + k_{21} x_{2} + u, \\qquad\n\\dot{x}_{2} = k_{12} x_{1} - k_{21} x_{2},\n$$\nwhere $k_{10}$ is the elimination rate from compartment $1$, and $k_{12}$ and $k_{21}$ are inter-compartmental transfer rates from compartment $1$ to $2$ and from compartment $2$ to $1$, respectively. Assume $k_{10}  0$, $k_{12} \\ge 0$, and $k_{21}  0$, consistent with physical realism.\n\nStarting from the core definition of the Jacobian matrix of a vector field, construct the Jacobian $A = \\partial f / \\partial x$ of the autonomous part of the system (i.e., treat $u$ as a constant and do not include it in the Jacobian). Then, using first principles (definitions of the characteristic polynomial and eigenvalues), derive the eigenvalues of $A$ in closed form as functions of $k_{10}$, $k_{12}$, and $k_{21}$, and determine the necessary and sufficient conditions under which all eigenvalues have strictly negative real parts.\n\nExpress your final answer as the ordered pair of eigenvalues of $A$ in closed form. No numerical rounding is required, and no units should be included in the final expression.",
            "solution": "The problem is valid as it presents a standard, well-defined problem in linear systems theory applied to a classical pharmacokinetic model. It is scientifically grounded, objective, and contains all necessary information to derive a unique solution.\n\nThe system of differential equations describing the two-compartment model is given by:\n$$\n\\dot{x}_{1} = -\\left(k_{10} + k_{12}\\right) x_{1} + k_{21} x_{2} + u\n$$\n$$\n\\dot{x}_{2} = k_{12} x_{1} - k_{21} x_{2}\n$$\nThis can be written in state-space form as $\\dot{\\mathbf{x}} = \\mathbf{f}(\\mathbf{x}) + \\mathbf{g}(u)$, where $\\mathbf{x} = \\begin{pmatrix} x_{1} \\\\ x_{2} \\end{pmatrix}$. The autonomous part of the system is described by the vector field $\\mathbf{f}(\\mathbf{x})$:\n$$\n\\mathbf{f}(\\mathbf{x}) = \\begin{pmatrix} f_1(x_1, x_2) \\\\ f_2(x_1, x_2) \\end{pmatrix} = \\begin{pmatrix} -\\left(k_{10} + k_{12}\\right) x_{1} + k_{21} x_{2} \\\\ k_{12} x_{1} - k_{21} x_{2} \\end{pmatrix}\n$$\nThe Jacobian matrix $A$ of this vector field is defined as $A = \\frac{\\partial \\mathbf{f}}{\\partial \\mathbf{x}}$. We compute the partial derivatives of the components of $\\mathbf{f}(\\mathbf{x})$ with respect to the state variables $x_1$ and $x_2$.\n$$\n\\frac{\\partial f_1}{\\partial x_1} = -\\left(k_{10} + k_{12}\\right)\n$$\n$$\n\\frac{\\partial f_1}{\\partial x_2} = k_{21}\n$$\n$$\n\\frac{\\partial f_2}{\\partial x_1} = k_{12}\n$$\n$$\n\\frac{\\partial f_2}{\\partial x_2} = -k_{21}\n$$\nAssembling these derivatives into a matrix gives the Jacobian $A$:\n$$\nA = \\begin{pmatrix}\n\\frac{\\partial f_1}{\\partial x_1}  \\frac{\\partial f_1}{\\partial x_2} \\\\\n\\frac{\\partial f_2}{\\partial x_1}  \\frac{\\partial f_2}{\\partial x_2}\n\\end{pmatrix}\n= \\begin{pmatrix}\n-(k_{10} + k_{12})  k_{21} \\\\\nk_{12}  -k_{21}\n\\end{pmatrix}\n$$\nThe eigenvalues $\\lambda$ of the matrix $A$ are the roots of the characteristic equation, which is defined by $\\det(A - \\lambda I) = 0$, where $I$ is the $2 \\times 2$ identity matrix.\n$$\nA - \\lambda I = \\begin{pmatrix}\n-(k_{10} + k_{12}) - \\lambda  k_{21} \\\\\nk_{12}  -k_{21} - \\lambda\n\\end{pmatrix}\n$$\nThe determinant is:\n$$\n\\det(A - \\lambda I) = \\left(-(k_{10} + k_{12}) - \\lambda\\right)\\left(-k_{21} - \\lambda\\right) - (k_{12})(k_{21}) = 0\n$$\nExpanding this expression yields the characteristic polynomial:\n$$\n(\\lambda + k_{10} + k_{12})(\\lambda + k_{21}) - k_{12}k_{21} = 0\n$$\n$$\n\\lambda^2 + (k_{21})\\lambda + (k_{10} + k_{12})\\lambda + (k_{10} + k_{12})k_{21} - k_{12}k_{21} = 0\n$$\n$$\n\\lambda^2 + (k_{10} + k_{12} + k_{21})\\lambda + k_{10}k_{21} + k_{12}k_{21} - k_{12}k_{21} = 0\n$$\n$$\n\\lambda^2 + (k_{10} + k_{12} + k_{21})\\lambda + k_{10}k_{21} = 0\n$$\nThis is a quadratic equation of the form $a\\lambda^2 + b\\lambda + c = 0$, with coefficients $a=1$, $b = k_{10} + k_{12} + k_{21}$, and $c = k_{10}k_{21}$. The roots (eigenvalues) can be found using the quadratic formula, $\\lambda = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$:\n$$\n\\lambda_{1,2} = \\frac{-(k_{10} + k_{12} + k_{21}) \\pm \\sqrt{(k_{10} + k_{12} + k_{21})^2 - 4k_{10}k_{21}}}{2}\n$$\nThis is the closed-form expression for the two eigenvalues of the Jacobian matrix $A$.\n\nNext, we determine the necessary and sufficient conditions for all eigenvalues to have strictly negative real parts. For a second-order characteristic polynomial $\\lambda^2 + p_1\\lambda + p_0 = 0$, the Routh-Hurwitz stability criterion states that the real parts of both roots are strictly negative if and only if both coefficients $p_1$ and $p_0$ are strictly positive.\nIn our case, the coefficients are:\n$$\np_1 = k_{10} + k_{12} + k_{21}\n$$\n$$\np_0 = k_{10}k_{21}\n$$\nThe problem states the physical constraints on the rate constants: $k_{10}  0$, $k_{12} \\ge 0$, and $k_{21}  0$.\nLet's check the positivity of the coefficients based on these constraints.\n1.  For $p_1$: Since $k_{10}$ and $k_{21}$ are strictly positive and $k_{12}$ is non-negative, their sum $p_1 = k_{10} + k_{12} + k_{21}$ must be strictly positive.\n2.  For $p_0$: Since $k_{10}$ and $k_{21}$ are both strictly positive, their product $p_0 = k_{10}k_{21}$ must also be strictly positive.\nBoth Routh-Hurwitz conditions ($p_1  0$ and $p_0  0$) are satisfied if and only if the given physical constraints ($k_{10}  0$ and $k_{21}  0$, with $k_{12} \\ge 0$) hold. Therefore, the necessary and sufficient conditions for all eigenvalues to have strictly negative real parts are precisely the given physical constraints on the rate constants, i.e., $k_{10}  0$, $k_{12} \\ge 0$, and $k_{21}  0$.\n\nThe final answer requires the ordered pair of eigenvalues in closed form.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{-(k_{10} + k_{12} + k_{21}) + \\sqrt{(k_{10} + k_{12} + k_{21})^2 - 4k_{10}k_{21}}}{2}  \\frac{-(k_{10} + k_{12} + k_{21}) - \\sqrt{(k_{10} + k_{12} + k_{21})^2 - 4k_{10}k_{21}}}{2} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "The Jacobian matrix is not only a tool for analyzing a system's dynamics but is also essential for assessing the model itself, particularly in the context of parameter estimation. This practice explores the concept of parameter identifiability, using a parameter-based Jacobian to determine if a model's parameters can be uniquely estimated from experimental data. You will investigate how an experimental design can lead to parameter confounding and learn how to diagnose and resolve this issue through reparameterization, a vital skill in quantitative systems pharmacology. ",
            "id": "3937826",
            "problem": "Consider a receptor–ligand binding experiment in biomedical systems modeling in which the measured bound receptors are modeled by a static input–output map $y = B_{\\text{bound}} = p_1 x \\big/ \\left(1 + p_2 x\\right)$, where $x$ is the free ligand concentration, and $(p_1, p_2)$ are unknown parameters. Suppose $N \\geq 2$ measurements $\\{(x_i, y_i)\\}_{i=1}^N$ are taken at distinct concentrations $x_i$ in a saturating regime satisfying $x_i \\gg 1/p_2$ for all $i$, and parameter estimation is performed by nonlinear least squares. Let the parameter Jacobian matrix $J_p$ be defined as the $N \\times 2$ matrix with entries $\\left[J_p\\right]_{i j} = \\partial y(x_i; p_1, p_2)/\\partial p_j$. \n\nUsing core definitions of the Jacobian, local sensitivity, and identifiability, reason from first principles to explain why a singular $J_p$ indicates parameter confounding in this setting. Then, choose the option that both (i) correctly explains the confounding in terms of local sensitivity and identifiable parameter combinations under the saturating design described, and (ii) proposes a reparameterization that yields a model with a parameter Jacobian of full column rank for $N \\geq 2$ distinct saturating concentrations $\\{x_i\\}$.\n\nWhich option is correct?\n\nA. A singular $J_p$ means the two sensitivity columns are linearly dependent, so only a combination such as $p_1/p_2$ is locally identifiable in the saturating regime where $x_i \\gg 1/p_2$. Reparameterize to $\\theta_1 = p_1/p_2$ and $\\theta_2 = 1/p_2$, yielding $y = \\theta_1 x \\big/ (\\theta_2 + x)$; for distinct $x_i$ in saturation, the parameter Jacobian in $(\\theta_1, \\theta_2)$ has full column rank.\n\nB. A singular $J_p$ simply indicates too few samples; with enough data, the original $(p_1, p_2)$ parameterization will automatically yield full rank without any change. No reparameterization is necessary.\n\nC. A singular $J_p$ occurs only when $p_2 = 0$, because the denominator then loses parameter dependence. A logarithmic reparameterization $\\theta_1 = \\ln p_1$, $\\theta_2 = \\ln p_2$ resolves the singularity and guarantees full column rank.\n\nD. A singular $J_p$ means the parameters are orthogonal and thus independently estimable; to remove the singularity, reparameterize to $\\theta_1 = p_1 p_2$ and $\\theta_2 = p_2$, which yields $y = (\\theta_1/\\theta_2) x \\big/ \\left(1 + \\theta_2 x\\right)$ and a full-rank Jacobian in the saturating regime.",
            "solution": "## Problem Validation\n\n### Step 1: Extract Givens\n- **Model:** $y = B_{\\text{bound}} = p_1 x \\big/ \\left(1 + p_2 x\\right)$\n- **Variables:** $x$ is the free ligand concentration (input), $y$ is the measured bound receptors (output).\n- **Parameters:** $(p_1, p_2)$ are unknown parameters.\n- **Data:** $N \\geq 2$ measurements $\\{(x_i, y_i)\\}_{i=1}^N$ are taken at distinct concentrations $x_i$.\n- **Experimental Condition:** The experiment is conducted in a saturating regime, satisfying $x_i \\gg 1/p_2$ for all $i$.\n- **Parameter Estimation Method:** Nonlinear least squares.\n- **Definition of Jacobian:** The parameter Jacobian matrix $J_p$ is an $N \\times 2$ matrix with entries $\\left[J_p\\right]_{i j} = \\partial y(x_i; p_1, p_2)/\\partial p_j$.\n- **Task:** Explain why a singular $J_p$ indicates parameter confounding under the given conditions and choose the option that correctly explains this and proposes a valid reparameterization that resolves the issue.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded:** The model $y = p_1 x / (1 + p_2 x)$ is a form of the Langmuir isotherm or Michaelis-Menten kinetics, a fundamental and well-established model in biochemistry and pharmacology. Rewriting it as $y = (p_1/p_2) x / (1/p_2 + x)$ allows identification with the standard form $B = B_{\\max} [L] / (K_d + [L])$, where $B_{\\max} = p_1/p_2$ and $K_d = 1/p_2$. The problem is scientifically sound.\n2.  **Well-Posed:** The problem is well-posed. It asks for an analysis of parameter identifiability using the Jacobian matrix under a specific, clearly defined experimental condition ($x_i \\gg 1/p_2$). The concepts of Jacobian, sensitivity, confounding, and reparameterization are standard in systems identification and statistical modeling. The question has a unique, derivable answer.\n3.  **Objective:** The problem is stated in precise, objective, mathematical language. It is free from ambiguity and subjective claims.\n\nThe problem statement is scientifically sound, well-posed, objective, and complete. It does not violate any of the invalidity criteria.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. The solution process will now proceed.\n\n## Solution Derivation\n\nThe analysis of parameter identifiability hinges on the properties of the parameter Jacobian matrix $J_p$. In the context of nonlinear least squares, the approximate covariance matrix of the parameter estimates is proportional to $(J_p^T J_p)^{-1}$. If $J_p$ does not have full column rank, then $J_p^T J_p$ is singular and its inverse does not exist. This situation indicates that the parameters are not locally identifiable from the given data. For an $N \\times 2$ Jacobian matrix ($N \\geq 2$), not having full column rank means its two columns are linearly dependent.\n\nThe model is given by:\n$$ y(x; p_1, p_2) = \\frac{p_1 x}{1 + p_2 x} $$\n\nThe columns of the Jacobian matrix $J_p$ are the sensitivity vectors with respect to each parameter, evaluated at the measurement points $\\{x_i\\}_{i=1}^N$. The entries are the partial derivatives:\n1.  Sensitivity with respect to $p_1$:\n    $$ \\frac{\\partial y}{\\partial p_1} = \\frac{x}{1 + p_2 x} $$\n2.  Sensitivity with respect to $p_2$:\n    $$ \\frac{\\partial y}{\\partial p_2} = \\frac{\\partial}{\\partial p_2} \\left[ p_1 x (1 + p_2 x)^{-1} \\right] = p_1 x (-1)(1 + p_2 x)^{-2} (x) = -\\frac{p_1 x^2}{(1 + p_2 x)^2} $$\n\nThe Jacobian is an $N \\times 2$ matrix:\n$$ J_p = \\begin{pmatrix}\n\\frac{x_1}{1 + p_2 x_1}  -\\frac{p_1 x_1^2}{(1 + p_2 x_1)^2} \\\\\n\\vdots  \\vdots \\\\\n\\frac{x_N}{1 + p_2 x_N}  -\\frac{p_1 x_N^2}{(1 + p_2 x_N)^2}\n\\end{pmatrix} $$\n\nThe problem specifies that the measurements are taken in a saturating regime where $x_i \\gg 1/p_2$ for all $i$. This is equivalent to $p_2 x_i \\gg 1$. Let's analyze the sensitivities under this approximation.\nFor the first column:\n$$ \\frac{\\partial y}{\\partial p_1} = \\frac{x_i}{1 + p_2 x_i} \\approx \\frac{x_i}{p_2 x_i} = \\frac{1}{p_2} $$\nFor the second column:\n$$ \\frac{\\partial y}{\\partial p_2} = -\\frac{p_1 x_i^2}{(1 + p_2 x_i)^2} \\approx -\\frac{p_1 x_i^2}{(p_2 x_i)^2} = -\\frac{p_1 x_i^2}{p_2^2 x_i^2} = -\\frac{p_1}{p_2^2} $$\n\nUnder this condition, the Jacobian matrix becomes approximately:\n$$ J_p \\approx \\begin{pmatrix}\n1/p_2  -p_1/p_2^2 \\\\\n\\vdots  \\vdots \\\\\n1/p_2  -p_1/p_2^2\n\\end{pmatrix} $$\nLet the first column be $S_1$ and the second be $S_2$. We can see that for each row $i$:\n$$ [S_2]_i \\approx -\\frac{p_1}{p_2^2} = \\left(-\\frac{p_1}{p_2}\\right) \\left(\\frac{1}{p_2}\\right) \\approx \\left(-\\frac{p_1}{p_2}\\right) [S_1]_i $$\nThe two columns are approximately linearly dependent, with $S_2 \\approx (-p_1/p_2) S_1$. Therefore, the Jacobian $J_p$ is singular (or numerically singular). This linear dependence of the sensitivity vectors is the mathematical representation of parameter confounding. It signifies that changing $p_1$ has nearly the same effect on the model output as changing $p_2$ in a fixed proportion. The data collected in this regime cannot distinguish between the individual effects of $p_1$ and $p_2$.\nThe model output itself under saturation is $y \\approx p_1x / (p_2x) = p_1/p_2$. This shows that the data primarily contain information about the ratio $p_1/p_2$, not about $p_1$ and $p_2$ individually. This ratio is the only locally identifiable parameter combination from this experimental design.\n\nTo resolve this, we must reparameterize the model in terms of parameters that are identifiable. A natural choice is to define new parameters based on the identifiable combination. Let:\n$$ \\theta_1 = \\frac{p_1}{p_2} \\quad \\text{(the saturation level)} $$\nTo complete the parameter set, we can rewrite the original model as:\n$$ y = \\frac{(p_1/p_2) x}{1/p_2 + x} = \\frac{\\theta_1 x}{1/p_2 + x} $$\nThis suggests defining the second parameter as:\n$$ \\theta_2 = \\frac{1}{p_2} \\quad \\text{(the half-saturation constant, } K_d \\text{)} $$\nThe reparameterized model is:\n$$ y(x; \\theta_1, \\theta_2) = \\frac{\\theta_1 x}{\\theta_2 + x} $$\nNow, let's compute the Jacobian $J_\\theta$ for these new parameters.\n1.  Sensitivity with respect to $\\theta_1$:\n    $$ \\frac{\\partial y}{\\partial \\theta_1} = \\frac{x}{\\theta_2 + x} $$\n2.  Sensitivity with respect to $\\theta_2$:\n    $$ \\frac{\\partial y}{\\partial \\theta_2} = \\frac{\\partial}{\\partial \\theta_2} \\left[ \\theta_1 x (\\theta_2 + x)^{-1} \\right] = \\theta_1 x (-1)(\\theta_2 + x)^{-2} = -\\frac{\\theta_1 x}{(\\theta_2 + x)^2} $$\nThe new Jacobian $J_\\theta$ is:\n$$ J_\\theta = \\begin{pmatrix}\n\\frac{x_1}{\\theta_2 + x_1}  -\\frac{\\theta_1 x_1}{(\\theta_2 + x_1)^2} \\\\\n\\vdots  \\vdots \\\\\n\\frac{x_N}{\\theta_2 + x_N}  -\\frac{\\theta_1 x_N}{(\\theta_2 + x_N)^2}\n\\end{pmatrix} $$\nWe analyze $J_\\theta$ in the same saturating regime: $x_i \\gg 1/p_2$, which translates to $x_i \\gg \\theta_2$.\nFor the first column of $J_\\theta$:\n$$ \\frac{\\partial y}{\\partial \\theta_1} = \\frac{x_i}{\\theta_2 + x_i} \\approx \\frac{x_i}{x_i} = 1 $$\nFor the second column of $J_\\theta$:\n$$ \\frac{\\partial y}{\\partial \\theta_2} = -\\frac{\\theta_1 x_i}{(\\theta_2 + x_i)^2} \\approx -\\frac{\\theta_1 x_i}{x_i^2} = -\\frac{\\theta_1}{x_i} $$\nThe approximate Jacobian is:\n$$ J_\\theta \\approx \\begin{pmatrix}\n1  -\\theta_1/x_1 \\\\\n1  -\\theta_1/x_2 \\\\\n\\vdots  \\vdots \\\\\n1  -\\theta_1/x_N\n\\end{pmatrix} $$\nFor $J_\\theta$ to have full column rank, its columns must be linearly independent. The first column is a vector of ones. The second column is a vector with elements $-\\theta_1/x_i$. Since the problem states that concentrations $x_i$ are distinct and $N \\geq 2$, the elements of the second column are all different (assuming $\\theta_1 \\neq 0$). A constant vector and a non-constant vector are always linearly independent. Thus, the columns of $J_\\theta$ are linearly independent, and $J_\\theta$ has full column rank of $2$. This reparameterization successfully resolves the confounding issue for the given experimental design.\n\n## Option-by-Option Analysis\n\n**A. A singular $J_p$ means the two sensitivity columns are linearly dependent, so only a combination such as $p_1/p_2$ is locally identifiable in the saturating regime where $x_i \\gg 1/p_2$. Reparameterize to $\\theta_1 = p_1/p_2$ and $\\theta_2 = 1/p_2$, yielding $y = \\theta_1 x \\big/ (\\theta_2 + x)$; for distinct $x_i$ in saturation, the parameter Jacobian in $(\\theta_1, \\theta_2)$ has full column rank.**\nThis option correctly states that a singular $J_p$ implies linearly dependent sensitivity columns. It correctly identifies from the model's asymptotic behavior ($y \\to p_1/p_2$) that $p_1/p_2$ is the identifiable combination in the saturating regime. The proposed reparameterization $\\theta_1=p_1/p_2$, $\\theta_2=1/p_2$ and the resulting model are correct. Finally, the claim that the new Jacobian has full column rank under the specified conditions is consistent with our derivation.\n**Verdict:** Correct.\n\n**B. A singular $J_p$ simply indicates too few samples; with enough data, the original $(p_1, p_2)$ parameterization will automatically yield full rank without any change. No reparameterization is necessary.**\nThis is incorrect. The singularity is caused by an uninformative experimental design (all data in saturation), which leads to a *practical non-identifiability* problem. Adding more data points from the same saturating regime will not make the sensitivity columns linearly independent. The issue is the *quality* of the data (its location on the concentration curve), not the *quantity*.\n**Verdict:** Incorrect.\n\n**C. A singular $J_p$ occurs only when $p_2 = 0$, because the denominator then loses parameter dependence. A logarithmic reparameterization $\\theta_1 = \\ln p_1$, $\\theta_2 = \\ln p_2$ resolves the singularity and guarantees full column rank.**\nThis is incorrect. The singularity arises in the limit $p_2 x_i \\to \\infty$, not when $p_2=0$. If $p_2=0$, the Jacobian columns are not linearly dependent (for distinct non-zero $x_i$). A logarithmic reparameterization is a change of coordinates that does not alter the fundamental linear dependence of the sensitivity effects; it does not resolve the confounding caused by the experimental design.\n**Verdict:** Incorrect.\n\n**D. A singular $J_p$ means the parameters are orthogonal and thus independently estimable; to remove the singularity, reparameterize to $\\theta_1 = p_1 p_2$ and $\\theta_2 = p_2$, which yields $y = (\\theta_1/\\theta_2) x \\big/ \\left(1 + \\theta_2 x\\right)$ and a full-rank Jacobian in the saturating regime.**\nThis is incorrect on multiple grounds. First, a singular Jacobian (linearly dependent columns) is the antithesis of orthogonality. Orthogonal columns would imply maximal identifiability. Second, the proposed reparameterization is trivial; it is equivalent to the original parameterization ($p_1 = \\theta_1/\\theta_2, p_2=\\theta_2$) and does not change the model structure or address the confounding. It merely relabels the problem.\n**Verdict:** Incorrect.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}