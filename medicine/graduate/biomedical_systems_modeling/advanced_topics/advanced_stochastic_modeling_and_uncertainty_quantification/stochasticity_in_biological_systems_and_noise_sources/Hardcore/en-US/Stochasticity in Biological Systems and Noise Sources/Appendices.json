{
    "hands_on_practices": [
        {
            "introduction": "The simple birth-death process is a canonical model in systems biology, capturing the essential dynamics of constitutive gene expression where molecules are produced at a constant rate and degrade proportionally to their abundance. This first exercise guides you through a foundational analysis using the Chemical Master Equation (CME), the exact mathematical description for this stochastic process . By finding the stationary solution, you will derive the Poisson distribution that governs molecule counts at steady state and calculate key noise metrics, providing a direct link between reaction kinetics and the origins of intrinsic noise.",
            "id": "3932684",
            "problem": "Consider a single intracellular molecular species undergoing constitutive synthesis and first-order degradation in a well-mixed single cell. Let $N(t)$ denote the molecular copy number at time $t$. The system is modeled as a continuous-time Markov process (birth-death process) with the following elementary reactions: a synthesis reaction that increases the copy number by one with constant propensity $k$ (units: molecules per minute), and a degradation reaction that decreases the copy number by one with propensity $\\gamma N(t)$ (units: per minute times molecules). Assume the system is ergodic and reaches a unique stationary distribution. Starting from the Chemical Master Equation (CME) for this birth-death process, derive the stationary distribution. Then, compute at stationarity the mean $\\mathbb{E}[N]$, the variance $\\mathrm{Var}[N]$, and the coefficient of variation defined as $\\mathrm{CV} = \\sqrt{\\mathrm{Var}[N]}/\\mathbb{E}[N]$. Use the parameter values $k = 20\\ \\text{molecules/min}$ and $\\gamma = 1\\ \\text{min}^{-1}$. Express the mean in molecules, the variance in molecules squared, and the coefficient of variation as a dimensionless quantity. No rounding is required; provide exact values.",
            "solution": "The problem describes a birth-death process for the number of molecules, $N(t)$, of a single species in a cell. The process consists of two elementary reactions:\n1. Synthesis (birth): $\\emptyset \\xrightarrow{k} \\text{Molecule}$, with a constant propensity (rate) $k$. This reaction increases the number of molecules from $n$ to $n+1$.\n2. Degradation (death): $\\text{Molecule} \\xrightarrow{\\gamma} \\emptyset$, with a propensity $\\gamma n$, where $n$ is the current number of molecules. This reaction decreases the number of molecules from $n$ to $n-1$.\n\nLet $P(n, t)$ be the probability that the system has $n$ molecules at time $t$. The time evolution of this probability distribution is governed by the Chemical Master Equation (CME). For an integer number of molecules $n \\ge 0$, the CME is a set of coupled ordinary differential equations:\n$$\n\\frac{d P(n, t)}{d t} = (\\text{flux into state } n) - (\\text{flux out of state } n)\n$$\nThe flux into state $n$ comes from state $n-1$ via synthesis and from state $n+1$ via degradation. The flux out of state $n$ goes to state $n+1$ via synthesis and to state $n-1$ via degradation.\nFor $n \\ge 1$, the CME is:\n$$\n\\frac{d P(n, t)}{d t} = k P(n-1, t) + \\gamma (n+1) P(n+1, t) - (k + \\gamma n) P(n, t)\n$$\nFor the boundary case $n=0$, synthesis from a non-existent state $n=-1$ is impossible, and degradation from state $n=0$ is also impossible. Thus, the CME for $n=0$ is:\n$$\n\\frac{d P(0, t)}{d t} = \\gamma (1) P(1, t) - k P(0, t)\n$$\nThe problem states that the system reaches a unique stationary distribution. At stationarity, the probability distribution is time-invariant, so $\\frac{d P(n, t)}{d t} = 0$ for all $n$. Let $P_n$ denote the stationary probability $P(n, t \\to \\infty)$. The stationary CME equations are:\n$$\n0 = k P_{n-1} + \\gamma (n+1) P_{n+1} - (k + \\gamma n) P_n \\quad \\text{for } n \\ge 1\n$$\n$$\n0 = \\gamma P_1 - k P_0 \\quad \\text{for } n=0\n$$\nFor a one-dimensional birth-death process, the stationary state is characterized by the principle of detailed balance, where the net probabilistic flux between any two adjacent states is zero. That is, the flux from state $n$ to $n+1$ must equal the flux from state $n+1$ to $n$:\n$$\n\\text{Flux}(n \\to n+1) = \\text{Flux}(n+1 \\to n)\n$$\n$$\nk P_n = \\gamma (n+1) P_{n+1} \\quad \\text{for all } n \\ge 0\n$$\nThis single recurrence relation satisfies both stationary CME equations. From it, we derive a relation for $P_{n+1}$ in terms of $P_n$:\n$$\nP_{n+1} = \\frac{k}{\\gamma(n+1)} P_n\n$$\nLet us define the parameter $\\alpha = k/\\gamma$. The recurrence becomes:\n$$\nP_{n+1} = \\frac{\\alpha}{n+1} P_n\n$$\nWe can solve this recurrence relation by iteration:\nFor $n=1$: $P_1 = \\frac{\\alpha}{1} P_0$\nFor $n=2$: $P_2 = \\frac{\\alpha}{2} P_1 = \\frac{\\alpha}{2} \\left(\\frac{\\alpha}{1} P_0\\right) = \\frac{\\alpha^2}{2 \\cdot 1} P_0 = \\frac{\\alpha^2}{2!} P_0$\nFor $n=3$: $P_3 = \\frac{\\alpha}{3} P_2 = \\frac{\\alpha}{3} \\left(\\frac{\\alpha^2}{2!} P_0\\right) = \\frac{\\alpha^3}{3!} P_0$\nBy induction, the general solution is:\n$$\nP_n = \\frac{\\alpha^n}{n!} P_0\n$$\nTo find the constant $P_0$, we use the normalization condition that the sum of all probabilities must be unity:\n$$\n\\sum_{n=0}^{\\infty} P_n = 1\n$$\n$$\n\\sum_{n=0}^{\\infty} \\frac{\\alpha^n}{n!} P_0 = P_0 \\sum_{n=0}^{\\infty} \\frac{\\alpha^n}{n!} = 1\n$$\nThe sum is the Taylor series expansion of the exponential function, $\\sum_{n=0}^{\\infty} \\frac{\\alpha^n}{n!} = \\exp(\\alpha)$.\n$$\nP_0 \\exp(\\alpha) = 1 \\implies P_0 = \\exp(-\\alpha)\n$$\nSubstituting this back into the expression for $P_n$, we obtain the stationary distribution:\n$$\nP_n = \\frac{\\alpha^n \\exp(-\\alpha)}{n!}\n$$\nThis is the probability mass function of a Poisson distribution with parameter $\\alpha = k/\\gamma$.\n\nNow, we compute the required statistical moments at stationarity.\nThe mean (expected value) of the number of molecules, $\\mathbb{E}[N]$, is:\n$$\n\\mathbb{E}[N] = \\sum_{n=0}^{\\infty} n P_n = \\sum_{n=1}^{\\infty} n \\frac{\\alpha^n \\exp(-\\alpha)}{n!} = \\exp(-\\alpha) \\sum_{n=1}^{\\infty} \\frac{\\alpha^n}{(n-1)!}\n$$\nLet $m = n-1$. The sum becomes:\n$$\n\\mathbb{E}[N] = \\exp(-\\alpha) \\sum_{m=0}^{\\infty} \\frac{\\alpha^{m+1}}{m!} = \\alpha \\exp(-\\alpha) \\sum_{m=0}^{\\infty} \\frac{\\alpha^m}{m!} = \\alpha \\exp(-\\alpha) \\exp(\\alpha) = \\alpha\n$$\nThus, the mean is $\\mathbb{E}[N] = \\alpha = k/\\gamma$.\n\nThe variance is $\\mathrm{Var}[N] = \\mathbb{E}[N^2] - (\\mathbb{E}[N])^2$. We first compute the second factorial moment, $\\mathbb{E}[N(N-1)]$:\n$$\n\\mathbb{E}[N(N-1)] = \\sum_{n=0}^{\\infty} n(n-1) P_n = \\sum_{n=2}^{\\infty} n(n-1) \\frac{\\alpha^n \\exp(-\\alpha)}{n!} = \\exp(-\\alpha) \\sum_{n=2}^{\\infty} \\frac{\\alpha^n}{(n-2)!}\n$$\nLet $m = n-2$. The sum becomes:\n$$\n\\mathbb{E}[N(N-1)] = \\exp(-\\alpha) \\sum_{m=0}^{\\infty} \\frac{\\alpha^{m+2}}{m!} = \\alpha^2 \\exp(-\\alpha) \\sum_{m=0}^{\\infty} \\frac{\\alpha^m}{m!} = \\alpha^2 \\exp(-\\alpha) \\exp(\\alpha) = \\alpha^2\n$$\nThe second moment $\\mathbb{E}[N^2]$ is related to the factorial moment by $\\mathbb{E}[N^2] = \\mathbb{E}[N(N-1) + N] = \\mathbb{E}[N(N-1)] + \\mathbb{E}[N]$.\n$$\n\\mathbb{E}[N^2] = \\alpha^2 + \\alpha\n$$\nNow, we can compute the variance:\n$$\n\\mathrm{Var}[N] = \\mathbb{E}[N^2] - (\\mathbb{E}[N])^2 = (\\alpha^2 + \\alpha) - (\\alpha)^2 = \\alpha\n$$\nThus, the variance is $\\mathrm{Var}[N] = \\alpha = k/\\gamma$.\n\nThe coefficient of variation, $\\mathrm{CV}$, is defined as the ratio of the standard deviation to the mean:\n$$\n\\mathrm{CV} = \\frac{\\sqrt{\\mathrm{Var}[N]}}{\\mathbb{E}[N]} = \\frac{\\sqrt{\\alpha}}{\\alpha} = \\frac{1}{\\sqrt{\\alpha}}\n$$\n\nFinally, we substitute the given parameter values: $k = 20\\ \\text{molecules/min}$ and $\\gamma = 1\\ \\text{min}^{-1}$.\nThe parameter $\\alpha$ is:\n$$\n\\alpha = \\frac{k}{\\gamma} = \\frac{20}{1} = 20\n$$\nThe mean number of molecules is:\n$$\n\\mathbb{E}[N] = \\alpha = 20\\ \\text{molecules}\n$$\nThe variance of the number of molecules is:\n$$\n\\mathrm{Var}[N] = \\alpha = 20\\ \\text{molecules}^2\n$$\nThe coefficient of variation is:\n$$\n\\mathrm{CV} = \\frac{1}{\\sqrt{\\alpha}} = \\frac{1}{\\sqrt{20}} = \\frac{1}{\\sqrt{4 \\times 5}} = \\frac{1}{2\\sqrt{5}} = \\frac{\\sqrt{5}}{10}\n$$\nThis quantity is dimensionless. The problem requires exact values, which we have provided.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 20 & 20 & \\frac{\\sqrt{5}}{10} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "While the Chemical Master Equation provides an exact description, it is often analytically intractable for more complex systems. This practice introduces the Chemical Langevin Equation (CLE), a powerful stochastic differential equation that approximates the discrete dynamics as a continuous process . Here, you will apply the formidable tool of Itô’s lemma to the CLE representation of the same birth-death process to derive ordinary differential equations that govern the system's mean and variance, demonstrating how to extract statistical insights without solving for the full probability distribution.",
            "id": "3932700",
            "problem": "A single-species birth–death reaction network with constant birth propensity and linear death propensity can be approximated by the Chemical Langevin Equation (CLE), which is a stochastic differential equation driven by a Standard Wiener Process (SWP). Consider a molecular count variable $X(t)$ governed by the CLE\n$$\ndX(t) = \\left(k - \\gamma X(t)\\right)\\,dt + \\sqrt{k + \\gamma X(t)}\\,dW_{t},\n$$\nwhere $k>0$ is the birth rate constant, $\\gamma>0$ is the death rate constant, and $W_{t}$ is the SWP. The diffusion term $\\sqrt{k + \\gamma X(t)}$ reflects the shot noise arising from the sum of the birth and death reaction propensities, consistent with the diffusion approximation for reaction networks.\n\nStarting from the definition of variance $\\mathrm{Var}(X(t)) = \\mathbb{E}[X(t)^{2}] - \\left(\\mathbb{E}[X(t)]\\right)^{2}$ and the Itô calculus for a twice-differentiable test function $f(X)$ applied to the CLE, derive the coupled ordinary differential equations that govern the time evolution of the first moment $\\mathbb{E}[X(t)]$ and the variance $\\mathrm{Var}(X(t))$. Your derivation must begin from the general form of Itô’s lemma and proceed by selecting appropriate functions $f(X)$, without invoking any precomputed moment equations or closure approximations.\n\nExpress your final answer as a single row matrix whose first entry is the expression for $\\frac{d}{dt}\\mathbb{E}[X(t)]$ and whose second entry is the expression for $\\frac{d}{dt}\\mathrm{Var}(X(t))$. No numerical evaluation is required. No units are required. Do not include any equality signs or additional commentary in the final answer.",
            "solution": "The problem requires the derivation of the coupled ordinary differential equations (ODEs) for the time evolution of the first moment, $\\mathbb{E}[X(t)]$, and the variance, $\\mathrm{Var}(X(t))$, of a molecular count variable $X(t)$. The dynamics of $X(t)$ are governed by the Chemical Langevin Equation (CLE), which is a specific type of Itô stochastic differential equation (SDE).\n\nThe given SDE is:\n$$\ndX(t) = \\left(k - \\gamma X(t)\\right)\\,dt + \\sqrt{k + \\gamma X(t)}\\,dW_{t}\n$$\nThis SDE is of the general form $dX(t) = a(X,t)dt + b(X,t)dW_t$, where the drift coefficient is $a(X,t) = k - \\gamma X(t)$ and the diffusion coefficient is $b(X,t) = \\sqrt{k + \\gamma X(t)}$. Note that $b(X,t)^2 = k + \\gamma X(t)$. The constants $k$ and $\\gamma$ are positive.\n\nThe derivation relies on Itô's lemma. For a twice-differentiable test function $f(X)$ that does not have explicit time dependence, Itô's lemma states that the differential $df(X(t))$ is given by:\n$$\ndf(X(t)) = \\left( a(X,t) \\frac{df}{dX} + \\frac{1}{2} b(X,t)^2 \\frac{d^2f}{dX^2} \\right) dt + b(X,t) \\frac{df}{dX} dW_t\n$$\nTo find the ODEs for the moments, we will apply this lemma to specific choices of $f(X)$, and then take the expectation. A key property of the Itô integral is that for a suitable non-anticipating process $\\phi(t)$, the expectation $\\mathbb{E}\\left[\\int_0^t \\phi(s) dW_s\\right] = 0$.\n\n**1. Derivation of the ODE for the First Moment $\\mathbb{E}[X(t)]$**\n\nLet the test function be $f(X) = X$. The derivatives are:\n$$\n\\frac{df}{dX} = 1, \\quad \\frac{d^2f}{dX^2} = 0\n$$\nSubstituting these and the expressions for $a(X,t)$ and $b(X,t)^2$ into Itô's lemma yields:\n$$\ndX(t) = \\left( (k - \\gamma X(t)) \\cdot 1 + \\frac{1}{2} (k + \\gamma X(t)) \\cdot 0 \\right) dt + \\sqrt{k + \\gamma X(t)} \\cdot 1 \\cdot dW_t\n$$\n$$\ndX(t) = (k - \\gamma X(t)) dt + \\sqrt{k + \\gamma X(t)} dW_t\n$$\nThis recovers the original SDE. To obtain the dynamics of the mean, we take the expectation of this equation. In its integral form from $t=0$ to $t$:\n$$\n\\mathbb{E}[X(t) - X(0)] = \\mathbb{E}\\left[ \\int_0^t (k - \\gamma X(s)) ds \\right] + \\mathbb{E}\\left[ \\int_0^t \\sqrt{k + \\gamma X(s)} dW_s \\right]\n$$\nUsing the linearity of expectation and the zero-mean property of the Itô integral, we get:\n$$\n\\mathbb{E}[X(t)] - \\mathbb{E}[X(0)] = \\int_0^t \\mathbb{E}[k - \\gamma X(s)] ds = \\int_0^t (k - \\gamma \\mathbb{E}[X(s)]) ds\n$$\nDifferentiating both sides with respect to $t$ using the Fundamental Theorem of Calculus gives the ODE for the mean:\n$$\n\\frac{d}{dt}\\mathbb{E}[X(t)] = k - \\gamma \\mathbb{E}[X(t)]\n$$\n\n**2. Derivation of the ODE for the Variance $\\mathrm{Var}(X(t))$**\n\nThe variance is defined as $\\mathrm{Var}(X(t)) = \\mathbb{E}[X(t)^2] - (\\mathbb{E}[X(t)])^2$. To find its time evolution, we first need the ODE for the second moment, $\\mathbb{E}[X(t)^2]$.\n\nLet the test function be $f(X) = X^2$. The derivatives are:\n$$\n\\frac{df}{dX} = 2X, \\quad \\frac{d^2f}{dX^2} = 2\n$$\nSubstituting into Itô's lemma:\n$$\nd(X(t)^2) = \\left( (k - \\gamma X(t)) \\cdot (2X(t)) + \\frac{1}{2} (k + \\gamma X(t)) \\cdot 2 \\right) dt + \\sqrt{k + \\gamma X(t)} \\cdot (2X(t)) \\cdot dW_t\n$$\n$$\nd(X(t)^2) = (2kX(t) - 2\\gamma X(t)^2 + k + \\gamma X(t)) dt + 2X(t)\\sqrt{k + \\gamma X(t)} dW_t\n$$\nTaking the expectation of the integral form:\n$$\n\\mathbb{E}[X(t)^2 - X(0)^2] = \\mathbb{E}\\left[ \\int_0^t (2kX(s) - 2\\gamma X(s)^2 + k + \\gamma X(s)) ds \\right] + \\mathbb{E}\\left[ \\int_0^t 2X(s)\\sqrt{k + \\gamma X(s)} dW_s \\right]\n$$\nThe expectation of the stochastic integral is zero. Applying linearity of expectation:\n$$\n\\mathbb{E}[X(t)^2] - \\mathbb{E}[X(0)^2] = \\int_0^t ( (2k+\\gamma)\\mathbb{E}[X(s)] - 2\\gamma \\mathbb{E}[X(s)^2] + k ) ds\n$$\nDifferentiating with respect to $t$ gives the ODE for the second moment:\n$$\n\\frac{d}{dt}\\mathbb{E}[X(t)^2] = k + (2k+\\gamma)\\mathbb{E}[X(t)] - 2\\gamma \\mathbb{E}[X(t)^2]\n$$\nNow we can find the derivative of the variance, $\\mathrm{Var}(X(t))$, using the chain rule:\n$$\n\\frac{d}{dt}\\mathrm{Var}(X(t)) = \\frac{d}{dt}\\left( \\mathbb{E}[X(t)^2] - (\\mathbb{E}[X(t)])^2 \\right) = \\frac{d}{dt}\\mathbb{E}[X(t)^2] - 2\\mathbb{E}[X(t)] \\frac{d}{dt}\\mathbb{E}[X(t)]\n$$\nSubstitute the derived expressions for the derivatives of the first and second moments:\n$$\n\\frac{d}{dt}\\mathrm{Var}(X(t)) = \\left( k + (2k+\\gamma)\\mathbb{E}[X(t)] - 2\\gamma \\mathbb{E}[X(t)^2] \\right) - 2\\mathbb{E}[X(t)] \\left( k - \\gamma \\mathbb{E}[X(t)] \\right)\n$$\nExpand the terms:\n$$\n\\frac{d}{dt}\\mathrm{Var}(X(t)) = k + 2k\\mathbb{E}[X(t)] + \\gamma\\mathbb{E}[X(t)] - 2\\gamma \\mathbb{E}[X(t)^2] - 2k\\mathbb{E}[X(t)] + 2\\gamma (\\mathbb{E}[X(t)])^2\n$$\nThe terms $2k\\mathbb{E}[X(t)]$ cancel, leaving:\n$$\n\\frac{d}{dt}\\mathrm{Var}(X(t)) = k + \\gamma\\mathbb{E}[X(t)] - 2\\gamma \\mathbb{E}[X(t)^2] + 2\\gamma (\\mathbb{E}[X(t)])^2\n$$\nFactor out $-2\\gamma$ from the last two terms:\n$$\n\\frac{d}{dt}\\mathrm{Var}(X(t)) = k + \\gamma\\mathbb{E}[X(t)] - 2\\gamma \\left( \\mathbb{E}[X(t)^2] - (\\mathbb{E}[X(t)])^2 \\right)\n$$\nRecognizing that the term in the parentheses is the definition of variance, $\\mathrm{Var}(X(t))$, we arrive at the final ODE for the variance:\n$$\n\\frac{d}{dt}\\mathrm{Var}(X(t)) = k + \\gamma\\mathbb{E}[X(t)] - 2\\gamma \\mathrm{Var}(X(t))\n$$\nThe two resulting ODEs form a closed, coupled linear system for the mean and variance. The requested expressions for the derivatives are:\nFor the first moment: $k - \\gamma \\mathbb{E}[X(t)]$\nFor the variance: $k + \\gamma\\mathbb{E}[X(t)] - 2\\gamma \\mathrm{Var}(X(t))$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} k - \\gamma \\mathbb{E}[X(t)] & k + \\gamma \\mathbb{E}[X(t)] - 2\\gamma \\mathrm{Var}(X(t)) \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "When analytical solutions are out of reach, direct simulation becomes an indispensable tool. This exercise delves into the theoretical underpinnings of the Stochastic Simulation Algorithm (SSA), the standard method for generating exact trajectories of a Markov jump process . You will derive the probability distribution for the waiting time between successive reaction events, which is the core of the algorithm's logic, and analyze how this time step scales with system size, offering crucial insights into the connection between microscopic stochastic events and macroscopic deterministic behavior.",
            "id": "3932668",
            "problem": "Consider a well-mixed stochastic biochemical reaction network with $N$ molecular species and $M$ reactions inside a compartment of size (e.g., volume) $\\Omega>0$. Let the state at time $t$ be $\\boldsymbol{X}(t)=\\boldsymbol{x}\\in\\mathbb{N}^{N}$. For reaction $j\\in\\{1,\\dots,M\\}$, let $a_{j}(\\boldsymbol{x})$ denote its propensity at state $\\boldsymbol{x}$, and define the total propensity $a_{0}(\\boldsymbol{x})=\\sum_{j=1}^{M}a_{j}(\\boldsymbol{x})$. Assume the standard well-mixed continuous-time Markov jump process model: conditional on $\\boldsymbol{X}(t)=\\boldsymbol{x}$, over an infinitesimal interval $\\mathrm{d}t$, the probability that reaction $j$ fires in $[t,t+\\mathrm{d}t)$ is $a_{j}(\\boldsymbol{x})\\,\\mathrm{d}t+o(\\mathrm{d}t)$, and the probability of more than one firing is $o(\\mathrm{d}t)$. The Stochastic Simulation Algorithm (SSA) uses the inter-event waiting time $\\tau$ from the current state $\\boldsymbol{x}$ to the next reaction firing.\n\nTask 1. Starting from the definitions above and basic properties of survival and hazard for a continuous-time Markov jump process with state-dependent but piecewise-constant hazard between jumps, derive the probability density of the waiting time $\\tau$ to the next reaction firing given $\\boldsymbol{X}(t)=\\boldsymbol{x}$, and compute the expected value $\\mathbb{E}[\\tau\\mid \\boldsymbol{X}(t)=\\boldsymbol{x}]$ in closed form as a function of $a_{0}(\\boldsymbol{x})$.\n\nTask 2. Now impose the density-dependent scaling commonly used to connect mesoscopic stochastic kinetics to macroscopic concentrations: suppose there exist functions $\\lambda_{j}:\\mathbb{R}_{+}^{N}\\to\\mathbb{R}_{+}$, $j\\in\\{1,\\dots,M\\}$, such that for any state $\\boldsymbol{x}$ the propensities satisfy $a_{j}(\\boldsymbol{x})=\\Omega\\,\\lambda_{j}(\\boldsymbol{x}/\\Omega)$. Let the concentration vector be $\\boldsymbol{\\phi}=\\boldsymbol{x}/\\Omega$, treated as fixed as $\\Omega$ varies. Using this scaling, express $\\mathbb{E}[\\tau\\mid \\boldsymbol{\\phi},\\Omega]$ entirely in terms of $\\Omega$ and the functions $\\lambda_{j}(\\boldsymbol{\\phi})$. State explicitly how $\\mathbb{E}[\\tau\\mid \\boldsymbol{\\phi},\\Omega]$ depends on $\\Omega$.\n\nYour final answer must be a single closed-form analytic expression for $\\mathbb{E}[\\tau\\mid \\boldsymbol{\\phi},\\Omega]$ in terms of $\\Omega$ and $\\{\\lambda_{j}(\\boldsymbol{\\phi})\\}_{j=1}^{M}$. Do not include any units, and do not include any prose in the final answer. If you introduce any approximations, clearly justify them in your derivation in the solution. No numerical rounding is required for this problem.",
            "solution": "The problem is well-posed and scientifically sound, resting on the foundational principles of continuous-time Markov jump processes as applied to stochastic chemical kinetics. We will proceed with a rigorous derivation.\n\nThe problem is divided into two tasks. First, we derive the waiting time probability distribution and its expectation. Second, we apply a specific scaling law to analyze the system size dependency.\n\nTask 1: Derivation of the waiting time distribution and its expectation.\nLet the system be in a state $\\boldsymbol{X}(t)=\\boldsymbol{x}$ at time $t$. The state $\\boldsymbol{x}$ is an $N$-dimensional vector of non-negative integers, $\\boldsymbol{x} \\in \\mathbb{N}^{N}$. The system remains in this state until a reaction occurs. The time until the next reaction event is a random variable, which we denote by $\\tau$. We are asked to find the probability density function (PDF) of $\\tau$, conditioned on the current state $\\boldsymbol{x}$, and its expected value $\\mathbb{E}[\\tau \\mid \\boldsymbol{X}(t)=\\boldsymbol{x}]$.\n\nThe total rate at which any reaction can occur is the sum of the individual reaction propensities, $a_0(\\boldsymbol{x}) = \\sum_{j=1}^{M} a_j(\\boldsymbol{x})$. According to the problem statement, the probability of any single reaction occurring in an infinitesimal time interval $[t, t+\\mathrm{d}t)$ is $a_0(\\boldsymbol{x})\\mathrm{d}t + o(\\mathrm{d}t)$. Since the state $\\boldsymbol{x}$ is constant during the waiting period, the total propensity $a_0(\\boldsymbol{x})$ acts as a constant hazard rate.\n\nLet $S(\\tau' \\mid \\boldsymbol{x}) = P(\\tau > \\tau' \\mid \\boldsymbol{X}(t)=\\boldsymbol{x})$ be the survival function, which is the probability that no reaction has occurred by time $t+\\tau'$.\nThe probability of no event in the interval $[t, t+\\tau'+\\mathrm{d}t')$ can be expressed as the probability of no event in $[t, t+\\tau')$ multiplied by the probability of no event in the subsequent interval $[t+\\tau', t+\\tau'+\\mathrm{d}t')$. Due to the Markov property, these are independent.\nThe probability of no reaction in $[t+\\tau', t+\\tau'+\\mathrm{d}t')$ is $1 - a_0(\\boldsymbol{x})\\mathrm{d}t' + o(\\mathrm{d}t')$.\nThus, we can write:\n$$\nP(\\tau > \\tau'+\\mathrm{d}t' \\mid \\boldsymbol{x}) = P(\\tau > \\tau' \\mid \\boldsymbol{x}) \\times P(\\text{no event in } [t+\\tau', t+\\tau'+\\mathrm{d}t') \\mid \\boldsymbol{x})\n$$\nIn terms of the survival function:\n$$\nS(\\tau'+\\mathrm{d}t' \\mid \\boldsymbol{x}) = S(\\tau' \\mid \\boldsymbol{x}) (1 - a_0(\\boldsymbol{x})\\mathrm{d}t')\n$$\nRearranging this gives:\n$$\nS(\\tau'+\\mathrm{d}t' \\mid \\boldsymbol{x}) - S(\\tau' \\mid \\boldsymbol{x}) = -a_0(\\boldsymbol{x}) S(\\tau' \\mid \\boldsymbol{x}) \\mathrm{d}t'\n$$\nDividing by $\\mathrm{d}t'$ and taking the limit as $\\mathrm{d}t' \\to 0$ gives the differential equation for the survival function:\n$$\n\\frac{\\mathrm{d}S(\\tau' \\mid \\boldsymbol{x})}{\\mathrm{d}\\tau'} = -a_0(\\boldsymbol{x}) S(\\tau' \\mid \\boldsymbol{x})\n$$\nThis is a first-order linear ordinary differential equation. The initial condition is that the probability of surviving past time $0$ is $1$, i.e., $S(0 \\mid \\boldsymbol{x}) = 1$. The solution is:\n$$\nS(\\tau' \\mid \\boldsymbol{x}) = \\exp(-a_0(\\boldsymbol{x})\\tau')\n$$\nThe probability density function $p(\\tau' \\mid \\boldsymbol{x})$ is related to the survival function by $p(\\tau' \\mid \\boldsymbol{x}) = -\\frac{\\mathrm{d}S(\\tau' \\mid \\boldsymbol{x})}{\\mathrm{d}\\tau'}$. Differentiating the survival function, we obtain:\n$$\np(\\tau' \\mid \\boldsymbol{x}) = - \\frac{\\mathrm{d}}{\\mathrm{d}\\tau'} \\exp(-a_0(\\boldsymbol{x})\\tau') = -(-a_0(\\boldsymbol{x})) \\exp(-a_0(\\boldsymbol{x})\\tau') = a_0(\\boldsymbol{x}) \\exp(-a_0(\\boldsymbol{x})\\tau')\n$$\nThis is the PDF of an exponential distribution with rate parameter $a_0(\\boldsymbol{x})$.\n\nThe expected value of a random variable with this PDF is found by integration:\n$$\n\\mathbb{E}[\\tau \\mid \\boldsymbol{X}(t)=\\boldsymbol{x}] = \\int_0^\\infty \\tau' p(\\tau' \\mid \\boldsymbol{x}) \\mathrm{d}\\tau' = \\int_0^\\infty \\tau' a_0(\\boldsymbol{x}) \\exp(-a_0(\\boldsymbol{x})\\tau') \\mathrm{d}\\tau'\n$$\nThis is a standard integral. Using integration by parts with $u = \\tau'$ and $\\mathrm{d}v = a_0(\\boldsymbol{x}) \\exp(-a_0(\\boldsymbol{x})\\tau') \\mathrm{d}\\tau'$, we get $\\mathrm{d}u = \\mathrm{d}\\tau'$ and $v = -\\exp(-a_0(\\boldsymbol{x})\\tau')$.\n$$\n\\mathbb{E}[\\tau \\mid \\boldsymbol{X}(t)=\\boldsymbol{x}] = \\left[-\\tau' \\exp(-a_0(\\boldsymbol{x})\\tau')\\right]_0^\\infty - \\int_0^\\infty (-\\exp(-a_0(\\boldsymbol{x})\\tau')) \\mathrm{d}\\tau'\n$$\nThe first term evaluates to $0$ at both limits (since $\\lim_{\\tau'\\to\\infty} \\tau'\\exp(-c\\tau')=0$ for $c>0$). The integral becomes:\n$$\n\\mathbb{E}[\\tau \\mid \\boldsymbol{X}(t)=\\boldsymbol{x}] = \\int_0^\\infty \\exp(-a_0(\\boldsymbol{x})\\tau') \\mathrm{d}\\tau' = \\left[-\\frac{1}{a_0(\\boldsymbol{x})} \\exp(-a_0(\\boldsymbol{x})\\tau')\\right]_0^\\infty\n$$\n$$\n\\mathbb{E}[\\tau \\mid \\boldsymbol{X}(t)=\\boldsymbol{x}] = -\\frac{1}{a_0(\\boldsymbol{x})} (0 - 1) = \\frac{1}{a_0(\\boldsymbol{x})}\n$$\nThis completes Task 1. The expected waiting time to the next reaction is the reciprocal of the total propensity.\n\nTask 2: Application of density-dependent scaling.\nWe are given a scaling relationship for the propensities: $a_j(\\boldsymbol{x}) = \\Omega \\lambda_j(\\boldsymbol{x}/\\Omega)$, where $\\Omega$ is the system size and $\\boldsymbol{\\phi} = \\boldsymbol{x}/\\Omega$ is the concentration vector. We need to express a newly-defined conditional expectation $\\mathbb{E}[\\tau \\mid \\boldsymbol{\\phi}, \\Omega]$ in terms of $\\Omega$ and the functions $\\{\\lambda_j(\\boldsymbol{\\phi})\\}_{j=1}^M$.\n\nThe conditioning on $(\\boldsymbol{\\phi}, \\Omega)$ is equivalent to conditioning on $\\boldsymbol{x}$, since $\\boldsymbol{x} = \\Omega \\boldsymbol{\\phi}$. Thus, we can use the result from Task 1:\n$$\n\\mathbb{E}[\\tau \\mid \\boldsymbol{\\phi}, \\Omega] = \\mathbb{E}[\\tau \\mid \\boldsymbol{X}(t)=\\boldsymbol{x}] = \\frac{1}{a_0(\\boldsymbol{x})}\n$$\nNow, we express $a_0(\\boldsymbol{x})$ using the given scaling law. The total propensity is:\n$$\na_0(\\boldsymbol{x}) = \\sum_{j=1}^{M} a_j(\\boldsymbol{x})\n$$\nSubstituting the scaling relationship for each $a_j(\\boldsymbol{x})$:\n$$\na_0(\\boldsymbol{x}) = \\sum_{j=1}^{M} \\Omega \\lambda_j(\\boldsymbol{x}/\\Omega)\n$$\nWe can factor out the constant system size $\\Omega$:\n$$\na_0(\\boldsymbol{x}) = \\Omega \\sum_{j=1}^{M} \\lambda_j(\\boldsymbol{x}/\\Omega)\n$$\nSubstituting the definition of the concentration vector $\\boldsymbol{\\phi} = \\boldsymbol{x}/\\Omega$:\n$$\na_0(\\boldsymbol{x}) = \\Omega \\sum_{j=1}^{M} \\lambda_j(\\boldsymbol{\\phi})\n$$\nThe sum $\\sum_{j=1}^{M} \\lambda_j(\\boldsymbol{\\phi})$ represents the total macroscopic reaction rate, which we could denote as $\\lambda_0(\\boldsymbol{\\phi})$.\nNow we substitute this expression for $a_0(\\boldsymbol{x})$ back into the formula for the expected waiting time:\n$$\n\\mathbb{E}[\\tau \\mid \\boldsymbol{\\phi}, \\Omega] = \\frac{1}{\\Omega \\sum_{j=1}^{M} \\lambda_j(\\boldsymbol{\\phi})}\n$$\nThis is the final expression.\n\nFrom this result, we can explicitly state the dependency of the expected waiting time on the system size $\\Omega$. For a fixed concentration vector $\\boldsymbol{\\phi}$, the quantity $\\sum_{j=1}^{M} \\lambda_j(\\boldsymbol{\\phi})$ is a constant. Therefore, the expected waiting time is inversely proportional to the system size:\n$$\n\\mathbb{E}[\\tau \\mid \\boldsymbol{\\phi}, \\Omega] \\propto \\frac{1}{\\Omega}\n$$\nThis relationship is a key concept in connecting stochastic kinetics to deterministic rate equations. As the system size $\\Omega$ approaches infinity (the thermodynamic limit), the expected time between reaction events approaches $0$. This implies that reactions occur with increasing frequency, leading to a continuous, deterministic evolution of concentrations, which is described by ordinary differential equations.",
            "answer": "$$\n\\boxed{\\frac{1}{\\Omega \\sum_{j=1}^{M} \\lambda_{j}(\\boldsymbol{\\phi})}}\n$$"
        }
    ]
}