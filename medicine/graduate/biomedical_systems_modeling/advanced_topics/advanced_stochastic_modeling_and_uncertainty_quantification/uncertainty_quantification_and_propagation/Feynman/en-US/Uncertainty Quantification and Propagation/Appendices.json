{
    "hands_on_practices": [
        {
            "introduction": "Our exploration of hands-on practices begins with a cornerstone of Bayesian analysis: the use of conjugate priors. This exercise  demonstrates how to formally quantify and update uncertainty about the rate of a biological process modeled by a Poisson distribution. By deriving the posterior distribution analytically, you will gain a concrete understanding of how prior beliefs are mathematically combined with observed data to yield updated knowledge.",
            "id": "3941467",
            "problem": "A research team is modeling spontaneous exocytosis events in single pancreatic beta cells. During each observation window of length $t$ minutes, the number of exocytosis events is recorded. The team assumes a stationary Poisson process with unknown event rate $\\lambda$ (events per minute), so that the count $y_i$ observed in window $i$ is modeled as independent and identically distributed according to a Poisson law with mean $\\lambda t$. The team wishes to perform Bayesian uncertainty quantification for $\\lambda$ and uncertainty propagation to future counts using only fundamental principles: the definition of the Poisson likelihood and Bayes' theorem.\n\nYou are tasked with the following, starting from these base definitions:\n\n- Construct a conjugate prior family for the Poisson rate parameter $\\lambda$ appropriate for this setting and justify its choice in terms of conjugacy and interpretability of hyperparameters in the biomedical context.\n- Derive the posterior distribution of $\\lambda$ after observing $n$ independent windows with counts $\\{y_1,\\dots,y_n\\}$, each of duration $t$ minutes.\n- Derive the one-step-ahead posterior predictive distribution for the count in a future observation window of duration $t^{\\star}$ minutes, and express its probability mass function in closed form.\n- Explain qualitatively, in terms of effective prior information, how the posterior variance of $\\lambda$ depends on prior hyperparameters and the observed data.\n\nAnswer specification:\n- Your final answer must be the exact closed-form analytic expression for the posterior variance of $\\lambda$ given the $n$ observations, expressed in terms of the prior hyperparameters $a_0$ and $b_0$, the common window length $t$, the number of windows $n$, and the total count $\\sum_{i=1}^{n} y_i$. Do not substitute numerical values.\n- Do not include units inside the final answer box; express the variance symbolically.",
            "solution": "The problem statement is parsed and validated according to the specified protocol.\n\n### Step 1: Extract Givens\n-   Observation window length: $t$ minutes.\n-   Number of exocytosis events in window $i$: $y_i$.\n-   Model for counts: $y_i \\sim \\text{Poisson}(\\lambda t)$, independent and identically distributed for $i=1, \\dots, n$.\n-   Unknown parameter: $\\lambda$ (event rate in events per minute).\n-   Task 1: Construct a conjugate prior for $\\lambda$ and justify its choice.\n-   Task 2: Derive the posterior distribution of $\\lambda$ given observations $\\{y_1, \\dots, y_n\\}$.\n-   Task 3: Derive the one-step-ahead posterior predictive distribution for a count in a future window of duration $t^{\\star}$.\n-   Task 4: Qualitatively explain the dependence of the posterior variance of $\\lambda$ on prior hyperparameters and data.\n-   Final Answer Requirement: Provide the closed-form expression for the posterior variance of $\\lambda$, denoted $\\text{Var}(\\lambda | y_1, \\dots, y_n)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective.\n-   **Scientific Grounding**: Modeling event counts with a Poisson process is a standard and fundamental technique in many scientific fields, including biomedical research. The use of Bayesian inference with conjugate priors is a mathematically rigorous and widely accepted method for uncertainty quantification. The problem is based on sound statistical and mathematical principles.\n-   **Well-Posedness**: The problem is clearly specified. It provides a statistical model and asks for standard derivations within the Bayesian framework (prior specification, posterior derivation, predictive distribution derivation, and interpretation of posterior moments). These tasks lead to a unique and meaningful solution.\n-   **Objectivity**: The problem is stated in precise, formal language, free of ambiguity, subjectivity, or non-scientific claims.\n\nThe problem does not exhibit any of the flaws listed in the invalidation criteria. It is a standard, well-defined problem in Bayesian statistics applied to a biomedical context.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A full solution will be provided.\n\n### Solution Derivations\n\nThe solution proceeds by addressing the four tasks outlined in the problem statement.\n\n**Task 1: Conjugate Prior for $\\lambda$**\n\nFirst, we write down the likelihood function for the observed data. For a single observation window $i$, the probability mass function (PMF) of the count $y_i$ is given by the Poisson distribution:\n$$P(y_i|\\lambda) = \\frac{(\\lambda t)^{y_i} \\exp(-\\lambda t)}{y_i!}$$\nSince the $n$ observations $\\{y_1, \\dots, y_n\\}$ are independent and identically distributed, the total likelihood function $L(\\lambda | y_1, \\dots, y_n)$ is the product of the individual PMFs:\n$$L(\\lambda | y_1, \\dots, y_n) = \\prod_{i=1}^{n} P(y_i|\\lambda) = \\prod_{i=1}^{n} \\frac{(\\lambda t)^{y_i} \\exp(-\\lambda t)}{y_i!}$$\n$$= \\frac{(t)^{\\sum_{i=1}^{n} y_i}}{\\prod_{i=1}^{n} y_i!} (\\lambda)^{\\sum_{i=1}^{n} y_i} \\exp(-n \\lambda t)$$\nFor the purpose of finding a conjugate prior, we only need the kernel of the likelihood, which is the part that depends on $\\lambda$:\n$$L(\\lambda | y_1, \\dots, y_n) \\propto \\lambda^{\\sum_{i=1}^{n} y_i} \\exp(-n t \\lambda)$$\nThis kernel has the form $\\lambda^{k} \\exp(-c \\lambda)$, which is characteristic of the Gamma distribution. A probability distribution is a conjugate prior for a likelihood function if the resulting posterior distribution is in the same family as the prior. Therefore, the conjugate prior for the Poisson rate parameter $\\lambda$ is the Gamma distribution.\n\nWe define the prior for $\\lambda$ as a Gamma distribution with shape parameter $a_0$ and rate parameter $b_0$:\n$$p(\\lambda) = \\text{Gamma}(\\lambda | a_0, b_0) = \\frac{b_0^{a_0}}{\\Gamma(a_0)} \\lambda^{a_0-1} \\exp(-b_0 \\lambda)$$\nwhere $\\Gamma(\\cdot)$ is the gamma function.\n\n*   **Justification of Choice:**\n    1.  **Conjugacy**: As will be shown in the next step, using a Gamma prior for a Poisson rate parameter results in a Gamma posterior. This property, known as conjugacy, is computationally convenient as it yields a closed-form analytical expression for the posterior, avoiding the need for numerical integration methods like MCMC.\n    2.  **Interpretability**: The hyperparameters $a_0$ and $b_0$ have a clear interpretation in this biomedical context. They can be thought of as representing prior information equivalent to having observed $a_0$ events during a total observation time of $b_0$ minutes. The prior mean rate is $E[\\lambda] = a_0/b_0$, and the prior variance is $\\text{Var}(\\lambda) = a_0/b_0^2$. A large $b_0$ relative to $a_0$ represents strong prior belief (low variance) about the value of $\\lambda$. This allows researchers to formally incorporate knowledge from previous studies or established literature.\n\n**Task 2: Posterior Distribution of $\\lambda$**\n\nAccording to Bayes' theorem, the posterior distribution $p(\\lambda | y_1, \\dots, y_n)$ is proportional to the product of the likelihood and the prior:\n$$p(\\lambda | y_1, \\dots, y_n) \\propto L(\\lambda | y_1, \\dots, y_n) \\times p(\\lambda)$$\n$$p(\\lambda | y_1, \\dots, y_n) \\propto \\left( \\lambda^{\\sum_{i=1}^{n} y_i} \\exp(-n t \\lambda) \\right) \\times \\left( \\lambda^{a_0-1} \\exp(-b_0 \\lambda) \\right)$$\nCombining the terms involving $\\lambda$:\n$$p(\\lambda | y_1, \\dots, y_n) \\propto \\lambda^{(\\sum_{i=1}^{n} y_i) + a_0 - 1} \\exp(-(nt + b_0)\\lambda)$$\nThis is the kernel of a Gamma distribution. We can identify the updated (posterior) hyperparameters, which we will call $a_n$ and $b_n$:\n$$a_n = a_0 + \\sum_{i=1}^{n} y_i$$\n$$b_n = b_0 + nt$$\nThus, the posterior distribution for $\\lambda$ is a Gamma distribution:\n$$p(\\lambda | y_1, \\dots, y_n) = \\text{Gamma}(\\lambda | a_n, b_n) = \\text{Gamma}\\left(\\lambda \\bigg| a_0 + \\sum_{i=1}^{n} y_i, b_0 + nt\\right)$$\nThe posterior mean is $E[\\lambda | y_1, \\dots, y_n] = a_n/b_n$, and the posterior variance is $\\text{Var}(\\lambda | y_1, \\dots, y_n) = a_n/b_n^2$.\n\n**Task 3: Posterior Predictive Distribution**\n\nThe posterior predictive distribution gives the probability of a new observation, $y_{\\text{new}}$, given the previous data. The new observation is from a window of duration $t^{\\star}$, so its likelihood is $p(y_{\\text{new}}|\\lambda) = \\text{Poisson}(y_{\\text{new}}|\\lambda t^{\\star})$. We find the posterior predictive PMF by marginalizing the product of this likelihood and the posterior distribution of $\\lambda$ over all possible values of $\\lambda$:\n$$p(y_{\\text{new}}|y_1, \\dots, y_n) = \\int_{0}^{\\infty} p(y_{\\text{new}}|\\lambda) p(\\lambda|y_1, \\dots, y_n) d\\lambda$$\nSubstituting the respective functions:\n$$p(y_{\\text{new}}|y_1, \\dots, y_n) = \\int_{0}^{\\infty} \\left[ \\frac{(\\lambda t^{\\star})^{y_{\\text{new}}} \\exp(-\\lambda t^{\\star})}{y_{\\text{new}}!} \\right] \\left[ \\frac{b_n^{a_n}}{\\Gamma(a_n)} \\lambda^{a_n-1} \\exp(-b_n \\lambda) \\right] d\\lambda$$\nWe group terms that do not depend on $\\lambda$ outside the integral:\n$$= \\frac{(t^{\\star})^{y_{\\text{new}}} b_n^{a_n}}{y_{\\text{new}}! \\Gamma(a_n)} \\int_{0}^{\\infty} \\lambda^{y_{\\text{new}}} \\exp(-\\lambda t^{\\star}) \\lambda^{a_n-1} \\exp(-b_n \\lambda) d\\lambda$$\n$$= \\frac{(t^{\\star})^{y_{\\text{new}}} b_n^{a_n}}{y_{\\text{new}}! \\Gamma(a_n)} \\int_{0}^{\\infty} \\lambda^{a_n + y_{\\text{new}} - 1} \\exp(-(b_n + t^{\\star})\\lambda) d\\lambda$$\nThe integral is the kernel of a Gamma distribution, $\\text{Gamma}(a_n + y_{\\text{new}}, b_n + t^{\\star})$, integrated over its domain. The value of this integral is $\\frac{\\Gamma(a_n + y_{\\text{new}})}{(b_n + t^{\\star})^{a_n + y_{\\text{new}}}}$.\nSubstituting this back:\n$$p(y_{\\text{new}}|y_1, \\dots, y_n) = \\frac{(t^{\\star})^{y_{\\text{new}}} b_n^{a_n}}{y_{\\text{new}}! \\Gamma(a_n)} \\frac{\\Gamma(a_n + y_{\\text{new}})}{(b_n + t^{\\star})^{a_n + y_{\\text{new}}}}$$\nRearranging the terms:\n$$= \\frac{\\Gamma(a_n + y_{\\text{new}})}{\\Gamma(y_{\\text{new}}+1) \\Gamma(a_n)} \\left(\\frac{b_n}{b_n + t^{\\star}}\\right)^{a_n} \\left(\\frac{t^{\\star}}{b_n + t^{\\star}}\\right)^{y_{\\text{new}}}$$\nwhere we used $y_{\\text{new}}! = \\Gamma(y_{\\text{new}}+1)$. The term $\\frac{\\Gamma(a_n + y_{\\text{new}})}{\\Gamma(y_{\\text{new}}+1) \\Gamma(a_n)}$ is the definition of the binomial coefficient $\\binom{a_n+y_{\\text{new}}-1}{y_{\\text{new}}}$.\nThis is the PMF of a Negative Binomial distribution. Let $r = a_n$ and $p = \\frac{b_n}{b_n + t^{\\star}}$. The PMF is:\n$$p(y_{\\text{new}}|y_1, \\dots, y_n) = \\binom{y_{\\text{new}} + r - 1}{y_{\\text{new}}} p^r (1-p)^{y_{\\text{new}}}$$\nSo, the posterior predictive distribution for $y_{\\text{new}}$ is a Negative Binomial distribution, $y_{\\text{new}}|y_1, \\dots, y_n \\sim \\text{NB}(r, p)$, with parameters:\n$$r = a_n = a_0 + \\sum_{i=1}^{n} y_i$$\n$$p = \\frac{b_n}{b_n + t^{\\star}} = \\frac{b_0 + nt}{b_0 + nt + t^{\\star}}$$\n\n**Task 4: Qualitative Explanation of Posterior Variance**\n\nThe posterior variance of $\\lambda$ is given by the variance of the posterior distribution $\\text{Gamma}(\\lambda | a_n, b_n)$, which is:\n$$\\text{Var}(\\lambda | y_1, \\dots, y_n) = \\frac{a_n}{b_n^2} = \\frac{a_0 + \\sum_{i=1}^{n} y_i}{(b_0 + nt)^2}$$\n-   **Dependence on Prior Hyperparameters ($a_0$, $b_0$)**: The hyperparameters $a_0$ and $b_0$ represent the effective information from a prior experiment. An increase in $a_0$ (prior event count) increases the numerator, tending to increase variance. An increase in $b_0$ (prior observation time) increases the denominator quadratically, strongly decreasing the variance. Therefore, a more informative prior (larger $b_0$) leads to a smaller posterior variance, as the prior beliefs have more weight and constrain the possible values of $\\lambda$.\n-   **Dependence on Observed Data ($n$, $\\sum y_i$)**: The observed data contribute to the posterior through the number of windows, $n$, and the total count, $\\sum_{i=1}^{n} y_i$.\n    -   As the number of observations $n$ increases, the total data observation time $nt$ increases. This term appears in the denominator squared, as $(b_0 + nt)^2$. The total count $\\sum y_i$ is expected to grow linearly with $n$ (approximately as $n \\lambda t$). The denominator grows as $n^2$ while the numerator grows as $n$. The quadratic growth in the denominator dominates, causing the posterior variance to decrease as $n$ increases. This reflects the principle that more data leads to greater certainty (less variance) about the parameter.\n    -   For a fixed number of observations $n$, a larger total count $\\sum y_i$ implies a higher estimate for the rate $\\lambda$. This larger total count increases the numerator $a_n$, thus increasing the posterior variance. This might seem counterintuitive, but it is a property of the Gamma distribution where the variance ($a/b^2$) is proportional to the mean ($a/b$) divided by the rate parameter ($b$). For a fixed exposure time ($b_n$), a process with a higher mean rate naturally has a higher absolute uncertainty (variance).",
            "answer": "$$\\boxed{\\frac{a_0 + \\sum_{i=1}^{n} y_i}{(b_0 + nt)^2}}$$"
        },
        {
            "introduction": "Propagating uncertainty through a model requires careful attention to the relationship between variables, a concept this problem vividly illustrates . When the same uncertain parameter appears multiple times in an equation, naive propagation methods can produce misleadingly wide uncertainty bounds. This practice will challenge you to identify and correctly handle such dependencies, a crucial skill for avoiding spurious uncertainty and ensuring the integrity of your model's predictions.",
            "id": "3941594",
            "problem": "A single-subject linear pharmacokinetic model is used to quantify exposure following two separate intravenous bolus doses of the same drug. In linear pharmacokinetics, the area under the plasma concentration–time curve (AUC) for an intravenous bolus is given by $ \\mathrm{AUC} = \\mathrm{Dose}/\\mathrm{CL} $, where $ \\mathrm{CL} $ is the systemic clearance. Suppose the same subject receives two bolus doses, $ D_{1} = 100\\,\\mathrm{mg} $ and $ D_{2} = 150\\,\\mathrm{mg} $, separated by sufficient time so that the pharmacokinetics are independent across occasions. The subject’s true clearance $ \\mathrm{CL} $ is constant across both occasions but is epistemically uncertain to the modeler, known only to lie in the interval $ \\mathrm{CL} \\in [3.0,\\,5.0]\\,\\mathrm{L\\,h^{-1}} $.\n\nDefine the exposure ratio $ R $ as the ratio of the two AUCs for the two occasions:\n$$\nR = \\frac{\\mathrm{AUC}_{1}}{\\mathrm{AUC}_{2}} = \\frac{D_{1}/\\mathrm{CL}}{D_{2}/\\mathrm{CL}}.\n$$\n\nYou are asked to analyze uncertainty propagation in $ R $ under two treatments of the shared uncertain parameter $ \\mathrm{CL} $:\n- The correct dependent treatment, which acknowledges that the same $ \\mathrm{CL} $ appears in both numerator and denominator.\n- A naive interval-arithmetic treatment that ignores the dependency and treats the $ \\mathrm{CL} $ in the numerator and the $ \\mathrm{CL} $ in the denominator as if they were independent occurrences ranging over $ [3.0,\\,5.0]\\,\\mathrm{L\\,h^{-1}} $.\n\nFrom first principles, determine the exact image set of $ R $ under the dependent treatment, and the naive interval bound obtained by treating the two occurrences of $ \\mathrm{CL} $ independently. Then, compute the width of the naive interval bound for $ R $.\n\nExpress the final width as a pure number (dimensionless) and round your answer to four significant figures.",
            "solution": "The user has provided a problem concerning uncertainty propagation in a pharmacokinetic model. The first step is to validate the problem statement.\n\n### Step 1: Extract Givens\n- **Pharmacokinetic Model**: Single-subject, linear pharmacokinetics.\n- **Dose-AUC relationship**: $ \\mathrm{AUC} = \\mathrm{Dose}/\\mathrm{CL} $ for an intravenous bolus.\n- **Dose 1**: $ D_{1} = 100\\,\\mathrm{mg} $.\n- **Dose 2**: $ D_{2} = 150\\,\\mathrm{mg} $.\n- **Clearance ($ \\mathrm{CL} $)**: Constant for the subject, but epistemically uncertain.\n- **Uncertainty Interval for Clearance**: $ \\mathrm{CL} \\in [3.0,\\,5.0]\\,\\mathrm{L\\,h^{-1}} $.\n- **Definition of Exposure Ratio ($ R $)**:\n$$\nR = \\frac{\\mathrm{AUC}_{1}}{\\mathrm{AUC}_{2}} = \\frac{D_{1}/\\mathrm{CL}}{D_{2}/\\mathrm{CL}}\n$$\n- **Tasks**:\n  1. Determine the exact image set of $ R $ under the correct dependent treatment of $ \\mathrm{CL} $.\n  2. Determine the naive interval bound for $ R $ treating the two occurrences of $ \\mathrm{CL} $ as independent.\n  3. Compute the width of the naive interval bound for $ R $.\n  4. Express the final width as a pure number rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the validation criteria.\n- **Scientifically Grounded**: The problem uses a standard, valid model from pharmacokinetics ($ \\mathrm{AUC} = \\mathrm{Dose}/\\mathrm{CL} $). The concept of analyzing uncertainty propagation for epistemically uncertain parameters is a fundamental topic in biomedical systems modeling and engineering. The problem is scientifically sound.\n- **Well-Posed**: The problem is clearly defined with all necessary data ($ D_1 $, $ D_2 $, the interval for $ \\mathrm{CL} $) and a precise objective. The distinction between dependent and independent parameter treatment is a standard concept in uncertainty analysis, and the tasks lead to a unique, meaningful solution.\n- **Objective**: The problem is stated in precise, objective, and quantitative terms.\n- **Other Flaws**: The problem does not exhibit any of the listed flaws. It is not incomplete, contradictory, unrealistic, or ill-posed. It presents a conceptually important, non-trivial problem in uncertainty quantification.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A full solution will be provided.\n\n### Solution Derivation\nThe problem requires an analysis of the uncertainty in the exposure ratio $ R $ under two different assumptions about the shared parameter $ \\mathrm{CL} $.\n\n**1. Dependent Treatment (Correct Analysis)**\n\nThe exposure ratio $ R $ is defined as:\n$$\nR = \\frac{\\mathrm{AUC}_{1}}{\\mathrm{AUC}_{2}}\n$$\nSubstituting the formula for $ \\mathrm{AUC} $, we get:\n$$\nR = \\frac{D_{1}/\\mathrm{CL}}{D_{2}/\\mathrm{CL}}\n$$\nIn this treatment, we recognize that the parameter $ \\mathrm{CL} $ is the same physical quantity in both the numerator and the denominator. The subject's clearance does not change between the two administrations. Since the problem specifies that $ \\mathrm{CL} \\in [3.0, 5.0] $, $ \\mathrm{CL} $ is strictly positive, so we can simplify the expression by multiplication with $ \\frac{\\mathrm{CL}}{\\mathrm{CL}} = 1 $:\n$$\nR = \\left(\\frac{D_{1}}{\\mathrm{CL}}\\right) \\left(\\frac{\\mathrm{CL}}{D_{2}}\\right) = \\frac{D_{1}}{D_{2}}\n$$\nThe parameter $ \\mathrm{CL} $ algebraically cancels out. Therefore, the value of $ R $ is independent of the value of $ \\mathrm{CL} $. The uncertainty in $ \\mathrm{CL} $ does not propagate to $ R $. We can calculate the exact value of $ R $ using the given doses:\n$$\nR = \\frac{D_{1}}{D_{2}} = \\frac{100\\,\\mathrm{mg}}{150\\,\\mathrm{mg}} = \\frac{100}{150} = \\frac{2}{3}\n$$\nThe image set of $ R $ under the correct dependent treatment is a single point, $\\{ \\frac{2}{3} \\}$.\n\n**2. Naive Interval-Arithmetic Treatment (Incorrect Analysis)**\n\nThis approach incorrectly ignores the dependency of the sub-expressions on the same parameter $ \\mathrm{CL} $. It treats the $ \\mathrm{CL} $ in the numerator's expression and the $ \\mathrm{CL} $ in the denominator's expression as if they were two independent variables, let's call them $ \\mathrm{CL}_{num} $ and $ \\mathrm{CL}_{den} $, both varying over the interval $ [3.0,\\,5.0] $.\n\nThe expression for $ R $ under this naive assumption, denoted $ R_{\\text{naive}} $, becomes:\n$$\nR_{\\text{naive}} = \\frac{D_{1}/\\mathrm{CL}_{num}}{D_{2}/\\mathrm{CL}_{den}} = \\frac{D_{1}}{D_{2}} \\cdot \\frac{\\mathrm{CL}_{den}}{\\mathrm{CL}_{num}}\n$$\nTo find the interval bound for $ R_{\\text{naive}} $, we apply the rules of interval arithmetic. The interval for $ R_{\\text{naive}} $ is given by $ [\\min(R_{\\text{naive}}), \\max(R_{\\text{naive}})] $.\n\nThe term $ \\frac{D_{1}}{D_{2}} $ is a constant, $ \\frac{2}{3} $. We need to find the interval for the ratio $ \\frac{\\mathrm{CL}_{den}}{\\mathrm{CL}_{num}} $.\n\nThe minimum value of $ R_{\\text{naive}} $ occurs when the ratio $ \\frac{\\mathrm{CL}_{den}}{\\mathrm{CL}_{num}} $ is minimized. This happens when the numerator, $ \\mathrm{CL}_{den} $, is at its minimum value and the denominator, $ \\mathrm{CL}_{num} $, is at its maximum value.\n$$\n\\min(R_{\\text{naive}}) = \\frac{D_{1}}{D_{2}} \\cdot \\frac{\\min(\\mathrm{CL}_{den})}{\\max(\\mathrm{CL}_{num})} = \\frac{100}{150} \\cdot \\frac{3.0}{5.0} = \\frac{2}{3} \\cdot \\frac{3}{5} = \\frac{2}{5}\n$$\n\nThe maximum value of $ R_{\\text{naive}} $ occurs when the ratio $ \\frac{\\mathrm{CL}_{den}}{\\mathrm{CL}_{num}} $ is maximized. This happens when the numerator, $ \\mathrm{CL}_{den} $, is at its maximum value and the denominator, $ \\mathrm{CL}_{num} $, is at its minimum value.\n$$\n\\max(R_{\\text{naive}}) = \\frac{D_{1}}{D_{2}} \\cdot \\frac{\\max(\\mathrm{CL}_{den})}{\\min(\\mathrm{CL}_{num})} = \\frac{100}{150} \\cdot \\frac{5.0}{3.0} = \\frac{2}{3} \\cdot \\frac{5}{3} = \\frac{10}{9}\n$$\nThus, the naive interval bound for $ R $ is $ [\\frac{2}{5}, \\frac{10}{9}] $.\n\n**3. Compute the Width of the Naive Interval Bound**\n\nThe width of an interval $ [a, b] $ is given by $ b - a $. The width of the naive interval for $ R $ is:\n$$\n\\text{Width} = \\max(R_{\\text{naive}}) - \\min(R_{\\text{naive}}) = \\frac{10}{9} - \\frac{2}{5}\n$$\nTo subtract the fractions, we find a common denominator, which is $ 9 \\times 5 = 45 $.\n$$\n\\text{Width} = \\frac{10 \\cdot 5}{9 \\cdot 5} - \\frac{2 \\cdot 9}{5 \\cdot 9} = \\frac{50}{45} - \\frac{18}{45} = \\frac{50 - 18}{45} = \\frac{32}{45}\n$$\nThe problem asks for this result as a pure number rounded to four significant figures.\n$$\n\\text{Width} = \\frac{32}{45} \\approx 0.711111...\n$$\nRounding to four significant figures, we get $ 0.7111 $. This non-zero width represents the spurious uncertainty introduced by ignoring the dependency of the sub-expressions on the shared parameter $ \\mathrm{CL} $.",
            "answer": "$$\n\\boxed{0.7111}\n$$"
        },
        {
            "introduction": "While analytical methods are foundational, most complex biomedical models require computational approaches to explore the posterior landscape of uncertain parameters. This exercise  guides you through the derivation of the Metropolis-Hastings acceptance probability, a core component of many Markov Chain Monte Carlo (MCMC) algorithms. By building this essential rule from the first principle of detailed balance, you will gain a deep appreciation for the theoretical guarantees that allow MCMC methods to reliably sample from otherwise intractable posterior distributions.",
            "id": "3941544",
            "problem": "Consider a one-compartment intravenous bolus pharmacokinetic model used in biomedical systems modeling to describe plasma concentration following administration of a known dose. Let the model parameters be the clearance and volume, denoted by $CL$ and $V$, respectively. Define the transformed parameters $\\varphi = (\\eta, \\nu)$ with $\\eta = \\ln(CL)$ and $\\nu = \\ln(V)$ so that $CL = \\exp(\\eta)$ and $V = \\exp(\\nu)$. The concentration-time profile under the one-compartment model is given by\n$$\nC_{p}(t; CL, V) = \\frac{D}{V}\\exp\\!\\left(-\\frac{CL}{V}t\\right),\n$$\nwhere $D$ is the known administered dose.\n\nSuppose observations $\\{y_{i}\\}_{i=1}^{n}$ at times $\\{t_{i}\\}_{i=1}^{n}$ are collected with independent Gaussian measurement noise of known standard deviation $\\sigma$, so that the likelihood of the data given $\\varphi$ is\n$$\np\\!\\left(y \\mid \\varphi\\right) = \\prod_{i=1}^{n}\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\!\\left(-\\frac{\\left[y_{i} - \\frac{D}{\\exp(\\nu)}\\exp\\!\\left(-\\frac{\\exp(\\eta)}{\\exp(\\nu)}t_{i}\\right)\\right]^{2}}{2\\sigma^{2}}\\right).\n$$\nAssume a Gaussian prior on the transformed parameters, $\\varphi \\sim \\mathcal{N}(m, S)$, with mean vector $m \\in \\mathbb{R}^{2}$ and positive definite covariance matrix $S \\in \\mathbb{R}^{2\\times 2}$, so that\n$$\np\\!\\left(\\varphi\\right) = \\frac{1}{(2\\pi)^{1}\\sqrt{\\det(S)}}\\exp\\!\\left(-\\frac{1}{2}\\left(\\varphi - m\\right)^{\\top}S^{-1}\\left(\\varphi - m\\right)\\right).\n$$\n\nYou wish to construct a Markov Chain Monte Carlo (MCMC) algorithm targeting the posterior distribution $p(\\varphi \\mid y)$ up to proportionality via the Metropolis–Hastings (MH) method with a generic proposal density $q(\\varphi' \\mid \\varphi)$ that is not necessarily symmetric. Starting from the definitions of Bayes’ theorem, Markov chain transition kernels, and the detailed balance condition, derive the acceptance probability $\\alpha(\\varphi, \\varphi')$ for the Metropolis–Hastings algorithm that ensures the posterior distribution is a stationary distribution of the chain. In your derivation, use only foundational principles and do not assume any pre-specified acceptance formula.\n\nFinally, provide the closed-form symbolic expression for the Metropolis–Hastings acceptance probability $\\alpha(\\varphi, \\varphi')$ in terms of the data $\\{y_{i}, t_{i}\\}$, the dose $D$, the noise level $\\sigma$, the prior parameters $(m, S)$, the current state $\\varphi$, the proposed state $\\varphi'$, and the proposal densities $q(\\varphi' \\mid \\varphi)$ and $q(\\varphi \\mid \\varphi')$. The final answer must be a single analytic expression. No numerical approximation is required.",
            "solution": "The objective is to derive the acceptance probability, $\\alpha(\\varphi, \\varphi')$, for the Metropolis-Hastings (MH) algorithm in the context of a specific Bayesian pharmacokinetic model. The derivation will be founded upon the principle of detailed balance, which ensures that the constructed Markov chain converges to the desired target posterior distribution as its stationary distribution.\n\nLet the target distribution be the posterior distribution of the parameters $\\varphi$, denoted by $\\pi(\\varphi) = p(\\varphi \\mid y)$. According to Bayes' theorem, the posterior is proportional to the product of the likelihood and the prior:\n$$\n\\pi(\\varphi) = p(\\varphi \\mid y) = \\frac{p(y \\mid \\varphi) p(\\varphi)}{p(y)} \\propto p(y \\mid \\varphi) p(\\varphi)\n$$\nwhere $p(y)$ is the marginal likelihood, a constant with respect to $\\varphi$, which can be ignored when working with ratios of the target density.\n\nThe Metropolis-Hastings algorithm generates a sequence of parameter values $\\{\\varphi^{(0)}, \\varphi^{(1)}, \\varphi^{(2)}, \\dots\\}$ that form a Markov chain. The transition from the current state, $\\varphi$, to a new state, $\\varphi'$, is governed by a transition kernel $K(\\varphi' \\mid \\varphi)$. For the chain to have $\\pi(\\varphi)$ as its stationary distribution, a sufficient condition is the detailed balance or reversibility condition:\n$$\n\\pi(\\varphi) K(\\varphi' \\mid \\varphi) = \\pi(\\varphi') K(\\varphi \\mid \\varphi')\n$$\nThis equation states that the rate of flow from state $\\varphi$ to state $\\varphi'$ must equal the rate of flow from $\\varphi'$ to $\\varphi$ when the system is in equilibrium.\n\nThe Metropolis-Hastings transition kernel is defined by a two-step process. First, a candidate state $\\varphi'$ is proposed from a proposal distribution $q(\\varphi' \\mid \\varphi)$. Second, this proposed state is accepted with a probability $\\alpha(\\varphi, \\varphi')$. If the proposal is accepted, the next state in the chain is $\\varphi'$. If it is rejected, the chain remains at the current state, $\\varphi$. For distinct states $\\varphi' \\neq \\varphi$, the transition kernel is given by the probability of proposing $\\varphi'$ and then accepting it:\n$$\nK(\\varphi' \\mid \\varphi) = q(\\varphi' \\mid \\varphi) \\alpha(\\varphi, \\varphi')\n$$\nSubstituting this definition into the detailed balance equation, we have:\n$$\n\\pi(\\varphi) q(\\varphi' \\mid \\varphi) \\alpha(\\varphi, \\varphi') = \\pi(\\varphi') q(\\varphi \\mid \\varphi') \\alpha(\\varphi', \\varphi)\n$$\nThis equation must hold for any pair of states $(\\varphi, \\varphi')$. Rearranging this gives a constraint on the ratio of the acceptance probabilities:\n$$\n\\frac{\\alpha(\\varphi, \\varphi')}{\\alpha(\\varphi', \\varphi)} = \\frac{\\pi(\\varphi') q(\\varphi \\mid \\varphi')}{\\pi(\\varphi) q(\\varphi' \\mid \\varphi)}\n$$\nTo satisfy this condition while maximizing the acceptance rate (which generally improves the efficiency of the MCMC sampler), the standard choice for the acceptance probability is:\n$$\n\\alpha(\\varphi, \\varphi') = \\min\\left(1, \\frac{\\pi(\\varphi') q(\\varphi \\mid \\varphi')}{\\pi(\\varphi) q(\\varphi' \\mid \\varphi)}\\right)\n$$\nThis construction guarantees that the detailed balance condition is met.\n\nNow, we apply this general formula to the specific problem. The ratio of the target densities is given by:\n$$\n\\frac{\\pi(\\varphi')}{\\pi(\\varphi)} = \\frac{p(y \\mid \\varphi') p(\\varphi')}{p(y \\mid \\varphi) p(\\varphi)}\n$$\nWe must compute the ratios for the likelihood and the prior. Let the current state be $\\varphi = (\\eta, \\nu)$ and the proposed state be $\\varphi' = (\\eta', \\nu')$.\n\nThe likelihood function is $p(y \\mid \\varphi) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp(-\\frac{1}{2\\sigma^2}[y_{i} - C_{p}(t_{i}; \\varphi)]^{2})$, where $C_{p}(t_{i}; \\varphi) = \\frac{D}{\\exp(\\nu)}\\exp(-\\frac{\\exp(\\eta)}{\\exp(\\nu)}t_{i})$. The ratio of likelihoods is:\n$$\n\\frac{p(y \\mid \\varphi')}{p(y \\mid \\varphi)} = \\frac{\\prod_{i=1}^{n} \\exp\\left(-\\frac{1}{2\\sigma^{2}}\\left[y_{i} - C_{p}(t_{i}; \\varphi')\\right]^{2}\\right)}{\\prod_{i=1}^{n} \\exp\\left(-\\frac{1}{2\\sigma^{2}}\\left[y_{i} - C_{p}(t_{i}; \\varphi)\\right]^{2}\\right)}\n$$\n$$\n= \\exp\\left(-\\frac{1}{2\\sigma^{2}} \\sum_{i=1}^{n} \\left( \\left[y_{i} - C_{p}(t_{i}; \\varphi')\\right]^{2} - \\left[y_{i} - C_{p}(t_{i}; \\varphi)\\right]^{2} \\right)\\right)\n$$\nLet's define $C_{p,i}(\\varphi) = C_{p}(t_i; \\varphi)$ for notational brevity. The expression becomes:\n$$\n\\frac{p(y \\mid \\varphi')}{p(y \\mid \\varphi)} = \\exp\\left(\\frac{1}{2\\sigma^{2}} \\sum_{i=1}^{n} \\left( \\left[y_{i} - C_{p,i}(\\varphi)\\right]^{2} - \\left[y_{i} - C_{p,i}(\\varphi')\\right]^{2} \\right)\\right)\n$$\n\nThe prior density is $p(\\varphi) = \\frac{1}{(2\\pi)^{1}\\sqrt{\\det(S)}} \\exp(-\\frac{1}{2}(\\varphi - m)^{\\top}S^{-1}(\\varphi - m))$. The ratio of priors is:\n$$\n\\frac{p(\\varphi')}{p(\\varphi)} = \\frac{\\exp\\left(-\\frac{1}{2}(\\varphi' - m)^{\\top}S^{-1}(\\varphi' - m)\\right)}{\\exp\\left(-\\frac{1}{2}(\\varphi - m)^{\\top}S^{-1}(\\varphi - m)\\right)}\n$$\n$$\n= \\exp\\left(-\\frac{1}{2} \\left( (\\varphi' - m)^{\\top}S^{-1}(\\varphi' - m) - (\\varphi - m)^{\\top}S^{-1}(\\varphi - m) \\right)\\right)\n$$\n$$\n= \\exp\\left(\\frac{1}{2} \\left( (\\varphi - m)^{\\top}S^{-1}(\\varphi - m) - (\\varphi' - m)^{\\top}S^{-1}(\\varphi' - m) \\right)\\right)\n$$\n\nCombining these two ratios, we get:\n$$\n\\frac{\\pi(\\varphi')}{\\pi(\\varphi)} = \\exp\\left(\\frac{1}{2\\sigma^{2}} \\sum_{i=1}^{n} \\left( \\left[y_{i} - C_{p,i}(\\varphi)\\right]^{2} - \\left[y_{i} - C_{p,i}(\\varphi')\\right]^{2} \\right) + \\frac{1}{2} \\left( (\\varphi - m)^{\\top}S^{-1}(\\varphi - m) - (\\varphi' - m)^{\\top}S^{-1}(\\varphi' - m) \\right)\\right)\n$$\nSubstituting this back into the expression for the acceptance probability $\\alpha(\\varphi, \\varphi')$, we obtain the final closed-form expression. The concentration terms are explicitly:\n$C_{p,i}(\\varphi) = \\frac{D}{\\exp(\\nu)}\\exp(-\\exp(\\eta-\\nu)t_{i})$ and $C_{p,i}(\\varphi') = \\frac{D}{\\exp(\\nu')}\\exp(-\\exp(\\eta'-\\nu')t_{i})$.\n\nThe complete acceptance probability is:\n$$\n\\alpha(\\varphi, \\varphi') = \\min\\left(1, \\frac{q(\\varphi \\mid \\varphi')}{q(\\varphi' \\mid \\varphi)} \\exp\\left[ \\frac{1}{2\\sigma^{2}}\\sum_{i=1}^{n}\\left(\\left(y_i - C_{p,i}(\\varphi)\\right)^2 - \\left(y_i - C_{p,i}(\\varphi')\\right)^2\\right) + \\frac{1}{2}\\left((\\varphi - m)^{\\top}S^{-1}(\\varphi - m) - (\\varphi' - m)^{\\top}S^{-1}(\\varphi' - m)\\right) \\right] \\right)\n$$\nThis expression provides the probability of accepting a move from state $\\varphi = (\\eta, \\nu)$ to $\\varphi' = (\\eta', \\nu')$ in the Metropolis-Hastings algorithm, ensuring the generated chain properly samples from the target posterior distribution $p(\\varphi \\mid y)$.",
            "answer": "$$ \\boxed{ \\min\\left(1, \\frac{q(\\varphi \\mid \\varphi')}{q(\\varphi' \\mid \\varphi)} \\exp\\left[ \\sum_{i=1}^{n}\\frac{\\left(y_{i} - \\frac{D}{\\exp(\\nu)}\\exp(-\\frac{\\exp(\\eta)}{\\exp(\\nu)}t_{i})\\right)^{2} - \\left(y_{i} - \\frac{D}{\\exp(\\nu')}\\exp(-\\frac{\\exp(\\eta')}{\\exp(\\nu')}t_{i})\\right)^{2}}{2\\sigma^{2}} + \\frac{1}{2}\\left((\\varphi - m)^{\\top}S^{-1}(\\varphi - m) - (\\varphi' - m)^{\\top}S^{-1}(\\varphi' - m)\\right) \\right] \\right) } $$"
        }
    ]
}