{
    "hands_on_practices": [
        {
            "introduction": "Before we can effectively solve stiff systems, we must first learn to identify and quantify stiffness. This practice guides you through analyzing the classic Robertson problem, a canonical model for stiff chemical kinetics often seen in biomedical systems. By computing the eigenvalues of the system's Jacobian matrix, you will directly observe the wide separation of timescales that defines stiffness and calculate the restrictive stability limit this imposes on explicit methods like Forward Euler . This exercise provides a concrete, quantitative answer to the question, \"How stiff is this system?\"",
            "id": "3913778",
            "problem": "Consider the three-species Robertson kinetics model used in biomedical systems modeling to represent fast-slow biochemical reaction networks. Let the state vector be $\\mathbf{y} = [y_1, y_2, y_3]^\\top$, and let the reaction rates be governed by mass-action kinetics with rate constants $k_1$, $k_2$, and $k_3$. The governing ordinary differential equations (ODE) are\n$$\n\\frac{dy_1}{dt} = -k_1 y_1 + k_3 y_2 y_3,\\quad\n\\frac{dy_2}{dt} = k_1 y_1 - k_2 y_2^2 - k_3 y_2 y_3,\\quad\n\\frac{dy_3}{dt} = k_2 y_2^2.\n$$\nAt a fixed representative state $\\mathbf{y}$, assume a local linearization of the dynamics given by\n$$\n\\frac{d\\mathbf{y}}{dt} \\approx \\mathbf{J}(\\mathbf{y})\\,\\mathbf{y},\n$$\nwhere $\\mathbf{J}(\\mathbf{y})$ is the Jacobian matrix of partial derivatives of the vector field with respect to $\\mathbf{y}$. Derive and implement a method to compute the eigenvalues of $\\mathbf{J}(\\mathbf{y})$ and use them to estimate the maximum stable time step for the explicit Forward Euler method based on linear stability analysis.\n\nUse the following foundational bases:\n- Mass-action kinetics defining reaction rates as polynomial functions of species concentrations.\n- The Jacobian matrix definition $\\mathbf{J}_{ij} = \\frac{\\partial f_i}{\\partial y_j}$ for the vector field $\\mathbf{f}(\\mathbf{y})$.\n- Linear stability of the Forward Euler method applied to the linear test equation $\\frac{dz}{dt} = \\lambda z$, which requires $\\lvert 1 + h\\lambda \\rvert \\le 1$ for stability, where $h$ is the time step and $\\lambda$ is an eigenvalue of the Jacobian.\n\nTasks:\n1. Starting from the ODEs and mass-action kinetics, write out the Jacobian $\\mathbf{J}(\\mathbf{y})$ explicitly in terms of $k_1$, $k_2$, $k_3$, and the components of $\\mathbf{y}$. Do not use any shortcut formulas; derive each partial derivative from first principles.\n2. For a given state $\\mathbf{y}$ and parameters $(k_1, k_2, k_3)$, compute the eigenvalues $\\{\\lambda_i\\}$ of $\\mathbf{J}(\\mathbf{y})$.\n3. Using the Forward Euler absolute stability condition $\\lvert 1 + h\\lambda \\rvert \\le 1$, determine for each eigenvalue with negative real part the critical step size $h_i$ at the boundary of stability by solving the equality condition in $h \\ge 0$. Then select the global maximum stable time step $h_{\\max}$ that satisfies the condition for all eigenvalues simultaneously.\n4. Express all final step sizes in seconds.\n\nFor the stability boundary computation, you must solve the inequality $\\lvert 1 + h\\lambda \\rvert \\le 1$ exactly for $h$ by algebra, without assuming purely real eigenvalues. Your result must be universally applicable to any complex eigenvalue $\\lambda$ with negative real part.\n\nTest suite:\nUse the base parameter values $k_1 = 0.04\\,\\mathrm{s}^{-1}$, $k_2 = 3\\times 10^7\\,\\mathrm{s}^{-1}$ (per squared concentration unit), and $k_3 = 1\\times 10^4\\,\\mathrm{s}^{-1}$, unless otherwise specified for a test case. The states are chosen to respect $y_1 + y_2 + y_3 = 1$.\n\n- Test case $1$: $\\mathbf{y} = [1.0, 0.0, 0.0]$, $k_1 = 0.04$, $k_2 = 3\\times 10^7$, $k_3 = 1\\times 10^4$.\n- Test case $2$: $\\mathbf{y} = [0.999, 1\\times 10^{-4}, 9\\times 10^{-4}]$, $k_1 = 0.04$, $k_2 = 3\\times 10^7$, $k_3 = 1\\times 10^4$.\n- Test case $3$: $\\mathbf{y} = [0.9, 5\\times 10^{-3}, 9.5\\times 10^{-2}]$, $k_1 = 0.04$, $k_2 = 3\\times 10^7$, $k_3 = 1\\times 10^4$.\n- Test case $4$: $\\mathbf{y} = [0.5, 1\\times 10^{-2}, 4.9\\times 10^{-1}]$, $k_1 = 0.04$, $k_2 = 3\\times 10^7$, $k_3 = 1\\times 10^4$.\n- Test case $5$: $\\mathbf{y} = [0.9, 5\\times 10^{-3}, 9.5\\times 10^{-2}]$, $k_1 = 0.04$, $k_2 = 3\\times 10^6$, $k_3 = 1\\times 10^4$.\n- Test case $6$: $\\mathbf{y} = [0.999, 1\\times 10^{-4}, 9\\times 10^{-4}]$, $k_1 = 0.04$, $k_2 = 3\\times 10^7$, $k_3 = 1\\times 10^5$.\n\nYour program should compute, for each test case, the maximum Forward Euler time step $h_{\\max}$ in seconds based on the Jacobian eigenvalues at the specified state. The final output must be a single line containing the six results as a comma-separated list enclosed in square brackets, for example, $\\texttt{[h1,h2,h3,h4,h5,h6]}$, with each $h_i$ in seconds.",
            "solution": "The present problem requires an analysis of the numerical stability of the Forward Euler method when applied to the Robertson kinetics model, a canonical example of a stiff system in chemical and biomedical modeling. The primary tasks are to derive the Jacobian of the system, use its eigenvalues to determine the region of absolute stability for the Forward Euler method, and compute the maximum permissible time step $h_{\\max}$ for several test cases.\n\nThe governing system of ordinary differential equations (ODEs) is given by the vector field $\\mathbf{f}(\\mathbf{y}) = [f_1(\\mathbf{y}), f_2(\\mathbf{y}), f_3(\\mathbf{y})]^\\top$, where $\\mathbf{y} = [y_1, y_2, y_3]^\\top$ is the state vector of species concentrations. The components of the vector field are:\n$$\n\\frac{dy_1}{dt} = f_1(\\mathbf{y}) = -k_1 y_1 + k_3 y_2 y_3\n$$\n$$\n\\frac{dy_2}{dt} = f_2(\\mathbf{y}) = k_1 y_1 - k_2 y_2^2 - k_3 y_2 y_3\n$$\n$$\n\\frac{dy_3}{dt} = f_3(\\mathbf{y}) = k_2 y_2^2\n$$\n\nA fundamental property of this system is the conservation of total concentration. Summing the three equations yields:\n$$\n\\frac{d(y_1 + y_2 + y_3)}{dt} = (-k_1 y_1 + k_3 y_2 y_3) + (k_1 y_1 - k_2 y_2^2 - k_3 y_2 y_3) + (k_2 y_2^2) = 0\n$$\nThis implies that $y_1 + y_2 + y_3$ is a constant of motion. This property has a direct consequence on the system's Jacobian, as will be demonstrated.\n\n**Step 1: Derivation of the Jacobian Matrix**\n\nThe stability of a numerical method at a state $\\mathbf{y}$ is determined by the eigenvalues of the Jacobian matrix $\\mathbf{J}(\\mathbf{y})$ evaluated at that state. The Jacobian is defined by its elements $\\mathbf{J}_{ij} = \\frac{\\partial f_i}{\\partial y_j}$. We derive each element from first principles.\n\nThe partial derivatives for $f_1 = -k_1 y_1 + k_3 y_2 y_3$ are:\n- $\\mathbf{J}_{11} = \\frac{\\partial f_1}{\\partial y_1} = -k_1$\n- $\\mathbf{J}_{12} = \\frac{\\partial f_1}{\\partial y_2} = k_3 y_3$\n- $\\mathbf{J}_{13} = \\frac{\\partial f_1}{\\partial y_3} = k_3 y_2$\n\nThe partial derivatives for $f_2 = k_1 y_1 - k_2 y_2^2 - k_3 y_2 y_3$ are:\n- $\\mathbf{J}_{21} = \\frac{\\partial f_2}{\\partial y_1} = k_1$\n- $\\mathbf{J}_{22} = \\frac{\\partial f_2}{\\partial y_2} = -2k_2 y_2 - k_3 y_3$\n- $\\mathbf{J}_{23} = \\frac{\\partial f_2}{\\partial y_3} = -k_3 y_2$\n\nThe partial derivatives for $f_3 = k_2 y_2^2$ are:\n- $\\mathbf{J}_{31} = \\frac{\\partial f_3}{\\partial y_1} = 0$\n- $\\mathbf{J}_{32} = \\frac{\\partial f_3}{\\partial y_2} = 2k_2 y_2$\n- $\\mathbf{J}_{33} = \\frac{\\partial f_3}{\\partial y_3} = 0$\n\nAssembling these elements, the Jacobian matrix is:\n$$\n\\mathbf{J}(\\mathbf{y}) = \\begin{pmatrix}\n-k_1  k_3 y_3  k_3 y_2 \\\\\nk_1  -2k_2 y_2 - k_3 y_3  -k_3 y_2 \\\\\n0  2k_2 y_2  0\n\\end{pmatrix}\n$$\nThe conservation law $y_1+y_2+y_3=\\text{constant}$ implies that the vector $\\mathbf{v} = [1, 1, 1]^\\top$ is a left eigenvector of $\\mathbf{J}$ with an eigenvalue of $0$. This can be verified by summing the columns of $\\mathbf{J}$:\n$$\n\\sum_{i=1}^3 \\mathbf{J}_{i1} = -k_1 + k_1 + 0 = 0\n$$\n$$\n\\sum_{i=1}^3 \\mathbf{J}_{i2} = k_3 y_3 + (-2k_2 y_2 - k_3 y_3) + 2k_2 y_2 = 0\n$$\n$$\n\\sum_{i=1}^3 \\mathbf{J}_{i3} = k_3 y_2 - k_3 y_2 + 0 = 0\n$$\nSince each column sums to zero, it is evident that $[1, 1, 1]\\mathbf{J} = [0, 0, 0]$. This confirms that $\\lambda=0$ is always an eigenvalue of $\\mathbf{J}$.\n\n**Step 2: Derivation of the Maximum Stable Time Step**\n\nThe Forward Euler method applied to the linear test equation $\\frac{dz}{dt} = \\lambda z$ yields the recurrence $z_{n+1} = z_n + h\\lambda z_n = (1+h\\lambda)z_n$. For the method to be stable, the magnitude of the amplification factor must not exceed unity, i.e., $|1+h\\lambda| \\le 1$. We must find the maximum step size $h \\ge 0$ that satisfies this condition for all eigenvalues $\\lambda$ of the Jacobian $\\mathbf{J}$.\n\nLet an eigenvalue $\\lambda$ be a complex number, $\\lambda = a + ib$, where $a = \\operatorname{Re}(\\lambda)$ and $b = \\operatorname{Im}(\\lambda)$. For a dynamically stable mode, we must have $a  0$. The problem correctly specifies to consider only eigenvalues with negative real parts.\n\nThe stability condition is:\n$$\n|1 + h(a+ib)| \\le 1\n$$\n$$\n|(1+ha) + i(hb)| \\le 1\n$$\nSquaring the magnitude, we get:\n$$\n(1+ha)^2 + (hb)^2 \\le 1\n$$\n$$\n1 + 2ha + h^2 a^2 + h^2 b^2 \\le 1\n$$\n$$\n2ha + h^2(a^2 + b^2) \\le 0\n$$\nSince we are seeking a non-trivial step size $h  0$, we can divide by $h$:\n$$\n2a + h(a^2 + b^2) \\le 0\n$$\n$$\nh(a^2 + b^2) \\le -2a\n$$\nThe term $a^2 + b^2$ is the squared magnitude of the eigenvalue, $|\\lambda|^2$. Since $a0$, the right-hand side $-2a$ is positive. We can divide by $|\\lambda|^2$ (assuming $\\lambda \\ne 0$, which is assured since $a0$):\n$$\nh \\le \\frac{-2a}{a^2+b^2} = \\frac{-2\\operatorname{Re}(\\lambda)}{|\\lambda|^2}\n$$\nThis inequality provides the upper bound on the time step $h_i$ for each eigenvalue $\\lambda_i$ with a negative real part. To ensure stability for the entire system, the time step $h$ must satisfy this condition for all such eigenvalues simultaneously. Therefore, the global maximum stable time step $h_{\\max}$ is the minimum of these individual bounds:\n$$\nh_{\\max} = \\min_{i, \\text{ s.t. } \\operatorname{Re}(\\lambda_i)  0} \\left( \\frac{-2\\operatorname{Re}(\\lambda_i)}{|\\lambda_i|^2} \\right)\n$$\nThe eigenvalue $\\lambda = 0$ results in the condition $|1+0| \\le 1$, which is always true and thus imposes no constraint on $h$.\n\n**Step 3: Algorithmic Implementation**\n\nFor each test case, the solution proceeds as follows:\n1.  The numerical values for the state vector $\\mathbf{y} = [y_1, y_2, y_3]^\\top$ and the rate constants $\\mathbf{k} = [k_1, k_2, k_3]^\\top$ are substituted into the analytical expression of the Jacobian matrix $\\mathbf{J}(\\mathbf{y})$.\n2.  The eigenvalues $\\{\\lambda_i\\}$ of the resulting numerical matrix $\\mathbf{J}$ are computed.\n3.  For each eigenvalue $\\lambda_i$ with $\\operatorname{Re}(\\lambda_i)  0$, the critical time step $h_i = \\frac{-2\\operatorname{Re}(\\lambda_i)}{|\\lambda_i|^2}$ is calculated.\n4.  The maximum stable time step for the test case, $h_{\\max}$, is determined as the minimum of all such calculated $h_i$. If no eigenvalues have a negative real part, $h_{\\max}$ is considered unbounded (infinite).\n\nThis procedure is implemented in the provided Python code, using the `numpy` library for numerical linear algebra operations, specifically `numpy.linalg.eigvals` to compute the eigenvalues. The final results for all test cases are then formatted and printed as required.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the maximum stable Forward Euler time step for the Robertson\n    kinetics model for a series of test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (y_vector, k_vector)\n    # y_vector: [y1, y2, y3]\n    # k_vector: [k1, k2, k3]\n    test_cases = [\n        ([1.0, 0.0, 0.0], [0.04, 3e7, 1e4]),\n        ([0.999, 1.0e-4, 9.0e-4], [0.04, 3e7, 1e4]),\n        ([0.9, 5.0e-3, 9.5e-2], [0.04, 3e7, 1e4]),\n        ([0.5, 1.0e-2, 4.9e-1], [0.04, 3e7, 1e4]),\n        ([0.9, 5.0e-3, 9.5e-2], [0.04, 3e6, 1e4]),\n        ([0.999, 1.0e-4, 9.0e-4], [0.04, 3e7, 1e5]),\n    ]\n\n    results = []\n    for y_vec, k_vec in test_cases:\n        y1, y2, y3 = y_vec\n        k1, k2, k3 = k_vec\n\n        # Construct the Jacobian matrix J based on the derived formula.\n        # J_ij = d(f_i)/d(y_j)\n        # f1 = -k1*y1 + k3*y2*y3\n        # f2 = k1*y1 - k2*y2**2 - k3*y2*y3\n        # f3 = k2*y2**2\n        J = np.array([\n            [-k1, k3 * y3, k3 * y2],\n            [k1, -2.0 * k2 * y2 - k3 * y3, -k3 * y2],\n            [0.0, 2.0 * k2 * y2, 0.0]\n        ], dtype=float)\n\n        # Compute the eigenvalues of the Jacobian matrix.\n        eigenvalues = np.linalg.eigvals(J)\n\n        h_max = float('inf')\n        \n        # Iterate through eigenvalues to find the most restrictive stability limit.\n        for lam in eigenvalues:\n            real_part = lam.real\n            \n            # The stability condition is relevant only for eigenvalues with a\n            # negative real part, which correspond to stable modes.\n            if real_part  0:\n                # Calculate the squared magnitude of the eigenvalue.\n                mag_sq = lam.real**2 + lam.imag**2\n                \n                # Avoid division by zero, although this case is already handled by\n                # the real_part  0 check, as lambda=0 has a zero real part.\n                if mag_sq  1e-15: # A small tolerance for floating point safety.\n                    # Calculate the critical time step for this eigenvalue.\n                    # h_crit = -2 * Re(lambda) / |lambda|^2\n                    h_crit = -2.0 * real_part / mag_sq\n                    \n                    # The overall maximum step size is the minimum of all critical step sizes.\n                    if h_crit  h_max:\n                        h_max = h_crit\n                        \n        results.append(h_max)\n\n    # Format the final output as a comma-separated list in brackets.\n    # Scientific notation with 8 decimal places is used for precision.\n    print(f\"[{','.join(f'{r:.8e}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Having seen the severe limitations of explicit methods when faced with stiffness, the next logical step is to implement a solver that is not bound by them. This exercise challenges you to build a robust integrator for the Robertson system using the A-stable backward Euler method, a cornerstone of stiff integration . You will use Newton's method with an analytically derived Jacobian to solve the implicit algebraic equation at each time step, allowing you to take large steps and witness firsthand the stability and power of implicit schemes.",
            "id": "3913755",
            "problem": "You are to design and implement a complete, runnable program that numerically integrates a stiff chemical kinetics model arising in biomedical systems modeling using an implicit method. The model is the Robertson system, a canonical stiff system that represents a three-species reaction network under mass-action kinetics. The purpose is to demonstrate why numerical stiffness necessitates implicit methods and how an analytically computed Jacobian enables robust Newton iterations inside a backward Euler scheme. Your task must be framed purely in mathematical and algorithmic terms and must be solvable without domain-specific laboratory data.\n\nThe Robertson system, expressed in concentrations as functions of time, is given by the ordinary differential equation (ODE) system\n$$\n\\frac{d\\mathbf{y}}{dt} = \\mathbf{f}(\\mathbf{y}), \\quad \\mathbf{y}(t) = \\begin{bmatrix} x(t) \\\\ y(t) \\\\ z(t) \\end{bmatrix},\n$$\nwith the right-hand side defined by\n$$\nf_1(x,y,z) = -k_1 x + k_2 y z, \\quad\nf_2(x,y,z) = k_1 x - k_2 y z - k_3 y^2, \\quad\nf_3(x,y,z) = k_3 y^2,\n$$\nand initial condition\n$$\n\\mathbf{y}(0) = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}.\n$$\nUse the scientifically standard rate constants\n$$\nk_1 = 0.04 \\text{ s}^{-1}, \\quad k_2 = 10^4 \\text{ (mol/L)}^{-1}\\text{ s}^{-1}, \\quad k_3 = 3\\times 10^7 \\text{ (mol/L)}^{-1}\\text{ s}^{-1}.\n$$\nAll concentrations $x$, $y$, and $z$ are in $\\text{mol/L}$ and time $t$ is in $\\text{s}$. The system is stiff because $k_3$ is much larger than $k_1$ and $k_2$ in magnitude, making the component $y(t)$ evolve on a very fast time scale relative to $x(t)$ and $z(t)$.\n\nImplement two numerical integrators for this initial value problem:\n- An explicit Euler method (forward Euler) with fixed time step $h$:\n$$\n\\mathbf{y}_{n+1}^{\\mathrm{EE}} = \\mathbf{y}_n + h\\,\\mathbf{f}(\\mathbf{y}_n).\n$$\n- A backward Euler method (implicit Euler) with fixed time step $h$, solved by Newton's method at each time step:\n$$\n\\mathbf{y}_{n+1}^{\\mathrm{BE}} \\text{ satisfies } \\mathbf{y}_{n+1}^{\\mathrm{BE}} = \\mathbf{y}_n + h\\,\\mathbf{f}(\\mathbf{y}_{n+1}^{\\mathrm{BE}}).\n$$\nFormulate backward Euler as the root-finding problem for the residual\n$$\n\\mathbf{R}(\\mathbf{y}) = \\mathbf{y} - \\mathbf{y}_n - h\\,\\mathbf{f}(\\mathbf{y}),\n$$\nand solve $\\mathbf{R}(\\mathbf{y}) = \\mathbf{0}$ using Newton's method with backtracking line search. You must compute the Jacobian matrix of $\\mathbf{f}$ analytically and use it to construct the Newton Jacobian\n$$\n\\mathbf{J}_{\\mathbf{R}}(\\mathbf{y}) = \\mathbf{I} - h\\,\\mathbf{J}_{\\mathbf{f}}(\\mathbf{y}),\n$$\nwhere $\\mathbf{I}$ is the $3\\times 3$ identity matrix. Derive and implement the analytical Jacobian $\\mathbf{J}_{\\mathbf{f}}$:\n$$\n\\mathbf{J}_{\\mathbf{f}}(\\mathbf{y}) =\n\\begin{bmatrix}\n\\frac{\\partial f_1}{\\partial x}  \\frac{\\partial f_1}{\\partial y}  \\frac{\\partial f_1}{\\partial z} \\\\\n\\frac{\\partial f_2}{\\partial x}  \\frac{\\partial f_2}{\\partial y}  \\frac{\\partial f_2}{\\partial z} \\\\\n\\frac{\\partial f_3}{\\partial x}  \\frac{\\partial f_3}{\\partial y}  \\frac{\\partial f_3}{\\partial z}\n\\end{bmatrix} =\n\\begin{bmatrix}\n-k_1  k_2 z  k_2 y \\\\\nk_1  -k_2 z - 2 k_3 y  -k_2 y \\\\\n0  2 k_3 y  0\n\\end{bmatrix}.\n$$\n\nDefine \"stable integration\" for this problem as follows: an integration is considered numerically stable if all computed concentrations remain nonnegative at all time steps (i.e., $x_n \\ge 0$, $y_n \\ge 0$, $z_n \\ge 0$ for all $n$) and the Newton solver converges at every backward Euler step to a residual norm below a small tolerance. An explicit Euler run is considered to have \"failed\" if any concentration becomes negative at any time step or if any non-finite value (not a number or infinity) is produced.\n\nUse the following fixed-step test suite, with each case specified by end time $T$ and time step $h$:\n1. $T = 2\\ \\text{s}$, $h = 1\\ \\text{s}$.\n2. $T = 1\\ \\text{s}$, $h = 0.1\\ \\text{s}$.\n3. $T = 10^{-3}\\ \\text{s}$, $h = 10^{-5}\\ \\text{s}$.\n4. $T = 5\\ \\text{s}$, $h = 5\\ \\text{s}$.\n\nFor each test case, run both integrators from the common initial condition $\\mathbf{y}(0)$ to the specified final time $T$ using the specified step size $h$. For the backward Euler method, use Newton's method with an analytically computed Jacobian and a backtracking line search that enforces nonnegativity of concentrations at trial iterates. Use a convergence tolerance of $10^{-12}$ on the residual norm and a maximum of $50$ Newton iterations per step. If Newton's method fails to converge within the iteration limit or produces non-finite values at any step, deem the backward Euler integration \"unstable\" for that case.\n\nYour program should produce a single line of output containing a list of four integers, one per test case, in the order given above, with the following encoding:\n- Output $2$ if backward Euler is stable and explicit Euler fails for that case.\n- Output $1$ if both methods are stable for that case.\n- Output $0$ if backward Euler is unstable for that case (regardless of explicit Euler).\n\nThe final output must be printed exactly as a comma-separated list enclosed in square brackets, for example, $[2,2,1,2]$. All time quantities are in seconds, and all concentration quantities are in $\\text{mol/L}$. Angles are not involved in this problem. Percentages are not involved; if any fractional quantities are needed, express them as decimals.\n\nThe test suite is designed to cover the following aspects: a \"happy path\" case where small steps stabilize both methods, large-step cases that are expected to fail for explicit Euler due to stiffness but remain stable for backward Euler, and a boundary case of a single large implicit step requiring robust Newton convergence. The answer for each test case is an integer as defined above. Your final program must be self-contained, runnable as is, and must not require user input or external files. Ensure all mathematical operations and variables are represented appropriately and consistently.",
            "solution": "The problem requires the implementation and comparison of two numerical integration methods, explicit Euler and backward Euler, for solving the Robertson system, a canonical example of a stiff system of ordinary differential equations (ODEs) frequently encountered in biomedical chemical kinetics. The validation of the problem statement confirms its scientific soundness, completeness, and objectivity. All provided mathematical formulations, including the ODE system, its Jacobian, and the numerical methods, are standard and correct. We proceed with the solution.\n\nThe system of ODEs is given by $\\frac{d\\mathbf{y}}{dt} = \\mathbf{f}(\\mathbf{y})$, with $\\mathbf{y}(t) = [x(t), y(t), z(t)]^T$. The components of the vector function $\\mathbf{f}$ are:\n$$\n\\begin{aligned}\nf_1(x,y,z) = -k_1 x + k_2 y z \\\\\nf_2(x,y,z) = k_1 x - k_2 y z - k_3 y^2 \\\\\nf_3(x,y,z) = k_3 y^2\n\\end{aligned}\n$$\nThe system is subject to the initial condition $\\mathbf{y}(0) = [1, 0, 0]^T$ with rate constants $k_1 = 0.04$, $k_2 = 10^4$, and $k_3 = 3 \\times 10^7$. The large disparity in the magnitudes of these constants introduces stiffness, meaning there are vastly different time scales of evolution among the components, posing a significant challenge for numerical solvers.\n\n**Explicit Euler Method**\n\nThe explicit Euler (forward Euler) method is the simplest numerical scheme for ODEs. Given a state $\\mathbf{y}_n$ at time $t_n$, the state at the next time step $t_{n+1} = t_n + h$ is approximated by:\n$$\n\\mathbf{y}_{n+1} = \\mathbf{y}_n + h\\,\\mathbf{f}(\\mathbf{y}_n)\n$$\nThis method is computationally inexpensive as it only requires one evaluation of the function $\\mathbf{f}$ per step. However, its major drawback is conditional stability. For stiff systems, the stability of the explicit Euler method is constrained by the fastest dynamics, requiring the step size $h$ to be prohibitively small. Specifically, the product of $h$ and the magnitude of the largest eigenvalue of the Jacobian matrix $\\mathbf{J}_{\\mathbf{f}}$ must be within a small, stable region. For the Robertson system, this implies a very small $h$, making the method impractical for long-time integration. An integration is deemed to have failed if at any step a species concentration becomes negative or a non-finite value is computed, which is a common outcome when the stability condition is violated.\n\n**Backward Euler Method and Newton's Method**\n\nThe backward Euler (implicit Euler) method computes the next state $\\mathbf{y}_{n+1}$ by solving the implicit equation:\n$$\n\\mathbf{y}_{n+1} = \\mathbf{y}_n + h\\,\\mathbf{f}(\\mathbf{y}_{n+1})\n$$\nThis method is A-stable, meaning it is numerically stable for any step size $h  0$ when applied to linear stiff problems. This property makes it highly suitable for stiff systems. However, since $\\mathbf{y}_{n+1}$ appears on both sides of the equation, a system of (generally nonlinear) algebraic equations must be solved at each time step.\n\nTo solve for $\\mathbf{y}_{n+1}$, we reformulate the equation as a root-finding problem. Let $\\mathbf{y}$ be the unknown value of $\\mathbf{y}_{n+1}$. We seek the root of the residual function $\\mathbf{R}(\\mathbf{y})$:\n$$\n\\mathbf{R}(\\mathbf{y}) = \\mathbf{y} - \\mathbf{y}_n - h\\,\\mathbf{f}(\\mathbf{y}) = \\mathbf{0}\n$$\nThis root is found using Newton's method, an iterative procedure. Starting with an initial guess $\\mathbf{y}^{(0)}$ (typically $\\mathbf{y}_n$), successive approximations $\\mathbf{y}^{(k)}$ are computed as:\n$$\n\\mathbf{y}^{(k+1)} = \\mathbf{y}^{(k)} - [\\mathbf{J}_{\\mathbf{R}}(\\mathbf{y}^{(k)})]^{-1} \\mathbf{R}(\\mathbf{y}^{(k)})\n$$\nwhere $\\mathbf{J}_{\\mathbf{R}}$ is the Jacobian of the residual function $\\mathbf{R}$. The Jacobian is derived as:\n$$\n\\mathbf{J}_{\\mathbf{R}}(\\mathbf{y}) = \\frac{\\partial \\mathbf{R}}{\\partial \\mathbf{y}} = \\mathbf{I} - h\\,\\frac{\\partial \\mathbf{f}}{\\partial \\mathbf{y}} = \\mathbf{I} - h\\,\\mathbf{J}_{\\mathbf{f}}(\\mathbf{y})\n$$\nHere, $\\mathbf{I}$ is the $3 \\times 3$ identity matrix and $\\mathbf{J}_{\\mathbf{f}}$ is the analytically computed Jacobian of the ODE system, as provided in the problem statement. In each Newton iteration, we solve the linear system $\\mathbf{J}_{\\mathbf{R}}(\\mathbf{y}^{(k)}) \\Delta\\mathbf{y}^{(k)} = -\\mathbf{R}(\\mathbf{y}^{(k)})$ for the update step $\\Delta\\mathbf{y}^{(k)}$.\n\nTo ensure robust convergence, especially with a poor initial guess or large step size $h$, a backtracking line search is incorporated. After computing the Newton step $\\Delta\\mathbf{y}^{(k)}$, the next iterate is found as $\\mathbf{y}^{(k+1)} = \\mathbf{y}^{(k)} + \\alpha \\Delta\\mathbf{y}^{(k)}$, where the step length $\\alpha \\in (0, 1]$ is reduced from an initial value of $1$ until the trial iterate $\\mathbf{y}^{(k+1)}$ satisfies certain conditions. For this problem, the critical condition is the non-negativity of all concentrations, i.e., all components of $\\mathbf{y}^{(k+1)}$ must be greater than or equal to $0$. The Newton solver is considered to have failed if it does not converge to a residual norm below the tolerance of $10^{-12}$ within $50$ iterations, or if the line search fails to find a non-negative trial iterate.\n\n**Implementation and Test Case Analysis**\n\nThe implementation consists of separate functions for $\\mathbf{f}(\\mathbf{y})$, $\\mathbf{J}_{\\mathbf{f}}(\\mathbf{y})$, a Newton solver for the backward Euler step, and a main driver to run the integrations for each test case.\n\n1.  **Case 1: $T = 2\\ \\text{s}$, $h = 1\\ \\text{s}$**. The step size $h=1$ is far too large for the explicit Euler method to remain stable, as it severely violates the stability condition imposed by the stiff components. This will lead to oscillations and negative concentrations, causing the explicit method to fail. The backward Euler method, being A-stable, will handle this large step size, and its robust Newton solver is expected to converge. Thus, the expected outcome is $2$.\n2.  **Case 2: $T = 1\\ \\text{s}$, $h = 0.1\\ \\text{s}$**. Similar to the first case, $h=0.1$ is still much larger than the stability limit for explicit Euler, leading to failure. Backward Euler should remain stable. Expected outcome: $2$.\n3.  **Case 3: $T = 10^{-3}\\ \\text{s}$, $h = 10^{-5}\\ \\text{s}$**. Here, the step size $h=10^{-5}$ is very small and falls within the stability region of the explicit Euler method. Therefore, both explicit and backward Euler methods are expected to complete the integration successfully, resulting in non-negative concentrations throughout. Expected outcome: $1$.\n4.  **Case 4: $T = 5\\ \\text{s}$, $h = 5\\ \\text{s}$**. This involves a single, extremely large step. For explicit Euler, the update $\\mathbf{y}_1 = \\mathbf{y}_0 + h\\,\\mathbf{f}(\\mathbf{y}_0)$ yields $[0.8, 0.2, 0]^T$, which is non-negative. According to the problem's strict definition of stability (checking only the computed non-negative values at time steps $t_0, t_1$), this integration is considered \"stable\", despite the result being physically inaccurate. For backward Euler, this single large step presents a significant challenge to the Newton solver, but it is designed to converge under such conditions. Therefore, both methods are deemed stable by the problem's criteria. Expected outcome: $1$.\n\nThe program calculates these outcomes and formats them as a list of integers.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# language: Python\n# version: 3.12\n# libraries:\n#     name: numpy, version: 1.23.5\n\ndef solve():\n    \"\"\"\n    Solves the Robertson stiff ODE problem by comparing Explicit and\n    Backward Euler methods over a suite of test cases.\n    \"\"\"\n\n    # --- Problem Constants ---\n    K1 = 0.04\n    K2 = 1.0e4\n    K3 = 3.0e7\n\n    # --- ODE System Definition ---\n\n    def robertson_f(y, k1, k2, k3):\n        \"\"\"Computes the right-hand side of the Robertson ODE system.\"\"\"\n        x, y_comp, z = y\n        dydt = np.zeros(3, dtype=np.float64)\n        dydt[0] = -k1 * x + k2 * y_comp * z\n        dydt[1] = k1 * x - k2 * y_comp * z - k3 * y_comp**2\n        dydt[2] = k3 * y_comp**2\n        return dydt\n\n    def robertson_Jf(y, k1, k2, k3):\n        \"\"\"Computes the analytical Jacobian of the Robertson ODE system.\"\"\"\n        x, y_comp, z = y\n        Jf = np.zeros((3, 3), dtype=np.float64)\n        # df1/dx, df1/dy, df1/dz\n        Jf[0, 0] = -k1\n        Jf[0, 1] = k2 * z\n        Jf[0, 2] = k2 * y_comp\n        # df2/dx, df2/dy, df2/dz\n        Jf[1, 0] = k1\n        Jf[1, 1] = -k2 * z - 2 * k3 * y_comp\n        Jf[1, 2] = -k2 * y_comp\n        # df3/dx, df3/dy, df3/dz\n        Jf[2, 0] = 0.0\n        Jf[2, 1] = 2 * k3 * y_comp\n        Jf[2, 2] = 0.0\n        return Jf\n\n    # --- Backward Euler Newton Solver ---\n\n    def solve_be_step(y_n, h, k1, k2, k3, tol=1e-12, max_iter=50):\n        \"\"\"\n        Solves for y_{n+1} in a single backward Euler step using Newton's method\n        with backtracking line search.\n        Finds the root of R(y) = y - y_n - h * f(y) = 0.\n        \"\"\"\n        y_k = np.copy(y_n)\n        \n        for _ in range(max_iter):\n            f_yk = robertson_f(y_k, k1, k2, k3)\n            R = y_k - y_n - h * f_yk\n            \n            if not np.all(np.isfinite(R)):\n                return None, False\n\n            if np.linalg.norm(R)  tol:\n                return y_k, True\n\n            Jf_yk = robertson_Jf(y_k, k1, k2, k3)\n            JR = np.eye(3) - h * Jf_yk\n            \n            try:\n                delta_y = np.linalg.solve(JR, -R)\n            except np.linalg.LinAlgError:\n                return None, False\n\n            if not np.all(np.isfinite(delta_y)):\n                return None, False\n\n            alpha = 1.0\n            line_search_success = False\n            for _ in range(10):  # Max 10 backtracking steps\n                y_trial = y_k + alpha * delta_y\n                if np.all(y_trial = 0):\n                    y_k = y_trial\n                    line_search_success = True\n                    break\n                alpha /= 2.0\n            \n            if not line_search_success:\n                return None, False\n        \n        return None, False\n\n    # --- Integrator Driver ---\n\n    def run_integration(integrator_type, y0, T, h, k1, k2, k3):\n        \"\"\"\n        Integrates the ODE system and returns True for stable integration, False for failure.\n        \"\"\"\n        y = np.copy(y0)\n        num_steps = int(round(T / h))\n\n        if num_steps == 0:\n            return True\n\n        for _ in range(num_steps):\n            if integrator_type == 'ee':\n                if not np.all(np.isfinite(y)): return False\n                dydt = robertson_f(y, k1, k2, k3)\n                if not np.all(np.isfinite(dydt)): return False\n                y_next = y + h * dydt\n                if np.any(y_next  0) or not np.all(np.isfinite(y_next)):\n                    return False\n                y = y_next\n            \n            elif integrator_type == 'be':\n                y_next, converged = solve_be_step(y, h, k1, k2, k3)\n                if not converged:\n                    return False\n                y = y_next\n        \n        return True\n\n    # --- Main Execution Logic ---\n    \n    y0 = np.array([1.0, 0.0, 0.0], dtype=np.float64)\n    \n    test_cases = [\n        (2.0, 1.0),\n        (1.0, 0.1),\n        (1.0e-3, 1.0e-5),\n        (5.0, 5.0)\n    ]\n    \n    results = []\n    for T, h in test_cases:\n        ee_stable = run_integration('ee', y0, T, h, K1, K2, K3)\n        be_stable = run_integration('be', y0, T, h, K1, K2, K3)\n        \n        if not be_stable:\n            result_code = 0\n        elif be_stable and not ee_stable:\n            result_code = 2\n        elif be_stable and ee_stable:\n            result_code = 1\n        else: # Should not be reached\n            result_code = -1 \n            \n        results.append(result_code)\n            \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While the backward Euler method is robustly stable, higher-order methods are often required to achieve a desired accuracy efficiently. This advanced practice moves beyond first-order methods to the construction of the two-stage Radau IIA method, a powerful, third-order implicit Runge-Kutta (IRK) scheme widely used in professional software . You will construct the method from its foundational principles and develop a Newton solver for its coupled stage equations, gaining crucial insight into the structure and implementation of modern, high-performance stiff ODE solvers.",
            "id": "3913789",
            "problem": "Consider a two-species enzyme-catalyzed biochemical reaction network under mass-action kinetics, reduced by conservation of the free enzyme. The species are substrate $S$ with concentration $[S]$, and enzyme-substrate complex $C$ with concentration $[C]$. The free enzyme concentration is $[E] = E_{\\text{tot}} - [C]$, where $E_{\\text{tot}}$ is the conserved total enzyme concentration. The reactions are binding $S + E \\xrightarrow{k_1} C$, unbinding $C \\xrightarrow{k_{-1}} S + E$, and catalysis $C \\xrightarrow{k_2} P + E$, where $P$ is the product. The governing Ordinary Differential Equation (ODE) system is\n$$\n\\frac{d}{dt}\n\\begin{bmatrix}\n[S]\\\\\n[C]\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n- k_1 [S][E] + k_{-1}[C]\\\\\n\\phantom{-} k_1 [S][E] - (k_{-1} + k_2)[C]\n\\end{bmatrix}, \\quad [E] = E_{\\text{tot}} - [C].\n$$\nAssume parameters $k_1 = 10^{6}\\ \\text{M}^{-1}\\ \\text{s}^{-1}$, $k_{-1} = 10^{3}\\ \\text{s}^{-1}$, $k_2 = 10^{2}\\ \\text{s}^{-1}$, $E_{\\text{tot}} = 10^{-6}\\ \\text{M}$, and initial conditions at time $t_n = 0\\ \\text{s}$ given by $[S](0) = 10^{-4}\\ \\text{M}$, $[C](0) = 0\\ \\text{M}$. Treat the system with the two-stage Radau Implicit Runge–Kutta method of type IIA, and analyze numerical stiffness and nonlinear solve strategies.\n\nYour tasks are:\n\n1. From the initial value problem definition $\\frac{d\\mathbf{y}}{dt} = \\mathbf{f}(\\mathbf{y})$ with $\\mathbf{y} = \\begin{bmatrix}[S]\\\\[C]\\end{bmatrix}^{\\top}$ and the core definition of a $s$-stage implicit Runge–Kutta method, construct the two-stage Radau IIA method by identifying the node locations $c_i$, the coefficients $a_{ij}$, and the weights $b_i$ that satisfy the defining properties of the Radau IIA family (including stiff accuracy and order conditions up to order $2s-1$). Do not use any shortcut formulas not derived from the defining properties.\n\n2. Using your constructed coefficients, write down the coupled nonlinear stage equations for the stage values $\\mathbf{Y}_1$ and $\\mathbf{Y}_2$ at time levels $t_n + c_1 h$ and $t_n + c_2 h$, where $h$ is the time step in seconds. Express the stage equations in the residual form $\\mathbf{F}(\\mathbf{Z}) = \\mathbf{0}$, where $\\mathbf{Z} = \\begin{bmatrix}\\mathbf{Y}_1 \\\\ \\mathbf{Y}_2\\end{bmatrix} \\in \\mathbb{R}^{4}$ stacks the two stage vectors.\n\n3. Starting from the fundamental definition of Newton’s method for systems, derive a Newton iteration to solve the stage equations, including the explicit $4 \\times 4$ block Jacobian structure in terms of the Jacobian of $\\mathbf{f}$ evaluated at the current stage iterates. Clearly indicate the role of the identity matrix and the coupling through the Runge–Kutta coefficients. Your derivation should begin with the definitions of residual and Jacobian for a nonlinear system and proceed to the linear system to be solved at each Newton iteration. State a convergence criterion based on the Euclidean norm of the residual and a maximum allowable iteration count. If a step-length strategy is needed, specify a standard backtracking rule and its parameters based on first principles.\n\n4. Implement a single-step advance from $t_n$ to $t_{n+1} = t_n + h$ using the two-stage Radau IIA method and your Newton solver for the stage equations. Use the following test suite of time steps $h$ in seconds to examine behavior across regimes:\n- Happy path: $h = 10^{-6}\\ \\text{s}$.\n- Moderately stiff: $h = 10^{-3}\\ \\text{s}$.\n- Strongly stiff: $h = 10^{-1}\\ \\text{s}$.\n- Extremely large step: $h = 10^{1}\\ \\text{s}$.\n\nFor each $h$, start the Newton iteration with the initial guess $\\mathbf{Y}_1^{(0)} = \\mathbf{y}_n$ and $\\mathbf{Y}_2^{(0)} = \\mathbf{y}_n$. Use a convergence tolerance requiring $\\lVert \\mathbf{F}(\\mathbf{Z}^{(k)}) \\rVert_2 \\le 10^{-12}$ and at most $50$ Newton iterations. If backtracking is used, employ a sufficient-decrease condition on the squared residual norm with a standard Armijo-type parameter choice.\n\nYour program must compute, for each $h$ in the test suite, the number of Newton iterations required to solve the stage equations to the stated tolerance. If the solver fails to converge within the maximum iteration count, report the integer $-1$ for that case. No other quantities are required to be reported. The output is a single line containing the results as a comma-separated list enclosed in square brackets, for example $[n_1,n_2,n_3,n_4]$, where each $n_i$ is an integer Newton iteration count for the corresponding $h$.\n\nAll physical quantities in your derivation must respect their units as given. Angles do not appear. Percentages do not appear. The program must be completely self-contained and must not require any user input.",
            "solution": "The problem statement has been validated and is deemed scientifically grounded, well-posed, objective, and complete. It presents a standard but non-trivial exercise in the numerical solution of stiff ordinary differential equations (ODEs), which is highly relevant to the modeling of biomedical systems. The tasks are logically structured and all required data are provided. We may therefore proceed with a full solution.\n\nThe problem requires a single-step integration of a stiff biochemical ODE system from time $t_n=0$ to $t_{n+1} = t_n+h$ using a specific implicit Runge-Kutta (IRK) method. This involves three primary components, which will be addressed in order: the derivation of the IRK method's coefficients from first principles, the formulation of the resulting nonlinear system of equations for the internal stage values, and the derivation and implementation of a Newton-Raphson solver to solve this system.\n\nLet the state vector be $\\mathbf{y}(t) = \\begin{bmatrix} [S](t) \\\\ [C](t) \\end{bmatrix} \\in \\mathbb{R}^2$. The governing ODE system is $\\frac{d\\mathbf{y}}{dt} = \\mathbf{f}(\\mathbf{y})$, where the function $\\mathbf{f}: \\mathbb{R}^2 \\to \\mathbb{R}^2$ is given by\n$$\n\\mathbf{f}(\\mathbf{y}) = \\begin{bmatrix} -k_1 y_1 (E_{\\text{tot}} - y_2) + k_{-1} y_2 \\\\ k_1 y_1 (E_{\\text{tot}} - y_2) - (k_{-1} + k_2) y_2 \\end{bmatrix},\n$$\nwith $y_1 = [S]$ and $y_2 = [C]$. The given parameters are $k_1 = 10^6 \\ \\text{M}^{-1}\\text{s}^{-1}$, $k_{-1} = 10^3 \\ \\text{s}^{-1}$, $k_2 = 10^2 \\ \\text{s}^{-1}$, and $E_{\\text{tot}} = 10^{-6} \\ \\text{M}$. The initial condition at $t_n=0 \\ \\text{s}$ is $\\mathbf{y}_n = \\mathbf{y}(0) = \\begin{bmatrix} 10^{-4} \\\\ 0 \\end{bmatrix} \\ \\text{M}$.\n\n**1. Construction of the Two-Stage Radau IIA Method**\n\nAn $s$-stage implicit Runge-Kutta method is defined by its Butcher tableau $(\\mathbf{c}, \\mathbf{A}, \\mathbf{b})$. The stage values $\\mathbf{Y}_i \\in \\mathbb{R}^2$ are approximations to the solution at intermediate times $t_n + c_i h$, and are defined by the system of equations:\n$$\n\\mathbf{Y}_i = \\mathbf{y}_n + h \\sum_{j=1}^s a_{ij} \\mathbf{f}(\\mathbf{Y}_j), \\quad \\text{for } i=1, \\dots, s.\n$$\nThe solution at the next time step is given by:\n$$\n\\mathbf{y}_{n+1} = \\mathbf{y}_n + h \\sum_{i=1}^s b_i \\mathbf{f}(\\mathbf{Y}_i).\n$$\nThe Radau IIA methods are a specific class of collocation methods. For $s=2$, the defining properties are:\n1.  The method is a collocation method, where the nodes $c_i$ are the roots of the polynomial derived from shifted Legendre polynomials. For Radau IIA, the nodes are the roots of the polynomial $P_s'(x)$ where $P_s(x) = x^{s-1}(x-1)^s$. For $s=2$, this is $\\frac{d}{dx}[x(x-1)^2] = (x-1)^2 + 2x(x-1) = (x-1)(x-1+2x) = (x-1)(3x-1)$. The roots, which define the nodes, are $c_1=1/3$ and $c_2=1$.\n2.  The coefficients $a_{ij}$ are given by $a_{ij} = \\int_0^{c_i} L_j(\\tau) d\\tau$, where $L_j(\\tau)$ are the Lagrange basis polynomials for the nodes $\\{c_1, c_2\\}$.\n    The Lagrange polynomials are:\n    $L_1(\\tau) = \\frac{\\tau - c_2}{c_1 - c_2} = \\frac{\\tau - 1}{1/3 - 1} = -\\frac{3}{2}(\\tau-1)$.\n    $L_2(\\tau) = \\frac{\\tau - c_1}{c_2 - c_1} = \\frac{\\tau - 1/3}{1 - 1/3} = \\frac{3}{2}(\\tau-1/3)$.\n    The coefficients $a_{ij}$ are calculated by integration:\n    $a_{11} = \\int_0^{1/3} L_1(\\tau) d\\tau = \\int_0^{1/3} -\\frac{3}{2}(\\tau-1) d\\tau = -\\frac{3}{2}[\\frac{\\tau^2}{2} - \\tau]_0^{1/3} = -\\frac{3}{2}(\\frac{1}{18} - \\frac{1}{3}) = \\frac{5}{12}$.\n    $a_{12} = \\int_0^{1/3} L_2(\\tau) d\\tau = \\int_0^{1/3} \\frac{3}{2}(\\tau-1/3) d\\tau = \\frac{3}{2}[\\frac{\\tau^2}{2} - \\frac{\\tau}{3}]_0^{1/3} = \\frac{3}{2}(\\frac{1}{18} - \\frac{1}{9}) = -\\frac{1}{12}$.\n    $a_{21} = \\int_0^{1} L_1(\\tau) d\\tau = \\int_0^{1} -\\frac{3}{2}(\\tau-1) d\\tau = -\\frac{3}{2}[\\frac{\\tau^2}{2} - \\tau]_0^{1} = -\\frac{3}{2}(\\frac{1}{2} - 1) = \\frac{3}{4}$.\n    $a_{22} = \\int_0^{1} L_2(\\tau) d\\tau = \\int_0^{1} \\frac{3}{2}(\\tau-1/3) d\\tau = \\frac{3}{2}[\\frac{\\tau^2}{2} - \\frac{\\tau}{3}]_0^{1} = \\frac{3}{2}(\\frac{1}{2} - \\frac{1}{3}) = \\frac{1}{4}$.\n3.  The weights $b_i$ are given by $b_i = \\int_0^1 L_i(\\tau) d\\tau$. Since $c_2=1$, we immediately have $b_i = a_{2i}$.\n    $b_1 = a_{21} = 3/4$.\n    $b_2 = a_{22} = 1/4$.\nThis method has order $p = 2s-1 = 3$. The resulting Butcher tableau is:\n$$\n\\begin{array}{c|cc}\n1/3  5/12  -1/12 \\\\\n1  3/4  1/4 \\\\\n\\hline\n 3/4  1/4\n\\end{array}\n$$\nThe method is stiffly accurate because $c_s = c_2 = 1$ and $a_{sj} = b_j$ for $j=1, 2$. This implies that $\\mathbf{y}_{n+1} = \\mathbf{Y}_s = \\mathbf{Y}_2$, which is computationally advantageous.\n\n**2. The Nonlinear System for Stage Values**\n\nUsing the derived coefficients, the stage equations for $\\mathbf{Y}_1, \\mathbf{Y}_2 \\in \\mathbb{R}^2$ are:\n$$\n\\mathbf{Y}_1 = \\mathbf{y}_n + h \\left( \\frac{5}{12} \\mathbf{f}(\\mathbf{Y}_1) - \\frac{1}{12} \\mathbf{f}(\\mathbf{Y}_2) \\right)\n$$\n$$\n\\mathbf{Y}_2 = \\mathbf{y}_n + h \\left( \\frac{3}{4} \\mathbf{f}(\\mathbf{Y}_1) + \\frac{1}{4} \\mathbf{f}(\\mathbf{Y}_2) \\right)\n$$\nThis is a coupled system of $2 \\times 2 = 4$ nonlinear algebraic equations. To solve it using Newton's method, we express it in the residual form $\\mathbf{F}(\\mathbf{Z}) = \\mathbf{0}$. Let $\\mathbf{Z} \\in \\mathbb{R}^4$ be the stacked vector of stage values, $\\mathbf{Z} = \\begin{bmatrix} \\mathbf{Y}_1 \\\\ \\mathbf{Y}_2 \\end{bmatrix}$. The residual function $\\mathbf{F}: \\mathbb{R}^4 \\to \\mathbb{R}^4$ is:\n$$\n\\mathbf{F}(\\mathbf{Z}) =\n\\begin{bmatrix}\n\\mathbf{F}_1(\\mathbf{Y}_1, \\mathbf{Y}_2) \\\\\n\\mathbf{F}_2(\\mathbf{Y}_1, \\mathbf{Y}_2)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\mathbf{Y}_1 - \\mathbf{y}_n - h \\left( \\frac{5}{12} \\mathbf{f}(\\mathbf{Y}_1) - \\frac{1}{12} \\mathbf{f}(\\mathbf{Y}_2) \\right) \\\\\n\\mathbf{Y}_2 - \\mathbf{y}_n - h \\left( \\frac{3}{4} \\mathbf{f}(\\mathbf{Y}_1) + \\frac{1}{4} \\mathbf{f}(\\mathbf{Y}_2) \\right)\n\\end{bmatrix}\n= \\mathbf{0}\n$$\n\n**3. Newton's Method for the Stage Equations**\n\nNewton's method for solving the system $\\mathbf{F}(\\mathbf{Z}) = \\mathbf{0}$ is an iterative procedure. Starting from an initial guess $\\mathbf{Z}^{(0)}$, each iteration computes a new approximation $\\mathbf{Z}^{(k+1)}$ by solving a linear system for the update step $\\Delta \\mathbf{Z}^{(k)}$:\n$$\n\\mathbf{J}_{\\mathbf{F}}(\\mathbf{Z}^{(k)}) \\Delta \\mathbf{Z}^{(k)} = -\\mathbf{F}(\\mathbf{Z}^{(k)})\n$$\n$$\n\\mathbf{Z}^{(k+1)} = \\mathbf{Z}^{(k)} + \\alpha_k \\Delta \\mathbf{Z}^{(k)}\n$$\nwhere $\\mathbf{J}_{\\mathbf{F}}$ is the $4 \\times 4$ Jacobian matrix of the residual function $\\mathbf{F}$, and $\\alpha_k \\in (0, 1]$ is a step length determined by a line search algorithm.\n\nThe Jacobian $\\mathbf{J}_{\\mathbf{F}}$ has a $2 \\times 2$ block structure, where each block is a $2 \\times 2$ matrix:\n$$\n\\mathbf{J}_{\\mathbf{F}}(\\mathbf{Z}) = \\begin{bmatrix} \\frac{\\partial \\mathbf{F}_1}{\\partial \\mathbf{Y}_1}  \\frac{\\partial \\mathbf{F}_1}{\\partial \\mathbf{Y}_2} \\\\ \\frac{\\partial \\mathbf{F}_2}{\\partial \\mathbf{Y}_1}  \\frac{\\partial \\mathbf{F}_2}{\\partial \\mathbf{Y}_2} \\end{bmatrix}\n$$\nLet $\\mathbf{J}_{\\mathbf{f}}(\\mathbf{y})$ be the $2 \\times 2$ Jacobian of the ODE function $\\mathbf{f}$. The blocks of $\\mathbf{J}_{\\mathbf{F}}$ are:\n$\\frac{\\partial \\mathbf{F}_1}{\\partial \\mathbf{Y}_1} = \\mathbf{I} - h a_{11} \\mathbf{J}_{\\mathbf{f}}(\\mathbf{Y}_1) = \\mathbf{I} - h \\frac{5}{12} \\mathbf{J}_{\\mathbf{f}}(\\mathbf{Y}_1)$\n$\\frac{\\partial \\mathbf{F}_1}{\\partial \\mathbf{Y}_2} = -h a_{12} \\mathbf{J}_{\\mathbf{f}}(\\mathbf{Y}_2) = h \\frac{1}{12} \\mathbf{J}_{\\mathbf{f}}(\\mathbf{Y}_2)$\n$\\frac{\\partial \\mathbf{F}_2}{\\partial \\mathbf{Y}_1} = -h a_{21} \\mathbf{J}_{\\mathbf{f}}(\\mathbf{Y}_1) = -h \\frac{3}{4} \\mathbf{J}_{\\mathbf{f}}(\\mathbf{Y}_1)$\n$\\frac{\\partial \\mathbf{F}_2}{\\partial \\mathbf{Y}_2} = \\mathbf{I} - h a_{22} \\mathbf{J}_{\\mathbf{f}}(\\mathbf{Y}_2) = \\mathbf{I} - h \\frac{1}{4} \\mathbf{J}_{\\mathbf{f}}(\\mathbf{Y}_2)$\nwhere $\\mathbf{I}$ is the $2 \\times 2$ identity matrix.\n\nThe Jacobian of the ODE function $\\mathbf{f}(\\mathbf{y})$ with respect to $\\mathbf{y} = [y_1, y_2]^T$ is:\n$$\n\\mathbf{J}_{\\mathbf{f}}(\\mathbf{y}) =\n\\begin{bmatrix}\n\\frac{\\partial f_1}{\\partial y_1}  \\frac{\\partial f_1}{\\partial y_2} \\\\\n\\frac{\\partial f_2}{\\partial y_1}  \\frac{\\partial f_2}{\\partial y_2}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n-k_1(E_{\\text{tot}} - y_2)  k_1 y_1 + k_{-1} \\\\\nk_1(E_{\\text{tot}} - y_2)  -k_1 y_1 - (k_{-1} + k_2)\n\\end{bmatrix}.\n$$\nAt each Newton step $k$, $\\mathbf{J}_{\\mathbf{f}}$ is evaluated at the current stage value iterates $\\mathbf{Y}_1^{(k)}$ and $\\mathbf{Y}_2^{(k)}$ to assemble the full $4 \\times 4$ matrix $\\mathbf{J}_{\\mathbf{F}}(\\mathbf{Z}^{(k)})$.\n\nThe iteration continues until the convergence criterion $\\lVert \\mathbf{F}(\\mathbf{Z}^{(k)}) \\rVert_2 \\le 10^{-12}$ is met, or until a maximum of $50$ iterations is reached.\nTo ensure robust convergence, especially for large step sizes $h$, a backtracking line search is employed for the step length $\\alpha_k$. We seek the largest $\\alpha_k \\in \\{1, 1/2, 1/4, \\dots\\}$ that satisfies the Armijo-Goldstein sufficient decrease condition:\n$$\n\\left\\lVert \\mathbf{F}\\left(\\mathbf{Z}^{(k)} + \\alpha_k \\Delta \\mathbf{Z}^{(k)}\\right) \\right\\rVert_2^2 \\le \\left(1 - 2\\sigma\\alpha_k\\right) \\left\\lVert \\mathbf{F}\\left(\\mathbf{Z}^{(k)}\\right) \\right\\rVert_2^2\n$$\nA standard choice for the control parameter is $\\sigma = 10^{-4}$.\n\n**4. Implementation and Numerical Test**\n\nThe algorithm for a single step from $t_n=0$ is as follows:\n1.  Initialize the parameters and the initial state $\\mathbf{y}_n = [10^{-4}, 0]^T$.\n2.  For each time step $h$ in the test suite $\\{10^{-6}, 10^{-3}, 10^{-1}, 10^{1}\\}$ seconds:\n    a. Initialize the Newton solver with the guess $\\mathbf{Z}^{(0)} = [\\mathbf{y}_n^T, \\mathbf{y}_n^T]^T$ and iteration count `iter = 0`.\n    b. Loop for $k = 0, \\dots, 49$:\n        i.   Compute the residual $\\mathbf{F}(\\mathbf{Z}^{(k)})$.\n        ii.  Check if $\\lVert \\mathbf{F}(\\mathbf{Z}^{(k)}) \\rVert_2 \\le 10^{-12}$. If true, store `iter` and break the loop.\n        iii. Construct the Jacobian $\\mathbf{J}_{\\mathbf{F}}(\\mathbf{Z}^{(k)})$ by evaluating $\\mathbf{J}_{\\mathbf{f}}$ at $\\mathbf{Y}_1^{(k)}$ and $\\mathbf{Y}_2^{(k)}$.\n        iv.  Solve the linear system $\\mathbf{J}_{\\mathbf{F}} \\Delta \\mathbf{Z} = -\\mathbf{F}$ for the Newton step $\\Delta \\mathbf{Z}$.\n        v.   Perform the backtracking line search to find an acceptable step length $\\alpha_k$.\n        vi.  Update the solution: $\\mathbf{Z}^{(k+1)} = \\mathbf{Z}^{(k)} + \\alpha_k \\Delta \\mathbf{Z}$.\n        vii. Increment `iter`.\n    c. If the loop completes without convergence, record `-1`.\n3.  The final output is the list of iteration counts. The following program implements this procedure.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the number of Newton iterations required for a single step of the\n    2-stage Radau IIA method applied to a stiff biochemical system.\n    \"\"\"\n    # 1. Define model parameters and initial conditions\n    k1 = 1.0e6  # M^-1 s^-1\n    k_1 = 1.0e3  # s^-1\n    k2 = 1.0e2  # s^-1\n    Etot = 1.0e-6  # M\n    y_n = np.array([1.0e-4, 0.0])  # [S], [C] at t_n=0\n\n    # 2. Define the ODE system function f(y) and its Jacobian J_f(y)\n    def f(y):\n        \"\"\"ODE function dy/dt = f(y)\"\"\"\n        s, c = y[0], y[1]\n        e = Etot - c\n        ds_dt = -k1 * s * e + k_1 * c\n        dc_dt = k1 * s * e - (k_1 + k2) * c\n        return np.array([ds_dt, dc_dt])\n\n    def J_f(y):\n        \"\"\"Jacobian of f(y)\"\"\"\n        s, c = y[0], y[1]\n        # df1/ds, df1/dc\n        # df2/ds, df2/dc\n        J = np.array([\n            [-k1 * (Etot - c), k1 * s + k_1],\n            [k1 * (Etot - c), -k1 * s - (k_1 + k2)]\n        ])\n        return J\n\n    # 3. Radau IIA (s=2) method coefficients\n    a11, a12 = 5.0 / 12.0, -1.0 / 12.0\n    a21, a22 = 3.0 / 4.0, 1.0 / 4.0\n\n    # 4. Define test cases for the step size h\n    test_cases_h = [1.0e-6, 1.0e-3, 1.0e-1, 1.0e1]\n\n    # 5. Newton solver parameters\n    tol = 1.0e-12\n    max_iter = 50\n    sigma = 1.0e-4  # Armijo condition parameter\n\n    results = []\n\n    # 6. Loop over each test case\n    for h in test_cases_h:\n        # Initial guess for stage values Y1, Y2\n        Z = np.concatenate([y_n, y_n])\n        \n        num_iter = -1 # Default to failure\n\n        for k in range(max_iter):\n            Y1, Y2 = Z[0:2], Z[2:4]\n\n            # Calculate residual F(Z)\n            f_Y1 = f(Y1)\n            f_Y2 = f(Y2)\n            F1 = Y1 - y_n - h * (a11 * f_Y1 + a12 * f_Y2)\n            F2 = Y2 - y_n - h * (a21 * f_Y1 + a22 * f_Y2)\n            F_Z = np.concatenate([F1, F2])\n\n            # Check for convergence\n            res_norm_sq = np.dot(F_Z, F_Z)\n            if np.sqrt(res_norm_sq)  tol:\n                num_iter = k\n                break\n\n            # Assemble the 4x4 Jacobian J_F(Z)\n            J_f_Y1 = J_f(Y1)\n            J_f_Y2 = J_f(Y2)\n            I2 = np.identity(2)\n\n            J_F = np.block([\n                [I2 - h * a11 * J_f_Y1, -h * a12 * J_f_Y2],\n                [-h * a21 * J_f_Y1, I2 - h * a22 * J_f_Y2]\n            ])\n\n            # Solve the linear system for Newton step dZ\n            try:\n                dZ = np.linalg.solve(J_F, -F_Z)\n            except np.linalg.LinAlgError:\n                # Jacobian is singular, can't proceed\n                break\n\n            # Backtracking line search\n            alpha = 1.0\n            while alpha  1e-8:\n                Z_new = Z + alpha * dZ\n                Y1_new, Y2_new = Z_new[0:2], Z_new[2:4]\n                f_Y1_new, f_Y2_new = f(Y1_new), f(Y2_new)\n                F1_new = Y1_new - y_n - h * (a11 * f_Y1_new + a12 * f_Y2_new)\n                F2_new = Y2_new - y_n - h * (a21 * f_Y1_new + a22 * f_Y2_new)\n                F_new = np.concatenate([F1_new, F2_new])\n                \n                res_norm_new_sq = np.dot(F_new, F_new)\n                \n                # Check Armijo condition\n                if res_norm_new_sq = (1 - 2 * sigma * alpha) * res_norm_sq:\n                    Z = Z_new\n                    break\n                \n                alpha /= 2.0\n            else:\n                # Line search failed to find a step\n                break\n\n        # After Newton loop, if num_iter is still -1, it means we hit max_iter\n        if num_iter == -1 and k == max_iter - 1:\n            # Check final residual if loop terminated\n            Y1, Y2 = Z[0:2], Z[2:4]\n            f_Y1, f_Y2 = f(Y1), f(Y2)\n            F1 = Y1 - y_n - h * (a11 * f_Y1 + a12 * f_Y2)\n            F2 = Y2 - y_n - h * (a21 * f_Y1 + a22 * f_Y2)\n            F_Z = np.concatenate([F1, F2])\n            if np.linalg.norm(F_Z)  tol:\n                num_iter = k + 1\n        \n        results.append(num_iter)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}