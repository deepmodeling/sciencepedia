{
    "hands_on_practices": [
        {
            "introduction": "This first practice grounds our exploration in the fundamental concept of structural identifiability, which assesses whether model parameters can be uniquely determined from ideal, noise-free data. Using a simple yet common single-compartment model, this exercise guides you to uncover a \"scaling symmetry\"—a transformation of parameters that leaves the model's output unchanged, rendering the original parameters unidentifiable . By identifying this invariance and constructing a new, identifiable parameter combination, you will practice the essential skill of reparameterization, a critical first step in building a robust and estimable model.",
            "id": "3892821",
            "problem": "Consider a single-compartment biomedical system driven by a known input and observed through its state. The system dynamics are given by the ordinary differential equation $\\dot{x}(t) = -\\alpha \\beta \\, x(t) + u(t)$, with output $y(t) = x(t)$, where $x(t)$ denotes a measurable state (for example, a biomarker concentration), $u(t)$ is a known and controllable input (for example, an infusion rate), and $\\alpha$ and $\\beta$ are positive unknown parameters characterizing the kinetics. Assume the initial condition $x(0) = 0$, and that the experimenter can choose $u(t)$.\n\nStarting from the standard definition of structural identifiability (uniqueness of parameter mapping from ideal input-output data under noise-free conditions) and the concept of invariance transformations in parameter space, do the following:\n\n1. Using first principles of input-output invariance, demonstrate the existence of a continuous symmetry in parameter space such that the induced transformation leaves the input-output behavior invariant for all admissible inputs $u(t)$ and initial conditions. Identify the class of transformations and their infinitesimal generator in parameter space.\n\n2. Construct an identifiable reparameterization $\\gamma$ that removes the non-identifiable direction(s) induced by the symmetry and rewrite the model in terms of $\\gamma$.\n\n3. Propose an identifiability-guided experimental design by choosing a constant input $u(t) = U$ for $t \\geq 0$ and justify, using the solution of the reparameterized model, that the parameter $\\gamma$ can be identified from a single steady-state measurement of the output $y(t)$. Derive a closed-form expression for $\\gamma$ in terms of the input amplitude $U$ and the measured steady-state output $x_{\\infty} = \\lim_{t \\to \\infty} x(t)$.\n\nProvide your final answer as the symbolic expression for $\\gamma$ in terms of $U$ and $x_{\\infty}$. Express the unit of the parameter as reciprocal seconds (s$^{-1}$). No numerical approximation is required.",
            "solution": "The problem is first validated to ensure it is scientifically sound, self-contained, and well-posed.\n\n**Step 1: Extract Givens**\n- System dynamics: $\\dot{x}(t) = -\\alpha \\beta \\, x(t) + u(t)$\n- Output: $y(t) = x(t)$\n- State: $x(t)$, measurable\n- Input: $u(t)$, known and controllable\n- Parameters: $\\alpha  0$, $\\beta  0$, both unknown\n- Initial condition: $x(0) = 0$\n- Experimental condition for Part 3: constant input $u(t) = U$ for $t \\geq 0$\n- Measurement for Part 3: steady-state output $x_{\\infty} = \\lim_{t \\to \\infty} x(t)$\n\n**Step 2: Validate Using Extracted Givens**\nThe problem describes a single-compartment linear time-invariant (LTI) system, a fundamental and widely used model in biomedical engineering and pharmacokinetics. The concepts of structural identifiability, invariance transformations, and infinitesimal generators are standard, rigorous mathematical tools for analyzing such systems. The tasks are clearly defined, logically sequential, and do not contain contradictory or missing information. The problem is scientifically grounded, well-posed, and objective.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A full solution is provided below.\n\n**Part 1: Invariance Transformation and Infinitesimal Generator**\n\nStructural identifiability concerns the uniqueness of parameters for a given model structure and ideal input-output data. A lack of structural identifiability implies that there exist distinct parameter sets that yield identical input-output behavior for all admissible inputs. This can be formalized by seeking transformations in the parameter space that leave the model's output invariant.\n\nThe system is described by the input-output equation:\n$$\n\\dot{y}(t) = -\\alpha \\beta \\, y(t) + u(t)\n$$\nwith the initial condition $y(0) = 0$, since $y(t) = x(t)$.\n\nLet $\\mathbf{p} = (\\alpha, \\beta)$ be the vector of parameters. We are looking for a transformation $\\mathbf{p} \\to \\mathbf{p}' = (\\alpha', \\beta')$ such that the input-output relationship is preserved. This means that for the same input $u(t)$ and initial condition, the output $y(t)$ must be identical for both parameter sets $\\mathbf{p}$ and $\\mathbf{p}'$. For the differential equation to remain the same, the coefficients must be identical.\n$$\n-\\alpha \\beta \\, y(t) = -\\alpha' \\beta' \\, y(t)\n$$\nThis must hold for any trajectory $y(t)$, which requires the equality of the coefficients:\n$$\n\\alpha \\beta = \\alpha' \\beta'\n$$\nThis equation defines the invariance. Any transformation that preserves the product $\\alpha \\beta$ will leave the system's input-output behavior unchanged. We can define a one-parameter group of continuous transformations, parameterized by a scalar $s \\in \\mathbb{R}^+$, as follows:\n$$\n\\alpha' = s \\alpha\n$$\n$$\n\\beta' = \\frac{1}{s} \\beta\n$$\nUnder this transformation, the product is invariant: $\\alpha' \\beta' = (s\\alpha)(\\frac{1}{s}\\beta) = \\alpha\\beta$. This demonstrates the existence of a continuous symmetry in the parameter space.\n\nThe infinitesimal generator of this transformation group is a vector field in the parameter space that points in the direction of the transformation at the identity element (where $s=1$). It is calculated as the derivative of the transformation with respect to the group parameter $s$, evaluated at $s=1$.\nLet the transformation be $\\Phi_s(\\alpha, \\beta) = (s\\alpha, s^{-1}\\beta)$. The generator $V$ is:\n$$\nV = \\left. \\frac{d}{ds} \\Phi_s(\\alpha, \\beta) \\right|_{s=1} = \\left. \\frac{d}{ds} \\left( s\\alpha, \\frac{1}{s}\\beta \\right) \\right|_{s=1}\n$$\nThe components of the vector field are:\n$$\nV_\\alpha = \\left. \\frac{d}{ds}(s\\alpha) \\right|_{s=1} = \\alpha\n$$\n$$\nV_\\beta = \\left. \\frac{d}{ds}\\left(\\frac{1}{s}\\beta\\right) \\right|_{s=1} = \\left. -\\frac{1}{s^2}\\beta \\right|_{s=1} = -\\beta\n$$\nThus, the infinitesimal generator is the vector field $V = (\\alpha, -\\beta)$. In operator form, it is $V = \\alpha \\frac{\\partial}{\\partial\\alpha} - \\beta \\frac{\\partial}{\\partial\\beta}$. This generator defines the direction in the parameter space along which the parameters can be varied without changing the observable output, which is the source of the structural non-identifiability.\n\n**Part 2: Identifiable Reparameterization**\n\nThe analysis from Part 1 shows that we cannot independently determine $\\alpha$ and $\\beta$. We can only determine quantities that are invariant under the identified symmetry transformation. An invariant quantity $I(\\alpha, \\beta)$ is one for which the directional derivative along the generator is zero, i.e., $V(I) = 0$.\nLet's test the product $\\alpha\\beta$:\n$$\nV(\\alpha\\beta) = \\left(\\alpha \\frac{\\partial}{\\partial\\alpha} - \\beta \\frac{\\partial}{\\partial\\beta}\\right) (\\alpha\\beta) = \\alpha \\frac{\\partial(\\alpha\\beta)}{\\partial\\alpha} - \\beta \\frac{\\partial(\\alpha\\beta)}{\\partial\\beta} = \\alpha(\\beta) - \\beta(\\alpha) = 0\n$$\nSince the Lie derivative is zero, the product $\\alpha\\beta$ is an invariant of the symmetry group. This is the only combination of $\\alpha$ and $\\beta$ that appears in the model equation, and it is the quantity that can be uniquely identified from input-output data.\n\nWe can therefore construct an identifiable reparameterization by defining a new parameter, $\\gamma$, as this invariant combination:\n$$\n\\gamma = \\alpha \\beta\n$$\nSubstituting this into the original model equation gives the reparameterized model:\n$$\n\\dot{x}(t) = -\\gamma x(t) + u(t)\n$$\nwith $y(t) = x(t)$ and $x(0)=0$. This model now has a single parameter, $\\gamma$. To confirm its identifiability, suppose two different parameter values, $\\gamma_1$ and $\\gamma_2$, produce the same output $y(t)$ for all inputs $u(t)$. Then we must have:\n$$\n-\\gamma_1 y(t) + u(t) = -\\gamma_2 y(t) + u(t) \\implies (\\gamma_2 - \\gamma_1) y(t) = 0\n$$\nSince this must hold for any admissible input $u(t)$, we can choose an input that produces a non-trivial output, $y(t) \\neq 0$. This forces $\\gamma_2 - \\gamma_1 = 0$, or $\\gamma_1 = \\gamma_2$. Therefore, $\\gamma$ is structurally identifiable.\n\n**Part 3: Identifiability-Guided Experimental Design**\n\nThe problem asks for an experimental design to determine $\\gamma$. We follow the suggestion to use a constant input, $u(t) = U$, for $t \\geq 0$, where $U$ is a known constant. The reparameterized model becomes:\n$$\n\\dot{x}(t) = -\\gamma x(t) + U\n$$\nThis is a first-order linear ordinary differential equation. We can solve it using an integrating factor or by finding the homogeneous and particular solutions. The general solution has the form $x(t) = x_h(t) + x_p(t)$. The homogeneous solution to $\\dot{x}_h(t) + \\gamma x_h(t) = 0$ is $x_h(t) = C e^{-\\gamma t}$. For the particular solution, we can assume a constant $x_p(t) = K$, which gives $0 = -\\gamma K + U$, so $K = U/\\gamma$.\nThe full solution is:\n$$\nx(t) = C e^{-\\gamma t} + \\frac{U}{\\gamma}\n$$\nUsing the initial condition $x(0)=0$:\n$$\n0 = C e^0 + \\frac{U}{\\gamma} \\implies C = -\\frac{U}{\\gamma}\n$$\nSubstituting $C$ back into the solution gives the state trajectory:\n$$\nx(t) = \\frac{U}{\\gamma} (1 - e^{-\\gamma t})\n$$\nThe problem proposes to measure the steady-state value of the output, $x_{\\infty}$. We can calculate this by taking the limit as $t \\to \\infty$. Since $\\alpha  0$ and $\\beta  0$, we have $\\gamma = \\alpha\\beta  0$. Therefore, the exponential term decays to zero:\n$$\nx_{\\infty} = \\lim_{t \\to \\infty} x(t) = \\lim_{t \\to \\infty} \\frac{U}{\\gamma} (1 - e^{-\\gamma t}) = \\frac{U}{\\gamma} (1 - 0) = \\frac{U}{\\gamma}\n$$\nThis simple algebraic relationship, $x_{\\infty} = \\frac{U}{\\gamma}$, connects the identifiable parameter $\\gamma$ to the known input amplitude $U$ and the measurable steady-state output $x_{\\infty}$. This justifies the proposed experimental design: applying a constant input and measuring the resulting steady-state concentration is sufficient to determine the only identifiable parameter of the system.\n\nTo find the expression for $\\gamma$, we simply rearrange the equation:\n$$\n\\gamma = \\frac{U}{x_{\\infty}}\n$$\nThis provides a closed-form expression for $\\gamma$ in terms of experimentally controlled and measured quantities. Regarding the units, if $x$ is a concentration (e.g., mol/L) and $u$ is a rate of change of concentration (e.g., mol/L/s), then from the equation $\\dot{x} = -\\gamma x + u$, dimensional analysis requires the unit of $\\gamma$ to be inverse time (e.g., s$^{-1}$). This is consistent with the derived expression $\\gamma = U/x_{\\infty}$, where the units are $(\\text{conc}/\\text{time}) / \\text{conc} = 1/\\text{time}$.",
            "answer": "$$\n\\boxed{\\frac{U}{x_{\\infty}}}\n$$"
        },
        {
            "introduction": "While structural identifiability deals with an idealized scenario, practical identifiability addresses the challenges of parameter estimation from real-world, noisy data. Even when a model is structurally identifiable, high correlations between parameters can make them practically impossible to estimate reliably. This exercise introduces the Fisher Information Matrix ($FIM$) as a cornerstone for quantifying the information content of an experiment and diagnosing such practical non-identifiability, which often manifests as elongated, flat \"ridges\" in the likelihood surface . By analyzing the $FIM$ for a classic exponential decay model, you will derive the analytical expression for parameter correlation and gain intuition for how experimental data quality affects estimation confidence.",
            "id": "3892885",
            "problem": "Consider a single-compartment intravenous bolus model of a biomarker concentration in plasma, where the measured concentration at time $t$ is described by the parametric function $y(t;\\theta) = A \\exp(-k t)$ with parameters $\\theta = (A,k)$. Assume independent and identically distributed Gaussian measurement noise with variance $\\sigma^{2}$, so that the negative log-likelihood for $N$ measurements $\\{(t_{i},y_{i})\\}_{i=1}^{N}$ is\n$$\n\\ell(\\theta) = \\frac{1}{2\\sigma^{2}} \\sum_{i=1}^{N} \\left(y_{i} - A \\exp(-k t_{i})\\right)^{2} + \\text{constant}.\n$$\nLet $\\hat{\\theta} = (\\hat{A},\\hat{k})$ be the maximum likelihood estimate (MLE) with $\\hat{A}  0$. In the large-sample regime and under regularity conditions, the Hessian of $\\ell(\\theta)$ at $\\hat{\\theta}$ is well-approximated by the Fisher Information Matrix (FIM), denoted $F(\\hat{\\theta})$, whose entries are given by\n$$\nF_{mn}(\\hat{\\theta}) = \\frac{1}{\\sigma^{2}} \\sum_{i=1}^{N} \\frac{\\partial y(t_{i};\\theta)}{\\partial \\theta_{m}} \\bigg|_{\\theta=\\hat{\\theta}} \\cdot \\frac{\\partial y(t_{i};\\theta)}{\\partial \\theta_{n}} \\bigg|_{\\theta=\\hat{\\theta}}.\n$$\nStarting from these definitions and using only basic properties of the exponential function and matrix algebra, analyze how correlations between parameters manifest as ridges in the likelihood through the eigenstructure of $F(\\hat{\\theta})$. Then, compute the asymptotic correlation coefficient between $A$ and $k$ implied by the inverse Fisher matrix $F(\\hat{\\theta})^{-1}$ and express it as a single closed-form analytic expression in terms of $\\{t_{i}\\}_{i=1}^{N}$ and $\\hat{k}$.\n\nYour final answer must be a single closed-form analytic expression for the correlation coefficient $\\rho_{A,k}$ between $A$ and $k$ implied by $F(\\hat{\\theta})^{-1}$. No numerical evaluation is required. No units should be included in the final answer. If any simplification is possible, present the simplified form. Round nothing; provide an exact expression.",
            "solution": "The user wants me to validate the problem statement and, if it is valid, provide a complete solution.\n\n### Step 1: Extract Givens\n-   **Model function:** $y(t;\\theta) = A \\exp(-k t)$\n-   **Parameters:** $\\theta = (A,k)$\n-   **Measurement noise:** Independent and identically distributed Gaussian with variance $\\sigma^{2}$.\n-   **Negative log-likelihood function:** $\\ell(\\theta) = \\frac{1}{2\\sigma^{2}} \\sum_{i=1}^{N} \\left(y_{i} - A \\exp(-k t_{i})\\right)^{2} + \\text{constant}$\n-   **Maximum Likelihood Estimate (MLE):** $\\hat{\\theta} = (\\hat{A},\\hat{k})$ with the condition $\\hat{A}  0$.\n-   **Fisher Information Matrix (FIM) definition:** $F_{mn}(\\hat{\\theta}) = \\frac{1}{\\sigma^{2}} \\sum_{i=1}^{N} \\frac{\\partial y(t_{i};\\theta)}{\\partial \\theta_{m}} \\bigg|_{\\theta=\\hat{\\theta}} \\cdot \\frac{\\partial y(t_{i};\\theta)}{\\partial \\theta_{n}} \\bigg|_{\\theta=\\hat{\\theta}}$\n-   **Premise:** The Hessian of $\\ell(\\theta)$ at $\\hat{\\theta}$ is well-approximated by the FIM, $F(\\hat{\\theta})$.\n-   **Objective:**\n    1.  Analyze how parameter correlations manifest as ridges in the likelihood through the eigenstructure of $F(\\hat{\\theta})$.\n    2.  Compute the asymptotic correlation coefficient $\\rho_{A,k}$ between $A$ and $k$ implied by the inverse Fisher matrix $F(\\hat{\\theta})^{-1}$.\n    3.  Express $\\rho_{A,k}$ as a single closed-form analytic expression in terms of $\\{t_{i}\\}_{i=1}^{N}$ and $\\hat{k}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is subjected to validation against the specified criteria.\n-   **Scientifically Grounded:** The problem is firmly rooted in standard pharmacokinetic modeling and statistical estimation theory. The single-compartment exponential decay model is fundamental in pharmacology. The use of the negative log-likelihood for Gaussian noise, the Fisher Information Matrix, and its connection to the parameter covariance matrix (via the Cramér-Rao lower bound) are all established principles in statistics and system identification. The problem is scientifically sound.\n-   **Well-Posed:** The problem is clearly stated, providing all necessary definitions and context to derive a unique analytical solution. The objective is specific and achievable.\n-   **Objective:** The problem is formulated using precise, unambiguous mathematical and statistical language, free of any subjective or opinion-based content.\n\nThe problem does not exhibit any of the listed flaws (e.g., scientific unsoundness, incompleteness, ambiguity). It is a well-defined theoretical question within the specified field of biomedical systems modeling.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. The solution process will now proceed.\n\nThe problem requires an analysis of parameter correlation through the Fisher Information Matrix (FIM) and the derivation of the specific correlation coefficient for the given model.\n\nFirst, we analyze the relationship between the FIM, the likelihood surface, and parameter correlations. The FIM, $F(\\hat{\\theta})$, approximates the Hessian of the negative log-likelihood function at the point of the maximum likelihood estimate, $\\hat{\\theta}$. The Hessian describes the curvature of this surface. In the large-sample limit, the inverse of the FIM, $F(\\hat{\\theta})^{-1}$, provides an approximation of the covariance matrix of the parameter estimates, $Cov(\\hat{\\theta})$.\n\nThe level sets of the quadratic approximation of the negative log-likelihood surface near its minimum are ellipsoids defined by $(\\theta - \\hat{\\theta})^T F(\\hat{\\theta}) (\\theta - \\hat{\\theta}) = c$, where $c$ is a constant. The eigenvectors of $F(\\hat{\\theta})$ point along the principal axes of these ellipsoids, and the corresponding eigenvalues are inversely proportional to the squared lengths of these axes. A small eigenvalue indicates a long axis, which corresponds to a \"ridge\" or a \"shallow valley\" in the likelihood landscape. Moving along this ridge results in a very small change in the likelihood, meaning that many different parameter combinations along this direction fit the data almost equally well. If the eigenvector corresponding to this small eigenvalue is not aligned with any of the parameter axes, it implies that a specific combination of parameters can be varied without significantly degrading the fit, which is the definition of high parameter correlation. A large off-diagonal element in the normalized FIM (or the covariance matrix) signals such non-alignment and thus high correlation.\n\nNow, we proceed to compute the FIM for the given model. The parameters are $\\theta_1 = A$ and $\\theta_2 = k$. The model is $y(t_i; A, k) = A \\exp(-k t_i)$. We must first compute the partial derivatives of $y$ with respect to each parameter, evaluated at the MLE $\\hat{\\theta} = (\\hat{A}, \\hat{k})$.\n\nThe partial derivative with respect to $A$ is:\n$$\n\\frac{\\partial y(t_i; \\theta)}{\\partial A} \\bigg|_{\\theta=\\hat{\\theta}} = \\exp(-\\hat{k} t_i)\n$$\n\nThe partial derivative with respect to $k$ is:\n$$\n\\frac{\\partial y(t_i; \\theta)}{\\partial k} \\bigg|_{\\theta=\\hat{\\theta}} = - \\hat{A} t_i \\exp(-\\hat{k} t_i)\n$$\n\nUsing the definition of the FIM, we construct the $2 \\times 2$ matrix $F(\\hat{\\theta})$. Let's denote it simply as $F$.\n\nThe entry $F_{11}$ (or $F_{AA}$) is:\n$$\nF_{11} = \\frac{1}{\\sigma^2} \\sum_{i=1}^{N} \\left( \\frac{\\partial y_i}{\\partial A} \\right)^2 = \\frac{1}{\\sigma^2} \\sum_{i=1}^{N} (\\exp(-\\hat{k} t_i))^2 = \\frac{1}{\\sigma^2} \\sum_{i=1}^{N} \\exp(-2 \\hat{k} t_i)\n$$\n\nThe off-diagonal entries $F_{12}$ and $F_{21}$ (or $F_{Ak}$ and $F_{kA}$) are equal:\n$$\nF_{12} = F_{21} = \\frac{1}{\\sigma^2} \\sum_{i=1}^{N} \\left( \\frac{\\partial y_i}{\\partial A} \\right) \\left( \\frac{\\partial y_i}{\\partial k} \\right) = \\frac{1}{\\sigma^2} \\sum_{i=1}^{N} \\left( \\exp(-\\hat{k} t_i) \\right) \\left( - \\hat{A} t_i \\exp(-\\hat{k} t_i) \\right) = -\\frac{\\hat{A}}{\\sigma^2} \\sum_{i=1}^{N} t_i \\exp(-2 \\hat{k} t_i)\n$$\n\nThe entry $F_{22}$ (or $F_{kk}$) is:\n$$\nF_{22} = \\frac{1}{\\sigma^2} \\sum_{i=1}^{N} \\left( \\frac{\\partial y_i}{\\partial k} \\right)^2 = \\frac{1}{\\sigma^2} \\sum_{i=1}^{N} \\left( - \\hat{A} t_i \\exp(-\\hat{k} t_i) \\right)^2 = \\frac{\\hat{A}^2}{\\sigma^2} \\sum_{i=1}^{N} t_i^2 \\exp(-2 \\hat{k} t_i)\n$$\n\nTo simplify the expressions, let us define the following sums:\n$$\nS_0 = \\sum_{i=1}^{N} \\exp(-2 \\hat{k} t_i)\n$$\n$$\nS_1 = \\sum_{i=1}^{N} t_i \\exp(-2 \\hat{k} t_i)\n$$\n$$\nS_2 = \\sum_{i=1}^{N} t_i^2 \\exp(-2 \\hat{k} t_i)\n$$\nThe FIM can now be written as:\n$$\nF = \\frac{1}{\\sigma^2} \\begin{pmatrix} S_0  -\\hat{A} S_1 \\\\ -\\hat{A} S_1  \\hat{A}^2 S_2 \\end{pmatrix}\n$$\nThe asymptotic correlation coefficient $\\rho_{A,k}$ between the estimators $\\hat{A}$ and $\\hat{k}$ is given by the formula relating it to the components of the FIM for a $2$-parameter problem:\n$$\n\\rho_{A,k} = \\frac{-F_{12}}{\\sqrt{F_{11} F_{22}}}\n$$\nSubstituting the components of our calculated FIM into this formula:\n$$\n\\rho_{A,k} = \\frac{- \\left( -\\frac{\\hat{A}}{\\sigma^2} S_1 \\right)}{\\sqrt{\\left( \\frac{1}{\\sigma^2} S_0 \\right) \\left( \\frac{\\hat{A}^2}{\\sigma^2} S_2 \\right)}}\n$$\n$$\n\\rho_{A,k} = \\frac{\\frac{\\hat{A} S_1}{\\sigma^2}}{\\sqrt{\\frac{\\hat{A}^2 S_0 S_2}{\\sigma^4}}}\n$$\n$$\n\\rho_{A,k} = \\frac{\\frac{\\hat{A} S_1}{\\sigma^2}}{\\frac{\\hat{A} \\sqrt{S_0 S_2}}{\\sigma^2}}\n$$\nThe terms $\\hat{A}$ and $\\sigma^2$ cancel out, yielding:\n$$\n\\rho_{A,k} = \\frac{S_1}{\\sqrt{S_0 S_2}}\n$$\nFinally, we substitute the definitions of $S_0$, $S_1$, and $S_2$ back into this expression to obtain the final closed-form answer in terms of $\\{t_i\\}_{i=1}^N$ and $\\hat{k}$.\n$$\n\\rho_{A,k} = \\frac{\\sum_{i=1}^{N} t_i \\exp(-2 \\hat{k} t_i)}{\\sqrt{\\left( \\sum_{i=1}^{N} \\exp(-2 \\hat{k} t_i) \\right) \\left( \\sum_{i=1}^{N} t_i^2 \\exp(-2 \\hat{k} t_i) \\right)}}\n$$\nBy the Cauchy-Schwarz inequality, $(S_1)^2 \\le S_0 S_2$, which ensures that $|\\rho_{A,k}| \\le 1$, as required for a correlation coefficient. Since $t_i  0$ for a time course experiment and $\\hat{k}$ is expected to be positive for a decay process, the sums $S_0, S_1, S_2$ are all positive, and thus the correlation coefficient $\\rho_{A,k}$ is positive. This implies that an increase in the estimate of $A$ tends to be accompanied by an increase in the estimate of $k$.",
            "answer": "$$\\boxed{\\frac{\\sum_{i=1}^{N} t_i \\exp(-2 \\hat{k} t_i)}{\\sqrt{\\left(\\sum_{i=1}^{N} \\exp(-2 \\hat{k} t_i)\\right) \\left(\\sum_{i=1}^{N} t_i^2 \\exp(-2 \\hat{k} t_i)\\right)}}}$$"
        },
        {
            "introduction": "The ultimate goal of identifiability analysis is to inform better experimental design. This final practice moves from diagnosing identifiability issues to proactively designing experiments that maximize the information we can obtain about our model parameters. Using a nonlinear Michaelis-Menten enzyme kinetics model, you will apply the Fisher Information Matrix ($FIM$) framework to compare different experimental input strategies—a constant step, a ramp, and a sinusoidal input—based on the D-optimality criterion . This computational exercise requires you to solve the sensitivity equations to construct the $FIM$, providing hands-on experience in the powerful methodology of model-based optimal experimental design.",
            "id": "3892890",
            "problem": "Consider a single-state enzyme kinetics model for substrate dynamics and measurement defined by the ordinary differential equation and output relation\n$$\\dot{S}(t) = -\\frac{V_{\\max} S(t)}{K_M + S(t)} + u(t), \\quad y(t) = S(t),$$\nwhere $S(t)$ is the substrate concentration, $V_{\\max}$ is the maximum reaction velocity, $K_M$ is the Michaelis constant, and $u(t)$ is a controllable input representing the substrate infusion rate. The true parameters are $\\theta = [V_{\\max}, K_M]^{\\top}$. Assume additive zero-mean Gaussian measurement noise with constant variance $\\sigma^2$, so that the observation is $y_{\\text{obs}}(t) = y(t) + \\varepsilon(t)$, where $\\varepsilon(t)$ is Gaussian with variance $\\sigma^2$ and independent across time. The aim is to compare three input families $u(t)$ under a fixed amplitude budget and to select the input family that yields the largest determinant of the Fisher Information Matrix (FIM), which defines D-optimality.\n\nUse the following fundamental bases:\n- For continuously observed scalar outputs with independent Gaussian noise of variance $\\sigma^2$, the Fisher Information Matrix (FIM) for parameters $\\theta$ is given by\n$$\\mathcal{I}(\\theta) = \\int_{0}^{T} \\frac{1}{\\sigma^2} \\left(\\frac{\\partial y(t)}{\\partial \\theta}\\right)\\left(\\frac{\\partial y(t)}{\\partial \\theta}\\right)^{\\top} \\, dt,$$\nwhere $\\frac{\\partial y(t)}{\\partial \\theta}$ is the sensitivity of the output with respect to parameters $\\theta$.\n- Sensitivity dynamics for the state-dependent output follow from the chain rule: if $\\dot{S}(t) = f(S(t), \\theta, t)$ and $y(t) = S(t)$, then for each parameter $\\theta_i$, the sensitivity $z_i(t) = \\frac{\\partial S(t)}{\\partial \\theta_i}$ evolves according to\n$$\\dot{z}_i(t) = \\frac{\\partial f}{\\partial S}(S(t), \\theta, t) \\, z_i(t) + \\frac{\\partial f}{\\partial \\theta_i}(S(t), \\theta, t), \\quad z_i(0) = 0,$$\nprovided the initial condition $S(0)$ does not depend on $\\theta$.\n\nYou must compute the expected information and D-optimality for three input families:\n- Step: $$u_{\\text{step}}(t) = A,$$\n- Ramp: $$u_{\\text{ramp}}(t) = A \\frac{t}{T},$$\n- Sinusoidal: $$u_{\\text{sin}}(t) = A \\sin(\\omega t),$$\nwhere $A$ is a fixed amplitude in units of mol/(L·s), $T$ is the experiment duration in seconds, and $\\omega$ is the angular frequency in radians per second.\n\nPhysical units:\n- $S(t)$ in mol/L,\n- $V_{\\max}$ in mol/(L·s),\n- $K_M$ in mol/L,\n- $u(t)$ in mol/(L·s),\n- $t$ in s,\n- $\\omega$ in radians/s,\n- $\\sigma^2$ in (mol/L)$^2$.\n\nAngle unit specification: $\\omega$ is provided in radians per second and must be used as such.\n\nYour program must, for each test case, compute the continuous-time FIM using the sensitivity equations, and then compute the D-optimality as the determinant of the FIM. For fairness, use the same amplitude $A$ across the three inputs in a given test case. Select the input family that maximizes the determinant; in case of ties (equal determinants within numerical precision), select the smallest index according to the mapping step $\\to 0$, ramp $\\to 1$, sinusoidal $\\to 2$.\n\nDesign details to be adhered to:\n- Use the sensitivity dynamics implied by the enzyme kinetics model:\n$$f(S,\\theta,t) = -\\frac{V_{\\max} S}{K_M + S} + u(t).$$\n- The Jacobians needed for sensitivity propagation are\n$$\\frac{\\partial f}{\\partial S} = -\\frac{V_{\\max} K_M}{(K_M + S)^2}, \\quad \\frac{\\partial f}{\\partial V_{\\max}} = -\\frac{S}{K_M + S}, \\quad \\frac{\\partial f}{\\partial K_M} = \\frac{V_{\\max} S}{(K_M + S)^2}.$$\n- The Fisher Information Matrix integrand is formed from the sensitivity of $y(t)=S(t)$ with respect to $\\theta$, i.e., the vector $\\left[z_{V_{\\max}}(t), z_{K_M}(t)\\right]^{\\top}$, so that the determinant is\n$$\\det(\\mathcal{I}(\\theta)) = \\det\\left(\\int_{0}^{T} \\frac{1}{\\sigma^2}\n\\begin{bmatrix}\nz_{V_{\\max}}(t)^2  z_{V_{\\max}}(t) z_{K_M}(t) \\\\\nz_{V_{\\max}}(t) z_{K_M}(t)  z_{K_M}(t)^2\n\\end{bmatrix} dt \\right).$$\n\nTest suite:\nProvide results for the following three test cases, each specified as a tuple $(V_{\\max}, K_M, S_0, T, \\sigma, A, \\omega)$ with the units as defined above.\n1. Case $1$ (moderate kinetics; long duration; low noise): $(1.0\\times 10^{-3}, 5.0\\times 10^{-4}, 1.0\\times 10^{-3}, 200, 1.0\\times 10^{-6}, 2.0\\times 10^{-4}, 0.05)$.\n2. Case $2$ (near-saturation regime; short duration; moderate noise): $(1.0\\times 10^{-2}, 1.0\\times 10^{-5}, 5.0\\times 10^{-5}, 50, 5.0\\times 10^{-6}, 1.0\\times 10^{-4}, 0.5)$.\n3. Case $3$ (slow kinetics; long duration; moderate noise): $(5.0\\times 10^{-5}, 1.0\\times 10^{-3}, 1.0\\times 10^{-3}, 500, 2.0\\times 10^{-6}, 1.0\\times 10^{-5}, 0.02)$.\n\nAlgorithmic requirements:\n- Integrate the augmented system consisting of the state $S(t)$ and sensitivity states $\\left[z_{V_{\\max}}(t), z_{K_M}(t)\\right]$ from $t=0$ to $t=T$ with initial conditions $S(0)=S_0$ and $z_{V_{\\max}}(0)=z_{K_M}(0)=0$.\n- Accumulate the FIM continuously by integrating the ordinary differential equations for the matrix entries\n$$\\dot{J}_{11}(t) = \\frac{1}{\\sigma^2} z_{V_{\\max}}(t)^2, \\quad \\dot{J}_{12}(t) = \\frac{1}{\\sigma^2} z_{V_{\\max}}(t) z_{K_M}(t), \\quad \\dot{J}_{22}(t) = \\frac{1}{\\sigma^2} z_{K_M}(t)^2,$$\nwith $J_{11}(0)=J_{12}(0)=J_{22}(0)=0$, and compute $\\det(\\mathcal{I}(\\theta)) = J_{11}(T) J_{22}(T) - \\left(J_{12}(T)\\right)^2$.\n- Ensure numerical stability by avoiding division by zero when $K_M + S(t)$ is small; treat $K_M + S(t)$ as bounded below by a small positive constant when evaluating the Jacobians.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[0,1,2]$), where each entry is an integer in $\\{0,1,2\\}$ indicating the index of the input family (step $=0$, ramp $=1$, sinusoidal $=2$) that maximizes D-optimality for the corresponding test case. No additional text should be printed. Angles must be in radians, and all physical quantities must be used in their specified units. The output must be exactly one line.",
            "solution": "The problem requires a comparative analysis of three different input stimuli for an enzyme kinetics experiment, with the goal of selecting the input that provides the most information about the model parameters $\\theta = [V_{\\max}, K_M]^{\\top}$. The measure of information is the D-optimality criterion, which corresponds to the determinant of the Fisher Information Matrix (FIM), $\\det(\\mathcal{I}(\\theta))$. The analysis will be performed by numerically solving a system of ordinary differential equations (ODEs) that describe the system dynamics, parameter sensitivities, and the evolution of the FIM components.\n\nThe core of the problem lies in the Michaelis-Menten enzyme kinetics model, where the rate of change of substrate concentration $S(t)$ is given by:\n$$\n\\dot{S}(t) = f(S(t), \\theta, t) = -\\frac{V_{\\max} S(t)}{K_M + S(t)} + u(t)\n$$\nHere, $S(t)$ is the substrate concentration, $V_{\\max}$ is the maximum reaction velocity, $K_M$ is the Michaelis constant, and $u(t)$ is the externally controlled substrate infusion rate. The measured output is the substrate concentration itself, $y(t) = S(t)$, subject to additive Gaussian noise.\n\nThe Fisher Information Matrix, for a continuously observed scalar output with independent Gaussian noise of constant variance $\\sigma^2$, quantifies the amount of information the experimental data provides about the parameters $\\theta$. It is defined by the integral:\n$$\n\\mathcal{I}(\\theta) = \\int_{0}^{T} \\frac{1}{\\sigma^2} \\left(\\frac{\\partial y(t)}{\\partial \\theta}\\right)\\left(\\frac{\\partial y(t)}{\\partial \\theta}\\right)^{\\top} \\, dt\n$$\nwhere $T$ is the duration of the experiment, and $\\frac{\\partial y(t)}{\\partial \\theta}$ is the vector of sensitivities of the output with respect to the parameters. Since $y(t) = S(t)$, the output sensitivities are identical to the state sensitivities, denoted as $z_i(t) = \\frac{\\partial S(t)}{\\partial \\theta_i}$.\n\nTo compute the FIM, we must first determine the sensitivity functions $z_{V_{\\max}}(t)$ and $z_{K_M}(t)$. These are found by solving the sensitivity equations, which are derived by differentiating the state equation with respect to each parameter. The general form is:\n$$\n\\dot{z}_i(t) = \\frac{\\partial f}{\\partial S}(S(t), \\theta, t) \\, z_i(t) + \\frac{\\partial f}{\\partial \\theta_i}(S(t), \\theta, t)\n$$\nwith initial conditions $z_i(0) = 0$ as the initial state $S(0)$ is assumed to be known and not dependent on $\\theta$. For our specific model, the required partial derivatives are:\n$$\n\\frac{\\partial f}{\\partial S} = -\\frac{V_{\\max} K_M}{(K_M + S)^2}, \\quad \\frac{\\partial f}{\\partial V_{\\max}} = -\\frac{S}{K_M + S}, \\quad \\frac{\\partial f}{\\partial K_M} = \\frac{V_{\\max} S}{(K_M + S)^2}\n$$\nThis leads to a system of two coupled ODEs for the sensitivities $z_{V_{\\max}}(t)$ and $z_{K_M}(t)$.\n\nThe problem is computationally addressed by creating and solving an augmented system of ODEs. This is necessary because the sensitivity equations depend on the state $S(t)$, and the FIM calculation requires integrating functions of the sensitivities. The augmented system consists of $6$ states: the primary state $S(t)$, the two sensitivities $z_{V_{\\max}}(t)$ and $z_{K_M}(t)$, and the three unique entries of the symmetric FIM, which we will denote as $J_{11}(t)$, $J_{12}(t)$, and $J_{22}(t)$. The ODE for each FIM component is derived from the FIM definition, e.g., $J_{11}(T) = \\int_{0}^{T} \\frac{1}{\\sigma^2} (z_{V_{\\max}}(t))^2 dt$, which implies $\\dot{J}_{11}(t) = \\frac{1}{\\sigma^2} (z_{V_{\\max}}(t))^2$.\n\nThe complete augmented system of six ODEs is:\n$1$. $\\dot{S}(t) = -\\frac{V_{\\max} S(t)}{K_M + S(t)} + u(t)$\n$2$. $\\dot{z}_{V_{\\max}}(t) = \\left(-\\frac{V_{\\max} K_M}{(K_M + S(t))^2}\\right) z_{V_{\\max}}(t) - \\frac{S(t)}{K_M + S(t)}$\n$3$. $\\dot{z}_{K_M}(t) = \\left(-\\frac{V_{\\max} K_M}{(K_M + S(t))^2}\\right) z_{K_M}(t) + \\frac{V_{\\max} S(t)}{(K_M + S(t))^2}$\n$4$. $\\dot{J}_{11}(t) = \\frac{1}{\\sigma^2} (z_{V_{\\max}}(t))^2$\n$5$. $\\dot{J}_{12}(t) = \\frac{1}{\\sigma^2} z_{V_{\\max}}(t) z_{K_M}(t)$\n$6$. $\\dot{J}_{22}(t) = \\frac{1}{\\sigma^2} (z_{K_M}(t))^2$\n\nThe initial conditions for this system at $t=0$ are:\n$$\nS(0) = S_0, \\quad z_{V_{\\max}}(0) = 0, \\quad z_{K_M}(0) = 0, \\quad J_{11}(0) = 0, \\quad J_{12}(0) = 0, \\quad J_{22}(0) = 0\n$$\n\nThe computational procedure for each test case is as follows:\nFor each of the three input families ($u_{\\text{step}}$, $u_{\\text{ramp}}$, $u_{\\text{sin}}$), the augmented ODE system is numerically integrated over the interval $[0, T]$. The numerical integration is performed using a standard solver, such as a Runge-Kutta method. A small positive constant is added to the denominator term $K_M + S(t)$ during computation to prevent numerical instability, particularly if $S(t)$ approaches $-K_M$ due to numerical error, although physical concentrations must be non-negative.\n\nThe integration yields the values of the FIM components at the final time $T$: $\\mathcal{I}_{11} = J_{11}(T)$, $\\mathcal{I}_{12} = J_{12}(T)$, and $\\mathcal{I}_{22} = J_{22}(T)$. The D-optimality criterion is then calculated as the determinant of the FIM:\n$$\n\\det(\\mathcal{I}(\\theta)) = \\mathcal{I}_{11} \\mathcal{I}_{22} - (\\mathcal{I}_{12})^2\n$$\nFinally, for each test case, the three resulting determinants are compared. The input function that yields the maximum determinant is identified as the optimal choice. The indices are mapped as step $\\to 0$, ramp $\\to 1$, and sinusoidal $\\to 2$. In the case of a tie, the input with the smallest index is chosen. This entire process is repeated for all provided test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.integrate import solve_ivp\n\ndef solve():\n    \"\"\"\n    Solves the optimal experimental design problem for the given test cases.\n    \"\"\"\n    \n    # Test cases: (V_max, K_M, S0, T, sigma, A, omega)\n    test_cases = [\n        (1.0e-3, 5.0e-4, 1.0e-3, 200, 1.0e-6, 2.0e-4, 0.05),\n        (1.0e-2, 1.0e-5, 5.0e-5, 50, 5.0e-6, 1.0e-4, 0.5),\n        (5.0e-5, 1.0e-3, 1.0e-3, 500, 2.0e-6, 1.0e-5, 0.02)\n    ]\n    \n    input_families = ['step', 'ramp', 'sin']\n    results = []\n\n    for case_params in test_cases:\n        V_max, K_M, S0, T, sigma, A, omega = case_params\n        determinants = []\n\n        for input_type in input_families:\n            \n            def ode_system(t, y, V_max, K_M, sigma, A, T, omega, input_type):\n                S, z_Vmax, z_KM, J11, J12, J22 = y\n                \n                # Select input u(t)\n                if input_type == 'step':\n                    u = A\n                elif input_type == 'ramp':\n                    u = A * t / T\n                elif input_type == 'sin':\n                    u = A * np.sin(omega * t)\n                else:\n                    u = 0.0\n\n                # Numerical stability for denominator\n                denom = K_M + S\n                # Safeguard against division by small numbers. S should be non-negative.\n                if denom  1e-12:\n                    denom = 1e-12\n                \n                denom_sq = denom**2\n                \n                # State equation\n                dS_dt = -V_max * S / denom + u\n\n                # Sensitivity equations\n                df_dS = -V_max * K_M / denom_sq\n                df_dVmax = -S / denom\n                df_dKM = V_max * S / denom_sq\n                \n                dz_Vmax_dt = df_dS * z_Vmax + df_dVmax\n                dz_KM_dt = df_dS * z_KM + df_dKM\n\n                # FIM component equations\n                sigma_sq = sigma**2\n                dJ11_dt = z_Vmax**2 / sigma_sq\n                dJ12_dt = z_Vmax * z_KM / sigma_sq\n                dJ22_dt = z_KM**2 / sigma_sq\n\n                return [dS_dt, dz_Vmax_dt, dz_KM_dt, dJ11_dt, dJ12_dt, dJ22_dt]\n\n            y0 = [S0, 0, 0, 0, 0, 0]\n            \n            sol = solve_ivp(\n                fun=ode_system,\n                t_span=[0, T],\n                y0=y0,\n                args=(V_max, K_M, sigma, A, T, omega, input_type),\n                method='RK45', \n                rtol=1e-6, \n                atol=1e-9\n            )\n            \n            # Extract final FIM components\n            final_values = sol.y[:, -1]\n            J11_T = final_values[3]\n            J12_T = final_values[4]\n            J22_T = final_values[5]\n            \n            # Calculate determinant of FIM\n            det_fim = J11_T * J22_T - J12_T**2\n            determinants.append(det_fim)\n\n        # Find index of the input family that maximizes the determinant\n        # np.argmax returns the index of the first occurrence of the maximum,\n        # which satisfies the tie-breaking rule (smallest index wins).\n        best_index = np.argmax(determinants)\n        results.append(best_index)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}