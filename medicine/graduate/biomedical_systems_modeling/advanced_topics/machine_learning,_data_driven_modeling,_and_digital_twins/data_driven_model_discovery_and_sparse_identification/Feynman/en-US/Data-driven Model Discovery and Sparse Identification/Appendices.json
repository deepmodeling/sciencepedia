{
    "hands_on_practices": [
        {
            "introduction": "Before deploying any data-driven model, it is crucial to understand its potential sources of error. This foundational exercise  guides you through a mathematical derivation to quantify how two ubiquitous challenges—measurement noise and the numerical approximation of derivatives—introduce systematic bias into the parameters identified by SINDy. By completing this analysis, you will gain a quantitative intuition for the fundamental trade-offs between sampling rate, noise level, and the accuracy of your discovered model.",
            "id": "3880596",
            "problem": "Consider a single-biomarker dynamical system in a biomedical assay near equilibrium, modeled by the scalar ordinary differential equation $\\frac{dx}{dt} = \\alpha x$ with deterministic trajectory $x(t) = x_0 \\exp(\\alpha t)$, where $\\alpha$ is an unknown constant rate parameter. Measurements are obtained at uniform times $t_k = k\\Delta t$ for $k = 0,1,\\dots,N$, with additive measurement noise modeled as $x_k = x(t_k) + \\epsilon_k$, where $\\epsilon_k$ are independent, identically distributed, zero-mean Gaussian random variables with variance $\\sigma^2$. The derivative is estimated by the forward finite difference of the noisy measurements:\n$$y_k = \\frac{x_{k+1} - x_k}{\\Delta t},\\quad k = 0,1,\\dots,N-1,$$\nand a Sparse Identification of Nonlinear Dynamics (SINDy) regression with a single library function $\\phi(x)=x$ is performed using ordinary least squares to estimate $\\alpha$ by regressing $y_k$ onto the predictor $z_k = x_k$. The ordinary least squares estimator is\n$$\\hat{\\alpha} = \\frac{\\sum_{k=0}^{N-1} z_k y_k}{\\sum_{k=0}^{N-1} z_k^2}.$$\n\nStarting from the fundamental definitions of the derivative and the properties of forward finite differences and ordinary least squares, derive the expected bias $\\mathbb{E}[\\hat{\\alpha}] - \\alpha$ induced jointly by the finite difference derivative approximation and the measurement noise. Assume $\\alpha \\Delta t \\neq 0$ so that geometric series have a nontrivial ratio, and justify passing expectations inside ratios by appealing to large-sample consistency (i.e., that the ratio of random sums converges to the ratio of their expectations under the law of large numbers and Slutsky’s theorem). Express your final answer as a single closed-form analytic expression in terms of $\\alpha$, $x_0$, $\\Delta t$, $N$, and $\\sigma^2$. No numerical evaluation is required.",
            "solution": "The problem asks for the expected bias of an ordinary least squares (OLS) estimator for the rate constant $\\alpha$ in the dynamical system $\\frac{dx}{dt} = \\alpha x$. The bias arises from two sources: the use of a forward finite difference to approximate the time derivative and the presence of additive measurement noise.\n\nThe provided OLS estimator for $\\alpha$ is:\n$$\n\\hat{\\alpha} = \\frac{\\sum_{k=0}^{N-1} z_k y_k}{\\sum_{k=0}^{N-1} z_k^2}\n$$\nwhere $z_k = x_k$ is the noisy measurement and $y_k = \\frac{x_{k+1} - x_k}{\\Delta t}$ is the estimated derivative. The problem instructs us to approximate the expectation of this ratio by the ratio of the expectations, a valid approach for large sample sizes $N$ where the denominator converges to a non-zero constant. Thus, we compute:\n$$\n\\mathbb{E}[\\hat{\\alpha}] \\approx \\frac{\\mathbb{E}\\left[\\sum_{k=0}^{N-1} z_k y_k\\right]}{\\mathbb{E}\\left[\\sum_{k=0}^{N-1} z_k^2\\right]}\n$$\nWe will compute the expected value of the numerator and the denominator separately.\n\nFirst, we define the components. The true trajectory is $x(t_k) = x_0 \\exp(\\alpha k \\Delta t)$. The noisy measurement is $x_k = x(t_k) + \\epsilon_k$, where $\\epsilon_k$ are i.i.d. Gaussian random variables with $\\mathbb{E}[\\epsilon_k] = 0$ and $\\mathbb{E}[\\epsilon_k^2] = \\sigma^2$. The independence implies $\\mathbb{E}[\\epsilon_i \\epsilon_j] = 0$ for $i \\neq j$. The regressor is $z_k = x_k$. The derivative estimate is $y_k = \\frac{x_{k+1} - x_k}{\\Delta t} = \\frac{(x(t_{k+1}) + \\epsilon_{k+1}) - (x(t_k) + \\epsilon_k)}{\\Delta t}$.\n\nLet's calculate the expected value of the denominator, $E_{den} = \\mathbb{E}\\left[\\sum_{k=0}^{N-1} z_k^2\\right]$. Using the linearity of expectation:\n$$\nE_{den} = \\sum_{k=0}^{N-1} \\mathbb{E}[z_k^2] = \\sum_{k=0}^{N-1} \\mathbb{E}[(x(t_k) + \\epsilon_k)^2]\n$$\nExpanding the square and taking the expectation of each term:\n$$\n\\mathbb{E}[z_k^2] = \\mathbb{E}[x(t_k)^2 + 2x(t_k)\\epsilon_k + \\epsilon_k^2] = \\mathbb{E}[x(t_k)^2] + 2x(t_k)\\mathbb{E}[\\epsilon_k] + \\mathbb{E}[\\epsilon_k^2]\n$$\nSince $x(t_k)$ is deterministic, $\\mathbb{E}[x(t_k)^2] = x(t_k)^2$. With $\\mathbb{E}[\\epsilon_k]=0$ and $\\mathbb{E}[\\epsilon_k^2]=\\sigma^2$, we have:\n$$\n\\mathbb{E}[z_k^2] = x(t_k)^2 + \\sigma^2 = (x_0 \\exp(\\alpha k \\Delta t))^2 + \\sigma^2 = x_0^2 \\exp(2\\alpha k \\Delta t) + \\sigma^2\n$$\nSumming from $k=0$ to $N-1$:\n$$\nE_{den} = \\sum_{k=0}^{N-1} (x_0^2 \\exp(2\\alpha k \\Delta t) + \\sigma^2) = x_0^2 \\sum_{k=0}^{N-1} (\\exp(2\\alpha \\Delta t))^k + \\sum_{k=0}^{N-1} \\sigma^2\n$$\nThe first term is a geometric series with ratio $r = \\exp(2\\alpha \\Delta t)$. The sum is $\\frac{r^N-1}{r-1} = \\frac{\\exp(2\\alpha N \\Delta t) - 1}{\\exp(2\\alpha \\Delta t) - 1}$. Let's denote this sum by $S_{geom}$.\n$$\nE_{den} = x_0^2 S_{geom} + N\\sigma^2\n$$\n\nNext, we calculate the expected value of the numerator, $E_{num} = \\mathbb{E}\\left[\\sum_{k=0}^{N-1} z_k y_k\\right]$.\n$$\nE_{num} = \\sum_{k=0}^{N-1} \\mathbb{E}[z_k y_k] = \\sum_{k=0}^{N-1} \\mathbb{E}\\left[ (x(t_k) + \\epsilon_k) \\left( \\frac{x(t_{k+1}) - x(t_k)}{\\Delta t} + \\frac{\\epsilon_{k+1} - \\epsilon_k}{\\Delta t} \\right) \\right]\n$$\nWe expand the product and take the expectation of the four resulting terms:\n1.  $\\mathbb{E}\\left[ x(t_k) \\frac{x(t_{k+1}) - x(t_k)}{\\Delta t} \\right] = x(t_k) \\frac{x(t_{k+1}) - x(t_k)}{\\Delta t}$ (deterministic).\n2.  $\\mathbb{E}\\left[ x(t_k) \\frac{\\epsilon_{k+1} - \\epsilon_k}{\\Delta t} \\right] = \\frac{x(t_k)}{\\Delta t} (\\mathbb{E}[\\epsilon_{k+1}] - \\mathbb{E}[\\epsilon_k]) = 0$.\n3.  $\\mathbb{E}\\left[ \\epsilon_k \\frac{x(t_{k+1}) - x(t_k)}{\\Delta t} \\right] = \\frac{x(t_{k+1}) - x(t_k)}{\\Delta t} \\mathbb{E}[\\epsilon_k] = 0$.\n4.  $\\mathbb{E}\\left[ \\epsilon_k \\frac{\\epsilon_{k+1} - \\epsilon_k}{\\Delta t} \\right] = \\frac{1}{\\Delta t} (\\mathbb{E}[\\epsilon_k \\epsilon_{k+1}] - \\mathbb{E}[\\epsilon_k^2]) = \\frac{1}{\\Delta t} (0 - \\sigma^2) = -\\frac{\\sigma^2}{\\Delta t}$.\n\nCombining these, the expected value of the product is:\n$$\n\\mathbb{E}[z_k y_k] = x(t_k) \\frac{x(t_{k+1}) - x(t_k)}{\\Delta t} - \\frac{\\sigma^2}{\\Delta t}\n$$\nSubstituting the expression for $x(t_k)$:\n$$\n\\mathbb{E}[z_k y_k] = x_0 \\exp(\\alpha k \\Delta t) \\frac{x_0 \\exp(\\alpha (k+1) \\Delta t) - x_0 \\exp(\\alpha k \\Delta t)}{\\Delta t} - \\frac{\\sigma^2}{\\Delta t}\n$$\n$$\n= \\frac{x_0^2}{\\Delta t} \\exp(\\alpha k \\Delta t) \\exp(\\alpha k \\Delta t) (\\exp(\\alpha \\Delta t) - 1) - \\frac{\\sigma^2}{\\Delta t} = \\frac{x_0^2}{\\Delta t} (\\exp(\\alpha \\Delta t) - 1) \\exp(2\\alpha k \\Delta t) - \\frac{\\sigma^2}{\\Delta t}\n$$\nSumming from $k=0$ to $N-1$:\n$$\nE_{num} = \\sum_{k=0}^{N-1} \\left( \\frac{x_0^2}{\\Delta t} (\\exp(\\alpha \\Delta t) - 1) \\exp(2\\alpha k \\Delta t) - \\frac{\\sigma^2}{\\Delta t} \\right)\n$$\n$$\n= \\frac{x_0^2}{\\Delta t} (\\exp(\\alpha \\Delta t) - 1) \\sum_{k=0}^{N-1} \\exp(2\\alpha k \\Delta t) - \\sum_{k=0}^{N-1} \\frac{\\sigma^2}{\\Delta t} = \\frac{x_0^2}{\\Delta t} (\\exp(\\alpha \\Delta t) - 1) S_{geom} - \\frac{N\\sigma^2}{\\Delta t}\n$$\n\nNow, we can write the expression for the expected bias, $\\mathbb{E}[\\hat{\\alpha}] - \\alpha$:\n$$\n\\mathbb{E}[\\hat{\\alpha}] - \\alpha \\approx \\frac{E_{num}}{E_{den}} - \\alpha = \\frac{E_{num} - \\alpha E_{den}}{E_{den}}\n$$\nLet's compute the numerator of the bias, $E_{num} - \\alpha E_{den}$:\n$$\nE_{num} - \\alpha E_{den} = \\left( \\frac{x_0^2}{\\Delta t} (\\exp(\\alpha \\Delta t) - 1) S_{geom} - \\frac{N\\sigma^2}{\\Delta t} \\right) - \\alpha (x_0^2 S_{geom} + N\\sigma^2)\n$$\nGroup terms by $S_{geom}$ and $N\\sigma^2$:\n$$\n= x_0^2 S_{geom} \\left( \\frac{\\exp(\\alpha \\Delta t) - 1}{\\Delta t} - \\alpha \\right) - N\\sigma^2 \\left( \\frac{1}{\\Delta t} + \\alpha \\right)\n$$\n$$\n= x_0^2 S_{geom} \\left( \\frac{\\exp(\\alpha \\Delta t) - 1 - \\alpha\\Delta t}{\\Delta t} \\right) - N\\sigma^2 \\left( \\frac{1 + \\alpha\\Delta t}{\\Delta t} \\right)\n$$\nPutting it all together, the bias is:\n$$\n\\mathbb{E}[\\hat{\\alpha}] - \\alpha \\approx \\frac{x_0^2 S_{geom} \\left( \\frac{\\exp(\\alpha \\Delta t) - 1 - \\alpha\\Delta t}{\\Delta t} \\right) - N\\sigma^2 \\left( \\frac{1 + \\alpha\\Delta t}{\\Delta t} \\right)}{x_0^2 S_{geom} + N\\sigma^2}\n$$\nWe can factor out $\\frac{1}{\\Delta t}$ from the numerator:\n$$\n\\mathbb{E}[\\hat{\\alpha}] - \\alpha \\approx \\frac{x_0^2 S_{geom} (\\exp(\\alpha \\Delta t) - 1 - \\alpha\\Delta t) - N\\sigma^2 (1 + \\alpha\\Delta t)}{\\Delta t(x_0^2 S_{geom} + N\\sigma^2)}\n$$\nFinally, substituting the closed-form expression for the geometric sum $S_{geom} = \\frac{\\exp(2\\alpha N \\Delta t) - 1}{\\exp(2\\alpha \\Delta t) - 1}$ gives the final result.\n\nThe final expression for the bias is:\n$$\n\\frac{x_0^2 \\left(\\frac{\\exp(2\\alpha N \\Delta t) - 1}{\\exp(2\\alpha \\Delta t) - 1}\\right) (\\exp(\\alpha \\Delta t) - 1 - \\alpha\\Delta t) - N\\sigma^2 (1 + \\alpha\\Delta t)}{\\Delta t \\left[ x_0^2 \\left(\\frac{\\exp(2\\alpha N \\Delta t) - 1}{\\exp(2\\alpha \\Delta t) - 1}\\right) + N\\sigma^2 \\right]}\n$$\nThis expression encapsulates the two sources of bias: the term proportional to $(\\exp(\\alpha \\Delta t) - 1 - \\alpha\\Delta t)$ arises from the finite difference approximation error, and the term proportional to $\\sigma^2$ arises from measurement noise.",
            "answer": "$$\n\\boxed{\\frac{x_0^2 \\left(\\frac{\\exp(2\\alpha N \\Delta t) - 1}{\\exp(2\\alpha \\Delta t) - 1}\\right) \\left(\\exp(\\alpha \\Delta t) - 1 - \\alpha\\Delta t\\right) - N\\sigma^2 (1 + \\alpha\\Delta t)}{\\Delta t \\left(x_0^2 \\left(\\frac{\\exp(2\\alpha N \\Delta t) - 1}{\\exp(2\\alpha \\Delta t) - 1}\\right) + N\\sigma^2\\right)}}\n$$"
        },
        {
            "introduction": "Purely data-driven methods can sometimes yield models that are inconsistent with known scientific principles. This practice  demonstrates a powerful technique to create more robust and interpretable models by embedding biophysical knowledge directly into the SINDy framework. You will first derive sign constraints to enforce monotonic repression in a gene regulatory network and then apply these constraints to solve a practical parameter estimation problem.",
            "id": "3880595",
            "problem": "Consider a single-gene module in a transcriptional regulatory network, where the gene product concentration is denoted by $y(t)$ and a repressor protein concentration is denoted by $r(t)$. Assume $r(t) \\geq 0$ for all measured times. In the Sparse Identification of Nonlinear Dynamics (SINDy) framework, a candidate model for the gene’s rate of change is built by linearly combining functions from a library. Let the library for the $y$-equation be the four-term collection $\\{1, r, r^{2}, y\\}$, and suppose we seek a model of the form\n$$\n\\dot{y}(t) = \\xi_{0}\\cdot 1 + \\xi_{r}\\cdot r(t) + \\xi_{r^{2}}\\cdot r(t)^{2} + \\xi_{y}\\cdot y(t),\n$$\nwhere the coefficients are collected in the coefficient matrix $\\Xi$, and for this single equation they appear in the row vector $(\\xi_{0}, \\xi_{r}, \\xi_{r^{2}}, \\xi_{y})$.\n\nA biophysically consistent representation of repression requires that increasing the repressor concentration $r$ does not increase the production rate of $y$. Formally, monotonic repression requires\n$$\n\\frac{\\partial \\dot{y}}{\\partial r}(t) \\leq 0 \\quad \\text{for all } r(t) \\geq 0.\n$$\nDerive the sign constraints on the relevant coefficients in $\\Xi$ that are sufficient to enforce this monotonicity, using only the facts that $\\frac{\\partial}{\\partial r} r = 1$ and $\\frac{\\partial}{\\partial r} r^{2} = 2r$, together with the assumption $r \\geq 0$.\n\nNext, estimate the repressor coefficient $\\xi_{r}$ under these sign constraints using constrained least squares for the single-parameter subproblem in which $\\xi_{0}$ and $\\xi_{y}$ have been fixed by prior identification and their contributions have been subtracted from the measured $\\dot{y}$. Concretely, let the regression vector for the repressor be\n$$\n\\theta = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix},\n$$\nand let the residual vector after removing the identified constant and degradation contributions be\n$$\na = \\begin{pmatrix} 0.5 \\\\ 1 \\\\ 1.5 \\end{pmatrix}.\n$$\nCompute the value of $\\xi_{r}$ that minimizes\n$$\n\\| a + \\xi_{r} \\theta \\|_{2}^{2}\n$$\nsubject to the monotonic repression sign constraint you derived. Express your final answer as an exact value with no rounding, and treat all quantities as dimensionless (no units).",
            "solution": "The problem consists of two parts. The first part is to derive sign constraints on the model coefficients to enforce a biophysical principle. The second part is to estimate one of these coefficients using constrained least squares.\n\n**Part 1: Derivation of Sign Constraints for Monotonic Repression**\n\nWe are given a model for the rate of change of a gene product's concentration, $y(t)$, as a function of the repressor concentration, $r(t) \\geq 0$. The model is:\n$$\n\\dot{y}(t) = \\xi_{0} + \\xi_{r}r(t) + \\xi_{r^{2}}r(t)^{2} + \\xi_{y}y(t)\n$$\nThe biophysical constraint of monotonic repression requires that an increase in the repressor concentration $r$ must not lead to an increase in the production rate $\\dot{y}$. Mathematically, this is expressed as:\n$$\n\\frac{\\partial \\dot{y}}{\\partial r} \\leq 0 \\quad \\text{for all } r \\geq 0\n$$\nTo find the implications of this constraint on the coefficients $\\xi_{r}$ and $\\xi_{r^{2}}$, we first compute the partial derivative of $\\dot{y}$ with respect to $r$. In this context, the coefficients $\\xi_i$ and the state variable $y$ are treated as independent of $r$.\n$$\n\\frac{\\partial \\doty}{\\partial r} = \\frac{\\partial}{\\partial r} \\left( \\xi_{0} + \\xi_{r}r + \\xi_{r^{2}}r^{2} + \\xi_{y}y \\right)\n$$\nUsing the linearity of the derivative operator and the provided derivatives $\\frac{\\partial}{\\partial r} r = 1$ and $\\frac{\\partial}{\\partial r} r^{2} = 2r$, we obtain:\n$$\n\\frac{\\partial \\dot{y}}{\\partial r} = \\xi_{r} \\frac{\\partial r}{\\partial r} + \\xi_{r^{2}} \\frac{\\partial r^{2}}{\\partial r} = \\xi_{r}(1) + \\xi_{r^{2}}(2r)\n$$\n$$\n\\frac{\\partial \\dot{y}}{\\partial r} = \\xi_{r} + 2\\xi_{r^{2}}r\n$$\nThe monotonic repression constraint thus becomes:\n$$\n\\xi_{r} + 2\\xi_{r^{2}}r \\leq 0 \\quad \\text{for all } r \\geq 0\n$$\nThis inequality is a linear function of $r$ and must hold for all non-negative values of $r$. Let us analyze this condition.\nFirst, consider the case where $r=0$. The inequality must hold at this boundary, which gives:\n$$\n\\xi_{r} + 2\\xi_{r^{2}}(0) \\leq 0 \\implies \\xi_{r} \\leq 0\n$$\nSo, the coefficient $\\xi_r$ must be non-positive.\n\nNext, consider the behavior for $r > 0$. The term $2\\xi_{r^{2}}r$ determines the slope of the function.\nIf we were to assume $\\xi_{r^{2}} > 0$, then for a sufficiently large positive $r$, the term $2\\xi_{r^{2}}r$ would become arbitrarily large and positive. It would eventually dominate the (non-positive) constant term $\\xi_r$, causing the entire expression $\\xi_{r} + 2\\xi_{r^{2}}r$ to become positive. This would violate the inequality. Therefore, the case $\\xi_{r^{2}} > 0$ is inadmissible.\nFor the inequality to hold for all $r \\geq 0$, we must have $\\xi_{r^{2}} \\leq 0$.\nIf both $\\xi_r \\leq 0$ and $\\xi_{r^{2}} \\leq 0$, then for any $r \\geq 0$, the term $\\xi_r$ is non-positive and the term $2\\xi_{r^{2}}r$ is also non-positive. Their sum, $\\xi_{r} + 2\\xi_{r^{2}}r$, must therefore be non-positive.\n\nThus, the sufficient sign constraints on the relevant coefficients to enforce monotonic repression are $\\xi_{r} \\leq 0$ and $\\xi_{r^{2}} \\leq 0$. The subproblem in the second part only involves $\\xi_r$, so the relevant constraint for that part is $\\xi_r \\leq 0$.\n\n**Part 2: Constrained Least Squares Estimation of $\\xi_r$**\n\nWe are tasked with finding the value of $\\xi_r$ that minimizes the objective function $J(\\xi_r) = \\| a + \\xi_{r} \\theta \\|_{2}^{2}$, subject to the constraint $\\xi_r \\leq 0$ derived in Part 1.\nThe given vectors are:\n$$\na = \\begin{pmatrix} 0.5 \\\\ 1 \\\\ 1.5 \\end{pmatrix} \\quad \\text{and} \\quad \\theta = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}\n$$\nThe objective function is the squared Euclidean norm of the vector $a + \\xi_r \\theta$. We can express this as a scalar function of $\\xi_r$:\n$$\nJ(\\xi_r) = (a + \\xi_r \\theta)^{T} (a + \\xi_r \\theta)\n$$\nExpanding this expression, we get:\n$$\nJ(\\xi_r) = a^T a + a^T(\\xi_r \\theta) + (\\xi_r \\theta)^T a + (\\xi_r \\theta)^T(\\xi_r \\theta)\n$$\n$$\nJ(\\xi_r) = a^T a + 2\\xi_r(a^T \\theta) + \\xi_r^2(\\theta^T \\theta)\n$$\nThis is a quadratic function of $\\xi_r$. To find the unconstrained minimum, we differentiate $J(\\xi_r)$ with respect to $\\xi_r$ and set the result to zero.\n$$\n\\frac{dJ}{d\\xi_r} = 2(a^T \\theta) + 2\\xi_r(\\theta^T \\theta)\n$$\nSetting $\\frac{dJ}{d\\xi_r} = 0$ gives the unconstrained optimal value, which we denote as $\\xi_r^*$:\n$$\n2(a^T \\theta) + 2\\xi_r^*(\\theta^T \\theta) = 0 \\implies \\xi_r^* = - \\frac{a^T \\theta}{\\theta^T \\theta}\n$$\nNow, we compute the scalar products $a^T \\theta$ and $\\theta^T \\theta$:\n$$\na^T \\theta = \\begin{pmatrix} 0.5 & 1 & 1.5 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix} = (0.5)(1) + (1)(2) + (1.5)(3) = 0.5 + 2 + 4.5 = 7\n$$\n$$\n\\theta^T \\theta = \\begin{pmatrix} 1 & 2 & 3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix} = 1^2 + 2^2 + 3^2 = 1 + 4 + 9 = 14\n$$\nSubstituting these values into the expression for $\\xi_r^*$:\n$$\n\\xi_r^* = - \\frac{7}{14} = - \\frac{1}{2} = -0.5\n$$\nThe final step is to apply the constraint $\\xi_r \\leq 0$. The unconstrained solution is $\\xi_r^* = -0.5$. Since this value satisfies the constraint (i.e., $-0.5 \\leq 0$), the constrained minimum is the same as the unconstrained minimum. The objective function $J(\\xi_r)$ is a parabola opening upwards (since its quadratic coefficient $\\theta^T\\theta = 14$ is positive), and its vertex lies within the feasible region defined by the constraint.\nTherefore, the value of $\\xi_r$ that minimizes the objective function subject to the monotonic repression constraint is $-0.5$.",
            "answer": "$$\n\\boxed{-0.5}\n$$"
        },
        {
            "introduction": "Modern model discovery relies on computationally intensive methods like cross-validation and stability selection to ensure robustness. This exercise  addresses the practical challenge of computational scaling, asking you to design and analyze a pipeline for parallelizing a large SINDy workload. Mastering these concepts is essential for efficiently executing the large-scale computational experiments that underpin contemporary biomedical systems modeling.",
            "id": "3880519",
            "problem": "You are given a family of independent data-fitting tasks that arise in Sparse Identification of Nonlinear Dynamics (SINDy) for biomedical systems modeling. The SINDy approach seeks sparse coefficients in a linear-in-parameters model for the time derivative, written as $$\\dot{\\mathbf{x}}(t) = \\Theta(\\mathbf{x}(t)) \\boldsymbol{\\Xi},$$ where $\\Theta(\\cdot)$ is a predetermined library of candidate functions and $\\boldsymbol{\\Xi}$ is a sparse matrix of coefficients. In practice, a typical pipeline for data-driven model discovery employs $K$-fold cross-validation and stability selection (i.e., repeated subsampling or bootstrapping) to assess the robustness of the discovered supports. Each fold-resample fit is an independent task that can be executed in parallel across multiple processor cores.\n\nStarting from fundamental principles, assume the following base model of computational cost for one SINDy fit on a single fold-resample task. Each fit uses iterative thresholded least squares over $J$ iterations, where each iteration is dominated by forming normal equations and solving a linear system. Using standard linear algebra cost models, assembling and solving steps scale on the order of matrix dimensions, which we approximate deterministically as a scalar time model:\n$$t_{\\text{task}}(i) = \\alpha \\, n_i \\, m_i^2 \\, J_i + \\beta,$$\nwhere $n_i$ is the number of samples for experiment $i$, $m_i$ is the number of candidate functions (columns) for experiment $i$, $J_i$ is the number of thresholding iterations for experiment $i$, $\\alpha$ is a proportionality constant that captures floating-point throughput and algorithmic constants in seconds per operation unit, and $\\beta$ is a constant per-task overhead in seconds. For each experiment $i$, there is also a serial, non-parallelizable setup time $g_i$ in seconds, accounting for steps such as data loading and library construction. All fold-resample tasks are independent and fully parallelizable; the $g_i$ are the only globally serial components.\n\nDefine the total number of tasks for experiment $i$ as\n$$T_i = K \\times B,$$\nwhere $K$ is the number of cross-validation folds and $B$ is the number of stability selection resamples per fold. Across $E$ experiments, the sequential wall-clock time is\n$$T_{\\text{seq}} = \\sum_{i=1}^E \\left( g_i + T_i \\, t_{\\text{task}}(i) \\right).$$\nOn a parallel system with $p$ identical cores, the makespan for the parallelizable tasks is modeled by non-preemptive list scheduling with the Longest Processing Time (LPT) rule: sort all task durations in descending order and greedily assign each task to the currently least-loaded core. Let $M(\\{t_k\\}_{k=1}^{N}, p)$ denote the resulting makespan for $N$ tasks on $p$ cores. Then the total parallel wall-clock time is\n$$T_{\\text{par}} = \\left( \\sum_{i=1}^E g_i \\right) + M\\Big(\\underbrace{\\{ \\overbrace{t_{\\text{task}}(1), \\dots, t_{\\text{task}}(1)}^{T_1 \\text{ times}}, \\dots, \\overbrace{t_{\\text{task}}(E), \\dots, t_{\\text{task}}(E)}^{T_E \\text{ times}} \\}}_{N=\\sum_i T_i \\text{ tasks}}, \\, p \\Big).$$\nDefine the achieved speedup as\n$$S_{\\text{par}} = \\frac{T_{\\text{seq}}}{T_{\\text{par}}},$$\nthe Amdahl-predicted speedup bound as\n$$S_{\\text{Amdahl}} = \\frac{1}{s + \\frac{1 - s}{p}}, \\quad s = \\frac{\\sum_{i=1}^E g_i}{T_{\\text{seq}}},$$\nand the parallel efficiency as\n$$\\eta = \\frac{S_{\\text{par}}}{p}.$$\n\nYour task is to write a program that, for each test case, constructs the multiexperiment SINDy workload using the model above, computes $T_{\\text{seq}}$, $T_{\\text{par}}$ via LPT scheduling, and then reports the triplet $\\left(S_{\\text{par}}, S_{\\text{Amdahl}}, \\eta\\right)$.\n\nAll answers must be numerical and unitless. Report all floating-point results rounded to six decimal places.\n\nUse the following test suite of four cases, each defined by the number of cores $p$, the cross-validation and stability selection parameters $(K,B)$, the proportionality constants $(\\alpha,\\beta)$, and a list of experiments with tuples $(n_i, m_i, J_i, g_i)$.\n\n- Test case $1$ (happy path, heterogeneous tasks):\n  - $p = 8$, $K = 5$, $B = 8$, $\\alpha = 2\\times 10^{-8}$, $\\beta = 0.02$.\n  - Experiments: $(n_1, m_1, J_1, g_1) = (800, 20, 10, 0.5)$, $(n_2, m_2, J_2, g_2) = (1200, 30, 12, 0.5)$, $(n_3, m_3, J_3, g_3) = (600, 15, 8, 0.5)$.\n\n- Test case $2$ (boundary, single core):\n  - $p = 1$, $K = 5$, $B = 8$, $\\alpha = 2\\times 10^{-8}$, $\\beta = 0.02$.\n  - Experiments identical to test case $1$.\n\n- Test case $3$ (edge, many cores with limited concurrency):\n  - $p = 256$, $K = 3$, $B = 4$, $\\alpha = 2\\times 10^{-8}$, $\\beta = 0.01$.\n  - Experiments: $(n_1, m_1, J_1, g_1) = (500, 25, 10, 0.3)$, $(n_2, m_2, J_2, g_2) = (1000, 40, 10, 0.3)$.\n\n- Test case $4$ (edge, identical tasks, zero serial overhead):\n  - $p = 16$, $K = 4$, $B = 8$, $\\alpha = 2\\times 10^{-8}$, $\\beta = 0.0$.\n  - Experiments: $(n_1, m_1, J_1, g_1) = (500, 20, 5, 0.0)$.\n\nYour program should produce a single line of output containing the results aggregated across the four test cases as a comma-separated list enclosed in square brackets. For each test case, append the three rounded values $\\left(S_{\\text{par}}, S_{\\text{Amdahl}}, \\eta\\right)$ in that order, yielding a flat list of $12$ floats. For example, the output format should be\n$$[\\text{S}_{\\text{par}}^{(1)},\\text{S}_{\\text{Amdahl}}^{(1)},\\eta^{(1)},\\text{S}_{\\text{par}}^{(2)},\\text{S}_{\\text{Amdahl}}^{(2)},\\eta^{(2)},\\text{S}_{\\text{par}}^{(3)},\\text{S}_{\\text{Amdahl}}^{(3)},\\eta^{(3)},\\text{S}_{\\text{par}}^{(4)},\\text{S}_{\\text{Amdahl}}^{(4)},\\eta^{(4)}].$$\nNo additional text should be printed.",
            "solution": "The problem requires implementing a performance model for a parallel SINDy workload and computing three metrics: achieved speedup ($S_{\\text{par}}$), Amdahl-predicted speedup ($S_{\\text{Amdahl}}$), and parallel efficiency ($\\eta$). The solution involves following the specified calculation steps for several test cases.\n\nFirst, we calculate the computational time for a single SINDy fit task for a given experiment $i$ using the deterministic cost model:\n$$t_{\\text{task}}(i) = \\alpha \\, n_i \\, m_i^2 \\, J_i + \\beta$$\nwhere $n_i$ is the number of data samples, $m_i$ is the number of candidate functions, $J_i$ is the number of iterations, $\\alpha$ is a time constant, and $\\beta$ is a fixed overhead.\n\nFor each experiment $i$, the total number of independent tasks is:\n$$T_i = K \\times B$$\nwhere $K$ is the number of cross-validation folds and $B$ is the number of stability resamples.\n\nThe total sequential wall-clock time, $T_{\\text{seq}}$, is the sum of all serial setup times and the cumulative time of all tasks:\n$$T_{\\text{seq}} = \\sum_{i=1}^E \\left( g_i + T_i \\, t_{\\text{task}}(i) \\right)$$\nwhere $g_i$ is the non-parallelizable setup time for experiment $i$.\n\nThe total parallel wall-clock time, $T_{\\text{par}}$, on a system with $p$ cores is the sum of the serial setup times and the makespan of the parallelizable tasks. The makespan, $M$, is determined by scheduling all $N = \\sum_{i=1}^E T_i$ tasks on $p$ cores using the Longest Processing Time (LPT) scheduling heuristic.\n$$T_{\\text{par}} = \\left( \\sum_{i=1}^E g_i \\right) + M\\left(\\left\\{t_{\\text{task}, k}\\right\\}_{k=1}^{N}, p\\right)$$\nThe LPT algorithm is implemented by creating a list of all $N$ task durations, sorting it in descending order, and then iteratively assigning each task to the core with the minimum current load. The makespan $M$ is the maximum load across all cores after all tasks are assigned. A min-heap is an efficient data structure for tracking the least-loaded core.\n\nWith $T_{\\text{seq}}$ and $T_{\\text{par}}$ computed, we can find the performance metrics. The achieved speedup $S_{\\text{par}}$ is:\n$$S_{\\text{par}} = \\frac{T_{\\text{seq}}}{T_{\\text{par}}}$$\nThe Amdahl-predicted speedup, $S_{\\text{Amdahl}}$, is calculated based on the serial fraction $s$ of the program, which is the proportion of the sequential execution time that is non-parallelizable.\n$$s = \\frac{\\sum_{i=1}^E g_i}{T_{\\text{seq}}}$$\n$$S_{\\text{Amdahl}} = \\frac{1}{s + \\frac{1 - s}{p}}$$\nFinally, the parallel efficiency $\\eta$ measures how effectively the parallel resources are utilized:\n$$\\eta = \\frac{S_{\\text{par}}}{p}$$\nThe program will execute these calculations for each test case, round the resulting triplet $(S_{\\text{par}}, S_{\\text{Amdahl}}, \\eta)$ to six decimal places, and aggregate them into the final output list.",
            "answer": "```python\nimport heapq\nimport numpy as np\n\ndef lpt_scheduler(task_durations, num_cores):\n    \"\"\"\n    Calculates the makespan for a list of tasks on a set of cores\n    using the Longest Processing Time (LPT) scheduling algorithm.\n    This implementation uses a min-heap for efficiency.\n    \"\"\"\n    if not task_durations:\n        return 0.0\n\n    # A min-heap to store the current load of each core.\n    # This allows finding the least-loaded core in O(log p) time.\n    core_loads = [0.0] * num_cores\n    heapq.heapify(core_loads)\n    \n    # Sort tasks by duration in descending order.\n    sorted_tasks = sorted(task_durations, reverse=True)\n    \n    for task_duration in sorted_tasks:\n        # Get the core with the minimum load (earliest finish time).\n        min_load = heapq.heappop(core_loads)\n        \n        # Assign the current task to this core.\n        new_load = min_load + task_duration\n        \n        # Update the core's load in the heap.\n        heapq.heappush(core_loads, new_load)\n        \n    # The makespan is the time the last core finishes, which is the maximum load.\n    return max(core_loads)\n\ndef solve():\n    \"\"\"\n    Main function to process all test cases and compute performance metrics.\n    \"\"\"\n    test_cases = [\n        {\n            \"p\": 8, \"K\": 5, \"B\": 8, \"alpha\": 2e-8, \"beta\": 0.02,\n            \"experiments\": [\n                (800, 20, 10, 0.5),\n                (1200, 30, 12, 0.5),\n                (600, 15, 8, 0.5)\n            ]\n        },\n        {\n            \"p\": 1, \"K\": 5, \"B\": 8, \"alpha\": 2e-8, \"beta\": 0.02,\n            \"experiments\": [\n                (800, 20, 10, 0.5),\n                (1200, 30, 12, 0.5),\n                (600, 15, 8, 0.5)\n            ]\n        },\n        {\n            \"p\": 256, \"K\": 3, \"B\": 4, \"alpha\": 2e-8, \"beta\": 0.01,\n            \"experiments\": [\n                (500, 25, 10, 0.3),\n                (1000, 40, 10, 0.3)\n            ]\n        },\n        {\n            \"p\": 16, \"K\": 4, \"B\": 8, \"alpha\": 2e-8, \"beta\": 0.0,\n            \"experiments\": [\n                (500, 20, 5, 0.0)\n            ]\n        }\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        p = case[\"p\"]\n        K = case[\"K\"]\n        B = case[\"B\"]\n        alpha = case[\"alpha\"]\n        beta = case[\"beta\"]\n        \n        total_serial_setup_time = 0.0\n        total_parallel_work = 0.0\n        all_task_durations = []\n        \n        for n_i, m_i, J_i, g_i in case[\"experiments\"]:\n            # Calculate single task duration\n            t_task_i = alpha * n_i * (m_i ** 2) * J_i + beta\n            \n            # Calculate total tasks for this experiment type\n            T_i = K * B\n            \n            # Aggregate times and task list\n            total_serial_setup_time += g_i\n            total_parallel_work += T_i * t_task_i\n            all_task_durations.extend([t_task_i] * T_i)\n\n        # Calculate T_seq\n        T_seq = total_serial_setup_time + total_parallel_work\n        \n        # Calculate makespan M and T_par\n        makespan_M = lpt_scheduler(all_task_durations, p)\n        T_par = total_serial_setup_time + makespan_M\n        \n        # Calculate performance metrics\n        # Handle division by zero for T_par, though not expected in these cases.\n        S_par = T_seq / T_par if T_par > 0 else 0.0\n        \n        # Calculate serial fraction s\n        # Handle division by zero for T_seq, also not expected.\n        s = total_serial_setup_time / T_seq if T_seq > 0 else 0.0\n        \n        # Amdahl's Law\n        denominator_amdahl = s + (1 - s) / p\n        S_Amdahl = 1 / denominator_amdahl if denominator_amdahl > 0 else float('inf')\n\n        # Parallel efficiency\n        eta = S_par / p\n\n        # Append rounded results to the list\n        all_results.extend([\n            round(S_par, 6),\n            round(S_Amdahl, 6),\n            round(eta, 6)\n        ])\n\n    # Format the final output string as specified\n    output_str = \"[\" + \",\".join([f\"{r:.6f}\" for r in all_results]) + \"]\"\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}