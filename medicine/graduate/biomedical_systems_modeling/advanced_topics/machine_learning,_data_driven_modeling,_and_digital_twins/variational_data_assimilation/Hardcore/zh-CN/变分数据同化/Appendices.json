{
    "hands_on_practices": [
        {
            "introduction": "变分数据同化的核心思想源于贝叶斯推断，它将寻找最大后验（MAP）估计问题转化为一个优化问题。本练习通过一个最简单的标量示例，从第一性原理出发，引导您亲手推导变分代价函数。通过这个过程，您将清晰地理解代价函数中的背景项和观测项是如何从先验概率和似然函数中产生，以及它们各自的误差方差（$B$ 和 $R$）是如何作为权重来平衡模型预测和实际观测的 。",
            "id": "3864732",
            "problem": "在一个遥感与环境模拟系统中，考虑一个单网格单元环境状态变量 $x$，它表示分析时刻的柱平均示踪剂浓度。你正在使用一次卫星观测，进行三维变分（3D-Var）数据同化，这是四维变分（4D-Var）数据同化的时间无关极限。假设一个线性观测算子 $H=1$，因此观测模型为 $y=Hx+\\epsilon$，其中 $\\epsilon$ 是加性零均值高斯仪器噪声。先验（背景）状态被建模为一个均值为 $x_b$、方差为 $B$ 的高斯随机变量。观测误差是高斯的，方差为 $R$。在这些假设下，最大后验（MAP）估计等于从贝叶斯定理在线性高斯模型下推导出的二次三维变分代价函数的最小值点。\n\n从高斯先验 $p(x)$ 和高斯似然 $p(y\\mid x)$ 的定义出发，利用贝叶斯定理构建后验 $p(x\\mid y)$，推导出当 $H=1$ 时的标量最大后验估计量 $x_a$ 和相关的分析方差 $\\sigma_a^2$。然后，对于 $x_b=2$，$B=4$，$y=5$，$H=1$ 和 $R=1$ 的情况，计算你的表达式的值。\n\n将最终答案以单行矩阵 $\\begin{pmatrix}x_a & \\sigma_a^2\\end{pmatrix}$ 的形式报告。数值需表示为精确值，无需四舍五入。",
            "solution": "该问题是有效的，因为它具有科学依据、问题明确、客观，并包含获得唯一解所需的所有信息。它代表了贝叶斯推断在数据同化领域的一个标准应用。\n\n目标是推导最大后验（MAP）估计（表示为分析状态 $x_a$）以及相应的分析方差 $\\sigma_a^2$。推导从贝叶斯定理开始，该定理将给定观测 $y$ 时状态 $x$ 的后验概率与状态的先验概率以及给定状态时观测的似然联系起来：\n\n$$p(x \\mid y) = \\frac{p(y \\mid x) p(x)}{p(y)}$$\n\n项 $p(y)$ 是一个归一化常数，与 $x$ 无关。因此，为了找到使后验概率最大化的 $x$ 值，我们可以写成：\n\n$$p(x \\mid y) \\propto p(y \\mid x) p(x)$$\n\n问题指定了状态变量 $x$ 的高斯先验分布，其均值为 $x_b$（背景状态），方差为 $B$。其概率密度函数（PDF）为：\n\n$$p(x) = \\frac{1}{\\sqrt{2\\pi B}} \\exp\\left(-\\frac{1}{2} \\frac{(x - x_b)^2}{B}\\right)$$\n\n观测模型为 $y = Hx + \\epsilon$，其中 $H=1$，观测误差 $\\epsilon$ 来自一个均值为零、方差为 $R$ 的高斯分布。这定义了似然函数 $p(y \\mid x)$，它是一个以 $Hx=x$ 为中心、方差为 $R$ 的关于 $y$ 的高斯分布：\n\n$$p(y \\mid x) = \\frac{1}{\\sqrt{2\\pi R}} \\exp\\left(-\\frac{1}{2} \\frac{(y - x)^2}{R}\\right)$$\n\n将先验和似然的PDF代入后验的比例关系式中，得到：\n\n$$p(x \\mid y) \\propto \\left[ \\frac{1}{\\sqrt{2\\pi R}} \\exp\\left(-\\frac{1}{2} \\frac{(y - x)^2}{R}\\right) \\right] \\left[ \\frac{1}{\\sqrt{2\\pi B}} \\exp\\left(-\\frac{1}{2} \\frac{(x - x_b)^2}{B}\\right) \\right]$$\n\n合并指数项并忽略常数系数，我们得到：\n\n$$p(x \\mid y) \\propto \\exp\\left(-\\frac{1}{2} \\left[ \\frac{(x - x_b)^2}{B} + \\frac{(y - x)^2}{R} \\right]\\right)$$\n\n最大后验估计 $x_a$ 是使这个后验概率最大化的 $x$ 值。最大化 $p(x \\mid y)$ 等价于最小化其自然对数的负值。这定义了三维变分代价函数 $J(x)$：\n\n$$J(x) = \\frac{1}{2} \\left[ \\frac{(x - x_b)^2}{B} + \\frac{(y - x)^2}{R} \\right]$$\n\n为了找到 $J(x)$ 的最小值，我们计算它关于 $x$ 的一阶导数并令其为零：\n\n$$\\frac{dJ}{dx} = \\frac{1}{2} \\left[ \\frac{2(x - x_b)}{B} + \\frac{2(y - x)(-1)}{R} \\right] = \\frac{x - x_b}{B} - \\frac{y - x}{R}$$\n\n在 $x = x_a$ 处令导数为零：\n\n$$\\frac{x_a - x_b}{B} - \\frac{y - x_a}{R} = 0$$\n\n$$\\frac{x_a - x_b}{B} = \\frac{y - x_a}{R}$$\n\n求解 $x_a$：\n\n$$R(x_a - x_b) = B(y - x_a)$$\n$$Rx_a - Rx_b = By - Bx_a$$\n$$Rx_a + Bx_a = By + Rx_b$$\n$$x_a(R + B) = By + Rx_b$$\n\n这就得出了分析状态 $x_a$ 的最大后验估计量：\n\n$$x_a = \\frac{By + Rx_b}{B+R}$$\n\n后验分布 $p(x \\mid y)$ 本身是高斯的，因为它与两个高斯函数的乘积成正比。该后验分布的方差是分析方差 $\\sigma_a^2$。在变分数据同化中，分析（后验）协方差是在最小值点处计算的代价函数的海森矩阵的逆。对于这个标量问题，分析方差是 $J(x)$ 的二阶导数的逆。\n\n代价函数的二阶导数是：\n\n$$\\frac{d^2 J}{dx^2} = \\frac{d}{dx} \\left( \\frac{x - x_b}{B} - \\frac{y - x}{R} \\right) = \\frac{1}{B} + \\frac{1}{R}$$\n\n分析方差 $\\sigma_a^2$ 是此表达式的逆：\n\n$$\\sigma_a^2 = \\left(\\frac{d^2 J}{dx^2}\\right)^{-1} = \\left(\\frac{1}{B} + \\frac{1}{R}\\right)^{-1} = \\left(\\frac{R + B}{BR}\\right)^{-1} = \\frac{BR}{B+R}$$\n\n现在我们使用给定的数值计算这些表达式：$x_b=2$，$B=4$，$y=5$ 和 $R=1$。\n\n分析状态 $x_a$ 是：\n\n$$x_a = \\frac{(4)(5) + (1)(2)}{4+1} = \\frac{20 + 2}{5} = \\frac{22}{5}$$\n\n分析方差 $\\sigma_a^2$ 是：\n\n$$\\sigma_a^2 = \\frac{(4)(1)}{4+1} = \\frac{4}{5}$$\n\n最终答案是序对 $(x_a, \\sigma_a^2)$，以行矩阵的形式呈现。",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{22}{5} & \\frac{4}{5} \\end{pmatrix}}$$"
        },
        {
            "introduction": "在掌握了标量情况下的基本原理后，我们将问题推广到更贴近现实应用的多维系统。本练习引入了向量-矩阵形式的代价函数，让您熟悉在多维状态空间中进行数据同化所需的线性代数操作。您将学习如何使用背景误差协方差矩阵 $B$ 和观测算子 $H$ 来构建和求解正规方程，从而计算出分析增量和最终的分析状态 。",
            "id": "3864622",
            "problem": "考虑一个双分量环境状态向量 $x \\in \\mathbb{R}^{2}$，它代表了在线性设定下，从单一卫星派生观测中要分析的空间聚合量。假设一个源于高斯误差统计和线性观测算子的三维变分（3D-Var）数据同化框架，其中分析场 $x^{a}$ 最小化了由背景和观测不匹配度构建的二次代价函数。设背景误差协方差为 $B=\\begin{bmatrix}1 & 0 \\\\ 0 & 4\\end{bmatrix}$，观测算子为 $H=\\begin{bmatrix}1 & 1\\end{bmatrix}$，观测误差协方差为 $R=1$，背景场（也称先验）为 $x_{b}=\\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$，观测值为 $y=3$。\n\n从在线性高斯假设下结合背景和观测不匹配度的3D-Var代价函数定义出发，推导出表征最小值的一阶最优性条件（正规方程）。然后，使用给定的数值，计算分析增量 $\\delta x^{a}=x^{a}-x_{b}$ 和分析状态 $x^{a}$。将您的最终结果表示为顺序为 $\\big[\\delta x^{a}_{1},\\,\\delta x^{a}_{2},\\,x^{a}_{1},\\,x^{a}_{2}\\big]$ 的单行矩阵。\n\n无需单位。提供精确值，无需四舍五入。",
            "solution": "3D-Var分析状态 $x^a$ 是使代价函数 $J(x)$ 最小化的状态向量 $x$。代价函数衡量与背景状态和观测值的偏差，并由它们各自的误差协方差加权。对于线性高斯系统，其表达式为：\n$$J(x) = \\frac{1}{2}(x - x_b)^T B^{-1} (x - x_b) + \\frac{1}{2}(y - Hx)^T R^{-1} (y - Hx)$$\n为了找到最小值，我们计算 $J(x)$ 关于 $x$ 的梯度并将其设为零。梯度 $\\nabla_x J(x)$ 为：\n$$\\nabla_x J(x) = B^{-1}(x - x_b) - H^T R^{-1} (y - Hx)$$\n在 $x=x^a$ 处将梯度设为零，得到一阶最优性条件，也称为正规方程：\n$$B^{-1}(x^a - x_b) - H^T R^{-1} (y - Hx^a) = 0$$\n这就是所要求的正规方程的推导。\n\n我们被要求求解分析增量，定义为 $\\delta x^a = x^a - x_b$。我们将 $x^a = x_b + \\delta x^a$ 代入正规方程：\n$$B^{-1}(\\delta x^a) - H^T R^{-1} (y - H(x_b + \\delta x^a)) = 0$$\n$$B^{-1}(\\delta x^a) - H^T R^{-1} (y - Hx_b - H\\delta x^a) = 0$$\n重新整理各项以求解 $\\delta x^a$：\n$$B^{-1}(\\delta x^a) + H^T R^{-1} H\\delta x^a = H^T R^{-1} (y - Hx_b)$$\n$$(B^{-1} + H^T R^{-1} H) \\delta x^a = H^T R^{-1} (y - Hx_b)$$\n这个方程可以用来求解分析增量 $\\delta x^a$。\n\n现在，我们代入给定的数值：\n-   $x_b = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$\n-   $B = \\begin{bmatrix} 1 & 0 \\\\ 0 & 4 \\end{bmatrix} \\implies B^{-1} = \\begin{bmatrix} 1 & 0 \\\\ 0 & \\frac{1}{4} \\end{bmatrix}$\n-   $H = \\begin{bmatrix} 1 & 1 \\end{bmatrix} \\implies H^T = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$\n-   $R = 1 \\implies R^{-1} = 1$\n-   $y = 3$\n\n首先，让我们计算方程中求解 $\\delta x^a$ 的各个部分。\n项 $(y - Hx_b)$ 是新息：\n$$y - Hx_b = 3 - \\begin{bmatrix} 1 & 1 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} = 3 - 0 = 3$$\n方程的右侧是：\n$$H^T R^{-1} (y - Hx_b) = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} (1) (3) = \\begin{bmatrix} 3 \\\\ 3 \\end{bmatrix}$$\n方程左侧的矩阵是代价函数的Hessian矩阵：\n$$B^{-1} + H^T R^{-1} H = \\begin{bmatrix} 1 & 0 \\\\ 0 & \\frac{1}{4} \\end{bmatrix} + \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} (1) \\begin{bmatrix} 1 & 1 \\end{bmatrix}$$\n$$= \\begin{bmatrix} 1 & 0 \\\\ 0 & \\frac{1}{4} \\end{bmatrix} + \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} = \\begin{bmatrix} 2 & 1 \\\\ 1 & 1 + \\frac{1}{4} \\end{bmatrix} = \\begin{bmatrix} 2 & 1 \\\\ 1 & \\frac{5}{4} \\end{bmatrix}$$\n为了求解 $\\delta x^a$，我们需要对该矩阵求逆。其行列式为：\n$$\\det\\left(\\begin{bmatrix} 2 & 1 \\\\ 1 & \\frac{5}{4} \\end{bmatrix}\\right) = (2)\\left(\\frac{5}{4}\\right) - (1)(1) = \\frac{5}{2} - 1 = \\frac{3}{2}$$\n逆矩阵是：\n$$\\begin{bmatrix} 2 & 1 \\\\ 1 & \\frac{5}{4} \\end{bmatrix}^{-1} = \\frac{1}{\\frac{3}{2}} \\begin{bmatrix} \\frac{5}{4} & -1 \\\\ -1 & 2 \\end{bmatrix} = \\frac{2}{3} \\begin{bmatrix} \\frac{5}{4} & -1 \\\\ -1 & 2 \\end{bmatrix} = \\begin{bmatrix} \\frac{10}{12} & -\\frac{2}{3} \\\\ -\\frac{2}{3} & \\frac{4}{3} \\end{bmatrix} = \\begin{bmatrix} \\frac{5}{6} & -\\frac{2}{3} \\\\ -\\frac{2}{3} & \\frac{4}{3} \\end{bmatrix}$$\n现在我们可以求解 $\\delta x^a$：\n$$\\delta x^a = \\begin{bmatrix} \\frac{5}{6} & -\\frac{2}{3} \\\\ -\\frac{2}{3} & \\frac{4}{3} \\end{bmatrix} \\begin{bmatrix} 3 \\\\ 3 \\end{bmatrix} = \\begin{bmatrix} \\frac{5}{6}(3) - \\frac{2}{3}(3) \\\\ -\\frac{2}{3}(3) + \\frac{4}{3}(3) \\end{bmatrix} = \\begin{bmatrix} \\frac{5}{2} - 2 \\\\ -2 + 4 \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{2} \\\\ 2 \\end{bmatrix}$$\n所以，分析增量的分量是 $\\delta x^a_1 = \\frac{1}{2}$ 和 $\\delta x^a_2 = 2$。\n\n最后，我们计算分析状态 $x^a$：\n$$x^a = x_b + \\delta x^a = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} \\frac{1}{2} \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{2} \\\\ 2 \\end{bmatrix}$$\n分析状态的分量是 $x^a_1 = \\frac{1}{2}$ 和 $x^a_2 = 2$。\n\n最终结果要求以单行矩阵的形式，按顺序 $\\big[\\delta x^{a}_{1},\\,\\delta x^{a}_{2},\\,x^{a}_{1},\\,x^{a}_{2}\\big]$ 给出。这得到：\n$$\\begin{pmatrix} \\frac{1}{2} & 2 & \\frac{1}{2} & 2 \\end{pmatrix}$$",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{1}{2} & 2 & \\frac{1}{2} & 2 \\end{pmatrix}}$$"
        },
        {
            "introduction": "真实的生物医学系统不仅是动态演化的，其观测过程也未必遵循高斯统计。本练习将变分框架从静态的（3D-Var）推广至动态的（类4D-Var），并要求您比较高斯与泊松两种不同观测模型的同化效果。通过这个编码实践，您将体会到变分方法的灵活性，并深刻认识到为特定物理过程（如荧光成像中的光子计数）选择正确统计模型的重要性，尤其是在生物医学领域常见的低信噪比场景下 。",
            "id": "3942146",
            "problem": "考虑一个一维动态荧光成像实验，其中潜荧光强度状态序列 $\\{x_t\\}_{t=0}^{T-1}$ 根据一个线性时间离散衰减模型演化。数据同化任务是根据给定的模型和先验信息，从观测到的光子计数 $\\{y_t\\}$ 中推断出潜轨迹 $\\{x_t\\}$。\n\n基本原理和定义：\n- 用于后验推断的贝叶斯法则：后验密度与先验密度和似然函数的乘积成正比。\n- 最大后验（MAP）估计：使后验密度最大化的参数，等价于使负对数后验最小化。\n- 荧光的离散时间线性衰减模型：$x_{t+1} = a x_t + \\xi_t$，其中 $a = \\exp(-k \\Delta t)$，$k$ 为衰减率，$\\Delta t$ 为时间步长。模型失配 $\\xi_t$ 被建模为方差为 $q$ 的零均值高斯分布。\n- 初始状态的高斯先验：$x_0 \\sim \\mathcal{N}(m_0, p_0)$。\n- 将期望光子计数与潜状态联系起来的两种观测模型：\n  1. 泊松计数模型：$y_t \\sim \\mathrm{Poisson}(\\lambda_t)$，其中 $\\lambda_t = s x_t + b$，$s$ 是系统灵敏度（每单位荧光的计数），$b$ 是背景计数。\n  2. 高斯噪声模型：$y_t \\sim \\mathcal{N}(s x_t + b, \\sigma^2)$，其中方差 $\\sigma^2$ 为常数。\n\n任务：\n1. 严格从贝叶斯法则和上述定义出发，推导在泊松和高斯观测模型下的 MAP 估计问题。将每个问题表述为在物理约束 $x_t \\ge 0$（对所有 $t$）下，对轨迹 $\\{x_t\\}$ 最小化一个负对数后验目标。\n2. 设计一个有原则的变分数据同化算法，用于计算两种模型下的 MAP 轨迹。该算法应将动力学模型作为二次正则化（软约束）并应用边界约束 $x_t \\ge 0$。推导每个目标函数关于轨迹分量的梯度，以支持基于梯度的数值优化。通过要求 $s x_t + b > 0$ 来明确确保泊松情况下的定义域有效性。\n3. 对于以下测试套件，计算两种模型的 MAP 轨迹，然后计算每个 MAP 轨迹与真实轨迹之间的均方根误差 (RMSE)。对于每个测试用例，报告差值 $\\text{RMSE}_{\\text{Gaussian}} - \\text{RMSE}_{\\text{Poisson}}$ 的浮点数值。\n\n真实值和动力学参数：\n- 帧数：$T = 12$。\n- 时间步长：$\\Delta t = 1$ 秒。\n- 衰减率：$k = 0.2$ 每秒，因此 $a = \\exp(-k \\Delta t)$。\n- 初始状态真实值：$x_0^{\\mathrm{true}} = 2$（任意荧光单位）。\n- 真实轨迹：$x_t^{\\mathrm{true}} = x_0^{\\mathrm{true}} a^t$，其中 $t = 0,\\dots,11$。\n- $x_0$ 的先验均值和方差：$m_0 = 1.5$，$p_0 = 0.25$。\n- 模型失配方差：$q = 0.04$。\n\n三个测试用例的观测模型参数和观测光子计数：\n- 案例 1（高计数状态）：\n  - 灵敏度：$s = 80$ 计数/单位荧光。\n  - 背景：$b = 20$ 计数。\n  - 高斯噪声标准差：$\\sigma = 5$ 计数。\n  - 观测计数值列表：$[174, 152, 132, 111, 96, 76, 68, 61, 55, 47, 43, 36]$。\n- 案例 2（低计数状态）：\n  - 灵敏度：$s = 8$ 计数/单位荧光。\n  - 背景：$b = 1$ 计数。\n  - 高斯噪声标准差：$\\sigma = 3$ 计数。\n  - 观测计数值列表：$[16, 13, 12, 9, 8, 7, 6, 5, 6, 4, 4, 3]$。\n- 案例 3（高背景状态）：\n  - 灵敏度：$s = 15$ 计数/单位荧光。\n  - 背景：$b = 40$ 计数。\n  - 高斯噪声标准差：$\\sigma = 5$ 计数。\n  - 观测计数值列表：$[68, 63, 59, 58, 52, 53, 49, 48, 47, 45, 44, 43]$。\n\n算法要求和输出：\n- 对两种模型使用带有边界约束 $x_t \\ge 0$ 的基于梯度的凸优化方法。目标函数必须包括来自模型的二次动力学正则化项和相应的观测负对数似然项。\n- 计算每个模型相对于 $x_t^{\\mathrm{true}}$ 的 RMSE：\n  $$\\mathrm{RMSE} = \\sqrt{\\frac{1}{T} \\sum_{t=0}^{T-1} \\left(x_t^{\\mathrm{MAP}} - x_t^{\\mathrm{true}}\\right)^2}.$$\n- 最终输出格式：您的程序应生成单行输出，其中包含一个方括号括起来的、以逗号分隔的列表，列表包含三个结果。每个元素是相应测试用例的 $\\text{RMSE}_{\\text{Gaussian}} - \\text{RMSE}_{\\text{Poisson}}$ 的浮点值，四舍五入到六位小数。例如：$[r_1,r_2,r_3]$。",
            "solution": "该问题具有科学依据，提法恰当，客观，并包含推导唯一解所需的所有信息。这是一个变分数据同化中的标准问题，在地球科学中也称为 4D-Var，在统计学中称为 MAP 平滑。其物理模型和数学框架是标准且一致的。因此，该问题被认为是有效的。\n\n### 步骤 1：MAP 目标函数的推导\n\n目标是根据观测值 $\\mathbf{y} = \\{y_t\\}_{t=0}^{T-1}$，找到状态轨迹 $\\mathbf{x} = \\{x_t\\}_{t=0}^{T-1}$ 的最大后验 (MAP) 估计。根据贝叶斯法则，后验概率密度为：\n$$\np(\\mathbf{x} | \\mathbf{y}) \\propto p(\\mathbf{y} | \\mathbf{x}) p(\\mathbf{x})\n$$\n其中 $p(\\mathbf{y} | \\mathbf{x})$ 是似然，$p(\\mathbf{x})$ 是先验。MAP 估计通过最大化该后验概率得到，这等价于最小化其负对数：\n$$\nJ(\\mathbf{x}) = - \\log p(\\mathbf{x} | \\mathbf{y}) \\propto - \\log p(\\mathbf{y} | \\mathbf{x}) - \\log p(\\mathbf{x})\n$$\n我们将目标函数 $J(\\mathbf{x})$ 定义为负对数先验 $J_{prior}(\\mathbf{x})$ 和负对数似然（观测成本）$J_{obs}(\\mathbf{x})$ 的和。\n\n**1.1. 负对数先验项, $J_{prior}(\\mathbf{x})$**\n\n轨迹的先验 $p(\\mathbf{x})$ 由初始状态先验和动力学模型定义。状态构成一个马尔可夫链，因此 $p(\\mathbf{x}) = p(x_0) \\prod_{t=0}^{T-2} p(x_{t+1}|x_t)$。\n- 初始状态为高斯分布：$x_0 \\sim \\mathcal{N}(m_0, p_0)$，因此 $p(x_0) = \\frac{1}{\\sqrt{2\\pi p_0}} \\exp\\left(-\\frac{(x_0 - m_0)^2}{2p_0}\\right)$。\n- 动力学模型为 $x_{t+1} = a x_t + \\xi_t$，其中 $\\xi_t \\sim \\mathcal{N}(0, q)$。这意味着转移概率为 $p(x_{t+1}|x_t) = \\mathcal{N}(x_{t+1}; a x_t, q) = \\frac{1}{\\sqrt{2\\pi q}} \\exp\\left(-\\frac{(x_{t+1} - a x_t)^2}{2q}\\right)$。\n\n负对数先验（忽略常数项）为：\n$$\nJ_{prior}(\\mathbf{x}) = -\\log p(x_0) - \\sum_{t=0}^{T-2} \\log p(x_{t+1}|x_t) = \\frac{1}{2p_0}(x_0 - m_0)^2 + \\frac{1}{2q}\\sum_{t=0}^{T-2}(x_{t+1} - a x_t)^2\n$$\n此项将动力学模型表示为二次正则化，惩罚对模型动力学的偏离。\n\n**1.2. 负对数似然项, $J_{obs}(\\mathbf{x})$**\n\n假设观测值 $y_t$ 在给定状态 $x_t$ 的条件下是条件独立的。因此，总似然为 $p(\\mathbf{y} | \\mathbf{x}) = \\prod_{t=0}^{T-1} p(y_t | x_t)$。\n\n**1.2.1. 高斯观测模型**\n模型为 $y_t \\sim \\mathcal{N}(s x_t + b, \\sigma^2)$，其概率密度为 $p(y_t | x_t) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_t - (s x_t + b))^2}{2\\sigma^2}\\right)$。\n负对数似然（忽略常数）为：\n$$\nJ_{obs,G}(\\mathbf{x}) = \\sum_{t=0}^{T-1} \\frac{(s x_t + b - y_t)^2}{2\\sigma^2}\n$$\n\n**1.2.2. 泊松观测模型**\n模型为 $y_t \\sim \\mathrm{Poisson}(\\lambda_t)$，其中 $\\lambda_t = s x_t + b$。其概率质量函数为 $p(y_t | x_t) = \\frac{\\lambda_t^{y_t} e^{-\\lambda_t}}{y_t!}$。\n负对数似然（忽略常数项 $\\log(y_t!)$）为：\n$$\nJ_{obs,P}(\\mathbf{x}) = - \\sum_{t=0}^{T-1} (y_t \\log(\\lambda_t) - \\lambda_t) = \\sum_{t=0}^{T-1} (s x_t + b - y_t \\log(s x_t + b))\n$$\n必须强制执行物理约束 $x_t \\ge 0$（对所有 $t$）。此外，对数的参数必须为正：$s x_t + b > 0$。由于所有给定的参数 $s, b$ 都是正数，如果 $x_t \\ge 0$，这个条件会自动满足。\n\n**1.3. 完整的 MAP 目标函数**\n\n结合先验项和观测项，我们得到两个目标函数，需要在约束 $x_t \\ge 0$（对所有 $t=0, ..., T-1$）下进行最小化。\n\n**高斯模型目标函数：**\n$$\nJ_G(\\mathbf{x}) = \\frac{1}{2p_0}(x_0 - m_0)^2 + \\frac{1}{2q}\\sum_{t=0}^{T-2}(x_{t+1} - a x_t)^2 + \\frac{1}{2\\sigma^2}\\sum_{t=0}^{T-1} (s x_t + b - y_t)^2\n$$\n\n**泊松模型目标函数：**\n$$\nJ_P(\\mathbf{x}) = \\frac{1}{2p_0}(x_0 - m_0)^2 + \\frac{1}{2q}\\sum_{t=0}^{T-2}(x_{t+1} - a x_t)^2 + \\sum_{t=0}^{T-1} (s x_t + b - y_t \\log(s x_t + b))\n$$\n两个目标函数都是凸函数，这确保了基于梯度的优化器可以找到唯一的全局最小值。\n\n### 步骤 2：用于优化的梯度\n\n要使用基于梯度的优化器，我们需要每个目标函数相对于轨迹 $\\mathbf{x}$ 的每个分量 $x_k$ 的梯度。梯度 $\\nabla J(\\mathbf{x})$ 的分量为 $(\\nabla J(\\mathbf{x}))_k = \\frac{\\partial J}{\\partial x_k}$。\n\n**2.1. 先验项的梯度**\n先验项对两个模型是相同的。其梯度分量为：\n$$\n\\frac{\\partial J_{prior}}{\\partial x_k} =\n\\begin{cases}\n\\frac{1}{p_0}(x_0 - m_0) - \\frac{a}{q}(x_1 - a x_0) & \\text{if } k=0 \\\\\n\\frac{1}{q}(x_k - a x_{k-1}) - \\frac{a}{q}(x_{k+1} - a x_k) & \\text{if } 0 < k < T-1 \\\\\n\\frac{1}{q}(x_{T-1} - a x_{T-2}) & \\text{if } k=T-1\n\\end{cases}\n$$\n\n**2.2. 观测项的梯度**\n观测成本项的导数仅影响梯度的第 $k$ 个分量。\n\n**高斯模型：**\n$$\n\\frac{\\partial J_{obs,G}}{\\partial x_k} = \\frac{\\partial}{\\partial x_k} \\left( \\frac{(s x_k + b - y_k)^2}{2\\sigma^2} \\right) = \\frac{s(s x_k + b - y_k)}{\\sigma^2}\n$$\n\n**泊松模型：**\n$$\n\\frac{\\partial J_{obs,P}}{\\partial x_k} = \\frac{\\partial}{\\partial x_k} (s x_k + b - y_k \\log(s x_k + b)) = s - \\frac{s y_k}{s x_k + b} = s\\left(1 - \\frac{y_k}{s x_k + b}\\right)\n$$\n\n**2.3. 完整梯度**\n完整梯度是先验梯度和观测梯度的和：$\\nabla J(\\mathbf{x}) = \\nabla J_{prior}(\\mathbf{x}) + \\nabla J_{obs}(\\mathbf{x})$。对于每个分量 $k$：\n$(\\nabla J_G(\\mathbf{x}))_k = \\frac{\\partial J_{prior}}{\\partial x_k} + \\frac{s(s x_k + b - y_k)}{\\sigma^2}$\n$(\\nabla J_P(\\mathbf{x}))_k = \\frac{\\partial J_{prior}}{\\partial x_k} + s\\left(1 - \\frac{y_k}{s x_k + b}\\right)$\n\n这些表达式在一个数值优化程序中实现，以找到 MAP 轨迹 $x^{\\mathrm{MAP}}$。\n\n### 步骤 3：数值计算和 RMSE\n\n使用支持边界约束（$x_t \\ge 0$）的数值优化器（带有 L-BFGS-B 方法的 `scipy.optimize.minimize`），为每个测试用例计算两种模型的 MAP 轨迹。使用上面推导出的目标函数及其梯度。然后，计算每个估计轨迹相对于真实轨迹的均方根误差 (RMSE)：\n$$\n\\mathrm{RMSE} = \\sqrt{\\frac{1}{T} \\sum_{t=0}^{T-1} \\left(x_t^{\\mathrm{MAP}} - x_t^{\\mathrm{true}}\\right)^2}\n$$\n每个案例最终报告的值是差值 $\\mathrm{RMSE}_{\\text{Gaussian}} - \\mathrm{RMSE}_{\\text{Poisson}}$。",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Main function to solve the variational data assimilation problem for all test cases.\n    \"\"\"\n\n    # Ground truth and dynamical parameters (shared across all cases)\n    T = 12\n    delta_t = 1.0\n    k = 0.2\n    a = np.exp(-k * delta_t)\n    x0_true = 2.0\n    x_true = x0_true * (a ** np.arange(T))\n    \n    # Prior and model parameters\n    m0 = 1.5\n    p0 = 0.25\n    q = 0.04\n\n    # Test case definitions\n    test_cases = [\n        {\n            \"s\": 80.0, \"b\": 20.0, \"sigma\": 5.0,\n            \"y\": np.array([174, 152, 132, 111, 96, 76, 68, 61, 55, 47, 43, 36])\n        },\n        {\n            \"s\": 8.0, \"b\": 1.0, \"sigma\": 3.0,\n            \"y\": np.array([16, 13, 12, 9, 8, 7, 6, 5, 6, 4, 4, 3])\n        },\n        {\n            \"s\": 15.0, \"b\": 40.0, \"sigma\": 5.0,\n            \"y\": np.array([68, 63, 59, 58, 52, 53, 49, 48, 47, 45, 44, 43])\n        }\n    ]\n\n    results = []\n    \n    # Common settings for the optimizer\n    bounds = [(0, None)] * T\n\n    for case in test_cases:\n        s, b, sigma, y_obs = case[\"s\"], case[\"b\"], case[\"sigma\"], case[\"y\"]\n\n        # A physically-motivated initial guess for the optimizer\n        x_init = np.maximum(1e-6, (y_obs - b) / s)\n\n        # Solve for Gaussian model\n        res_g = minimize(\n            fun=objective_and_grad,\n            x0=x_init,\n            args=(T, a, m0, p0, q, 'gaussian', y_obs, s, b, sigma),\n            method='L-BFGS-B',\n            jac=True,\n            bounds=bounds\n        )\n        x_map_g = res_g.x\n        \n        # Solve for Poisson model\n        res_p = minimize(\n            fun=objective_and_grad,\n            x0=x_init,\n            args=(T, a, m0, p0, q, 'poisson', y_obs, s, b, sigma),\n            method='L-BFGS-B',\n            jac=True,\n            bounds=bounds\n        )\n        x_map_p = res_p.x\n\n        # Calculate RMSE for both models\n        rmse_g = np.sqrt(np.mean((x_map_g - x_true)**2))\n        rmse_p = np.sqrt(np.mean((x_map_p - x_true)**2))\n\n        # Store the difference\n        results.append(rmse_g - rmse_p)\n\n    # Format and print the final output\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\ndef objective_and_grad(x, T, a, m0, p0, q, model_type, y, s, b, sigma):\n    \"\"\"\n    Computes the objective function and its gradient for both models.\n    This function is passed to scipy.optimize.minimize.\n    \"\"\"\n    # 1. Prior term and its gradient (common to both models)\n    \n    # Objective\n    prior_obj = 0.5 * (x[0] - m0)**2 / p0 + \\\n                0.5 * np.sum((x[1:] - a * x[:-1])**2) / q\n    \n    # Gradient\n    grad_prior = np.zeros(T)\n    grad_prior[0] = (x[0] - m0) / p0 - a * (x[1] - a * x[0]) / q\n    \n    # Using array slicing for the middle part\n    dyn_term1 = (x[1:-1] - a * x[:-2]) / q\n    dyn_term2 = -a * (x[2:] - a * x[1:-1]) / q\n    grad_prior[1:-1] = dyn_term1 + dyn_term2\n    \n    grad_prior[T-1] = (x[T-1] - a * x[T-2]) / q\n\n    # 2. Observation term and its gradient (model-specific)\n    if model_type == 'gaussian':\n        # Objective\n        obs_term = s * x + b - y\n        obs_obj = 0.5 * np.sum(obs_term**2) / sigma**2\n        \n        # Gradient\n        grad_obs = s * obs_term / sigma**2\n        \n    elif model_type == 'poisson':\n        # Objective\n        lambda_t = s * x + b\n        # Add a small epsilon for numerical stability if x is at bound 0 and b is 0\n        # although problem statement guarantees b>0.\n        lambda_t = np.maximum(1e-9, lambda_t)\n        obs_obj = np.sum(lambda_t - y * np.log(lambda_t))\n        \n        # Gradient\n        grad_obs = s * (1 - y / lambda_t)\n        \n    else:\n        raise ValueError(\"Unknown model type\")\n\n    # 3. Total objective and gradient\n    total_obj = prior_obj + obs_obj\n    total_grad = grad_prior + grad_obs\n    \n    return total_obj, total_grad\n\nsolve()\n```"
        }
    ]
}