{
    "hands_on_practices": [
        {
            "introduction": "The core of any particle filter is the \"update\" step, where the belief about the hidden state is refined using a new measurement. This exercise provides a concrete, step-by-step walkthrough of this fundamental mechanism. By manually calculating the unnormalized and normalized importance weights for a small set of particles, you will gain a deep, quantitative understanding of how the likelihood function re-weights the particle population to favor states that are more consistent with the observed data . This practice demystifies the Bayesian update at the heart of the filter.",
            "id": "3917036",
            "problem": "A biomedical engineer is using particle filtering to estimate plasma glucose concentration in a person with Type 1 diabetes during routine monitoring with a continuous glucose monitor (CGM). The hidden state $x_t$ represents the plasma glucose concentration at time $t$ expressed in mg/dL, and the observation model is specified by a Gaussian likelihood with known sensor noise variance. The engineer has already performed the prediction step and generated a particle set with associated prior weights. The observation at time $t$ is a single CGM reading, and the objective is to compute the normalized importance weights and the filtered posterior mean of the plasma glucose. The physiological context is realistic: CGM sensors exhibit approximately Gaussian measurement errors around the true plasma glucose with a known standard deviation due to sensor noise and calibration uncertainty.\n\nUse the following scientifically plausible data:\n- Number of particles $N$ is $5$.\n- Predicted particle states (in mg/dL): $x_t^{(1)} = 140$, $x_t^{(2)} = 148$, $x_t^{(3)} = 152$, $x_t^{(4)} = 160$, $x_t^{(5)} = 172$.\n- Prior weights (sum to $1$): $w_{t-1}^{(1)} = 0.10$, $w_{t-1}^{(2)} = 0.25$, $w_{t-1}^{(3)} = 0.30$, $w_{t-1}^{(4)} = 0.20$, $w_{t-1}^{(5)} = 0.15$.\n- Observation at time $t$ (CGM reading, in mg/dL): $y_t = 154$.\n- Measurement noise standard deviation: $\\sigma_v = 6$ mg/dL, with the likelihood $p(y_t \\mid x_t)$ modeled as Gaussian around $x_t$ with variance $\\sigma_v^2$.\n\nStarting from first principles of Bayesian inference and sequential importance sampling in particle filtering, compute:\n1. The unnormalized importance weights $\\tilde{w}_t^{(i)}$ for $i = 1, \\dots, N$ using the measurement likelihood and the prior weights.\n2. The normalized weights $\\bar{w}_t^{(i)}$ obtained by dividing each $\\tilde{w}_t^{(i)}$ by the sum over all particles.\n3. The filtered posterior mean $\\hat{x}_t = \\sum_{i=1}^{N} \\bar{w}_t^{(i)} x_t^{(i)}$.\n\nThen provide a brief clinical interpretation of the value of $\\hat{x}_t$ relative to typical glycemic targets. Express the final numerical value of $\\hat{x}_t$ in mg/dL and round your answer to four significant figures. The final boxed answer must contain only the numerical value without units.",
            "solution": "The problem is scientifically grounded, well-posed, objective, and contains all necessary information to compute a unique and meaningful solution. It represents a standard application of sequential importance sampling, the core of a particle filter, to a realistic biomedical state estimation task. All provided data are physically and physiologically plausible. The problem is therefore deemed valid.\n\nThe objective is to compute the filtered posterior mean of a hidden state, the plasma glucose concentration $x_t$, using a particle filter. We are given a set of $N=5$ particles $\\{x_t^{(i)}\\}_{i=1}^N$ that represent samples from the predicted state distribution $p(x_t \\mid y_{1:t-1})$. Each particle has an associated prior weight $w_{t-1}^{(i)}$, which for a standard particle filter without a specific proposal distribution other than the state transition model, is carried over from the previous time step's posterior weights, satisfying $\\sum_{i=1}^N w_{t-1}^{(i)} = 1$. Upon receiving a new measurement $y_t$, we update these weights according to Bayes' rule.\n\nThe posterior distribution of the state $x_t$ given all observations up to time $t$ is given by Bayes' theorem:\n$$p(x_t \\mid y_{1:t}) \\propto p(y_t \\mid x_t) p(x_t \\mid y_{1:t-1})$$\nHere, $p(y_t \\mid x_t)$ is the likelihood and $p(x_t \\mid y_{1:t-1})$ is the prior (or prediction) distribution. In a particle filter, the prior distribution is approximated by the weighted set of particles:\n$$p(x_t \\mid y_{1:t-1}) \\approx \\sum_{i=1}^N w_{t-1}^{(i)} \\delta(x_t - x_t^{(i)})$$\nwhere $\\delta(\\cdot)$ is the Dirac delta function.\n\nThe update step involves computing new importance weights for each particle. The unnormalized importance weight for the $i$-th particle, $\\tilde{w}_t^{(i)}$, is calculated by multiplying its prior weight by the likelihood of the observation given the particle's state:\n$$ \\tilde{w}_t^{(i)} = w_{t-1}^{(i)} p(y_t \\mid x_t^{(i)}) $$\nThe likelihood $p(y_t \\mid x_t)$ is modeled as a Gaussian distribution with mean $x_t$ and variance $\\sigma_v^2$. Its probability density function is:\n$$p(y_t \\mid x_t^{(i)}) = \\frac{1}{\\sqrt{2\\pi\\sigma_v^2}} \\exp\\left(-\\frac{(y_t - x_t^{(i)})^2}{2\\sigma_v^2}\\right)$$\nThe constant term $\\frac{1}{\\sqrt{2\\pi\\sigma_v^2}}$ is a common scaling factor for all particles and can be omitted when computing intermediate unnormalized weights, as it will be canceled during the normalization step. We can thus compute proportional unnormalized weights as $\\tilde{w}_t^{(i)} \\propto w_{t-1}^{(i)} \\exp\\left(-\\frac{(y_t - x_t^{(i)})^2}{2\\sigma_v^2}\\right)$.\n\nThe given data are:\n- Number of particles: $N = 5$.\n- Particle states (mg/dL): $x_t^{(1)} = 140$, $x_t^{(2)} = 148$, $x_t^{(3)} = 152$, $x_t^{(4)} = 160$, $x_t^{(5)} = 172$.\n- Prior weights: $w_{t-1}^{(1)} = 0.10$, $w_{t-1}^{(2)} = 0.25$, $w_{t-1}^{(3)} = 0.30$, $w_{t-1}^{(4)} = 0.20$, $w_{t-1}^{(5)} = 0.15$.\n- Observation (mg/dL): $y_t = 154$.\n- Measurement noise standard deviation (mg/dL): $\\sigma_v = 6$, so the variance is $\\sigma_v^2 = 36$.\n\n**1. Computation of Unnormalized Importance Weights $\\tilde{w}_t^{(i)}$**\n\nWe compute the exponent term $-\\frac{(y_t - x_t^{(i)})^2}{2\\sigma_v^2} = -\\frac{(154 - x_t^{(i)})^2}{72}$ for each particle.\n- For $i=1$: $-\\frac{(154 - 140)^2}{72} = -\\frac{14^2}{72} = -\\frac{196}{72} = -\\frac{49}{18}$.\n- For $i=2$: $-\\frac{(154 - 148)^2}{72} = -\\frac{6^2}{72} = -\\frac{36}{72} = -0.5$.\n- For $i=3$: $-\\frac{(154 - 152)^2}{72} = -\\frac{2^2}{72} = -\\frac{4}{72} = -\\frac{1}{18}$.\n- For $i=4$: $-\\frac{(154 - 160)^2}{72} = -\\frac{(-6)^2}{72} = -\\frac{36}{72} = -0.5$.\n- For $i=5$: $-\\frac{(154 - 172)^2}{72} = -\\frac{(-18)^2}{72} = -\\frac{324}{72} = -4.5$.\n\nNow, we calculate the unnormalized weights, $\\tilde{w}_t^{(i)} \\propto w_{t-1}^{(i)} \\exp(\\dots)$:\n- $\\tilde{w}_t^{(1)} \\propto 0.10 \\times \\exp(-49/18) \\approx 0.10 \\times 0.065733 = 0.0065733$\n- $\\tilde{w}_t^{(2)} \\propto 0.25 \\times \\exp(-0.5) \\approx 0.25 \\times 0.606531 = 0.1516328$\n- $\\tilde{w}_t^{(3)} \\propto 0.30 \\times \\exp(-1/18) \\approx 0.30 \\times 0.945959 = 0.2837877$\n- $\\tilde{w}_t^{(4)} \\propto 0.20 \\times \\exp(-0.5) \\approx 0.20 \\times 0.606531 = 0.1213062$\n- $\\tilde{w}_t^{(5)} \\propto 0.15 \\times \\exp(-4.5) \\approx 0.15 \\times 0.011109 = 0.0016664$\n\n**2. Computation of Normalized Weights $\\bar{w}_t^{(i)}$**\n\nFirst, sum the unnormalized weights to find the normalization constant:\n$$ S = \\sum_{j=1}^{N} \\tilde{w}_t^{(j)} \\approx 0.0065733 + 0.1516328 + 0.2837877 + 0.1213062 + 0.0016664 \\approx 0.5649664 $$\nNext, normalize each weight by dividing by the sum $S$: $\\bar{w}_t^{(i)} = \\frac{\\tilde{w}_t^{(i)}}{S}$.\n- $\\bar{w}_t^{(1)} = \\frac{0.0065733}{0.5649664} \\approx 0.0116348$\n- $\\bar{w}_t^{(2)} = \\frac{0.1516328}{0.5649664} \\approx 0.2683931$\n- $\\bar{w}_t^{(3)} = \\frac{0.2837877}{0.5649664} \\approx 0.5023101$\n- $\\bar{w}_t^{(4)} = \\frac{0.1213062}{0.5649664} \\approx 0.2147139$\n- $\\bar{w}_t^{(5)} = \\frac{0.0016664}{0.5649664} \\approx 0.0029496$\nThe sum of these normalized weights is approximately $1$, as expected.\n\n**3. Computation of the Filtered Posterior Mean $\\hat{x}_t$**\n\nThe filtered posterior mean, which provides the final state estimate at time $t$, is the weighted average of the particle states using the new normalized weights:\n$$ \\hat{x}_t = \\sum_{i=1}^{N} \\bar{w}_t^{(i)} x_t^{(i)} $$\nSubstituting the values:\n$$ \\hat{x}_t \\approx (0.0116348 \\times 140) + (0.2683931 \\times 148) + (0.5023101 \\times 152) + (0.2147139 \\times 160) + (0.0029496 \\times 172) $$\n$$ \\hat{x}_t \\approx 1.628872 + 39.722179 + 76.351135 + 34.354224 + 0.507331 $$\n$$ \\hat{x}_t \\approx 152.563741 $$\nRounding to four significant figures, the filtered posterior mean is $152.6$ mg/dL.\n\n**Clinical Interpretation**\n\nThe filtered estimate of the plasma glucose concentration is $\\hat{x}_t \\approx 152.6$ mg/dL. This value represents the model's best estimate of the true glucose level, having fused the prior prediction (represented by the initial weighted particles) with the new CGM measurement $y_t=154$ mg/dL. The estimate is very close to the measurement because the most likely particle from the prediction step, $x_t^{(3)}=152$ mg/dL, which had the highest prior weight ($w_{t-1}^{(3)} = 0.30$), was also the closest particle to the new observation. This combination of high prior belief and high likelihood resulted in a very large posterior weight for this particle ($\\bar{w}_t^{(3)} \\approx 0.50$), heavily influencing the posterior mean.\n\nIn a clinical context, a glucose level of $152.6$ mg/dL is considered mildly elevated (mild hyperglycemia). Typical glycemic targets for individuals with diabetes, as recommended by the American Diabetes Association, are generally $80-130$ mg/dL before meals and below $180$ mg/dL one to two hours after the start of a meal. The estimated value falls within the acceptable post-meal range but is above the pre-meal target, suggesting the individual's glucose is trending high but is not at a level of immediate alarm. This filtered estimate provides a more robust picture of the glycemic state than the raw sensor reading alone, as it incorporates knowledge of the system's dynamics and past states.",
            "answer": "$$\\boxed{152.6}$$"
        },
        {
            "introduction": "Moving from theory to a functional implementation often reveals practical challenges that are not apparent on paper. When calculating importance weights, which involve products of probabilities, we can easily encounter numerical underflow, where values become too small for a computer to represent. This exercise introduces the \"log-sum-exp\" trick, a critical technique for ensuring numerical stability in any particle filter implementation . Working through the reasoning will equip you with a robust method to normalize weights, preventing your filter from failing in real-world scenarios with very small likelihood values.",
            "id": "3917055",
            "problem": "In a particle filter (PF) for a clinically validated glucose-insulin minimal model used to estimate latent plasma glucose concentration, suppose a set of $N=4$ particles at time $t$ have accumulated unnormalized log-weights $\\ell_t^{(i)}=\\log \\tilde{w}_t^{(i)}$ after incorporating the likelihood of a noisy continuous glucose monitor reading. The normalized weights $\\bar{w}_t^{(i)}$ must satisfy $\\bar{w}_t^{(i)} \\propto \\exp\\{\\ell_t^{(i)}\\}$ and $\\sum_{i=1}^{N} \\bar{w}_t^{(i)}=1$. Due to long sequences and informative measurements, the log-weights can be very negative. You observe the following values:\n- $\\ell_t^{(1)}=-1200.0$,\n- $\\ell_t^{(2)}=-1198.0$,\n- $\\ell_t^{(3)}=-1205.0$,\n- $\\ell_t^{(4)}=-1199.5$.\n\nDirect exponentiation of $\\ell_t^{(i)}$ underflows in standard double precision arithmetic. Starting only from the definition of normalized importance weights and the requirement that multiplying the numerator and denominator of a fraction by a common positive constant leaves the ratio unchanged, reason about a numerically stable way to compute $\\bar{w}_t^{(i)}$ from $\\ell_t^{(i)}$ that avoids underflow. Then, using that reasoning, determine which option below both (i) correctly describes a numerically stable procedure rooted in these principles and (ii) yields the correct numerical values of the normalized weights for the given $\\ell_t^{(i)}$, rounded to $3$ significant figures.\n\nA. Let $a=\\max_{i}\\ell_t^{(i)}$, form shifted log-weights $u^{(i)}=\\ell_t^{(i)}-a$, compute $s=\\sum_{j=1}^{N}\\exp\\{u^{(j)}\\}$, and set $\\bar{w}_t^{(i)}=\\exp\\{u^{(i)}\\}/s$. Equivalently, compute the log-normalizer $\\log Z=a+\\log\\big(\\sum_{j=1}^{N}\\exp\\{\\ell_t^{(j)}-a\\}\\big)$ and then set $\\bar{w}_t^{(i)}=\\exp\\{\\ell_t^{(i)}-\\log Z\\}$. For the given $\\ell_t^{(i)}$, this yields $\\big(0.0996,\\;0.7359,\\;0.000671,\\;0.164\\big)$.\n\nB. Center the log-weights by their mean $\\bar{\\ell}=\\frac{1}{N}\\sum_{j=1}^{N}\\ell_t^{(j)}$ to obtain $v^{(i)}=\\ell_t^{(i)}-\\bar{\\ell}$, exponentiate to $r^{(i)}=\\exp\\{v^{(i)}\\}$, and define $\\bar{w}_t^{(i)}=r^{(i)}/N$ because centering around the mean controls the scale while division by $N$ enforces averaging.\n\nC. Use the identity $\\bar{w}_t^{(i)}=\\exp\\{\\ell_t^{(i)}\\}/\\sum_{j=1}^{N}\\ell_t^{(j)}$, replacing the potentially unstable sum of exponentials with the sum of log-weights to avoid underflow, which stabilizes the denominator without needing rescaling.\n\nD. To avoid underflow, clip each $\\ell_t^{(i)}$ below a threshold (for example, set all $\\ell_t^{(i)}<-700$ to $-700$), exponentiate the clipped values, and normalize by their sum; this discards extremely unlikely particles and stabilizes the exponentials without any additional transformation.\n\nSelect the single best option.",
            "solution": "The problem statement is first subjected to validation.\n\n### Step 1: Extract Givens\n- A particle filter (PF) is used for state estimation in a glucose-insulin minimal model.\n- The number of particles is $N=4$.\n- The unnormalized log-weights at time $t$ are given as:\n  - $\\ell_t^{(1)} = -1200.0$\n  - $\\ell_t^{(2)} = -1198.0$\n  - $\\ell_t^{(3)} = -1205.0$\n  - $\\ell_t^{(4)} = -1199.5$\n  where $\\ell_t^{(i)} = \\log \\tilde{w}_t^{(i)}$ and $\\tilde{w}_t^{(i)}$ is the unnormalized weight.\n- The normalized weights, $\\bar{w}_t^{(i)}$, must satisfy two conditions:\n  1. Proportionality: $\\bar{w}_t^{(i)} \\propto \\exp\\{\\ell_t^{(i)}\\}$\n  2. Sum-to-one: $\\sum_{i=1}^{N} \\bar{w}_t^{(i)} = 1$\n- A numerical issue is stated: direct exponentiation of $\\ell_t^{(i)}$ results in underflow in standard double precision arithmetic.\n- The task is to derive a numerically stable method for computing $\\bar{w}_t^{(i)}$ based on the definition of normalized weights and the property that a fraction is unchanged if its numerator and denominator are multiplied by the same positive constant.\n- Finally, evaluate the given options based on the correctness of the described procedure and the accuracy of the computed numerical values (rounded to $3$ significant figures).\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is well-grounded in computational statistics and its application to biomedical systems modeling. Particle filtering, importance weights, log-weights, and the numerical challenge of normalization (the log-sum-exp trick) are all standard and fundamental concepts in this field.\n- **Well-Posed**: The problem is clearly defined. The given log-weights and the definition of normalization uniquely determine the normalized weights. The question asks for a principled derivation of a stable computational method and its application, which is a well-posed task.\n- **Objective**: All information is provided in objective, mathematical terms. There is no ambiguity or subjectivity.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. It is scientifically sound, well-posed, and objective. I will proceed with the derivation and solution.\n\n### Derivation of a Numerically Stable Method\n\nThe normalized importance weight for the $i$-th particle is defined as the ratio of its unnormalized weight to the sum of all unnormalized weights:\n$$ \\bar{w}_t^{(i)} = \\frac{\\tilde{w}_t^{(i)}}{\\sum_{j=1}^{N} \\tilde{w}_t^{(j)}} $$\nGiven the log-weights $\\ell_t^{(i)} = \\log \\tilde{w}_t^{(i)}$, we have $\\tilde{w}_t^{(i)} = \\exp\\{\\ell_t^{(i)}\\}$. Substituting this into the definition gives:\n$$ \\bar{w}_t^{(i)} = \\frac{\\exp\\{\\ell_t^{(i)}\\}}{\\sum_{j=1}^{N} \\exp\\{\\ell_t^{(j)}\\}} $$\nAs stated in the problem, for large negative values of $\\ell_t^{(j)}$ (e.g., $-1200.0$), the term $\\exp\\{\\ell_t^{(j)}\\}$ will evaluate to zero in standard floating-point arithmetic (underflow). This would lead to a division by zero if all terms underflow.\n\nTo address this, we use the principle that a fraction's value is unchanged if we multiply its numerator and denominator by the same positive constant, say $C$.\n$$ \\bar{w}_t^{(i)} = \\frac{C \\cdot \\exp\\{\\ell_t^{(i)}\\}}{C \\cdot \\sum_{j=1}^{N} \\exp\\{\\ell_t^{(j)}\\}} = \\frac{C \\cdot \\exp\\{\\ell_t^{(i)}\\}}{\\sum_{j=1}^{N} (C \\cdot \\exp\\{\\ell_t^{(j)}\\})} $$\nLet's choose the constant $C$ in a form that is useful for exponential arguments. Let $C = \\exp\\{-a\\}$ for some real number $a$.\n$$ \\bar{w}_t^{(i)} = \\frac{\\exp\\{-a\\} \\cdot \\exp\\{\\ell_t^{(i)}\\}}{\\sum_{j=1}^{N} (\\exp\\{-a\\} \\cdot \\exp\\{\\ell_t^{(j)}\\})} = \\frac{\\exp\\{\\ell_t^{(i)} - a\\}}{\\sum_{j=1}^{N} \\exp\\{\\ell_t^{(j)} - a\\}} $$\nThis transformation is exact for any $a$. To ensure numerical stability, we must choose $a$ such that the arguments of the exponentials, $\\ell_t^{(j)} - a$, are not large negative numbers. A robust choice is to shift all log-weights so that the largest becomes zero. This is achieved by setting $a$ to be the maximum of all log-weights:\n$$ a = \\max_{j=1, \\dots, N} \\ell_t^{(j)} $$\nWith this choice, the new exponent for the particle with the maximum log-weight is $a - a = 0$, so its corresponding term $\\exp\\{a-a\\}$ is $\\exp\\{0\\} = 1$. All other exponents, $\\ell_t^{(j)} - a$, will be less than or equal to zero, so their exponentials will be between $0$ and $1$. This prevents overflow (as exponents are non-positive) and underflow of the entire sum (as at least one term is $1$). This is commonly known as the log-sum-exp trick.\n\n### Numerical Calculation\n\nLet's apply this procedure to the given log-weights:\n$\\ell_t^{(1)}=-1200.0$, $\\ell_t^{(2)}=-1198.0$, $\\ell_t^{(3)}=-1205.0$, $\\ell_t^{(4)}=-1199.5$.\n\n1.  Find the maximum log-weight:\n    $$ a = \\max\\{-1200.0, -1198.0, -1205.0, -1199.5\\} = -1198.0 $$\n2.  Compute the shifted log-weights, $u^{(i)} = \\ell_t^{(i)} - a$:\n    - $u^{(1)} = -1200.0 - (-1198.0) = -2.0$\n    - $u^{(2)} = -1198.0 - (-1198.0) = 0.0$\n    - $u^{(3)} = -1205.0 - (-1198.0) = -7.0$\n    - $u^{(4)} = -1199.5 - (-1198.0) = -1.5$\n3.  Compute the sum of the exponentiated shifted log-weights, $s = \\sum_{j=1}^{N} \\exp\\{u^{(j)}\\}$:\n    $$ s = \\exp\\{-2.0\\} + \\exp\\{0.0\\} + \\exp\\{-7.0\\} + \\exp\\{-1.5\\} $$\n    Using a calculator:\n    $$ s \\approx 0.135335 + 1.0 + 0.000912 + 0.223130 \\approx 1.359377 $$\n4.  Compute the normalized weights, $\\bar{w}_t^{(i)} = \\exp\\{u^{(i)}\\} / s$, and round to $3$ significant figures:\n    - $\\bar{w}_t^{(1)} = \\frac{\\exp\\{-2.0\\}}{s} \\approx \\frac{0.135335}{1.359377} \\approx 0.0995568 \\rightarrow 0.0996$\n    - $\\bar{w}_t^{(2)} = \\frac{\\exp\\{0.0\\}}{s} \\approx \\frac{1.0}{1.359377} \\approx 0.7356333 \\rightarrow 0.736$\n    - $\\bar{w}_t^{(3)} = \\frac{\\exp\\{-7.0\\}}{s} \\approx \\frac{0.000912}{1.359377} \\approx 0.0006708 \\rightarrow 0.000671$\n    - $\\bar{w}_t^{(4)} = \\frac{\\exp\\{-1.5\\}}{s} \\approx \\frac{0.223130}{1.359377} \\approx 0.1641390 \\rightarrow 0.164$\n\nThe correctly calculated normalized weights, rounded to 3 significant figures, are approximately $(0.0996, 0.736, 0.000671, 0.164)$.\n\n### Option-by-Option Analysis\n\n**A. Let $a=\\max_{i}\\ell_t^{(i)}$, form shifted log-weights $u^{(i)}=\\ell_t^{(i)}-a$, compute $s=\\sum_{j=1}^{N}\\exp\\{u^{(j)}\\}$, and set $\\bar{w}_t^{(i)}=\\exp\\{u^{(i)}\\}/s$. Equivalently, compute the log-normalizer $\\log Z=a+\\log\\big(\\sum_{j=1}^{N}\\exp\\{\\ell_t^{(j)}-a\\}\\big)$ and then set $\\bar{w}_t^{(i)}=\\exp\\{\\ell_t^{(i)}-\\log Z\\}$. For the given $\\ell_t^{(i)}$, this yields $\\big(0.0996,\\;0.7359,\\;0.000671,\\;0.164\\big)$.**\n\n- **Procedure**: The described procedure is exactly the log-sum-exp method derived from first principles. Both formulations presented are correct ways to implement this stable normalization.\n- **Numerical Values**: The provided values are $\\big(0.0996,\\;0.7359,\\;0.000671,\\;0.164\\big)$. Comparing these to my calculated values $\\big(0.0996,\\;0.736,\\;0.000671,\\;0.164\\big)$, three of the four values match perfectly. The second value, $\\bar{w}_t^{(2)}$, is given as $0.7359$ (to $4$ significant figures) while the correct value rounded to $3$ significant figures is $0.736$. This appears to be a minor error or typo in the option's text.\n- **Verdict**: **Correct**. The described procedure is sound and principled. Despite the minor numerical discrepancy in one value, this option is vastly superior to the others, which describe fundamentally flawed methods. It is the \"single best option\".\n\n**B. Center the log-weights by their mean $\\bar{\\ell}=\\frac{1}{N}\\sum_{j=1}^{N}\\ell_t^{(j)}$ to obtain $v^{(i)}=\\ell_t^{(i)}-\\bar{\\ell}$, exponentiate to $r^{(i)}=\\exp\\{v^{(i)}\\}$, and define $\\bar{w}_t^{(i)}=r^{(i)}/N$ because centering around the mean controls the scale while division by $N$ enforces averaging.**\n\n- **Procedure**: This procedure is mathematically incorrect. The correct denominator for normalization is the sum of the exponentiated shifted log-weights, $\\sum_{j=1}^N \\exp\\{v^{(j)}\\}$, not the number of particles $N$. The statement $\\sum_{j=1}^N \\exp\\{v^{(j)}\\} = N$ is not true in general.\n- **Verdict**: **Incorrect**.\n\n**C. Use the identity $\\bar{w}_t^{(i)}=\\exp\\{\\ell_t^{(i)}\\}/\\sum_{j=1}^{N}\\ell_t^{(j)}$, replacing the potentially unstable sum of exponentials with the sum of log-weights to avoid underflow, which stabilizes the denominator without needing rescaling.**\n\n- **Procedure**: The proposed \"identity\" is false. It incorrectly replaces the sum of weights, $\\sum_{j=1}^N \\tilde{w}_t^{(j)} = \\sum_{j=1}^N \\exp\\{\\ell_t^{(j)}\\}$, with the sum of log-weights, $\\sum_{j=1}^N \\ell_t^{(j)}$. These two quantities are not equal. This is a fundamental mathematical error.\n- **Verdict**: **Incorrect**.\n\n**D. To avoid underflow, clip each $\\ell_t^{(i)}$ below a threshold (for example, set all $\\ell_t^{(i)}<-700$ to $-700$), exponentiate the clipped values, and normalize by their sum; this discards extremely unlikely particles and stabilizes the exponentials without any additional transformation.**\n\n- **Procedure**: This method is not derived from first principles but is an ad-hoc approximation. Clipping the log-weights alters their values, which in turn changes the relative proportions of the weights. For instance, it might make two particles with different original probabilities seem equally probable. This introduces bias and results in an incorrect posterior distribution. The log-sum-exp trick is an exact transformation that introduces no such bias.\n- **Verdict**: **Incorrect**.\n\n### Conclusion\nOption A is the only one that describes a mathematically correct and principled procedure for numerically stable weight normalization. Although it contains a minor numerical error in one of the reported values, it is unambiguously the best option among the choices, as options B, C, and D propose methods that are fundamentally flawed.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "This final practice integrates the concepts of prediction, update, and numerical stability into a coding exercise. You will implement a full time-step of a bootstrap particle filter for a nonlinear biomedical model, translating the mathematical framework into working code. Beyond just implementation, this exercise encourages you to use your filter as an experimental tool to investigate a crucial aspect of modeling: the impact of model mismatch on filter performance . By observing how the effective sample size ($ESS$) degrades under different incorrect assumptions, you will build practical intuition about the filter's sensitivity and failure modes.",
            "id": "3917022",
            "problem": "Consider a scalar latent physiological state $x_t$ evolving according to a nonlinear state-space model and observed through a nonlinear sensor with additive Gaussian noise. The generative assumptions are: $x_t = f(x_{t-1}) + \\eta_t$ with $\\eta_t \\sim \\mathcal{N}(0,Q)$ and $y_t = h(x_t) + v_t$ with $v_t \\sim \\mathcal{N}(0,R)$. A bootstrap particle filter is required to perform one prediction-update time step given a single observation $y_t$. The bootstrap particle filter uses the transition prior as the proposal and weights particles by the observation likelihood. Use Bayes’ theorem and the independence of process and measurement noises as the fundamental base. Do not use any shortcut formulas beyond these principles. The state $x_t$ and observation $y_t$ are dimensionless scalars, so no physical unit reporting is required.\n\nDefine the nonlinear state transition as $f(x) = x + \\alpha \\sin(x)$ and the nonlinear observation as $h(x) = \\dfrac{1}{1 + \\exp\\left(-\\beta(x - x_0)\\right)}$. The prior particle distribution at time $t-1$ is $\\mathcal{N}(m_{t-1},P_{t-1})$ with $m_{t-1} = 0.0$ and $P_{t-1} = 1.0$. The true data-generating parameters are $\\alpha_{\\text{true}} = 0.7$, $\\beta_{\\text{true}} = 1.0$, $x_{0,\\text{true}} = 0.0$, $Q_{\\text{true}} = 0.2$, and $R_{\\text{true}} = 0.1$. Generate a single observation $y_t$ as follows: sample $x_{t-1}^{\\text{true}} \\sim \\mathcal{N}(m_{t-1},P_{t-1})$, propagate $x_t^{\\text{true}} = f_{\\text{true}}(x_{t-1}^{\\text{true}}) + \\eta_t^{\\text{true}}$ with $\\eta_t^{\\text{true}} \\sim \\mathcal{N}(0,Q_{\\text{true}})$, and then sample $y_t = h_{\\text{true}}(x_t^{\\text{true}}) + v_t^{\\text{true}}$ with $v_t^{\\text{true}} \\sim \\mathcal{N}(0,R_{\\text{true}})$. Use a fixed pseudorandom seed of $123$ for all random sampling to ensure reproducibility.\n\nImplement one bootstrap particle filter time step for a set of test cases in which the assumed filter model may mismatch the true data-generating model. For each test case:\n- Draw $N$ particles $x_{t-1}^{(i)} \\sim \\mathcal{N}(m_{t-1},P_{t-1})$.\n- Propagate particles under the filter’s assumed transition $f_{\\text{assumed}}(x) = x + \\alpha_{\\text{assumed}} \\sin(x)$ with additive process noise $\\eta_t^{(i)} \\sim \\mathcal{N}(0,Q_{\\text{assumed}})$ to obtain proposals $x_t^{(i)}$.\n- Compute unnormalized weights $w^{(i)} \\propto p(y_t \\mid x_t^{(i)})$ using the filter’s assumed observation $h_{\\text{assumed}}$ and measurement noise variance $R_{\\text{assumed}}$, and then normalize the weights to sum to $1$.\n- Report the effective sample size $\\mathrm{ESS} = \\dfrac{1}{\\sum_{i=1}^{N} (w^{(i)})^2}$ for each test case as a float rounded to six decimal places. The effective sample size is a standard weight-spread metric for particle degeneracy.\n\nThe test suite consists of the following six cases, all using the same single $y_t$ generated from the true parameters above:\n1. Matched model (happy path): $N = 1000$, $\\alpha_{\\text{assumed}} = 0.7$, $Q_{\\text{assumed}} = 0.2$, $h_{\\text{assumed}}$ is the same sigmoid with $\\beta_{\\text{assumed}} = 1.0$ and $x_{0,\\text{assumed}} = 0.0$, $R_{\\text{assumed}} = 0.1$.\n2. Underestimated process noise: $N = 1000$, $\\alpha_{\\text{assumed}} = 0.7$, $Q_{\\text{assumed}} = 0.02$, $h_{\\text{assumed}}$ sigmoid with $\\beta_{\\text{assumed}} = 1.0$, $x_{0,\\text{assumed}} = 0.0$, $R_{\\text{assumed}} = 0.1$.\n3. Mis-specified observation mapping: $N = 1000$, $\\alpha_{\\text{assumed}} = 0.7$, $Q_{\\text{assumed}} = 0.2$, $h_{\\text{assumed}}(x) = x$ (identity), $R_{\\text{assumed}} = 0.1$.\n4. Large assumed measurement noise: $N = 1000$, $\\alpha_{\\text{assumed}} = 0.7$, $Q_{\\text{assumed}} = 0.2$, $h_{\\text{assumed}}$ sigmoid with $\\beta_{\\text{assumed}} = 1.0$, $x_{0,\\text{assumed}} = 0.0$, $R_{\\text{assumed}} = 1.0$.\n5. Small particle count boundary: $N = 20$, $\\alpha_{\\text{assumed}} = 0.7$, $Q_{\\text{assumed}} = 0.2$, $h_{\\text{assumed}}$ sigmoid with $\\beta_{\\text{assumed}} = 1.0$, $x_{0,\\text{assumed}} = 0.0$, $R_{\\text{assumed}} = 0.1$.\n6. Near-deterministic assumed dynamics (edge case): $N = 1000$, $\\alpha_{\\text{assumed}} = 0.7$, $Q_{\\text{assumed}} = 10^{-6}$, $h_{\\text{assumed}}$ sigmoid with $\\beta_{\\text{assumed}} = 1.0$, $x_{0,\\text{assumed}} = 0.0$, $R_{\\text{assumed}} = 0.1$.\n\nYour program should produce a single line of output containing the effective sample sizes for the six test cases as a comma-separated list enclosed in square brackets (for example, $[e_1,e_2,e_3,e_4,e_5,e_6]$), where each $e_k$ is rounded to six decimal places.",
            "solution": "The problem requires the implementation of a single time step of a bootstrap particle filter to estimate a latent physiological state. The performance of the filter is evaluated under several scenarios of model mismatch by calculating the effective sample size (ESS). The solution is constructed from first principles, as stipulated.\n\n**1. Foundational Principles: Bayesian Filtering**\n\nThe core objective of state-space filtering is to recursively estimate the probability distribution of a hidden state $x_t$ at time $t$, given a sequence of observations $y_{1:t} = \\{y_1, y_2, \\dots, y_t\\}$. This is captured by the posterior distribution $p(x_t | y_{1:t})$. The estimation process consists of two stages:\n\n- **Prediction:** The posterior from the previous time step, $p(x_{t-1} | y_{1:t-1})$, is propagated forward in time using the state transition model, $p(x_t | x_{t-1})$. This yields the prior (or predictive) distribution for the current state:\n$$p(x_t | y_{1:t-1}) = \\int p(x_t | x_{t-1}) p(x_{t-1} | y_{1:t-1}) dx_{t-1}$$\nThe state transition model is given by $x_t = f(x_{t-1}) + \\eta_t$, with process noise $\\eta_t \\sim \\mathcal{N}(0, Q)$.\n\n- **Update:** The predictive distribution is updated with the new observation $y_t$ via Bayes' theorem. The observation model is given by $y_t = h(x_t) + v_t$, with measurement noise $v_t \\sim \\mathcal{N}(0, R)$. The likelihood of the observation given the state is $p(y_t | x_t)$.\n$$p(x_t | y_{1:t}) = \\frac{p(y_t | x_t) p(x_t | y_{1:t-1})}{p(y_t | y_{1:t-1})}$$\nThe denominator, $p(y_t | y_{1:t-1}) = \\int p(y_t | x_t) p(x_t | y_{1:t-1}) dx_t$, is a normalizing constant.\n\nFor the nonlinear functions $f(x)$ and $h(x)$ specified in the problem, these integrals are analytically intractable.\n\n**2. The Particle Filter Approximation**\n\nParticle filters, a class of Sequential Monte Carlo methods, circumvent this intractability by approximating the posterior distribution with a finite set of $N$ weighted samples, or \"particles\": $\\{x_t^{(i)}, \\tilde{w}_t^{(i)}\\}_{i=1}^N$. The posterior is represented as a weighted sum of Dirac delta functions:\n$$p(x_t | y_{1:t}) \\approx \\sum_{i=1}^N \\tilde{w}_t^{(i)} \\delta(x_t - x_t^{(i)})$$\nwhere $\\sum_{i=1}^N \\tilde{w}_t^{(i)} = 1$. The key idea is to generate particles from a tractable proposal distribution $q(x_t | x_{t-1}, y_t)$ and then assign importance weights to correct for the difference between the proposal and the true posterior.\n\n**3. The Bootstrap Filter: A Specific Implementation**\n\nThe bootstrap particle filter, the simplest and most common variant, uses the state transition prior as its proposal distribution:\n$$q(x_t^{(i)} | x_{t-1}^{(i)}, y_t) = p(x_t^{(i)} | x_{t-1}^{(i)})$$\nThe general importance weight update rule is $w_t^{(i)} \\propto w_{t-1}^{(i)} \\frac{p(y_t | x_t^{(i)}) p(x_t^{(i)} | x_{t-1}^{(i)})}{q(x_t^{(i)} | x_{t-1}^{(i)}, y_t)}$. With the bootstrap proposal, the terms $p(x_t^{(i)} | x_{t-1}^{(i)})$ and $q(\\cdot)$ cancel. Assuming equal weights from the previous step ($w_{t-1}^{(i)} = 1/N$, as is the case after resampling or at initialization), the weight update simplifies to being proportional to the observation likelihood:\n$$w_t^{(i)} \\propto p(y_t | x_t^{(i)})$$\nThis simplification makes the algorithm straightforward to implement.\n\n**4. Algorithmic Steps for a Single Time Step**\n\nThe procedure for one time step, as required by the problem, is as follows:\n\n- **Step 0: Generate a Single Observation $y_t$**\nBefore running the filter, we generate a single fixed observation $y_t$ based on the true model parameters. A fixed random seed of $123$ ensures this process is reproducible.\n1. Draw the true state at the previous time: $x_{t-1}^{\\text{true}} \\sim \\mathcal{N}(m_{t-1}, P_{t-1})$, where $m_{t-1}=0.0$ and $P_{t-1}=1.0$.\n2. Propagate the true state: $x_t^{\\text{true}} = f_{\\text{true}}(x_{t-1}^{\\text{true}}) + \\eta_t^{\\text{true}}$, with $\\eta_t^{\\text{true}} \\sim \\mathcal{N}(0, Q_{\\text{true}})$, using $f_{\\text{true}}(x) = x + \\alpha_{\\text{true}} \\sin(x)$.\n3. Generate the observation: $y_t = h_{\\text{true}}(x_t^{\\text{true}}) + v_t^{\\text{true}}$, with $v_t^{\\text{true}} \\sim \\mathcal{N}(0, R_{\\text{true}})$, using $h_{\\text{true}}(x) = (1 + \\exp(-\\beta_{\\text{true}}(x - x_{0,\\text{true}})))^{-1}$.\nThis value of $y_t$ remains constant across all test cases.\n\n- **Step 1: Initialization**\nFor each test case, we begin with a set of $N$ particles representing the knowledge at time $t-1$. These are drawn from the prior distribution $p(x_{t-1})$:\n$$x_{t-1}^{(i)} \\sim \\mathcal{N}(m_{t-1}, P_{t-1}) \\quad \\text{for } i=1, \\dots, N$$\n\n- **Step 2: Prediction (Propagation)**\nEach particle $x_{t-1}^{(i)}$ is propagated through the filter's assumed state dynamics to generate a proposal particle $x_t^{(i)}$. This is a sample from the proposal distribution $p(x_t | x_{t-1}^{(i)})$.\n$$x_t^{(i)} = f_{\\text{assumed}}(x_{t-1}^{(i)}) + \\eta_t^{(i)}$$\nwhere $\\eta_t^{(i)} \\sim \\mathcal{N}(0, Q_{\\text{assumed}})$ and $f_{\\text{assumed}}(x) = x + \\alpha_{\\text{assumed}} \\sin(x)$.\n\n- **Step 3: Update (Weighting)**\nUnnormalized weights $w_t^{(i)}$ are computed for each particle based on the likelihood of the observation $y_t$ given the particle's state $x_t^{(i)}$. The likelihood function $p(y_t | x_t)$ is defined by the assumed observation model $y_t = h_{\\text{assumed}}(x_t) + v_t$ with $v_t \\sim \\mathcal{N}(0, R_{\\text{assumed}})$.\n$$w_t^{(i)} \\propto p(y_t | x_t^{(i)}) = \\frac{1}{\\sqrt{2\\pi R_{\\text{assumed}}}} \\exp\\left(-\\frac{(y_t - h_{\\text{assumed}}(x_t^{(i)}))^2}{2 R_{\\text{assumed}}}\\right)$$\nThe constant pre-factor can be ignored as weights are normalized. For numerical stability, it is better to compute log-weights and then exponentiate. Let $\\hat{y}_t^{(i)} = h_{\\text{assumed}}(x_t^{(i)})$. The unnormalized weights are computed as $w_t^{(i)} = \\exp\\left( -\\frac{(y_t - \\hat{y}_t^{(i)})^2}{2R_{\\text{assumed}}} \\right)$.\n\n- **Step 4: Normalization**\nThe weights are normalized to sum to $1$:\n$$\\tilde{w}_t^{(i)} = \\frac{w_t^{(i)}}{\\sum_{j=1}^N w_t^{(j)}}$$\n\n- **Step 5: Performance Evaluation**\nThe degree of particle degeneracy—the phenomenon where a few particles acquire most of the weight—is quantified by the Effective Sample Size (ESS):\n$$\\mathrm{ESS} = \\frac{1}{\\sum_{i=1}^N (\\tilde{w}_t^{(i)})^2}$$\nThe ESS ranges from $1$ (complete degeneracy) to $N$ (all particles have equal weight). This entire procedure is repeated for each of the six test cases specified, using the same generated $y_t$ but with fresh particle sets drawn according to the problem's sequential random sampling scheme.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Implements one step of a bootstrap particle filter for a nonlinear state-space model,\n    calculating the Effective Sample Size (ESS) for six different test cases.\n    \"\"\"\n\n    # --- Step 0: Generate a single reproducible observation y_t ---\n\n    # Fixed random seed for reproducibility\n    rng = np.random.default_rng(123)\n\n    # True data-generating parameters\n    m_tm1 = 0.0\n    P_tm1 = 1.0\n    alpha_true = 0.7\n    beta_true = 1.0\n    x0_true = 0.0\n    Q_true = 0.2\n    R_true = 0.1\n\n    # True model functions\n    def f_true(x):\n        return x + alpha_true * np.sin(x)\n\n    def h_true(x):\n        return 1.0 / (1.0 + np.exp(-beta_true * (x - x0_true)))\n\n    # Generate the single observation y_t\n    x_tm1_true = rng.normal(loc=m_tm1, scale=np.sqrt(P_tm1))\n    eta_t_true = rng.normal(loc=0.0, scale=np.sqrt(Q_true))\n    x_t_true = f_true(x_tm1_true) + eta_t_true\n    v_t_true = rng.normal(loc=0.0, scale=np.sqrt(R_true))\n    y_t = h_true(x_t_true) + v_t_true\n\n    # --- Define Test Cases ---\n    test_cases = [\n        {\n            \"N\": 1000, \"alpha\": 0.7, \"Q\": 0.2,\n            \"h_type\": \"sigmoid\", \"beta\": 1.0, \"x0\": 0.0, \"R\": 0.1\n        },\n        {\n            \"N\": 1000, \"alpha\": 0.7, \"Q\": 0.02,\n            \"h_type\": \"sigmoid\", \"beta\": 1.0, \"x0\": 0.0, \"R\": 0.1\n        },\n        {\n            \"N\": 1000, \"alpha\": 0.7, \"Q\": 0.2,\n            \"h_type\": \"identity\", \"beta\": None, \"x0\": None, \"R\": 0.1\n        },\n        {\n            \"N\": 1000, \"alpha\": 0.7, \"Q\": 0.2,\n            \"h_type\": \"sigmoid\", \"beta\": 1.0, \"x0\": 0.0, \"R\": 1.0\n        },\n        {\n            \"N\": 20, \"alpha\": 0.7, \"Q\": 0.2,\n            \"h_type\": \"sigmoid\", \"beta\": 1.0, \"x0\": 0.0, \"R\": 0.1\n        },\n        {\n            \"N\": 1000, \"alpha\": 0.7, \"Q\": 1e-6,\n            \"h_type\": \"sigmoid\", \"beta\": 1.0, \"x0\": 0.0, \"R\": 0.1\n        },\n    ]\n\n    results = []\n\n    # --- Run Particle Filter for Each Case ---\n    for case in test_cases:\n        N = case[\"N\"]\n        alpha_assumed = case[\"alpha\"]\n        Q_assumed = case[\"Q\"]\n        R_assumed = case[\"R\"]\n\n        # Define assumed model functions for the current case\n        def f_assumed(x):\n            return x + alpha_assumed * np.sin(x)\n\n        if case[\"h_type\"] == \"sigmoid\":\n            beta_assumed = case[\"beta\"]\n            x0_assumed = case[\"x0\"]\n            def h_assumed(x):\n                return 1.0 / (1.0 + np.exp(-beta_assumed * (x - x0_assumed)))\n        else: # identity\n            def h_assumed(x):\n                return x\n\n        # Step 1: Initialization - Draw particles from the prior\n        x_tm1_particles = rng.normal(loc=m_tm1, scale=np.sqrt(P_tm1), size=N)\n\n        # Step 2: Prediction - Propagate particles\n        eta_t_particles = rng.normal(loc=0.0, scale=np.sqrt(Q_assumed), size=N)\n        x_t_particles = f_assumed(x_tm1_particles) + eta_t_particles\n\n        # Step 3: Update - Compute weights\n        # Calculate expected observations for each particle\n        y_hat_particles = h_assumed(x_t_particles)\n        \n        # Calculate log weights for numerical stability\n        log_weights = -0.5 * ((y_t - y_hat_particles)**2) / R_assumed\n\n        # Stabilize by subtracting the max log weight before exponentiating\n        log_weights -= np.max(log_weights)\n        unnormalized_weights = np.exp(log_weights)\n\n        # Step 4: Normalization\n        sum_weights = np.sum(unnormalized_weights)\n        if sum_weights == 0:\n            # Handle case where all weights are zero (underflow)\n            normalized_weights = np.full(N, 1.0 / N)\n        else:\n            normalized_weights = unnormalized_weights / sum_weights\n\n        # Step 5: Calculate Effective Sample Size (ESS)\n        ess = 1.0 / np.sum(normalized_weights**2)\n\n        results.append(ess)\n\n    # Final output formatting\n    # Note: rounding to 6 decimal places as required.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}