{
    "hands_on_practices": [
        {
            "introduction": "We begin our practical exploration with the heart of the particle filter: the measurement update step. This exercise provides a concrete, step-by-step calculation of how a new observation reshapes our belief about the hidden state, demonstrating the principle of sequential importance sampling in action. By manually computing the updated weights and the resulting state estimate for a small set of particles, you will gain a foundational understanding of how information is fused within the filter .",
            "id": "3917036",
            "problem": "A biomedical engineer is using particle filtering to estimate plasma glucose concentration in a person with Type 1 diabetes during routine monitoring with a continuous glucose monitor (CGM). The hidden state $x_t$ represents the plasma glucose concentration at time $t$ expressed in mg/dL, and the observation model is specified by a Gaussian likelihood with known sensor noise variance. The engineer has already performed the prediction step and generated a particle set with associated prior weights. The observation at time $t$ is a single CGM reading, and the objective is to compute the normalized importance weights and the filtered posterior mean of the plasma glucose. The physiological context is realistic: CGM sensors exhibit approximately Gaussian measurement errors around the true plasma glucose with a known standard deviation due to sensor noise and calibration uncertainty.\n\nUse the following scientifically plausible data:\n- Number of particles $N$ is $5$.\n- Predicted particle states (in mg/dL): $x_t^{(1)} = 140$, $x_t^{(2)} = 148$, $x_t^{(3)} = 152$, $x_t^{(4)} = 160$, $x_t^{(5)} = 172$.\n- Prior weights (sum to $1$): $w_{t-1}^{(1)} = 0.10$, $w_{t-1}^{(2)} = 0.25$, $w_{t-1}^{(3)} = 0.30$, $w_{t-1}^{(4)} = 0.20$, $w_{t-1}^{(5)} = 0.15$.\n- Observation at time $t$ (CGM reading, in mg/dL): $y_t = 154$.\n- Measurement noise standard deviation: $\\sigma_v = 6$ mg/dL, with the likelihood $p(y_t \\mid x_t)$ modeled as Gaussian around $x_t$ with variance $\\sigma_v^2$.\n\nStarting from first principles of Bayesian inference and sequential importance sampling in particle filtering, compute:\n1. The unnormalized importance weights $\\tilde{w}_t^{(i)}$ for $i = 1, \\dots, N$ using the measurement likelihood and the prior weights.\n2. The normalized weights $\\bar{w}_t^{(i)}$ obtained by dividing each $\\tilde{w}_t^{(i)}$ by the sum over all particles.\n3. The filtered posterior mean $\\hat{x}_t = \\sum_{i=1}^{N} \\bar{w}_t^{(i)} x_t^{(i)}$.\n\nThen provide a brief clinical interpretation of the value of $\\hat{x}_t$ relative to typical glycemic targets. Express the final numerical value of $\\hat{x}_t$ in mg/dL and round your answer to four significant figures. The final boxed answer must contain only the numerical value without units.",
            "solution": "The problem is scientifically grounded, well-posed, objective, and contains all necessary information to compute a unique and meaningful solution. It represents a standard application of sequential importance sampling, the core of a particle filter, to a realistic biomedical state estimation task. All provided data are physically and physiologically plausible. The problem is therefore deemed valid.\n\nThe objective is to compute the filtered posterior mean of a hidden state, the plasma glucose concentration $x_t$, using a particle filter. We are given a set of $N=5$ particles $\\{x_t^{(i)}\\}_{i=1}^N$ that represent samples from the predicted state distribution $p(x_t \\mid y_{1:t-1})$. Each particle has an associated prior weight $w_{t-1}^{(i)}$, which for a standard particle filter without a specific proposal distribution other than the state transition model, is carried over from the previous time step's posterior weights, satisfying $\\sum_{i=1}^N w_{t-1}^{(i)} = 1$. Upon receiving a new measurement $y_t$, we update these weights according to Bayes' rule.\n\nThe posterior distribution of the state $x_t$ given all observations up to time $t$ is given by Bayes' theorem:\n$$p(x_t \\mid y_{1:t}) \\propto p(y_t \\mid x_t) p(x_t \\mid y_{1:t-1})$$\nHere, $p(y_t \\mid x_t)$ is the likelihood and $p(x_t \\mid y_{1:t-1})$ is the prior (or prediction) distribution. In a particle filter, the prior distribution is approximated by the weighted set of particles:\n$$p(x_t \\mid y_{1:t-1}) \\approx \\sum_{i=1}^N w_{t-1}^{(i)} \\delta(x_t - x_t^{(i)})$$\nwhere $\\delta(\\cdot)$ is the Dirac delta function.\n\nThe update step involves computing new importance weights for each particle. The unnormalized importance weight for the $i$-th particle, $\\tilde{w}_t^{(i)}$, is calculated by multiplying its prior weight by the likelihood of the observation given the particle's state:\n$$ \\tilde{w}_t^{(i)} = w_{t-1}^{(i)} p(y_t \\mid x_t^{(i)}) $$\nThe likelihood $p(y_t \\mid x_t)$ is modeled as a Gaussian distribution with mean $x_t$ and variance $\\sigma_v^2$. Its probability density function is:\n$$p(y_t \\mid x_t^{(i)}) = \\frac{1}{\\sqrt{2\\pi\\sigma_v^2}} \\exp\\left(-\\frac{(y_t - x_t^{(i)})^2}{2\\sigma_v^2}\\right)$$\nThe constant term $\\frac{1}{\\sqrt{2\\pi\\sigma_v^2}}$ is a common scaling factor for all particles and can be omitted when computing intermediate unnormalized weights, as it will be canceled during the normalization step. We can thus compute proportional unnormalized weights as $\\tilde{w}_t^{(i)} \\propto w_{t-1}^{(i)} \\exp\\left(-\\frac{(y_t - x_t^{(i)})^2}{2\\sigma_v^2}\\right)$.\n\nThe given data are:\n- Number of particles: $N = 5$.\n- Particle states (mg/dL): $x_t^{(1)} = 140$, $x_t^{(2)} = 148$, $x_t^{(3)} = 152$, $x_t^{(4)} = 160$, $x_t^{(5)} = 172$.\n- Prior weights: $w_{t-1}^{(1)} = 0.10$, $w_{t-1}^{(2)} = 0.25$, $w_{t-1}^{(3)} = 0.30$, $w_{t-1}^{(4)} = 0.20$, $w_{t-1}^{(5)} = 0.15$.\n- Observation (mg/dL): $y_t = 154$.\n- Measurement noise standard deviation (mg/dL): $\\sigma_v = 6$, so the variance is $\\sigma_v^2 = 36$.\n\n**1. Computation of Unnormalized Importance Weights $\\tilde{w}_t^{(i)}$**\n\nWe compute the exponent term $-\\frac{(y_t - x_t^{(i)})^2}{2\\sigma_v^2} = -\\frac{(154 - x_t^{(i)})^2}{72}$ for each particle.\n- For $i=1$: $-\\frac{(154 - 140)^2}{72} = -\\frac{14^2}{72} = -\\frac{196}{72} = -\\frac{49}{18}$.\n- For $i=2$: $-\\frac{(154 - 148)^2}{72} = -\\frac{6^2}{72} = -\\frac{36}{72} = -0.5$.\n- For $i=3$: $-\\frac{(154 - 152)^2}{72} = -\\frac{2^2}{72} = -\\frac{4}{72} = -\\frac{1}{18}$.\n- For $i=4$: $-\\frac{(154 - 160)^2}{72} = -\\frac{(-6)^2}{72} = -\\frac{36}{72} = -0.5$.\n- For $i=5$: $-\\frac{(154 - 172)^2}{72} = -\\frac{(-18)^2}{72} = -\\frac{324}{72} = -4.5$.\n\nNow, we calculate the unnormalized weights, $\\tilde{w}_t^{(i)} \\propto w_{t-1}^{(i)} \\exp(\\dots)$:\n- $\\tilde{w}_t^{(1)} \\propto 0.10 \\times \\exp(-49/18) \\approx 0.10 \\times 0.065733 = 0.0065733$\n- $\\tilde{w}_t^{(2)} \\propto 0.25 \\times \\exp(-0.5) \\approx 0.25 \\times 0.606531 = 0.1516328$\n- $\\tilde{w}_t^{(3)} \\propto 0.30 \\times \\exp(-1/18) \\approx 0.30 \\times 0.945959 = 0.2837877$\n- $\\tilde{w}_t^{(4)} \\propto 0.20 \\times \\exp(-0.5) \\approx 0.20 \\times 0.606531 = 0.1213062$\n- $\\tilde{w}_t^{(5)} \\propto 0.15 \\times \\exp(-4.5) \\approx 0.15 \\times 0.011109 = 0.0016664$\n\n**2. Computation of Normalized Weights $\\bar{w}_t^{(i)}$**\n\nFirst, sum the unnormalized weights to find the normalization constant:\n$$ S = \\sum_{j=1}^{N} \\tilde{w}_t^{(j)} \\approx 0.0065733 + 0.1516328 + 0.2837877 + 0.1213062 + 0.0016664 \\approx 0.5649664 $$\nNext, normalize each weight by dividing by the sum $S$: $\\bar{w}_t^{(i)} = \\frac{\\tilde{w}_t^{(i)}}{S}$.\n- $\\bar{w}_t^{(1)} = \\frac{0.0065733}{0.5649664} \\approx 0.0116348$\n- $\\bar{w}_t^{(2)} = \\frac{0.1516328}{0.5649664} \\approx 0.2683931$\n- $\\bar{w}_t^{(3)} = \\frac{0.2837877}{0.5649664} \\approx 0.5023101$\n- $\\bar{w}_t^{(4)} = \\frac{0.1213062}{0.5649664} \\approx 0.2147139$\n- $\\bar{w}_t^{(5)} = \\frac{0.0016664}{0.5649664} \\approx 0.0029496$\nThe sum of these normalized weights is approximately $1$, as expected.\n\n**3. Computation of the Filtered Posterior Mean $\\hat{x}_t$**\n\nThe filtered posterior mean, which provides the final state estimate at time $t$, is the weighted average of the particle states using the new normalized weights:\n$$ \\hat{x}_t = \\sum_{i=1}^{N} \\bar{w}_t^{(i)} x_t^{(i)} $$\nSubstituting the values:\n$$ \\hat{x}_t \\approx (0.0116348 \\times 140) + (0.2683931 \\times 148) + (0.5023101 \\times 152) + (0.2147139 \\times 160) + (0.0029496 \\times 172) $$\n$$ \\hat{x}_t \\approx 1.628872 + 39.722179 + 76.351135 + 34.354224 + 0.507331 $$\n$$ \\hat{x}_t \\approx 152.563741 $$\nRounding to four significant figures, the filtered posterior mean is $152.6$ mg/dL.\n\n**Clinical Interpretation**\n\nThe filtered estimate of the plasma glucose concentration is $\\hat{x}_t \\approx 152.6$ mg/dL. This value represents the model's best estimate of the true glucose level, having fused the prior prediction (represented by the initial weighted particles) with the new CGM measurement $y_t=154$ mg/dL. The estimate is very close to the measurement because the most likely particle from the prediction step, $x_t^{(3)}=152$ mg/dL, which had the highest prior weight ($w_{t-1}^{(3)} = 0.30$), was also the closest particle to the new observation. This combination of high prior belief and high likelihood resulted in a very large posterior weight for this particle ($\\bar{w}_t^{(3)} \\approx 0.50$), heavily influencing the posterior mean.\n\nIn a clinical context, a glucose level of $152.6$ mg/dL is considered mildly elevated (mild hyperglycemia). Typical glycemic targets for individuals with diabetes, as recommended by the American Diabetes Association, are generally $80-130$ mg/dL before meals and below $180$ mg/dL one to two hours after the start of a meal. The estimated value falls within the acceptable post-meal range but is above the pre-meal target, suggesting the individual's glucose is trending high but is not at a level of immediate alarm. This filtered estimate provides a more robust picture of the glycemic state than the raw sensor reading alone, as it incorporates knowledge of the system's dynamics and past states.",
            "answer": "$$\\boxed{152.6}$$"
        },
        {
            "introduction": "The update step, while powerful, inevitably leads to a challenge known as particle degeneracy, where a few particles dominate the posterior representation. This practice introduces the concept of the Effective Sample Size ($N_{\\mathrm{eff}}$) as a crucial metric for diagnosing the filter's health and develops a principled threshold for deciding when to perform resampling . Understanding this trade-off between reducing degeneracy and avoiding sample impoverishment is key to building robust particle filters.",
            "id": "3916982",
            "problem": "A research group is deploying a Sequential Monte Carlo particle filter (PF) to estimate plasma glucose concentration in an intensive care unit using a nonlinear state-space model. At a particular update time $t$, the PF has $N=8$ particles with normalized importance weights $\\{\\bar{w}_{t}^{(i)}\\}_{i=1}^{8}$ given by\n$$\n\\left[0.35,\\;0.25,\\;0.15,\\;0.10,\\;0.06,\\;0.04,\\;0.03,\\;0.02\\right].\n$$\nThe group wants to set a principled resampling threshold based on a mean squared error (MSE) trade-off. They adopt the following decision rule: they will resample whenever the effective sample size falls below a threshold $N_{\\mathrm{th}}$ defined implicitly as the value at which the expected one-step-ahead MSE of the self-normalized importance sampling estimator of the posterior mean is the same with and without resampling.\n\nAssume the following modeling assumptions grounded in standard importance sampling variance scaling. Let $\\kappa$ denote the posterior state variance scale at time $t$ in the glucose model when all particles are equally weighted; take $\\kappa = 49$ in units of $(\\text{mg}\\,\\text{dL}^{-1})^{2}$. Without resampling, the one-step-ahead MSE of the posterior mean estimator is approximated by $\\kappa$ divided by the effective sample size. With systematic resampling and roughening, the one-step-ahead MSE is approximated by the sum of the equally weighted Monte Carlo variance and an additive sample impoverishment penalty $\\beta$; take $\\beta = 1.225$ in units of $(\\text{mg}\\,\\text{dL}^{-1})^{2}$.\n\nTasks:\n1) Starting from the definition of effective sample size as the number of equally weighted samples that produce the same estimator variance as the current weighted sample, derive a formula for $N_{\\mathrm{eff}}$ in terms of $\\{\\bar{w}_{t}^{(i)}\\}_{i=1}^{N}$ and compute $N_{\\mathrm{eff}}$ for the given weights.\n\n2) Starting from the decision rule that the resampling threshold $N_{\\mathrm{th}}$ is the value of $N_{\\mathrm{eff}}$ at which the expected one-step-ahead MSE with resampling equals that without resampling, derive a closed-form expression for $N_{\\mathrm{th}}$ in terms of $N$, $\\kappa$, and $\\beta$, and then evaluate it numerically for the provided values.\n\n3) Briefly explain, in terms of variance and bias of the posterior mean estimator in this biomedical filtering context, the qualitative consequences of choosing $N_{\\mathrm{th}}$ significantly higher or lower than the value obtained in item $2)$.\n\nYour final reported answer should be only the value of $N_{\\mathrm{th}}$ from item $2)$, expressed as an exact fraction in simplest terms (no decimal approximation). Do not include units in your final answer.",
            "solution": "The problem as stated is scientifically grounded, well-posed, and objective. All necessary data and definitions are provided, and there are no internal contradictions or violations of fundamental principles in particle filtering theory. We can therefore proceed with a full solution.\n\nThe problem asks for three tasks related to resampling in a particle filter. We will address them in sequence.\n\n1) Derivation and computation of the effective sample size, $N_{\\mathrm{eff}}$.\n\nThe effective sample size, $N_{\\mathrm{eff}}$, is defined as the number of hypothetical, equally-weighted particles that would yield an estimator with the same variance as the current set of $N$ unequally-weighted particles.\n\nLet the state of interest be $x$, and let the particle filter approximate its posterior distribution using a set of $N$ particles $\\{x_t^{(i)}\\}_{i=1}^{N}$ with corresponding normalized importance weights $\\{\\bar{w}_{t}^{(i)}\\}_{i=1}^{N}$ such that $\\sum_{i=1}^{N} \\bar{w}_{t}^{(i)} = 1$. The posterior mean is estimated by $\\hat{E}[x_t] = \\sum_{i=1}^{N} \\bar{w}_{t}^{(i)} x_t^{(i)}$.\n\nAssuming the particles $x_t^{(i)}$ are independent and identically distributed draws from a proposal distribution with variance $\\sigma^2_p$, the variance of this estimator is:\n$$ \\text{Var}(\\hat{E}[x_t]) = \\text{Var}\\left(\\sum_{i=1}^{N} \\bar{w}_{t}^{(i)} x_t^{(i)}\\right) = \\sum_{i=1}^{N} (\\bar{w}_{t}^{(i)})^2 \\text{Var}(x_t^{(i)}) = \\sigma_p^2 \\sum_{i=1}^{N} (\\bar{w}_{t}^{(i)})^2 $$\nNow, consider a hypothetical sample of size $N_{\\mathrm{eff}}$ with equal weights, i.e., $w^{(j)} = 1/N_{\\mathrm{eff}}$ for $j=1, \\dots, N_{\\mathrm{eff}}$. The variance of the mean estimator for this uniform sample would be:\n$$ \\text{Var}(\\hat{E}_{\\text{unif}}[x_t]) = \\sigma_p^2 \\sum_{j=1}^{N_{\\mathrm{eff}}} \\left(\\frac{1}{N_{\\mathrm{eff}}}\\right)^2 = \\sigma_p^2 \\cdot N_{\\mathrm{eff}} \\cdot \\frac{1}{N_{\\mathrm{eff}}^2} = \\frac{\\sigma_p^2}{N_{\\mathrm{eff}}} $$\nEquating the two variances according to the definition of $N_{\\mathrm{eff}}$:\n$$ \\frac{\\sigma_p^2}{N_{\\mathrm{eff}}} = \\sigma_p^2 \\sum_{i=1}^{N} (\\bar{w}_{t}^{(i)})^2 $$\nSolving for $N_{\\mathrm{eff}}$, we arrive at the standard formula:\n$$ N_{\\mathrm{eff}} = \\frac{1}{\\sum_{i=1}^{N} (\\bar{w}_{t}^{(i)})^2} $$\nFor the given problem, we have $N=8$ and the weights $\\{\\bar{w}_{t}^{(i)}\\}_{i=1}^{8}$ are given as $[0.35, 0.25, 0.15, 0.10, 0.06, 0.04, 0.03, 0.02]$. We compute the sum of squared weights:\n$$ \\sum_{i=1}^{8} (\\bar{w}_{t}^{(i)})^2 = (0.35)^2 + (0.25)^2 + (0.15)^2 + (0.10)^2 + (0.06)^2 + (0.04)^2 + (0.03)^2 + (0.02)^2 $$\n$$ \\sum_{i=1}^{8} (\\bar{w}_{t}^{(i)})^2 = 0.1225 + 0.0625 + 0.0225 + 0.0100 + 0.0036 + 0.0016 + 0.0009 + 0.0004 = 0.224 $$\nTherefore, the effective sample size is:\n$$ N_{\\mathrm{eff}} = \\frac{1}{0.224} = \\frac{1}{224/1000} = \\frac{1000}{224} = \\frac{125}{28} \\approx 4.46 $$\n\n2) Derivation and computation of the resampling threshold, $N_{\\mathrm{th}}$.\n\nThe resampling threshold $N_{\\mathrm{th}}$ is defined as the value of $N_{\\mathrm{eff}}$ at which the expected one-step-ahead Mean Squared Error (MSE) is identical with or without resampling. The problem provides approximations for these MSEs:\n$$ \\text{MSE}_{\\text{no-resample}} \\approx \\frac{\\kappa}{N_{\\mathrm{eff}}} $$\n$$ \\text{MSE}_{\\text{resample}} \\approx \\frac{\\kappa}{N} + \\beta $$\nHere, $\\kappa$ is the posterior state variance scale, $N$ is the total number of particles, and $\\beta$ is a sample impoverishment penalty. The term $\\kappa/N$ represents the variance of an equally-weighted Monte Carlo estimator. The decision rule sets the MSEs equal at the threshold value $N_{\\mathrm{eff}} = N_{\\mathrm{th}}$:\n$$ \\frac{\\kappa}{N_{\\mathrm{th}}} = \\frac{\\kappa}{N} + \\beta $$\nWe solve this equation for $N_{\\mathrm{th}}$. First, we combine the terms on the right-hand side:\n$$ \\frac{\\kappa}{N_{\\mathrm{th}}} = \\frac{\\kappa + N\\beta}{N} $$\nInverting both sides gives:\n$$ \\frac{N_{\\mathrm{th}}}{\\kappa} = \\frac{N}{\\kappa + N\\beta} $$\nFinally, multiplying by $\\kappa$ yields the closed-form expression for the threshold:\n$$ N_{\\mathrm{th}} = \\frac{\\kappa N}{\\kappa + N\\beta} $$\nWe are given the values $N=8$, $\\kappa=49$, and $\\beta=1.225$. To ensure an exact fractional answer, we convert $\\beta$ to a fraction: $\\beta = 1.225 = \\frac{1225}{1000} = \\frac{245}{200} = \\frac{49}{40}$.\nNow we substitute these values into the expression for $N_{\\mathrm{th}}$:\n$$ N_{\\mathrm{th}} = \\frac{49 \\cdot 8}{49 + 8 \\cdot \\left(\\frac{49}{40}\\right)} $$\nFirst, we simplify the denominator:\n$$ 49 + 8 \\cdot \\frac{49}{40} = 49 + \\frac{49}{5} = \\frac{5 \\cdot 49}{5} + \\frac{49}{5} = \\frac{6 \\cdot 49}{5} $$\nNow we substitute this back into the expression for $N_{\\mathrm{th}}$:\n$$ N_{\\mathrm{th}} = \\frac{49 \\cdot 8}{\\frac{6 \\cdot 49}{5}} = \\frac{49 \\cdot 8 \\cdot 5}{6 \\cdot 49} $$\nThe factor of $49$ cancels out:\n$$ N_{\\mathrm{th}} = \\frac{8 \\cdot 5}{6} = \\frac{40}{6} = \\frac{20}{3} $$\n\n3) Qualitative consequences of the choice of $N_{\\mathrm{th}}$.\n\nThe value $N_{\\mathrm{th}} = 20/3 \\approx 6.67$ represents the optimal trade-off point for resampling based on the provided MSE models. Deviating significantly from this value has deleterious consequences for filter performance, specifically impacting the variance of the posterior mean estimator.\n\nChoosing $N_{\\mathrm{th}}$ significantly higher than $20/3$: This leads to overly frequent resampling. The filter will resample even when the particle weights are not highly degenerate. While resampling at each step would reduce the one-step estimator variance to its minimum ($\\kappa/N$), this action is not without cost. The stochastic nature of resampling introduces its own variability, which manifests as sample impoverishment—the loss of particle diversity over time as particles become highly correlated. The model captures this cost through the additive penalty $\\beta$. By resampling too often, one incurs the penalty $\\beta$ unnecessarily, increasing the average MSE above the achievable minimum. The estimator's variance over multiple time steps increases due to the impoverished particle set's inability to adapt to new information, despite being momentarily unbiased after each resampling step.\n\nChoosing $N_{\\mathrm{th}}$ significantly lower than $20/3$: This leads to infrequent resampling. The filter will tolerate very high levels of particle degeneracy before intervening. As the filter evolves without resampling, the variance of the importance weights increases, causing $N_{\\mathrm{eff}}$ to plummet. The variance of the posterior mean estimator, approximated as $\\kappa/N_{\\mathrm{eff}}$, will grow without bound. The filter's computational effort is wasted on propagating a majority of particles with negligible weights, while the estimate relies on a very small number of \"fitter\" particles. This leads to an unstable estimate with high variance. While this choice avoids the sample impoverishment penalty $\\beta$, it does so at the cost of catastrophic estimator variance from particle degeneracy, resulting in a much higher MSE and rendering the filter ineffective at tracking the plasma glucose concentration. The estimator remains technically unbiased but its extreme variance makes it practically useless.",
            "answer": "$$\\boxed{\\frac{20}{3}}$$"
        },
        {
            "introduction": "Having explored the update and resampling decisions, we now transition from manual calculation to computational practice. This exercise guides you through implementing a single step of a bootstrap particle filter in code to investigate one of the most significant challenges in applied filtering: model mismatch . By simulating various scenarios and observing the effect on the Effective Sample Size, you will develop a practical intuition for how the filter's performance is tied to the accuracy of its underlying model.",
            "id": "3917022",
            "problem": "Consider a scalar latent physiological state $x_t$ evolving according to a nonlinear state-space model and observed through a nonlinear sensor with additive Gaussian noise. The generative assumptions are: $x_t = f(x_{t-1}) + \\eta_t$ with $\\eta_t \\sim \\mathcal{N}(0,Q)$ and $y_t = h(x_t) + v_t$ with $v_t \\sim \\mathcal{N}(0,R)$. A bootstrap particle filter is required to perform one prediction-update time step given a single observation $y_t$. The bootstrap particle filter uses the transition prior as the proposal and weights particles by the observation likelihood. Use Bayes’ theorem and the independence of process and measurement noises as the fundamental base. Do not use any shortcut formulas beyond these principles. The state $x_t$ and observation $y_t$ are dimensionless scalars, so no physical unit reporting is required.\n\nDefine the nonlinear state transition as $f(x) = x + \\alpha \\sin(x)$ and the nonlinear observation as $h(x) = \\dfrac{1}{1 + \\exp\\left(-\\beta(x - x_0)\\right)}$. The prior particle distribution at time $t-1$ is $\\mathcal{N}(m_{t-1},P_{t-1})$ with $m_{t-1} = 0.0$ and $P_{t-1} = 1.0$. The true data-generating parameters are $\\alpha_{\\text{true}} = 0.7$, $\\beta_{\\text{true}} = 1.0$, $x_{0,\\text{true}} = 0.0$, $Q_{\\text{true}} = 0.2$, and $R_{\\text{true}} = 0.1$. Generate a single observation $y_t$ as follows: sample $x_{t-1}^{\\text{true}} \\sim \\mathcal{N}(m_{t-1},P_{t-1})$, propagate $x_t^{\\text{true}} = f_{\\text{true}}(x_{t-1}^{\\text{true}}) + \\eta_t^{\\text{true}}$ with $\\eta_t^{\\text{true}} \\sim \\mathcal{N}(0,Q_{\\text{true}})$, and then sample $y_t = h_{\\text{true}}(x_t^{\\text{true}}) + v_t^{\\text{true}}$ with $v_t^{\\text{true}} \\sim \\mathcal{N}(0,R_{\\text{true}})$. Use a fixed pseudorandom seed of $123$ for all random sampling to ensure reproducibility.\n\nImplement one bootstrap particle filter time step for a set of test cases in which the assumed filter model may mismatch the true data-generating model. For each test case:\n- Draw $N$ particles $x_{t-1}^{(i)} \\sim \\mathcal{N}(m_{t-1},P_{t-1})$.\n- Propagate particles under the filter’s assumed transition $f_{\\text{assumed}}(x) = x + \\alpha_{\\text{assumed}} \\sin(x)$ with additive process noise $\\eta_t^{(i)} \\sim \\mathcal{N}(0,Q_{\\text{assumed}})$ to obtain proposals $x_t^{(i)}$.\n- Compute unnormalized weights $w^{(i)} \\propto p(y_t \\mid x_t^{(i)})$ using the filter’s assumed observation $h_{\\text{assumed}}$ and measurement noise variance $R_{\\text{assumed}}$, and then normalize the weights to sum to $1$.\n- Report the effective sample size $\\mathrm{ESS} = \\dfrac{1}{\\sum_{i=1}^{N} (w^{(i)})^2}$ for each test case as a float rounded to six decimal places. The effective sample size is a standard weight-spread metric for particle degeneracy.\n\nThe test suite consists of the following six cases, all using the same single $y_t$ generated from the true parameters above:\n1. Matched model (happy path): $N = 1000$, $\\alpha_{\\text{assumed}} = 0.7$, $Q_{\\text{assumed}} = 0.2$, $h_{\\text{assumed}}$ is the same sigmoid with $\\beta_{\\text{assumed}} = 1.0$ and $x_{0,\\text{assumed}} = 0.0$, $R_{\\text{assumed}} = 0.1$.\n2. Underestimated process noise: $N = 1000$, $\\alpha_{\\text{assumed}} = 0.7$, $Q_{\\text{assumed}} = 0.02$, $h_{\\text{assumed}}$ sigmoid with $\\beta_{\\text{assumed}} = 1.0$, $x_{0,\\text{assumed}} = 0.0$, $R_{\\text{assumed}} = 0.1$.\n3. Mis-specified observation mapping: $N = 1000$, $\\alpha_{\\text{assumed}} = 0.7$, $Q_{\\text{assumed}} = 0.2$, $h_{\\text{assumed}}(x) = x$ (identity), $R_{\\text{assumed}} = 0.1$.\n4. Large assumed measurement noise: $N = 1000$, $\\alpha_{\\text{assumed}} = 0.7$, $Q_{\\text{assumed}} = 0.2$, $h_{\\text{assumed}}$ sigmoid with $\\beta_{\\text{assumed}} = 1.0$, $x_{0,\\text{assumed}} = 0.0$, $R_{\\text{assumed}} = 1.0$.\n5. Small particle count boundary: $N = 20$, $\\alpha_{\\text{assumed}} = 0.7$, $Q_{\\text{assumed}} = 0.2$, $h_{\\text{assumed}}$ sigmoid with $\\beta_{\\text{assumed}} = 1.0$, $x_{0,\\text{assumed}} = 0.0$, $R_{\\text{assumed}} = 0.1$.\n6. Near-deterministic assumed dynamics (edge case): $N = 1000$, $\\alpha_{\\text{assumed}} = 0.7$, $Q_{\\text{assumed}} = 10^{-6}$, $h_{\\text{assumed}}$ sigmoid with $\\beta_{\\text{assumed}} = 1.0$, $x_{0,\\text{assumed}} = 0.0$, $R_{\\text{assumed}} = 0.1$.\n\nYour program should produce a single line of output containing the effective sample sizes for the six test cases as a comma-separated list enclosed in square brackets (for example, $[e_1,e_2,e_3,e_4,e_5,e_6]$), where each $e_k$ is rounded to six decimal places.",
            "solution": "The problem requires the implementation of a single time step of a bootstrap particle filter to estimate a latent physiological state. The performance of the filter is evaluated under several scenarios of model mismatch by calculating the effective sample size (ESS). The solution is constructed from first principles, as stipulated.\n\n**1. Foundational Principles: Bayesian Filtering**\n\nThe core objective of state-space filtering is to recursively estimate the probability distribution of a hidden state $x_t$ at time $t$, given a sequence of observations $y_{1:t} = \\{y_1, y_2, \\dots, y_t\\}$. This is captured by the posterior distribution $p(x_t | y_{1:t})$. The estimation process consists of two stages:\n\n- **Prediction:** The posterior from the previous time step, $p(x_{t-1} | y_{1:t-1})$, is propagated forward in time using the state transition model, $p(x_t | x_{t-1})$. This yields the prior (or predictive) distribution for the current state:\n$$p(x_t | y_{1:t-1}) = \\int p(x_t | x_{t-1}) p(x_{t-1} | y_{1:t-1}) dx_{t-1}$$\nThe state transition model is given by $x_t = f(x_{t-1}) + \\eta_t$, with process noise $\\eta_t \\sim \\mathcal{N}(0, Q)$.\n\n- **Update:** The predictive distribution is updated with the new observation $y_t$ via Bayes' theorem. The observation model is given by $y_t = h(x_t) + v_t$, with measurement noise $v_t \\sim \\mathcal{N}(0, R)$. The likelihood of the observation given the state is $p(y_t | x_t)$.\n$$p(x_t | y_{1:t}) = \\frac{p(y_t | x_t) p(x_t | y_{1:t-1})}{p(y_t | y_{1:t-1})}$$\nThe denominator, $p(y_t | y_{1:t-1}) = \\int p(y_t | x_t) p(x_t | y_{1:t-1}) dx_t$, is a normalizing constant.\n\nFor the nonlinear functions $f(x)$ and $h(x)$ specified in the problem, these integrals are analytically intractable.\n\n**2. The Particle Filter Approximation**\n\nParticle filters, a class of Sequential Monte Carlo methods, circumvent this intractability by approximating the posterior distribution with a finite set of $N$ weighted samples, or \"particles\": $\\{x_t^{(i)}, \\tilde{w}_t^{(i)}\\}_{i=1}^N$. The posterior is represented as a weighted sum of Dirac delta functions:\n$$p(x_t | y_{1:t}) \\approx \\sum_{i=1}^N \\tilde{w}_t^{(i)} \\delta(x_t - x_t^{(i)})$$\nwhere $\\sum_{i=1}^N \\tilde{w}_t^{(i)} = 1$. The key idea is to generate particles from a tractable proposal distribution $q(x_t | x_{t-1}, y_t)$ and then assign importance weights to correct for the difference between the proposal and the true posterior.\n\n**3. The Bootstrap Filter: A Specific Implementation**\n\nThe bootstrap particle filter, the simplest and most common variant, uses the state transition prior as its proposal distribution:\n$$q(x_t^{(i)} | x_{t-1}^{(i)}, y_t) = p(x_t^{(i)} | x_{t-1}^{(i)})$$\nThe general importance weight update rule is $w_t^{(i)} \\propto w_{t-1}^{(i)} \\frac{p(y_t | x_t^{(i)}) p(x_t^{(i)} | x_{t-1}^{(i)})}{q(x_t^{(i)} | x_{t-1}^{(i)}, y_t)}$. With the bootstrap proposal, the terms $p(x_t^{(i)} | x_{t-1}^{(i)})$ and $q(\\cdot)$ cancel. Assuming equal weights from the previous step ($w_{t-1}^{(i)} = 1/N$, as is the case after resampling or at initialization), the weight update simplifies to being proportional to the observation likelihood:\n$$w_t^{(i)} \\propto p(y_t | x_t^{(i)})$$\nThis simplification makes the algorithm straightforward to implement.\n\n**4. Algorithmic Steps for a Single Time Step**\n\nThe procedure for one time step, as required by the problem, is as follows:\n\n- **Step 0: Generate a Single Observation $y_t$**\nBefore running the filter, we generate a single fixed observation $y_t$ based on the true model parameters. A fixed random seed of $123$ ensures this process is reproducible.\n1. Draw the true state at the previous time: $x_{t-1}^{\\text{true}} \\sim \\mathcal{N}(m_{t-1}, P_{t-1})$, where $m_{t-1}=0.0$ and $P_{t-1}=1.0$.\n2. Propagate the true state: $x_t^{\\text{true}} = f_{\\text{true}}(x_{t-1}^{\\text{true}}) + \\eta_t^{\\text{true}}$, with $\\eta_t^{\\text{true}} \\sim \\mathcal{N}(0, Q_{\\text{true}})$, using $f_{\\text{true}}(x) = x + \\alpha_{\\text{true}} \\sin(x)$.\n3. Generate the observation: $y_t = h_{\\text{true}}(x_t^{\\text{true}}) + v_t^{\\text{true}}$, with $v_t^{\\text{true}} \\sim \\mathcal{N}(0, R_{\\text{true}})$, using $h_{\\text{true}}(x) = (1 + \\exp(-\\beta_{\\text{true}}(x - x_{0,\\text{true}})))^{-1}$.\nThis value of $y_t$ remains constant across all test cases.\n\n- **Step 1: Initialization**\nFor each test case, we begin with a set of $N$ particles representing the knowledge at time $t-1$. These are drawn from the prior distribution $p(x_{t-1})$:\n$$x_{t-1}^{(i)} \\sim \\mathcal{N}(m_{t-1}, P_{t-1}) \\quad \\text{for } i=1, \\dots, N$$\n\n- **Step 2: Prediction (Propagation)**\nEach particle $x_{t-1}^{(i)}$ is propagated through the filter's assumed state dynamics to generate a proposal particle $x_t^{(i)}$. This is a sample from the proposal distribution $p(x_t | x_{t-1}^{(i)})$.\n$$x_t^{(i)} = f_{\\text{assumed}}(x_{t-1}^{(i)}) + \\eta_t^{(i)}$$\nwhere $\\eta_t^{(i)} \\sim \\mathcal{N}(0, Q_{\\text{assumed}})$ and $f_{\\text{assumed}}(x) = x + \\alpha_{\\text{assumed}} \\sin(x)$.\n\n- **Step 3: Update (Weighting)**\nUnnormalized weights $w_t^{(i)}$ are computed for each particle based on the likelihood of the observation $y_t$ given the particle's state $x_t^{(i)}$. The likelihood function $p(y_t | x_t)$ is defined by the assumed observation model $y_t = h_{\\text{assumed}}(x_t) + v_t$ with $v_t \\sim \\mathcal{N}(0, R_{\\text{assumed}})$.\n$$w_t^{(i)} \\propto p(y_t | x_t^{(i)}) = \\frac{1}{\\sqrt{2\\pi R_{\\text{assumed}}}} \\exp\\left(-\\frac{(y_t - h_{\\text{assumed}}(x_t^{(i)}))^2}{2 R_{\\text{assumed}}}\\right)$$\nThe constant pre-factor can be ignored as weights are normalized. For numerical stability, it is better to compute log-weights and then exponentiate. Let $\\hat{y}_t^{(i)} = h_{\\text{assumed}}(x_t^{(i)})$. The unnormalized weights are computed as $w_t^{(i)} = \\exp\\left( -\\frac{(y_t - \\hat{y}_t^{(i)})^2}{2R_{\\text{assumed}}} \\right)$.\n\n- **Step 4: Normalization**\nThe weights are normalized to sum to $1$:\n$$\\tilde{w}_t^{(i)} = \\frac{w_t^{(i)}}{\\sum_{j=1}^N w_t^{(j)}}$$\n\n- **Step 5: Performance Evaluation**\nThe degree of particle degeneracy—the phenomenon where a few particles acquire most of the weight—is quantified by the Effective Sample Size (ESS):\n$$\\mathrm{ESS} = \\frac{1}{\\sum_{i=1}^N (\\tilde{w}_t^{(i)})^2}$$\nThe ESS ranges from $1$ (complete degeneracy) to $N$ (all particles have equal weight). This entire procedure is repeated for each of the six test cases specified, using the same generated $y_t$ but with fresh particle sets drawn according to the problem's sequential random sampling scheme.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Implements one step of a bootstrap particle filter for a nonlinear state-space model,\n    calculating the Effective Sample Size (ESS) for six different test cases.\n    \"\"\"\n\n    # --- Step 0: Generate a single reproducible observation y_t ---\n\n    # Fixed random seed for reproducibility\n    rng = np.random.default_rng(123)\n\n    # True data-generating parameters\n    m_tm1 = 0.0\n    P_tm1 = 1.0\n    alpha_true = 0.7\n    beta_true = 1.0\n    x0_true = 0.0\n    Q_true = 0.2\n    R_true = 0.1\n\n    # True model functions\n    def f_true(x):\n        return x + alpha_true * np.sin(x)\n\n    def h_true(x):\n        return 1.0 / (1.0 + np.exp(-beta_true * (x - x0_true)))\n\n    # Generate the single observation y_t\n    x_tm1_true = rng.normal(loc=m_tm1, scale=np.sqrt(P_tm1))\n    eta_t_true = rng.normal(loc=0.0, scale=np.sqrt(Q_true))\n    x_t_true = f_true(x_tm1_true) + eta_t_true\n    v_t_true = rng.normal(loc=0.0, scale=np.sqrt(R_true))\n    y_t = h_true(x_t_true) + v_t_true\n\n    # --- Define Test Cases ---\n    test_cases = [\n        {\n            \"N\": 1000, \"alpha\": 0.7, \"Q\": 0.2,\n            \"h_type\": \"sigmoid\", \"beta\": 1.0, \"x0\": 0.0, \"R\": 0.1\n        },\n        {\n            \"N\": 1000, \"alpha\": 0.7, \"Q\": 0.02,\n            \"h_type\": \"sigmoid\", \"beta\": 1.0, \"x0\": 0.0, \"R\": 0.1\n        },\n        {\n            \"N\": 1000, \"alpha\": 0.7, \"Q\": 0.2,\n            \"h_type\": \"identity\", \"beta\": None, \"x0\": None, \"R\": 0.1\n        },\n        {\n            \"N\": 1000, \"alpha\": 0.7, \"Q\": 0.2,\n            \"h_type\": \"sigmoid\", \"beta\": 1.0, \"x0\": 0.0, \"R\": 1.0\n        },\n        {\n            \"N\": 20, \"alpha\": 0.7, \"Q\": 0.2,\n            \"h_type\": \"sigmoid\", \"beta\": 1.0, \"x0\": 0.0, \"R\": 0.1\n        },\n        {\n            \"N\": 1000, \"alpha\": 0.7, \"Q\": 1e-6,\n            \"h_type\": \"sigmoid\", \"beta\": 1.0, \"x0\": 0.0, \"R\": 0.1\n        },\n    ]\n\n    results = []\n\n    # --- Run Particle Filter for Each Case ---\n    for case in test_cases:\n        N = case[\"N\"]\n        alpha_assumed = case[\"alpha\"]\n        Q_assumed = case[\"Q\"]\n        R_assumed = case[\"R\"]\n\n        # Define assumed model functions for the current case\n        def f_assumed(x):\n            return x + alpha_assumed * np.sin(x)\n\n        if case[\"h_type\"] == \"sigmoid\":\n            beta_assumed = case[\"beta\"]\n            x0_assumed = case[\"x0\"]\n            def h_assumed(x):\n                return 1.0 / (1.0 + np.exp(-beta_assumed * (x - x0_assumed)))\n        else: # identity\n            def h_assumed(x):\n                return x\n\n        # Step 1: Initialization - Draw particles from the prior\n        x_tm1_particles = rng.normal(loc=m_tm1, scale=np.sqrt(P_tm1), size=N)\n\n        # Step 2: Prediction - Propagate particles\n        eta_t_particles = rng.normal(loc=0.0, scale=np.sqrt(Q_assumed), size=N)\n        x_t_particles = f_assumed(x_tm1_particles) + eta_t_particles\n\n        # Step 3: Update - Compute weights\n        # Calculate expected observations for each particle\n        y_hat_particles = h_assumed(x_t_particles)\n        \n        # Calculate log weights for numerical stability\n        log_weights = -0.5 * ((y_t - y_hat_particles)**2) / R_assumed\n\n        # Stabilize by subtracting the max log weight before exponentiating\n        log_weights -= np.max(log_weights)\n        unnormalized_weights = np.exp(log_weights)\n\n        # Step 4: Normalization\n        sum_weights = np.sum(unnormalized_weights)\n        if sum_weights == 0:\n            # Handle case where all weights are zero (underflow)\n            normalized_weights = np.full(N, 1.0 / N)\n        else:\n            normalized_weights = unnormalized_weights / sum_weights\n\n        # Step 5: Calculate Effective Sample Size (ESS)\n        ess = 1.0 / np.sum(normalized_weights**2)\n\n        results.append(ess)\n\n    # Final output formatting\n    # Note: rounding to 6 decimal places as required.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}