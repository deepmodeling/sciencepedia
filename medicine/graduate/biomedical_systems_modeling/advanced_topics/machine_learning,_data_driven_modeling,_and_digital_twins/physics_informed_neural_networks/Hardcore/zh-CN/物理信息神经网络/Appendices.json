{
    "hands_on_practices": [
        {
            "introduction": "构建物理信息神经网络（PINN）的第一步是将控制物理系统的偏微分方程（PDE）转化为可优化的损失函数。这项实践的核心任务是推导PDE残差——即当我们将神经网络的输出代入控制方程时所得到的表达式。通过求解量子力学中一个典型的一维谐振子问题，您将练习如何为一个给定的神经网络结构推导其精确的PDE残差表达式，这是理解PINN如何将物理定律编码到其学习过程中的基础。",
            "id": "2126326",
            "problem": "一位研究人员正在开发一个物理信息神经网络（PINN），用于近似质量为 $m$ 的粒子在一维量子谐振子势中的不含时薛定谔方程的基态解。其控制方程由下式给出：\n$$-\\frac{\\hbar^2}{2m} \\frac{d^2\\psi(x)}{dx^2} + V(x)\\psi(x) = E\\psi(x)$$\n其中 $\\psi(x)$ 是波函数，$\\hbar$ 是约化普朗克常数，$E$ 是能量本征值，势为 $V(x) = \\frac{1}{2}kx^2$，$k$ 为弹性系数。\n\n研究人员提出了一个简单的神经网络架构作为波函数的拟设（ansatz），记为 $\\psi_{NN}(x)$。该网络有一个用于位置 $x$ 的输入神经元，一个使用双曲正切激活函数的单神经元隐藏层，以及一个线性输出神经元。其数学形式为：\n$$\\psi_{NN}(x) = w_2 \\tanh(w_1 x + b_1) + b_2$$\n其中 $w_1, b_1$ 是隐藏层的权重和偏置，$w_2, b_2$ 是输出层的权重和偏置。\n\nPINN 训练过程的一个核心部分是偏微分方程（PDE）残差的最小化。残差记为 $R(x)$，定义为将神经网络的近似解代入控制微分方程时得到的值。即 $R(x) = -\\frac{\\hbar^2}{2m} \\frac{d^2\\psi_{NN}}{dx^2} + V(x)\\psi_{NN} - E\\psi_{NN}$。\n\n推导 PDE 残差 $R(x)$ 关于输入 $x$、网络参数（$w_1, b_1, w_2, b_2$）、能量 $E$ 和物理常数（$m, \\hbar, k$）的完整解析表达式。",
            "solution": "给定不含时薛定谔方程\n$$-\\frac{\\hbar^{2}}{2m}\\frac{d^{2}\\psi(x)}{dx^{2}}+V(x)\\psi(x)=E\\psi(x),$$\n其中 $V(x)=\\frac{k}{2}x^{2}$，以及神经网络拟设\n$$\\psi_{NN}(x)=w_{2}\\tanh(w_{1}x+b_{1})+b_{2}.$$\nPDE 残差定义为\n$$R(x)=-\\frac{\\hbar^{2}}{2m}\\frac{d^{2}\\psi_{NN}}{dx^{2}}+V(x)\\psi_{NN}-E\\psi_{NN}.$$\n\n使用链式法则和 $d(\\tanh u)/du=\\frac{1}{\\cosh^{2}u}$ 计算一阶导数：\n$$\\frac{d\\psi_{NN}}{dx}=w_{2}\\cdot \\frac{d}{dx}\\tanh(w_{1}x+b_{1})=w_{2}\\cdot \\frac{1}{\\cosh^{2}(w_{1}x+b_{1})}\\cdot w_{1}=\\frac{w_{1}w_{2}}{\\cosh^{2}(w_{1}x+b_{1})}.$$\n\n再次使用链式法则和 $d(\\cosh^{-2}u)/du=-2\\cosh^{-3}u\\sinh u$ 计算二阶导数：\n$$\\frac{d^{2}\\psi_{NN}}{dx^{2}}=w_{1}w_{2}\\cdot \\frac{d}{dx}\\big[\\cosh^{-2}(w_{1}x+b_{1})\\big]=w_{1}w_{2}\\cdot \\big[-2\\cosh^{-3}(w_{1}x+b_{1})\\sinh(w_{1}x+b_{1})\\big]\\cdot w_{1},$$\n所以\n$$\\frac{d^{2}\\psi_{NN}}{dx^{2}}=-2w_{1}^{2}w_{2}\\,\\frac{\\sinh(w_{1}x+b_{1})}{\\cosh^{3}(w_{1}x+b_{1})}=-2w_{1}^{2}w_{2}\\,\\frac{\\tanh(w_{1}x+b_{1})}{\\cosh^{2}(w_{1}x+b_{1})}.$$\n\n将结果代入残差定义，其中 $V(x)=\\frac{k}{2}x^{2}$：\n$$R(x)=-\\frac{\\hbar^{2}}{2m}\\left[-2w_{1}^{2}w_{2}\\,\\frac{\\tanh(w_{1}x+b_{1})}{\\cosh^{2}(w_{1}x+b_{1})}\\right]+\\frac{k}{2}x^{2}\\big[w_{2}\\tanh(w_{1}x+b_{1})+b_{2}\\big]-E\\big[w_{2}\\tanh(w_{1}x+b_{1})+b_{2}\\big].$$\n\n化简动能项得到\n$$R(x)=\\frac{\\hbar^{2}}{m}w_{1}^{2}w_{2}\\,\\frac{\\tanh(w_{1}x+b_{1})}{\\cosh^{2}(w_{1}x+b_{1})}+\\left(\\frac{k}{2}x^{2}-E\\right)\\left[w_{2}\\tanh(w_{1}x+b_{1})+b_{2}\\right].$$\n这就是 PDE 残差关于 $x$、网络参数、$E$ 和物理常数的完整解析表达式。",
            "answer": "$$\\boxed{\\frac{\\hbar^{2}}{m}w_{2}w_{1}^{2}\\,\\frac{\\tanh\\!\\left(w_{1}x+b_{1}\\right)}{\\cosh^{2}\\!\\left(w_{1}x+b_{1}\\right)}+\\left(\\frac{k}{2}x^{2}-E\\right)\\left[w_{2}\\tanh\\!\\left(w_{1}x+b_{1}\\right)+b_{2}\\right]}$$"
        },
        {
            "introduction": "一个偏微分方程的解不仅取决于其控制方程本身，还唯一地取决于其边界条件。本练习旨在解决在PINN中施加边界条件的实际问题，特别是当边界上同时存在狄利克雷（Dirichlet）条件（指定函数值）和诺伊曼（Neumann）条件（指定法向导数）时。您将学习如何为这两种不同类型的边界数据构建一个组合的、归一化的损失函数，并理解自动微分（AD）在处理诺伊曼边界条件中的关键作用。",
            "id": "4235901",
            "problem": "一个用于稳态导电场的信息物理系统数字孪生由拉普拉斯方程建模，该方程是在电导率恒定且无源的条件下根据通量守恒推导出来的。设物理势为 $u:\\Omega \\to \\mathbb{R}$，其中 $\\Omega \\subset \\mathbb{R}^{d}$ 是一个具有Lipschitz边界 $\\partial \\Omega$ 的有界域。控制方程为 $\\Omega$ 内的 $-\\Delta u = 0$，并带有混合边界条件：在 $\\Gamma_{D} \\subset \\partial \\Omega$ 上 $u=g$，在 $\\Gamma_{N}=\\partial \\Omega \\setminus \\Gamma_{D}$ 上 $\\partial_{n} u = h$。其中 $g:\\Gamma_{D} \\to \\mathbb{R}$ 和 $h:\\Gamma_{N} \\to \\mathbb{R}$ 由边界传感器测量，$\\partial_{n} u$ 表示法向导数。\n\n您部署一个带有参数 $\\theta$ 的物理信息神经网络（PINN）$u_{\\theta}:\\Omega \\to \\mathbb{R}$，作为数字孪生代理模型。为进行边界训练，您在每个边界段上收集求积样本：\n- 狄利克雷样本 $\\{x_{i}^{D}\\}_{i=1}^{N_{D}} \\subset \\Gamma_{D}$，及其相关的测量值 $\\{g_{i}\\}_{i=1}^{N_{D}}$ 和正的求积权重 $\\{w_{i}^{D}\\}_{i=1}^{N_{D}}$，这些权重用于近似在 $\\Gamma_{D}$ 上的积分。\n- 诺伊曼样本 $\\{x_{j}^{N}\\}_{j=1}^{N_{N}} \\subset \\Gamma_{N}$，及其测量的通量 $\\{h_{j}\\}_{j=1}^{N_{N}}$、单位外法向量 $\\{n_{j}\\}_{j=1}^{N_{N}} \\subset \\mathbb{R}^{d}$ 和正的求积权重 $\\{w_{j}^{N}\\}_{j=1}^{N_{N}}$。\n\n令 $W_{D}=\\sum_{i=1}^{N_{D}} w_{i}^{D}$ 和 $W_{N}=\\sum_{j=1}^{N_{N}} w_{j}^{N}$ 为总求积权重。假设您选择正的惩罚系数 $\\lambda_{D}$ 和 $\\lambda_{N}$ 来平衡狄利克雷和诺伊曼边界条件的施加强度。\n\n从基本定律 $-\\Delta u = 0$ 以及狄利克雷和诺伊曼边界条件的定义出发，分别在 $\\Gamma_{D}$ 和 $\\Gamma_{N}$ 上推导独立的边界残差泛函。这些泛函分别用于度量 $u_{\\theta}$ 与 $g$ 之间以及 $\\nabla u_{\\theta} \\cdot n$ 与 $h$ 之间的平方失配。然后，使用给定样本和权重的求积近似，构建一个单一的、复合的、无量纲的纯边界损失泛函 $L_{\\mathrm{bc}}(\\theta)$。其值是由 $W_{D}$ 和 $W_{N}$ 归一化后的两个边界残差的加权和。请用给定的数据 $\\{x_{i}^{D},g_{i},w_{i}^{D}\\}_{i=1}^{N_{D}}$、$\\{x_{j}^{N},h_{j},n_{j},w_{j}^{N}\\}_{j=1}^{N_{N}}$ 和网络 $u_{\\theta}$，将 $L_{\\mathrm{bc}}(\\theta)$ 显式地表达为一个闭式解析表达式。在您的推导过程中，讨论如何使用自动微分（AD）在边界点上获得 $\\nabla u_{\\theta}$，从而施加每个边界残差。\n\n您的最终答案必须是 $L_{\\mathrm{bc}}(\\theta)$ 作为 $\\theta$ 的函数的单个显式表达式，仅包含符号量和给定数据。不应包含单位。无需四舍五入。",
            "solution": "该问题要求为一个物理势 $u$ 的物理信息神经网络（PINN）代理模型 $u_{\\theta}$ 推导一个复合、无量纲的纯边界损失泛函，记作 $L_{\\mathrm{bc}}(\\theta)$。该系统由定义在域 $\\Omega \\subset \\mathbb{R}^{d}$ 上的拉普拉斯方程 $-\\Delta u = 0$ 和混合狄利克雷与诺伊曼边界条件所控制。损失泛函是根据边界上的离散传感器数据构建的。\n\n我们首先为每种类型的边界条件形式化地定义连续残差泛函。这些泛函用于度量 PINN 代理模型 $u_{\\theta}$ 未能满足边界 $\\partial \\Omega$ 上给定条件的程度。\n\n首先，考虑狄利克雷边界 $\\Gamma_{D}$，在该边界上势被指定为 $u=g$。在任意点 $x \\in \\Gamma_D$ 上的失配（或残差）由差值 $u_{\\theta}(x) - g(x)$ 给出。为了度量该边界段上的总误差，我们对该残差的平方进行积分。这就得到了连续的狄利克雷边界残差泛函 $R_{D}(\\theta)$:\n$$\nR_{D}(\\theta) = \\int_{\\Gamma_{D}} (u_{\\theta}(x) - g(x))^2 \\, dS\n$$\n其中 $dS$ 是边界 $\\partial\\Omega$ 上的表面测度。\n\n其次，考虑诺伊曼边界 $\\Gamma_{N}$，在该边界上势的法向导数被指定为 $\\partial_{n} u = h$。法向导数是梯度在单位外法向量 $n(x)$ 上的投影，即 $\\partial_{n} u(x) = \\nabla u(x) \\cdot n(x)$。在点 $x \\in \\Gamma_N$ 处的残差是 PINN 近似模型的法向导数与给定函数 $h(x)$ 之间的差值，即 $\\nabla u_{\\theta}(x) \\cdot n(x) - h(x)$。那么，诺伊曼边界上的总平方误差由连续的诺伊曼边界残差泛函 $R_{N}(\\theta)$ 给出：\n$$\nR_{N}(\\theta) = \\int_{\\Gamma_{N}} (\\nabla u_{\\theta}(x) \\cdot n(x) - h(x))^2 \\, dS\n$$\n\n在实际应用中，我们没有连续函数 $g$ 和 $h$，而是在特定传感器位置上的离散测量值。题目提供了样本点集和求积权重，用于近似 $R_{D}(\\theta)$ 和 $R_{N}(\\theta)$ 的积分。\n\n对于狄利克雷边界，我们有 $N_D$ 个样本点 $\\{x_{i}^{D}\\}_{i=1}^{N_{D}} \\subset \\Gamma_{D}$，以及对应的势测量值 $\\{g_{i}\\}_{i=1}^{N_{D}}$（其中 $g_i = g(x_i^D)$）和正的求积权重 $\\{w_{i}^{D}\\}_{i=1}^{N_{D}}$。$R_{D}(\\theta)$ 的积分通过一个加权和来近似，我们将其表示为离散残差 $\\hat{R}_{D}(\\theta)$:\n$$\n\\hat{R}_{D}(\\theta) = \\sum_{i=1}^{N_{D}} w_{i}^{D} (u_{\\theta}(x_{i}^{D}) - g_{i})^2\n$$\n这是积分 $\\int_{\\Gamma_{D}} (u_{\\theta} - g)^2 \\, dS$ 的一个数值求积近似。\n\n对于诺伊曼边界，我们有 $N_N$ 个样本点 $\\{x_{j}^{N}\\}_{j=1}^{N_{N}} \\subset \\Gamma_{N}$，以及通量测量值 $\\{h_{j}\\}_{j=1}^{N_{N}}$、单位外法向量 $\\{n_{j}\\}_{j=1}^{N_{N}}$ 和正的求积权重 $\\{w_{j}^{N}\\}_{j=1}^{N_{N}}$。这里的关键计算是项 $\\nabla u_{\\theta}(x_j^N)$，即神经网络输出关于其空间输入的梯度。这是使用自动微分（AD）计算的。神经网络 $u_{\\theta}$ 是可微基本函数（例如，仿射变换和激活函数）的复合。因此，它对其输入的导数可以通过算法计算到机器精度。深度学习框架原生提供此功能，通常通过反向模式AD（反向传播）实现，从而可以在计算损失所需的任意点 $x_j^N$ 处高效地评估 $\\nabla u_{\\theta}$。\n\n使用 AD 获得 $\\nabla u_{\\theta}(x_{j}^{N})$ 后，我们就可以用离散的诺伊曼残差 $\\hat{R}_{N}(\\theta)$ 来近似 $R_{N}(\\theta)$ 的积分：\n$$\n\\hat{R}_{N}(\\theta) = \\sum_{j=1}^{N_{N}} w_{j}^{N} (\\nabla u_{\\theta}(x_{j}^{N}) \\cdot n_{j} - h_{j})^2\n$$\n\n题目要求一个单一的复合损失泛函 $L_{\\mathrm{bc}}(\\theta)$，它是两个边界残差的加权和，并通过它们各自的总求积权重 $W_{D}=\\sum_{i=1}^{N_{D}} w_{i}^{D}$ 和 $W_{N}=\\sum_{j=1}^{N_{N}} w_{j}^{N}$ 进行归一化。这种归一化将每个离散残差转化为一个均方误差近似值。我们还得到正的惩罚系数 $\\lambda_{D}$ 和 $\\lambda_{N}$，以平衡施加每个边界条件的相对重要性。\n归一化的狄利克雷损失项为 $\\lambda_{D} \\frac{\\hat{R}_{D}(\\theta)}{W_{D}}$。\n归一化的诺伊曼损失项为 $\\lambda_{N} \\frac{\\hat{R}_{N}(\\theta)}{W_{N}}$。\n\n题目规定最终的损失泛函 $L_{\\mathrm{bc}}(\\theta)$ 应该是无量纲的。这通常意味着控制方程和边界条件已事先使用长度、势等的特征尺度进行了无量纲化处理。在这个标准假设下，损失函数中的所有量都是无量纲的，惩罚系数 $\\lambda_D$ 和 $\\lambda_N$ 也是用于调整训练过程的无量纲因子。\n\n综合这些组成部分，复合边界损失泛函是归一化和加权的离散残差之和：\n$$\nL_{\\mathrm{bc}}(\\theta) = \\lambda_{D} \\frac{\\hat{R}_{D}(\\theta)}{W_{D}} + \\lambda_{N} \\frac{\\hat{R}_{N}(\\theta)}{W_{N}}\n$$\n代入 $\\hat{R}_{D}(\\theta)$、$\\hat{R}_{N}(\\theta)$、$W_{D}$ 和 $W_{N}$ 的表达式：\n$$\nL_{\\mathrm{bc}}(\\theta) = \\lambda_{D} \\left( \\frac{\\sum_{i=1}^{N_{D}} w_{i}^{D} (u_{\\theta}(x_{i}^{D}) - g_{i})^2}{\\sum_{i=1}^{N_{D}} w_{i}^{D}} \\right) + \\lambda_{N} \\left( \\frac{\\sum_{j=1}^{N_{N}} w_{j}^{N} (\\nabla u_{\\theta}(x_{j}^{N}) \\cdot n_{j} - h_{j})^2}{\\sum_{j=1}^{N_{N}} w_{j}^{N}} \\right)\n$$\n使用给定的定义 $W_{D}$ 和 $W_{N}$，这可以写得更简洁：\n$$\nL_{\\mathrm{bc}}(\\theta) = \\frac{\\lambda_{D}}{W_{D}} \\sum_{i=1}^{N_{D}} w_{i}^{D} (u_{\\theta}(x_{i}^{D}) - g_{i})^2 + \\frac{\\lambda_{N}}{W_{N}} \\sum_{j=1}^{N_{N}} w_{j}^{N} (\\nabla u_{\\theta}(x_{j}^{N}) \\cdot n_{j} - h_{j})^2\n$$\n该表达式代表了纯边界损失泛函 $L_{\\mathrm{bc}}(\\theta)$ 的最终闭式解析形式，它是网络参数 $\\theta$ 和给定数据的函数。通过对 $\\theta$ 最小化此损失泛函，可以训练网络在最小二乘意义上满足边界条件。",
            "answer": "$$\n\\boxed{\\frac{\\lambda_{D}}{W_{D}} \\sum_{i=1}^{N_{D}} w_{i}^{D} (u_{\\theta}(x_{i}^{D}) - g_{i})^2 + \\frac{\\lambda_{N}}{W_{N}} \\sum_{j=1}^{N_{N}} w_{j}^{N} (\\nabla u_{\\theta}(x_{j}^{N}) \\cdot n_{j} - h_{j})^2}\n$$"
        },
        {
            "introduction": "除了求解已知方程（正问题），PINN在根据稀疏观测数据发现未知物理定律（逆问题）方面也显示出巨大潜力。这项实践将引导您探索这一前沿应用，其目标是根据一组分散的数据点来确定一个假设的非线性PDE中的未知物理系数。通过构建一个包含数据匹配项和物理残差项的混合损失函数，您将理解PINN如何同时学习解的函数形式和控制方程本身的参数，从而实现数据驱动的科学发现。",
            "id": "2126328",
            "problem": "一位研究人员正在研究一个由一维偏微分方程（PDE）控制的未知物理过程。通过实验，他们收集了一组稀疏的、关于量 $u(x, t)$ 的 $N_u$ 个测量值，以数据点 $\\{ (x_i, t_i, u_i) \\}_{i=1}^{N_u}$ 的形式给出。研究人员假设其底层的偏微分方程是带有未知系数的伯格斯方程（Burgers' equation）的一种形式：\n$$\nu_t + c_1 u u_x - c_2 u_{xx} = 0\n$$\n其中 $u_t = \\frac{\\partial u}{\\partial t}$，$u_x = \\frac{\\partial u}{\\partial x}$，以及 $u_{xx} = \\frac{\\partial^2 u}{\\partial x^2}$。实值系数 $c_1$ 和 $c_2$ 需要从数据中发现。\n\n为了解决这个反问题，研究人员采用了一种物理信息神经网络（Physics-Informed Neural Network, PINN）。解 $u(x, t)$ 由一个神经网络 $\\mathcal{N}(x, t; \\boldsymbol{\\theta})$ 来近似，其中 $\\boldsymbol{\\theta}$ 表示网络的可训练权重和偏置。未知系数 $c_1$ 和 $c_2$ 也被视为可训练参数。\n\n训练过程通过最小化一个总损失函数 $L = L_{data} + L_{PDE}$ 来优化 $\\boldsymbol{\\theta}$、$c_1$ 和 $c_2$。数据保真度损失 $L_{data}$ 是网络预测值与测量值之间的均方误差：\n$$\nL_{data} = \\frac{1}{N_u} \\sum_{i=1}^{N_u} \\left| \\mathcal{N}(x_i, t_i; \\boldsymbol{\\theta}) - u_i \\right|^2\n$$\n物理信息损失 $L_{PDE}$ 用来强制施加所假设的偏微分方程的结构。它被定义为在整个区域内分布的一组 $N_f$ 个配置点 $\\{ (x_j^f, t_j^f) \\}_{j=1}^{N_f}$ 上评估的偏微分方程残差的均方误差。$\\mathcal{N}$ 的所有偏导数都使用自动微分计算。\n\n下列哪个表达式正确定义了物理信息损失项 $L_{PDE}$？为简洁起见，记号 $\\mathcal{N}_j$ 表示 $\\mathcal{N}(x_j^f, t_j^f; \\boldsymbol{\\theta})$，并且偏导数在配置点 $(x_j^f, t_j^f)$ 处进行评估。\n\nA. $\\frac{1}{N_f} \\sum_{j=1}^{N_f} \\left| \\frac{\\partial \\mathcal{N}}{\\partial t} + c_1 \\mathcal{N}_j \\frac{\\partial \\mathcal{N}}{\\partial x} - c_2 \\frac{\\partial^2 \\mathcal{N}}{\\partial x^2} \\right|^2$\n\nB. $\\frac{1}{N_f} \\sum_{j=1}^{N_f} \\left| \\frac{\\partial \\mathcal{N}}{\\partial t} + \\frac{\\partial (c_1 \\mathcal{N}_j)}{\\partial x} - \\frac{\\partial^2 (c_2 \\mathcal{N}_j)}{\\partial x^2} \\right|^2$\n\nC. $\\frac{1}{N_u} \\sum_{i=1}^{N_u} \\left| \\frac{\\partial \\mathcal{N}}{\\partial t} + c_1 u_i \\frac{\\partial \\mathcal{N}}{\\partial x} - c_2 \\frac{\\partial^2 \\mathcal{N}}{\\partial x^2} \\right|^2$\n\nD. $\\frac{1}{N_f} \\sum_{j=1}^{N_f} \\left( \\frac{\\partial \\mathcal{N}}{\\partial t} + c_1 \\mathcal{N}_j \\frac{\\partial \\mathcal{N}}{\\partial x} - c_2 \\frac{\\partial^2 \\mathcal{N}}{\\partial x^2} \\right)$\n\nE. $\\frac{1}{N_f} \\sum_{j=1}^{N_f} \\left| \\mathcal{N}_j - u_j \\right|^2 + \\frac{1}{N_f} \\sum_{j=1}^{N_f} \\left| c_1 \\mathcal{N}_j \\frac{\\partial \\mathcal{N}}{\\partial x} - c_2 \\frac{\\partial^2 \\mathcal{N}}{\\partial x^2} \\right|^2$",
            "solution": "目标是为发现假设的偏微分方程中的参数 $c_1$ 和 $c_2$ 构建物理信息损失项 $L_{PDE}$：\n$$\nu_t + c_1 u u_x - c_2 u_{xx} = 0\n$$\n\n步骤1：定义偏微分方程（PDE）残差。\n偏微分方程的残差，记为 $f(x, t)$，是当 $u(x, t)$ 为真解时应等于零的表达式。\n$$\nf(x, t) = u_t + c_1 u u_x - c_2 u_{xx}\n$$\n\n步骤2：使用PINN近似解及其导数。\n物理信息神经网络（PINN）框架用神经网络的输出 $\\mathcal{N}(x, t; \\boldsymbol{\\theta})$ 来近似未知解 $u(x, t)$。PINN的关键特征是使用自动微分来计算网络输出对其输入的必要偏导数。\n因此，我们有以下近似：\n-   $u(x, t) \\approx \\mathcal{N}(x, t; \\boldsymbol{\\theta})$\n-   $u_t(x, t) \\approx \\frac{\\partial \\mathcal{N}}{\\partial t}(x, t; \\boldsymbol{\\theta})$\n-   $u_x(x, t) \\approx \\frac{\\partial \\mathcal{N}}{\\partial x}(x, t; \\boldsymbol{\\theta})$\n-   $u_{xx}(x, t) \\approx \\frac{\\partial^2 \\mathcal{N}}{\\partial x^2}(x, t; \\boldsymbol{\\theta})$\n\n步骤3：为PINN构建残差。\n通过将这些近似值代入偏微分方程的残差表达式中，我们得到了作为网络和未知参数 $c_1, c_2$ 的函数的残差。\n$$\nf(x, t; \\boldsymbol{\\theta}, c_1, c_2) \\approx \\frac{\\partial \\mathcal{N}}{\\partial t} + c_1 \\mathcal{N} \\frac{\\partial \\mathcal{N}}{\\partial x} - c_2 \\frac{\\partial^2 \\mathcal{N}}{\\partial x^2}\n$$\n请注意，在非线性项 $u u_x$ 中，$u$ 的两个实例都必须被同一个网络近似 $\\mathcal{N}$ 所替代。\n\n步骤4：定义物理信息损失 $L_{PDE}$。\n物理信息损失旨在通过最小化此残差的大小来强制执行偏微分方程。这通常通过最小化在一组 $N_f$ 个配置点 $\\{ (x_j^f, t_j^f) \\}_{j=1}^{N_f}$ 上评估的残差的均方误差来完成。对于每个点 $j$，残差的平方为：\n$$\n\\left| \\frac{\\partial \\mathcal{N}}{\\partial t}\\bigg|_{(x_j^f, t_j^f)} + c_1 \\mathcal{N}(x_j^f, t_j^f) \\frac{\\partial \\mathcal{N}}{\\partial x}\\bigg|_{(x_j^f, t_j^f)} - c_2 \\frac{\\partial^2 \\mathcal{N}}{\\partial x^2}\\bigg|_{(x_j^f, t_j^f)} \\right|^2\n$$\n均方误差是所有 $N_f$ 个配置点上这些值的平均值：\n$$\nL_{PDE} = \\frac{1}{N_f} \\sum_{j=1}^{N_f} \\left| \\frac{\\partial \\mathcal{N}}{\\partial t}\\bigg|_{(x_j^f, t_j^f)} + c_1 \\mathcal{N}(x_j^f, t_j^f) \\frac{\\partial \\mathcal{N}}{\\partial x}\\bigg|_{(x_j^f, t_j^f)} - c_2 \\frac{\\partial^2 \\mathcal{N}}{\\partial x^2}\\bigg|_{(x_j^f, t_j^f)} \\right|^2\n$$\n使用问题中提供的简写符号，这可以表示为：\n$$\nL_{PDE} = \\frac{1}{N_f} \\sum_{j=1}^{N_f} \\left| \\frac{\\partial \\mathcal{N}}{\\partial t} + c_1 \\mathcal{N}_j \\frac{\\partial \\mathcal{N}}{\\partial x} - c_2 \\frac{\\partial^2 \\mathcal{N}}{\\partial x^2} \\right|^2\n$$\n该表达式与选项A匹配。\n\n步骤5：分析不正确的选项。\n-   **选项B：** 它错误地将系数 $c_1$ 和 $c_2$ 放在了微分算子内部。由于 $c_1$ 和 $c_2$ 是关于 $x$ 和 $t$ 的常数，这种形式等价于 $\\frac{\\partial (c_1 \\mathcal{N}_j)}{\\partial x} = c_1 \\frac{\\partial \\mathcal{N}_j}{\\partial x}$ 和 $\\frac{\\partial^2 (c_2 \\mathcal{N}_j)}{\\partial x^2} = c_2 \\frac{\\partial^2 \\mathcal{N}_j}{\\partial x^2}$。然而，在该形式中项 $c_1 u u_x$ 变成了 $\\frac{\\partial(c_1 \\mathcal{N})}{\\partial x}$，这是错误的。该项应为 $c_1 \\mathcal{N} \\frac{\\partial \\mathcal{N}}{\\partial x}$。\n-   **选项C：** 这个表达式有两个基本错误。首先，它在一组稀疏的 $N_u$ 个数据点上评估PDE残差，而不是在更大的 $N_f$ 个配置点集上。配置点的目的是在整个域上强制执行PDE，而不仅仅是在有数据的地方。其次，它在残差表达式中错误地使用了测量数据值 $u_i$（$c_1 u_i \\frac{\\partial \\mathcal{N}}{\\partial x}$）。为了保持数学上的一致性，残差必须仅用网络近似 $\\mathcal{N}$ 及其导数来表示。\n-   **选项D：** 这个表达式求的是带符号残差的和，而不是像均方误差这样的非负量。损失函数必须是非负的，并且在期望的解处有最小值。即使单个残差很大且非零（例如，$+100$ 和 $-100$ 相互抵消），带符号值的和也可能为零，这不会导向正确的解。\n-   **选项E：** 这个表达式错误地混合了不同部分。它包含一个项 $|\\mathcal{N}_j - u_j|^2$，看起来像一个数据损失项，但它是在配置点上评估的，而在这些点上真值 $u_j$ 是未知的。此外，第二项是一个不完整的残差，因为它缺少时间导数项 $\\frac{\\partial \\mathcal{N}}{\\partial t}$。这对应于一个不同的物理模型。\n\n因此，物理信息损失 $L_{PDE}$ 的唯一正确表达式是选项A中给出的那个。",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}