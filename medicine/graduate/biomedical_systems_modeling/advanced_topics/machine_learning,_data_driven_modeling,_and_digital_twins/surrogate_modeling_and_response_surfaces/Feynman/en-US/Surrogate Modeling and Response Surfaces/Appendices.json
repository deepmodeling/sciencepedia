{
    "hands_on_practices": [
        {
            "introduction": "This first exercise provides a foundational experience in constructing a surrogate model from the ground up. You will implement a polynomial response surface, a cornerstone of surrogate modeling, and fit it to data using regularized least squares . This practice is essential for translating the mathematical theory of linear regression and Tikhonov regularization into a practical, working algorithm, and it highlights how regularization helps create robust models that avoid overfitting to noisy data.",
            "id": "3933515",
            "problem": "You are given scalar training data pairs $\\{(x_i,y_i)\\}_{i=1}^n$ originating from a deterministic, noise-perturbed computational cardiac model that maps a scalar input parameter $x$ (dimensionless) to a scalar model output $y$ (dimensionless). Your task is to construct a surrogate model and evaluate its predictive performance on specified query inputs. Start from the following fundamental base: the surrogate model is a linear combination of fixed basis functions, and the best fit under a quadratic loss with Tikhonov (ridge) regularization is obtained by minimizing the regularized sum of squared residuals. You must use a univariate polynomial basis of degree $d$, that is, basis functions $\\{\\phi_k(x)\\}_{k=0}^d$ with $\\phi_k(x)=x^k$. The surrogate model is $f_s(x)=\\sum_{k=0}^d w_k \\phi_k(x)$ with coefficients $\\{w_k\\}_{k=0}^d$ that minimize the regularized empirical risk\n$$\nJ(\\mathbf{w})=\\sum_{i=1}^n\\left(f_s(x_i)-y_i\\right)^2+\\lambda\\lVert \\mathbf{w}\\rVert_2^2,\n$$\nwhere $\\lambda \\ge 0$ is the Tikhonov regularization parameter. After fitting, compute the predicted value $f_s(x^\\star)$ at a specified query $x^\\star$ and the training Root Mean Squared Error (RMSE)\n$$\n\\mathrm{RMSE}=\\sqrt{\\frac{1}{n}\\sum_{i=1}^n\\left(f_s(x_i)-y_i\\right)^2}.\n$$\nAll inputs $x$, outputs $y$, coefficients $\\mathbf{w}$, and predictions are dimensionless. Angles are not involved. You must round every reported floating-point value to $6$ decimal places.\n\nImplement a program that, for each provided test case, fits the polynomial surrogate of degree $d$ with regularization $\\lambda$, computes $f_s(x^\\star)$ and the training $\\mathrm{RMSE}$, and returns both values.\n\nTest Suite:\nFor each case below, use the given tuples $(\\{x_i\\},\\{y_i\\},d,\\lambda,x^\\star)$.\n\n- Case $1$ (exact quadratic fit):\n  - $\\{x_i\\}$:\n    $$\n    [-2.0,-1.0,0.0,1.0,2.0]\n    $$\n  - $\\{y_i\\}$, generated by $y=0.5x^2-x+2$:\n    $$\n    \\left[0.5\\cdot(-2.0)^2-(-2.0)+2,\\;0.5\\cdot(-1.0)^2-(-1.0)+2,\\;0.5\\cdot 0.0^2-0.0+2,\\;0.5\\cdot 1.0^2-1.0+2,\\;0.5\\cdot 2.0^2-2.0+2\\right]\n    $$\n    that is,\n    $$\n    [6.0,3.5,2.0,1.5,2.0]\n    $$\n  - Degree $d$:\n    $$\n    2\n    $$\n  - Regularization $\\lambda$:\n    $$\n    0.0\n    $$\n  - Query $x^\\star$:\n    $$\n    0.5\n    $$\n\n- Case $2$ (cubic with specified perturbations):\n  - $\\{x_i\\}$:\n    $$\n    [-2.0,-1.5,-1.0,-0.5,0.0,0.5,1.0,1.5,2.0]\n    $$\n  - $\\{y_i\\}$, specified as $y=x^3-2x+\\epsilon$ with fixed $\\epsilon$:\n    $$\n    \\epsilon=\\left[0.05,-0.04,0.02,0.01,0.0,-0.03,0.04,0.02,-0.05\\right]\n    $$\n    hence\n    $$\n    \\{y_i\\}=\\left[(-2.0)^3-2(-2.0)+0.05,\\;(-1.5)^3-2(-1.5)-0.04,\\;(-1.0)^3-2(-1.0)+0.02,\\;(-0.5)^3-2(-0.5)+0.01,\\;0.0^3-2(0.0)+0.0,\\;(0.5)^3-2(0.5)-0.03,\\;1.0^3-2(1.0)+0.04,\\;1.5^3-2(1.5)+0.02,\\;2.0^3-2(2.0)-0.05\\right]\n    $$\n  - Degree $d$:\n    $$\n    3\n    $$\n  - Regularization $\\lambda$:\n    $$\n    0.001\n    $$\n  - Query $x^\\star$:\n    $$\n    1.2\n    $$\n\n- Case $3$ (duplicated inputs with linear trend):\n  - $\\{x_i\\}$:\n    $$\n    [0.0,0.0,0.5,0.5,1.0,1.0]\n    $$\n  - $\\{y_i\\}$, specified as $y=0.2+1.5x+\\eta$ with fixed $\\eta$:\n    $$\n    \\eta=[0.0,0.01,-0.02,0.02,0.0,-0.01]\n    $$\n    hence\n    $$\n    \\{y_i\\}=[0.2,0.21,0.93,0.97,1.7,1.69]\n    $$\n  - Degree $d$:\n    $$\n    1\n    $$\n  - Regularization $\\lambda$:\n    $$\n    0.01\n    $$\n  - Query $x^\\star$:\n    $$\n    0.8\n    $$\n\n- Case $4$ (higher-order polynomial, near-interpolation):\n  - $\\{x_i\\}$:\n    $$\n    [-1.0,-0.6,-0.2,0.2,0.6,1.0]\n    $$\n  - $\\{y_i\\}$, specified exactly by\n    $$\n    y=1.0-0.5x+0.3x^2-0.1x^3+0.05x^4-0.02x^5\n    $$\n    evaluated at the given $\\{x_i\\}$.\n  - Degree $d$:\n    $$\n    5\n    $$\n  - Regularization $\\lambda$:\n    $$\n    10^{-8}\n    $$\n  - Query $x^\\star$:\n    $$\n    -0.9\n    $$\n\n- Case $5$ (single-point, constant model):\n  - $\\{x_i\\}$:\n    $$\n    [1.2]\n    $$\n  - $\\{y_i\\}$:\n    $$\n    [3.4]\n    $$\n  - Degree $d$:\n    $$\n    0\n    $$\n  - Regularization $\\lambda$:\n    $$\n    0.0\n    $$\n  - Query $x^\\star$:\n    $$\n    2.0\n    $$\n\nYour program should produce a single line of output containing the results across all cases as a list of lists, where each inner list contains the two rounded floats $[f_s(x^\\star),\\mathrm{RMSE}]$ for the corresponding case, in the same order as above. For example, the output format must be\n$$\n\\left[\\,[f_s(x^\\star_1),\\mathrm{RMSE}_1],[f_s(x^\\star_2),\\mathrm{RMSE}_2],\\dots\\,[f_s(x^\\star_5),\\mathrm{RMSE}_5]\\,\\right],\n$$\nwith every float rounded to $6$ decimal places and no additional text.",
            "solution": "The posed problem is a well-defined application of regularized linear regression for the purpose of constructing a surrogate model. All provided data and parameters are consistent and sufficient for deriving a unique and meaningful solution. The problem is scientifically and mathematically sound, grounded in the standard principles of numerical-functional approximation and machine learning. We will therefore proceed with a complete solution.\n\nThe core of the problem is to find a set of coefficients $\\mathbf{w} = [w_0, w_1, \\dots, w_d]^T$ for a polynomial surrogate model $f_s(x) = \\sum_{k=0}^d w_k x^k$ that best fits the given training data $\\{(x_i, y_i)\\}_{i=1}^n$. The criterion for \"best fit\" is the minimization of the Tikhonov-regularized sum of squared residuals, given by the objective function:\n$$\nJ(\\mathbf{w}) = \\sum_{i=1}^n \\left( f_s(x_i) - y_i \\right)^2 + \\lambda \\lVert \\mathbf{w} \\rVert_2^2\n$$\nHere, $\\lambda \\ge 0$ is the regularization parameter that penalizes large coefficient values, thereby preventing overfitting and improving the conditioning of the problem.\n\nTo solve this minimization problem, we first express it in matrix-vector notation. Let $\\mathbf{y} = [y_1, y_2, \\dots, y_n]^T$ be the vector of observed outputs. The model's predictions for all training inputs can be written as $\\hat{\\mathbf{y}} = \\mathbf{\\Phi}\\mathbf{w}$, where $\\mathbf{\\Phi}$ is the $n \\times (d+1)$ design matrix. Each row of $\\mathbf{\\Phi}$ corresponds to a data point $x_i$, and each column corresponds to a basis function $\\phi_k(x) = x^k$. Thus, the element at row $i$ and column $j$ (using $0$-based indexing, $j=k$) is $\\mathbf{\\Phi}_{ij} = x_i^j$.\n$$\n\\mathbf{\\Phi} = \\begin{pmatrix}\n1 & x_1 & x_1^2 & \\dots & x_1^d \\\\\n1 & x_2 & x_2^2 & \\dots & x_2^d \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_n & x_n^2 & \\dots & x_n^d\n\\end{pmatrix}\n$$\nThe objective function $J(\\mathbf{w})$ can now be written in terms of matrices and vectors:\n$$\nJ(\\mathbf{w}) = (\\mathbf{\\Phi}\\mathbf{w} - \\mathbf{y})^T (\\mathbf{\\Phi}\\mathbf{w} - \\mathbf{y}) + \\lambda \\mathbf{w}^T \\mathbf{w}\n$$\nThis is a quadratic function of $\\mathbf{w}$. To find the minimum, we compute the gradient of $J(\\mathbf{w})$ with respect to $\\mathbf{w}$ and set it to the zero vector.\n$$\n\\nabla_{\\mathbf{w}} J(\\mathbf{w}) = \\nabla_{\\mathbf{w}} \\left( \\mathbf{w}^T \\mathbf{\\Phi}^T \\mathbf{\\Phi} \\mathbf{w} - 2\\mathbf{y}^T \\mathbf{\\Phi} \\mathbf{w} + \\mathbf{y}^T \\mathbf{y} + \\lambda \\mathbf{w}^T \\mathbf{I} \\mathbf{w} \\right)\n$$\nwhere $\\mathbf{I}$ is the $(d+1) \\times (d+1)$ identity matrix. Applying standard matrix calculus rules ($\\nabla_{\\mathbf{z}} (\\mathbf{z}^T \\mathbf{A} \\mathbf{z}) = 2\\mathbf{A}\\mathbf{z}$ for symmetric $\\mathbf{A}$, and $\\nabla_{\\mathbf{z}} (\\mathbf{b}^T \\mathbf{z}) = \\mathbf{b}$), we obtain:\n$$\n\\nabla_{\\mathbf{w}} J(\\mathbf{w}) = 2\\mathbf{\\Phi}^T \\mathbf{\\Phi} \\mathbf{w} - 2\\mathbf{\\Phi}^T \\mathbf{y} + 2\\lambda \\mathbf{I} \\mathbf{w}\n$$\nSetting the gradient to zero yields the normal equations for ridge regression:\n$$\n2\\mathbf{\\Phi}^T \\mathbf{\\Phi} \\mathbf{w} - 2\\mathbf{\\Phi}^T \\mathbf{y} + 2\\lambda \\mathbf{I} \\mathbf{w} = \\mathbf{0}\n$$\n$$\n(\\mathbf{\\Phi}^T \\mathbf{\\Phi} + \\lambda \\mathbf{I}) \\mathbf{w} = \\mathbf{\\Phi}^T \\mathbf{y}\n$$\nThe optimal coefficient vector $\\mathbf{w}$ is the solution to this linear system of equations. The matrix $(\\mathbf{\\Phi}^T \\mathbf{\\Phi} + \\lambda \\mathbf{I})$ is guaranteed to be invertible for any $\\lambda > 0$, ensuring a unique solution. For $\\lambda = 0$, a unique solution exists if $\\mathbf{\\Phi}$ has full column rank, which is generally true if the number of unique data points $n$ is greater than or equal to the number of coefficients $d+1$. We can therefore solve for $\\mathbf{w}$:\n$$\n\\mathbf{w}^{\\star} = (\\mathbf{\\Phi}^T \\mathbf{\\Phi} + \\lambda \\mathbf{I})^{-1} \\mathbf{\\Phi}^T \\mathbf{y}\n$$\nNumerically, it is preferable to solve the linear system directly rather than computing the matrix inverse.\n\nOnce the optimal coefficients $\\mathbf{w}^{\\star}$ are determined, we can perform the required calculations:\n1.  **Prediction**: The predicted value at a new query point $x^\\star$ is calculated by evaluating the surrogate model:\n    $$\n    f_s(x^\\star) = \\sum_{k=0}^d w^{\\star}_k (x^\\star)^k = \\mathbf{\\phi}(x^\\star)^T \\mathbf{w}^{\\star}\n    $$\n    where $\\mathbf{\\phi}(x^\\star) = [1, x^\\star, (x^\\star)^2, \\dots, (x^\\star)^d]^T$ is the vector of basis functions evaluated at $x^\\star$.\n\n2.  **Training Root Mean Squared Error (RMSE)**: This metric quantifies the average magnitude of the error on the training data. First, we compute the model's predictions for all training points, $\\hat{\\mathbf{y}} = \\mathbf{\\Phi} \\mathbf{w}^{\\star}$. The RMSE is then given by:\n    $$\n    \\mathrm{RMSE} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n (\\hat{y}_i - y_i)^2} = \\sqrt{\\frac{1}{n} (\\mathbf{\\Phi}\\mathbf{w}^{\\star} - \\mathbf{y})^T (\\mathbf{\\Phi}\\mathbf{w}^{\\star} - \\mathbf{y})}\n    $$\n\nThe algorithm to be implemented will follow these steps for each test case: a) construct the design matrix $\\mathbf{\\Phi}$ and vector $\\mathbf{y}$ from the input data; b) solve the linear system $(\\mathbf{\\Phi}^T \\mathbf{\\Phi} + \\lambda \\mathbf{I}) \\mathbf{w} = \\mathbf{\\Phi}^T \\mathbf{y}$ to find $\\mathbf{w}$; c) compute the prediction $f_s(x^\\star)$ at the query point; d) compute the training RMSE; e) round and report the results.",
            "answer": "```python\nimport numpy as np\n\ndef fit_and_evaluate(x_train, y_train, d, lambd, x_query):\n    \"\"\"\n    Constructs and evaluates a polynomial surrogate model using Tikhonov regularization.\n\n    Args:\n        x_train (list or np.ndarray): The scalar input training data {x_i}.\n        y_train (list or np.ndarray): The scalar output training data {y_i}.\n        d (int): The degree of the polynomial basis.\n        lambd (float): The Tikhonov regularization parameter.\n        x_query (float): The query point at which to predict the output.\n\n    Returns:\n        list: A list containing [predicted_value, training_rmse], rounded to 6 decimal places.\n    \"\"\"\n    # Ensure inputs are numpy arrays for vectorized operations.\n    x_train = np.array(x_train)\n    y_train = np.array(y_train)\n    \n    n = len(x_train)\n    num_coeffs = d + 1\n\n    # Construct the design matrix Phi using a Vandermonde matrix.\n    # The basis is {x^0, x^1, ..., x^d}, which corresponds to increasing=True.\n    Phi = np.vander(x_train, N=num_coeffs, increasing=True)\n\n    # Solve the normal equations for ridge regression to find the coefficients w.\n    # (Phi^T * Phi + lambda * I) w = Phi^T * y\n    A = Phi.T @ Phi + lambd * np.identity(num_coeffs)\n    b = Phi.T @ y_train\n    w = np.linalg.solve(A, b)\n\n    # Calculate the prediction at the query point x_query.\n    # Construct the basis vector for the query point.\n    phi_query = np.array([x_query**k for k in range(num_coeffs)])\n    prediction = phi_query @ w\n\n    # Calculate the training Root Mean Squared Error (RMSE).\n    # First, get the model's predictions on the training data.\n    y_pred_train = Phi @ w\n    # Calculate the sum of squared errors (SSE) and then the RMSE.\n    sse = np.sum((y_pred_train - y_train)**2)\n    rmse = np.sqrt(sse / n)\n\n    # Round the final values to 6 decimal places as required.\n    prediction_rounded = round(prediction, 6)\n    rmse_rounded = round(rmse, 6)\n    \n    return [prediction_rounded, rmse_rounded]\n\ndef solve():\n    \"\"\"\n    Main function to process all test cases and print the final output.\n    \"\"\"\n    # Case 1: Exact quadratic fit, no regularization.\n    case1 = {\n        \"x\": [-2.0, -1.0, 0.0, 1.0, 2.0],\n        \"y\": [6.0, 3.5, 2.0, 1.5, 2.0],\n        \"d\": 2, \"lambda\": 0.0, \"x_star\": 0.5\n    }\n\n    # Case 2: Cubic fit with perturbations and regularization.\n    x2 = [-2.0, -1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0]\n    eps2 = [0.05, -0.04, 0.02, 0.01, 0.0, -0.03, 0.04, 0.02, -0.05]\n    y2 = [xi**3 - 2*xi + e for xi, e in zip(x2, eps2)]\n    case2 = {\n        \"x\": x2, \"y\": y2, \"d\": 3, \"lambda\": 0.001, \"x_star\": 1.2\n    }\n\n    # Case 3: Linear fit with duplicated inputs and regularization.\n    case3 = {\n        \"x\": [0.0, 0.0, 0.5, 0.5, 1.0, 1.0],\n        \"y\": [0.2, 0.21, 0.93, 0.97, 1.7, 1.69],\n        \"d\": 1, \"lambda\": 0.01, \"x_star\": 0.8\n    }\n\n    # Case 4: Higher-order polynomial fit, near-interpolation.\n    x4 = [-1.0, -0.6, -0.2, 0.2, 0.6, 1.0]\n    y4_func = lambda x: 1.0 - 0.5*x + 0.3*x**2 - 0.1*x**3 + 0.05*x**4 - 0.02*x**5\n    y4 = [y4_func(xi) for xi in x4]\n    case4 = {\n        \"x\": x4, \"y\": y4, \"d\": 5, \"lambda\": 1e-8, \"x_star\": -0.9\n    }\n\n    # Case 5: Single-point, constant model.\n    case5 = {\n        \"x\": [1.2], \"y\": [3.4], \"d\": 0, \"lambda\": 0.0, \"x_star\": 2.0\n    }\n\n    test_cases = [case1, case2, case3, case4, case5]\n\n    results_str = []\n    for case in test_cases:\n        result = fit_and_evaluate(case[\"x\"], case[\"y\"], case[\"d\"], case[\"lambda\"], case[\"x_star\"])\n        # Format each result pair into \"[val1,val2]\" string format\n        results_str.append(f\"[{result[0]},{result[1]}]\")\n\n    # Print the final result in the exact required format: [[...],[...],...]\n    print(f\"[{','.join(results_str)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Building a model is only the first step; choosing the right model is just as critical. This practice moves from fitting a single surrogate to the crucial task of model selection, addressing the fundamental bias-variance tradeoff . By comparing a simpler quadratic model against a more complex cubic one, you will use $K$-fold cross-validation to estimate their true predictive power, gaining hands-on skill in a powerful technique for preventing overfitting and ensuring your model generalizes well to new, unseen data.",
            "id": "3933528",
            "problem": "You are given a synthetic biomedical systems modeling task in which the scalar output represents blood glucose concentration in milligrams per deciliter (mg/dL) and depends on three controllable inputs: exogenous glucose load from a meal, insulin infusion rate, and physical activity intensity. The underlying mapping is unknown to the modeler and is to be approximated with surrogate response surfaces using polynomial basis functions that may contain interactions. The scientific foundation is the principle that surrogate modeling approximates a smooth input-output relationship by projecting onto a finite set of basis functions, and the best-fit coefficients are obtained by minimizing the sum of squared residuals, a well-tested approach in linear regression.\n\nDefine the inputs and output as follows. Let the input vector be $x = (x_1, x_2, x_3)$, where $x_1$ is exogenous glucose load in grams, $x_2$ is insulin infusion rate in milli-units per minute, and $x_3$ is dimensionless physical activity intensity. The output is $y$, blood glucose in mg/dL. The synthetic ground-truth map for data generation will be a polynomial in $x$ with coefficients chosen to be scientifically plausible given typical physiological effects under steady conditions, with ranges $x_1 \\in [0, 100]$, $x_2 \\in [0, 10]$, $x_3 \\in [0, 10]$. You will consider two surrogate models: a quadratic polynomial basis (including all pairwise interactions) and a cubic polynomial basis (including all third-degree monomials and interactions).\n\nYour program must perform the following tasks, grounded in core definitions:\n- Construct a surrogate model that assumes an unknown smooth function $f: \\mathbb{R}^3 \\to \\mathbb{R}$ and approximates it with a polynomial basis. For degree $p$, define the design matrix with an intercept and all monomials in $(x_1, x_2, x_3)$ up to total degree $p$, with degree $p=2$ for the quadratic surrogate and degree $p=3$ for the cubic surrogate, all including interaction terms.\n- Fit the surrogate coefficients by minimizing the sum of squared residuals on training data. Do not use shortcut formulas; rely on the fact that the minimizer of squared residuals over linear basis functions is obtained by solving the normal equations or their numerically stable equivalent.\n- Estimate out-of-sample generalization error using $K$-fold cross-validation with $K=5$, computing the Root Mean Square Error (RMSE), defined as $\\mathrm{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} \\left( \\hat{y}_i - y_i \\right)^2}$, in mg/dL on each held-out fold, and averaged across folds.\n\nData generation protocol for test cases must use the following scientifically grounded synthetic map, which is a static response surface capturing plausible steady-state effects:\n- Baseline: $90$ mg/dL.\n- Linear terms: meal increases glucose with coefficient $a_1 = 0.4$ mg/dL per gram; insulin reduces glucose with coefficient $a_2 = 8$ mg/dL per milli-unit per minute; activity reduces glucose with coefficient $a_3 = 2$ mg/dL per unit.\n- Quadratic terms: interaction of meal and insulin $b_{12} = -0.02$ mg/dL per $(\\text{gram} \\cdot \\text{mU/min})$, interaction of insulin and activity $b_{23} = -0.1$ mg/dL per $(\\text{mU/min} \\cdot \\text{activity unit})$, interaction of meal and activity $b_{13} = -0.02$ mg/dL per $(\\text{gram} \\cdot \\text{activity unit})$, and self-quadratics $c_1 = 0.005$ mg/dL per $\\text{gram}^2$, $c_2 = 0.15$ mg/dL per $(\\text{mU/min})^2$, $c_3 = -0.05$ mg/dL per $(\\text{activity unit})^2$.\n- Cubic terms: $d_1 = 0.00002$ mg/dL per $\\text{gram}^3$, $d_2 = 0.01$ mg/dL per $(\\text{mU/min})^3$, $d_3 = -0.002$ mg/dL per $(\\text{activity unit})^3$, and the triple interaction $d_{123} = -0.0005$ mg/dL per $(\\text{gram} \\cdot \\text{mU/min} \\cdot \\text{activity unit})$.\n\nThe ground-truth function used to generate data for a single sample $x$ is\n$$\nG(x) = 90 + a_1 x_1 - a_2 x_2 - a_3 x_3 + b_{12} x_1 x_2 + b_{23} x_2 x_3 + b_{13} x_1 x_3 + c_1 x_1^2 + c_2 x_2^2 + c_3 x_3^2 + \\gamma \\left( d_1 x_1^3 + d_2 x_2^3 + d_3 x_3^3 + d_{123} x_1 x_2 x_3 \\right),\n$$\nwhere $\\gamma \\in \\{0, 0.5, 1\\}$ controls the presence and strength of cubic effects. Observations are $y = G(x) + \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$ with $\\sigma$ in mg/dL.\n\nDecision criterion: For each test case, compute cross-validated RMSEs for quadratic surrogate ($p=2$) and cubic surrogate ($p=3$). Decide that including cubic terms improves predictive performance if the average cubic RMSE is at least a $5\\%$ relative reduction compared to the quadratic RMSE, that is, if $\\mathrm{RMSE}_{3} \\le 0.95 \\cdot \\mathrm{RMSE}_{2}$. This yields a boolean decision per test case.\n\nTest suite:\n- Case $1$: $N=120$ samples, $\\sigma=5$ mg/dL, $\\gamma=1$ (strong cubic), random seed $7$.\n- Case $2$: $N=120$ samples, $\\sigma=5$ mg/dL, $\\gamma=0$ (purely quadratic), random seed $11$.\n- Case $3$: $N=40$ samples, $\\sigma=15$ mg/dL, $\\gamma=1$ (strong cubic), random seed $13$.\n- Case $4$: $N=300$ samples, $\\sigma=2$ mg/dL, $\\gamma=0.5$ (mild cubic), random seed $17$.\n- Case $5$: $N=300$ samples, $\\sigma=0$ mg/dL, $\\gamma=1$ (strong cubic, noiseless), random seed $19$.\n\nYour program must:\n- Generate input samples $x$ uniformly over the specified ranges using the given seeds for reproducibility.\n- Compute outputs $y$ using the above $G(x)$ and additive noise for each case.\n- Fit quadratic and cubic surrogates using $5$-fold cross-validation and compute the average RMSE in mg/dL for each surrogate per case.\n- Decide improvement using the $5\\%$ relative criterion defined above.\n\nFinal output format:\nYour program should produce a single line of output containing the boolean decisions for the five test cases as a comma-separated list enclosed in square brackets, for example, $[\\text{True},\\text{False},\\text{True},\\text{True},\\text{False}]$. Reported RMSEs are internally computed in mg/dL, but the final output consists only of booleans indicating whether cubic terms improve performance according to the criterion.",
            "solution": "The problem statement is valid. It presents a well-posed, scientifically grounded, and objective task in the domain of biomedical systems modeling using surrogate response surfaces. All necessary data, models, and evaluation criteria are specified unambiguously, allowing for a unique and verifiable solution.\n\nThe core of this problem lies in model selection, specifically deciding whether a more complex surrogate model provides a statistically meaningful improvement in predictive accuracy over a simpler one. We are tasked with comparing a quadratic polynomial model to a cubic polynomial model in approximating an unknown function $f: \\mathbb{R}^3 \\to \\mathbb{R}$. The decision is based on the generalization error, which we estimate using $K$-fold cross-validation.\n\nFirst, we formalize the surrogate models. A surrogate model approximates the true function $y = f(x)$ using a linear combination of basis functions, $\\hat{y} = \\sum_{j=0}^{M-1} \\beta_j \\phi_j(x)$. This can be expressed in vector form for a single data point as $\\hat{y} = \\Phi(x) \\beta$, where $\\Phi(x) = [\\phi_0(x), \\phi_1(x), ..., \\phi_{M-1}(x)]$ is a row vector of basis functions evaluated at input $x$, and $\\beta$ is a column vector of coefficients $[\\beta_0, \\beta_1, ..., \\beta_{M-1}]^T$.\n\nFor a dataset of $N$ observations $(X, Y)$, where $X$ is the $N \\times 3$ matrix of inputs and $Y$ is the $N \\times 1$ vector of outputs, the model is written as $\\hat{Y} = \\mathbf{\\Phi}\\beta$. Here, $\\mathbf{\\Phi}$ is the $N \\times M$ design matrix, where the $i$-th row is $\\Phi(x^{(i)})$. The coefficients $\\beta$ are determined by minimizing the sum of squared residuals (SSR), which is the $L_2$ norm of the error vector:\n$$\n\\mathrm{SSR} = \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2 = \\|Y - \\mathbf{\\Phi}\\beta\\|_2^2\n$$\nThe solution to this minimization problem is found by solving the normal equations, $(\\mathbf{\\Phi}^T \\mathbf{\\Phi}) \\hat{\\beta} = \\mathbf{\\Phi}^T Y$. The optimal coefficient vector is thus $\\hat{\\beta} = (\\mathbf{\\Phi}^T \\mathbf{\\Phi})^{-1} \\mathbf{\\Phi}^T Y$. Numerically, it is more stable to solve this system using methods like QR decomposition, which is the standard approach in linear least squares solvers.\n\nThe two competing surrogate models are defined by their set of basis functions, determined by the polynomial degree $p$. The input vector is $x = (x_1, x_2, x_3)$.\n\n1.  **Quadratic Surrogate ($p=2$):** This model includes an intercept, all linear terms, and all second-degree monomials (pure squares and two-way interactions). The number of basis functions is $M_2 = \\binom{3+2}{2} = 10$. The basis functions are:\n    $$\n    \\{1, x_1, x_2, x_3, x_1^2, x_2^2, x_3^2, x_1 x_2, x_1 x_3, x_2 x_3\\}\n    $$\n\n2.  **Cubic Surrogate ($p=3$):** This model includes all monomials up to a total degree of $3$. The number of basis functions is $M_3 = \\binom{3+3}{3} = 20$. The basis is formed by the $10$ quadratic basis functions plus the $10$ unique third-degree monomials:\n    $$\n    \\{\\dots, x_1^3, x_2^3, x_3^3, x_1^2 x_2, x_1 x_2^2, x_1^2 x_3, x_1 x_3^2, x_2^2 x_3, x_2 x_3^2, x_1 x_2 x_3\\}\n    $$\n\nTo evaluate and compare these models, we employ $K$-fold cross-validation with $K=5$. The dataset is partitioned into $5$ equally-sized, disjoint folds. For each fold $k \\in \\{1, ..., 5\\}$, we train the model on the other $4$ folds and test its performance on the held-out fold $k$. The performance is quantified by the Root Mean Square Error (RMSE):\n$$\n\\mathrm{RMSE}_{\\text{fold } k} = \\sqrt{\\frac{1}{n_k} \\sum_{i \\in \\text{fold } k} (\\hat{y}_i - y_i)^2}\n$$\nwhere $n_k$ is the number of samples in fold $k$. The final performance metric for each model (quadratic and cubic) is the average RMSE across all $5$ folds, denoted $\\mathrm{RMSE}_2$ and $\\mathrm{RMSE}_3$, respectively.\n\nThe decision criterion for model improvement is that the cubic model must provide at least a $5\\%$ relative reduction in average RMSE compared to the quadratic model:\n$$\n\\mathrm{RMSE}_3 \\le 0.95 \\cdot \\mathrm{RMSE}_2\n$$\nThis criterion balances the desire for a better fit with a penalty for increased complexity, as a more complex model must demonstrate a substantial, not merely marginal, improvement.\n\nThe data generation process for each test case is as follows:\n1.  Set the random number generator seed for reproducibility.\n2.  Generate $N$ input samples $x^{(i)}$ by drawing $x_1, x_2, x_3$ from uniform distributions over their respective ranges: $x_1 \\sim U(0, 100)$, $x_2 \\sim U(0, 10)$, and $x_3 \\sim U(0, 10)$.\n3.  Compute the true, noise-free output $G(x^{(i)})$ using the provided ground-truth function with the specified coefficients and the case-specific value of $\\gamma$.\n    $$\n    G(x) = 90 + 0.4 x_1 - 8 x_2 - 2 x_3 - 0.02 x_1 x_2 - 0.1 x_2 x_3 - 0.02 x_1 x_3 + 0.005 x_1^2 + 0.15 x_2^2 - 0.05 x_3^2 + \\gamma \\left( 0.00002 x_1^3 + 0.01 x_2^3 - 0.002 x_3^3 - 0.0005 x_1 x_2 x_3 \\right)\n    $$\n4.  Generate noisy observations $y_i = G(x^{(i)}) + \\epsilon_i$, where $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$ with the case-specific noise standard deviation $\\sigma$.\n\nFor each test case, we will execute this procedure, calculate $\\mathrm{RMSE}_2$ and $\\mathrm{RMSE}_3$, and apply the decision criterion to yield a boolean result. This entire process is encapsulated in the provided Python program.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the surrogate modeling problem for five test cases.\n    \"\"\"\n    \n    test_cases = [\n        {'N': 120, 'sigma': 5, 'gamma': 1, 'seed': 7},\n        {'N': 120, 'sigma': 5, 'gamma': 0, 'seed': 11},\n        {'N': 40, 'sigma': 15, 'gamma': 1, 'seed': 13},\n        {'N': 300, 'sigma': 2, 'gamma': 0.5, 'seed': 17},\n        {'N': 300, 'sigma': 0, 'gamma': 1, 'seed': 19},\n    ]\n\n    def ground_truth_model(x_vec):\n        \"\"\"\n        Computes the output of the synthetic ground-truth function G(x).\n        \n        Args:\n            x_vec: A numpy array of shape (N, 3) with columns [x1, x2, x3].\n            \n        Returns:\n            A numpy array of shape (N,) with the G(x) values.\n        \"\"\"\n        x1, x2, x3 = x_vec[:, 0], x_vec[:, 1], x_vec[:, 2]\n        \n        # Coefficients\n        a1 = 0.4\n        a2 = 8\n        a3 = 2\n        \n        b12 = -0.02\n        b23 = -0.1\n        b13 = -0.02\n        \n        c1 = 0.005\n        c2 = 0.15\n        c3 = -0.05\n        \n        d1 = 0.00002\n        d2 = 0.01\n        d3 = -0.002\n        d123 = -0.0005\n        \n        gamma = case['gamma']\n        \n        # Calculate G(x)\n        val = (90\n               + a1 * x1\n               - a2 * x2\n               - a3 * x3\n               + b12 * x1 * x2\n               + b23 * x2 * x3\n               + b13 * x1 * x3\n               + c1 * x1**2\n               + c2 * x2**2\n               + c3 * x3**2\n               + gamma * (d1 * x1**3 +\n                          d2 * x2**3 +\n                          d3 * x3**3 +\n                          d123 * x1 * x2 * x3)\n              )\n        return val\n\n    def generate_data(case):\n        \"\"\"\n        Generates synthetic data for a given test case.\n        \"\"\"\n        N, sigma, seed = case['N'], case['sigma'], case['seed']\n        rng = np.random.default_rng(seed)\n        \n        x1 = rng.uniform(0, 100, N)\n        x2 = rng.uniform(0, 10, N)\n        x3 = rng.uniform(0, 10, N)\n        X = np.c_[x1, x2, x3]\n        \n        y_true = ground_truth_model(X)\n        noise = rng.normal(0, sigma, N)\n        y = y_true + noise\n        \n        return X, y\n\n    def create_design_matrix(X, degree):\n        \"\"\"\n        Constructs the design matrix for polynomial regression.\n        \"\"\"\n        N = X.shape[0]\n        x1, x2, x3 = X[:, 0], X[:, 1], X[:, 2]\n        \n        # Intercept term\n        phi = [np.ones(N)]\n        \n        # Linear terms (degree 1)\n        if degree >= 1:\n            phi.extend([x1, x2, x3])\n        \n        # Quadratic terms (degree 2)\n        if degree >= 2:\n            phi.extend([x1**2, x2**2, x3**2, x1*x2, x1*x3, x2*x3])\n            \n        # Cubic terms (degree 3)\n        if degree >= 3:\n            phi.extend([x1**3, x2**3, x3**3, x1**2*x2, x1*x2**2, \n                        x1**2*x3, x1*x3**2, x2**2*x3, x2*x3**2, x1*x2*x3])\n            \n        return np.stack(phi, axis=1)\n\n    def calculate_cv_rmse(X, y, degree, k_folds, seed):\n        \"\"\"\n        Calculates the average RMSE using K-fold cross-validation.\n        \"\"\"\n        N = X.shape[0]\n        indices = np.arange(N)\n        rng = np.random.default_rng(seed) # Use same seed for consistent folds\n        rng.shuffle(indices)\n        \n        fold_indices = np.array_split(indices, k_folds)\n        \n        fold_rmses = []\n        \n        for k in range(k_folds):\n            test_idx = fold_indices[k]\n            train_idx = np.concatenate([fold_indices[i] for i in range(k_folds) if i != k])\n            \n            X_train, y_train = X[train_idx], y[train_idx]\n            X_test, y_test = X[test_idx], y[test_idx]\n            \n            phi_train = create_design_matrix(X_train, degree)\n            phi_test = create_design_matrix(X_test, degree)\n            \n            try:\n                coeffs, _, _, _ = np.linalg.lstsq(phi_train, y_train, rcond=None)\n                y_pred = phi_test @ coeffs\n                rmse = np.sqrt(np.mean((y_test - y_pred)**2))\n                fold_rmses.append(rmse)\n            except np.linalg.LinAlgError:\n                # Handle cases where the matrix is singular, likely with very small N\n                fold_rmses.append(np.inf)\n\n        return np.mean(fold_rmses)\n\n    results = []\n    k_folds = 5\n    \n    for case in test_cases:\n        X, y = generate_data(case)\n        \n        # Calculate RMSE for quadratic model (p=2)\n        rmse_2 = calculate_cv_rmse(X, y, degree=2, k_folds=k_folds, seed=case['seed'])\n        \n        # Calculate RMSE for cubic model (p=3)\n        rmse_3 = calculate_cv_rmse(X, y, degree=3, k_folds=k_folds, seed=case['seed'])\n        \n        # Decision: is cubic model at least 5% better?\n        improvement = rmse_3 = 0.95 * rmse_2\n        results.append(improvement)\n\n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "We now advance from parametric polynomials to a more powerful, non-parametric framework: Gaussian Process (GP) regression. This exercise guides you through the implementation of a GP surrogate, a flexible method capable of modeling complex, nonlinear relationships without assuming a rigid functional form . The key takeaway is the GP's ability to provide principled uncertainty estimates, allowing you to not only predict a response but also to quantify the model's confidence in that predictionâ€”a critical feature for reliable decision-making in biomedical applications.",
            "id": "3933600",
            "problem": "You are asked to implement a universal, purely mathematical program that computes the predictive mean and the epistemic variance of a Gaussian Process (GP) surrogate model for a one-dimensional biomedical response surface. Begin from first principles: a Gaussian Process (GP) is defined as a collection of random variables, any finite number of which have a joint Gaussian distribution. The fundamental base for this problem is the combination of the definitions of a Gaussian Process prior and a Gaussian likelihood model, together with the conditioning rules of a multivariate Gaussian distribution. Specifically, assume a latent function $f$ with a zero-mean Gaussian Process prior $f \\sim \\mathcal{GP}(0, k)$, and noisy observations $y_i$ produced by the model $y_i = f(x_i) + \\epsilon_i$, where $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma_n^2)$ are independent and identically distributed. The covariance is given by a stationary, positive-definite kernel $k(\\cdot, \\cdot)$.\n\nUse the squared-exponential (also called radial basis function) kernel with hyperparameters signal variance $\\sigma_f^2$ and length scale $l$ defined by\n$$\nk(x, x') = \\sigma_f^2 \\exp\\left(-\\frac{(x - x')^2}{2 l^2}\\right).\n$$\nAll quantities are dimensionless. Angles are not used. No percentages are used.\n\nYour program must, for each test case, compute the predictive mean and epistemic variance (that is, the posterior variance of the latent function $f$ at the test input) at a single new point $x^\\star$, based on training inputs $X$ and outputs $y$. The epistemic variance is the variance of $f(x^\\star)$ under the posterior and does not include the observation noise variance $\\sigma_n^2$. Derive the algorithm starting from the joint Gaussian distribution implied by the GP prior and the Gaussian likelihood, and by applying the conditioning rules of multivariate Gaussian distributions. For numerical stability, your implementation must add a small positive diagonal jitter $\\delta$ to the training covariance matrix before factorization, with $\\delta$ chosen as $\\delta = 10^{-10}$.\n\nTo instantiate the biomedical context in a scientifically realistic and self-consistent way, the training outputs $y$ are generated deterministically from a normalized dose-response curve representative of receptor-ligand binding. Use the logistic-sigmoid form\n$$\nf(x) = \\frac{1}{1 + \\exp\\left(-a (x - b)\\right)},\n$$\nwith steepness parameter $a = 8$ and midpoint $b = 0.5$. For any given training set $X$, define $y_i = f(x_i)$ unless otherwise specified.\n\nImplement the following four test cases that exercise the algorithm across a range of conditions. In all cases, the quantities are dimensionless:\n\n- Test case $1$ (happy path, moderate noise, interpolation):\n  - Training inputs $X = [0.0, 0.25, 0.5, 0.75, 1.0]$.\n  - Training outputs $y_i = f(x_i)$.\n  - Hyperparameters: length scale $l = 0.2$, signal variance $\\sigma_f = 1.0$, observation noise standard deviation $\\sigma_n = 0.1$.\n  - Query input $x^\\star = 0.6$.\n\n- Test case $2$ (boundary condition, zero noise, exact training input):\n  - Training inputs $X = [0.0, 0.25, 0.5, 0.75, 1.0]$.\n  - Training outputs $y_i = f(x_i)$.\n  - Hyperparameters: length scale $l = 0.3$, signal variance $\\sigma_f = 1.0$, observation noise standard deviation $\\sigma_n = 0.0$.\n  - Query input $x^\\star = 0.5$.\n\n- Test case $3$ (edge case, extrapolation far outside training domain):\n  - Training inputs $X = [0.0, 0.25, 0.5, 0.75, 1.0]$.\n  - Training outputs $y_i = f(x_i)$.\n  - Hyperparameters: length scale $l = 0.2$, signal variance $\\sigma_f = 1.0$, observation noise standard deviation $\\sigma_n = 0.1$.\n  - Query input $x^\\star = 2.0$.\n\n- Test case $4$ (edge case, duplicate training inputs with small noise):\n  - Training inputs $X = [0.0, 0.25, 0.5, 0.5, 0.75, 1.0]$.\n  - Training outputs aligned elementwise with $X$ as follows:\n    - For $x = 0.0$, $y = f(0.0)$.\n    - For $x = 0.25$, $y = f(0.25)$.\n    - For the third entry $x = 0.5$, use $y = f(0.5) - 0.02$.\n    - For the fourth entry $x = 0.5$, use $y = f(0.5) + 0.02$.\n    - For $x = 0.75$, $y = f(0.75)$.\n    - For $x = 1.0$, $y = f(1.0)$.\n  - Hyperparameters: length scale $l = 0.15$, signal variance $\\sigma_f = 1.0$, observation noise standard deviation $\\sigma_n = 0.05$.\n  - Query input $x^\\star = 0.5$.\n\nYour program must, for each test case, output the predictive mean $\\mu(x^\\star)$ and the epistemic variance $\\sigma^2(x^\\star)$ as floating-point numbers. Aggregate the results for the four test cases into a single line of output containing eight numbers in the order $\\left[\\mu_1, \\sigma_1^2, \\mu_2, \\sigma_2^2, \\mu_3, \\sigma_3^2, \\mu_4, \\sigma_4^2\\right]$. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[\\text{result1},\\text{result2},\\text{result3}]$). All quantities are dimensionless. Express all numbers as decimal floating-point values.",
            "solution": "The problem requires the implementation of Gaussian Process (GP) regression to compute the predictive posterior mean and epistemic variance for a latent function $f$ at a new input point $x^\\star$. The derivation proceeds from the fundamental definition of a GP and the rules for conditioning multivariate Gaussian distributions.\n\n**1. Model Specification**\n\nA Gaussian Process defines a prior distribution over functions. We assume the latent function $f(x)$ follows a GP with a zero mean and a covariance function (kernel) $k(x, x')$. This is denoted as:\n$$\nf(x) \\sim \\mathcal{GP}(0, k(x, x'))\n$$\nThe problem specifies the squared-exponential kernel:\n$$\nk(x, x') = \\sigma_f^2 \\exp\\left(-\\frac{(x - x')^2}{2 l^2}\\right)\n$$\nwhere $\\sigma_f^2$ is the signal variance and $l$ is the length scale.\n\nWe are given a set of $N$ training inputs $X = \\{x_1, \\dots, x_N\\}$ with corresponding noisy observations $\\mathbf{y} = [y_1, \\dots, y_N]^T$. The observation model is:\n$$\ny_i = f(x_i) + \\epsilon_i, \\quad \\text{where} \\quad \\epsilon_i \\sim \\mathcal{N}(0, \\sigma_n^2)\n$$\nThe noise terms $\\epsilon_i$ are assumed to be independent and identically distributed.\n\n**2. Joint Prior Distribution**\n\nBy the definition of a GP, any finite set of function values has a joint Gaussian distribution. Let $\\mathbf{f}$ be the vector of latent function values at the training inputs, $\\mathbf{f} = [f(x_1), \\dots, f(x_N)]^T$, and $f^\\star$ be the latent function value at a new test input $x^\\star$. Their joint distribution under the prior is:\n$$\n\\begin{pmatrix} \\mathbf{f} \\\\ f^\\star \\end{pmatrix} \\sim \\mathcal{N} \\left( \\mathbf{0}, \\begin{pmatrix} K(X, X)  K(X, x^\\star) \\\\ K(x^\\star, X)  k(x^\\star, x^\\star) \\end{pmatrix} \\right)\n$$\nwhere:\n- $K(X, X)$ is the $N \\times N$ covariance matrix of training inputs, with entries $[K(X, X)]_{ij} = k(x_i, x_j)$.\n- $K(X, x^\\star)$ is the $N \\times 1$ vector of covariances between training inputs and the test input, with entries $[K(X, x^\\star)]_i = k(x_i, x^\\star)$.\n- $K(x^\\star, X) = K(X, x^\\star)^T$.\n- $k(x^\\star, x^\\star)$ is the prior variance of the function at the test input, which is $\\sigma_f^2$ for the squared-exponential kernel.\n\n**3. Joint Distribution of Observations and Test Value**\n\nThe vector of observations $\\mathbf{y}$ is related to the latent function values $\\mathbf{f}$ by $\\mathbf{y} = \\mathbf{f} + \\boldsymbol{\\epsilon}$, where $\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma_n^2 I)$. We need the joint distribution of the observed data $\\mathbf{y}$ and the latent value $f^\\star$. Since they are formed by linear operations on Gaussian variables, they are also jointly Gaussian.\n\nThe mean vector is zero:\n$$\nE\\left[\\begin{pmatrix} \\mathbf{y} \\\\ f^\\star \\end{pmatrix}\\right] = \\begin{pmatrix} E[\\mathbf{f} + \\boldsymbol{\\epsilon}] \\\\ E[f^\\star] \\end{pmatrix} = \\begin{pmatrix} \\mathbf{0} \\\\ 0 \\end{pmatrix}\n$$\nThe covariance matrix is:\n$$\n\\text{Cov}\\left(\\begin{pmatrix} \\mathbf{y} \\\\ f^\\star \\end{pmatrix}\\right) = E\\left[ \\begin{pmatrix} \\mathbf{y} \\\\ f^\\star \\end{pmatrix} \\begin{pmatrix} \\mathbf{y}^T  (f^\\star)^T \\end{pmatrix} \\right] = \\begin{pmatrix} E[\\mathbf{y}\\mathbf{y}^T]  E[\\mathbf{y}(f^\\star)^T] \\\\ E[f^\\star \\mathbf{y}^T]  E[f^\\star(f^\\star)^T] \\end{pmatrix}\n$$\nThe blocks of this covariance matrix are:\n- $\\text{Cov}(\\mathbf{y}, \\mathbf{y}) = E[(\\mathbf{f}+\\boldsymbol{\\epsilon})(\\mathbf{f}+\\boldsymbol{\\epsilon})^T] = E[\\mathbf{f}\\mathbf{f}^T] + E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] = K(X, X) + \\sigma_n^2 I$.\n- $\\text{Cov}(\\mathbf{y}, f^\\star) = E[(\\mathbf{f}+\\boldsymbol{\\epsilon})(f^\\star)^T] = E[\\mathbf{f}(f^\\star)^T] + E[\\boldsymbol{\\epsilon}(f^\\star)^T] = K(X, x^\\star)$.\n- $\\text{Cov}(f^\\star, f^\\star) = k(x^\\star, x^\\star)$.\n\nThus, the joint distribution is:\n$$\n\\begin{pmatrix} \\mathbf{y} \\\\ f^\\star \\end{pmatrix} \\sim \\mathcal{N} \\left( \\mathbf{0}, \\begin{pmatrix} K(X, X) + \\sigma_n^2 I  K(X, x^\\star) \\\\ K(x^\\star, X)  k(x^\\star, x^\\star) \\end{pmatrix} \\right)\n$$\n\n**4. Posterior Distribution (Conditioning)**\n\nOur goal is to find the posterior distribution of the latent function value, $p(f^\\star | X, \\mathbf{y}, x^\\star)$, conditioned on the observed data. For a general multivariate Gaussian partition $\\begin{pmatrix} \\mathbf{a} \\\\ \\mathbf{b} \\end{pmatrix} \\sim \\mathcal{N} \\left( \\begin{pmatrix} \\boldsymbol{\\mu}_a \\\\ \\boldsymbol{\\mu}_b \\end{pmatrix}, \\begin{pmatrix} \\Sigma_{aa}  \\Sigma_{ab} \\\\ \\Sigma_{ba}  \\Sigma_{bb} \\end{pmatrix} \\right)$, the conditional distribution $p(\\mathbf{b}|\\mathbf{a})$ is $\\mathcal{N}(\\boldsymbol{\\mu}_{b|a}, \\Sigma_{b|a})$, with:\n$$\n\\boldsymbol{\\mu}_{b|a} = \\boldsymbol{\\mu}_b + \\Sigma_{ba} \\Sigma_{aa}^{-1} (\\mathbf{a} - \\boldsymbol{\\mu}_a)\n$$\n$$\n\\Sigma_{b|a} = \\Sigma_{bb} - \\Sigma_{ba} \\Sigma_{aa}^{-1} \\Sigma_{ab}\n$$\nBy substituting our terms ($\\mathbf{a} \\to \\mathbf{y}$, $\\mathbf{b} \\to f^\\star$, zero means), we obtain the posterior distribution for $f^\\star$ as a Gaussian with mean $\\mu(x^\\star)$ and variance $\\sigma^2(x^\\star)$:\n\n- **Predictive Mean:**\n$$\n\\mu(x^\\star) = K(x^\\star, X) [K(X, X) + \\sigma_n^2 I]^{-1} \\mathbf{y}\n$$\n\n- **Epistemic Variance:**\n$$\n\\sigma^2(x^\\star) = k(x^\\star, x^\\star) - K(x^\\star, X) [K(X, X) + \\sigma_n^2 I]^{-1} K(X, x^\\star)\n$$\nThe problem asks for the epistemic variance, which is the variance of the latent function $f^\\star$. The predictive variance for a new observation $y^\\star$ would include the noise term, i.e., $\\sigma^2_{pred}(x^\\star) = \\sigma^2(x^\\star) + \\sigma_n^2$. We only compute $\\sigma^2(x^\\star)$.\n\n**5. Implementation Algorithm**\n\nFor numerical stability, a small jitter $\\delta = 10^{-10}$ is added to the diagonal of the covariance matrix of observations. Let $K_y = K(X, X) + \\sigma_n^2 I + \\delta I$.\n\nThe computational procedure is as follows:\n1. Construct the kernel matrices: $K(X, X)$, $K(X, x^\\star)$, and the scalar $k(x^\\star, x^\\star)$ using the squared-exponential kernel formula.\n2. Form the matrix to be inverted: $K_y = K(X, X) + (\\sigma_n^2 + \\delta)I_{N \\times N}$.\n3. To avoid direct matrix inversion, solve the linear system $K_y \\boldsymbol{\\alpha} = \\mathbf{y}$ for the vector $\\boldsymbol{\\alpha}$.\n4. Compute the predictive mean: $\\mu(x^\\star) = K(x^\\star, X) \\boldsymbol{\\alpha} = \\sum_{i=1}^N \\alpha_i k(x_i, x^\\star)$.\n5. To compute the variance, solve a second linear system $K_y \\mathbf{v} = K(X, x^\\star)$ for the vector $\\mathbf{v}$.\n6. Compute the epistemic variance: $\\sigma^2(x^\\star) = k(x^\\star, x^\\star) - K(x^\\star, X) \\mathbf{v}$.\n\nThis procedure is applied to each of the four test cases. The training data $y_i$ are generated using the logistic-sigmoid function $f(x) = (1 + \\exp(-a (x - b)))^{-1}$ with parameters $a=8$ and $b=0.5$, except where specified otherwise.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the predictive mean and epistemic variance for a Gaussian Process\n    surrogate model for four specified test cases.\n    \"\"\"\n\n    def dose_response(x, a=8.0, b=0.5):\n        \"\"\"\n        Calculates the value of the normalized dose-response curve.\n        \"\"\"\n        return 1.0 / (1.0 + np.exp(-a * (np.asarray(x) - b)))\n\n    def se_kernel(X1, X2, l, sigma_f):\n        \"\"\"\n        Computes the squared-exponential kernel matrix between two sets of 1D points.\n        \"\"\"\n        l_sq = l**2\n        sigma_f_sq = sigma_f**2\n        # Ensure inputs are column vectors for broadcasting\n        X1 = np.asarray(X1).reshape(-1, 1)\n        X2 = np.asarray(X2).reshape(-1, 1)\n        sqdist = (X1.reshape(-1, 1) - X2.reshape(1, -1))**2\n        return sigma_f_sq * np.exp(-0.5 * sqdist / l_sq)\n\n    def gp_predict(X_train, y_train, x_star, l, sigma_f, sigma_n):\n        \"\"\"\n        Calculates the GP predictive mean and epistemic variance at a point x_star.\n        \"\"\"\n        delta = 1e-10  # Diagonal jitter for numerical stability\n        X_train = np.asarray(X_train).reshape(-1, 1)\n        y_train = np.asarray(y_train).reshape(-1, 1)\n        x_star_arr = np.array([x_star]).reshape(-1, 1)\n        n_train = len(X_train)\n\n        # Compute kernel matrices\n        K_XX = se_kernel(X_train, X_train, l, sigma_f)\n        K_sX = se_kernel(x_star_arr, X_train, l, sigma_f) # K(x*, X)\n        K_ss = se_kernel(x_star_arr, x_star_arr, l, sigma_f) # k(x*, x*)\n\n        # Form the matrix Ky = K(X, X) + sigma_n^2*I + delta*I\n        Ky = K_XX + (sigma_n**2) * np.eye(n_train) + delta * np.eye(n_train)\n\n        # Solve for alpha = Ky^-1 * y_train\n        alpha = np.linalg.solve(Ky, y_train)\n\n        # Predictive mean: mu(x*) = K(x*, X) * alpha\n        pred_mean = K_sX @ alpha\n\n        # For variance, solve for v = Ky^-1 * K(X, x*)\n        # K(X, x*) is the transpose of K(x*, X)\n        K_Xs = K_sX.T\n        v = np.linalg.solve(Ky, K_Xs)\n\n        # Epistemic variance: var(f*) = k(x*,x*) - K(x*,X) * v\n        pred_var = K_ss - K_sX @ v\n\n        return pred_mean.item(), pred_var.item()\n\n    test_cases = [\n        # Test case 1 (happy path, moderate noise, interpolation)\n        {\n            \"X\": [0.0, 0.25, 0.5, 0.75, 1.0],\n            \"y\": dose_response([0.0, 0.25, 0.5, 0.75, 1.0]),\n            \"l\": 0.2, \"sigma_f\": 1.0, \"sigma_n\": 0.1,\n            \"x_star\": 0.6\n        },\n        # Test case 2 (boundary condition, zero noise, exact training input)\n        {\n            \"X\": [0.0, 0.25, 0.5, 0.75, 1.0],\n            \"y\": dose_response([0.0, 0.25, 0.5, 0.75, 1.0]),\n            \"l\": 0.3, \"sigma_f\": 1.0, \"sigma_n\": 0.0,\n            \"x_star\": 0.5\n        },\n        # Test case 3 (edge case, extrapolation far outside training domain)\n        {\n            \"X\": [0.0, 0.25, 0.5, 0.75, 1.0],\n            \"y\": dose_response([0.0, 0.25, 0.5, 0.75, 1.0]),\n            \"l\": 0.2, \"sigma_f\": 1.0, \"sigma_n\": 0.1,\n            \"x_star\": 2.0\n        },\n        # Test case 4 (edge case, duplicate training inputs with small noise)\n        {\n            \"X\": [0.0, 0.25, 0.5, 0.5, 0.75, 1.0],\n            \"y\": [\n                dose_response(0.0),\n                dose_response(0.25),\n                dose_response(0.5) - 0.02,\n                dose_response(0.5) + 0.02,\n                dose_response(0.75),\n                dose_response(1.0)\n            ],\n            \"l\": 0.15, \"sigma_f\": 1.0, \"sigma_n\": 0.05,\n            \"x_star\": 0.5\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        mean, variance = gp_predict(\n            case[\"X\"], case[\"y\"], case[\"x_star\"],\n            case[\"l\"], case[\"sigma_f\"], case[\"sigma_n\"]\n        )\n        results.extend([mean, variance])\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}