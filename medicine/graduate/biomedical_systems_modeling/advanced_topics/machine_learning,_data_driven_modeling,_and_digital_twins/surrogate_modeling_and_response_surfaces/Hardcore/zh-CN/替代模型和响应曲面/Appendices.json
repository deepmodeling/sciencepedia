{
    "hands_on_practices": [
        {
            "introduction": "掌握代理建模的第一步是学习如何构建和拟合模型。本练习将指导您从头开始，使用多项式基函数为给定的数据集构建一个代理模型，并应用 Tikhonov (Ridge) 正则化来防止过拟合，这是处理复杂或含噪声数据时的关键技术。通过这个实践，您将深入理解从数据到预测模型的完整流程 。",
            "id": "3933515",
            "problem": "给定标量训练数据对 $\\{(x_i,y_i)\\}_{i=1}^n$，这些数据源自一个确定性的、受噪声扰动的计算心脏模型。该模型将一个标量输入参数 $x$（无量纲）映射到一个标量模型输出 $y$（无量纲）。您的任务是构建一个代理模型，并在指定的查询输入上评估其预测性能。从以下基本原理出发：代理模型是一组固定基函数的线性组合，在带有 Tikhonov（岭）正则化的二次损失下，通过最小化正则化的残差平方和来获得最佳拟合。您必须使用一个次数为 $d$ 的单变量多项式基，即基函数为 $\\{\\phi_k(x)\\}_{k=0}^d$，其中 $\\phi_k(x)=x^k$。代理模型为 $f_s(x)=\\sum_{k=0}^d w_k \\phi_k(x)$，其系数 $\\{w_k\\}_{k=0}^d$ 使正则化经验风险最小化\n$$\nJ(\\mathbf{w})=\\sum_{i=1}^n\\left(f_s(x_i)-y_i\\right)^2+\\lambda\\lVert \\mathbf{w}\\rVert_2^2,\n$$\n其中 $\\lambda \\ge 0$ 是 Tikhonov 正则化参数。拟合后，计算在指定查询点 $x^\\star$ 处的预测值 $f_s(x^\\star)$ 以及训练均方根误差 (RMSE)\n$$\n\\mathrm{RMSE}=\\sqrt{\\frac{1}{n}\\sum_{i=1}^n\\left(f_s(x_i)-y_i\\right)^2}.\n$$\n所有输入 $x$、输出 $y$、系数 $\\mathbf{w}$ 和预测值都是无量纲的。不涉及角度。您必须将报告的每个浮点数值四舍五入到 $6$ 位小数。\n\n实现一个程序，对于每个提供的测试用例，拟合次数为 $d$、正则化参数为 $\\lambda$ 的多项式代理模型，计算 $f_s(x^\\star)$ 和训练 $\\mathrm{RMSE}$，并返回这两个值。\n\n测试套件：\n对于下面的每个案例，使用给定的元组 $(\\{x_i\\},\\{y_i\\},d,\\lambda,x^\\star)$。\n\n- 案例 1（精确二次拟合）：\n  - $\\{x_i\\}$:\n    $$\n    [-2.0,-1.0,0.0,1.0,2.0]\n    $$\n  - $\\{y_i\\}$，由 $y=0.5x^2-x+2$ 生成：\n    $$\n    \\left[0.5\\cdot(-2.0)^2-(-2.0)+2,\\;0.5\\cdot(-1.0)^2-(-1.0)+2,\\;0.5\\cdot 0.0^2-0.0+2,\\;0.5\\cdot 1.0^2-1.0+2,\\;0.5\\cdot 2.0^2-2.0+2\\right]\n    $$\n    即，\n    $$\n    [6.0,3.5,2.0,1.5,2.0]\n    $$\n  - 次数 $d$：\n    $$\n    2\n    $$\n  - 正则化 $\\lambda$：\n    $$\n    0.0\n    $$\n  - 查询点 $x^\\star$：\n    $$\n    0.5\n    $$\n\n- 案例 2（带指定扰动的三次拟合）：\n  - $\\{x_i\\}$:\n    $$\n    [-2.0,-1.5,-1.0,-0.5,0.0,0.5,1.0,1.5,2.0]\n    $$\n  - $\\{y_i\\}$，指定为 $y=x^3-2x+\\epsilon$，其中 $\\epsilon$ 为固定值：\n    $$\n    \\epsilon=\\left[0.05,-0.04,0.02,0.01,0.0,-0.03,0.04,0.02,-0.05\\right]\n    $$\n    因此\n    $$\n    \\{y_i\\}=\\left[(-2.0)^3-2(-2.0)+0.05,\\;(-1.5)^3-2(-1.5)-0.04,\\;(-1.0)^3-2(-1.0)+0.02,\\;(-0.5)^3-2(-0.5)+0.01,\\;0.0^3-2(0.0)+0.0,\\;(0.5)^3-2(0.5)-0.03,\\;1.0^3-2(1.0)+0.04,\\;1.5^3-2(1.5)+0.02,\\;2.0^3-2(2.0)-0.05\\right]\n    $$\n  - 次数 $d$：\n    $$\n    3\n    $$\n  - 正则化 $\\lambda$：\n    $$\n    0.001\n    $$\n  - 查询点 $x^\\star$：\n    $$\n    1.2\n    $$\n\n- 案例 3（具有线性趋势的重复输入）：\n  - $\\{x_i\\}$:\n    $$\n    [0.0,0.0,0.5,0.5,1.0,1.0]\n    $$\n  - $\\{y_i\\}$，指定为 $y=0.2+1.5x+\\eta$，其中 $\\eta$ 为固定值：\n    $$\n    \\eta=[0.0,0.01,-0.02,0.02,0.0,-0.01]\n    $$\n    因此\n    $$\n    \\{y_i\\}=[0.2,0.21,0.93,0.97,1.7,1.69]\n    $$\n  - 次数 $d$：\n    $$\n    1\n    $$\n  - 正则化 $\\lambda$：\n    $$\n    0.01\n    $$\n  - 查询点 $x^\\star$：\n    $$\n    0.8\n    $$\n\n- 案例 4（高阶多项式，近似插值）：\n  - $\\{x_i\\}$:\n    $$\n    [-1.0,-0.6,-0.2,0.2,0.6,1.0]\n    $$\n  - $\\{y_i\\}$，由下式精确指定\n    $$\n    y=1.0-0.5x+0.3x^2-0.1x^3+0.05x^4-0.02x^5\n    $$\n    在给定的 $\\{x_i\\}$ 处求值。\n  - 次数 $d$：\n    $$\n    5\n    $$\n  - 正则化 $\\lambda$：\n    $$\n    10^{-8}\n    $$\n  - 查询点 $x^\\star$：\n    $$\n    -0.9\n    $$\n\n- 案例 5（单点，常数模型）：\n  - $\\{x_i\\}$:\n    $$\n    [1.2]\n    $$\n  - $\\{y_i\\}$:\n    $$\n    [3.4]\n    $$\n  - 次数 $d$：\n    $$\n    0\n    $$\n  - 正则化 $\\lambda$：\n    $$\n    0.0\n    $$\n  - 查询点 $x^\\star$：\n    $$\n    2.0\n    $$\n\n您的程序应生成单行输出，其中包含所有案例的结果，格式为列表的列表。每个内部列表包含对应案例的两个四舍五入后的浮点数 $[f_s(x^\\star),\\mathrm{RMSE}]$，顺序与上述案例顺序相同。例如，输出格式必须为\n$$\n\\left[\\,[f_s(x^\\star_1),\\mathrm{RMSE}_1],[f_s(x^\\star_2),\\mathrm{RMSE}_2],\\dots\\,[f_s(x^\\star_5),\\mathrm{RMSE}_5]\\,\\right],\n$$\n每个浮点数都四舍五入到 $6$ 位小数，且无额外文本。",
            "solution": "所提出的问题是为了构建代理模型而应用正则化线性回归的一个明确定义的应用。所有提供的数据和参数都是一致且充分的，足以推导出一个唯一且有意义的解。该问题在科学上和数学上都是合理的，基于数值函数逼近和机器学习的标准原理。因此，我们将着手提供一个完整的解。\n\n问题的核心是为多项式代理模型 $f_s(x) = \\sum_{k=0}^d w_k x^k$ 找到一组系数 $\\mathbf{w} = [w_0, w_1, \\dots, w_d]^T$，使其能最好地拟合给定的训练数据 $\\{(x_i, y_i)\\}_{i=1}^n$。“最佳拟合”的标准是最小化 Tikhonov 正则化的残差平方和，由以下目标函数给出：\n$$\nJ(\\mathbf{w}) = \\sum_{i=1}^n \\left( f_s(x_i) - y_i \\right)^2 + \\lambda \\lVert \\mathbf{w} \\rVert_2^2\n$$\n此处，$\\lambda \\ge 0$ 是正则化参数，它惩罚大的系数值，从而防止过拟合并改善问题的条件数。\n\n为解决此最小化问题，我们首先将其表示为矩阵-向量形式。设 $\\mathbf{y} = [y_1, y_2, \\dots, y_n]^T$ 为观测输出的向量。模型对所有训练输入的预测可以写成 $\\hat{\\mathbf{y}} = \\mathbf{\\Phi}\\mathbf{w}$，其中 $\\mathbf{\\Phi}$ 是 $n \\times (d+1)$ 的设计矩阵。$\\mathbf{\\Phi}$ 的每一行对应一个数据点 $x_i$，每一列对应一个基函数 $\\phi_k(x) = x^k$。因此，第 $i$ 行、第 $j$ 列的元素（使用从0开始的索引，$j=k$）是 $\\mathbf{\\Phi}_{ij} = x_i^j$。\n$$\n\\mathbf{\\Phi} = \\begin{pmatrix}\n1 & x_1 & x_1^2 & \\dots & x_1^d \\\\\n1 & x_2 & x_2^2 & \\dots & x_2^d \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_n & x_n^2 & \\dots & x_n^d\n\\end{pmatrix}\n$$\n目标函数 $J(\\mathbf{w})$现在可以用矩阵和向量表示：\n$$\nJ(\\mathbf{w}) = (\\mathbf{\\Phi}\\mathbf{w} - \\mathbf{y})^T (\\mathbf{\\Phi}\\mathbf{w} - \\mathbf{y}) + \\lambda \\mathbf{w}^T \\mathbf{w}\n$$\n这是一个关于 $\\mathbf{w}$ 的二次函数。为了找到最小值，我们计算 $J(\\mathbf{w})$ 关于 $\\mathbf{w}$ 的梯度，并将其设为零向量。\n$$\n\\nabla_{\\mathbf{w}} J(\\mathbf{w}) = \\nabla_{\\mathbf{w}} \\left( \\mathbf{w}^T \\mathbf{\\Phi}^T \\mathbf{\\Phi} \\mathbf{w} - 2\\mathbf{y}^T \\mathbf{\\Phi} \\mathbf{w} + \\mathbf{y}^T \\mathbf{y} + \\lambda \\mathbf{w}^T \\mathbf{I} \\mathbf{w} \\right)\n$$\n其中 $\\mathbf{I}$ 是 $(d+1) \\times (d+1)$ 的单位矩阵。应用标准矩阵微积分法则（对于对称矩阵 $\\mathbf{A}$，有 $\\nabla_{\\mathbf{z}} (\\mathbf{z}^T \\mathbf{A} \\mathbf{z}) = 2\\mathbf{A}\\mathbf{z}$；以及 $\\nabla_{\\mathbf{z}} (\\mathbf{b}^T \\mathbf{z}) = \\mathbf{b}$），我们得到：\n$$\n\\nabla_{\\mathbf{w}} J(\\mathbf{w}) = 2\\mathbf{\\Phi}^T \\mathbf{\\Phi} \\mathbf{w} - 2\\mathbf{\\Phi}^T \\mathbf{y} + 2\\lambda \\mathbf{I} \\mathbf{w}\n$$\n将梯度设为零，得到岭回归的法方程：\n$$\n2\\mathbf{\\Phi}^T \\mathbf{\\Phi} \\mathbf{w} - 2\\mathbf{\\Phi}^T \\mathbf{y} + 2\\lambda \\mathbf{I} \\mathbf{w} = \\mathbf{0}\n$$\n$$\n(\\mathbf{\\Phi}^T \\mathbf{\\Phi} + \\lambda \\mathbf{I}) \\mathbf{w} = \\mathbf{\\Phi}^T \\mathbf{y}\n$$\n最优系数向量 $\\mathbf{w}$ 是这个线性方程组的解。对于任何 $\\lambda > 0$，矩阵 $(\\mathbf{\\Phi}^T \\mathbf{\\Phi} + \\lambda \\mathbf{I})$ 保证是可逆的，从而确保解的唯一性。对于 $\\lambda = 0$，如果 $\\mathbf{\\Phi}$ 具有满列秩，则存在唯一解，这在唯一数据点数 $n$ 大于或等于系数数量 $d+1$ 时通常成立。因此，我们可以求解 $\\mathbf{w}$：\n$$\n\\mathbf{w}^{\\star} = (\\mathbf{\\Phi}^T \\mathbf{\\Phi} + \\lambda \\mathbf{I})^{-1} \\mathbf{\\Phi}^T \\mathbf{y}\n$$\n在数值计算上，直接求解线性系统比计算矩阵的逆更可取。\n\n一旦确定了最优系数 $\\mathbf{w}^{\\star}$，我们就可以执行所需的计算：\n1.  **预测**：在新查询点 $x^\\star$ 处的预测值是通过评估代理模型来计算的：\n    $$\n    f_s(x^\\star) = \\sum_{k=0}^d w^{\\star}_k (x^\\star)^k = \\mathbf{\\phi}(x^\\star)^T \\mathbf{w}^{\\star}\n    $$\n    其中 $\\mathbf{\\phi}(x^\\star) = [1, x^\\star, (x^\\star)^2, \\dots, (x^\\star)^d]^T$ 是在 $x^\\star$ 处求值的基函数向量。\n\n2.  **训练均方根误差 (RMSE)**：该指标量化了在训练数据上的平均误差幅度。首先，我们计算模型对所有训练点的预测值，$\\hat{\\mathbf{y}} = \\mathbf{\\Phi} \\mathbf{w}^{\\star}$。然后，RMSE 由下式给出：\n    $$\n    \\mathrm{RMSE} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n (\\hat{y}_i - y_i)^2} = \\sqrt{\\frac{1}{n} (\\mathbf{\\Phi}\\mathbf{w}^{\\star} - \\mathbf{y})^T (\\mathbf{\\Phi}\\mathbf{w}^{\\star} - \\mathbf{y})}\n    $$\n\n将要实现的算法将对每个测试用例遵循以下步骤：a) 从输入数据构建设计矩阵 $\\mathbf{\\Phi}$ 和向量 $\\mathbf{y}$；b) 求解线性方程组 $(\\mathbf{\\Phi}^T \\mathbf{\\Phi} + \\lambda \\mathbf{I}) \\mathbf{w} = \\mathbf{\\Phi}^T \\mathbf{y}$ 以求得 $\\mathbf{w}$；c) 计算查询点处的预测值 $f_s(x^\\star)$；d) 计算训练 RMSE；e) 对结果进行四舍五入并报告。",
            "answer": "```python\nimport numpy as np\n\ndef fit_and_evaluate(x_train, y_train, d, lambd, x_query):\n    \"\"\"\n    Constructs and evaluates a polynomial surrogate model using Tikhonov regularization.\n\n    Args:\n        x_train (list or np.ndarray): The scalar input training data {x_i}.\n        y_train (list or np.ndarray): The scalar output training data {y_i}.\n        d (int): The degree of the polynomial basis.\n        lambd (float): The Tikhonov regularization parameter.\n        x_query (float): The query point at which to predict the output.\n\n    Returns:\n        list: A list containing [predicted_value, training_rmse], rounded to 6 decimal places.\n    \"\"\"\n    # Ensure inputs are numpy arrays for vectorized operations.\n    x_train = np.array(x_train)\n    y_train = np.array(y_train)\n    \n    n = len(x_train)\n    num_coeffs = d + 1\n\n    # Construct the design matrix Phi using a Vandermonde matrix.\n    # The basis is {x^0, x^1, ..., x^d}, which corresponds to increasing=True.\n    Phi = np.vander(x_train, N=num_coeffs, increasing=True)\n\n    # Solve the normal equations for ridge regression to find the coefficients w.\n    # (Phi^T * Phi + lambda * I) w = Phi^T * y\n    A = Phi.T @ Phi + lambd * np.identity(num_coeffs)\n    b = Phi.T @ y_train\n    w = np.linalg.solve(A, b)\n\n    # Calculate the prediction at the query point x_query.\n    # Construct the basis vector for the query point.\n    phi_query = np.array([x_query**k for k in range(num_coeffs)])\n    prediction = phi_query @ w\n\n    # Calculate the training Root Mean Squared Error (RMSE).\n    # First, get the model's predictions on the training data.\n    y_pred_train = Phi @ w\n    # Calculate the sum of squared errors (SSE) and then the RMSE.\n    sse = np.sum((y_pred_train - y_train)**2)\n    rmse = np.sqrt(sse / n)\n\n    # Round the final values to 6 decimal places as required.\n    prediction_rounded = round(prediction, 6)\n    rmse_rounded = round(rmse, 6)\n    \n    return [prediction_rounded, rmse_rounded]\n\ndef solve():\n    \"\"\"\n    Main function to process all test cases and print the final output.\n    \"\"\"\n    # Case 1: Exact quadratic fit, no regularization.\n    case1 = {\n        \"x\": [-2.0, -1.0, 0.0, 1.0, 2.0],\n        \"y\": [6.0, 3.5, 2.0, 1.5, 2.0],\n        \"d\": 2, \"lambda\": 0.0, \"x_star\": 0.5\n    }\n\n    # Case 2: Cubic fit with perturbations and regularization.\n    x2 = [-2.0, -1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0]\n    eps2 = [0.05, -0.04, 0.02, 0.01, 0.0, -0.03, 0.04, 0.02, -0.05]\n    y2 = [xi**3 - 2*xi + e for xi, e in zip(x2, eps2)]\n    case2 = {\n        \"x\": x2, \"y\": y2, \"d\": 3, \"lambda\": 0.001, \"x_star\": 1.2\n    }\n\n    # Case 3: Linear fit with duplicated inputs and regularization.\n    case3 = {\n        \"x\": [0.0, 0.0, 0.5, 0.5, 1.0, 1.0],\n        \"y\": [0.2, 0.21, 0.93, 0.97, 1.7, 1.69],\n        \"d\": 1, \"lambda\": 0.01, \"x_star\": 0.8\n    }\n\n    # Case 4: Higher-order polynomial fit, near-interpolation.\n    x4 = [-1.0, -0.6, -0.2, 0.2, 0.6, 1.0]\n    y4_func = lambda x: 1.0 - 0.5*x + 0.3*x**2 - 0.1*x**3 + 0.05*x**4 - 0.02*x**5\n    y4 = [y4_func(xi) for xi in x4]\n    case4 = {\n        \"x\": x4, \"y\": y4, \"d\": 5, \"lambda\": 1e-8, \"x_star\": -0.9\n    }\n\n    # Case 5: Single-point, constant model.\n    case5 = {\n        \"x\": [1.2], \"y\": [3.4], \"d\": 0, \"lambda\": 0.0, \"x_star\": 2.0\n    }\n\n    test_cases = [case1, case2, case3, case4, case5]\n\n    results_str = []\n    for case in test_cases:\n        result = fit_and_evaluate(case[\"x\"], case[\"y\"], case[\"d\"], case[\"lambda\"], case[\"x_star\"])\n        # Format each result pair into \"[val1,val2]\" string format\n        results_str.append(f\"[{result[0]},{result[1]}]\")\n\n    # Print the final result in the exact required format: [[...],[...],...]\n    print(f\"[{','.join(results_str)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "代理模型的一个核心应用是优化和系统分析。本练习将带您超越简单的模型拟合，进入响应面分析的领域，您将学习如何利用已拟合的二次响应面模型，通过计算其梯度和 Hessian 矩阵来定位并分类其稳态点（如最大值、最小值或鞍点）。这项技能对于在生物医学系统中寻找最佳操作条件至关重要 。",
            "id": "3933554",
            "problem": "在生物医学系统建模中，一个剂量-反应实验测量作为两个可控输入（药物剂量和孵育时间）函数的标量响应。考虑一个用于响应的二次代理模型（一个二次响应面），其中输入为剂量和时间。令 $x_1$ 表示剂量（单位为微摩尔/升），令 $x_2$ 表示时间（单位为小时）。建模的响应是一个标量函数 $r(x_1,x_2)$，它将由一个二次多项式近似。该多项式的系数通过普通最小二乘法（Ordinary Least Squares, OLS）从数据中确定。实验提供了以下数据集（每个数据集是一个由三元组 $(x_1,x_2,r)$ 组成的列表）。模型、拟合的驻点以及分类必须完全用数学术语表示，不得假定超出给定定义之外的外部领域知识。\n\n对于每个数据集，您的任务是：\n- 使用 OLS 方法，通过最小化测量值与二次模型之间的残差平方和，从数据中拟合一个二次响应面 $r(x_1,x_2)$。\n- 通过求解梯度等于零的位置，计算拟合的二次模型的驻点 $(x_1^\\star,x_2^\\star)$。\n- 计算拟合的二次模型的 Hessian 矩阵 $H$，并使用多元微积分中的标准二阶检验，根据 $H$ 的特征值对驻点进行分类。使用以下分类规则，数值容差为 $\\tau = 10^{-5}$：如果 $H$ 的任何一个特征值的绝对值严格小于 $\\tau$，则返回代码 $2$（由于近奇异曲率而无法确定）；否则，如果所有特征值均为正，则返回代码 $-1$（最小值）；否则，如果所有特征值均为负，则返回代码 $1$（最大值）；否则，返回代码 $0$（鞍点）。\n- 最终输出严格为分类代码，而不是驻点的坐标。最终答案中不需要物理单位。\n\n使用以下由四个数据集组成的测试套件。每个集合列出了 $(x_1,x_2,r)$ 的值，其中 $x_1$ 和 $x_2$ 是输入设置，$r$ 是测量的响应：\n- 数据集 $1$（良态最小值情况）：\n  $(x_1,x_2,r)$ 等于\n  (0, 0, 3.0), \n  (0, 1, 5.8), \n  (0, 2, 10.2), \n  (1, 0, 4.5), \n  (1, 1, 7.4), \n  (1, 2, 11.9), \n  (2, 0, 7.0), \n  (2, 1, 10.0), \n  (2, 2, 14.6).\n- 数据集 $2$（良态最大值情况）：\n  $(x_1,x_2,r)$ 等于\n  (0, 0, 2.0), \n  (0, 1, 2.0), \n  (0, 2, 1.0), \n  (1, 0, -0.7), \n  (1, 1, -0.9), \n  (1, 2, -2.1), \n  (2, 0, -5.8), \n  (2, 1, -6.2), \n  (2, 2, -7.6).\n- 数据集 $3$（鞍点情况）：\n  $(x_1,x_2,r)$ 等于\n  (0, 0, 1.0), \n  (0, 1, 0.0), \n  (0, 2, -2.6), \n  (1, 0, 1.6), \n  (1, 1, 0.6), \n  (1, 2, -2.0), \n  (2, 0, 3.2), \n  (2, 1, 2.2), \n  (2, 2, -0.4).\n- 数据集 $4$（剂量方向近奇异曲率情况）：\n  $(x_1,x_2,r)$ 等于\n  (0, 0, 1.0), \n  (0, 1, 2.0), \n  (0, 2, 5.0), \n  (1, 0, 1.000001), \n  (1, 1, 2.000001), \n  (1, 2, 5.000001), \n  (2, 0, 1.000004), \n  (2, 1, 2.000004), \n  (2, 2, 5.000004).\n\n建模假设和要求：\n- 二次代理模型必须为 $r(x_1,x_2) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1^2 + \\beta_4 x_1 x_2 + \\beta_5 x_2^2$ 的形式，其中系数 $\\beta_0,\\beta_1,\\beta_2,\\beta_3,\\beta_4,\\beta_5$ 通过最小化给定数据集上的残差平方和来确定。\n- 驻点 $(x_1^\\star,x_2^\\star)$ 必须满足 $r$ 的梯度在 $(x_1^\\star,x_2^\\star)$ 处等于零，并且必须使用 $r$ 的 Hessian 矩阵 $H$ 通过检查 $H$ 的特征值（容差为 $\\tau = 10^{-5}$）来对驻点的性质进行分类。\n\n您的程序应生成单行输出，其中包含四个数据集的分类代码，格式为用方括号括起来的逗号分隔列表，顺序为数据集 $1$、数据集 $2$、数据集 $3$、数据集 $4$（例如 $\\left[ \\text{code}_1,\\text{code}_2,\\text{code}_3,\\text{code}_4 \\right]$）。每个代码必须是上面指定的整数 $-1$、$0$、$1$ 或 $2$ 中的一个。不允许有其他输出。",
            "solution": "该问题要求我们为四个不同的数据集分析一个二次响应面模型。对于每个数据集，我们必须拟合模型，找到其驻点，并对该点的性质（最小值、最大值、鞍点或不确定）进行分类。该过程涉及应用普通最小二乘法（OLS），然后是多元微积分中的标准二阶导数检验。\n\n### 步骤 1：模型构建与普通最小二乘法（OLS）\n\n关于响应 $r$ 作为两个输入（剂量 $x_1$ 和时间 $x_2$）函数的先验二次代理模型由下式给出：\n$$r(x_1, x_2) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1^2 + \\beta_4 x_1 x_2 + \\beta_5 x_2^2$$\n该模型相对于其系数 $\\boldsymbol{\\beta} = [\\beta_0, \\beta_1, \\beta_2, \\beta_3, \\beta_4, \\beta_5]^T$ 是线性的。给定一组 $N$ 个数据点 $(x_{1i}, x_{2i}, r_i)$，我们可以将所有点的关系表示为矩阵形式：\n$$ \\mathbf{r} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon} $$\n其中 $\\mathbf{r}$ 是一个观测响应的 $N \\times 1$ 向量，$\\mathbf{X}$ 是 $N \\times 6$ 的设计矩阵，$\\boldsymbol{\\beta}$ 是 $6 \\times 1$ 的系数向量，$\\boldsymbol{\\epsilon}$ 是残差向量。设计矩阵 $\\mathbf{X}$ 的第 $i$ 行对应第 $i$ 个数据点，构造如下：\n$$ \\mathbf{X}_i = [1, x_{1i}, x_{2i}, x_{1i}^2, x_{1i}x_{2i}, x_{2i}^2] $$\nOLS 方法找到最小化残差平方和 $S = \\boldsymbol{\\epsilon}^T\\boldsymbol{\\epsilon} = (\\mathbf{r} - \\mathbf{X}\\boldsymbol{\\beta})^T(\\mathbf{r} - \\mathbf{X}\\boldsymbol{\\beta})$ 的系数向量 $\\hat{\\boldsymbol{\\beta}}$。这个最小化问题的解由正规方程给出：\n$$ (\\mathbf{X}^T\\mathbf{X})\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}^T\\mathbf{r} $$\n假设矩阵 $\\mathbf{X}^T\\mathbf{X}$ 是可逆的，则系数的唯一 OLS 估计为：\n$$ \\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{r} $$\n对四个数据集中的每一个都执行此计算，以获得特定的拟合模型 $\\hat{r}(x_1, x_2)$。\n\n### 步骤 2：寻找驻点\n\n拟合响应面 $\\hat{r}(x_1, x_2)$ 的一个驻点 $(x_1^\\star, x_2^\\star)$ 是函数梯度为零向量的点：\n$$ \\nabla \\hat{r}(x_1^\\star, x_2^\\star) = \\mathbf{0} $$\n梯度向量为：\n$$ \\nabla \\hat{r} = \\begin{bmatrix} \\frac{\\partial \\hat{r}}{\\partial x_1} \\\\ \\frac{\\partial \\hat{r}}{\\partial x_2} \\end{bmatrix} = \\begin{bmatrix} \\hat{\\beta}_1 + 2\\hat{\\beta}_3 x_1 + \\hat{\\beta}_4 x_2 \\\\ \\hat{\\beta}_2 + \\hat{\\beta}_4 x_1 + 2\\hat{\\beta}_5 x_2 \\end{bmatrix} $$\n将梯度设为零，得到一个关于 $x_1^\\star$ 和 $x_2^\\star$ 的二元线性方程组：\n$$ 2\\hat{\\beta}_3 x_1^\\star + \\hat{\\beta}_4 x_2^\\star = -\\hat{\\beta}_1 $$\n$$ \\hat{\\beta}_4 x_1^\\star + 2\\hat{\\beta}_5 x_2^\\star = -\\hat{\\beta}_2 $$\n该方程组可以写成矩阵形式：\n$$ \\begin{bmatrix} 2\\hat{\\beta}_3 & \\hat{\\beta}_4 \\\\ \\hat{\\beta}_4 & 2\\hat{\\beta}_5 \\end{bmatrix} \\begin{bmatrix} x_1^\\star \\\\ x_2^\\star \\end{bmatrix} = \\begin{bmatrix} -\\hat{\\beta}_1 \\\\ -\\hat{\\beta}_2 \\end{bmatrix} $$\n\n### 步骤 3：使用 Hessian 矩阵对驻点进行分类\n\n驻点的性质由二阶偏导数决定，这些偏导数排列在 Hessian 矩阵 $H$ 中。对于二次模型，Hessian 矩阵是一个常数矩阵：\n$$ H = \\begin{bmatrix} \\frac{\\partial^2 \\hat{r}}{\\partial x_1^2} & \\frac{\\partial^2 \\hat{r}}{\\partial x_1 \\partial x_2} \\\\ \\frac{\\partial^2 \\hat{r}}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 \\hat{r}}{\\partial x_2^2} \\end{bmatrix} = \\begin{bmatrix} 2\\hat{\\beta}_3 & \\hat{\\beta}_4 \\\\ \\hat{\\beta}_4 & 2\\hat{\\beta}_5 \\end{bmatrix} $$\n注意，这与求解驻点的线性方程组中出现的矩阵是同一个。分类基于 $H$ 的特征值 $\\lambda_1$ 和 $\\lambda_2$ 的符号。问题指定了以下分类规则，数值容差为 $\\tau = 10^{-5}$：\n\n- **无法确定（代码 $2$）**：如果 $|\\lambda_1| < \\tau$ 或 $|\\lambda_2| < \\tau$。这表明曲率在至少一个方向上接近平坦，使得分类在数值上不稳定，或者驻点是“谷”或“脊”的一部分，而不是一个孤立点。\n- **局部最小值（代码 $-1$）**：如果两个特征值都为正（即 $\\lambda_1 > 0$ 且 $\\lambda_2 > 0$），并且未触发无法确定的情况。这对应于一个在所有方向上都向上凹的曲面。\n- **局部最大值（代码 $1$）**：如果两个特征值都为负（即 $\\lambda_1 < 0$ 且 $\\lambda_2 < 0$），并且未触发无法确定的情况。这对应于一个在所有方向上都向下凹的曲面。\n- **鞍点（代码 $0$）**：其他情况。当特征值符号相反（$\\lambda_1 \\lambda_2 < 0$）且未触发无法确定的情况时发生。曲面在一个方向上向上凹，在另一个方向上向下凹。\n\n对每个数据集实施以下步骤：\n1. 从给定的 $(x_1, x_2, r)$ 三元组构建设计矩阵 $\\mathbf{X}$ 和响应向量 $\\mathbf{r}$。\n2. 使用数值 OLS 求解器计算系数向量 $\\hat{\\boldsymbol{\\beta}}$，这等同于计算 $\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{r}$。\n3. 使用估计的系数 $\\hat{\\beta}_3$、$\\hat{\\beta}_4$ 和 $\\hat{\\beta}_5$ 构建 Hessian 矩阵 $H$。\n4. 计算对称矩阵 $H$ 的特征值。\n5. 应用指定的分类逻辑来确定整数代码。\n\n对所有四个数据集重复此过程，以生成最终的分类代码列表。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Fits a quadratic response surface to data sets and classifies the stationary points.\n    \"\"\"\n    \n    # Test suite containing the four data sets.\n    # Each data set is a list of (x1, x2, r) tuples.\n    test_suite = [\n        # Data Set 1 (well-conditioned minimum case)\n        np.array([\n            [0, 0, 3.0], [0, 1, 5.8], [0, 2, 10.2],\n            [1, 0, 4.5], [1, 1, 7.4], [1, 2, 11.9],\n            [2, 0, 7.0], [2, 1, 10.0], [2, 2, 14.6]\n        ]),\n        # Data Set 2 (well-conditioned maximum case)\n        np.array([\n            [0, 0, 2.0], [0, 1, 2.0], [0, 2, 1.0],\n            [1, 0, -0.7], [1, 1, -0.9], [1, 2, -2.1],\n            [2, 0, -5.8], [2, 1, -6.2], [2, 2, -7.6]\n        ]),\n        # Data Set 3 (saddle case)\n        np.array([\n            [0, 0, 1.0], [0, 1, 0.0], [0, 2, -2.6],\n            [1, 0, 1.6], [1, 1, 0.6], [1, 2, -2.0],\n            [2, 0, 3.2], [2, 1, 2.2], [2, 2, -0.4]\n        ]),\n        # Data Set 4 (near-singular curvature along dose)\n        np.array([\n            [0, 0, 1.0], [0, 1, 2.0], [0, 2, 5.0],\n            [1, 0, 1.000001], [1, 1, 2.000001], [1, 2, 5.000001],\n            [2, 0, 1.000004], [2, 1, 2.000004], [2, 2, 5.000004]\n        ])\n    ]\n\n    results = []\n    tau = 1e-5\n\n    for data_set in test_suite:\n        # Extract inputs (x1, x2) and responses (r)\n        x1 = data_set[:, 0]\n        x2 = data_set[:, 1]\n        r = data_set[:, 2]\n\n        # Construct the design matrix X for the model:\n        # r = b0 + b1*x1 + b2*x2 + b3*x1^2 + b4*x1*x2 + b5*x2^2\n        X = np.c_[np.ones(x1.shape[0]), x1, x2, x1**2, x1*x2, x2**2]\n\n        # Solve for the coefficients beta using Ordinary Least Squares (OLS)\n        beta = np.linalg.lstsq(X, r, rcond=None)[0]\n        b3, b4, b5 = beta[3], beta[4], beta[5]\n\n        # Construct the Hessian matrix H\n        # H = [[2*b3, b4],\n        #      [b4, 2*b5]]\n        H = np.array([[2 * b3, b4], [b4, 2 * b5]])\n\n        # Compute the eigenvalues of the Hessian\n        # eigvalsh is used for real symmetric matrices\n        eigenvalues = np.linalg.eigvalsh(H)\n        \n        # Classify the stationary point based on the eigenvalues\n        classification_code = 0\n        if np.any(np.abs(eigenvalues) < tau):\n            classification_code = 2  # Indeterminate\n        elif np.all(eigenvalues > 0):\n            classification_code = -1 # Minimum\n        elif np.all(eigenvalues < 0):\n            classification_code = 1  # Maximum\n        else:\n            classification_code = 0  # Saddle\n\n        results.append(classification_code)\n    \n    # Print the results in the required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "虽然多项式模型很有用，但更高级的非参数化方法，如高斯过程 (GP) 回归，提供了更大的灵活性和信息。本练习将引导您从第一性原理出发实现一个 GP 模型，它不仅能捕捉数据中复杂的非线性关系，还能提供对预测不确定性的 principled 度量——即认知方差 (epistemic variance)。理解和量化这种不确定性对于在模型引导的决策和实验设计中评估置信度至关重要 。",
            "id": "3933600",
            "problem": "您需要实现一个通用的、纯数学的程序，用于计算一维生物医学响应面的高斯过程（GP）代理模型的预测均值和认知方差。请从第一性原理出发：高斯过程（GP）被定义为一组随机变量的集合，其中任意有限个变量都服从联合高斯分布。此问题的基本基础是高斯过程先验和高斯似然模型定义的结合，以及多元高斯分布的条件化规则。具体来说，假设一个潜函数 $f$ 服从零均值高斯过程先验 $f \\sim \\mathcal{GP}(0, k)$，并且带噪声的观测值 $y_i$ 由模型 $y_i = f(x_i) + \\epsilon_i$ 生成，其中 $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma_n^2)$ 是独立同分布的。协方差由一个平稳、正定的核函数 $k(\\cdot, \\cdot)$ 给出。\n\n使用平方指数核（也称为径向基函数核），其超参数为信号方差 $\\sigma_f^2$ 和长度尺度 $l$，定义如下\n$$\nk(x, x') = \\sigma_f^2 \\exp\\left(-\\frac{(x - x')^2}{2 l^2}\\right).\n$$\n所有量都是无量纲的。不使用角度。不使用百分比。\n\n您的程序必须在每个测试案例中，基于训练输入 $X$ 和输出 $y$，计算在单个新点 $x^\\star$ 处的预测均值和认知方差（即潜函数 $f$ 在测试输入处的后验方差）。认知方差是 $f(x^\\star)$ 在后验分布下的方差，不包括观测噪声方差 $\\sigma_n^2$。请从 GP 先验和高斯似然所蕴含的联合高斯分布出发，并应用多元高斯分布的条件化规则来推导算法。为了数值稳定性，您的实现必须在分解前向训练协方差矩阵的对角线添加一个小的正对角抖动 $\\delta$，其中 $\\delta$ 选择为 $\\delta = 10^{-10}$。\n\n为了以科学上真实且自洽的方式实例化生物医学背景，训练输出 $y$ 是从代表受体-配体结合的归一化剂量-反应曲线确定性地生成的。使用 logistic-sigmoid 形式\n$$\nf(x) = \\frac{1}{1 + \\exp\\left(-a (x - b)\\right)},\n$$\n其中陡度参数 $a = 8$，中点 $b = 0.5$。对于任何给定的训练集 $X$，除非另有说明，定义 $y_i = f(x_i)$。\n\n实现以下四个测试案例，以在各种条件下检验算法。在所有情况下，所有量都是无量纲的：\n\n- 测试案例 1（理想路径，中等噪声，插值）：\n  - 训练输入 $X = [0.0, 0.25, 0.5, 0.75, 1.0]$。\n  - 训练输出 $y_i = f(x_i)$。\n  - 超参数：长度尺度 $l = 0.2$，信号方差 $\\sigma_f = 1.0$，观测噪声标准差 $\\sigma_n = 0.1$。\n  - 查询输入 $x^\\star = 0.6$。\n\n- 测试案例 2（边界条件，零噪声，精确训练输入）：\n  - 训练输入 $X = [0.0, 0.25, 0.5, 0.75, 1.0]$。\n  - 训练输出 $y_i = f(x_i)$。\n  - 超参数：长度尺度 $l = 0.3$，信号方差 $\\sigma_f = 1.0$，观测噪声标准差 $\\sigma_n = 0.0$。\n  - 查询输入 $x^\\star = 0.5$。\n\n- 测试案例 3（边缘情况，远超训练域的外推）：\n  - 训练输入 $X = [0.0, 0.25, 0.5, 0.75, 1.0]$。\n  - 训练输出 $y_i = f(x_i)$。\n  - 超参数：长度尺度 $l = 0.2$，信号方差 $\\sigma_f = 1.0$，观测噪声标准差 $\\sigma_n = 0.1$。\n  - 查询输入 $x^\\star = 2.0$。\n\n- 测试案例 4（边缘情况，带有小噪声的重复训练输入）：\n  - 训练输入 $X = [0.0, 0.25, 0.5, 0.5, 0.75, 1.0]$。\n  - 训练输出与 $X$ 按元素对齐如下：\n    - 对于 $x = 0.0$， $y = f(0.0)$。\n    - 对于 $x = 0.25$， $y = f(0.25)$。\n    - 对于第三个条目 $x = 0.5$，使用 $y = f(0.5) - 0.02$。\n    - 对于第四个条目 $x = 0.5$，使用 $y = f(0.5) + 0.02$。\n    - 对于 $x = 0.75$， $y = f(0.75)$。\n    - 对于 $x = 1.0$， $y = f(1.0)$。\n  - 超参数：长度尺度 $l = 0.15$，信号方差 $\\sigma_f = 1.0$，观测噪声标准差 $\\sigma_n = 0.05$。\n  - 查询输入 $x^\\star = 0.5$。\n\n您的程序必须为每个测试案例输出预测均值 $\\mu(x^\\star)$ 和认知方差 $\\sigma^2(x^\\star)$ 作为浮点数。将四个测试案例的结果汇总到单行输出中，包含八个数字，顺序为 $\\left[\\mu_1, \\sigma_1^2, \\mu_2, \\sigma_2^2, \\mu_3, \\sigma_3^2, \\mu_4, \\sigma_4^2\\right]$。您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，$[\\text{result1},\\text{result2},\\text{result3}]$）。所有量都是无量纲的。所有数字都表示为十进制浮点值。",
            "solution": "该问题要求实现高斯过程（GP）回归，以计算潜函数 $f$ 在新输入点 $x^\\star$ 处的预测后验均值和认知方差。推导过程从 GP 的基本定义以及多元高斯分布的条件化规则出发。\n\n**1. 模型设定**\n\n高斯过程定义了函数上的先验分布。我们假设潜函数 $f(x)$ 服从一个零均值和一个协方差函数（核函数）$k(x, x')$ 的高斯过程。这表示为：\n$$\nf(x) \\sim \\mathcal{GP}(0, k(x, x'))\n$$\n问题指定了平方指数核：\n$$\nk(x, x') = \\sigma_f^2 \\exp\\left(-\\frac{(x - x')^2}{2 l^2}\\right)\n$$\n其中 $\\sigma_f^2$ 是信号方差，$l$ 是长度尺度。\n\n我们给定一组 $N$ 个训练输入 $X = \\{x_1, \\dots, x_N\\}$ 以及相应的带噪声的观测值 $\\mathbf{y} = [y_1, \\dots, y_N]^T$。观测模型为：\n$$\ny_i = f(x_i) + \\epsilon_i, \\quad \\text{其中} \\quad \\epsilon_i \\sim \\mathcal{N}(0, \\sigma_n^2)\n$$\n噪声项 $\\epsilon_i$ 假定为独立同分布。\n\n**2. 联合先验分布**\n\n根据 GP 的定义，任意有限个函数值的集合都服从联合高斯分布。设 $\\mathbf{f}$ 为训练输入处的潜函数值向量，$\\mathbf{f} = [f(x_1), \\dots, f(x_N)]^T$，而 $f^\\star$ 为新测试输入 $x^\\star$ 处的潜函数值。它们在先验下的联合分布为：\n$$\n\\begin{pmatrix} \\mathbf{f} \\\\ f^\\star \\end{pmatrix} \\sim \\mathcal{N} \\left( \\mathbf{0}, \\begin{pmatrix} K(X, X) & K(X, x^\\star) \\\\ K(x^\\star, X) & k(x^\\star, x^\\star) \\end{pmatrix} \\right)\n$$\n其中：\n- $K(X, X)$ 是训练输入的 $N \\times N$ 协方差矩阵，其元素为 $[K(X, X)]_{ij} = k(x_i, x_j)$。\n- $K(X, x^\\star)$ 是训练输入与测试输入之间协方差的 $N \\times 1$ 向量，其元素为 $[K(X, x^\\star)]_i = k(x_i, x^\\star)$。\n- $K(x^\\star, X) = K(X, x^\\star)^T$。\n- $k(x^\\star, x^\\star)$ 是函数在测试输入处的先验方差，对于平方指数核，该值为 $\\sigma_f^2$。\n\n**3. 观测值与测试值的联合分布**\n\n观测值向量 $\\mathbf{y}$ 通过 $\\mathbf{y} = \\mathbf{f} + \\boldsymbol{\\epsilon}$ 与潜函数值 $\\mathbf{f}$ 相关联，其中 $\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma_n^2 I)$。我们需要观测数据 $\\mathbf{y}$ 和潜函数值 $f^\\star$ 的联合分布。由于它们是通过对高斯变量进行线性运算形成的，它们也服从联合高斯分布。\n\n均值向量为零：\n$$\nE\\left[\\begin{pmatrix} \\mathbf{y} \\\\ f^\\star \\end{pmatrix}\\right] = \\begin{pmatrix} E[\\mathbf{f} + \\boldsymbol{\\epsilon}] \\\\ E[f^\\star] \\end{pmatrix} = \\begin{pmatrix} \\mathbf{0} \\\\ 0 \\end{pmatrix}\n$$\n协方差矩阵为：\n$$\n\\text{Cov}\\left(\\begin{pmatrix} \\mathbf{y} \\\\ f^\\star \\end{pmatrix}\\right) = E\\left[ \\begin{pmatrix} \\mathbf{y} \\\\ f^\\star \\end{pmatrix} \\begin{pmatrix} \\mathbf{y}^T & (f^\\star)^T \\end{pmatrix} \\right] = \\begin{pmatrix} E[\\mathbf{y}\\mathbf{y}^T] & E[\\mathbf{y}(f^\\star)^T] \\\\ E[f^\\star \\mathbf{y}^T] & E[f^\\star(f^\\star)^T] \\end{pmatrix}\n$$\n该协方差矩阵的各分块为：\n- $\\text{Cov}(\\mathbf{y}, \\mathbf{y}) = E[(\\mathbf{f}+\\boldsymbol{\\epsilon})(\\mathbf{f}+\\boldsymbol{\\epsilon})^T] = E[\\mathbf{f}\\mathbf{f}^T] + E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] = K(X, X) + \\sigma_n^2 I$。\n- $\\text{Cov}(\\mathbf{y}, f^\\star) = E[(\\mathbf{f}+\\boldsymbol{\\epsilon})(f^\\star)^T] = E[\\mathbf{f}(f^\\star)^T] + E[\\boldsymbol{\\epsilon}(f^\\star)^T] = K(X, x^\\star)$。\n- $\\text{Cov}(f^\\star, f^\\star) = k(x^\\star, x^\\star)$。\n\n因此，联合分布为：\n$$\n\\begin{pmatrix} \\mathbf{y} \\\\ f^\\star \\end{pmatrix} \\sim \\mathcal{N} \\left( \\mathbf{0}, \\begin{pmatrix} K(X, X) + \\sigma_n^2 I & K(X, x^\\star) \\\\ K(x^\\star, X) & k(x^\\star, x^\\star) \\end{pmatrix} \\right)\n$$\n\n**4. 后验分布（条件化）**\n\n我们的目标是找到在给定观测数据条件下潜函数值的后验分布 $p(f^\\star | X, \\mathbf{y}, x^\\star)$。对于一个一般的多元高斯划分 $\\begin{pmatrix} \\mathbf{a} \\\\ \\mathbf{b} \\end{pmatrix} \\sim \\mathcal{N} \\left( \\begin{pmatrix} \\boldsymbol{\\mu}_a \\\\ \\boldsymbol{\\mu}_b \\end{pmatrix}, \\begin{pmatrix} \\Sigma_{aa} & \\Sigma_{ab} \\\\ \\Sigma_{ba} & \\Sigma_{bb} \\end{pmatrix} \\right)$，其条件分布 $p(\\mathbf{b}|\\mathbf{a})$ 为 $\\mathcal{N}(\\boldsymbol{\\mu}_{b|a}, \\Sigma_{b|a})$，其中：\n$$\n\\boldsymbol{\\mu}_{b|a} = \\boldsymbol{\\mu}_b + \\Sigma_{ba} \\Sigma_{aa}^{-1} (\\mathbf{a} - \\boldsymbol{\\mu}_a)\n$$\n$$\n\\Sigma_{b|a} = \\Sigma_{bb} - \\Sigma_{ba} \\Sigma_{aa}^{-1} \\Sigma_{ab}\n$$\n通过代入我们的项（$\\mathbf{a} \\to \\mathbf{y}$，$\\mathbf{b} \\to f^\\star$，零均值），我们得到 $f^\\star$ 的后验分布为一个高斯分布，其均值为 $\\mu(x^\\star)$，方差为 $\\sigma^2(x^\\star)$：\n\n- **预测均值：**\n$$\n\\mu(x^\\star) = K(x^\\star, X) [K(X, X) + \\sigma_n^2 I]^{-1} \\mathbf{y}\n$$\n\n- **认知方差：**\n$$\n\\sigma^2(x^\\star) = k(x^\\star, x^\\star) - K(x^\\star, X) [K(X, X) + \\sigma_n^2 I]^{-1} K(X, x^\\star)\n$$\n问题要求的是认知方差，即潜函数 $f^\\star$ 的方差。对于新观测值 $y^\\star$ 的预测方差会包含噪声项，即 $\\sigma^2_{pred}(x^\\star) = \\sigma^2(x^\\star) + \\sigma_n^2$。我们仅计算 $\\sigma^2(x^\\star)$。\n\n**5. 实现算法**\n\n为了数值稳定性，在观测值的协方差矩阵对角线上添加一个小的抖动 $\\delta = 10^{-10}$。令 $K_y = K(X, X) + \\sigma_n^2 I + \\delta I$。\n\n计算步骤如下：\n1. 使用平方指数核函数公式构建核矩阵：$K(X, X)$、$K(X, x^\\star)$ 以及标量 $k(x^\\star, x^\\star)$。\n2. 构造待求逆的矩阵：$K_y = K(X, X) + (\\sigma_n^2 + \\delta)I_{N \\times N}$。\n3. 为避免直接矩阵求逆，求解线性系统 $K_y \\boldsymbol{\\alpha} = \\mathbf{y}$ 以获得向量 $\\boldsymbol{\\alpha}$。\n4. 计算预测均值：$\\mu(x^\\star) = K(x^\\star, X) \\boldsymbol{\\alpha} = \\sum_{i=1}^N \\alpha_i k(x_i, x^\\star)$。\n5. 为计算方差，求解第二个线性系统 $K_y \\mathbf{v} = K(X, x^\\star)$ 以获得向量 $\\mathbf{v}$。\n6. 计算认知方差：$\\sigma^2(x^\\star) = k(x^\\star, x^\\star) - K(x^\\star, X) \\mathbf{v}$。\n\n此过程应用于四个测试案例中的每一个。除非另有规定，训练数据 $y_i$ 使用 logistic-sigmoid 函数 $f(x) = (1 + \\exp(-a (x - b)))^{-1}$（参数 $a=8$ 和 $b=0.5$）生成。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the predictive mean and epistemic variance for a Gaussian Process\n    surrogate model for four specified test cases.\n    \"\"\"\n\n    def dose_response(x, a=8.0, b=0.5):\n        \"\"\"\n        Calculates the value of the normalized dose-response curve.\n        \"\"\"\n        return 1.0 / (1.0 + np.exp(-a * (np.asarray(x) - b)))\n\n    def se_kernel(X1, X2, l, sigma_f):\n        \"\"\"\n        Computes the squared-exponential kernel matrix between two sets of 1D points.\n        \"\"\"\n        l_sq = l**2\n        sigma_f_sq = sigma_f**2\n        # Ensure inputs are column vectors for broadcasting\n        X1 = np.asarray(X1).reshape(-1, 1)\n        X2 = np.asarray(X2).reshape(-1, 1)\n        sqdist = (X1.reshape(-1, 1) - X2.reshape(1, -1))**2\n        return sigma_f_sq * np.exp(-0.5 * sqdist / l_sq)\n\n    def gp_predict(X_train, y_train, x_star, l, sigma_f, sigma_n):\n        \"\"\"\n        Calculates the GP predictive mean and epistemic variance at a point x_star.\n        \"\"\"\n        delta = 1e-10  # Diagonal jitter for numerical stability\n        X_train = np.asarray(X_train).reshape(-1, 1)\n        y_train = np.asarray(y_train).reshape(-1, 1)\n        x_star_arr = np.array([x_star]).reshape(-1, 1)\n        n_train = len(X_train)\n\n        # Compute kernel matrices\n        K_XX = se_kernel(X_train, X_train, l, sigma_f)\n        K_sX = se_kernel(x_star_arr, X_train, l, sigma_f) # K(x*, X)\n        K_ss = se_kernel(x_star_arr, x_star_arr, l, sigma_f) # k(x*, x*)\n\n        # Form the matrix Ky = K(X, X) + sigma_n^2*I + delta*I\n        Ky = K_XX + (sigma_n**2) * np.eye(n_train) + delta * np.eye(n_train)\n\n        # Solve for alpha = Ky^-1 * y_train\n        alpha = np.linalg.solve(Ky, y_train)\n\n        # Predictive mean: mu(x*) = K(x*, X) * alpha\n        pred_mean = K_sX @ alpha\n\n        # For variance, solve for v = Ky^-1 * K(X, x*)\n        # K(X, x*) is the transpose of K(x*, X)\n        K_Xs = K_sX.T\n        v = np.linalg.solve(Ky, K_Xs)\n\n        # Epistemic variance: var(f*) = k(x*,x*) - K(x*,X) * v\n        pred_var = K_ss - K_sX @ v\n\n        return pred_mean.item(), pred_var.item()\n\n    test_cases = [\n        # Test case 1 (happy path, moderate noise, interpolation)\n        {\n            \"X\": [0.0, 0.25, 0.5, 0.75, 1.0],\n            \"y\": dose_response([0.0, 0.25, 0.5, 0.75, 1.0]),\n            \"l\": 0.2, \"sigma_f\": 1.0, \"sigma_n\": 0.1,\n            \"x_star\": 0.6\n        },\n        # Test case 2 (boundary condition, zero noise, exact training input)\n        {\n            \"X\": [0.0, 0.25, 0.5, 0.75, 1.0],\n            \"y\": dose_response([0.0, 0.25, 0.5, 0.75, 1.0]),\n            \"l\": 0.3, \"sigma_f\": 1.0, \"sigma_n\": 0.0,\n            \"x_star\": 0.5\n        },\n        # Test case 3 (edge case, extrapolation far outside training domain)\n        {\n            \"X\": [0.0, 0.25, 0.5, 0.75, 1.0],\n            \"y\": dose_response([0.0, 0.25, 0.5, 0.75, 1.0]),\n            \"l\": 0.2, \"sigma_f\": 1.0, \"sigma_n\": 0.1,\n            \"x_star\": 2.0\n        },\n        # Test case 4 (edge case, duplicate training inputs with small noise)\n        {\n            \"X\": [0.0, 0.25, 0.5, 0.5, 0.75, 1.0],\n            \"y\": [\n                dose_response(0.0),\n                dose_response(0.25),\n                dose_response(0.5) - 0.02,\n                dose_response(0.5) + 0.02,\n                dose_response(0.75),\n                dose_response(1.0)\n            ],\n            \"l\": 0.15, \"sigma_f\": 1.0, \"sigma_n\": 0.05,\n            \"x_star\": 0.5\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        mean, variance = gp_predict(\n            case[\"X\"], case[\"y\"], case[\"x_star\"],\n            case[\"l\"], case[\"sigma_f\"], case[\"sigma_n\"]\n        )\n        results.extend([mean, variance])\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}