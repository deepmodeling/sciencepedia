{
    "hands_on_practices": [
        {
            "introduction": "The first step in mastering surrogate modeling is to build one from the ground up. This practice focuses on the foundational technique of polynomial regression, where we approximate an unknown function using a weighted sum of polynomial basis functions. You will implement the fitting process by solving the normal equations for ridge regression, which incorporates Tikhonov regularization—a crucial method for preventing overfitting and improving model stability, especially with noisy or sparse data . This exercise provides a concrete understanding of how coefficients are learned and how to evaluate the resulting model's performance.",
            "id": "3933515",
            "problem": "You are given scalar training data pairs $\\{(x_i,y_i)\\}_{i=1}^n$ originating from a deterministic, noise-perturbed computational cardiac model that maps a scalar input parameter $x$ (dimensionless) to a scalar model output $y$ (dimensionless). Your task is to construct a surrogate model and evaluate its predictive performance on specified query inputs. Start from the following fundamental base: the surrogate model is a linear combination of fixed basis functions, and the best fit under a quadratic loss with Tikhonov (ridge) regularization is obtained by minimizing the regularized sum of squared residuals. You must use a univariate polynomial basis of degree $d$, that is, basis functions $\\{\\phi_k(x)\\}_{k=0}^d$ with $\\phi_k(x)=x^k$. The surrogate model is $f_s(x)=\\sum_{k=0}^d w_k \\phi_k(x)$ with coefficients $\\{w_k\\}_{k=0}^d$ that minimize the regularized empirical risk\n$$\nJ(\\mathbf{w})=\\sum_{i=1}^n\\left(f_s(x_i)-y_i\\right)^2+\\lambda\\lVert \\mathbf{w}\\rVert_2^2,\n$$\nwhere $\\lambda \\ge 0$ is the Tikhonov regularization parameter. After fitting, compute the predicted value $f_s(x^\\star)$ at a specified query $x^\\star$ and the training Root Mean Squared Error (RMSE)\n$$\n\\mathrm{RMSE}=\\sqrt{\\frac{1}{n}\\sum_{i=1}^n\\left(f_s(x_i)-y_i\\right)^2}.\n$$\nAll inputs $x$, outputs $y$, coefficients $\\mathbf{w}$, and predictions are dimensionless. Angles are not involved. You must round every reported floating-point value to $6$ decimal places.\n\nImplement a program that, for each provided test case, fits the polynomial surrogate of degree $d$ with regularization $\\lambda$, computes $f_s(x^\\star)$ and the training $\\mathrm{RMSE}$, and returns both values.\n\nTest Suite:\nFor each case below, use the given tuples $(\\{x_i\\},\\{y_i\\},d,\\lambda,x^\\star)$.\n\n- Case $1$ (exact quadratic fit):\n  - $\\{x_i\\}$:\n    $$\n    [-2.0,-1.0,0.0,1.0,2.0]\n    $$\n  - $\\{y_i\\}$, generated by $y=0.5x^2-x+2$:\n    $$\n    \\left[0.5\\cdot(-2.0)^2-(-2.0)+2,\\;0.5\\cdot(-1.0)^2-(-1.0)+2,\\;0.5\\cdot 0.0^2-0.0+2,\\;0.5\\cdot 1.0^2-1.0+2,\\;0.5\\cdot 2.0^2-2.0+2\\right]\n    $$\n    that is,\n    $$\n    [6.0,3.5,2.0,1.5,2.0]\n    $$\n  - Degree $d$:\n    $$\n    2\n    $$\n  - Regularization $\\lambda$:\n    $$\n    0.0\n    $$\n  - Query $x^\\star$:\n    $$\n    0.5\n    $$\n\n- Case $2$ (cubic with specified perturbations):\n  - $\\{x_i\\}$:\n    $$\n    [-2.0,-1.5,-1.0,-0.5,0.0,0.5,1.0,1.5,2.0]\n    $$\n  - $\\{y_i\\}$, specified as $y=x^3-2x+\\epsilon$ with fixed $\\epsilon$:\n    $$\n    \\epsilon=\\left[0.05,-0.04,0.02,0.01,0.0,-0.03,0.04,0.02,-0.05\\right]\n    $$\n    hence\n    $$\n    \\{y_i\\}=\\left[(-2.0)^3-2(-2.0)+0.05,\\;(-1.5)^3-2(-1.5)-0.04,\\;(-1.0)^3-2(-1.0)+0.02,\\;(-0.5)^3-2(-0.5)+0.01,\\;0.0^3-2(0.0)+0.0,\\;(0.5)^3-2(0.5)-0.03,\\;1.0^3-2(1.0)+0.04,\\;1.5^3-2(1.5)+0.02,\\;2.0^3-2(2.0)-0.05\\right]\n    $$\n  - Degree $d$:\n    $$\n    3\n    $$\n  - Regularization $\\lambda$:\n    $$\n    0.001\n    $$\n  - Query $x^\\star$:\n    $$\n    1.2\n    $$\n\n- Case $3$ (duplicated inputs with linear trend):\n  - $\\{x_i\\}$:\n    $$\n    [0.0,0.0,0.5,0.5,1.0,1.0]\n    $$\n  - $\\{y_i\\}$, specified as $y=0.2+1.5x+\\eta$ with fixed $\\eta$:\n    $$\n    \\eta=[0.0,0.01,-0.02,0.02,0.0,-0.01]\n    $$\n    hence\n    $$\n    \\{y_i\\}=[0.2,0.21,0.93,0.97,1.7,1.69]\n    $$\n  - Degree $d$:\n    $$\n    1\n    $$\n  - Regularization $\\lambda$:\n    $$\n    0.01\n    $$\n  - Query $x^\\star$:\n    $$\n    0.8\n    $$\n\n- Case $4$ (higher-order polynomial, near-interpolation):\n  - $\\{x_i\\}$:\n    $$\n    [-1.0,-0.6,-0.2,0.2,0.6,1.0]\n    $$\n  - $\\{y_i\\}$, specified exactly by\n    $$\n    y=1.0-0.5x+0.3x^2-0.1x^3+0.05x^4-0.02x^5\n    $$\n    evaluated at the given $\\{x_i\\}$.\n  - Degree $d$:\n    $$\n    5\n    $$\n  - Regularization $\\lambda$:\n    $$\n    10^{-8}\n    $$\n  - Query $x^\\star$:\n    $$\n    -0.9\n    $$\n\n- Case $5$ (single-point, constant model):\n  - $\\{x_i\\}$:\n    $$\n    [1.2]\n    $$\n  - $\\{y_i\\}$:\n    $$\n    [3.4]\n    $$\n  - Degree $d$:\n    $$\n    0\n    $$\n  - Regularization $\\lambda$:\n    $$\n    0.0\n    $$\n  - Query $x^\\star$:\n    $$\n    2.0\n    $$\n\nYour program should produce a single line of output containing the results across all cases as a list of lists, where each inner list contains the two rounded floats $[f_s(x^\\star),\\mathrm{RMSE}]$ for the corresponding case, in the same order as above. For example, the output format must be\n$$\n\\left[\\,[f_s(x^\\star_1),\\mathrm{RMSE}_1],[f_s(x^\\star_2),\\mathrm{RMSE}_2],\\dots\\,[f_s(x^\\star_5),\\mathrm{RMSE}_5]\\,\\right],\n$$\nwith every float rounded to $6$ decimal places and no additional text.",
            "solution": "The posed problem is a well-defined application of regularized linear regression for the purpose of constructing a surrogate model. All provided data and parameters are consistent and sufficient for deriving a unique and meaningful solution. The problem is scientifically and mathematically sound, grounded in the standard principles of numerical-functional approximation and machine learning. We will therefore proceed with a complete solution.\n\nThe core of the problem is to find a set of coefficients $\\mathbf{w} = [w_0, w_1, \\dots, w_d]^T$ for a polynomial surrogate model $f_s(x) = \\sum_{k=0}^d w_k x^k$ that best fits the given training data $\\{(x_i, y_i)\\}_{i=1}^n$. The criterion for \"best fit\" is the minimization of the Tikhonov-regularized sum of squared residuals, given by the objective function:\n$$\nJ(\\mathbf{w}) = \\sum_{i=1}^n \\left( f_s(x_i) - y_i \\right)^2 + \\lambda \\lVert \\mathbf{w} \\rVert_2^2\n$$\nHere, $\\lambda \\ge 0$ is the regularization parameter that penalizes large coefficient values, thereby preventing overfitting and improving the conditioning of the problem.\n\nTo solve this minimization problem, we first express it in matrix-vector notation. Let $\\mathbf{y} = [y_1, y_2, \\dots, y_n]^T$ be the vector of observed outputs. The model's predictions for all training inputs can be written as $\\hat{\\mathbf{y}} = \\mathbf{\\Phi}\\mathbf{w}$, where $\\mathbf{\\Phi}$ is the $n \\times (d+1)$ design matrix. Each row of $\\mathbf{\\Phi}$ corresponds to a data point $x_i$, and each column corresponds to a basis function $\\phi_k(x) = x^k$. Thus, the element at row $i$ and column $j$ (using $0$-based indexing, $j=k$) is $\\mathbf{\\Phi}_{ij} = x_i^j$.\n$$\n\\mathbf{\\Phi} = \\begin{pmatrix}\n1 & x_1 & x_1^2 & \\dots & x_1^d \\\\\n1 & x_2 & x_2^2 & \\dots & x_2^d \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_n & x_n^2 & \\dots & x_n^d\n\\end{pmatrix}\n$$\nThe objective function $J(\\mathbf{w})$ can now be written in terms of matrices and vectors:\n$$\nJ(\\mathbf{w}) = (\\mathbf{\\Phi}\\mathbf{w} - \\mathbf{y})^T (\\mathbf{\\Phi}\\mathbf{w} - \\mathbf{y}) + \\lambda \\mathbf{w}^T \\mathbf{w}\n$$\nThis is a quadratic function of $\\mathbf{w}$. To find the minimum, we compute the gradient of $J(\\mathbf{w})$ with respect to $\\mathbf{w}$ and set it to the zero vector.\n$$\n\\nabla_{\\mathbf{w}} J(\\mathbf{w}) = \\nabla_{\\mathbf{w}} \\left( \\mathbf{w}^T \\mathbf{\\Phi}^T \\mathbf{\\Phi} \\mathbf{w} - 2\\mathbf{y}^T \\mathbf{\\Phi} \\mathbf{w} + \\mathbf{y}^T \\mathbf{y} + \\lambda \\mathbf{w}^T \\mathbf{I} \\mathbf{w} \\right)\n$$\nwhere $\\mathbf{I}$ is the $(d+1) \\times (d+1)$ identity matrix. Applying standard matrix calculus rules ($\\nabla_{\\mathbf{z}} (\\mathbf{z}^T \\mathbf{A} \\mathbf{z}) = 2\\mathbf{A}\\mathbf{z}$ for symmetric $\\mathbf{A}$, and $\\nabla_{\\mathbf{z}} (\\mathbf{b}^T \\mathbf{z}) = \\mathbf{b}$), we obtain:\n$$\n\\nabla_{\\mathbf{w}} J(\\mathbf{w}) = 2\\mathbf{\\Phi}^T \\mathbf{\\Phi} \\mathbf{w} - 2\\mathbf{\\Phi}^T \\mathbf{y} + 2\\lambda \\mathbf{I} \\mathbf{w}\n$$\nSetting the gradient to zero yields the normal equations for ridge regression:\n$$\n2\\mathbf{\\Phi}^T \\mathbf{\\Phi} \\mathbf{w} - 2\\mathbf{\\Phi}^T \\mathbf{y} + 2\\lambda \\mathbf{I} \\mathbf{w} = \\mathbf{0}\n$$\n$$\n(\\mathbf{\\Phi}^T \\mathbf{\\Phi} + \\lambda \\mathbf{I}) \\mathbf{w} = \\mathbf{\\Phi}^T \\mathbf{y}\n$$\nThe optimal coefficient vector $\\mathbf{w}$ is the solution to this linear system of equations. The matrix $(\\mathbf{\\Phi}^T \\mathbf{\\Phi} + \\lambda \\mathbf{I})$ is guaranteed to be invertible for any $\\lambda > 0$, ensuring a unique solution. For $\\lambda = 0$, a unique solution exists if $\\mathbf{\\Phi}$ has full column rank, which is generally true if the number of unique data points $n$ is greater than or equal to the number of coefficients $d+1$. We can therefore solve for $\\mathbf{w}$:\n$$\n\\mathbf{w}^{\\star} = (\\mathbf{\\Phi}^T \\mathbf{\\Phi} + \\lambda \\mathbf{I})^{-1} \\mathbf{\\Phi}^T \\mathbf{y}\n$$\nNumerically, it is preferable to solve the linear system directly rather than computing the matrix inverse.\n\nOnce the optimal coefficients $\\mathbf{w}^{\\star}$ are determined, we can perform the required calculations:\n1.  **Prediction**: The predicted value at a new query point $x^\\star$ is calculated by evaluating the surrogate model:\n    $$\n    f_s(x^\\star) = \\sum_{k=0}^d w^{\\star}_k (x^\\star)^k = \\mathbf{\\phi}(x^\\star)^T \\mathbf{w}^{\\star}\n    $$\n    where $\\mathbf{\\phi}(x^\\star) = [1, x^\\star, (x^\\star)^2, \\dots, (x^\\star)^d]^T$ is the vector of basis functions evaluated at $x^\\star$.\n\n2.  **Training Root Mean Squared Error (RMSE)**: This metric quantifies the average magnitude of the error on the training data. First, we compute the model's predictions for all training points, $\\hat{\\mathbf{y}} = \\mathbf{\\Phi} \\mathbf{w}^{\\star}$. The RMSE is then given by:\n    $$\n    \\mathrm{RMSE} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n (\\hat{y}_i - y_i)^2} = \\sqrt{\\frac{1}{n} (\\mathbf{\\Phi}\\mathbf{w}^{\\star} - \\mathbf{y})^T (\\mathbf{\\Phi}\\mathbf{w}^{\\star} - \\mathbf{y})}\n    $$\n\nThe algorithm to be implemented will follow these steps for each test case: a) construct the design matrix $\\mathbf{\\Phi}$ and vector $\\mathbf{y}$ from the input data; b) solve the linear system $(\\mathbf{\\Phi}^T \\mathbf{\\Phi} + \\lambda \\mathbf{I}) \\mathbf{w} = \\mathbf{\\Phi}^T \\mathbf{y}$ to find $\\mathbf{w}$; c) compute the prediction $f_s(x^\\star)$ at the query point; d) compute the training RMSE; e) round and report the results.",
            "answer": "```python\nimport numpy as np\n\ndef fit_and_evaluate(x_train, y_train, d, lambd, x_query):\n    \"\"\"\n    Constructs and evaluates a polynomial surrogate model using Tikhonov regularization.\n\n    Args:\n        x_train (list or np.ndarray): The scalar input training data {x_i}.\n        y_train (list or np.ndarray): The scalar output training data {y_i}.\n        d (int): The degree of the polynomial basis.\n        lambd (float): The Tikhonov regularization parameter.\n        x_query (float): The query point at which to predict the output.\n\n    Returns:\n        list: A list containing [predicted_value, training_rmse], rounded to 6 decimal places.\n    \"\"\"\n    # Ensure inputs are numpy arrays for vectorized operations.\n    x_train = np.array(x_train)\n    y_train = np.array(y_train)\n    \n    n = len(x_train)\n    num_coeffs = d + 1\n\n    # Construct the design matrix Phi using a Vandermonde matrix.\n    # The basis is {x^0, x^1, ..., x^d}, which corresponds to increasing=True.\n    Phi = np.vander(x_train, N=num_coeffs, increasing=True)\n\n    # Solve the normal equations for ridge regression to find the coefficients w.\n    # (Phi^T * Phi + lambda * I) w = Phi^T * y\n    A = Phi.T @ Phi + lambd * np.identity(num_coeffs)\n    b = Phi.T @ y_train\n    w = np.linalg.solve(A, b)\n\n    # Calculate the prediction at the query point x_query.\n    # Construct the basis vector for the query point.\n    phi_query = np.array([x_query**k for k in range(num_coeffs)])\n    prediction = phi_query @ w\n\n    # Calculate the training Root Mean Squared Error (RMSE).\n    # First, get the model's predictions on the training data.\n    y_pred_train = Phi @ w\n    # Calculate the sum of squared errors (SSE) and then the RMSE.\n    sse = np.sum((y_pred_train - y_train)**2)\n    rmse = np.sqrt(sse / n)\n\n    # Round the final values to 6 decimal places as required.\n    prediction_rounded = round(prediction, 6)\n    rmse_rounded = round(rmse, 6)\n    \n    return [prediction_rounded, rmse_rounded]\n\ndef solve():\n    \"\"\"\n    Main function to process all test cases and print the final output.\n    \"\"\"\n    # Case 1: Exact quadratic fit, no regularization.\n    case1 = {\n        \"x\": [-2.0, -1.0, 0.0, 1.0, 2.0],\n        \"y\": [6.0, 3.5, 2.0, 1.5, 2.0],\n        \"d\": 2, \"lambda\": 0.0, \"x_star\": 0.5\n    }\n\n    # Case 2: Cubic fit with perturbations and regularization.\n    x2 = [-2.0, -1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0]\n    eps2 = [0.05, -0.04, 0.02, 0.01, 0.0, -0.03, 0.04, 0.02, -0.05]\n    y2 = [xi**3 - 2*xi + e for xi, e in zip(x2, eps2)]\n    case2 = {\n        \"x\": x2, \"y\": y2, \"d\": 3, \"lambda\": 0.001, \"x_star\": 1.2\n    }\n\n    # Case 3: Linear fit with duplicated inputs and regularization.\n    case3 = {\n        \"x\": [0.0, 0.0, 0.5, 0.5, 1.0, 1.0],\n        \"y\": [0.2, 0.21, 0.93, 0.97, 1.7, 1.69],\n        \"d\": 1, \"lambda\": 0.01, \"x_star\": 0.8\n    }\n\n    # Case 4: Higher-order polynomial fit, near-interpolation.\n    x4 = [-1.0, -0.6, -0.2, 0.2, 0.6, 1.0]\n    y4_func = lambda x: 1.0 - 0.5*x + 0.3*x**2 - 0.1*x**3 + 0.05*x**4 - 0.02*x**5\n    y4 = [y4_func(xi) for xi in x4]\n    case4 = {\n        \"x\": x4, \"y\": y4, \"d\": 5, \"lambda\": 1e-8, \"x_star\": -0.9\n    }\n\n    # Case 5: Single-point, constant model.\n    case5 = {\n        \"x\": [1.2], \"y\": [3.4], \"d\": 0, \"lambda\": 0.0, \"x_star\": 2.0\n    }\n\n    test_cases = [case1, case2, case3, case4, case5]\n\n    results_str = []\n    for case in test_cases:\n        result = fit_and_evaluate(case[\"x\"], case[\"y\"], case[\"d\"], case[\"lambda\"], case[\"x_star\"])\n        # Format each result pair into \"[val1,val2]\" string format\n        results_str.append(f\"[{result[0]},{result[1]}]\")\n\n    # Print the final result in the exact required format: [[...],[...],...]\n    print(f\"[{','.join(results_str)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Once a surrogate model is constructed, its primary value often lies in analysis and optimization. This exercise moves from simply fitting a model to using it for a key application in biomedical systems: response surface analysis. You will fit a quadratic response surface to simulated dose-response data and then use principles of multivariate calculus to locate and classify the model's stationary point . By analyzing the Hessian matrix and its eigenvalues, you will determine whether this point represents a maximum response, a minimum, or a saddle point, providing direct insight into optimal system conditions.",
            "id": "3933554",
            "problem": "A dose–response experiment in biomedical systems modeling measures a scalar response as a function of two controllable inputs: drug dose and incubation time. Consider a quadratic surrogate model (a quadratic response surface) for the response, where the inputs are dose and time. Let $x_1$ denote dose in micromoles per liter and let $x_2$ denote time in hours. The modeled response is a scalar function $r(x_1,x_2)$ that will be approximated by a quadratic polynomial whose coefficients are determined from data via Ordinary Least Squares (OLS). The experiment provides the following data sets (each data set is a list of triplets $(x_1,x_2,r)$). The model, the fitted stationary point, and the classification must be expressed entirely in mathematical terms without assuming external domain knowledge beyond the given definitions.\n\nYour tasks, for each data set, are to:\n- Fit a quadratic response surface $r(x_1,x_2)$ from the data using OLS by minimizing the sum of squared residuals between measurements and the quadratic model.\n- Compute the stationary point $(x_1^\\star,x_2^\\star)$ of the fitted quadratic model by solving for where the gradient equals zero.\n- Compute the Hessian matrix $H$ of the fitted quadratic model and classify the stationary point based on the eigenvalues of $H$ using the standard second-order test from multivariate calculus. Use the following classification rule with a numerical tolerance $\\tau = 10^{-5}$: if any eigenvalue of $H$ has absolute value strictly less than $\\tau$, return the code $2$ (indeterminate due to near-singular curvature); else if all eigenvalues are positive, return the code $-1$ (minimum); else if all eigenvalues are negative, return the code $1$ (maximum); otherwise, return the code $0$ (saddle).\n- The final outputs are strictly the classification codes, not the coordinates of the stationary points. No physical units are required in the final answers.\n\nUse the following test suite, which consists of four data sets. Each set lists $(x_1,x_2,r)$ values, where $x_1$ and $x_2$ are input settings and $r$ is the measured response:\n- Data Set $1$ (well-conditioned minimum case): \n  $(x_1,x_2,r)$ equals \n  (0, 0, 3.0), \n  (0, 1, 5.8), \n  (0, 2, 10.2), \n  (1, 0, 4.5), \n  (1, 1, 7.4), \n  (1, 2, 11.9), \n  (2, 0, 7.0), \n  (2, 1, 10.0), \n  (2, 2, 14.6).\n- Data Set $2$ (well-conditioned maximum case): \n  $(x_1,x_2,r)$ equals \n  (0, 0, 2.0), \n  (0, 1, 2.0), \n  (0, 2, 1.0), \n  (1, 0, -0.7), \n  (1, 1, -0.9), \n  (1, 2, -2.1), \n  (2, 0, -5.8), \n  (2, 1, -6.2), \n  (2, 2, -7.6).\n- Data Set $3$ (saddle case): \n  $(x_1,x_2,r)$ equals \n  (0, 0, 1.0), \n  (0, 1, 0.0), \n  (0, 2, -2.6), \n  (1, 0, 1.6), \n  (1, 1, 0.6), \n  (1, 2, -2.0), \n  (2, 0, 3.2), \n  (2, 1, 2.2), \n  (2, 2, -0.4).\n- Data Set $4$ (near-singular curvature along dose): \n  $(x_1,x_2,r)$ equals \n  (0, 0, 1.0), \n  (0, 1, 2.0), \n  (0, 2, 5.0), \n  (1, 0, 1.000001), \n  (1, 1, 2.000001), \n  (1, 2, 5.000001), \n  (2, 0, 1.000004), \n  (2, 1, 2.000004), \n  (2, 2, 5.000004).\n\nModeling assumptions and requirements:\n- The quadratic surrogate model must be of the form $r(x_1,x_2) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1^2 + \\beta_4 x_1 x_2 + \\beta_5 x_2^2$, where the coefficients $\\beta_0,\\beta_1,\\beta_2,\\beta_3,\\beta_4,\\beta_5$ are determined by minimizing the sum of squared residuals over the given data set.\n- The stationary point $(x_1^\\star,x_2^\\star)$ must satisfy that the gradient of $r$ equals zero at $(x_1^\\star,x_2^\\star)$, and the Hessian matrix $H$ of $r$ must be used to classify the nature of the stationary point by examining the eigenvalues of $H$ with the tolerance $\\tau = 10^{-5}$.\n\nYour program should produce a single line of output containing the classification codes for the four data sets as a comma-separated list enclosed in square brackets, in the order Data Set $1$, Data Set $2$, Data Set $3$, Data Set $4$ (for example, $\\left[ \\text{code}_1,\\text{code}_2,\\text{code}_3,\\text{code}_4 \\right]$). Each code must be one of the integers $-1$, $0$, $1$, or $2$ as specified above. No other output is permitted.",
            "solution": "The problem requires us to analyze a quadratic response surface model for four distinct data sets. For each data set, we must fit the model, find its stationary point, and classify the point's nature (minimum, maximum, saddle point, or indeterminate). The process involves applying Ordinary Least Squares (OLS), followed by standard second-derivative tests from multivariate calculus.\n\n### Step 1: Model Formulation and Ordinary Least Squares (OLS)\n\nThe a priori quadratic surrogate model for the response $r$ as a function of two inputs, dose $x_1$ and time $x_2$, is given by:\n$$r(x_1, x_2) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1^2 + \\beta_4 x_1 x_2 + \\beta_5 x_2^2$$\nThis model is linear with respect to its coefficients $\\boldsymbol{\\beta} = [\\beta_0, \\beta_1, \\beta_2, \\beta_3, \\beta_4, \\beta_5]^T$. Given a set of $N$ data points $(x_{1i}, x_{2i}, r_i)$, we can express the relationship for all points in matrix form:\n$$ \\mathbf{r} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon} $$\nwhere $\\mathbf{r}$ is an $N \\times 1$ vector of observed responses, $\\mathbf{X}$ is the $N \\times 6$ design matrix, $\\boldsymbol{\\beta}$ is the $6 \\times 1$ vector of coefficients, and $\\boldsymbol{\\epsilon}$ is the vector of residuals. The $i$-th row of the design matrix $\\mathbf{X}$ corresponds to the $i$-th data point and is constructed as:\n$$ \\mathbf{X}_i = [1, x_{1i}, x_{2i}, x_{1i}^2, x_{1i}x_{2i}, x_{2i}^2] $$\nThe OLS method finds the coefficient vector $\\hat{\\boldsymbol{\\beta}}$ that minimizes the sum of squared residuals, $S = \\boldsymbol{\\epsilon}^T\\boldsymbol{\\epsilon} = (\\mathbf{r} - \\mathbf{X}\\boldsymbol{\\beta})^T(\\mathbf{r} - \\mathbf{X}\\boldsymbol{\\beta})$. The solution to this minimization problem is given by the normal equations:\n$$ (\\mathbf{X}^T\\mathbf{X})\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}^T\\mathbf{r} $$\nAssuming the matrix $\\mathbf{X}^T\\mathbf{X}$ is invertible, the unique OLS estimate for the coefficients is:\n$$ \\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{r} $$\nThis computation is performed for each of the four data sets to obtain a specific fitted model $\\hat{r}(x_1, x_2)$.\n\n### Step 2: Finding the Stationary Point\n\nA stationary point $(x_1^\\star, x_2^\\star)$ of the fitted response surface $\\hat{r}(x_1, x_2)$ is a point where the gradient of the function is the zero vector:\n$$ \\nabla \\hat{r}(x_1^\\star, x_2^\\star) = \\mathbf{0} $$\nThe gradient vector is:\n$$ \\nabla \\hat{r} = \\begin{bmatrix} \\frac{\\partial \\hat{r}}{\\partial x_1} \\\\ \\frac{\\partial \\hat{r}}{\\partial x_2} \\end{bmatrix} = \\begin{bmatrix} \\hat{\\beta}_1 + 2\\hat{\\beta}_3 x_1 + \\hat{\\beta}_4 x_2 \\\\ \\hat{\\beta}_2 + \\hat{\\beta}_4 x_1 + 2\\hat{\\beta}_5 x_2 \\end{bmatrix} $$\nSetting the gradient to zero yields a system of two linear equations in $x_1^\\star$ and $x_2^\\star$:\n$$ 2\\hat{\\beta}_3 x_1^\\star + \\hat{\\beta}_4 x_2^\\star = -\\hat{\\beta}_1 $$\n$$ \\hat{\\beta}_4 x_1^\\star + 2\\hat{\\beta}_5 x_2^\\star = -\\hat{\\beta}_2 $$\nThis system can be written in matrix form:\n$$ \\begin{bmatrix} 2\\hat{\\beta}_3 & \\hat{\\beta}_4 \\\\ \\hat{\\beta}_4 & 2\\hat{\\beta}_5 \\end{bmatrix} \\begin{bmatrix} x_1^\\star \\\\ x_2^\\star \\end{bmatrix} = \\begin{bmatrix} -\\hat{\\beta}_1 \\\\ -\\hat{\\beta}_2 \\end{bmatrix} $$\n\n### Step 3: Classifying the Stationary Point using the Hessian Matrix\n\nThe nature of the stationary point is determined by the second-order partial derivatives, which are arranged in the Hessian matrix $H$. For the quadratic model, the Hessian is a constant matrix:\n$$ H = \\begin{bmatrix} \\frac{\\partial^2 \\hat{r}}{\\partial x_1^2} & \\frac{\\partial^2 \\hat{r}}{\\partial x_1 \\partial x_2} \\\\ \\frac{\\partial^2 \\hat{r}}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 \\hat{r}}{\\partial x_2^2} \\end{bmatrix} = \\begin{bmatrix} 2\\hat{\\beta}_3 & \\hat{\\beta}_4 \\\\ \\hat{\\beta}_4 & 2\\hat{\\beta}_5 \\end{bmatrix} $$\nNotice that this is the same matrix that appears in the linear system for the stationary point. Classification is based on the signs of the eigenvalues, $\\lambda_1$ and $\\lambda_2$, of $H$. The problem specifies the following classification rule with a numerical tolerance $\\tau = 10^{-5}$:\n\n- **Indeterminate (Code $2$)**: If $|\\lambda_1| < \\tau$ or $|\\lambda_2| < \\tau$. This indicates that the curvature is nearly flat in at least one direction, making the classification numerically unstable or the stationary point part of a \"trough\" or \"ridge\" rather than an isolated point.\n- **Local Minimum (Code $-1$)**: If both eigenvalues are positive (i.e., $\\lambda_1 > 0$ and $\\lambda_2 > 0$) and do not trigger the indeterminate case. This corresponds to a surface that is concave up in all directions.\n- **Local Maximum (Code $1$)**: If both eigenvalues are negative (i.e., $\\lambda_1 < 0$ and $\\lambda_2 < 0$) and do not trigger the indeterminate case. This corresponds to a surface that is concave down in all directions.\n- **Saddle Point (Code $0$)**: Otherwise. This occurs when the eigenvalues have opposite signs ($\\lambda_1 \\lambda_2 < 0$) and do not trigger the indeterminate case. The surface is concave up in one direction and concave down in another.\n\nThe following procedure is implemented for each data set:\n1. Construct the design matrix $\\mathbf{X}$ and the response vector $\\mathbf{r}$ from the given $(x_1, x_2, r)$ triplets.\n2. Compute the coefficient vector $\\hat{\\boldsymbol{\\beta}}$ using a numerical OLS solver, equivalent to calculating $\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{r}$.\n3. Form the Hessian matrix $H$ using the estimated coefficients $\\hat{\\beta}_3$, $\\hat{\\beta}_4$, and $\\hat{\\beta}_5$.\n4. Compute the eigenvalues of the symmetric matrix $H$.\n5. Apply the specified classification logic to determine the integer code.\n\nThis process is repeated for all four data sets to generate the final list of classification codes.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Fits a quadratic response surface to data sets and classifies the stationary points.\n    \"\"\"\n    \n    # Test suite containing the four data sets.\n    # Each data set is a list of (x1, x2, r) tuples.\n    test_suite = [\n        # Data Set 1 (well-conditioned minimum case)\n        np.array([\n            [0, 0, 3.0], [0, 1, 5.8], [0, 2, 10.2],\n            [1, 0, 4.5], [1, 1, 7.4], [1, 2, 11.9],\n            [2, 0, 7.0], [2, 1, 10.0], [2, 2, 14.6]\n        ]),\n        # Data Set 2 (well-conditioned maximum case)\n        np.array([\n            [0, 0, 2.0], [0, 1, 2.0], [0, 2, 1.0],\n            [1, 0, -0.7], [1, 1, -0.9], [1, 2, -2.1],\n            [2, 0, -5.8], [2, 1, -6.2], [2, 2, -7.6]\n        ]),\n        # Data Set 3 (saddle case)\n        np.array([\n            [0, 0, 1.0], [0, 1, 0.0], [0, 2, -2.6],\n            [1, 0, 1.6], [1, 1, 0.6], [1, 2, -2.0],\n            [2, 0, 3.2], [2, 1, 2.2], [2, 2, -0.4]\n        ]),\n        # Data Set 4 (near-singular curvature along dose)\n        np.array([\n            [0, 0, 1.0], [0, 1, 2.0], [0, 2, 5.0],\n            [1, 0, 1.000001], [1, 1, 2.000001], [1, 2, 5.000001],\n            [2, 0, 1.000004], [2, 1, 2.000004], [2, 2, 5.000004]\n        ])\n    ]\n\n    results = []\n    tau = 1e-5\n\n    for data_set in test_suite:\n        # Extract inputs (x1, x2) and responses (r)\n        x1 = data_set[:, 0]\n        x2 = data_set[:, 1]\n        r = data_set[:, 2]\n\n        # Construct the design matrix X for the model:\n        # r = b0 + b1*x1 + b2*x2 + b3*x1^2 + b4*x1*x2 + b5*x2^2\n        X = np.c_[np.ones(x1.shape[0]), x1, x2, x1**2, x1*x2, x2**2]\n\n        # Solve for the coefficients beta using Ordinary Least Squares (OLS)\n        beta = np.linalg.lstsq(X, r, rcond=None)[0]\n        b3, b4, b5 = beta[3], beta[4], beta[5]\n\n        # Construct the Hessian matrix H\n        # H = [[2*b3, b4],\n        #      [b4, 2*b5]]\n        H = np.array([[2 * b3, b4], [b4, 2 * b5]])\n\n        # Compute the eigenvalues of the Hessian\n        # eigvalsh is used for real symmetric matrices\n        eigenvalues = np.linalg.eigvalsh(H)\n        \n        # Classify the stationary point based on the eigenvalues\n        classification_code = 0\n        if np.any(np.abs(eigenvalues) < tau):\n            classification_code = 2  # Indeterminate\n        elif np.all(eigenvalues > 0):\n            classification_code = -1 # Minimum\n        elif np.all(eigenvalues < 0):\n            classification_code = 1  # Maximum\n        else:\n            classification_code = 0  # Saddle\n\n        results.append(classification_code)\n    \n    # Print the results in the required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "While polynomial models are powerful, many complex systems require more flexible, non-parametric approaches. This advanced practice introduces Gaussian Process (GP) regression, a cornerstone of modern surrogate modeling that treats the unknown function as a random draw from a distribution of functions. A key advantage of GPs, which you will explore here, is their intrinsic ability to provide a principled measure of predictive uncertainty . You will implement the core GP prediction equations to compute not only the most likely output but also the model's confidence in that prediction, known as epistemic variance, which is essential for risk-aware decision-making and experimental design.",
            "id": "3933600",
            "problem": "You are asked to implement a universal, purely mathematical program that computes the predictive mean and the epistemic variance of a Gaussian Process (GP) surrogate model for a one-dimensional biomedical response surface. Begin from first principles: a Gaussian Process (GP) is defined as a collection of random variables, any finite number of which have a joint Gaussian distribution. The fundamental base for this problem is the combination of the definitions of a Gaussian Process prior and a Gaussian likelihood model, together with the conditioning rules of a multivariate Gaussian distribution. Specifically, assume a latent function $f$ with a zero-mean Gaussian Process prior $f \\sim \\mathcal{GP}(0, k)$, and noisy observations $y_i$ produced by the model $y_i = f(x_i) + \\epsilon_i$, where $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma_n^2)$ are independent and identically distributed. The covariance is given by a stationary, positive-definite kernel $k(\\cdot, \\cdot)$.\n\nUse the squared-exponential (also called radial basis function) kernel with hyperparameters signal variance $\\sigma_f^2$ and length scale $l$ defined by\n$$\nk(x, x') = \\sigma_f^2 \\exp\\left(-\\frac{(x - x')^2}{2 l^2}\\right).\n$$\nAll quantities are dimensionless. Angles are not used. No percentages are used.\n\nYour program must, for each test case, compute the predictive mean and epistemic variance (that is, the posterior variance of the latent function $f$ at the test input) at a single new point $x^\\star$, based on training inputs $X$ and outputs $y$. The epistemic variance is the variance of $f(x^\\star)$ under the posterior and does not include the observation noise variance $\\sigma_n^2$. Derive the algorithm starting from the joint Gaussian distribution implied by the GP prior and the Gaussian likelihood, and by applying the conditioning rules of multivariate Gaussian distributions. For numerical stability, your implementation must add a small positive diagonal jitter $\\delta$ to the training covariance matrix before factorization, with $\\delta$ chosen as $\\delta = 10^{-10}$.\n\nTo instantiate the biomedical context in a scientifically realistic and self-consistent way, the training outputs $y$ are generated deterministically from a normalized dose-response curve representative of receptor-ligand binding. Use the logistic-sigmoid form\n$$\nf(x) = \\frac{1}{1 + \\exp\\left(-a (x - b)\\right)},\n$$\nwith steepness parameter $a = 8$ and midpoint $b = 0.5$. For any given training set $X$, define $y_i = f(x_i)$ unless otherwise specified.\n\nImplement the following four test cases that exercise the algorithm across a range of conditions. In all cases, the quantities are dimensionless:\n\n- Test case $1$ (happy path, moderate noise, interpolation):\n  - Training inputs $X = [0.0, 0.25, 0.5, 0.75, 1.0]$.\n  - Training outputs $y_i = f(x_i)$.\n  - Hyperparameters: length scale $l = 0.2$, signal variance $\\sigma_f = 1.0$, observation noise standard deviation $\\sigma_n = 0.1$.\n  - Query input $x^\\star = 0.6$.\n\n- Test case $2$ (boundary condition, zero noise, exact training input):\n  - Training inputs $X = [0.0, 0.25, 0.5, 0.75, 1.0]$.\n  - Training outputs $y_i = f(x_i)$.\n  - Hyperparameters: length scale $l = 0.3$, signal variance $\\sigma_f = 1.0$, observation noise standard deviation $\\sigma_n = 0.0$.\n  - Query input $x^\\star = 0.5$.\n\n- Test case $3$ (edge case, extrapolation far outside training domain):\n  - Training inputs $X = [0.0, 0.25, 0.5, 0.75, 1.0]$.\n  - Training outputs $y_i = f(x_i)$.\n  - Hyperparameters: length scale $l = 0.2$, signal variance $\\sigma_f = 1.0$, observation noise standard deviation $\\sigma_n = 0.1$.\n  - Query input $x^\\star = 2.0$.\n\n- Test case $4$ (edge case, duplicate training inputs with small noise):\n  - Training inputs $X = [0.0, 0.25, 0.5, 0.5, 0.75, 1.0]$.\n  - Training outputs aligned elementwise with $X$ as follows:\n    - For $x = 0.0$, $y = f(0.0)$.\n    - For $x = 0.25$, $y = f(0.25)$.\n    - For the third entry $x = 0.5$, use $y = f(0.5) - 0.02$.\n    - For the fourth entry $x = 0.5$, use $y = f(0.5) + 0.02$.\n    - For $x = 0.75$, $y = f(0.75)$.\n    - For $x = 1.0$, $y = f(1.0)$.\n  - Hyperparameters: length scale $l = 0.15$, signal variance $\\sigma_f = 1.0$, observation noise standard deviation $\\sigma_n = 0.05$.\n  - Query input $x^\\star = 0.5$.\n\nYour program must, for each test case, output the predictive mean $\\mu(x^\\star)$ and the epistemic variance $\\sigma^2(x^\\star)$ as floating-point numbers. Aggregate the results for the four test cases into a single line of output containing eight numbers in the order $\\left[\\mu_1, \\sigma_1^2, \\mu_2, \\sigma_2^2, \\mu_3, \\sigma_3^2, \\mu_4, \\sigma_4^2\\right]$. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[\\text{result1},\\text{result2},\\text{result3}]$). All quantities are dimensionless. Express all numbers as decimal floating-point values.",
            "solution": "The problem requires the implementation of Gaussian Process (GP) regression to compute the predictive posterior mean and epistemic variance for a latent function $f$ at a new input point $x^\\star$. The derivation proceeds from the fundamental definition of a GP and the rules for conditioning multivariate Gaussian distributions.\n\n**1. Model Specification**\n\nA Gaussian Process defines a prior distribution over functions. We assume the latent function $f(x)$ follows a GP with a zero mean and a covariance function (kernel) $k(x, x')$. This is denoted as:\n$$\nf(x) \\sim \\mathcal{GP}(0, k(x, x'))\n$$\nThe problem specifies the squared-exponential kernel:\n$$\nk(x, x') = \\sigma_f^2 \\exp\\left(-\\frac{(x - x')^2}{2 l^2}\\right)\n$$\nwhere $\\sigma_f^2$ is the signal variance and $l$ is the length scale.\n\nWe are given a set of $N$ training inputs $X = \\{x_1, \\dots, x_N\\}$ with corresponding noisy observations $\\mathbf{y} = [y_1, \\dots, y_N]^T$. The observation model is:\n$$\ny_i = f(x_i) + \\epsilon_i, \\quad \\text{where} \\quad \\epsilon_i \\sim \\mathcal{N}(0, \\sigma_n^2)\n$$\nThe noise terms $\\epsilon_i$ are assumed to be independent and identically distributed.\n\n**2. Joint Prior Distribution**\n\nBy the definition of a GP, any finite set of function values has a joint Gaussian distribution. Let $\\mathbf{f}$ be the vector of latent function values at the training inputs, $\\mathbf{f} = [f(x_1), \\dots, f(x_N)]^T$, and $f^\\star$ be the latent function value at a new test input $x^\\star$. Their joint distribution under the prior is:\n$$\n\\begin{pmatrix} \\mathbf{f} \\\\ f^\\star \\end{pmatrix} \\sim \\mathcal{N} \\left( \\mathbf{0}, \\begin{pmatrix} K(X, X) & K(X, x^\\star) \\\\ K(x^\\star, X) & k(x^\\star, x^\\star) \\end{pmatrix} \\right)\n$$\nwhere:\n- $K(X, X)$ is the $N \\times N$ covariance matrix of training inputs, with entries $[K(X, X)]_{ij} = k(x_i, x_j)$.\n- $K(X, x^\\star)$ is the $N \\times 1$ vector of covariances between training inputs and the test input, with entries $[K(X, x^\\star)]_i = k(x_i, x^\\star)$.\n- $K(x^\\star, X) = K(X, x^\\star)^T$.\n- $k(x^\\star, x^\\star)$ is the prior variance of the function at the test input, which is $\\sigma_f^2$ for the squared-exponential kernel.\n\n**3. Joint Distribution of Observations and Test Value**\n\nThe vector of observations $\\mathbf{y}$ is related to the latent function values $\\mathbf{f}$ by $\\mathbf{y} = \\mathbf{f} + \\boldsymbol{\\epsilon}$, where $\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma_n^2 I)$. We need the joint distribution of the observed data $\\mathbf{y}$ and the latent value $f^\\star$. Since they are formed by linear operations on Gaussian variables, they are also jointly Gaussian.\n\nThe mean vector is zero:\n$$\nE\\left[\\begin{pmatrix} \\mathbf{y} \\\\ f^\\star \\end{pmatrix}\\right] = \\begin{pmatrix} E[\\mathbf{f} + \\boldsymbol{\\epsilon}] \\\\ E[f^\\star] \\end{pmatrix} = \\begin{pmatrix} \\mathbf{0} \\\\ 0 \\end{pmatrix}\n$$\nThe covariance matrix is:\n$$\n\\text{Cov}\\left(\\begin{pmatrix} \\mathbf{y} \\\\ f^\\star \\end{pmatrix}\\right) = E\\left[ \\begin{pmatrix} \\mathbf{y} \\\\ f^\\star \\end{pmatrix} \\begin{pmatrix} \\mathbf{y}^T & (f^\\star)^T \\end{pmatrix} \\right] = \\begin{pmatrix} E[\\mathbf{y}\\mathbf{y}^T] & E[\\mathbf{y}(f^\\star)^T] \\\\ E[f^\\star \\mathbf{y}^T] & E[f^\\star(f^\\star)^T] \\end{pmatrix}\n$$\nThe blocks of this covariance matrix are:\n- $\\text{Cov}(\\mathbf{y}, \\mathbf{y}) = E[(\\mathbf{f}+\\boldsymbol{\\epsilon})(\\mathbf{f}+\\boldsymbol{\\epsilon})^T] = E[\\mathbf{f}\\mathbf{f}^T] + E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] = K(X, X) + \\sigma_n^2 I$.\n- $\\text{Cov}(\\mathbf{y}, f^\\star) = E[(\\mathbf{f}+\\boldsymbol{\\epsilon})(f^\\star)^T] = E[\\mathbf{f}(f^\\star)^T] + E[\\boldsymbol{\\epsilon}(f^\\star)^T] = K(X, x^\\star)$.\n- $\\text{Cov}(f^\\star, f^\\star) = k(x^\\star, x^\\star)$.\n\nThus, the joint distribution is:\n$$\n\\begin{pmatrix} \\mathbf{y} \\\\ f^\\star \\end{pmatrix} \\sim \\mathcal{N} \\left( \\mathbf{0}, \\begin{pmatrix} K(X, X) + \\sigma_n^2 I & K(X, x^\\star) \\\\ K(x^\\star, X) & k(x^\\star, x^\\star) \\end{pmatrix} \\right)\n$$\n\n**4. Posterior Distribution (Conditioning)**\n\nOur goal is to find the posterior distribution of the latent function value, $p(f^\\star | X, \\mathbf{y}, x^\\star)$, conditioned on the observed data. For a general multivariate Gaussian partition $\\begin{pmatrix} \\mathbf{a} \\\\ \\mathbf{b} \\end{pmatrix} \\sim \\mathcal{N} \\left( \\begin{pmatrix} \\boldsymbol{\\mu}_a \\\\ \\boldsymbol{\\mu}_b \\end{pmatrix}, \\begin{pmatrix} \\Sigma_{aa} & \\Sigma_{ab} \\\\ \\Sigma_{ba} & \\Sigma_{bb} \\end{pmatrix} \\right)$, the conditional distribution $p(\\mathbf{b}|\\mathbf{a})$ is $\\mathcal{N}(\\boldsymbol{\\mu}_{b|a}, \\Sigma_{b|a})$, with:\n$$\n\\boldsymbol{\\mu}_{b|a} = \\boldsymbol{\\mu}_b + \\Sigma_{ba} \\Sigma_{aa}^{-1} (\\mathbf{a} - \\boldsymbol{\\mu}_a)\n$$\n$$\n\\Sigma_{b|a} = \\Sigma_{bb} - \\Sigma_{ba} \\Sigma_{aa}^{-1} \\Sigma_{ab}\n$$\nBy substituting our terms ($\\mathbf{a} \\to \\mathbf{y}$, $\\mathbf{b} \\to f^\\star$, zero means), we obtain the posterior distribution for $f^\\star$ as a Gaussian with mean $\\mu(x^\\star)$ and variance $\\sigma^2(x^\\star)$:\n\n- **Predictive Mean:**\n$$\n\\mu(x^\\star) = K(x^\\star, X) [K(X, X) + \\sigma_n^2 I]^{-1} \\mathbf{y}\n$$\n\n- **Epistemic Variance:**\n$$\n\\sigma^2(x^\\star) = k(x^\\star, x^\\star) - K(x^\\star, X) [K(X, X) + \\sigma_n^2 I]^{-1} K(X, x^\\star)\n$$\nThe problem asks for the epistemic variance, which is the variance of the latent function $f^\\star$. The predictive variance for a new observation $y^\\star$ would include the noise term, i.e., $\\sigma^2_{pred}(x^\\star) = \\sigma^2(x^\\star) + \\sigma_n^2$. We only compute $\\sigma^2(x^\\star)$.\n\n**5. Implementation Algorithm**\n\nFor numerical stability, a small jitter $\\delta = 10^{-10}$ is added to the diagonal of the covariance matrix of observations. Let $K_y = K(X, X) + \\sigma_n^2 I + \\delta I$.\n\nThe computational procedure is as follows:\n1. Construct the kernel matrices: $K(X, X)$, $K(X, x^\\star)$, and the scalar $k(x^\\star, x^\\star)$ using the squared-exponential kernel formula.\n2. Form the matrix to be inverted: $K_y = K(X, X) + (\\sigma_n^2 + \\delta)I_{N \\times N}$.\n3. To avoid direct matrix inversion, solve the linear system $K_y \\boldsymbol{\\alpha} = \\mathbf{y}$ for the vector $\\boldsymbol{\\alpha}$.\n4. Compute the predictive mean: $\\mu(x^\\star) = K(x^\\star, X) \\boldsymbol{\\alpha} = \\sum_{i=1}^N \\alpha_i k(x_i, x^\\star)$.\n5. To compute the variance, solve a second linear system $K_y \\mathbf{v} = K(X, x^\\star)$ for the vector $\\mathbf{v}$.\n6. Compute the epistemic variance: $\\sigma^2(x^\\star) = k(x^\\star, x^\\star) - K(x^\\star, X) \\mathbf{v}$.\n\nThis procedure is applied to each of the four test cases. The training data $y_i$ are generated using the logistic-sigmoid function $f(x) = (1 + \\exp(-a (x - b)))^{-1}$ with parameters $a=8$ and $b=0.5$, except where specified otherwise.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the predictive mean and epistemic variance for a Gaussian Process\n    surrogate model for four specified test cases.\n    \"\"\"\n\n    def dose_response(x, a=8.0, b=0.5):\n        \"\"\"\n        Calculates the value of the normalized dose-response curve.\n        \"\"\"\n        return 1.0 / (1.0 + np.exp(-a * (np.asarray(x) - b)))\n\n    def se_kernel(X1, X2, l, sigma_f):\n        \"\"\"\n        Computes the squared-exponential kernel matrix between two sets of 1D points.\n        \"\"\"\n        l_sq = l**2\n        sigma_f_sq = sigma_f**2\n        # Ensure inputs are column vectors for broadcasting\n        X1 = np.asarray(X1).reshape(-1, 1)\n        X2 = np.asarray(X2).reshape(-1, 1)\n        sqdist = (X1.reshape(-1, 1) - X2.reshape(1, -1))**2\n        return sigma_f_sq * np.exp(-0.5 * sqdist / l_sq)\n\n    def gp_predict(X_train, y_train, x_star, l, sigma_f, sigma_n):\n        \"\"\"\n        Calculates the GP predictive mean and epistemic variance at a point x_star.\n        \"\"\"\n        delta = 1e-10  # Diagonal jitter for numerical stability\n        X_train = np.asarray(X_train).reshape(-1, 1)\n        y_train = np.asarray(y_train).reshape(-1, 1)\n        x_star_arr = np.array([x_star]).reshape(-1, 1)\n        n_train = len(X_train)\n\n        # Compute kernel matrices\n        K_XX = se_kernel(X_train, X_train, l, sigma_f)\n        K_sX = se_kernel(x_star_arr, X_train, l, sigma_f) # K(x*, X)\n        K_ss = se_kernel(x_star_arr, x_star_arr, l, sigma_f) # k(x*, x*)\n\n        # Form the matrix Ky = K(X, X) + sigma_n^2*I + delta*I\n        Ky = K_XX + (sigma_n**2) * np.eye(n_train) + delta * np.eye(n_train)\n\n        # Solve for alpha = Ky^-1 * y_train\n        alpha = np.linalg.solve(Ky, y_train)\n\n        # Predictive mean: mu(x*) = K(x*, X) * alpha\n        pred_mean = K_sX @ alpha\n\n        # For variance, solve for v = Ky^-1 * K(X, x*)\n        # K(X, x*) is the transpose of K(x*, X)\n        K_Xs = K_sX.T\n        v = np.linalg.solve(Ky, K_Xs)\n\n        # Epistemic variance: var(f*) = k(x*,x*) - K(x*,X) * v\n        pred_var = K_ss - K_sX @ v\n\n        return pred_mean.item(), pred_var.item()\n\n    test_cases = [\n        # Test case 1 (happy path, moderate noise, interpolation)\n        {\n            \"X\": [0.0, 0.25, 0.5, 0.75, 1.0],\n            \"y\": dose_response([0.0, 0.25, 0.5, 0.75, 1.0]),\n            \"l\": 0.2, \"sigma_f\": 1.0, \"sigma_n\": 0.1,\n            \"x_star\": 0.6\n        },\n        # Test case 2 (boundary condition, zero noise, exact training input)\n        {\n            \"X\": [0.0, 0.25, 0.5, 0.75, 1.0],\n            \"y\": dose_response([0.0, 0.25, 0.5, 0.75, 1.0]),\n            \"l\": 0.3, \"sigma_f\": 1.0, \"sigma_n\": 0.0,\n            \"x_star\": 0.5\n        },\n        # Test case 3 (edge case, extrapolation far outside training domain)\n        {\n            \"X\": [0.0, 0.25, 0.5, 0.75, 1.0],\n            \"y\": dose_response([0.0, 0.25, 0.5, 0.75, 1.0]),\n            \"l\": 0.2, \"sigma_f\": 1.0, \"sigma_n\": 0.1,\n            \"x_star\": 2.0\n        },\n        # Test case 4 (edge case, duplicate training inputs with small noise)\n        {\n            \"X\": [0.0, 0.25, 0.5, 0.5, 0.75, 1.0],\n            \"y\": [\n                dose_response(0.0),\n                dose_response(0.25),\n                dose_response(0.5) - 0.02,\n                dose_response(0.5) + 0.02,\n                dose_response(0.75),\n                dose_response(1.0)\n            ],\n            \"l\": 0.15, \"sigma_f\": 1.0, \"sigma_n\": 0.05,\n            \"x_star\": 0.5\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        mean, variance = gp_predict(\n            case[\"X\"], case[\"y\"], case[\"x_star\"],\n            case[\"l\"], case[\"sigma_f\"], case[\"sigma_n\"]\n        )\n        results.extend([mean, variance])\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}