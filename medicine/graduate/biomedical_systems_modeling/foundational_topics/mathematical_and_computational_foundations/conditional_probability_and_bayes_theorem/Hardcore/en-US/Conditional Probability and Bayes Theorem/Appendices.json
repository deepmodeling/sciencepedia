{
    "hands_on_practices": [
        {
            "introduction": "We begin with a challenge that lies at the heart of statistical reasoning in medicine: the interpretation of evidence in the presence of confounding factors. This exercise presents a classic case of Simpson's Paradox, where an aggregated analysis of clinical trial data suggests one conclusion, while a properly stratified analysis reveals the opposite. By working through this problem , you will develop the crucial skill of using conditional probability to uncover the true relationships hidden within complex datasets.",
            "id": "3878086",
            "problem": "A multi-center pragmatic clinical trial compares two treatments, denoted $A$ and $B$, for patients admitted to the intensive care unit with sepsis-induced acute kidney injury. Due to protocol-based contraindications, patients with high severity (denoted $H$) were more frequently allocated to treatment $A$, and patients with low severity (denoted $L$) were more frequently allocated to treatment $B$. The primary endpoint is a binary clinical success defined as sustained renal recovery by day $28$ without dialysis. The following counts were recorded:\n\n- High severity stratum $H$: Under treatment $A$, $900$ patients were treated and $360$ achieved clinical success; under treatment $B$, $100$ patients were treated and $30$ achieved clinical success.\n- Low severity stratum $L$: Under treatment $A$, $100$ patients were treated and $70$ achieved clinical success; under treatment $B$, $900$ patients were treated and $540$ achieved clinical success.\n\nAssume patients within each severity stratum are exchangeable and that the empirical frequencies approximate probabilities. Use only the core definitions of conditional probability, Bayes’ theorem, and the law of total probability as the fundamental base.\n\nTasks:\n- Compute the aggregated success probabilities $P(\\text{Success}\\mid A)$ and $P(\\text{Success}\\mid B)$ from the counts.\n- Compute the stratum-specific conditional success probabilities $P(\\text{Success}\\mid A,H)$, $P(\\text{Success}\\mid B,H)$, $P(\\text{Success}\\mid A,L)$, and $P(\\text{Success}\\mid B,L)$ from the counts.\n- Using Bayes’ theorem and the law of total probability, express $P(\\text{Success}\\mid A)$ and $P(\\text{Success}\\mid B)$ as weighted averages over strata and identify the weights in terms of $P(H\\mid A)$, $P(L\\mid A)$, $P(H\\mid B)$, and $P(L\\mid B)$.\n- Based on your derivations, select the single best interpretation below:\n\nA. Treatment $B$ is intrinsically superior because $P(\\text{Success}\\mid B)  P(\\text{Success}\\mid A)$ in aggregate; stratification by severity confirms $B$ has higher success in each stratum, so the aggregate and conditional conclusions agree.\n\nB. Treatment $B$ appears superior in aggregate because its recipients are disproportionately low severity; conditioning on severity shows $A$ has higher success in both $H$ and $L$. The correct decomposition is $P(\\text{Success}\\mid T) = \\sum_{s\\in\\{H,L\\}} P(\\text{Success}\\mid T,s)\\,P(s\\mid T)$ for $T\\in\\{A,B\\}$, which explains the reversal.\n\nC. The reversal is due solely to random sampling error; as the sample sizes grow large, the aggregate success rates must align with the stratum-specific ordering, and no conditioning or Bayes’ theorem is required.\n\nD. The appropriate weighting in the aggregate decomposition is $P(s)$ rather than $P(s\\mid T)$, so $P(\\text{Success}\\mid T) = \\sum_{s\\in\\{H,L\\}} P(\\text{Success}\\mid T,s)\\,P(s)$; using $P(s\\mid T)$ causes the apparent paradox.\n\nE. This paradox can occur only if $P(\\text{Success}\\mid H)$ equals $P(\\text{Success}\\mid L)$, i.e., identical baseline risks across strata; otherwise, reversal is impossible.",
            "solution": "The problem statement will first be validated for scientific and logical integrity.\n\n### Step 1: Extract Givens\nLet $S$ denote the event of clinical success, $A$ and $B$ denote the treatments, and $H$ and $L$ denote the high and low severity strata, respectively. The data provided are counts of patients and successes:\n-   **Stratum $H$ (High Severity):**\n    -   Treatment $A$: $900$ patients, $360$ successes. We denote these counts as $N(A,H) = 900$ and $N(S,A,H) = 360$.\n    -   Treatment $B$: $100$ patients, $30$ successes. We denote these counts as $N(B,H) = 100$ and $N(S,B,H) = 30$.\n-   **Stratum $L$ (Low Severity):**\n    -   Treatment $A$: $100$ patients, $70$ successes. We denote these counts as $N(A,L) = 100$ and $N(S,A,L) = 70$.\n    -   Treatment $B$: $900$ patients, $540$ successes. We denote these counts as $N(B,L) = 900$ and $N(S,B,L) = 540$.\n-   **Assumption:** Empirical frequencies from these counts can be used to approximate the true probabilities.\n-   **Tasks:**\n    1.  Compute $P(S\\mid A)$ and $P(S\\mid B)$.\n    2.  Compute $P(S\\mid A,H)$, $P(S\\mid B,H)$, $P(S\\mid A,L)$, and $P(S\\mid B,L)$.\n    3.  Express $P(S\\mid A)$ and $P(S\\mid B)$ as weighted averages, identifying the weights.\n    4.  Select the best interpretation.\n\n### Step 2: Validate Using Extracted Givens\nThe problem describes a scenario of confounding in a clinical trial, a classic topic in biostatistics and epidemiology known as Simpson's Paradox.\n-   **Scientifically Grounded:** The setup is scientifically plausible. Pragmatic trials often have less strict randomization, leading to imbalances in patient characteristics (confounders) across treatment arms. The concept of severity as a confounder that is associated with both treatment choice and outcome is a fundamental problem in observational studies and some clinical trial designs.\n-   **Well-Posed:** The problem is well-posed. All necessary data (patient counts) are provided to calculate the required probabilities. The instructions are clear and lead to a unique set of calculations and a conclusion based on those calculations.\n-   **Objective:** The problem statement is objective, presenting numerical data and asking for calculation and interpretation based on established probabilistic principles. It does not contain subjective or opinion-based claims.\n\nThe problem does not violate any of the criteria for invalidity. It is a well-structured problem that tests the understanding of conditional probability, confounding, and Simpson's Paradox.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. The solution will now proceed as requested.\n\n### Derivations\n\n**Task 1: Compute aggregated success probabilities**\n\nThe total number of patients receiving treatment $A$ is $N(A) = N(A,H) + N(A,L) = 900 + 100 = 1000$.\nThe total number of successes with treatment $A$ is $N(S,A) = N(S,A,H) + N(S,A,L) = 360 + 70 = 430$.\nThe aggregated success probability for treatment $A$ is:\n$$\nP(S\\mid A) = \\frac{N(S,A)}{N(A)} = \\frac{430}{1000} = 0.43\n$$\nThe total number of patients receiving treatment $B$ is $N(B) = N(B,H) + N(B,L) = 100 + 900 = 1000$.\nThe total number of successes with treatment $B$ is $N(S,B) = N(S,B,H) + N(S,B,L) = 30 + 540 = 570$.\nThe aggregated success probability for treatment $B$ is:\n$$\nP(S\\mid B) = \\frac{N(S,B)}{N(B)} = \\frac{570}{1000} = 0.57\n$$\nBased on the aggregated data, treatment $B$ appears superior, as $P(S\\mid B)  P(S\\mid A)$.\n\n**Task 2: Compute stratum-specific conditional success probabilities**\n\nFor the high severity stratum ($H$):\n$$\nP(S\\mid A,H) = \\frac{N(S,A,H)}{N(A,H)} = \\frac{360}{900} = 0.40\n$$\n$$\nP(S\\mid B,H) = \\frac{N(S,B,H)}{N(B,H)} = \\frac{30}{100} = 0.30\n$$\nIn stratum $H$, treatment $A$ is superior, as $P(S\\mid A,H)  P(S\\mid B,H)$.\n\nFor the low severity stratum ($L$):\n$$\nP(S\\mid A,L) = \\frac{N(S,A,L)}{N(A,L)} = \\frac{70}{100} = 0.70\n$$\n$$\nP(S\\mid B,L) = \\frac{N(S,B,L)}{N(B,L)} = \\frac{540}{900} = 0.60\n$$\nIn stratum $L$, treatment $A$ is also superior, as $P(S\\mid A,L)  P(S\\mid B,L)$.\n\nThis reveals a reversal of association: treatment $B$ appears superior in aggregate, but treatment $A$ is superior within each severity stratum. This is an instance of Simpson's Paradox.\n\n**Task 3: Express aggregate probabilities as weighted averages**\n\nThe aggregate probability of success for a given treatment can be decomposed using the Law of Total Probability over the partition of the sample space defined by the severity strata $\\{H, L\\}$. For any treatment $T \\in \\{A, B\\}$, the probability of success given the treatment is:\n$$\nP(S\\mid T) = P(S \\mid T, H) P(H \\mid T) + P(S \\mid T, L) P(L \\mid T)\n$$\nThis formula expresses the aggregate success rate $P(S\\mid T)$ as a weighted average of the stratum-specific success rates $P(S\\mid T,H)$ and $P(S\\mid T,L)$. The weights are the proportions of patients in each stratum who received treatment $T$, i.e., $P(H\\mid T)$ and $P(L\\mid T)$.\n\nLet's calculate these weights from the given counts:\n- For treatment $A$:\n  $$P(H\\mid A) = \\frac{N(A,H)}{N(A)} = \\frac{900}{1000} = 0.9$$\n  $$P(L\\mid A) = \\frac{N(A,L)}{N(A)} = \\frac{100}{1000} = 0.1$$\n- For treatment $B$:\n  $$P(H\\mid B) = \\frac{N(B,H)}{N(B)} = \\frac{100}{1000} = 0.1$$\n  $$P(L\\mid B) = \\frac{N(B,L)}{N(B)} = \\frac{900}{1000} = 0.9$$\n\nNow, we can verify the decomposition:\n- For treatment $A$:\n  $$P(S\\mid A) = (0.40)(0.9) + (0.70)(0.1) = 0.36 + 0.07 = 0.43$$\n- For treatment $B$:\n  $$P(S\\mid B) = (0.30)(0.1) + (0.60)(0.9) = 0.03 + 0.54 = 0.57$$\nThe calculations match the aggregate probabilities computed directly. The paradox is explained by the weights: treatment $A$ was predominantly given to high-severity patients ($90\\%$), who have a lower chance of success overall. Conversely, treatment $B$ was predominantly given to low-severity patients ($90\\%$), who have a higher chance of success overall. The confounding variable (severity) is associated with both treatment assignment and outcome, leading to the misleading aggregate result.\n\n### Option-by-Option Analysis\n\n**A. Treatment $B$ is intrinsically superior because $P(\\text{Success}\\mid B)  P(\\text{Success}\\mid A)$ in aggregate; stratification by severity confirms $B$ has higher success in each stratum, so the aggregate and conditional conclusions agree.**\nThis statement is factually incorrect. While it is true that $P(S\\mid B)  P(S\\mid A)$ in aggregate, the stratification shows the opposite: $P(S\\mid A,H)  P(S\\mid B,H)$ and $P(S\\mid A,L)  P(S\\mid B,L)$. The aggregate and conditional conclusions contradict each other. **Incorrect**.\n\n**B. Treatment $B$ appears superior in aggregate because its recipients are disproportionately low severity; conditioning on severity shows $A$ has higher success in both $H$ and $L$. The correct decomposition is $P(\\text{Success}\\mid T) = \\sum_{s\\in\\{H,L\\}} P(\\text{Success}\\mid T,s)\\,P(s\\mid T)$ for $T\\in\\{A,B\\}$, which explains the reversal.**\nThis statement is a precise and complete explanation of the phenomenon. It correctly notes that treatment $B$ seems better in aggregate. It correctly identifies the reason: the confounding due to disproportionate allocation by severity ($P(L\\mid B) = 0.9$ vs $P(L\\mid A) = 0.1$). It correctly states that conditioning on severity reveals the true superiority of treatment $A$ in both strata. Finally, it provides the mathematically correct decomposition based on the law of total probability that formally explains how the reversal occurs due to a difference in the weights $P(s\\mid T)$. **Correct**.\n\n**C. The reversal is due solely to random sampling error; as the sample sizes grow large, the aggregate success rates must align with the stratum-specific ordering, and no conditioning or Bayes’ theorem is required.**\nThis statement is incorrect. The paradox is a structural phenomenon caused by confounding, not random error. The problem assumes the observed frequencies represent the true probabilities, thereby analyzing the structural issue itself. The paradox would persist even with infinitely large samples as long as the confounding allocation pattern ($P(s \\mid A) \\neq P(s \\mid B)$) is maintained. The aggregate and stratum-specific orderings do not have to align. **Incorrect**.\n\n**D. The appropriate weighting in the aggregate decomposition is $P(s)$ rather than $P(s\\mid T)$, so $P(\\text{Success}\\mid T) = \\sum_{s\\in\\{H,L\\}} P(\\text{Success}\\mid T,s)\\,P(s)$; using $P(s\\mid T)$ causes the apparent paradox.**\nThis statement misrepresents the mathematical decomposition of the observed probability $P(S \\mid T)$. The correct decomposition, as derived from the law of total probability, uses the weights $P(s \\mid T)$. The formula proposed, $\\sum_{s} P(S \\mid T,s)\\,P(s)$, represents a standardized rate, which is a method to *adjust for* confounding, not a decomposition of the observed aggregate rate. Furthermore, the statement that using $P(s \\mid T)$ *causes* the paradox is misleading; using these weights *explains* the paradox, which is caused by the underlying confounding structure of the data. **Incorrect**.\n\n**E. This paradox can occur only if $P(\\text{Success}\\mid H)$ equals $P(\\text{Success}\\mid L)$, i.e., identical baseline risks across strata; otherwise, reversal is impossible.**\nThis statement is the opposite of the truth. A necessary condition for severity to be a confounder is that it must be associated with the outcome, meaning the success rates must differ between strata ($P(S\\mid H) \\neq P(S\\mid L)$). If the success rates were identical across strata, the severity variable would not be a confounder, and the paradox could not occur. In our problem, the overall success rate in stratum $H$ is $(360+30)/(900+100) = 0.39$, while in stratum $L$, it is $(70+540)/(100+900) = 0.61$. These are clearly not equal, and it is this difference that allows the paradox to manifest. **Incorrect**.",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "Having seen the importance of proper conditioning, we now turn to the core mechanism of Bayesian inference: updating our knowledge in light of new data. This practice problem  guides you through the derivation of a posterior distribution for a treatment's response rate using the elegant properties of conjugate priors. You will not only quantify uncertainty after observing trial data but also learn how to use this updated knowledge to make predictions about future patient outcomes.",
            "id": "4956933",
            "problem": "A single-arm early-phase clinical trial evaluates a new oncology treatment with a binary outcome per patient: each patient either achieves a confirmed objective response or does not. Let $\\,\\theta \\in (0,1)\\,$ denote the unknown true response probability for this treatment in the studied population. Suppose prior clinical and mechanistic knowledge is summarized by a continuous prior density on $\\,\\theta\\,$ proportional to $\\,\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}\\,$ for fixed hyperparameters $\\,\\alpha0\\,$ and $\\,\\beta0\\,$. A cohort of $\\,n\\,$ independent and identically distributed patients is treated, and $\\,x\\,$ respond.\n\nStarting from the definition of conditional probability and Bayes' theorem, and using only the standard forms of the Binomial likelihood and the Beta family, derive the analytical form of the posterior distribution for $\\,\\theta\\,$ given $\\,x\\,$ responses out of $\\,n\\,$ treated patients. In your derivation, begin from first principles and show all steps needed to identify the distributional family and its parameters.\n\nNow consider the following scientifically realistic numbers: take the prior hyperparameters $\\,\\alpha=1\\,$ and $\\,\\beta=1\\,$, and suppose the trial observes $\\,x=5\\,$ responses among $\\,n=10\\,$ patients. Using the posterior you derived, compute the posterior predictive probability that at least $\\,2\\,$ of the next $\\,m=3\\,$ independent patients treated with the same regimen will respond. Express your final answer as a decimal and round your result to four significant figures.",
            "solution": "The problem asks for two parts: first, to derive the posterior distribution of a binomial proportion $\\theta$ using a beta prior, and second, to use this posterior to compute a posterior predictive probability for a specific numerical case.\n\n### Part 1: Derivation of the Posterior Distribution\n\nLet $\\theta \\in (0,1)$ be the true response probability. The problem specifies a continuous prior distribution for $\\theta$ with a probability density function (PDF) $p(\\theta)$ proportional to $\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}$ for some hyperparameters $\\alpha0$ and $\\beta0$. This is the kernel of a Beta distribution, so the prior is $\\theta \\sim \\text{Beta}(\\alpha, \\beta)$. The full PDF is:\n$$p(\\theta) = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\theta^{\\alpha-1} (1-\\theta)^{\\beta-1}$$\nThe trial data consists of observing $x$ responses in $n$ independent and identically distributed patients. The number of responses $x$ for a given value of $\\theta$ follows a Binomial distribution. The probability mass function is given by:\n$$P(x|\\theta, n) = \\binom{n}{x} \\theta^x (1-\\theta)^{n-x}$$\nThis function, when viewed as a function of $\\theta$ for observed data $(x,n)$, is the likelihood function, $L(\\theta|x,n)$.\n\nAccording to Bayes' theorem, the posterior distribution of $\\theta$ given the data is proportional to the product of the likelihood and the prior distribution:\n$$p(\\theta|x,n) \\propto L(\\theta|x,n) \\cdot p(\\theta)$$\nSubstituting the expressions for the likelihood and the prior, we have:\n$$p(\\theta|x,n) \\propto \\left( \\binom{n}{x} \\theta^x (1-\\theta)^{n-x} \\right) \\cdot \\left( \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\theta^{\\alpha-1} (1-\\theta)^{\\beta-1} \\right)$$\nIn this proportionality, we can drop any terms that do not depend on $\\theta$, such as $\\binom{n}{x}$ and the normalization constant of the prior. This simplifies the expression for the posterior kernel:\n$$p(\\theta|x,n) \\propto \\theta^x (1-\\theta)^{n-x} \\cdot \\theta^{\\alpha-1} (1-\\theta)^{\\beta-1}$$\nCombining the powers of $\\theta$ and $(1-\\theta)$:\n$$p(\\theta|x,n) \\propto \\theta^{x+\\alpha-1} (1-\\theta)^{n-x+\\beta-1}$$\nThis resulting kernel is recognizable as the kernel of a Beta distribution. A random variable $Y$ follows a $\\text{Beta}(a,b)$ distribution if its PDF is proportional to $y^{a-1}(1-y)^{b-1}$. By comparing the posterior kernel with this form, we can identify the parameters of the posterior distribution. Let the posterior parameters be $\\alpha'$ and $\\beta'$.\nWe have:\n$$ \\alpha' - 1 = x+\\alpha-1 \\implies \\alpha' = \\alpha+x $$\n$$ \\beta' - 1 = n-x+\\beta-1 \\implies \\beta' = \\beta+n-x $$\nTherefore, the posterior distribution for $\\theta$ is a Beta distribution with parameters $\\alpha+x$ and $\\beta+n-x$:\n$$ \\theta | x,n \\sim \\text{Beta}(\\alpha+x, \\beta+n-x) $$\nThe full PDF of the posterior distribution is:\n$$ p(\\theta|x,n) = \\frac{\\Gamma(\\alpha+x+\\beta+n-x)}{\\Gamma(\\alpha+x)\\Gamma(\\beta+n-x)} \\theta^{\\alpha+x-1} (1-\\theta)^{\\beta+n-x-1} = \\frac{\\Gamma(\\alpha+\\beta+n)}{\\Gamma(\\alpha+x)\\Gamma(\\beta+n-x)} \\theta^{\\alpha+x-1} (1-\\theta)^{\\beta+n-x-1} $$\n\n### Part 2: Calculation of the Posterior Predictive Probability\n\nThe problem specifies the prior hyperparameters $\\alpha=1$ and $\\beta=1$, and the trial data $x=5$ responses among $n=10$ patients. The task is to compute the posterior predictive probability that at least $2$ of the next $m=3$ independent patients will respond.\n\nFirst, we determine the posterior distribution for $\\theta$ with the given values. Using the formula derived above, the posterior parameters are:\n$$ \\alpha' = \\alpha+x = 1+5 = 6 $$\n$$ \\beta' = \\beta+n-x = 1+10-5 = 6 $$\nSo, the posterior distribution is $\\theta | x=5, n=10 \\sim \\text{Beta}(6,6)$. The PDF is:\n$$ p(\\theta|x=5,n=10) = \\frac{\\Gamma(12)}{\\Gamma(6)\\Gamma(6)} \\theta^{5}(1-\\theta)^{5} $$\nNote that a $\\text{Beta}(1,1)$ prior is a Uniform$(0,1)$ distribution. The posterior $\\text{Beta}(6,6)$ is symmetric around $\\theta=0.5$.\n\nLet $Y$ be the number of responses in the next $m=3$ patients. We want to find the posterior predictive probability $P(Y \\ge 2 | x=5, n=10)$. This is the sum $P(Y=2 | x=5, n=10) + P(Y=3 | x=5, n=10)$.\n\nThe posterior predictive probability for a specific outcome $Y=y$ is obtained by averaging the binomial probability $P(Y=y|\\theta, m)$ over the posterior distribution of $\\theta$:\n$$ P(Y=y | x,n) = \\int_0^1 P(Y=y|\\theta,m) \\cdot p(\\theta|x,n) d\\theta $$\nHere, $P(Y=y|\\theta,m) = \\binom{m}{y}\\theta^y(1-\\theta)^{m-y}$ and $p(\\theta|x,n)$ is the PDF of a $\\text{Beta}(\\alpha', \\beta')$ distribution.\n$$ P(Y=y | x,n) = \\int_0^1 \\binom{m}{y}\\theta^y(1-\\theta)^{m-y} \\frac{\\Gamma(\\alpha'+\\beta')}{\\Gamma(\\alpha')\\Gamma(\\beta')} \\theta^{\\alpha'-1}(1-\\theta)^{\\beta'-1} d\\theta $$\n$$ P(Y=y | x,n) = \\binom{m}{y} \\frac{\\Gamma(\\alpha'+\\beta')}{\\Gamma(\\alpha')\\Gamma(\\beta')} \\int_0^1 \\theta^{y+\\alpha'-1}(1-\\theta)^{m-y+\\beta'-1} d\\theta $$\nThe integral is the Beta function $B(y+\\alpha', m-y+\\beta') = \\frac{\\Gamma(y+\\alpha')\\Gamma(m-y+\\beta')}{\\Gamma(y+\\alpha' + m-y+\\beta')}$. This gives the probability mass function of the Beta-Binomial distribution.\n$$P(Y=y|x,n) = \\binom{m}{y} \\frac{\\Gamma(\\alpha'+\\beta')}{\\Gamma(\\alpha')\\Gamma(\\beta')} \\frac{\\Gamma(y+\\alpha')\\Gamma(m-y+\\beta')}{\\Gamma(m+\\alpha'+\\beta')}$$\nUsing our specific values: $m=3$, $\\alpha'=6$, $\\beta'=6$.\n\nFor $y=2$:\n$$ P(Y=2|x=5,n=10) = \\binom{3}{2} \\frac{\\Gamma(12)}{\\Gamma(6)\\Gamma(6)} \\frac{\\Gamma(2+6)\\Gamma(3-2+6)}{\\Gamma(3+12)} = 3 \\frac{\\Gamma(12)}{\\Gamma(6)\\Gamma(6)} \\frac{\\Gamma(8)\\Gamma(7)}{\\Gamma(15)} $$\nUsing $\\Gamma(k)=(k-1)!$:\n$$ P(Y=2|...) = 3 \\frac{11!}{5!5!} \\frac{7!6!}{14!} = 3 \\frac{11! \\cdot 7! \\cdot 6!}{14 \\cdot 13 \\cdot 12 \\cdot 11! \\cdot 5! \\cdot 5!} = 3 \\frac{(7 \\cdot 6 \\cdot 5!) \\cdot (6 \\cdot 5!)}{14 \\cdot 13 \\cdot 12 \\cdot 5! \\cdot 5!} = 3 \\frac{7 \\cdot 6 \\cdot 6}{14 \\cdot 13 \\cdot 12} = 3 \\frac{252}{2184} $$\nSimplifying the fraction:\n$$ 3 \\frac{7 \\cdot 6 \\cdot 6}{(2 \\cdot 7) \\cdot 13 \\cdot (2 \\cdot 6)} = 3 \\frac{6}{2 \\cdot 13 \\cdot 2} = 3 \\frac{3}{26} = \\frac{9}{26} $$\n\nFor $y=3$:\n$$ P(Y=3|x=5,n=10) = \\binom{3}{3} \\frac{\\Gamma(12)}{\\Gamma(6)\\Gamma(6)} \\frac{\\Gamma(3+6)\\Gamma(3-3+6)}{\\Gamma(3+12)} = 1 \\frac{11!}{5!5!} \\frac{8!5!}{14!} $$\n$$ P(Y=3|...) = \\frac{11! \\cdot 8! \\cdot 5!}{14 \\cdot 13 \\cdot 12 \\cdot 11! \\cdot 5! \\cdot 5!} = \\frac{8!}{14 \\cdot 13 \\cdot 12 \\cdot 5!} = \\frac{8 \\cdot 7 \\cdot 6}{14 \\cdot 13 \\cdot 12} = \\frac{336}{2184} $$\nSimplifying the fraction:\n$$ \\frac{8 \\cdot 7 \\cdot 6}{(2 \\cdot 7) \\cdot 13 \\cdot (2 \\cdot 6)} = \\frac{8}{4 \\cdot 13} = \\frac{2}{13} $$\n\nThe total probability is the sum of these two probabilities:\n$$ P(Y \\ge 2 | x=5,n=10) = P(Y=2|...) + P(Y=3|...) = \\frac{9}{26} + \\frac{2}{13} = \\frac{9}{26} + \\frac{4}{26} = \\frac{13}{26} = \\frac{1}{2} $$\nThis result can be intuited from the symmetry of the posterior distribution. Since the posterior is $\\text{Beta}(6,6)$, it is symmetric about $\\theta=0.5$. This symmetry implies that the posterior predictive distribution for $Y$ is also symmetric, i.e., $P(Y=y) = P(Y=m-y)$. For $m=3$, this means $P(Y=0)=P(Y=3)$ and $P(Y=1)=P(Y=2)$. The total probability is $P(Y=0)+P(Y=1)+P(Y=2)+P(Y=3)=1$. The desired probability is $P(Y \\ge 2) = P(Y=2)+P(Y=3)$. Due to symmetry, this equals $P(Y=1)+P(Y=0)$. Therefore, $P(Y \\ge 2)$ must be exactly half of the total probability, which is $1/2$.\n\nThe final answer is $0.5$. The problem requires this to be expressed as a decimal rounded to four significant figures.\n\nFinal value: $0.5000$.",
            "answer": "$$\\boxed{0.5000}$$"
        },
        {
            "introduction": "Real-world biomedical systems are often hierarchical, with data nested within groups like patients within hospitals. This final exercise  challenges you to apply Bayesian principles to such a structure by deriving the components for a Gibbs sampler, a powerful Markov Chain Monte Carlo (MCMC) algorithm. By working through this hierarchical model, you will see how conditional probability enables us to build sophisticated models that borrow strength across groups and provide more robust inferences.",
            "id": "3878057",
            "problem": "A biomedical systems modeling team is investigating hospital-acquired infection dynamics across multiple hospitals using a two-level hierarchical probabilistic model. At the hospital level, each hospital $h$ has an observed infection count $y_h$ over exposure $e_h$ patient-days, and an unobserved infection rate $\\theta_h$ in infections per patient-day. At the population level, a global rate-control parameter $\\beta$ governs the prior distribution of hospital rates $\\theta_h$. The objective is to derive the conditional posterior distributions needed for Gibbs Sampling (GS) within Markov Chain Monte Carlo (MCMC) inference using fundamental probability definitions and widely accepted statistical modeling assumptions. The goal is to implement a program that computes, for specified test suites, the conditional posterior parameters and means for the hospital-level infection rates and the global rate-control parameter under conjugate structures where possible.\n\nFoundational base:\n- Use the definition of conditional probability and Bayes' theorem: for any parameters $\\phi$ and data $D$, Bayes' theorem states that $p(\\phi \\mid D) = \\dfrac{p(D \\mid \\phi)\\,p(\\phi)}{p(D)}$, and conditional independence is used where appropriate.\n- Use the Poisson likelihood for count data: for observed count $y_h$ with rate $\\lambda_h$, the probability mass function is $P(Y_h = y_h \\mid \\lambda_h) = \\dfrac{\\lambda_h^{y_h} e^{-\\lambda_h}}{y_h!}$.\n- Use the Gamma distribution as a conjugate prior for Poisson rates: for shape $\\alpha$ and rate $\\beta$, the density is $p(\\theta \\mid \\alpha, \\beta) = \\dfrac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\theta^{\\alpha - 1} e^{-\\beta \\theta}$.\n\nModel specification and assumptions:\n- For each hospital $h \\in \\{1,\\dots,H\\}$, the observed count $y_h$ is modeled as $Y_h \\mid \\theta_h \\sim \\text{Poisson}(e_h \\theta_h)$, where $e_h$ is the known exposure in patient-days and $\\theta_h$ is the unknown rate in infections per patient-day.\n- At the hospital level, the rates have a Gamma prior: $\\theta_h \\mid \\alpha, \\beta \\sim \\text{Gamma}(\\alpha, \\beta)$, with known shape $\\alpha  0$ and unknown rate $\\beta  0$.\n- At the population level, the global rate-control parameter has a Gamma hyperprior: $\\beta \\sim \\text{Gamma}(c, d)$, with known hyperparameters $c  0$ and $d  0$.\n- The two-level hierarchy consists of hospital-specific parameters $\\theta_h$ and a population-level hyperparameter $\\beta$; the shape parameter $\\alpha$ is considered fixed and known.\n\nDerivation task:\n- Starting from Bayes' theorem and the specified likelihood and prior, derive the conditional posterior distribution of $\\theta_h$ given $y_h$, $e_h$, $\\alpha$, and the current value of $\\beta$. Do not assume any shortcut formulas; justify every step using the definitions and algebraic manipulation of the Poisson likelihood and Gamma prior.\n- Starting from Bayes' theorem and the prior structure, derive the conditional posterior distribution of $\\beta$ given the current values of $\\{\\theta_h\\}_{h=1}^H$, $c$, and $d$. Use the independence of $\\theta_h$ given $\\alpha$ and $\\beta$ to form the joint likelihood over hospitals, and show the conjugate update explicitly.\n- Use these derived conditionals to define computational updates that a GS algorithm would employ: for each hospital $h$, compute the posterior shape and rate parameters for $\\theta_h \\mid y_h, e_h, \\alpha, \\beta$; and compute the posterior shape and rate parameters for $\\beta \\mid \\{\\theta_h\\}_{h=1}^H, c, d$. Additionally, compute the posterior means for these Gamma conditionals.\n\nImplementation task:\n- Implement a program that, for each test case provided below, takes the observed counts $y_h$, exposures $e_h$, known shape $\\alpha$, hyperparameters $c$ and $d$, a current value of $\\beta$ (used to compute the conditional posterior of $\\theta_h$), and current values of $\\{\\theta_h\\}_{h=1}^H$ (used to compute the conditional posterior of $\\beta$). The program must compute:\n    - For each hospital $h$: the posterior shape parameter $a_h^{\\text{post}}$ and posterior rate parameter $b_h^{\\text{post}}$ for $\\theta_h$, and the posterior mean $m_h^{\\text{post}}$.\n    - For the global rate-control parameter $\\beta$: the posterior shape parameter $c^{\\text{post}}$, posterior rate parameter $d^{\\text{post}}$, and posterior mean $m_{\\beta}^{\\text{post}}$.\n- Express all infection rates (the means $m_h^{\\text{post}}$) in infections per patient-day as decimals. The mean $m_{\\beta}^{\\text{post}}$ has units reciprocal to infections per patient-day; express it as a decimal.\n\nTest suite:\n- Case $1$ (general case): $H = 3$, $y = [12, 2, 25]$, $e = [1200, 800, 1500]$ patient-days, $\\alpha = 2.0$, $c = 1.5$, $d = 0.05$, current $\\beta = 0.002$, current $\\theta = [0.01, 0.0025, 0.0166667]$.\n- Case $2$ (zero counts edge case): $H = 2$, $y = [0, 0]$, $e = [500, 1000]$ patient-days, $\\alpha = 0.5$, $c = 0.5$, $d = 0.001$, current $\\beta = 0.0005$, current $\\theta = [0.0001, 0.00005]$.\n- Case $3$ (high counts and exposures): $H = 4$, $y = [75, 60, 90, 120]$, $e = [2000, 2500, 3000, 3500]$ patient-days, $\\alpha = 3.0$, $c = 2.0$, $d = 0.05$, current $\\beta = 0.02$, current $\\theta = [0.0375, 0.024, 0.03, 0.0342857]$.\n- Case $4$ (single hospital boundary): $H = 1$, $y = [1]$, $e = [1]$ patient-day, $\\alpha = 1.0$, $c = 1.0$, $d = 1.0$, current $\\beta = 0.5$, current $\\theta = [1.0]$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results for all test cases aggregated into a single top-level comma-separated list enclosed in square brackets. Each test case’s result must itself be a list of the form $[\\text{theta\\_shapes}, \\text{theta\\_rates}, \\text{theta\\_means}, \\text{beta\\_shape}, \\text{beta\\_rate}, \\text{beta\\_mean}]$, where $\\text{theta\\_shapes}$, $\\text{theta\\_rates}$, and $\\text{theta\\_means}$ are lists ordered by hospital index and the remaining entries are floats. For example, the printed output should look like $[[\\ldots],[\\ldots],[\\ldots],[\\ldots]]$ with no additional text. All numeric quantities are to be represented as decimals or integers; do not use a percentage sign.",
            "solution": "The problem statement has been validated and is deemed sound. It is a well-posed problem in Bayesian statistical modeling, grounded in established principles of probability theory. The model structure, a hierarchical Poisson-Gamma model, is standard for analyzing count data in fields like biomedical research. The request is to derive the full conditional posterior distributions necessary for a Gibbs sampling algorithm and to implement these derivations computationally for a given set of test cases.\n\nThe hierarchical model is specified as follows:\n1.  **Likelihood**: For each of $H$ hospitals, the observed infection count $y_h$ given the hospital-specific infection rate $\\theta_h$ and exposure $e_h$ follows a Poisson distribution:\n    $$y_h \\mid \\theta_h \\sim \\text{Poisson}(e_h \\theta_h)$$\n2.  **Prior on Hospital Rates**: The unobserved rates $\\theta_h$ for each hospital are drawn from a common Gamma distribution, governed by a fixed shape parameter $\\alpha$ and a global rate-control parameter $\\beta$:\n    $$\\theta_h \\mid \\alpha, \\beta \\sim \\text{Gamma}(\\alpha, \\beta)$$\n3.  **Hyperprior on Global Rate-Control Parameter**: The parameter $\\beta$ is itself given a Gamma prior distribution with fixed hyperparameters $c$ and $d$:\n    $$\\beta \\mid c, d \\sim \\text{Gamma}(c, d)$$\n\nOur objective is to derive the full conditional posterior distributions for the unknown parameters, namely $p(\\theta_h \\mid y_h, e_h, \\alpha, \\beta)$ for each hospital $h \\in \\{1, \\dots, H\\}$, and $p(\\beta \\mid \\{\\theta_h\\}_{h=1}^H, \\alpha, c, d)$. These distributions are the core components of a Gibbs sampler for this model.\n\n**1. Derivation of the Conditional Posterior for Hospital Rate $\\theta_h$**\n\nWe seek the distribution of $\\theta_h$ conditioned on the observed data $y_h$ and all other parameters, which in the context of Gibbs sampling are treated as fixed at their current values. Applying Bayes' theorem, the posterior probability density is proportional to the product of the likelihood and the prior:\n$$p(\\theta_h \\mid y_h, e_h, \\alpha, \\beta) \\propto p(y_h \\mid \\theta_h, e_h) \\, p(\\theta_h \\mid \\alpha, \\beta)$$\nThe likelihood $p(y_h \\mid \\theta_h, e_h)$ is given by the Poisson probability mass function, where the rate is $\\lambda_h = e_h \\theta_h$:\n$$p(y_h \\mid \\theta_h, e_h) = \\frac{(e_h \\theta_h)^{y_h} e^{-e_h \\theta_h}}{y_h!}$$\nAs a function of $\\theta_h$, dropping constants of proportionality, the likelihood kernel is:\n$$p(y_h \\mid \\theta_h, e_h) \\propto \\theta_h^{y_h} e^{-e_h \\theta_h}$$\nThe prior distribution for $\\theta_h$ is a Gamma distribution, $p(\\theta_h \\mid \\alpha, \\beta) = \\text{Gamma}(\\theta_h; \\alpha, \\beta)$. Its probability density function is:\n$$p(\\theta_h \\mid \\alpha, \\beta) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\theta_h^{\\alpha - 1} e^{-\\beta \\theta_h}$$\nThe kernel of the prior, as a function of $\\theta_h$, is:\n$$p(\\theta_h \\mid \\alpha, \\beta) \\propto \\theta_h^{\\alpha - 1} e^{-\\beta \\theta_h}$$\nMultiplying the likelihood kernel and the prior kernel, we obtain the kernel of the conditional posterior distribution for $\\theta_h$:\n$$p(\\theta_h \\mid y_h, e_h, \\alpha, \\beta) \\propto \\left( \\theta_h^{y_h} e^{-e_h \\theta_h} \\right) \\times \\left( \\theta_h^{\\alpha - 1} e^{-\\beta \\theta_h} \\right)$$\n$$p(\\theta_h \\mid y_h, e_h, \\alpha, \\beta) \\propto \\theta_h^{y_h + \\alpha - 1} e^{-(e_h + \\beta)\\theta_h}$$\nThis resulting kernel is proportional to that of a Gamma distribution. By inspection, we identify the shape and rate parameters of this posterior distribution.\nThe posterior distribution for $\\theta_h$ is therefore:\n$$\\theta_h \\mid y_h, e_h, \\alpha, \\beta \\sim \\text{Gamma}(\\alpha + y_h, \\beta + e_h)$$\nThe posterior parameters and mean are:\n-   Posterior shape: $a_h^{\\text{post}} = \\alpha + y_h$\n-   Posterior rate: $b_h^{\\text{post}} = \\beta + e_h$\n-   Posterior mean: $m_h^{\\text{post}} = E[\\theta_h \\mid \\cdot] = \\frac{a_h^{\\text{post}}}{b_h^{\\text{post}}} = \\frac{\\alpha + y_h}{\\beta + e_h}$\n\n**2. Derivation of the Conditional Posterior for Global Parameter $\\beta$**\n\nNext, we derive the conditional posterior for the global rate-control parameter $\\beta$. This distribution is conditioned on the current values of all hospital-specific rates, $\\{\\theta_h\\}_{h=1}^H$, and the hyperparameters $\\alpha, c, d$. Due to the hierarchical structure, the data $y_h$ are conditionally independent of $\\beta$ given the $\\theta_h$ values.\nUsing Bayes' theorem:\n$$p(\\beta \\mid \\{\\theta_h\\}_{h=1}^H, \\alpha, c, d) \\propto p(\\{\\theta_h\\}_{h=1}^H \\mid \\alpha, \\beta) \\, p(\\beta \\mid c, d)$$\nThe term $p(\\{\\theta_h\\}_{h=1}^H \\mid \\alpha, \\beta)$ functions as the likelihood for $\\beta$. Given $\\alpha$ and $\\beta$, the rates $\\theta_h$ are conditionally independent. Thus, the joint probability is the product of the individual probabilities:\n$$p(\\{\\theta_h\\}_{h=1}^H \\mid \\alpha, \\beta) = \\prod_{h=1}^H p(\\theta_h \\mid \\alpha, \\beta)$$\nSubstituting the Gamma density for each $\\theta_h$:\n$$p(\\{\\theta_h\\}_{h=1}^H \\mid \\alpha, \\beta) = \\prod_{h=1}^H \\left( \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\theta_h^{\\alpha - 1} e^{-\\beta \\theta_h} \\right)$$\nGrouping terms that depend on $\\beta$:\n$$p(\\{\\theta_h\\}_{h=1}^H \\mid \\alpha, \\beta) \\propto \\prod_{h=1}^H (\\beta^\\alpha e^{-\\beta \\theta_h}) = (\\beta^\\alpha)^H \\exp\\left(-\\beta \\sum_{h=1}^H \\theta_h\\right) = \\beta^{\\alpha H} \\exp\\left(-\\beta \\sum_{h=1}^H \\theta_h\\right)$$\nThe prior on $\\beta$ is $p(\\beta \\mid c, d) = \\text{Gamma}(\\beta; c, d)$, with the kernel:\n$$p(\\beta \\mid c, d) \\propto \\beta^{c-1} e^{-d\\beta}$$\nThe posterior kernel for $\\beta$ is the product of the likelihood and prior kernels:\n$$p(\\beta \\mid \\cdot) \\propto \\left( \\beta^{\\alpha H} \\exp\\left(-\\beta \\sum_{h=1}^H \\theta_h\\right) \\right) \\times \\left( \\beta^{c-1} e^{-d\\beta} \\right)$$\n$$p(\\beta \\mid \\cdot) \\propto \\beta^{\\alpha H + c - 1} \\exp\\left(-\\left(d + \\sum_{h=1}^H \\theta_h\\right)\\beta\\right)$$\nThis is the kernel of a Gamma distribution. The posterior distribution for $\\beta$ is:\n$$\\beta \\mid \\{\\theta_h\\}_{h=1}^H, \\alpha, c, d \\sim \\text{Gamma}\\left(c + \\alpha H, d + \\sum_{h=1}^H \\theta_h\\right)$$\nThe posterior parameters and mean are:\n-   Posterior shape: $c^{\\text{post}} = c + \\alpha H$\n-   Posterior rate: $d^{\\text{post}} = d + \\sum_{h=1}^H \\theta_h$\n-   Posterior mean: $m_{\\beta}^{\\text{post}} = E[\\beta \\mid \\cdot] = \\frac{c^{\\text{post}}}{d^{\\text{post}}} = \\frac{c + \\alpha H}{d + \\sum_{h=1}^H \\theta_h}$\n\nThese derived formulas directly provide the update rules for a Gibbs sampling procedure and will be implemented to solve the specified test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes conditional posterior parameters and means for a hierarchical\n    Poisson-Gamma model based on provided test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"H\": 3, \"y\": [12, 2, 25], \"e\": [1200, 800, 1500],\n            \"alpha\": 2.0, \"c\": 1.5, \"d\": 0.05,\n            \"current_beta\": 0.002, \"current_theta\": [0.01, 0.0025, 0.0166667]\n        },\n        {\n            \"H\": 2, \"y\": [0, 0], \"e\": [500, 1000],\n            \"alpha\": 0.5, \"c\": 0.5, \"d\": 0.001,\n            \"current_beta\": 0.0005, \"current_theta\": [0.0001, 0.00005]\n        },\n        {\n            \"H\": 4, \"y\": [75, 60, 90, 120], \"e\": [2000, 2500, 3000, 3500],\n            \"alpha\": 3.0, \"c\": 2.0, \"d\": 0.05,\n            \"current_beta\": 0.02, \"current_theta\": [0.0375, 0.024, 0.03, 0.0342857]\n        },\n        {\n            \"H\": 1, \"y\": [1], \"e\": [1],\n            \"alpha\": 1.0, \"c\": 1.0, \"d\": 1.0,\n            \"current_beta\": 0.5, \"current_theta\": [1.0]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        # Extract data for the current case\n        H = case[\"H\"]\n        y = np.array(case[\"y\"])\n        e = np.array(case[\"e\"])\n        alpha = case[\"alpha\"]\n        c = case[\"c\"]\n        d = case[\"d\"]\n        current_beta = case[\"current_beta\"]\n        current_theta = np.array(case[\"current_theta\"])\n        \n        # --- Calculations for hospital-level rate parameters theta_h ---\n        # The conditional posterior for theta_h is Gamma(alpha + y_h, beta + e_h).\n        \n        # Posterior shape parameters for theta_h\n        theta_shapes = (alpha + y).tolist()\n        \n        # Posterior rate parameters for theta_h\n        theta_rates = (current_beta + e).tolist()\n        \n        # Posterior means for theta_h\n        theta_means = ((alpha + y) / (current_beta + e)).tolist()\n        \n        # --- Calculations for global rate-control parameter beta ---\n        # The conditional posterior for beta is Gamma(c + alpha*H, d + sum(theta_h)).\n\n        # Posterior shape parameter for beta\n        beta_shape = c + alpha * H\n        \n        # Posterior rate parameter for beta\n        beta_rate = d + np.sum(current_theta)\n        \n        # Posterior mean for beta\n        beta_mean = beta_shape / beta_rate\n        \n        # Assemble the results for the current case\n        case_result = [\n            theta_shapes,\n            theta_rates,\n            theta_means,\n            beta_shape,\n            beta_rate,\n            beta_mean\n        ]\n        results.append(case_result)\n\n    # The final print statement must produce a single-line string representation\n    # of the list of results, with each test case's result being a list itself.\n    # The default str() representation of lists will be used as per the\n    # example print statement format provided in the problem description.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}