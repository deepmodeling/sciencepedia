{
    "hands_on_practices": [
        {
            "introduction": "We begin with a foundational exercise that is central to statistical inference: estimating the parameters of a normal distribution. Many biological and measurement processes are approximately normal, making this a widely applicable skill. This practice  will guide you through the canonical workflow of deriving maximum likelihood estimators (MLEs) for both the mean $\\mu$ and variance $\\sigma^2$ simultaneously, solidifying the core mathematical mechanics of the method.",
            "id": "1933634",
            "problem": "A materials scientist is studying the performance of a newly developed type of thermoelectric generator. The continuous output voltage of a single generator module is modeled as a random variable that follows a normal distribution. The mean voltage, $\\mu$, represents the ideal performance of the module under specific thermal conditions, while the variance, $\\sigma^2$, quantifies measurement noise and manufacturing inconsistencies. To statistically characterize the production batch, a random sample of $n$ modules is selected, and their output voltages are measured, yielding the values $X_1, X_2, \\dots, X_n$. These measurements are assumed to be independent and identically distributed draws from a normal distribution with an unknown mean $\\mu$ and an unknown, positive variance $\\sigma^2$.\n\nYour task is to use the method of maximum likelihood to find the estimators for both $\\mu$ and $\\sigma^2$. Derive the expressions for the Maximum Likelihood Estimators (MLEs), denoted as $\\hat{\\mu}$ and $\\hat{\\sigma}^2$, respectively. Your final expressions for both estimators must be in terms of the sample values $X_1, X_2, \\dots, X_n$ and the sample size $n$.",
            "solution": "Let $X_{1},\\dots,X_{n}$ be independent and identically distributed as $N(\\mu,\\sigma^{2})$ with $\\sigma^{2}>0$. The joint likelihood as a function of $(\\mu,\\sigma^{2})$ is\n$$\nL(\\mu,\\sigma^{2};x_{1},\\dots,x_{n})=\\prod_{i=1}^{n}\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}\\,\\exp\\!\\left(-\\frac{(x_{i}-\\mu)^{2}}{2\\sigma^{2}}\\right).\n$$\nThe log-likelihood is\n$$\n\\ell(\\mu,\\sigma^{2})=\\ln L(\\mu,\\sigma^{2})=-\\frac{n}{2}\\ln(2\\pi)-\\frac{n}{2}\\ln(\\sigma^{2})-\\frac{1}{2\\sigma^{2}}\\sum_{i=1}^{n}(x_{i}-\\mu)^{2}.\n$$\nTo find the MLE for $\\mu$, differentiate $\\ell$ with respect to $\\mu$ and set the derivative to zero:\n$$\n\\frac{\\partial \\ell}{\\partial \\mu}=\\frac{1}{\\sigma^{2}}\\sum_{i=1}^{n}(x_{i}-\\mu)=0\n\\;\\;\\Longrightarrow\\;\\;\n\\sum_{i=1}^{n}x_{i}-n\\mu=0\n\\;\\;\\Longrightarrow\\;\\;\n\\hat{\\mu}=\\frac{1}{n}\\sum_{i=1}^{n}x_{i}.\n$$\nThe second derivative is $\\frac{\\partial^{2}\\ell}{\\partial \\mu^{2}}=-\\frac{n}{\\sigma^{2}}0$, so this critical point is a maximizer for $\\mu$.\n\nTo find the MLE for $\\sigma^{2}$, differentiate $\\ell$ with respect to $\\sigma^{2}$ and set to zero:\n$$\n\\frac{\\partial \\ell}{\\partial \\sigma^{2}}=-\\frac{n}{2}\\frac{1}{\\sigma^{2}}+\\frac{1}{2}\\frac{1}{(\\sigma^{2})^{2}}\\sum_{i=1}^{n}(x_{i}-\\mu)^{2}=0.\n$$\nMultiplying both sides by $2(\\sigma^{2})^{2}$ gives\n$$\n-n\\sigma^{2}+\\sum_{i=1}^{n}(x_{i}-\\mu)^{2}=0\n\\;\\;\\Longrightarrow\\;\\;\n\\hat{\\sigma}^{2}=\\frac{1}{n}\\sum_{i=1}^{n}(x_{i}-\\hat{\\mu})^{2}.\n$$\nSubstituting $\\hat{\\mu}=\\frac{1}{n}\\sum_{j=1}^{n}x_{j}$ yields\n$$\n\\hat{\\sigma}^{2}=\\frac{1}{n}\\sum_{i=1}^{n}\\left(x_{i}-\\frac{1}{n}\\sum_{j=1}^{n}x_{j}\\right)^{2}.\n$$\nTo verify this is a maximum in $\\sigma^{2}$, compute the second derivative\n$$\n\\frac{\\partial^{2}\\ell}{\\partial(\\sigma^{2})^{2}}=\\frac{n}{2(\\sigma^{2})^{2}}-\\frac{1}{(\\sigma^{2})^{3}}\\sum_{i=1}^{n}(x_{i}-\\mu)^{2},\n$$\nwhich, at the critical point where $\\sum_{i=1}^{n}(x_{i}-\\hat{\\mu})^{2}=n\\hat{\\sigma}^{2}$, becomes $-\\frac{n}{2(\\hat{\\sigma}^{2})^{2}}0$. Hence both estimators maximize the log-likelihood.\n\nTherefore, the MLEs are the sample mean and the average squared deviation from the sample mean:\n$$\n\\hat{\\mu}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i},\n\\quad\n\\hat{\\sigma}^{2}=\\frac{1}{n}\\sum_{i=1}^{n}\\left(X_{i}-\\frac{1}{n}\\sum_{j=1}^{n}X_{j}\\right)^{2}.\n$$",
            "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{n}\\sum_{i=1}^{n}X_{i}  \\frac{1}{n}\\sum_{i=1}^{n}\\left(X_{i}-\\frac{1}{n}\\sum_{j=1}^{n}X_{j}\\right)^{2}\\end{pmatrix}}$$"
        },
        {
            "introduction": "Maximum likelihood estimators are renowned for their excellent properties, but these properties hinge on assumptions about the data-generating process. This next exercise  presents a cautionary tale, highly relevant in fields like bioinformatics, where data filtering is common. By analytically calculating the bias of an estimator after seemingly innocuous data filtering, you will gain a deeper appreciation for how pre-analytical steps can impact statistical inference and the importance of modeling the true observation process.",
            "id": "2402432",
            "problem": "In a targeted amplicon sequencing assay used to quantify a rare allele at a single locus, each sample yields exactly $n$ independent reads, each read reporting the allele as mutant with probability $p$ and wild-type with probability $1-p$, independently across reads. Let $X$ denote the number of mutant reads in a sample, so that $X \\sim \\mathrm{Binomial}(n,p)$. As part of a quality-control filter, the bioinformatics pipeline discards any sample with $X=0$ (no mutant reads observed) and only retains samples with $X0$. An analyst then fits a binomial model to the retained samples using the standard binomial likelihood (ignoring the filtering) and reports the maximum likelihood estimator (MLE) $\\hat{p}=X/n$ for $p$ from a single retained sample.\n\nTreat the entire process (data generation and filtering) as the data-generating mechanism, and take $p \\in (0,1)$ and $n \\in \\{1,2,3,\\dots\\}$ as fixed. Using only first principles and definitions, derive the analytical expression for the bias of this reported estimator under the true data-generating mechanism, that is,\n$$\n\\mathrm{Bias}(\\hat{p}) \\equiv \\mathbb{E}[\\hat{p}]-p,\n$$\nwhere the expectation is taken with respect to the conditional distribution of $X$ given $X0$. Provide your final answer as a closed-form expression in terms of $p$ and $n$. Do not approximate. Your final answer must be a single expression with no units.",
            "solution": "The estimator for the probability of a mutant allele, $p$, is given as $\\hat{p} = \\frac{X}{n}$, where $X$ is the number of mutant reads out of a total of $n$ reads. The data-generating process includes a filter that discards any sample for which $X=0$. Consequently, the analysis is performed only on samples where $X0$.\n\nThe bias of the estimator $\\hat{p}$ is defined as:\n$$\n\\mathrm{Bias}(\\hat{p}) \\equiv \\mathbb{E}[\\hat{p}] - p\n$$\nThe expectation, $\\mathbb{E}[\\cdot]$, must be taken with respect to the distribution of the data that is actually observed. Due to the filtering step, this is the conditional distribution of $X$ given that $X > 0$.\n\nFirst, we express the conditional expectation of the estimator $\\hat{p}$:\n$$\n\\mathbb{E}[\\hat{p} | X0] = \\mathbb{E}\\left[\\frac{X}{n} \\Big| X0\\right]\n$$\nBy the linearity of expectation, we can write:\n$$\n\\mathbb{E}[\\hat{p} | X0] = \\frac{1}{n} \\mathbb{E}[X | X0]\n$$\nOur primary task is to compute the conditional expectation $\\mathbb{E}[X | X0]$. The random variable $X$ follows a binomial distribution, $X \\sim \\mathrm{Binomial}(n,p)$, with probability mass function (PMF):\n$$\nP(X=k) = \\binom{n}{k} p^k (1-p)^{n-k} \\quad \\text{for } k \\in \\{0, 1, 2, \\dots, n\\}\n$$\nThe definition of conditional expectation for a discrete random variable is:\n$$\n\\mathbb{E}[X | X0] = \\sum_{k} k \\cdot P(X=k | X0)\n$$\nThe sum is over all possible values of $X$ given the condition. Since $X0$, the possible values for $k$ are $\\{1, 2, \\dots, n\\}$.\nBy the definition of conditional probability, for any $k > 0$:\n$$\nP(X=k | X0) = \\frac{P(X=k \\text{ and } X0)}{P(X0)}\n$$\nSince the event $\\{X=k\\}$ for $k \\ge 1$ is a subset of the event $\\{X0\\}$, their intersection is simply $\\{X=k\\}$. Therefore:\n$$\nP(X=k | X0) = \\frac{P(X=k)}{P(X0)}\n$$\nThe probability of the conditioning event, $P(X0)$, is the complement of the probability of observing zero mutant reads:\n$$\nP(X0) = 1 - P(X=0)\n$$\nFor a binomial distribution, the probability of zero successes is:\n$$\nP(X=0) = \\binom{n}{0} p^0 (1-p)^{n-0} = (1-p)^n\n$$\nThus, the probability of the conditioning event is:\n$$\nP(X0) = 1 - (1-p)^n\n$$\nWe can now write the expression for the conditional expectation:\n$$\n\\mathbb{E}[X | X0] = \\sum_{k=1}^{n} k \\cdot \\frac{P(X=k)}{P(X0)} = \\frac{1}{1 - (1-p)^n} \\sum_{k=1}^{n} k \\cdot P(X=k)\n$$\nThe sum $\\sum_{k=1}^{n} k \\cdot P(X=k)$ is almost the definition of the unconditional expectation of $X$, $\\mathbb{E}[X] = \\sum_{k=0}^{n} k \\cdot P(X=k)$. The term for $k=0$ in the unconditional sum is $0 \\cdot P(X=0) = 0$, so it does not contribute to the sum. Therefore, the sum from $k=1$ to $n$ is identical to the sum from $k=0$ to $n$:\n$$\n\\sum_{k=1}^{n} k \\cdot P(X=k) = \\sum_{k=0}^{n} k \\cdot P(X=k) = \\mathbb{E}[X]\n$$\nThe unconditional expectation of a binomial random variable $X \\sim \\mathrm{Binomial}(n,p)$ is well-known to be $\\mathbb{E}[X] = np$.\nSubstituting this result back into our expression for the conditional expectation:\n$$\n\\mathbb{E}[X | X0] = \\frac{np}{1 - (1-p)^n}\n$$\nNow we can compute the conditional expectation of our estimator $\\hat{p}$:\n$$\n\\mathbb{E}[\\hat{p} | X0] = \\frac{1}{n} \\mathbb{E}[X | X0] = \\frac{1}{n} \\left(\\frac{np}{1 - (1-p)^n}\\right) = \\frac{p}{1 - (1-p)^n}\n$$\nFinally, we can calculate the bias by substituting this result into the definition of bias:\n$$\n\\mathrm{Bias}(\\hat{p}) = \\mathbb{E}[\\hat{p} | X0] - p = \\frac{p}{1 - (1-p)^n} - p\n$$\nTo simplify this expression, we find a common denominator:\n$$\n\\mathrm{Bias}(\\hat{p}) = \\frac{p - p \\left(1 - (1-p)^n\\right)}{1 - (1-p)^n} = \\frac{p - p + p(1-p)^n}{1 - (1-p)^n}\n$$\nThis simplifies to the final analytical expression for the bias:\n$$\n\\mathrm{Bias}(\\hat{p}) = \\frac{p(1-p)^n}{1 - (1-p)^n}\n$$\nThis expression is the closed-form representation of the bias of the estimator $\\hat{p}$ under the specified data-generating and filtering mechanism, in terms of the parameters $p$ and $n$. Since $p \\in (0,1)$, both the numerator and denominator are positive, indicating a positive bias, as expected from filtering out the lowest possible outcome ($X=0$).",
            "answer": "$$\n\\boxed{\\frac{p(1-p)^n}{1 - (1-p)^n}}\n$$"
        },
        {
            "introduction": "This final practice bridges theory and application, addressing a core challenge in biomedical systems modeling: connecting mechanistic models with experimental data. You will move from analytical derivations to computational implementation, using maximum likelihood to estimate a drug efficacy parameter ($k$) within a viral dynamics model described by an ordinary differential equation (ODE). This exercise  encapsulates the power of MLE as a tool to lend quantitative, statistical support to biological hypotheses embodied in mathematical models.",
            "id": "2402430",
            "problem": "You are given a mechanistic viral dynamics model and synthetic observations of a patientâ€™s viral load over time during treatment. The goal is to implement maximum likelihood estimation to infer the drug efficacy parameter.\n\nThe state variable is the viral load $V(t)$, measured in copies per mL, evolving according to the ordinary differential equation\n$$\n\\frac{dV}{dt} \\;=\\; P \\;-\\; c\\,V(t) \\;-\\; k\\,V(t)\\,I(t),\n$$\nwhere $P$ is the production rate in copies per mL per day, $c$ is the natural clearance rate in day$^{-1}$, $I(t)$ is a known, piecewise-constant drug concentration in mg/L, and $k \\ge 0$ is the drug efficacy parameter to be estimated with units day$^{-1}$ per (mg/L). The initial condition $V(0)=V_0$ is known.\n\nObservations are the natural logarithm of the viral load at specified time points. Specifically, for observation times $\\{t_i\\}_{i=1}^n$, the measured quantities satisfy\n$$\nY_i \\;=\\; \\ln V(t_i) + \\varepsilon_i,\n$$\nwhere $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$ are independent Gaussian noise terms with known standard deviation $\\sigma$. Time is in days, concentration in mg/L, and the final estimate of $k$ must be reported in day$^{-1}$ per (mg/L) as a real number.\n\nImplement a program that, for each test case below, computes the maximum likelihood estimate of $k$ given the specified parameters, the observation times, the piecewise-constant function $I(t)$, and the observed values $\\{Y_i\\}$. Use the natural logarithm for all logarithmic operations. Assume $k \\in [0,5]$.\n\nTest suite (three cases):\n- Case A (general decay under constant drug):\n  - Parameters: $P=100000$, $c=1.2$, $V_0=200000$, $\\sigma=0.05$.\n  - Drug schedule $I(t)$: one interval $[0,12]$ with $I(t)=2.0$ mg/L.\n  - Observation times (days): $[0,1,3,6,9,12]$.\n  - Observed values $Y$ (dimensionless, natural logarithm): $[12.206073,\\,11.220599,\\,10.880400,\\,10.871102,\\,10.871071,\\,10.871071]$.\n- Case B (step change in drug concentration):\n  - Parameters: $P=80000$, $c=0.8$, $V_0=100000$, $\\sigma=0.05$.\n  - Drug schedule $I(t)$: two intervals $[0,2]$ with $I(t)=0.0$ mg/L, and $(2,8]$ with $I(t)=4.0$ mg/L.\n  - Observation times (days): $[0,1,2,3,5,8]$.\n  - Observed values $Y$ (dimensionless, natural logarithm): $[11.512925,\\,11.512925,\\,11.512925,\\,10.242055,\\,10.126838,\\,10.126635]$.\n- Case C (slow approach to steady state under low drug, sparse sampling):\n  - Parameters: $P=60000$, $c=1.0$, $V_0=50000$, $\\sigma=0.05$.\n  - Drug schedule $I(t)$: one interval $[0,8]$ with $I(t)=0.5$ mg/L.\n  - Observation times (days): $[0,2,4,8]$.\n  - Observed values $Y$ (dimensionless, natural logarithm): $[10.819778,\\,10.937881,\\,10.951433,\\,10.953281]$.\n\nYour program must, for each test case, produce the maximum likelihood estimate $\\hat{k}$ consistent with the model and observations, respecting the bound $k \\in [0,5]$. Express each $\\hat{k}$ as a real number in day$^{-1}$ per (mg/L), rounded to three decimal places (do not include units in the output).\n\nFinal output format:\n- Your program should produce a single line of output containing the three estimates aggregated as a comma-separated list enclosed in square brackets, in the order of the test cases A, B, C. For example, the output must look like $[\\hat{k}_A,\\hat{k}_B,\\hat{k}_C]$ with each entry rounded to three decimals and no additional whitespace.\n\nAll calculations must be self-contained in your program. No user interaction or external data may be used.",
            "solution": "The core of the problem is to find the value of $k$ that maximizes the likelihood of observing the given data $\\{Y_i\\}_{i=1}^n$. The observation model is stated as:\n$$\nY_i = \\ln V(t_i) + \\varepsilon_i\n$$\nwhere $\\varepsilon_i$ are independent and identically distributed random variables from a normal distribution with mean $0$ and known variance $\\sigma^2$, i.e., $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$.\n\nFrom this, the probability distribution of a single observation $Y_i$, given the model prediction $\\ln V(t_i; k)$, is:\n$$\np(Y_i | k) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(Y_i - \\ln V(t_i; k))^2}{2\\sigma^2} \\right)\n$$\nThe notation $V(t_i; k)$ emphasizes the dependence of the viral load prediction on the parameter $k$.\n\nSince the noise terms $\\varepsilon_i$ are independent, the total likelihood function $L(k)$ for the set of all $n$ observations is the product of the individual probabilities:\n$$\nL(k) = \\prod_{i=1}^n p(Y_i | k) = \\left( \\frac{1}{2\\pi\\sigma^2} \\right)^{n/2} \\exp\\left( -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (Y_i - \\ln V(t_i; k))^2 \\right)\n$$\n\nMaximizing the likelihood function $L(k)$ is equivalent to maximizing its natural logarithm, the log-likelihood function $\\ell(k)$:\n$$\n\\ell(k) = \\ln L(k) = -\\frac{n}{2}\\ln(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (Y_i - \\ln V(t_i; k))^2\n$$\n\nTo find the maximum likelihood estimate $\\hat{k}$, we must find the value of $k$ that maximizes $\\ell(k)$. The first term, $-\\frac{n}{2}\\ln(2\\pi\\sigma^2)$, and the scaling factor $-\\frac{1}{2\\sigma^2}$ are constant with respect to $k$. Therefore, maximizing $\\ell(k)$ is equivalent to minimizing the sum of squared residuals (SSR) between the observed log-data and the predicted log-data:\n$$\n\\hat{k} = \\arg\\min_{k \\in [0,5]} \\text{SSR}(k) \\quad \\text{where} \\quad \\text{SSR}(k) = \\sum_{i=1}^n (Y_i - \\ln V(t_i; k))^2\n$$\nThe search for $\\hat{k}$ is constrained to the interval $[0, 5]$.\n\nTo evaluate the SSR for a given $k$, we must compute the model predictions $V(t_i; k)$. The governing ODE is:\n$$\n\\frac{dV}{dt} = P - (c + kI(t))V(t)\n$$\nThis is a linear first-order ODE. Since the drug concentration $I(t)$ is specified as a piecewise-constant function, we can solve this ODE analytically for each time interval where $I(t)$ is constant. Let's consider an interval $[t_A, t_B]$ where $I(t) = I_{\\text{const}}$. The ODE becomes:\n$$\n\\frac{dV}{dt} + \\lambda V = P, \\quad \\text{with} \\quad \\lambda = c + kI_{\\text{const}}\n$$\nThe solution to this equation, given an initial condition $V(t_A)$, is:\n$$\nV(t) = V_{ss} + (V(t_A) - V_{ss}) e^{-\\lambda(t - t_A)} \\quad \\text{for} \\quad t \\in [t_A, t_B]\n$$\nwhere $V_{ss} = P/\\lambda$ is the steady-state viral load for the constant drug concentration $I_{\\text{const}}$. This analytical solution allows for precise computation of $V(t)$ at any time $t$. Note that since $P > 0$ and $V(0) > 0$, the viral load $V(t)$ remains positive for all $t \\ge 0$, so its logarithm is always well-defined.\n\nThe computational strategy is as follows:\n1.  Define an objective function that takes a value of $k$ and returns the corresponding SSR.\n2.  Inside this function, for a given $k$:\n    a.  A simulation routine is required to compute $V(t_i; k)$ for all observation times $\\{t_i\\}_{i=1}^n$.\n    b.  This routine first establishes a unified timeline of points consisting of all observation times and all points where the drug concentration $I(t)$ changes.\n    c.  It then integrates the ODE from one time point to the next using the analytical solution, propagating the value of $V(t)$ forward in a piecewise manner. The initial condition for each segment is the final value from the previous segment, starting with $V(0) = V_0$.\n    d.  After computing $V(t_i)$ for all $i$, the function calculates $\\ln V(t_i; k)$ and computes the SSR against the observed data $\\{Y_i\\}$.\n3.  Use a numerical optimization algorithm to find the value of $k$ in the specified range $[0, 5]$ that minimizes the objective function. A suitable choice is a bounded scalar minimization method, such as the one available in the SciPy library. This procedure is repeated for each of the three test cases provided.",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize_scalar\n\ndef solve():\n    \"\"\"\n    Computes the Maximum Likelihood Estimate (MLE) for the drug efficacy parameter k\n    for three test cases of a viral dynamics model.\n    \"\"\"\n\n    test_cases = [\n        # Case A\n        {\n            'P': 100000.0, 'c': 1.2, 'V0': 200000.0, 'sigma': 0.05,\n            'drug_schedule': [(12.0, 2.0)],  # (t_end, concentration)\n            'obs_times': [0.0, 1.0, 3.0, 6.0, 9.0, 12.0],\n            'Y_obs': [12.206073, 11.220599, 10.880400, 10.871102, 10.871071, 10.871071]\n        },\n        # Case B\n        {\n            'P': 80000.0, 'c': 0.8, 'V0': 100000.0, 'sigma': 0.05,\n            'drug_schedule': [(2.0, 0.0), (8.0, 4.0)],\n            'obs_times': [0.0, 1.0, 2.0, 3.0, 5.0, 8.0],\n            'Y_obs': [11.512925, 11.512925, 11.512925, 10.242055, 10.126838, 10.126635]\n        },\n        # Case C\n        {\n            'P': 60000.0, 'c': 1.0, 'V0': 50000.0, 'sigma': 0.05,\n            'drug_schedule': [(8.0, 0.5)],\n            'obs_times': [0.0, 2.0, 4.0, 8.0],\n            'Y_obs': [10.819778, 10.937881, 10.951433, 10.953281]\n        }\n    ]\n\n    def get_I(t, drug_schedule):\n        \"\"\"\n        Returns the drug concentration I at a given time t based on the piecewise schedule.\n        \"\"\"\n        for t_end, I_val in drug_schedule:\n            # The problem defines intervals like [0, T1] and (T1, T2].\n            # A time t equal to an endpoint T1 falls into the first interval.\n            if t = t_end:\n                return I_val\n        # This part should not be reached if observation times are within the schedule.\n        return drug_schedule[-1][1]\n\n    def simulate_V_at_times(k, P, c, V0, drug_schedule, obs_times):\n        \"\"\"\n        Simulates the viral load V(t) at specified observation times\n        by solving the ODE piecewise.\n        \"\"\"\n        # Create a unified timeline of all relevant time points.\n        schedule_endpoints = [s[0] for s in drug_schedule]\n        # Using a set handles duplicate times.\n        all_unique_times = sorted(list(set([0.0] + list(obs_times) + schedule_endpoints)))\n        \n        # Filter out any times beyond the final observation/schedule time\n        max_time = max(max(obs_times), max(schedule_endpoints))\n        all_unique_times = [t for t in all_unique_times if t = max_time]\n        \n        v_results = {0.0: V0}\n        v_current = V0\n        \n        for i in range(len(all_unique_times) - 1):\n            t_start = all_unique_times[i]\n            t_end = all_unique_times[i+1]\n            \n            if t_start >= t_end:\n                continue\n            \n            dt = t_end - t_start\n            \n            # I(t) is constant on (t_start, t_end]. A point in the middle ensures correctness.\n            mid_point_t = (t_start + t_end) / 2.0\n            I_val = get_I(mid_point_t, drug_schedule)\n            \n            lam = c + k * I_val\n            \n            if lam == 0.0:\n                v_end = v_current + P * dt\n            else:\n                v_ss = P / lam\n                v_end = v_ss + (v_current - v_ss) * np.exp(-lam * dt)\n                \n            v_results[t_end] = v_end\n            v_current = v_end\n            \n        # Extract V values in the order of the original observation times\n        return np.array([v_results[t] for t in obs_times])\n\n    def make_objective_function(P, c, V0, drug_schedule, obs_times, Y_obs):\n        \"\"\"\n        Creates the objective function (Sum of Squared Residuals) to be minimized.\n        \"\"\"\n        Y_obs_arr = np.array(Y_obs)\n        \n        def objective(k):\n            V_pred = simulate_V_at_times(k, P, c, V0, drug_schedule, obs_times)\n            \n            # V(t) is guaranteed to be positive given P>0, V0>0.\n            Y_pred = np.log(V_pred)\n            \n            ssr = np.sum((Y_obs_arr - Y_pred)**2)\n            return ssr\n            \n        return objective\n\n    results = []\n    for case in test_cases:\n        objective_func = make_objective_function(\n            case['P'], case['c'], case['V0'], \n            case['drug_schedule'], case['obs_times'], case['Y_obs']\n        )\n        \n        # Find the value of k that minimizes the objective function, bounded in [0, 5].\n        opt_result = minimize_scalar(\n            objective_func, \n            bounds=(0, 5), \n            method='bounded'\n        )\n        \n        results.append(opt_result.x)\n\n    # Format the output as specified: a comma-separated list in brackets,\n    # with each value rounded to three decimal places.\n    formatted_results = [f\"{r:.3f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}