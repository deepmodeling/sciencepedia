{
    "hands_on_practices": [
        {
            "introduction": "Before we can correct for measurement error, we must first ask a more fundamental question: can we even distinguish the true signal from the noise using the data we have? This practice explores the crucial concept of parameter identifiability, which is the theoretical bedrock of all statistical estimation. By working through this exercise , you will understand why simple regression fails in the presence of predictor error and learn how study designs with replicate measures or validation data provide the necessary information to make the problem solvable.",
            "id": "3884562",
            "problem": "In a biomedical cohort study, a biomarker is intended to predict a continuous clinical outcome but is measured with error. Let the clinical outcome be modeled by the classical linear regression with additive noise, and the biomarker measurement be contaminated by additive measurement error. Specifically, for each individual $i$, suppose the latent true biomarker $X_i$ and the observed quantities $(W_i, Y_i)$ satisfy\n- $Y_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i$,\n- $W_i = X_i + U_i$,\nwhere $U_i$ denotes predictor measurement error and $\\varepsilon_i$ denotes outcome noise. Assume $U_i$ and $\\varepsilon_i$ are mean-zero, have finite variances, and are independent of $X_i$ and of each other. The parameters of interest are $(\\beta_0, \\beta_1, \\sigma_U^2)$, where $\\sigma_U^2 = \\operatorname{Var}(U_i)$. You only observe $(W_i, Y_i)$ at the study level and you aim to infer $(\\beta_0, \\beta_1, \\sigma_U^2)$.\n\nUsing the definition that a parameter is identifiable if distinct parameter values induce distinct population distributions of the observable data, reason from first principles about the observable implications of the above model. Then assess whether identifiability of $(\\beta_0, \\beta_1, \\sigma_U^2)$ holds or fails when only $(W_i, Y_i)$ are observed and no external information is available. Finally, consider two common biomedical designs: (i) replicate biomarker measurements per subject, that is, two independent measurements $W_{i1} = X_i + U_{i1}$ and $W_{i2} = X_i + U_{i2}$ with $U_{i1}$ and $U_{i2}$ independent and identically distributed given $X_i$, and (ii) a validation subsample in which a subset of subjects has access to a gold-standard measurement of $X_i$ in addition to $W_i$ and $Y_i$. Determine how these designs affect identifiability of $(\\beta_0, \\beta_1, \\sigma_U^2)$.\n\nWhich of the following statements correctly characterizes the identifiability failure and recovery for $(\\beta_0, \\beta_1, \\sigma_U^2)$ under the above setup?\n\nA. Without external information, observing only $(W_i, Y_i)$ does not identify $(\\beta_0, \\beta_1, \\sigma_U^2)$ because the decomposition of $\\operatorname{Var}(W_i)$ into $\\operatorname{Var}(X_i)$ and $\\sigma_U^2$ is not determined by the observable distribution. Replicate biomarker measurements per subject with independent errors identify $\\sigma_U^2$ and $\\operatorname{Var}(X_i)$, thereby enabling identification of $\\beta_1$ from the observable cross-covariance and then $\\beta_0$ from the observable means. A validation subsample with gold-standard $X_i$ similarly identifies $\\sigma_U^2$ and $\\operatorname{Var}(X_i)$, restoring identifiability of $(\\beta_0, \\beta_1, \\sigma_U^2)$.\n\nB. If $U_i$ and $\\varepsilon_i$ are Gaussian, then the joint distribution of $(W_i, Y_i)$ is bivariate normal and uniquely determines $(\\beta_0, \\beta_1, \\sigma_U^2)$, so identifiability holds without replicates or validation.\n\nC. Because $U_i$ is mean-zero, $\\beta_0$ is identifiable from $\\operatorname{E}[Y_i]$ and $\\operatorname{E}[W_i]$ alone, even when $\\beta_1$ and $\\sigma_U^2$ are unknown, so only $\\beta_1$ and $\\sigma_U^2$ suffer identifiability issues.\n\nD. Replicate outcome measurements $Y_{i1}$ and $Y_{i2}$ with independent $\\varepsilon_{i1}$ and $\\varepsilon_{i2}$ (but a single biomarker measurement $W_i$) are sufficient to restore identifiability of $(\\beta_0, \\beta_1, \\sigma_U^2)$ because they reveal $\\sigma_U^2$ from the variability in $Y_{i1} - Y_{i2}$.",
            "solution": "The user requires an analysis of parameter identifiability in a classical measurement error model, a canonical problem in biomedical statistics. The analysis will proceed in three stages: first, assessing the identifiability of the model parameters $(\\beta_0, \\beta_1, \\sigma_U^2)$ given only the observable data $(W_i, Y_i)$; second, investigating how identifiability is affected by a design with replicate biomarker measurements; and third, investigating the effect of a validation subsample.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   Model for the clinical outcome: $Y_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i$.\n-   Model for the biomarker measurement: $W_i = X_i + U_i$.\n-   Latent (unobserved) variable: $X_i$ (true biomarker value).\n-   Observed variables: $(W_i, Y_i)$.\n-   Error terms: $U_i$ (predictor measurement error) and $\\varepsilon_i$ (outcome noise).\n-   Assumptions on errors and latent variable:\n    -   $\\operatorname{E}[U_i] = 0$, $\\operatorname{Var}(U_i) = \\sigma_U^2$.\n    -   $\\operatorname{E}[\\varepsilon_i] = 0$, $\\operatorname{Var}(\\varepsilon_i) = \\sigma_\\varepsilon^2$.\n    -   $U_i$, $\\varepsilon_i$, and $X_i$ are mutually independent.\n-   Parameters of interest: $(\\beta_0, \\beta_1, \\sigma_U^2)$.\n-   Additional designs for consideration:\n    -   (i) Replicates: Two independent measurements $W_{i1} = X_i + U_{i1}$ and $W_{i2} = X_i + U_{i2}$, where $U_{i1}, U_{i2}$ are independent and identically distributed.\n    -   (ii) Validation subsample: A subset of subjects has a gold-standard (true) measurement of $X_i$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem statement is well-defined and scientifically grounded. It presents the standard classical linear measurement error model, a fundamental topic in statistics, biostatistics, and econometrics. The assumptions are standard, and the question about parameter identifiability is a core, non-trivial question within this framework. The problem is objective, formally stated, and does not violate any scientific or mathematical principles. It is a valid problem for analysis.\n\n**Step 3: Verdict and Action**\nThe problem is valid. The solution will be derived from first principles.\n\n### Derivation and Analysis\n\nThe identifiability of a set of parameters depends on whether they can be uniquely determined from the probability distribution of the observable data. The distribution is characterized by its moments. We will express the population moments of the observable variables $(W_i, Y_i)$ in terms of the model parameters and nuisance parameters. Let $\\operatorname{E}[X_i] = \\mu_X$ and $\\operatorname{Var}(X_i) = \\sigma_X^2$. The full set of unknown parameters is $(\\beta_0, \\beta_1, \\sigma_U^2, \\mu_X, \\sigma_X^2, \\sigma_\\varepsilon^2)$.\n\n**Part 1: Analysis of the Base Case (Observing only $(W_i, Y_i)$)**\n\nWe calculate the first and second moments of the observable vector $(W_i, Y_i)$.\n\n1.  **Expected Values:**\n    -   $\\operatorname{E}[W_i] = \\operatorname{E}[X_i + U_i] = \\operatorname{E}[X_i] + \\operatorname{E}[U_i] = \\mu_X + 0 = \\mu_X$.\n    -   $\\operatorname{E}[Y_i] = \\operatorname{E}[\\beta_0 + \\beta_1 X_i + \\varepsilon_i] = \\beta_0 + \\beta_1 \\operatorname{E}[X_i] + \\operatorname{E}[\\varepsilon_i] = \\beta_0 + \\beta_1 \\mu_X + 0 = \\beta_0 + \\beta_1 \\mu_X$.\n\n2.  **Variances and Covariance:**\n    -   $\\operatorname{Var}(W_i) = \\operatorname{Var}(X_i + U_i) = \\operatorname{Var}(X_i) + \\operatorname{Var}(U_i) + 2\\operatorname{Cov}(X_i, U_i) = \\sigma_X^2 + \\sigma_U^2$ (due to independence of $X_i$ and $U_i$).\n    -   $\\operatorname{Var}(Y_i) = \\operatorname{Var}(\\beta_0 + \\beta_1 X_i + \\varepsilon_i) = \\beta_1^2 \\operatorname{Var}(X_i) + \\operatorname{Var}(\\varepsilon_i) = \\beta_1^2 \\sigma_X^2 + \\sigma_\\varepsilon^2$ (due to independence of $X_i$ and $\\varepsilon_i$).\n    -   $\\operatorname{Cov}(W_i, Y_i) = \\operatorname{Cov}(X_i + U_i, \\beta_0 + \\beta_1 X_i + \\varepsilon_i)$. Expanding this and using the mutual independence of $X_i, U_i, \\varepsilon_i$:\n        $\\operatorname{Cov}(W_i, Y_i) = \\operatorname{Cov}(X_i, \\beta_1 X_i) + \\operatorname{Cov}(X_i, \\varepsilon_i) + \\operatorname{Cov}(U_i, \\beta_1 X_i) + \\operatorname{Cov}(U_i, \\varepsilon_i)$\n        $= \\beta_1 \\operatorname{Var}(X_i) + 0 + 0 + 0 = \\beta_1 \\sigma_X^2$.\n\nFrom the distribution of $(W_i, Y_i)$, we can determine the observable moments on the left-hand side of the equations. Let's assess if we can uniquely solve for the parameters of interest $(\\beta_0, \\beta_1, \\sigma_U^2)$.\n\n-   From $\\operatorname{E}[W_i] = \\mu_X$, the parameter $\\mu_X$ is identified.\n-   The equations involving second moments are:\n    (I) $\\operatorname{Var}(W_i) = \\sigma_X^2 + \\sigma_U^2$\n    (II) $\\operatorname{Cov}(W_i, Y_i) = \\beta_1 \\sigma_X^2$\n    (III) $\\operatorname{Var}(Y_i) = \\beta_1^2 \\sigma_X^2 + \\sigma_\\varepsilon^2$\n\nFrom these three equations, we must solve for the unknowns $\\beta_1, \\sigma_U^2, \\sigma_X^2, \\sigma_\\varepsilon^2$. This system has four unknowns but only three equations. Specifically, from equations (I) and (II), we have two equations for three unknowns: $\\beta_1, \\sigma_X^2, \\sigma_U^2$. This sub-system is underdetermined. For any given observable values of $\\operatorname{Var}(W_i)$ and $\\operatorname{Cov}(W_i, Y_i)$, we can choose any valid $\\sigma_X^2 \\in (0, \\operatorname{Var}(W_i))$, which then determines $\\sigma_U^2 = \\operatorname{Var}(W_i) - \\sigma_X^2$ and $\\beta_1 = \\operatorname{Cov}(W_i, Y_i) / \\sigma_X^2$. Since there is no unique solution for $\\sigma_X^2$, there is no unique solution for $\\beta_1$ and $\\sigma_U^2$. They are not identifiable.\n-   Since $\\beta_1$ is not identifiable, we cannot identify $\\beta_0$ from the equation $\\operatorname{E}[Y_i] = \\beta_0 + \\beta_1 \\mu_X$, as $\\beta_0 = \\operatorname{E}[Y_i] - \\beta_1 \\operatorname{E}[W_i]$ depends on the non-identified parameter $\\beta_1$.\n-   Conclusion for Base Case: The parameters $(\\beta_0, \\beta_1, \\sigma_U^2)$ are not identifiable when only $(W_i, Y_i)$ are observed. The fundamental issue is the inability to parse the total variance of the observed predictor, $\\operatorname{Var}(W_i)$, into its components: the true variance, $\\sigma_X^2$, and the error variance, $\\sigma_U^2$.\n\n**Part 2: Analysis of Design (i) - Replicate Measurements**\n\nWe now have observable data $(W_{i1}, W_{i2}, Y_i)$ per subject, where $W_{i1} = X_i + U_{i1}$ and $W_{i2} = X_i + U_{i2}$, with $U_{i1}, U_{i2}$ being i.i.d. draws from the distribution of $U_i$. This design provides new observable moments. A crucial new piece of information is the covariance between the replicate measurements:\n\n-   $\\operatorname{Cov}(W_{i1}, W_{i2}) = \\operatorname{Cov}(X_i + U_{i1}, X_i + U_{i2})$\n    $= \\operatorname{Cov}(X_i, X_i) + \\operatorname{Cov}(X_i, U_{i2}) + \\operatorname{Cov}(U_{i1}, X_i) + \\operatorname{Cov}(U_{i1}, U_{i2})$\n    $= \\operatorname{Var}(X_i) + 0 + 0 + 0 = \\sigma_X^2$.\n    The independence of the measurement errors ($U_{i1} \\perp U_{i2}$) is critical here.\n\nNow, $\\sigma_X^2$ is identified from the observable covariance $\\operatorname{Cov}(W_{i1}, W_{i2})$. With $\\sigma_X^2$ identified, we can solve for the other parameters:\n-   From $\\operatorname{Var}(W_{i1}) = \\sigma_X^2 + \\sigma_U^2$, we can identify $\\sigma_U^2 = \\operatorname{Var}(W_{i1}) - \\operatorname{Cov}(W_{i1}, W_{i2})$.\n-   From $\\operatorname{Cov}(W_{i1}, Y_i) = \\beta_1 \\sigma_X^2$, we can identify $\\beta_1 = \\operatorname{Cov}(W_{i1}, Y_i) / \\sigma_X^2$ (assuming $\\sigma_X^2 > 0$).\n-   Finally, with $\\beta_1$ and $\\mu_X = \\operatorname{E}[W_{i1}]$ identified, we identify $\\beta_0 = \\operatorname{E}[Y_i] - \\beta_1 \\operatorname{E}[W_{i1}]$.\n\nConclusion for Design (i): Replicate biomarker measurements restore identifiability for the entire parameter vector of interest $(\\beta_0, \\beta_1, \\sigma_U^2)$.\n\n**Part 3: Analysis of Design (ii) - Validation Subsample**\n\nIn a validation subsample, for a subset of subjects, we observe the triplet $(W_i, Y_i, X_i)$. The availability of the true predictor $X_i$ provides direct information.\n-   From the validation subjects, we can directly compute the measurement errors $U_i = W_i - X_i$. The variance of these errors, $\\operatorname{Var}(W_i - X_i)$, directly identifies $\\sigma_U^2$.\n-   Similarly, we can directly compute the sample variance of the observed $X_i$ values in the validation set to identify $\\sigma_X^2$.\n-   Even more directly, we can use the validation data to fit the regression model $Y_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i$ using standard ordinary least squares, since $X_i$ is observed. This regression directly identifies $\\beta_0$ and $\\beta_1$. With $\\beta_1$ known, we could have then proceeded as in the replicate case.\n\nConclusion for Design (ii): A validation subsample restores identifiability for $(\\beta_0, \\beta_1, \\sigma_U^2)$.\n\n### Option-by-Option Analysis\n\n**A. Without external information, observing only $(W_i, Y_i)$ does not identify $(\\beta_0, \\beta_1, \\sigma_U^2)$ because the decomposition of $\\operatorname{Var}(W_i)$ into $\\operatorname{Var}(X_i)$ and $\\sigma_U^2$ is not determined by the observable distribution. Replicate biomarker measurements per subject with independent errors identify $\\sigma_U^2$ and $\\operatorname{Var}(X_i)$, thereby enabling identification of $\\beta_1$ from the observable cross-covariance and then $\\beta_0$ from the observable means. A validation subsample with gold-standard $X_i$ similarly identifies $\\sigma_U^2$ and $\\operatorname{Var}(X_i)$, restoring identifiability of $(\\beta_0, \\beta_1, \\sigma_U^2)$.**\nThis statement accurately summarizes the conclusions from all three parts of the analysis above. The reason for non-identifiability is correctly stated. The mechanisms by which replicate measurements and a validation subsample restore identifiability are also correctly described.\n**Verdict: Correct**\n\n**B. If $U_i$ and $\\varepsilon_i$ are Gaussian, then the joint distribution of $(W_i, Y_i)$ is bivariate normal and uniquely determines $(\\beta_0, \\beta_1, \\sigma_U^2)$, so identifiability holds without replicates or validation.**\nIf $X_i, U_i, \\varepsilon_i$ are all Gaussian, then $(W_i, Y_i)$ is indeed bivariate normal. A bivariate normal distribution is fully specified by its five moments (two means, two variances, one covariance). Our analysis in Part 1 was based solely on these five moments and demonstrated that they are insufficient to uniquely determine the parameters. The assumption of normality provides no additional equations or constraints to resolve the underdetermined system. Therefore, non-identifiability persists even under Gaussian assumptions. This is a classic result in measurement error models.\n**Verdict: Incorrect**\n\n**C. Because $U_i$ is mean-zero, $\\beta_0$ is identifiable from $\\operatorname{E}[Y_i]$ and $\\operatorname{E}[W_i]$ alone, even when $\\beta_1$ and $\\sigma_U^2$ are unknown, so only $\\beta_1$ and $\\sigma_U^2$ suffer identifiability issues.**\nFrom our analysis, $\\operatorname{E}[Y_i] = \\beta_0 + \\beta_1 \\operatorname{E}[W_i]$. Rearranging gives $\\beta_0 = \\operatorname{E}[Y_i] - \\beta_1 \\operatorname{E}[W_i]$. This expression shows that identifying $\\beta_0$ requires knowledge of $\\beta_1$. As we established that $\\beta_1$ is not identifiable in the base case, it follows that $\\beta_0$ is also not identifiable. The non-identifiability of $\\beta_1$ propagates to $\\beta_0$.\n**Verdict: Incorrect**\n\n**D. Replicate outcome measurements $Y_{i1}$ and $Y_{i2}$ with independent $\\varepsilon_{i1}$ and $\\varepsilon_{i2}$ (but a single biomarker measurement $W_i$) are sufficient to restore identifiability of $(\\beta_0, \\beta_1, \\sigma_U^2)$ because they reveal $\\sigma_U^2$ from the variability in $Y_{i1} - Y_{i2}$.**\nLet's analyze this proposed design. We have $Y_{i1} = \\beta_0 + \\beta_1 X_i + \\varepsilon_{i1}$ and $Y_{i2} = \\beta_0 + \\beta_1 X_i + \\varepsilon_{i2}$. The difference is $Y_{i1} - Y_{i2} = \\varepsilon_{i1} - \\varepsilon_{i2}$. The variance of this difference is $\\operatorname{Var}(Y_{i1} - Y_{i2}) = \\operatorname{Var}(\\varepsilon_{i1}) + \\operatorname{Var}(\\varepsilon_{i2}) = 2\\sigma_\\varepsilon^2$. This design reveals the outcome noise variance $\\sigma_\\varepsilon^2$, not the predictor error variance $\\sigma_U^2$. The fundamental problem of confounding between $\\sigma_X^2$ and $\\sigma_U^2$ in the equation $\\operatorname{Var}(W_i) = \\sigma_X^2 + \\sigma_U^2$ remains completely unresolved. This design does not restore identifiability of $(\\beta_0, \\beta_1, \\sigma_U^2)$.\n**Verdict: Incorrect**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Once a model is identifiable, the next step is to construct an estimator that corrects for the measurement error. This exercise  guides you through the derivation of a 'corrected' slope estimator in a linear errors-in-variables model, a common task in fields like epidemiology and pharmacology. Furthermore, you will investigate the estimator's finite-sample bias, revealing the important principle that even theoretically consistent estimators can have systematic inaccuracies in realistic, finite-data scenarios.",
            "id": "3884491",
            "problem": "A cohort study aims to model the relationship between a latent plasma biomarker that modulates cardiac output and a hemodynamic outcome. For each subject $i \\in \\{1,\\dots,n\\}$, the latent biomarker concentration $X_i$ is not directly observed. Instead, an assay device produces an observed measurement $W_i = X_i + U_i$, where $U_i$ is additive measurement error with known variance $\\sigma_U^2$ from assay calibration. The clinical outcome satisfies $Y_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i$. Assume the following data-generating process:\n- $X_i \\sim \\mathcal{N}(\\mu_X, \\sigma_X^2)$, $U_i \\sim \\mathcal{N}(0, \\sigma_U^2)$, $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_{\\varepsilon}^2)$,\n- $\\{X_i, U_i, \\varepsilon_i\\}$ are mutually independent across subjects and within subjects,\n- $\\sigma_U^2$ is known; $\\sigma_X^2$ and $\\sigma_{\\varepsilon}^2$ are unknown.\n\nLet $\\bar{W} = \\frac{1}{n}\\sum_{i=1}^{n} W_i$, $\\bar{Y} = \\frac{1}{n}\\sum_{i=1}^{n} Y_i$, and define the unbiased sample covariance and variance\n$$\nS_{WY} = \\frac{1}{n-1} \\sum_{i=1}^{n} (W_i - \\bar{W})(Y_i - \\bar{Y}), \\quad S_W^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (W_i - \\bar{W})^2.\n$$\nStarting from the classical additive measurement error model and the definitions of variance and covariance, derive the corrected least squares estimator for the biomarker-outcome slope $\\beta_1$ by adjusting $S_{WY}$ and $S_W^2$ to estimate $\\operatorname{Cov}(X,Y)$ and $\\operatorname{Var}(X)$. Then, under the stated normality and independence assumptions, assess the finite-sample bias of this corrected estimator by obtaining a first-order (in $1/(n-1)$) approximation to its expectation. Your approximation must be expressed as a single closed-form analytic expression in terms of $\\beta_1$, $\\sigma_X^2$, $\\sigma_U^2$, and $n$.\n\nState the final expression for the first-order approximation to the expectation of the corrected estimator. No numerical evaluation is required.",
            "solution": "The user wants me to validate and solve a statistics problem involving an errors-in-variables model.\n\n### Step 1: Extract Givens\n- **Model for observed measurement:** $W_i = X_i + U_i$ for $i \\in \\{1,\\dots,n\\}$.\n- **Model for clinical outcome:** $Y_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i$.\n- **Distribution of latent variable:** $X_i \\sim \\mathcal{N}(\\mu_X, \\sigma_X^2)$.\n- **Distribution of measurement error:** $U_i \\sim \\mathcal{N}(0, \\sigma_U^2)$.\n- **Distribution of model error:** $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_{\\varepsilon}^2)$.\n- **Independence:** The random variables $\\{X_i, U_i, \\varepsilon_i\\}$ are mutually independent.\n- **Known/Unknown Parameters:** $\\sigma_U^2$ is known; $\\sigma_X^2$ and $\\sigma_{\\varepsilon}^2$ are unknown.\n- **Sample Statistics:**\n  - $\\bar{W} = \\frac{1}{n}\\sum_{i=1}^{n} W_i$\n  - $\\bar{Y} = \\frac{1}{n}\\sum_{i=1}^{n} Y_i$\n  - $S_{WY} = \\frac{1}{n-1} \\sum_{i=1}^{n} (W_i - \\bar{W})(Y_i - \\bar{Y})$\n  - $S_W^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (W_i - \\bar{W})^2$\n- **Task:**\n  1. Derive the corrected least squares estimator for $\\beta_1$.\n  2. Obtain a first-order (in $1/(n-1)$) approximation to the expectation of this corrected estimator, expressed in terms of $\\beta_1$, $\\sigma_X^2$, $\\sigma_U^2$, and $n$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem describes a classical additive measurement error model (errors-in-variables model), a standard and fundamental topic in statistics, biostatistics, and econometrics. The assumptions of normality and independence are common and form a well-understood basis for analysis. The entire setup is scientifically sound and based on established statistical principles.\n- **Well-Posed:** The problem is clearly stated, providing all necessary definitions, assumptions, and models to derive the requested estimator and its approximate expectation. The goal is specific and achievable.\n- **Objective:** The problem is formulated in precise, objective mathematical language.\n- **Conclusion:** The problem is free of any scientific or logical flaws, is completely specified, and is a standard, non-trivial problem in statistical theory. It is therefore valid.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will proceed with the solution.\n\nThe solution proceeds in two parts. First, we derive the corrected estimator for the slope parameter $\\beta_1$. Second, we derive a first-order approximation to the expectation of this estimator to assess its finite-sample bias.\n\n**Part 1: Derivation of the Corrected Estimator**\n\nThe true slope parameter $\\beta_1$ in the regression of $Y_i$ on $X_i$ is given by the ratio of the covariance to the variance:\n$$\n\\beta_1 = \\frac{\\operatorname{Cov}(X,Y)}{\\operatorname{Var}(X)}\n$$\nOur goal is to express $\\operatorname{Cov}(X,Y)$ and $\\operatorname{Var}(X)$ in terms of the moments of the observable variables $W_i$ and $Y_i$. We use the given model definitions and independence assumptions.\n\nFirst, we find the covariance between the observable measurement $W$ and the outcome $Y$:\n$$\n\\begin{aligned}\n\\operatorname{Cov}(W,Y) &= \\operatorname{Cov}(X_i + U_i, \\beta_0 + \\beta_1 X_i + \\varepsilon_i) \\\\\n&= \\operatorname{Cov}(X_i, \\beta_1 X_i) + \\operatorname{Cov}(X_i, \\varepsilon_i) + \\operatorname{Cov}(U_i, \\beta_1 X_i) + \\operatorname{Cov}(U_i, \\varepsilon_i) \\\\\n&= \\beta_1 \\operatorname{Cov}(X_i, X_i) + 0 + 0 + 0 \\\\\n&= \\beta_1 \\operatorname{Var}(X) = \\beta_1 \\sigma_X^2\n\\end{aligned}\n$$\nThe other covariance terms are zero due to the mutual independence of $X_i$, $U_i$, and $\\varepsilon_i$. This shows that $\\operatorname{Cov}(X,Y) = \\operatorname{Cov}(W,Y)$.\n\nNext, we find the variance of the observable measurement $W$:\n$$\n\\begin{aligned}\n\\operatorname{Var}(W) &= \\operatorname{Var}(X_i + U_i) \\\\\n&= \\operatorname{Var}(X_i) + \\operatorname{Var}(U_i) + 2\\operatorname{Cov}(X_i, U_i) \\\\\n&= \\sigma_X^2 + \\sigma_U^2 + 0 = \\sigma_X^2 + \\sigma_U^2\n\\end{aligned}\n$$\nThis is because $X_i$ and $U_i$ are independent. From this, we can express the variance of the latent variable $X$ as:\n$$\n\\operatorname{Var}(X) = \\sigma_X^2 = \\operatorname{Var}(W) - \\sigma_U^2\n$$\nSubstituting these moment relationships into the definition of $\\beta_1$:\n$$\n\\beta_1 = \\frac{\\operatorname{Cov}(W,Y)}{\\operatorname{Var}(W) - \\sigma_U^2}\n$$\nThis is the basis for a method-of-moments estimator. We replace the population moments $\\operatorname{Cov}(W,Y)$ and $\\operatorname{Var}(W)$ with their unbiased sample estimators, $S_{WY}$ and $S_W^2$, respectively. Since $\\sigma_U^2$ is known, we can construct the corrected least squares estimator, which we denote as $\\hat{\\beta}_1$:\n$$\n\\hat{\\beta}_1 = \\frac{S_{WY}}{S_W^2 - \\sigma_U^2}\n$$\n\n**Part 2: Finite-Sample Bias of the Corrected Estimator**\n\nThe estimator $\\hat{\\beta}_1$ is a ratio of two random variables, so in general, $\\mathbb{E}[\\hat{\\beta}_1] \\neq \\mathbb{E}[S_{WY}] / \\mathbb{E}[S_W^2 - \\sigma_U^2]$. To find an approximation for its expectation, we use a second-order Taylor series expansion (a form of the delta method) for the expectation of a ratio of random variables $A/B$:\n$$\n\\mathbb{E}\\left[\\frac{A}{B}\\right] \\approx \\frac{\\mathbb{E}[A]}{\\mathbb{E}[B]} \\left( 1 + \\frac{\\operatorname{Var}(B)}{(\\mathbb{E}[B])^2} - \\frac{\\operatorname{Cov}(A,B)}{\\mathbb{E}[A]\\mathbb{E}[B]} \\right)\n$$\nThis approximation is first-order in the variances and covariances, which are of order $1/(n-1)$.\nLet $A = S_{WY}$ and $B = S_W^2 - \\sigma_U^2$. We need to find the expectations, variances, and covariances of these terms.\n\nThe expectations are:\n$$\n\\mathbb{E}[A] = \\mathbb{E}[S_{WY}] = \\operatorname{Cov}(W,Y) = \\beta_1 \\sigma_X^2\n$$\n$$\n\\mathbb{E}[B] = \\mathbb{E}[S_W^2 - \\sigma_U^2] = \\mathbb{E}[S_W^2] - \\sigma_U^2 = \\operatorname{Var}(W) - \\sigma_U^2 = (\\sigma_X^2 + \\sigma_U^2) - \\sigma_U^2 = \\sigma_X^2\n$$\nThe ratio of expectations is $\\frac{\\mathbb{E}[A]}{\\mathbb{E}[B]} = \\frac{\\beta_1 \\sigma_X^2}{\\sigma_X^2} = \\beta_1$, confirming the estimator is consistent.\n\nNext, we find the variance and covariance terms. Since $W_i$ and $Y_i$ are linear combinations of independent normal variables, the vector $(W_i, Y_i)$ follows a bivariate normal distribution. For a bivariate normal sample of size $n$, the variance of the sample variance and the covariance between the sample variance and sample covariance are given by:\n$$\n\\operatorname{Var}(S_W^2) = \\frac{2(\\operatorname{Var}(W))^2}{n-1}\n$$\n$$\n\\operatorname{Cov}(S_{WY}, S_W^2) = \\frac{2\\operatorname{Var}(W)\\operatorname{Cov}(W,Y)}{n-1}\n$$\nWe have $\\operatorname{Var}(B) = \\operatorname{Var}(S_W^2 - \\sigma_U^2) = \\operatorname{Var}(S_W^2)$ since $\\sigma_U^2$ is a known constant.\n$$\n\\operatorname{Var}(B) = \\frac{2(\\sigma_X^2 + \\sigma_U^2)^2}{n-1}\n$$\nAnd for the covariance between $A$ and $B$:\n$$\n\\operatorname{Cov}(A,B) = \\operatorname{Cov}(S_{WY}, S_W^2 - \\sigma_U^2) = \\operatorname{Cov}(S_{WY}, S_W^2) = \\frac{2(\\sigma_X^2 + \\sigma_U^2)(\\beta_1 \\sigma_X^2)}{n-1}\n$$\nNow, we substitute these components into the Taylor approximation formula.\nThe correction term is $1 + \\frac{\\operatorname{Var}(B)}{(\\mathbb{E}[B])^2} - \\frac{\\operatorname{Cov}(A,B)}{\\mathbb{E}[A]\\mathbb{E}[B]}$. Let's compute the two fractions:\n$$\n\\frac{\\operatorname{Var}(B)}{(\\mathbb{E}[B])^2} = \\frac{\\frac{2(\\sigma_X^2 + \\sigma_U^2)^2}{n-1}}{(\\sigma_X^2)^2} = \\frac{2(\\sigma_X^2 + \\sigma_U^2)^2}{(n-1)(\\sigma_X^2)^2}\n$$\n$$\n\\frac{\\operatorname{Cov}(A,B)}{\\mathbb{E}[A]\\mathbb{E}[B]} = \\frac{\\frac{2(\\sigma_X^2 + \\sigma_U^2)(\\beta_1 \\sigma_X^2)}{n-1}}{(\\beta_1 \\sigma_X^2)(\\sigma_X^2)} = \\frac{2(\\sigma_X^2 + \\sigma_U^2)}{(n-1)\\sigma_X^2}\n$$\nSubtracting the second from the first:\n$$\n\\begin{aligned}\n\\frac{\\operatorname{Var}(B)}{(\\mathbb{E}[B])^2} - \\frac{\\operatorname{Cov}(A,B)}{\\mathbb{E}[A]\\mathbb{E}[B]} &= \\frac{2(\\sigma_X^2 + \\sigma_U^2)^2}{(n-1)(\\sigma_X^2)^2} - \\frac{2(\\sigma_X^2 + \\sigma_U^2)}{(n-1)\\sigma_X^2} \\\\\n&= \\frac{2(\\sigma_X^2 + \\sigma_U^2)}{(n-1)(\\sigma_X^2)^2} \\left[ (\\sigma_X^2 + \\sigma_U^2) - \\sigma_X^2 \\right] \\\\\n&= \\frac{2(\\sigma_X^2 + \\sigma_U^2)}{(n-1)(\\sigma_X^2)^2} [\\sigma_U^2] \\\\\n&= \\frac{2\\sigma_U^2(\\sigma_X^2 + \\sigma_U^2)}{(n-1)(\\sigma_X^2)^2}\n\\end{aligned}\n$$\nFinally, we substitute this back into the full expression for the approximate expectation:\n$$\n\\mathbb{E}[\\hat{\\beta}_1] \\approx \\beta_1 \\left( 1 + \\frac{2\\sigma_U^2(\\sigma_X^2 + \\sigma_U^2)}{(n-1)(\\sigma_X^2)^2} \\right)\n$$\nThis is the requested first-order approximation to the expectation of the corrected estimator. The bias is the second term, which is of order $O(1/n)$ and positive (assuming $\\beta_1 \\neq 0$), indicating that the estimator tends to overestimate the magnitude of $\\beta_1$ in finite samples.",
            "answer": "$$\n\\boxed{\\beta_1 \\left( 1 + \\frac{2\\sigma_U^2 (\\sigma_X^2 + \\sigma_U^2)}{(n-1)(\\sigma_X^2)^2} \\right)}\n$$"
        },
        {
            "introduction": "Modern biomedical data, such as from single-cell genomics, often present complex error structures that go beyond simple additive noise. This practice  tackles the Zero-Inflated Negative Binomial (ZINB) model, a sophisticated tool for handling count data with excess zeros and overdispersion. You will derive the components of the Expectation-Maximization (EM) algorithm, a powerful iterative method for fitting latent variable models, providing you with the foundational understanding to model complex, high-dimensional biological datasets.",
            "id": "3884514",
            "problem": "A research team is modeling the distribution of messenger RNA molecule counts $y_1, \\dots, y_n$ for a single gene measured across $n$ cells in single-cell sequencing. Due to biochemical dropout events, a non-negligible fraction of zeros are structural rather than sampling zeros. The team chooses a Zero-Inflated Negative Binomial (ZINB) error model to capture this phenomenon, with parameters $ \\pi \\in [0,1] $ (probability of a structural zero), $ \\mu > 0 $ (mean of the Negative Binomial component), and $ \\alpha > 0 $ (dispersion of the Negative Binomial component).\n\nAssume the following foundational elements:\n- The model is a two-component mixture: with probability $ \\pi $ a sample is a structural zero, and with probability $ 1 - \\pi $ the count is generated by a Negative Binomial distribution with mean $ \\mu $ and dispersion $ \\alpha $.\n- The Negative Binomial distribution is parameterized by $ r = 1/\\alpha $ and $ p = r/(r + \\mu) = 1/(1 + \\alpha \\mu) $, so that its probability mass function is\n$$\nf_{\\mathrm{NB}}(k \\mid \\mu, \\alpha) \\;=\\; \\frac{\\Gamma(k + r)}{\\Gamma(r)\\,k!}\\,p^{r}\\,(1-p)^{k} \\quad \\text{for } k \\in \\{0,1,2,\\dots\\}.\n$$\n\nStarting from the mixture model definition and the above Negative Binomial parameterization, do the following:\n1. Derive the complete-data likelihood by introducing latent indicator variables $ Z_i \\in \\{0,1\\} $ where $ Z_i = 1 $ denotes that $ y_i $ arose from the structural zero component.\n2. Marginalize over $ Z_i $ to obtain the observed-data likelihood $ L(\\pi,\\mu,\\alpha \\mid y_1,\\dots,y_n) $ and its log-likelihood.\n3. Use the Expectation-Maximization (EM) framework, defining the responsibilities $ \\gamma_i = \\mathbb{E}[Z_i \\mid y_i, \\pi, \\mu, \\alpha] $, to outline the E-step and the M-step for estimating $ (\\pi,\\mu,\\alpha) $ from the observed counts. Explicitly specify how to update $ \\pi $ in closed form and write down the weighted log-likelihood used to update $ \\mu $ and $ \\alpha $ for the Negative Binomial component.\n4. Focusing on the E-step, derive the posterior responsibility of the structural zero component for an observed zero count $ y_i = 0 $ as a closed-form analytic expression in terms of $ \\pi $, $ \\mu $, and $ \\alpha $, simplifying it using the expression for $ f_{\\mathrm{NB}}(0 \\mid \\mu, \\alpha) $.\n\nProvide your final answer as the simplified analytic expression for the E-step responsibility for a single observed zero $ y_i = 0 $ in terms of $ \\pi $, $ \\mu $, and $ \\alpha $ only. No numerical rounding is required, and no units are involved because this is a dimensionless probability. For clarity, abbreviations must be defined on first use, such as Zero-Inflated Negative Binomial (ZINB) and Expectation-Maximization (EM).",
            "solution": "The problem statement is found to be valid. It is scientifically grounded, well-posed, and objective. It presents a standard problem in computational biology and statistical modeling: deriving the components of the Expectation-Maximization (EM) algorithm for a Zero-Inflated Negative Binomial (ZINB) model. The definitions and parameterizations are consistent with established literature. We will proceed with the derivation.\n\nLet $\\theta = (\\pi, \\mu, \\alpha)$ be the vector of parameters, where $\\pi \\in [0,1]$ is the zero-inflation probability, $\\mu > 0$ is the mean of the Negative Binomial (NB) component, and $\\alpha > 0$ is the dispersion of the NB component. The observed data are the counts $y_1, \\dots, y_n$.\n\n### 1. Complete-Data Likelihood\n\nTo derive the complete-data likelihood, we introduce a set of latent indicator variables $Z_i \\in \\{0,1\\}$ for each observation $y_i$. Let $Z_i = 1$ if the observation $y_i$ is a structural zero (arising from the zero-inflation component), and $Z_i = 0$ if it is generated by the Negative Binomial component. The probability of $Z_i$ is given by a Bernoulli distribution with parameter $\\pi$, such that $P(Z_i=1 \\mid \\pi) = \\pi$ and $P(Z_i=0 \\mid \\pi) = 1-\\pi$. This can be written compactly as $P(z_i \\mid \\pi) = \\pi^{z_i}(1-\\pi)^{1-z_i}$.\n\nThe conditional probability of an observation $y_i$ given the latent variable $z_i$ is:\n- If $z_i=1$, the observation must be a structural zero, so $P(y_i \\mid Z_i=1, \\theta) = \\mathbb{I}(y_i=0)$, where $\\mathbb{I}(\\cdot)$ is the indicator function.\n- If $z_i=0$, the observation is drawn from the NB distribution, so $P(y_i \\mid Z_i=0, \\theta) = f_{\\mathrm{NB}}(y_i \\mid \\mu, \\alpha)$.\n\nThe joint probability of a single complete observation $(y_i, z_i)$ is $P(y_i, z_i \\mid \\theta) = P(y_i \\mid z_i, \\theta) P(z_i \\mid \\theta)$. This can be written as:\n$$ P(y_i, z_i \\mid \\theta) = \\left[\\pi \\cdot \\mathbb{I}(y_i=0)\\right]^{z_i} \\left[(1-\\pi) f_{\\mathrm{NB}}(y_i \\mid \\mu, \\alpha)\\right]^{1-z_i} $$\nFor a set of $n$ independent and identically distributed observations, the complete-data likelihood $L_c(\\theta \\mid \\mathbf{y}, \\mathbf{z})$ is the product of these individual probabilities:\n$$ L_c(\\theta \\mid \\mathbf{y}, \\mathbf{z}) = \\prod_{i=1}^n P(y_i, z_i \\mid \\theta) = \\prod_{i=1}^n \\left[\\pi \\cdot \\mathbb{I}(y_i=0)\\right]^{z_i} \\left[(1-\\pi) f_{\\mathrm{NB}}(y_i \\mid \\mu, \\alpha)\\right]^{1-z_i} $$\nSince $z_i=1$ implies $y_i=0$, the indicator function is implicitly handled, and the complete-data log-likelihood $\\ell_c(\\theta \\mid \\mathbf{y}, \\mathbf{z}) = \\ln L_c$ is:\n$$ \\ell_c(\\theta \\mid \\mathbf{y}, \\mathbf{z}) = \\sum_{i=1}^n \\left( z_i \\ln\\pi + (1-z_i) \\ln(1-\\pi) + (1-z_i) \\ln f_{\\mathrm{NB}}(y_i \\mid \\mu, \\alpha) \\right) $$\n\n### 2. Observed-Data Likelihood\n\nThe observed-data likelihood is obtained by marginalizing (summing) over the latent variables $Z_i$ for each observation $y_i$.\nThe probability of a single observation $y_i$ is $P(y_i \\mid \\theta) = \\sum_{z_i \\in \\{0,1\\}} P(y_i, z_i \\mid \\theta)$.\n\nWe consider two cases for the value of $y_i$:\n- If $y_i > 0$, it cannot be a structural zero, so $Z_i$ must be $0$. The probability is:\n  $$ P(y_i \\mid \\theta) = P(y_i \\mid Z_i=0, \\theta) P(Z_i=0 \\mid \\theta) = (1-\\pi) f_{\\mathrm{NB}}(y_i \\mid \\mu, \\alpha) \\quad \\text{for } y_i > 0. $$\n- If $y_i = 0$, it could be either a structural zero ($Z_i=1$) or a sampling zero from the NB distribution ($Z_i=0$). The probability is the sum of these two mutually exclusive events:\n  $$ P(y_i=0 \\mid \\theta) = P(y_i=0 \\mid Z_i=1, \\theta) P(Z_i=1 \\mid \\theta) + P(y_i=0 \\mid Z_i=0, \\theta) P(Z_i=0 \\mid \\theta) $$\n  $$ P(y_i=0 \\mid \\theta) = 1 \\cdot \\pi + f_{\\mathrm{NB}}(0 \\mid \\mu, \\alpha) \\cdot (1-\\pi) = \\pi + (1-\\pi) f_{\\mathrm{NB}}(0 \\mid \\mu, \\alpha). $$\nThe observed-data likelihood for all $n$ observations is $L(\\theta \\mid \\mathbf{y}) = \\prod_{i=1}^n P(y_i \\mid \\theta)$.\nLet $I_0 = \\{i \\mid y_i=0\\}$ and $I_+ = \\{i \\mid y_i>0\\}$ be the index sets for zero and non-zero counts, respectively. Let $n_0 = |I_0|$ and $n_+ = |I_+|$, with $n = n_0 + n_+$.\n$$ L(\\pi, \\mu, \\alpha \\mid \\mathbf{y}) = \\prod_{i \\in I_0} \\left[ \\pi + (1-\\pi) f_{\\mathrm{NB}}(0 \\mid \\mu, \\alpha) \\right] \\prod_{i \\in I_+} \\left[ (1-\\pi) f_{\\mathrm{NB}}(y_i \\mid \\mu, \\alpha) \\right] $$\nThe observed-data log-likelihood is $\\ell(\\theta \\mid \\mathbf{y}) = \\ln L(\\theta \\mid \\mathbf{y})$:\n$$ \\ell(\\pi, \\mu, \\alpha \\mid \\mathbf{y}) = \\sum_{i \\in I_0} \\ln\\left[ \\pi + (1-\\pi) f_{\\mathrm{NB}}(0 \\mid \\mu, \\alpha) \\right] + \\sum_{i \\in I_+} \\ln\\left[ (1-\\pi) f_{\\mathrm{NB}}(y_i \\mid \\mu, \\alpha) \\right] $$\n\n### 3. Expectation-Maximization (EM) Framework\n\nThe EM algorithm is an iterative procedure for finding maximum likelihood estimates of parameters in statistical models with latent variables. It consists of an Expectation (E) step and a Maximization (M) step. Let $\\theta^{(t)} = (\\pi^{(t)}, \\mu^{(t)}, \\alpha^{(t)})$ be the parameter estimates at iteration $t$.\n\n**E-step**: In this step, we compute the expected value of the complete-data log-likelihood, with respect to the conditional distribution of the latent variables $\\mathbf{Z}$ given the observed data $\\mathbf{y}$ and the current parameter estimates $\\theta^{(t)}$. This involves calculating the posterior probabilities of the latent variables, known as responsibilities. The responsibility $\\gamma_i$ is the posterior probability that observation $y_i$ belongs to the structural zero component:\n$$ \\gamma_i = \\mathbb{E}[Z_i \\mid y_i, \\theta^{(t)}] = P(Z_i=1 \\mid y_i, \\theta^{(t)}) $$\nUsing Bayes' theorem:\n$$ \\gamma_i = \\frac{P(y_i \\mid Z_i=1, \\theta^{(t)}) P(Z_i=1 \\mid \\theta^{(t)})}{P(y_i \\mid \\theta^{(t)})} $$\nIf $y_i > 0$, then $P(y_i \\mid Z_i=1, \\theta^{(t)})=0$, so $\\gamma_i = 0$.\nIf $y_i = 0$, the responsibility is:\n$$ \\gamma_i = \\frac{\\pi^{(t)}}{\\pi^{(t)} + (1-\\pi^{(t)}) f_{\\mathrm{NB}}(0 \\mid \\mu^{(t)}, \\alpha^{(t)})} \\quad \\text{for } y_i=0. $$\n\n**M-step**: In this step, we maximize the expected complete-data log-likelihood (the $Q$-function) with respect to the parameters $\\theta$ to obtain the updated estimates $\\theta^{(t+1)}$.\n$$ Q(\\theta \\mid \\theta^{(t)}) = \\sum_{i=1}^n \\left( \\gamma_i \\ln\\pi + (1-\\gamma_i) \\ln(1-\\pi) + (1-\\gamma_i) \\ln f_{\\mathrm{NB}}(y_i \\mid \\mu, \\alpha) \\right) $$\nWe can maximize the terms for $\\pi$ and $(\\mu, \\alpha)$ separately.\n\nTo update $\\pi$, we maximize $\\sum_{i=1}^n \\left( \\gamma_i \\ln\\pi + (1-\\gamma_i) \\ln(1-\\pi) \\right)$. Setting the derivative with respect to $\\pi$ to zero gives the closed-form update:\n$$ \\pi^{(t+1)} = \\frac{1}{n} \\sum_{i=1}^n \\gamma_i $$\nSince $\\gamma_i = 0$ for $y_i > 0$, this sum is only over the observations with $y_i=0$.\n\nTo update $\\mu$ and $\\alpha$, we maximize the weighted NB log-likelihood term:\n$$ Q_{\\mathrm{NB}}(\\mu, \\alpha) = \\sum_{i=1}^n (1-\\gamma_i) \\ln f_{\\mathrm{NB}}(y_i \\mid \\mu, \\alpha) $$\nThe observations with $y_i>0$ have weight $1-\\gamma_i = 1$, and observations with $y_i=0$ have weight $1-\\gamma_i$. This maximization typically requires numerical optimization methods as there is no general closed-form solution.\n\n### 4. Responsibility for an Observed Zero\n\nWe are asked to derive the closed-form analytic expression for the E-step responsibility for a single observed zero, $y_i = 0$. This is the posterior probability that an observed zero is a structural zero, given the parameters $\\pi$, $\\mu$, and $\\alpha$. From the E-step derivation, this is:\n$$ \\gamma_{i \\mid y_i=0} = P(Z_i=1 \\mid y_i=0, \\pi, \\mu, \\alpha) = \\frac{\\pi}{\\pi + (1-\\pi) f_{\\mathrm{NB}}(0 \\mid \\mu, \\alpha)} $$\nWe need to evaluate $f_{\\mathrm{NB}}(0 \\mid \\mu, \\alpha)$. The NB PMF is given as $f_{\\mathrm{NB}}(k \\mid \\mu, \\alpha) = \\frac{\\Gamma(k + r)}{\\Gamma(r)\\,k!}\\,p^{r}\\,(1-p)^{k}$, with $r=1/\\alpha$ and $p=1/(1+\\alpha\\mu)$.\nFor $k=0$, we have:\n$$ f_{\\mathrm{NB}}(0 \\mid \\mu, \\alpha) = \\frac{\\Gamma(0 + r)}{\\Gamma(r)\\,0!}\\,p^{r}\\,(1-p)^{0} $$\nSince $\\Gamma(r) \\neq 0$ for $r>0$ and $0!=1$, this simplifies to:\n$$ f_{\\mathrm{NB}}(0 \\mid \\mu, \\alpha) = \\frac{\\Gamma(r)}{\\Gamma(r) \\cdot 1} \\cdot p^r \\cdot 1 = p^r $$\nSubstituting the parameterizations for $p$ and $r$:\n$$ f_{\\mathrm{NB}}(0 \\mid \\mu, \\alpha) = \\left( \\frac{1}{1 + \\alpha\\mu} \\right)^{1/\\alpha} $$\nFinally, we substitute this expression back into the formula for the responsibility $\\gamma_{i \\mid y_i=0}$:\n$$ \\gamma_{i \\mid y_i=0} = \\frac{\\pi}{\\pi + (1-\\pi) \\left( \\frac{1}{1+\\alpha\\mu} \\right)^{1/\\alpha}} $$\nThis is the final simplified expression as requested.",
            "answer": "$$ \\boxed{ \\frac{\\pi}{\\pi + (1-\\pi) \\left( \\frac{1}{1+\\alpha\\mu} \\right)^{1/\\alpha}} } $$"
        }
    ]
}