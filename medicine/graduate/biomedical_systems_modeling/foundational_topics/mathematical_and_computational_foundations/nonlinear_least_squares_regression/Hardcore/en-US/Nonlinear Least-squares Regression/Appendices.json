{
    "hands_on_practices": [
        {
            "introduction": "At the heart of nonlinear least-squares regression lies the challenge of iteratively finding the parameter values that best fit the data. The Levenberg-Marquardt algorithm provides a robust method for this, intelligently navigating the complex error landscape. This practice  delves into the core mechanics of the algorithm, guiding you to derive the damped least-squares step using Singular Value Decomposition (SVD) and demonstrating how this technique provides stability in the face of ill-conditioned problems.",
            "id": "3911613",
            "problem": "Consider a two-parameter nonlinear model for a microdialysis-based cytokine concentration sensor in a biomedical system, with parameter vector $\\boldsymbol{\\theta} \\in \\mathbb{R}^{2}$ describing transport-rate and binding-affinity effects. The model is fit to $m=3$ measurements at times $t_{1}, t_{2}, t_{3}$, yielding a residual vector $\\boldsymbol{r}(\\boldsymbol{\\theta}) \\in \\mathbb{R}^{3}$ with components $r_{i}(\\boldsymbol{\\theta}) = y_{i}^{\\text{obs}} - y(t_{i}; \\boldsymbol{\\theta})$. In nonlinear least-squares regression, the objective is to minimize $S(\\boldsymbol{\\theta}) = \\frac{1}{2}\\|\\boldsymbol{r}(\\boldsymbol{\\theta})\\|_{2}^{2}$.\n\nAt a current iterate $\\boldsymbol{\\theta}_{0}$, the residual is $\\boldsymbol{r} = \\begin{pmatrix} 0.5 \\\\ -0.3 \\\\ 0.1 \\end{pmatrix}$, and the Jacobian matrix of partial derivatives with respect to the parameters, $\\boldsymbol{J}(\\boldsymbol{\\theta}_{0}) \\in \\mathbb{R}^{3 \\times 2}$, admits the thin singular value decomposition (SVD)\n$$\n\\boldsymbol{J} = \\boldsymbol{U}\\,\\boldsymbol{\\Sigma}\\,\\boldsymbol{V}^{\\mathsf{T}},\n$$\nwhere $\\boldsymbol{U} \\in \\mathbb{R}^{3 \\times 2}$ has orthonormal columns, $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{2 \\times 2}$ is diagonal with positive entries, and $\\boldsymbol{V} \\in \\mathbb{R}^{2 \\times 2}$ is orthogonal. The specific factors are\n$$\n\\boldsymbol{U} = \\begin{pmatrix}\n\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{6}} \\\\\n\\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{6}} \\\\\n0 & \\frac{2}{\\sqrt{6}}\n\\end{pmatrix},\\quad\n\\boldsymbol{\\Sigma} = \\begin{pmatrix}\n10 & 0 \\\\\n0 & \\frac{1}{10}\n\\end{pmatrix},\\quad\n\\boldsymbol{V} = \\begin{pmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{pmatrix}.\n$$\n\nStarting from the foundational definition of the nonlinear least-squares objective and its Gauss–Newton linearization, derive the damped least-squares (Levenberg–Marquardt) step $\\boldsymbol{p} \\in \\mathbb{R}^{2}$ from first principles by solving in the SVD basis with a positive damping parameter $\\lambda$. Explicitly show how the singular values enter the expression and explain how small singular values are treated by damping. Then, for the given factors and residual, evaluate the damped step for $\\lambda = \\frac{9}{10}$.\n\nExpress your final answer as a single row vector representing $\\boldsymbol{p}$, using exact analytical values with no rounding. The parameters are unitless for this computation, so no physical units should be included in the final answer.",
            "solution": "The problem is valid. It is a well-posed and self-contained problem in numerical optimization, specifically nonlinear least-squares regression using the Levenberg-Marquardt algorithm. The context is scientifically grounded, and all necessary data for the derivation and calculation are provided.\n\nThe objective in nonlinear least-squares is to find the parameter vector $\\boldsymbol{\\theta}$ that minimizes the sum of squared residuals, given by the function $S(\\boldsymbol{\\theta})$:\n$$\nS(\\boldsymbol{\\theta}) = \\frac{1}{2} \\|\\boldsymbol{r}(\\boldsymbol{\\theta})\\|_{2}^{2} = \\frac{1}{2} \\boldsymbol{r}(\\boldsymbol{\\theta})^{\\mathsf{T}}\\boldsymbol{r}(\\boldsymbol{\\theta})\n$$\nIterative methods are used to find the minimum. Given a current iterate $\\boldsymbol{\\theta}_{k}$, we seek a step $\\boldsymbol{p}$ to find the next iterate $\\boldsymbol{\\theta}_{k+1} = \\boldsymbol{\\theta}_{k} + \\boldsymbol{p}$. The Gauss-Newton method linearizes the residual function around $\\boldsymbol{\\theta}_{k}$:\n$$\n\\boldsymbol{r}(\\boldsymbol{\\theta}_{k} + \\boldsymbol{p}) \\approx \\boldsymbol{r}(\\boldsymbol{\\theta}_{k}) + \\boldsymbol{J}(\\boldsymbol{\\theta}_{k})\\boldsymbol{p}\n$$\nwhere $\\boldsymbol{J}(\\boldsymbol{\\theta}_{k})$ is the Jacobian matrix of $\\boldsymbol{r}$ at $\\boldsymbol{\\theta}_{k}$. Let $\\boldsymbol{r}_k = \\boldsymbol{r}(\\boldsymbol{\\theta}_k)$ and $\\boldsymbol{J}_k = \\boldsymbol{J}(\\boldsymbol{\\theta}_k)$. We minimize the linearized objective function:\n$$\n\\min_{\\boldsymbol{p}} \\frac{1}{2} \\|\\boldsymbol{r}_k + \\boldsymbol{J}_k \\boldsymbol{p}\\|_2^2\n$$\nThis is a linear least-squares problem, and its solution $\\boldsymbol{p}$ satisfies the normal equations:\n$$\n(\\boldsymbol{J}_k^{\\mathsf{T}}\\boldsymbol{J}_k)\\boldsymbol{p} = -\\boldsymbol{J}_k^{\\mathsf{T}}\\boldsymbol{r}_k\n$$\nThe damped least-squares, or Levenberg-Marquardt, method addresses potential ill-conditioning of the matrix $\\boldsymbol{J}_k^{\\mathsf{T}}\\boldsymbol{J}_k$ by adding a positive damping term $\\lambda > 0$. The step $\\boldsymbol{p}$ is then found by solving the modified system. We drop the subscript $k$ for the current iterate $\\boldsymbol{\\theta}_0$.\n$$\n(\\boldsymbol{J}^{\\mathsf{T}}\\boldsymbol{J} + \\lambda \\boldsymbol{I})\\boldsymbol{p} = -\\boldsymbol{J}^{\\mathsf{T}}\\boldsymbol{r}\n$$\nThis is the foundational equation from which we derive the solution. We are given the thin Singular Value Decomposition (SVD) of the Jacobian, $\\boldsymbol{J} = \\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{V}^{\\mathsf{T}}$. We substitute this into the equation.\n\nFirst, we express the terms $\\boldsymbol{J}^{\\mathsf{T}}\\boldsymbol{J}$ and $\\boldsymbol{J}^{\\mathsf{T}}\\boldsymbol{r}$ using the SVD factors:\n$$\n\\boldsymbol{J}^{\\mathsf{T}}\\boldsymbol{J} = (\\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{V}^{\\mathsf{T}})^{\\mathsf{T}}(\\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{V}^{\\mathsf{T}}) = \\boldsymbol{V}\\boldsymbol{\\Sigma}^{\\mathsf{T}}\\boldsymbol{U}^{\\mathsf{T}}\\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{V}^{\\mathsf{T}}\n$$\nSince the columns of $\\boldsymbol{U}$ are orthonormal, $\\boldsymbol{U}^{\\mathsf{T}}\\boldsymbol{U} = \\boldsymbol{I}$. Also, $\\boldsymbol{\\Sigma}$ is diagonal, so $\\boldsymbol{\\Sigma}^{\\mathsf{T}} = \\boldsymbol{\\Sigma}$.\n$$\n\\boldsymbol{J}^{\\mathsf{T}}\\boldsymbol{J} = \\boldsymbol{V}\\boldsymbol{\\Sigma}\\boldsymbol{I}\\boldsymbol{\\Sigma}\\boldsymbol{V}^{\\mathsf{T}} = \\boldsymbol{V}\\boldsymbol{\\Sigma}^{2}\\boldsymbol{V}^{\\mathsf{T}}\n$$\nThe right-hand side becomes:\n$$\n\\boldsymbol{J}^{\\mathsf{T}}\\boldsymbol{r} = (\\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{V}^{\\mathsf{T}})^{\\mathsf{T}}\\boldsymbol{r} = \\boldsymbol{V}\\boldsymbol{\\Sigma}^{\\mathsf{T}}\\boldsymbol{U}^{\\mathsf{T}}\\boldsymbol{r} = \\boldsymbol{V}\\boldsymbol{\\Sigma}\\boldsymbol{U}^{\\mathsf{T}}\\boldsymbol{r}\n$$\nSubstituting these into the Levenberg-Marquardt equation gives:\n$$\n(\\boldsymbol{V}\\boldsymbol{\\Sigma}^{2}\\boldsymbol{V}^{\\mathsf{T}} + \\lambda \\boldsymbol{I})\\boldsymbol{p} = -\\boldsymbol{V}\\boldsymbol{\\Sigma}\\boldsymbol{U}^{\\mathsf{T}}\\boldsymbol{r}\n$$\nSince $\\boldsymbol{V}$ is an orthogonal matrix, $\\boldsymbol{V}\\boldsymbol{V}^{\\mathsf{T}} = \\boldsymbol{I}$. We can write $\\lambda \\boldsymbol{I} = \\lambda \\boldsymbol{V}\\boldsymbol{V}^{\\mathsf{T}}$ and substitute it into the left-hand side:\n$$\n(\\boldsymbol{V}\\boldsymbol{\\Sigma}^{2}\\boldsymbol{V}^{\\mathsf{T}} + \\lambda \\boldsymbol{V}\\boldsymbol{V}^{\\mathsf{T}})\\boldsymbol{p} = \\boldsymbol{V}(\\boldsymbol{\\Sigma}^{2} + \\lambda \\boldsymbol{I})\\boldsymbol{V}^{\\mathsf{T}}\\boldsymbol{p} = -\\boldsymbol{V}\\boldsymbol{\\Sigma}\\boldsymbol{U}^{\\mathsf{T}}\\boldsymbol{r}\n$$\nLeft-multiplying by $\\boldsymbol{V}^{\\mathsf{T}}$ (which is $\\boldsymbol{V}^{-1}$) isolates the term containing $\\boldsymbol{p}$:\n$$\n\\boldsymbol{V}^{\\mathsf{T}}\\boldsymbol{V}(\\boldsymbol{\\Sigma}^{2} + \\lambda \\boldsymbol{I})\\boldsymbol{V}^{\\mathsf{T}}\\boldsymbol{p} = -\\boldsymbol{V}^{\\mathsf{T}}\\boldsymbol{V}\\boldsymbol{\\Sigma}\\boldsymbol{U}^{\\mathsf{T}}\\boldsymbol{r}\n$$\n$$\n(\\boldsymbol{\\Sigma}^{2} + \\lambda \\boldsymbol{I})\\boldsymbol{V}^{\\mathsf{T}}\\boldsymbol{p} = -\\boldsymbol{\\Sigma}\\boldsymbol{U}^{\\mathsf{T}}\\boldsymbol{r}\n$$\nThe matrix $(\\boldsymbol{\\Sigma}^{2} + \\lambda \\boldsymbol{I})$ is diagonal with entries $(\\sigma_j^2 + \\lambda)$, where $\\sigma_j$ are the singular values. Its inverse is also diagonal with entries $1/(\\sigma_j^2 + \\lambda)$. We can solve for $\\boldsymbol{V}^{\\mathsf{T}}\\boldsymbol{p}$:\n$$\n\\boldsymbol{V}^{\\mathsf{T}}\\boldsymbol{p} = -(\\boldsymbol{\\Sigma}^{2} + \\lambda \\boldsymbol{I})^{-1}\\boldsymbol{\\Sigma}\\boldsymbol{U}^{\\mathsf{T}}\\boldsymbol{r}\n$$\nFinally, left-multiplying by $\\boldsymbol{V}$ yields the expression for the step $\\boldsymbol{p}$:\n$$\n\\boldsymbol{p} = -\\boldsymbol{V}(\\boldsymbol{\\Sigma}^{2} + \\lambda \\boldsymbol{I})^{-1}\\boldsymbol{\\Sigma}\\boldsymbol{U}^{\\mathsf{T}}\\boldsymbol{r}\n$$\nTo explicitly show how singular values enter the expression, we can write $\\boldsymbol{p}$ as a sum. Let $\\boldsymbol{u}_j$ and $\\boldsymbol{v}_j$ be the columns of $\\boldsymbol{U}$ and $\\boldsymbol{V}$ respectively. The step vector $\\boldsymbol{p}$ is:\n$$\n\\boldsymbol{p} = \\sum_{j=1}^{n} \\left( \\frac{-\\sigma_j}{\\sigma_j^2 + \\lambda} \\right) (\\boldsymbol{u}_j^{\\mathsf{T}}\\boldsymbol{r}) \\boldsymbol{v}_j\n$$\nThis expression shows that the step $\\boldsymbol{p}$ is a linear combination of the right singular vectors $\\boldsymbol{v}_j$. The coefficient for each $\\boldsymbol{v}_j$ is determined by the corresponding singular value $\\sigma_j$, the damping parameter $\\lambda$, and the projection of the residual $\\boldsymbol{r}$ onto the corresponding left singular vector $\\boldsymbol{u}_j$.\n\nThe role of damping is to stabilize the step calculation when small singular values are present. The scaling factor for the $j$-th component is $\\frac{\\sigma_j}{\\sigma_j^2 + \\lambda}$.\nIf $\\sigma_j$ is large (i.e., $\\sigma_j^2 \\gg \\lambda$), this factor is approximately $\\frac{\\sigma_j}{\\sigma_j^2} = \\frac{1}{\\sigma_j}$. This is the same scaling as the undamped Gauss-Newton step.\nIf $\\sigma_j$ is small (i.e., $\\sigma_j^2 \\ll \\lambda$), this factor is approximately $\\frac{\\sigma_j}{\\lambda}$. Without damping, the factor would be $\\frac{1}{\\sigma_j}$, which would be very large, causing a large and unreliable step in the direction $\\boldsymbol{v}_j$. The damping parameter $\\lambda$ in the denominator ensures that the contribution from directions associated with small singular values is suppressed, preventing the step $\\boldsymbol{p}$ from becoming excessively large. This yields a more stable and robust optimization process.\n\nNow, we evaluate the damped step $\\boldsymbol{p}$ for the given data.\nThe residual is $\\boldsymbol{r} = \\begin{pmatrix} 0.5 \\\\ -0.3 \\\\ 0.1 \\end{pmatrix}$. The damping parameter is $\\lambda = \\frac{9}{10}$.\nThe SVD factors are:\n$\\boldsymbol{U} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{6}} \\\\ \\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{6}} \\\\ 0 & \\frac{2}{\\sqrt{6}} \\end{pmatrix}$, $\\boldsymbol{\\Sigma} = \\begin{pmatrix} 10 & 0 \\\\ 0 & \\frac{1}{10} \\end{pmatrix}$, $\\boldsymbol{V} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$.\nThe columns of $\\boldsymbol{U}$ are $\\boldsymbol{u}_1 = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\\\ 0 \\end{pmatrix}$ and $\\boldsymbol{u}_2 = \\begin{pmatrix} \\frac{1}{\\sqrt{6}} \\\\ -\\frac{1}{\\sqrt{6}} \\\\ \\frac{2}{\\sqrt{6}} \\end{pmatrix}$.\nThe singular values are $\\sigma_1 = 10$ and $\\sigma_2 = \\frac{1}{10}$.\nSince $\\boldsymbol{V} = \\boldsymbol{I}$, we have $\\boldsymbol{p} = \\boldsymbol{V}^{\\mathsf{T}}\\boldsymbol{p}$, so $\\boldsymbol{p}$ can be computed directly from $\\boldsymbol{p} = -(\\boldsymbol{\\Sigma}^{2} + \\lambda \\boldsymbol{I})^{-1}\\boldsymbol{\\Sigma}\\boldsymbol{U}^{\\mathsf{T}}\\boldsymbol{r}$.\n\nFirst, calculate the vector $\\boldsymbol{U}^{\\mathsf{T}}\\boldsymbol{r}$:\n$$\n\\boldsymbol{u}_1^{\\mathsf{T}}\\boldsymbol{r} = \\frac{1}{\\sqrt{2}}(0.5) + \\frac{1}{\\sqrt{2}}(-0.3) + 0(0.1) = \\frac{0.2}{\\sqrt{2}} = \\frac{2}{10\\sqrt{2}} = \\frac{1}{5\\sqrt{2}}\n$$\n$$\n\\boldsymbol{u}_2^{\\mathsf{T}}\\boldsymbol{r} = \\frac{1}{\\sqrt{6}}(0.5) - \\frac{1}{\\sqrt{6}}(-0.3) + \\frac{2}{\\sqrt{6}}(0.1) = \\frac{1}{\\sqrt{6}}(0.5 + 0.3 + 0.2) = \\frac{1}{\\sqrt{6}}\n$$\nSo, $\\boldsymbol{U}^{\\mathsf{T}}\\boldsymbol{r} = \\begin{pmatrix} \\frac{1}{5\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{6}} \\end{pmatrix}$.\n\nNext, we compute the components of $\\boldsymbol{p}$.\nFor the first component, $p_1$:\n$$\np_1 = -\\left( \\frac{\\sigma_1}{\\sigma_1^2 + \\lambda} \\right) (\\boldsymbol{u}_1^{\\mathsf{T}}\\boldsymbol{r}) = -\\left( \\frac{10}{10^2 + \\frac{9}{10}} \\right) \\left( \\frac{1}{5\\sqrt{2}} \\right) = -\\left( \\frac{10}{100 + \\frac{9}{10}} \\right) \\left( \\frac{1}{5\\sqrt{2}} \\right)\n$$\n$$\np_1 = -\\left( \\frac{10}{\\frac{1009}{10}} \\right) \\left( \\frac{1}{5\\sqrt{2}} \\right) = -\\left( \\frac{100}{1009} \\right) \\left( \\frac{1}{5\\sqrt{2}} \\right) = -\\frac{20}{1009\\sqrt{2}} = -\\frac{20\\sqrt{2}}{1009 \\cdot 2} = -\\frac{10\\sqrt{2}}{1009}\n$$\nFor the second component, $p_2$:\n$$\np_2 = -\\left( \\frac{\\sigma_2}{\\sigma_2^2 + \\lambda} \\right) (\\boldsymbol{u}_2^{\\mathsf{T}}\\boldsymbol{r}) = -\\left( \\frac{\\frac{1}{10}}{(\\frac{1}{10})^2 + \\frac{9}{10}} \\right) \\left( \\frac{1}{\\sqrt{6}} \\right) = -\\left( \\frac{\\frac{1}{10}}{\\frac{1}{100} + \\frac{90}{100}} \\right) \\left( \\frac{1}{\\sqrt{6}} \\right)\n$$\n$$\np_2 = -\\left( \\frac{\\frac{1}{10}}{\\frac{91}{100}} \\right) \\left( \\frac{1}{\\sqrt{6}} \\right) = -\\left( \\frac{1}{10} \\cdot \\frac{100}{91} \\right) \\left( \\frac{1}{\\sqrt{6}} \\right) = -\\frac{10}{91\\sqrt{6}} = -\\frac{10\\sqrt{6}}{91 \\cdot 6} = -\\frac{10\\sqrt{6}}{546} = -\\frac{5\\sqrt{6}}{273}\n$$\nThe step vector $\\boldsymbol{p}$ is $\\begin{pmatrix} -\\frac{10\\sqrt{2}}{1009} \\\\ -\\frac{5\\sqrt{6}}{273} \\end{pmatrix}$.\nThe problem asks for the answer as a single row vector.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n-\\frac{10\\sqrt{2}}{1009} & -\\frac{5\\sqrt{6}}{273}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Biomedical models are grounded in physical reality, often requiring parameters like reaction rates or concentrations to be strictly positive. Directly applying a standard optimization algorithm can lead to nonsensical negative estimates. This practice  introduces an elegant solution: reparameterizing the model, for instance by using an exponential map like $\\theta_j = \\exp(\\phi_j)$. You will learn how to transform the least-squares objective function and, critically, its gradient with respect to the new, unconstrained parameters, a fundamental skill for robustly fitting mechanistic models.",
            "id": "3911605",
            "problem": "A researcher is calibrating a mechanistic model of an enzyme-catalyzed reaction in a biomedical assay. The measured initial reaction velocities at substrate concentrations $s_i$ are modeled by the Michaelis–Menten relation $f(s_i;\\boldsymbol{\\theta}) = \\dfrac{\\theta_1 s_i}{\\theta_2 + s_i}$, where $\\boldsymbol{\\theta} = (\\theta_1,\\theta_2)^{\\top}$ has strictly positive components corresponding to the maximum velocity and the Michaelis constant, respectively. Let the observed velocities be $y_i$ for $i=1,\\dots,n$, and assume independent additive noise with constant variance. The researcher uses nonlinear least-squares regression to estimate $\\boldsymbol{\\theta}$ by minimizing the objective $J(\\boldsymbol{\\theta}) = \\dfrac{1}{2}\\sum_{i=1}^{n}\\left(y_i - f(s_i;\\boldsymbol{\\theta})\\right)^2$.\n\nTo enforce positivity of parameters in a numerically stable way, the researcher reparameterizes $\\boldsymbol{\\theta}$ via an elementwise exponential map $\\boldsymbol{\\theta} = \\exp(\\boldsymbol{\\phi})$, where $\\boldsymbol{\\phi} = (\\phi_1,\\phi_2)^{\\top} \\in \\mathbb{R}^2$ and the exponential is applied componentwise, so that $\\theta_j = \\exp(\\phi_j)$ for $j=1,2$.\n\nStarting from the definition of nonlinear least-squares and the basic rules of multivariable calculus (product rule, chain rule), derive the transformed objective $J(\\boldsymbol{\\phi}) = J(\\exp(\\boldsymbol{\\phi}))$ and its gradient with respect to $\\boldsymbol{\\phi}$. Express your final result explicitly in terms of the data $\\{(s_i,y_i)\\}_{i=1}^{n}$ and the reparameterized variables $\\phi_1$ and $\\phi_2$, without introducing any additional parameters. Your derivation should be rigorous and should clearly justify each step from first principles.\n\nProvide your final answer as three expressions: the scalar objective $J(\\boldsymbol{\\phi})$, followed by the two scalar components of the gradient vector $\\nabla_{\\boldsymbol{\\phi}} J(\\boldsymbol{\\phi})$, namely $\\dfrac{\\partial J}{\\partial \\phi_1}$ and $\\dfrac{\\partial J}{\\partial \\phi_2}$. No numerical evaluation is required, and no rounding is needed. Do not include units in your final expressions.",
            "solution": "The problem statement is scientifically grounded, well-posed, and objective. It describes a standard scenario in biochemical modeling and parameter estimation. The use of Michaelis-Menten kinetics, nonlinear least-squares, and exponential reparameterization to enforce positivity are all established and valid techniques. The task is a direct application of multivariable calculus and is fully specified. Therefore, the problem is deemed valid and a solution will be provided.\n\nThe objective is to derive the transformed objective function $J(\\boldsymbol{\\phi})$ and its gradient $\\nabla_{\\boldsymbol{\\phi}} J(\\boldsymbol{\\phi})$ with respect to the reparameterized variables $\\boldsymbol{\\phi} = (\\phi_1, \\phi_2)^{\\top}$.\n\nThe original model and parameters are:\n$$f(s_i;\\boldsymbol{\\theta}) = \\frac{\\theta_1 s_i}{\\theta_2 + s_i}$$\nwhere $\\boldsymbol{\\theta} = (\\theta_1, \\theta_2)^{\\top}$.\n\nThe reparameterization is given by an elementwise exponential map:\n$$\\theta_1 = \\exp(\\phi_1)$$\n$$\\theta_2 = \\exp(\\phi_2)$$\nThis transformation correctly enforces the positivity constraints $\\theta_1 > 0$ and $\\theta_2 > 0$ for any real values of $\\phi_1$ and $\\phi_2$.\n\nFirst, we express the model function $f$ in terms of the new parameters $\\boldsymbol{\\phi}$:\n$$f(s_i; \\boldsymbol{\\phi}) = \\frac{\\exp(\\phi_1) s_i}{\\exp(\\phi_2) + s_i}$$\n\nThe objective function for nonlinear least-squares, $J(\\boldsymbol{\\theta})$, is given by:\n$$J(\\boldsymbol{\\theta}) = \\frac{1}{2}\\sum_{i=1}^{n}\\left(y_i - f(s_i;\\boldsymbol{\\theta})\\right)^2$$\nSubstituting the reparameterized model, we obtain the transformed objective function $J(\\boldsymbol{\\phi})$:\n$$J(\\boldsymbol{\\phi}) = \\frac{1}{2}\\sum_{i=1}^{n}\\left(y_i - \\frac{\\exp(\\phi_1) s_i}{\\exp(\\phi_2) + s_i}\\right)^2$$\nThis is the first required expression.\n\nNext, we derive the gradient of $J(\\boldsymbol{\\phi})$, which consists of the partial derivatives $\\frac{\\partial J}{\\partial \\phi_1}$ and $\\frac{\\partial J}{\\partial \\phi_2}$. We apply the chain rule. For a generic component $\\phi_j$ (where $j=1$ or $j=2$):\n$$\\frac{\\partial J}{\\partial \\phi_j} = \\frac{\\partial}{\\partial \\phi_j} \\left[ \\frac{1}{2}\\sum_{i=1}^{n}\\left(y_i - f(s_i;\\boldsymbol{\\phi})\\right)^2 \\right]$$\nApplying the chain rule and sum rule:\n$$\\frac{\\partial J}{\\partial \\phi_j} = \\frac{1}{2}\\sum_{i=1}^{n} 2 \\left(y_i - f(s_i;\\boldsymbol{\\phi})\\right) \\left(-\\frac{\\partial f(s_i;\\boldsymbol{\\phi})}{\\partial \\phi_j}\\right)$$\n$$\\frac{\\partial J}{\\partial \\phi_j} = -\\sum_{i=1}^{n} \\left(y_i - f(s_i;\\boldsymbol{\\phi})\\right) \\frac{\\partial f(s_i;\\boldsymbol{\\phi})}{\\partial \\phi_j}$$\n\nTo proceed, we must compute the partial derivatives of the model function $f$ with respect to $\\phi_1$ and $\\phi_2$. We use the chain rule again, treating $f$ as a composition of functions $f(\\boldsymbol{\\theta}(\\boldsymbol{\\phi}))$.\n$$\\frac{\\partial f}{\\partial \\phi_j} = \\frac{\\partial f}{\\partial \\theta_1}\\frac{\\partial \\theta_1}{\\partial \\phi_j} + \\frac{\\partial f}{\\partial \\theta_2}\\frac{\\partial \\theta_2}{\\partial \\phi_j}$$\nThe derivatives of the reparameterization are:\n$$\\frac{\\partial \\theta_1}{\\partial \\phi_1} = \\frac{\\partial}{\\partial \\phi_1}(\\exp(\\phi_1)) = \\exp(\\phi_1) = \\theta_1; \\quad \\frac{\\partial \\theta_1}{\\partial \\phi_2} = 0$$\n$$\\frac{\\partial \\theta_2}{\\partial \\phi_2} = \\frac{\\partial}{\\partial \\phi_2}(\\exp(\\phi_2)) = \\exp(\\phi_2) = \\theta_2; \\quad \\frac{\\partial \\theta_2}{\\partial \\phi_1} = 0$$\n\nThe partial derivatives of $f$ with respect to the original parameters $\\boldsymbol{\\theta}$ are:\n$$\\frac{\\partial f}{\\partial \\theta_1} = \\frac{\\partial}{\\partial \\theta_1}\\left(\\frac{\\theta_1 s_i}{\\theta_2 + s_i}\\right) = \\frac{s_i}{\\theta_2 + s_i}$$\n$$\\frac{\\partial f}{\\partial \\theta_2} = \\frac{\\partial}{\\partial \\theta_2}\\left(\\frac{\\theta_1 s_i}{\\theta_2 + s_i}\\right) = \\theta_1 s_i \\frac{\\partial}{\\partial \\theta_2}(\\theta_2 + s_i)^{-1} = -\\frac{\\theta_1 s_i}{(\\theta_2 + s_i)^2}$$\n\nNow we can compute $\\frac{\\partial f}{\\partial \\phi_1}$ and $\\frac{\\partial f}{\\partial \\phi_2}$.\nFor $\\phi_1$:\n$$\\frac{\\partial f}{\\partial \\phi_1} = \\frac{\\partial f}{\\partial \\theta_1}\\frac{\\partial \\theta_1}{\\partial \\phi_1} + \\frac{\\partial f}{\\partial \\theta_2}\\frac{\\partial \\theta_2}{\\partial \\phi_1} = \\left(\\frac{s_i}{\\theta_2 + s_i}\\right) \\theta_1 + 0 = \\frac{\\theta_1 s_i}{\\theta_2 + s_i} = f(s_i;\\boldsymbol{\\theta})$$\nSubstituting back $\\theta_j = \\exp(\\phi_j)$:\n$$\\frac{\\partial f(s_i;\\boldsymbol{\\phi})}{\\partial \\phi_1} = \\frac{\\exp(\\phi_1)s_i}{\\exp(\\phi_2) + s_i}$$\n\nFor $\\phi_2$:\n$$\\frac{\\partial f}{\\partial \\phi_2} = \\frac{\\partial f}{\\partial \\theta_1}\\frac{\\partial \\theta_1}{\\partial \\phi_2} + \\frac{\\partial f}{\\partial \\theta_2}\\frac{\\partial \\theta_2}{\\partial \\phi_2} = 0 + \\left(-\\frac{\\theta_1 s_i}{(\\theta_2 + s_i)^2}\\right) \\theta_2 = -\\frac{\\theta_1 \\theta_2 s_i}{(\\theta_2 + s_i)^2}$$\nSubstituting back $\\theta_j = \\exp(\\phi_j)$:\n$$\\frac{\\partial f(s_i;\\boldsymbol{\\phi})}{\\partial \\phi_2} = -\\frac{\\exp(\\phi_1) \\exp(\\phi_2) s_i}{(\\exp(\\phi_2) + s_i)^2}$$\n\nFinally, we substitute these derivatives into the expressions for the gradient components of $J(\\boldsymbol{\\phi})$.\nThe first component of the gradient is:\n$$\\frac{\\partial J}{\\partial \\phi_1} = -\\sum_{i=1}^{n} \\left(y_i - f(s_i;\\boldsymbol{\\phi})\\right) \\frac{\\partial f(s_i;\\boldsymbol{\\phi})}{\\partial \\phi_1}$$\n$$\\frac{\\partial J}{\\partial \\phi_1} = -\\sum_{i=1}^{n} \\left(y_i - \\frac{\\exp(\\phi_1) s_i}{\\exp(\\phi_2) + s_i}\\right) \\left(\\frac{\\exp(\\phi_1) s_i}{\\exp(\\phi_2) + s_i}\\right)$$\nThis can be written as:\n$$\\frac{\\partial J}{\\partial \\phi_1} = \\sum_{i=1}^{n} \\left(\\frac{\\exp(\\phi_1) s_i}{\\exp(\\phi_2) + s_i} - y_i\\right) \\left(\\frac{\\exp(\\phi_1) s_i}{\\exp(\\phi_2) + s_i}\\right)$$\n\nThe second component of the gradient is:\n$$\\frac{\\partial J}{\\partial \\phi_2} = -\\sum_{i=1}^{n} \\left(y_i - f(s_i;\\boldsymbol{\\phi})\\right) \\frac{\\partial f(s_i;\\boldsymbol{\\phi})}{\\partial \\phi_2}$$\n$$\\frac{\\partial J}{\\partial \\phi_2} = -\\sum_{i=1}^{n} \\left(y_i - \\frac{\\exp(\\phi_1) s_i}{\\exp(\\phi_2) + s_i}\\right) \\left(-\\frac{\\exp(\\phi_1) \\exp(\\phi_2) s_i}{(\\exp(\\phi_2) + s_i)^2}\\right)$$\n$$\\frac{\\partial J}{\\partial \\phi_2} = \\sum_{i=1}^{n} \\left(y_i - \\frac{\\exp(\\phi_1) s_i}{\\exp(\\phi_2) + s_i}\\right) \\left(\\frac{\\exp(\\phi_1) \\exp(\\phi_2) s_i}{(\\exp(\\phi_2) + s_i)^2}\\right)$$\n\nThese are the required expressions for the objective function and the two components of its gradient, expressed explicitly in terms of the data and the parameters $\\phi_1$ and $\\phi_2$.",
            "answer": "$$ \\boxed{ \\begin{pmatrix} \\frac{1}{2}\\sum_{i=1}^{n}\\left(y_i - \\frac{\\exp(\\phi_1) s_i}{\\exp(\\phi_2) + s_i}\\right)^2 & \\sum_{i=1}^{n} \\left(\\frac{\\exp(\\phi_1) s_i}{\\exp(\\phi_2) + s_i} - y_i\\right) \\left(\\frac{\\exp(\\phi_1) s_i}{\\exp(\\phi_2) + s_i}\\right) & \\sum_{i=1}^{n} \\left(y_i - \\frac{\\exp(\\phi_1) s_i}{\\exp(\\phi_2) + s_i}\\right) \\left(\\frac{\\exp(\\phi_1) \\exp(\\phi_2) s_i}{(\\exp(\\phi_2) + s_i)^2}\\right) \\end{pmatrix} } $$"
        },
        {
            "introduction": "After finding the best-fit parameters, the job is not complete; a point estimate is of limited use without a measure of its uncertainty. This final practice closes the loop on the modeling process, moving from estimation to interpretation. You will learn how to translate the statistical uncertainty of an estimated parameter, often calculated in a transformed space for numerical stability, back into a physically meaningful confidence interval for the original parameter . This skill is essential for reporting results and assessing the reliability of your model.",
            "id": "3911614",
            "problem": "A pharmacokinetic one-compartment intravenous bolus model for plasma concentration is given by $C(t;\\theta)=\\frac{D}{V}\\exp(-k t)$, where $D$ is a known dose, $V$ is the distribution volume, and $k$ is the elimination rate constant. To enforce positivity during estimation, the elimination rate is parameterized as $\\xi=\\ln(k)$. Concentration measurements are modeled as $y_i=C(t_i;\\theta)+\\varepsilon_i$, where the measurement errors $\\varepsilon_i$ are independent and identically distributed with $\\varepsilon_i\\sim\\mathcal{N}(0,\\sigma^2)$.\n\nThe parameters are estimated by Nonlinear Least Squares (NLS), yielding the estimate $\\hat{\\xi}$ for $\\xi$. Under standard large-sample regularity assumptions for NLS estimators in nonlinear regression, $\\hat{\\xi}$ is asymptotically normal with covariance that can be approximated from the Gauss–Newton linearization via the residual variance and the sensitivity (Jacobian) of the model with respect to the parameters.\n\nSuppose a dataset fit produced $\\hat{\\xi}=-2.40$ and an estimated asymptotic variance for $\\hat{\\xi}$ of $\\widehat{\\Sigma}_{\\xi\\xi}=0.0081$. Using the asymptotic normality of $\\hat{\\xi}$ and the monotonicity of the exponential transformation that maps $\\xi$ to $k=\\exp(\\xi)$, construct a two-sided confidence interval at confidence level $0.95$ for $k$ by first forming the interval for $\\xi$ and then mapping its endpoints to the $k$-scale. Compute the upper endpoint of this $0.95$ confidence interval for $k$.\n\nExpress your final answer in $\\mathrm{min}^{-1}$. Round your final numerical value to four significant figures.",
            "solution": "The problem requires the construction of a $0.95$ confidence interval for the elimination rate constant, $k$, based on an estimate of its logarithm, $\\xi = \\ln(k)$. The procedure involves first establishing a confidence interval for $\\xi$ and then transforming its endpoints to the scale of $k$.\n\nWe are given the estimate for $\\xi$ as $\\hat{\\xi} = -2.40$, and its estimated asymptotic variance as $\\widehat{\\Sigma}_{\\xi\\xi} = 0.0081$.\n\nUnder the assumption that the estimator $\\hat{\\xi}$ is asymptotically normally distributed, its distribution can be approximated as:\n$$\n\\hat{\\xi} \\sim \\mathcal{N}(\\xi, \\widehat{\\Sigma}_{\\xi\\xi})\n$$\nwhere $\\xi$ is the true (unknown) value of the parameter. The standard error of the estimate $\\hat{\\xi}$ is the square root of its variance:\n$$\nSE(\\hat{\\xi}) = \\sqrt{\\widehat{\\Sigma}_{\\xi\\xi}} = \\sqrt{0.0081} = 0.09\n$$\n\nA two-sided confidence interval for $\\xi$ with a confidence level of $1-\\alpha$ is constructed as:\n$$\n[\\hat{\\xi} - z_{1-\\alpha/2} \\cdot SE(\\hat{\\xi}), \\quad \\hat{\\xi} + z_{1-\\alpha/2} \\cdot SE(\\hat{\\xi})]\n$$\nFor a confidence level of $0.95$, we have $1-\\alpha = 0.95$, which gives $\\alpha = 0.05$. The critical value from the standard normal distribution is $z_{1-\\alpha/2} = z_{1-0.025} = z_{0.975}$. This value is approximately $1.96$.\n\nThe $0.95$ confidence interval for $\\xi$ is therefore:\n$$\n[\\xi_L, \\xi_U] = [-2.40 - 1.96 \\times 0.09, \\quad -2.40 + 1.96 \\times 0.09]\n$$\n$$\n[\\xi_L, \\xi_U] = [-2.40 - 0.1764, \\quad -2.40 + 0.1764]\n$$\n$$\n[\\xi_L, \\xi_U] = [-2.5764, \\quad -2.2236]\n$$\n\nThe relationship between $k$ and $\\xi$ is given by $k = \\exp(\\xi)$. Since the exponential function, $\\exp(\\cdot)$, is a strictly monotonic increasing function, the confidence interval for $k$ can be obtained by applying the transformation to the endpoints of the confidence interval for $\\xi$.\nThe confidence interval for $k$ is $[k_L, k_U]$, where:\n$$\nk_L = \\exp(\\xi_L)\n$$\n$$\nk_U = \\exp(\\xi_U)\n$$\nThe problem asks for the upper endpoint of this confidence interval, $k_U$.\n\nWe calculate $k_U$ using the value of $\\xi_U$:\n$$\nk_U = \\exp(\\xi_U) = \\exp(-2.2236)\n$$\nPerforming the numerical calculation:\n$$\nk_U \\approx 0.10822605...\n$$\nThe problem requires the final answer to be rounded to four significant figures.\n$$\nk_U \\approx 0.1082\n$$\nThe units of $k$ are specified as $\\mathrm{min}^{-1}$, which is consistent with an elimination rate constant.",
            "answer": "$$\n\\boxed{0.1082}\n$$"
        }
    ]
}