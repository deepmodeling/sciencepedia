{
    "hands_on_practices": [
        {
            "introduction": "在生物医学建模中，许多参数（如速率常数或浓度）在物理上必须为正值。此练习将引导您掌握一种处理此类约束的强大技术：参数重整化 (reparameterization)。通过应用指数变换，我们将一个有约束的优化问题转化为一个无约束的问题，然后运用多元微积分中的链式法则来推导新的目标函数及其梯度，这是实现稳健的非线性回归算法的关键一步。",
            "id": "3911605",
            "problem": "一位研究人员正在校准一个生物医学分析中酶促反应的机制模型。在底物浓度为 $s_i$ 时，测得的初始反应速率由 Michaelis–Menten 关系 $f(s_i;\\boldsymbol{\\theta}) = \\frac{\\theta_1 s_i}{\\theta_2 + s_i}$ 建模，其中 $\\boldsymbol{\\theta} = (\\theta_1,\\theta_2)^{\\top}$ 的分量严格为正，分别对应最大速率和 Michaelis 常数。设观测到的速率为 $y_i$（其中 $i=1,\\dots,n$），并假设存在方差恒定的独立加性噪声。研究人员使用非线性最小二乘回归，通过最小化目标函数 $J(\\boldsymbol{\\theta}) = \\frac{1}{2}\\sum_{i=1}^{n}\\left(y_i - f(s_i;\\boldsymbol{\\theta})\\right)^2$ 来估计 $\\boldsymbol{\\theta}$。\n\n为了以数值稳定的方式强制参数为正，研究人员通过逐元素的指数映射 $\\boldsymbol{\\theta} = \\exp(\\boldsymbol{\\phi})$ 对 $\\boldsymbol{\\theta}$ 进行重参数化，其中 $\\boldsymbol{\\phi} = (\\phi_1,\\phi_2)^{\\top} \\in \\mathbb{R}^2$ 且指数函数逐分量作用，因此 $\\theta_j = \\exp(\\phi_j)$（$j=1,2$）。\n\n从非线性最小二乘的定义和多变量微积分的基本法则（乘法法则、链式法则）出发，推导变换后的目标函数 $J(\\boldsymbol{\\phi}) = J(\\exp(\\boldsymbol{\\phi}))$ 及其关于 $\\boldsymbol{\\phi}$ 的梯度。请用数据 $\\{(s_i,y_i)\\}_{i=1}^{n}$ 和重参数化后的变量 $\\phi_1$、$\\phi_2$ 明确表示您的最终结果，不要引入任何额外的参数。您的推导应严谨，并从第一性原理出发清晰地论证每一步。\n\n请以三个表达式的形式给出您的最终答案：标量目标函数 $J(\\boldsymbol{\\phi})$，以及梯度向量 $\\nabla_{\\boldsymbol{\\phi}} J(\\boldsymbol{\\phi})$ 的两个标量分量，即 $\\frac{\\partial J}{\\partial \\phi_1}$ 和 $\\frac{\\partial J}{\\partial \\phi_2}$。不需要进行数值计算，也不需要四舍五入。最终表达式中不要包含单位。",
            "solution": "该问题陈述具有科学依据，是适定且客观的。它描述了生物化学建模和参数估计中的一个标准场景。使用 Michaelis-Menten 动力学、非线性最小二乘法以及为强制正性而进行的指数重参数化都是成熟且有效的技术。该任务是多变量微积分的直接应用，并且已完全明确。因此，该问题被视为有效，并将提供解答。\n\n目标是推导变换后的目标函数 $J(\\boldsymbol{\\phi})$ 及其相对于重参数化变量 $\\boldsymbol{\\phi} = (\\phi_1, \\phi_2)^{\\top}$ 的梯度 $\\nabla_{\\boldsymbol{\\phi}} J(\\boldsymbol{\\phi})$。\n\n原始模型和参数为：\n$$f(s_i;\\boldsymbol{\\theta}) = \\frac{\\theta_1 s_i}{\\theta_2 + s_i}$$\n其中 $\\boldsymbol{\\theta} = (\\theta_1, \\theta_2)^{\\top}$。\n\n重参数化由逐元素的指数映射给出：\n$$\\theta_1 = \\exp(\\phi_1)$$\n$$\\theta_2 = \\exp(\\phi_2)$$\n对于任意实数值的 $\\phi_1$ 和 $\\phi_2$，此变换能正确地强制施加正性约束 $\\theta_1 > 0$ 和 $\\theta_2 > 0$。\n\n首先，我们用新参数 $\\boldsymbol{\\phi}$ 表示模型函数 $f$：\n$$f(s_i; \\boldsymbol{\\phi}) = \\frac{\\exp(\\phi_1) s_i}{\\exp(\\phi_2) + s_i}$$\n\n非线性最小二乘的目标函数 $J(\\boldsymbol{\\theta})$ 由下式给出：\n$$J(\\boldsymbol{\\theta}) = \\frac{1}{2}\\sum_{i=1}^{n}\\left(y_i - f(s_i;\\boldsymbol{\\theta})\\right)^2$$\n代入重参数化后的模型，我们得到变换后的目标函数 $J(\\boldsymbol{\\phi})$：\n$$J(\\boldsymbol{\\phi}) = \\frac{1}{2}\\sum_{i=1}^{n}\\left(y_i - \\frac{\\exp(\\phi_1) s_i}{\\exp(\\phi_2) + s_i}\\right)^2$$\n这是第一个要求的表达式。\n\n接下来，我们推导 $J(\\boldsymbol{\\phi})$ 的梯度，它由偏导数 $\\frac{\\partial J}{\\partial \\phi_1}$ 和 $\\frac{\\partial J}{\\partial \\phi_2}$ 组成。我们应用链式法则。对于一个通用分量 $\\phi_j$（其中 $j=1$ 或 $j=2$）：\n$$\\frac{\\partial J}{\\partial \\phi_j} = \\frac{\\partial}{\\partial \\phi_j} \\left[ \\frac{1}{2}\\sum_{i=1}^{n}\\left(y_i - f(s_i;\\boldsymbol{\\phi})\\right)^2 \\right]$$\n应用链式法则和求和法则：\n$$\\frac{\\partial J}{\\partial \\phi_j} = \\frac{1}{2}\\sum_{i=1}^{n} 2 \\left(y_i - f(s_i;\\boldsymbol{\\phi})\\right) \\left(-\\frac{\\partial f(s_i;\\boldsymbol{\\phi})}{\\partial \\phi_j}\\right)$$\n$$\\frac{\\partial J}{\\partial \\phi_j} = -\\sum_{i=1}^{n} \\left(y_i - f(s_i;\\boldsymbol{\\phi})\\right) \\frac{\\partial f(s_i;\\boldsymbol{\\phi})}{\\partial \\phi_j}$$\n\n为继续进行，我们必须计算模型函数 $f$ 关于 $\\phi_1$ 和 $\\phi_2$ 的偏导数。我们再次使用链式法则，将 $f$ 视为复合函数 $f(\\boldsymbol{\\theta}(\\boldsymbol{\\phi}))$。\n$$\\frac{\\partial f}{\\partial \\phi_j} = \\frac{\\partial f}{\\partial \\theta_1}\\frac{\\partial \\theta_1}{\\partial \\phi_j} + \\frac{\\partial f}{\\partial \\theta_2}\\frac{\\partial \\theta_2}{\\partial \\phi_j}$$\n重参数化的导数为：\n$$\\frac{\\partial \\theta_1}{\\partial \\phi_1} = \\frac{\\partial}{\\partial \\phi_1}(\\exp(\\phi_1)) = \\exp(\\phi_1) = \\theta_1; \\quad \\frac{\\partial \\theta_1}{\\partial \\phi_2} = 0$$\n$$\\frac{\\partial \\theta_2}{\\partial \\phi_2} = \\frac{\\partial}{\\partial \\phi_2}(\\exp(\\phi_2)) = \\exp(\\phi_2) = \\theta_2; \\quad \\frac{\\partial \\theta_2}{\\partial \\phi_1} = 0$$\n\n$f$ 关于原始参数 $\\boldsymbol{\\theta}$ 的偏导数为：\n$$\\frac{\\partial f}{\\partial \\theta_1} = \\frac{\\partial}{\\partial \\theta_1}\\left(\\frac{\\theta_1 s_i}{\\theta_2 + s_i}\\right) = \\frac{s_i}{\\theta_2 + s_i}$$\n$$\\frac{\\partial f}{\\partial \\theta_2} = \\frac{\\partial}{\\partial \\theta_2}\\left(\\frac{\\theta_1 s_i}{\\theta_2 + s_i}\\right) = \\theta_1 s_i \\frac{\\partial}{\\partial \\theta_2}(\\theta_2 + s_i)^{-1} = -\\frac{\\theta_1 s_i}{(\\theta_2 + s_i)^2}$$\n\n现在我们可以计算 $\\frac{\\partial f}{\\partial \\phi_1}$ 和 $\\frac{\\partial f}{\\partial \\phi_2}$。\n对于 $\\phi_1$：\n$$\\frac{\\partial f}{\\partial \\phi_1} = \\frac{\\partial f}{\\partial \\theta_1}\\frac{\\partial \\theta_1}{\\partial \\phi_1} + \\frac{\\partial f}{\\partial \\theta_2}\\frac{\\partial \\theta_2}{\\partial \\phi_1} = \\left(\\frac{s_i}{\\theta_2 + s_i}\\right) \\theta_1 + 0 = \\frac{\\theta_1 s_i}{\\theta_2 + s_i} = f(s_i;\\boldsymbol{\\theta})$$\n将 $\\theta_j = \\exp(\\phi_j)$ 代回：\n$$\\frac{\\partial f(s_i;\\boldsymbol{\\phi})}{\\partial \\phi_1} = \\frac{\\exp(\\phi_1)s_i}{\\exp(\\phi_2) + s_i}$$\n\n对于 $\\phi_2$：\n$$\\frac{\\partial f}{\\partial \\phi_2} = \\frac{\\partial f}{\\partial \\theta_1}\\frac{\\partial \\theta_1}{\\partial \\phi_2} + \\frac{\\partial f}{\\partial \\theta_2}\\frac{\\partial \\theta_2}{\\partial \\phi_2} = 0 + \\left(-\\frac{\\theta_1 s_i}{(\\theta_2 + s_i)^2}\\right) \\theta_2 = -\\frac{\\theta_1 \\theta_2 s_i}{(\\theta_2 + s_i)^2}$$\n将 $\\theta_j = \\exp(\\phi_j)$ 代回：\n$$\\frac{\\partial f(s_i;\\boldsymbol{\\phi})}{\\partial \\phi_2} = -\\frac{\\exp(\\phi_1) \\exp(\\phi_2) s_i}{(\\exp(\\phi_2) + s_i)^2}$$\n\n最后，我们将这些导数代入 $J(\\boldsymbol{\\phi})$ 的梯度分量的表达式中。\n梯度的第一个分量是：\n$$\\frac{\\partial J}{\\partial \\phi_1} = -\\sum_{i=1}^{n} \\left(y_i - f(s_i;\\boldsymbol{\\phi})\\right) \\frac{\\partial f(s_i;\\boldsymbol{\\phi})}{\\partial \\phi_1}$$\n$$\\frac{\\partial J}{\\partial \\phi_1} = -\\sum_{i=1}^{n} \\left(y_i - \\frac{\\exp(\\phi_1) s_i}{\\exp(\\phi_2) + s_i}\\right) \\left(\\frac{\\exp(\\phi_1) s_i}{\\exp(\\phi_2) + s_i}\\right)$$\n这可以写成：\n$$\\frac{\\partial J}{\\partial \\phi_1} = \\sum_{i=1}^{n} \\left(\\frac{\\exp(\\phi_1) s_i}{\\exp(\\phi_2) + s_i} - y_i\\right) \\left(\\frac{\\exp(\\phi_1) s_i}{\\exp(\\phi_2) + s_i}\\right)$$\n\n梯度的第二个分量是：\n$$\\frac{\\partial J}{\\partial \\phi_2} = -\\sum_{i=1}^{n} \\left(y_i - f(s_i;\\boldsymbol{\\phi})\\right) \\frac{\\partial f(s_i;\\boldsymbol{\\phi})}{\\partial \\phi_2}$$\n$$\\frac{\\partial J}{\\partial \\phi_2} = -\\sum_{i=1}^{n} \\left(y_i - \\frac{\\exp(\\phi_1) s_i}{\\exp(\\phi_2) + s_i}\\right) \\left(-\\frac{\\exp(\\phi_1) \\exp(\\phi_2) s_i}{(\\exp(\\phi_2) + s_i)^2}\\right)$$\n$$\\frac{\\partial J}{\\partial \\phi_2} = \\sum_{i=1}^{n} \\left(y_i - \\frac{\\exp(\\phi_1) s_i}{\\exp(\\phi_2) + s_i}\\right) \\left(\\frac{\\exp(\\phi_1) \\exp(\\phi_2) s_i}{(\\exp(\\phi_2) + s_i)^2}\\right)$$\n\n这些就是所要求的目标函数及其梯度的两个分量的表达式，用数据以及参数 $\\phi_1$ 和 $\\phi_2$ 明确表示。",
            "answer": "$$ \\boxed{ \\begin{pmatrix} \\frac{1}{2}\\sum_{i=1}^{n}\\left(y_i - \\frac{\\exp(\\phi_1) s_i}{\\exp(\\phi_2) + s_i}\\right)^2 \\\\ \\sum_{i=1}^{n} \\left(\\frac{\\exp(\\phi_1) s_i}{\\exp(\\phi_2) + s_i} - y_i\\right) \\left(\\frac{\\exp(\\phi_1) s_i}{\\exp(\\phi_2) + s_i}\\right) \\\\ \\sum_{i=1}^{n} \\left(y_i - \\frac{\\exp(\\phi_1) s_i}{\\exp(\\phi_2) + s_i}\\right) \\left(\\frac{\\exp(\\phi_1) \\exp(\\phi_2) s_i}{(\\exp(\\phi_2) + s_i)^2}\\right) \\end{pmatrix} } $$"
        },
        {
            "introduction": "非线性最小二乘问题的求解通常依赖于迭代算法，但当模型对某些参数组合不敏感时，标准算法可能会变得不稳定。本练习将带您深入了解Levenberg–Marquardt等阻尼最小二乘算法的核心机制。通过利用雅可比矩阵的奇异值分解（SVD），您将亲手计算一个阻尼更新步长，从而直观地理解阻尼参数如何通过抑制与小奇异值相关的不确定方向来确保优化过程的稳定性和鲁棒性。",
            "id": "3911613",
            "problem": "考虑一个生物医学系统中的双参数非线性模型，该模型用于微透析细胞因子浓度传感器，其参数向量 $\\boldsymbol{\\theta} \\in \\mathbb{R}^{2}$ 描述了输运速率和结合亲和力效应。该模型拟合于时间点 $t_{1}, t_{2}, t_{3}$ 处的 $m=3$ 个测量值，得到一个残差向量 $\\boldsymbol{r}(\\boldsymbol{\\theta}) \\in \\mathbb{R}^{3}$，其分量为 $r_{i}(\\boldsymbol{\\theta}) = y_{i}^{\\text{obs}} - y(t_{i}; \\boldsymbol{\\theta})$。在非线性最小二乘回归中，目标是最小化 $S(\\boldsymbol{\\theta}) = \\frac{1}{2}\\|\\boldsymbol{r}(\\boldsymbol{\\theta})\\|_{2}^{2}$。\n\n在当前迭代点 $\\boldsymbol{\\theta}_{0}$ 处，残差为 $\\boldsymbol{r} = \\begin{pmatrix}0.5 & -0.3 & 0.1\\end{pmatrix}^{\\mathsf{T}}$，关于参数的偏导数雅可比矩阵 $\\boldsymbol{J}(\\boldsymbol{\\theta}_{0}) \\in \\mathbb{R}^{3 \\times 2}$ 的薄奇异值分解（SVD）为\n$$\n\\boldsymbol{J} = \\boldsymbol{U}\\,\\boldsymbol{\\Sigma}\\,\\boldsymbol{V}^{\\mathsf{T}},\n$$\n其中 $\\boldsymbol{U} \\in \\mathbb{R}^{3 \\times 2}$ 的列是标准正交的，$\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{2 \\times 2}$ 是一个对角元为正的对角矩阵，$\\boldsymbol{V} \\in \\mathbb{R}^{2 \\times 2}$ 是正交矩阵。具体的因子为\n$$\n\\boldsymbol{U} = \\begin{pmatrix}\n\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{6}} \\\\\n\\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{6}} \\\\\n0 & \\frac{2}{\\sqrt{6}}\n\\end{pmatrix},\\quad\n\\boldsymbol{\\Sigma} = \\begin{pmatrix}\n10 & 0 \\\\\n0 & \\frac{1}{10}\n\\end{pmatrix},\\quad\n\\boldsymbol{V} = \\begin{pmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{pmatrix}.\n$$\n\n从非线性最小二乘目标及其高斯–牛顿（Gauss–Newton）线性化的基本定义出发，通过在 SVD 基中求解，从第一性原理推导阻尼最小二乘（Levenberg–Marquardt）步长 $\\boldsymbol{p} \\in \\mathbb{R}^{2}$，其中阻尼参数 $\\lambda$ 为正。明确展示奇异值如何出现在表达式中，并解释阻尼是如何处理小奇异值的。然后，对于给定的因子和残差，计算当 $\\lambda = \\frac{9}{10}$ 时的阻尼步长。\n\n将您的最终答案表示为代表 $\\boldsymbol{p}$ 的单个行向量，使用精确的解析值，不要四舍五入。在此计算中，参数是无量纲的，因此最终答案不应包含任何物理单位。",
            "solution": "该问题是有效的。这是一个适定且自洽的数值优化问题，具体涉及使用 Levenberg-Marquardt 算法的非线性最小二乘回归。其背景具有科学依据，并提供了推导和计算所需的所有数据。\n\n非线性最小二乘法的目标是找到参数向量 $\\boldsymbol{\\theta}$，以最小化由函数 $S(\\boldsymbol{\\theta})$ 给出的残差平方和：\n$$\nS(\\boldsymbol{\\theta}) = \\frac{1}{2} \\|\\boldsymbol{r}(\\boldsymbol{\\theta})\\|_{2}^{2} = \\frac{1}{2} \\boldsymbol{r}(\\boldsymbol{\\theta})^{\\mathsf{T}}\\boldsymbol{r}(\\boldsymbol{\\theta})\n$$\n通常使用迭代方法来寻找最小值。给定当前迭代点 $\\boldsymbol{\\theta}_{k}$，我们寻求一个步长 $\\boldsymbol{p}$ 来找到下一个迭代点 $\\boldsymbol{\\theta}_{k+1} = \\boldsymbol{\\theta}_{k} + \\boldsymbol{p}$。高斯-牛顿法将残差函数在 $\\boldsymbol{\\theta}_{k}$ 附近线性化：\n$$\n\\boldsymbol{r}(\\boldsymbol{\\theta}_{k} + \\boldsymbol{p}) \\approx \\boldsymbol{r}(\\boldsymbol{\\theta}_{k}) + \\boldsymbol{J}(\\boldsymbol{\\theta}_{k})\\boldsymbol{p}\n$$\n其中 $\\boldsymbol{J}(\\boldsymbol{\\theta}_{k})$ 是 $\\boldsymbol{r}$ 在 $\\boldsymbol{\\theta}_{k}$ 处的雅可比矩阵。令 $\\boldsymbol{r}_k = \\boldsymbol{r}(\\boldsymbol{\\theta}_k)$ 且 $\\boldsymbol{J}_k = \\boldsymbol{J}(\\boldsymbol{\\theta}_k)$。我们最小化线性化的目标函数：\n$$\n\\min_{\\boldsymbol{p}} \\frac{1}{2} \\|\\boldsymbol{r}_k + \\boldsymbol{J}_k \\boldsymbol{p}\\|_2^2\n$$\n这是一个线性最小二乘问题，其解 $\\boldsymbol{p}$ 满足正规方程：\n$$\n(\\boldsymbol{J}_k^{\\mathsf{T}}\\boldsymbol{J}_k)\\boldsymbol{p} = -\\boldsymbol{J}_k^{\\mathsf{T}}\\boldsymbol{r}_k\n$$\n阻尼最小二乘法，或称 Levenberg-Marquardt 方法，通过添加一个正的阻尼项 $\\lambda > 0$ 来解决矩阵 $\\boldsymbol{J}_k^{\\mathsf{T}}\\boldsymbol{J}_k$ 可能存在的病态问题。然后通过求解修正后的系统来找到步长 $\\boldsymbol{p}$。对于当前迭代点 $\\boldsymbol{\\theta}_0$，我们省略下标 $k$。\n$$\n(\\boldsymbol{J}^{\\mathsf{T}}\\boldsymbol{J} + \\lambda \\boldsymbol{I})\\boldsymbol{p} = -\\boldsymbol{J}^{\\mathsf{T}}\\boldsymbol{r}\n$$\n这是我们推导解所依据的基础方程。我们已知雅可比矩阵的薄奇异值分解（SVD），$\\boldsymbol{J} = \\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{V}^{\\mathsf{T}}$。我们将其代入方程中。\n\n首先，我们使用 SVD 因子来表示 $\\boldsymbol{J}^{\\mathsf{T}}\\boldsymbol{J}$ 和 $\\boldsymbol{J}^{\\mathsf{T}}\\boldsymbol{r}$ 这两项：\n$$\n\\boldsymbol{J}^{\\mathsf{T}}\\boldsymbol{J} = (\\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{V}^{\\mathsf{T}})^{\\mathsf{T}}(\\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{V}^{\\mathsf{T}}) = \\boldsymbol{V}\\boldsymbol{\\Sigma}^{\\mathsf{T}}\\boldsymbol{U}^{\\mathsf{T}}\\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{V}^{\\mathsf{T}}\n$$\n由于 $\\boldsymbol{U}$ 的列是标准正交的，所以 $\\boldsymbol{U}^{\\mathsf{T}}\\boldsymbol{U} = \\boldsymbol{I}$。此外，$\\boldsymbol{\\Sigma}$ 是对角矩阵，所以 $\\boldsymbol{\\Sigma}^{\\mathsf{T}} = \\boldsymbol{\\Sigma}$。\n$$\n\\boldsymbol{J}^{\\mathsf{T}}\\boldsymbol{J} = \\boldsymbol{V}\\boldsymbol{\\Sigma}\\boldsymbol{I}\\boldsymbol{\\Sigma}\\boldsymbol{V}^{\\mathsf{T}} = \\boldsymbol{V}\\boldsymbol{\\Sigma}^{2}\\boldsymbol{V}^{\\mathsf{T}}\n$$\n方程右边变为：\n$$\n\\boldsymbol{J}^{\\mathsf{T}}\\boldsymbol{r} = (\\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{V}^{\\mathsf{T}})^{\\mathsf{T}}\\boldsymbol{r} = \\boldsymbol{V}\\boldsymbol{\\Sigma}^{\\mathsf{T}}\\boldsymbol{U}^{\\mathsf{T}}\\boldsymbol{r} = \\boldsymbol{V}\\boldsymbol{\\Sigma}\\boldsymbol{U}^{\\mathsf{T}}\\boldsymbol{r}\n$$\n将这些代入 Levenberg-Marquardt 方程得到：\n$$\n(\\boldsymbol{V}\\boldsymbol{\\Sigma}^{2}\\boldsymbol{V}^{\\mathsf{T}} + \\lambda \\boldsymbol{I})\\boldsymbol{p} = -\\boldsymbol{V}\\boldsymbol{\\Sigma}\\boldsymbol{U}^{\\mathsf{T}}\\boldsymbol{r}\n$$\n由于 $\\boldsymbol{V}$ 是正交矩阵，所以 $\\boldsymbol{V}\\boldsymbol{V}^{\\mathsf{T}} = \\boldsymbol{I}$。我们可以将 $\\lambda \\boldsymbol{I}$ 写成 $\\lambda \\boldsymbol{V}\\boldsymbol{V}^{\\mathsf{T}}$ 并将其代入左边：\n$$\n(\\boldsymbol{V}\\boldsymbol{\\Sigma}^{2}\\boldsymbol{V}^{\\mathsf{T}} + \\lambda \\boldsymbol{V}\\boldsymbol{V}^{\\mathsf{T}})\\boldsymbol{p} = \\boldsymbol{V}(\\boldsymbol{\\Sigma}^{2} + \\lambda \\boldsymbol{I})\\boldsymbol{V}^{\\mathsf{T}}\\boldsymbol{p} = -\\boldsymbol{V}\\boldsymbol{\\Sigma}\\boldsymbol{U}^{\\mathsf{T}}\\boldsymbol{r}\n$$\n左乘 $\\boldsymbol{V}^{\\mathsf{T}}$（即 $\\boldsymbol{V}^{-1}$）分离出包含 $\\boldsymbol{p}$ 的项：\n$$\n\\boldsymbol{V}^{\\mathsf{T}}\\boldsymbol{V}(\\boldsymbol{\\Sigma}^{2} + \\lambda \\boldsymbol{I})\\boldsymbol{V}^{\\mathsf{T}}\\boldsymbol{p} = -\\boldsymbol{V}^{\\mathsf{T}}\\boldsymbol{V}\\boldsymbol{\\Sigma}\\boldsymbol{U}^{\\mathsf{T}}\\boldsymbol{r}\n$$\n$$\n(\\boldsymbol{\\Sigma}^{2} + \\lambda \\boldsymbol{I})\\boldsymbol{V}^{\\mathsf{T}}\\boldsymbol{p} = -\\boldsymbol{\\Sigma}\\boldsymbol{U}^{\\mathsf{T}}\\boldsymbol{r}\n$$\n矩阵 $(\\boldsymbol{\\Sigma}^{2} + \\lambda \\boldsymbol{I})$ 是对角矩阵，其对角元为 $(\\sigma_j^2 + \\lambda)$，其中 $\\sigma_j$ 是奇异值。它的逆矩阵也是对角矩阵，对角元为 $1/(\\sigma_j^2 + \\lambda)$。我们可以解出 $\\boldsymbol{V}^{\\mathsf{T}}\\boldsymbol{p}$：\n$$\n\\boldsymbol{V}^{\\mathsf{T}}\\boldsymbol{p} = -(\\boldsymbol{\\Sigma}^{2} + \\lambda \\boldsymbol{I})^{-1}\\boldsymbol{\\Sigma}\\boldsymbol{U}^{\\mathsf{T}}\\boldsymbol{r}\n$$\n最后，左乘 $\\boldsymbol{V}$ 得到步长 $\\boldsymbol{p}$ 的表达式：\n$$\n\\boldsymbol{p} = -\\boldsymbol{V}(\\boldsymbol{\\Sigma}^{2} + \\lambda \\boldsymbol{I})^{-1}\\boldsymbol{\\Sigma}\\boldsymbol{U}^{\\mathsf{T}}\\boldsymbol{r}\n$$\n为了明确展示奇异值如何出现在表达式中，我们可以将 $\\boldsymbol{p}$ 写成和的形式。设 $\\boldsymbol{u}_j$ 和 $\\boldsymbol{v}_j$ 分别是 $\\boldsymbol{U}$ 和 $\\boldsymbol{V}$ 的列向量。步长向量 $\\boldsymbol{p}$ 为：\n$$\n\\boldsymbol{p} = \\sum_{j=1}^{n} \\left( \\frac{-\\sigma_j}{\\sigma_j^2 + \\lambda} \\right) (\\boldsymbol{u}_j^{\\mathsf{T}}\\boldsymbol{r}) \\boldsymbol{v}_j\n$$\n该表达式表明，步长 $\\boldsymbol{p}$ 是右奇异向量 $\\boldsymbol{v}_j$ 的线性组合。每个 $\\boldsymbol{v}_j$ 的系数由相应的奇异值 $\\sigma_j$、阻尼参数 $\\lambda$ 以及残差 $\\boldsymbol{r}$ 在相应左奇异向量 $\\boldsymbol{u}_j$ 上的投影决定。\n\n阻尼的作用是在出现小奇异值时稳定步长的计算。第 $j$ 个分量的缩放因子是 $\\frac{\\sigma_j}{\\sigma_j^2 + \\lambda}$。\n如果 $\\sigma_j$ 很大（即 $\\sigma_j^2 \\gg \\lambda$），该因子约等于 $\\frac{\\sigma_j}{\\sigma_j^2} = \\frac{1}{\\sigma_j}$。这与无阻尼的高斯-牛顿步长的缩放比例相同。\n如果 $\\sigma_j$ 很小（即 $\\sigma_j^2 \\ll \\lambda$），该因子约等于 $\\frac{\\sigma_j}{\\lambda}$。如果没有阻尼，该因子将是 $\\frac{1}{\\sigma_j}$，这个值会非常大，导致在 $\\boldsymbol{v}_j$ 方向上产生一个巨大且不可靠的步长。分母中的阻尼参数 $\\lambda$ 确保了与小奇异值相关的方向的贡献被抑制，从而防止步长 $\\boldsymbol{p}$ 变得过大。这使得优化过程更加稳定和鲁棒。\n\n现在，我们根据给定的数据计算阻尼步长 $\\boldsymbol{p}$。\n残差为 $\\boldsymbol{r} = \\begin{pmatrix} 0.5 \\\\ -0.3 \\\\ 0.1 \\end{pmatrix}$。阻尼参数为 $\\lambda = \\frac{9}{10}$。\nSVD 因子为：\n$\\boldsymbol{U} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}}  \\frac{1}{\\sqrt{6}} \\\\ \\frac{1}{\\sqrt{2}}  -\\frac{1}{\\sqrt{6}} \\\\ 0  \\frac{2}{\\sqrt{6}} \\end{pmatrix}$，$\\boldsymbol{\\Sigma} = \\begin{pmatrix} 10  0 \\\\ 0  \\frac{1}{10} \\end{pmatrix}$，$\\boldsymbol{V} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}$。\n$\\boldsymbol{U}$ 的列向量为 $\\boldsymbol{u}_1 = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\\\ 0 \\end{pmatrix}$ 和 $\\boldsymbol{u}_2 = \\begin{pmatrix} \\frac{1}{\\sqrt{6}} \\\\ -\\frac{1}{\\sqrt{6}} \\\\ \\frac{2}{\\sqrt{6}} \\end{pmatrix}$。\n奇异值为 $\\sigma_1 = 10$ 和 $\\sigma_2 = \\frac{1}{10}$。\n由于 $\\boldsymbol{V} = \\boldsymbol{I}$，我们有 $\\boldsymbol{p} = \\boldsymbol{V}^{\\mathsf{T}}\\boldsymbol{p}$，因此 $\\boldsymbol{p}$ 可以直接通过 $\\boldsymbol{p} = -(\\boldsymbol{\\Sigma}^{2} + \\lambda \\boldsymbol{I})^{-1}\\boldsymbol{\\Sigma}\\boldsymbol{U}^{\\mathsf{T}}\\boldsymbol{r}$ 计算。\n\n首先，计算向量 $\\boldsymbol{U}^{\\mathsf{T}}\\boldsymbol{r}$：\n$$\n\\boldsymbol{u}_1^{\\mathsf{T}}\\boldsymbol{r} = \\frac{1}{\\sqrt{2}}(0.5) + \\frac{1}{\\sqrt{2}}(-0.3) + 0(0.1) = \\frac{0.2}{\\sqrt{2}} = \\frac{2}{10\\sqrt{2}} = \\frac{1}{5\\sqrt{2}}\n$$\n$$\n\\boldsymbol{u}_2^{\\mathsf{T}}\\boldsymbol{r} = \\frac{1}{\\sqrt{6}}(0.5) - \\frac{1}{\\sqrt{6}}(-0.3) + \\frac{2}{\\sqrt{6}}(0.1) = \\frac{1}{\\sqrt{6}}(0.5 + 0.3 + 0.2) = \\frac{1}{\\sqrt{6}}\n$$\n所以，$\\boldsymbol{U}^{\\mathsf{T}}\\boldsymbol{r} = \\begin{pmatrix} \\frac{1}{5\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{6}} \\end{pmatrix}$。\n\n接下来，我们计算 $\\boldsymbol{p}$ 的分量。\n对于第一个分量 $p_1$：\n$$\np_1 = -\\left( \\frac{\\sigma_1}{\\sigma_1^2 + \\lambda} \\right) (\\boldsymbol{u}_1^{\\mathsf{T}}\\boldsymbol{r}) = -\\left( \\frac{10}{10^2 + \\frac{9}{10}} \\right) \\left( \\frac{1}{5\\sqrt{2}} \\right) = -\\left( \\frac{10}{100 + \\frac{9}{10}} \\right) \\left( \\frac{1}{5\\sqrt{2}} \\right)\n$$\n$$\np_1 = -\\left( \\frac{10}{\\frac{1000+9}{10}} \\right) \\left( \\frac{1}{5\\sqrt{2}} \\right) = -\\left( \\frac{100}{1009} \\right) \\left( \\frac{1}{5\\sqrt{2}} \\right) = -\\frac{20}{1009\\sqrt{2}} = -\\frac{20\\sqrt{2}}{1009 \\cdot 2} = -\\frac{10\\sqrt{2}}{1009}\n$$\n对于第二个分量 $p_2$：\n$$\np_2 = -\\left( \\frac{\\sigma_2}{\\sigma_2^2 + \\lambda} \\right) (\\boldsymbol{u}_2^{\\mathsf{T}}\\boldsymbol{r}) = -\\left( \\frac{\\frac{1}{10}}{(\\frac{1}{10})^2 + \\frac{9}{10}} \\right) \\left( \\frac{1}{\\sqrt{6}} \\right) = -\\left( \\frac{\\frac{1}{10}}{\\frac{1}{100} + \\frac{90}{100}} \\right) \\left( \\frac{1}{\\sqrt{6}} \\right)\n$$\n$$\np_2 = -\\left( \\frac{\\frac{1}{10}}{\\frac{91}{100}} \\right) \\left( \\frac{1}{\\sqrt{6}} \\right) = -\\left( \\frac{1}{10} \\cdot \\frac{100}{91} \\right) \\left( \\frac{1}{\\sqrt{6}} \\right) = -\\frac{10}{91\\sqrt{6}} = -\\frac{10\\sqrt{6}}{91 \\cdot 6} = -\\frac{5\\sqrt{6}}{273}\n$$\n步长向量 $\\boldsymbol{p}$ 是 $\\begin{pmatrix} -\\frac{10\\sqrt{2}}{1009} \\\\ -\\frac{5\\sqrt{6}}{273} \\end{pmatrix}$。\n问题要求以单个行向量的形式给出答案。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n-\\frac{10\\sqrt{2}}{1009} & -\\frac{5\\sqrt{6}}{273}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "在模型拟合中，获得参数的最佳估计值只是第一步，评估这些估计值的可靠性同样至关重要。本练习聚焦于参数不确定性的量化，特别是如何为经过对数变换后估计的参数构建置信区间。您将应用大样本理论和变换法则，为一个在对数尺度上估计的参数计算置信区间，并将其映射回原始尺度，这是解释回归结果并赋予其物理意义的关键实践技能。",
            "id": "3911614",
            "problem": "一种用于血浆浓度的药代动力学单室静脉推注模型由 $C(t;\\theta)=\\frac{D}{V}\\exp(-k t)$ 给出，其中 $D$ 是已知剂量，$V$ 是分布容积，$k$ 是消除速率常数。为了在估计过程中强制保证正值，消除速率被参数化为 $\\xi=\\ln(k)$。浓度测量值被建模为 $y_i=C(t_i;\\theta)+\\varepsilon_i$，其中测量误差 $\\varepsilon_i$ 是独立同分布的，服从 $\\varepsilon_i\\sim\\mathcal{N}(0,\\sigma^2)$。\n\n参数通过非线性最小二乘法（NLS (Nonlinear Least Squares)）进行估计，得到 $\\xi$ 的估计值 $\\hat{\\xi}$。在非线性回归中 NLS 估计量的标准大样本正则性假设下，$\\hat{\\xi}$ 是渐近正态的，其协方差可以通过高斯–牛顿（Gauss–Newton）线性化，利用残差方差和模型关于参数的灵敏度（雅可比矩阵）来近似。\n\n假设一次数据集拟合产生了 $\\hat{\\xi}=-2.40$ 以及 $\\hat{\\xi}$ 的估计渐近方差为 $\\widehat{\\Sigma}_{\\xi\\xi}=0.0081$。利用 $\\hat{\\xi}$ 的渐近正态性和将 $\\xi$ 映射到 $k=\\exp(\\xi)$ 的指数变换的单调性，首先构建 $\\xi$ 的置信区间，然后将其端点映射到 $k$ 的尺度上，从而为 $k$ 构建一个置信水平为 $0.95$ 的双侧置信区间。计算这个 $0.95$ 置信水平的 $k$ 的置信区间的上端点。\n\n以 $\\mathrm{min}^{-1}$ 为单位表示您的最终答案。将最终数值四舍五入到四位有效数字。",
            "solution": "该问题要求基于消除速率常数 $k$ 的对数 $\\xi = \\ln(k)$ 的估计值，构建 $k$ 的 $0.95$ 置信区间。该过程包括首先建立 $\\xi$ 的置信区间，然后将其端点转换到 $k$ 的尺度。\n\n我们已知 $\\xi$ 的估计值为 $\\hat{\\xi} = -2.40$，其估计的渐近方差为 $\\widehat{\\Sigma}_{\\xi\\xi} = 0.0081$。\n\n在估计量 $\\hat{\\xi}$ 是渐近正态分布的假设下，其分布可以近似为：\n$$\n\\hat{\\xi} \\sim \\mathcal{N}(\\xi, \\widehat{\\Sigma}_{\\xi\\xi})\n$$\n其中 $\\xi$ 是参数的真实（未知）值。估计值 $\\hat{\\xi}$ 的标准误是其方差的平方根：\n$$\nSE(\\hat{\\xi}) = \\sqrt{\\widehat{\\Sigma}_{\\xi\\xi}} = \\sqrt{0.0081} = 0.09\n$$\n\n对于置信水平为 $1-\\alpha$ 的 $\\xi$ 的双侧置信区间的构建公式为：\n$$\n[\\hat{\\xi} - z_{1-\\alpha/2} \\cdot SE(\\hat{\\xi}), \\quad \\hat{\\xi} + z_{1-\\alpha/2} \\cdot SE(\\hat{\\xi})]\n$$\n对于 $0.95$ 的置信水平，我们有 $1-\\alpha = 0.95$，即 $\\alpha = 0.05$。来自标准正态分布的临界值为 $z_{1-\\alpha/2} = z_{1-0.025} = z_{0.975}$。该值约等于 $1.96$。\n\n因此，$\\xi$ 的 $0.95$ 置信区间为：\n$$\n[\\xi_L, \\xi_U] = [-2.40 - 1.96 \\times 0.09, \\quad -2.40 + 1.96 \\times 0.09]\n$$\n$$\n[\\xi_L, \\xi_U] = [-2.40 - 0.1764, \\quad -2.40 + 0.1764]\n$$\n$$\n[\\xi_L, \\xi_U] = [-2.5764, \\quad -2.2236]\n$$\n\n$k$ 和 $\\xi$ 之间的关系由 $k = \\exp(\\xi)$ 给出。由于指数函数 $\\exp(\\cdot)$ 是一个严格单调递增函数，因此可以通过将变换应用于 $\\xi$ 置信区间的端点来获得 $k$ 的置信区间。\n$k$ 的置信区间是 $[k_L, k_U]$，其中：\n$$\nk_L = \\exp(\\xi_L)\n$$\n$$\nk_U = \\exp(\\xi_U)\n$$\n问题要求计算该置信区间的上端点 $k_U$。\n\n我们使用 $\\xi_U$ 的值来计算 $k_U$：\n$$\nk_U = \\exp(\\xi_U) = \\exp(-2.2236)\n$$\n进行数值计算：\n$$\nk_U \\approx 0.10822605...\n$$\n问题要求将最终答案四舍五入到四位有效数字。\n$$\nk_U \\approx 0.1082\n$$\n$k$ 的单位被指定为 $\\mathrm{min}^{-1}$，这与消除速率常数是一致的。",
            "answer": "$$\n\\boxed{0.1082}\n$$"
        }
    ]
}