{
    "hands_on_practices": [
        {
            "introduction": "A crucial first step in modeling physical systems is ensuring that parameter estimates respect known physical constraints, such as positive reaction rates or concentrations. A powerful method to achieve this is reparameterization, which transforms a constrained optimization problem into an unconstrained one. This exercise  demonstrates this technique for the classic Michaelis-Menten enzyme kinetics model by using an exponential map, requiring you to derive the new objective function and its gradient, a fundamental skill for implementing custom model fitting routines.",
            "id": "3911605",
            "problem": "A researcher is calibrating a mechanistic model of an enzyme-catalyzed reaction in a biomedical assay. The measured initial reaction velocities at substrate concentrations $s_i$ are modeled by the Michaelis–Menten relation $f(s_i;\\boldsymbol{\\theta}) = \\dfrac{\\theta_1 s_i}{\\theta_2 + s_i}$, where $\\boldsymbol{\\theta} = (\\theta_1,\\theta_2)^{\\top}$ has strictly positive components corresponding to the maximum velocity and the Michaelis constant, respectively. Let the observed velocities be $y_i$ for $i=1,\\dots,n$, and assume independent additive noise with constant variance. The researcher uses nonlinear least-squares regression to estimate $\\boldsymbol{\\theta}$ by minimizing the objective $J(\\boldsymbol{\\theta}) = \\dfrac{1}{2}\\sum_{i=1}^{n}\\left(y_i - f(s_i;\\boldsymbol{\\theta})\\right)^2$.\n\nTo enforce positivity of parameters in a numerically stable way, the researcher reparameterizes $\\boldsymbol{\\theta}$ via an elementwise exponential map $\\boldsymbol{\\theta} = \\exp(\\boldsymbol{\\phi})$, where $\\boldsymbol{\\phi} = (\\phi_1,\\phi_2)^{\\top} \\in \\mathbb{R}^2$ and the exponential is applied componentwise, so that $\\theta_j = \\exp(\\phi_j)$ for $j=1,2$.\n\nStarting from the definition of nonlinear least-squares and the basic rules of multivariable calculus (product rule, chain rule), derive the transformed objective $J(\\boldsymbol{\\phi}) = J(\\exp(\\boldsymbol{\\phi}))$ and its gradient with respect to $\\boldsymbol{\\phi}$. Express your final result explicitly in terms of the data $\\{(s_i,y_i)\\}_{i=1}^{n}$ and the reparameterized variables $\\phi_1$ and $\\phi_2$, without introducing any additional parameters. Your derivation should be rigorous and should clearly justify each step from first principles.\n\nProvide your final answer as three expressions: the scalar objective $J(\\boldsymbol{\\phi})$, followed by the two scalar components of the gradient vector $\\nabla_{\\boldsymbol{\\phi}} J(\\boldsymbol{\\phi})$, namely $\\dfrac{\\partial J}{\\partial \\phi_1}$ and $\\dfrac{\\partial J}{\\partial \\phi_2}$. No numerical evaluation is required, and no rounding is needed. Do not include units in your final expressions.",
            "solution": "The problem statement is scientifically grounded, well-posed, and objective. It describes a standard scenario in biochemical modeling and parameter estimation. The use of Michaelis-Menten kinetics, nonlinear least-squares, and exponential reparameterization to enforce positivity are all established and valid techniques. The task is a direct application of multivariable calculus and is fully specified. Therefore, the problem is deemed valid and a solution will be provided.\n\nThe objective is to derive the transformed objective function $J(\\boldsymbol{\\phi})$ and its gradient $\\nabla_{\\boldsymbol{\\phi}} J(\\boldsymbol{\\phi})$ with respect to the reparameterized variables $\\boldsymbol{\\phi} = (\\phi_1, \\phi_2)^{\\top}$.\n\nThe original model and parameters are:\n$$f(s_i;\\boldsymbol{\\theta}) = \\frac{\\theta_1 s_i}{\\theta_2 + s_i}$$\nwhere $\\boldsymbol{\\theta} = (\\theta_1, \\theta_2)^{\\top}$.\n\nThe reparameterization is given by an elementwise exponential map:\n$$\\theta_1 = \\exp(\\phi_1)$$\n$$\\theta_2 = \\exp(\\phi_2)$$\nThis transformation correctly enforces the positivity constraints $\\theta_1 > 0$ and $\\theta_2 > 0$ for any real values of $\\phi_1$ and $\\phi_2$.\n\nFirst, we express the model function $f$ in terms of the new parameters $\\boldsymbol{\\phi}$:\n$$f(s_i; \\boldsymbol{\\phi}) = \\frac{\\exp(\\phi_1) s_i}{\\exp(\\phi_2) + s_i}$$\n\nThe objective function for nonlinear least-squares, $J(\\boldsymbol{\\theta})$, is given by:\n$$J(\\boldsymbol{\\theta}) = \\frac{1}{2}\\sum_{i=1}^{n}\\left(y_i - f(s_i;\\boldsymbol{\\theta})\\right)^2$$\nSubstituting the reparameterized model, we obtain the transformed objective function $J(\\boldsymbol{\\phi})$:\n$$J(\\boldsymbol{\\phi}) = \\frac{1}{2}\\sum_{i=1}^{n}\\left(y_i - \\frac{\\exp(\\phi_1) s_i}{\\exp(\\phi_2) + s_i}\\right)^2$$\nThis is the first required expression.\n\nNext, we derive the gradient of $J(\\boldsymbol{\\phi})$, which consists of the partial derivatives $\\frac{\\partial J}{\\partial \\phi_1}$ and $\\frac{\\partial J}{\\partial \\phi_2}$. We apply the chain rule. For a generic component $\\phi_j$ (where $j=1$ or $j=2$):\n$$\\frac{\\partial J}{\\partial \\phi_j} = \\frac{\\partial}{\\partial \\phi_j} \\left[ \\frac{1}{2}\\sum_{i=1}^{n}\\left(y_i - f(s_i;\\boldsymbol{\\phi})\\right)^2 \\right]$$\nApplying the chain rule and sum rule:\n$$\\frac{\\partial J}{\\partial \\phi_j} = \\frac{1}{2}\\sum_{i=1}^{n} 2 \\left(y_i - f(s_i;\\boldsymbol{\\phi})\\right) \\left(-\\frac{\\partial f(s_i;\\boldsymbol{\\phi})}{\\partial \\phi_j}\\right)$$\n$$\\frac{\\partial J}{\\partial \\phi_j} = -\\sum_{i=1}^{n} \\left(y_i - f(s_i;\\boldsymbol{\\phi})\\right) \\frac{\\partial f(s_i;\\boldsymbol{\\phi})}{\\partial \\phi_j}$$\n\nTo proceed, we must compute the partial derivatives of the model function $f$ with respect to $\\phi_1$ and $\\phi_2$. We use the chain rule again, treating $f$ as a composition of functions $f(\\boldsymbol{\\theta}(\\boldsymbol{\\phi}))$.\n$$\\frac{\\partial f}{\\partial \\phi_j} = \\frac{\\partial f}{\\partial \\theta_1}\\frac{\\partial \\theta_1}{\\partial \\phi_j} + \\frac{\\partial f}{\\partial \\theta_2}\\frac{\\partial \\theta_2}{\\partial \\phi_j}$$\nThe derivatives of the reparameterization are:\n$$\\frac{\\partial \\theta_1}{\\partial \\phi_1} = \\frac{\\partial}{\\partial \\phi_1}(\\exp(\\phi_1)) = \\exp(\\phi_1) = \\theta_1; \\quad \\frac{\\partial \\theta_1}{\\partial \\phi_2} = 0$$\n$$\\frac{\\partial \\theta_2}{\\partial \\phi_2} = \\frac{\\partial}{\\partial \\phi_2}(\\exp(\\phi_2)) = \\exp(\\phi_2) = \\theta_2; \\quad \\frac{\\partial \\theta_2}{\\partial \\phi_1} = 0$$\n\nThe partial derivatives of $f$ with respect to the original parameters $\\boldsymbol{\\theta}$ are:\n$$\\frac{\\partial f}{\\partial \\theta_1} = \\frac{\\partial}{\\partial \\theta_1}\\left(\\frac{\\theta_1 s_i}{\\theta_2 + s_i}\\right) = \\frac{s_i}{\\theta_2 + s_i}$$\n$$\\frac{\\partial f}{\\partial \\theta_2} = \\frac{\\partial}{\\partial \\theta_2}\\left(\\frac{\\theta_1 s_i}{\\theta_2 + s_i}\\right) = \\theta_1 s_i \\frac{\\partial}{\\partial \\theta_2}(\\theta_2 + s_i)^{-1} = -\\frac{\\theta_1 s_i}{(\\theta_2 + s_i)^2}$$\n\nNow we can compute $\\frac{\\partial f}{\\partial \\phi_1}$ and $\\frac{\\partial f}{\\partial \\phi_2}$.\nFor $\\phi_1$:\n$$\\frac{\\partial f}{\\partial \\phi_1} = \\frac{\\partial f}{\\partial \\theta_1}\\frac{\\partial \\theta_1}{\\partial \\phi_1} + \\frac{\\partial f}{\\partial \\theta_2}\\frac{\\partial \\theta_2}{\\partial \\phi_1} = \\left(\\frac{s_i}{\\theta_2 + s_i}\\right) \\theta_1 + 0 = \\frac{\\theta_1 s_i}{\\theta_2 + s_i} = f(s_i;\\boldsymbol{\\theta})$$\nSubstituting back $\\theta_j = \\exp(\\phi_j)$:\n$$\\frac{\\partial f(s_i;\\boldsymbol{\\phi})}{\\partial \\phi_1} = \\frac{\\exp(\\phi_1)s_i}{\\exp(\\phi_2) + s_i}$$\n\nFor $\\phi_2$:\n$$\\frac{\\partial f}{\\partial \\phi_2} = \\frac{\\partial f}{\\partial \\theta_1}\\frac{\\partial \\theta_1}{\\partial \\phi_2} + \\frac{\\partial f}{\\partial \\theta_2}\\frac{\\partial \\theta_2}{\\partial \\phi_2} = 0 + \\left(-\\frac{\\theta_1 s_i}{(\\theta_2 + s_i)^2}\\right) \\theta_2 = -\\frac{\\theta_1 \\theta_2 s_i}{(\\theta_2 + s_i)^2}$$\nSubstituting back $\\theta_j = \\exp(\\phi_j)$:\n$$\\frac{\\partial f(s_i;\\boldsymbol{\\phi})}{\\partial \\phi_2} = -\\frac{\\exp(\\phi_1) \\exp(\\phi_2) s_i}{(\\exp(\\phi_2) + s_i)^2}$$\n\nFinally, we substitute these derivatives into the expressions for the gradient components of $J(\\boldsymbol{\\phi})$.\nThe first component of the gradient is:\n$$\\frac{\\partial J}{\\partial \\phi_1} = -\\sum_{i=1}^{n} \\left(y_i - f(s_i;\\boldsymbol{\\phi})\\right) \\frac{\\partial f(s_i;\\boldsymbol{\\phi})}{\\partial \\phi_1}$$\n$$\\frac{\\partial J}{\\partial \\phi_1} = -\\sum_{i=1}^{n} \\left(y_i - \\frac{\\exp(\\phi_1) s_i}{\\exp(\\phi_2) + s_i}\\right) \\left(\\frac{\\exp(\\phi_1) s_i}{\\exp(\\phi_2) + s_i}\\right)$$\nThis can be written as:\n$$\\frac{\\partial J}{\\partial \\phi_1} = \\sum_{i=1}^{n} \\left(\\frac{\\exp(\\phi_1) s_i}{\\exp(\\phi_2) + s_i} - y_i\\right) \\left(\\frac{\\exp(\\phi_1) s_i}{\\exp(\\phi_2) + s_i}\\right)$$\n\nThe second component of the gradient is:\n$$\\frac{\\partial J}{\\partial \\phi_2} = -\\sum_{i=1}^{n} \\left(y_i - f(s_i;\\boldsymbol{\\phi})\\right) \\frac{\\partial f(s_i;\\boldsymbol{\\phi})}{\\partial \\phi_2}$$\n$$\\frac{\\partial J}{\\partial \\phi_2} = -\\sum_{i=1}^{n} \\left(y_i - \\frac{\\exp(\\phi_1) s_i}{\\exp(\\phi_2) + s_i}\\right) \\left(-\\frac{\\exp(\\phi_1) \\exp(\\phi_2) s_i}{(\\exp(\\phi_2) + s_i)^2}\\right)$$\n$$\\frac{\\partial J}{\\partial \\phi_2} = \\sum_{i=1}^{n} \\left(y_i - \\frac{\\exp(\\phi_1) s_i}{\\exp(\\phi_2) + s_i}\\right) \\left(\\frac{\\exp(\\phi_1) \\exp(\\phi_2) s_i}{(\\exp(\\phi_2) + s_i)^2}\\right)$$\n\nThese are the required expressions for the objective function and the two components of its gradient, expressed explicitly in terms of the data and the parameters $\\phi_1$ and $\\phi_2$.",
            "answer": "$$ \\boxed{ \\begin{pmatrix} \\frac{1}{2}\\sum_{i=1}^{n}\\left(y_i - \\frac{\\exp(\\phi_1) s_i}{\\exp(\\phi_2) + s_i}\\right)^2 & \\sum_{i=1}^{n} \\left(\\frac{\\exp(\\phi_1) s_i}{\\exp(\\phi_2) + s_i} - y_i\\right) \\left(\\frac{\\exp(\\phi_1) s_i}{\\exp(\\phi_2) + s_i}\\right) & \\sum_{i=1}^{n} \\left(y_i - \\frac{\\exp(\\phi_1) s_i}{\\exp(\\phi_2) + s_i}\\right) \\left(\\frac{\\exp(\\phi_1) \\exp(\\phi_2) s_i}{(\\exp(\\phi_2) + s_i)^2}\\right) \\end{pmatrix} } $$"
        },
        {
            "introduction": "Nonlinear least-squares problems in biomedicine are often ill-conditioned, meaning small changes in the data can lead to large changes in parameter estimates. The Levenberg-Marquardt algorithm is a workhorse method that provides a robust solution by \"damping\" the Gauss-Newton step. This practice  takes you under the hood of the algorithm, using the singular value decomposition (SVD) of the Jacobian matrix to derive the damped step and understand precisely how it stabilizes the optimization by intelligently managing parameter updates along sensitive and insensitive directions.",
            "id": "3911613",
            "problem": "Consider a two-parameter nonlinear model for a microdialysis-based cytokine concentration sensor in a biomedical system, with parameter vector $\\boldsymbol{\\theta} \\in \\mathbb{R}^{2}$ describing transport-rate and binding-affinity effects. The model is fit to $m=3$ measurements at times $t_{1}, t_{2}, t_{3}$, yielding a residual vector $\\boldsymbol{r}(\\boldsymbol{\\theta}) \\in \\mathbb{R}^{3}$ with components $r_{i}(\\boldsymbol{\\theta}) = y_{i}^{\\text{obs}} - y(t_{i}; \\boldsymbol{\\theta})$. In nonlinear least-squares regression, the objective is to minimize $S(\\boldsymbol{\\theta}) = \\frac{1}{2}\\|\\boldsymbol{r}(\\boldsymbol{\\theta})\\|_{2}^{2}$.\n\nAt a current iterate $\\boldsymbol{\\theta}_{0}$, the residual is $\\boldsymbol{r} = \\begin{pmatrix}0.5 & -0.3 & 0.1\\end{pmatrix}^{\\mathsf{T}}$, and the Jacobian matrix of partial derivatives with respect to the parameters, $\\boldsymbol{J}(\\boldsymbol{\\theta}_{0}) \\in \\mathbb{R}^{3 \\times 2}$, admits the thin singular value decomposition (SVD)\n$$\n\\boldsymbol{J} = \\boldsymbol{U}\\,\\boldsymbol{\\Sigma}\\,\\boldsymbol{V}^{\\mathsf{T}},\n$$\nwhere $\\boldsymbol{U} \\in \\mathbb{R}^{3 \\times 2}$ has orthonormal columns, $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{2 \\times 2}$ is diagonal with positive entries, and $\\boldsymbol{V} \\in \\mathbb{R}^{2 \\times 2}$ is orthogonal. The specific factors are\n$$\n\\boldsymbol{U} = \\begin{pmatrix}\n\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{6}} \\\\\n\\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{6}} \\\\\n0 & \\frac{2}{\\sqrt{6}}\n\\end{pmatrix},\\quad\n\\boldsymbol{\\Sigma} = \\begin{pmatrix}\n10 & 0 \\\\\n0 & \\frac{1}{10}\n\\end{pmatrix},\\quad\n\\boldsymbol{V} = \\begin{pmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{pmatrix}.\n$$\n\nStarting from the foundational definition of the nonlinear least-squares objective and its Gauss–Newton linearization, derive the damped least-squares (Levenberg–Marquardt) step $\\boldsymbol{p} \\in \\mathbb{R}^{2}$ from first principles by solving in the SVD basis with a positive damping parameter $\\lambda$. Explicitly show how the singular values enter the expression and explain how small singular values are treated by damping. Then, for the given factors and residual, evaluate the damped step for $\\lambda = \\frac{9}{10}$.\n\nExpress your final answer as a single row vector representing $\\boldsymbol{p}$, using exact analytical values with no rounding. The parameters are unitless for this computation, so no physical units should be included in the final answer.",
            "solution": "The problem is valid. It is a well-posed and self-contained problem in numerical optimization, specifically nonlinear least-squares regression using the Levenberg-Marquardt algorithm. The context is scientifically grounded, and all necessary data for the derivation and calculation are provided.\n\nThe objective in nonlinear least-squares is to find the parameter vector $\\boldsymbol{\\theta}$ that minimizes the sum of squared residuals, given by the function $S(\\boldsymbol{\\theta})$:\n$$\nS(\\boldsymbol{\\theta}) = \\frac{1}{2} \\|\\boldsymbol{r}(\\boldsymbol{\\theta})\\|_{2}^{2} = \\frac{1}{2} \\boldsymbol{r}(\\boldsymbol{\\theta})^{\\mathsf{T}}\\boldsymbol{r}(\\boldsymbol{\\theta})\n$$\nIterative methods are used to find the minimum. Given a current iterate $\\boldsymbol{\\theta}_{k}$, we seek a step $\\boldsymbol{p}$ to find the next iterate $\\boldsymbol{\\theta}_{k+1} = \\boldsymbol{\\theta}_{k} + \\boldsymbol{p}$. The Gauss-Newton method linearizes the residual function around $\\boldsymbol{\\theta}_{k}$:\n$$\n\\boldsymbol{r}(\\boldsymbol{\\theta}_{k} + \\boldsymbol{p}) \\approx \\boldsymbol{r}(\\boldsymbol{\\theta}_{k}) + \\boldsymbol{J}(\\boldsymbol{\\theta}_{k})\\boldsymbol{p}\n$$\nwhere $\\boldsymbol{J}(\\boldsymbol{\\theta}_{k})$ is the Jacobian matrix of $\\boldsymbol{r}$ at $\\boldsymbol{\\theta}_{k}$. Let $\\boldsymbol{r}_k = \\boldsymbol{r}(\\boldsymbol{\\theta}_k)$ and $\\boldsymbol{J}_k = \\boldsymbol{J}(\\boldsymbol{\\theta}_k)$. We minimize the linearized objective function:\n$$\n\\min_{\\boldsymbol{p}} \\frac{1}{2} \\|\\boldsymbol{r}_k + \\boldsymbol{J}_k \\boldsymbol{p}\\|_2^2\n$$\nThis is a linear least-squares problem, and its solution $\\boldsymbol{p}$ satisfies the normal equations:\n$$\n(\\boldsymbol{J}_k^{\\mathsf{T}}\\boldsymbol{J}_k)\\boldsymbol{p} = -\\boldsymbol{J}_k^{\\mathsf{T}}\\boldsymbol{r}_k\n$$\nThe damped least-squares, or Levenberg-Marquardt, method addresses potential ill-conditioning of the matrix $\\boldsymbol{J}_k^{\\mathsf{T}}\\boldsymbol{J}_k$ by adding a positive damping term $\\lambda > 0$. The step $\\boldsymbol{p}$ is then found by solving the modified system. We drop the subscript $k$ for the current iterate $\\boldsymbol{\\theta}_0$.\n$$\n(\\boldsymbol{J}^{\\mathsf{T}}\\boldsymbol{J} + \\lambda \\boldsymbol{I})\\boldsymbol{p} = -\\boldsymbol{J}^{\\mathsf{T}}\\boldsymbol{r}\n$$\nThis is the foundational equation from which we derive the solution. We are given the thin Singular Value Decomposition (SVD) of the Jacobian, $\\boldsymbol{J} = \\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{V}^{\\mathsf{T}}$. We substitute this into the equation.\n\nFirst, we express the terms $\\boldsymbol{J}^{\\mathsf{T}}\\boldsymbol{J}$ and $\\boldsymbol{J}^{\\mathsf{T}}\\boldsymbol{r}$ using the SVD factors:\n$$\n\\boldsymbol{J}^{\\mathsf{T}}\\boldsymbol{J} = (\\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{V}^{\\mathsf{T}})^{\\mathsf{T}}(\\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{V}^{\\mathsf{T}}) = \\boldsymbol{V}\\boldsymbol{\\Sigma}^{\\mathsf{T}}\\boldsymbol{U}^{\\mathsf{T}}\\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{V}^{\\mathsf{T}}\n$$\nSince the columns of $\\boldsymbol{U}$ are orthonormal, $\\boldsymbol{U}^{\\mathsf{T}}\\boldsymbol{U} = \\boldsymbol{I}$. Also, $\\boldsymbol{\\Sigma}$ is diagonal, so $\\boldsymbol{\\Sigma}^{\\mathsf{T}} = \\boldsymbol{\\Sigma}$.\n$$\n\\boldsymbol{J}^{\\mathsf{T}}\\boldsymbol{J} = \\boldsymbol{V}\\boldsymbol{\\Sigma}\\boldsymbol{I}\\boldsymbol{\\Sigma}\\boldsymbol{V}^{\\mathsf{T}} = \\boldsymbol{V}\\boldsymbol{\\Sigma}^{2}\\boldsymbol{V}^{\\mathsf{T}}\n$$\nThe right-hand side becomes:\n$$\n\\boldsymbol{J}^{\\mathsf{T}}\\boldsymbol{r} = (\\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{V}^{\\mathsf{T}})^{\\mathsf{T}}\\boldsymbol{r} = \\boldsymbol{V}\\boldsymbol{\\Sigma}^{\\mathsf{T}}\\boldsymbol{U}^{\\mathsf{T}}\\boldsymbol{r} = \\boldsymbol{V}\\boldsymbol{\\Sigma}\\boldsymbol{U}^{\\mathsf{T}}\\boldsymbol{r}\n$$\nSubstituting these into the Levenberg-Marquardt equation gives:\n$$\n(\\boldsymbol{V}\\boldsymbol{\\Sigma}^{2}\\boldsymbol{V}^{\\mathsf{T}} + \\lambda \\boldsymbol{I})\\boldsymbol{p} = -\\boldsymbol{V}\\boldsymbol{\\Sigma}\\boldsymbol{U}^{\\mathsf{T}}\\boldsymbol{r}\n$$\nSince $\\boldsymbol{V}$ is an orthogonal matrix, $\\boldsymbol{V}\\boldsymbol{V}^{\\mathsf{T}} = \\boldsymbol{I}$. We can write $\\lambda \\boldsymbol{I} = \\lambda \\boldsymbol{V}\\boldsymbol{V}^{\\mathsf{T}}$ and substitute it into the left-hand side:\n$$\n(\\boldsymbol{V}\\boldsymbol{\\Sigma}^{2}\\boldsymbol{V}^{\\mathsf{T}} + \\lambda \\boldsymbol{V}\\boldsymbol{V}^{\\mathsf{T}})\\boldsymbol{p} = \\boldsymbol{V}(\\boldsymbol{\\Sigma}^{2} + \\lambda \\boldsymbol{I})\\boldsymbol{V}^{\\mathsf{T}}\\boldsymbol{p} = -\\boldsymbol{V}\\boldsymbol{\\Sigma}\\boldsymbol{U}^{\\mathsf{T}}\\boldsymbol{r}\n$$\nLeft-multiplying by $\\boldsymbol{V}^{\\mathsf{T}}$ (which is $\\boldsymbol{V}^{-1}$) isolates the term containing $\\boldsymbol{p}$:\n$$\n\\boldsymbol{V}^{\\mathsf{T}}\\boldsymbol{V}(\\boldsymbol{\\Sigma}^{2} + \\lambda \\boldsymbol{I})\\boldsymbol{V}^{\\mathsf{T}}\\boldsymbol{p} = -\\boldsymbol{V}^{\\mathsf{T}}\\boldsymbol{V}\\boldsymbol{\\Sigma}\\boldsymbol{U}^{\\mathsf{T}}\\boldsymbol{r}\n$$\n$$\n(\\boldsymbol{\\Sigma}^{2} + \\lambda \\boldsymbol{I})\\boldsymbol{V}^{\\mathsf{T}}\\boldsymbol{p} = -\\boldsymbol{\\Sigma}\\boldsymbol{U}^{\\mathsf{T}}\\boldsymbol{r}\n$$\nThe matrix $(\\boldsymbol{\\Sigma}^{2} + \\lambda \\boldsymbol{I})$ is diagonal with entries $(\\sigma_j^2 + \\lambda)$, where $\\sigma_j$ are the singular values. Its inverse is also diagonal with entries $1/(\\sigma_j^2 + \\lambda)$. We can solve for $\\boldsymbol{V}^{\\mathsf{T}}\\boldsymbol{p}$:\n$$\n\\boldsymbol{V}^{\\mathsf{T}}\\boldsymbol{p} = -(\\boldsymbol{\\Sigma}^{2} + \\lambda \\boldsymbol{I})^{-1}\\boldsymbol{\\Sigma}\\boldsymbol{U}^{\\mathsf{T}}\\boldsymbol{r}\n$$\nFinally, left-multiplying by $\\boldsymbol{V}$ yields the expression for the step $\\boldsymbol{p}$:\n$$\n\\boldsymbol{p} = -\\boldsymbol{V}(\\boldsymbol{\\Sigma}^{2} + \\lambda \\boldsymbol{I})^{-1}\\boldsymbol{\\Sigma}\\boldsymbol{U}^{\\mathsf{T}}\\boldsymbol{r}\n$$\nTo explicitly show how singular values enter the expression, we can write $\\boldsymbol{p}$ as a sum. Let $\\boldsymbol{u}_j$ and $\\boldsymbol{v}_j$ be the columns of $\\boldsymbol{U}$ and $\\boldsymbol{V}$ respectively. The step vector $\\boldsymbol{p}$ is:\n$$\n\\boldsymbol{p} = \\sum_{j=1}^{n} \\left( \\frac{-\\sigma_j}{\\sigma_j^2 + \\lambda} \\right) (\\boldsymbol{u}_j^{\\mathsf{T}}\\boldsymbol{r}) \\boldsymbol{v}_j\n$$\nThis expression shows that the step $\\boldsymbol{p}$ is a linear combination of the right singular vectors $\\boldsymbol{v}_j$. The coefficient for each $\\boldsymbol{v}_j$ is determined by the corresponding singular value $\\sigma_j$, the damping parameter $\\lambda$, and the projection of the residual $\\boldsymbol{r}$ onto the corresponding left singular vector $\\boldsymbol{u}_j$.\n\nThe role of damping is to stabilize the step calculation when small singular values are present. The scaling factor for the $j$-th component is $\\frac{\\sigma_j}{\\sigma_j^2 + \\lambda}$.\nIf $\\sigma_j$ is large (i.e., $\\sigma_j^2 \\gg \\lambda$), this factor is approximately $\\frac{\\sigma_j}{\\sigma_j^2} = \\frac{1}{\\sigma_j}$. This is the same scaling as the undamped Gauss-Newton step.\nIf $\\sigma_j$ is small (i.e., $\\sigma_j^2 \\ll \\lambda$), this factor is approximately $\\frac{\\sigma_j}{\\lambda}$. Without damping, the factor would be $\\frac{1}{\\sigma_j}$, which would be very large, causing a large and unreliable step in the direction $\\boldsymbol{v}_j$. The damping parameter $\\lambda$ in the denominator ensures that the contribution from directions associated with small singular values is suppressed, preventing the step $\\boldsymbol{p}$ from becoming excessively large. This yields a more stable and robust optimization process.\n\nNow, we evaluate the damped step $\\boldsymbol{p}$ for the given data.\nThe residual is $\\boldsymbol{r} = \\begin{pmatrix} 0.5 \\\\ -0.3 \\\\ 0.1 \\end{pmatrix}$. The damping parameter is $\\lambda = \\frac{9}{10}$.\nThe SVD factors are:\n$\\boldsymbol{U} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{6}} \\\\ \\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{6}} \\\\ 0 & \\frac{2}{\\sqrt{6}} \\end{pmatrix}$, $\\boldsymbol{\\Sigma} = \\begin{pmatrix} 10 & 0 \\\\ 0 & \\frac{1}{10} \\end{pmatrix}$, $\\boldsymbol{V} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$.\nThe columns of $\\boldsymbol{U}$ are $\\boldsymbol{u}_1 = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\\\ 0 \\end{pmatrix}$ and $\\boldsymbol{u}_2 = \\begin{pmatrix} \\frac{1}{\\sqrt{6}} \\\\ -\\frac{1}{\\sqrt{6}} \\\\ \\frac{2}{\\sqrt{6}} \\end{pmatrix}$.\nThe singular values are $\\sigma_1 = 10$ and $\\sigma_2 = \\frac{1}{10}$.\nSince $\\boldsymbol{V} = \\boldsymbol{I}$, we have $\\boldsymbol{p} = \\boldsymbol{V}^{\\mathsf{T}}\\boldsymbol{p}$, so $\\boldsymbol{p}$ can be computed directly from $\\boldsymbol{p} = -(\\boldsymbol{\\Sigma}^{2} + \\lambda \\boldsymbol{I})^{-1}\\boldsymbol{\\Sigma}\\boldsymbol{U}^{\\mathsf{T}}\\boldsymbol{r}$.\n\nFirst, calculate the vector $\\boldsymbol{U}^{\\mathsf{T}}\\boldsymbol{r}$:\n$$\n\\boldsymbol{u}_1^{\\mathsf{T}}\\boldsymbol{r} = \\frac{1}{\\sqrt{2}}(0.5) + \\frac{1}{\\sqrt{2}}(-0.3) + 0(0.1) = \\frac{0.2}{\\sqrt{2}} = \\frac{2}{10\\sqrt{2}} = \\frac{1}{5\\sqrt{2}}\n$$\n$$\n\\boldsymbol{u}_2^{\\mathsf{T}}\\boldsymbol{r} = \\frac{1}{\\sqrt{6}}(0.5) - \\frac{1}{\\sqrt{6}}(-0.3) + \\frac{2}{\\sqrt{6}}(0.1) = \\frac{1}{\\sqrt{6}}(0.5 + 0.3 + 0.2) = \\frac{1}{\\sqrt{6}}\n$$\nSo, $\\boldsymbol{U}^{\\mathsf{T}}\\boldsymbol{r} = \\begin{pmatrix} \\frac{1}{5\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{6}} \\end{pmatrix}$.\n\nNext, we compute the components of $\\boldsymbol{p}$.\nFor the first component, $p_1$:\n$$\np_1 = -\\left( \\frac{\\sigma_1}{\\sigma_1^2 + \\lambda} \\right) (\\boldsymbol{u}_1^{\\mathsf{T}}\\boldsymbol{r}) = -\\left( \\frac{10}{10^2 + \\frac{9}{10}} \\right) \\left( \\frac{1}{5\\sqrt{2}} \\right) = -\\left( \\frac{10}{100 + \\frac{9}{10}} \\right) \\left( \\frac{1}{5\\sqrt{2}} \\right)\n$$\n$$\np_1 = -\\left( \\frac{10}{\\frac{1000+9}{10}} \\right) \\left( \\frac{1}{5\\sqrt{2}} \\right) = -\\left( \\frac{100}{1009} \\right) \\left( \\frac{1}{5\\sqrt{2}} \\right) = -\\frac{20}{1009\\sqrt{2}} = -\\frac{20\\sqrt{2}}{1009 \\cdot 2} = -\\frac{10\\sqrt{2}}{1009}\n$$\nFor the second component, $p_2$:\n$$\np_2 = -\\left( \\frac{\\sigma_2}{\\sigma_2^2 + \\lambda} \\right) (\\boldsymbol{u}_2^{\\mathsf{T}}\\boldsymbol{r}) = -\\left( \\frac{\\frac{1}{10}}{(\\frac{1}{10})^2 + \\frac{9}{10}} \\right) \\left( \\frac{1}{\\sqrt{6}} \\right) = -\\left( \\frac{\\frac{1}{10}}{\\frac{1}{100} + \\frac{90}{100}} \\right) \\left( \\frac{1}{\\sqrt{6}} \\right)\n$$\n$$\np_2 = -\\left( \\frac{\\frac{1}{10}}{\\frac{91}{100}} \\right) \\left( \\frac{1}{\\sqrt{6}} \\right) = -\\left( \\frac{1}{10} \\cdot \\frac{100}{91} \\right) \\left( \\frac{1}{\\sqrt{6}} \\right) = -\\frac{10}{91\\sqrt{6}} = -\\frac{10\\sqrt{6}}{91 \\cdot 6} = -\\frac{10\\sqrt{6}}{546} = -\\frac{5\\sqrt{6}}{273}\n$$\nThe step vector $\\boldsymbol{p}$ is $\\begin{pmatrix} -\\frac{10\\sqrt{2}}{1009} \\\\ -\\frac{5\\sqrt{6}}{273} \\end{pmatrix}$.\nThe problem asks for the answer as a single row vector.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n-\\frac{10\\sqrt{2}}{1009} & -\\frac{5\\sqrt{6}}{273}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "After building a complex model, a critical question arises: can all of its parameters be uniquely determined from experimental data? This is the question of identifiability, which can be a fundamental structural property of the model or a practical limitation of a specific experimental design. This advanced exercise  introduces a principled framework for distinguishing between these two scenarios by analyzing the aggregated Jacobian matrix from multiple, distinct experiments, providing a powerful diagnostic tool to assess and improve both your model and your experimental strategy.",
            "id": "3911599",
            "problem": "In a nonlinear least-squares regression setting for biomedical systems modeling, consider a continuous-time model with known, user-controlled inputs. Let the system be described by an ordinary differential equation (ODE) state model with outputs\n$$\n\\dot{x}(t) = f\\big(x(t), u(t), \\theta\\big), \\quad y(t) = h\\big(x(t), \\theta\\big),\n$$\nwhere $x(t) \\in \\mathbb{R}^{n_x}$ is the state, $u(t) \\in \\mathbb{R}^{n_u}$ is the input, $\\theta \\in \\mathbb{R}^{p}$ is the parameter vector, and $y(t) \\in \\mathbb{R}^{n_y}$ is the model output. Suppose $E$ distinct experiments are conducted, each with a different input signal $u_e(t)$ and possibly different initial conditions $x_e(0)$, indexed by $e \\in \\{1,\\dots,E\\}$. For experiment $e$, measurements are collected at times $\\{t_{e,i}\\}_{i=1}^{N_e}$, yielding observed data $\\{y^{\\text{obs}}_{e,i}\\}_{i=1}^{N_e}$. Define residuals and a weighted nonlinear least-squares (NLS) objective\n$$\nr_{e,i}(\\theta) = y^{\\text{obs}}_{e,i} - h\\big(x_e(t_{e,i}; \\theta, u_e, x_e(0)), \\theta\\big), \\quad S(\\theta) = \\frac{1}{2}\\sum_{e=1}^{E} \\sum_{i=1}^{N_e} w_{e,i}\\, r_{e,i}(\\theta)^2,\n$$\nwith known positive weights $w_{e,i} > 0$. Let $J_e(\\theta) \\in \\mathbb{R}^{N_e n_y \\times p}$ denote the sensitivity (Jacobian) of the stacked residual vector for experiment $e$ with respect to $\\theta$, and $W_e$ the corresponding positive definite block-diagonal weight matrix with diagonal entries $w_{e,i}$. The Gauss–Newton approximation to the Fisher Information Matrix (FIM) for experiment $e$ is $F_e(\\theta) = J_e(\\theta)^{\\top} W_e J_e(\\theta)$. The aggregated sensitivity across experiments can be written as the row-stacked Jacobian\n$$\nJ_{\\text{stack}}(\\theta) = \\begin{bmatrix}\nW_1^{1/2} J_1(\\theta) \\\\ \\vdots \\\\ W_E^{1/2} J_E(\\theta)\n\\end{bmatrix},\n$$\nso that the aggregated Gauss–Newton FIM equals $F_{\\text{stack}}(\\theta) = J_{\\text{stack}}(\\theta)^{\\top} J_{\\text{stack}}(\\theta)$.\n\nConsider the notions of structural identifiability of parameters based on the parameter-to-output map $\\theta \\mapsto \\{y_e(t;\\theta)\\}_{e=1}^{E}$, where $y_e(t;\\theta)$ denotes the model output under input $u_e$ and initial condition $x_e(0)$, and also the concept of practical, design-dependent flat directions in $S(\\theta)$ associated with local ill-conditioning or rank deficiency of $J_e(\\theta)$ at a candidate $\\hat{\\theta}$.\n\nWhich option correctly explains the difference between global and local structural identifiability in this context, and provides a principled criterion using multiple experiment settings $\\{u_e\\}_{e=1}^{E}$ to distinguish directions that are only locally flat under a specific experiment from parameter combinations that are truly non-identifiable?\n\nA. Global structural identifiability means that the parameter-to-output map is injective for almost all parameter values across an admissible class of inputs, whereas local structural identifiability means the map is injective only in a neighborhood of a specific parameter value (possibly up to a finite number of indistinguishable parameter sets). A practical criterion is to compute $J_{\\text{stack}}(\\hat{\\theta})$ at a candidate $\\hat{\\theta}$ for several sufficiently distinct experiments and examine its column rank. If $\\mathrm{rank}\\big(J_{\\text{stack}}(\\hat{\\theta})\\big) = p$, then any rank deficiency observed in a single-experiment $J_e(\\hat{\\theta})$ was due to a locally flat, design-induced direction (practical non-identifiability). If a nontrivial nullspace persists, i.e., $\\ker\\big(J_{\\text{stack}}(\\hat{\\theta})\\big) \\neq \\{0\\}$ despite varied inputs, then the corresponding parameter combinations are not identifiable from any linear combination of the chosen experiments and are indicative of structural non-identifiability unless the input class is further enlarged.\n\nB. Global structural identifiability is guaranteed whenever the Hessian $\\nabla^2 S(\\hat{\\theta})$ from any single experiment is positive definite at its minimizer $\\hat{\\theta}$, and local identifiability only requires that at least one diagonal entry of $\\nabla^2 S(\\hat{\\theta})$ be positive. To distinguish flat versus non-identifiable directions, it suffices to check the determinants $\\det(F_e(\\hat{\\theta}))$ for each experiment separately: if any $\\det(F_e(\\hat{\\theta}))=0$, the affected parameters are globally non-identifiable.\n\nC. Global structural identifiability requires that the Jacobian $J_e(\\theta)$ have full column rank $p$ for at least one experiment $e$, while local identifiability requires that $J_e(\\theta)$ have full row rank. Multiple experiments do not change identifiability conclusions; if a parameter is unidentifiable in each single experiment, then it remains unidentifiable under any combination, and changing inputs or initial conditions cannot resolve flat directions.\n\nD. Global structural identifiability is equivalent to the FIM $F_e(\\theta)$ being diagonal for some experiment $e$, ensuring independent estimation of parameters, while local identifiability corresponds to $F_e(\\theta)$ being merely symmetric. To distinguish flat directions from structural non-identifiability, compare the smallest eigenvalue of $F_e(\\hat{\\theta})$ across experiments: if it increases under any experiment, the parameter is globally identifiable; otherwise, it is structurally non-identifiable.",
            "solution": "The user wants a critical validation of the provided problem statement, followed by a detailed solution and evaluation of the given options.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- **System Model**: Continuous-time ordinary differential equation (ODE) state model with outputs.\n    - State equation: $\\dot{x}(t) = f\\big(x(t), u(t), \\theta\\big)$\n    - Output equation: $y(t) = h\\big(x(t), \\theta\\big)$\n- **Variables and Dimensions**:\n    - State: $x(t) \\in \\mathbb{R}^{n_x}$\n    - Input: $u(t) \\in \\mathbb{R}^{n_u}$\n    - Parameter vector: $\\theta \\in \\mathbb{R}^{p}$\n    - Model output: $y(t) \\in \\mathbb{R}^{n_y}$\n- **Experimental Setup**:\n    - Number of experiments: $E$\n    - Per-experiment index: $e \\in \\{1,\\dots,E\\}$\n    - Per-experiment input signal: $u_e(t)$\n    - Per-experiment initial conditions: $x_e(0)$\n    - Per-experiment measurement times: $\\{t_{e,i}\\}_{i=1}^{N_e}$\n    - Per-experiment observed data: $\\{y^{\\text{obs}}_{e,i}\\}_{i=1}^{N_e}$\n- **Nonlinear Least-Squares (NLS) Formulation**:\n    - Residuals: $r_{e,i}(\\theta) = y^{\\text{obs}}_{e,i} - h\\big(x_e(t_{e,i}; \\theta, u_e, x_e(0)), \\theta\\big)$\n    - Weighted NLS objective function: $S(\\theta) = \\frac{1}{2}\\sum_{e=1}^{E} \\sum_{i=1}^{N_e} w_{e,i}\\, r_{e,i}(\\theta)^2$\n    - Weights: $w_{e,i} > 0$\n- **Matrices and Sensitivities**:\n    - Per-experiment sensitivity (Jacobian): $J_e(\\theta) \\in \\mathbb{R}^{N_e n_y \\times p}$\n    - Per-experiment weight matrix: $W_e$, positive definite, block-diagonal with entries $w_{e,i}$\n    - Per-experiment Gauss–Newton FIM: $F_e(\\theta) = J_e(\\theta)^{\\top} W_e J_e(\\theta)$\n    - Aggregated row-stacked Jacobian: $J_{\\text{stack}}(\\theta) = \\begin{bmatrix} W_1^{1/2} J_1(\\theta) \\\\ \\vdots \\\\ W_E^{1/2} J_E(\\theta) \\end{bmatrix}$\n    - Aggregated Gauss–Newton FIM: $F_{\\text{stack}}(\\theta) = J_{\\text{stack}}(\\theta)^{\\top} J_{\\text{stack}}(\\theta)$\n- **Concepts**:\n    - Structural identifiability (global and local) based on the map $\\theta \\mapsto \\{y_e(t;\\theta)\\}_{e=1}^{E}$.\n    - Practical, design-dependent flat directions in $S(\\theta)$ associated with ill-conditioning of $J_e(\\theta)$.\n- **Question**: Which option correctly explains the difference between global and local structural identifiability and provides a principled criterion using multiple experiments to distinguish locally flat directions from true non-identifiability?\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded**: The problem statement is firmly rooted in the established theory of system identification, parameter estimation, and nonlinear regression, particularly as applied to dynamic systems in fields like biomedical engineering and systems biology. All definitions provided—the NLS objective function, the Jacobian (sensitivity matrix), the Gauss-Newton approximation to the Hessian, and the Fisher Information Matrix (FIM)—are standard and mathematically correct. The formulation of aggregating information from multiple experiments by summing FIMs or stacking Jacobians is also canonical.\n- **Well-Posed**: The question is well-posed. It asks for the correct conceptual explanation and a corresponding mathematical criterion, which is a standard task in theoretical and applied engineering disciplines. A clear, unique, and meaningful answer exists based on identifiability theory.\n- **Objective**: The language is precise, formal, and free of any subjective or opinion-based content.\n\n**Flaw Checklist**:\n1.  **Scientific or Factual Unsoundness**: None. All concepts and equations are standard and correct.\n2.  **Non-Formalizable or Irrelevant**: The problem is highly formalizable and directly relevant to the stated topic.\n3.  **Incomplete or Contradictory Setup**: The setup is comprehensive. The definitions are self-consistent. For example, it is correctly stated that $F_{\\text{stack}}(\\theta) = J_{\\text{stack}}(\\theta)^{\\top} J_{\\text{stack}}(\\theta)$, which follows from the definitions of $J_{\\text{stack}}$ and the fact that $F_{\\text{stack}}(\\theta) = \\sum_{e=1}^E F_e(\\theta) = \\sum_{e=1}^E J_e(\\theta)^\\top W_e J_e(\\theta)$.\n4.  **Unrealistic or Infeasible**: No unrealistic conditions are present.\n5.  **Ill-Posed or Poorly Structured**: The terminology used (structural vs. practical identifiability, flat directions) is standard and well-defined in this context. The question's structure is logical.\n6.  **Pseudo-Profound, Trivial, or Tautological**: The problem addresses a non-trivial and fundamental challenge in parameterizing complex models from data.\n7.  **Outside Scientific Verifiability**: The concepts are mathematically defined and verifiable.\n\n**Step 3: Verdict and Action**\nThe problem statement is **valid**. It is a well-formulated, scientifically sound question concerning core concepts in parameter estimation for dynamic systems. I will proceed with a full solution.\n\n### Principle-Based Derivation and Option Analysis\n\nThe core of the problem lies in distinguishing two types of parameter non-identifiability:\n1.  **Structural Non-identifiability**: A property of the model itself, independent of the quantity or quality of data. It implies that even with perfect, continuous, noise-free data, the parameter vector $\\theta$ cannot be uniquely determined because different parameter vectors $\\theta_1 \\neq \\theta_2$ produce identical model outputs $y(t;\\theta_1) = y(t;\\theta_2)$ for all time $t$ and all admissible inputs $u(t)$.\n    -   **Global Structural Identifiability (GSI)**: The model is GSI if for almost any $\\theta_1$, the condition $y(t;\\theta_1) = y(t;\\theta_2)$ for all $t$ and all admissible inputs implies $\\theta_1 = \\theta_2$.\n    -   **Local Structural Identifiability (LSI)**: The model is LSI at a point $\\theta^*$ if there exists a neighborhood around $\\theta^*$ such that for any $\\theta$ in that neighborhood, $y(t;\\theta) = y(t;\\theta^*)$ implies $\\theta = \\theta^*$. LSI is a necessary but not sufficient condition for GSI.\n\n2.  **Practical Non-identifiability**: A property of a specific experiment. The model may be structurally identifiable, but the chosen experimental design (input signal $u_e(t)$, sampling times $\\{t_{e,i}\\}$, initial conditions $x_e(0)$) and the presence of measurement noise make it impossible to estimate certain parameters with adequate precision. This manifests as an ill-conditioned estimation problem, characterized by a nearly \"flat\" objective function $S(\\theta)$ in certain directions.\n\nThe link between these concepts and the given matrices is the **Fisher Information Matrix (FIM)**, approximated here by the Gauss-Newton method as $F_e(\\theta) = J_e(\\theta)^{\\top} W_e J_e(\\theta)$. The FIM is related to the curvature of the log-likelihood function (or the NLS objective function $S(\\theta)$) at a given $\\theta$. A singular FIM at $\\theta$ indicates that the objective function is locally flat along the directions specified by the nullspace of the FIM, making the parameters locally non-identifiable. The rank of $F_e(\\theta)$ is equal to the column rank of $J_e(\\theta)$. Thus, a model is locally structurally identifiable if the Jacobian of the continuous output, $\\partial y(t;\\theta)/\\partial\\theta$, has full column rank. For discrete data, we check the rank of the Jacobian matrix $J_e(\\theta)$. If $\\mathrm{rank}(J_e(\\theta)) < p$, the parameters are not locally identifiable from experiment $e$.\n\nThe key to the question is how to use multiple experiments. If a rank deficiency in $J_e(\\theta)$ is due to *practical non-identifiability*, it is specific to the design of experiment $e$. A different experiment, say experiment $k$, with a different input $u_k(t)$, might excite the system dynamics in a way that resolves the ambiguity. In this case, $J_k(\\theta)$ would not share the same nullspace as $J_e(\\theta)$.\n\nCombining information from all $E$ experiments is achieved by considering the aggregated FIM, $F_{\\text{stack}}(\\theta) = \\sum_{e=1}^E F_e(\\theta)$. A parameter combination $v \\in \\mathbb{R}^p$ is unidentifiable from the combined experiment if it lies in the nullspace of $F_{\\text{stack}}(\\theta)$.\n$$ F_{\\text{stack}}(\\theta)v = 0 \\iff \\left(\\sum_{e=1}^E J_e(\\theta)^{\\top} W_e J_e(\\theta)\\right) v = 0 \\iff \\sum_{e=1}^E v^{\\top} J_e(\\theta)^{\\top} W_e J_e(\\theta) v = 0 $$\nSince each weight matrix $W_e$ is positive definite, each term $v^{\\top} J_e(\\theta)^{\\top} W_e J_e(\\theta) v = \\|(W_e)^{1/2} J_e(\\theta) v\\|^2$ is non-negative. The sum is zero if and only if every term is zero, i.e., $J_e(\\theta)v = 0$ for all $e \\in \\{1,\\dots,E\\}$.\nThis means that the nullspace of the aggregated FIM is the intersection of the nullspaces of the individual FIMs: $\\ker(F_{\\text{stack}}(\\theta)) = \\bigcap_{e=1}^E \\ker(F_e(\\theta))$.\n\nEquivalently, using the stacked Jacobian $J_{\\text{stack}}(\\theta)$, the aggregated FIM is $F_{\\text{stack}}(\\theta) = J_{\\text{stack}}(\\theta)^{\\top} J_{\\text{stack}}(\\theta)$. $F_{\\text{stack}}(\\theta)$ has full rank $p$ if and only if $J_{\\text{stack}}(\\theta)$ has full column rank $p$.\n\n**Criterion**:\n1.  If a single-experiment Jacobian $J_e(\\hat{\\theta})$ is rank-deficient (i.e., $\\mathrm{rank}(J_e(\\hat{\\theta})) < p$), but the aggregated Jacobian $J_{\\text{stack}}(\\hat{\\theta})$ has full rank ($\\mathrm{rank}(J_{\\text{stack}}(\\hat{\\theta})) = p$), the non-identifiability observed in experiment $e$ was a *practical* one, induced by the specific design of that experiment. The combination of experiments resolved it.\n2.  If the aggregated Jacobian $J_{\\text{stack}}(\\hat{\\theta})$ is still rank-deficient after combining a set of rich and varied experiments, it is strong evidence for *structural* non-identifiability. The nullspace of $J_{\\text{stack}}(\\hat{\\theta})$ reveals the parameter combinations that are fundamentally unresolvable with the given model structure and the class of inputs used.\n\nNow, we evaluate the options based on this principled analysis.\n\n**Option A**:\n- **Identifiability Definitions**: It correctly states that global structural identifiability relates to the injectivity of the parameter-to-output map, while local structural identifiability refers to injectivity in a local neighborhood. This is a correct conceptual distinction.\n- **Criterion**: It proposes to examine the column rank of $J_{\\text{stack}}(\\hat{\\theta})$. It correctly states that if $\\mathrm{rank}(J_{\\text{stack}}(\\hat{\\theta})) = p$, any rank deficiency in a single $J_e(\\hat{\\theta})$ points to a design-induced, practical non-identifiability. It also correctly states that a persistent nullspace in $J_{\\text{stack}}(\\hat{\\theta})$ across varied inputs is indicative of structural non-identifiability. This entire line of reasoning is consistent with our derivation.\n- **Verdict**: **Correct**.\n\n**Option B**:\n- **Identifiability Definitions**: It incorrectly equates global identifiability with a positive definite Hessian from a single experiment (this ensures *local* identifiability). It also presents a nonsensical condition for local identifiability (\"at least one diagonal entry... be positive\").\n- **Criterion**: It incorrectly claims that if $\\det(F_e(\\hat{\\theta}))=0$ for any single experiment, the issue is global non-identifiability. As explained, a rank-deficient $F_e(\\hat{\\theta})$ only signals local non-identifiability for that specific experiment, which may be a practical issue solvable by other experiments.\n- **Verdict**: **Incorrect**.\n\n**Option C**:\n- **Identifiability Definitions**: It confuses the conditions for local and global identifiability and the roles of column and row rank. Full column rank of the Jacobian is the standard test for *local* identifiability. The Jacobian is typically a \"tall\" matrix ($N_e n_y \\gg p$), so full row rank is not expected or relevant.\n- **Criterion**: The statement \"Multiple experiments do not change identifiability conclusions\" is fundamentally false. The primary motivation for multi-experiment design is often to resolve identifiability issues present in single experiments.\n- **Verdict**: **Incorrect**.\n\n**Option D**:\n- **Identifiability Definitions**: It makes bizarre claims: GSI is equivalent to a diagonal FIM, and LSI corresponds to a symmetric FIM. A diagonal FIM implies uncorrelated parameter estimates, not GSI. An FIM is *always* symmetric by definition. LSI requires the FIM to be non-singular (positive definite).\n- **Criterion**: The criterion based on the smallest eigenvalue is poorly formulated and does not correctly distinguish the concepts. While the smallest eigenvalue is a useful diagnostic for practical identifiability, the logic presented is incorrect and does not lead to a conclusion about global or structural identifiability.\n- **Verdict**: **Incorrect**.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}