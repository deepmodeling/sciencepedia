{
    "hands_on_practices": [
        {
            "introduction": "The induced matrix $2$-norm, $\\|A\\|_2$, is a cornerstone concept for analyzing linear systems. This exercise will guide you through connecting this abstract definition to a concrete, physical meaning: the maximum amplification factor that a system, represented by matrix $A$, can apply to an input signal. By computing this for a neural connectivity matrix, you will gain a deeper intuition for how a matrix's intrinsic properties dictate the worst-case response and stability of a biomedical network .",
            "id": "3899454",
            "problem": "Consider a Linear Time-Invariant (LTI) network model of a microcircuit with state vector $\\mathbf{x} \\in \\mathbb{R}^{5}$ governed locally by the linear map $\\mathbf{x} \\mapsto A \\mathbf{x}$, where $A \\in \\mathbb{R}^{5 \\times 5}$ is a symmetric, dimensionless connectivity matrix representing undirected coupling strengths. The $i$th diagonal entry of $A$ quantifies the intrinsic self-coupling of subpopulation $i$, and each off-diagonal entry quantifies uniform bidirectional coupling between distinct subpopulations. Specifically, let\n$$\nA \\;=\\; \\frac{1}{5}\\begin{pmatrix}\n3 & 1 & 1 & 1 & 1 \\\\\n1 & 3 & 1 & 1 & 1 \\\\\n1 & 1 & 3 & 1 & 1 \\\\\n1 & 1 & 1 & 3 & 1 \\\\\n1 & 1 & 1 & 1 & 3 \\\\\n\\end{pmatrix}.\n$$\nStarting from the core definitions of the vector $2$-norm $\\|\\mathbf{x}\\|_{2} = \\sqrt{\\mathbf{x}^{\\top}\\mathbf{x}}$, the induced (operator) matrix $2$-norm\n$$\n\\|A\\|_{2} \\;=\\; \\sup_{\\|\\mathbf{x}\\|_{2} = 1} \\|A\\mathbf{x}\\|_{2},\n$$\nand the singular values of $A$ as the square roots of the eigenvalues of $A^{\\top}A$, derive an expression for $\\|A\\|_{2}$ for symmetric $A$ and compute $\\|A\\|_{2}$ for the given connectivity matrix. In your derivation, use only these foundational definitions and well-tested facts about eigenvalues of $A^{\\top}A$ and the all-ones matrix. Briefly justify the implication of the computed $\\|A\\|_{2}$ for worst-case amplification of small input perturbations under the instantaneous linear map $\\mathbf{x} \\mapsto A\\mathbf{x}$ in this microcircuit. Express the final numerical value of $\\|A\\|_{2}$ as a rational number with no units, and no rounding is required.",
            "solution": "The problem statement is evaluated to be valid. It is scientifically grounded in linear systems theory, well-posed, objective, self-contained, and mathematically verifiable. The problem requires a standard derivation and calculation in matrix analysis within a plausible application context.\n\nWe begin by establishing the relationship between the induced $2$-norm $\\|A\\|_{2}$ and the eigenvalues of a symmetric matrix $A$. The definition of the induced $2$-norm is\n$$\n\\|A\\|_{2} = \\sup_{\\|\\mathbf{x}\\|_{2} = 1} \\|A\\mathbf{x}\\|_{2}\n$$\nwhere $\\|\\mathbf{x}\\|_{2} = \\sqrt{\\mathbf{x}^{\\top}\\mathbf{x}}$.\n\nSince the norm is always non-negative, we can equivalently work with its square:\n$$\n\\|A\\|_{2}^2 = \\left( \\sup_{\\|\\mathbf{x}\\|_{2} = 1} \\|A\\mathbf{x}\\|_{2} \\right)^2 = \\sup_{\\|\\mathbf{x}\\|_{2} = 1} \\|A\\mathbf{x}\\|_{2}^2\n$$\nUsing the definition of the vector $2$-norm, we can rewrite the term inside the supremum:\n$$\n\\|A\\mathbf{x}\\|_{2}^2 = (A\\mathbf{x})^{\\top}(A\\mathbf{x}) = \\mathbf{x}^{\\top}A^{\\top}A\\mathbf{x}\n$$\nThus, we have\n$$\n\\|A\\|_{2}^2 = \\sup_{\\|\\mathbf{x}\\|_{2} = 1} \\mathbf{x}^{\\top}(A^{\\top}A)\\mathbf{x}\n$$\nThe expression $\\mathbf{x}^{\\top}(A^{\\top}A)\\mathbf{x}$ for $\\|\\mathbf{x}\\|_{2}=1$ is the Rayleigh quotient of the matrix $A^{\\top}A$. A fundamental result of linear algebra states that the supremum of the Rayleigh quotient of a symmetric matrix is its largest eigenvalue. The matrix $A^{\\top}A$ is always symmetric. Let $\\lambda_{\\max}(M)$ denote the largest eigenvalue of a compatible matrix $M$. Then,\n$$\n\\|A\\|_{2}^2 = \\lambda_{\\max}(A^{\\top}A)\n$$\nBy the problem's definition, the singular values of $A$, denoted $\\sigma_i$, are the square roots of the eigenvalues of $A^{\\top}A$. The largest singular value is $\\sigma_{\\max} = \\sqrt{\\lambda_{\\max}(A^{\\top}A)}$. Therefore, in general, $\\|A\\|_{2} = \\sigma_{\\max}(A)$.\n\nFor the specific case where $A$ is symmetric, we have $A^{\\top} = A$. The expression for the squared norm becomes:\n$$\n\\|A\\|_{2}^2 = \\lambda_{\\max}(A^{\\top}A) = \\lambda_{\\max}(A^2)\n$$\nIf $\\lambda_i$ are the eigenvalues of $A$ with corresponding eigenvectors $\\mathbf{v}_i$, then $A\\mathbf{v}_i = \\lambda_i\\mathbf{v}_i$. Applying $A$ again gives $A^2\\mathbf{v}_i = A(\\lambda_i\\mathbf{v}_i) = \\lambda_i(A\\mathbf{v}_i) = \\lambda_i(\\lambda_i\\mathbf{v}_i) = \\lambda_i^2\\mathbf{v}_i$. Thus, the eigenvalues of $A^2$ are the squares of the eigenvalues of $A$. The largest eigenvalue of $A^2$ is the maximum of these squared values:\n$$\n\\lambda_{\\max}(A^2) = \\max_{i} (\\lambda_i^2) = (\\max_{i} |\\lambda_i|)^2\n$$\nSubstituting this back into the expression for the norm:\n$$\n\\|A\\|_{2}^2 = (\\max_{i} |\\lambda_i|)^2\n$$\nTaking the square root of both sides, we arrive at the desired expression for a symmetric matrix:\n$$\n\\|A\\|_{2} = \\max_{i} |\\lambda_i|\n$$\nThis quantity is also known as the spectral radius of $A$.\n\nNext, we compute the eigenvalues for the given connectivity matrix $A$:\n$$\nA \\;=\\; \\frac{1}{5}\\begin{pmatrix}\n3 & 1 & 1 & 1 & 1 \\\\\n1 & 3 & 1 & 1 & 1 \\\\\n1 & 1 & 3 & 1 & 1 \\\\\n1 & 1 & 1 & 3 & 1 \\\\\n1 & 1 & 1 & 1 & 3 \\\\\n\\end{pmatrix}\n$$\nThis matrix can be conveniently expressed using the $5 \\times 5$ identity matrix $I$ and the $5 \\times 5$ all-ones matrix $J$:\n$$\nA = \\frac{1}{5} (2I + J)\n$$\nThe eigenvalues of $A$ can be determined from the well-known eigenvalues of $J$. The matrix $J$ has rank $1$. The vector of all ones, $\\mathbf{v}_1 = [1, 1, 1, 1, 1]^{\\top}$, is an eigenvector of $J$ with eigenvalue $5$, since $J\\mathbf{v}_1 = 5\\mathbf{v}_1$. The null space of $J$ has dimension $5-1=4$. Any vector orthogonal to $\\mathbf{v}_1$ lies in the null space, so $J$ has an eigenvalue of $0$ with multiplicity $4$.\n\nThe eigenvectors of $J$ are also eigenvectors of $A = c_1 I + c_2 J$ for constants $c_1, c_2$. If $J\\mathbf{v} = \\lambda_J \\mathbf{v}$, then\n$$\nA\\mathbf{v} = \\left(\\frac{2}{5}I + \\frac{1}{5}J\\right)\\mathbf{v} = \\frac{2}{5}I\\mathbf{v} + \\frac{1}{5}J\\mathbf{v} = \\frac{2}{5}\\mathbf{v} + \\frac{1}{5}\\lambda_J\\mathbf{v} = \\left(\\frac{2}{5} + \\frac{1}{5}\\lambda_J\\right)\\mathbf{v}\n$$\nSo the eigenvalues of $A$, denoted $\\lambda_A$, are given by $\\lambda_A = \\frac{2 + \\lambda_J}{5}$.\n\nFor the eigenvalue $\\lambda_J = 5$ (multiplicity $1$), the corresponding eigenvalue of $A$ is:\n$$\n\\lambda_1 = \\frac{2 + 5}{5} = \\frac{7}{5}\n$$\nFor the eigenvalue $\\lambda_J = 0$ (multiplicity $4$), the corresponding eigenvalue of $A$ is:\n$$\n\\lambda_{2,3,4,5} = \\frac{2 + 0}{5} = \\frac{2}{5}\n$$\nThe set of eigenvalues of $A$ is $\\{\\frac{7}{5}, \\frac{2}{5}, \\frac{2}{5}, \\frac{2}{5}, \\frac{2}{5}\\}$.\nSince $A$ is symmetric, its $2$-norm is the maximum of the absolute values of its eigenvalues:\n$$\n\\|A\\|_{2} = \\max\\left(\\left|\\frac{7}{5}\\right|, \\left|\\frac{2}{5}\\right|\\right) = \\frac{7}{5}\n$$\nThe value $\\|A\\|_{2} = \\frac{7}{5} = 1.4$ represents the maximum amplification factor for the magnitude of any state vector $\\mathbf{x}$ under the instantaneous linear map $\\mathbf{x} \\mapsto A\\mathbf{x}$. By definition, $\\|A\\|_{2} = \\sup_{\\mathbf{x} \\neq \\mathbf{0}} \\frac{\\|A\\mathbf{x}\\|_{2}}{\\|\\mathbf{x}\\|_{2}}$. In the context of small perturbations $\\delta\\mathbf{x}$ to the system's state, the norm $\\|A\\|_{2}$ gives the worst-case scenario for the instantaneous amplification of the perturbation's magnitude. A value of $\\|A\\|_{2} > 1$ indicates that there are directions in the state space along which perturbations are amplified. In this model, a perturbation can be amplified by a factor of at most $1.4$. This worst-case amplification occurs for perturbations aligned with the eigenvector corresponding to the largest eigenvalue $\\lambda_1 = 7/5$, which is the all-ones vector $[1, 1, 1, 1, 1]^{\\top}$.",
            "answer": "$$\\boxed{\\frac{7}{5}}$$"
        },
        {
            "introduction": "In many biomedical applications, from imaging to genomics, we work with sets of measurement vectors that are not orthogonal. The Gram-Schmidt process provides a classic method for constructing an orthonormal basis from such a set, which is critical for the stability of subsequent analyses . This practice moves beyond the standard algorithm, challenging you to analyze a scenario with nearly-collinear vectors, a common issue in real-world data, and understand from first principles why this situation poses a numerical challenge and requires more robust computational methods.",
            "id": "3899444",
            "problem": "In diffusion Magnetic Resonance Imaging (MRI), a set of diffusion-encoding gradient directions can be represented as vectors in $\\mathbb{R}^{3}$. When these directions are nearly colinear, the corresponding design matrix becomes ill-conditioned, and orthonormalization is required to stabilize estimation in linear models of tissue microstructure. Consider the following three diffusion direction vectors, parameterized by a small positive parameter $\\epsilon$:\n$$\n\\mathbf{a}_{1} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad\n\\mathbf{a}_{2} = \\begin{pmatrix} 1 \\\\ \\epsilon \\\\ 0 \\end{pmatrix}, \\quad\n\\mathbf{a}_{3} = \\begin{pmatrix} 1 \\\\ \\epsilon \\\\ \\epsilon^{2} \\end{pmatrix},\n$$\nwith $\\epsilon > 0$ and $\\epsilon \\ll 1$. Let $A = \\begin{pmatrix} \\mathbf{a}_{1} & \\mathbf{a}_{2} & \\mathbf{a}_{3} \\end{pmatrix} \\in \\mathbb{R}^{3 \\times 3}$. Your tasks are:\n\n- Using only the definitions of the Euclidean inner product and orthogonal projection as your starting point, apply the Gram–Schmidt process to obtain an orthonormal basis $\\{\\mathbf{q}_{1}, \\mathbf{q}_{2}, \\mathbf{q}_{3}\\}$ for the column space of $A$, and thereby form the matrices $Q = \\begin{pmatrix} \\mathbf{q}_{1} & \\mathbf{q}_{2} & \\mathbf{q}_{3} \\end{pmatrix}$ and the upper triangular $R$ such that $A = QR$.\n- Identify the diagonal entries $r_{11}$, $r_{22}$, and $r_{33}$ of $R$ explicitly as functions of $\\epsilon$.\n- Explain, from first principles of projection and finite-precision computation, why near linear dependence (small $\\epsilon$) is numerically problematic for classical Gram–Schmidt, and discuss a principled remedy appropriate for biomedical systems modeling workflows.\n- For the final reported result, provide the analytic expression of the third diagonal entry $r_{33}$ of $R$ as a function of $\\epsilon$.\n\nExpress the final answer as a closed-form analytic expression in $\\epsilon$. No rounding is required. The final answer is unitless and should be written as a single mathematical expression.",
            "solution": "We begin from the definitions in a real inner-product space $\\mathbb{R}^{3}$ with the Euclidean inner product. For vectors $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^{3}$, the inner product is $\\langle \\mathbf{x}, \\mathbf{y} \\rangle = \\mathbf{x}^{\\top}\\mathbf{y}$, and the orthogonal projection of $\\mathbf{v}$ onto a nonzero vector $\\mathbf{u}$ is $\\mathrm{proj}_{\\mathbf{u}}(\\mathbf{v}) = \\frac{\\langle \\mathbf{u}, \\mathbf{v} \\rangle}{\\langle \\mathbf{u}, \\mathbf{u} \\rangle} \\mathbf{u}$. The Gram–Schmidt process constructs orthonormal vectors $\\mathbf{q}_{1}, \\mathbf{q}_{2}, \\mathbf{q}_{3}$ from the input $\\mathbf{a}_{1}, \\mathbf{a}_{2}, \\mathbf{a}_{3}$ by iteratively subtracting projections and normalizing.\n\nStep $1$: Initialize with $\\mathbf{a}_{1} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$. Set\n$$\n\\mathbf{u}_{1} = \\mathbf{a}_{1}, \\quad r_{11} = \\|\\mathbf{u}_{1}\\|_{2} = \\sqrt{\\langle \\mathbf{u}_{1}, \\mathbf{u}_{1} \\rangle} = \\sqrt{1^{2} + 0^{2} + 0^{2}} = 1, \\quad \\mathbf{q}_{1} = \\frac{\\mathbf{u}_{1}}{r_{11}} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\n\nStep $2$: Orthogonalize $\\mathbf{a}_{2} = \\begin{pmatrix} 1 \\\\ \\epsilon \\\\ 0 \\end{pmatrix}$ against $\\mathbf{q}_{1}$ and normalize. Compute\n$$\nr_{12} = \\langle \\mathbf{q}_{1}, \\mathbf{a}_{2} \\rangle = \\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ \\epsilon \\\\ 0 \\end{pmatrix} = 1,\n$$\n$$\n\\mathbf{u}_{2} = \\mathbf{a}_{2} - r_{12} \\mathbf{q}_{1} = \\begin{pmatrix} 1 \\\\ \\epsilon \\\\ 0 \\end{pmatrix} - 1 \\cdot \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ \\epsilon \\\\ 0 \\end{pmatrix}.\n$$\nThen\n$$\nr_{22} = \\|\\mathbf{u}_{2}\\|_{2} = \\sqrt{0^{2} + \\epsilon^{2} + 0^{2}} = \\epsilon \\quad (\\text{given } \\epsilon > 0),\n$$\nand\n$$\n\\mathbf{q}_{2} = \\frac{\\mathbf{u}_{2}}{r_{22}} = \\frac{1}{\\epsilon} \\begin{pmatrix} 0 \\\\ \\epsilon \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}.\n$$\n\nStep $3$: Orthogonalize $\\mathbf{a}_{3} = \\begin{pmatrix} 1 \\\\ \\epsilon \\\\ \\epsilon^{2} \\end{pmatrix}$ against $\\mathbf{q}_{1}$ and $\\mathbf{q}_{2}$ and normalize. First,\n$$\nr_{13} = \\langle \\mathbf{q}_{1}, \\mathbf{a}_{3} \\rangle = \\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ \\epsilon \\\\ \\epsilon^{2} \\end{pmatrix} = 1,\n$$\n$$\n\\mathbf{w}_{3} = \\mathbf{a}_{3} - r_{13} \\mathbf{q}_{1} = \\begin{pmatrix} 1 \\\\ \\epsilon \\\\ \\epsilon^{2} \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ \\epsilon \\\\ \\epsilon^{2} \\end{pmatrix}.\n$$\nNext,\n$$\nr_{23} = \\langle \\mathbf{q}_{2}, \\mathbf{w}_{3} \\rangle = \\begin{pmatrix} 0 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ \\epsilon \\\\ \\epsilon^{2} \\end{pmatrix} = \\epsilon,\n$$\n$$\n\\mathbf{u}_{3} = \\mathbf{w}_{3} - r_{23} \\mathbf{q}_{2} = \\begin{pmatrix} 0 \\\\ \\epsilon \\\\ \\epsilon^{2} \\end{pmatrix} - \\epsilon \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ \\epsilon^{2} \\end{pmatrix}.\n$$\nThen\n$$\nr_{33} = \\|\\mathbf{u}_{3}\\|_{2} = \\sqrt{0^{2} + 0^{2} + \\epsilon^{4}} = \\epsilon^{2},\n$$\nand\n$$\n\\mathbf{q}_{3} = \\frac{\\mathbf{u}_{3}}{r_{33}} = \\frac{1}{\\epsilon^{2}} \\begin{pmatrix} 0 \\\\ 0 \\\\ \\epsilon^{2} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}.\n$$\n\nCollecting, the orthonormal basis is\n$$\nQ = \\begin{pmatrix} \\mathbf{q}_{1} & \\mathbf{q}_{2} & \\mathbf{q}_{3} \\end{pmatrix} = \n\\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix},\n$$\nand the upper triangular factor has entries\n$$\nR = \\begin{pmatrix}\nr_{11} & r_{12} & r_{13} \\\\\n0 & r_{22} & r_{23} \\\\\n0 & 0 & r_{33}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1 & 1 & 1 \\\\\n0 & \\epsilon & \\epsilon \\\\\n0 & 0 & \\epsilon^{2}\n\\end{pmatrix}.\n$$\nThus, explicitly, $r_{11} = 1$, $r_{22} = \\epsilon$, and $r_{33} = \\epsilon^{2}$.\n\nNumerical issues for small $\\epsilon$: When $\\epsilon \\ll 1$, the columns of $A$ are nearly colinear, so the problem is ill-conditioned. In classical Gram–Schmidt, the orthogonalization step for $\\mathbf{a}_{2}$ performs a subtraction of nearly equal vectors,\n$$\n\\mathbf{u}_{2} = \\mathbf{a}_{2} - \\langle \\mathbf{q}_{1}, \\mathbf{a}_{2} \\rangle \\mathbf{q}_{1} = \\begin{pmatrix} 1 \\\\ \\epsilon \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ \\epsilon \\\\ 0 \\end{pmatrix},\n$$\nwhich in finite precision arithmetic suffers from catastrophic cancellation in the first component: rounding errors at the scale of machine precision $u$ relative to the magnitude $1$ can leave a spurious residual of order $u$ in the first component of $\\mathbf{u}_{2}$. Subsequent normalization amplifies the relative effect of this roundoff by a factor of approximately $1/\\epsilon$, since $\\|\\mathbf{u}_{2}\\|_{2} = \\epsilon$. Similarly, the computation of $\\mathbf{u}_{3}$ involves subtracting a projection of size $\\epsilon$ from $\\mathbf{w}_{3}$, leaving a vector of norm $\\epsilon^{2}$. Any accumulated roundoff of order $u$ at the scale of the larger components can dominate the true signal when $\\epsilon^{2} \\lesssim u$, leading to loss of orthogonality among the computed $\\mathbf{q}_{j}$ and unreliable diagonals in $R$. In particular, when $\\epsilon \\lesssim \\sqrt{u}$ in double precision (where $u \\approx 2^{-53}$), the magnitude of $r_{33} = \\epsilon^{2}$ approaches machine precision and becomes difficult to resolve accurately.\n\nA principled remedy is to use a numerically stable orthogonalization, such as modified Gram–Schmidt (which reorthogonalizes sequentially against the current orthonormal vectors and reduces cancellation) or Householder reflections (which construct $Q$ via orthogonal transformations with superior stability). In biomedical systems modeling pipelines, especially for diffusion MRI design matrices, combining Householder-based $\\mathrm{QR}$ factorization with column pivoting can further mitigate the impact of near dependence by revealing numerical rank and controlling conditioning.\n\nFinally, from the derivation above, the third diagonal entry of $R$ as a function of $\\epsilon$ is\n$$\nr_{33}(\\epsilon) = \\epsilon^{2}.\n$$",
            "answer": "$$\\boxed{\\epsilon^{2}}$$"
        },
        {
            "introduction": "The challenge of nearly-collinear data, explored in the previous practice, has profound consequences in statistical modeling. This exercise demonstrates how ill-conditioning in a design matrix can lead to massive variance inflation in least-squares parameter estimates, rendering them unreliable . You will not only diagnose this common problem but also implement a powerful solution, ridge regression, learning how to choose a regularization parameter to restore statistical stability and obtain meaningful results from your model.",
            "id": "3899461",
            "problem": "A biomedical chemometric sensing system uses three spectral channels to estimate two metabolite concentrations collected in the vector $\\mathbf{x} \\in \\mathbb{R}^{2}$ from the measurement model $\\mathbf{y} = A \\mathbf{x} + \\varepsilon$, where $\\mathbf{y} \\in \\mathbb{R}^{3}$, $A \\in \\mathbb{R}^{3 \\times 2}$, and $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{3})$. Due to overlapping spectral signatures, the columns of $A$ are highly collinear. Let the columns be $a_{1}, a_{2} \\in \\mathbb{R}^{3}$ with $\\|a_{1}\\|_{2} = \\|a_{2}\\|_{2} = 1$ and inner product $a_{1}^{\\top} a_{2} = s = 0.999$, so that the Gram matrix is $G = A^{\\top} A = \\begin{pmatrix} 1 & s \\\\ s & 1 \\end{pmatrix}$. The ordinary least squares (OLS) estimator $\\hat{\\mathbf{x}}_{\\text{OLS}}$ has covariance $\\operatorname{Cov}(\\hat{\\mathbf{x}}_{\\text{OLS}}) = \\sigma^{2} (A^{\\top} A)^{-1}$. The ridge (Tikhonov) estimator is $\\hat{\\mathbf{x}}_{\\lambda} = (A^{\\top} A + \\lambda I_{2})^{-1} A^{\\top} \\mathbf{y}$, with covariance $\\operatorname{Cov}(\\hat{\\mathbf{x}}_{\\lambda})$ determined by the noise model and $A$.\n\nStarting from the linear model and these definitions:\n- Derive the closed-form expression for $\\operatorname{Cov}(\\hat{\\mathbf{x}}_{\\text{OLS}})$ in terms of $s$ and $\\sigma^{2}$, and compute the variances of the two components of $\\hat{\\mathbf{x}}_{\\text{OLS}}$ for $\\sigma^{2} = 1$ and $s = 0.999$.\n- Using spectral reasoning on $G$, derive the eigenvalues of $\\operatorname{Cov}(\\hat{\\mathbf{x}}_{\\lambda})$ as functions of the eigenvalues of $G$ and $\\lambda$. Then, determine the smallest $\\lambda > 0$ such that the largest eigenvalue of $\\operatorname{Cov}(\\hat{\\mathbf{x}}_{\\lambda})$ equals the target $v^{\\star} = 0.1$ for $\\sigma^{2} = 1$ and $s = 0.999$.\n\nRound your final numerical value of $\\lambda$ to four significant figures. Express $\\lambda$ as a dimensionless quantity. The final answer must be a single number.",
            "solution": "The linear Gaussian model $\\mathbf{y} = A \\mathbf{x} + \\varepsilon$ with $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{3})$ yields an ordinary least squares (OLS) estimator $\\hat{\\mathbf{x}}_{\\text{OLS}} = (A^{\\top} A)^{-1} A^{\\top} \\mathbf{y}$. The covariance of any linear estimator $L \\mathbf{y}$ under isotropic noise is $\\operatorname{Cov}(L \\mathbf{y}) = L \\operatorname{Cov}(\\mathbf{y}) L^{\\top} = \\sigma^{2} L L^{\\top}$. Therefore, for OLS, $\\operatorname{Cov}(\\hat{\\mathbf{x}}_{\\text{OLS}}) = \\sigma^{2} (A^{\\top} A)^{-1}$.\n\nFirst, we analyze the effect of collinearity in $A$ through the Gram matrix $G = A^{\\top} A$. Given $\\|a_{1}\\|_{2} = \\|a_{2}\\|_{2} = 1$ and $a_{1}^{\\top} a_{2} = s$, the Gram matrix is\n$$\nG = \\begin{pmatrix} 1 & s \\\\ s & 1 \\end{pmatrix}.\n$$\nFor a $2 \\times 2$ symmetric matrix of the form\n$$\n\\begin{pmatrix} \\alpha & \\beta \\\\ \\beta & \\alpha \\end{pmatrix},\n$$\nthe inverse (when $\\alpha^{2} - \\beta^{2} \\neq 0$) is\n$$\n\\frac{1}{\\alpha^{2} - \\beta^{2}} \\begin{pmatrix} \\alpha & -\\beta \\\\ -\\beta & \\alpha \\end{pmatrix}.\n$$\nApplying this with $\\alpha = 1$ and $\\beta = s$, we obtain\n$$\nG^{-1} = \\frac{1}{1 - s^{2}} \\begin{pmatrix} 1 & -s \\\\ -s & 1 \\end{pmatrix}.\n$$\nTherefore,\n$$\n\\operatorname{Cov}(\\hat{\\mathbf{x}}_{\\text{OLS}}) = \\sigma^{2} G^{-1} = \\frac{\\sigma^{2}}{1 - s^{2}} \\begin{pmatrix} 1 & -s \\\\ -s & 1 \\end{pmatrix}.\n$$\nThe component variances are the diagonal entries:\n$$\n\\operatorname{Var}(\\hat{\\mathbf{x}}_{\\text{OLS},1}) = \\operatorname{Var}(\\hat{\\mathbf{x}}_{\\text{OLS},2}) = \\frac{\\sigma^{2}}{1 - s^{2}}.\n$$\nWith $\\sigma^{2} = 1$ and $s = 0.999$, we have $s^{2} = 0.998001$ and $1 - s^{2} = 0.001999$, so\n$$\n\\operatorname{Var}(\\hat{\\mathbf{x}}_{\\text{OLS},1}) = \\operatorname{Var}(\\hat{\\mathbf{x}}_{\\text{OLS},2}) = \\frac{1}{0.001999} \\approx 500.2501250625313.\n$$\nThis demonstrates that near-collinearity ($s \\approx 1$) inflates the variances dramatically because $1 - s^{2}$ becomes small, making $G$ ill-conditioned.\n\nTo propose a remedy via regularization, consider the ridge estimator\n$$\n\\hat{\\mathbf{x}}_{\\lambda} = (A^{\\top} A + \\lambda I_{2})^{-1} A^{\\top} \\mathbf{y},\n$$\nwhich is a linear function of $\\mathbf{y}$ with $L_{\\lambda} = (A^{\\top} A + \\lambda I_{2})^{-1} A^{\\top}$. The covariance is\n$$\n\\operatorname{Cov}(\\hat{\\mathbf{x}}_{\\lambda}) = \\sigma^{2} L_{\\lambda} L_{\\lambda}^{\\top} = \\sigma^{2} (A^{\\top} A + \\lambda I_{2})^{-1} A^{\\top} A (A^{\\top} A + \\lambda I_{2})^{-1}.\n$$\nLet $G = A^{\\top} A$. Since $G$ is real symmetric and positive semidefinite, it admits an eigenvalue decomposition\n$$\nG = U \\begin{pmatrix} \\mu_{1} & 0 \\\\ 0 & \\mu_{2} \\end{pmatrix} U^{\\top},\n$$\nwith orthonormal $U$ and eigenvalues $\\mu_{1}, \\mu_{2} \\geq 0$. Then\n$$\nA^{\\top} A + \\lambda I_{2} = U \\begin{pmatrix} \\mu_{1} + \\lambda & 0 \\\\ 0 & \\mu_{2} + \\lambda \\end{pmatrix} U^{\\top},\n$$\nand\n$$\n(A^{\\top} A + \\lambda I_{2})^{-1} A^{\\top} A (A^{\\top} A + \\lambda I_{2})^{-1} = U \\begin{pmatrix} \\dfrac{\\mu_{1}}{(\\mu_{1} + \\lambda)^{2}} & 0 \\\\ 0 & \\dfrac{\\mu_{2}}{(\\mu_{2} + \\lambda)^{2}} \\end{pmatrix} U^{\\top}.\n$$\nTherefore, the eigenvalues of $\\operatorname{Cov}(\\hat{\\mathbf{x}}_{\\lambda})$ are\n$$\n\\sigma^{2} \\frac{\\mu_{i}}{(\\mu_{i} + \\lambda)^{2}}, \\quad i = 1,2.\n$$\nFor our $G = \\begin{pmatrix} 1 & s \\\\ s & 1 \\end{pmatrix}$, the eigenvalues are well known:\n$$\n\\mu_{\\max} = 1 + s, \\quad \\mu_{\\min} = 1 - s.\n$$\nWith $s = 0.999$, we have\n$$\n\\mu_{\\max} = 1.999, \\quad \\mu_{\\min} = 0.001.\n$$\nWe are asked to choose the smallest $\\lambda > 0$ such that the largest eigenvalue of $\\operatorname{Cov}(\\hat{\\mathbf{x}}_{\\lambda})$ equals the target $v^{\\star} = 0.1$ for $\\sigma^{2} = 1$. The largest eigenvalue of $\\operatorname{Cov}(\\hat{\\mathbf{x}}_{\\lambda})$ is\n$$\n\\max \\left\\{ \\sigma^{2} \\frac{\\mu_{\\min}}{(\\mu_{\\min} + \\lambda)^{2}}, \\ \\sigma^{2} \\frac{\\mu_{\\max}}{(\\mu_{\\max} + \\lambda)^{2}} \\right\\}.\n$$\nThe function $f(\\mu) = \\mu / (\\mu + \\lambda)^{2}$ satisfies $\\frac{\\partial f}{\\partial \\mu} = \\frac{\\lambda - \\mu}{(\\mu + \\lambda)^{3}}$. For very small $\\lambda$, the term with $\\mu_{\\min}$ dominates as $1/\\mu_{\\min}$. As $\\lambda$ increases, both terms decrease, but the term with $\\mu_{\\min}$ decreases much faster. To find the smallest $\\lambda$ that bounds the maximum eigenvalue, we must increase $\\lambda$ until the dominant term comes down to the target value. The dominant term for small to moderate $\\lambda$ will be associated with the smaller eigenvalue $\\mu_{\\min}$ if $1/\\mu_{\\min}$ is the starting point, but will eventually be taken over by the term with $\\mu_{\\max}$. The correct approach is to find the value of $\\lambda$ that satisfies the condition, which will uniquely determine which term is dominant. Solving for both possibilities reveals the correct path.\nIf we set $\\frac{\\sigma^{2} \\mu_{\\min}}{(\\mu_{\\min} + \\lambda)^{2}} = v^\\star$, we find $\\lambda \\approx 0.099$. At this $\\lambda$, the other eigenvalue term is $\\frac{1.999}{(1.999+0.099)^2} \\approx 0.454$, which is greater than $v^\\star$. Therefore, we must increase $\\lambda$ further until this larger term is reduced to $v^\\star$.\nWe must solve\n$$\n\\sigma^{2} \\frac{\\mu_{\\max}}{(\\mu_{\\max} + \\lambda)^{2}} = v^{\\star}.\n$$\nSolving for $\\lambda$ gives\n$$\n\\mu_{\\max} + \\lambda = \\sqrt{\\frac{\\sigma^{2} \\mu_{\\max}}{v^{\\star}}} \\quad \\Longrightarrow \\quad \\lambda = \\sqrt{\\frac{\\sigma^{2} \\mu_{\\max}}{v^{\\star}}} - \\mu_{\\max}.\n$$\nSubstituting $\\sigma^{2} = 1$, $\\mu_{\\max} = 1.999$, and $v^{\\star} = 0.1$ yields\n$$\n\\lambda = \\sqrt{\\frac{1 \\times 1.999}{0.1}} - 1.999 = \\sqrt{19.99} - 1.999.\n$$\nCompute $\\sqrt{19.99} \\approx 4.471017955$, so\n$$\n\\lambda \\approx 4.471017955 - 1.999 = 2.472017955.\n$$\nAt this value of $\\lambda$, the other eigenvalue is $\\frac{0.001}{(0.001+2.472)^2} \\approx 1.63 \\times 10^{-4}$, which is well below $v^\\star=0.1$. Thus, this is the correct solution.\nRounding to four significant figures,\n$$\n\\lambda \\approx 2.472.\n$$\nThis ridge parameter reduces the largest eigenvalue of the covariance of the ridge estimator to the target $v^{\\star} = 0.1$, thereby mitigating the variance inflation caused by collinearity in the columns of $A$.",
            "answer": "$$\\boxed{2.472}$$"
        }
    ]
}