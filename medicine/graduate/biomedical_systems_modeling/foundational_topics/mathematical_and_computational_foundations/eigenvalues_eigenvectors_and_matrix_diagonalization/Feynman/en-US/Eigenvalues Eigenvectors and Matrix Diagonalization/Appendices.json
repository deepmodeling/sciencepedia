{
    "hands_on_practices": [
        {
            "introduction": "This practice forms the bedrock of analyzing linear dynamical systems by computing the matrix exponential, $\\exp(At)$, which maps an initial state $x(0)$ to a future state $x(t)$. By leveraging the eigendecomposition $A = PDP^{-1}$, you will see how a complex, coupled system can be transformed into a set of simple, decoupled scalar equations, making the solution transparent and intuitive . This exercise is fundamental for understanding how a system's response is a weighted sum of its natural modes, each decaying or growing at a rate determined by an eigenvalue.",
            "id": "3883075",
            "problem": "A three-module linear signaling motif in a cellular pathway is linearized about a homeostatic equilibrium so that small deviations in module activities, collected as the state vector $x(t) \\in \\mathbb{R}^{3}$, satisfy the homogeneous linear ordinary differential equation (ODE) $\\dot{x}(t) = A x(t)$ for $t \\ge 0$. The linearization reflects first-order mass-action exchange and degradation kinetics, which are well approximated by constant coefficients in a neighborhood of the equilibrium. The system matrix $A$ is known to be diagonalizable with three distinct, strictly negative eigenvalues (so that the equilibrium is locally asymptotically stable). Specifically, $A$ admits the eigendecomposition $A = P D P^{-1}$ with\n$$\nP \\;=\\; \\begin{pmatrix}\n1  1  1 \\\\\n1  0  -1 \\\\\n0  1  1\n\\end{pmatrix}, \n\\qquad\nD \\;=\\; \\mathrm{diag}\\!\\big(-0.5,\\,-1.2,\\,-2.8\\big)\\ \\text{hours}^{-1},\n$$\nand $P^{-1}$ is the inverse of $P$. The initial perturbation corresponds to a bolus in the first module, $x(0) = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\n\nStarting from the definition of the matrix exponential as the uniformly convergent power series and the concept of diagonalization of a matrix via eigenvalues and eigenvectors, compute the closed-form expression for $\\exp(A t)$ for $t \\ge 0$ (with $t$ measured in hours), and use it to write $x(t)$ explicitly in terms of the eigenvalues and modal projections. Express your final $\\exp(A t)$ exactly using combinations of $\\exp(\\lambda t)$ with the given eigenvalues. Do not approximate any exponentials. No rounding is required. The final answer to report should be the matrix $\\exp(A t)$ as a single exact analytic expression with entries written in terms of $t$.",
            "solution": "The problem is validated as scientifically grounded, well-posed, objective, and complete. It is a standard exercise in solving a system of linear ordinary differential equations using matrix diagonalization, a core technique in systems theory and its applications to biomedical modeling. All necessary data are provided and are consistent. We may proceed with the solution.\n\nThe system of linear ordinary differential equations is given by $\\dot{x}(t) = A x(t)$, with the initial condition $x(0)$. The unique solution to this initial value problem is given by $x(t) = \\exp(A t) x(0)$, where $\\exp(A t)$ is the matrix exponential.\n\nThe problem requires us to first compute the matrix exponential $\\exp(A t)$. By definition, the matrix exponential is given by the power series:\n$$\n\\exp(M) = \\sum_{k=0}^{\\infty} \\frac{1}{k!} M^k\n$$\nFor $M = A t$, where $t$ is a scalar, this becomes:\n$$\n\\exp(A t) = \\sum_{k=0}^{\\infty} \\frac{t^k}{k!} A^k\n$$\nThe system matrix $A$ is given to be diagonalizable, with the eigendecomposition $A = P D P^{-1}$, where $P$ is the matrix of eigenvectors and $D$ is the diagonal matrix of eigenvalues.\nA key property of this decomposition is that powers of $A$ can be computed easily:\n$$\nA^k = (P D P^{-1})^k = (P D P^{-1})(P D P^{-1})\\cdots(P D P^{-1}) = P D^k P^{-1}\n$$\nSubstituting this into the power series for $\\exp(A t)$:\n$$\n\\exp(A t) = \\sum_{k=0}^{\\infty} \\frac{t^k}{k!} (P D^k P^{-1})\n$$\nSince $P$ and $P^{-1}$ are constant matrices, they can be factored out of the summation:\n$$\n\\exp(A t) = P \\left( \\sum_{k=0}^{\\infty} \\frac{t^k}{k!} D^k \\right) P^{-1} = P \\left( \\sum_{k=0}^{\\infty} \\frac{(D t)^k}{k!} \\right) P^{-1}\n$$\nThe summation is the definition of the matrix exponential of $D t$, so we arrive at the fundamental formula:\n$$\n\\exp(A t) = P \\exp(D t) P^{-1}\n$$\nThe given matrices are:\n$$\nP = \\begin{pmatrix} 1  1  1 \\\\ 1  0  -1 \\\\ 0  1  1 \\end{pmatrix}\n\\qquad\nD = \\begin{pmatrix} -0.5  0  0 \\\\ 0  -1.2  0 \\\\ 0  0  -2.8 \\end{pmatrix}\n$$\nLet the eigenvalues be $\\lambda_1 = -0.5$, $\\lambda_2 = -1.2$, and $\\lambda_3 = -2.8$.\nFor a diagonal matrix $D$, the matrix exponential $\\exp(D t)$ is found by taking the exponential of each diagonal element:\n$$\n\\exp(D t) = \\begin{pmatrix} \\exp(\\lambda_1 t)  0  0 \\\\ 0  \\exp(\\lambda_2 t)  0 \\\\ 0  0  \\exp(\\lambda_3 t) \\end{pmatrix} = \\begin{pmatrix} \\exp(-0.5 t)  0  0 \\\\ 0  \\exp(-1.2 t)  0 \\\\ 0  0  \\exp(-2.8 t) \\end{pmatrix}\n$$\nNext, we must compute the inverse of $P$. We first find the determinant of $P$:\n$$\n\\det(P) = 1(0 \\cdot 1 - (-1) \\cdot 1) - 1(1 \\cdot 1 - (-1) \\cdot 0) + 1(1 \\cdot 1 - 0 \\cdot 0) = 1(1) - 1(1) + 1(1) = 1\n$$\nSince $\\det(P) \\neq 0$, the inverse exists. The inverse is given by $P^{-1} = \\frac{1}{\\det(P)} \\mathrm{adj}(P)$, where $\\mathrm{adj}(P)$ is the adjugate matrix of $P$ (the transpose of the cofactor matrix).\nThe cofactor matrix $C$ of $P$ has entries $C_{ij} = (-1)^{i+j} M_{ij}$, where $M_{ij}$ is the minor.\n$$\nC = \\begin{pmatrix}\n(0 - (-1))  -(1 - 0)  (1 - 0) \\\\\n-(1 - 1)  (1 - 0)  -(1 - 0) \\\\\n(-1 - 0)  -(-1 - 1)  (0 - 1)\n\\end{pmatrix}\n= \\begin{pmatrix}\n1  -1  1 \\\\\n0  1  -1 \\\\\n-1  2  -1\n\\end{pmatrix}\n$$\nThe adjugate matrix is the transpose of $C$:\n$$\n\\mathrm{adj}(P) = C^T = \\begin{pmatrix}\n1  0  -1 \\\\\n-1  1  2 \\\\\n1  -1  -1\n\\end{pmatrix}\n$$\nSince $\\det(P) = 1$, the inverse is $P^{-1} = \\mathrm{adj}(P)$:\n$$\nP^{-1} = \\begin{pmatrix}\n1  0  -1 \\\\\n-1  1  2 \\\\\n1  -1  -1\n\\end{pmatrix}\n$$\nNow we can compute $\\exp(A t) = P \\exp(D t) P^{-1}$. Let's denote $e_1(t) = \\exp(-0.5t)$, $e_2(t) = \\exp(-1.2t)$, and $e_3(t) = \\exp(-2.8t)$ for brevity.\nFirst, we compute the product $P \\exp(D t)$:\n$$\nP \\exp(D t) = \\begin{pmatrix} 1  1  1 \\\\ 1  0  -1 \\\\ 0  1  1 \\end{pmatrix}\n\\begin{pmatrix} e_1(t)  0  0 \\\\ 0  e_2(t)  0 \\\\ 0  0  e_3(t) \\end{pmatrix}\n= \\begin{pmatrix} e_1(t)  e_2(t)  e_3(t) \\\\ e_1(t)  0  -e_3(t) \\\\ 0  e_2(t)  e_3(t) \\end{pmatrix}\n$$\nFinally, we multiply this result by $P^{-1}$:\n$$\n\\exp(A t) = \\begin{pmatrix} e_1(t)  e_2(t)  e_3(t) \\\\ e_1(t)  0  -e_3(t) \\\\ 0  e_2(t)  e_3(t) \\end{pmatrix}\n\\begin{pmatrix} 1  0  -1 \\\\ -1  1  2 \\\\ 1  -1  -1 \\end{pmatrix}\n$$\nPerforming the matrix multiplication gives the entries of $\\exp(A t)$:\nRow 1:\n- $(1,1) = e_1(t) - e_2(t) + e_3(t)$\n- $(1,2) = e_2(t) - e_3(t)$\n- $(1,3) = -e_1(t) + 2e_2(t) - e_3(t)$\nRow 2:\n- $(2,1) = e_1(t) - e_3(t)$\n- $(2,2) = e_3(t)$\n- $(2,3) = -e_1(t) + e_3(t)$\nRow 3:\n- $(3,1) = -e_2(t) + e_3(t)$\n- $(3,2) = e_2(t) - e_3(t)$\n- $(3,3) = 2e_2(t) - e_3(t)$\n\nSubstituting back the full exponential expressions, the matrix $\\exp(A t)$ is:\n$$\n\\exp(A t) = \\begin{pmatrix}\n\\exp(-0.5t) - \\exp(-1.2t) + \\exp(-2.8t)  \\exp(-1.2t) - \\exp(-2.8t)  -\\exp(-0.5t) + 2\\exp(-1.2t) - \\exp(-2.8t) \\\\\n\\exp(-0.5t) - \\exp(-2.8t)  \\exp(-2.8t)  -\\exp(-0.5t) + \\exp(-2.8t) \\\\\n-\\exp(-1.2t) + \\exp(-2.8t)  \\exp(-1.2t) - \\exp(-2.8t)  2\\exp(-1.2t) - \\exp(-2.8t)\n\\end{pmatrix}\n$$\nThe problem also mentions writing $x(t)$ explicitly. With the initial condition $x(0) = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$, the solution is $x(t) = \\exp(A t) x(0)$, which is simply the first column of the $\\exp(A t)$ matrix:\n$$\nx(t) = \\begin{pmatrix}\n\\exp(-0.5t) - \\exp(-1.2t) + \\exp(-2.8t) \\\\\n\\exp(-0.5t) - \\exp(-2.8t) \\\\\n-\\exp(-1.2t) + \\exp(-2.8t)\n\\end{pmatrix}\n$$\nThis demonstrates the use of the matrix exponential. The final required answer is the closed-form expression for $\\exp(A t)$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\exp(-0.5t) - \\exp(-1.2t) + \\exp(-2.8t)  \\exp(-1.2t) - \\exp(-2.8t)  -\\exp(-0.5t) + 2\\exp(-1.2t) - \\exp(-2.8t) \\\\\n\\exp(-0.5t) - \\exp(-2.8t)  \\exp(-2.8t)  -\\exp(-0.5t) + \\exp(-2.8t) \\\\\n-\\exp(-1.2t) + \\exp(-2.8t)  \\exp(-1.2t) - \\exp(-2.8t)  2\\exp(-1.2t) - \\exp(-2.8t)\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Many biomedical systems, from neural circuits to cardiovascular regulation, exhibit oscillatory behavior, which mathematically corresponds to complex-conjugate eigenvalues. This practice explores this connection by first asking you to demonstrate how an eigenpair of the form $\\lambda = \\alpha \\pm i\\omega$ produces sinusoidal dynamics, and then to apply this insight to a model of a hemodynamic oscillator . This exercise is crucial for linking the abstract algebraic properties of a system matrix to tangible behaviors like damped oscillations and for quantifying their key features, such as amplitude and decay envelopes.",
            "id": "3883057",
            "problem": "Consider a linearized, lumped two-state hemodynamic oscillator that models small deviations around a cardiovascular equilibrium, where the normalized state vector $x(t) \\in \\mathbb{R}^{2}$ evolves according to the Ordinary Differential Equation (ODE) $ \\dot{x}(t) = A x(t) $, with a constant real system matrix $A \\in \\mathbb{R}^{2 \\times 2}$. It is known that the linearization of such coupled flowâ€“compliance dynamics yields real matrices $A$ whose eigenvalues may form a complex-conjugate pair near oscillatory equilibria. Starting from the spectral decomposition principle for linear systems $ \\dot{x}(t) = A x(t) $, and the fundamental property that real matrices have complex eigenvalues in conjugate pairs, show that a complex-conjugate eigenpair $ \\lambda = \\alpha \\pm i \\omega $ in a real system gives rise to sinusoidal components in $x(t)$, and derive the amplitude and phase of a component $x_{1}(t)$ from the projections of the initial condition onto the corresponding eigen-directions.\n\nThen, specialize to the physiologically consistent parametric form\n$$\nA = \\begin{pmatrix}\n-\\beta  -\\omega \\\\\n\\omega  -\\beta\n\\end{pmatrix},\n$$\nwith $ \\beta  0 $ and $ \\omega  0 $, and the normalized initial deviation $ x(0) = \\begin{pmatrix} x_{1}(0) \\\\ x_{2}(0) \\end{pmatrix} $. For the specific case $ \\beta = 0.15 \\text{ s}^{-1} $, $ \\omega = 7.5 \\text{ rad/s} $, and $ x(0) = \\begin{pmatrix} 0.8 \\\\ -0.6 \\end{pmatrix} $, determine the amplitude envelope of the sinusoidal component of $ x_{1}(t) $ at time $ T = 0.4 \\text{ s} $. Express your final numerical answer as a dimensionless quantity and round your answer to five significant figures. All angles, if introduced, must be expressed in radians.",
            "solution": "The user-provided problem is assessed for validity.\n\n### Step 1: Extract Givens\n-   The system is governed by the linear Ordinary Differential Equation (ODE) $\\dot{x}(t) = A x(t)$, where $x(t) \\in \\mathbb{R}^{2}$ is the state vector and $A \\in \\mathbb{R}^{2 \\times 2}$ is a constant real system matrix.\n-   The context is a linearized, lumped two-state hemodynamic oscillator model.\n-   The eigenvalues of $A$ may form a complex-conjugate pair, denoted $\\lambda = \\alpha \\pm i \\omega$.\n-   The first part of the task is to show that such an eigenpair leads to sinusoidal components in $x(t)$ and to derive the amplitude and phase of a component $x_{1}(t)$ based on the initial condition's projection.\n-   The second part specializes to the matrix form $A = \\begin{pmatrix} -\\beta  -\\omega \\\\ \\omega  -\\beta \\end{pmatrix}$, with $\\beta  0$ and $\\omega  0$.\n-   The initial condition is $x(0) = \\begin{pmatrix} x_{1}(0) \\\\ x_{2}(0) \\end{pmatrix}$.\n-   Specific parameters for the numerical calculation are: $\\beta = 0.15 \\text{ s}^{-1}$, $\\omega = 7.5 \\text{ rad/s}$, and the initial condition $x(0) = \\begin{pmatrix} 0.8 \\\\ -0.6 \\end{pmatrix}$.\n-   The final objective is to compute the value of the amplitude envelope of $x_{1}(t)$ at time $T = 0.4 \\text{ s}$.\n-   The final numerical answer should be dimensionless and rounded to five significant figures.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded**: The problem posits a linear time-invariant (LTI) system $\\dot{x} = Ax$, which is a fundamental and rigorously established framework for modeling the dynamics of systems near an equilibrium point. The specific matrix structure corresponds to a damped harmonic oscillator, a classic and scientifically sound model for oscillatory phenomena in physics and biology, including cardiovascular dynamics. The problem is based on standard principles of linear algebra and differential equations.\n-   **Well-Posed**: The problem is clearly defined with all necessary information. It provides the governing equation, the form of the system matrix, all required parameters, and a specific initial condition. The tasks are unambiguous, leading to a unique mathematical solution.\n-   **Objective**: The problem is stated in precise, formal mathematical and technical language, free from subjective or non-scientific claims.\n\n### Step 3: Verdict and Action\nThe problem is scientifically grounded, well-posed, and objective. It contains no discernible flaws. Therefore, the problem is deemed **valid**, and a full solution will be provided.\n\n### Solution Derivation\n\nThe solution to the linear system $\\dot{x}(t) = A x(t)$ can be expressed using the spectral decomposition of the matrix $A$. If $A$ is diagonalizable, it has a set of eigenvalues $\\lambda_k$ and corresponding eigenvectors $v_k$ that satisfy $A v_k = \\lambda_k v_k$. The general solution is a linear combination of the eigenmodes:\n$$\nx(t) = \\sum_{k=1}^{n} c_k \\exp(\\lambda_k t) v_k\n$$\nwhere the coefficients $c_k$ are determined by the initial condition $x(0) = \\sum_{k=1}^{n} c_k v_k$.\n\nFor a real $2 \\times 2$ matrix $A$ with a complex-conjugate pair of eigenvalues $\\lambda_1 = \\alpha + i\\omega$ and $\\lambda_2 = \\alpha - i\\omega$ (with $\\omega \\neq 0$), the corresponding eigenvectors must also be a complex-conjugate pair. Let them be $v_1$ and $v_2 = \\bar{v}_1$. The solution is then:\n$$\nx(t) = c_1 \\exp(\\lambda_1 t) v_1 + c_2 \\exp(\\lambda_2 t) v_2\n$$\nSince the state vector $x(t)$ must be real for all time $t$, we must have $c_2 = \\bar{c}_1$. Thus, the solution can be written as:\n$$\nx(t) = c_1 \\exp(\\lambda_1 t) v_1 + \\bar{c}_1 \\exp(\\bar{\\lambda}_1 t) \\bar{v}_1 = 2 \\text{Re} \\{ c_1 \\exp(\\lambda_1 t) v_1 \\}\n$$\nSubstituting $\\lambda_1 = \\alpha + i\\omega$:\n$$\nx(t) = 2 \\text{Re} \\{ c_1 v_1 \\exp((\\alpha+i\\omega)t) \\} = 2 \\exp(\\alpha t) \\text{Re} \\{ c_1 v_1 \\exp(i\\omega t) \\}\n$$\nLet the complex vector $c_1 v_1$ be denoted as $Z = R + iI$, where $R$ and $I$ are real vectors. The solution becomes:\n$$\nx(t) = 2 \\exp(\\alpha t) \\text{Re} \\{ (R+iI)(\\cos(\\omega t) + i\\sin(\\omega t)) \\} = 2 \\exp(\\alpha t) [R \\cos(\\omega t) - I \\sin(\\omega t)]\n$$\nThis demonstrates that the solution's components are sinusoids with frequency $\\omega$, modulated by an exponential envelope $\\exp(\\alpha t)$.\n\nNow, we specialize to the given system:\n$$\nA = \\begin{pmatrix} -\\beta  -\\omega \\\\ \\omega  -\\beta \\end{pmatrix}\n$$\nFirst, we find the eigenvalues of $A$ by solving $\\det(A - \\lambda I) = 0$:\n$$\n\\det\\begin{pmatrix} -\\beta - \\lambda  -\\omega \\\\ \\omega  -\\beta - \\lambda \\end{pmatrix} = (-\\beta - \\lambda)^2 + \\omega^2 = 0\n$$\n$$\n(\\lambda + \\beta)^2 = - \\omega^2 \\implies \\lambda + \\beta = \\pm i\\omega \\implies \\lambda = -\\beta \\pm i\\omega\n$$\nThis confirms the general form with $\\alpha = -\\beta$. We choose $\\lambda_1 = -\\beta + i\\omega$ and $\\lambda_2 = -\\beta - i\\omega$.\n\nNext, we find the corresponding eigenvectors. For $\\lambda_1 = -\\beta + i\\omega$:\n$$\n(A-\\lambda_1 I) v_1 = \\begin{pmatrix} -i\\omega  -\\omega \\\\ \\omega  -i\\omega \\end{pmatrix} \\begin{pmatrix} (v_1)_1 \\\\ (v_1)_2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nThe first row gives $-i\\omega (v_1)_1 - \\omega (v_1)_2 = 0$, which simplifies to $(v_1)_2 = -i (v_1)_1$. We can choose $(v_1)_1 = 1$, which gives an eigenvector $v_1 = \\begin{pmatrix} 1 \\\\ -i \\end{pmatrix}$. The other eigenvector is $v_2 = \\bar{v}_1 = \\begin{pmatrix} 1 \\\\ i \\end{pmatrix}$.\n\nThe initial condition $x(0)$ is decomposed in this eigenbasis:\n$$\nx(0) = \\begin{pmatrix} 0.8 \\\\ -0.6 \\end{pmatrix} = c_1 v_1 + c_2 v_2 = c_1 \\begin{pmatrix} 1 \\\\ -i \\end{pmatrix} + c_2 \\begin{pmatrix} 1 \\\\ i \\end{pmatrix} = \\begin{pmatrix} c_1 + c_2 \\\\ -ic_1 + ic_2 \\end{pmatrix}\n$$\nThis yields the system of equations:\n$1$. $c_1 + c_2 = 0.8$\n$2$. $-ic_1 + ic_2 = -0.6 \\implies i(c_2 - c_1) = -0.6 \\implies c_2 - c_1 = -0.6/i = 0.6i$\n\nAdding the two equations gives: $2c_2 = 0.8 + 0.6i \\implies c_2 = 0.4 + 0.3i$.\nSubtracting the second from the first gives: $2c_1 = 0.8 - 0.6i \\implies c_1 = 0.4 - 0.3i$.\nAs required for a real solution, $c_1$ and $c_2$ are complex conjugates ($c_1 = \\bar{c_2}$).\n\nThe solution for the first state, $x_1(t)$, is given by:\n$x_1(t) = c_1 (v_1)_1 \\exp(\\lambda_1 t) + c_2 (v_2)_1 \\exp(\\lambda_2 t)$\nSince $(v_1)_1 = 1$ and $(v_2)_1 = 1$:\n$x_1(t) = (0.4-0.3i)\\exp((-\\beta+i\\omega)t) + (0.4+0.3i)\\exp((-\\beta-i\\omega)t)$\n$x_1(t) = \\exp(-\\beta t) \\left[ (0.4-0.3i)\\exp(i\\omega t) + (0.4+0.3i)\\exp(-i\\omega t) \\right]$\nThis simplifies to $2 \\text{Re}\\{ (0.4-0.3i)\\exp(i\\omega t) \\}$:\n$x_1(t) = \\exp(-\\beta t) \\left[ 2 \\text{Re}\\{ (0.4-0.3i)(\\cos(\\omega t)+i\\sin(\\omega t)) \\} \\right]$\n$x_1(t) = \\exp(-\\beta t) \\left[ 2(0.4\\cos(\\omega t) + 0.3\\sin(\\omega t)) \\right]$\n$x_1(t) = \\exp(-\\beta t)(0.8\\cos(\\omega t) + 0.6\\sin(\\omega t))$\n\nTo find the amplitude, we combine the sinusoidal terms into the form $R\\cos(\\omega t - \\phi)$. The amplitude $R$ is given by:\n$R = \\sqrt{(0.8)^2 + (0.6)^2} = \\sqrt{0.64 + 0.36} = \\sqrt{1} = 1$.\nThus, the full expression for $x_1(t)$ is $x_1(t) = (1) \\cdot \\exp(-\\beta t) \\cos(\\omega t - \\phi)$.\nThe amplitude envelope of $x_1(t)$ is the function $E(t) = (1) \\cdot \\exp(-\\beta t) = \\exp(-\\beta t)$.\n\nThe problem asks for the value of this envelope at time $T = 0.4 \\text{ s}$.\nThe parameters are $\\beta = 0.15 \\text{ s}^{-1}$ and $T = 0.4 \\text{ s}$.\n$$\nE(T) = \\exp(-\\beta T) = \\exp(-0.15 \\times 0.4) = \\exp(-0.06)\n$$\nNow, we calculate the numerical value and round to five significant figures.\n$$\n\\exp(-0.06) \\approx 0.94176453...\n$$\nRounding to five significant figures gives $0.94176$.\nThe quantity is dimensionless as requested, since the product $\\beta T$ has units of $(\\text{s}^{-1})(\\text{s})$, which is dimensionless.",
            "answer": "$$\\boxed{0.94176}$$"
        },
        {
            "introduction": "While most systems can be analyzed through diagonalization, some exhibit structural degeneracies resulting in \"defective\" matrices that require the Jordan canonical form. This practice delves into this advanced scenario, guiding you to derive the matrix exponential for a Jordan block and discover the emergence of polynomial-in-time factors, such as $t \\exp(\\lambda t)$ . Understanding this is critical, as it reveals how an otherwise stable system (where $\\lambda  0$) can exhibit significant transient growth before decaying, a key insight for predicting the peak response of biological signaling cascades.",
            "id": "3883076",
            "problem": "Consider a three-compartment linearized biomolecular signaling cascade near a homeostatic equilibrium, where the deviations of concentrations from equilibrium are represented by the state vector $x(t) \\in \\mathbb{R}^{3}$. Linearization of the nonlinear ordinary differential equation (ODE) model yields a system of the form $\\dot{x}(t) = A x(t)$, where $A$ is the Jacobian matrix evaluated at the equilibrium. Suppose the Jacobian $A$ is defective due to a structural degeneracy in the reaction network (for example, an exact parameter symmetry or a conserved moiety), and in its Jordan canonical form it contains a single $3 \\times 3$ Jordan block associated with a real eigenvalue $\\lambda \\in \\mathbb{R}$. In Jordan coordinates $z(t)$ satisfying $\\dot{z}(t) = J z(t)$, where $J$ is the Jordan block, the dynamics are governed by\n$$\nJ \\;=\\; \\begin{pmatrix}\n\\lambda  1  0 \\\\\n0  \\lambda  1 \\\\\n0  0  \\lambda\n\\end{pmatrix}.\n$$\nStarting from the series definition of the matrix exponential and the core properties of nilpotent matrices, derive the closed-form expression for $\\exp(J t)$ and identify the polynomial-in-time factors that modulate the transient dynamics in each state component. Then, interpret how these polynomial factors alter the transient magnitude when $\\lambda  0$ (for example, in a dissipative biochemical network with net decay rates), distinguishing between exponential decay and algebraic amplification.\n\nFinally, in the Jordan coordinates above, consider the initial condition $z(0) = \\begin{pmatrix}0 \\\\ 0 \\\\ 1\\end{pmatrix}$. Compute the closed-form expression for the first component $z_{1}(t)$ for all $t \\ge 0$. Your final answer must be a single analytic expression with no units. Do not round.",
            "solution": "The problem is assessed as valid. It is scientifically grounded in the theory of linear ordinary differential equations, which is a standard method for analyzing the local stability of nonlinear dynamical systems in fields like biomedical engineering. The problem is well-posed, providing a specific Jordan block matrix $J$ and a clear objective to compute its matrix exponential and analyze the resulting dynamics. All terms are mathematically precise, and the setup is self-contained and consistent.\n\nThe objective is to analyze the dynamics of the system $\\dot{z}(t) = J z(t)$, where the matrix $J$ is a $3 \\times 3$ Jordan block associated with a real eigenvalue $\\lambda$. The solution to this linear system is given by $z(t) = \\exp(J t) z(0)$. The core of the problem is to derive the matrix exponential $\\exp(J t)$.\n\nThe matrix $J$ can be decomposed into the sum of a diagonal matrix and a nilpotent matrix. Let $I$ be the $3 \\times 3$ identity matrix. We can write $J$ as:\n$$ J \\;=\\; \\lambda I + N $$\nwhere\n$$\nN \\;=\\; \\begin{pmatrix}\n0  1  0 \\\\\n0  0  1 \\\\\n0  0  0\n\\end{pmatrix}\n$$\nThe matrix to be exponentiated is $J t$:\n$$ J t \\;=\\; (\\lambda I + N)t \\;=\\; \\lambda I t + N t $$\nThe matrix exponential of a sum of two matrices, $A$ and $B$, is equal to the product of their individual exponentials, $\\exp(A+B) = \\exp(A)\\exp(B)$, if and only if the matrices commute ($AB=BA$). In our case, the matrices are $A = \\lambda I t$ and $B = N t$. Let us verify commutativity:\n$$ (\\lambda I t)(N t) = \\lambda t^2 (IN) = \\lambda t^2 N $$\n$$ (N t)(\\lambda I t) = \\lambda t^2 (NI) = \\lambda t^2 N $$\nSince $(\\lambda I t)(N t) = (N t)(\\lambda I t)$, the matrices commute. Therefore, we can write:\n$$ \\exp(J t) \\;=\\; \\exp(\\lambda I t + N t) \\;=\\; \\exp(\\lambda I t) \\exp(N t) $$\nWe now compute each exponential term separately.\n\nFirst, the exponential of the diagonal part $\\lambda I t$ is:\n$$ \\exp(\\lambda I t) \\;=\\; \\sum_{k=0}^{\\infty} \\frac{(\\lambda I t)^k}{k!} \\;=\\; \\sum_{k=0}^{\\infty} \\frac{\\lambda^k t^k I^k}{k!} \\;=\\; I \\sum_{k=0}^{\\infty} \\frac{(\\lambda t)^k}{k!} \\;=\\; I \\exp(\\lambda t) $$\n$$ \\exp(\\lambda I t) \\;=\\; \\begin{pmatrix}\n\\exp(\\lambda t)  0  0 \\\\\n0  \\exp(\\lambda t)  0 \\\\\n0  0  \\exp(\\lambda t)\n\\end{pmatrix} $$\n\nSecond, we compute the exponential of the nilpotent part $N t$. The matrix $N$ is nilpotent because some power of it is the zero matrix. We compute its powers:\n$$ N^2 \\;=\\; N \\cdot N \\;=\\; \\begin{pmatrix}\n0  1  0 \\\\\n0  0  1 \\\\\n0  0  0\n\\end{pmatrix} \\begin{pmatrix}\n0  1  0 \\\\\n0  0  1 \\\\\n0  0  0\n\\end{pmatrix} \\;=\\; \\begin{pmatrix}\n0  0  1 \\\\\n0  0  0 \\\\\n0  0  0\n\\end{pmatrix} $$\n$$ N^3 \\;=\\; N^2 \\cdot N \\;=\\; \\begin{pmatrix}\n0  0  1 \\\\\n0  0  0 \\\\\n0  0  0\n\\end{pmatrix} \\begin{pmatrix}\n0  1  0 \\\\\n0  0  1 \\\\\n0  0  0\n\\end{pmatrix} \\;=\\; \\begin{pmatrix}\n0  0  0 \\\\\n0  0  0 \\\\\n0  0  0\n\\end{pmatrix} \\;=\\; \\mathbf{0} $$\nSince $N^3 = \\mathbf{0}$, all higher powers $N^k$ for $k \\ge 3$ are also the zero matrix. This means the infinite series for $\\exp(N t)$ truncates after the $k=2$ term:\n$$ \\exp(N t) \\;=\\; \\sum_{k=0}^{\\infty} \\frac{(N t)^k}{k!} \\;=\\; \\frac{(N t)^0}{0!} + \\frac{(N t)^1}{1!} + \\frac{(N t)^2}{2!} + \\mathbf{0} + \\dots $$\n$$ \\exp(N t) \\;=\\; I + N t + \\frac{1}{2} N^2 t^2 $$\n$$ \\exp(N t) \\;=\\; \\begin{pmatrix}\n1  0  0 \\\\\n0  1  0 \\\\\n0  0  1\n\\end{pmatrix} + t \\begin{pmatrix}\n0  1  0 \\\\\n0  0  1 \\\\\n0  0  0\n\\end{pmatrix} + \\frac{t^2}{2} \\begin{pmatrix}\n0  0  1 \\\\\n0  0  0 \\\\\n0  0  0\n\\end{pmatrix} \\;=\\; \\begin{pmatrix}\n1  t  \\frac{t^2}{2} \\\\\n0  1  t \\\\\n0  0  1\n\\end{pmatrix} $$\n\nCombining these results, we find the closed-form expression for $\\exp(J t)$:\n$$ \\exp(J t) \\;=\\; \\exp(\\lambda I t) \\exp(N t) \\;=\\; \\exp(\\lambda t) I \\begin{pmatrix}\n1  t  \\frac{t^2}{2} \\\\\n0  1  t \\\\\n0  0  1\n\\end{pmatrix} $$\n$$ \\exp(J t) \\;=\\; \\begin{pmatrix}\n\\exp(\\lambda t)  t \\exp(\\lambda t)  \\frac{t^2}{2} \\exp(\\lambda t) \\\\\n0  \\exp(\\lambda t)  t \\exp(\\lambda t) \\\\\n0  0  \\exp(\\lambda t)\n\\end{pmatrix} $$\n\nThe components of the solution $z(t) = \\exp(Jt)z(0)$ are linear combinations of the terms in the matrix $\\exp(Jt)$. These terms are of the form $p_k(t) \\exp(\\lambda t)$, where $p_k(t)$ are polynomials in time $t$. The polynomials arising from the $3 \\times 3$ Jordan block are $1$, $t$, and $\\frac{t^2}{2}$. These are the polynomial-in-time factors that modulate the transient dynamics.\n\nFor a stable system, such as a dissipative biochemical network, the real part of the eigenvalues must be negative, so $\\lambda  0$. The dynamics are governed by terms of the form $t^n \\exp(\\lambda t)$ where $n \\in \\{0, 1, 2\\}$. The term $\\exp(\\lambda t)$ ensures that the state deviation $z(t)$ ultimately decays to zero as $t \\to \\infty$, since exponential decay always dominates polynomial growth. However, the polynomial factors $t^n$ cause a temporary, or transient, growth in the magnitude of the state variables. For example, the function $f(t) = t^n \\exp(\\lambda t)$ for $n>0$ and $\\lambda  0$ starts at $f(0)=0$, increases to a maximum at $t = -n/\\lambda$, and then decays to zero. This phenomenon is an algebraic amplification of the transient response. It means that even for a stable equilibrium, a perturbation can initially grow, moving the system further from equilibrium before it eventually returns. This is a hallmark of defective dynamics.\n\nFinally, we compute the first component of the state vector, $z_1(t)$, given the initial condition $z(0) = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}$.\nThe state vector at time $t$ is given by $z(t) = \\exp(J t) z(0)$:\n$$ \\begin{pmatrix} z_1(t) \\\\ z_2(t) \\\\ z_3(t) \\end{pmatrix} \\;=\\; \\begin{pmatrix}\n\\exp(\\lambda t)  t \\exp(\\lambda t)  \\frac{t^2}{2} \\exp(\\lambda t) \\\\\n0  \\exp(\\lambda t)  t \\exp(\\lambda t) \\\\\n0  0  \\exp(\\lambda t)\n\\end{pmatrix} \\begin{pmatrix}\n0 \\\\\n0 \\\\\n1\n\\end{pmatrix} $$\nPerforming the matrix-vector multiplication, we focus on the first row to find $z_1(t)$:\n$$ z_1(t) \\;=\\; (0) \\cdot \\exp(\\lambda t) + (0) \\cdot t \\exp(\\lambda t) + (1) \\cdot \\frac{t^2}{2} \\exp(\\lambda t) $$\n$$ z_1(t) \\;=\\; \\frac{t^2}{2} \\exp(\\lambda t) $$\nThis expression represents the evolution of the first component of the state in Jordan coordinates. It clearly shows the interplay between the algebraic growth term $t^2/2$ and the exponential decay term $\\exp(\\lambda t)$.",
            "answer": "$$\\boxed{\\frac{t^2}{2} \\exp(\\lambda t)}$$"
        }
    ]
}