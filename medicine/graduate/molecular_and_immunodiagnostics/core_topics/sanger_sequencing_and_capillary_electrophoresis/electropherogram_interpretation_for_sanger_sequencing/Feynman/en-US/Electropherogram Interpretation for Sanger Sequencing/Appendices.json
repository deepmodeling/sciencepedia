{
    "hands_on_practices": [
        {
            "introduction": "Accurate interpretation of Sanger sequencing data begins with understanding that the raw electropherogram does not directly report the quantity of each nucleotide. This first exercise focuses on the essential step of quantitative correction. By accounting for baseline noise and dye-specific fluorescence response factors, you will practice transforming raw peak heights into a more accurate estimate of allele fractions, a critical skill for analyzing heterozygous positions or quantifying mixtures .",
            "id": "5111691",
            "problem": "A laboratory performing capillary Sanger sequencing for Human Leukocyte Antigen (HLA) typing is interpreting a biallelic Single Nucleotide Polymorphism (SNP) in an HLA gene from a mixed specimen. At the polymorphic position, two dye-labeled terminators corresponding to bases are observed with distinct peak heights in Relative Fluorescence Units (RFU). The instrument was previously calibrated to estimate per-dye response factors, and baseline offsets were estimated locally from a window surrounding the peaks. Assume the following:\n\n- Sanger peak height is linearly proportional to the amount of dye-labeled terminator incorporated into the fragment at that position, with a dye-specific proportionality constant (the response factor).\n- Observed peak height is the sum of true signal and baseline offset.\n- Peak width is approximately constant across channels in the local region, so peak height is a valid proxy for area for relative quantification.\n- The dye-specific response factor is defined so that signal after baseline subtraction is equal to response factor times the amount of labeled terminator; to infer relative amount, divide baseline-subtracted signal by the response factor.\n\nThe observed data at the SNP position are:\n\n- Reference allele peak height $H_{\\mathrm{ref}} = 1480$ RFU with baseline $b_{\\mathrm{ref}} = 90$ RFU and dye response factor $r_{\\mathrm{ref}} = 0.92$.\n- Variant allele peak height $H_{\\mathrm{var}} = 1120$ RFU with baseline $b_{\\mathrm{var}} = 105$ RFU and dye response factor $r_{\\mathrm{var}} = 1.08$.\n\nUsing the fundamental measurement model described above, compute the normalized variant allele fraction after subtracting baseline and adjusting for dye-specific response factors. Express your final answer as a decimal and round to four significant figures. No units are required.",
            "solution": "The interpretation starts from the linear detection model for Sanger sequencing fluorescence. Let $N$ denote the amount of dye-labeled terminator corresponding to a base incorporated at the position, $I$ the true fluorescence signal after baseline subtraction, and $r$ the dye-specific response factor. The linear model asserts\n$$\nI = r \\, N,\n$$\nso that the amount is inferred by\n$$\nN = \\frac{I}{r}.\n$$\nHowever, the instrument reports an observed peak height $H$ that includes baseline $b$, so the true signal is\n$$\nI = H - b.\n$$\nCombining these, the inferred amount for a given peak is\n$$\nN = \\frac{H - b}{r}.\n$$\n\nFor a biallelic position with a reference allele and a variant allele, denote their inferred amounts by $N_{\\mathrm{ref}}$ and $N_{\\mathrm{var}}$, respectively. The normalized variant allele fraction is defined by the ratio of the variant amount to the total amount:\n$$\nf_{\\mathrm{var}} = \\frac{N_{\\mathrm{var}}}{N_{\\mathrm{var}} + N_{\\mathrm{ref}}}.\n$$\nSubstituting the measurement relationships,\n$$\nf_{\\mathrm{var}} = \\frac{\\dfrac{H_{\\mathrm{var}} - b_{\\mathrm{var}}}{r_{\\mathrm{var}}}}{\\dfrac{H_{\\mathrm{var}} - b_{\\mathrm{var}}}{r_{\\mathrm{var}}} + \\dfrac{H_{\\mathrm{ref}} - b_{\\mathrm{ref}}}{r_{\\mathrm{ref}}}}.\n$$\n\nInsert the given values $H_{\\mathrm{ref}} = 1480$, $b_{\\mathrm{ref}} = 90$, $r_{\\mathrm{ref}} = 0.92$, and $H_{\\mathrm{var}} = 1120$, $b_{\\mathrm{var}} = 105$, $r_{\\mathrm{var}} = 1.08$:\n$$\nN_{\\mathrm{ref}} = \\frac{1480 - 90}{0.92} = \\frac{1390}{0.92} = \\frac{139000}{92} = \\frac{34750}{23},\n$$\n$$\nN_{\\mathrm{var}} = \\frac{1120 - 105}{1.08} = \\frac{1015}{1.08} = \\frac{101500}{108} = \\frac{25375}{27}.\n$$\nThus,\n$$\nf_{\\mathrm{var}} = \\frac{\\dfrac{25375}{27}}{\\dfrac{25375}{27} + \\dfrac{34750}{23}}.\n$$\nCompute the denominator by using a common denominator $27 \\cdot 23 = 621$:\n$$\n\\dfrac{25375}{27} + \\dfrac{34750}{23} = \\dfrac{25375 \\cdot 23 + 34750 \\cdot 27}{621} = \\dfrac{583625 + 938250}{621} = \\dfrac{1521875}{621}.\n$$\nTherefore,\n$$\nf_{\\mathrm{var}} = \\frac{\\dfrac{25375}{27}}{\\dfrac{1521875}{621}} = \\frac{25375}{27} \\cdot \\frac{621}{1521875} = \\frac{25375 \\cdot 621}{27 \\cdot 1521875}.\n$$\nSimplify:\n$$\n25375 \\cdot 621 = 25375 \\cdot (600 + 21) = 15225000 + 532875 = 15757875,\n$$\n$$\n27 \\cdot 1521875 = 411? \\text{Compute precisely: } 1521875 \\cdot 27 = 1521875 \\cdot (30 - 3) = 45656250 - 4565625 = 41090625,\n$$\nhence\n$$\nf_{\\mathrm{var}} = \\frac{15757875}{41090625}.\n$$\nCancel common factors to reduce:\n$$\n\\frac{15757875}{41090625} = \\frac{15757875 \\div 9}{41090625 \\div 9} = \\frac{1750875}{4565625} = \\frac{1750875 \\div 5}{4565625 \\div 5} = \\frac{350175}{913125} = \\frac{350175 \\div 3}{913125 \\div 3} = \\frac{116725}{304375} = \\frac{116725 \\div 5}{304375 \\div 5} = \\frac{23345}{60875} = \\frac{23345 \\div 5}{60875 \\div 5} = \\frac{4669}{12175}.\n$$\nThus the exact fraction is\n$$\nf_{\\mathrm{var}} = \\frac{4669}{12175}.\n$$\nConvert to a decimal and round to four significant figures:\n$$\nf_{\\mathrm{var}} \\approx 0.383490759\\ldots \\rightarrow 0.3835 \\text{ (four significant figures)}.\n$$",
            "answer": "$$\\boxed{0.3835}$$"
        },
        {
            "introduction": "Building upon basic signal correction, we now address a more complex artifact: spectral cross-talk, where the fluorescence from one dye \"leaks\" into the detection channel of another. This practice challenges you to move beyond simple visual inspection of overlapping peaks and apply a demixing model to resolve the true underlying signals. This process of using a mathematical model and statistical thresholds to make a robust heterozygous base call  is fundamental to automated base-calling algorithms and reliable variant detection.",
            "id": "5111699",
            "problem": "A four-color electropherogram from Sanger sequencing of a polymerase chain reaction amplicon in a molecular and immunodiagnostics workflow is recorded using Capillary Electrophoresis (CE). At position $k$ in the read, the trace shows overlapping peaks in the channels corresponding to adenine and guanine dyes, with the other two channels near baseline. The instrument’s spectral calibration reports known cross-talk fractions between these two channels due to dye emission overlap and imperfect spectral separation: the fraction of true adenine signal contributing to the guanine channel is $0.12$, and the fraction of true guanine signal contributing to the adenine channel is $0.15$. Cross-talk from cytosine or thymine into adenine or guanine is negligible (at most $0.01$ of the true signal). The observed intensities (arbitrary units) at position $k$ are $y_{\\text{adenine}} = 1000$, $y_{\\text{guanine}} = 900$, $y_{\\text{cytosine}} = 120$, and $y_{\\text{thymine}} = 80$. The baseline noise in each observed channel is well modeled as independent, zero-mean Gaussian with standard deviation $\\sigma = 70$.\n\nAssume the following principles and facts as the fundamental base:\n- In Sanger sequencing with four dye-labeled dideoxynucleotides, the measured four-channel fluorescence at a single position is a linear superposition of dye emissions, and spectral cross-talk is linear. Thus the observed intensity vector is the linear mixing of the true base intensity vector plus measurement noise.\n- The noise is approximately Gaussian and independent across channels under stable CE conditions.\n- Base calling corresponds to deciding which base(s) have nonzero true intensity at position $k$, consistent with the observed data and noise.\n\nUnder these assumptions, infer the most probable base call at position $k$ and justify the decision rule that leads to your call. Choose the single best option:\n\nA. Declare a mixed adenine/guanine call (International Union of Pure and Applied Chemistry (IUPAC) code “R”) based on a maximum likelihood demixing criterion: estimate the demixed true intensities by inverting the $2 \\times 2$ cross-talk system for adenine/guanine; then accept a mixed call if both demixed components have $z$-scores $z \\ge 3$ and the ratio of the top to second demixed intensities is $\\le 1.5$.\n\nB. Declare a single-base adenine call using maximum likelihood demixing, requiring only that the demixed adenine intensity exceed $3 \\sigma$; ignore the demixed guanine component if adenine passes threshold.\n\nC. Declare a single-base guanine call using the raw observed intensities only (no cross-talk correction), because $y_{\\text{guanine}}$ is large and the adenine and guanine peaks overlap.\n\nD. Declare “no call” because the overlapping peaks indicate poor signal-to-noise ratio (SNR), and neither demixed component can be significantly distinguished from baseline at $4 \\sigma$.",
            "solution": "### Step 1: Extract Givens\n\nThe problem provides the following information for a Sanger sequencing read at position $k$:\n-   Observed intensity for adenine: $y_{\\text{adenine}} = 1000$\n-   Observed intensity for guanine: $y_{\\text{guanine}} = 900$\n-   Observed intensity for cytosine: $y_{\\text{cytosine}} = 120$\n-   Observed intensity for thymine: $y_{\\text{thymine}} = 80$\n-   Cross-talk from true adenine signal to guanine channel: $c_{GA} = 0.12$\n-   Cross-talk from true guanine signal to adenine channel: $c_{AG} = 0.15$\n-   Cross-talk from cytosine or thymine into adenine or guanine is negligible ($\\le 0.01$).\n-   Baseline noise model: Independent, zero-mean Gaussian.\n-   Standard deviation of a single channel's noise: $\\sigma = 70$.\n-   Fundamental principle: Observed intensity is a linear superposition of true base intensities plus noise.\n-   Fundamental principle: Base calling involves inferring which true base intensities are non-zero.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem statement is scientifically grounded, well-posed, and objective.\n-   **Scientific Grounding**: The scenario describes spectral cross-talk in four-color Sanger sequencing, a well-documented phenomenon. The use of a linear mixing model to describe this cross-talk is the standard, physically motivated approach in the field. The provided values for cross-talk, signal intensities, and noise are realistic.\n-   **Well-Posedness**: The problem provides a clear mathematical model (linear mixing plus Gaussian noise) and sufficient data to solve for the unknown true intensities. The task is to infer the most probable state (base call) based on this model, which is a standard problem in statistical signal processing and is well-posed.\n-   **Objectivity**: The problem is stated using precise, quantitative language, free of ambiguity or subjective claims.\n\nThe problem does not violate any of the invalidity criteria. It is a formalizable, scientifically relevant problem with a complete and consistent setup.\n\n### Step 3: Verdict and Action\n\nThe problem statement is **valid**. The solution process will proceed.\n\n### Derivation of Solution\n\nThe problem requires us to infer the true base composition at position $k$ from the observed fluorescence intensities, accounting for spectral cross-talk and noise.\n\nLet $x_A$ and $x_G$ be the true, unknown signal intensities for adenine and guanine, respectively. Let $y_A$ and $y_G$ be the observed intensities. The problem states that the observed signals are a linear mixture of the true signals plus noise. Given the negligible cross-talk from cytosine (C) and thymine (T), and their low observed intensities ($y_C = 120$ and $y_T=80$, which are less than $2\\sigma$ from baseline), we can focus on the $2 \\times 2$ system for adenine (A) and guanine (G).\n\nThe linear mixing model is:\n$$ y_A = 1 \\cdot x_A + c_{AG} \\cdot x_G + \\epsilon_A $$\n$$ y_G = c_{GA} \\cdot x_A + 1 \\cdot x_G + \\epsilon_G $$\nwhere $\\epsilon_A$ and $\\epsilon_G$ are independent Gaussian noise terms with mean $0$ and standard deviation $\\sigma=70$.\n\nIn matrix form, $\\mathbf{y} = M \\mathbf{x} + \\boldsymbol{\\epsilon}$, where:\n$$ \\mathbf{y} = \\begin{pmatrix} y_A \\\\ y_G \\end{pmatrix} = \\begin{pmatrix} 1000 \\\\ 900 \\end{pmatrix}, \\quad \\mathbf{x} = \\begin{pmatrix} x_A \\\\ x_G \\end{pmatrix}, \\quad M = \\begin{pmatrix} 1 & c_{AG} \\\\ c_{GA} & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 0.15 \\\\ 0.12 & 1 \\end{pmatrix} $$\n\nThe most probable estimate for the true intensities $\\mathbf{x}$ is the maximum likelihood estimate, which, for Gaussian noise, is found by solving the linear system $\\mathbf{y} = M \\mathbf{x}$. This is called \"demixing\" or \"unmixing\". The solution is $\\mathbf{\\hat{x}} = M^{-1} \\mathbf{y}$.\n\nFirst, we compute the inverse of the mixing matrix $M$:\n$$ \\det(M) = (1)(1) - (0.15)(0.12) = 1 - 0.018 = 0.982 $$\n$$ M^{-1} = \\frac{1}{\\det(M)} \\begin{pmatrix} 1 & -c_{AG} \\\\ -c_{GA} & 1 \\end{pmatrix} = \\frac{1}{0.982} \\begin{pmatrix} 1 & -0.15 \\\\ -0.12 & 1 \\end{pmatrix} $$\n\nNow, we calculate the estimated true intensities, $\\mathbf{\\hat{x}}$:\n$$ \\mathbf{\\hat{x}} = \\begin{pmatrix} \\hat{x}_A \\\\ \\hat{x}_G \\end{pmatrix} = \\frac{1}{0.982} \\begin{pmatrix} 1 & -0.15 \\\\ -0.12 & 1 \\end{pmatrix} \\begin{pmatrix} 1000 \\\\ 900 \\end{pmatrix} $$\n$$ \\begin{pmatrix} \\hat{x}_A \\\\ \\hat{x}_G \\end{pmatrix} = \\frac{1}{0.982} \\begin{pmatrix} 1000 - (0.15 \\times 900) \\\\ -(0.12 \\times 1000) + 900 \\end{pmatrix} = \\frac{1}{0.982} \\begin{pmatrix} 1000 - 135 \\\\ -120 + 900 \\end{pmatrix} = \\frac{1}{0.982} \\begin{pmatrix} 865 \\\\ 780 \\end{pmatrix} $$\n$$ \\hat{x}_A = \\frac{865}{0.982} \\approx 880.86 $$\n$$ \\hat{x}_G = \\frac{780}{0.982} \\approx 794.30 $$\n\nTo assess the statistical significance of these demixed signals, we must propagate the noise from the observed signals $\\mathbf{y}$ to the estimated true signals $\\mathbf{\\hat{x}}$. The covariance matrix of the noise in $\\mathbf{y}$ is $\\Sigma_\\epsilon = \\sigma^2 I$, where $I$ is the identity matrix. The covariance matrix of the estimated signal $\\mathbf{\\hat{x}}$ is $\\Sigma_{\\hat{x}} = M^{-1} \\Sigma_\\epsilon (M^{-1})^T = \\sigma^2 M^{-1} (M^{-1})^T$. The variances of $\\hat{x}_A$ and $\\hat{x}_G$ are the diagonal elements of this matrix.\nThe standard deviation of the noise in the demixed signals is slightly larger than $\\sigma$. Let's calculate them.\n$$ \\sigma_{\\hat{x}_A}^2 = Var(\\hat{x}_A) = \\frac{\\sigma^2}{\\det(M)^2} (1^2 + (-c_{AG})^2) = \\frac{70^2}{0.982^2} (1^2 + (-0.15)^2) \\approx 5195.6 $$\n$$ \\sigma_{\\hat{x}_A} = \\sqrt{5195.6} \\approx 72.08 $$\n$$ \\sigma_{\\hat{x}_G}^2 = Var(\\hat{x}_G) = \\frac{\\sigma^2}{\\det(M)^2} ((-c_{GA})^2 + 1^2) = \\frac{70^2}{0.982^2} ((-0.12)^2 + 1^2) \\approx 5154.5 $$\n$$ \\sigma_{\\hat{x}_G} = \\sqrt{5154.5} \\approx 71.80 $$\nThe $z$-score for each component, testing against the null hypothesis of zero true signal, is the estimated signal divided by its standard deviation:\n$$ z_A = \\frac{\\hat{x}_A}{\\sigma_{\\hat{x}_A}} = \\frac{880.86}{72.08} \\approx 12.22 $$\n$$ z_G = \\frac{\\hat{x}_G}{\\sigma_{\\hat{x}_G}} = \\frac{794.30}{71.80} \\approx 11.06 $$\n\nBoth signals are highly statistically significant.\n\n### Option-by-Option Analysis\n\n**A. Declare a mixed adenine/guanine call (International Union of Pure and Applied Chemistry (IUPAC) code “R”) based on a maximum likelihood demixing criterion: estimate the demixed true intensities by inverting the $2 \\times 2$ cross-talk system for adenine/guanine; then accept a mixed call if both demixed components have $z$-scores $z \\ge 3$ and the ratio of the top to second demixed intensities is $\\le 1.5$.**\nThis option describes a scientifically sound procedure.\n1.  **Demixing**: The method of inverting the $2 \\times 2$ cross-talk system is the correct maximum likelihood approach, which we have performed.\n2.  **Significance Test ($z \\ge 3$)**: Our calculated $z$-scores are $z_A \\approx 12.22$ and $z_G \\approx 11.06$. Both are far greater than $3$, so this condition is met. This indicates that both an adenine and a guanine signal are present with very high confidence.\n3.  **Intensity Ratio Test ($\\le 1.5$)**: The demixed intensities are $\\hat{x}_A \\approx 880.86$ (top) and $\\hat{x}_G \\approx 794.30$ (second). The ratio is $\\frac{880.86}{794.30} \\approx 1.11$. This value is less than or equal to $1.5$. This condition is also met.\nThe combination of these criteria (both signals significant and their intensities are not drastically different) is a standard and robust heuristic for calling a heterozygous base (IUPAC code \"R\" for A or G). The procedure and conclusion are fully supported by the data.\n**Verdict: Correct**\n\n**B. Declare a single-base adenine call using maximum likelihood demixing, requiring only that the demixed adenine intensity exceed $3 \\sigma$; ignore the demixed guanine component if adenine passes threshold.**\nThis option correctly advocates for demixing. However, its decision logic is flawed. The rule to \"ignore the demixed guanine component\" is scientifically unsound. The purpose of demixing is to obtain the best estimates for *all* components. Our analysis showed that the demixed guanine signal $\\hat{x}_G$ is extremely strong ($z_G \\approx 11.06$). To willfully ignore this strong evidence for guanine is incorrect and would lead to a misinterpretation of the data. The data strongly support the presence of both bases.\n**Verdict: Incorrect**\n\n**C. Declare a single-base guanine call using the raw observed intensities only (no cross-talk correction), because $y_{\\text{guanine}}$ is large and the adenine and guanine peaks overlap.**\nThis option is incorrect for two primary reasons. First, it advocates using raw intensities and ignoring the known cross-talk. This is a flawed methodology, as it fails to correct for a known systematic error. The principle of linear superposition provided in the problem statement explicitly justifies cross-talk correction. Second, even with this flawed methodology, the conclusion is wrong. The raw intensity for adenine is $y_A = 1000$ and for guanine is $y_G = 900$. Based on raw peak height, adenine has the stronger signal, not guanine. The option incorrectly suggests a guanine call.\n**Verdict: Incorrect**\n\n**D. Declare “no call” because the overlapping peaks indicate poor signal-to-noise ratio (SNR), and neither demixed component can be significantly distinguished from baseline at $4 \\sigma$.**\nThis option presents two factually incorrect claims. First, \"overlapping peaks indicate poor signal-to-noise ratio\" is a false premise. Overlapping peaks are caused by spectral overlap or heterozygosity and are independent of SNR. In this case, the raw signals ($1000$ and $900$) are very large compared to the noise standard deviation ($\\sigma=70$), indicating excellent SNR ($y_A/\\sigma_A \\approx 14.3$). Second, the claim that \"neither demixed component can be significantly distinguished from baseline at $4 \\sigma$\" is false. Our calculated $z$-scores ($z_A \\approx 12.22$ and $z_G \\approx 11.06$) are both substantially larger than $4$. Therefore, both components are highly significant. Declaring \"no call\" would be an incorrect decision.\n**Verdict: Incorrect**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "After mastering the interpretation of individual base positions, the final step in quality assurance is to assess the reliability of the entire sequencing read. The quality of Sanger data typically degrades towards the end of the read, making it necessary to identify the high-confidence region suitable for diagnostic analysis. This computational exercise guides you through the implementation of a quality trimming algorithm , a standard and indispensable procedure in bioinformatics pipelines for ensuring data integrity.",
            "id": "5111645",
            "problem": "You are given a contiguous base-by-base Deoxyribonucleic Acid (DNA) read from Sanger sequencing summarized as a vector of base-calling Phred quality scores $Q_1, Q_2, \\ldots, Q_n$, where each $Q_i$ is defined by the Phred relation $Q_i = -10 \\log_{10}(p_{e,i})$ for error probability $p_{e,i} \\in [0,1]$. In the context of molecular and immunodiagnostics, a common practice is to extract a contiguous high-confidence region whose average quality is at least a threshold corresponding to a low base-calling error rate. Let the threshold be $T = 30$, which corresponds to $p_e \\le 10^{-3}$. Given a minimum window size $w \\in \\mathbb{Z}_{>0}$, define a contiguous region by its boundaries $(s,e)$ with $1 \\le s \\le e \\le n$ and length $L = e - s + 1$. The region is admissible if $L \\ge w$ and its mean quality satisfies\n$$\n\\frac{1}{L} \\sum_{i=s}^{e} Q_i \\ge T.\n$$\nAmong all admissible regions, select one with the maximum length $L$. If multiple regions share the same maximum length, choose the one with the smallest start index $s$, and if still tied, choose the one with the smallest end index $e$. If no admissible region exists, output the sentinel pair $[-1,-1]$. All boundary indices must be reported as $1$-based inclusive positions.\n\nYour task is to implement a program that, for each provided test case consisting of a list $[Q_1,\\ldots,Q_n]$ (with $Q_i$ real-valued) and an integer $w$, returns the boundaries $[s,e]$ of the selected region according to the rules above.\n\nFundamental base to use:\n- Phred quality definition $Q_i = -10 \\log_{10}(p_{e,i})$ and the interpretation that $Q \\ge 30$ implies a mean per-base error probability of at most $10^{-3}$.\n- The definition of mean and inequalities on contiguous sums.\n\nDesign your algorithm from these principles without assuming any shortcut formulas.\n\nInput for testing is embedded within your program. Use the following test suite of $(Q, w)$ pairs:\n1. $Q = [10,12,15,28,32,35,37,33,30,31,29,28,27,20]$, $w = 3$.\n2. $Q = [20,22,25,27,29]$, $w = 2$.\n3. $Q = [30,30,30]$, $w = 3$.\n4. $Q = [40,10,40,10,40,10]$, $w = 3$.\n5. $Q = [29,31,28,32,25,35,27,30]$, $w = 1$.\n6. $Q = [35,35,35]$, $w = 4$.\n\nThe required final output format is a single line containing a comma-separated list of the six results, each result being a two-integer list $[s,e]$. Use $[-1,-1]$ if no admissible region exists. For example, a valid output looks like\n$$\n[[s_1,e_1],[s_2,e_2],[s_3,e_3],[s_4,e_4],[s_5,e_5],[s_6,e_6]]\n$$\nwith no spaces. There are no physical units or angles to report in this problem; all outputs are integers representing $1$-based indices, and no percentages are required.",
            "solution": "The goal is to select the longest contiguous region $(s,e)$ with length $L = e - s + 1 \\ge w$ whose mean quality satisfies $\\frac{1}{L} \\sum_{i=s}^{e} Q_i \\ge T$, with $T = 30$. Starting from the Phred quality definition $Q_i = -10 \\log_{10}(p_{e,i})$, the threshold $T = 30$ encodes a maximum admissible mean error probability. Operationally, the selection reduces to a constraint on the arithmetic mean of the contiguous subarray of quality scores.\n\nTo express this selection purely in terms of sums and inequalities, note that the mean criterion can be rewritten as\n$$\n\\frac{1}{L} \\sum_{i=s}^{e} Q_i \\ge T \\quad \\Longleftrightarrow \\quad \\sum_{i=s}^{e} (Q_i - T) \\ge 0.\n$$\nDefine a transformed sequence $A_i = Q_i - T$. Then the admissible region condition becomes $\\sum_{i=s}^{e} A_i \\ge 0$ with $L \\ge w$. Introduce the prefix sums\n$$\nS_0 = 0, \\quad S_j = \\sum_{i=1}^{j} A_i \\quad \\text{for} \\quad j = 1,2,\\ldots,n.\n$$\nFor any candidate interval $(s,e)$, one has\n$$\n\\sum_{i=s}^{e} A_i = S_e - S_{s-1}.\n$$\nThus, the admissibility becomes $S_e - S_{s-1} \\ge 0$ and $e - s + 1 \\ge w$. Let $i = s - 1$ and $j = e$. The constraints rewrite as\n$$\nS_j \\ge S_i \\quad \\text{and} \\quad j - i \\ge w, \\quad \\text{with} \\quad 0 \\le i < j \\le n.\n$$\nWe want to maximize $L = j - i$ subject to these constraints and then apply tie-breaking rules: choose minimal $s = i + 1$, and if tied, minimal $e = j$.\n\nA principle-based algorithm arises from the structure of prefix sums. Construct a list of indices $L_{\\text{dec}}$ where the prefix sums are strictly decreasing:\n$$\nL_{\\text{dec}} = [\\ell_0, \\ell_1, \\ldots], \\quad \\text{with} \\quad S_{\\ell_0} > S_{\\ell_1} > \\cdots,\n$$\nand $\\ell_k$ strictly increasing as positions. This can be obtained by scanning $S_0, S_1, \\ldots, S_n$ and appending index $i$ whenever $S_i$ is strictly less than the last stored $S_{\\ell}$ value. The reason this structure is useful is that, for any $j$, finding an $i \\le j - w$ with $S_i \\le S_j$ and maximizing $j - i$ is equivalent to choosing the smallest index $i$ (earliest position) among those with $S_i \\le S_j$, because a smaller $i$ yields a longer $j - i$. Within $L_{\\text{dec}}$, since the stored $S_i$ values are strictly decreasing and the indices are increasing, the set of candidates $i \\le j - w$ forms a prefix of $L_{\\text{dec}}$. Over that prefix, to satisfy $S_i \\le S_j$, one needs the leftmost element whose $S$ value is less than or equal to $S_j$. Because the $S$ values are decreasing, a binary search over this prefix returns the leftmost index $i$ with $S_i \\le S_j$, which is also the earliest position among all admissible starts up to $j - w$. This directly yields the longest admissible interval ending at $j$.\n\nAlgorithmic steps derived from these principles:\n1. Transform the input $Q_i$ to $A_i = Q_i - T$, with $T = 30$.\n2. Compute prefix sums $S_0, S_1, \\ldots, S_n$ of $A_i$.\n3. Build $L_{\\text{dec}}$ as the list of indices where $S$ is strictly decreasing versus the last appended value, ensuring indices are in increasing order while $S$ values are strictly decreasing.\n4. For each $j$ from $n$ down to $w$, restrict the candidates to indices $i \\le j - w$ in $L_{\\text{dec}}$ (a prefix of the list). Over that prefix, perform a binary search on the decreasing sequence of $S$ values to find the leftmost index $i$ such that $S_i \\le S_j$. If found, compute the candidate interval $(i+1, j)$ and update the best interval according to maximal length and tie-breaking rules (minimal $s$, then minimal $e$).\n5. If no candidate interval is found across all $j$, output $[-1,-1]$.\n\nCorrectness follows from the characterization of admissible intervals via prefix sums and the monotonicity properties exploited by $L_{\\text{dec}}$. The binary search guarantees, for each $j$, the earliest start $i$ among all valid starts up to $j - w$, hence the longest $j - i$. Scanning $j$ from larger to smaller allows discovering longer intervals first, but tie-breaking is explicitly enforced to prefer smaller $s$ and then smaller $e$ when lengths are equal.\n\nComplexity: computing $A_i$ and $S_j$ is $\\mathcal{O}(n)$; building $L_{\\text{dec}}$ is $\\mathcal{O}(n)$; each $j$ performs a binary search over a prefix of $L_{\\text{dec}}$, which is $\\mathcal{O}(\\log n)$. The overall complexity is $\\mathcal{O}(n \\log n)$, which is efficient for typical read lengths in Sanger sequencing.\n\nApplying this procedure to the test suite yields the following selections:\n1. $[4,14]$.\n2. $[-1,-1]$.\n3. $[1,3]$.\n4. $[1,3]$.\n5. $[1,6]$.\n6. $[-1,-1]$.\n\nThese indices are $1$-based inclusive, as required. The final program prints these six results in the specified single-line, comma-separated, bracketed format with no spaces.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom bisect import bisect_right\n\ndef longest_high_quality_region(Q, w, threshold=30.0):\n    \"\"\"\n    Find the longest contiguous region [s,e] (1-based inclusive) with mean(Q) >= threshold\n    and length >= w. If multiple regions have the same maximal length, choose the one\n    with smallest start s; if still tied, choose the one with smallest end e.\n    Return [-1,-1] if no such region exists or if w > n or w <= 0.\n    \"\"\"\n    n = len(Q)\n    if w <= 0 or w > n:\n        return [-1, -1]\n\n    # Transform to A_i = Q_i - threshold and compute prefix sums S.\n    # S[0] = 0, S[j] = sum_{i=1..j} A_i\n    A = [float(q) - float(threshold) for q in Q]\n    S = [0.0] * (n + 1)\n    for j in range(1, n + 1):\n        S[j] = S[j - 1] + A[j - 1]\n\n    # Build list of indices where S is strictly decreasing (monotone decreasing values).\n    dec_idx = []\n    for i in range(n + 1):\n        if not dec_idx or S[i] < S[dec_idx[-1]]:\n            dec_idx.append(i)\n    dec_vals = [S[i] for i in dec_idx]  # strictly decreasing sequence\n\n    # Helper: binary search for the leftmost index k in dec_vals[0..hi]\n    # such that dec_vals[k] <= target, given dec_vals is strictly decreasing.\n    def leftmost_leq_in_decreasing(dec_vals, target, hi):\n        # Search in [0, hi]\n        left, right = 0, hi\n        while left < right:\n            mid = (left + right) // 2\n            if dec_vals[mid] <= target:\n                right = mid\n            else:\n                left = mid + 1\n        if hi >= 0 and dec_vals[left] <= target:\n            return left\n        return None\n\n    best_len = -1\n    best_s = -1\n    best_e = -1\n\n    # Iterate j from n down to w to consider all possible ends.\n    for j in range(n, w - 1, -1):\n        # Eligible start indices i must satisfy i <= j - w.\n        # dec_idx is increasing in i, so eligible candidates are dec_idx[0..m],\n        # where m is the rightmost position with dec_idx[m] <= j - w.\n        m = bisect_right(dec_idx, j - w) - 1\n        if m < 0:\n            continue\n        # Find earliest i in that prefix such that S[i] <= S[j].\n        k = leftmost_leq_in_decreasing(dec_vals, S[j], m)\n        if k is None:\n            continue\n        i = dec_idx[k]\n        length = j - i\n        s = i + 1  # convert to 1-based start\n        e = j      # 1-based end equals j\n\n        # Update best with tie-breaking: max length, then smallest s, then smallest e.\n        if length > best_len:\n            best_len = length\n            best_s = s\n            best_e = e\n        elif length == best_len:\n            if best_s == -1 or s < best_s or (s == best_s and e < best_e):\n                best_s = s\n                best_e = e\n\n    if best_len >= w:\n        return [best_s, best_e]\n    else:\n        return [-1, -1]\n\ndef format_results_no_spaces(results):\n    # Format as [[a,b],[c,d],...] with no spaces.\n    parts = []\n    for pair in results:\n        parts.append(\"[\" + \",\".join(str(x) for x in pair) + \"]\")\n    return \"[\" + \",\".join(parts) + \"]\"\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        ([10,12,15,28,32,35,37,33,30,31,29,28,27,20], 3),\n        ([20,22,25,27,29], 2),\n        ([30,30,30], 3),\n        ([40,10,40,10,40,10], 3),\n        ([29,31,28,32,25,35,27,30], 1),\n        ([35,35,35], 4),\n    ]\n\n    results = []\n    for Q, w in test_cases:\n        result = longest_high_quality_region(Q, w, threshold=30.0)\n        results.append(result)\n\n    # Final print statement in the exact required format: no spaces.\n    print(format_results_no_spaces(results))\n\nsolve()\n```"
        }
    ]
}