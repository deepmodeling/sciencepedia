{
    "hands_on_practices": [
        {
            "introduction": "The first step in planning any sequencing project is determining the required amount of data. This is quantified by sequencing coverage, which represents the average number of times each base in a target region is sequenced. This exercise  provides a foundational understanding of this crucial parameter by asking you to derive the classic relationship between coverage, read count, read length, and genome size from first principles. You will then apply this formula to a practical immunogenomics scenario, calculating the instrument time needed to achieve a target coverage for robust analysis.",
            "id": "5163228",
            "problem": "A clinical immunogenomics laboratory plans to use Single Molecule Real-Time (SMRT) long-read sequencing to characterize the extended Major Histocompatibility Complex (MHC) region. Let the target region length be $G$ base pairs. Define the expected coverage $C$ at a randomly chosen base as the expected number of sequenced bases overlapping that base, under the standard assumptions that read starts are uniformly distributed over the region and that edge effects can be neglected because $G \\gg L$, where $L$ is the mean read length. Starting from this definition and using only linearity of expectation and the assumption of uniform read start positions, derive an expression for $C$ in terms of the number of reads $N$, the mean read length $L$, and the region length $G$.\n\nThe laboratory’s assay targets the classical human leukocyte antigen (HLA) region of length $G = 3.9 \\times 10^{6}$ base pairs. To achieve robust haplotype phasing in an immunodiagnostic setting, they require an expected coverage of $C^{\\star} = 47$. Their SMRT instrument and library preparation yield, on average, $r_{N} = 2.5 \\times 10^{4}$ polymerase reads per hour with a mean read length $L = 1.8 \\times 10^{4}$ base pairs, of which a usable and on-target fraction $f_{u} = 0.51$ contributes to the target region after quality filtering and hybridization capture specificity. Assume read production rate and usable fraction are constant over time and independent of the target.\n\nUsing your derived expression and first principles, compute the total runtime in hours required to achieve the target expected coverage $C^{\\star}$ on the $G$-base-pair region with the above instrument performance. Round your final runtime to four significant figures. Express your answer in hours.",
            "solution": "The problem as stated is scientifically grounded, well-posed, and contains all necessary information for a unique solution. The parameters provided are realistic for the described application in immunogenomics. I will proceed with the derivation and calculation.\n\nThe problem is divided into two parts. First, we must derive an expression for the expected coverage $C$ as a function of the number of reads $N$, the mean read length $L$, and the target region length $G$. Second, we must use this expression to calculate the required instrument runtime to achieve a specific target coverage $C^{\\star}$.\n\nPart 1: Derivation of the Expected Coverage Formula\n\nLet the target region be represented by the interval $[0, G]$. Consider an arbitrary base at a position $x$ within this region. We are asked to find the expected number of reads that cover this position $x$. This is the definition of expected coverage, $C$.\n\nLet there be $N$ total reads. Let's consider a single read, indexed by $i$, where $i \\in \\{1, 2, ..., N\\}$. This read has a length $L_i$. The problem states that the mean read length is $L$, which means $$L = E[L_i] = \\frac{1}{N} \\sum_{i=1}^{N} L_i$$\n\nThe starting position of read $i$, denoted by $s_i$, is assumed to be uniformly distributed over the target region. Given the assumption that edge effects are negligible ($G \\gg L$), we can model the start positions as being uniformly distributed over the interval $[0, G]$.\n\nFor read $i$ to cover the base at position $x$, its start position $s_i$ must fall within the interval $[x - L_i + 1, x]$. The length of this interval of valid starting positions is $L_i$. The probability, $p_i$, that a randomly chosen start position $s_i$ falls within this interval is the ratio of the length of the valid interval to the length of the total possible interval for start positions.\n$$p_i = P(\\text{read } i \\text{ covers } x) = \\frac{L_i}{G}$$\n\nLet us define an indicator random variable $I_i$ for each read:\n$$I_i = \\begin{cases} 1 & \\text{if read } i \\text{ covers position } x \\\\ 0 & \\text{otherwise} \\end{cases}$$\nThe expected value of this indicator variable is the probability that the event occurs:\n$$E[I_i] = 1 \\cdot P(I_i=1) + 0 \\cdot P(I_i=0) = P(I_i=1) = p_i = \\frac{L_i}{G}$$\n\nThe total coverage at position $x$, denoted $C_x$, is the sum of the indicator variables for all $N$ reads:\n$$C_x = \\sum_{i=1}^{N} I_i$$\nThe expected coverage, $C$, is the expectation of $C_x$. Using the linearity of expectation, which allows us to write the expectation of a sum as the sum of expectations, we have:\n$$C = E[C_x] = E\\left[\\sum_{i=1}^{N} I_i\\right] = \\sum_{i=1}^{N} E[I_i]$$\nSubstituting the expression for $E[I_i]$:\n$$C = \\sum_{i=1}^{N} \\frac{L_i}{G} = \\frac{1}{G} \\sum_{i=1}^{N} L_i$$\nWe know that the total number of sequenced bases is $\\sum_{i=1}^{N} L_i$. This sum can be expressed in terms of the mean read length $L$ and the total number of reads $N$:\n$$\\sum_{i=1}^{N} L_i = N \\cdot L$$\nSubstituting this into the expression for $C$, we arrive at the required formula:\n$$C = \\frac{N L}{G}$$\nThis expression relates the expected coverage $C$ to the number of reads $N$, the mean read length $L$, and the genome or region length $G$.\n\nPart 2: Calculation of the Required Runtime\n\nThe goal is to achieve a target expected coverage of $C^{\\star} = 47$ on a region of length $G = 3.9 \\times 10^{6}$ base pairs. The sequencing process yields a mean read length of $L = 1.8 \\times 10^{4}$ base pairs.\n\nFirst, we calculate the total number of *usable, on-target* reads, let's call this $N_{\\text{usable}}$, required to achieve the coverage $C^{\\star}$. Using the formula we just derived:\n$$C^{\\star} = \\frac{N_{\\text{usable}} L}{G}$$\nRearranging to solve for $N_{\\text{usable}}$:\n$$N_{\\text{usable}} = \\frac{C^{\\star} G}{L}$$\nSubstituting the given values:\n$$N_{\\text{usable}} = \\frac{47 \\cdot (3.9 \\times 10^{6})}{(1.8 \\times 10^{4})} = \\frac{183.3 \\times 10^{6}}{1.8 \\times 10^{4}} = 101.833... \\times 10^{2} = 10183.33...$$\nSo, approximately $10183$ usable reads are required.\n\nNext, we must determine the rate at which these usable reads are generated. The instrument produces a total of $r_{N} = 2.5 \\times 10^{4}$ polymerase reads per hour. However, only a fraction $f_{u} = 0.51$ of these are usable and on-target. The effective rate of usable read generation, $r_{\\text{usable}}$, is therefore:\n$$r_{\\text{usable}} = r_{N} \\cdot f_{u} = (2.5 \\times 10^{4} \\text{ reads/hour}) \\cdot 0.51 = 12750 \\text{ reads/hour}$$\n\nThe total runtime, $T$, is the total number of usable reads required divided by the rate at which they are generated:\n$$T = \\frac{N_{\\text{usable}}}{r_{\\text{usable}}}$$\nSubstituting the expressions for $N_{\\text{usable}}$ and $r_{\\text{usable}}$:\n$$T = \\frac{C^{\\star} G / L}{r_{N} f_{u}} = \\frac{C^{\\star} G}{L r_{N} f_{u}}$$\nNow, we plug in all the numerical values:\n$$T = \\frac{47 \\cdot (3.9 \\times 10^{6})}{(1.8 \\times 10^{4}) \\cdot (2.5 \\times 10^{4}) \\cdot 0.51}$$\n$$T = \\frac{1.833 \\times 10^{8}}{(1.8 \\times 10^{4}) \\cdot (2.5 \\times 10^{4}) \\cdot 0.51} = \\frac{1.833 \\times 10^{8}}{(4.5 \\times 10^{8}) \\cdot 0.51} = \\frac{1.833 \\times 10^{8}}{2.295 \\times 10^{8}}$$\n$$T = \\frac{1.833}{2.295} \\approx 0.7986928... \\text{ hours}$$\n\nThe problem requires the final runtime to be rounded to four significant figures. The fifth significant digit is $9$, so we round up the fourth digit.\n$$T \\approx 0.7987 \\text{ hours}$$",
            "answer": "$$\\boxed{0.7987}$$"
        },
        {
            "introduction": "One of the signature strengths of modern long-read sequencing is the ability to generate not just long reads, but highly accurate ones, often referred to as HiFi reads. This high fidelity is not the result of a single perfect measurement, but rather a clever statistical process called circular consensus sequencing (CCS). This practice  delves into the mathematics behind CCS, modeling how multiple independent, error-prone passes of a single DNA molecule can be combined to produce a consensus sequence with an exponentially lower error rate, allowing you to calculate the number of passes needed to achieve a desired quality score.",
            "id": "5163237",
            "problem": "A clinical laboratory is evaluating Single-Molecule Real-Time (SMRT) circular consensus sequencing for a single-nucleotide variant genotyping assay. Each polymerase traversal around a circularized template yields an independent subread pass. Suppose that in a given genomic position, each pass independently calls the true base with probability $(1-p)$ and an incorrect base with probability $p$, with $0  p  \\frac{1}{2}$. The laboratory constructs a consensus by majority voting over $n$ independent passes, where $n$ is constrained to be odd so that no ties arise. Define the consensus error rate $p_c$ as the probability that the majority-voted consensus base is incorrect. The Phred Quality Value (QV) is defined as $QV = -10\\log_{10}(p_c)$.\n\nUsing only first principles about independent Bernoulli trials and the binomial distribution, do the following:\n\n1. Derive an analytic expression for $p_c$ in terms of $p$ and $n$ under the majority voting rule described above.\n2. Given a per-pass error rate $p=0.15$, determine the minimal odd integer $n$ such that the resulting consensus achieves a target Phred Quality Value $QV^{\\ast}=30$. Equivalently, determine the minimal odd $n$ for which $p_c \\leq 10^{-QV^{\\ast}/10}$. Express your final answer as the minimal odd integer $n$. No rounding is required, and no units are needed in the final answer.",
            "solution": "The problem is valid as it is scientifically grounded in the principles of molecular diagnostics and probability theory, well-posed with a clear objective and sufficient data, and free from any factual or logical inconsistencies.\n\nThe task is addressed in two parts as requested. First, the derivation of an analytic expression for the consensus error rate, $p_c$. Second, the calculation of the minimal number of passes, $n$, to meet a specified quality threshold.\n\nPart 1: Derivation of the analytic expression for $p_c$.\n\nThe process of determining the base at a single genomic position across $n$ independent passes can be modeled as a sequence of $n$ Bernoulli trials. For each trial (pass), there are two outcomes: an incorrect base call (an \"error\") or a correct base call (a \"success\" in terms of getting the right base).\n\nLet $p$ be the probability of an incorrect base call for any single pass. This is given as $0  p  \\frac{1}{2}$.\nThe probability of a correct base call is consequently $1-p$.\n\nLet $K$ be the random variable representing the total number of incorrect base calls over the $n$ independent passes. As the passes are independent and the probability of error $p$ is constant for each, $K$ follows a binomial distribution with parameters $n$ (the number of trials) and $p$ (the probability of an error on each trial). This is denoted as $K \\sim B(n, p)$.\n\nThe probability mass function (PMF) of $K$ is given by the binomial formula:\n$$P(K=k) = \\binom{n}{k} p^k (1-p)^{n-k}$$\nwhere $k$ is an integer representing the number of incorrect calls, $k \\in \\{0, 1, 2, \\dots, n\\}$.\n\nA consensus error occurs if the majority of the passes report an incorrect base. The number of incorrect calls is $k$ and the number of correct calls is $n-k$. A majority of incorrect calls means that $k > n-k$, which simplifies to $2k > n$, or $k > \\frac{n}{2}$.\n\nThe problem specifies that $n$ is an odd integer. Let $n = 2m+1$ for some non-negative integer $m$. The condition for an error becomes $k > \\frac{2m+1}{2} = m + \\frac{1}{2}$. Since $k$ must be an integer, the smallest integer value of $k$ that satisfies this inequality is $m+1$. Substituting $m = \\frac{n-1}{2}$, the condition for a consensus error is $k \\ge \\frac{n-1}{2} + 1 = \\frac{n+1}{2}$.\n\nThe consensus error rate, $p_c$, is the total probability of all outcomes where the consensus is incorrect. This is the probability that $K$ is greater than or equal to $\\frac{n+1}{2}$. We can express this by summing the probabilities for all relevant values of $k$:\n$$p_c(n, p) = P\\left(K \\ge \\frac{n+1}{2}\\right) = \\sum_{k=\\frac{n+1}{2}}^{n} P(K=k)$$\nSubstituting the PMF of the binomial distribution, we arrive at the final analytic expression for $p_c$:\n$$p_c(n, p) = \\sum_{k=\\frac{n+1}{2}}^{n} \\binom{n}{k} p^k (1-p)^{n-k}$$\n\nPart 2: Determination of the minimal odd integer $n$.\n\nWe are given the per-pass error rate $p = 0.15$ and a target Phred Quality Value $QV^{\\ast} = 30$. The quality value is defined as $QV = -10\\log_{10}(p_c)$. The laboratory requires the consensus quality to be at least this target value, so we must have $QV \\ge QV^{\\ast}$.\n$$-10\\log_{10}(p_c) \\ge 30$$\nDividing by $-10$ reverses the inequality:\n$$\\log_{10}(p_c) \\le -3$$\nApplying the exponential function to both sides (base $10$):\n$$p_c \\le 10^{-3}$$\nSo, we must find the minimal odd integer $n$ such that the consensus error rate is no more than $0.001$. With $p=0.15$, the probability of a correct pass is $1-p = 0.85$. The inequality to solve is:\n$$p_c(n) = \\sum_{k=\\frac{n+1}{2}}^{n} \\binom{n}{k} (0.15)^k (0.85)^{n-k} \\le 0.001$$\nSince $p=0.15  0.5$, increasing the number of passes $n$ will decrease the probability of a majority error. Thus, $p_c(n)$ is a monotonically decreasing function of $n$. We can find the minimal $n$ by testing successive odd integers.\n\nFor $n=1$, $p_c(1) = 0.15$, which is greater than $0.001$.\nFor $n=3$, $p_c(3) \\approx 0.06075$, which is greater than $0.001$.\nWe continue testing for $n=5, 7, 9, 11, \\dots$. Let's examine the values near the threshold.\n\nFor $n=13$:\nThe required number of errors for a consensus failure is $k \\ge \\frac{13+1}{2} = 7$.\n$$p_c(13) = \\sum_{k=7}^{13} \\binom{13}{k} (0.15)^k (0.85)^{13-k}$$\nNumerical evaluation of this sum, which is the tail probability of a $B(13, 0.15)$ distribution, yields:\n$$p_c(13) \\approx 0.001265$$\nSince $0.001265 > 0.001$, $n=13$ passes are insufficient to meet the quality requirement.\n\nFor the next odd integer, $n=15$:\nThe required number of errors for a consensus failure is $k \\ge \\frac{15+1}{2} = 8$.\n$$p_c(15) = \\sum_{k=8}^{15} \\binom{15}{k} (0.15)^k (0.85)^{15-k}$$\nNumerical evaluation of this sum for the $B(15, 0.15)$ distribution yields:\n$$p_c(15) \\approx 0.000608$$\nSince $0.000608 \\le 0.001$, $n=15$ passes are sufficient to meet the quality requirement.\n\nGiven that $p_c(n)$ is a decreasing function for increasing $n$, and knowing that $n=13$ is insufficient while $n=15$ is sufficient, the minimal odd integer number of passes required is $15$.",
            "answer": "$$\\boxed{15}$$"
        },
        {
            "introduction": "The true power of long reads is unleashed when analyzing structurally complex genomic regions, such as the hypervariable Human Leukocyte Antigen (HLA) loci, where a single linear reference genome is inadequate. Representing this variation requires a more sophisticated structure, like a graph genome, and a correspondingly advanced alignment algorithm. This hands-on challenge  guides you through the derivation and implementation of a partial order alignment (POA) algorithm, the key to mapping long reads to these complex graph-based references. This exercise bridges the gap from sequencing data to meaningful biological discovery in the most challenging parts of the genome.",
            "id": "5163278",
            "problem": "You are given a directed acyclic graph (DAG) representing a partial-order genome for Human Leukocyte Antigen (HLA) alleles and a Single Molecule Real-Time (SMRT) long-read sequence. From first principles, derive and implement a partial order alignment procedure that generalizes classical global alignment to a node-labeled DAG, using linear gap penalties. Base your derivation on the dynamic programming principle of optimality, without assuming any pre-derived graph-alignment formulae. Your implementation must compute two quantities for each test case: (i) the optimal global alignment score of the read to any path in the DAG, and (ii) a quantitative estimate of the computational effort defined below, to substantiate the theoretical time complexity.\n\nFundamental base to use:\n- Principle of optimal substructure and Bellman optimality in dynamic programming.\n- Needleman–Wunsch global alignment as a special case when the graph is a linear chain.\n- Linear gap model with constant penalty per insertion or deletion.\n- Topological ordering of a directed acyclic graph.\n\nDefinitions and constraints:\n- The graph is a node-labeled directed acyclic graph, where each node carries a single nucleotide from the alphabet $\\{\\text{A},\\text{C},\\text{G},\\text{T}\\}$. An alignment maps the read to a path in the graph, allowing matches, mismatches, and gaps (insertions in the read or deletions in the graph path).\n- Use a linear gap penalty model: each insertion or deletion incurs a constant penalty.\n- Let $V$ be the number of nodes, $E$ be the number of directed edges, and $L$ be the read length.\n- Define three dynamic programming states per node and read position, derived from dynamic programming first principles:\n  - $M$: alignment ending by consuming one graph node and one read character,\n  - $I$: alignment ending by consuming one read character (gap in the graph),\n  - $D$: alignment ending by consuming one graph node (gap in the read).\n- Use global alignment of the entire read to a path in the DAG, with linear penalties applied to all gaps, including those at the ends.\n\nComputational effort metric:\n- Define the computational effort as the exact count of scalar predecessor-state candidates evaluated in taking maxima across all dynamic programming state transitions, summed over the entire dynamic program. Count candidates as follows:\n  - For each node $v$ and position $j$ in $\\{0,1,\\dots,L\\}$, the $D$-state considers $\\deg^{-}(v)$ candidate predecessors if $\\deg^{-}(v) > 0$, otherwise $1$ start candidate.\n  - For each node $v$ and positions $j$ in $\\{1,\\dots,L\\}$, the $I$-state considers $3$ predecessor-state candidates at $(v,j-1)$.\n  - For each node $v$ and positions $j$ in $\\{1,\\dots,L\\}$, the $M$-state considers $3 \\cdot \\deg^{-}(v)$ predecessor-state candidates if $\\deg^{-}(v) > 0$, otherwise $1$ start candidate.\n- The metric does not count additions, only the number of scalar values compared in maxima as specified.\n\nScoring:\n- Match score: $+2$.\n- Mismatch penalty: $-2$.\n- Gap penalty: $-3$.\n\nTest suite:\n- Test case $1$ (SNP bubble DAG, HLA-A-like exon):\n  - Nodes (by index to label): $0{:}A$, $1{:}C$, $2{:}G$, $3{:}T$, $4{:}A$, $5{:}C$, $6{:}G$.\n  - Directed edges $(u \\rightarrow v)$: $(0 \\rightarrow 1)$, $(1 \\rightarrow 2)$, $(1 \\rightarrow 3)$, $(2 \\rightarrow 4)$, $(3 \\rightarrow 4)$, $(4 \\rightarrow 5)$, $(5 \\rightarrow 6)$.\n  - Read: $ACGACG$ so that $L = 6$.\n- Test case $2$ (indel bubble DAG, alternative skipping one node):\n  - Nodes: $0{:}A$, $1{:}C$, $2{:}G$, $3{:}A$, $4{:}C$, $5{:}G$.\n  - Edges: $(0 \\rightarrow 1)$, $(1 \\rightarrow 2)$, $(2 \\rightarrow 3)$, $(3 \\rightarrow 4)$, $(2 \\rightarrow 4)$, $(4 \\rightarrow 5)$.\n  - Read: $ACGCG$ so that $L = 5$.\n- Test case $3$ (mismatch-heavy read on test case $1$ DAG):\n  - Use the same graph as in test case $1$.\n  - Read: $TTTTTT$ so that $L = 6$.\n\nTasks:\n- For each test case, compute:\n  1. The optimal global alignment score as an integer using the scoring above.\n  2. The computational effort metric as defined.\n- Theoretical complexity: derive, from first principles, the asymptotic time complexity in terms of $V$, $E$, and $L$ of your dynamic programming procedure.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Aggregate the results across test cases in order, flattening each pair as two consecutive integers. For example, the output must look like [$s_1,c_1,s_2,c_2,s_3,c_3$] where $s_i$ is the alignment score and $c_i$ is the computational effort for test case $i$.",
            "solution": "The problem requires the derivation and implementation of a partial order alignment (POA) algorithm for aligning a DNA sequence (a read) to a partial-order graph, which is represented as a node-labeled directed acyclic graph (DAG). The alignment must be global, covering the entire read, and must use a linear gap penalty model. The solution must be derived from the first principles of dynamic programming (DP), specifically the principle of optimality.\n\n### 1. Principle of Optimality and State Definitions\n\nThe foundation of our dynamic programming approach is the principle of optimal substructure, which states that an optimal solution to a problem can be constructed from optimal solutions to its subproblems. In the context of sequence alignment, this means the score of the optimal alignment of a read prefix to a graph path can be calculated by extending an optimal alignment of a smaller subproblem.\n\nThe alignment of a read to a path in the DAG can end in one of three ways at any given point:\n1.  A match or mismatch: the last character of the read prefix is aligned to the last node of the graph path.\n2.  An insertion: the last character of the read prefix is aligned to a gap.\n3.  A deletion: the last node of the graph path is aligned to a gap.\n\nTo capture these possibilities, we define three DP states for each node $v$ in the graph and each position $j$ in the read (representing the prefix of length $j$, i.e., $read[1 \\dots j]$). Let $V$ be the set of nodes and $L$ be the length of the read. Let $g$ be the linear gap penalty, and $s(c_1, c_2)$ be the score for aligning character $c_1$ to $c_2$.\n\n-   $M(v, j)$: The optimal score of aligning the read prefix $read[1 \\dots j]$ to a path in the DAG ending at node $v$, where the read character $read[j]$ is aligned to the character label of node $v$.\n-   $I(v, j)$: The optimal score of aligning $read[1 \\dots j]$ to a path ending at $v$, where $read[j]$ is aligned to a gap (an insertion relative to the graph path).\n-   $D(v, j)$: The optimal score of aligning $read[1 \\dots j]$ to a path ending at $v$, where the character label of $v$ is aligned to a gap (a deletion relative to the read).\n\n### 2. Derivation of Recurrence Relations\n\nWe derive the recurrence relations for each state by considering how an alignment can be extended. Let $Pred(v)$ be the set of immediate predecessors of node $v$.\n\n**State $I(v, j)$ (Insertion):** An alignment ending in an insertion of $read[j]$ at node $v$ must extend an alignment of $read[1 \\dots j-1]$ that already ends at node $v$. This previous alignment could have ended in a match/mismatch ($M(v, j-1)$), an insertion ($I(v, j-1)$), or a deletion ($D(v, j-1)$). We take the maximum of these possibilities and add the gap penalty $g$.\n$$I(v, j) = g + \\max \\left\\{ M(v, j-1), I(v, j-1), D(v, j-1) \\right\\}$$\nThe computational effort, as per the problem, for this maximization is $3$ candidates.\n\n**State $M(v, j)$ (Match/Mismatch):** An alignment ending with $read[j]$ matched to node $v$ must extend an alignment of $read[1 \\dots j-1]$ to a path ending at a predecessor node $u \\in Pred(v)$. For each predecessor $u$, the previous alignment could have been any of the three states. We take the maximum over all predecessors and all three states, and add the score for aligning the characters.\n$$M(v, j) = s(\\text{label}(v), read[j]) + \\max_{u \\in Pred(v)} \\left\\{ M(u, j-1), I(u, j-1), D(u, j-1) \\right\\}$$\nThe effort for this is $3 \\cdot \\deg^{-}(v)$ candidates, where $\\deg^{-}(v)$ is the in-degree of $v$. If $v$ is a source node ($\\deg^{-}(v) = 0$), this transition is handled as a special base case.\n\n**State $D(v, j)$ (Deletion):** An alignment ending in a deletion of node $v$ against the read prefix $read[1 \\dots j]$ must extend an alignment of the same read prefix to a path ending at a predecessor $u \\in Pred(v)$. Let $S(u,j) = \\max\\{M(u,j), I(u,j), D(u,j)\\}$. The recurrence becomes:\n$$D(v, j) = g + \\max_{u \\in Pred(v)} \\left\\{ S(u, j) \\right\\}$$\nThis is consistent with the specified effort count of $\\deg^{-}(v)$ candidates.\n\n### 3. Topological Ordering and Algorithm Structure\n\nThe calculation of $D(v,j)$ depends on states at predecessor nodes $u$ at the same read position $j$. This creates a dependency within the \"slice\" of the DP table for a given $j$. To resolve this, nodes must be processed in a topological order. Let the topologically sorted nodes be $v_1, v_2, \\dots, v_{|V|}$.\n\nThe overall algorithm is as follows:\n1.  Compute a topological sort of the graph nodes.\n2.  Initialize the DP tables $M$, $I$, and $D$ for $j=0$.\n3.  Iterate $j$ from $1$ to $L$. In each iteration:\n4.  Iterate through the nodes $v$ in their topological order. For each $v$:\n5.  Calculate $I(v, j)$, then $M(v, j)$, and finally $D(v, j)$ using the recurrence relations. The topological ordering ensures that when computing states for $v$, the required states for all its predecessors $u$ are already available.\n\n### 4. Initialization (Base Cases)\n\n-   **DP Table Initialization:** The tables $M, I, D$ of size $|V| \\times (L+1)$ are initialized to a very large negative number representing $-\\infty$.\n\n-   **Alignment to Empty Read ($j=0$):** An alignment of an empty read to a graph path can only consist of deletions.\n    - For a source node $v$ ($\\deg^{-}(v)=0$), a path of length one consists of only $v$. Aligning this to an empty read means one deletion. Thus, $D(v, 0) = g$. This can be conceptualized as $D(v,0) = g + \\max\\{0\\}$, where $0$ is the score of a virtual start state.\n    - For any other node $v$, a path of deletions ending at $v$ must extend a path of deletions ending at one of its predecessors.\n      $$D(v, 0) = g + \\max_{u \\in Pred(v)} \\{D(u, 0)\\}$$\n    - The states $M(v,0)$ and $I(v,0)$ are impossible and remain $-\\infty$.\n\n-   **Source Nodes ($\\deg^{-}(v)=0$) for $j>0$:** For a source node $v$, there are no predecessors to extend from. This corresponds to starting a new alignment path.\n    -   **$M(v,j)$:** The alignment starts at node $v$, matching $read[j]$. The preceding read characters $read[1 \\dots j-1]$ must have been insertions. The score is $M(v,j) = s(\\text{label}(v), read[j]) + (j-1) \\cdot g$. The effort is $1$ start candidate.\n    -   **$D(v,j)$:** To delete $v$ after aligning $read[1 \\dots j]$, the prefix must be aligned to an empty path (i.e., all insertions). The score is $D(v,j) = g + j \\cdot g$. The effort is $1$ start candidate.\n\n### 5. Final Score Calculation\n\nThe problem asks for the optimal score of a global alignment to *any path* in the DAG. A path does not necessarily span from a source to a sink. Thus, after filling the DP tables up to $j=L$, the final optimal score is the maximum value across all states for all nodes at the final read position $L$.\n$$ \\text{Score}_{\\text{optimal}} = \\max_{v \\in V} \\{ M(v, L), I(v, L), D(v, L) \\} $$\n\n### 6. Computational Complexity Analysis\n\n1.  **Topological Sort:** Can be performed using Kahn's algorithm or DFS in $O(V+E)$ time.\n2.  **Initialization ($j=0$):** Iterating through all nodes in topological order. For each node, we iterate over its predecessors. The total number of predecessor lookups is $\\sum_{v \\in V} \\deg^{-}(v) = E$. This step is $O(V+E)$.\n3.  **Main Loop:** The outer loop runs $L$ times. The inner loop iterates over all $V$ nodes.\n    -   For each $(v, j)$, calculating $I(v,j)$ takes $O(1)$ time. Total: $O(VL)$.\n    -   For each $(v, j)$, calculating $M(v,j)$ takes $O(\\deg^{-}(v))$ time. Over all $v$ for a fixed $j$, this is $\\sum_{v \\in V} O(\\deg^{-}(v)) = O(E)$. Total: $O(EL)$.\n    -   Similarly, calculating $D(v,j)$ takes $O(\\deg^{-}(v))$ time. Total: $O(EL)$.\nThe total time complexity is dominated by the main loop, resulting in $O(VL + EL) = O(L(V+E))$.\n\n### 7. Computational Effort Metric\n\nThe implementation will maintain a counter, `effort`. This counter will be incremented precisely according to the rules specified in the problem statement during the calculation of each DP state. This ensures that the implemented recurrences and base cases exactly match the problem's formal definition. For base cases at source nodes which are defined without an explicit `max`, we implement them as `max` over a single candidate to conform to the effort metric.",
            "answer": "```python\nimport numpy as np\n\ndef topological_sort(graph):\n    \"\"\"\n    Performs a topological sort on the graph using Kahn's algorithm.\n    \"\"\"\n    num_nodes = len(graph['nodes'])\n    in_degree = dict(graph['in_degree'])\n    queue = [i for i in range(num_nodes) if in_degree[i] == 0]\n    topo_order = []\n    \n    while queue:\n        u = queue.pop(0)\n        topo_order.append(u)\n        \n        for v in sorted(graph['adj'].get(u, [])): # sorted for determinism\n            in_degree[v] -= 1\n            if in_degree[v] == 0:\n                queue.append(v)\n                \n    if len(topo_order) != num_nodes:\n        raise ValueError(\"Graph has a cycle\")\n        \n    return topo_order\n\ndef partial_order_alignment(graph, read, match_score, mismatch_penalty, gap_penalty):\n    \"\"\"\n    Performs partial order alignment of a read to a DAG.\n    \"\"\"\n    V = len(graph['nodes'])\n    L = len(read)\n    g = gap_penalty\n    \n    # Initialize DP tables with a large negative number\n    NEG_INF = -10**9\n    M = np.full((V, L + 1), NEG_INF, dtype=np.int64)\n    I = np.full((V, L + 1), NEG_INF, dtype=np.int64)\n    D = np.full((V, L + 1), NEG_INF, dtype=np.int64)\n    \n    effort = 0\n    \n    topo_order = topological_sort(graph)\n    \n    # Initialization for j=0 (empty read prefix)\n    for v in topo_order:\n        preds = graph['pred'].get(v, [])\n        if not preds: # Source node\n            # Conceptually, D[v,0] = g + 0 (score from virtual start node)\n            max_pred_D0 = 0 \n            D[v, 0] = g + max_pred_D0\n            effort += 1 # 1 start candidate\n        else:\n            pred_scores = [D[u, 0] for u in preds]\n            if pred_scores:\n                max_pred_D0 = max(pred_scores)\n                if max_pred_D0 > NEG_INF:\n                   D[v, 0] = g + max_pred_D0\n                effort += len(preds)\n\n    # Main DP loop\n    for j in range(1, L + 1):\n        for v in topo_order:\n            preds = graph['pred'].get(v, [])\n            \n            # State I: Insertion\n            prev_score_I = max(M[v, j-1], I[v, j-1], D[v, j-1])\n            if prev_score_I > NEG_INF:\n                I[v, j] = g + prev_score_I\n                effort += 3\n            \n            # State M: Match/Mismatch\n            v_char = graph['nodes'][v]\n            read_char = read[j-1]\n            score_s = match_score if v_char == read_char else mismatch_penalty\n            \n            max_pred_M = NEG_INF\n            if not preds: # Source node\n                # Alignment starts here, after j-1 insertions\n                max_pred_M = (j - 1) * g\n                effort += 1 # 1 start candidate\n            else:\n                pred_scores_M = []\n                for u in preds:\n                    pred_scores_M.extend([M[u, j-1], I[u, j-1], D[u, j-1]])\n                max_pred_M = max(pred_scores_M) if pred_scores_M else NEG_INF\n                effort += 3 * len(preds)\n            \n            if max_pred_M > NEG_INF:\n                M[v, j] = score_s + max_pred_M\n\n            # State D: Deletion\n            max_pred_D = NEG_INF\n            if not preds: # Source node\n                # To delete v, read[0...j-1] must all be insertions\n                max_pred_D = j * g\n                effort += 1 # 1 start candidate\n            else:\n                # Per effort spec, this takes max over a single score from each predecessor\n                S_u_j = []\n                for u in preds:\n                    s_val = max(M[u, j], I[u, j], D[u, j])\n                    if s_val > NEG_INF:\n                        S_u_j.append(s_val)\n                if S_u_j:\n                    max_pred_D = max(S_u_j)\n                effort += len(preds)\n            \n            if max_pred_D > NEG_INF:\n                D[v, j] = g + max_pred_D\n\n    # Final score is the max over all states at j=L for any node v\n    final_score = NEG_INF\n    if V > 0:\n        final_scores = np.concatenate((M[:, L], I[:, L], D[:, L]))\n        final_score = np.max(final_scores)\n\n    return int(final_score), effort\n\n\ndef solve():\n    \"\"\"\n    Sets up and solves the partial order alignment problem for the given test cases.\n    \"\"\"\n    match_score = 2\n    mismatch_penalty = -2\n    gap_penalty = -3\n\n    def build_graph(nodes, edges):\n        num_nodes = len(nodes)\n        adj = {i: [] for i in range(num_nodes)}\n        pred = {i: [] for i in range(num_nodes)}\n        in_degree = {i: 0 for i in range(num_nodes)}\n        for u, v in edges:\n            adj[u].append(v)\n            pred[v].append(u)\n            in_degree[v] += 1\n        return {'nodes': nodes, 'adj': adj, 'pred': pred, 'in_degree': in_degree}\n\n    test_cases = [\n        {\n            \"nodes\": ['A', 'C', 'G', 'T', 'A', 'C', 'G'],\n            \"edges\": [(0, 1), (1, 2), (1, 3), (2, 4), (3, 4), (4, 5), (5, 6)],\n            \"read\": \"ACGACG\"\n        },\n        {\n            \"nodes\": ['A', 'C', 'G', 'A', 'C', 'G'],\n            \"edges\": [(0, 1), (1, 2), (2, 3), (3, 4), (2, 4), (4, 5)],\n            \"read\": \"ACGCG\"\n        },\n        {\n            \"nodes\": ['A', 'C', 'G', 'T', 'A', 'C', 'G'],\n            \"edges\": [(0, 1), (1, 2), (1, 3), (2, 4), (3, 4), (4, 5), (5, 6)],\n            \"read\": \"TTTTTT\"\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        graph = build_graph(case[\"nodes\"], case[\"edges\"])\n        score, effort = partial_order_alignment(graph, case[\"read\"], match_score, mismatch_penalty, gap_penalty)\n        results.extend([score, effort])\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}