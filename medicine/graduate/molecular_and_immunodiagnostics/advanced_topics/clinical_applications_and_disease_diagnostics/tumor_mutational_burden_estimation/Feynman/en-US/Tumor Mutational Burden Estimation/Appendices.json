{
    "hands_on_practices": [
        {
            "introduction": "The first step in mastering any quantitative biomarker is to understand its fundamental definition and calculation. This exercise grounds you in the core formula for Tumor Mutational Burden (TMB), treating it as a normalized density of mutations. By exploring how a change in the 'callable' genomic territory affects the TMB estimate, you will gain a critical appreciation for the importance of assay consistency and the sources of variability in TMB reporting .",
            "id": "5169530",
            "problem": "In a clinical molecular and immunodiagnostics workflow, Tumor Mutational Burden (TMB) is operationally defined as the count of somatic nonsynonymous mutations identified within a technically \"callable\" genomic territory, normalized per megabase (Mb) of that callable territory. The callable territory reflects regions where variant detection is reliable under the assay's performance characteristics. Assume a single tumor sample profiled by a targeted sequencing panel yields $N_{\\mathrm{somatic}}=120$ nonsynonymous somatic single-nucleotide variants, and the assay's callable region is $L_{\\mathrm{callable}}=1.2$ megabases. The assay's variant-calling pipeline, thresholds, and tumor purity adjustment are held fixed. \n\nStarting from foundational definitions of rate normalization (events per unit length) and treating TMB as inversely proportional to callable length when the numerator is unchanged, do the following:\n1. Compute the baseline TMB for this sample in mutations per megabase (mutations/Mb).\n2. Analyze the sensitivity of this estimate to a reduction in the callable region by $10\\%$, assuming $N_{\\mathrm{somatic}}$ is unchanged. Explicitly compute the new TMB after the reduction and the multiplicative factor by which the TMB changes relative to baseline.\n\nRound any reported TMB values to four significant figures and express them in mutations/Mb. For your final answer, provide only the multiplicative factor by which the TMB changes under the $10\\%$ reduction, expressed as a simplified exact fraction with no units.",
            "solution": "The problem statement is subjected to validation before proceeding to a solution.\n\n### Step 1: Extract Givens\n- Definition of Tumor Mutational Burden (TMB): the count of somatic nonsynonymous mutations identified within a technically \"callable\" genomic territory, normalized per megabase (Mb) of that callable territory.\n- Number of nonsynonymous somatic single-nucleotide variants: $N_{\\mathrm{somatic}} = 120$.\n- Size of the assay's callable region: $L_{\\mathrm{callable}} = 1.2$ megabases (Mb).\n- Assay conditions (variant-calling pipeline, thresholds, tumor purity adjustment) are held fixed.\n- Task 1: Compute the baseline TMB in mutations/Mb.\n- Task 2: Analyze the sensitivity to a reduction in the callable region by $10\\%$, assuming $N_{\\mathrm{somatic}}$ is unchanged. This includes computing the new TMB and the multiplicative factor of change.\n- Rounding instruction: Report TMB values to four significant figures.\n- Final answer format: The multiplicative factor as a simplified exact fraction.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is scientifically sound. The operational definition of TMB is standard practice in clinical genomics and oncology. The concepts of a sequencing panel, callable genomic territory, and rate normalization (mutations per megabase) are fundamental to the field. The provided numerical values are plausible for a targeted sequencing assay.\n- **Well-Posed**: The problem is well-posed. It provides a clear definition, all necessary numerical data ($N_{\\mathrm{somatic}}$, $L_{\\mathrm{callable}}$), and specific instructions for the required calculations. The objectives are clear and lead to a unique solution.\n- **Objective**: The problem is stated in precise, objective language, free of subjective or ambiguous terminology.\n- **Completeness and Consistency**: The problem is self-contained. The explicit assumption that $N_{\\mathrm{somatic}}$ remains unchanged during the sensitivity analysis is a standard simplification used to isolate the effect of a change in a single variable (in this case, $L_{\\mathrm{callable}}$). This does not represent a contradiction but rather a specified condition for the analysis.\n- **Other Flaws**: The problem does not exhibit any other flaws such as being unrealistic, ill-posed, trivial, or unverifiable.\n\n### Step 3: Verdict and Action\nThe problem is valid. A solution will be provided.\n\n### Solution Derivation\nThe Tumor Mutational Burden (TMB) is defined as the number of qualifying mutations, $N_{\\mathrm{somatic}}$, divided by the length of the callable genomic region, $L_{\\mathrm{callable}}$, expressed in megabases. The formula is:\n$$\n\\mathrm{TMB} = \\frac{N_{\\mathrm{somatic}}}{L_{\\mathrm{callable}}}\n$$\n\n**1. Baseline TMB Calculation**\n\nWe are given $N_{\\mathrm{somatic}} = 120$ and the baseline callable length $L_{\\mathrm{callable}} = 1.2 \\ \\mathrm{Mb}$. The baseline TMB, denoted as $\\mathrm{TMB}_{\\mathrm{baseline}}$, is calculated as:\n$$\n\\mathrm{TMB}_{\\mathrm{baseline}} = \\frac{120}{1.2} = 100 \\ \\text{mutations/Mb}\n$$\nFollowing the instruction to round to four significant figures, we express this as $100.0$ mutations/Mb.\n\n**2. Sensitivity Analysis**\n\nThe problem asks to analyze the effect of a $10\\%$ reduction in the callable region. Let the new callable length be $L'_{\\mathrm{callable}}$. This reduction of $10\\%$, or a factor of $0.10$, is calculated as:\n$$\nL'_{\\mathrm{callable}} = L_{\\mathrm{callable}} - (0.10 \\times L_{\\mathrm{callable}}) = (1 - 0.10) \\times L_{\\mathrm{callable}} = 0.90 \\times L_{\\mathrm{callable}}\n$$\nSubstituting the value of $L_{\\mathrm{callable}}$:\n$$\nL'_{\\mathrm{callable}} = 0.90 \\times 1.2 \\ \\mathrm{Mb} = 1.08 \\ \\mathrm{Mb}\n$$\nThe problem explicitly states to assume that $N_{\\mathrm{somatic}}$ is unchanged, so for this calculation, we still use $N_{\\mathrm{somatic}} = 120$. This assumption isolates the mathematical consequence of altering the denominator, which is central to understanding the stability of the TMB metric. The new TMB, denoted as $\\mathrm{TMB}_{\\mathrm{new}}$, is:\n$$\n\\mathrm{TMB}_{\\mathrm{new}} = \\frac{120}{1.08} = \\frac{12000}{108} = \\frac{1000}{9} \\approx 111.111... \\ \\text{mutations/Mb}\n$$\nRounding to four significant figures as requested, the new TMB is $111.1$ mutations/Mb.\n\n**3. Multiplicative Factor Calculation**\n\nThe multiplicative factor, $F$, by which the TMB changes is the ratio of the new TMB to the baseline TMB:\n$$\nF = \\frac{\\mathrm{TMB}_{\\mathrm{new}}}{\\mathrm{TMB}_{\\mathrm{baseline}}}\n$$\nWe can substitute the symbolic expressions to derive a general relationship:\n$$\nF = \\frac{\\left(\\frac{N_{\\mathrm{somatic}}}{L'_{\\mathrm{callable}}}\\right)}{\\left(\\frac{N_{\\mathrm{somatic}}}{L_{\\mathrm{callable}}}\\right)} = \\frac{\\left(\\frac{N_{\\mathrm{somatic}}}{0.90 \\times L_{\\mathrm{callable}}}\\right)}{\\left(\\frac{N_{\\mathrm{somatic}}}{L_{\\mathrm{callable}}}\\right)}\n$$\nThe terms $N_{\\mathrm{somatic}}$ and $L_{\\mathrm{callable}}$ cancel out, leaving:\n$$\nF = \\frac{1}{0.90}\n$$\nThis result demonstrates the inverse proportionality between TMB and the callable length, as stated in the problem. To express this factor as a simplified exact fraction, we convert the decimal to a fraction:\n$$\n0.90 = \\frac{90}{100} = \\frac{9}{10}\n$$\nTherefore, the multiplicative factor is:\n$$\nF = \\frac{1}{\\frac{9}{10}} = \\frac{10}{9}\n$$\nThis is the final required answer.",
            "answer": "$$\\boxed{\\frac{10}{9}}$$"
        },
        {
            "introduction": "A calculated TMB value is a point estimate, but the true mutation rate in a tumor is an unknown parameter that we can only infer. This practice introduces statistical rigor by modeling mutation counts as a Poisson process, allowing you to quantify the uncertainty of your TMB estimate by calculating a confidence interval. Furthermore, you will derive from first principles how increasing the size of the sequencing panel drastically improves the precision of the estimate, highlighting a key trade-off in assay design .",
            "id": "5169510",
            "problem": "A tumor-only targeted sequencing assay interrogates a contiguous coding region of length $0.5$ megabases (Mb). After stringent somatic filtering, the laboratory observes $N_{\\mathrm{somatic}} = 15$ nonsynonymous somatic single-nucleotide variants in the tumor. Tumor Mutational Burden (TMB) is defined as the somatic mutation rate per megabase, that is, the expected number of somatic mutations per Mb. Assume that somatic mutation events arise as a Poisson process with a constant rate per Mb across the assayed territory.\n\nTasks:\n1. Using the definition of TMB as a mutation density, compute the point estimate of TMB from these data. Express the result in mutations per megabase and round to three significant figures.\n2. Under the Poisson model, construct an exact central $95\\%$ confidence interval for the unknown Poisson mean over the assayed length and convert it to a confidence interval for TMB (mutations per megabase). Express the interval endpoints in mutations per megabase and round each to three significant figures.\n3. Consider an exome-scale assay, for example Whole-Exome Sequencing (WES), that yields a callable coding territory of $30$ Mb on the same tumor. Under the same Poisson process with the same underlying per-megabase mutation rate as in this sample, derive—starting from the variance properties of the Poisson distribution and without using any pre-memorized TMB scaling formulas—the expected fold-reduction in the per-megabase $95\\%$ confidence interval half-width when moving from the $0.5$ Mb panel to the $30$ Mb exome-scale assay. For this derivation, you may use the asymptotic normal approximation to the Poisson distribution for large counts. Report this fold-reduction as a dimensionless number rounded to four significant figures.\n\nYour final reported answer must be only the fold-reduction value from Task $3$.",
            "solution": "The problem has been validated and is deemed sound, well-posed, and scientifically grounded. The tasks are sequential and logically connected, concerning the estimation of Tumor Mutational Burden (TMB) from sequencing data. We will address each task in order.\n\nLet $\\lambda$ be the true TMB, defined as the mean number of somatic mutations per megabase (Mb). The somatic mutation process is modeled as a Poisson process. For a sequenced region of length $L$ in megabases, the number of observed somatic mutations, $N$, follows a Poisson distribution with mean $\\mu = \\lambda L$. That is, $N \\sim \\text{Poisson}(\\lambda L)$.\n\nThe problem provides the following data:\n-   Length of the targeted sequencing panel: $L_1 = 0.5$ Mb.\n-   Observed number of nonsynonymous somatic mutations: $N_1 = 15$.\n-   Confidence level for interval estimation: $95\\%$, which corresponds to $\\alpha = 0.05$.\n-   Length of the whole-exome sequencing (WES) territory: $L_2 = 30$ Mb.\n\n**Task 1: Point Estimate of TMB**\n\nThe parameter of interest is the mutation rate $\\lambda$. The observed data is $N_1 = 15$ mutations in a region of length $L_1 = 0.5$ Mb. The expected number of mutations in this region is $\\mu_1 = \\lambda L_1$.\nThe maximum likelihood estimator (MLE) for the mean of a Poisson distribution is the observed count. Therefore, the point estimate for $\\mu_1$ is $\\hat{\\mu}_1 = N_1 = 15$.\n\nFrom the relationship $\\mu_1 = \\lambda L_1$, we can derive the point estimate for $\\lambda$, denoted $\\hat{\\lambda}$:\n$$\n\\hat{\\lambda} = \\frac{\\hat{\\mu}_1}{L_1} = \\frac{N_1}{L_1}\n$$\nSubstituting the given values:\n$$\n\\hat{\\lambda} = \\frac{15}{0.5} = 30 \\text{ mutations/Mb}\n$$\nThe problem asks for this result to be rounded to three significant figures. The point estimate of TMB is $30.0$ mutations/Mb.\n\n**Task 2: 95% Confidence Interval for TMB**\n\nTo find the confidence interval for TMB ($\\lambda$), we first construct an exact $95\\%$ confidence interval for the Poisson mean $\\mu_1$ based on the observation $N_1 = 15$. The standard exact method, known as the Clopper-Pearson interval, is based on the relationship between the cumulative distribution function (CDF) of the Poisson distribution and the CDF of the chi-squared ($\\chi^2$) distribution.\n\nFor an observed count of $k$, the exact $100(1-\\alpha)\\%$ confidence interval for the mean $\\mu$ is given by $[\\mu_L, \\mu_U]$, where:\n$$\n\\mu_L = \\frac{1}{2} \\chi^2_{\\alpha/2, 2k} \\quad \\text{and} \\quad \\mu_U = \\frac{1}{2} \\chi^2_{1-\\alpha/2, 2(k+1)}\n$$\nHere, $\\chi^2_{p, \\nu}$ is the $p$-th percentile of the chi-squared distribution with $\\nu$ degrees of freedom.\n\nIn our case, $k = N_1 = 15$ and $\\alpha = 0.05$. Thus, $\\alpha/2 = 0.025$ and $1-\\alpha/2 = 0.975$.\nThe degrees of freedom for the lower bound are $2k = 2(15) = 30$.\nThe degrees of freedom for the upper bound are $2(k+1) = 2(15+1) = 32$.\n\nThe interval for $\\mu_1$ is:\n$$\n\\mu_{1,L} = \\frac{1}{2} \\chi^2_{0.025, 30}\n$$\n$$\n\\mu_{1,U} = \\frac{1}{2} \\chi^2_{0.975, 32}\n$$\nUsing standard statistical tables or software for the chi-squared critical values:\n$$\n\\chi^2_{0.025, 30} \\approx 16.791\n$$\n$$\n\\chi^2_{0.975, 32} \\approx 49.480\n$$\nSubstituting these values to find the interval for $\\mu_1$:\n$$\n\\mu_{1,L} \\approx \\frac{1}{2}(16.791) = 8.3955\n$$\n$$\n\\mu_{1,U} \\approx \\frac{1}{2}(49.480) = 24.740\n$$\nSo, the $95\\%$ confidence interval for the expected number of mutations in the $0.5$ Mb region is approximately $[8.3955, 24.740]$.\n\nTo obtain the confidence interval for the TMB, $\\lambda = \\mu_1/L_1$, we divide the endpoints of the interval for $\\mu_1$ by the region length $L_1 = 0.5$ Mb:\n$$\n\\text{CI}_{\\lambda} = \\left[ \\frac{\\mu_{1,L}}{L_1}, \\frac{\\mu_{1,U}}{L_1} \\right] = \\left[ \\frac{8.3955}{0.5}, \\frac{24.740}{0.5} \\right] = [16.791, 49.480]\n$$\nRounding each endpoint to three significant figures, the $95\\%$ confidence interval for TMB is $[16.8, 49.5]$ mutations/Mb.\n\n**Task 3: Fold-Reduction in Confidence Interval Half-Width**\n\nThis task requires deriving the expected fold-reduction in the CI half-width when moving from the panel of length $L_1 = 0.5$ Mb to an exome-scale assay of length $L_2 = 30$ Mb. We are instructed to use the asymptotic normal approximation to the Poisson distribution.\n\nFor a Poisson random variable $N$ with a large mean $\\mu$, its distribution can be approximated by a normal distribution, $N \\approx \\mathcal{N}(\\mu, \\sigma^2=\\mu)$, as the variance of a Poisson distribution is equal to its mean.\nBased on this approximation, a $100(1-\\alpha)\\%$ confidence interval for the mean $\\mu$, given an observation $N=k$, is:\n$$\nk \\pm z_{1-\\alpha/2} \\sqrt{k}\n$$\nwhere $z_{1-\\alpha/2}$ is the standard normal quantile. For a $95\\%$ CI, $\\alpha=0.05$ and $z_{0.975} \\approx 1.96$.\nThe half-width of this confidence interval for the count $\\mu$ is $W_{\\mu} = z_{1-\\alpha/2}\\sqrt{k}$.\n\nTo find the confidence interval for the TMB, $\\lambda = \\mu/L$, we divide the interval for $\\mu$ by the length $L$:\n$$\n\\hat{\\lambda} \\pm \\frac{W_{\\mu}}{L} = \\frac{k}{L} \\pm \\frac{z_{1-\\alpha/2}\\sqrt{k}}{L}\n$$\nThe half-width of the confidence interval for TMB, $W_{\\lambda}$, is therefore:\n$$\nW_{\\lambda} = \\frac{z_{1-\\alpha/2}\\sqrt{k}}{L}\n$$\nThe problem asks for the *expected* fold-reduction. We should therefore consider the expected half-width by replacing the observed count $k$ with its expected value, $E[N] = \\mu = \\lambda L$.\n$$\nE[W_{\\lambda}] = \\frac{z_{1-\\alpha/2}\\sqrt{E[N]}}{L} = \\frac{z_{1-\\alpha/2}\\sqrt{\\lambda L}}{L} = z_{1-\\alpha/2}\\frac{\\sqrt{\\lambda}}{\\sqrt{L}}\n$$\nThis derivation, which starts from the variance property $\\text{Var}(N)=\\mu$, shows that the expected half-width of the TMB-CI is inversely proportional to the square root of the interrogated length $L$.\n\nLet $W_1$ and $W_2$ be the expected half-widths for the assays of length $L_1$ and $L_2$, respectively.\n$$\nW_1 = z_{1-\\alpha/2}\\frac{\\sqrt{\\lambda}}{\\sqrt{L_1}}\n$$\n$$\nW_2 = z_{1-\\alpha/2}\\frac{\\sqrt{\\lambda}}{\\sqrt{L_2}}\n$$\nThe fold-reduction in the half-width is the ratio $W_1 / W_2$:\n$$\n\\text{Fold-reduction} = \\frac{W_1}{W_2} = \\frac{z_{1-\\alpha/2}\\frac{\\sqrt{\\lambda}}{\\sqrt{L_1}}}{z_{1-\\alpha/2}\\frac{\\sqrt{\\lambda}}{\\sqrt{L_2}}} = \\frac{1/\\sqrt{L_1}}{1/\\sqrt{L_2}} = \\frac{\\sqrt{L_2}}{\\sqrt{L_1}} = \\sqrt{\\frac{L_2}{L_1}}\n$$\nNote that this result is independent of the true TMB $\\lambda$ and the confidence level.\n\nNow, we substitute the given lengths $L_1 = 0.5$ Mb and $L_2 = 30$ Mb:\n$$\n\\text{Fold-reduction} = \\sqrt{\\frac{30}{0.5}} = \\sqrt{60}\n$$\nCalculating the numerical value:\n$$\n\\sqrt{60} = \\sqrt{4 \\times 15} = 2\\sqrt{15} \\approx 7.74596669...\n$$\nRounding to four significant figures as requested, the expected fold-reduction is $7.746$. This means we expect the CI for TMB from the $30$ Mb exome to be approximately $7.746$ times narrower than the CI from the $0.5$ Mb panel.",
            "answer": "$$\n\\boxed{7.746}\n$$"
        },
        {
            "introduction": "The accuracy of a TMB estimate depends entirely on the quality of the input: the list of somatic mutations. This advanced, hands-on exercise simulates a real-world bioinformatics challenge by tasking you with designing a filter to remove specific sequencing artifacts ($8$-oxoguanine) that can masquerade as true mutations. By implementing a Bayesian classifier, you will learn how to integrate multiple lines of evidence—such as read orientation and base quality—to refine the mutation list and produce a more reliable TMB score .",
            "id": "5169533",
            "problem": "You are tasked with formalizing and implementing a statistically principled combined filter to remove guanine-to-thymine ($\\text{G}\\rightarrow\\text{T}$) artifacts attributed to $8$-oxoguanine (OxoG) while preserving true somatic variants, and then validating the effect of this filter on Tumor Mutational Burden (TMB) estimation. Start from fundamental, widely accepted facts: Next-Generation Sequencing (NGS) produces base calls with associated Phred quality scores, higher-quality bases have lower error probabilities; OxoG-related artifacts are enriched in $\\text{G}\\rightarrow\\text{T}$ substitutions, exhibit read orientation imbalance, occur more frequently in cytosine-phosphate-guanine (CpG) context, and tend to have lower average base quality; TMB is the count of retained true somatic single-nucleotide variants per megabase.\n\nDefine two classes for a variant call: artifact ($Y=0$) and true ($Y=1$). For each call, the observable features are read orientation imbalance $x_o$, CpG context indicator $x_c \\in \\{0,1\\}$, and normalized mean base quality $x_q \\in [0,1]$. The read orientation imbalance is computed from alternate-supporting forward and reverse read counts, $f$ and $r$, as\n$$\nx_o = \\left|\\frac{f}{f+r} - \\frac{1}{2}\\right|,\\quad z_o = 2x_o \\in [0,1].\n$$\nThe normalized mean base quality is\n$$\nx_q = \\frac{\\min(Q,60)}{60},\n$$\nwhere $Q$ is the mean Phred quality of the variant-supporting bases. A combined filter must classify each $\\text{G}\\rightarrow\\text{T}$ call using $x_o$, $x_c$, and $x_q$ via a decision rule grounded in Bayes' theorem and independence of features conditioned on class. Use class-dependent priors for $\\text{G}\\rightarrow\\text{T}$ and non-$\\text{G}\\rightarrow\\text{T}$ calls. Only $\\text{G}\\rightarrow\\text{T}$ calls should be removed when classified as artifacts; other substitution types are not subject to OxoG-specific removal.\n\nAssume the following class-conditional models for the features:\n- For artifacts ($Y=0$): $z_o \\sim \\mathrm{Beta}(\\alpha_0,\\beta_0)$, $x_q \\sim \\mathcal{N}(\\mu_0,\\sigma_0^2)$ truncated to $[0,1]$, $x_c \\sim \\mathrm{Bernoulli}(p_{c0})$.\n- For true ($Y=1$): $z_o \\sim \\mathrm{Beta}(\\alpha_1,\\beta_1)$, $x_q \\sim \\mathcal{N}(\\mu_1,\\sigma_1^2)$ truncated to $[0,1]$, $x_c \\sim \\mathrm{Bernoulli}(p_{c1})$.\n\nUse the following calibration constants for the classifier:\n- $\\alpha_0 = 6$, $\\beta_0 = 1$; $\\alpha_1 = 1$, $\\beta_1 = 6$.\n- $\\mu_0 = 0.50$, $\\sigma_0 = 0.10$; $\\mu_1 = 0.72$, $\\sigma_1 = 0.08$.\n- $p_{c0} = 0.65$, $p_{c1} = 0.35$.\n- Priors for $\\text{G}\\rightarrow\\text{T}$ calls: $\\pi_0^{\\mathrm{GT}} = 0.40$, $\\pi_1^{\\mathrm{GT}} = 0.60$.\n- Priors for non-$\\text{G}\\rightarrow\\text{T}$ calls: $\\pi_0^{\\mathrm{other}} = 0.05$, $\\pi_1^{\\mathrm{other}} = 0.95$.\n\nUsing equal misclassification costs, derive a Bayes-optimal decision rule that classifies a call as artifact if the posterior probability of $Y=0$ exceeds that of $Y=1$ given $(x_o,x_c,x_q)$ under the independence model. Apply removal to a call if and only if it is a $\\text{G}\\rightarrow\\text{T}$ call and classified as artifact.\n\nCompute, for each test case described below:\n1. TMB in mutations per megabase, defined as $$\\mathrm{TMB}=\\frac{\\text{number of retained true somatic variants}}{\\text{callable region size in megabases}},$$ expressed as a float in units of mutations per megabase.\n2. True positive retention fraction, defined as $$\\frac{\\text{number of retained true somatic variants}}{\\text{number of true somatic variants present}},$$ expressed as a decimal.\n3. False positive removal fraction, defined as $$\\frac{\\text{number of removed artifacts}}{\\text{number of artifacts present}},$$ expressed as a decimal. If the denominator is zero for the true positive retention fraction, define the fraction as $1.0$. If the denominator is zero for the false positive removal fraction, define the fraction as $0.0$.\n\nYour program should produce a single line of output containing the results aggregated across test cases as a comma-separated list enclosed in square brackets, ordered as $[\\mathrm{TMB}_1,\\mathrm{Ret}_1,\\mathrm{FPRem}_1,\\mathrm{TMB}_2,\\mathrm{Ret}_2,\\mathrm{FPRem}_2,\\mathrm{TMB}_3,\\mathrm{Ret}_3,\\mathrm{FPRem}_3]$.\n\nTest suite specification:\n\nTest case $1$ (happy path; mixed $\\text{G}\\rightarrow\\text{T}$ artifacts and true variants), callable region size $= 1.5$ megabases. Calls are given as tuples: $(\\text{substitution}, y, f, r, x_c, Q)$.\n- $(\\text{G}\\rightarrow\\text{T}, 0, 15, 1, 1, 27)$\n- $(\\text{G}\\rightarrow\\text{T}, 1, 12, 13, 0, 38)$\n- $(\\text{C}\\rightarrow\\text{T}, 1, 8, 7, 0, 35)$\n- $(\\text{G}\\rightarrow\\text{T}, 0, 9, 3, 1, 30)$\n- $(\\text{G}\\rightarrow\\text{T}, 1, 11, 10, 0, 33)$\n- $(\\text{A}\\rightarrow\\text{G}, 1, 6, 5, 0, 35)$\n- $(\\text{G}\\rightarrow\\text{T}, 0, 20, 0, 1, 25)$\n- $(\\text{G}\\rightarrow\\text{T}, 1, 13, 13, 0, 40)$\n- $(\\text{T}\\rightarrow\\text{C}, 1, 7, 8, 0, 37)$\n- $(\\text{G}\\rightarrow\\text{T}, 0, 10, 2, 1, 29)$\n\nTest case $2$ (boundary; predominantly extreme-orientation artifacts), callable region size $= 0.5$ megabases.\n- $(\\text{G}\\rightarrow\\text{T}, 0, 30, 0, 1, 26)$\n- $(\\text{G}\\rightarrow\\text{T}, 0, 18, 0, 1, 28)$\n- $(\\text{G}\\rightarrow\\text{T}, 0, 25, 0, 1, 24)$\n- $(\\text{C}\\rightarrow\\text{A}, 1, 9, 9, 0, 39)$\n\nTest case $3$ (edge; all $\\text{G}\\rightarrow\\text{T}$ calls are true and high-quality), callable region size $= 0.8$ megabases.\n- $(\\text{G}\\rightarrow\\text{T}, 1, 21, 20, 0, 42)$\n- $(\\text{G}\\rightarrow\\text{T}, 1, 10, 11, 0, 36)$\n- $(\\text{G}\\rightarrow\\text{T}, 1, 17, 18, 1, 41)$\n- $(\\text{A}\\rightarrow\\text{T}, 1, 8, 8, 0, 34)$\n\nImplement the classifier and compute the specified metrics for each test case. Express TMB in mutations per megabase as floats and the fractions as decimals. The final program must produce the single-line aggregate output in the specified format.",
            "solution": "The problem is valid. It presents a clear, scientifically grounded, and well-posed statistical classification task within the domain of genomic data analysis. All necessary data, models, and parameters are provided, and the task is to implement a Bayesian classifier and evaluate its performance on provided test cases.\n\nThe core of the problem is to construct a Bayes-optimal classifier for distinguishing true somatic variants ($Y=1$) from sequencing artifacts ($Y=0$) for guanine-to-thymine ($\\text{G}\\rightarrow\\text{T}$) substitutions. This classifier will be based on a vector of observable features $\\vec{x} = (x_o, x_c, x_q)$.\n\nFirst, we formalize the decision rule. With equal costs for misclassification, the Bayes-optimal rule is to select the class with the highest posterior probability. A variant is classified as an artifact if:\n$$P(Y=0 | \\vec{x}) > P(Y=1 | \\vec{x})$$\nUsing Bayes' theorem, $P(Y=k | \\vec{x}) = \\frac{P(\\vec{x} | Y=k) P(Y=k)}{P(\\vec{x})}$, where $P(\\vec{x} | Y=k)$ is the class-conditional likelihood and $P(Y=k)$ is the class prior. The decision rule simplifies to comparing the products of likelihood and prior, as the evidence $P(\\vec{x})$ is a common denominator:\n$$P(\\vec{x} | Y=0) P(Y=0) > P(\\vec{x} | Y=1) P(Y=1)$$\nThe problem states that the features $(x_o, x_c, x_q)$ are independent conditioned on the class $Y$. Therefore, the likelihood function for class $k$ factorizes:\n$$P(\\vec{x} | Y=k) = P(x_o | Y=k) P(x_c | Y=k) P(x_q | Y=k)$$\nThe problem defines the feature for the orientation bias model as $z_o = 2x_o = 2|\\frac{f}{f+r} - \\frac{1}{2}|$. The class-conditional probability density and mass functions for the features are given as:\n- $z_o | Y=k \\sim \\mathrm{Beta}(\\alpha_k, \\beta_k)$, with PDF $f_{Z_o,k}(z_o)$.\n- $x_c | Y=k \\sim \\mathrm{Bernoulli}(p_{ck})$, with PMF $p_{X_c,k}(x_c)$.\n- $x_q | Y=k \\sim \\mathcal{N}(\\mu_k, \\sigma_k^2)$ truncated to $[0,1]$, with PDF $f_{X_q,k}(x_q)$.\n\nThe priors, $\\pi_k = P(Y=k)$, are specified for $\\text{G}\\rightarrow\\text{T}$ calls: $\\pi_0^{\\mathrm{GT}} = 0.40$ and $\\pi_1^{\\mathrm{GT}} = 0.60$.\nThe decision rule for a $\\text{G}\\rightarrow\\text{T}$ call is to classify it as an artifact if:\n$$f_{Z_o,0}(z_o) \\cdot p_{X_c,0}(x_c) \\cdot f_{X_q,0}(x_q) \\cdot \\pi_0^{\\mathrm{GT}} > f_{Z_o,1}(z_o) \\cdot p_{X_c,1}(x_c) \\cdot f_{X_q,1}(x_q) \\cdot \\pi_1^{\\mathrm{GT}}$$\nTo avoid numerical underflow, we will work with log-probabilities. The decision rule becomes: classify as artifact if $\\log(S_0) > \\log(S_1)$, where $S_k$ is the score for class $k$:\n$$\\log(S_k) = \\log(f_{Z_o,k}(z_o)) + \\log(p_{X_c,k}(x_c)) + \\log(f_{X_q,k}(x_q)) + \\log(\\pi_k^{\\mathrm{GT}})$$\nThe individual log-probability terms are:\n1.  **Beta distribution for $z_o$**: The log-PDF is\n    $$\\log(f_{Z_o,k}(z_o)) = (\\alpha_k-1)\\log(z_o) + (\\beta_k-1)\\log(1-z_o) - B(\\alpha_k, \\beta_k)$$\n    where $B(\\alpha_k, \\beta_k)$ is the Beta function. This is evaluated using standard library functions.\n2.  **Bernoulli distribution for $x_c$**: The log-PMF is\n    $$\\log(p_{X_c,k}(x_c)) = x_c \\log(p_{ck}) + (1-x_c)\\log(1-p_{ck})$$\n3.  **Truncated Normal distribution for $x_q$**: The PDF for a normal distribution $\\mathcal{N}(\\mu, \\sigma^2)$ truncated to an interval $[a, b]$ is\n    $$f(x) = \\frac{\\frac{1}{\\sigma}\\phi(\\frac{x-\\mu}{\\sigma})}{\\Phi(\\frac{b-\\mu}{\\sigma}) - \\Phi(\\frac{a-\\mu}{\\sigma})}$$\n    where $\\phi$ is the standard normal PDF and $\\Phi$ is the standard normal CDF. For our case, $a=0$ and $b=1$. The log-PDF is:\n    $$\\log(f_{X_q,k}(x_q)) = \\log\\left(\\frac{1}{\\sigma_k}\\phi\\left(\\frac{x_q-\\mu_k}{\\sigma_k}\\right)\\right) - \\log\\left(\\Phi\\left(\\frac{1-\\mu_k}{\\sigma_k}\\right) - \\Phi\\left(\\frac{-\\mu_k}{\\sigma_k}\\right)\\right)$$\n\nThe algorithm proceeds as follows for each test case:\n1.  Initialize counts: number of true variants, number of artifacts, number of retained true variants, and number of removed artifacts.\n2.  Iterate through each variant call in the test set.\n    -   A variant call is defined by its substitution type, ground truth class ($y$), read counts ($f, r$), CpG context ($x_c$), and mean base quality ($Q$).\n    -   Update the total counts of true variants and artifacts based on the ground truth $y$.\n3.  For each call, determine if it is retained or removed.\n    -   If the substitution type is not $\\text{G}\\rightarrow\\text{T}$, the variant is automatically retained.\n    -   If it is a $\\text{G}\\rightarrow\\text{T}$ variant, the classifier is applied:\n        a. Calculate features: $z_o = 2|\\frac{f}{f+r} - \\frac{1}{2}|$ and $x_q = \\frac{\\min(Q,60)}{60}$.\n        b. Calculate the log-scores $\\log(S_0)$ and $\\log(S_1)$ using the given parameters:\n           - Class 0 (Artifact): $\\alpha_0=6, \\beta_0=1, \\mu_0=0.5, \\sigma_0=0.1, p_{c0}=0.65, \\pi_0^{\\mathrm{GT}}=0.4$.\n           - Class 1 (True): $\\alpha_1=1, \\beta_1=6, \\mu_1=0.72, \\sigma_1=0.08, p_{c1}=0.35, \\pi_1^{\\mathrm{GT}}=0.6$.\n        c. If $\\log(S_0) > \\log(S_1)$, the variant is classified as an artifact and is removed. Otherwise, it is retained.\n4. Update the counters for `retained_true` and `removed_artifacts` based on the classification outcome and the ground truth.\n5. After processing all calls, calculate the final metrics:\n   -   $\\mathrm{TMB} = \\frac{\\text{number of retained true variants}}{\\text{callable region size in megabases}}$\n   -   True Positive Retention Fraction ($\\mathrm{Ret}$) = $\\frac{\\text{number of retained true variants}}{\\text{number of true variants present}}$. If the denominator is $0$, the value is $1.0$.\n   -   False Positive Removal Fraction ($\\mathrm{FPRem}$) = $\\frac{\\text{number of removed artifacts}}{\\text{number of artifacts present}}$. If the denominator is $0$, the value is $0.0$.\nThe results from each test case are then aggregated into the specified output format.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import beta, norm\nfrom scipy.special import betaln\n\ndef solve():\n    \"\"\"\n    Solves the Tumor Mutational Burden estimation problem by implementing and\n    applying a Bayesian classifier for sequencing artifacts.\n    \"\"\"\n\n    # --- Calibration Constants ---\n    # Class 0: Artifact\n    ALPHA0, BETA0 = 6, 1\n    MU0, SIGMA0 = 0.50, 0.10\n    P_C0 = 0.65\n    PI_0_GT = 0.40\n\n    # Class 1: True Variant\n    ALPHA1, BETA1 = 1, 6\n    MU1, SIGMA1 = 0.72, 0.08\n    P_C1 = 0.35\n    PI_1_GT = 0.60\n    \n    # Log priors for G->T calls\n    LOG_PI_0_GT = np.log(PI_0_GT)\n    LOG_PI_1_GT = np.log(PI_1_GT)\n    \n    # Truncated Normal Denominators (normalization constants)\n    # For class 0\n    Z_0 = norm.cdf((1 - MU0) / SIGMA0) - norm.cdf((0 - MU0) / SIGMA0)\n    LOG_Z_0 = np.log(Z_0)\n    \n    # For class 1\n    Z_1 = norm.cdf((1 - MU1) / SIGMA1) - norm.cdf((0 - MU1) / SIGMA1)\n    LOG_Z_1 = np.log(Z_1)\n    \n\n    test_cases = [\n        {\n            \"callable_region_size\": 1.5,\n            \"calls\": [\n                (\"G->T\", 0, 15, 1, 1, 27), (\"G->T\", 1, 12, 13, 0, 38), (\"C->T\", 1, 8, 7, 0, 35),\n                (\"G->T\", 0, 9, 3, 1, 30), (\"G->T\", 1, 11, 10, 0, 33), (\"A->G\", 1, 6, 5, 0, 35),\n                (\"G->T\", 0, 20, 0, 1, 25), (\"G->T\", 1, 13, 13, 0, 40), (\"T->C\", 1, 7, 8, 0, 37),\n                (\"G->T\", 0, 10, 2, 1, 29)\n            ]\n        },\n        {\n            \"callable_region_size\": 0.5,\n            \"calls\": [\n                (\"G->T\", 0, 30, 0, 1, 26), (\"G->T\", 0, 18, 0, 1, 28),\n                (\"G->T\", 0, 25, 0, 1, 24), (\"C->A\", 1, 9, 9, 0, 39)\n            ]\n        },\n        {\n            \"callable_region_size\": 0.8,\n            \"calls\": [\n                (\"G->T\", 1, 21, 20, 0, 42), (\"G->T\", 1, 10, 11, 0, 36),\n                (\"G->T\", 1, 17, 18, 1, 41), (\"A->T\", 1, 8, 8, 0, 34)\n            ]\n        }\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        callable_size = case[\"callable_region_size\"]\n        \n        total_true_variants = 0\n        total_artifacts = 0\n        retained_true_variants = 0\n        removed_artifacts = 0\n\n        for call in case[\"calls\"]:\n            sub, y, f, r, x_c, Q = call\n            \n            if y == 1:\n                total_true_variants += 1\n            else: # y == 0\n                total_artifacts += 1\n\n            is_removed = False\n            if sub == \"G->T\":\n                # 1. Calculate features\n                if f + r == 0:\n                    # Avoid division by zero, though not present in test data.\n                    # This state is uninformative for orientation bias. Let's assume neutral.\n                    z_o = 0.0\n                else:\n                    x_o = abs(f / (f + r) - 0.5)\n                    z_o = 2 * x_o\n                \n                x_q = min(Q, 60) / 60.0\n\n                # 2. Calculate log-scores for each class\n                # Class 0 (Artifact)\n                logp_zo_0 = beta.logpdf(z_o, a=ALPHA0, b=BETA0)\n                logp_xc_0 = np.log(P_C0) if x_c == 1 else np.log(1 - P_C0)\n                logp_xq_0 = norm.logpdf(x_q, loc=MU0, scale=SIGMA0) - LOG_Z_0\n                log_s0 = logp_zo_0 + logp_xc_0 + logp_xq_0 + LOG_PI_0_GT\n\n                # Class 1 (True)\n                logp_zo_1 = beta.logpdf(z_o, a=ALPHA1, b=BETA1)\n                logp_xc_1 = np.log(P_C1) if x_c == 1 else np.log(1 - P_C1)\n                logp_xq_1 = norm.logpdf(x_q, loc=MU1, scale=SIGMA1) - LOG_Z_1\n                log_s1 = logp_zo_1 + logp_xc_1 + logp_xq_1 + LOG_PI_1_GT\n                \n                # 3. Decision rule\n                if log_s0 > log_s1:\n                    is_removed = True\n            \n            # Update metrics based on decision\n            if is_removed:\n                if y == 0:\n                    removed_artifacts += 1\n            else: # is_retained\n                if y == 1:\n                    retained_true_variants += 1\n        \n        # Calculate final metrics for the test case\n        tmb = retained_true_variants / callable_size if callable_size > 0 else 0.0\n        \n        if total_true_variants > 0:\n            retention_fraction = retained_true_variants / total_true_variants\n        else:\n            retention_fraction = 1.0\n            \n        if total_artifacts > 0:\n            fp_removal_fraction = removed_artifacts / total_artifacts\n        else:\n            fp_removal_fraction = 0.0\n            \n        all_results.extend([tmb, retention_fraction, fp_removal_fraction])\n\n    # Format the final output string\n    formatted_results = [f\"{x:.1f}\" for x in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}