{
    "hands_on_practices": [
        {
            "introduction": "Quantitative PCR (qPCR) is a cornerstone of molecular diagnostics, but a true mastery of the technique requires understanding its kinetics from first principles. This exercise challenges you to move beyond the black-box application of qPCR and derive the exponential amplification law directly from the underlying molecular determinants of primer binding and polymerase activity. By building this model, you will gain a deeper appreciation for how parameters like primer affinity and enzyme efficiency dictate the resulting threshold cycle ($C_t$) and overall assay performance. ",
            "id": "5134112",
            "problem": "A quantitative Polymerase Chain Reaction (qPCR) assay is used to detect a low-abundance nucleic acid biomarker whose molecular determinant for primer binding is the dissociation constant, defined as $K_d = \\frac{[T][P]}{[TP]}$, where $[T]$ is the concentration of free template, $[P]$ is the concentration of free primer, and $[TP]$ is the concentration of the primer-template complex. Assume a single amplicon and the following mechanistic base:\n- The per-cycle steps are denaturation, primer annealing, and extension.\n- Under pseudo-first-order conditions for annealing ($[P]_{\\text{tot}} \\gg [T]_{\\text{tot}}$), the fraction of templates annealed at the end of the annealing stage is given by the equilibrium occupancy $f_{\\text{bind}} = \\frac{[P]}{K_d + [P]}$, with $[P] \\approx [P]_{\\text{tot}}$.\n- Extension of an annealed template to a full-length product requires a time $L/k_{\\text{pol}}$, where $L$ is the amplicon length and $k_{\\text{pol}}$ is the polymerase elongation rate. If the extension time per cycle is $\\tau$, then the extension success fraction is $f_{\\text{ext}} = 1$ whenever $\\tau \\ge \\frac{L}{k_{\\text{pol}}}$ and $f_{\\text{ext}} = 0$ otherwise.\n- Let $p = f_{\\text{bind}} f_{\\text{ext}}$ denote the per-cycle probability that a given existing double-stranded molecule yields one additional double-stranded copy by the end of the cycle. Assume independence across templates and cycles so that the expectation evolves deterministically.\n\nStarting from the recursion implied by the above mechanistic base and without assuming any specific form of the solution a priori, derive the closed-form expression for the expected number of template molecules after $t$ cycles in terms of the initial copy number $N_0$ and the per-cycle success probability $p$. Then, for an assay configured with $N_0 = 300$ initial molecules, $[P]_{\\text{tot}} = 250\\,\\text{nM}$, $K_d = 50\\,\\text{nM}$, $L = 120\\,\\text{nt}$, $k_{\\text{pol}} = 80\\,\\text{nt}\\,\\text{s}^{-1}$, and $\\tau = 30\\,\\text{s}$, compute the real-valued cycle count $t^{\\ast}$ at which the expected copy number first equals the fluorescence threshold corresponding to $N_{\\text{th}} = 1.0 \\times 10^9$ molecules. Report $t^{\\ast}$ rounded to four significant figures and express it in cycles. Do not round $t^{\\ast}$ to an integer.",
            "solution": "The problem is found to be scientifically grounded, well-posed, and internally consistent. All necessary parameters and definitions are provided to derive a unique and meaningful solution. The underlying model of polymerase chain reaction (qPCR) is a standard, albeit simplified, representation of the process. Therefore, a full solution can be derived.\n\nThe first step is to derive the closed-form expression for the expected number of template molecules, $N(t)$, after $t$ cycles. Let $N(t-1)$ be the expected number of double-stranded DNA molecules at the start of cycle $t$ (or, equivalently, at the end of cycle $t-1$). According to the problem statement, $p$ is the per-cycle probability that a given molecule yields one additional copy. Assuming independence across all templates, the expected number of new molecules generated during cycle $t$ is the product of the number of molecules present at the start of the cycle, $N(t-1)$, and the probability of successful replication, $p$.\n\nExpected new molecules in cycle $t$ = $p \\cdot N(t-1)$.\n\nThe total number of molecules at the end of cycle $t$, $N(t)$, is the sum of the molecules that were present at the start of the cycle and the newly synthesized ones. This gives the recurrence relation:\n$$N(t) = N(t-1) + p \\cdot N(t-1)$$\n$$N(t) = (1+p) \\cdot N(t-1)$$\nThis is a first-order linear homogeneous recurrence relation with the initial condition $N(0) = N_0$. We can solve this by iteration:\nFor $t=1$: $N(1) = (1+p)N(0) = (1+p)N_0$\nFor $t=2$: $N(2) = (1+p)N(1) = (1+p)[(1+p)N_0] = (1+p)^2 N_0$\nFor $t=3$: $N(3) = (1+p)N(2) = (1+p)[(1+p)^2 N_0] = (1+p)^3 N_0$\n\nBy induction, the general closed-form expression for the expected number of molecules after $t$ cycles is:\n$$N(t) = N_0 (1+p)^t$$\nThis completes the first part of the problem.\n\nThe second step is to calculate the specific value of the per-cycle success probability, $p$, using the provided assay parameters. The probability $p$ is defined as $p = f_{\\text{bind}} f_{\\text{ext}}$.\n\nFirst, we determine the fraction of templates annealed, $f_{\\text{bind}}$. Under the pseudo-first-order condition $[P]_{\\text{tot}} \\gg [T]_{\\text{tot}}$, we can use the approximation $[P] \\approx [P]_{\\text{tot}}$. The formula for $f_{\\text{bind}}$ is:\n$$f_{\\text{bind}} = \\frac{[P]}{K_d + [P]} \\approx \\frac{[P]_{\\text{tot}}}{K_d + [P]_{\\text{tot}}}$$\nSubstituting the given values $[P]_{\\text{tot}} = 250\\,\\text{nM}$ and $K_d = 50\\,\\text{nM}$:\n$$f_{\\text{bind}} = \\frac{250\\,\\text{nM}}{50\\,\\text{nM} + 250\\,\\text{nM}} = \\frac{250}{300} = \\frac{5}{6}$$\n\nNext, we determine the extension success fraction, $f_{\\text{ext}}$. This fraction is $1$ if the extension time per cycle, $\\tau$, is greater than or equal to the time required for the polymerase to synthesize the amplicon, $L/k_{\\text{pol}}$. Otherwise, it is $0$.\nThe required extension time is:\n$$\\frac{L}{k_{\\text{pol}}} = \\frac{120\\,\\text{nt}}{80\\,\\text{nt}\\,\\text{s}^{-1}} = 1.5\\,\\text{s}$$\nThe provided extension time per cycle is $\\tau = 30\\,\\text{s}$. Comparing the two times:\n$$30\\,\\text{s} \\ge 1.5\\,\\text{s}$$\nSince $\\tau \\ge L/k_{\\text{pol}}$, the extension success fraction is:\n$$f_{\\text{ext}} = 1$$\n\nNow, we can calculate the overall per-cycle success probability $p$:\n$$p = f_{\\text{bind}} \\cdot f_{\\text{ext}} = \\frac{5}{6} \\cdot 1 = \\frac{5}{6}$$\n\nThe final step is to compute the real-valued cycle count, $t^{\\ast}$, at which the expected copy number, $N(t^{\\ast})$, reaches the fluorescence threshold, $N_{\\text{th}} = 1.0 \\times 10^9$ molecules. We set $N(t^{\\ast}) = N_{\\text{th}}$ in our derived equation:\n$$N_{\\text{th}} = N_0 (1+p)^{t^{\\ast}}$$\nSubstituting the known values $N_0 = 300$, $N_{\\text{th}} = 1.0 \\times 10^9$, and $p = 5/6$:\n$$1.0 \\times 10^9 = 300 \\left(1 + \\frac{5}{6}\\right)^{t^{\\ast}}$$\n$$1.0 \\times 10^9 = 300 \\left(\\frac{11}{6}\\right)^{t^{\\ast}}$$\nTo solve for $t^{\\ast}$, we first isolate the exponential term:\n$$\\frac{1.0 \\times 10^9}{300} = \\left(\\frac{11}{6}\\right)^{t^{\\ast}}$$\n$$\\frac{10^7}{3} = \\left(\\frac{11}{6}\\right)^{t^{\\ast}}$$\nNow, take the natural logarithm of both sides:\n$$\\ln\\left(\\frac{10^7}{3}\\right) = \\ln\\left(\\left(\\frac{11}{6}\\right)^{t^{\\ast}}\\right)$$\nUsing the logarithm property $\\ln(a^b) = b \\ln(a)$:\n$$\\ln\\left(\\frac{10^7}{3}\\right) = t^{\\ast} \\ln\\left(\\frac{11}{6}\\right)$$\nFinally, solve for $t^{\\ast}$:\n$$t^{\\ast} = \\frac{\\ln\\left(\\frac{10^7}{3}\\right)}{\\ln\\left(\\frac{11}{6}\\right)}$$\nThis is the exact analytical solution for $t^{\\ast}$. Now, we compute its numerical value:\n$$t^{\\ast} = \\frac{\\ln(10^7) - \\ln(3)}{\\ln(11) - \\ln(6)} \\approx \\frac{16.11809565 - 1.098612289}{2.397895273 - 1.791759469} = \\frac{15.01948336}{0.606135804} \\approx 24.778839$$\nThe problem requires the answer to be rounded to four significant figures.\n$$t^{\\ast} \\approx 24.78$$\nThis is the real-valued cycle count required to reach the threshold.",
            "answer": "$$\\boxed{24.78}$$"
        },
        {
            "introduction": "Accurate biomarker quantification is contingent upon a properly constructed calibration curve, which translates raw instrument signals into meaningful concentration values. While simple linear regression is often taught, real-world assays, particularly sensitive immunoassays, exhibit heteroscedastic noise where the measurement variance depends on the signal strength. This practice will guide you through the use of weighted least squares (WLS), a more robust statistical method that accounts for this non-uniform variance, allowing you to build a more accurate calibration model grounded in the physical sources of measurement noise. ",
            "id": "5134101",
            "problem": "An electrochemiluminescence (ECL) immunoassay is used to quantify a protein biomarker whose molecular recognition is mediated by high-affinity capture and detection antibodies. Under conditions where the binding is in the linear dynamic range of the assay and the ECL emission yield per bound analyte is constant, the expected signal is linear in concentration: $y_i = \\beta_0 + \\beta_1 c_i + \\varepsilon_i$, where $c_i$ is the calibrator concentration, $y_i$ is the measured mean photon count for calibrator $i$, and $\\varepsilon_i$ is the measurement error. The dominant noise sources are photon shot noise and instrument baseline read noise; for large counts, the Poisson-distributed shot noise is well approximated by a Gaussian with variance equal to its mean. The instrument baseline contributes an additive Gaussian noise with variance $\\sigma_b^2$. Thus, a validated error model from replicate measurements is $\\operatorname{Var}(y_i \\mid c_i) \\approx y_i + \\sigma_b^2$, with $\\sigma_b^2$ determined from blanks.\n\nYou are provided calibrator concentrations and corresponding mean signals measured on the same instrument session:\n- Concentrations (in nanomolar): $c = \\{0, 5, 10, 20\\}$.\n- Mean signals (in counts): $y = \\{100, 350, 600, 1100\\}$.\n- Baseline variance from blanks: $\\sigma_b^2 = 100$ counts$^2$.\n\nUsing the foundational principles above and the Gaussian approximation to Poisson statistics, construct the calibration curve by performing weighted least squares with weights $w_i = 1/\\operatorname{Var}(y_i \\mid c_i) = 1/(y_i + \\sigma_b^2)$. Then, use the resulting calibration to estimate the concentration $c^{\\ast}$ (in nanomolar) of an unknown sample that produced a mean signal $y^{\\ast} = 860$ counts in the same session. Round your final numerical estimate of $c^{\\ast}$ to three significant figures and express it in $\\mathrm{nM}$.",
            "solution": "The problem requires the construction of a calibration curve from a set of calibrator concentrations and their corresponding signals using the method of weighted least squares (WLS). Subsequently, this curve is to be used to determine the concentration of an unknown sample.\n\nThe relationship between the mean photon count, $y_i$, and the calibrator concentration, $c_i$, is given by the linear model:\n$$ y_i = \\beta_0 + \\beta_1 c_i + \\varepsilon_i $$\nwhere $\\beta_0$ is the intercept, $\\beta_1$ is the slope, and $\\varepsilon_i$ is the measurement error.\n\nThe error model specifies that the variance of the measured signal $y_i$ is dependent on the signal itself:\n$$ \\operatorname{Var}(y_i \\mid c_i) \\equiv \\sigma_i^2 \\approx y_i + \\sigma_b^2 $$\nThe problem provides the baseline variance from blank measurements as $\\sigma_b^2 = 100$ counts$^2$.\n\nThe method of weighted least squares is appropriate for linear regression when the error variances are not equal (heteroscedasticity). The parameters $\\beta_0$ and $\\beta_1$ are found by minimizing the weighted sum of squared residuals, $S$:\n$$ S(\\beta_0, \\beta_1) = \\sum_{i=1}^{n} w_i (y_i - (\\beta_0 + \\beta_1 c_i))^2 $$\nThe weights, $w_i$, are the reciprocal of the variances:\n$$ w_i = \\frac{1}{\\sigma_i^2} = \\frac{1}{y_i + \\sigma_b^2} $$\n\nThe provided data for the $n=4$ calibrators are:\n-   Concentrations $c_i$ (in $\\mathrm{nM}$): $\\{0, 5, 10, 20\\}$\n-   Mean signals $y_i$ (in counts): $\\{100, 350, 600, 1100\\}$\n\nFirst, we calculate the variance and weight for each data point using $\\sigma_b^2 = 100$:\n-   For $i=1$: $c_1=0$, $y_1=100$. $\\sigma_1^2 = 100 + 100 = 200$. $w_1 = \\frac{1}{200}$.\n-   For $i=2$: $c_2=5$, $y_2=350$. $\\sigma_2^2 = 350 + 100 = 450$. $w_2 = \\frac{1}{450}$.\n-   For $i=3$: $c_3=10$, $y_3=600$. $\\sigma_3^2 = 600 + 100 = 700$. $w_3 = \\frac{1}{700}$.\n-   For $i=4$: $c_4=20$, $y_4=1100$. $\\sigma_4^2 = 1100 + 100 = 1200$. $w_4 = \\frac{1}{1200}$.\n\nMinimizing $S$ with respect to $\\beta_0$ and $\\beta_1$ leads to the normal equations, which can be expressed in matrix form as $(X^T W X) \\hat{\\beta} = X^T W Y$, where $\\hat{\\beta} = [\\beta_0, \\beta_1]^T$, $X$ is the design matrix, $W$ is a diagonal matrix of weights, and $Y$ is the vector of observed signals. The components of the matrices are sums involving the data points:\n$$ \\begin{pmatrix} \\sum w_i & \\sum w_i c_i \\\\ \\sum w_i c_i & \\sum w_i c_i^2 \\end{pmatrix} \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\end{pmatrix} = \\begin{pmatrix} \\sum w_i y_i \\\\ \\sum w_i c_i y_i \\end{pmatrix} $$\n\nWe compute the required sums using the calculated weights:\n-   $\\sum w_i = \\frac{1}{200} + \\frac{1}{450} + \\frac{1}{700} + \\frac{1}{1200} = \\frac{239}{25200}$\n-   $\\sum w_i c_i = \\frac{1}{200}(0) + \\frac{1}{450}(5) + \\frac{1}{700}(10) + \\frac{1}{1200}(20) = \\frac{53}{1260}$\n-   $\\sum w_i c_i^2 = \\frac{1}{200}(0^2) + \\frac{1}{450}(5^2) + \\frac{1}{700}(10^2) + \\frac{1}{1200}(20^2) = \\frac{67}{126}$\n-   $\\sum w_i y_i = \\frac{1}{200}(100) + \\frac{1}{450}(350) + \\frac{1}{700}(600) + \\frac{1}{1200}(1100) = \\frac{769}{252}$\n-   $\\sum w_i c_i y_i = \\frac{1}{200}(0)(100) + \\frac{1}{450}(5)(350) + \\frac{1}{700}(10)(600) + \\frac{1}{1200}(20)(1100) = \\frac{1940}{63}$\n\nLet the system be denoted by $A \\beta_0 + B \\beta_1 = C$ and $B \\beta_0 + D \\beta_1 = E$, where:\n$A = \\sum w_i = \\frac{239}{25200}$, $B = \\sum w_i c_i = \\frac{53}{1260}$, $D = \\sum w_i c_i^2 = \\frac{67}{126}$, $C = \\sum w_i y_i = \\frac{769}{252}$, $E = \\sum w_i c_i y_i = \\frac{1940}{63}$.\n\nThe solution for $\\beta_0$ and $\\beta_1$ is given by Cramer's rule or matrix inversion:\n$$ \\beta_0 = \\frac{CD - BE}{AD - B^2} \\quad \\text{and} \\quad \\beta_1 = \\frac{AE - BC}{AD - B^2} $$\nWe calculate the determinant of the matrix, $\\Delta = AD - B^2$:\n$$ \\Delta = \\left(\\frac{239}{25200}\\right)\\left(\\frac{67}{126}\\right) - \\left(\\frac{53}{1260}\\right)^2 = \\frac{16013}{3175200} - \\frac{2809}{1587600} = \\frac{16013 - 5618}{3175200} = \\frac{10395}{3175200} = \\frac{11}{3360} $$\nNext, we calculate the numerators for $\\beta_0$ and $\\beta_1$:\nFor $\\beta_1$: $AE - BC = \\left(\\frac{239}{25200}\\right)\\left(\\frac{1940}{63}\\right) - \\left(\\frac{53}{1260}\\right)\\left(\\frac{769}{252}\\right) = \\frac{92732 - 40757}{317520} = \\frac{51975}{317520} = \\frac{55}{336}$.\nFor $\\beta_0$: $CD - BE = \\left(\\frac{769}{252}\\right)\\left(\\frac{67}{126}\\right) - \\left(\\frac{53}{1260}\\right)\\left(\\frac{1940}{63}\\right) = \\frac{51523}{31752} - \\frac{41128}{31752} = \\frac{10395}{31752} = \\frac{55}{168}$.\n\nNow we can solve for the parameters:\n$$ \\beta_1 = \\frac{AE - BC}{\\Delta} = \\frac{55/336}{11/3360} = \\frac{55}{336} \\cdot \\frac{3360}{11} = 5 \\cdot 10 = 50 $$\n$$ \\beta_0 = \\frac{CD - BE}{\\Delta} = \\frac{55/168}{11/3360} = \\frac{55}{168} \\cdot \\frac{3360}{11} = 5 \\cdot 20 = 100 $$\nThe resulting calibration curve is:\n$$ y = 100 + 50c $$\nThis equation happens to pass exactly through all given data points, which is a feature of the specific dataset provided.\n\nThe final step is to estimate the concentration $c^{\\ast}$ for an unknown sample that gives a mean signal of $y^{\\ast} = 860$ counts. We invert the calibration equation:\n$$ c^{\\ast} = \\frac{y^{\\ast} - \\beta_0}{\\beta_1} $$\nSubstituting the values:\n$$ c^{\\ast} = \\frac{860 - 100}{50} = \\frac{760}{50} = \\frac{76}{5} = 15.2 $$\nThe concentration is in nanomolar ($\\mathrm{nM}$). The problem asks for the answer to three significant figures, which $15.2$ already is.",
            "answer": "$$\\boxed{15.2}$$"
        },
        {
            "introduction": "Modern diagnostics are increasingly moving from single-analyte measurements to the interpretation of complex, high-dimensional molecular signatures. This practice introduces you to this frontier by focusing on cell-free DNA (cfDNA) fragmentation patterns, a powerful biomarker for non-invasive cancer detection and tissue-of-origin mapping. You will develop a Bayesian classifier from its foundational statistical definitions, gaining hands-on experience in how to translate a molecular hypothesis into a working diagnostic algorithm and handle the computational challenges involved in its implementation. ",
            "id": "5134070",
            "problem": "A developer is asked to formalize a principled tissue-of-origin classifier grounded in the molecular determinants of diagnostic biomarkers. The molecular context is as follows: end-motif frequencies of cell-free Deoxyribonucleic Acid (cfDNA) fragments are tissue-dependent because nuclease preferences and chromatin architecture imprint characteristic cleavage patterns. For a fixed vocabulary of end motifs, one can use tissue-specific motif frequency profiles as a molecular signature.\n\nYour task is to derive a classifier that, given observed end-motif counts from a cfDNA sample, computes class posterior probabilities using a likelihood derived from the definition of the multinomial distribution and Bayes’ theorem. The derivation must begin strictly from the core definitions: the probability mass function of the multinomial distribution and the statement of Bayes’ theorem. You must reason from these bases to obtain a computable expression for the class posterior probabilities, then implement the computation in a numerically stable way.\n\nSet up:\n- There are $C = 3$ tissues: Liver, Lung, and Colon. Use the class order $[ \\text{Liver}, \\text{Lung}, \\text{Colon} ]$ throughout.\n- There are $K = 8$ end motifs. For each tissue $c \\in \\{1,2,3\\}$ and motif index $k \\in \\{1,\\dots,8\\}$, let $\\theta_{c,k}$ denote the tissue-specific probability of motif $k$. The tissue-specific probabilities are given by rows of the following matrix (each row sums to $1$ and all entries are positive):\n  - Liver: $\\left[0.30, 0.25, 0.20, 0.10, 0.10, 0.03, 0.01, 0.01\\right]$\n  - Lung: $\\left[0.10, 0.15, 0.25, 0.20, 0.15, 0.10, 0.03, 0.02\\right]$\n  - Colon: $\\left[0.05, 0.10, 0.10, 0.15, 0.20, 0.20, 0.10, 0.10\\right]$\n- The class priors are uniform: for each class $c$, $\\pi_c = 1/3$.\n\nDefinitions you must start from:\n- If $X = (X_1,\\dots,X_K)$ are counts of $K$ categories from $N = \\sum_{k=1}^K X_k$ independent trials with category probabilities $\\Theta = (\\theta_1,\\dots,\\theta_K)$, the multinomial probability mass function is defined by\n  $$P(X \\mid \\Theta) = \\frac{N!}{\\prod_{k=1}^K X_k!} \\prod_{k=1}^K \\theta_k^{X_k}.$$\n- Bayes’ theorem for discrete classes states\n  $$P(c \\mid X) = \\frac{P(X \\mid c)\\,\\pi_c}{\\sum_{c'} P(X \\mid c')\\,\\pi_{c'}}.$$\n\nDerivation requirements:\n- Begin with the above definitions and derive a computable expression for $P(c \\mid X)$ in terms of $\\pi_c$, $\\theta_{c,k}$, and $X_k$, making clear how the combinatorial factor in the multinomial probability mass function is handled when forming the posterior.\n- Explain how to implement the computation in log space to ensure numerical stability, and state how to normalize to obtain valid posterior probabilities that sum to $1$.\n- Describe how the boundary case of $N = 0$ (all counts are zero) reduces to a posterior equal to the prior under the adopted model and definitions.\n\nTest suite:\nFor each of the following observed count vectors $X \\in \\mathbb{N}_0^8$ (ordered by the $8$ motifs), compute the posterior vector $\\left[P(\\text{Liver} \\mid X), P(\\text{Lung} \\mid X), P(\\text{Colon} \\mid X)\\right]$, and round each posterior to exactly $6$ decimal places.\n\n- Case A (general case, $N = 100$): $X = [30, 26, 19, 10, 9, 3, 2, 1]$.\n- Case B (general case, $N = 100$): $X = [9, 16, 24, 20, 15, 10, 3, 3]$.\n- Case C (small-count edge, $N = 5$): $X = [0, 1, 1, 1, 1, 1, 0, 0]$.\n- Case D (ambiguous mix, $N = 100$): $X = [12, 12, 13, 13, 12, 13, 12, 13]$.\n- Case E (zero-count boundary, $N = 0$): $X = [0, 0, 0, 0, 0, 0, 0, 0]$.\n\nFinal output format:\n- Your program must produce a single line of output containing the list of posterior vectors for the test cases in the order A, B, C, D, E. Each posterior vector must be a list of three floats in the fixed class order $[ \\text{Liver}, \\text{Lung}, \\text{Colon} ]$, with each float rounded to exactly $6$ decimal places. The entire output must be a single list of lists printed on one line, for example: $[[a_{11},a_{12},a_{13}],[a_{21},a_{22},a_{23}],\\dots]$.",
            "solution": "The user-provided problem is assessed to be valid. It is scientifically grounded in established principles of molecular biology (tissue-specific cfDNA fragmentation) and statistics (Bayesian classification with a multinomial model). The problem is well-posed, with all necessary parameters, definitions, and data provided. It is objective, mathematically formalizable, and presents a non-trivial but solvable challenge. Therefore, a complete solution is provided.\n\nThe objective is to derive and implement a Bayesian classifier to determine the tissue-of-origin for a cell-free DNA (cfDNA) sample based on observed end-motif counts. The classifier must compute the posterior probability for each tissue class, $P(c \\mid X)$, given an observed count vector $X = (X_1, \\dots, X_K)$.\n\n### Step 1: Derivation of the Posterior Probability Expression\n\nThe derivation begins with the two definitions provided: Bayes' theorem and the probability mass function (PMF) of the multinomial distribution.\n\nBayes' theorem for a discrete set of classes $c \\in \\{1, \\dots, C\\}$ and observed data $X$ is given by:\n$$P(c \\mid X) = \\frac{P(X \\mid c)\\,\\pi_c}{\\sum_{c'=1}^C P(X \\mid c')\\,\\pi_{c'}}$$\nwhere $P(c \\mid X)$ is the posterior probability of class $c$, $P(X \\mid c)$ is the likelihood of observing data $X$ given class $c$, and $\\pi_c$ is the prior probability of class $c$.\n\nThe problem states that the observed end-motif counts $X = (X_1, \\dots, X_K)$ for a given tissue of origin $c$ follow a multinomial distribution. The parameters of this distribution are the total number of observed motifs, $N = \\sum_{k=1}^K X_k$, and the tissue-specific motif probabilities, $\\Theta_c = (\\theta_{c,1}, \\dots, \\theta_{c,K})$. The likelihood $P(X \\mid c)$ is therefore given by the multinomial PMF:\n$$P(X \\mid c) = P(X \\mid \\Theta_c) = \\frac{N!}{\\prod_{k=1}^K X_k!} \\prod_{k=1}^K \\theta_{c,k}^{X_k}$$\n\nSubstituting this likelihood expression into Bayes' theorem yields:\n$$P(c \\mid X) = \\frac{\\left( \\frac{N!}{\\prod_{k=1}^K X_k!} \\prod_{k=1}^K \\theta_{c,k}^{X_k} \\right) \\pi_c}{\\sum_{c'=1}^C \\left( \\frac{N!}{\\prod_{k=1}^K X_k!} \\prod_{k=1}^K \\theta_{c',k}^{X_k} \\right) \\pi_{c'}}$$\n\nA key simplification arises from observing the multinomial coefficient, $\\frac{N!}{\\prod_{k=1}^K X_k!}$. This term depends only on the total count $N$ and the individual counts $X_k$, but it does **not** depend on the class $c$. As such, it is a common factor in the numerator and in every term of the summation in the denominator. We can factor it out and cancel it:\n$$P(c \\mid X) = \\frac{\\cancel{\\left(\\frac{N!}{\\prod_{k=1}^K X_k!}\\right)} \\left( \\prod_{k=1}^K \\theta_{c,k}^{X_k} \\right) \\pi_c}{\\cancel{\\left(\\frac{N!}{\\prod_{k=1}^K X_k!}\\right)} \\sum_{c'=1}^C \\left( \\prod_{k=1}^K \\theta_{c',k}^{X_k} \\right) \\pi_{c'}}$$\n\nThis leads to the simplified, computable expression for the posterior probability:\n$$P(c \\mid X) = \\frac{\\left( \\prod_{k=1}^K \\theta_{c,k}^{X_k} \\right) \\pi_c}{\\sum_{c'=1}^C \\left( \\prod_{k=1}^K \\theta_{c',k}^{X_k} \\right) \\pi_{c'}}$$\nThe numerator is proportional to the joint probability $P(X, c)$, and the denominator is the marginal probability of the data $P(X)$, which serves as a normalization constant ensuring that $\\sum_{c=1}^C P(c \\mid X) = 1$.\n\n### Step 2: Numerically Stable Implementation in Log Space\n\nThe product term $\\prod_{k=1}^K \\theta_{c,k}^{X_k}$ involves multiplying many probabilities, which are numbers less than $1$. For large counts $X_k$, this product can become vanishingly small, leading to numerical underflow in standard floating-point arithmetic. To mitigate this, computations are performed in logarithmic space.\n\nWe define the unnormalized log-posterior for each class $c$, denoted $L_c$, which is the logarithm of the numerator in the simplified expression:\n$$L_c = \\log\\left( \\left( \\prod_{k=1}^K \\theta_{c,k}^{X_k} \\right) \\pi_c \\right) = \\log(\\pi_c) + \\log\\left(\\prod_{k=1}^K \\theta_{c,k}^{X_k}\\right)$$\nUsing the properties of logarithms, this becomes a sum:\n$$L_c = \\log(\\pi_c) + \\sum_{k=1}^K X_k \\log(\\theta_{c,k})$$\nThis form, involving sums of logarithms, is numerically stable and avoids underflow.\n\nThe posterior probability can be recovered by exponentiating and normalizing:\n$$P(c \\mid X) = \\frac{\\exp(L_c)}{\\sum_{c'=1}^C \\exp(L_{c'})}$$\nHowever, if the $L_c$ values are large and positive, or large and negative, $\\exp(L_c)$ can lead to overflow or underflow, respectively. To handle this, we use the log-sum-exp trick. We find the maximum value among all log-posteriors, $L_{\\max} = \\max_{c'} \\{L_{c'}\\}$, and subtract it from each $L_c$ before exponentiating:\n$$P(c \\mid X) = \\frac{\\exp(L_c - L_{\\max})}{\\sum_{c'=1}^C \\exp(L_{c'} - L_{\\max})}$$\nBy construction, the largest exponent is now $0$, and all other exponents are negative. This prevents overflow, and since at least one term in the denominator's sum is $\\exp(0)=1$, the denominator cannot be zero. This provides a robust method for computing the final posterior probabilities.\n\n### Step 3: Analysis of the Boundary Case $N=0$\n\nConsider the case where no motifs are observed, i.e., the count vector is $X = (0, 0, \\dots, 0)$. In this case, the total count is $N = \\sum_{k=1}^K X_k = 0$.\n\nApplying this to our expression for the unnormalized log-posterior $L_c$:\n$$L_c = \\log(\\pi_c) + \\sum_{k=1}^K 0 \\cdot \\log(\\theta_{c,k})$$\nSince $0$ multiplied by any finite number is $0$, the summation term vanishes:\n$$L_c = \\log(\\pi_c) + 0 = \\log(\\pi_c)$$\nThe unnormalized log-posterior is simply the log-prior.\n\nTo find the posterior probability, we exponentiate and normalize:\n$$P(c \\mid X=\\mathbf{0}) = \\frac{\\exp(L_c)}{\\sum_{c'} \\exp(L_{c'})} = \\frac{\\exp(\\log(\\pi_c))}{\\sum_{c'} \\exp(\\log(\\pi_{c'}))}$$\nThis simplifies to:\n$$P(c \\mid X=\\mathbf{0}) = \\frac{\\pi_c}{\\sum_{c'} \\pi_{c'}}$$\nGiven that the prior probabilities must sum to one ($\\sum_{c'} \\pi_{c'} = 1$), the expression reduces to:\n$$P(c \\mid X=\\mathbf{0}) = \\pi_c$$\nThis result is intuitive: in the complete absence of evidence (zero counts), our posterior belief about the tissue of origin is identical to our prior belief. For this problem, where the priors are uniform ($\\pi_c = 1/3$), the posterior probabilities for the $N=0$ case will be $(1/3, 1/3, 1/3)$.",
            "answer": "```\n[[0.999999,0.000001,0.000000],[0.000171,0.997230,0.002599],[0.119330,0.528652,0.352018],[0.000000,0.000003,0.999997],[0.333333,0.333333,0.333333]]\n```"
        }
    ]
}