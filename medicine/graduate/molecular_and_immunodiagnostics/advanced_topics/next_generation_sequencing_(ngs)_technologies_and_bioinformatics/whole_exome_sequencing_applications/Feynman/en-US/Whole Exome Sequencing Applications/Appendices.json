{
    "hands_on_practices": [
        {
            "introduction": "A key application of Whole Exome Sequencing (WES) in oncology is the quantification of Tumor Mutational Burden (TMB), a biomarker that can predict response to immunotherapy. This exercise provides direct practice in calculating TMB from processed sequencing data, emphasizing the critical distinction between raw variant counts and the properly normalized value based on the \"callable\" genomic territory. Mastering this calculation is fundamental to interpreting genomic reports in a clinical cancer context .",
            "id": "4396805",
            "problem": "A clinical laboratory performs Whole Exome Sequencing (WES) on a solid tumor and its matched normal to estimate the Tumor Mutational Burden (TMB), defined here as the number of nonsynonymous somatic single-nucleotide variants per megabase of callable coding sequence. The following fundamental bases are assumed: the Central Dogma of molecular biology establishes DNA as the heritable substrate from which protein-coding sequences (the exome) are derived; WES targets protein-coding regions (exons); single-nucleotide variants (SNVs) are point changes at a single base; the callable territory is the subset of targeted bases that meet prespecified quality criteria (for example, depth of coverage threshold, mapping quality, and mappability), ensuring that somatic variants can be confidently detected; and TMB, when defined as above, is a density of nonsynonymous somatic SNVs over the callable coding megabases.\n\nFor a single tumor sample, variant calling with a matched normal identifies a total of $N_{\\text{SNV}}^{\\text{total}}=512$ somatic single-nucleotide variants in targeted coding regions. Quality control and functional annotation yield the following partition within the targeted exome:\n- Within the callable coding territory, $N_{\\text{callable}}=438$ somatic SNVs are detected.\n- Of these callable SNVs, $N_{\\text{nonsyn}}=346$ are nonsynonymous and $N_{\\text{syn}}=92$ are synonymous.\n- Outside the callable territory (either low coverage or poor mappability), $N_{\\text{noncallable}}=74$ somatic SNVs are detected; these are excluded from the TMB calculation by definition of callable territory.\n\nThe callable coding territory length, after applying the laboratory’s quality filters, is $L_{\\text{callable}}=28.7$ megabases.\n\nStarting from the above fundamental bases, derive the analytic expression used to compute TMB under this definition and then compute the TMB for this sample using only the appropriate counts and territory length. Express your final answer in mutations per megabase and round your numerical result to four significant figures.",
            "solution": "The problem requires the derivation of an analytic expression for Tumor Mutational Burden (TMB) based on a specific definition and the subsequent calculation of its value for a given tumor sample. The process begins with a formal statement of the definition.\n\nThe problem defines Tumor Mutational Burden (TMB) as the number of nonsynonymous somatic single-nucleotide variants per megabase of callable coding sequence. Let us formalize this definition into a mathematical expression.\n\nLet $TMB$ represent the Tumor Mutational Burden.\nLet $N_{\\text{nonsyn}}$ be the count of nonsynonymous somatic single-nucleotide variants identified within the callable territory.\nLet $L_{\\text{callable}}$ be the length of the callable coding sequence, expressed in units of megabases (Mb).\n\nBased on the definition \"number... per megabase,\" TMB is a density, calculated as the ratio of the count of relevant variants to the length of the genomic territory over which they were counted. The analytic expression is therefore:\n$$\nTMB = \\frac{N_{\\text{nonsyn}}}{L_{\\text{callable}}}\n$$\nThe units of this quantity will be mutations per megabase (mut/Mb).\n\nThe problem provides the following data derived from the analysis of a tumor sample:\n- Total somatic SNVs in targeted coding regions: $N_{\\text{SNV}}^{\\text{total}} = 512$\n- Somatic SNVs within the callable coding territory: $N_{\\text{callable}} = 438$\n- Nonsynonymous somatic SNVs within the callable territory: $N_{\\text{nonsyn}} = 346$\n- Synonymous somatic SNVs within the callable territory: $N_{\\text{syn}} = 92$\n- Somatic SNVs outside the callable territory: $N_{\\text{noncallable}} = 74$\n- Callable coding territory length: $L_{\\text{callable}} = 28.7$ Mb\n\nTo compute the TMB, we must select the correct values from this list according to the derived formula. The definition explicitly requires the count of *nonsynonymous* SNVs. Therefore, we must use $N_{\\text{nonsyn}} = 346$. The other counts, such as synonymous SNVs ($N_{\\text{syn}}$), non-callable SNVs ($N_{\\text{noncallable}}$), and the total SNVs ($N_{\\text{SNV}}^{\\text{total}}$), are irrelevant to the calculation of TMB as defined in this problem statement. The denominator is explicitly the *callable* coding territory length, which is given as $L_{\\text{callable}} = 28.7$ Mb.\n\nSubstituting the appropriate values into the expression for TMB:\n$$\nTMB = \\frac{346}{28.7}\n$$\n\nNow, we perform the division to obtain a numerical value:\n$$\nTMB \\approx 12.0557491289... \\text{ mut/Mb}\n$$\n\nThe problem requires the final answer to be rounded to four significant figures. The first four significant figures of the result are $1$, $2$, $0$, and $5$. The fifth significant figure is $5$, which dictates that we round up the fourth significant figure. Thus, $12.05$ becomes $12.06$.\n\nThe final calculated TMB for this sample is $12.06$ mut/Mb.",
            "answer": "$$\\boxed{12.06}$$"
        },
        {
            "introduction": "The clinical utility of any WES assay depends directly on which disease-relevant genes it can successfully capture and sequence. This practical programming exercise simulates a crucial task in bioinformatics: evaluating the diagnostic coverage of a commercial exome kit. By intersecting the kit's target regions with a list of clinically significant genomic positions, you will quantify the \"diagnostic gap\" and gain a deeper appreciation for the technical limitations that must be considered when choosing and interpreting WES-based tests .",
            "id": "5171428",
            "problem": "You are given a formalization of Whole Exome Sequencing (WES) target capture and clinically annotated coding bases to quantify, from first principles, the expected fraction of disease-relevant coding bases that are not captured by a particular exome kit. The fundamental base for this problem draws on the Central Dogma of Molecular Biology (deoxyribonucleic acid to ribonucleic acid to protein) and the operational definitions used in exome capture and clinical variant annotation: exons are contiguous coding segments of deoxyribonucleic acid, exome kits specify target regions typically via Browser Extensible Data (BED) files, and disease-relevant coding bases are annotated positions derived from Clinical Variants (ClinVar) and Online Mendelian Inheritance in Man (OMIM). BED intervals are represented as half-open intervals.\n\nDefinitions and assumptions:\n- Let the genome positions be integers forming a countable set. Let $D$ be the finite set of disease-relevant coding base coordinates collected from Clinical Variants (ClinVar) and Online Mendelian Inheritance in Man (OMIM), expressed as integers.\n- Let the exome kit’s targeted regions be a finite union of half-open intervals $[s_i, e_i)$ on the integer line, where $s_i$ and $e_i$ are integers and $s_i \\lt e_i$. These intervals correspond to the kit’s intended captured bases consistent with Browser Extensible Data (BED) semantics; a base at integer coordinate $x$ is considered targeted if there exists an $i$ such that $s_i \\le x \\lt e_i$.\n- Let $C = \\bigcup_i [s_i, e_i)$ denote the kit’s capture set as a union of half-open intervals.\n- Let the indicator function $I(x)$ be defined as $I(x) = 1$ if $x \\in C$ and $I(x) = 0$ otherwise.\n- The expected fraction of disease-relevant coding bases not captured by the kit, under a uniform selection over $D$, is the complement of the expected capture probability over $D$. Formally, define the not-captured set $U = D \\setminus C$ and the fraction\n$$\nF = \\begin{cases}\n\\frac{|U|}{|D|}, & \\text{if } |D| \\gt 0, \\\\\n0, & \\text{if } |D| = 0 \\text{ (by convention for diagnostics to avoid undefined ratios)}.\n\\end{cases}\n$$\nThis can also be expressed using the indicator function as\n$$\nF = 1 - \\frac{1}{|D|} \\sum_{x \\in D} I(x), \\quad \\text{for } |D| \\gt 0.\n$$\nYour task is to write a complete, runnable program that, for the given test suite of parameter sets below, computes $F$ for each case. The program must implement interval union logic consistent with half-open intervals. Use only the provided data inside the program with no external inputs.\n\nImportant specifications:\n- Intervals are half-open $[s, e)$: a base at coordinate $x$ is covered if and only if $s \\le x \\lt e$.\n- All answers must be expressed as decimals (do not use the percentage sign).\n- If $|D| = 0$, return $0$ as specified above.\n\nTest suite parameter sets:\n- Case $1$ (general happy path):\n  - $D = \\{100, 101, 102, 1000, 1001, 1002\\}$\n  - $C = [(90, 110), (995, 1005), (2000, 3000)]$\n- Case $2$ (boundary condition at half-open end):\n  - $D = \\{9, 10\\}$\n  - $C = [(0, 10)]$\n- Case $3$ (overlapping intervals):\n  - $D = \\{54, 55, 60, 64, 65\\}$\n  - $C = [(50, 60), (55, 65)]$\n- Case $4$ (empty capture kit):\n  - $D = \\{1, 2, 3\\}$\n  - $C = []$\n- Case $5$ (no annotated disease bases, defined result):\n  - $D = \\{\\}$\n  - $C = [(0, 100)]$\n- Case $6$ (mixed realistic boundaries):\n  - $D = \\{200, 250, 300, 349, 350, 351, 400, 450, 500\\}$\n  - $C = [(190, 210), (248, 260), (300, 350), (351, 375), (420, 430)]$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,...]\"), ordered as Case $1$ through Case $6$ where each result is the decimal $F$ for that case.",
            "solution": "The problem requires the computation of the expected fraction of disease-relevant coding bases that are not captured by a given Whole Exome Sequencing (WES) kit. This is a practical problem in clinical bioinformatics, where understanding the coverage limitations of a diagnostic test is critical. The problem is well-posed, scientifically grounded, and provides all necessary definitions and data for a unique solution.\n\nThe fundamental components of the problem are:\n1.  A finite set of integer coordinates, $D$, representing the positions of known disease-relevant bases.\n2.  A set of capture regions, $C$, defined as a finite union of half-open intervals, $C = \\bigcup_i [s_i, e_i)$, where $s_i$ and $e_i$ are integers representing the start and end coordinates of a targeted genomic region.\n\nThe primary task is to calculate the fraction $F$, defined as the ratio of the number of disease-relevant bases not captured by the kit to the total number of disease-relevant bases. Formally, let $U = D \\setminus C$ be the set of bases in $D$ that are not in $C$. The fraction $F$ is given by:\n$$\nF = \\begin{cases}\n\\frac{|U|}{|D|} & \\text{if } |D| > 0 \\\\\n0 & \\text{if } |D| = 0\n\\end{cases}\n$$\nThe condition for a base at an integer coordinate $x$ to be captured by the kit is that it must fall within at least one of the half-open intervals defined in $C$. That is, $x \\in C$ if there exists an interval $[s_i, e_i) \\in C$ such that $s_i \\le x < e_i$.\n\nThe algorithmic approach to solving this for a given pair of $D$ and $C$ follows directly from these definitions:\n\n1.  First, we address the special case where the set of disease-relevant bases $D$ is empty. As per the problem's convention, if $|D| = 0$, the fraction $F$ is defined to be $0$. This avoids division by zero and provides a sensible default for diagnostic reporting.\n\n2.  If $|D| > 0$, we determine the number of captured bases. We can iterate through each base coordinate $x \\in D$. For each $x$, we check if it is captured.\n\n3.  To check if a base $x$ is captured, we iterate through the list of intervals $[s_i, e_i)$ that constitute $C$. If we find any interval such that the condition $s_i \\le x < e_i$ is met, the base $x$ is considered captured. At this point, we can stop checking further intervals for this particular base $x$ and proceed to the next base in $D$.\n\n4.  We maintain a count of the captured bases. Let this be denoted by $|D \\cap C|$.\n\n5.  Once all bases in $D$ have been evaluated, we can calculate the number of uncaptured bases, $|U|$, using the relation $|U| = |D| - |D \\cap C|$.\n\n6.  Finally, the fraction $F$ is computed as the ratio of uncaptured bases to the total number of bases: $F = \\frac{|D| - |D \\cap C|}{|D|}$.\n\nThis procedure is applied to each of the $6$ test cases provided. For example, in Case $2$ with $D = \\{9, 10\\}$ and $C = [(0, 10)]$:\n- The total number of bases is $|D| = 2$.\n- For base $x = 9$: we check if $0 \\le 9 < 10$. This is true, so the base is captured.\n- For base $x = 10$: we check if $0 \\le 10 < 10$. This is false due to the strict inequality at the end of the half-open interval. The base is not captured.\n- The number of captured bases is $1$.\n- The number of uncaptured bases is $|U| = 2 - 1 = 1$.\n- The fraction is $F = \\frac{1}{2} = 0.5$.\n\nThis methodology is systematically applied to all parameter sets to generate the final list of results.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Main function to solve the WES capture fraction problem for all test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"D\": {100, 101, 102, 1000, 1001, 1002},\n            \"C\": [(90, 110), (995, 1005), (2000, 3000)],\n        },\n        {\n            \"D\": {9, 10},\n            \"C\": [(0, 10)],\n        },\n        {\n            \"D\": {54, 55, 60, 64, 65},\n            \"C\": [(50, 60), (55, 65)],\n        },\n        {\n            \"D\": {1, 2, 3},\n            \"C\": [],\n        },\n        {\n            \"D\": set(),\n            \"C\": [(0, 100)],\n        },\n        {\n            \"D\": {200, 250, 300, 349, 350, 351, 400, 450, 500},\n            \"C\": [(190, 210), (248, 260), (300, 350), (351, 375), (420, 430)],\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        D_set = case[\"D\"]\n        C_intervals = case[\"C\"]\n        result = compute_uncaptured_fraction(D_set, C_intervals)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef compute_uncaptured_fraction(d_set, c_intervals):\n    \"\"\"\n    Computes the fraction of disease-relevant bases not captured by the kit.\n\n    Args:\n        d_set (set): A set of integers representing disease-relevant base coordinates.\n        c_intervals (list of tuples): A list of (start, end) tuples representing\n                                      half-open capture intervals [start, end).\n\n    Returns:\n        float: The fraction of bases in d_set not covered by c_intervals.\n    \"\"\"\n    num_d = len(d_set)\n    if num_d == 0:\n        return 0.0\n\n    captured_count = 0\n    for x in d_set:\n        is_captured = False\n        for start, end in c_intervals:\n            if start = x  end:\n                is_captured = True\n                break  # The base is captured; no need to check other intervals.\n        if is_captured:\n            captured_count += 1\n\n    uncaptured_count = num_d - captured_count\n    fraction = uncaptured_count / num_d\n    return fraction\n\nsolve()\n```"
        },
        {
            "introduction": "Distinguishing true genetic variants from technical artifacts is one of the most significant challenges in analyzing sequencing data. This advanced practice guides you through the process of building a Bayesian statistical model to tackle a notorious source of errors: polymerase slippage in homopolymer regions, which can mimic small insertions or deletions. By deriving the posterior odds of a true variant versus an artifact, you will develop an intuition for the probabilistic reasoning that underpins modern variant calling pipelines and the necessity of robust filtering strategies .",
            "id": "5171488",
            "problem": "A molecular diagnostics lab performs amplicon-enriched whole exome sequencing to detect pathogenic small insertions and deletions (indels) in genes relevant to immune-mediated disease. In homopolymer runs, deoxyribonucleic acid (DNA) polymerase slippage during amplification can produce stutter artifacts that mimic true indels. To control such artifacts, design a probabilistic model for indel calling that mixes a true-variant process with a homopolymer slippage artifact process, starting from the following fundamental base: independent sampling of molecules into reads, the Binomial law for the number of reads supporting a hypothesis given a per-read success probability, and Bayes’ theorem relating prior probability, likelihood, and posterior probability.\n\nAssume the following generative components:\n- Under the true-variant hypothesis, each unique input molecule independently yields a read indicating the indel with probability $p_{t}$ equal to the true allelic fraction.\n- Under the artifact hypothesis, each unique input molecule independently yields a read indicating a $\\pm 1$-base indel within a homopolymer run of length $L$ with probability $q(L)$ that increases with $L$ according to $q(L) = q_{0}\\,\\exp\\!\\big(\\lambda\\,(L-1)\\big)$.\n- Amplicon enrichment introduces read duplication; therefore, collapse duplicates by unique molecular identifiers (UMIs) or unique start positions, and model only the $m$ unique molecules, where $m = n/d$, $n$ is the total read count at the locus, and $d$ is the average duplicate factor per molecule.\n- Let the prior probability that any given locus harbors a true small indel be $\\pi_{t}$.\n\nYou observe a candidate indel in a homopolymer run with parameters $n = 240$, $d = 3$, $L = 9$, $q_{0} = 2.0 \\times 10^{-4}$, $\\lambda = 0.40$, $p_{t} = 0.45$, and $\\pi_{t} = 1.0 \\times 10^{-4}$. Using Bayes’ theorem and the Binomial model, derive the posterior odds for the true-variant hypothesis versus the artifact hypothesis as a function of the number $x$ of unique molecules supporting the indel, and then determine the smallest integer $x^{\\star}$ such that the posterior probability of the true-variant hypothesis is at least $0.95$. Express the final answer as the integer $x^{\\star}$. No rounding instruction is needed because $x^{\\star}$ is an integer.\n\nIn addition to the calculation, justify, in the context of your model, at least two additional filters that would specifically mitigate polymerase slippage artifacts in amplicon-enriched exomes (for example, constraints on strand balance, minimum unique molecule support, homopolymer-length-dependent penalties, and stutter ladder symmetry checks). Your justification must be grounded in the model’s assumptions and the amplification context without appealing to shortcut formulas.",
            "solution": "The problem statement is scientifically grounded, well-posed, objective, and contains all necessary information to derive a unique solution. The model is a standard, albeit simplified, representation of artifact modeling in bioinformatics. Therefore, the problem is valid, and a solution can be formulated.\n\nThe task is to determine the minimum number of unique molecules, $x^{\\star}$, supporting a candidate indel that is required to achieve a posterior probability of at least $0.95$ for the true-variant hypothesis. This involves developing a Bayesian model and then applying it to the given parameters. The problem also requires a justification for two additional filtering methods.\n\nLet $H_t$ be the hypothesis that the candidate indel is a true genetic variant. Let $H_a$ be the hypothesis that the candidate indel is a polymerase slippage artifact. These are considered mutually exclusive and exhaustive hypotheses for the cause of the observed indel-supporting reads.\n\nThe prior probabilities for these hypotheses are given by:\n$P(H_t) = \\pi_t = 1.0 \\times 10^{-4}$\n$P(H_a) = 1 - \\pi_t = 1 - 1.0 \\times 10^{-4} = 0.9999$\n\nThe data, $D$, consists of observing $x$ unique molecules supporting the indel out of a total of $m$ unique molecules sampled at the locus. The total number of unique molecules, $m$, is calculated from the total read count, $n$, and the average duplicate factor, $d$:\n$m = \\frac{n}{d} = \\frac{240}{3} = 80$\n\nThe problem specifies using a Binomial model for the likelihood of observing the data under each hypothesis. The likelihood function for observing $x$ successes in $m$ trials is given by $P(x|m, p) = \\binom{m}{x} p^x (1-p)^{m-x}$, where $p$ is the probability of success in a single trial.\n\nUnder the true-variant hypothesis, $H_t$, the probability of any given unique molecule supporting the indel is the true allelic fraction, $p_t = 0.45$. The likelihood is:\n$$ P(D|H_t) = \\binom{m}{x} p_t^x (1-p_t)^{m-x} $$\n\nUnder the artifact hypothesis, $H_a$, the probability of a unique molecule supporting the indel is due to polymerase slippage in a homopolymer run of length $L$. This probability is given by $q(L) = q_{0}\\exp(\\lambda(L-1))$. With the provided parameters $L=9$, $q_0 = 2.0 \\times 10^{-4}$, and $\\lambda = 0.40$, this probability is:\n$q(9) = (2.0 \\times 10^{-4}) \\exp(0.40 \\times (9-1)) = (2.0 \\times 10^{-4}) \\exp(3.2)$\nThe likelihood under the artifact hypothesis is:\n$$ P(D|H_a) = \\binom{m}{x} q(9)^x (1-q(9))^{m-x} $$\n\nWe use Bayes' theorem to find the posterior probability of the true-variant hypothesis, $P(H_t|D)$. It is more convenient to work with posterior odds, $O_{post}$, which is the ratio of the posterior probabilities:\n$$ O_{post} = \\frac{P(H_t|D)}{P(H_a|D)} = \\frac{P(D|H_t)P(H_t)}{P(D|H_a)P(H_a)} = \\left( \\frac{P(D|H_t)}{P(D|H_a)} \\right) \\left( \\frac{P(H_t)}{P(H_a)} \\right) $$\nThe first term is the Bayes Factor ($BF$) and the second is the prior odds.\n\nSubstituting the expressions for the likelihoods and priors:\n$$ O_{post} = \\frac{\\binom{m}{x} p_t^x (1-p_t)^{m-x}}{\\binom{m}{x} q(9)^x (1-q(9))^{m-x}} \\times \\frac{\\pi_t}{1-\\pi_t} $$\nThe binomial coefficient $\\binom{m}{x}$ cancels out, yielding the posterior odds as a function of $x$:\n$$ O_{post}(x) = \\left( \\frac{p_t}{q(9)} \\right)^x \\left( \\frac{1-p_t}{1-q(9)} \\right)^{m-x} \\left( \\frac{\\pi_t}{1-\\pi_t} \\right) $$\nThe problem requires finding the smallest integer $x^{\\star}$ such that $P(H_t|D) \\ge 0.95$. A posterior probability $P$ is related to the odds $O$ by $O = P / (1-P)$. Therefore, the required threshold for the posterior odds is:\n$O_{thresh} = \\frac{0.95}{1 - 0.95} = \\frac{0.95}{0.05} = 19$\n\nWe must find the smallest integer $x$ for which $O_{post}(x) \\ge 19$.\n$$ \\left( \\frac{p_t}{q(9)} \\right)^x \\left( \\frac{1-p_t}{1-q(9)} \\right)^{m-x} \\left( \\frac{\\pi_t}{1-\\pi_t} \\right) \\ge 19 $$\nTo solve for $x$, we take the natural logarithm of both sides:\n$$ x \\ln\\left(\\frac{p_t}{q(9)}\\right) + (m-x) \\ln\\left(\\frac{1-p_t}{1-q(9)}\\right) + \\ln\\left(\\frac{\\pi_t}{1-\\pi_t}\\right) \\ge \\ln(19) $$\nRearranging the terms to isolate $x$:\n$$ x \\left[ \\ln\\left(\\frac{p_t}{q(9)}\\right) - \\ln\\left(\\frac{1-p_t}{1-q(9)}\\right) \\right] \\ge \\ln(19) - m \\ln\\left(\\frac{1-p_t}{1-q(9)}\\right) - \\ln\\left(\\frac{\\pi_t}{1-\\pi_t}\\right) $$\n$$ x \\ln\\left( \\frac{p_t(1-q(9))}{q(9)(1-p_t)} \\right) \\ge \\ln(19) - m \\ln\\left(\\frac{1-p_t}{1-q(9)}\\right) - \\ln\\left(\\frac{\\pi_t}{1-\\pi_t}\\right) $$\nNow, we substitute the numerical values:\n$m = 80$\n$p_t = 0.45$\n$q(9) = 2.0 \\times 10^{-4} \\exp(3.2) \\approx 2.0 \\times 10^{-4} \\times 24.5325 = 0.0049065$\n$\\pi_t = 1.0 \\times 10^{-4}$\n\nLet's compute the logarithmic terms:\n$\\ln\\left( \\frac{p_t(1-q(9))}{q(9)(1-p_t)} \\right) = \\ln\\left( \\frac{0.45 \\times (1-0.0049065)}{0.0049065 \\times (1-0.45)} \\right) = \\ln\\left( \\frac{0.44779}{0.0026986} \\right) = \\ln(165.938) \\approx 5.1116$\n$\\ln\\left(\\frac{1-p_t}{1-q(9)}\\right) = \\ln\\left(\\frac{0.55}{1-0.0049065}\\right) = \\ln\\left(\\frac{0.55}{0.9950935}\\right) = \\ln(0.55271) \\approx -0.5929$\n$\\ln\\left(\\frac{\\pi_t}{1-\\pi_t}\\right) = \\ln\\left(\\frac{10^{-4}}{1-10^{-4}}\\right) = \\ln(1.0001 \\times 10^{-4}) \\approx -9.2102$\n$\\ln(19) \\approx 2.9444$\n\nSubstituting these values into the inequality:\n$$ x(5.1116) \\ge 2.9444 - 80(-0.5929) - (-9.2102) $$\n$$ x(5.1116) \\ge 2.9444 + 47.432 + 9.2102 $$\n$$ x(5.1116) \\ge 59.5866 $$\n$$ x \\ge \\frac{59.5866}{5.1116} \\approx 11.657 $$\nSince $x$ must be an integer representing the count of molecules, the smallest integer value that satisfies this condition is $x^{\\star} = 12$.\n\nJustification for two additional filters:\n\n1.  Strand Balance Filter: True heterozygous variants are present on both parental chromosomes and thus on both the forward and reverse strands of the DNA double helix. The model assumes a single probability $p_t \\approx 0.5$ for observing the variant on any given molecule, implying that reads supporting the variant should arise from both strands. An artifact, however, can be strand-specific due to biases in polymerase activity or primer hybridization during PCR amplification. A strong strand bias, where nearly all $x$ supporting molecules originate from one strand, is more probable under an artifact hypothesis ($H_a$) than a true-variant hypothesis ($H_t$). We can refine the model by treating reads from forward and reverse strands separately. Let $x_f$ and $m_f$ be the supporting and total unique molecules on the forward strand, and $x_r$ and $m_r$ be for the reverse strand. Under $H_t$, we expect $x_f$ to follow a binomial distribution with probability $p_t$ on $m_f$ molecules, and similarly for $x_r$. A significant deviation from a balanced distribution (e.g., $x_f \\gg x_r$ or vice versa) would substantially lower the joint likelihood $P(x_f, x_r|H_t)$ and thus reduce the posterior odds for a true variant. A filter that requires a minimum number of supporting reads on both strands, or rejects candidates failing a Fisher's exact test for strand bias, directly penalizes a known failure mode of the simple true-variant model, thus increasing specificity.\n\n2.  Stutter Ladder Symmetry Filter: The model describes slippage as a single event creating a $\\pm 1$ indel with probability $q(L)$. However, the process is recursive. A native DNA molecule with a homopolymer of length $L$ can produce artifactual reads of length $L-1$. A true variant molecule, which already has a homopolymer of length $L-1$, will itself be subject to slippage, producing artifactual reads of length $L-2$ with probability $q(L-1)$. An observation is therefore best explained by a hypothesis that accounts for the entire \"ladder\" of observed read lengths. Under the artifact hypothesis ($H_a$), we expect to see a large number of reads for the wild-type allele (length $L$) and a smaller number of artifactual reads at length $L-1$. Under the true-variant hypothesis ($H_t$), we expect to see large read counts for both the wild-type ($L$) and true variant ($L-1$) alleles, but also artifactual stutter reads at length $L-2$ created from the true variant templates. The absence of a stutter ladder symmetric with the main allelic peaks (i.e., observing stutter at $L-1$ but nothing at $L-2$) is inconsistent with the presence of a true $L-1$ allele. A filter based on this principle would analyze the distribution of molecule lengths around the candidate indel. It would require the observed pattern to be more consistent with a mixture of two alleles (wild-type and variant), each with its own stutter profile, than with a single wild-type allele producing a simple stutter artifact. This provides discriminatory power beyond simply counting reads for the primary indel.",
            "answer": "$$\\boxed{12}$$"
        }
    ]
}