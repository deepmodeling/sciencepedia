{
    "hands_on_practices": [
        {
            "introduction": "Before delving into methods of error correction, it is essential to first quantify the problem we aim to solve. This exercise provides a foundational understanding of the background noise in sequencing data by asking you to calculate the expected number of false positive reads at a given genomic site . Mastering this simple but critical calculation helps in appreciating why raw sequencing data is often insufficient for detecting rare variants and sets the stage for the error-correction techniques that follow.",
            "id": "5113749",
            "problem": "In a circulating tumor DNA assay used in molecular and immunodiagnostics, imagine a single genomic locus that is truly homozygous for the reference base. Sequencing is performed with an error-corrected workflow that reduces errors to an effective per-base substitution error rate of $p$ per read at that locus. Assume a symmetric substitution error model: when a substitution error occurs at the locus in a read, it results in one of the three non-reference nucleotides with equal probability, and insertion-deletion errors are negligible. The total read depth at the locus is $D$, and reads are independent conditional on the true base. A variant caller evaluates a pre-specified single-nucleotide variant (SNV; single nucleotide variant) corresponding to a specific alternate base at this locus and counts a read as variant-supporting if and only if the read reports this specific alternate base at the locus.\n\nStarting from first principles of probability and expectation, derive an analytical expression in terms of $D$ and $p$ for the expected number of false positive variant-supporting reads at this locus (i.e., reads that support the pre-specified alternate base solely due to sequencing substitution error, given that the true base is the reference). Express your final answer as a closed-form expression in $D$ and $p$. Do not round your answer.",
            "solution": "The problem statement is first validated for scientific and logical integrity.\n\n**Step 1: Extract Givens**\n- The genomic locus is truly homozygous for the reference base.\n- The effective per-base substitution error rate is $p$.\n- The substitution error model is symmetric: if a substitution error occurs, it results in one of the three non-reference nucleotides with equal probability.\n- Insertion-deletion errors are negligible.\n- The total read depth at the locus is $D$.\n- Reads are independent conditional on the true base.\n- A pre-specified single-nucleotide variant (SNV) corresponds to a specific alternate base.\n- A read is counted as variant-supporting if and only if it reports this specific alternate base at the locus.\n- The objective is to derive the expected number of false positive variant-supporting reads.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, well-posed, and objective. It describes a standard probabilistic model used in bioinformatics and molecular diagnostics to quantify noise in sequencing data. The parameters ($p$, $D$) and the error model are clearly defined. The question asks for a derivable quantity (an expectation) based on these parameters. The problem is self-contained, consistent, and does not violate any scientific principles or contain ambiguities. It is a formalizable problem directly relevant to error-corrected sequencing.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A reasoned solution will be provided.\n\n**Derivation of the Solution**\nThe goal is to find the expected number of reads that erroneously report a specific alternate base at a locus where the true base is the reference. Such reads are, by definition, false positives.\n\nLet us begin by considering a single read. We need to determine the probability that this single read is a false positive variant-supporting read. Let this probability be denoted by $q$.\n\nThe problem states that for a read to be counted as variant-supporting, it must report a *specific* pre-specified alternate base. Since the true base at the locus is the reference base, this can only happen if a sequencing error occurs.\n\nThe overall probability of a per-base substitution error at the locus for any given read is given as $p$.\n\nThe substitution error model is symmetric. This means that if a substitution error occurs, the machine will report one of the three possible non-reference bases with equal probability. Let the set of bases be $\\{A, C, G, T\\}$. If the reference base is, for instance, $A$, then the three non-reference bases are $\\{C, G, T\\}$. The SNV of interest corresponds to exactly one of these, say $G$.\n\nTherefore, for a single read to be a false positive supporting this specific SNV, two conditions must be met:\n1.  A substitution error must occur. The probability of this event is $P(\\text{error}) = p$.\n2.  Given that a substitution error has occurred, the error must be to the specific alternate base of interest. Since there are three possible non-reference bases, and each is equally likely upon an error, the conditional probability of this event is $P(\\text{specific alternate} | \\text{error}) = \\frac{1}{3}$.\n\nThe probability $q$ that a single read is a false positive supporting the specific variant is the joint probability of these two events. Since the type of error is independent of the occurrence of the error, we can multiply their probabilities:\n$$q = P(\\text{error} \\cap \\text{specific alternate}) = P(\\text{error}) \\times P(\\text{specific alternate} | \\text{error})$$\n$$q = p \\times \\frac{1}{3} = \\frac{p}{3}$$\n\nNow, we must consider the total of $D$ independent reads at the locus. Let $X_i$ be an indicator random variable for the $i$-th read, where $i$ ranges from $1$ to $D$. The variable $X_i$ is defined as:\n$$\nX_i =\n\\begin{cases}\n1 & \\text{if read } i \\text{ is a false positive variant-supporting read} \\\\\n0 & \\text{otherwise}\n\\end{cases}\n$$\n\nThe probability that $X_i=1$ is $q = \\frac{p}{3}$, and the probability that $X_i=0$ is $1-q = 1-\\frac{p}{3}$.\n\nThe total number of false positive variant-supporting reads in the set of $D$ reads is the random variable $X = \\sum_{i=1}^{D} X_i$. We are asked to find the expected value of $X$, denoted $E[X]$.\n\nA fundamental property of expectation is its linearity. The expectation of a sum of random variables is the sum of their individual expectations, regardless of whether they are independent.\n$$E[X] = E\\left[\\sum_{i=1}^{D} X_i\\right] = \\sum_{i=1}^{D} E[X_i]$$\n\nThe expectation of an indicator random variable $X_i$ is simply the probability of the event it indicates:\n$$E[X_i] = 1 \\cdot P(X_i=1) + 0 \\cdot P(X_i=0) = P(X_i=1) = q = \\frac{p}{3}$$\n\nSince each read is subject to the same error process, the expectation is the same for all $i$.\n$$E[X_i] = \\frac{p}{3} \\quad \\text{for all } i=1, \\dots, D$$\n\nSubstituting this back into the sum:\n$$E[X] = \\sum_{i=1}^{D} \\frac{p}{3}$$\n\nThis is a sum of $D$ identical terms. Therefore, the expected number of false positive variant-supporting reads is:\n$$E[X] = D \\times \\frac{p}{3} = \\frac{Dp}{3}$$\n\nThis result is derived from first principles, using the definition of the event, the chain rule for probability, and the linearity of expectation.",
            "answer": "$$\\boxed{\\frac{Dp}{3}}$$"
        },
        {
            "introduction": "This practice explores the core mechanism of error suppression using Unique Molecular Identifiers (UMIs), which allow for the grouping of sequencing reads originating from a single parent molecule. You will derive the probability that a consensus sequence, determined by a majority vote among reads in a UMI family, is incorrect . This exercise provides direct, quantitative insight into how consensus-based approaches can dramatically reduce the error rate, transforming noisy data into a high-fidelity signal.",
            "id": "5113757",
            "problem": "In error-corrected sequencing for rare variant detection within molecular and immunodiagnostics, unique molecular identifiers (UMIs) are used to tag original template molecules prior to amplification and sequencing in next-generation sequencing (NGS). Consider a single UMI family originating from one true template base, and suppose that each of the $k$ reads of this family reports the base independently and identically, with a per-read base-substitution error probability $p$, and no other error modes are operative. A consensus base for the family is called by a simple majority vote rule over the $k$ reads. Assume $k$ is odd, so ties cannot occur. Under these assumptions, derive from first principles an analytical expression for the probability that the consensus base is incorrect as a function of $k$ and $p$, starting from the definition of independent Bernoulli trials and the binomial distribution.\n\nThen, for a UMI family size of $k=7$ and per-read error probability $p=0.01$, evaluate the consensus error probability. Round your final numerical answer to four significant figures. Provide the final answer as a pure number without any units.",
            "solution": "The user-provided problem is first subjected to a rigorous validation process.\n\n### Step 1: Extract Givens\n- A single UMI family originates from one true template base.\n- There are $k$ reads in the family.\n- Each read reports the base independently and identically.\n- The per-read base-substitution error probability is $p$.\n- No other error modes are operative.\n- Consensus base is called by a simple majority vote.\n- $k$ is an odd integer.\n- An analytical expression for the probability of an incorrect consensus base is to be derived from first principles.\n- The derivation must start from the definition of independent Bernoulli trials and the binomial distribution.\n- The derived expression must be evaluated for $k=7$ and $p=0.01$.\n- The final numerical answer must be rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientific Grounding**: The problem is scientifically grounded. The model described is a standard and fundamental simplification used in molecular diagnostics and bioinformatics to understand error correction using Unique Molecular Identifiers (UMIs). The use of the binomial distribution to model independent error events is a core concept in this field.\n2.  **Well-Posedness**: The problem is well-posed. All necessary parameters ($k$, $p$) and conditions (independent and identical reads, majority vote) are clearly defined. The constraint that $k$ is odd is crucial as it prevents ties in the majority vote, ensuring that a unique consensus base is always called. This leads to a unique, computable probability.\n3.  **Objectivity**: The problem is stated in objective, formal language, free from subjective or ambiguous terms.\n4.  **Completeness and Consistency**: The problem is self-contained and consistent. It provides all information required to derive the analytical expression and compute the numerical value. There are no contradictions in the provided data. The values $k=7$ and $p=0.01$ are realistic for next-generation sequencing applications.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. It is scientifically sound, well-posed, and all necessary information is provided. The solution process will now proceed.\n\n### Derivation of the Analytical Expression\n\nLet us model the state of each of the $k$ reads as a Bernoulli trial. For a given read, let the outcome be an \"error\" if the read reports a base different from the true template base, and \"correct\" otherwise. The problem states that the probability of a base-substitution error is $p$. Therefore, for a single trial:\n- The probability of an \"error\" is $p$.\n- The probability of being \"correct\" is $1-p$.\n\nWe have $k$ such reads, which are stipulated to be independent and identical. Let $X$ be the random variable representing the total number of errors (incorrect reads) among the $k$ reads. Since the trials are independent and have the same error probability, $X$ follows a binomial distribution with parameters $k$ (number of trials) and $p$ (probability of success, i.e., an error). We write this as $X \\sim \\text{Binomial}(k, p)$.\n\nThe probability mass function (PMF) of the binomial distribution gives the probability of observing exactly $i$ errors in $k$ trials:\n$$\nP(X=i) = \\binom{k}{i} p^i (1-p)^{k-i}\n$$\nwhere $\\binom{k}{i} = \\frac{k!}{i!(k-i)!}$ is the binomial coefficient, representing the number of ways to choose $i$ error-containing reads from a total of $k$ reads. This formula is valid for $i = 0, 1, 2, \\dots, k$.\n\nA consensus base for the UMI family is determined by a simple majority vote. An incorrect consensus will be called if the number of reads with an error is greater than the number of correct reads. Let $i$ be the number of reads with an error. The number of correct reads is then $k-i$. The condition for an incorrect consensus is:\n$$\ni > k-i\n$$\n$$\n2i > k\n$$\n$$\ni > \\frac{k}{2}\n$$\nThe problem specifies that $k$ is an odd integer. Let $k = 2m+1$ for some non-negative integer $m$. The condition for an incorrect consensus becomes $i > \\frac{2m+1}{2} = m + \\frac{1}{2}$. Since $i$ must be an integer, this is equivalent to $i \\ge m+1$. Noting that $m = \\frac{k-1}{2}$, the minimum number of errors required for an incorrect call is $i_{min} = m+1 = \\frac{k-1}{2} + 1 = \\frac{k+1}{2}$.\n\nThe probability of an incorrect consensus, denoted $P_{\\text{error}}$, is the probability that the number of errors $X$ is greater than $k/2$. This is the sum of the probabilities of all possible numbers of errors that satisfy this condition, which are $i = \\frac{k+1}{2}, \\frac{k+1}{2}+1, \\dots, k$.\nTherefore, the analytical expression for the consensus error probability is:\n$$\nP_{\\text{error}} = P\\left(X \\ge \\frac{k+1}{2}\\right) = \\sum_{i=\\frac{k+1}{2}}^{k} P(X=i)\n$$\nSubstituting the binomial PMF, we obtain the final expression:\n$$\nP_{\\text{error}}(k, p) = \\sum_{i=\\frac{k+1}{2}}^{k} \\binom{k}{i} p^i (1-p)^{k-i}\n$$\nThis completes the derivation from first principles.\n\n### Evaluation for Specific Parameters\n\nWe are asked to evaluate this probability for a UMI family size of $k=7$ and a per-read error probability of $p=0.01$.\nFor $k=7$, the minimum number of errors for an incorrect consensus is $\\frac{7+1}{2} = 4$. Thus, an incorrect consensus occurs if there are $4$, $5$, $6$, or $7$ errors. We must calculate $P(X \\ge 4)$.\n\nThe probability is given by the sum:\n$$\nP_{\\text{error}} = P(X=4) + P(X=5) + P(X=6) + P(X=7)\n$$\nUsing the derived formula with $k=7$, $p=0.01$, and $1-p=0.99$:\n$$\nP_{\\text{error}} = \\sum_{i=4}^{7} \\binom{7}{i} (0.01)^i (0.99)^{7-i}\n$$\nWe calculate each term:\n- For $i=4$: $\\binom{7}{4} (0.01)^4 (0.99)^3 = \\frac{7!}{4!3!} (10^{-2})^4 (0.99)^3 = 35 \\times 10^{-8} \\times (0.970299) \\approx 3.3960465 \\times 10^{-7}$\n- For $i=5$: $\\binom{7}{5} (0.01)^5 (0.99)^2 = \\frac{7!}{5!2!} (10^{-2})^5 (0.99)^2 = 21 \\times 10^{-10} \\times (0.9801) \\approx 2.05821 \\times 10^{-9}$\n- For $i=6$: $\\binom{7}{6} (0.01)^6 (0.99)^1 = \\frac{7!}{6!1!} (10^{-2})^6 (0.99)^1 = 7 \\times 10^{-12} \\times (0.99) \\approx 6.93 \\times 10^{-12}$\n- For $i=7$: $\\binom{7}{7} (0.01)^7 (0.99)^0 = \\frac{7!}{7!0!} (10^{-2})^7 (1) = 1 \\times 10^{-14}$\n\nSumming these terms:\n$$\nP_{\\text{error}} \\approx 3.3960465 \\times 10^{-7} + 2.05821 \\times 10^{-9} + 6.93 \\times 10^{-12} + 1 \\times 10^{-14}\n$$\n$$\nP_{\\text{error}} \\approx 3.3960465 \\times 10^{-7} + 0.0205821 \\times 10^{-7} + 0.0000693 \\times 10^{-7} + 0.0000001 \\times 10^{-7}\n$$\n$$\nP_{\\text{error}} \\approx 3.4166979 \\times 10^{-7}\n$$\nRounding the result to four significant figures, we get $3.417 \\times 10^{-7}$.",
            "answer": "$$\\boxed{3.417 \\times 10^{-7}}$$"
        },
        {
            "introduction": "Bridging theory and practice, this final exercise demonstrates how the high-fidelity data from error-corrected sequencing is used to make clinically relevant decisions. You are tasked with developing and implementing a change-point detection algorithm to analyze a time series of circulating tumor DNA (ctDNA) data, a common scenario in cancer monitoring . By applying maximum likelihood estimation and a likelihood ratio test, you will learn to distinguish a true biological change from random fluctuation, simulating the process of translating raw data into an actionable diagnostic insight.",
            "id": "5113782",
            "problem": "You are given longitudinal observations from error-corrected sequencing of circulating tumor DNA (ctDNA) using Unique Molecular Identifier (UMI) consensus reads. At each time point indexed by an integer $t \\in \\{1,2,\\dots,T\\}$, you observe the total number of UMI consensus molecules covering a locus, denoted $N_t$, and the number of those UMIs that are classified as mutant, denoted $x_t$. The unknown biological quantity of interest at time $t$ is the true mutant allele fraction $\\theta_t \\in [0,1]$, defined as the fraction of original template molecules that carry the mutation at the locus.\n\nIn error-corrected sequencing with UMIs, each UMI consensus read is a classification outcome that can be modeled as a Bernoulli random variable reflecting whether the consensus is called mutant or wild-type. Let the per-UMI false positive rate be $r_{\\mathrm{fp}} \\in [0,1]$ (probability that a truly wild-type UMI is called mutant) and the per-UMI false negative rate be $r_{\\mathrm{fn}} \\in [0,1]$ (probability that a truly mutant UMI is called wild-type). Assume that within a given time segment the true allele fraction is constant, and that UMIs are independent conditional on $\\theta_t$.\n\nFundamental base for modeling:\n- Each UMI consensus at time $t$ is generated from a true mutant molecule with probability $\\theta_t$ and from a true wild-type molecule with probability $1-\\theta_t$.\n- A truly mutant UMI is observed as mutant with probability $1 - r_{\\mathrm{fn}}$.\n- A truly wild-type UMI is observed as mutant with probability $r_{\\mathrm{fp}}$.\n- Given $N_t$ UMIs at time $t$, the observed mutant count $x_t$ is modeled as a Binomial random variable with parameter $N_t$ and a success probability that must be derived from the above ingredients.\n\nChange-point model:\n- Under the null model (no change), the true allele fraction is constant across all time points, i.e., $\\theta_t = \\theta$ for all $t$.\n- Under the alternative model (single change), there exists an index $k \\in \\{1,2,\\dots,T-1\\}$ such that $\\theta_t = \\theta_1$ for $t \\le k$ and $\\theta_t = \\theta_2$ for $t \\ge k+1$.\n- For disease progression detection, you must decide that a progression has occurred only if both of the following are true for the maximum-likelihood change-point: (i) the later-segment allele fraction exceeds the earlier-segment allele fraction by at least a specified minimal increase $\\Delta_{\\min} > 0$, and (ii) a specified log-likelihood ratio threshold $\\Lambda > 0$ is exceeded.\n\nYour tasks:\n1. Starting from the modeling statements above, derive the Bernoulli success probability at time $t$ as a function of $\\theta_t$, $r_{\\mathrm{fp}}$, and $r_{\\mathrm{fn}}$. Then, using the Binomial likelihood, set up the log-likelihood for the null and single-change models.\n2. Using maximum likelihood estimation under each model, compute the maximum log-likelihood under the null model and, for each possible change-point $k \\in \\{1,\\dots,T-1\\}$, the maximum log-likelihood under the alternative model with a change at $k$. The parameter space must respect that the observable Bernoulli success probability induced by $\\theta$ is bounded within $[r_{\\mathrm{fp}}, 1 - r_{\\mathrm{fn}}]$.\n3. Compute the log-likelihood ratio for the best single change-point as $2$ times the difference between the alternative and null maximum log-likelihoods. Let $\\widehat{k}$ be the index that maximizes this ratio, and let $\\widehat{\\theta}_1$ and $\\widehat{\\theta}_2$ be the corresponding maximum-likelihood allele fractions for the earlier and later segments, respectively.\n4. Apply the progression decision rule: declare progression only if the maximum log-likelihood ratio is at least $\\Lambda$ and $\\widehat{\\theta}_2 - \\widehat{\\theta}_1 \\ge \\Delta_{\\min}$. If progression is declared, output the change-point index $\\widehat{k}$ (using $1$-based indexing, meaning the change is between time $\\widehat{k}$ and $\\widehat{k}+1$). Otherwise, output $-1$.\n\nNumerical stability requirements:\n- When evaluating any logarithm of a probability, clamp the probability to the closed interval $\\left[\\varepsilon, 1-\\varepsilon\\right]$ with $\\varepsilon = 10^{-12}$ to avoid taking $\\log(0)$.\n\nTest suite and parameters:\n- Use the following constants for all test cases: $r_{\\mathrm{fp}} = 10^{-4}$, $r_{\\mathrm{fn}} = 2 \\times 10^{-2}$, $\\Delta_{\\min} = 3 \\times 10^{-4}$, and $\\Lambda = 10$.\n- There are four independent test cases. For each test case $i \\in \\{1,2,3,4\\}$, you are given the time series lengths $T^{(i)}$, the vectors $(N^{(i)}_t)_{t=1}^{T^{(i)}}$, and $(x^{(i)}_t)_{t=1}^{T^{(i)}}$:\n\nTest case $1$ (clear increase):\n- $T^{(1)} = 8$\n- $N^{(1)} = (10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000)$\n- $x^{(1)} = (6, 5, 7, 6, 49, 52, 51, 48)$\n\nTest case $2$ (no change; near background):\n- $T^{(2)} = 6$\n- $N^{(2)} = (15000, 15000, 15000, 15000, 15000, 15000)$\n- $x^{(2)} = (1, 2, 2, 1, 3, 0)$\n\nTest case $3$ (subtle increase near limit of detection):\n- $T^{(3)} = 10$\n- $N^{(3)} = (8000, 9500, 7000, 12000, 10000, 11000, 9000, 10000, 10000, 12000)$\n- $x^{(3)} = (3, 3, 2, 4, 3, 8, 6, 7, 7, 8)$\n\nTest case $4$ (decrease; should not be called progression):\n- $T^{(4)} = 7$\n- $N^{(4)} = (10000, 10000, 10000, 10000, 10000, 10000, 10000)$\n- $x^{(4)} = (85, 79, 82, 80, 12, 10, 9)$\n\nFinal output format:\n- Your program should process the four test cases above and produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, output a single integer: the detected change-point index $\\widehat{k}$ (using $1$-based indexing) if progression is declared, or $-1$ otherwise. For example, a valid output line has the form $[a,b,c,d]$ where each of $a,b,c,d$ is an integer as specified.",
            "solution": "The problem requires the development of a change-point detection algorithm for longitudinal circulating tumor DNA (ctDNA) sequencing data. The goal is to determine if a statistically significant increase in the true mutant allele fraction, $\\theta_t$, has occurred. This involves modeling the sequencing data, performing maximum likelihood estimation (MLE) under null and alternative hypotheses, and using a likelihood ratio test combined with an effect size constraint to make a decision.\n\nFirst, we derive the probability of observing a mutant Unique Molecular Identifier (UMI) consensus read. Let $p_t$ be the probability that a UMI consensus read at time $t$ is classified as mutant. This event can occur in two mutually exclusive ways: either the original DNA molecule was mutant and it was correctly identified, or the molecule was wild-type and it was incorrectly identified as mutant (a false positive). Using the law of total probability, we can express $p_t$ as a function of the true mutant allele fraction $\\theta_t$, the per-UMI false positive rate $r_{\\mathrm{fp}}$, and the per-UMI false negative rate $r_{\\mathrm{fn}}$.\n\nThe probability of a UMI originating from a true mutant molecule is $\\theta_t$, and the probability of it originating from a true wild-type molecule is $1 - \\theta_t$. A true mutant is observed as mutant with probability $1 - r_{\\mathrm{fn}}$, and a true wild-type is observed as mutant with probability $r_{\\mathrm{fp}}$. Therefore, $p_t$ is given by:\n$$p_t = P(\\text{observed mutant}) = P(\\text{observed mutant} | \\text{true mutant})P(\\text{true mutant}) + P(\\text{observed mutant} | \\text{true wild-type})P(\\text{true wild-type})$$\n$$p_t = (1 - r_{\\mathrm{fn}})\\theta_t + r_{\\mathrm{fp}}(1 - \\theta_t)$$\nRearranging this linear relationship gives:\n$$p_t = r_{\\mathrm{fp}} + (1 - r_{\\mathrm{fp}} - r_{\\mathrm{fn}})\\theta_t$$\nThe problem specifies that the observed UMI counts $x_t$ at time $t$ follow a Binomial distribution, $x_t \\sim \\text{Binomial}(N_t, p_t)$, given the total UMI count $N_t$. The log-likelihood for a single observation $(x_t, N_t)$, omitting the constant binomial coefficient $\\log\\binom{N_t}{x_t}$ which does not affect maximization, is:\n$$\\mathcal{L}(p_t | x_t, N_t) \\propto x_t \\log(p_t) + (N_t - x_t) \\log(1 - p_t)$$\n\nNext, we define the procedure for maximum likelihood estimation. For a given segment of time points $S$, where the true allele fraction $\\theta$ is assumed to be constant, the total log-likelihood is the sum over the individual log-likelihoods:\n$$\\mathcal{L}(\\theta; S) = \\sum_{t \\in S} \\left[ x_t \\log(p) + (N_t - x_t) \\log(1 - p) \\right]$$\nwhere $p = r_{\\mathrm{fp}} + (1 - r_{\\mathrm{fp}} - r_{\\mathrm{fn}})\\theta$. This is equivalent to the log-likelihood of a single Binomial experiment with $\\sum_{t \\in S} x_t$ successes in $\\sum_{t \\in S} N_t$ trials. The maximum likelihood estimate for the observed probability $p$ is the pooled frequency:\n$$\\widehat{p} = \\frac{\\sum_{t \\in S} x_t}{\\sum_{t \\in S} N_t}$$\nThe problem states that the parameter space for the observable probability $p$ is $[r_{\\mathrm{fp}}, 1 - r_{\\mathrm{fn}}]$, corresponding to $\\theta \\in [0, 1]$. Therefore, the MLE $\\widehat{p}$ must be constrained to this interval. Let $\\widehat{p}_{\\text{raw}} = (\\sum x_t) / (\\sum N_t)$. Then the constrained MLE is $\\widehat{p} = \\max(r_{\\mathrm{fp}}, \\min(1 - r_{\\mathrm{fn}}, \\widehat{p}_{\\text{raw}}))$.\nFrom this, we find the MLE for the true allele fraction, $\\widehat{\\theta}$, by inverting the linear relationship:\n$$\\widehat{\\theta} = \\frac{\\widehat{p} - r_{\\mathrm{fp}}}{1 - r_{\\mathrm{fp}} - r_{\\mathrm{fn}}}$$\nThis procedure allows us to find the maximum log-likelihood for any segment of data by first computing $\\widehat{p}$ and then evaluating the log-likelihood function using this estimate. For numerical stability, we clamp the argument of any logarithm to the interval $[\\varepsilon, 1-\\varepsilon]$, with $\\varepsilon = 10^{-12}$.\n\nWe now apply this framework to the null and alternative models.\nUnder the null model ($H_0$), the allele fraction is constant across all time points, $\\theta_t = \\theta$ for $t \\in \\{1, \\dots, T\\}$. We compute the MLE $\\widehat{p}_{\\text{null}}$ by pooling all data: $S = \\{1, \\dots, T\\}$. The maximum log-likelihood under the null model is:\n$$\\mathcal{L}_{\\text{null, max}} = \\mathcal{L}(\\widehat{\\theta}_{\\text{null}}; S=\\{1, \\dots, T\\}) = \\sum_{t=1}^{T} \\left[ x_t \\log(\\widehat{p}_{\\text{null}}) + (N_t - x_t) \\log(1 - \\widehat{p}_{\\text{null}}) \\right]$$\n\nUnder the alternative model ($H_1$), there is a single change-point $k \\in \\{1, \\dots, T-1\\}$. For each possible $k$, the data is split into two segments: $S_1 = \\{1, \\dots, k\\}$ and $S_2 = \\{k+1, \\dots, T\\}$. We assume $\\theta_t = \\theta_1$ for $t \\in S_1$ and $\\theta_t = \\theta_2$ for $t \\in S_2$. We perform MLE independently for each segment to find $\\widehat{p}_{1,k}$ and $\\widehat{p}_{2,k}$. The total maximum log-likelihood for a given $k$ is the sum of the maximum log-likelihoods for the two segments:\n$$\\mathcal{L}_{\\text{alt, max}}(k) = \\left( \\sum_{t=1}^{k} \\left[x_t \\log(\\widehat{p}_{1,k}) + (N_t - x_t) \\log(1 - \\widehat{p}_{1,k})\\right] \\right) + \\left( \\sum_{t=k+1}^{T} \\left[x_t \\log(\\widehat{p}_{2,k}) + (N_t - x_t) \\log(1 - \\widehat{p}_{2,k})\\right] \\right)$$\nWe find the best change-point $\\widehat{k}$ by maximizing this quantity over all possible $k$:\n$$\\widehat{k} = \\arg\\max_{k \\in \\{1, \\dots, T-1\\}} \\mathcal{L}_{\\text{alt, max}}(k)$$\nThe overall maximum log-likelihood for the alternative model is $\\mathcal{L}_{\\text{alt, max}} = \\mathcal{L}_{\\text{alt, max}}(\\widehat{k})$.\n\nThe log-likelihood ratio (LRT) statistic is computed as:\n$$\\text{LRT} = 2 \\times (\\mathcal{L}_{\\text{alt, max}} - \\mathcal{L}_{\\text{null, max}})$$\nThis statistic quantifies the evidence for the single change-point model over the no-change model.\n\nFinally, the decision rule for declaring a disease progression is applied. A progression is detected if and only if both of the following conditions are met for the best change-point $\\widehat{k}$:\n1. The LRT exceeds a threshold: $\\text{LRT} \\ge \\Lambda$.\n2. The estimated increase in allele fraction meets a minimum threshold: $\\widehat{\\theta}_2 - \\widehat{\\theta}_1 \\ge \\Delta_{\\min}$, where $\\widehat{\\theta}_1$ and $\\widehat{\\theta}_2$ are the MLEs corresponding to the segments defined by $\\widehat{k}$.\nIf both conditions are satisfied, the output is the $1$-based index of the change-point, $\\widehat{k}$. Otherwise, no progression is declared, and the output is $-1$. This second condition ensures that we only detect increases in allele fraction, not decreases or changes that are too small to be clinically meaningful.",
            "answer": "```python\nimport numpy as np\n\n# Constants from the problem statement\nR_FP = 1e-4\nR_FN = 2e-2\nDELTA_MIN = 3e-4\nLAMBDA = 10.0\nEPSILON = 1e-12\n\ndef calculate_mle_and_loglik(x_segment, n_segment, r_fp, r_fn):\n    \"\"\"\n    Calculates the MLEs for p and theta, and the maximum log-likelihood for a data segment.\n    \n    Args:\n        x_segment (np.ndarray): Array of mutant counts.\n        n_segment (np.ndarray): Array of total counts.\n        r_fp (float): False positive rate.\n        r_fn (float): False negative rate.\n\n    Returns:\n        tuple: (theta_hat, max_log_likelihood)\n    \"\"\"\n    if np.sum(n_segment) == 0:\n        return 0.0, -np.inf\n\n    # Calculate raw MLE for the observed probability p by pooling data\n    p_hat_raw = np.sum(x_segment) / np.sum(n_segment)\n\n    # Clamp p_hat to the valid range [r_fp, 1 - r_fn]\n    p_hat_clamped = np.clip(p_hat_raw, r_fp, 1.0 - r_fn)\n\n    # Invert to find the MLE for the true allele fraction theta\n    denominator = 1.0 - r_fp - r_fn\n    theta_hat = (p_hat_clamped - r_fp) / denominator\n\n    # Clamp probability again for numerical stability before taking the logarithm\n    p_log = np.clip(p_hat_clamped, EPSILON, 1.0 - EPSILON)\n    \n    # Calculate the total log-likelihood for the segment\n    loglik = np.sum(\n        x_segment * np.log(p_log) + (n_segment - x_segment) * np.log(1.0 - p_log)\n    )\n\n    return theta_hat, loglik\n\ndef solve_case(N, x, r_fp, r_fn, delta_min, Lambda):\n    \"\"\"\n    Processes a single test case for change-point detection.\n    \n    Returns:\n        int: The 1-based change-point index if progression is detected, otherwise -1.\n    \"\"\"\n    T = len(N)\n    N = np.array(N, dtype=np.float64)\n    x = np.array(x, dtype=np.float64)\n\n    # 1. Null Model (H0: no change)\n    # Calculate the maximum log-likelihood assuming one constant allele fraction\n    _, loglik_null = calculate_mle_and_loglik(x, N, r_fp, r_fn)\n\n    # 2. Alternative Model (H1: one change-point)\n    # Find the best change-point by maximizing the log-likelihood\n    max_loglik_alt = -np.inf\n    best_k = -1\n    best_theta1 = 0.0\n    best_theta2 = 0.0\n\n    # Iterate through all possible change-points k (1-based index).\n    # k represents the last time point of the first segment.\n    for k in range(1, T):\n        # Segment 1: time points 1..k (0-indexed: 0..k-1)\n        x1, n1 = x[:k], N[:k]\n        # Segment 2: time points k+1..T (0-indexed: k..T-1)\n        x2, n2 = x[k:], N[k:]\n\n        theta1, loglik1 = calculate_mle_and_loglik(x1, n1, r_fp, r_fn)\n        theta2, loglik2 = calculate_mle_and_loglik(x2, n2, r_fp, r_fn)\n\n        current_loglik_alt = loglik1 + loglik2\n\n        if current_loglik_alt > max_loglik_alt:\n            max_loglik_alt = current_loglik_alt\n            best_k = k\n            best_theta1 = theta1\n            best_theta2 = theta2\n            \n    # 3. Decision Logic\n    if best_k == -1: # Handles case of T<=1, no possible change-point\n        return -1\n        \n    # Calculate the Log-Likelihood Ratio statistic\n    log_likelihood_ratio = 2 * (max_loglik_alt - loglik_null)\n\n    # Check both conditions for progression:\n    # 1. Statistical significance (LRT)\n    # 2. Effect size (magnitude of increase in theta)\n    is_lrt_significant = log_likelihood_ratio >= Lambda\n    is_increase_meaningful = (best_theta2 - best_theta1) >= delta_min\n\n    if is_lrt_significant and is_increase_meaningful:\n        return best_k\n    else:\n        return -1\n        \ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    # Test cases from the problem statement\n    test_cases = [\n        # Case 1: clear increase\n        (\n            [10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000],\n            [6, 5, 7, 6, 49, 52, 51, 48]\n        ),\n        # Case 2: no change; near background\n        (\n            [15000, 15000, 15000, 15000, 15000, 15000],\n            [1, 2, 2, 1, 3, 0]\n        ),\n        # Case 3: subtle increase\n        (\n            [8000, 9500, 7000, 12000, 10000, 11000, 9000, 10000, 10000, 12000],\n            [3, 3, 2, 4, 3, 8, 6, 7, 7, 8]\n        ),\n        # Case 4: decrease\n        (\n            [10000, 10000, 10000, 10000, 10000, 10000, 10000],\n            [85, 79, 82, 80, 12, 10, 9]\n        )\n    ]\n\n    results = []\n    for N, x in test_cases:\n        result = solve_case(N, x, R_FP, R_FN, DELTA_MIN, LAMBDA)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}