{"hands_on_practices": [{"introduction": "本练习聚焦于平均场博弈理论的基石：线性二次（LQ）模型。通过解决这个问题，你将实践在哈密顿-雅可比-贝尔曼（Hamilton-Jacobi-Bellman, HJB）框架内使用二次价值函数拟设这一基本技巧。这使你能够推导出最优反馈控制，从而具体理解个体最优决策是如何被自身状态和群体总体行为共同影响的。[@problem_id:2987064]", "problem": "考虑一个由连续统同质代理人组成的线性二次平均场博弈。对于一个代表性代理人，其受控状态过程 $(X_t)_{t \\in [0,T]}$ 遵循以下随机微分方程演化：\n$$\n\\mathrm{d}X_t = \\left(a X_t + b \\alpha_t + c m_t\\right)\\mathrm{d}t + \\sigma \\mathrm{d}W_t,\n$$\n其中 $a$、$b$、$c$ 和 $\\sigma$ 是实常数，$(W_t)_{t \\in [0,T]}$ 是一个标准布朗运动，$\\alpha_t$ 是代理人的控制，而 $m_t$ 表示总体均值 $m_t = \\mathbb{E}[X_t]$。代表性代理人旨在最小化以下期望二次成本：\n$$\nJ(\\alpha) = \\mathbb{E}\\Bigg[\\int_{0}^{T} \\left(q X_t^{2} + \\bar{q}\\left(X_t - m_t\\right)^{2} + r \\alpha_t^{2}\\right)\\mathrm{d}t + g X_T^{2} + \\bar{g}\\left(X_T - m_T\\right)^{2}\\Bigg],\n$$\n其中给定常数 $q>0$，$\\bar{q}\\geq 0$，$r>0$，$g\\geq 0$ 和 $\\bar{g}\\geq 0$。在求解代表性代理人问题时，假设平均场 $(m_t)_{t\\in[0,T]}$ 是一个给定的、关于时间的确定性函数。\n\n从动态规划原理和 Hamilton-Jacobi-Bellman (HJB) 方程出发，对价值函数使用如下形式的拟设：\n$$\nV(t,x) = \\tfrac{1}{2}P_t x^{2} + S_t m_t x + \\tfrac{1}{2}U_t m_t^{2} + \\phi_t,\n$$\n其中 $(P_t,S_t,U_t,\\phi_t)$ 是关于时间的确定性系数函数，其选择应使 $V$ 满足 HJB 方程及终端条件 $V(T,x) = g x^{2} + \\bar{g}\\left(x - m_T\\right)^{2}$。通过最小化 HJB 方程中关于控制的哈密顿量，推导出以函数 $P_t$ 和 $S_t$ 表示的反馈控制 $\\alpha_t$。你可以利用 HJB 方程中的系数匹配会得到关于 $(P_t,S_t)$ 的耦合 Riccati 系统这一事实，但你不需要显式求解该系统。请将你的最终答案表示为 $\\alpha_t$ 关于 $b$、$r$、$P_t$、$S_t$、$X_t$ 和 $m_t$ 的单个闭式解析表达式。无需进行数值计算。", "solution": "该问题是有效的，因为它提出了平均场博弈理论领域中的一个标准的、适定的问题，具体来说是一个线性二次 (LQ) 模型。所有参数和目标都已明确定义，并且在随机最优控制的背景下，其前提在科学上是合理的。\n\n目标是为代表性代理人找到最优反馈控制 $\\alpha_t$。代理人的状态 $X_t$ 遵循随机微分方程 (SDE)：\n$$\n\\mathrm{d}X_t = \\left(a X_t + b \\alpha_t + c m_t\\right)\\mathrm{d}t + \\sigma \\mathrm{d}W_t\n$$\n代理人旨在最小化一个二次成本泛函 $J(\\alpha)$。在平均场博弈框架中，第一步是求解代表性代理人的优化问题，假设平均场项 $m_t = \\mathbb{E}[X_t]$ 是一个已知的时间确定性函数。\n\n代理人问题的价值函数定义为：\n$$\nV(t,x) = \\min_{\\alpha} \\mathbb{E}\\Bigg[\\int_{t}^{T} \\left(q X_s^{2} + \\bar{q}\\left(X_s - m_s\\right)^{2} + r \\alpha_s^{2}\\right)\\mathrm{d}s + g X_T^{2} + \\bar{g}\\left(X_T - m_T\\right)^{2} \\Bigg| X_t=x \\Bigg]\n$$\n该价值函数 $V(t,x)$ 必须满足 Hamilton-Jacobi-Bellman (HJB) 偏微分方程。HJB 方程由下式给出：\n$$\n-\\frac{\\partial V}{\\partial t}(t,x) = \\min_{\\alpha \\in \\mathbb{R}} \\left\\{ L(t,x,\\alpha,m_t) + \\mathcal{L}^\\alpha V(t,x) \\right\\}\n$$\n其中 $L(t,x,\\alpha,m_t)$ 是瞬时成本，$\\mathcal{L}^\\alpha$ 是在控制 $\\alpha$ 下状态过程 $X_t$ 的无穷小生成元。\n\n瞬时成本为：\n$$\nL(t,x,\\alpha,m_t) = q x^{2} + \\bar{q}\\left(x - m_t\\right)^{2} + r \\alpha^{2}\n$$\n无穷小生成元 $\\mathcal{L}^\\alpha$ 应用于价值函数 $V(t,x)$ 的结果是：\n$$\n\\mathcal{L}^\\alpha V(t,x) = \\left(a x + b \\alpha + c m_t\\right) \\frac{\\partial V}{\\partial x}(t,x) + \\frac{1}{2}\\sigma^2 \\frac{\\partial^2 V}{\\partial x^2}(t,x)\n$$\nHJB 方程中最小化算子内部的项是哈密顿量 $\\mathcal{H}$，它依赖于状态 $x$、时间 $t$、控制 $\\alpha$ 以及价值函数的偏导数 $V_x = \\frac{\\partial V}{\\partial x}$ 和 $V_{xx} = \\frac{\\partial^2 V}{\\partial x^2}$。\n$$\n\\mathcal{H}(t, x, \\alpha, V_x, V_{xx}) = q x^{2} + \\bar{q}\\left(x - m_t\\right)^{2} + r \\alpha^{2} + \\left(a x + b \\alpha + c m_t\\right) V_x + \\frac{1}{2}\\sigma^2 V_{xx}\n$$\n因此，HJB 方程可以写成：\n$$\n-\\frac{\\partial V}{\\partial t}(t,x) = \\min_{\\alpha \\in \\mathbb{R}} \\mathcal{H}(t, x, \\alpha, V_x, V_{xx})\n$$\n为了找到最优控制 $\\alpha_t^*$，我们必须对于一个固定的状态 $(t,x)$，将哈密顿量关于 $\\alpha$ 进行最小化。我们可以重写哈密顿量以分离出包含 $\\alpha$ 的项：\n$$\n\\mathcal{H} = r \\alpha^{2} + b \\alpha V_x + \\left( q x^{2} + \\bar{q}\\left(x - m_t\\right)^{2} + (a x + c m_t) V_x + \\frac{1}{2}\\sigma^2 V_{xx} \\right)\n$$\n这是关于 $\\alpha$ 的二次函数。由于参数 $r > 0$，该抛物线开口向上，其最小值可以通过将其关于 $\\alpha$ 的导数设为零来找到。\n$$\n\\frac{\\partial \\mathcal{H}}{\\partial \\alpha} = 2r \\alpha + b V_x = 0\n$$\n对 $\\alpha$ 求解，得到以价值函数的空间导数表示的最优控制，记为 $\\alpha^*$：\n$$\n\\alpha^*(t,x) = -\\frac{b}{2r} \\frac{\\partial V}{\\partial x}(t,x)\n$$\n问题为价值函数提供了一个拟设：\n$$\nV(t,x) = \\frac{1}{2}P_t x^{2} + S_t m_t x + \\frac{1}{2}U_t m_t^{2} + \\phi_t\n$$\n其中 $P_t$、$S_t$、$U_t$ 和 $\\phi_t$ 是时间的确定性函数。我们计算此拟设关于 $x$ 的偏导数：\n$$\n\\frac{\\partial V}{\\partial x}(t,x) = P_t x + S_t m_t\n$$\n将此表达式代入最优控制 $\\alpha^*(t,x)$ 的公式中，得到：\n$$\n\\alpha^*(t,x) = -\\frac{b}{2r} \\left(P_t x + S_t m_t\\right)\n$$\n这就是在时间 $t$ 给定状态 $x$ 下的反馈形式的最优控制。对于在时间 $t$ 状态为 $X_t$ 的代表性代理人，其最优控制 $\\alpha_t$ 是通过将通用状态变量 $x$ 替换为随机变量 $X_t$ 得到的：\n$$\n\\alpha_t = -\\frac{b}{2r} \\left(P_t X_t + S_t m_t\\right)\n$$\n该表达式按要求提供了以状态 $X_t$、平均场 $m_t$、问题参数 $b$ 和 $r$ 以及来自价值函数拟设的系数函数 $P_t$ 和 $S_t$ 表示的最优控制 $\\alpha_t$。终端条件 $V(T,x) = g x^{2} + \\bar{g}\\left(x - m_T\\right)^{2}$ 提供了系数的终端值，例如 $P_T = 2(g+\\bar{g})$ 和 $S_T = -2\\bar{g}$，这些值将用于求解关于 $P_t$ 和 $S_t$ 的 Riccati 系统。然而，问题陈述并未要求求解此系统。", "answer": "$$\n\\boxed{-\\frac{b}{2r}(P_t X_t + S_t m_t)}\n$$", "id": "2987064"}, {"introduction": "本练习将视角从连续状态空间拓展到离散状态空间，探讨一个构建于简单网络上的平均场博弈。你将分析一个场景，其中个体通过控制在两个节点间的转移速率，以最小化包含拥堵效应的长期平均成本。这项实践将引入遍历HJB方程和“开关”式（bang-bang）控制的概念，展示了该框架在种群动态和排队模型等领域的广泛适用性。[@problem_id:2987138]", "problem": "考虑一个连续时间平均场博弈 (MFG)，其中有连续统的同质代理人，其个体状态为两点集 $\\{0,1\\}$ 中的一个节点。位于节点 $i \\in \\{0,1\\}$ 的代理人选择一个转移率 $u_i \\in [0,\\bar{u}_i]$ 以跳转到节点 $1-i$。在稳态马尔可夫控制 $u = (u_0,u_1)$ 下，该群体根据一个受控连续时间马尔可夫链 (CTMC) 进行演化，其生成元 $\\mathcal{L}^{u}$ 对任意有界函数 $f$ 定义为 $\\mathcal{L}^{u} f(i) = u_i \\big(f(1-i) - f(i)\\big)$。\n\n代理人寻求最小化遍历（长期平均）成本。在节点 $i$ 的瞬时运行成本是线性拥堵成本 $\\gamma_i m_i$ 加上线性控制成本 $\\alpha_i u_i$，其中 $m_i$ 是在节点 $i$ 的群体稳态比例。参数为\n$$\n\\gamma_0 = 0.6,\\quad \\gamma_1 = 1.4,\\quad \\alpha_0 = -1,\\quad \\alpha_1 = 0.5,\\quad \\bar{u}_0 = 3,\\quad \\bar{u}_1 = 2.\n$$\n一个平均场纳什均衡是一个点对 $(u^{\\ast},m^{\\ast})$，使得：\n- 对于稳态分布 $m^{\\ast} = (m_0^{\\ast},m_1^{\\ast})$，控制 $u^{\\ast}$ 最小化代理人的遍历 Hamilton–Jacobi–Bellman (HJB) 方程，其中值函数为 $V = (V_0,V_1)$，遍历常数为 $\\lambda$。\n- 稳态分布 $m^{\\ast}$ 与由 $u^{\\ast}$ 驱动的 CTMC 一致，即它满足稳态（全局平衡）条件。\n\n从这些定义和原理出发，推导该具有线性成本和有界转移率的两节点 CTMC 的遍历 HJB 最优性条件。然后，显式求解均衡转移率 $u_0^{\\ast}$ 和 $u_1^{\\ast}$ 以及稳态分布 $m_0^{\\ast}$ 和 $m_1^{\\ast}$。\n\n将最终答案表示为包含 $(u_0^{\\ast},\\,u_1^{\\ast},\\,m_0^{\\ast},\\,m_1^{\\ast})$ 的单行矩阵。无需四舍五入。", "solution": "该问题要求解一个两状态连续时间马尔可夫链 (CTMC) 模型的平均场纳什均衡。一个均衡是一个点对 $(u^{\\ast}, m^{\\ast})$，它由最优控制策略 $u^{\\ast} = (u_0^{\\ast}, u_1^{\\ast})$ 和一个一致的稳态群体分布 $m^{\\ast} = (m_0^{\\ast}, m_1^{\\ast})$ 组成，且满足两个主要条件：代理人最优性和群体一致性。\n\n**1. 代理人最优性：遍历 Hamilton-Jacobi-Bellman (HJB) 方程**\n\n对于单个代理人，群体分布 $m = (m_0, m_1)$ 被视为固定的。代理人的目标是选择一个控制策略 $u = (u_0, u_1)$ 来最小化其长期平均（遍历）成本。设 $V = (V_0, V_1)$ 为代表每个状态下总期望未来成本的值函数，并设 $\\lambda$ 为恒定遍历成本。最优性条件由遍历 Hamilton-Jacobi-Bellman (HJB) 方程给出。对于每个状态 $i \\in \\{0, 1\\}$，HJB 方程为：\n$$ \\lambda = \\min_{u_i \\in [0, \\bar{u}_i]} \\left\\{ \\text{instantaneous cost at } i + \\mathcal{L}^{u} V(i) \\right\\} $$\n在状态 $i$ 的瞬时成本是 $\\gamma_i m_i + \\alpha_i u_i$。CTMC 的生成元为 $\\mathcal{L}^{u} V(i) = u_i (V_{1-i} - V_i)$。将这些代入 HJB 系统，我们得到：\n$$ \\lambda = \\min_{u_0 \\in [0, \\bar{u}_0]} \\left\\{ \\gamma_0 m_0 + \\alpha_0 u_0 + u_0(V_1 - V_0) \\right\\} $$\n$$ \\lambda = \\min_{u_1 \\in [0, \\bar{u}_1]} \\left\\{ \\gamma_1 m_1 + \\alpha_1 u_1 + u_1(V_0 - V_1) \\right\\} $$\n为简化起见，我们定义相对值，或势，$P = V_1 - V_0$。HJB 方程可以重写为：\n$$ \\lambda = \\gamma_0 m_0 + \\min_{u_0 \\in [0, \\bar{u}_0]} \\left\\{ u_0 (\\alpha_0 + P) \\right\\} $$\n$$ \\lambda = \\gamma_1 m_1 + \\min_{u_1 \\in [0, \\bar{u}_1]} \\left\\{ u_1 (\\alpha_1 - P) \\right\\} $$\n由于控制 $u_i$ 在目标函数中是线性的，最优控制 $u_i^{\\ast}$ 由其系数的符号决定。这导致了一个“砰-砰”（bang-bang）或奇异控制策略。\n最优控制 $u_0^{\\ast}$ 是：\n$$ u_0^{\\ast}(P) = \\begin{cases} \\bar{u}_0 & \\text{if } \\alpha_0 + P  0 \\\\ \\text{any value in } [0, \\bar{u}_0]  \\text{if } \\alpha_0 + P = 0 \\\\ 0  \\text{if } \\alpha_0 + P > 0 \\end{cases} $$\n最优控制 $u_1^{\\ast}$ 是：\n$$ u_1^{\\ast}(P) = \\begin{cases} \\bar{u}_1  \\text{if } \\alpha_1 - P  0 \\\\ \\text{any value in } [0, \\bar{u}_1]  \\text{if } \\alpha_1 - P = 0 \\\\ 0  \\text{if } \\alpha_1 - P > 0 \\end{cases} $$\n\n**2. 群体一致性：稳态分布**\n\n稳态分布 $m^{\\ast} = (m_0^{\\ast}, m_1^{\\ast})$ 必须与最优控制 $u^{\\ast} = (u_0^{\\ast}, u_1^{\\ast})$ 引致的动力学一致。这由 CTMC 的稳态（或全局平衡）方程所描述，该方程指出从状态 0 到 1 的代理人流必须等于从状态 1 到 0 的流：\n$$ m_0^{\\ast} u_0^{\\ast} = m_1^{\\ast} u_1^{\\ast} $$\n结合群体比例之和必须为一的事实，$m_0^{\\ast} + m_1^{\\ast} = 1$，我们可以解出稳态分布。假设 $u_0^{\\ast} + u_1^{\\ast} > 0$，我们有：\n$$ m_0^{\\ast} = \\frac{u_1^{\\ast}}{u_0^{\\ast} + u_1^{\\ast}} \\quad \\text{and} \\quad m_1^{\\ast} = \\frac{u_0^{\\ast}}{u_0^{\\ast} + u_1^{\\ast}} $$\n\n**3. 求解平均场均衡**\n\n均衡是同时满足 HJB 方程和一致性条件的状态 $(u^{\\ast}, m^{\\ast}, P)$。我们可以通过令两个 HJB 方程相等来消去 $\\lambda$ 以找到均衡：\n$$ \\gamma_0 m_0^{\\ast} + u_0^{\\ast}(\\alpha_0 + P) = \\gamma_1 m_1^{\\ast} + u_1^{\\ast}(\\alpha_1 - P) $$\n给定参数为：$\\gamma_0 = 0.6$, $\\gamma_1 = 1.4$, $\\alpha_0 = -1$, $\\alpha_1 = 0.5$, $\\bar{u}_0 = 3$, $\\bar{u}_1 = 2$。\n决定控制策略的势 $P$ 的临界值为 $P = -\\alpha_0 = 1$ 和 $P = \\alpha_1 = 0.5$。这将对 $P$ 的搜索划分为几种情况。我们来分析均衡控制处于其最大值的情况，这发生在 $\\alpha_0 + P  0$ 和 $\\alpha_1 - P  0$ 时。这对应于区间 $P \\in (0.5, 1)$。\n\n在此候选情况下，最优控制为：\n- $u_0^{\\ast} = \\bar{u}_0 = 3$\n- $u_1^{\\ast} = \\bar{u}_1 = 2$\n\n使用这些控制，一致的稳态分布为：\n- $m_0^{\\ast} = \\frac{u_1^{\\ast}}{u_0^{\\ast} + u_1^{\\ast}} = \\frac{2}{3 + 2} = \\frac{2}{5}$\n- $m_1^{\\ast} = \\frac{u_0^{\\ast}}{u_0^{\\ast} + u_1^{\\ast}} = \\frac{3}{3 + 2} = \\frac{3}{5}$\n\n现在，我们必须验证这些值与关于 $P$ 的 HJB 等式是否一致。将 $u_0^{\\ast}, u_1^{\\ast}, m_0^{\\ast}, m_1^{\\ast}$ 代入该方程：\n$$ \\gamma_0 m_0^{\\ast} + \\bar{u}_0(\\alpha_0 + P) = \\gamma_1 m_1^{\\ast} + \\bar{u}_1(\\alpha_1 - P) $$\n代入数值：\n$$ (0.6)\\left(\\frac{2}{5}\\right) + 3(-1 + P) = (1.4)\\left(\\frac{3}{5}\\right) + 2(0.5 - P) $$\n$$ 0.24 - 3 + 3P = 0.84 + 1 - 2P $$\n$$ 3P - 2.76 = 1.84 - 2P $$\n$$ 5P = 1.84 + 2.76 $$\n$$ 5P = 4.6 $$\n$$ P = \\frac{4.6}{5} = 0.92 $$\n值 $P = 0.92$ 位于假设的区间 $(0.5, 1)$ 内，这证实了我们对控制策略的选择是正确的，并且解是自洽的。对其他情况（例如 $P  0.5$ 或 $P > 1$）的穷举分析会导致矛盾，从而证实此均衡是唯一的。\n\n因此，均衡转移率和稳态分布为：\n- $u_0^{\\ast} = 3$\n- $u_1^{\\ast} = 2$\n- $m_0^{\\ast} = \\frac{2}{5}$\n- $m_1^{\\ast} = \\frac{3}{5}$", "answer": "$$\n\\boxed{\n\\begin{pmatrix} 3  2  \\frac{2}{5}  \\frac{3}{5} \\end{pmatrix}\n}\n$$", "id": "2987138"}, {"introduction": "这项高级练习深入探讨了一个关键的理论问题：均衡的唯一性。你将在同一个线性二次（LQ）框架内，对比非合作的平均场博弈（MFG）与其合作的对应问题——平均场控制（或称社会计划者）问题。通过分析纳什均衡的定点方程，你将发现一个临界参数值，在该值下唯一性失效，导致无穷多个均衡的出现——这是计划者问题因其内在凸性而能避免的一种病态现象，深刻揭示了博弈论互动与中心化优化之间的结构性差异。[@problem_id:2987096]", "problem": "考虑一个由相同智能体组成的连续统，其状态由随机微分方程 $dX_t = u_t\\,dt + \\sigma\\,dW_t$ 在时间区间 $[0,T]$ 上所控制，其中 $W_t$ 是标准布朗运动，$\\sigma>0$ 是扩散系数，$u_t$ 是具有有限二次能量的循序可测控制。初始状态 $X_0$ 是可积的，其均值为 $m_0 := \\mathbb{E}[X_0]$。令 $\\bar{m}_t := \\mathbb{E}[X_t]$ 表示在时间 $t$ 的种群均值。\n\n每个智能体评估一个线性二次代价泛函\n$$\nJ(u;\\bar{m}_T) \\;=\\; \\mathbb{E}\\left[ \\frac{r}{2} \\int_0^T u_t^2\\,dt \\;+\\; \\frac{\\delta}{2}\\left(X_T - \\alpha\\,\\bar{m}_T\\right)^2 \\right],\n$$\n其中参数 $r>0$，$ \\delta>0$，$ \\alpha\\in\\mathbb{R}$。耦合项取决于终端种群均值 $\\bar{m}_T$。考虑两种表述：\n\n1. 平均场博弈 (MFG)：对于给定的 $\\bar{m}_T$，纳什均衡是一个最小化 $J(u;\\bar{m}_T)$ 的控制，并且其所引出的终端均值 $\\bar{m}_T$ 与由该最优控制生成的种群分布相一致。\n\n2. 平均场控制 (社会计划者问题)：为整个种群选择一个共同的控制，以最小化平均成本，这等价于最小化 $J(u;\\bar{m}_T)$，同时理解 $\\bar{m}_T$ 是由所选控制决定的。\n\n从第一性原理（平均场博弈中纳什均衡的定义、线性二次随机控制的结构，以及如柯西-施瓦茨不等式等基本不等式）出发，完成以下任务：\n\n(a) 推导在平均场博弈中终端均值 $\\bar{m}_T$ 满足的标量不动点方程，并用 $\\alpha$、$r$、$\\delta$、$T$ 和 $m_0$ 表示。\n\n(b) 证明即使当耦合项 $x\\mapsto \\frac{\\delta}{2}(x - \\alpha\\bar{m}_T)^2$ 不满足 Lasry–Lions 单调性条件时，平均场控制问题由于其严格凸性，仍存在唯一的终端均值 $\\bar{m}_T$。为 $\\alpha>0$ 的情况提供一个简要论证，以说明 Lasry–Lions 单调性条件的失效。\n\n(c) 考虑 $m_0 = 0$ 的特殊情况。确定精确的耦合强度 $\\alpha^{\\star}$，在该强度下，平均场博弈存在无穷多个纳什均衡（即 (a) 中的不动点方程对于 $\\bar{m}_T$ 有无穷多解），而平均场控制的最优解在相同数据下仍然是唯一的。将你的最终答案表示为 $\\alpha^{\\star}$ 的封闭形式解析表达式。", "solution": "用户提供了一个问题陈述，在尝试解答之前需要进行验证。\n\n### 步骤1：提取已知条件\n- 智能体状态动态：$dX_t = u_t\\,dt + \\sigma\\,dW_t$ 对 $t \\in [0,T]$。\n- $W_t$ 是标准布朗运动。\n- $\\sigma > 0$ 是扩散系数。\n- $u_t$ 是具有有限二次能量的循序可测控制。\n- 初始状态 $X_0$ 是可积的，其均值为 $m_0 := \\mathbb{E}[X_0]$。\n- 种群均值：$\\bar{m}_t := \\mathbb{E}[X_t]$。\n- 智能体代价泛函：$J(u;\\bar{m}_T) = \\mathbb{E}\\left[ \\frac{r}{2} \\int_0^T u_t^2\\,dt + \\frac{\\delta}{2}\\left(X_T - \\alpha\\,\\bar{m}_T\\right)^2 \\right]$。\n- 参数：$r>0$, $\\delta>0$, $\\alpha\\in\\mathbb{R}$。\n- 平均场博弈 (MFG)：对于给定的 $\\bar{m}_T$，纳什均衡是最优控制能产生一个一致的种群均值。\n- 平均场控制 (MFC)：选择一个共同控制来最小化平均成本 $J(u;\\bar{m}_T)$，其中 $\\bar{m}_T$ 由 $u$ 引出。\n\n### 步骤2：使用提取的已知条件进行验证\n该问题具有科学依据、良态且客观。\n- **科学合理性**：该问题是一个标准的线性二次 (LQ) 平均场博弈，这是随机微分方程和控制理论中一个成熟的课题。SDE、代价泛函和参数约束都是经典的。\n- **良态性**：该问题结构清晰，提供了推导所要求量所需的所有信息。任务 (a)、(b) 和 (c) 都是具体的，并能得出确定的数学答案。\n- **客观性**：该问题以精确的数学语言陈述，没有歧义或主观论断。\n\n### 步骤3：结论与行动\n问题有效。我现在将继续进行解答。\n\n这是一个线性二次 (LQ) 随机博弈，可以求得显式解。我们将按顺序解决每个部分。\n\n**(a) 平均场博弈不动点方程的推导**\n\n在 MFG 设置中，每个智能体针对一个*给定*的、固定的终端种群均值 $\\bar{m}_T$ 求解一个最优控制问题。智能体寻求最小化：\n$$J(u) = \\mathbb{E}\\left[ \\frac{r}{2} \\int_0^T u_t^2\\,dt + \\frac{\\delta}{2}\\left(X_T - \\alpha\\,\\bar{m}_T\\right)^2 \\right]$$\n在动态 $dX_t = u_t\\,dt + \\sigma\\,dW_t$ 的约束下。这是一个标准的 LQ 随机控制问题。我们使用针对 SDE 的庞特里亚金极大值原理。哈密顿量为：\n$$H(t, x, y, u) = \\frac{r}{2} u^2 + y u$$\n一阶最优性条件是 $\\frac{\\partial H}{\\partial u} = ru + y = 0$，这给出了最优控制的候选形式：\n$$u_t^* = -\\frac{1}{r} Y_t$$\n其中 $(Y_t, Z_t)$ 是由倒向随机微分方程 (BSDE) 控制的伴随过程：\n$$dY_t = -\\frac{\\partial H}{\\partial x}(t, X_t^*, Y_t, u_t^*)\\,dt + Z_t\\,dW_t = 0 \\cdot dt + Z_t dW_t = Z_t dW_t$$\n其终端条件由终端成本的导数给出：\n$$Y_T = \\frac{\\partial}{\\partial x} \\left[\\frac{\\delta}{2}(x - \\alpha\\bar{m}_T)^2\\right]\\bigg|_{x=X_T^*} = \\delta(X_T^* - \\alpha\\bar{m}_T)$$\n由于 $dY_t = Z_t dW_t$，$Y_t$ 是一个鞅，其期望值 $\\bar{y}_t := \\mathbb{E}[Y_t]$ 必须随时间保持不变。令 $\\bar{y} := \\mathbb{E}[Y_t]$ 对所有 $t \\in [0,T]$ 成立。\n对 $Y_T$ 的终端条件取期望：\n$$\\bar{y} = \\mathbb{E}[Y_T] = \\mathbb{E}[\\delta(X_T^* - \\alpha\\bar{m}_T)] = \\delta(\\mathbb{E}[X_T^*] - \\alpha\\bar{m}_T) = \\delta(\\bar{m}_T^* - \\alpha\\bar{m}_T)$$\n其中 $\\bar{m}_T^*$ 是由智能体的最优控制引出的终端均值。\n\n接下来，我们求解均值状态 $\\bar{m}_t^* = \\mathbb{E}[X_t^*]$ 的动态。对状态 SDE 取期望：\n$$d\\bar{m}_t^* = \\mathbb{E}[dX_t^*] = \\mathbb{E}[u_t^*]\\,dt + \\mathbb{E}[\\sigma\\,dW_t] = \\mathbb{E}[-\\frac{1}{r}Y_t]\\,dt = -\\frac{1}{r}\\bar{y}\\,dt$$\n从 $t=0$ 到 $t=T$ 积分：\n$$\\bar{m}_T^* - \\bar{m}_0 = \\int_0^T \\left(-\\frac{1}{r}\\bar{y}\\right) dt = -\\frac{T}{r}\\bar{y}$$\n已知 $\\bar{m}_0=m_0$，我们有 $\\bar{m}_T^* = m_0 - \\frac{T}{r}\\bar{y}$。\n\n至此，对于一个给定的预期均值 $\\bar{m}_T$，我们得到了两个关联结果均值 $\\bar{m}_T^*$ 和常数期望伴随值 $\\bar{y}$ 的方程：\n1. $\\bar{y} = \\delta(\\bar{m}_T^* - \\alpha\\bar{m}_T)$\n2. $\\bar{m}_T^* = m_0 - \\frac{T}{r}\\bar{y}$\n\nMFG 一致性条件要求预期均值等于结果均值：$\\bar{m}_T^* = \\bar{m}_T$。将此代入方程：\n1. $\\bar{y} = \\delta(\\bar{m}_T - \\alpha\\bar{m}_T) = \\delta(1-\\alpha)\\bar{m}_T$\n2. $\\bar{m}_T = m_0 - \\frac{T}{r}\\bar{y}$\n\n现在我们可以求出 $\\bar{m}_T$ 的不动点方程。将第一个方程中的 $\\bar{y}$ 代入第二个方程：\n$$\\bar{m}_T = m_0 - \\frac{T}{r} \\left( \\delta(1-\\alpha)\\bar{m}_T \\right)$$\n重排各项以求解 $\\bar{m}_T$：\n$$\\bar{m}_T + \\frac{\\delta T(1-\\alpha)}{r}\\bar{m}_T = m_0$$\n$$\\bar{m}_T \\left( 1 + \\frac{\\delta T(1-\\alpha)}{r} \\right) = m_0$$\n两边乘以 $r$：\n$$\\bar{m}_T \\left( r + \\delta T(1-\\alpha) \\right) = m_0 r$$\n这就是在平均场博弈中，终端均值 $\\bar{m}_T$ 所满足的标量不动点方程。\n\n**(b) Lasry–Lions 单调性与平均场控制的唯一性**\n\nLasry-Lions (LL) 单调性条件是 MFG 均衡唯一性的一个充分条件。对于本问题，耦合通过项 $\\frac{\\delta}{2}(X_T - \\alpha\\bar{m}_T)^2$ 实现。平均场对单个智能体施加的“力”可以被认为是成本相对于均值的导数，即 $-\\delta\\alpha(X_T - \\alpha\\bar{m}_T)$。对于 $\\alpha>0$，智能体因接近 $\\alpha\\bar{m}_T$ 而受到惩罚，这可以解释为一种排斥性相互作用。这种排斥性相互作用通常会违反 LL 单调性条件。对于 $\\alpha>0$，系统具有非单调结构，LL 条件可能失效，从而可能出现非唯一均衡。\n\n现在，考虑平均场控制（社会计划者）问题。计划者为所有智能体选择一个共同的控制 $u_t$ 以最小化平均成本。由于控制是共同的，它必须是确定性的。一个代表性智能体的状态是 $X_t = X_0 + \\int_0^t u_s\\,ds + \\sigma W_t$。均值状态是 $\\bar{m}_t = m_0 + \\int_0^t u_s\\,ds$。计划者的成本函数是：\n$$J_{MFC}(u) = \\mathbb{E}\\left[ \\frac{r}{2} \\int_0^T u_t^2\\,dt + \\frac{\\delta}{2}\\left(X_T - \\alpha\\,\\bar{m}_T\\right)^2 \\right]$$\n我们通过在均值附近展开来分析终端成本项：\n$$\\mathbb{E}[(X_T - \\alpha\\bar{m}_T)^2] = \\mathbb{E}[ ( (X_T - \\bar{m}_T) + (1-\\alpha)\\bar{m}_T )^2 ]$$\n$$= \\mathbb{E}[(X_T-\\bar{m}_T)^2] + 2(1-\\alpha)\\bar{m}_T\\mathbb{E}[X_T-\\bar{m}_T] + (1-\\alpha)^2\\bar{m}_T^2$$\n由于 $\\mathbb{E}[X_T-\\bar{m}_T]=0$，交叉项消失。第一项是 $X_T$ 的方差：\n$$\\text{Var}(X_T) = \\text{Var}(X_0 + \\text{const} + \\sigma W_T) = \\text{Var}(X_0) + \\sigma^2 T$$\n该方差与控制 $u_t$ 无关。因此，最小化 $J_{MFC}$ 等价于最小化：\n$$\\hat{J}_{MFC}(u) = \\frac{r}{2} \\int_0^T u_t^2\\,dt + \\frac{\\delta(1-\\alpha)^2}{2} \\bar{m}_T^2$$\n其中 $\\bar{m}_T = m_0 + \\int_0^T u_t\\,dt$。令 $U = \\int_0^T u_t\\,dt$。根据柯西-施瓦茨不等式，对于一个固定的 $U$ 值，当 $u_t$ 为常数时，即 $u_t = U/T$ 时，积分 $\\int_0^T u_t^2\\,dt$ 达到最小值。在这种情况下，$\\int_0^T u_t^2\\,dt = (U/T)^2 T = U^2/T$。问题简化为最小化单个标量变量 $U$ 的函数：\n$$F(U) = \\frac{r}{2T}U^2 + \\frac{\\delta(1-\\alpha)^2}{2}(m_0+U)^2$$\n这是一个关于 $U$ 的二次函数。为了检查最小值的唯一性，我们考察它的二阶导数：\n$$F''(U) = \\frac{d^2 F}{d U^2} = \\frac{r}{T} + \\delta(1-\\alpha)^2$$\n由于 $r>0$，$T>0$，$\\delta>0$，且 $(1-\\alpha)^2 \\ge 0$，对于所有 $\\alpha \\in \\mathbb{R}$，二阶导数 $F''(U)$ 都是严格为正的。这证明了 $F(U)$ 是一个严格凸函数。一个严格凸函数最多只有一个最小值点。由于 $F(U)$ 是一个开口向上的二次函数，它有一个唯一的全局最小值。因此，无论 $\\alpha$ 的值如何，平均场控制问题都存在唯一的最优控制和唯一的最优终端均值 $\\bar{m}_T$。\n\n**(c) MFG 中存在无限均衡的条件**\n\n我们考虑 $m_0=0$ 的特殊情况。来自 (a) 部分的不动点方程变为：\n$$\\bar{m}_T \\left( r + \\delta T(1-\\alpha) \\right) = 0$$\n这个方程决定了 $\\bar{m}_T$ 可能的均衡值。\n- 如果系数 $r + \\delta T(1-\\alpha) \\neq 0$，唯一的解是 $\\bar{m}_T=0$。在这种情况下，MFG 有一个唯一的纳什均衡。\n- 如果系数 $r + \\delta T(1-\\alpha) = 0$，方程变为 $0 \\cdot \\bar{m}_T = 0$，这对任何 $\\bar{m}_T \\in \\mathbb{R}$ 都成立。在这种情况下，任何实数都是一个有效的均衡终端均值，这意味着存在无穷多个纳什均衡。\n\n我们正在寻找导致第二种情况的耦合强度 $\\alpha^{\\star}$。我们将系数设为零：\n$$r + \\delta T(1-\\alpha^{\\star}) = 0$$\n$$r + \\delta T - \\delta T \\alpha^{\\star} = 0$$\n$$\\delta T \\alpha^{\\star} = r + \\delta T$$\n$$\\alpha^{\\star} = \\frac{r+\\delta T}{\\delta T} = 1 + \\frac{r}{\\delta T}$$\n在这个临界值 $\\alpha^{\\star}$ 下，平均场博弈存在无穷多个均衡。正如在 (b) 部分所证明的，平均场控制问题的目标函数仍然是严格凸的，因为 $F''(U) = \\frac{r}{T} + \\delta(1-\\alpha^{\\star})^2 = \\frac{r}{T} + \\delta(-\\frac{r}{\\delta T})^2 = \\frac{r}{T} + \\frac{r^2}{\\delta T^2} > 0$。因此，即使在 $\\alpha = \\alpha^{\\star}$ 时，社会计划者的问题仍然有唯一的解（即 $U=0$，导致 $\\bar{m}_T = 0$），这与博弈设置中均衡的多重性形成鲜明对比。", "answer": "$$\\boxed{1 + \\frac{r}{\\delta T}}$$", "id": "2987096"}]}