{"hands_on_practices": [{"introduction": "我们从模拟的基础任务开始。我们如何利用简单的均匀分布随机数来生成像泊松分布这样更复杂的分布样本？这项练习 [@problem_id:3044284] 揭示了一种优美而高效的算法，展示了均匀分布、指数分布和泊松分布之间的深刻联系。掌握这种方法是从头开始构建随机模拟的关键。", "problem": "一个具有独立平稳增量且事件间间隔时间呈单位速率指数分布的计数过程称为速率为1的泊松过程。令 $X$ 为一个参数为 $\\lambda0$ 的泊松随机变量，记作 $X \\sim \\mathrm{Pois}(\\lambda)$，其概率质量函数为 $p(k)=\\exp(-\\lambda)\\lambda^{k}/k!$，$k \\in \\{0,1,2,\\dots\\}$。对于累积分布函数为 $F(k)=\\sum_{j=0}^{k}p(j)$ 的离散分布，其逆变换采样方法通过从连续均匀分布 Uniform$(0,1)$ 中抽取一个 $U$，并返回满足 $F(k)\\geq U$ 的最小整数 $k$ 来生成一个样本。一个专门针对泊松分布的等价顺序搜索反演方法是以下过程：从 Uniform$(0,1)$ 分布中抽取独立的 $U_{1},U_{2},\\dots$；形成连乘积 $P_{n}=\\prod_{i=1}^{n}U_{i}$；在第一个满足 $P_{K}\\leq \\exp(-\\lambda)$ 的索引 $K$ 处停止，并输出 $K-1$。\n\n从泊松过程和指数分布的基本定义和事实出发，完成以下任务：\n\n- 证明上述顺序搜索过程确实从 $\\mathrm{Pois}(\\lambda)$ 分布中采样。\n- 令 $M_{\\lambda}$ 表示顺序搜索过程直到停止所消耗的独立 Uniform$(0,1)$ 抽取的总次数。推导 $\\mathbb{E}[M_{\\lambda}]$ 作为 $\\lambda$ 函数的精确闭式表达式。\n- 使用您的精确表达式，确定当 $\\lambda \\to 0^{+}$ 和 $\\lambda \\to \\infty$ 时 $\\mathbb{E}[M_{\\lambda}]$ 的主导阶行为。\n\n您的最终答案必须是 $\\mathbb{E}[M_{\\lambda}]$ 关于 $\\lambda$ 的单个闭式解析表达式（无单位）。不要提供不等式或方程。不要四舍五入；不需要数值近似。", "solution": "该问题被评估为有效。它在概率论中有科学依据，问题陈述清晰，目标明确，且所有提供的信息都是自洽和一致的。因此，我们可以进行完整解答。\n\n该问题要求三个不同的部分：采样过程的证明、期望的推导以及渐近分析。我们将依次处理每一部分。\n\n### 采样过程的证明\n\n该算法生成一个随机整数 $K-1$，其中 $K$ 是使得 $K$ 个独立的 $\\text{Uniform}(0,1)$ 随机变量的乘积低于一个阈值的第一个索引。设均匀随机变量序列为 $U_1, U_2, \\dots$，其中每个 $U_i \\sim \\text{Uniform}(0,1)$ 且相互独立。连乘积为 $P_n = \\prod_{i=1}^{n} U_i$。停止时间 $K$ 定义为满足 $P_K \\leq \\exp(-\\lambda)$ 的最小整数。算法的输出是随机变量 $X = K-1$。我们必须证明 $X \\sim \\mathrm{Pois}(\\lambda)$。\n\n条件 $P_K \\leq \\exp(-\\lambda)$ 可以通过对两边取自然对数来转换，自然对数是一个单调函数。\n$$ \\ln\\left(\\prod_{i=1}^{K} U_i\\right) \\leq \\ln(\\exp(-\\lambda)) $$\n$$ \\sum_{i=1}^{K} \\ln(U_i) \\leq -\\lambda $$\n两边乘以 $-1$ 会反转不等号：\n$$ \\sum_{i=1}^{K} (-\\ln(U_i)) \\geq \\lambda $$\n我们定义一组新的随机变量 $E_i = -\\ln(U_i)$，$i=1, 2, \\dots$。为了找到 $E_i$ 的分布，我们可以计算其对于 $x0$ 的累积分布函数（CDF）：\n$$ F_{E_i}(x) = \\mathbb{P}(E_i \\leq x) = \\mathbb{P}(-\\ln(U_i) \\leq x) = \\mathbb{P}(\\ln(U_i) \\geq -x) = \\mathbb{P}(U_i \\geq \\exp(-x)) $$\n因为 $U_i$ 是在 $(0,1)$ 上均匀分布的连续随机变量，其CDF为 $F_{U_i}(u)=u$，$u \\in [0,1]$。因此，对于 $y \\in (0,1)$，$\\mathbb{P}(U_i \\geq y) = 1 - \\mathbb{P}(U_i  y) = 1 - y$。对于 $x0$，有 $\\exp(-x) \\in (0,1)$，所以我们得到：\n$$ F_{E_i}(x) = 1 - \\exp(-x) $$\n这是速率参数为 $1$ 的指数分布的CDF。因此，随机变量 $E_i$ 是根据 $\\text{Exp}(1)$ 分布独立同分布的。\n\n令 $S_n = \\sum_{i=1}^{n} E_i$。现在可以用这个和来重新表述算法的停止条件。算法在第一个满足 $S_K \\geq \\lambda$ 的整数 $K$ 处停止。输出为 $X=K-1$。\n对于 $k \\in \\{0, 1, 2, \\dots\\}$，事件 $\\{X=k\\}$ 等价于事件 $\\{K-1=k\\}$，即 $\\{K=k+1\\}$。这意味着算法在第 $k+1$ 步停止。这种情况发生于停止条件在第 $1, 2, \\dots, k$ 步未满足，但在第 $k+1$ 步满足。\n$$ \\{K=k+1\\} = \\{S_1  \\lambda, S_2  \\lambda, \\dots, S_k  \\lambda, \\text{ and } S_{k+1} \\geq \\lambda\\} $$\n由于 $E_i = -\\ln(U_i)  0$ 几乎必然成立（因为 $\\mathbb{P}(U_i=1)=0$），部分和序列 $S_n$ 是严格递增的。因此，条件 $S_1  \\lambda, \\dots, S_k  \\lambda$ 可以简化为单个条件 $S_k  \\lambda$。该事件变为：\n$$ \\{K=k+1\\} = \\{S_k  \\lambda \\text{ and } S_{k+1} \\geq \\lambda\\} $$\n$n$ 个独立同分布的 $\\text{Exp}(1)$ 随机变量之和 $S_n$ 表示速率为 $1$ 的齐次泊松过程中第 $n$ 次到达的时间。令 $\\{N(t), t \\geq 0\\}$ 为这样一个过程。随机变量 $N(t)$ 计入区间 $[0, t]$ 内的到达次数，并服从参数为 $t$ 的泊松分布。\n事件 $\\{N(\\lambda) = k\\}$ 表示在区间 $[0, \\lambda]$ 内恰好有 $k$ 次到达。这意味着第 $k$ 次到达的时间 $S_k$ 发生在时间 $\\lambda$ 或之前，而第 $(k+1)$ 次到达的时间 $S_{k+1}$ 发生在时间 $\\lambda$ 之后。形式上，$\\{N(\\lambda) = k\\} = \\{S_k \\leq \\lambda  S_{k+1}\\}$。\n由于 $S_k$ 是一个连续随机变量，$\\mathbb{P}(S_k = \\lambda) = 0$。因此，事件 $\\{S_k  \\lambda \\text{ 且 } S_{k+1} \\geq \\lambda\\}$ 在概率上等价于 $\\{S_k \\leq \\lambda  S_{k+1}\\}$。\n因此，我们已经建立了事件的等价性：\n$$ \\mathbb{P}(X=k) = \\mathbb{P}(K=k+1) = \\mathbb{P}(S_k  \\lambda \\text{ and } S_{k+1} \\geq \\lambda) = \\mathbb{P}(N(\\lambda)=k) $$\n由于 $N(\\lambda)$ 服从参数为 $\\lambda$ 的泊松分布，其概率质量函数为 $\\mathbb{P}(N(\\lambda)=k) = \\frac{\\exp(-\\lambda)\\lambda^k}{k!}$。\n这证实了由顺序搜索过程生成的随机变量 $X=K-1$ 确实是从 $\\mathrm{Pois}(\\lambda)$ 分布中抽取的样本。\n\n### $\\mathbb{E}[M_{\\lambda}]$ 的推导\n\n消耗的均匀抽取的总次数为 $M_{\\lambda} = K$。我们寻求求出 $K$ 的期望值 $\\mathbb{E}[K]$。对于一个取值为正整数 $\\{1, 2, 3, \\dots\\}$ 的随机变量 $K$，其期望可以计算为：\n$$ \\mathbb{E}[K] = \\sum_{j=1}^{\\infty} \\mathbb{P}(K \\geq j) $$\n事件 $\\{K \\geq j\\}$ 意味着算法在第 $j-1$ 步之前尚未停止。这等价于条件 $S_{j-1}  \\lambda$。对于 $j=1$，$S_0=0$，并且因为 $\\lambda0$，所以 $\\mathbb{P}(S_0  \\lambda)=\\mathbb{P}(0  \\lambda)=1$。\n因此，我们可以将期望写为：\n$$ \\mathbb{E}[K] = \\sum_{j=1}^{\\infty} \\mathbb{P}(S_{j-1}  \\lambda) $$\n通过用 $m = j-1$ 替换求和索引，我们得到：\n$$ \\mathbb{E}[K] = \\sum_{m=0}^{\\infty} \\mathbb{P}(S_m  \\lambda) $$\n有一个基本恒等式将伽马分布随机变量（$S_m \\sim \\text{Gamma}(m,1)$）的CDF与泊松分布随机变量的CDF联系起来。事件 $\\{S_m  \\lambda\\}$（需要至少 $m$ 次事件才能超过时间 $\\lambda$）等价于 $\\{N(\\lambda) \\geq m\\}$（到时间 $\\lambda$ 为止至少发生了 $m$ 次事件）。\n$$ \\mathbb{P}(S_m  \\lambda) = \\mathbb{P}(N(\\lambda) \\geq m) $$\n将此代入 $\\mathbb{E}[K]$ 的表达式中：\n$$ \\mathbb{E}[K] = \\sum_{m=0}^{\\infty} \\mathbb{P}(N(\\lambda) \\geq m) $$\n令 $Y$ 为一个非负整值随机变量。其期望的一个标准公式是 $\\mathbb{E}[Y] = \\sum_{m=1}^{\\infty} \\mathbb{P}(Y \\geq m)$。我们这里有一个从 $m=0$ 开始的和。注意到 $\\sum_{m=0}^{\\infty} \\mathbb{P}(Y \\geq m) = \\mathbb{P}(Y \\geq 0) + \\sum_{m=1}^{\\infty} \\mathbb{P}(Y \\geq m)$。由于 $Y$ 是非负的，$\\mathbb{P}(Y \\geq 0) = 1$。因此，$\\sum_{m=0}^{\\infty} \\mathbb{P}(Y \\geq m) = 1 + \\mathbb{E}[Y] = \\mathbb{E}[Y+1]$。\n将此应用于我们的情况，其中 $Y = N(\\lambda) \\sim \\mathrm{Pois}(\\lambda)$：\n$$ \\mathbb{E}[K] = \\sum_{m=0}^{\\infty} \\mathbb{P}(N(\\lambda) \\geq m) = \\mathbb{E}[N(\\lambda) + 1] = \\mathbb{E}[N(\\lambda)] + 1 $$\n参数为 $\\lambda$ 的泊松随机变量的期望是 $\\lambda$。\n$$ \\mathbb{E}[N(\\lambda)] = \\lambda $$\n因此，期望的均匀抽取次数为：\n$$ \\mathbb{E}[M_{\\lambda}] = \\mathbb{E}[K] = \\lambda + 1 $$\n\n### $\\mathbb{E}[M_{\\lambda}]$ 的渐近行为\n\n期望抽取次数的精确闭式表达式为 $\\mathbb{E}[M_{\\lambda}] = \\lambda + 1$。我们现在可以分析其在指定极限下的行为。\n\n1.  当 $\\lambda \\to 0^{+}$ 时：\n    表达式 $\\lambda + 1$ 是一个简单的线性函数。\n    $$ \\lim_{\\lambda \\to 0^{+}} \\mathbb{E}[M_{\\lambda}] = \\lim_{\\lambda \\to 0^{+}} (\\lambda + 1) = 1 $$\n    主导阶行为是 $1$。这在直觉上是正确的，因为对于非常小的 $\\lambda$，阈值 $\\exp(-\\lambda)$ 接近于 $1$，所以第一次抽取的 $U_1$ 很有可能小于该阈值，导致算法立即以 $K=1$ 停止。\n\n2.  当 $\\lambda \\to \\infty$ 时：\n    我们研究当 $\\lambda$ 很大时的行为。\n    $$ \\mathbb{E}[M_{\\lambda}] = \\lambda + 1 $$\n    当 $\\lambda$ 变得很大时，常数项 $1$ 与 $\\lambda$ 相比可以忽略不计。因此，主导阶行为是 $\\lambda$。\n    $$ \\mathbb{E}[M_{\\lambda}] \\sim \\lambda \\quad \\text{as} \\quad \\lambda \\to \\infty $$\n    这与初等更新定理一致，该定理会预测 $\\mathbb{E}[K] / \\lambda \\to 1/\\mathbb{E}[E_i] = 1$。我们的精确结果表明收敛形式为 $(\\lambda+1)/\\lambda \\to 1$。\n\n该问题要求以 $\\mathbb{E}[M_{\\lambda}]$ 的单个闭式表达式作为最终答案。", "answer": "$$\n\\boxed{\\lambda+1}\n$$", "id": "3044284"}, {"introduction": "在生成随机变量之后，我们转向分析它们的性质。单个伯努利试验是独立的，但是当我们固定成功的总次数时，它们之间的关系会发生什么变化？本练习 [@problem_id:3044297] 运用全协方差定律，探讨了在给定条件下产生的微妙的负相关性。这是理解无放回抽样和受约束随机系统的关键概念。", "problem": "考虑一个由 $n$ 次独立同分布的伯努利试验组成的序列，其成功概率为 $p \\in (0,1)$。令 $Y_{1}, Y_{2}, \\dots, Y_{n}$ 表示试验结果，其中如果第 $i$ 次试验成功，则 $Y_{i} = 1$，否则 $Y_{i} = 0$。定义总成功次数为 $X = \\sum_{i=1}^{n} Y_{i}$，因此 $X \\sim \\mathrm{Bin}(n,p)$。对于两个不同的下标 $i \\neq j$，考虑第 $i$ 次和第 $j$ 次试验成功的指示变量，即 $Y_{i}$ 和 $Y_{j}$。\n\n仅使用概率论中的基本定义，包括协方差的定义、在以 $X$ 为条件下的可交换性以及全协方差定律，按如下步骤进行：\n\n- 推导 $\\mathrm{Cov}(Y_{i}, Y_{j})$。\n- 以 $n$ 和 $k$ 的函数形式推导 $\\mathrm{Cov}(Y_{i}, Y_{j} \\mid X = k)$ 的闭合表达式，并解释其符号。\n- 使用全协方差定律计算 $\\mathbb{E}\\!\\left[\\mathrm{Cov}(Y_{i}, Y_{j} \\mid X)\\right]$，以 $n$ 和 $p$ 的函数形式给出闭合表达式。\n\n解释你的推导如何捕捉到在以总成功次数为条件下产生的负相关现象。报告关于 $n$ 和 $p$ 的 $\\mathbb{E}\\!\\left[\\mathrm{Cov}(Y_{i}, Y_{j} \\mid X)\\right]$ 的精确闭合表达式作为你的最终答案。不要对结果进行四舍五入。无需单位。", "solution": "用户希望推导两次伯努利试验结果之间的无条件协方差、以总成功次数为条件的协方差，以及最后这个条件协方差的期望值。这将使用协方差、条件期望和全协方差定律的定义来完成。\n\n首先，我们确定随机变量的性质。给定一个由 $n$ 次独立同分布 (i.i.d.) 的伯努利试验组成的序列 $Y_{1}, Y_{2}, \\dots, Y_{n}$，其成功概率为 $p \\in (0,1)$。第 $i$ 次试验的结果是 $Y_i$，其中 $Y_i=1$ 表示成功，$Y_i=0$ 表示失败。对于任意试验 $i$，期望值为 $\\mathbb{E}[Y_i] = 1 \\cdot P(Y_i=1) + 0 \\cdot P(Y_i=0) = p$，方差为 $\\mathrm{Var}(Y_i) = \\mathbb{E}[Y_i^2] - (\\mathbb{E}[Y_i])^2 = p - p^2 = p(1-p)$。总成功次数为 $X = \\sum_{i=1}^{n} Y_{i}$，它服从二项分布，$X \\sim \\mathrm{Bin}(n,p)$。\n\n我们将进行三个要求的推导。\n\n1.  **推导 $\\mathrm{Cov}(Y_{i}, Y_{j})$**\n\n对于两个不同的下标 $i \\neq j$，我们希望计算协方差 $\\mathrm{Cov}(Y_{i}, Y_{j})$。根据定义，\n$$\n\\mathrm{Cov}(Y_{i}, Y_{j}) = \\mathbb{E}[Y_{i}Y_{j}] - \\mathbb{E}[Y_{i}]\\mathbb{E}[Y_{j}]\n$$\n试验是独立的，这意味着当 $i \\neq j$ 时，随机变量 $Y_i$ 和 $Y_j$ 是独立的。独立随机变量的一个关键性质是它们乘积的期望等于它们各自期望的乘积。\n$$\n\\mathbb{E}[Y_{i}Y_{j}] = \\mathbb{E}[Y_{i}]\\mathbb{E}[Y_{j}]\n$$\n如前所述，$\\mathbb{E}[Y_i] = p$ 且 $\\mathbb{E}[Y_j] = p$。因此，\n$$\n\\mathbb{E}[Y_{i}Y_{j}] = p \\cdot p = p^2\n$$\n将此代入协方差公式可得：\n$$\n\\mathrm{Cov}(Y_{i}, Y_{j}) = p^2 - (p)(p) = 0\n$$\n无条件协方差为 $0$，这与试验的独立性是一致的。\n\n2.  **推导 $\\mathrm{Cov}(Y_{i}, Y_{j} \\mid X = k)$**\n\n接下来，我们计算在总成功次数为 $X = k$ 的条件下，$Y_i$ 和 $Y_j$ 之间的协方差。条件协方差的定义是：\n$$\n\\mathrm{Cov}(Y_{i}, Y_{j} \\mid X = k) = \\mathbb{E}[Y_{i}Y_{j} \\mid X=k] - \\mathbb{E}[Y_{i} \\mid X=k]\\mathbb{E}[Y_{j} \\mid X=k]\n$$\n我们需要求出条件期望。首先，我们求 $\\mathbb{E}[Y_{i} \\mid X=k]$。由于 $Y_i$ 是一个指示变量，其期望等于它为 $1$ 的概率。\n$$\n\\mathbb{E}[Y_{i} \\mid X=k] = P(Y_i=1 \\mid X=k) = \\frac{P(Y_i=1, X=k)}{P(X=k)}\n$$\n事件 $\\{Y_i=1 \\text{ and } X=k\\}$ 是指试验 $i$ 成功且总成功次数为 $k$。这意味着在其他 $n-1$ 次试验中必须有 $k-1$ 次成功。令 $X' = \\sum_{l \\neq i} Y_l$。那么 $X'$ 是 $n-1$ 次 i.i.d. 伯努利试验的和，因此 $X' \\sim \\mathrm{Bin}(n-1, p)$。该事件为 $\\{Y_i=1, X'=k-1\\}$。由于独立性，\n$$\nP(Y_i=1, X=k) = P(Y_i=1) P(X'=k-1) = p \\left[ \\binom{n-1}{k-1} p^{k-1} (1-p)^{(n-1)-(k-1)} \\right] = \\binom{n-1}{k-1} p^k (1-p)^{n-k}\n$$\n对于 $X \\sim \\mathrm{Bin}(n,p)$，概率 $P(X=k)$ 是 $\\binom{n}{k} p^k (1-p)^{n-k}$。因此，\n$$\n\\mathbb{E}[Y_{i} \\mid X=k] = \\frac{\\binom{n-1}{k-1} p^k (1-p)^{n-k}}{\\binom{n}{k} p^k (1-p)^{n-k}} = \\frac{\\binom{n-1}{k-1}}{\\binom{n}{k}} = \\frac{\\frac{(n-1)!}{(k-1)!(n-k)!}}{\\frac{n!}{k!(n-k)!}} = \\frac{(n-1)! k!}{n! (k-1)!} = \\frac{k}{n}\n$$\n这个结果是直观的：如果我们知道在 $n$ 个位置中有 $k$ 次成功，那么任何特定位置是成功的概率就是 $k/n$。由于试验在以其总和为条件下具有可交换性，因此对于任何 $j$ 都有 $\\mathbb{E}[Y_{j} \\mid X=k] = \\frac{k}{n}$。\n\n接下来，我们求 $\\mathbb{E}[Y_{i}Y_{j} \\mid X=k]$。由于 $Y_iY_j$ 是试验 $i$ 和 $j$ 都成功的指示变量，\n$$\n\\mathbb{E}[Y_{i}Y_{j} \\mid X=k] = P(Y_i=1, Y_j=1 \\mid X=k) = \\frac{P(Y_i=1, Y_j=1, X=k)}{P(X=k)}\n$$\n事件 $\\{Y_i=1, Y_j=1, X=k\\}$ 意味着试验 $i$ 和 $j$ 成功，并且在其他 $n-2$ 次试验中有 $k-2$ 次成功。与上面类似，\n$$\nP(Y_i=1, Y_j=1, X=k) = P(Y_i=1)P(Y_j=1)P\\left(\\sum_{l \\neq i,j} Y_l = k-2\\right) = p^2 \\left[ \\binom{n-2}{k-2} p^{k-2} (1-p)^{n-k} \\right] = \\binom{n-2}{k-2} p^k (1-p)^{n-k}\n$$\n所以，条件期望为：\n$$\n\\mathbb{E}[Y_{i}Y_{j} \\mid X=k] = \\frac{\\binom{n-2}{k-2} p^k (1-p)^{n-k}}{\\binom{n}{k} p^k (1-p)^{n-k}} = \\frac{\\binom{n-2}{k-2}}{\\binom{n}{k}} = \\frac{\\frac{(n-2)!}{(k-2)!(n-k)!}}{\\frac{n!}{k!(n-k)!}} = \\frac{(n-2)! k!}{n! (k-2)!} = \\frac{k(k-1)}{n(n-1)}\n$$\n这也是直观的：它是在位置 $i$ 挑选到一次成功的概率（$k/n$），然后从剩余项中为位置 $j$ 再挑选到一次成功的概率（$(k-1)/(n-1)$）。\n\n现在我们可以组合出条件协方差：\n$$\n\\mathrm{Cov}(Y_{i}, Y_{j} \\mid X = k) = \\frac{k(k-1)}{n(n-1)} - \\left(\\frac{k}{n}\\right)\\left(\\frac{k}{n}\\right) = \\frac{k}{n} \\left( \\frac{k-1}{n-1} - \\frac{k}{n} \\right) = \\frac{k}{n} \\left( \\frac{n(k-1) - k(n-1)}{n(n-1)} \\right)\n$$\n$$\n= \\frac{k}{n} \\left( \\frac{nk - n - nk + k}{n(n-1)} \\right) = \\frac{k(k - n)}{n^2(n-1)} = -\\frac{k(n - k)}{n^2(n-1)}\n$$\n为使该表达式有意义，我们需要 $n \\ge 2$ 并且 $k$ 的取值范围是 $0$ 到 $n$。如果 $k=0$ 或 $k=n$，协方差为 $0$。如果 $0  k  n$，则 $k  0$，$n-k  0$，$n^2(n-1)  0$。因此，对于 $0  k  n$，$\\mathrm{Cov}(Y_{i}, Y_{j} \\mid X = k)$ 是严格为负的。这反映了以总数作为条件所引起的负相关性：如果我们知道总成功次数是固定的，那么在试验 $i$ 中发现一次成功，会使得在试验 $j$ 中发现一次成功的可能性降低，因为一个可用的“成功名额”已经被占用了。\n\n3.  **通过全协方差定律推导 $\\mathbb{E}\\!\\left[\\mathrm{Cov}(Y_{i}, Y_{j} \\mid X)\\right]$**\n\n全协方差定律表明：\n$$\n\\mathrm{Cov}(Y_{i}, Y_{j}) = \\mathbb{E}[\\mathrm{Cov}(Y_{i}, Y_{j} \\mid X)] + \\mathrm{Cov}(\\mathbb{E}[Y_{i} \\mid X], \\mathbb{E}[Y_{j} \\mid X])\n$$\n我们需要求 $\\mathbb{E}[\\mathrm{Cov}(Y_{i}, Y_{j} \\mid X)]$。重新整理公式：\n$$\n\\mathbb{E}[\\mathrm{Cov}(Y_{i}, Y_{j} \\mid X)] = \\mathrm{Cov}(Y_{i}, Y_{j}) - \\mathrm{Cov}(\\mathbb{E}[Y_{i} \\mid X], \\mathbb{E}[Y_{j} \\mid X])\n$$\n从第1部分，我们知道 $\\mathrm{Cov}(Y_{i}, Y_{j}) = 0$。\n第二部分的项是条件期望，但被看作是随机变量 $X$ 的函数。从第2部分，我们发现 $\\mathbb{E}[Y_{i} \\mid X=k] = k/n$。因此，随机变量 $\\mathbb{E}[Y_{i} \\mid X]$ 就是 $X/n$。类似地，$\\mathbb{E}[Y_{j} \\mid X] = X/n$。\n我们需要计算 $\\mathrm{Cov}(X/n, X/n)$。使用协方差的性质：\n$$\n\\mathrm{Cov}\\left(\\frac{X}{n}, \\frac{X}{n}\\right) = \\frac{1}{n^2} \\mathrm{Cov}(X, X) = \\frac{1}{n^2} \\mathrm{Var}(X)\n$$\n由于 $X \\sim \\mathrm{Bin}(n,p)$，其方差是 $\\mathrm{Var}(X) = np(1-p)$。\n$$\n\\mathrm{Cov}\\left(\\frac{X}{n}, \\frac{X}{n}\\right) = \\frac{1}{n^2} [np(1-p)] = \\frac{p(1-p)}{n}\n$$\n这一项表示 $Y_i$ 和 $Y_j$ 之间协方差的分量，该分量是通过它们对随机总量 $X$ 的共同依赖关系来调节的。\n将这些结果代回重新整理的全协方差定律中：\n$$\n\\mathbb{E}[\\mathrm{Cov}(Y_{i}, Y_{j} \\mid X)] = 0 - \\frac{p(1-p)}{n} = -\\frac{p(1-p)}{n}\n$$\n这个最终表达式量化了当总成功次数保持不变时，试验结果之间的平均负相关性。无条件独立性，即 $\\mathrm{Cov}(Y_{i}, Y_{j})=0$，被揭示为一种平衡：一方是当 $X$ 固定时平均存在的负协方差，另一方是由 $X$ 本身的变异性引起的大小相等、符号相反的正协方差。\n\n我们可以通过直接计算来验证这个结果：\n$$ \\mathbb{E}[\\mathrm{Cov}(Y_i, Y_j \\mid X)] = \\mathbb{E}\\left[-\\frac{X(n-X)}{n^2(n-1)}\\right] = -\\frac{1}{n^2(n-1)} \\left(n\\mathbb{E}[X] - \\mathbb{E}[X^2]\\right) $$\n由于 $\\mathbb{E}[X]=np$ 且 $\\mathbb{E}[X^2] = \\mathrm{Var}(X) + (\\mathbb{E}[X])^2 = np(1-p) + (np)^2$，我们有：\n$$ n\\mathbb{E}[X] - \\mathbb{E}[X^2] = n(np) - [np(1-p) + n^2p^2] = n^2p - np + np^2 - n^2p^2 = (n^2-n)p(1-p) = n(n-1)p(1-p) $$\n$$ \\mathbb{E}[\\mathrm{Cov}(Y_i, Y_j \\mid X)] = -\\frac{1}{n^2(n-1)} [n(n-1)p(1-p)] = -\\frac{p(1-p)}{n} $$\n直接计算证实了从全协方差定律得到的结果。\n\n最终答案是 $\\mathbb{E}[\\mathrm{Cov}(Y_i, Y_j \\mid X)]$ 的闭合表达式。", "answer": "$$\n\\boxed{-\\frac{p(1-p)}{n}}\n$$", "id": "3044297"}, {"introduction": "最后，我们将理论应用于随机微分方程（SDE）数值模拟中的一个实际问题。在模拟带有随机跳跃过程时，一个关键挑战是选择一个既高效又精确的时间步长。本练习 [@problem_id:3044306] 展示了如何利用泊松分布的性质来推导一个自适应步长规则，以控制错过多个跳跃的概率。这是离散分布理论在确保随机数值方法稳健性方面的直接应用。", "problem": "考虑一个一维随机微分方程 (SDE)，该方程包含一个纯跳跃部分，其跳跃强度是状态依赖的，记作 $\\lambda(x) \\ge 0$。您正在一个单步 $[t, t + h]$ 上实现一个跳跃自适应欧拉格式。令 $N_h$ 表示在此区间内发生的跳跃次数。假设根据在时间 $t$ 的局部信息，您已经计算出一个上界 $\\overline{\\lambda}  0$，该上界在您打算进行的步长内对所有 $s \\in [t, t+h]$ 满足 $\\lambda(X_s) \\le \\overline{\\lambda}$。您的目标是选择一个自适应步长 $h$，使得在该步长内观测到超过一次跳跃的概率由用户指定的容差 $\\varepsilon \\in (0,1)$ 控制。\n\n从计数过程和经典离散分布的第一性原理出发，使用以下基本事实：\n\n- 如果在一个长度为 $h$ 的区间上，跳跃强度被一个常数 $\\overline{\\lambda}$ 上界所限制，那么该区间上的跳跃次数受到一个均值为 $\\mu = \\overline{\\lambda} h$ 的泊松随机变量的随机占优。\n\n- 仅使用泊松分布的定义和指数函数的标准不等式，推导一个关于 $\\mu$ 的 $\\mathbb{P}(N_h \\ge 2)$ 的界，该界对所有 $\\mu \\ge 0$ 成立。\n\n使用这个界来得出一个保守的自适应步长方案 $h_{\\max}(\\varepsilon, \\overline{\\lambda})$，以保证 $\\mathbb{P}(N_h \\ge 2) \\le \\varepsilon$。\n\n请以 $\\varepsilon$ 和 $\\overline{\\lambda}$ 的单个闭式符号表达式形式给出您对 $h_{\\max}(\\varepsilon, \\overline{\\lambda})$ 的最终答案。不要进行任何数值舍入。请以符号形式表达您的最终答案；最终表达式中不需要物理单位。", "solution": "所述问题具有科学依据，是适定且客观的。它基于随机微积分和随机微分方程数值分析的标准原理。其前提在数学上是合理的，目标定义清晰，并提供了足够的信息来推导出唯一解。因此，该问题被认为是有效的。我们可以开始求解。\n\n目标是推导出一个保守的自适应步长方案，记作 $h_{\\max}(\\varepsilon, \\overline{\\lambda})$，该方案保证在长度为 $h$ 的时间间隔内观测到两次或更多次跳跃的概率不大于指定的容差 $\\varepsilon$。需要满足的条件是 $\\mathbb{P}(N_h \\ge 2) \\le \\varepsilon$，其中 $N_h$ 是区间 $[t, t+h]$ 内的跳跃次数。\n\n问题提供了一个基本事实：如果状态依赖的跳跃强度 $\\lambda(X_s)$ 在 $s \\in [t, t+h]$ 上被一个常数 $\\overline{\\lambda}$ 上界所限制，那么该区间内的跳跃次数 $N_h$ 受到一个泊松随机变量（我们称之为 $M$）的随机占优，该泊松随机变量的均值参数为 $\\mu = \\overline{\\lambda} h$。随机占优意味着对于任何整数 $k \\ge 0$，累积分布函数满足 $\\mathbb{P}(N_h \\le k) \\ge \\mathbb{P}(M \\le k)$，这等价于 $\\mathbb{P}(N_h  k) \\le \\mathbb{P}(M  k)$ 或 $\\mathbb{P}(N_h \\ge k) \\le \\mathbb{P}(M \\ge k)$。\n\n因此，为了满足目标条件 $\\mathbb{P}(N_h \\ge 2) \\le \\varepsilon$，对占优变量 $M$ 施加一个更严格的条件就足够了：\n$$\n\\mathbb{P}(M \\ge 2) \\le \\varepsilon\n$$\n\n接下来，我们必须推导一个关于 $\\mu$ 的 $\\mathbb{P}(M \\ge 2)$ 的可用界。均值为 $\\mu$ 的泊松随机变量 $M$ 的概率质量函数由 $P(M=k) = \\frac{e^{-\\mu} \\mu^k}{k!}$ 给出，其中 $k \\in \\{0, 1, 2, \\dots\\}$。观测到两次或更多事件的概率是观测到零次或一次事件的补集：\n$$\n\\mathbb{P}(M \\ge 2) = 1 - \\mathbb{P}(M  2) = 1 - (\\mathbb{P}(M=0) + \\mathbb{P}(M=1))\n$$\n代入泊松概率：\n$$\n\\mathbb{P}(M \\ge 2) = 1 - \\left( \\frac{e^{-\\mu} \\mu^0}{0!} + \\frac{e^{-\\mu} \\mu^1}{1!} \\right) = 1 - (e^{-\\mu} + \\mu e^{-\\mu}) = 1 - e^{-\\mu}(1+\\mu)\n$$\n这个表达式是精确的。然而，问题要求我们推导一个更简单的界。我们可以将 $\\mathbb{P}(M \\ge 2)$ 表示为一个无穷级数：\n$$\n\\mathbb{P}(M \\ge 2) = \\sum_{k=2}^{\\infty} \\frac{e^{-\\mu} \\mu^k}{k!} = e^{-\\mu} \\left( \\frac{\\mu^2}{2!} + \\frac{\\mu^3}{3!} + \\frac{\\mu^4}{4!} + \\dots \\right)\n$$\n我们可以为这个级数建立一个上界。提出因子 $\\frac{\\mu^2}{2}$：\n$$\n\\mathbb{P}(M \\ge 2) = e^{-\\mu} \\frac{\\mu^2}{2} \\left( 1 + \\frac{\\mu}{3} + \\frac{\\mu^2}{3 \\cdot 4} + \\frac{\\mu^3}{3 \\cdot 4 \\cdot 5} + \\dots \\right) = e^{-\\mu} \\frac{\\mu^2}{2} \\sum_{j=0}^{\\infty} \\frac{2 \\mu^j}{(j+2)!}\n$$\n对于任何整数 $j \\ge 0$，我们有不等式 $(j+2)! = (j+2)(j+1)j! \\ge 2 \\cdot j!$。因此，$\\frac{1}{(j+2)!} \\le \\frac{1}{2 \\cdot j!}$。利用这一点，我们可以对求和进行放缩：\n$$\n\\sum_{j=0}^{\\infty} \\frac{2 \\mu^j}{(j+2)!} \\le \\sum_{j=0}^{\\infty} \\frac{2 \\mu^j}{2 j!} = \\sum_{j=0}^{\\infty} \\frac{\\mu^j}{j!}\n$$\n右侧得到的和是指数函数的泰勒级数展开式，$\\sum_{j=0}^{\\infty} \\frac{\\mu^j}{j!} = e^{\\mu}$。\n将此不等式代回到我们关于 $\\mathbb{P}(M \\ge 2)$ 的表达式中，我们得到界：\n$$\n\\mathbb{P}(M \\ge 2) \\le e^{-\\mu} \\frac{\\mu^2}{2} (e^{\\mu}) = \\frac{\\mu^2}{2}\n$$\n这个不等式 $\\mathbb{P}(M \\ge 2) \\le \\frac{\\mu^2}{2}$ 对所有 $\\mu \\ge 0$ 成立。为了正式验证这一点，令 $f(\\mu) = \\frac{\\mu^2}{2} - \\mathbb{P}(M \\ge 2) = \\frac{\\mu^2}{2} - (1 - (1+\\mu)e^{-\\mu})$。在 $\\mu=0$ 处，$f(0) = 0 - (1 - (1)e^0) = 0$。关于 $\\mu$ 的导数是：\n$$\nf'(\\mu) = \\frac{d}{d\\mu} \\left( \\frac{\\mu^2}{2} - 1 + e^{-\\mu} + \\mu e^{-\\mu} \\right) = \\mu - e^{-\\mu} + (e^{-\\mu} - \\mu e^{-\\mu}) = \\mu - \\mu e^{-\\mu} = \\mu(1 - e^{-\\mu})\n$$\n对于所有 $\\mu  0$，我们有 $\\mu  0$ 和 $e^{-\\mu}  1$，这意味着 $(1 - e^{-\\mu})  0$。因此，对于 $\\mu  0$，$f'(\\mu)  0$。由于 $f(0)=0$ 且该函数对于 $\\mu \\ge 0$ 是单调递增的，我们得出结论，对于所有 $\\mu \\ge 0$，$f(\\mu) \\ge 0$，这证实了 $\\mathbb{P}(M \\ge 2) \\le \\frac{\\mu^2}{2}$ 是对所有非负 $\\mu$ 的一个有效上界。\n\n有了这个保守的界，我们可以通过施加更严格的条件来满足控制目标 $\\mathbb{P}(M \\ge 2) \\le \\varepsilon$：\n$$\n\\frac{\\mu^2}{2} \\le \\varepsilon\n$$\n现在，代入均值的定义，$\\mu = \\overline{\\lambda} h$：\n$$\n\\frac{(\\overline{\\lambda} h)^2}{2} \\le \\varepsilon\n$$\n我们对步长 $h$ 求解这个不等式。已知 $\\overline{\\lambda}  0$ 和 $h \\ge 0$：\n$$\n(\\overline{\\lambda} h)^2 \\le 2\\varepsilon\n$$\n$$\n\\overline{\\lambda} h \\le \\sqrt{2\\varepsilon}\n$$\n$$\nh \\le \\frac{\\sqrt{2\\varepsilon}}{\\overline{\\lambda}}\n$$\n这个不等式给出了步长 $h$ 的一个上限，保证了我们的概率约束得到满足。问题要求的是最大允许的步长 $h_{\\max}(\\varepsilon, \\overline{\\lambda})$，也就是这个区间的上边界。\n$$\nh_{\\max}(\\varepsilon, \\overline{\\lambda}) = \\frac{\\sqrt{2\\varepsilon}}{\\overline{\\lambda}}\n$$\n这就是所求的保守自适应步长方案。", "answer": "$$\n\\boxed{\\frac{\\sqrt{2\\varepsilon}}{\\overline{\\lambda}}}\n$$", "id": "3044306"}]}