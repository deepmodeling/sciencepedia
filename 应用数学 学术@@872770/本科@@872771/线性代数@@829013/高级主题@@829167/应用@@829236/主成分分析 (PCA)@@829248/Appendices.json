{"hands_on_practices": [{"introduction": "为了真正理解主成分分析（PCA），没有什么比亲手计算一遍更有效了。这个练习将引导你处理一个简单的数据集，通过数据中心化、构建协方差矩阵和求解主特征向量等核心步骤，手动计算出第一个主成分。这个过程将揭开PCA的神秘面纱，让你直观地理解其算法的内部工作原理 [@problem_id:2416060]。", "problem": "一个基因表达实验测量了基因 $G_1$、$G_2$ 和 $G_3$ 在样本 $S_1$、$S_2$、$S_3$ 和 $S_4$ 中的对数转换后的表达值。数据矩阵 $X \\in \\mathbb{R}^{4 \\times 3}$ 以样本为行，基因为列：\n$$\nX \\;=\\; \\begin{pmatrix}\n1  2  3 \\\\\n2  3  4 \\\\\n3  4  5 \\\\\n4  5  6\n\\end{pmatrix}.\n$$\n将样本视为独立观测值，基因视为变量。使用主成分分析 (PCA)，通过以下步骤计算基因空间中的第一个主成分载荷向量：\n- 对 $X$ 进行列中心化，\n- 对于 $n=4$ 个样本，以 $n-1$ 为分母构建基因间的样本协方差矩阵，以及\n- 取该协方差矩阵对应于最大特征值的单位范数特征向量。\n\n报告该载荷向量，形式为 $1 \\times 3$ 行矩阵，按 $(G_1, G_2, G_3)$ 顺序排列，并选择符号使其第一个非零项为正。无需四舍五入。", "solution": "我们需要通过对基因间的样本协方差矩阵进行特征分解，来计算基因空间中的第一个主成分载荷向量。设样本数量为 $n=4$，基因数量为 $p=3$。数据矩阵为 $X \\in \\mathbb{R}^{n \\times p}$，其行为样本，列为基因。\n\n步骤 $1$：列中心化。计算 $X$ 的列均值：\n$$\n\\bar{x}_{\\cdot 1} \\;=\\; \\frac{1+2+3+4}{4} \\;=\\; 2.5,\\quad\n\\bar{x}_{\\cdot 2} \\;=\\; \\frac{2+3+4+5}{4} \\;=\\; 3.5,\\quad\n\\bar{x}_{\\cdot 3} \\;=\\; \\frac{3+4+5+6}{4} \\;=\\; 4.5.\n$$\n从每列中减去这些均值，得到中心化矩阵 $Z$：\n$$\nZ \\;=\\; X - \\mathbf{1}\\bar{x}^{\\top}\n\\;=\\;\n\\begin{pmatrix}\n1-2.5  2-3.5  3-4.5 \\\\\n2-2.5  3-3.5  4-4.5 \\\\\n3-2.5  4-3.5  5-4.5 \\\\\n4-2.5  5-3.5  6-4.5\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n-1.5  -1.5  -1.5 \\\\\n-0.5  -0.5  -0.5 \\\\\n\\phantom{-}0.5  \\phantom{-}0.5  \\phantom{-}0.5 \\\\\n\\phantom{-}1.5  \\phantom{-}1.5  \\phantom{-}1.5\n\\end{pmatrix}.\n$$\n\n步骤 $2$：基因间的样本协方差矩阵。使用分母 $n-1=3$，基因的样本协方差为\n$$\nS \\;=\\; \\frac{1}{n-1}\\, Z^{\\top} Z \\;=\\; \\frac{1}{3}\\, Z^{\\top} Z.\n$$\n观察到 $Z$ 的每一行都是 $(1,\\,1,\\,1)$ 的标量倍数，因此所有三个中心化的基因列都是相同的。计算 $Z^{\\top}Z$ 时，请注意对于任意两列 $j$ 和 $k$，$(j,k)$ 位置的元素等于 $\\sum_{i=1}^{n} Z_{ij} Z_{ik}$。由于所有三列都相同，$Z^{\\top}Z$ 的每个元素都等于单个中心化列的平方和：\n$$\n\\sum_{i=1}^{4} Z_{i1}^{2} \\;=\\; (-1.5)^{2} + (-0.5)^{2} + (0.5)^{2} + (1.5)^{2} \\;=\\; 2.25 + 0.25 + 0.25 + 2.25 \\;=\\; 5.\n$$\n因此，\n$$\nZ^{\\top} Z \\;=\\; 5 \\begin{pmatrix}\n1  1  1 \\\\\n1  1  1 \\\\\n1  1  1\n\\end{pmatrix}\n\\quad\\text{and}\\quad\nS \\;=\\; \\frac{1}{3} Z^{\\top}Z \\;=\\; \\frac{5}{3}\n\\begin{pmatrix}\n1  1  1 \\\\\n1  1  1 \\\\\n1  1  1\n\\end{pmatrix}.\n$$\n\n步骤 $3$：特征分解。设 $J \\in \\mathbb{R}^{3 \\times 3}$ 表示全一矩阵，即对于所有 $j,k$，$J_{jk}=1$。根据基本原理可知，$J$ 的秩为 $1$，其特征值为 $3$ 和 $0$（$0$ 的重数为 $2$）。对应于特征值 $3$ 的一个单位范数特征向量与 $(1,\\,1,\\,1)^{\\top}$ 成比例，具体为\n$$\nv \\;=\\; \\frac{1}{\\sqrt{3}} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}.\n$$\n由于 $S = \\frac{5}{3} J$，$S$ 的特征值为 $\\lambda_{1} = \\frac{5}{3} \\cdot 3 = 5$ 以及 $\\lambda_{2} = 0$, $\\lambda_{3} = 0$，其特征向量与 $J$ 相同。因此，基因空间中的第一个主成分载荷向量是与 $\\lambda_{1}=5$ 相关联的单位范数特征向量，即上面的 $v$。选择符号使得第一个非零项为正，得到\n$$\nv \\;=\\; \\left( \\frac{1}{\\sqrt{3}},\\; \\frac{1}{\\sqrt{3}},\\; \\frac{1}{\\sqrt{3}} \\right).\n$$\n\n因此，按 $(G_1, G_2, G_3)$ 的顺序排列并写成 $1 \\times 3$ 行矩阵的形式，第一个主成分载荷向量是\n$$\n\\begin{pmatrix}\n\\frac{1}{\\sqrt{3}}  \\frac{1}{\\sqrt{3}}  \\frac{1}{\\sqrt{3}}\n\\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{\\sqrt{3}}  \\frac{1}{\\sqrt{3}}  \\frac{1}{\\sqrt{3}}\\end{pmatrix}}$$", "id": "2416060"}, {"introduction": "在掌握了PCA的计算步骤之后，我们来探讨其背后的“为什么”。本练习通过一个简化的理论模型，揭示了第一主成分解释的方差比例与变量间的相关性是如何直接关联的 [@problem_id:1946278]。这将帮助你建立一种直观的理解，即将PCA视为一种总结数据中冗余信息的强大工具。", "problem": "一架自主环境监测无人机使用一对相同的传感器来测量大气压。设这两个传感器的读数在减去其长期平均值进行中心化后，由随机变量 $X_1$ 和 $X_2$ 表示。\n\n这些读数的联合行为由一个二元随机向量 $(X_1, X_2)$ 描述，其协方差矩阵为 $\\Sigma$。由于这两个传感器类型相同，且受到类似的环境波动影响，它们具有相同的方差，$\\text{Var}(X_1) = \\text{Var}(X_2) = \\sigma^2$，其中 $\\sigma > 0$ 为某个常数。它们的读数也是相关的，相关系数为 $\\rho$，满足 $0  \\rho  1$。因此，协方差矩阵由下式给出：\n$$\n\\Sigma = \\begin{pmatrix} \\sigma^2  \\rho\\sigma^2 \\\\ \\rho\\sigma^2  \\sigma^2 \\end{pmatrix}\n$$\n为了减少数据冗余并识别变异的主轴，工程团队应用了主成分分析（PCA）。PCA将原始的相关变量 $(X_1, X_2)$ 转换为一组新的不相关变量，称为主成分。第一主成分被定义为能够捕捉最大可能方差的 $X_1$ 和 $X_2$ 的线性组合。\n\n确定数据中总方差由第一主成分解释的比例。将您的答案表示为以 $\\rho$ 为变量的符号表达式。", "solution": "我们给定一个中心化的二元随机向量，其协方差矩阵为\n$$\n\\Sigma=\\begin{pmatrix}\\sigma^{2}  \\rho\\sigma^{2} \\\\ \\rho\\sigma^{2}  \\sigma^{2}\\end{pmatrix},\n$$\n其中 $0  \\rho  1$ 且 $\\sigma > 0$。在PCA中，主成分的方差是协方差矩阵的特征值。第一主成分所解释的总方差比例等于其特征值除以总方差，总方差即为 $\\Sigma$ 的迹。\n\n首先，通过求解特征方程来计算 $\\Sigma$ 的特征值\n$$\n\\det(\\Sigma-\\lambda I)=0.\n$$\n我们有\n$$\n\\det\\begin{pmatrix}\\sigma^{2}-\\lambda  \\rho\\sigma^{2} \\\\ \\rho\\sigma^{2}  \\sigma^{2}-\\lambda\\end{pmatrix}\n=(\\sigma^{2}-\\lambda)^{2}-(\\rho\\sigma^{2})^{2}=0.\n$$\n因此，\n$$\n(\\sigma^{2}-\\lambda)^{2}=\\rho^{2}\\sigma^{4}\n\\quad\\Longrightarrow\\quad\n\\sigma^{2}-\\lambda=\\pm\\rho\\sigma^{2}\n\\quad\\Longrightarrow\\quad\n\\lambda=\\sigma^{2}(1\\pm\\rho).\n$$\n由于 $0  \\rho  1$，最大的特征值为\n$$\n\\lambda_{1}=\\sigma^{2}(1+\\rho).\n$$\n总方差等于 $\\Sigma$ 的迹，\n$$\n\\operatorname{tr}(\\Sigma)=\\sigma^{2}+\\sigma^{2}=2\\sigma^{2},\n$$\n这也等于特征值之和 $\\sigma^{2}(1+\\rho)+\\sigma^{2}(1-\\rho)=2\\sigma^{2}$。因此，由第一主成分解释的总方差比例为\n$$\n\\frac{\\lambda_{1}}{\\operatorname{tr}(\\Sigma)}=\\frac{\\sigma^{2}(1+\\rho)}{2\\sigma^{2}}=\\frac{1+\\rho}{2}.\n$$", "answer": "$$\\boxed{\\frac{1+\\rho}{2}}$$", "id": "1946278"}, {"introduction": "将理论和计算基础付诸实践时，我们会遇到一些关键的“陷阱”。本练习通过一个编码模拟，生动地展示了当变量具有不同单位或尺度时，直接对原始数据进行PCA会产生多么具有误导性的结果，以及数据标准化是如何解决这个问题的 [@problem_id:2421735]。这个练习强调了在大多数真实世界应用中，标准化是进行PCA前一个不可或缺的步骤。", "problem": "要求您使用主成分分析（PCA）的基本原理来演示，当变量以不同单位度量时，若不进行标准化，会如何扭曲估计出的主方向和解释方差。请在一个纯粹的数学框架下进行，使用一个合成数据生成过程，该过程模拟典型的金融变量（如价格和交易量）。您将实现完整的分析流程，并报告量化诊断指标，以比较对原始数据进行 PCA 与对标准化数据进行 PCA 的结果。\n\n基本原理：\n- PCA 寻求最大化样本方差的正交基方向。给定一个中心化的数据矩阵 $X \\in \\mathbb{R}^{T \\times n}$，样本协方差矩阵为 $\\Sigma = \\frac{1}{T-1} X^\\top X$。主成分是 $\\Sigma$ 的特征向量，对应其特征值，并按从大到小的顺序排列。\n- 标准化将每个变量 $x_j$ 转换为 $\\tilde{x}_j = \\frac{x_j - \\bar{x}_j}{\\hat{\\sigma}_j}$，其中 $\\bar{x}_j$ 是样本均值，$\\hat{\\sigma}_j$ 是样本标准差，使得每个标准化后的变量都具有单位样本方差。对标准化数据进行 PCA 等同于对样本相关系数矩阵进行 PCA。\n- 对变量应用对角缩放 $D = \\operatorname{diag}(s_1,\\dots,s_n)$，即 $X \\mapsto X D$，会将协方差矩阵的元素乘以 $s_i s_j$，从而改变特征向量，除非所有的 $s_j$ 都相等。\n\n数据生成过程：\n- 对于每个测试用例 $k$，固定样本数量 $T_k \\in \\mathbb{N}$、变量数量 $n_k \\in \\mathbb{N}$、因子载荷 $b^{(k)} \\in \\mathbb{R}^{n_k}$、异质性标准差 $u^{(k)} \\in \\mathbb{R}^{n_k}$ 和单位尺度 $s^{(k)} \\in \\mathbb{R}^{n_k}$。\n- 为 $t = 1,\\dots,T_k$ 生成一个共同因子 $f_t \\sim \\mathcal{N}(0,1)$，并为每个 $t$ 和 $j$ 生成相互独立的异质性噪声 $e_{t,j} \\sim \\mathcal{N}(0,(u^{(k)}_j)^2)$。\n- 为 $t=1,\\dots,T_k$ 和 $j=1,\\dots,n_k$ 构建原始观测值 $x_{t,j} = s^{(k)}_j \\left( b^{(k)}_j f_t + e_{t,j} \\right)$。\n- 在计算任何协方差之前，通过减去其样本均值来中心化 $X$ 的每一列。\n\n每个测试用例的计算任务：\n- 从中心化的原始数据 $X$ 计算样本协方差矩阵 $\\Sigma_{\\text{raw}}$，并获得第一个主成分的特征向量 $v_{\\text{raw}}$（单位范数）及其特征值 $\\lambda_{\\text{raw}}$。\n- 将 $X$ 的每一列标准化为单位样本方差以获得 $Z$，计算 $\\Sigma_{\\text{std}} = \\frac{1}{T_k-1} Z^\\top Z$（即样本相关系数矩阵），并获得第一个主成分的特征向量 $v_{\\text{std}}$（单位范数）及其特征值 $\\lambda_{\\text{std}}$。\n- 计算角度 $\\theta = \\arccos\\!\\left( \\left| v_{\\text{raw}}^\\top v_{\\text{std}} \\right| \\right)$；以弧度报告 $\\theta$。\n- 计算解释方差份额的差异 $\\Delta = \\left| \\frac{\\lambda_{\\text{raw}}}{\\operatorname{tr}(\\Sigma_{\\text{raw}})} - \\frac{\\lambda_{\\text{std}}}{\\operatorname{tr}(\\Sigma_{\\text{std}})} \\right|$，该值必须以小数形式报告（而非百分比）。\n\n随机性与可复现性：\n- 在整个实验中使用固定的伪随机数生成器种子 $314159$ 以确保结果可复现。\n\n测试套件：\n- 共有 3 个测试用例。对于每个测试用例 $k$，使用以下参数 $(T_k, n_k, b^{(k)}, u^{(k)}, s^{(k)})$：\n  - 用例 1（单位相似，两个变量）：\n    - $T_1 = 500$, $n_1 = 2$,\n    - $b^{(1)} = [1.0, 0.9]$,\n    - $u^{(1)} = [0.1, 0.1]$,\n    - $s^{(1)} = [1.0, 1.2]$。\n  - 用例 2（单位不匹配，两个变量：一个因尺度而占主导地位）：\n    - $T_2 = 500$, $n_2 = 2$,\n    - $b^{(2)} = [1.0, 0.9]$,\n    - $u^{(2)} = [0.1, 0.1]$,\n    - $s^{(2)} = [1000.0, 1.0]$。\n  - 用例 3（单位不匹配，三个变量：一个巨大尺度，一个中等尺度，一个微小尺度）：\n    - $T_3 = 800$, $n_3 = 3$,\n    - $b^{(3)} = [0.2, 1.0, 1.0]$,\n    - $u^{(3)} = [0.3, 0.2, 0.2]$,\n    - $s^{(3)} = [1000.0, 1.0, 0.01]$。\n\n每个测试用例的所需输出：\n- 一个包含两个浮点数的列表 $[\\theta, \\Delta]$，其中 $\\theta$ 是以弧度为单位的角度，$\\Delta$ 是解释方差份额的绝对差。两个值都必须精确到 6 位小数。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含结果，格式为一个由各用例列表组成的逗号分隔列表，并用方括号括起来，例如 $[[\\theta_1,\\Delta_1],[\\theta_2,\\Delta_2],[\\theta_3,\\Delta_3]]$，每个浮点数都精确到 6 位小数，角度以弧度为单位。", "solution": "所提出的问题是一个有效且定义明确的计算统计学练习，专门用于演示主成分分析（PCA）对变量尺度的敏感性。它在科学上是合理的，基于线性代数和统计学的基础原理，并且所有参数和程序都已足够清晰地指定，以允许一个唯一、可验证的解。我们将继续进行分析。\n\n其核心论点是，PCA作为一种方差最大化技术，不具有尺度不变性。当变量以迥然不同的单位度量时（例如，以美元计的股价与其以百万股计的交易量），方差最大的变量将在机制上主导第一个主成分。这通常是所选单位造成的人为结果，而不是真正潜在重要性的指标。标准化是标准的补救措施，它将所有变量转换为一个共同的尺度（单位方差），从而使分析侧重于数据的相关性结构，而不是任意的度量尺度。\n\n我们首先将数据生成和分析流程形式化。\n\n**1. 数据生成过程**\n\n对于每个测试用例 $k$，我们给定样本量 $T_k$、变量数 $n_k$、因子载荷 $b^{(k)} \\in \\mathbb{R}^{n_k}$、异质性标准差 $u^{(k)} \\in \\mathbb{R}^{n_k}$ 以及单位尺度 $s^{(k)} \\in \\mathbb{R}^{n_k}$。\n\n数据由单因子模型生成。对于每个时间点 $t=1, \\dots, T_k$，从标准正态分布中抽取一个共同的潜在因子 $f_t \\sim \\mathcal{N}(0, 1)$。对于每个变量 $j=1, \\dots, n_k$，从 $\\mathcal{N}(0, (u^{(k)}_j)^2)$ 中抽取一个异质性噪声项 $e_{t,j}$。所有的 $f_t$ 和 $e_{t,j}$ 都是相互独立的。\n\n变量 $j$ 在时间 $t$ 的观测值构建如下：\n$$\nx_{t,j} = s^{(k)}_j \\left( b^{(k)}_j f_t + e_{t,j} \\right)\n$$\n这构成了一个数据矩阵 $X \\in \\mathbb{R}^{T_k \\times n_k}$，其列代表不同的变量。尺度因子 $s^{(k)}_j$ 代表变量 $j$ 的任意度量单位。\n\n**2. 对原始数据进行 PCA（基于协方差的 PCA）**\n\nPCA 的第一步是通过减去列样本均值来中心化数据。令 $\\bar{x}_j = \\frac{1}{T_k} \\sum_{t=1}^{T_k} x_{t,j}$ 为第 $j$ 个变量的样本均值。中心化后的数据矩阵记为 $X_c$，其元素为 $(X_c)_{t,j} = x_{t,j} - \\bar{x}_j$。\n\n然后计算样本协方差矩阵 $\\Sigma_{\\text{raw}}$：\n$$\n\\Sigma_{\\text{raw}} = \\frac{1}{T_k-1} X_c^\\top X_c\n$$\n主成分是 $\\Sigma_{\\text{raw}}$ 的特征向量。我们对该矩阵进行特征分解：\n$$\n\\Sigma_{\\text{raw}} V = V \\Lambda\n$$\n其中 $V$ 是标准正交特征向量组成的矩阵，$\\Lambda$ 是相应特征值组成的对角矩阵。特征值按降序排列，$\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_{n_k}$。第一个主成分是与最大特征值 $\\lambda_1$ 相关联的特征向量 $v_1$。对于此问题，我们将其表示为 $v_{\\text{raw}}$，特征值表示为 $\\lambda_{\\text{raw}}$。\n\n**3. 对标准化数据进行 PCA（基于相关系数的 PCA）**\n\n为了消除任意尺度的影响，我们对数据进行标准化。对于原始数据矩阵 $X$ 的每一列 $j$，我们计算其样本标准差，$\\hat{\\sigma}_j = \\sqrt{\\frac{1}{T_k-1} \\sum_{t=1}^{T_k} (x_{t,j} - \\bar{x}_j)^2}$。\n\n标准化数据矩阵 $Z$ 的元素构建如下：\n$$\nz_{t,j} = \\frac{x_{t,j} - \\bar{x}_j}{\\hat{\\sigma}_j}\n$$\n根据定义，$Z$ 的每一列的样本均值为 0，样本方差为 1。\n\n然后对这个标准化数据 $Z$ 进行 PCA。相关的矩阵是 $Z$ 的样本协方差矩阵，我们记为 $\\Sigma_{\\text{std}}$：\n$$\n\\Sigma_{\\text{std}} = \\frac{1}{T_k-1} Z^\\top Z\n$$\n由于 $Z$ 的每一列都具有单位方差，$\\Sigma_{\\text{std}}$ 的对角元素都为 1，而非对角元素 $(i, j)$ 是原始变量 $x_i$ 和 $x_j$ 之间的样本相关系数。因此，$\\Sigma_{\\text{std}}$ 是 $X$ 的样本相关系数矩阵。\n\n我们对 $\\Sigma_{\\text{std}}$ 进行特征分解，以找到其最大特征值 $\\lambda_{\\text{std}}$ 和相应的特征向量 $v_{\\text{std}}$。\n\n**4. 诊断指标**\n\n为了量化因未标准化而造成的扭曲，我们计算两个指标：\n\n- **主成分之间的夹角**：主成分方向 $v_{\\text{raw}}$ 和 $v_{\\text{std}}$ 是 $\\mathbb{R}^{n_k}$ 中的单位向量。它们之间的夹角衡量了最大方差方向移动了多少。由于特征向量的定义只到符号为止（即，如果 $v$ 是一个特征向量，那么 $-v$ 也是），我们计算它们所张成的直线之间的锐角：\n  $$\n  \\theta = \\arccos\\left( \\left| v_{\\text{raw}}^\\top v_{\\text{std}} \\right| \\right)\n  $$\n  $\\theta=0$ 的值表示完全对齐，而一个大的角度（接近 $\\pi/2$）表示严重错位。\n\n- **解释方差份额的差异**：由第一个主成分解释的总方差比例由其特征值除以所有特征值之和给出。特征值之和等于矩阵的迹，$\\operatorname{tr}(\\Sigma) = \\sum_{j=1}^{n_k} \\Sigma_{jj}$，代表数据中的总方差。我们计算解释方差份额的绝对差：\n  $$\n  \\Delta = \\left| \\frac{\\lambda_{\\text{raw}}}{\\operatorname{tr}(\\Sigma_{\\text{raw}})} - \\frac{\\lambda_{\\text{std}}}{\\operatorname{tr}(\\Sigma_{\\text{std}})} \\right|\n  $$\n  注意，对于标准化数据，$\\operatorname{tr}(\\Sigma_{\\text{std}}) = n_k$，即变量的数量。一个大的 $\\Delta$ 值表明两种方法对第一个主成分的重要性给出了截然不同的评估。\n\n该过程将使用指定的参数和固定的随机种子对每个测试用例执行，以保证可复现性。预计结果将显示用例 1（尺度相似）的扭曲最小，而用例 2 和用例 3（尺度迥异）的扭曲显著，从而验证了在实践中标准化的必要性。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the PCA problem by comparing results from raw and standardized data.\n\n    This function iterates through a set of predefined test cases. For each case, it:\n    1. Generates synthetic financial data based on a single-factor model with specified scales.\n    2. Performs PCA on the raw, centered data.\n    3. Performs PCA on the standardized data (equivalent to using the correlation matrix).\n    4. Computes two diagnostic metrics:\n        - The angle between the first principal components from the raw and standardized analyses.\n        - The absolute difference in the fraction of variance explained by the first component.\n    5. Collects and formats the results according to the problem specification.\n    \"\"\"\n    # Use a fixed pseudorandom number generator seed for reproducibility.\n    seed = 314159\n    rng = np.random.default_rng(seed)\n\n    # Test cases defined as (T_k, n_k, b^(k), u^(k), s^(k)))\n    test_cases = [\n        (500, 2, np.array([1.0, 0.9]), np.array([0.1, 0.1]), np.array([1.0, 1.2])),\n        (500, 2, np.array([1.0, 0.9]), np.array([0.1, 0.1]), np.array([1000.0, 1.0])),\n        (800, 3, np.array([0.2, 1.0, 1.0]), np.array([0.3, 0.2, 0.2]), np.array([1000.0, 1.0, 0.01]))\n    ]\n\n    all_results = []\n\n    for T, n, b, u, s in test_cases:\n        # 1. Data Generation\n        # Generate common factor f_t ~ N(0,1)\n        f = rng.normal(loc=0.0, scale=1.0, size=T)\n        \n        # Generate idiosyncratic noises e_{t,j} ~ N(0, u_j^2)\n        # E is a T x n matrix\n        E = rng.normal(loc=0.0, scale=u, size=(T, n))\n        \n        # Construct raw observations x_{t,j} = s_j * (b_j * f_t + e_{t,j})\n        X = s * (np.outer(f, b) + E)\n\n        # 2. PCA on Raw Data\n        # Center the data matrix X\n        X_centered = X - np.mean(X, axis=0)\n        \n        # Compute the sample covariance matrix (ddof=1 for unbiased estimator)\n        Sigma_raw = np.cov(X_centered, rowvar=False, ddof=1)\n        \n        # Eigendecomposition of the symmetric covariance matrix\n        # eigh returns eigenvalues in ascending order and corresponding eigenvectors in columns\n        eigvals_raw, eigvecs_raw = np.linalg.eigh(Sigma_raw)\n        \n        # The first principal component corresponds to the largest eigenvalue\n        lambda_raw = eigvals_raw[-1]\n        v_raw = eigvecs_raw[:, -1]\n\n        # 3. PCA on Standardized Data\n        # Standardize the data matrix X\n        stds = np.std(X, axis=0, ddof=1)\n        Z = X_centered / stds\n        \n        # Compute the sample correlation matrix (covariance of standardized data)\n        Sigma_std = np.cov(Z, rowvar=False, ddof=1)\n        \n        # Eigendecomposition of the correlation matrix\n        eigvals_std, eigvecs_std = np.linalg.eigh(Sigma_std)\n        \n        # The first principal component corresponds to the largest eigenvalue\n        lambda_std = eigvals_std[-1]\n        v_std = eigvecs_std[:, -1]\n\n        # 4. Diagnostic Computations\n        # Angle theta between the first principal components\n        # Take absolute value of dot product to handle sign ambiguity of eigenvectors\n        cos_theta = np.abs(np.dot(v_raw, v_std))\n        # Clip to prevent domain errors with arccos due to potential floating point inaccuracies\n        theta = np.arccos(np.clip(cos_theta, -1.0, 1.0))\n\n        # Difference in explained variance shares\n        total_var_raw = np.trace(Sigma_raw)\n        total_var_std = np.trace(Sigma_std) # This is always n for a correlation matrix\n        \n        share_raw = lambda_raw / total_var_raw\n        share_std = lambda_std / total_var_std\n        \n        delta = np.abs(share_raw - share_std)\n\n        # Append results, rounded to 6 decimal places\n        all_results.append([round(theta, 6), round(delta, 6)])\n\n    # Format the final output string as specified\n    formatted_results = [f\"[{theta:.6f},{delta:.6f}]\" for theta, delta in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2421735"}]}