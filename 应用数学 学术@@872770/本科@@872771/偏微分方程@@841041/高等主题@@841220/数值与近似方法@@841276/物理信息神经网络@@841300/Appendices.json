{"hands_on_practices": [{"introduction": "应用物理信息神经网络（PINN）的第一步，也是最关键的一步，是将一个物理问题（由偏微分方程及其边界条件定义）转化为一个神经网络可以优化的数学目标。这个练习将指导你完成这一核心过程。我们将以泊松方程为例，这是一个在静电学、热传导和流体力学中无处不在的方程，你将学习如何构建一个包含偏微分方程（PDE）残差和边界条件（BC）的总损失函数[@problem_id:2126324]。", "problem": "一位研究人员正在构建一个物理信息神经网络（PINN），以求解二维方形区域内的静电势 $V(x,y)$ 的近似解。电势的物理行为由泊松方程描述：\n$$\n\\nabla^2 V(x,y) = -f(x,y)\n$$\n其中 $f(x,y)$ 表示给定的电荷分布密度，$\\nabla^2 = \\frac{\\partial^2}{\\partial x^2} + \\frac{\\partial^2}{\\partial y^2}$ 是拉普拉斯算子。电势定义在域 $D = \\{(x,y) \\mid -L \\le x \\le L, -L \\le y \\le L\\}$ 上。该域的边界 $\\partial D$ 保持在零电势（接地），这施加了边界条件 $V(x,y) = 0$ 对于所有 $(x,y) \\in \\partial D$。\n\nPINN模型，记作 $\\hat{V}(x,y; \\theta)$，通过最小化一个包含问题物理信息的损失函数 $L(\\theta)$ 来学习近似 $V(x,y)$。这里，$\\theta$ 表示神经网络的所有可训练参数。损失函数使用两组离散点计算：\n1.  一组位于域 $D$ 内部的 $N_{pde}$ 个配置点， $S_{pde} = \\{(x_i, y_i) \\mid i=1, \\dots, N_{pde}\\}$。\n2.  一组位于边界 $\\partial D$ 上的 $N_{bc}$ 个边界点， $S_{bc} = \\{(x_j, y_j) \\mid j=1, \\dots, N_{bc}\\}$。\n\n总损失函数 $L(\\theta)$ 是两项均方误差之和：一项用于控制偏微分方程（$L_{pde}$），另一项用于边界条件（$L_{bc}$）。\n\n构建总损失函数 $L(\\theta) = L_{pde} + L_{bc}$ 的数学表达式。您的表达式应使用网络输出 $\\hat{V}$、其二阶偏导数、函数 $f$、给定的点集及其各自的大小 $N_{pde}$ 和 $N_{bc}$ 来表示。", "solution": "我们从控制泊松方程和边界条件开始：\n$$\n\\nabla^{2}V(x,y)=-f(x,y), \\quad V(x,y)=0 \\text{ for } (x,y)\\in \\partial D.\n$$\n一个物理信息神经网络通过 $\\hat{V}(x,y;\\theta)$ 来近似 $V$。在内部配置点 $(x_{i},y_{i})\\in S_{pde}$ 处的偏微分方程（PDE）残差通过将泊松方程施加于 $\\hat{V}$ 来定义：\n$$\nr_{i}(\\theta)=\\nabla^{2}\\hat{V}(x_{i},y_{i};\\theta)+f(x_{i},y_{i}).\n$$\n使用二维拉普拉斯算子的定义，这等效于\n$$\nr_{i}(\\theta)=\\frac{\\partial^{2}\\hat{V}}{\\partial x^{2}}(x_{i},y_{i};\\theta)+\\frac{\\partial^{2}\\hat{V}}{\\partial y^{2}}(x_{i},y_{i};\\theta)+f(x_{i},y_{i}).\n$$\n在 $S_{pde}$ 上强制执行偏微分方程的均方误差则为\n$$\nL_{pde}(\\theta)=\\frac{1}{N_{pde}}\\sum_{i=1}^{N_{pde}}\\left(r_{i}(\\theta)\\right)^{2}=\\frac{1}{N_{pde}}\\sum_{i=1}^{N_{pde}}\\left(\\frac{\\partial^{2}\\hat{V}}{\\partial x^{2}}(x_{i},y_{i};\\theta)+\\frac{\\partial^{2}\\hat{V}}{\\partial y^{2}}(x_{i},y_{i};\\theta)+f(x_{i},y_{i})\\right)^{2}.\n$$\n边界条件 $V=0$ 在 $\\partial D$ 上通过惩罚在边界点 $(x_{j},y_{j})\\in S_{bc}$ 处 $\\hat{V}$ 与零的偏差来强制执行：\n$$\nL_{bc}(\\theta)=\\frac{1}{N_{bc}}\\sum_{j=1}^{N_{bc}}\\left(\\hat{V}(x_{j},y_{j};\\theta)-0\\right)^{2}=\\frac{1}{N_{bc}}\\sum_{j=1}^{N_{bc}}\\left(\\hat{V}(x_{j},y_{j};\\theta)\\right)^{2}.\n$$\n因此，总损失是这两项均方误差之和：\n$$\nL(\\theta)=L_{pde}(\\theta)+L_{bc}(\\theta)=\\frac{1}{N_{pde}}\\sum_{i=1}^{N_{pde}}\\left(\\frac{\\partial^{2}\\hat{V}}{\\partial x^{2}}(x_{i},y_{i};\\theta)+\\frac{\\partial^{2}\\hat{V}}{\\partial y^{2}}(x_{i},y_{i};\\theta)+f(x_{i},y_{i})\\right)^{2}+\\frac{1}{N_{bc}}\\sum_{j=1}^{N_{bc}}\\left(\\hat{V}(x_{j},y_{j};\\theta)\\right)^{2}.\n$$", "answer": "$$\\boxed{\\frac{1}{N_{pde}}\\sum_{i=1}^{N_{pde}}\\left(\\frac{\\partial^{2}\\hat{V}}{\\partial x^{2}}(x_{i},y_{i};\\theta)+\\frac{\\partial^{2}\\hat{V}}{\\partial y^{2}}(x_{i},y_{i};\\theta)+f(x_{i},y_{i})\\right)^{2}+\\frac{1}{N_{bc}}\\sum_{j=1}^{N_{bc}}\\left(\\hat{V}(x_{j},y_{j};\\theta)\\right)^{2}}$$", "id": "2126324"}, {"introduction": "虽然在求解域内部强制执行局部偏微分方程是 PINN 的基础，但我们可以通过引入全局物理原理（如守恒律）来使其更加稳健和准确。这种方法确保了网络的预测不仅在每个点上都近似满足方程，而且在整体上还遵守了系统的基本物理特性。在这个练习中，你将探索如何将能量守恒定律直接编码到波动方程的损失函数中[@problem_id:2126322]，这代表了“物理启发”范式的一种更高级的应用。", "problem": "物理信息神经网络 (PINN) 是一种机器学习模型，用于寻找偏微分方程 (PDE) 的近似解 $\\hat{u}(x,t; \\theta)$，其中 $\\theta$ 代表网络的可训练参数。训练过程旨在最小化一个损失函数，该函数惩罚对已知物理定律、初始条件和边界条件的偏离。\n\n考虑在时空域 $(x,t) \\in [x_L, x_R] \\times [0, T]$ 上的一维线性波动方程：\n$$ \\frac{\\partial^2 u}{\\partial t^2} - c^2 \\frac{\\partial^2 u}{\\partial x^2} = 0 $$\n其中波速 $c$ 为常数。该系统满足初始条件 $u(x,0) = g(x)$ 和 $\\frac{\\partial u}{\\partial t}(x,0) = h(x)$，以及边界条件 $u(x_L, t) = u_L(t)$ 和 $u(x_R, t) = u_R(t)$。\n\n波动方程的一个已知性质是总能量守恒，其定义为：\n$$ E(t) = \\frac{1}{2} \\int_{x_L}^{x_R} \\left( \\left(\\frac{\\partial u}{\\partial t}\\right)^2 + c^2 \\left(\\frac{\\partial u}{\\partial x}\\right)^2 \\right) dx $$\n对于任何有效解，在所有 $t \\in [0,T]$ 时， $E(t)$ 必须保持恒定并等于其初始值 $E(0)$。\n\n您的任务是构建一个修改后的总损失函数 $\\mathcal{L}_{\\text{total}}$，该函数显式地包含对任何违反此能量守恒定律的惩罚项。总损失是四个分量的加权和：\n$$ \\mathcal{L}_{\\text{total}} = \\lambda_{\\text{PDE}}\\mathcal{L}_{\\text{PDE}} + \\lambda_{\\text{IC}}\\mathcal{L}_{\\text{IC}} + \\lambda_{\\text{BC}}\\mathcal{L}_{\\text{BC}} + \\lambda_{\\text{E}}\\mathcal{L}_{\\text{E}} $$\n其中 $\\lambda$ 项是正的加权超参数。\n\n基于以下配置点集定义损失函数的分量，将每个分量表示为均方误差 (MSE)：\n- **PDE损失 $\\mathcal{L}_{\\text{PDE}}$**：在一组位于域内部的 $N_{\\text{PDE}}$ 个点 $\\mathcal{S}_{\\text{PDE}} = \\{(x_i, t_i)\\}_{i=1}^{N_{\\text{PDE}}}$ 上进行评估。\n- **初始条件损失 $\\mathcal{L}_{\\text{IC}}$**：在一组位于初始时间线上的 $N_{\\text{IC}}$ 个点 $\\mathcal{S}_{\\text{IC}} = \\{(x_j, 0)\\}_{j=1}^{N_{\\text{IC}}}$ 上进行评估。MSE 应同时考虑 $u$ 及其时间导数 $u_t$ 的误差。\n- **边界条件损失 $\\mathcal{L}_{\\text{BC}}$**：在一组位于空间边界 $x \\in \\{x_L, x_R\\}$ 上的 $N_{\\text{BC}}$ 个点 $\\mathcal{S}_{\\text{BC}} = \\{(x_k, t_k)\\}_{k=1}^{N_{\\text{BC}}}$ 上进行评估。对于点 $(x_k, t_k) \\in \\mathcal{S}_{\\text{BC}}$，目标边界值为 $u_{\\text{BC}}(x_k, t_k)$，如果 $x_k=x_L$，则其等于 $u_L(t_k)$；如果 $x_k=x_R$，则其等于 $u_R(t_k)$。\n- **能量守恒损失 $\\mathcal{L}_{\\text{E}}$**：此损失惩罚网络在不同时刻预测的能量与真实初始能量之间的偏差。能量积分将使用中点黎曼和在 $[x_L, x_R]$ 的 $N_E$ 个均匀子区间上进行近似，每个子区间的宽度为 $\\Delta x = (x_R - x_L)/N_E$。该求和的空间评估点为 $\\{x_m = x_L + (m - 1/2)\\Delta x\\}_{m=1}^{N_E}$。守恒定律在一组 $N_T$ 个时间实例 $\\mathcal{T}_E = \\{t_l\\}_{l=1}^{N_T}$（其中 $t_l > 0$）上强制执行。\n\n设神经网络对解的近似为 $\\hat{u}(x,t; \\theta)$，其关于 $x$ 和 $t$ 的偏导数分别表示为 $\\hat{u}_x$, $\\hat{u}_t$, $\\hat{u}_{xx}$ 和 $\\hat{u}_{tt}$。假设函数 $g(x)$, $h(x)$ 以及空间导数 $g'(x)$ 是已知的，并且可以在所需点上进行评估。请构建 $\\mathcal{L}_{\\text{total}}$ 的完整表达式。您的最终答案应为单个解析表达式，用网络输出（例如 $\\hat{u}(x,t)$）、其导数、给定函数、指定点和加权因子来表示。", "solution": "我们寻求一个总损失函数，该函数惩罚对PDE、初始条件、边界条件和能量守恒定律的违反。对于一个PINN近似解 $\\hat{u}(x,t;\\theta)$，我们根据波动方程在内部配置点 $(x_{i},t_{i}) \\in \\mathcal{S}_{\\text{PDE}}$ 上定义PDE残差：\n$$\nr(x_{i},t_{i};\\theta) = \\hat{u}_{tt}(x_{i},t_{i};\\theta) - c^{2}\\hat{u}_{xx}(x_{i},t_{i};\\theta).\n$$\nPDE损失是均方残差：\n$$\n\\mathcal{L}_{\\text{PDE}} = \\frac{1}{N_{\\text{PDE}}}\\sum_{i=1}^{N_{\\text{PDE}}} \\left(r(x_{i},t_{i};\\theta)\\right)^{2} = \\frac{1}{N_{\\text{PDE}}}\\sum_{i=1}^{N_{\\text{PDE}}} \\left(\\hat{u}_{tt}(x_{i},t_{i};\\theta) - c^{2}\\hat{u}_{xx}(x_{i},t_{i};\\theta)\\right)^{2}.\n$$\n对于初始条件，在点 $(x_{j},0) \\in \\mathcal{S}_{\\text{IC}}$ 处，目标值为 $u(x_{j},0)=g(x_{j})$ 和 $u_{t}(x_{j},0)=h(x_{j})$。初始条件损失是关于 $u$ 和 $u_{t}$ 的均方误差：\n$$\n\\mathcal{L}_{\\text{IC}} = \\frac{1}{2N_{\\text{IC}}}\\sum_{j=1}^{N_{\\text{IC}}} \\left[\\left(\\hat{u}(x_{j},0;\\theta)-g(x_{j})\\right)^{2} + \\left(\\hat{u}_{t}(x_{j},0;\\theta)-h(x_{j})\\right)^{2}\\right].\n$$\n对于边界条件，在点 $(x_{k},t_{k}) \\in \\mathcal{S}_{\\text{BC}}$ 且 $x_{k}\\in\\{x_{L},x_{R}\\}$ 处，目标边界值为 $u_{\\text{BC}}(x_{k},t_{k})$ （如果 $x_{k}=x_{L}$ 则等于 $u_{L}(t_{k})$，如果 $x_{k}=x_{R}$ 则等于 $u_{R}(t_{k})$）。边界损失是均方差异：\n$$\n\\mathcal{L}_{\\text{BC}} = \\frac{1}{N_{\\text{BC}}}\\sum_{k=1}^{N_{\\text{BC}}} \\left(\\hat{u}(x_{k},t_{k};\\theta) - u_{\\text{BC}}(x_{k},t_{k})\\right)^{2}.\n$$\n能量守恒要求对于所有 $t$ 都有 $E(t)=E(0)$。我们通过中点黎曼和来近似能量积分，该黎曼和作用于 $[x_{L},x_{R}]$ 上的 $N_{E}$ 个均匀子区间，其空间中点为 $x_{m}=x_{L}+\\left(m-\\frac{1}{2}\\right)\\frac{x_{R}-x_{L}}{N_{E}}$，宽度为 $\\Delta x=\\frac{x_{R}-x_{L}}{N_{E}}$。网络在时间 $t_{l}\\in\\mathcal{T}_{E}$ 预测的能量为\n$$\n\\widehat{E}(t_{l}) = \\frac{1}{2}\\frac{x_{R}-x_{L}}{N_{E}} \\sum_{m=1}^{N_{E}}\\left(\\hat{u}_{t}\\!\\left(x_{L}+\\left(m-\\frac{1}{2}\\right)\\frac{x_{R}-x_{L}}{N_{E}},t_{l};\\theta\\right)^{2} + c^{2}\\hat{u}_{x}\\!\\left(x_{L}+\\left(m-\\frac{1}{2}\\right)\\frac{x_{R}-x_{L}}{N_{E}},t_{l};\\theta\\right)^{2}\\right).\n$$\n使用相同的求积点从已知初始数据计算出的真实初始能量为\n$$\nE_{0} = \\frac{1}{2}\\frac{x_{R}-x_{L}}{N_{E}} \\sum_{m=1}^{N_{E}}\\left(h\\!\\left(x_{L}+\\left(m-\\frac{1}{2}\\right)\\frac{x_{R}-x_{L}}{N_{E}}\\right)^{2} + c^{2}\\left(g'\\!\\left(x_{L}+\\left(m-\\frac{1}{2}\\right)\\frac{x_{R}-x_{L}}{N_{E}}\\right)\\right)^{2}\\right).\n$$\n能量守恒损失是在选定时间实例上 $\\widehat{E}(t_{l})$ 与 $E_{0}$ 之间偏差的均方值：\n$\n\\mathcal{L}_{\\text{E}} = \\frac{1}{N_{T}}\\sum_{l=1}^{N_{T}}\\left(\\widehat{E}(t_{l}) - E_{0}\\right)^{2}.\n$\n将所有四个分量与正权重 $\\lambda_{\\text{PDE}},\\lambda_{\\text{IC}},\\lambda_{\\text{BC}},\\lambda_{\\text{E}}$ 相结合，得到总损失：\n$$\n\\mathcal{L}_{\\text{total}} = \\lambda_{\\text{PDE}}\\mathcal{L}_{\\text{PDE}} + \\lambda_{\\text{IC}}\\mathcal{L}_{\\text{IC}} + \\lambda_{\\text{BC}}\\mathcal{L}_{\\text{BC}} + \\lambda_{\\text{E}}\\mathcal{L}_{\\text{E}}.\n$$\n代入所有分量的显式形式，即可得到一个关于 $\\hat{u}$、其导数、给定数据、配置点和权重的单一解析表达式，符合要求。", "answer": "$$\\boxed{\\lambda_{\\text{PDE}} \\frac{1}{N_{\\text{PDE}}}\\sum_{i=1}^{N_{\\text{PDE}}}\\left(\\hat{u}_{tt}(x_{i},t_{i};\\theta)-c^{2}\\hat{u}_{xx}(x_{i},t_{i};\\theta)\\right)^{2}+\\lambda_{\\text{IC}}\\frac{1}{2N_{\\text{IC}}}\\sum_{j=1}^{N_{\\text{IC}}}\\left[\\left(\\hat{u}(x_{j},0;\\theta)-g(x_{j})\\right)^{2}+\\left(\\hat{u}_{t}(x_{j},0;\\theta)-h(x_{j})\\right)^{2}\\right]+\\lambda_{\\text{BC}}\\frac{1}{N_{\\text{BC}}}\\sum_{k=1}^{N_{\\text{BC}}}\\left(\\hat{u}(x_{k},t_{k};\\theta)-u_{\\text{BC}}(x_{k},t_{k})\\right)^{2}+\\lambda_{\\text{E}}\\frac{1}{N_{T}}\\sum_{l=1}^{N_{T}}\\left(\\frac{1}{2}\\frac{x_{R}-x_{L}}{N_{E}}\\sum_{m=1}^{N_{E}}\\left[\\hat{u}_{t}\\!\\left(x_{L}+\\left(m-\\frac{1}{2}\\right)\\frac{x_{R}-x_{L}}{N_{E}},t_{l};\\theta\\right)^{2}+c^{2}\\hat{u}_{x}\\!\\left(x_{L}+\\left(m-\\frac{1}{2}\\right)\\frac{x_{R}-x_{L}}{N_{E}},t_{l};\\theta\\right)^{2}\\right]-\\frac{1}{2}\\frac{x_{R}-x_{L}}{N_{E}}\\sum_{m=1}^{N_{E}}\\left[h\\!\\left(x_{L}+\\left(m-\\frac{1}{2}\\right)\\frac{x_{R}-x_{L}}{N_{E}}\\right)^{2}+c^{2}\\left(g'\\!\\left(x_{L}+\\left(m-\\frac{1}{2}\\right)\\frac{x_{R}-x_{L}}{N_{E}}\\right)\\right)^{2}\\right]\\right)^{2}}$$", "id": "2126322"}, {"introduction": "“物理启发”的原则不仅限于设计损失函数，它还可以深刻地影响神经网络本身的架构。通过精心选择网络的基函数（或激活函数），使其天生就满足控制方程，我们可以极大地简化学习过程。这个高级的动手实践将引导你解决时变薛定谔方程，你将构建一个其基函数自动满足薛定谔方程的网络[@problem_id:2427209]。这个练习不仅是一个编码挑战，它还揭示了 PINN 与经典谱方法之间的深刻联系，展示了将物理洞察力融入模型设计的强大能力。", "problem": "您需要实现一个完整、可运行的程序，该程序使用复值网络构建并求解一个用于时间相关薛定谔方程的物理信息神经网络。该偏微分方程是针对单个自由粒子的时间相关薛定谔方程，其哈密顿算符源于基本关系式，即质量为 $m$ 的自由粒子的哈密顿算符为 $H = -\\frac{\\hbar^{2}}{2m}\\frac{\\partial^{2}}{\\partial x^{2}}$，控制方程为 $i\\hbar \\frac{\\partial \\psi}{\\partial t} = H\\psi$。在无量纲单位制中进行计算，其中 $\\hbar = 1$ 且 $m = 1$，因此方程简化为 $i \\frac{\\partial \\psi}{\\partial t} = -\\frac{1}{2}\\frac{\\partial^{2} \\psi}{\\partial x^{2}}$。在一维空间域 $x \\in [0, 2\\pi]$ 上施加周期性边界条件，并考虑时间 $t \\in [0, T]$，其中 $T > 0$。初始条件为平面波 $\\psi(x, 0) = e^{i k x}$，其中 $k$ 为给定的整数波数。用于验证的精确解为 $\\psi(x,t) = e^{i(kx - \\omega t)}$，其中 $\\omega = \\frac{1}{2}k^{2}$。该解是通过将平面波代入薛定谔方程并使用哈密顿算符所蕴含的色散关系得出的。\n\n您的任务是通过构建以下形式的复值神经网络来推导、实现和解决一个物理信息学习问题。设该网络为一个具有复值激活函数和复值参数的单隐藏层模型。隐藏单元使用激活函数 $\\phi(z) = e^{i z}$，其中 $z$ 是输入的仿射函数，因此单个隐藏单元的形式为 $\\phi_{j}(x,t) = e^{i(k_{j} x - \\omega_{j} t)}$，其中 $k_{j}$ 和 $\\omega_{j}$ 为实数。网络输出为线性组合\n$$\n\\psi_{\\theta}(x,t) = \\sum_{j=1}^{M} \\alpha_{j} \\, e^{i(k_{j} x - \\omega_{j} t)},\n$$\n其中 $\\theta$ 表示复数输出权重 $\\{\\alpha_{j}\\}_{j=1}^{M}$ 和固定的隐藏参数 $\\{k_{j}, \\omega_{j}\\}_{j=1}^{M}$ 的集合。使用哈密顿算符的基本定义和链式法则来计算该模型的薛定谔方程残差，其形式为\n$$\n\\mathcal{R}(x,t;\\theta) = i \\frac{\\partial \\psi_{\\theta}}{\\partial t}(x,t) + \\frac{1}{2} \\frac{\\partial^{2} \\psi_{\\theta}}{\\partial x^{2}}(x,t).\n$$\n从上述核心定义出发，推导每个隐藏单元的残差，并展示物理原理如何通过色散关系来指导网络架构。然后，将物理信息学习问题构建为一个复值线性最小二乘系统，该系统同时强制满足：\n- 通过在配置点上最小化 $\\mathcal{R}(x,t;\\theta)$ 的模平方，在时空域内部满足偏微分方程，\n- 通过在初始点 $x \\in [0, 2\\pi]$ 上最小化 $\\left|\\psi_{\\theta}(x, 0) - e^{i k x}\\right|^{2}$，满足初始条件，\n- 通过在定义域边界上强制 $\\psi_{\\theta}(0, t) - \\psi_{\\theta}(2\\pi, t) = 0$ 和 $\\frac{\\partial \\psi_{\\theta}}{\\partial x}(0, t) - \\frac{\\partial \\psi_{\\theta}}{\\partial x}(2\\pi, t) = 0$（对于 $t \\in [0, T]$），满足周期性边界条件。\n\n为确保科学真实性和数值稳定性，选择隐藏参数 $\\{k_{j}\\}_{j=1}^{M}$ 为与周期性域匹配的整数波数，并使用基于物理的色散关系 $\\omega_{j} = \\frac{1}{2}k_{j}^{2}$，使得每个隐藏单元都精确满足薛定谔方程，从而使每个隐藏单元的残差恒为零。然后，学习过程简化为在最小二乘意义上确定能够最好地满足初始条件和周期性边界约束的复系数 $\\{\\alpha_{j}\\}_{j=1}^{M}$。您必须将其实现为一个单一、完整的程序，通过复数最小二乘法求解 $\\{\\alpha_{j}\\}_{j=1}^{M}$，然后评估学习到的解。\n\n测试套件和要求输出：\n- 使用三个测试用例，其波数 $k$ 和最终时间 $T$ 的参数集如下：\n  - 用例 1：$k = 0$, $T = 1$。\n  - 用例 2：$k = 1$, $T = \\frac{3}{4}$。\n  - 用例 3：$k = 2$, $T = \\frac{1}{2}$。\n- 对于每个用例，使用均匀覆盖 $[-K, K]$ 的整数波数 $k_{j}$ 构建具有 $M$ 个复数单元的特征集，其中 $K$ 为某个正整数且 $M = 2K+1$，并设置 $\\omega_{j} = \\frac{1}{2}k_{j}^{2}$。使用足够数量的初始条件采样点和周期性边界采样点，以确保最小二乘问题是良态的。物理残差必须被公式化，但由于色散关系，对于所选的特征，它在所有配置点上的值都将精确为零。\n- 对于每个用例，在求出系数后，在 $[0, 2\\pi] \\times [0, T]$ 上的均匀时空网格上评估学习到的解，并报告相对均方根误差\n$$\n\\varepsilon = \\left( \\frac{\\sum_{i=1}^{N_{x}}\\sum_{n=1}^{N_{t}} \\left|\\psi_{\\theta}(x_{i}, t_{n}) - e^{i(k x_{i} - \\frac{1}{2}k^{2} t_{n})}\\right|^{2}}{\\sum_{i=1}^{N_{x}}\\sum_{n=1}^{N_{t}} \\left|e^{i(k x_{i} - \\frac{1}{2}k^{2} t_{n})}\\right|^{2}} \\right)^{1/2}.\n$$\n由于精确解的模为单位1，分母可简化为评估点的数量，但您必须按照定义计算该表达式。\n- 您的程序必须生成单行输出，其中包含三个误差，格式为用方括号括起来的逗号分隔的 Python 列表，并按上述用例的顺序排列，例如 $[\\varepsilon_{1},\\varepsilon_{2},\\varepsilon_{3}]$。\n\n角度单位不适用。由于无量纲化，不需要物理单位。输出中的所有数值必须是浮点数。您的实现必须是自包含的，并且只能使用指定的库。每个测试用例的结果是一个浮点数。", "solution": "任务是为一维自由粒子时间相关的薛定谔方程构建并求解一个物理信息神经网络。在进行求解之前，需要对问题陈述进行严格的验证。\n\n首先，我将逐字提取给定的信息。\n- **控制方程**：在无量纲单位制（$\\hbar=1$，$m=1$）下，时间相关的薛定谔方程为 $i \\frac{\\partial \\psi}{\\partial t} = -\\frac{1}{2}\\frac{\\partial^{2} \\psi}{\\partial x^{2}}$。\n- **定义域**：空间 $x \\in [0, 2\\pi]$ 和时间 $t \\in [0, T]$，其中 $T > 0$。\n- **边界条件 (BCs)**：施加周期性边界条件。具体来说，$\\psi(0, t) = \\psi(2\\pi, t)$ 且 $\\frac{\\partial \\psi}{\\partial x}(0, t) = \\frac{\\partial \\psi}{\\partial x}(2\\pi, t)$。\n- **初始条件 (IC)**：平面波 $\\psi(x, 0) = e^{i k x}$，其中 $k$ 为给定的整数波数。\n- **用于验证的精确解**：$\\psi(x,t) = e^{i(kx - \\omega t)}$，色散关系为 $\\omega = \\frac{1}{2}k^{2}$。\n- **模型拟设 (Ansatz)**：一个复值的单隐藏层网络，形式为 $\\psi_{\\theta}(x,t) = \\sum_{j=1}^{M} \\alpha_{j} \\, e^{i(k_{j} x - \\omega_{j} t)}$，其中参数 $\\theta$ 是复系数 $\\{\\alpha_j\\}_{j=1}^M$。隐藏参数 $\\{k_{j}, \\omega_{j}\\}_{j=1}^{M}$ 是固定的。\n- **偏微分方程残差**：$\\mathcal{R}(x,t;\\theta) = i \\frac{\\partial \\psi_{\\theta}}{\\partial t}(x,t) + \\frac{1}{2} \\frac{\\partial^{2} \\psi_{\\theta}}{\\partial x^{2}}(x,t)$。\n- **学习目标**：最小化一个由偏微分方程残差、初始条件失配和边界条件失配组成的损失函数，该问题被构建为一个复值线性最小二乘问题。\n- **物理信息基函数选择**：隐藏参数被选为满足色散关系 $\\omega_{j} = \\frac{1}{2}k_{j}^{2}$ 的整数波数 $k_j$ 和频率 $\\omega_{j}$。\n- **测试用例**：$(k=0, T=1)$，$(k=1, T=3/4)$，$(k=2, T=1/2)$。\n- **基组构建**：包含 $M$ 个波数的集合 $\\{k_j\\}$ 被选为均匀覆盖 $[-K, K]$ 的整数，其中 $M=2K+1$。\n- **误差度量**：相对均方根误差 $\\varepsilon = \\left( \\frac{\\sum_{i=1}^{N_{x}}\\sum_{n=1}^{N_{t}} \\left|\\psi_{\\theta}(x_{i}, t_{n}) - e^{i(k x_{i} - \\frac{1}{2}k^{2} t_{n})}\\right|^{2}}{\\sum_{i=1}^{N_{x}}\\sum_{n=1}^{N_{t}} \\left|e^{i(k x_{i} - \\frac{1}{2}k^{2} t_{n})}\\right|^{2}} \\right)^{1/2}$。\n\n接下来，我将验证该问题。该问题在科学上**有坚实基础**，植根于量子力学（薛定谔方程）和计算数学（谱方法、最小二乘近似）的基本原理。该问题是**适定的**，提供了明确的目标、具体的模型以及足以确定模型参数的约束条件。该方法虽然以现代术语“物理信息神经网络”呈现，但它是一种与谱方法密切相关的成熟技术，其中基函数的选择是为了满足控制微分方程。该问题是**客观的**，所有术语都得到了精确定义。其设置并非不完整、矛盾或模棱两可。它提出了一个计算物理学中具体、可解的问题。因此，该问题被认为是**有效的**。\n\n现在我将提供解决方案。\n\n控制偏微分方程是一维空间中的自由粒子薛定谔方程：\n$$\ni \\frac{\\partial \\psi}{\\partial t} + \\frac{1}{2} \\frac{\\partial^{2} \\psi}{\\partial x^{2}} = 0\n$$\n所提出的波函数 $\\psi(x,t)$ 模型，或称拟设 (ansatz)，是复指数的线性组合：\n$$\n\\psi_{\\theta}(x,t) = \\sum_{j=1}^{M} \\alpha_{j} \\, e^{i(k_{j} x - \\omega_{j} t)}\n$$\n在此，待学习的参数是复系数 $\\theta = \\{\\alpha_j\\}_{j=1}^M$。波数集合 $\\{k_j\\}$ 和频率集合 $\\{\\omega_j\\}$ 是固定的，它们定义了模型的基函数。\n\n物理信息方法的核心是强制执行控制方程。我们通过将拟设 $\\psi_{\\theta}$ 代入薛定谔方程来构建偏微分方程残差 $\\mathcal{R}(x,t;\\theta)$。首先，我们计算所需的偏导数：\n$$\n\\frac{\\partial \\psi_{\\theta}}{\\partial t} = \\sum_{j=1}^{M} \\alpha_{j} (-i \\omega_{j}) e^{i(k_{j} x - \\omega_{j} t)}\n$$\n$$\n\\frac{\\partial \\psi_{\\theta}}{\\partial x} = \\sum_{j=1}^{M} \\alpha_{j} (i k_{j}) e^{i(k_{j} x - \\omega_{j} t)}\n$$\n$$\n\\frac{\\partial^{2} \\psi_{\\theta}}{\\partial x^{2}} = \\sum_{j=1}^{M} \\alpha_{j} (i k_{j})^{2} e^{i(k_{j} x - \\omega_{j} t)} = \\sum_{j=1}^{M} \\alpha_{j} (-k_{j}^{2}) e^{i(k_{j} x - \\omega_{j} t)}\n$$\n将这些代入残差定义中可得：\n$$\n\\mathcal{R}(x,t;\\theta) = i \\left( \\sum_{j=1}^{M} \\alpha_{j} (-i \\omega_{j}) e^{i(k_{j} x - \\omega_{j} t)} \\right) + \\frac{1}{2} \\left( \\sum_{j=1}^{M} \\alpha_{j} (-k_{j}^{2}) e^{i(k_{j} x - \\omega_{j} t)} \\right)\n$$\n根据算子的线性性质，我们可以将其写为和的形式：\n$$\n\\mathcal{R}(x,t;\\theta) = \\sum_{j=1}^{M} \\alpha_{j} \\left( \\omega_{j} - \\frac{1}{2} k_{j}^{2} \\right) e^{i(k_{j} x - \\omega_{j} t)}\n$$\n问题陈述要求对基函数进行特定的物理信息选择：每对 $(k_j, \\omega_j)$ 都必须满足自由粒子的色散关系，即 $\\omega_{j} = \\frac{1}{2}k_{j}^{2}$。通过这种选择，括号中的项变为 $\\left( \\frac{1}{2}k_{j}^{2} - \\frac{1}{2}k_{j}^{2} \\right) = 0$。因此，对于任何系数 $\\alpha_j$ 的选择，偏微分方程残差都恒为零：\n$$\n\\mathcal{R}(x,t;\\theta) = 0\n$$\n这是一个关键的简化。模型的构造方式使得其基函数的任何线性组合都是薛定谔方程的精确解。因此，学习问题从一个受偏微分方程约束的优化问题简化为一个只需满足初始条件和边界条件的问题。\n\n接下来，我们分析周期性边界条件。定义域为 $x \\in [0, 2\\pi]$。第一个条件是 $\\psi_{\\theta}(0, t) = \\psi_{\\theta}(2\\pi, t)$。在边界处评估模型：\n$$\n\\psi_{\\theta}(0, t) = \\sum_{j=1}^{M} \\alpha_{j} e^{-i \\omega_{j} t}\n$$\n$$\n\\psi_{\\theta}(2\\pi, t) = \\sum_{j=1}^{M} \\alpha_{j} e^{i(k_{j} 2\\pi - \\omega_{j} t)} = \\sum_{j=1}^{M} \\alpha_{j} e^{i k_{j} 2\\pi} e^{-i \\omega_{j} t}\n$$\n问题指定波数 $\\{k_j\\}$ 为整数。对于任何整数 $k_j$，根据欧拉公式有 $e^{i k_{j} 2\\pi} = \\cos(2\\pi k_j) + i \\sin(2\\pi k_j) = 1$。因此，对于任何 $\\alpha_j$ 的选择，$\\psi_{\\theta}(0, t) = \\psi_{\\theta}(2\\pi, t)$ 都自动满足。\n\n第二个边界条件是关于空间导数的，即 $\\frac{\\partial \\psi_{\\theta}}{\\partial x}(0, t) = \\frac{\\partial \\psi_{\\theta}}{\\partial x}(2\\pi, t)$。计算该导数：\n$$\n\\frac{\\partial \\psi_{\\theta}}{\\partial x}(0, t) = \\sum_{j=1}^{M} \\alpha_{j} (i k_{j}) e^{-i \\omega_{j} t}\n$$\n$$\n\\frac{\\partial \\psi_{\\theta}}{\\partial x}(2\\pi, t) = \\sum_{j=1}^{M} \\alpha_{j} (i k_{j}) e^{i(k_{j} 2\\pi - \\omega_{j} t)} = \\left(\\sum_{j=1}^{M} \\alpha_{j} (i k_{j}) e^{-i \\omega_{j} t}\\right) e^{i k_j 2\\pi}\n$$\n同样，由于 $k_j$ 是整数，$e^{i k_j 2\\pi} = 1$，因此该边界条件也自动满足。\n\n现在问题已简化为仅需在最小二乘意义上满足初始条件 $\\psi(x, 0) = e^{i k x}$。在时间 $t=0$ 时，我们的模型变为：\n$$\n\\psi_{\\theta}(x, 0) = \\sum_{j=1}^{M} \\alpha_{j} e^{i k_{j} x}\n$$\n我们必须找到复系数 $\\{\\alpha_j\\}$，以最小化模型预测与真实初始状态在定义域 $[0, 2\\pi]$ 中的一组 $N_{IC}$ 个配置点 $\\{x_p\\}_{p=1}^{N_{IC}}$ 上的平方误差。目标是最小化：\n$$\nL(\\theta) = \\sum_{p=1}^{N_{IC}} \\left| \\psi_{\\theta}(x_p, 0) - e^{i k x_p} \\right|^2 = \\sum_{p=1}^{N_{IC}} \\left| \\sum_{j=1}^{M} \\alpha_{j} e^{i k_{j} x_p} - e^{i k x_p} \\right|^2\n$$\n这是一个标准的复值线性最小二乘问题，形式为 $\\| A\\mathbf{a} - \\mathbf{b} \\|_2^2$，其中：\n- $\\mathbf{a}$ 是未知系数的列向量 $[\\alpha_1, \\alpha_2, \\ldots, \\alpha_M]^T$。\n- $A$ 是一个 $N_{IC} \\times M$ 的矩阵，称为设计矩阵，其元素为 $A_{pj} = e^{i k_j x_p}$。\n- $\\mathbf{b}$ 是一个长度为 $N_{IC}$ 的列向量，其元素为 $b_p = e^{i k x_p}$。\n\n为解决此问题，我们选择一组基波数 $k_j \\in \\{-K, -K+1, \\ldots, K\\}$，总共 $M=2K+1$ 个函数。目标初始条件的波数为 $k$。只要 $k$ 包含在我们的集合 $\\{k_j\\}$ 中，该基底就能完美地表示初始条件。最小二乘解应该对 $k_j=k$ 的基函数得出 $\\alpha_j = 1$，对所有其他 $j$ 得出 $\\alpha_j=0$。通过 `numpy.linalg.lstsq` 获得的数值解将高精度地逼近这个理想结果。\n\n实现将按以下步骤进行：\n1. 对于每个测试用例 $(k, T)$，定义基组 $\\{k_j\\}$ 和相应的 $\\{\\omega_j\\}$。\n2. 为初始条件生成 $N_{IC}$ 个配置点 $\\{x_p\\}$。\n3. 按照上述定义构建矩阵 $A$ 和向量 $\\mathbf{b}$。\n4. 求解系统 $A\\mathbf{a} \\approx \\mathbf{b}$ 以得到系数向量 $\\mathbf{a}$。\n5. 使用计算出的系数，在精细的时空网格上构建学习到的解 $\\psi_{\\theta}$。\n6. 在同一网格上构建精确解 $\\psi_{exact}$。\n7. 计算相对均方根误差 $\\varepsilon$ 并报告它。\n此程序构成了对所提出问题的完整且正确的解决方案。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs and solves a physics-informed model for the time-dependent\n    Schrödinger equation using a complex-valued linear least-squares approach.\n    \"\"\"\n    \n    # Test cases: (k, T) where k is the initial wavenumber and T is the final time.\n    test_cases = [\n        (0, 1.0),\n        (1, 0.75),\n        (2, 0.5)\n    ]\n\n    results = []\n\n    # --- Model and Solver Configuration ---\n    # K defines the range of integer wavenumbers [-K, K] for the basis.\n    # M = 2*K + 1 is the total number of basis functions.\n    K = 10\n    M = 2 * K + 1\n\n    # Number of collocation points for the initial condition.\n    # Should be >= M for a well-posed least-squares problem.\n    N_IC = 101\n\n    # Number of points for the final error evaluation grid.\n    N_x_eval = 256\n    N_t_eval = 101\n    \n    # Define the basis wavenumbers {k_j}\n    k_j = np.arange(-K, K + 1)\n    \n    # Define the initial condition collocation points {x_p} on [0, 2*pi]\n    x_p = np.linspace(0, 2 * np.pi, N_IC, endpoint=False)\n\n    # Construct the design matrix A for the least-squares problem.\n    # The matrix A is independent of the test cases.\n    # A_pj = exp(i * k_j * x_p)\n    A = np.exp(1j * np.outer(x_p, k_j))\n\n    for k_target, T in test_cases:\n        # --- Step 1: Formulate and Solve the Least-Squares Problem ---\n\n        # The problem reduces to fitting the initial condition psi(x, 0) = exp(i*k*x).\n        # We solve A * alpha = b for the complex coefficients alpha.\n        \n        # Construct the target vector b for the initial condition.\n        # b_p = exp(i * k_target * x_p)\n        b = np.exp(1j * k_target * x_p)\n\n        # Solve the complex-valued linear least-squares system.\n        # This finds the coefficients 'alpha' that minimize ||A*alpha - b||^2.\n        alpha, _, _, _ = np.linalg.lstsq(A, b, rcond=None)\n\n        # --- Step 2: Evaluate the Learned Solution and Compute Error ---\n\n        # Create the space-time grid for evaluation.\n        x_eval = np.linspace(0, 2 * np.pi, N_x_eval)\n        t_eval = np.linspace(0, T, N_t_eval)\n        x_grid, t_grid = np.meshgrid(x_eval, t_eval)\n\n        # Calculate the frequencies omega_j from the dispersion relation.\n        omega_j = 0.5 * k_j**2\n\n        # Construct the learned solution psi_theta(x, t) on the evaluation grid.\n        # psi_theta is a linear combination of basis functions with the learned coefficients.\n        psi_theta = np.zeros_like(x_grid, dtype=np.complex128)\n        for j in range(M):\n            # Basis function: exp(i * (k_j * x - omega_j * t))\n            basis_func = np.exp(1j * (k_j[j] * x_grid - omega_j[j] * t_grid))\n            psi_theta += alpha[j] * basis_func\n\n        # Construct the exact solution on the evaluation grid.\n        omega_target = 0.5 * k_target**2\n        psi_exact = np.exp(1j * (k_target * x_grid - omega_target * t_grid))\n\n        # --- Step 3: Calculate the Relative Root-Mean-Square Error (RRMSE) ---\n        \n        # Numerator of the error formula\n        error_numerator = np.sum(np.abs(psi_theta - psi_exact)**2)\n\n        # Denominator of the error formula\n        error_denominator = np.sum(np.abs(psi_exact)**2)\n\n        # Calculate the relative RMSE.\n        # The denominator simplifies to N_x_eval * N_t_eval since |psi_exact| = 1,\n        # but we compute it as per the formula for correctness.\n        if error_denominator == 0:\n            # Handle the case of a zero denominator, though unlikely here.\n            relative_error = np.sqrt(error_numerator)\n        else:\n            relative_error = np.sqrt(error_numerator / error_denominator)\n        \n        results.append(relative_error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2427209"}]}