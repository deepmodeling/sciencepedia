## 引言
在当今数据驱动的世界里，我们面临的数据日益复杂，早已超越了传统表格所能描述的范畴。从彩色视频（高度×宽度×颜色×时间）到[推荐系统](@entry_id:172804)中的用户-商品-情境交互，[多维数据](@entry_id:189051)无处不在。虽然矩阵在二维数据分析中取得了巨大成功，但当数据维度超过两个时，强行将其“压平”为矩阵会导致关键的结构信息丢失。这就引出了一个核心问题：我们如何才能有效地表示和分析这些[高维数据](@entry_id:138874)，并从中提取有价值的见解？

数值张量方法正是应对这一挑战的强大数学框架。通过将张量视为矩阵向多维的自然推广，这些方法提供了一套用于处理多维数组的系统性工具。本文旨在为您揭开数值张量方法的面纱，带领您从基本原理走向前沿应用。在接下来的内容中，您将学习到：

在“**原理与机制**”一章中，我们将建立坚实的理论基础，探讨张量如何作为多维数组来表示数据，学习其基本运算（如缩并和模-n积），并深入研究两种核心的分解技术——[Tucker分解](@entry_id:182831)和[CP分解](@entry_id:203488)，它们是理解[多维数据](@entry_id:189051)结构的关键。

接着，在“**应用与[交叉](@entry_id:147634)学科联系**”一章中，我们将把理论付诸实践，探索张量方法如何在物理学、工程学、数据科学和机器学习等不同领域大放异彩，从模拟物理场到构建先进的推荐系统。

最后，通过“**动手实践**”部分提供的练习，您将有机会亲手应用所学知识，巩固对张量运算和分解的理解，为解决真实世界中的[多维数据](@entry_id:189051)问题做好准备。

## 原理与机制

在上一章介绍张量的基本概念之后，本章将深入探讨数值张量方法的核心原理与机制。我们将张量视为多维数组，并系统地学习其基本运算、关键变换以及强大的分解技术。这些方法构成了现代数据科学、机器学习和[科学计算](@entry_id:143987)中许多先进应用的基础。我们将从基本的[数据表示](@entry_id:636977)和操作开始，逐步过渡到能够揭示[多维数据](@entry_id:189051)中隐藏结构的复杂算法。

### 作为多维数组的张量

在数值计算中，最直接理解张量的方式是将其视为标量、向量和矩阵的自然推广。一个标量可以被看作是零阶张量 (order-0 tensor)，一个向量是一阶张量 (order-1 tensor)，而一个矩阵是[二阶张量](@entry_id:199780) (order-2 tensor)。遵循这个逻辑，一个**三阶张量 (order-3 tensor)** 就是一个三维的数字数组，其元素由三个索引来定位。通常，张量的**阶 (order)** 或**模 (mode)** 的数量就是确定其任意元素所需的索引数量。

为了具象化这个概念，让我们思考两个实际场景。

首先，想象一下大学需要存储学生在不同学年、不同课程中的成绩。一个三阶张量 $G_{ijk}$ 可以非常自然地组织这些数据 ([@problem_id:1527717])。我们可以设定：
- 第一个索引 $i$ 代表学生ID。
- 第二个索引 $j$ 代表课程ID。
- 第三个索引 $k$ 代表学年。
这样，张量中的每一个元素 $G_{ijk}$ 就精确地对应着学生 $i$ 在学年 $k$ 修读课程 $j$ 所获得的绩点。例如，$G_{2,3,1}$ 就代表了ID为2的学生在第一学年修读ID为3的课程所获得的绩点。这种结构将原本可能分散在多个表格中的数据整合进一个统一的数学对象中，便于进行系统性分析。

其次，考虑一个数字彩色图像的表示。一张彩色图像由像素网格组成，每个像素本身又包含红(R)、绿(G)、蓝(B)三个颜色通道的强度值。这同样可以用一个三阶张量 $T_{ijk}$ 来完美描述 ([@problem_id:1527687])。
- 第一个索引 $i$ 代表像素的行位置。
- 第二个索引 $j$ 代表像素的列位置。
- 第三个索引 $k$ 代表颜色通道（例如，$k=1$ 为红色，$k=2$ 为绿色，$k=3$ 为蓝色）。
因此，$T_{ij1}$ 就表示位于 $(i, j)$ 位置像素的红色分量强度。整个张量 $T$ 就构成了一个数据“立方体”，其尺寸为 (图像高度 $\times$ 图像宽度 $\times$ 3)。

### 基本张量运算与子结构

理解了张量作为数据容器的角色后，我们接下来探讨如何从中提取信息并进行操作。

#### 张量纤维与切片

张量的子结构对于数据分析至关重要。通过固定部分索引，我们可以从张量中提取出向量或矩阵。

- **纤维 (Fiber)**：通过固定除一个索引外的所有索引，我们得到一个向量，称为纤维。纤维是张量最基本的定向元素。例如，对于一个三阶张量 $T_{ijk}$，如果我们固定 $i=i_0$ 和 $j=j_0$，让 $k$ 遍历其所有可能的值，就得到一个沿第三个模变化的向量。这被称为一个**模-3纤维 (mode-3 fiber)**。

  举一个具体的例子，假设一个三阶张量由解析表达式 $T_{ijk} = 2i^2 - 4j + k$ 定义，其中 $i \in \{1, 2, 3\}$, $j \in \{1, 2\}$, $k \in \{1, 2, 3, 4\}$。要确定当 $i=3, j=1$ 时的模-3纤维 ([@problem_id:1527701])，我们只需将这些固定值代入表达式，并让 $k$ 变化：
  $T_{3,1,k} = 2(3)^2 - 4(1) + k = 14 + k$
  当 $k$ 取遍 $\{1, 2, 3, 4\}$ 时，我们得到的纤维是一个行向量 $\begin{pmatrix} 15  16  17  18 \end{pmatrix}$。

- **切片 (Slice)**：通过固定除两个索引外的所有索引，我们得到一个矩阵，称为切片。在前面的例子中，学生成绩张量 $G_{ijk}$ 在固定学年 $k=1$ 时的矩阵 $G_{ij1}$，以及彩色图像张量 $T_{ijk}$ 在固定颜色通道 $k=1$ 时的红色通道矩阵 $T_{ij1}$，都是张量切片的实例。

#### [张量缩并](@entry_id:193373)

**缩并 (Contraction)** 是一种基本运算，它通过对一个或多个共享的索引进行求和来降低张量的阶。

一个直观的例子是将彩色图像转换为灰度图像 ([@problem_id:1527687])。灰度值通常是红、绿、蓝三个通道强度的加权平均。如果我们有一个权重向量 $w = (w_1, w_2, w_3)$，其中 $w_1=0.3, w_2=0.6, w_3=0.1$，则灰度图像矩阵 $G_{ij}$ 的每个像素值可以通过以下方式计算：
$$
G_{ij} = \sum_{k=1}^{3} w_k T_{ijk}
$$
这个运算将三阶的颜色张量 $T_{ijk}$ 和一阶的权重张量（向量）$w_k$ 缩并为一个二阶的灰度张量（矩阵）$G_{ij}$。求和是在共享的索引 $k$ 上进行的。

更一般地，张量可以作为多[线性映射](@entry_id:185132)。例如，在物理学中，材料的响应可能由一个张量和多个输入向量决定。考虑一个输出向量 $y_i$ 由一个三阶张量 $T_{ijk}$ 和两个输入向量 $x_j, z_k$ 生成的关系 ([@problem_id:1527702])：
$$
y_i = \sum_{j=1}^{J} \sum_{k=1}^{K} T_{ijk} x_j z_k
$$
在这里，我们对两个索引 $j$ 和 $k$ 进行了求和。这种表达式在处理多重交互时非常常见。按照**爱因斯坦求和约定 (Einstein summation convention)**，当一个索引在表达式的单项中重复出现时（通常一次为上标，一次为下标，但在数值环境中常简化为仅看索引字母），就意味着对该索引的所有可[能值](@entry_id:187992)进行求和。因此，上述表达式可以简写为 $y_i = T_{ijk} x_j z_k$。这种缩并操作将一个三阶张量和两个一阶张量映射为一个一阶张量。

另一个缩并的应用是计算加权平均值，例如计算学生的累积GPA ([@problem_id:1527717])。学生的总品质点 (Quality Points) 是通过将其成绩张量 $G_{ijk}$ 与课程学分向量 $C_j$ 相乘并对课程 $j$ 和学年 $k$ 求和得到的。对于学生 $i$，总品[质点](@entry_id:186768) $Q_i = \sum_k \sum_j G_{ijk} C_j$。这本质上也是一个缩并运算。

### [矩阵化](@entry_id:751739)：通往线性代数的桥梁

尽管张量是强大的理论工具，但我们的大部分数值算法工具箱（如[奇异值分解](@entry_id:138057)SVD、[特征值分解](@entry_id:272091)等）都是为矩阵设计的。因此，一个至关重要的操作是**[矩阵化](@entry_id:751739) (matricization)**，也称为**展开 (unfolding)**。这个过程将一个[高阶张量](@entry_id:200122)重新[排列](@entry_id:136432)成一个矩阵。

对于一个 $N$ 阶张量，有 $N$ 种不同的[矩阵化](@entry_id:751739)方式，分别称为模-1展开、模-2展开，以此类推。一个**模-n展开 (mode-n unfolding)**，记作 $\mathcal{T}_{(n)}$，是这样一个矩阵：它的列由张量的所有模-n纤维组成。

让我们以一个三阶张量 $\mathcal{A} \in \mathbb{R}^{I \times J \times K}$ 的模-1展开为例进行详细说明。模-1展开将张量 $\mathcal{A}$ 变成一个 $I \times (JK)$ 的矩阵 $\mathbf{A}_{(1)}$。张量中的元素 $\mathcal{A}_{ijk}$ 映射到矩阵 $\mathbf{A}_{(1)}$ 中的位置 $(r, c)$，其映射规则如下：
1.  矩阵的行索引 $r$ 等于张量的第一个索引 $i$。
2.  矩阵的列索引 $c$ 由另外两个索引 $(j, k)$ 共同决定。一个标准的[列主序](@entry_id:637645)（column-major）映射方式是：$c = j + (k-1)J$。

这种映射将张量的所有模-1纤维（即固定 $j$ 和 $k$ 时，沿 $i$ 变化的列向量）依次[排列](@entry_id:136432)为新矩阵的列。

例如，考虑一个 $3 \times 4 \times 5$ 的张量 $\mathcal{A}$，其元素为 $\mathcal{A}_{ijk} = i + 10j + 100k$ ([@problem_id:1527714])。我们要找到其模-1展开矩阵 $\mathbf{A}_{(1)}$ 的第2行第13列的元素值。
- 首先，行索引 $r=2$ 告诉我们 $i=2$。
- 其次，列索引 $c=13$ 和维度 $J=4$ 帮助我们反解出 $j$ 和 $k$。根据公式 $c = j + (k-1)J$，我们有 $13 = j + (k-1)4$。由于 $j$ 的范围是 $1 \le j \le 4$，我们可以测试 $k$ 的可[能值](@entry_id:187992)。当 $k=4$ 时，$13 = j + (4-1)4 = j + 12$，解得 $j=1$。这对 $(j,k)=(1,4)$ 是满足索引范围的唯一解。
- 因此，[矩阵元](@entry_id:186505)素 $(\mathbf{A}_{(1)})_{2,13}$ 对应于张量元素 $\mathcal{A}_{2,1,4}$。
- 计算其值为：$\mathcal{A}_{2,1,4} = 2 + 10(1) + 100(4) = 412$。

[矩阵化](@entry_id:751739)是后续高级算法（如[张量分解](@entry_id:173366)）的基石，因为它允许我们将强大的[矩阵分析](@entry_id:204325)工具应用于[高维数据](@entry_id:138874)。

### 模-n积：变换张量

有了[矩阵化](@entry_id:751739)作为桥梁，我们可以定义张量与矩阵之间的一种重要乘法——**模-n积 (mode-n product)**。一个张量 $\mathcal{T}$ 与一个矩阵 $M$ 的模-n积，记作 $\mathcal{S} = \mathcal{T} \times_n M$，其结果是一个新的张量。这个运算的本质是沿着张量的第 $n$ 个模对每个模-n纤维应用矩阵 $M$ 所代表的[线性变换](@entry_id:149133)。

以模-1积为例，假设我们有一个三阶张量 $\mathcal{T} \in \mathbb{R}^{I \times J \times K}$ 和一个矩阵 $M \in \mathbb{R}^{I' \times I}$。它们的模-1积是一个新的张量 $\mathcal{S} \in \mathbb{R}^{I' \times J \times K}$，其元素定义为：
$$
S_{i'jk} = \sum_{i=1}^{I} M_{i'i} T_{ijk}
$$
这个公式表明，要计算新张量 $\mathcal{S}$ 的一个元素，我们固定索引 $j$ 和 $k$，这相当于选择了一个 $\mathcal{T}$ 中的模-1纤维 $T_{:jk}$（一个列向量）。然后，我们将这个纤维左乘矩阵 $M$，得到的结果就是新张量 $\mathcal{S}$ 中对应的模-1纤维 $S_{:jk}$。

让我们通过一个数值例子来理解这个过程 ([@problem_id:1527719])。给定一个 $2 \times 3 \times 2$ 的张量 $\mathcal{T}$ 和一个 $2 \times 2$ 的矩阵 $M$：
$$
\mathcal{T}_{:,:,2} = \begin{pmatrix} 7  8  9 \\ 10  11  12 \end{pmatrix}, \quad M = \begin{pmatrix} 2  -1 \\ 0  3 \end{pmatrix}
$$
我们要计算结果张量 $\mathcal{S}$ 的元素 $S_{212}$。根据定义：
$$
S_{212} = \sum_{i=1}^{2} M_{2i} T_{i12} = M_{21} T_{112} + M_{22} T_{212}
$$
从给定的数据中，我们找到 $M_{21}=0$, $M_{22}=3$, $T_{112}=7$, $T_{212}=10$。代入计算：
$$
S_{212} = (0)(7) + (3)(10) = 30
$$
模-n积是[张量分解](@entry_id:173366)（如[Tucker分解](@entry_id:182831)）的核心构建块，因为它提供了一种用更紧凑的基来表示张量各个模式的方式。

### 用于数据分析的[张量分解](@entry_id:173366)

[矩阵分解](@entry_id:139760)（如SVD）通过将[矩阵分解](@entry_id:139760)为多个结构更简单的矩阵的乘积，在数据降维和[特征提取](@entry_id:164394)方面取得了巨大成功。同样，**[张量分解](@entry_id:173366) (tensor decomposition)** 将一个[高阶张量](@entry_id:200122)表示为多个低阶张量（通常是向量和/或一个较小的[核心张量](@entry_id:747891)）的组合。这使得我们能够压缩数据、发现潜在的因子以及解释[多维数据](@entry_id:189051)中的复杂[交互作用](@entry_id:176776)。

#### [Tucker分解](@entry_id:182831)

**[Tucker分解](@entry_id:182831) (Tucker decomposition)** 将一个张量 $\mathcal{T}$ 近似表示为一个**[核心张量](@entry_id:747891) (core tensor)** $\mathcal{G}$ 与一系列**因子矩阵 (factor matrices)** $\{U^{(n)}\}$ 的模-n积。对于一个三阶张量，分解形式为：
$$
\mathcal{T} \approx \mathcal{G} \times_1 U^{(1)} \times_2 U^{(2)} \times_3 U^{(3)}
$$
这里，因子矩阵 $U^{(n)}$ 通常是正交矩阵，其列构成了第 $n$ 个模的“主成分”或[基向量](@entry_id:199546)。[核心张量](@entry_id:747891) $\mathcal{G}$ 的尺寸通常远小于原始张量 $\mathcal{T}$，它描述了不同模式下这些主成分之间的交互强度。如果[核心张量](@entry_id:747891)是对角的，[Tucker分解](@entry_id:182831)就退化为下面将要介绍的[CP分解](@entry_id:203488)。

计算[Tucker分解](@entry_id:182831)的标准算法是**[高阶奇异值分解](@entry_id:197696) (Higher-Order Singular Value Decomposition, [HOSVD](@entry_id:197696))**。该算法非常直观：第 $n$ 个因子矩阵 $U^{(n)}$ 的列由张量 $\mathcal{T}$ 的模-n展开矩阵 $\mathcal{T}_{(n)}$ 的前 $r_n$ 个[左奇异向量](@entry_id:751233)构成，其中 $r_n$ 是分解在第 $n$ 个模上的目标秩。

让我们看看如何计算因子矩阵 $U^{(1)}$ ([@problem_id:1527716])。根据[HOSVD](@entry_id:197696)，我们需要找到 $\mathcal{T}_{(1)}$ 的[左奇异向量](@entry_id:751233)。这些向量等价于矩阵 $\mathcal{T}_{(1)}\mathcal{T}_{(1)}^T$ 的[特征向量](@entry_id:151813)。

假设我们有一个 $2 \times 2 \times 2$ 的张量 $\mathcal{T}$，其模-1展开为：
$$
\mathcal{T}_{(1)} = \begin{pmatrix} 1  2  3  4 \\ 4  3  2  1 \end{pmatrix}
$$
首先计算 $\mathcal{T}_{(1)}\mathcal{T}_{(1)}^T$：
$$
\mathcal{T}_{(1)}\mathcal{T}_{(1)}^T = \begin{pmatrix} 1  2  3  4 \\ 4  3  2  1 \end{pmatrix} \begin{pmatrix} 1  4 \\ 2  3 \\ 3  2 \\ 4  1 \end{pmatrix} = \begin{pmatrix} 30  20 \\ 20  30 \end{pmatrix}
$$
接下来，我们求解这个矩阵的[特征值](@entry_id:154894)和[特征向量](@entry_id:151813)。其[特征值](@entry_id:154894)为 $\lambda_1 = 50, \lambda_2 = 10$ ([@problem_id:1527690])。

如果我们想要一个秩为 $(1, 2, 2)$ 的[Tucker分解](@entry_id:182831)，那么 $r_1=1$。这意味着 $U^{(1)}$ 将由对应于最大[特征值](@entry_id:154894) $\lambda_1=50$ 的[特征向量](@entry_id:151813)构成。该[特征向量](@entry_id:151813)为 $\begin{pmatrix} 1 \\ 1 \end{pmatrix}$。归一化后，我们得到 $U^{(1)}$ 的唯一一列为 $\frac{1}{\sqrt{2}}\begin{pmatrix} 1 \\ 1 \end{pmatrix}$ ([@problem_id:1527716])。对其他模（模-2和模-3）重复此过程，即可得到所有因子矩阵。

#### CANDECOMP/[PARAFAC](@entry_id:753095) (CP) 分解

**CANDECOMP/[PARAFAC](@entry_id:753095) (CP) 分解**，也称为**[典范分解](@entry_id:634116) (Canonical Decomposition)** 或**[平行因子分析](@entry_id:753095) (Parallel Factor Analysis)**，将一个[张量表示](@entry_id:180492)为一系列**秩-1张量 (rank-1 tensors)** 的和。一个三阶秩-1张量是三个向量 $\mathbf{a}, \mathbf{b}, \mathbf{c}$ 的**外积 (outer product)**，记作 $\mathbf{a} \circ \mathbf{b} \circ \mathbf{c}$，其元素为 $(a_i b_j c_k)$。[CP分解](@entry_id:203488)的形式为：
$$
\mathcal{T} \approx \sum_{r=1}^{R} \mathbf{a}_r \circ \mathbf{b}_r \circ \mathbf{c}_r
$$
其中 $R$ 是分解的秩。与[Tucker分解](@entry_id:182831)不同，[CP分解](@entry_id:203488)在结构上更受限，但其因子通常具有更直观的物理解释。

计算[CP分解](@entry_id:203488)通常采用**[交替最小二乘法](@entry_id:746387) (Alternating Least Squares, ALS)**。ALS是一个迭代算法，其核心思想是：当我们在优化一个因子矩阵（如 $A$）时，我们暂时固定所有其他因子矩阵（如 $B$ 和 $C$）。此时，最小化近似误差的[优化问题](@entry_id:266749)就变成了一个标准的线性最小二乘问题，可以高效求解。然后，我们交替地对每个因子矩阵重复这个过程，直到算法收敛。

让我们探究一下ALS的更新步骤 ([@problem_id:1527685])。假设我们固定了因子矩阵 $B$ 和 $C$，并希望求解 $A$。我们可以利用[矩阵化](@entry_id:751739)来简化问题。CP模型的模-1展开可以写成：
$$
\mathbf{T}_{(1)} \approx A (C \odot B)^T
$$
这里 $\odot$ 表示[Khatri-Rao积](@entry_id:751014)（矩阵的按列Kronecker积）。这个公式揭示了ALS更新步骤的关键：当因子矩阵 $B$ 和 $C$ 固定时，原始的[非线性优化](@entry_id:143978)问题就转化为了一个关于矩阵 $A$ 的标[准线性](@entry_id:637689)最小二乘问题。其标准解形式为：
$$
A = \mathbf{T}_{(1)} [ ( C \odot B )^T ]^\dagger
$$
其中 $\dagger$ 表示[伪逆](@entry_id:140762)。在实践中，这个最小二乘问题通常通过更高效的数值方法求解。通过交替地对每个因子矩阵重复这个过程，ALS算法逐步逼近最佳的因子矩阵。

### 高级应用：张量补全

在现实世界的数据中，由于测量失败、记录错误或其他原因，我们经常会遇到数据缺失的问题。**张量补全 (tensor completion)** 的目标就是根据已观测到的数据来估计和填补这些缺失的条目。

其核心思想是假设完整的潜在张量是**低秩 (low-rank)** 的。也就是说，尽管数据维度很高，但其内在结构可以用较少的因子来表示。因此，我们的任务是找到一个与观测数据最匹配的低秩张量，然后用这个低秩张量的对应元素来填充缺失值。

ALS算法可以很自然地被调整来解决这个问题。在为某个因子矩阵（例如 $A$）求解最小二乘问题时，我们不再是最小化模型与整个张量 $\mathcal{T}$ 之间的误差，而是只最小化与**已观测到**的条目之间的误差。

考虑一个 $2 \times 2 \times 2$ 的张量 $\mathcal{T}$，其中一个元素 $\mathcal{T}_{222}$ 未知 ([@problem_id:1527724])。我们希望用一个秩为2的CP模型来估计它。在ALS的更新步骤中，例如，当我们为矩阵 $A$ 的第二行 $\mathbf{a}_2^T = (A_{21}, A_{22})$ 求解时，我们只使用已知的 $\mathcal{T}_{2jk}$ 值，即 $\mathcal{T}_{211}, \mathcal{T}_{212}, \mathcal{T}_{221}$。对于这三个已观测点，我们建立一个线性最小二乘问题：
$$
\min_{A_{21}, A_{22}} \sum_{(j,k) \in \{(1,1), (1,2), (2,1)\}} \left(\mathcal{T}_{2jk} - (A_{21} B_{j1} C_{k1} + A_{22} B_{j2} C_{k2})\right)^2
$$
通过求解这个（以及为 $A$ 的第一行建立的类似）最小二乘问题，我们可以获得更新后的因子矩阵 $A^{(1)}$。一旦我们得到了更新后的因子（即使只是经过一步迭代），我们就可以用它们来预测缺失值：
$$
\widehat{\mathcal{T}}_{222} = \sum_{r=1}^{2} A_{2r}^{(1)} B_{2r}^{(0)} C_{2r}^{(0)}
$$
这个过程展示了[张量分解](@entry_id:173366)如何将[代数结构](@entry_id:137052)与[优化方法](@entry_id:164468)相结合，从而解决像数据补全这样具有挑战性的实际问题。例如，在[推荐系统](@entry_id:172804)中，一个用户-商品-时间的张量可能包含大量未观测的条目（用户未对商品在特定时间评分），张量补全技术可以用来预测用户的潜在偏好。

通过本章的学习，我们已经从张量的[基本表示](@entry_id:157678)，经过核心的运算与变换，最终到达了如Tucker和[CP分解](@entry_id:203488)这样的高级数据分析工具。这些原理和机制为利用张量方法解决[多维数据](@entry_id:189051)科学中的复杂问题奠定了坚实的基础。