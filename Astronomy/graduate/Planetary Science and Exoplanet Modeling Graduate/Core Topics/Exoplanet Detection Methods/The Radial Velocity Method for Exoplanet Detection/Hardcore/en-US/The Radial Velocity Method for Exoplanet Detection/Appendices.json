{
    "hands_on_practices": [
        {
            "introduction": "Understanding the radial velocity method begins with its physical foundations. This first practice challenges you to derive the expected stellar velocity variation from first principles, starting with Newton's laws and the two-body problem . By working through this derivation, you will build a solid intuition for how a planet's mass and orbit translate into a detectable spectroscopic signal, quantified by the semi-amplitude $K$.",
            "id": "4184022",
            "problem": "A Sun-like host star of mass $M_{\\star} = 1\\,M_{\\odot}$ is orbited by a planet of mass $m_{p} = 1\\,M_{\\mathrm{J}}$ on a circular orbit with semi-major axis $a = 1\\,\\mathrm{AU}$ and orbital inclination $i = \\pi/2$ (edge-on). Using only Newton’s law of universal gravitation and the two-body dynamics for circular orbits, derive from first principles an expression for the star’s line-of-sight radial velocity as a function of time and identify the radial velocity semi-amplitude. Then evaluate the semi-amplitude numerically for the given system. Do not use any pre-derived exoplanet-specific formula; start from fundamental mechanics and the center-of-mass description of the two-body problem. Finally, briefly comment on whether such a signal is detectable with present-day high-precision spectrographs.\n\nTake the following constants:\n- Gravitational constant $G = 6.67430\\times 10^{-11}\\,\\mathrm{m^{3}\\,kg^{-1}\\,s^{-2}}$.\n- Solar mass $M_{\\odot} = 1.98847\\times 10^{30}\\,\\mathrm{kg}$.\n- Jupiter mass $M_{\\mathrm{J}} = 1.89813\\times 10^{27}\\,\\mathrm{kg}$.\n- Astronomical Unit $\\mathrm{AU} = 1.495978707\\times 10^{11}\\,\\mathrm{m}$.\n\nRound your final numerical answer for the semi-amplitude to three significant figures. Express the final semi-amplitude in meters per second (m/s).",
            "solution": "The problem as stated is scientifically grounded, self-contained, and well-posed. All data and constants necessary for a solution are provided, and there are no internal contradictions or violations of fundamental physical principles. The problem is therefore deemed valid and a solution will be derived.\n\nThe problem asks for the derivation of the star's line-of-sight radial velocity from first principles for a two-body system consisting of a star of mass $M_{\\star}$ and a planet of mass $m_p$. The two bodies orbit their common center of mass (CM) due to their mutual gravitational attraction.\n\nIn the center of mass frame, the position vectors of the star ($\\vec{r}_{\\star}$) and the planet ($\\vec{r}_p$) relative to the CM are related by the definition of the center of mass:\n$$M_{\\star}\\vec{r}_{\\star} + m_p\\vec{r}_p = \\vec{0}$$\nThe separation vector between the star and the planet is defined as $\\vec{a} = \\vec{r}_p - \\vec{r}_{\\star}$. From the CM equation, we have $\\vec{r}_p = -\\frac{M_{\\star}}{m_p}\\vec{r}_{\\star}$. Substituting this into the definition of $\\vec{a}$:\n$$\\vec{a} = -\\frac{M_{\\star}}{m_p}\\vec{r}_{\\star} - \\vec{r}_{\\star} = -\\left(\\frac{M_{\\star} + m_p}{m_p}\\right)\\vec{r}_{\\star}$$\nThis allows us to express the star's position vector relative to the CM in terms of the separation vector $\\vec{a}$:\n$$\\vec{r}_{\\star} = -\\frac{m_p}{M_{\\star} + m_p}\\vec{a}$$\nFor a circular orbit, the magnitude of the separation vector is constant, $|\\vec{a}| = a$, which is the semi-major axis. The star thus moves in a circular orbit around the CM with a radius $a_{\\star}$ given by:\n$$a_{\\star} = |\\vec{r}_{\\star}| = \\frac{m_p}{M_{\\star} + m_p}a$$\nThe gravitational force exerted by the planet on the star provides the necessary centripetal force for the star's orbit. However, it is more direct to analyze the dynamics of the equivalent one-body problem, where a reduced mass $\\mu = \\frac{M_{\\star}m_p}{M_{\\star}+m_p}$ orbits the total mass $M_{\\mathrm{tot}} = M_{\\star} + m_p$ at a distance $a$. The gravitational force is given by Newton's law:\n$$F_g = \\frac{G M_{\\star} m_p}{a^2}$$\nThis force provides the centripetal acceleration for the system. Using the relative coordinate framework, the equation of motion is $F_g = \\mu a_{\\mathrm{rel}}$, where $a_{\\mathrm{rel}}$ is the relative acceleration. For a circular orbit with angular velocity $\\omega$, the relative acceleration magnitude is $\\omega^2 a$.\n$$\\frac{G M_{\\star} m_p}{a^2} = \\left(\\frac{M_{\\star} m_p}{M_{\\star} + m_p}\\right) \\omega^2 a$$\nSolving for $\\omega^2$ gives Kepler's Third Law for a two-body system:\n$$\\omega^2 = \\frac{G(M_{\\star} + m_p)}{a^3}$$\nThe star orbits the CM with this same angular velocity $\\omega$. The magnitude of the star's orbital velocity, $v_{\\star}$, is therefore:\n$$v_{\\star} = \\omega a_{\\star} = \\sqrt{\\frac{G(M_{\\star} + m_p)}{a^3}} \\left(\\frac{m_p}{M_{\\star} + m_p}a\\right)$$\nSimplifying this expression, we get:\n$$v_{\\star} = \\frac{m_p \\sqrt{G(M_{\\star} + m_p)} a}{\\sqrt{a^3} (M_{\\star} + m_p)} = \\frac{m_p \\sqrt{G}}{\\sqrt{a} \\sqrt{M_{\\star} + m_p}} = m_p \\sqrt{\\frac{G}{a(M_{\\star} + m_p)}}$$\nThe line-of-sight radial velocity, $v_r(t)$, is the component of the star's orbital velocity vector, $\\vec{v}_{\\star}(t)$, along the observer's line of sight. The orbital inclination, $i$, is the angle between the normal to the orbital plane and the line of sight. For a circular orbit, the velocity vector $\\vec{v}_{\\star}(t)$ has a constant magnitude $v_{\\star}$ and rotates in the orbital plane. The component along the line of sight varies sinusoidally with time. The maximum value of this component, known as the radial velocity semi-amplitude $K$, is given by:\n$$K = v_{\\star} \\sin i$$\nThe full time-dependent expression for the star's radial velocity is $v_r(t) = K \\cos(\\omega t + \\phi)$, where $\\phi$ is a phase constant. The problem asks for the expression for $v_r(t)$ and the semi-amplitude $K$.\nThe expression for the semi-amplitude is:\n$$K = \\left(m_p \\sqrt{\\frac{G}{a(M_{\\star} + m_p)}}\\right) \\sin i$$\nThe star's line-of-sight radial velocity as a function of time is:\n$$v_r(t) = \\left(m_p \\sin i \\sqrt{\\frac{G}{a(M_{\\star} + m_p)}}\\right) \\cos\\left(\\sqrt{\\frac{G(M_{\\star} + m_p)}{a^3}} t + \\phi\\right)$$\nNow, we evaluate the semi-amplitude $K$ numerically. The given values are:\n$M_{\\star} = 1\\,M_{\\odot} = 1.98847\\times 10^{30}\\,\\mathrm{kg}$\n$m_{p} = 1\\,M_{\\mathrm{J}} = 1.89813\\times 10^{27}\\,\\mathrm{kg}$\n$a = 1\\,\\mathrm{AU} = 1.495978707\\times 10^{11}\\,\\mathrm{m}$\n$i = \\pi/2$, so $\\sin i = \\sin(\\pi/2) = 1$\n$G = 6.67430\\times 10^{-11}\\,\\mathrm{m^{3}\\,kg^{-1}\\,s^{-2}}$\n\nFirst, calculate the total mass $M_{\\star} + m_p$:\n$$M_{\\star} + m_p = 1.98847\\times 10^{30}\\,\\mathrm{kg} + 1.89813\\times 10^{27}\\,\\mathrm{kg} = 1.99036813\\times 10^{30}\\,\\mathrm{kg}$$\nNext, we calculate the term $a(M_{\\star} + m_p)$:\n$$a(M_{\\star} + m_p) = (1.495978707\\times 10^{11}\\,\\mathrm{m}) \\times (1.99036813\\times 10^{30}\\,\\mathrm{kg}) \\approx 2.977543\\times 10^{41}\\,\\mathrm{kg \\cdot m}$$\nNow we compute the term inside the square root:\n$$\\frac{G}{a(M_{\\star} + m_p)} = \\frac{6.67430\\times 10^{-11}\\,\\mathrm{m^{3}\\,kg^{-1}\\,s^{-2}}}{2.977543\\times 10^{41}\\,\\mathrm{kg \\cdot m}} \\approx 2.24158\\times 10^{-52}\\,\\mathrm{m^2\\,kg^{-2}\\,s^{-2}}$$\nTaking the square root:\n$$\\sqrt{\\frac{G}{a(M_{\\star} + m_p)}} \\approx \\sqrt{2.24158\\times 10^{-52}\\,\\mathrm{m^2\\,kg^{-2}\\,s^{-2}}} \\approx 1.49719\\times 10^{-26}\\,\\mathrm{m\\,kg^{-1}\\,s^{-1}}$$\nFinally, we calculate $K$:\n$$K = m_p \\sin i \\sqrt{\\frac{G}{a(M_{\\star} + m_p)}} = (1.89813\\times 10^{27}\\,\\mathrm{kg}) \\times 1 \\times (1.49719\\times 10^{-26}\\,\\mathrm{m\\,kg^{-1}\\,s^{-1}})$$\n$$K \\approx 28.418\\,\\mathrm{m/s}$$\nRounding to three significant figures, the radial velocity semi-amplitude is $28.4\\,\\mathrm{m/s}$.\n\nRegarding detectability, a radial velocity signal with a semi-amplitude of $K \\approx 28.4\\,\\mathrm{m/s}$ is well within the detection capabilities of modern high-precision spectrographs. Instruments like the High Accuracy Radial velocity Planet Searcher (HARPS) can achieve a long-term precision of approximately $1\\,\\mathrm{m/s}$, and newer spectrographs like ESPRESSO aim for a precision of $10\\,\\mathrm{cm/s}$. Therefore, a signal of this magnitude would be considered a strong and readily detectable signature of an orbiting exoplanet. The first exoplanet discovered around a sun-like star, $51\\,\\mathrm{Pegasi\\,b}$, induced a radial velocity variation with a semi-amplitude of about $56\\,\\mathrm{m/s}$. Jupiter induces a variation of about $12.4\\,\\mathrm{m/s}$ in the Sun's velocity. The calculated signal is thus very significant.",
            "answer": "$$\n\\boxed{28.4}\n$$"
        },
        {
            "introduction": "While circular orbits provide a simple starting point, most exoplanets follow elliptical paths, which requires solving Kepler's equation to predict their position over time. This practice guides you through deriving and implementing numerical methods to solve this fundamental equation of celestial mechanics . Mastering this computational task is a prerequisite for fitting Keplerian models to radial velocity data for eccentric orbits.",
            "id": "4184031",
            "problem": "Consider the task of inferring orbital phase for a star–planet system in the Radial Velocity (RV) method for exoplanet detection. The core computational subproblem is solving Kepler’s equation for an elliptic orbit,\n$$\nM = E - e\\sin E,\n$$\nwhere $M$ is the mean anomaly, $E$ is the eccentric anomaly, and $e$ is the orbital eccentricity with $0 \\le e  1$. Angles must be treated in radians.\n\nYour tasks are:\n\n1. Starting from the definition of the root-finding problem $$f(E) = E - e\\sin E - M,$$ and using only elementary calculus and well-tested numerical analysis facts, derive an iterative scheme based on Newton–Raphson to solve for $E$ given $M$ and $e$. Do not shortcut to a pre-stated iteration; instead, derive the iteration rule from first principles.\n\n2. Derive a fixed-point iteration by rearranging Kepler’s equation into the form $$E = g(E)$$ and justify when this mapping is a contraction on $\\mathbb{R}$, guaranteeing global convergence. Explicitly state conditions under which convergence is guaranteed, including any bounds required.\n\n3. Provide a computable sufficient condition for guaranteed convergence of Newton–Raphson using a standard one-dimensional Kantorovich-type bound that depends on $f(E)$, $f'(E)$, and a Lipschitz bound on $f'(E)$ in a neighborhood of the initial guess.\n\n4. Implement a program that:\n   - Computes $E$ by Newton–Raphson with an initial guess chosen from first principles (motivate your choice in the solution).\n   - Computes $E$ by fixed-point iteration.\n   - Evaluates the sufficient Newton–Raphson convergence condition at the chosen initial guess.\n   - Uses the stopping criterion $\\lvert f(E)\\rvert \\le 1\\times 10^{-12}$ for both methods.\n   - Uses a maximum of $50$ iterations for Newton–Raphson and $10000$ iterations for fixed-point iteration.\n   - Angles are in radians and all calculations must be carried out in radians.\n\n5. Test Suite. Run your implementation for the following scientifically plausible test cases (angles in radians):\n   - Case A (happy path): $e = 0.1$, $M = 1.0$.\n   - Case B (circular boundary): $e = 0.0$, $M = 2.0$.\n   - Case C (high eccentricity, near apocenter): $e = 0.9$, $M = 3.1$.\n   - Case D (boundary mean anomaly): $e = 0.7$, $M = 0.0$.\n   - Case E (near $2\\pi$): $e = 0.5$, $M = 6.0$.\n\n6. Output specification. For each test case, produce the triple consisting of:\n   - The Newton–Raphson solution $E_{\\mathrm{NR}}$ (a float in radians),\n   - The fixed-point solution $E_{\\mathrm{FP}}$ (a float in radians),\n   - A boolean indicating whether the Kantorovich-type sufficient condition is satisfied at the chosen initial guess.\nAggregate the results across all test cases, in the order listed above, into a single line printed as a comma-separated list enclosed in square brackets, with entries ordered by test case and within each case by method and then the boolean. For example, the output must have the form\n$$\n[ E_{\\mathrm{NR},A}, E_{\\mathrm{FP},A}, \\text{bool}_A, E_{\\mathrm{NR},B}, E_{\\mathrm{FP},B}, \\text{bool}_B, \\dots, E_{\\mathrm{NR},E}, E_{\\mathrm{FP},E}, \\text{bool}_E ].\n$$\nAll angles must be reported in radians as floats, and booleans must be literal $\\text{True}$ or $\\text{False}$.\n\nYour program must be self-contained, require no input, and strictly adhere to the specified output format.",
            "solution": "The problem requires the derivation, analysis, and implementation of numerical methods to solve Kepler's equation, $M = E - e\\sin E$, for the eccentric anomaly $E$, given the mean anomaly $M$ and orbital eccentricity $e$.\n\nThe root-finding problem is defined by the function $f(E) = E - e\\sin E - M = 0$. All calculations and variable interpretations involving angles must be in radians, and the eccentricity is constrained by $0 \\le e  1$.\n\n### 1. Derivation of the Newton-Raphson Iterative Scheme\n\nThe Newton-Raphson method is a root-finding algorithm that produces successively better approximations to the roots of a real-valued function. The general iterative formula for finding a root of a function $f(x)$ is:\n$$\nx_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)}\n$$\nTo apply this to Kepler's equation, we first define our function $f(E)$ and find its derivative with respect to $E$.\nThe function is given as:\n$$\nf(E) = E - e\\sin E - M\n$$\nThe derivative, $f'(E)$, is found using elementary calculus:\n$$\nf'(E) = \\frac{d}{dE}(E - e\\sin E - M) = 1 - e\\cos E\n$$\nA crucial property for convergence and stability is that $f'(E)$ is never zero. Since the eccentricity $e$ is strictly less than $1$ ($0 \\le e  1$) and $|\\cos E| \\le 1$, the minimum value of $f'(E)$ is $1 - e$, which is strictly positive. Thus, division by zero is never a risk.\n\nSubstituting $f(E)$ and $f'(E)$ into the general Newton-Raphson formula, we obtain the specific iterative scheme for solving Kepler's equation:\n$$\nE_{k+1} = E_k - \\frac{E_k - e\\sin E_k - M}{1 - e\\cos E_k}\n$$\nThis formula allows us to compute the next approximation, $E_{k+1}$, from the current one, $E_k$.\n\n### 2. Derivation and Convergence of Fixed-Point Iteration\n\nA fixed-point iteration finds a solution to an equation of the form $x = g(x)$ by iterating $x_{k+1} = g(x_k)$. To derive such a scheme, we rearrange Kepler's equation, $M = E - e\\sin E$, to isolate $E$ on one side:\n$$\nE = M + e\\sin E\n$$\nThis gives us a natural choice for the function $g(E)$:\n$$\ng(E) = M + e\\sin E\n$$\nThe corresponding fixed-point iterative scheme is:\n$$\nE_{k+1} = M + e\\sin E_k\n$$\nTo justify when this iteration guarantees convergence, we use the Banach Fixed-Point Theorem (or Contraction Mapping Principle). This theorem states that if $g$ is a contraction mapping on a complete metric space, it has a unique fixed point, and the iteration $E_{k+1} = g(E_k)$ will converge to this fixed point for any initial guess $E_0$ in that space.\n\nFor a real-valued function $g(E)$ on an interval, it is a contraction mapping if its derivative is uniformly bounded by a constant less than $1$. That is, there exists a constant $L$ such that $|g'(E)| \\le L  1$ for all $E$ in the domain.\nLet's compute the derivative of our $g(E)$:\n$$\ng'(E) = \\frac{d}{dE}(M + e\\sin E) = e\\cos E\n$$\nTo find the bound on $|g'(E)|$, we take the absolute value:\n$$\n|g'(E)| = |e\\cos E| = |e||\\cos E|\n$$\nSince the eccentricity is given as $0 \\le e  1$ and the cosine function is bounded by $|\\cos E| \\le 1$, we have:\n$$\n|g'(E)| \\le e \\cdot 1 = e\n$$\nThe problem states $0 \\le e  1$. Therefore, we have $|g'(E)| \\le e  1$ for all real numbers $E$. This means that $g(E) = M + e\\sin E$ is a contraction mapping on the entire real line $\\mathbb{R}$. Consequently, by the Banach Fixed-Point Theorem, the fixed-point iteration $E_{k+1} = M + e\\sin E_k$ is guaranteed to converge to the unique solution of Kepler's equation for any real initial guess $E_0$, provided that $0 \\le e  1$.\n\n### 3. Computable Sufficient Condition for Newton-Raphson Convergence\n\nThe Kantorovich theorem provides a sufficient condition for the convergence of the Newton-Raphson method from a given starting point $E_0$. A one-dimensional version of the theorem states that if the following inequality holds for an initial guess $E_0$:\n$$\nh = |f'(E_0)^{-1} f(E_0)| \\cdot L \\cdot |f'(E_0)^{-1}| \\le \\frac{1}{2}\n$$\nwhere $L$ is a Lipschitz constant for $f'(E)$ in a neighborhood of the solution, then Newton's method is guaranteed to converge to a root. The Lipschitz condition means $|f'(x) - f'(y)| \\le L|x-y|$. A sufficient condition for this is a bound on the second derivative, i.e., $L \\ge |f''(E)|$.\n\nLet's find the second derivative of our function $f(E)$:\n$$\nf'(E) = 1 - e\\cos E\n$$\n$$\nf''(E) = \\frac{d}{dE}(1 - e\\cos E) = e\\sin E\n$$\nThe second derivative is bounded over $\\mathbb{R}$ by $|f''(E)| = |e\\sin E| \\le |e| = e$. We can therefore choose the Lipschitz constant $L = e$.\n\nSubstituting our functions and the Lipschitz constant $L=e$ into the Kantorovich condition:\n$$\nh = \\frac{|f(E_0)|}{|f'(E_0)|} \\cdot e \\cdot \\frac{1}{|f'(E_0)|} = \\frac{e |f(E_0)|}{(f'(E_0))^2} \\le \\frac{1}{2}\n$$\n(We use $(f'(E_0))^2$ instead of $|f'(E_0)|^2$ because $f'(E) = 1 - e\\cos E  0$).\nSubstituting the expressions for $f(E_0)$ and $f'(E_0)$:\n$$\n\\frac{e |E_0 - e\\sin E_0 - M|}{(1 - e\\cos E_0)^2} \\le \\frac{1}{2}\n$$\nThis is the computable sufficient condition for the guaranteed convergence of the Newton-Raphson method.\n\n### 4. Implementation Choices\n\n**Initial Guess ($E_0$):** For both iterative methods, a well-chosen initial guess is important for efficiency. For Kepler's equation, if the eccentricity $e$ were $0$, the equation simplifies to $E = M$. This provides a physically motivated and simple first approximation. We will therefore use the initial guess:\n$$\nE_0 = M\n$$\nThis choice is robust, especially for low eccentricities, and serves as a standard baseline.\n\n**Kantorovich Condition at $E_0=M$:** With the choice $E_0=M$, the sufficient condition derived above becomes specific and easy to compute.\nWe evaluate $f(E_0)$ and $f'(E_0)$ at $E_0=M$:\n$$\nf(M) = M - e\\sin M - M = -e\\sin M\n$$\n$$\nf'(M) = 1 - e\\cos M\n$$\nSubstituting these into the condition:\n$$\n\\frac{e |-e\\sin M|}{(1 - e\\cos M)^2} \\le \\frac{1}{2}\n$$\n$$\n\\frac{e^2 |\\sin M|}{(1 - e\\cos M)^2} \\le \\frac{1}{2}\n$$\nThis is the specific boolean condition that will be evaluated for each test case at the chosen initial guess $E_0=M$.\n\n**Stopping Criterion:** As specified, both algorithms will terminate when the magnitude of the residual of Kepler's equation is below a tolerance $\\epsilon = 1 \\times 10^{-12}$. That is, an estimate $E_k$ is accepted as the solution if $|f(E_k)| = |E_k - e\\sin E_k - M| \\le 1 \\times 10^{-12}$.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves Kepler's equation using Newton-Raphson and fixed-point iteration methods\n    for a suite of test cases and evaluates a Kantorovich-type sufficient condition\n    for Newton-Raphson convergence.\n    \"\"\"\n\n    # Test cases: (e, M)\n    test_cases = [\n        (0.1, 1.0),  # Case A: Happy path\n        (0.0, 2.0),  # Case B: Circular boundary\n        (0.9, 3.1),  # Case C: High eccentricity, near apocenter\n        (0.7, 0.0),  # Case D: Boundary mean anomaly\n        (0.5, 6.0)   # Case E: Near 2*pi\n    ]\n\n    results = []\n    \n    # Define constants for the solvers\n    TOLERANCE = 1.0e-12\n    NR_MAX_ITER = 50\n    FP_MAX_ITER = 10000\n\n    def kepler_f(E, e, M):\n        \"\"\"The function f(E) = E - e*sin(E) - M for which we seek the root.\"\"\"\n        return E - e * np.sin(E) - M\n\n    def kepler_f_prime(E, e):\n        \"\"\"The derivative f'(E) = 1 - e*cos(E).\"\"\"\n        return 1.0 - e * np.cos(E)\n\n    def compute_nr(e, M):\n        \"\"\"Computes eccentric anomaly E using Newton-Raphson.\"\"\"\n        # Initial guess E_0 = M.\n        # This is a good first-order approximation, as E=M for e=0.\n        E = float(M)\n\n        for _ in range(NR_MAX_ITER):\n            f_val = kepler_f(E, e, M)\n            if np.abs(f_val) = TOLERANCE:\n                return E\n            \n            f_prime_val = kepler_f_prime(E, e)\n            # Since 0 = e  1, f_prime_val = 1 - e*cos(E) = 1 - e  0.\n            # No division-by-zero check is necessary.\n            E = E - f_val / f_prime_val\n        \n        # Return the last computed value if max iterations are reached\n        return E\n\n    def compute_fp(e, M):\n        \"\"\"Computes eccentric anomaly E using fixed-point iteration.\"\"\"\n        # Initial guess E_0 = M.\n        # As per the derivation, convergence is guaranteed for any E_0.\n        E = float(M)\n\n        # The loop must check the condition on f(E), not on the step size.\n        # So we check f(E) before computing the next E.\n        for _ in range(FP_MAX_ITER):\n            if np.abs(kepler_f(E, e, M)) = TOLERANCE:\n                return E\n            \n            # The fixed-point update rule: E_{k+1} = M + e*sin(E_k)\n            E = M + e * np.sin(E)\n        \n        # Return the last computed value if max iterations are reached\n        # One final check on the final value\n        if np.abs(kepler_f(E, e, M)) = TOLERANCE:\n            return E\n        # Failed to converge within max iterations to the required tolerance.\n        # Although theoretically guaranteed, this indicates slow convergence.\n        # Returning the last value as per problem structure.\n        return E\n\n    def check_kantorovich(e, M):\n        \"\"\"\n        Checks the sufficient condition for Newton-Raphson convergence\n        for the initial guess E_0 = M.\n        Condition: e^2 * |sin(M)| / (1 - e*cos(M))^2 = 0.5\n        \"\"\"\n        # For e=0, the condition is 0 = 0.5, which is True.\n        if e == 0.0:\n            return True\n\n        E0 = float(M)\n        numerator = e**2 * np.abs(np.sin(E0))\n        denominator = (1.0 - e * np.cos(E0))**2\n\n        # Denominator is guaranteed to be non-zero for e  1.\n        h = numerator / denominator\n        return h = 0.5\n\n    for e, M in test_cases:\n        e_nr = compute_nr(e, M)\n        e_fp = compute_fp(e, M)\n        kantorovich_satisfied = check_kantorovich(e, M)\n        \n        results.extend([e_nr, e_fp, kantorovich_satisfied])\n\n    # Format the output as a single comma-separated list in brackets\n    # Boolean values are automatically converted to \"True\" or \"False\" by str()\n    output_str = \"[\" + \",\".join(map(str, results)) + \"]\"\n    print(output_str)\n\nsolve()\n\n```"
        },
        {
            "introduction": "A successful exoplanet detection relies on correctly understanding and modeling the noise in your data. This exercise addresses the common scenario where instrumental uncertainties alone do not capture the full data scatter, requiring you to model an additional \"jitter\" term . You will use the powerful technique of maximum likelihood estimation to determine the level of this extra noise, a critical skill for pushing towards the detection of low-mass planets.",
            "id": "4184035",
            "problem": "You are given several independent sets of one-dimensional radial velocity residuals modeled as the output of the radial velocity method after subtraction of any deterministic signal (e.g., a Keplerian fit), together with per-measurement formal uncertainties that are suspected to underpredict the observed scatter. Assume the following physically and statistically grounded model: each measured residual $y_i$ is the sum of an unknown constant offset $\\gamma$ and a noise term that is the sum of two independent Gaussian contributions: a formal measurement error with known standard deviation $\\sigma_i$ and an additional constant white-noise \"jitter\" with unknown standard deviation $\\sigma_\\mathrm{jit} \\ge 0$. Under independence and Gaussianity, the total noise for point $i$ is Gaussian with zero mean and variance $v_i = \\sigma_i^2 + \\sigma_\\mathrm{jit}^2$. The joint likelihood of the data is the product of the individual Gaussian densities. Your task is to estimate $\\sigma_\\mathrm{jit}$ by maximum likelihood, profiling (i.e., optimizing) over the unknown offset $\\gamma$ at each trial value of $\\sigma_\\mathrm{jit}$.\n\nBase your derivation on the following fundamental principles and facts only: (i) independence of measurements; (ii) properties of Gaussian random variables, including that independent Gaussian variances add; (iii) the definition of likelihood under independence; (iv) the equivalence of maximum likelihood to minimizing the negative log-likelihood; (v) optimization of a scalar function over a closed interval.\n\nFor each dataset below, compute the maximum likelihood estimate $\\hat{\\sigma}_\\mathrm{jit}$ in $\\mathrm{m/s}$, rounded to three decimal places. You must treat $\\gamma$ as an unknown nuisance parameter to be optimized jointly (via profiling) with $\\sigma_\\mathrm{jit}$. Angles do not appear in this problem. Ensure that your program returns physically meaningful non-negative estimates, respecting the boundary at $\\sigma_\\mathrm{jit}=0$. Results must be expressed in $\\mathrm{m/s}$.\n\nTest suite (each dataset is a pair of lists, the first being residuals $y_i$ in $\\mathrm{m/s}$ and the second being formal uncertainties $\\sigma_i$ in $\\mathrm{m/s}$):\n\n- Case A (underestimated errors, moderate scatter):\n  $y = [1.7,\\,-2.1,\\,0.3,\\,3.2,\\,-1.8,\\,2.4,\\,-0.7,\\,1.1,\\,-3.0,\\,2.7,\\,-1.2,\\,0.9]$,\n  $\\sigma = [1.0,\\,1.1,\\,0.9,\\,1.2,\\,1.0,\\,1.0,\\,1.1,\\,0.9,\\,1.0,\\,1.2,\\,1.1,\\,1.0]$.\n\n- Case B (noise consistent with formal errors; boundary at zero jitter is relevant):\n  $y = [0.1,\\,-0.3,\\,0.5,\\,-0.2,\\,0.0,\\,0.4,\\,-0.6,\\,0.2]$,\n  $\\sigma = [0.5,\\,0.6,\\,0.4,\\,0.5,\\,0.4,\\,0.5,\\,0.6,\\,0.5]$.\n\n- Case C (heteroscedastic formal errors; large additional scatter):\n  $y = [8.0,\\,-7.5,\\,5.2,\\,-6.1,\\,7.9,\\,-5.3,\\,6.8,\\,-7.2,\\,5.5,\\,-6.4]$,\n  $\\sigma = [2.0,\\,1.5,\\,2.5,\\,1.8,\\,2.2,\\,1.7,\\,2.1,\\,1.6,\\,2.4,\\,1.9]$.\n\n- Case D (high-precision formal errors with small but non-negligible extra noise):\n  $y = [0.10,\\,0.12,\\,-0.05,\\,0.08,\\,-0.02,\\,0.03,\\,-0.07,\\,0.09,\\,-0.01,\\,0.04]$,\n  $\\sigma = [0.05,\\,0.05,\\,0.05,\\,0.05,\\,0.05,\\,0.05,\\,0.05,\\,0.05,\\,0.05,\\,0.05]$.\n\nYour program must compute $\\hat{\\sigma}_\\mathrm{jit}$ for each case by maximizing the likelihood with respect to $\\sigma_\\mathrm{jit}$ and $\\gamma$. The final output format must be a single line containing the four results as a comma-separated list enclosed in square brackets, in the order A, B, C, D, for example $[r_A,r_B,r_C,r_D]$, where each $r_\\cdot$ is a float rounded to three decimal places and implicitly in $\\mathrm{m/s}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, as just specified. No other output is permitted.",
            "solution": "We begin from a generative model consistent with radial velocity residuals. Let the measured residuals be $\\{y_i\\}_{i=1}^N$ with formal uncertainties $\\{\\sigma_i\\}_{i=1}^N$. Assume an unknown constant offset $\\gamma$ and an additional white-noise \"jitter\" with standard deviation $\\sigma_\\mathrm{jit} \\ge 0$ that is independent of the formal measurement errors. Each datum is modeled as\n$$\ny_i = \\gamma + \\epsilon_i + \\eta_i,\n$$\nwhere $\\epsilon_i \\sim \\mathcal{N}(0,\\sigma_i^2)$ and $\\eta_i \\sim \\mathcal{N}(0,\\sigma_\\mathrm{jit}^2)$ are independent. By the properties of independent Gaussian random variables, $\\epsilon_i + \\eta_i \\sim \\mathcal{N}(0, \\sigma_i^2 + \\sigma_\\mathrm{jit}^2)$. Define $v_i(\\sigma_\\mathrm{jit}) = \\sigma_i^2 + \\sigma_\\mathrm{jit}^2$.\n\nUnder independence across $i$, the likelihood of the data given $(\\gamma,\\sigma_\\mathrm{jit})$ is\n$$\n\\mathcal{L}(\\gamma,\\sigma_\\mathrm{jit}) = \\prod_{i=1}^N \\frac{1}{\\sqrt{2\\pi v_i(\\sigma_\\mathrm{jit})}} \\exp\\!\\left( -\\frac{(y_i - \\gamma)^2}{2 v_i(\\sigma_\\mathrm{jit})} \\right).\n$$\nMaximizing the likelihood is equivalent to minimizing the negative log-likelihood\n$$\n\\mathrm{NLL}(\\gamma,\\sigma_\\mathrm{jit}) = \\frac{1}{2}\\sum_{i=1}^N \\left[ \\ln\\!\\left(2\\pi v_i(\\sigma_\\mathrm{jit})\\right) + \\frac{(y_i - \\gamma)^2}{v_i(\\sigma_\\mathrm{jit})} \\right].\n$$\nWe seek the maximum likelihood estimate $\\hat{\\sigma}_\\mathrm{jit}$ while treating $\\gamma$ as a nuisance parameter. For any fixed $\\sigma_\\mathrm{jit}$, the function $\\mathrm{NLL}(\\gamma,\\sigma_\\mathrm{jit})$ is strictly convex in $\\gamma$ because it is a weighted sum of squared residuals with positive weights $w_i(\\sigma_\\mathrm{jit}) = 1/v_i(\\sigma_\\mathrm{jit})$. Setting the derivative with respect to $\\gamma$ to zero yields\n$$\n\\frac{\\partial \\mathrm{NLL}}{\\partial \\gamma} = - \\sum_{i=1}^N \\frac{y_i - \\gamma}{v_i(\\sigma_\\mathrm{jit})} = 0 \\quad \\Longrightarrow \\quad \\hat{\\gamma}(\\sigma_\\mathrm{jit}) = \\frac{\\sum_{i=1}^N w_i(\\sigma_\\mathrm{jit}) y_i}{\\sum_{i=1}^N w_i(\\sigma_\\mathrm{jit})},\n$$\nwhich is the weighted mean with weights $w_i(\\sigma_\\mathrm{jit}) = 1/v_i(\\sigma_\\mathrm{jit})$. Substituting $\\hat{\\gamma}(\\sigma_\\mathrm{jit})$ back into $\\mathrm{NLL}$ yields the profiled negative log-likelihood\n$$\n\\mathrm{NLL}_p(\\sigma_\\mathrm{jit}) = \\frac{1}{2}\\sum_{i=1}^N \\left[ \\ln\\!\\left(2\\pi v_i(\\sigma_\\mathrm{jit})\\right) + \\frac{(y_i - \\hat{\\gamma}(\\sigma_\\mathrm{jit}))^2}{v_i(\\sigma_\\mathrm{jit})} \\right].\n$$\nThe maximum likelihood estimate $\\hat{\\sigma}_\\mathrm{jit}$ is any minimizer of $\\mathrm{NLL}_p(\\sigma_\\mathrm{jit})$ over the closed interval $\\sigma_\\mathrm{jit} \\in [0, \\infty)$. In practice, we search over $\\sigma_\\mathrm{jit} \\in [0, S_\\mathrm{max}]$ with a sufficiently large $S_\\mathrm{max}$ to contain the optimum. Because $v_i(\\sigma_\\mathrm{jit})$ grows with $\\sigma_\\mathrm{jit}$, $\\mathrm{NLL}_p(\\sigma_\\mathrm{jit})$ tends to increase for very large $\\sigma_\\mathrm{jit}$, so it suffices to set $S_\\mathrm{max}$ several times larger than the observed scatter.\n\nAlgorithmic procedure for each dataset:\n1. Given $y$ and $\\sigma$, define $v_i(\\sigma_\\mathrm{jit}) = \\sigma_i^2 + \\sigma_\\mathrm{jit}^2$.\n2. For any trial $\\sigma_\\mathrm{jit}  0$, compute weights $w_i(\\sigma_\\mathrm{jit}) = 1/v_i(\\sigma_\\mathrm{jit})$, the profiled offset $\\hat{\\gamma}(\\sigma_\\mathrm{jit}) = \\left(\\sum_i w_i y_i\\right)/\\left(\\sum_i w_i\\right)$, and the profiled negative log-likelihood $\\mathrm{NLL}_p(\\sigma_\\mathrm{jit})$ as above.\n3. Numerically minimize $\\mathrm{NLL}_p(\\sigma_\\mathrm{jit})$ over $\\sigma_\\mathrm{jit} \\in (0, S_\\mathrm{max}]$ using a bounded one-dimensional optimizer. Separately evaluate $\\mathrm{NLL}_p(0)$ corresponding to no additional jitter. Select the $\\sigma_\\mathrm{jit}$ that yields the smaller value between the interior minimizer and the boundary at $\\sigma_\\mathrm{jit}=0$.\n4. Report $\\hat{\\sigma}_\\mathrm{jit}$ in $\\mathrm{m/s}$, rounded to three decimal places.\n\nInterpretation of $\\hat{\\sigma}_\\mathrm{jit}$: If $\\hat{\\sigma}_\\mathrm{jit}$ is close to $0$, the formal uncertainties $\\{\\sigma_i\\}$ adequately describe the observed scatter. A strictly positive $\\hat{\\sigma}_\\mathrm{jit}$ indicates additional unmodeled white noise, which may be of astrophysical origin (e.g., stellar activity) or instrumental/systematic origin. The magnitude of $\\hat{\\sigma}_\\mathrm{jit}$ relative to a characteristic formal uncertainty (such as the median of $\\{\\sigma_i\\}$) conveys whether the extra noise is negligible, comparable, or dominant. Because the estimation is based on maximum likelihood with a profiled nuisance parameter, $\\hat{\\sigma}_\\mathrm{jit}$ reflects the amount of additional variance required to reconcile the distribution of residuals with the Gaussian noise model.\n\nThe implementation uses a robust bounded scalar minimization of $\\mathrm{NLL}_p(\\sigma_\\mathrm{jit})$ and an explicit comparison against the boundary at $\\sigma_\\mathrm{jit}=0$ to respect the physical constraint $\\sigma_\\mathrm{jit} \\ge 0$. The final printed list contains four floats corresponding to Cases A–D, in $\\mathrm{m/s}$, rounded to three decimal places, formatted as a comma-separated list enclosed in square brackets.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize_scalar\n\ndef profiled_nll(s_jit, y, sig):\n    \"\"\"\n    Compute the profiled negative log-likelihood for given sigma_jit  0,\n    profiling over gamma (systemic offset).\n    \"\"\"\n    # Ensure non-negative s_jit inside; optimizer will keep s_jit  0 for interior search.\n    s2 = s_jit * s_jit\n    v = sig**2 + s2\n    # Numerical guard: avoid zero or negative variances\n    if np.any(v = 0.0) or not np.isfinite(s2):\n        return np.inf\n    w = 1.0 / v\n    # Weighted mean for gamma at this s_jit\n    gamma_hat = np.sum(w * y) / np.sum(w)\n    r = y - gamma_hat\n    # Negative log-likelihood\n    nll = 0.5 * np.sum(np.log(2.0 * np.pi * v) + (r * r) / v)\n    return float(nll)\n\ndef profiled_nll_at_zero(y, sig):\n    \"\"\"\n    Compute profiled NLL at s_jit = 0, handling that case explicitly.\n    \"\"\"\n    v = sig**2\n    w = 1.0 / v\n    gamma_hat = np.sum(w * y) / np.sum(w)\n    r = y - gamma_hat\n    nll = 0.5 * np.sum(np.log(2.0 * np.pi * v) + (r * r) / v)\n    return float(nll)\n\ndef estimate_sigma_jit_mle(y, sig):\n    \"\"\"\n    Estimate sigma_jit via maximum likelihood with profiling over gamma.\n    Returns the non-negative MLE.\n    \"\"\"\n    y = np.asarray(y, dtype=float)\n    sig = np.asarray(sig, dtype=float)\n\n    # Define an upper bound for search that is safely large relative to the data scatter.\n    # Use observed scatter and formal errors to scale the bound conservatively.\n    # Add margins to reduce risk of truncating the optimum.\n    scatter = np.std(y, ddof=1) if y.size  1 else np.abs(y[0])\n    s_upper = max(5.0 * scatter + 5.0 * np.median(sig), 50.0)\n\n    # Define objective for optimizer (interior of (0, s_upper])\n    obj = lambda s: profiled_nll(s, y, sig)\n\n    # Perform bounded scalar minimization on (eps, s_upper)\n    eps = 1e-12\n    res = minimize_scalar(obj, bounds=(eps, s_upper), method='bounded', options={'xatol': 1e-10})\n\n    # Evaluate at boundary s=0 explicitly\n    nll0 = profiled_nll_at_zero(y, sig)\n    s_opt = res.x if res.success else s_upper\n    nll_opt = obj(s_opt)\n\n    # Choose the better of s=0 and interior optimum\n    if nll0 = nll_opt + 0.0:\n        return 0.0\n    else:\n        # Numerical safety: enforce non-negativity\n        return max(0.0, float(s_opt))\n\ndef solve():\n    # Define the test cases from the problem statement.\n\n    # Case A\n    yA = np.array([1.7, -2.1, 0.3, 3.2, -1.8, 2.4, -0.7, 1.1, -3.0, 2.7, -1.2, 0.9], dtype=float)\n    sA = np.array([1.0, 1.1, 0.9, 1.2, 1.0, 1.0, 1.1, 0.9, 1.0, 1.2, 1.1, 1.0], dtype=float)\n\n    # Case B\n    yB = np.array([0.1, -0.3, 0.5, -0.2, 0.0, 0.4, -0.6, 0.2], dtype=float)\n    sB = np.array([0.5, 0.6, 0.4, 0.5, 0.4, 0.5, 0.6, 0.5], dtype=float)\n\n    # Case C\n    yC = np.array([8.0, -7.5, 5.2, -6.1, 7.9, -5.3, 6.8, -7.2, 5.5, -6.4], dtype=float)\n    sC = np.array([2.0, 1.5, 2.5, 1.8, 2.2, 1.7, 2.1, 1.6, 2.4, 1.9], dtype=float)\n\n    # Case D\n    yD = np.array([0.10, 0.12, -0.05, 0.08, -0.02, 0.03, -0.07, 0.09, -0.01, 0.04], dtype=float)\n    sD = np.array([0.05]*10, dtype=float)\n\n    test_cases = [\n        (yA, sA),\n        (yB, sB),\n        (yC, sC),\n        (yD, sD),\n    ]\n\n    results = []\n    for y, sig in test_cases:\n        sj = estimate_sigma_jit_mle(y, sig)\n        # Round to three decimals as required\n        results.append(f\"{sj:.3f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}