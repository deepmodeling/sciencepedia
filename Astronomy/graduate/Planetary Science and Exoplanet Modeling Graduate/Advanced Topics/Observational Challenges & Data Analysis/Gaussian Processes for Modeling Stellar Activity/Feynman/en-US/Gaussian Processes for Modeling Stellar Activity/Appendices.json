{
    "hands_on_practices": [
        {
            "introduction": "Gaussian Process kernels are not arbitrary functions; they often encode physical assumptions about the underlying process. A powerful example is the kernel derived from a stochastically driven Simple Harmonic Oscillator (SHO), which can model the quasi-periodic signal of rotating starspots. This exercise  challenges you to derive the conditions under which the SHO model produces the oscillatory behavior needed to represent such signals, connecting the mathematical form of the kernel directly to its physical interpretation.",
            "id": "4160728",
            "problem": "A star hosting exoplanets exhibits rotationally modulated brightness variations due to evolving photospheric starspots. These signals can be modeled as a Gaussian Process (GP) with a covariance structure induced by a stochastically driven damped Simple Harmonic Oscillator (SHO). Let the latent process $s(t)$ obey the linear stochastic differential equation\n$$\n\\frac{d^{2} s}{dt^{2}} + \\frac{\\omega_{0}}{Q}\\,\\frac{d s}{dt} + \\omega_{0}^{2}\\,s = \\xi(t),\n$$\nwhere $\\omega_{0}$ is the stellar rotation angular frequency, $Q$ is the dimensionless quality factor, and $\\xi(t)$ is zero-mean Gaussian white noise with constant spectral power. The SHO kernel used for stellar activity in Gaussian Process modeling has its qualitative form determined by the homogeneous solution of the above equation. In particular, the kernel is quasi-periodic (oscillatory with exponential decay) when the homogeneous solution has complex-conjugate roots, and it is non-oscillatory (a sum of decaying exponentials) when the homogeneous solution has distinct real roots.\n\nStarting from this equation and using only the fundamental properties of linear ordinary differential equations with constant coefficients, derive the condition on the quality factor $Q$ that yields underdamped quasi-periodic behavior appropriate for spot-modulated signals, and contrast it with the overdamped regime. Then, determine the critical value $Q_{c}$ that separates these regimes. Express your final answer as the single exact value of $Q_{c}$ with no units. No rounding is required.",
            "solution": "The problem statement is assessed to be valid. It is scientifically grounded in the theory of ordinary differential equations and its application to astrophysical modeling, specifically using Gaussian Processes to model stellar activity. The problem is well-posed, objective, self-contained, and free from internal contradictions or ambiguities. It requests the derivation of a condition on a physical parameter based on the mathematical properties of a given differential equation, which is a standard and solvable task in physics and engineering.\n\nThe core of the problem lies in analyzing the homogeneous form of the provided stochastic differential equation. The behavior of the solution to a second-order linear homogeneous ordinary differential equation with constant coefficients is entirely determined by the roots of its characteristic equation.\n\nThe given stochastic differential equation is:\n$$\n\\frac{d^{2} s}{dt^{2}} + \\frac{\\omega_{0}}{Q}\\,\\frac{d s}{dt} + \\omega_{0}^{2}\\,s = \\xi(t)\n$$\nThe qualitative behavior of the system, which dictates the form of the Gaussian Process kernel, is determined by the homogeneous part of this equation, obtained by setting the driving term $\\xi(t)$ to zero:\n$$\n\\frac{d^{2} s}{dt^{2}} + \\frac{\\omega_{0}}{Q}\\,\\frac{d s}{dt} + \\omega_{0}^{2}\\,s = 0\n$$\nTo find the solution, we assume a trial solution of the form $s(t) = \\exp(\\lambda t)$. Substituting this into the homogeneous differential equation yields:\n$$\n\\lambda^{2}\\exp(\\lambda t) + \\frac{\\omega_{0}}{Q}\\,\\lambda\\exp(\\lambda t) + \\omega_{0}^{2}\\exp(\\lambda t) = 0\n$$\nSince $\\exp(\\lambda t)$ is non-zero for all finite $t$, we can divide by it to obtain the characteristic (or auxiliary) equation:\n$$\n\\lambda^{2} + \\left(\\frac{\\omega_{0}}{Q}\\right)\\lambda + \\omega_{0}^{2} = 0\n$$\nThis is a quadratic equation for $\\lambda$ of the form $a\\lambda^2 + b\\lambda + c = 0$, with coefficients $a=1$, $b=\\frac{\\omega_{0}}{Q}$, and $c=\\omega_{0}^{2}$. The roots are found using the quadratic formula:\n$$\n\\lambda = \\frac{-b \\pm \\sqrt{b^{2} - 4ac}}{2a} = \\frac{-\\frac{\\omega_{0}}{Q} \\pm \\sqrt{\\left(\\frac{\\omega_{0}}{Q}\\right)^{2} - 4(1)(\\omega_{0}^{2})}}{2}\n$$\nSimplifying the expression for the roots:\n$$\n\\lambda = \\frac{-\\frac{\\omega_{0}}{Q} \\pm \\sqrt{\\frac{\\omega_{0}^{2}}{Q^{2}} - 4\\omega_{0}^{2}}}{2} = \\frac{-\\frac{\\omega_{0}}{Q} \\pm \\sqrt{\\omega_{0}^{2}\\left(\\frac{1}{Q^{2}} - 4\\right)}}{2}\n$$\nSince the rotation angular frequency $\\omega_{0}$ is a physical quantity, we can assume $\\omega_{0} > 0$. This allows us to factor $\\omega_0$ out of the square root:\n$$\n\\lambda = -\\frac{\\omega_{0}}{2Q} \\pm \\frac{\\omega_{0}}{2}\\sqrt{\\frac{1}{Q^{2}} - 4}\n$$\nThe nature of the roots, and thus the behavior of the system, depends on the sign of the discriminant, $\\Delta = b^2 - 4ac$, which in this context is the term inside the square root: $\\frac{1}{Q^{2}} - 4$.\n\n1.  **Underdamped, Quasi-periodic Regime:** The problem states this corresponds to complex-conjugate roots. This occurs when the discriminant is negative.\n    $$\n    \\frac{1}{Q^{2}} - 4 < 0\n    $$\n    $$\n    \\frac{1}{Q^{2}} < 4\n    $$\n    The quality factor $Q$ is a positive, dimensionless quantity, so we can write:\n    $$\n    1 < 4Q^2 \\implies Q^2 > \\frac{1}{4}\n    $$\n    Taking the positive square root (since $Q>0$) yields the condition for underdamped, quasi-periodic behavior:\n    $$\n    Q > \\frac{1}{2}\n    $$\n    In this case, the solution $s(t)$ is a sinusoidal oscillation with an exponentially decaying amplitude, which is appropriate for modeling rotationally modulated starspot signals that evolve over time.\n\n2.  **Overdamped, Non-oscillatory Regime:** The problem states this corresponds to distinct real roots. This occurs when the discriminant is positive.\n    $$\n    \\frac{1}{Q^{2}} - 4 > 0\n    $$\n    $$\n    \\frac{1}{Q^{2}} > 4\n    $$\n    $$\n    1 > 4Q^2 \\implies Q^2 < \\frac{1}{4}\n    $$\n    Again, since $Q>0$, this gives the condition for overdamped behavior:\n    $$\n    0 < Q < \\frac{1}{2}\n    $$\n    In this case, the solution is a sum of two decaying exponential terms without oscillation.\n\n3.  **Critically Damped Regime:** The transition between the oscillatory and non-oscillatory regimes occurs when the discriminant is exactly zero. This corresponds to a single, repeated real root and defines the critical value of the quality factor, $Q_{c}$.\n    $$\n    \\frac{1}{Q_{c}^{2}} - 4 = 0\n    $$\n    $$\n    \\frac{1}{Q_{c}^{2}} = 4\n    $$\n    $$\n    Q_{c}^{2} = \\frac{1}{4}\n    $$\n    Since $Q_{c}$ must be positive, the critical value is:\n    $$\n    Q_{c} = \\frac{1}{2}\n    $$\nThis critical value $Q_{c} = \\frac{1}{2}$ marks the boundary. For $Q > Q_{c}$, the system is underdamped and exhibits quasi-periodic oscillations. For $Q < Q_{c}$, the system is overdamped and exhibits non-oscillatory decay. The problem asks for this specific critical value.",
            "answer": "$$\\boxed{\\frac{1}{2}}$$"
        },
        {
            "introduction": "Fitting a Gaussian Process model to data involves optimizing its hyperparameters, a task most efficiently performed using gradient-based methods. This requires calculating the derivatives of the covariance kernel with respect to each hyperparameter. In this practical exercise , you will derive the first and second derivatives for a widely-used quasi-periodic kernel, a fundamental skill for implementing or customizing GP models for stellar activity.",
            "id": "4160755",
            "problem": "A common approach to model stellar activity in exoplanet radial velocity and photometric time series is to use a Gaussian Process (GP) with a quasi-periodic covariance kernel that encodes rotational modulation and finite active-region lifetimes. Consider a GP with covariance between two observation times $t$ and $t'$ given by\n$$\nk(t,t') \\;=\\; \\sigma^{2} \\,\\exp\\!\\left(-\\frac{(t-t')^{2}}{2\\lambda^{2}} \\;-\\; \\Gamma^{2}\\,\\sin^{2}\\!\\left(\\frac{\\pi (t-t')}{P_{\\mathrm{rot}}}\\right)\\right),\n$$\nwhere $\\sigma$ is the covariance amplitude, $\\lambda$ is the evolutionary timescale of active regions, $\\Gamma$ controls the harmonic complexity of the rotational modulation, and $P_{\\mathrm{rot}}$ is the stellar rotation period. Let $\\Delta \\equiv t - t'$ and define $a \\equiv \\pi \\Delta / P_{\\mathrm{rot}}$ for notational convenience.\n\nStarting from first principles in calculus and the definition of the covariance kernel above, derive closed-form analytic expressions for the first derivatives $\\partial k/\\partial \\theta$ and the second derivatives $\\partial^{2}k/\\partial \\theta^{2}$ with respect to each hyperparameter $\\theta \\in \\{\\sigma,\\lambda,\\Gamma,P_{\\mathrm{rot}}\\}$. These derivatives are required to enable gradient-based hyperparameter optimization of the GP log marginal likelihood in exoplanet modeling workflows.\n\nExpress your final result as a single row matrix containing eight entries in the following order:\n$$\n\\left(\\frac{\\partial k}{\\partial \\sigma},\\;\\frac{\\partial k}{\\partial \\lambda},\\;\\frac{\\partial k}{\\partial \\Gamma},\\;\\frac{\\partial k}{\\partial P_{\\mathrm{rot}}},\\;\\frac{\\partial^{2}k}{\\partial \\sigma^{2}},\\;\\frac{\\partial^{2}k}{\\partial \\lambda^{2}},\\;\\frac{\\partial^{2}k}{\\partial \\Gamma^{2}},\\;\\frac{\\partial^{2}k}{\\partial P_{\\mathrm{rot}}^{2}}\\right).\n$$\nNo numerical approximation is required; provide exact analytical expressions. Do not include any units in your final expressions.",
            "solution": "The problem statement is valid. It presents a well-defined mathematical problem grounded in the established practices of time-series analysis in astrophysics, specifically using Gaussian Processes to model stellar activity. All variables and functions are clearly specified, and the problem is self-contained. The task is to derive the first and second partial derivatives of the given covariance kernel with respect to its hyperparameters.\n\nLet the covariance kernel be denoted by $k$. It is a function of the time difference $\\Delta \\equiv t-t'$ and a set of hyperparameters $\\theta \\in \\{\\sigma, \\lambda, \\Gamma, P_{\\mathrm{rot}}\\}$. The kernel is given by:\n$$\nk \\;=\\; \\sigma^{2} \\exp\\left(-\\frac{\\Delta^{2}}{2\\lambda^{2}} - \\Gamma^{2}\\sin^{2}\\left(\\frac{\\pi \\Delta}{P_{\\mathrm{rot}}}\\right)\\right)\n$$\nFor notational simplicity, we define the argument of the exponential, the time difference, and the argument of the sine function as:\n$$\nE \\equiv -\\frac{\\Delta^{2}}{2\\lambda^{2}} - \\Gamma^{2}\\sin^{2}(a)\n$$\n$$\n\\Delta \\equiv t-t'\n$$\n$$\na \\equiv \\frac{\\pi \\Delta}{P_{\\mathrm{rot}}}\n$$\nThe kernel can then be written as $k = \\sigma^{2} \\exp(E)$. We will now derive the required partial derivatives with respect to each hyperparameter by applying the rules of differential calculus.\n\nFor a generic hyperparameter $\\theta$ that only appears in the exponent $E$ (i.e., $\\theta \\in \\{\\lambda, \\Gamma, P_{\\mathrm{rot}}\\}$), the first and second derivatives of $k$ follow a general pattern derived from the chain rule:\n$$\n\\frac{\\partial k}{\\partial \\theta} = \\sigma^{2} \\exp(E) \\frac{\\partial E}{\\partial \\theta} = k \\frac{\\partial E}{\\partial \\theta}\n$$\n$$\n\\frac{\\partial^{2} k}{\\partial \\theta^{2}} = \\frac{\\partial}{\\partial \\theta} \\left(k \\frac{\\partial E}{\\partial \\theta}\\right) = \\frac{\\partial k}{\\partial \\theta} \\frac{\\partial E}{\\partial \\theta} + k \\frac{\\partial^{2} E}{\\partial \\theta^{2}} = \\left(k \\frac{\\partial E}{\\partial \\theta}\\right)\\frac{\\partial E}{\\partial \\theta} + k \\frac{\\partial^{2} E}{\\partial \\theta^{2}} = k \\left[ \\left(\\frac{\\partial E}{\\partial \\theta}\\right)^{2} + \\frac{\\partial^{2} E}{\\partial \\theta^{2}} \\right]\n$$\nThe hyperparameter $\\sigma$ is a special case as it is a pre-factor.\n\n### Derivatives with respect to $\\sigma$\nThe exponent $E$ does not depend on $\\sigma$. The derivatives are straightforward:\n$$\n\\frac{\\partial k}{\\partial \\sigma} = \\frac{\\partial}{\\partial \\sigma} \\left(\\sigma^{2} \\exp(E)\\right) = 2\\sigma \\exp(E) = \\frac{2k}{\\sigma}\n$$\n$$\n\\frac{\\partial^{2} k}{\\partial \\sigma^{2}} = \\frac{\\partial}{\\partial \\sigma} \\left(2\\sigma \\exp(E)\\right) = 2 \\exp(E) = \\frac{2k}{\\sigma^{2}}\n$$\n\n### Derivatives with respect to $\\lambda$\nThe exponent $E$ depends on $\\lambda$ through the term $-\\frac{\\Delta^2}{2\\lambda^2}$. The first and second derivatives of $E$ with respect to $\\lambda$ are:\n$$\n\\frac{\\partial E}{\\partial \\lambda} = \\frac{\\partial}{\\partial \\lambda} \\left(-\\frac{\\Delta^{2}}{2}\\lambda^{-2}\\right) = -\\frac{\\Delta^{2}}{2}(-2\\lambda^{-3}) = \\frac{\\Delta^{2}}{\\lambda^{3}}\n$$\n$$\n\\frac{\\partial^{2} E}{\\partial \\lambda^{2}} = \\frac{\\partial}{\\partial \\lambda} \\left(\\Delta^{2}\\lambda^{-3}\\right) = \\Delta^{2}(-3\\lambda^{-4}) = -\\frac{3\\Delta^{2}}{\\lambda^{4}}\n$$\nUsing the general formulas, the derivatives of $k$ are:\n$$\n\\frac{\\partial k}{\\partial \\lambda} = k \\frac{\\partial E}{\\partial \\lambda} = k \\frac{\\Delta^{2}}{\\lambda^{3}}\n$$\n$$\n\\frac{\\partial^{2} k}{\\partial \\lambda^{2}} = k \\left[ \\left(\\frac{\\Delta^{2}}{\\lambda^{3}}\\right)^{2} - \\frac{3\\Delta^{2}}{\\lambda^{4}} \\right] = k \\left( \\frac{\\Delta^{4}}{\\lambda^{6}} - \\frac{3\\Delta^{2}}{\\lambda^{4}} \\right)\n$$\n\n### Derivatives with respect to $\\Gamma$\nThe exponent $E$ depends on $\\Gamma$ through the term $-\\Gamma^{2}\\sin^{2}(a)$. The derivatives of $E$ are:\n$$\n\\frac{\\partial E}{\\partial \\Gamma} = \\frac{\\partial}{\\partial \\Gamma} \\left(-\\Gamma^{2}\\sin^{2}(a)\\right) = -2\\Gamma\\sin^{2}(a)\n$$\n$$\n\\frac{\\partial^{2} E}{\\partial \\Gamma^{2}} = \\frac{\\partial}{\\partial \\Gamma} \\left(-2\\Gamma\\sin^{2}(a)\\right) = -2\\sin^{2}(a)\n$$\nThe derivatives of $k$ are then:\n$$\n\\frac{\\partial k}{\\partial \\Gamma} = k \\frac{\\partial E}{\\partial \\Gamma} = -2k\\Gamma\\sin^{2}(a)\n$$\n$$\n\\frac{\\partial^{2} k}{\\partial \\Gamma^{2}} = k \\left[ (-2\\Gamma\\sin^{2}(a))^{2} - 2\\sin^{2}(a) \\right] = k \\left( 4\\Gamma^{2}\\sin^{4}(a) - 2\\sin^{2}(a) \\right) = 2k\\sin^{2}(a) \\left( 2\\Gamma^{2}\\sin^{2}(a) - 1 \\right)\n$$\n\n### Derivatives with respect to $P_{\\mathrm{rot}}$\nThe exponent $E$ depends on $P_{\\mathrm{rot}}$ via $a = \\frac{\\pi \\Delta}{P_{\\mathrm{rot}}}$. We use the chain rule. First, we find the derivatives of $a$ with respect to $P_{\\mathrm{rot}}$:\n$$\n\\frac{\\partial a}{\\partial P_{\\mathrm{rot}}} = \\frac{\\partial}{\\partial P_{\\mathrm{rot}}} \\left(\\pi \\Delta P_{\\mathrm{rot}}^{-1}\\right) = -\\pi \\Delta P_{\\mathrm{rot}}^{-2} = -\\frac{a}{P_{\\mathrm{rot}}}\n$$\n$$\n\\frac{\\partial^{2} a}{\\partial P_{\\mathrm{rot}}^{2}} = \\frac{\\partial}{\\partial P_{\\mathrm{rot}}} \\left(-\\pi \\Delta P_{\\mathrm{rot}}^{-2}\\right) = 2\\pi \\Delta P_{\\mathrm{rot}}^{-3} = \\frac{2a}{P_{\\mathrm{rot}}^{2}}\n$$\nNext, we find the derivatives of $E$ with respect to $a$:\n$$\n\\frac{\\partial E}{\\partial a} = \\frac{\\partial}{\\partial a} \\left(-\\Gamma^{2}\\sin^{2}(a)\\right) = -\\Gamma^{2}(2\\sin(a)\\cos(a)) = -\\Gamma^{2}\\sin(2a)\n$$\n$$\n\\frac{\\partial^{2} E}{\\partial a^{2}} = \\frac{\\partial}{\\partial a} \\left(-\\Gamma^{2}\\sin(2a)\\right) = -2\\Gamma^{2}\\cos(2a)\n$$\nNow, we find the derivatives of $E$ with respect to $P_{\\mathrm{rot}}$ using the chain rule:\n$$\n\\frac{\\partial E}{\\partial P_{\\mathrm{rot}}} = \\frac{\\partial E}{\\partial a} \\frac{\\partial a}{\\partial P_{\\mathrm{rot}}} = (-\\Gamma^{2}\\sin(2a))\\left(-\\frac{a}{P_{\\mathrm{rot}}}\\right) = \\frac{a\\Gamma^{2}}{P_{\\mathrm{rot}}}\\sin(2a)\n$$\n$$\n\\frac{\\partial^{2} E}{\\partial P_{\\mathrm{rot}}^{2}} = \\frac{\\partial^{2} E}{\\partial a^{2}}\\left(\\frac{\\partial a}{\\partial P_{\\mathrm{rot}}}\\right)^{2} + \\frac{\\partial E}{\\partial a}\\frac{\\partial^{2} a}{\\partial P_{\\mathrm{rot}}^{2}} = (-2\\Gamma^{2}\\cos(2a))\\left(-\\frac{a}{P_{\\mathrm{rot}}}\\right)^{2} + (-\\Gamma^{2}\\sin(2a))\\left(\\frac{2a}{P_{\\mathrm{rot}}^{2}}\\right) = -\\frac{2a^{2}\\Gamma^{2}}{P_{\\mathrm{rot}}^{2}}\\cos(2a) - \\frac{2a\\Gamma^{2}}{P_{\\mathrm{rot}}^{2}}\\sin(2a)\n$$\nFinally, we compute the derivatives of $k$ with respect to $P_{\\mathrm{rot}}$:\n$$\n\\frac{\\partial k}{\\partial P_{\\mathrm{rot}}} = k \\frac{\\partial E}{\\partial P_{\\mathrm{rot}}} = k \\frac{a\\Gamma^{2}}{P_{\\mathrm{rot}}}\\sin(2a)\n$$\n$$\n\\frac{\\partial^{2} k}{\\partial P_{\\mathrm{rot}}^{2}} = k \\left[ \\left(\\frac{\\partial E}{\\partial P_{\\mathrm{rot}}}\\right)^{2} + \\frac{\\partial^{2} E}{\\partial P_{\\mathrm{rot}}^{2}} \\right] = k \\left[ \\left(\\frac{a\\Gamma^{2}}{P_{\\mathrm{rot}}}\\sin(2a)\\right)^{2} - \\frac{2a^{2}\\Gamma^{2}}{P_{\\mathrm{rot}}^{2}}\\cos(2a) - \\frac{2a\\Gamma^{2}}{P_{\\mathrm{rot}}^{2}}\\sin(2a) \\right]\n$$\n$$\n\\frac{\\partial^{2} k}{\\partial P_{\\mathrm{rot}}^{2}} = k\\left[ \\frac{a^{2}\\Gamma^{4}}{P_{\\mathrm{rot}}^{2}}\\sin^{2}(2a) - \\frac{2a\\Gamma^{2}}{P_{\\mathrm{rot}}^{2}}\\sin(2a) - \\frac{2a^{2}\\Gamma^{2}}{P_{\\mathrm{rot}}^{2}}\\cos(2a) \\right]\n$$\nThis expression can be factored for compactness:\n$$\n\\frac{\\partial^{2} k}{\\partial P_{\\mathrm{rot}}^{2}} = \\frac{ka\\Gamma^{2}}{P_{\\mathrm{rot}}^{2}} \\left[ a\\Gamma^{2}\\sin^{2}(2a) - 2\\sin(2a) - 2a\\cos(2a) \\right]\n$$\nWe have now derived all eight required partial derivatives. We will assemble them into the specified matrix format for the final answer.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{2k}{\\sigma} & k\\frac{\\Delta^{2}}{\\lambda^{3}} & -2k\\Gamma\\sin^{2}(a) & k\\frac{a\\Gamma^{2}}{P_{\\mathrm{rot}}}\\sin(2a) & \\frac{2k}{\\sigma^{2}} & k\\left(\\frac{\\Delta^{4}}{\\lambda^{6}} - \\frac{3\\Delta^{2}}{\\lambda^{4}}\\right) & 2k\\sin^{2}(a)\\left(2\\Gamma^{2}\\sin^{2}(a) - 1\\right) & \\frac{ka\\Gamma^{2}}{P_{\\mathrm{rot}}^{2}}\\left(a\\Gamma^{2}\\sin^{2}(2a) - 2\\sin(2a) - 2a\\cos(2a)\\right) \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "A key strength of Gaussian Processes is their ability to make principled predictions at unobserved locations, complete with uncertainty estimates. This is invaluable for handling gaps in astronomical time series, allowing us to visualize the likely behavior of stellar activity even where we have no data. This practice  guides you through the derivation of the GP posterior predictive equations, providing the mathematical foundation for imputing missing data without biasing the overall model inference.",
            "id": "4160699",
            "problem": "A star with rotationally modulated magnetic activity produces quasi-periodic variability in its radial velocity (RV) time series, which complicates exoplanet inference. Suppose you have $N_{o}$ observed RV measurements $y_{o} \\in \\mathbb{R}^{N_{o}}$ taken at times $t_{o} = (t_{o,1}, \\dots, t_{o,N_{o}})$, and $N_{m}$ unobserved times $t_{m} = (t_{m,1}, \\dots, t_{m,N_{m}})$ that fall within gaps caused by observatory downtime. Measurement noise is independent and identically distributed with known variance $\\sigma_{n}^{2}$. Model the stellar activity signal $f(t)$ with a zero-mean Gaussian Process (GP) prior, that is, a Gaussian Process (GP) with mean function 0 and a quasi-periodic covariance function representative of rotating active regions:\n$$\nk_{\\mathrm{QP}}(t, t') = A^{2} \\exp\\!\\left(-\\frac{(t - t')^{2}}{2\\lambda^{2}} - \\frac{\\sin^{2}\\!\\big(\\pi (t - t') / P\\big)}{2w^{2}}\\right),\n$$\nwhere $A > 0$ is the amplitude, $\\lambda > 0$ is the evolutionary timescale of active regions, $P > 0$ is the stellar rotation period, and $w > 0$ controls coherence of the periodic component. Let $K_{oo} \\in \\mathbb{R}^{N_{o} \\times N_{o}}$, $K_{mm} \\in \\mathbb{R}^{N_{m} \\times N_{m}}$, and $K_{mo} \\in \\mathbb{R}^{N_{m} \\times N_{o}}$ be the covariance matrices formed by evaluating $k_{\\mathrm{QP}}$ on $(t_{o}, t_{o})$, $(t_{m}, t_{m})$, and $(t_{m}, t_{o})$, respectively.\n\nStarting from the definition of a Gaussian Process prior and the standard construction of a multivariate normal distribution over finite collections of function values, derive the conditional distribution of the latent activity values $f_{m} = \\big(f(t_{m,1}), \\dots, f(t_{m,N_{m}})\\big)^{\\top}$ given the noisy observations $y_{o}$. Explain each step, including the incorporation of measurement noise, the formation of the joint Gaussian distribution over $(f_{m}, y_{o})$, and the conditioning to obtain the posterior predictive distribution for $f_{m}$.\n\nThen, within the same framework, propose a scientifically sound strategy to impute the unobserved segments for visualization that does not bias parameter inference for exoplanet signals, and justify why the strategy avoids double-counting data or altering the likelihood used for inference.\n\nProvide the closed-form analytic expression for the conditional mean vector $\\mu_{m} = \\mathbb{E}[f_{m} \\mid y_{o}]$ in terms of $K_{oo}$, $K_{mo}$, and $\\sigma_{n}^{2}$. Your final answer must be a single analytic expression. No numerical approximation is required or permitted.",
            "solution": "The problem requires the derivation of the posterior predictive distribution for a set of unobserved stellar activity values, $f_m$, given a set of noisy observations, $y_o$, within a Gaussian Process (GP) framework. A scientifically sound strategy for imputing these unobserved values for visualization must also be proposed and justified.\n\nLet us begin by formalizing the model. The stellar activity signal, $f(t)$, is modeled as a zero-mean Gaussian Process, denoted as $f(t) \\sim \\mathcal{GP}(0, k_{\\mathrm{QP}}(t, t'))$. This implies that any finite collection of function values is jointly Gaussian. We are interested in the latent function values at the observation times $t_o$, which we denote by the vector $f_o \\in \\mathbb{R}^{N_o}$, and the latent function values at the unobserved times $t_m$, denoted by the vector $f_m \\in \\mathbb{R}^{N_m}$.\n\nAccording to the definition of a Gaussian Process, the joint distribution of the latent function values $(f_o, f_m)$ is a multivariate normal distribution with a mean of zero and a block covariance matrix constructed from the kernel function $k_{\\mathrm{QP}}$:\n$$\n\\begin{pmatrix} f_o \\\\ f_m \\end{pmatrix} \\sim \\mathcal{N} \\! \\left( \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} K_{oo} & K_{om} \\\\ K_{mo} & K_{mm} \\end{pmatrix} \\right)\n$$\nHere, the covariance matrices are defined by their elements: $(K_{oo})_{ij} = k_{\\mathrm{QP}}(t_{o,i}, t_{o,j})$, $(K_{mm})_{ij} = k_{\\mathrm{QP}}(t_{m,i}, t_{m,j})$, and $(K_{mo})_{ij} = k_{\\mathrm{QP}}(t_{m,i}, t_{o,j})$. The matrix $K_{om}$ is the transpose of $K_{mo}$, i.e., $K_{om} = K_{mo}^{\\top}$.\n\nThe observations $y_o$ are not direct measurements of $f_o$; they are corrupted by independent and identically distributed Gaussian noise with variance $\\sigma_n^2$. The measurement model is:\n$$\ny_o = f_o + \\epsilon\n$$\nwhere $\\epsilon \\sim \\mathcal{N}(0, \\sigma_n^2 I_{N_o})$, with $I_{N_o}$ being the $N_o \\times N_o$ identity matrix. The noise $\\epsilon$ is assumed to be independent of the stellar activity signal $f(t)$.\n\nTo find the conditional distribution $p(f_m | y_o)$, we first need the joint distribution of $(f_m, y_o)$. This is a multivariate normal distribution because both $f_m$ and $y_o$ are linear combinations of Gaussian random variables. The mean of this joint distribution is zero:\n$$\n\\mathbb{E} \\begin{pmatrix} f_m \\\\ y_o \\end{pmatrix} = \\begin{pmatrix} \\mathbb{E}[f_m] \\\\ \\mathbb{E}[y_o] \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ \\mathbb{E}[f_o + \\epsilon] \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ \\mathbb{E}[f_o] + \\mathbb{E}[\\epsilon] \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nThe covariance matrix of the joint distribution requires computing three block components:\n1.  The auto-covariance of $f_m$:\n    $$\n    \\text{Cov}(f_m, f_m) = \\mathbb{E}[f_m f_m^{\\top}] - \\mathbb{E}[f_m]\\mathbb{E}[f_m]^{\\top} = K_{mm}\n    $$\n2.  The auto-covariance of $y_o$:\n    $$\n    \\text{Cov}(y_o, y_o) = \\mathbb{E}[y_o y_o^{\\top}] = \\mathbb{E}[(f_o + \\epsilon)(f_o + \\epsilon)^{\\top}] = \\mathbb{E}[f_o f_o^{\\top} + f_o \\epsilon^{\\top} + \\epsilon f_o^{\\top} + \\epsilon \\epsilon^{\\top}]\n    $$\n    Due to the independence of $f_o$ and $\\epsilon$, the cross-terms have zero expectation, $\\mathbb{E}[f_o \\epsilon^{\\top}] = \\mathbb{E}[f_o]\\mathbb{E}[\\epsilon]^{\\top} = 0$. This simplifies the expression to:\n    $$\n    \\text{Cov}(y_o, y_o) = \\mathbb{E}[f_o f_o^{\\top}] + \\mathbb{E}[\\epsilon \\epsilon^{\\top}] = K_{oo} + \\sigma_n^2 I_{N_o}\n    $$\n3.  The cross-covariance between $f_m$ and $y_o$:\n    $$\n    \\text{Cov}(f_m, y_o) = \\mathbb{E}[f_m y_o^{\\top}] = \\mathbb{E}[f_m (f_o + \\epsilon)^{\\top}] = \\mathbb{E}[f_m f_o^{\\top}] + \\mathbb{E}[f_m \\epsilon^{\\top}]\n    $$\n    Again, due to independence, $\\mathbb{E}[f_m \\epsilon^{\\top}] = 0$, leaving:\n    $$\n    \\text{Cov}(f_m, y_o) = \\mathbb{E}[f_m f_o^{\\top}] = K_{mo}\n    $$\n    The corresponding transpose is $\\text{Cov}(y_o, f_m) = \\text{Cov}(f_m, y_o)^{\\top} = K_{mo}^{\\top}$.\n\nCombining these results, the joint distribution of $(f_m, y_o)$ is:\n$$\n\\begin{pmatrix} f_m \\\\ y_o \\end{pmatrix} \\sim \\mathcal{N} \\! \\left( \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} K_{mm} & K_{mo} \\\\ K_{mo}^{\\top} & K_{oo} + \\sigma_n^2 I_{N_o} \\end{pmatrix} \\right)\n$$\nNow we can use the standard rule for conditioning multivariate normal distributions. For a generic partitioned Gaussian vector $(x_1, x_2)$ with mean $(\\mu_1, \\mu_2)$ and covariance matrix $\\begin{pmatrix} \\Sigma_{11} & \\Sigma_{12} \\\\ \\Sigma_{21} & \\Sigma_{22} \\end{pmatrix}$, the conditional distribution of $x_1$ given $x_2=a$ is Gaussian, $p(x_1 | x_2=a) = \\mathcal{N}(\\mu_{1|2}, \\Sigma_{1|2})$, with:\n$$\n\\mu_{1|2} = \\mu_1 + \\Sigma_{12} \\Sigma_{22}^{-1} (a - \\mu_2)\n$$\n$$\n\\Sigma_{1|2} = \\Sigma_{11} - \\Sigma_{12} \\Sigma_{22}^{-1} \\Sigma_{21}\n$$\nBy identifying $x_1 \\equiv f_m$, $x_2 \\equiv y_o$, $\\mu_1 = 0$, $\\mu_2 = 0$, $\\Sigma_{11} = K_{mm}$, $\\Sigma_{12} = K_{mo}$, $\\Sigma_{21} = K_{mo}^{\\top}$, and $\\Sigma_{22} = K_{oo} + \\sigma_n^2 I_{N_o}$, we obtain the conditional distribution of $f_m$ given the observations $y_o$. The conditional mean, $\\mu_m = \\mathbb{E}[f_m | y_o]$, is:\n$$\n\\mu_m = 0 + K_{mo} (K_{oo} + \\sigma_n^2 I_{N_o})^{-1} (y_o - 0) = K_{mo} (K_{oo} + \\sigma_n^2 I_{N_o})^{-1} y_o\n$$\nThe conditional covariance is:\n$$\n\\text{Cov}(f_m | y_o) = K_{mm} - K_{mo} (K_{oo} + \\sigma_n^2 I_{N_o})^{-1} K_{mo}^{\\top}\n$$\nThe problem asks for the closed-form expression for the conditional mean vector $\\mu_m$, which we have derived.\n\nFor the second part of the problem, we propose a strategy to impute the unobserved segments for visualization. The correct strategy is to use the posterior predictive distribution derived above. Specifically, one should:\n1.  First, infer the GP hyperparameters ($A$, $\\lambda$, $P$, $w$), along with any parameters of an exoplanet model if present. This inference step is performed by maximizing the marginal log-likelihood of the *observed data* $y_o$. The marginal likelihood is obtained by integrating out the latent function $f_o$ and is given by the probability density of the Gaussian distribution $y_o \\sim \\mathcal{N}(0, K_{oo} + \\sigma_n^2 I_{N_o})$. Thus, the log-likelihood function is:\n    $$\n    \\log p(y_o | A, \\lambda, P, w, \\sigma_n^2) = -\\frac{1}{2} y_o^{\\top} (K_{oo} + \\sigma_n^2 I_{N_o})^{-1} y_o - \\frac{1}{2} \\log \\det(K_{oo} + \\sigma_n^2 I_{N_o}) - \\frac{N_o}{2} \\log(2\\pi)\n    $$\n    (We assume for simplicity that the exoplanet signal has been accounted for, e.g., by joint modeling, which does not change the core logic here.)\n2.  After obtaining the optimal (e.g., maximum likelihood or maximum a posteriori) hyperparameters, $\\hat{\\theta} = (\\hat{A}, \\hat{\\lambda}, \\hat{P}, \\hat{w})$, these values are fixed and used to construct the covariance matrices $K_{oo}$, $K_{mm}$, and $K_{mo}$.\n3.  The unobserved segments are then imputed by plotting the posterior predictive mean $\\mu_m = \\mathbb{E}[f_m | y_o, \\hat{\\theta}]$ as the most probable signal. Optionally, credible intervals can be constructed from the diagonal of the posterior covariance matrix, $\\text{Cov}(f_m | y_o, \\hat{\\theta})$, to visualize the uncertainty of the imputation.\n\nThis strategy is scientifically sound and avoids biasing inference for the following reasons:\n-   **No Alteration of Likelihood**: The imputation is a *post-processing* step. Parameter inference is based exclusively on the likelihood of the observed data, $p(y_o | \\theta)$. The imputed values $f_m$ are not used as data and therefore do not enter the likelihood function.\n-   **No Double-Counting**: The imputed values are predictions from the model that has already been fitted to the observed data. They are consequences of the inference, not inputs to it. Using them for visualization is simply a way to inspect what the fitted model implies about the unobserved periods. This does not constitute feeding data back into the model, which would be an instance of double-counting.\n\nThis procedure correctly separates the process of model fitting (inference on parameters using $y_o$) from the process of prediction (calculating the expected signal $f_m$ given the fitted model and $y_o$), ensuring that the visualization does not corrupt the statistical integrity of the parameter estimation for any component of the model, including exoplanet signals.\n\nThe final required expression is the conditional mean vector. Denoting the identity matrix as $I$, this is:\n$$\n\\mu_m = K_{mo} (K_{oo} + \\sigma_n^2 I)^{-1} y_o\n$$",
            "answer": "$$\n\\boxed{K_{mo} (K_{oo} + \\sigma_{n}^{2} I)^{-1} y_{o}}\n$$"
        }
    ]
}