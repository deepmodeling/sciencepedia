{
    "hands_on_practices": [
        {
            "introduction": "Before we can interpret an observed exoplanet spectrum, we must first build a physical \"forward model\" that predicts the spectrum for a given set of atmospheric properties. This foundational exercise guides you through the derivation of the transit radius as a function of wavelength, $R(\\lambda)$, under the common simplifying assumptions of an isothermal, hydrostatic atmosphere . Mastering this derivation provides crucial intuition for how atmospheric parameters like temperature, composition, and gravity sculpt the features we observe in transmission spectra.",
            "id": "4154218",
            "problem": "An exoplanetary atmosphere is modeled for transmission spectroscopy within a Bayesian inference framework, where the forward model maps composition and thermodynamic parameters to the wavelength-dependent transit radius. Consider a spherically symmetric atmosphere that is isothermal and in hydrostatic equilibrium with constant gravitational acceleration $g$. Let the reference radius $R_{0}$ be defined at the reference pressure $P_{0}$ where the atmosphere becomes opaque at lower altitudes. Assume the ideal gas law with Boltzmann constant $k_B$, a uniform mean mass per particle $\\mu$, and a single absorbing species with constant volume mixing ratio $\\xi$ and absorption cross section $\\sigma_{\\lambda}$ that is independent of temperature and pressure. Let the stellar radius be $R_{\\star}$, and assume the thin-atmosphere limit $H \\ll R_{0}$, where $H$ is the pressure scale height under isothermal hydrostatic conditions.\n\nStarting only from hydrostatic equilibrium $dP/dr = -\\rho g$, the ideal gas law $P = n k_B T$, and the definition of the chord optical depth integral along a ray of impact parameter $b$ through the limb,\n$$\n\\tau_{\\lambda}(b) = \\int_{-\\infty}^{+\\infty} \\xi \\, \\sigma_{\\lambda} \\, n\\!\\left(\\sqrt{b^{2}+x^{2}}\\right) \\, dx,\n$$\nderive the analytic expression, valid to leading order in $H/R_{0}$, for the wavelength-dependent effective transit radius $R(\\lambda)$ defined by the occulted area condition\n$$\n\\pi R^2(\\lambda) = \\pi R_0^2 + 2 \\pi \\int_{0}^{\\infty} \\left[1 - \\exp\\left(-\\tau_{\\lambda}(R_{0}+z)\\right)\\right] (R_{0}+z) \\, dz.\n$$\nExpress the final closed-form result for $R(\\lambda)$ in terms of $R_0$, $P_0$, $T$, $\\mu$, $g$, $\\xi$, $\\sigma_{\\lambda}$, and fundamental constants, and use the atmospheric number density at the reference level $n_{0} = P_{0}/(k_B T)$. You may assume and use the thin-limb approximation that near the tangent point the radius satisfies $r \\approx b + x^{2}/(2b)$ for $|x| \\ll b$, with $b \\approx R_{0}+z$. The expression should include all leading-order terms in $H/R_{0}$, including any constants arising from the limb integration.\n\nFinally, enumerate at least four scientifically grounded limitations of the isothermal, hydrostatic, single-species chord optical depth approximation as it is used in atmospheric retrieval, discussing regimes in which the derived forward model would introduce biased inferences.\n\nProvide your final answer as a single closed-form analytic expression for $R(\\lambda)$ only. Do not include any units inside your final answer. No rounding is required.",
            "solution": "The problem is valid as it is scientifically grounded, well-posed, objective, and provides a complete and consistent setup for a standard derivation in the field of exoplanetary atmospheric science.\n\nThe derivation proceeds in three main stages: first, determining the atmospheric number density profile $n(r)$ under the given assumptions; second, calculating the chord optical depth $\\tau_{\\lambda}(b)$; and third, substituting this into the definition of the effective transit radius $R(\\lambda)$ and evaluating the integral to leading order.\n\n**1. Atmospheric Density Profile**\n\nWe begin with the equation of hydrostatic equilibrium for a spherically symmetric atmosphere with constant gravitational acceleration $g$:\n$$\n\\frac{dP}{dr} = -\\rho g\n$$\nwhere $P$ is the pressure, $r$ is the radial distance from the planet's center, and $\\rho$ is the mass density. The density is related to the number density $n$ and the mean mass per particle $\\mu$ by $\\rho = n\\mu$. Substituting this gives:\n$$\n\\frac{dP}{dr} = -n\\mu g\n$$\nThe atmosphere is assumed to follow the ideal gas law, $P = n k_B T$, where $k_B$ is the Boltzmann constant and $T$ is the constant temperature. We can express the number density as $n = P/(k_B T)$. Substituting this into the hydrostatic equilibrium equation yields:\n$$\n\\frac{dP}{dr} = -\\frac{P}{k_B T}\\mu g\n$$\nThis is a first-order linear ordinary differential equation for $P(r)$. We can rearrange it to separate variables:\n$$\n\\frac{dP}{P} = -\\frac{\\mu g}{k_B T} dr\n$$\nWe define the pressure scale height $H$ as the characteristic vertical length scale over which the pressure changes by a factor of $e$. For an isothermal atmosphere, it is a constant:\n$$\nH = \\frac{k_B T}{\\mu g}\n$$\nSubstituting $H$ into the differential equation gives:\n$$\n\\frac{dP}{P} = -\\frac{1}{H} dr\n$$\nWe integrate this equation from the reference level $(R_0, P_0)$ to an arbitrary level $(r, P(r))$:\n$$\n\\int_{P_0}^{P(r)} \\frac{dP'}{P'} = \\int_{R_0}^{r} -\\frac{1}{H} dr'\n$$\n$$\n\\ln\\left(\\frac{P(r)}{P_0}\\right) = -\\frac{r-R_0}{H}\n$$\nSolving for $P(r)$, we find the pressure profile:\n$$\nP(r) = P_0 \\exp\\left(-\\frac{r-R_0}{H}\\right)\n$$\nSince $n = P/(k_B T)$ and $T$ is constant, the number density profile follows the same exponential form:\n$$\nn(r) = \\frac{P_0}{k_B T} \\exp\\left(-\\frac{r-R_0}{H}\\right) = n_0 \\exp\\left(-\\frac{r-R_0}{H}\\right)\n$$\nwhere $n_0 = P_0/(k_B T)$ is the number density at the reference radius $R_0$.\n\n**2. Chord Optical Depth**\n\nThe chord optical depth $\\tau_{\\lambda}(b)$ through the atmosphere limb along a ray with impact parameter $b$ is given by the integral:\n$$\n\\tau_{\\lambda}(b) = \\int_{-\\infty}^{+\\infty} \\xi \\, \\sigma_{\\lambda} \\, n(r) \\, dx\n$$\nHere, $x$ is the coordinate along the ray, with $x=0$ at the point of closest approach. The radial distance $r$ is related to $b$ and $x$ by $r = \\sqrt{b^2 + x^2}$. Substituting the density profile into the integral:\n$$\n\\tau_{\\lambda}(b) = \\int_{-\\infty}^{+\\infty} \\xi \\, \\sigma_{\\lambda} \\, n_0 \\exp\\left(-\\frac{\\sqrt{b^2 + x^2} - R_0}{H}\\right) \\, dx\n$$\nThe problem specifies using the thin-limb approximation for the radial distance near the tangent point, valid for $|x| \\ll b$:\n$$\nr = \\sqrt{b^2 + x^2} = b\\sqrt{1 + (x/b)^2} \\approx b\\left(1 + \\frac{x^2}{2b^2}\\right) = b + \\frac{x^2}{2b}\n$$\nThis approximation is valid because the exponential term in the integrand decreases rapidly away from $x=0$ due to the small scale height $H$, so the main contribution to the integral comes from the region where $|x|$ is small. Substituting this approximation for $r$:\n$$\n\\tau_{\\lambda}(b) \\approx \\int_{-\\infty}^{+\\infty} \\xi \\, \\sigma_{\\lambda} \\, n_0 \\exp\\left(-\\frac{b + x^2/(2b) - R_0}{H}\\right) \\, dx\n$$\nWe can separate the terms in the exponent that do not depend on $x$:\n$$\n\\tau_{\\lambda}(b) \\approx \\xi \\, \\sigma_{\\lambda} \\, n_0 \\exp\\left(-\\frac{b - R_0}{H}\\right) \\int_{-\\infty}^{+\\infty} \\exp\\left(-\\frac{x^2}{2bH}\\right) \\, dx\n$$\nThe remaining integral is a standard Gaussian integral of the form $\\int_{-\\infty}^{+\\infty} \\exp(-ax^2)dx = \\sqrt{\\pi/a}$. In our case, $a = 1/(2bH)$, so the integral evaluates to $\\sqrt{2\\pi bH}$. This gives the chord optical depth as:\n$$\n\\tau_{\\lambda}(b) \\approx \\xi \\, \\sigma_{\\lambda} \\, n_0 \\sqrt{2\\pi bH} \\exp\\left(-\\frac{b - R_0}{H}\\right)\n$$\n\n**3. Effective Transit Radius**\n\nThe effective transit radius $R(\\lambda)$ is defined by the total occulted area:\n$$\n\\pi R^2(\\lambda) = \\pi R_0^2 + 2 \\pi \\int_{0}^{\\infty} \\left[1 - \\exp(-\\tau_{\\lambda}(R_{0}+z))\\right] (R_{0}+z) \\, dz\n$$\nHere, the integration variable is the altitude $z$ above the reference opaque level $R_0$, and the impact parameter is $b = R_0 + z$. We first substitute our expression for $\\tau_{\\lambda}(b)$ with $b=R_0+z$:\n$$\n\\tau_{\\lambda}(R_0+z) \\approx \\xi \\sigma_{\\lambda} n_0 \\sqrt{2\\pi (R_0+z)H} \\exp\\left(-\\frac{(R_0+z)-R_0}{H}\\right) = \\xi \\sigma_{\\lambda} n_0 \\sqrt{2\\pi (R_0+z)H} \\exp\\left(-\\frac{z}{H}\\right)\n$$\nIn the thin-atmosphere limit ($H \\ll R_0$), the integrand is significant only for small values of $z/R_0$. We can therefore make the leading-order approximation that $(R_0+z) \\approx R_0$ within the terms that vary slowly with $z$. This applies to the factor $(R_0+z)$ in the integral and the $\\sqrt{R_0+z}$ term within the optical depth expression. This yields:\n$$\n\\pi (R^2(\\lambda) - R_0^2) \\approx 2\\pi R_0 \\int_{0}^{\\infty} \\left[1 - \\exp\\left(-\\left(\\xi \\sigma_{\\lambda} n_0 \\sqrt{2\\pi R_0 H}\\right) \\exp(-z/H)\\right)\\right] dz\n$$\nLet's define the optical depth at the reference altitude level $z=0$ (grazing $R_0$) as $\\tau_0(\\lambda)$:\n$$\n\\tau_0(\\lambda) = \\xi \\sigma_{\\lambda} n_0 \\sqrt{2\\pi R_0 H}\n$$\nThe integral becomes:\n$$\nR^2(\\lambda) - R_0^2 \\approx 2R_0 \\int_{0}^{\\infty} \\left[1 - \\exp\\left(-\\tau_0(\\lambda) \\exp(-z/H)\\right)\\right] dz\n$$\nTo solve this integral, we perform a substitution. Let $u = \\tau_0(\\lambda) \\exp(-z/H)$. Then $du = -\\frac{1}{H} \\tau_0(\\lambda) \\exp(-z/H) dz = -\\frac{u}{H} dz$, which means $dz = -\\frac{H}{u} du$. The limits of integration change from $z \\in [0, \\infty)$ to $u \\in [\\tau_0(\\lambda), 0]$.\n$$\n\\int_{0}^{\\infty} \\dots dz = \\int_{\\tau_0(\\lambda)}^{0} [1 - \\exp(-u)] \\left(-\\frac{H}{u}\\right) du = H \\int_{0}^{\\tau_0(\\lambda)} \\frac{1-\\exp(-u)}{u} du\n$$\nThis integral defines the exponential integral function $\\text{Ein}(x) = \\int_0^x \\frac{1-e^{-t}}{t} dt$. For large arguments, which is a reasonable assumption for the optical depths that define the transit radius, this function has the asymptotic expansion $\\text{Ein}(x) \\approx \\gamma + \\ln(x)$, where $\\gamma \\approx 0.5772$ is the Euler-Mascheroni constant. This constant arises from the integral and must be included.\nThus, the integral evaluates to $H(\\gamma + \\ln(\\tau_0(\\lambda)))$.\nSubstituting this back into the expression for $R^2(\\lambda)$:\n$$\nR^2(\\lambda) - R_0^2 \\approx 2R_0 H (\\gamma + \\ln(\\tau_0(\\lambda)))\n$$\n$$\nR^2(\\lambda) \\approx R_0^2 \\left[1 + \\frac{2H}{R_0}(\\gamma + \\ln(\\tau_0(\\lambda)))\\right]\n$$\nTaking the square root and using the first-order binomial approximation $(1+x)^{1/2} \\approx 1 + x/2$ for small $x = \\frac{2H}{R_0}(\\dots)$, which is valid in the thin-atmosphere limit $H \\ll R_0$:\n$$\nR(\\lambda) \\approx R_0 \\left[1 + \\frac{1}{2} \\frac{2H}{R_0}(\\gamma + \\ln(\\tau_0(\\lambda)))\\right] = R_0 + H(\\gamma + \\ln(\\tau_0(\\lambda)))\n$$\nFinally, we substitute the expressions for $H$, $n_0$, and $\\tau_0(\\lambda)$ to obtain the final closed-form result for $R(\\lambda)$:\n$$\nR(\\lambda) = R_0 + H \\left(\\gamma + \\ln\\left(\\xi \\sigma_{\\lambda} n_0 \\sqrt{2\\pi R_0 H}\\right)\\right)\n$$\n$$\nR(\\lambda) = R_0 + \\frac{k_B T}{\\mu g} \\left(\\gamma + \\ln\\left(\\xi \\sigma_{\\lambda} \\frac{P_0}{k_B T} \\sqrt{2\\pi R_0 \\frac{k_B T}{\\mu g}}\\right)\\right)\n$$\nThis expression provides the wavelength-dependent effective transit radius in terms of the specified physical parameters and fundamental constants.\n\n**Limitations of the Model**\n\nThe derived forward model, while analytically convenient, relies on several strong simplifications. Failure to meet these assumptions in a real planetary atmosphere can introduce significant biases in the parameters retrieved from observational data. Four prominent limitations are:\n\n1.  **Isothermal Assumption:** Real planetary atmospheres exhibit temperature-pressure profiles that vary with altitude due to radiative heating/cooling and dynamics. An isothermal model assigns a single temperature $T$ to the entire atmosphere, causing the scale height $H$ to be constant. In reality, $H$ is a function of altitude, $H(r)$. For example, a stratosphere with a temperature inversion would make the atmosphere expand more at higher altitudes than an isothermal model predicts. Fitting data from such an atmosphere with an isothermal model would lead to biased retrievals, potentially overestimating absorber abundances or mischaracterizing the reference pressure level.\n\n2.  **Constant Volume Mixing Ratio ($\\xi$):** The model assumes the absorbing species is uniformly mixed throughout the atmosphere. In reality, mixing ratios change with altitude due to processes like photochemistry (e.g., dissociation of molecules by stellar UV radiation at high altitudes), condensation (e.g., formation of water ice clouds at a cold trap), and diffusive separation (where lighter species become enhanced at the top of the atmosphere). Using a constant-$\\xi$ model for an atmosphere with a vertically varying composition will result in a retrieved abundance that is a complex, altitude-weighted average that may not be representative of any specific atmospheric layer, biasing our understanding of the planet's atmospheric chemistry.\n\n3.  **Pressure and Temperature Independent Cross-Sections ($\\sigma_{\\lambda}$):** The absorption cross-section of a molecule is not constant; it depends on the local temperature and pressure. These effects broaden the spectral lines, altering their shape. Pressure (or collisional) broadening dominates in the lower, denser regions of the atmosphere, while thermal (Doppler) broadening is more significant in the upper, hotter, and more rarefied regions. By ignoring line broadening, the model misrepresents the shape of spectral features. This can lead to significant errors in the retrieved temperature profile and abundances, as the model cannot correctly interpret the information contained in the wings of strong absorption lines.\n\n4.  **Neglect of Clouds and Hazes:** The model only accounts for gaseous absorption. Real exoplanetary atmospheres frequently contain aerosols, such as clouds (condensates) or photochemical hazes. These particles provide a significant source of opacity, which is typically a smooth function of wavelength and can obscure the gaseous absorption features below a certain altitude. Neglecting aerosols when they are present forces the retrieval model to misinterpret the muted spectral features, often leading to a severe underestimation of gaseous abundances and a retrieved temperature profile that is erroneously flattened (more isothermal). This is a well-known degeneracy in atmospheric retrieval.",
            "answer": "$$\\boxed{R_0 + \\frac{k_B T}{\\mu g} \\left(\\gamma + \\ln\\left(\\xi \\sigma_{\\lambda} \\frac{P_0}{k_B T} \\sqrt{2\\pi R_0 \\frac{k_B T}{\\mu g}}\\right)\\right)}$$"
        },
        {
            "introduction": "The core of atmospheric retrieval lies in solving the inverse problem: inferring physical parameters from noisy data. This practice moves from theory to application, demonstrating how to combine information from multiple instruments to produce a joint posterior distribution for shared atmospheric parameters and instrument-specific nuisance parameters . By implementing the Bayesian update for a linear-Gaussian model, you will gain hands-on experience with the mathematical machinery that underpins modern retrieval codes and learn how to formally fuse diverse datasets.",
            "id": "4154226",
            "problem": "Consider a simplified transmission spectroscopy atmospheric retrieval for an exoplanet, in which an observer collects two independent datasets from two instruments with distinct noise covariances. The signal in each dataset is modeled by a linearized forward model around a reference atmospheric state, consistent with standard first-order sensitivity analysis in radiative transfer. Let the shared atmospheric state vector be $\\mathbf{x} \\in \\mathbb{R}^2$ with components $\\mathbf{x} = [T,\\ \\ln q]^\\top$, where $T$ is temperature in kelvin and $\\ln q$ is the natural logarithm of the volume mixing ratio (dimensionless). Each instrument $i \\in \\{1,2\\}$ has an instrument-specific nuisance parameter $w_i \\in \\mathbb{R}$ representing a scalar baseline offset (dimensionless) that is constant across its spectral channels. For dataset $i$, suppose the observation vector $\\mathbf{y}_i \\in \\mathbb{R}^{m_i}$ is related to the parameters by\n$$\n\\mathbf{y}_i = \\mathbf{J}_i \\mathbf{x} + \\mathbf{L}_i w_i + \\boldsymbol{\\epsilon}_i,\n$$\nwhere $\\mathbf{J}_i \\in \\mathbb{R}^{m_i \\times 2}$ is the linearized Jacobian of the transit depth with respect to the atmospheric parameters $(T,\\ \\ln q)$, $\\mathbf{L}_i \\in \\mathbb{R}^{m_i \\times 1}$ maps the scalar offset to all channels (a column of ones), and $\\boldsymbol{\\epsilon}_i \\sim \\mathcal{N}(\\mathbf{0}, \\boldsymbol{\\Sigma}_i)$ is Gaussian noise with known covariance $\\boldsymbol{\\Sigma}_i$. Assume the datasets are conditionally independent given the parameters, so the joint likelihood factorizes.\n\nAdopt a Gaussian prior on all parameters,\n$$\n\\mathbf{x} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_x,\\ \\boldsymbol{\\Sigma}_x), \\quad w_1 \\sim \\mathcal{N}(\\mu_{w_1},\\ \\sigma_{w_1}^2), \\quad w_2 \\sim \\mathcal{N}(\\mu_{w_2},\\ \\sigma_{w_2}^2),\n$$\nwith block-diagonal joint prior covariance and concatenated prior mean for the parameter vector $\\boldsymbol{\\theta} = [T,\\ \\ln q,\\ w_1,\\ w_2]^\\top \\in \\mathbb{R}^4$ given by\n$$\n\\boldsymbol{\\mu}_0 = \\begin{bmatrix}\\boldsymbol{\\mu}_x \\\\ \\mu_{w_1} \\\\ \\mu_{w_2}\\end{bmatrix}, \\quad \\mathbf{S}_0 = \\mathrm{diag}(\\boldsymbol{\\Sigma}_x,\\ \\sigma_{w_1}^2,\\ \\sigma_{w_2}^2).\n$$\nLet the concatenated observation vector be $\\mathbf{y} = [\\mathbf{y}_1^\\top,\\ \\mathbf{y}_2^\\top]^\\top \\in \\mathbb{R}^{m_1+m_2}$. The global design matrix that maps $\\boldsymbol{\\theta}$ to $\\mathbf{y}$ is\n$$\n\\mathbf{H} \\in \\mathbb{R}^{(m_1+m_2)\\times 4}, \\quad\n\\mathbf{H} = \\begin{bmatrix}\n\\mathbf{J}_1 & \\mathbf{L}_1 & \\mathbf{0}_{m_1\\times 1} \\\\\n\\mathbf{J}_2 & \\mathbf{0}_{m_2\\times 1} & \\mathbf{L}_2\n\\end{bmatrix}.\n$$\nThe joint noise covariance is block-diagonal,\n$$\n\\boldsymbol{\\Sigma} = \\begin{bmatrix}\n\\boldsymbol{\\Sigma}_1 & \\mathbf{0} \\\\\n\\mathbf{0} & \\boldsymbol{\\Sigma}_2\n\\end{bmatrix}.\n$$\nUnder these assumptions, the joint posterior for $\\boldsymbol{\\theta}$ is Gaussian. Your task is to compute the joint posterior mean and covariance, and then report the marginal constraints (posterior mean and standard deviation) for each component of $\\boldsymbol{\\theta}$.\n\nFundamental base to be used:\n- Bayes’ rule with Gaussian prior and Gaussian likelihood, yielding a Gaussian posterior for linear models.\n- Properties of multivariate normal distributions, particularly the information form and conjugacy.\n\nMathematical objective:\n- Derive and implement the posterior precision and mean for the linear-Gaussian model,\n$$\n\\mathbf{J}_{\\text{post}} = \\mathbf{S}_0^{-1} + \\mathbf{H}^\\top \\boldsymbol{\\Sigma}^{-1} \\mathbf{H}, \\quad\n\\boldsymbol{\\mu}_{\\text{post}} = \\mathbf{J}_{\\text{post}}^{-1}\\left(\\mathbf{S}_0^{-1}\\boldsymbol{\\mu}_0 + \\mathbf{H}^\\top \\boldsymbol{\\Sigma}^{-1}\\mathbf{y}\\right),\n$$\nand extract marginal variances from $\\mathbf{J}_{\\text{post}}^{-1}$.\n\nUnits and reporting requirements:\n- Report the posterior mean and standard deviation for $T$ in kelvin.\n- Report the posterior mean and standard deviation for $\\ln q$ as dimensionless values.\n- Report the posterior mean and standard deviation for $w_1$ and $w_2$ as dimensionless values.\n- All reported quantities must be floats. No angle units are involved. No percentages are involved.\n\nTest suite and parameter specification:\nUse the following three test cases, each with $m_1 = 3$ channels for dataset $1$ and $m_2 = 4$ channels for dataset $2$. In all cases, $\\mathbf{L}_1$ and $\\mathbf{L}_2$ are columns of ones of lengths $m_1$ and $m_2$, respectively.\n\nShared prior for all cases:\n- $\\boldsymbol{\\mu}_x = [1000,\\ -5]^\\top$ with units $[\\,\\mathrm{K},\\ \\text{dimensionless}\\,]$.\n- $\\boldsymbol{\\Sigma}_x = \\mathrm{diag}([100^2,\\ 1^2])$.\n- $\\mu_{w_1} = 0$ with $\\sigma_{w_1} = 0.01$.\n- $\\mu_{w_2} = 0$ with $\\sigma_{w_2} = 0.02$.\n\nCase $1$ (balanced noise with moderate correlation in dataset $2$):\n- $\\mathbf{y}_1 = [0.01225,\\ 0.01080,\\ 0.00950]^\\top$.\n- $\\mathbf{y}_2 = [0.01360,\\ 0.01310,\\ 0.00990,\\ 0.00920]^\\top$.\n- $\\mathbf{J}_1 = \\begin{bmatrix}\n1.2\\times 10^{-5} & 6.0\\times 10^{-4} \\\\\n1.0\\times 10^{-5} & 5.0\\times 10^{-4} \\\\\n0.8\\times 10^{-5} & 4.0\\times 10^{-4}\n\\end{bmatrix}$.\n- $\\mathbf{J}_2 = \\begin{bmatrix}\n1.5\\times 10^{-5} & 7.0\\times 10^{-4} \\\\\n1.4\\times 10^{-5} & 6.0\\times 10^{-4} \\\\\n1.0\\times 10^{-5} & 5.0\\times 10^{-4} \\\\\n0.9\\times 10^{-5} & 4.5\\times 10^{-4}\n\\end{bmatrix}$.\n- $\\boldsymbol{\\Sigma}_1 = \\mathrm{diag}([1.0\\times 10^{-6},\\ 2.0\\times 10^{-6},\\ 1.5\\times 10^{-6}])$.\n- $\\boldsymbol{\\Sigma}_2$ constructed from variances $[1.2\\times 10^{-6},\\ 1.2\\times 10^{-6},\\ 1.5\\times 10^{-6},\\ 1.5\\times 10^{-6}]$ and correlation coefficient $\\rho = 0.6$ via\n$$\n(\\boldsymbol{\\Sigma}_2)_{ij} = \\rho^{|i-j|}\\,\\sqrt{v_i v_j}, \\quad \\text{with } v_i \\text{ the specified variances}.\n$$\n\nCase $2$ (dataset $2$ has high noise with weak correlation):\n- $\\mathbf{y}_1 = [0.01200,\\ 0.01070,\\ 0.00940]^\\top$.\n- $\\mathbf{y}_2 = [0.01380,\\ 0.01300,\\ 0.00970,\\ 0.00910]^\\top$.\n- $\\mathbf{J}_1$ and $\\mathbf{J}_2$ identical to Case $1$.\n- $\\boldsymbol{\\Sigma}_1 = \\mathrm{diag}([1.0\\times 10^{-6},\\ 2.0\\times 10^{-6},\\ 1.5\\times 10^{-6}])$.\n- $\\boldsymbol{\\Sigma}_2$ constructed from variances $[4.0\\times 10^{-6},\\ 4.0\\times 10^{-6},\\ 3.5\\times 10^{-6},\\ 3.5\\times 10^{-6}]$ and correlation coefficient $\\rho = 0.3$ via the same formula.\n\nCase $3$ (dataset $2$ strongly correlated and low noise; dataset $1$ low noise):\n- $\\mathbf{y}_1 = [0.01230,\\ 0.01060,\\ 0.00960]^\\top$.\n- $\\mathbf{y}_2 = [0.01350,\\ 0.01320,\\ 0.00980,\\ 0.00930]^\\top$.\n- $\\mathbf{J}_1$ and $\\mathbf{J}_2$ identical to Case $1$.\n- $\\boldsymbol{\\Sigma}_1 = \\mathrm{diag}([1.0\\times 10^{-6},\\ 1.0\\times 10^{-6},\\ 1.0\\times 10^{-6}])$.\n- $\\boldsymbol{\\Sigma}_2$ constructed from variances $[1.0\\times 10^{-6},\\ 1.0\\times 10^{-6},\\ 1.0\\times 10^{-6},\\ 1.0\\times 10^{-6}]$ and correlation coefficient $\\rho = 0.95$ via the same formula.\n\nImplementation instructions:\n- Build $\\mathbf{H}$ and $\\boldsymbol{\\Sigma}$ for each case exactly as specified.\n- Compute $\\mathbf{J}_{\\text{post}}$ and $\\boldsymbol{\\mu}_{\\text{post}}$ using the information form. Compute the posterior covariance $\\mathbf{C}_{\\text{post}} = \\mathbf{J}_{\\text{post}}^{-1}$.\n- Extract the marginal posterior mean and standard deviation for each parameter in the order $[T,\\ \\ln q,\\ w_1,\\ w_2]$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results for all three cases, with each case's result formatted as a list.\n- The result for each test case must be a flat list containing eight values: the posterior mean followed by the posterior standard deviation for each of the four parameters, in the order $[T,\\ \\ln q,\\ w_1,\\ w_2]$.\n- Therefore, the final output must look like `[[mean_T, std_T, mean_lnq, std_lnq, mean_w1, std_w1, mean_w2, std_w2], [case 2 results], ...]`\nNo additional text should be printed.",
            "solution": "The problem requires the computation of the posterior distribution for a set of parameters in a simplified atmospheric retrieval model. The model is linear and the noise is assumed to be Gaussian, and the prior distribution on the parameters is also Gaussian. This setup is a classic example of Bayesian linear regression, for which the posterior distribution is also Gaussian and can be derived analytically.\n\nThe overall model combines two datasets and can be expressed in a compact, global form. Let the full parameter vector be $\\boldsymbol{\\theta} = [T, \\ln q, w_1, w_2]^\\top \\in \\mathbb{R}^4$. The concatenated observation vector $\\mathbf{y} = [\\mathbf{y}_1^\\top, \\mathbf{y}_2^\\top]^\\top \\in \\mathbb{R}^{m_1+m_2}$ is related to the parameters via the linear model:\n$$\n\\mathbf{y} = \\mathbf{H}\\boldsymbol{\\theta} + \\boldsymbol{\\epsilon}\n$$\nwhere $\\mathbf{H}$ is the global design matrix that maps the parameters to the observations, and $\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\boldsymbol{\\Sigma})$ represents the concatenated Gaussian noise with a block-diagonal covariance matrix $\\boldsymbol{\\Sigma}$.\n\nBayes' rule states that the posterior probability density is proportional to the product of the likelihood and the prior:\n$$\np(\\boldsymbol{\\theta} | \\mathbf{y}) \\propto p(\\mathbf{y} | \\boldsymbol{\\theta}) p(\\boldsymbol{\\theta})\n$$\nIn this problem, both the likelihood, derived from the linear model with Gaussian noise, and the prior are multivariate normal distributions.\nThe likelihood is $p(\\mathbf{y} | \\boldsymbol{\\theta}) = \\mathcal{N}(\\mathbf{y}; \\mathbf{H}\\boldsymbol{\\theta}, \\boldsymbol{\\Sigma})$, with probability density function proportional to:\n$$\n\\exp\\left(-\\frac{1}{2}(\\mathbf{y} - \\mathbf{H}\\boldsymbol{\\theta})^\\top \\boldsymbol{\\Sigma}^{-1} (\\mathbf{y} - \\mathbf{H}\\boldsymbol{\\theta})\\right)\n$$\nThe prior is $p(\\boldsymbol{\\theta}) = \\mathcal{N}(\\boldsymbol{\\theta}; \\boldsymbol{\\mu}_0, \\mathbf{S}_0)$, with probability density function proportional to:\n$$\n\\exp\\left(-\\frac{1}{2}(\\boldsymbol{\\theta} - \\boldsymbol{\\mu}_0)^\\top \\mathbf{S}_0^{-1} (\\boldsymbol{\\theta} - \\boldsymbol{\\mu}_0)\\right)\n$$\nThe product of these two functions yields the posterior, which will also be a Gaussian, $\\mathcal{N}(\\boldsymbol{\\theta}; \\boldsymbol{\\mu}_{\\text{post}}, \\mathbf{C}_{\\text{post}})$. The argument of the exponential for the posterior is the sum of the arguments from the likelihood and prior. By expanding these quadratic forms and collecting terms, one can identify the posterior precision matrix $\\mathbf{J}_{\\text{post}} = \\mathbf{C}_{\\text{post}}^{-1}$ and the posterior mean $\\boldsymbol{\\mu}_{\\text{post}}$. The result of this well-known derivation is:\n$$\n\\mathbf{J}_{\\text{post}} = \\mathbf{S}_0^{-1} + \\mathbf{H}^\\top \\boldsymbol{\\Sigma}^{-1} \\mathbf{H}\n$$\n$$\n\\boldsymbol{\\mu}_{\\text{post}} = \\mathbf{J}_{\\text{post}}^{-1} \\left( \\mathbf{S}_0^{-1}\\boldsymbol{\\mu}_0 + \\mathbf{H}^\\top \\boldsymbol{\\Sigma}^{-1}\\mathbf{y} \\right)\n$$\nThese are the formulas provided in the problem statement, which we will implement. The quantity $\\mathbf{S}_0^{-1}$ is the prior precision matrix, and $\\mathbf{H}^\\top \\boldsymbol{\\Sigma}^{-1} \\mathbf{H}$ is the precision (or Fisher information) contributed by the data. The posterior precision is simply the sum of the prior and data precisions.\n\nThe computational procedure for each test case is as follows:\n$1$. Construct the prior mean vector $\\boldsymbol{\\mu}_0 \\in \\mathbb{R}^4$ and the prior covariance matrix $\\mathbf{S}_0 \\in \\mathbb{R}^{4 \\times 4}$. The prior covariance $\\mathbf{S}_0$ is diagonal, so its inverse, the prior precision $\\mathbf{S}_0^{-1}$, is easily found by taking the reciprocal of its diagonal elements.\n$2$. Construct the global observation vector $\\mathbf{y} \\in \\mathbb{R}^{7}$ by concatenating the observation vectors $\\mathbf{y}_1 \\in \\mathbb{R}^3$ and $\\mathbf{y}_2 \\in \\mathbb{R}^4$.\n$3$. Construct the global design matrix $\\mathbf{H} \\in \\mathbb{R}^{7 \\times 4}$ by assembling the Jacobians $\\mathbf{J}_1$ and $\\mathbf{J}_2$ and the offset mapping vectors $\\mathbf{L}_1$ and $\\mathbf{L}_2$ into the specified block structure.\n$4$. Construct the global noise covariance matrix $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{7 \\times 7}$. This is a block-diagonal matrix composed of $\\boldsymbol{\\Sigma}_1 \\in \\mathbb{R}^{3 \\times 3}$ and $\\boldsymbol{\\Sigma}_2 \\in \\mathbb{R}^{4 \\times 4}$. For each case, $\\boldsymbol{\\Sigma}_1$ is given as a diagonal matrix. The matrix $\\boldsymbol{\\Sigma}_2$ must be constructed from the given channel variances $v_i$ and correlation coefficient $\\rho$ using the formula $(\\boldsymbol{\\Sigma}_2)_{ij} = \\rho^{|i-j|}\\sqrt{v_i v_j}$.\n$5$. Compute the inverse of the noise covariance matrix, $\\boldsymbol{\\Sigma}^{-1}$. Since $\\boldsymbol{\\Sigma}$ is block-diagonal, its inverse is also block-diagonal with blocks $\\boldsymbol{\\Sigma}_1^{-1}$ and $\\boldsymbol{\\Sigma}_2^{-1}$.\n$6$. Compute the posterior precision matrix $\\mathbf{J}_{\\text{post}}$ and the posterior covariance matrix $\\mathbf{C}_{\\text{post}} = \\mathbf{J}_{\\text{post}}^{-1}$.\n$7$. Compute the posterior mean vector $\\boldsymbol{\\mu}_{\\text{post}}$.\n$8$. Extract the marginal posterior means and standard deviations. The means are the elements of $\\boldsymbol{\\mu}_{\\text{post}}$. The variances are the diagonal elements of $\\mathbf{C}_{\\text{post}}$, and the standard deviations are their square roots.\n\nThis procedure is applied to each of the three test cases, and the results are reported in the specified format. All matrix operations are performed using numerical linear algebra routines.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Bayesian atmospheric retrieval problem for the three specified test cases.\n    \"\"\"\n    # Shared prior parameters for all cases\n    mu_x = np.array([1000.0, -5.0])\n    Sigma_x = np.diag([100.0**2, 1.0**2])\n    mu_w1, sigma_w1 = 0.0, 0.01\n    mu_w2, sigma_w2 = 0.0, 0.02\n    \n    mu0 = np.array([mu_x[0], mu_x[1], mu_w1, mu_w2])\n    S0 = np.block([\n        [Sigma_x, np.zeros((2, 2))],\n        [np.zeros((2, 2)), np.diag([sigma_w1**2, sigma_w2**2])]\n    ])\n    \n    # Test cases definition\n    J1 = np.array([\n        [1.2e-5, 6.0e-4],\n        [1.0e-5, 5.0e-4],\n        [0.8e-5, 4.0e-4]\n    ])\n    \n    J2 = np.array([\n        [1.5e-5, 7.0e-4],\n        [1.4e-5, 6.0e-4],\n        [1.0e-5, 5.0e-4],\n        [0.9e-5, 4.5e-4]\n    ])\n\n    test_cases = [\n        {\n            \"y1\": np.array([0.01225, 0.01080, 0.00950]),\n            \"y2\": np.array([0.01360, 0.01310, 0.00990, 0.00920]),\n            \"Sigma1_vars\": np.array([1.0e-6, 2.0e-6, 1.5e-6]),\n            \"Sigma2_vars\": np.array([1.2e-6, 1.2e-6, 1.5e-6, 1.5e-6]),\n            \"Sigma2_rho\": 0.6\n        },\n        {\n            \"y1\": np.array([0.01200, 0.01070, 0.00940]),\n            \"y2\": np.array([0.01380, 0.01300, 0.00970, 0.00910]),\n            \"Sigma1_vars\": np.array([1.0e-6, 2.0e-6, 1.5e-6]),\n            \"Sigma2_vars\": np.array([4.0e-6, 4.0e-6, 3.5e-6, 3.5e-6]),\n            \"Sigma2_rho\": 0.3\n        },\n        {\n            \"y1\": np.array([0.01230, 0.01060, 0.00960]),\n            \"y2\": np.array([0.01350, 0.01320, 0.00980, 0.00930]),\n            \"Sigma1_vars\": np.array([1.0e-6, 1.0e-6, 1.0e-6]),\n            \"Sigma2_vars\": np.array([1.0e-6, 1.0e-6, 1.0e-6, 1.0e-6]),\n            \"Sigma2_rho\": 0.95\n        }\n    ]\n\n    def construct_sigma2(variances, rho):\n        \"\"\"Constructs the correlated covariance matrix Sigma_2.\"\"\"\n        n = len(variances)\n        stds = np.sqrt(variances)\n        sigma2 = np.zeros((n, n))\n        for i in range(n):\n            for j in range(n):\n                sigma2[i, j] = (rho**abs(i - j)) * stds[i] * stds[j]\n        return sigma2\n\n    def calculate_posterior(case_data):\n        \"\"\"\n        Calculates the posterior mean and standard deviation for a given case.\n        \"\"\"\n        m1, m2 = 3, 4\n        \n        # 1. Construct prior-related terms\n        S0_inv = np.linalg.inv(S0)\n        S0_inv_mu0 = S0_inv @ mu0\n        \n        # 2. Construct global vectors and matrices\n        y = np.concatenate((case_data[\"y1\"], case_data[\"y2\"]))\n        \n        L1 = np.ones((m1, 1))\n        L2 = np.ones((m2, 1))\n        H = np.block([\n            [J1, L1, np.zeros((m1, 1))],\n            [J2, np.zeros((m2, 1)), L2]\n        ])\n        \n        Sigma1 = np.diag(case_data[\"Sigma1_vars\"])\n        Sigma2 = construct_sigma2(case_data[\"Sigma2_vars\"], case_data[\"Sigma2_rho\"])\n        \n        Sigma = np.zeros((m1 + m2, m1 + m2))\n        Sigma[:m1, :m1] = Sigma1\n        Sigma[m1:, m1:] = Sigma2\n        \n        # 3. Perform Bayesian update\n        Sigma_inv = np.linalg.inv(Sigma)\n        H_T_Sigma_inv = H.T @ Sigma_inv\n        \n        J_post = S0_inv + H_T_Sigma_inv @ H\n        C_post = np.linalg.inv(J_post)\n        \n        mu_post = C_post @ (S0_inv_mu0 + H_T_Sigma_inv @ y)\n        \n        # 4. Extract marginals\n        posterior_stds = np.sqrt(np.diag(C_post))\n        \n        return mu_post, posterior_stds\n\n    results_strings = []\n    for case in test_cases:\n        means, stds = calculate_posterior(case)\n        case_result_values = []\n        for i in range(len(means)):\n            case_result_values.append(means[i])\n            case_result_values.append(stds[i])\n        \n        # Format the result for this case into a string \"[v1,v2,...]\"\n        case_string = f\"[{','.join(str(v) for v in case_result_values)}]\"\n        results_strings.append(case_string)\n\n    # Format the final output as a single string \"[[...],[...],[...]]\"\n    print(f\"[{','.join(results_strings)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A powerful feature of Bayesian inference is its ability to perform quantitative model comparison, allowing us to weigh the evidence for competing physical hypotheses. This exercise focuses on interpreting the output of a retrieval to decide between different atmospheric models, such as one with clouds and one without . You will learn to compute the Bayes factor, $K$, from the marginal likelihoods provided by methods like nested sampling, and to use it to assess the strength of evidence for one model over another, a critical step in drawing robust scientific conclusions.",
            "id": "4154220",
            "problem": "You are comparing two competing atmospheric cloud models in an exoplanet retrieval context, denoted by $M_A$ and $M_B$. In Bayesian model comparison, the central quantities are the marginal likelihoods (also called Bayesian evidences) $Z_A$ and $Z_B$, which are integrals of the likelihood weighted by the prior over the parameter spaces of $M_A$ and $M_B$. Nested sampling provides numerical estimates of the logarithm of these evidences, denoted $\\ln Z_A$ and $\\ln Z_B$. The Bayes factor $K$ between the models is defined through the ratio of evidences and determines posterior odds when combined with prior model probabilities. Your task is to, from first principles, determine how to compute the Bayes factor $K$ from $\\ln Z_A$ and $\\ln Z_B$ in a numerically stable manner, and to assess the strength of evidence using a calibrated scale. You must also evaluate prior sensitivity by checking whether small, scientifically plausible changes to the prior lead to changes in the strength-of-evidence classification.\n\nStarting point and definitions to be used:\n- Bayes theorem and the definition of marginal likelihood for a model $M$: $Z = \\int L(d \\mid \\theta, M)\\,\\pi(\\theta \\mid M)\\,\\mathrm{d}\\theta$, where $L$ is the likelihood of data $d$, and $\\pi$ is the prior for parameters $\\theta$ under model $M$.\n- The Bayes factor $K$ between $M_A$ and $M_B$ is the ratio of marginal likelihoods $Z_A$ and $Z_B$. You must derive how $K$ can be computed from $\\ln Z_A$ and $\\ln Z_B$ without loss of numerical precision and without intermediate overflow or underflow when $\\ln Z$ values are large in magnitude.\n\nEvidence strength calibration:\n- Use the commonly adopted Kass and Raftery calibrated scale based on $2 \\ln K$ to classify the strength of evidence for $M_A$ against $M_B$. Map the intervals of $2 \\ln K$ to integer codes $S$ as follows:\n    - If $2 \\ln K < 2$, then $S = 0$ (bare mention).\n    - If $2 \\ln K \\in [2, 6)$, then $S = 1$ (positive).\n    - If $2 \\ln K \\in [6, 10)$, then $S = 2$ (strong).\n    - If $2 \\ln K \\ge 10$, then $S = 3$ (very strong).\n\nPrior sensitivity assessment:\n- For each test case, you are provided a second set of $\\ln Z_A'$ and $\\ln Z_B'$ computed under modified priors (e.g., slightly broadened or tightened prior widths but still scientifically sensible). Determine whether the classification code $S$ changes when computed from $(\\ln Z_A', \\ln Z_B')$ instead of $(\\ln Z_A, \\ln Z_B)$. Define a boolean flag $\\text{sensitive}$ that is $\\text{True}$ if $S$ changes, and $\\text{False}$ otherwise.\n\nTest suite:\nFor each test case, the inputs are $(\\ln Z_A, \\ln Z_B, \\ln Z_A', \\ln Z_B')$, where the primes denote values under modified priors. Use the following test cases:\n1. $(\\ln Z_A, \\ln Z_B, \\ln Z_A', \\ln Z_B') = (120.0, 116.0, 119.5, 116.2)$\n2. $(\\ln Z_A, \\ln Z_B, \\ln Z_A', \\ln Z_B') = (10.0, 9.0, 9.9, 9.0)$\n3. $(\\ln Z_A, \\ln Z_B, \\ln Z_A', \\ln Z_B') = (50.0, 50.0, 50.0, 50.0)$\n4. $(\\ln Z_A, \\ln Z_B, \\ln Z_A', \\ln Z_B') = (210.0, 207.0, 208.5, 206.1)$\n\nRequirements:\n- Derive and implement the numerically stable computation of $K$ from $\\ln Z_A$ and $\\ln Z_B$.\n- Compute $2 \\ln K$ and the corresponding classification code $S$ for the baseline priors $(\\ln Z_A, \\ln Z_B)$.\n- Compute the classification code $S'$ for the modified priors $(\\ln Z_A', \\ln Z_B')$ and set the boolean flag $\\text{sensitive}$ to $\\text{True}$ if $S' \\ne S$, otherwise $\\text{False}$.\n- Your program must process all test cases and produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case’s result must be an inner list with the format $[K, S, \\text{sensitive}]$. For example, the overall output should look like $[[K_1,S_1,\\text{sensitive}_1],[K_2,S_2,\\text{sensitive}_2],\\ldots]$.\n- All calculations are dimensionless; no physical units apply in this problem.",
            "solution": "We begin with Bayes theorem and the definition of the marginal likelihood (Bayesian evidence) for a model $M$,\n$$\nZ = \\int L(d \\mid \\theta, M)\\,\\pi(\\theta \\mid M)\\,\\mathrm{d}\\theta,\n$$\nwhere $L$ is the likelihood of the data $d$ given parameters $\\theta$ and model $M$, and $\\pi$ is the prior distribution for $\\theta$ under $M$. In Bayesian model comparison between $M_A$ and $M_B$, the Bayes factor $K$ is defined as\n$$\nK = \\frac{Z_A}{Z_B}.\n$$\nNested sampling produces estimates of the logarithm of the evidence, $\\ln Z_A$ and $\\ln Z_B$. To compute $K$ in a numerically stable way, we utilize the fact that working in logarithms avoids overflow or underflow. Taking the natural logarithm of $K$,\n$$\n\\ln K = \\ln Z_A - \\ln Z_B.\n$$\nExponentiating the difference yields $K$ directly without computing $Z_A$ or $Z_B$ individually:\n$$\nK = \\exp(\\ln Z_A - \\ln Z_B).\n$$\nThis approach is numerically stable because it avoids exponentiating large $\\ln Z$ values separately and dividing, which could lead to overflow or underflow; instead, it exponentiates the difference $\\ln Z_A - \\ln Z_B$, which is typically of moderate magnitude when comparing realistic models.\n\nTo assess the strength of evidence, we use the Kass and Raftery calibrated scale based on $2 \\ln K$. We define the classification code $S$ via:\n- If $2 \\ln K < 2$, then $S = 0$ (bare mention).\n- If $2 \\ln K \\in [2, 6)$, then $S = 1$ (positive).\n- If $2 \\ln K \\in [6, 10)$, then $S = 2$ (strong).\n- If $2 \\ln K \\ge 10$, then $S = 3$ (very strong).\n\nPrior sensitivity is evaluated by comparing the classification under baseline priors $(\\ln Z_A, \\ln Z_B)$ and modified priors $(\\ln Z_A', \\ln Z_B')$. Specifically, we compute\n$$\n\\ln K = \\ln Z_A - \\ln Z_B, \\quad 2 \\ln K = 2(\\ln Z_A - \\ln Z_B),\n$$\nand obtain $S$ from the calibrated scale. Similarly, we compute\n$$\n\\ln K' = \\ln Z_A' - \\ln Z_B', \\quad 2 \\ln K' = 2(\\ln Z_A' - \\ln Z_B'),\n$$\nand obtain $S'$ from the same scale. The prior sensitivity flag is then set as\n$$\n\\text{sensitive} = (S' \\ne S).\n$$\n\nWe now apply this procedure to the test suite:\n\nTest case $1$: $(\\ln Z_A, \\ln Z_B, \\ln Z_A', \\ln Z_B') = (120.0, 116.0, 119.5, 116.2)$.\n- Baseline: $\\ln K = 120.0 - 116.0 = 4.0$, hence $K = \\exp(4.0) \\approx 54.5981500331$, and $2 \\ln K = 2 \\times 4.0 = 8.0$. Classification is $S = 2$ (strong).\n- Modified: $\\ln K' = 119.5 - 116.2 = 3.3$, thus $2 \\ln K' = 6.6$, classification $S' = 2$ (strong).\n- Sensitivity: $\\text{sensitive} = \\text{False}$ because $S' = S$.\n\nTest case $2$: $(\\ln Z_A, \\ln Z_B, \\ln Z_A', \\ln Z_B') = (10.0, 9.0, 9.9, 9.0)$.\n- Baseline: $\\ln K = 10.0 - 9.0 = 1.0$, $K = \\exp(1.0) \\approx 2.7182818285$, $2 \\ln K = 2.0$. Classification is $S = 1$ (positive).\n- Modified: $\\ln K' = 9.9 - 9.0 = 0.9$, $2 \\ln K' = 1.8$. Classification is $S' = 0$ (bare mention).\n- Sensitivity: $\\text{sensitive} = \\text{True}$ because $S' \\ne S$.\n\nTest case $3$: $(\\ln Z_A, \\ln Z_B, \\ln Z_A', \\ln Z_B') = (50.0, 50.0, 50.0, 50.0)$.\n- Baseline: $\\ln K = 50.0 - 50.0 = 0.0$, $K = \\exp(0.0) = 1.0$, $2 \\ln K = 0.0$. Classification is $S = 0$ (bare mention).\n- Modified: $\\ln K' = 0.0$, $2 \\ln K' = 0.0$, $S' = 0$ (bare mention).\n- Sensitivity: $\\text{sensitive} = \\text{False}$.\n\nTest case $4$: $(\\ln Z_A, \\ln Z_B, \\ln Z_A', \\ln Z_B') = (210.0, 207.0, 208.5, 206.1)$.\n- Baseline: $\\ln K = 210.0 - 207.0 = 3.0$, $K = \\exp(3.0) \\approx 20.0855369232$, $2 \\ln K = 6.0$. Classification is $S = 2$ (strong).\n- Modified: $\\ln K' = 208.5 - 206.1 = 2.4$, $2 \\ln K' = 4.8$. Classification is $S' = 1$ (positive).\n- Sensitivity: $\\text{sensitive} = \\text{True}$.\n\nAlgorithmic design:\n- For each test case, compute $\\Delta = \\ln Z_A - \\ln Z_B$ and set $K = \\exp(\\Delta)$.\n- Compute $T = 2 \\Delta$ and classify $S$ according to the intervals above.\n- Repeat with primed values to get $S'$ and set the boolean flag to whether $S' \\ne S$.\n- Aggregate outputs into a list of inner lists $[K, S, \\text{sensitive}]$ and print the overall list on a single line.\n\nThis procedure integrates the core Bayesian principles with numerically stable computation and a calibrated interpretation of evidence strength, and it provides a concrete indicator of prior sensitivity through classification changes.",
            "answer": "```python\nimport numpy as np\n\ndef classify_strength(two_ln_K: float) -> int:\n    \"\"\"\n    Map 2 ln K to Kass-Raftery strength codes:\n    0: bare mention (<2)\n    1: positive [2,6)\n    2: strong [6,10)\n    3: very strong (>=10)\n    \"\"\"\n    if two_ln_K < 2.0:\n        return 0\n    elif two_ln_K < 6.0:\n        return 1\n    elif two_ln_K < 10.0:\n        return 2\n    else:\n        return 3\n\ndef solve():\n    # Define the test cases from the problem statement:\n    # Each tuple: (lnZ_A, lnZ_B, lnZ_A_mod, lnZ_B_mod)\n    test_cases = [\n        (120.0, 116.0, 119.5, 116.2),\n        (10.0, 9.0, 9.9, 9.0),\n        (50.0, 50.0, 50.0, 50.0),\n        (210.0, 207.0, 208.5, 206.1),\n    ]\n\n    results_strs = []\n    for lnZ_A, lnZ_B, lnZ_A_mod, lnZ_B_mod in test_cases:\n        # Numerically stable Bayes factor: K = exp(lnZ_A - lnZ_B)\n        delta = lnZ_A - lnZ_B\n        K = float(np.exp(delta))\n        two_ln_K = 2.0 * delta\n        S = classify_strength(two_ln_K)\n\n        # Modified priors\n        delta_mod = lnZ_A_mod - lnZ_B_mod\n        two_ln_K_mod = 2.0 * delta_mod\n        S_mod = classify_strength(two_ln_K_mod)\n\n        sensitive = (S_mod != S)\n\n        # Format inner result as [K,S,sensitive]\n        # Use default float formatting and boolean string conversion\n        results_strs.append(f\"[{K},{S},{sensitive}]\")\n\n    # Final print statement in the exact required format: a single line\n    print(f\"[{','.join(results_strs)}]\")\n\nsolve()\n```"
        }
    ]
}