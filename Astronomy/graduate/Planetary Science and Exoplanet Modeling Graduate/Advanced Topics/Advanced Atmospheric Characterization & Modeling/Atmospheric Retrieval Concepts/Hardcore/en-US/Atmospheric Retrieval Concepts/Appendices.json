{
    "hands_on_practices": [
        {
            "introduction": "The uncertainty in a retrieved atmospheric parameter is not solely due to measurement noise. This exercise explores how uncertainties in the forward model itself, specifically in physical parameters like pressure-broadening coefficients, propagate into the final result. By deriving the total variance from first principles, you will gain a quantitative understanding of how these distinct error sources combine and learn to identify which one dominates the error budget .",
            "id": "4153605",
            "problem": "Consider a simplified spectral retrieval problem for a single, isolated molecular absorption line in a transmission or emission spectrum. The forward model follows the Beer–Lambert law in the optically thin limit, so that a discretized spectrum at spectral samples indexed by $i$ can be represented as a linear model\n$$\ny_i \\;=\\; x_{\\text{true}}\\, s_i(\\gamma_{\\mathrm{true}}) \\;+\\; \\varepsilon_i,\n$$\nwhere $y_i$ is the measured signal, $x_{\\text{true}}$ is a dimensionless gas abundance scaling to be retrieved, $\\gamma_{\\mathrm{true}}$ is the pressure-broadened half-width parameter, $s_i(\\gamma)$ is the known spectral template at sample $i$ for a given broadening parameter $\\gamma$, and $\\varepsilon_i$ is independent, identically distributed Gaussian noise with zero mean and variance $\\sigma_\\varepsilon^2$. Assume the line shape is Lorentzian with line center at frequency $\\nu_0$, and the spectral template is\n$$\ns_i(\\gamma)\\;=\\; S \\, L(\\nu_i;\\gamma), \\quad L(\\nu;\\gamma)\\;=\\;\\frac{1}{\\pi}\\,\\frac{\\gamma}{(\\nu-\\nu_0)^2+\\gamma^2},\n$$\nwhere $S$ is a known line strength (dimensionless in this problem), and $\\{\\nu_i\\}$ is a uniformly sampled frequency grid. The pressure-broadened half-width depends on pressure $P$ and temperature $T$ by a commonly used relation\n$$\n\\gamma(P,T)\\;=\\;\\gamma_{\\mathrm{ref}}\\,\\left(\\frac{P}{P_{\\mathrm{ref}}}\\right)\\,\\left(\\frac{T_{\\mathrm{ref}}}{T}\\right)^{n},\n$$\nwith known reference values $\\gamma_{\\mathrm{ref}}$, $P_{\\mathrm{ref}}$, $T_{\\mathrm{ref}}$, and temperature exponent $n$. In practice, the retrieval algorithm fixes the broadening parameter to a nominal value $\\gamma_0=\\gamma(P,T)$ computed from the above relation, and then estimates $x$ by least squares using this fixed template:\n$$\n\\widehat{x}\\;=\\;\\frac{\\sum_{i} s_i(\\gamma_0)\\,y_i}{\\sum_{i} s_i(\\gamma_0)^2}.\n$$\nHowever, the true broadening parameter $\\gamma_{\\mathrm{true}}$ is uncertain. Model this uncertainty as a zero-mean Gaussian perturbation $\\delta\\gamma$ around $\\gamma_0$, i.e., $\\gamma_{\\mathrm{true}}=\\gamma_0+\\delta\\gamma$ with $\\delta\\gamma\\sim\\mathcal{N}(0,\\sigma_\\gamma^2)$, independent of the measurement noise. Assume that $\\sigma_\\gamma$ is specified as a fixed fraction $r$ of $\\gamma_0$, i.e., $\\sigma_\\gamma=r\\,\\gamma_0$. Work in the regime where a first-order Taylor expansion of $s_i(\\gamma)$ about $\\gamma_0$ is valid.\n\nTask:\n- Starting from the above definitions and no other shortcut formulas, derive a first-order expression for the variance of $\\widehat{x}$ that accounts for both measurement noise and the uncertainty in the pressure broadening parameter. The derivation must begin from the linear model definition, the least-squares estimator expression, and a first-order Taylor expansion of $s_i(\\gamma)$ about $\\gamma_0$.\n- Using your derived expression, implement a program that computes the total standard deviation of $\\widehat{x}$, defined as the square root of the total variance, for each of the test cases below.\n\nImportant implementation details and conventions:\n- Use the analytical Lorentzian line shape as given. All quantities in this problem are treated as dimensionless for computational purposes, and no physical unit conversion is required.\n- The frequency grid is uniform: $\\nu_i = \\nu_0 + (i - (N-1)/2)\\,\\Delta\\nu$ for $i=0,1,\\dots,N-1$, where $N$ is the number of samples and $\\Delta\\nu$ is the grid spacing.\n- You must compute the derivative $\\partial s_i/\\partial \\gamma$ analytically from the given Lorentzian definition to evaluate the first-order propagation term.\n- Angles are not involved in this problem.\n- Your program must output the total standard deviation of $\\widehat{x}$ for each test case, rounded and formatted in scientific notation with exactly $6$ significant digits.\n\nTest suite:\n- Case A (general well-conditioned case):\n  - $\\nu_0 = 0$\n  - $N = 1001$\n  - $\\Delta\\nu = 0.01$\n  - $S = 1$\n  - $\\gamma_{\\mathrm{ref}} = 0.05$, $P = 1$, $P_{\\mathrm{ref}} = 1$, $T = 296$, $T_{\\mathrm{ref}} = 296$, $n=0.7$\n  - $r = 0.05$\n  - $\\sigma_\\varepsilon = 1\\times 10^{-4}$\n  - $x_{\\text{true}} = 1$\n- Case B (measurement-noise dominated, very small broadening uncertainty):\n  - $\\nu_0 = 0$\n  - $N = 1001$\n  - $\\Delta\\nu = 0.01$\n  - $S = 1$\n  - $\\gamma_{\\mathrm{ref}} = 0.05$, $P = 1$, $P_{\\mathrm{ref}} = 1$, $T = 296$, $T_{\\mathrm{ref}} = 296$, $n=0.7$\n  - $r = 0.001$\n  - $\\sigma_\\varepsilon = 1\\times 10^{-3}$\n  - $x_{\\text{true}} = 1$\n- Case C (broadening-uncertainty dominated, very high signal-to-noise):\n  - $\\nu_0 = 0$\n  - $N = 2001$\n  - $\\Delta\\nu = 0.005$\n  - $S = 1$\n  - $\\gamma_{\\mathrm{ref}} = 0.05$, $P = 1$, $P_{\\mathrm{ref}} = 1$, $T = 296$, $T_{\\mathrm{ref}} = 296$, $n=0.7$\n  - $r = 0.2$\n  - $\\sigma_\\varepsilon = 1\\times 10^{-6}$\n  - $x_{\\text{true}} = 1$\n- Case D (narrow line at lower pressure, fine sampling):\n  - $\\nu_0 = 0$\n  - $N = 4001$\n  - $\\Delta\\nu = 0.0005$\n  - $S = 1$\n  - $\\gamma_{\\mathrm{ref}} = 0.05$, $P = 0.1$, $P_{\\mathrm{ref}} = 1$, $T = 296$, $T_{\\mathrm{ref}} = 296$, $n=0.7$\n  - $r = 0.05$\n  - $\\sigma_\\varepsilon = 1\\times 10^{-6}$\n  - $x_{\\text{true}} = 1$\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order [Case A, Case B, Case C, Case D], where each value is the total standard deviation of $\\widehat{x}$ for the corresponding case, formatted in scientific notation with exactly $6$ significant digits (for example, $[\"1.234560\\mathrm{e}{-3}\",\"...\"]$ is not acceptable; you must print as $[1.234560e-03,...]$).",
            "solution": "The problem is valid as it is scientifically grounded in the principles of atmospheric spectroscopy and error analysis, is well-posed with all necessary information provided, and is expressed in objective, formal language. We may proceed with the solution.\n\nThe task is to derive an expression for the variance of a least-squares estimator $\\widehat{x}$ in the presence of two independent sources of uncertainty: measurement noise and an error in a model parameter. We will then implement this expression to compute the total standard deviation of $\\widehat{x}$ for several test cases.\n\nThe measured signal at spectral sample $i$ is given by the linear model:\n$$\ny_i = x_{\\text{true}}\\, s_i(\\gamma_{\\text{true}}) + \\varepsilon_i\n$$\nwhere $x_{\\text{true}}$ is the true abundance scaling, $s_i(\\gamma_{\\text{true}})$ is the spectral template evaluated with the true broadening parameter $\\gamma_{\\text{true}}$, and $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_\\varepsilon^2)$ is the measurement noise.\n\nThe retrieval algorithm, however, uses a nominal broadening parameter $\\gamma_0$ and computes the estimate $\\widehat{x}$ via least squares:\n$$\n\\widehat{x} = \\frac{\\sum_{i} s_i(\\gamma_0)\\,y_i}{\\sum_{i} s_i(\\gamma_0)^2}\n$$\n\nLet us substitute the model for $y_i$ into the expression for $\\widehat{x}$:\n$$\n\\widehat{x} = \\frac{\\sum_{i} s_i(\\gamma_0)\\,\\left(x_{\\text{true}}\\, s_i(\\gamma_{\\text{true}}) + \\varepsilon_i\\right)}{\\sum_{j} s_j(\\gamma_0)^2}\n$$\nwhere the index in the denominator has been changed to $j$ for clarity. We can separate this into two terms:\n$$\n\\widehat{x} = \\frac{x_{\\text{true}} \\sum_{i} s_i(\\gamma_0) s_i(\\gamma_{\\text{true}})}{\\sum_{j} s_j(\\gamma_0)^2} + \\frac{\\sum_{i} s_i(\\gamma_0) \\varepsilon_i}{\\sum_{j} s_j(\\gamma_0)^2}\n$$\n\nThe true broadening parameter $\\gamma_{\\text{true}}$ is related to the nominal parameter $\\gamma_0$ by a small, zero-mean random perturbation $\\delta\\gamma$:\n$$\n\\gamma_{\\text{true}} = \\gamma_0 + \\delta\\gamma, \\quad \\text{with} \\quad \\delta\\gamma \\sim \\mathcal{N}(0, \\sigma_\\gamma^2)\n$$\nWe are given that a first-order Taylor expansion of $s_i(\\gamma)$ about $\\gamma_0$ is a valid approximation. We thus write:\n$$\ns_i(\\gamma_{\\text{true}}) \\approx s_i(\\gamma_0) + \\frac{\\partial s_i}{\\partial \\gamma}\\bigg|_{\\gamma_0} (\\gamma_{\\text{true}} - \\gamma_0) = s_i(\\gamma_0) + s_i'(\\gamma_0) \\delta\\gamma\n$$\nwhere $s_i'(\\gamma_0)$ denotes the derivative of $s_i$ with respect to $\\gamma$ evaluated at $\\gamma_0$.\n\nSubstituting this expansion into the expression for $\\widehat{x}$:\n$$\n\\widehat{x} \\approx \\frac{x_{\\text{true}} \\sum_{i} s_i(\\gamma_0) \\left(s_i(\\gamma_0) + s_i'(\\gamma_0) \\delta\\gamma\\right)}{\\sum_{j} s_j(\\gamma_0)^2} + \\frac{\\sum_{i} s_i(\\gamma_0) \\varepsilon_i}{\\sum_{j} s_j(\\gamma_0)^2}\n$$\n$$\n\\widehat{x} \\approx \\frac{x_{\\text{true}} \\sum_{i} s_i(\\gamma_0)^2}{\\sum_{j} s_j(\\gamma_0)^2} + \\frac{x_{\\text{true}} \\sum_{i} s_i(\\gamma_0) s_i'(\\gamma_0)}{\\sum_{j} s_j(\\gamma_0)^2} \\delta\\gamma + \\frac{\\sum_{i} s_i(\\gamma_0) \\varepsilon_i}{\\sum_{j} s_j(\\gamma_0)^2}\n$$\nThe first term simplifies to $x_{\\text{true}}$. The expression for the estimated parameter becomes:\n$$\n\\widehat{x} \\approx x_{\\text{true}} + \\left(x_{\\text{true}} \\frac{\\sum_{i} s_i(\\gamma_0) s_i'(\\gamma_0)}{\\sum_{j} s_j(\\gamma_0)^2}\\right) \\delta\\gamma + \\left(\\frac{\\sum_{i} s_i(\\gamma_0) \\varepsilon_i}{\\sum_{j} s_j(\\gamma_0)^2}\\right)\n$$\n\nTo find the variance, $\\text{Var}(\\widehat{x})$, we first find the expectation $E[\\widehat{x}]$. Given that $E[\\varepsilon_i]=0$ and $E[\\delta\\gamma]=0$, the expectation of the latter two terms is zero. Thus, to first order, the estimator is unbiased:\n$$\nE[\\widehat{x}] \\approx x_{\\text{true}}\n$$\nThe variance is the expectation of the squared deviation from the mean, $\\text{Var}(\\widehat{x}) = E[(\\widehat{x} - E[\\widehat{x}])^2] \\approx E[(\\widehat{x} - x_{\\text{true}})^2]$.\n$$\n\\widehat{x} - x_{\\text{true}} \\approx \\underbrace{\\left(x_{\\text{true}} \\frac{\\sum_{i} s_i(\\gamma_0) s_i'(\\gamma_0)}{\\sum_{j} s_j(\\gamma_0)^2}\\right) \\delta\\gamma}_{\\text{Term A}} + \\underbrace{\\left(\\frac{\\sum_{i} s_i(\\gamma_0) \\varepsilon_i}{\\sum_{j} s_j(\\gamma_0)^2}\\right)}_{\\text{Term B}}\n$$\nThe total variance is $\\text{Var}(\\widehat{x}) \\approx E[(A+B)^2] = E[A^2] + E[B^2] + 2E[AB]$. The random variables $\\delta\\gamma$ and $\\varepsilon_i$ are stated to be independent. Since both are zero-mean, the expectation of their product is zero, $E[\\delta\\gamma \\varepsilon_i] = E[\\delta\\gamma]E[\\varepsilon_i] = 0$. Consequently, the cross-term $E[AB]$ is zero.\n\nThe total variance is the sum of the variances from the two independent error sources:\n$$\n\\text{Var}(\\widehat{x}) \\approx E[A^2] + E[B^2] = \\text{Var}(A) + \\text{Var}(B)\n$$\n\nThe first component, due to measurement noise, is:\n$$\n\\text{Var}_{\\varepsilon}(\\widehat{x}) = E[B^2] = E\\left[ \\left(\\frac{\\sum_{i} s_i(\\gamma_0) \\varepsilon_i}{\\sum_{j} s_j(\\gamma_0)^2}\\right)^2 \\right] = \\frac{1}{(\\sum_{j} s_j(\\gamma_0)^2)^2} \\sum_{i,k} s_i(\\gamma_0) s_k(\\gamma_0) E[\\varepsilon_i \\varepsilon_k]\n$$\nSince $\\varepsilon_i$ are i.i.d. with variance $\\sigma_\\varepsilon^2$, we have $E[\\varepsilon_i \\varepsilon_k] = \\sigma_\\varepsilon^2 \\delta_{ik}$, where $\\delta_{ik}$ is the Kronecker delta. The double summation collapses to a single summation:\n$$\n\\text{Var}_{\\varepsilon}(\\widehat{x}) = \\frac{\\sum_{i} s_i(\\gamma_0)^2 \\sigma_\\varepsilon^2}{(\\sum_{j} s_j(\\gamma_0)^2)^2} = \\frac{\\sigma_\\varepsilon^2}{\\sum_{i} s_i(\\gamma_0)^2}\n$$\n\nThe second component, due to broadening parameter uncertainty, is:\n$$\n\\text{Var}_{\\gamma}(\\widehat{x}) = E[A^2] = E\\left[ \\left(x_{\\text{true}} \\frac{\\sum_{i} s_i(\\gamma_0) s_i'(\\gamma_0)}{\\sum_{j} s_j(\\gamma_0)^2} \\delta\\gamma \\right)^2 \\right]\n$$\n$$\n\\text{Var}_{\\gamma}(\\widehat{x}) = \\left(x_{\\text{true}} \\frac{\\sum_{i} s_i(\\gamma_0) s_i'(\\gamma_0)}{\\sum_{j} s_j(\\gamma_0)^2}\\right)^2 E[\\delta\\gamma^2]\n$$\nSince $E[\\delta\\gamma]=0$, $E[\\delta\\gamma^2] = \\text{Var}(\\delta\\gamma) = \\sigma_\\gamma^2$.\n$$\n\\text{Var}_{\\gamma}(\\widehat{x}) = \\left(x_{\\text{true}} \\frac{\\sum_{i} s_i(\\gamma_0) s_i'(\\gamma_0)}{\\sum_{j} s_j(\\gamma_0)^2}\\right)^2 \\sigma_\\gamma^2\n$$\n\nThe total variance of the estimator $\\widehat{x}$ is the sum of these two components:\n$$\n\\text{Var}(\\widehat{x}) \\approx \\frac{\\sigma_\\varepsilon^2}{\\sum_i s_i(\\gamma_0)^2} + \\left(x_{\\text{true}} \\frac{\\sum_i s_i(\\gamma_0) s_i'(\\gamma_0)}{\\sum_j s_j(\\gamma_0)^2}\\right)^2 \\sigma_\\gamma^2\n$$\nThe total standard deviation is $\\sigma_{\\widehat{x}} = \\sqrt{\\text{Var}(\\widehat{x})}$.\n\nTo implement this, we need the analytical derivative of the spectral template $s_i(\\gamma) = S L(\\nu_i; \\gamma) = \\frac{S}{\\pi} \\frac{\\gamma}{(\\nu_i-\\nu_0)^2 + \\gamma^2}$.\nUsing the quotient rule, we find:\n$$\n\\frac{\\partial s_i}{\\partial \\gamma} = \\frac{S}{\\pi} \\frac{\\partial}{\\partial \\gamma} \\left(\\frac{\\gamma}{(\\nu_i-\\nu_0)^2 + \\gamma^2}\\right) = \\frac{S}{\\pi} \\frac{1 \\cdot ((\\nu_i-\\nu_0)^2 + \\gamma^2) - \\gamma \\cdot (2\\gamma)}{((\\nu_i-\\nu_0)^2 + \\gamma^2)^2}\n$$\n$$\ns_i'(\\gamma) = \\frac{\\partial s_i}{\\partial \\gamma} = \\frac{S}{\\pi} \\frac{(\\nu_i-\\nu_0)^2 - \\gamma^2}{((\\nu_i-\\nu_0)^2 + \\gamma^2)^2}\n$$\nThis derivative will be evaluated at $\\gamma = \\gamma_0$ for use in the variance expression.\nThe value of $\\gamma_0$ is computed from the given atmospheric state $(P,T)$:\n$$\n\\gamma_0 = \\gamma(P,T)\\;=\\;\\gamma_{\\mathrm{ref}}\\,\\left(\\frac{P}{P_{\\mathrm{ref}}}\\right)\\,\\left(\\frac{T_{\\mathrm{ref}}}{T}\\right)^{n}\n$$\nThe broadening uncertainty standard deviation is given by $\\sigma_\\gamma = r \\gamma_0$.\nThe following Python code implements this derivation to compute the total standard deviation of $\\widehat{x}$ for the specified test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_std_dev(params: dict) -> float:\n    \"\"\"\n    Calculates the total standard deviation of the retrieved abundance parameter.\n\n    Args:\n        params: A dictionary containing all the necessary parameters for the calculation.\n\n    Returns:\n        The total standard deviation of x_hat.\n    \"\"\"\n    # Unpack parameters from the dictionary\n    nu0 = params[\"nu0\"]\n    N = params[\"N\"]\n    delta_nu = params[\"delta_nu\"]\n    S = params[\"S\"]\n    gamma_ref = params[\"gamma_ref\"]\n    P = params[\"P\"]\n    P_ref = params[\"P_ref\"]\n    T = params[\"T\"]\n    T_ref = params[\"T_ref\"]\n    n = params[\"n\"]\n    r = params[\"r\"]\n    sigma_eps = params[\"sigma_eps\"]\n    x_true = params[\"x_true\"]\n\n    # Step 1: Calculate the nominal broadening parameter gamma_0.\n    gamma0 = gamma_ref * (P / P_ref) * (T_ref / T)**n\n    \n    # Step 2: Calculate the standard deviation of the broadening parameter, sigma_gamma.\n    sigma_gamma = r * gamma0\n\n    # Step 3: Generate the frequency grid.\n    i_indices = np.arange(N)\n    nu_grid = nu0 + (i_indices - (N - 1) / 2.0) * delta_nu\n\n    # Step 4: Calculate the spectral template s(gamma_0) and its derivative s'(gamma_0).\n    nu_minus_nu0_sq = (nu_grid - nu0)**2\n    lorentz_denominator = nu_minus_nu0_sq + gamma0**2\n    \n    # Spectral template s_i(gamma_0)\n    s_gamma0 = (S / np.pi) * (gamma0 / lorentz_denominator)\n    \n    # Derivative of the spectral template, s'_i(gamma_0)\n    s_prime_gamma0 = (S / np.pi) * (nu_minus_nu0_sq - gamma0**2) / (lorentz_denominator**2)\n\n    # Step 5: Compute the required sums over the spectral grid.\n    sum_s_sq = np.sum(s_gamma0**2)\n    sum_s_s_prime = np.sum(s_gamma0 * s_prime_gamma0)\n\n    # Step 6: Calculate the two variance components.\n    # Variance due to measurement noise\n    var_noise = sigma_eps**2 / sum_s_sq\n    \n    # Variance due to broadening parameter uncertainty\n    var_broadening = (x_true * sum_s_s_prime / sum_s_sq)**2 * sigma_gamma**2\n    \n    # Step 7: Sum the variances to get the total variance and then the standard deviation.\n    total_var = var_noise + var_broadening\n    total_std_dev = np.sqrt(total_var)\n    \n    return total_std_dev\n\n\ndef solve():\n    \"\"\"\n    Executes the calculation for all test cases and prints the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A\n        {\"nu0\": 0.0, \"N\": 1001, \"delta_nu\": 0.01, \"S\": 1.0, \n         \"gamma_ref\": 0.05, \"P\": 1.0, \"P_ref\": 1.0, \"T\": 296.0, \"T_ref\": 296.0, \"n\": 0.7, \n         \"r\": 0.05, \"sigma_eps\": 1e-4, \"x_true\": 1.0},\n        # Case B\n        {\"nu0\": 0.0, \"N\": 1001, \"delta_nu\": 0.01, \"S\": 1.0, \n         \"gamma_ref\": 0.05, \"P\": 1.0, \"P_ref\": 1.0, \"T\": 296.0, \"T_ref\": 296.0, \"n\": 0.7, \n         \"r\": 0.001, \"sigma_eps\": 1e-3, \"x_true\": 1.0},\n        # Case C\n        {\"nu0\": 0.0, \"N\": 2001, \"delta_nu\": 0.005, \"S\": 1.0, \n         \"gamma_ref\": 0.05, \"P\": 1.0, \"P_ref\": 1.0, \"T\": 296.0, \"T_ref\": 296.0, \"n\": 0.7, \n         \"r\": 0.2, \"sigma_eps\": 1e-6, \"x_true\": 1.0},\n        # Case D\n        {\"nu0\": 0.0, \"N\": 4001, \"delta_nu\": 0.0005, \"S\": 1.0, \n         \"gamma_ref\": 0.05, \"P\": 0.1, \"P_ref\": 1.0, \"T\": 296.0, \"T_ref\": 296.0, \"n\": 0.7, \n         \"r\": 0.05, \"sigma_eps\": 1e-6, \"x_true\": 1.0}\n    ]\n\n    results = []\n    for case in test_cases:\n        std_dev = calculate_std_dev(case)\n        # Format the result in scientific notation with 6 digits after the decimal.\n        results.append(f\"{std_dev:.6e}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Building on the concept of model uncertainty, we now move to a more comprehensive probabilistic framework. Instead of treating errors separately, this practice demonstrates how to construct a marginal likelihood by formally integrating out multiple nuisance parameters, including uncertainties in spectroscopic line lists and a term for generic model inadequacy. This powerful technique results in a more robust and statistically sound basis for parameter inference .",
            "id": "4153640",
            "problem": "Consider an exoplanet transmission spectroscopy setting in which the observed differential transit depth at a small set of wavelengths is modeled in a linearized regime. Begin from the Beer–Lambert law for attenuation, which states that the optical depth at wavelength $\\lambda$, denoted by $\\tau(\\lambda)$, satisfies $\\tau(\\lambda) = \\int \\rho(l) \\kappa(\\lambda, l) \\, dl$, where $\\rho(l)$ is the absorber density and $\\kappa(\\lambda, l)$ is the absorption coefficient along the path $l$. In the optically thin limit, the transmittance is approximated by $T(\\lambda) \\approx 1 - \\tau(\\lambda)$, and the measured residual (relative to a baseline level) is proportional to $\\tau(\\lambda)$. Model the wavelength-dependent residual as a linear combination of predetermined line-shape basis functions and their strengths.\n\nAssume there are $N_\\lambda$ wavelength samples and $N_{\\text{lines}}$ known spectral lines. Let the matrix $G \\in \\mathbb{R}^{N_\\lambda \\times N_{\\text{lines}}}$ contain the line-shape basis functions evaluated at the wavelengths, the vector of nominal line strengths be $s_0 \\in \\mathbb{R}^{N_{\\text{lines}}}$, the unknown mixing ratio be $q \\in \\mathbb{R}_{\\ge 0}$, and the unknown additive baseline offset be $b \\in \\mathbb{R}$. The measured data vector is $y \\in \\mathbb{R}^{N_\\lambda}$. Under a linearized forward model consistent with the optically thin limit, the residuals satisfy\n$$y = b \\mathbf{1} + q G s_0 + n + \\delta + q G \\epsilon,$$\nwhere $\\mathbf{1} \\in \\mathbb{R}^{N_\\lambda}$ is the vector of ones, $n \\in \\mathbb{R}^{N_\\lambda}$ is measurement noise, $\\delta \\in \\mathbb{R}^{N_\\lambda}$ represents model inadequacy, and $\\epsilon \\in \\mathbb{R}^{N_{\\text{lines}}}$ captures line-strength errors relative to $s_0$. Assume the following probabilistic structure:\n- Measurement noise is Independent and Identically Distributed (IID) Gaussian: $n \\sim \\mathcal{N}(0, \\sigma_n^2 I)$.\n- Line-strength errors are Gaussian with nominal mean: $\\epsilon \\sim \\mathcal{N}(0, \\sigma_s^2 I)$.\n- Model inadequacy is approximated as IID Gaussian: $\\delta \\sim \\mathcal{N}(0, \\sigma_\\delta^2 I)$.\n\nFrom these assumptions and the linear model, derive the exact Gaussian marginal likelihood for $y$ conditioned on $q$ and $b$ by analytically integrating out both $\\epsilon$ and $\\delta$. Your derivation must start from the Beer–Lambert law, the optically thin approximation, the linear forward model stated above, and the specified Gaussian priors, and proceed to the marginal distribution $p(y \\mid q, b)$ without introducing any shortcut formulas. Explicitly characterize the mean and covariance of the resulting Gaussian distribution in terms of $G$, $s_0$, $\\sigma_n$, $\\sigma_s$, $\\sigma_\\delta$, and $q$.\n\nUsing the derived marginal likelihood, obtain the Maximum Likelihood Estimate (MLE) of the baseline $b$ for a fixed $q$, and then construct the profile log-marginal-likelihood as a function of $q$ alone by substituting the $b$-MLE. Finally, compute the $q$ and $b$ that maximize the marginal likelihood. All computations must be done in exact arithmetic with no approximations beyond those stated.\n\nImplement a program that, for each test case, computes:\n- The MLE mixing ratio $\\hat{q}$ (dimensionless).\n- The corresponding MLE baseline $\\hat{b}$ (dimensionless).\n- The maximized log marginal likelihood $\\log \\mathcal{L}(\\hat{q}, \\hat{b})$ (dimensionless).\n\nAll quantities are dimensionless, and angles are not involved. Express the final results as decimal floats. Your program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order $[\\hat{q}_1,\\hat{b}_1,\\log \\mathcal{L}_1,\\hat{q}_2,\\hat{b}_2,\\log \\mathcal{L}_2,\\hat{q}_3,\\hat{b}_3,\\log \\mathcal{L}_3]$.\n\nUse the following test suite of parameter values to ensure coverage of a typical case, a boundary case with no line-list uncertainty, and an edge case with dominant model inadequacy. The same line-shape matrix $G$ and nominal strengths $s_0$ are used across cases.\n\nLet $N_\\lambda = 5$, $N_{\\text{lines}} = 2$, and\n$$\nG = \\begin{bmatrix}\n0.8 & 0.0 \\\\\n0.6 & 0.1 \\\\\n0.3 & 0.3 \\\\\n0.1 & 0.7 \\\\\n0.0 & 0.9\n\\end{bmatrix}, \\quad\ns_0 = \\begin{bmatrix} 1.0 \\\\ 0.7 \\end{bmatrix}.\n$$\n\nCompute the data vectors $y$ deterministically for each case using the forward model components specified. The program must construct $y$ as indicated below, and then perform retrieval under the stated uncertainty parameters.\n\n- Case $1$ (happy path): Use $q_{\\text{true}} = 0.6$, $b_{\\text{true}} = 0.001$, $s_{\\text{true}} = s_0 + [0.2, -0.1]^T$, measurement noise $n = [0.0002, -0.0001, 0.0, 0.00015, -0.00005]^T$, and model inadequacy $\\delta = [0.0, 0.0, 0.0, 0.0, 0.0]^T$. Form $y = b_{\\text{true}} \\mathbf{1} + q_{\\text{true}} G s_{\\text{true}} + n + \\delta$. Retrieval assumptions: $\\sigma_n = 0.0003$, $\\sigma_s = 0.15$, $\\sigma_\\delta = 0.0002$.\n\n- Case $2$ (boundary, no line-list uncertainty): Use the same $y$ as in Case $1$. Retrieval assumptions: $\\sigma_n = 0.0003$, $\\sigma_s = 0.0$, $\\sigma_\\delta = 0.0002$.\n\n- Case $3$ (edge, dominant model inadequacy): Use $q_{\\text{true}} = 0.5$, $b_{\\text{true}} = 0.0$, $s_{\\text{true}} = s_0$, measurement noise $n = [0.0, 0.0, 0.0, 0.0, 0.0]^T$, and model inadequacy $\\delta = [0.001, -0.0005, 0.0008, -0.0012, 0.0004]^T$. Form $y = b_{\\text{true}} \\mathbf{1} + q_{\\text{true}} G s_{\\text{true}} + n + \\delta$. Retrieval assumptions: $\\sigma_n = 0.0001$, $\\sigma_s = 0.2$, $\\sigma_\\delta = 0.001$.\n\nYour program must:\n- Implement the exact Gaussian marginalization over line-strength errors $\\epsilon$ and model inadequacy $\\delta$.\n- Use a numerically stable method to evaluate the log marginal likelihood (for example, via Cholesky factorization).\n- Maximize the profile log marginal likelihood over $q \\in [0, 10]$ and compute the corresponding $\\hat{b}$ via its MLE for that $\\hat{q}$.\n- Produce the final single-line output in the exact required format.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded in the principles of spectroscopy and statistical inference, is well-posed with a clear objective and sufficient data, and is free of ambiguities or contradictions.\n\nThe solution proceeds by first deriving the marginal likelihood as requested, then finding the maximum likelihood estimates for the unknown parameters.\n\n### 1. Derivation of the Marginal Likelihood $p(y \\mid q, b)$\n\nThe analysis begins with the provided linear forward model:\n$$y = b \\mathbf{1} + q G s_0 + n + \\delta + q G \\epsilon$$\nHere, $y$ is the data vector, $b$ and $q$ are the parameters of interest, and $n$, $\\delta$, and $\\epsilon$ are random variables representing different sources of uncertainty. To derive the likelihood of the data $p(y \\mid q, b)$, we must integrate out, or marginalize over, the nuisance random variables $\\epsilon$ and $\\delta$.\n\nWe can rearrange the model equation to group the deterministic and random terms. Let the deterministic mean model be $\\mu(q, b) = b \\mathbf{1} + q G s_0$. The equation becomes:\n$$y - \\mu(q, b) = n + \\delta + q G \\epsilon$$\nThe right-hand side is a composite noise term, which is a linear combination of the independent random vectors $n$, $\\delta$, and $\\epsilon$. A fundamental property of Gaussian distributions is that a linear transformation of a Gaussian random vector is also a Gaussian random vector. Furthermore, the sum of independent Gaussian random vectors is a Gaussian random vector.\n\nThe individual random vectors have the following distributions:\n- Measurement noise: $n \\sim \\mathcal{N}(0, \\sigma_n^2 I_{N_\\lambda})$\n- Model inadequacy: $\\delta \\sim \\mathcal{N}(0, \\sigma_\\delta^2 I_{N_\\lambda})$\n- Line-strength errors: $\\epsilon \\sim \\mathcal{N}(0, \\sigma_s^2 I_{N_{\\text{lines}}})$\n\nLet the composite noise term be $v = n + \\delta + q G \\epsilon$. The distribution of $v$ is Gaussian. We determine its mean and covariance.\n\nThe mean of $v$ is the sum of the means of its components:\n$$\\mathbb{E}[v] = \\mathbb{E}[n] + \\mathbb{E}[\\delta] + \\mathbb{E}[q G \\epsilon]$$\nSince $\\mathbb{E}[n] = 0$, $\\mathbb{E}[\\delta] = 0$, and $\\mathbb{E}[\\epsilon] = 0$, we have:\n$$\\mathbb{E}[v] = 0 + 0 + q G \\mathbb{E}[\\epsilon] = 0$$\n\nThe covariance of $v$, which we denote $C_y(q)$, is the sum of the covariances of its independent components. For a random vector $X$ and a matrix $A$, $\\text{Cov}(AX) = A \\text{Cov}(X) A^T$.\n$$\\text{Cov}(v) = C_y(q) = \\text{Cov}(n) + \\text{Cov}(\\delta) + \\text{Cov}(q G \\epsilon)$$\n$$C_y(q) = \\sigma_n^2 I_{N_\\lambda} + \\sigma_\\delta^2 I_{N_\\lambda} + (qG) \\text{Cov}(\\epsilon) (qG)^T$$\n$$C_y(q) = (\\sigma_n^2 + \\sigma_\\delta^2) I_{N_\\lambda} + q^2 \\sigma_s^2 G I_{N_{\\text{lines}}} G^T$$\n$$C_y(q) = (\\sigma_n^2 + \\sigma_\\delta^2) I_{N_\\lambda} + q^2 \\sigma_s^2 G G^T$$\nThe covariance matrix $C_y$ depends on the mixing ratio $q$ but not on the baseline $b$.\n\nSince $y - \\mu(q, b) = v$, it follows that $y$ conditioned on $q$ and $b$ is a multivariate Gaussian random variable with mean $\\mu(q, b)$ and covariance $C_y(q)$:\n$$y \\mid q, b \\sim \\mathcal{N}(\\mu(q, b), C_y(q))$$\nThe probability density function for this distribution is the desired marginal likelihood, $\\mathcal{L}(q, b) \\equiv p(y \\mid q, b)$:\n$$\\mathcal{L}(q, b) = \\frac{1}{\\sqrt{(2\\pi)^{N_\\lambda} \\det(C_y(q))}} \\exp\\left(-\\frac{1}{2} (y - \\mu(q, b))^T C_y(q)^{-1} (y - \\mu(q, b))\\right)$$\nThe log-marginal-likelihood is:\n$$\\log \\mathcal{L}(q, b) = -\\frac{N_\\lambda}{2} \\log(2\\pi) - \\frac{1}{2} \\log \\det(C_y(q)) - \\frac{1}{2} (y - (b\\mathbf{1} + q G s_0))^T C_y(q)^{-1} (y - (b\\mathbf{1} + q G s_0))$$\n\n### 2. Maximum Likelihood Estimate (MLE) of the Baseline $b$\n\nTo find the MLE for $b$ for a fixed value of $q$, denoted $\\hat{b}(q)$, we maximize $\\log \\mathcal{L}(q, b)$ with respect to $b$. This is equivalent to minimizing the quadratic term in the exponent:\n$$\\chi^2(b) = (y - b\\mathbf{1} - q G s_0)^T C_y(q)^{-1} (y - b\\mathbf{1} - q G s_0)$$\nLet $z(q) = y - q G s_0$. The expression becomes $\\chi^2(b) = (z(q) - b\\mathbf{1})^T C_y(q)^{-1} (z(q) - b\\mathbf{1})$. We find the minimum by setting the derivative with respect to $b$ to zero:\n$$\\frac{\\partial \\chi^2}{\\partial b} = -2 \\mathbf{1}^T C_y(q)^{-1} (z(q) - b\\mathbf{1}) = 0$$\n$$\\mathbf{1}^T C_y(q)^{-1} z(q) - b (\\mathbf{1}^T C_y(q)^{-1} \\mathbf{1}) = 0$$\nSolving for $b$ gives the MLE $\\hat{b}(q)$:\n$$\\hat{b}(q) = \\frac{\\mathbf{1}^T C_y(q)^{-1} z(q)}{\\mathbf{1}^T C_y(q)^{-1} \\mathbf{1}} = \\frac{\\mathbf{1}^T C_y(q)^{-1} (y - q G s_0)}{\\mathbf{1}^T C_y(q)^{-1} \\mathbf{1}}$$\nThis is the expression for the generalized least squares estimate of the intercept parameter $b$.\n\n### 3. Profile Log-Likelihood and Parameter Estimation\n\nBy substituting $\\hat{b}(q)$ back into the log-likelihood function, we obtain the profile log-likelihood, which is a function of $q$ alone:\n$$\\log \\mathcal{L}_{\\text{prof}}(q) = \\log \\mathcal{L}(q, \\hat{b}(q))$$\nThe overall MLE for the mixing ratio, $\\hat{q}$, is found by maximizing this profile log-likelihood over the allowed range for $q$, which is specified as $q \\in [0, 10]$ for the numerical implementation:\n$$\\hat{q} = \\arg\\max_{q \\in [0, 10]} \\log \\mathcal{L}_{\\text{prof}}(q)$$\nThis is a one-dimensional optimization problem. Once $\\hat{q}$ is found, the corresponding MLE for the baseline, $\\hat{b}$, is computed directly using the formula for $\\hat{b}(\\hat{q})$:\n$$\\hat{b} = \\hat{b}(\\hat{q}) = \\frac{\\mathbf{1}^T C_y(\\hat{q})^{-1} (y - \\hat{q} G s_0)}{\\mathbf{1}^T C_y(\\hat{q})^{-1} \\mathbf{1}}$$\nFinally, the maximized log-marginal-likelihood is $\\log \\mathcal{L}(\\hat{q}, \\hat{b})$.\n\n### 4. Algorithmic Implementation\n\nThe numerical implementation proceeds as follows:\n1.  For each test case, the necessary parameters ($G, s_0$) and case-specific data ($y$, $\\sigma_n, \\sigma_s, \\sigma_\\delta$) are defined.\n2.  A Python function is created to compute the profile log-likelihood, $\\log \\mathcal{L}_{\\text{prof}}(q)$, and the corresponding $\\hat{b}(q)$.\n    -   Inside this function, for a given $q$, the covariance matrix $C_y(q)$ is constructed.\n    -   To compute $\\hat{b}(q)$ and the final $\\chi^2$ term, direct matrix inversion is avoided for numerical stability. Instead, systems of linear equations are solved using `numpy.linalg.solve`. For example, to compute a term like $C_y^{-1}v$, we solve the system $C_y x = v$ for $x$.\n    -   The term $\\log \\det(C_y(q))$ is computed stably using `numpy.linalg.slogdet`, which returns the sign and natural logarithm of the determinant.\n3.  The negative of the profile log-likelihood function is minimized with respect to $q$ over the interval $[0, 10]$ using `scipy.optimize.minimize_scalar` with the `bounded` method. This yields $\\hat{q}$.\n4.  With the optimized $\\hat{q}$, the corresponding $\\hat{b}$ is re-calculated using its explicit formula, and the maximum log-likelihood value $\\log \\mathcal{L}(\\hat{q}, \\hat{b})$ is obtained.\n5.  The computed triplet $(\\hat{q}, \\hat{b}, \\log \\mathcal{L})$ is stored for each test case.\n6.  The final results are formatted and printed as a single comma-separated list.",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize_scalar\n\ndef solve():\n    \"\"\"\n    Solves for the MLE of q and b based on a marginalized likelihood.\n    \"\"\"\n    #\n    # --- Define common parameters from the problem statement ---\n    #\n    N_lambda = 5\n    N_lines = 2\n    G = np.array([\n        [0.8, 0.0],\n        [0.6, 0.1],\n        [0.3, 0.3],\n        [0.1, 0.7],\n        [0.0, 0.9]\n    ])\n    s0 = np.array([1.0, 0.7])\n    \n    one_vec = np.ones(N_lambda)\n    \n    # Pre-compute terms to optimize calculations inside the loop\n    GGt = G @ G.T\n    Gs0 = G @ s0\n\n    #\n    # --- Define test cases ---\n    #\n    test_cases_spec = [\n        {\n            \"case_name\": \"Case 1: Happy Path\",\n            \"y_gen\": {\n                \"q_true\": 0.6,\n                \"b_true\": 0.001,\n                \"s_true\": s0 + np.array([0.2, -0.1]),\n                \"n\": np.array([0.0002, -0.0001, 0.0, 0.00015, -0.00005]),\n                \"delta\": np.zeros(N_lambda),\n            },\n            \"retrieval\": {\"sigma_n\": 0.0003, \"sigma_s\": 0.15, \"sigma_delta\": 0.0002}\n        },\n        {\n            \"case_name\": \"Case 2: No Line-list Uncertainty\",\n            \"y_gen\": \"use_case_1\", # Use the same y as case 1\n            \"retrieval\": {\"sigma_n\": 0.0003, \"sigma_s\": 0.0, \"sigma_delta\": 0.0002}\n        },\n        {\n            \"case_name\": \"Case 3: Dominant Model Inadequacy\",\n            \"y_gen\": {\n                \"q_true\": 0.5,\n                \"b_true\": 0.0,\n                \"s_true\": s0,\n                \"n\": np.zeros(N_lambda),\n                \"delta\": np.array([0.001, -0.0005, 0.0008, -0.0012, 0.0004]),\n            },\n            \"retrieval\": {\"sigma_n\": 0.0001, \"sigma_s\": 0.2, \"sigma_delta\": 0.001}\n        }\n    ]\n\n    all_results = []\n    y_case1 = None\n\n    #\n    # --- Define helper function for likelihood calculation and optimization ---\n    #\n    def get_profile_logL_and_b(q, y, Gs0_term, GGt_term, one_vec_term, ret_params):\n        \"\"\"\n        Calculates the profile log-likelihood for a given q, also returning b_hat.\n        \"\"\"\n        sigma_n = ret_params[\"sigma_n\"]\n        sigma_s = ret_params[\"sigma_s\"]\n        sigma_delta = ret_params[\"sigma_delta\"]\n\n        var_n_delta = sigma_n**2 + sigma_delta**2\n        var_s = sigma_s**2\n        \n        # Construct the covariance matrix C_y(q)\n        C_y = var_n_delta * np.eye(N_lambda) + (q**2 * var_s) * GGt_term\n        \n        # Calculate b_hat(q) using a stable solver\n        z = y - q * Gs0_term\n        try:\n            # Solve C_y * x = 1_vec to get C_y_inv_one\n            C_y_inv_one = np.linalg.solve(C_y, one_vec_term)\n            b_hat_num = C_y_inv_one @ z\n            b_hat_den = C_y_inv_one @ one_vec_term\n            if np.isclose(b_hat_den, 0): return -np.inf, 0 # Avoid division by zero\n            b_hat = b_hat_num / b_hat_den\n        except np.linalg.LinAlgError:\n            return -np.inf, 0 # Return a very small logL if matrix is singular\n\n        # Calculate the log-likelihood\n        sign, log_det_Cy = np.linalg.slogdet(C_y)\n        if sign <= 0: return -np.inf, b_hat # Covariance not positive definite\n\n        residual = y - b_hat * one_vec_term - q * Gs0_term\n        try:\n            # Calculate chi-squared term: res^T * C_y^{-1} * res\n            chi2 = residual @ np.linalg.solve(C_y, residual)\n        except np.linalg.LinAlgError:\n            return -np.inf, b_hat\n\n        logL = -0.5 * (N_lambda * np.log(2 * np.pi) + log_det_Cy + chi2)\n        return logL, b_hat\n\n    #\n    # --- Loop through test cases ---\n    #\n    for spec in test_cases_spec:\n        # Step 1: Generate or retrieve the data vector y\n        if spec[\"y_gen\"] == \"use_case_1\":\n            y = y_case1\n        else:\n            y_params = spec[\"y_gen\"]\n            y = (y_params[\"b_true\"] * one_vec +\n                 y_params[\"q_true\"] * (G @ y_params[\"s_true\"]) +\n                 y_params[\"n\"] +\n                 y_params[\"delta\"])\n            if y_case1 is None:\n                y_case1 = y\n\n        retrieval_params = spec[\"retrieval\"]\n\n        # Step 2: Define objective function for the optimizer\n        def objective_func(q, y, Gs0, GGt, one_vec, ret_params):\n            logL, _ = get_profile_logL_and_b(q, y, Gs0, GGt, one_vec, ret_params)\n            return -logL\n\n        # Step 3: Find q_hat by maximizing the profile log-likelihood\n        opt_result = minimize_scalar(\n            objective_func,\n            args=(y, Gs0, GGt, one_vec, retrieval_params),\n            bounds=(0, 10),\n            method='bounded'\n        )\n        q_hat = opt_result.x\n        \n        # Step 4: Calculate the final b_hat and log-likelihood at q_hat\n        log_L_hat, b_hat = get_profile_logL_and_b(q_hat, y, Gs0, GGt, one_vec, retrieval_params)\n\n        all_results.extend([q_hat, b_hat, log_L_hat])\n\n    #\n    # --- Final Print Statement ---\n    #\n    print(f\"[{','.join(f'{x:.7f}' for x in all_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A well-defined posterior distribution is only useful if it can be explored efficiently. This final practice addresses a critical challenge in high-dimensional retrievals: the poor conditioning of the posterior, which can hinder sampling algorithms like Markov Chain Monte Carlo (MCMC). You will implement a 'prior-whitening' transformation and demonstrate quantitatively how this reparameterization can improve the posterior geometry, leading to more efficient and reliable inference .",
            "id": "4153635",
            "problem": "Consider a linearized atmospheric retrieval for an exoplanet transmission spectrum around a reference state. The measurement model is given by a linear map from the state to the data with additive Gaussian noise. Let $x \\in \\mathbb{R}^n$ be the vector of $n$ atmospheric parameters (for example, logarithms of gas mixing ratios and a temperature parameter, treated here as dimensionless for generality), and let $y \\in \\mathbb{R}^m$ be the vector of $m$ spectral data points. The forward model is linearized as\n$$\ny = K x + \\varepsilon,\n$$\nwhere $K \\in \\mathbb{R}^{m \\times n}$ is the Jacobian matrix of the forward model evaluated at the reference state, and $\\varepsilon \\sim \\mathcal{N}(0, N)$ is zero-mean Gaussian noise with covariance $N \\in \\mathbb{R}^{m \\times m}$, which is symmetric positive definite. The prior for $x$ is Gaussian,\n$$\nx \\sim \\mathcal{N}(\\mu, S),\n$$\nwith mean $\\mu \\in \\mathbb{R}^n$ and covariance $S \\in \\mathbb{R}^{n \\times n}$, which is symmetric positive definite.\n\nIn high-dimensional atmospheric retrievals, Markov Chain Monte Carlo (MCMC) sampling efficiency can degrade when parameters are strongly correlated or scale-separated. A classic remedy is to reparameterize the state vector using a prior-whitening transform. Let $L \\in \\mathbb{R}^{n \\times n}$ be the lower-triangular Cholesky factor of $S$ such that $S = L L^\\top$. Define the reparameterized variable $z \\in \\mathbb{R}^n$ by\n$$\nx = \\mu + L z,\n$$\nwhich implies a standard normal prior $z \\sim \\mathcal{N}(0, I)$ with $I \\in \\mathbb{R}^{n \\times n}$ the identity matrix. In terms of $z$, the measurement model becomes\n$$\ny = K \\mu + K L z + \\varepsilon.\n$$\n\nStarting from Bayes’ theorem and the given Gaussian prior and likelihood, derive from first principles the posterior distributions in both the original parameterization and the prior-whitened parameterization. Your derivation must begin at the definitions above and proceed using standard rules for multivariate Gaussians (completing the square), without invoking any shortcut formulas not justified by the fundamental setup.\n\nDefine the notion of numerical conditioning for a symmetric positive definite covariance matrix. Specifically, for a symmetric positive definite matrix $A$, define its $2$-norm condition number by\n$$\n\\kappa_2(A) = \\frac{\\lambda_{\\max}(A)}{\\lambda_{\\min}(A)},\n$$\nwhere $\\lambda_{\\max}(A)$ and $\\lambda_{\\min}(A)$ are the largest and smallest eigenvalues of $A$, respectively.\n\nImplement a program that, for each provided test case, computes the posterior covariance in the original parameterization and in the prior-whitened parameterization, and then outputs the ratio of their condition numbers, namely\n$$\nr = \\frac{\\kappa_2(\\Sigma_x)}{\\kappa_2(\\Sigma_z)},\n$$\nwhere $\\Sigma_x$ is the posterior covariance of $x$ and $\\Sigma_z$ is the posterior covariance of $z$. A value $r > 1$ indicates that the prior-whitened parameterization yields a better-conditioned posterior than the original parameterization.\n\nAll quantities in this problem are dimensionless. Express each ratio $r$ as a decimal number rounded to six decimal places.\n\nTest suite:\n- Case 1 (correlated sensitivities and prior correlations): Let $n = 3$, $m = 3$, and\n$$\nK_1 = \\begin{pmatrix}\n1.0 & 0.9 & 0.1 \\\\\n0.9 & 1.0 & 0.2 \\\\\n0.1 & 0.2 & 0.5\n\\end{pmatrix},\\quad\nN_1 = \\operatorname{diag}(0.01,\\,0.02,\\,0.015),\\quad\nS_1 = \\begin{pmatrix}\n0.5 & 0.45 & 0.05 \\\\\n0.45 & 0.5 & 0.05 \\\\\n0.05 & 0.05 & 0.2\n\\end{pmatrix}.\n$$\nTake $\\mu_1 = \\begin{pmatrix}0 \\\\ 0 \\\\ 0\\end{pmatrix}$.\n- Case 2 (nearly collinear columns in $K$ and broad correlated prior): Let $n = 3$, $m = 3$, and\n$$\nK_2 = \\begin{pmatrix}\n1.0 & 1.0 & 0.0 \\\\\n0.99 & 1.0 & 0.01 \\\\\n0.0 & 0.01 & 0.0\n\\end{pmatrix},\\quad\nN_2 = \\operatorname{diag}(1.0,\\,1.0,\\,1.0),\\quad\nS_2 = \\begin{pmatrix}\n2.0 & 1.8 & 0.0 \\\\\n1.8 & 2.0 & 0.0 \\\\\n0.0 & 0.0 & 0.5\n\\end{pmatrix}.\n$$\nTake $\\mu_2 = \\begin{pmatrix}0 \\\\ 0 \\\\ 0\\end{pmatrix}$.\n- Case 3 (orthogonal sensitivities and low noise): Let $n = 3$, $m = 3$, and\n$$\nK_3 = I = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix},\\quad\nN_3 = \\operatorname{diag}(10^{-4},\\,10^{-4},\\,10^{-4}),\\quad\nS_3 = \\operatorname{diag}(1.0,\\,1.0,\\,1.0).\n$$\nTake $\\mu_3 = \\begin{pmatrix}0 \\\\ 0 \\\\ 0\\end{pmatrix}$.\n\nYour program should produce a single line of output containing the three computed ratios as a comma-separated list enclosed in square brackets, for example, $[r_1,r_2,r_3]$, with each $r_i$ rounded to six decimal places. No other text should be printed.",
            "solution": "The user has requested a derivation of posterior distributions for a linearized Bayesian atmospheric retrieval problem in both its original and prior-whitened parameterizations, followed by a numerical implementation to compare the conditioning of these posteriors.\n\n### 1. Derivation of Posterior Distributions\n\nThe problem is defined by a Gaussian likelihood and a Gaussian prior. The posterior distribution is found using Bayes' theorem, which states that the posterior probability density function (PDF) is proportional to the product of the likelihood PDF and the prior PDF. For a multivariate Gaussian distribution $\\mathcal{N}(v | \\bar{v}, \\Sigma_v)$, its PDF is given by $p(v) \\propto \\exp\\left(-\\frac{1}{2} (v - \\bar{v})^\\top \\Sigma_v^{-1} (v - \\bar{v})\\right)$.\n\n#### 1.1 Posterior for Original Parameters ($x$)\n\nThe model for the original parameters $x \\in \\mathbb{R}^n$ is:\n-   Likelihood: $p(y|x) = \\mathcal{N}(y | Kx, N)$, where $y \\in \\mathbb{R}^m$, $K \\in \\mathbb{R}^{m \\times n}$, and $N$ is the noise covariance matrix.\n-   Prior: $p(x) = \\mathcal{N}(x | \\mu, S)$, where $\\mu \\in \\mathbb{R}^n$ and $S$ is the prior covariance matrix.\n\nAccording to Bayes' theorem, the posterior PDF $p(x|y)$ is:\n$$\np(x|y) \\propto p(y|x)p(x)\n$$\nSubstituting the Gaussian forms:\n$$\np(x|y) \\propto \\exp\\left( -\\frac{1}{2} (y - Kx)^\\top N^{-1} (y - Kx) \\right) \\cdot \\exp\\left( -\\frac{1}{2} (x - \\mu)^\\top S^{-1} (x - \\mu) \\right)\n$$\n$$\np(x|y) \\propto \\exp\\left( -\\frac{1}{2} \\left[ (y - Kx)^\\top N^{-1} (y - Kx) + (x - \\mu)^\\top S^{-1} (x - \\mu) \\right] \\right)\n$$\nTo identify the form of the posterior distribution, we expand the quadratic expression in the exponent, which we denote as $J(x)$:\n$$\nJ(x) = (y^\\top - x^\\top K^\\top) N^{-1} (y - Kx) + (x^\\top - \\mu^\\top) S^{-1} (x - \\mu)\n$$\n$$\nJ(x) = (y^\\top N^{-1} y - y^\\top N^{-1} Kx - x^\\top K^\\top N^{-1} y + x^\\top K^\\top N^{-1} Kx) + (x^\\top S^{-1} x - x^\\top S^{-1} \\mu - \\mu^\\top S^{-1} x + \\mu^\\top S^{-1} \\mu)\n$$\nRecognizing that the transpose of a scalar is itself (e.g., $x^\\top K^\\top N^{-1} y = y^\\top N^{-1} Kx$), we combine the terms linear in $x$:\n$$\nJ(x) = y^\\top N^{-1} y - 2x^\\top K^\\top N^{-1} y + x^\\top K^\\top N^{-1} Kx + x^\\top S^{-1} x - 2x^\\top S^{-1} \\mu + \\mu^\\top S^{-1} \\mu\n$$\nGrouping terms by powers of $x$:\n$$\nJ(x) = x^\\top (K^\\top N^{-1} K + S^{-1}) x - 2x^\\top (K^\\top N^{-1} y + S^{-1}\\mu) + \\text{const}\n$$\nwhere `const` includes all terms not dependent on $x$. This is a quadratic form in $x$, which implies the posterior distribution is also Gaussian, say $p(x|y) = \\mathcal{N}(x | \\mu_x, \\Sigma_x)$. The exponent of its PDF is $-\\frac{1}{2}(x - \\mu_x)^\\top \\Sigma_x^{-1} (x - \\mu_x)$, which expands to:\n$$\n-\\frac{1}{2} (x^\\top \\Sigma_x^{-1} x - 2x^\\top \\Sigma_x^{-1}\\mu_x + \\mu_x^\\top \\Sigma_x^{-1} \\mu_x)\n$$\nBy comparing the quadratic term in $x$ from $J(x)$ with this form, we identify the inverse posterior covariance (also known as the precision matrix):\n$$\n\\Sigma_x^{-1} = K^\\top N^{-1} K + S^{-1}\n$$\nThe posterior covariance for the original parameterization $x$ is therefore:\n$$\n\\Sigma_x = (K^\\top N^{-1} K + S^{-1})^{-1}\n$$\nNote that the posterior covariance $\\Sigma_x$ is independent of the measurement data $y$.\n\n#### 1.2 Posterior for Prior-Whitened Parameters ($z$)\n\nThe prior-whitened parameters $z \\in \\mathbb{R}^n$ are related to $x$ by the transformation $x = \\mu + Lz$, where $S = LL^\\top$ is the Cholesky decomposition of the prior covariance $S$. This transformation implies a standard Gaussian prior for $z$: $p(z) = \\mathcal{N}(z | 0, I)$, where $I$ is the identity matrix.\n\nWe rewrite the measurement model in terms of $z$:\n$$\ny = K(\\mu + Lz) + \\varepsilon = K\\mu + KLz + \\varepsilon\n$$\nLet's define a shifted data vector $y' = y - K\\mu$. The model becomes $y' = (KL)z + \\varepsilon$.\n-   Likelihood: $p(y|z) = p(y'|z) = \\mathcal{N}(y' | KLz, N)$.\n-   Prior: $p(z) = \\mathcal{N}(z | 0, I)$.\n\nApplying Bayes' theorem for $z$:\n$$\np(z|y) \\propto p(y|z)p(z)\n$$\n$$\np(z|y) \\propto \\exp\\left( -\\frac{1}{2} (y' - KLz)^\\top N^{-1} (y' - KLz) \\right) \\cdot \\exp\\left( -\\frac{1}{2} z^\\top I^{-1} z \\right)\n$$\n$$\np(z|y) \\propto \\exp\\left( -\\frac{1}{2} \\left[ (y - K\\mu - KLz)^\\top N^{-1} (y - K\\mu - KLz) + z^\\top z \\right] \\right)\n$$\nLet the term in the exponent be $-J(z)/2$. We expand $J(z)$:\n$$\nJ(z) = (y' - KLz)^\\top N^{-1} (y' - KLz) + z^\\top z\n$$\n$$\nJ(z) = z^\\top (KL)^\\top N^{-1} (KL)z - 2z^\\top (KL)^\\top N^{-1} y' + (y')^\\top N^{-1} y' + z^\\top z\n$$\nGrouping terms by powers of $z$:\n$$\nJ(z) = z^\\top (L^\\top K^\\top N^{-1} K L + I) z - 2z^\\top L^\\top K^\\top N^{-1} y' + \\text{const}\n$$\nThis quadratic form implies a Gaussian posterior for $z$, $p(z|y) = \\mathcal{N}(z | \\mu_z, \\Sigma_z)$. By comparing the quadratic term with that of a general Gaussian PDF, we identify the inverse posterior covariance for $z$:\n$$\n\\Sigma_z^{-1} = L^\\top K^\\top N^{-1} K L + I\n$$\nThe posterior covariance for the prior-whitened parameterization $z$ is:\n$$\n\\Sigma_z = (L^\\top K^\\top N^{-1} K L + I)^{-1}\n$$\n\n### 2. Numerical Conditioning and Computational Procedure\n\nThe numerical conditioning of a symmetric positive definite (SPD) covariance matrix $A$ is given by its $2$-norm condition number:\n$$\n\\kappa_2(A) = \\frac{\\lambda_{\\max}(A)}{\\lambda_{\\min}(A)}\n$$\nwhere $\\lambda_{\\max}(A)$ and $\\lambda_{\\min}(A)$ are the maximum and minimum eigenvalues of $A$, respectively. A smaller condition number (closer to $1$) indicates a better-conditioned matrix.\n\nThe program will compute the ratio $r = \\kappa_2(\\Sigma_x) / \\kappa_2(\\Sigma_z)$ for each test case. The following steps are performed:\n1.  Given matrices $K$, $S$, and the diagonal of $N$, construct the full matrices.\n2.  Compute the Cholesky decomposition of the prior covariance $S$ to find the lower-triangular matrix $L$.\n3.  Compute the inverse matrices $N^{-1}$ and $S^{-1}$.\n4.  Calculate the posterior covariance $\\Sigma_x = (K^\\top N^{-1} K + S^{-1})^{-1}$.\n5.  Calculate the posterior covariance $\\Sigma_z = (L^\\top K^\\top N^{-1} K L + I)^{-1}$.\n6.  For both $\\Sigma_x$ and $\\Sigma_z$, compute their eigenvalues and find the ratio of the largest to the smallest to obtain their respective condition numbers.\n7.  Compute the final ratio $r$ and round it to the specified precision.\n\nThis procedure is applied to each of the three test cases provided.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import cholesky\n\ndef solve():\n    \"\"\"\n    Solves the atmospheric retrieval conditioning problem for the given test cases.\n    \"\"\"\n\n    test_cases = [\n        # Case 1: Correlated sensitivities and prior\n        {\n            \"K\": np.array([\n                [1.0, 0.9, 0.1],\n                [0.9, 1.0, 0.2],\n                [0.1, 0.2, 0.5]\n            ]),\n            \"N_diag\": np.array([0.01, 0.02, 0.015]),\n            \"S\": np.array([\n                [0.5, 0.45, 0.05],\n                [0.45, 0.5, 0.05],\n                [0.05, 0.05, 0.2]\n            ]),\n        },\n        # Case 2: Nearly collinear columns in K and broad correlated prior\n        {\n            \"K\": np.array([\n                [1.0, 1.0, 0.0],\n                [0.99, 1.0, 0.01],\n                [0.0, 0.01, 0.0]\n            ]),\n            \"N_diag\": np.array([1.0, 1.0, 1.0]),\n            \"S\": np.array([\n                [2.0, 1.8, 0.0],\n                [1.8, 2.0, 0.0],\n                [0.0, 0.0, 0.5]\n            ]),\n        },\n        # Case 3: Orthogonal sensitivities and low noise\n        {\n            \"K\": np.array([\n                [1.0, 0.0, 0.0],\n                [0.0, 1.0, 0.0],\n                [0.0, 0.0, 1.0]\n            ]),\n            \"N_diag\": np.array([1e-4, 1e-4, 1e-4]),\n            \"S\": np.array([\n                [1.0, 0.0, 0.0],\n                [0.0, 1.0, 0.0],\n                [0.0, 0.0, 1.0]\n            ]),\n        }\n    ]\n\n    results = []\n\n    for case in test_cases:\n        K = case[\"K\"]\n        S = case[\"S\"]\n        N = np.diag(case[\"N_diag\"])\n        \n        n = K.shape[1]\n        I = np.identity(n)\n\n        # Inverses needed for the formulas.\n        N_inv = np.linalg.inv(N)\n        S_inv = np.linalg.inv(S)\n        \n        # Cholesky factor of the prior covariance S such that S = L L^T\n        # We use scipy.linalg.cholesky to ensure we get the lower triangular factor.\n        L = cholesky(S, lower=True)\n\n        # 1. Posterior covariance for original parameters x\n        # Sigma_x = (K^T N^-1 K + S^-1)^-1\n        precision_x = K.T @ N_inv @ K + S_inv\n        Sigma_x = np.linalg.inv(precision_x)\n\n        # 2. Posterior covariance for whitened parameters z\n        # Sigma_z = (L^T K^T N^-1 K L + I)^-1\n        precision_z = L.T @ K.T @ N_inv @ K @ L + I\n        Sigma_z = np.linalg.inv(precision_z)\n        \n        def condition_number_2(A):\n            \"\"\"\n            Calculates the 2-norm condition number for a symmetric positive definite matrix.\n            kappa_2(A) = lambda_max(A) / lambda_min(A)\n            \"\"\"\n            # eigvalsh is for hermitian (or real symmetric) matrices.\n            # It returns eigenvalues in ascending order.\n            eigenvalues = np.linalg.eigvalsh(A)\n            return eigenvalues[-1] / eigenvalues[0]\n\n        kappa_Sigma_x = condition_number_2(Sigma_x)\n        kappa_Sigma_z = condition_number_2(Sigma_z)\n\n        # Compute the ratio of condition numbers\n        ratio = kappa_Sigma_x / kappa_Sigma_z\n        results.append(f\"{ratio:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}