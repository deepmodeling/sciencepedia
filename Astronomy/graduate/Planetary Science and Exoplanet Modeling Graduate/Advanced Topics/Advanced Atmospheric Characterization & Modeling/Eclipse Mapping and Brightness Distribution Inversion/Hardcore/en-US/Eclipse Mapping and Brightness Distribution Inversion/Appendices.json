{
    "hands_on_practices": [
        {
            "introduction": "Before attempting to solve an inverse problem, it is crucial to understand the structure and limitations of the forward model itself. The Singular Value Decomposition (SVD) is a powerful mathematical tool that allows us to dissect the linear operator connecting the planet's map to the observed light curve. This practice  will guide you through constructing a simplified eclipse mapping operator and using its singular value spectrum to quantify the practical number of map features that can be resolved from the data, revealing the problem's inherently ill-posed nature.",
            "id": "4159111",
            "problem": "Consider the forward problem of eclipse mapping for a thermally emitting exoplanet observed during secondary eclipse. Assume the planet emits isotropically (no beaming), and the measured flux at time $t$ is the line-of-sight integral of the brightness distribution $g(x)$ over the visible, unocculted portion of the projected planetary disk. Model the planet’s projected disk by the unit circle, and reduce the brightness distribution to a one-dimensional function along the transit chord coordinate $x \\in [-1,1]$. The instantaneous contribution of a longitudinal slice at position $x$ to the observed flux is weighted by the projected chord length, which is proportional to $\\sqrt{1-x^2}$.\n\nFor ingress, define the occultation boundary as $x_{b}(t) = 1 - 2t$ with dimensionless time $t \\in [0,1]$, so that the unocculted region at time $t$ is $\\{x \\in [-1,1] : x \\le x_{b}(t)\\}$. For egress, define $x_{b}(t) = -1 + 2t$ with $t \\in [0,1]$, and the unocculted region is $\\{x \\in [-1,1] : x \\ge x_{b}(t)\\}$. Discretize $x$ into $N$ uniformly spaced points $x_i$ over $[-1,1]$ and time $t$ into $T$ uniformly spaced midpoints $t_j = \\frac{j+1/2}{T}$ for $j = 0,\\dots,T-1$. Construct the linear forward operator $A \\in \\mathbb{R}^{T \\times N}$ for ingress by\n$$\nA_{j,i} = \\sqrt{1 - x_i^2}\\,\\mathbb{I}\\{x_i \\le x_b(t_j)\\}\\,\\Delta x,\n$$\nand for egress by\n$$\nA_{j,i} = \\sqrt{1 - x_i^2}\\,\\mathbb{I}\\{x_i \\ge x_b(t_j)\\}\\,\\Delta x,\n$$\nwhere $\\Delta x$ is the grid spacing and $\\mathbb{I}\\{\\cdot\\}$ is the indicator function. For the combined ingress+egress case, stack the ingress and egress operators vertically to form $A \\in \\mathbb{R}^{(2T) \\times N}$.\n\nStarting from this construction, use singular value decomposition (SVD) to analyze the practical resolving power of eclipse mapping. For a given operator $A$, compute its singular values $\\{s_k\\}_{k=1}^{r}$, where $r = \\min(T,N)$ for ingress or egress individually and $r = \\min(2T,N)$ for the combined case. Define the cumulative energy fraction captured by the first $K$ singular modes as\n$$\nE(K) = \\frac{\\sum_{k=1}^{K} s_k^2}{\\sum_{k=1}^{r} s_k^2},\n$$\nwhich equals the fraction of the squared Frobenius norm of $A$ explained by its top $K$ singular values. Interpret $E(K)$ as a function of mode index $K$ to assess how many modes are practically resolvable before diminishing returns set in.\n\nYour program must construct $A$ according to the rules above (no user input) and compute, for each test case, the smallest mode index $K_{0.9}$ such that $E(K_{0.9}) \\ge 0.9$ and the smallest mode index $K_{0.99}$ such that $E(K_{0.99}) \\ge 0.99$. There are no physical units in this problem; the quantities are dimensionless by construction.\n\nTest suite:\n- Case $1$ (happy path, ingress): $N=64$, $T=64$.\n- Case $2$ (undersampled ingress): $N=64$, $T=8$.\n- Case $3$ (egress symmetry check): $N=64$, $T=64$.\n- Case $4$ (combined ingress+egress): $N=64$, $T=64$ for each phase, stacked to yield $A \\in \\mathbb{R}^{128 \\times 64}$.\n\nFor each case, compute $K_{0.9}$ and $K_{0.99}$ as integers. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order\n$$\n[K_{0.9}^{(1)},K_{0.99}^{(1)},K_{0.9}^{(2)},K_{0.99}^{(2)},K_{0.9}^{(3)},K_{0.99}^{(3)},K_{0.9}^{(4)},K_{0.99}^{(4)}],\n$$\nwhere the superscripts denote the case index. All intermediate computations must adhere to the definitions above; the final outputs are integers with no units.",
            "solution": "The user has provided a problem from the field of planetary science, specifically concerning the analysis of an exoplanet's thermal brightness distribution using eclipse mapping. The task is to construct a linear forward model that maps the planet's one-dimensional brightness profile to the observed light curve during a secondary eclipse and then to analyze this model's resolving power using Singular Value Decomposition (SVD).\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n*   **Physical Model**: The planet is a thermally emitting, isotropic source. The observed flux is the line-of-sight integral of the brightness distribution $g(x)$ over the visible, unocculted portion of the projected disk.\n*   **Geometric Model**: The projected planetary disk is the unit circle. The brightness distribution is simplified to a one-dimensional function $g(x)$ along the transit chord coordinate $x \\in [-1,1]$.\n*   **Flux Weighting**: A slice at position $x$ contributes to the flux with a weight proportional to the projected chord length, $\\sqrt{1-x^2}$.\n*   **Time and Occultation Dynamics**:\n    *   Dimensionless time $t \\in [0,1]$.\n    *   Ingress occultation boundary: $x_{b}(t) = 1 - 2t$. Unocculted region: $\\{x \\in [-1,1] : x \\le x_{b}(t)\\}$.\n    *   Egress occultation boundary: $x_{b}(t) = -1 + 2t$. Unocculted region: $\\{x \\in [-1,1] : x \\ge x_{b}(t)\\}$.\n*   **Discretization**:\n    *   Spatial coordinate $x$ is discretized into $N$ uniformly spaced points $x_i$ over $[-1,1]$.\n    *   Time coordinate $t$ is discretized into $T$ uniformly spaced midpoints $t_j = \\frac{j+1/2}{T}$ for $j = 0, \\dots, T-1$.\n*   **Forward Operator Definition**:\n    *   The grid spacing for $x$ is $\\Delta x$.\n    *   The indicator function is $\\mathbb{I}\\{\\cdot\\}$.\n    *   Ingress operator $A \\in \\mathbb{R}^{T \\times N}$: $A_{j,i} = \\sqrt{1 - x_i^2}\\,\\mathbb{I}\\{x_i \\le x_b(t_j)\\}\\,\\Delta x$.\n    *   Egress operator $A \\in \\mathbb{R}^{T \\times N}$: $A_{j,i} = \\sqrt{1 - x_i^2}\\,\\mathbb{I}\\{x_i \\ge x_b(t_j)\\}\\,\\Delta x$.\n    *   Combined operator $A \\in \\mathbb{R}^{(2T) \\times N}$: Formed by vertically stacking the ingress and egress operators.\n*   **SVD Analysis**:\n    *   Singular values of $A$: $\\{s_k\\}_{k=1}^{r}$, where $r = \\min(\\text{rows of } A, \\text{cols of } A)$.\n    *   Cumulative energy fraction: $E(K) = \\frac{\\sum_{k=1}^{K} s_k^2}{\\sum_{k=1}^{r} s_k^2}$.\n*   **Objective**:\n    *   Find the smallest integer mode index $K_{0.9}$ such that $E(K_{0.9}) \\ge 0.9$.\n    *   Find the smallest integer mode index $K_{0.99}$ such that $E(K_{0.99}) \\ge 0.99$.\n*   **Test Suite**:\n    *   Case 1: Ingress, $N=64$, $T=64$.\n    *   Case 2: Ingress, $N=64$, $T=8$.\n    *   Case 3: Egress, $N=64$, $T=64$.\n    *   Case 4: Combined Ingress+Egress, $N=64$, $T=64$ (for each phase).\n\n**Step 2: Validate Using Extracted Givens**\n\n1.  **Scientifically Grounded**: The problem is a well-established and standard simplified model in exoplanet science for introducing the concepts of eclipse mapping and the associated inverse problem. The formulation of the forward operator as a weighted integral is physically correct for the stated assumptions. The use of SVD is a fundamental technique in inverse theory to analyze the information content and conditioning of such problems.\n\n2.  **Well-Posed**: The computational task is well-posed. All parameters, definitions, and discretization schemes are specified unambiguously, leading to a single, uniquely computable numerical result for each test case. While the underlying physical inverse problem (recovering $g(x)$ from flux data) is ill-posed, the task here is to analyze the forward operator itself, which is a well-defined mathematical procedure.\n\n3.  **Objective**: The problem is stated in precise, objective mathematical language. There are no subjective elements.\n\n4.  **No Flaws Found**: The problem does not violate any of the invalidity criteria. It is scientifically sound (as a model), complete, consistent, and computationally feasible. The task is substantive and directly addresses a core concept in the analysis of ill-posed inverse problems.\n\n**Step 3: Verdict and Action**\n\nThe problem is **valid**. A solution will be provided.\n\n### Solution\n\nThe problem requires us to construct a linear forward operator, denoted by the matrix $A$, which models the process of measuring an exoplanet's thermal flux during a secondary eclipse. We will then use Singular Value Decomposition (SVD) to assess the number of independent pieces of information (or \"modes\") that can be reliably extracted from the data, a measure of the system's practical resolving power.\n\n**1. Discretization of the Domain**\nThe planetary brightness is modeled along a one-dimensional coordinate $x \\in [-1, 1]$. We discretize this interval into $N$ uniformly spaced points, $\\{x_i\\}_{i=0}^{N-1}$. These points can be generated using $x_i = -1 + i\\Delta x$, where the grid spacing is $\\Delta x = \\frac{2}{N-1}$. The corresponding vector of spatial coordinates is $\\mathbf{x} = [x_0, x_1, \\dots, x_{N-1}]$.\n\nThe observation time $t \\in [0, 1]$ is discretized into $T$ time points, which are taken as the midpoints of $T$ uniform intervals. The $j$-th time point is $t_j = \\frac{j+1/2}{T}$ for $j=0, \\dots, T-1$. The corresponding vector of observation times is $\\mathbf{t} = [t_0, t_1, \\dots, t_{T-1}]$.\n\n**2. Construction of the Forward Operator $A$**\nThe forward model relating the discretized brightness distribution vector $\\mathbf{g} \\in \\mathbb{R}^{N}$ to the measured flux vector $\\mathbf{f} \\in \\mathbb{R}^{T}$ is given by the linear equation $\\mathbf{f} = A\\mathbf{g}$. The matrix $A$ encapsulates the geometry of the occultation. An element $A_{j,i}$ represents the contribution of the brightness at spatial location $x_i$ to the total measured flux at time $t_j$.\n\nAs defined, an element $A_{j,i}$ is the product of the chord-length weighting factor, $\\sqrt{1 - x_i^2}$, and an indicator function that is unity if the point $x_i$ is visible at time $t_j$ and zero otherwise, all scaled by the grid spacing $\\Delta x$.\n\nFor an **ingress** event, the occulting body moves from right to left. The boundary of the occultation is at $x_b(t_j) = 1 - 2t_j$. A point $x_i$ is visible if it is to the left of the boundary, i.e., $x_i \\le x_b(t_j)$. The matrix elements are:\n$$\nA_{j,i}^{\\text{ingress}} = \\sqrt{1 - x_i^2} \\cdot \\mathbb{I}\\{x_i \\le 1 - 2t_j\\} \\cdot \\Delta x\n$$\n\nFor an **egress** event, the occulting body moves from left to right. The boundary is at $x_b(t_j) = -1 + 2t_j$. A point $x_i$ is visible if it is to the right of the boundary, i.e., $x_i \\ge x_b(t_j)$. The matrix elements are:\n$$\nA_{j,i}^{\\text{egress}} = \\sqrt{1 - x_i^2} \\cdot \\mathbb{I}\\{x_i \\ge -1 + 2t_j\\} \\cdot \\Delta x\n$$\n\nFor the **combined ingress and egress** case, we assume we have measurements from both phases. This yields more information. The forward operator is constructed by vertically stacking the ingress and egress matrices. If $A_{\\text{ingress}}$ and $A_{\\text{egress}}$ are both of size $T \\times N$, the combined operator $A_{\\text{comb}}$ will be of size $(2T) \\times N$:\n$$\nA_{\\text{comb}} = \\begin{pmatrix} A_{\\text{ingress}} \\\\ A_{\\text{egress}} \\end{pmatrix}\n$$\n\n**3. Analysis via Singular Value Decomposition (SVD)**\nThe SVD of the matrix $A$ is a factorization $A = U S V^T$, where $U$ and $V$ are orthogonal matrices whose columns are the left and right singular vectors, respectively, and $S$ is a diagonal matrix containing the singular values, $\\{s_k\\}$, sorted in descending order. The number of non-zero singular values is the rank of the matrix, $r$.\n\nThe singular values quantify the \"amplification\" of corresponding modes from the input space (brightness maps) to the output space (light curves). A steep drop-off in singular values indicates that the problem is ill-conditioned; high-frequency components in the brightness map are highly de-amplified and are thus difficult to recover from noisy data.\n\nThe squared Frobenius norm of $A$, $\\|A\\|_F^2 = \\sum_{j,i} A_{j,i}^2$, represents the total \"energy\" of the linear operator. It is also equal to the sum of the squared singular values: $\\|A\\|_F^2 = \\sum_{k=1}^{r} s_k^2$.\n\nWe define the cumulative energy fraction $E(K)$ as the fraction of the total energy captured by the first $K$ singular modes:\n$$\nE(K) = \\frac{\\sum_{k=1}^{K} s_k^2}{\\sum_{k=1}^{r} s_k^2}\n$$\nThis quantity indicates how well the operator $A$ can be approximated by a rank-$K$ matrix. A value of $K$ for which $E(K)$ is close to $1$ (e.g., $0.9$ or $0.99$) gives us an estimate of the \"effective rank\" of the system, or the number of independent parameters of the brightness map that can be robustly constrained by the data.\n\n**4. Computation of $K_{0.9}$ and $K_{0.99}$**\nFor each test case, the procedure is as follows:\n1.  Set the parameters $N$ and $T$.\n2.  Construct the appropriate matrix $A$ (ingress, egress, or combined).\n3.  Compute the singular values $s_k$ of $A$.\n4.  Calculate the squared singular values $s_k^2$.\n5.  Compute the cumulative sum of $s_k^2$.\n6.  Normalize the cumulative sum by the total sum of $s_k^2$ to obtain the cumulative energy fraction array, $E(K)$.\n7.  Find the smallest integer $K_{0.9}$ such that $E(K_{0.9}) \\ge 0.9$. This is the first index (plus one, for 1-based counting) where the cumulative energy fraction array exceeds or equals $0.9$.\n8.  Similarly, find the smallest integer $K_{0.99}$ such that $E(K_{0.99}) \\ge 0.99$.\n\nThis procedure will be applied to the four specified test cases. The results will quantify how sampling ($T$) and the inclusion of more data (combined ingress/egress) affect the number of resolvable components of the planet's brightness map. We expect the egress-only case to be symmetric to the ingress-only case, yielding identical results. The undersampled case ($T<N$) will have fewer resolvable modes, limited by $T$. The combined case will have more resolvable modes than the single-phase cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the eclipse mapping SVD analysis problem for the given test cases.\n    \"\"\"\n\n    test_cases = [\n        # Case 1 (happy path, ingress)\n        {'N': 64, 'T': 64, 'type': 'ingress'},\n        # Case 2 (undersampled ingress)\n        {'N': 64, 'T': 8, 'type': 'ingress'},\n        # Case 3 (egress symmetry check)\n        {'N': 64, 'T': 64, 'type': 'egress'},\n        # Case 4 (combined ingress+egress)\n        {'N': 64, 'T': 64, 'type': 'combined'},\n    ]\n\n    results = []\n\n    for case in test_cases:\n        N = case['N']\n        T = case['T']\n        case_type = case['type']\n\n        # 1. Discretization\n        x_i = np.linspace(-1.0, 1.0, N)\n        delta_x = 2.0 / (N - 1)\n        t_j = (np.arange(T) + 0.5) / T\n\n        # 2. Construct the Forward Operator A\n        A = None\n        # Pre-compute chord-length weights\n        # Use np.maximum to avoid floating point issues at the boundaries\n        weights = np.sqrt(np.maximum(0, 1 - x_i**2))\n\n        if case_type == 'ingress':\n            # Ingress boundary: x_b(t) = 1 - 2t\n            x_b = 1 - 2 * t_j\n            # Create a T x N boolean matrix for the indicator function\n            # x_i is broadcasted across rows, x_b is broadcasted across columns\n            indicator = x_i[np.newaxis, :] <= x_b[:, np.newaxis]\n            # Construct A\n            A = np.sqrt(1 - x_i**2)[np.newaxis, :] * indicator * delta_x\n\n        elif case_type == 'egress':\n            # Egress boundary: x_b(t) = -1 + 2t\n            x_b = -1 + 2 * t_j\n            # Indicator function: x_i >= x_b\n            indicator = x_i[np.newaxis, :] >= x_b[:, np.newaxis]\n            # Construct A\n            A = np.sqrt(1 - x_i**2)[np.newaxis, :] * indicator * delta_x\n\n        elif case_type == 'combined':\n            # Ingress part\n            x_b_ing = 1 - 2 * t_j\n            indicator_ing = x_i[np.newaxis, :] <= x_b_ing[:, np.newaxis]\n            A_ing = np.sqrt(1 - x_i**2)[np.newaxis, :] * indicator_ing * delta_x\n            \n            # Egress part\n            x_b_egr = -1 + 2 * t_j\n            indicator_egr = x_i[np.newaxis, :] >= x_b_egr[:, np.newaxis]\n            A_egr = np.sqrt(1 - x_i**2)[np.newaxis, :] * indicator_egr * delta_x\n            \n            # Stack vertically\n            A = np.vstack((A_ing, A_egr))\n\n        # 3. Analysis via Singular Value Decomposition\n        # We only need the singular values, so compute_uv=False is efficient\n        s = np.linalg.svd(A, compute_uv=False)\n\n        # 4. Cumulative Energy Analysis\n        s_squared = s**2\n        \n        # Handle cases where A is all zeros (s will be all zeros)\n        total_energy = np.sum(s_squared)\n        if total_energy == 0:\n            cumulative_energy = np.zeros_like(s_squared)\n        else:\n            cumulative_energy = np.cumsum(s_squared) / total_energy\n\n        # 5. Find K_0.9 and K_0.99\n        # np.searchsorted finds the index where an element would be inserted to\n        # maintain order. This gives the count of elements smaller than the target.\n        # Adding 1 gives the rank K needed to exceed the threshold.\n        # This is equivalent to finding the first index i where cumulative_energy[i] >= threshold\n        # and reporting i+1.\n        \n        # In the edge case of all singular values being 0, searchsorted would\n        # return len(cumulative_energy). Handle this by setting K to 0.\n        if total_energy == 0:\n            k_90 = 0\n            k_99 = 0\n        else:\n            k_90 = np.searchsorted(cumulative_energy, 0.9) + 1\n            k_99 = np.searchsorted(cumulative_energy, 0.99) + 1\n        \n        results.extend([k_90, k_99])\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Knowing that eclipse mapping is an ill-posed problem necessitates the use of regularization to obtain a stable and physically meaningful solution. However, a solution is incomplete without a measure of its uncertainty. This exercise  provides a fundamental look at how regularization impacts the statistical properties of the inverted map by tasking you with deriving the covariance matrix for a regularized linear estimator. This connects the abstract concept of a regularization penalty to the concrete, pixel-level error bars on your final map.",
            "id": "4159145",
            "problem": "Consider secondary eclipse mapping of a tidally locked exoplanet, where the planet’s dayside brightness distribution is parameterized by a two-pixel longitudinal map with unknown intensities $x \\in \\mathbb{R}^{2}$. The observed normalized flux time series $y \\in \\mathbb{R}^{3}$ during ingress/egress is modeled as a linear forward problem $y = A x + n$, where $A \\in \\mathbb{R}^{3 \\times 2}$ encodes the fractional visibility of each pixel at each observation time, and $n$ is zero-mean Gaussian noise with covariance $C_{n} \\in \\mathbb{R}^{3 \\times 3}$. To stabilize the inversion, a quadratic Tikhonov regularization is imposed with $R = \\lambda I$, where $I$ is the identity matrix and $\\lambda > 0$.\n\nStarting from the definitions of the linear model and Gaussian noise, and using only the standard properties of multivariate Gaussian distributions and linear estimators, derive the covariance of the linear estimator $\\hat{x}$ that minimizes the weighted least-squares objective with quadratic regularization. Then, for the specific eclipse geometry and noise model below, compute the pixel-level standard deviations (uncertainties) as the square roots of the diagonal entries of the covariance of $\\hat{x}$.\n\nUse the following matrices and parameter values:\n- Forward model\n$$\nA \\;=\\; \\begin{pmatrix}\n1 & 1 \\\\\n1 & 0 \\\\\n0 & 1\n\\end{pmatrix}.\n$$\nThis represents $3$ flux samples: the first with both pixels fully visible, the second with only the western pixel visible, and the third with only the eastern pixel visible.\n- Noise covariance (independent identically distributed samples)\n$$\nC_{n} \\;=\\; \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix}.\n$$\n- Regularization parameter\n$$\n\\lambda \\;=\\; 1.\n$$\n\nReport the two pixel-level uncertainties as a single row vector. Round your final numerical values to four significant figures. Express the uncertainties in the same arbitrary intensity units as $x$ (do not include units in the final boxed answer).",
            "solution": "The problem is valid as it represents a standard, well-posed inverse problem in exoplanet science based on established principles of linear algebra and statistics. All necessary data and parameters are provided, and there are no contradictions or ambiguities.\n\nThe problem asks for the covariance of a linear estimator for the pixel intensities $x$ and the corresponding uncertainties. The estimator $\\hat{x}$ is defined as the minimizer of a regularized, weighted least-squares objective function, $J(x)$.\n\nThe objective function consists of two parts: a data fidelity term and a regularization term. The data fidelity term is the Mahalanobis distance squared between the observed data $y$ and the model prediction $Ax$, which corresponds to the negative log-likelihood of the data given the parameters, assuming Gaussian noise. The regularization term is a quadratic penalty on the magnitude of the solution vector $x$.\n\nThe objective function to minimize is:\n$$\nJ(x) = (y - Ax)^T C_{n}^{-1} (y - Ax) + x^T R x\n$$\nGiven the regularization matrix $R = \\lambda I$, the expression becomes:\n$$\nJ(x) = (y - Ax)^T C_{n}^{-1} (y - Ax) + \\lambda x^T x\n$$\nTo find the estimator $\\hat{x}$ that minimizes $J(x)$, we compute the gradient of $J(x)$ with respect to $x$ and set it to zero. First, we expand the expression for $J(x)$:\n$$\nJ(x) = y^T C_{n}^{-1} y - y^T C_{n}^{-1} Ax - x^T A^T C_{n}^{-1} y + x^T A^T C_{n}^{-1} Ax + \\lambda x^T x\n$$\nSince $y^T C_{n}^{-1} Ax$ is a scalar, it is equal to its transpose, $(y^T C_{n}^{-1} Ax)^T = x^T A^T (C_{n}^{-1})^T y$. A covariance matrix and its inverse are symmetric, so $(C_{n}^{-1})^T = C_{n}^{-1}$. Thus, the two cross terms are identical.\n$$\nJ(x) = y^T C_{n}^{-1} y - 2 x^T A^T C_{n}^{-1} y + x^T (A^T C_{n}^{-1} A + \\lambda I) x\n$$\nThe gradient of $J(x)$ with respect to $x$ is:\n$$\n\\frac{\\partial J(x)}{\\partial x} = -2 A^T C_{n}^{-1} y + 2 (A^T C_{n}^{-1} A + \\lambda I) x\n$$\nSetting the gradient to zero to find the optimal $\\hat{x}$:\n$$\n-2 A^T C_{n}^{-1} y + 2 (A^T C_{n}^{-1} A + \\lambda I) \\hat{x} = 0\n$$\n$$\n(A^T C_{n}^{-1} A + \\lambda I) \\hat{x} = A^T C_{n}^{-1} y\n$$\nSolving for the estimator $\\hat{x}$:\n$$\n\\hat{x} = (A^T C_{n}^{-1} A + \\lambda I)^{-1} A^T C_{n}^{-1} y\n$$\nThis shows that $\\hat{x}$ is a linear estimator of the form $\\hat{x} = M y$, where $M = (A^T C_{n}^{-1} A + \\lambda I)^{-1} A^T C_{n}^{-1}$.\n\nThe covariance of the estimator, $C_{\\hat{x}} = \\text{Cov}(\\hat{x})$, is found by propagating the covariance of the data $y$. The model is $y = Ax + n$, where $x$ is a fixed (but unknown) parameter vector. The randomness in $y$ comes entirely from the noise $n$. Therefore, $\\text{Cov}(y) = \\text{Cov}(n) = C_n$.\nThe covariance of a linear transformation $My$ is given by $M \\text{Cov}(y) M^T$.\n$$\nC_{\\hat{x}} = \\text{Cov}(My) = M C_n M^T\n$$\nSubstituting the expression for $M$:\n$$\nC_{\\hat{x}} = \\left[(A^T C_{n}^{-1} A + \\lambda I)^{-1} A^T C_{n}^{-1}\\right] C_n \\left[(A^T C_{n}^{-1} A + \\lambda I)^{-1} A^T C_{n}^{-1}\\right]^T\n$$\nThe transpose of the second bracket is $M^T = (C_{n}^{-1})^T (A^T)^T ((A^T C_{n}^{-1} A + \\lambda I)^{-1})^T$. Since $C_{n}^{-1}$ is symmetric and $(A^T C_{n}^{-1} A + \\lambda I)$ is symmetric, this simplifies to $M^T = C_{n}^{-1} A (A^T C_{n}^{-1} A + \\lambda I)^{-1}$.\n$$\nC_{\\hat{x}} = (A^T C_{n}^{-1} A + \\lambda I)^{-1} A^T C_{n}^{-1} C_n C_{n}^{-1} A (A^T C_{n}^{-1} A + \\lambda I)^{-1}\n$$\nSince $C_{n}^{-1} C_n = I$, this simplifies to:\n$$\nC_{\\hat{x}} = (A^T C_{n}^{-1} A + \\lambda I)^{-1} (A^T C_{n}^{-1} A) (A^T C_{n}^{-1} A + \\lambda I)^{-1}\n$$\nNow, we substitute the given values:\n$A = \\begin{pmatrix} 1 & 1 \\\\ 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$, $C_n = I_3$ (the $3 \\times 3$ identity matrix), and $\\lambda = 1$.\nSince $C_n = I_3$, its inverse is also $C_{n}^{-1} = I_3$. Let's compute the matrix $H = A^T C_{n}^{-1} A = A^T I A = A^T A$.\n$$\nA^T = \\begin{pmatrix} 1 & 1 & 0 \\\\ 1 & 0 & 1 \\end{pmatrix}\n$$\n$$\nH = A^T A = \\begin{pmatrix} 1 & 1 & 0 \\\\ 1 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\\\ 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 1 + 1 \\cdot 1 & 1 \\cdot 1 + 1 \\cdot 0 \\\\ 1 \\cdot 1 + 0 \\cdot 1 & 1 \\cdot 1 + 1 \\cdot 1 \\end{pmatrix} = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}\n$$\nNext, we compute the matrix to be inverted, $H + \\lambda I_2$:\n$$\nH + \\lambda I_2 = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} + 1 \\cdot \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 3 & 1 \\\\ 1 & 3 \\end{pmatrix}\n$$\nThe inverse of this $2 \\times 2$ matrix is:\n$$\n(H + \\lambda I_2)^{-1} = \\frac{1}{3 \\cdot 3 - 1 \\cdot 1} \\begin{pmatrix} 3 & -1 \\\\ -1 & 3 \\end{pmatrix} = \\frac{1}{8} \\begin{pmatrix} 3 & -1 \\\\ -1 & 3 \\end{pmatrix}\n$$\nNow we can compute the full covariance matrix $C_{\\hat{x}}$:\n$$\nC_{\\hat{x}} = \\left(\\frac{1}{8} \\begin{pmatrix} 3 & -1 \\\\ -1 & 3 \\end{pmatrix}\\right) \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} \\left(\\frac{1}{8} \\begin{pmatrix} 3 & -1 \\\\ -1 & 3 \\end{pmatrix}\\right)\n$$\n$$\nC_{\\hat{x}} = \\frac{1}{64} \\begin{pmatrix} 3 & -1 \\\\ -1 & 3 \\end{pmatrix} \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} \\begin{pmatrix} 3 & -1 \\\\ -1 & 3 \\end{pmatrix}\n$$\nFirst, multiply the first two matrices:\n$$\n\\begin{pmatrix} 3 & -1 \\\\ -1 & 3 \\end{pmatrix} \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} = \\begin{pmatrix} 3 \\cdot 2 - 1 \\cdot 1 & 3 \\cdot 1 - 1 \\cdot 2 \\\\ -1 \\cdot 2 + 3 \\cdot 1 & -1 \\cdot 1 + 3 \\cdot 2 \\end{pmatrix} = \\begin{pmatrix} 5 & 1 \\\\ 1 & 5 \\end{pmatrix}\n$$\nNow, complete the multiplication:\n$$\nC_{\\hat{x}} = \\frac{1}{64} \\begin{pmatrix} 5 & 1 \\\\ 1 & 5 \\end{pmatrix} \\begin{pmatrix} 3 & -1 \\\\ -1 & 3 \\end{pmatrix} = \\frac{1}{64} \\begin{pmatrix} 5 \\cdot 3 + 1 \\cdot (-1) & 5 \\cdot (-1) + 1 \\cdot 3 \\\\ 1 \\cdot 3 + 5 \\cdot (-1) & 1 \\cdot (-1) + 5 \\cdot 3 \\end{pmatrix}\n$$\n$$\nC_{\\hat{x}} = \\frac{1}{64} \\begin{pmatrix} 14 & -2 \\\\ -2 & 14 \\end{pmatrix} = \\begin{pmatrix} \\frac{14}{64} & -\\frac{2}{64} \\\\ -\\frac{2}{64} & \\frac{14}{64} \\end{pmatrix} = \\begin{pmatrix} \\frac{7}{32} & -\\frac{1}{32} \\\\ -\\frac{1}{32} & \\frac{7}{32} \\end{pmatrix}\n$$\nThe pixel-level uncertainties (standard deviations) are the square roots of the diagonal elements of the covariance matrix $C_{\\hat{x}}$. Let $\\sigma_1$ and $\\sigma_2$ be the uncertainties for the first and second pixels, respectively.\nThe variances are $\\sigma_1^2 = \\frac{7}{32}$ and $\\sigma_2^2 = \\frac{7}{32}$.\nThe standard deviations are:\n$$\n\\sigma_1 = \\sqrt{\\frac{7}{32}} \\quad \\text{and} \\quad \\sigma_2 = \\sqrt{\\frac{7}{32}}\n$$\nNumerically, $\\frac{7}{32} = 0.21875$.\n$$\n\\sigma_1 = \\sigma_2 = \\sqrt{0.21875} \\approx 0.46770717...\n$$\nRounding to four significant figures, the uncertainties are both $0.4677$. The final answer is requested as a row vector.",
            "answer": "$$\\boxed{\\begin{pmatrix} 0.4677 & 0.4677 \\end{pmatrix}}$$"
        },
        {
            "introduction": "A statistically stable solution with well-defined uncertainties is a major step, but it relies on the assumption that our forward model is an accurate representation of reality. This final practice introduces Posterior Predictive Checks (PPCs), a sophisticated Bayesian technique for diagnosing potential model misspecification. You will learn to formalize your judgment about the quality of a fit by comparing the observed data to replicate datasets generated from the model , allowing you to quantitatively assess if your underlying assumptions are valid.",
            "id": "4159172",
            "problem": "Consider a secondary eclipse mapping scenario for an exoplanet, where the planet’s dayside brightness distribution is discretized along the projected sky-plane longitudinal coordinate. Let the planet be modeled as a unit-radius disk, with projected sky-plane longitudinal coordinate $u \\in [-1,1]$. The dayside brightness distribution is represented by a vector $x \\in \\mathbb{R}^M$, whose entries $x_j$ correspond to $M$ equal-width columns centered at $u_j$, spaced uniformly across the interval $[-1,1]$, with $u_j = -1 + (j - 0.5)\\Delta u$ for $j \\in \\{1,\\dots,M\\}$ and $\\Delta u = 2/M$. For a column at coordinate $u$, the visible cross-sectional span in the orthogonal direction is proportional to $2\\sqrt{1-u^2}$, representing the length of the chord across the unit disk at that coordinate.\n\nAssume the occulting stellar limb during ingress/egress is well-approximated by a straight line (stellar limb local planarity approximation). Let the occultor boundary be the vertical chord at position $c(t)$, so that at time $t$ the portion of the planetary disk with $u < c(t)$ is fully occulted and the portion with $u \\ge c(t)$ is fully visible. The mask for column $u$ at time $t$ is\n$$\nM(u,t) = \\begin{cases}\n0, & u < c(t),\\\\\n1, & u \\ge c(t).\n\\end{cases}\n$$\nLet the modeled column weight be $w_{\\mathrm{model}}(u) = 2\\sqrt{1-u^2}\\Delta u$, corresponding to isotropic, uniform specific intensity across the visible disk. The resulting forward model is linear:\n$$\ny_t = \\sum_{j=1}^M A_{t j} x_j + \\epsilon_t, \\quad A_{t j} = w_{\\mathrm{model}}(u_j) M(u_j,t),\n$$\nwhere $y_t$ is the measured planet flux at time $t$ in arbitrary dimensionless units, and $\\epsilon_t \\sim \\mathcal{N}(0,\\sigma^2)$ is independent Gaussian noise with constant standard deviation $\\sigma$.\n\nLet the true data-generating process potentially deviate from the model through an emission anisotropy factor that modifies the column weighting to\n$$\nw_{\\mathrm{true}}(u) = 2\\sqrt{1-u^2} \\left(1 - d + d \\sqrt{1-u^2}\\right)\\Delta u,\n$$\nwhere $d \\in [0,1)$ controls a center-brightening effect, with $d = 0$ recovering the modeled isotropic case. The observed flux time series is generated by\n$$\ny_t^{\\mathrm{obs}} = \\sum_{j=1}^M w_{\\mathrm{true}}(u_j) M(u_j,t) x_j^{\\mathrm{true}} + \\epsilon_t,\n$$\nwith $x^{\\mathrm{true}}$ the true brightness map.\n\nThe motion of the occultor boundary across the planetary disk is parameterized by\n$$\nc(t) = c_{\\min} + \\frac{c_{\\max} - c_{\\min}}{\\tau}\\left(t + \\frac{\\tau}{2}\\right),\n$$\nfor a time grid $t \\in \\{-\\tau/2, -\\tau/2 + \\Delta t, \\dots, +\\tau/2\\}$, where $\\tau$ is the eclipse window length in seconds and $\\Delta t$ is the cadence in seconds. The constants $c_{\\min}$ and $c_{\\max}$ satisfy $c_{\\min} < -1$ and $c_{\\max} > +1$, such that the chord sweeps fully across the disk and the dataset contains out-of-eclipse, partial eclipse (ingress/egress), and full eclipse measurements. Angles are not required explicitly in this formulation, and all time quantities must be treated in seconds, while fluxes must be treated as dimensionless.\n\nGiven observed fluxes $y^{\\mathrm{obs}}$ and a forward operator $A$ built under the isotropic modeled weights $w_{\\mathrm{model}}$, invert for the brightness map $x$ using Tikhonov-regularized least squares. Define the regularized estimator as the minimizer of\n$$\n\\mathcal{J}(x) = \\frac{1}{\\sigma^2}\\|A x - y\\|_2^2 + \\lambda \\|L x\\|_2^2,\n$$\nwhere $L$ is the first-difference operator $(L x)_i = x_{i+1} - x_i$ for $i=1,\\dots,M-1$ (with $L \\in \\mathbb{R}^{(M-1)\\times M}$), and $\\lambda > 0$ controls the smoothness penalty. The solution $x^{\\mathrm{hat}}$ solves the normal equations\n$$\n\\left(\\frac{1}{\\sigma^2} A^\\top A + \\lambda L^\\top L\\right) x^{\\mathrm{hat}} = \\frac{1}{\\sigma^2} A^\\top y.\n$$\n\nDesign Posterior Predictive Checks (PPC) to assess potential model misspecification by simulating light curves from the inferred $x^{\\mathrm{hat}}$ and comparing them to the observed data. Use the Gaussian likelihood to generate posterior predictive replicates for the flux at the ingress/egress subset of times where $-1 < c(t) < 1$. Draw $K$ independent replicates\n$$\ny^{\\mathrm{rep},(k)} \\sim \\mathcal{N}\\left(A x^{\\mathrm{hat}}, \\sigma^2 I\\right), \\quad k = 1,\\dots,K.\n$$\nFormalize a discrepancy measure that emphasizes the shape and amplitude of residuals during ingress/egress. Define\n$$\nT(y) = \\sum_{t: \\, -1 < c(t) < 1} \\left(\\frac{y_t - (A x^{\\mathrm{hat}})_t}{\\sigma}\\right)^2,\n$$\ncompute $T\\left(y^{\\mathrm{obs}}\\right)$ for the data, and compute $T\\left(y^{\\mathrm{rep},(k)}\\right)$ for each replicate. Define the two-sided Bayesian $p$-value\n$$\np_B = 2 \\min\\left\\{ \\frac{1}{K} \\sum_{k=1}^K \\mathbb{I}\\left[T\\left(y^{\\mathrm{rep},(k)}\\right) \\ge T\\left(y^{\\mathrm{obs}}\\right)\\right], \\, \\frac{1}{K} \\sum_{k=1}^K \\mathbb{I}\\left[T\\left(y^{\\mathrm{rep},(k)}\\right) \\le T\\left(y^{\\mathrm{obs}}\\right)\\right] \\right\\}.\n$$\nSmall values of $p_B$ (for example below the conventional threshold $0.05$) indicate potential model misspecification.\n\nImplement the following complete procedure:\n- Construct the true brightness map as $x_j^{\\mathrm{true}} = 1 + \\alpha \\cos(\\pi u_j)$ for $j \\in \\{1,\\dots,M\\}$, with $\\alpha \\in (0,1)$ chosen to avoid negativity.\n- Generate synthetic observed flux $y^{\\mathrm{obs}}$ over the full time grid using $w_{\\mathrm{true}}$ with the specified $d$ and independent Gaussian noise of standard deviation $\\sigma$.\n- Build the modeled forward operator $A$ using $w_{\\mathrm{model}}$ and restrict to the ingress/egress times where $-1 < c(t) < 1$ for inversion and for the discrepancy measure.\n- Solve for $x^{\\mathrm{hat}}$ with the Tikhonov-regularized normal equations using the given $\\lambda$ and first-difference $L$.\n- Generate $K$ posterior predictive replicates under the Gaussian model and compute the Bayesian $p$-value $p_B$.\n\nYou must implement the program in the Python language, version $3.12$, using only the allowed libraries. Your program must not read any input and must produce results for the test suite below.\n\nTest suite:\n- Case $1$ (well-specified model, moderate noise): $M=128$, $\\tau=7200$ seconds, $\\Delta t=60$ seconds, $c_{\\min}=-1.2$, $c_{\\max}=+1.2$, $\\alpha=0.3$, $d=0.0$, $\\sigma=5\\times 10^{-4}$, $\\lambda=0.01$, $K=300$.\n- Case $2$ (misspecified anisotropy, moderate noise): $M=128$, $\\tau=7200$ seconds, $\\Delta t=60$ seconds, $c_{\\min}=-1.2$, $c_{\\max}=+1.2$, $\\alpha=0.3$, $d=0.6$, $\\sigma=5\\times 10^{-4}$, $\\lambda=0.01$, $K=300$.\n- Case $3$ (misspecified anisotropy, near-zero noise boundary): $M=128$, $\\tau=7200$ seconds, $\\Delta t=60$ seconds, $c_{\\min}=-1.2$, $c_{\\max}=+1.2$, $\\alpha=0.3$, $d=0.6$, $\\sigma=10^{-6}$, $\\lambda=0.01$, $K=300$.\n\nAnswer specification:\n- For each case, compute the Bayesian $p$-value $p_B$ as a float.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the cases listed above, for example, $[p_1,p_2,p_3]$, where each $p_i$ is the Bayesian $p$-value for case $i$.\n\nAll fluxes are in dimensionless units, all times must be treated in seconds, and angles (if used) must be in radians. The final outputs must be floats (dimensionless), aggregated exactly as specified.",
            "solution": "The problem requires the implementation of a statistical procedure known as a Posterior Predictive Check (PPC) to assess model misspecification in the context of exoplanet secondary eclipse mapping. The core task is to simulate an observed eclipse light curve using a \"true\" data-generating process and then attempt to fit this data with a simplified, potentially incorrect, \"modeled\" process. The quality of the fit and the plausibility of the data under the fitted model are quantified using a discrepancy statistic and a corresponding Bayesian $p$-value, $p_B$.\n\nThe procedure can be decomposed into the following steps:\n\n**1. Discretization and Grid Setup**\n\nThe exoplanet's disk is modeled as a unit circle in the projected sky-plane. We discretize its dayside brightness distribution into $M$ vertical columns. The longitudinal coordinate is $u \\in [-1, 1]$.\n- The number of columns is $M$.\n- The width of each column is $\\Delta u = 2/M$.\n- The center of the $j$-th column is $u_j = -1 + (j - 0.5)\\Delta u$ for $j \\in \\{1, \\dots, M\\}$.\n\nThe occulting stellar limb moves across the planet. Its position at time $t$ is given by a linear function:\n$$\nc(t) = c_{\\min} + \\frac{c_{\\max} - c_{\\min}}{\\tau}\\left(t + \\frac{\\tau}{2}\\right)\n$$\nThe time grid is uniformly sampled with cadence $\\Delta t$ over the interval $[-\\tau/2, +\\tau/2]$. The parameters $c_{\\min} < -1$ and $c_{\\max} > +1$ ensure the time series captures the full eclipse event, including out-of-eclipse phases.\n\nThe visibility of each column $j$ at time $t$ is described by a mask function:\n$$\nM(u_j,t) = \\begin{cases}\n0, & u_j < c(t) \\quad (\\text{occulted})\\\\\n1, & u_j \\ge c(t) \\quad (\\text{visible})\n\\end{cases}\n$$\n\n**2. True Data Generation**\n\nFirst, we generate a synthetic \"observed\" light curve, $y^{\\mathrm{obs}}$, using a true underlying physical model.\n- The true brightness map, $x^{\\mathrm{true}}$, is defined as $x_j^{\\mathrm{true}} = 1 + \\alpha \\cos(\\pi u_j)$. The parameter $\\alpha \\in (0,1)$ ensures the brightness is positive.\n- The true weighting for each column, $w_{\\mathrm{true}}$, accounts for geometric projection and a potential emission anisotropy (center-brightening) controlled by the parameter $d \\in [0,1)$:\n$$\nw_{\\mathrm{true}}(u_j) = 2\\sqrt{1-u_j^2} \\left(1 - d + d \\sqrt{1-u_j^2}\\right)\\Delta u\n$$\n- The true forward operator, $A_{\\mathrm{true}}$, is a matrix with elements $(A_{\\mathrm{true}})_{tj} = w_{\\mathrm{true}}(u_j) M(u_j,t)$.\n- The observed flux at each time $t$ is generated by:\n$$\ny_t^{\\mathrm{obs}} = \\sum_{j=1}^M (A_{\\mathrm{true}})_{tj} x_j^{\\mathrm{true}} + \\epsilon_t, \\quad \\text{where} \\quad \\epsilon_t \\sim \\mathcal{N}(0,\\sigma^2)\n$$\nThis is performed over the full time grid.\n\n**3. Model-Based Inversion**\n\nNext, we attempt to recover the brightness map $x$ from the generated data $y^{\\mathrm{obs}}$, but using a simplified model that assumes isotropic emission ($d=0$).\n- The modeled column weight, $w_{\\mathrm{model}}$, is:\n$$\nw_{\\mathrm{model}}(u_j) = 2\\sqrt{1-u_j^2}\\Delta u\n$$\nThis corresponds to the case $d=0$ in the true weight function.\n- The modeled forward operator, $A$, has elements $A_{tj} = w_{\\mathrm{model}}(u_j) M(u_j,t)$.\n- For the inversion, we use only the subset of data corresponding to the ingress/egress phases, where $-1 < c(t) < 1$. Let's denote the restricted operator as $A_{\\mathrm{ie}}$ and the corresponding observed flux vector as $y^{\\mathrm{obs}}_{\\mathrm{ie}}$.\n\nWe seek an estimate of the brightness map, $x^{\\mathrm{hat}}$, by minimizing a Tikhonov-regularized least-squares objective function:\n$$\n\\mathcal{J}(x) = \\frac{1}{\\sigma^2}\\|A_{\\mathrm{ie}} x - y^{\\mathrm{obs}}_{\\mathrm{ie}}\\|_2^2 + \\lambda \\|L x\\|_2^2\n$$\nHere, $L$ is the first-difference operator, promoting smooth solutions. Its elements are defined such that $(L x)_i = x_{i+1} - x_i$. The minimizer $x^{\\mathrm{hat}}$ is found by solving the normal equations, which form a system of linear equations:\n$$\n\\left(\\frac{1}{\\sigma^2} A_{\\mathrm{ie}}^\\top A_{\\mathrm{ie}} + \\lambda L^\\top L\\right) x^{\\mathrm{hat}} = \\frac{1}{\\sigma^2} A_{\\mathrm{ie}}^\\top y^{\\mathrm{obs}}_{\\mathrm{ie}}\n$$\nThis system can be solved for $x^{\\mathrm{hat}}$ using standard numerical linear algebra libraries.\n\n**4. Posterior Predictive Check (PPC)**\n\nThe PPC assesses whether the observed data is plausible under the fitted model. If the model is a poor representation of reality (e.g., because $d>0$ but the model assumes $d=0$), the observed data may appear as an outlier.\n- The fitted model implies a posterior predictive distribution for the flux data: $y^{\\mathrm{rep}} \\sim \\mathcal{N}(A_{\\mathrm{ie}} x^{\\mathrm{hat}}, \\sigma^2 I)$. We draw $K$ replicate datasets, $y^{\\mathrm{rep},(k)}$, from this distribution.\n- We define a discrepancy measure, $T(y)$, which quantifies the mismatch between a dataset $y$ and the model's predictions. The chosen statistic is the sum of squared normalized residuals:\n$$\nT(y) = \\sum_{t: \\, -1 < c(t) < 1} \\left(\\frac{y_t - (A_{\\mathrm{ie}} x^{\\mathrm{hat}})_t}{\\sigma}\\right)^2\n$$\n- We compute this statistic for the actual observed data, $T(y^{\\mathrm{obs}}_{\\mathrm{ie}})$, and for each of the $K$ replicated datasets, $T(y^{\\mathrm{rep},(k)})$. For a replicate $y^{\\mathrm{rep},(k)}$, the term $(y^{\\mathrm{rep},(k)}_t - (A_{\\mathrm{ie}} x^{\\mathrm{hat}})_t)/\\sigma$ is, by construction, a standard normal variate. Thus, $T(y^{\\mathrm{rep},(k)})$ is a sum of squares of these variates, following a chi-squared distribution, $T(y^{\\mathrm{rep},(k)}) \\sim \\chi^2_{N_{\\mathrm{ie}}}$, where $N_{\\mathrm{ie}}$ is the number of data points during ingress/egress.\n\n**5. Bayesian $p$-value Calculation**\n\nFinally, we compare the observed discrepancy statistic to the distribution of replicated discrepancy statistics to compute the Bayesian $p$-value, $p_B$.\n$$\np_B = 2 \\min\\left\\{ \\frac{1}{K} \\sum_{k=1}^K \\mathbb{I}\\left[T\\left(y^{\\mathrm{rep},(k)}\\right) \\ge T\\left(y^{\\mathrm{obs}}_{\\mathrm{ie}}\\right)\\right], \\, \\frac{1}{K} \\sum_{k=1}^K \\mathbb{I}\\left[T\\left(y^{\\mathrm{rep},(k)}\\right) \\le T\\left(y^{\\mathrm{obs}}_{\\mathrm{ie}}\\right)\\right] \\right\\}\n$$\nwhere $\\mathbb{I}[\\cdot]$ is the indicator function. A very small $p_B$ (e.g., $<0.05$) suggests that the observed data is located in the tails of the posterior predictive distribution, indicating a significant mismatch between the model and the data, and thus potential model misspecification.\n\nFor Case $1$, where the true model and the fitted model match ($d=0$), we expect $p_B$ to be drawn from a uniform distribution on $[0,1]$ and thus not be systematically small. For Cases $2$ and $3$, the model is misspecified ($d=0.6$), which should lead to systematic residuals, a large value for $T(y^{\\mathrm{obs}}_{\\mathrm{ie}})$, and consequently a very small $p_B$. The effect should be more pronounced in Case $3$ due to the much lower noise level $\\sigma$.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for the eclipse mapping problem.\n    \"\"\"\n    \n    test_cases = [\n        # Case 1 (well-specified model, moderate noise)\n        {'M': 128, 'tau': 7200, 'dt': 60, 'c_min': -1.2, 'c_max': 1.2, \n         'alpha': 0.3, 'd': 0.0, 'sigma': 5e-4, 'lambda_reg': 0.01, 'K': 300},\n        # Case 2 (misspecified anisotropy, moderate noise)\n        {'M': 128, 'tau': 7200, 'dt': 60, 'c_min': -1.2, 'c_max': 1.2, \n         'alpha': 0.3, 'd': 0.6, 'sigma': 5e-4, 'lambda_reg': 0.01, 'K': 300},\n        # Case 3 (misspecified anisotropy, near-zero noise)\n        {'M': 128, 'tau': 7200, 'dt': 60, 'c_min': -1.2, 'c_max': 1.2, \n         'alpha': 0.3, 'd': 0.6, 'sigma': 1e-6, 'lambda_reg': 0.01, 'K': 300},\n    ]\n\n    results = []\n    # Use a fixed seed for reproducibility of random noise generation\n    rng = np.random.default_rng(seed=42)\n\n    for params in test_cases:\n        p_value = compute_p_value(params, rng)\n        results.append(p_value)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef compute_p_value(params, rng):\n    \"\"\"\n    Computes the Bayesian p-value for a single test case.\n    \"\"\"\n    # Unpack parameters\n    M = params['M']\n    tau = params['tau']\n    dt = params['dt']\n    c_min = params['c_min']\n    c_max = params['c_max']\n    alpha = params['alpha']\n    d = params['d']\n    sigma = params['sigma']\n    lambda_reg = params['lambda_reg']\n    K = params['K']\n\n    # 1. Setup and Discretization\n    delta_u = 2.0 / M\n    u_j = -1.0 + (np.arange(1, M + 1) - 0.5) * delta_u\n\n    t = np.arange(-tau / 2, tau / 2 + dt, dt)\n    c_t = c_min + ((c_max - c_min) / tau) * (t + tau / 2)\n    \n    # Mask function M(u, t)\n    # M_full[i, j] is 1 if u_j >= c_t[i], 0 otherwise\n    M_full = (u_j[np.newaxis, :] >= c_t[:, np.newaxis]).astype(float)\n    \n    # 2. True Data Generation\n    # True brightness map\n    x_true = 1.0 + alpha * np.cos(np.pi * u_j)\n\n    # True column weights\n    geom_factor = 2 * np.sqrt(np.maximum(0, 1 - u_j**2))\n    anisotropy = 1 - d + d * np.sqrt(np.maximum(0, 1 - u_j**2))\n    w_true = geom_factor * anisotropy * delta_u\n    \n    # True forward operator\n    A_true = w_true[np.newaxis, :] * M_full\n    \n    # Generate observed flux\n    y_true = A_true @ x_true\n    noise = rng.normal(0, sigma, size=y_true.shape)\n    y_obs_full = y_true + noise\n\n    # 3. Model-Based Inversion\n    # Restrict to ingress/egress times\n    ie_mask = (c_t > -1) & (c_t < 1)\n    y_obs_ie = y_obs_full[ie_mask]\n    \n    # Modeled column weights (assumes d=0)\n    w_model = geom_factor * delta_u\n    \n    # Modeled forward operator for ingress/egress\n    A_model = w_model[np.newaxis, :] * M_full[ie_mask, :]\n    \n    # First-difference operator L\n    L = -np.eye(M - 1, M, k=0) + np.eye(M - 1, M, k=1)\n    \n    # Set up and solve the normal equations\n    # Q * x_hat = b\n    Q = (1 / sigma**2) * (A_model.T @ A_model) + lambda_reg * (L.T @ L)\n    b = (1 / sigma**2) * (A_model.T @ y_obs_ie)\n    x_hat = np.linalg.solve(Q, b)\n\n    # 4. Posterior Predictive Check (PPC)\n    # Model predictions for ingress/egress\n    y_pred_ie = A_model @ x_hat\n    \n    # Discrepancy for observed data\n    T_obs = np.sum(((y_obs_ie - y_pred_ie) / sigma)**2)\n    \n    # Generate K posterior predictive replicates\n    N_ie = A_model.shape[0]\n    rep_noise = rng.normal(0, 1, size=(K, N_ie))\n    \n    # Discrepancy for replicates\n    # T_rep = sum of N_ie squared standard normal variables for each replicate\n    T_rep = np.sum(rep_noise**2, axis=1)\n\n    # 5. Bayesian p-value Calculation\n    p_ge = np.mean(T_rep >= T_obs)\n    p_le = np.mean(T_rep <= T_obs)\n    \n    p_B = 2 * min(p_ge, p_le)\n    \n    return p_B\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}