{
    "hands_on_practices": [
        {
            "introduction": "The performance of an Ensemble Kalman Filter depends critically on the quality of its initial ensemble. This ensemble must represent not only the uncertainty in the initial state but also the correct statistical relationships, or \"balance,\" between different components of the system, such as large and small scales. This practice guides you through the fundamental procedure of generating an ensemble with a prescribed multiscale covariance, providing hands-on experience with the practical first step of any ensemble-based data assimilation experiment .",
            "id": "3784267",
            "problem": "Consider a multiscale state representation in which the system state is expressed in a scale-separated, orthonormal basis as a two-component vector $x = (x_{L}, x_{S})^{\\top} \\in \\mathbb{R}^{2}$, where $x_{L}$ and $x_{S}$ are the large-scale and small-scale amplitudes, respectively. Suppose the prior statistical model for the initial state is jointly Gaussian with zero mean. The multiscale covariance model prescribes distinct scale variances and a nonzero cross-scale covariance:\n$$\n\\operatorname{Var}(x_{L}) = \\sigma_{L}^{2}, \\quad \\operatorname{Var}(x_{S}) = \\sigma_{S}^{2}, \\quad \\operatorname{Cov}(x_{L}, x_{S}) = \\rho\\, \\sigma_{L}\\sigma_{S},\n$$\nwhere $\\sigma_{L} > 0$, $\\sigma_{S} > 0$, and $|\\rho| < 1$. This captures a physically plausible balance between scales and is consistent with multiscale dynamics in Ensemble Kalman Filter (EnKF) spin-up, where initial ensembles must reflect correct cross-scale structure to avoid spurious adjustment transients.\n\nYou will design a linear sampling procedure that produces an initial ensemble $\\{x^{(j)}\\}_{j=1}^{N}$ by transforming independent, standard normal samples $\\xi^{(j)} \\sim \\mathcal{N}(0, I_{2})$, with $x^{(j)} = L\\, \\xi^{(j)}$. The matrix $L \\in \\mathbb{R}^{2 \\times 2}$ must be chosen so that the resulting ensemble has mean zero, large-scale variance $\\sigma_{L}^{2}$, small-scale variance $\\sigma_{S}^{2}$, and cross-scale correlation equal to $\\rho$.\n\nStarting from core definitions of Gaussian random vectors, covariance, correlation, and conditional Gaussian distributions, derive a sampling transformation $L$ that ensures the desired multiscale covariance and cross-scale correlation in the initial ensemble while preserving scientific realism and balance. Provide your final answer as a single, closed-form analytical expression for $L$ in terms of $\\sigma_{L}$, $\\sigma_{S}$, and $\\rho$. No numerical evaluation is required, and no units are involved in the answer.",
            "solution": "The problem is well-posed and scientifically grounded. It addresses a fundamental task in statistical modeling and data assimilation: generating a set of random vectors (an ensemble) with a prescribed covariance structure from a set of independent, standard normal random vectors. This is a standard procedure for initializing Ensemble Kalman Filters (EnKF) in a way that respects the known statistical relationships between different components of the system state, in this case, large and small scales.\n\nLet the state vector be $x = (x_{L}, x_{S})^{\\top} \\in \\mathbb{R}^{2}$. We are given that the desired statistical properties for $x$ are a zero mean, $E[x] = 0$, and a specific covariance matrix $C_x$. The components of this covariance matrix are derived from the problem statement:\n$$\nC_x = \\begin{pmatrix} \\operatorname{Cov}(x_{L}, x_{L}) & \\operatorname{Cov}(x_{L}, x_{S}) \\\\ \\operatorname{Cov}(x_{S}, x_{L}) & \\operatorname{Cov}(x_{S}, x_{S}) \\end{pmatrix}\n$$\nBy definition, $\\operatorname{Cov}(x_{L}, x_{L}) = \\operatorname{Var}(x_{L})$ and $\\operatorname{Cov}(x_{S}, x_{S}) = \\operatorname{Var}(x_{S})$. The covariance matrix is also symmetric, so $\\operatorname{Cov}(x_{S}, x_{L}) = \\operatorname{Cov}(x_{L}, x_{S})$. Using the provided definitions:\n$\\operatorname{Var}(x_{L}) = \\sigma_{L}^{2}$\n$\\operatorname{Var}(x_{S}) = \\sigma_{S}^{2}$\n$\\operatorname{Cov}(x_{L}, x_{S}) = \\rho \\sigma_{L} \\sigma_{S}$\nSubstituting these into the matrix structure gives the target covariance matrix:\n$$\nC_x = \\begin{pmatrix} \\sigma_{L}^{2} & \\rho \\sigma_{L} \\sigma_{S} \\\\ \\rho \\sigma_{L} \\sigma_{S} & \\sigma_{S}^{2} \\end{pmatrix}\n$$\nThe problem requires designing a linear transformation $x = L\\xi$, where $\\xi \\sim \\mathcal{N}(0, I_{2})$ is a vector of two independent standard normal random variables, and $L \\in \\mathbb{R}^{2 \\times 2}$ is the transformation matrix we need to find. The covariance matrix of $\\xi$, denoted $C_{\\xi}$, is the $2 \\times 2$ identity matrix, $I_{2}$.\n\nWe can derive the relationship between the covariance of $x$ and the covariance of $\\xi$. The mean of $x$ is $E[x] = E[L\\xi] = L E[\\xi] = L \\cdot 0 = 0$, which correctly yields a zero-mean distribution for $x$. The covariance matrix of $x$ is given by:\n$$\nC_x = E[(x - E[x])(x - E[x])^{\\top}] = E[x x^{\\top}]\n$$\nSubstituting $x = L\\xi$:\n$$\nC_x = E[(L\\xi)(L\\xi)^{\\top}] = E[L\\xi\\xi^{\\top}L^{\\top}]\n$$\nSince $L$ is a deterministic matrix, it can be factored out of the expectation:\n$$\nC_x = L E[\\xi\\xi^{\\top}] L^{\\top}\n$$\nThe term $E[\\xi\\xi^{\\top}]$ is the covariance matrix of $\\xi$, which is $C_{\\xi} = I_2$. Therefore, we have the core relationship:\n$$\nC_x = L I_2 L^{\\top} = LL^{\\top}\n$$\nOur goal is to find a matrix $L$ such that $LL^{\\top}$ equals the target covariance matrix $C_x$. This is a matrix decomposition problem. A common and constructive method for this is the Cholesky decomposition, which for a positive-definite symmetric matrix $C_x$ finds a unique lower triangular matrix $L$ with positive diagonal entries such that $LL^{\\top} = C_x$. The condition $|\\rho| < 1$ ensures that $C_x$ is indeed positive definite, as its determinant is $\\det(C_x) = \\sigma_{L}^{2}\\sigma_{S}^{2} - (\\rho \\sigma_{L} \\sigma_{S})^2 = \\sigma_{L}^{2}\\sigma_{S}^{2}(1-\\rho^2) > 0$.\n\nLet us assume $L$ is a lower triangular matrix of the form:\n$$\nL = \\begin{pmatrix} l_{11} & 0 \\\\ l_{21} & l_{22} \\end{pmatrix}\n$$\nNow, we compute $LL^{\\top}$:\n$$\nLL^{\\top} = \\begin{pmatrix} l_{11} & 0 \\\\ l_{21} & l_{22} \\end{pmatrix} \\begin{pmatrix} l_{11} & l_{21} \\\\ 0 & l_{22} \\end{pmatrix} = \\begin{pmatrix} l_{11}^{2} & l_{11} l_{21} \\\\ l_{11} l_{21} & l_{21}^{2} + l_{22}^{2} \\end{pmatrix}\n$$\nWe equate this with the target covariance matrix $C_x$:\n$$\n\\begin{pmatrix} l_{11}^{2} & l_{11} l_{21} \\\\ l_{11} l_{21} & l_{21}^{2} + l_{22}^{2} \\end{pmatrix} = \\begin{pmatrix} \\sigma_{L}^{2} & \\rho \\sigma_{L} \\sigma_{S} \\\\ \\rho \\sigma_{L} \\sigma_{S} & \\sigma_{S}^{2} \\end{pmatrix}\n$$\nWe can now solve for the elements of $L$ by equating the corresponding matrix entries.\n\nFrom the $(1,1)$ entry:\n$l_{11}^{2} = \\sigma_{L}^{2}$. Since $\\sigma_{L} > 0$ and the diagonal elements of the Cholesky factor are positive, we have $l_{11} = \\sigma_{L}$.\n\nFrom the $(2,1)$ entry (or $(1,2)$ entry):\n$l_{11} l_{21} = \\rho \\sigma_{L} \\sigma_{S}$.\nSubstituting $l_{11} = \\sigma_{L}$:\n$\\sigma_{L} l_{21} = \\rho \\sigma_{L} \\sigma_{S}$.\nSince $\\sigma_{L} > 0$, we can divide by it to find $l_{21}$:\n$l_{21} = \\rho \\sigma_{S}$.\n\nFrom the $(2,2)$ entry:\n$l_{21}^{2} + l_{22}^{2} = \\sigma_{S}^{2}$.\nSubstituting the value of $l_{21}$:\n$(\\rho \\sigma_{S})^{2} + l_{22}^{2} = \\sigma_{S}^{2}$\n$\\rho^{2} \\sigma_{S}^{2} + l_{22}^{2} = \\sigma_{S}^{2}$\n$l_{22}^{2} = \\sigma_{S}^{2} - \\rho^{2} \\sigma_{S}^{2} = \\sigma_{S}^{2}(1 - \\rho^{2})$.\nSince $\\sigma_{S} > 0$ and $|\\rho| < 1$, we have $1-\\rho^2 > 0$. We take the positive square root for the diagonal entry:\n$l_{22} = \\sqrt{\\sigma_{S}^{2}(1 - \\rho^{2})} = \\sigma_{S}\\sqrt{1 - \\rho^{2}}$.\n\nAssembling the matrix $L$ from its components gives the final expression for the sampling transformation:\n$$\nL = \\begin{pmatrix} \\sigma_{L} & 0 \\\\ \\rho \\sigma_{S} & \\sigma_{S} \\sqrt{1 - \\rho^{2}} \\end{pmatrix}\n$$\nThis matrix, when applied to a vector of independent standard normal samples, will produce a new random vector with the desired multiscale variance and cross-scale correlation structure appropriate for initializing an ensemble.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\sigma_{L} & 0 \\\\\n\\rho \\sigma_{S} & \\sigma_{S} \\sqrt{1 - \\rho^{2}}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "At the heart of any Kalman filter is the optimal weighting of prior information (the forecast) and new information (the observations). In multiscale systems, this principle is key, as the filter must intelligently adjust its updates based on the relative certainty of the forecast and observations at each individual scale. This exercise uses a simplified setting to let you derive the scale-wise Kalman gain from first principles, helping you understand how the filter uses error variances to determine how much to trust new data at each scale .",
            "id": "3784259",
            "problem": "Consider a finite-dimensional state vector $x \\in \\mathbb{R}^{n}$ represented in an orthonormal multiresolution basis with three scales. The scales have sizes $m_{1} = 4$, $m_{2} = 2$, and $m_{3} = 1$, so that $n = m_{1} + m_{2} + m_{3} = 7$. Denote the multiresolution coefficient vector by $c \\in \\mathbb{R}^{7}$, where $c$ is partitioned by scale as $c = (c^{(1)}, c^{(2)}, c^{(3)})$ with $c^{(s)} \\in \\mathbb{R}^{m_{s}}$ for $s \\in \\{1,2,3\\}$. The multiresolution basis is assumed orthonormal, so the transformation between $x$ and $c$ is unitary.\n\nA multiresolution observation network measures all coefficients at each scale directly, so the observation operator in coefficient space is $H = I_{7}$ (the identity matrix). The observation errors are independent across scales and within scales, with variances $r_{1}$, $r_{2}$, and $r_{3}$ for scales $s = 1, 2, 3$, respectively. The forecast (prior) coefficient covariance is block-diagonal across scales, with scale-wise blocks $\\sigma_{1}^{2} I_{m_{1}}$, $\\sigma_{2}^{2} I_{m_{2}}$, and $\\sigma_{3}^{2} I_{m_{3}}$, where $I_{k}$ denotes the $k \\times k$ identity matrix. Assume the Ensemble Kalman Filter (EnKF) is implemented in coefficient space, which is equivalent to a state-space implementation due to orthonormality.\n\n(a) Construct the observation error covariance matrix $R \\in \\mathbb{R}^{7 \\times 7}$ as a block-diagonal matrix consistent with the multiresolution structure and the given scale-wise error variances.\n\n(b) Starting from the Bayesian linear-Gaussian data assimilation setting with Gaussian prior $c \\sim \\mathcal{N}(c_{f}, P_{f})$ and Gaussian observation model $y = H c + \\varepsilon$ with $\\varepsilon \\sim \\mathcal{N}(0, R)$, derive the expression for the scale-wise innovation weighting embedded in the EnKF gain. That is, identify the scalar factor at each scale that multiplies the innovation components $(y - H c_{f})$ in the analysis update when $P_{f}$ and $R$ have the stated block-diagonal multiscale structure.\n\n(c) Using the specific values $\\sigma_{1}^{2} = 2$, $\\sigma_{2}^{2} = \\frac{1}{2}$, $\\sigma_{3}^{2} = 3$ and $r_{1} = 1$, $r_{2} = \\frac{1}{10}$, $r_{3} = 4$, compute the vector of scale-wise innovation weighting factors derived in part (b). Provide your final answer as a single row vector containing the three scale-wise weights. No rounding is required, and no units are involved in this computation.",
            "solution": "The problem is addressed in three parts as requested.\n\n**(a) Construction of the Observation Error Covariance Matrix $R$**\n\nThe problem states that the observation errors are independent both across and within scales. This implies that the observation error covariance matrix, $R$, is a diagonal matrix. The diagonal entries are the variances of the observation errors for each corresponding component of the coefficient vector $c$.\n\nThe coefficient vector $c$ is partitioned by scale: $c = (c^{(1)}, c^{(2)}, c^{(3)})$.\n- For scale $s=1$, there are $m_1 = 4$ coefficients. The observation error variance for each of these coefficients is given as $r_1$.\n- For scale $s=2$, there are $m_2 = 2$ coefficients. The observation error variance for each of these is $r_2$.\n- For scale $s=3$, there is $m_3 = 1$ coefficient. The observation error variance for this is $r_3$.\n\nDue to the independence of errors, all off-diagonal elements of $R$ are zero. The diagonal elements corresponding to scale $s$ are all equal to $r_s$. This structure results in a block-diagonal matrix, where each block is a scalar multiple of an identity matrix.\n\nThe matrix $R \\in \\mathbb{R}^{7 \\times 7}$ is constructed as:\n$$\nR = \\begin{pmatrix}\nR^{(1)} & 0 & 0 \\\\\n0 & R^{(2)} & 0 \\\\\n0 & 0 & R^{(3)}\n\\end{pmatrix}\n$$\nwhere $R^{(s)}$ is the observation error covariance matrix for scale $s$. Given the stated variances, these blocks are:\n- $R^{(1)} = r_1 I_{m_1} = r_1 I_4 \\in \\mathbb{R}^{4 \\times 4}$\n- $R^{(2)} = r_2 I_{m_2} = r_2 I_2 \\in \\mathbb{R}^{2 \\times 2}$\n- $R^{(3)} = r_3 I_{m_3} = r_3 I_1 = r_3 \\in \\mathbb{R}^{1 \\times 1}$\n\nTherefore, the full observation error covariance matrix is:\n$$\nR = \\begin{pmatrix}\nr_1 I_4 & 0 & 0 \\\\\n0 & r_2 I_2 & 0 \\\\\n0 & 0 & r_3\n\\end{pmatrix} = \\text{diag}(r_1, r_1, r_1, r_1, r_2, r_2, r_3)\n$$\n\n**(b) Derivation of Scale-Wise Innovation Weighting**\n\nThe analysis update in the Kalman filter is given by:\n$$ c_a = c_f + K (y - H c_f) $$\nwhere $c_a$ is the analysis (posterior) state estimate, $c_f$ is the forecast (prior) state estimate, $y$ is the observation vector, and $K$ is the Kalman gain. The term $(y - H c_f)$ is the innovation. The \"innovation weighting\" is the Kalman gain matrix $K$.\n\nThe Kalman gain is defined as:\n$$ K = P_f H^T (H P_f H^T + R)^{-1} $$\nWe are given the structure of $P_f$, $H$, and $R$:\n- $H = I_7$ (the $7 \\times 7$ identity matrix)\n- $P_f = \\text{diag}(\\sigma_1^2 I_{m_1}, \\sigma_2^2 I_{m_2}, \\sigma_3^2 I_{m_3})$\n- $R = \\text{diag}(r_1 I_{m_1}, r_2 I_{m_2}, r_3 I_{m_3})$\n\nSubstituting $H=I_7$ into the gain formula simplifies it considerably:\n$$ K = P_f (I_7)^T (I_7 P_f (I_7)^T + R)^{-1} = P_f (P_f + R)^{-1} $$\nSince both $P_f$ and $R$ are block-diagonal matrices with the same block structure, their sum $P_f + R$ is also block-diagonal:\n$$\nP_f + R = \\begin{pmatrix}\n(\\sigma_1^2 + r_1) I_{m_1} & 0 & 0 \\\\\n0 & (\\sigma_2^2 + r_2) I_{m_2} & 0 \\\\\n0 & 0 & (\\sigma_3^2 + r_3) I_{m_3}\n\\end{pmatrix}\n$$\nThe inverse of a block-diagonal matrix is the block-diagonal matrix of the inverses of the blocks. The inverse of a scalar multiple of an identity matrix, $(\\alpha)I$, is $(\\frac{1}{\\alpha})I$. Thus:\n$$\n(P_f + R)^{-1} = \\begin{pmatrix}\n\\frac{1}{\\sigma_1^2 + r_1} I_{m_1} & 0 & 0 \\\\\n0 & \\frac{1}{\\sigma_2^2 + r_2} I_{m_2} & 0 \\\\\n0 & 0 & \\frac{1}{\\sigma_3^2 + r_3} I_{m_3}\n\\end{pmatrix}\n$$\nNow we compute the gain $K = P_f (P_f + R)^{-1}$. As the product of two block-diagonal matrices with the same structure, the result is a block-diagonal matrix where each block is the product of the corresponding blocks:\n$$\nK = \\begin{pmatrix}\n\\frac{\\sigma_1^2}{\\sigma_1^2 + r_1} I_{m_1} & 0 & 0 \\\\\n0 & \\frac{\\sigma_2^2}{\\sigma_2^2 + r_2} I_{m_2} & 0 \\\\\n0 & 0 & \\frac{\\sigma_3^2}{\\sigma_3^2 + r_3} I_{m_3}\n\\end{pmatrix}\n$$\nThe analysis update for the coefficients at a specific scale $s$, $c^{(s)}$, is given by:\n$$ c_a^{(s)} = c_f^{(s)} + \\frac{\\sigma_s^2}{\\sigma_s^2 + r_s} (y^{(s)} - c_f^{(s)}) $$\nThe \"scale-wise innovation weighting\" is the scalar factor multiplying the innovation vector at each scale. This factor for scale $s$ is:\n$$ k_s = \\frac{\\sigma_s^2}{\\sigma_s^2 + r_s} $$\n\n**(c) Computation of the Scale-Wise Weights**\n\nUsing the specific values provided:\n- $\\sigma_{1}^{2} = 2$, $r_{1} = 1$\n- $\\sigma_{2}^{2} = \\frac{1}{2}$, $r_{2} = \\frac{1}{10}$\n- $\\sigma_{3}^{2} = 3$, $r_{3} = 4$\n\nWe compute the weighting factor $k_s$ for each scale $s \\in \\{1, 2, 3\\}$.\n\nFor scale $s=1$:\n$$ k_1 = \\frac{\\sigma_1^2}{\\sigma_1^2 + r_1} = \\frac{2}{2 + 1} = \\frac{2}{3} $$\n\nFor scale $s=2$:\n$$ k_2 = \\frac{\\sigma_2^2}{\\sigma_2^2 + r_2} = \\frac{\\frac{1}{2}}{\\frac{1}{2} + \\frac{1}{10}} = \\frac{\\frac{1}{2}}{\\frac{5}{10} + \\frac{1}{10}} = \\frac{\\frac{1}{2}}{\\frac{6}{10}} = \\frac{1}{2} \\cdot \\frac{10}{6} = \\frac{10}{12} = \\frac{5}{6} $$\n\nFor scale $s=3$:\n$$ k_3 = \\frac{\\sigma_3^2}{\\sigma_3^2 + r_3} = \\frac{3}{3 + 4} = \\frac{3}{7} $$\n\nThe vector of scale-wise innovation weighting factors is $(k_1, k_2, k_3)$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{2}{3} & \\frac{5}{6} & \\frac{3}{7}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "While powerful, the Ensemble Kalman Filter's reliance on a sample covariance can introduce spurious correlations, a particularly dangerous issue when only some scales of a system are observed. This critical exercise provides a concrete counterexample demonstrating how such spurious correlations cause the filter to make incorrect updates to an unobserved, unstable mode, leading to catastrophic filter divergence. By working through the calculations, you will gain a crucial, hands-on understanding of this key failure mode, which motivates the need for essential techniques like covariance localization .",
            "id": "3784250",
            "problem": "Consider a two-scale linear, time-discrete system with a fast unstable mode and a slow stable mode, intended to model multiscale dynamics. The state is $x_k \\in \\mathbb{R}^2$ and the dynamics and observations are\n$$\nx_{k+1} = A\\,x_k, \\quad A = \\begin{pmatrix} \\alpha & 0 \\\\ 0 & \\beta \\end{pmatrix}, \\qquad y_k = H\\,x_k + v_k, \\quad H = \\begin{pmatrix} 0 & 1 \\end{pmatrix},\n$$\nwhere $v_k$ is zero-mean observation noise with covariance $R$. Assume no process noise. Take $\\alpha = \\tfrac{3}{2}$, $\\beta = \\tfrac{1}{2}$, and $R = 1$. At assimilation cycle $k = 0$, let the true state be $x_0^{\\mathrm{t}} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ and the realized observation be noise-free, so $y_0 = H x_0^{\\mathrm{t}}$. Suppose the Ensemble Kalman Filter (EnKF) uses an unlocalized, finite-ensemble, sample covariance. The forecast ensemble mean at $k = 0$ is $\\bar{x}_0^{\\mathrm{f}} = \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix}$. The ensemble size is $m = 2$ with anomalies $\\pm a$ about the mean, where $a = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}$, so the sample forecast covariance is rank-deficient and given by $\\widehat{P}_0^{\\mathrm{f}} = a a^{\\top}$. \n\nUsing the standard linear-Gaussian Ensemble Kalman Filter (EnKF) analysis step with the unlocalized sample covariance $\\widehat{P}_0^{\\mathrm{f}}$, compute the one-cycle Euclidean squared error amplification factor\n$$\n\\gamma \\equiv \\frac{\\left\\| \\bar{x}_1^{\\mathrm{f}} - x_1^{\\mathrm{t}} \\right\\|_2^2}{\\left\\| \\bar{x}_0^{\\mathrm{f}} - x_0^{\\mathrm{t}} \\right\\|_2^2},\n$$\nwhere $\\bar{x}_1^{\\mathrm{f}} = A\\,\\bar{x}_0^{\\mathrm{a}}$ is the forecast mean after propagating the analysis mean $\\bar{x}_0^{\\mathrm{a}}$ one step, and $x_1^{\\mathrm{t}} = A\\,x_0^{\\mathrm{t}}$ is the propagated truth. Your answer must be a single exact value (no rounding). This construct demonstrates a counterexample in which the unlocalized, rank-deficient sample covariance drives divergence in the unobserved unstable mode. Provide the exact value of $\\gamma$ as a single number or a closed-form expression.",
            "solution": "We begin with the two-scale linear model and observation operator. The analysis step in the linear-Gaussian Ensemble Kalman Filter (EnKF) for the mean uses the same form as the classical Kalman filter mean update but with the ensemble sample forecast covariance. Specifically, with $y_0 = H x_0^{\\mathrm{t}}$ (noise-free realization) and the unlocalized sample forecast covariance, the analysis mean is\n$$\n\\bar{x}_0^{\\mathrm{a}} = \\bar{x}_0^{\\mathrm{f}} + K_0 \\left( y_0 - H \\bar{x}_0^{\\mathrm{f}} \\right),\n$$\nwhere $K_0$ is the Kalman gain computed from the sample covariance. Define the forecast mean error $e_0^{\\mathrm{f}} \\equiv \\bar{x}_0^{\\mathrm{f}} - x_0^{\\mathrm{t}}$. Using $y_0 = H x_0^{\\mathrm{t}}$, the innovation is $y_0 - H \\bar{x}_0^{\\mathrm{f}} = H x_0^{\\mathrm{t}} - H \\bar{x}_0^{\\mathrm{f}} = - H e_0^{\\mathrm{f}}$. Therefore, the analysis mean error is\n$$\ne_0^{\\mathrm{a}} \\equiv \\bar{x}_0^{\\mathrm{a}} - x_0^{\\mathrm{t}} = \\left( I - K_0 H \\right) e_0^{\\mathrm{f}}.\n$$\nThe one-step forecast mean error after propagation is\n$$\ne_1^{\\mathrm{f}} \\equiv \\bar{x}_1^{\\mathrm{f}} - x_1^{\\mathrm{t}} = A \\bar{x}_0^{\\mathrm{a}} - A x_0^{\\mathrm{t}} = A e_0^{\\mathrm{a}} = A \\left( I - K_0 H \\right) e_0^{\\mathrm{f}}.\n$$\n\nWe now compute all required quantities for the specified counterexample. The sample forecast covariance is constructed from the two-member ensemble with anomalies $\\pm a$, $a = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}$. With $m = 2$, the unbiased sample covariance is\n$$\n\\widehat{P}_0^{\\mathrm{f}} = a a^{\\top} = \\begin{pmatrix} 1 & -2 \\\\ -2 & 4 \\end{pmatrix}.\n$$\nThis matrix is rank-deficient, since $\\det(\\widehat{P}_0^{\\mathrm{f}}) = 1 \\cdot 4 - (-2)\\cdot(-2) = 4 - 4 = 0$, hence $\\operatorname{rank}(\\widehat{P}_0^{\\mathrm{f}}) = 1$.\n\nThe Kalman gain based on $\\widehat{P}_0^{\\mathrm{f}}$ is\n$$\nK_0 = \\widehat{P}_0^{\\mathrm{f}} H^{\\top} \\left( H \\widehat{P}_0^{\\mathrm{f}} H^{\\top} + R \\right)^{-1}.\n$$\nCompute the ingredients:\n$$\n\\widehat{P}_0^{\\mathrm{f}} H^{\\top} = \\begin{pmatrix} 1 & -2 \\\\ -2 & 4 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -2 \\\\ 4 \\end{pmatrix}, \\qquad\nH \\widehat{P}_0^{\\mathrm{f}} H^{\\top} = \\begin{pmatrix} 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & -2 \\\\ -2 & 4 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = 4.\n$$\nWith $R = 1$, the scalar denominator is $4 + 1 = 5$. Thus,\n$$\nK_0 = \\frac{1}{5} \\begin{pmatrix} -2 \\\\ 4 \\end{pmatrix}.\n$$\nCompute the matrix $I - K_0 H$:\n$$\nK_0 H = \\frac{1}{5} \\begin{pmatrix} -2 \\\\ 4 \\end{pmatrix} \\begin{pmatrix} 0 & 1 \\end{pmatrix}\n= \\frac{1}{5} \\begin{pmatrix} 0 & -2 \\\\ 0 & 4 \\end{pmatrix}, \\quad\nI - K_0 H = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} - \\frac{1}{5} \\begin{pmatrix} 0 & -2 \\\\ 0 & 4 \\end{pmatrix}\n= \\begin{pmatrix} 1 & \\tfrac{2}{5} \\\\ 0 & \\tfrac{1}{5} \\end{pmatrix}.\n$$\nThe forecast mean error at $k=0$ is\n$$\ne_0^{\\mathrm{f}} = \\bar{x}_0^{\\mathrm{f}} - x_0^{\\mathrm{t}} = \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ -1 \\end{pmatrix}.\n$$\nTherefore, the analysis mean error is\n$$\ne_0^{\\mathrm{a}} = \\left( I - K_0 H \\right) e_0^{\\mathrm{f}} = \\begin{pmatrix} 1 & \\tfrac{2}{5} \\\\ 0 & \\tfrac{1}{5} \\end{pmatrix} \\begin{pmatrix} -1 \\\\ -1 \\end{pmatrix}\n= \\begin{pmatrix} -1 - \\tfrac{2}{5} \\\\ - \\tfrac{1}{5} \\end{pmatrix} = \\begin{pmatrix} -\\tfrac{7}{5} \\\\ -\\tfrac{1}{5} \\end{pmatrix}.\n$$\nPropagating one step with $A = \\operatorname{diag}(\\alpha, \\beta) = \\operatorname{diag}\\!\\left( \\tfrac{3}{2}, \\tfrac{1}{2} \\right)$ gives\n$$\ne_1^{\\mathrm{f}} = A e_0^{\\mathrm{a}} = \\begin{pmatrix} \\tfrac{3}{2} & 0 \\\\ 0 & \\tfrac{1}{2} \\end{pmatrix} \\begin{pmatrix} -\\tfrac{7}{5} \\\\ -\\tfrac{1}{5} \\end{pmatrix}\n= \\begin{pmatrix} -\\tfrac{21}{10} \\\\ -\\tfrac{1}{10} \\end{pmatrix}.\n$$\nCompute the squared Euclidean norms needed for $\\gamma$:\n$$\n\\left\\| e_0^{\\mathrm{f}} \\right\\|_2^2 = (-1)^2 + (-1)^2 = 2, \\quad\n\\left\\| e_1^{\\mathrm{f}} \\right\\|_2^2 = \\left( \\tfrac{21}{10} \\right)^2 + \\left( \\tfrac{1}{10} \\right)^2 = \\tfrac{441}{100} + \\tfrac{1}{100} = \\tfrac{442}{100} = \\tfrac{221}{50}.\n$$\nThus the one-cycle Euclidean squared error amplification factor is\n$$\n\\gamma = \\frac{ \\left\\| e_1^{\\mathrm{f}} \\right\\|_2^2 }{ \\left\\| e_0^{\\mathrm{f}} \\right\\|_2^2 } = \\frac{ \\tfrac{221}{50} }{ 2 } = \\tfrac{221}{100}.\n$$\nThis value satisfies $\\gamma > 1$, demonstrating that with an unlocalized, rank-deficient sample covariance induced by a two-member ensemble and a spurious cross-covariance, the EnKF analysis injects error into the unobserved unstable mode that subsequently amplifies under the dynamics, providing the requested counterexample.",
            "answer": "$$\\boxed{\\frac{221}{100}}$$"
        }
    ]
}