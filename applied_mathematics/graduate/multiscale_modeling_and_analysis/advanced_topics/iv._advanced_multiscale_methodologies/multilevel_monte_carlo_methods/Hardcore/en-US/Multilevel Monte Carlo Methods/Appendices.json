{
    "hands_on_practices": [
        {
            "introduction": "The remarkable efficiency of the Multilevel Monte Carlo (MLMC) method hinges on a clever statistical insight rather than a more accurate discretization scheme. The core idea is to cleverly couple simulations across consecutive levels to dramatically reduce the variance of the correction terms in the telescoping sum. This exercise  provides a foundational look at this mechanism, using a simple stochastic differential equation to analytically demonstrate the profound impact of correct coupling on the variance of level differences.",
            "id": "3068034",
            "problem": "Consider the scalar stochastic differential equation (SDE) $dX_t = \\sigma\\, dW_t$ with $X_0 = 0$ on the time interval $[0,T]$, where $W_t$ is a standard Wiener process and $\\sigma > 0$ is a constant. Let $P_\\ell$ denote the Euler–Maruyama terminal approximation of $X_T$ computed on level $\\ell$ with $N_\\ell$ steps of size $h_\\ell = T/N_\\ell$, and let $P_{\\ell-1}$ denote the corresponding approximation computed on level $\\ell-1$ with $N_{\\ell-1} = N_\\ell/2$ steps of size $h_{\\ell-1} = 2 h_\\ell$. In both cases, the Euler–Maruyama scheme is defined by $X_{n+1} = X_n + \\sigma\\, \\Delta W_n$, where each Brownian increment $\\Delta W_n$ is distributed as $\\mathcal{N}(0,h)$ with $h$ equal to the relevant step size. Define $P_\\ell = \\sum_{n=1}^{N_\\ell} \\sigma\\, \\Delta W^{(\\ell)}_n$ and $P_{\\ell-1} = \\sum_{m=1}^{N_{\\ell-1}} \\sigma\\, \\Delta W^{(\\ell-1)}_m$. In the Multilevel Monte Carlo (MLMC) method, coupling across levels is used to reduce the variance of the level differences.\n\nTwo coupling strategies are considered:\n- Correct coupling: for each $m \\in \\{1,\\dots,N_{\\ell-1}\\}$, set $\\Delta W^{(\\ell-1)}_m = \\Delta W^{(\\ell)}_{2m-1} + \\Delta W^{(\\ell)}_{2m}$, where the fine-level increments $\\{\\Delta W^{(\\ell)}_n\\}_{n=1}^{N_\\ell}$ are independent and identically distributed with law $\\mathcal{N}(0,h_\\ell)$.\n- Incorrect coupling: generate $\\{\\Delta W^{(\\ell-1)}_m\\}_{m=1}^{N_{\\ell-1}}$ as independent and identically distributed $\\mathcal{N}(0,h_{\\ell-1})$ increments that are independent of the fine-level increments $\\{\\Delta W^{(\\ell)}_n\\}_{n=1}^{N_\\ell}$.\n\nStarting only from the properties of independent Gaussian increments of Brownian motion and the variance additivity for independent random variables, derive $\\mathrm{Var}(P_\\ell - P_{\\ell-1})$ under each coupling strategy. Then, express the incorrect-coupling variance $\\mathrm{Var}(P_\\ell - P_{\\ell-1})$ as a simplified closed-form analytic expression in terms of $\\sigma$ and $T$. Your final answer must be this expression. No rounding is required.",
            "solution": "The problem requires the derivation of the variance of the difference between a fine and a coarse Euler-Maruyama approximation, $\\mathrm{Var}(P_\\ell - P_{\\ell-1})$, under two different coupling strategies for the stochastic differential equation (SDE) $dX_t = \\sigma\\, dW_t$ with $X_0 = 0$. The final answer is the specific expression for this variance under the incorrect coupling strategy.\n\nLet $P_\\ell$ and $P_{\\ell-1}$ be the random variables representing the numerical approximations at the terminal time $T$ on level $\\ell$ (fine) and level $\\ell-1$ (coarse), respectively. The general formula for the variance of the difference of two random variables is:\n$$\n\\mathrm{Var}(P_\\ell - P_{\\ell-1}) = \\mathrm{Var}(P_\\ell) + \\mathrm{Var}(P_{\\ell-1}) - 2 \\mathrm{Cov}(P_\\ell, P_{\\ell-1})\n$$\nWe analyze each coupling strategy by first calculating the individual variances, $\\mathrm{Var}(P_\\ell)$ and $\\mathrm{Var}(P_{\\ell-1})$, and then evaluating the covariance term.\n\nThe fine-level approximation is given by $P_\\ell = \\sum_{n=1}^{N_\\ell} \\sigma\\, \\Delta W^{(\\ell)}_n$, where $\\{\\Delta W^{(\\ell)}_n\\}_{n=1}^{N_\\ell}$ are independent and identically distributed (i.i.d.) random variables with distribution $\\mathcal{N}(0,h_\\ell)$. The variance of $P_\\ell$ is calculated using the property of variance additivity for independent random variables and the scaling property $\\mathrm{Var}(aZ) = a^2 \\mathrm{Var}(Z)$:\n$$\n\\mathrm{Var}(P_\\ell) = \\mathrm{Var}\\left(\\sigma \\sum_{n=1}^{N_\\ell} \\Delta W^{(\\ell)}_n\\right) = \\sigma^2 \\mathrm{Var}\\left(\\sum_{n=1}^{N_\\ell} \\Delta W^{(\\ell)}_n\\right)\n$$\nSince the increments $\\Delta W^{(\\ell)}_n$ are independent, the variance of their sum is the sum of their variances:\n$$\n\\mathrm{Var}(P_\\ell) = \\sigma^2 \\sum_{n=1}^{N_\\ell} \\mathrm{Var}(\\Delta W^{(\\ell)}_n) = \\sigma^2 \\sum_{n=1}^{N_\\ell} h_\\ell = \\sigma^2 N_\\ell h_\\ell\n$$\nGiven that the step size is $h_\\ell = T/N_\\ell$, we have $N_\\ell h_\\ell = T$. Therefore, the variance of the fine-level approximation is:\n$$\n\\mathrm{Var}(P_\\ell) = \\sigma^2 T\n$$\nSimilarly, the coarse-level approximation is given by $P_{\\ell-1} = \\sum_{m=1}^{N_{\\ell-1}} \\sigma\\, \\Delta W^{(\\ell-1)}_m$, where the increments $\\Delta W^{(\\ell-1)}_m$ are i.i.d. $\\mathcal{N}(0,h_{\\ell-1})$. Following the same logic:\n$$\n\\mathrm{Var}(P_{\\ell-1}) = \\sigma^2 \\sum_{m=1}^{N_{\\ell-1}} \\mathrm{Var}(\\Delta W^{(\\ell-1)}_m) = \\sigma^2 \\sum_{m=1}^{N_{\\ell-1}} h_{\\ell-1} = \\sigma^2 N_{\\ell-1} h_{\\ell-1}\n$$\nGiven $h_{\\ell-1} = T/N_{\\ell-1}$, we have $N_{\\ell-1} h_{\\ell-1} = T$. Thus, the variance of the coarse-level approximation is:\n$$\n\\mathrm{Var}(P_{\\ell-1}) = \\sigma^2 T\n$$\nWe now proceed to evaluate $\\mathrm{Var}(P_\\ell - P_{\\ell-1})$ for each coupling strategy.\n\n**1. Incorrect Coupling Strategy**\n\nUnder this strategy, the set of coarse increments $\\{\\Delta W^{(\\ell-1)}_m\\}_{m=1}^{N_{\\ell-1}}$ is generated independently from the set of fine increments $\\{\\Delta W^{(\\ell)}_n\\}_{n=1}^{N_\\ell}$. Since $P_\\ell$ is a function only of the fine increments and $P_{\\ell-1}$ is a function only of the coarse increments, the random variables $P_\\ell$ and $P_{\\ell-1}$ are independent. For independent random variables, the covariance is zero:\n$$\n\\mathrm{Cov}(P_\\ell, P_{\\ell-1}) = 0\n$$\nSubstituting this into the general variance formula gives:\n$$\n\\mathrm{Var}(P_\\ell - P_{\\ell-1}) = \\mathrm{Var}(P_\\ell) + \\mathrm{Var}(P_{\\ell-1})\n$$\nUsing the previously derived variances for $P_\\ell$ and $P_{\\ell-1}$:\n$$\n\\mathrm{Var}(P_\\ell - P_{\\ell-1}) = \\sigma^2 T + \\sigma^2 T = 2 \\sigma^2 T\n$$\nThis is the variance under the incorrect coupling.\n\n**2. Correct Coupling Strategy**\n\nUnder this strategy, the coarse increments are constructed from the fine increments: $\\Delta W^{(\\ell-1)}_m = \\Delta W^{(\\ell)}_{2m-1} + \\Delta W^{(\\ell)}_{2m}$. We compute the difference $P_\\ell - P_{\\ell-1}$ directly:\n$$\nP_\\ell - P_{\\ell-1} = \\sum_{n=1}^{N_\\ell} \\sigma\\, \\Delta W^{(\\ell)}_n - \\sum_{m=1}^{N_{\\ell-1}} \\sigma\\, \\Delta W^{(\\ell-1)}_m\n$$\nSubstituting the coupling definition into the second term:\n$$\n\\sum_{m=1}^{N_{\\ell-1}} \\sigma\\, \\Delta W^{(\\ell-1)}_m = \\sigma \\sum_{m=1}^{N_{\\ell-1}} \\left( \\Delta W^{(\\ell)}_{2m-1} + \\Delta W^{(\\ell)}_{2m} \\right)\n$$\nThis sum covers all the fine increments from $n=1$ to $n=2N_{\\ell-1}$. Since $N_\\ell = 2N_{\\ell-1}$, this sum is precisely the sum of all fine increments:\n$$\n\\sigma \\sum_{m=1}^{N_{\\ell-1}} \\left( \\Delta W^{(\\ell)}_{2m-1} + \\Delta W^{(\\ell)}_{2m} \\right) = \\sigma \\sum_{n=1}^{N_\\ell} \\Delta W^{(\\ell)}_n = P_\\ell\n$$\nTherefore, for this specific SDE, the coarse approximation $P_{\\ell-1}$ is identical to the fine approximation $P_\\ell$ when correct coupling is used. The difference is:\n$$\nP_\\ell - P_{\\ell-1} = P_\\ell - P_\\ell = 0\n$$\nThe variance of a constant is zero:\n$$\n\\mathrm{Var}(P_\\ell - P_{\\ell-1}) = \\mathrm{Var}(0) = 0\n$$\nThis result highlights the a priori variance reduction achieved by correct coupling, which is the foundational principle of the Multilevel Monte Carlo method. For this simple SDE, the reduction is maximal.\n\nThe problem asks for the simplified closed-form analytic expression for the incorrect-coupling variance in terms of $\\sigma$ and $T$. As derived above, this expression is $2 \\sigma^2 T$.",
            "answer": "$$\n\\boxed{2 \\sigma^{2} T}\n$$"
        },
        {
            "introduction": "Understanding that coupling reduces the variance of level differences, $Y_\\ell = P_\\ell - P_{\\ell-1}$, naturally leads to the next critical question: how should we allocate a finite computational budget across the different levels? This practice  guides you through the cornerstone optimization problem of MLMC, where the goal is to minimize total computational cost for a given target variance. By applying the method of Lagrange multipliers, you will derive the celebrated formula for the optimal number of samples, $N_\\ell$, required on each level.",
            "id": "3783568",
            "problem": "Consider a hierarchical sequence of discretizations indexed by levels $\\ell \\in \\{0,1,\\dots,L\\}$ for approximating a scalar quantity of interest $P$ arising in a multiscale model. On each level $\\ell$, define the level-difference random variable $\\Delta P_{\\ell} = P_{\\ell} - P_{\\ell-1}$ with $P_{-1} \\equiv 0$, and assume that, conditional on level, independent and identically distributed samples of $\\Delta P_{\\ell}$ can be generated. Let $Y$ denote the Multilevel Monte Carlo (MLMC) estimator given by\n$$\nY \\;=\\; \\sum_{\\ell=0}^{L} \\frac{1}{N_{\\ell}} \\sum_{i=1}^{N_{\\ell}} \\Delta P_{\\ell}^{(i)},\n$$\nwhere $N_{\\ell}$ is the number of samples drawn at level $\\ell$ and $\\Delta P_{\\ell}^{(i)}$ are independent copies of $\\Delta P_{\\ell}$. Assume the following fundamental facts: for each level $\\ell$, the variance of the sample mean equals the level variance divided by the number of samples, that is,\n$$\n\\operatorname{Var}\\!\\left(\\frac{1}{N_{\\ell}} \\sum_{i=1}^{N_{\\ell}} \\Delta P_{\\ell}^{(i)}\\right) \\;=\\; \\frac{V_{\\ell}}{N_{\\ell}},\n$$\nwhere $V_{\\ell} = \\operatorname{Var}(\\Delta P_{\\ell})$, and that the total variance of $Y$ is the sum of the independent level variances. Further, suppose that the expected computational cost to generate a single sample of $\\Delta P_{\\ell}$ is $C_{\\ell}$, so the total expected cost is \n$$\n\\mathcal{C} \\;=\\; \\sum_{\\ell=0}^{L} N_{\\ell} C_{\\ell}.\n$$\nIn a pilot study, you have obtained empirical estimators $\\hat{V}_{\\ell}$ and $\\hat{C}_{\\ell}$ for $V_{\\ell}$ and $C_{\\ell}$, respectively. You wish to choose $\\{N_{\\ell}\\}_{\\ell=0}^{L}$ to minimize $\\mathcal{C}$ subject to the variance constraint \n$$\n\\operatorname{Var}(Y) \\;=\\; \\sum_{\\ell=0}^{L} \\frac{V_{\\ell}}{N_{\\ell}} \\;\\le\\; \\frac{1}{2}\\,\\varepsilon^{2},\n$$\nwhere $\\varepsilon>0$ is a prescribed accuracy tolerance for the mean-squared error and the remaining half of the error budget is reserved for the squared bias via an appropriate choice of $L$. Treat $\\{N_{\\ell}\\}_{\\ell=0}^{L}$ as continuous positive decision variables and use the pilot estimators in place of the unknown true $V_{\\ell}$ and $C_{\\ell}$. Derive a closed-form expression for the optimal allocation $N_{\\ell}$ in terms of $\\varepsilon$, $\\hat{V}_{\\ell}$, $\\hat{C}_{\\ell}$, and $L$ that minimizes the cost while meeting the variance constraint with equality. Provide your final result as a single analytical expression for $N_{\\ell}$ in terms of these symbols. No rounding is required and no units apply.",
            "solution": "The task is to find the optimal allocation of samples, $\\{N_{\\ell}\\}_{\\ell=0}^{L}$, for a Multilevel Monte Carlo (MLMC) estimator. This is a classic constrained optimization problem where the goal is to minimize the total computational cost subject to a constraint on the total variance.\n\nThe problem can be formally stated as:\nMinimize the total expected cost,\n$$\n\\mathcal{C}(\\{N_{\\ell}\\}_{\\ell=0}^{L}) \\;=\\; \\sum_{\\ell=0}^{L} N_{\\ell} \\hat{C}_{\\ell}\n$$\nsubject to the variance constraint being met with equality,\n$$\n\\operatorname{Var}(Y) \\;=\\; \\sum_{\\ell=0}^{L} \\frac{\\hat{V}_{\\ell}}{N_{\\ell}} \\;=\\; \\frac{1}{2}\\,\\varepsilon^{2}.\n$$\nHere, $\\{\\hat{V}_{\\ell}\\}_{\\ell=0}^{L}$ and $\\{\\hat{C}_{\\ell}\\}_{\\ell=0}^{L}$ are the empirically estimated level variances and costs, respectively. The sample sizes $\\{N_{\\ell}\\}_{\\ell=0}^{L}$ are treated as continuous positive variables.\n\nThis constrained optimization problem is amenable to the method of Lagrange multipliers. We define the Lagrangian function $\\mathcal{L}$ as the objective function plus a Lagrange multiplier $\\lambda$ times the constraint function:\n$$\n\\mathcal{L}(\\{N_{\\ell}\\}, \\lambda) \\;=\\; \\sum_{\\ell=0}^{L} N_{\\ell} \\hat{C}_{\\ell} + \\lambda \\left( \\left(\\sum_{\\ell=0}^{L} \\frac{\\hat{V}_{\\ell}}{N_{\\ell}}\\right) - \\frac{1}{2}\\varepsilon^{2} \\right).\n$$\nTo find the minimum, we compute the partial derivatives of $\\mathcal{L}$ with respect to each $N_{k}$ for $k \\in \\{0, 1, \\dots, L\\}$ and set them to zero.\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial N_{k}} \\;=\\; \\hat{C}_{k} - \\lambda \\frac{\\hat{V}_{k}}{N_{k}^{2}} \\;=\\; 0.\n$$\nSince $\\hat{C}_{k} > 0$ and $\\hat{V}_{k} \\ge 0$, and we require $N_k > 0$, it follows that the Lagrange multiplier $\\lambda$ must be positive. We can rearrange the equation to solve for $N_{k}$:\n$$\n\\hat{C}_{k} N_{k}^{2} \\;=\\; \\lambda \\hat{V}_{k} \\quad \\implies \\quad N_{k}^{2} \\;=\\; \\lambda \\frac{\\hat{V}_{k}}{\\hat{C}_{k}}.\n$$\nTaking the positive square root, as $N_k$ must be a positive quantity, we get:\n$$\nN_{k} \\;=\\; \\sqrt{\\lambda} \\sqrt{\\frac{\\hat{V}_{k}}{\\hat{C}_{k}}}.\n$$\nThis expression gives the optimal number of samples for each level $k$ in terms of the unknown Lagrange multiplier $\\lambda$. To determine $\\lambda$, we substitute this expression for $N_{k}$ back into the variance constraint equation:\n$$\n\\sum_{k=0}^{L} \\frac{\\hat{V}_{k}}{N_{k}} \\;=\\; \\frac{1}{2}\\varepsilon^{2}.\n$$\n$$\n\\sum_{k=0}^{L} \\frac{\\hat{V}_{k}}{\\sqrt{\\lambda} \\sqrt{\\frac{\\hat{V}_{k}}{\\hat{C}_{k}}}} \\;=\\; \\frac{1}{2}\\varepsilon^{2}.\n$$\nSimplifying the term inside the summation:\n$$\n\\frac{\\hat{V}_{k}}{\\sqrt{\\frac{\\hat{V}_{k}}{\\hat{C}_{k}}}} \\;=\\; \\frac{(\\sqrt{\\hat{V}_{k}})^{2} \\sqrt{\\hat{C}_{k}}}{\\sqrt{\\hat{V}_{k}}} \\;=\\; \\sqrt{\\hat{V}_{k} \\hat{C}_{k}}.\n$$\nThe constraint equation becomes:\n$$\n\\sum_{k=0}^{L} \\frac{\\sqrt{\\hat{V}_{k} \\hat{C}_{k}}}{\\sqrt{\\lambda}} \\;=\\; \\frac{1}{2}\\varepsilon^{2}.\n$$\nWe can factor out the term $1/\\sqrt{\\lambda}$ from the summation:\n$$\n\\frac{1}{\\sqrt{\\lambda}} \\sum_{k=0}^{L} \\sqrt{\\hat{V}_{k} \\hat{C}_{k}} \\;=\\; \\frac{1}{2}\\varepsilon^{2}.\n$$\nNow, we solve for $\\sqrt{\\lambda}$:\n$$\n\\sqrt{\\lambda} \\;=\\; \\frac{2}{\\varepsilon^{2}} \\sum_{k=0}^{L} \\sqrt{\\hat{V}_{k} \\hat{C}_{k}}.\n$$\nFinally, we substitute this expression for $\\sqrt{\\lambda}$ back into our equation for the optimal $N_{\\ell}$ (using the index $\\ell$ as requested in the problem statement):\n$$\nN_{\\ell} \\;=\\; \\sqrt{\\lambda} \\sqrt{\\frac{\\hat{V}_{\\ell}}{\\hat{C}_{\\ell}}} \\;=\\; \\left( \\frac{2}{\\varepsilon^{2}} \\sum_{k=0}^{L} \\sqrt{\\hat{V}_{k} \\hat{C}_{k}} \\right) \\sqrt{\\frac{\\hat{V}_{\\ell}}{\\hat{C}_{\\ell}}}.\n$$\nThis equation provides the closed-form expression for the optimal number of samples $N_{\\ell}$ at each level $\\ell$, which minimizes the total computational cost while satisfying the specified variance constraint.",
            "answer": "$$\\boxed{ N_{\\ell} = \\frac{2}{\\varepsilon^{2}} \\sqrt{\\frac{\\hat{V}_{\\ell}}{\\hat{C}_{\\ell}}} \\sum_{k=0}^{L} \\sqrt{\\hat{V}_{k} \\hat{C}_{k}} }$$"
        },
        {
            "introduction": "The theoretical formula for optimal sample allocation is powerful, but it depends on per-level variances and costs, $V_\\ell$ and $C_\\ell$, which are typically unknown in practical applications. This challenge brings us to the design of a realistic, adaptive algorithm. This exercise  asks you to synthesize the principles of MLMC into a practical workflow, evaluating strategies for an algorithm that uses pilot samples to estimate unknowns and iteratively refines its sample allocation to meet an error target efficiently. This practice bridges the crucial gap between abstract theory and robust implementation.",
            "id": "3783570",
            "problem": "Consider a multilevel estimator for a quantity of interest arising in multiscale modeling, where a sequence of discretizations at levels $\\ell = 0, 1, \\dots, L$ with mesh sizes $h_\\ell$ approximates a fine-scale model. Let $P_\\ell$ denote the level-$\\ell$ approximation of a scalar functional of the solution, and define the level differences by $Y_\\ell = P_\\ell - P_{\\ell-1}$ with $P_{-1} = 0$. The Multilevel Monte Carlo (MLMC) estimator of $\\mathbb{E}[P_L]$ uses independent samples of $Y_\\ell$ at each level $\\ell$ and forms the estimator $\\widehat{Q} = \\sum_{\\ell=0}^{L} \\overline{Y}_\\ell$, where $\\overline{Y}_\\ell$ is the sample mean of $Y_\\ell$ from $N_\\ell$ independent realizations. The total Monte Carlo variance is $\\operatorname{Var}(\\widehat{Q}) = \\sum_{\\ell=0}^{L} \\operatorname{Var}(Y_\\ell)/N_\\ell$, and the total expected cost is $\\sum_{\\ell=0}^{L} C_\\ell N_\\ell$, where $C_\\ell$ is the expected cost per sample of $Y_\\ell$.\n\nSuppose one seeks to design an adaptive Multilevel Monte Carlo algorithm that:\n- Initializes with pilot sampling to estimate the per-level variance and cost, and\n- Iteratively updates the per-level sample sizes $N_\\ell$ to meet a prescribed Monte Carlo variance target with minimal computational cost.\n\nAssume the following baseline facts and definitions:\n- The Monte Carlo variance of the sample mean at level $\\ell$ is $\\operatorname{Var}(\\overline{Y}_\\ell) = \\operatorname{Var}(Y_\\ell)/N_\\ell$ by independence and the definition of variance of a sample mean.\n- The total variance of the MLMC estimator is the sum of per-level variances, that is $\\sum_{\\ell=0}^{L} \\operatorname{Var}(Y_\\ell)/N_\\ell$, because the samples across levels are taken independently.\n- The total expected cost is $\\sum_{\\ell=0}^{L} C_\\ell N_\\ell$, and the mean per-sample cost $C_\\ell$ at level $\\ell$ can be estimated by empirical averages of measured wall-clock times or operation counts.\n\nThe algorithm must target a prescribed variance threshold $\\sigma^2$ for the Monte Carlo component, that is enforce $\\sum_{\\ell=0}^{L} \\operatorname{Var}(Y_\\ell)/N_\\ell \\le \\sigma^2$ while asymptotically minimizing $\\sum_{\\ell=0}^{L} C_\\ell N_\\ell$ under the constraint, using only estimators available from sampling.\n\nWhich option most accurately describes a scientifically sound adaptive MLMC algorithm that uses an initial pilot of samples per level to estimate $\\widehat{V}_\\ell \\approx \\operatorname{Var}(Y_\\ell)$ and $\\widehat{C}_\\ell \\approx C_\\ell$, and then iteratively updates $N_\\ell$ to attain the variance target $\\sigma^2$ with minimal expected cost?\n\nA. Initialize with $M_0$ pilot samples on each level $\\ell$, compute the empirical variance $\\widehat{V}_\\ell$ of $Y_\\ell$ and the empirical mean cost $\\widehat{C}_\\ell$ per sample. Then determine target allocations by solving the constrained minimization problem $\\min_{N_\\ell > 0} \\sum_{\\ell=0}^{L} \\widehat{C}_\\ell N_\\ell$ subject to $\\sum_{\\ell=0}^{L} \\widehat{V}_\\ell/N_\\ell \\le \\sigma^2$ via Lagrange multipliers, yielding\n$$\nN_\\ell^{\\star} \\;=\\; \\frac{1}{\\sigma^2}\\left(\\sum_{j=0}^{L} \\sqrt{\\widehat{V}_j\\,\\widehat{C}_j}\\right)\\sqrt{\\frac{\\widehat{V}_\\ell}{\\widehat{C}_\\ell}}.\n$$\nRound $N_\\ell^{\\star}$ to integers (e.g., by ceiling), add incremental samples on each level to reach the current targets, update $\\widehat{V}_\\ell$ and $\\widehat{C}_\\ell$ with the enlarged data, and recompute $N_\\ell^{\\star}$. Iterate this process until the empirical variance $\\sum_{\\ell=0}^{L} \\widehat{V}_\\ell/N_\\ell$ is at or below $\\sigma^2$. Optionally, monitor the bias by checking $\\left|\\mathbb{E}[Y_L]\\right|$ via the sample mean at the finest level and, if too large relative to a bias tolerance, increase $L$ and repeat the pilot-estimate-update cycle.\n\nB. Initialize with $M_0$ pilot samples on each level $\\ell$, compute $\\widehat{V}_\\ell$ and $\\widehat{C}_\\ell$, and then allocate samples proportionally to the product of variance and cost, that is set\n$$\nN_\\ell^{\\star} \\;\\propto\\; \\widehat{V}_\\ell\\,\\widehat{C}_\\ell,\n$$\nnormalizing so that the total variance drops below $\\sigma^2$. Iterate by uniformly scaling all $N_\\ell$ together until $\\sum_{\\ell=0}^{L} \\widehat{V}_\\ell/N_\\ell \\le \\sigma^2$.\n\nC. Skip pilot sampling. Initialize with a uniform number of samples $N_\\ell = N$ across levels, and iteratively double $N$ until $\\sum_{\\ell=0}^{L} \\widehat{V}_\\ell/N_\\ell \\le \\sigma^2$, where $\\widehat{V}_\\ell$ is approximated from an assumed model $\\widehat{V}_\\ell \\approx a h_\\ell^{\\beta}$ with known exponent $\\beta$ and unknown constant $a$. Allocate samples to favor cheaper levels by setting\n$$\nN_\\ell^{\\star} \\;\\propto\\; \\frac{1}{\\widehat{C}_\\ell}.\n$$\n\nD. Initialize with $M_0$ pilot samples on the coarsest level only, estimate $\\widehat{V}_0$ and $\\widehat{C}_0$, and set the initial allocation by enforcing\n$$\n\\sum_{\\ell=0}^{L} \\frac{\\widehat{V}_0}{N_\\ell} \\;\\le\\; \\sigma^2,\n$$\nwhich yields identical $N_\\ell$ across levels. Update by adding the same number of samples to each level at each iteration, regardless of per-level variance or cost, until the variance target is met.\n\nE. Initialize with $M_0$ pilot samples on each level, compute $\\widehat{V}_\\ell$ and $\\widehat{C}_\\ell$, and then choose\n$$\nN_\\ell^{\\star} \\;=\\; \\left\\lceil \\frac{\\widehat{V}_\\ell}{\\sigma^2} \\right\\rceil,\n$$\nso that each level individually satisfies $\\widehat{V}_\\ell/N_\\ell \\le \\sigma^2$. Iterate by refining levels with the largest $\\widehat{V}_\\ell$ until convergence, ignoring $\\widehat{C}_\\ell$ since cost is dominated by the finest level.\n\nSelect the single best option.",
            "solution": "### Derivation of the Optimal Sample Allocation\n\nThe goal is to minimize the total cost, $\\mathcal{C}(\\{N_\\ell\\}) = \\sum_{\\ell=0}^{L} C_\\ell N_\\ell$, subject to the constraint on the total variance, $\\mathcal{V}(\\{N_\\ell\\}) = \\sum_{\\ell=0}^{L} V_\\ell/N_\\ell \\le \\sigma^2$, where for notational simplicity we let $V_\\ell = \\operatorname{Var}(Y_\\ell)$. To achieve the minimum cost, the variance constraint must be active, i.e., $\\sum_{\\ell=0}^{L} V_\\ell/N_\\ell = \\sigma^2$.\n\nWe can solve this constrained optimization problem using the method of Lagrange multipliers. We define the Lagrangian $\\mathcal{L}$ treating the sample sizes $N_\\ell$ as continuous positive real numbers:\n$$\n\\mathcal{L}(\\{N_\\ell\\}, \\lambda) = \\sum_{\\ell=0}^{L} C_\\ell N_\\ell + \\lambda \\left( \\sum_{\\ell=0}^{L} \\frac{V_\\ell}{N_\\ell} - \\sigma^2 \\right)\n$$\nTo find the minimum, we set the partial derivatives of $\\mathcal{L}$ with respect to each $N_k$ to zero:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial N_k} = C_k - \\lambda \\frac{V_k}{N_k^2} = 0 \\quad \\text{for } k = 0, \\dots, L\n$$\nThis gives a relationship between $N_k$, $C_k$, and $V_k$:\n$$\nC_k N_k^2 = \\lambda V_k \\implies N_k = \\sqrt{\\lambda} \\sqrt{\\frac{V_k}{C_k}}\n$$\nThis shows that the optimal number of samples $N_k$ is proportional to the square root of the ratio of the variance to the cost at that level.\n\nTo determine the Lagrange multiplier $\\lambda$, we substitute this expression for $N_k$ back into the active variance constraint:\n$$\n\\sum_{\\ell=0}^{L} \\frac{V_\\ell}{N_\\ell} = \\sum_{\\ell=0}^{L} \\frac{V_\\ell}{\\sqrt{\\lambda} \\sqrt{V_\\ell/C_\\ell}} = \\sum_{\\ell=0}^{L} \\frac{\\sqrt{V_\\ell C_\\ell}}{\\sqrt{\\lambda}} = \\sigma^2\n$$\nSolving for $\\sqrt{\\lambda}$:\n$$\n\\frac{1}{\\sqrt{\\lambda}} \\left( \\sum_{\\ell=0}^{L} \\sqrt{V_\\ell C_\\ell} \\right) = \\sigma^2 \\implies \\sqrt{\\lambda} = \\frac{1}{\\sigma^2} \\sum_{j=0}^{L} \\sqrt{V_j C_j}\n$$\nFinally, substituting this expression for $\\sqrt{\\lambda}$ back into the equation for $N_k$ gives the optimal allocation:\n$$\nN_k = \\left( \\frac{1}{\\sigma^2} \\sum_{j=0}^{L} \\sqrt{V_j C_j} \\right) \\sqrt{\\frac{V_k}{C_k}}\n$$\nIn a practical adaptive algorithm, the true variances $V_\\ell$ and costs $C_\\ell$ are unknown. They are replaced by their empirical estimates, $\\widehat{V}_\\ell$ and $\\widehat{C}_\\ell$, obtained from samples. The number of samples $N_\\ell$ must be an integer, so the calculated value is typically rounded up using the ceiling function, $N_\\ell = \\lceil N_\\ell^{\\star} \\rceil$, to ensure the variance constraint is satisfied. The algorithm proceeds iteratively: initial pilot samples provide first estimates for $\\widehat{V}_\\ell$ and $\\widehat{C}_\\ell$; the optimal $N_\\ell$ are computed; more samples are generated to meet these targets; the estimates are updated; and the process repeats until convergence. A comprehensive algorithm also addresses the bias error, typically by adapting the number of levels $L$.\n\n### Option-by-Option Analysis\n\nNow we evaluate each option based on the derived principles.\n\n**Option A:** This option proposes an algorithm that starts with pilot sampling to compute estimates $\\widehat{V}_\\ell$ and $\\widehat{C}_\\ell$. It then uses the formula\n$$\nN_\\ell^{\\star} \\;=\\; \\frac{1}{\\sigma^2}\\left(\\sum_{j=0}^{L} \\sqrt{\\widehat{V}_j\\,\\widehat{C}_j}\\right)\\sqrt{\\frac{\\widehat{V}_\\ell}{\\widehat{C}_\\ell}}\n$$\nto determine the target sample sizes. This formula is identical to the one derived from the constrained cost-minimization problem, with the true quantities correctly replaced by their sample-based estimators. The description of the iterative process—rounding, adding incremental samples, updating estimates, and re-computing targets—is a standard and correct implementation of an adaptive MLMC method. Furthermore, the optional but crucial step of monitoring the bias (approximated by the mean of the finest-level difference, $\\overline{Y}_L$) and increasing $L$ if necessary, describes a complete and robust algorithm.\n**Verdict: Correct.**\n\n**Option B:** This option suggests an allocation $N_\\ell^{\\star} \\propto \\widehat{V}_\\ell\\,\\widehat{C}_\\ell$. As derived, the optimal allocation is $N_\\ell^{\\star} \\propto \\sqrt{\\widehat{V}_\\ell/\\widehat{C}_\\ell}$. The proposed proportionality is fundamentally incorrect and does not result from the cost-minimization principle. This allocation would grossly over-sample levels where both variance and cost are high, leading to an inefficient use of computational resources.\n**Verdict: Incorrect.**\n\n**Option C:** This option suggests skipping pilot sampling and using a uniform number of samples $N$ that is iteratively doubled. This is non-adaptive and highly inefficient. A core strength of MLMC is the non-uniform allocation of samples across levels. The proposed allocation rule $N_\\ell^{\\star} \\propto 1/\\widehat{C}_\\ell$ ignores the variance $\\widehat{V}_\\ell$ entirely, which is a critical factor in the optimization. Allocating samples based only on cost is suboptimal, as it may neglect high-variance levels that are crucial for reducing the overall statistical error.\n**Verdict: Incorrect.**\n\n**Option D:** This option suggests initializing with samples on the coarsest level only and using its variance $\\widehat{V}_0$ as a proxy for all other levels. This is a flawed assumption, as $\\operatorname{Var}(Y_\\ell)$ typically decreases significantly with increasing $\\ell$. The strategy of adding the same number of samples to each level in subsequent iterations is non-adaptive and ignores the vast differences in cost and variance across levels, thus defeating the purpose of the MLMC framework.\n**Verdict: Incorrect.**\n\n**Option E:** This option proposes setting $N_\\ell^{\\star} = \\lceil \\widehat{V}_\\ell / \\sigma^2 \\rceil$. This formula ensures that the variance contribution from each level *individually* is less than or equal to $\\sigma^2$. The total variance would then be approximately $\\sum_{\\ell=0}^{L} (\\sigma^2) = (L+1)\\sigma^2$ (or less), which is not the target. This approach is massively over-conservative, leading to a much higher computational cost than necessary. The total variance constraint $\\sum_{\\ell=0}^{L} \\widehat{V}_\\ell/N_\\ell \\le \\sigma^2$ allows for a trade-off between levels, which this strategy ignores. Ignoring the cost $\\widehat{C}_\\ell$ is also a fundamental error that contradicts the goal of cost minimization.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}