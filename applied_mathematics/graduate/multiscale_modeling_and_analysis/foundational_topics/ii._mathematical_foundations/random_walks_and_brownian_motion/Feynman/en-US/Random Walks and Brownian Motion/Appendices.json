{
    "hands_on_practices": [
        {
            "introduction": "We begin our practical exploration with the most fundamental model: the one-dimensional symmetric random walk. A key question in the study of any stochastic process is whether it tends to return to its starting point. This exercise tasks you with calculating the exact probability of a random walker returning to the origin after an even number of steps, a classic problem solved through combinatorial path counting. By then applying asymptotic analysis, you will deduce whether the walk is recurrent—a foundational property that distinguishes random walks in different dimensions .",
            "id": "3800177",
            "problem": "Consider a discrete-time one-dimensional symmetric random walk $\\{S_{k}\\}_{k \\geq 0}$ defined by $S_{0} = 0$ and $S_{k} = \\sum_{j=1}^{k} X_{j}$, where $\\{X_{j}\\}_{j \\geq 1}$ are independent and identically distributed random variables taking values $+1$ and $-1$ with probability $1/2$ each. The walk is viewed as a single-scale model at the lattice level, and its coarse-grained behavior relates to Brownian motion under diffusive scaling. Starting from the foundational definitions of independence, identical distribution, and discrete-time Markov property, do the following:\n\n1. Compute the exact probability $P(S_{2n} = 0)$, the probability that the walk returns to the origin at time $2n$, by counting admissible paths and normalizing by the total number of paths. Express your answer in a closed-form analytic expression in terms of $n$.\n\n2. Using asymptotic analysis based on Stirling’s formula and the recurrence criterion for discrete-time Markov chains, deduce whether the one-dimensional symmetric random walk is recurrent. Your reasoning must begin from the fundamental discrete path-counting perspective and may invoke well-tested asymptotic approximations and standard recurrence criteria from Markov chain theory. State the final conclusion as the probability that the random walk returns to the origin infinitely often.\n\nYour final answer must be a single closed-form analytic expression for $P(S_{2n} = 0)$ and a single real number for the recurrence conclusion, written together as a two-entry row matrix. No rounding is required, and no physical units are involved.",
            "solution": "The problem is divided into two parts. The first part requires the calculation of an exact probability for a one-dimensional symmetric random walk. The second part asks for a conclusion on the recurrence of this walk using asymptotic analysis.\n\nPart 1: Computation of $P(S_{2n} = 0)$\n\nA one-dimensional symmetric random walk $\\{S_k\\}_{k \\geq 0}$ is defined by $S_0 = 0$ and $S_k = \\sum_{j=1}^{k} X_j$, where the $X_j$ are independent and identically distributed (i.i.d.) random variables with $P(X_j = +1) = P(X_j = -1) = 1/2$.\n\nFor the walker to be at the origin at time $k$, its position $S_k$ must be $0$. Since each step $X_j$ is either $+1$ or $-1$, the position $S_k$ after $k$ steps has the same parity as $k$. That is, if $k$ is odd, $S_k$ must be odd, and if $k$ is even, $S_k$ must be even. For $S_k$ to be $0$, $k$ must necessarily be an even integer. Therefore, we consider the position at time $k=2n$ for some non-negative integer $n$.\n\nLet $n_+$ be the number of steps to the right (value $+1$) and $n_-$ be the number of steps to the left (value $-1$) in a path of length $2n$.\nThe total number of steps is $2n$, so we have the relation:\n$$n_+ + n_- = 2n$$\nThe final position is the sum of all steps:\n$$S_{2n} = (+1)n_+ + (-1)n_- = n_+ - n_-$$\nFor the walk to return to the origin, we must have $S_{2n} = 0$, which implies:\n$$n_+ - n_- = 0 \\implies n_+ = n_-$$\nSubstituting $n_+ = n_-$ into the first equation yields $n_+ + n_+ = 2n$, which gives $2n_+ = 2n$ and thus $n_+ = n$. Consequently, $n_- = n$.\nThis means that for a path of length $2n$ to end at the origin, it must consist of exactly $n$ steps of $+1$ and $n$ steps of $-1$.\n\nThe total number of distinct paths of length $2n$ is $2^{2n}$, since at each of the $2n$ steps, there are two choices, and each choice is independent.\nThe probability of any single specific path (e.g., $n$ steps of $+1$ followed by $n$ steps of $-1$) is $(\\frac{1}{2})^{2n}$, because the steps are i.i.d. random variables.\n\nThe number of admissible paths that end at the origin is the number of ways to arrange $n$ symbols '$+1$' and $n$ symbols '$-1$' in a sequence of length $2n$. This is a combinatorial problem, and the number of such arrangements is given by the binomial coefficient $\\binom{2n}{n}$.\n$$\\text{Number of admissible paths} = \\binom{2n}{n} = \\frac{(2n)!}{n!n!}$$\nThe probability of the event $S_{2n} = 0$ is the product of the number of admissible paths and the probability of any single path:\n$$P(S_{2n} = 0) = (\\text{Number of admissible paths}) \\times (\\text{Probability of one path})$$\n$$P(S_{2n} = 0) = \\binom{2n}{n} \\left(\\frac{1}{2}\\right)^{2n}$$\nThis is the closed-form analytic expression for the probability of the walk returning to the origin at time $2n$.\n\nPart 2: Recurrence of the One-Dimensional Symmetric Random Walk\n\nAccording to the theory of discrete-time Markov chains, a state $i$ is defined as recurrent if, starting from state $i$, the probability of eventually returning to state $i$ is $1$. For an irreducible Markov chain (where every state can be reached from every other state), all states are of the same type: either all are recurrent or all are transient. The one-dimensional symmetric random walk on the integers $\\mathbb{Z}$ is irreducible.\n\nA standard criterion for recurrence of a state $i$ is that the expected number of returns to state $i$ is infinite. This is equivalent to the divergence of the sum of the probabilities of being at state $i$ at time $k$, over all $k \\geq 1$:\n$$\\text{State } i \\text{ is recurrent} \\iff \\sum_{k=1}^{\\infty} p_{ii}^{(k)} = \\infty$$\nwhere $p_{ii}^{(k)}$ is the probability of being at state $i$ at time $k$ after starting at state $i$. In our case, the state is the origin $i=0$ and $p_{00}^{(k)} = P(S_k = 0 \\mid S_0 = 0) = P(S_k=0)$. As established, $P(S_k = 0) = 0$ for odd $k$. Thus, the criterion becomes:\n$$\\text{State } 0 \\text{ is recurrent} \\iff \\sum_{n=1}^{\\infty} P(S_{2n} = 0) = \\infty$$\nTo analyze the convergence of this series, we examine the asymptotic behavior of $P(S_{2n} = 0)$ for large $n$ using Stirling's approximation for the factorial function: $m! \\sim \\sqrt{2\\pi m} \\left(\\frac{m}{e}\\right)^m$.\n\nLet's apply this to the expression for $P(S_{2n} = 0)$:\n$$P(S_{2n} = 0) = \\frac{(2n)!}{(n!)^2} \\left(\\frac{1}{2}\\right)^{2n}$$\nThe binomial coefficient $\\binom{2n}{n}$ is approximated as:\n$$\\binom{2n}{n} = \\frac{(2n)!}{(n!)^2} \\sim \\frac{\\sqrt{2\\pi(2n)} \\left(\\frac{2n}{e}\\right)^{2n}}{\\left(\\sqrt{2\\pi n} \\left(\\frac{n}{e}\\right)^n\\right)^2} = \\frac{\\sqrt{4\\pi n} \\left(\\frac{2n}{e}\\right)^{2n}}{2\\pi n \\left(\\frac{n}{e}\\right)^{2n}}$$\nSimplifying the expression:\n$$\\binom{2n}{n} \\sim \\frac{2\\sqrt{\\pi n}}{2\\pi n} \\frac{(2n)^{2n}}{(n)^{2n}} \\frac{e^{-2n}}{e^{-2n}} = \\frac{1}{\\sqrt{\\pi n}} \\frac{2^{2n}n^{2n}}{n^{2n}} = \\frac{2^{2n}}{\\sqrt{\\pi n}}$$\nNow we substitute this back into the expression for the probability:\n$$P(S_{2n} = 0) = \\binom{2n}{n} \\left(\\frac{1}{2}\\right)^{2n} \\sim \\frac{2^{2n}}{\\sqrt{\\pi n}} \\left(\\frac{1}{2}\\right)^{2n} = \\frac{1}{\\sqrt{\\pi n}}$$\nThe series $\\sum_{n=1}^{\\infty} P(S_{2n} = 0)$ thus behaves like the series $\\sum_{n=1}^{\\infty} \\frac{1}{\\sqrt{\\pi n}}$. This is a p-series of the form $\\sum_{n=1}^{\\infty} c \\cdot n^{-p}$ with $c = 1/\\sqrt{\\pi}$ and $p=1/2$. The p-series test states that such a series diverges if $p \\leq 1$ and converges if $p > 1$. Since $p = 1/2 \\leq 1$, the series diverges.\n$$\\sum_{n=1}^{\\infty} P(S_{2n} = 0) = \\infty$$\nTherefore, the state $0$ is recurrent. Since the random walk is irreducible, all states are recurrent.\n\nA fundamental theorem in Markov chain theory states that if a state is recurrent, then the probability of visiting that state infinitely often, starting from that state, is $1$. This is a consequence of the second Borel-Cantelli lemma applied to the events of returning to the origin.\nThus, the probability that the one-dimensional symmetric random walk returns to the origin infinitely often is $1$.\n\nThe two parts of the final answer are the expression for $P(S_{2n} = 0)$ and the value $1$ for the probability of returning to the origin infinitely often.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\binom{2n}{n} \\left(\\frac{1}{2}\\right)^{2n} & 1\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Having explored the qualitative property of recurrence, we now turn to a quantitative measure of a random walk's behavior. This practice moves to a two-dimensional lattice and focuses on calculating the mean squared displacement, which measures how far, on average, the particle has traveled from its starting point. This quantity is a cornerstone of diffusion theory, and its linear growth with the number of steps, which you will derive, is the defining signature of normal diffusion and the crucial link between the microscopic random walk and macroscopic Brownian motion .",
            "id": "1330646",
            "problem": "A simplified model for the diffusion of a particle on a crystal lattice describes its movement as a random walk on a 2D Cartesian grid. The particle starts at the origin $(0,0)$. At each discrete time step, the particle moves from its current position to an adjacent grid point. The step is chosen with equal probability from the four possible moves of unit length along the grid axes: a step in the positive x-direction, negative x-direction, positive y-direction, or negative y-direction. After a total of $N$ such steps, the particle is at a final position $(X_N, Y_N)$.\n\nCalculate the expected value of the squared Euclidean distance of the particle from the origin after $N$ steps. Express your answer as a function of $N$.",
            "solution": "Let the step at time $t$ be the random vector $(\\Delta X_{t}, \\Delta Y_{t})$, where\n$$\n(\\Delta X_{t}, \\Delta Y_{t})=\\begin{cases}\n(1,0) & \\text{with probability } \\frac{1}{4},\\\\\n(-1,0) & \\text{with probability } \\frac{1}{4},\\\\\n(0,1) & \\text{with probability } \\frac{1}{4},\\\\\n(0,-1) & \\text{with probability } \\frac{1}{4}.\n\\end{cases}\n$$\nThe position after $N$ steps is\n$$\nX_{N}=\\sum_{t=1}^{N}\\Delta X_{t}, \\qquad Y_{N}=\\sum_{t=1}^{N}\\Delta Y_{t}.\n$$\nWe seek $E[X_{N}^{2}+Y_{N}^{2}]$, which by linearity of expectation is\n$$\nE[X_{N}^{2}+Y_{N}^{2}]=E[X_{N}^{2}]+E[Y_{N}^{2}].\n$$\nFor the $x$-component, compute the first two moments of a single-step increment:\n$$\nE[\\Delta X_{t}]=\\left(1\\right)\\frac{1}{4}+\\left(-1\\right)\\frac{1}{4}+\\left(0\\right)\\frac{1}{2}=0,\n$$\n$$\nE[\\Delta X_{t}^{2}]=\\left(1^{2}\\right)\\frac{1}{4}+\\left((-1)^{2}\\right)\\frac{1}{4}+\\left(0^{2}\\right)\\frac{1}{2}=\\frac{1}{2}.\n$$\nThus $\\operatorname{Var}(\\Delta X_{t})=E[\\Delta X_{t}^{2}]-\\left(E[\\Delta X_{t}]\\right)^{2}=\\frac{1}{2}$. Since steps are independent and identically distributed,\n$$\n\\operatorname{Var}(X_{N})=\\sum_{t=1}^{N}\\operatorname{Var}(\\Delta X_{t})=N\\cdot\\frac{1}{2}=\\frac{N}{2}.\n$$\nMoreover, $E[X_{N}]=\\sum_{t=1}^{N}E[\\Delta X_{t}]=0$, so\n$$\nE[X_{N}^{2}]=\\operatorname{Var}(X_{N})+\\left(E[X_{N}]\\right)^{2}=\\frac{N}{2}.\n$$\nBy symmetry, the same holds for $Y_{N}$:\n$$\nE[\\Delta Y_{t}]=0, \\qquad E[\\Delta Y_{t}^{2}]=\\frac{1}{2}, \\qquad E[Y_{N}^{2}]=\\frac{N}{2}.\n$$\nTherefore,\n$$\nE[X_{N}^{2}+Y_{N}^{2}]=\\frac{N}{2}+\\frac{N}{2}=N.\n$$\nThis is the expected squared Euclidean distance from the origin after $N$ steps.",
            "answer": "$$\\boxed{N}$$"
        },
        {
            "introduction": "The Central Limit Theorem suggests that the sum of many random variables often converges to a Gaussian distribution, which underpins the connection between random walks and Brownian motion. This advanced exercise challenges that universality by comparing two scaling limits: one for a walk with finite-variance steps and another for a walk with steps drawn from a heavy-tailed Cauchy distribution, which has infinite variance. By analyzing their characteristic functions, you will find that the latter case leads to anomalous diffusion, or a Lévy flight, a critical concept in modern statistical physics that demonstrates how the nature of microscopic fluctuations can profoundly alter the emergent macroscopic law .",
            "id": "1330635",
            "problem": "Consider two distinct one-dimensional random walk models describing the position of a particle after $n$ discrete time steps, $S_n = \\sum_{i=1}^{n} \\xi_i$, where each step $\\xi_i$ is an independent and identically distributed random variable.\n\nIn a theoretical model of **normal diffusion**, the particle's position at a macroscopic time $t$ is described by the continuum limit of a scaled random walk. Let this process be $W(t) = \\lim_{N \\to \\infty} W_N(t)$, where $N$ is a large scaling factor. The discrete-time approximation is given by\n$$W_N(t) = \\frac{1}{\\sqrt{N}} \\sum_{i=1}^{\\lfloor Nt \\rfloor} X_i$$\nHere, the individual steps $X_i$ are drawn from a probability distribution with zero mean ($\\langle X \\rangle = 0$) and a finite variance $\\langle X^2 \\rangle = \\sigma^2$.\n\nIn contrast, consider a model of **anomalous diffusion**, where rare, large steps dominate the particle's trajectory. This process, $L(t) = \\lim_{N \\to \\infty} L_N(t)$, is modeled with a different scaling:\n$$L_N(t) = \\frac{1}{N} \\sum_{i=1}^{\\lfloor Nt \\rfloor} Y_i$$\nThe steps $Y_i$ are drawn from a symmetric Cauchy distribution, which has a Probability Density Function (PDF) given by $p(y) = \\frac{1}{\\pi} \\frac{\\gamma}{y^2 + \\gamma^2}$, where $\\gamma$ is a positive scale parameter. This distribution notably has an undefined mean and infinite variance.\n\nFor a random process $Z(t)$, its statistical properties can be captured by its characteristic function, $\\phi_Z(k, t) = \\langle \\exp(ikZ(t)) \\rangle$, where $k$ is a real number corresponding to a spatial frequency.\n\nYour task is to compare the statistical nature of these two limiting processes. Specifically, calculate the ratio $R = \\frac{\\ln|\\phi_W(k_0, t_0)|}{\\ln|\\phi_L(k_0, t_0)|}$ for a given positive time $t_0 > 0$ and positive spatial frequency $k_0 > 0$. Provide your answer as a symbolic expression in terms of $\\sigma$, $\\gamma$, and $k_0$.",
            "solution": "We begin with the normal diffusion process. For the scaled walk\n$$\nW_{N}(t)=\\frac{1}{\\sqrt{N}}\\sum_{i=1}^{\\lfloor Nt\\rfloor}X_{i},\n$$\nthe characteristic function, using independence, is\n$$\n\\phi_{W_{N}}(k,t)=\\left\\langle \\exp\\!\\left(ik\\,W_{N}(t)\\right)\\right\\rangle=\\left[\\phi_{X}\\!\\left(\\frac{k}{\\sqrt{N}}\\right)\\right]^{\\lfloor Nt\\rfloor},\n$$\nwhere $\\phi_{X}(u)=\\langle \\exp(iuX)\\rangle$ is the characteristic function of $X$. Since $\\langle X\\rangle=0$ and $\\langle X^{2}\\rangle=\\sigma^{2}<\\infty$, the small-$u$ expansion is\n$$\n\\phi_{X}(u)=1-\\frac{1}{2}\\sigma^{2}u^{2}+o(u^{2}).\n$$\nTherefore,\n$$\n\\ln \\phi_{W_{N}}(k,t)=\\lfloor Nt\\rfloor \\ln\\!\\left(1-\\frac{1}{2}\\sigma^{2}\\frac{k^{2}}{N}+o\\!\\left(\\frac{1}{N}\\right)\\right)\n= -\\frac{1}{2}\\sigma^{2}t\\,k^{2}+o(1),\n$$\nso in the limit $N\\to\\infty$,\n$$\n\\phi_{W}(k,t)=\\lim_{N\\to\\infty}\\phi_{W_{N}}(k,t)=\\exp\\!\\left(-\\frac{1}{2}\\sigma^{2}t\\,k^{2}\\right).\n$$\nHence\n$$\n\\ln|\\phi_{W}(k_{0},t_{0})|=-\\frac{1}{2}\\sigma^{2}t_{0}\\,k_{0}^{2}.\n$$\n\nNext consider the anomalous diffusion process\n$$\nL_{N}(t)=\\frac{1}{N}\\sum_{i=1}^{\\lfloor Nt\\rfloor}Y_{i},\n$$\nwith $Y_{i}$ i.i.d. symmetric Cauchy with scale parameter $\\gamma$, whose characteristic function is\n$$\n\\phi_{Y}(u)=\\exp(-\\gamma|u|).\n$$\nBy independence,\n$$\n\\phi_{L_{N}}(k,t)=\\left[\\phi_{Y}\\!\\left(\\frac{k}{N}\\right)\\right]^{\\lfloor Nt\\rfloor}\n=\\exp\\!\\left(-\\lfloor Nt\\rfloor\\,\\gamma\\,\\frac{|k|}{N}\\right).\n$$\nTaking $N\\to\\infty$ yields\n$$\n\\phi_{L}(k,t)=\\exp\\!\\left(-\\gamma t\\,|k|\\right),\n$$\nso\n$$\n\\ln|\\phi_{L}(k_{0},t_{0})|=-\\gamma t_{0}\\,|k_{0}|.\n$$\n\nTherefore, the required ratio is\n$$\nR=\\frac{\\ln|\\phi_{W}(k_{0},t_{0})|}{\\ln|\\phi_{L}(k_{0},t_{0})|}\n=\\frac{-\\frac{1}{2}\\sigma^{2}t_{0}k_{0}^{2}}{-\\gamma t_{0}|k_{0}|}\n=\\frac{\\sigma^{2}k_{0}^{2}}{2\\gamma |k_{0}|}.\n$$\nFor $k_{0}>0$, we have $|k_{0}|=k_{0}$, so\n$$\nR=\\frac{\\sigma^{2}k_{0}}{2\\gamma}.\n$$\nThis expression depends only on $\\sigma$, $\\gamma$, and $k_{0}$ as requested, with $t_{0}$ canceling out.",
            "answer": "$$\\boxed{\\frac{\\sigma^{2}k_{0}}{2\\gamma}}$$"
        }
    ]
}