{
    "hands_on_practices": [
        {
            "introduction": "The Itô integral is the bedrock of stochastic calculus, yet its definition contains a subtlety that is crucial for its entire structure. This practice explores the foundational requirement that an integrand must be a predictable process, a condition stronger than simple adaptedness. By working through a counterexample and then constructing a valid integral , you will gain a deeper appreciation for the rigorous framework that prevents \"seeing into the future\" and ensures the Itô integral's essential martingale properties.",
            "id": "3809462",
            "problem": "Consider a filtered probability space $(\\Omega,\\mathcal{F},(\\mathcal{F}_t)_{t \\in [0,1]},\\mathbb{P})$ satisfying the usual conditions, and let $(W_t)_{t \\in [0,1]}$ be a standard Brownian motion adapted to $(\\mathcal{F}_t)_{t \\in [0,1]}$. Recall that the Itô integral $\\int_0^1 H_s \\,\\mathrm{d}W_s$ is defined for processes $H_s$ that are predictable and square-integrable. Define the process $H_s = W_1$ for all $s \\in [0,1]$.\n\n(a) Using only the foundational definitions of adaptedness, predictability, and the definition of the Itô integral via isometry or approximation by simple predictable processes, explain why $H_s = W_1$ is not predictable and why the Itô integral $\\int_0^1 H_s \\,\\mathrm{d}W_s$ is therefore undefined under the usual construction.\n\n(b) Modify $H_s$ to a process $\\widetilde{H}_s$ that is predictable and square-integrable on $[0,1]$, and justify its predictability from first principles. Then, compute the Itô integral $\\int_0^1 \\widetilde{H}_s \\,\\mathrm{d}W_s$ in closed form as a function of $W_1$ by deriving it from fundamental results of stochastic calculus without assuming any shortcut formulas in the problem statement.\n\nYour final answer must be the single closed-form analytic expression for the computed Itô integral in part (b), expressed in terms of $W_1$. Do not provide an inequality or an equation as the final answer; provide the expression only. No rounding is required.",
            "solution": "The problem is first validated by dissecting its components and assessing them against established criteria for scientific and mathematical soundness.\n\n### Step 1: Extract Givens\n- A filtered probability space $(\\Omega,\\mathcal{F},(\\mathcal{F}_t)_{t \\in [0,1]},\\mathbb{P})$ satisfying the usual conditions.\n- $(W_t)_{t \\in [0,1]}$ is a standard Brownian motion adapted to the filtration $(\\mathcal{F}_t)_{t \\in [0,1]}$.\n- The Itô integral $\\int_0^1 H_s \\,\\mathrm{d}W_s$ is defined for processes $H_s$ that are predictable and square-integrable.\n- The process $H_s$ is defined as $H_s = W_1$ for all $s \\in [0,1]$.\n- Part (a) requires an explanation of why $H_s$ is not predictable and why its Itô integral is thus undefined.\n- Part (b) requires modifying $H_s$ to a predictable process $\\widetilde{H}_s$, justifying its properties, and computing the Itô integral $\\int_0^1 \\widetilde{H}_s \\,\\mathrm{d}W_s$ from fundamental principles.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically and mathematically sound. It is firmly grounded in the standard theory of stochastic calculus, using well-defined concepts such as Brownian motion, filtrations, adaptedness, predictability, and the Itô integral. The questions posed are standard and serve to test a foundational understanding of the prerequisites for Itô integration. The problem is self-contained, unambiguous, and well-posed, leading to a unique and meaningful solution. It is free of any factual errors, pseudoscience, or subjective claims.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A full solution is warranted.\n\n***\n\n### Solution\nThis solution is divided into two parts, corresponding to the two parts of the problem statement.\n\n**(a) Analysis of the process $H_s = W_1$**\n\nA stochastic process $(X_t)_{t \\in [0,1]}$ is said to be adapted to the filtration $(\\mathcal{F}_t)_{t \\in [0,1]}$ if for every $t \\in [0,1]$, the random variable $X_t$ is $\\mathcal{F}_t$-measurable. This means the value of the process at time $t$ is determined by the information available up to time $t$.\n\nA process $(H_s)_{s \\in [0,1]}$ is predictable if it is measurable with respect to the predictable $\\sigma$-algebra on $[0,1] \\times \\Omega$, which is the $\\sigma$-algebra generated by all left-continuous adapted processes. An important consequence is that predictable processes are a subset of adapted processes. For the Itô integral to be well-defined, the integrand must be predictable. This condition ensures that the integrand $H_s$ does not depend on future increments of the integrator $W_s$, a scenario akin to insider trading which would violate the martingale property of the Itô integral.\n\nLet us analyze the process $H_s = W_1$ for $s \\in [0,1]$. To determine if $H_s$ is predictable, we first check if it is adapted. For $H_s$ to be adapted, the random variable $H_s$ must be $\\mathcal{F}_s$-measurable for every $s \\in [0,1]$.\n\nConsider any time $s \\in [0,1)$. The random variable is $H_s = W_1$. For $W_1$ to be $\\mathcal{F}_s$-measurable, its value must be fully determined by the information in $\\mathcal{F}_s$. However, by the definition of a standard Brownian motion, the increment $W_1 - W_s$ is independent of the past filtration $\\mathcal{F}_s$. We can express $W_1$ as $W_1 = W_s + (W_1 - W_s)$. Since $W_s$ is $\\mathcal{F}_s$-measurable (as $(W_t)$ is an adapted process), if $W_1$ were also $\\mathcal{F}_s$-measurable, then their difference, $W_1 - W_s$, would have to be $\\mathcal{F}_s$-measurable. A random variable that is independent of a $\\sigma$-algebra $\\mathcal{G}$ can only be $\\mathcal{G}$-measurable if it is constant almost surely. The increment $W_1 - W_s$ follows a normal distribution with mean $0$ and variance $1-s > 0$, so it is not a constant. Therefore, $W_1$ is not $\\mathcal{F}_s$-measurable for any $s  1$.\n\nSince the process $H_s = W_1$ is not adapted for $s \\in [0,1)$, it cannot be predictable. As the Itô integral $\\int_0^1 H_s \\,\\mathrm{d}W_s$ is defined only for integrands that are predictable (and square-integrable), the expression $\\int_0^1 W_1 \\,\\mathrm{d}W_s$ is undefined within the standard framework of Itô calculus.\n\n**(b) Modification and Computation of the Integral**\n\nThe most natural way to create a predictable process from $H_s = W_1$ is to replace its value at each time $s$ with the best possible estimate of $W_1$ given the information available at that time. This best estimate is the conditional expectation. We define the modified process $\\widetilde{H}_s$ as:\n$$ \\widetilde{H}_s = \\mathbb{E}[H_s | \\mathcal{F}_s] = \\mathbb{E}[W_1 | \\mathcal{F}_s] $$\nTo compute this, we use the properties of Brownian motion. For any $s \\in [0,1]$:\n$$ \\widetilde{H}_s = \\mathbb{E}[W_s + (W_1 - W_s) | \\mathcal{F}_s] = \\mathbb{E}[W_s | \\mathcal{F}_s] + \\mathbb{E}[W_1 - W_s | \\mathcal{F}_s] $$\nSince $W_s$ is $\\mathcal{F}_s$-measurable, $\\mathbb{E}[W_s | \\mathcal{F}_s] = W_s$. Since the increment $W_1 - W_s$ is independent of $\\mathcal{F}_s$ (for $s1$), its conditional expectation equals its unconditional expectation: $\\mathbb{E}[W_1 - W_s | \\mathcal{F}_s] = \\mathbb{E}[W_1 - W_s] = \\mathbb{E}[W_1] - \\mathbb{E}[W_s] = 0 - 0 = 0$. For $s=1$, $\\mathbb{E}[W_1|\\mathcal{F}_1]=W_1$. Thus, for all $s \\in [0,1]$, the modified process is:\n$$ \\widetilde{H}_s = W_s $$\nThe process $\\widetilde{H}_s = W_s$ is predictable. A standard theorem states that any adapted process with almost surely continuous paths is predictable. The Brownian motion $(W_s)$ is adapted by definition and has continuous paths almost surely, so it is a predictable process.\n\nNext, we verify that $\\widetilde{H}_s$ is square-integrable on $[0,1]$. This requires $\\mathbb{E}\\left[\\int_0^1 \\widetilde{H}_s^2 \\,\\mathrm{d}s\\right]  \\infty$. Using Fubini-Tonelli's theorem:\n$$ \\mathbb{E}\\left[\\int_0^1 W_s^2 \\,\\mathrm{d}s\\right] = \\int_0^1 \\mathbb{E}[W_s^2] \\,\\mathrm{d}s $$\nFor a standard Brownian motion, $W_s$ is normally distributed with mean $0$ and variance $s$. Thus, $\\mathbb{E}[W_s^2] = \\text{Var}(W_s) + (\\mathbb{E}[W_s])^2 = s + 0^2 = s$. The integral becomes:\n$$ \\int_0^1 s \\,\\mathrm{d}s = \\left[\\frac{s^2}{2}\\right]_0^1 = \\frac{1}{2} $$\nSince $\\frac{1}{2}  \\infty$, the process $\\widetilde{H}_s = W_s$ is square-integrable.\n\nWe now compute the Itô integral $I = \\int_0^1 \\widetilde{H}_s \\,\\mathrm{d}W_s = \\int_0^1 W_s \\,\\mathrm{d}W_s$. We derive this from the definition of the Itô integral as the mean-square limit of approximating sums, without direct use of Itô's lemma. Let $\\Pi_n = \\{0=t_0, t_1, \\dots, t_n=1\\}$ be a partition of $[0,1]$ with $t_k = \\frac{k}{n}$ for $k=0, 1, \\dots, n$. The integral is the $L^2(\\Omega)$ limit of the sum:\n$$ S_n = \\sum_{k=0}^{n-1} W_{t_k} (W_{t_{k+1}} - W_{t_k}) $$\nWe utilize the algebraic identity $x(y-x) = \\frac{1}{2}(y^2 - x^2) - \\frac{1}{2}(y-x)^2$. Applying this with $x = W_{t_k}$ and $y = W_{t_{k+1}}$:\n$$ W_{t_k} (W_{t_{k+1}} - W_{t_k}) = \\frac{1}{2}(W_{t_{k+1}}^2 - W_{t_k}^2) - \\frac{1}{2}(W_{t_{k+1}} - W_{t_k})^2 $$\nSumming over $k$ from $0$ to $n-1$:\n$$ S_n = \\sum_{k=0}^{n-1} \\frac{1}{2}(W_{t_{k+1}}^2 - W_{t_k}^2) - \\sum_{k=0}^{n-1} \\frac{1}{2}(W_{t_{k+1}} - W_{t_k})^2 $$\nThe first sum is a telescoping series:\n$$ \\frac{1}{2} \\sum_{k=0}^{n-1} (W_{t_{k+1}}^2 - W_{t_k}^2) = \\frac{1}{2}(W_{t_n}^2 - W_{t_0}^2) = \\frac{1}{2}(W_1^2 - W_0^2) = \\frac{1}{2}W_1^2 $$\nsince $W_0=0$ almost surely. The second sum is half the quadratic variation of $W_t$ over the partition $\\Pi_n$. A fundamental result of stochastic calculus is that the quadratic variation of a standard Brownian motion over an interval $[0,T]$ converges in $L^2$ to $T$ as the mesh of the partition goes to zero. For our case, with $T=1$:\n$$ \\lim_{n \\to \\infty} \\sum_{k=0}^{n-1} (W_{t_{k+1}} - W_{t_k})^2 = 1 \\quad (\\text{in } L^2(\\Omega)) $$\nCombining these results, we take the limit of $S_n$ as $n \\to \\infty$ to find the value of the integral:\n$$ \\int_0^1 W_s \\,\\mathrm{d}W_s = \\lim_{n \\to \\infty} S_n = \\frac{1}{2}W_1^2 - \\frac{1}{2} \\lim_{n \\to \\infty} \\sum_{k=0}^{n-1} (W_{t_{k+1}} - W_{t_k})^2 = \\frac{1}{2}W_1^2 - \\frac{1}{2}(1) $$\nTherefore, the value of the Itô integral is $\\frac{1}{2}W_1^2 - \\frac{1}{2}$.",
            "answer": "$$\n\\boxed{\\frac{1}{2}W_1^2 - \\frac{1}{2}}\n$$"
        },
        {
            "introduction": "Often, a complex stochastic differential equation can be simplified into a more manageable form through a clever change of variables. This exercise introduces you to the Lamperti transformation, a powerful technique that uses Itô's formula to convert an SDE with state-dependent noise into one with a constant, unit diffusion coefficient. Mastering this transformation  is a valuable skill, as it can make SDEs that appear intractable amenable to analytical study or more efficient numerical simulation.",
            "id": "3809540",
            "problem": "Consider the scalar Itô stochastic differential equation (SDE) with multiplicative noise\n$$\n\\mathrm{d}X_{t} = \\mu(X_{t})\\,\\mathrm{d}t + \\sigma(X_{t})\\,\\mathrm{d}W_{t},\n$$\nwhere $W_{t}$ is a standard Wiener process (Brownian motion), $X_{t} \\in \\mathbb{R}$, and the coefficients are given by\n$$\n\\mu(x) = -\\left(\\frac{\\gamma}{\\varepsilon} + \\lambda\\right)x,\\qquad \\sigma(x) = \\sqrt{\\alpha + \\beta x^{2}},\n$$\nwith parameters $\\alpha  0$, $\\beta  0$, $\\gamma  0$, $\\lambda \\ge 0$, and $\\varepsilon \\in (0,1]$. This form models a linear restoring force with a stiff contribution of order $1/\\varepsilon$ coupled to state-dependent noise typical in multiscale modeling contexts.\n\nUsing only fundamental definitions from stochastic calculus (in particular, Itô’s formula), construct a twice continuously differentiable, strictly increasing transformation $Y_{t} = g(X_{t})$ with $g(0)=0$ such that the SDE for $Y_{t}$ has unit diffusion coefficient. Then compute the transformed drift $b(y)$ explicitly as a function of $y$ and the parameters $\\alpha$, $\\beta$, $\\gamma$, $\\lambda$, and $\\varepsilon$. Provide your final answer as a single closed-form analytic expression for $b(y)$ in terms of $y$, $\\beta$, $\\gamma$, $\\lambda$, and $\\varepsilon$ only. Do not include units. No rounding is required.",
            "solution": "The problem statement is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n### Step 1: Extract Givens\nThe problem provides the following information:\n- A scalar Itô stochastic differential equation (SDE):\n$$ \\mathrm{d}X_{t} = \\mu(X_{t})\\,\\mathrm{d}t + \\sigma(X_{t})\\,\\mathrm{d}W_{t} $$\n- The state variable is $X_{t} \\in \\mathbb{R}$.\n- $W_{t}$ is a standard Wiener process.\n- The drift coefficient is $\\mu(x) = -\\left(\\frac{\\gamma}{\\varepsilon} + \\lambda\\right)x$.\n- The diffusion coefficient is $\\sigma(x) = \\sqrt{\\alpha + \\beta x^{2}}$.\n- The parameters are constrained as follows: $\\alpha  0$, $\\beta  0$, $\\gamma  0$, $\\lambda \\ge 0$, and $\\varepsilon \\in (0,1]$.\n- A transformation $Y_{t} = g(X_{t})$ is sought with the following properties:\n    1. $g$ is twice continuously differentiable ($g \\in C^2(\\mathbb{R})$).\n    2. $g$ is strictly increasing, i.e., $g'(x)  0$ for all $x \\in \\mathbb{R}$.\n    3. $g(0) = 0$.\n- The SDE for $Y_{t}$ must have a unit diffusion coefficient.\n- The objective is to compute the transformed drift coefficient, $b(y)$, as a function of $y$, $\\beta$, $\\gamma$, $\\lambda$, and $\\varepsilon$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is set within the established mathematical framework of stochastic calculus and Itô SDEs. The equation is a variant of the Ornstein-Uhlenbeck process with state-dependent diffusion, a standard model in physics and finance. The task of transforming an SDE to one with constant diffusion (the Lamperti transformation) is a fundamental technique in the field.\n- **Well-Posed**: The problem is clearly stated. The conditions on the transformation function $g(x)$ are sufficient to determine it uniquely. The existence of a solution for the transformed drift $b(y)$ is guaranteed under these conditions.\n- **Objective**: The problem is formulated in precise, unambiguous mathematical language.\n- **Completeness and Consistency**: All necessary functions, parameters, and constraints are explicitly provided. The constraint $\\alpha  0$ ensures that $\\sigma(x)$ is always real and strictly positive, which is crucial for the transformation to be well-defined. There are no contradictions in the givens.\n\n### Step 3: Verdict and Action\nThe problem is valid. A rigorous solution can be constructed.\n\n### Solution Derivation\n\nWe seek a transformation $Y_{t} = g(X_{t})$ that transforms the given SDE into one with a unit diffusion coefficient. We apply Itô’s formula to $g(X_{t})$. For a twice continuously differentiable function $g$, the differential $\\mathrm{d}Y_{t}$ is given by:\n$$ \\mathrm{d}Y_{t} = g'(X_{t})\\,\\mathrm{d}X_{t} + \\frac{1}{2}g''(X_{t})\\,(\\mathrm{d}X_{t})^{2} $$\nThe rules of Itô calculus dictate that $(\\mathrm{d}X_{t})^{2} = \\sigma(X_t)^2\\,\\mathrm{d}t$. Substituting the SDE for $\\mathrm{d}X_{t}$:\n$$ \\mathrm{d}Y_{t} = g'(X_{t})\\left(\\mu(X_{t})\\,\\mathrm{d}t + \\sigma(X_{t})\\,\\mathrm{d}W_{t}\\right) + \\frac{1}{2}g''(X_{t})\\sigma(X_{t})^{2}\\,\\mathrm{d}t $$\nGrouping the $\\mathrm{d}t$ and $\\mathrm{d}W_{t}$ terms, we obtain the SDE for $Y_{t}$:\n$$ \\mathrm{d}Y_{t} = \\left[g'(X_{t})\\mu(X_{t}) + \\frac{1}{2}g''(X_{t})\\sigma(X_{t})^{2}\\right]\\,\\mathrm{d}t + g'(X_{t})\\sigma(X_{t})\\,\\mathrm{d}W_{t} $$\nThe new drift coefficient is the term in the square brackets, and the new diffusion coefficient is the prefactor of $\\mathrm{d}W_{t}$. The problem requires that the diffusion coefficient of the SDE for $Y_t$ be unity. This imposes the following condition on $g(x)$:\n$$ g'(x)\\sigma(x) = 1 $$\nThis implies that the derivative of our transformation function must be:\n$$ g'(x) = \\frac{1}{\\sigma(x)} = \\frac{1}{\\sqrt{\\alpha + \\beta x^2}} $$\nTo find $g(x)$, we integrate $g'(x)$. The condition $g(0)=0$ determines the constant of integration.\n$$ g(x) = \\int_{0}^{x} g'(u)\\,\\mathrm{d}u = \\int_{0}^{x} \\frac{1}{\\sqrt{\\alpha + \\beta u^2}}\\,\\mathrm{d}u $$\nWe evaluate this integral:\n$$ g(x) = \\frac{1}{\\sqrt{\\beta}}\\int_{0}^{x} \\frac{1}{\\sqrt{\\frac{\\alpha}{\\beta} + u^2}}\\,\\mathrm{d}u $$\nThe integral is a standard form. Using the identity $\\int \\frac{\\mathrm{d}u}{\\sqrt{a^2+u^2}} = \\arcsinh(\\frac{u}{a})$, we get:\n$$ g(x) = \\frac{1}{\\sqrt{\\beta}}\\left[\\arcsinh\\left(\\frac{u}{\\sqrt{\\alpha/\\beta}}\\right)\\right]_{0}^{x} = \\frac{1}{\\sqrt{\\beta}}\\left(\\arcsinh\\left(x\\sqrt{\\frac{\\beta}{\\alpha}}\\right) - \\arcsinh(0)\\right) $$\n$$ g(x) = \\frac{1}{\\sqrt{\\beta}}\\arcsinh\\left(x\\sqrt{\\frac{\\beta}{\\alpha}}\\right) $$\nThis function $g(x)$ satisfies all the required properties. Since $\\alpha0$ and $\\beta0$, $g'(x) = (\\alpha + \\beta x^2)^{-1/2}  0$, so $g(x)$ is strictly increasing. It is also twice continuously differentiable.\n\nThe transformed drift, let's call it $B(x)$ in the $x$-coordinate, is:\n$$ B(x) = g'(x)\\mu(x) + \\frac{1}{2}g''(x)\\sigma(x)^{2} $$\nWe need the second derivative of $g(x)$:\n$$ g''(x) = \\frac{\\mathrm{d}}{\\mathrm{d}x}\\left((\\alpha + \\beta x^2)^{-1/2}\\right) = -\\frac{1}{2}(\\alpha + \\beta x^2)^{-3/2}(2\\beta x) = -\\frac{\\beta x}{(\\alpha + \\beta x^2)^{3/2}} $$\nSubstituting the expressions for $\\mu(x)$, $\\sigma(x)$, $g'(x)$, and $g''(x)$ into the drift formula:\n$$ B(x) = \\frac{1}{\\sqrt{\\alpha + \\beta x^2}}\\left(-\\left(\\frac{\\gamma}{\\varepsilon} + \\lambda\\right)x\\right) + \\frac{1}{2}\\left(-\\frac{\\beta x}{(\\alpha + \\beta x^2)^{3/2}}\\right)(\\alpha + \\beta x^2) $$\n$$ B(x) = -\\frac{(\\frac{\\gamma}{\\varepsilon} + \\lambda)x}{\\sqrt{\\alpha + \\beta x^2}} - \\frac{\\beta x}{2\\sqrt{\\alpha + \\beta x^2}} $$\n$$ B(x) = -\\left(\\frac{\\gamma}{\\varepsilon} + \\lambda + \\frac{\\beta}{2}\\right)\\frac{x}{\\sqrt{\\alpha + \\beta x^2}} $$\nThe problem asks for the drift $b(y)$ as a function of $y = g(x)$. To achieve this, we must express the term $\\frac{x}{\\sqrt{\\alpha + \\beta x^2}}$ as a function of $y$. First, we find the inverse transformation $x = g^{-1}(y)$:\n$$ y = \\frac{1}{\\sqrt{\\beta}}\\arcsinh\\left(x\\sqrt{\\frac{\\beta}{\\alpha}}\\right) \\implies \\sqrt{\\beta}y = \\arcsinh\\left(x\\sqrt{\\frac{\\beta}{\\alpha}}\\right) $$\n$$ \\sinh(\\sqrt{\\beta}y) = x\\sqrt{\\frac{\\beta}{\\alpha}} \\implies x = \\sqrt{\\frac{\\alpha}{\\beta}}\\sinh(\\sqrt{\\beta}y) $$\nNow, we substitute this expression for $x$ into the term we need to transform:\n$$ \\frac{x}{\\sqrt{\\alpha + \\beta x^2}} = \\frac{\\sqrt{\\frac{\\alpha}{\\beta}}\\sinh(\\sqrt{\\beta}y)}{\\sqrt{\\alpha + \\beta \\left(\\sqrt{\\frac{\\alpha}{\\beta}}\\sinh(\\sqrt{\\beta}y)\\right)^2}} = \\frac{\\sqrt{\\frac{\\alpha}{\\beta}}\\sinh(\\sqrt{\\beta}y)}{\\sqrt{\\alpha + \\alpha\\sinh^2(\\sqrt{\\beta}y)}} $$\nUsing the identity $\\cosh^2(z) = 1 + \\sinh^2(z)$:\n$$ \\frac{x}{\\sqrt{\\alpha + \\beta x^2}} = \\frac{\\sqrt{\\frac{\\alpha}{\\beta}}\\sinh(\\sqrt{\\beta}y)}{\\sqrt{\\alpha\\cosh^2(\\sqrt{\\beta}y)}} = \\frac{\\sqrt{\\frac{\\alpha}{\\beta}}\\sinh(\\sqrt{\\beta}y)}{\\sqrt{\\alpha}\\cosh(\\sqrt{\\beta}y)} $$\nNote that $\\cosh(z) > 0$ for all real $z$.\n$$ \\frac{x}{\\sqrt{\\alpha + \\beta x^2}} = \\frac{1}{\\sqrt{\\beta}}\\frac{\\sinh(\\sqrt{\\beta}y)}{\\cosh(\\sqrt{\\beta}y)} = \\frac{1}{\\sqrt{\\beta}}\\tanh(\\sqrt{\\beta}y) $$\nNoticeably, the parameter $\\alpha$ has cancelled out. Finally, we substitute this result back into the expression for the drift $B(x)$ to get $b(y)$:\n$$ b(y) = -\\left(\\frac{\\gamma}{\\varepsilon} + \\lambda + \\frac{\\beta}{2}\\right) \\left(\\frac{1}{\\sqrt{\\beta}}\\tanh(\\sqrt{\\beta}y)\\right) $$\nThis expression can be slightly rearranged for the final answer.\n$$ b(y) = -\\frac{1}{\\sqrt{\\beta}}\\left(\\frac{\\gamma}{\\varepsilon} + \\lambda + \\frac{\\beta}{2}\\right)\\tanh(\\sqrt{\\beta}y) $$\nThis is the required closed-form analytic expression for the transformed drift $b(y)$.",
            "answer": "$$ \\boxed{-\\frac{1}{\\sqrt{\\beta}}\\left(\\frac{\\gamma}{\\varepsilon} + \\lambda + \\frac{\\beta}{2}\\right)\\tanh(\\sqrt{\\beta}y)} $$"
        },
        {
            "introduction": "While we can write down continuous-time SDEs, in practice we almost always rely on numerical methods to approximate their solutions. This exercise bridges the gap between continuous theory and discrete computation by analyzing the stability of the Euler-Maruyama method, a fundamental SDE solver. By deriving the step-size constraint for the mean-square stability of a simulated Ornstein-Uhlenbeck process , you will learn a critical lesson: the stability of a continuous system does not automatically guarantee the stability of its numerical approximation.",
            "id": "3809429",
            "problem": "Consider the Ornstein–Uhlenbeck (OU) stochastic differential equation (SDE)\n$$\n\\mathrm{d}X_{t} \\;=\\; -\\theta\\,X_{t}\\,\\mathrm{d}t \\;+\\; \\sigma\\,\\mathrm{d}W_{t},\n$$\nwhere $\\theta0$ and $\\sigma0$ are constants and $W_{t}$ is a standard Wiener process (also called Brownian motion). A time discretization with uniform step size $h0$ and time grid $t_{n}=nh$ is constructed via the Euler–Maruyama (EM) method, which is the explicit one-step method for SDEs defined by\n$$\nX_{n+1} \\;=\\; X_{n} \\;+\\; f(X_{n})\\,h \\;+\\; g(X_{n})\\,\\Delta W_{n},\n$$\nwith $\\Delta W_{n} := W_{t_{n+1}} - W_{t_{n}}$ and drift $f$ and diffusion $g$ inherited from the SDE.\n\nStarting from the fundamental definitions of the OU SDE and the Euler–Maruyama method, and using only the basic properties of the Wiener process increments (namely, $\\Delta W_{n}$ are independent, mean-zero Gaussian random variables with $\\mathbb{E}[\\Delta W_{n}^{2}]=h$), derive the closed-form recursion for the second moment $\\mathbb{E}[X_{n}^{2}]$ of the EM approximation. Then, using the definition of mean-square stability for a stochastic numerical method applied to a linear SDE with additive noise—that is, that the second moment remains uniformly bounded in $n$ and admits a finite stationary limit—determine the largest admissible step size $h_{\\max}$ (expressed solely in terms of $\\theta$) such that the Euler–Maruyama method applied to the OU SDE is mean-square stable.\n\nProvide your final answer as a single symbolic expression for $h_{\\max}$. No numerical approximation or rounding is required.",
            "solution": "The problem is validated as self-contained, scientifically grounded in the theory of stochastic differential equations and their numerical approximation, and is well-posed. We proceed with the solution.\n\nThe Ornstein–Uhlenbeck (OU) stochastic differential equation (SDE) is given by\n$$\n\\mathrm{d}X_{t} \\;=\\; -\\theta\\,X_{t}\\,\\mathrm{d}t \\;+\\; \\sigma\\,\\mathrm{d}W_{t}\n$$\nwhere $\\theta > 0$ and $\\sigma > 0$ are constants. For this SDE, the drift function is $f(x) = -\\theta x$ and the diffusion function is $g(x) = \\sigma$.\n\nThe Euler–Maruyama (EM) method provides a discrete-time approximation $X_n \\approx X_{t_n}$ at times $t_n = nh$ for a step size $h > 0$. The general form of the EM method is\n$$\nX_{n+1} \\;=\\; X_{n} \\;+\\; f(X_{n})\\,h \\;+\\; g(X_{n})\\,\\Delta W_{n}\n$$\nwhere $\\Delta W_{n} := W_{t_{n+1}} - W_{t_{n}}$. Substituting the specific drift and diffusion for the OU process, we obtain the scheme:\n$$\nX_{n+1} \\;=\\; X_{n} \\;+\\; (-\\theta X_{n})\\,h \\;+\\; \\sigma\\,\\Delta W_{n}\n$$\nThis can be rearranged as:\n$$\nX_{n+1} \\;=\\; (1 - \\theta h) X_{n} \\;+\\; \\sigma\\,\\Delta W_{n}\n$$\n\nOur first objective is to derive the recursion for the second moment, $\\mathbb{E}[X_{n}^{2}]$. We start by squaring the expression for $X_{n+1}$:\n$$\nX_{n+1}^{2} \\;=\\; \\left( (1 - \\theta h) X_{n} \\;+\\; \\sigma\\,\\Delta W_{n} \\right)^{2}\n$$\nExpanding the square, we get:\n$$\nX_{n+1}^{2} \\;=\\; (1 - \\theta h)^{2} X_{n}^{2} \\;+\\; 2(1 - \\theta h)\\sigma X_{n} \\Delta W_{n} \\;+\\; \\sigma^{2} (\\Delta W_{n})^{2}\n$$\nNext, we take the expectation of both sides of this equation. By the linearity of the expectation operator $\\mathbb{E}[\\cdot]$, we have:\n$$\n\\mathbb{E}[X_{n+1}^{2}] \\;=\\; \\mathbb{E}\\left[ (1 - \\theta h)^{2} X_{n}^{2} \\right] \\;+\\; \\mathbb{E}\\left[ 2(1 - \\theta h)\\sigma X_{n} \\Delta W_{n} \\right] \\;+\\; \\mathbb{E}\\left[ \\sigma^{2} (\\Delta W_{n})^{2} \\right]\n$$\nThe constant terms can be factored out of the expectations:\n$$\n\\mathbb{E}[X_{n+1}^{2}] \\;=\\; (1 - \\theta h)^{2} \\mathbb{E}[X_{n}^{2}] \\;+\\; 2(1 - \\theta h)\\sigma\\, \\mathbb{E}[X_{n} \\Delta W_{n}] \\;+\\; \\sigma^{2} \\mathbb{E}[(\\Delta W_{n})^{2}]\n$$\nWe now evaluate the individual expectation terms.\nThe term $\\mathbb{E}[(\\Delta W_{n})^{2}]$ is given in the problem statement as the variance of the Wiener increment over a time step $h$, so $\\mathbb{E}[(\\Delta W_{n})^{2}] = h$.\n\nFor the term $\\mathbb{E}[X_{n} \\Delta W_{n}]$, we must consider the properties of the Wiener process. The value of the numerical solution $X_{n}$ at step $n$ is determined by the history of the Wiener process up to time $t_{n}$. Specifically, $X_{n}$ is a function of $X_{0}$ and the increments $\\Delta W_{0}, \\Delta W_{1}, \\dots, \\Delta W_{n-1}$. The increment $\\Delta W_{n} = W_{t_{n+1}} - W_{t_{n}}$ is independent of the past of the Wiener process up to time $t_{n}$. Therefore, $X_{n}$ and $\\Delta W_{n}$ are independent random variables.\nFor independent random variables, the expectation of their product is the product of their expectations:\n$$\n\\mathbb{E}[X_{n} \\Delta W_{n}] \\;=\\; \\mathbb{E}[X_{n}]\\,\\mathbb{E}[\\Delta W_{n}]\n$$\nThe problem states that $\\Delta W_{n}$ are mean-zero Gaussian random variables, so $\\mathbb{E}[\\Delta W_{n}] = 0$. This implies:\n$$\n\\mathbb{E}[X_{n} \\Delta W_{n}] \\;=\\; \\mathbb{E}[X_{n}] \\cdot 0 \\;=\\; 0\n$$\nSubstituting these results back into the equation for $\\mathbb{E}[X_{n+1}^{2}]$, we get:\n$$\n\\mathbb{E}[X_{n+1}^{2}] \\;=\\; (1 - \\theta h)^{2} \\mathbb{E}[X_{n}^{2}] \\;+\\; 2(1 - \\theta h)\\sigma \\cdot 0 \\;+\\; \\sigma^{2} h\n$$\nThis simplifies to the closed-form recursion for the second moment:\n$$\n\\mathbb{E}[X_{n+1}^{2}] \\;=\\; (1 - \\theta h)^{2} \\mathbb{E}[X_{n}^{2}] \\;+\\; \\sigma^{2} h\n$$\n\nOur second objective is to determine the condition for mean-square stability. Let $M_{n} = \\mathbb{E}[X_{n}^{2}]$. The recursion is a linear first-order recurrence relation of the form $M_{n+1} = a M_{n} + b$, with coefficients $a = (1 - \\theta h)^{2}$ and $b = \\sigma^{2} h$.\n\nThe problem defines mean-square stability as the condition that the second moment $M_n$ remains uniformly bounded and admits a finite stationary limit, $M_{\\infty} = \\lim_{n\\to\\infty} M_n$. For a linear recurrence of this form, starting from a finite initial value $M_0$, the sequence $M_n$ converges to a finite limit if and only if the absolute value of the coefficient $a$ is strictly less than $1$. That is, $|a|  1$.\n\nApplying this condition to our coefficients:\n$$\n| (1 - \\theta h)^{2} | \\;\\; 1\n$$\nSince $(1 - \\theta h)^{2}$ is always non-negative, this inequality is equivalent to:\n$$\n(1 - \\theta h)^{2} \\;\\; 1\n$$\nTaking the square root of both sides yields:\n$$\n| 1 - \\theta h | \\;\\; 1\n$$\nThis absolute value inequality can be written as a compound inequality:\n$$\n-1 \\;\\; 1 - \\theta h \\;\\; 1\n$$\nWe solve this compound inequality in two parts.\nFirst, the left-hand side inequality:\n$$\n-1 \\;\\; 1 - \\theta h\n$$\n$$\n\\theta h \\;\\; 1 - (-1)\n$$\n$$\n\\theta h \\;\\; 2\n$$\nSince $\\theta0$ is given, we can divide by it without changing the inequality direction:\n$$\nh \\;\\; \\frac{2}{\\theta}\n$$\nSecond, the right-hand side inequality:\n$$\n1 - \\theta h \\;\\; 1\n$$\n$$\n- \\theta h \\;\\; 0\n$$\nSince $\\theta  0$, multiplying by $-1/\\theta$ reverses the inequality sign:\n$$\nh \\;\\; 0\n$$\nThe condition $h0$ is already part of the problem definition for a time step.\nCombining the two results, the condition for mean-square stability of the Euler–Maruyama method applied to the Ornstein–Uhlenbeck SDE is:\n$$\n0 \\;\\; h \\;\\; \\frac{2}{\\theta}\n$$\nThe problem asks for the largest admissible step size, $h_{\\max}$, for which the method is mean-square stable. This corresponds to the supremum of the interval of stability. If $h = 2/\\theta$, the coefficient $a=(1-\\theta(2/\\theta))^2 = (1-2)^2 = 1$, and the recurrence becomes $M_{n+1}=M_n+\\sigma^2 h$, which grows linearly and does not converge to a finite limit. Therefore, the inequality must be strict. The largest admissible step size is the upper bound of this interval.\n$$\nh_{\\max} \\;=\\; \\frac{2}{\\theta}\n$$",
            "answer": "$$\n\\boxed{\\frac{2}{\\theta}}\n$$"
        }
    ]
}