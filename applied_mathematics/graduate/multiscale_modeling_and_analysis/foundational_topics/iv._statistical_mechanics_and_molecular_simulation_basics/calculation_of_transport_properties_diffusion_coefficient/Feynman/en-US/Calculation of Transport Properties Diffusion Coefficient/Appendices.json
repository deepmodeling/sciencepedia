{
    "hands_on_practices": [
        {
            "introduction": "The diffusion coefficient can be understood from both a macroscopic and microscopic perspective. This exercise bridges these two viewpoints by deriving the celebrated Green-Kubo relation. You will start from the macroscopic definition of diffusion based on the mean-squared displacement and, through a series of steps, connect it to the time integral of the velocity autocorrelation function (VACF)—a measure of microscopic memory in the system. This derivation is a cornerstone of statistical mechanics, providing a powerful link between transport coefficients and the time-correlation functions of microscopic fluctuations. ",
            "id": "3740786",
            "problem": "Consider a tracer particle moving in an isotropic medium in $d$ spatial dimensions at thermal equilibrium. Let $\\mathbf{r}(t)$ be the position and $\\mathbf{v}(t)=\\dot{\\mathbf{r}}(t)$ the velocity of the tracer. The self-diffusion coefficient $D$ is defined through the long-time growth of the mean-squared displacement,\n$$\n\\lim_{t\\to\\infty}\\frac{\\langle|\\mathbf{r}(t)-\\mathbf{r}(0)|^{2}\\rangle}{2 d\\, t}=D,\n$$\nwhere $\\langle\\cdot\\rangle$ denotes an equilibrium ensemble average. Starting from this definition and the identity $\\mathbf{v}(t)=\\dot{\\mathbf{r}}(t)$, derive an exact representation of $D$ in terms of the time integral of the equilibrium velocity autocorrelation function (VACF), $\\langle \\mathbf{v}(0)\\cdot \\mathbf{v}(t)\\rangle$. Then, for a medium in which the VACF decays exponentially,\n$$\n\\langle \\mathbf{v}(0)\\cdot \\mathbf{v}(t)\\rangle = v_0^{2}\\,\\exp\\!\\left(-\\frac{t}{\\tau}\\right),\n$$\nevaluate $D$.\n\nNext, relate the parameters $v_0^{2}$ and $\\tau$ to microscopic quantities under the following physically motivated assumptions:\n- By the equipartition theorem for Maxwell–Boltzmann statistics, identify $v_0^{2}$ in terms of the tracer mass $m$ and temperature $T$.\n- Under a linear friction model (a special case of a generalized Langevin description) with instantaneous drag $m\\,\\dot{\\mathbf{v}}(t)=-\\zeta\\,\\mathbf{v}(t)+\\boldsymbol{\\xi}(t)$ and a random force $\\boldsymbol{\\xi}(t)$ that maintains equilibrium, identify the momentum relaxation time $\\tau$ in terms of $m$ and the microscopic friction coefficient $\\zeta$.\n\nExpress your final answer for $D$ as a single closed-form analytic expression in terms of $k_{B}$, $T$, and $\\zeta$ (no numerical evaluation required, no units in the final expression).",
            "solution": "The problem statement is scientifically grounded, well-posed, and contains all necessary information to derive a unique and meaningful solution. We may therefore proceed with the derivation.\n\nThe problem requires us to find the self-diffusion coefficient $D$ for a tracer particle in a $d$-dimensional isotropic medium. We begin with the definition provided:\n$$\nD = \\lim_{t\\to\\infty}\\frac{\\langle|\\mathbf{r}(t)-\\mathbf{r}(0)|^{2}\\rangle}{2 d\\, t}\n$$\nwhere $\\mathbf{r}(t)$ is the particle's position at time $t$ and $\\langle\\cdot\\rangle$ denotes an equilibrium ensemble average.\n\n**Step 1: Relate the diffusion coefficient to the velocity autocorrelation function (VACF).**\n\nThe displacement of the particle, $\\mathbf{r}(t)-\\mathbf{r}(0)$, can be expressed as the time integral of its velocity, $\\mathbf{v}(t) = \\dot{\\mathbf{r}}(t)$:\n$$\n\\mathbf{r}(t) - \\mathbf{r}(0) = \\int_{0}^{t} \\mathbf{v}(t') dt'\n$$\nThe squared displacement is the dot product of this integral with itself:\n$$\n|\\mathbf{r}(t)-\\mathbf{r}(0)|^{2} = \\left(\\int_{0}^{t} \\mathbf{v}(t') dt'\\right) \\cdot \\left(\\int_{0}^{t} \\mathbf{v}(t'') dt''\\right) = \\int_{0}^{t} dt' \\int_{0}^{t} dt'' \\, \\mathbf{v}(t') \\cdot \\mathbf{v}(t'')\n$$\nTaking the ensemble average, we obtain the mean-squared displacement (MSD):\n$$\n\\langle |\\mathbf{r}(t)-\\mathbf{r}(0)|^{2} \\rangle = \\int_{0}^{t} dt' \\int_{0}^{t} dt'' \\, \\langle \\mathbf{v}(t') \\cdot \\mathbf{v}(t'') \\rangle\n$$\nFor a system in thermal equilibrium, the time-translation invariance of the ensemble implies that correlation functions depend only on the time difference. Thus, the VACF is stationary:\n$$\n\\langle \\mathbf{v}(t') \\cdot \\mathbf{v}(t'') \\rangle = \\langle \\mathbf{v}(0) \\cdot \\mathbf{v}(t''-t') \\rangle\n$$\nLet's define $C_v(\\tau) = \\langle \\mathbf{v}(0) \\cdot \\mathbf{v}(\\tau) \\rangle$. To evaluate the MSD, we differentiate it with respect to time:\n$$\n\\frac{d}{dt} \\langle |\\mathbf{r}(t)-\\mathbf{r}(0)|^{2} \\rangle = \\frac{d}{dt} \\langle (\\mathbf{r}(t)-\\mathbf{r}(0)) \\cdot (\\mathbf{r}(t)-\\mathbf{r}(0)) \\rangle = 2 \\langle (\\mathbf{r}(t)-\\mathbf{r}(0)) \\cdot \\dot{\\mathbf{r}}(t) \\rangle\n$$\n$$\n= 2 \\left\\langle \\left(\\int_{0}^{t} \\mathbf{v}(t') dt'\\right) \\cdot \\mathbf{v}(t) \\right\\rangle = 2 \\int_{0}^{t} \\langle \\mathbf{v}(t') \\cdot \\mathbf{v}(t) \\rangle dt'\n$$\nUsing stationarity, $\\langle \\mathbf{v}(t') \\cdot \\mathbf{v}(t) \\rangle = C_v(t-t')$. With a change of variables $s=t-t'$, this becomes:\n$$\n\\frac{d}{dt} \\langle |\\mathbf{r}(t)-\\mathbf{r}(0)|^{2} \\rangle = 2 \\int_{0}^{t} C_v(s) ds\n$$\nFrom the definition of $D$, for large $t$, the MSD is linear in time: $\\langle |\\mathbf{r}(t)-\\mathbf{r}(0)|^{2} \\rangle \\approx 2dtD$. The time derivative in this limit approaches a constant:\n$$\n\\lim_{t\\to\\infty} \\frac{d}{dt} \\langle |\\mathbf{r}(t)-\\mathbf{r}(0)|^{2} \\rangle = 2dD\n$$\nEquating this with the long-time limit of our integral expression:\n$$\n2dD = \\lim_{t\\to\\infty} 2 \\int_{0}^{t} C_v(s) ds = 2 \\int_{0}^{\\infty} C_v(s) ds\n$$\nThis assumes the VACF, $C_v(s)$, decays to zero sufficiently quickly for the integral to converge, which is physically required for a diffusive system. This yields the Green-Kubo relation for the diffusion coefficient:\n$$\nD = \\frac{1}{d} \\int_{0}^{\\infty} \\langle \\mathbf{v}(0) \\cdot \\mathbf{v}(t) \\rangle dt\n$$\n\n**Step 2: Evaluate $D$ for the given exponential VACF.**\n\nThe problem specifies an exponential decay for the VACF:\n$$\n\\langle \\mathbf{v}(0)\\cdot \\mathbf{v}(t)\\rangle = v_0^{2}\\,\\exp\\left(-\\frac{t}{\\tau}\\right)\n$$\nSubstituting this into our expression for $D$:\n$$\nD = \\frac{1}{d} \\int_{0}^{\\infty} v_0^{2}\\,\\exp\\left(-\\frac{t}{\\tau}\\right) dt = \\frac{v_0^{2}}{d} \\int_{0}^{\\infty} \\exp\\left(-\\frac{t}{\\tau}\\right) dt\n$$\nThe integral evaluates to:\n$$\n\\int_{0}^{\\infty} \\exp\\left(-\\frac{t}{\\tau}\\right) dt = \\left[ -\\tau \\exp\\left(-\\frac{t}{\\tau}\\right) \\right]_{0}^{\\infty} = -\\tau(0-1) = \\tau\n$$\nThus, the diffusion coefficient is:\n$$\nD = \\frac{v_0^{2} \\tau}{d}\n$$\n\n**Step 3: Relate $v_0^2$ and $\\tau$ to microscopic quantities.**\n\nFirst, we identify $v_0^2$. By setting $t=0$ in the VACF expression, we see that $v_0^2 = \\langle \\mathbf{v}(0)\\cdot \\mathbf{v}(0)\\rangle = \\langle |\\mathbf{v}|^2 \\rangle$. The equipartition theorem states that for a system in thermal equilibrium at temperature $T$, the average energy associated with each quadratic degree of freedom is $\\frac{1}{2} k_B T$, where $k_B$ is the Boltzmann constant. The kinetic energy of the particle of mass $m$ is $\\frac{1}{2} m |\\mathbf{v}|^2 = \\frac{1}{2}m\\sum_{i=1}^d v_i^2$. This corresponds to $d$ quadratic degrees of freedom.\n$$\n\\left\\langle \\frac{1}{2} m |\\mathbf{v}|^2 \\right\\rangle = d \\cdot \\frac{1}{2} k_B T\n$$\n$$\n\\frac{1}{2} m \\langle |\\mathbf{v}|^2 \\rangle = \\frac{d}{2} k_B T \\implies \\langle |\\mathbf{v}|^2 \\rangle = \\frac{d k_B T}{m}\n$$\nTherefore, we have $v_0^2 = \\frac{d k_B T}{m}$.\n\nNext, we identify the momentum relaxation time $\\tau$ using the provided linear friction model (Langevin equation):\n$$\nm\\,\\dot{\\mathbf{v}}(t)=-\\zeta\\,\\mathbf{v}(t)+\\boldsymbol{\\xi}(t)\n$$\nwhere $\\zeta$ is the friction coefficient and $\\boldsymbol{\\xi}(t)$ is a random force. To find the characteristic decay time of velocity correlations, we multiply the equation by $\\mathbf{v}(0)$ and take the ensemble average for $t0$:\n$$\nm \\frac{d}{dt}\\langle \\mathbf{v}(0) \\cdot \\mathbf{v}(t) \\rangle = -\\zeta \\langle \\mathbf{v}(0) \\cdot \\mathbf{v}(t) \\rangle + \\langle \\mathbf{v}(0) \\cdot \\boldsymbol{\\xi}(t) \\rangle\n$$\nThe random force $\\boldsymbol{\\xi}(t)$ is uncorrelated with the velocity at any prior time, so $\\langle \\mathbf{v}(0) \\cdot \\boldsymbol{\\xi}(t) \\rangle = 0$ for $t  0$. This gives a differential equation for the VACF, $C_v(t) = \\langle \\mathbf{v}(0) \\cdot \\mathbf{v}(t) \\rangle$:\n$$\nm \\frac{d C_v(t)}{dt} = -\\zeta C_v(t)\n$$\nThe solution is an exponential decay:\n$$\nC_v(t) = C_v(0) \\exp\\left(-\\frac{\\zeta}{m} t\\right)\n$$\nComparing this with the given form $\\langle \\mathbf{v}(0)\\cdot \\mathbf{v}(t)\\rangle = v_0^{2}\\,\\exp\\left(-\\frac{t}{\\tau}\\right)$, we can directly identify the relaxation time $\\tau$ as:\n$$\n\\tau = \\frac{m}{\\zeta}\n$$\n\n**Step 4: Combine results to find the final expression for $D$.**\n\nWe substitute the expressions for $v_0^2$ and $\\tau$ into our equation for $D$:\n$$\nD = \\frac{v_0^{2} \\tau}{d} = \\frac{1}{d} \\left( \\frac{d k_B T}{m} \\right) \\left( \\frac{m}{\\zeta} \\right)\n$$\nThe factors of dimensionality $d$ and mass $m$ cancel, yielding the final expression for the diffusion coefficient, which is the Einstein-Smoluchowski relation:\n$$\nD = \\frac{k_B T}{\\zeta}\n$$\nThis result remarkably shows that the diffusion coefficient is independent of the particle's mass and the dimensionality of the space, depending only on the thermal energy scale $k_B T$ and the microscopic friction coefficient $\\zeta$.",
            "answer": "$$\n\\boxed{\\frac{k_B T}{\\zeta}}\n$$"
        },
        {
            "introduction": "Extracting physical observables from raw simulation data is a core skill in computational science. This practice guides you through the process of calculating the diffusion coefficient from a particle's trajectory, a common task in analyzing molecular dynamics or single-particle tracking experiments. You will implement an algorithm to compute the mean-squared displacement (MSD) and, critically, develop a robust procedure to identify the linear, diffusive regime from which the diffusion coefficient is extracted. This exercise highlights how to handle common real-world complexities, such as ballistic motion at short times and confinement effects at long times, to ensure an accurate estimation. ",
            "id": "3740784",
            "problem": "You are given discrete-time trajectories of a single particle in $d$ spatial dimensions from simulated dynamics. The task is to compute the time-averaged mean-square displacement and, from its linear-in-time regime, estimate the diffusion coefficient in the specified unit. The estimation must be robust to non-diffusive transients (such as ballistic motion at short times), measurement noise, and confinement that induces long-time deviations from linearity.\n\nFundamental base and definitions:\n- For a discrete trajectory $\\{\\mathbf{r}_n\\}_{n=0}^{N-1}$ sampled at uniform time step $\\Delta t$, the time-averaged mean-square displacement at lag $\\tau_k = k \\Delta t$ is defined by\n$$\n\\mathrm{MSD}(k) = \\frac{1}{N-k} \\sum_{n=0}^{N-k-1} \\left\\|\\mathbf{r}_{n+k} - \\mathbf{r}_n \\right\\|^2.\n$$\n- The time window for estimating the diffusion coefficient must correspond to a regime in which $\\mathrm{MSD}(\\tau)$ scales linearly with $\\tau$; that is, on a log-log plot of $\\mathrm{MSD}$ versus $\\tau$, the local slope should be close to $1$.\n- The diffusion coefficient $D$ must be estimated by linearly fitting $\\mathrm{MSD}(\\tau)$ versus $\\tau$ in the selected window, using an appropriate relation between the slope and $D$ in $d$ dimensions (do not assume or use any shortcut formulas given here; derive and implement the correct relation as part of the solution).\n\nAlgorithmic requirements:\n1. Compute the unbiased time-averaged mean-square displacement $\\mathrm{MSD}(k)$ for integer lags $k = 1,2,\\ldots,K_{\\max}$, where $K_{\\max}$ is chosen so that the averaging denominator $N-k$ is sufficiently large to limit statistical error. For this, you must implement a method whose computational complexity scales no worse than $O(N \\log N)$ in the trajectory length $N$, such as a method based on Fast Fourier Transform (FFT)-accelerated autocorrelation to compute the time averages efficiently.\n2. Design a principled procedure to select an appropriate time window $[\\tau_{k_{\\min}}, \\tau_{k_{\\max}}]$ in which the $\\mathrm{MSD}(\\tau)$ dependence is linear in time. Your procedure must:\n   - Use a local scaling analysis of $\\log(\\mathrm{MSD}(\\tau))$ versus $\\log(\\tau)$ to identify contiguous lag indices where the local slope is near $1$.\n   - Enforce a minimum number of averaging pairs $N-k$ in the window to ensure statistical reliability.\n   - If no contiguous window meets the criteria, fall back to a search over candidate windows with sufficient length, choosing the window whose average local slope is closest to $1$ and whose linear fit has the largest goodness-of-fit (for example, weighted coefficient of determination).\n3. In the selected window, perform a weighted linear regression of $\\mathrm{MSD}(\\tau)$ versus $\\tau$ where weights reflect the number of averaging pairs available at each lag. From the fitted slope and the spatial dimension $d$, compute the diffusion coefficient $D$ using the correct relation derived from first principles as part of your solution.\n4. Express each diffusion coefficient in m^2/s, rounded to six significant figures. Use scientific notation in the final output.\n\nTest suite:\nFor reproducibility, the following four test cases define how to generate the trajectories. In each case, the program must internally generate the trajectory according to the specified parameters and random seed, then compute and report the estimated diffusion coefficient.\n\n- Case $1$ (general happy path, free diffusion in $3$ dimensions):\n  - Dimension $d = 3$.\n  - Time step $\\Delta t = 10^{-3}$ s.\n  - Length $N = 5000$ samples.\n  - True diffusion coefficient $D_{\\mathrm{true}} = 10^{-9}$ m$^2$/s.\n  - Generation rule: start at $\\mathbf{r}_0 = \\mathbf{0}$ and evolve as a discrete random walk with independent Gaussian increments per coordinate having variance $2 D_{\\mathrm{true}} \\Delta t$.\n  - Random seed $s = 12345$.\n\n- Case $2$ (ballistic short-time drift plus diffusion in $2$ dimensions):\n  - Dimension $d = 2$.\n  - Time step $\\Delta t = 5 \\times 10^{-3}$ s.\n  - Length $N = 6000$ samples.\n  - True diffusion coefficient $D_{\\mathrm{true}} = 5 \\times 10^{-10}$ m$^2$/s.\n  - Constant drift velocity $\\mathbf{v} = (10^{-6}, -10^{-6})$ m/s, added to the increments at all times.\n  - Generation rule: start at $\\mathbf{r}_0 = \\mathbf{0}$ and evolve with increments $\\mathbf{v} \\Delta t + \\boldsymbol{\\eta}$, where $\\boldsymbol{\\eta}$ is Gaussian per coordinate with variance $2 D_{\\mathrm{true}} \\Delta t$.\n  - Random seed $s = 54321$.\n\n- Case $3$ (overdamped harmonic confinement in $1$ dimension, inducing plateau at long times):\n  - Dimension $d = 1$.\n  - Time step $\\Delta t = 10^{-3}$ s.\n  - Length $N = 7000$ samples.\n  - True diffusion coefficient $D_{\\mathrm{true}} = 2 \\times 10^{-12}$ m$^2$/s.\n  - Confinement time constant $\\tau_c = 0.2$ s.\n  - Generation rule: overdamped Ornstein–Uhlenbeck positional dynamics $r_{n+1} = \\alpha r_n + \\sigma \\xi_n$ with $\\alpha = \\exp(-\\Delta t / \\tau_c)$, $\\xi_n$ independent standard normal, and $\\sigma = \\sqrt{D_{\\mathrm{true}} \\tau_c \\left(1 - \\exp(-2 \\Delta t / \\tau_c)\\right)}$, starting from $r_0 = 0$.\n  - Random seed $s = 777$.\n\n- Case $4$ (free diffusion in $2$ dimensions with additive measurement noise on positions):\n  - Dimension $d = 2$.\n  - Time step $\\Delta t = 2 \\times 10^{-3}$ s.\n  - Length $N = 3000$ samples.\n  - True diffusion coefficient $D_{\\mathrm{true}} = 3 \\times 10^{-10}$ m$^2$/s.\n  - Measurement noise standard deviation $\\sigma_{\\mathrm{meas}} = 5 \\times 10^{-7}$ m, added independently to each coordinate of each observed position sample.\n  - Generation rule: generate free diffusion trajectory as in Case $1$ (with the above $d$, $\\Delta t$, $N$, and $D_{\\mathrm{true}}$) using random seed $s = 888$, then add independent Gaussian measurement noise with variance $\\sigma_{\\mathrm{meas}}^2$ to each position sample.\n\nFinal output specification:\n- Your program should produce a single line of output containing the estimated diffusion coefficients for the four cases, in m^2/s, rounded to six significant figures, in scientific notation, as a comma-separated list enclosed in square brackets (for example, \"[1.00000e-09,1.23457e-10,9.87654e-12,2.46813e-10]\").",
            "solution": "The problem of estimating a diffusion coefficient from a particle trajectory is a cornerstone of computational statistical physics. The problem as stated is valid, well-posed, and scientifically grounded. It presents a realistic scenario where an ideal diffusive process is perturbed by common experimental or simulational artifacts: ballistic motion at short times, confinement at long times, and measurement noise. A robust analysis method must correctly identify the time window corresponding to Fickian diffusion and extract the coefficient from it.\n\nHerein, I will develop a comprehensive, step-by-step solution that adheres to the provided algorithmic requirements.\n\n### 1. Theoretical Foundation: MSD and the Diffusion Coefficient\n\nThe mean-square displacement (MSD) is the primary observable for characterizing the motion of a diffusing particle. For a particle undergoing Brownian motion in $d$ spatial dimensions, its position $\\mathbf{r}(t)$ is described by a stochastic process. The displacement over a time interval $\\tau$ is $\\Delta\\mathbf{r}(\\tau) = \\mathbf{r}(t+\\tau) - \\mathbf{r}(t)$.\n\nFor an isotropic, memoryless random walk (a Wiener process), the displacements along each Cartesian coordinate are independent and identically distributed Gaussian random variables with zero mean and variance proportional to $\\tau$.\nLet $\\Delta x_i(\\tau)$ be the displacement along the $i$-th coordinate. The variance is given by the Einstein relation in one dimension:\n$$\n\\langle (\\Delta x_i(\\tau))^2 \\rangle = 2 D \\tau\n$$\nwhere $D$ is the diffusion coefficient. The angle brackets $\\langle \\cdot \\rangle$ denote an ensemble average.\n\nThe squared magnitude of the total displacement vector is the sum of the squared displacements along each coordinate:\n$$\n\\|\\Delta\\mathbf{r}(\\tau)\\|^2 = \\sum_{i=1}^d (\\Delta x_i(\\tau))^2\n$$\nTaking the ensemble average and using the independence of motion along each coordinate, we find the ensemble-averaged MSD:\n$$\n\\langle \\|\\Delta\\mathbf{r}(\\tau)\\|^2 \\rangle = \\sum_{i=1}^d \\langle (\\Delta x_i(\\tau))^2 \\rangle = \\sum_{i=1}^d 2 D \\tau = 2 d D \\tau\n$$\nThus, for a pure diffusive process, the MSD is a linear function of the time lag $\\tau$. The slope of the $\\mathrm{MSD}(\\tau)$ versus $\\tau$ plot is $m = 2dD$. Consequently, the diffusion coefficient can be calculated from the empirically determined slope $m$ as:\n$$\nD = \\frac{m}{2d}\n$$\nThis fundamental relation will be used to compute $D$ from the slope obtained via linear regression. In practice, we use the time-averaged MSD as an estimator for the ensemble-averaged MSD, assuming ergodicity.\n\n### 2. Algorithmic Design\n\n#### 2.1. Trajectory Generation\nFor each test case, a discrete-time trajectory $\\{\\mathbf{r}_n\\}_{n=0}^{N-1}$ is generated according to the specified stochastic evolution rule. The `numpy.random` module, with its seed fixed for reproducibility, is used to generate the required random increments.\n\n- **Case 1 (Free Diffusion):** $\\mathbf{r}_{n+1} = \\mathbf{r}_n + \\boldsymbol{\\eta}_n$, where each component of $\\boldsymbol{\\eta}_n$ is drawn from a Gaussian distribution with mean $0$ and variance $2 D_{\\mathrm{true}} \\Delta t$.\n- **Case 2 (Drift-Diffusion):** $\\mathbf{r}_{n+1} = \\mathbf{r}_n + \\mathbf{v} \\Delta t + \\boldsymbol{\\eta}_n$. This adds a deterministic drift to the random walk.\n- **Case 3 (Ornstein-Uhlenbeck):** $r_{n+1} = \\alpha r_n + \\sigma \\xi_n$, with $\\alpha = \\exp(-\\Delta t / \\tau_c)$, $\\sigma = \\sqrt{D_{\\mathrm{true}} \\tau_c (1 - \\alpha^2)}$, and $\\xi_n \\sim \\mathcal{N}(0,1)$. This models diffusion in a harmonic potential.\n- **Case 4 (Diffusion with Noise):** A free diffusion trajectory $\\mathbf{r}'_n$ is first generated. The observed trajectory is then $\\mathbf{r}_n = \\mathbf{r}'_n + \\boldsymbol{\\epsilon}_n$, where each component of the measurement noise $\\boldsymbol{\\epsilon}_n$ is drawn from a Gaussian distribution with mean $0$ and variance $\\sigma_{\\mathrm{meas}}^2$.\n\n#### 2.2. Fast MSD Calculation\nA naive computation of the time-averaged MSD involves nested loops, resulting in $O(N^2)$ complexity. The problem requires a more efficient $O(N \\log N)$ algorithm, which can be achieved using the Fast Fourier Transform (FFT) to accelerate the calculation of autocorrelation functions.\n\nThe MSD formula is:\n$$\n\\mathrm{MSD}(k) = \\frac{1}{N-k} \\sum_{n=0}^{N-k-1} \\|\\mathbf{r}_{n+k} - \\mathbf{r}_n\\|^2\n$$\nExpanding the squared norm:\n$$\n\\mathrm{MSD}(k) = \\frac{1}{N-k} \\left[ \\sum_{n=0}^{N-k-1} \\|\\mathbf{r}_{n+k}\\|^2 + \\sum_{n=0}^{N-k-1} \\|\\mathbf{r}_n\\|^2 - 2 \\sum_{n=0}^{N-k-1} \\mathbf{r}_{n+k} \\cdot \\mathbf{r}_n \\right]\n$$\nThe dot product term can be written as a sum over dimensions $j$: $\\sum_{j=1}^d \\sum_{n=0}^{N-k-1} r_{j, n+k} r_{j, n}$. Each inner sum is the autocorrelation of the $j$-th coordinate series. According to the Wiener-Khinchin theorem, the autocorrelation of a signal can be computed as the inverse FFT of its power spectral density. The `scipy.signal.fftconvolve` function provides an efficient way to compute this. For a series `x`, `fftconvolve(x, x[::-1], mode='full')` calculates the unnormalized autocorrelation.\n\nThe algorithm is as follows:\n1.  For the trajectory $\\mathbf{r}$ of shape $(N, d)$, compute the squared magnitude at each time step: $S_2[n] = \\|\\mathbf{r}_n\\|^2$.\n2.  The two sums involving $S_2$ can be computed efficiently from the cumulative sum of the $S_2$ array.\n3.  For each dimension $j=1, \\dots, d$, compute the autocorrelation function $A_j(k) = \\sum_{n=0}^{N-k-1} r_{j,n} r_{j,n+k}$ using `fftconvolve`.\n4.  Combine these terms to get $\\mathrm{MSD}(k)$ for lags $k=1, \\dots, K_{\\max}$. A maximum lag of $K_{\\max} = \\lfloor N/4 \\rfloor$ is chosen to ensure the number of averaging pairs, $N-k$, remains large enough for good statistics.\n\n#### 2.3. Principled Window Selection\nThis is the most critical part of the algorithm, as it must distinguish the linear diffusive regime from non-linear artifacts.\n\n1.  **Local Scaling Analysis:** The local log-log slope of the MSD curve, $\\alpha(\\tau) = d(\\log \\mathrm{MSD}) / d(\\log \\tau)$, is a key indicator. For pure diffusion, $\\alpha=1$. For ballistic motion, $\\alpha=2$. For confinement, $\\alpha$ approaches $0$. We estimate $\\alpha$ at each lag $k$ using a centered finite difference on the log-transformed data:\n    $$\n    \\alpha_k \\approx \\frac{\\log(\\mathrm{MSD}_{k+1}) - \\log(\\mathrm{MSD}_{k-1})}{\\log(\\tau_{k+1}) - \\log(\\tau_{k-1})}\n    $$\n    This is calculated for $k$ from a small starting value up to $K_{\\max}-1$.\n\n2.  **Primary Search Strategy:** We search for the longest contiguous window of lags, $[k_{\\min}, k_{\\max}]$, that satisfies two conditions:\n    a. The window must have a minimum length (e.g., $20$ points) to be statistically meaningful for a fit.\n    b. For every lag $k$ in the window, the local slope $\\alpha_k$ must be close to $1$, i.e., $| \\alpha_k - 1 |  \\epsilon$, where a tolerance of $\\epsilon=0.15$ is used.\n\n3.  **Fallback Search Strategy:** If the primary strategy fails to find a suitable window (e.g., due to noise or a very short linear regime), a more exhaustive search is performed.\n    a. A collection of candidate windows of varying lengths (e.g., from $20$ to $50$ points) is generated across the available lags.\n    b. For each candidate window, a weighted linear regression is performed on $\\mathrm{MSD}(\\tau)$ vs. $\\tau$. The weights are set to $w_k = N-k$ to give more importance to shorter lags with better statistics. A weighted coefficient of determination, $R^2_w$, is computed.\n    c. We filter these candidate windows, keeping only those with an excellent goodness-of-fit (e.g., $R^2_w  0.999$).\n    d. From this filtered set, we select the window whose average log-log slope, $\\bar{\\alpha}$, is closest to $1$. This ensures we select the most linear window that also exhibits the signature of diffusion.\n\n#### 2.4. Weighted Linear Regression and Final Calculation\nOnce the optimal window $[k_{\\min}, k_{\\max}]$ is identified:\n1.  A weighted linear regression of $\\mathrm{MSD}(\\tau_k)$ versus $\\tau_k = k \\Delta t$ is performed for $k \\in [k_{\\min}, k_{\\max}]$. The `numpy.polyfit` function is used with weights $w_k = N-k$.\n2.  The fit yields a slope, $m$.\n3.  The diffusion coefficient is calculated using the derived formula: $D = m / (2d)$.\n4.  The result is formatted to six significant figures in scientific notation.\n\nThis comprehensive approach ensures that the estimation is robust to the challenges presented in the test suite, correctly identifying the diffusive regime in each case and providing an accurate estimate of the diffusion coefficient.",
            "answer": "```python\nimport numpy as np\nfrom scipy.signal import fftconvolve\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        {\n            \"id\": 1, \"d\": 3, \"delta_t\": 1e-3, \"N\": 5000, \n            \"D_true\": 1e-9, \"seed\": 12345, \"type\": \"free_diffusion\"\n        },\n        {\n            \"id\": 2, \"d\": 2, \"delta_t\": 5e-3, \"N\": 6000, \n            \"D_true\": 5e-10, \"v_drift\": np.array([1e-6, -1e-6]), \n            \"seed\": 54321, \"type\": \"drift_diffusion\"\n        },\n        {\n            \"id\": 3, \"d\": 1, \"delta_t\": 1e-3, \"N\": 7000, \n            \"D_true\": 2e-12, \"tau_c\": 0.2, \"seed\": 777, \"type\": \"ornstein_uhlenbeck\"\n        },\n        {\n            \"id\": 4, \"d\": 2, \"delta_t\": 2e-3, \"N\": 3000, \n            \"D_true\": 3e-10, \"sigma_meas\": 5e-7, \"seed\": 888, \n            \"type\": \"diffusion_with_noise\"\n        }\n    ]\n\n    results = []\n    for params in test_cases:\n        D_est = compute_diffusion_coefficient(params)\n        results.append(f\"{D_est:.5e}\")\n\n    print(f\"[{','.join(results)}]\")\n\ndef compute_diffusion_coefficient(params):\n    \"\"\"\n    Orchestrates the calculation of the diffusion coefficient for a single case.\n    \"\"\"\n    # 1. Generate trajectory\n    trajectory = generate_trajectory(params)\n\n    # 2. Compute MSD using FFT\n    N = params['N']\n    k_max = N // 4  # Use up to 1/4 of the trajectory for MSD lags\n    msd = calculate_msd_fft(trajectory, N, k_max)\n\n    # 3. Find the optimal linear fitting window\n    k_min, k_max_fit = find_linear_regime(msd, N, params['delta_t'], k_max)\n    \n    # 4. Perform weighted linear regression and compute D\n    lags = np.arange(k_min, k_max_fit + 1)\n    taus = lags * params['delta_t']\n    msd_window = msd[lags - 1] # msd is 0-indexed, lags are 1-indexed\n\n    # Weights are the number of samples used for each lag\n    weights = N - lags\n\n    # Perform weighted linear regression\n    slope, _ = np.polyfit(taus, msd_window, 1, w=weights)\n    \n    # Calculate diffusion coefficient\n    D = slope / (2 * params['d'])\n    \n    return D\n\ndef generate_trajectory(params):\n    \"\"\"\n    Generates a particle trajectory based on the provided parameters.\n    \"\"\"\n    np.random.seed(params['seed'])\n    N, d, delta_t = params['N'], params['d'], params['delta_t']\n    \n    if params['type'] == 'free_diffusion':\n        increments = np.random.normal(\n            loc=0.0, \n            scale=np.sqrt(2 * params['D_true'] * delta_t), \n            size=(N - 1, d)\n        )\n        trajectory = np.concatenate([np.zeros((1, d)), np.cumsum(increments, axis=0)])\n        return trajectory\n\n    elif params['type'] == 'drift_diffusion':\n        noise = np.random.normal(\n            loc=0.0, \n            scale=np.sqrt(2 * params['D_true'] * delta_t), \n            size=(N - 1, d)\n        )\n        drift = params['v_drift'] * delta_t\n        increments = noise + drift\n        trajectory = np.concatenate([np.zeros((1, d)), np.cumsum(increments, axis=0)])\n        return trajectory\n        \n    elif params['type'] == 'ornstein_uhlenbeck':\n        alpha = np.exp(-delta_t / params['tau_c'])\n        sigma = np.sqrt(params['D_true'] * params['tau_c'] * (1 - alpha**2))\n        \n        trajectory = np.zeros((N, d))\n        for n in range(N - 1):\n            noise = np.random.normal(loc=0.0, scale=sigma)\n            trajectory[n+1] = alpha * trajectory[n] + noise\n        return trajectory\n\n    elif params['type'] == 'diffusion_with_noise':\n        # Generate underlying true trajectory\n        increments = np.random.normal(\n            loc=0.0, \n            scale=np.sqrt(2 * params['D_true'] * delta_t), \n            size=(N - 1, d)\n        )\n        true_trajectory = np.concatenate([np.zeros((1, d)), np.cumsum(increments, axis=0)])\n        \n        # Add measurement noise\n        measurement_noise = np.random.normal(\n            loc=0.0, \n            scale=params['sigma_meas'], \n            size=(N, d)\n        )\n        return true_trajectory + measurement_noise\n    \n    else:\n        raise ValueError(f\"Unknown trajectory type: {params['type']}\")\n\ndef calculate_msd_fft(r, N, k_max):\n    \"\"\"\n    Computes time-averaged MSD using FFT-based autocorrelation.\n    Complexity: O(N log N).\n    \"\"\"\n    # Pad to 2N for acyclic convolution\n    M = 2 * N\n    \n    # Term 3: Autocorrelation part\n    autocorr_sum = np.zeros(N)\n    for i in range(r.shape[1]):\n        x = r[:, i]\n        # FFT-based autocorrelation\n        Fx = np.fft.fft(x, n=M)\n        PSD = Fx * np.conj(Fx)\n        autocorr = np.fft.ifft(PSD)\n        # We need sum_{n=0}^{N-k-1} x_{n}x_{n+k}, which is the (N-k)-th element\n        # of the convolution of x with reversed x. fftconvolve is cleaner.\n        ac = fftconvolve(x, x[::-1], mode='full')[N-1:]\n        autocorr_sum += ac\n\n    # Term 1  2: Sums of squared magnitudes\n    S2 = np.sum(r**2, axis=1)\n    Q = np.zeros(N)\n    # cumsum is faster than a loop\n    S2_cumsum = np.concatenate(([0], np.cumsum(S2)))\n    \n    for k in range(1, N):\n        Q[k] = S2_cumsum[N] - S2_cumsum[k] + S2_cumsum[N-k]\n\n    msd_num = Q - 2 * autocorr_sum\n    # Denominator\n    denom = np.arange(N, 0, -1)\n    \n    msd = msd_num[1:k_max+1] / denom[1:k_max+1]\n    return msd\n\ndef find_linear_regime(msd, N, delta_t, k_max):\n    \"\"\"\n    Finds the optimal window for linear fitting of MSD vs. time.\n    \"\"\"\n    # 1. Local slope analysis\n    lags = np.arange(1, len(msd) + 1)\n    log_msd = np.log(msd)\n    log_tau = np.log(lags * delta_t)\n\n    # Use centered difference for slope, avoiding boundaries\n    slopes = (log_msd[2:] - log_msd[:-2]) / (log_tau[2:] - log_tau[:-2])\n    slope_lags = lags[1:-1]\n\n    # 2. Primary strategy: find longest contiguous window where slope is near 1\n    slope_tolerance = 0.15\n    min_window_len = 20\n    is_linear = np.abs(slopes - 1.0)  slope_tolerance\n    \n    longest_window = (0, -1) # (start, end)\n    current_start = -1\n    for i, is_lin in enumerate(is_linear):\n        if is_lin and current_start == -1:\n            current_start = i\n        elif not is_lin and current_start != -1:\n            if i - current_start  longest_window[1] - longest_window[0] + 1:\n                longest_window = (current_start, i-1)\n            current_start = -1\n    if current_start != -1: # check for segment at the end\n        if len(is_linear) - current_start  longest_window[1] - longest_window[0] + 1:\n            longest_window = (current_start, len(is_linear)-1)\n\n    if longest_window[1] - longest_window[0] + 1 = min_window_len:\n        k_min = slope_lags[longest_window[0]]\n        k_max_fit = slope_lags[longest_window[1]]\n        return k_min, k_max_fit\n\n    # 3. Fallback strategy: search over candidate windows\n    best_window = (0,0)\n    best_score = float('inf')\n    \n    # Filter for high R-squared windows first\n    good_r2_windows = []\n    \n    for length in range(20, 51, 5):\n        for k_start in range(2, k_max - length):\n            k_end = k_start + length - 1\n            \n            win_lags = np.arange(k_start, k_end + 1)\n            win_taus = win_lags * delta_t\n            win_msd = msd[win_lags - 1]\n            win_weights = N - win_lags\n\n            # Weighted R^2 calculation\n            p, res, _, _, _ = np.polyfit(win_taus, win_msd, 1, w=win_weights, full=True)\n            y_pred = np.polyval(p, win_taus)\n            y_mean_w = np.sum(win_msd * win_weights) / np.sum(win_weights)\n            ss_tot_w = np.sum(win_weights * (win_msd - y_mean_w)**2)\n            ss_res_w = res[0]\n            if ss_tot_w  1e-12: # Avoid division by zero\n                r2_w = 1 - ss_res_w / ss_tot_w\n                if r2_w  0.999: # goodness-of-fit threshold\n                    # Calculate average log-log slope for this window\n                    avg_slope = np.mean(slopes[(slope_lags = k_start)  (slope_lags = k_end)])\n                    good_r2_windows.append(((k_start, k_end), avg_slope))\n\n    if good_r2_windows:\n        # From good R2 windows, pick one with slope closest to 1\n        best_window, _ = min(good_r2_windows, key=lambda item: abs(item[1] - 1.0))\n        return best_window[0], best_window[1]\n    \n    # Final desperation: if no window has good R2, just find one closest to slope=1\n    for length in range(20, 51, 5):\n        for k_start in range(2, k_max - length):\n            k_end = k_start+length-1\n            avg_slope = np.mean(slopes[(slope_lags = k_start)  (slope_lags = k_end)])\n            score = abs(avg_slope - 1.0)\n            if score  best_score:\n                best_score = score\n                best_window = (k_start, k_end)\n\n    return best_window[0], best_window[1]\n\nif __name__ == \"__main__\":\n    solve()\n\n```"
        },
        {
            "introduction": "Computer simulations are performed on finite systems, yet we often seek to describe the properties of macroscopic matter. The periodic boundary conditions used to mimic an infinite system introduce artificial correlations that lead to systematic, size-dependent errors in calculated transport properties. This hands-on practice introduces the crucial technique of finite-size scaling to correct for these artifacts. By analyzing how the measured diffusion coefficient $D_L$ varies with the simulation box size $L$, you will learn to extrapolate to the infinite-system limit, $D_\\infty$, providing a result that is directly comparable to experimental measurements. ",
            "id": "3740818",
            "problem": "You are given measurements of the diffusion coefficient in finite cubic simulation boxes under periodic boundary conditions. The objective is to construct and implement a scientifically principled finite-size scaling procedure to extrapolate the infinite-system diffusion coefficient and to quantify its uncertainty. The approach must start from first principles appropriate to multiscale modeling and analysis and must reason from fundamental statistical mechanics and continuum hydrodynamics. Specifically, you must derive the leading-order dependence of the measured diffusion coefficient on the simulation box size, justify the regression model you use, and design uncertainty quantification that is scientifically defensible for heterogeneous measurement noise.\n\nYour program must, for each provided test case:\n- Derive an appropriate scaling variable from the box size $L$ to capture the leading finite-size dependence of the diffusion coefficient under periodic boundary conditions.\n- Pose a linear statistical model for the measured diffusion coefficients $D_L$ that is consistent with your derivation and treats heteroscedastic measurement errors (i.e., different uncertainties for different box sizes) appropriately.\n- Estimate the infinite-system diffusion coefficient $D_\\infty$ using a weighted least squares procedure grounded in your derivation.\n- Estimate the uncertainty in $D_\\infty$ using both an analytical approach based on the covariance of the weighted estimator and a parametric bootstrap procedure that is consistent with the given measurement uncertainties.\n- Using the connection between hydrodynamic mobility and diffusion via the Einstein relation, assess whether the slope implied by continuum hydrodynamics for cubic periodic boundary conditions is statistically consistent with the fitted slope at the $95\\%$ confidence level.\n- Produce numerical outputs for each test case in the exact format specified below.\n\nAll outputs involving diffusion coefficients must be expressed in $\\mathrm{m}^2/\\mathrm{s}$, and all other quantities must be in International System of Units (SI). Angles are not present in this problem. Probabilities and confidence levels should be treated as decimals.\n\nYour program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element of this list corresponds to one test case and must itself be a list containing:\n- The estimated infinite-system diffusion coefficient $D_\\infty$ in $\\mathrm{m}^2/\\mathrm{s}$ as a float.\n- The bootstrap-based standard uncertainty (standard deviation) of $D_\\infty$ in $\\mathrm{m}^2/\\mathrm{s}$ as a float.\n- A boolean indicating whether the theoretically predicted slope from continuum hydrodynamics lies within the $95\\%$ confidence interval of the fitted slope (true if consistent, false otherwise).\n\nThus, for $N$ test cases, the final output must be a single line of the form $[[D_{\\infty,1},u_{1},b_{1}],\\ldots,[D_{\\infty,N},u_{N},b_{N}]]$.\n\nTest Suite and Inputs:\nYou must use the following four test cases. In each case, $k_\\mathrm{B}$ denotes the Boltzmann constant, $T$ the temperature, $\\eta$ the dynamic viscosity, and $\\xi$ the geometry-dependent constant for cubic periodic boundary conditions. The measurement uncertainties are one standard deviation for each reported diffusion coefficient. All lengths are given in meters, diffusion coefficients in $\\mathrm{m}^2/\\mathrm{s}$, temperatures in Kelvin, and viscosities in $\\mathrm{Pa}\\cdot\\mathrm{s}$.\n\n- Case $1$ (general happy path, multiple sizes):\n  - $T = 300\\,\\mathrm{K}$, $\\eta = 8.9\\times 10^{-4}\\,\\mathrm{Pa}\\cdot\\mathrm{s}$, $\\xi = 2.837297$.\n  - Sizes $L$: $[3.0\\times 10^{-9},\\,4.0\\times 10^{-9},\\,6.0\\times 10^{-9},\\,8.0\\times 10^{-9}]$.\n  - Measured $D_L$: $[1.97\\times 10^{-9},\\,2.03\\times 10^{-9},\\,2.09\\times 10^{-9},\\,2.11\\times 10^{-9}]$.\n  - Measurement uncertainties $\\sigma$: $[2.0\\times 10^{-11},\\,1.5\\times 10^{-11},\\,1.0\\times 10^{-11},\\,1.0\\times 10^{-11}]$.\n\n- Case $2$ (boundary case with only two sizes):\n  - $T = 350\\,\\mathrm{K}$, $\\eta = 5.0\\times 10^{-4}\\,\\mathrm{Pa}\\cdot\\mathrm{s}$, $\\xi = 2.837297$.\n  - Sizes $L$: $[4.0\\times 10^{-9},\\,8.0\\times 10^{-9}]$.\n  - Measured $D_L$: $[2.64\\times 10^{-9},\\,2.80\\times 10^{-9}]$.\n  - Measurement uncertainties $\\sigma$: $[3.0\\times 10^{-11},\\,2.0\\times 10^{-11}]$.\n\n- Case $3$ (near-collinearity in predictor due to nearly equal sizes):\n  - $T = 280\\,\\mathrm{K}$, $\\eta = 1.5\\times 10^{-3}\\,\\mathrm{Pa}\\cdot\\mathrm{s}$, $\\xi = 2.837297$.\n  - Sizes $L$: $[5.0\\times 10^{-9},\\,5.1\\times 10^{-9},\\,9.0\\times 10^{-9}]$.\n  - Measured $D_L$: $[1.12\\times 10^{-9},\\,1.125\\times 10^{-9},\\,1.158\\times 10^{-9}]$.\n  - Measurement uncertainties $\\sigma$: $[5.0\\times 10^{-12},\\,5.0\\times 10^{-12},\\,3.0\\times 10^{-12}]$.\n\n- Case $4$ (small boxes with stronger finite-size effects):\n  - $T = 300\\,\\mathrm{K}$, $\\eta = 8.9\\times 10^{-4}\\,\\mathrm{Pa}\\cdot\\mathrm{s}$, $\\xi = 2.837297$.\n  - Sizes $L$: $[2.0\\times 10^{-9},\\,3.0\\times 10^{-9},\\,4.0\\times 10^{-9}]$.\n  - Measured $D_L$: $[2.15\\times 10^{-9},\\,2.26\\times 10^{-9},\\,2.34\\times 10^{-9}]$.\n  - Measurement uncertainties $\\sigma$: $[2.0\\times 10^{-11},\\,2.0\\times 10^{-11},\\,1.5\\times 10^{-11}]$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is a list $[D_\\infty,u,\\text{boolean}]$ for the corresponding test case, in the same order as above. For example: $[[d_1,u_1,b_1],[d_2,u_2,b_2],[d_3,u_3,b_3],[d_4,u_4,b_4]]$. All $d_i$ and $u_i$ must be floats representing values in $\\mathrm{m}^2/\\mathrm{s}$, and $b_i$ must be booleans.",
            "solution": "The problem requires the extrapolation of the infinite-system diffusion coefficient, $D_\\infty$, from measurements of diffusion coefficients, $D_L$, in finite cubic simulation boxes of side length $L$. This involves deriving a finite-size scaling law, applying a statistically sound regression procedure, and quantifying uncertainties.\n\n### Theoretical Derivation of the Finite-Size Scaling Law\n\nThe self-diffusion coefficient, $D$, of a particle in a fluid is related to its thermal energy and the friction it experiences from the fluid. This relationship is given by the Einstein relation:\n$$D = \\frac{k_\\mathrm{B} T}{\\zeta}$$\nwhere $k_\\mathrm{B}$ is the Boltzmann constant, $T$ is the absolute temperature, and $\\zeta$ is the friction coefficient.\n\nIn a computer simulation employing periodic boundary conditions (PBCs), a particle interacts not only with other particles in the primary simulation box but also with their periodic images. This is particularly significant for long-range interactions, such as the hydrodynamic interactions that govern friction. The motion of a particle creates a flow field in the fluid, which, due to the PBCs, interacts with the particle's own periodic images. This self-interaction effectively increases the friction on the particle compared to what it would experience in an infinite, non-periodic system.\n\nThe friction coefficient in a finite box of size $L$, denoted $\\zeta_L$, is therefore larger than the infinite-system friction coefficient, $\\zeta_\\infty$. As a result, the measured diffusion coefficient, $D_L = k_\\mathrm{B} T / \\zeta_L$, is smaller than the true infinite-system diffusion coefficient, $D_\\infty = k_\\mathrm{B} T / \\zeta_\\infty$.\n\nThe leading-order correction to the diffusion coefficient due to finite-size effects in a cubic periodic system was derived from continuum hydrodynamics by Dünweg and Kremer, and later refined by Yeh and Hummer. The result is:\n$$D_L = D_\\infty - \\frac{k_\\mathrm{B} T \\xi}{6 \\pi \\eta L}$$\nHere, $\\eta$ is the dynamic viscosity of the solvent, $L$ is the side length of the cubic a box, and $\\xi$ is a dimensionless constant that depends on the geometry of the periodic lattice. For a simple cubic lattice, its value is $\\xi \\approx 2.837297$.\n\nThis equation provides the required scaling relationship. It predicts a linear relationship between the measured diffusion coefficient, $D_L$, and the inverse of the box size, $1/L$.\n\n### Statistical Model and Parameter Estimation\n\nThe derived scaling law can be formulated as a linear regression model. Let $y_i = D_{L_i}$ be the measured diffusion coefficient for a box of size $L_i$, and let the scaling variable be $x_i = 1/L_i$. The model is:\n$$y_i = D_\\infty + m \\cdot x_i + \\epsilon_i$$\nwhere the intercept is the parameter of interest, $D_\\infty$, and the slope is predicted by theory to be $m_{\\text{theory}} = -\\frac{k_\\mathrm{B} T \\xi}{6 \\pi \\eta}$. The term $\\epsilon_i$ represents the measurement error for the $i$-th data point.\n\nThe problem specifies that the measurements have different uncertainties, $\\sigma_i$. This is a case of heteroscedasticity. The appropriate method for parameter estimation is Weighted Least Squares (WLS). WLS minimizes the weighted sum of squared residuals, where each residual is weighted by the inverse of its corresponding variance. The weights are $w_i = 1/\\sigma_i^2$.\n\nThe WLS problem can be solved using matrix algebra. We define the vector of measurements $Y$, the design matrix $X$, and the diagonal weight matrix $W$:\n$$Y = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_N \\end{pmatrix}, \\quad X = \\begin{pmatrix} 1  x_1 \\\\ 1  x_2 \\\\ \\vdots  \\vdots \\\\ 1  x_N \\end{pmatrix}, \\quad W = \\begin{pmatrix} w_1  0  \\cdots  0 \\\\ 0  w_2   0 \\\\ \\vdots   \\ddots  \\vdots \\\\ 0  0  \\cdots  w_N \\end{pmatrix}$$\nThe vector of estimated parameters, $\\hat{\\beta} = (\\hat{D}_\\infty, \\hat{m})^T$, is given by the normal equations for WLS:\n$$\\hat{\\beta} = (X^T W X)^{-1} X^T W Y$$\nThe estimated infinite-system diffusion coefficient is the first component of this vector, $\\hat{D}_\\infty = \\hat{\\beta}_1$.\n\n### Uncertainty Quantification\n\nTwo methods are required to quantify the uncertainty in the estimate $\\hat{D}_\\infty$.\n\n1.  **Analytical Uncertainty (for slope testing)**: Under the assumption that the linear model is correct and the errors are normally distributed, the covariance matrix of the estimated parameters $\\hat{\\beta}$ is given by:\n    $$\\text{Cov}(\\hat{\\beta}) = (X^T W X)^{-1}$$\n    The variance of each parameter estimate is found on the diagonal of this matrix. Specifically, the variance of the estimated slope is $\\text{Var}(\\hat{m}) = [\\text{Cov}(\\hat{\\beta})]_{22}$. The standard uncertainty is $u(\\hat{m}) = \\sqrt{\\text{Var}(\\hat{m})}$.\n\n2.  **Parametric Bootstrap (for $D_\\infty$ uncertainty)**: The bootstrap is a non-parametric resampling method, but here we use a parametric version since we have a model for the data-generating process. It provides a robust estimate of the uncertainty, especially for small sample sizes. The procedure is:\n    a. Estimate the parameters $\\hat{D}_\\infty$ and $\\hat{m}$ from the original data using WLS.\n    b. For a large number of repetitions ($B$):\n        i. Generate a synthetic dataset. For each original data point $(x_i, \\sigma_i)$, create a new pseudo-measurement $y_i^{\\text{boot}} = (\\hat{D}_\\infty + \\hat{m} x_i) + \\delta_i$, where $\\delta_i$ is a random variate drawn from a normal distribution $\\mathcal{N}(0, \\sigma_i^2)$.\n        ii. Perform a WLS fit on this synthetic dataset $(x_i, y_i^{\\text{boot}}, \\sigma_i)$ to get a new estimate, $\\hat{D}_{\\infty}^{\\text{boot}}$.\n    c. The collection of $B$ estimates, $\\{\\hat{D}_{\\infty}^{\\text{boot}}\\}$, forms an empirical distribution for the estimator. The standard deviation of this distribution is the bootstrap estimate of the standard uncertainty of $D_\\infty$.\n\n### Consistency Check of the Slope\n\nThe final task is to assess whether the fitted slope $\\hat{m}$ is statistically consistent with the theoretical prediction $m_{\\text{theory}} = -\\frac{k_\\mathrm{B} T \\xi}{6 \\pi \\eta}$. This is done by constructing a confidence interval for the true slope based on the WLS fit and checking if $m_{\\text{theory}}$ falls within it.\n\nA two-sided $95\\%$ confidence interval for the slope $m$ is calculated as:\n$$\\text{CI}_{95\\%} = [\\hat{m} - z_{0.975} \\cdot u(\\hat{m}), \\quad \\hat{m} + z_{0.975} \\cdot u(\\hat{m})]$$\nwhere $\\hat{m}$ is the WLS estimate, $u(\\hat{m})$ is its analytical standard uncertainty, and $z_{0.975} \\approx 1.96$ is the critical value from the standard normal distribution corresponding to a cumulative probability of $0.975$. The theoretical slope is deemed consistent if $m_{\\text{theory}} \\in \\text{CI}_{95\\%}$.\n\nThis comprehensive procedure, from first-principles derivation to statistical analysis, provides a rigorous method for analyzing finite-size effects in diffusion coefficients, yielding an estimate for $D_\\infty$, its uncertainty, and a physical consistency check.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Main function to process test cases and calculate finite-size corrected diffusion coefficients.\n    \"\"\"\n    k_B = 1.380649e-23  # Boltzmann constant in J/K\n\n    test_cases = [\n        {\n            \"T\": 300.0, \"eta\": 8.9e-4, \"xi\": 2.837297,\n            \"L\": np.array([3.0e-9, 4.0e-9, 6.0e-9, 8.0e-9]),\n            \"D_L\": np.array([1.97e-9, 2.03e-9, 2.09e-9, 2.11e-9]),\n            \"sigma\": np.array([2.0e-11, 1.5e-11, 1.0e-11, 1.0e-11])\n        },\n        {\n            \"T\": 350.0, \"eta\": 5.0e-4, \"xi\": 2.837297,\n            \"L\": np.array([4.0e-9, 8.0e-9]),\n            \"D_L\": np.array([2.64e-9, 2.80e-9]),\n            \"sigma\": np.array([3.0e-11, 2.0e-11])\n        },\n        {\n            \"T\": 280.0, \"eta\": 1.5e-3, \"xi\": 2.837297,\n            \"L\": np.array([5.0e-9, 5.1e-9, 9.0e-9]),\n            \"D_L\": np.array([1.12e-9, 1.125e-9, 1.158e-9]),\n            \"sigma\": np.array([5.0e-12, 5.0e-12, 3.0e-12])\n        },\n        {\n            \"T\": 300.0, \"eta\": 8.9e-4, \"xi\": 2.837297,\n            \"L\": np.array([2.0e-9, 3.0e-9, 4.0e-9]),\n            \"D_L\": np.array([2.15e-9, 2.26e-9, 2.34e-9]),\n            \"sigma\": np.array([2.0e-11, 2.0e-11, 1.5e-11])\n        }\n    ]\n\n    results = []\n    n_bootstrap = 10000\n    rng = np.random.default_rng(seed=42) # for reproducibility\n\n    for case in test_cases:\n        T, eta, xi = case[\"T\"], case[\"eta\"], case[\"xi\"]\n        L, D_L, sigma = case[\"L\"], case[\"D_L\"], case[\"sigma\"]\n\n        # 1. Prepare data for Weighted Least Squares (WLS)\n        x = 1.0 / L\n        y = D_L\n        N = len(y)\n        \n        X = np.vstack([np.ones(N), x]).T\n        W = np.diag(1.0 / sigma**2)\n\n        # 2. Perform WLS to get D_inf and slope\n        # beta_hat = (X^T W X)^-1 X^T W Y\n        XTWX_inv = np.linalg.inv(X.T @ W @ X)\n        beta_hat = XTWX_inv @ X.T @ W @ y\n        D_inf_hat = beta_hat[0]\n        m_hat = beta_hat[1]\n\n        # 3. Parametric Bootstrap for uncertainty in D_inf\n        y_fit = D_inf_hat + m_hat * x\n        bootstrap_D_inf_estimates = np.zeros(n_bootstrap)\n        for i in range(n_bootstrap):\n            y_boot = rng.normal(loc=y_fit, scale=sigma)\n            # Re-use pre-calculated (X^T W X)^-1 for efficiency\n            beta_boot = XTWX_inv @ X.T @ W @ y_boot\n            bootstrap_D_inf_estimates[i] = beta_boot[0]\n        \n        # Standard deviation of bootstrap estimates is the standard error\n        u_D_inf_bootstrap = np.std(bootstrap_D_inf_estimates, ddof=1)\n\n        # 4. Slope consistency check\n        # Theoretical slope\n        m_theory = -(k_B * T * xi) / (6 * np.pi * eta)\n        \n        # Analytical uncertainty of the fitted slope\n        var_m_hat = XTWX_inv[1, 1]\n        u_m_hat = np.sqrt(var_m_hat)\n        \n        # 95% confidence interval\n        z_crit = norm.ppf(0.975) # Approximately 1.96\n        ci_lower = m_hat - z_crit * u_m_hat\n        ci_upper = m_hat + z_crit * u_m_hat\n        \n        is_consistent = (ci_lower = m_theory = ci_upper)\n\n        # 5. Store results\n        results.append([D_inf_hat, u_D_inf_bootstrap, is_consistent])\n\n    # Format the final output string\n    sub_list_strs = []\n    for d_inf, u_d, is_consist in results:\n        # Pydantic-like boolean formatting for robustness ('true'/'false')\n        bool_str = 'true' if is_consist else 'false'\n        sub_list_strs.append(f\"[{d_inf},{u_d},{bool_str}]\")\n    \n    print(f\"[{','.join(sub_list_strs)}]\")\n\nsolve()\n```"
        }
    ]
}