{
    "hands_on_practices": [
        {
            "introduction": "In many physical systems, from liquids to magnetic materials, experimental probes such as neutron or X-ray scattering do not directly measure real-space correlations. Instead, they measure the static structure factor, $S(k)$, which is the Fourier transform of the pair correlation function. This exercise provides fundamental practice in bridging theory and experiment by deriving the structure factor for a system with exponentially decaying correlations, a model that arises in the theory of critical phenomena and screened interactions . Mastering this calculation is key to interpreting scattering data and understanding the reciprocal relationship between real-space organization and Fourier-space signatures.",
            "id": "3813218",
            "problem": "Consider a stationary, statistically homogeneous and isotropic fluid in three spatial dimensions with number density fluctuations denoted by $\\delta n(\\mathbf{r})$. The connected two-point correlation function is defined as $C_{nn}(\\mathbf{r})=\\langle \\delta n(\\mathbf{0})\\,\\delta n(\\mathbf{r})\\rangle$, which depends only on the radial separation $r=|\\mathbf{r}|$ due to isotropy. The static structure factor $S(\\mathbf{k})$ is defined as the spatial Fourier transform of the correlation function, $S(\\mathbf{k})=\\int_{\\mathbb{R}^{3}} \\exp(-\\mathrm{i}\\,\\mathbf{k}\\cdot \\mathbf{r})\\,C_{nn}(\\mathbf{r})\\,\\mathrm{d}^{3}\\mathbf{r}$, and is assumed to be measured along a one-dimensional cut through wavevector space with scalar wavenumber $k=|\\mathbf{k}|$. Suppose that the connected correlation function decays exponentially with a three-dimensional Yukawa form\n$$\nC_{nn}(r)=A\\,\\frac{\\exp(-r/\\xi)}{r},\n$$\nwhere $A>0$ is a constant amplitude and $\\xi>0$ is the correlation length. Starting from the definitions of statistical homogeneity, isotropy, and the Fourier transform, derive an analytic expression for the structure factor $S(k)$ as a function of $k$ and $\\xi$. Then, characterize the width of its main peak at $k=0$ by computing the full width at half maximum (FWHM). Provide the FWHM, denoted $\\Delta k$, as an explicit function of $\\xi$. Express your final answer as a two-entry row matrix containing first the closed-form $S(k)$ and second the closed-form $\\Delta k$. No numerical rounding is required. Do not include any units. For clarity, define the full width at half maximum (FWHM) as the distance in $k$ between the two points where $S(k)$ equals one-half of its maximum value at $k=0$ along the one-dimensional cut.",
            "solution": "The problem is first validated.\n\n### Step 1: Extract Givens\n- System: A stationary, statistically homogeneous and isotropic fluid in three spatial dimensions.\n- Fluctuation field: Number density fluctuations, $\\delta n(\\mathbf{r})$.\n- Correlation function: The connected two-point correlation function is $C_{nn}(\\mathbf{r})=\\langle \\delta n(\\mathbf{0})\\,\\delta n(\\mathbf{r})\\rangle$. Due to isotropy, $C_{nn}$ depends only on $r=|\\mathbf{r}|$. The specific form is given as $C_{nn}(r)=A\\,\\frac{\\exp(-r/\\xi)}{r}$, where $A>0$ is a constant amplitude and $\\xi>0$ is the correlation length.\n- Structure factor: The static structure factor $S(\\mathbf{k})$ is the spatial Fourier transform of the correlation function, $S(\\mathbf{k})=\\int_{\\mathbb{R}^{3}} \\exp(-\\mathrm{i}\\,\\mathbf{k}\\cdot \\mathbf{r})\\,C_{nn}(\\mathbf{r})\\,\\mathrm{d}^{3}\\mathbf{r}$. It is measured along a one-dimensional cut, with $k=|\\mathbf{k}|$.\n- FWHM definition: The full width at half maximum (FWHM) of the main peak at $k=0$ is the distance in $k$ between the two points where $S(k)$ equals one-half of its maximum value.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is well-grounded in statistical mechanics and condensed matter physics. The relationship between a real-space correlation function and its Fourier transform (the structure factor) is fundamental. The Yukawa form for the correlation function is physically significant, arising in theories of screened interactions and critical phenomena (e.g., Ornstein-Zernike theory).\n- **Well-Posed**: The problem is well-posed. It provides all necessary definitions, a specific functional form for the correlation function, and a clear, unambiguous task: to compute the structure factor and its FWHM. A unique, stable, and meaningful solution exists.\n- **Objective**: The problem is stated in precise, objective, mathematical language, free from any subjective or opinion-based claims.\n- All other validation criteria are met. The problem is complete, consistent, and directly relevant to the topic of spatial correlation functions.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be provided.\n\nThe static structure factor $S(\\mathbf{k})$ is the three-dimensional Fourier transform of the two-point correlation function $C_{nn}(\\mathbf{r})$:\n$$\nS(\\mathbf{k})=\\int_{\\mathbb{R}^{3}} \\exp(-\\mathrm{i}\\,\\mathbf{k}\\cdot \\mathbf{r})\\,C_{nn}(\\mathbf{r})\\,\\mathrm{d}^{3}\\mathbf{r}\n$$\nGiven that the system is isotropic, the correlation function depends only on the magnitude of the separation vector, $r=|\\mathbf{r}|$, so $C_{nn}(\\mathbf{r}) = C_{nn}(r)$. Similarly, the structure factor will depend only on the magnitude of the wavevector, $k=|\\mathbf{k}|$, so $S(\\mathbf{k})=S(k)$.\n\nTo evaluate the integral, we switch to spherical coordinates $(r, \\theta, \\phi)$ for the integration variable $\\mathbf{r}$. The volume element is $\\mathrm{d}^3\\mathbf{r} = r^2 \\sin\\theta \\, \\mathrm{d}r \\, \\mathrm{d}\\theta \\, \\mathrm{d}\\phi$. Due to isotropy, we are free to choose the orientation of our coordinate system. We align the $z$-axis with the wavevector $\\mathbf{k}$, so that $\\mathbf{k} = (0, 0, k)$. The dot product in the exponential becomes $\\mathbf{k}\\cdot\\mathbf{r} = kr\\cos\\theta$.\n\nThe integral for $S(k)$ is now:\n$$\nS(k) = \\int_0^\\infty \\int_0^\\pi \\int_0^{2\\pi} \\exp(-\\mathrm{i}kr\\cos\\theta) C_{nn}(r) \\, r^2 \\sin\\theta \\, \\mathrm{d}\\phi \\, \\mathrm{d}\\theta \\, \\mathrm{d}r\n$$\nThe integration over the azimuthal angle $\\phi$ is trivial, as the integrand is independent of it:\n$$\n\\int_0^{2\\pi} \\mathrm{d}\\phi = 2\\pi\n$$\nSubstituting the given form for the correlation function, $C_{nn}(r) = A \\frac{\\exp(-r/\\xi)}{r}$:\n$$\nS(k) = 2\\pi \\int_0^\\infty \\int_0^\\pi \\exp(-\\mathrm{i}kr\\cos\\theta) \\left(A \\frac{\\exp(-r/\\xi)}{r}\\right) r^2 \\sin\\theta \\, \\mathrm{d}\\theta \\, \\mathrm{d}r\n$$\nSimplifying the expression by canceling one power of $r$:\n$$\nS(k) = 2\\pi A \\int_0^\\infty r \\exp(-r/\\xi) \\left( \\int_0^\\pi \\exp(-\\mathrm{i}kr\\cos\\theta) \\sin\\theta \\, \\mathrm{d}\\theta \\right) \\mathrm{d}r\n$$\nWe first evaluate the inner integral over $\\theta$. Let $u = \\cos\\theta$, so $\\mathrm{d}u = -\\sin\\theta \\, \\mathrm{d}\\theta$. The limits of integration change from $\\theta \\in [0, \\pi]$ to $u \\in [1, -1]$.\n$$\n\\int_0^\\pi \\exp(-\\mathrm{i}kr\\cos\\theta) \\sin\\theta \\, \\mathrm{d}\\theta = \\int_1^{-1} \\exp(-\\mathrm{i}kru) (-\\mathrm{d}u) = \\int_{-1}^1 \\exp(-\\mathrm{i}kru) \\, \\mathrm{d}u\n$$\nThis integral is readily evaluated:\n$$\n\\int_{-1}^1 \\exp(-\\mathrm{i}kru) \\, \\mathrm{d}u = \\left[ \\frac{\\exp(-\\mathrm{i}kru)}{-\\mathrm{i}kr} \\right]_{-1}^1 = \\frac{\\exp(-\\mathrm{i}kr) - \\exp(\\mathrm{i}kr)}{-\\mathrm{i}kr} = \\frac{-2\\mathrm{i}\\sin(kr)}{-\\mathrm{i}kr} = \\frac{2\\sin(kr)}{kr}\n$$\nSubstituting this result back into the expression for $S(k)$:\n$$\nS(k) = 2\\pi A \\int_0^\\infty r \\exp(-r/\\xi) \\left( \\frac{2\\sin(kr)}{kr} \\right) \\mathrm{d}r\n$$\nThe term $kr$ in the denominator cancels with the $r$ outside the parenthesis and the $k$ can be factored out:\n$$\nS(k) = \\frac{4\\pi A}{k} \\int_0^\\infty \\sin(kr) \\exp(-r/\\xi) \\, \\mathrm{d}r\n$$\nThe remaining integral is a standard form, recognizable as the Laplace transform of $\\sin(kr)$ with transform variable $s=1/\\xi$. The Laplace transform $\\mathcal{L}\\{ \\sin(bt) \\}(s) = \\int_0^\\infty e^{-st} \\sin(bt) dt = \\frac{b}{s^2+b^2}$.\nHere, our integration variable is $r$, $s = 1/\\xi$, and $b = k$. Thus, the integral evaluates to:\n$$\n\\int_0^\\infty \\exp(-r/\\xi) \\sin(kr) \\, \\mathrm{d}r = \\frac{k}{(1/\\xi)^2 + k^2} = \\frac{k}{1/\\xi^2 + k^2} = \\frac{k\\xi^2}{1 + k^2\\xi^2}\n$$\nSubstituting this final piece into the expression for $S(k)$:\n$$\nS(k) = \\frac{4\\pi A}{k} \\left( \\frac{k\\xi^2}{1 + k^2\\xi^2} \\right) = \\frac{4\\pi A \\xi^2}{1 + k^2\\xi^2}\n$$\nThis is the required analytic expression for the structure factor, which is a Lorentzian function of $k$.\n\nNext, we compute the full width at half maximum (FWHM), denoted $\\Delta k$. The peak of the function $S(k)$ occurs at $k=0$. The maximum value is:\n$$\nS_{\\text{max}} = S(0) = \\frac{4\\pi A \\xi^2}{1 + 0^2 \\cdot \\xi^2} = 4\\pi A \\xi^2\n$$\nThe FWHM is the distance between the two values of $k$ for which $S(k)$ equals half its maximum value, i.e., $S(k) = S_{\\text{max}}/2$.\n$$\n\\frac{4\\pi A \\xi^2}{1 + k^2\\xi^2} = \\frac{1}{2} \\left( 4\\pi A \\xi^2 \\right)\n$$\nSince $A>0$ and $\\xi>0$, we can cancel the term $4\\pi A \\xi^2$ from both sides:\n$$\n\\frac{1}{1 + k^2\\xi^2} = \\frac{1}{2}\n$$\nThis implies:\n$$\n1 + k^2\\xi^2 = 2\n$$\n$$\nk^2\\xi^2 = 1\n$$\n$$\nk^2 = \\frac{1}{\\xi^2}\n$$\nThe solutions for $k$ are $k = \\pm\\frac{1}{\\xi}$. The function is at half-maximum at $k_1 = -1/\\xi$ and $k_2 = 1/\\xi$. The full width at half maximum is the distance between these two points:\n$$\n\\Delta k = k_2 - k_1 = \\frac{1}{\\xi} - \\left(-\\frac{1}{\\xi}\\right) = \\frac{2}{\\xi}\n$$\nThe two required expressions are $S(k) = \\frac{4\\pi A \\xi^2}{1+k^2\\xi^2}$ and $\\Delta k = \\frac{2}{\\xi}$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{4 \\pi A \\xi^{2}}{1 + k^{2} \\xi^{2}} & \\frac{2}{\\xi} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "While correlation functions like $\\langle u(\\mathbf{x}) u(\\mathbf{x}+\\mathbf{r}) \\rangle$ describe the statistical similarity of a field at two points, the study of complex systems like turbulent fluids often focuses on velocity *increments*, $\\delta u = u(\\mathbf{x}+\\mathbf{r}) - u(\\mathbf{x})$. The statistics of these increments are captured by structure functions, such as $D_{LL}(r) = \\langle (\\delta u_L)^2 \\rangle$. This practice problem  guides you through deriving the exact and fundamental identity that connects the correlation function to the structure function, demonstrating how these two statistical descriptions are intimately related.",
            "id": "3813209",
            "problem": "Consider a statistically homogeneous, zero-mean, incompressible velocity field $\\mathbf{u}(\\mathbf{x})$ in three-dimensional space. For a fixed separation vector $\\mathbf{r}$ with magnitude $r=|\\mathbf{r}|$ and associated unit vector $\\hat{\\mathbf{r}}=\\mathbf{r}/r$, define the longitudinal velocity component $u_{L}(\\mathbf{x})=\\mathbf{u}(\\mathbf{x})\\cdot\\hat{\\mathbf{r}}$, the two-point longitudinal correlation function $R_{LL}(r)=\\langle u_{L}(\\mathbf{x})\\,u_{L}(\\mathbf{x}+\\mathbf{r})\\rangle$, and the second-order longitudinal structure function $D_{LL}(r)=\\langle\\left[u_{L}(\\mathbf{x}+\\mathbf{r})-u_{L}(\\mathbf{x})\\right]^{2}\\rangle$, where $\\langle\\cdot\\rangle$ denotes an ensemble average. \n\nStarting from these definitions and using only the assumptions of statistical homogeneity and the linearity of the ensemble average, derive an exact identity linking $D_{LL}(r)$ to $R_{LL}(r)$ and $R_{LL}(0)$.\n\nThen, to illustrate multiscale behavior in a phenomenological model, suppose the longitudinal correlation function is given by a convex combination of two positive-definite kernels with distinct characteristic length scales $L_{1}>0$ and $L_{2}>0$:\n$$\nR_{LL}(r)=R_{LL}(0)\\left[a\\,\\exp\\!\\left(-\\left(\\frac{r}{L_{1}}\\right)^{2}\\right)+(1-a)\\,\\exp\\!\\left(-\\frac{r}{L_{2}}\\right)\\right],\n$$\nwhere $0<a<1$. Using your derived identity, compute the normalized second-order longitudinal structure function \n$$\nS(r)=\\frac{D_{LL}(r)}{2\\,R_{LL}(0)}\n$$\nas a closed-form analytic expression in terms of $r$, $a$, $L_{1}$, and $L_{2}$. Provide the final expression for $S(r)$. The final answer must be a single analytical expression. Do not substitute numerical values. The normalized quantity $S(r)$ is dimensionless, so no unit specification is needed.",
            "solution": "The problem is first validated and found to be scientifically grounded, well-posed, and objective. It is a standard exercise in the statistical analysis of fields, specifically in the context of turbulence or multiscale systems. The problem is self-contained and free of contradictions.\n\nThe first task is to derive an exact identity linking the second-order longitudinal structure function, $D_{LL}(r)$, to the two-point longitudinal correlation function, $R_{LL}(r)$. We begin with the definition of $D_{LL}(r)$:\n$$\nD_{LL}(r) = \\langle\\left[u_{L}(\\mathbf{x}+\\mathbf{r})-u_{L}(\\mathbf{x})\\right]^{2}\\rangle\n$$\nHere, $u_{L}(\\mathbf{x}) = \\mathbf{u}(\\mathbf{x})\\cdot\\hat{\\mathbf{r}}$ is the longitudinal velocity component, and $\\langle\\cdot\\rangle$ denotes the ensemble average.\n\nWe expand the squared term within the ensemble average:\n$$\nD_{LL}(r) = \\langle u_{L}(\\mathbf{x}+\\mathbf{r})^{2} - 2\\,u_{L}(\\mathbf{x}+\\mathbf{r})\\,u_{L}(\\mathbf{x}) + u_{L}(\\mathbf{x})^{2} \\rangle\n$$\nThe ensemble average is a linear operator, so we can distribute it over the terms:\n$$\nD_{LL}(r) = \\langle u_{L}(\\mathbf{x}+\\mathbf{r})^{2} \\rangle - \\langle 2\\,u_{L}(\\mathbf{x})\\,u_{L}(\\mathbf{x}+\\mathbf{r}) \\rangle + \\langle u_{L}(\\mathbf{x})^{2} \\rangle\n$$\nWe analyze each term using the provided definitions and the assumption of statistical homogeneity.\n$1$. The middle term is, by definition, twice the longitudinal correlation function:\n$$\n\\langle u_{L}(\\mathbf{x})\\,u_{L}(\\mathbf{x}+\\mathbf{r}) \\rangle = R_{LL}(r)\n$$\n$2$. The third term, $\\langle u_{L}(\\mathbf{x})^{2} \\rangle$, is the correlation of the velocity component at a point with itself. This corresponds to the correlation function at zero separation, $r=0$:\n$$\n\\langle u_{L}(\\mathbf{x})^{2} \\rangle = \\langle u_{L}(\\mathbf{x})\\,u_{L}(\\mathbf{x}+\\mathbf{0}) \\rangle = R_{LL}(0)\n$$\n$3$. For the first term, $\\langle u_{L}(\\mathbf{x}+\\mathbf{r})^{2} \\rangle$, we invoke the assumption of statistical homogeneity. This property implies that statistical averages are invariant under spatial translation. Therefore, the mean square value of the velocity component is the same at any point in space, i.e., $\\langle u_{L}(\\mathbf{y})^{2} \\rangle$ is independent of the position $\\mathbf{y}$. Setting $\\mathbf{y} = \\mathbf{x}+\\mathbf{r}$ or $\\mathbf{y} = \\mathbf{x}$ gives the same result:\n$$\n\\langle u_{L}(\\mathbf{x}+\\mathbf{r})^{2} \\rangle = \\langle u_{L}(\\mathbf{x})^{2} \\rangle = R_{LL}(0)\n$$\nSubstituting these results back into the expression for $D_{LL}(r)$:\n$$\nD_{LL}(r) = R_{LL}(0) - 2\\,R_{LL}(r) + R_{LL}(0)\n$$\nCombining terms, we arrive at the desired identity:\n$$\nD_{LL}(r) = 2 \\left[ R_{LL}(0) - R_{LL}(r) \\right]\n$$\nThis identity is a fundamental relationship in the statistical theory of homogeneous fields.\n\nThe second task is to compute the normalized second-order longitudinal structure function, $S(r)$, for a specific phenomenological model of $R_{LL}(r)$. The definition of $S(r)$ is given as:\n$$\nS(r) = \\frac{D_{LL}(r)}{2\\,R_{LL}(0)}\n$$\nSubstituting the identity we just derived for $D_{LL}(r)$:\n$$\nS(r) = \\frac{2 \\left[ R_{LL}(0) - R_{LL}(r) \\right]}{2\\,R_{LL}(0)} = 1 - \\frac{R_{LL}(r)}{R_{LL}(0)}\n$$\nNow, we use the provided multiscale model for the correlation function:\n$$\nR_{LL}(r) = R_{LL}(0)\\left[a\\,\\exp\\left(-\\left(\\frac{r}{L_{1}}\\right)^{2}\\right)+(1-a)\\,\\exp\\left(-\\frac{r}{L_{2}}\\right)\\right]\n$$\nwhere $0<a<1$ and $L_{1}, L_{2} > 0$. We compute the ratio $\\frac{R_{LL}(r)}{R_{LL}(0)}$:\n$$\n\\frac{R_{LL}(r)}{R_{LL}(0)} = a\\,\\exp\\left(-\\frac{r^{2}}{L_{1}^{2}}\\right) + (1-a)\\,\\exp\\left(-\\frac{r}{L_{2}}\\right)\n$$\nFinally, we substitute this expression into the equation for $S(r)$:\n$$\nS(r) = 1 - \\left[ a\\,\\exp\\left(-\\frac{r^{2}}{L_{1}^{2}}\\right) + (1-a)\\,\\exp\\left(-\\frac{r}{L_{2}}\\right) \\right]\n$$\nThis simplifies to the final closed-form analytic expression for $S(r)$:\n$$\nS(r) = 1 - a\\,\\exp\\left(-\\frac{r^{2}}{L_{1}^{2}}\\right) - (1-a)\\exp\\left(-\\frac{r}{L_{2}}\\right)\n$$\nThis expression represents the normalized structure function for a process characterized by a superposition of Gaussian and exponential correlations at two distinct length scales, $L_1$ and $L_2$.",
            "answer": "$$\n\\boxed{1 - a\\,\\exp\\left(-\\frac{r^{2}}{L_{1}^{2}}\\right) - (1-a)\\exp\\left(-\\frac{r}{L_{2}}\\right)}\n$$"
        },
        {
            "introduction": "Moving from theoretical models to experimental or simulation data presents a new challenge: how can we determine the underlying physical law from a noisy signal? The long-range behavior of a correlation function, for instance whether it decays exponentially or as a power-law, can have profound implications for the nature of the system. This final hands-on practice  is a capstone exercise in scientific data analysis, tasking you with implementing a statistically principled procedure using the Bayesian Information Criterion (BIC) to distinguish between these two fundamental decay models, a crucial skill for any modern researcher.",
            "id": "3813174",
            "problem": "You are given the task of constructing a statistically principled test to distinguish exponential decay from power-law decay in a temporal autocorrelation function $C(\\tau)$ of a stationary process, where $\\tau$ denotes the time lag. The test must be based on applying regression in appropriately transformed coordinates and selecting the superior model using a consistent, likelihood-based criterion. The scientific foundation should proceed from definitions of temporal correlation functions, the structure of multiplicative noise implying additive Gaussian residuals in log-transformed space, and maximum likelihood principles under the Gaussian hypothesis.\n\nYou must implement the following classification procedure for each dataset:\n- Fit an exponential decay model parameterized by $C(\\tau) = A \\exp(-\\tau/\\tau_{0})$ via a log-linear regression, which is the linear relationship $y = \\log C(\\tau)$ versus $x = \\tau$.\n- Fit a power-law decay model parameterized by $C(\\tau) = B \\tau^{-\\alpha}$ via a log-log regression, which is the linear relationship $y = \\log C(\\tau)$ versus $x = \\log \\tau$.\n- Under the assumption that the residuals in the log-transformed domain are independent and identically distributed Gaussian random variables with constant variance, derive and compute the Bayesian Information Criterion (BIC) for each regression and select the model with the lower BIC. Bayesian Information Criterion (BIC) is defined from the maximized Gaussian log-likelihood and a penalty for the number of fitted parameters, and must be computed on the log-transformed domain. For each linear regression of the form $y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$ with $n$ data points and residual variance estimated by the maximum likelihood estimator $s^2 = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2$, use $k = 2$ for the number of regression parameters $\\beta_0$ and $\\beta_1$, and the Gaussian log-likelihood $\\log L = -\\frac{n}{2}\\left[\\log(2\\pi s^2) + 1\\right]$ to obtain a numerical BIC value. State and implement any tie-breaking rules you use.\n- Use a decision rule with a threshold $\\Delta_{\\mathrm{BIC}}$: if the difference in BIC between the two models is less than a chosen threshold, declare the classification inconclusive. Otherwise, choose the model with the lower BIC as the preferred decay law. In your implementation, set the threshold to $\\Delta_{\\mathrm{BIC}} = 2.0$.\n\nYou must generate the datasets internally using deterministic pseudorandom multiplicative log-normal noise to ensure that $C(\\tau) > 0$ and that the Gaussian assumption in the log domain is plausible. Specifically, if $C_{\\mathrm{true}}(\\tau)$ is the noise-free correlation, then the observed data should be $C(\\tau) = C_{\\mathrm{true}}(\\tau) \\exp(\\epsilon)$ with $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$ independent across samples. Use a fixed random seed so that results are reproducible.\n\nImplement the above for the following test suite of parameterized datasets. In each case, generate $n$ evenly spaced lags $\\tau$ on the specified interval and draw independent noise with the given $\\sigma$ in the log domain. Use the following cases:\n- Case $1$ (exponential, happy path): $C_{\\mathrm{true}}(\\tau) = A \\exp(-\\tau/\\tau_0)$ with $A = 1.1$, $\\tau_0 = 4.0$, $n = 60$, $\\tau \\in [0.2, 10.0]$, $\\sigma = 0.05$.\n- Case $2$ (power-law, happy path): $C_{\\mathrm{true}}(\\tau) = B \\tau^{-\\alpha}$ with $B = 2.2$, $\\alpha = 1.2$, $n = 80$, $\\tau \\in [0.5, 50.0]$, $\\sigma = 0.06$.\n- Case $3$ (stretched exponential, ambiguous boundary): $C_{\\mathrm{true}}(\\tau) = A \\exp\\left[-(\\tau/\\tau_0)^{\\beta}\\right]$ with $A = 1.5$, $\\tau_0 = 7.0$, $\\beta = 0.7$, $n = 90$, $\\tau \\in [0.2, 30.0]$, $\\sigma = 0.08$. This case may not be well described by either a pure exponential or a pure power-law on the full range and is intended to test model selection behavior.\n- Case $4$ (exponential with high noise, robustness test): $C_{\\mathrm{true}}(\\tau) = A \\exp(-\\tau/\\tau_0)$ with $A = 0.9$, $\\tau_0 = 6.0$, $n = 50$, $\\tau \\in [0.5, 20.0]$, $\\sigma = 0.30$.\n- Case $5$ (power-law with few points, small-sample test): $C_{\\mathrm{true}}(\\tau) = B \\tau^{-\\alpha}$ with $B = 1.0$, $\\alpha = 0.9$, $n = 6$, $\\tau \\in [1.0, 10.0]$, $\\sigma = 0.05$.\n\nDefine the output coding for each case as an integer:\n- Output $0$ if the exponential model is selected.\n- Output $1$ if the power-law model is selected.\n- Output $2$ if the classification is inconclusive, defined by $|\\mathrm{BIC}_{\\mathrm{exp}} - \\mathrm{BIC}_{\\mathrm{pow}}| < 2.0$.\n\nYour program should produce a single line of output containing the results of all five cases as a comma-separated list enclosed in square brackets (for example, $[0,1,2,0,1]$). There are no physical units required; all quantities are dimensionless. All angles are not applicable. You must ensure that your implementation filters out any nonpositive $C(\\tau)$ before taking logarithms; since multiplicative log-normal noise is used, $C(\\tau)$ will remain strictly positive, but the check must remain in place to guarantee robustness. The program must be self-contained, reproducible, and must not read any external input.",
            "solution": "The problem of distinguishing between different functional forms of decay in physical observables, such as temporal autocorrelation functions, is a common task in data analysis across many scientific disciplines. The provided problem is scientifically grounded, well-posed, and all its components are rigorously defined. It outlines a complete statistical procedure for model selection between an exponential and a power-law decay model based on the Bayesian Information Criterion (BIC). All parameters, models, and criteria are explicitly and consistently stated. The problem is therefore valid.\n\nThe core of the task is to perform a model selection procedure on noisy data. We are given two candidate models for a temporal autocorrelation function $C(\\tau)$, where $\\tau$ is the time lag.\n\nThe first model is an exponential decay:\n$$ C(\\tau) = A \\exp\\left(-\\frac{\\tau}{\\tau_0}\\right) $$\nwhere $A$ is the amplitude and $\\tau_0$ is the characteristic decay time.\n\nThe second model is a power-law decay:\n$$ C(\\tau) = B \\tau^{-\\alpha} $$\nwhere $B$ is a scaling constant and $\\alpha$ is the decay exponent.\n\nThe methodology prescribed involves linearizing these models through a logarithmic transformation. This is motivated by the data generation process, which assumes multiplicative log-normal noise: $C_{obs}(\\tau_i) = C_{true}(\\tau_i) \\exp(\\epsilon_i)$, where $\\epsilon_i$ are independent and identically distributed (i.i.d.) random variables drawn from a Gaussian distribution $\\mathcal{N}(0, \\sigma^2)$. Taking the natural logarithm of this equation yields:\n$$ \\log C_{obs}(\\tau_i) = \\log C_{true}(\\tau_i) + \\epsilon_i $$\nThis transformation converts the multiplicative noise on $C(\\tau)$ into additive Gaussian noise on $\\log C(\\tau)$, validating the use of ordinary least squares regression in the transformed space.\n\nFor the exponential model, taking the logarithm gives a log-linear relationship:\n$$ \\log C(\\tau) = \\log A - \\frac{1}{\\tau_0} \\tau $$\nThis corresponds to a linear model $y = \\beta_0 + \\beta_1 x$, with $y = \\log C(\\tau)$, $x = \\tau$, intercept $\\beta_0 = \\log A$, and slope $\\beta_1 = -1/\\tau_0$.\n\nFor the power-law model, the logarithm yields a log-log relationship:\n$$ \\log C(\\tau) = \\log B - \\alpha \\log \\tau $$\nThis also corresponds to a linear model $y = \\beta'_0 + \\beta'_1 x$, but with different coordinates: $y = \\log C(\\tau)$, $x = \\log \\tau$, intercept $\\beta'_0 = \\log B$, and slope $\\beta'_1 = -\\alpha$.\n\nTo compare the goodness-of-fit of these two linear models, we employ the Bayesian Information Criterion (BIC). The BIC is defined as:\n$$ \\mathrm{BIC} = k \\log(n) - 2 \\log L_{\\text{max}} $$\nwhere $n$ is the number of data points, $k$ is the number of estimated parameters in the model, and $L_{\\text{max}}$ is the maximized value of the likelihood function.\n\nThe problem specifies using $k=2$ for the number of regression parameters (intercept and slope). For a linear regression with $n$ data points and assuming i.i.d. Gaussian errors with variance $\\sigma^2_e$, the maximized log-likelihood is given by:\n$$ \\log L_{\\text{max}} = -\\frac{n}{2} \\left[ \\log(2\\pi s^2) + 1 \\right] $$\nHere, $s^2$ is the maximum likelihood estimate of the error variance, calculated from the residuals $r_i = y_i - \\hat{y}_i$ of the fit:\n$$ s^2 = \\frac{1}{n} \\sum_{i=1}^{n} r_i^2 $$\nSubstituting the expression for $\\log L_{\\text{max}}$ and $k=2$ into the BIC formula, we derive the specific formula used for computation:\n$$ \\mathrm{BIC} = 2 \\log(n) - 2 \\left( -\\frac{n}{2} \\left[ \\log(2\\pi s^2) + 1 \\right] \\right) = 2 \\log(n) + n \\left[ \\log(2\\pi s^2) + 1 \\right] $$\nThis BIC value is computed for both the exponential fit ($BIC_{exp}$) and the power-law fit ($BIC_{pow}$).\n\nThe final step is the model selection based on the computed BIC values. The model with the lower BIC is preferred, as it indicates a better balance between model fit (lower residual variance) and complexity (number of parameters). A decision threshold $\\Delta_{\\mathrm{BIC}} = 2.0$ is introduced to handle ambiguity. The decision logic is as follows:\n1.  If $|\\mathrm{BIC}_{\\mathrm{exp}} - \\mathrm{BIC}_{\\mathrm{pow}}| < 2.0$, the evidence is not strong enough to favor one model over the other. The result is 'inconclusive', coded as $2$.\n2.  If $\\mathrm{BIC}_{\\mathrm{exp}} < \\mathrm{BIC}_{\\mathrm{pow}}$ (and the difference is not less than $2.0$), the exponential model is selected. This is coded as $0$.\n3.  If $\\mathrm{BIC}_{\\mathrm{pow}} < \\mathrm{BIC}_{\\mathrm{exp}}$ (and the difference is not less than $2.0$), the power-law model is selected. This is coded as $1$.\n\nThe implementation will proceed by first generating the datasets for each of the five test cases using a fixed random seed for reproducibility. For each dataset, we will perform both log-linear and log-log regressions, compute their respective BIC values using the derived formula, and apply the decision rule to classify the decay type. A necessary robustness check involves filtering out any non-positive data points before applying the logarithm, though the specified noise model ensures $C(\\tau) > 0$. Similarly, for the power-law fit, we must ensure $\\tau > 0$, which is guaranteed by the specified domains.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the model selection problem for all test cases.\n    \"\"\"\n    \n    # Set a fixed random seed for reproducibility of the entire process.\n    np.random.seed(42)\n\n    # Define the five test cases as specified in the problem statement.\n    test_cases = [\n        {\n            \"name\": \"Case 1 (exponential, happy path)\",\n            \"model\": \"exponential\",\n            \"params\": {\"A\": 1.1, \"tau0\": 4.0},\n            \"n\": 60,\n            \"tau_range\": (0.2, 10.0),\n            \"sigma\": 0.05,\n        },\n        {\n            \"name\": \"Case 2 (power-law, happy path)\",\n            \"model\": \"power_law\",\n            \"params\": {\"B\": 2.2, \"alpha\": 1.2},\n            \"n\": 80,\n            \"tau_range\": (0.5, 50.0),\n            \"sigma\": 0.06,\n        },\n        {\n            \"name\": \"Case 3 (stretched exponential, ambiguous boundary)\",\n            \"model\": \"stretched_exp\",\n            \"params\": {\"A\": 1.5, \"tau0\": 7.0, \"beta\": 0.7},\n            \"n\": 90,\n            \"tau_range\": (0.2, 30.0),\n            \"sigma\": 0.08,\n        },\n        {\n            \"name\": \"Case 4 (exponential with high noise, robustness test)\",\n            \"model\": \"exponential\",\n            \"params\": {\"A\": 0.9, \"tau0\": 6.0},\n            \"n\": 50,\n            \"tau_range\": (0.5, 20.0),\n            \"sigma\": 0.30,\n        },\n        {\n            \"name\": \"Case 5 (power-law with few points, small-sample test)\",\n            \"model\": \"power_law\",\n            \"params\": {\"B\": 1.0, \"alpha\": 0.9},\n            \"n\": 6,\n            \"tau_range\": (1.0, 10.0),\n            \"sigma\": 0.05,\n        },\n    ]\n\n    def _calculate_bic(x, y):\n        \"\"\"\n        Calculates BIC for a linear fit of y vs x.\n        \"\"\"\n        n = len(x)\n        if n  2:  # Cannot perform regression with fewer than 2 points\n            return np.inf\n\n        # Perform linear regression y = beta1*x + beta0\n        # np.polyfit returns [beta1, beta0] for degree 1\n        beta1, beta0 = np.polyfit(x, y, 1)\n        \n        # Calculate predicted y values\n        y_pred = beta1 * x + beta0\n        \n        # Calculate residuals and the ML estimate for variance\n        residuals = y - y_pred\n        s2 = np.mean(residuals**2)\n\n        # Fail-safe for a perfect fit, which is highly unlikely with noise.\n        # If s2 is zero or negative, log would be undefined. Return inf BIC.\n        if s2 = 0:\n            return np.inf\n\n        # Number of parameters k=2 (slope and intercept) as per problem spec\n        k = 2\n        \n        # Calculate the maximized log-likelihood\n        log_likelihood_max = -n/2 * (np.log(2 * np.pi * s2) + 1)\n        \n        # Calculate BIC\n        bic = k * np.log(n) - 2 * log_likelihood_max\n        \n        return bic\n\n    def _classify_decay(case_params):\n        \"\"\"\n        Generates data and classifies decay type for a single case.\n        \"\"\"\n        # Generate time lags\n        tau = np.linspace(case_params[\"tau_range\"][0], case_params[\"tau_range\"][1], case_params[\"n\"])\n        \n        # Generate true C(tau) based on the model\n        model = case_params[\"model\"]\n        params = case_params[\"params\"]\n        if model == \"exponential\":\n            C_true = params[\"A\"] * np.exp(-tau / params[\"tau0\"])\n        elif model == \"power_law\":\n            C_true = params[\"B\"] * tau**(-params[\"alpha\"])\n        elif model == \"stretched_exp\":\n            C_true = params[\"A\"] * np.exp(- (tau / params[\"tau0\"])**params[\"beta\"])\n        \n        # Generate noisy data with multiplicative log-normal noise\n        noise = np.random.normal(0.0, case_params[\"sigma\"], case_params[\"n\"])\n        C_obs = C_true * np.exp(noise)\n        \n        # Robustness check: filter out non-positive data points.\n        # This is good practice, though not expected to trigger with this noise model.\n        valid_mask = C_obs > 0\n        tau_valid = tau[valid_mask]\n        C_valid = C_obs[valid_mask]\n\n        if len(C_valid)  2:\n            return 2 # Inconclusive if not enough data\n\n        log_C = np.log(C_valid)\n        \n        # 1. Exponential model fit (log-linear)\n        # y = log(C), x = tau\n        bic_exp = _calculate_bic(tau_valid, log_C)\n\n        # 2. Power-law model fit (log-log)\n        # y = log(C), x = log(tau)\n        # Robustness check for tau > 0 for log(tau)\n        pow_mask = tau_valid > 0\n        if np.sum(pow_mask)  2:\n            bic_pow = np.inf\n        else:\n            log_tau_pow = np.log(tau_valid[pow_mask])\n            log_C_pow = log_C[pow_mask]\n            bic_pow = _calculate_bic(log_tau_pow, log_C_pow)\n        \n        # 3. Decision rule\n        delta_bic_threshold = 2.0\n        \n        if abs(bic_exp - bic_pow)  delta_bic_threshold:\n            return 2  # Inconclusive\n        elif bic_exp  bic_pow:\n            return 0  # Exponential\n        else:\n            return 1  # Power-law\n\n    results = []\n    for case in test_cases:\n        result_code = _classify_decay(case)\n        results.append(result_code)\n\n    # Print results in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}