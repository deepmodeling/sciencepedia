{
    "hands_on_practices": [
        {
            "introduction": "A fundamental starting point for understanding detailed balance is to apply it to a well-understood, reversible system. This exercise explores a symmetric random walk on a finite lattice with reflecting boundaries, a common coarse-grained model for diffusion. By directly applying the detailed balance condition, you will see how it provides a straightforward path to determining the system's invariant or equilibrium distribution, bypassing the more general global balance equations. ",
            "id": "3748914",
            "problem": "Consider a discrete-time, nearest-neighbor random walk on the finite lattice of states $\\{0,1,\\dots,N\\}$, where $N \\in \\mathbb{N}$ and $N \\geq 2$. The dynamics is defined by the transition probabilities\n$$\nP(i,i-1) = \\frac{1}{2}, \\quad P(i,i+1) = \\frac{1}{2} \\quad \\text{for all } i \\in \\{1,2,\\dots,N-1\\},\n$$\nand reflecting boundaries\n$$\nP(0,0) = \\frac{1}{2}, \\quad P(0,1) = \\frac{1}{2}, \\qquad P(N,N) = \\frac{1}{2}, \\quad P(N,N-1) = \\frac{1}{2}.\n$$\nThis random walk can be viewed as a coarse-grained model for reversible diffusion with Neumann boundary conditions and is widely used in multiscale modeling and analysis when constructing mesoscopic approximations that respect local equilibrium at interfaces. Let $\\pi$ denote an invariant distribution of this Markov chain, defined by the global balance condition $\\pi = \\pi P$, and recall that the detailed balance condition requires $\\pi(i) P(i,j) = \\pi(j) P(j,i)$ for all pairs of states $i$ and $j$.\n\nStarting only from the given transition structure and the definitions of invariant distribution and detailed balance, derive $\\pi$ and verify the detailed balance condition on all adjacent pairs of states. Report, as your final answer, the common value $c$ of the invariant probability assigned to each state, expressed in closed form in terms of $N$. No rounding is required, and no physical units are involved. Your answer must be a single analytic expression.",
            "solution": "The problem requires us to derive the invariant distribution, denoted by $\\pi$, for a discrete-time Markov chain on the state space $\\{0, 1, \\dots, N\\}$. We must start from the provided transition probabilities and the definition of detailed balance, verify that this condition holds for adjacent states, and determine the constant value of the invariant probability for each state.\n\nThe state space is $S = \\{0, 1, \\dots, N\\}$, where $N \\in \\mathbb{N}$ and $N \\geq 2$. The total number of states is $N+1$. The non-zero transition probabilities $P(i,j)$ are given as:\nFor interior states $i \\in \\{1, 2, \\dots, N-1\\}$:\n$$\nP(i, i-1) = \\frac{1}{2}, \\quad P(i, i+1) = \\frac{1}{2}\n$$\nFor the boundary state $i=0$:\n$$\nP(0, 0) = \\frac{1}{2}, \\quad P(0, 1) = \\frac{1}{2}\n$$\nFor the boundary state $i=N$:\n$$\nP(N, N) = \\frac{1}{2}, \\quad P(N, N-1) = \\frac{1}{2}\n$$\nAn invariant distribution $\\pi$ is a probability distribution that satisfies the global balance condition $\\sum_{i \\in S} \\pi(i) P(i,j) = \\pi(j)$ for all $j \\in S$. The detailed balance condition is a stronger condition given by:\n$$\n\\pi(i) P(i,j) = \\pi(j) P(j,i) \\quad \\text{for all } i, j \\in S\n$$\nIf a distribution $\\pi$ satisfies the detailed balance condition, it is guaranteed to satisfy the global balance condition and is therefore an invariant distribution. We will use this principle to find $\\pi$. Our approach is to assume that such a $\\pi$ exists and satisfies detailed balance, and then use this assumption to determine the form of $\\pi$.\n\nWe will systematically check the detailed balance condition for all pairs of adjacent states $(i, i+1)$ for $i \\in \\{0, 1, \\dots, N-1\\}$.\n\nCase 1: The boundary pair $(0, 1)$.\nThe detailed balance equation for this pair is $\\pi(0) P(0,1) = \\pi(1) P(1,0)$.\nFrom the problem statement, we have $P(0,1) = \\frac{1}{2}$ and $P(1,0) = \\frac{1}{2}$.\nSubstituting these values gives:\n$$\n\\pi(0) \\cdot \\frac{1}{2} = \\pi(1) \\cdot \\frac{1}{2}\n$$\nThis implies $\\pi(0) = \\pi(1)$.\n\nCase 2: Interior pairs $(i, i+1)$ for $i \\in \\{1, 2, \\dots, N-2\\}$.\nThe detailed balance equation is $\\pi(i) P(i,i+1) = \\pi(i+1) P(i+1,i)$.\nThe transition probabilities for these states are $P(i, i+1) = \\frac{1}{2}$ and $P(i+1, i) = \\frac{1}{2}$.\nSubstituting these values:\n$$\n\\pi(i) \\cdot \\frac{1}{2} = \\pi(i+1) \\cdot \\frac{1}{2}\n$$\nThis implies $\\pi(i) = \\pi(i+1)$ for all $i = 1, 2, \\dots, N-2$.\n\nCase 3: The boundary pair $(N-1, N)$.\nThe detailed balance equation is $\\pi(N-1) P(N-1,N) = \\pi(N) P(N,N-1)$.\nThe transition probabilities are $P(N-1, N) = \\frac{1}{2}$ and $P(N, N-1) = \\frac{1}{2}$.\nSubstituting these values:\n$$\n\\pi(N-1) \\cdot \\frac{1}{2} = \\pi(N) \\cdot \\frac{1}{2}\n$$\nThis implies $\\pi(N-1) = \\pi(N)$.\n\nCombining the results from all cases, we have a chain of equalities:\n$$\n\\pi(0) = \\pi(1) = \\pi(2) = \\dots = \\pi(N-1) = \\pi(N)\n$$\nThis demonstrates that the invariant distribution must be a uniform distribution over the state space. Let us denote the common value of the probability for each state by $c$, so $\\pi(i) = c$ for all $i \\in \\{0, 1, \\dots, N\\}$.\n\nWe have now verified the detailed balance condition for all adjacent pairs, as requested. For completeness, we should also check non-adjacent pairs and self-transitions.\nFor any non-adjacent pair $(i,j)$ where $|i-j|  1$, we have $P(i,j) = 0$ and $P(j,i) = 0$. The detailed balance condition becomes $c \\cdot 0 = c \\cdot 0$, which is trivially satisfied.\nFor self-transitions, such as at state $0$, the condition is $\\pi(0)P(0,0)=\\pi(0)P(0,0)$. With $\\pi(0)=c$ and $P(0,0)=\\frac{1}{2}$, this is $c \\cdot \\frac{1}{2} = c \\cdot \\frac{1}{2}$, which is also trivially true. The same logic applies to the self-transition at state $N$.\n\nTherefore, the uniform distribution $\\pi(i)=c$ for all $i$ satisfies the detailed balance condition for all pairs of states. Since this condition holds, $\\pi$ is indeed an invariant distribution.\n\nThe final step is to determine the value of the constant $c$. Since $\\pi$ is a probability distribution, the sum of probabilities over all states must be equal to $1$:\n$$\n\\sum_{i=0}^{N} \\pi(i) = 1\n$$\nSubstituting $\\pi(i)=c$ for all $i$:\n$$\n\\sum_{i=0}^{N} c = 1\n$$\nThe sum consists of $N+1$ identical terms, each equal to $c$. Thus, the equation becomes:\n$$\n(N+1)c = 1\n$$\nSolving for $c$, we find the value of the invariant probability for each state:\n$$\nc = \\frac{1}{N+1}\n$$\nThis is the common value of the invariant probability assigned to each state, expressed in closed form in terms of $N$. The invariant distribution is $\\pi(i) = \\frac{1}{N+1}$ for all $i \\in \\{0, 1, \\dots, N\\}$.",
            "answer": "$$\n\\boxed{\\frac{1}{N+1}}\n$$"
        },
        {
            "introduction": "While many systems at thermal equilibrium satisfy detailed balance, numerous physical and biological processes operate in non-equilibrium steady states (NESS). This practice examines a canonical model of a system driven out of equilibrium: a three-state ring with biased transition rates. You will demonstrate that breaking the symmetry between clockwise and counterclockwise rates violates detailed balance, leading to a persistent probability current, a hallmark of non-equilibrium dynamics. ",
            "id": "3748918",
            "problem": "Consider a Continuous-Time Markov Chain (CTMC) on three coarse-grained metastable states $1$, $2$, and $3$ arranged on a ring, modeling transitions between basins in a multiscale system under an isothermal environment with a single heat bath. Assume translational symmetry due to coarse-graining such that all clockwise transitions have the same rate $r  0$, specifically $k_{1,2} = r$, $k_{2,3} = r$, and $k_{3,1} = r$, and all counterclockwise transitions have the same rate $s  0$, specifically $k_{2,1} = s$, $k_{3,2} = s$, and $k_{1,3} = s$. The infinitesimal generator $L$ has off-diagonal entries $L_{i,j} = k_{i,j}$ for $i \\neq j$ and diagonal entries $L_{i,i} = -\\sum_{j \\neq i} k_{i,j}$. The stationary distribution $\\pi$ satisfies $\\pi L = 0$ with $\\sum_{i=1}^{3} \\pi_{i} = 1$.\n\nStarting from the master equation and the definition of detailed balance (namely, the condition that in equilibrium $ \\pi_{i} k_{i,j} = \\pi_{j} k_{j,i}$ for all pairs $i,j$), do the following:\n\n1. Using only these principles, establish whether the presence of equal clockwise rates $r$ by itself is compatible with detailed balance, and determine the additional condition that must hold on the counterclockwise rates for detailed balance to be satisfied. Your reasoning must not rely on any pre-packaged cycle criteria; it should follow directly from the definition stated above.\n\n2. Compute the stationary edge currents in the nonequilibrium steady state (NESS), defined for each oriented edge by $J_{i \\to j} = \\pi_{i} k_{i,j} - \\pi_{j} k_{j,i}$, with the convention that positive current is clockwise. Express your answer as a single row matrix $\\begin{pmatrix} J_{1 \\to 2}  J_{2 \\to 3}  J_{3 \\to 1} \\end{pmatrix}$ in terms of $r$ and $s$. If your final expression simplifies to an exact closed form, provide that exact form. No numerical evaluation is required.",
            "solution": "The analysis is divided into two parts as requested by the problem statement.\n\nPart 1: Condition for Detailed Balance\n\nThe principle of detailed balance requires that for a system in equilibrium with a stationary distribution $\\pi = (\\pi_1, \\pi_2, \\dots)$, the net flux between any two states $i$ and $j$ is zero. This is expressed by the condition:\n$$\n\\pi_{i} k_{i,j} = \\pi_{j} k_{j,i} \\quad \\text{for all pairs } (i,j)\n$$\nwhere $k_{i,j}$ is the transition rate from state $i$ to state $j$.\n\nFor the given three-state system on a ring, we must check this condition for the three distinct pairs of connected states: $(1,2)$, $(2,3)$, and $(3,1)$.\n\nThe given transition rates are:\nClockwise rates: $k_{1,2} = r$, $k_{2,3} = r$, $k_{3,1} = r$, where $r  0$.\nCounterclockwise rates: $k_{2,1} = s$, $k_{3,2} = s$, $k_{1,3} = s$, where $s  0$.\n\nApplying the detailed balance condition to each pair:\n1. For the pair $(1,2)$:\n$$\n\\pi_1 k_{1,2} = \\pi_2 k_{2,1} \\implies \\pi_1 r = \\pi_2 s \\implies \\pi_2 = \\pi_1 \\frac{r}{s}\n$$\n2. For the pair $(2,3)$:\n$$\n\\pi_2 k_{2,3} = \\pi_3 k_{3,2} \\implies \\pi_2 r = \\pi_3 s \\implies \\pi_3 = \\pi_2 \\frac{r}{s}\n$$\n3. For the pair $(3,1)$:\n$$\n\\pi_3 k_{3,1} = \\pi_1 k_{1,3} \\implies \\pi_3 r = \\pi_1 s\n$$\n\nFor detailed balance to hold, all three conditions must be satisfied simultaneously. We can use the first two equations to express $\\pi_2$ and $\\pi_3$ in terms of $\\pi_1$:\nFrom condition $1$, we have $\\pi_2 = \\pi_1 (r/s)$.\nSubstituting this into condition $2$, we obtain $\\pi_3 = (\\pi_1 (r/s)) (r/s) = \\pi_1 (r/s)^2$.\n\nNow, we substitute this expression for $\\pi_3$ into the third condition:\n$$\n\\left( \\pi_1 \\left(\\frac{r}{s}\\right)^2 \\right) r = \\pi_1 s\n$$\nSince the stationary probability $\\pi_1$ must be non-zero for any physically meaningful state (i.e., $\\pi_1  0$), we can divide both sides by $\\pi_1$:\n$$\n\\frac{r^3}{s^2} = s \\implies r^3 = s^3\n$$\nGiven that the rates $r$ and $s$ are real and positive numbers, the only real solution to this equation is:\n$$\nr = s\n$$\nThus, the presence of equal clockwise rates $r$ is compatible with detailed balance, but only under the specific additional condition that the counterclockwise rates $s$ are also equal to $r$. If $r \\neq s$, the system cannot satisfy detailed balance, as it would lead to a logical contradiction in the simultaneous equations above. This breaking of detailed balance is due to the presence of a net probability current cycle around the ring.\n\nPart 2: Stationary Edge Currents in the Nonequilibrium Steady State (NESS)\n\nA nonequilibrium steady state (NESS) occurs when the system has a time-independent stationary distribution $\\pi$, but detailed balance is not satisfied. This happens here when $r \\neq s$. We first need to compute the stationary distribution $\\pi = (\\pi_1, \\pi_2, \\pi_3)$.\n\nThe infinitesimal generator $L$ of the CTMC is given by:\n$$\nL = \\begin{pmatrix} -(k_{1,2} + k_{1,3})  k_{1,2}  k_{1,3} \\\\ k_{2,1}  -(k_{2,1} + k_{2,3})  k_{2,3} \\\\ k_{3,1}  k_{3,2}  -(k_{3,1} + k_{3,2}) \\end{pmatrix} = \\begin{pmatrix} -(r+s)  r  s \\\\ s  -(r+s)  r \\\\ r  s  -(r+s) \\end{pmatrix}\n$$\nThe stationary distribution $\\pi$ is the left eigenvector of $L$ corresponding to the eigenvalue $0$, satisfying $\\pi L = 0$ and the normalization condition $\\sum_i \\pi_i = 1$. The system of equations from $\\pi L = 0$ is:\n$$\n-\\pi_1(r+s) + \\pi_2 s + \\pi_3 r = 0 \\\\\n\\pi_1 r - \\pi_2(r+s) + \\pi_3 s = 0 \\\\\n\\pi_1 s + \\pi_2 r - \\pi_3(r+s) = 0\n$$\nDue to the translational symmetry of the problem setup (the rates only depend on whether the transition is clockwise or counterclockwise, not on the specific states), the generator matrix has a circulant structure. The corresponding stationary distribution must be uniform. Let us verify this by setting $\\pi_1 = \\pi_2 = \\pi_3 = c$.\nFrom the normalization condition, $\\pi_1 + \\pi_2 + \\pi_3 = 3c = 1$, which gives $c = 1/3$.\nSo, we test $\\pi_1 = \\pi_2 = \\pi_3 = 1/3$. Substituting into the first equation:\n$$\n-\\frac{1}{3}(r+s) + \\frac{1}{3} s + \\frac{1}{3} r = \\frac{1}{3}(-r - s + s + r) = 0\n$$\nThe equation holds. By symmetry, the other two equations will also hold. Therefore, the stationary distribution is $\\pi = (1/3, 1/3, 1/3)$. This result is valid for any choice of $r  0$ and $s  0$.\n\nThe stationary edge current for an oriented edge $i \\to j$ is defined as $J_{i \\to j} = \\pi_{i} k_{i,j} - \\pi_{j} k_{j,i}$. A positive current corresponds to a net clockwise flow. We calculate the currents for the three clockwise edges:\n1. Current from state $1$ to $2$:\n$$\nJ_{1 \\to 2} = \\pi_1 k_{1,2} - \\pi_2 k_{2,1} = \\frac{1}{3} r - \\frac{1}{3} s = \\frac{1}{3}(r-s)\n$$\n2. Current from state $2$ to $3$:\n$$\nJ_{2 \\to 3} = \\pi_2 k_{2,3} - \\pi_3 k_{3,2} = \\frac{1}{3} r - \\frac{1}{3} s = \\frac{1}{3}(r-s)\n$$\n3. Current from state $3$ to $1$:\n$$\nJ_{3 \\to 1} = \\pi_3 k_{3,1} - \\pi_1 k_{1,3} = \\frac{1}{3} r - \\frac{1}{3} s = \\frac{1}{3}(r-s)\n$$\nAs expected in a NESS with a single cycle, the net current is constant along the cycle. The value $J = \\frac{1}{3}(r-s)$ quantifies the net probability flow around the ring. If $r  s$, there is a net clockwise current. If $s  r$, there is a net counterclockwise current (negative $J$). If $r=s$, the current is zero, and we recover the equilibrium case where detailed balance holds.\n\nThe final answer is the row matrix of these currents.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{3}(r-s)  \\frac{1}{3}(r-s)  \\frac{1}{3}(r-s) \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "In practical multiscale modeling, transition kernels are often estimated from empirical data and may not automatically satisfy physical constraints like detailed balance. This advanced exercise introduces a powerful technique to enforce reversibility by projecting an arbitrary kernel onto the nearest valid reversible one. The problem elegantly connects the detailed balance condition to the concept of self-adjointness in a weighted Hilbert space, $L^2(\\pi)$, and formulates the projection as a constrained convex optimization problem. ",
            "id": "3748963",
            "problem": "Consider a finite-state discrete-time Markov process on a state space of size $n$, with an estimated transition kernel represented by a matrix $K \\in \\mathbb{R}^{n \\times n}$ and a strictly positive stationary distribution vector $\\pi \\in \\mathbb{R}^{n}$ satisfying $\\sum_{i=1}^{n} \\pi_{i} = 1$ and $\\pi_{i}  0$ for all $i$. A Markov kernel $P \\in \\mathbb{R}^{n \\times n}$ is said to be reversible with respect to $\\pi$ if it satisfies the detailed balance condition, namely $ \\pi_{i} P_{ij} = \\pi_{j} P_{ji} $ for all $i,j$, and the Markov constraints, namely $P_{ij} \\geq 0$ and $\\sum_{j=1}^{n} P_{ij} = 1$ for all $i$. Let $L^{2}(\\pi)$ denote the Hilbert space of real-valued functions on the state space with inner product $\\langle f, g \\rangle_{\\pi} = \\sum_{i=1}^{n} \\pi_{i} f(i) g(i)$.\n\nStarting from the core definitions of the Hilbert space $L^{2}(\\pi)$ and the notion of the adjoint of a linear operator with respect to $\\langle \\cdot, \\cdot \\rangle_{\\pi}$, derive a principled construction that projects the given estimated kernel $K$ onto the nearest reversible kernel $P$ with respect to the $L^{2}(\\pi)$-induced squared distance on operators, under the Markov constraints. Formulate this as a constrained optimization problem whose objective is the squared $L^{2}(\\pi)$ distance between $K$ and $P$ viewed as linear operators on $L^{2}(\\pi)$, and whose feasible set enforces the detailed balance condition together with the Markov constraints.\n\nYour task is to:\n- Use the base definitions of adjoint operators in $L^{2}(\\pi)$ and the characterization of reversibility via self-adjointness to motivate the projection concept.\n- Construct a convex optimization problem over $P \\in \\mathbb{R}^{n \\times n}$ that minimizes the squared distance induced by $L^{2}(\\pi)$ while enforcing detailed balance and the Markov constraints.\n- Implement a robust numerical algorithm to solve the optimization for the given test suite.\n- Verify the solution satisfies detailed balance and the Markov constraints to within a numerical tolerance.\n\nThe squared $L^{2}(\\pi)$ operator distance between $K$ and $P$ should be computed as the weighted Frobenius norm with row weights given by $\\pi$, namely $$\\sum_{i=1}^{n} \\pi_{i} \\sum_{j=1}^{n} \\left(P_{ij} - K_{ij}\\right)^{2}.$$\n\nDesign the optimization so that it is convex, with linear equality and inequality constraints expressing detailed balance and the Markov constraints. The final program must solve the optimization for each test case in the suite below and report, for each case, the minimal objective value (as a float) and booleans indicating whether the solution satisfies detailed balance, the row-stochastic property, and nonnegativity within a tolerance of $10^{-8}$.\n\nTest suite:\n1. $n = 2$, $\\pi = [0.6, 0.4]$, $K = \\begin{bmatrix} 0.9  0.1 \\\\ 0.2  0.8 \\end{bmatrix}$.\n2. $n = 2$, $\\pi = [0.01, 0.99]$, $K = \\begin{bmatrix} 0.7  0.3 \\\\ 0.4  0.6 \\end{bmatrix}$.\n3. $n = 3$, $\\pi = [0.2, 0.5, 0.3]$, $K = \\begin{bmatrix} 0.2  0.5  0.3 \\\\ 0.3  0.4  0.3 \\\\ 0.5  0.1  0.4 \\end{bmatrix}$.\n4. $n = 3$, $\\pi = [0.1, 0.3, 0.6]$, $K = P_{\\mathrm{rev}}$ where $P_{\\mathrm{rev}}$ has rows identical to $\\pi$, i.e., $P_{\\mathrm{rev}, ij} = \\pi_{j}$ for all $i,j$.\n\nFor each test case, compute the optimal reversible kernel $P^{\\star}$ and report the following per-case tuple:\n- The minimized objective value $\\sum_{i=1}^{n} \\pi_{i} \\sum_{j=1}^{n} \\left(P^{\\star}_{ij} - K_{ij}\\right)^{2}$ as a float.\n- A boolean indicating whether the detailed balance residual $\\max_{i,j} \\left| \\pi_{i} P^{\\star}_{ij} - \\pi_{j} P^{\\star}_{ji} \\right|$ is less than $10^{-8}$.\n- A boolean indicating whether the row-stochastic residual $\\max_{i} \\left| \\sum_{j=1}^{n} P^{\\star}_{ij} - 1 \\right|$ is less than $10^{-8}$.\n- A boolean indicating whether $\\min_{i,j} P^{\\star}_{ij} \\geq -10^{-10}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list of per-case tuples enclosed in square brackets (e.g., \"[[obj1,flag_db1,flag_row1,flag_nonneg1],[obj2,flag_db2,flag_row2,flag_nonneg2],...]\"). No physical units are involved. Angles do not appear. All boolean indicators must be printed in their native boolean form. The program must take no input and must be self-contained.",
            "solution": "The problem requires us to find a transition kernel $P$ that is reversible with respect to a stationary distribution $\\pi$ and is closest to a given estimated kernel $K$. The distance is measured by a weighted squared Frobenius norm induced by the Hilbert space $L^2(\\pi)$. This is a constrained optimization problem, which we will formulate and solve.\n\n**1. Theoretical Foundation: Reversibility and Self-Adjointness**\n\nLet the finite state space be $\\mathcal{S} = \\{1, 2, \\dots, n\\}$. The Hilbert space $L^2(\\pi)$ consists of real-valued functions on $\\mathcal{S}$, which can be represented by vectors in $\\mathbb{R}^n$. The inner product on this space is defined as:\n$$ \\langle f, g \\rangle_{\\pi} = \\sum_{i=1}^{n} \\pi_i f(i) g(i) $$\nwhere $f, g \\in \\mathbb{R}^n$ are functions on $\\mathcal{S}$, and $\\pi \\in \\mathbb{R}^n$ is a strictly positive probability distribution, $\\pi_i  0$ for all $i$.\n\nA transition kernel $P \\in \\mathbb{R}^{n \\times n}$ acts as a linear operator on functions in $L^2(\\pi)$, with the action defined as $(Pf)(i) = \\sum_{j=1}^{n} P_{ij} f(j)$. The adjoint operator $P^*$ of $P$ with respect to the $\\langle \\cdot, \\cdot \\rangle_{\\pi}$ inner product is uniquely defined by the relation $\\langle Pf, g \\rangle_{\\pi} = \\langle f, P^*g \\rangle_{\\pi}$ for all $f, g \\in L^2(\\pi)$.\n\nWe can derive the matrix representation of $P^*$. Let's expand both sides of the defining relation:\n$$ \\langle Pf, g \\rangle_{\\pi} = \\sum_{i=1}^{n} \\pi_i (Pf)(i) g(i) = \\sum_{i=1}^{n} \\pi_i \\left( \\sum_{j=1}^{n} P_{ij} f(j) \\right) g(i) = \\sum_{i,j} \\pi_i P_{ij} g(i) f(j) $$\n$$ \\langle f, P^*g \\rangle_{\\pi} = \\sum_{j=1}^{n} \\pi_j f(j) (P^*g)(j) = \\sum_{j=1}^{n} \\pi_j f(j) \\left( \\sum_{i=1}^{n} (P^*)_{ji} g(i) \\right) = \\sum_{i,j} \\pi_j (P^*)_{ji} g(i) f(j) $$\nFor these expressions to be equal for all functions $f$ and $g$, the coefficients of $f(j)g(i)$ must be equal for all $i,j$. This yields:\n$$ \\pi_i P_{ij} = \\pi_j (P^*)_{ji} $$\nFrom this, we can express the entries of the adjoint matrix $P^*$ in terms of $P$:\n$$ (P^*)_{ij} = \\frac{\\pi_j}{\\pi_i} P_{ji} $$\nAn operator $P$ is self-adjoint in $L^2(\\pi)$ if $P = P^*$. Equating the matrix elements, $P_{ij} = (P^*)_{ij}$, we get:\n$$ P_{ij} = \\frac{\\pi_j}{\\pi_i} P_{ji} \\implies \\pi_i P_{ij} = \\pi_j P_{ji} $$\nThis is precisely the detailed balance condition. Therefore, a transition kernel $P$ is reversible with respect to $\\pi$ if and only if it is a self-adjoint operator on the Hilbert space $L^2(\\pi)$.\n\n**2. Constrained Optimization Problem Formulation**\n\nThe problem is to find the reversible kernel $P$ that is \"nearest\" to a given kernel $K$. The distance is the weighted squared Frobenius norm, which is the objective function to minimize:\n$$ \\mathcal{D}(P, K) = \\sum_{i=1}^{n} \\pi_i \\sum_{j=1}^{n} (P_{ij} - K_{ij})^2 $$\nThis minimization must be performed subject to constraints that ensure $P$ is a valid reversible transition kernel.\n\nThe constraints are:\n1.  **Detailed Balance (Self-Adjointness):** $\\pi_i P_{ij} - \\pi_j P_{ji} = 0$ for all $i, j \\in \\{1, \\dots, n\\}$. These are linear equality constraints.\n2.  **Row Stochasticity:** $\\sum_{j=1}^{n} P_{ij} = 1$ for all $i \\in \\{1, \\dots, n\\}$. These are also linear equality constraints.\n3.  **Non-negativity:** $P_{ij} \\ge 0$ for all $i, j \\in \\{1, \\dots, n\\}$. These are linear inequality constraints.\n\nThe objective function is a strictly convex quadratic function of the entries of $P$. The feasible set defined by the linear equality and inequality constraints is a convex set. The problem of minimizing a strictly convex function over a non-empty, closed, convex set has a unique solution. This formulation is a Quadratic Program (QP).\n\nThe full optimization problem is stated as:\n$$ \\underset{P \\in \\mathbb{R}^{n \\times n}}{\\text{minimize}} \\quad \\sum_{i=1}^{n} \\pi_i \\sum_{j=1}^{n} (P_{ij} - K_{ij})^2 $$\nSubject to:\n$$ \\begin{cases} \\pi_i P_{ij} - \\pi_j P_{ji} = 0,  \\forall i,j \\in \\{1, \\dots, n\\} \\\\ \\sum_{j=1}^{n} P_{ij} = 1,  \\forall i \\in \\{1, \\dots, n\\} \\\\ P_{ij} \\ge 0,  \\forall i,j \\in \\{1, \\dots, n\\} \\end{cases} $$\n\n**3. Numerical Solution Strategy**\n\nWe will solve this QP numerically using the `scipy.optimize.minimize` function with the Sequential Least Squares Programming (`SLSQP`) method, which is well-suited for such problems.\n\nFirst, we vectorize the decision variable $P \\in \\mathbb{R}^{n \\times n}$ into a vector $x \\in \\mathbb{R}^{n^2}$, such that $x_{i \\cdot n + j} = P_{ij}$.\n\nThe objective function and its gradient (Jacobian) are provided to the solver:\n-   **Objective:** $f(x) = \\sum_{i=0}^{n-1} \\pi_i \\sum_{j=0}^{n-1} (x_{i \\cdot n + j} - K_{ij})^2$\n-   **Gradient:** $\\frac{\\partial f}{\\partial x_{i \\cdot n + j}} = 2 \\pi_i (P_{ij} - K_{ij})$\n\nThe constraints are reformulated for the solver:\n-   **Linear Equality Constraints:** The detailed balance and row-stochasticity constraints can be written in the matrix form $A_{\\text{eq}} x = b_{\\text{eq}}$. We construct the matrix $A_{\\text{eq}} \\in \\mathbb{R}^{(n + n(n-1)/2) \\times n^2}$ and vector $b_{\\text{eq}} \\in \\mathbb{R}^{n + n(n-1)/2}$.\n    -   For the $n$ row-sum constraints $\\sum_j P_{ij} = 1$, the corresponding rows in $A_{\\text{eq}}$ have entries equal to $1$ for columns associated with $P_{ij}$ (for a fixed $i$), and the corresponding entry in $b_{\\text{eq}}$ is $1$.\n    -   For the $n(n-1)/2$ independent detailed balance constraints $\\pi_i P_{ij} - \\pi_j P_{ji} = 0$ (for $ij$), the corresponding rows in $A_{\\text{eq}}$ have an entry $\\pi_i$ at the column for $P_{ij}$ and $-\\pi_j$ at the column for $P_{ji}$. The corresponding entry in $b_{\\text{eq}}$ is $0$.\n-   **Bounds:** The non-negativity constraint $P_{ij} \\ge 0$ is handled by setting a lower bound of $0$ for all variables in $x$. An upper bound of $1$ is also natural and can be applied.\n\nThese components—objective function, gradient, linear constraints, and bounds—are passed to the `SLSQP` solver to find the optimal vectorized kernel $x^*$, which is then reshaped into the final matrix $P^*$. The solution is then verified against the problem's criteria for correctness.",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize, Bounds, LinearConstraint\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run the optimization for each,\n    and print the formatted results.\n    \"\"\"\n    # Test suite definition from the problem statement.\n    test_cases = [\n        (2, [0.6, 0.4], [[0.9, 0.1], [0.2, 0.8]]),\n        (2, [0.01, 0.99], [[0.7, 0.3], [0.4, 0.6]]),\n        (3, [0.2, 0.5, 0.3], [[0.2, 0.5, 0.3], [0.3, 0.4, 0.3], [0.5, 0.1, 0.4]]),\n        (3, [0.1, 0.3, 0.6], [[0.1, 0.3, 0.6], [0.1, 0.3, 0.6], [0.1, 0.3, 0.6]])\n    ]\n\n    results = []\n    for n, pi_list, K_list in test_cases:\n        pi = np.array(pi_list)\n        K = np.array(K_list)\n        result_tuple = run_optimization_and_verify(n, pi, K)\n        results.append(result_tuple)\n\n    # Format the final output string to match the problem specification\n    # e.g., [[obj1,True,True,True],[obj2,True,True,True]].\n    # Using str() on a list of lists adds spaces, so we remove them.\n    final_output = str([list(r) for r in results]).replace(\" \", \"\")\n    print(final_output)\n\ndef run_optimization_and_verify(n, pi, K):\n    \"\"\"\n    Solves the constrained optimization problem for a single test case and verifies the result.\n\n    Args:\n        n (int): The size of the state space.\n        pi (np.ndarray): The stationary distribution vector.\n        K (np.ndarray): The initial estimated kernel matrix.\n\n    Returns:\n        tuple: A tuple containing the objective value and three boolean verification flags.\n    \"\"\"\n    num_vars = n * n\n\n    # 1. Define the objective function and its Jacobian (gradient).\n    # The variable x is the flattened version of the matrix P.\n    def objective_func(x, pi_vec, K_mat):\n        P = x.reshape((n, n))\n        diff = P - K_mat\n        return np.sum(pi_vec[:, np.newaxis] * (diff**2))\n\n    def objective_jac(x, pi_vec, K_mat):\n        P = x.reshape((n, n))\n        grad_P = 2 * pi_vec[:, np.newaxis] * (P - K_mat)\n        return grad_P.flatten()\n\n    # 2. Define the linear equality constraints A*x = b.\n    num_rs_cons = n\n    num_db_cons = n * (n - 1) // 2\n    num_eq_cons = num_rs_cons + num_db_cons\n\n    A_eq = np.zeros((num_eq_cons, num_vars))\n    b_eq = np.zeros(num_eq_cons)\n\n    # Part 1: Row-stochastic constraints: sum_j(P_ij) = 1 for each i.\n    for i in range(n):\n        for j in range(n):\n            A_eq[i, i * n + j] = 1.0\n        b_eq[i] = 1.0\n\n    # Part 2: Detailed balance constraints: pi_i * P_ij - pi_j * P_ji = 0 for i  j.\n    row_idx = n\n    for i in range(n):\n        for j in range(i + 1, n):\n            A_eq[row_idx, i * n + j] = pi[i]\n            A_eq[row_idx, j * n + i] = -pi[j]\n            # b_eq is already zero for these rows, so no need to set b_eq[row_idx] = 0.\n            row_idx += 1\n    \n    linear_constraints = LinearConstraint(A_eq, b_eq, b_eq)\n\n    # 3. Define the bounds for the variables (P_ij = 0).\n    # As P_ij are probabilities, they are also bounded by 1.\n    bounds = Bounds(np.zeros(num_vars), np.ones(num_vars))\n\n    # 4. Set initial guess and run the optimizer.\n    x0 = K.flatten()\n    \n    res = minimize(\n        fun=objective_func,\n        x0=x0,\n        args=(pi, K),\n        method='SLSQP',\n        jac=objective_jac,\n        constraints=[linear_constraints],\n        bounds=bounds,\n        tol=1e-12,\n        options={'maxiter': 1000, 'disp': False}\n    )\n\n    # 5. Extract and verify the solution.\n    P_star = res.x.reshape((n, n))\n    obj_val = res.fun\n    \n    # Verification check 1: Detailed balance residual\n    db_residual = np.max(np.abs(pi[:, np.newaxis] * P_star - (pi[:, np.newaxis] * P_star).T))\n    db_ok = db_residual  1e-8\n    \n    # Verification check 2: Row-stochastic residual\n    row_sum_residual = np.max(np.abs(np.sum(P_star, axis=1) - 1))\n    row_ok = row_sum_residual  1e-8\n\n    # Verification check 3: Non-negativity\n    min_p_val = np.min(P_star)\n    nonneg_ok = min_p_val = -1e-10\n\n    return (obj_val, db_ok, row_ok, nonneg_ok)\n\nsolve()\n```"
        }
    ]
}