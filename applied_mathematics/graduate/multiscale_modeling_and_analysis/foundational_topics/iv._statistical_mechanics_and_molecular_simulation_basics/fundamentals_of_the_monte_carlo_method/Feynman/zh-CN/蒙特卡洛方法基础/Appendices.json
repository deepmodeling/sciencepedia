{
    "hands_on_practices": [
        {
            "introduction": "当我们无法直接从目标概率分布中采样时，接受-拒绝采样法提供了一种巧妙的解决方案。这项练习将引导你使用一个更简单的“提议分布”来设计采样器，并通过推导其接受率来量化其效率 。通过这个过程，你将加深对该方法基本原理的理解，这是蒙特卡洛工具箱中的一项基本技能。",
            "id": "3762695",
            "problem": "考虑一个正随机变量，其在 $(0,\\infty)$ 上的目标密度与 $x^{k-1}\\exp(-x)$ 成正比，其中 $k>0$ 是一个已知的形状参数。在多尺度建模和分析中，常常需要高效地对此类变量作为潜在尺度参数进行采样。要求你设计一个基于伽马提议分布的接受-拒绝采样方案，并从基本定义出发计算其接受率。\n  \n出发点：\n- 形状参数为 $k>0$、速率参数为 $\\lambda>0$ 的伽马分布，其密度函数对于 $x>0$ 为 $g(x)=\\frac{\\lambda^{k}}{\\Gamma(k)}x^{k-1}\\exp(-\\lambda x)$，其中 $\\Gamma(k)$ 是伽马函数。\n- 接受-拒绝法从一个提议密度 $g(x)$ 中抽取一个提议样本 $Y$，并以概率 $\\min\\{1, f(Y)/(M g(Y))\\}$ 接受它，其中 $f(x)$ 是归一化的目标密度，且 $M\\geq \\sup_{x>0} f(x)/g(x)$ 是一个有限界，确保对于所有 $x>0$ 都有 $f(x) \\leq M g(x)$。平均接受率等于在提议分布下接受概率的均值。\n\n任务：\n1. 归一化目标密度以得到 $f(x)$，并选择一个提议分布族 $g_{\\beta}(x)$，使其为具有*相同*形状参数 $k$ 和速率参数 $\\beta \\in (0,1]$ 的伽马分布。通过确保存在有限的包络常数 $M(k,\\beta)$ 来证明此选择的合理性。\n2. 通过分析 $\\sup_{x>0} f(x)/g_{\\beta}(x)$，从第一性原理出发推导最小包络常数 $M(k,\\beta)$。\n3. 使用接受-拒绝法的定义，推导平均接受率作为 $k$ 和 $\\beta$ 的函数的封闭形式表达式。\n4. 给出接受率的简化后的最终表达式。\n\n你的最终答案必须是一个关于 $k$ 和 $\\beta$ 的单一封闭形式解析表达式。不需要四舍五入，也不应包含任何单位。",
            "solution": "我们首先确定归一化的目标分布和提议分布。给定的在 $(0,\\infty)$ 上的未归一化目标密度与 $x^{k-1}\\exp(-x)$ 成正比（其中 $k>0$）。相应的归一化目标密度是形状参数为 $k$、速率参数为 $1$ 的伽马分布：\n$$\nf(x) \\;=\\; \\frac{1}{\\Gamma(k)}\\, x^{k-1}\\exp(-x), \\quad x>0.\n$$\n我们提出一个具有相同形状参数 $k$ 和速率参数 $\\beta \\in (0,1]$ 的伽马分布作为提议分布：\n$$\ng_{\\beta}(x) \\;=\\; \\frac{\\beta^{k}}{\\Gamma(k)}\\, x^{k-1}\\exp(-\\beta x), \\quad x>0.\n$$\n\n接受-拒绝法要求存在一个有限的包络常数 $M(k,\\beta)$，使得对于所有 $x>0$ 都满足 $f(x) \\leq M(k,\\beta) g_{\\beta}(x)$。等价地，我们需要 $M(k,\\beta) \\geq \\sup_{x>0} \\frac{f(x)}{g_{\\beta}(x)}$。我们来计算这个上确界。使用 $f$ 和 $g_{\\beta}$ 的表达式，\n$$\n\\frac{f(x)}{g_{\\beta}(x)} \n=\\frac{\\frac{1}{\\Gamma(k)} x^{k-1}\\exp(-x)}{\\frac{\\beta^{k}}{\\Gamma(k)} x^{k-1}\\exp(-\\beta x)}\n=\\beta^{-k}\\exp\\!\\bigl(-x + \\beta x\\bigr)\n=\\beta^{-k}\\exp\\!\\bigl(-(1-\\beta)x\\bigr).\n$$\n我们现在考察当 $\\beta \\in (0,1]$ 时，$\\beta^{-k}\\exp\\!\\bigl(-(1-\\beta)x\\bigr)$ 在 $x>0$ 上的行为：\n- 如果 $\\beta=1$，那么对于所有 $x>0$，我们有 $\\frac{f(x)}{g_{\\beta}(x)} = 1$，从而得到最小包络常数 $M(k,1)=1$。\n- 如果 $\\beta \\in (0,1)$，那么 $(1-\\beta)>0$，因此 $\\exp\\!\\bigl(-(1-\\beta)x\\bigr)$ 是关于 $x$ 的递减函数，并在 $x=0$ 处达到其上确界，值为 $1$。因此，\n$$\n\\sup_{x>0} \\frac{f(x)}{g_{\\beta}(x)}\n=\\beta^{-k} \\cdot \\sup_{x>0}\\exp\\!\\bigl(-(1-\\beta)x\\bigr)\n=\\beta^{-k}\\cdot 1\n=\\beta^{-k}.\n$$\n因此，对于 $\\beta \\in (0,1]$，最小包络常数为\n$$\nM(k,\\beta)=\\beta^{-k}.\n$$\n为了完整起见，请注意，如果 $\\beta>1$，则 $(1-\\beta)0$，且当 $x\\to\\infty$ 时 $\\exp\\!\\bigl(-(1-\\beta)x\\bigr)=\\exp\\!\\bigl((\\beta-1)x\\bigr)$ 无界增长，这意味着 $\\sup_{x0} \\frac{f(x)}{g_{\\beta}(x)}=+\\infty$，使得接受-拒绝方案无效。这证明了限制 $\\beta\\in(0,1]$ 的合理性。\n\n确定了 $M(k,\\beta)$ 后，接受-拒绝算法如下：\n- 独立地从 $g_{\\beta}(y)$ 中抽取样本 $Y$ 和从 $\\mathrm{Uniform}(0,1)$ 中抽取样本 $U$。\n- 如果满足\n$$\nU \\leq \\frac{f(Y)}{M(k,\\beta) g_{\\beta}(Y)}.\n$$\n则接受 $Y$。\n使用推导出的形式，\n$$\n\\frac{f(Y)}{M(k,\\beta) g_{\\beta}(Y)}\n=\\frac{\\beta^{-k}\\exp\\!\\bigl(-(1-\\beta)Y\\bigr)}{\\beta^{-k}}\n=\\exp\\!\\bigl(-(1-\\beta)Y\\bigr).\n$$\n因此，接受准则简化为\n$$\nU \\leq \\exp\\!\\bigl(-(1-\\beta)Y\\bigr).\n$$\n\n我们现在计算平均接受率，即在提议分布 $g_{\\beta}$ 下接受概率的期望值。根据接受-拒绝法的定义，\n$$\n\\text{Acceptance rate} \\;=\\; \\int_{0}^{\\infty} g_{\\beta}(x)\\, \\frac{f(x)}{M(k,\\beta) g_{\\beta}(x)}\\, dx\n\\;=\\; \\frac{1}{M(k,\\beta)} \\int_{0}^{\\infty} f(x)\\, dx\n\\;=\\; \\frac{1}{M(k,\\beta)},\n$$\n因为 $f$ 是一个归一化的密度函数，所以 $\\int_{0}^{\\infty} f(x)\\, dx = 1$。使用 $M(k,\\beta)=\\beta^{-k}$，我们得到\n$$\n\\text{Acceptance rate} \\;=\\; \\beta^{k}.\n$$\n作为使用显式接受概率的交叉检验，请注意\n$$\n\\int_{0}^{\\infty} g_{\\beta}(x)\\, \\exp\\!\\bigl(-(1-\\beta)x\\bigr)\\, dx\n= \\int_{0}^{\\infty} \\frac{\\beta^{k}}{\\Gamma(k)} x^{k-1} \\exp\\!\\bigl(-\\beta x\\bigr)\\exp\\!\\bigl(-(1-\\beta)x\\bigr)\\, dx\n= \\frac{\\beta^{k}}{\\Gamma(k)} \\int_{0}^{\\infty} x^{k-1}\\exp(-x)\\, dx\n= \\frac{\\beta^{k}}{\\Gamma(k)} \\Gamma(k)\n= \\beta^{k},\n$$\n从而证实了接受率。\n\n因此，对于具有相同形状参数 $k$ 和速率参数 $\\beta \\in (0,1]$ 的伽马提议分布 $g_{\\beta}$，接受-拒绝方案的接受率为 $\\beta^{k}$。选择 $\\beta=1$ 会得到接受率 $1$，这相当于直接从目标分布中采样，而任何 $\\beta \\in (0,1)$ 都会得到严格小于 $1$ 的接受率，并且随着 $\\beta$ 的减小，接受率在 $(0,1)$ 上单调递减。",
            "answer": "$$\\boxed{\\beta^{k}}$$"
        },
        {
            "introduction": "对于高维或复杂联合分布，直接采样或接受-拒绝采样往往不可行，而马尔可夫链蒙特卡洛（MCMC）方法为此提供了强大的解决方案。这项练习将让你为经典的二元正态模型实现吉布斯采样器，这是一个核心的MCMC算法 。你将从联合密度中推导出必要的条件分布，并评估生成样本链的自相关性，从而获得MCMC方法探索参数空间的实践经验。",
            "id": "3762723",
            "problem": "考虑一个目标二元正态分布，其均值向量和协方差矩阵已知。设随机向量为 $X = (X_1, X_2)^\\top$，其均值为 $\\mu = (\\mu_1, \\mu_2)^\\top$，协方差矩阵为\n$$\n\\Sigma = \\begin{pmatrix}\n\\sigma_{11}  \\sigma_{12} \\\\\n\\sigma_{12}  \\sigma_{22}\n\\end{pmatrix},\n$$\n其中 $\\Sigma$ 是对称正定矩阵。本问题的基础是多元正态分布的联合密度函数：对于 $d=2$ 的情况，\n$$\np(x) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma|^{1/2}} \\exp\\left( -\\frac{1}{2} (x - \\mu)^\\top \\Sigma^{-1} (x - \\mu) \\right),\n$$\n其中 $x = (x_1, x_2)^\\top$。\n\n任务：\n1. 从上述联合密度定义出发，仅使用标准的代数变换（如配方法和正定矩阵的性质），推导该二元正态模型的条件分布 $X_1 \\mid X_2$ 和 $X_2 \\mid X_1$。不要使用已有的条件正态分布的快捷公式；需展示条件均值和方差是如何从联合密度中推导出来的。\n2. 使用推导出的条件分布，实现一个双分量 Gibbs 采样器，交替更新 $X_1$ 和 $X_2$。具体来说，在第 $t$ 次迭代中，根据当前的 $X_2^{(t-1)}$ 从 $X_1$ 的条件分布中采样 $X_1^{(t)}$，然后根据新更新的 $X_1^{(t)}$ 从 $X_2$ 的条件分布中采样 $X_2^{(t)}$。将 $X^{(0)}$ 初始化为均值 $\\mu$。为了可复现性，使用固定的伪随机数生成器种子 $42$。\n3. 在舍弃 $B$ 次“预烧期”（burn-in）迭代后，为每个坐标序列收集 $T$ 个连续样本。对于一个序列 $Y_1, Y_2, \\dots, Y_T$，定义样本均值 $\\bar{Y} = \\frac{1}{T}\\sum_{t=1}^T Y_t$，滞后 $k$ 阶的样本自协方差为\n$$\n\\hat{\\gamma}(k) = \\frac{1}{T} \\sum_{t=k+1}^T (Y_t - \\bar{Y})(Y_{t-k} - \\bar{Y}),\n$$\n以及滞后-1 自相关为\n$$\n\\hat{\\rho}(1) = \\frac{\\hat{\\gamma}(1)}{\\hat{\\gamma}(0)}。\n$$\n使用此定义分别为每个坐标序列计算 $\\hat{\\rho}(1)$。\n\n测试集：\n使用以下四组参数集，每组由 $(\\mu, \\Sigma, B, T)$ 指定，初始化 $X^{(0)} = \\mu$，并使用固定种子 $42$：\n- 情况 A：$\\mu = (0, 0)^\\top$, $\\Sigma = \\begin{pmatrix} 1  0.9 \\\\ 0.9  1 \\end{pmatrix}$, $B = 5000$, $T = 30000$。\n- 情况 B：$\\mu = (0, 0)^\\top$, $\\Sigma = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}$, $B = 5000$, $T = 30000$。\n- 情况 C：$\\mu = (1, -2)^\\top$, $\\Sigma = \\begin{pmatrix} 4  -3.6 \\\\ -3.6  9 \\end{pmatrix}$, $B = 5000$, $T = 30000$。\n- 情况 D：$\\mu = (0, 0)^\\top$, $\\Sigma = \\begin{pmatrix} 1  0.999 \\\\ 0.999  1 \\end{pmatrix}$, $B = 5000$, $T = 30000$。\n\n输出规格：\n您的程序应生成单行输出，包含八个浮点数，对应于滞后-1 自相关系数，按情况和每个情况内的坐标排序。顺序为\n$$\n[\\hat{\\rho}_1^{A}(1), \\hat{\\rho}_2^{A}(1), \\hat{\\rho}_1^{B}(1), \\hat{\\rho}_2^{B}(1), \\hat{\\rho}_1^{C}(1), \\hat{\\rho}_2^{C}(1), \\hat{\\rho}_1^{D}(1), \\hat{\\rho}_2^{D}(1)],\n$$\n其中 $\\hat{\\rho}_j^{\\text{Case}}(1)$ 表示指定情况下坐标 $j$ 的滞后-1 自相关。每个数字表示为四舍五入到六位小数。您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果（例如 $[r_1, r_2, \\dots, r_8]$）。本问题不涉及物理单位或角度单位；所有量均为无量纲实数。",
            "solution": "该问题要求推导二元正态随机向量的条件分布，基于这些条件分布实现 Gibbs 采样器，并计算所得样本链的滞后-1 自相关。解答过程分为三部分，对应于问题陈述中列出的三个任务。\n\n### 任务 1：条件分布的推导\n\n设二元随机向量为 $X = (X_1, X_2)^\\top$，其均值向量为 $\\mu = (\\mu_1, \\mu_2)^\\top$，协方差矩阵 $\\Sigma$ 为对称正定矩阵：\n$$\n\\Sigma = \\begin{pmatrix} \\sigma_{11}  \\sigma_{12} \\\\ \\sigma_{12}  \\sigma_{22} \\end{pmatrix}\n$$\n其联合概率密度函数 (PDF) 由下式给出：\n$$\np(x_1, x_2) = \\frac{1}{2\\pi |\\Sigma|^{1/2}} \\exp\\left( -\\frac{1}{2} (x - \\mu)^\\top \\Sigma^{-1} (x - \\mu) \\right)\n$$\n其中 $x = (x_1, x_2)^\\top$，$|\\Sigma|$ 是 $\\Sigma$ 的行列式。给定 $X_2=x_2$ 时 $X_1$ 的条件分布（记作 $p(x_1|x_2)$）与联合概率密度函数成正比，即 $p(x_1|x_2) \\propto p(x_1, x_2)$，其中 $x_2$ 被视为一个固定值。为了推导这个分布，我们分析指数中的二次型 $Q(x_1, x_2) = (x - \\mu)^\\top \\Sigma^{-1} (x - \\mu)$。\n\n首先，我们计算协方差矩阵 $\\Sigma$ 的逆矩阵：\n$$\n\\Sigma^{-1} = \\frac{1}{\\sigma_{11}\\sigma_{22} - \\sigma_{12}^2} \\begin{pmatrix} \\sigma_{22}  -\\sigma_{12} \\\\ -\\sigma_{12}  \\sigma_{11} \\end{pmatrix} = \\frac{1}{|\\Sigma|} \\begin{pmatrix} \\sigma_{22}  -\\sigma_{12} \\\\ -\\sigma_{12}  \\sigma_{11} \\end{pmatrix}\n$$\n现在，我们展开二次型 $Q(x_1, x_2)$：\n$$\nQ(x_1, x_2) = \\begin{pmatrix} x_1 - \\mu_1  x_2 - \\mu_2 \\end{pmatrix} \\frac{1}{|\\Sigma|} \\begin{pmatrix} \\sigma_{22}  -\\sigma_{12} \\\\ -\\sigma_{12}  \\sigma_{11} \\end{pmatrix} \\begin{pmatrix} x_1 - \\mu_1 \\\\ x_2 - \\mu_2 \\end{pmatrix}\n$$\n$$\nQ(x_1, x_2) = \\frac{1}{|\\Sigma|} \\left[ \\sigma_{22}(x_1 - \\mu_1)^2 - 2\\sigma_{12}(x_1 - \\mu_1)(x_2 - \\mu_2) + \\sigma_{11}(x_2 - \\mu_2)^2 \\right]\n$$\n为了找到在 $X_2=x_2$ 条件下 $X_1$ 的分布，我们将 $Q(x_1, x_2)$ 中的项按 $x_1$ 的幂次进行分组，并将所有只涉及 $x_2$ 的项视为常数。\n$$\nQ(x_1, x_2) = \\frac{1}{|\\Sigma|} \\left[ \\sigma_{22}x_1^2 - 2\\sigma_{22}\\mu_1x_1 - 2\\sigma_{12}x_1(x_2 - \\mu_2) \\right] + \\text{terms not involving } x_1\n$$\n$$\nQ(x_1, x_2) = \\frac{\\sigma_{22}}{|\\Sigma|}x_1^2 - 2 \\frac{\\sigma_{22}\\mu_1 + \\sigma_{12}(x_2 - \\mu_2)}{|\\Sigma|}x_1 + C(x_2)\n$$\n其中 $C(x_2)$ 包含了所有不依赖于 $x_1$ 的项。条件概率密度函数 $p(x_1|x_2)$ 的指数是 $-\\frac{1}{2}Q(x_1, x_2)$。这个关于 $x_1$ 的二次型表明条件分布是正态分布。设此分布为 $N(\\mu_{1|2}, \\sigma_{1|2}^2)$。其概率密度函数的指数为：\n$$\n-\\frac{(x_1 - \\mu_{1|2})^2}{2\\sigma_{1|2}^2} = -\\frac{1}{2\\sigma_{1|2}^2}x_1^2 + \\frac{\\mu_{1|2}}{\\sigma_{1|2}^2}x_1 - \\frac{\\mu_{1|2}^2}{2\\sigma_{1|2}^2}\n$$\n通过比较此形式与 $-\\frac{1}{2}Q(x_1, x_2)$ 中 $x_1^2$ 和 $x_1$ 的系数，我们可以确定条件均值和方差。\n\n比较 $x_1^2$ 的系数：\n$$\n-\\frac{1}{2\\sigma_{1|2}^2} = -\\frac{1}{2} \\frac{\\sigma_{22}}{|\\Sigma|} \\implies \\sigma_{1|2}^2 = \\frac{|\\Sigma|}{\\sigma_{22}} = \\frac{\\sigma_{11}\\sigma_{22} - \\sigma_{12}^2}{\\sigma_{22}} = \\sigma_{11} - \\frac{\\sigma_{12}^2}{\\sigma_{22}}\n$$\n比较 $x_1$ 的系数：\n$$\n\\frac{\\mu_{1|2}}{\\sigma_{1|2}^2} = -\\frac{1}{2} \\left( -2 \\frac{\\sigma_{22}\\mu_1 + \\sigma_{12}(x_2 - \\mu_2)}{|\\Sigma|} \\right) = \\frac{\\sigma_{22}\\mu_1 + \\sigma_{12}(x_2 - \\mu_2)}{|\\Sigma|}\n$$\n求解条件均值 $\\mu_{1|2}$：\n$$\n\\mu_{1|2} = \\sigma_{1|2}^2 \\left( \\frac{\\sigma_{22}\\mu_1 + \\sigma_{12}(x_2 - \\mu_2)}{|\\Sigma|} \\right) = \\frac{|\\Sigma|}{\\sigma_{22}} \\left( \\frac{\\sigma_{22}\\mu_1 + \\sigma_{12}(x_2 - \\mu_2)}{|\\Sigma|} \\right)\n$$\n$$\n\\mu_{1|2} = \\frac{\\sigma_{22}\\mu_1 + \\sigma_{12}(x_2 - \\mu_2)}{\\sigma_{22}} = \\mu_1 + \\frac{\\sigma_{12}}{\\sigma_{22}}(x_2 - \\mu_2)\n$$\n因此，给定 $X_2=x_2$ 时 $X_1$ 的条件分布是一个正态分布：\n$$\nX_1 | X_2=x_2 \\sim N\\left(\\mu_1 + \\frac{\\sigma_{12}}{\\sigma_{22}}(x_2 - \\mu_2), \\sigma_{11} - \\frac{\\sigma_{12}^2}{\\sigma_{22}}\\right)\n$$\n根据对称性（交换下标 1 和 2），给定 $X_1=x_1$ 时 $X_2$ 的条件分布为：\n$$\nX_2 | X_1=x_1 \\sim N\\left(\\mu_2 + \\frac{\\sigma_{12}}{\\sigma_{11}}(x_1 - \\mu_1), \\sigma_{22} - \\frac{\\sigma_{12}^2}{\\sigma_{11}}\\right)\n$$\n\n### 任务 2：Gibbs 采样算法\n\nGibbs 采样器是一种 MCMC 算法，它通过从条件分布中迭代采样，来生成联合分布的一系列样本。对于二元正态分布的情况，算法流程如下：\n1.  初始化状态 $X^{(0)} = (X_1^{(0)}, X_2^{(0)})^\\top$。问题指定在均值处初始化，因此 $X^{(0)} = \\mu = (\\mu_1, \\mu_2)^\\top$。\n2.  对每次迭代 $t=1, 2, \\dots, B+T$：\n    a. 从第一个分量 $X_1^{(t)}$ 的条件分布中采样一个新值，该分布以第二个分量的当前值 $X_2^{(t-1)}$ 为条件：\n       $$\n       X_1^{(t)} \\sim p(X_1 | X_2 = X_2^{(t-1)}) = N(\\mu_{1|2}^{(t-1)}, \\sigma_{1|2}^2)\n       $$\n       其中 $\\mu_{1|2}^{(t-1)} = \\mu_1 + \\frac{\\sigma_{12}}{\\sigma_{22}}(X_2^{(t-1)} - \\mu_2)$ 且 $\\sigma_{1|2}^2 = \\sigma_{11} - \\frac{\\sigma_{12}^2}{\\sigma_{22}}$。\n    b. 从第二个分量 $X_2^{(t)}$ 的条件分布中采样一个新值，该分布以第一个分量的新更新值 $X_1^{(t)}$ 为条件：\n       $$\n       X_2^{(t)} \\sim p(X_2 | X_1 = X_1^{(t)}) = N(\\mu_{2|1}^{(t)}, \\sigma_{2|1}^2)\n       $$\n       其中 $\\mu_{2|1}^{(t)} = \\mu_2 + \\frac{\\sigma_{12}}{\\sigma_{11}}(X_1^{(t)} - \\mu_1)$ 且 $\\sigma_{2|1}^2 = \\sigma_{22} - \\frac{\\sigma_{12}^2}{\\sigma_{11}}$。\n3.  向量序列 $\\{X^{(t)}\\}_{t=1}^{B+T}$ 构成一个马尔可夫链，其平稳分布是目标二元正态分布。前 $B$ 个样本作为“预烧期”（burn-in）被丢弃，以使链收敛到此平稳分布。随后的 $T$ 个样本，$\\{X^{(t)}\\}_{t=B+1}^{B+T}$，用于分析。\n\n### 任务 3：滞后-1 自相关计算\n\n生成样本序列后，我们分析它们的时间相关性。对于每个坐标的 $T$ 个样本序列，记为 $\\{Y_t\\}_{t=1}^T$，我们计算滞后-1 自相关 $\\hat{\\rho}(1)$。这需要首先计算样本均值 $\\bar{Y}$ 和滞后 $k=0$ 及 $k=1$ 的样本自协方差。\n\n根据所提供的定义：\n1.  样本均值：$\\bar{Y} = \\frac{1}{T}\\sum_{t=1}^T Y_t$。\n2.  滞后 $k$ 阶的样本自协方差：$\\hat{\\gamma}(k) = \\frac{1}{T} \\sum_{t=k+1}^T (Y_t - \\bar{Y})(Y_{t-k} - \\bar{Y})$。\n    -   对于滞后 $k=0$，这给出了（有偏）样本方差：$\\hat{\\gamma}(0) = \\frac{1}{T} \\sum_{t=1}^T (Y_t - \\bar{Y})^2$。\n    -   对于滞后 $k=1$：$\\hat{\\gamma}(1) = \\frac{1}{T} \\sum_{t=2}^T (Y_t - \\bar{Y})(Y_{t-1} - \\bar{Y})$。\n3.  滞后-1 自相关：$\\hat{\\rho}(1) = \\frac{\\hat{\\gamma}(1)}{\\hat{\\gamma}(0)}$。\n\n这个量 $\\hat{\\rho}(1)$ 衡量了链中连续样本之间的相关性。接近 1 的值表示高相关性和对样本空间的缓慢探索，而接近 0 的值则表明样本几乎是独立的。\n\n实现将把这些步骤应用于指定的四个测试用例。使用种子 42 初始化一个伪随机数生成器，并用于所有模拟，以确保可复现性。使用推导出的条件参数生成样本，然后根据指定的自相关公式进行分析。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the Gibbs sampler for all test cases and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A\n        {'mu': (0.0, 0.0), 'Sigma': [[1.0, 0.9], [0.9, 1.0]], 'B': 5000, 'T': 30000},\n        # Case B\n        {'mu': (0.0, 0.0), 'Sigma': [[1.0, 0.0], [0.0, 1.0]], 'B': 5000, 'T': 30000},\n        # Case C\n        {'mu': (1.0, -2.0), 'Sigma': [[4.0, -3.6], [-3.6, 9.0]], 'B': 5000, 'T': 30000},\n        # Case D\n        {'mu': (0.0, 0.0), 'Sigma': [[1.0, 0.999], [0.999, 1.0]], 'B': 5000, 'T': 30000},\n    ]\n\n    # Initialize a single random number generator for reproducibility across all cases.\n    rng = np.random.default_rng(42)\n    \n    results = []\n    for case in test_cases:\n        mu = np.array(case['mu'])\n        Sigma = np.array(case['Sigma'])\n        B = case['B']\n        T = case['T']\n        \n        rho1_x1, rho1_x2 = run_gibbs_sampler(mu, Sigma, B, T, rng)\n        \n        # Format results to six decimal places.\n        results.append(f\"{rho1_x1:.6f}\")\n        results.append(f\"{rho1_x2:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\ndef run_gibbs_sampler(mu, Sigma, B, T, rng):\n    \"\"\"\n    Runs the Gibbs sampler for a bivariate normal distribution.\n\n    Args:\n        mu (np.ndarray): The mean vector (2,).\n        Sigma (np.ndarray): The covariance matrix (2, 2).\n        B (int): The number of burn-in iterations.\n        T (int): The number of samples to collect after burn-in.\n        rng (np.random.Generator): The random number generator instance.\n\n    Returns:\n        tuple: A tuple containing the lag-1 autocorrelation for X1 and X2.\n    \"\"\"\n    mu1, mu2 = mu[0], mu[1]\n    sigma11, sigma12, sigma22 = Sigma[0, 0], Sigma[0, 1], Sigma[1, 1]\n\n    # Pre-calculate parameters for the conditional distributions\n    # X1 | X2=x2 ~ N(mu_1|2, var_1|2)\n    var_1_cond = sigma11 - (sigma12**2) / sigma22\n    std_1_cond = np.sqrt(var_1_cond)\n    coeff_1_cond = sigma12 / sigma22\n\n    # X2 | X1=x1 ~ N(mu_2|1, var_2|1)\n    var_2_cond = sigma22 - (sigma12**2) / sigma11\n    std_2_cond = np.sqrt(var_2_cond)\n    coeff_2_cond = sigma12 / sigma11\n\n    # Initialize the sampler at the mean\n    x1, x2 = mu1, mu2\n    \n    x1_samples = np.empty(T)\n    x2_samples = np.empty(T)\n    \n    total_iterations = B + T\n    for i in range(total_iterations):\n        # Sample X1 from p(X1|X2)\n        mean_1_cond = mu1 + coeff_1_cond * (x2 - mu2)\n        x1 = rng.normal(loc=mean_1_cond, scale=std_1_cond)\n        \n        # Sample X2 from p(X2|X1)\n        mean_2_cond = mu2 + coeff_2_cond * (x1 - mu1)\n        x2 = rng.normal(loc=mean_2_cond, scale=std_2_cond)\n        \n        # Collect samples after burn-in period\n        if i >= B:\n            sample_index = i - B\n            x1_samples[sample_index] = x1\n            x2_samples[sample_index] = x2\n\n    # Calculate lag-1 autocorrelation for each component\n    rho1_x1 = calculate_lag1_autocorr(x1_samples, T)\n    rho1_x2 = calculate_lag1_autocorr(x2_samples, T)\n    \n    return rho1_x1, rho1_x2\n\ndef calculate_lag1_autocorr(y, T):\n    \"\"\"\n    Calculates the lag-1 autocorrelation based on the problem's definition.\n\n    Args:\n        y (np.ndarray): A time series of samples.\n        T (int): The number of samples in the series.\n\n    Returns:\n        float: The lag-1 autocorrelation.\n    \"\"\"\n    if T  2:\n        return 0.0\n\n    y_bar = np.mean(y)\n    y_centered = y - y_bar\n    \n    # Autocovariance at lag 0: gamma(0) = (1/T) * sum((y_t - y_bar)^2)\n    gamma_0 = np.dot(y_centered, y_centered) / T\n    \n    # Autocovariance at lag 1: gamma(1) = (1/T) * sum((y_t - y_bar)*(y_{t-1} - y_bar))\n    gamma_1 = np.dot(y_centered[1:], y_centered[:-1]) / T\n    \n    # Lag-1 autocorrelation\n    if gamma_0 == 0:\n        return 0.0\n        \n    return gamma_1 / gamma_0\n\n# Execute the solution\nsolve()\n\n```"
        },
        {
            "introduction": "蒙特卡洛估计的关键目标不仅是获得无偏估计，更是在给定的计算预算下尽可能减小其方差。对偶采样是一种简单而有效的方差缩减技术，它巧妙地利用了样本间的负相关性 。在本练习中，你将推导对偶估计量的精确方差，从数学上理解它为何以及如何优于标准蒙特卡洛估计，这是提升模拟效率的关键一步。",
            "id": "3762717",
            "problem": "在一个多尺度建模任务中，假设一个粗尺度上的标量可观测量表示为期望 $\\mu = \\mathbb{E}[f(X)]$，其中 $X \\sim U(0,1)$ 且 $f(x) = \\exp(\\lambda x)$，$\\lambda \\in \\mathbb{R} \\setminus \\{0\\}$ 是一个固定参数。为了在蒙特卡洛（MC）方法中使用对偶采样来减少估计量方差，考虑 $K$ 个独立对 $\\{(U_{i},1-U_{i})\\}_{i=1}^{K}$，其中 $U_{i} \\overset{\\text{i.i.d.}}{\\sim} U(0,1)$，并定义对偶估计量\n$$\n\\widehat{\\mu}_{\\mathrm{anti}} = \\frac{1}{K} \\sum_{i=1}^{K} \\frac{f(U_{i}) + f(1-U_{i})}{2}.\n$$\n从期望、方差和协方差的定义出发，仅使用 $[0,1]$ 上均匀分布的性质，推导精确方差 $\\mathrm{Var}(\\widehat{\\mu}_{\\mathrm{anti}})$ 作为 $\\lambda$ 和 $K$ 的函数的闭式表达式。将最终答案表示为单个解析表达式。无需四舍五入，且不涉及任何单位。",
            "solution": "目标是推导对偶估计量方差 $\\mathrm{Var}(\\widehat{\\mu}_{\\mathrm{anti}})$ 的闭式表达式。该估计量由下式给出\n$$\n\\widehat{\\mu}_{\\mathrm{anti}} = \\frac{1}{K} \\sum_{i=1}^{K} \\frac{f(U_{i}) + f(1-U_{i})}{2}\n$$\n其中 $f(x) = \\exp(\\lambda x)$，$\\lambda \\in \\mathbb{R} \\setminus \\{0\\}$，且 $U_{i} \\overset{\\text{i.i.d.}}{\\sim} U(0,1)$。\n\n让我们为每对样本定义一个新的随机变量 $Y_i$：\n$$\nY_i = \\frac{f(U_i) + f(1-U_i)}{2}\n$$\n那么该估计量可以写成这些变量的样本均值：\n$$\n\\widehat{\\mu}_{\\mathrm{anti}} = \\frac{1}{K} \\sum_{i=1}^{K} Y_i\n$$\n由于 $U_i$ 是独立同分布的，所以变量 $Y_i$ 也是独立同分布的。独立随机变量之和的方差等于它们各自方差的和。利用方差的性质，我们有：\n$$\n\\mathrm{Var}(\\widehat{\\mu}_{\\mathrm{anti}}) = \\mathrm{Var}\\left(\\frac{1}{K} \\sum_{i=1}^{K} Y_i\\right) = \\frac{1}{K^2} \\sum_{i=1}^{K} \\mathrm{Var}(Y_i)\n$$\n由于 $Y_i$ 是同分布的，$\\mathrm{Var}(Y_i)$ 对所有 $i$ 都是一个常数。我们把 $\\mathrm{Var}(Y_1)$ 记为 $\\mathrm{Var}(Y)$。\n$$\n\\mathrm{Var}(\\widehat{\\mu}_{\\mathrm{anti}}) = \\frac{1}{K^2} (K \\cdot \\mathrm{Var}(Y)) = \\frac{1}{K} \\mathrm{Var}(Y)\n$$\n我们的任务简化为计算 $\\mathrm{Var}(Y)$。根据方差的定义，\n$$\n\\mathrm{Var}(Y) = \\mathbb{E}[Y^2] - (\\mathbb{E}[Y])^2\n$$\n我们将分别计算 $\\mathbb{E}[Y]$ 和 $\\mathbb{E}[Y^2]$这两项。设 $U \\sim U(0,1)$。\n\n首先，我们计算期望 $\\mathbb{E}[Y]$：\n$$\n\\mathbb{E}[Y] = \\mathbb{E}\\left[\\frac{f(U) + f(1-U)}{2}\\right] = \\frac{1}{2} (\\mathbb{E}[f(U)] + \\mathbb{E}[f(1-U)])\n$$\n$[0, 1]$ 上均匀分布的一个基本性质是，如果 $U \\sim U(0,1)$，那么随机变量 $1-U$ 也服从 $U(0,1)$ 分布。因此，对于任意函数 $g$，都有 $\\mathbb{E}[g(U)] = \\mathbb{E}[g(1-U)]$。应用此性质，我们得到：\n$$\n\\mathbb{E}[Y] = \\frac{1}{2} (\\mathbb{E}[f(U)] + \\mathbb{E}[f(U)]) = \\mathbb{E}[f(U)]\n$$\n我们通过在均匀分布的定义域上进行积分来计算这个期望：\n$$\n\\mathbb{E}[f(U)] = \\mathbb{E}[\\exp(\\lambda U)] = \\int_{0}^{1} \\exp(\\lambda u) \\cdot 1 \\, du\n$$\n由于 $\\lambda \\neq 0$，该积分为：\n$$\n\\mathbb{E}[f(U)] = \\left[\\frac{1}{\\lambda} \\exp(\\lambda u)\\right]_{0}^{1} = \\frac{1}{\\lambda}(\\exp(\\lambda) - \\exp(0)) = \\frac{\\exp(\\lambda)-1}{\\lambda}\n$$\n因此，$\\mathbb{E}[Y] = \\frac{\\exp(\\lambda)-1}{\\lambda}$。\n\n接下来，我们计算二阶矩 $\\mathbb{E}[Y^2]$：\n$$\n\\mathbb{E}[Y^2] = \\mathbb{E}\\left[\\left(\\frac{f(U) + f(1-U)}{2}\\right)^2\\right] = \\frac{1}{4} \\mathbb{E}\\left[f(U)^2 + 2f(U)f(1-U) + f(1-U)^2\\right]\n$$\n利用期望的线性性质：\n$$\n\\mathbb{E}[Y^2] = \\frac{1}{4} \\left(\\mathbb{E}[f(U)^2] + 2\\mathbb{E}[f(U)f(1-U)] + \\mathbb{E}[f(1-U)^2]\\right)\n$$\n我们需要计算括号内的三项期望。\n1.  $\\mathbb{E}[f(U)^2]$:\n    $$\n    \\mathbb{E}[f(U)^2] = \\mathbb{E}[(\\exp(\\lambda U))^2] = \\mathbb{E}[\\exp(2\\lambda U)] = \\int_{0}^{1} \\exp(2\\lambda u) \\, du\n    $$\n    $$\n    = \\left[\\frac{1}{2\\lambda} \\exp(2\\lambda u)\\right]_{0}^{1} = \\frac{\\exp(2\\lambda)-1}{2\\lambda}\n    $$\n2.  $\\mathbb{E}[f(1-U)^2]$: 由于 $U$ 和 $1-U$ 同分布，所以 $\\mathbb{E}[f(1-U)^2] = \\mathbb{E}[f(U)^2] = \\frac{\\exp(2\\lambda)-1}{2\\lambda}$。\n3.  $\\mathbb{E}[f(U)f(1-U)]$:\n    $$\n    \\mathbb{E}[f(U)f(1-U)] = \\mathbb{E}[\\exp(\\lambda U)\\exp(\\lambda(1-U))] = \\mathbb{E}[\\exp(\\lambda U + \\lambda - \\lambda U)] = \\mathbb{E}[\\exp(\\lambda)]\n    $$\n    由于 $\\lambda$ 是一个常数，常数 $\\exp(\\lambda)$ 的期望就是 $\\exp(\\lambda)$。\n\n将这些结果代回 $\\mathbb{E}[Y^2]$ 的表达式中：\n$$\n\\mathbb{E}[Y^2] = \\frac{1}{4}\\left(\\frac{\\exp(2\\lambda)-1}{2\\lambda} + 2\\exp(\\lambda) + \\frac{\\exp(2\\lambda)-1}{2\\lambda}\\right)\n$$\n$$\n= \\frac{1}{4}\\left(2 \\cdot \\frac{\\exp(2\\lambda)-1}{2\\lambda} + 2\\exp(\\lambda)\\right) = \\frac{1}{2}\\left(\\frac{\\exp(2\\lambda)-1}{2\\lambda} + \\exp(\\lambda)\\right)\n$$\n$$\n= \\frac{\\exp(2\\lambda)-1}{4\\lambda} + \\frac{\\exp(\\lambda)}{2}\n$$\n现在我们使用 $\\mathbb{E}[Y]$ 和 $\\mathbb{E}[Y^2]$ 来组合 $\\mathrm{Var}(Y)$：\n$$\n\\mathrm{Var}(Y) = \\mathbb{E}[Y^2] - (\\mathbb{E}[Y])^2 = \\left( \\frac{\\exp(2\\lambda)-1}{4\\lambda} + \\frac{\\exp(\\lambda)}{2} \\right) - \\left( \\frac{\\exp(\\lambda)-1}{\\lambda} \\right)^2\n$$\n$$\n\\mathrm{Var}(Y) = \\frac{\\exp(2\\lambda)-1}{4\\lambda} + \\frac{\\exp(\\lambda)}{2} - \\frac{(\\exp(\\lambda)-1)^2}{\\lambda^2}\n$$\n最后，我们将 $\\mathrm{Var}(Y)$ 的这个表达式代入我们的对偶估计量方差公式中：\n$$\n\\mathrm{Var}(\\widehat{\\mu}_{\\mathrm{anti}}) = \\frac{1}{K} \\mathrm{Var}(Y) = \\frac{1}{K} \\left[ \\frac{\\exp(2\\lambda)-1}{4\\lambda} + \\frac{\\exp(\\lambda)}{2} - \\frac{(\\exp(\\lambda)-1)^2}{\\lambda^2} \\right]\n$$\n这就是精确方差作为 $\\lambda$ 和 $K$ 的函数的闭式表达式。",
            "answer": "$$\\boxed{\\frac{1}{K} \\left[ \\frac{\\exp(2\\lambda)-1}{4\\lambda} + \\frac{\\exp(\\lambda)}{2} - \\frac{(\\exp(\\lambda)-1)^2}{\\lambda^2} \\right]}$$"
        }
    ]
}