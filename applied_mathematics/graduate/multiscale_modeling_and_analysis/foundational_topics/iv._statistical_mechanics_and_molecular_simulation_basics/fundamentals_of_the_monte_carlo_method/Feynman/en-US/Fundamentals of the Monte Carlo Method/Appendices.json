{
    "hands_on_practices": [
        {
            "introduction": "The raw power of the Monte Carlo method lies in its simplicity, yet the precision of the standard sample mean estimator can be limited by high variance. This exercise delves into antithetic sampling, a foundational variance reduction technique. By generating pairs of negatively correlated random numbers, we can construct an estimator that is often significantly more precise than one based on independent samples. This practice challenges you to derive the exact variance of an antithetic estimator from first principles, offering a deep, quantitative understanding of how inducing specific statistical dependencies can enhance Monte Carlo efficiency .",
            "id": "3762717",
            "problem": "In a multiscale modeling task, suppose a scalar observable at the coarse scale is represented as the expectation $\\mu = \\mathbb{E}[f(X)]$, where $X \\sim U(0,1)$ and $f(x) = \\exp(\\lambda x)$ for a fixed parameter $\\lambda \\in \\mathbb{R} \\setminus \\{0\\}$. To reduce estimator variance using antithetic sampling in the Monte Carlo (MC) method, consider $K$ independent pairs $\\{(U_{i},1-U_{i})\\}_{i=1}^{K}$ where $U_{i} \\overset{\\text{i.i.d.}}{\\sim} U(0,1)$, and define the antithetic estimator\n$$\n\\widehat{\\mu}_{\\mathrm{anti}} = \\frac{1}{K} \\sum_{i=1}^{K} \\frac{f(U_{i}) + f(1-U_{i})}{2}.\n$$\nStarting from the definitions of expectation, variance, and covariance, and using only properties of the uniform distribution on $[0,1]$, derive a closed-form expression for the exact variance $\\mathrm{Var}(\\widehat{\\mu}_{\\mathrm{anti}})$ as a function of $\\lambda$ and $K$. Express your final answer as a single analytic expression. No rounding is required, and no units are involved.",
            "solution": "The objective is to derive a closed-form expression for the variance of the antithetic estimator, $\\mathrm{Var}(\\widehat{\\mu}_{\\mathrm{anti}})$. The estimator is given by\n$$\n\\widehat{\\mu}_{\\mathrm{anti}} = \\frac{1}{K} \\sum_{i=1}^{K} \\frac{f(U_{i}) + f(1-U_{i})}{2}\n$$\nwhere $f(x) = \\exp(\\lambda x)$, $\\lambda \\in \\mathbb{R} \\setminus \\{0\\}$, and $U_{i} \\overset{\\text{i.i.d.}}{\\sim} U(0,1)$.\n\nLet us define a new random variable $Y_i$ for each pair of samples:\n$$\nY_i = \\frac{f(U_i) + f(1-U_i)}{2}\n$$\nThe estimator can then be written as the sample mean of these variables:\n$$\n\\widehat{\\mu}_{\\mathrm{anti}} = \\frac{1}{K} \\sum_{i=1}^{K} Y_i\n$$\nSince the $U_i$ are independent and identically distributed, the variables $Y_i$ are also independent and identically distributed. The variance of the sum of independent random variables is the sum of their variances. Using the properties of variance, we have:\n$$\n\\mathrm{Var}(\\widehat{\\mu}_{\\mathrm{anti}}) = \\mathrm{Var}\\left(\\frac{1}{K} \\sum_{i=1}^{K} Y_i\\right) = \\frac{1}{K^2} \\sum_{i=1}^{K} \\mathrm{Var}(Y_i)\n$$\nAs the $Y_i$ are identically distributed, $\\mathrm{Var}(Y_i)$ is constant for all $i$. Let's denote $\\mathrm{Var}(Y_1)$ as $\\mathrm{Var}(Y)$.\n$$\n\\mathrm{Var}(\\widehat{\\mu}_{\\mathrm{anti}}) = \\frac{1}{K^2} (K \\cdot \\mathrm{Var}(Y)) = \\frac{1}{K} \\mathrm{Var}(Y)\n$$\nOur task reduces to calculating $\\mathrm{Var}(Y)$. According to the definition of variance,\n$$\n\\mathrm{Var}(Y) = \\mathbb{E}[Y^2] - (\\mathbb{E}[Y])^2\n$$\nWe will compute the two terms, $\\mathbb{E}[Y]$ and $\\mathbb{E}[Y^2]$, separately. Let $U \\sim U(0,1)$.\n\nFirst, we compute the expectation $\\mathbb{E}[Y]$:\n$$\n\\mathbb{E}[Y] = \\mathbb{E}\\left[\\frac{f(U) + f(1-U)}{2}\\right] = \\frac{1}{2} (\\mathbb{E}[f(U)] + \\mathbb{E}[f(1-U)])\n$$\nA fundamental property of the uniform distribution on $[0, 1]$ is that if $U \\sim U(0,1)$, then the random variable $1-U$ is also distributed as $U(0,1)$. Consequently, for any function $g$, $\\mathbb{E}[g(U)] = \\mathbb{E}[g(1-U)]$. Applying this property, we get:\n$$\n\\mathbb{E}[Y] = \\frac{1}{2} (\\mathbb{E}[f(U)] + \\mathbb{E}[f(U)]) = \\mathbb{E}[f(U)]\n$$\nWe calculate this expectation by integration over the domain of the uniform distribution:\n$$\n\\mathbb{E}[f(U)] = \\mathbb{E}[\\exp(\\lambda U)] = \\int_{0}^{1} \\exp(\\lambda u) \\cdot 1 \\, du\n$$\nSince $\\lambda \\neq 0$, the integral is:\n$$\n\\mathbb{E}[f(U)] = \\left[\\frac{1}{\\lambda} \\exp(\\lambda u)\\right]_{0}^{1} = \\frac{1}{\\lambda}(\\exp(\\lambda) - \\exp(0)) = \\frac{\\exp(\\lambda)-1}{\\lambda}\n$$\nThus, $\\mathbb{E}[Y] = \\frac{\\exp(\\lambda)-1}{\\lambda}$.\n\nNext, we compute the second moment, $\\mathbb{E}[Y^2]$:\n$$\n\\mathbb{E}[Y^2] = \\mathbb{E}\\left[\\left(\\frac{f(U) + f(1-U)}{2}\\right)^2\\right] = \\frac{1}{4} \\mathbb{E}\\left[f(U)^2 + 2f(U)f(1-U) + f(1-U)^2\\right]\n$$\nUsing the linearity of expectation:\n$$\n\\mathbb{E}[Y^2] = \\frac{1}{4} \\left(\\mathbb{E}[f(U)^2] + 2\\mathbb{E}[f(U)f(1-U)] + \\mathbb{E}[f(1-U)^2]\\right)\n$$\nWe need to evaluate the three expectation terms inside the parentheses.\n1.  $\\mathbb{E}[f(U)^2]$:\n    $$\n    \\mathbb{E}[f(U)^2] = \\mathbb{E}[(\\exp(\\lambda U))^2] = \\mathbb{E}[\\exp(2\\lambda U)] = \\int_{0}^{1} \\exp(2\\lambda u) \\, du\n    $$\n    $$\n    = \\left[\\frac{1}{2\\lambda} \\exp(2\\lambda u)\\right]_{0}^{1} = \\frac{\\exp(2\\lambda)-1}{2\\lambda}\n    $$\n2.  $\\mathbb{E}[f(1-U)^2]$: Since $U$ and $1-U$ are identically distributed, $\\mathbb{E}[f(1-U)^2] = \\mathbb{E}[f(U)^2] = \\frac{\\exp(2\\lambda)-1}{2\\lambda}$.\n3.  $\\mathbb{E}[f(U)f(1-U)]$:\n    $$\n    \\mathbb{E}[f(U)f(1-U)] = \\mathbb{E}[\\exp(\\lambda U)\\exp(\\lambda(1-U))] = \\mathbb{E}[\\exp(\\lambda U + \\lambda - \\lambda U)] = \\mathbb{E}[\\exp(\\lambda)]\n    $$\n    Since $\\lambda$ is a constant, the expectation of the constant $\\exp(\\lambda)$ is simply $\\exp(\\lambda)$.\n\nSubstituting these results back into the expression for $\\mathbb{E}[Y^2]$:\n$$\n\\mathbb{E}[Y^2] = \\frac{1}{4}\\left(\\frac{\\exp(2\\lambda)-1}{2\\lambda} + 2\\exp(\\lambda) + \\frac{\\exp(2\\lambda)-1}{2\\lambda}\\right)\n$$\n$$\n= \\frac{1}{4}\\left(2 \\cdot \\frac{\\exp(2\\lambda)-1}{2\\lambda} + 2\\exp(\\lambda)\\right) = \\frac{1}{2}\\left(\\frac{\\exp(2\\lambda)-1}{2\\lambda} + \\exp(\\lambda)\\right)\n$$\n$$\n= \\frac{\\exp(2\\lambda)-1}{4\\lambda} + \\frac{\\exp(\\lambda)}{2}\n$$\nNow we assemble $\\mathrm{Var}(Y)$ using $\\mathbb{E}[Y]$ and $\\mathbb{E}[Y^2]$:\n$$\n\\mathrm{Var}(Y) = \\mathbb{E}[Y^2] - (\\mathbb{E}[Y])^2 = \\left( \\frac{\\exp(2\\lambda)-1}{4\\lambda} + \\frac{\\exp(\\lambda)}{2} \\right) - \\left( \\frac{\\exp(\\lambda)-1}{\\lambda} \\right)^2\n$$\n$$\n\\mathrm{Var}(Y) = \\frac{\\exp(2\\lambda)-1}{4\\lambda} + \\frac{\\exp(\\lambda)}{2} - \\frac{(\\exp(\\lambda)-1)^2}{\\lambda^2}\n$$\nFinally, we substitute this expression for $\\mathrm{Var}(Y)$ into our formula for the variance of the antithetic estimator:\n$$\n\\mathrm{Var}(\\widehat{\\mu}_{\\mathrm{anti}}) = \\frac{1}{K} \\mathrm{Var}(Y) = \\frac{1}{K} \\left[ \\frac{\\exp(2\\lambda)-1}{4\\lambda} + \\frac{\\exp(\\lambda)}{2} - \\frac{(\\exp(\\lambda)-1)^2}{\\lambda^2} \\right]\n$$\nThis is the closed-form expression for the exact variance as a function of $\\lambda$ and $K$.",
            "answer": "$$\\boxed{\\frac{1}{K} \\left[ \\frac{\\exp(2\\lambda)-1}{4\\lambda} + \\frac{\\exp(\\lambda)}{2} - \\frac{(\\exp(\\lambda)-1)^2}{\\lambda^2} \\right]}$$"
        },
        {
            "introduction": "Monte Carlo methods are predicated on our ability to draw samples from a target probability distribution. While simple for standard distributions, this task becomes challenging for custom or complex densities. This is where acceptance-rejection sampling provides an elegant and powerful solution. The core idea is to sample from a simpler, \"proposal\" distribution and then probabilistically accept or reject those samples to match the desired target distribution. This problem guides you through the essential design steps: selecting a suitable proposal, calculating the optimal bounding constant, and deriving the method's efficiency, providing a robust understanding of this fundamental sampling algorithm .",
            "id": "3762695",
            "problem": "Consider a positive random variable with target density on $(0,\\infty)$ proportional to $x^{k-1}\\exp(-x)$, where $k>0$ is a known shape parameter. In multiscale modeling and analysis, one frequently needs to sample such variables as latent scale parameters efficiently. You are asked to design an acceptance–rejection sampling scheme based on a Gamma proposal and compute its acceptance rate, starting from fundamental definitions.\n  \nStarting point:\n- The Gamma distribution with shape $k>0$ and rate $\\lambda>0$ has density $g(x)=\\frac{\\lambda^{k}}{\\Gamma(k)}x^{k-1}\\exp(-\\lambda x)$ for $x>0$, where $\\Gamma(k)$ is the Gamma function.\n- The acceptance–rejection method samples a proposal $Y$ from a proposal density $g(x)$ and accepts it with probability $\\min\\{1, f(Y)/(M g(Y))\\}$, where $f(x)$ is the normalized target density and $M\\geq \\sup_{x>0} f(x)/g(x)$ is a finite bound ensuring $f(x) \\leq M g(x)$ for all $x>0$. The average acceptance rate equals the mean of the acceptance probability under the proposal.\n\nTask:\n1. Normalize the target to obtain $f(x)$ and choose a proposal family $g_{\\beta}(x)$ that is a Gamma distribution with the \\emph{same} shape parameter $k$ and rate parameter $\\beta \\in (0,1]$. Justify this choice by ensuring a finite envelope constant $M(k,\\beta)$.\n2. Derive the minimal bounding constant $M(k,\\beta)$ from first principles by analyzing $\\sup_{x>0} f(x)/g_{\\beta}(x)$.\n3. Using the acceptance–rejection definition, derive a closed-form expression for the average acceptance rate as a function of $k$ and $\\beta$.\n4. Provide the final expression, simplified, for the acceptance rate.\n\nYour final answer must be a single closed-form analytic expression in terms of $k$ and $\\beta$. No rounding is required, and you should not include any units.",
            "solution": "We begin by identifying the normalized target and the proposal. The given unnormalized target density on $(0,\\infty)$ is proportional to $x^{k-1}\\exp(-x)$ with $k>0$. The corresponding normalized target density is the Gamma distribution with shape $k$ and rate $1$:\n$$\nf(x) \\;=\\; \\frac{1}{\\Gamma(k)}\\, x^{k-1}\\exp(-x), \\quad x>0.\n$$\nWe propose a Gamma distribution with the same shape $k$ and rate parameter $\\beta \\in (0,1]$:\n$$\ng_{\\beta}(x) \\;=\\; \\frac{\\beta^{k}}{\\Gamma(k)}\\, x^{k-1}\\exp(-\\beta x), \\quad x>0.\n$$\n\nThe acceptance–rejection method requires a finite envelope constant $M(k,\\beta)$ such that $f(x) \\leq M(k,\\beta) g_{\\beta}(x)$ for all $x>0$. Equivalently, we need $M(k,\\beta) \\geq \\sup_{x>0} \\frac{f(x)}{g_{\\beta}(x)}$. We compute this supremum. Using the expressions for $f$ and $g_{\\beta}$,\n$$\n\\frac{f(x)}{g_{\\beta}(x)} \n=\\frac{\\frac{1}{\\Gamma(k)} x^{k-1}\\exp(-x)}{\\frac{\\beta^{k}}{\\Gamma(k)} x^{k-1}\\exp(-\\beta x)}\n=\\beta^{-k}\\exp\\!\\bigl(-x + \\beta x\\bigr)\n=\\beta^{-k}\\exp\\!\\bigl(-(1-\\beta)x\\bigr).\n$$\nWe now examine the behavior of $\\beta^{-k}\\exp\\!\\bigl(-(1-\\beta)x\\bigr)$ on $x>0$ for $\\beta \\in (0,1]$:\n- If $\\beta=1$, we have $\\frac{f(x)}{g_{\\beta}(x)} = 1$ for all $x>0$, yielding a minimal envelope constant $M(k,1)=1$.\n- If $\\beta \\in (0,1)$, then $(1-\\beta)>0$, so $\\exp\\!\\bigl(-(1-\\beta)x\\bigr)$ is decreasing in $x$ and attains its supremum at $x=0$, where it equals $1$. Therefore,\n$$\n\\sup_{x>0} \\frac{f(x)}{g_{\\beta}(x)}\n=\\beta^{-k} \\cdot \\sup_{x>0}\\exp\\!\\bigl(-(1-\\beta)x\\bigr)\n=\\beta^{-k}\\cdot 1\n=\\beta^{-k}.\n$$\nHence, the minimal bounding constant for $\\beta \\in (0,1]$ is\n$$\nM(k,\\beta)=\\beta^{-k}.\n$$\nFor completeness, note that if $\\beta>1$, then $(1-\\beta)0$ and $\\exp\\!\\bigl(-(1-\\beta)x\\bigr)=\\exp\\!\\bigl((\\beta-1)x\\bigr)$ grows unbounded as $x\\to\\infty$, implying $\\sup_{x>0} \\frac{f(x)}{g_{\\beta}(x)}=+\\infty$ and making the acceptance–rejection scheme invalid. This justifies the restriction $\\beta\\in(0,1]$.\n\nWith $M(k,\\beta)$ established, the acceptance–rejection algorithm is:\n- Sample $Y \\sim g_{\\beta}(y)$ and $U \\sim \\mathrm{Uniform}(0,1)$, independently.\n- Accept $Y$ if\n$$\nU \\leq \\frac{f(Y)}{M(k,\\beta) g_{\\beta}(Y)}.\n$$\nUsing the derived forms,\n$$\n\\frac{f(Y)}{M(k,\\beta) g_{\\beta}(Y)}\n=\\frac{\\beta^{-k}\\exp\\!\\bigl(-(1-\\beta)Y\\bigr)}{\\beta^{-k}}\n=\\exp\\!\\bigl(-(1-\\beta)Y\\bigr).\n$$\nThus the acceptance criterion simplifies to\n$$\nU \\leq \\exp\\!\\bigl(-(1-\\beta)Y\\bigr).\n$$\n\nWe now compute the average acceptance rate, which is the expectation of the acceptance probability under the proposal $g_{\\beta}$. By the definition of acceptance–rejection,\n$$\n\\text{Acceptance rate} \\;=\\; \\int_{0}^{\\infty} g_{\\beta}(x)\\, \\frac{f(x)}{M(k,\\beta) g_{\\beta}(x)}\\, dx\n\\;=\\; \\frac{1}{M(k,\\beta)} \\int_{0}^{\\infty} f(x)\\, dx\n\\;=\\; \\frac{1}{M(k,\\beta)},\n$$\nsince $f$ is a normalized density and $\\int_{0}^{\\infty} f(x)\\, dx = 1$. Using $M(k,\\beta)=\\beta^{-k}$, we obtain\n$$\n\\text{Acceptance rate} \\;=\\; \\beta^{k}.\n$$\nAs a cross-check using the explicit acceptance probability, note that\n$$\n\\int_{0}^{\\infty} g_{\\beta}(x)\\, \\exp\\!\\bigl(-(1-\\beta)x\\bigr)\\, dx\n= \\int_{0}^{\\infty} \\frac{\\beta^{k}}{\\Gamma(k)} x^{k-1} \\exp\\!\\bigl(-\\beta x\\bigr)\\exp\\!\\bigl(-(1-\\beta)x\\bigr)\\, dx\n= \\frac{\\beta^{k}}{\\Gamma(k)} \\int_{0}^{\\infty} x^{k-1}\\exp(-x)\\, dx\n= \\frac{\\beta^{k}}{\\Gamma(k)} \\Gamma(k)\n= \\beta^{k},\n$$\nconfirming the acceptance rate.\n\nTherefore, for the Gamma proposal $g_{\\beta}$ with the same shape $k$ and rate $\\beta \\in (0,1]$, the acceptance–rejection scheme has acceptance rate $\\beta^{k}$. The choice $\\beta=1$ yields acceptance rate $1$ and reduces to direct sampling from the target, whereas any $\\beta \\in (0,1)$ yields acceptance rate strictly less than $1$, decreasing monotonically in $(0,1)$ as $\\beta$ decreases.",
            "answer": "$$\\boxed{\\beta^{k}}$$"
        },
        {
            "introduction": "When faced with high-dimensional and correlated probability distributions, both direct sampling and simple rejection methods become infeasible. This is the domain of Markov Chain Monte Carlo (MCMC), and the Gibbs sampler is a cornerstone algorithm in this family. It tackles a complex multivariate problem by breaking it down into a series of simpler, iterative draws from each variable's conditional distribution. This hands-on practice bridges theory and implementation: you will first derive the conditional distributions for a bivariate normal model and then build a Gibbs sampler to explore it. By analyzing the sampler's performance under varying levels of correlation, you will gain tangible insight into the power and potential pitfalls of MCMC methods in practice .",
            "id": "3762723",
            "problem": "Consider a target bivariate normal distribution with a known mean vector and covariance matrix. Let the random vector be $X = (X_1, X_2)^\\top$ with mean $\\mu = (\\mu_1, \\mu_2)^\\top$ and covariance matrix \n$$\n\\Sigma = \\begin{pmatrix}\n\\sigma_{11}  \\sigma_{12} \\\\\n\\sigma_{12}  \\sigma_{22}\n\\end{pmatrix},\n$$\nwhere $\\Sigma$ is symmetric positive definite. The fundamental base for this problem is the joint density of a multivariate normal distribution: for $d=2$, \n$$\np(x) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma|^{1/2}} \\exp\\left( -\\frac{1}{2} (x - \\mu)^\\top \\Sigma^{-1} (x - \\mu) \\right),\n$$\nwith $x = (x_1, x_2)^\\top$. \n\nTasks:\n1. Starting from the joint density definition above and using only standard algebraic manipulation (such as completing the square and properties of positive definite matrices), derive the conditional distributions $X_1 \\mid X_2$ and $X_2 \\mid X_1$ for this bivariate normal model. Do not use pre-quoted shortcut identities for conditional normals; show how the conditional mean and variance arise from the joint density.\n2. Use the derived conditional distributions to implement a two-component Gibbs sampler that alternately updates $X_1$ and $X_2$. Specifically, at iteration $t$, sample $X_1^{(t)}$ from the conditional distribution of $X_1$ given the current $X_2^{(t-1)}$, then sample $X_2^{(t)}$ from the conditional distribution of $X_2$ given the newly updated $X_1^{(t)}$. Initialize $X^{(0)}$ at the mean $\\mu$. Use a fixed pseudorandom number generator seed $42$ for reproducibility.\n3. After discarding a burn-in of $B$ iterations, collect $T$ successive samples of each coordinate sequence. For a sequence $Y_1, Y_2, \\dots, Y_T$, define the sample mean $\\bar{Y} = \\frac{1}{T}\\sum_{t=1}^T Y_t$, the sample autocovariance at lag $k$ as \n$$\n\\hat{\\gamma}(k) = \\frac{1}{T} \\sum_{t=k+1}^T (Y_t - \\bar{Y})(Y_{t-k} - \\bar{Y}),\n$$\nand the lag-$1$ autocorrelation as \n$$\n\\hat{\\rho}(1) = \\frac{\\hat{\\gamma}(1)}{\\hat{\\gamma}(0)}.\n$$\nCompute $\\hat{\\rho}(1)$ for each coordinate sequence separately using this definition.\n\nTest suite:\nUse the following four parameter sets, each specified by $(\\mu, \\Sigma, B, T)$, with initialization $X^{(0)} = \\mu$ and the fixed seed $42$:\n- Case A: $\\mu = (0, 0)^\\top$, $\\Sigma = \\begin{pmatrix} 1  0.9 \\\\ 0.9  1 \\end{pmatrix}$, $B = 5000$, $T = 30000$.\n- Case B: $\\mu = (0, 0)^\\top$, $\\Sigma = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}$, $B = 5000$, $T = 30000$.\n- Case C: $\\mu = (1, -2)^\\top$, $\\Sigma = \\begin{pmatrix} 4  -3.6 \\\\ -3.6  9 \\end{pmatrix}$, $B = 5000$, $T = 30000$.\n- Case D: $\\mu = (0, 0)^\\top$, $\\Sigma = \\begin{pmatrix} 1  0.999 \\\\ 0.999  1 \\end{pmatrix}$, $B = 5000$, $T = 30000$.\n\nOutput specification:\nYour program should produce a single line of output containing eight floating-point numbers corresponding to the lag-$1$ autocorrelations, ordered by case and by coordinate within each case. The order is \n$$\n[\\hat{\\rho}_1^{A}(1), \\hat{\\rho}_2^{A}(1), \\hat{\\rho}_1^{B}(1), \\hat{\\rho}_2^{B}(1), \\hat{\\rho}_1^{C}(1), \\hat{\\rho}_2^{C}(1), \\hat{\\rho}_1^{D}(1), \\hat{\\rho}_2^{D}(1)],\n$$\nwhere $\\hat{\\rho}_j^{\\text{Case}}(1)$ denotes the lag-$1$ autocorrelation of coordinate $j$ for the specified case. Express each number rounded to six decimal places. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1, r_2, \\dots, r_8]$). No physical units or angle units apply to this problem; all quantities are dimensionless real numbers.",
            "solution": "The problem requires the derivation of the conditional distributions for a bivariate normal random vector, the implementation of a Gibbs sampler based on these conditionals, and the computation of the lag-$1$ autocorrelation of the resulting sample chains. The solution proceeds in three parts, corresponding to the three tasks outlined in the problem statement.\n\n### Task 1: Derivation of Conditional Distributions\n\nLet the bivariate random vector be $X = (X_1, X_2)^\\top$ with mean vector $\\mu = (\\mu_1, \\mu_2)^\\top$ and a symmetric positive definite covariance matrix $\\Sigma$:\n$$\n\\Sigma = \\begin{pmatrix} \\sigma_{11}  \\sigma_{12} \\\\ \\sigma_{12}  \\sigma_{22} \\end{pmatrix}\n$$\nThe joint probability density function (PDF) is given by:\n$$\np(x_1, x_2) = \\frac{1}{2\\pi |\\Sigma|^{1/2}} \\exp\\left( -\\frac{1}{2} (x - \\mu)^\\top \\Sigma^{-1} (x - \\mu) \\right)\n$$\nwhere $x = (x_1, x_2)^\\top$ and $|\\Sigma|$ is the determinant of $\\Sigma$. The conditional distribution of $X_1$ given $X_2=x_2$, denoted $p(x_1|x_2)$, is proportional to the joint PDF, i.e., $p(x_1|x_2) \\propto p(x_1, x_2)$, where $x_2$ is treated as a fixed value. To derive this distribution, we analyze the quadratic form in the exponent, $Q(x_1, x_2) = (x - \\mu)^\\top \\Sigma^{-1} (x - \\mu)$.\n\nFirst, we compute the inverse of the covariance matrix $\\Sigma$:\n$$\n\\Sigma^{-1} = \\frac{1}{\\sigma_{11}\\sigma_{22} - \\sigma_{12}^2} \\begin{pmatrix} \\sigma_{22}  -\\sigma_{12} \\\\ -\\sigma_{12}  \\sigma_{11} \\end{pmatrix} = \\frac{1}{|\\Sigma|} \\begin{pmatrix} \\sigma_{22}  -\\sigma_{12} \\\\ -\\sigma_{12}  \\sigma_{11} \\end{pmatrix}\n$$\nNow, we expand the quadratic form $Q(x_1, x_2)$:\n$$\nQ(x_1, x_2) = \\begin{pmatrix} x_1 - \\mu_1  x_2 - \\mu_2 \\end{pmatrix} \\frac{1}{|\\Sigma|} \\begin{pmatrix} \\sigma_{22}  -\\sigma_{12} \\\\ -\\sigma_{12}  \\sigma_{11} \\end{pmatrix} \\begin{pmatrix} x_1 - \\mu_1 \\\\ x_2 - \\mu_2 \\end{pmatrix}\n$$\n$$\nQ(x_1, x_2) = \\frac{1}{|\\Sigma|} \\left[ \\sigma_{22}(x_1 - \\mu_1)^2 - 2\\sigma_{12}(x_1 - \\mu_1)(x_2 - \\mu_2) + \\sigma_{11}(x_2 - \\mu_2)^2 \\right]\n$$\nTo find the distribution of $X_1$ conditional on $X_2=x_2$, we group terms in $Q(x_1, x_2)$ by powers of $x_1$, treating all terms involving only $x_2$ as constants.\n$$\nQ(x_1, x_2) = \\frac{1}{|\\Sigma|} \\left[ \\sigma_{22}x_1^2 - 2\\sigma_{22}\\mu_1x_1 - 2\\sigma_{12}x_1(x_2 - \\mu_2) \\right] + \\text{terms not involving } x_1\n$$\n$$\nQ(x_1, x_2) = \\frac{\\sigma_{22}}{|\\Sigma|}x_1^2 - 2 \\frac{\\sigma_{22}\\mu_1 + \\sigma_{12}(x_2 - \\mu_2)}{|\\Sigma|}x_1 + C(x_2)\n$$\nwhere $C(x_2)$ subsumes all terms that do not depend on $x_1$. The exponent of the conditional PDF $p(x_1|x_2)$ is $-\\frac{1}{2}Q(x_1, x_2)$. This quadratic form in $x_1$ indicates that the conditional distribution is normal. Let this distribution be $N(\\mu_{1|2}, \\sigma_{1|2}^2)$. The exponent of its PDF is:\n$$\n-\\frac{(x_1 - \\mu_{1|2})^2}{2\\sigma_{1|2}^2} = -\\frac{1}{2\\sigma_{1|2}^2}x_1^2 + \\frac{\\mu_{1|2}}{\\sigma_{1|2}^2}x_1 - \\frac{\\mu_{1|2}^2}{2\\sigma_{1|2}^2}\n$$\nBy comparing the coefficients of $x_1^2$ and $x_1$ between this form and $-\\frac{1}{2}Q(x_1, x_2)$, we can identify the conditional mean and variance.\n\nComparing the $x_1^2$ coefficients:\n$$\n-\\frac{1}{2\\sigma_{1|2}^2} = -\\frac{1}{2} \\frac{\\sigma_{22}}{|\\Sigma|} \\implies \\sigma_{1|2}^2 = \\frac{|\\Sigma|}{\\sigma_{22}} = \\frac{\\sigma_{11}\\sigma_{22} - \\sigma_{12}^2}{\\sigma_{22}} = \\sigma_{11} - \\frac{\\sigma_{12}^2}{\\sigma_{22}}\n$$\nComparing the $x_1$ coefficients:\n$$\n\\frac{\\mu_{1|2}}{\\sigma_{1|2}^2} = -\\frac{1}{2} \\left( -2 \\frac{\\sigma_{22}\\mu_1 + \\sigma_{12}(x_2 - \\mu_2)}{|\\Sigma|} \\right) = \\frac{\\sigma_{22}\\mu_1 + \\sigma_{12}(x_2 - \\mu_2)}{|\\Sigma|}\n$$\nSolving for the conditional mean $\\mu_{1|2}$:\n$$\n\\mu_{1|2} = \\sigma_{1|2}^2 \\left( \\frac{\\sigma_{22}\\mu_1 + \\sigma_{12}(x_2 - \\mu_2)}{|\\Sigma|} \\right) = \\frac{|\\Sigma|}{\\sigma_{22}} \\left( \\frac{\\sigma_{22}\\mu_1 + \\sigma_{12}(x_2 - \\mu_2)}{|\\Sigma|} \\right)\n$$\n$$\n\\mu_{1|2} = \\frac{\\sigma_{22}\\mu_1 + \\sigma_{12}(x_2 - \\mu_2)}{\\sigma_{22}} = \\mu_1 + \\frac{\\sigma_{12}}{\\sigma_{22}}(x_2 - \\mu_2)\n$$\nThus, the conditional distribution of $X_1$ given $X_2=x_2$ is a normal distribution:\n$$\nX_1 | X_2=x_2 \\sim N\\left(\\mu_1 + \\frac{\\sigma_{12}}{\\sigma_{22}}(x_2 - \\mu_2), \\sigma_{11} - \\frac{\\sigma_{12}^2}{\\sigma_{22}}\\right)\n$$\nBy symmetry (swapping indices $1$ and $2$), the conditional distribution of $X_2$ given $X_1=x_1$ is:\n$$\nX_2 | X_1=x_1 \\sim N\\left(\\mu_2 + \\frac{\\sigma_{12}}{\\sigma_{11}}(x_1 - \\mu_1), \\sigma_{22} - \\frac{\\sigma_{12}^2}{\\sigma_{11}}\\right)\n$$\n\n### Task 2: Gibbs Sampling Algorithm\n\nThe Gibbs sampler is an MCMC algorithm that generates a sequence of samples from a joint distribution by iteratively sampling from its conditional distributions. For the bivariate normal case, the algorithm proceeds as follows:\n1.  Initialize the state $X^{(0)} = (X_1^{(0)}, X_2^{(0)})^\\top$. The problem specifies initialization at the mean, so $X^{(0)} = \\mu = (\\mu_1, \\mu_2)^\\top$.\n2.  For each iteration $t=1, 2, \\dots, B+T$:\n    a. Sample a new value for the first component, $X_1^{(t)}$, from its distribution conditional on the current value of the second component, $X_2^{(t-1)}$:\n       $$\n       X_1^{(t)} \\sim p(X_1 | X_2 = X_2^{(t-1)}) = N(\\mu_{1|2}^{(t-1)}, \\sigma_{1|2}^2)\n       $$\n       where $\\mu_{1|2}^{(t-1)} = \\mu_1 + \\frac{\\sigma_{12}}{\\sigma_{22}}(X_2^{(t-1)} - \\mu_2)$ and $\\sigma_{1|2}^2 = \\sigma_{11} - \\frac{\\sigma_{12}^2}{\\sigma_{22}}$.\n    b. Sample a new value for the second component, $X_2^{(t)}$, from its distribution conditional on the newly updated value of the first component, $X_1^{(t)}$:\n       $$\n       X_2^{(t)} \\sim p(X_2 | X_1 = X_1^{(t)}) = N(\\mu_{2|1}^{(t)}, \\sigma_{2|1}^2)\n       $$\n       where $\\mu_{2|1}^{(t)} = \\mu_2 + \\frac{\\sigma_{12}}{\\sigma_{11}}(X_1^{(t)} - \\mu_1)$ and $\\sigma_{2|1}^2 = \\sigma_{22} - \\frac{\\sigma_{12}^2}{\\sigma_{11}}$.\n3.  The sequence of vectors $\\{X^{(t)}\\}_{t=1}^{B+T}$ forms a Markov chain whose stationary distribution is the target bivariate normal distribution. The first $B$ samples are discarded as \"burn-in\" to allow the chain to converge to this stationary distribution. The subsequent $T$ samples, $\\{X^{(t)}\\}_{t=B+1}^{B+T}$, are used for analysis.\n\n### Task 3: Lag-$1$ Autocorrelation Calculation\n\nAfter generating the sample sequences, we analyze their temporal correlation. For each coordinate's sequence of $T$ samples, denoted $\\{Y_t\\}_{t=1}^T$, we compute the lag-$1$ autocorrelation, $\\hat{\\rho}(1)$. This requires first calculating the sample mean $\\bar{Y}$ and the sample autocovariance at lags $k=0$ and $k=1$.\n\nFollowing the definitions provided:\n1.  Sample Mean: $\\bar{Y} = \\frac{1}{T}\\sum_{t=1}^T Y_t$.\n2.  Sample Autocovariance at lag $k$: $\\hat{\\gamma}(k) = \\frac{1}{T} \\sum_{t=k+1}^T (Y_t - \\bar{Y})(Y_{t-k} - \\bar{Y})$.\n    -   For lag $k=0$, this gives the (biased) sample variance: $\\hat{\\gamma}(0) = \\frac{1}{T} \\sum_{t=1}^T (Y_t - \\bar{Y})^2$.\n    -   For lag $k=1$: $\\hat{\\gamma}(1) = \\frac{1}{T} \\sum_{t=2}^T (Y_t - \\bar{Y})(Y_{t-1} - \\bar{Y})$.\n3.  Lag-$1$ Autocorrelation: $\\hat{\\rho}(1) = \\frac{\\hat{\\gamma}(1)}{\\hat{\\gamma}(0)}$.\n\nThis quantity $\\hat{\\rho}(1)$ measures the correlation between successive samples in the chain. A value close to $1$ indicates high correlation and slow exploration of the sample space, while a value close to $0$ suggests that samples are nearly independent.\n\nThe implementation will apply these steps to the four test cases specified. A single pseudorandom number generator is seeded with $42$ and used for all simulations to ensure reproducibility. The derived conditional parameters are used to generate samples, which are then analyzed according to the specified autocorrelation formulas.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the Gibbs sampler for all test cases and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A\n        {'mu': (0.0, 0.0), 'Sigma': [[1.0, 0.9], [0.9, 1.0]], 'B': 5000, 'T': 30000},\n        # Case B\n        {'mu': (0.0, 0.0), 'Sigma': [[1.0, 0.0], [0.0, 1.0]], 'B': 5000, 'T': 30000},\n        # Case C\n        {'mu': (1.0, -2.0), 'Sigma': [[4.0, -3.6], [-3.6, 9.0]], 'B': 5000, 'T': 30000},\n        # Case D\n        {'mu': (0.0, 0.0), 'Sigma': [[1.0, 0.999], [0.999, 1.0]], 'B': 5000, 'T': 30000},\n    ]\n\n    # Initialize a single random number generator for reproducibility across all cases.\n    rng = np.random.default_rng(42)\n    \n    results = []\n    for case in test_cases:\n        mu = np.array(case['mu'])\n        Sigma = np.array(case['Sigma'])\n        B = case['B']\n        T = case['T']\n        \n        rho1_x1, rho1_x2 = run_gibbs_sampler(mu, Sigma, B, T, rng)\n        \n        # Format results to six decimal places.\n        results.append(f\"{rho1_x1:.6f}\")\n        results.append(f\"{rho1_x2:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\ndef run_gibbs_sampler(mu, Sigma, B, T, rng):\n    \"\"\"\n    Runs the Gibbs sampler for a bivariate normal distribution.\n\n    Args:\n        mu (np.ndarray): The mean vector (2,).\n        Sigma (np.ndarray): The covariance matrix (2, 2).\n        B (int): The number of burn-in iterations.\n        T (int): The number of samples to collect after burn-in.\n        rng (np.random.Generator): The random number generator instance.\n\n    Returns:\n        tuple: A tuple containing the lag-1 autocorrelation for X1 and X2.\n    \"\"\"\n    mu1, mu2 = mu[0], mu[1]\n    sigma11, sigma12, sigma22 = Sigma[0, 0], Sigma[0, 1], Sigma[1, 1]\n\n    # Pre-calculate parameters for the conditional distributions\n    # X1 | X2=x2 ~ N(mu_1|2, var_1|2)\n    var_1_cond = sigma11 - (sigma12**2) / sigma22\n    std_1_cond = np.sqrt(var_1_cond)\n    coeff_1_cond = sigma12 / sigma22\n\n    # X2 | X1=x1 ~ N(mu_2|1, var_2|1)\n    var_2_cond = sigma22 - (sigma12**2) / sigma11\n    std_2_cond = np.sqrt(var_2_cond)\n    coeff_2_cond = sigma12 / sigma11\n\n    # Initialize the sampler at the mean\n    x1, x2 = mu1, mu2\n    \n    x1_samples = np.empty(T)\n    x2_samples = np.empty(T)\n    \n    total_iterations = B + T\n    for i in range(total_iterations):\n        # Sample X1 from p(X1|X2)\n        mean_1_cond = mu1 + coeff_1_cond * (x2 - mu2)\n        x1 = rng.normal(loc=mean_1_cond, scale=std_1_cond)\n        \n        # Sample X2 from p(X2|X1)\n        mean_2_cond = mu2 + coeff_2_cond * (x1 - mu1)\n        x2 = rng.normal(loc=mean_2_cond, scale=std_2_cond)\n        \n        # Collect samples after burn-in period\n        if i = B:\n            sample_index = i - B\n            x1_samples[sample_index] = x1\n            x2_samples[sample_index] = x2\n\n    # Calculate lag-1 autocorrelation for each component\n    rho1_x1 = calculate_lag1_autocorr(x1_samples, T)\n    rho1_x2 = calculate_lag1_autocorr(x2_samples, T)\n    \n    return rho1_x1, rho1_x2\n\ndef calculate_lag1_autocorr(y, T):\n    \"\"\"\n    Calculates the lag-1 autocorrelation based on the problem's definition.\n\n    Args:\n        y (np.ndarray): A time series of samples.\n        T (int): The number of samples in the series.\n\n    Returns:\n        float: The lag-1 autocorrelation.\n    \"\"\"\n    if T  2:\n        return 0.0\n\n    y_bar = np.mean(y)\n    y_centered = y - y_bar\n    \n    # Autocovariance at lag 0: gamma(0) = (1/T) * sum((y_t - y_bar)^2)\n    gamma_0 = np.dot(y_centered, y_centered) / T\n    \n    # Autocovariance at lag 1: gamma(1) = (1/T) * sum((y_t - y_bar)*(y_{t-1} - y_bar))\n    gamma_1 = np.dot(y_centered[1:], y_centered[:-1]) / T\n    \n    # Lag-1 autocorrelation\n    if gamma_0 == 0:\n        return 0.0\n        \n    return gamma_1 / gamma_0\n\n# Execute the solution\nsolve()\n\n```"
        }
    ]
}