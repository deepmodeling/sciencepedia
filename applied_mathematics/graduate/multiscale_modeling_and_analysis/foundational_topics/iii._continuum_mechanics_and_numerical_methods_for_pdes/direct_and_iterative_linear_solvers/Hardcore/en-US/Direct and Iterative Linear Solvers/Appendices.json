{
    "hands_on_practices": [
        {
            "introduction": "The convergence of iterative solvers is a primary concern in numerical linear algebra. This practice  uses the Jacobi method to illustrate a fundamental reason for divergence—the lack of diagonal dominance—and demonstrates how a simple modification, diagonal stabilization, can restore convergence. By calculating the spectral radius of the iteration matrix, you will gain a concrete understanding of the mathematical conditions that govern the success or failure of iterative schemes.",
            "id": "3749921",
            "problem": "Consider a linear system $A x = b$ arising from a two-scale coupling in a multiscale discretization where coarse-scale elimination produces a dense, symmetric positive semidefinite operator with strong inter-node couplings. Let the coarse operator be modeled by the matrix\n$$\nA \\in \\mathbb{R}^{3 \\times 3}, \\quad\nA = \\begin{pmatrix}\n1  2  2 \\\\\n2  1  2 \\\\\n2  2  1\n\\end{pmatrix}.\n$$\nDefine the standard Jacobi method via the matrix splitting $A = D + L + U$, where $D$ is the diagonal of $A$, $L$ is the strict lower-triangular part, and $U$ is the strict upper-triangular part. The Jacobi iteration is $x^{(k+1)} = D^{-1}(b - (L+U) x^{(k)})$, with iteration matrix $G_{J} = -D^{-1}(L+U)$. Convergence of the Jacobi method is governed by the spectral radius (SR) criterion, namely, Jacobi converges if and only if $\\rho(G_{J})  1$, where $\\rho(\\cdot)$ denotes the spectral radius.\n\nTask:\n- Construct and demonstrate explicitly that the above $A$ is a counterexample where the Jacobi method fails to converge, by computing $\\rho(G_{J})$.\n- Now consider a diagonal stabilization motivated by coarse-scale penalty regularization: define the perturbed operator $A_{\\gamma} = A + \\gamma I$, where $I$ is the identity matrix and $\\gamma \\in \\mathbb{R}$ is a scalar. For the Jacobi method applied to $A_{\\gamma}$, derive the exact condition on $\\gamma$ that ensures $\\rho(G_{J}(\\gamma))  1$ and determine the critical threshold $\\gamma_{\\star}$ such that Jacobi converges for all $\\gamma  \\gamma_{\\star}$.\n\nProvide the final answer as the exact analytical value of the critical threshold $\\gamma_{\\star}$. No rounding is required and no physical units are involved.",
            "solution": "The problem requires an analysis of the convergence of the Jacobi method for a given matrix $A$ and a perturbed version $A_{\\gamma}$. The validation of the problem statement confirms that it is well-posed, scientifically grounded, and contains all necessary information to proceed with a solution.\n\nThe problem is divided into two tasks. First, to demonstrate that the Jacobi method fails to converge for the matrix $A$. Second, to find the condition on a stabilization parameter $\\gamma$ for the method to converge when applied to the perturbed matrix $A_{\\gamma}$.\n\n**Part 1: Convergence Analysis for Matrix $A$**\n\nThe given matrix is:\n$$\nA = \\begin{pmatrix}\n1  2  2 \\\\\n2  1  2 \\\\\n2  2  1\n\\end{pmatrix}\n$$\nThe Jacobi method is defined by the matrix splitting $A = D + L + U$, where $D$ is the diagonal part of $A$, $L$ is the strictly lower-triangular part, and $U$ is the strictly upper-triangular part.\nFor the given matrix $A$, we have:\n$$\nD = \\begin{pmatrix}\n1  0  0 \\\\\n0  1  0 \\\\\n0  0  1\n\\end{pmatrix} = I\n$$\n$$\nL + U = \\begin{pmatrix}\n0  2  2 \\\\\n2  0  2 \\\\\n2  2  0\n\\end{pmatrix}\n$$\nThe Jacobi iteration matrix, $G_{J}$, is given by $G_{J} = -D^{-1}(L+U)$.\nSince $D = I$, its inverse is $D^{-1} = I$. Therefore, the iteration matrix is:\n$$\nG_{J} = -I (L+U) = -(L+U) = \\begin{pmatrix}\n0  -2  -2 \\\\\n-2  0  -2 \\\\\n-2  -2  0\n\\end{pmatrix}\n$$\nThe convergence of the Jacobi method is determined by the spectral radius of the iteration matrix, $\\rho(G_{J})$. The method converges if and only if $\\rho(G_{J})  1$. The spectral radius is the maximum absolute value of the eigenvalues of $G_{J}$.\n\nTo find the eigenvalues $\\lambda$ of $G_{J}$, we solve the characteristic equation $\\det(G_{J} - \\lambda I) = 0$:\n$$\n\\det \\begin{pmatrix}\n-\\lambda  -2  -2 \\\\\n-2  -\\lambda  -2 \\\\\n-2  -2  -\\lambda\n\\end{pmatrix} = 0\n$$\nExpanding the determinant:\n$$\n(-\\lambda)((-\\lambda)(-\\lambda) - (-2)(-2)) - (-2)((-2)(-\\lambda) - (-2)(-2)) + (-2)((-2)(-2) - (-\\lambda)(-2)) = 0\n$$\n$$\n-\\lambda(\\lambda^2 - 4) + 2(2\\lambda - 4) - 2(4 - 2\\lambda) = 0\n$$\n$$\n-\\lambda^3 + 4\\lambda + 4\\lambda - 8 - 8 + 4\\lambda = 0\n$$\n$$\n-\\lambda^3 + 12\\lambda - 16 = 0\n$$\nMultiplying by $-1$, we get the characteristic polynomial:\n$$\n\\lambda^3 - 12\\lambda + 16 = 0\n$$\nWe can find the roots of this polynomial. By the rational root theorem, we can test integer divisors of $16$.\nFor $\\lambda = 2$: $2^3 - 12(2) + 16 = 8 - 24 + 16 = 0$. So, $\\lambda = 2$ is a root.\nWe can perform polynomial division by $(\\lambda - 2)$:\n$$\n\\frac{\\lambda^3 - 12\\lambda + 16}{\\lambda - 2} = \\lambda^2 + 2\\lambda - 8\n$$\nThe quadratic factor can be factored further: $\\lambda^2 + 2\\lambda - 8 = (\\lambda + 4)(\\lambda - 2)$.\nSo, the characteristic equation is $(\\lambda - 2)(\\lambda + 4)(\\lambda - 2) = 0$, which simplifies to $(\\lambda - 2)^2(\\lambda + 4) = 0$.\nThe eigenvalues of $G_{J}$ are $\\lambda_1 = -4$, $\\lambda_2 = 2$, and $\\lambda_3 = 2$.\n\nThe spectral radius is $\\rho(G_{J}) = \\max \\{|\\lambda_1|, |\\lambda_2|, |\\lambda_3|\\} = \\max \\{|-4|, |2|, |2|\\} = 4$.\nSince $\\rho(G_{J}) = 4 \\ge 1$, the Jacobi method for the matrix $A$ does not converge. This completes the first task.\n\n**Part 2: Convergence Analysis for Perturbed Matrix $A_{\\gamma}$**\n\nNow, we consider the perturbed matrix $A_{\\gamma} = A + \\gamma I$, where $\\gamma \\in \\mathbb{R}$.\n$$\nA_{\\gamma} = \\begin{pmatrix}\n1  2  2 \\\\\n2  1  2 \\\\\n2  2  1\n\\end{pmatrix} + \\gamma \\begin{pmatrix}\n1  0  0 \\\\\n0  1  0 \\\\\n0  0  1\n\\end{pmatrix} = \\begin{pmatrix}\n1+\\gamma  2  2 \\\\\n2  1+\\gamma  2 \\\\\n2  2  1+\\gamma\n\\end{pmatrix}\n$$\nWe apply the Jacobi method to the system $A_{\\gamma}x = b$. The splitting of $A_{\\gamma}$ is $A_{\\gamma} = D_{\\gamma} + L_{\\gamma} + U_{\\gamma}$.\n$$\nD_{\\gamma} = \\begin{pmatrix}\n1+\\gamma  0  0 \\\\\n0  1+\\gamma  0 \\\\\n0  0  1+\\gamma\n\\end{pmatrix} = (1+\\gamma)I\n$$\nThe off-diagonal parts remain unchanged, so $L_{\\gamma} + U_{\\gamma} = L+U$.\nThe new Jacobi iteration matrix, $G_{J}(\\gamma)$, is:\n$$\nG_{J}(\\gamma) = -D_{\\gamma}^{-1}(L_{\\gamma}+U_{\\gamma})\n$$\nFor $D_{\\gamma}$ to be invertible, we must have $1+\\gamma \\neq 0$. Assuming this condition holds, $D_{\\gamma}^{-1} = \\frac{1}{1+\\gamma}I$.\n$$\nG_{J}(\\gamma) = -\\left(\\frac{1}{1+\\gamma}I\\right)(L+U) = \\frac{1}{1+\\gamma} \\left( - (L+U) \\right) = \\frac{1}{1+\\gamma} G_{J}\n$$\nIf $\\lambda$ is an eigenvalue of $G_{J}$ with corresponding eigenvector $v$, then $G_{J}v = \\lambda v$.\nFor the new iteration matrix, we have:\n$$\nG_{J}(\\gamma)v = \\frac{1}{1+\\gamma} G_{J}v = \\frac{\\lambda}{1+\\gamma} v\n$$\nThus, the eigenvalues of $G_{J}(\\gamma)$, let's call them $\\mu$, are related to the eigenvalues of $G_{J}$ by $\\mu = \\frac{\\lambda}{1+\\gamma}$.\nThe eigenvalues of $G_{J}(\\gamma)$ are:\n$$\n\\mu_1 = \\frac{-4}{1+\\gamma}, \\quad \\mu_2 = \\frac{2}{1+\\gamma}, \\quad \\mu_3 = \\frac{2}{1+\\gamma}\n$$\nThe spectral radius of $G_{J}(\\gamma)$ is:\n$$\n\\rho(G_{J}(\\gamma)) = \\max \\left\\{ \\left|\\frac{-4}{1+\\gamma}\\right|, \\left|\\frac{2}{1+\\gamma}\\right| \\right\\} = \\frac{1}{|1+\\gamma|} \\max\\{4, 2\\} = \\frac{4}{|1+\\gamma|}\n$$\nThe condition for convergence is $\\rho(G_{J}(\\gamma))  1$.\n$$\n\\frac{4}{|1+\\gamma|}  1\n$$\nThis is equivalent to $|1+\\gamma|  4$. This inequality holds if either $1+\\gamma  4$ or $1+\\gamma  -4$.\nCase 1: $1+\\gamma  4 \\implies \\gamma  3$.\nCase 2: $1+\\gamma  -4 \\implies \\gamma  -5$.\nSo, the Jacobi method for $A_{\\gamma}$ converges if and only if $\\gamma  3$ or $\\gamma  -5$.\n\nThe problem asks for the critical threshold $\\gamma_{\\star}$ such that Jacobi converges for all $\\gamma  \\gamma_{\\star}$. This corresponds to the interval $(3, \\infty)$. The lower bound of this interval defines the critical threshold.\nTherefore, the critical threshold is $\\gamma_{\\star} = 3$.",
            "answer": "$$\n\\boxed{3}\n$$"
        },
        {
            "introduction": "For challenging linear systems, the performance of iterative solvers hinges on effective preconditioning. This exercise  introduces the Incomplete LU (ILU) factorization, a powerful preconditioning technique that balances computational cost and solver acceleration. You will investigate how a drop tolerance, which controls the sparsity of the ILU factors, directly impacts the spectral condition number of the preconditioned system, revealing a key trade-off in designing practical solvers.",
            "id": "3749911",
            "problem": "Consider a symmetric positive definite sparse matrix $A(\\delta) \\in \\mathbb{R}^{3 \\times 3}$ arising from a two-scale diffusion model, in which fine-scale nearest-neighbor interactions are encoded by the tridiagonal entries and a coarse-scale nonlocal interaction is homogenized into a weak long-range coupling of magnitude $\\delta  0$ between the first and third nodes:\n$$\nA(\\delta) \\;=\\; \\begin{pmatrix}\n2  -1  \\delta \\\\\n-1  2  -1 \\\\\n\\delta  -1  2\n\\end{pmatrix}.\n$$\nWe construct a preconditioner $M$ by performing an Incomplete Lower-Upper factorization (ILU) with level-of-fill $k=0$ together with an absolute drop tolerance $\\tau  0$ applied to all entries allowed by the $k=0$ pattern. The rule is: any original or intermediate fill-in entry of magnitude strictly less than $\\tau$ is set to zero before factorization. In particular, if $\\delta  \\tau$, the nonlocal entries at $(1,3)$ and $(3,1)$ are dropped, producing the tridiagonal matrix\n$$\n\\tilde{A} \\;=\\; A(0) \\;=\\; \\begin{pmatrix}\n2  -1  0 \\\\\n-1  2  -1 \\\\\n0  -1  2\n\\end{pmatrix},\n$$\nand $M$ is taken as the exact $LU$ factors of $\\tilde{A}$ under the $k=0$ pattern; otherwise, when $\\delta \\ge \\tau$, $M$ is taken as the exact $LU$ factors of $A(\\delta)$ under the same $k=0$ pattern.\n\nStarting from the core definitions of factorization and preconditioning, analyze how the choice of $\\tau$ affects both the sparsity of $M$ and the spectral condition of the preconditioned system. Then, for a concrete instance with $\\delta = 0.2$ and $\\tau = 0.3$, compute the spectral condition number\n$$\n\\kappa_{2}\\!\\left(M^{-1}A(\\delta)\\right) \\;=\\; \\frac{\\lambda_{\\max}\\!\\left(M^{-1}A(\\delta)\\right)}{\\lambda_{\\min}\\!\\left(M^{-1}A(\\delta)\\right)}\n$$\nwhere $\\lambda_{\\max}$ and $\\lambda_{\\min}$ denote the largest and smallest eigenvalues, respectively. Round your final numerical answer to four significant figures. Express the final answer as a pure number without units.",
            "solution": "The problem is evaluated to be valid. It is scientifically grounded in the principles of numerical linear algebra, specifically preconditioning techniques for sparse linear systems arising from physical models. The problem is well-posed, providing all necessary matrices, parameters, and definitions to arrive at a unique solution. It is stated objectively and contains no contradictions or ambiguities.\n\nThe problem asks for an analysis of a specific preconditioning strategy and a concrete calculation of a spectral condition number.\n\nPart 1: Analysis of the Preconditioner\n\nThe preconditioner $M$ is constructed based on an Incomplete LU factorization with level-of-fill $k=0$ (ILU(0)) and a drop tolerance $\\tau$. The matrix being preconditioned is $A(\\delta)$. The construction of $M$ depends on the comparison between $\\delta$ and $\\tau$.\n\n1.  Influence on Sparsity: The sparsity of the preconditioner $M$ is determined by the sparsity of the matrix that is being factorized. The ILU(0) process, by definition, only allows non-zero entries in the factors $L$ and $U$ where there were non-zero entries in the original matrix.\n    *   If $\\delta  \\tau$, the entries $A_{13}$ and $A_{31}$, both equal to $\\delta$, are dropped. The factorization is performed on the tridiagonal matrix $\\tilde{A} = A(0)$. The LU factors of a tridiagonal matrix are bidiagonal, preserving the tridiagonal sparsity pattern. Thus, $M$ (representing the incomplete factors of $\\tilde{A}$) is sparser. Specifically, the storage for the factors corresponds to a tridiagonal structure.\n    *   If $\\delta \\ge \\tau$, no entries are dropped from $A(\\delta)$. The ILU(0) factorization is performed on $A(\\delta)$, which has non-zero entries at positions $(1,3)$ and $(3,1)$. The factors $L$ and $U$ will have non-zeros in these positions. The resulting preconditioner $M$ is thus denser than in the previous case, as it retains the full sparsity pattern of $A(\\delta)$.\n\n    In summary, a larger value of $\\tau$ relative to $\\delta$ promotes sparsity in $M$ by dropping weak long-range couplings.\n\n2.  Influence on Spectral Condition: The goal of a preconditioner $M$ is to be a good approximation of $A(\\delta)$, such that $M^{-1}A(\\delta)$ is close to the identity matrix $I$. The closer $M^{-1}A(\\delta)$ is to $I$, the closer its eigenvalues are to $1$, and the smaller its spectral condition number $\\kappa_2(M^{-1}A(\\delta))$.\n    *   If $\\delta  \\tau$, $M$ is constructed from $\\tilde{A}=A(0)$. The preconditioned matrix is $M^{-1}A(\\delta)$. Here, $M^{-1}$ is not a good approximation of $A(\\delta)^{-1}$ if $\\delta$ is non-negligible, as $M$ completely ignores the coupling $\\delta$. The difference $A(\\delta) - M = A(\\delta) - A(0)$ introduces an error that can move the eigenvalues of $M^{-1}A(\\delta)$ away from $1$, potentially increasing the condition number.\n    *   If $\\delta \\ge \\tau$, $M$ is the ILU(0) factorization of $A(\\delta)$. For a $3 \\times 3$ matrix with the given sparsity pattern, the LU factorization does not introduce any fill-in. Therefore, the ILU(0) factorization is the exact LU factorization, meaning $M = A(\\delta)$. In this ideal scenario, the preconditioned matrix is $M^{-1}A(\\delta) = (A(\\delta))^{-1}A(\\delta) = I$. The eigenvalues are all $1$, and the condition number is $\\kappa_2(I) = 1$, which is the optimal value.\n\n    In summary, a smaller value of $\\tau$ relative to $\\delta$ leads to a more accurate preconditioner and a better (smaller) condition number, with the trade-off of requiring a denser factorization.\n\nPart 2: Calculation for $\\delta = 0.2$ and $\\tau = 0.3$\n\nWe are given the specific values $\\delta = 0.2$ and $\\tau = 0.3$. Since $0.2  0.3$, we have $\\delta  \\tau$. This corresponds to the first case in our analysis.\n\nThe matrix to be preconditioned is:\n$$\nA(\\delta) \\;=\\; A(0.2) \\;=\\; \\begin{pmatrix}\n2  -1  0.2 \\\\\n-1  2  -1 \\\\\n0.2  -1  2\n\\end{pmatrix}\n$$\nAccording to the problem rule for $\\delta  \\tau$, the off-diagonal entries with magnitude $\\delta$ are dropped. The resulting matrix, denoted $\\tilde{A}$, is used to construct the preconditioner $M$. The problem states that $M$ is taken as the exact LU factors of $\\tilde{A}$ under the $k=0$ pattern. For the tridiagonal matrix $\\tilde{A}$, the exact LU factorization produces no fill-in, so this means we can take $M = \\tilde{A}$.\n$$\nM \\;=\\; \\tilde{A} \\;=\\; A(0) \\;=\\; \\begin{pmatrix}\n2  -1  0 \\\\\n-1  2  -1 \\\\\n0  -1  2\n\\end{pmatrix}\n$$\nWe need to compute the spectral condition number of $M^{-1}A(0.2)$. First, we compute the inverse of $M$. The determinant of $M$ is:\n$$\n\\det(M) = 2(2 \\cdot 2 - (-1)(-1)) - (-1)(-1 \\cdot 2 - (-1) \\cdot 0) = 2(3) - (-1)(-2) = 6 - 2 = 4\n$$\nThe inverse $M^{-1}$ is given by $\\frac{1}{\\det(M)}\\text{adj}(M)$. The adjugate matrix, which is the transpose of the cofactor matrix, is:\n$$\n\\text{adj}(M) = \\begin{pmatrix}\n3  2  1 \\\\\n2  4  2 \\\\\n1  2  3\n\\end{pmatrix}\n$$\nSo, the inverse is:\n$$\nM^{-1} = \\frac{1}{4} \\begin{pmatrix}\n3  2  1 \\\\\n2  4  2 \\\\\n1  2  3\n\\end{pmatrix}\n$$\nNext, we compute the preconditioned matrix $C = M^{-1}A(0.2)$:\n$$\nC = \\frac{1}{4} \\begin{pmatrix}\n3  2  1 \\\\\n2  4  2 \\\\\n1  2  3\n\\end{pmatrix}\n\\begin{pmatrix}\n2  -1  0.2 \\\\\n-1  2  -1 \\\\\n0.2  -1  2\n\\end{pmatrix}\n$$\nPerforming the matrix multiplication:\n$$\n4C = \\begin{pmatrix}\n3(2)+2(-1)+1(0.2)  3(-1)+2(2)+1(-1)  3(0.2)+2(-1)+1(2) \\\\\n2(2)+4(-1)+2(0.2)  2(-1)+4(2)+2(-1)  2(0.2)+4(-1)+2(2) \\\\\n1(2)+2(-1)+3(0.2)  1(-1)+2(2)+3(-1)  1(0.2)+2(-1)+3(2)\n\\end{pmatrix}\n$$\n$$\n4C = \\begin{pmatrix}\n6-2+0.2  -3+4-1  0.6-2+2 \\\\\n4-4+0.4  -2+8-2  0.4-4+4 \\\\\n2-2+0.6  -1+4-3  0.2-2+6\n\\end{pmatrix}\n= \\begin{pmatrix}\n4.2  0  0.6 \\\\\n0.4  4  0.4 \\\\\n0.6  0  4.2\n\\end{pmatrix}\n$$\nDividing by $4$:\n$$\nC = \\begin{pmatrix}\n1.05  0  0.15 \\\\\n0.1  1  0.1 \\\\\n0.15  0  1.05\n\\end{pmatrix}\n$$\nTo find the condition number, we need the eigenvalues of $C$. We solve the characteristic equation $\\det(C - \\lambda I) = 0$:\n$$\n\\det\\begin{pmatrix}\n1.05 - \\lambda  0  0.15 \\\\\n0.1  1 - \\lambda  0.1 \\\\\n0.15  0  1.05 - \\lambda\n\\end{pmatrix} = 0\n$$\nWe expand the determinant along the second column:\n$$\n(1 - \\lambda) \\det\\begin{pmatrix}\n1.05 - \\lambda  0.15 \\\\\n0.15  1.05 - \\lambda\n\\end{pmatrix} = 0\n$$\nThis gives $(1 - \\lambda) [ (1.05 - \\lambda)^2 - (0.15)^2 ] = 0$.\nThe solutions for $\\lambda$ are found from two factors:\n1.  $1 - \\lambda = 0 \\implies \\lambda_1 = 1$.\n2.  $(1.05 - \\lambda)^2 - (0.15)^2 = 0 \\implies (1.05 - \\lambda)^2 = (0.15)^2$.\n    Taking the square root of both sides gives $1.05 - \\lambda = \\pm 0.15$.\n    This yields two more eigenvalues:\n    $\\lambda_2 = 1.05 - 0.15 = 0.9$.\n    $\\lambda_3 = 1.05 + 0.15 = 1.2$.\n\nThe eigenvalues of the preconditioned matrix $C$ are $\\{0.9, 1.0, 1.2\\}$.\nThe largest eigenvalue is $\\lambda_{\\max} = 1.2$.\nThe smallest eigenvalue is $\\lambda_{\\min} = 0.9$.\n\nThe spectral condition number is the ratio of the magnitude of the largest eigenvalue to the smallest eigenvalue:\n$$\n\\kappa_2(M^{-1}A(\\delta)) = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}} = \\frac{1.2}{0.9} = \\frac{12}{9} = \\frac{4}{3}\n$$\nAs a decimal, this is $1.3333\\ldots$. Rounding to four significant figures gives $1.333$.",
            "answer": "$$\\boxed{1.333}$$"
        },
        {
            "introduction": "The performance of a numerical method often depends on the specific structure of the matrix, which arises from the discretization of a physical problem. This practice  applies the Jacobi method to a system derived from a tokamak diffusion model, where varying physical cell volumes introduce non-uniformity into the matrix. By analyzing the effect of diagonal equilibration—a preconditioning technique designed for such cases—you will see how careful mathematical analysis can sometimes yield counter-intuitive results and deepen your understanding of solver behavior.",
            "id": "3967008",
            "problem": "Consider flux-surface-averaged cross-field diffusion of a scalar field $\\phi(r)$ in a tokamak of large aspect ratio, with major radius $R_0$, minor radius $a$, constant cross-field diffusivity $\\chi$, and Dirichlet boundary conditions $\\phi(0)=\\phi_0$, $\\phi(a)=\\phi_a$. The governing steady-state equation in the radial coordinate $r \\in (0,a)$ is the divergence form\n$$\n\\frac{d}{dr}\\Big(A(r)\\,\\chi\\,\\frac{d\\phi}{dr}\\Big) = s(r),\n$$\nwhere the flux-surface area is $A(r) = 4\\pi^{2} R_0\\, r$. Using a standard finite-volume discretization on a uniform radial grid with $N$ control volumes centered at $r_i = i\\,\\Delta r$ for $i=1,\\dots,N$, $\\Delta r = a/(N+1)$, with cell volumes $V_i = A(r_i)\\,\\Delta r$ and face areas $A_{i\\pm \\frac{1}{2}} = A(r_{i\\pm \\frac{1}{2}})$, the discrete operator acting on the vector $\\boldsymbol{\\phi} = (\\phi_1,\\dots,\\phi_N)^{\\top}$ has the tridiagonal matrix $\\boldsymbol{A} \\in \\mathbb{R}^{N \\times N}$ with entries\n$$\nA_{ii} = \\frac{\\alpha_{i+\\frac{1}{2}} + \\alpha_{i-\\frac{1}{2}}}{V_i},\\quad\nA_{i,i+1} = -\\frac{\\alpha_{i+\\frac{1}{2}}}{V_i},\\quad\nA_{i,i-1} = -\\frac{\\alpha_{i-\\frac{1}{2}}}{V_i},\n$$\nfor $i=1,\\dots,N$, with the convention that $\\alpha_{1-\\frac{1}{2}}$ and $\\alpha_{N+\\frac{1}{2}}$ incorporate the Dirichlet boundary conditions at $r=0$ and $r=a$. Here $\\alpha_{i+\\frac{1}{2}} := \\chi\\,\\frac{A_{i+\\frac{1}{2}}}{\\Delta r}$ are the face conductances, and $V_i = A(r_i)\\,\\Delta r$ are the control-volume measures. Let $\\boldsymbol{D} = \\mathrm{diag}(\\boldsymbol{A})$ denote the diagonal of $\\boldsymbol{A}$ and define the Jacobi iteration matrix\n$$\n\\boldsymbol{M}_{J} := \\boldsymbol{I} - \\boldsymbol{D}^{-1}\\boldsymbol{A}.\n$$\n\nDiagonal row equilibration is often used to mitigate strong variations in control-volume magnitudes by scaling the equations with the diagonal matrix $\\boldsymbol{R} := \\mathrm{diag}(V_1,\\dots,V_N)$, leading to the left-scaled system $\\boldsymbol{R}\\boldsymbol{A}\\boldsymbol{\\phi} = \\boldsymbol{R}\\boldsymbol{s}$. Let $\\boldsymbol{M}_{J}^{(\\mathrm{eq})}$ denote the Jacobi iteration matrix constructed from the equilibrated operator, that is\n$$\n\\boldsymbol{M}_{J}^{(\\mathrm{eq})} := \\boldsymbol{I} - \\big(\\mathrm{diag}(\\boldsymbol{R}\\boldsymbol{A})\\big)^{-1}\\,\\boldsymbol{R}\\boldsymbol{A}.\n$$\n\nStarting from the definitions above and first principles of the finite-volume discretization and Jacobi iteration, derive the effect of diagonal scaling on $\\boldsymbol{M}_J$ for this tokamak diffusion operator with varying cell volumes. Then, using only fundamental linear algebra properties, quantify the change in spectral radius\n$$\n\\Delta \\rho := \\rho\\!\\left(\\boldsymbol{M}_{J}^{(\\mathrm{eq})}\\right) - \\rho\\!\\left(\\boldsymbol{M}_{J}\\right),\n$$\nwhere $\\rho(\\cdot)$ denotes the spectral radius. Express the final answer for $\\Delta \\rho$ as a single real number. No rounding is required.",
            "solution": "The user wants me to analyze the effect of diagonal row equilibration on the Jacobi iteration matrix for a given one-dimensional diffusion problem in a tokamak. The goal is to compute the change in the spectral radius of the Jacobi matrix due to this scaling.\n\n### Step 1: Problem Validation\n\nI will first validate the problem statement.\n\n**Givens:**\n- Governing Equation: $\\frac{d}{dr}\\Big(A(r)\\,\\chi\\,\\frac{d\\phi}{dr}\\Big) = s(r)$ for $r \\in (0,a)$.\n- Flux-surface area: $A(r) = 4\\pi^{2} R_0\\, r$.\n- Diffusivity: $\\chi$ is constant.\n- Boundary Conditions: $\\phi(0)=\\phi_0$, $\\phi(a)=\\phi_a$.\n- Discretization: Finite-volume on a uniform grid with $N$ control volumes.\n- Grid points: $r_i = i\\,\\Delta r$ for $i=1,\\dots,N$.\n- Grid spacing: $\\Delta r = a/(N+1)$.\n- Control volume: $V_i = A(r_i)\\,\\Delta r$.\n- Face areas: $A_{i\\pm \\frac{1}{2}} = A(r_{i\\pm \\frac{1}{2}})$.\n- Face conductance: $\\alpha_{i+\\frac{1}{2}} := \\chi\\,\\frac{A_{i+\\frac{1}{2}}}{\\Delta r}$.\n- Original matrix entries: $A_{ii} = \\frac{\\alpha_{i+\\frac{1}{2}} + \\alpha_{i-\\frac{1}{2}}}{V_i}$, $A_{i,i+1} = -\\frac{\\alpha_{i+\\frac{1}{2}}}{V_i}$, $A_{i,i-1} = -\\frac{\\alpha_{i-\\frac{1}{2}}}{V_i}$.\n- Original Jacobi matrix: $\\boldsymbol{M}_{J} := \\boldsymbol{I} - \\boldsymbol{D}^{-1}\\boldsymbol{A}$, where $\\boldsymbol{D} = \\mathrm{diag}(\\boldsymbol{A})$.\n- Equilibration scaling matrix: $\\boldsymbol{R} := \\mathrm{diag}(V_1,\\dots,V_N)$.\n- Equilibrated system: $\\boldsymbol{R}\\boldsymbol{A}\\boldsymbol{\\phi} = \\boldsymbol{R}\\boldsymbol{s}$.\n- Equilibrated Jacobi matrix: $\\boldsymbol{M}_{J}^{(\\mathrm{eq})} := \\boldsymbol{I} - \\big(\\mathrm{diag}(\\boldsymbol{R}\\boldsymbol{A})\\big)^{-1}\\,\\boldsymbol{R}\\boldsymbol{A}$.\n- Quantity to find: $\\Delta \\rho := \\rho\\!\\left(\\boldsymbol{M}_{J}^{(\\mathrm{eq})}\\right) - \\rho\\!\\left(\\boldsymbol{M}_{J}\\right)$.\n\n**Validation:**\n1.  **Scientifically Grounded:** The problem describes a simplified (large aspect ratio, constant diffusivity) but standard model for radial transport in a tokamak. The finite-volume discretization is a standard and robust numerical method. The concept of Jacobi iteration and diagonal equilibration (scaling) are fundamental topics in numerical linear algebra. The problem is scientifically and mathematically sound.\n2.  **Well-Posed:** All terms are defined, and the task is to derive a specific quantity ($\\Delta \\rho$) based on these definitions. A unique answer can be determined.\n3.  **Objective:** The language is formal and precise. No subjective elements are present.\n4.  **Complete and Consistent:** The problem provides all necessary definitions and relationships to proceed with a derivation.\n5.  **Not Pseudo-Profound:** While the setup is detailed, it does not artificially contrive conditions to bypass the core reasoning. The challenge lies in carefully executing the derivation based on the provided first principles, which may reveal a non-obvious simplification. This is a valid test of analytical skill.\n\n**Verdict:**\nThe problem is valid. I will now proceed with the solution.\n\n### Step 2: Derivation of the Original Jacobi Matrix $\\boldsymbol{M}_J$\n\nFirst, I will derive the explicit entries of the matrix $\\boldsymbol{A}$. To do this, I need to express $\\alpha_{i\\pm 1/2}$ and $V_i$ in terms of the grid index $i$.\nThe radial position of cell centers and faces are:\n$$\nr_i = i\\,\\Delta r\n$$\n$$\nr_{i\\pm \\frac{1}{2}} = (i\\pm\\frac{1}{2})\\,\\Delta r\n$$\nThe flux-surface area $A(r)$ is proportional to $r$: $A(r) = C_A r$, where $C_A = 4\\pi^2 R_0$ is a constant.\nThe control volume $V_i$ is:\n$$\nV_i = A(r_i)\\,\\Delta r = (C_A r_i)\\,\\Delta r = C_A (i\\,\\Delta r)\\,\\Delta r = C_A i (\\Delta r)^2\n$$\nThe face conductance $\\alpha_{i+1/2}$ is:\n$$\n\\alpha_{i+\\frac{1}{2}} = \\chi\\frac{A_{i+\\frac{1}{2}}}{\\Delta r} = \\chi\\frac{C_A r_{i+\\frac{1}{2}}}{\\Delta r} = \\chi\\frac{C_A (i+\\frac{1}{2})\\Delta r}{\\Delta r} = C_A \\chi (i+\\frac{1}{2})\n$$\nSimilarly, for the other face:\n$$\n\\alpha_{i-\\frac{1}{2}} = C_A \\chi (i-\\frac{1}{2})\n$$\nNow, I can compute the entries of the matrix $\\boldsymbol{A}$.\nThe diagonal entries are:\n$$\nA_{ii} = \\frac{\\alpha_{i+\\frac{1}{2}} + \\alpha_{i-\\frac{1}{2}}}{V_i} = \\frac{C_A \\chi (i+\\frac{1}{2}) + C_A \\chi (i-\\frac{1}{2})}{C_A i (\\Delta r)^2} = \\frac{C_A \\chi (2i)}{C_A i (\\Delta r)^2} = \\frac{2\\chi}{(\\Delta r)^2}\n$$\nCrucially, the diagonal entries $A_{ii}$ are constant for all $i=1,\\dots,N$. Let's denote this constant by $c$.\n$$\nc := \\frac{2\\chi}{(\\Delta r)^2}\n$$\nSo, the diagonal of $\\boldsymbol{A}$ is $\\boldsymbol{D} = \\mathrm{diag}(\\boldsymbol{A}) = c\\,\\boldsymbol{I}$, where $\\boldsymbol{I}$ is the identity matrix.\n\nThe off-diagonal entries are:\n$$\nA_{i,i+1} = -\\frac{\\alpha_{i+\\frac{1}{2}}}{V_i} = -\\frac{C_A \\chi (i+\\frac{1}{2})}{C_A i (\\Delta r)^2} = -\\frac{\\chi}{(\\Delta r)^2} \\frac{i+\\frac{1}{2}}{i} = -\\frac{c}{2} \\left(1 + \\frac{1}{2i}\\right)\n$$\n$$\nA_{i,i-1} = -\\frac{\\alpha_{i-\\frac{1}{2}}}{V_i} = -\\frac{C_A \\chi (i-\\frac{1}{2})}{C_A i (\\Delta r)^2} = -\\frac{\\chi}{(\\Delta r)^2} \\frac{i-\\frac{1}{2}}{i} = -\\frac{c}{2} \\left(1 - \\frac{1}{2i}\\right)\n$$\nThe Jacobi iteration matrix $\\boldsymbol{M}_J$ is defined as $\\boldsymbol{M}_{J} = \\boldsymbol{I} - \\boldsymbol{D}^{-1}\\boldsymbol{A}$. Since $\\boldsymbol{D} = c\\,\\boldsymbol{I}$, its inverse is $\\boldsymbol{D}^{-1} = \\frac{1}{c}\\boldsymbol{I}$.\n$$\n\\boldsymbol{M}_J = \\boldsymbol{I} - \\left(\\frac{1}{c}\\boldsymbol{I}\\right)\\boldsymbol{A} = \\boldsymbol{I} - \\frac{1}{c}\\boldsymbol{A}\n$$\nThe entries of $\\boldsymbol{M}_J$ are:\n- $(\\boldsymbol{M}_J)_{ii} = 1 - \\frac{1}{c}A_{ii} = 1 - \\frac{1}{c}c = 0$.\n- $(\\boldsymbol{M}_J)_{i,i+1} = -\\frac{1}{c} A_{i,i+1} = -\\frac{1}{c} \\left(-\\frac{c}{2} \\left(1 + \\frac{1}{2i}\\right)\\right) = \\frac{1}{2} \\left(1 + \\frac{1}{2i}\\right)$.\n- $(\\boldsymbol{M}_J)_{i,i-1} = -\\frac{1}{c} A_{i,i-1} = -\\frac{1}{c} \\left(-\\frac{c}{2} \\left(1 - \\frac{1}{2i}\\right)\\right) = \\frac{1}{2} \\left(1 - \\frac{1}{2i}\\right)$.\n\n### Step 3: Derivation of the Equilibrated Jacobi Matrix $\\boldsymbol{M}_J^{(\\mathrm{eq})}$\n\nThe equilibration is performed by left-multiplying the system by the diagonal matrix $\\boldsymbol{R} = \\mathrm{diag}(V_1, \\dots, V_N)$. The new system matrix is $\\boldsymbol{A}' = \\boldsymbol{R}\\boldsymbol{A}$.\nThe Jacobi matrix for this new system is $\\boldsymbol{M}_{J}^{(\\mathrm{eq})} = \\boldsymbol{I} - (\\mathrm{diag}(\\boldsymbol{A}'))^{-1} \\boldsymbol{A}'$.\nLet's first find the diagonal of $\\boldsymbol{A}' = \\boldsymbol{R}\\boldsymbol{A}$. Let $\\boldsymbol{D}' = \\mathrm{diag}(\\boldsymbol{A}')$.\nThe diagonal entries of $\\boldsymbol{A}'$ are:\n$$\nA'_{ii} = R_{ii} A_{ii} = V_i A_{ii}\n$$\nSince we found that $A_{ii} = c$ for all $i$, we have:\n$$\nA'_{ii} = V_i c\n$$\nThus, the diagonal of the scaled matrix is $\\boldsymbol{D}' = \\mathrm{diag}(V_1 c, V_2 c, \\dots, V_N c) = c\\,\\mathrm{diag}(V_1, \\dots, V_N) = c\\,\\boldsymbol{R}$.\nThe inverse of $\\boldsymbol{D}'$ is $(\\boldsymbol{D}')^{-1} = (c\\boldsymbol{R})^{-1} = \\frac{1}{c}\\boldsymbol{R}^{-1}$.\nNow, we can write the expression for the equilibrated Jacobi matrix:\n$$\n\\boldsymbol{M}_{J}^{(\\mathrm{eq})} = \\boldsymbol{I} - (\\boldsymbol{D}')^{-1} \\boldsymbol{A}' = \\boldsymbol{I} - \\left(\\frac{1}{c}\\boldsymbol{R}^{-1}\\right) (\\boldsymbol{R}\\boldsymbol{A})\n$$\nSince $\\boldsymbol{R}$ is a diagonal matrix, it is invertible, and $\\boldsymbol{R}^{-1}\\boldsymbol{R} = \\boldsymbol{I}$. Therefore, the expression simplifies to:\n$$\n\\boldsymbol{M}_{J}^{(\\mathrm{eq})} = \\boldsymbol{I} - \\frac{1}{c}(\\boldsymbol{R}^{-1}\\boldsymbol{R})\\boldsymbol{A} = \\boldsymbol{I} - \\frac{1}{c}\\boldsymbol{I}\\boldsymbol{A} = \\boldsymbol{I} - \\frac{1}{c}\\boldsymbol{A}\n$$\n\n### Step 4: Comparison and Final Calculation\n\nBy comparing the derived expressions, we find that the two Jacobi iteration matrices are identical:\n$$\n\\boldsymbol{M}_{J}^{(\\mathrm{eq})} = \\boldsymbol{I} - \\frac{1}{c}\\boldsymbol{A}\n$$\n$$\n\\boldsymbol{M}_{J} = \\boldsymbol{I} - \\frac{1}{c}\\boldsymbol{A}\n$$\nTherefore, $\\boldsymbol{M}_{J}^{(\\mathrm{eq})} = \\boldsymbol{M}_{J}$.\n\nThis result arises from the specific feature of the problem's discretization: the diagonal entries of the original matrix $\\boldsymbol{A}$ are constant ($A_{ii}=c$). This causes the effect of the scaling matrix $\\boldsymbol{R}$ on the diagonal, `diag(RA)`, to be perfectly cancelled by the premultiplication of `RA` with its inverse diagonal `diag(RA)^(-1)`.\n\nSince the matrices are identical, their eigenvalues must be identical. The spectral radius $\\rho(\\cdot)$ of a matrix is the maximum of the absolute values of its eigenvalues. Consequently, their spectral radii must also be identical:\n$$\n\\rho(\\boldsymbol{M}_{J}^{(\\mathrm{eq})}) = \\rho(\\boldsymbol{M}_{J})\n$$\nThe problem asks for the change in the spectral radius, $\\Delta \\rho$:\n$$\n\\Delta \\rho = \\rho(\\boldsymbol{M}_{J}^{(\\mathrm{eq})}) - \\rho(\\boldsymbol{M}_{J}) = \\rho(\\boldsymbol{M}_{J}) - \\rho(\\boldsymbol{M}_{J}) = 0\n$$\nThus, for this specific problem, the diagonal row equilibration has no effect on the Jacobi iteration matrix or its spectral radius.",
            "answer": "$$\n\\boxed{0}\n$$"
        }
    ]
}