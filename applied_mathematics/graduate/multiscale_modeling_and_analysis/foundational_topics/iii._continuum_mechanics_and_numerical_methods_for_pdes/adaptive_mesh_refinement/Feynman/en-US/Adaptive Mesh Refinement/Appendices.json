{
    "hands_on_practices": [
        {
            "introduction": "The primary motivation for using Adaptive Mesh Refinement (AMR) is computational efficiency, especially for problems with highly localized features. This practice moves beyond theoretical arguments by having you numerically verify the superior scaling of AMR compared to uniform refinement. By implementing a simple refinement strategy focused on a synthetic point singularity, you will generate data to show how the number of required grid cells grows far more slowly, directly demonstrating the power of AMR in reducing computational complexity for a given accuracy .",
            "id": "3730591",
            "problem": "Consider the one-dimensional domain $[0,1]$ and a synthetic benchmark that models a localized singularity at $x=0$ via a weight function. We study Adaptive Mesh Refinement (AMR) in multiscale modeling from first principles by tracking how the number of degrees of freedom scales with the smallest spatial scale resolved. The goal is to demonstrate, through a scaling argument and numerical validation, that AMR reduces computational complexity from $O(N)$ to $O(N^{\\alpha})$ with $\\alpha<1$ when singular behavior is localized to a set of measure zero. The program must compute effective complexity exponents from data generated by uniform refinement and an AMR strategy and return aggregated exponents for a specified test suite.\n\nFundamental base for scaling:\n- The domain $[0,1]$ discretized by a partition into intervals yields a number of degrees of freedom proportional to the number of intervals, denoted $N$.\n- For uniform refinement to achieve a minimum cell size $h_{\\min}$, the number of intervals satisfies $N \\approx 1/h_{\\min}$, which implies that achieving resolution at scale $h_{\\min}$ requires $O(1/h_{\\min})$ intervals, i.e., complexity proportional to the inverse of the smallest resolved scale.\n- An AMR strategy that refines only a vanishing neighborhood of a localized singularity, using binary bisection, produces a hierarchy of local refinements. If only the interval containing the singularity is bisected at each step, then after $L$ local refinements the smallest cell size satisfies $h_{\\min} \\approx h_{0} 2^{-L}$, where $h_{0}$ is the initial cell size near the singularity. Therefore, $L \\approx \\log_{2}(h_{0}/h_{\\min})$. The total number of intervals is $N \\approx N_{0} + L$, where $N_{0}$ is the initial number of intervals. As $h_{\\min} \\to 0$, $N \\sim \\log(1/h_{\\min})$, which can be recast as $N \\sim (1/h_{\\min})^{\\alpha}$ with an effective exponent $\\alpha \\to 0$. More generally, if refinement targets a lower-dimensional set of singularity of dimension $d_{s}$ in a $D$-dimensional domain, then the refined region’s measure scales like $h_{\\min}^{D-d_{s}}$, and the total number of intervals (or cells) scales like $(1/h_{\\min})^{d_{s}}$ up to logarithmic factors, so the effective exponent $\\alpha = d_{s}/D < 1$ for $d_{s}<D$.\n\nBenchmark definition:\n- Use a weight-based refinement criterion that is monotone increasing as $x \\to 0$, emulating residual concentration near a point singularity. Define the cell-wise indicator as\n$$\n\\eta_{i} = \\left(x_{i}^{c} + \\varepsilon \\right)^{-\\gamma},\n$$\nwhere $x_{i}^{c}$ is the midpoint of interval $i$, $\\varepsilon>0$ is a small regularization constant, and $\\gamma>0$ controls the sharpness of localization. At each refinement step, bisect the interval with the largest $\\eta_{i}$.\n- For a target smallest resolved scale characterized by $M$ such that $h_{\\min} \\leq 1/M$, run the AMR strategy starting from a uniform partition into $N_{0}$ intervals of size $h_{0}=1/N_{0}$, and count the final number of intervals $N_{\\text{AMR}}(M)$ when the condition $h_{\\min} \\leq 1/M$ is first met.\n- For uniform refinement, $N_{\\text{uni}}(M)=M$ intervals exactly meet $h_{\\min}=1/M$.\n\nComplexity estimation protocol:\n- For a sequence of target resolutions $\\{M_{k}\\}$, compute $(\\log M_{k}, \\log N_{\\text{AMR}}(M_{k}))$ and estimate the effective exponent $\\alpha$ as the slope of the least-squares fit line for $\\log N$ versus $\\log M$. For uniform refinement, the slope is exactly $1$.\n- Numerically validated $\\alpha<1$ demonstrates sublinear growth of $N$ with refinement $M$ when refinement is localized.\n\nYour task:\n- Implement the AMR strategy described above, using bisection of the interval with the maximum indicator $\\eta_{i}$ at each step, until $h_{\\min} \\leq 1/M$ for each $M$ in the test suite.\n- For each test case, estimate the effective exponent $\\alpha$ by fitting a line to $(\\log M_{k}, \\log N_{\\text{AMR}}(M_{k}))$.\n- The program must produce a single line of output containing the estimated $\\alpha$ values for all test cases as a comma-separated list enclosed in square brackets.\n\nTest suite:\n- Case $1$: $\\gamma=2.0$, $\\varepsilon=10^{-12}$, initial partition $N_{0}=8$, and $M$ values $\\{64,256,1024,4096,16384\\}$.\n- Case $2$: $\\gamma=4.0$, $\\varepsilon=10^{-12}$, initial partition $N_{0}=8$, and $M$ values $\\{64,256,1024,4096,16384\\}$.\n- Case $3$: $\\gamma=1.0$, $\\varepsilon=10^{-12}$, initial partition $N_{0}=16$, and $M$ values $\\{64,256,1024,4096,16384\\}$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result1,result2,result3]$).\n- Each $result$ must be a floating-point number representing the estimated $\\alpha$ for the corresponding test case. Values should be printed in standard Python float formatting; no units are involved.\n\nAll quantities in this problem are expressed in pure mathematical terms, without physical units, and angles are not involved.",
            "solution": "The analysis of the provided problem begins with a validation of its premises and structure, which are found to be sound and well-posed within the domain of numerical analysis and multiscale modeling. The problem asks for a numerical validation of the computational complexity scaling of an Adaptive Mesh Refinement (AMR) strategy applied to a one-dimensional problem with a localized singularity.\n\nThe fundamental principle being investigated is that AMR can significantly reduce the computational cost, measured by the number of degrees of freedom ($N$), for resolving problems with localized features. For a spatial domain of dimension $D$ with singularities confined to a set of dimension $d_s < D$, the number of mesh cells $N$ required to achieve a minimum resolution of $h_{\\min}$ is expected to scale as $N \\sim (1/h_{\\min})^{d_s}$, disregarding logarithmic factors. This is in contrast to uniform mesh refinement, where resolving the entire domain to a scale $h_{\\min}$ requires $N \\sim (1/h_{\\min})^D$. The efficiency of AMR stems from the fact that the effective scaling exponent, $\\alpha = d_s/D$, is less than $1$. In the specific problem, we have a point singularity at $x=0$ in a one-dimensional domain $[0, 1]$. Thus, $d_s=0$ and $D=1$, yielding a theoretical exponent of $\\alpha = 0$. This implies that $N$ should grow much more slowly than any positive power of $M = 1/h_{\\min}$, specifically, logarithmically: $N \\sim \\log(M)$.\n\nOur task is to implement the specified AMR algorithm and numerically estimate the effective exponent $\\alpha$ by fitting a power law, $N(M) \\approx C M^{\\alpha}$, to the simulation data. This is equivalent to performing a linear regression on the log-transformed data: $\\log(N) = \\alpha \\log(M) + \\log(C)$. The slope of this line provides the estimate for $\\alpha$.\n\nThe AMR algorithm is defined as follows:\n$1$. Initialize the domain $[0, 1]$ with a uniform mesh of $N_0$ intervals.\n$2$. For a given target resolution parameter $M$, enter a refinement loop. The loop continues until the minimum interval width, $h_{\\min}$, is less than or equal to $1/M$.\n$3$. Within the loop, compute a refinement indicator $\\eta_i$ for each interval $i$. The indicator is given by the function $\\eta_i = (x_i^c + \\varepsilon)^{-\\gamma}$, where $x_i^c$ is the midpoint of interval $i$, $\\varepsilon > 0$ is a small regularization parameter, and $\\gamma>0$ dictates the sharpness of the indicator.\n$4$. Identify the interval with the maximum indicator value, $\\eta_{\\max}$. The choice of indicator function, being a monotonically decreasing function of $x_i^c$, ensures that refinement is concentrated where $x_i^c$ is smallest, i.e., near the singularity at $x=0$.\n$5$. Bisect this target interval, creating two new intervals and increasing the total count $N$ by one.\n$6$. Once the termination condition $h_{\\min} \\le 1/M$ is met, the simulation for that $M$ value concludes. The final number of intervals, $N_{\\text{AMR}}(M)$, is recorded.\n\nThis procedure is repeated for a sequence of increasing resolution targets $\\{M_k\\}$. The resulting data pairs $(M_k, N_{\\text{AMR}}(M_k))$ are collected. To estimate $\\alpha$, we compute the arrays $x = [\\log(M_k)]$ and $y = [\\log(N_{\\text{AMR}}(M_k))]$. A linear least-squares fit is applied to find the slope of the best-fit line through these points, which serves as our numerical estimate of $\\alpha$.\n\nThe implementation will consist of a primary function to iterate through the specified test cases. For each case, it will call a sub-function that executes the AMR simulation for each required $M_k$. This sub-function will manage the mesh, represented as a sorted array of boundary points, and perform the iterative refinement process. After collecting the data, the main function will use the `numpy.polyfit` function to perform the linear regression and extract the slope $\\alpha$. The computed exponents for all test cases will be aggregated and printed in the required format. The expected values for $\\alpha$ are small positive numbers, close to the theoretical value of $0$, reflecting the logarithmic growth of $N$ with $M$.",
            "answer": "```python\nimport numpy as np\n\ndef run_amr_simulation(gamma, epsilon, N0, M):\n    \"\"\"\n    Runs the Adaptive Mesh Refinement (AMR) simulation for a single test case.\n\n    Args:\n        gamma (float): The exponent in the indicator function.\n        epsilon (float): The regularization constant.\n        N0 (int): The initial number of uniform intervals.\n        M (int): The target resolution parameter, such that h_min <= 1/M.\n\n    Returns:\n        int: The final number of intervals (degrees of freedom).\n    \"\"\"\n    # Initialize the mesh as a sorted array of boundary points.\n    boundaries = np.linspace(0.0, 1.0, N0 + 1, dtype=np.float64)\n\n    while True:\n        # Calculate cell widths and find the minimum.\n        h = np.diff(boundaries)\n        h_min = np.min(h)\n\n        # Check for termination condition.\n        if h_min <= 1.0 / M:\n            break\n\n        # Calculate cell midpoints.\n        midpoints = boundaries[:-1] + h / 2.0\n        \n        # Compute the refinement indicator for each cell.\n        indicators = (midpoints + epsilon)**(-gamma)\n        \n        # Find the index of the cell with the maximum indicator to be refined.\n        idx_to_refine = np.argmax(indicators)\n        \n        # Bisect the chosen cell by inserting a new boundary point at its midpoint.\n        new_boundary = (boundaries[idx_to_refine] + boundaries[idx_to_refine + 1]) / 2.0\n        boundaries = np.insert(boundaries, idx_to_refine + 1, new_boundary)\n        \n    # The number of intervals is one less than the number of boundary points.\n    return len(boundaries) - 1\n\ndef solve():\n    \"\"\"\n    Main function to execute the test suite and compute effective complexity exponents.\n    \"\"\"\n    test_cases = [\n        # (gamma, epsilon, N0, M_values)\n        (2.0, 1e-12, 8, [64, 256, 1024, 4096, 16384]),\n        (4.0, 1e-12, 8, [64, 256, 1024, 4096, 16384]),\n        (1.0, 1e-12, 16, [64, 256, 1024, 4096, 16384]),\n    ]\n\n    results = []\n    for gamma, epsilon, N0, M_values in test_cases:\n        \n        N_amr_values = []\n        for M in M_values:\n            # Run the AMR simulation for each M and store the resulting cell count.\n            n_amr = run_amr_simulation(gamma, epsilon, N0, M)\n            N_amr_values.append(n_amr)\n\n        # Prepare data for log-log regression.\n        # Ensure data types are float for calculations.\n        log_M = np.log(np.array(M_values, dtype=np.float64))\n        log_N = np.log(np.array(N_amr_values, dtype=np.float64))\n        \n        # Perform linear regression: log(N) = alpha * log(M) + intercept.\n        # np.polyfit returns coefficients [slope, intercept].\n        alpha = np.polyfit(log_M, log_N, 1)[0]\n        results.append(alpha)\n\n    # Format the final output as a comma-separated list in brackets.\n    print(f\"[{','.join(f'{r}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A critical component of any conservative AMR algorithm is the prolongation operator, which transfers data from a coarse grid to a newly created fine grid. This transfer must be conservative, meaning the total quantity in a coarse cell equals the sum of the quantities in its child fine cells, to respect underlying physical laws. This exercise challenges you to implement a higher-order, conservative prolongation scheme based on local polynomial reconstruction, a fundamental technique in modern finite volume methods for accurately handling inter-grid data transfer .",
            "id": "3730542",
            "problem": "Consider a one-dimensional uniform mesh on a periodic domain of length $L$, partitioned into $N$ coarse cells with cell width $h = L/N$ and centers at $x_i = \\left(i + \\tfrac{1}{2}\\right) h$ for $i = 0, 1, \\dots, N-1$. Let the coarse cell averages be denoted by $\\bar{U}_i$, where $\\bar{U}_i$ approximates the mean of an underlying sufficiently smooth function $u(x)$ over the coarse cell interval $[x_{i-\\frac{1}{2}}, x_{i+\\frac{1}{2}}]$. Adaptive Mesh Refinement (AMR) requires prolongation from coarse to fine grids such that the fine cell averages are consistent with conservation.\n\nYour task is to construct a conservative prolongation operator using a higher-order reconstruction within each coarse cell. Specifically:\n\n- Within each coarse cell $i$, reconstruct a polynomial $p_i(x)$ of at least degree two such that the mean of $p_i(x)$ over $[x_{i-\\frac{1}{2}}, x_{i+\\frac{1}{2}}]$ equals $\\bar{U}_i$. Use only information from $\\bar{U}_{i-1}$, $\\bar{U}_i$, and $\\bar{U}_{i+1}$ with periodic indexing for $i=-1$ and $i=N$.\n- Refine each coarse cell into $r$ equal-width fine subcells. For each fine subcell, compute the fine cell average as the exact average of the reconstructed $p_i(x)$ over that subcell.\n- Verify exact conservation for constant states by checking that, for each coarse cell, the sum of the fine subcell averages multiplied by the fine width equals the coarse cell average multiplied by the coarse width. This verification must be purely numerical using floating-point arithmetic.\n\nBase your construction only on fundamental definitions from the finite volume method and central difference approximations that are consistent with multiscale modeling. Do not assume any pre-derived prolongation formula beyond the requirement that the reconstruction be at least quadratic and conservative in the sense defined above.\n\nAngle units for any trigonometric functions must be in radians.\n\nYour program must implement the above and compute the following metrics for the provided test suite:\n\n- For each test case, compute the maximum absolute conservation error across all coarse cells, defined as\n$$\n\\max_i \\left| \\left(\\sum_{k=0}^{r-1} \\bar{u}_{i}^{(k)} \\cdot \\tfrac{h}{r}\\right) - \\bar{U}_i \\cdot h \\right|,\n$$\nwhere $\\bar{u}_{i}^{(k)}$ is the fine subcell average in subcell $k$ of coarse cell $i$.\n- For non-constant underlying functions, compute the maximum absolute reconstruction error across all fine subcells over the entire domain, defined as\n$$\n\\max_{i,k} \\left| \\bar{u}_{i}^{(k)} - \\frac{1}{\\Delta x_f} \\int_{x_{i,k}^{L}}^{x_{i,k}^{R}} u(x)\\, dx \\right|,\n$$\nwhere $\\Delta x_f = h/r$ is the fine subcell width and $[x_{i,k}^{L}, x_{i,k}^{R}]$ is the $k$-th fine subcell interval in coarse cell $i$.\n\nTest Suite:\nUse the following four test cases. For all cases, the domain length is $L=1$ and periodic boundaries are assumed. Angles in trigonometric functions are in radians.\n\n1. Constant state:\n   - $N = 3$,\n   - $r = 4$,\n   - $u(x) = C$ with $C = 1.25$.\n   - Output per test case: a list $[\\text{cons\\_err}, \\text{max\\_err}, \\text{const\\_exact}]$, where $\\text{cons\\_err}$ is the maximum absolute conservation error as a float, $\\text{max\\_err}$ is the maximum absolute reconstruction error as a float, and $\\text{const\\_exact}$ is a boolean indicating whether every fine subcell average equals $C$ within a strict numerical tolerance.\n\n2. Linear state:\n   - $N = 10$,\n   - $r = 3$,\n   - $u(x) = \\alpha + \\beta x$ with $\\alpha = 2.0$ and $\\beta = -0.7$.\n   - Output per test case: a list $[\\text{cons\\_err}, \\text{max\\_err}]$.\n\n3. Quadratic state:\n   - $N = 12$,\n   - $r = 2$,\n   - $u(x) = a x^2 + b x + c$ with $a = -2.0$, $b = 1.0$, and $c = 1.0$.\n   - Output per test case: a list $[\\text{cons\\_err}, \\text{max\\_err}]$.\n\n4. Sinusoidal state:\n   - $N = 16$,\n   - $r = 5$,\n   - $u(x) = \\sin(2\\pi k x)$ with $k = 3$.\n   - Output per test case: a list $[\\text{cons\\_err}, \\text{max\\_err}]$.\n\nCoarse cell averages $\\bar{U}_i$ must be computed exactly from $u(x)$ wherever analytic integration is straightforward. For the sinusoidal case, use the exact integral expression for the mean over each interval.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each test case’s result represented as a list as specified above. For example, the output must resemble\n$[ [\\dots], [\\dots], [\\dots], [\\dots] ]$\nwith no additional text.\n\nAll numeric answers must be dimensionless floats or booleans. No physical units are involved in this problem, and angles are specified in radians.",
            "solution": "The problem asks for the construction and evaluation of a conservative, second-order prolongation operator for an Adaptive Mesh Refinement (AMR) framework on a one-dimensional periodic domain. The core of the task is to reconstruct a quadratic polynomial within each coarse grid cell and then use it to define volume-averaged values on a refined grid within that cell, ensuring conservation.\n\nThe solution proceeds in several steps for each test case:\n1.  **Mesh and Initial Data**: A uniform coarse grid with $N$ cells of width $h = L/N$ is defined on the domain $[0, L]$. For a given underlying function $u(x)$, the exact coarse cell averages $\\bar{U}_i$ are computed for each cell $i \\in \\{0, ..., N-1\\}$.\n    $$\n    \\bar{U}_i = \\frac{1}{h} \\int_{ih}^{(i+1)h} u(x) \\, dx\n    $$\n2.  **Polynomial Reconstruction**: For each coarse cell $i$, we construct a quadratic polynomial $p_i(x)$ that is used to represent the sub-cell distribution of the quantity. To maintain consistency with the local coarse data, this polynomial must satisfy the conservation property: its average over the coarse cell must be $\\bar{U}_i$. We represent the polynomial centered at the coarse cell midpoint $x_i = (i + \\frac{1}{2})h$:\n    $$\n    p_i(x) = a_i (x - x_i)^2 + b_i (x - x_i) + c_i\n    $$\n    The conservation constraint is imposed by integrating $p_i(x)$ over the cell $[x_i - h/2, x_i + h/2]$:\n    $$\n    \\bar{U}_i = \\frac{1}{h} \\int_{x_i - h/2}^{x_i + h/2} \\left[a_i (x - x_i)^2 + b_i (x - x_i) + c_i\\right] \\, dx = a_i \\frac{h^2}{12} + c_i\n    $$\n    This gives one equation for the three coefficients $a_i, b_i, c_i$. The remaining two degrees of freedom are determined using information from the neighboring cells, as specified. We use central difference approximations for the first and second derivatives of the underlying function at the cell center $x_i$, based on the coarse cell averages $\\bar{U}_{i-1}, \\bar{U}_i, \\bar{U}_{i+1}$.\n    The second derivative $p_i''(x_i) = 2a_i$ is approximated as:\n    $$\n    2a_i \\approx \\frac{\\bar{U}_{i+1} - 2\\bar{U}_i + \\bar{U}_{i-1}}{h^2} \\implies a_i = \\frac{\\bar{U}_{i+1} - 2\\bar{U}_i + \\bar{U}_{i-1}}{2h^2}\n    $$\n    The first derivative $p_i'(x_i) = b_i$ is approximated as:\n    $$\n    b_i \\approx \\frac{\\bar{U}_{i+1} - \\bar{U}_{i-1}}{2h}\n    $$\n    With $a_i$ and $b_i$ determined, $c_i$ is found from the conservation constraint:\n    $$\n    c_i = \\bar{U}_i - a_i \\frac{h^2}{12}\n    $$\n    Periodic boundary conditions are used to find stencil values $\\bar{U}_{-1}$ and $\\bar{U}_{N}$. This uniquely defines the reconstruction polynomial $p_i(x)$ for each cell $i$. This method is a standard approach in higher-order finite volume methods.\n\n3.  **Prolongation**: Each coarse cell $i$ is divided into $r$ fine subcells of width $\\Delta x_f = h/r$. The $k$-th subcell, for $k \\in \\{0, ..., r-1\\}$, occupies the interval $[x_{i,k}^L, x_{i,k}^R] = [ih+k\\Delta x_f, ih+(k+1)\\Delta x_f]$. The prolonged fine cell average $\\bar{u}_{i}^{(k)}$ is computed by taking the exact average of the reconstructed polynomial $p_i(x)$ over this subcell:\n    $$\n    \\bar{u}_{i}^{(k)} = \\frac{1}{\\Delta x_f} \\int_{x_{i,k}^L}^{x_{i,k}^R} p_i(x) \\, dx\n    $$\n    This integral is computed analytically using the antiderivative of $p_i(x)$.\n\n4.  **Error Analysis**:\n    *   **Conservation Error**: This metric verifies that the prolongation operator is conservative. By construction, the sum of a quantity over the fine cells must equal the quantity in the coarse cell. The conservation error for a coarse cell $i$ is calculated numerically:\n        $$\n        E_{\\text{cons}, i} = \\left| \\left(\\sum_{k=0}^{r-1} \\bar{u}_{i}^{(k)} \\cdot \\Delta x_f\\right) - \\bar{U}_i \\cdot h \\right|\n        $$\n        Theoretically, since $\\sum_{k=0}^{r-1} \\int_{x_{i,k}^L}^{x_{i,k}^R} p_i(x) \\, dx = \\int_{ih}^{(i+1)h} p_i(x) \\, dx = \\bar{U}_i h$, this error should be zero up to floating-point precision. The maximum of $E_{\\text{cons}, i}$ across all coarse cells is reported.\n    *   **Reconstruction Error**: This metric measures the accuracy of the reconstruction. It is the maximum absolute difference between the computed fine cell averages and the true fine cell averages (obtained by integrating the original function $u(x)$):\n        $$\n        E_{\\text{recon}} = \\max_{i,k} \\left| \\bar{u}_{i}^{(k)} - \\frac{1}{\\Delta x_f} \\int_{x_{i,k}^{L}}^{x_{i,k}^{R}} u(x)\\, dx \\right|\n        $$\n    As demonstrated by analysis, this reconstruction method is exact for underlying functions $u(x)$ that are polynomials of degree up to $2$. Consequently, for the constant, linear, and quadratic test cases, both the conservation and reconstruction errors are expected to be near machine precision. For the sinusoidal case, a non-zero reconstruction error is expected, as the polynomial is only an approximation of the sine function.\n\nThe implementation proceeds by defining functions for each test case's $u(x)$ and its analytical integral. A general function processes each case by calculating coarse averages, reconstructing polynomials, computing fine averages, and evaluating the specified error metrics.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef process_case(N, r, u_integral_func, L=1.0, case_info=None):\n    \"\"\"\n    Processes a single test case for conservative prolongation.\n\n    Args:\n        N (int): Number of coarse cells.\n        r (int): Refinement ratio.\n        u_integral_func (callable): The antiderivative of the true function u(x).\n        L (float): Domain length.\n        case_info (dict, optional): Dictionary with special case parameters.\n\n    Returns:\n        list: A list containing the computed error metrics for the case.\n    \"\"\"\n    # 1. Mesh setup\n    h = L / N\n    dx_f = h / r\n    coarse_cell_bounds = np.linspace(0, L, N + 1)\n    coarse_cell_centers = coarse_cell_bounds[:-1] + h / 2\n\n    # 2. Compute exact coarse cell averages\n    U_bar = np.zeros(N)\n    for i in range(N):\n        x_L, x_R = coarse_cell_bounds[i], coarse_cell_bounds[i + 1]\n        U_bar[i] = (u_integral_func(x_R) - u_integral_func(x_L)) / h\n\n    # Use np.roll for efficient periodic boundary handling\n    U_bar_plus_1 = np.roll(U_bar, -1)\n    U_bar_minus_1 = np.roll(U_bar, 1)\n\n    conservation_errors = []\n    reconstruction_errors = []\n    all_fine_averages = []\n\n    # 3. Loop over each coarse cell for reconstruction and prolongation\n    for i in range(N):\n        x_i_center = coarse_cell_centers[i]\n        \n        # Get stencil values for cell i\n        U_im1, U_i, U_ip1 = U_bar_minus_1[i], U_bar[i], U_bar_plus_1[i]\n\n        # Reconstruct quadratic p_i(x) = a(x-x_i)^2 + b(x-x_i) + c\n        a_coeff = (U_ip1 - 2 * U_i + U_im1) / (2 * h**2)\n        b_coeff = (U_ip1 - U_im1) / (2 * h)\n        c_coeff = U_i - a_coeff * h**2 / 12\n\n        def F_poly_antiderivative(x):\n            \"\"\"Antiderivative of the reconstructed polynomial p_i(x).\"\"\"\n            dx = x - x_i_center\n            return (a_coeff / 3.0) * dx**3 + (b_coeff / 2.0) * dx**2 + c_coeff * dx\n\n        sum_fine_volume = 0.0\n\n        # Loop over fine subcells within the coarse cell i\n        for k in range(r):\n            # 4. Compute fine cell average from polynomial\n            x_fine_L = coarse_cell_bounds[i] + k * dx_f\n            x_fine_R = coarse_cell_bounds[i] + (k + 1) * dx_f\n\n            integral_p = F_poly_antiderivative(x_fine_R) - F_poly_antiderivative(x_fine_L)\n            u_fine_avg = integral_p / dx_f\n            all_fine_averages.append(u_fine_avg)\n            \n            sum_fine_volume += u_fine_avg * dx_f\n\n            # 5. Compute true fine average for error calculation\n            integral_u_true = u_integral_func(x_fine_R) - u_integral_func(x_fine_L)\n            u_true_fine_avg = integral_u_true / dx_f\n            \n            reconstruction_errors.append(np.abs(u_fine_avg - u_true_fine_avg))\n\n        # 6. Compute conservation error for the coarse cell\n        conservation_errors.append(np.abs(sum_fine_volume - U_i * h))\n        \n    max_cons_err = np.max(conservation_errors) if conservation_errors else 0.0\n    max_recon_err = np.max(reconstruction_errors) if reconstruction_errors else 0.0\n\n    if case_info and case_info.get(\"type\") == \"constant\":\n        C_val = case_info[\"C\"]\n        const_exact = np.allclose(all_fine_averages, C_val, rtol=0, atol=1e-14)\n        return [max_cons_err, max_recon_err, const_exact]\n        \n    return [max_cons_err, max_recon_err]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"N\": 3, \"r\": 4,\n            \"u_integral\": lambda x, C=1.25: C * x,\n            \"info\": {\"type\": \"constant\", \"C\": 1.25}\n        },\n        {\n            \"N\": 10, \"r\": 3,\n            \"u_integral\": lambda x, alpha=2.0, beta=-0.7: alpha * x + 0.5 * beta * x**2,\n            \"info\": None\n        },\n        {\n            \"N\": 12, \"r\": 2,\n            \"u_integral\": lambda x, a=-2.0, b=1.0, c=1.0: (a/3.0)*x**3 + (b/2.0)*x**2 + c*x,\n            \"info\": None\n        },\n        {\n            \"N\": 16, \"r\": 5,\n            \"u_integral\": lambda x, k=3: -np.cos(2 * np.pi * k * x) / (2 * np.pi * k),\n            \"info\": None\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        # Main logic to calculate the result for one case goes here.\n        result = process_case(case[\"N\"], case[\"r\"], case[\"u_integral\"], case_info=case[\"info\"])\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # The default string conversion for lists and booleans matches the required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Simply coupling an AMR framework to a standard PDE solver can introduce subtle but severe numerical instabilities. This practice explores a crucial interaction between grid refinement and the stability of explicit time-stepping schemes for diffusion equations. You will investigate how refinement interfaces can cause a naive time step to violate the Discrete Maximum Principle (DMP), leading to unphysical solutions, and then implement a robust fix based on a rigorous local stability analysis .",
            "id": "3095000",
            "problem": "Consider one-dimensional diffusion with constant diffusivity on a bounded interval, formulated by conservation of mass and Fick’s law. The conservation law states that the time rate of change of a scalar field $u(x,t)$ inside a control volume equals the net diffusive flux entering it. Fick’s law states that the diffusive flux is $J(x,t) = -D \\, \\partial u / \\partial x$, where $D$ is a positive constant diffusivity. Combining these gives the parabolic partial differential equation $u_t = D \\, u_{xx}$ for $x \\in [0,1]$ and $t \\ge 0$, with Dirichlet boundary conditions $u(0,t) = 0$ and $u(1,t) = 1$.\n\nThe Discrete Maximum Principle (DMP) for diffusion asserts that, under a monotone discretization, the discrete solution values remain within the range of boundary values for all time if the initial data is within that range. Adaptive Mesh Refinement (AMR) introduces multiple spatial step sizes that meet at refinement level transitions. Your task is to investigate how refinement level transitions affect the DMP under explicit time integration and to propose and implement a fix that prevents overshoots.\n\nStarting from the fundamental principles above, derive a consistent nonuniform-grid finite-difference or finite-volume semi-discrete form for $u_t$ at an interior node $x_i$ in terms of neighboring scalar values $u_{i-1}$ and $u_{i+1}$, the local diffusivity $D$, and the adjacent spacings $h_{i-1}$ and $h_i$, where $h_i = x_{i+1} - x_i$ and $h_{i-1} = x_i - x_{i-1}$. Then discretize in time by forward Euler with time step $\\Delta t$. Define overshoot as any discrete value $u_i^n$ that violates $u_i^n \\in [0,1]$ at any time level $n$. Using this base, do the following:\n\n- Implement two explicit time stepping strategies on a one-dimensional nonuniform grid generated by two refinement levels meeting at $x = 1/2$:\n  1. A naive global time step selected using only the coarse spacing $h_c$ (coarse-level stability consideration).\n  2. A level-aware fix that prevents overshoots by choosing a global time step that respects the strongest local constraint induced by refinement level transitions.\n\n- For each case, integrate from initial condition $u(x,0) = 0$ for $x \\in [0,1]$ while maintaining the Dirichlet boundary conditions $u(0,t) = 0$ and $u(1,t) = 1$ at each time step. Use a constant diffusivity $D = 1$ (nondimensional), and run a fixed number of forward Euler steps.\n\n- Determine whether an overshoot occurs for each strategy.\n\nYou must construct the nonuniform grid as follows. Let the domain be $[0,1]$. Use the coarse spacing $h_c$ to tile $[0,1/2]$, ensuring that $1/2$ is exactly a grid point. Use the fine spacing $h_f$ to tile $(1/2,1]$, ensuring that $1$ is exactly a grid point and there is a refinement transition at $x = 1/2$. In AMR tests, set $h_f = h_c / r$ for a refinement ratio $r \\in \\{2,4\\}$.\n\nYou must implement both strategies explicitly:\n- Naive strategy: choose $\\Delta t$ based only on $h_c$ and $D$.\n- Fix strategy: choose $\\Delta t$ based on a local constraint that accounts for adjacent spacings $h_{i-1}$ and $h_i$ at all interior nodes, selecting the most restrictive bound to ensure a monotone update at every node.\n\nYour program must evaluate the following test suite and report whether an overshoot occurs for each strategy:\n- Test $1$ (uniform baseline): $h_c = 1/64$, $r = 1$ (so $h_f = h_c$), number of steps $N = 50$.\n- Test $2$ (refinement ratio two): $h_c = 1/16$, $r = 2$, number of steps $N = 50$.\n- Test $3$ (refinement ratio four): $h_c = 1/16$, $r = 4$, number of steps $N = 50$.\n- Test $4$ (small-grid edge case): $h_c = 1/8$, $r = 2$, number of steps $N = 50$.\n\nIn all tests, take $D = 1$, enforce the Dirichlet boundary conditions at each step, and detect overshoot using a tolerance of $10^{-12}$, that is, declare overshoot if any discrete value is less than $0 - 10^{-12}$ or greater than $1 + 10^{-12}$ at any time during evolution.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets of lists, where each inner list contains two boolean values for one test in the order $[\\text{naive\\_overshoot}, \\text{fixed\\_overshoot}]$. For example, $[[\\text{True},\\text{False}],[\\text{False},\\text{False}],\\dots]$.",
            "solution": "We begin from conservation of mass and Fick’s law. Conservation of mass in one dimension states that the time rate of change of the scalar field $u(x,t)$ in a control volume $(x_{i-1/2}, x_{i+1/2})$ equals the net flux entering the volume, so\n$$\n\\frac{d}{dt}\\int_{x_{i-1/2}}^{x_{i+1/2}} u(x,t)\\,dx \\;=\\; J(x_{i-1/2},t) - J(x_{i+1/2},t),\n$$\nwhere $J(x,t) = -D \\, \\frac{\\partial u}{\\partial x}(x,t)$ is the diffusive flux by Fick’s law. Assuming $u$ is smooth and $D$ is constant, dividing by the volume length leads to the pointwise parabolic equation $u_t = D \\, u_{xx}$.\n\nTo derive a consistent nonuniform discrete spatial operator, we use either finite volume with linear reconstructions across cell faces or nonuniform finite differences. Let $x_i$ be a node, with $h_{i-1} = x_i - x_{i-1}$ and $h_i = x_{i+1} - x_i$. Approximating the gradient at faces by first-order differences yields face fluxes\n$$\nJ_{i+1/2} \\approx -D \\frac{u_{i+1} - u_i}{h_i}, \\quad J_{i-1/2} \\approx -D \\frac{u_i - u_{i-1}}{h_{i-1}}.\n$$\nA control-volume centered update based on flux balance over the staggered interval $(x_{i-1/2}, x_{i+1/2})$ with $x_{i\\pm 1/2}$ midway between nodes gives\n$$\nu_t(x_i,t) \\approx \\frac{J_{i-1/2} - J_{i+1/2}}{\\Delta x_i},\n$$\nwhere $\\Delta x_i$ is the effective control-volume length. For midpoint control volumes on a nonuniform grid, $\\Delta x_i = \\frac{h_{i-1} + h_i}{2}$. Substituting the flux approximations, we obtain the standard nonuniform second derivative approximation\n$$\nu_t(x_i,t) \\;\\approx\\; D \\cdot \\frac{2}{h_{i-1} + h_i} \\left( \\frac{u_{i+1} - u_i}{h_i} - \\frac{u_i - u_{i-1}}{h_{i-1}} \\right).\n$$\nThis operator is conservative and, for appropriate time stepping, monotone.\n\nDiscretizing in time by forward Euler with time step $\\Delta t$ yields, for interior nodes,\n$$\nu_i^{n+1} \\;=\\; u_i^n \\;+\\; \\Delta t \\cdot D \\cdot \\frac{2}{h_{i-1} + h_i} \\left( \\frac{u_{i+1}^n - u_i^n}{h_i} - \\frac{u_i^n - u_{i-1}^n}{h_{i-1}} \\right).\n$$\nRearranging, we can write it as a convex combination condition. Define\n$$\nc_{+} \\;=\\; \\Delta t \\cdot D \\cdot \\frac{2}{h_{i-1} + h_i} \\cdot \\frac{1}{h_i}, \\qquad\nc_{-} \\;=\\; \\Delta t \\cdot D \\cdot \\frac{2}{h_{i-1} + h_i} \\cdot \\frac{1}{h_{i-1}}.\n$$\nThen\n$$\nu_i^{n+1} \\;=\\; (1 - c_{+} - c_{-}) \\, u_i^n \\;+\\; c_{+} \\, u_{i+1}^n \\;+\\; c_{-} \\, u_{i-1}^n.\n$$\nFor the update to be monotone and to satisfy the Discrete Maximum Principle, we require nonnegative weights with the central coefficient not less than zero and the sum of off-diagonal coefficients not exceeding one:\n$$\nc_{+} \\ge 0, \\quad c_{-} \\ge 0, \\quad c_{+} + c_{-} \\le 1.\n$$\nThe first two are guaranteed if $\\Delta t \\ge 0$. The third provides the explicit stability and monotonicity condition. Using the definitions of $c_{\\pm}$,\n$$\nc_{+} + c_{-} = \\Delta t \\cdot D \\cdot \\frac{2}{h_{i-1} + h_i} \\left( \\frac{1}{h_i} + \\frac{1}{h_{i-1}} \\right)\n= \\Delta t \\cdot D \\cdot \\frac{2}{h_{i-1} h_i}.\n$$\nTherefore, a sufficient and necessary condition for $c_{+} + c_{-} \\le 1$ is\n$$\n\\Delta t \\;\\le\\; \\frac{h_{i-1} h_i}{2D}.\n$$\nOn a uniform grid with $h_{i-1} = h_i = h$, this reduces to the familiar bound $\\Delta t \\le h^2/(2D)$. At a refinement level transition where $h_{i-1}$ and $h_i$ differ, the most restrictive bound is the local product bound $\\Delta t \\le h_{i-1} h_i/(2D)$, which can be much smaller than the coarse-grid bound $h_c^2/(2D)$ if $h_i$ is fine.\n\nThis analysis demonstrates why a naive global time step chosen with only the coarse spacing $h_c$ can violate the Discrete Maximum Principle at refinement transitions: for a node with $h_{i-1} = h_c$ and $h_i = h_f$, the monotonicity requirement becomes $\\Delta t \\le h_c h_f/(2D)$. If $\\Delta t$ is set to be on the order of $h_c^2/(2D)$, then the ratio $(h_c^2)/(h_c h_f) = h_c / h_f$ equals the refinement ratio $r$, so $c_{+} + c_{-} \\approx r \\cdot \\text{CFL}$, which exceeds $1$ when $r > 1$ for typical Courant–Friedrichs–Lewy (CFL) factors, leading to negative central weight, non-M-matrix behavior, and potential overshoot.\n\nA principled fix is to choose a global time step that satisfies the strongest local constraint across all interior nodes:\n$$\n\\Delta t \\;=\\; \\theta \\cdot \\min_i \\left( \\frac{h_{i-1} h_i}{2D} \\right),\n$$\nwhere $\\theta \\in (0,1)$ is a safety factor such as $\\theta = 0.9$. This ensures $c_{+} + c_{-} \\le \\theta < 1$ everywhere, preserving monotonicity and the Discrete Maximum Principle regardless of refinement transitions. Alternatives include level subcycling (taking multiple fine-grid substeps per coarse step) or implicit monotone schemes, but the global minimum-product bound is a simple and effective fix compatible with single-rate explicit integration.\n\nAlgorithmic design:\n- Generate the nonuniform grid: tile $[0,1/2]$ with spacing $h_c$ so that $x = 1/2$ is a node; tile $(1/2,1]$ with spacing $h_f = h_c / r$ so that $x = 1$ is a node. Collect all nodes $x_i$.\n- Initialize $u_i^0 = 0$ for all interior nodes and enforce $u_0^n = 0$, $u_{N}^n = 1$ for all $n$, where $N$ is the last index.\n- For the naive strategy, set $\\Delta t_{\\text{naive}} = \\theta \\cdot h_c^2 / (2D)$ with $\\theta = 0.9$. For the fix strategy, set $\\Delta t_{\\text{fix}} = \\theta \\cdot \\min_i (h_{i-1} h_i / (2D))$ over interior nodes with the same $\\theta$.\n- At each time step, update interior nodes using the nonuniform explicit formula\n$$\nu_i^{n+1} = u_i^n + \\Delta t \\cdot D \\cdot \\frac{2}{h_{i-1} + h_i} \\left( \\frac{u_{i+1}^n - u_i^n}{h_i} - \\frac{u_i^n - u_{i-1}^n}{h_{i-1}} \\right),\n$$\nand re-enforce boundary values $u_0^{n+1} = 0$, $u_N^{n+1} = 1$.\n- Detect overshoot by checking whether any $u_i^n$ is less than $0 - 10^{-12}$ or greater than $1 + 10^{-12}$ during the integration.\n\nTest suite rationale:\n- Test $1$ (uniform baseline) verifies that both strategies respect the DMP on a uniform grid.\n- Test $2$ (refinement ratio $2$) and Test $3$ (refinement ratio $4$) stress the refinement transition. The naive coarse-based time step violates the local product bound at the transition and can cause overshoot, while the fix prevents it.\n- Test $4$ (small-grid edge case) ensures behavior is consistent even with few cells.\n\nThe final program implements these steps, runs the specified tests, and outputs a single line with a list of two booleans per test in the order $[\\text{naive\\_overshoot}, \\text{fixed\\_overshoot}]$. By construction, the fix strategy maintains $c_{+} + c_{-} \\le \\theta < 1$ at every interior node, establishing a monotone update and preventing overshoot, thereby preserving the Discrete Maximum Principle in the presence of refinement level transitions.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef build_grid(hc: float, r: int):\n    \"\"\"\n    Build a 1D nonuniform grid on [0,1] with a refinement transition at x=0.5.\n    Left half [0,0.5] uses spacing hc (coarse).\n    Right half (0.5,1] uses spacing hf = hc/r (fine), with r>=1.\n    Returns array of grid points x.\n    \"\"\"\n    if r < 1:\n        raise ValueError(\"Refinement ratio r must be >= 1\")\n    hf = hc if r == 1 else hc / r\n    # Ensure 0.5 is hit exactly by coarse spacing and 1.0 by fine spacing.\n    # For our test suite, these are chosen to be exact integers.\n    n_coarse = int(round(0.5 / hc))\n    if abs(n_coarse * hc - 0.5) > 1e-12 or n_coarse <= 0:\n        raise ValueError(\"0.5 must be an integer multiple of hc for this setup.\")\n    n_fine = int(round(0.5 / hf))\n    if abs(n_fine * hf - 0.5) > 1e-12 or n_fine <= 0:\n        raise ValueError(\"0.5 must be an integer multiple of hf for this setup.\")\n    # Left segment includes 0 and 0.5\n    x_left = np.linspace(0.0, 0.5, n_coarse + 1)\n    # Right segment excludes 0.5, includes 1.0\n    x_right = 0.5 + hf * np.arange(1, n_fine + 1)\n    x = np.concatenate([x_left, x_right])\n    return x\n\ndef explicit_step_nonuniform(u, x, dt, D=1.0):\n    \"\"\"\n    Perform one explicit forward Euler step for u_t = D u_xx on nonuniform grid x.\n    Dirichlet boundaries at endpoints are maintained (u[0], u[-1] fixed).\n    \"\"\"\n    u_new = u.copy()\n    h = np.diff(x)  # h[i] = x[i+1]-x[i]\n    # interior update\n    for i in range(1, len(x) - 1):\n        h_minus = h[i - 1]\n        h_plus = h[i]\n        denom = h_minus + h_plus\n        # Nonuniform second derivative via flux difference\n        lap = (2.0 / denom) * ((u[i + 1] - u[i]) / h_plus - (u[i] - u[i - 1]) / h_minus)\n        u_new[i] = u[i] + dt * D * lap\n    # Enforce Dirichlet boundaries\n    u_new[0] = u[0]\n    u_new[-1] = u[-1]\n    return u_new\n\ndef has_overshoot(u, tol=1e-12):\n    return (np.max(u) > 1.0 + tol) or (np.min(u) < 0.0 - tol)\n\ndef run_case(hc, r, N_steps, strategy, D=1.0, theta=0.9):\n    \"\"\"\n    Run one test case with given grid parameters and strategy:\n    strategy: 'naive' or 'fixed'.\n    Returns boolean indicating whether an overshoot occurred during evolution.\n    \"\"\"\n    x = build_grid(hc, r)\n    # Initialize u: zeros everywhere except boundary u(1)=1\n    u = np.zeros_like(x)\n    u[0] = 0.0\n    u[-1] = 1.0\n\n    # Compute time step\n    if strategy == 'naive':\n        dt = theta * (hc ** 2) / (2.0 * D)\n    elif strategy == 'fixed':\n        h = np.diff(x)\n        # local product bound min_i h_{i-1}*h_i/(2D)\n        # iterate interior nodes i=1..N-2\n        products = []\n        for i in range(1, len(x) - 1):\n            products.append(h[i - 1] * h[i])\n        dt = theta * (min(products) / (2.0 * D))\n    else:\n        raise ValueError(\"Unknown strategy\")\n\n    overshoot = False\n    for _ in range(N_steps):\n        u = explicit_step_nonuniform(u, x, dt, D=D)\n        # enforce boundaries each step\n        u[0] = 0.0\n        u[-1] = 1.0\n        if has_overshoot(u):\n            overshoot = True\n            break\n    return overshoot\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (hc, r, N_steps)\n    test_cases = [\n        (1.0/64.0, 1, 50),  # Test 1: uniform baseline\n        (1.0/16.0, 2, 50),  # Test 2: refinement ratio 2\n        (1.0/16.0, 4, 50),  # Test 3: refinement ratio 4\n        (1.0/8.0, 2, 50),   # Test 4: small-grid edge case\n    ]\n\n    results = []\n    for hc, r, N_steps in test_cases:\n        naive_overshoot = run_case(hc, r, N_steps, strategy='naive', D=1.0, theta=0.9)\n        fixed_overshoot = run_case(hc, r, N_steps, strategy='fixed', D=1.0, theta=0.9)\n        results.append([naive_overshoot, fixed_overshoot])\n\n    # Final print statement in the exact required format.\n    # A single line containing a comma-separated list enclosed in square brackets.\n    print(f\"[{','.join([f'[{str(pair[0]).title()},{str(pair[1]).title()}]' for pair in results])}]\")\n\nsolve()\n```"
        }
    ]
}