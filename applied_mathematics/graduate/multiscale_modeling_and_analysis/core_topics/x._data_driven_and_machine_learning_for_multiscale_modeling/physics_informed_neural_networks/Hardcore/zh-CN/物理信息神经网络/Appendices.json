{
    "hands_on_practices": [
        {
            "introduction": "在构建物理启发神经网络（PINN）时，一个首要任务是确保其解严格遵守问题的边界条件。一种强大而直接的方法是通过网络架构本身来强制执行这些条件，这种技术通常被称为“硬约束”编码。这项练习将指导您设计一个变换，将原始神经网络的输出映射到一个自动满足给定狄利克雷边界条件的新函数，从而确保物理上的一致性，而无需将其作为损失函数的一部分进行惩罚。",
            "id": "2126300",
            "problem": "在科学计算领域，物理信息神经网络（PINNs）已成为求解微分方程的强大工具。设计 PINN 的一个关键方面是确保其输出（即解的近似值）满足给定的边界条件。实现这一点的一种稳健方法是构造网络的最终输出函数，使其通过构造的方式满足这些条件。\n\n考虑空间域 $x \\in [0, L]$ 上的一个一维问题。一个神经网络提供了一个原始、无约束的输出函数，记为 $\\hat{u}_{NN}(x)$。我们希望使用这个网络来寻找一个微分方程的近似解 $u(x)$，该方程服从以下非齐次狄利克雷边界条件：\n$$u(0) = A$$\n$$u(L) = B$$\n其中，$A$、$B$ 和 $L > 0$ 是给定的实常数。\n\n你的任务是设计一个变换，将原始网络输出 $\\hat{u}_{NN}(x)$ 转换为一个新函数 $u_{NN}(x)$，作为最终的近似解。这个变换必须保证，无论网络产生什么样的函数 $\\hat{u}_{NN}(x)$，$u_{NN}(x)$ 都能严格满足指定的边界条件。\n\n请提供一个用原始网络输出 $\\hat{u}_{NN}(x)$ 以及参数 $x$、$L$、$A$ 和 $B$ 表示的 $u_{NN}(x)$ 的表达式。",
            "solution": "我们寻求一个变换，它能将原始网络输出 $\\hat{u}_{NN}(x)$ 映射到一个函数 $u_{NN}(x)$，使得对于任何 $\\hat{u}_{NN}(x)$，该函数都强制满足狄利克雷边界条件 $u_{NN}(0)=A$ 和 $u_{NN}(L)=B$。一种标准的构造方法是将 $u_{NN}(x)$ 分解为\n$$\nu_{NN}(x)=g(x)+s(x)\\,\\hat{u}_{NN}(x),\n$$\n其中 $g(x)$ 是任何满足边界条件的固定函数，而 $s(x)$ 是任何在两个边界处都为零的函数。具体来说，我们要求\n$$\ng(0)=A,\\quad g(L)=B,\\quad s(0)=0,\\quad s(L)=0.\n$$\n对于 $g(x)$，一个方便的选择是线性插值函数，\n$$\ng(x)=A\\left(1-\\frac{x}{L}\\right)+B\\left(\\frac{x}{L}\\right)=A+\\frac{B-A}{L}\\,x,\n$$\n以及简单的消失因子\n$$\ns(x)=x(L-x),\n$$\n它满足 $s(0)=0$ 和 $s(L)=0$。因此，定义\n$$\nu_{NN}(x)=A\\left(1-\\frac{x}{L}\\right)+B\\left(\\frac{x}{L}\\right)+x(L-x)\\,\\hat{u}_{NN}(x).\n$$\n为了验证边界条件，在 $x=0$ 和 $x=L$ 处求值：\n$$\nu_{NN}(0)=A\\left(1-0\\right)+B\\left(0\\right)+0\\cdot L\\,\\hat{u}_{NN}(0)=A,\n$$\n$$\nu_{NN}(L)=A\\left(1-1\\right)+B\\left(\\frac{L}{L}\\right)+L( L-L)\\,\\hat{u}_{NN}(L)=B.\n$$\n因此，对于任何 $\\hat{u}_{NN}(x)$，构造出的 $u_{NN}(x)$ 都严格满足 $u_{NN}(0)=A$ 和 $u_{NN}(L)=B$。",
            "answer": "$$\\boxed{A\\left(1-\\frac{x}{L}\\right)+B\\left(\\frac{x}{L}\\right)+x\\left(L-x\\right)\\hat{u}_{NN}(x)}$$"
        },
        {
            "introduction": "物理启发神经网络（PINN）的训练效果在很大程度上取决于所求解的偏微分方程（PDE）的数值特性。当方程中各项的量级差异巨大时，梯度下降优化过程可能会变得不稳定或收敛缓慢。这项练习探讨了一个关键的预处理步骤——无量纲化，它通过重新缩放变量来平衡方程中各项的贡献，从而改善优化问题的条件数，使训练过程更加稳健高效。",
            "id": "4235875",
            "problem": "一个信息物理系统中的一维粘性输运过程的数字孪生由粘性伯格斯方程建模，其物理形式为 $u_{t} + u u_{x} - \\nu u_{xx} = 0$，定义在长度为 $L$ 的空间区间和感兴趣的时间范围内。将通过在配置点上强制施加偏微分方程残差以及初始和边界数据，来训练一个物理信息神经网络（PINN）以学习该模型。为了在不同运行工况下促进稳健的优化，您需要对该模型进行无量纲化。\n\n从物理方程以及空间、时间和速度量级的特征尺度的核心定义出发，通过设置 $x = L \\,\\xi$，$t = T \\,\\tau$ 和 $u = U \\,v$ 引入无量纲变量，其中 $L$ 和 $U$ 是固定的特征长度和速度尺度，$T$ 是待确定的特征时间尺度。仅使用这些尺度变换和链式法则，推导关于 $v(\\xi,\\tau)$ 的无量纲形式方程。选择 $T$ 使得无量纲方程中时间导数项和对流非线性项的系数均为一量级。识别出乘以扩散项的、控制对流与扩散之间平衡的单一无量纲数群。\n\n然后，运用关于基于梯度的优化和条件数的基本原理，提供一个简明的论证，说明为何在数字孪生中实现时，对无量纲形式的模型训练物理信息神经网络（PINN）可以改善优化的稳定性和收敛性。\n\n您最终报告的答案必须是与此无量纲化相关的雷诺数的解析表达式，用 $U$、$L$ 和 $\\nu$ 表示。不要包含单位。如果您选择进行任何中间数值计算，请不要报告它们。最终答案必须是单一的闭式表达式。",
            "solution": "该问题要求对一维粘性伯格斯方程进行无量纲化，分析得到的无量纲数群，并论证为何此过程有利于物理信息神经网络（PINN）的训练。\n\n粘性伯格斯方程的物理形式如下：\n$$\nu_{t} + u u_{x} - \\nu u_{xx} = 0\n$$\n其中 $u(x,t)$ 是速度，$\\nu$ 是运动粘度，$u_t = \\frac{\\partial u}{\\partial t}$，$u_x = \\frac{\\partial u}{\\partial x}$，以及 $u_{xx} = \\frac{\\partial^2 u}{\\partial x^2}$。\n\n问题指定通过以下尺度关系引入无量纲变量 $\\xi$、$\\tau$ 和 $v$：\n$$\nx = L \\,\\xi, \\quad t = T \\,\\tau, \\quad u = U \\,v\n$$\n在此，$L$ 是特征长度尺度，$U$ 是特征速度尺度。特征时间尺度 $T$ 待确定。函数 $v$ 是无量纲速度，它依赖于无量纲空间和时间变量，即 $v = v(\\xi, \\tau)$。\n\n为了变换该偏微分方程（PDE），我们必须将 $u$ 对 $x$ 和 $t$ 的偏导数用 $v$ 对 $\\xi$ 和 $\\tau$ 的偏导数来表示。我们使用链式法则进行此变换。\n\n首先，我们求时间导数 $u_t$：\n$$\nu_{t} = \\frac{\\partial u}{\\partial t} = \\frac{\\partial (Uv)}{\\partial t} = U \\frac{\\partial v}{\\partial t} = U \\frac{\\partial v}{\\partial \\tau} \\frac{\\partial \\tau}{\\partial t}\n$$\n根据关系式 $t = T\\tau$，我们有 $\\frac{\\partial \\tau}{\\partial t} = \\frac{1}{T}$。因此，时间导数变为：\n$$\nu_{t} = \\frac{U}{T} \\frac{\\partial v}{\\partial \\tau} = \\frac{U}{T} v_{\\tau}\n$$\n\n接下来，我们求一阶空间导数 $u_x$：\n$$\nu_{x} = \\frac{\\partial u}{\\partial x} = \\frac{\\partial (Uv)}{\\partial x} = U \\frac{\\partial v}{\\partial x} = U \\frac{\\partial v}{\\partial \\xi} \\frac{\\partial \\xi}{\\partial x}\n$$\n根据关系式 $x = L\\xi$，我们有 $\\frac{\\partial \\xi}{\\partial x} = \\frac{1}{L}$。因此，一阶空间导数为：\n$$\nu_{x} = \\frac{U}{L} \\frac{\\partial v}{\\partial \\xi} = \\frac{U}{L} v_{\\xi}\n$$\n\n最后，我们求二阶空间导数 $u_{xx}$：\n$$\nu_{xx} = \\frac{\\partial^2 u}{\\partial x^2} = \\frac{\\partial}{\\partial x} \\left( \\frac{\\partial u}{\\partial x} \\right) = \\frac{\\partial}{\\partial x} \\left( \\frac{U}{L} v_{\\xi} \\right) = \\frac{U}{L} \\frac{\\partial v_{\\xi}}{\\partial x} = \\frac{U}{L} \\left( \\frac{\\partial v_{\\xi}}{\\partial \\xi} \\frac{\\partial \\xi}{\\partial x} \\right) = \\frac{U}{L} \\left( v_{\\xi\\xi} \\frac{1}{L} \\right)\n$$\n化简后得到：\n$$\nu_{xx} = \\frac{U}{L^2} v_{\\xi\\xi}\n$$\n\n现在，我们将这些导数表达式以及 $u$ 本身代入原始的粘性伯格斯方程中：\n$$\n\\left( \\frac{U}{T} v_{\\tau} \\right) + (Uv) \\left( \\frac{U}{L} v_{\\xi} \\right) - \\nu \\left( \\frac{U}{L^2} v_{\\xi\\xi} \\right) = 0\n$$\n合并系数，我们得到：\n$$\n\\frac{U}{T} v_{\\tau} + \\frac{U^2}{L} v v_{\\xi} - \\frac{\\nu U}{L^2} v_{\\xi\\xi} = 0\n$$\n为了得到系数为一量级的无量纲形式，我们可以将整个方程除以其中一个系数。一个标准的选择是除以非线性对流项的系数 $\\frac{U^2}{L}$，使其系数恰好为 $1$：\n$$\n\\left( \\frac{U}{T} \\frac{L}{U^2} \\right) v_{\\tau} + \\left( \\frac{U^2}{L} \\frac{L}{U^2} \\right) v v_{\\xi} - \\left( \\frac{\\nu U}{L^2} \\frac{L}{U^2} \\right) v_{\\xi\\xi} = 0\n$$\n化简系数得到：\n$$\n\\left( \\frac{L}{UT} \\right) v_{\\tau} + v v_{\\xi} - \\left( \\frac{\\nu}{UL} \\right) v_{\\xi\\xi} = 0\n$$\n问题要求选择特征时间尺度 $T$，使得时间导数项 ($v_\\tau$) 和对流非线性项 ($v v_\\xi$) 的系数均为一量级。我们已经将 $v v_\\xi$ 的系数设为 $1$。为了也将 $v_\\tau$ 的系数设为 $1$，我们必须有：\n$$\n\\frac{L}{UT} = 1 \\implies T = \\frac{L}{U}\n$$\n这个对 $T$ 的选择代表了对流时间尺度：即流体质点以特征速度 $U$ 行进特征长度 $L$ 所需的时间。\n\n将这个 $T$ 的表达式代回尺度变换后的方程，我们得到粘性伯格斯方程的最终无量纲形式：\n$$\nv_{\\tau} + v v_{\\xi} - \\left( \\frac{\\nu}{UL} \\right) v_{\\xi\\xi} = 0\n$$\n乘以扩散项 ($v_{\\xi\\xi}$) 的单一无量纲数群是 $\\frac{\\nu}{UL}$。这个数群是雷诺数的倒数。雷诺数是流体力学中的一个基本无量纲参数，用于量化惯性力与粘性力之比。雷诺数记作 $Re$，定义为：\n$$\nRe = \\frac{\\text{inertial forces}}{\\text{viscous forces}} \\propto \\frac{U^2/L}{\\nu U/L^2} = \\frac{UL}{\\nu}\n$$\n因此，无量纲系数为 $\\frac{1}{Re}$，方程为：\n$$\nv_{\\tau} + v v_{\\xi} - \\frac{1}{Re} v_{\\xi\\xi} = 0\n$$\n所以，雷诺数的表达式为 $\\frac{UL}{\\nu}$。\n\n关于为何在无量纲形式上训练PINN更有优势的论点，与优化问题的数值条件性有关。PINN的训练是通过最小化一个包含控制PDE残差的损失函数来完成的。对于原始物理方程，PDE残差为 $r = u_t + u u_x - \\nu u_{xx}$。损失函数通常是该残差在一组配置点上的均方误差，即 $\\mathcal{L}_{PDE} = \\langle (u_t + u u_x - \\nu u_{xx})^2 \\rangle$。\n\n在一个物理系统中，$u_t$、$u u_x$ 和 $\\nu u_{xx}$ 这些项的量级可能会相差好几个数量级。例如，在高速、低粘度的流动中，对流项 $u u_x$ 可能远大于粘性项 $\\nu u_{xx}$。当一个基于梯度的优化器（例如Adam）最小化损失函数时，损失函数相对于神经网络参数（权重和偏置）的梯度将被残差中最大的项所主导。这会造成一个病态的优化景观，其中包含陡峭的“峡谷”和平坦的“高原”，使优化器难以收敛到好的解。神经网络可能学会满足主导物理（如对流），而很大程度上忽略次主导物理（如扩散），从而导致模型不准确。\n\n无量纲化起到了一种预处理的作用。通过缩放变量，使得 $v, \\xi, \\tau$ 均为一量级，我们确保了无量纲残差 $r' = v_\\tau + v v_\\xi - \\frac{1}{Re} v_{\\xi\\xi}$ 中的各项具有可比的量级（除非 $Re$ 极大或极小，此时物理过程进入一个独特的渐近区域）。对于中等大小的 $Re$，所有系数均为一量级。这种项之间的平衡导致了条件更好的损失景观。源于每个物理项（时间变化、对流、扩散）的梯度被更均匀地缩放。因此，优化器可以在最小化所有物理效应的贡献方面取得更均衡的进展，从而实现更稳定、更高效的训练，并最终获得一个更精确的基于PINN的数字孪生。",
            "answer": "$$\n\\boxed{\\frac{UL}{\\nu}}\n$$"
        },
        {
            "introduction": "神经网络在学习函数时表现出一种固有的倾向，即优先拟合低频分量，而学习高频细节则要慢得多，这种现象被称为“谱偏差”（spectral bias）。理解谱偏差对于诊断和改进物理启发神经网络（PINN）的性能至关重要，尤其是在处理包含多尺度现象的问题时。通过这个编码实践，您将亲手实现一个PINN来求解一个具有已知多频分解解的常微分方程，并定量地观察到网络在训练早期是如何优先捕获低频信号的。",
            "id": "2427229",
            "problem": "您将实现一个完整的、可运行的程序，以展示物理信息神经网络（PINN）的光谱偏差（spectral bias）。其核心思想是训练一个PINN来求解一个一维边值问题，该问题的已知解是低频和高频正弦波的叠加，即 $u(x) = \\sin(x) + \\sin(25x)$，并定量观察在训练过程中哪个频率分量被首先学习。在整个过程中，角度必须使用弧度制。\n\n从以下具有周期性边界条件的物理一致常微分方程（ODE）开始：\n给定域 $x \\in [0, 2\\pi]$，考虑\n$$\nu''(x) + u(x) = -624 \\sin(25x),\n$$\n其周期性边界条件为\n$$\nu(0) = u(2\\pi), \\quad u'(0) = u'(2\\pi).\n$$\n一个经过充分检验的事实是，如果 $u(x) = \\sin(x) + \\sin(25x)$，那么 $u''(x) + u(x) = -624 \\sin(25x)$ 并且周期性边界条件成立。除了边界条件外，您不得使用任何关于 $u(x)$ 的标记训练数据；相反，您应在损失函数中使用ODE残差和边界残差，这是物理信息神经网络（PINN）的标准做法。\n\n构建一个具有 $H$ 个隐藏单元和双曲正切激活函数的单隐藏层神经网络 $u_{\\theta}(x)$ 作为试探解。定义隐藏层预激活值为 $z_i(x) = w_i x + b_i$（其中 $i \\in \\{1,\\dots,H\\}$），隐藏层激活值为 $h_i(x) = \\tanh(z_i(x))$，输出为\n$$\nu_{\\theta}(x) = \\sum_{i=1}^{H} a_i h_i(x) + c.\n$$\n使用链式法则和乘积法则，以闭式形式计算 $u_{\\theta}(x)$ 关于 $x$ 的一阶和二阶导数。回顾双曲正切函数及其导数的标准恒等式：\n$$\n\\tanh'(z) = \\operatorname{sech}^2(z), \\quad \\frac{d}{dz}\\operatorname{sech}^2(z) = -2\\operatorname{sech}^2(z)\\tanh(z), \\quad \\operatorname{sech}^2(z) = 1 - \\tanh^2(z).\n$$\n为配置点 $\\{x_n\\}_{n=1}^{N}$ 定义逐点物理残差为\n$$\nr_{\\text{phys}}(x_n;\\theta) = u_{\\theta}''(x_n) + u_{\\theta}(x_n) - \\left(-624 \\sin(25 x_n)\\right),\n$$\n以及周期性边界残差为\n$$\nr_{\\text{bc},1}(\\theta) = u_{\\theta}(0) - u_{\\theta}(2\\pi), \\quad r_{\\text{bc},2}(\\theta) = u_{\\theta}'(0) - u_{\\theta}'(2\\pi).\n$$\n使用带有边界权重 $\\lambda_{\\text{bc}}$ 的均方残差损失：\n$$\n\\mathcal{L}(\\theta) = \\frac{1}{N}\\sum_{n=1}^{N} r_{\\text{phys}}(x_n;\\theta)^2 + \\lambda_{\\text{bc}}\\left(r_{\\text{bc},1}(\\theta)^2 + r_{\\text{bc},2}(\\theta)^2\\right).\n$$\n通过基于梯度的优化方法，从随机初始化开始训练参数 $\\theta = \\{a_i, w_i, b_i, c\\}_{i=1}^{H}$。为了定量评估光谱偏差，在较短的训练预算结束后，通过最小二乘法将学习到的函数 $u_{\\theta}(x)$ 投影到 $[0, 2\\pi)$ 上密集均匀网格的两个基函数 $\\sin(x)$ 和 $\\sin(25x)$ 上。也就是说，找到最小化以下表达式的系数 $(\\hat{\\alpha}_1, \\hat{\\alpha}_{25})$：\n$$\n\\sum_{m=1}^{M}\\left(u_{\\theta}(x_m) - \\hat{\\alpha}_1 \\sin(x_m) - \\hat{\\alpha}_{25}\\sin(25 x_m)\\right)^2,\n$$\n其中 $x_m$ 在 $[0, 2\\pi)$ 内均匀分布。将学习到的振幅定义为 $A_1 = |\\hat{\\alpha}_1|$ 和 $A_{25} = |\\hat{\\alpha}_{25}|$。如果在早期训练中 $A_1 > A_{25}$，则认为存在光谱偏差。\n\n仅使用ODE残差和边界残差，实现一个具有完全向量化训练循环和关于所有网络参数的闭式梯度的程序。不要使用任何外部自动微分库。\n\n测试套件与输出规范：\n- 使用以下三个测试用例来检验不同方案。每个用例指定了 $(H, N, K, \\eta)$，其中 $H$ 是隐藏单元的数量， $N$ 是配置点的数量， $K$ 是梯度步数，$\\eta$ 是学习率。在所有用例中，使用 $\\lambda_{\\text{bc}} = 1$。角度以弧度为单位。\n  1. 用例1: $(H, N, K, \\eta) = (20, 128, 60, 0.01)$。\n  2. 用例2: $(H, N, K, \\eta) = (10, 64, 80, 0.01)$。\n  3. 用例3: $(H, N, K, \\eta) = (5, 128, 120, 0.01)$。\n- 对于每个用例，使用固定的种子初始化参数，以确保结果是确定性的。训练 $K$ 步后，通过在包含 $M = 4096$ 个点的密集网格上进行最小二乘投影来计算 $A_1$ 和 $A_{25}$。记录该用例的布尔结果，定义如下：\n$$\n\\text{result} = \\begin{cases}\n\\text{True},  \\text{if } A_1 > A_{25},\\\\\n\\text{False},  \\text{otherwise.}\n\\end{cases}\n$$\n- 最终输出格式：您的程序应生成单行输出，其中包含用方括号括起来的、以逗号分隔的结果列表（例如，“[True,True,False]”）。\n\n您的程序必须是自包含的，不接收任何输入，并且可以直接运行。角度必须使用弧度制。所有数值答案都是无量纲的，最终输出是布尔值。训练和投影必须使用上述公式，通过纯线性代数实现，不使用任何外部机器学习框架。目标是通过这些测试用例证明，物理信息神经网络（PINN）学习低频分量 $\\sin(x)$ 的速度要早于高频分量 $\\sin(25x)$，这与光谱偏差的现象一致。",
            "solution": "所提出的问题是计算物理学中一个有效且适定的练习，旨在具体展示物理信息神经网络（PINNs）中的光谱偏差现象。它具有科学依据，给出了陈述正确的微分方程及其解析解。所有参数和步骤都已明确指定，从而可以得到唯一且可验证的计算结果。我将着手提供一个解决方案。\n\n目标是训练一个神经网络 $u_{\\theta}(x)$ 来近似一维常微分方程（ODE）的解\n$$\nu''(x) + u(x) = -624 \\sin(25x)\n$$\n在域 $x \\in [0, 2\\pi]$ 上，并满足周期性边界条件 $u(0) = u(2\\pi)$ 和 $u'(0) = u'(2\\pi)$。其解析解 $u(x) = \\sin(x) + \\sin(25x)$ 是一个低频分量和一个高频分量的叠加。我们将证明，对PINN损失函数进行基于梯度的优化会使网络学习低频分量 $\\sin(x)$ 的速度快于高频分量 $\\sin(25x)$。\n\n首先，我们定义神经网络的拟设（ansatz），这是一个具有 $H$ 个神经元和 $\\tanh$ 激活函数的单隐藏层感知机：\n$$\nu_{\\theta}(x) = \\sum_{i=1}^{H} a_i \\tanh(w_i x + b_i) + c\n$$\n网络的参数为 $\\theta = \\{a_i, w_i, b_i, c\\}_{i=1}^{H}$。为了强制满足ODE，我们必须计算 $u_{\\theta}(x)$ 关于 $x$ 的一阶和二阶导数。使用链式法则以及恒等式 $\\frac{d}{dz}\\tanh(z) = \\operatorname{sech}^2(z)$ 和 $\\frac{d}{dz}\\operatorname{sech}^2(z) = -2\\operatorname{sech}^2(z)\\tanh(z)$，我们得到：\n$$\nu'_{\\theta}(x) = \\frac{d u_{\\theta}}{dx} = \\sum_{i=1}^{H} a_i w_i \\operatorname{sech}^2(w_i x + b_i)\n$$\n$$\nu''_{\\theta}(x) = \\frac{d^2 u_{\\theta}}{dx^2} = -2 \\sum_{i=1}^{H} a_i w_i^2 \\operatorname{sech}^2(w_i x + b_i) \\tanh(w_i x + b_i)\n$$\n\n通过最小化一个由ODE残差和边界条件残差的均方误差组成的损失函数来训练网络。在一组 $N$ 个配置点 $\\{x_n\\}$ 上的物理残差为：\n$$\nr_{\\text{phys}}(x_n;\\theta) = u_{\\theta}''(x_n) + u_{\\theta}(x_n) + 624 \\sin(25 x_n)\n$$\n周期性边界条件残差为：\n$$\nr_{\\text{bc},1}(\\theta) = u_{\\theta}(0) - u_{\\theta}(2\\pi)\n$$\n$$\nr_{\\text{bc},2}(\\theta) = u_{\\theta}'(0) - u_{\\theta}'(2\\pi)\n$$\n总损失函数是一个加权和：\n$$\n\\mathcal{L}(\\theta) = \\mathcal{L}_{\\text{phys}} + \\lambda_{\\text{bc}} \\mathcal{L}_{\\text{bc}} = \\frac{1}{N}\\sum_{n=1}^{N} r_{\\text{phys}}(x_n;\\theta)^2 + \\lambda_{\\text{bc}}\\left(r_{\\text{bc},1}(\\theta)^2 + r_{\\text{bc},2}(\\theta)^2\\right)\n$$\n其中 $\\lambda_{\\text{bc}}$ 是一个用于平衡各项的超参数，给定值为 $\\lambda_{\\text{bc}} = 1$。\n\n训练使用梯度下降法进行。参数根据 $\\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta} \\mathcal{L}(\\theta)$ 更新，其中 $\\eta$ 是学习率。我们必须推导出解析梯度 $\\nabla_{\\theta} \\mathcal{L}(\\theta)$。损失函数关于任意参数 $p \\in \\theta$ 的梯度为：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial p} = \\frac{2}{N}\\sum_{n=1}^{N} r_{\\text{phys}}(x_n) \\left(\\frac{\\partial u''_{\\theta}(x_n)}{\\partial p} + \\frac{\\partial u_{\\theta}(x_n)}{\\partial p}\\right) + 2\\lambda_{\\text{bc}}r_{\\text{bc},1}\\left(\\frac{\\partial u_{\\theta}(0)}{\\partial p} - \\frac{\\partial u_{\\theta}(2\\pi)}{\\partial p}\\right) + 2\\lambda_{\\text{bc}}r_{\\text{bc},2}\\left(\\frac{\\partial u'_{\\theta}(0)}{\\partial p} - \\frac{\\partial u'_{\\theta}(2\\pi)}{\\partial p}\\right)\n$$\n网络输出及其空间导数相对于参数 $\\{a_k, w_k, b_k, c\\}$ 的导数通过链式法则计算。这些推导过程繁琐但系统，并以向量化形式实现以提高计算效率。例如，关于输出权重 $a_k$ 的梯度包含诸如 $\\frac{\\partial u_{\\theta}(x)}{\\partial a_k} = \\tanh(w_k x + b_k)$ 的项。所有梯度的完整表达式都在代码中实现。\n\n在训练指定步数后，我们量化学习到的频率分量。我们在 $[0, 2\\pi)$ 范围内一个包含 $M$ 个点的密集网格 $\\{x_m\\}$ 上评估训练好的网络 $u_{\\theta}(x)$。然后，我们通过求解一个线性最小二乘问题，将这个学习到的函数投影到基函数 $\\sin(x)$ 和 $\\sin(25x)$ 上，以找到最小化以下表达式的系数 $(\\hat{\\alpha}_1, \\hat{\\alpha}_{25})$：\n$$\n\\sum_{m=1}^{M}\\left(u_{\\theta}(x_m) - \\hat{\\alpha}_1 \\sin(x_m) - \\hat{\\alpha}_{25}\\sin(25 x_m)\\right)^2\n$$\n该问题的解由 $\\hat{\\boldsymbol{\\alpha}} = (\\mathbf{B}^T\\mathbf{B})^{-1}\\mathbf{B}^T\\mathbf{y}$ 给出，其中 $\\mathbf{y}$ 是网络预测值 $u_{\\theta}(x_m)$ 的向量，$\\mathbf{B}$ 是以 $\\sin(x_m)$ 和 $\\sin(25x_m)$ 为列的设计矩阵。学习到的振幅为 $A_1 = |\\hat{\\alpha}_1|$ 和 $A_{25} = |\\hat{\\alpha}_{25}|$。如果 $A_1 > A_{25}$，我们得出结论：观察到了光谱偏差。\n\n实现将遵循这些原则，使用 `numpy` 进行向量化数值计算，包括完全解析的梯度计算和标准的梯度下降循环。参数初始化将使用固定的随机种子和 Glorot/Xavier 缩放，以确保可复现性和训练的稳定性。",
            "answer": "```python\nimport numpy as np\n\nclass PINN:\n    \"\"\"\n    A Physics-Informed Neural Network to demonstrate spectral bias.\n    The implementation is fully vectorized and uses analytical gradients.\n    \"\"\"\n    def __init__(self, H, N, seed):\n        \"\"\"\n        Initializes the PINN.\n        H: number of hidden units\n        N: number of collocation points\n        seed: random seed for parameter initialization\n        \"\"\"\n        self.H = H\n        self.N = N\n        self.lambda_bc = 1.0\n        self.rng = np.random.default_rng(seed)\n\n        # Xavier/Glorot initialization\n        # For weights w, n_in=1, n_out=1 (conceptual). limit = sqrt(6 / (1+1)) = sqrt(3)\n        limit_w = np.sqrt(3.0)\n        self.w = self.rng.uniform(-limit_w, limit_w, size=(1, self.H))\n        \n        # For weights a, n_in=H, n_out=1. limit = sqrt(6 / (H+1))\n        limit_a = np.sqrt(6.0 / (self.H + 1.0))\n        self.a = self.rng.uniform(-limit_a, limit_a, size=(self.H, 1))\n\n        self.b = np.zeros((1, self.H))\n        self.c = np.zeros((1, 1))\n\n        self.x_colloc = np.linspace(0, 2 * np.pi, self.N, endpoint=False).reshape(-1, 1)\n\n    def forward(self, x):\n        \"\"\"\n        Computes the network output u and its derivatives u', u'' w.r.t. x.\n        x: input points, shape (num_points, 1)\n        \"\"\"\n        z = x @ self.w + self.b\n        h = np.tanh(z)\n        s = 1.0 - h**2  # sech^2(z)\n\n        u = h @ self.a + self.c\n        u_prime = (s * self.w) @ self.a\n        u_double_prime = (-2.0 * s * h * (self.w**2)) @ self.a\n\n        return u, u_prime, u_double_prime, h, s\n\n    def _compute_gradients(self):\n        \"\"\"\n        Computes the loss and the gradients of the loss w.r.t. all parameters.\n        All calculations are vectorized.\n        \"\"\"\n        # --- Physics Loss and Gradients ---\n        u, _, u_pp, H_c, S_c = self.forward(self.x_colloc)\n        \n        f_term = -624.0 * np.sin(25.0 * self.x_colloc)\n        r_phys = u_pp + u - f_term\n        loss_phys = np.mean(r_phys**2)\n\n        # Common factor for physics gradients\n        grad_common_phys = (2.0 / self.N) * r_phys\n        \n        # Gradient w.r.t. a\n        d_u_da = H_c\n        d_u_pp_da = -2.0 * S_c * H_c * self.w**2\n        grad_a_phys = (d_u_da + d_u_pp_da).T @ grad_common_phys\n\n        # Gradient w.r.t. b\n        d_u_db = self.a.T * S_c\n        d_u_pp_db = self.a.T * (4.0 * self.w**2 * S_c * H_c**2 - 2.0 * self.w**2 * S_c**2)\n        grad_b_phys = np.sum(grad_common_phys * (d_u_db + d_u_pp_db), axis=0)\n\n        # Gradient w.r.t. w\n        d_u_dw = self.a.T * self.x_colloc * S_c\n        d_u_pp_dw = self.a.T * (\n            -4.0 * self.w * S_c * H_c + \n            self.x_colloc * (4.0 * self.w**2 * S_c * H_c**2 - 2.0 * self.w**2 * S_c**2)\n        )\n        grad_w_phys = np.sum(grad_common_phys * (d_u_dw + d_u_pp_dw), axis=0)\n        \n        # Gradient w.r.t. c\n        grad_c_phys = np.sum(grad_common_phys)\n\n        # --- Boundary Loss and Gradients ---\n        x_bc = np.array([[0.0], [2 * np.pi]])\n        u_bc, u_p_bc, _, H_bc, S_bc = self.forward(x_bc)\n        \n        u0, u2pi = u_bc[0], u_bc[1]\n        u0_p, u2pi_p = u_p_bc[0], u_p_bc[1]\n\n        r_bc1 = u0 - u2pi\n        r_bc2 = u0_p - u2pi_p\n        loss_bc = r_bc1**2 + r_bc2**2\n        \n        H0, H2pi = H_bc[0:1, :], H_bc[1:2, :]\n        S0, S2pi = S_bc[0:1, :], S_bc[1:2, :]\n        \n        # Common factors for BC gradients\n        common1 = 2.0 * self.lambda_bc * r_bc1\n        common2 = 2.0 * self.lambda_bc * r_bc2\n\n        # Gradient w.r.t. a\n        delta_u_da = (H0 - H2pi).T\n        delta_u_p_da = (self.w * (S0 - S2pi)).T\n        grad_a_bc = common1 * delta_u_da + common2 * delta_u_p_da\n        \n        # Gradient w.r.t. b\n        delta_u_db = self.a.T * (S0 - S2pi)\n        delta_u_p_db = -2.0 * self.a.T * self.w * (S0 * H0 - S2pi * H2pi)\n        grad_b_bc = common1 * delta_u_db + common2 * delta_u_p_db\n\n        # Gradient w.r.t. w\n        delta_u_dw = -2.0 * np.pi * self.a.T * S2pi\n        delta_u_p_dw = self.a.T * (S0 - S2pi) + 4.0 * np.pi * self.a.T * self.w * S2pi * H2pi\n        grad_w_bc = common1 * delta_u_dw + common2 * delta_u_p_dw\n\n        # Gradient w.r.t. c (is zero)\n        grad_c_bc = 0.0\n\n        # --- Total Loss and Gradients ---\n        loss = loss_phys + self.lambda_bc * loss_bc\n        grad_a = grad_a_phys + grad_a_bc\n        grad_w = grad_w_phys.reshape(1, -1) + grad_w_bc\n        grad_b = grad_b_phys.reshape(1, -1) + grad_b_bc\n        grad_c = grad_c_phys + grad_c_bc\n\n        return loss, grad_a, grad_w, grad_b, grad_c\n\n    def train(self, K, eta):\n        \"\"\"\n        Trains the network using gradient descent.\n        K: number of training steps\n        eta: learning rate\n        \"\"\"\n        for _ in range(K):\n            loss, grad_a, grad_w, grad_b, grad_c = self._compute_gradients()\n            \n            self.a -= eta * grad_a\n            self.w -= eta * grad_w\n            self.b -= eta * grad_b\n            self.c -= eta * grad_c\n\n    def project_and_analyze(self):\n        \"\"\"\n        Projects the learned function onto sin(x) and sin(25x) and checks for spectral bias.\n        \"\"\"\n        M = 4096\n        x_dense = np.linspace(0, 2 * np.pi, M, endpoint=False).reshape(-1, 1)\n        \n        u_pred, _, _, _, _ = self.forward(x_dense)\n        u_pred = u_pred.flatten()\n        \n        # Create design matrix for least squares\n        B = np.zeros((M, 2))\n        B[:, 0] = np.sin(x_dense.flatten())\n        B[:, 1] = np.sin(25.0 * x_dense.flatten())\n        \n        # Solve least squares problem: B * alpha = u_pred\n        alpha, _, _, _ = np.linalg.lstsq(B, u_pred, rcond=None)\n        \n        A1 = np.abs(alpha[0])\n        A25 = np.abs(alpha[1])\n        \n        return A1 > A25\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and produce the final output.\n    \"\"\"\n    test_cases = [\n        (20, 128, 60, 0.01),  # Case 1: (H, N, K, eta)\n        (10, 64, 80, 0.01),   # Case 2\n        (5, 128, 120, 0.01),  # Case 3\n    ]\n\n    results = []\n    base_seed = 42\n\n    for i, (H, N, K, eta) in enumerate(test_cases):\n        seed = base_seed + i\n        pinn = PINN(H=H, N=N, seed=seed)\n        pinn.train(K=K, eta=eta)\n        result = pinn.project_and_analyze()\n        results.append(result)\n\n    # Format the final output as specified\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}