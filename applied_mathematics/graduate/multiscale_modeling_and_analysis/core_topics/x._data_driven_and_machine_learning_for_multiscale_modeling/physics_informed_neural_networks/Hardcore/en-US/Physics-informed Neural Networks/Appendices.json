{
    "hands_on_practices": [
        {
            "introduction": "Enforcing boundary conditions is a critical step in solving differential equations with neural networks. While it is common to add boundary residuals to the loss function, an alternative and often more robust approach is to enforce them by construction. This practice challenges you to design a network output transformation that analytically satisfies non-homogeneous Dirichlet boundary conditions, a technique that can significantly stabilize and accelerate the training process .",
            "id": "2126300",
            "problem": "In the field of scientific computing, Physics-Informed Neural Networks (PINNs) have emerged as a powerful tool for solving differential equations. A key aspect of designing a PINN is ensuring that its output, which approximates the solution, respects the given boundary conditions. One robust method to achieve this is to structure the network's final output function so that it satisfies these conditions by construction.\n\nConsider a one-dimensional problem on the spatial domain $x \\in [0, L]$. A neural network provides a raw, unconstrained output function denoted by $\\hat{u}_{NN}(x)$. We wish to use this network to find an approximate solution, $u(x)$, to a differential equation that is subject to the following non-homogeneous Dirichlet boundary conditions:\n$$u(0) = A$$\n$$u(L) = B$$\nHere, $A$, $B$, and $L > 0$ are given real constants.\n\nYour task is to devise a transformation that takes the raw network output $\\hat{u}_{NN}(x)$ and produces a new function, $u_{NN}(x)$, that serves as the final approximation. This transformation must guarantee that $u_{NN}(x)$ strictly satisfies the specified boundary conditions, regardless of the function $\\hat{u}_{NN}(x)$ produced by the network.\n\nProvide an expression for $u_{NN}(x)$ in terms of the raw network output $\\hat{u}_{NN}(x)$ and the parameters $x$, $L$, $A$, and $B$.",
            "solution": "We seek a transformation that maps the raw network output $\\hat{u}_{NN}(x)$ to a function $u_{NN}(x)$ that enforces the Dirichlet boundary conditions $u_{NN}(0)=A$ and $u_{NN}(L)=B$ for any $\\hat{u}_{NN}(x)$. A standard construction is to decompose $u_{NN}(x)$ as\n$$\nu_{NN}(x)=g(x)+s(x)\\,\\hat{u}_{NN}(x),\n$$\nwhere $g(x)$ is any fixed function that satisfies the boundary conditions and $s(x)$ is any function that vanishes at both boundaries. Specifically, we require\n$$\ng(0)=A,\\quad g(L)=B,\\quad s(0)=0,\\quad s(L)=0.\n$$\nA convenient choice is the linear interpolant for $g(x)$,\n$$\ng(x)=A\\left(1-\\frac{x}{L}\\right)+B\\left(\\frac{x}{L}\\right)=A+\\frac{B-A}{L}\\,x,\n$$\nand the simple vanishing factor\n$$\ns(x)=x(L-x),\n$$\nwhich satisfies $s(0)=0$ and $s(L)=0$. Therefore, define\n$$\nu_{NN}(x)=A\\left(1-\\frac{x}{L}\\right)+B\\left(\\frac{x}{L}\\right)+x(L-x)\\,\\hat{u}_{NN}(x).\n$$\nTo verify the boundary conditions, evaluate at $x=0$ and $x=L$:\n$$\nu_{NN}(0)=A\\left(1-0\\right)+B\\left(0\\right)+0\\cdot L\\,\\hat{u}_{NN}(0)=A,\n$$\n$$\nu_{NN}(L)=A\\left(1-1\\right)+B\\left(\\frac{L}{L}\\right)+L( L-L)\\,\\hat{u}_{NN}(L)=B.\n$$\nThus, for any $\\hat{u}_{NN}(x)$, the constructed $u_{NN}(x)$ strictly satisfies $u_{NN}(0)=A$ and $u_{NN}(L)=B$.",
            "answer": "$$\\boxed{A\\left(1-\\frac{x}{L}\\right)+B\\left(\\frac{x}{L}\\right)+x\\left(L-x\\right)\\hat{u}_{NN}(x)}$$"
        },
        {
            "introduction": "The convergence of gradient-based optimizers is sensitive to the relative magnitudes of the terms in the loss function, and a poorly scaled PDE residual can hinder learning. This exercise demonstrates the power of nondimensionalization, a classical technique in applied mathematics, to improve the conditioning of the optimization problem for a PINN. By recasting the viscous Burgers' equation into a dimensionless form, you will see how to balance the physical terms and identify the key dimensionless parameter governing the system's behavior .",
            "id": "4235875",
            "problem": "A digital twin of a one-dimensional viscous transport process in a cyber-physical system is modeled by the viscous Burgers equation, which at the physical level reads $u_{t} + u u_{x} - \\nu u_{xx} = 0$ on a spatial interval of length $L$ and a time horizon of interest. A Physics-Informed Neural Network (PINN) is to be trained on this model by enforcing the partial differential equation residual at collocation points together with initial and boundary data. To promote robust optimization across different operating regimes, you will nondimensionalize the model.\n\nStarting from the physical equation and the core definitions of characteristic scales for space, time, and velocity magnitudes, introduce dimensionless variables by setting $x = L \\,\\xi$, $t = T \\,\\tau$, and $u = U \\,v$, where $L$ and $U$ are fixed characteristic length and velocity scales, and $T$ is a characteristic time scale to be determined. Derive the nondimensional form of the equation for $v(\\xi,\\tau)$ using only these scalings and the chain rule. Choose $T$ so that the nondimensional equation has order-one coefficients for both the temporal derivative and the convective nonlinearity. Identify the single dimensionless group multiplying the diffusion term that controls the balance between convection and diffusion.\n\nThen, using first principles about gradient-based optimization and conditioning, provide a concise argument for why training a Physics-Informed Neural Network (PINN) on the nondimensional form can improve optimization stability and convergence when implemented in the digital twin.\n\nYour final reported answer must be the analytic expression for the Reynolds number associated with this nondimensionalization in terms of $U$, $L$, and $\\nu$. Do not include units. If you choose to perform any intermediate numerical evaluations, do not report them. The final answer must be a single closed-form expression.",
            "solution": "The problem requires the nondimensionalization of the one-dimensional viscous Burgers equation, an analysis of the resulting dimensionless group, and an argument for why this procedure benefits the training of a Physics-Informed Neural Network (PINN).\n\nThe physical form of the viscous Burgers equation is given as:\n$$\nu_{t} + u u_{x} - \\nu u_{xx} = 0\n$$\nwhere $u(x,t)$ is the velocity, $\\nu$ is the kinematic viscosity, $u_t = \\frac{\\partial u}{\\partial t}$, $u_x = \\frac{\\partial u}{\\partial x}$, and $u_{xx} = \\frac{\\partial^2 u}{\\partial x^2}$.\n\nThe problem specifies the introduction of dimensionless variables $\\xi$, $\\tau$, and $v$ through the following scaling relations:\n$$\nx = L \\,\\xi, \\quad t = T \\,\\tau, \\quad u = U \\,v\n$$\nHere, $L$ is a characteristic length scale and $U$ is a characteristic velocity scale. The characteristic time scale $T$ is to be determined. The function $v$ is the dimensionless velocity, which depends on the dimensionless space and time variables, i.e., $v = v(\\xi, \\tau)$.\n\nTo transform the partial differential equation (PDE), we must express the partial derivatives of $u$ with respect to $x$ and $t$ in terms of the partial derivatives of $v$ with respect to $\\xi$ and $\\tau$. We use the chain rule for this transformation.\n\nFirst, we find the temporal derivative, $u_t$:\n$$\nu_{t} = \\frac{\\partial u}{\\partial t} = \\frac{\\partial (Uv)}{\\partial t} = U \\frac{\\partial v}{\\partial t} = U \\frac{\\partial v}{\\partial \\tau} \\frac{\\partial \\tau}{\\partial t}\n$$\nFrom the relation $t = T\\tau$, we have $\\frac{\\partial \\tau}{\\partial t} = \\frac{1}{T}$. Therefore, the temporal derivative becomes:\n$$\nu_{t} = \\frac{U}{T} \\frac{\\partial v}{\\partial \\tau} = \\frac{U}{T} v_{\\tau}\n$$\n\nNext, we find the first spatial derivative, $u_x$:\n$$\nu_{x} = \\frac{\\partial u}{\\partial x} = \\frac{\\partial (Uv)}{\\partial x} = U \\frac{\\partial v}{\\partial x} = U \\frac{\\partial v}{\\partial \\xi} \\frac{\\partial \\xi}{\\partial x}\n$$\nFrom the relation $x = L\\xi$, we have $\\frac{\\partial \\xi}{\\partial x} = \\frac{1}{L}$. Thus, the first spatial derivative is:\n$$\nu_{x} = \\frac{U}{L} \\frac{\\partial v}{\\partial \\xi} = \\frac{U}{L} v_{\\xi}\n$$\n\nFinally, we find the second spatial derivative, $u_{xx}$:\n$$\nu_{xx} = \\frac{\\partial^2 u}{\\partial x^2} = \\frac{\\partial}{\\partial x} \\left( \\frac{\\partial u}{\\partial x} \\right) = \\frac{\\partial}{\\partial x} \\left( \\frac{U}{L} v_{\\xi} \\right) = \\frac{U}{L} \\frac{\\partial v_{\\xi}}{\\partial x} = \\frac{U}{L} \\left( \\frac{\\partial v_{\\xi}}{\\partial \\xi} \\frac{\\partial \\xi}{\\partial x} \\right) = \\frac{U}{L} \\left( v_{\\xi\\xi} \\frac{1}{L} \\right)\n$$\nThis simplifies to:\n$$\nu_{xx} = \\frac{U}{L^2} v_{\\xi\\xi}\n$$\n\nNow, we substitute these expressions for the derivatives and for $u$ itself back into the original viscous Burgers equation:\n$$\n\\left( \\frac{U}{T} v_{\\tau} \\right) + (Uv) \\left( \\frac{U}{L} v_{\\xi} \\right) - \\nu \\left( \\frac{U}{L^2} v_{\\xi\\xi} \\right) = 0\n$$\nCombining the coefficients, we get:\n$$\n\\frac{U}{T} v_{\\tau} + \\frac{U^2}{L} v v_{\\xi} - \\frac{\\nu U}{L^2} v_{\\xi\\xi} = 0\n$$\nTo obtain the nondimensional form where coefficients are of order-one, we can divide the entire equation by one of the coefficients. A standard choice is to divide by the coefficient of the nonlinear convective term, $\\frac{U^2}{L}$, to make its coefficient exactly $1$:\n$$\n\\left( \\frac{U}{T} \\frac{L}{U^2} \\right) v_{\\tau} + \\left( \\frac{U^2}{L} \\frac{L}{U^2} \\right) v v_{\\xi} - \\left( \\frac{\\nu U}{L^2} \\frac{L}{U^2} \\right) v_{\\xi\\xi} = 0\n$$\nSimplifying the coefficients yields:\n$$\n\\left( \\frac{L}{UT} \\right) v_{\\tau} + v v_{\\xi} - \\left( \\frac{\\nu}{UL} \\right) v_{\\xi\\xi} = 0\n$$\nThe problem requires choosing the characteristic time scale $T$ such that the coefficients of the temporal derivative ($v_\\tau$) and the convective nonlinearity ($v v_\\xi$) are both of order-one. We have already set the coefficient of $v v_\\xi$ to $1$. To set the coefficient of $v_\\tau$ also to $1$, we must have:\n$$\n\\frac{L}{UT} = 1 \\implies T = \\frac{L}{U}\n$$\nThis choice for $T$ represents the convective time scale: the time it takes for a fluid parcel to travel the characteristic length $L$ at the characteristic velocity $U$.\n\nSubstituting this expression for $T$ back into the scaled equation, we obtain the final nondimensional form of the viscous Burgers equation:\n$$\nv_{\\tau} + v v_{\\xi} - \\left( \\frac{\\nu}{UL} \\right) v_{\\xi\\xi} = 0\n$$\nThe single dimensionless group multiplying the diffusion term ($v_{\\xi\\xi}$) is $\\frac{\\nu}{UL}$. This group is the reciprocal of the Reynolds number, a fundamental dimensionless parameter in fluid mechanics that quantifies the ratio of inertial forces to viscous forces. The Reynolds number, denoted $Re$, is defined as:\n$$\nRe = \\frac{\\text{inertial forces}}{\\text{viscous forces}} \\propto \\frac{U^2/L}{\\nu U/L^2} = \\frac{UL}{\\nu}\n$$\nThus, the dimensionless coefficient is $\\frac{1}{Re}$, and the equation is:\n$$\nv_{\\tau} + v v_{\\xi} - \\frac{1}{Re} v_{\\xi\\xi} = 0\n$$\nThe expression for the Reynolds number is therefore $\\frac{UL}{\\nu}$.\n\nThe argument for why training a PINN on the nondimensional form is advantageous relates to the numerical conditioning of the optimization problem. A PINN is trained by minimizing a loss function that includes the residual of the governing PDE. For the original physical equation, the PDE residual is $r = u_t + u u_x - \\nu u_{xx}$. The loss is typically the mean squared error of this residual over a set of collocation points, $\\mathcal{L}_{PDE} = \\langle (u_t + u u_x - \\nu u_{xx})^2 \\rangle$.\n\nIn a physical system, the magnitudes of the terms $u_t$, $u u_x$, and $\\nu u_{xx}$ can vary by many orders of magnitude. For instance, in a high-speed, low-viscosity flow, the convective term $u u_x$ may be far larger than the viscous term $\\nu u_{xx}$. When a gradient-based optimizer (e.g., Adam) minimizes the loss function, the gradients of the loss with respect to the neural network's parameters (weights and biases) will be dominated by the largest terms in the residual. This creates an ill-conditioned optimization landscape with steep \"canyons\" and flat \"plateaus,\" making it difficult for the optimizer to converge to a good solution. The network may learn to satisfy the dominant physics (e.g., convection) while largely ignoring the sub-dominant physics (e.g., diffusion), leading to an inaccurate model.\n\nNondimensionalization acts as a form of preconditioning. By scaling variables such that $v, \\xi, \\tau$ are all of order-one, we ensure that the terms in the nondimensional residual, $r' = v_\\tau + v v_\\xi - \\frac{1}{Re} v_{\\xi\\xi}$, are of comparable magnitude (unless $Re$ is extremely large or small, in which case the physics enters a distinct asymptotic regime). For a moderate $Re$, the coefficients are all of order-one. This balancing of terms leads to a better-conditioned loss landscape. The gradients originating from each physical term (temporal change, convection, diffusion) are more evenly scaled. As a result, the optimizer can make more uniform progress in minimizing the contributions from all physical effects simultaneously, leading to more stable and efficient training, and ultimately, a more accurate PINN-based digital twin.",
            "answer": "$$\n\\boxed{\\frac{UL}{\\nu}}\n$$"
        },
        {
            "introduction": "Neural networks exhibit a phenomenon known as \"spectral bias,\" where they preferentially learn low-frequency patterns before high-frequency ones during training. This has profound implications for PINNs, especially when approximating solutions with complex, multi-scale features or sharp gradients. In this hands-on coding exercise, you will directly observe and quantify this bias by training a PINN to solve an ODE whose solution is a sum of two sine waves of different frequencies .",
            "id": "2427229",
            "problem": "You will implement a complete, runnable program that demonstrates the spectral bias of a Physics-Informed Neural Network (PINN). The central idea is to train a PINN to solve a one-dimensional boundary value problem whose known solution is the superposition of a low-frequency and a high-frequency sine, namely $u(x) = \\sin(x) + \\sin(25x)$, and to quantitatively observe which frequency component is learned first during training. Angles must be in radians throughout.\n\nStart from the following physically consistent ordinary differential equation (ODE) with periodic boundary conditions:\nGiven the domain $x \\in [0, 2\\pi]$, consider\n$$\nu''(x) + u(x) = -624 \\sin(25x),\n$$\nwith periodic boundary conditions\n$$\nu(0) = u(2\\pi), \\quad u'(0) = u'(2\\pi).\n$$\nIt is a well-tested fact that if $u(x) = \\sin(x) + \\sin(25x)$ then $u''(x) + u(x) = -624 \\sin(25x)$ and the periodic boundary conditions hold. You must not use any labeled training data for $u(x)$ except the boundary conditions; instead, use the ODE residual and boundary residuals in the loss, as is standard for a Physics-Informed Neural Network (PINN).\n\nConstruct a single-hidden-layer neural network $u_{\\theta}(x)$ with $H$ hidden units and hyperbolic tangent activation as the trial solution. Define the hidden pre-activations as $z_i(x) = w_i x + b_i$ for $i \\in \\{1,\\dots,H\\}$, the hidden activations as $h_i(x) = \\tanh(z_i(x))$, and the output as\n$$\nu_{\\theta}(x) = \\sum_{i=1}^{H} a_i h_i(x) + c.\n$$\nUse the chain rule and the product rule to compute the first and second derivatives of $u_{\\theta}(x)$ with respect to $x$ in closed form. Recall the standard identities for the hyperbolic tangent and its derivatives:\n$$\n\\tanh'(z) = \\operatorname{sech}^2(z), \\quad \\frac{d}{dz}\\operatorname{sech}^2(z) = -2\\operatorname{sech}^2(z)\\tanh(z), \\quad \\operatorname{sech}^2(z) = 1 - \\tanh^2(z).\n$$\nDefine the pointwise physics residual for collocation points $\\{x_n\\}_{n=1}^{N}$ as\n$$\nr_{\\text{phys}}(x_n;\\theta) = u_{\\theta}''(x_n) + u_{\\theta}(x_n) - \\left(-624 \\sin(25 x_n)\\right),\n$$\nand the periodic boundary residuals as\n$$\nr_{\\text{bc},1}(\\theta) = u_{\\theta}(0) - u_{\\theta}(2\\pi), \\quad r_{\\text{bc},2}(\\theta) = u_{\\theta}'(0) - u_{\\theta}'(2\\pi).\n$$\nUse the mean-squared residual loss with a boundary weight $\\lambda_{\\text{bc}}$:\n$$\n\\mathcal{L}(\\theta) = \\frac{1}{N}\\sum_{n=1}^{N} r_{\\text{phys}}(x_n;\\theta)^2 + \\lambda_{\\text{bc}}\\left(r_{\\text{bc},1}(\\theta)^2 + r_{\\text{bc},2}(\\theta)^2\\right).\n$$\nTrain the parameters $\\theta = \\{a_i, w_i, b_i, c\\}_{i=1}^{H}$ by gradient-based optimization from random initialization. To quantitatively assess spectral bias, at the end of a short training budget, project the learned function $u_{\\theta}(x)$ onto the two basis functions $\\sin(x)$ and $\\sin(25x)$ over a dense uniform grid on $[0, 2\\pi)$ by least squares. That is, find coefficients $(\\hat{\\alpha}_1, \\hat{\\alpha}_{25})$ minimizing\n$$\n\\sum_{m=1}^{M}\\left(u_{\\theta}(x_m) - \\hat{\\alpha}_1 \\sin(x_m) - \\hat{\\alpha}_{25}\\sin(25 x_m)\\right)^2,\n$$\nwith $x_m$ uniformly spaced in $[0, 2\\pi)$. Define the learned amplitudes as $A_1 = |\\hat{\\alpha}_1|$ and $A_{25} = |\\hat{\\alpha}_{25}|$. Spectral bias is deemed present at early training if $A_1 > A_{25}$.\n\nImplement the program with a fully vectorized training loop and closed-form gradients with respect to all network parameters using only the ODE residual and boundary residuals. Do not use any external automatic differentiation library.\n\nTest Suite and Output Specification:\n- Use the following three test cases to exercise different regimes. Each case specifies $(H, N, K, \\eta)$ where $H$ is the number of hidden units, $N$ is the number of collocation points, $K$ is the number of gradient steps, and $\\eta$ is the learning rate. Use $\\lambda_{\\text{bc}} = 1$ in all cases. Angles are in radians.\n  1. Case $1$: $(H, N, K, \\eta) = (20, 128, 60, 0.01)$.\n  2. Case $2$: $(H, N, K, \\eta) = (10, 64, 80, 0.01)$.\n  3. Case $3$: $(H, N, K, \\eta) = (5, 128, 120, 0.01)$.\n- For each case, initialize parameters with a fixed seed so that results are deterministic. After training for $K$ steps, compute $A_1$ and $A_{25}$ by least squares projection over a dense grid of $M$ points with $M = 4096$. Record a boolean result for the case defined as\n$$\n\\text{result} = \\begin{cases}\n\\text{True}, & \\text{if } A_1 > A_{25},\\\\\n\\text{False}, & \\text{otherwise.}\n\\end{cases}\n$$\n- Final Output Format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[True,True,False]\").\n\nYour program must be self-contained, receive no input, and run as-is. Angles must be in radians. All numerical answers are dimensionless, and the final outputs are booleans. The training and projections must be implemented in pure linear algebra using the formulas above, without any external machine learning framework. The goal is to demonstrate, via these test cases, that the low-frequency component $\\sin(x)$ is learned earlier than the high-frequency component $\\sin(25x)$ by a Physics-Informed Neural Network (PINN), consistent with spectral bias.",
            "solution": "The problem presented is a valid and well-posed exercise in computational physics, specifically demonstrating the spectral bias phenomenon in Physics-Informed Neural Networks (PINNs). It is scientifically grounded, with a correctly stated differential equation and its analytical solution. All parameters and procedures are specified, permitting a unique and verifiable computational outcome. I will proceed with a solution.\n\nThe objective is to train a neural network $u_{\\theta}(x)$ to approximate the solution of the one-dimensional ordinary differential equation (ODE)\n$$\nu''(x) + u(x) = -624 \\sin(25x)\n$$\non the domain $x \\in [0, 2\\pi]$ with periodic boundary conditions $u(0) = u(2\\pi)$ and $u'(0) = u'(2\\pi)$. The analytical solution, $u(x) = \\sin(x) + \\sin(25x)$, is a superposition of a low-frequency component and a high-frequency component. We will demonstrate that gradient-based optimization of the PINN loss causes the network to learn the low-frequency component, $\\sin(x)$, faster than the high-frequency component, $\\sin(25x)$.\n\nFirst, we define the neural network ansatz, a single-hidden-layer perceptron with $H$ neurons and $\\tanh$ activation function:\n$$\nu_{\\theta}(x) = \\sum_{i=1}^{H} a_i \\tanh(w_i x + b_i) + c\n$$\nThe parameters of the network are $\\theta = \\{a_i, w_i, b_i, c\\}_{i=1}^{H}$. To enforce the ODE, we must compute the first and second derivatives of $u_{\\theta}(x)$ with respect to $x$. Using the chain rule and the identities $\\frac{d}{dz}\\tanh(z) = \\operatorname{sech}^2(z)$ and $\\frac{d}{dz}\\operatorname{sech}^2(z) = -2\\operatorname{sech}^2(z)\\tanh(z)$, we obtain:\n$$\nu'_{\\theta}(x) = \\frac{d u_{\\theta}}{dx} = \\sum_{i=1}^{H} a_i w_i \\operatorname{sech}^2(w_i x + b_i)\n$$\n$$\nu''_{\\theta}(x) = \\frac{d^2 u_{\\theta}}{dx^2} = -2 \\sum_{i=1}^{H} a_i w_i^2 \\operatorname{sech}^2(w_i x + b_i) \\tanh(w_i x + b_i)\n$$\n\nThe network is trained by minimizing a loss function composed of the mean squared error of the ODE residual and the boundary condition residuals. The physics residual at a set of $N$ collocation points $\\{x_n\\}$ is:\n$$\nr_{\\text{phys}}(x_n;\\theta) = u_{\\theta}''(x_n) + u_{\\theta}(x_n) + 624 \\sin(25 x_n)\n$$\nThe periodic boundary condition residuals are:\n$$\nr_{\\text{bc},1}(\\theta) = u_{\\theta}(0) - u_{\\theta}(2\\pi)\n$$\n$$\nr_{\\text{bc},2}(\\theta) = u_{\\theta}'(0) - u_{\\theta}'(2\\pi)\n$$\nThe total loss function is a weighted sum:\n$$\n\\mathcal{L}(\\theta) = \\mathcal{L}_{\\text{phys}} + \\lambda_{\\text{bc}} \\mathcal{L}_{\\text{bc}} = \\frac{1}{N}\\sum_{n=1}^{N} r_{\\text{phys}}(x_n;\\theta)^2 + \\lambda_{\\text{bc}}\\left(r_{\\text{bc},1}(\\theta)^2 + r_{\\text{bc},2}(\\theta)^2\\right)\n$$\nwhere $\\lambda_{\\text{bc}}$ is a hyperparameter to balance the terms, given as $\\lambda_{\\text{bc}} = 1$.\n\nTraining is performed using gradient descent. The parameters are updated according to $\\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta} \\mathcal{L}(\\theta)$, where $\\eta$ is the learning rate. We must derive the analytical gradients $\\nabla_{\\theta} \\mathcal{L}(\\theta)$. The gradient of the loss with respect to any parameter $p \\in \\theta$ is:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial p} = \\frac{2}{N}\\sum_{n=1}^{N} r_{\\text{phys}}(x_n) \\left(\\frac{\\partial u''_{\\theta}(x_n)}{\\partial p} + \\frac{\\partial u_{\\theta}(x_n)}{\\partial p}\\right) + 2\\lambda_{\\text{bc}}r_{\\text{bc},1}\\left(\\frac{\\partial u_{\\theta}(0)}{\\partial p} - \\frac{\\partial u_{\\theta}(2\\pi)}{\\partial p}\\right) + 2\\lambda_{\\text{bc}}r_{\\text{bc},2}\\left(\\frac{\\partial u'_{\\theta}(0)}{\\partial p} - \\frac{\\partial u'_{\\theta}(2\\pi)}{\\partial p}\\right)\n$$\nThe derivatives of the network output and its spatial derivatives with respect to the parameters $\\{a_k, w_k, b_k, c\\}$ are computed via the chain rule. These derivations are tedious but systematic and are implemented in vectorized form for computational efficiency. For example, the gradient with respect to an output weight $a_k$ involves terms like $\\frac{\\partial u_{\\theta}(x)}{\\partial a_k} = \\tanh(w_k x + b_k)$. Complete expressions for all gradients are implemented in the code.\n\nAfter training for a specified number of steps, we quantify the learned frequency components. We evaluate the trained network $u_{\\theta}(x)$ over a dense grid of $M$ points $\\{x_m\\}$ in $[0, 2\\pi)$. We then project this learned function onto the basis functions $\\sin(x)$ and $\\sin(25x)$ by solving a linear least-squares problem to find coefficients $(\\hat{\\alpha}_1, \\hat{\\alpha}_{25})$ that minimize:\n$$\n\\sum_{m=1}^{M}\\left(u_{\\theta}(x_m) - \\hat{\\alpha}_1 \\sin(x_m) - \\hat{\\alpha}_{25}\\sin(25 x_m)\\right)^2\n$$\nThe solution to this problem is given by $\\hat{\\boldsymbol{\\alpha}} = (\\mathbf{B}^T\\mathbf{B})^{-1}\\mathbf{B}^T\\mathbf{y}$, where $\\mathbf{y}$ is the vector of network predictions $u_{\\theta}(x_m)$ and $\\mathbf{B}$ is the design matrix with columns $\\sin(x_m)$ and $\\sin(25x_m)$. The learned amplitudes are $A_1 = |\\hat{\\alpha}_1|$ and $A_{25} = |\\hat{\\alpha}_{25}|$. We conclude that spectral bias is observed if $A_1 > A_{25}$.\n\nThe implementation will follow these principles, using `numpy` for vectorized numerical computation, including a fully analytical gradient calculation and a standard gradient descent loop. Parameter initialization will use a fixed random seed and Glorot/Xavier scaling for reproducibility and stable training.",
            "answer": "```python\nimport numpy as np\n\nclass PINN:\n    \"\"\"\n    A Physics-Informed Neural Network to demonstrate spectral bias.\n    The implementation is fully vectorized and uses analytical gradients.\n    \"\"\"\n    def __init__(self, H, N, seed):\n        \"\"\"\n        Initializes the PINN.\n        H: number of hidden units\n        N: number of collocation points\n        seed: random seed for parameter initialization\n        \"\"\"\n        self.H = H\n        self.N = N\n        self.lambda_bc = 1.0\n        self.rng = np.random.default_rng(seed)\n\n        # Xavier/Glorot initialization\n        # For weights w, n_in=1, n_out=1 (conceptual). limit = sqrt(6 / (1+1)) = sqrt(3)\n        limit_w = np.sqrt(3.0)\n        self.w = self.rng.uniform(-limit_w, limit_w, size=(1, self.H))\n        \n        # For weights a, n_in=H, n_out=1. limit = sqrt(6 / (H+1))\n        limit_a = np.sqrt(6.0 / (self.H + 1.0))\n        self.a = self.rng.uniform(-limit_a, limit_a, size=(self.H, 1))\n\n        self.b = np.zeros((1, self.H))\n        self.c = np.zeros((1, 1))\n\n        self.x_colloc = np.linspace(0, 2 * np.pi, self.N, endpoint=False).reshape(-1, 1)\n\n    def forward(self, x):\n        \"\"\"\n        Computes the network output u and its derivatives u', u'' w.r.t. x.\n        x: input points, shape (num_points, 1)\n        \"\"\"\n        z = x @ self.w + self.b\n        h = np.tanh(z)\n        s = 1.0 - h**2  # sech^2(z)\n\n        u = h @ self.a + self.c\n        u_prime = (s * self.w) @ self.a\n        u_double_prime = (-2.0 * s * h * (self.w**2)) @ self.a\n\n        return u, u_prime, u_double_prime, h, s\n\n    def _compute_gradients(self):\n        \"\"\"\n        Computes the loss and the gradients of the loss w.r.t. all parameters.\n        All calculations are vectorized.\n        \"\"\"\n        # --- Physics Loss and Gradients ---\n        u, _, u_pp, H_c, S_c = self.forward(self.x_colloc)\n        \n        f_term = -624.0 * np.sin(25.0 * self.x_colloc)\n        r_phys = u_pp + u - f_term\n        loss_phys = np.mean(r_phys**2)\n\n        # Common factor for physics gradients\n        grad_common_phys = (2.0 / self.N) * r_phys\n        \n        # Gradient w.r.t. a\n        d_u_da = H_c\n        d_u_pp_da = -2.0 * S_c * H_c * self.w**2\n        grad_a_phys = (d_u_da + d_u_pp_da).T @ grad_common_phys\n\n        # Gradient w.r.t. b\n        d_u_db = self.a.T * S_c\n        d_u_pp_db = self.a.T * (4.0 * self.w**2 * S_c * H_c**2 - 2.0 * self.w**2 * S_c**2)\n        grad_b_phys = np.sum(grad_common_phys * (d_u_db + d_u_pp_db), axis=0)\n\n        # Gradient w.r.t. w\n        d_u_dw = self.a.T * self.x_colloc * S_c\n        d_u_pp_dw = self.a.T * (\n            -4.0 * self.w * S_c * H_c + \n            self.x_colloc * (4.0 * self.w**2 * S_c * H_c**2 - 2.0 * self.w**2 * S_c**2)\n        )\n        grad_w_phys = np.sum(grad_common_phys * (d_u_dw + d_u_pp_dw), axis=0)\n        \n        # Gradient w.r.t. c\n        grad_c_phys = np.sum(grad_common_phys)\n\n        # --- Boundary Loss and Gradients ---\n        x_bc = np.array([[0.0], [2 * np.pi]])\n        u_bc, u_p_bc, _, H_bc, S_bc = self.forward(x_bc)\n        \n        u0, u2pi = u_bc[0], u_bc[1]\n        u0_p, u2pi_p = u_p_bc[0], u_p_bc[1]\n\n        r_bc1 = u0 - u2pi\n        r_bc2 = u0_p - u2pi_p\n        loss_bc = r_bc1**2 + r_bc2**2\n        \n        H0, H2pi = H_bc[0:1, :], H_bc[1:2, :]\n        S0, S2pi = S_bc[0:1, :], S_bc[1:2, :]\n        \n        # Common factors for BC gradients\n        common1 = 2.0 * self.lambda_bc * r_bc1\n        common2 = 2.0 * self.lambda_bc * r_bc2\n\n        # Gradient w.r.t. a\n        delta_u_da = (H0 - H2pi).T\n        delta_u_p_da = (self.w * (S0 - S2pi)).T\n        grad_a_bc = common1 * delta_u_da + common2 * delta_u_p_da\n        \n        # Gradient w.r.t. b\n        delta_u_db = self.a.T * (S0 - S2pi)\n        delta_u_p_db = -2.0 * self.a.T * self.w * (S0 * H0 - S2pi * H2pi)\n        grad_b_bc = common1 * delta_u_db + common2 * delta_u_p_db\n\n        # Gradient w.r.t. w\n        delta_u_dw = -2.0 * np.pi * self.a.T * S2pi\n        delta_u_p_dw = self.a.T * (S0 - S2pi) + 4.0 * np.pi * self.a.T * self.w * S2pi * H2pi\n        grad_w_bc = common1 * delta_u_dw + common2 * delta_u_p_dw\n\n        # Gradient w.r.t. c (is zero)\n        grad_c_bc = 0.0\n\n        # --- Total Loss and Gradients ---\n        loss = loss_phys + self.lambda_bc * loss_bc\n        grad_a = grad_a_phys + grad_a_bc\n        grad_w = grad_w_phys.reshape(1, -1) + grad_w_bc\n        grad_b = grad_b_phys.reshape(1, -1) + grad_b_bc\n        grad_c = grad_c_phys + grad_c_bc\n\n        return loss, grad_a, grad_w, grad_b, grad_c\n\n    def train(self, K, eta):\n        \"\"\"\n        Trains the network using gradient descent.\n        K: number of training steps\n        eta: learning rate\n        \"\"\"\n        for _ in range(K):\n            loss, grad_a, grad_w, grad_b, grad_c = self._compute_gradients()\n            \n            self.a -= eta * grad_a\n            self.w -= eta * grad_w\n            self.b -= eta * grad_b\n            self.c -= eta * grad_c\n\n    def project_and_analyze(self):\n        \"\"\"\n        Projects the learned function onto sin(x) and sin(25x) and checks for spectral bias.\n        \"\"\"\n        M = 4096\n        x_dense = np.linspace(0, 2 * np.pi, M, endpoint=False).reshape(-1, 1)\n        \n        u_pred, _, _, _, _ = self.forward(x_dense)\n        u_pred = u_pred.flatten()\n        \n        # Create design matrix for least squares\n        B = np.zeros((M, 2))\n        B[:, 0] = np.sin(x_dense.flatten())\n        B[:, 1] = np.sin(25.0 * x_dense.flatten())\n        \n        # Solve least squares problem: B * alpha = u_pred\n        alpha, _, _, _ = np.linalg.lstsq(B, u_pred, rcond=None)\n        \n        A1 = np.abs(alpha[0])\n        A25 = np.abs(alpha[1])\n        \n        return A1 > A25\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and produce the final output.\n    \"\"\"\n    test_cases = [\n        (20, 128, 60, 0.01),  # Case 1: (H, N, K, eta)\n        (10, 64, 80, 0.01),   # Case 2\n        (5, 128, 120, 0.01),  # Case 3\n    ]\n\n    results = []\n    base_seed = 42\n\n    for i, (H, N, K, eta) in enumerate(test_cases):\n        seed = base_seed + i\n        pinn = PINN(H=H, N=N, seed=seed)\n        pinn.train(K=K, eta=eta)\n        result = pinn.project_and_analyze()\n        results.append(result)\n\n    # Format the final output as specified\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}