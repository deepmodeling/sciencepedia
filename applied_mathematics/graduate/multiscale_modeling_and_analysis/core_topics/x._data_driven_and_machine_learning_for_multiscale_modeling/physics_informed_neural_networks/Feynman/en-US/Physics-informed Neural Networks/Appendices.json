{
    "hands_on_practices": [
        {
            "introduction": "A core challenge in applying neural networks to differential equations is ensuring the solution adheres to prescribed boundary conditions. While these conditions can be enforced as soft penalties in the loss function, a more elegant and robust approach is to design the network architecture to satisfy them by construction. This practice, often called \"hard-coding\" boundary conditions, simplifies the optimization landscape and guarantees the solution is exact on the boundary throughout training, allowing the optimizer to focus solely on satisfying the physics in the domain's interior .",
            "id": "2126300",
            "problem": "In the field of scientific computing, Physics-Informed Neural Networks (PINNs) have emerged as a powerful tool for solving differential equations. A key aspect of designing a PINN is ensuring that its output, which approximates the solution, respects the given boundary conditions. One robust method to achieve this is to structure the network's final output function so that it satisfies these conditions by construction.\n\nConsider a one-dimensional problem on the spatial domain $x \\in [0, L]$. A neural network provides a raw, unconstrained output function denoted by $\\hat{u}_{NN}(x)$. We wish to use this network to find an approximate solution, $u(x)$, to a differential equation that is subject to the following non-homogeneous Dirichlet boundary conditions:\n$$u(0) = A$$\n$$u(L) = B$$\nHere, $A$, $B$, and $L > 0$ are given real constants.\n\nYour task is to devise a transformation that takes the raw network output $\\hat{u}_{NN}(x)$ and produces a new function, $u_{NN}(x)$, that serves as the final approximation. This transformation must guarantee that $u_{NN}(x)$ strictly satisfies the specified boundary conditions, regardless of the function $\\hat{u}_{NN}(x)$ produced by the network.\n\nProvide an expression for $u_{NN}(x)$ in terms of the raw network output $\\hat{u}_{NN}(x)$ and the parameters $x$, $L$, $A$, and $B$.",
            "solution": "We seek a transformation that maps the raw network output $\\hat{u}_{NN}(x)$ to a function $u_{NN}(x)$ that enforces the Dirichlet boundary conditions $u_{NN}(0)=A$ and $u_{NN}(L)=B$ for any $\\hat{u}_{NN}(x)$. A standard construction is to decompose $u_{NN}(x)$ as\n$$\nu_{NN}(x)=g(x)+s(x)\\,\\hat{u}_{NN}(x),\n$$\nwhere $g(x)$ is any fixed function that satisfies the boundary conditions and $s(x)$ is any function that vanishes at both boundaries. Specifically, we require\n$$\ng(0)=A,\\quad g(L)=B,\\quad s(0)=0,\\quad s(L)=0.\n$$\nA convenient choice is the linear interpolant for $g(x)$,\n$$\ng(x)=A\\left(1-\\frac{x}{L}\\right)+B\\left(\\frac{x}{L}\\right)=A+\\frac{B-A}{L}\\,x,\n$$\nand the simple vanishing factor\n$$\ns(x)=x(L-x),\n$$\nwhich satisfies $s(0)=0$ and $s(L)=0$. Therefore, define\n$$\nu_{NN}(x)=A\\left(1-\\frac{x}{L}\\right)+B\\left(\\frac{x}{L}\\right)+x(L-x)\\,\\hat{u}_{NN}(x).\n$$\nTo verify the boundary conditions, evaluate at $x=0$ and $x=L$:\n$$\nu_{NN}(0)=A\\left(1-0\\right)+B\\left(0\\right)+0\\cdot L\\,\\hat{u}_{NN}(0)=A,\n$$\n$$\nu_{NN}(L)=A\\left(1-1\\right)+B\\left(\\frac{L}{L}\\right)+L( L-L)\\,\\hat{u}_{NN}(L)=B.\n$$\nThus, for any $\\hat{u}_{NN}(x)$, the constructed $u_{NN}(x)$ strictly satisfies $u_{NN}(0)=A$ and $u_{NN}(L)=B$.",
            "answer": "$$\\boxed{A\\left(1-\\frac{x}{L}\\right)+B\\left(\\frac{x}{L}\\right)+x\\left(L-x\\right)\\hat{u}_{NN}(x)}$$"
        },
        {
            "introduction": "The performance of any numerical optimization, including the training of a PINN, is highly dependent on the conditioning of the problem. When physical quantities in a governing equation span vastly different scales, the loss function can become ill-conditioned, hindering the training process. This exercise demonstrates the power of nondimensionalization, a classical technique from physics and engineering, to precondition the PDE before it is ever presented to the network, ensuring all terms in the residual are of comparable magnitude and leading to more stable and efficient training .",
            "id": "4235875",
            "problem": "A digital twin of a one-dimensional viscous transport process in a cyber-physical system is modeled by the viscous Burgers equation, which at the physical level reads $u_{t} + u u_{x} - \\nu u_{xx} = 0$ on a spatial interval of length $L$ and a time horizon of interest. A Physics-Informed Neural Network (PINN) is to be trained on this model by enforcing the partial differential equation residual at collocation points together with initial and boundary data. To promote robust optimization across different operating regimes, you will nondimensionalize the model.\n\nStarting from the physical equation and the core definitions of characteristic scales for space, time, and velocity magnitudes, introduce dimensionless variables by setting $x = L \\,\\xi$, $t = T \\,\\tau$, and $u = U \\,v$, where $L$ and $U$ are fixed characteristic length and velocity scales, and $T$ is a characteristic time scale to be determined. Derive the nondimensional form of the equation for $v(\\xi,\\tau)$ using only these scalings and the chain rule. Choose $T$ so that the nondimensional equation has order-one coefficients for both the temporal derivative and the convective nonlinearity. Identify the single dimensionless group multiplying the diffusion term that controls the balance between convection and diffusion.\n\nThen, using first principles about gradient-based optimization and conditioning, provide a concise argument for why training a Physics-Informed Neural Network (PINN) on the nondimensional form can improve optimization stability and convergence when implemented in the digital twin.\n\nYour final reported answer must be the analytic expression for the Reynolds number associated with this nondimensionalization in terms of $U$, $L$, and $\\nu$. Do not include units. If you choose to perform any intermediate numerical evaluations, do not report them. The final answer must be a single closed-form expression.",
            "solution": "The problem requires the nondimensionalization of the one-dimensional viscous Burgers equation, an analysis of the resulting dimensionless group, and an argument for why this procedure benefits the training of a Physics-Informed Neural Network (PINN).\n\nThe physical form of the viscous Burgers equation is given as:\n$$\nu_{t} + u u_{x} - \\nu u_{xx} = 0\n$$\nwhere $u(x,t)$ is the velocity, $\\nu$ is the kinematic viscosity, $u_t = \\frac{\\partial u}{\\partial t}$, $u_x = \\frac{\\partial u}{\\partial x}$, and $u_{xx} = \\frac{\\partial^2 u}{\\partial x^2}$.\n\nThe problem specifies the introduction of dimensionless variables $\\xi$, $\\tau$, and $v$ through the following scaling relations:\n$$\nx = L \\,\\xi, \\quad t = T \\,\\tau, \\quad u = U \\,v\n$$\nHere, $L$ is a characteristic length scale and $U$ is a characteristic velocity scale. The characteristic time scale $T$ is to be determined. The function $v$ is the dimensionless velocity, which depends on the dimensionless space and time variables, i.e., $v = v(\\xi, \\tau)$.\n\nTo transform the partial differential equation (PDE), we must express the partial derivatives of $u$ with respect to $x$ and $t$ in terms of the partial derivatives of $v$ with respect to $\\xi$ and $\\tau$. We use the chain rule for this transformation.\n\nFirst, we find the temporal derivative, $u_t$:\n$$\nu_{t} = \\frac{\\partial u}{\\partial t} = \\frac{\\partial (Uv)}{\\partial t} = U \\frac{\\partial v}{\\partial t} = U \\frac{\\partial v}{\\partial \\tau} \\frac{\\partial \\tau}{\\partial t}\n$$\nFrom the relation $t = T\\tau$, we have $\\frac{\\partial \\tau}{\\partial t} = \\frac{1}{T}$. Therefore, the temporal derivative becomes:\n$$\nu_{t} = \\frac{U}{T} \\frac{\\partial v}{\\partial \\tau} = \\frac{U}{T} v_{\\tau}\n$$\n\nNext, we find the first spatial derivative, $u_x$:\n$$\nu_{x} = \\frac{\\partial u}{\\partial x} = \\frac{\\partial (Uv)}{\\partial x} = U \\frac{\\partial v}{\\partial x} = U \\frac{\\partial v}{\\partial \\xi} \\frac{\\partial \\xi}{\\partial x}\n$$\nFrom the relation $x = L\\xi$, we have $\\frac{\\partial \\xi}{\\partial x} = \\frac{1}{L}$. Thus, the first spatial derivative is:\n$$\nu_{x} = \\frac{U}{L} \\frac{\\partial v}{\\partial \\xi} = \\frac{U}{L} v_{\\xi}\n$$\n\nFinally, we find the second spatial derivative, $u_{xx}$:\n$$\nu_{xx} = \\frac{\\partial^2 u}{\\partial x^2} = \\frac{\\partial}{\\partial x} \\left( \\frac{\\partial u}{\\partial x} \\right) = \\frac{\\partial}{\\partial x} \\left( \\frac{U}{L} v_{\\xi} \\right) = \\frac{U}{L} \\frac{\\partial v_{\\xi}}{\\partial x} = \\frac{U}{L} \\left( \\frac{\\partial v_{\\xi}}{\\partial \\xi} \\frac{\\partial \\xi}{\\partial x} \\right) = \\frac{U}{L} \\left( v_{\\xi\\xi} \\frac{1}{L} \\right)\n$$\nThis simplifies to:\n$$\nu_{xx} = \\frac{U}{L^2} v_{\\xi\\xi}\n$$\n\nNow, we substitute these expressions for the derivatives and for $u$ itself back into the original viscous Burgers equation:\n$$\n\\left( \\frac{U}{T} v_{\\tau} \\right) + (Uv) \\left( \\frac{U}{L} v_{\\xi} \\right) - \\nu \\left( \\frac{U}{L^2} v_{\\xi\\xi} \\right) = 0\n$$\nCombining the coefficients, we get:\n$$\n\\frac{U}{T} v_{\\tau} + \\frac{U^2}{L} v v_{\\xi} - \\frac{\\nu U}{L^2} v_{\\xi\\xi} = 0\n$$\nTo obtain the nondimensional form where coefficients are of order-one, we can divide the entire equation by one of the coefficients. A standard choice is to divide by the coefficient of the nonlinear convective term, $\\frac{U^2}{L}$, to make its coefficient exactly $1$:\n$$\n\\left( \\frac{U}{T} \\frac{L}{U^2} \\right) v_{\\tau} + \\left( \\frac{U^2}{L} \\frac{L}{U^2} \\right) v v_{\\xi} - \\left( \\frac{\\nu U}{L^2} \\frac{L}{U^2} \\right) v_{\\xi\\xi} = 0\n$$\nSimplifying the coefficients yields:\n$$\n\\left( \\frac{L}{UT} \\right) v_{\\tau} + v v_{\\xi} - \\left( \\frac{\\nu}{UL} \\right) v_{\\xi\\xi} = 0\n$$\nThe problem requires choosing the characteristic time scale $T$ such that the coefficients of the temporal derivative ($v_\\tau$) and the convective nonlinearity ($v v_\\xi$) are both of order-one. We have already set the coefficient of $v v_\\xi$ to $1$. To set the coefficient of $v_\\tau$ also to $1$, we must have:\n$$\n\\frac{L}{UT} = 1 \\implies T = \\frac{L}{U}\n$$\nThis choice for $T$ represents the convective time scale: the time it takes for a fluid parcel to travel the characteristic length $L$ at the characteristic velocity $U$.\n\nSubstituting this expression for $T$ back into the scaled equation, we obtain the final nondimensional form of the viscous Burgers equation:\n$$\nv_{\\tau} + v v_{\\xi} - \\left( \\frac{\\nu}{UL} \\right) v_{\\xi\\xi} = 0\n$$\nThe single dimensionless group multiplying the diffusion term ($v_{\\xi\\xi}$) is $\\frac{\\nu}{UL}$. This group is the reciprocal of the Reynolds number, a fundamental dimensionless parameter in fluid mechanics that quantifies the ratio of inertial forces to viscous forces. The Reynolds number, denoted $Re$, is defined as:\n$$\nRe = \\frac{\\text{inertial forces}}{\\text{viscous forces}} \\propto \\frac{U^2/L}{\\nu U/L^2} = \\frac{UL}{\\nu}\n$$\nThus, the dimensionless coefficient is $\\frac{1}{Re}$, and the equation is:\n$$\nv_{\\tau} + v v_{\\xi} - \\frac{1}{Re} v_{\\xi\\xi} = 0\n$$\nThe expression for the Reynolds number is therefore $\\frac{UL}{\\nu}$.\n\nThe argument for why training a PINN on the nondimensional form is advantageous relates to the numerical conditioning of the optimization problem. A PINN is trained by minimizing a loss function that includes the residual of the governing PDE. For the original physical equation, the PDE residual is $r = u_t + u u_x - \\nu u_{xx}$. The loss is typically the mean squared error of this residual over a set of collocation points, $\\mathcal{L}_{PDE} = \\langle (u_t + u u_x - \\nu u_{xx})^2 \\rangle$.\n\nIn a physical system, the magnitudes of the terms $u_t$, $u u_x$, and $\\nu u_{xx}$ can vary by many orders of magnitude. For instance, in a high-speed, low-viscosity flow, the convective term $u u_x$ may be far larger than the viscous term $\\nu u_{xx}$. When a gradient-based optimizer (e.g., Adam) minimizes the loss function, the gradients of the loss with respect to the neural network's parameters (weights and biases) will be dominated by the largest terms in the residual. This creates an ill-conditioned optimization landscape with steep \"canyons\" and flat \"plateaus,\" making it difficult for the optimizer to converge to a good solution. The network may learn to satisfy the dominant physics (e.g., convection) while largely ignoring the sub-dominant physics (e.g., diffusion), leading to an inaccurate model.\n\nNondimensionalization acts as a form of preconditioning. By scaling variables such that $v, \\xi, \\tau$ are all of order-one, we ensure that the terms in the nondimensional residual, $r' = v_\\tau + v v_\\xi - \\frac{1}{Re} v_{\\xi\\xi}$, are of comparable magnitude (unless $Re$ is extremely large or small, in which case the physics enters a distinct asymptotic regime). For a moderate $Re$, the coefficients are all of order-one. This balancing of terms leads to a better-conditioned loss landscape. The gradients originating from each physical term (temporal change, convection, diffusion) are more evenly scaled. As a result, the optimizer can make more uniform progress in minimizing the contributions from all physical effects simultaneously, leading to more stable and efficient training, and ultimately, a more accurate PINN-based digital twin.",
            "answer": "$$\n\\boxed{\\frac{UL}{\\nu}}\n$$"
        },
        {
            "introduction": "Physics-Informed Neural Networks are not immune to the classical numerical pathologies that challenge traditional methods like the Finite Element Method. A prime example is \"volumetric locking\" in solid mechanics, which occurs when modeling nearly incompressible materials. This exercise explores how this physical limit manifests as a severe ill-conditioning in the PINN loss function and introduces the \"mixed formulation\" as a powerful technique to reformulate the physics, thereby circumventing the numerical instability and enabling stable training in these challenging physical regimes .",
            "id": "2668960",
            "problem": "Consider small-strain, static, isotropic linear elasticity in spatial dimension $d \\in \\{2,3\\}$. The Cauchy stress tensor $\\boldsymbol{\\sigma}$ and the small-strain tensor $\\boldsymbol{\\varepsilon}(\\mathbf{u}) = \\tfrac{1}{2}\\left(\\nabla \\mathbf{u} + \\nabla \\mathbf{u}^{\\top}\\right)$ are related by the isotropic Hooke law $\\boldsymbol{\\sigma} = 2\\,\\mu\\,\\boldsymbol{\\varepsilon} + \\lambda\\,\\mathrm{tr}(\\boldsymbol{\\varepsilon})\\,\\mathbf{I}$, where $(\\lambda,\\mu)$ are the Lamé parameters. The Young’s modulus $E$ and the Poisson’s ratio $\\nu$ are defined by a uniaxial test: prescribe a uniaxial Cauchy stress $\\sigma_{11} = \\sigma$ with lateral stresses $\\sigma_{22}=\\sigma_{33}=0$ (for $d=3$; adapt accordingly for $d=2$ plane stress), and define $E = \\sigma_{11}/\\varepsilon_{11}$ and $\\nu = -\\varepsilon_{22}/\\varepsilon_{11} = -\\varepsilon_{33}/\\varepsilon_{11}$ under this loading. Starting from these definitions and the constitutive law, derive $(\\lambda,\\mu)$ in terms of $(E,\\nu)$.\n\nNow consider training a physics-informed neural network (PINN) that outputs the displacement field $\\mathbf{u}(\\mathbf{x})$ to solve the equilibrium partial differential equation (PDE) $\\nabla \\cdot \\boldsymbol{\\sigma}(\\mathbf{u}) = \\mathbf{0}$ with given boundary conditions, using the above constitutive model. Discuss, from first principles, how the limit $\\nu \\to 0.5$ (nearly incompressible) impacts the conditioning of the PDE residual and the learnability of $\\mathbf{u}$ in a displacement-only PINN, and what modeling or loss-design changes can remedy this.\n\nWhich of the following statements are correct?\n\nA. The derived mapping yields $\\mu = \\dfrac{E}{2(1+\\nu)}$ and $\\lambda = \\dfrac{E\\,\\nu}{(1+\\nu)(1-2\\nu)}$.\n\nB. As $\\nu \\to 0.5$, the parameter $\\lambda$ remains bounded while $\\mu \\to \\infty$, so the shear term dominates the PDE residual; therefore near-incompressibility does not cause ill-conditioning in displacement-only PINNs.\n\nC. In the nearly incompressible regime, a displacement-only PINN commonly suffers from an ill-conditioned loss landscape dominated by volumetric contributions. Simply rescaling inputs and outputs or reweighting loss terms is, by itself, sufficient to fully resolve training pathologies for all $\\nu$ arbitrarily close to $0.5$.\n\nD. A robust remedy is a mixed displacement–pressure formulation in which one introduces $p$ as an additional unknown and writes $\\boldsymbol{\\sigma}(\\mathbf{u},p) = 2\\,\\mu\\,\\boldsymbol{\\varepsilon}^{\\mathrm{dev}}(\\mathbf{u}) - p\\,\\mathbf{I}$, together with a volumetric closure $p + \\kappa\\,\\nabla \\cdot \\mathbf{u} = 0$ where $\\kappa = \\lambda + \\frac{2}{3}\\mu$ is the bulk modulus. The momentum equation becomes $\\nabla \\cdot \\left(2\\,\\mu\\,\\boldsymbol{\\varepsilon}^{\\mathrm{dev}}(\\mathbf{u})\\right) - \\nabla p = \\mathbf{0}$, which avoids unbounded coefficients in the momentum residual as $\\nu \\to 0.5$ and can be stably enforced in a PINN by separately scaling the constraint.\n\nE. Adaptive loss balancing based on the Neural Tangent Kernel (NTK) guarantees that a displacement-only PINN trained on the original momentum residual with $(\\lambda,\\mu)$ remains well-conditioned as $\\nu \\to 0.5$, obviating the need for mixed formulations.",
            "solution": "### Derivation and Analysis\n\n**Part 1: Derivation of $(\\lambda, \\mu)$ in terms of $(E, \\nu)$**\n\nWe start with the isotropic Hooke's law:\n$$\n\\boldsymbol{\\sigma} = 2\\,\\mu\\,\\boldsymbol{\\varepsilon} + \\lambda\\,\\mathrm{tr}(\\boldsymbol{\\varepsilon})\\,\\mathbf{I}\n$$\nWe need to invert this relation to express $\\boldsymbol{\\varepsilon}$ as a function of $\\boldsymbol{\\sigma}$. Taking the trace of the equation (for dimension $d=3$):\n$$\n\\mathrm{tr}(\\boldsymbol{\\sigma}) = \\mathrm{tr}(2\\,\\mu\\,\\boldsymbol{\\varepsilon} + \\lambda\\,\\mathrm{tr}(\\boldsymbol{\\varepsilon})\\,\\mathbf{I}) = 2\\,\\mu\\,\\mathrm{tr}(\\boldsymbol{\\varepsilon}) + \\lambda\\,\\mathrm{tr}(\\boldsymbol{\\varepsilon})\\,\\mathrm{tr}(\\mathbf{I}) = (2\\mu + 3\\lambda)\\,\\mathrm{tr}(\\boldsymbol{\\varepsilon})\n$$\nThus, the volumetric strain is:\n$$\n\\mathrm{tr}(\\boldsymbol{\\varepsilon}) = \\frac{\\mathrm{tr}(\\boldsymbol{\\sigma})}{2\\mu + 3\\lambda}\n$$\nSubstituting this back into the first equation:\n$$\n\\boldsymbol{\\sigma} = 2\\,\\mu\\,\\boldsymbol{\\varepsilon} + \\lambda \\left(\\frac{\\mathrm{tr}(\\boldsymbol{\\sigma})}{2\\mu + 3\\lambda}\\right)\\mathbf{I}\n$$\nSolving for $\\boldsymbol{\\varepsilon}$:\n$$\n2\\,\\mu\\,\\boldsymbol{\\varepsilon} = \\boldsymbol{\\sigma} - \\frac{\\lambda\\,\\mathrm{tr}(\\boldsymbol{\\sigma})}{2\\mu + 3\\lambda}\\,\\mathbf{I} \\implies \\boldsymbol{\\varepsilon} = \\frac{1}{2\\mu}\\boldsymbol{\\sigma} - \\frac{\\lambda}{2\\mu(2\\mu + 3\\lambda)}\\mathrm{tr}(\\boldsymbol{\\sigma})\\,\\mathbf{I}\n$$\nNow, we apply the conditions of the uniaxial test: $\\sigma_{11} = \\sigma$ and all other $\\sigma_{ij}=0$. This gives $\\mathrm{tr}(\\boldsymbol{\\sigma}) = \\sigma$.\nThe strain components are:\n$$\n\\varepsilon_{11} = \\frac{1}{2\\mu}\\sigma_{11} - \\frac{\\lambda}{2\\mu(2\\mu + 3\\lambda)}\\sigma = \\frac{\\sigma}{2\\mu}\\left(1 - \\frac{\\lambda}{2\\mu + 3\\lambda}\\right) = \\frac{\\sigma}{2\\mu}\\left(\\frac{2\\mu + 3\\lambda - \\lambda}{2\\mu + 3\\lambda}\\right) = \\sigma\\frac{\\mu+\\lambda}{\\mu(2\\mu+3\\lambda)}\n$$\n$$\n\\varepsilon_{22} = \\frac{1}{2\\mu}\\sigma_{22} - \\frac{\\lambda}{2\\mu(2\\mu + 3\\lambda)}\\sigma = 0 - \\frac{\\lambda\\sigma}{2\\mu(2\\mu + 3\\lambda)}\n$$\nUsing the definitions of $E$ and $\\nu$:\n$$\nE = \\frac{\\sigma_{11}}{\\varepsilon_{11}} = \\frac{\\sigma}{\\sigma\\frac{\\mu+\\lambda}{\\mu(2\\mu+3\\lambda)}} = \\frac{\\mu(2\\mu+3\\lambda)}{\\mu+\\lambda}\n$$\n$$\n\\nu = -\\frac{\\varepsilon_{22}}{\\varepsilon_{11}} = -\\frac{-\\lambda\\sigma / [2\\mu(2\\mu+3\\lambda)]}{\\sigma(\\mu+\\lambda) / [\\mu(2\\mu+3\\lambda)]} = \\frac{\\lambda}{2(\\mu+\\lambda)}\n$$\nWe now solve the system of two equations for $\\mu$ and $\\lambda$. From the equation for $\\nu$:\n$2\\nu(\\mu+\\lambda) = \\lambda \\implies 2\\nu\\mu = \\lambda(1-2\\nu) \\implies \\lambda = \\frac{2\\nu\\mu}{1-2\\nu}$.\nSubstitute this into the equation for $E$:\n$E = 2\\mu(1+\\nu)$. To see this: $E = \\frac{\\mu(2\\mu+3\\lambda)}{\\mu+\\lambda} = \\frac{\\mu(2\\mu+3\\frac{2\\nu\\mu}{1-2\\nu})}{\\mu+\\frac{2\\nu\\mu}{1-2\\nu}} = \\mu \\frac{2(1-2\\nu)+6\\nu}{1-2\\nu+2\\nu} = \\mu(2-4\\nu+6\\nu) = 2\\mu(1+\\nu)$.\nFrom $E = 2\\mu(1+\\nu)$, we find the shear modulus $\\mu$:\n$$\n\\mu = \\frac{E}{2(1+\\nu)}\n$$\nNow substitute this $\\mu$ back into the expression for $\\lambda$:\n$$\n\\lambda = \\frac{2\\nu}{1-2\\nu}\\mu = \\frac{2\\nu}{1-2\\nu} \\frac{E}{2(1+\\nu)} = \\frac{E\\nu}{(1+\\nu)(1-2\\nu)}\n$$\nThe derivation is complete.\n\n**Part 2: Analysis of PINNs for Nearly Incompressible Elasticity ($\\nu \\to 0.5$)**\n\nThe equilibrium PDE in terms of displacement is the Navier-Cauchy equation:\n$$\n\\nabla \\cdot \\boldsymbol{\\sigma} = \\mu \\nabla^2 \\mathbf{u} + (\\lambda+\\mu) \\nabla(\\nabla \\cdot \\mathbf{u}) = \\mathbf{0}\n$$\nA displacement-only PINN minimizes a loss based on the residual of this equation. We examine the behavior of the coefficients as $\\nu \\to 0.5$:\n-   $\\mu = \\frac{E}{2(1+\\nu)} \\to \\frac{E}{2(1.5)} = \\frac{E}{3}$. The coefficient $\\mu$ remains bounded and well-behaved.\n-   $\\lambda = \\frac{E\\nu}{(1+\\nu)(1-2\\nu)} \\to \\infty$ because the denominator $(1-2\\nu) \\to 0$.\nThe coefficient of the volumetric term, $(\\lambda+\\mu)$, therefore also diverges: $(\\lambda+\\mu) \\to \\infty$.\n\nThe PDE becomes dominated by the term $(\\lambda+\\mu) \\nabla(\\nabla \\cdot \\mathbf{u})$. For the equation to hold, the network must learn a displacement field $\\mathbf{u}$ such that $\\nabla \\cdot \\mathbf{u} \\approx 0$ with extreme precision. The loss function becomes\n$\\mathcal{L}_{PDE} \\approx ||(\\lambda+\\mu) \\nabla(\\nabla \\cdot \\mathbf{u})||^2$. The extremely large pre-factor $(\\lambda+\\mu)$ creates an ill-conditioned loss landscape with enormous gradients related to any non-zero volumetric strain. This phenomenon, known as **volumetric locking**, makes it extremely difficult for gradient-based optimizers to find a good solution. The training stagnates, unable to resolve the shear behavior because the loss is completely dominated by the penalty on volumetric deformation.\n\n### Evaluation of Options\n\n**A. The derived mapping yields $\\mu = \\dfrac{E}{2(1+\\nu)}$ and $\\lambda = \\dfrac{E\\,\\nu}{(1+\\nu)(1-2\\nu)}$.**\nOur derivation in Part 1 confirms these expressions precisely.\n**Verdict: Correct.**\n\n**B. As $\\nu \\to 0.5$, the parameter $\\lambda$ remains bounded while $\\mu \\to \\infty$, so the shear term dominates the PDE residual; therefore near-incompressibility does not cause ill-conditioning in displacement-only PINNs.**\nThis statement is incorrect on multiple counts. As derived, when $\\nu \\to 0.5$, $\\mu$ remains bounded while $\\lambda \\to \\infty$. The term that dominates the PDE is the volumetric term containing $\\lambda$, not the shear term containing $\\mu$. Consequently, near-incompressibility is a classic cause of severe ill-conditioning (locking).\n**Verdict: Incorrect.**\n\n**C. In the nearly incompressible regime, a displacement-only PINN commonly suffers from an ill-conditioned loss landscape dominated by volumetric contributions. Simply rescaling inputs and outputs or reweighting loss terms is, by itself, sufficient to fully resolve training pathologies for all $\\nu$ arbitrarily close to $0.5$.**\nThe first part of the statement, describing the ill-conditioned loss landscape, is correct. However, the second part makes an overly strong claim. While simple techniques like reweighting loss terms can provide some relief for moderate values of $\\nu$, they are not a \"sufficient\" remedy to \"fully resolve\" the pathology for $\\nu$ arbitrarily close to $0.5$. The problem is fundamental to the displacement-only formulation, which becomes singular in the limit. Such simple fixes do not change the formulation and cannot robustly handle the singularity. A more profound change, such as a mixed formulation, is required for robust performance in the severe incompressible regime.\n**Verdict: Incorrect.**\n\n**D. A robust remedy is a mixed displacement–pressure formulation in which one introduces $p$ as an additional unknown and writes $\\boldsymbol{\\sigma}(\\mathbf{u},p) = 2\\,\\mu\\,\\boldsymbol{\\varepsilon}^{\\mathrm{dev}}(\\mathbf{u}) - p\\,\\mathbf{I}$, together with a volumetric closure $p + \\kappa\\,\\nabla \\cdot \\mathbf{u} = 0$ where $\\kappa = \\lambda + \\tfrac{2}{3}\\mu$ is the bulk modulus. The momentum equation becomes $\\nabla \\cdot \\left(2\\,\\mu\\,\\boldsymbol{\\varepsilon}^{\\mathrm{dev}}(\\mathbf{u})\\right) - \\nabla p = \\mathbf{0}$, which avoids unbounded coefficients in the momentum residual as $\\nu \\to 0.5$ and can be stably enforced in a PINN by separately scaling the constraint.**\nThis accurately describes the standard mixed $u-p$ formulation. The decomposition of stress into deviatoric and hydrostatic parts, $\\boldsymbol{\\sigma} = 2\\mu\\boldsymbol{\\varepsilon}^{\\mathrm{dev}} - p\\mathbf{I}$, with pressure $p = -\\kappa \\nabla \\cdot \\mathbf{u}$, is correct. The resulting equilibrium equation $\\nabla \\cdot (2\\mu\\boldsymbol{\\varepsilon}^{\\mathrm{dev}}) - \\nabla p = \\mathbf{0}$ involves only the bounded shear modulus $\\mu$. The unboundedness is isolated in the scalar constitutive relation for pressure, $p + \\kappa \\nabla \\cdot \\mathbf{u} = 0$, where the bulk modulus $\\kappa = E/(3(1-2\\nu)) \\to \\infty$. By treating the momentum equation and the pressure constraint as two separate residuals in the PINN loss, one can apply separate scaling or weighting to manage the stiff constraint, leading to a much more stable training process. This is the standard and effective approach to overcome volumetric locking.\n**Verdict: Correct.**\n\n**E. Adaptive loss balancing based on the Neural Tangent Kernel (NTK; neural tangent kernel) guarantees that a displacement-only PINN trained on the original momentum residual with $(\\lambda,\\mu)$ remains well-conditioned as $\\nu \\to 0.5$, obviating the need for mixed formulations.**\nThis is an overstatement. NTK-based weighting typically balances different loss *terms* (e.g., PDE residual vs. boundary condition residual). It does not address the ill-conditioning *within* the single PDE residual term caused by the diverging ratio of material parameters $(\\lambda+\\mu)/\\mu$. The gradients of the loss function would still be pathologically dominated by the volumetric component. Furthermore, no adaptive weighting scheme can \"guarantee\" a well-conditioned problem in a singular limit without changing the underlying physical formulation. Mixed formulations represent a change in the formulation itself, which is a more fundamental and robust solution than re-weighting the loss of an ill-posed system.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{AD}$$"
        }
    ]
}