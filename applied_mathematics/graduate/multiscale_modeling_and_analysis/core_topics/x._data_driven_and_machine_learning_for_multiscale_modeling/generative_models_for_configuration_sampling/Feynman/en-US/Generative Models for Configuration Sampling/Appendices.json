{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of building effective models for physical systems is embedding known symmetries directly into the network architecture. For molecular or rigid-body systems, the underlying physics are often invariant to rotation and translation, a symmetry described by the Special Euclidean group, $\\mathrm{SE}(3)$. This practice will guide you through the implementation of a simple but principled $\\mathrm{SE}(3)$-equivariant generator, translating abstract group theory into a concrete numerical verification . Mastering this concept is key to creating models that are not only more accurate but also vastly more data-efficient, as they learn the fundamental structure of the problem space.",
            "id": "3764911",
            "problem": "Implement a program that constructs and verifies a simple equivariant generator for rigid body configurations in three dimensions, focusing on the Special Euclidean group in three dimensions, denoted by $\\mathrm{SE}(3)$. This setting is grounded in the theory of generative models such as Generative Adversarial Network (GAN) and Variational Autoencoder (VAE), where a generator $G$ maps a latent representation $z$ to a physical configuration. The requirement is to demonstrate and numerically verify $\\mathrm{SE}(3)$-equivariance of the generator for prescribed transformations and latent inputs, as would be needed for configuration sampling across scales in multiscale modeling and analysis.\n\nYou must use the following fundamental base:\n- The Special Orthogonal group in three dimensions $\\mathrm{SO}(3)$ consists of rotation matrices $R \\in \\mathbb{R}^{3 \\times 3}$ satisfying $R^\\top R = I$ and $\\det(R)=1$. The Special Euclidean group $\\mathrm{SE}(3)$ consists of pairs $(R,t)$ with $R \\in \\mathrm{SO}(3)$ and $t \\in \\mathbb{R}^3$, with composition defined by $(R_1,t_1) \\cdot (R_2,t_2) = (R_1 R_2, R_1 t_2 + t_1)$.\n- Equivariance of a generator $G$ under a group action by $h \\in \\mathrm{SE}(3)$ means $G(h \\cdot z) = h \\cdot G(z)$, where $z$ is a latent variable and $\\cdot$ denotes the appropriate group action on the latent space and on the output configuration space.\n\nDefine a latent variable $z$ as a pair of vectors $(u, w)$ with $u \\in \\mathbb{R}^3$ and $w \\in \\mathbb{R}^3$. The generator $G$ maps $z$ to a rigid body configuration $(R, t)$ by constructing a rotation $R$ via the axis-angle exponential map from $u$ and setting the translation $t$ equal to $w$. The left action of $h = (R_h,t_h) \\in \\mathrm{SE}(3)$ on latent variables is defined by $h \\cdot (u,w) = (u', w')$ where $u'$ is any axis-angle vector whose exponential map yields $R_h R$ with $R$ the exponential of $u$, and $w' = R_h w + t_h$. The corresponding left action on outputs is the usual $\\mathrm{SE}(3)$ composition.\n\nYour task is to:\n1. Implement the generator $G$ and the left action $h \\cdot z$ on the latent space using axis-angle rotations.\n2. For each test case, compute $A = h \\cdot G(z)$ and $B = G(h \\cdot z)$, and verify equivariance numerically by computing:\n   - The rotation error angle $\\Delta_{\\mathrm{rot}}$ in radians as the geodesic distance in $\\mathrm{SO}(3)$, i.e., the magnitude of the axis-angle vector of the relative rotation $R_A^{-1} R_B$.\n   - The translation error norm $\\Delta_{\\mathrm{trans}} = \\| t_A - t_B \\|_2$.\n3. Declare the test case a pass if $\\Delta_{\\mathrm{rot}} \\leq \\varepsilon_{\\mathrm{rot}}$ and $\\Delta_{\\mathrm{trans}} \\leq \\varepsilon_{\\mathrm{trans}}$, using a rotation tolerance $\\varepsilon_{\\mathrm{rot}} = 10^{-9}$ radians and a translation tolerance $\\varepsilon_{\\mathrm{trans}} = 10^{-6}$ in dimensionless units.\n\nAngle units must be in radians. Translation units are dimensionless.\n\nUse the following test suite of parameter sets, where each test case provides scale parameters $\\alpha$ and $\\beta$, a latent base $(u_{\\mathrm{base}}, w_{\\mathrm{base}})$, and a transformation $h = (u_h, w_h)$; the latent to be sampled is defined by $u = \\alpha \\, u_{\\mathrm{base}}$ and $w = \\beta \\, w_{\\mathrm{base}}$:\n\n- Test case 1 (general case):\n  - $\\alpha = 0.5$, $\\beta = 1.2$,\n  - $u_{\\mathrm{base}} = [0.3, -0.2, 0.1]$,\n  - $w_{\\mathrm{base}} = [0.5, -1.0, 0.25]$,\n  - $u_h = [0.2, 0.1, -0.15]$,\n  - $w_h = [0.1, -0.05, 0.2]$.\n\n- Test case 2 (identity boundary):\n  - $\\alpha = 1.0$, $\\beta = 1.0$,\n  - $u_{\\mathrm{base}} = [0.0, 0.0, 0.0]$,\n  - $w_{\\mathrm{base}} = [0.0, 0.0, 0.0]$,\n  - $u_h = [0.0, 0.0, 0.0]$,\n  - $w_h = [0.0, 0.0, 0.0]$.\n\n- Test case 3 (near $\\pi$ rotation edge):\n  - $\\alpha = 1.0$, $\\beta = 0.1$,\n  - $u_{\\mathrm{base}} = [\\pi - 10^{-6}, 0.0, 0.0]$,\n  - $w_{\\mathrm{base}} = [0.01, 0.02, -0.03]$,\n  - $u_h = [0.05, -0.07, 0.02]$,\n  - $w_h = [0.0, 0.0, 0.0]$.\n\n- Test case 4 (small-angle stability):\n  - $\\alpha = 1.0$, $\\beta = 1.0$,\n  - $u_{\\mathrm{base}} = [10^{-9}, -2 \\cdot 10^{-9}, 3 \\cdot 10^{-9}]$,\n  - $w_{\\mathrm{base}} = [1.0, 2.0, 3.0]$,\n  - $u_h = [0.1, 0.0, -0.1]$,\n  - $w_h = [0.0, 0.0, 0.0]$.\n\n- Test case 5 (large translation magnitude):\n  - $\\alpha = 1.0$, $\\beta = 1.0$,\n  - $u_{\\mathrm{base}} = [0.1, 0.2, -0.1]$,\n  - $w_{\\mathrm{base}} = [10^6, -10^6, 5 \\cdot 10^5]$,\n  - $u_h = [0.3, -0.1, 0.4]$,\n  - $w_h = [10^3, -2 \\cdot 10^3, 3 \\cdot 10^3]$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[r_1,r_2,r_3,r_4,r_5]$, where each $r_i$ is a boolean indicating whether the equivariance verification passed for the corresponding test case.",
            "solution": "The problem requires the implementation and numerical verification of an equivariant generator for rigid body configurations. This task is situated within the mathematical framework of geometric deep learning, specifically concerning generative models for physical systems possessing symmetries, such as those described by the Special Euclidean group $\\mathrm{SE}(3)$.\n\nThe core principle to be verified is **equivariance**. A function (or a neural network layer, or a generator) $G$ is said to be equivariant with respect to a group action if transforming the input and then applying the function yields the same result as applying the function first and then transforming the output. Formally, for a transformation $h$ from a group, this is expressed as $G(h \\cdot z) = h \\cdot G(z)$, where $z$ is the input and `路` denotes the group action on the respective spaces.\n\nFirst, we formalize the components of the problem.\nThe space of rigid body configurations is modeled by the Special Euclidean group $\\mathrm{SE}(3)$, whose elements are pairs $(R, t)$ where $R$ is a $3$D rotation matrix in $\\mathrm{SO}(3)$ and $t$ is a $3$D translation vector in $\\mathbb{R}^3$. The group composition law, which defines the left action on the configuration space, is $(R_1,t_1) \\cdot (R_2,t_2) = (R_1 R_2, R_1 t_2 + t_1)$.\n\nThe generator $G$ is a function that maps a latent variable $z = (u, w) \\in \\mathbb{R}^3 \\times \\mathbb{R}^3$ to an $\\mathrm{SE}(3)$ configuration. The definition provided is:\n$$ G(z) = G((u, w)) = (\\mathrm{exp}(u), w) $$\nHere, $\\mathrm{exp}(u)$ is the matrix exponential map from an axis-angle vector $u \\in \\mathbb{R}^3$ (representing an element of the Lie algebra $\\mathfrak{so}(3)$) to a rotation matrix $R \\in \\mathrm{SO}(3)$. The magnitude $\\|u\\|_2$ is the angle of rotation, and the direction $u/\\|u\\|_2$ is the axis of rotation. The translation part of the configuration is simply set to $w$.\n\nThe left action of a transformation $h = (R_h, t_h) \\in \\mathrm{SE}(3)$ on the latent space is defined as $h \\cdot z = (u', w')$, where:\n$$ w' = R_h w + t_h $$\n$$ \\mathrm{exp}(u') = R_h \\mathrm{exp}(u) $$\nThe definition for $u'$ implies that it is the axis-angle representation of the composed rotation $R_h R$, where $R = \\mathrm{exp}(u)$. This can be written as $u' = \\mathrm{log}(R_h \\mathrm{exp}(u))$, where $\\mathrm{log}$ is the matrix logarithm map from $\\mathrm{SO}(3)$ to $\\mathfrak{so}(3)$.\n\nThe validation task is to numerically verify the equivariance property $h \\cdot G(z) = G(h \\cdot z)$ for given test cases. We compute both sides of the equation and measure the difference.\nLet $h=(R_h, t_h)$ where $R_h = \\mathrm{exp}(u_h)$ and $t_h = w_h$.\n\nThe left-hand side, $A = (R_A, t_A) = h \\cdot G(z)$, is computed as:\n$$ (R_A, t_A) = h \\cdot (\\mathrm{exp}(u), w) = (R_h \\mathrm{exp}(u), R_h w + t_h) $$\n\nThe right-hand side, $B = (R_B, t_B) = G(h \\cdot z)$, is computed as:\n$$ (R_B, t_B) = G((u', w')) = (\\mathrm{exp}(u'), w') $$\nSubstituting the definitions for $u'$ and $w'$, we get:\n$$ R_B = \\mathrm{exp}(\\mathrm{log}(R_h \\mathrm{exp}(u))) $$\n$$ t_B = R_h w + t_h $$\n\nAnalytically, $R_B$ simplifies to $R_h \\mathrm{exp}(u)$, making $(R_B, t_B)$ identical to $(R_A, t_A)$. The numerical verification thus becomes a test of the implementation's accuracy. Any discrepancy arises from floating-point errors in the sequence of operations, particularly the round trip through the exponential and logarithm maps for the rotation part.\n\nTo quantify the numerical discrepancy, we use two metrics:\n1.  **Rotation Error ($\\Delta_{\\mathrm{rot}}$)**: The geodesic distance in $\\mathrm{SO}(3)$ between $R_A$ and $R_B$. This is calculated as the angle of the relative rotation $R_A^{-1} R_B = R_A^\\top R_B$.\n    $$ \\Delta_{\\mathrm{rot}} = \\|\\mathrm{log}(R_A^\\top R_B)\\|_2 $$\n2.  **Translation Error ($\\Delta_{\\mathrm{trans}}$)**: The standard Euclidean distance between $t_A$ and $t_B$.\n    $$ \\Delta_{\\mathrm{trans}} = \\|t_A - t_B\\|_2 $$\n\nThe implementation utilizes the `scipy.spatial.transform.Rotation` class, which provides numerically robust methods for converting between axis-angle vectors (`rotvec`) and rotation matrices. This is crucial for handling edge cases like very small rotation angles (where Taylor expansions are needed to avoid division by zero) and rotations by angles close to $\\pi$ (a singularity in some formulations).\n\nFor each test case, the algorithm performs the following steps:\n1.  Construct the latent variable $z=(u, w)$ and the transformation $h=(R_h, t_h)$ from the given parameters.\n2.  Compute the configuration $A = (R_A, t_A)$ by applying the generator and then the transformation.\n3.  Compute the configuration $B = (R_B, t_B)$ by applying the transformation to the latent space and then applying the generator.\n4.  Calculate the errors $\\Delta_{\\mathrm{rot}}$ and $\\Delta_{\\mathrm{trans}}$.\n5.  A test case passes if $\\Delta_{\\mathrm{rot}} \\le 10^{-9}$ and $\\Delta_{\\mathrm{trans}} \\le 10^{-6}$. The results are collected and formatted as a list of booleans.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.spatial.transform import Rotation\n\ndef solve():\n    \"\"\"\n    Constructs and verifies a simple SE(3)-equivariant generator.\n    \"\"\"\n    # Define tolerances for verification\n    TOL_ROT = 1e-9\n    TOL_TRANS = 1e-6\n\n    # Define the test suite of parameter sets\n    test_cases = [\n        # Test case 1 (general case)\n        {\n            \"alpha\": 0.5, \"beta\": 1.2,\n            \"u_base\": np.array([0.3, -0.2, 0.1]),\n            \"w_base\": np.array([0.5, -1.0, 0.25]),\n            \"u_h\": np.array([0.2, 0.1, -0.15]),\n            \"w_h\": np.array([0.1, -0.05, 0.2])\n        },\n        # Test case 2 (identity boundary)\n        {\n            \"alpha\": 1.0, \"beta\": 1.0,\n            \"u_base\": np.array([0.0, 0.0, 0.0]),\n            \"w_base\": np.array([0.0, 0.0, 0.0]),\n            \"u_h\": np.array([0.0, 0.0, 0.0]),\n            \"w_h\": np.array([0.0, 0.0, 0.0])\n        },\n        # Test case 3 (near pi rotation edge)\n        {\n            \"alpha\": 1.0, \"beta\": 0.1,\n            \"u_base\": np.array([np.pi - 1e-6, 0.0, 0.0]),\n            \"w_base\": np.array([0.01, 0.02, -0.03]),\n            \"u_h\": np.array([0.05, -0.07, 0.02]),\n            \"w_h\": np.array([0.0, 0.0, 0.0])\n        },\n        # Test case 4 (small-angle stability)\n        {\n            \"alpha\": 1.0, \"beta\": 1.0,\n            \"u_base\": np.array([1e-9, -2e-9, 3e-9]),\n            \"w_base\": np.array([1.0, 2.0, 3.0]),\n            \"u_h\": np.array([0.1, 0.0, -0.1]),\n            \"w_h\": np.array([0.0, 0.0, 0.0])\n        },\n        # Test case 5 (large translation magnitude)\n        {\n            \"alpha\": 1.0, \"beta\": 1.0,\n            \"u_base\": np.array([0.1, 0.2, -0.1]),\n            \"w_base\": np.array([1e6, -1e6, 5e5]),\n            \"u_h\": np.array([0.3, -0.1, 0.4]),\n            \"w_h\": np.array([1e3, -2e3, 3e3])\n        }\n    ]\n\n    results = []\n    \n    # Helper functions for SO(3) exponential and logarithm maps\n    def exp_map(u_vec):\n        \"\"\"Maps an axis-angle vector to a rotation matrix.\"\"\"\n        return Rotation.from_rotvec(u_vec).as_matrix()\n\n    def log_map(R_mat):\n        \"\"\"Maps a rotation matrix to an axis-angle vector.\"\"\"\n        return Rotation.from_matrix(R_mat).as_rotvec()\n\n    for case in test_cases:\n        # Step 1: Define inputs for the current test case\n        # Latent variable z = (u, w)\n        u = case[\"alpha\"] * case[\"u_base\"]\n        w = case[\"beta\"] * case[\"w_base\"]\n        \n        # SE(3) transformation h = (R_h, t_h)\n        u_h = case[\"u_h\"]\n        t_h = case[\"w_h\"]\n        R_h = exp_map(u_h)\n\n        # Step 2: Compute the left side of the equivariance equation: A = h 路 G(z)\n        # The generator G maps z to configuration (R, t)\n        R = exp_map(u)\n        t = w\n        \n        # The left action of h on G(z) is the SE(3) composition\n        # A = (R_A, t_A) = (R_h * R, R_h * t + t_h)\n        R_A = R_h @ R\n        t_A = R_h @ t + t_h\n\n        # Step 3: Compute the right side of the equivariance equation: B = G(h 路 z)\n        # First, compute the transformed latent variable h 路 z = (u', w')\n        # The action on w is w' = R_h * w + t_h\n        w_prime = R_h @ w + t_h\n        \n        # The action on u gives u' such that exp(u') = R_h * R\n        # This is equivalent to u' = log(R_h * R)\n        u_prime = log_map(R_h @ R)\n        \n        # Then, apply the generator G to the transformed latent z' = (u', w')\n        # B = (R_B, t_B) = G(u', w') = (exp(u'), w')\n        R_B = exp_map(u_prime)\n        t_B = w_prime\n\n        # Step 4: Compute errors and verify against tolerances\n        # Rotation error: geodesic distance in SO(3)\n        # Relative rotation is R_A^{-1} * R_B = R_A^T * R_B\n        R_relative = R_A.T @ R_B\n        # The error angle is the magnitude of the axis-angle vector of the relative rotation\n        delta_rot = np.linalg.norm(log_map(R_relative))\n        \n        # Translation error: Euclidean norm of the difference vector\n        delta_trans = np.linalg.norm(t_A - t_B)\n        \n        # Check if both errors are within the given tolerances\n        passed = (delta_rot <= TOL_ROT) and (delta_trans <= TOL_TRANS)\n        results.append(passed)\n\n    # Final print statement in the exact required format \"[True,True,True,True,True]\"\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Training generative models to capture the intricate, multi-modal energy landscapes of complex physical systems is a significant challenge, often plagued by convergence to poor local minima. This exercise introduces curriculum learning, an intuitive yet powerful strategy that guides the training process by starting with a simplified version of the target distribution and gradually increasing its complexity. You will implement a full training and analysis pipeline from first principles to quantitatively compare curriculum-based training against a direct approach, using the Hessian of the loss function to analyze the conditioning of the final solution and understand how curriculum learning can navigate difficult optimization landscapes more effectively .",
            "id": "3765027",
            "problem": "You are asked to formalize, implement, and quantitatively assess a curriculum training strategy for a simplified generative model used in configuration sampling within multiscale modeling and analysis. The model, objective, curriculum, and evaluation metrics must be defined from first principles, without shortcut formulas. Your task is to derive and implement a minimal yet rigorous experiment that compares curriculum training to direct training on a final complex target.\n\nFormulation. Let the generator family be the isotropic Gaussian model in dimension $d$, parameterized by mean $m \\in \\mathbb{R}^d$ and scale $s \\in \\mathbb{R}_{>0}$, with parameter vector $\\theta = (m, \\log s) \\in \\mathbb{R}^{d+1}$. Denote by $Q_\\theta = \\mathcal{N}(m, s^2 I_d)$ the model distribution. The target data distribution at complexity level $c$ is a uniform mixture of $k$ isotropic Gaussian components in $\\mathbb{R}^d$ placed on a circle of radius $R$, with common covariance $\\tau^2 I_d$. Specifically, $P_c = \\sum_{j=1}^k w_j \\mathcal{N}(\\mu_j, \\tau^2 I_d)$ with $w_j = 1/k$, and $\\mu_j = R [\\cos(2\\pi j/k), \\sin(2\\pi j/k)]^\\top$ for $d=2$ (extend by zeros if $d > 2$). The curriculum is a sequence of complexities $c_1 \\prec c_2 \\prec \\cdots \\prec c_T$ with nondecreasing $k$ and $R$, culminating in a specified target $c_T$.\n\nObjective. Use the squared Maximum Mean Discrepancy (MMD) with a Gaussian kernel as the training objective. For a positive kernel bandwidth parameter $\\ell \\in \\mathbb{R}_{>0}$ and the Gaussian radial basis function kernel $k(x,y) = \\exp\\left(-\\frac{\\|x-y\\|_2^2}{2 \\ell^2}\\right)$, the squared Maximum Mean Discrepancy between distributions $P$ and $Q$ is\n$$\n\\mathrm{MMD}^2(P,Q) = \\mathbb{E}_{x,x' \\sim P}[k(x,x')] + \\mathbb{E}_{y,y' \\sim Q}[k(y,y')] - 2 \\mathbb{E}_{x \\sim P, y \\sim Q}[k(x,y)].\n$$\nFor Gaussian inputs, the expectation of $k(x,y)$ can be expressed in closed form from the following well-tested fact: if $x \\sim \\mathcal{N}(m_1, S_1)$ and $y \\sim \\mathcal{N}(m_2, S_2)$ are independent, then\n$$\n\\mathbb{E}\\left[\\exp\\left(-\\frac{\\|x-y\\|_2^2}{2 \\ell^2}\\right)\\right]\n=\n\\left\\lvert I_d + \\frac{S_1 + S_2}{\\ell^2} \\right\\rvert^{-\\frac{1}{2}}\n\\exp\\left(-\\frac{1}{2} (m_1 - m_2)^\\top \\left(S_1 + S_2 + \\ell^2 I_d \\right)^{-1} (m_1 - m_2)\\right).\n$$\nUse this identity only as a foundational building block to derive, by linearity of expectation, a closed-form expression for the objective $\\mathcal{L}(\\theta; c, \\ell) = \\mathrm{MMD}^2(P_c, Q_\\theta)$, which is a smooth function of $\\theta$ for fixed $c$ and $\\ell$.\n\nLoss landscape and sampling quality metrics. To quantify the impact of curriculum training on the loss landscape and sampling quality:\n- Define the loss landscape curvature at a parameter $\\hat{\\theta}$ by the spectral condition number of the Hessian of $\\mathcal{L}(\\theta; c_T, \\ell)$ at $\\hat{\\theta}$, that is, $\\kappa(\\hat{\\theta}) = \\lambda_{\\max}(H) / \\lambda_{\\min}(H)$ where $H$ is the symmetric Hessian matrix of second derivatives with respect to the coordinates of $\\theta$ and where $\\lambda_{\\min}(H)$ denotes the smallest strictly positive eigenvalue. The Hessian must be approximated numerically using second-order central finite differences around $\\hat{\\theta}$. Ensure numerical symmetry by averaging $H$ with its transpose if needed, and regularize only as required to avoid division by zero by clipping eigenvalues below a small positive threshold.\n- Define the sampling quality at $\\hat{\\theta}$ by the achieved objective value $\\mathcal{L}(\\hat{\\theta}; c_T, \\ell)$, that is, the squared Maximum Mean Discrepancy at the final parameter estimate relative to the final target complexity.\n\nTraining protocol. Given a final target complexity $c_T$ and bandwidth $\\ell$, consider two training regimes:\n- Curriculum training: start from an initial parameter $\\theta_0$ and sequentially minimize $\\mathcal{L}(\\theta; c_t, \\ell)$ over $t = 1, \\dots, T$, using the minimizer at stage $t$ as the initialization for stage $t+1$. Use unconstrained optimization in $\\theta = (m, \\log s)$ by first principles gradient descent with Armijo backtracking line search, where gradients are approximated by central finite differences. All updates must be performed on $\\theta$ directly; the scale used in the model is $s = \\exp(\\log s)$.\n- Baseline (non-curriculum) training: start from the same initial parameter $\\theta_0$ and minimize $\\mathcal{L}(\\theta; c_T, \\ell)$ directly, using the same total number of gradient-descent iterations as the total number used across all curriculum stages.\n\nFor both regimes, use only the above-defined objective. The gradient and Hessian must be computed numerically using central differences. The Armijo rule must use a standard sufficient decrease condition with a fixed constant and geometric backtracking.\n\nTest suite. Fix $d = 2$, the initial parameter $\\theta_0$ corresponding to $m = (0,0)$ and $\\log s = 0$, and the component variance $\\tau^2 = 0.05$. For each test case, define a final target complexity $c_T = (k, R)$ and kernel bandwidth $\\ell$, and a curriculum schedule with stages $c_1 \\prec \\cdots \\prec c_T$ defined by nondecreasing $k$ and $R$ values. Use the same per-stage iteration budget in the curriculum and match the baseline total iteration budget accordingly. Use the following universal numerical settings unless otherwise stated: finite-difference step size $h = 10^{-4}$ for gradients and Hessians, Armijo constant $\\alpha = 10^{-4}$, backtracking factor $\\beta = 0.5$, initial step size $1.0$. Use a maximum of $N_s = 40$ iterations per stage. Use the following three test cases:\n- Case A (boundary): $k = 1$, $R = 1.0$, $\\ell = 1.0$, curriculum schedule $[(1, 1.0)]$.\n- Case B (happy path): $k = 4$, $R = 1.0$, $\\ell = 0.5$, curriculum schedule $[(1, 0.5), (2, 0.8), (4, 1.0)]$.\n- Case C (edge, sharper kernel and more modes): $k = 6$, $R = 1.2$, $\\ell = 0.25$, curriculum schedule $[(1, 0.4), (3, 0.8), (6, 1.2)]$.\n\nFor each case, perform both curriculum and baseline training, compute the final sampling quality $\\mathcal{L}(\\hat{\\theta}; c_T, \\ell)$ and the Hessian condition number $\\kappa(\\hat{\\theta})$ at the final parameter $\\hat{\\theta}$ for the final target $c_T$. Define the two reported metrics per case as:\n- $\\Delta \\mathrm{MMD}^2 = \\mathcal{L}(\\hat{\\theta}_{\\mathrm{baseline}}; c_T, \\ell) - \\mathcal{L}(\\hat{\\theta}_{\\mathrm{curriculum}}; c_T, \\ell)$,\n- $\\rho_\\kappa = \\kappa(\\hat{\\theta}_{\\mathrm{baseline}}) / \\kappa(\\hat{\\theta}_{\\mathrm{curriculum}})$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, aggregating the two metrics for the three cases in order A, B, C, and within each case the order $(\\Delta \\mathrm{MMD}^2, \\rho_\\kappa)$. The exact format must be:\n\"[$\\Delta \\mathrm{MMD}^2_{\\mathrm{A}},\\rho_{\\kappa,\\mathrm{A}},\\Delta \\mathrm{MMD}^2_{\\mathrm{B}},\\rho_{\\kappa,\\mathrm{B}},\\Delta \\mathrm{MMD}^2_{\\mathrm{C}},\\rho_{\\kappa,\\mathrm{C}}$]\".\nAll numerical results must be real numbers. No physical units or angles are involved in this problem.",
            "solution": "The problem asks for a rigorous quantitative comparison between a curriculum-based training strategy and a direct training baseline for a simple generative model. The comparison is to be performed on a specific task of learning a target distribution, which is a mixture of Gaussians. The evaluation is based on the final achieved objective value and the curvature of the loss landscape at the optimized parameters.\n\nFirst, we formalize the problem components as specified.\n\n**1. Model and Target Distributions**\n\nThe generative model is a $d$-dimensional isotropic Gaussian distribution $Q_\\theta = \\mathcal{N}(m, s^2 I_d)$. Its parameters are the mean $m \\in \\mathbb{R}^d$ and the scale $s \\in \\mathbb{R}_{>0}$. For unconstrained optimization, we use the parameter vector $\\theta = (m, \\log s) \\in \\mathbb{R}^{d+1}$, from which $s$ is recovered as $s = \\exp(\\log s)$. For this problem, the dimension is fixed at $d=2$.\n\nThe target distribution at complexity level $c=(k, R)$ is a uniform mixture of $k$ isotropic Gaussian components, $P_c = \\sum_{j=1}^k w_j \\mathcal{N}(\\mu_j, \\tau^2 I_d)$, where the weights are uniform, $w_j = 1/k$, and the component means $\\mu_j$ are placed on a circle of radius $R$:\n$$\n\\mu_j = R \\begin{pmatrix} \\cos(2\\pi j/k) \\\\ \\sin(2\\pi j/k) \\end{pmatrix} \\in \\mathbb{R}^2, \\quad \\text{for } j=1, \\dots, k.\n$$\nThe component covariance is $\\tau^2 I_d$, with $\\tau^2$ being a fixed constant.\n\n**2. Objective Function: Squared Maximum Mean Discrepancy (MMD)**\n\nThe training objective is the squared MMD with a Gaussian kernel, $k(x,y) = \\exp\\left(-\\frac{\\|x-y\\|_2^2}{2 \\ell^2}\\right)$, where $\\ell$ is the kernel bandwidth. The squared MMD between two distributions $P$ and $Q$ is given by:\n$$\n\\mathcal{L}(\\theta; c, \\ell) = \\mathrm{MMD}^2(P_c, Q_\\theta) = \\mathbb{E}_{x,x' \\sim P_c}[k(x,x')] + \\mathbb{E}_{y,y' \\sim Q_\\theta}[k(y,y')] - 2 \\mathbb{E}_{x \\sim P_c, y \\sim Q_\\theta}[k(x,y)].\n$$\nTo derive a closed-form expression for this objective, we use the provided identity for the expected kernel value between two independent Gaussian random variables $x \\sim \\mathcal{N}(m_1, S_1)$ and $y \\sim \\mathcal{N}(m_2, S_2)$:\n$$\nE(m_1, S_1, m_2, S_2, \\ell) = \\mathbb{E}\\left[k(x,y)\\right] = \\left\\lvert I_d + \\frac{S_1 + S_2}{\\ell^2} \\right\\rvert^{-\\frac{1}{2}} \\exp\\left(-\\frac{1}{2} (m_1 - m_2)^\\top \\left(S_1 + S_2 + \\ell^2 I_d \\right)^{-1} (m_1 - m_2)\\right).\n$$\nSince all covariances in this problem are isotropic (i.e., of the form $\\sigma^2 I_d$), we can simplify this expression. For $S_1 = \\sigma_1^2 I_d$ and $S_2 = \\sigma_2^2 I_d$, the identity simplifies to:\n$$\nH(m_1, \\sigma_1^2, m_2, \\sigma_2^2, \\ell) = \\left(1 + \\frac{\\sigma_1^2 + \\sigma_2^2}{\\ell^2}\\right)^{-d/2} \\exp\\left(-\\frac{\\|m_1 - m_2\\|^2}{2(\\sigma_1^2 + \\sigma_2^2 + \\ell^2)}\\right).\n$$\nUsing this simplified helper function $H$ and the linearity of expectation, we can express the three terms of the $\\mathrm{MMD}^2$ loss:\n\nTerm 1: $\\mathbb{E}_{x,x' \\sim P_c}[k(x,x')]$\nSince $x$ and $x'$ are drawn independently from the mixture $P_c = \\frac{1}{k}\\sum_{i=1}^k \\mathcal{N}(\\mu_i, \\tau^2 I_d)$, we have:\n$$\n\\mathbb{E}_{x,x' \\sim P_c}[k(x,x')] = \\sum_{i=1}^k \\sum_{j=1}^k \\frac{1}{k^2} \\mathbb{E}_{x_i \\sim \\mathcal{N}(\\mu_i, \\tau^2 I_d), x'_j \\sim \\mathcal{N}(\\mu_j, \\tau^2 I_d)}[k(x_i, x'_j)] = \\frac{1}{k^2} \\sum_{i=1}^k \\sum_{j=1}^k H(\\mu_i, \\tau^2, \\mu_j, \\tau^2, \\ell).\n$$\n\nTerm 2: $\\mathbb{E}_{y,y' \\sim Q_\\theta}[k(y,y')]$\nHere, $y, y' \\sim \\mathcal{N}(m, s^2 I_d)$ are drawn independently from the model distribution:\n$$\n\\mathbb{E}_{y,y' \\sim Q_\\theta}[k(y,y')] = H(m, s^2, m, s^2, \\ell).\n$$\n\nTerm 3: $-2 \\mathbb{E}_{x \\sim P_c, y \\sim Q_\\theta}[k(x,y)]$\nA sample $x$ is drawn from the mixture $P_c$ and a sample $y$ from the model $Q_\\theta$:\n$$\n-2 \\mathbb{E}_{x \\sim P_c, y \\sim Q_\\theta}[k(x,y)] = -2 \\sum_{j=1}^k \\frac{1}{k} \\mathbb{E}_{x_j \\sim \\mathcal{N}(\\mu_j, \\tau^2 I_d), y \\sim \\mathcal{N}(m, s^2 I_d)}[k(x_j, y)] = -\\frac{2}{k} \\sum_{j=1}^k H(\\mu_j, \\tau^2, m, s^2, \\ell).\n$$\nCombining these terms yields the full, differentiable objective function $\\mathcal{L}(\\theta; c, \\ell)$.\n\n**3. Training and Evaluation**\n\nThe training is performed via gradient descent on the parameter vector $\\theta = (m_x, m_y, \\log s)$. The gradient $\\nabla_\\theta \\mathcal{L}$ is computed numerically using central finite differences with a step size $h$:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\theta_i} \\approx \\frac{\\mathcal{L}(\\theta + h e_i) - \\mathcal{L}(\\theta - h e_i)}{2h}.\n$$\nThe step size for each gradient descent update is determined by an Armijo backtracking line search, which ensures sufficient decrease in the objective function at each iteration.\n\nThe evaluation metrics are computed at the end of training for the final target complexity $c_T$ and bandwidth $\\ell$.\n- **Sampling Quality**: The final loss value $\\mathcal{L}(\\hat{\\theta}; c_T, \\ell)$.\n- **Loss Landscape Curvature**: The spectral condition number $\\kappa(\\hat{\\theta})$ of the Hessian matrix $H$ of the final objective $\\mathcal{L}(\\theta; c_T, \\ell)$ evaluated at the final parameters $\\hat{\\theta}$. The Hessian is also computed numerically using second-order central finite differences:\n  $$\n  H_{ii} \\approx \\frac{\\mathcal{L}(\\hat{\\theta}+he_i) - 2\\mathcal{L}(\\hat{\\theta}) + \\mathcal{L}(\\hat{\\theta}-he_i)}{h^2}\n  $$\n  $$\n  H_{ij} \\approx \\frac{\\mathcal{L}(\\hat{\\theta}+he_i+he_j) - \\mathcal{L}(\\hat{\\theta}+he_i-he_j) - \\mathcal{L}(\\hat{\\theta}-he_i+he_j) + \\mathcal{L}(\\hat{\\theta}-he_i-he_j)}{4h^2}\n  $$\n  The condition number is $\\kappa(\\hat{\\theta}) = \\lambda_{\\max}(H) / \\lambda_{\\min\\_pos}(H)$, where $\\lambda_{\\min\\_pos}$ is the smallest strictly positive eigenvalue, enforced by clipping any eigenvalues below a small positive threshold $\\epsilon > 0$ to $\\epsilon$.\n\nThe experiment compares two training regimes:\n- **Curriculum Training**: Sequentially optimizes the objective for a schedule of increasing complexities $c_1, \\dots, c_T$, using a fixed number of iterations $N_s$ per stage. The result of one stage initializes the next.\n- **Baseline Training**: Directly optimizes the objective for the final complexity $c_T$, using a total number of iterations equal to the total used in the curriculum training ($N_s \\times T$).\n\nBoth regimes start from the same initial parameter $\\theta_0$ corresponding to a standard normal distribution, i.e., $m=(0,0)$ and $\\log s = 0$. The comparison is based on two derived metrics: $\\Delta \\mathrm{MMD}^2$, the difference in final loss between baseline and curriculum, and $\\rho_\\kappa$, the ratio of their final Hessian condition numbers. A positive $\\Delta \\mathrm{MMD}^2$ indicates the curriculum achieved a better (lower) final loss. A value of $\\rho_\\kappa > 1$ indicates the curriculum found a solution in a better-conditioned (flatter) region of the loss landscape.\n\nThe implementation follows these principles to execute the specified test cases and compute the final metrics.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the curriculum training experiment as specified.\n    \"\"\"\n    # --- Universal Numerical Settings ---\n    D = 2\n    TAU_SQ = 0.05\n    H_FD = 1e-4\n    ARMIJO_ALPHA = 1e-4\n    ARMIJO_BETA = 0.5\n    INIT_STEP_SIZE = 1.0\n    N_S = 40\n    EIG_CLIP_THRESHOLD = 1e-10\n    THETA_0 = np.array([0.0, 0.0, 0.0]) # m=(0,0), log(s)=0\n\n    # --- Test Cases Definition ---\n    test_cases = [\n        {\n            \"name\": \"A\",\n            \"k\": 1, \"R\": 1.0, \"l\": 1.0,\n            \"schedule\": [(1, 1.0)]\n        },\n        {\n            \"name\": \"B\",\n            \"k\": 4, \"R\": 1.0, \"l\": 0.5,\n            \"schedule\": [(1, 0.5), (2, 0.8), (4, 1.0)]\n        },\n        {\n            \"name\": \"C\",\n            \"k\": 6, \"R\": 1.2, \"l\": 0.25,\n            \"schedule\": [(1, 0.4), (3, 0.8), (6, 1.2)]\n        }\n    ]\n\n    # --- Core Algorithmic Components ---\n\n    def exp_k_xy_gauss(m1, s1_sq, m2, s2_sq, l_sq):\n        \"\"\"Computes E[k(x,y)] for isotropic Gaussians x and y.\"\"\"\n        term1_base = 1.0 + (s1_sq + s2_sq) / l_sq\n        term1 = term1_base ** (-D / 2.0)\n        m_diff_sq = np.sum((m1 - m2)**2)\n        term2_denom = 2.0 * (s1_sq + s2_sq + l_sq)\n        term2 = np.exp(-m_diff_sq / term2_denom)\n        return term1 * term2\n\n    _mu_cache = {}\n    def compute_mu(k, R):\n        \"\"\"Computes means of the target Gaussian mixture components.\"\"\"\n        if (k, R) in _mu_cache:\n            return _mu_cache[(k, R)]\n        if k == 0:\n            return np.array([])\n        angles = 2 * np.pi * np.arange(1, k + 1) / k\n        mus = np.zeros((k, D))\n        mus[:, 0] = R * np.cos(angles)\n        mus[:, 1] = R * np.sin(angles)\n        _mu_cache[(k, R)] = mus\n        return mus\n\n    _loss_cache = {}\n    def mmd2_loss(theta_tuple, k, R, l, tau_sq):\n        \"\"\"Computes the squared MMD loss L(theta; c, l).\"\"\"\n        theta_key = (theta_tuple, k, R, l, tau_sq)\n        if theta_key in _loss_cache:\n            return _loss_cache[theta_key]\n\n        theta = np.array(theta_tuple)\n        m = theta[:D]\n        s_sq = np.exp(2 * theta[D])\n        l_sq = l**2\n        mus = compute_mu(k, R)\n\n        # Term 1: E_{x,x' ~ P}[k(x,x')]\n        term1 = 0.0\n        if k > 0:\n            for i in range(k):\n                for j in range(k):\n                    term1 += exp_k_xy_gauss(mus[i], tau_sq, mus[j], tau_sq, l_sq)\n            term1 /= (k**2)\n\n        # Term 2: E_{y,y' ~ Q}[k(y,y')]\n        term2 = exp_k_xy_gauss(m, s_sq, m, s_sq, l_sq)\n\n        # Term 3: -2 * E_{x ~ P, y ~ Q}[k(x,y)]\n        term3 = 0.0\n        if k > 0:\n            for j in range(k):\n                term3 += exp_k_xy_gauss(mus[j], tau_sq, m, s_sq, l_sq)\n            term3 *= (2.0 / k)\n            \n        result = term1 + term2 - term3\n        _loss_cache[theta_key] = result\n        return result\n\n    def gradient(func, theta, h):\n        \"\"\"Computes gradient using central finite differences.\"\"\"\n        grad = np.zeros_like(theta)\n        for i in range(len(theta)):\n            theta_plus_h, theta_minus_h = theta.copy(), theta.copy()\n            theta_plus_h[i] += h\n            theta_minus_h[i] -= h\n            grad[i] = (func(tuple(theta_plus_h)) - func(tuple(theta_minus_h))) / (2 * h)\n        return grad\n\n    def gradient_descent_armijo(func, theta_0, max_iter):\n        \"\"\"Performs gradient descent with Armijo backtracking line search.\"\"\"\n        theta = theta_0.copy()\n        for _ in range(max_iter):\n            loss_val = func(tuple(theta))\n            grad_val = gradient(func, theta, H_FD)\n            eta = INIT_STEP_SIZE\n            while True:\n                theta_new = theta - eta * grad_val\n                sufficient_decrease = loss_val - ARMIJO_ALPHA * eta * np.dot(grad_val, grad_val)\n                if func(tuple(theta_new)) <= sufficient_decrease:\n                    break\n                eta *= ARMIJO_BETA\n                if eta < 1e-12: break \n            theta = theta_new\n        return theta\n\n    def hessian(func, theta, h):\n        \"\"\"Computes Hessian using second-order central finite differences.\"\"\"\n        n = len(theta)\n        H = np.zeros((n, n))\n        f_theta = func(tuple(theta))\n        for i in range(n):\n            for j in range(i, n):\n                if i == j:\n                    tp, tm = theta.copy(), theta.copy()\n                    tp[i] += h; tm[i] -= h\n                    H[i, i] = (func(tuple(tp)) - 2 * f_theta + func(tuple(tm))) / (h**2)\n                else:\n                    tpp, tpm, tmp, tmm = theta.copy(), theta.copy(), theta.copy(), theta.copy()\n                    tpp[i]+=h; tpp[j]+=h\n                    tpm[i]+=h; tpm[j]-=h\n                    tmp[i]-=h; tmp[j]+=h\n                    tmm[i]-=h; tmm[j]-=h\n                    val = (func(tuple(tpp)) - func(tuple(tpm)) - func(tuple(tmp)) + func(tuple(tmm))) / (4 * h**2)\n                    H[i, j] = H[j, i] = val\n        H = (H + H.T) / 2.0\n        return H\n\n    def condition_number(H):\n        \"\"\"Computes the condition number from the Hessian.\"\"\"\n        eigenvalues = np.linalg.eigvalsh(H)\n        eigenvalues = np.maximum(eigenvalues, EIG_CLIP_THRESHOLD)\n        return np.max(eigenvalues) / np.min(eigenvalues)\n\n    results = []\n    for case in test_cases:\n        _loss_cache.clear()\n        k_final, R_final, l_final = case[\"k\"], case[\"R\"], case[\"l\"]\n        schedule = case[\"schedule\"]\n        \n        loss_final = lambda theta_tuple: mmd2_loss(theta_tuple, k_final, R_final, l_final, TAU_SQ)\n        \n        # Curriculum Training\n        theta_curr = THETA_0.copy()\n        for k_stage, R_stage in schedule:\n            loss_stage = lambda t_tuple: mmd2_loss(t_tuple, k_stage, R_stage, l_final, TAU_SQ)\n            theta_curr = gradient_descent_armijo(loss_stage, theta_curr, N_S)\n        \n        theta_hat_curr = theta_curr\n        mmd2_curr = loss_final(tuple(theta_hat_curr))\n        H_curr = hessian(loss_final, theta_hat_curr, H_FD)\n        kappa_curr = condition_number(H_curr)\n\n        # Baseline Training\n        total_iter = N_S * len(schedule)\n        theta_base = gradient_descent_armijo(loss_final, THETA_0.copy(), total_iter)\n\n        theta_hat_base = theta_base\n        mmd2_base = loss_final(tuple(theta_hat_base))\n        H_base = hessian(loss_final, theta_hat_base, H_FD)\n        kappa_base = condition_number(H_base)\n\n        # Calculate and store metrics\n        delta_mmd2 = mmd2_base - mmd2_curr\n        rho_kappa = kappa_base / kappa_curr if kappa_curr != 0 else np.inf\n        \n        results.extend([delta_mmd2, rho_kappa])\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Once a generative model is trained, its primary purpose is often to calculate equilibrium observables. However, the model's distribution $p_{\\theta}(x)$ is almost always an approximation of the true target (e.g., Boltzmann) distribution $p^{\\star}(x)$. This exercise tackles this crucial issue by having you derive the theory of importance sampling, a technique to correct for this mismatch and obtain unbiased estimates . You will develop the self-normalized importance sampling estimator from first principles and derive its asymptotic variance, providing a rigorous foundation for using generative models in quantitative scientific computation and for understanding the uncertainty associated with their predictions.",
            "id": "3765001",
            "problem": "Consider a multiscale molecular system with a one-dimensional coarse variable $x$ that captures a slow collective mode. The equilibrium target distribution for $x$ is the Boltzmann distribution $p^{\\star}(x) \\propto \\exp(-U(x))$ at temperature chosen so that the normalized target density is the standard normal law $p^{\\star}(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\!\\left(-\\frac{x^{2}}{2}\\right)$. A generative model with an explicit density, as in a Variational Autoencoder (VAE), proposes samples from the parametric family $p_{\\theta}(x) = \\frac{1}{\\sqrt{2\\pi}\\,\\sigma} \\exp\\!\\left(-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}\\right)$, where $\\theta = (\\mu,\\sigma)$ are model parameters. You draw $n$ independent samples $\\{x_{i}\\}_{i=1}^{n}$ from $p_{\\theta}$ and seek to compute equilibrium expectations under $p^{\\star}$ for observables using importance sampling.\n\nStarting from the fundamental definition of importance sampling and the law of large numbers, derive the reweighting scheme $w(x) = \\frac{p^{\\star}(x)}{p_{\\theta}(x)}$ and the self-normalized importance sampling estimator for the observable $f(x) = x^{2}$. Then, using a first-principles derivation based on appropriate limit theorems for sample averages, obtain the asymptotic variance of the self-normalized estimator in terms of expectations under $p_{\\theta}$.\n\nFinally, specialize to the scientifically realistic case $\\mu = 0$ and $\\sigma = 1$ (i.e., the generative model matches the target), and evaluate the asymptotic variance of the self-normalized importance sampling estimator for $f(x) = x^{2}$ as a single real-valued number. Express the final answer without units. No rounding is required.",
            "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and all necessary information is provided.\n\n**Step 1: Extract Givens**\n- One-dimensional coarse variable: $x$.\n- Equilibrium target density: $p^{\\star}(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^{2}}{2}\\right)$, which is the standard normal distribution $\\mathcal{N}(0,1)$.\n- Generative model parametric density: $p_{\\theta}(x) = \\frac{1}{\\sqrt{2\\pi}\\,\\sigma} \\exp\\left(-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}\\right)$, which is the normal distribution $\\mathcal{N}(\\mu, \\sigma^2)$ with parameters $\\theta = (\\mu, \\sigma)$.\n- Samples: A set of $n$ independent and identically distributed samples $\\{x_{i}\\}_{i=1}^{n}$ drawn from $p_{\\theta}(x)$.\n- Observable: $f(x) = x^{2}$.\n- Task 1: Derive the reweighting scheme $w(x) = \\frac{p^{\\star}(x)}{p_{\\theta}(x)}$ and the self-normalized importance sampling (SNIS) estimator for $\\langle f(x) \\rangle_{p^{\\star}}$.\n- Task 2: Derive the asymptotic variance of the SNIS estimator in terms of expectations under $p_{\\theta}$.\n- Task 3: Evaluate this asymptotic variance for the specific case where the generative model perfectly matches the target, i.e., $\\mu = 0$ and $\\sigma = 1$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is valid. It is a well-posed and standard problem in statistical mechanics and machine learning. The concepts of Boltzmann distributions, generative models, normal distributions, and importance sampling are all scientifically established. The problem statement is self-contained, objective, and provides all necessary information for a unique solution. The final calculation is for a scientifically realistic benchmark case (a perfect model).\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete, reasoned solution will be provided.\n\n**Derivation of the Self-Normalized Importance Sampling (SNIS) Estimator**\n\nThe goal is to compute the expectation of an observable $f(x)$ with respect to the target distribution $p^{\\star}(x)$, denoted as $\\langle f \\rangle_{p^{\\star}}$:\n$$\n\\langle f \\rangle_{p^{\\star}} = \\int f(x) p^{\\star}(x) \\,dx\n$$\nWe have samples from a different distribution, the proposal distribution $p_{\\theta}(x)$. To express this expectation in terms of $p_{\\theta}(x)$, we multiply and divide the integrand by $p_{\\theta}(x)$, assuming $p_{\\theta}(x) > 0$ whenever $f(x)p^{\\star}(x) \\neq 0$:\n$$\n\\langle f \\rangle_{p^{\\star}} = \\int f(x) \\frac{p^{\\star}(x)}{p_{\\theta}(x)} p_{\\theta}(x) \\,dx\n$$\nWe define the importance weight (or reweighting factor) as the ratio of the target density to the proposal density:\n$$\nw(x) = \\frac{p^{\\star}(x)}{p_{\\theta}(x)}\n$$\nThe expectation can now be written as an expectation with respect to the proposal distribution $p_{\\theta}(x)$:\n$$\n\\langle f \\rangle_{p^{\\star}} = \\int (f(x) w(x)) p_{\\theta}(x) \\,dx = \\mathbb{E}_{p_{\\theta}}[f(x)w(x)]\n$$\nIn many practical scenarios, the distributions are only known up to a normalization constant. Let's write $\\langle f \\rangle_{p^{\\star}}$ as a ratio of integrals:\n$$\n\\langle f \\rangle_{p^{\\star}} = \\frac{\\int f(x) p^{\\star}(x) \\,dx}{\\int p^{\\star}(x) \\,dx}\n$$\nBy introducing $p_{\\theta}(x)/p_{\\theta}(x)$ in both the numerator and the denominator, we get:\n$$\n\\langle f \\rangle_{p^{\\star}} = \\frac{\\int f(x) w(x) p_{\\theta}(x) \\,dx}{\\int w(x) p_{\\theta}(x) \\,dx} = \\frac{\\mathbb{E}_{p_{\\theta}}[f(x)w(x)]}{\\mathbb{E}_{p_{\\theta}}[w(x)]}\n$$\nGiven $n$ i.i.d. samples $\\{x_i\\}_{i=1}^n$ from $p_{\\theta}(x)$, the law of large numbers suggests we can approximate the expectations in the numerator and denominator using sample means:\n$$\n\\mathbb{E}_{p_{\\theta}}[f(x)w(x)] \\approx \\frac{1}{n} \\sum_{i=1}^{n} f(x_i)w(x_i)\n$$\n$$\n\\mathbb{E}_{p_{\\theta}}[w(x)] \\approx \\frac{1}{n} \\sum_{i=1}^{n} w(x_i)\n$$\nSubstituting these approximations into the ratio gives the self-normalized importance sampling (SNIS) estimator, $\\hat{f}_{SNIS}$:\n$$\n\\hat{f}_{SNIS} = \\frac{\\frac{1}{n} \\sum_{i=1}^{n} f(x_i)w(x_i)}{\\frac{1}{n} \\sum_{i=1}^{n} w(x_i)} = \\frac{\\sum_{i=1}^{n} f(x_i)w(x_i)}{\\sum_{i=1}^{n} w(x_i)}\n$$\nwhere $w_i = w(x_i)$. This completes the first part of the derivation.\n\n**Derivation of the Asymptotic Variance**\n\nThe SNIS estimator $\\hat{f}_{SNIS}$ is a ratio of two sample means. To find its asymptotic variance, we employ the multivariate Central Limit Theorem and the Delta Method. Let us define two random variables derived from each sample $x_i$: $A_i = f(x_i)w(x_i)$ and $B_i = w(x_i)$. Their sample means are $\\bar{A}_n = \\frac{1}{n}\\sum A_i$ and $\\bar{B}_n = \\frac{1}{n}\\sum B_i$, so that $\\hat{f}_{SNIS} = \\bar{A}_n / \\bar{B}_n$.\n\nBy the law of large numbers, as $n \\to \\infty$:\n$\\bar{A}_n \\to \\mu_A = \\mathbb{E}_{p_{\\theta}}[f(x)w(x)] = \\langle f \\rangle_{p^{\\star}}$\n$\\bar{B}_n \\to \\mu_B = \\mathbb{E}_{p_{\\theta}}[w(x)] = \\int \\frac{p^{\\star}(x)}{p_{\\theta}(x)} p_{\\theta}(x) \\,dx = \\int p^{\\star}(x) \\,dx = 1$ (since $p^{\\star}$ is a normalized density).\n\nThe multivariate Central Limit Theorem states that the vector of sample means converges in distribution to a multivariate normal distribution:\n$$\n\\sqrt{n} \\left( \\begin{pmatrix} \\bar{A}_n \\\\ \\bar{B}_n \\end{pmatrix} - \\begin{pmatrix} \\mu_A \\\\ \\mu_B \\end{pmatrix} \\right) \\xrightarrow{d} \\mathcal{N}\\left( \\mathbf{0}, \\Sigma \\right)\n$$\nwhere $\\Sigma$ is the covariance matrix of the vector $(A_i, B_i)^T$:\n$$\n\\Sigma = \\begin{pmatrix} \\text{Var}_{p_{\\theta}}(A) & \\text{Cov}_{p_{\\theta}}(A, B) \\\\ \\text{Cov}_{p_{\\theta}}(A, B) & \\text{Var}_{p_{\\theta}}(B) \\end{pmatrix} = \\begin{pmatrix} \\text{Var}_{p_{\\theta}}(fw) & \\text{Cov}_{p_{\\theta}}(fw, w) \\\\ \\text{Cov}_{p_{\\theta}}(fw, w) & \\text{Var}_{p_{\\theta}}(w) \\end{pmatrix}\n$$\nWe are interested in the function $g(a,b) = a/b$. The estimator is $\\hat{f}_{SNIS} = g(\\bar{A}_n, \\bar{B}_n)$. The Delta Method provides the asymptotic distribution of this estimator. The variance of the asymptotic distribution of $\\sqrt{n}(\\hat{f}_{SNIS} - g(\\mu_A, \\mu_B))$ is given by $V_{asym} = \\nabla g^T \\Sigma \\nabla g$, where the gradient $\\nabla g$ is evaluated at $(\\mu_A, \\mu_B)$.\n\nThe gradient of $g(a,b)$ is $\\nabla g = (\\frac{\\partial g}{\\partial a}, \\frac{\\partial g}{\\partial b})^T = (1/b, -a/b^2)^T$.\nEvaluating at $(\\mu_A, \\mu_B) = (\\langle f \\rangle_{p^{\\star}}, 1)$:\n$$\n\\nabla g(\\mu_A, \\mu_B) = \\left( 1, -\\langle f \\rangle_{p^{\\star}} \\right)^T\n$$\nThe asymptotic variance is then:\n$$\nV_{asym} = \\begin{pmatrix} 1 & -\\langle f \\rangle_{p^{\\star}} \\end{pmatrix} \\begin{pmatrix} \\text{Var}_{p_{\\theta}}(fw) & \\text{Cov}_{p_{\\theta}}(fw, w) \\\\ \\text{Cov}_{p_{\\theta}}(fw, w) & \\text{Var}_{p_{\\theta}}(w) \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -\\langle f \\rangle_{p^{\\star}} \\end{pmatrix}\n$$\n$V_{asym} = \\text{Var}_{p_{\\theta}}(fw) - 2\\langle f \\rangle_{p^{\\star}}\\text{Cov}_{p_{\\theta}}(fw, w) + \\langle f \\rangle_{p^{\\star}}^2\\text{Var}_{p_{\\theta}}(w)$\nThis expression can be simplified by recognizing it as the variance of a linear combination of random variables:\n$V_{asym} = \\text{Var}_{p_{\\theta}}(1 \\cdot (fw) - \\langle f \\rangle_{p^{\\star}} \\cdot w) = \\text{Var}_{p_{\\theta}}((f - \\langle f \\rangle_{p^{\\star}})w)$.\n\nWe can expand this variance: $\\text{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$.\nLet $X = (f - \\langle f \\rangle_{p^{\\star}})w$. The expectation of $X$ is:\n$\\mathbb{E}_{p_{\\theta}}[X] = \\mathbb{E}_{p_{\\theta}}[fw - \\langle f \\rangle_{p^{\\star}}w] = \\mathbb{E}_{p_{\\theta}}[fw] - \\langle f \\rangle_{p^{\\star}}\\mathbb{E}_{p_{\\theta}}[w]$.\nWe know $\\mathbb{E}_{p_{\\theta}}[fw] = \\langle f \\rangle_{p^{\\star}}$ and $\\mathbb{E}_{p_{\\theta}}[w] = 1$.\nSo, $\\mathbb{E}_{p_{\\theta}}[X] = \\langle f \\rangle_{p^{\\star}} - \\langle f \\rangle_{p^{\\star}}(1) = 0$.\nTherefore, the variance is simply the expectation of $X^2$:\n$$\nV_{asym} = \\mathbb{E}_{p_{\\theta}}[((f(x) - \\langle f \\rangle_{p^{\\star}})w(x))^2] = \\mathbb{E}_{p_{\\theta}}[(f(x) - \\langle f \\rangle_{p^{\\star}})^2 w(x)^2]\n$$\nThis is the desired expression for the asymptotic variance of the SNIS estimator in terms of expectations under the proposal distribution $p_{\\theta}$. The variance of the estimator $\\hat{f}_{SNIS}$ for large $n$ is approximately $\\frac{1}{n} V_{asym}$.\n\n**Evaluation for the Specific Case**\n\nWe now specialize to the case where the generative model is perfect: $\\mu = 0$ and $\\sigma = 1$.\n- Target distribution: $p^{\\star}(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^{2}}{2}\\right)$.\n- Proposal distribution: $p_{\\theta}(x) = \\frac{1}{\\sqrt{2\\pi}(1)} \\exp\\left(-\\frac{(x-0)^{2}}{2(1)^{2}}\\right) = p^{\\star}(x)$.\n- Observable: $f(x) = x^2$.\n\nFirst, we determine the weight function $w(x)$:\n$$\nw(x) = \\frac{p^{\\star}(x)}{p_{\\theta}(x)} = \\frac{\\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^{2}}{2}\\right)}{\\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^{2}}{2}\\right)} = 1\n$$\nThe weight is exactly $1$ for all $x$.\n\nNext, we compute the true expectation of the observable, $\\langle f \\rangle_{p^{\\star}}$:\n$$\n\\langle f \\rangle_{p^{\\star}} = \\langle x^2 \\rangle_{p^{\\star}} = \\int_{-\\infty}^{\\infty} x^2 p^{\\star}(x) \\,dx\n$$\nThis is the second moment of the standard normal distribution $\\mathcal{N}(0,1)$. Since the mean is $0$, the second moment is equal to the variance, which is $1$.\n$$\n\\langle f \\rangle_{p^{\\star}} = 1\n$$\n\nFinally, we substitute these quantities into the formula for the asymptotic variance:\n$$\nV_{asym} = \\mathbb{E}_{p_{\\theta}}[(f(x) - \\langle f \\rangle_{p^{\\star}})^2 w(x)^2]\n$$\n$$\nV_{asym} = \\mathbb{E}_{p_{\\theta}}[(x^2 - 1)^2 (1)^2] = \\mathbb{E}_{p_{\\theta}}[(x^2 - 1)^2]\n$$\nThe expectation is with respect to $p_{\\theta}(x)$, which is the standard normal distribution.\nWe expand the polynomial: $(x^2 - 1)^2 = x^4 - 2x^2 + 1$.\nUsing the linearity of expectation:\n$$\nV_{asym} = \\mathbb{E}_{p_{\\theta}}[x^4 - 2x^2 + 1] = \\mathbb{E}_{p_{\\theta}}[x^4] - 2\\mathbb{E}_{p_{\\theta}}[x^2] + \\mathbb{E}_{p_{\\theta}}[1]\n$$\nWe need the moments of the standard normal distribution ($Z \\sim \\mathcal{N}(0,1)$):\n- $\\mathbb{E}_{p_{\\theta}}[1] = 1$.\n- $\\mathbb{E}_{p_{\\theta}}[x^2]$ is the variance, which is $1$.\n- $\\mathbb{E}_{p_{\\theta}}[x^4]$ is the fourth moment. For a general normal distribution $\\mathcal{N}(\\mu, \\sigma^2)$, the fourth central moment is $3\\sigma^4$. Since our mean $\\mu=0$ and standard deviation $\\sigma=1$, the fourth moment is $3(1)^4 = 3$.\n\nSubstituting these values:\n$$\nV_{asym} = 3 - 2(1) + 1 = 3 - 2 + 1 = 2\n$$\nThus, the asymptotic variance of the self-normalized importance sampling estimator for $f(x)=x^2$ in the case of a perfect generative model is $2$.",
            "answer": "$$\n\\boxed{2}\n$$"
        }
    ]
}