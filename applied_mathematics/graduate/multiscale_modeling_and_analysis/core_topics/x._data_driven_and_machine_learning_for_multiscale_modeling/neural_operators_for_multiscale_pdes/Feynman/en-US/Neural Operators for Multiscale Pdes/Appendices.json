{
    "hands_on_practices": [
        {
            "introduction": "The heart of the Fourier Neural Operator (FNO) is its spectral convolution layer, which operates in the frequency domain. This first practice delves into the essential structure of this layer, exploring how to correctly parameterize the learnable kernel for physical problems that involve real-valued fields. By deriving and enforcing the conjugate symmetry constraint, you will understand a key mechanism that guarantees real-valued outputs and ensures an efficient, non-redundant set of learnable parameters .",
            "id": "3787660",
            "problem": "Consider a Fourier Neural Operator (FNO) applied to a two-dimensional periodic domain for learning solution operators of multiscale partial differential equations. Let the input and output fields be real-valued, with $C_{\\text{in}}$ input channels and $C_{\\text{out}}$ output channels, respectively. In the spectral convolution layer, the learned complex-valued kernel is a tensor $R(\\mathbf{k}) \\in \\mathbb{C}^{C_{\\text{out}} \\times C_{\\text{in}}}$, indexed by discrete two-dimensional wavenumbers $\\mathbf{k} = (k_{x}, k_{y})$ drawn from the truncated rectangular set\n$$\n\\mathcal{I} = \\{ (k_{x}, k_{y}) \\,:\\, k_{x} \\in \\{-K_{x}, -K_{x}+1, \\dots, K_{x}\\}, \\; k_{y} \\in \\{-K_{y}, -K_{y}+1, \\dots, K_{y}\\} \\},\n$$\nwhere $K_{x}$ and $K_{y}$ are nonnegative integers specifying the spectral truncation in each coordinate.\n\nThe spectral convolution acts as $\\widehat{y}(\\mathbf{k}) = R(\\mathbf{k}) \\,\\widehat{x}(\\mathbf{k})$, where $\\widehat{x}(\\mathbf{k})$ and $\\widehat{y}(\\mathbf{k})$ are the Discrete Fourier Transform (DFT) coefficients of the input and output, respectively. Use the fundamental property of the DFT of real-valued fields that $\\widehat{f}(-\\mathbf{k}) = \\overline{\\widehat{f}(\\mathbf{k})}$ to derive the conjugate symmetry constraint that $R(\\mathbf{k})$ must satisfy so that $\\widehat{y}(\\mathbf{k})$ corresponds to a real-valued output field for all real-valued inputs. Then, for spatial dimension $d=2$, construct an explicit parameterization of $R(\\mathbf{k})$ by defining a representative half-set of modes $\\mathcal{H} \\subset \\mathcal{I}$ and real-valued parameter tensors that ensure the required conjugate symmetry $R(-\\mathbf{k}) = \\overline{R(\\mathbf{k})}$ across $\\mathcal{I}$, including the special treatment of the zero mode. Finally, compute a closed-form expression for the total number of independent real scalar parameters needed to specify $R(\\mathbf{k})$ under this parameterization, expressed in terms of $K_{x}$, $K_{y}$, $C_{\\text{in}}$, and $C_{\\text{out}}$.\n\nYour final answer must be a single analytic expression. No rounding is required, and no units are involved. Express the final expression in its simplest exact form.",
            "solution": "The user wants to analyze the parameterization of a spectral convolution layer in a Fourier Neural Operator (FNO).\n\n### Step 1: Extract Givens\n-   The domain is a two-dimensional periodic domain.\n-   Input and output fields are real-valued.\n-   The number of input channels is $C_{\\text{in}}$.\n-   The number of output channels is $C_{\\text{out}}$.\n-   The learned complex-valued kernel is $R(\\mathbf{k}) \\in \\mathbb{C}^{C_{\\text{out}} \\times C_{\\text{in}}}$.\n-   The wavenumbers $\\mathbf{k} = (k_{x}, k_{y})$ are from the truncated set $\\mathcal{I} = \\{ (k_{x}, k_{y}) \\,:\\, k_{x} \\in \\{-K_{x}, \\dots, K_{x}\\}, \\; k_{y} \\in \\{-K_{y}, \\dots, K_{y}\\} \\}$.\n-   The spectral convolution is given by $\\widehat{y}(\\mathbf{k}) = R(\\mathbf{k}) \\,\\widehat{x}(\\mathbf{k})$.\n-   The property for Discrete Fourier Transform (DFT) of any real-valued field $f$ is $\\widehat{f}(-\\mathbf{k}) = \\overline{\\widehat{f}(\\mathbf{k})}$.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded**: The problem is based on the established mathematical principles of Fourier analysis and its application in modern machine learning models, specifically Fourier Neural Operators. The properties of the DFT for real-valued functions are standard. The setup is scientifically sound.\n-   **Well-Posed**: The problem requests the derivation of a constraint, construction of a parameterization, and calculation of a number, all of which are well-defined mathematical tasks based on the givens. A unique and meaningful solution is expected.\n-   **Objective**: The problem is stated in precise, objective mathematical language. Terms like \"real-valued\", \"complex-valued kernel\", and the definition of the wavenumber set $\\mathcal{I}$ are unambiguous.\n-   **Self-Contained and Consistent**: All necessary information (definitions, properties) is provided. There are no internal contradictions.\n-   **Topic Relevance**: The problem is centrally located within the specified topic of *neural operators for multiscale PDEs* and the field of *multiscale modeling and analysis*.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will proceed with the solution.\n\n### Derivation of the Conjugate Symmetry Constraint\n\nThe problem states that both the input field $x$ and the output field $y$ are real-valued. According to the provided property of the DFT for real-valued fields, their Fourier coefficients must satisfy the following conjugate symmetry conditions:\n$$\n\\widehat{x}(-\\mathbf{k}) = \\overline{\\widehat{x}(\\mathbf{k})}\n$$\n$$\n\\widehat{y}(-\\mathbf{k}) = \\overline{\\widehat{y}(\\mathbf{k})}\n$$\nThe spectral convolution layer is defined as $\\widehat{y}(\\mathbf{k}) = R(\\mathbf{k}) \\,\\widehat{x}(\\mathbf{k})$. We can write this relationship for the wavenumber $-\\mathbf{k}$ as:\n$$\n\\widehat{y}(-\\mathbf{k}) = R(-\\mathbf{k}) \\,\\widehat{x}(-\\mathbf{k})\n$$\nNow, we substitute the symmetry properties of $\\widehat{x}$ and $\\widehat{y}$ into this equation:\n$$\n\\overline{\\widehat{y}(\\mathbf{k})} = R(-\\mathbf{k}) \\,\\overline{\\widehat{x}(\\mathbf{k})}\n$$\nFrom the original definition of the layer, we can take the complex conjugate of both sides:\n$$\n\\overline{\\widehat{y}(\\mathbf{k})} = \\overline{R(\\mathbf{k}) \\,\\widehat{x}(\\mathbf{k})} = \\overline{R(\\mathbf{k})} \\;\\overline{\\widehat{x}(\\mathbf{k})}\n$$\nBy equating the two expressions for $\\overline{\\widehat{y}(\\mathbf{k})}$, we get:\n$$\nR(-\\mathbf{k}) \\,\\overline{\\widehat{x}(\\mathbf{k})} = \\overline{R(\\mathbf{k})} \\;\\overline{\\widehat{x}(\\mathbf{k})}\n$$\nThis equation must hold for any real-valued input field $x$, which means it must be true for any set of Fourier coefficients $\\widehat{x}(\\mathbf{k})$ that satisfies the real-valuedness constraint. In general, $\\overline{\\widehat{x}(\\mathbf{k})}$ is a non-zero vector, so we can conclude that the matrices themselves must be equal:\n$$\nR(-\\mathbf{k}) = \\overline{R(\\mathbf{k})}\n$$\nThis is the required conjugate symmetry constraint on the kernel tensor $R(\\mathbf{k})$.\n\n### Explicit Parameterization of the Kernel\n\nTo ensure the constraint $R(-\\mathbf{k}) = \\overline{R(\\mathbf{k})}$ is met, we only need to define the parameters for a representative \"half\" of the wavenumbers. The values for the other half are then determined by the symmetry. The set of all wavenumbers $\\mathcal{I}$ is symmetric with respect to the origin, i.e., if $\\mathbf{k} \\in \\mathcal{I}$, then $-\\mathbf{k} \\in \\mathcal{I}$. We partition $\\mathcal{I}$ into three types of wavenumbers: the zero mode $\\mathbf{k}=(0,0)$, pairs of non-zero modes $\\{\\mathbf{k}, -\\mathbf{k}\\}$, and any modes on the boundary that might be their own negative (which only occurs for the zero mode).\n\nLet's define a representative half-set of modes, $\\mathcal{H}$, as follows:\n$$\n\\mathcal{H} = \\{ (k_{x}, k_{y}) \\in \\mathcal{I} \\,:\\, k_{y} > 0 \\} \\cup \\{ (k_{x}, k_{y}) \\in \\mathcal{I} \\,:\\, k_{y}=0, k_{x} \\ge 0 \\}\n$$\nThis set contains exactly one representative for each pair $\\{\\mathbf{k}, -\\mathbf{k}\\}$ (for $\\mathbf{k} \\neq (0,0)$) and includes the zero mode $\\mathbf{k}=(0,0)$.\n\nWe can parameterize the complex kernel $R(\\mathbf{k})$ using two real-valued tensors $A(\\mathbf{k}), B(\\mathbf{k}) \\in \\mathbb{R}^{C_{\\text{out}} \\times C_{\\text{in}}}$ defined for $\\mathbf{k} \\in \\mathcal{H}$. The construction is as follows:\n1.  For any $\\mathbf{k} \\in \\mathcal{H}$:\n    -   If $\\mathbf{k}=(0,0)$, the symmetry constraint is $R(0,0) = \\overline{R(0,0)}$, which implies that $R(0,0)$ must be a real matrix. We therefore set $R(0,0) = A(0,0)$ and enforce that its imaginary part is zero.\n    -   If $\\mathbf{k} \\in \\mathcal{H}$ and $\\mathbf{k} \\neq (0,0)$, we define $R(\\mathbf{k}) = A(\\mathbf{k}) + i B(\\mathbf{k})$.\n2.  For any $\\mathbf{k} \\in \\mathcal{I} \\setminus \\mathcal{H}$:\n    -   By construction of $\\mathcal{H}$, we know that $-\\mathbf{k} \\in \\mathcal{H}$ and $-\\mathbf{k} \\neq (0,0)$. We then define $R(\\mathbf{k})$ using the symmetry constraint:\n    $$\n    R(\\mathbf{k}) = \\overline{R(-\\mathbf{k})} = \\overline{A(-\\mathbf{k}) + i B(-\\mathbf{k})} = A(-\\mathbf{k}) - i B(-\\mathbf{k})\n    $$\nThis construction guarantees that $R(-\\mathbf{k}) = \\overline{R(\\mathbf{k})}$ for all $\\mathbf{k} \\in \\mathcal{I}$ by design. The independent learnable parameters are the entries of the real tensors $A(\\mathbf{k})$ for all $\\mathbf{k} \\in \\mathcal{H}$ and $B(\\mathbf{k})$ for all $\\mathbf{k} \\in \\mathcal{H} \\setminus \\{(0,0)\\}$.\n\n### Calculation of the Total Number of Parameters\n\nWe need to count the total number of independent real scalar parameters required to specify the kernel $R(\\mathbf{k})$ over the entire set $\\mathcal{I}$. This is equivalent to counting the number of scalar parameters in the real tensors $A$ and $B$ used in our construction.\n\nLet $N = C_{\\text{in}} C_{\\text{out}}$ be the number of entries in a single-channel matrix.\nThe total number of wavenumbers in $\\mathcal{I}$ is $|\\mathcal{I}| = (2K_{x}+1)(2K_{y}+1)$.\n\nWe can count the parameters by considering the distinct types of modes based on the symmetry:\n1.  **The zero mode $\\mathbf{k}=(0,0)$**: The kernel $R(0,0)$ must be a real matrix. The number of parameters is the number of entries in this real matrix, which is $C_{\\text{out}} \\times C_{\\text{in}} = N$.\n\n2.  **Non-zero modes $\\mathbf{k} \\neq (0,0)$**: These modes can be grouped into pairs $\\{\\mathbf{k}, -\\mathbf{k}\\}$. The total number of non-zero modes is $|\\mathcal{I}| - 1$. The number of such pairs is $\\frac{|\\mathcal{I}|-1}{2}$. For each pair, we only need to store the parameters for one representative, say $\\mathbf{k} \\in \\mathcal{H}$. The kernel $R(\\mathbf{k})$ is a general complex matrix, which requires $2$ real numbers (real and imaginary parts) for each of its $N$ entries. Thus, for each pair, we need $2N$ parameters. The value of $R(-\\mathbf{k})$ is then fully determined.\n    The number of parameters for all these pairs is:\n    $$\n    \\left( \\frac{|\\mathcal{I}|-1}{2} \\right) \\times (2N) = (|\\mathcal{I}|-1)N\n    $$\n\nThe total number of independent real scalar parameters is the sum of the parameters for the zero mode and all the non-zero mode pairs.\n$$\n\\text{Total Parameters} = (\\text{Params for } \\mathbf{k}=0) + (\\text{Params for } \\mathbf{k} \\neq 0)\n$$\n$$\n\\text{Total Parameters} = N + (|\\mathcal{I}|-1)N = N + |\\mathcal{I}|N - N = |\\mathcal{I}|N\n$$\nSubstituting the expressions for $|\\mathcal{I}|$ and $N$:\n$$\n\\text{Total Parameters} = ((2K_{x}+1)(2K_{y}+1)) \\times (C_{\\text{in}}C_{\\text{out}})\n$$\nThus, the total number of independent real parameters is:\n$$\nC_{\\text{in}}C_{\\text{out}}(2K_{x}+1)(2K_{y}+1)\n$$\nThis elegant result shows that the number of parameters is exactly the number of real parameters needed to store one real matrix of size $C_{\\text{out}} \\times C_{\\text{in}}$ for each mode in the frequency grid $\\mathcal{I}$.",
            "answer": "$$\n\\boxed{C_{\\text{in}}C_{\\text{out}}(2K_{x}+1)(2K_{y}+1)}\n$$"
        },
        {
            "introduction": "While the standard FNO is naturally suited for periodic domains, many scientific and engineering problems are defined on bounded domains with specific boundary conditions. This practice demonstrates how to extend the power of neural operators to such non-periodic settings by replacing the standard Fourier basis with a more suitable one. You will derive the appropriate sine and cosine eigenbasis for a PDE with mixed boundary conditions and see how this custom basis diagonalizes the differential operator, providing an efficient foundation for a generalized FNO architecture .",
            "id": "3787628",
            "problem": "Consider the second-order linear elliptic partial differential equation $-\\kappa \\Delta u + \\mu u = f$ on the rectangular domain $\\Omega = (0,L_{x}) \\times (0,L_{y})$, where $\\kappa > 0$, $\\mu \\ge 0$, and $f$ is a square-integrable forcing. Impose homogeneous Dirichlet boundary conditions on the vertical boundaries and homogeneous Neumann boundary conditions on the horizontal boundaries, that is, $u(0,y) = u(L_{x},y) = 0$ for all $y \\in [0,L_{y}]$, and $\\partial_{y} u(x,0) = \\partial_{y} u(x,L_{y}) = 0$ for all $x \\in [0,L_{x}]$. Using the principles of Sturm–Liouville theory and separation of variables, derive an orthonormal basis of eigenfunctions on $\\Omega$ that is compatible with these boundary conditions, and identify the corresponding eigenvalues of the Laplace operator.\n\nAssume constant coefficients and exploit the orthogonal expansion of $u$ and $f$ in this basis to express the spectral relation between the expansion coefficients. Then, by viewing the inverse of $-\\kappa \\Delta + \\mu$ as a convolution-type operator diagonalized in this basis, derive the closed-form spectral multiplier $m_{k,\\ell}$ that maps the coefficient of $f$ at mode $(k,\\ell)$ to the coefficient of $u$ at the same mode. Finally, compute the numerical value of $m_{2,1}$ for $L_{x} = 2$, $L_{y} = 3$, $\\kappa = 0.5$, and $\\mu = 0.2$. Express the final numerical coefficient as a dimensionless number and round your answer to four significant figures.\n\nIn your derivation, clearly indicate how the spectral multiplier would be incorporated into a Fourier Neural Operator (FNO) layer by replacing the standard periodic Fourier basis with the sine/cosine transform basis compatible with the boundary conditions, without using any shortcut formulas and starting from fundamental definitions and properties of orthogonal eigenfunction expansions.",
            "solution": "The user wants me to solve the following problem statement.\n\n**Problem Statement:**\n\nConsider the second-order linear elliptic partial differential equation $-\\kappa \\Delta u + \\mu u = f$ on the rectangular domain $\\Omega = (0,L_{x}) \\times (0,L_{y})$, where $\\kappa > 0$, $\\mu \\ge 0$, and $f$ is a square-integrable forcing. Impose homogeneous Dirichlet boundary conditions on the vertical boundaries and homogeneous Neumann boundary conditions on the horizontal boundaries, that is, $u(0,y) = u(L_{x},y) = 0$ for all $y \\in [0,L_{y}]$, and $\\partial_{y} u(x,0) = \\partial_{y} u(x,L_{y}) = 0$ for all $x \\in [0,L_{x}]$. Using the principles of Sturm–Liouville theory and separation of variables, derive an orthonormal basis of eigenfunctions on $\\Omega$ that is compatible with these boundary conditions, and identify the corresponding eigenvalues of the Laplace operator.\n\nAssume constant coefficients and exploit the orthogonal expansion of $u$ and $f$ in this basis to express the spectral relation between the expansion coefficients. Then, by viewing the inverse of $-\\kappa \\Delta + \\mu$ as a convolution-type operator diagonalized in this basis, derive the closed-form spectral multiplier $m_{k,\\ell}$ that maps the coefficient of $f$ at mode $(k,\\ell)$ to the coefficient of $u$ at the same mode. Finally, compute the numerical value of $m_{2,1}$ for $L_{x} = 2$, $L_{y} = 3$, $\\kappa = 0.5$, and $\\mu = 0.2$. Express the final numerical coefficient as a dimensionless number and round your answer to four significant figures.\n\nIn your derivation, clearly indicate how the spectral multiplier would be incorporated into a Fourier Neural Operator (FNO, Fourier Neural Operator) layer by replacing the standard periodic Fourier basis with the sine/cosine transform basis compatible with the boundary conditions, without using any shortcut formulas and starting from fundamental definitions and properties of orthogonal eigenfunction expansions.\n\n### Step 1: Problem Validation\n\n**Extracted Givens:**\n- PDE: $-\\kappa \\Delta u + \\mu u = f$\n- Domain: $\\Omega = (0,L_{x}) \\times (0,L_{y})$\n- Parameters: $\\kappa > 0$, $\\mu \\ge 0$\n- Forcing term: $f \\in L^2(\\Omega)$\n- Boundary Conditions:\n    - $u(0,y) = u(L_{x},y) = 0$ for $y \\in [0,L_{y}]$ (Homogeneous Dirichlet)\n    - $\\partial_{y} u(x,0) = \\partial_{y} u(x,L_{y}) = 0$ for $x \\in [0,L_{x}]$ (Homogeneous Neumann)\n- Numerical values for final calculation: $L_{x} = 2$, $L_{y} = 3$, $\\kappa = 0.5$, $\\mu = 0.2$. Specific mode is $(k,\\ell) = (2,1)$.\n\n**Validation:**\n1.  **Scientific or Factual Unsoundness**: The problem is scientifically sound. It describes a standard boundary value problem for a linear elliptic PDE (a Helmholtz-type equation), a cornerstone of mathematical physics and engineering.\n2.  **Non-Formalizable or Irrelevant**: The problem is highly formalizable and directly relevant to the topic of neural operators for multiscale PDEs, as explicitly stated.\n3.  **Incomplete or Contradictory Setup**: All necessary data, including the PDE, domain, parameters, and boundary conditions, are provided\n    and are consistent.\n4.  **Unrealistic or Infeasible**: The physical and mathematical setup is standard and entirely feasible.\n5.  **Ill-Posed or Poorly Structured**: The problem is well-posed. For a given $f \\in L^2(\\Omega)$, the specified elliptic boundary value problem has a unique weak solution. The structure of the question is logical, guiding the solver from first principles to the final application.\n6.  **Pseudo-Profound, Trivial, or Tautological**: The problem is substantive, requiring a multi-step derivation involving separation of variables, Sturm-Liouville theory, eigenfunction expansions, and application to a modern machine learning context.\n7.  **Outside Scientific Verifiability**: The derivation and result are entirely verifiable through standard mathematical procedures.\n\n**Verdict:** The problem is valid.\n\n### Step 2: Solution Derivation\n\nThe solution proceeds in five parts as requested: (1) derivation of eigenfunctions and eigenvalues, (2) derivation of the spectral relation, (3) derivation of the spectral multiplier, (4) computation of a specific numerical value, and (5) explanation of the connection to Fourier Neural Operators.\n\n**Part 1: Eigenfunctions and Eigenvalues of the Laplace Operator**\n\nWe seek the eigenfunctions $\\phi(x,y)$ and eigenvalues $\\lambda$ of the negative Laplacian, $-\\Delta$, subject to the given boundary conditions. The eigenvalue problem is:\n$$ -\\Delta \\phi = \\lambda \\phi \\quad \\text{on } \\Omega $$\nwith boundary conditions:\n$$ \\phi(0,y) = \\phi(L_{x},y) = 0 $$\n$$ \\partial_{y} \\phi(x,0) = \\partial_{y} \\phi(x,L_{y}) = 0 $$\nWe use the method of separation of variables, assuming a solution of the form $\\phi(x,y) = X(x)Y(y)$. Substituting this into the eigenvalue equation gives:\n$$ - (X''(x)Y(y) + X(x)Y''(y)) = \\lambda X(x)Y(y) $$\nDividing by $X(x)Y(y)$ allows for separation of the variables:\n$$ -\\frac{X''(x)}{X(x)} - \\frac{Y''(y)}{Y(y)} = \\lambda $$\nLet $-\\frac{X''(x)}{X(x)} = \\lambda_{x}$ and $-\\frac{Y''(y)}{Y(y)} = \\lambda_{y}$, where $\\lambda = \\lambda_x + \\lambda_y$. This results in two independent one-dimensional Sturm-Liouville problems.\n\n**Problem for $X(x)$:**\nThe ODE is $X''(x) + \\lambda_x X(x) = 0$ on $(0,L_x)$. The boundary conditions for $\\phi$ imply $X(0)Y(y) = 0$ and $X(L_x)Y(y) = 0$. For a non-trivial solution ($Y(y) \\not\\equiv 0$), we must have $X(0) = 0$ and $X(L_x) = 0$.\nThe general solution for $\\lambda_x > 0$ is $X(x) = A \\sin(\\sqrt{\\lambda_x} x) + B \\cos(\\sqrt{\\lambda_x} x)$.\n- $X(0)=0 \\implies B=0$.\n- $X(L_x)=0 \\implies A \\sin(\\sqrt{\\lambda_x} L_x) = 0$.\nFor a non-trivial solution ($A \\neq 0$), we must have $\\sqrt{\\lambda_x} L_x = k\\pi$ for an integer $k$. Since $k=0$ gives the trivial solution $X(x)=0$ and negative $k$ values are redundant, we have $k = 1, 2, 3, \\ldots$.\nThe eigenvalues are $\\lambda_{x,k} = (\\frac{k\\pi}{L_x})^2$ for $k \\in \\{1, 2, 3, \\dots\\}$.\nThe corresponding eigenfunctions are $X_k(x) = \\sin(\\frac{k\\pi x}{L_x})$.\n\n**Problem for $Y(y)$:**\nThe ODE is $Y''(y) + \\lambda_y Y(y) = 0$ on $(0,L_y)$. The boundary conditions for $\\phi$ imply $X(x)Y'(0)=0$ and $X(x)Y'(L_y)=0$. For a non-trivial solution ($X(x) \\not\\equiv 0$), we must have $Y'(0) = 0$ and $Y'(L_y) = 0$.\nThe general solution for $\\lambda_y > 0$ is $Y(y) = C \\cos(\\sqrt{\\lambda_y} y) + D \\sin(\\sqrt{\\lambda_y} y)$.\nThe derivative is $Y'(y) = -C\\sqrt{\\lambda_y} \\sin(\\sqrt{\\lambda_y} y) + D\\sqrt{\\lambda_y} \\cos(\\sqrt{\\lambda_y} y)$.\n- $Y'(0)=0 \\implies D\\sqrt{\\lambda_y}=0 \\implies D=0$.\n- $Y'(L_y)=0 \\implies -C\\sqrt{\\lambda_y} \\sin(\\sqrt{\\lambda_y} L_y) = 0$.\nFor a non-trivial solution ($C \\neq 0$), we must have $\\sin(\\sqrt{\\lambda_y} L_y) = 0$, which means $\\sqrt{\\lambda_y} L_y = \\ell\\pi$ for an integer $\\ell$.\nWe must also consider the case $\\lambda_y=0$. The ODE becomes $Y''(y)=0$, with general solution $Y(y) = C_1 y + C_2$. The derivative is $Y'(y) = C_1$. Conditions $Y'(0)=0$ and $Y'(L_y)=0$ both imply $C_1=0$. Thus, $Y(y)=C_2$ (a constant) is a valid eigenfunction corresponding to $\\lambda_y=0$. This case is naturally included by allowing $\\ell=0$ in the general formula.\nThe eigenvalues are $\\lambda_{y,\\ell} = (\\frac{\\ell\\pi}{L_y})^2$ for $\\ell \\in \\{0, 1, 2, \\dots\\}$.\nThe corresponding eigenfunctions are $Y_\\ell(y) = \\cos(\\frac{\\ell\\pi y}{L_y})$.\n\n**2D Eigenfunctions and Eigenvalues:**\nThe eigenfunctions for the operator $-\\Delta$ on $\\Omega$ are the products $\\phi_{k,\\ell}(x,y) = X_k(x)Y_\\ell(y)$:\n$$ \\phi_{k,\\ell}(x,y) = \\sin\\left(\\frac{k\\pi x}{L_x}\\right) \\cos\\left(\\frac{\\ell\\pi y}{L_y}\\right) $$\nfor integer indices $k \\ge 1$ and $\\ell \\ge 0$.\nThe corresponding eigenvalues $\\lambda_{k,\\ell}$ are the sums of the one-dimensional eigenvalues:\n$$ \\lambda_{k,\\ell} = \\lambda_{x,k} + \\lambda_{y,\\ell} = \\left(\\frac{k\\pi}{L_x}\\right)^2 + \\left(\\frac{\\ell\\pi}{L_y}\\right)^2 $$\n\n**Orthonormal Basis:**\nTo create an orthonormal basis, we normalize these eigenfunctions with respect to the $L^2$ inner product $\\langle g, h \\rangle = \\int_0^{L_y} \\int_0^{L_x} g(x,y)h(x,y) \\,dx\\,dy$.\nThe squared norm of $\\phi_{k,\\ell}$ is:\n$$ \\|\\phi_{k,\\ell}\\|^2 = \\left(\\int_0^{L_x} \\sin^2\\left(\\frac{k\\pi x}{L_x}\\right) dx\\right) \\left(\\int_0^{L_y} \\cos^2\\left(\\frac{\\ell\\pi y}{L_y}\\right) dy\\right) $$\nThe standard integrals are $\\int_0^L \\sin^2(\\frac{k\\pi x}{L}) dx = \\frac{L}{2}$ for $k \\ge 1$ and $\\int_0^L \\cos^2(\\frac{\\ell\\pi y}{L}) dy = \\frac{L}{2}$ for $\\ell \\ge 1$. For $\\ell=0$, $\\cos(0)=1$, so $\\int_0^{L_y} \\cos^2(0) dy = L_y$.\n- For $\\ell \\ge 1$: $\\|\\phi_{k,\\ell}\\|^2 = (\\frac{L_x}{2})(\\frac{L_y}{2}) = \\frac{L_x L_y}{4}$. The normalization constant is $\\frac{2}{\\sqrt{L_x L_y}}$.\n- For $\\ell = 0$: $\\|\\phi_{k,0}\\|^2 = (\\frac{L_x}{2})(L_y) = \\frac{L_x L_y}{2}$. The normalization constant is $\\sqrt{\\frac{2}{L_x L_y}}$.\n\nThe orthonormal basis of eigenfunctions, denoted $\\{\\psi_{k,\\ell}\\}$, is:\n$$ \\psi_{k,\\ell}(x,y) = \\begin{cases} \\sqrt{\\frac{2}{L_x L_y}} \\sin\\left(\\frac{k\\pi x}{L_x}\\right) & \\text{if } \\ell=0 \\\\ \\frac{2}{\\sqrt{L_x L_y}} \\sin\\left(\\frac{k\\pi x}{L_x}\\right) \\cos\\left(\\frac{\\ell\\pi y}{L_y}\\right) & \\text{if } \\ell \\ge 1 \\end{cases} $$\nfor $k \\ge 1, \\ell \\ge 0$.\n\n**Part 2: Spectral Relation**\nWe expand the solution $u(x,y)$ and the forcing term $f(x,y)$ in the orthonormal basis $\\{\\psi_{k,\\ell}\\}$:\n$$ u(x,y) = \\sum_{k=1}^{\\infty}\\sum_{\\ell=0}^{\\infty} \\hat{u}_{k,\\ell} \\psi_{k,\\ell}(x,y) $$\n$$ f(x,y) = \\sum_{k=1}^{\\infty}\\sum_{\\ell=0}^{\\infty} \\hat{f}_{k,\\ell} \\psi_{k,\\ell}(x,y) $$\nwhere the coefficients are found by projection: $\\hat{u}_{k,\\ell} = \\langle u, \\psi_{k,\\ell} \\rangle$ and $\\hat{f}_{k,\\ell} = \\langle f, \\psi_{k,\\ell} \\rangle$.\nSubstitute these expansions into the PDE $-\\kappa \\Delta u + \\mu u = f$:\n$$ -\\kappa \\Delta \\left(\\sum_{k,\\ell} \\hat{u}_{k,\\ell} \\psi_{k,\\ell}\\right) + \\mu \\left(\\sum_{k,\\ell} \\hat{u}_{k,\\ell} \\psi_{k,\\ell}\\right) = \\sum_{k,\\ell} \\hat{f}_{k,\\ell} \\psi_{k,\\ell} $$\nUsing the linearity of the operators and the fact that $-\\Delta \\psi_{k,\\ell} = \\lambda_{k,\\ell} \\psi_{k,\\ell}$:\n$$ \\sum_{k,\\ell} \\left( \\kappa \\lambda_{k,\\ell} \\hat{u}_{k,\\ell} + \\mu \\hat{u}_{k,\\ell} \\right) \\psi_{k,\\ell} = \\sum_{k,\\ell} \\hat{f}_{k,\\ell} \\psi_{k,\\ell} $$\nBy the orthogonality of the basis functions, we can equate the coefficients for each mode $(k,\\ell)$:\n$$ (\\kappa \\lambda_{k,\\ell} + \\mu) \\hat{u}_{k,\\ell} = \\hat{f}_{k,\\ell} $$\nThis is the spectral relation between the expansion coefficients of the solution and the forcing term.\n\n**Part 3: Spectral Multiplier**\nThe spectral multiplier $m_{k,\\ell}$ is defined as the factor that maps the coefficient of the input $f$ to the coefficient of the output $u$, i.e., $\\hat{u}_{k,\\ell} = m_{k,\\ell} \\hat{f}_{k,\\ell}$. From the spectral relation above, we can directly identify $m_{k,\\ell}$:\n$$ m_{k,\\ell} = \\frac{1}{\\kappa \\lambda_{k,\\ell} + \\mu} $$\nSubstituting the expression for the eigenvalues $\\lambda_{k,\\ell}$:\n$$ m_{k,\\ell} = \\frac{1}{\\kappa \\left[ \\left(\\frac{k\\pi}{L_x}\\right)^2 + \\left(\\frac{\\ell\\pi}{L_y}\\right)^2 \\right] + \\mu} $$\n\n**Part 4: Numerical Computation**\nWe are asked to compute $m_{2,1}$ given the parameters $L_x = 2$, $L_y = 3$, $\\kappa = 0.5$, and $\\mu = 0.2$. The mode is $(k,\\ell) = (2,1)$.\n$$ m_{2,1} = \\frac{1}{(0.5) \\left[ \\left(\\frac{2\\pi}{2}\\right)^2 + \\left(\\frac{1\\pi}{3}\\right)^2 \\right] + 0.2} $$\n$$ m_{2,1} = \\frac{1}{0.5 \\left[ \\pi^2 + \\frac{\\pi^2}{9} \\right] + 0.2} $$\n$$ m_{2,1} = \\frac{1}{0.5 \\left[ \\frac{9\\pi^2 + \\pi^2}{9} \\right] + 0.2} $$\n$$ m_{2,1} = \\frac{1}{0.5 \\left( \\frac{10\\pi^2}{9} \\right) + 0.2} $$\n$$ m_{2,1} = \\frac{1}{\\frac{5\\pi^2}{9} + 0.2} $$\nNow, we compute the numerical value:\n$$ m_{2,1} = \\frac{1}{\\frac{5 \\times (9.8696044...)}{9} + 0.2} = \\frac{1}{5.4831135... + 0.2} = \\frac{1}{5.6831135...} \\approx 0.1759600... $$\nRounding to four significant figures, we get $0.1760$.\n\n**Part 5: Connection to Fourier Neural Operators (FNO)**\nThe derivation above shows that the solution operator $\\mathcal{G}: f \\mapsto u$ for the PDE $-\\kappa\\Delta u + \\mu u = f$ acts as a simple multiplication in the spectral domain defined by the eigenfunctions $\\{\\psi_{k,\\ell}\\}$. This means $\\mathcal{G}$ is a convolution-type operator that is diagonalized by the basis of sine-cosine functions derived. The values on this diagonal are precisely the spectral multipliers $m_{k,\\ell}$.\n\nA Fourier Neural Operator (FNO) is a neural network architecture designed to learn such solution operators. A standard FNO works with periodic boundary conditions, for which the eigenfunctions of the Laplacian are the complex exponentials $\\{e^{i \\mathbf{k} \\cdot \\mathbf{x}}\\}$. The transformation to and from this spectral basis is the Fourier Transform, efficiently implemented by the Fast Fourier Transform (FFT).\n\nTo adapt the FNO architecture for the non-periodic boundary conditions in this problem, one must replace the standard Fourier basis with the problem-specific eigenbasis. The procedure for a generalized FNO layer that learns the operator for this problem would be:\n\n1.  **Input**: The layer receives an input function $v(x,y)$ discretized on a grid.\n2.  **Forward Transform**: Instead of applying the FFT, apply a 2D transform that projects the input $v$ onto the derived eigenbasis. This is practically achieved by performing a Discrete Sine Transform (DST) along the $x$-dimension and a Discrete Cosine Transform (DCT) along the $y$-dimension. Let's denote this combined transform as $\\mathcal{T}$. This step computes the spectral coefficients $\\hat{v}_{k,\\ell} = (\\mathcal{T}(v))_{k,\\ell}$.\n3.  **Spectral Multiplication (Learned Kernel)**: In the spectral domain, apply a point-wise multiplication with a set of learnable parameters (weights) $R_{k,\\ell}$. These parameters are the neural network's approximation of the true spectral multiplier. The FNO typically truncates the series to a maximum number of modes $(k_{\\max}, \\ell_{\\max})$.\n    $$ (\\widehat{K(v)})_{k,\\ell} = R_{k,\\ell} \\cdot \\hat{v}_{k,\\ell} \\quad \\text{for } 1 \\le k \\le k_{\\max}, \\, 0 \\le \\ell \\le \\ell_{\\max} $$\n    The core idea of FNO is that $R_{k,\\ell}$ is learnable through backpropagation, allowing the network to approximate the response of any similar operator, not just the one with the specific $\\kappa$ and $\\mu$ derived here.\n4.  **Inverse Transform**: Apply the inverse transform $\\mathcal{T}^{-1}$ (consisting of an Inverse DST and an Inverse DCT) to the modified coefficients to return the result to the physical domain.\n    $$ K(v)(x,y) = \\mathcal{T}^{-1}\\left(R \\cdot \\hat{v}\\right)(x,y) $$\n5.  **Local Path and Activation**: As in a standard residual network, add a local linear transformation of the input (e.g., a point-wise linear layer implemented as a $1 \\times 1$ convolution, $Wv$) and apply a non-linear activation function $\\sigma$.\n    $$ v_{\\text{out}}(x,y) = \\sigma(K(v)(x,y) + Wv(x,y)) $$\n\nBy choosing the transform $\\mathcal{T}$ that diagonalizes the underlying differential operator, the FNO is provided with the most efficient representation for learning the solution operator. The learnable filter $R_{k,\\ell}$ only needs to learn a diagonal operator, which is far more parameter-efficient and easier to train than learning a full, dense operator in a non-optimal basis (such as the standard Fourier basis for this non-periodic problem).",
            "answer": "$$ \\boxed{0.1760} $$"
        },
        {
            "introduction": "A powerful architecture is only useful if it can be trained effectively, and deep models like FNOs can be prone to numerical instability, especially when learning multiscale dynamics. This final practice addresses the critical challenge of ensuring a stable training process by focusing on the behavior of the solution's derivatives, as measured by the Sobolev $H^1$ norm. You will learn to diagnose instability by monitoring norm growth across layers and explore principled corrective measures, such as dealiasing and operator normalization, to build robust and reliable models .",
            "id": "3787624",
            "problem": "Consider a Fourier Neural Operator (FNO) layer acting on a periodic field on a $d$-dimensional torus $\\Omega = [0,2\\pi]^d$ discretized on a uniform grid of $N^d$ points. Let $\\mathcal{F}$ denote the discrete Fourier transform and let $u^\\ell$ denote the hidden field at layer $\\ell$. A typical FNO layer can be written as $u^{\\ell+1} = \\sigma\\!\\left(\\mathcal{F}^{-1}\\!\\left(M^\\ell \\cdot \\mathcal{F}u^\\ell\\right) + S^\\ell u^\\ell + b^\\ell\\right)$, where $M^\\ell$ is a learned complex-valued spectral multiplier restricted to modes $\\{k \\in \\mathbb{Z}^d: |k| \\le k_{\\max}\\}$, $S^\\ell$ is a learned local linear operator in physical space, $b^\\ell$ is a bias, and $\\sigma$ is a pointwise nonlinearity. The Sobolev space $H^1(\\Omega)$ norm is defined by $\\|u\\|_{H^1}^2 = \\int_\\Omega \\left(|u(x)|^2 + |\\nabla u(x)|^2\\right)\\,dx$, which by Parseval's identity is equivalent to $\\|u\\|_{H^1}^2 = \\sum_{k \\in \\mathbb{Z}^d} \\left(1+|k|^2\\right)\\,|u_k|^2$, where $u_k$ are Fourier coefficients. Suppose the network is trained to approximate a multiscale partial differential equation-to-solution operator $u_\\theta$ on inputs exhibiting significant energy across a range of scales.\n\nIn practice, pointwise nonlinearities $\\sigma$ in physical space generate high-frequency content whose Fourier support exceeds the spectral truncation $\\{|k|\\le k_{\\max}\\}$; projection back onto the truncated spectrum can therefore induce aliasing. Additionally, aggressive spectral multipliers and unnormalized residual pathways can cause layerwise growth of $\\|u^\\ell\\|_{H^1}$, which is associated with instability in learned discretizations of multiscale dynamics. You are tasked with designing a diagnostic test that uses the growth behavior of $\\|u^\\ell\\|_{H^1}$ across layers to detect instability, and proposing a corrective architectural modification that directly addresses aliasing and/or normalization in a way consistent with stable multiscale modeling.\n\nAssume you have observed, on a representative validation set, per-layer growth factors $r_\\ell = \\|u^{\\ell+1}\\|_{H^1}/\\|u^\\ell\\|_{H^1}$ that satisfy $r_\\ell \\approx 1.25$ for $\\ell$ in a range of layers, and that the fraction of $H^1$-weighted spectral energy in the high-frequency band $B = \\{k: 2k_{\\max}/3 < |k| \\le k_{\\max}\\}$ after application of $\\sigma$ exceeds $0.3$. From first principles, define a diagnostic and a corrective modification that are appropriate for the FNO class and multiscale PDE settings.\n\nSelect all options that correctly specify both a sound diagnostic based on $\\|u^\\ell\\|_{H^1}$ growth and an effective corrective architectural modification.\n\nA. Diagnostic: compute $r_\\ell = \\|u^{\\ell+1}\\|_{H^1}/\\|u^\\ell\\|_{H^1}$ over validation inputs and flag instability if the geometric mean $(\\prod_{\\ell=0}^{L-1} r_\\ell)^{1/L}$ exceeds a threshold greater than $1$. Correction: introduce $2/3$-rule dealiasing by zero-padding the field to size $3N/2$ in each dimension before applying the pointwise nonlinearity, then low-pass filter and truncate back to $N$, and insert a per-layer Sobolev normalization that rescales $u^{\\ell+1} \\leftarrow \\alpha_\\ell u^{\\ell+1}$ with $\\alpha_\\ell$ chosen so that $\\|u^{\\ell+1}\\|_{H^1} \\le \\beta \\|u^\\ell\\|_{H^1}$ for a prescribed $\\beta \\approx 1$.\n\nB. Diagnostic: monitor only the $L^2(\\Omega)$ norm $\\|u^\\ell\\|_{L^2}$ across layers and flag instability if $\\|u^\\ell\\|_{L^2}$ grows monotonically. Correction: apply batch normalization in physical space after each nonlinearity without any spectral filtering or constraints on $M^\\ell$.\n\nC. Diagnostic: track $\\max_{k} |u_k^\\ell|$ across layers and flag instability if it increases. Correction: increase the truncation bandwidth $k_{\\max}$ while leaving the nonlinearity and layer scaling unchanged.\n\nD. Diagnostic: for each layer, estimate the largest singular value (spectral radius) of the Fréchet derivative of the layer mapping with respect to the $H^1$ inner product via power iteration on validation inputs; flag instability if this estimate exceeds $1$. Correction: impose spectral normalization on the spectral multipliers and residual pathways to bound the overall Lipschitz constant in $H^1$ strictly below $1$ (for example, rescale $M^\\ell$ and $S^\\ell$ so that the induced operator norm from $H^1$ to $H^1$ is less than $1$), ensuring per-layer contraction in $H^1$.",
            "solution": "The problem requires the design of a diagnostic test for instability and a corresponding corrective architectural modification for a Fourier Neural Operator (FNO). The instability is characterized by the layer-wise growth of the Sobolev $H^1$ norm of the hidden fields, and potential causes are identified as aliasing from the pointwise nonlinearity and amplification from the linear operators in the layer update.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- **Model:** Fourier Neural Operator (FNO) layer on a periodic domain $\\Omega = [0,2\\pi]^d$.\n- **Discretization:** Uniform grid, $N^d$ points.\n- **Layer Update Rule:** $u^{\\ell+1} = \\sigma\\!\\left(\\mathcal{F}^{-1}\\!\\left(M^\\ell \\cdot \\mathcal{F}u^\\ell\\right) + S^\\ell u^\\ell + b^\\ell\\right)$.\n- **Variables & Operators:**\n    - $u^\\ell$: hidden field at layer $\\ell$.\n    - $\\mathcal{F}$: discrete Fourier transform.\n    - $M^\\ell$: learned spectral multiplier, non-zero for modes $|k| \\le k_{\\max}$.\n    - $S^\\ell$: learned local linear operator.\n    - $b^\\ell$: bias term.\n    - $\\sigma$: pointwise nonlinearity.\n- **Sobolev $H^1$ Norm:** Defined by $\\|u\\|_{H^1}^2 = \\sum_{k \\in \\mathbb{Z}^d} \\left(1+|k|^2\\right)\\,|u_k|^2$, where $u_k$ are Fourier coefficients.\n- **Context:** Approximation of a multiscale PDE solution operator.\n- **Observed Phenomena:**\n    1. A per-layer $H^1$ norm growth factor $r_\\ell = \\|u^{\\ell+1}\\|_{H^1}/\\|u^\\ell\\|_{H^1} \\approx 1.25$.\n    2. The fraction of $H^1$-weighted spectral energy in the high-frequency band $B = \\{k: 2k_{\\max}/3 < |k| \\le k_{\\max}\\}$ after applying $\\sigma$ exceeds $0.3$.\n- **Task:** Define a diagnostic and a corrective modification based on these observations and first principles.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem statement is scientifically sound, well-posed, and objective.\n- **Scientific Grounding:** The problem is rooted in established principles of numerical analysis for PDEs (pseudospectral methods, stability, aliasing), functional analysis (Sobolev spaces), and deep learning (neural operators, training instability). The FNO architecture, the use of the $H^1$ norm for PDE solutions, and the mechanism of aliasing via pointwise multiplication are all standard concepts.\n- **Well-Posedness & Consistency:** The problem is well-posed. It provides specific quantitative observations ($r_\\ell \\approx 1.25$, energy concentration) and asks for the evaluation of proposed diagnostic/correction pairs. The provided information is internally consistent: a growth factor greater than $1$ signifies instability, and the energy concentration in a specific high-frequency band points towards aliasing, a known cause of numerical artifacts and instability.\n- **Objectivity & Clarity:** The problem uses precise, standard terminology from mathematics and computer science. There is no ambiguity or reliance on subjective claims.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. I will proceed with the derivation and analysis of the options.\n\n### Derivation and Option Analysis\n\nThe core problem is instability, evidenced by the exponential growth of the $H^1$ norm across layers ($r_\\ell \\approx 1.25 > 1$). Such growth leads to exploding activations and gradients, preventing successful training. An effective solution must address the underlying causes: amplification by the layer's operators and potential error accumulation from aliasing.\n\n**Option A Analysis**\n- **Diagnostic:** \"compute $r_\\ell = \\|u^{\\ell+1}\\|_{H^1}/\\|u^\\ell\\|_{H^1}$ over validation inputs and flag instability if the geometric mean $\\left(\\prod_{\\ell=0}^{L-1} r_\\ell\\right)^{1/L}$ exceeds a threshold greater than $1$.\"\n  This diagnostic directly measures the average per-layer growth of the $H^1$ norm. Since the observed growth factor is $r_\\ell \\approx 1.25$, the geometric mean would be approximately $1.25$, which is greater than $1$, correctly flagging the network as unstable. This is a direct, empirical, and sound diagnostic method.\n- **Correction:** \"introduce $2/3$-rule dealiasing by zero-padding the field to size $3N/2$ in each dimension before applying the pointwise nonlinearity, then low-pass filter and truncate back to $N$, and insert a per-layer Sobolev normalization that rescales $u^{\\ell+1} \\leftarrow \\alpha_\\ell u^{\\ell+1}$ with $\\alpha_\\ell$ chosen so that $\\|u^{\\ell+1}\\|_{H^1} \\le \\beta \\|u^\\ell\\|_{H^1}$ for a prescribed $\\beta \\approx 1$.\"\n  This correction has two components.\n  1.  **Dealiasing:** The pointwise nonlinearity $\\sigma(u)$ in physical space corresponds to convolutions in Fourier space, which can generate frequencies up to multiples of $k_{\\max}$. The observation of significant energy in the band $\\{k: 2k_{\\max}/3 < |k| \\le k_{\\max}\\}$ strongly suggests that aliasing from quadratic-like interactions is a problem. The described procedure is the standard **2/3-rule** for dealiasing, a classic and effective technique from pseudospectral methods that prevents aliasing contamination from quadratic nonlinearities. This directly and appropriately addresses the aliasing issue.\n  2.  **Normalization:** The explicit rescaling of the layer's output $u^{\\ell+1}$ to enforce the condition $\\|u^{\\ell+1}\\|_{H^1} \\le \\beta \\|u^\\ell\\|_{H^1}$ with $\\beta \\approx 1$ directly counteracts the observed norm explosion. This \"brute-force\" normalization guarantees stability by preventing layer-wise amplification. Using $\\beta \\approx 1$ provides stability without being overly restrictive.\n  This two-pronged correction effectively addresses both aliasing and amplification.\n- **Verdict:** Correct. The diagnostic is sound, and the corrective modifications are well-established, effective, and directly address the phenomena described in the problem statement.\n\n**Option B Analysis**\n- **Diagnostic:** \"monitor only the $L^2(\\Omega)$ norm $\\|u^\\ell\\|_{L^2}$ across layers and flag instability if $\\|u^\\ell\\|_{L^2}$ grows monotonically.\"\n  The $H^1$ norm includes derivatives ($|k|^2$ weighting in the Fourier domain), making it sensitive to high-frequency instabilities. A function's $L^2$ norm can remain bounded while its $H^1$ norm explodes, which corresponds to the solution becoming highly oscillatory and its derivatives blowing up. For PDE modeling, controlling derivatives is crucial for stability and accuracy. Therefore, monitoring only the $L^2$ norm is an insufficient diagnostic.\n- **Correction:** \"apply batch normalization in physical space after each nonlinearity without any spectral filtering or constraints on $M^\\ell$.\"\n  Standard batch normalization normalizes the mean and variance of features pointwise across a batch. It is not designed to control a function-space norm like $H^1$ that involves derivatives. It does nothing to address aliasing in the Fourier domain and does not directly constrain the amplification properties of the spectral operator $M^\\ell$. It is a generic heuristic and not a targeted fix for the specific instability mechanism of this problem.\n- **Verdict:** Incorrect. Both the diagnostic and correction are inadequate for the problem's context.\n\n**Option C Analysis**\n- **Diagnostic:** \"track $\\max_{k} |u_k^\\ell|$ across layers and flag instability if it increases.\"\n  The overall stability and energy of the solution are captured by an integrated quantity like a norm (e.g., $\\|u\\|_{H^1}^2 = \\sum_k (1+|k|^2)|u_k|^2$), not the magnitude of a single Fourier coefficient. Energy can be distributed across many modes, leading to norm growth even if $\\max_k |u_k^\\ell|$ does not increase. This diagnostic is not a reliable measure of the overall stability.\n- **Correction:** \"increase the truncation bandwidth $k_{\\max}$ while leaving the nonlinearity and layer scaling unchanged.\"\n  This correction is misguided. While increasing $k_{\\max}$ provides more modes to represent the high-frequency content generated by the nonlinearity, it does not solve the core problem of amplification. The unstable operators $M^\\ell$ and $S^\\ell$ would now act on a larger number of modes, potentially exacerbating the instability and increasing computational cost. It fails to address the root cause of amplification, which is the layer's operator norm being greater than $1$.\n- **Verdict:** Incorrect. The diagnostic is a poor metric, and the correction is ineffective and potentially detrimental.\n\n**Option D Analysis**\n- **Diagnostic:** \"for each layer, estimate the largest singular value (spectral radius) of the Fréchet derivative of the layer mapping with respect to the $H^1$ inner product via power iteration on validation inputs; flag instability if this estimate exceeds $1$.\"\n  This is a fundamentally sound and sophisticated diagnostic. The layer update $u^{\\ell+1} = \\mathcal{G}(u^\\ell)$ can be viewed as a discrete-time dynamical system. Its local stability at a point $u^\\ell$ is determined by the operator norm (and spectral radius) of its Fréchet derivative, $\\mathcal{G}'(u^\\ell)$. An operator norm greater than $1$ in the $H^1$ space implies local amplification of perturbations in that norm, leading to instability. Power iteration is the standard numerical method for estimating the largest singular value. This diagnostic is theoretically rigorous.\n- **Correction:** \"impose spectral normalization on the spectral multipliers and residual pathways to bound the overall Lipschitz constant in $H^1$ strictly below $1$ (for example, rescale $M^\\ell$ and $S^\\ell$ so that the induced operator norm from $H^1$ to $H^1$ is less than $1$), ensuring per-layer contraction in $H^1$.\"\n  This correction directly targets the cause of amplification. By ensuring each layer is a contraction mapping in the relevant function space ($H^1$), i.e., its Lipschitz constant is less than $1$, the stability of the entire network is guaranteed. The composition of contraction mappings is a contraction, preventing any possibility of exponential growth. This is a highly principled and effective method for ensuring stability. While it does not explicitly perform dealiasing, enforcing a strict $H^1$ contraction would implicitly penalize and suppress any mechanism, including aliasing, that leads to uncontrolled growth in the $H^1$ norm. The proposed method correctly addresses the \"normalization\" aspect of the problem in a powerful way.\n- **Verdict:** Correct. This option provides a theoretically rigorous diagnostic and a powerful, principled corrective measure that guarantees stability.\n\nBoth Options A and D present valid and effective solutions to the problem. Option A is a pragmatic, engineering-focused solution that directly uses the specific data provided in the prompt. Option D is a more abstract, mathematically principled solution based on operator theory. Both are correct strategies for the stated task.",
            "answer": "$$\\boxed{AD}$$"
        }
    ]
}