## 引言
在物理学中，序参量是描述复杂系统从无序到有序转变的关键概念，它以简洁的方式捕捉了系统宏观状态的本质。然而，在许多前沿研究中，尤其是在面对海量高维实验或模拟数据时，如何先验地确定甚至定义合适的序参量，本身就是一个巨大的挑战。数据驱动的方法，特别是[深度学习](@entry_id:142022)，为解决这一知识鸿沟提供了强大的新范式。

本文系统性地探讨了如何利用自编码器——一种经典的[无监督学习](@entry_id:160566)模型——来自动发现物理系统中的[序参量](@entry_id:144819)。我们将穿越三个循序渐进的章节，为读者构建一个从理论到实践的完整知识体系。首先，在“原理与机制”一章中，我们将深入剖析自编码器作为[流形学习](@entry_id:156668)工具的核心思想，阐明其如何从[高维数据](@entry_id:138874)中提取低维结构，并探讨如何将物理对称性、噪声处理等关键考量融入模型设计。接着，在“应用与跨学科连接”一章中，我们将展示该方法在统计物理、工程计算、生命科学乃至地球科学等多个领域的广泛应用，揭示其作为科学发现框架的强大潜力。最后，“动手实践”部分将引导读者思考如何将理论知识应用于实际问题的验证与评估。

通过本文的学习，读者将掌握利用自编码器分析复杂系统的核心技术，并理解如何将机器学习与深刻的物理洞察力相结合，从而在前沿科学探索中开辟新的道路。让我们首先从理解其基本工作原理开始。

## 原理与机制

继前一章对自编码器在多尺度物理系统中识别[序参量](@entry_id:144819)的基本思想进行介绍之后，本章将深入探讨其核心工作原理与关键机制。我们将从自编码器作为一种[流形学习](@entry_id:156668)工具的基本概念出发，系统地阐述如何利用其发现复杂数据中的低维结构。随后，我们将探讨如何使[潜空间](@entry_id:171820)（latent space）的几何结构与物理系统的对称性相匹配，并分析在面对真实物理系统的挑战（如噪声和多[尺度效应](@entry_id:153734)）时，如何设计更先进的自编码器架构与[损失函数](@entry_id:634569)。最后，本章将从概率论和信息论的视角提供更深层次的理论框架，并讨论一些关键的实践考量和该方法的理论边界。

### 自编码器作为[流形学习](@entry_id:156668)器

在许多物理系统中，尽管微观状态的构型空间维度极高，但系统的宏观行为通常由少数几个慢变的[集体变量](@entry_id:165625)（即[序参量](@entry_id:144819)）所决定。这意味着，高维的构观数据实际上分布在一个嵌入在高维空间中的低维**流形（manifold）**上。自编码器的核心任务，正是学习这一流形的内在结构。

一个标准的自编码器由两部分组成：**编码器（encoder）** $f_\theta: \mathbb{R}^n \to \mathbb{R}^d$ 和**解码器（decoder）** $g_\phi: \mathbb{R}^d \to \mathbb{R}^n$。编码器将高维输入数据 $x \in \mathbb{R}^n$ 压缩成一个低维的**[潜变量](@entry_id:143771)（latent variable）** $z = f_\theta(x) \in \mathbb{R}^d$。解码器则尝试从这个潜变量 $z$ 中恢复出原始输入，得到重构数据 $\hat{x} = g_\phi(z)$。整个网络通过最小化**[重构损失](@entry_id:636740)（reconstruction loss）**来训练，通常采用均方误差（Mean Squared Error, MSE）：

$$
L = \mathbb{E}_{x \sim \mathcal{D}}\left\|x - g_\phi(f_\theta(x))\right\|^2
$$

其中 $\mathcal{D}$ 是数据的真实分布。潜空间的维度 $d$ 远小于输入维度 $n$ ($d \ll n$)，这个低维的“**瓶颈（bottleneck）**”是自编码器能够学习数据内在结构的关键。为了在不丢失信息的前提下完成压缩与重构，网络必须学会识别并编码数据中最具代表性的特征，而忽略那些冗余或不重要的细节。这迫使潜变量 $z$ 成为原始数据 $x$ 的一种高效、紧凑的表示。

一个基本的问题是，瓶颈维度 $d$ 与系统慢变量的真实维度 $k$ 之间有何关系？ 假设系统的慢动力学流形 $\mathcal{M}$ 是一个 $k$ 维光滑子流形。为了能够[完美重构](@entry_id:194472)流形上的每一个点（即对所有 $x \in \mathcal{M}$，都有 $\hat{x} = x$），从编码器到解码器的映射必须是可逆的。这意味着编码器在流形 $\mathcal{M}$ 上的限制 $f_\theta|_{\mathcal{M}}$ 必须是一个**[单射](@entry_id:183792)（injective map）**。否则，如果存在两个不同的点 $x_1 \neq x_2$ 被映射到同一个潜变量 $z$，解码器将无法同时重构出 $x_1$ 和 $x_2$。拓扑学的一个基本结论是，从一个 $k$ 维流形到一个 $d$ 维[欧氏空间](@entry_id:138052)的[连续单射](@entry_id:275991)映射存在的必要条件是 $d \ge k$。因此，为了无损地编码一个 $k$ 维的[序参量](@entry_id:144819)空间，[潜空间](@entry_id:171820)的维度至少需要为 $k$。

值得注意的是，$d \ge k$ 仅仅是一个必要条件。对于具有复杂拓扑结构的流形（例如，一个[克莱因瓶](@entry_id:149661)），根据[惠特尼嵌入定理](@entry_id:161137)（Whitney's embedding theorem），需要更高的维度（如 $d \ge 2k$）才能保证无自相交的嵌入。然而，在许多物理问题中，序参量空间具有相对简单的拓扑结构（如 $\mathbb{R}^k$），此时 $d=k$ 通常是可行的。

为了更直观地理解自编码器的作用，我们可以考察最简单的线性情况。若数据本身就位于一个 $k$ 维[线性子空间](@entry_id:151815)中，例如 $x = Ay$，其中 $y \in \mathbb{R}^k$ 且 $A \in \mathbb{R}^{n \times k}$ 是一个列满秩矩阵。在这种情况下，一个线性的自编码器（即编码器和解码器均为[线性映射](@entry_id:185132)）为了达到零重构误差，其学习到的[投影算子](@entry_id:154142) $P = W_{dec}W_{enc}$ 必须是到 $A$ 的[列空间](@entry_id:156444)上的投影。由于投影算子的秩受限于瓶颈维度 $d$，即 $\mathrm{rank}(P) \le d$，而要覆盖整个 $k$ 维子空间需要 $\mathrm{rank}(P)=k$，因此最小的瓶颈维度必须是 $d=k$。在这种情况下，自编码器学习到的恰好是**[主成分分析](@entry_id:145395)（Principal Component Analysis, PCA）**所找到的主子空间。这表明，自编码器可以被看作是PCA的一种强大的[非线性](@entry_id:637147)推广。

### [潜空间](@entry_id:171820)的几何结构与物理对称性

一个成功的自编码器不仅要实现维度约减，其学习到的潜空间几何结构还应与物理系统[序参量](@entry_id:144819)空间的几何结构相匹配。这对于正确诠释物理规律至关重要。

我们以经典的二维**[XY模型](@entry_id:140763)**为例来说明这一点。 该模型描述了在一个[晶格](@entry_id:148274)上，每个格点的自旋都可以在一个二维平面内自由旋转。在低温的铁[磁相](@entry_id:161372)，所有自旋趋于指向同一个方向，这破坏了系统的全局$O(2)$旋转对称性。描述这种集体行为的[序参量](@entry_id:144819)是系统的总磁化强度矢量 $M = \frac{1}{N} \sum_{i=1}^{N} (\cos \theta_i, \sin \theta_i)$，其中 $\theta_i$ 是第 $i$ 个自旋的角度。由于系统能量在所有自旋同时旋转一个相同角度时保持不变，基态是简并的。所有可能的基态（对应于总磁化强度指向平面内的任意方向）构成了一个与[圆环](@entry_id:163678) $\mathbb{S}^1$ 同构的流形。

因此，一个理想的自编码器在学习[XY模型](@entry_id:140763)铁[磁相](@entry_id:161372)的构型时，其潜空间也应该具有 $\mathbb{S}^1$ 的拓扑结构。如果将[潜空间](@entry_id:171820)设为一维实数区间（如 $[0,1]$），则会产生拓扑不匹配的问题：在圆环上，靠近 $0$ 度的点和靠近 $360$ 度的点是相邻的，但在实数区间上， $0$ 和 $1$ 却是距离最远的点。为了解决这个问题，一种常见的策略是使用二维[潜空间](@entry_id:171820) $(z_1, z_2)$，并通过在损失函数中加入约束（例如，一个正则项惩罚 $z_1^2 + z_2^2 \neq 1$）来促使所有潜变量分布在一个圆环上。这样，自编码器就在二维[欧氏空间](@entry_id:138052)中学习到了一个 $\mathbb{S}^1$ 的嵌入，从而正确地捕捉了序参量空间的周期性。

除了[连续对称性](@entry_id:137257)，[离散对称性](@entry_id:146994)同样对自编码器的学习过程产生深远影响。考虑**[伊辛模型](@entry_id:139066)（Ising model）**，其[哈密顿量](@entry_id:144286)在零外场下具有全局自旋翻转（$s_i \to -s_i$ for all $i$）的$\mathbb{Z}_2$对称性。 这意味着一个构型 $\mathbf{s}$ 和其完全翻转的构型 $-\mathbf{s}$ 出现的概率是相同的。如果自编码器学习到一个[潜变量](@entry_id:143771) $z = f_\theta(\mathbf{s})$ 能很好地代表磁化强度，那么由于物理上的对称性，$-z$ 也应该是一个同样好的表示。这导致自编码器的损失函数景观出现**简并（degeneracy）**：如果一套网络参数 $(\theta, \phi)$ 是一个最优解，那么另一套能产生相反潜变量（即 $f_{\theta'}(\mathbf{s}) = -f_\theta(\mathbf{s})$）的参数 $(\theta', \phi')$ 必然也是一个等价的最优解。

这种简并性会导致一个实际问题：在不同的训练运行中，[潜变量](@entry_id:143771) $z$ 的符号可能与物理[序参量](@entry_id:144819)（总磁化强度 $m(\mathbf{s})$）的符号随机地正相关或负相关，给后续的物理分析带来不便。为了获得一个确定且一致的表示，我们需要**显式地打破这种对称性**。一个有效的方法是在[损失函数](@entry_id:634569)中加入一个微小的**对称性破缺偏置（symmetry-breaking bias）**项。例如，我们可以添加一项 $B(\theta) = -\lambda \mathbb{E}_{\mathbf{s}} [f_\theta(\mathbf{s}) m(\mathbf{s})]$，其中 $\lambda>0$ 是一个小常数。为了最小化总损失，优化器会倾向于让 $f_\theta(\mathbf{s})$ 和 $m(\mathbf{s})$ 的乘积期望为正，即迫使[潜变量](@entry_id:143771) $z$ 与总磁化强度 $m$ 正相关。这个小小的偏置项如同在系统中施加了一个无穷小的“钉扎场”（pinning field），它不影响系统的主要物理，却能帮助我们从两个简并的解中确定地选择一个。

### 挑战与先进架构

标准的自编码器在应用于真实的物理系统时会遇到一系列挑战，这催生了多种更先进的架构和训练策略。

#### 挑战一：噪声的主导地位

物理系统本质上是含噪声的，尤其是在相变点附近，[热涨落](@entry_id:143642)非常剧烈。在一个[多尺度系统](@entry_id:1128345)中，微观状态 $x$ 可以分解为由[序参量](@entry_id:144819) $m$ 决定的宏观结构 $h(m)$ 和快速的微观涨落 $\varepsilon$，即 $x = h(m) + \varepsilon$。通常，微观涨落的总方差 $\mathrm{Tr}(\Sigma_{\varepsilon})$ 远大于宏观结构变化的方差 $\mathrm{Var}(h(m))$。 由于标准自编码器旨在最小化均方重构误差，它会优先学习如何重构数据中方差最大的部分。因此，在上述情况下，自编码器会将其有限的“瓶颈容量”用于编码高方差的噪声 $\varepsilon$，而完全忽略方差较小但对物理学家而言至关重要的序参量 $m$。

#### 解决方案一：降噪自编码器 (DAE)

**[降噪](@entry_id:144387)自编码器（Denoising Autoencoder, DAE）** 为解决噪声问题提供了一个优雅的方案。 DAE的训练方式略有不同：它接收一个被噪声 $\varepsilon'$ 污染的输入 $\tilde{x} = x + \varepsilon'$，但其任务是重构出**原始的、干净的**输入 $x$。其[损失函数](@entry_id:634569)为 $L = \mathbb{E} \|x - g(f(\tilde{x}))\|^2$。

这种训练方式的有效性可以从两个层面理解。首先，从[统计估计理论](@entry_id:173693)来看，最小化均方误差的最优重构函数 $r(\tilde{x}) = g(f(\tilde{x}))$ 是[条件期望](@entry_id:159140) $\mathbb{E}[x|\tilde{x}]$。这个[条件期望](@entry_id:159140)通过对所有可能产生噪声观测 $\tilde{x}$ 的原始信号 $x$ 进行加权平均，自然地起到了平滑和[降噪](@entry_id:144387)的作用。其次，有一个深刻的理论联系指出，在噪声方差 $\sigma^2$ 很小的情况下，最小化DAE损失函数等价于**[分数匹配](@entry_id:635640)（score matching）**。这指的是学习数据分布的对数概率梯度，即**分数（score）** $\nabla_x \log p(x)$。对于玻尔兹曼分布 $p(x) \propto \exp(-\beta H(x))$，分数正比于系统受到的[广义力](@entry_id:169699) $-\nabla_x H(x)$。学习分数意味着学习将偏离[数据流形](@entry_id:636422)的点“推回”到高概率区域的动力学。因此，DAE被迫学习数据分布的内在几何结构，而不是表面的噪声。 这种机制使得DAE成为一种数据驱动的**[粗粒化](@entry_id:141933)（coarse-graining）**方法，它能有效地滤除快速涨落，从而提取出与[序参量](@entry_id:144819)相关的慢变量。

#### 解决方案二：加权损失函数

另一种对抗噪声的直接方法是修改[损失函数](@entry_id:634569)，使其更加关注宏观结构。 我们可以设计一个加权的[损失函数](@entry_id:634569)，例如：
1.  **宏观惩罚项**：如果我们有一个已知的[宏观可观测量](@entry_id:751601)计算函数 $g(x)$（如磁化强度），我们可以在损失函数中直接加入一项，惩罚重构前后该观测量的不一致性，如 $\beta |g(x) - g(\hat{x})|^2$。
2.  **频域加权**：物理上，宏观结构通常对应于低频分量，而微观噪声对应于高频分量。利用这一点，我们可以在傅里叶空间中定义损失。通过帕萨瓦尔定理，$\|x - \hat{x}\|^2$ 正比于[傅里叶系数](@entry_id:144886)的平方差之和。我们可以引入一个频率依赖的权重 $w(k)$，对低频部分的重构误差给予更大的权重，而对高频部分给予较小的权重。

#### 挑战二：空间结构与局域性

对于定义在[晶格](@entry_id:148274)上的物理系统（如[伊辛模型](@entry_id:139066)），数据具有天然的空间结构，且物理相互作用通常是局域的（short-ranged）。

#### 解决方案三：[卷积自编码器](@entry_id:905501) (CAE)

**[卷积自编码器](@entry_id:905501)（Convolutional Autoencoder, CAE）** 的架构设计与这类系统的物理特性天然契合。
*   **小尺寸[卷积核](@entry_id:1123051)**：CAE使用小尺寸的卷积核在输入数据上滑动，每个输出神经元只依赖于输入的一个小区域（[感受野](@entry_id:636171)）。这与物理学中相互作用的**局域性（locality）**原理相符。
*   **[权重共享](@entry_id:633885)**：在同一卷积层中，所有空间位置共享同一套卷积核权重。这在网络中植入了一个**[平移不变性](@entry_id:195885)（stationarity）**的先验知识，即认为在空间中任何位置，识别同一种模式的规则是相同的。这恰好对应于具有平移不变哈密顿量的物理系统。
*   **[平移等变性](@entry_id:636340)与[不变性](@entry_id:140168)**：[权重共享](@entry_id:633885)使得卷积层具有**[平移等变性](@entry_id:636340)（translational equivariance）**：如果输入平移，输出的[特征图](@entry_id:637719)也相应平移。而[序参量](@entry_id:144819)通常是全局量，应具有**[平移不变性](@entry_id:195885)（translational invariance）**。为了从等变的[特征图](@entry_id:637719)中获得不变的[潜变量](@entry_id:143771)，我们可以在编码器的最后一步使用一个全局聚合操作，例如**[全局平均池化](@entry_id:634018)（global average pooling）**。这个操作会整合所有空间位置的信息，从而消除空间依赖性，产生一个平移不变的标量或矢量作为序参量。

### 概率与信息论视角

除了基于重构误差的观点，我们还可以从更根本的概率和信息论框架来理解自编码器如何识别序参量。

#### [变分自编码器 (VAE)](@entry_id:141132)

**[变分自编码器](@entry_id:177996)（Variational Autoencoder, VAE）** 是一种生成模型，它将自编码器框架与[概率图模型](@entry_id:899342)和[变分推断](@entry_id:634275)结合起来。 VAE的目标是最大化数据边际对数似然 $\log p(x)$ 的一个下界，这等价于最小化一个[损失函数](@entry_id:634569)，通常被称为**[证据下界](@entry_id:634110)（Evidence Lower Bound, ELBO）**的负值：

$$
L(\theta, \phi) = \mathbb{E}_{x \sim \mathcal{D}} \left[ \mathbb{E}_{q_\phi(z|x)} [-\log p_\theta(x|z)] + \mathrm{KL}(q_\phi(z|x) \,\|\, p(z)) \right]
$$

这个损失函数包含两项：
1.  **重构项**：$\mathbb{E}_{q_\phi(z|x)} [-\log p_\theta(x|z)]$，与标准AE的[重构损失](@entry_id:636740)类似，鼓励解码器 $p_\theta(x|z)$ 能够从[潜变量](@entry_id:143771) $z$ 生成与输入 $x$ 相似的数据。
2.  **正则化项**：$\mathrm{KL}(q_\phi(z|x) \,\|\, p(z))$，是一个KL散度项，它衡量了编码器产生的近似后验分布 $q_\phi(z|x)$ 与一个预先设定的**[先验分布](@entry_id:141376)（prior distribution）** $p(z)$ 之间的差异。

KL正则化项是VAE的关键。它迫使编码器学习到的[潜空间](@entry_id:171820)结构趋向于我们设定的[先验分布](@entry_id:141376) $p(z)$。这为我们提供了一个强大的工具来注入关于序参量的物理先验知识。 例如：
*   如果期望[序参量](@entry_id:144819)是单值的，可以选择一个[标准正态分布](@entry_id:184509)作为先验。
*   如果系统存在多个稳定的[宏观态](@entry_id:140003)，我们可以选择一个**[高斯混合模型](@entry_id:634640)（Gaussian Mixture Model, GMM）**作为先验，以鼓励潜变量在潜空间中形成多个聚类。
*   如果[序参量](@entry_id:144819)是周期性的（如[XY模型](@entry_id:140763)中的相角），可以选择一个[von Mises分布](@entry_id:1133904)或其他能体现周期性的分布作为先验。

#### [信息瓶颈](@entry_id:263638)原理

**[信息瓶颈](@entry_id:263638)（Information Bottleneck, IB）**原理提供了另一种不依赖于重构的视角。 IB的目标是学习一个关于输入 $x$ 的压缩表示 $z$，这个 $z$ 在尽可能“忘记” $x$ 的细节的同时，最大限度地保留与某个我们关心的目标变量 $Y$ 相关的信息。这通过最小化一个拉格朗日量来实现：

$$
\mathcal{L}_{IB} = I(x;z) - \beta I(z;Y)
$$

其中 $I(\cdot;\cdot)$ 表示互信息，$\beta$ 是一个控制压缩与相关性之间权衡的参数。根据[数据处理不等式](@entry_id:142686)，我们总是有 $I(z;Y) \le I(x;Y)$。IB的目标就是让 $z$ 在丢弃与 $Y$ 无关的信息（以减小 $I(x;z)$）的同时，尽可能地保留与 $Y$ 相关的信息（以最大化 $I(z;Y)$）。

在识别序参量的语境下，我们可以选择一个已知的[宏观可观测量](@entry_id:751601)（例如，通过[粗粒化](@entry_id:141933)计算得到的量）作为目标变量 $Y$。由于 $Y$ 被设计为反映系统的慢变自由度，IB原理将自然地引导编码器学习一个[潜变量](@entry_id:143771) $z$，它专门编码与 $Y$ 相关的慢变量信息（即序参量），并主动丢弃所有其他微观细节。

### 实践考量与理论边界

最后，成功地应用自编码器不仅需要精巧的模型设计，还需要对数据生成过程和方法的理论局限有深刻的理解。

#### [实验设计](@entry_id:142447)：在[临界点](@entry_id:144653)附近采样

为了让自编码器学习到一个描述相变的序参量，它必须接触到来自相变点两侧以及[临界区](@entry_id:172793)域本身的构型数据。物理学中的**[有限尺寸标度](@entry_id:142952)（finite-size scaling）**理论告诉我们，在一个线性尺寸为 $L$ 的有限系统中，相变的奇异性会在一个围绕[临界温度](@entry_id:146683) $T_c$ 的窗口内被“抹平”。这个窗口的宽度 $\Delta T$ 与系统尺寸 $L$ 之间存在标度关系 $\Delta T \sim L^{-1/\nu}$，其中 $\nu$ 是关联长度的临界指数。 因此，一个有原则的训练数据生成策略是，在这个由[有限尺寸标度](@entry_id:142952)理论确定的临界窗口 $[T_c - \Delta(L), T_c + \Delta(L)]$ 内对称地、均衡地采样。这确保了自编码器能够观察到从有序到无序的完整转变过程，以及[临界点](@entry_id:144653)附近特有的、跨越所有尺度的大幅涨落，从而学习到一个能够刻画整个[相变过程](@entry_id:147919)的[潜变量](@entry_id:143771)。

#### 与[重整化群](@entry_id:147717)的联系

自编码器所执行的维度约减，与理论物理的基石——**[重整化群](@entry_id:147717)（Renormalization Group, RG）**思想有着深刻的联系。 RG研究的是物理系统在不同尺度下的行为以及物理量如何随尺度变化。一个真正的序参量（或任何“慢”的、相关的可观测量）在[粗粒化](@entry_id:141933)操作下应该是相对稳定的。我们可以定义一个**块自旋（block-spin）**变换，例如将一小块区域内的自旋取平均值，作为一次[粗粒化](@entry_id:141933)操作 $\mathcal{B}_b$。一个好的潜变量 $z(x)$ 应该近似满足在[粗粒化](@entry_id:141933)操作下的[不变性](@entry_id:140168)，即 $z(x) \approx z(\mathcal{B}_b x)$。通过分析可以发现，像总磁化强度这样的全局[守恒量](@entry_id:161475)是严格不变的，而一些基于低频傅里叶模式并经过恰当归一化的量也近似不变。相反，像最近邻关联函数这类依赖于短程细节的量，在[粗粒化](@entry_id:141933)后会发生显著改变。这个视角为我们从RG的观点检验和设计自编码器的[潜变量](@entry_id:143771)提供了理论指导。

#### 局域与非局域序参量的局限性

最后，必须认识到，自编码器的架构必须与待求序参量的物理性质相匹配。我们可以对比两种性质截然不同的序参量：
1.  **局域（或可加）序参量**：如[伊辛模型](@entry_id:139066)中的磁化强度 $m = \frac{1}{N}\sum_i s_i$。虽然 $m$ 是一个全局量，但它是一个局域量的简单加和。在有序相中，任何一个局部区域的平均磁化强度都与全局磁化强度高度相关。因此，一个具有局域[感受野](@entry_id:636171)的[卷积自编码器](@entry_id:905501)（CAE）能够通过观察局部斑块的统计特性，成功地推断并编码全局的磁化序参量。
2.  **非局域（拓扑）序参量**：如在环面上定义的$\mathbb{Z}_2$[拓扑序](@entry_id:147345)。其序参量由跨越整个系统的非平凡路径（Wilson loop）上的算符乘积定义。[拓扑序](@entry_id:147345)的一个关键特征是，处于不同拓扑扇区的基态在任何**局域**尺度上都是不可区分的。

对于第二种情况，一个仅具有局域[感受野](@entry_id:636171)（$R \ll L$）且没有全局信息整合机制（如全局池化）的CAE，将根本无法区分不同的拓扑态。 这是因为网络能接触到的所有局部信息在不同拓扑态之间都是完全相同的，驱动网络学习的[重构损失](@entry_id:636740)梯度将不包含任何关于拓扑扇区的信息。这揭示了一个深刻的限制：**[网络架构](@entry_id:268981)的局域性决定了它只能学习那些在局域有所体现的[序参量](@entry_id:144819)**。要学习真正的非局域、[拓扑序](@entry_id:147345)参量，必须采用具有全局信息处理能力的架构，或者在[损失函数](@entry_id:634569)中明确地引入非局域的监督信号。