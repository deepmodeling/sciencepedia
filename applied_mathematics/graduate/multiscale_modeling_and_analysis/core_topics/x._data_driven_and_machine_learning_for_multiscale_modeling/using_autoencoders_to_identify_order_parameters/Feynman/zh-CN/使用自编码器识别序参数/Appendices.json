{
    "hands_on_practices": [
        {
            "introduction": "当您怀疑自编码器学到的某个潜变量对应一个已知的物理序参量时，我们需要一个严谨的方法来验证这种对应关系。本练习将引导您实践一种基于典型相关分析（Canonical Correlation Analysis）的统计检验方法，用以量化所学变量与已知序参量之间的线性关系。通过实施该检验，您将学会如何从定性的视觉检查，迈向对模型发现进行定量验证的严谨步骤。",
            "id": "3828675",
            "problem": "考虑一个包含$N$个样本的数据集，这些样本索引了一个多尺度系统的微观状态$\\{x_i\\}_{i=1}^N$。一个在该数据集上训练的自编码器（AE）为每个样本$i$生成一个一维潜变量$z_i \\in \\mathbb{R}$，并且对于这些相同的样本，还有一个已知的宏观序参量（OP）$m_i \\in \\mathbb{R}$。典型相关分析（CCA）旨在寻找能够最大化两组变量之间相关性的线性组合。一般而言，对于集合$X \\in \\mathbb{R}^p$和$Y \\in \\mathbb{R}^q$，CCA求解$\\max_{\\alpha \\in \\mathbb{R}^p, \\beta \\in \\mathbb{R}^q} \\mathrm{corr}(\\alpha^\\top X, \\beta^\\top Y)$。当两组变量都是一维时，即$p=q=1$，CCA简化为标准化标量$z$和$m$之间的绝对Pearson相关性。要求你形式化一个决策准则，用以检验AE潜变量$z$是否对应于已知的OP $m$，并将其实现为一个程序。\n\n你的形式化决策准则必须基于以下步骤：计算数据集上$z$和$m$之间的样本Pearson相关系数$r$，在真实相关性为零的原假设下评估其统计显著性，然后对典型相关性的大小应用一个阈值。具体来说，定义样本Pearson相关系数为\n$$\nr = \\frac{\\sum_{i=1}^N (z_i - \\bar{z})(m_i - \\bar{m})}{\\sqrt{\\sum_{i=1}^N (z_i - \\bar{z})^2} \\sqrt{\\sum_{i=1}^N (m_i - \\bar{m})^2}},\n$$\n其中$\\bar{z} = \\frac{1}{N}\\sum_{i=1}^N z_i$且$\\bar{m} = \\frac{1}{N}\\sum_{i=1}^N m_i$。在原假设$H_0: \\rho = 0$（其中$\\rho$是总体相关性）下，并假设样本独立同分布且方差有限，统计量\n$$\nt = r \\sqrt{\\frac{N - 2}{1 - r^2}}\n$$\n服从自由度为$N - 2$的Student $t$分布。双边$p$值为\n$$\np = 2 \\left(1 - F_{t_{N-2}}(|t|)\\right),\n$$\n其中$F_{t_{N-2}}$是自由度为$N - 2$的Student $t$分布的累积分布函数。设决策阈值为相关性大小$\\tau \\in [0,1]$和显著性水平$\\alpha \\in (0,1)$。你的决策规则必须是：\n- 如果$\\mathrm{Var}(z) = 0$或$\\mathrm{Var}(m) = 0$，则判定$z$不对应于$m$。\n- 否则，按上述方法计算$r$和$p$，并且当且仅当$|r| \\ge \\tau$且$p \\le \\alpha$时，判定$z$对应于$m$。\n\n对于任何三角函数，角度都必须以弧度为单位处理。程序必须实现此准则，并评估以下确定性案例的测试套件，每个案例由通过关于索引$i$的显式公式构造的$(z, m)$对定义：\n\n- 案例1（一般成功路径）：$N = 200$, $m_i = \\sin\\left(\\frac{2\\pi i}{N}\\right)$, $z_i = 1.1 \\, m_i$。使用$\\tau = 0.8$和$\\alpha = 0.01$。角度单位是弧度。预期行为：强正典型相关。\n- 案例2（正交性边界情况）：$N = 200$, $m_i = \\sin\\left(\\frac{2\\pi i}{N}\\right)$, $z_i = \\cos\\left(\\frac{2\\pi i}{N}\\right)$。使用$\\tau = 0.8$和$\\alpha = 0.01$。角度单位是弧度。预期行为：可忽略的典型相关。\n- 案例3（负相关一般情况）：$N = 150$， $m_i$ 线性间隔为 $m_i = -1 + \\frac{2(i-1)}{N-1}$，$z_i = - m_i + 0.05 \\, \\sin\\left(\\frac{4\\pi i}{N}\\right)$。使用$\\tau = 0.8$和$\\alpha = 0.01$。角度单位是弧度。预期行为：强负典型相关。\n- 案例4（退化方差边界）：$N = 100$, $m_i = 0$ 对所有 $i$ 成立，$z_i = \\cos\\left(\\frac{2\\pi i}{N}\\right)$。使用$\\tau = 0.8$和$\\alpha = 0.01$。角度单位是弧度。预期行为：由于$m$的方差为零导致相关性未定义，必须视为拒绝。\n- 案例5（小样本中等相关，显著性边缘）：$N = 20$，$m_i = -1 + \\frac{2(i-1)}{N-1}$，$z_i = 0.4 \\, m_i + 0.9 \\, \\sin\\left(\\frac{2\\pi i}{N}\\right)$。使用$\\tau = 0.8$和$\\alpha = 0.01$。角度单位是弧度。预期行为：相关性大小低于阈值，并且在给定的$\\alpha$水平下很可能不具有统计显著性。\n\n你的程序必须产生单行输出，其中包含五个案例的决策规则结果，格式为方括号括起来的、用逗号分隔的布尔值列表，顺序与上面列出的案例一致（例如，$[结果_1,结果_2,结果_3,结果_4,结果_5]$）。输出值必须是布尔类型，并且不得打印任何额外文本。",
            "solution": "该问题要求形式化并实现一个决策准则，以评估由自编码器从一组微观状态中学到的一维潜变量$z$是否对应于已知的宏观序参量$m$。这种对应关系基于两个变量$z$和$m$之间线性关系的强度和统计显著性进行评估。\n\n决策准则的核心在于样本Pearson相关系数$r$及相关的假设检验。对于两组配对数据$\\{z_i\\}_{i=1}^N$和$\\{m_i\\}_{i=1}^N$，样本Pearson相关系数由下式给出：\n$$\nr = \\frac{\\sum_{i=1}^N (z_i - \\bar{z})(m_i - \\bar{m})}{\\sqrt{\\sum_{i=1}^N (z_i - \\bar{z})^2} \\sqrt{\\sum_{i=1}^N (m_i - \\bar{m})^2}}\n$$\n其中$\\bar{z} = \\frac{1}{N}\\sum_{i=1}^N z_i$和$\\bar{m} = \\frac{1}{N}\\sum_{i=1}^N m_i$是样本均值。在两个一维变量之间的典型相关分析（CCA）的背景下，最大化的相关性就是Pearson系数的绝对值$|r|$。\n\n该决策准则是一个包含指定阈值的两部分检验：最小相关性大小$\\tau$和统计显著性水平$\\alpha$。\n\n步骤1：前提条件验证。\n如果任一变量的方差为零，则Pearson相关系数是未定义的。方差为零的变量是一个常数，不能与另一个变量协变。因此，该准则的第一步是检查样本方差$\\mathrm{Var}(z)$和$\\mathrm{Var}(m)$。如果$\\mathrm{Var}(z) = 0$或$\\mathrm{Var}(m) = 0$，则这两个变量不能有意义地相关。在这种情况下，我们判定$z$不对应于$m$。在数值实现中，必须使用一个小的容差来执行此检查，以考虑浮点运算的限制。\n\n步骤2：相关性大小检验。\n如果两个变量的方差都非零，我们继续计算$r$。决策规则的第一部分评估相关的强度。只有当潜变量$z$与序参量$m$之间线性关系的大小足够大时，才认为$z$是$m$的一个潜在表示。这被形式化为条件：\n$$\n|r| \\ge \\tau\n$$\n如果不满足此条件，我们判定对应关系太弱，该准则不通过。\n\n步骤3：统计显著性检验。\n一个大的样本相关性$r$可能偶然发生，尤其是在小数据集中。为防止这种情况，我们检验观察到的相关性的统计显著性。我们构建一个原假设$H_0$，即样本所来自的潜在总体中没有相关性，也就是说，总体相关系数$\\rho$为零（$H_0: \\rho = 0$）。\n\n在样本$(z_i, m_i)$独立同分布于一个二元正态分布（或对于大的$N$值，根据中心极限定理）的假设下，检验统计量\n$$\nt = r \\sqrt{\\frac{N - 2}{1 - r^2}}\n$$\n服从自由度为$N-2$的Student's $t$分布。一个大的$t$绝对值提供了反对原假设的证据。\n\n根据计算出的$t$统计量，我们确定双边$p$值，它是在原假设为真的情况下，观察到至少与$r$一样极端的相关性的概率。$p$值由下式给出：\n$$\np_{\\text{value}} = 2 \\left(1 - F_{t_{N-2}}(|t|)\\right)\n$$\n其中$F_{t_{N-2}}$是自由度为$N-2$的Student's $t$分布的累积分布函数（CDF）。如果该$p$值小于或等于指定的显著性水平$\\alpha$，则认为相关性具有统计显著性：\n$$\np_{\\text{value}} \\le \\alpha\n$$\n\n当$|r|=1$时会出现一种特殊情况。$t$统计量的公式会涉及除以零。然而，完美相关（$|r|=1$）是可能的最大效应量，意味着观察到更极端结果的概率为零。因此，对于$|r|=1$，$p$值为0，对于任何$\\alpha > 0$，显著性检验总是通过。\n\n最终决策规则：\n综合这些步骤，当且仅当所有三个条件都满足时，潜变量$z$才被判定为对应于序参量$m$：\n1. $\\mathrm{Var}(z) > 0$ 且 $\\mathrm{Var}(m) > 0$。\n2. 相关性大小足够：$|r| \\ge \\tau$。\n3. 相关性具有统计显著性：$p_{\\text{value}} \\le \\alpha$。\n\n这个流程将被实现并应用于五个指定的测试案例。在实现中，我们将利用数值库来确保方差、相关系数和Student's $t$分布CDF的稳健计算。\n- 案例1：$z_i = 1.1 m_i$，所以$r=1$。$|r| = 1 \\ge 0.8$且$p_{\\text{value}} = 0 \\le 0.01$。准则通过。\n- 案例2：$m_i$和$z_i$是在一个完整周期上的正交正弦和余弦函数。它们的样本相关性$r$将接近于0。条件$|r| \\ge 0.8$将不满足。\n- 案例3：$z_i$是$m_i$的强反相关版本，带有轻微噪声。我们预期$r$接近-1。因此，$|r| \\approx 1 \\ge 0.8$。由于$N=150$，这种强相关性将是高度显著的，所以$p_{\\text{value}} \\ll 0.01$。准则通过。\n- 案例4：$m_i$是一个常数向量，所以$\\mathrm{Var}(m) = 0$。前提条件不满足，准则立即返回否定结果。\n- 案例5：预期存在中等相关性，但样本量$N=20$很小，且噪声项很大。最终的相关性大小$|r|$不太可能达到$\\tau = 0.8$的高阈值。此外，即使达到了，小样本量也可能导致结果在严格的$\\alpha=0.01$水平下不具有统计显著性。预期准则不通过。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t as student_t\n\ndef solve():\n    \"\"\"\n    Main function to evaluate the decision criterion for all test cases.\n    \"\"\"\n\n    def decision_criterion(z, m, N, tau, alpha):\n        \"\"\"\n        Applies the formal decision criterion to determine if latent variable z\n        corresponds to order parameter m.\n\n        Args:\n            z (np.ndarray): The latent variable vector.\n            m (np.ndarray): The order parameter vector.\n            N (int): The number of samples.\n            tau (float): The correlation magnitude threshold.\n            alpha (float): The significance level.\n\n        Returns:\n            bool: True if z corresponds to m, False otherwise.\n        \"\"\"\n        # Step 1: Pre-condition Validation (non-zero variance)\n        # Using a small tolerance for floating-point comparison.\n        if np.var(m)  1e-15 or np.var(z)  1e-15:\n            return False\n\n        # Compute sample Pearson correlation coefficient r\n        # np.corrcoef returns a 2x2 matrix, the value is at [0, 1] or [1, 0]\n        r = np.corrcoef(z, m)[0, 1]\n\n        # Step 2: Correlation Magnitude Test\n        if abs(r)  tau:\n            return False\n\n        # Step 3: Statistical Significance Test\n        # Handle the edge case of perfect correlation to avoid division by zero.\n        # If |r| is extremely close to 1, the p-value is effectively 0.\n        if abs(r) > 1.0 - 1e-9:\n            p_val = 0.0\n        else:\n            # Calculate the t-statistic\n            t_stat = r * np.sqrt((N - 2) / (1 - r**2))\n            \n            # Degrees of freedom\n            df = N - 2\n            \n            # Calculate the two-sided p-value using the Student's t-distribution CDF\n            p_val = 2 * (1 - student_t.cdf(abs(t_stat), df))\n\n        # Final decision: True if p-value is below significance level\n        return p_val = alpha\n\n    # Define the test cases from the problem statement.\n    # Each tuple: (N, m_func, z_func, tau, alpha)\n    test_cases = [\n        (200, lambda i, N: np.sin(2 * np.pi * i / N), lambda i, N: 1.1 * np.sin(2 * np.pi * i / N), 0.8, 0.01),\n        (200, lambda i, N: np.sin(2 * np.pi * i / N), lambda i, N: np.cos(2 * np.pi * i / N), 0.8, 0.01),\n        (150, lambda i, N: -1 + 2 * (i - 1) / (N - 1), lambda i, N: -(-1 + 2 * (i - 1) / (N - 1)) + 0.05 * np.sin(4 * np.pi * i / N), 0.8, 0.01),\n        (100, lambda i, N: np.zeros_like(i, dtype=float), lambda i, N: np.cos(2 * np.pi * i / N), 0.8, 0.01),\n        (20, lambda i, N: -1 + 2 * (i - 1) / (N - 1), lambda i, N: 0.4 * (-1 + 2 * (i - 1) / (N - 1)) + 0.9 * np.sin(2 * np.pi * i / N), 0.8, 0.01),\n    ]\n\n    results = []\n    for case in test_cases:\n        N, m_func, z_func, tau, alpha = case\n        \n        # The problem states index i from 1 to N\n        i_indices = np.arange(1, N + 1)\n        \n        m_vec = m_func(i_indices, N)\n        z_vec = z_func(i_indices, N)\n        \n        result = decision_criterion(z_vec, m_vec, N, tau, alpha)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # Python's str() for booleans is 'True'/'False' (capitalized).\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "一个性能优越的自编码器不应仅仅满足于数据的压缩与解压，其学到的表示更应尊重控制系统的基本物理定律。本练习将指导您实施一项至关重要的物理一致性检查：验证诸如总质量和能量等全局不变量的守恒性。通过量化重构数据中这些守恒量的偏差，您将构建一个“物理性得分”，用以评估潜变量空间是否成功捕捉了系统的核心物理特性。",
            "id": "3828693",
            "problem": "给定一个在区间 $[0,2\\pi)$ 上的一维周期性标量场 $u(x)$，该场被均匀离散化为 $N$ 个网格点，网格间距为 $\\Delta x = \\frac{2\\pi}{N}$。假设使用一个自编码器将 $u(x)$ 压缩成一个潜变量，然后解码重构为 $\\hat{u}(x)$。在多尺度建模与分析中，有意义的潜变量（序参量）应当编码那些遵循全局守恒律的物理相关特征。为了评估这一点，请实现一个一致性检验，量化解码器的重构是否保持了预设的全局不变量。请使用以下基本依据：\n\n- 对于类平流或哈密顿动力学占主导地位且耗散可忽略不计的周期性系统，总质量和二次能量等全局不变量是守恒的。将质量定义为 $I_1[u] = \\int_0^{2\\pi} u(x)\\,dx$，将二次能量定义为 $I_2[u] = \\int_0^{2\\pi} u(x)^2\\,dx$。\n- 在间距为 $\\Delta x$ 的均匀网格上，用黎曼和来近似这些积分，即 $I_1[u] \\approx \\Delta x \\sum_{j=0}^{N-1} u_j$ 和 $I_2[u] \\approx \\Delta x \\sum_{j=0}^{N-1} u_j^2$，其中 $u_j = u(x_j)$ 且 $x_j = j\\Delta x$。\n- 守恒性检验比较原始场 $u$ 和重构场 $\\hat{u}$ 的不变量。令 $\\Delta_k = I_k[\\hat{u}] - I_k[u]$，其中 $k \\in \\{1,2\\}$。\n\n通过用具有物理意义的尺度对绝对差进行归一化，为每个不变量定义一个无量纲偏差：\n- 对于质量，使用 $d_1 = \\frac{|\\Delta_1|}{\\max\\left(|I_1[u]|, \\|u\\|_{L^1}\\right)}$，其中 $\\|u\\|_{L^1} \\approx \\Delta x \\sum_{j=0}^{N-1} |u_j|$。\n- 对于能量，使用 $d_2 = \\frac{|\\Delta_2|}{\\max\\left(|I_2[u]|, \\epsilon\\right)}$，其中 $\\epsilon$ 是任意正下界，以避免在 $I_2[u]=0$ 时出现除以零的情况。在本问题中，测试套件会避免 $I_2[u]=0$ 的情况，因此您可以将 $\\epsilon$ 设置为 $0$。\n\n将这些偏差汇总成一个标量潜物理性分数\n$$\nS = \\exp\\left(-\\frac{d_1 + d_2}{2}\\right),\n$$\n使得 $S \\in (0,1]$，$S=1$ 表示精确守恒，且 $S$ 随着偏差的增加而衰减。\n\n实现一个程序，该程序：\n- 在 $[0,2\\pi)$ 上使用 $N=512$ 个均匀间隔的点（角度单位为弧度）构建基准场 $u_{\\text{base}}(x) = \\sin(x) + 0.5\\cos(2x)$。\n- 使用以下四个测试案例来定义从 $u_{\\text{base}}$ 重构的 $\\hat{u}$：\n    1. 案例 A（理想情况）：精确重构 $\\hat{u}(x) = u_{\\text{base}}(x)$。\n    2. 案例 B（谱高斯模糊）：在傅里叶空间中，对 $u_{\\text{base}}$ 应用标准差为 $\\sigma = 0.2$（在 $x$ 坐标中）的高斯滤波器，然后逆变换到实空间。这应该会保持零波数模式，从而保持质量，同时减少高频能量。\n    3. 案例 C（振幅缩放）：$\\hat{u}(x) = s \\, u_{\\text{base}}(x)$，其中 $s = 1.1$。\n    4. 案例 D（加性偏置）：$\\hat{u}(x) = u_{\\text{base}}(x) + b$，其中 $b = 0.05$。\n\n对于每个案例，计算 $I_1[u]$、$I_2[u]$、$I_1[\\hat{u}]$、$I_2[\\hat{u}]$、如上定义的偏差 $d_1$ 和 $d_2$ 以及标量分数 $S$。角度单位为弧度。在本问题中，所有量都是无量纲的。\n\n您的程序应生成单行输出，其中包含案例 A、B、C 和 D 的四个分数 $S$，每个分数四舍五入到六位小数，并以方括号括起来的逗号分隔列表形式呈现（例如，“[0.999999,0.912345,0.812345,0.456789]”）。\n\n需要实现的测试套件详情：\n- 域长度 $L = 2\\pi$。\n- 网格点数 $N = 512$。\n- 高斯模糊参数 $\\sigma = 0.2$（以 $x$ 为单位）。\n- 缩放因子 $s = 1.1$。\n- 加性偏置 $b = 0.05$。\n\n最终输出是如上指定的包含四个浮点数的列表。无需用户输入。",
            "solution": "该问题要求实现一个物理性分数 $S$，以评估一个重构场 $\\hat{u}(x)$ 是否保持了原始场 $u(x)$ 的关键全局不变量。该分数基于在一维周期性标量场（定义域为 $[0, 2\\pi)$）上总质量 $I_1[u]$ 和总二次能量 $I_2[u]$ 的守恒性。解决方案按以下步骤进行：定义离散系统，计算基准场及其不变量，生成四个不同的重构场，并为每个重构场计算基于偏差的分数。\n\n首先，我们建立计算域。场 $u(x)$ 在一个由 $N=512$ 个点组成的均匀网格上进行离散化，记为 $x_j = j \\Delta x$，其中 $j \\in \\{0, 1, \\dots, N-1\\}$。网格间距为 $\\Delta x = \\frac{2\\pi}{N}$。基准场由函数 $u_{\\text{base}}(x) = \\sin(x) + 0.5\\cos(2x)$ 给出。我们在此离散网格上评估此函数，得到向量 $u_{\\text{base},j} = u_{\\text{base}}(x_j)$。\n\n接下来，我们定义全局不变量的数值近似。总质量 $I_1[u] = \\int_0^{2\\pi} u(x)\\,dx$ 和总二次能量 $I_2[u] = \\int_0^{2\\pi} u(x)^2\\,dx$ 使用黎曼和进行近似：\n$$I_1[u] \\approx \\Delta x \\sum_{j=0}^{N-1} u_j$$\n$$I_2[u] \\approx \\Delta x \\sum_{j=0}^{N-1} u_j^2$$\n类似地，归一化所需的 $L^1$-范数 $\\|u\\|_{L^1} = \\int_0^{2\\pi} |u(x)|\\,dx$ 近似为：\n$$\\|u\\|_{L^1} \\approx \\Delta x \\sum_{j=0}^{N-1} |u_j|$$\n使用这些公式，我们为离散基准场计算不变量 $I_1[u_{\\text{base}}]$、$I_2[u_{\\text{base}}]$ 以及范数 $\\|u_{\\text{base}}\\|_{L^1}$。从解析上看，$I_1[u_{\\text{base}}] = 0$，而 $I_2[u_{\\text{base}}] = 1.25\\pi$。我们的数值计算将得到非常接近这些值的结果。\n\n问题的核心在于为四个从 $u_{\\text{base}}(x)$ 派生出的不同重构场 $\\hat{u}(x)$ 评估物理性分数。\n\n案例 A（精确重构）：$\\hat{u}(x) = u_{\\text{base}}(x)$。在这个平凡的案例中，重构是完美的。\n\n案例 B（谱高斯模糊）：这种重构模拟了一种常见的信息损失形式，即高频细节被衰减。这在傅里叶空间中实现。首先，我们计算基准场的离散傅里叶变换 (DFT)，$\\tilde{u}_{\\text{base}} = \\mathcal{F}\\{u_{\\text{base}}\\}$。相应的角波数由向量 $k$ 给出，其中 $k_m = 2\\pi f_m$，$f_m$ 是标准 FFT 频率函数为长度为 $N$、采样间距为 $\\Delta x$ 的信号所提供的离散频率。然后我们定义傅里叶域中的高斯滤波器：\n$$G(k) = \\exp\\left(-\\frac{k^2 \\sigma^2}{2}\\right)$$\n其中给定的实空间标准差为 $\\sigma = 0.2$。傅里叶空间中经过滤波的场为 $\\tilde{\\hat{u}} = \\tilde{u}_{\\text{base}} \\odot G$，其中 $\\odot$ 表示逐元素相乘。然后通过应用逆离散傅里叶变换 (IDFT) 得到重构场 $\\hat{u}(x)$，即 $\\hat{u} = \\text{Re}(\\mathcal{F}^{-1}\\{\\tilde{\\hat{u}}\\})$。取实部是为了舍弃由数值精度误差产生的可忽略的虚部。\n\n案例 C（振幅缩放）：此案例模拟一个简单的增益误差，其中重构场被均匀缩放：$\\hat{u}(x) = s \\cdot u_{\\text{base}}(x)$，缩放因子为 $s = 1.1$。\n\n案例 D（加性偏置）：此案例模拟一个恒定偏移误差：$\\hat{u}(x) = u_{\\text{base}}(x) + b$，偏置为 $b = 0.05$。\n\n对于这四个案例中的每一个，我们计算物理性分数 $S$。首先，我们计算重构场的不变量 $I_1[\\hat{u}]$ 和 $I_2[\\hat{u}]$。然后，我们计算与基准场不变量的差值：$\\Delta_1 = I_1[\\hat{u}] - I_1[u_{\\text{base}}]$ 和 $\\Delta_2 = I_2[\\hat{u}] - I_2[u_{\\text{base}}]$。\n\n通过归一化，这些差值被转换为无量纲偏差 $d_1$ 和 $d_2$：\n$$d_1 = \\frac{|\\Delta_1|}{\\max\\left(|I_1[u_{\\text{base}}]|, \\|u_{\\text{base}}\\|_{L^1}\\right)}$$\n$$d_2 = \\frac{|\\Delta_2|}{|I_2[u_{\\text{base}}]|}$$\n$d_1$ 的分母能稳健地处理场均值为零的情况，这对于 $u_{\\text{base}}$ 是成立的。$d_2$ 的分母是安全的，因为测试套件避免了 $I_2[u] = 0$ 的情况，所以 $I_2[u_{\\text{base}}]$ 严格为正。\n\n最后，通过汇总偏差计算标量物理性分数 $S$：\n$$S = \\exp\\left(-\\frac{d_1 + d_2}{2}\\right)$$\n该分数将两个偏差映射到区间 $(0, 1]$ 上，其中 $S=1$ 表示两个不变量都完美守恒，且分数会随着守恒误差的增大而减小。对四个案例中的每一个都重复此过程，并将得到的分数格式化为列表。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes a physicality score for four test cases of field reconstructions\n    based on the conservation of mass and energy invariants.\n    \"\"\"\n    # Define problem parameters\n    N = 512\n    L = 2.0 * np.pi\n    dx = L / N\n    sigma = 0.2\n    s = 1.1\n    b = 0.05\n\n    # Construct the grid and base field\n    x = np.linspace(0.0, L, N, endpoint=False)\n    u_base = np.sin(x) + 0.5 * np.cos(2.0 * x)\n\n    # --- Define test cases for reconstructions ---\n    test_cases = []\n\n    # Case A: Exact reconstruction\n    u_hat_A = u_base\n    test_cases.append(u_hat_A)\n\n    # Case B: Spectral Gaussian blur\n    # Compute Fourier transform and corresponding wavenumbers\n    u_base_tilde = np.fft.fft(u_base)\n    # Frequencies in cycles per unit of sample spacing\n    # `d=dx` means the unit is physical length\n    freq = np.fft.fftfreq(N, d=dx)\n    # Convert to angular wavenumbers (rad/length)\n    k = 2.0 * np.pi * freq\n    # Define Gaussian filter in Fourier space\n    gaussian_filter = np.exp(-(k**2 * sigma**2) / 2.0)\n    # Apply filter and inverse transform, taking real part to remove numerical noise\n    u_hat_tilde_B = u_base_tilde * gaussian_filter\n    u_hat_B = np.fft.ifft(u_hat_tilde_B).real\n    test_cases.append(u_hat_B)\n\n    # Case C: Amplitude scaling\n    u_hat_C = s * u_base\n    test_cases.append(u_hat_C)\n\n    # Case D: Additive bias\n    u_hat_D = u_base + b\n    test_cases.append(u_hat_D)\n\n    # --- Calculate invariants for the base field ---\n    # Riemann sum approximation of the integral\n    I1_base = np.sum(u_base) * dx\n    I2_base = np.sum(u_base**2) * dx\n    L1_norm_base = np.sum(np.abs(u_base)) * dx\n\n    scores = []\n    # --- Process each test case to calculate its score ---\n    for u_hat in test_cases:\n        # Calculate invariants for the reconstructed field\n        I1_hat = np.sum(u_hat) * dx\n        I2_hat = np.sum(u_hat**2) * dx\n\n        # Calculate differences and dimensionless deviations\n        delta_1 = I1_hat - I1_base\n        delta_2 = I2_hat - I2_base\n\n        # Normalization for d1, robust to I1_base being near zero\n        d1_denom = np.maximum(np.abs(I1_base), L1_norm_base)\n        d1 = np.abs(delta_1) / d1_denom if d1_denom != 0 else 0.0\n\n        # Normalization for d2\n        # Problem statement guarantees I2_base is not zero for the given test suite\n        d2 = np.abs(delta_2) / np.abs(I2_base)\n        \n        # Calculate the final physicality score\n        score = np.exp(-(d1 + d2) / 2.0)\n        scores.append(score)\n\n    # Format the output as a comma-separated list with 6 decimal places\n    formatted_scores = [f\"{score:.6f}\" for score in scores]\n    print(f\"[{','.join(formatted_scores)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "使用自编码器的真正威力在于发现那些先前未知的序参量，这需要我们深入分析潜空间的内在结构与几何形态。本项高级练习将向您介绍拓扑数据分析（Topological Data Analysis, TDA）及其核心工具——持续同调（persistent homology），这是一种能够在潜变量点云中识别如环状或空洞等非平凡特征的强大技术。通过编写算法来检测这些拓扑印记，您将学习如何揭示宏观状态所处的潜在流形，从而洞察相与相变的本质。",
            "id": "3828633",
            "problem": "给定您的是合成的潜在点云，这些点云代表了由自编码器生成的低维编码。该自编码器经过训练，旨在从微观构型中捕捉宏观序参量。目标是通过计算潜在点云的持续同调，验证与不同相相关联的非平凡拓扑特征的存在，并在多尺度框架内解释这些特征对宏观行为的意义。\n\n从以下基本定义和事实开始：\n- 序参量是一个函数，它将微观状态映射到区分不同相的宏观描述符。当自编码器学习到一种保留了显著宏观结构的潜在表示时，其潜在编码可能携带相的拓扑特征。\n- 在点云 $X \\subset \\mathbb{R}^d$ 上，尺度为 $r$ 的 Vietoris–Rips 复形是一个抽象单纯复形，其顶点是 $X$ 中的点，其 $k$-单纯形是由 $k+1$ 个点组成的集合，这些点之间的两两欧几里得距离小于或等于 $r$。\n- 贝蒂数 $\\beta_k$ 计算拓扑空间中 $k$ 维孔洞的数量：$\\beta_0$ 计算连通分支的数量，$\\beta_1$ 计算一维环（圈）的数量。\n- 持续同调追踪同调特征随过滤参数 $r$ 变化的“生”与“灭”，通过量化它们的生命周期来区分信号与噪声。\n\n您的任务是，从第一性原理出发，实现一个算法，该算法能够：\n- 使用欧几里得度量在潜在点云上构建 Vietoris–Rips 过滤。\n- 随着 $r$ 的增加，使用并查集结构处理出现的边，计算连通分支的数量 $\\beta_0$。\n- 在每个尺度 $r$ 上，通过正确计算由两两邻近性构建的团复形中的边和填充三角形，计算一维环的数量 $\\beta_1$，并使用一个有原则的变换将计数数据转换为 $\\beta_1$。当所考虑的潜在数据中高维同调可以忽略不计时，该变换与旗复形的欧拉示性数一致。\n- 通过从离散过滤尺度上 $\\beta_1$ 的变化中提取其生灭区间，来近似一维持续同调，并通过任何 $\\beta_1$ 特征的最大生命周期来总结每个数据集。\n\n任何参数化构建中的角度单位必须是弧度。不涉及物理单位。所有数值输出必须是实数。最终结果必须表示为浮点数。\n\n实现一个程序，生成以下潜在点云（每个都在 $\\mathbb{R}^2$ 中）：\n- 情况 R (单环): $N = 50$ 个点，从以 $(0,0)$ 为中心、半径为 $1$ 的圆上采样，每个坐标添加标准差为 $\\sigma = 0.02$ 的独立高斯噪声。角度在 $[0,2\\pi)$ 弧度范围内均匀采样。\n- 情况 B (斑点): $N = 50$ 个点，从均值为零、每个坐标标准差为 $0.4$ 的各向同性高斯分布中采样。\n- 情况 2R (两个不相交的环): $N = 80$ 个点，其中 $40$ 个点在以 $(-2, 0)$ 为中心、半径为 $1$ 的圆上， $40$ 个点在以 $(2, 0)$ 为中心、半径为 $1$ 的圆上；每个坐标添加标准差为 $\\sigma = 0.02$ 的独立高斯噪声；角度在 $[0,2\\pi)$ 弧度范围内均匀采样。\n- 情况 NR (含噪声的环): $N = 50$ 个点，从以 $(0,0)$ 为中心、半径为 $1$ 的圆上采样，每个坐标添加标准差为 $\\sigma = 0.12$ 的独立高斯噪声；角度在 $[0,2\\pi)$ 弧度范围内均匀采样。\n- 情况 SR (稀疏环): $N = 20$ 个点，从以 $(0,0)$ 为中心、半径为 $1$ 的圆上采样，每个坐标添加标准差为 $\\sigma = 0.02$ 的独立高斯噪声；角度在 $[0,2\\pi)$ 弧度范围内均匀采样。\n\n在每个数据集上，计算在 $r \\in \\{ r_1, r_2, \\dots, r_M \\}$ 上的 Vietoris–Rips 过滤，其中 $M = 40$ 且 $r_m$ 在 $[0.02, 0.6]$ 区间内线性分布。在每个尺度 $r_m$ 上，计算与由两两距离小于或等于 $r_m$ 的边形成的团复形一致的 $\\beta_0(r_m)$ 和 $\\beta_1(r_m)$，并在所有三个对应边都存在时填充 $2$-单纯形（三角形）。从序列 $\\{ \\beta_1(r_m) \\}_{m=1}^M$ 中，将 $\\beta_1$ 的每次增加视为在该 $r_m$ 处的“生”，每次减少视为在该 $r_m$ 处的“灭”，并以尊重过滤顺序的方式将它们配对，从而提取一维特征的近似生灭区间。对于在最大尺度上任何剩余的“生”，记录其在 $r = 0.6$ 处“灭”。\n\n对于每个数据集，报告一个浮点数，该浮点数等于提取的 $\\beta_1$ 区间中的最大生命周期。您的程序应生成单行输出，其中包含这五个浮点数，格式为逗号分隔的列表并用方括号括起来（例如，\"[x_R,x_B,x_2R,x_NR,x_SR]\"）。\n\n测试套件设计：\n- 不同相的覆盖范围：情况 R 应表现出一个具有相当长生命周期的非平凡环（$\\beta_1$ 特征），情况 B 不应有，情况 2R 应表现出两个环，因此至少有一个长生命周期，情况 NR 应保留一个环，但其生命周期相对于情况 R 会缩短，情况 SR 由于欠采样会挑战检测能力并产生短生命周期。\n- 边界条件：最小尺度 $r_1 = 0.02$ 应产生孤立的点（大的 $\\beta_0$，零 $\\beta_1$），而最大尺度 $r_M = 0.6$ 应产生高度连接的复形，其中环被填充。\n- 可量化的答案：输出是五个情况的最大生命周期（浮点数）列表，格式需与指定的最终输出格式完全一致。",
            "solution": "该问题要求从合成的潜在点云中计算和解释拓扑特征，特别是一维环（圈）。这项任务属于拓扑数据分析（TDA）的范畴，该领域使用代数拓扑学的工具来分析数据的“形状”。解决方案的核心在于实现一个计算持续同调的算法，该算法量化了拓扑特征在一系列尺度上的“生”与“灭”。\n\n其基本原理是，一个点云本身在拓扑上是平凡的（一个离散的点集），但可以通过构建一系列称为过滤（filtration）的单纯复形来赋予其更丰富的结构。我们将使用 Vietoris–Rips (VR) 复形，它由一个邻近参数 $r$ 决定。对于给定的点集 $X$，VR 复形 $VR(X, r)$ 为 $X$ 中每组两两欧几里得距离在 $r$ 以内的 $k+1$ 个点构成一个 $k$-单纯形。\n\n我们的主要目标是随着 $r$ 的增加跟踪这些复形的贝蒂数 $\\beta_k$。$\\beta_0$ 计算连通分支的数量，$\\beta_1$ 计算一维环或圈的数量。持续同调捕捉这些特征的生命周期，表示为生灭区间 $[r_{birth}, r_{death})$。生命周期长的特征被认为是显著的拓扑信号，而生命周期短的特征通常归因于噪声。\n\n算法按以下步骤进行：\n\n1.  **数据生成**：对于五种情况中的每一种，根据指定的参数生成一个 $\\mathbb{R}^2$ 中的点云。使用固定的随机种子以确保可复现性。这些情况旨在代表不同的拓扑场景：一个完美的单环（情况 R）、一个可收缩的斑点（情况 B）、两个不相交的环（情况 2R）、一个含噪声的环（情况 NR）和一个稀疏的环（情况 SR）。\n\n2.  **Vietoris-Rips 过滤**：为了提高效率，我们不为 $M=40$ 个尺度 $r_m \\in [0.02, 0.6]$ 中的每一个都重建 VR 复形，而是采用一种增量方法。\n    a. 计算 $N$ 个点之间的所有两两欧几里得距离，形成一个潜在边的列表。\n    b. 按距离升序对这些边进行排序。这个排序后的列表决定了向复形中添加单纯形的顺序。\n\n3.  **贝蒂数的计算**：我们遍历离散尺度 $r_m$，对于每个尺度，通过处理所有长度小于或等于当前 $r_m$ 的边来更新单纯形和连通分支的计数。\n    a. **$\\beta_0(r)$**：使用并查集或不相交集联合（DSU）数据结构来跟踪连通分支的数量。最初有 $N$ 个点，$\\beta_0 = N$。当一条边 $(u, v)$ 被添加到复形中时，如果 $u$ 和 $v$ 属于不同的连通分支，则将它们合并，并将 $\\beta_0$ 减 1。\n    b. **$\\beta_1(r)$**：第一贝蒂数使用从单纯复形 $K$ 的欧拉-庞加莱公式导出的近似值来计算：$\\chi(K) = \\sum_{k=0}^{\\infty} (-1)^k c_k = \\sum_{k=0}^{\\infty} (-1)^k \\beta_k$，其中 $c_k$ 是 $k$-单纯形的数量。对于我们在 $\\mathbb{R}^2$ 中的点云，我们可以假设高维同调可以忽略不计（即对于 $k \\ge 2$，$\\beta_k \\approx 0$）。VR 复形是一个旗复形，这意味着我们将图中的所有团都视作填充的单纯形。因此，公式简化为 $c_0 - c_1 + c_2 \\approx \\beta_0 - \\beta_1$。为 $\\beta_1$ 重新整理，我们得到每个尺度 $r$ 的计算公式：\n    $$\n    \\beta_1(r) \\approx \\beta_0(r) - c_0(r) + c_1(r) - c_2(r)\n    $$\n    这里，$c_0(r)=N$ 是顶点的数量，$c_1(r)$ 是长度 $\\le r$ 的边的数量，$c_2(r)$ 是所有三条边长度都 $\\le r$ 的 $2$-单纯形（三角形）的数量，$\\beta_0(r)$ 是连通分支的数量。当算法遍历排序后的边直到当前尺度 $r_m$ 时，它会增量地更新 $c_1$、$c_2$ 和 $\\beta_0$。每添加一条边 $(u, v)$，它与 $u$ 和 $v$ 的每个共同邻居都会形成一个新的三角形。\n\n4.  **持续性区间的提取**：在计算出序列 $\\{\\beta_1(r_m)\\}_{m=1}^M$ 后，我们提取一维特征的生灭区间。这里采用一个简单的贪心配对算法：\n    a. 维护一个活跃特征的出生尺度列表。\n    b. 当 $\\beta_1(r_m) > \\beta_1(r_{m-1})$ 时，差值 $\\Delta \\beta_1 = \\beta_1(r_m) - \\beta_1(r_{m-1})$ 对应于在尺度 $r_m$ 处新生的 $\\Delta \\beta_1$ 个特征。它们的出生尺度被添加到活跃列表中。\n    c. 当 $\\beta_1(r_m)  \\beta_1(r_{m-1})$ 时，差值 $|\\Delta \\beta_1|$ 对应于特征的消亡。这些消亡与活跃列表中出生尺度最早的特征配对（先进先出原则）。对于每一个这样的配对 $(r_{birth}, r_m)$，记录一个持续性区间。\n    d. 在最终尺度 $r_M = 0.6$ 时，任何仍留在活跃列表中的特征被认为在观察范围内无限持续，并被赋予一个为 $r_M$ 的消亡时间。\n\n5.  **最大生命周期**：对于每个数据集，区间 $[r_{birth}, r_{death})$ 的生命周期是 $r_{death} - r_{birth}$。最终报告的值是在所有已识别的 $\\beta_1$ 区间中找到的最大生命周期。该值可以作为数据中最显著的环状特征的稳健指标。\n\n这种结构化的方法使我们能够系统地量化潜在数据的拓扑“形状”，并由此推断自编码器学会区分的宏观相的性质。",
            "answer": "```python\nimport numpy as np\n\nclass UnionFind:\n    \"\"\"A simple Union-Find data structure.\"\"\"\n    def __init__(self, n):\n        self.parent = list(range(n))\n        self.num_components = n\n\n    def find(self, i):\n        if self.parent[i] == i:\n            return i\n        self.parent[i] = self.find(self.parent[i])\n        return self.parent[i]\n\n    def union(self, i, j):\n        root_i = self.find(i)\n        root_j = self.find(j)\n        if root_i != root_j:\n            self.parent[root_j] = root_i\n            self.num_components -= 1\n            return True\n        return False\n\ndef generate_points(case_name):\n    \"\"\"Generates point clouds based on the case name.\"\"\"\n    if case_name == 'R':\n        N, radius, center, sigma = 50, 1.0, (0.0, 0.0), 0.02\n        angles = np.random.uniform(0, 2 * np.pi, N)\n        points = np.zeros((N, 2))\n        points[:, 0] = center[0] + radius * np.cos(angles)\n        points[:, 1] = center[1] + radius * np.sin(angles)\n        points += np.random.normal(0, sigma, size=points.shape)\n        return points\n    elif case_name == 'B':\n        N, sigma = 50, 0.4\n        points = np.random.normal(0, sigma, size=(N, 2))\n        return points\n    elif case_name == '2R':\n        N_half, radius, centers, sigma = 40, 1.0, [(-2.0, 0.0), (2.0, 0.0)], 0.02\n        points = []\n        for center in centers:\n            angles = np.random.uniform(0, 2 * np.pi, N_half)\n            ring_points = np.zeros((N_half, 2))\n            ring_points[:, 0] = center[0] + radius * np.cos(angles)\n            ring_points[:, 1] = center[1] + radius * np.sin(angles)\n            ring_points += np.random.normal(0, sigma, size=ring_points.shape)\n            points.append(ring_points)\n        return np.vstack(points)\n    elif case_name == 'NR':\n        N, radius, center, sigma = 50, 1.0, (0.0, 0.0), 0.12\n        angles = np.random.uniform(0, 2 * np.pi, N)\n        points = np.zeros((N, 2))\n        points[:, 0] = center[0] + radius * np.cos(angles)\n        points[:, 1] = center[1] + radius * np.sin(angles)\n        points += np.random.normal(0, sigma, size=points.shape)\n        return points\n    elif case_name == 'SR':\n        N, radius, center, sigma = 20, 1.0, (0.0, 0.0), 0.02\n        angles = np.random.uniform(0, 2 * np.pi, N)\n        points = np.zeros((N, 2))\n        points[:, 0] = center[0] + radius * np.cos(angles)\n        points[:, 1] = center[1] + radius * np.sin(angles)\n        points += np.random.normal(0, sigma, size=points.shape)\n        return points\n    return None\n\ndef calculate_max_lifetime(points, r_min, r_max, M):\n    \"\"\"\n    Computes VR filtration and max Betti 1 lifetime.\n    \"\"\"\n    N = points.shape[0]\n    if N == 0:\n        return 0.0\n\n    # 1. Compute pairwise distances and sort edges\n    dists = np.sqrt(np.sum((points[:, np.newaxis, :] - points[np.newaxis, :, :])**2, axis=-1))\n    edges = []\n    for i in range(N):\n        for j in range(i + 1, N):\n            edges.append((dists[i, j], i, j))\n    edges.sort()\n\n    scales = np.linspace(r_min, r_max, M)\n    betti1_sequence = []\n    \n    # 2. Iterate through filtration scales\n    edge_idx = 0\n    uf = UnionFind(N)\n    adj = [set() for _ in range(N)]\n    num_edges = 0\n    num_triangles = 0\n    c0 = N\n\n    for r_m in scales:\n        while edge_idx  len(edges) and edges[edge_idx][0] = r_m:\n            dist, u, v = edges[edge_idx]\n            \n            # Update c1\n            num_edges += 1\n            \n            # Update c2 (count new triangles formed by adding edge (u,v))\n            num_new_triangles = len(adj[u].intersection(adj[v]))\n            num_triangles += num_new_triangles\n            \n            # Update graph and beta0\n            adj[u].add(v)\n            adj[v].add(u)\n            uf.union(u, v)\n\n            edge_idx += 1\n        \n        beta0 = uf.num_components\n        c1 = num_edges\n        c2 = num_triangles\n        \n        # Calculate beta_1 using Euler-Poincare formula approximation\n        beta1 = beta0 - c0 + c1 - c2\n        betti1_sequence.append(beta1)\n    \n    # 3. Extract birth-death intervals\n    births = []\n    intervals = []\n    prev_b1 = 0\n    for i, current_b1 in enumerate(betti1_sequence):\n        r_m = scales[i]\n        delta_b1 = current_b1 - prev_b1\n        if delta_b1 > 0:\n            for _ in range(delta_b1):\n                births.append(r_m)\n            births.sort()\n        elif delta_b1  0:\n            num_deaths = -delta_b1\n            for _ in range(min(num_deaths, len(births))):\n                birth_time = births.pop(0)\n                intervals.append((birth_time, r_m))\n        prev_b1 = current_b1\n        \n    # 4. Handle remaining births\n    for birth_time in births:\n        intervals.append((birth_time, r_max))\n        \n    # 5. Calculate max lifetime\n    if not intervals:\n        return 0.0\n        \n    max_lifetime = max(death - birth for birth, death in intervals)\n    return max_lifetime\n\ndef solve():\n    np.random.seed(0)\n    \n    cases = ['R', 'B', '2R', 'NR', 'SR']\n    \n    # Filtration parameters\n    r_min, r_max, M = 0.02, 0.6, 40\n    \n    results = []\n    for case_name in cases:\n        points = generate_points(case_name)\n        max_lifetime = calculate_max_lifetime(points, r_min, r_max, M)\n        results.append(max_lifetime)\n    \n    # Format and print the final output\n    print(f\"[{','.join(f'{r:.10f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}