{
    "hands_on_practices": [
        {
            "introduction": "Reinforcement learning is fundamentally about learning to make optimal decisions. At the heart of many foundational algorithms lies the principle of dynamic programming and the Bellman optimality operator. This first exercise provides a direct, hands-on calculation of a single step of value iteration, the quintessential algorithm for solving a known Markov Decision Process (MDP) . By manually applying the Bellman backup to a given value function, you will gain a concrete intuition for how information about rewards and future states propagates through the state space, iteratively refining the estimate of a state's true value.",
            "id": "3802027",
            "problem": "Consider a Markov Decision Process (MDP) with state space $\\mathcal{S} = \\{s_1, s_2, s_3\\}$ and action space $\\mathcal{A} = \\{a, b\\}$. This MDP represents a coarse-grained control model for multiscale materials design, where each state $s_i$ summarizes a microstructural regime. Let the transition kernel be defined by the probabilities $P(s' \\mid s, a)$ and the immediate rewards be given by $R(s, a)$, with discount factor $\\gamma \\in (0,1)$. For this MDP, the components are specified as follows:\n- Transitions from $s_1$:\n  - $P(s_1 \\mid s_1, a) = \\frac{1}{2}$, $P(s_2 \\mid s_1, a) = \\frac{1}{4}$, $P(s_3 \\mid s_1, a) = \\frac{1}{4}$;\n  - $P(s_1 \\mid s_1, b) = 0$, $P(s_2 \\mid s_1, b) = \\frac{3}{4}$, $P(s_3 \\mid s_1, b) = \\frac{1}{4}$.\n- Transitions from $s_2$:\n  - $P(s_1 \\mid s_2, a) = \\frac{1}{3}$, $P(s_2 \\mid s_2, a) = \\frac{1}{3}$, $P(s_3 \\mid s_2, a) = \\frac{1}{3}$;\n  - $P(s_1 \\mid s_2, b) = 0$, $P(s_2 \\mid s_2, b) = 0$, $P(s_3 \\mid s_2, b) = 1$.\n- Transitions from $s_3$:\n  - $P(s_1 \\mid s_3, a) = 0$, $P(s_2 \\mid s_3, a) = 0$, $P(s_3 \\mid s_3, a) = 1$;\n  - $P(s_1 \\mid s_3, b) = \\frac{1}{2}$, $P(s_2 \\mid s_3, b) = \\frac{1}{2}$, $P(s_3 \\mid s_3, b) = 0$.\nThe immediate rewards are:\n- $R(s_1, a) = 1$, $R(s_1, b) = 0$;\n- $R(s_2, a) = 2$, $R(s_2, b) = -1$;\n- $R(s_3, a) = 0$, $R(s_3, b) = 1$.\nLet the discount factor be $\\gamma = \\frac{1}{2}$. Given an initial value function $V_k$ with $V_k(s_1) = 2$, $V_k(s_2) = -1$, and $V_k(s_3) = 0$, perform one step of value iteration to obtain $V_{k+1}$ by applying the Bellman optimality operator to $V_k$ at each state. Then compute the residual norm $\\|V_{k+1} - V_k\\|_{\\infty}$, where $\\|\\cdot\\|_{\\infty}$ denotes the supremum norm defined by $\\|x\\|_{\\infty} = \\max_i |x_i|$. Express the final answer as a single exact rational number with no rounding.",
            "solution": "The problem is set within the framework of Reinforcement Learning (RL) for control and design, and uses the standard definition of a Markov Decision Process (MDP). The value iteration update is governed by the Bellman optimality operator. Starting from the fundamental definition, for any value function $V$ and state $s \\in \\mathcal{S}$, the Bellman optimality operator $\\mathcal{T}$ is defined componentwise by\n$$\n(\\mathcal{T}V)(s) = \\max_{a \\in \\mathcal{A}} \\left[ R(s,a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s' \\mid s, a)\\, V(s') \\right].\n$$\nA single step of value iteration computes $V_{k+1} = \\mathcal{T} V_k$. We apply this state by state using the provided $V_k$, transition probabilities, rewards, and discount factor $\\gamma = \\frac{1}{2}$.\n\nCompute $V_{k+1}(s_1)$:\n- For action $a$ at $s_1$,\n$$\n\\sum_{s'} P(s' \\mid s_1, a) V_k(s') = \\frac{1}{2} V_k(s_1) + \\frac{1}{4} V_k(s_2) + \\frac{1}{4} V_k(s_3) = \\frac{1}{2} \\cdot 2 + \\frac{1}{4} \\cdot (-1) + \\frac{1}{4} \\cdot 0 = 1 - \\frac{1}{4} + 0 = \\frac{3}{4}.\n$$\nTherefore,\n$$\nQ(s_1, a) = R(s_1,a) + \\gamma \\sum_{s'} P(s' \\mid s_1, a) V_k(s') = 1 + \\frac{1}{2} \\cdot \\frac{3}{4} = 1 + \\frac{3}{8} = \\frac{11}{8}.\n$$\n- For action $b$ at $s_1$,\n$$\n\\sum_{s'} P(s' \\mid s_1, b) V_k(s') = 0 \\cdot V_k(s_1) + \\frac{3}{4} V_k(s_2) + \\frac{1}{4} V_k(s_3) = 0 + \\frac{3}{4} \\cdot (-1) + \\frac{1}{4} \\cdot 0 = -\\frac{3}{4}.\n$$\nTherefore,\n$$\nQ(s_1, b) = R(s_1,b) + \\gamma \\sum_{s'} P(s' \\mid s_1, b) V_k(s') = 0 + \\frac{1}{2} \\cdot \\left(-\\frac{3}{4}\\right) = -\\frac{3}{8}.\n$$\nThus,\n$$\nV_{k+1}(s_1) = \\max\\left\\{ \\frac{11}{8}, -\\frac{3}{8} \\right\\} = \\frac{11}{8}.\n$$\n\nCompute $V_{k+1}(s_2)$:\n- For action $a$ at $s_2$,\n$$\n\\sum_{s'} P(s' \\mid s_2, a) V_k(s') = \\frac{1}{3} V_k(s_1) + \\frac{1}{3} V_k(s_2) + \\frac{1}{3} V_k(s_3) = \\frac{1}{3} \\cdot 2 + \\frac{1}{3} \\cdot (-1) + \\frac{1}{3} \\cdot 0 = \\frac{2}{3} - \\frac{1}{3} + 0 = \\frac{1}{3}.\n$$\nTherefore,\n$$\nQ(s_2, a) = R(s_2,a) + \\gamma \\sum_{s'} P(s' \\mid s_2, a) V_k(s') = 2 + \\frac{1}{2} \\cdot \\frac{1}{3} = 2 + \\frac{1}{6} = \\frac{13}{6}.\n$$\n- For action $b$ at $s_2$,\n$$\n\\sum_{s'} P(s' \\mid s_2, b) V_k(s') = 0 \\cdot V_k(s_1) + 0 \\cdot V_k(s_2) + 1 \\cdot V_k(s_3) = 0,\n$$\nso\n$$\nQ(s_2, b) = R(s_2,b) + \\gamma \\sum_{s'} P(s' \\mid s_2, b) V_k(s') = -1 + \\frac{1}{2} \\cdot 0 = -1.\n$$\nThus,\n$$\nV_{k+1}(s_2) = \\max\\left\\{ \\frac{13}{6}, -1 \\right\\} = \\frac{13}{6}.\n$$\n\nCompute $V_{k+1}(s_3)$:\n- For action $a$ at $s_3$,\n$$\n\\sum_{s'} P(s' \\mid s_3, a) V_k(s') = 1 \\cdot V_k(s_3) = 0,\n$$\nso\n$$\nQ(s_3, a) = R(s_3,a) + \\gamma \\cdot 0 = 0.\n$$\n- For action $b$ at $s_3$,\n$$\n\\sum_{s'} P(s' \\mid s_3, b) V_k(s') = \\frac{1}{2} V_k(s_1) + \\frac{1}{2} V_k(s_2) + 0 \\cdot V_k(s_3) = \\frac{1}{2} \\cdot 2 + \\frac{1}{2} \\cdot (-1) + 0 = 1 - \\frac{1}{2} = \\frac{1}{2}.\n$$\nTherefore,\n$$\nQ(s_3, b) = R(s_3,b) + \\gamma \\sum_{s'} P(s' \\mid s_3, b) V_k(s') = 1 + \\frac{1}{2} \\cdot \\frac{1}{2} = 1 + \\frac{1}{4} = \\frac{5}{4}.\n$$\nThus,\n$$\nV_{k+1}(s_3) = \\max\\left\\{ 0, \\frac{5}{4} \\right\\} = \\frac{5}{4}.\n$$\n\nCollecting the results,\n$$\nV_{k+1} = \\left( V_{k+1}(s_1), V_{k+1}(s_2), V_{k+1}(s_3) \\right) = \\left( \\frac{11}{8}, \\frac{13}{6}, \\frac{5}{4} \\right).\n$$\n\nNow compute the residual norm $\\|V_{k+1} - V_k\\|_{\\infty}$. First, compute the componentwise differences:\n$$\nV_{k+1}(s_1) - V_k(s_1) = \\frac{11}{8} - 2 = \\frac{11}{8} - \\frac{16}{8} = -\\frac{5}{8},\n$$\n$$\nV_{k+1}(s_2) - V_k(s_2) = \\frac{13}{6} - (-1) = \\frac{13}{6} + 1 = \\frac{13}{6} + \\frac{6}{6} = \\frac{19}{6},\n$$\n$$\nV_{k+1}(s_3) - V_k(s_3) = \\frac{5}{4} - 0 = \\frac{5}{4}.\n$$\nTake absolute values:\n$$\n\\left| -\\frac{5}{8} \\right| = \\frac{5}{8}, \\quad \\left| \\frac{19}{6} \\right| = \\frac{19}{6}, \\quad \\left| \\frac{5}{4} \\right| = \\frac{5}{4}.\n$$\nThe supremum norm is the maximum of these three values:\n$$\n\\|V_{k+1} - V_k\\|_{\\infty} = \\max\\left\\{ \\frac{5}{8}, \\frac{19}{6}, \\frac{5}{4} \\right\\} = \\frac{19}{6}.\n$$\n\nThis residual quantifies the largest pointwise change in the value function across states after one application of the Bellman optimality operator to $V_k$. By the contraction property of the Bellman optimality operator under the supremum norm, such residuals decrease as iterations proceed, guiding convergence toward the optimal value function, although that broader convergence analysis is beyond the immediate calculation requested.",
            "answer": "$$\\boxed{\\frac{19}{6}}$$"
        },
        {
            "introduction": "While value iteration is powerful, it requires a complete model of the environment, which is often unavailable. This brings us to Temporal-Difference (TD) learning, a cornerstone of modern reinforcement learning that learns directly from raw experience. This practice focuses on the $\\mathrm{TD}(0)$ algorithm, demonstrating how to update a value function estimate using a single transition tuple $(s_t, a_t, r_t, s_{t+1})$ . You will calculate the TD error—the discrepancy between your current estimate and a bootstrapped target—which serves as the fundamental learning signal in many model-free RL agents.",
            "id": "3802032",
            "problem": "A controller is trained by Reinforcement Learning (RL) to regulate a multiscale materials process whose coarse-grained state $s$ aggregates microstructural statistics. At each decision time $t$, the state $s_t$ is summarized by three observables: the micro-scale energy density $e(s)$, the two-point correlation at one lattice spacing $c(s)$, and the interfacial area density $a(s)$. A linear state-value approximation $\\hat{V}(s;\\theta)$ is employed with a feature map\n$$\n\\phi(s) = \\begin{pmatrix} 1 \\\\ e(s) \\\\ c(s) \\\\ a(s) \\\\ e(s)\\,c(s) \\end{pmatrix} \\in \\mathbb{R}^{5}, \n$$\nso that \n$$\n\\hat{V}(s;\\theta) = \\theta^{\\top}\\phi(s),\n$$\nwhere $\\theta \\in \\mathbb{R}^{5}$ is the parameter vector. Consider the following single sample transition $(s_t,a_t,r_t,s_{t+1})$ generated under a fixed policy:\n- For $s_t$: $e(s_t)=\\frac{3}{2}$, $c(s_t)=\\frac{1}{5}$, $a(s_t)=\\frac{7}{10}$.\n- For $s_{t+1}$: $e(s_{t+1})=\\frac{4}{3}$, $c(s_{t+1})=\\frac{2}{5}$, $a(s_{t+1})=\\frac{3}{5}$.\n- The immediate reward is $r_t=\\frac{2}{5}$.\n- The discount factor is $\\gamma=\\frac{9}{10}$.\n\nThe initial parameter is\n$$\n\\theta_t = \\begin{pmatrix} \\frac{1}{2} \\\\ -\\frac{1}{5} \\\\ \\frac{2}{3} \\\\ \\frac{1}{4} \\\\ -\\frac{3}{10} \\end{pmatrix},\n$$\nand the learning rate is $\\alpha=\\frac{3}{20}$.\n\nUsing Temporal-Difference (TD) learning with zero-step bootstrapping $\\mathrm{TD}(0)$ and the linear function approximation described above, carry out one parameter update at time $t$ for this single sample and evaluate the temporal-difference error at time $t$. Express your final answer as a single exact fraction representing the temporal-difference error. Do not round.",
            "solution": "The problem requires the calculation of the temporal-difference (TD) error for a single sample update using the $\\mathrm{TD}(0)$ algorithm. For a state-value function approximation $\\hat{V}(s;\\theta)$, the TD error at time step $t$, denoted $\\delta_t$, is defined as the difference between the TD target and the current value estimate of state $s_t$.\n\nThe TD target is the sum of the immediate reward $r_t$ and the discounted value of the next state $s_{t+1}$. The formula for the TD error is:\n$$\n\\delta_t = r_t + \\gamma \\hat{V}(s_{t+1}; \\theta_t) - \\hat{V}(s_t; \\theta_t)\n$$\nHere, both value estimates are computed using the parameter vector at time $t$, $\\theta_t$. The value function is given by the linear approximation $\\hat{V}(s;\\theta_t) = \\theta_t^{\\top}\\phi(s)$.\n\nFirst, we construct the feature vectors $\\phi(s_t)$ and $\\phi(s_{t+1})$ using the provided state observables.\n\nFor state $s_t$:\n$e(s_t) = \\frac{3}{2}$, $c(s_t) = \\frac{1}{5}$, $a(s_t) = \\frac{7}{10}$.\nThe interaction term is $e(s_t)c(s_t) = \\frac{3}{2} \\times \\frac{1}{5} = \\frac{3}{10}$.\nThe feature vector $\\phi(s_t)$ is:\n$$\n\\phi(s_t) = \\begin{pmatrix} 1 \\\\ \\frac{3}{2} \\\\ \\frac{1}{5} \\\\ \\frac{7}{10} \\\\ \\frac{3}{10} \\end{pmatrix}\n$$\n\nFor state $s_{t+1}$:\n$e(s_{t+1}) = \\frac{4}{3}$, $c(s_{t+1}) = \\frac{2}{5}$, $a(s_{t+1}) = \\frac{3}{5}$.\nThe interaction term is $e(s_{t+1})c(s_{t+1}) = \\frac{4}{3} \\times \\frac{2}{5} = \\frac{8}{15}$.\nThe feature vector $\\phi(s_{t+1})$ is:\n$$\n\\phi(s_{t+1}) = \\begin{pmatrix} 1 \\\\ \\frac{4}{3} \\\\ \\frac{2}{5} \\\\ \\frac{3}{5} \\\\ \\frac{8}{15} \\end{pmatrix}\n$$\n\nNext, we compute the state-value estimates, $\\hat{V}(s_t; \\theta_t)$ and $\\hat{V}(s_{t+1}; \\theta_t)$, using the given parameter vector $\\theta_t$.\n\nThe value estimate for $s_t$ is:\n$$\n\\hat{V}(s_t; \\theta_t) = \\theta_t^{\\top}\\phi(s_t) = \\begin{pmatrix} \\frac{1}{2} & -\\frac{1}{5} & \\frac{2}{3} & \\frac{1}{4} & -\\frac{3}{10} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ \\frac{3}{2} \\\\ \\frac{1}{5} \\\\ \\frac{7}{10} \\\\ \\frac{3}{10} \\end{pmatrix}\n$$\n$$\n\\hat{V}(s_t; \\theta_t) = \\left(\\frac{1}{2}\\right)(1) + \\left(-\\frac{1}{5}\\right)\\left(\\frac{3}{2}\\right) + \\left(\\frac{2}{3}\\right)\\left(\\frac{1}{5}\\right) + \\left(\\frac{1}{4}\\right)\\left(\\frac{7}{10}\\right) + \\left(-\\frac{3}{10}\\right)\\left(\\frac{3}{10}\\right)\n$$\n$$\n\\hat{V}(s_t; \\theta_t) = \\frac{1}{2} - \\frac{3}{10} + \\frac{2}{15} + \\frac{7}{40} - \\frac{9}{100}\n$$\nTo sum these fractions, we find a common denominator. The least common multiple of $2$, $10$, $15$, $40$, and $100$ is $600$.\n$$\n\\hat{V}(s_t; \\theta_t) = \\frac{300}{600} - \\frac{180}{600} + \\frac{80}{600} + \\frac{105}{600} - \\frac{54}{600} = \\frac{300 - 180 + 80 + 105 - 54}{600} = \\frac{251}{600}\n$$\n\nThe value estimate for $s_{t+1}$ is:\n$$\n\\hat{V}(s_{t+1}; \\theta_t) = \\theta_t^{\\top}\\phi(s_{t+1}) = \\begin{pmatrix} \\frac{1}{2} & -\\frac{1}{5} & \\frac{2}{3} & \\frac{1}{4} & -\\frac{3}{10} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ \\frac{4}{3} \\\\ \\frac{2}{5} \\\\ \\frac{3}{5} \\\\ \\frac{8}{15} \\end{pmatrix}\n$$\n$$\n\\hat{V}(s_{t+1}; \\theta_t) = \\left(\\frac{1}{2}\\right)(1) + \\left(-\\frac{1}{5}\\right)\\left(\\frac{4}{3}\\right) + \\left(\\frac{2}{3}\\right)\\left(\\frac{2}{5}\\right) + \\left(\\frac{1}{4}\\right)\\left(\\frac{3}{5}\\right) + \\left(-\\frac{3}{10}\\right)\\left(\\frac{8}{15}\\right)\n$$\n$$\n\\hat{V}(s_{t+1}; \\theta_t) = \\frac{1}{2} - \\frac{4}{15} + \\frac{4}{15} + \\frac{3}{20} - \\frac{24}{150}\n$$\nThe second and third terms cancel. Simplifying the last term gives $\\frac{24}{150} = \\frac{4}{25}$.\n$$\n\\hat{V}(s_{t+1}; \\theta_t) = \\frac{1}{2} + \\frac{3}{20} - \\frac{4}{25}\n$$\nThe least common multiple of $2$, $20$, and $25$ is $100$.\n$$\n\\hat{V}(s_{t+1}; \\theta_t) = \\frac{50}{100} + \\frac{15}{100} - \\frac{16}{100} = \\frac{50 + 15 - 16}{100} = \\frac{49}{100}\n$$\n\nFinally, we compute the TD error $\\delta_t$ using its definition.\n$$\n\\delta_t = r_t + \\gamma \\hat{V}(s_{t+1}; \\theta_t) - \\hat{V}(s_t; \\theta_t)\n$$\nSubstituting the given values and our calculated estimates:\n$$\n\\delta_t = \\frac{2}{5} + \\left(\\frac{9}{10}\\right)\\left(\\frac{49}{100}\\right) - \\frac{251}{600}\n$$\nFirst, calculate the TD target, which is the first part of the expression:\n$$\n\\text{TD Target} = \\frac{2}{5} + \\frac{9 \\times 49}{10 \\times 100} = \\frac{2}{5} + \\frac{441}{1000} = \\frac{400}{1000} + \\frac{441}{1000} = \\frac{841}{1000}\n$$\nNow, subtract the value estimate for the current state:\n$$\n\\delta_t = \\frac{841}{1000} - \\frac{251}{600}\n$$\nThe least common multiple of $1000$ and $600$ is $3000$.\n$$\n\\delta_t = \\frac{841 \\times 3}{3000} - \\frac{251 \\times 5}{3000} = \\frac{2523}{3000} - \\frac{1255}{3000} = \\frac{2523 - 1255}{3000} = \\frac{1268}{3000}\n$$\nThis fraction can be simplified by dividing the numerator and denominator by their greatest common divisor. Both are even.\n$$\n\\delta_t = \\frac{1268 \\div 4}{3000 \\div 4} = \\frac{317}{750}\n$$\nThe number $317$ is prime, and the prime factors of $750$ are $2 \\cdot 3 \\cdot 5^3$. Thus, the fraction is in its simplest form.\nThe temporal-difference error is $\\frac{317}{750}$.",
            "answer": "$$\n\\boxed{\\frac{317}{750}}\n$$"
        },
        {
            "introduction": "As RL systems move from simulation to real-world applications, ensuring safety becomes a paramount concern. An unconstrained RL agent may take actions that violate system limits or lead to hazardous states, especially during exploration. This final practice challenges you to design and implement a safety filter, a crucial component for safe RL that acts as a supervisory layer . By formulating the problem as a quadratic program, you will learn how to project a potentially unsafe action proposed by an RL agent onto a certifiably safe set, guaranteeing that the system's operational constraints are always respected.",
            "id": "3801990",
            "problem": "Design and implement a safety filter for Reinforcement Learning (RL) that corrects a proposed action by solving a strictly convex quadratic program derived from first principles. The safety filter must project a proposed action $u \\in \\mathbb{R}^m$ to the closest feasible action $u' \\in \\mathbb{R}^m$ that satisfies linearized control and state constraints arising from multiscale dynamics, with feasibility defined in terms of the next-step macro-scale safety constraints.\n\nStart from the following fundamental bases:\n- For a discrete-time controlled system obtained from a first-order forward Euler discretization of a continuous-time model, the next state satisfies $x^{+} \\approx x + B u'$, where $x \\in \\mathbb{R}^n$ is the current state, $u' \\in \\mathbb{R}^m$ is the (to-be-corrected) action, and $B \\in \\mathbb{R}^{n \\times m}$ is the input Jacobian (i.e., the partial derivative of the dynamics with respect to the control, multiplied by the time step).\n- Macro-scale safety constraints are modeled as linear inequalities on the next state, $H x^{+} \\le h$, where $H \\in \\mathbb{R}^{p \\times n}$ and $h \\in \\mathbb{R}^p$. Linearization around the current state and the proposed action yields a linear constraint on $u'$: $H (x + B u') \\le h$, or equivalently $(H B) u' \\le h - H x$.\n- The control must also obey component-wise box constraints $L \\le u' \\le U$, where $L \\in \\mathbb{R}^m$ and $U \\in \\mathbb{R}^m$.\n\nThe safety filter must return the action $u'$ that solves the quadratic program\nminimize over $u' \\in \\mathbb{R}^m$: $\\tfrac{1}{2} \\lVert u' - u \\rVert_2^2$ subject to $A u' \\le b$,\nwhere the constraint matrix $A \\in \\mathbb{R}^{q \\times m}$ and vector $b \\in \\mathbb{R}^q$ stack:\n- the linearized next-state safety constraints $(H B) u' \\le h - H x$, and\n- the box constraints written as inequalities $u' \\le U$ and $-u' \\le -L$.\n\nYour program must:\n1. Derive from the Lagrangian and Karush–Kuhn–Tucker (KKT) conditions a dual problem suitable for algorithmic solution using projected gradient descent on the nonnegative orthant of Lagrange multipliers, and recover the primal solution $u'$ from the optimal dual variables.\n2. Implement a solver that takes $u$, $A$, and $b$, and returns the corrected action $u'$ that solves the quadratic program.\n3. Construct the constraints from the provided data for each test case, and apply the solver to compute $u'$.\n\nUse the following test suite. For each case, $m = 2$ and $n = 2$. Unless otherwise stated, take $H = I_2$ (the $2 \\times 2$ identity). Provide all answers as plain numerical lists; no physical units are involved.\n\n- Test Case $1$ (happy path, interior feasible):\n  - Proposed action $u = [\\,0.5,\\,0.5\\,]$.\n  - Current state $x = [\\,0.2,\\,0.1\\,]$.\n  - Input Jacobian $B = \\begin{bmatrix} 0.5 & 0.2 \\\\ 0.0 & 0.8 \\end{bmatrix}$.\n  - Safety thresholds $h = [\\,1.0,\\,0.7\\,]$.\n  - Box bounds $L = [\\,-1.0,\\,-1.0\\,],\\ U = [\\,1.0,\\,1.0\\,]$.\n\n- Test Case $2$ (violates a state constraint, requires minimal adjustment):\n  - Proposed action $u = [\\,1.0,\\,1.0\\,]$.\n  - Current state $x = [\\,0.2,\\,0.1\\,]$.\n  - Input Jacobian $B = \\begin{bmatrix} 0.5 & 0.2 \\\\ 0.0 & 0.8 \\end{bmatrix}$.\n  - Safety thresholds $h = [\\,1.0,\\,0.7\\,]$.\n  - Box bounds $L = [\\,-1.0,\\,-1.0\\,],\\ U = [\\,1.0,\\,1.0\\,]$.\n\n- Test Case $3$ (tighter safety, coupled constraint active):\n  - Proposed action $u = [\\,0.9,\\,0.9\\,]$.\n  - Current state $x = [\\,0.5,\\,0.5\\,]$.\n  - Input Jacobian $B = \\begin{bmatrix} 0.5 & 0.2 \\\\ 0.0 & 0.8 \\end{bmatrix}$.\n  - Safety thresholds $h = [\\,1.0,\\,0.7\\,]$.\n  - Box bounds $L = [\\,-1.0,\\,-1.0\\,],\\ U = [\\,1.0,\\,1.0\\,]$.\n\n- Test Case $4$ (violates only control bounds; state constraints inactive):\n  - Proposed action $u = [\\,1.5,\\,-1.5\\,]$.\n  - Current state $x = [\\,0.0,\\,0.0\\,]$.\n  - Input Jacobian $B = \\begin{bmatrix} 0.5 & 0.2 \\\\ 0.0 & 0.8 \\end{bmatrix}$.\n  - Safety thresholds $h = [\\,10.0,\\,10.0\\,]$.\n  - Box bounds $L = [\\,-1.0,\\,-1.0\\,],\\ U = [\\,1.0,\\,1.0\\,]$.\n\n- Test Case $5$ (redundant constraints; projection under duplicates):\n  - Proposed action $u = [\\,0.3,\\,0.3\\,]$.\n  - Current state $x = [\\,0.0,\\,0.0\\,]$.\n  - Input Jacobian $B = \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 1.0 \\end{bmatrix}$.\n  - Safety matrix $H = \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 1.0 \\\\ 0.0 & 1.0 \\end{bmatrix}$.\n  - Safety thresholds $h = [\\,0.2,\\,0.2,\\,0.2\\,]$.\n  - Box bounds $L = [\\,-1.0,\\,-1.0\\,],\\ U = [\\,1.0,\\,1.0\\,]$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the corrected action for the corresponding test case represented as a bracketed list of two floating-point numbers. For example: \"[[a1,a2],[b1,b2],...]\" with numeric values. Round each component to no more than six decimal places in the printed output. The final answers are pure numbers without units or angle measures, and no percentages should be used.",
            "solution": "The problem is to design a safety filter for a reinforcement learning agent. This filter takes a proposed control action $u$ and projects it onto a safety-certified feasible set, yielding a corrected action $u'$. The feasible set is defined by linear constraints on the control action, derived from linearized system dynamics and a set of macro-scale safety requirements. The projection is defined as the point in the feasible set that is closest to the proposed action $u$ in the Euclidean norm sense. This is a classic strictly convex Quadratic Program (QP).\n\nThe primal problem is formulated as:\n$$\n\\underset{u' \\in \\mathbb{R}^m}{\\text{minimize}} \\quad \\frac{1}{2} \\lVert u' - u \\rVert_2^2\n$$\n$$\n\\text{subject to} \\quad A u' \\le b\n$$\nwhere $u \\in \\mathbb{R}^m$ is the proposed action, $u' \\in \\mathbb{R}^m$ is the corrected action, and the inequality constraints $A u' \\le b$ represent the feasible set. The matrix $A \\in \\mathbb{R}^{q \\times m}$ and vector $b \\in \\mathbb{R}^q$ are constructed by stacking the linearized state safety constraints, $(HB) u' \\le h - Hx$, and the control input box constraints, $Iu' \\le U$ and $-Iu' \\le -L$.\n\nWe will solve this QP by deriving and solving its Lagrangian dual. This approach is powerful, especially when the number of constraints $q$ is large, and it transforms the problem into a QP with simpler non-negativity constraints, which can be efficiently solved using projected gradient descent.\n\nFirst, we form the Lagrangian by introducing the dual variables (Lagrange multipliers) $\\lambda \\in \\mathbb{R}^q$, with $\\lambda \\ge 0$:\n$$\n\\mathcal{L}(u', \\lambda) = \\frac{1}{2} \\lVert u' - u \\rVert_2^2 + \\lambda^T (A u' - b)\n$$\nFor an optimal primal-dual pair $(u'^*, \\lambda^*)$, the Karush-Kuhn-Tucker (KKT) conditions must be satisfied. The stationarity condition requires the gradient of the Lagrangian with respect to the primal variable $u'$ to be zero:\n$$\n\\nabla_{u'} \\mathcal{L}(u'^*, \\lambda^*) = (u'^* - u) + A^T \\lambda^* = 0\n$$\nFrom this, we can express the optimal primal solution $u'^*$ in terms of the optimal dual variable $\\lambda^*$:\n$$\nu'^*(\\lambda^*) = u - A^T \\lambda^*\n$$\n\nNext, we formulate the Lagrange dual function $g(\\lambda)$ by minimizing the Lagrangian over $u'$ for a fixed $\\lambda$:\n$$\ng(\\lambda) = \\inf_{u' \\in \\mathbb{R}^m} \\mathcal{L}(u', \\lambda)\n$$\nSubstituting the expression for $u'(\\lambda)$ from the stationarity condition into the Lagrangian, we get:\n$$\ng(\\lambda) = \\frac{1}{2} \\lVert (u - A^T \\lambda) - u \\rVert_2^2 + \\lambda^T (A(u - A^T \\lambda) - b)\n$$\n$$\ng(\\lambda) = \\frac{1}{2} \\lVert -A^T \\lambda \\rVert_2^2 + \\lambda^T A u - \\lambda^T A A^T \\lambda - \\lambda^T b\n$$\n$$\ng(\\lambda) = \\frac{1}{2} \\lambda^T A A^T \\lambda - \\lambda^T A A^T \\lambda + \\lambda^T A u - \\lambda^T b\n$$\n$$\ng(\\lambda) = -\\frac{1}{2} \\lambda^T (A A^T) \\lambda - \\lambda^T (b - A u)\n$$\n\nThe dual problem is to maximize $g(\\lambda)$ subject to the dual feasibility constraint $\\lambda \\ge 0$. Maximizing $g(\\lambda)$ is equivalent to minimizing $-g(\\lambda)$. Let this dual objective function be $\\mathcal{J}(\\lambda)$:\n$$\n\\underset{\\lambda \\in \\mathbb{R}^q}{\\text{minimize}} \\quad \\mathcal{J}(\\lambda) = \\frac{1}{2} \\lambda^T (A A^T) \\lambda + \\lambda^T (b - A u)\n$$\n$$\n\\text{subject to} \\quad \\lambda \\ge 0\n$$\nThis is a convex QP with simple non-negativity constraints, making it suitable for a projected gradient descent algorithm. The gradient of the dual objective $\\mathcal{J}(\\lambda)$ with respect to $\\lambda$ is:\n$$\n\\nabla_\\lambda \\mathcal{J}(\\lambda) = (A A^T) \\lambda + (b - A u)\n$$\nThe projected gradient descent algorithm iteratively updates $\\lambda$ to minimize $\\mathcal{J}(\\lambda)$. At each iteration $k$, the update rule is:\n$$\n\\lambda_{k+1} = \\Pi_{\\mathbb{R}^q_+} \\left( \\lambda_k - \\alpha \\nabla_\\lambda \\mathcal{J}(\\lambda_k) \\right)\n$$\nwhere $\\alpha > 0$ is a step size, and $\\Pi_{\\mathbb{R}^q_+}$ is the projection onto the non-negative orthant, which is simply an element-wise $\\max(0, \\cdot)$ operation. The step size $\\alpha$ can be chosen as $\\alpha = 1/L$, where $L$ is the Lipschitz constant of the gradient $\\nabla_\\lambda \\mathcal{J}(\\lambda)$, which is the largest eigenvalue of the positive-semidefinite matrix $A A^T$.\n\nThe overall algorithm is as follows:\n$1$. For each test case, assemble the constraint matrix $A$ and vector $b$ from the provided system parameters $x, B, H, h, L, U$.\n$2$. Initialize the dual variable $\\lambda_0 = 0$.\n$3$. Iteratively update $\\lambda_k$ using the projected gradient descent rule until convergence.\n$4$. Once the optimal dual variable $\\lambda^*$ is found, recover the optimal primal solution $u'^*$ using the relationship derived from the stationarity condition: $u'^* = u - A^T \\lambda^*$.\nThis procedure yields the corrected action $u'$ that is closest to the proposed action $u$ while satisfying all safety and control constraints.",
            "answer": "```python\nimport numpy as np\n\ndef solve_qp_dual_pgd(u, A, b, n_iter=50000, tol=1e-12):\n    \"\"\"\n    Solves a strictly convex QP using projected gradient descent on the dual problem.\n    Primal problem: min 0.5 * ||u' - u||^2_2 s.t. A u' <= b.\n    \n    Args:\n        u (np.ndarray): The vector to be projected, shape (m,).\n        A (np.ndarray): The constraint matrix, shape (q, m).\n        b (np.ndarray): The constraint vector, shape (q,).\n        n_iter (int): Maximum number of iterations for gradient descent.\n        tol (float): Tolerance for convergence.\n    \n    Returns:\n        np.ndarray: The optimal primal solution u', shape (m,).\n    \"\"\"\n    # Precompute matrices for the dual problem.\n    # The dual objective is J(lambda) = 0.5 * lambda.T @ Qd @ lambda + cd.T @ lambda\n    AT = A.T\n    Qd = A @ AT\n    cd = b - (A @ u)\n\n    # Determine step size alpha from the Lipschitz constant of the gradient.\n    # The Lipschitz constant is the largest eigenvalue of Qd = A @ A.T.\n    try:\n        # np.linalg.eigvalsh is for Hermitian (or real symmetric) matrices.\n        # It's more efficient and numerically stable for this case.\n        all_eigenvalues = np.linalg.eigvalsh(Qd)\n        # Check if there are any eigenvalues before taking max\n        L_grad = np.max(all_eigenvalues) if len(all_eigenvalues) > 0 else 0.0\n    except np.linalg.LinAlgError:\n        # Fallback to spectral norm if eigvalsh fails, though unlikely.\n        L_grad = np.linalg.norm(Qd, ord=2)\n\n    # A stable and effective step size is 1/L.\n    if L_grad < 1e-9:\n        alpha = 1.0\n    else:\n        alpha = 1.0 / L_grad\n\n    # Initialize dual variable lambda\n    q_dim = A.shape[0]\n    Lambda = np.zeros(q_dim)\n\n    # Projected gradient descent loop\n    for _ in range(n_iter):\n        # Calculate gradient of the dual objective\n        grad = Qd @ Lambda + cd\n        \n        # Update lambda\n        Lambda_new = Lambda - alpha * grad\n        \n        # Project lambda onto the non-negative orthant\n        Lambda_new = np.maximum(0., Lambda_new)\n        \n        # Check for convergence\n        if np.linalg.norm(Lambda_new - Lambda) < tol:\n            Lambda = Lambda_new\n            break\n        Lambda = Lambda_new\n        \n    # Recover the primal solution u' from the optimal dual variable lambda\n    u_prime = u - (AT @ Lambda)\n    return u_prime\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results in the required format.\n    \"\"\"\n    test_cases = [\n        # Test Case 1 (happy path, interior feasible)\n        {\n            \"u\": np.array([0.5, 0.5]), \"x\": np.array([0.2, 0.1]),\n            \"B\": np.array([[0.5, 0.2], [0.0, 0.8]]), \"H\": np.eye(2),\n            \"h\": np.array([1.0, 0.7]), \"L\": np.array([-1.0, -1.0]), \"U\": np.array([1.0, 1.0])\n        },\n        # Test Case 2 (violates a state constraint)\n        {\n            \"u\": np.array([1.0, 1.0]), \"x\": np.array([0.2, 0.1]),\n            \"B\": np.array([[0.5, 0.2], [0.0, 0.8]]), \"H\": np.eye(2),\n            \"h\": np.array([1.0, 0.7]), \"L\": np.array([-1.0, -1.0]), \"U\": np.array([1.0, 1.0])\n        },\n        # Test Case 3 (tighter safety, coupled constraint active)\n        {\n            \"u\": np.array([0.9, 0.9]), \"x\": np.array([0.5, 0.5]),\n            \"B\": np.array([[0.5, 0.2], [0.0, 0.8]]), \"H\": np.eye(2),\n            \"h\": np.array([1.0, 0.7]), \"L\": np.array([-1.0, -1.0]), \"U\": np.array([1.0, 1.0])\n        },\n        # Test Case 4 (violates only control bounds)\n        {\n            \"u\": np.array([1.5, -1.5]), \"x\": np.array([0.0, 0.0]),\n            \"B\": np.array([[0.5, 0.2], [0.0, 0.8]]), \"H\": np.eye(2),\n            \"h\": np.array([10.0, 10.0]), \"L\": np.array([-1.0, -1.0]), \"U\": np.array([1.0, 1.0])\n        },\n        # Test Case 5 (redundant constraints)\n        {\n            \"u\": np.array([0.3, 0.3]), \"x\": np.array([0.0, 0.0]),\n            \"B\": np.eye(2), \"H\": np.array([[1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]),\n            \"h\": np.array([0.2, 0.2, 0.2]), \"L\": np.array([-1.0, -1.0]), \"U\": np.array([1.0, 1.0])\n        }\n    ]\n\n    results = []\n    for params in test_cases:\n        u, x, B, H, h, L, U = params[\"u\"], params[\"x\"], params[\"B\"], params[\"H\"], params[\"h\"], params[\"L\"], params[\"U\"]\n        \n        m = u.shape[0]\n\n        # Construct the aggregated constraint matrix A and vector b\n        # A u' <= b\n        \n        # State constraints: (H B) u' <= h - H x\n        A_state = H @ B\n        b_state = h - (H @ x)\n        \n        # Control upper-bound constraints: I u' <= U\n        A_ctrl_up = np.eye(m)\n        b_ctrl_up = U\n        \n        # Control lower-bound constraints: -I u' <= -L\n        A_ctrl_low = -np.eye(m)\n        b_ctrl_low = -L\n        \n        # Stack all constraints\n        A = np.vstack([A_state, A_ctrl_up, A_ctrl_low])\n        b = np.concatenate([b_state, b_ctrl_up, b_ctrl_low])\n        \n        # Solve the QP to find the corrected action u_prime\n        u_prime = solve_qp_dual_pgd(u, A, b)\n        \n        # Round the result to 6 decimal places and append\n        rounded_result = [round(val, 6) for val in u_prime]\n        results.append(str(rounded_result))\n\n    # Print the final output in the exact required format\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n\n```"
        }
    ]
}