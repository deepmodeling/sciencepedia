{
    "hands_on_practices": [
        {
            "introduction": "掌握强化学习的第一步是深刻理解其核心目标：最优价值函数 $V^\\star$。这个练习将带你回归本源，直接利用贝尔曼最优方程求解一个小型马尔可夫决策过程（MDP）。通过构建并求解这组不动点方程，你将亲手计算出最优策略下的期望回报，从而为理解更高级的迭代和学习算法奠定坚实的理论基础。",
            "id": "3801996",
            "problem": "考虑一个源于多尺度建模的粗粒度控制器设计问题，该问题被构建为一个马尔可夫决策过程 (MDP)，其中微观涨落被聚合成宏观状态，并由控制动作影响其演化。设该 MDP 有两个状态（标记为 $1$ 和 $2$）和两个动作（标记为 $a_1$ 和 $a_2$）。该过程是无限时域的，并带有一个满足 $0  \\gamma  1$ 的折扣因子 $\\gamma$。受控转移概率由 $P(1 \\mid 1, a_1) = 0.9$，$P(2 \\mid 1, a_1) = 0.1$，$P(1 \\mid 1, a_2) = 0.5$，$P(2 \\mid 1, a_2) = 0.5$，$P(2 \\mid 2, a_1) = 1$ 和 $P(2 \\mid 2, a_2) = 1$ 给出。即时奖励为 $r(1, a_1) = 1$，$r(1, a_2) = 0$，$r(2, a_1) = 0$ 和 $r(2, a_2) = 0$。折扣因子为 $\\gamma = 0.95$。强化学习 (RL) 旨在计算最优价值函数 $V^\\star$，该函数对应于所有可行策略下折扣奖励期望总和的上确界。\n\n从无限时域折扣 MDP、马尔可夫性质和最优性原理的核心定义出发，推导出 $V^\\star(1)$ 和 $V^\\star(2)$ 必须满足的不动点关系，并利用这些关系精确计算该系统的最优价值函数 $V^\\star$。将最终结果表示为精确的解析表达式，不要进行四舍五入。",
            "solution": "首先对问题进行验证。\n\n### 第 1 步：提取已知条件\n-   **状态空间**：$S = \\{1, 2\\}$\n-   **动作空间**：$A = \\{a_1, a_2\\}$\n-   **折扣因子**：$\\gamma = 0.95$，满足条件 $0  \\gamma  1$。\n-   **转移概率**，$P(s' \\mid s, a)$:\n    -   $P(1 \\mid 1, a_1) = 0.9$\n    -   $P(2 \\mid 1, a_1) = 0.1$\n    -   $P(1 \\mid 1, a_2) = 0.5$\n    -   $P(2 \\mid 1, a_2) = 0.5$\n    -   $P(2 \\mid 2, a_1) = 1$，这意味着 $P(1 \\mid 2, a_1) = 0$。\n    -   $P(2 \\mid 2, a_2) = 1$，这意味着 $P(1 \\mid 2, a_2) = 0$。\n-   **奖励函数**，$r(s, a)$:\n    -   $r(1, a_1) = 1$\n    -   $r(1, a_2) = 0$\n    -   $r(2, a_1) = 0$\n    -   $r(2, a_2) = 0$\n-   **目标**：计算最优价值函数 $V^\\star$，它是一个向量 $(V^\\star(1), V^\\star(2))$。\n\n### 第 2 步：使用提取的已知条件进行验证\n该问题描述了一个有限状态、有限动作、无限时域的折扣马尔可夫决策过程 (MDP)。\n-   **科学依据**：该问题使用 MDP 的标准数学框架构建，这是强化学习和最优控制理论的基石。所有定义和关系都与既定理论一致。\n-   **适定性**：对于一个折扣因子 $\\gamma \\in [0, 1)$ 的有限 MDP，贝尔曼最优算子是一个压缩映射。根据巴拿赫不动点定理，该算子存在一个唯一的不动点，这个不动点正是最优价值函数 $V^\\star$。因此，存在一个唯一的、稳定的、有意义的解。\n-   **目标**：问题陈述使用了精确的数学定义和数值，没有歧义或主观语言。\n-   **不完整或矛盾的设置**：该问题是自洽的。所有必要的组成部分（状态、动作、转移概率、奖励和折扣因子）都已提供。每个状态-动作对的转移概率之和正确地等于 1。例如，$P(1 \\mid 1, a_1) + P(2 \\mid 1, a_1) = 0.9 + 0.1 = 1$。没有矛盾之处。\n\n### 第 3 步：结论与行动\n问题有效。将推导出一个完整的解。\n\n解决这个问题的核心原理是最优性原理，该原理指出，一个最优策略具有这样的性质：无论初始状态和初始决策是什么，余下的决策对于由第一个决策所产生的状态而言，必须构成一个最优策略。这个原理引出了贝尔曼最优方程，该方程为最优价值函数 $V^\\star(s)$ 提供了一个递归定义。对于任意状态 $s \\in S$，方程如下：\n$$V^\\star(s) = \\max_{a \\in A} \\left\\{ r(s, a) + \\gamma \\sum_{s' \\in S} P(s' \\mid s, a) V^\\star(s') \\right\\}$$\n这个方程建立了一个 $V^\\star$ 的分量必须满足的不动点关系系统。我们现在为每个状态 $s=1$ 和 $s=2$ 写出这些方程。\n\n对于状态 $s=2$：\n$V^\\star(2)$ 的贝尔曼最优方程是：\n$$V^\\star(2) = \\max_{a \\in \\{a_1, a_2\\}} \\left\\{ r(2, a) + \\gamma \\left( P(1 \\mid 2, a)V^\\star(1) + P(2 \\mid 2, a)V^\\star(2) \\right) \\right\\}$$\n我们对每个动作计算最大化内部的项：\n-   对于动作 $a_1$：$r(2, a_1) + \\gamma (P(1 \\mid 2, a_1)V^\\star(1) + P(2 \\mid 2, a_1)V^\\star(2)) = 0 + \\gamma (0 \\cdot V^\\star(1) + 1 \\cdot V^\\star(2)) = \\gamma V^\\star(2)$。\n-   对于动作 $a_2$：$r(2, a_2) + \\gamma (P(1 \\mid 2, a_2)V^\\star(1) + P(2 \\mid 2, a_2)V^\\star(2)) = 0 + \\gamma (0 \\cdot V^\\star(1) + 1 \\cdot V^\\star(2)) = \\gamma V^\\star(2)$。\n\n因此，$V^\\star(2)$ 的方程变为：\n$$V^\\star(2) = \\max\\{\\gamma V^\\star(2), \\gamma V^\\star(2)\\} = \\gamma V^\\star(2)$$\n这意味着 $V^\\star(2) - \\gamma V^\\star(2) = 0$，即 $V^\\star(2)(1 - \\gamma) = 0$。因为给定 $0  \\gamma  1$，所以 $(1 - \\gamma)$ 项不为零。因此，唯一的解是：\n$$V^\\star(2) = 0$$\n这是很直观的，因为状态 2 是一个吸收态，在所有后续时间步中都会产生零奖励。\n\n对于状态 $s=1$：\n$V^\\star(1)$ 的贝尔曼最优方程是：\n$$V^\\star(1) = \\max_{a \\in \\{a_1, a_2\\}} \\left\\{ r(1, a) + \\gamma \\left( P(1 \\mid 1, a)V^\\star(1) + P(2 \\mid 1, a)V^\\star(2) \\right) \\right\\}$$\n我们再次对每个动作计算最大化内部的项（即最优动作-价值函数 $Q^\\star(1, a)$），并代入 $V^\\star(2) = 0$：\n-   对于动作 $a_1$：\n    $Q^\\star(1, a_1) = r(1, a_1) + \\gamma \\left( P(1 \\mid 1, a_1)V^\\star(1) + P(2 \\mid 1, a_1)V^\\star(2) \\right)$\n    $Q^\\star(1, a_1) = 1 + \\gamma \\left( 0.9 \\cdot V^\\star(1) + 0.1 \\cdot 0 \\right) = 1 + 0.9\\gamma V^\\star(1)$\n-   对于动作 $a_2$：\n    $Q^\\star(1, a_2) = r(1, a_2) + \\gamma \\left( P(1 \\mid 1, a_2)V^\\star(1) + P(2 \\mid 1, a_2)V^\\star(2) \\right)$\n    $Q^\\star(1, a_2) = 0 + \\gamma \\left( 0.5 \\cdot V^\\star(1) + 0.5 \\cdot 0 \\right) = 0.5\\gamma V^\\star(1)$\n\n因此，$V^\\star(1)$ 的方程是：\n$$V^\\star(1) = \\max \\{ 1 + 0.9\\gamma V^\\star(1), 0.5\\gamma V^\\star(1) \\}$$\n由于所有奖励都是非负的，价值函数也必须是非负的，即 $V^\\star(1) \\ge 0$。又因为 $\\gamma > 0$，我们有 $0.9\\gamma > 0.5\\gamma$。因此，$1 + 0.9\\gamma V^\\star(1) > 0.5\\gamma V^\\star(1)$。所以，最大值总是通过选择动作 $a_1$ 来达到。\n$V^\\star(1)$ 的方程简化为一个线性方程：\n$$V^\\star(1) = 1 + 0.9\\gamma V^\\star(1)$$\n我们求解 $V^\\star(1)$：\n$$V^\\star(1) - 0.9\\gamma V^\\star(1) = 1$$\n$$V^\\star(1)(1 - 0.9\\gamma) = 1$$\n$$V^\\star(1) = \\frac{1}{1 - 0.9\\gamma}$$\n现在，我们代入给定的值 $\\gamma = 0.95$：\n$$V^\\star(1) = \\frac{1}{1 - 0.9(0.95)}$$\n$$V^\\star(1) = \\frac{1}{1 - 0.855}$$\n$$V^\\star(1) = \\frac{1}{0.145}$$\n为了得到一个精确的有理数表达式，我们将分母写成分数形式：$0.145 = \\frac{145}{1000} = \\frac{29}{200}$。\n$$V^\\star(1) = \\frac{1}{\\frac{29}{200}} = \\frac{200}{29}$$\n因此，最优价值函数是向量 $V^\\star = (V^\\star(1), V^\\star(2))$，即 $(\\frac{200}{29}, 0)$。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{200}{29}  0\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "在许多实际的控制与设计问题中，我们无法获知系统完整的动力学模型。这个练习将引导你从基于模型的动态规划转向模型无关的强化学习，探索时序差分（TD）学习的核心机制。你将通过一个具体的样本转换，为带函数近似的状态价值函数计算一次TD(0)更新，这正是让强化学习能够扩展到高维乃至连续状态空间的关键一步。",
            "id": "3802032",
            "problem": "一个控制器通过强化学习 (RL) 进行训练，以调节一个多尺度材料过程，其粗粒度状态 $s$ 汇总了微观结构统计量。在每个决策时间 $t$，状态 $s_t$ 由三个可观测量概括：微尺度能量密度 $e(s)$、一个晶格间距上的两点相关性 $c(s)$，以及界面面积密度 $a(s)$。采用线性状态值函数近似 $\\hat{V}(s;\\theta)$，其特征映射为\n$$\n\\phi(s) \\;=\\; \\begin{pmatrix} 1 \\\\ e(s) \\\\ c(s) \\\\ a(s) \\\\ e(s)\\,c(s) \\end{pmatrix} \\in \\mathbb{R}^{5}, \n$$\n因此\n$$\n\\hat{V}(s;\\theta) \\;=\\; \\theta^{\\top}\\phi(s),\n$$\n其中 $\\theta \\in \\mathbb{R}^{5}$ 是参数向量。考虑在固定策略下生成的以下单一樣本转移 $(s_t,a_t,r_t,s_{t+1})$：\n- 对于 $s_t$：$e(s_t)=\\frac{3}{2}$，$c(s_t)=\\frac{1}{5}$，$a(s_t)=\\frac{7}{10}$。\n- 对于 $s_{t+1}$：$e(s_{t+1})=\\frac{4}{3}$，$c(s_{t+1})=\\frac{2}{5}$，$a(s_{t+1})=\\frac{3}{5}$。\n- 即时奖励为 $r_t=\\frac{2}{5}$。\n- 折扣因子为 $\\gamma=\\frac{9}{10}$。\n\n初始参数为\n$$\n\\theta_t \\;=\\; \\begin{pmatrix} \\frac{1}{2} \\\\ -\\frac{1}{5} \\\\ \\frac{2}{3} \\\\ \\frac{1}{4} \\\\ -\\frac{3}{10} \\end{pmatrix},\n$$\n学习率为 $\\alpha=\\frac{3}{20}$。\n\n使用零步自举的时间差分 (TD) 学习 $\\mathrm{TD}(0)$ 以及上述的线性函数近似，对该单一样本在时间 $t$ 进行一次参数更新，并评估时间 $t$ 的时间差分误差。将您的最终答案表示为代表时间差分误差的单个精确分数。不要四舍五入。",
            "solution": "在尝试任何解答之前，需对问题陈述进行验证。\n\n### 步骤 1：提取已知条件\n- **状态值函数近似：** $\\hat{V}(s;\\theta) = \\theta^{\\top}\\phi(s)$\n- **特征映射：** $\\phi(s) = \\begin{pmatrix} 1 \\\\ e(s) \\\\ c(s) \\\\ a(s) \\\\ e(s)\\,c(s) \\end{pmatrix}$\n- **参数向量：** $\\theta \\in \\mathbb{R}^{5}$\n- **状态 $s_t$ 的可观测量：** $e(s_t)=\\frac{3}{2}$，$c(s_t)=\\frac{1}{5}$，$a(s_t)=\\frac{7}{10}$\n- **状态 $s_{t+1}$ 的可观测量：** $e(s_{t+1})=\\frac{4}{3}$，$c(s_{t+1})=\\frac{2}{5}$，$a(s_{t+1})=\\frac{3}{5}$\n- **即时奖励：** $r_t=\\frac{2}{5}$\n- **折扣因子：** $\\gamma=\\frac{9}{10}$\n- **初始参数向量：** $\\theta_t = \\begin{pmatrix} \\frac{1}{2} \\\\ -\\frac{1}{5} \\\\ \\frac{2}{3} \\\\ \\frac{1}{4} \\\\ -\\frac{3}{10} \\end{pmatrix}$\n- **学习率：** $\\alpha=\\frac{3}{20}$\n- **算法：** 零步自举的时间差分学习，$\\mathrm{TD}(0)$。\n- **目标：** 评估时间 $t$ 的时间差分误差，记为 $\\delta_t$。\n\n### 步骤 2：使用提取的已知条件进行验证\n该问题是强化学习领域中一个明确定义的计算练习。它在科学上基于时间差分学习与线性函数近似的既定原则。所有必要的数据和定义均已提供，且没有内部矛盾。多尺度材料建模的背景具有科学相关性，并为此类算法的应用提供了一个现实的场景。该问题是客观、明確且可解的。没有违反验证标准。\n\n### 步骤 3：结论与行动\n问题有效。将提供合理的解答。\n\n该问题要求使用 $\\mathrm{TD}(0)$ 算法计算单一样本更新的时间差分 (TD) 误差。对于状态值函数近似 $\\hat{V}(s;\\theta)$，时间步 $t$ 的 TD 误差（记为 $\\delta_t$）定义为 TD 目标与状态 $s_t$ 当前值估计之间的差值。\n\nTD 目标是即时奖励 $r_t$ 与下一状态 $s_{t+1}$ 的折扣值之和。TD 误差的公式为：\n$$\n\\delta_t = r_t + \\gamma \\hat{V}(s_{t+1}; \\theta_t) - \\hat{V}(s_t; \\theta_t)\n$$\n在这里，两个值估计都是使用时间 $t$ 的参数向量 $\\theta_t$ 计算的。值函数由线性近似 $\\hat{V}(s;\\theta_t) = \\theta_t^{\\top}\\phi(s)$ 给出。\n\n首先，我们使用给定的状态可观测量构建特征向量 $\\phi(s_t)$ 和 $\\phi(s_{t+1})$。\n\n对于状态 $s_t$：\n$e(s_t) = \\frac{3}{2}$，$c(s_t) = \\frac{1}{5}$，$a(s_t) = \\frac{7}{10}$。\n交互项为 $e(s_t)c(s_t) = \\frac{3}{2} \\times \\frac{1}{5} = \\frac{3}{10}$。\n特征向量 $\\phi(s_t)$ 为：\n$$\n\\phi(s_t) = \\begin{pmatrix} 1 \\\\ \\frac{3}{2} \\\\ \\frac{1}{5} \\\\ \\frac{7}{10} \\\\ \\frac{3}{10} \\end{pmatrix}\n$$\n\n对于状态 $s_{t+1}$：\n$e(s_{t+1}) = \\frac{4}{3}$，$c(s_{t+1}) = \\frac{2}{5}$，$a(s_{t+1}) = \\frac{3}{5}$。\n交互项为 $e(s_{t+1})c(s_{t+1}) = \\frac{4}{3} \\times \\frac{2}{5} = \\frac{8}{15}$。\n特征向量 $\\phi(s_{t+1})$ 为：\n$$\n\\phi(s_{t+1}) = \\begin{pmatrix} 1 \\\\ \\frac{4}{3} \\\\ \\frac{2}{5} \\\\ \\frac{3}{5} \\\\ \\frac{8}{15} \\end{pmatrix}\n$$\n\n接下来，我们使用给定的参数向量 $\\theta_t$ 计算状态值估计 $\\hat{V}(s_t; \\theta_t)$ 和 $\\hat{V}(s_{t+1}; \\theta_t)$。\n\n$s_t$ 的值估计为：\n$$\n\\hat{V}(s_t; \\theta_t) = \\theta_t^{\\top}\\phi(s_t) = \\begin{pmatrix} \\frac{1}{2}  -\\frac{1}{5}  \\frac{2}{3}  \\frac{1}{4}  -\\frac{3}{10} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ \\frac{3}{2} \\\\ \\frac{1}{5} \\\\ \\frac{7}{10} \\\\ \\frac{3}{10} \\end{pmatrix}\n$$\n$$\n\\hat{V}(s_t; \\theta_t) = \\left(\\frac{1}{2}\\right)(1) + \\left(-\\frac{1}{5}\\right)\\left(\\frac{3}{2}\\right) + \\left(\\frac{2}{3}\\right)\\left(\\frac{1}{5}\\right) + \\left(\\frac{1}{4}\\right)\\left(\\frac{7}{10}\\right) + \\left(-\\frac{3}{10}\\right)\\left(\\frac{3}{10}\\right)\n$$\n$$\n\\hat{V}(s_t; \\theta_t) = \\frac{1}{2} - \\frac{3}{10} + \\frac{2}{15} + \\frac{7}{40} - \\frac{9}{100}\n$$\n为了将这些分数相加，我们找到一个公分母。$2$、$10$、$15$、$40$ 和 $100$ 的最小公倍数是 $600$。\n$$\n\\hat{V}(s_t; \\theta_t) = \\frac{300}{600} - \\frac{180}{600} + \\frac{80}{600} + \\frac{105}{600} - \\frac{54}{600} = \\frac{300 - 180 + 80 + 105 - 54}{600} = \\frac{251}{600}\n$$\n\n$s_{t+1}$ 的值估计为：\n$$\n\\hat{V}(s_{t+1}; \\theta_t) = \\theta_t^{\\top}\\phi(s_{t+1}) = \\begin{pmatrix} \\frac{1}{2}  -\\frac{1}{5}  \\frac{2}{3}  \\frac{1}{4}  -\\frac{3}{10} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ \\frac{4}{3} \\\\ \\frac{2}{5} \\\\ \\frac{3}{5} \\\\ \\frac{8}{15} \\end{pmatrix}\n$$\n$$\n\\hat{V}(s_{t+1}; \\theta_t) = \\left(\\frac{1}{2}\\right)(1) + \\left(-\\frac{1}{5}\\right)\\left(\\frac{4}{3}\\right) + \\left(\\frac{2}{3}\\right)\\left(\\frac{2}{5}\\right) + \\left(\\frac{1}{4}\\right)\\left(\\frac{3}{5}\\right) + \\left(-\\frac{3}{10}\\right)\\left(\\frac{8}{15}\\right)\n$$\n$$\n\\hat{V}(s_{t+1}; \\theta_t) = \\frac{1}{2} - \\frac{4}{15} + \\frac{4}{15} + \\frac{3}{20} - \\frac{24}{150}\n$$\n第二项和第三项抵消。简化最后一项得到 $\\frac{24}{150} = \\frac{4}{25}$。\n$$\n\\hat{V}(s_{t+1}; \\theta_t) = \\frac{1}{2} + \\frac{3}{20} - \\frac{4}{25}\n$$\n$2$、$20$ 和 $25$ 的最小公倍数是 $100$。\n$$\n\\hat{V}(s_{t+1}; \\theta_t) = \\frac{50}{100} + \\frac{15}{100} - \\frac{16}{100} = \\frac{50 + 15 - 16}{100} = \\frac{49}{100}\n$$\n\n最后，我们使用其定义计算 TD 误差 $\\delta_t$。\n$$\n\\delta_t = r_t + \\gamma \\hat{V}(s_{t+1}; \\theta_t) - \\hat{V}(s_t; \\theta_t)\n$$\n代入给定的值和我们计算出的估计值：\n$$\n\\delta_t = \\frac{2}{5} + \\left(\\frac{9}{10}\\right)\\left(\\frac{49}{100}\\right) - \\frac{251}{600}\n$$\n首先，计算 TD 目标，即表达式的第一部分：\n$$\n\\text{TD 目标} = \\frac{2}{5} + \\frac{9 \\times 49}{10 \\times 100} = \\frac{2}{5} + \\frac{441}{1000} = \\frac{400}{1000} + \\frac{441}{1000} = \\frac{841}{1000}\n$$\n现在，减去当前状态的值估计：\n$$\n\\delta_t = \\frac{841}{1000} - \\frac{251}{600}\n$$\n$1000$ 和 $600$ 的最小公倍数是 $3000$。\n$$\n\\delta_t = \\frac{841 \\times 3}{3000} - \\frac{251 \\times 5}{3000} = \\frac{2523}{3000} - \\frac{1255}{3000} = \\frac{2523 - 1255}{3000} = \\frac{1268}{3000}\n$$\n这个分数可以通过将分子和分母除以它们的最大公约数来简化。两者都是偶数。\n$$\n\\delta_t = \\frac{1268 \\div 4}{3000 \\div 4} = \\frac{317}{750}\n$$\n数字 $317$ 是素数，而 $750$ 的质因数是 $2 \\cdot 3 \\cdot 5^3$。因此，该分数已是最简形式。\n时间差分误差为 $\\frac{317}{750}$。",
            "answer": "$$\n\\boxed{\\frac{317}{750}}\n$$"
        },
        {
            "introduction": "在真实的工程应用中，系统的状态往往无法被完美观测，智能体必须在不确定性下做出决策。此练习将你引入部分可观测马尔可夫决策过程（POMDP）的框架，这是处理状态不确定性的标准模型。你将通过执行一次完整的贝叶斯信念更新，学习智能体如何根据采取的行动和收到的不完整观测，来推断和更新其对系统真实状态的信念分布。",
            "id": "3802030",
            "problem": "一位设计者正在使用强化学习（RL）来控制一个嵌入在多尺度流水线中的粗粒度、双状态宏观模型。宏观状态 $1$ 和 $2$ 代表了不同的微观结构体系。该系统被建模为一个部分可观察马尔可夫决策过程（POMDP），其中隐藏的宏观状态在控制动作下进行马尔可夫演化，并且一个传感器产生依赖于该宏观状态的观测。目标是在执行一个控制并接收到一个观测后，更新对宏观状态的信念。\n\n基础理论：使用贝叶斯定理和全概率定律，并结合控制动作下状态转移的马尔可夫性质。具体来说，从一个关于状态的先验信念 $b$、由所选动作 $a$ 引起的转移核 $P(s' \\mid s, a)$ 以及一个观测模型 $O(o \\mid s')$ 出发，更新过程通过全概率定律预测下一状态分布，然后通过贝叶斯定理对观测进行条件化。\n\n考虑以下设置：\n- 状态的先验信念为 $b=(0.6, 0.4)$，即 $b(1)=0.6$ 和 $b(2)=0.4$。\n- 在施加的控制 $a$ 下，下一状态的转移概率为 $P(1 \\mid 1, a)=0.8$，$P(2 \\mid 1, a)=0.2$，$P(1 \\mid 2, a)=0.1$ 和 $P(2 \\mid 2, a)=0.9$。\n- 传感器发出一个二元观测 $o \\in \\{0,1\\}$，其似然为 $O(o=1 \\mid 1)=0.9$ 和 $O(o=1 \\mid 2)=0.2$。\n\n你执行了动作 $a$ 并观测到 $o=1$。从贝叶斯定理和全概率定律出发，计算在结合此动作和观测后，对这两个宏观状态的后验信念 $b^{+}=(b^{+}(1), b^{+}(2))$。将最终的信念分量表示为精确的简化有理数。不需要四舍五入。最终答案应以一个包含两个元素的单行向量形式呈现。",
            "solution": "问题陈述经确认为科学上合理、良定且客观。在部分可观察马尔可夫决策过程（POMDP）中进行标准贝叶斯信念更新所需的所有必要参数均已提供，且不存在内部矛盾或歧义。\n\n设宏观状态空间为 $\\mathcal{S} = \\{1, 2\\}$。先验信念是这些状态上的一个概率分布，记为 $b(s) = P(s)$，其中 $s \\in \\mathcal{S}$。给定的先验信念向量为 $b = (b(1), b(2)) = (0.6, 0.4)$。\n\n从先验信念 $b$ 到后验信念 $b^{+}$ 的信念更新过程（在执行动作 $a$ 并接收到观测 $o$ 后）可以分解为两个步骤：预测和修正。\n\n1.  **预测步骤**：我们首先计算预测信念 $\\hat{b}(s')$，它是在执行动作 $a$ 之后、但在结合观测 $o$ 之前，关于下一状态 $s'$ 的概率分布。这通过使用全概率定律，对当前状态 $s$ 进行边缘化来计算。\n    $$\n    \\hat{b}(s') = P(s'|a, b) = \\sum_{s \\in \\mathcal{S}} P(s'|s, a)b(s)\n    $$\n    项 $P(s'|s, a)$ 是给定的状态转移概率。\n\n2.  **修正步骤**：然后，我们使用贝叶斯定理，利用来自观测 $o$ 的新信息来更新预测信念，从而获得后验信念 $b^{+}(s')$。\n    $$\n    b^{+}(s') = P(s' | o, a, b) = \\frac{P(o | s', a, b) P(s'| a, b)}{P(o | a, b)}\n    $$\n    假设观测 $o$ 仅依赖于新状态 $s'$，我们有 $P(o | s', a, b) = O(o|s')$，其中 $O$ 是观测模型。项 $P(s'| a, b)$ 是在第一步中计算出的预测信念 $\\hat{b}(s')$。分母 $P(o|a,b)$ 是一个归一化常数（也称为观测的证据或边缘似然），通过将分子对所有可能的下一状态 $s'$ 求和来计算。\n    $$\n    P(o|a,b) = \\sum_{s' \\in \\mathcal{S}} O(o|s')\\hat{b}(s')\n    $$\n    因此，后验信念为：\n    $$\n    b^{+}(s') = \\frac{O(o|s')\\hat{b}(s')}{\\sum_{s'' \\in \\mathcal{S}} O(o|s'')\\hat{b}(s'')}\n    $$\n    这可以写成 $b^{+}(s') = \\eta O(o|s')\\hat{b}(s')$，其中 $\\eta$ 是归一化常数。\n\n我们现在将此框架应用于给定值。\n给定条件如下：\n-   先验信念：$b(1) = 0.6$ 和 $b(2) = 0.4$。\n-   动作：$a$。\n-   动作 $a$ 下的转移概率：\n    $P(1 | 1, a) = 0.8$, $P(2 | 1, a) = 0.2$。\n    $P(1 | 2, a) = 0.1$, $P(2 | 2, a) = 0.9$。\n-   观测：$o = 1$。\n-   对于 $o=1$ 的观测似然：$O(1 | 1) = 0.9$，$O(1 | 2) = 0.2$。\n\n首先，我们执行预测步骤，以找到对于 $s' \\in \\{1, 2\\}$ 的预测信念 $\\hat{b}(s')$。\n对于 $s' = 1$：\n$$\n\\hat{b}(1) = P(1|1,a)b(1) + P(1|2,a)b(2) = (0.8)(0.6) + (0.1)(0.4) = 0.48 + 0.04 = 0.52\n$$\n对于 $s' = 2$：\n$$\n\\hat{b}(2) = P(2|1,a)b(1) + P(2|2,a)b(2) = (0.2)(0.6) + (0.9)(0.4) = 0.12 + 0.36 = 0.48\n$$\n作为检验，$\\hat{b}(1) + \\hat{b}(2) = 0.52 + 0.48 = 1.0$，这证实了它是一个有效的概率分布。预测信念为 $(\\hat{b}(1), \\hat{b}(2)) = (0.52, 0.48)$。\n\n接下来，我们使用观测 $o=1$ 执行修正步骤。我们通过将预测信念乘以观测似然来计算未归一化的后验信念分量，我们称之为 $\\tilde{b}^{+}(s')$。\n$$\n\\tilde{b}^{+}(s') = O(1 | s') \\hat{b}(s')\n$$\n对于 $s' = 1$：\n$$\n\\tilde{b}^{+}(1) = O(1 | 1) \\hat{b}(1) = (0.9)(0.52) = 0.468\n$$\n对于 $s' = 2$：\n$$\n\\tilde{b}^{+}(2) = O(1 | 2) \\hat{b}(2) = (0.2)(0.48) = 0.096\n$$\n现在，我们通过对这些分量求和来找到归一化常数 $\\eta^{-1}$。这就是证据 $P(o=1|a, b)$。\n$$\n\\eta^{-1} = \\tilde{b}^{+}(1) + \\tilde{b}^{+}(2) = 0.468 + 0.096 = 0.564\n$$\n最后，我们进行归一化以找到后验信念 $b^{+} = (b^{+}(1), b^{+}(2))$。\n$$\nb^{+}(1) = \\frac{\\tilde{b}^{+}(1)}{\\eta^{-1}} = \\frac{0.468}{0.564}\n$$\n$$\nb^{+}(2) = \\frac{\\tilde{b}^{+}(2)}{\\eta^{-1}} = \\frac{0.096}{0.564}\n$$\n问题要求答案为精确的简化有理数。\n$$\nb^{+}(1) = \\frac{468}{564} = \\frac{12 \\times 39}{12 \\times 47} = \\frac{39}{47}\n$$\n$$\nb^{+}(2) = \\frac{96}{564} = \\frac{12 \\times 8}{12 \\times 47} = \\frac{8}{47}\n$$\n作为最终检验，后验信念的分量之和为 $1$：$\\frac{39}{47} + \\frac{8}{47} = \\frac{47}{47} = 1$。\n后验信念向量为 $b^{+} = (\\frac{39}{47}, \\frac{8}{47})$。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{39}{47}  \\frac{8}{47} \\end{pmatrix}}\n$$"
        }
    ]
}