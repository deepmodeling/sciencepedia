## 引言
[强化学习](@entry_id:141144)（Reinforcement Learning, RL）作为人工智能的一个核心分支，正在迅速成为解决动态环境下最优决策问题的关键技术。特别是在控制与设计领域，传统方法在面对高维、[非线性](@entry_id:637147)以及模型未知的复杂系统时常常力不从心，而[强化学习](@entry_id:141144)提供了一套强大的、数据驱动的理论与方法框架来应对这些挑战。然而，从掌握其数学原理到洞悉其在不同学科中的深刻应用，存在着一条需要跨越的知识鸿沟。本文旨在系统性地搭建这座桥梁，引领读者深入探索[强化学习](@entry_id:141144)的理论核心与实践前沿。

在文章中，我们将首先在“**原理与机制**”一章中，奠定坚实的理论基础，从马尔可夫决策过程的形式化建模，到动态规划与贝尔曼最优性原理，再到[Q学习](@entry_id:144980)、[策略梯度](@entry_id:635542)等核心学习算法。随后，在“**应用与交叉学科联系**”一章，我们将视野拓展至实际应用，探讨强化学习如何与[最优控制理论](@entry_id:139992)结合，如何解决安全与鲁棒性等关键问题，并揭示其在[多尺度系统](@entry_id:1128345)、神经科学及[计算精神病学](@entry_id:187590)等领域的交叉融合。最后，通过“**动手实践**”部分，读者将有机会通过具体问题来巩固和应用所学知识。通过这一结构化的学习路径，本文将帮助你全面掌握强化学习在控制与设计中的应用。

## 原理与机制

在“引言”部分，我们概述了强化学习在控制与设计领域的巨大潜力。本章将深入探讨支撑这些应用的核心科学原理与算法机制。我们将从如何将[序贯决策问题](@entry_id:136955)形式化为数学模型开始，继而阐述最优性原理，并最终详细介绍从数据中学习[最优策略](@entry_id:138495)的各类算法。

### 决策过程的形式化建模

强化学习的核心是智能体（agent）与环境（environment）的交互。为了进行严谨的分析与设计，我们需要一个数学框架来描述这种交互。最核心的框架是[马尔可夫决策过程](@entry_id:140981)（Markov Decision Process, MDP）。

#### [马尔可夫决策过程](@entry_id:140981)（MDP）

一个**[马尔可夫决策过程](@entry_id:140981)**为一个五元组 $(\mathcal{S}, \mathcal{A}, P, r, \gamma)$，其中：
- $\mathcal{S}$ 是**[状态空间](@entry_id:160914)**，包含了智能体可能感知到的所有环境情况。
- $\mathcal{A}$ 是**动作空间**，包含了智能体可以执行的所有动作。
- $P(s' | s, a) = \mathbb{P}(s_{t+1}=s' | s_t=s, a_t=a)$ 是**状态转移核**，描述了在状态 $s$ 执行动作 $a$ 后，环境转移到下一状态 $s'$ 的概率。
- $r(s, a)$ 是**[奖励函数](@entry_id:138436)**，表示在状态 $s$ 执行动作 $a$ 后获得的即时标量奖励。
- $\gamma \in [0, 1)$ 是**折扣因子**，用于权衡即时奖励与未来奖励的重要性。

此框架的核心假设是**[马尔可夫性质](@entry_id:139474)（Markov Property）**，即系统的下一状态 $s_{t+1}$ 的概率分布只依赖于当前的状态 $s_t$ 和动作 $a_t$，而与所有之前的历史状态和动作无关。数学上表示为：
$$
\mathbb{P}(s_{t+1} | s_t, a_t, s_{t-1}, a_{t-1}, \dots) = \mathbb{P}(s_{t+1} | s_t, a_t)
$$
[马尔可夫性质](@entry_id:139474)是控制设计的基础，因为它意味着当前状态是未来决策的**充分统计量**。如果该性质成立，最优决策只需要依赖于当前状态，而无需记忆整个历史。这使得寻找一个固定的、从状态到动作的映射，即**静态马尔可夫策略** $\pi: \mathcal{S} \to \mathcal{A}$，成为可能，而不会损失相对于依赖历史的复杂策略的性能。

在许多工程与科学问题中，底层的物理系统本身就是马尔可夫的。例如，一个由微观状态 $x_t \in \mathbb{R}^n$ 演化的[随机系统](@entry_id:187663) $x_{t+1} = F(x_t, a_t, \xi_t)$，其中 $\xi_t$ 是[独立同分布](@entry_id:169067)的随机扰动，其本身就满足[马尔可夫性质](@entry_id:139474) 。在这种情况下，我们可以直接将[状态空间](@entry_id:160914)设为 $\mathcal{S} = \mathbb{R}^n$，从而构建一个有效的MDP。

然而，在**[多尺度系统](@entry_id:1128345)（multiscale systems）**中，我们往往希望在更粗糙的宏观尺度上进行控制。假设我们通过一个[粗粒化](@entry_id:141933)映射 $h: \mathbb{R}^n \to \mathcal{Z}$ 从微观状态 $x_t$ 得到宏观状态 $s_t = h(x_t)$。此时，宏观过程 $\{s_t\}$ 是否仍然满足[马尔可夫性质](@entry_id:139474)，是一个关键问题。如果对于任意宏观状态 $s$ 和动作 $a$，所有属于该宏观状态的微观状态 $x$（即 $h(x)=s$）在执行动作 $a$ 后，其下一时刻的宏观状态 $h(x_{t+1})$ 的概率分布都相同，那么这个宏观过程才是马尔可夫的。这种特殊的[粗粒化](@entry_id:141933)条件，有时被称为**受控集总（controlled lumping）**，是确保从复杂微观动力学中导出的宏观模型是有效MDP的关键 。

一个更普遍且强大的方法是**平均化原理（averaging principle）**。考虑一个具有快慢两个时间尺度的系统，其中慢变量为 $x_t$，快变量为 $y_t$。在每个慢时间步之间，快变量 $y_t$ 在固定的慢变量 $x_t$ 和控制 $u_t$ 的影响下迅速演化并达到一个唯一的[平稳分布](@entry_id:194199) $\mu_x^u(\mathrm{d}y)$。在这种情况下，我们可以通过对快变量的[平稳分布](@entry_id:194199)进行积分，来定义一个只依赖于慢变量的有效宏观MDP 。例如，有效的[奖励函数](@entry_id:138436)和转移核可以定义为：
$$
\bar{r}(x, u) \triangleq \mathbb{E}_{y \sim \mu_x^u} [r(x, y, u)], \quad \bar{P}(x' | x, u) \triangleq \mathbb{E}_{y \sim \mu_x^u} [P(x' | x, y, u)]
$$
这种平均化思想同样适用于[连续时间系统](@entry_id:276553)。对于由慢变量 $x_t$ 和快变量 $y_t$ 组成的[随机微分方程](@entry_id:146618)（SDE）系统，在[尺度分离](@entry_id:270204)极限下（$\epsilon \to 0$），系统的慢动态可以通过对快变量的平稳测度积分来近似，从而得到一个有效的慢SDE和相应的平均成本[贝尔曼方程](@entry_id:1121499) 。

#### 部分可观测[马尔可夫决策过程](@entry_id:140981)（[POMDP](@entry_id:637181)）

在许多实际应用中，系统的真实状态 $s_t$ 无法被直接观测。智能体只能接收到一个与状态相关的**观测** $o_t \in \mathcal{O}$。这种设定被称为**部分可观测马尔可夫决策过程（Partially Observable Markov Decision Process, [POMDP](@entry_id:637181)）**。一个[POMDP](@entry_id:637181)在MDP的基础上增加了观测空间 $\mathcal{O}$ 和观测模型 $O(o|s) = \mathbb{P}(o_t=o|s_t=s)$。

例如，在一个[多尺度系统](@entry_id:1128345)中，状态 $s_t=(x_t, m_t)$ 由连续的粗尺度变量 $x_t$ 和离散的微观模式 $m_t$ 组成。我们可能只能通过带有[高斯噪声](@entry_id:260752)的传感器得到关于 $x_t$ 的线性测量值 $o_t^{(x)}$，并通过一个有[混淆矩阵](@entry_id:1124649)的分类器得到关于 $m_t$ 的不准确读数 $o_t^{(m)}$。此时，观测空间为 $\mathcal{O} = \mathbb{R}^p \times \{1, \dots, K\}$，观测模型则由高斯似然和分类概率的乘积构成 。

由于状态未知，[马尔可夫性质](@entry_id:139474)对于观测历史不再成立。然而，我们可以通过维护一个关于当前真实状态的**信念状态（belief state）** $b_t(s) = \mathbb{P}(s_t=s | h_t)$ 来恢复马尔可夫性，其中 $h_t$ 是到时刻 $t$ 为止的完整观测-动作历史。信念状态 $b_t$ 是历史的充分统计量。[信念状态](@entry_id:195111)的演化遵循一个确定性的**[贝叶斯滤波](@entry_id:137269)（Bayes filter）**过程：
1.  **预测**：根据上一时刻的信念 $b_t$ 和动作 $a_t$，预测下一时刻的[先验信念](@entry_id:264565) $b_{t+1}^-$：
    $$
    b_{t+1}^-(s') = \sum_{s \in \mathcal{S}} P(s' | s, a_t) b_t(s)
    $$
2.  **更新**：在接收到新的观测 $o_{t+1}$ 后，使用[贝叶斯法则](@entry_id:275170)更新信念：
    $$
    b_{t+1}(s') = \frac{O(o_{t+1} | s') b_{t+1}^-(s')}{\mathbb{P}(o_{t+1} | h_t, a_t)}
    $$
    其中分母是[归一化常数](@entry_id:752675)。

通过这种方式，原始的[POMDP](@entry_id:637181)被转化为一个在连续的信念空间上的**信念MDP**。这个信念MDP是完全可观测的，其状态是信念 $b_t$，动作空间不变，转移由[贝叶斯滤波](@entry_id:137269)器和观测概率共同决定，而奖励则是在信念上的期望奖励 $\bar{r}(b_t, a_t) = \sum_s b_t(s) r(s, a_t)$。这为在部分可观测环境下应用[强化学习](@entry_id:141144)算法提供了理论基础 。

### 最优性原理与[动态规划](@entry_id:141107)

在建立了MDP模型后，我们的目标是找到一个策略 $\pi(a|s)$，它能最大化从任意初始状态开始的期望累积[折扣](@entry_id:139170)奖励。这个[期望值](@entry_id:150961)被称为**价值函数（value function）**。

对于一个给定的策略 $\pi$，其状态价值函数 $V^\pi(s)$ 定义为：
$$
V^\pi(s) = \mathbb{E}_\pi \left[ \sum_{t=0}^{\infty} \gamma^t r(s_t, a_t) | s_0 = s \right]
$$
其中动作 $a_t \sim \pi(\cdot|s_t)$。这个价值函数满足**贝尔曼期望方程（Bellman expectation equation）**：
$$
V^\pi(s) = \mathbb{E}_\pi [r(s_t, a_t) + \gamma V^\pi(s_{t+1}) | s_t = s]
$$
它将一个状态的价值与其后继状态的价值联系起来。

我们的最终目标是找到**最优价值函数** $V^*(s) = \sup_\pi V^\pi(s)$。最优[价值函数](@entry_id:144750)满足一个特殊的[贝尔曼方程](@entry_id:1121499)，即**贝尔曼最优方程（Bellman optimality equation）**：
$$
V^*(s) = \max_{a \in \mathcal{A}} \left\{ r(s, a) + \gamma \sum_{s' \in \mathcal{S}} P(s'|s, a) V^*(s') \right\}
$$
这个方程表达了**最优性原理（principle of optimality）**：一个[最优策略](@entry_id:138495)的子策略也必须是最优的。具体来说，无论初始状态和初始决策如何，其余的决策必须构成一个相对于由初始决策导致的状态的最优策略。

一旦我们得到了最优[价值函数](@entry_id:144750) $V^*$，就可以通过对其进行一步贪心来提取[最优策略](@entry_id:138495) $\pi^*$：
$$
\pi^*(s) \in \arg\max_{a \in \mathcal{A}} \left\{ r(s, a) + \gamma \sum_{s' \in \mathcal{S}} P(s'|s, a) V^*(s') \right\}
$$
这种基于价值函数进行贪心选择的策略，被称为**贪心策略（greedy policy）** 。对于动作价值函数 $Q^*(s, a)$，贝尔曼最优方程和最优策略提取更为直接：
$$
Q^*(s, a) = r(s, a) + \gamma \sum_{s' \in \mathcal{S}} P(s'|s, a) \max_{a'} Q^*(s', a')
$$
$$
\pi^*(s) \in \arg\max_{a \in \mathcal{A}} Q^*(s, a)
$$
当MDP模型（即 $P$ 和 $r$）已知时，可以通过**动态规划（Dynamic Programming, DP）**方法，如[价值迭代](@entry_id:146512)或策略迭代，来求解贝尔曼最优方程。然而，在强化学习中，我们通常假设模型是未知的。

### 从经验中学习：强化学习算法

当环境模型未知时，智能体必须通过与环境的直接交互（即经验样本 $(s_t, a_t, r_t, s_{t+1})$）来学习[最优策略](@entry_id:138495)。

#### 无模型价值学习

这类方法直接估计最优价值函数，而不试图学习环境模型。

**Q-学习（Q-learning）** 是一种经典的无模型、**离策略（off-policy）** 算法。离策略意味着学习[最优策略](@entry_id:138495)的同时，智能体可以遵循另一个（通常是更具探索性的）行为策略来生成数据。Q-学习的目标是直接学习最优动作[价值函数](@entry_id:144750) $Q^*$。其更新规则非常简洁：
$$
Q_{t+1}(s_t, a_t) \leftarrow Q_t(s_t, a_t) + \alpha_t \left[ r_t + \gamma \max_{a' \in \mathcal{A}} Q_t(s_{t+1}, a') - Q_t(s_t, a_t) \right]
$$
这里的 $\alpha_t$ 是学习率。核心思想是用观测到的奖励 $r_t$ 和对下一状态价值的当前估计 $\gamma \max_{a'} Q_t(s_{t+1}, a')$（称为**TD目标**）来“校正”当前对 $Q_t(s_t, a_t)$ 的估计。括号中的项 $r_t + \gamma \max_{a'} Q_t(s_{t+1}, a') - Q_t(s_t, a_t)$ 被称为**时序差分（Temporal Difference, TD）误差**。

对于表格型Q-学习（即[Q值](@entry_id:265045)存储在一个大表中），其收敛到最优值 $Q^*$ 有着严格的理论保证，前提是满足以下条件 ：
1.  MDP是有限的，奖励是有界的。
2.  行为策略要满足**无限探索下的贪心（Greedy in the Limit with Infinite Exploration, GLIE）**。这意味着每个状态-动作对 $(s,a)$ 都会被无限次访问，并且随着学习的进行，策略最终会收敛到相对于当前Q估计的贪心策略。
3.  学习率 $\alpha_t(s,a)$ 必须满足[随机近似](@entry_id:270652)理论的**[Robbins-Monro条件](@entry_id:634006)**：$\sum_t \alpha_t(s,a) = \infty$ 且 $\sum_t \alpha_t^2(s,a)  \infty$。第一个条件确保算法可以克服初始值的影响，持续学习；第二个条件确保[学习率](@entry_id:140210)最终会衰减，使得[Q值](@entry_id:265045)收敛。

#### [函数近似](@entry_id:141329)

当[状态空间](@entry_id:160914)或动作空间非常大甚至是连续时，使用表格来存储价值函数变得不可行。**[函数近似](@entry_id:141329)（function approximation）**通过使用一个带参数的函数 $V_w(s)$ 或 $Q_w(s,a)$ 来近似价值函数，从而解决了这个问题。

一个常用且理论上研究得最透彻的方法是**线性[函数近似](@entry_id:141329)**，其中价值被表示为状态[特征向量](@entry_id:151813) $\phi(s) \in \mathbb{R}^k$ 的[线性组合](@entry_id:154743)：
$$
V_w(s) = \phi(s)^\top w
$$
其中 $w \in \mathbb{R}^k$ 是待学习的权重参数向量。

在这种情况下，学习算法（如TD学习）的目标是找到一个权重向量 $w$，使得近似[价值函数](@entry_id:144750) $V_w$ 尽可能地接近真实的[价值函数](@entry_id:144750) $V^\pi$。从几何角度看，这相当于将高维空间中的真实价值函数[向量投影](@entry_id:147046)到由[特征向量](@entry_id:151813)张成的低维子空间上。这个投影不是标准的欧几里得投影，而是相对于由智能体访问状态的[平稳分布](@entry_id:194199) $d(s)$ 加权的范数下的投影 。

具体来说，TD学习会收敛到一个解，该解满足**投影[贝尔曼方程](@entry_id:1121499)**：
$$
V_w = \Pi T^\pi V_w
$$
其中 $T^\pi$ 是贝尔曼期望算子，而 $\Pi$ 是到特征子空间上的[投影算子](@entry_id:154142)。该投影算子的矩阵形式为 $\Pi = \Phi (\Phi^\top D \Phi)^{-1} \Phi^\top D$，其中 $\Phi$ 是特征矩阵，其行是[特征向量](@entry_id:151813) $\phi(s)^\top$，$D$ 是由状态分布 $d(s)$ 构成的[对角矩阵](@entry_id:637782)。这个解也被称为TD不动点，它满足 $\Phi^\top D(V_w - T^\pi V_w) = 0$，即[贝尔曼误差](@entry_id:636460)在[特征空间](@entry_id:638014)上与状态分布加权正交 。如果特征矩阵 $\Phi$ 不是满秩的，可以使用**[Moore-Penrose伪逆](@entry_id:147255)**来代替逆，投影仍然是唯一定义的。

#### [策略梯度](@entry_id:635542)与行动家-评论家方法

与学习价值函数不同，**[策略梯度](@entry_id:635542)（policy gradient）** 方法直接[参数化](@entry_id:265163)策略 $\pi_\theta(a|s)$ 并通过梯度上升来优化策略参数 $\theta$，以最大化性能目标 $J(\theta)$（如期望累积奖励）。根据**[策略梯度定理](@entry_id:635009)**，性能目标的梯度可以表示为一个期望的形式，这使得我们可以通过采样来估计梯度。

**行动家-评论家（Actor-Critic）** 方法是[策略梯度方法](@entry_id:634727)和价值函数学习的结合体。它包含两个部分：
- **评论家（Critic）**：学习一个价值函数（状态[价值函数](@entry_id:144750) $V_w(s)$ 或动作价值函数 $Q_w(s,a)$），用于评估当前策略的好坏。
- **行动家（Actor）**：根据评论家的评估来更新策略参数 $\theta$。

一个典型的**双时间尺度行动家-评论家算法**  的更新规则如下：
1.  **评论家更新**（快速时间尺度）：评论家使用TD学习来更新其权重 $w$。它计算[TD误差](@entry_id:634080) $\delta_t$，这可以被看作是对**[优势函数](@entry_id:635295)** $A(s,a) = Q(s,a) - V(s)$ 的一个单步估计。
    $$
    \delta_t = r_t + \gamma V_{w_t}(s_{t+1}) - V_{w_t}(s_t)
    $$
    $$
    w_{t+1} = w_t + \alpha_t \delta_t \nabla_w V_{w_t}(s_t) = w_t + \alpha_t \delta_t \phi(s_t)
    $$
2.  **行动家更新**（慢速时间尺度）：行动家使用[TD误差](@entry_id:634080) $\delta_t$ 来调整策略参数 $\theta$。正的 $\delta_t$ 意味着刚刚执行的动作 $a_t$ 比预期的要好，因此增加选择它的概率；反之亦然。
    $$
    \theta_{t+1} = \theta_t + \beta_t \delta_t \nabla_\theta \ln \pi_{\theta_t}(a_t|s_t)
    $$
此算法的收敛性依赖于**[时间尺度分离](@entry_id:149780)**：评论家的学习率 $\alpha_t$ 必须比行动家的[学习率](@entry_id:140210) $\beta_t$ 收敛得更慢，即 $\beta_t \to 0$ 且 $\beta_t/\alpha_t \to 0$。这确保了在行动家更新策略时，评论家已经（近似）收敛到了对当前策略的准确价值估计，从而为行动家提供了稳定的学习信号。

### 高级主题与实践挑战

尽管[强化学习](@entry_id:141144)的原理清晰，但在实践中，尤其是在与[函数近似](@entry_id:141329)结合时，会出现一些严峻的挑战。

#### “死亡三元组”：[函数近似](@entry_id:141329)、自举与[离策略学习](@entry_id:634676)

当**[函数近似](@entry_id:141329)（function approximation）**、**自举（bootstrapping）**（即使用当前估计值来更新自身，如TD学习）和**[离策略学习](@entry_id:634676)（off-policy learning）** 这三个要素同时出现时，可能会导致学习过程不稳定甚至发散。这被称为**“死亡三元组”（deadly triad）** 。

不稳定的核心机制在于，贝尔曼算子虽然在[无穷范数](@entry_id:637586)下是收缩的（保证了表格型DP的收敛），但与一个依赖于行为策略分布的投影算子复合后，得到的**投影贝尔曼算子** $\Pi T^\pi$ 不再保证是收缩的。如果这个复合[算子的谱半径](@entry_id:261858)大于等于1，[价值函数](@entry_id:144750)的迭代就会发散。这在数学上表现为，TD学习的期望动态由一个矩阵 $A = \mathbb{E}_{d_\mu}[\phi(s)(\phi(s) - \gamma \phi(s'))^\top]$ 驱动，如果这个矩阵不是**Hurwitz稳定**的（即存在特征值的实部为非正），那么学习过程就可能发散 。即使使用重要性采样来修正离策略分布不匹配的问题，如果行为策略与目标策略差异过大，会导致重要性采样比率的方差爆炸，同样引起发散。

#### [探索与利用的权衡](@entry_id:1124777)

智能体必须在**利用（exploitation）**已知的好策略和**探索（exploration）**未知动作以发现更好策略之间做出权衡。简单的策略如 $\epsilon$-greedy 在许多问题中有效，但更高级的方法试图以更有原则的方式指导探索。

**信息导向采样（Information-Directed Sampling, IDS）** 是一种基于[贝叶斯决策理论](@entry_id:909090)的先进探索策略 。IDS的核心思想是在最小化即时**贝叶斯遗憾（Bayesian regret）**（采取次优动作的期望损失）和最大化关于最优动作的**[信息增益](@entry_id:262008)（information gain）**之间进行权衡。在每一步，IDS选择能够最小化**信息比率（information ratio）**的动作：
$$
a_t = \arg\min_{a \in \mathcal{A}} \frac{(\Delta_t(a))^2}{I_t(A^*; Y(a))}
$$
其中，$\Delta_t(a)$ 是采取动作 $a$ 的期望即时遗憾，而 $I_t(A^*; Y(a))$ 是采取动作 $a$ 后得到的观测 $Y(a)$ 与最优动作 $A^*$ 之间的[互信息](@entry_id:138718)。这个准则优雅地量化了为获取单位信息所需付出的“遗憾代价”，从而实现了一种高效且有理论依据的探索。

#### 基于模型的[强化学习](@entry_id:141144)

与直接学习价值函数或策略的无模型方法相对的是**基于模型的[强化学习](@entry_id:141144)（Model-Based Reinforcement Learning）**。在这种范式中，智能体首先学习一个环境模型，然后利用这个模型进行规划来找到最优策略。一个完整的基于模型的[强化学习](@entry_id:141144)循环通常包括以下步骤 ：

1.  **[系统辨识](@entry_id:201290)（System Identification）**：从收集到的数据 $(s_t, a_t, s_{t+1})$ 中学习环境动力学模型 $s_{t+1} \approx \hat{f}(s_t, a_t)$ 的参数。例如，对于线性[参数模型](@entry_id:170911)，可以使用[正则化最小二乘法](@entry_id:754212)来获得参数的一致估计。为了保证模型的可辨识性，需要确保数据具有**[持续激励](@entry_id:263834)（persistent excitation）**的特性。

2.  **规划（Planning）**：使用学习到的模型 $\hat{f}$ 来计算控制策略。由于模型存在不确定性，稳健的规划方法，如**[鲁棒模型预测控制](@entry_id:174393)（Robust Model Predictive Control, MPC）**，至关重要。[鲁棒MPC](@entry_id:174393)会考虑模型参数的不确定性集和外部扰动，并使用[终端约束](@entry_id:176488)和终端[李雅普诺夫函数](@entry_id:273986)来保证闭环系统的稳定性和安全性。

3.  **执行与探索（Execution and Exploration）**：执行规划出的控制动作。为了持续改进模型，需要向[控制信号](@entry_id:747841)中注入一个有界的探索噪声。同时，必须使用**安全滤波器（safety filter）**来确保最终执行的动作满足所有物理约束（如执行器的饱和限制）。

4.  **数据收集（Data Collection）**：在执行过程中收集新的数据，并将其用于下一轮的模型更新。

在[多尺度系统](@entry_id:1128345)中，基于模型的[强化学习](@entry_id:141144)必须小心处理[时间尺度问题](@entry_id:178673)，通常将[参数辨识](@entry_id:275549)和规划放在较慢的宏观时间尺度上，以避免由模型快速变化引起的系统不稳定 。这种方法在需要高样本效率和满足严格安全约束的控制与设计问题中尤为重要。