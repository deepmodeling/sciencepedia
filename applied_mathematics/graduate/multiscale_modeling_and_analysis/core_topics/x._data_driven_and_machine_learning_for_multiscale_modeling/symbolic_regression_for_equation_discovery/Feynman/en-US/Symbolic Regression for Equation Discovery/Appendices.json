{
    "hands_on_practices": [
        {
            "introduction": "The first critical step in discovering differential equations from data is estimating derivatives from noisy measurements. This is a fundamentally ill-posed problem, as naive finite differences can drastically amplify noise. This exercise  guides you through a foundational analysis of the bias-variance trade-off for several common numerical differentiation schemes, including central differences and the Savitzky-Golay filter. Mastering this analysis is essential for understanding how the choice of differentiation method impacts the accuracy and stability of the entire equation discovery workflow.",
            "id": "3812522",
            "problem": "Consider a differentiable scalar state variable $x(t)$ observed on a uniform time grid $t_k = t_0 + k h$ with sampling interval $h > 0$. The observations are corrupted by additive independent and identically distributed Gaussian noise: $y_k = x(t_k) + \\varepsilon_k$, where $\\varepsilon_k \\sim \\mathcal{N}(0,\\sigma^2)$ and are mutually independent. In symbolic regression for equation discovery within multiscale modeling, accurate estimation of time derivatives under noisy measurements is critical for constructing residuals of candidate dynamical laws across scales.\n\nDefine the following three schemes for estimating $\\dot{x}(t_0)$ from $\\{y_k\\}$:\n- The forward difference estimator $\\widehat{D}_{\\mathrm{f}} = \\frac{y_{1} - y_{0}}{h}$.\n- The central difference estimator $\\widehat{D}_{\\mathrm{c}} = \\frac{y_{1} - y_{-1}}{2 h}$.\n- The Savitzky–Golay (SG) quadratic least-squares differentiation estimator $\\widehat{D}_{\\mathrm{sg}}$ obtained by fitting a quadratic polynomial $p(s) = a + b s + c s^2$ in the least-squares sense to the five points $(s_k,y_k)$ for $k \\in \\{-2,-1,0,1,2\\}$ where $s_k = k h$, and taking $\\widehat{D}_{\\mathrm{sg}} = b$ as the derivative estimate at $s=0$.\n\nAssume $x(t)$ is four times continuously differentiable in a neighborhood of $t_0$ so that the Taylor expansion about $t_0$ is valid up to fourth order. Using only fundamental expansions (Taylor series) and properties of independent Gaussian noise, derive the leading-order bias and variance of each estimator and write the corresponding leading-order mean squared error $\\mathrm{MSE} \\approx \\mathrm{Bias}^2 + \\mathrm{Var}$. Then, compute the ratio of the minimal achievable mean squared errors (optimized over $h$) of the central difference estimator and the Savitzky–Golay quadratic five-point estimator. Express your final ratio as a single exact analytic expression. No numerical rounding is required, and no units are necessary in the final answer.",
            "solution": "The problem requires the derivation and comparison of the mean squared error (MSE) for three different numerical differentiation estimators applied to noisy data. The foundation of the analysis lies in using Taylor series to separate the systematic error (bias) from the random error (variance) and then optimizing the trade-off between them by adjusting the sampling interval $h$.\n\nLet $x(t)$ be a scalar function that is four times continuously differentiable ($C^4$) in the vicinity of $t_0$. The observed data at time $t_k = t_0 + k h$ is $y_k = x(t_k) + \\varepsilon_k$, where $\\varepsilon_k$ are i.i.d. Gaussian random variables with mean $E[\\varepsilon_k] = 0$ and variance $\\mathrm{Var}(\\varepsilon_k) = \\sigma^2$. The target quantity is the first derivative $\\dot{x}(t_0)$, which we denote as $\\dot{x}_0$. Higher derivatives at $t_0$ are denoted similarly, e.g., $\\dddot{x}_0$.\n\nThe Taylor expansion of $x(t_k)$ around $t_0$ is given by:\n$$x(t_k) = x(t_0 + kh) = x_0 + (kh)\\dot{x}_0 + \\frac{(kh)^2}{2!}\\ddot{x}_0 + \\frac{(kh)^3}{3!}\\dddot{x}_0 + \\frac{(kh)^4}{4!}\\ddddot{x}_0 + O(h^5)$$\nSince the noise terms $\\varepsilon_k$ are independent with zero mean, the expectation of any linear combination of observations $\\sum_k c_k y_k$ is $E[\\sum_k c_k y_k] = \\sum_k c_k E[y_k] = \\sum_k c_k x(t_k)$. The variance is $\\mathrm{Var}(\\sum_k c_k y_k) = \\sum_k c_k^2 \\mathrm{Var}(y_k) = \\sigma^2 \\sum_k c_k^2$. The mean squared error of an estimator $\\widehat{D}$ is $\\mathrm{MSE}(\\widehat{D}) = E[(\\widehat{D} - \\dot{x}_0)^2] = \\mathrm{Var}(\\widehat{D}) + (\\mathrm{Bias}(\\widehat{D}))^2$, where $\\mathrm{Bias}(\\widehat{D}) = E[\\widehat{D}] - \\dot{x}_0$.\n\nWe analyze each estimator in turn.\n\nFirst, we consider the forward difference estimator $\\widehat{D}_{\\mathrm{f}} = \\frac{y_1 - y_0}{h}$.\nThe expectation is $E[\\widehat{D}_{\\mathrm{f}}] = \\frac{x(t_1) - x(t_0)}{h}$. Using the Taylor expansion for $x(t_1) = x_0 + h\\dot{x}_0 + \\frac{h^2}{2}\\ddot{x}_0 + O(h^3)$:\n$$E[\\widehat{D}_{\\mathrm{f}}] = \\frac{(x_0 + h\\dot{x}_0 + \\frac{h^2}{2}\\ddot{x}_0 + O(h^3)) - x_0}{h} = \\dot{x}_0 + \\frac{h}{2}\\ddot{x}_0 + O(h^2)$$\nThe leading-order bias is $\\mathrm{Bias}(\\widehat{D}_{\\mathrm{f}}) \\approx \\frac{h}{2}\\ddot{x}_0$.\nThe variance is $\\mathrm{Var}(\\widehat{D}_{\\mathrm{f}}) = \\mathrm{Var}\\left(\\frac{y_1 - y_0}{h}\\right) = \\frac{1}{h^2}(\\mathrm{Var}(y_1) + \\mathrm{Var}(y_0)) = \\frac{2\\sigma^2}{h^2}$.\nThe leading-order MSE is $\\mathrm{MSE}(\\widehat{D}_{\\mathrm{f}}) \\approx \\left(\\frac{h}{2}\\ddot{x}_0\\right)^2 + \\frac{2\\sigma^2}{h^2} = \\frac{h^2 \\ddot{x}_0^2}{4} + \\frac{2\\sigma^2}{h^2}$.\n\nSecond, we analyze the central difference estimator $\\widehat{D}_{\\mathrm{c}} = \\frac{y_1 - y_{-1}}{2h}$.\nThe expectation is $E[\\widehat{D}_{\\mathrm{c}}] = \\frac{x(t_1) - x(t_{-1})}{2h}$. We use more terms in the Taylor series for higher accuracy:\n$x(t_1) = x_0 + h\\dot{x}_0 + \\frac{h^2}{2}\\ddot{x}_0 + \\frac{h^3}{6}\\dddot{x}_0 + O(h^5)$\n$x(t_{-1}) = x_0 - h\\dot{x}_0 + \\frac{h^2}{2}\\ddot{x}_0 - \\frac{h^3}{6}\\dddot{x}_0 + O(h^5)$\nThe difference is $x(t_1) - x(t_{-1}) = 2h\\dot{x}_0 + \\frac{h^3}{3}\\dddot{x}_0 + O(h^5)$.\n$$E[\\widehat{D}_{\\mathrm{c}}] = \\frac{2h\\dot{x}_0 + \\frac{h^3}{3}\\dddot{x}_0 + O(h^5)}{2h} = \\dot{x}_0 + \\frac{h^2}{6}\\dddot{x}_0 + O(h^4)$$\nThe leading-order bias is $\\mathrm{Bias}(\\widehat{D}_{\\mathrm{c}}) \\approx \\frac{h^2}{6}\\dddot{x}_0$.\nThe variance is $\\mathrm{Var}(\\widehat{D}_{\\mathrm{c}}) = \\mathrm{Var}\\left(\\frac{y_1 - y_{-1}}{2h}\\right) = \\frac{1}{4h^2}(\\mathrm{Var}(y_1) + \\mathrm{Var}(y_{-1})) = \\frac{2\\sigma^2}{4h^2} = \\frac{\\sigma^2}{2h^2}$.\nThe leading-order MSE for the central difference estimator is:\n$$\\mathrm{MSE}_{\\mathrm{c}}(h) \\approx \\left(\\frac{h^2}{6}\\dddot{x}_0\\right)^2 + \\frac{\\sigma^2}{2h^2} = \\frac{(\\dddot{x}_0)^2}{36}h^4 + \\frac{\\sigma^2}{2}h^{-2}$$\n\nThird, we analyze the Savitzky–Golay (SG) estimator $\\widehat{D}_{\\mathrm{sg}}$. This is the coefficient $b$ from a least-squares fit of $p(s) = a+bs+cs^2$ to the points $(s_k, y_k)$ for $k \\in \\{-2, -1, 0, 1, 2\\}$, where $s_k=kh$. We minimize $S = \\sum_{k=-2}^2 (y_k - a - bkh - ck^2h^2)^2$. Due to the symmetry of the fitting window, the normal equations decouple. The equation for $b$ is found by setting $\\frac{\\partial S}{\\partial b}=0$, which yields $\\sum_k (-kh)(y_k - a - bkh - ck^2h^2) = 0$. This simplifies to $\\sum_k k y_k = b h \\sum_k k^2$.\nWe compute the sums: $\\sum_{k=-2}^2 k^2 = (-2)^2 + (-1)^2 + 0^2 + 1^2 + 2^2 = 10$.\nSo, $\\hat{b} = \\frac{\\sum_{k=-2}^2 k y_k}{10h}$.\nThe estimator is $\\widehat{D}_{\\mathrm{sg}} = \\frac{-2y_{-2} - y_{-1} + y_1 + 2y_2}{10h}$.\nThe expectation is $E[\\widehat{D}_{\\mathrm{sg}}] = \\frac{1}{10h}(-2x(t_{-2}) - x(t_{-1}) + x(t_1) + 2x(t_2))$.\nWe expand each $x(t_k)$ and sum the coefficients for each derivative of $x$ at $t_0$:\n- $x_0$: $-2-1+1+2 = 0$.\n- $\\dot{x}_0$: $h(-2(-2) -1(-1) + 1(1) + 2(2)) = 10h$.\n- $\\ddot{x}_0$: $\\frac{h^2}{2!}(-2(-2)^2 -1(-1)^2 + 1(1)^2 + 2(2)^2) = 0$.\n- $\\dddot{x}_0$: $\\frac{h^3}{3!}(-2(-2)^3 -1(-1)^3 + 1(1)^3 + 2(2)^3) = \\frac{h^3}{6}(16+1+1+16) = \\frac{34h^3}{6}$.\n- $\\ddddot{x}_0$: $\\frac{h^4}{4!}(-2(-2)^4 -1(-1)^4 + 1(1)^4 + 2(2)^4) = 0$.\nThe numerator of $E[\\widehat{D}_{\\mathrm{sg}}]$ is $10h\\dot{x}_0 + \\frac{34h^3}{6}\\dddot{x}_0 + O(h^5)$.\n$$E[\\widehat{D}_{\\mathrm{sg}}] = \\frac{10h\\dot{x}_0 + \\frac{17h^3}{3}\\dddot{x}_0 + O(h^5)}{10h} = \\dot{x}_0 + \\frac{17h^2}{30}\\dddot{x}_0 + O(h^4)$$\nThe leading-order bias is $\\mathrm{Bias}(\\widehat{D}_{\\mathrm{sg}}) \\approx \\frac{17h^2}{30}\\dddot{x}_0$.\nThe variance is $\\mathrm{Var}(\\widehat{D}_{\\mathrm{sg}}) = \\frac{1}{(10h)^2} \\mathrm{Var}(-2y_{-2} - y_{-1} + y_1 + 2y_2) = \\frac{\\sigma^2}{100h^2}((-2)^2 + (-1)^2 + 1^2 + 2^2) = \\frac{10\\sigma^2}{100h^2} = \\frac{\\sigma^2}{10h^2}$.\nThe leading-order MSE for the SG estimator is:\n$$\\mathrm{MSE}_{\\mathrm{sg}}(h) \\approx \\left(\\frac{17h^2}{30}\\dddot{x}_0\\right)^2 + \\frac{\\sigma^2}{10h^2} = \\frac{289(\\dddot{x}_0)^2}{900}h^4 + \\frac{\\sigma^2}{10}h^{-2}$$\n\nNext, we minimize both $\\mathrm{MSE}_{\\mathrm{c}}(h)$ and $\\mathrm{MSE}_{\\mathrm{sg}}(h)$ with respect to $h$. Both expressions are of the form $f(h) = A h^4 + B h^{-2}$. To find the minimum, we set the derivative to zero:\n$\\frac{df}{dh} = 4Ah^3 - 2Bh^{-3} = 0 \\implies 4Ah^6 = 2B \\implies h^6_{\\mathrm{opt}} = \\frac{B}{2A}$.\nAt this optimal $h$, the minimal MSE is:\n$\\mathrm{MSE}_{\\mathrm{min}} = A(h_{\\mathrm{opt}})^4 + B(h_{\\mathrm{opt}})^{-2} = A\\left(\\frac{B}{2A}\\right)^{2/3} + B\\left(\\frac{2A}{B}\\right)^{1/3} = A^{1/3}B^{2/3}2^{-2/3} + B^{2/3}A^{1/3}2^{1/3} = (2^{-2/3}+2^{1/3})A^{1/3}B^{2/3} = 3 \\cdot 2^{-2/3}A^{1/3}B^{2/3}$.\n\nFor the central difference estimator: $A_c = \\frac{(\\dddot{x}_0)^2}{36}$ and $B_c = \\frac{\\sigma^2}{2}$.\nThe minimal MSE is:\n$$\\mathrm{MSE}_{\\mathrm{c,min}} = 3 \\cdot 2^{-2/3} \\left(\\frac{(\\dddot{x}_0)^2}{36}\\right)^{1/3} \\left(\\frac{\\sigma^2}{2}\\right)^{2/3} = 3 \\cdot 2^{-2/3} \\frac{(\\dddot{x}_0)^{2/3}}{36^{1/3}} \\frac{\\sigma^{4/3}}{2^{2/3}} = 3 \\frac{(\\dddot{x}_0)^{2/3} \\sigma^{4/3}}{2^{4/3} (6^2)^{1/3}} = \\frac{3 (\\dddot{x}_0)^{2/3} \\sigma^{4/3}}{2^{4/3} 2^{2/3} 3^{2/3}} = \\frac{3^{1/3}}{4} (\\dddot{x}_0)^{2/3} \\sigma^{4/3}$$\n\nFor the SG estimator: $A_{\\mathrm{sg}} = \\frac{289(\\dddot{x}_0)^2}{900}$ and $B_{\\mathrm{sg}} = \\frac{\\sigma^2}{10}$.\nThe minimal MSE is:\n$$\\mathrm{MSE}_{\\mathrm{sg,min}} = 3 \\cdot 2^{-2/3} \\left(\\frac{289(\\dddot{x}_0)^2}{900}\\right)^{1/3} \\left(\\frac{\\sigma^2}{10}\\right)^{2/3} = 3 \\cdot 2^{-2/3} \\frac{(17^2)^{1/3}(\\dddot{x}_0)^{2/3}}{(900)^{1/3}} \\frac{\\sigma^{4/3}}{10^{2/3}}$$\n$$= 3 \\cdot 2^{-2/3} \\frac{17^{2/3}(\\dddot{x}_0)^{2/3}\\sigma^{4/3}}{(9 \\cdot 100)^{1/3} \\cdot 10^{2/3}} = 3 \\cdot 2^{-2/3} \\frac{17^{2/3}}{3^{2/3} \\cdot 10^{2/3} \\cdot 10^{2/3}} (\\dddot{x}_0)^{2/3}\\sigma^{4/3} = 3 \\cdot 2^{-2/3} \\frac{17^{2/3}}{3^{2/3} \\cdot 10^{4/3}} (\\dddot{x}_0)^{2/3}\\sigma^{4/3}$$\n$$= \\frac{3^{1/3} \\cdot 17^{2/3}}{2^{2/3} \\cdot (2 \\cdot 5)^{4/3}} (\\dddot{x}_0)^{2/3}\\sigma^{4/3} = \\frac{3^{1/3} \\cdot 17^{2/3}}{2^{2/3} \\cdot 2^{4/3} \\cdot 5^{4/3}} (\\dddot{x}_0)^{2/3}\\sigma^{4/3} = \\frac{3^{1/3} \\cdot 17^{2/3}}{4 \\cdot 5^{4/3}} (\\dddot{x}_0)^{2/3}\\sigma^{4/3}$$\n\nFinally, we compute the ratio of the minimal achievable MSEs:\n$$\\frac{\\mathrm{MSE}_{\\mathrm{c,min}}}{\\mathrm{MSE}_{\\mathrm{sg,min}}} = \\frac{\\frac{3^{1/3}}{4} (\\dddot{x}_0)^{2/3} \\sigma^{4/3}}{\\frac{3^{1/3} \\cdot 17^{2/3}}{4 \\cdot 5^{4/3}} (\\dddot{x}_0)^{2/3} \\sigma^{4/3}} = \\frac{1}{\\frac{17^{2/3}}{5^{4/3}}} = \\frac{5^{4/3}}{17^{2/3}}$$\nThis ratio is independent of the signal properties ($\\dddot{x}_0$) and noise level ($\\sigma$), reflecting a fundamental comparison of the estimators' structures.\nThe expression can also be written as $\\left(\\frac{5^4}{17^2}\\right)^{1/3} = \\left(\\frac{625}{289}\\right)^{1/3}$. The first form is simpler.",
            "answer": "$$\\boxed{\\frac{5^{4/3}}{17^{2/3}}}$$"
        },
        {
            "introduction": "After constructing a library of candidate functions, the core task of symbolic regression is to select the most parsimonious model that accurately describes the data. This is achieved by balancing model complexity against data fidelity, a trade-off typically controlled by a regularization parameter, $\\lambda$. This problem  provides a rigorous, first-principles derivation of the trade-off curves for a simple but illustrative system, allowing you to determine analytically the exact regimes where a simpler or more complex model is optimal. This practice builds a deep intuition for how penalized regression and information criteria perform model selection.",
            "id": "3812595",
            "problem": "Consider a symbolic regression setting in which the goal is to recover a parsimonious closed-form expression for a scalar response generated at a fine scale and observed at a coarse scale under noise. Let the input be distributed as a standard normal random variable $x \\sim \\mathcal{N}(0,1)$ and the response be generated by $y = x^{2} + \\epsilon$, where the noise $\\epsilon \\sim \\mathcal{N}(0,\\sigma^{2})$ is independent of $x$. You are given a candidate library of symbolic models restricted to single-term monomials with a free scalar coefficient, $\\mathcal{F} = \\{ f_{p}(x) = \\beta x^{p} : \\beta \\in \\mathbb{R},\\ p \\in \\{1,2,3\\} \\}$.\n\nDefine the population penalized objective\n$$\nL_{p}(\\lambda) \\equiv \\inf_{\\beta \\in \\mathbb{R}} \\ \\mathbb{E}\\big[(y - \\beta x^{p})^{2}\\big] + \\lambda\\,K(p),\n$$\nwhere $K(p)$ is a model complexity functional and $\\lambda \\ge 0$ is a regularization strength that trades off data fidelity and complexity. Assume the complexity functional equals the polynomial degree, $K(p) = p$.\n\nStarting from first principles, namely the definition of population mean-squared error and the independence and moment properties of the standard normal distribution, derive, for each candidate $p \\in \\{1,2,3\\}$, the optimal coefficient $\\beta_{p}^{\\star}$ minimizing the unpenalized expected error, the corresponding minimal expected error, and hence the explicit trade-off curves $L_{p}(\\lambda)$ as functions of $\\sigma^{2}$ and $\\lambda$. Using these expressions, determine the interval(s) of $\\lambda \\ge 0$ over which each candidate model is optimal, that is, attains the minimal penalized objective among the three candidates.\n\nIn your final answer, report the nonnegative critical regularization strength at which the optimal model switches away from $p=2$ to another candidate. No rounding is required.",
            "solution": "The problem is first assessed for validity.\n\n**Step 1: Extract Givens**\n- Input distribution: $x \\sim \\mathcal{N}(0,1)$.\n- True data generating process: $y = x^{2} + \\epsilon$.\n- Noise distribution: $\\epsilon \\sim \\mathcal{N}(0,\\sigma^{2})$, where $\\epsilon$ is independent of $x$.\n- Candidate model library: $\\mathcal{F} = \\{ f_{p}(x) = \\beta x^{p} : \\beta \\in \\mathbb{R},\\ p \\in \\{1,2,3\\} \\}$.\n- Population penalized objective function: $L_{p}(\\lambda) \\equiv \\inf_{\\beta \\in \\mathbb{R}} \\ \\mathbb{E}\\big[(y - \\beta x^{p})^{2}\\big] + \\lambda\\,K(p)$.\n- Regularization parameter: $\\lambda \\ge 0$.\n- Complexity functional: $K(p) = p$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is a standard exercise in statistical learning theory, specifically model selection using a penalized loss function (related to Akaike or Bayesian Information Criteria in spirit). It uses fundamental principles of probability, expectation, and optimization. All premises are well-established.\n- **Well-Posed:** The problem is well-posed. The objective function involves minimizing a quadratic function of the parameter $\\beta$, which has a unique minimum. The task is to find this minimum for each model and then compare the resulting penalized objectives, which is a clearly defined procedure leading to a unique set of intervals for $\\lambda$.\n- **Objective:** The problem is stated in precise mathematical language, free from ambiguity or subjective claims.\n\n**Step 3: Verdict and Action**\nThe problem is scientifically sound, well-posed, and formally stated. It is deemed **valid**. A full solution will be provided.\n\nThe core of the problem is to evaluate the penalized objective $L_{p}(\\lambda)$ for each candidate model indexed by $p \\in \\{1, 2, 3\\}$. The objective consists of two parts: the minimal expected mean-squared error (MSE) and a complexity penalty.\n$L_{p}(\\lambda) = \\min_{\\beta \\in \\mathbb{R}} \\mathbb{E}[(y - \\beta x^{p})^{2}] + \\lambda p$.\n\nLet the unpenalized expected error be $E_p(\\beta) = \\mathbb{E}[(y - \\beta x^{p})^{2}]$. To find the optimal coefficient $\\beta_p^{\\star}$ that minimizes this term, we take the derivative with respect to $\\beta$ and set it to zero.\n$$\n\\frac{dE_p(\\beta)}{d\\beta} = \\frac{d}{d\\beta} \\mathbb{E}[y^2 - 2\\beta yx^p + \\beta^2 (x^p)^2] = \\mathbb{E}[-2yx^p + 2\\beta x^{2p}]\n$$\nSetting the derivative to zero gives the optimal coefficient $\\beta_p^{\\star}$:\n$$\n-2\\mathbb{E}[yx^p] + 2\\beta_p^{\\star}\\mathbb{E}[x^{2p}] = 0 \\implies \\beta_p^{\\star} = \\frac{\\mathbb{E}[yx^p]}{\\mathbb{E}[x^{2p}]}\n$$\nThe second derivative is $2\\mathbb{E}[x^{2p}] > 0$, since $x^{2p}$ is a non-negative random variable that is not almost surely zero, confirming that this is a minimum.\n\nTo compute $\\beta_p^{\\star}$, we need to evaluate the expectations. The variable $x$ follows a standard normal distribution, $x \\sim \\mathcal{N}(0,1)$. The moments of a standard normal distribution are given by $\\mathbb{E}[x^k] = 0$ for odd $k$, and $\\mathbb{E}[x^k] = (k-1)!!$ for even $k$.\nThe required moments are:\n- $\\mathbb{E}[x^1] = 0$\n- $\\mathbb{E}[x^2] = 1$\n- $\\mathbb{E}[x^3] = 0$\n- $\\mathbb{E}[x^4] = (3)!! = 3 \\cdot 1 = 3$\n- $\\mathbb{E}[x^5] = 0$\n- $\\mathbb{E}[x^6] = (5)!! = 5 \\cdot 3 \\cdot 1 = 15$\n\nNext, we evaluate $\\mathbb{E}[yx^p]$. Substituting $y = x^2 + \\epsilon$:\n$$\n\\mathbb{E}[yx^p] = \\mathbb{E}[(x^2 + \\epsilon)x^p] = \\mathbb{E}[x^{p+2} + \\epsilon x^p] = \\mathbb{E}[x^{p+2}] + \\mathbb{E}[\\epsilon x^p]\n$$\nSince $x$ and $\\epsilon$ are independent, $\\mathbb{E}[\\epsilon x^p] = \\mathbb{E}[\\epsilon]\\mathbb{E}[x^p]$. Given $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$, we have $\\mathbb{E}[\\epsilon]=0$. Thus, $\\mathbb{E}[\\epsilon x^p] = 0$.\nSo, $\\mathbb{E}[yx^p] = \\mathbb{E}[x^{p+2}]$.\n\nNow we can compute $\\beta_p^{\\star}$ for each $p \\in \\{1, 2, 3\\}$:\n- For $p=1$: $\\beta_1^{\\star} = \\frac{\\mathbb{E}[x^{1+2}]}{\\mathbb{E}[x^{2 \\cdot 1}]} = \\frac{\\mathbb{E}[x^3]}{\\mathbb{E}[x^2]} = \\frac{0}{1} = 0$.\n- For $p=2$: $\\beta_2^{\\star} = \\frac{\\mathbb{E}[x^{2+2}]}{\\mathbb{E}[x^{2 \\cdot 2}]} = \\frac{\\mathbb{E}[x^4]}{\\mathbb{E}[x^4]} = \\frac{3}{3} = 1$.\n- For $p=3$: $\\beta_3^{\\star} = \\frac{\\mathbb{E}[x^{3+2}]}{\\mathbb{E}[x^{2 \\cdot 3}]} = \\frac{\\mathbb{E}[x^5]}{\\mathbb{E}[x^6]} = \\frac{0}{15} = 0$.\n\nNext, we find the minimal unpenalized error, $\\min_{\\beta} E_p(\\beta) = E_p(\\beta_p^{\\star})$.\n$$\nE_p(\\beta_p^{\\star}) = \\mathbb{E}[(y - \\beta_p^{\\star} x^p)^2] = \\mathbb{E}[y^2] - 2\\beta_p^{\\star}\\mathbb{E}[yx^p] + (\\beta_p^{\\star})^2\\mathbb{E}[x^{2p}]\n$$\nSubstituting $\\beta_p^{\\star} = \\frac{\\mathbb{E}[yx^p]}{\\mathbb{E}[x^{2p}]}$, this simplifies to:\n$$\nE_p(\\beta_p^{\\star}) = \\mathbb{E}[y^2] - \\frac{(\\mathbb{E}[yx^p])^2}{\\mathbb{E}[x^{2p}]}\n$$\nWe compute $\\mathbb{E}[y^2]$:\n$$\n\\mathbb{E}[y^2] = \\mathbb{E}[(x^2 + \\epsilon)^2] = \\mathbb{E}[x^4 + 2\\epsilon x^2 + \\epsilon^2] = \\mathbb{E}[x^4] + 2\\mathbb{E}[\\epsilon x^2] + \\mathbb{E}[\\epsilon^2]\n$$\nUsing independence $\\mathbb{E}[\\epsilon x^2]=\\mathbb{E}[\\epsilon]\\mathbb{E}[x^2] = 0 \\cdot 1 = 0$. For the noise, $\\mathbb{E}[\\epsilon^2] = \\text{Var}(\\epsilon) + (\\mathbb{E}[\\epsilon])^2 = \\sigma^2 + 0^2 = \\sigma^2$.\nThus, $\\mathbb{E}[y^2] = \\mathbb{E}[x^4] + \\sigma^2 = 3 + \\sigma^2$.\n\nNow we calculate the minimal error for each model:\n- For $p=1$: Minimal Error $= (3 + \\sigma^2) - \\frac{(\\mathbb{E}[x^3])^2}{\\mathbb{E}[x^2]} = (3 + \\sigma^2) - \\frac{0^2}{1} = 3 + \\sigma^2$.\n- For $p=2$: Minimal Error $= (3 + \\sigma^2) - \\frac{(\\mathbb{E}[x^4])^2}{\\mathbb{E}[x^4]} = (3 + \\sigma^2) - \\frac{3^2}{3} = 3 + \\sigma^2 - 3 = \\sigma^2$.\n- For $p=3$: Minimal Error $= (3 + \\sigma^2) - \\frac{(\\mathbb{E}[x^5])^2}{\\mathbb{E}[x^6]} = (3 + \\sigma^2) - \\frac{0^2}{15} = 3 + \\sigma^2$.\n\nThe penalized objective functions $L_p(\\lambda)$ are:\n- $L_1(\\lambda) = (3 + \\sigma^2) + \\lambda \\cdot 1 = 3 + \\sigma^2 + \\lambda$.\n- $L_2(\\lambda) = \\sigma^2 + \\lambda \\cdot 2 = \\sigma^2 + 2\\lambda$.\n- $L_3(\\lambda) = (3 + \\sigma^2) + \\lambda \\cdot 3 = 3 + \\sigma^2 + 3\\lambda$.\n\nTo find the intervals of $\\lambda$ where each model is optimal, we compare these three linear functions of $\\lambda$ for $\\lambda \\ge 0$.\n\nFirst, compare $L_1(\\lambda)$ and $L_3(\\lambda)$:\n$L_3(\\lambda) - L_1(\\lambda) = (3 + \\sigma^2 + 3\\lambda) - (3 + \\sigma^2 + \\lambda) = 2\\lambda$.\nSince $\\lambda \\ge 0$, $2\\lambda \\ge 0$. Thus, $L_3(\\lambda) \\ge L_1(\\lambda)$ for all $\\lambda \\ge 0$. The model $p=3$ is never strictly better than $p=1$ and is therefore never the unique optimal model.\n\nNow, compare the remaining candidates, $p=1$ and $p=2$. The optimal model will be the one with the smaller value of $L_p(\\lambda)$. We find the crossover point by setting $L_1(\\lambda) = L_2(\\lambda)$:\n$$\n3 + \\sigma^2 + \\lambda = \\sigma^2 + 2\\lambda\n$$\n$$\n3 = \\lambda\n$$\nThis is the critical regularization strength, $\\lambda_c = 3$.\n\nTo determine which model is optimal on either side of this critical value:\n- For $\\lambda < 3$: The term $3-\\lambda$ is positive. We have $3+\\lambda < 2\\lambda + (3-\\lambda) + \\lambda = 2\\lambda+3$.\n  A simpler comparison is direct:\n  $L_2(\\lambda) < L_1(\\lambda) \\iff \\sigma^2 + 2\\lambda < 3 + \\sigma^2 + \\lambda \\iff \\lambda < 3$.\n  Thus, for $0 \\le \\lambda < 3$, the model $p=2$ is optimal.\n- For $\\lambda > 3$:\n  $L_1(\\lambda) < L_2(\\lambda) \\iff 3 + \\sigma^2 + \\lambda < \\sigma^2 + 2\\lambda \\iff 3 < \\lambda$.\n  Thus, for $\\lambda > 3$, the model $p=1$ is optimal.\n- For $\\lambda = 3$:\n  $L_1(3) = L_2(3)$, so both models $p=1$ and $p=2$ are equally optimal.\n\nModel $p=2$ is the correct underlying model structure and has the best fit to the data, hence it is preferred when the complexity penalty (controlled by $\\lambda$) is small. As $\\lambda$ increases, the penalty for complexity ($2\\lambda$ for $p=2$) grows faster than for the simpler model ($1\\lambda$ for $p=1$). Eventually, at $\\lambda=3$, the penalty outweighs the better fit, and the simpler (but biased) model $p=1$ becomes preferred.\n\nThe question asks for the critical regularization strength at which the optimal model switches away from $p=2$. This occurs at the point where $L_2(\\lambda)$ ceases to be the unique minimum, which is at $\\lambda=3$.\n\nThe intervals of optimality are:\n- $p=2$ is optimal for $\\lambda \\in [0, 3]$.\n- $p=1$ is optimal for $\\lambda \\in [3, \\infty)$.\n- $p=3$ is never the uniquely optimal model for $\\lambda \\ge 0$.\n\nThe switch away from $p=2$ as the sole optimal model occurs at the boundary, which is the critical value $\\lambda_c = 3$.",
            "answer": "$$\\boxed{3}$$"
        },
        {
            "introduction": "A key advantage of modern equation discovery methods is the ability to embed prior physical knowledge, such as conservation laws, directly into the regression framework. This not only improves robustness to noise but also ensures the discovered models are physically consistent. In this hands-on coding problem , you will implement a pipeline to demonstrate how enforcing a conservation law via a constrained least-squares problem can be the deciding factor in correctly identifying a subtle nonlinear term in a PDE that an unconstrained method would miss. This powerful technique represents a crucial step towards building trustworthy, physics-informed models from data.",
            "id": "3812529",
            "problem": "Consider the task of equation discovery for a one-dimensional Partial Differential Equation (PDE) on a periodic domain using symbolic regression. The goal is to determine when and why adding a conservation constraint changes the discovered PDE model from a pure diffusion model to a reaction-diffusion model that includes a cubic nonlinearity. You will implement a program that constructs synthetic data from a known PDE, builds a feature library, performs sparse regression to identify the governing equation, and then repeats the identification with an equality constraint that encodes a conservation law. The program will report, for multiple test cases, whether the addition of the constraint changes the discovered PDE by selecting a previously omitted nonlinear term.\n\nFundamental base:\n- The governing PDE is assumed to be of the form\n$$\nu_t = \\nu\\, u_{xx} - \\epsilon\\, u^3,\n$$\nwith periodic boundary conditions on a domain of length $L = 2\\pi$, where $u(x,t)$ is the state, $\\nu > 0$ is the diffusion coefficient, and $\\epsilon \\ge 0$ controls the strength of the cubic reaction term.\n- For periodic boundary conditions, the integral of the Laplacian term vanishes:\n$$\n\\int_0^{L} u_{xx}\\, dx = 0.\n$$\n- Therefore, the spatial mean (also called the mass) $M(t) = \\int_0^{L} u(x,t)\\, dx$ obeys the balance\n$$\n\\frac{dM}{dt} = \\int_0^{L} u_t\\, dx = -\\epsilon \\int_0^{L} u^3\\, dx.\n$$\n\nProblem setup:\n1. Data generation. Simulate the PDE\n$$\nu_t = \\nu\\, u_{xx} - \\epsilon\\, u^3\n$$\non $x \\in [0,2\\pi]$ with periodic boundary conditions, using an explicit time-stepping scheme. Use a uniform spatial grid of $N$ points and a uniform time step $\\Delta t$, for $T$ time steps. Initialize with\n$$\nu(x,0) = a_0 + a_1 \\sin(x) + a_2 \\cos(2x),\n$$\nwhere $a_0$, $a_1$, $a_2$ are fixed real constants. Use the discrete Fourier transform to compute $u_{xx}$ spectrally via\n$$\n\\widehat{u_{xx}}(k) = -k^2 \\hat{u}(k),\n$$\nwhere $k$ are the angular wavenumbers, and $\\hat{u}(k)$ is the discrete Fourier transform of $u$.\n\n2. Library construction and regression. At each time step $n \\in \\{0,\\dots,T-1\\}$ and spatial grid point $j \\in \\{0,\\dots,N-1\\}$, form the target and features\n$$\ny_{n,j} = \\frac{u_{n+1,j} - u_{n,j}}{\\Delta t}, \\quad \\phi^{(1)}_{n,j} = (u_{xx})_{n,j}, \\quad \\phi^{(2)}_{n,j} = \\left(u_{n,j}\\right)^3,\n$$\nand stack these into a regression system\n$$\n\\Theta\\, c \\approx y,\n$$\nwhere $\\Theta \\in \\mathbb{R}^{(NT) \\times 2}$ has columns $\\phi^{(1)}$ and $\\phi^{(2)}$, $c \\in \\mathbb{R}^2$ are the unknown coefficients corresponding to $u_{xx}$ and $u^3$, and $y \\in \\mathbb{R}^{NT}$ stacks the entries $y_{n,j}$.\n\n3. Unconstrained sparse regression. Solve a ridge-regularized least-squares problem to obtain an initial estimate $\\hat{c}$, then apply Sequentially Thresholded Least Squares (STLSQ): zero any component of $\\hat{c}$ with absolute value below a threshold $\\tau$, and refit on the retained columns using ridge-regularized least squares. Denote the resulting coefficients by $c^{\\mathrm{un}}$. Define a detection threshold $\\delta > 0$, and say that a term is “present” if and only if its absolute coefficient is at least $\\delta$.\n\n4. Conservation-constrained regression. Impose the average-in-time mass-balance equality constraint derived from periodicity. In discrete form, with uniform grid spacing $\\Delta x = L/N$, let\n$$\ng = \\Delta x \\begin{bmatrix} \\sum_{n,j} \\phi^{(1)}_{n,j} \\\\ \\sum_{n,j} \\phi^{(2)}_{n,j} \\end{bmatrix}, \\quad h = \\Delta x \\sum_{n,j} y_{n,j}.\n$$\nSolve the ridge-regularized equality-constrained least-squares problem\n$$\n\\min_{c \\in \\mathbb{R}^2} \\left\\| \\Theta c - y \\right\\|_2^2 + \\alpha \\left\\| c \\right\\|_2^2 \\quad \\text{subject to} \\quad g^\\top c = h,\n$$\nwith a small $\\alpha > 0$ for numerical stability. Denote the resulting coefficients by $c^{\\mathrm{con}}$. Report whether the cubic term is “present” according to the same detection threshold $\\delta$.\n\n5. Decision rule. For each test case, output a boolean indicating whether adding the conservation constraint changes the discovered PDE from the diffusion-only model to the reaction-diffusion model with the cubic term. Formally, output\n$$\n\\text{changed} = \\left(\\left|c^{\\mathrm{un}}_2\\right| < \\delta\\right) \\wedge \\left(\\left|c^{\\mathrm{con}}_2\\right| \\ge \\delta\\right),\n$$\nwhere $c^{\\mathrm{un}}_2$ and $c^{\\mathrm{con}}_2$ are the coefficients multiplying $u^3$ in the unconstrained and constrained fits, respectively.\n\nTest suite:\nUse the following three parameter sets to test different facets of the solution. In all cases, take $L = 2\\pi$, $a_0 = 0.5$, $a_1 = 0.4$, $a_2 = -0.2$, ridge parameter $\\alpha = 10^{-8}$, detection threshold $\\delta = 0.01$, and STLSQ threshold $\\tau = 0.03$.\n\n- Test $1$ (happy path where the constraint matters): $N = 64$, $T = 200$, $\\Delta t = 0.001$, $\\nu = 0.1$, $\\epsilon = 0.02$.\n- Test $2$ (boundary case with no reaction): $N = 64$, $T = 200$, $\\Delta t = 0.001$, $\\nu = 0.1$, $\\epsilon = 0.0$.\n- Test $3$ (strong reaction where both methods recover the cubic term): $N = 64$, $T = 200$, $\\Delta t = 0.001$, $\\nu = 0.1$, $\\epsilon = 0.15$.\n\nRequired final output:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the tests above, for example\n$$\n[\\text{result}_1,\\text{result}_2,\\text{result}_3],\n$$\nwhere each $\\text{result}_i$ is a boolean computed as specified in the decision rule. No other output should be produced.\n\nAngle units: If any trigonometric functions are used, the angle variable must be in radians. No physical units need to be reported in the final answer.",
            "solution": "We begin from two fundamental facts. First, the periodic boundary condition implies $\\int_0^{2\\pi} u_{xx}\\, dx = 0$, because the second derivative integrates to a boundary term, which cancels over a periodic interval. Second, the governing model is assumed to have the general form $u_t = \\nu u_{xx} - \\epsilon u^3$, which is a reaction-diffusion equation with a cubic sink. Taking the spatial integral yields the mass balance\n$$\n\\frac{d}{dt}\\int_0^{2\\pi} u(x,t)\\, dx \\,=\\, \\nu \\int_0^{2\\pi} u_{xx}\\, dx \\,-\\, \\epsilon \\int_0^{2\\pi} u^3\\, dx \\,=\\, -\\epsilon \\int_0^{2\\pi} u^3\\, dx.\n$$\nHence, the diffusion term cannot change the spatial mean, while the cubic term generally does, provided $u^3$ has a nonzero spatial mean. This is the key physical and mathematical constraint we will encode into an equality-constrained regression.\n\nAlgorithmic design proceeds in four steps.\n\nStep $1$: data simulation. We discretize $x \\in [0,2\\pi]$ with a uniform grid of $N$ points, spacing $\\Delta x = 2\\pi/N$, and $t \\in \\{0,\\Delta t,2\\Delta t,\\dots,T\\Delta t\\}$. We initialize $u(x,0) = a_0 + a_1 \\sin(x) + a_2 \\cos(2x)$ to ensure a positive bias so that $\\int u^3 dx$ is generically nonzero. We advance in time with forward Euler,\n$$\nu^{n+1} = u^n + \\Delta t\\left(\\nu\\, u_{xx}^n - \\epsilon\\, (u^n)^3\\right),\n$$\nand compute the Laplacian spectrally for accuracy and to respect periodicity:\n$$\n\\widehat{u_{xx}}(k) = -k^2 \\widehat{u}(k),\n$$\nwhere $k$ are the angular wavenumbers given by $k = 2\\pi\\, \\mathrm{fftfreq}(N, \\Delta x)$, and $\\widehat{u}$ denotes the discrete Fourier transform of $u$.\n\nStep $2$: library construction. For each time step $n \\in \\{0,\\dots,T-1\\}$, we compute the instantaneous time derivative by finite difference\n$$\ny^n = \\frac{u^{n+1} - u^n}{\\Delta t},\n$$\nand the features\n$$\n\\phi^{(1),n} = u_{xx}^n,\\quad \\phi^{(2),n} = \\left(u^n\\right)^3.\n$$\nWe then stack all space-time samples to form the regression system $\\Theta c \\approx y$, with $\\Theta \\in \\mathbb{R}^{(NT)\\times 2}$.\n\nStep $3$: unconstrained sparse regression. We first solve a ridge-regularized least-squares problem\n$$\n\\hat{c} = \\arg\\min_c \\left\\|\\Theta c - y\\right\\|_2^2 + \\alpha \\left\\|c\\right\\|_2^2,\n$$\nwith a small $\\alpha > 0$ for numerical stability. Next, we apply Sequentially Thresholded Least Squares (STLSQ): with a user-specified threshold $\\tau > 0$, we set any component of $\\hat{c}$ with absolute value below $\\tau$ to zero, and refit the ridge-regularized least squares restricted to the remaining columns. This yields $c^{\\mathrm{un}}$. Finally, we declare that the cubic term is detected if and only if $\\left|c^{\\mathrm{un}}_2\\right| \\ge \\delta$, where $\\delta > 0$ is a detection threshold distinct from $\\tau$.\n\nIn practice, if $\\epsilon$ is small, then $\\left|c^{\\mathrm{un}}_2\\right|$ may be below $\\tau$ and the cubic term is pruned, resulting in a diffusion-only model $u_t \\approx \\nu u_{xx}$.\n\nStep $4$: conservation-constrained regression. We now encode the mass-balance constraint. In discrete form, with $\\Delta x = 2\\pi/N$, define\n$$\ng = \\Delta x \\begin{bmatrix} \\sum_{n=0}^{T-1}\\sum_{j=0}^{N-1} \\phi^{(1)}_{n,j} \\\\ \\sum_{n=0}^{T-1}\\sum_{j=0}^{N-1} \\phi^{(2)}_{n,j} \\end{bmatrix}, \\quad\nh = \\Delta x \\sum_{n=0}^{T-1}\\sum_{j=0}^{N-1} y_{n,j}.\n$$\nFor periodic grids, $\\sum_j \\phi^{(1)}_{n,j} \\approx 0$ at every $n$, so the first component of $g$ is approximately zero, while the second component captures the accumulated contribution of $\\int u^3 dx$. We solve the equality-constrained ridge regression\n$$\n\\min_c \\left\\|\\Theta c - y\\right\\|_2^2 + \\alpha \\left\\|c\\right\\|_2^2 \\quad \\text{subject to} \\quad g^\\top c = h.\n$$\nIntroducing a Lagrange multiplier $\\lambda$, the Karush–Kuhn–Tucker (KKT) system reads\n$$\n\\begin{bmatrix}\n2(\\Theta^\\top \\Theta + \\alpha I) & g \\\\\ng^\\top & 0\n\\end{bmatrix}\n\\begin{bmatrix}\nc \\\\ \\lambda\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n2 \\Theta^\\top y \\\\ h\n\\end{bmatrix}.\n$$\nSolving this linear system yields $c^{\\mathrm{con}}$. Because the diffusion term integrates to zero, any nonzero temporal change in the spatial mean observed in the data must be attributed to the cubic term, and the constraint enforces this attribution. Consequently, when $\\epsilon$ is small but nonzero, the unconstrained sparse regression may prune the cubic term, while the constrained regression will recover a nonzero coefficient consistent with the mass balance.\n\nTest suite and expected behaviors:\n- Test $1$ with $\\epsilon = 0.02$ and $\\tau = 0.03$: the unconstrained STLSQ prunes the cubic term because $\\left|c^{\\mathrm{un}}_2\\right| < \\tau$, resulting in diffusion-only. The constrained regression enforces mass loss, yielding $\\left|c^{\\mathrm{con}}_2\\right| \\gtrsim \\delta$, thus “changed” is true.\n- Test $2$ with $\\epsilon = 0.0$: both unconstrained and constrained methods find the cubic term absent, $\\left|c^{\\mathrm{un}}_2\\right| \\approx 0$ and $\\left|c^{\\mathrm{con}}_2\\right| \\approx 0$, so “changed” is false.\n- Test $3$ with $\\epsilon = 0.15$ and $\\tau = 0.03$: the cubic term is strong, so both methods detect it, and “changed” is false.\n\nThe program implements the simulation, feature construction, unconstrained STLSQ, and equality-constrained regression via the KKT system, and outputs a single line containing the boolean results for the three tests in order, as a bracketed, comma-separated list.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef spectral_laplacian(u, L):\n    \"\"\"\n    Compute u_xx using spectral differentiation on a periodic domain of length L.\n    u: shape (N,), real-valued\n    returns: shape (N,), real-valued u_xx\n    \"\"\"\n    N = u.size\n    dx = L / N\n    # Frequencies in cycles per unit length\n    freq = np.fft.fftfreq(N, d=dx)\n    k = 2.0 * np.pi * freq  # angular wavenumbers\n    uhat = np.fft.fft(u)\n    uxx_hat = -(k ** 2) * uhat\n    u_xx = np.fft.ifft(uxx_hat).real\n    return u_xx\n\ndef simulate_pde(N, T_steps, dt, nu, eps, L=2*np.pi, a0=0.5, a1=0.4, a2=-0.2):\n    \"\"\"\n    Simulate u_t = nu u_xx - eps u^3 on a periodic domain [0, L] with N grid points.\n    Forward Euler time stepping, spectral u_xx.\n    Returns:\n        U: array shape (T_steps+1, N)\n    \"\"\"\n    x = np.linspace(0.0, L, N, endpoint=False)\n    u = a0 + a1 * np.sin(x) + a2 * np.cos(2.0 * x)\n    U = np.empty((T_steps + 1, N), dtype=float)\n    U[0] = u\n    for n in range(T_steps):\n        u_xx = spectral_laplacian(u, L)\n        rhs = nu * u_xx - eps * (u ** 3)\n        u = u + dt * rhs\n        U[n + 1] = u\n    return U\n\ndef build_regression_system(U, dt, L=2*np.pi):\n    \"\"\"\n    Build y and Theta from simulation data U.\n    U: array shape (T_steps+1, N)\n    Returns:\n        y: shape (T_steps*N,)\n        Theta: shape (T_steps*N, 2) with columns [u_xx, u^3]\n    \"\"\"\n    T_steps = U.shape[0] - 1\n    N = U.shape[1]\n    # Preallocate\n    y_list = []\n    f1_list = []\n    f2_list = []\n    for n in range(T_steps):\n        u_n = U[n]\n        u_np1 = U[n + 1]\n        y_n = (u_np1 - u_n) / dt\n        u_xx_n = spectral_laplacian(u_n, L)\n        f1_list.append(u_xx_n.reshape(-1))\n        f2_list.append((u_n ** 3).reshape(-1))\n        y_list.append(y_n.reshape(-1))\n    f1 = np.concatenate(f1_list, axis=0)\n    f2 = np.concatenate(f2_list, axis=0)\n    y = np.concatenate(y_list, axis=0)\n    Theta = np.column_stack((f1, f2))\n    return y, Theta\n\ndef ridge_least_squares(Theta, y, alpha):\n    \"\"\"\n    Solve min ||Theta c - y||^2 + alpha ||c||^2\n    Returns c (shape (p,))\n    \"\"\"\n    p = Theta.shape[1]\n    A = Theta.T @ Theta + alpha * np.eye(p)\n    b = Theta.T @ y\n    c = np.linalg.solve(A, b)\n    return c\n\ndef stlsq(Theta, y, alpha, tau):\n    \"\"\"\n    Sequentially Thresholded Least Squares with ridge regularization.\n    1) Solve ridge LS for all features.\n    2) Zero coefficients with |c| < tau.\n    3) Refit ridge LS on retained features. If none retained, return zeros.\n    Returns c_full with length equal to number of columns in Theta.\n    \"\"\"\n    p = Theta.shape[1]\n    # Initial ridge solution\n    c = ridge_least_squares(Theta, y, alpha)\n    mask = np.abs(c) >= tau\n    if not np.any(mask):\n        return np.zeros(p)\n    # Refit with retained columns\n    Theta_reduced = Theta[:, mask]\n    c_reduced = ridge_least_squares(Theta_reduced, y, alpha)\n    c_full = np.zeros(p)\n    c_full[mask] = c_reduced\n    return c_full\n\ndef constrained_ridge_least_squares(Theta, y, alpha, g, h):\n    \"\"\"\n    Solve min ||Theta c - y||^2 + alpha ||c||^2 subject to g^T c = h\n    via KKT system:\n        [2(Theta^T Theta + alpha I), g] [c]   = [2 Theta^T y]\n        [g^T                          , 0] [λ]   [h]\n    Returns c.\n    \"\"\"\n    p = Theta.shape[1]\n    H = 2.0 * (Theta.T @ Theta + alpha * np.eye(p))\n    rhs1 = 2.0 * (Theta.T @ y)\n    # Ensure g is column vector shape (p,1)\n    g_col = g.reshape(p, 1)\n    # Build KKT matrix\n    KKT = np.block([[H, g_col],\n                    [g_col.T, np.zeros((1, 1))]])\n    rhs = np.concatenate([rhs1, np.array([h])], axis=0)\n    sol = np.linalg.solve(KKT, rhs)\n    c = sol[:p]\n    return c\n\ndef apply_conservation_constraint(y, Theta, L, N):\n    \"\"\"\n    Build discrete conservation constraint g^T c = h,\n    where g = dx * sum over all rows of Theta columns,\n          h = dx * sum over all entries of y.\n    \"\"\"\n    dx = L / N\n    g = dx * np.sum(Theta, axis=0)  # shape (2,)\n    h = dx * np.sum(y)\n    return g, h\n\ndef run_test_case(N, T_steps, dt, nu, eps, tau, delta, alpha, L=2*np.pi):\n    \"\"\"\n    Run a single test case: simulate, build system, fit unconstrained STLSQ and constrained ridge,\n    and decide if adding the constraint changed the discovered PDE by selecting the cubic term.\n    Returns boolean changed.\n    \"\"\"\n    # Simulate\n    U = simulate_pde(N=N, T_steps=T_steps, dt=dt, nu=nu, eps=eps, L=L)\n    # Build regression system\n    y, Theta = build_regression_system(U, dt, L=L)\n    # Unconstrained STLSQ\n    c_un = stlsq(Theta, y, alpha=alpha, tau=tau)\n    # Constrained ridge LS with mass-balance constraint\n    g, h = apply_conservation_constraint(y, Theta, L=L, N=N)\n    c_con = constrained_ridge_least_squares(Theta, y, alpha=alpha, g=g, h=h)\n    # Decide\n    cubic_un_present = np.abs(c_un[1]) >= delta\n    cubic_con_present = np.abs(c_con[1]) >= delta\n    changed = (not cubic_un_present) and cubic_con_present\n    return changed\n\ndef solve():\n    # Define constants\n    L = 2.0 * np.pi\n    alpha = 1e-8    # ridge regularization\n    delta = 1e-2    # detection threshold for presence\n    tau = 3e-2      # STLSQ pruning threshold\n\n    # Test cases: (N, T_steps, dt, nu, eps)\n    test_cases = [\n        (64, 200, 1e-3, 0.1, 0.02),  # Test 1: small cubic, constraint matters\n        (64, 200, 1e-3, 0.1, 0.0),   # Test 2: no cubic, both omit\n        (64, 200, 1e-3, 0.1, 0.15),  # Test 3: strong cubic, both include\n    ]\n\n    results = []\n    for N, T_steps, dt, nu, eps in test_cases:\n        changed = run_test_case(N=N, T_steps=T_steps, dt=dt, nu=nu, eps=eps,\n                                tau=tau, delta=delta, alpha=alpha, L=L)\n        results.append(changed)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}