{
    "hands_on_practices": [
        {
            "introduction": "从数据中发现控制方程的第一步，也是最关键的一步，是准确地估计时间导数和空间导数。然而，真实世界的测量数据总是被噪声污染，直接使用有限差分等方法会放大噪声，导致导数估计不可靠。本练习将通过分析几种经典的数值微分方案，带你深入理解在噪声影响下，估计偏差（由近似方案带来的系统误差）与方差（由测量噪声带来的随机误差）之间的基本权衡关系 。掌握这种权衡是选择和设计适用于符号回归的稳健微分技术的基础。",
            "id": "3812522",
            "problem": "考虑一个可微的标量状态变量 $x(t)$，它在一个均匀时间网格 $t_k = t_0 + k h$ 上被观测，采样间隔为 $h > 0$。观测值受到加性独立同分布高斯噪声的污染：$y_k = x(t_k) + \\varepsilon_k$，其中 $\\varepsilon_k \\sim \\mathcal{N}(0,\\sigma^2)$ 且相互独立。在用于多尺度建模中方程发现的符号回归中，在含噪测量下准确估计时间导数对于构建跨尺度的候选动力学定律的残差至关重要。\n\n定义以下三种从 $\\{y_k\\}$ 估计 $\\dot{x}(t_0)$ 的方案：\n- 前向差分估计量 $\\widehat{D}_{\\mathrm{f}} = \\frac{y_{1} - y_{0}}{h}$。\n- 中心差分估计量 $\\widehat{D}_{\\mathrm{c}} = \\frac{y_{1} - y_{-1}}{2 h}$。\n- Savitzky–Golay (SG) 二次最小二乘微分估计量 $\\widehat{D}_{\\mathrm{sg}}$，通过对五个点 $(s_k,y_k)$（其中 $k \\in \\{-2,-1,0,1,2\\}$，$s_k = k h$）进行最小二乘意义下的二次多项式 $p(s) = a + b s + c s^2$ 拟合得到，并取 $\\widehat{D}_{\\mathrm{sg}} = b$ 作为 $s=0$ 处的导数估计值。\n\n假设 $x(t)$ 在 $t_0$ 的一个邻域内是四次连续可微的，因此关于 $t_0$ 的泰勒展开式到四阶项都是有效的。仅使用基本展开（泰勒级数）和独立高斯噪声的性质，推导每个估计量的主阶偏差和方差，并写出相应的主阶均方误差 $\\mathrm{MSE} \\approx \\mathrm{Bias}^2 + \\mathrm{Var}$。然后，计算中心差分估计量和 Savitzky–Golay 二次五点估计量的最小可达均方误差（通过对 $h$ 进行优化）之比。将最终的比率表示为单个精确的解析表达式。最终答案不需要数值四舍五入，也不需要单位。",
            "solution": "该问题要求推导和比较应用于含噪数据的三种不同数值微分估计量的均方误差（MSE）。分析的基础在于使用泰勒级数将系统误差（偏差）与随机误差（方差）分离，然后通过调整采样间隔 $h$ 来优化它们之间的权衡。\n\n设 $x(t)$ 是一个在 $t_0$ 附近四次连续可微（$C^4$）的标量函数。在时间 $t_k = t_0 + k h$ 观测到的数据是 $y_k = x(t_k) + \\varepsilon_k$，其中 $\\varepsilon_k$ 是独立同分布（i.i.d.）的高斯随机变量，其均值为 $E[\\varepsilon_k] = 0$，方差为 $\\mathrm{Var}(\\varepsilon_k) = \\sigma^2$。目标量是一阶导数 $\\dot{x}(t_0)$，我们将其记为 $\\dot{x}_0$。在 $t_0$ 处的更高阶导数也类似地表示，例如 $\\dddot{x}_0$。\n\n$x(t_k)$ 在 $t_0$ 附近的泰勒展开式由下式给出：\n$$x(t_k) = x(t_0 + kh) = x_0 + (kh)\\dot{x}_0 + \\frac{(kh)^2}{2!}\\ddot{x}_0 + \\frac{(kh)^3}{3!}\\dddot{x}_0 + \\frac{(kh)^4}{4!}\\ddddot{x}_0 + O(h^5)$$\n由于噪声项 $\\varepsilon_k$ 是独立的且均值为零，任何观测值线性组合 $\\sum_k c_k y_k$ 的期望为 $E[\\sum_k c_k y_k] = \\sum_k c_k E[y_k] = \\sum_k c_k x(t_k)$。其方差为 $\\mathrm{Var}(\\sum_k c_k y_k) = \\sum_k c_k^2 \\mathrm{Var}(y_k) = \\sigma^2 \\sum_k c_k^2$。一个估计量 $\\widehat{D}$ 的均方误差是 $\\mathrm{MSE}(\\widehat{D}) = E[(\\widehat{D} - \\dot{x}_0)^2] = \\mathrm{Var}(\\widehat{D}) + (\\mathrm{Bias}(\\widehat{D}))^2$，其中偏差 $\\mathrm{Bias}(\\widehat{D}) = E[\\widehat{D}] - \\dot{x}_0$。\n\n我们依次分析每个估计量。\n\n首先，我们考虑前向差分估计量 $\\widehat{D}_{\\mathrm{f}} = \\frac{y_1 - y_0}{h}$。\n其期望为 $E[\\widehat{D}_{\\mathrm{f}}] = \\frac{x(t_1) - x(t_0)}{h}$。使用 $x(t_1) = x_0 + h\\dot{x}_0 + \\frac{h^2}{2}\\ddot{x}_0 + O(h^3)$ 的泰勒展开式：\n$$E[\\widehat{D}_{\\mathrm{f}}] = \\frac{(x_0 + h\\dot{x}_0 + \\frac{h^2}{2}\\ddot{x}_0 + O(h^3)) - x_0}{h} = \\dot{x}_0 + \\frac{h}{2}\\ddot{x}_0 + O(h^2)$$\n主阶偏差为 $\\mathrm{Bias}(\\widehat{D}_{\\mathrm{f}}) \\approx \\frac{h}{2}\\ddot{x}_0$。\n方差为 $\\mathrm{Var}(\\widehat{D}_{\\mathrm{f}}) = \\mathrm{Var}\\left(\\frac{y_1 - y_0}{h}\\right) = \\frac{1}{h^2}(\\mathrm{Var}(y_1) + \\mathrm{Var}(y_0)) = \\frac{2\\sigma^2}{h^2}$。\n主阶均方误差为 $\\mathrm{MSE}(\\widehat{D}_{\\mathrm{f}}) \\approx \\left(\\frac{h}{2}\\ddot{x}_0\\right)^2 + \\frac{2\\sigma^2}{h^2} = \\frac{h^2 \\ddot{x}_0^2}{4} + \\frac{2\\sigma^2}{h^2}$。\n\n其次，我们分析中心差分估计量 $\\widehat{D}_{\\mathrm{c}} = \\frac{y_1 - y_{-1}}{2h}$。\n其期望为 $E[\\widehat{D}_{\\mathrm{c}}] = \\frac{x(t_1) - x(t_{-1})}{2h}$。为了获得更高的精度，我们在泰勒级数中使用更多项：\n$x(t_1) = x_0 + h\\dot{x}_0 + \\frac{h^2}{2}\\ddot{x}_0 + \\frac{h^3}{6}\\dddot{x}_0 + O(h^5)$\n$x(t_{-1}) = x_0 - h\\dot{x}_0 + \\frac{h^2}{2}\\ddot{x}_0 - \\frac{h^3}{6}\\dddot{x}_0 + O(h^5)$\n差值为 $x(t_1) - x(t_{-1}) = 2h\\dot{x}_0 + \\frac{h^3}{3}\\dddot{x}_0 + O(h^5)$。\n$$E[\\widehat{D}_{\\mathrm{c}}] = \\frac{2h\\dot{x}_0 + \\frac{h^3}{3}\\dddot{x}_0 + O(h^5)}{2h} = \\dot{x}_0 + \\frac{h^2}{6}\\dddot{x}_0 + O(h^4)$$\n主阶偏差为 $\\mathrm{Bias}(\\widehat{D}_{\\mathrm{c}}) \\approx \\frac{h^2}{6}\\dddot{x}_0$。\n方差为 $\\mathrm{Var}(\\widehat{D}_{\\mathrm{c}}) = \\mathrm{Var}\\left(\\frac{y_1 - y_{-1}}{2h}\\right) = \\frac{1}{4h^2}(\\mathrm{Var}(y_1) + \\mathrm{Var}(y_{-1})) = \\frac{2\\sigma^2}{4h^2} = \\frac{\\sigma^2}{2h^2}$。\n中心差分估计量的主阶均方误差为：\n$$\\mathrm{MSE}_{\\mathrm{c}}(h) \\approx \\left(\\frac{h^2}{6}\\dddot{x}_0\\right)^2 + \\frac{\\sigma^2}{2h^2} = \\frac{(\\dddot{x}_0)^2}{36}h^4 + \\frac{\\sigma^2}{2}h^{-2}$$\n\n第三，我们分析 Savitzky–Golay (SG) 估计量 $\\widehat{D}_{\\mathrm{sg}}$。这是对点 $(s_k, y_k)$（其中 $k \\in \\{-2, -1, 0, 1, 2\\}$, $s_k=kh$）进行 $p(s) = a+bs+cs^2$ 最小二乘拟合得到的系数 $b$。我们最小化 $S = \\sum_{k=-2}^2 (y_k - a - bkh - ck^2h^2)^2$。由于拟合窗口的对称性，正规方程组解耦。通过设置 $\\frac{\\partial S}{\\partial b}=0$ 来找到关于 $b$ 的方程，得到 $\\sum_k (-kh)(y_k - a - bkh - ck^2h^2) = 0$。这可以简化为 $\\sum_k k y_k = b h \\sum_k k^2$。\n我们计算这个和：$\\sum_{k=-2}^2 k^2 = (-2)^2 + (-1)^2 + 0^2 + 1^2 + 2^2 = 10$。\n因此，$\\hat{b} = \\frac{\\sum_{k=-2}^2 k y_k}{10h}$。\n该估计量为 $\\widehat{D}_{\\mathrm{sg}} = \\frac{-2y_{-2} - y_{-1} + y_1 + 2y_2}{10h}$。\n其期望为 $E[\\widehat{D}_{\\mathrm{sg}}] = \\frac{1}{10h}(-2x(t_{-2}) - x(t_{-1}) + x(t_1) + 2x(t_2))$。\n我们展开每个 $x(t_k)$ 并对 $x$ 在 $t_0$ 处的各阶导数的系数求和：\n- $x_0$：$-2-1+1+2 = 0$。\n- $\\dot{x}_0$：$h(-2(-2) -1(-1) + 1(1) + 2(2)) = 10h$。\n- $\\ddot{x}_0$：$\\frac{h^2}{2!}(-2(-2)^2 -1(-1)^2 + 1(1)^2 + 2(2)^2) = 0$。\n- $\\dddot{x}_0$：$\\frac{h^3}{3!}(-2(-2)^3 -1(-1)^3 + 1(1)^3 + 2(2)^3) = \\frac{h^3}{6}(16+1+1+16) = \\frac{34h^3}{6}$。\n- $\\ddddot{x}_0$：$\\frac{h^4}{4!}(-2(-2)^4 -1(-1)^4 + 1(1)^4 + 2(2)^4) = 0$。\n$E[\\widehat{D}_{\\mathrm{sg}}]$ 的分子是 $10h\\dot{x}_0 + \\frac{17h^3}{3}\\dddot{x}_0 + O(h^5)$。\n$$E[\\widehat{D}_{\\mathrm{sg}}] = \\frac{10h\\dot{x}_0 + \\frac{17h^3}{3}\\dddot{x}_0 + O(h^5)}{10h} = \\dot{x}_0 + \\frac{17h^2}{30}\\dddot{x}_0 + O(h^4)$$\n主阶偏差为 $\\mathrm{Bias}(\\widehat{D}_{\\mathrm{sg}}) \\approx \\frac{17h^2}{30}\\dddot{x}_0$。\n方差为 $\\mathrm{Var}(\\widehat{D}_{\\mathrm{sg}}) = \\frac{1}{(10h)^2} \\mathrm{Var}(-2y_{-2} - y_{-1} + y_1 + 2y_2) = \\frac{\\sigma^2}{100h^2}((-2)^2 + (-1)^2 + 1^2 + 2^2) = \\frac{10\\sigma^2}{100h^2} = \\frac{\\sigma^2}{10h^2}$。\nSG 估计量的主阶均方误差为：\n$$\\mathrm{MSE}_{\\mathrm{sg}}(h) \\approx \\left(\\frac{17h^2}{30}\\dddot{x}_0\\right)^2 + \\frac{\\sigma^2}{10h^2} = \\frac{289(\\dddot{x}_0)^2}{900}h^4 + \\frac{\\sigma^2}{10}h^{-2}$$\n\n接下来，我们对 $\\mathrm{MSE}_{\\mathrm{c}}(h)$ 和 $\\mathrm{MSE}_{\\mathrm{sg}}(h)$ 关于 $h$ 进行最小化。两个表达式都具有 $f(h) = A h^4 + B h^{-2}$ 的形式。为了找到最小值，我们将导数设为零：\n$\\frac{df}{dh} = 4Ah^3 - 2Bh^{-3} = 0 \\implies 4Ah^6 = 2B \\implies h^6_{\\mathrm{opt}} = \\frac{B}{2A}$。\n在这个最优 $h$ 值下，最小均方误差为：\n$\\mathrm{MSE}_{\\mathrm{min}} = A(h_{\\mathrm{opt}})^4 + B(h_{\\mathrm{opt}})^{-2} = A\\left(\\frac{B}{2A}\\right)^{2/3} + B\\left(\\frac{2A}{B}\\right)^{1/3} = A^{1/3}B^{2/3}2^{-2/3} + B^{2/3}A^{1/3}2^{1/3} = (2^{-2/3}+2^{1/3})A^{1/3}B^{2/3} = 3 \\cdot 2^{-2/3}A^{1/3}B^{2/3}$。\n\n对于中心差分估计量：$A_c = \\frac{(\\dddot{x}_0)^2}{36}$ 且 $B_c = \\frac{\\sigma^2}{2}$。\n最小均方误差为：\n$$\\mathrm{MSE}_{\\mathrm{c,min}} = 3 \\cdot 2^{-2/3} \\left(\\frac{(\\dddot{x}_0)^2}{36}\\right)^{1/3} \\left(\\frac{\\sigma^2}{2}\\right)^{2/3} = 3 \\cdot 2^{-2/3} \\frac{(\\dddot{x}_0)^{2/3}}{36^{1/3}} \\frac{\\sigma^{4/3}}{2^{2/3}} = 3 \\frac{(\\dddot{x}_0)^{2/3} \\sigma^{4/3}}{2^{4/3} (6^2)^{1/3}} = \\frac{3 (\\dddot{x}_0)^{2/3} \\sigma^{4/3}}{2^{4/3} 2^{2/3} 3^{2/3}} = \\frac{3^{1/3}}{4} (\\dddot{x}_0)^{2/3} \\sigma^{4/3}$$\n\n对于 SG 估计量：$A_{\\mathrm{sg}} = \\frac{289(\\dddot{x}_0)^2}{900}$ 且 $B_{\\mathrm{sg}} = \\frac{\\sigma^2}{10}$。\n最小均方误差为：\n$$\\mathrm{MSE}_{\\mathrm{sg,min}} = 3 \\cdot 2^{-2/3} \\left(\\frac{289(\\dddot{x}_0)^2}{900}\\right)^{1/3} \\left(\\frac{\\sigma^2}{10}\\right)^{2/3} = 3 \\cdot 2^{-2/3} \\frac{(17^2)^{1/3}(\\dddot{x}_0)^{2/3}}{(900)^{1/3}} \\frac{\\sigma^{4/3}}{10^{2/3}}$$\n$$= 3 \\cdot 2^{-2/3} \\frac{17^{2/3}(\\dddot{x}_0)^{2/3}\\sigma^{4/3}}{(9 \\cdot 100)^{1/3} \\cdot 10^{2/3}} = 3 \\cdot 2^{-2/3} \\frac{17^{2/3}}{3^{2/3} \\cdot 10^{2/3} \\cdot 10^{2/3}} (\\dddot{x}_0)^{2/3}\\sigma^{4/3} = 3 \\cdot 2^{-2/3} \\frac{17^{2/3}}{3^{2/3} \\cdot 10^{4/3}} (\\dddot{x}_0)^{2/3}\\sigma^{4/3}$$\n$$= \\frac{3^{1/3} \\cdot 17^{2/3}}{2^{2/3} \\cdot (2 \\cdot 5)^{4/3}} (\\dddot{x}_0)^{2/3}\\sigma^{4/3} = \\frac{3^{1/3} \\cdot 17^{2/3}}{2^{2/3} \\cdot 2^{4/3} \\cdot 5^{4/3}} (\\dddot{x}_0)^{2/3}\\sigma^{4/3} = \\frac{3^{1/3} \\cdot 17^{2/3}}{4 \\cdot 5^{4/3}} (\\dddot{x}_0)^{2/3}\\sigma^{4/3}$$\n\n最后，我们计算最小可达均方误差的比率：\n$$\\frac{\\mathrm{MSE}_{\\mathrm{c,min}}}{\\mathrm{MSE}_{\\mathrm{sg,min}}} = \\frac{\\frac{3^{1/3}}{4} (\\dddot{x}_0)^{2/3} \\sigma^{4/3}}{\\frac{3^{1/3} \\cdot 17^{2/3}}{4 \\cdot 5^{4/3}} (\\dddot{x}_0)^{2/3} \\sigma^{4/3}} = \\frac{1}{\\frac{17^{2/3}}{5^{4/3}}} = \\frac{5^{4/3}}{17^{2/3}}$$\n这个比率与信号属性（$\\dddot{x}_0$）和噪声水平（$\\sigma$）无关，反映了估计量结构的根本性比较。\n该表达式也可以写成 $\\left(\\frac{5^4}{17^2}\\right)^{1/3} = \\left(\\frac{625}{289}\\right)^{1/3}$。第一种形式更简单。",
            "answer": "$$\\boxed{\\frac{5^{4/3}}{17^{2/3}}}$$"
        },
        {
            "introduction": "在获得了导数的估计值并构建了候选函数库（例如 $u, u^2, u_{xx}$ 等项）之后，符号回归的核心挑战转变为模型选择问题：如何从众多可能性中挑出“正确”的方程？奥卡姆剃刀原则——“如无必要，勿增实体”——为我们提供了指导，即我们寻求一个在精确性和简洁性之间达到最佳平衡的模型。本练习通过一个理想化的场景，让你从第一性原理出发，推导不同复杂度模型的带惩罚项的损失函数，并分析正则化参数 $\\lambda$ 如何控制模型选择的结果 。这个过程揭示了稀疏回归和信息准则（如AIC、BIC）背后的理论基石，是理解如何自动发现简约物理定律的关键。",
            "id": "3812595",
            "problem": "考虑一个符号回归问题，其目标是为一个在精细尺度上生成、在粗糙尺度上带有噪声观测的标量响应，恢复一个简约的闭式表达式。设输入为服从标准正态分布的随机变量 $x \\sim \\mathcal{N}(0,1)$，响应由 $y = x^{2} + \\epsilon$ 生成，其中噪声 $\\epsilon \\sim \\mathcal{N}(0,\\sigma^{2})$ 与 $x$ 独立。给定一个候选符号模型库，其仅限于带有自由标量系数的单项式，$\\mathcal{F} = \\{ f_{p}(x) = \\beta x^{p} : \\beta \\in \\mathbb{R},\\ p \\in \\{1,2,3\\} \\}$。\n\n定义总体惩罚目标\n$$\nL_{p}(\\lambda) \\equiv \\inf_{\\beta \\in \\mathbb{R}} \\ \\mathbb{E}\\big[(y - \\beta x^{p})^{2}\\big] + \\lambda\\,K(p),\n$$\n其中 $K(p)$ 是一个模型复杂度泛函，$\\lambda \\ge 0$ 是一个正则化强度，用于权衡数据保真度和复杂度。假设复杂度泛函等于多项式次数，$K(p) = p$。\n\n从第一性原理出发，即总体均方误差的定义以及标准正态分布的独立性和矩性质，对每个候选模型 $p \\in \\{1,2,3\\}$，推导使其未惩罚期望误差最小化的最优系数 $\\beta_{p}^{\\star}$、相应的最小期望误差，并由此得到作为 $\\sigma^{2}$ 和 $\\lambda$ 函数的显式权衡曲线 $L_{p}(\\lambda)$。使用这些表达式，确定在 $\\lambda \\ge 0$ 的哪个（些）区间上，每个候选模型是最优的，即在三个候选中达到最小的惩罚目标值。\n\n在最终答案中，报告最优模型从 $p=2$ 切换到另一个候选模型时的非负临界正则化强度。无需四舍五入。",
            "solution": "首先评估问题的有效性。\n\n**步骤1：提取已知条件**\n- 输入分布：$x \\sim \\mathcal{N}(0,1)$。\n- 真实数据生成过程：$y = x^{2} + \\epsilon$。\n- 噪声分布：$\\epsilon \\sim \\mathcal{N}(0,\\sigma^{2})$，其中 $\\epsilon$ 与 $x$ 独立。\n- 候选模型库：$\\mathcal{F} = \\{ f_{p}(x) = \\beta x^{p} : \\beta \\in \\mathbb{R},\\ p \\in \\{1,2,3\\} \\}$。\n- 总体惩罚目标函数：$L_{p}(\\lambda) \\equiv \\inf_{\\beta \\in \\mathbb{R}} \\ \\mathbb{E}\\big[(y - \\beta x^{p})^{2}\\big] + \\lambda\\,K(p)$。\n- 正则化参数：$\\lambda \\ge 0$。\n- 复杂度泛函：$K(p) = p$。\n\n**步骤2：使用提取的已知条件进行验证**\n- **科学依据：** 该问题是统计学习理论中的一个标准练习，具体涉及使用惩罚损失函数进行模型选择（本质上类似于赤池信息准则或贝叶斯信息准则）。它使用了概率、期望和优化的基本原理。所有前提都是完善确立的。\n- **适定性：** 该问题是适定的。目标函数涉及最小化参数 $\\beta$ 的一个二次函数，该函数有唯一的最小值。任务是为每个模型找到这个最小值，然后比较得到的惩罚目标，这是一个定义清晰的过程，会导向一组唯一的 $\\lambda$ 区间。\n- **客观性：** 该问题以精确的数学语言陈述，没有歧义或主观论断。\n\n**步骤3：结论与行动**\n该问题在科学上是合理的、适定的，并且陈述规范。它被认为是**有效的**。将提供完整解答。\n\n问题的核心是为由 $p \\in \\{1, 2, 3\\}$ 索引的每个候选模型评估惩罚目标 $L_{p}(\\lambda)$。该目标由两部分组成：最小期望均方误差 (MSE) 和一个复杂度惩罚。\n$L_{p}(\\lambda) = \\min_{\\beta \\in \\mathbb{R}} \\mathbb{E}[(y - \\beta x^{p})^{2}] + \\lambda p$。\n\n令未惩罚的期望误差为 $E_p(\\beta) = \\mathbb{E}[(y - \\beta x^{p})^{2}]$。为了找到最小化此项的最优系数 $\\beta_p^{\\star}$，我们对 $\\beta$ 求导并令其为零。\n$$\n\\frac{dE_p(\\beta)}{d\\beta} = \\frac{d}{d\\beta} \\mathbb{E}[y^2 - 2\\beta yx^p + \\beta^2 (x^p)^2] = \\mathbb{E}[-2yx^p + 2\\beta x^{2p}]\n$$\n将导数设为零得到最优系数 $\\beta_p^{\\star}$：\n$$\n-2\\mathbb{E}[yx^p] + 2\\beta_p^{\\star}\\mathbb{E}[x^{2p}] = 0 \\implies \\beta_p^{\\star} = \\frac{\\mathbb{E}[yx^p]}{\\mathbb{E}[x^{2p}]}\n$$\n二阶导数为 $2\\mathbb{E}[x^{2p}] > 0$，因为 $x^{2p}$ 是一个不几乎处处为零的非负随机变量，这证实了这是一个最小值。\n\n为了计算 $\\beta_p^{\\star}$，我们需要计算期望值。变量 $x$ 服从标准正态分布，$x \\sim \\mathcal{N}(0,1)$。标准正态分布的矩由以下公式给出：当 $k$ 为奇数时，$\\mathbb{E}[x^k] = 0$；当 $k$ 为偶数时，$\\mathbb{E}[x^k] = (k-1)!!$。\n所需的矩为：\n- $\\mathbb{E}[x^1] = 0$\n- $\\mathbb{E}[x^2] = 1$\n- $\\mathbb{E}[x^3] = 0$\n- $\\mathbb{E}[x^4] = (3)!! = 3 \\cdot 1 = 3$\n- $\\mathbb{E}[x^5] = 0$\n- $\\mathbb{E}[x^6] = (5)!! = 5 \\cdot 3 \\cdot 1 = 15$\n\n接下来，我们计算 $\\mathbb{E}[yx^p]$。代入 $y = x^2 + \\epsilon$：\n$$\n\\mathbb{E}[yx^p] = \\mathbb{E}[(x^2 + \\epsilon)x^p] = \\mathbb{E}[x^{p+2} + \\epsilon x^p] = \\mathbb{E}[x^{p+2}] + \\mathbb{E}[\\epsilon x^p]\n$$\n由于 $x$ 和 $\\epsilon$ 是独立的，$\\mathbb{E}[\\epsilon x^p] = \\mathbb{E}[\\epsilon]\\mathbb{E}[x^p]$。给定 $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$，我们有 $\\mathbb{E}[\\epsilon]=0$。因此，$\\mathbb{E}[\\epsilon x^p] = 0$。\n所以，$\\mathbb{E}[yx^p] = \\mathbb{E}[x^{p+2}]$。\n\n现在我们可以为每个 $p \\in \\{1, 2, 3\\}$ 计算 $\\beta_p^{\\star}$：\n- 对于 $p=1$：$\\beta_1^{\\star} = \\frac{\\mathbb{E}[x^{1+2}]}{\\mathbb{E}[x^{2 \\cdot 1}]} = \\frac{\\mathbb{E}[x^3]}{\\mathbb{E}[x^2]} = \\frac{0}{1} = 0$。\n- 对于 $p=2$：$\\beta_2^{\\star} = \\frac{\\mathbb{E}[x^{2+2}]}{\\mathbb{E}[x^{2 \\cdot 2}]} = \\frac{\\mathbb{E}[x^4]}{\\mathbb{E}[x^4]} = \\frac{3}{3} = 1$。\n- 对于 $p=3$：$\\beta_3^{\\star} = \\frac{\\mathbb{E}[x^{3+2}]}{\\mathbb{E}[x^{2 \\cdot 3}]} = \\frac{\\mathbb{E}[x^5]}{\\mathbb{E}[x^6]} = \\frac{0}{15} = 0$。\n\n接下来，我们求最小未惩罚误差，$\\min_{\\beta} E_p(\\beta) = E_p(\\beta_p^{\\star})$。\n$$\nE_p(\\beta_p^{\\star}) = \\mathbb{E}[(y - \\beta_p^{\\star} x^p)^2] = \\mathbb{E}[y^2] - 2\\beta_p^{\\star}\\mathbb{E}[yx^p] + (\\beta_p^{\\star})^2\\mathbb{E}[x^{2p}]\n$$\n代入 $\\beta_p^{\\star} = \\frac{\\mathbb{E}[yx^p]}{\\mathbb{E}[x^{2p}]}$，上式简化为：\n$$\nE_p(\\beta_p^{\\star}) = \\mathbb{E}[y^2] - \\frac{(\\mathbb{E}[yx^p])^2}{\\mathbb{E}[x^{2p}]}\n$$\n我们计算 $\\mathbb{E}[y^2]$：\n$$\n\\mathbb{E}[y^2] = \\mathbb{E}[(x^2 + \\epsilon)^2] = \\mathbb{E}[x^4 + 2\\epsilon x^2 + \\epsilon^2] = \\mathbb{E}[x^4] + 2\\mathbb{E}[\\epsilon x^2] + \\mathbb{E}[\\epsilon^2]\n$$\n利用独立性，$\\mathbb{E}[\\epsilon x^2]=\\mathbb{E}[\\epsilon]\\mathbb{E}[x^2] = 0 \\cdot 1 = 0$。对于噪声项，$\\mathbb{E}[\\epsilon^2] = \\text{Var}(\\epsilon) + (\\mathbb{E}[\\epsilon])^2 = \\sigma^2 + 0^2 = \\sigma^2$。\n因此，$\\mathbb{E}[y^2] = \\mathbb{E}[x^4] + \\sigma^2 = 3 + \\sigma^2$。\n\n现在我们为每个模型计算最小误差：\n- 对于 $p=1$：最小误差 $= (3 + \\sigma^2) - \\frac{(\\mathbb{E}[x^3])^2}{\\mathbb{E}[x^2]} = (3 + \\sigma^2) - \\frac{0^2}{1} = 3 + \\sigma^2$。\n- 对于 $p=2$：最小误差 $= (3 + \\sigma^2) - \\frac{(\\mathbb{E}[x^4])^2}{\\mathbb{E}[x^4]} = (3 + \\sigma^2) - \\frac{3^2}{3} = 3 + \\sigma^2 - 3 = \\sigma^2$。\n- 对于 $p=3$：最小误差 $= (3 + \\sigma^2) - \\frac{(\\mathbb{E}[x^5])^2}{\\mathbb{E}[x^6]} = (3 + \\sigma^2) - \\frac{0^2}{15} = 3 + \\sigma^2$。\n\n惩罚目标函数 $L_p(\\lambda)$ 为：\n- $L_1(\\lambda) = (3 + \\sigma^2) + \\lambda \\cdot 1 = 3 + \\sigma^2 + \\lambda$。\n- $L_2(\\lambda) = \\sigma^2 + \\lambda \\cdot 2 = \\sigma^2 + 2\\lambda$。\n- $L_3(\\lambda) = (3 + \\sigma^2) + \\lambda \\cdot 3 = 3 + \\sigma^2 + 3\\lambda$。\n\n为了找到每个模型最优的 $\\lambda$ 区间，我们对 $\\lambda \\ge 0$ 的情况比较这三个关于 $\\lambda$ 的线性函数。\n\n首先，比较 $L_1(\\lambda)$ 和 $L_3(\\lambda)$：\n$L_3(\\lambda) - L_1(\\lambda) = (3 + \\sigma^2 + 3\\lambda) - (3 + \\sigma^2 + \\lambda) = 2\\lambda$。\n由于 $\\lambda \\ge 0$，所以 $2\\lambda \\ge 0$。因此，对于所有 $\\lambda \\ge 0$，$L_3(\\lambda) \\ge L_1(\\lambda)$。模型 $p=3$ 永远不会严格优于 $p=1$，因此永远不是唯一的最优模型。\n\n现在，比较剩下的候选模型 $p=1$ 和 $p=2$。最优模型将是 $L_p(\\lambda)$ 值较小的那个。我们通过设 $L_1(\\lambda) = L_2(\\lambda)$ 来找到交叉点：\n$$\n3 + \\sigma^2 + \\lambda = \\sigma^2 + 2\\lambda\n$$\n$$\n3 = \\lambda\n$$\n这就是临界正则化强度，$\\lambda_c = 3$。\n\n为了确定在这个临界值的两侧哪个模型是最优的：\n- 当 $\\lambda  3$ 时：\n  一个更简单的比较是直接的：\n  $L_2(\\lambda)  L_1(\\lambda) \\iff \\sigma^2 + 2\\lambda  3 + \\sigma^2 + \\lambda \\iff \\lambda  3$。\n  因此，当 $0 \\le \\lambda  3$ 时，模型 $p=2$ 是最优的。\n- 当 $\\lambda > 3$ 时：\n  $L_1(\\lambda)  L_2(\\lambda) \\iff 3 + \\sigma^2 + \\lambda  \\sigma^2 + 2\\lambda \\iff 3  \\lambda$。\n  因此，当 $\\lambda > 3$ 时，模型 $p=1$ 是最优的。\n- 当 $\\lambda = 3$ 时：\n  $L_1(3) = L_2(3)$，所以模型 $p=1$ 和 $p=2$ 同等最优。\n\n模型 $p=2$ 是正确的底层模型结构，对数据有最好的拟合度，因此在复杂度惩罚（由 $\\lambda$ 控制）较小时更受青睐。随着 $\\lambda$ 的增加，$p=2$ 的复杂度惩罚（$2\\lambda$）比更简单的模型 $p=1$ 的惩罚（$\\lambda$）增长得更快。最终，在 $\\lambda=3$ 时，惩罚超过了更好拟合带来的优势，更简单（但有偏）的模型 $p=1$ 变得更优。\n\n问题要求的是最优模型从 $p=2$ 切换出去时的临界正则化强度。这发生在 $L_2(\\lambda)$ 不再是唯一最小值的那一点，即 $\\lambda=3$。\n\n最优区间为：\n- $p=2$ 在 $\\lambda \\in [0, 3]$ 上是最优的。\n- $p=1$ 在 $\\lambda \\in [3, \\infty)$ 上是最优的。\n- $p=3$ 在 $\\lambda \\ge 0$ 上永远不是唯一的最优模型。\n\n从 $p=2$ 作为唯一最优模型切换出去发生在边界处，即临界值 $\\lambda_c = 3$。",
            "answer": "$$\\boxed{3}$$"
        },
        {
            "introduction": "为了进一步提升方程发现的准确性和鲁棒性，我们可以将已知的物理先验知识整合到回归过程中。本练习将指导你完成一个完整的编程实践，展示一种强大的“物理信息”方法：将守恒定律（如质量守恒）作为等式约束，应用于稀疏回归求解器中 。通过对比无约束和有约束两种情况下的结果，你将亲身体会到，当某些项的信号微弱或数据含噪较大时，物理约束如何帮助算法做出正确的模型选择，从而发现更加符合物理现实的控制方程。",
            "id": "3812529",
            "problem": "考虑在一维周期性域上使用符号回归进行方程发现的任务，具体对象为偏微分方程（PDE）。目标是确定添加守恒约束在何时以及为何会将发现的PDE模型从纯扩散模型转变为包含三次非线性的反应扩散模型。您将实现一个程序，该程序从已知的PDE构建合成数据，建立特征库，执行稀疏回归以识别控制方程，然后使用编码守恒律的等式约束重复进行识别。该程序将针对多个测试用例报告，添加约束是否通过选择一个先前被省略的非线性项来改变已发现的PDE。\n\n基本原理：\n- 假设控制PDE的形式为\n$$\nu_t = \\nu\\, u_{xx} - \\epsilon\\, u^3,\n$$\n在长度为 $L = 2\\pi$ 的域上具有周期性边界条件，其中 $u(x,t)$ 是状态，$\\nu  0$ 是扩散系数，$\\epsilon \\ge 0$ 控制三次反应项的强度。\n- 对于周期性边界条件，拉普拉斯项的积分消失：\n$$\n\\int_0^{L} u_{xx}\\, dx = 0.\n$$\n- 因此，空间平均值（也称为质量）$M(t) = \\int_0^{L} u(x,t)\\, dx$ 遵循以下平衡方程\n$$\n\\frac{dM}{dt} = \\int_0^{L} u_t\\, dx = -\\epsilon \\int_0^{L} u^3\\, dx.\n$$\n\n问题设置：\n1. 数据生成。在 $x \\in [0,2\\pi]$ 上使用显式时间步进格式模拟具有周期性边界条件的PDE\n$$\nu_t = \\nu\\, u_{xx} - \\epsilon\\, u^3\n$$\n。使用 $N$ 个点的均匀空间网格和均匀时间步长 $\\Delta t$，共进行 $T$ 个时间步。初始化条件为\n$$\nu(x,0) = a_0 + a_1 \\sin(x) + a_2 \\cos(2x),\n$$\n其中 $a_0$、$a_1$、$a_2$ 是固定的实数常量。使用离散傅里叶变换，通过以下公式以谱方法计算 $u_{xx}$：\n$$\n\\widehat{u_{xx}}(k) = -k^2 \\hat{u}(k),\n$$\n其中 $k$ 是角波数，$\\hat{u}(k)$ 是 $u$ 的离散傅里叶变换。\n\n2. 特征库构建与回归。在每个时间步 $n \\in \\{0,\\dots,T-1\\}$ 和空间网格点 $j \\in \\{0,\\dots,N-1\\}$，构建目标和特征\n$$\ny_{n,j} = \\frac{u_{n+1,j} - u_{n,j}}{\\Delta t}, \\quad \\phi^{(1)}_{n,j} = (u_{xx})_{n,j}, \\quad \\phi^{(2)}_{n,j} = \\left(u_{n,j}\\right)^3,\n$$\n并将它们堆叠成一个回归系统\n$$\n\\Theta\\, c \\approx y,\n$$\n其中 $\\Theta \\in \\mathbb{R}^{(NT) \\times 2}$ 的列是 $\\phi^{(1)}$ 和 $\\phi^{(2)}$，$c \\in \\mathbb{R}^2$ 是对应于 $u_{xx}$ 和 $u^3$ 的未知系数，$y \\in \\mathbb{R}^{NT}$ 堆叠了各项 $y_{n,j}$。\n\n3. 无约束稀疏回归。求解一个岭正则化最小二乘问题以获得初始估计 $\\hat{c}$，然后应用序列阈值最小二乘法（STLSQ）：将 $\\hat{c}$ 中绝对值低于阈值 $\\tau$ 的任何分量置零，并在保留的列上使用岭正则化最小二乘法重新拟合。将得到的系数表示为 $c^{\\mathrm{un}}$。定义一个检测阈值 $\\delta  0$，并规定当且仅当一个项的系数绝对值至少为 $\\delta$ 时，该项才算“存在”。\n\n4. 守恒约束回归。施加从周期性推导出的时间平均质量平衡等式约束。在离散形式下，设均匀网格间距为 $\\Delta x = L/N$，令\n$$\ng = \\Delta x \\begin{bmatrix} \\sum_{n,j} \\phi^{(1)}_{n,j} \\\\ \\sum_{n,j} \\phi^{(2)}_{n,j} \\end{bmatrix}, \\quad h = \\Delta x \\sum_{n,j} y_{n,j}.\n$$\n求解岭正则化等式约束最小二乘问题\n$$\n\\min_{c \\in \\mathbb{R}^2} \\left\\| \\Theta c - y \\right\\|_2^2 + \\alpha \\left\\| c \\right\\|_2^2 \\quad \\text{subject to} \\quad g^\\top c = h,\n$$\n其中有一个小的 $\\alpha  0$ 以保证数值稳定性。将得到的系数表示为 $c^{\\mathrm{con}}$。根据相同的检测阈值 $\\delta$，报告三次项是否“存在”。\n\n5. 决策规则。对于每个测试用例，输出一个布尔值，指示添加守恒约束是否将发现的PDE从仅扩散模型更改为包含三次项的反应扩散模型。形式上，输出\n$$\n\\text{changed} = \\left(\\left|c^{\\mathrm{un}}_2\\right|  \\delta\\right) \\wedge \\left(\\left|c^{\\mathrm{con}}_2\\right| \\ge \\delta\\right),\n$$\n其中 $c^{\\mathrm{un}}_2$ 和 $c^{\\mathrm{con}}_2$ 分别是无约束和有约束拟合中乘以 $u^3$ 的系数。\n\n测试套件：\n使用以下三组参数来测试解决方案的不同方面。在所有情况下，取 $L = 2\\pi$，$a_0 = 0.5$，$a_1 = 0.4$，$a_2 = -0.2$，岭参数 $\\alpha = 10^{-8}$，检测阈值 $\\delta = 0.01$，以及STLSQ阈值 $\\tau = 0.03$。\n\n- 测试 1（约束起作用的典型情况）：$N = 64$，$T = 200$，$\\Delta t = 0.001$，$\\nu = 0.1$，$\\epsilon = 0.02$。\n- 测试 2（无反应的边界情况）：$N = 64$，$T = 200$，$\\Delta t = 0.001$，$\\nu = 0.1$，$\\epsilon = 0.0$。\n- 测试 3（强反应，两种方法都恢复三次项）：$N = 64$，$T = 200$，$\\Delta t = 0.001$，$\\nu = 0.1$，$\\epsilon = 0.15$。\n\n要求的最终输出：\n您的程序应生成单行输出，其中包含按上述测试顺序排列的结果，形式为方括号内以逗号分隔的列表，例如\n$$\n[\\text{result}_1,\\text{result}_2,\\text{result}_3],\n$$\n其中每个 $\\text{result}_i$ 是根据决策规则计算的布尔值。不应产生任何其他输出。\n\n角度单位：如果使用任何三角函数，角度变量必须以弧度为单位。最终答案中无需报告任何物理单位。",
            "solution": "我们从两个基本事实出发。首先，周期性边界条件意味着 $\\int_0^{2\\pi} u_{xx}\\, dx = 0$，因为二阶导数积分后得到一个边界项，在周期性区间上该项会抵消。其次，假设控制模型具有一般形式 $u_t = \\nu u_{xx} - \\epsilon u^3$，这是一个带有三次汇的反应扩散方程。对其进行空间积分，得到质量平衡\n$$\n\\frac{d}{dt}\\int_0^{2\\pi} u(x,t)\\, dx \\,=\\, \\nu \\int_0^{2\\pi} u_{xx}\\, dx \\,-\\, \\epsilon \\int_0^{2\\pi} u^3\\, dx \\,=\\, -\\epsilon \\int_0^{2\\pi} u^3\\, dx.\n$$\n因此，扩散项不能改变空间平均值，而三次项通常会改变，前提是 $u^3$ 的空间平均值非零。这是我们将在等式约束回归中编码的关键物理和数学约束。\n\n算法设计分四步进行。\n\n第 1 步：数据模拟。我们将 $x \\in [0,2\\pi]$ 离散化为一个包含 $N$ 个点的均匀网格，间距为 $\\Delta x = 2\\pi/N$，时间 $t \\in \\{0,\\Delta t,2\\Delta t,\\dots,T\\Delta t\\}$。我们将 $u(x,0) = a_0 + a_1 \\sin(x) + a_2 \\cos(2x)$ 初始化，以确保有一个正偏置，从而使 $\\int u^3 dx$ 通常非零。我们使用前向欧拉法进行时间推进，\n$$\nu^{n+1} = u^n + \\Delta t\\left(\\nu\\, u_{xx}^n - \\epsilon\\, (u^n)^3\\right),\n$$\n并为了准确性和周期性，用谱方法计算拉普拉斯算子：\n$$\n\\widehat{u_{xx}}(k) = -k^2 \\widehat{u}(k),\n$$\n其中 $k$ 是角波数，由 $k = 2\\pi\\, \\mathrm{fftfreq}(N, \\Delta x)$ 给出，$\\widehat{u}$ 表示 $u$ 的离散傅里叶变换。\n\n第 2 步：特征库构建。对于每个时间步 $n \\in \\{0,\\dots,T-1\\}$，我们通过有限差分计算瞬时时间导数\n$$\ny^n = \\frac{u^{n+1} - u^n}{\\Delta t},\n$$\n和特征\n$$\n\\phi^{(1),n} = u_{xx}^n,\\quad \\phi^{(2),n} = \\left(u^n\\right)^3.\n$$\n然后我们将所有时空样本堆叠起来，形成回归系统 $\\Theta c \\approx y$，其中 $\\Theta \\in \\mathbb{R}^{(NT)\\times 2}$。\n\n第 3 步：无约束稀疏回归。我们首先求解一个岭正则化最小二乘问题\n$$\n\\hat{c} = \\arg\\min_c \\left\\|\\Theta c - y\\right\\|_2^2 + \\alpha \\left\\|c\\right\\|_2^2,\n$$\n其中有一个小的 $\\alpha  0$ 以保证数值稳定性。接下来，我们应用序列阈值最小二乘法（STLSQ）：对于一个用户指定的阈值 $\\tau  0$，我们将 $\\hat{c}$ 中绝对值小于 $\\tau$ 的任何分量置零，并对保留下来的列进行岭正则化最小二乘的重新拟合。这得到 $c^{\\mathrm{un}}$。最后，我们声明当且仅当 $\\left|c^{\\mathrm{un}}_2\\right| \\ge \\delta$ 时，三次项被检测到，其中 $\\delta  0$ 是一个不同于 $\\tau$ 的检测阈值。\n\n在实践中，如果 $\\epsilon$ 很小，那么 $\\left|c^{\\mathrm{un}}_2\\right|$ 可能会低于 $\\tau$，三次项被剪除，从而得到一个仅扩散模型 $u_t \\approx \\nu u_{xx}$。\n\n第 4 步：守恒约束回归。我们现在编码质量平衡约束。在离散形式下，设 $\\Delta x = 2\\pi/N$，定义\n$$\ng = \\Delta x \\begin{bmatrix} \\sum_{n=0}^{T-1}\\sum_{j=0}^{N-1} \\phi^{(1)}_{n,j} \\\\ \\sum_{n=0}^{T-1}\\sum_{j=0}^{N-1} \\phi^{(2)}_{n,j} \\end{bmatrix}, \\quad\nh = \\Delta x \\sum_{n=0}^{T-1}\\sum_{j=0}^{N-1} y_{n,j}.\n$$\n对于周期性网格，在每个 $n$ 处，$\\sum_j \\phi^{(1)}_{n,j} \\approx 0$，所以 $g$ 的第一个分量近似为零，而第二个分量捕捉了 $\\int u^3 dx$ 的累积贡献。我们求解等式约束岭回归\n$$\n\\min_c \\left\\|\\Theta c - y\\right\\|_2^2 + \\alpha \\left\\|c\\right\\|_2^2 \\quad \\text{subject to} \\quad g^\\top c = h.\n$$\n引入一个拉格朗日乘子 $\\lambda$，Karush–Kuhn–Tucker (KKT) 系统为\n$$\n\\begin{bmatrix}\n2(\\Theta^\\top \\Theta + \\alpha I)  g \\\\\ng^\\top  0\n\\end{bmatrix}\n\\begin{bmatrix}\nc \\\\ \\lambda\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n2 \\Theta^\\top y \\\\ h\n\\end{bmatrix}.\n$$\n求解这个线性系统得到 $c^{\\mathrm{con}}$。因为扩散项积分为零，数据中观察到的空间平均值的任何非零时间变化都必须归因于三次项，而约束强制执行了这种归因。因此，当 $\\epsilon$ 很小但非零时，无约束稀疏回归可能会剪除三次项，而约束回归将恢复一个与质量平衡一致的非零系数。\n\n测试套件和预期行为：\n- 测试 1，$\\epsilon = 0.02$ 且 $\\tau = 0.03$：无约束STLSQ会剪除三次项，因为 $\\left|c^{\\mathrm{un}}_2\\right|  \\tau$，结果是仅扩散模型。约束回归强制执行质量损失，得到 $\\left|c^{\\mathrm{con}}_2\\right| \\gtrsim \\delta$，因此“changed”为真。\n- 测试 2，$\\epsilon = 0.0$：无约束和有约束方法都发现三次项不存在，$\\left|c^{\\mathrm{un}}_2\\right| \\approx 0$ 且 $\\left|c^{\\mathrm{con}}_2\\right| \\approx 0$，所以“changed”为假。\n- 测试 3，$\\epsilon = 0.15$ 且 $\\tau = 0.03$：三次项很强，所以两种方法都检测到它，“changed”为假。\n\n程序实现了模拟、特征构建、无约束STLSQ和通过KKT系统的等式约束回归，并按顺序输出包含三个测试的布尔结果的单行，格式为方括号内的逗号分隔列表。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef spectral_laplacian(u, L):\n    \"\"\"\n    Compute u_xx using spectral differentiation on a periodic domain of length L.\n    u: shape (N,), real-valued\n    returns: shape (N,), real-valued u_xx\n    \"\"\"\n    N = u.size\n    dx = L / N\n    # Frequencies in cycles per unit length\n    freq = np.fft.fftfreq(N, d=dx)\n    k = 2.0 * np.pi * freq  # angular wavenumbers\n    uhat = np.fft.fft(u)\n    uxx_hat = -(k ** 2) * uhat\n    u_xx = np.fft.ifft(uxx_hat).real\n    return u_xx\n\ndef simulate_pde(N, T_steps, dt, nu, eps, L=2*np.pi, a0=0.5, a1=0.4, a2=-0.2):\n    \"\"\"\n    Simulate u_t = nu u_xx - eps u^3 on a periodic domain [0, L] with N grid points.\n    Forward Euler time stepping, spectral u_xx.\n    Returns:\n        U: array shape (T_steps+1, N)\n    \"\"\"\n    x = np.linspace(0.0, L, N, endpoint=False)\n    u = a0 + a1 * np.sin(x) + a2 * np.cos(2.0 * x)\n    U = np.empty((T_steps + 1, N), dtype=float)\n    U[0] = u\n    for n in range(T_steps):\n        u_xx = spectral_laplacian(u, L)\n        rhs = nu * u_xx - eps * (u ** 3)\n        u = u + dt * rhs\n        U[n + 1] = u\n    return U\n\ndef build_regression_system(U, dt, L=2*np.pi):\n    \"\"\"\n    Build y and Theta from simulation data U.\n    U: array shape (T_steps+1, N)\n    Returns:\n        y: shape (T_steps*N,)\n        Theta: shape (T_steps*N, 2) with columns [u_xx, u^3]\n    \"\"\"\n    T_steps = U.shape[0] - 1\n    N = U.shape[1]\n    # Preallocate\n    y_list = []\n    f1_list = []\n    f2_list = []\n    for n in range(T_steps):\n        u_n = U[n]\n        u_np1 = U[n + 1]\n        y_n = (u_np1 - u_n) / dt\n        u_xx_n = spectral_laplacian(u_n, L)\n        f1_list.append(u_xx_n.reshape(-1))\n        f2_list.append((u_n ** 3).reshape(-1))\n        y_list.append(y_n.reshape(-1))\n    f1 = np.concatenate(f1_list, axis=0)\n    f2 = np.concatenate(f2_list, axis=0)\n    y = np.concatenate(y_list, axis=0)\n    Theta = np.column_stack((f1, f2))\n    return y, Theta\n\ndef ridge_least_squares(Theta, y, alpha):\n    \"\"\"\n    Solve min ||Theta c - y||^2 + alpha ||c||^2\n    Returns c (shape (p,))\n    \"\"\"\n    p = Theta.shape[1]\n    A = Theta.T @ Theta + alpha * np.eye(p)\n    b = Theta.T @ y\n    c = np.linalg.solve(A, b)\n    return c\n\ndef stlsq(Theta, y, alpha, tau):\n    \"\"\"\n    Sequentially Thresholded Least Squares with ridge regularization.\n    1) Solve ridge LS for all features.\n    2) Zero coefficients with |c|  tau.\n    3) Refit ridge LS on retained features. If none retained, return zeros.\n    Returns c_full with length equal to number of columns in Theta.\n    \"\"\"\n    p = Theta.shape[1]\n    # Initial ridge solution\n    c = ridge_least_squares(Theta, y, alpha)\n    mask = np.abs(c) >= tau\n    if not np.any(mask):\n        return np.zeros(p)\n    # Refit with retained columns\n    Theta_reduced = Theta[:, mask]\n    c_reduced = ridge_least_squares(Theta_reduced, y, alpha)\n    c_full = np.zeros(p)\n    c_full[mask] = c_reduced\n    return c_full\n\ndef constrained_ridge_least_squares(Theta, y, alpha, g, h):\n    \"\"\"\n    Solve min ||Theta c - y||^2 + alpha ||c||^2 subject to g^T c = h\n    via KKT system:\n        [2(Theta^T Theta + alpha I), g] [c]   = [2 Theta^T y]\n        [g^T                          , 0] [λ]   [h]\n    Returns c.\n    \"\"\"\n    p = Theta.shape[1]\n    H = 2.0 * (Theta.T @ Theta + alpha * np.eye(p))\n    rhs1 = 2.0 * (Theta.T @ y)\n    # Ensure g is column vector shape (p,1)\n    g_col = g.reshape(p, 1)\n    # Build KKT matrix\n    KKT = np.block([[H, g_col],\n                    [g_col.T, np.zeros((1, 1))]])\n    rhs = np.concatenate([rhs1, np.array([h])], axis=0)\n    sol = np.linalg.solve(KKT, rhs)\n    c = sol[:p]\n    return c\n\ndef apply_conservation_constraint(y, Theta, L, N):\n    \"\"\"\n    Build discrete conservation constraint g^T c = h,\n    where g = dx * sum over all rows of Theta columns,\n          h = dx * sum over all entries of y.\n    \"\"\"\n    dx = L / N\n    g = dx * np.sum(Theta, axis=0)  # shape (2,)\n    h = dx * np.sum(y)\n    return g, h\n\ndef run_test_case(N, T_steps, dt, nu, eps, tau, delta, alpha, L=2*np.pi):\n    \"\"\"\n    Run a single test case: simulate, build system, fit unconstrained STLSQ and constrained ridge,\n    and decide if adding the constraint changed the discovered PDE by selecting the cubic term.\n    Returns boolean changed.\n    \"\"\"\n    # Simulate\n    U = simulate_pde(N=N, T_steps=T_steps, dt=dt, nu=nu, eps=eps, L=L)\n    # Build regression system\n    y, Theta = build_regression_system(U, dt, L=L)\n    # Unconstrained STLSQ\n    c_un = stlsq(Theta, y, alpha=alpha, tau=tau)\n    # Constrained ridge LS with mass-balance constraint\n    g, h = apply_conservation_constraint(y, Theta, L=L, N=N)\n    c_con = constrained_ridge_least_squares(Theta, y, alpha=alpha, g=g, h=h)\n    # Decide\n    cubic_un_present = np.abs(c_un[1]) >= delta\n    cubic_con_present = np.abs(c_con[1]) >= delta\n    changed = (not cubic_un_present) and cubic_con_present\n    return changed\n\ndef solve():\n    # Define constants\n    L = 2.0 * np.pi\n    alpha = 1e-8    # ridge regularization\n    delta = 1e-2    # detection threshold for presence\n    tau = 3e-2      # STLSQ pruning threshold\n\n    # Test cases: (N, T_steps, dt, nu, eps)\n    test_cases = [\n        (64, 200, 1e-3, 0.1, 0.02),  # Test 1: small cubic, constraint matters\n        (64, 200, 1e-3, 0.1, 0.0),   # Test 2: no cubic, both omit\n        (64, 200, 1e-3, 0.1, 0.15),  # Test 3: strong cubic, both include\n    ]\n\n    results = []\n    for N, T_steps, dt, nu, eps in test_cases:\n        changed = run_test_case(N=N, T_steps=T_steps, dt=dt, nu=nu, eps=eps,\n                                tau=tau, delta=delta, alpha=alpha, L=L)\n        results.append(changed)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}