{
    "hands_on_practices": [
        {
            "introduction": "符号回归的核心不仅仅是找到一个完美拟合数据的模型，更在于发现能够充分解释数据的最简洁模型。这个练习将引导你通过一个带有惩罚项的目标函数来量化这种权衡关系。通过推导正则化参数 $\\lambda$ 如何影响模型选择，你将亲身体验奥卡姆剃刀原理在发现控制方程中的应用。",
            "id": "3812595",
            "problem": "考虑一个符号回归的情景，其目标是为一个在细尺度上生成并在粗尺度上带有噪声观测的标量响应，恢复一个简约的闭式表达式。设输入为一个标准正态随机变量 $x \\sim \\mathcal{N}(0,1)$，响应由 $y = x^{2} + \\epsilon$ 生成，其中噪声 $\\epsilon \\sim \\mathcal{N}(0,\\sigma^{2})$ 与 $x$ 独立。给定一个候选符号模型库，限定为带有一个自由标量系数的单项式，$\\mathcal{F} = \\{ f_{p}(x) = \\beta x^{p} : \\beta \\in \\mathbb{R},\\ p \\in \\{1,2,3\\} \\}$。\n\n定义总体惩罚目标\n$$\nL_{p}(\\lambda) \\equiv \\inf_{\\beta \\in \\mathbb{R}} \\ \\mathbb{E}\\big[(y - \\beta x^{p})^{2}\\big] + \\lambda\\,K(p),\n$$\n其中 $K(p)$ 是一个模型复杂度泛函，$\\lambda \\ge 0$ 是一个在数据保真度和复杂度之间进行权衡的正则化强度。假设复杂度泛函等于多项式次数，即 $K(p) = p$。\n\n从第一性原理出发，即总体均方误差的定义以及标准正态分布的独立性和矩性质，对每个候选 $p \\in \\{1,2,3\\}$，推导出最小化无惩罚期望误差的最优系数 $\\beta_{p}^{\\star}$、相应的最小期望误差，并由此推导出作为 $\\sigma^{2}$ 和 $\\lambda$ 函数的显式权衡曲线 $L_{p}(\\lambda)$。利用这些表达式，确定每个候选模型在哪个 $\\lambda \\ge 0$ 的区间上是最优的，即在三个候选中达到最小惩罚目标。\n\n在你的最终答案中，报告最优模型从 $p=2$ 切换到另一个候选模型时的非负临界正则化强度。无需四舍五入。",
            "solution": "首先评估问题的有效性。\n\n**步骤 1：提取给定信息**\n- 输入分布：$x \\sim \\mathcal{N}(0,1)$。\n- 真实数据生成过程：$y = x^{2} + \\epsilon$。\n- 噪声分布：$\\epsilon \\sim \\mathcal{N}(0,\\sigma^{2})$，其中 $\\epsilon$ 与 $x$ 独立。\n- 候选模型库：$\\mathcal{F} = \\{ f_{p}(x) = \\beta x^{p} : \\beta \\in \\mathbb{R},\\ p \\in \\{1,2,3\\} \\}$。\n- 总体惩罚目标函数：$L_{p}(\\lambda) \\equiv \\inf_{\\beta \\in \\mathbb{R}} \\ \\mathbb{E}\\big[(y - \\beta x^{p})^{2}\\big] + \\lambda\\,K(p)$。\n- 正则化参数：$\\lambda \\ge 0$。\n- 复杂度泛函：$K(p) = p$。\n\n**步骤 2：使用提取的给定信息进行验证**\n- **科学上合理：** 该问题是统计学习理论中的一个标准练习，特别是使用惩罚损失函数的模型选择（在精神上与赤池信息准则或贝叶斯信息准则相关）。它使用了概率、期望和优化的基本原理。所有前提都是公认的。\n- **适定性：** 该问题是适定的。目标函数涉及最小化参数 $\\beta$ 的二次函数，该函数有唯一的最小值。任务是为每个模型找到这个最小值，然后比较得到的惩罚目标，这是一个定义清晰的过程，可以得到一组唯一的 $\\lambda$ 区间。\n- **客观性：** 该问题以精确的数学语言陈述，没有歧义或主观论断。\n\n**步骤 3：结论与行动**\n该问题在科学上是合理的、适定的，并且陈述是形式化的。它被判定为**有效**。将提供完整解答。\n\n问题的核心是为由 $p \\in \\{1, 2, 3\\}$ 索引的每个候选模型评估惩罚目标 $L_{p}(\\lambda)$。该目标由两部分组成：最小期望均方误差（MSE）和复杂度惩罚。\n$L_{p}(\\lambda) = \\min_{\\beta \\in \\mathbb{R}} \\mathbb{E}[(y - \\beta x^{p})^{2}] + \\lambda p$。\n\n令无惩罚期望误差为 $E_p(\\beta) = \\mathbb{E}[(y - \\beta x^{p})^{2}]$。为了找到最小化此项的最优系数 $\\beta_p^{\\star}$，我们对 $\\beta$ 求导并令其为零。\n$$\n\\frac{dE_p(\\beta)}{d\\beta} = \\frac{d}{d\\beta} \\mathbb{E}[y^2 - 2\\beta yx^p + \\beta^2 (x^p)^2] = \\mathbb{E}[-2yx^p + 2\\beta x^{2p}]\n$$\n将导数设为零，得到最优系数 $\\beta_p^{\\star}$：\n$$\n-2\\mathbb{E}[yx^p] + 2\\beta_p^{\\star}\\mathbb{E}[x^{2p}] = 0 \\implies \\beta_p^{\\star} = \\frac{\\mathbb{E}[yx^p]}{\\mathbb{E}[x^{2p}]}\n$$\n二阶导数为 $2\\mathbb{E}[x^{2p}] > 0$，因为 $x^{2p}$ 是一个非负随机变量且不几乎必然为零，这证实了该点为最小值点。\n\n为了计算 $\\beta_p^{\\star}$，我们需要计算期望值。变量 $x$ 服从标准正态分布，$x \\sim \\mathcal{N}(0,1)$。标准正态分布的矩由以下公式给出：当 $k$ 为奇数时，$\\mathbb{E}[x^k] = 0$；当 $k$ 为偶数时，$\\mathbb{E}[x^k] = (k-1)!!$。\n所需的矩为：\n- $\\mathbb{E}[x^1] = 0$\n- $\\mathbb{E}[x^2] = 1$\n- $\\mathbb{E}[x^3] = 0$\n- $\\mathbb{E}[x^4] = (3)!! = 3 \\cdot 1 = 3$\n- $\\mathbb{E}[x^5] = 0$\n- $\\mathbb{E}[x^6] = (5)!! = 5 \\cdot 3 \\cdot 1 = 15$\n\n接下来，我们计算 $\\mathbb{E}[yx^p]$。代入 $y = x^2 + \\epsilon$：\n$$\n\\mathbb{E}[yx^p] = \\mathbb{E}[(x^2 + \\epsilon)x^p] = \\mathbb{E}[x^{p+2} + \\epsilon x^p] = \\mathbb{E}[x^{p+2}] + \\mathbb{E}[\\epsilon x^p]\n$$\n由于 $x$ 和 $\\epsilon$ 是独立的，$\\mathbb{E}[\\epsilon x^p] = \\mathbb{E}[\\epsilon]\\mathbb{E}[x^p]$。给定 $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$，我们有 $\\mathbb{E}[\\epsilon]=0$。因此，$\\mathbb{E}[\\epsilon x^p] = 0$。\n所以，$\\mathbb{E}[yx^p] = \\mathbb{E}[x^{p+2}]$。\n\n现在我们可以为每个 $p \\in \\{1, 2, 3\\}$ 计算 $\\beta_p^{\\star}$：\n- 对于 $p=1$：$\\beta_1^{\\star} = \\frac{\\mathbb{E}[x^{1+2}]}{\\mathbb{E}[x^{2 \\cdot 1}]} = \\frac{\\mathbb{E}[x^3]}{\\mathbb{E}[x^2]} = \\frac{0}{1} = 0$。\n- 对于 $p=2$：$\\beta_2^{\\star} = \\frac{\\mathbb{E}[x^{2+2}]}{\\mathbb{E}[x^{2 \\cdot 2}]} = \\frac{\\mathbb{E}[x^4]}{\\mathbb{E}[x^4]} = \\frac{3}{3} = 1$。\n- 对于 $p=3$：$\\beta_3^{\\star} = \\frac{\\mathbb{E}[x^{3+2}]}{\\mathbb{E}[x^{2 \\cdot 3}]} = \\frac{\\mathbb{E}[x^5]}{\\mathbb{E}[x^6]} = \\frac{0}{15} = 0$。\n\n接下来，我们找到最小无惩罚误差，$\\min_{\\beta} E_p(\\beta) = E_p(\\beta_p^{\\star})$。\n$$\nE_p(\\beta_p^{\\star}) = \\mathbb{E}[(y - \\beta_p^{\\star} x^p)^2] = \\mathbb{E}[y^2] - 2\\beta_p^{\\star}\\mathbb{E}[yx^p] + (\\beta_p^{\\star})^2\\mathbb{E}[x^{2p}]\n$$\n代入 $\\beta_p^{\\star} = \\frac{\\mathbb{E}[yx^p]}{\\mathbb{E}[x^{2p}]}$，可简化为：\n$$\nE_p(\\beta_p^{\\star}) = \\mathbb{E}[y^2] - \\frac{(\\mathbb{E}[yx^p])^2}{\\mathbb{E}[x^{2p}]}\n$$\n我们计算 $\\mathbb{E}[y^2]$：\n$$\n\\mathbb{E}[y^2] = \\mathbb{E}[(x^2 + \\epsilon)^2] = \\mathbb{E}[x^4 + 2\\epsilon x^2 + \\epsilon^2] = \\mathbb{E}[x^4] + 2\\mathbb{E}[\\epsilon x^2] + \\mathbb{E}[\\epsilon^2]\n$$\n利用独立性，$\\mathbb{E}[\\epsilon x^2]=\\mathbb{E}[\\epsilon]\\mathbb{E}[x^2] = 0 \\cdot 1 = 0$。对于噪声，$\\mathbb{E}[\\epsilon^2] = \\text{Var}(\\epsilon) + (\\mathbb{E}[\\epsilon])^2 = \\sigma^2 + 0^2 = \\sigma^2$。\n因此，$\\mathbb{E}[y^2] = \\mathbb{E}[x^4] + \\sigma^2 = 3 + \\sigma^2$。\n\n现在我们计算每个模型的最小误差：\n- 对于 $p=1$：最小误差 $= (3 + \\sigma^2) - \\frac{(\\mathbb{E}[x^3])^2}{\\mathbb{E}[x^2]} = (3 + \\sigma^2) - \\frac{0^2}{1} = 3 + \\sigma^2$。\n- 对于 $p=2$：最小误差 $= (3 + \\sigma^2) - \\frac{(\\mathbb{E}[x^4])^2}{\\mathbb{E}[x^4]} = (3 + \\sigma^2) - \\frac{3^2}{3} = 3 + \\sigma^2 - 3 = \\sigma^2$。\n- 对于 $p=3$：最小误差 $= (3 + \\sigma^2) - \\frac{(\\mathbb{E}[x^5])^2}{\\mathbb{E}[x^6]} = (3 + \\sigma^2) - \\frac{0^2}{15} = 3 + \\sigma^2$。\n\n惩罚目标函数 $L_p(\\lambda)$ 分别是：\n- $L_1(\\lambda) = (3 + \\sigma^2) + \\lambda \\cdot 1 = 3 + \\sigma^2 + \\lambda$。\n- $L_2(\\lambda) = \\sigma^2 + \\lambda \\cdot 2 = \\sigma^2 + 2\\lambda$。\n- $L_3(\\lambda) = (3 + \\sigma^2) + \\lambda \\cdot 3 = 3 + \\sigma^2 + 3\\lambda$。\n\n为了找到每个模型最优的 $\\lambda$ 区间，我们比较这三个关于 $\\lambda$ 的线性函数（对于 $\\lambda \\ge 0$）。\n\n首先，比较 $L_1(\\lambda)$ 和 $L_3(\\lambda)$：\n$L_3(\\lambda) - L_1(\\lambda) = (3 + \\sigma^2 + 3\\lambda) - (3 + \\sigma^2 + \\lambda) = 2\\lambda$。\n因为 $\\lambda \\ge 0$，所以 $2\\lambda \\ge 0$。因此，对于所有 $\\lambda \\ge 0$，$L_3(\\lambda) \\ge L_1(\\lambda)$。模型 $p=3$ 永远不会严格优于 $p=1$，因此永远不会是唯一的最优模型。\n\n现在，比较剩下的候选模型，$p=1$ 和 $p=2$。最优模型将是 $L_p(\\lambda)$ 值较小的那个。我们通过令 $L_1(\\lambda) = L_2(\\lambda)$ 来找到交叉点：\n$$\n3 + \\sigma^2 + \\lambda = \\sigma^2 + 2\\lambda\n$$\n$$\n3 = \\lambda\n$$\n这是临界正则化强度，$\\lambda_c = 3$。\n\n为了确定在此临界值的两侧哪个模型是最优的：\n- 对于 $\\lambda  3$：\n  $L_2(\\lambda)  L_1(\\lambda) \\iff \\sigma^2 + 2\\lambda  3 + \\sigma^2 + \\lambda \\iff \\lambda  3$。\n  因此，对于 $0 \\le \\lambda  3$，模型 $p=2$ 是最优的。\n- 对于 $\\lambda > 3$：\n  $L_1(\\lambda)  L_2(\\lambda) \\iff 3 + \\sigma^2 + \\lambda  \\sigma^2 + 2\\lambda \\iff 3  \\lambda$。\n  因此，对于 $\\lambda > 3$，模型 $p=1$ 是最优的。\n- 对于 $\\lambda = 3$：\n  $L_1(3) = L_2(3)$，所以模型 $p=1$ 和 $p=2$ 同等最优。\n\n模型 $p=2$ 是正确的底层模型结构，对数据的拟合最好，因此当复杂度惩罚（由 $\\lambda$ 控制）较小时，它被优先选择。随着 $\\lambda$ 的增加，对复杂度的惩罚（对于 $p=2$ 是 $2\\lambda$）比更简单模型（对于 $p=1$ 是 $1\\lambda$）增长得更快。最终，在 $\\lambda=3$ 时，惩罚超过了更好的拟合度，更简单（但有偏）的模型 $p=1$ 变得更优。\n\n问题要求的是最优模型从 $p=2$ 切换出去时的临界正则化强度。这发生在 $L_2(\\lambda)$ 不再是唯一最小值的那一点，即 $\\lambda=3$。\n\n最优性区间是：\n- $p=2$ 在 $\\lambda \\in [0, 3]$ 上是最优的。\n- $p=1$ 在 $\\lambda \\in [3, \\infty)$ 上是最优的。\n- 对于 $\\lambda \\ge 0$，$p=3$ 永远不是唯一的最优模型。\n\n从 $p=2$ 作为唯一最优模型的切换发生在边界处，即临界值 $\\lambda_c = 3$。",
            "answer": "$$\\boxed{3}$$"
        },
        {
            "introduction": "从理想的理论走向真实的挑战，现实世界的数据总是充满噪声，这对于依赖数值导数的方程发现方法是一个主要障碍。本练习提供了一个动手的编码实践，让你构建一个完整的方程发现流程。你将看到像全变分（Total Variation, TV）这样强大的去噪技术，如何成为一个关键的预处理步骤，帮助我们从受污染的观测数据中恢复出潜在的动力学行为。",
            "id": "3812583",
            "problem": "给定一个在一维空间域和时间区间上的合成偏微分方程数据集，该数据集由一个经典的扩散定律构建。目标是实现一个可复现的流程，展示在导数估计之前应用全变分 (TV) 去噪，如何将发现的符号模型从 $u_t$ 改变为 $u_t + \\beta u_{xx}$，其中 $u$ 是一个标量场，$t$ 是时间，$x$ 是空间，$\\beta$ 是一个待估计的标量系数。该流程必须生成场，添加受控噪声，可选地使用 TV 正则化对场进行去噪，估计导数，并执行稀疏回归以发现控制方程。\n\n使用的基本原理：\n- 质量守恒和 Fick 定律意味着对于恒定扩散系数 $\\nu$ 的扩散（热）方程为 $u_t = \\nu u_{xx}$，其中 $u_t$ 表示 $\\partial u/\\partial t$，$u_{xx}$ 表示 $\\partial^2 u/\\partial x^2$。\n- 对于周期性边界条件的域 $x \\in [0,2\\pi]$ 上的傅里叶模态初始条件 $u(x,0)=\\sin(kx)$，扩散方程的解为 $u(x,t) = \\sin(kx)\\exp(-\\nu k^2 t)$。\n- 离散中心差分提供了导数的一致性近似：对于内部点，$u_t(x_i,t_j) \\approx \\left(u(x_i,t_{j+1}) - u(x_i,t_{j-1})\\right)/(2\\Delta t)$ 且 $u_{xx}(x_i,t_j) \\approx \\left(u(x_{i+1},t_j) - 2u(x_i,t_j) + u(x_{i-1},t_j)\\right)/\\Delta x^2$。\n\n全变分 (TV) 去噪模型：\n- Rudin-Osher-Fatemi (ROF) 模型求解 $\\min_{v} \\tfrac{1}{2}\\lVert v - f\\rVert_2^2 + \\alpha \\operatorname{TV}(v)$，其中 $f$ 是给定的带噪信号，$\\alpha > 0$ 是正则化权重，$\\operatorname{TV}(v)$ 表示全变分半范数。实现一维分裂 Bregman 迭代，以沿空间维度 $x$ 对每个固定的时间切片 $t_j$ 求解 ROF 模型。\n\n符号回归与稀疏性：\n- 在内部网格点构建一个线性库 $\\Theta = [u, u_{xx}]$，并通过最小二乘法拟合 $u_t \\approx c_0 u + c_1 u_{xx}$。\n- 应用序贯阈值法：将任何满足 $|c_\\ell|  \\tau$ 的系数 $c_\\ell$ 置零，然后仅使用活动列重新拟合以获得稀疏系数。将发现的模型解释为 $u_t + \\beta u_{xx} = 0$，其中 $\\beta = -c_1$。\n- 如果活动集为空（即 $c_0$ 和 $c_1$ 都被阈值化为零），则将发现的模型定义为 $u_t$；如果 $c_1$ 在阈值化后保留下来，则定义为 $u_t + \\beta u_{xx}$。\n\n数据生成与多尺度内容：\n- 在 $x \\in [0,2\\pi]$，$t \\in [0,T]$ 上使用多尺度初始条件：$u(x,t) = \\sin(3x)\\exp(-\\nu \\cdot 9 t) + 0.5\\,\\sin(x)\\exp(-\\nu \\cdot 1 t)$，以组合尺度 $k=1$ 和 $k=3$，并测试跨尺度的鲁棒性。\n- 向场 $u(x,t)$ 添加标准差为 $\\sigma$ 的独立同分布零均值高斯噪声。\n\n数值设置与单位：\n- 使用弧度表示 $x$，使用一致的无量纲单位表示时间 $t$。不需要进行物理单位转换；输出为无量纲的浮点数和整数。\n\n实现以下程序行为：\n1. 为由 $N_x$ 个空间点和 $N_t$ 个时间点组成的固定网格，生成精确场 $u(x,t)$ 和带噪观测值 $u^{\\text{noisy}}(x,t)$。\n2. 对每个测试用例，运行两个流程：\n   - 流程 A（无去噪）：直接从 $u^{\\text{noisy}}$ 估计 $u_t$ 和 $u_{xx}$，并执行稀疏回归以发现 $c_1$。\n   - 流程 B（使用 TV 去噪）：沿 $x$ 轴对每个时间切片 $t_j$ 独立应用一维 TV 去噪以获得 $u^{\\text{tv}}(x,t)$，然后估计导数并执行稀疏回归以发现 $c_1$。\n3. 以浮点数形式报告 $\\beta_{\\text{noTV}} = -c_1^{\\text{noTV}}$ 和 $\\beta_{\\text{TV}} = -c_1^{\\text{TV}}$。同时报告发现的模型是否因去噪而从 $u_t$ 变为 $u_t + \\beta u_{xx}$，如果流程 A 发现 $u_t$ 而流程 B 发现 $u_t + \\beta u_{xx}$，则编码为整数 $1$，否则为 $0$。\n\n稀疏回归细节：\n- 使用普通最小二乘法进行系数估计。\n- 使用阈值参数 $\\tau$ 进行序贯阈值法和重新拟合。\n\n测试套件：\n- 固定扩散系数 $\\nu = 0.1$，$N_x = 128$，$N_t = 80$，$T = 2.0$。\n- 使用以下四个测试用例，每个由 $(\\sigma, \\alpha, \\tau, \\text{seed})$ 定义：\n  1. $(0.25, 0.35, 0.09, 1)$：一个旨在展示应用去噪后模型从 $u_t$ 变为 $u_t + \\beta u_{xx}$ 的情况。\n  2. $(0.08, 0.15, 0.05, 2)$：一个由于噪声较低，预计两个流程都能发现 $u_t + \\beta u_{xx}$ 的情况。\n  3. $(0.35, 0.30, 0.12, 3)$：一个高噪声情况，即使去噪也可能无法产生稳定的扩散项。\n  4. $(0.25, 0.00, 0.09, 4)$：一个无去噪（$\\alpha=0$）的边界情况，用于与情况 1 进行直接比较。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个由四个条目组成的逗号分隔列表，每个条目的形式为 $[\\beta_{\\text{noTV}}, \\beta_{\\text{TV}}, \\text{changed}]$，其中 $\\beta_{\\text{noTV}}$ 和 $\\beta_{\\text{TV}}$ 是浮点数，$\\text{changed}$ 是整数 $0$ 或 $1$。例如：\"[[0.0,0.095,1],[...],[...],[...]]\"。",
            "solution": "我们从一维空间中的扩散（热）方程 $u_t = \\nu u_{xx}$ 开始，该方程源于质量守恒和 Fick 定律（对于恒定扩散系数 $\\nu$）。在周期域 $x \\in [0,2\\pi]$ 上，傅里叶模态的基本解是 $u(x,t) = \\sin(kx)\\exp(-\\nu k^2 t)$，它随时间以 $\\nu k^2$ 的速率指数衰减。为了引入多尺度内容，我们叠加了两种模态：$k=1$ 和 $k=3$。精确解构造为 $u(x,t) = \\sin(3x)\\exp(-\\nu \\cdot 9 t) + 0.5\\,\\sin(x)\\exp(-\\nu \\cdot 1 t)$。\n\n我们用 $N_x$ 个空间点和 $N_t$ 个时间点对域进行离散化。设 $\\Delta x = 2\\pi/(N_x-1)$ 和 $\\Delta t = T/(N_t-1)$。我们添加标准差为 $\\sigma$ 的独立高斯噪声，以生成 $u^{\\text{noisy}}(x_i,t_j) = u(x_i,t_j) + \\eta_{ij}$，其中 $\\eta_{ij} \\sim \\mathcal{N}(0,\\sigma^2)$。然后，我们使用中心有限差分法估计内部网格点的导数：\n- $u_t(x_i,t_j) \\approx \\left(u(x_i,t_{j+1}) - u(x_i,t_{j-1})\\right)/(2\\Delta t)$，对于 $j=1,\\dots,N_t-2$，\n- $u_{xx}(x_i,t_j) \\approx \\left(u(x_{i+1},t_j) - 2u(x_i,t_j) + u(x_{i-1},t_j)\\right)/\\Delta x^2$，对于 $i=1,\\dots,N_x-2$。\n\n带噪有限差分通常会夸大导数估计中的方差和偏差，特别是对于像 $u_{xx}$ 这样的高阶空间导数。这种偏差和方差会阻碍旨在识别简约模型的符号回归。\n\n为了缓解这个问题，我们在导数估计之前应用全变分 (TV) 去噪。Rudin-Osher-Fatemi (ROF) 模型求解\n$$\n\\min_{v} \\frac{1}{2}\\lVert v - f \\rVert_2^2 + \\alpha\\,\\operatorname{TV}(v),\n$$\n其中 $\\operatorname{TV}(v)$ 表示全变分半范数，$\\alpha0$ 是正则化权重。与二次平滑相比，TV 去噪在减少高频噪声的同时能更好地保留尖锐特征。我们实现了一维分裂 Bregman 方法来求解每个时间切片 $f(x) = u^{\\text{noisy}}(x,t_j)$ 上的 ROF 模型，并迭代更新 $(u,d,b)$：\n- 分裂 Bregman 框架引入了一个辅助变量 $d \\approx Du$（其中 $D$ 是离散一阶差分算子）和一个 Bregman 变量 $b$。\n- 在每次迭代中，我们求解线性系统 $(I + \\mu D^\\top D) u = f + \\mu D^\\top (d - b)$ 以获得 $u$，应用收缩算子 $d = \\operatorname{shrink}(Du + b, \\alpha/\\mu)$，并更新 $b \\leftarrow b + Du - d$。这里 $\\mu0$ 是一个惩罚参数，$\\operatorname{shrink}(z,\\lambda) = \\operatorname{sign}(z)\\cdot\\max\\{|z|-\\lambda,0\\}$ 按分量应用。\n\n算子 $D^\\top D$ 产生一个类似于带有类 Neumann 边界处理的离散拉普拉斯算子的三对角矩阵，因此 $u$ 的更新简化为求解一个三对角线性系统，我们通过 Thomas 算法来实现它，以保证一维情况下的效率和精确性。\n\n在对每个时间切片进行去噪后，我们通过相同的中心差分公式来估计 $u_t$ 和 $u_{xx}$，但这次作用于去噪后的场 $u^{\\text{tv}}$。这提高了导数近似的稳定性。\n\n对于符号回归，我们构建一个线性候选库 $\\Theta = [u, u_{xx}]$ 并通过普通最小二乘法拟合\n$$\nu_t \\approx c_0\\, u + c_1\\, u_{xx}\n$$\n为促进稀疏性，我们应用序贯阈值法：当 $|c_\\ell|  \\tau$ 时，设置 $c_\\ell=0$（其中 $\\ell \\in \\{0,1\\}$），并仅使用剩余的活动列进行重新拟合，以减少多重共线性或噪声引起的偏差。我们将发现的控制方程解释为\n$$\nu_t + \\beta u_{xx} = 0,\n$$\n的形式，其中 $\\beta = -c_1$。如果 $c_0$ 和 $c_1$ 都被阈值化为零，则模型被解释为 $u_t$（即 $u_t \\approx 0$）。该框架是稀疏非线性动力学辨识 (SINDy) 的一个最小实例，应用于偏微分方程层面。\n\n该测试套件通过改变噪声水平 $\\sigma$、TV 正则化权重 $\\alpha$、阈值 $\\tau$ 和随机种子来探索不同的机制：\n- 一个“理想路径”，其中去噪揭示了扩散项，使发现的模型从 $u_t$（没有稳健的空间项）变为 $u_t + \\beta u_{xx}$。\n- 一个较低噪声的机制，其中两个流程都恢复了扩散项。\n- 一个高噪声的机制，其中在给定阈值下，两个流程都无法稳健地恢复扩散项。\n- 一个无去噪的边界情况，用作对照。\n\n对于每种情况，我们报告三个量：\n- $\\beta_{\\text{noTV}}$ (浮点数),\n- $\\beta_{\\text{TV}}$ (浮点数),\n- 一个整数标志，指示发现的模型是否因去噪而从 $u_t$ 变为 $u_t + \\beta u_{xx}$。\n\n最后，程序打印一行包含所有测试用例结果的逗号分隔列表。这提供了一个定量示例，说明在导数估计前进行 TV 去噪，如何通过在多尺度设置下稳定空间导数估计，将发现的模型从 $u_t$ 变为 $u_t + \\beta u_{xx}$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# -----------------------------\n# Utility: Tridiagonal solver (Thomas algorithm)\n# Solve A u = rhs, where A has subdiag 'a', diag 'b', superdiag 'c'\ndef solve_tridiagonal(a, b, c, rhs):\n    n = len(b)\n    cp = np.zeros(n-1)\n    rp = np.zeros(n)\n    bp = b.copy()\n    cp[0] = c[0] / bp[0]\n    rp[0] = rhs[0] / bp[0]\n    for i in range(1, n-1):\n        denom = bp[i] - a[i-1] * cp[i-1]\n        cp[i] = c[i] / denom\n        rp[i] = (rhs[i] - a[i-1] * rp[i-1]) / denom\n    denom = bp[n-1] - a[n-2] * cp[n-2]\n    rp[n-1] = (rhs[n-1] - a[n-2] * rp[n-2]) / denom\n    # Back substitution\n    u = np.zeros(n)\n    u[n-1] = rp[n-1]\n    for i in range(n-2, -1, -1):\n        u[i] = rp[i] - cp[i] * u[i+1]\n    return u\n\n# -----------------------------\n# 1D TV denoising via split Bregman (ROF model)\ndef tv_denoise_1d(f, alpha, mu=2.0, iters=50):\n    # If alpha == 0, return f\n    if alpha == 0.0:\n        return f.copy()\n    n = f.size\n    u = f.copy()\n    d = np.zeros(n-1)\n    b = np.zeros(n-1)\n    # Precompute tridiagonal matrix A = I + mu * D^T D\n    # D^T D has diagonal [1, 2, 2, ..., 2, 1] and off-diagonals -1\n    diag = np.ones(n)\n    diag[0] += mu * 1.0\n    diag[-1] += mu * 1.0\n    diag[1:-1] += mu * 2.0\n    off = -mu * np.ones(n-1)\n    # Iterations\n    for _ in range(iters):\n        # u-update: solve (I + mu D^T D) u = f + mu D^T (d - b)\n        v = d - b  # length n-1\n        Dt_v = np.zeros(n)\n        Dt_v[0] = -v[0]\n        Dt_v[1:-1] = v[:-1] - v[1:]\n        Dt_v[-1] = v[-1]\n        rhs = f + mu * Dt_v\n        u = solve_tridiagonal(off.copy(), diag.copy(), off.copy(), rhs)\n        # d-update: shrink(Du + b, alpha/mu)\n        Du = u[1:] - u[:-1]\n        w = Du + b\n        # Shrinkage\n        thresh = alpha / mu\n        d = np.sign(w) * np.maximum(np.abs(w) - thresh, 0.0)\n        # Bregman update\n        b = b + Du - d\n    return u\n\n# -----------------------------\n# Exact multiscale heat solution\ndef heat_solution(x, t, nu):\n    return np.sin(3.0 * x) * np.exp(-nu * 9.0 * t) + 0.5 * np.sin(x) * np.exp(-nu * 1.0 * t)\n\n# -----------------------------\n# Derivative estimation (central differences, interior points)\ndef compute_derivatives(U, dx, dt):\n    Nx, Nt = U.shape\n    # Interior indices\n    i_slice = slice(1, Nx-1)\n    j_slice = slice(1, Nt-1)\n    # u_t at interior\n    ut = (U[:, 2:] - U[:, :-2]) / (2.0 * dt)\n    ut = ut[1:-1, :]  # restrict to interior in space\n    # u_xx at interior\n    u_xx = (U[2:, :] - 2.0 * U[1:-1, :] + U[:-2, :]) / (dx * dx)\n    u_xx = u_xx[:, 1:-1]  # restrict to interior in time\n    # u values at interior\n    u_mid = U[1:-1, 1:-1]\n    # Flatten consistently\n    ut_flat = ut.flatten()\n    u_xx_flat = u_xx.flatten()\n    u_flat = u_mid.flatten()\n    # Library Theta: columns [u, u_xx]\n    Theta = np.column_stack([u_flat, u_xx_flat])\n    return ut_flat, Theta\n\n# -----------------------------\n# Sparse regression with sequential thresholding\ndef sparse_regression(ut, Theta, tau):\n    # Initial least squares\n    # Add small Tikhonov regularization to handle ill-conditioning if needed\n    c_ls, _, _, _ = np.linalg.lstsq(Theta, ut, rcond=None)\n    c = c_ls.copy()\n    mask = np.abs(c) >= tau\n    if not np.any(mask):\n        # No active terms\n        return np.zeros_like(c)\n    # Refit with active columns\n    Theta_active = Theta[:, mask]\n    c_active, _, _, _ = np.linalg.lstsq(Theta_active, ut, rcond=None)\n    c_final = np.zeros_like(c)\n    c_final[mask] = c_active\n    return c_final\n\n# -----------------------------\n# Pipeline for one test case\ndef run_case(sigma, alpha, tau, seed):\n    # Grid setup\n    Nx = 128\n    Nt = 80\n    L = 2.0 * np.pi\n    T = 2.0\n    x = np.linspace(0.0, L, Nx)\n    t = np.linspace(0.0, T, Nt)\n    dx = x[1] - x[0]\n    dt = t[1] - t[0]\n    nu = 0.1\n    # Exact field\n    X, TT = np.meshgrid(x, t, indexing='ij')\n    U_exact = heat_solution(X, TT, nu)\n    # Noisy field\n    rng = np.random.default_rng(seed)\n    noise = rng.normal(0.0, sigma, size=U_exact.shape)\n    U_noisy = U_exact + noise\n    # Pipeline A: no denoising\n    ut0, Theta0 = compute_derivatives(U_noisy, dx, dt)\n    c0 = sparse_regression(ut0, Theta0, tau)\n    # Beta is -c1 where c1 corresponds to u_xx (second column)\n    beta_noTV = float(-c0[1])\n    # Determine discovered model code for pipeline A\n    has_u_xx_0 = np.abs(c0[1]) > 1e-12\n    model_code_0 = 1 if has_u_xx_0 else 0\n    # Pipeline B: TV denoising along x for each time slice\n    U_tv = np.zeros_like(U_noisy)\n    for j in range(Nt):\n        U_tv[:, j] = tv_denoise_1d(U_noisy[:, j], alpha=alpha, mu=2.0, iters=50)\n    ut1, Theta1 = compute_derivatives(U_tv, dx, dt)\n    c1 = sparse_regression(ut1, Theta1, tau)\n    beta_TV = float(-c1[1])\n    has_u_xx_1 = np.abs(c1[1]) > 1e-12\n    model_code_1 = 1 if has_u_xx_1 else 0\n    # Change flag: from u_t (0) to u_t + beta u_xx (1) upon denoising\n    changed = 1 if (model_code_0 == 0 and model_code_1 == 1) else 0\n    return [beta_noTV, beta_TV, changed]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (0.25, 0.35, 0.09, 1),  # Case 1: expected change due to denoising\n        (0.08, 0.15, 0.05, 2),  # Case 2: both recover diffusion term\n        (0.35, 0.30, 0.12, 3),  # Case 3: high noise, both may fail\n        (0.25, 0.00, 0.09, 4),  # Case 4: no denoising boundary case\n    ]\n\n    results = []\n    for sigma, alpha, tau, seed in test_cases:\n        result = run_case(sigma, alpha, tau, seed)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # Ensure numbers are printed without extra spaces\n    def fmt_entry(entry):\n        return \"[\" + \",\".join(f\"{x:.6f}\" if isinstance(x, float) else str(x) for x in entry) + \"]\"\n    print(\"[\" + \",\".join(fmt_entry(r) for r in results) + \"]\")\n\nsolve()\n```"
        },
        {
            "introduction": "除了简单地对数据进行去噪，我们还可以利用已知的物理原理（如守恒律）来主动引导发现过程，这是一种更为精妙的方法。本练习将向你展示如何通过等式约束回归来强制施加物理定律。通过实践，你将理解这种方法为何能显著提升识别模型中微弱但物理上至关重要的项的能力，而这些项在无约束的方法中可能被忽略。",
            "id": "3812529",
            "problem": "考虑一个在一维周期性域上使用符号回归进行方程发现的任务。目标是确定添加守恒约束在何时以及为何会将发现的偏微分方程（PDE）模型从纯扩散模型变为包含三次非线性的反应扩散模型。你将实现一个程序，该程序从一个已知的 PDE 构建合成数据，建立一个特征库，执行稀疏回归以识别控制方程，然后使用一个编码了守恒律的等式约束重复进行识别。对于多个测试案例，该程序将报告添加约束是否通过选择一个先前被忽略的非线性项来改变发现的 PDE。\n\n基本原理：\n- 假设控制 PDE 的形式为\n$$\nu_t = \\nu\\, u_{xx} - \\epsilon\\, u^3,\n$$\n在长度为 $L = 2\\pi$ 的域上具有周期性边界条件，其中 $u(x,t)$ 是状态，$\\nu  0$ 是扩散系数，$\\epsilon \\ge 0$ 控制三次反应项的强度。\n- 对于周期性边界条件，拉普拉斯项的积分消失：\n$$\n\\int_0^{L} u_{xx}\\, dx = 0.\n$$\n- 因此，空间平均值（也称为质量）$M(t) = \\int_0^{L} u(x,t)\\, dx$ 遵循以下平衡关系\n$$\n\\frac{dM}{dt} = \\int_0^{L} u_t\\, dx = -\\epsilon \\int_0^{L} u^3\\, dx.\n$$\n\n问题设置：\n1. 数据生成。在 $x \\in [0,2\\pi]$ 上模拟 PDE\n$$\nu_t = \\nu\\, u_{xx} - \\epsilon\\, u^3\n$$\n，采用周期性边界条件和显式时间步进格式。使用一个包含 $N$ 个点的均匀空间网格和一个均匀时间步长 $\\Delta t$，共进行 $T$ 个时间步。初始化条件为\n$$\nu(x,0) = a_0 + a_1 \\sin(x) + a_2 \\cos(2x),\n$$\n其中 $a_0$、$a_1$、$a_2$ 是固定的实常数。使用离散傅里叶变换通过以下方式在谱空间计算 $u_{xx}$：\n$$\n\\widehat{u_{xx}}(k) = -k^2 \\hat{u}(k),\n$$\n其中 $k$ 是角波数，$\\hat{u}(k)$ 是 $u$ 的离散傅里叶变换。\n\n2. 库构建和回归。在每个时间步 $n \\in \\{0,\\dots,T-1\\}$ 和空间网格点 $j \\in \\{0,\\dots,N-1\\}$，构建目标和特征\n$$\ny_{n,j} = \\frac{u_{n+1,j} - u_{n,j}}{\\Delta t}, \\quad \\phi^{(1)}_{n,j} = (u_{xx})_{n,j}, \\quad \\phi^{(2)}_{n,j} = \\left(u_{n,j}\\right)^3,\n$$\n并将它们堆叠成一个回归系统\n$$\n\\Theta\\, c \\approx y,\n$$\n其中 $\\Theta \\in \\mathbb{R}^{(NT) \\times 2}$ 的列是 $\\phi^{(1)}$ 和 $\\phi^{(2)}$，$c \\in \\mathbb{R}^2$ 是对应于 $u_{xx}$ 和 $u^3$ 的未知系数，$y \\in \\mathbb{R}^{NT}$ 堆叠了各项 $y_{n,j}$。\n\n3. 无约束稀疏回归。求解一个岭正则化最小二乘问题以获得初始估计 $\\hat{c}$，然后应用序列阈值最小二乘法 (STLSQ)：将 $\\hat{c}$ 中绝对值低于阈值 $\\tau$ 的任何分量置零，并在保留的列上使用岭正则化最小二乘法重新拟合。将得到的系数表示为 $c^{\\mathrm{un}}$。定义一个检测阈值 $\\delta  0$，如果一个项的系数绝对值至少为 $\\delta$，则称该项“存在”。\n\n4. 守恒约束回归。施加由周期性导出的时间平均质量平衡等式约束。在离散形式下，设均匀网格间距为 $\\Delta x = L/N$，令\n$$\ng = \\Delta x \\begin{bmatrix} \\sum_{n,j} \\phi^{(1)}_{n,j} \\\\ \\sum_{n,j} \\phi^{(2)}_{n,j} \\end{bmatrix}, \\quad h = \\Delta x \\sum_{n,j} y_{n,j}.\n$$\n求解岭正则化等式约束最小二乘问题\n$$\n\\min_{c \\in \\mathbb{R}^2} \\left\\| \\Theta c - y \\right\\|_2^2 + \\alpha \\left\\| c \\right\\|_2^2 \\quad \\text{subject to} \\quad g^\\top c = h,\n$$\n其中 $\\alpha  0$ 是一个小的数值以保证稳定性。将得到的系数表示为 $c^{\\mathrm{con}}$。根据相同的检测阈值 $\\delta$ 报告三次项是否“存在”。\n\n5. 决策规则。对于每个测试案例，输出一个布尔值，指示添加守恒约束是否将发现的 PDE 从仅扩散模型更改为带有三次项的反应扩散模型。形式上，输出\n$$\n\\text{changed} = \\left(\\left|c^{\\mathrm{un}}_2\\right|  \\delta\\right) \\wedge \\left(\\left|c^{\\mathrm{con}}_2\\right| \\ge \\delta\\right),\n$$\n其中 $c^{\\mathrm{un}}_2$ 和 $c^{\\mathrm{con}}_2$ 分别是在无约束和有约束拟合中乘以 $u^3$ 的系数。\n\n测试套件：\n使用以下三组参数来测试解决方案的不同方面。在所有情况下，取 $L = 2\\pi$，$a_0 = 0.5$，$a_1 = 0.4$，$a_2 = -0.2$，岭参数 $\\alpha = 10^{-8}$，检测阈值 $\\delta = 0.01$，以及 STLSQ 阈值 $\\tau = 0.03$。\n\n- 测试 1（约束起作用的理想情况）：$N = 64$，$T = 200$，$\\Delta t = 0.001$，$\\nu = 0.1$，$\\epsilon = 0.02$。\n- 测试 2（无反应的边界情况）：$N = 64$，$T = 200$，$\\Delta t = 0.001$，$\\nu = 0.1$，$\\epsilon = 0.0$。\n- 测试 3（强反应，两种方法都恢复三次项）：$N = 64$，$T = 200$，$\\Delta t = 0.001$，$\\nu = 0.1$，$\\epsilon = 0.15$。\n\n要求的最终输出：\n你的程序应生成一行输出，其中包含按上述测试顺序排列的结果，形式为方括号内以逗号分隔的列表，例如\n$$\n[\\text{result}_1,\\text{result}_2,\\text{result}_3],\n$$\n其中每个 $\\text{result}_i$ 是按照决策规则中指定的方式计算的布尔值。不应产生任何其他输出。\n\n角度单位：如果使用任何三角函数，角度变量必须以弧度为单位。最终答案中无需报告任何物理单位。",
            "solution": "我们从两个基本事实出发。首先，周期性边界条件意味着 $\\int_0^{2\\pi} u_{xx}\\, dx = 0$，因为二阶导数积分后得到一个边界项，在周期性区间上该项会相互抵消。其次，假设控制模型具有通用形式 $u_t = \\nu u_{xx} - \\epsilon u^3$，这是一个带有三次汇项的反应扩散方程。对其进行空间积分可得到质量平衡：\n$$\n\\frac{d}{dt}\\int_0^{2\\pi} u(x,t)\\, dx \\,=\\, \\nu \\int_0^{2\\pi} u_{xx}\\, dx \\,-\\, \\epsilon \\int_0^{2\\pi} u^3\\, dx \\,=\\, -\\epsilon \\int_0^{2\\pi} u^3\\, dx.\n$$\n因此，扩散项不能改变空间平均值，而只要 $u^3$ 的空间平均值非零，三次项通常会改变它。这是我们将编码到等式约束回归中的关键物理和数学约束。\n\n算法设计分四步进行。\n\n步骤 1：数据模拟。我们将 $x \\in [0,2\\pi]$ 离散化为一个包含 $N$ 个点的均匀网格，间距为 $\\Delta x = 2\\pi/N$，时间 $t \\in \\{0,\\Delta t,2\\Delta t,\\dots,T\\Delta t\\}$。我们初始化 $u(x,0) = a_0 + a_1 \\sin(x) + a_2 \\cos(2x)$ 以确保存在一个正偏置，从而使 $\\int u^3 dx$ 通常不为零。我们使用前向欧拉法进行时间推进，\n$$\nu^{n+1} = u^n + \\Delta t\\left(\\nu\\, u_{xx}^n - \\epsilon\\, (u^n)^3\\right),\n$$\n并为了精度和周期性，在谱空间计算拉普拉斯算子：\n$$\n\\widehat{u_{xx}}(k) = -k^2 \\widehat{u}(k),\n$$\n其中 $k$ 是角波数，由 $k = 2\\pi\\, \\mathrm{fftfreq}(N, \\Delta x)$ 给出，而 $\\widehat{u}$ 表示 $u$ 的离散傅里叶变换。\n\n步骤 2：库构建。对于每个时间步 $n \\in \\{0,\\dots,T-1\\}$，我们通过有限差分计算瞬时时间导数\n$$\ny^n = \\frac{u^{n+1} - u^n}{\\Delta t},\n$$\n和特征\n$$\n\\phi^{(1),n} = u_{xx}^n,\\quad \\phi^{(2),n} = \\left(u^n\\right)^3.\n$$\n然后我们将所有时空样本堆叠起来，形成回归系统 $\\Theta c \\approx y$，其中 $\\Theta \\in \\mathbb{R}^{(NT)\\times 2}$。\n\n步骤 3：无约束稀疏回归。我们首先求解一个岭正则化最小二乘问题\n$$\n\\hat{c} = \\arg\\min_c \\left\\|\\Theta c - y\\right\\|_2^2 + \\alpha \\left\\|c\\right\\|_2^2,\n$$\n其中 $\\alpha  0$ 是一个小的数值以保证稳定性。接下来，我们应用序列阈值最小二乘法 (STLSQ)：使用用户指定的阈值 $\\tau  0$，我们将 $\\hat{c}$ 中绝对值小于 $\\tau$ 的任何分量设置为零，并对保留的列重新进行岭正则化最小二乘拟合。这会得到 $c^{\\mathrm{un}}$。最后，我们声明，当且仅当 $\\left|c^{\\mathrm{un}}_2\\right| \\ge \\delta$ 时，三次项被检测到，其中 $\\delta  0$ 是一个与 $\\tau$ 不同的检测阈值。\n\n在实践中，如果 $\\epsilon$ 很小，那么 $\\left|c^{\\mathrm{un}}_2\\right|$ 可能会低于 $\\tau$，三次项会被剪除，从而得到一个仅有扩散的模型 $u_t \\approx \\nu u_{xx}$。\n\n步骤 4：守恒约束回归。我们现在对质量平衡约束进行编码。在离散形式下，设 $\\Delta x = 2\\pi/N$，定义\n$$\ng = \\Delta x \\begin{bmatrix} \\sum_{n=0}^{T-1}\\sum_{j=0}^{N-1} \\phi^{(1)}_{n,j} \\\\ \\sum_{n=0}^{T-1}\\sum_{j=0}^{N-1} \\phi^{(2)}_{n,j} \\end{bmatrix}, \\quad\nh = \\Delta x \\sum_{n=0}^{T-1}\\sum_{j=0}^{N-1} y_{n,j}.\n$$\n对于周期性网格，在每个 $n$ 处 $\\sum_j \\phi^{(1)}_{n,j} \\approx 0$，因此 $g$ 的第一个分量近似为零，而第二个分量捕捉了 $\\int u^3 dx$ 的累积贡献。我们求解等式约束岭回归问题\n$$\n\\min_c \\left\\|\\Theta c - y\\right\\|_2^2 + \\alpha \\left\\|c\\right\\|_2^2 \\quad \\text{subject to} \\quad g^\\top c = h.\n$$\n引入一个拉格朗日乘子 $\\lambda$，卡罗需-库恩-塔克 (KKT) 系统为\n$$\n\\begin{bmatrix}\n2(\\Theta^\\top \\Theta + \\alpha I)  g \\\\\ng^\\top  0\n\\end{bmatrix}\n\\begin{bmatrix}\nc \\\\ \\lambda\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n2 \\Theta^\\top y \\\\ h\n\\end{bmatrix}.\n$$\n求解这个线性系统得到 $c^{\\mathrm{con}}$。因为扩散项积分为零，数据中观察到的空间平均值的任何非零时间变化都必须归因于三次项，而该约束强制执行了这种归因。因此，当 $\\epsilon$ 很小但非零时，无约束稀疏回归可能会剪除三次项，而约束回归将恢复一个与质量平衡一致的非零系数。\n\n测试套件和预期行为：\n- 测试 1，$\\epsilon = 0.02$ 且 $\\tau = 0.03$：无约束 STLSQ 会剪除三次项，因为 $\\left|c^{\\mathrm{un}}_2\\right|  \\tau$，导致仅有扩散模型。约束回归强制执行质量损失，得到 $\\left|c^{\\mathrm{con}}_2\\right| \\gtrsim \\delta$，因此“changed”为真。\n- 测试 2，$\\epsilon = 0.0$：无约束和有约束方法都发现三次项不存在，$\\left|c^{\\mathrm{un}}_2\\right| \\approx 0$ 且 $\\left|c^{\\mathrm{con}}_2\\right| \\approx 0$，因此“changed”为假。\n- 测试 3，$\\epsilon = 0.15$ 且 $\\tau = 0.03$：三次项很强，所以两种方法都能检测到它，“changed”为假。\n\n该程序实现了模拟、特征构建、无约束 STLSQ 和通过 KKT 系统的等式约束回归，并按顺序输出包含三个测试的布尔结果的单行文本，格式为带括号的逗号分隔列表。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef spectral_laplacian(u, L):\n    \"\"\"\n    Compute u_xx using spectral differentiation on a periodic domain of length L.\n    u: shape (N,), real-valued\n    returns: shape (N,), real-valued u_xx\n    \"\"\"\n    N = u.size\n    dx = L / N\n    # Frequencies in cycles per unit length\n    freq = np.fft.fftfreq(N, d=dx)\n    k = 2.0 * np.pi * freq  # angular wavenumbers\n    uhat = np.fft.fft(u)\n    uxx_hat = -(k ** 2) * uhat\n    u_xx = np.fft.ifft(uxx_hat).real\n    return u_xx\n\ndef simulate_pde(N, T_steps, dt, nu, eps, L=2*np.pi, a0=0.5, a1=0.4, a2=-0.2):\n    \"\"\"\n    Simulate u_t = nu u_xx - eps u^3 on a periodic domain [0, L] with N grid points.\n    Forward Euler time stepping, spectral u_xx.\n    Returns:\n        U: array shape (T_steps+1, N)\n    \"\"\"\n    x = np.linspace(0.0, L, N, endpoint=False)\n    u = a0 + a1 * np.sin(x) + a2 * np.cos(2.0 * x)\n    U = np.empty((T_steps + 1, N), dtype=float)\n    U[0] = u\n    for n in range(T_steps):\n        u_xx = spectral_laplacian(u, L)\n        rhs = nu * u_xx - eps * (u ** 3)\n        u = u + dt * rhs\n        U[n + 1] = u\n    return U\n\ndef build_regression_system(U, dt, L=2*np.pi):\n    \"\"\"\n    Build y and Theta from simulation data U.\n    U: array shape (T_steps+1, N)\n    Returns:\n        y: shape (T_steps*N,)\n        Theta: shape (T_steps*N, 2) with columns [u_xx, u^3]\n    \"\"\"\n    T_steps = U.shape[0] - 1\n    N = U.shape[1]\n    # Preallocate\n    y_list = []\n    f1_list = []\n    f2_list = []\n    for n in range(T_steps):\n        u_n = U[n]\n        u_np1 = U[n + 1]\n        y_n = (u_np1 - u_n) / dt\n        u_xx_n = spectral_laplacian(u_n, L)\n        f1_list.append(u_xx_n.reshape(-1))\n        f2_list.append((u_n ** 3).reshape(-1))\n        y_list.append(y_n.reshape(-1))\n    f1 = np.concatenate(f1_list, axis=0)\n    f2 = np.concatenate(f2_list, axis=0)\n    y = np.concatenate(y_list, axis=0)\n    Theta = np.column_stack((f1, f2))\n    return y, Theta\n\ndef ridge_least_squares(Theta, y, alpha):\n    \"\"\"\n    Solve min ||Theta c - y||^2 + alpha ||c||^2\n    Returns c (shape (p,))\n    \"\"\"\n    p = Theta.shape[1]\n    A = Theta.T @ Theta + alpha * np.eye(p)\n    b = Theta.T @ y\n    c = np.linalg.solve(A, b)\n    return c\n\ndef stlsq(Theta, y, alpha, tau):\n    \"\"\"\n    Sequentially Thresholded Least Squares with ridge regularization.\n    1) Solve ridge LS for all features.\n    2) Zero coefficients with |c|  tau.\n    3) Refit ridge LS on retained features. If none retained, return zeros.\n    Returns c_full with length equal to number of columns in Theta.\n    \"\"\"\n    p = Theta.shape[1]\n    # Initial ridge solution\n    c = ridge_least_squares(Theta, y, alpha)\n    mask = np.abs(c) >= tau\n    if not np.any(mask):\n        return np.zeros(p)\n    # Refit with retained columns\n    Theta_reduced = Theta[:, mask]\n    c_reduced = ridge_least_squares(Theta_reduced, y, alpha)\n    c_full = np.zeros(p)\n    c_full[mask] = c_reduced\n    return c_full\n\ndef constrained_ridge_least_squares(Theta, y, alpha, g, h):\n    \"\"\"\n    Solve min ||Theta c - y||^2 + alpha ||c||^2 subject to g^T c = h\n    via KKT system:\n        [2(Theta^T Theta + alpha I), g] [c]   = [2 Theta^T y]\n        [g^T                          , 0] [λ]   [h]\n    Returns c.\n    \"\"\"\n    p = Theta.shape[1]\n    H = 2.0 * (Theta.T @ Theta + alpha * np.eye(p))\n    rhs1 = 2.0 * (Theta.T @ y)\n    # Ensure g is column vector shape (p,1)\n    g_col = g.reshape(p, 1)\n    # Build KKT matrix\n    KKT = np.block([[H, g_col],\n                    [g_col.T, np.zeros((1, 1))]])\n    rhs = np.concatenate([rhs1, np.array([h])], axis=0)\n    sol = np.linalg.solve(KKT, rhs)\n    c = sol[:p]\n    return c\n\ndef apply_conservation_constraint(y, Theta, L, N):\n    \"\"\"\n    Build discrete conservation constraint g^T c = h,\n    where g = dx * sum over all rows of Theta columns,\n          h = dx * sum over all entries of y.\n    \"\"\"\n    dx = L / N\n    g = dx * np.sum(Theta, axis=0)  # shape (2,)\n    h = dx * np.sum(y)\n    return g, h\n\ndef run_test_case(N, T_steps, dt, nu, eps, tau, delta, alpha, L=2*np.pi):\n    \"\"\"\n    Run a single test case: simulate, build system, fit unconstrained STLSQ and constrained ridge,\n    and decide if adding the constraint changed the discovered PDE by selecting the cubic term.\n    Returns boolean changed.\n    \"\"\"\n    # Simulate\n    U = simulate_pde(N=N, T_steps=T_steps, dt=dt, nu=nu, eps=eps, L=L)\n    # Build regression system\n    y, Theta = build_regression_system(U, dt, L=L)\n    # Unconstrained STLSQ\n    c_un = stlsq(Theta, y, alpha=alpha, tau=tau)\n    # Constrained ridge LS with mass-balance constraint\n    g, h = apply_conservation_constraint(y, Theta, L=L, N=N)\n    c_con = constrained_ridge_least_squares(Theta, y, alpha=alpha, g=g, h=h)\n    # Decide\n    cubic_un_present = np.abs(c_un[1]) >= delta\n    cubic_con_present = np.abs(c_con[1]) >= delta\n    changed = (not cubic_un_present) and cubic_con_present\n    return changed\n\ndef solve():\n    # Define constants\n    L = 2.0 * np.pi\n    alpha = 1e-8    # ridge regularization\n    delta = 1e-2    # detection threshold for presence\n    tau = 3e-2      # STLSQ pruning threshold\n\n    # Test cases: (N, T_steps, dt, nu, eps)\n    test_cases = [\n        (64, 200, 1e-3, 0.1, 0.02),  # Test 1: small cubic, constraint matters\n        (64, 200, 1e-3, 0.1, 0.0),   # Test 2: no cubic, both omit\n        (64, 200, 1e-3, 0.1, 0.15),  # Test 3: strong cubic, both include\n    ]\n\n    results = []\n    for N, T_steps, dt, nu, eps in test_cases:\n        changed = run_test_case(N=N, T_steps=T_steps, dt=dt, nu=nu, eps=eps,\n                                tau=tau, delta=delta, alpha=alpha, L=L)\n        results.append(changed)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}