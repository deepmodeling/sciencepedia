## 引言
自[科学革命](@entry_id:919172)以来，从经验数据中提取普适的数学定律一直是推动人类知识边界的核心驱动力。今天，在数据洪流的时代，我们拥有前所未有的能力去观测复杂的自然与工程系统，但如何从这些海量、高维的数据中自动洞察其背后的控制方程，仍然是一个巨大的挑战。传统的“黑箱”机器学习模型，如[深度神经网络](@entry_id:636170)，虽然在预测上取得了巨大成功，但其不透明的内在机制往往使我们难以获得根本性的科学理解。这催生了一个关键的知识缺口：我们能否开发出既能精确拟[合数](@entry_id:263553)据，又能产生简洁、可解释的“白箱”数学模型的自动化方法？

本文聚焦于解决这一问题的强大范式——用于[方程发现](@entry_id:1124591)的[符号回归](@entry_id:140405)。它直接致力于在庞大的数学表达式空间中搜索，以期找到能够以[闭合形式](@entry_id:271343)描述系统动态的公式。读者将学习到，这一方法如何平衡模型的预测能力与可解释性，从而成为加速科学发现的有力工具。

为了系统性地掌握这一领域，本文将分为三个核心部分。首先，在“**原理与机制**”一章中，我们将深入探讨方程的表示方法、高效的[搜索算法](@entry_id:272182)，以及广受欢迎的[SINDy](@entry_id:266063)方法的内在机制。接着，在“**应用与跨学科连接**”一章中，我们将通过物理学、工程学、生物学等多个领域的真实案例，展示[符号回归](@entry_id:140405)如何从一个理论工具转变为解决实际问题的得力助手。最后，“**动手实践**”部分将提供具体的编程练习，让您有机会亲自应用这些概念，体验从含噪数据中发现物理定律的挑战与乐趣。

## 原理与机制

在从数据中发现控制方程的探索中，我们拥有多种建模范式可供选择。这些方法的选择范围很广，从能够精确拟合数据但其内部工作机制不透明的“黑箱”模型，到结构透明、易于人类理解的“白箱”模型。本章旨在深入阐释用于[方程发现](@entry_id:1124591)，特别是[符号回归](@entry_id:140405)领域的核心原理与机制。我们将从基本概念出发，逐步探讨表示、搜索、[模型选择](@entry_id:155601)以及在实际应用中遇到的关键挑战。

### [方程发现](@entry_id:1124591)的基本方法

给定一个动态系统在一段时间内的状态观测数据，我们的目标是揭示其背后隐藏的数学定律，通常以[微分](@entry_id:158422)方程的形式呈现。在这一宏大目标下，不同的方法体现了在预测精度和可解释性之间的不同权衡。

一种极端是**黑箱[深度学习模型](@entry_id:635298)**，例如[循环神经网络](@entry_id:634803)（RNNs）。这些模型拥有强大的[函数逼近](@entry_id:141329)能力，通过优化数百万个参数，能够学习到非常复杂的动力学行为。然而，它们的代价是牺牲了可解释性。模型学到的高维[参数化](@entry_id:265163)表示与人类可读的、简洁的方程之间没有直接的对应关系。因此，虽然它们可能在预测上表现出色，但我们很难从模型中提取出关于系统物理机制的深刻见解 。

另一个极端是**[符号回归](@entry_id:140405) (Symbolic Regression, SR)**，它的核心目标是在一个巨大的数学表达式空间中进行搜索，以期找到一个能够以[闭合形式](@entry_id:271343)（closed-form）描述数据的公式。这个过程通常由一个预定义的语法（grammar）来约束，该语法包含了基本的数学运算符（如加、减、乘、除）、变量以及[初等函数](@entry_id:181530)（如正弦、指数函数）。如果搜索成功，其输出就是一个明确的、人类可读的方程。为了引导搜索过程倾向于更简单的模型，[符号回归](@entry_id:140405)通常会在其优化目标中引入一个**[复杂度惩罚](@entry_id:1122726)**项，这体现了奥卡姆剃刀原理——在所有能够同样好地解释数据的模型中，我们应选择最简洁的一个。

在这两个极端之间，存在一种极具影响力的[混合方法](@entry_id:163463)，即**[非线性动力学的稀疏辨识](@entry_id:276479) (Sparse Identification of Nonlinear Dynamics, SINDy)**。SINDy 的核心思想是，尽管动力学可能是高度[非线性](@entry_id:637147)的，但其控制方程在某个合适的函[数基](@entry_id:634389)（或库）中通常是**稀疏**的。这意味着，我们只需该函数库中少数几个项的线性组合，就能精确地描述系统的演化。SINDy 的工作流程是：首先，构建一个包含候选[非线性](@entry_id:637147)函数（例如，多项式、[三角函数](@entry_id:178918)等）的库；然后，将问题转化为一个[线性回归](@entry_id:142318)问题，并施加一个强烈的[稀疏性](@entry_id:136793)约束，以期找到一个只有少数几个非零系数的解。最终得到的模型是一个显式的、项数很少的[微分](@entry_id:158422)方程，兼具了较高的可解释性和简洁性 。这些简洁的模型不仅有助于科学理解，也为数字孪生系统的验证和控制设计提供了便利。

### 方程的表示与搜索

为了系统地从数据中发现方程，我们首先需要一种形式化的方式来表示数学表达式，并需要有效的算法来搜索这个广阔的可能性空间。

#### [表达式树](@entry_id:1124785)与[上下文无关文法](@entry_id:266529)

在[符号回归](@entry_id:140405)中，数学表达式通常被表示为一种称为**[表达式树](@entry_id:1124785) (expression tree)** 的数据结构。这是一棵有根的、有序的树，其**叶节点**代表操作数（如变量 $x$ 或常数 $c$），而其**内部节点**代表运算符（如 `+`, `sin`）。树的结构直接编码了运算的层次和顺序。例如，表达式 $a + b \times x$ 可以表示为一棵树，其根节点是 `+`，左子节点是参数 $a$，右子节点是一个以 `*` 为根的子树。

每个内部节点的子节点数量必须与其所代表运算符的**目数 (arity)** 相匹配。例如，[二元运算](@entry_id:152272)符 `+` 必须有两个子节点，而一元运算符 `sin` 则必须有一个子节点。此外，树是**有序的**，因为子节点的顺序至关重要，尤其对于像减法和除法这样的[非交换](@entry_id:136599)运算。

为了确保生成的表达式在语法上是合法的，我们通常使用**[上下文无关文法](@entry_id:266529) (Context-Free Grammar, CFG)** 来定义表达式的集合。一个设计良好的、无歧义的 CFG 可以通过其产生式规则直接强制实施关键的句法属性。例如，一个典型的算术表达式文法可能会使用分层的非终结符（如 `Expression`, `Term`, `Factor`）来正确处理运算符的**优先级**（例如，乘法优先于加法）和**[结合性](@entry_id:147258)**（例如，$a-b-c$ 被解析为 $(a-b)-c$）。通过在产生式规则中使用[左递归](@entry_id:751232)或右递归，可以分别编码左[结合性](@entry_id:147258)或右[结合性](@entry_id:147258)。例如，产生式 $E \to E + T$ 编码了加法的左[结合性](@entry_id:147258)。这种基于文法的[形式化方法](@entry_id:1125241)，确保了从一个给定的表达式字符串到其唯一的[表达式树](@entry_id:1124785)的解析过程是明确无误的 。

#### [搜索算法](@entry_id:272182)：完备性与搜索偏好

[符号回归](@entry_id:140405)的本质是在由文法定义的、通常是无限的表达式空间 $\mathcal{E}$ 中进行搜索。不同的[搜索算法](@entry_id:272182)在该任务中表现出不同的特性，主要体现在**完备性 (completeness)** 和**搜索偏好 (search bias)** 上。完备性指的是一个算法是否能保证最终找到全局最优解，而搜索偏好指的是算法在搜索过程中倾向于哪些类型的候选解。

*   **穷举枚举 (Exhaustive Enumeration)**：这种方法按照某种公平的顺序（例如，按[表达式树](@entry_id:1124785)的节点数量从小到大）遍历整个表达式空间。由于它系统地检查了每一个可能的表达式，因此它是一种**完备的**算法——只要存在一个最优解，它最终一定会被找到。它的搜索偏好由其枚举顺序决定，通常是偏好更简单的表达式，这与奥卡姆剃刀的精神不谋而合。然而，其巨大的计算成本使得它在实践中只适用于非常简单的问题 。

*   **[启发式搜索](@entry_id:637758) (Heuristic Search)**：像**[集束搜索](@entry_id:634146) (Beam Search)** 这样的算法通过使用[启发式](@entry_id:261307)函数来评估部分解的优劣，从而引导搜索方向。在每一步扩展中，[集束搜索](@entry_id:634146)只保留有限数量（即“集束宽度” $b$）的最优候选解，并剪除其余所有分支。这种贪婪的剪枝策略使得算法变得不完备：一旦通往最优解的路径在某一步被剪除，它就再也无法被找到。其搜索偏好由启发式函数和集束宽度共同决定。只有当集束宽度趋于无穷大时，它才退化为穷举搜索并恢复完备性 。

*   **[随机搜索](@entry_id:637353) (Stochastic Search)**：**遗传编程 (Genetic Programming, GP)** 是[符号回归](@entry_id:140405)中最常用的一类[随机搜索](@entry_id:637353)算法。它维护一个由候选表达式组成的“种群”，并通过模拟生物进化的过程（如基于适应度的**选择**、**交叉**和**变异**）来迭代地改进种群。在实际应用中，由于种群规模有限，GP 通常是不完备的，可能会收敛到局部最优解。然而，在理论上，如果变异算子具有“完全支持”（即从任何表达式出发，都有非零概率通过有限次变异到达任何其他表达式），那么 GP 就是**概率完备的**，即随着迭代次数趋于无穷，找到[全局最优解](@entry_id:175747)的概率将趋近于 1。GP 的搜索偏好是复杂的，它源于[适应度](@entry_id:154711)选择、表达式表示方式以及变异算子设计之间精妙的相互作用 。

### [稀疏辨识](@entry_id:1132025) ([SINDy](@entry_id:266063)) 的核心机制

[SINDy](@entry_id:266063) 作为一个高效的[方程发现](@entry_id:1124591)框架，其核心在于一个精妙的数学假设和相应的[优化算法](@entry_id:147840)。

#### 从稀疏假设到线性回归

SINDy 的基石是假设一个动力系统的控制方程 $\dot{\mathbf{x}} = f(\mathbf{x})$ 在一个大型的候选函数库 $\mathbf{\Theta}(\mathbf{x})$ 中是稀疏的。这个库是一个矩阵，其列由各种关于[状态变量](@entry_id:138790) $\mathbf{x}$ 的[非线性](@entry_id:637147)函数组成，例如常数、多项式项（$x_1, x_2, x_1^2, x_1 x_2, \dots$）或[三角函数](@entry_id:178918)项（$\sin(x_1), \cos(x_2), \dots$）。

如果[稀疏性](@entry_id:136793)假设成立，我们就可以将[非线性动力学](@entry_id:901750)问题近似为一个[线性模型](@entry_id:178302)：
$$
\dot{\mathbf{X}} \approx \mathbf{\Theta}(\mathbf{X}) \mathbf{\Xi}
$$
在这里，$\dot{\mathbf{X}}$ 是一个矩阵，其行是不同时间点上[状态向量](@entry_id:154607)的时间导数（通常通过数值方法从[时间序列数据](@entry_id:262935)中估计得到）。$\mathbf{\Theta}(\mathbf{X})$ 是在这些时间点上评估的库函数矩阵。$\mathbf{\Xi}$ 是一个待求的[系数矩阵](@entry_id:151473)，其中每一列 $\boldsymbol{\xi}_i$ 对应于[状态变量](@entry_id:138790) $\dot{x}_i$ 的方程。稀疏性假设意味着 $\mathbf{\Xi}$ 的每一列中只有少数几个元素是非零的。

因此，[方程发现](@entry_id:1124591)问题转化为一个[稀疏回归](@entry_id:276495)问题。最常用的方法是 **LASSO (Least Absolute Shrinkage and Selection Operator)**，其[目标函数](@entry_id:267263)如下：
$$
\min_{\mathbf{\Xi}} \|\dot{\mathbf{X}} - \mathbf{\Theta}(\mathbf{X})\mathbf{\Xi}\|_2^2 + \lambda \|\mathbf{\Xi}\|_1
$$
该目标函数由两部分组成：第一部分是**均方误差项**（$L_2$ 范数平方），它衡量模型对数据的[拟合优度](@entry_id:176037)；第二部分是**正则化项**（$L_1$ 范数），它对系数向量的绝对值之和进行惩罚。参数 $\lambda$ 控制着在这两者之间的权衡。

#### L1 范数与稀疏性的[贝叶斯解释](@entry_id:265644)

$L_1$ 正则化项是实现[稀疏性](@entry_id:136793)的关键。从贝叶斯统计的视角来看，[LASSO](@entry_id:751223) [目标函数](@entry_id:267263)等价于在给定数据下对系数 $\mathbf{\Xi}$ 进行**[最大后验概率](@entry_id:268939) (Maximum A Posteriori, MAP)** 估计。具体而言，这对应于假设测量误差服从**高斯分布**（这导出了 $L_2$ 均方误差项），并假设模型系数的[先验分布](@entry_id:141376)为**[拉普拉斯分布](@entry_id:266437)** (Laplace distribution)。[拉普拉斯分布](@entry_id:266437)的[概率密度函数](@entry_id:140610)为 $p(\xi_j) \propto \exp(-|\xi_j|/b)$，其尖锐的峰值在零点，并具有比高斯分布更重的尾部。正是这种在零点处的尖峰特性，使得 MAP 估计天然地偏好将许多系数精确地设置为零，从而产生[稀疏解](@entry_id:187463)。在这个框架下，[正则化参数](@entry_id:162917) $\lambda$ 与[高斯噪声](@entry_id:260752)的方差 $\sigma^2$ 和拉普拉斯先验的[尺度参数](@entry_id:268705) $b$ 直接相关，即 $\lambda = 2\sigma^2/b$ 。

与 $L_1$ 正则化形成对比的是 $L_2$ 正则化（[岭回归](@entry_id:140984), Ridge Regression），它使用系数的 $L_2$ 范数平方 $\|\mathbf{\Xi}\|_2^2$ 作为惩罚项。$L_2$ 惩罚会使系数向零收缩，但通常不会将它们精确地置为零。因此，[LASSO](@entry_id:751223) 因其能够同时进行[变量选择](@entry_id:177971)和[系数估计](@entry_id:175952)而成为[稀疏建模](@entry_id:204712)的首选。

对于一个特殊的正交设计情况，即库矩阵的列是正交的（$X^\top X = I_p$），[LASSO](@entry_id:751223) 问题有一个解析解，称为**[软阈值算子](@entry_id:755010) (soft-thresholding operator)**。解的形式为 $\hat{\theta}_j = \operatorname{sign}(z_j)\max(|z_j| - \lambda/2, 0)$，其中 $z_j$ 是特征与响应之间的相关性度量。这个表达式清晰地表明，如果特征与响应的相关性不够强（即 $|z_j| \le \lambda/2$），其对应的系数 $\hat{\theta}_j$ 将被精确地置为零 。这正是 $L_1$ 范数实现[稀疏性](@entry_id:136793)的核心数学机制。值得注意的是，尽管 $L_1$ 范数在零点处不可导，但整个 LASSO [目标函数](@entry_id:267263)是凸的，这意味着我们可以高效地找到其全局最优解。

### 实践挑战与高级考量

尽管[符号回归](@entry_id:140405)和 [SINDy](@entry_id:266063) 的原理直观而强大，但在实际应用中，研究者必须面对一系列复杂的挑战。

#### [模型选择](@entry_id:155601)：拟合度与复杂度的权衡

一个核心的挑战是在模型的**拟合优度**和**复杂度**之间找到恰当的平衡。一个过于复杂的模型可能会完美地拟合训练数据（[过拟合](@entry_id:139093)），但在预测新数据时表现很差。**奥卡姆剃刀**原则为我们提供了指导思想：在所有能够解释数据的模型中，最简单的那个往往是最好的。

**[最小描述长度](@entry_id:261078) (Minimum Description Length, MDL)** 原理是奥卡姆剃刀的一个形式化版本。它主张，最好的模型是那个能够以最短的总“码长”来描述数据和模型本身的。总码长由两部分构成：$L(\text{模型})$，即编码模型结构和参数所需的比特数；以及 $L(\text{数据}|\text{模型})$，即在已知模型的前提下编码数据（通常是残差）所需的比特数。

考虑一个情景：我们有两个候选模型 $M_1: y = a + bx$ 和 $M_2: y = a + bx + c \sin(2\pi x)$。如果我们的训练数据是在整数点 $x_i \in \{0, 1, 2, \dots\}$ 上采集的，那么 $\sin(2\pi x_i)$ 项将恒为零。因此，两个模型在这些数据点上的预测完全相同，它们的[均方根误差 (RMSE)](@entry_id:1131101) 也完全相同，从而 $L(\text{数据}|M_1) = L(\text{数据}|M_2)$。然而，$M_2$ 的结构显然比 $M_1$ 更复杂，编码 $M_2$ 需要更多的比特（因为它包含更多的运算符、参数和变量）。根据 MDL 原理，我们应该选择 $M_1$，因为它用更简洁的描述实现了相同的拟合效果 。

在更一般的情况下，[模型选择](@entry_id:155601)可以被构建为一个**[多目标优化](@entry_id:637420)**问题。我们希望同时最小化多个相互冲突的目标，例如，一个或多个衡量[模型误差](@entry_id:175815)的指标（如 RMSE）和一个或多个衡量[模型复杂度](@entry_id:145563)的指标（如[表达式树](@entry_id:1124785)的节点数）。在这种情况下，通常不存在一个在所有目标上都是最优的“唯一最佳”模型。取而代之的是一个被称为**[帕累托前沿](@entry_id:634123) (Pareto front)** 的[解集](@entry_id:154326)。

一个模型 $g$ **[帕累托支配](@entry_id:634846) (Pareto-dominates)** 另一个模型 $h$，如果 $g$ 在所有目标上都不劣于 $h$，并且至少在一个目标上严格优于 $h$。[帕累托前沿](@entry_id:634123)由所有**非支配**的模型组成，它们代表了误差与复杂度之间的最佳权衡。任何不在前沿上的模型都是次优的，因为总存在一个前沿上的模型，它在至少一个维度上更好，而在其他维度上不差。最终的模型选择过程通常是在[帕累托前沿](@entry_id:634123)上进行，例如，选择前沿上“拐点”处的模型，该点被认为是复杂度和精度之间最具性价比的平衡点 。在处理来自不同尺度或实验条件的数据时，对[误差指标](@entry_id:173250)进行适当的归一化，以确保它们是无量纲且可比较的，这一点至关重要。

#### 噪声的影响与[统计显著性](@entry_id:147554)

真实的测量数据总是伴随着噪声。噪声会模糊底层动力学的信号，使得[方程发现](@entry_id:1124591)变得更加困难。一个关键问题是：我们需要多强的信号才能在噪声背景中可靠地辨识出一个存在的项？

这可以通过[统计功效分析](@entry_id:177130)来回答。假设我们使用一个[线性模型](@entry_id:178302)（如 SINDy），并对每个候选系数 $\theta_j$ 进行假设检验，以判断其是否显著不为零。为了在同时检验多个假设时控制总体错误率（例如，使用 **Bonferroni 校正**），我们需要一个比单次检验更严格的[显著性水平](@entry_id:902699)。

一个项能否被成功辨识，取决于其**[信噪比](@entry_id:271861) (Signal-to-Noise Ratio, SNR)**，它量化了由该项产生的信号强度与背景噪声水平的相对大小。对于一个特定的项（如三次项 $x^3$），其系数为 $\theta_3$，我们可以定义其[信噪比](@entry_id:271861)为 $\mathrm{SNR}_3 = n\theta_3^2 / \sigma^2$，其中 $n$ 是[样本量](@entry_id:910360)，$\sigma^2$ 是噪声方差。为了以至少 $1-\beta$ 的概率（即统计功效）正确地识别出这个项，其 $\mathrm{SNR}_3$ 必须达到一个最小阈值。这个阈值取决于我们设定的显著性水平 $\alpha$、功效 $1-\beta$ 以及候选库中的项数 $p$。一个精确的推导表明，这个最小[信噪比](@entry_id:271861)为 $(\Phi^{-1}(1 - \frac{\alpha}{2p}) + \Phi^{-1}(1-\beta))^2$，其中 $\Phi^{-1}$ 是[标准正态分布](@entry_id:184509)的[逆累积分布函数](@entry_id:266870) 。这个结果凸显了一个重要的事实：在噪声存在的情况下，即使是真实存在于动力学中的项，如果其系数过小（信号弱），也可能因为统计上不显著而被忽略。

#### [可辨识性](@entry_id:194150)问题

即使在理想的无噪声情况下，我们也可能面临**可辨识性 (identifiability)** 的挑战。可辨识性涉及我们能否从数据中唯一地确定模型的结构和参数。

*   **[参数可辨识性](@entry_id:197485) (Parameter Identifiability)**：对于一个**固定**的模型结构，我们能否唯一地确定其参数值？答案是否定的。例如，对于模型 $f(x) = \theta_1 \theta_2 x$，我们只能从数据中确定斜率，即乘积 $p = \theta_1 \theta_2$。任何满足 $\theta_1 \theta_2 = p$ 的参数对 $(\theta_1, \theta_2)$ 都会产生完全相同的函数，因此 $\theta_1$ 和 $\theta_2$ 是不可单独辨识的 。

*   **结构可辨识性 (Structural Identifiability)**：我们能否唯一地确定模型本身的函数形式？这也可能是一个挑战。例如，表达式 $\theta x$ 和 $\phi \exp(\ln x)$ 在语法上（即[表达式树](@entry_id:1124785)结构上）是不同的。然而，在定义域 $x>0$ 上，它们在数学上是等价的。如果真实函数是 $f(x)=5x$，那么两个结构都能以 $\theta=5$ 或 $\phi=5$ 完美拟合，这就导致了结构上的非唯一性。解决这个问题通常需要对表达式进行**规范化 (canonicalization)**，即将所有在数学上等价的表达式归约到一个唯一的[标准形式](@entry_id:153058) 。

在[多尺度建模](@entry_id:154964)的背景下，这些问题尤为突出。例如，假设一个系统的真实微观定律是 $\frac{dy}{dt} = ky$，但我们观测到的时间尺度是 $t' = \beta t$，其中缩放因子 $\beta$ 未知。通过[链式法则](@entry_id:190743)，我们得到的观测宏观定律是 $\frac{dy}{dt'} = (k/\beta)y$。我们只能辨识出[集总参数](@entry_id:274932) $\tilde{k} = k/\beta$，而无法单独确定微观[速率常数](@entry_id:140362) $k$。然而，该定律的**结构**——“一阶[线性动力学](@entry_id:177848)”——仍然是可辨识的 。这个例子清楚地展示了[参数可辨识性](@entry_id:197485)的丧失可以独立于结构[可辨识性](@entry_id:194150)的保持。

#### 库设计与[共线性](@entry_id:270224)

对于 SINDy 等基于库的方法，其成功在很大程度上取决于候选函数库的设计。一个关键的实际问题是**[共线性](@entry_id:270224) (collinearity)**，也称为**[相互相干性](@entry_id:188177) (mutual coherence)**。当库中的两个或多个函数（列）高度相关时，回归算法很难将它们对输出的贡献区分开来。

例如，如果一个变量 $x$ 的数据只在一个非常有限的动态范围内采样（例如，均匀分布在 $[0, \Delta]$ 且 $\Delta$ 很小），那么函数 $\phi_1(x) = x$ 和 $\phi_2(x) = x^2$ 的行为会非常相似。一个形式化的分析表明，在这种情况下，这两个函数之间的皮尔逊相关系数的绝对值会趋近于一个非常高的值，即 $\frac{\sqrt{15}}{4} \approx 0.968$ 。如此高的相关性使得[稀疏回归](@entry_id:276495)算法难以确定动力学是依赖于 $x$ 还是 $x^2$，或者两者的某种组合，这会严重影响所发现方程的稳定性和[可解释性](@entry_id:637759)。

#### 融入物理约束

最后，一个增强[符号回归](@entry_id:140405)能力的强大策略是主动地将已知的物理原理作为硬性约束融入到搜索过程中。**[量纲分析](@entry_id:140259) (dimensional analysis)** 就是一个典型的例子。

根据**白金汉 $\Pi$ 定理 (Buckingham $\Pi$ theorem)**，任何物理上有效的方程都必须是量纲一致的。这意味着方程中的每一项都必须具有相同的物理单位。在流体力学中，一个依赖于流体密度 $\rho$、[动力粘度](@entry_id:268228) $\mu$、特征速度 $U$ 和特征长度 $L$ 的无量纲响应，必定是这些变量构成的[无量纲数](@entry_id:260863)（如雷诺数 $\text{Re} = \rho UL / \mu$）的函数。

通过在[符号回归](@entry_id:140405)的搜索过程中强制要求所有候选表达式必须是无量纲的，我们可以极大地**削减搜索空间**。例如，考虑一个由 $\rho^a \mu^b U^c L^d$ 形式的单项式构成的库，其中指数 $a,b,c,d$ 在 $[-2, 2]$ 之间取整数。在没有任何约束的情况下，共有 $5^4 = 625$ 个可能的单项式。然而，通过施加[量纲一致性](@entry_id:271193)约束，我们发现只有当指数向量 $(a,b,c,d)$ 是基向量 $(1, -1, 1, 1)$ 的整数倍时，该单项式才是无量纲的。在指定的指数范围内，这只留下了 5 个满足条件的单项式 。这种超过 99% 的搜索空间缩减，极大地提高了发现物理上有效方程的效率和可能性。