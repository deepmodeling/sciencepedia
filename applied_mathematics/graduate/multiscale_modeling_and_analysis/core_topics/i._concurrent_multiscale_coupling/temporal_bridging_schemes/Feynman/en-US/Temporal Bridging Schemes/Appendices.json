{
    "hands_on_practices": [
        {
            "introduction": "A crucial first step in designing any numerical integration scheme is to understand its stability properties. For temporal bridging schemes applied to systems with distinct fast and slow dynamics, Implicit-Explicit (IMEX) methods are a natural choice. This exercise guides you through a fundamental linear stability analysis of a simple IMEX scheme, revealing how the explicit treatment of slow dynamics imposes a constraint on the maximum allowable macro time step, even when the fast, stiff dynamics are handled implicitly .",
            "id": "3813574",
            "problem": "Consider an additive split linear system in continuous time given by $\\dot{u}(t) = A_{f} u(t) + A_{s} u(t)$, where $A_{f} \\in \\mathbb{R}^{n \\times n}$ models fast, stiff dynamics and $A_{s} \\in \\mathbb{R}^{n \\times n}$ models slow dynamics, with $\\|A_{f}\\| \\gg \\|A_{s}\\|$. Assume that $A_{f}$ is such that all its eigenvalues have strictly negative real parts and that $A_{s}$ is diagonalizable with real eigenvalues lying in the interval $(-\\infty, 0]$. Consider the following temporal bridging scheme of Implicit-Explicit (IMEX) type that treats the fast operator implicitly and the slow operator explicitly during one macro-step of size $\\Delta t$:\n$$(I - \\Delta t\\, A_{f})\\, u^{n+1} \\;=\\; \\big(I + \\Delta t\\, A_{s}\\big)\\, u^{n}.$$\nLet the scalar linear test problem be $\\dot{y}(t) = \\lambda_{f} y(t) + \\lambda_{s} y(t)$ with $\\lambda_{f} \\in \\mathbb{C}$ satisfying $\\operatorname{Re}(\\lambda_{f}) \\leq 0$ and $\\lambda_{s} \\in \\mathbb{R}$ satisfying $\\lambda_{s} \\leq 0$. Define the dimensionless parameters $z_{f} = \\Delta t\\, \\lambda_{f}$ and $z_{s} = \\Delta t\\, \\lambda_{s}$.\n\nStarting from fundamental linear stability analysis of numerical methods for the Dahlquist test equation, derive the scalar amplification factor for the above IMEX scheme and compute its absolute stability region in the $(z_{f}, z_{s})$ plane. Then, under the assumption that the fast part is to be stable for all $\\operatorname{Re}(z_{f}) \\leq 0$, deduce the allowable macro step size $\\Delta t$ as a closed-form expression in terms of the largest magnitude of the slow eigenvalues. Let $\\alpha > 0$ denote the quantity\n$$\\alpha \\;=\\; \\max_{1 \\leq j \\leq n} |\\lambda_{s,j}|,$$\nwhere $\\{\\lambda_{s,j}\\}_{j=1}^{n}$ are the eigenvalues of $A_{s}$, all lying on the nonpositive real axis. Express the final $\\Delta t$ in seconds. The final answer must be a single closed-form analytical expression for the maximum allowable macro step $\\Delta t$ in terms of $\\alpha$; no rounding is necessary.",
            "solution": "The problem asks for the maximum allowable macro-step size $\\Delta t$ for a given Implicit-Explicit (IMEX) temporal bridging scheme, ensuring stability. The derivation must proceed from a first-principles linear stability analysis.\n\nFirst, we validate the problem statement.\nThe problem provides a linear system of ordinary differential equations, $\\dot{u}(t) = A_{f} u(t) + A_{s} u(t)$, where the matrix operator is additively split into a stiff part $A_f$ and a non-stiff part $A_s$. The properties of the eigenvalues of $A_f$ and $A_s$ are specified, consistent with their roles. An IMEX scheme is proposed, which is a standard technique for such systems. The analysis is to be performed on the scalar Dahlquist test equation $\\dot{y}(t) = \\lambda_{f} y(t) + \\lambda_{s} y(t)$, which is the standard procedure for linear stability analysis. All terms and objectives are well-defined and mathematically and scientifically sound. The problem is self-contained, consistent, and well-posed. Therefore, the problem is deemed valid and we may proceed with the solution.\n\nThe IMEX scheme is given by:\n$$(I - \\Delta t\\, A_{f})\\, u^{n+1} \\;=\\; \\big(I + \\Delta t\\, A_{s}\\big)\\, u^{n}$$\nTo analyze the stability of this scheme, we apply it to the scalar linear test problem $\\dot{y}(t) = \\lambda_{f} y(t) + \\lambda_{s} y(t)$, where $\\lambda_f$ and $\\lambda_s$ are scalars representing eigenvalues of the fast and slow operators, respectively. The scheme becomes:\n$$(1 - \\Delta t\\, \\lambda_{f})\\, y^{n+1} \\;=\\; (1 + \\Delta t\\, \\lambda_{s})\\, y^{n}$$\nThe scalar amplification factor, $G$, is defined by the relation $y^{n+1} = G y^{n}$. From the scheme, we can solve for $G$:\n$$G \\;=\\; \\frac{1 + \\Delta t\\, \\lambda_{s}}{1 - \\Delta t\\, \\lambda_{f}}$$\nUsing the dimensionless parameters $z_{f} = \\Delta t\\, \\lambda_{f}$ and $z_{s} = \\Delta t\\, \\lambda_{s}$, the amplification factor can be written as:\n$$G(z_f, z_s) \\;=\\; \\frac{1 + z_{s}}{1 - z_{f}}$$\nFor the numerical scheme to be absolutely stable, the magnitude of the amplification factor must not exceed unity, i.e., $|G(z_f, z_s)| \\leq 1$.\n$$ \\left| \\frac{1 + z_{s}}{1 - z_{f}} \\right| \\leq 1 $$\nThis inequality can be rewritten as $|1 + z_{s}| \\leq |1 - z_{f}|$.\n\nThe problem specifies the domains for the eigenvalues: $\\lambda_{f} \\in \\mathbb{C}$ with $\\operatorname{Re}(\\lambda_{f}) \\leq 0$, and $\\lambda_{s} \\in \\mathbb{R}$ with $\\lambda_{s} \\leq 0$. Consequently, for any $\\Delta t > 0$, we have $z_{f} \\in \\mathbb{C}$ with $\\operatorname{Re}(z_{f}) \\leq 0$ (the left half of the complex plane, including the imaginary axis), and $z_{s} \\in \\mathbb{R}$ with $z_{s} \\leq 0$ (the non-positive real axis).\n\nA crucial part of the problem statement is the assumption that the scheme must be stable for all possible fast dynamics, meaning for all $z_f$ such that $\\operatorname{Re}(z_f) \\leq 0$. This requires that for a given $z_s$, the stability condition $|1 + z_{s}| \\leq |1 - z_{f}|$ must hold for every $z_f$ in the left half-plane. To ensure this, we must satisfy the condition for the \"worst-case\" $z_f$, which is the one that minimizes the right-hand side, $|1 - z_{f}|$.\n\nLet $z_f = x + iy$, where $x = \\operatorname{Re}(z_f) \\leq 0$ and $y = \\operatorname{Im}(z_f) \\in \\mathbb{R}$. The term $|1 - z_f|$ becomes:\n$$|1 - z_{f}| \\;=\\; |1 - (x + iy)| \\;=\\; |(1-x) - iy| \\;=\\; \\sqrt{(1-x)^2 + y^2}$$\nSince $x \\le 0$, it follows that $1-x \\ge 1$. Therefore, $(1-x)^2 \\ge 1$. The term $y^2$ is always non-negative. This means:\n$$|1 - z_{f}|^2 \\;=\\; (1-x)^2 + y^2 \\;\\geq\\; 1^2 + 0^2 \\;=\\; 1$$\nThe minimum value of $|1 - z_{f}|$ for $\\operatorname{Re}(z_f) \\leq 0$ is $1$, which occurs at $z_f = 0$.\n\nFor the stability inequality $|1 + z_{s}| \\leq |1 - z_{f}|$ to hold for all valid $z_f$, we must therefore require:\n$$|1 + z_{s}| \\leq \\min_{\\operatorname{Re}(z_f) \\leq 0} |1 - z_{f}| = 1$$\nThis yields the simplified stability constraint on the slow part of the scheme:\n$$|1 + z_{s}| \\leq 1$$\nGiven that $z_s$ is a non-positive real number ($z_s \\leq 0$), the term $1+z_s$ is at most $1$. The absolute value inequality is equivalent to the two-sided inequality:\n$$-1 \\leq 1 + z_{s} \\leq 1$$\nThe right side, $1 + z_{s} \\leq 1$, implies $z_s \\leq 0$, which is already given. The left side, $-1 \\leq 1 + z_s$, implies:\n$$z_{s} \\geq -2$$\nCombining these, the stability of the scheme under the given assumptions depends solely on the condition that $-2 \\leq z_s \\leq 0$. This is the stability interval for the Forward Euler method, which is appropriately the explicit part of the IMEX scheme.\n\nThis condition must hold for all eigenvalues of the slow operator $A_s$. That is, for every eigenvalue $\\lambda_{s,j}$ of $A_s$, the corresponding value $z_{s,j} = \\Delta t \\, \\lambda_{s,j}$ must lie in the stability interval $[-2, 0]$.\nSince all $\\lambda_{s,j} \\leq 0$, we have $\\Delta t \\, \\lambda_{s,j} \\leq 0$ for $\\Delta t > 0$, so we only need to satisfy:\n$$\\Delta t \\, \\lambda_{s,j} \\geq -2$$\nIf $\\lambda_{s,j} = 0$, the inequality $0 \\ge -2$ is trivially satisfied and imposes no constraint on $\\Delta t$. If $\\lambda_{s,j} < 0$, we can divide by it, which reverses the inequality sign:\n$$\\Delta t \\leq \\frac{-2}{\\lambda_{s,j}} = \\frac{2}{|\\lambda_{s,j}|}$$\nThis must hold for all $j$ corresponding to non-zero eigenvalues. To ensure this is true for all such eigenvalues simultaneously, $\\Delta t$ must be less than or equal to the minimum of these upper bounds:\n$$\\Delta t \\leq \\min_{j: \\lambda_{s,j} \\neq 0} \\left( \\frac{2}{|\\lambda_{s,j}|} \\right)$$\nThe minimum of the set $\\{ \\frac{2}{|\\lambda_{s,j}|} \\}$ is determined by the largest value of $|\\lambda_{s,j}|$. The problem defines this quantity as $\\alpha = \\max_{1 \\leq j \\leq n} |\\lambda_{s,j}|$. Thus, the constraint on the macro-step size becomes:\n$$\\Delta t \\leq \\frac{2}{\\alpha}$$\nThe maximum allowable macro-step size is therefore the upper bound of this inequality. If the units of the eigenvalues $\\lambda_{s,j}$ are $s^{-1}$, then the unit of $\\alpha$ is also $s^{-1}$, and the final expression for $\\Delta t$ will be in seconds as requested.\n\nThe final answer is the analytical expression for this maximum step size.",
            "answer": "$$\n\\boxed{\\frac{2}{\\alpha}}\n$$"
        },
        {
            "introduction": "Beyond stability, the practical utility of a temporal bridging scheme is governed by its computational efficiency. This involves a delicate trade-off: longer macro steps reduce the overall number of steps but may require more expensive micro-simulations to maintain accuracy. This problem challenges you to formalize this trade-off by constructing and solving a constrained optimization problem, allowing you to derive the optimal macro step size and micro-simulation duration that minimize total computational cost for a given error tolerance .",
            "id": "3813575",
            "problem": "Consider a temporal bridging scheme in multiscale modeling where a macro-level integrator advances a coarse variable over a fixed horizon $T$ by using short micro-level simulation bursts to estimate coarse drift. Assume the following scientifically grounded facts:\n\n- The macro-level integrator has order $p \\geq 1$, so the global discretization error contribution scales as $a\\,\\Delta t^{p}$ for some constant $a>0$, where $\\Delta t$ is the macro time step.\n- The micro-level estimation error of the coarse drift, obtained by averaging an ergodic fast process over a micro burst of duration $\\tau$, scales as $b\\,\\tau^{-1/2}$ for some constant $b>0$, consistent with the Central Limit Theorem and assuming the micro burst is longer than the mixing time so that bias is negligible.\n- The computational cost of a single micro burst is linear in its duration, $C_{\\text{micro}}(\\tau)=\\alpha\\,\\tau$ with $\\alpha>0$ the cost per unit micro time. Assume the macro-level stepping overhead is negligible compared to micro burst cost.\n- The macro step count over the horizon $T$ is $N=T/\\Delta t$, treating $N$ as continuous and ignoring rounding effects.\n\nDefine the total cost over the horizon by $C_{\\text{tot}}(\\tau,\\Delta t)=N\\,C_{\\text{micro}}(\\tau)$ and the total error by $E_{\\text{tot}}(\\tau,\\Delta t)=a\\,\\Delta t^{p}+b\\,\\tau^{-1/2}$. For a given error tolerance $\\varepsilon>0$, formulate the constrained optimization problem that minimizes $C_{\\text{tot}}(\\tau,\\Delta t)$ subject to $E_{\\text{tot}}(\\tau,\\Delta t)\\leq \\varepsilon$. Derive the explicit analytic expressions for the optimal micro burst duration $\\tau^{\\star}$ and the optimal macro time step $\\Delta t^{\\star}$ in terms of $a$, $b$, $p$, and $\\varepsilon$.\n\nExpress your final answers for $\\tau^{\\star}$ and $\\Delta t^{\\star}$ as closed-form expressions in terms of $a$, $b$, $p$, and $\\varepsilon$. No numerical evaluation is required. No units are required in the final expressions.",
            "solution": "The problem is to find the optimal micro burst duration $\\tau^{\\star}$ and macro time step $\\Delta t^{\\star}$ that minimize the total computational cost subject to a constraint on the total error. This is a constrained optimization problem.\n\nThe given quantities are:\nThe total cost function: $C_{\\text{tot}}(\\tau, \\Delta t) = N\\,C_{\\text{micro}}(\\tau)$, where $N = T/\\Delta t$ and $C_{\\text{micro}}(\\tau) = \\alpha\\,\\tau$. Substituting these gives $C_{\\text{tot}}(\\tau, \\Delta t) = T\\alpha\\frac{\\tau}{\\Delta t}$.\nThe total error function: $E_{\\text{tot}}(\\tau, \\Delta t) = a\\,\\Delta t^{p} + b\\,\\tau^{-1/2}$.\nThe constraint is that the total error must not exceed a given tolerance $\\varepsilon > 0$, so $E_{\\text{tot}}(\\tau, \\Delta t) \\leq \\varepsilon$.\nThe parameters $a, b, \\alpha, T$ are positive constants, and the order of the macro-integrator is $p \\geq 1$. The optimization variables are $\\tau > 0$ and $\\Delta t > 0$.\n\nThe optimization problem is formally stated as:\nMinimize $C_{\\text{tot}}(\\tau, \\Delta t) = T\\alpha\\frac{\\tau}{\\Delta t}$\nsubject to $a\\,\\Delta t^{p} + b\\,\\tau^{-1/2} \\leq \\varepsilon$.\n\nThe cost function $C_{\\text{tot}}$ is a strictly increasing function of $\\tau$ and a strictly decreasing function of $\\Delta t$. To minimize the cost, we should seek the smallest possible $\\tau$ and the largest possible $\\Delta t$. The constraint $a\\,\\Delta t^{p} + b\\,\\tau^{-1/2} \\leq \\varepsilon$ limits how large $\\Delta t$ can be and how small $\\tau$ can be. If the inequality were strict, i.e., $a\\,\\Delta t^{p} + b\\,\\tau^{-1/2} < \\varepsilon$, one could always increase $\\Delta t$ or decrease $\\tau$ (or both) to further reduce the cost while still satisfying the constraint. Therefore, the minimum cost must be achieved when the constraint is active, meaning the equality holds:\n$a\\,\\Delta t^{p} + b\\,\\tau^{-1/2} = \\varepsilon$.\n\nWe use the method of Lagrange multipliers to solve this constrained optimization problem. The Lagrangian function $\\mathcal{L}$ is defined as:\n$$\n\\mathcal{L}(\\tau, \\Delta t, \\lambda) = C_{\\text{tot}}(\\tau, \\Delta t) + \\lambda (E_{\\text{tot}}(\\tau, \\Delta t) - \\varepsilon)\n$$\nSubstituting the explicit forms of the functions, we get:\n$$\n\\mathcal{L}(\\tau, \\Delta t, \\lambda) = T\\alpha\\frac{\\tau}{\\Delta t} + \\lambda(a\\,\\Delta t^{p} + b\\,\\tau^{-1/2} - \\varepsilon)\n$$\nThe optimal solution $(\\tau^{\\star}, \\Delta t^{\\star})$ must satisfy the system of equations derived by setting the partial derivatives of the Lagrangian with respect to $\\tau$, $\\Delta t$, and $\\lambda$ to zero.\n\n1.  Partial derivative with respect to $\\tau$:\n    $$\n    \\frac{\\partial\\mathcal{L}}{\\partial \\tau} = \\frac{T\\alpha}{\\Delta t} + \\lambda b \\left(-\\frac{1}{2}\\tau^{-3/2}\\right) = 0 \\implies \\frac{T\\alpha}{\\Delta t} = \\frac{\\lambda b}{2}\\tau^{-3/2}\n    $$\n2.  Partial derivative with respect to $\\Delta t$:\n    $$\n    \\frac{\\partial\\mathcal{L}}{\\partial \\Delta t} = T\\alpha\\tau \\left(-\\frac{1}{\\Delta t^2}\\right) + \\lambda a p \\Delta t^{p-1} = 0 \\implies \\frac{T\\alpha\\tau}{\\Delta t^2} = \\lambda a p \\Delta t^{p-1}\n    $$\n3.  Partial derivative with respect to $\\lambda$:\n    $$\n    \\frac{\\partial\\mathcal{L}}{\\partial \\lambda} = a\\,\\Delta t^{p} + b\\,\\tau^{-1/2} - \\varepsilon = 0\n    $$\n\nFrom the first two equations, we can solve for the Lagrange multiplier $\\lambda$:\nFrom equation (1):\n$$\n\\lambda = \\frac{2T\\alpha\\tau^{3/2}}{b\\Delta t}\n$$\nFrom equation (2):\n$$\n\\lambda = \\frac{T\\alpha\\tau}{ap\\Delta t^{p+1}}\n$$\n\nEquating these two expressions for $\\lambda$ allows us to find a relationship between $\\tau$ and $\\Delta t$ at the optimum:\n$$\n\\frac{2T\\alpha\\tau^{3/2}}{b\\Delta t} = \\frac{T\\alpha\\tau}{ap\\Delta t^{p+1}}\n$$\nSince $T>0$, $\\alpha>0$, $\\tau>0$, and $\\Delta t>0$, we can divide both sides by $T\\alpha\\tau/\\Delta t$:\n$$\n\\frac{2\\tau^{1/2}}{b} = \\frac{1}{ap\\Delta t^p}\n$$\nRearranging this gives the optimality condition:\n$$\n2ap\\Delta t^p = b\\tau^{-1/2}\n$$\nThis condition represents the optimal balance between the two sources of error. The macro-level error, $a\\Delta t^p$, should be a factor of $1/(2p)$ of the micro-level error, $b\\tau^{-1/2}$.\n\nNow we can use this condition to solve for the optimal values. Substitute $b\\tau^{-1/2} = 2ap\\Delta t^p$ into the constraint equation $a\\,\\Delta t^{p} + b\\,\\tau^{-1/2} = \\varepsilon$:\n$$\na\\,\\Delta t^{p} + 2ap\\Delta t^p = \\varepsilon\n$$\n$$\na\\Delta t^p (1 + 2p) = \\varepsilon\n$$\nSolving for $\\Delta t^p$:\n$$\n(\\Delta t^{\\star})^p = \\frac{\\varepsilon}{a(1+2p)}\n$$\nThus, the optimal macro time step is:\n$$\n\\Delta t^{\\star} = \\left(\\frac{\\varepsilon}{a(1+2p)}\\right)^{1/p}\n$$\nNext, to find the optimal $\\tau$, we substitute $a\\Delta t^p = \\frac{b\\tau^{-1/2}}{2p}$ into the constraint equation:\n$$\n\\frac{b\\tau^{-1/2}}{2p} + b\\tau^{-1/2} = \\varepsilon\n$$\n$$\nb\\tau^{-1/2}\\left(\\frac{1}{2p} + 1\\right) = \\varepsilon\n$$\n$$\nb\\tau^{-1/2}\\left(\\frac{1+2p}{2p}\\right) = \\varepsilon\n$$\nSolving for $\\tau^{-1/2}$:\n$$\n(\\tau^{\\star})^{-1/2} = \\frac{2p\\varepsilon}{b(1+2p)}\n$$\nSquaring both sides and taking the reciprocal gives the optimal micro burst duration:\n$$\n\\tau^{\\star} = \\left(\\frac{b(1+2p)}{2p\\varepsilon}\\right)^2\n$$\nThese expressions for $\\tau^{\\star}$ and $\\Delta t^{\\star}$ provide the optimal parameter choices that minimize the total computational cost for a given error tolerance $\\varepsilon$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\left( \\frac{b(1+2p)}{2p \\varepsilon} \\right)^2 & \\left( \\frac{\\varepsilon}{a(1+2p)} \\right)^{1/p} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "In many complex systems, the precise form of the coarse-grained equations of motion, particularly those involving memory effects, is not known analytically. This hands-on coding exercise delves into a powerful data-driven approach to construct these models from microscopic simulation data. You will implement Prony's method to approximate a memory kernel as a sum of decaying exponentials and use information criteria like the Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) to select the appropriate model complexity, a core task in modern computational science .",
            "id": "3813605",
            "problem": "You are given a micro-scale discrete time series that samples a memory kernel used in temporal bridging schemes for multiscale modeling. In many coarse-grained descriptions, an observable with memory obeys an integro-differential evolution of the form $$\\frac{d q(t)}{d t} = F\\big(q(t)\\big) + \\int_{0}^{t} K(t-s)\\, G\\big(q(s)\\big)\\, ds,$$ where the unknown memory kernel $$K(t)$$ can often be approximated as a sum of decaying exponentials, which enables efficient temporal bridging across scales. Assume that the micro time series provides samples $$s_n \\approx K(n\\,\\Delta t)$$ at equally spaced times $$t_n = n\\,\\Delta t$$ for $$n = 0,1,\\dots,N-1$$, and that $$K(t)$$ admits a representation $$K(t) \\approx \\sum_{j=1}^{p} a_j e^{-b_j t}$$ with unknown amplitudes $$a_j$$ and decay rates $$b_j > 0$$. Equivalently, in discrete time the model is $$s_n \\approx \\sum_{j=1}^{p} a_j r_j^n$$ with $$r_j = e^{-b_j \\Delta t}$$ satisfying $$0 < r_j < 1$$ for positive decays.\n\nStarting from this representation and the definition of discrete-time linear prediction (Prony’s method), pose model order selection as an information-theoretic problem under Gaussian measurement noise. Under the assumption that measurement errors are independent and identically distributed Gaussian with variance $$\\sigma^2$$, and that maximum likelihood estimation is used to fit the parameters, the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC) can be written in terms of the residual sum of squares $$\\mathrm{RSS} = \\sum_{n=0}^{N-1} \\left(s_n - \\hat{s}_n\\right)^2$$ and the number of free parameters $$k$$. For a $$p$$-exponential model in discrete time, treat $$k = 2p$$ (one parameter $$a_j$$ and one parameter $$r_j$$ per exponential).\n\nYour task is to implement, from first principles:\n- A Prony’s method estimator: For each candidate order $$p$$, compute the linear prediction coefficients by least squares, extract the discrete decay factors $$r_j$$ as the roots of the associated characteristic polynomial, and estimate the amplitudes $$a_j$$ using a least-squares fit to the Vandermonde system.\n- An information criterion evaluator: For each $$p$$, compute $$\\mathrm{RSS}$$ and then compute $$\\mathrm{AIC}(p) = N \\ln\\left(\\mathrm{RSS}/N\\right) + 2k$$ and $$\\mathrm{BIC}(p) = N \\ln\\left(\\mathrm{RSS}/N\\right) + k \\ln(N)$$.\n- A model order selector: Choose $$p^\\ast$$ as the $$p$$ that minimizes $$\\mathrm{BIC}(p)$$, breaking ties by preferring the smallest $$p$$.\n\nThe micro time series in this problem is synthetically generated for testability using known ground-truth parameters and additive Gaussian noise. For each test case, generate $$s_n$$ via $$s_n = \\sum_{j=1}^{p_{\\text{true}}} a_j e^{-b_j n \\Delta t} + \\varepsilon_n$$ with $$\\varepsilon_n \\sim \\mathcal{N}(0,\\sigma^2)$$ independent across $$n$$, and then perform the model selection described above.\n\nImplement your program so that it solves the following test suite:\n- Test case $$1$$ (happy path): $$\\Delta t = 0.05$$, $$N = 64$$, $$p_{\\text{true}} = 2$$, amplitudes $$a = [1.0, 0.3]$$, rates $$b = [1.8, 0.6]$$, noise standard deviation $$\\sigma = 0.002$$, maximum model order to consider $$p_{\\max} = 4$$.\n- Test case $$2$$ (boundary condition with near-minimal length and low noise): $$\\Delta t = 0.1$$, $$N = 20$$, $$p_{\\text{true}} = 1$$, amplitudes $$a = [0.8]$$, rates $$b = [1.2]$$, noise standard deviation $$\\sigma = 0.0005$$, maximum model order to consider $$p_{\\max} = 4$$.\n- Test case $$3$$ (edge case with three exponentials and moderate noise): $$\\Delta t = 0.02$$, $$N = 120$$, $$p_{\\text{true}} = 3$$, amplitudes $$a = [0.4, 0.6, 0.2]$$, rates $$b = [2.5, 1.0, 0.15]$$, noise standard deviation $$\\sigma = 0.003$$, maximum model order to consider $$p_{\\max} = 5$$.\n\nYour program should produce a single line of output containing the selected orders $$p^\\ast$$ for each of the three test cases, in order, as a comma-separated list enclosed in square brackets (for example, $$[p_1,p_2,p_3]$$). The outputs are integers. No physical units are involved; all quantities are dimensionless. Ensure reproducibility by using a fixed random seed for noise generation. Your implementation must be self-contained and runnable without any user input and must follow the execution environment constraints specified for the final answer.",
            "solution": "The problem requires the selection of an optimal model order for a signal represented as a sum of decaying exponentials. This task is situated within the context of multiscale modeling, where such exponential sums approximate memory kernels in coarse-grained dynamical equations. The methodology involves two principal components: parameter estimation for a given model order using Prony's method, and model selection using the Bayesian Information Criterion (BIC).\n\nThe underlying model for the discrete time series $$s_n$$ sampled at intervals $$\\Delta t$$ is given by:\n$$s_n = \\sum_{j=1}^{p} a_j r_j^n + \\varepsilon_n, \\quad n=0, 1, \\dots, N-1$$\nwhere $$a_j$$ are the amplitudes, $$r_j = e^{-b_j \\Delta t}$$ are the discrete decay factors corresponding to continuous-time decay rates $$b_j > 0$$, $$p$$ is the model order, and $$\\varepsilon_n$$ is independent and identically distributed Gaussian noise with mean $$0$$ and variance $$\\sigma^2$$. The goal is to determine the most plausible order $$p$$ from the data.\n\n### 1. Parameter Estimation via Prony's Method\n\nFor a fixed candidate order $$p$$, Prony's method provides a procedure to estimate the parameters $$\\{a_j, r_j\\}_{j=1}^p$$. The method cleverly transforms this non-linear estimation problem into a sequence of two linear least-squares problems.\n\n#### Step 1A: Linear Prediction and the Characteristic Polynomial\n\nThe foundation of Prony's method lies in the observation that a noiseless signal composed of $$p$$ exponentials, $$s_n = \\sum_{j=1}^{p} a_j r_j^n$$, is a solution to a $$p$$-th order linear homogeneous recurrence relation. This relation is defined by a characteristic polynomial, $$\\Psi(z)$$, whose roots are precisely the decay factors $$r_j$$:\n$$\\Psi(z) = \\prod_{j=1}^{p} (z - r_j) = z^p + c_1 z^{p-1} + \\dots + c_p$$\nHere, $$\\{c_i\\}_{i=1}^p$$ are the polynomial coefficients. The recurrence relation is $$\\Psi(E)s_n = 0$$, where $$E$$ is the shift operator ($$Es_n = s_{n+1}$$), which explicitly gives:\n$$s_{n+p} + c_1 s_{n+p-1} + \\dots + c_p s_n = 0$$\nThis can be rearranged into a linear prediction form:\n$$s_k = - \\sum_{i=1}^{p} c_i s_{k-i}$$\nfor $$k \\ge p$$. In the presence of noise, this relationship is not exact. We can, however, find the coefficients $$c_i$$ that best satisfy this relationship in a least-squares sense. We set up an overdetermined system of linear equations for the unknown coefficients $$\\mathbf{c} = [c_1, c_2, \\dots, c_p]^T$$. This system is constructed for $$k = p, p+1, \\dots, N-1$$:\n$$\n\\begin{pmatrix}\ns_{p-1} & s_{p-2} & \\dots & s_0 \\\\\ns_p & s_{p-1} & \\dots & s_1 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\ns_{N-2} & s_{N-3} & \\dots & s_{N-p-1}\n\\end{pmatrix}\n\\begin{pmatrix} c_1 \\\\ c_2 \\\\ \\vdots \\\\ c_p \\end{pmatrix}\n\\approx\n-\\begin{pmatrix} s_p \\\\ s_{p+1} \\\\ \\vdots \\\\ s_{N-1} \\end{pmatrix}\n$$\nThis is a standard linear least-squares problem of the form $$A\\mathbf{c} \\approx \\mathbf{b}$$, which is solved for the vector of coefficients $$\\mathbf{c}$$. The number of equations is $$N-p$$, and the number of unknowns is $$p$$. For a well-posed overdetermined system, we require $$N-p \\ge p$$, or $$N \\ge 2p$$.\n\n#### Step 1B: Estimation of Decay Factors ($$r_j$$)\n\nWith the estimated coefficients $$\\hat{c}_i$$, we form the estimated characteristic polynomial:\n$$\\hat{\\Psi}(z) = z^p + \\hat{c}_1 z^{p-1} + \\dots + \\hat{c}_p$$\nThe roots of this polynomial, $$\\hat{r}_1, \\hat{r}_2, \\dots, \\hat{r}_p$$, are the estimates for the discrete decay factors. These roots can be found using a standard numerical polynomial root-finding algorithm.\n\n#### Step 1C: Estimation of Amplitudes ($$a_j$$)\n\nOnce the decay factors $$\\hat{r}_j$$ are determined, the original model $$s_n \\approx \\sum_{j=1}^{p} a_j \\hat{r}_j^n$$ becomes linear in the unknown amplitudes $$a_j$$. We can thus solve for the amplitudes using another linear least-squares fit. We construct the following overdetermined system using all $$N$$ data points ($$n=0, \\dots, N-1$$):\n$$\n\\begin{pmatrix}\n\\hat{r}_1^0 & \\hat{r}_2^0 & \\dots & \\hat{r}_p^0 \\\\\n\\hat{r}_1^1 & \\hat{r}_2^1 & \\dots & \\hat{r}_p^1 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\hat{r}_1^{N-1} & \\hat{r}_2^{N-1} & \\dots & \\hat{r}_p^{N-1}\n\\end{pmatrix}\n\\begin{pmatrix} a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_p \\end{pmatrix}\n\\approx\n\\begin{pmatrix} s_0 \\\\ s_1 \\\\ \\vdots \\\\ s_{N-1} \\end{pmatrix}\n$$\nThe matrix on the left is a Vandermonde matrix, $$V$$. The system $$V\\mathbf{a} \\approx \\mathbf{s}$$ is solved for the amplitude vector $$\\mathbf{a} = [a_1, a_2, \\dots, a_p]^T$$, completing the parameter estimation for the given order $$p$$.\n\n### 2. Model Order Selection\n\nAfter estimating the parameters for a range of candidate orders $$p=1, 2, \\dots, p_{\\max}$$, we must select the most appropriate one. Information criteria provide a principled way to do this by balancing model fidelity (goodness of fit) against model complexity.\n\n#### Step 2A: Residual Sum of Squares (RSS)\n\nFor each model of order $$p$$, we first reconstruct the signal using the estimated parameters:\n$$\\hat{s}_n(p) = \\sum_{j=1}^{p} \\hat{a}_j \\hat{r}_j^n$$\nThe goodness of fit is quantified by the Residual Sum of Squares ($$\\mathrm{RSS}$$):\n$$\\mathrm{RSS}(p) = \\sum_{n=0}^{N-1} \\left(s_n - \\hat{s}_n(p)\\right)^2$$\nA lower $$\\mathrm{RSS}$$ indicates a better fit to the data.\n\n#### Step 2B: Bayesian Information Criterion (BIC)\n\nThe Bayesian Information Criterion (BIC) is defined as:\n$$\\mathrm{BIC}(p) = N \\ln\\left(\\frac{\\mathrm{RSS}(p)}{N}\\right) + k \\ln(N)$$\nwhere $$N$$ is the number of data points and $$k$$ is the number of free parameters in the model. For a $$p$$-exponential model, we have $$p$$ amplitudes ($$a_j$$) and $$p$$ decay factors ($$r_j$$), so the total number of parameters is $$k=2p$$.\n\nThe BIC objective function penalizes model complexity. The first term, $$N \\ln(\\mathrm{RSS}/N)$$, decreases as the model fits the data better (i.e., as $$\\mathrm{RSS}$$ decreases). The second term, $$k \\ln(N)$$, increases with the number of parameters $$p$$. The optimal model order, $$p^\\ast$$, is the one that minimizes the BIC, representing the best trade-off between fit and complexity.\n\n#### Step 2C: Selection Strategy\n\nThe overall procedure is as follows:\n1.  For each candidate order $$p \\in \\{1, 2, \\dots, p_{\\max}\\}$:\n    a.  Apply Prony's method to estimate the parameters $$\\{\\hat{a}_j, \\hat{r}_j\\}_{j=1}^p$$.\n    b.  Reconstruct the signal $$\\hat{s}_n(p)$$ and compute $$\\mathrm{RSS}(p)$$.\n    c.  Calculate $$\\mathrm{BIC}(p)$$ using the number of parameters $$k=2p$$.\n2.  Select the optimal order $$p^\\ast$$ as the $$p$$ that yields the minimum $$\\mathrm{BIC}(p)$$ value. In case of ties, the smallest $$p$$ is chosen.\n\nThis procedure is implemented for each of the provided test cases, starting with the generation of the synthetic time series according to the specified ground-truth parameters and noise level. A fixed random seed ensures the reproducibility of the noise generation and, consequently, the final results.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the model order selection problem for a sum of exponentials\n    using Prony's method and the Bayesian Information Criterion (BIC).\n    \"\"\"\n    # Use a fixed random seed for reproducible noise generation.\n    np.random.seed(0)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'dt': 0.05, 'N': 64, 'p_true': 2, 'a': [1.0, 0.3], 'b': [1.8, 0.6], 'sigma': 0.002, 'p_max': 4},\n        {'dt': 0.1, 'N': 20, 'p_true': 1, 'a': [0.8], 'b': [1.2], 'sigma': 0.0005, 'p_max': 4},\n        {'dt': 0.02, 'N': 120, 'p_true': 3, 'a': [0.4, 0.6, 0.2], 'b': [2.5, 1.0, 0.15], 'sigma': 0.003, 'p_max': 5}\n    ]\n\n    selected_orders = []\n\n    for case in test_cases:\n        dt, N, a_true, b_true, sigma, p_max = \\\n            case['dt'], case['N'], case['a'], case['b'], case['sigma'], case['p_max']\n\n        # 1. Generate the synthetic noisy time series s_n\n        t = np.arange(N) * dt\n        r_true = np.exp(-np.array(b_true) * dt)\n\n        # Generate the clean signal using a Vandermonde matrix approach\n        V_true = np.vander(r_true, N, increasing=True).T\n        s_clean = V_true @ np.array(a_true)\n        \n        # Add independent and identically distributed Gaussian noise\n        noise = np.random.normal(loc=0.0, scale=sigma, size=N)\n        s_n = s_clean + noise\n\n        # 2. Iterate through candidate model orders p to find the best one\n        min_bic = float('inf')\n        p_star = -1\n\n        for p in range(1, p_max + 1):\n            \n            # Prony's method requires N >= 2p for the first least squares problem\n            # to be overdetermined or exactly determined.\n            if N  2 * p:\n                continue\n\n            # --- Prony's Method Implementation ---\n\n            # 2a. Solve for linear prediction coefficients c.\n            # Set up the linear system Ac = b.\n            # A has shape (N-p, p), b has shape (N-p,).\n            A = np.zeros((N - p, p))\n            for i in range(p):\n                A[:, i] = s_n[p - 1 - i : N - 1 - i]\n            b = -s_n[p:N]\n            \n            # Solve for c using linear least squares.\n            c, _, _, _ = np.linalg.lstsq(A, b, rcond=None)\n            \n            # 2b. Find the roots of the characteristic polynomial.\n            # The coefficients are [1, c_1, c_2, ...].\n            poly_coeffs = np.concatenate(([1], c))\n            roots = np.roots(poly_coeffs)\n            \n            # 2c. Solve for the amplitudes a.\n            # Set up the Vandermonde system Va = s_n.\n            V = np.vander(roots, N, increasing=True).T\n            \n            # Solve for a using linear least squares.\n            a, residuals, _, _ = np.linalg.lstsq(V, s_n, rcond=None)\n            \n            # --- Model Selection using BIC ---\n            \n            # Reconstruct the fitted signal to calculate RSS.\n            # Note: The RSS can also be obtained directly from the residuals\n            # of the lstsq fit for 'a'.\n            s_hat = V @ a\n            rss = np.sum((s_n - s_hat)**2)\n            \n            # If the fit is perfect (unlikely with noise), rss could be 0.\n            # log(0) is -inf, so we handle this case to avoid errors.\n            if rss = 1e-12: # Check against a small epsilon\n                bic_val = -float('inf') \n            else:\n                # The number of free parameters k is 2*p (p amplitudes, p decays).\n                k = 2 * p\n                # Bayesian Information Criterion (BIC)\n                bic_val = N * np.log(rss / N) + k * np.log(N)\n            \n            # Update the best model order p* if a lower BIC is found.\n            # The loop structure naturally breaks ties by preferring smaller p.\n            if bic_val  min_bic:\n                min_bic = bic_val\n                p_star = p\n        \n        selected_orders.append(p_star)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, selected_orders))}]\")\n\nsolve()\n```"
        }
    ]
}