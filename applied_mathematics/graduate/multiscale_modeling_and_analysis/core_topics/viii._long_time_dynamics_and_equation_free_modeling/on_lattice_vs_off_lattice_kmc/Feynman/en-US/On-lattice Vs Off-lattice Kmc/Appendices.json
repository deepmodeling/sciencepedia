{
    "hands_on_practices": [
        {
            "introduction": "One of the primary applications of Kinetic Monte Carlo (KMC) simulations is to bridge the gap between discrete, microscopic events and continuous, macroscopic material properties. A classic example is calculating the diffusion coefficient, $D$, which quantifies how quickly particles spread through a medium. This exercise provides a hands-on opportunity to connect theory to practice by deriving the formula for the effective diffusion coefficient in an off-lattice system from first principles, namely the Einstein relation and the statistics of a compound Poisson process. By implementing this derivation, you will gain a fundamental understanding of how macroscopic transport behavior emerges from a catalog of stochastic microscopic jumps .",
            "id": "3790399",
            "problem": "Consider an off-lattice Kinetic Monte Carlo (KMC) model in which a point particle moves in continuous space due to a catalog of possible events. Each event is characterized by a rate $r_i$ in units of $\\mathrm{s}^{-1}$ and a deterministic displacement vector $\\Delta \\mathbf{x}_i \\in \\mathbb{R}^d$ in units of $\\mathrm{m}$. Events occur stochastically as a Continuous-Time Markov Chain (CTMC) with exponentially distributed waiting times and event selection proportional to their rates. The process can be viewed as a compound Poisson process where, in an infinitesimal time interval, the probability of a jump of type $i$ is proportional to $r_i$ and the total rate is $R = \\sum_i r_i$.\n\nStarting from the Einstein relation for diffusion, which connects the mean-squared displacement (MSD) to the diffusion coefficient, and from the definition of a compound Poisson jump process, derive an algorithm, grounded in first principles, to compute the effective scalar diffusion coefficient $D$ in $d$ dimensions, expressed in $\\mathrm{m}^2/\\mathrm{s}$, from the event catalog $\\{(r_i,\\Delta \\mathbf{x}_i)\\}_i$. Your derivation must identify how the MSD accumulates from event-specific displacements and rates under the stochastic dynamics described, and must logically relate this accumulation to $D$ via the appropriate dimensional factors in $d$ dimensions. Do not assume or use any pre-derived shortcut formulas for $D$; instead, build the computation from the base definitions specified above.\n\nImplement a program that, for each provided test case, computes the effective scalar diffusion coefficient $D$ in $\\mathrm{m}^2/\\mathrm{s}$ using your derived algorithm. Angles do not appear in this problem, so no angle unit specification is required. The program must produce results as real numbers (floating-point values).\n\nUse the following test suite (each test case defines a dimension $d$ and a list of events, where each event is a pair $(r_i, \\Delta \\mathbf{x}_i)$ with $r_i$ in $\\mathrm{s}^{-1}$ and $\\Delta \\mathbf{x}_i$ in $\\mathrm{m}$):\n\n- Test Case $1$ (happy path, isotropic in $d=2$): $d=2$; events $[ (\\,5.0,\\,(1e-9,\\,0)\\,),\\ (\\,5.0,\\,(0,\\,1e-9)\\,) ]$.\n- Test Case $2$ (anisotropic in $d=3$ with mixed magnitudes and rates): $d=3$; events $[ (\\,1e3,\\,(1e-10,\\,0,\\,0)\\,),\\ (\\,2e2,\\,(0,\\,2e-10,\\,0)\\,),\\ (\\,5e1,\\,(0,\\,0,\\,1e-9)\\,) ]$.\n- Test Case $3$ (boundary case, zero mobility due to zero displacements/rates): $d=2$; events $[ (\\,1e4,\\,(0,\\,0)\\,),\\ (\\,0,\\,(1e-9,\\,-1e-9)\\,) ]$.\n- Test Case $4$ (competition between rare long jumps and frequent short jumps in $d=2$): $d=2$; events $[ (\\,1.0,\\,(1e-6,\\,1e-6)\\,),\\ (\\,1e5,\\,(1e-9,\\,0)\\,) ]$.\n- Test Case $5$ (off-lattice non-orthogonal displacements in $d=3$): $d=3$; events $[ (\\,10,\\,(3e-10,\\,4e-10,\\,0)\\,),\\ (\\,20,\\,( -3e-10,\\,0,\\,4e-10)\\,),\\ (\\,30,\\,(0,\\,-4e-10,\\,3e-10)\\,) ]$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases above (for example, $[D_1,D_2,D_3,D_4,D_5]$), where each $D_k$ is the computed diffusion coefficient in $\\mathrm{m}^2/\\mathrm{s}$ for test case $k$.",
            "solution": "The objective is to derive an algorithm for the effective scalar diffusion coefficient $D$ for a particle undergoing a stochastic jump process in a $d$-dimensional continuous space, and then to implement this algorithm. The derivation must proceed from first principles, specifically the Einstein relation and the definition of the underlying compound Poisson process.\n\n### Derivation from First Principles\n\nThe foundation of our analysis is the Einstein relation for diffusion, which connects the Mean-Squared Displacement (MSD) of a particle to the diffusion coefficient $D$ in the long-time limit. In $d$ spatial dimensions, this relation is:\n$$\n\\langle \\|\\mathbf{x}(t) - \\mathbf{x}(0)\\|^2 \\rangle = 2 d D t\n$$\nwhere $\\mathbf{x}(t)$ is the particle's position at time $t$, and $\\langle \\cdot \\rangle$ denotes an ensemble average over all possible stochastic trajectories. The left-hand side is the MSD. From this, the diffusion coefficient can be defined as:\n$$\nD = \\lim_{t \\to \\infty} \\frac{\\langle \\|\\mathbf{x}(t) - \\mathbf{x}(0)\\|^2 \\rangle}{2 d t}\n$$\nThe problem specifies that the particle's movement is governed by a catalog of distinct event types, indexed by $i$. Each event type $i$ is characterized by a constant rate $r_i$ and a deterministic displacement vector $\\Delta \\mathbf{x}_i$. The occurrences of events of type $i$ constitute a Poisson process with rate $r_i$. The overall process is a compound Poisson process, meaning the different event types occur independently.\n\nLet us model the particle's position over time. Without loss of generality, we set the initial position to the origin, $\\mathbf{x}(0) = \\mathbf{0}$. The position at a later time $t$ is the vector sum of all displacement events that have occurred up to that time:\n$$\n\\mathbf{x}(t) = \\sum_i n_i(t) \\Delta \\mathbf{x}_i\n$$\nwhere $n_i(t)$ is the number of times event $i$ has occurred in the time interval $[0, t]$. Since each event type follows a Poisson process, $n_i(t)$ is a random variable drawn from a Poisson distribution with mean and variance given by:\n$$\n\\langle n_i(t) \\rangle = \\mathrm{Var}(n_i(t)) = r_i t\n$$\nFurthermore, due to the independence of the event processes, the covariance between the counts of different event types is zero:\n$$\n\\mathrm{Cov}(n_i(t), n_j(t)) = \\langle (n_i(t) - \\langle n_i(t) \\rangle)(n_j(t) - \\langle n_j(t) \\rangle) \\rangle = 0 \\quad \\text{for } i \\neq j\n$$\nCombining these, we can write $\\mathrm{Cov}(n_i(t), n_j(t)) = \\delta_{ij} r_i t$, where $\\delta_{ij}$ is the Kronecker delta.\n\nThe MSD in the Einstein relation refers to the variance of the particle's position, which measures the spread of particle positions around the mean position. Let's first calculate the mean position $\\langle \\mathbf{x}(t) \\rangle$:\n$$\n\\langle \\mathbf{x}(t) \\rangle = \\left\\langle \\sum_i n_i(t) \\Delta \\mathbf{x}_i \\right\\rangle = \\sum_i \\langle n_i(t) \\rangle \\Delta \\mathbf{x}_i = \\sum_i (r_i t) \\Delta \\mathbf{x}_i = t \\left( \\sum_i r_i \\Delta \\mathbf{x}_i \\right)\n$$\nThe term $\\mathbf{v}_{\\text{drift}} = \\sum_i r_i \\Delta \\mathbf{x}_i$ represents a constant drift velocity. The diffusive motion is the random fluctuation around this mean drift. The correct MSD for diffusion is therefore the variance of the position:\n$$\n\\mathrm{MSD}(t) = \\langle \\|\\mathbf{x}(t) - \\langle \\mathbf{x}(t) \\rangle\\|^2 \\rangle\n$$\nLet us now compute this variance. We express it as the trace of the covariance matrix of $\\mathbf{x}(t)$:\n$$\n\\mathrm{MSD}(t) = \\mathrm{Tr}(\\mathrm{Cov}(\\mathbf{x}(t))) = \\mathrm{Tr}\\left( \\left\\langle (\\mathbf{x}(t) - \\langle \\mathbf{x}(t) \\rangle)(\\mathbf{x}(t) - \\langle \\mathbf{x}(t) \\rangle)^T \\right\\rangle \\right)\n$$\nSubstituting the expression for $\\mathbf{x}(t)$:\n$$\n\\mathbf{x}(t) - \\langle \\mathbf{x}(t) \\rangle = \\sum_i n_i(t) \\Delta \\mathbf{x}_i - \\sum_i \\langle n_i(t) \\rangle \\Delta \\mathbf{x}_i = \\sum_i (n_i(t) - \\langle n_i(t) \\rangle) \\Delta \\mathbf{x}_i\n$$\nThe covariance matrix is then:\n$$\n\\mathrm{Cov}(\\mathbf{x}(t)) = \\left\\langle \\left( \\sum_i (n_i(t) - \\langle n_i(t) \\rangle) \\Delta \\mathbf{x}_i \\right) \\left( \\sum_j (n_j(t) - \\langle n_j(t) \\rangle) \\Delta \\mathbf{x}_j \\right)^T \\right\\rangle\n$$\n$$\n= \\sum_{i,j} \\Delta \\mathbf{x}_i \\Delta \\mathbf{x}_j^T \\left\\langle (n_i(t) - \\langle n_i(t) \\rangle)(n_j(t) - \\langle n_j(t) \\rangle) \\right\\rangle\n$$\nThe expectation term is precisely the covariance $\\mathrm{Cov}(n_i(t), n_j(t)) = \\delta_{ij} r_i t$. The double summation collapses to a single summation over $i$:\n$$\n\\mathrm{Cov}(\\mathbf{x}(t)) = \\sum_i \\Delta \\mathbf{x}_i \\Delta \\mathbf{x}_i^T (r_i t) = t \\sum_i r_i (\\Delta \\mathbf{x}_i \\otimes \\Delta \\mathbf{x}_i)\n$$\nwhere $\\otimes$ denotes the outer product. The MSD is the trace of this matrix:\n$$\n\\mathrm{MSD}(t) = \\mathrm{Tr}\\left( t \\sum_i r_i (\\Delta \\mathbf{x}_i \\otimes \\Delta \\mathbf{x}_i) \\right) = t \\sum_i r_i \\mathrm{Tr}(\\Delta \\mathbf{x}_i \\otimes \\Delta \\mathbf{x}_i)\n$$\nThe trace of the outer product of a vector with itself, $\\mathrm{Tr}(\\mathbf{v} \\otimes \\mathbf{v})$, is equal to the squared magnitude (or dot product) of the vector, $\\|\\mathbf{v}\\|^2$. Thus:\n$$\n\\mathrm{MSD}(t) = t \\sum_i r_i \\|\\Delta \\mathbf{x}_i\\|^2\n$$\nThis result shows that the MSD grows linearly with time, which is the hallmark of a diffusive process. The rate of growth is determined by the sum of each event's rate multiplied by its squared jump distance.\n\nFinally, we substitute this derived expression for the MSD back into the definition of the diffusion coefficient:\n$$\nD = \\lim_{t \\to \\infty} \\frac{t \\sum_i r_i \\|\\Delta \\mathbf{x}_i\\|^2}{2 d t}\n$$\nThe factor of $t$ cancels, yielding the final formula:\n$$\nD = \\frac{1}{2d} \\sum_i r_i \\|\\Delta \\mathbf{x}_i\\|^2\n$$\n\n### Algorithm\nBased on this derivation, the algorithm to compute the scalar diffusion coefficient $D$ is as follows:\n1.  Initialize a summation variable, $S$, to $0$.\n2.  For each event $i$ in the provided catalog $\\{(r_i, \\Delta \\mathbf{x}_i)\\}_i$:\n    a.  Take the displacement vector $\\Delta \\mathbf{x}_i$.\n    b.  Compute its squared Euclidean norm (magnitude): $\\|\\Delta \\mathbf{x}_i\\|^2 = \\Delta \\mathbf{x}_i \\cdot \\Delta \\mathbf{x}_i = \\sum_{j=1}^{d} (\\Delta x_{i,j})^2$.\n    c.  Multiply this squared norm by the event rate $r_i$.\n    d.  Add the resulting product $r_i \\|\\Delta \\mathbf{x}_i\\|^2$ to the summation variable $S$.\n3.  After iterating through all events, compute the diffusion coefficient by dividing the total sum $S$ by the factor $2d$, where $d$ is the dimensionality of the space.\n\nThis algorithm directly implements the derived formula and is grounded in the specified first principles.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the effective diffusion coefficient based on a catalog of\n    kMC events.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    # Each test case is a tuple: (dimension, list_of_events)\n    # Each event is a tuple: (rate, displacement_vector)\n    test_cases = [\n        # Test Case 1 (happy path, isotropic in d=2)\n        (2, [\n            (5.0, np.array([1e-9, 0.0])),\n            (5.0, np.array([0.0, 1e-9]))\n        ]),\n        \n        # Test Case 2 (anisotropic in d=3 with mixed magnitudes and rates)\n        (3, [\n            (1e3, np.array([1e-10, 0.0, 0.0])),\n            (2e2, np.array([0.0, 2e-10, 0.0])),\n            (5e1, np.array([0.0, 0.0, 1e-9]))\n        ]),\n\n        # Test Case 3 (boundary case, zero mobility)\n        (2, [\n            (1e4, np.array([0.0, 0.0])),\n            (0.0, np.array([1e-9, -1e-9]))\n        ]),\n\n        # Test Case 4 (competition between rare long jumps and frequent short jumps in d=2)\n        (2, [\n            (1.0, np.array([1e-6, 1e-6])),\n            (1e5, np.array([1e-9, 0.0]))\n        ]),\n\n        # Test Case 5 (off-lattice non-orthogonal displacements in d=3)\n        (3, [\n            (10.0, np.array([3e-10, 4e-10, 0.0])),\n            (20.0, np.array([-3e-10, 0.0, 4e-10])),\n            (30.0, np.array([0.0, -4e-10, 3e-10]))\n        ])\n    ]\n\n    results = []\n    \n    # Process each test case using the derived formula.\n    # D = (1 / (2*d)) * sum(r_i * ||dx_i||^2)\n    for d, events in test_cases:\n        # Initialize the sum of rate-weighted squared displacements\n        sum_r_dx_sq = 0.0\n        \n        for rate, dx_vec in events:\n            # Calculate the squared magnitude (norm) of the displacement vector\n            # ||dx_i||^2 is equivalent to the dot product of the vector with itself\n            squared_magnitude = np.dot(dx_vec, dx_vec)\n            \n            # Add the contribution of this event to the sum\n            sum_r_dx_sq += rate * squared_magnitude\n\n        # Handle the case where the denominator 2*d could be zero, though\n        # d is always >= 1 in this problem's context.\n        if d > 0:\n            diffusion_coefficient = sum_r_dx_sq / (2 * d)\n        else:\n            # Physically, d=0 is ill-defined for diffusion.\n            # For completeness, return 0.\n            diffusion_coefficient = 0.0\n\n        results.append(diffusion_coefficient)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "In computational science, a model is only as reliable as its implementation. This practice emphasizes the critical importance of verification by guiding you through the design of a unit test suite for a KMC simulation engine. You will develop tests to confirm that the core mechanics of the simulation—such as event selection probabilities, handling of boundary conditions, and adherence to the principle of detailed balance—are all functioning correctly. Building these tests  will not only sharpen your coding skills but also deepen your understanding of the theoretical pillars that ensure a KMC simulation is physically meaningful, whether it's on-lattice or off-lattice.",
            "id": "3790392",
            "problem": "Design and implement a program that constructs a suite of unit tests for Kinetic Monte Carlo (KMC) implementations to verify three core properties across both on-lattice and off-lattice models: rate normalization, detailed balance, and event selection correctness. Kinetic Monte Carlo (KMC) is modeled as a continuous-time Markov process with a discrete event list and an associated rate vector. The fundamental base for the derivation is the theory of continuous-time Markov chains and equilibrium statistical mechanics.\n\nBegin from the following fundamental definitions and facts. A continuous-time Markov chain is defined by a set of states and a rate matrix $\\mathbf{R}$, where $R_{i \\to j}$ is the transition rate from state $i$ to state $j$. For a single KMC step from a given state, one considers a set of candidate events with rates $\\{r_i\\}$. The total rate is $R = \\sum_i r_i$. The probability of selecting event $i$ is $p_i = r_i / R$ for $R > 0$, and the waiting time $\\Delta t$ to the next event is exponentially distributed with parameter $R$. In equilibrium statistical mechanics, the detailed balance condition is the identity $\\pi_i R_{i \\to j} = \\pi_j R_{j \\to i}$ for all pairs of states $\\{i,j\\}$, where $\\pi_i$ is the stationary probability of state $i$. For a system with energies $\\{E_i\\}$ measured in units of thermal energy $k_{\\mathrm{B}}T$, the Boltzmann distribution is $\\pi_i = \\exp(-E_i) / Z$ where $Z = \\sum_k \\exp(-E_k)$. A sufficient construction to satisfy detailed balance is to use symmetric prefactors and choose $R_{i \\to j} = \\nu \\exp(-(E_j - E_i)/2)$ for a constant attempt frequency $\\nu > 0$.\n\nYour program must implement unit tests to check the following properties:\n- Rate normalization: Given a finite rate vector $\\mathbf{r}$ with nonnegative entries $r_i \\ge 0$ and total rate $R = \\sum_i r_i$, the normalized probabilities satisfy $\\sum_i p_i = 1$ for $R > 0$. The program must empirically estimate event selection probabilities by random sampling and compare them against $p_i$ within a specified tolerance.\n- Detailed balance: For systems with specified energies $\\{E_i\\}$ (in units of $k_{\\mathrm{B}}T$), and rates constructed to satisfy detailed balance, the stationary distribution of the continuous-time Markov chain equals the Boltzmann distribution. The stationary distribution $\\boldsymbol{\\pi}$ satisfies $\\boldsymbol{\\pi}^\\top \\mathbf{G} = \\mathbf{0}$ and $\\sum_i \\pi_i = 1$, where $\\mathbf{G}$ is the generator matrix with off-diagonal entries $G_{i j} = R_{i \\to j}$ for $i \\ne j$ and diagonal entries $G_{i i} = -\\sum_{j \\ne i} R_{i \\to j}$.\n- Event selection correctness: The event sampling procedure must never select events with zero rate, must handle degenerate cases where only one event is non-zero, and must correctly handle boundary conditions where the total rate $R = 0$ (no event can be selected).\n\nAddress both on-lattice and off-lattice contexts. In an on-lattice KMC model, states are discrete lattice sites and allowed transitions are specified by an adjacency structure. In an off-lattice KMC model, the event list is constructed from geometric relations (for example, distances between entities), and rates may depend on continuous geometric variables; however, event selection is still over a discrete set of events with a rate vector. For the off-lattice test in this problem, use events whose rates depend exponentially on distances, $r_i = k_0 \\exp(-\\alpha d_i)$, where $k_0 > 0$, $\\alpha > 0$, and $d_i \\ge 0$ are dimensionless.\n\nImplement the following test suite. Each test must produce a boolean result indicating pass ($\\mathrm{True}$) or fail ($\\mathrm{False}$). Random sampling must use a fixed seed per test to ensure reproducibility. All energies, rates, distances, counts, and tolerances specified are dimensionless. No physical units are involved.\n\n- Test $1$ (on-lattice rate normalization and event selection): Use the rate vector $\\mathbf{r} = (1.0, 2.0, 3.0, 0.0)$, sampling count $N = 100000$, random seed $s = 12345$, normalization tolerance $\\varepsilon_{\\mathrm{norm}} = 10^{-12}$, and frequency deviation tolerance $\\varepsilon_{\\mathrm{freq}} = 5 \\times 10^{-3}$. The test passes if $\\left|\\sum_i p_i - 1\\right|  \\varepsilon_{\\mathrm{norm}}$ and the maximum absolute deviation between empirical frequencies and $p_i$ is less than $\\varepsilon_{\\mathrm{freq}}$.\n- Test $2$ (off-lattice rate normalization and event selection): Use distances $\\mathbf{d} = (0.1, 0.2, 1.0)$, $k_0 = 1.0$, $\\alpha = 2.5$, sampling count $N = 200000$, random seed $s = 54321$, normalization tolerance $\\varepsilon_{\\mathrm{norm}} = 10^{-12}$, and frequency deviation tolerance $\\varepsilon_{\\mathrm{freq}} = 5 \\times 10^{-3}$. The test passes under the same criteria as Test $1$.\n- Test $3$ (two-state detailed balance): Use energies $\\mathbf{E} = (0.0, 1.0)$ in units of $k_{\\mathrm{B}}T$, attempt frequency $\\nu = 1.0$, full adjacency between the two states, and detailed-balance rates $R_{i \\to j} = \\nu \\exp(-(E_j - E_i)/2)$. Construct the generator $\\mathbf{G}$, compute the stationary distribution $\\boldsymbol{\\pi}$ by solving $\\boldsymbol{\\pi}^\\top \\mathbf{G} = \\mathbf{0}$ with $\\sum_i \\pi_i = 1$, and compare to the Boltzmann distribution. The test passes if the maximum absolute difference between $\\boldsymbol{\\pi}$ and the Boltzmann distribution is less than $\\varepsilon_{\\mathrm{db}} = 10^{-8}$.\n- Test $4$ (three-state ring detailed balance on-lattice): Use energies $\\mathbf{E} = (0.0, 0.5, 1.0)$ in units of $k_{\\mathrm{B}}T$, attempt frequency $\\nu = 2.0$. Use a ring adjacency where each state connects to its two neighbors (for three states, this connects each state to the other two). Construct rates $R_{i \\to j} = \\nu \\exp(-(E_j - E_i)/2)$, build $\\mathbf{G}$, compute $\\boldsymbol{\\pi}$, and compare to the Boltzmann distribution. The test passes if the maximum absolute difference is less than $\\varepsilon_{\\mathrm{db}} = 10^{-8}$.\n- Test $5$ (boundary case $R = 0$): Use the rate vector $\\mathbf{r} = (0.0, 0.0, 0.0)$. The test passes if the event selection procedure correctly returns \"no event\" (i.e., does not select any index).\n- Test $6$ (deterministic event selection): Use the rate vector $\\mathbf{r} = (0.0, 0.0, 5.0)$, sampling count $N = 10000$, random seed $s = 111$. The test passes if the only selected event index is the one with the non-zero rate for all samples.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result1,result2,result3]$), where each $resultk$ is a boolean indicating the pass/fail outcome of Test $k$. No additional text may be printed.",
            "solution": "The principles underpinning this problem are fundamental to Kinetic Monte Carlo (KMC) simulations, which model the time evolution of a system as a continuous-time, discrete-state Markov process. The design of the unit tests focuses on verifying the core machinery of a KMC implementation against these principles.\n\nA KMC simulation step involves two parts: selecting which event will occur next and determining how long the system waits for this event to happen. This problem focuses on the correctness of the event selection mechanism. Given a set of $N$ possible events from the current state, each with a rate $r_i \\ge 0$, the total rate is $R = \\sum_{i=1}^{N} r_i$. According to the theory of Markov processes, the probability $p_i$ of selecting event $i$ is proportional to its rate, given by $p_i = r_i / R$. The waiting time $\\Delta t$ for the selected event to occur is a random variable drawn from an exponential distribution with parameter $R$. The unit tests will validate three critical properties related to this process.\n\n1.  **Rate Normalization and Event Selection Correctness**:\n    The theoretical probabilities $p_i = r_i/R$ must sum to unity, i.e., $\\sum_{i} p_i = 1$, provided the total rate $R > 0$. An event selection algorithm must respect these probabilities. A standard and robust method for selecting an event, which will be implemented for the tests, is the \"first-passage\" or \"cumulative rate\" method. First, a uniform random number $u$ is drawn from the interval $[0, R)$. Then, the event index $k$ is chosen such that $\\sum_{i=0}^{k-1} r_i \\le u  \\sum_{i=0}^{k} r_i$. This ensures that events with rate $r_i = 0$ are never chosen (as they do not increase the cumulative sum) and that events with higher rates cover a larger portion of the interval $[0, R)$, making them proportionally more likely to be selected. The tests verify this by repeatedly sampling events for a given rate vector $\\mathbf{r}$ and comparing the empirical frequencies of selection against the theoretical probabilities $p_i$. The law of large numbers dictates that for a large number of samples $N_s$, the empirical frequency should converge to $p_i$. The tests also evaluate boundary conditions: the case where all rates are zero ($R=0$), where no event should be selected, and the deterministic case in which only one event has a non-zero rate.\n\n2.  **Detailed Balance**:\n    For systems in thermodynamic equilibrium, the principle of detailed balance must hold. This principle states that at equilibrium, the rate of every microscopic process is balanced by the rate of its reverse process. For a Markov process describing transitions between states $i$ and $j$, this is expressed as $\\pi_i R_{i \\to j} = \\pi_j R_{j \\to i}$, where $\\pi_k$ is the stationary (equilibrium) probability of being in state $k$, and $R_{i \\to j}$ is the transition rate from $i$ to $j$. A system whose dynamics obey detailed balance will eventually relax to a stationary distribution that is consistent with the equilibrium distribution from statistical mechanics. For a system with discrete energy levels $E_i$ (in units of thermal energy $k_{\\mathrm{B}}T$), this equilibrium distribution is the Boltzmann distribution, $\\pi_{i, \\text{B}} = \\exp(-E_i) / Z$, where $Z = \\sum_k \\exp(-E_k)$ is the partition function.\n    The problem provides a specific rate form, $R_{i \\to j} = \\nu \\exp(-(E_j - E_i)/2)$, which is a sufficient (but not necessary) condition to satisfy detailed balance with respect to the Boltzmann distribution. The tests will verify this property by:\n    a. Constructing the generator matrix $\\mathbf{G}$ for the continuous-time Markov chain. The off-diagonal elements are the transition rates, $G_{ij} = R_{i \\to j}$ for $i \\ne j$, and the diagonal elements are the negative total exit rates, $G_{ii} = -\\sum_{j \\ne i} R_{i \\to j}$.\n    b. Numerically solving for the stationary distribution $\\boldsymbol{\\pi}$, which is the unique normalized vector satisfying the linear system $\\boldsymbol{\\pi}^\\top \\mathbf{G} = \\mathbf{0}$ (equivalent to $\\mathbf{G}^\\top \\boldsymbol{\\pi} = \\mathbf{0}$) subject to the normalization constraint $\\sum_i \\pi_i = 1$. This is achieved by solving a system of linear equations derived from these conditions.\n    c. Comparing the numerically obtained stationary distribution $\\boldsymbol{\\pi}$ with the analytically derived Boltzmann distribution $\\boldsymbol{\\pi}_{\\text{B}}$.\n\nThe test suite is structured to examine these properties in both on-lattice (where state transitions are defined by a fixed adjacency) and off-lattice contexts (where event rates may depend on continuous variables like distance).\n\n**Test Implementations**\n\n**Tests $1$ and $2$ (Rate Normalization and Selection)**:\nThese tests will use an implementation of the cumulative rate selection algorithm. For a given rate vector $\\mathbf{r}$, first the theoretical probabilities $\\mathbf{p} = \\mathbf{r} / \\sum \\mathbf{r}$ are computed. The validity of the normalization, $|\\sum p_i - 1|  \\varepsilon_{\\mathrm{norm}}$, is checked. Then, a large number of events ($N=100000$ for Test $1$, $N=200000$ for Test $2$) are sampled using a fixed random seed for reproducibility. The empirical frequencies of each event are computed and compared to the theoretical probabilities. The test passes if the maximum absolute deviation is within the specified tolerance $\\varepsilon_{\\mathrm{freq}}$. For Test $1$, the rates are given directly. for Test $2$, the rates $r_i$ are first calculated from the off-lattice model $r_i = k_0 \\exp(-\\alpha d_i)$ using the provided parameters.\n\n**Tests $3$ and $4$ (Detailed Balance)**:\nThese tests verify that the specified rate construction yields a stationary distribution consistent with the Boltzmann distribution. For the given energies $\\mathbf{E}$ and attempt frequency $\\nu$, the rate matrix $\\mathbf{R}$ is constructed: $R_{i \\to j} = \\nu \\exp(-(E_j - E_i)/2)$ for all specified allowed transitions $i \\to j$. From $\\mathbf{R}$, the generator matrix $\\mathbf{G}$ is built. The stationary distribution $\\boldsymbol{\\pi}$ is found by solving the linear system formed by replacing one equation of $\\mathbf{G}^\\top \\boldsymbol{\\pi} = \\mathbf{0}$ with the normalization condition $\\sum_i \\pi_i = 1$. The resulting $\\boldsymbol{\\pi}$ is then compared to the theoretical Boltzmann distribution, $\\boldsymbol{\\pi}_{\\text{B}} = \\exp(-\\mathbf{E})/\\sum\\exp(-\\mathbf{E})$. The test passes if the maximum absolute difference between the components of $\\boldsymbol{\\pi}$ and $\\boldsymbol{\\pi}_{\\text{B}}$ is less than $\\varepsilon_{\\mathrm{db}} = 10^{-8}$. Test $3$ uses a $2$-state system, and Test $4$ uses a $3$-state system.\n\n**Test $5$ (Boundary Case $R=0$)**:\nThis test checks the handling of a state with no possible exit events. The rate vector is $\\mathbf{r} = (0.0, 0.0, 0.0)$, so the total rate is $R=0$. A correct KMC implementation should recognize this as a terminal state and select no event. The test passes if the selection function returns an indicator for \"no event\".\n\n**Test $6$ (Deterministic Selection)**:\nThis test verifies the correct behavior when only one event has a non-zero rate, e.g., $\\mathbf{r} = (0.0, 0.0, 5.0)$. In this case, the selection must be deterministic: the event with the positive rate must always be chosen. The test performs $N=10000$ selections and passes only if the correct event index is chosen every time.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the kMC unit test suite and print results.\n    \"\"\"\n\n    def select_event(rates: np.ndarray, rng: np.random.Generator):\n        \"\"\"\n        Selects an event based on a list of rates using the cumulative rate method.\n        \n        Args:\n            rates: A numpy array of event rates.\n            rng: A numpy random number generator instance.\n        \n        Returns:\n            The index of the selected event, or None if the total rate is zero.\n        \"\"\"\n        total_rate = np.sum(rates)\n        \n        # Handle the boundary case where no events are possible.\n        if total_rate = 1e-15: # Use a small tolerance for floating point comparison\n            return None\n\n        # Generate a random number u in [0, total_rate).\n        random_draw = rng.random() * total_rate\n        \n        # Use searchsorted for a robust and efficient implementation of the\n        # first-passage algorithm.\n        cumulative_rates = np.cumsum(rates)\n        \n        # Find the index of the first cumulative rate greater than the random draw.\n        selected_index = np.searchsorted(cumulative_rates, random_draw, side='right')\n        \n        return int(selected_index)\n\n    def test_1():\n        \"\"\"\n        Test 1: On-lattice rate normalization and event selection.\n        \"\"\"\n        rates = np.array([1.0, 2.0, 3.0, 0.0])\n        n_samples = 100000\n        seed = 12345\n        eps_norm = 1e-12\n        eps_freq = 5e-3\n        \n        total_rate = np.sum(rates)\n        if total_rate = 1e-15: return False\n\n        p_theoretical = rates / total_rate\n        \n        # 1. Check rate normalization\n        norm_check = abs(np.sum(p_theoretical) - 1.0)  eps_norm\n        if not norm_check: return False\n        \n        # 2. Empirically estimate probabilities\n        rng = np.random.default_rng(seed)\n        counts = np.zeros_like(rates, dtype=int)\n        for _ in range(n_samples):\n            idx = select_event(rates, rng)\n            if idx is not None:\n                counts[idx] += 1\n        \n        # Check that the event with rate 0 was never selected\n        if counts[3] != 0: return False\n\n        p_empirical = counts / n_samples\n        \n        # 3. Check frequency deviation\n        freq_check = np.max(np.abs(p_empirical - p_theoretical))  eps_freq\n        \n        return freq_check\n\n    def test_2():\n        \"\"\"\n        Test 2: Off-lattice rate normalization and event selection.\n        \"\"\"\n        distances = np.array([0.1, 0.2, 1.0])\n        k0 = 1.0\n        alpha = 2.5\n        n_samples = 200000\n        seed = 54321\n        eps_norm = 1e-12\n        eps_freq = 5e-3\n        \n        rates = k0 * np.exp(-alpha * distances)\n        total_rate = np.sum(rates)\n        if total_rate = 1e-15: return False\n        \n        p_theoretical = rates / total_rate\n        \n        # 1. Check rate normalization\n        norm_check = abs(np.sum(p_theoretical) - 1.0)  eps_norm\n        if not norm_check: return False\n        \n        # 2. Empirically estimate probabilities\n        rng = np.random.default_rng(seed)\n        counts = np.zeros_like(rates, dtype=int)\n        for _ in range(n_samples):\n            idx = select_event(rates, rng)\n            if idx is not None:\n                 counts[idx] += 1\n        \n        p_empirical = counts / n_samples\n        \n        # 3. Check frequency deviation\n        freq_check = np.max(np.abs(p_empirical - p_theoretical))  eps_freq\n        \n        return freq_check\n\n    def test_3():\n        \"\"\"\n        Test 3: Two-state detailed balance.\n        \"\"\"\n        energies = np.array([0.0, 1.0])\n        nu = 1.0\n        eps_db = 1e-8\n        n_states = len(energies)\n        \n        # 1. Calculate theoretical Boltzmann distribution\n        pi_boltzmann = np.exp(-energies) / np.sum(np.exp(-energies))\n        \n        # 2. Construct rate and generator matrices\n        rate_matrix = np.zeros((n_states, n_states))\n        for i in range(n_states):\n            for j in range(n_states):\n                if i != j:\n                    rate_matrix[i, j] = nu * np.exp(-(energies[j] - energies[i]) / 2.0)\n        \n        generator_matrix = rate_matrix - np.diag(np.sum(rate_matrix, axis=1))\n\n        # 3. Solve for the stationary distribution\n        # System is G.T @ pi = 0, with sum(pi) = 1\n        A = generator_matrix.T\n        A[-1, :] = 1.0 # Replace last row with normalization condition\n        b = np.zeros(n_states)\n        b[-1] = 1.0\n        \n        try:\n            pi_stationary = np.linalg.solve(A, b)\n        except np.linalg.LinAlgError:\n            return False\n\n        # 4. Compare distributions\n        return np.max(np.abs(pi_stationary - pi_boltzmann))  eps_db\n\n    def test_4():\n        \"\"\"\n        Test 4: Three-state ring detailed balance on-lattice.\n        \"\"\"\n        energies = np.array([0.0, 0.5, 1.0])\n        nu = 2.0\n        eps_db = 1e-8\n        n_states = len(energies)\n        \n        # 1. Calculate theoretical Boltzmann distribution\n        pi_boltzmann = np.exp(-energies) / np.sum(np.exp(-energies))\n        \n        # 2. Construct rate and generator matrices\n        # For 3 states, a ring is a fully connected graph.\n        rate_matrix = np.zeros((n_states, n_states))\n        for i in range(n_states):\n            for j in range(n_states):\n                if i != j:\n                    rate_matrix[i, j] = nu * np.exp(-(energies[j] - energies[i]) / 2.0)\n\n        generator_matrix = rate_matrix - np.diag(np.sum(rate_matrix, axis=1))\n\n        # 3. Solve for the stationary distribution\n        A = generator_matrix.T\n        A[-1, :] = 1.0\n        b = np.zeros(n_states)\n        b[-1] = 1.0\n        \n        try:\n            pi_stationary = np.linalg.solve(A, b)\n        except np.linalg.LinAlgError:\n            return False\n\n        # 4. Compare distributions\n        return np.max(np.abs(pi_stationary - pi_boltzmann))  eps_db\n\n    def test_5():\n        \"\"\"\n        Test 5: Boundary case R = 0.\n        \"\"\"\n        rates = np.array([0.0, 0.0, 0.0])\n        rng = np.random.default_rng() # seed doesn't matter\n        \n        result = select_event(rates, rng)\n        \n        return result is None\n\n    def test_6():\n        \"\"\"\n        Test 6: Deterministic event selection.\n        \"\"\"\n        rates = np.array([0.0, 0.0, 5.0])\n        n_samples = 10000\n        seed = 111\n        \n        rng = np.random.default_rng(seed)\n        correct_index = 2\n        \n        for _ in range(n_samples):\n            idx = select_event(rates, rng)\n            if idx != correct_index:\n                return False\n                \n        return True\n\n    results = [\n        test_1(),\n        test_2(),\n        test_3(),\n        test_4(),\n        test_5(),\n        test_6(),\n    ]\n    \n    # Format and print the final output as specified\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While on-lattice KMC models are strictly Markovian by construction, more complex off-lattice or coarse-grained systems can exhibit \"memory,\" where the future evolution depends on the system's history. Such non-Markovian behavior manifests as non-exponential waiting time distributions between events, violating a core assumption of standard KMC. This advanced practice introduces a powerful statistical method, based on the time-rescaling theorem, to analyze waiting time data from a simulation and test for the Markovian property. By applying this diagnostic test , you will learn how to validate the underlying assumptions of your model and distinguish between true Markovian dynamics and systems that require more sophisticated non-Markovian modeling approaches.",
            "id": "3790261",
            "problem": "You are tasked with designing and implementing a diagnostic test, grounded in first principles of stochastic processes, to distinguish Markovian on-lattice kinetic Monte Carlo from coarse-grained off-lattice dynamics that exhibit memory via non-exponential waiting-time statistics. In Markovian on-lattice kinetic Monte Carlo, event waiting times are independent and identically distributed exponential random variables due to the memoryless property. In contrast, coarse-graining, unresolved degrees of freedom, or collective off-lattice moves can induce effective memory kernels and non-exponential waiting times.\n\nStarting from the following foundational base:\n\n- A renewal process has independent and identically distributed waiting times $\\{T_i\\}_{i=1}^N$ with cumulative distribution function $F(t)$, probability density function $f(t) = F'(t)$, survival function $S(t) = 1 - F(t)$, and hazard function $h(t) = \\frac{f(t)}{S(t)}$.\n- A Poisson process has the memoryless property $P(T > t + s \\mid T > s) = P(T > t)$, which is equivalent to $h(t) = \\lambda$ (constant) and $f(t) = \\lambda e^{-\\lambda t}$ (exponential distribution).\n- The time-rescaling theorem states that, if the hazard function model $h(t)$ is correct, then the transformed variables $\\tau_i = \\int_0^{T_i} h(s)\\,ds$ are independent and identically distributed exponential random variables with rate $1$, and further $U_i = 1 - e^{-\\tau_i}$ are independent and identically distributed uniform random variables on $[0,1]$.\n\nYour goal is to construct a program that applies a time-rescaling based goodness-of-fit test for exponential waiting times (the Markovian null hypothesis), thereby diagnosing deviations indicative of memory kernels. Use the following test specification:\n\n- Estimate the constant hazard under the null as $\\hat{\\lambda} = 1/\\bar{T}$, where $\\bar{T}$ is the sample mean of the waiting times.\n- Compute $\\tau_i = \\hat{\\lambda} T_i$ and $U_i = 1 - e^{-\\tau_i}$.\n- Perform a Kolmogorov–Smirnov test of $\\{U_i\\}$ against the $\\mathrm{Uniform}(0,1)$ distribution at significance level $\\alpha = 0.01$. Rejecting the null indicates non-exponential statistics and hence a memory-kernel effect.\n\nIn addition, compute a hazard variability index (for interpretability, not for decision-making) defined on $B$ equally spaced bins of the empirical support as follows: let $\\widehat{f}_b$ be the histogram-based density estimate in bin $b$ and $\\widehat{S}_b$ the empirical survival at the left edge of bin $b$. Define the binned hazard estimate $\\widehat{h}_b = \\widehat{f}_b / \\widehat{S}_b$ for bins where $\\widehat{S}_b > 0.05$, and the coefficient of variation $\\mathrm{CV}_h = \\frac{\\mathrm{std}(\\{\\widehat{h}_b\\})}{\\mathrm{mean}(\\{\\widehat{h}_b\\})}$. A near-zero $\\mathrm{CV}_h$ is characteristic of a constant hazard.\n\nConstruct synthetic waiting-time samples in seconds for the following test suite to exercise different facets of the method:\n\n- Case A (happy path, Markovian on-lattice): Exponential with rate $\\lambda_A = 2\\,\\mathrm{s}^{-1}$; sample size $N_A = 5000$.\n- Case B (off-lattice with aging memory; boundary of heavy-tail behavior): Weibull with shape $k_B = 0.7$ and scale $\\theta_B$ chosen so that $\\mathbb{E}[T] = 0.5\\,\\mathrm{s}$, i.e., $\\theta_B = 0.5 / \\Gamma(1 + 1/k_B)$; sample size $N_B = 5000$.\n- Case C (hidden heterogeneity; mixture causing non-exponential statistics): Hyperexponential mixture where, with probability $p_C = 0.3$, $T \\sim \\mathrm{Exp}(\\lambda_1)$ with $\\lambda_1 = 1\\,\\mathrm{s}^{-1}$, and with probability $1 - p_C$, $T \\sim \\mathrm{Exp}(\\lambda_2)$ with $\\lambda_2 = 5\\,\\mathrm{s}^{-1}$; sample size $N_C = 5000$.\n- Case D (small-sample boundary condition, Markovian): Exponential with rate $\\lambda_D = 2\\,\\mathrm{s}^{-1}$; sample size $N_D = 50$.\n\nAll waiting times are in seconds. For reproducibility, use a fixed pseudorandom seed. Your program must:\n\n- Generate the waiting-time samples for the four cases.\n- Apply the described time-rescaling test at $\\alpha = 0.01$.\n- Produce a single line of output containing a list of four booleans $[b_A, b_B, b_C, b_D]$, where $b_X$ is $\\mathrm{True}$ if the test rejects the exponential null for Case $X$ and $\\mathrm{False}$ otherwise. The line must be printed exactly as a comma-separated Python-style list, e.g., \"[True,False,True,False]\".\n\nEnsure scientific realism by using the given distributions and parameters. The output has no physical unit because it consists of booleans. The code must be self-contained and runnable without user input.",
            "solution": "The problem requires the design and implementation of a statistical test to diagnose deviations from Markovian behavior in kinetic Monte Carlo (KMC) simulations. The physical basis for this test is the intimate connection between the Markovian property of a stochastic process and the statistical distribution of its waiting times.\n\nA fundamental tenet of on-lattice KMC is that the system's evolution is a continuous-time Markov chain. This implies the process is memoryless: the probability of transitioning to a future state depends only on the current state, not on the path taken to reach it. A direct mathematical consequence of this memoryless property is that the waiting time, $T$, until the next event follows an exponential distribution, $f(t) = \\lambda e^{-\\lambda t}$, where the rate parameter $\\lambda$ is the sum of the rates of all possible events from the current state. The sequence of waiting times $\\{T_i\\}_{i=1}^N$ in a simulation where the total rate is constant constitutes a sample from a Poisson process.\n\nIn contrast, off-lattice systems or coarse-grained models often exhibit memory. This \"memory\" arises from unresolved degrees of freedom or collective motions, leading to a scenario where the probability of an event depends on the system's history. Such a process is non-Markovian, and its waiting times are not exponentially distributed. These processes are more generally described as renewal processes, characterized by a sequence of independent and identically distributed (i.i.d.) waiting times $\\{T_i\\}$ drawn from some non-exponential distribution.\n\nAny renewal process can be characterized by its hazard function, $h(t)$, which gives the instantaneous probability rate of an event occurring at time $t$, given that it has not occurred yet. It is formally defined as $h(t) = f(t)/S(t)$, where $f(t)$ is the probability density function (PDF) and $S(t) = 1 - F(t)$ is the survival function (with $F(t)$ being the cumulative distribution function, CDF). The memoryless property of a Markovian process is uniquely equivalent to having a constant hazard function, $h(t) = \\lambda$. Any deviation from a constant hazard indicates a non-Markovian, memory-dependent process.\n\nThe diagnostic test leverages this principle via the time-rescaling theorem. This theorem states that if a set of waiting times $\\{T_i\\}$ is drawn from a distribution with a known hazard function $h(t)$, the transformed variables $\\tau_i = \\int_0^{T_i} h(s)\\,ds$ are i.i.d. and follow an exponential distribution with a rate of $1$.\n\nWe can formulate a statistical test based on this theorem. The null hypothesis, $H_0$, is that the waiting times are exponentially distributed, corresponding to a Markovian process.\n$H_0$: The waiting times $\\{T_i\\}$ are drawn from an $\\mathrm{Exponential}(\\lambda)$ distribution for some unknown $\\lambda > 0$.\n\nUnder $H_0$, the hazard function is constant, $h(t) = \\lambda$. The maximum likelihood estimator for this rate, given a sample of $N$ waiting times, is $\\hat{\\lambda} = 1/\\bar{T}$, where $\\bar{T} = \\frac{1}{N}\\sum_{i=1}^N T_i$ is the sample mean.\n\nApplying the time-rescaling theorem with this estimated hazard function, we compute the rescaled times:\n$$ \\tau_i = \\int_0^{T_i} \\hat{\\lambda}\\,ds = \\hat{\\lambda} T_i $$\nIf $H_0$ is true, the set $\\{\\tau_i\\}$ should be a sample from an $\\mathrm{Exponential}(1)$ distribution.\n\nTo facilitate testing, we apply the probability integral transform. If a random variable $\\tau$ is $\\mathrm{Exponential}(1)$, its CDF is $F_{\\tau}(t) = 1 - e^{-t}$. The transformed variable $U = F_{\\tau}(\\tau) = 1 - e^{-\\tau}$ will then be uniformly distributed on the interval $[0, 1]$. Therefore, by computing $U_i = 1 - e^{-\\tau_i} = 1 - e^{-\\hat{\\lambda} T_i}$ for our entire sample, we can reframe our problem as testing a new null hypothesis, $H'_0$:\n$H'_0$: The sample $\\{U_i\\}$ is drawn from a $\\mathrm{Uniform}(0, 1)$ distribution.\n\nThis hypothesis is tested using the two-sided Kolmogorov-Smirnov (KS) test. The KS test compares the empirical CDF of the sample $\\{U_i\\}$ with the CDF of the $\\mathrm{Uniform}(0, 1)$ distribution. It quantifies the maximum discrepancy and calculates a p-value. This p-value represents the probability of observing such a discrepancy (or a larger one) if the data truly were from a $\\mathrm{Uniform}(0, 1)$ distribution. If the p-value is less than a predetermined significance level $\\alpha$ (here, $\\alpha = 0.01$), we reject $H'_0$ and, by extension, the original hypothesis $H_0$. A rejection indicates that the waiting-time data is not consistent with an exponential distribution, providing strong evidence for non-Markovian dynamics with memory.\n\nThe provided test cases are designed to validate this procedure:\n- Case A (Exponential, $N_A=5000$, $\\lambda_A = 2\\,\\mathrm{s}^{-1}$): The null hypothesis is true. We expect a high p-value and failure to reject $H_0$ (result: `False`).\n- Case B (Weibull, $N_B=5000$, $k_B=0.7$): The shape parameter $k_B  1$ implies a decreasing hazard rate, a clear violation of $H_0$. With a large sample, the test should be powerful enough to detect this. We expect to reject $H_0$ (result: `True`).\n- Case C (Hyperexponential, $N_C=5000$): This mixture of exponentials is not itself exponential and has a non-constant hazard function. We expect to reject $H_0$ (result: `True`).\n- Case D (Exponential, $N_D=50$, $\\lambda_D = 2\\,\\mathrm{s}^{-1}$): The null hypothesis is true, but the small sample size greatly reduces the statistical power of the test. A failure to reject $H_0$ is the most likely outcome, illustrating the limitation of statistical testing with insufficient data (result: `False`).",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats, special\n\ndef solve():\n    \"\"\"\n    Runs a diagnostic test on four synthetic waiting-time datasets to distinguish\n    Markovian (exponential) from non-Markovian (non-exponential) dynamics.\n    \"\"\"\n    # Define constants and the pseudorandom number generator for reproducibility.\n    SEED = 12345\n    rng = np.random.default_rng(SEED)\n    ALPHA = 0.01\n\n    # --- Case A: Exponential (Markovian null hypothesis is true) ---\n    N_A = 5000\n    lambda_A = 2.0\n    # The 'scale' parameter for numpy.random.exponential is 1/lambda.\n    samples_A = rng.exponential(scale=1.0/lambda_A, size=N_A)\n\n    # --- Case B: Weibull (Non-Markovian, aging memory) ---\n    N_B = 5000\n    k_B = 0.7  # Shape parameter\n    mean_B = 0.5\n    # The mean of a Weibull(k, theta) distribution is theta * Gamma(1 + 1/k).\n    # We solve for the scale parameter theta.\n    theta_B = mean_B / special.gamma(1.0 + 1.0/k_B)\n    # numpy.random.weibull(a) generates samples with scale=1. We must\n    # multiply by the calculated scale parameter theta_B.\n    samples_B = theta_B * rng.weibull(a=k_B, size=N_B)\n\n    # --- Case C: Hyperexponential (Non-Markovian, mixture) ---\n    N_C = 5000\n    p_C = 0.3\n    lambda1_C = 1.0\n    lambda2_C = 5.0\n    \n    # Generate mixture samples.\n    choices_C = rng.uniform(size=N_C)\n    samples_C = np.zeros(N_C)\n    # Create a boolean mask to select which samples come from the first exponential.\n    mask1 = choices_C  p_C\n    n1 = np.sum(mask1)\n    n2 = N_C - n1\n    samples_C[mask1] = rng.exponential(scale=1.0/lambda1_C, size=n1)\n    samples_C[~mask1] = rng.exponential(scale=1.0/lambda2_C, size=n2)\n\n    # --- Case D: Exponential, small sample (test of statistical power) ---\n    N_D = 50\n    lambda_D = 2.0\n    samples_D = rng.exponential(scale=1.0/lambda_D, size=N_D)\n\n    all_samples = [samples_A, samples_B, samples_C, samples_D]\n    results = []\n\n    for T_values in all_samples:\n        # --- Apply the time-rescaling goodness-of-fit test ---\n        \n        # 1. Estimate the constant hazard rate lambda under the null hypothesis.\n        # The Maximum Likelihood Estimator for an exponential distribution is 1/mean.\n        T_bar = np.mean(T_values)\n        \n        # Edge case handling: a non-positive mean is impossible for waiting times\n        # and would not be exponential.\n        if T_bar = 0:\n            results.append(True)  # Reject the null hypothesis.\n            continue\n            \n        lambda_hat = 1.0 / T_bar\n\n        # 2. Apply the time-rescaling theorem and the probability integral transform.\n        # If the data is exponential, the transformed variables U should be Uniform(0,1).\n        U_values = 1.0 - np.exp(-lambda_hat * T_values)\n\n        # 3. Perform a Kolmogorov-Smirnov test against the Uniform(0,1) distribution.\n        # The null hypothesis of kstest is that the data is drawn from the specified dist.\n        ks_statistic, p_value = stats.kstest(U_values, 'uniform')\n\n        # 4. Make a decision based on the significance level alpha.\n        # We reject the null hypothesis if the p-value is less than alpha.\n        is_rejected = p_value  ALPHA\n        results.append(is_rejected)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(str(r) for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}