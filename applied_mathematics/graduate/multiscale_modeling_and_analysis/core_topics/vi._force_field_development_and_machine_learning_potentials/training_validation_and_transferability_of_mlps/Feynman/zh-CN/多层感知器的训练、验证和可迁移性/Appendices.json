{
    "hands_on_practices": [
        {
            "introduction": "神经网络训练的核心是梯度下降。因此，透彻理解损失函数相对于模型输出（logits）的梯度形式，对于深入分析和排查训练动态至关重要。本练习将引导你从第一性原理出发，推导在分类任务中最常用的Softmax激活函数和交叉熵损失的梯度表达式。这不仅仅是一次数值计算练习；其简洁的最终形式将揭示训练过程如何对类别不平衡等问题产生敏感性，而这正是多尺度建模中一个常见且棘手的挑战。",
            "id": "3825699",
            "problem": "考虑一个多层感知机（MLP），其最后一层为 $k$ 个类别生成一个 logits 向量 $u \\in \\mathbb{R}^{k}$，预测的类别概率通过 softmax 函数 $\\sigma$ 获得，其定义为 $\\sigma(u)_{c} = \\frac{\\exp(u_{c})}{\\sum_{j=1}^{k} \\exp(u_{j})}$，其中 $c \\in \\{1,\\dots,k\\}$。监督学习的目标是一个标签向量 $y \\in \\mathbb{R}^{k}$，其取值在概率单纯形中（对于独热编码，$y$ 只有一个分量等于 $1$，其余分量等于 $0$，但在推导中你不能假设这一点，除非它是一个有效标签向量的形式化定义的推论）。交叉熵损失定义为 $\\ell(u,y) = -\\sum_{c=1}^{k} y_{c} \\,\\ln\\big(\\sigma(u)_{c}\\big)$。\n\n从 softmax 函数和交叉熵的定义出发，结合多元微积分的标准法则（乘法法则、链式法则），推导关于 logits $u$ 的梯度 $\\nabla_{u} \\ell(u,y)$ 的闭式表达式。你的推导除了已声明的定义外，不得依赖任何捷径恒等式。\n\n然后，考虑一个多尺度训练机制，其中每个数据点都与一个离散尺度索引 $s \\in \\{1,\\dots,S\\}$ 相关联，该索引捕捉了分辨率或特征长度尺度，且特定尺度的类别分布由一个概率向量 $\\pi^{(s)} \\in \\mathbb{R}^{k}$ 给出，其中 $\\sum_{c=1}^{k} \\pi^{(s)}_{c} = 1$。假设随机训练过程在一个固定但任意的 logit 向量 $u$ 下，根据 $y \\sim \\pi^{(s)}$ 抽取标签 $y$。使用你推导出的梯度表达式，推导在固定的 $u$ 和 $s$ 下，梯度相对于标签随机性的期望，并以此来论证训练动态对与尺度相关的类别不平衡（例如，稀有类别在精细尺度上占主导，而在粗糙尺度上则相反）的敏感性，及其对验证和跨尺度可迁移性的影响。\n\n将最终答案表示为梯度 $\\nabla_{u} \\ell(u,y)$ 的单一闭式解析表达式。不需要进行数值舍入。不涉及物理单位。",
            "solution": "我们从基本定义开始。softmax 函数 $\\sigma: \\mathbb{R}^{k} \\to \\mathbb{R}^{k}$ 按分量定义为\n$$\n\\sigma(u)_{c} = \\frac{\\exp(u_{c})}{\\sum_{j=1}^{k} \\exp(u_{j})} \\quad \\text{其中 } c \\in \\{1,\\dots,k\\}.\n$$\n令 $Z(u) = \\sum_{j=1}^{k} \\exp(u_{j})$，因此 $\\sigma(u)_{c} = \\frac{\\exp(u_{c})}{Z(u)}$。交叉熵损失定义为\n$$\n\\ell(u,y) = -\\sum_{c=1}^{k} y_{c} \\,\\ln\\big(\\sigma(u)_{c}\\big).\n$$\n我们将使用链式法则和乘法法则来推导 $\\nabla_{u} \\ell(u,y)$，而不使用任何记住的捷径恒等式。\n\n首先，将 softmax 的对数写成便于求导的形式：\n$$\n\\ln\\big(\\sigma(u)_{c}\\big) = \\ln\\big(\\exp(u_{c})\\big) - \\ln\\big(Z(u)\\big) = u_{c} - \\ln\\big(Z(u)\\big).\n$$\n将此代入损失函数：\n$$\n\\ell(u,y) = -\\sum_{c=1}^{k} y_{c}\\,\\big(u_{c} - \\ln(Z(u))\\big) = -\\sum_{c=1}^{k} y_{c}\\,u_{c} + \\bigg(\\sum_{c=1}^{k} y_{c}\\bigg)\\,\\ln(Z(u)).\n$$\n注意到 $y$ 是一个概率向量，因此 $\\sum_{c=1}^{k} y_{c} = 1$；这可由概率单纯形中有效标签向量的定义得出。因此，\n$$\n\\ell(u,y) = -\\sum_{c=1}^{k} y_{c}\\,u_{c} + \\ln\\big(Z(u)\\big).\n$$\n我们现在计算关于 $u$ 的梯度。考虑梯度的第 $i$ 个分量 $\\frac{\\partial \\ell}{\\partial u_{i}}$。逐项求导：\n$$\n\\frac{\\partial}{\\partial u_{i}}\\Big(-\\sum_{c=1}^{k} y_{c}\\,u_{c}\\Big) = -y_{i},\n$$\n以及\n$$\n\\frac{\\partial}{\\partial u_{i}} \\ln\\big(Z(u)\\big) = \\frac{1}{Z(u)}\\,\\frac{\\partial Z(u)}{\\partial u_{i}}.\n$$\n根据定义，$Z(u) = \\sum_{j=1}^{k} \\exp(u_{j})$，所以\n$$\n\\frac{\\partial Z(u)}{\\partial u_{i}} = \\exp(u_{i}).\n$$\n因此，\n$$\n\\frac{\\partial}{\\partial u_{i}} \\ln\\big(Z(u)\\big) = \\frac{\\exp(u_{i})}{Z(u)} = \\sigma(u)_{i}.\n$$\n综合这些结果，\n$$\n\\frac{\\partial \\ell(u,y)}{\\partial u_{i}} = -y_{i} + \\sigma(u)_{i}.\n$$\n由于这对每个 $i \\in \\{1,\\dots,k\\}$ 都成立，梯度的向量形式为\n$$\n\\nabla_{u} \\ell(u,y) = \\sigma(u) - y.\n$$\n\n现在我们使用这个梯度表达式来分析对与尺度相关的类别不平衡的敏感性。假设在固定尺度 $s$ 下的训练从一个尺度相关的类别分布 $\\pi^{(s)}$ 中抽取标签 $y$，即 $y \\sim \\pi^{(s)}$。令 $u$ 固定（即当前输入在当前参数设置下的 logits）。那么梯度是随机向量\n$$\ng(u,y) = \\sigma(u) - y.\n$$\n在固定的 $u$ 和 $s$ 下，关于 $y$ 的随机性取期望，我们得到\n$$\n\\mathbb{E}_{y \\sim \\pi^{(s)}}\\big[g(u,y)\\big] = \\sigma(u) - \\mathbb{E}_{y \\sim \\pi^{(s)}}[y] = \\sigma(u) - \\pi^{(s)}.\n$$\n因此，在尺度 $s$ 下的期望梯度方向将 $\\sigma(u)$ 推向 $\\pi^{(s)}$。如果 $\\pi^{(s)}$ 是不平衡的（例如，一个类别的概率接近 $1$，而其他类别很稀有），那么期望梯度主要与该尺度下的多数类对齐。因此，当聚合那些本身分布在不同尺度上的样本的随机梯度时，净训练动态会偏向于匹配类别分布的尺度加权混合。如果精细尺度的数据与稀有类别相关，而粗糙尺度的数据与常见类别相关，那么当训练从那些稀有类别出现频率较低的尺度中不成比例地抽取样本时，稀有类别的期望梯度幅度就会减小，导致对稀有类别的误差修正变慢。当验证尺度分布不同时，这种不平衡会影响验证结果，并且它会降低跨尺度的可迁移性，因为学习到的分类器将倾向于匹配训练混合分布，而不是一个尺度不变的分布。\n\n为了减轻这种敏感性并提高可迁移性，可以按尺度相关因子或类别相关因子对损失进行重加权。例如，使用重加权的损失 $\\tilde{\\ell}(u,y;s) = w^{(s)}\\,\\ell(u,y)$ 会将梯度变为 $w^{(s)}(\\sigma(u) - y)$，其期望变为 $w^{(s)}(\\sigma(u) - \\pi^{(s)})$，这使得实践者能够抵消尺度频率偏差（例如，选择与尺度 $s$ 的频率成反比的 $w^{(s)}$）或类别频率偏差（选择与 $\\pi^{(s)}_{c}$ 成反比的类别权重）。虽然这种重加权不会改变基本的梯度形式，但它会修改有效曲率和步长大小，这通过海森矩阵 (Hessian) $H(u) = \\nabla^{2}_{u} \\ell(u,y) = \\operatorname{diag}(\\sigma(u)) - \\sigma(u)\\sigma(u)^{\\top}$，影响了优化动态和跨尺度的泛化能力。\n\n总而言之，从第一性原理出发，梯度是 $\\sigma(u) - y$，其在尺度相关的标签分布下的期望是 $\\sigma(u) - \\pi^{(s)}$，这突显了对可能与尺度相关的类别不平衡的直接敏感性，从而影响多尺度机制中的验证和可迁移性。",
            "answer": "$$\\boxed{\\sigma(u)-y}$$"
        },
        {
            "introduction": "经典的机器学习理论告诉我们，模型复杂性与测试误差之间存在一个简单的U型权衡关系。然而，现代的高度过参数化MLP模型常常颠覆这一认知，展现出“双下降”（double descent）现象：随着模型规模的增加，测试误差会先下降，再上升，然后再次下降。本练习将深入探讨这一反直觉行为背后的理论，借助神经正切核（NTK）的框架，理解对含噪声的多尺度数据进行插值学习时，为何会导致这种现象的发生。",
            "id": "3825655",
            "problem": "您正在使用多层感知器（MLP）为双尺度材料模型构建一个代理模型。目标是根据复合材料的微观结构和载荷的描述符来预测其有效弹性模量。目标映射分解为 $y = f_{\\ell}(x) + \\eta(x) + \\varepsilon$，其中 $f_{\\ell}$ 是一个在 $x$ 上平滑的宏观尺度分量，$\\eta$ 是一个相对于学习表示呈现高频的微观尺度波动分量，$\\varepsilon$ 是方差为 $\\sigma^{2}$ 的独立测量噪声。您使用平方损失进行训练，不加显式正则化，并在所有设置中，通过增加优化步骤，在可能的情况下将训练误差驱动到（数值上的）零。假设宽网络极限证明了神经正切核（NTK）近似的合理性，因此当插值可行时，梯度下降在诱导的特征空间中收敛到最小范数插值器。\n\n您收集了从一个固定的关于 $x$ 的训练分布中抽取的 $N$ 个独立同分布的输入-输出对 $(x_{i},y_{i})$。考虑两个实验族：\n\n- P 族：固定 $N$ 并改变可训练参数的数量 $P$，从欠参数化（$P \\ll N$）到高度过参数化（$P \\gg N$）。将 $P$ 中的插值阈值定义为经验训练误差首次达到零的最小 $P$ 值。\n\n- N 族：固定架构及其初始化，使其足够宽以至于 NTK 稳定，并改变 $N$ 从小到大，将 $N$ 中的插值阈值定义为对于该固定模型和优化协议，经验训练误差首次达到零的最小 $N$ 值。\n\n在两个族中，将期望测试均方误差定义为 $R = \\mathbb{E}_{x,\\mathcal{D}}[(\\hat{f}(x)-f(x))^{2}]$，其中 $f(x) = f_{\\ell}(x) + \\eta(x)$，$\\hat{f}$ 是学习到的预测器，期望是针对来自相同分布的新测试输入 $x$ 和训练数据集 $\\mathcal{D}$ 的随机抽取计算的，而 $\\varepsilon$ 仅通过其对学习到的 $\\hat{f}$ 的影响做出贡献。\n\n从偏差-方差分解 $R = \\text{bias}^{2} + \\text{variance} + \\text{irreducible noise}$ 和 NTK 特征空间中训练动力学的线性化出发，推断经验格拉姆矩阵的条件数以及微观尺度分量 $\\eta$ 中的能量如何控制插值点附近的标签噪声和未解析高频内容的放大。运用这些原理来评估 $R$ 作为 $P$ 或 $N$ 的函数时的行为，以及多尺度结构（例如，由大范围相关长度尺度引起的核特征值的缓慢衰减）如何改变这些行为。\n\n在下述陈述中，哪些正确地描述了在此多尺度设置下，关于数据集大小和参数数量的双重下降现象，并预测了期望测试误差 $R$ 先上升后下降的区间？\n\nA. 在 P 族中，对于固定的 $N$ 和增加的 $P$，$R$ 最初减少（偏差减小），然后在插值阈值附近由于接近奇异的经验法向矩阵（empirical normal matrix）引起的方差放大而增加，然后在 $P \\gg N$ 时再次减少，因为 NTK 机制中的最小范数隐式偏置抑制了高频拟合；当组合的高频能量 $\\mathbb{E}[\\eta(x)^{2}] + \\sigma^{2}$ 较大时，峰值的高度也较大。\n\nB. 在 N 族中，对于一个处于无岭（ridgeless）NTK 机制下的固定的非常宽的网络，$R$ 作为 $N$ 的函数是严格非增的，无论核的谱以及 $\\mathbb{E}[\\eta(x)^{2}]$ 和 $\\sigma^{2}$ 的大小如何，因为更多的数据总是毫无例外地同时减少偏差和方差。\n\nC. 相对于学习到的核，一个显著的、有效呈高频的微观尺度分量 $\\eta$ 的存在，会增加两个族中插值点附近的方差项，因此降低 $\\eta$ 的振幅（例如，通过均匀化来平滑标签）会降低甚至消除双重下降峰，可能恢复 $R$ 的单调递减。\n\nD. 如果训练和测试分布仅在微观尺度内容的比例上有所不同（测试集的 $\\eta$ 能量比训练集多），那么对于 P 族，在 $P$ 很大时，$R$ 中的双重下降峰必然会消失，因为 MLP 的谱偏置永久性地偏好低频，从而阻止了对高频测试内容的过拟合。\n\nE. 当训练分布上的 NTK 谱衰减缓慢时，例如特征值 $\\lambda_{j}$ 遵循 $\\lambda_{j} \\sim j^{-p}$（其中由于多尺度结构 $p$ 很小），有效维度较大，经验格拉姆矩阵在插值点附近的更宽范围内是病态的，使得 $R$ 的先升后降现象更加显著；添加显式的 $\\ell_{2}$ 正则化或提前停止可以收窄或抑制这个峰值。\n\n选择所有适用选项。",
            "solution": "该问题要求在多尺度材料背景下，分析多层感知器（MLP）代理模型的期望测试误差 $R$，重点关注双重下降现象。分析需要在神经正切核（NTK）的框架内进行。\n\n首先，让我们根据问题陈述建立理论基础。我们处于宽网络极限下，其中梯度下降下的 MLP 训练动力学等效于使用 NTK（表示为 $K(x, x')$）的核方法。由于训练使用平方损失，无显式正则化，并被驱动至零误差，因此得到的预测器 $\\hat{f}$ 是 NTK 的再生核希尔伯特空间（RKHS）中的最小范数插值器。对于一个训练数据集 $\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^N$，该预测器由以下公式给出：\n$$ \\hat{f}(x) = \\mathbf{k}(x)^T \\mathbf{K}^{-1} \\mathbf{y} $$\n其中 $\\mathbf{k}(x)$ 是核函数求值向量 $[K(x, x_1), \\dots, K(x, x_N)]^T$，$\\mathbf{K}$ 是 $N \\times N$ 的格拉姆矩阵，其元素为 $K_{ij} = K(x_i, x_j)$，$\\mathbf{y} = [y_1, \\dots, y_N]^T$ 是训练标签向量。\n\n标签 $y_i$ 由 $y_i = f_{\\ell}(x_i) + \\eta(x_i) + \\varepsilon_i$ 给出。我们旨在学习的真实函数是 $f(x) = f_{\\ell}(x) + \\eta(x)$。因此，训练标签向量可以写成 $\\mathbf{y} = \\mathbf{f} + \\boldsymbol{\\varepsilon}$，其中 $\\mathbf{f} = [f(x_1), \\dots, f(x_N)]^T$，$\\boldsymbol{\\varepsilon} = [\\varepsilon_1, \\dots, \\varepsilon_N]^T$。预测器则为：\n$$ \\hat{f}(x) = \\mathbf{k}(x)^T \\mathbf{K}^{-1} (\\mathbf{f} + \\boldsymbol{\\varepsilon}) $$\n期望测试误差为 $R = \\mathbb{E}_{x,\\mathcal{D}}[(\\hat{f}(x) - f(x))^2]$。我们可以将其分解为偏差和方差：\n$$ R = \\text{Bias}^2 + \\text{Variance} $$\n对噪声 $\\boldsymbol{\\varepsilon}$ 的抽样取平均的预测器（假设训练输入 $x_i$ 固定）是 $\\bar{f}(x) = \\mathbb{E}_{\\boldsymbol{\\varepsilon}}[\\hat{f}(x)] = \\mathbf{k}(x)^T \\mathbf{K}^{-1} \\mathbf{f}$。\n偏差的平方是 $\\text{Bias}^2 = \\mathbb{E}_{x,\\mathcal{D}}[(\\bar{f}(x) - f(x))^2]$。该项衡量了模型在系统上无法表示真实函数 $f(x) = f_{\\ell}(x) + \\eta(x)$ 的程度。由于 NTK 的谱偏置偏好平滑的低频函数，因此在逼近高频分量 $\\eta(x)$ 时会存在显著的偏差。\n方差是 $\\text{Variance} = \\mathbb{E}_{x,\\mathcal{D}}[(\\hat{f}(x) - \\bar{f}(x))^2] = \\mathbb{E}_{x,\\mathcal{D}}[(\\mathbf{k}(x)^T \\mathbf{K}^{-1} \\boldsymbol{\\varepsilon})^2]$。鉴于 $\\mathbb{E}[\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}^T] = \\sigma^2 \\mathbf{I}$，这变为：\n$$ \\text{Variance} = \\sigma^2 \\mathbb{E}_{\\mathcal{D}}[\\mathbf{k}(x)^T \\mathbf{K}^{-2} \\mathbf{k}(x)] $$\n此方差项量化了由于训练标签中的随机噪声 $\\varepsilon$ 导致预测器变化的程度。关键是，其大小由矩阵逆 $\\mathbf{K}^{-1}$（或其平方）控制。\n\n双重下降现象源于插值阈值处 $\\mathbf{K}^{-1}$ 的行为。当模型复杂度（在 P 族中与 $P$ 相关，在 N 族中与 $N$ 成反比）刚好足以拟合 $N$ 个训练点时，达到此阈值。在这一点上，格拉姆矩阵 $\\mathbf{K}$ 变为奇异或接近奇异。$\\mathbf{K}^{-1}$ 和 $\\mathbf{K}^{-2}$ 的范数会爆炸式增长，导致方差项出现一个尖峰，从而导致总测试误差 $R$ 出现尖峰。\n\n此外，模型被迫对 $y_i = f(x_i) + \\varepsilon_i$ 进行插值。与信号平滑部分 $f_{\\ell}(x_i)$ 的偏差是 $\\eta(x_i) + \\varepsilon_i$。模型必须拟合这种组合起来的“有效噪声”。$\\eta(x)$ 的高频特性使其从平滑核的角度看像噪声一样，因此拟合 $\\eta$ 的挑战与拟合 $\\varepsilon$ 类似，都会加剧峰值附近的过拟合问题。\n\n基于此基础，我们可以分析每个陈述。\n\n**A. 在 P 族中，对于固定的 $N$ 和增加的 $P$，$R$ 最初减少（偏差减小），然后在插值阈值附近由于接近奇异的经验法向矩阵引起的方差放大而增加，然后在 $P \\gg N$ 时再次减少，因为 NTK 机制中的最小范数隐式偏置抑制了高频拟合；当组合的高频能量 $\\mathbb{E}[\\eta(x)^{2}] + \\sigma^{2}$ 较大时，峰值的高度也较大。**\n\n这个陈述准确地描述了模型级双重下降曲线。\n1.  对于 $P \\ll N$（欠参数化），增加 $P$ 会使偏差减少得比方差增加得快，因此 $R$ 减少。这是经典机制。\n2.  在插值阈值附近（$P \\approx N_{crit}$），模型刚好获得足够的容量进行插值。格拉姆矩阵 $\\mathbf{K}$ 变得病态，其逆矩阵会急剧增大，方差项随之飙升。\n3.  对于 $P \\gg N$（过参数化），我们处于 NTK 机制中，梯度下降会找到最小 RKHS 范数的插值器。这种隐式正则化偏好更平滑的解，从而控制了方差。$R$ 再次减少。\n峰值的高度取决于被过拟合的对象的量级。模型被迫拟合与平滑函数的总偏差，这既包括测量噪声 $\\varepsilon$，也包括结构性高频分量 $\\eta$。此有效噪声的总方差为 $\\text{Var}(\\eta(x) + \\varepsilon) = \\mathbb{E}[\\eta(x)^2] + \\sigma^2$（假设独立）。该和值越大，意味着不稳定的模型需要拟合的高频内容越多，导致更高的方差峰值。该陈述完全正确。\n结论：**正确**。\n\n**B. 在 N 族中，对于一个处于无岭（ridgeless）NTK 机制下的固定的非常宽的网络，$R$ 作为 $N$ 的函数是严格非增的，无论核的谱以及 $\\mathbb{E}[\\eta(x)^{2}]$ 和 $\\sigma^{2}$ 的大小如何，因为更多的数据总是毫无例外地同时减少偏差和方差。**\n\n该陈述声称样本级双重下降不会发生。这在事实上是不正确的。对于一个固定的、高度过参数化的模型，双重下降现象同样会作为样本数量 $N$ 的函数而出现。其机制类似于模型级双重下降。对于一个固定的模型，存在一个有效复杂性或维度，我们称之为 $d_{eff}$。\n1.  当 $N \\ll d_{eff}$ 时，模型实际上是过参数化的，我们处于“现代”插值机制中。\n2.  当 $N$ 增加并接近 $d_{eff}$ 时，格拉姆矩阵 $\\mathbf{K}$ 再次变得病态，导致方差飙升和测试误差 $R$ 出现峰值。\n3.  当 $N \\gg d_{eff}$ 时，模型相对于数据变得欠参数化，我们进入经典学习机制，其中增加更多数据会单调地改善性能。\n“更多的数据总是毫无例外地同时减少偏差和方差”这一说法是经典统计学的一个原则，在现代过参数化设置下，当接近此插值阈值时，该原则被打破。\n结论：**不正确**。\n\n**C. 相对于学习到的核，一个显著的、有效呈高频的微观尺度分量 $\\eta$ 的存在，会增加两个族中插值点附近的方差项，因此降低 $\\eta$ 的振幅（例如，通过均匀化来平滑标签）会降低甚至消除双重下降峰，可能恢复 $R$ 的单调递减。**\n\n这个陈述正确地指出了微观尺度分量 $\\eta$ 的作用。如前所述，$\\eta$ 充当一种结构性噪声或模型设定错误噪声。为了实现零训练误差，模型必须对训练点处的 $\\eta(x_i)$ 值进行插值。因为 $\\eta$ 是高频的，这需要一个复杂的、高范数的解，而这样的解泛化能力差，从而导致插值点附近的误差峰值。被病态的逆矩阵 $\\mathbf{K}^{-1}$ 放大的总“噪声”实际上是 $\\eta$ 和 $\\varepsilon$ 的组合。如果 $\\eta$ 的振幅减小（例如，通过对标签进行滤波或均匀化，这将产生一个更平滑的目标函数），那么被过拟合的分量的量级就会变小。这将直接导致一个较低的方差峰值。如果来自 $\\eta$ 和 $\\varepsilon$ 的组合有效噪声足够小，峰值可能会变得可以忽略不计，测试误差曲线可能会呈现单调性。\n结论：**正确**。\n\n**D. 如果训练和测试分布仅在微观尺度内容的比例上有所不同（测试集的 $\\eta$ 能量比训练集多），那么对于 P 族，在 $P$ 很大时，$R$ 中的双重下降峰必然会消失，因为 MLP 的谱偏置永久性地偏好低频，从而阻止了对高频测试内容的过拟合。**\n\n这个陈述包含几个错误。首先，双重下降峰出现在插值阈值附近，而不是在“$P$ 很大时”。在 $P \\gg N$ 的情况下，模型处于第二个下降阶段，已经越过了峰值。所给出的理由也是有缺陷的。模型是在训练分布（低 $\\eta$ 含量）上训练的。在过参数化的 NTK 机制（$P \\gg N$）中，最小范数偏置将产生一个平滑的函数 $\\hat{f}$ 来插值训练数据。因为训练数据是平滑的，所以 $\\hat{f}$ 本身也将是平滑的。当这个平滑的预测器 $\\hat{f}(x)$ 在测试分布上评估时，由于真实的测试函数 $f_{test}(x) = f_{\\ell}(x) + \\eta_{test}(x)$ 在 $\\eta_{test}$ 中有高能量，将会存在很大的不匹配。测试误差 $R = \\mathbb{E}[(\\hat{f}(x) - f_{test}(x))^2]$ 将会很大，主要由模型无法学习的函数部分的能量决定，即大约为 $\\mathbb{E}[\\eta_{test}(x)^2]$。谱偏置并不能防止高误差；相反，它导致了相对于高频测试函数的巨大偏差，从而导致高测试误差。模型不是在“过拟合测试内容”；它只是一个不适用于测试分布的差模型。\n结论：**不正确**。\n\n**E. 当训练分布上的 NTK 谱衰减缓慢时，例如特征值 $\\lambda_{j}$ 遵循 $\\lambda_{j} \\sim j^{-p}$（其中由于多尺度结构 $p$ 很小），有效维度较大，经验格拉姆矩阵在插值点附近的更宽范围内是病态的，使得 $R$ 的先升后降现象更加显著；添加显式的 $\\ell_{2}$ 正则化或提前停止可以收窄或抑制这个峰值。**\n\n这个陈述是对核谱特性影响的复杂而正确的分析。\n1.  特征值 $\\lambda_j$ 的缓慢衰减（$\\lambda_j \\sim j^{-p}$ 中 $p$ 较小）表明核可以表示复杂的、“粗糙”的函数。这在具有多尺度结构的问题中很典型。\n2.  缓慢的谱衰减意味着有效维度很大，因为许多本征模式都很重要。\n3.  这意味着从过参数化机制（$N \\ll d_{eff}$）到欠参数化机制（$N \\gg d_{eff}$）的过渡不那么尖锐。格拉姆矩阵 $\\mathbf{K}$ 将在更宽的 $N$（或 $P$）范围内呈病态，使得双重下降峰更宽、更“显著”。\n4.  显式的 $\\ell_2$ 正则化（岭回归）将 $\\mathbf{K}^{-1}$ 替换为 $(\\mathbf{K} + \\lambda\\mathbf{I})^{-1}$，对于任何 $\\lambda > 0$ 该矩阵都是良态的。这直接对逆矩阵进行了正则化，抑制了方差爆炸并压制了峰值。\n5.  提前停止梯度下降是一种隐式正则化形式，已知其等效于一种谱滤波操作，阻止拟合收敛到有噪声的高频分量。这也控制了方差并压制了峰值。\n整个陈述与高等学习理论一致。\n结论：**正确**。",
            "answer": "$$\\boxed{ACE}$$"
        }
    ]
}