{
    "hands_on_practices": [
        {
            "introduction": "The choice of a loss function is a fundamental decision in training any machine learning model, dictating not only how error is quantified but also what statistical property of the data the model learns to predict. This practice explores the profound differences between two common regression losses: the Mean Squared Error ($MSE$) and the Mean Absolute Error ($MAE$). By working through this exercise (), you will develop a principled understanding of how to select a loss function that aligns with your modeling goals and the inherent noise structure of your data, thereby enhancing model robustness and transferability in multiscale systems where data quality may vary across scales.",
            "id": "3825709",
            "problem": "A research group is training a Multi-Layer Perceptron (MLP) to map multiscale inputs $X$ (with features representing both coarse and fine scales) to outputs $Y$ in a setting where the response exhibits scale-dependent variability. Let the data-generating process be $Y = g(X) + \\epsilon$, where $g$ is a deterministic target map and $\\epsilon$ is a random noise term whose conditional distribution $p(\\epsilon \\mid X)$ is zero-centered at each $X$ but heteroskedastic across scales: at coarse-scale regimes the conditional variance is relatively small, while at micro-scale regimes it may be large and may exhibit heavy tails. The model is trained by Empirical Risk Minimization (ERM) to minimize the average loss over a dataset of size $n$, with either the Mean Squared Error (MSE) $\\ell_{\\text{MSE}}(u,y) = \\|u-y\\|^{2}$ or the Mean Absolute Error (MAE) $\\ell_{\\text{MAE}}(u,y) = \\|u-y\\|_{1}$, where $u$ denotes the prediction and $y$ the observed response. Consider the expected risk $R_{\\ell}(f) = \\mathbb{E}\\left[\\ell\\left(f(X), Y\\right)\\right]$, the induced pointwise Bayes estimator $u^{*}(x) \\in \\arg\\min_{u} \\mathbb{E}\\left[\\ell\\left(u, Y\\right) \\mid X=x\\right]$, and the gradient of the empirical risk with respect to model parameters under each loss. The group also evaluates models using a validation metric consistent with the training loss. The target application requires transferability to a related domain in which the distribution $p(Y \\mid X)$ changes only in its micro-scale tail heaviness and variance, while its central tendency (location) remains approximately the same at each $X$.\n\nWhich of the following statements are correct in this setting?\n\nA. Minimizing the expected risk under the squared loss induces the pointwise estimator $u^{*}(x)$ equal to the conditional expectation $\\mathbb{E}\\left[Y \\mid X=x\\right]$, whereas minimizing the expected risk under the absolute loss induces the pointwise estimator equal to a conditional median of $Y$ given $X=x$. Under heteroskedastic multiscale noise, the gradient contributions in ERM with squared loss scale with residual magnitude, causing high-variance regimes to dominate optimization; with absolute loss, the per-sample influence is bounded, increasing robustness to such heteroskedasticity.\n\nB. For any symmetric unimodal conditional distribution $p(Y \\mid X=x)$, both MSE and MAE induce the same optimal estimator equal to the conditional mode; moreover, the squared loss is inherently robust to outliers since averaging reduces their impact.\n\nC. If at the microscale the conditional variance of $\\epsilon$ is multiplied by a factor $c^{2}$ with $c>0$ while the conditional mean of $Y$ is unchanged, then the Bayes estimator under squared loss scales by a factor $c$ (i.e., $u^{*}(x)$ under squared loss changes multiplicatively with $c$), whereas the Bayes estimator under absolute loss remains unchanged.\n\nD. Using MSE as the validation metric tends to select models that prioritize reduction of large residuals associated with high-variance scales, potentially at the expense of performance on low-variance scales; using MAE as the validation metric reduces this dominance by penalizing residuals linearly, mitigating sensitivity to heteroskedasticity across scales.\n\nE. If the target domain differs from the source only by heavier tails and larger variance at the microscale while preserving the conditional location (for example, median) of $Y$ at each $X$, then, all else equal, an MAE-trained MLP is expected to transfer more robustly than an MSE-trained MLP because the bounded per-sample influence under absolute loss makes optimization and generalization less sensitive to the increased tail heaviness, whereas the unbounded influence under squared loss can be dominated by rare, large residuals in the new domain.\n\nSelect all that apply.",
            "solution": "The problem statement is subjected to validation before proceeding to a solution.\n\n### Step 1: Extract Givens\n- **Model**: Multi-Layer Perceptron (MLP).\n- **Mapping**: From multiscale inputs $X$ to outputs $Y$.\n- **Data-Generating Process**: $Y = g(X) + \\epsilon$, where $g$ is a deterministic map and $\\epsilon$ is a random noise term.\n- **Noise Characteristics**: The conditional distribution $p(\\epsilon \\mid X)$ is zero-centered, i.e., $\\mathbb{E}[\\epsilon \\mid X=x] = 0$ for all $x$. The noise is heteroskedastic: conditional variance is small for coarse-scale inputs and large for micro-scale inputs, where the distribution may also have heavy tails.\n- **Training**: Empirical Risk Minimization (ERM) on a dataset of size $n$.\n- **Loss Functions**:\n    1.  Mean Squared Error (MSE): $\\ell_{\\text{MSE}}(u,y) = \\|u-y\\|^{2}$.\n    2.  Mean Absolute Error (MAE): $\\ell_{\\text{MAE}}(u,y) = \\|u-y\\|_{1}$.\n- **Notation**: $u$ denotes the model prediction, $y$ the observed response. The use of norms suggests $Y$ could be a vector.\n- **Theoretical Quantities**:\n    - Expected Risk: $R_{\\ell}(f) = \\mathbb{E}\\left[\\ell\\left(f(X), Y\\right)\\right]$.\n    - Pointwise Bayes Estimator: $u^{*}(x) \\in \\arg\\min_{u} \\mathbb{E}\\left[\\ell\\left(u, Y\\right) \\mid X=x\\right]$.\n- **Evaluation**: A validation metric consistent with the training loss is used.\n- **Transfer Learning Scenario**: Transfer to a related domain where $p(Y \\mid X)$ changes only in its micro-scale tail heaviness and variance. The central tendency (location) of $p(Y \\mid X)$ remains approximately the same at each $X$.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The problem is well-grounded in statistical learning theory. The concepts of ERM, MSE and MAE losses, Bayes estimators, heteroskedasticity, and transfer learning are standard and central to machine learning. The setup describes a common regression problem with realistic complexities.\n2.  **Well-Posed**: The problem is well-posed, asking for an evaluation of several statements based on a clearly defined statistical setting. There is sufficient information to derive the properties of the estimators and training dynamics.\n3.  **Objective**: The language is precise and objective, using standard terminology from statistics and machine learning. There are no subjective or opinion-based claims in the problem statement.\n4.  **Complete and Consistent**: The problem statement is self-contained. The assumption $\\mathbb{E}[\\epsilon \\mid X=x] = 0$ implies $\\mathbb{E}[Y \\mid X=x] = \\mathbb{E}[g(X) + \\epsilon \\mid X=x] = g(x)$. This is consistent with the notion of a deterministic target map $g$ and additive, zero-centered noise. The description of the transfer domain is also consistent with this setup.\n5.  **No Other Flaws**: The problem is not trivial, unrealistic, or ill-posed. It addresses a core conceptual issue in robust machine learning and model selection.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. A solution will be derived by analyzing the properties of the loss functions and their implications in the given context.\n\n### Derivations and Analysis\n\n**1. Pointwise Bayes Estimators**\nThe pointwise Bayes estimator $u^*(x)$ minimizes the expected loss conditioned on $X=x$.\n\n- **For MSE ($\\ell_2$ loss)**: We seek to minimize $J(u) = \\mathbb{E}\\left[\\|u-Y\\|^2 \\mid X=x\\right]$. Taking the gradient with respect to the vector $u$ and setting it to zero yields $u = \\mathbb{E}[Y \\mid X=x]$. Thus, the Bayes estimator for MSE is the conditional expectation (mean) of $Y$.\n\n- **For MAE ($\\ell_1$ loss)**: We seek to minimize $J(u) = \\mathbb{E}\\left[\\|u-Y\\|_1 \\mid X=x\\right]$. This minimization can be done component-wise. For each component $j$, the value of $u_j$ that minimizes $\\mathbb{E}\\left[|u_j - Y_j| \\mid X=x\\right]$ is the conditional median of the random variable $Y_j$ given $X=x$.\n\n**2. ERM Gradient Analysis**\nFor a single sample $(x_i, y_i)$, the gradient of the empirical risk with respect to model parameters $\\theta$ contains the term $\\frac{\\partial \\ell(u_i, y_i)}{\\partial u_i} \\nabla_\\theta f_\\theta(x_i)$, where $u_i = f_\\theta(x_i)$.\n\n- **For MSE**: The derivative with respect to the prediction is $\\frac{\\partial \\ell}{\\partial u_i} = 2(u_i - y_i)$. The gradient contribution is proportional to the residual. Samples with large residuals (from high-variance regions) will result in large gradients, dominating the optimization process.\n\n- **For MAE**: The subgradient with respect to the prediction is $\\frac{\\partial \\ell}{\\partial u_i} = \\text{sign}(u_i - y_i)$. The magnitude of this term is bounded by $1$, regardless of the magnitude of the residual. This bounds the influence of any single sample on the gradient updates, making the optimization process more robust to outliers and heteroskedasticity.\n\n### Option-by-Option Analysis\n\n**A. Minimizing the expected risk under the squared loss induces the pointwise estimator $u^{*}(x)$ equal to the conditional expectation $\\mathbb{E}\\left[Y \\mid X=x\\right]$, whereas minimizing the expected risk under the absolute loss induces the pointwise estimator equal to a conditional median of $Y$ given $X=x$. Under heteroskedastic multiscale noise, the gradient contributions in ERM with squared loss scale with residual magnitude, causing high-variance regimes to dominate optimization; with absolute loss, the per-sample influence is bounded, increasing robustness to such heteroskedasticity.**\nThis statement is correct. It correctly identifies the Bayes estimators and accurately describes the robustness properties of MAE versus MSE during optimization with heteroskedastic data.\n\nVerdict: **Correct**.\n\n**B. For any symmetric unimodal conditional distribution $p(Y \\mid X=x)$, both MSE and MAE induce the same optimal estimator equal to the conditional mode; moreover, the squared loss is inherently robust to outliers since averaging reduces their impact.**\nThis statement is incorrect. While the first part is true (for symmetric unimodal distributions, mean=median=mode), the second part is false. Squared loss is *sensitive* to outliers because squaring magnifies their effect, it is not robust.\n\nVerdict: **Incorrect**.\n\n**C. If at the microscale the conditional variance of $\\epsilon$ is multiplied by a factor $c^{2}$ with $c>0$ while the conditional mean of $Y$ is unchanged, then the Bayes estimator under squared loss scales by a factor $c$ (i.e., $u^{*}(x)$ under squared loss changes multiplicatively with $c$), whereas the Bayes estimator under absolute loss remains unchanged.**\nThis statement is incorrect. The Bayes estimator for squared loss is the conditional mean, $u^*(x) = \\mathbb{E}[Y \\mid X=x]$. The premise explicitly states that \"the conditional mean of $Y$ is unchanged\". Therefore, the Bayes estimator under squared loss is also unchanged.\n\nVerdict: **Incorrect**.\n\n**D. Using MSE as the validation metric tends to select models that prioritize reduction of large residuals associated with high-variance scales, potentially at the expense of performance on low-variance scales; using MAE as the validation metric reduces this dominance by penalizing residuals linearly, mitigating sensitivity to heteroskedasticity across scales.**\nThis statement is correct. An MSE metric gives disproportionate weight to large errors, thus a model selection process based on MSE will favor models that perform well in high-variance regions. An MAE metric gives equal weight to errors of the same magnitude, leading to a more balanced assessment across different noise scales.\n\nVerdict: **Correct**.\n\n**E. If the target domain differs from the source only by heavier tails and larger variance at the microscale while preserving the conditional location (for example, median) of $Y$ at each $X$, then, all else equal, an MAE-trained MLP is expected to transfer more robustly than an MSE-trained MLP because the bounded per-sample influence under absolute loss makes optimization and generalization less sensitive to the increased tail heaviness, whereas the unbounded influence under squared loss can be dominated by rare, large residuals in the new domain.**\nThis statement is correct. The MAE-trained model learns the conditional median, which is assumed to be invariant between the source and target domains. Because MAE is robust to heavy tails and outliers during training, the learned model is a more faithful approximation of this invariant median. The MSE-trained model is sensitive to the noise characteristics of the source domain and will generalize less robustly to the target domain with increased variance and heavier tails.\n\nVerdict: **Correct**.",
            "answer": "$$\\boxed{ADE}$$"
        },
        {
            "introduction": "Beyond the loss function, the very architecture of a neural network embeds a powerful, implicit set of assumptions about the function it is trying to learn, a concept known as \"inductive bias.\" This exercise () contrasts a standard Rectified Linear Unit ($ReLU$) network with a Sinusoidal Representation Network ($SIREN$) to illustrate this principle. You will analyze the \"spectral bias\" of $ReLU$ networks, which favors low-frequency functions, and compare it to the inherent ability of sinusoidal activations to represent high-frequency details, a critical capability for multiscale modeling and out-of-distribution generalization.",
            "id": "3825653",
            "problem": "Consider learning a one-dimensional multiscale field on the unit interval. Let the target field be $u:[0,1]\\to\\mathbb{R}$ given by a band-limited Fourier series $u(x)=\\sum_{k\\in\\mathcal{K}_{\\mathrm{all}}}a_k\\sin(2\\pi k x)$ with amplitudes $a_k$ decaying as $|a_k|\\le C k^{-\\beta}$ for some constants $C>0$ and $\\beta>1$. A training dataset $\\mathcal{D}_{\\mathrm{train}}=\\{(x_i,u(x_i))\\}_{i=1}^N$ is constructed by sampling $x_i$ uniformly in $[0,1]$, but the generating series for $u$ during training only includes a subset of frequencies $\\mathcal{K}_{\\mathrm{train}}\\subset\\mathcal{K}_{\\mathrm{all}}$ that are bounded above by $K_0$, i.e., $\\max \\mathcal{K}_{\\mathrm{train}}=K_0$. A validation dataset $\\mathcal{D}_{\\mathrm{val}}$ is independently drawn from the same $x$-distribution and the same frequency set $\\mathcal{K}_{\\mathrm{train}}$ and is used for early stopping based on the validation Mean Squared Error (MSE).\n\nTwo models are trained under identical optimization settings (same optimizer, step sizes, batch sizes, and weight decay): (i) a Multilayer Perceptron (MLP) with Rectified Linear Unit (ReLU) activations, denoted $f_R(x;\\theta)$; (ii) a Sinusoidal Representation Network (SIREN) with sine activations, denoted $f_S(x;\\phi)$, using a first-layer frequency scale parameter $\\omega_0$ chosen so that the distribution of pre-activations yields $\\mathbb{E}[|\\partial f_S/\\partial x|]\\approx \\mathrm{const}$ at initialization. Both models have sufficient width and depth to be universal approximators on $[0,1]$ under standard assumptions. After training with early stopping guided by $\\mathcal{D}_{\\mathrm{val}}$, the models are evaluated on a test dataset $\\mathcal{D}_{\\mathrm{test}}$ drawn from $x\\sim \\mathrm{Unif}([0,1])$ but with target fields containing frequencies exclusively in an unseen band $\\mathcal{K}_{\\mathrm{test}}\\subset\\mathcal{K}_{\\mathrm{all}}$ satisfying $\\min \\mathcal{K}_{\\mathrm{test}}=K_0+\\Delta K$ for some $\\Delta K>0$ and $\\mathcal{K}_{\\mathrm{test}}\\cap \\mathcal{K}_{\\mathrm{train}}=\\emptyset$.\n\nAssume the following well-tested facts and definitions hold: functions on $[0,1]$ admit Fourier representations; piecewise linear functions have Fourier spectra with amplitudes that decay with frequency; gradient-based training of ReLU networks exhibits a spectral bias favoring low-frequency components; sine activations can represent high-frequency oscillations with moderate parameter norms by adjusting effective frequencies through weights; early stopping controls overfitting to training distributions but does not alter the inductive bias of the activation nonlinearity.\n\nWhich statement best predicts, from first principles, the comparative ability of $f_R$ and $f_S$ to represent fine-scale oscillations and to transfer to the unseen high-frequency band $\\mathcal{K}_{\\mathrm{test}}$ under the described training and validation regime?\n\nA. The ReLU MLP $f_R$ will outperform the SIREN $f_S$ on both representing fine-scale oscillations and transfer to $\\mathcal{K}_{\\mathrm{test}}$, because piecewise linear functions can approximate any sinusoid with sufficiently many breakpoints and universal approximation implies equal inductive bias.\n\nB. The SIREN $f_S$, with sine activations and appropriate $\\omega_0$ initialization ensuring stable derivatives, will more compactly represent fine-scale oscillations and will yield better transfer to $\\mathcal{K}_{\\mathrm{test}}$ than the ReLU MLP $f_R$, because its periodic inductive bias and capacity to realize high effective frequencies with moderate parameter norms align with the unseen test band.\n\nC. Both $f_R$ and $f_S$ will perform equally on representing fine-scale oscillations and transfer to $\\mathcal{K}_{\\mathrm{test}}$, provided training continues until training MSE is near zero, because validation early stopping only prevents overfitting and does not interact with frequency content.\n\nD. A ReLU MLP $f_R$ augmented with fixed random Fourier feature positional encoding $z(x)=[\\cos(2\\pi f_j x),\\sin(2\\pi f_j x)]_{j=1}^M$ for frequencies $f_j\\le K_0$ will strictly surpass $f_S$ on transfer to $\\mathcal{K}_{\\mathrm{test}}$, since the encoded features guarantee extrapolation to any higher frequency outside the training band.",
            "solution": "This problem tests the understanding of **inductive bias**, which is the inherent preference of a model architecture for certain types of functions, and how this bias affects out-of-distribution generalization.\n\n1.  **Inductive Bias of the ReLU MLP ($f_R$)**: A ReLU network represents a continuous piecewise linear function. A well-established property of training ReLU networks with gradient-based methods is **spectral bias**: the model learns low-frequency functions much faster and more easily than high-frequency functions. Its functional building blocks (linear segments) are not inherently oscillatory. To approximate a high-frequency signal, it must construct it from many small linear pieces, which is an inefficient representation. This bias means the model does not naturally extrapolate to unseen higher frequencies. The low-frequency function learned on the training set $\\mathcal{K}_{\\mathrm{train}}$ provides no principled basis for generating oscillations in the unseen high-frequency band $\\mathcal{K}_{\\mathrm{test}}$.\n\n2.  **Inductive Bias of the SIREN ($f_S$)**: A SIREN uses the sine function as its activation. The entire network is a composition of sinusoids. This architecture has a strong **periodic inductive bias**, making it exceptionally well-suited for representing signals and fields as superpositions of sinusoids (like a Fourier series). By adjusting its weights, the network can compactly represent oscillatory functions with varying frequencies, amplitudes, and phases. The special initialization protocol for $\\omega_0$ ensures that the network is capable of representing a broad spectrum of frequencies from the outset. This bias is perfectly aligned with the problem of learning and extrapolating an oscillatory signal.\n\n3.  **Generalization Task Analysis**: The test set $\\mathcal{D}_{\\mathrm{test}}$ contains frequencies exclusively in a band $\\mathcal{K}_{\\mathrm{test}}$ that was not seen during training. This is an out-of-distribution generalization task that specifically tests for extrapolation to higher frequencies. The model's success is determined by how well its inductive bias aligns with this task.\n    *   The ReLU MLP's low-frequency bias is misaligned with the test task.\n    *   The SIREN's periodic bias is perfectly aligned with the test task.\n\n4.  **Evaluation of Options**:\n    *   **A**: Incorrect. The Universal Approximation Theorem guarantees expressivity but says nothing about the efficiency of representation or the inductive bias, which governs generalization. The inductive biases of ReLU MLPs and SIRENs are fundamentally different.\n    *   **B**: Correct. The SIREN's periodic inductive bias, stemming from its sine activations, allows it to naturally and compactly represent oscillatory functions. This makes it far superior at generalizing to the unseen high-frequency band $\\mathcal{K}_{\\mathrm{test}}$ compared to the ReLU MLP, whose spectral bias favors low frequencies.\n    *   **C**: Incorrect. Reaching zero training error does not change a model's inductive bias. The way a model interpolates between training points and extrapolates to new domains is dictated by its architecture, not just its performance on the training set. The difference in generalization capability would remain.\n    *   **D**: Incorrect. Augmenting a ReLU MLP with fixed Fourier features containing frequencies up to $K_0$ helps it learn the functions in the training band $\\mathcal{K}_{\\mathrm{train}}$ more effectively. However, since the features are fixed and do not contain any basis functions for frequencies *above* $K_0$, the model still lacks a direct mechanism to represent functions in $\\mathcal{K}_{\\mathrm{test}}$. It must still rely on the poor extrapolation capabilities of its underlying ReLU structure. A SIREN, with its learnable frequency representations, is superior for this task.",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "Classical machine learning theory posits a simple bias-variance trade-off, but modern, highly overparameterized models exhibit a more complex behavior known as \"double descent.\" This advanced practice () guides you through the modern theory of generalization using the Neural Tangent Kernel ($NTK$) framework, exploring why test error can peak at the interpolation threshold and then decrease again as model size grows. Engaging with this problem will refine your intuition about the interplay between model capacity, implicit regularization, and data, revealing how phenomena like multiscale structure influence generalization performance in deep learning models.",
            "id": "3825655",
            "problem": "You are building a surrogate for a two-scale materials model using a Multilayer Perceptron (MLP). The goal is to predict the effective elastic modulus of a composite from descriptors of its microstructure and loading. The target map decomposes as $y = f_{\\ell}(x) + \\eta(x) + \\varepsilon$, where $f_{\\ell}$ is a macroscale component that is smooth in $x$, $\\eta$ is a microscale fluctuation component that appears high-frequency with respect to the learned representation, and $\\varepsilon$ is independent measurement noise with variance $\\sigma^{2}$. You train with squared loss, no explicit regularization, and in all settings you drive training error to (numerically) zero when possible by increasing optimization steps. Assume the wide-network limit justifies the Neural Tangent Kernel (NTK) approximation, so that gradient descent converges to the minimum-norm interpolant in the induced feature space when interpolation is feasible.\n\nYou collect $N$ independent and identically distributed input-output pairs $(x_{i},y_{i})$ drawn from a fixed training distribution over $x$. Consider two experimental families:\n\n- Family P: Fix $N$ and vary the number of trainable parameters $P$ from underparameterized ($P \\ll N$) to highly overparameterized ($P \\gg N$). Define the interpolation threshold in $P$ as the smallest $P$ at which the empirical training error first reaches zero.\n\n- Family N: Fix the architecture and its initialization to be sufficiently wide that the NTK is stable, and vary $N$ from small to large, defining the interpolation threshold in $N$ as the smallest $N$ at which the empirical training error first reaches zero for that fixed model and optimization protocol.\n\nIn both families, define the expected test mean-squared error as $R = \\mathbb{E}_{x,\\mathcal{D}}[(\\hat{f}(x)-f(x))^{2}]$, where $f(x) = f_{\\ell}(x) + \\eta(x)$, $\\hat{f}$ is the learned predictor, the expectation is over a fresh test input $x$ from the same distribution and over the random draw of the training dataset $\\mathcal{D}$, and $\\varepsilon$ contributes only through its effect on the learned $\\hat{f}$.\n\nStarting from the bias-variance decomposition $R = \\text{bias}^{2} + \\text{variance} + \\text{irreducible noise}$ and the linearization of training dynamics in the NTK feature space, reason about how the conditioning of the empirical Gram matrix and the energy in the microscale component $\\eta$ control the amplification of label noise and unresolved high-frequency content near interpolation. Use these principles to assess how $R$ behaves as a function of $P$ or $N$, and how multiscale structure (e.g., slow decay of kernel eigenvalues induced by a wide range of relevant length scales) modifies these behaviors.\n\nWhich of the following statements correctly characterize double descent with respect to dataset size and parameter count in this multiscale setting and predict regimes where the expected test error $R$ rises and then falls?\n\nA. In Family P, for fixed $N$ and increasing $P$, $R$ initially decreases (bias reduction), then increases near the interpolation threshold due to variance amplification from a nearly singular empirical normal matrix, and then decreases again for $P \\gg N$ as the minimum-norm implicit bias in the NTK regime suppresses high-frequency fitting; the height of the peak is larger when the combined high-frequency energy $\\mathbb{E}[\\eta(x)^{2}] + \\sigma^{2}$ is larger.\n\nB. In Family N, for a fixed very wide network in the ridgeless NTK regime, $R$ as a function of $N$ is strictly nonincreasing regardless of the spectrum of the kernel and the sizes of $\\mathbb{E}[\\eta(x)^{2}]$ and $\\sigma^{2}$, because more data always reduces both bias and variance without exception.\n\nC. The presence of a substantial microscale component $\\eta$ that is effectively high-frequency relative to the learned kernel increases the variance term near interpolation in both families, so that reducing the amplitude of $\\eta$ (e.g., by smoothing labels via homogenization) lowers or can even remove the double-descent peak, potentially restoring a monotone decrease of $R$.\n\nD. If training and test distributions differ only in the proportion of microscale content (with test having more $\\eta$ energy than train), then for Family P the double-descent peak in $R$ necessarily disappears at large $P$, because spectral bias of the MLP permanently favors low frequencies and prevents overfitting of high-frequency test content.\n\nE. When the NTK spectrum over the training distribution decays slowly, for example with eigenvalues $\\lambda_{j}$ obeying $\\lambda_{j} \\sim j^{-p}$ with small $p$ due to multiscale structure, the effective dimension is large and the empirical Gram matrix is ill-conditioned over a broader range near interpolation, making the rise-then-fall in $R$ more pronounced; adding explicit $\\ell_{2}$ regularization or early stopping narrows or suppresses this peak.\n\nSelect all that apply.",
            "solution": "The problem asks for an analysis of the expected test error, $R$, for a Multilayer Perceptron (MLP) surrogate model in a multiscale materials context, focusing on the double descent phenomenon. The analysis is to be performed within the framework of the Neural Tangent Kernel (NTK).\n\nFirst, let us establish the theoretical foundation. In the wide-network, NTK regime, the predictor $\\hat{f}$ found by gradient descent is the minimum-norm interpolant in the kernel's RKHS. The test error $R$ can be decomposed into bias and variance. The double descent peak occurs at the interpolation threshold, where the model capacity is just enough to fit the training data. At this point, the empirical Gram matrix $\\mathbf{K}$ becomes ill-conditioned, and its inverse $\\mathbf{K}^{-1}$ explodes in norm. This causes the variance term, which depends on $\\mathbf{K}^{-1}$, to spike. The model is forced to interpolate the labels $y_i = f_{\\ell}(x_i) + \\eta(x_i) + \\varepsilon_i$. The high-frequency microscale component $\\eta(x)$ acts as a form of \"structural noise\" in addition to the measurement noise $\\varepsilon$. The variance peak is driven by the model overfitting this total effective noise.\n\nWith this foundation, we can analyze each statement.\n\n**A. In Family P, for fixed $N$ and increasing $P$, $R$ initially decreases (bias reduction), then increases near the interpolation threshold due to variance amplification from a nearly singular empirical normal matrix, and then decreases again for $P \\gg N$ as the minimum-norm implicit bias in the NTK regime suppresses high-frequency fitting; the height of the peak is larger when the combined high-frequency energy $\\mathbb{E}[\\eta(x)^{2}] + \\sigma^{2}$ is larger.**\nThis statement is a perfect description of model-wise double descent. For $P \\ll N$, we are in the classical bias-variance trade-off. Near the interpolation threshold ($P \\approx N_{crit}$), the model becomes unstable, causing a variance spike. For $P \\gg N$, we are in the overparameterized regime where implicit regularization from finding the minimum-norm solution tames the variance, causing the error to decrease again. The height of the peak is determined by the magnitude of the effective noise ($\\eta + \\varepsilon$) that the unstable model is forced to fit. A larger combined energy leads to a larger peak.\nVerdict: **Correct**.\n\n**B. In Family N, for a fixed very wide network in the ridgeless NTK regime, $R$ as a function of $N$ is strictly nonincreasing regardless of the spectrum of the kernel and the sizes of $\\mathbb{E}[\\eta(x)^{2}]$ and $\\sigma^{2}$, because more data always reduces both bias and variance without exception.**\nThis statement is incorrect. Double descent is also observed as a function of sample size $N$ for a fixed overparameterized model. As $N$ increases and approaches the effective complexity of the model, the same ill-conditioning and variance spike can occur. The classical notion that \"more data is always better\" is violated near this sample-wise interpolation threshold.\nVerdict: **Incorrect**.\n\n**C. The presence of a substantial microscale component $\\eta$ that is effectively high-frequency relative to the learned kernel increases the variance term near interpolation in both families, so that reducing the amplitude of $\\eta$ (e.g., by smoothing labels via homogenization) lowers or can even remove the double-descent peak, potentially restoring a monotone decrease of $R$.**\nThis statement is correct. The component $\\eta$ acts as structural noise. By forcing the model to interpolate these high-frequency fluctuations, it amplifies the overfitting behavior at the interpolation threshold. Reducing the amplitude of $\\eta$ (i.e., making the target function smoother) reduces the amount of \"noise\" the model has to fit, which directly reduces the height of the variance peak. If the total effective noise is small enough, the peak may become negligible.\nVerdict: **Correct**.\n\n**D. If training and test distributions differ only in the proportion of microscale content (with test having more $\\eta$ energy than train), then for Family P the double-descent peak in $R$ necessarily disappears at large $P$, because spectral bias of the MLP permanently favors low frequencies and prevents overfitting of high-frequency test content.**\nThis statement is incorrect. The double descent peak occurs near the interpolation threshold, not at large $P$. At large $P \\gg N$, the model is past the peak. The reasoning is also flawed. A model trained on data with low $\\eta$ energy will be smooth due to implicit bias. When evaluated on test data with high $\\eta$ energy, the test error will be large due to a high bias (the model cannot represent the test function's high-frequency components), not because of overfitting. The spectral bias causes a large mismatch, not a disappearance of error.\nVerdict: **Incorrect**.\n\n**E. When the NTK spectrum over the training distribution decays slowly, for example with eigenvalues $\\lambda_{j}$ obeying $\\lambda_{j} \\sim j^{-p}$ with small $p$ due to multiscale structure, the effective dimension is large and the empirical Gram matrix is ill-conditioned over a broader range near interpolation, making the rise-then-fall in $R$ more pronounced; adding explicit $\\ell_{2}$ regularization or early stopping narrows or suppresses this peak.**\nThis statement is correct. A slow spectral decay (typical for multiscale problems) means the kernel is complex and has a high effective dimension. This makes the transition region around the interpolation threshold wider, causing a more pronounced double descent peak. Both explicit $\\ell_2$ regularization (which makes the Gram matrix invertible via $(\\mathbf{K} + \\lambda\\mathbf{I})^{-1}$) and implicit regularization from early stopping are known to mitigate this variance explosion, thereby suppressing or narrowing the peak.\nVerdict: **Correct**.",
            "answer": "$$\\boxed{ACE}$$"
        }
    ]
}