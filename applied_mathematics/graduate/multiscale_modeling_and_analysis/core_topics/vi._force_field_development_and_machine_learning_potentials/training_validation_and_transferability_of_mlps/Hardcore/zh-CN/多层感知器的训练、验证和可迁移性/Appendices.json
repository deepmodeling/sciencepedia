{
    "hands_on_practices": [
        {
            "introduction": "在训练神经网络时，选择损失函数是定义学习目标的关键步骤。在多尺度系统中，噪声和数据方差在粗尺度和细尺度之间可能存在巨大差异，因此这一选择尤为重要。本练习  旨在引导您分析均方误差 ($L_2$ 损失) 和平均绝对误差 ($L_1$ 损失) 之间的根本区别，揭示它们在存在异方差噪声时对模型鲁棒性和可迁移性的不同影响。",
            "id": "3825709",
            "problem": "一个研究小组正在训练一个多层感知器（MLP），用于将多尺度输入 $X$（其特征代表粗尺度和细尺度）映射到输出 $Y$。在该设定中，响应表现出尺度依赖的可变性。设数据生成过程为 $Y = g(X) + \\epsilon$，其中 $g$ 是一个确定性目标映射，$\\epsilon$ 是一个随机噪声项，其条件分布 $p(\\epsilon \\mid X)$ 在每个 $X$ 处均值为零，但在不同尺度上是异方差的：在粗尺度区域，条件方差相对较小，而在微观尺度区域，它可能很大并可能表现出重尾。该模型通过经验风险最小化（ERM）进行训练，以在大小为 $n$ 的数据集上最小化平均损失，损失函数为均方误差（MSE）$\\ell_{\\text{MSE}}(u,y) = \\|u-y\\|^{2}$ 或平均绝对误差（MAE）$\\ell_{\\text{MAE}}(u,y) = \\|u-y\\|_{1}$，其中 $u$ 表示预测值，$y$ 表示观测响应。考虑期望风险 $R_{\\ell}(f) = \\mathbb{E}\\left[\\ell\\left(f(X), Y\\right)\\right]$，诱导的逐点贝叶斯估计量 $u^{*}(x) \\in \\arg\\min_{u} \\mathbb{E}\\left[\\ell\\left(u, Y\\right) \\mid X=x\\right]$，以及在每种损失下经验风险关于模型参数的梯度。该小组还使用与训练损失一致的验证指标来评估模型。目标应用要求模型能够迁移到一个相关领域，在该领域中，分布 $p(Y \\mid X)$ 仅在微观尺度的尾部厚重度和方差上发生变化，而其中心趋势（位置）在每个 $X$ 处大致保持不变。\n\n在这种情况下，以下哪些陈述是正确的？\n\nA. 在平方损失下最小化期望风险会诱导出等于条件期望 $\\mathbb{E}\\left[Y \\mid X=x\\right]$ 的逐点估计量 $u^{*}(x)$，而在绝对损失下最小化期望风险会诱导出等于给定 $X=x$ 时 $Y$ 的条件中位数的逐点估计量。在异方差多尺度噪声下，使用平方损失的ERM中的梯度贡献与残差大小成比例，导致高方差区域主导优化过程；而使用绝对损失时，每个样本的影响是有界的，从而增加了对此类异方差的鲁棒性。\n\nB. 对于任何对称单峰条件分布 $p(Y \\mid X=x)$，MSE和MAE都会诱导出等于条件众数的相同最优估计量；此外，平方损失对异常值具有内在的鲁棒性，因为平均过程会减少其影响。\n\nC. 如果在微观尺度上，$\\epsilon$ 的条件方差乘以一个因子 $c^{2}$（其中 $c>0$），而 $Y$ 的条件均值保持不变，那么平方损失下的贝叶斯估计量将按因子 $c$ 缩放（即，平方损失下的 $u^{*}(x)$ 随 $c$ 发生乘法变化），而绝对损失下的贝叶斯估计量保持不变。\n\nD. 使用MSE作为验证指标倾向于选择优先减少与高方差尺度相关的大残差的模型，这可能会以牺牲低方差尺度上的性能为代价；使用MAE作为验证指标通过线性惩罚残差来减少这种主导作用，从而减轻了对跨尺度异方差的敏感性。\n\nE. 如果目标域与源域的不同仅在于微观尺度上更重的尾部和更大的方差，同时在每个 $X$ 处保持 $Y$ 的条件位置（例如，中位数）不变，那么在其他条件相同的情况下，一个用MAE训练的MLP预期会比用MSE训练的MLP迁移得更鲁棒，因为绝对损失下有界的每样本影响使得优化和泛化对增加的尾部厚重度不那么敏感，而平方损失下无界的影响可能会被新域中罕见的大残差所主导。\n\n选择所有适用项。",
            "solution": "在进行求解之前，对问题陈述进行验证。\n\n### 第1步：提取已知条件\n- **模型**：多层感知器（MLP）。\n- **映射**：从多尺度输入 $X$ 到输出 $Y$。\n- **数据生成过程**：$Y = g(X) + \\epsilon$，其中 $g$ 是一个确定性映射，$\\epsilon$ 是一个随机噪声项。\n- **噪声特征**：条件分布 $p(\\epsilon \\mid X)$ 是零中心的，即对于所有 $x$，$\\mathbb{E}[\\epsilon \\mid X=x] = 0$。噪声是异方差的：对于粗尺度输入，条件方差较小；对于微观尺度输入，条件方差较大，且分布可能具有重尾。\n- **训练**：在大小为 $n$ 的数据集上进行经验风险最小化（ERM）。\n- **损失函数**：\n    1.  均方误差（MSE）：$\\ell_{\\text{MSE}}(u,y) = \\|u-y\\|^{2}$。\n    2.  平均绝对误差（MAE）：$\\ell_{\\text{MAE}}(u,y) = \\|u-y\\|_{1}$。\n- **符号**：$u$ 表示模型预测值，$y$ 表示观测响应。范数的使用表明 $Y$ 可能是一个向量。\n- **理论量**：\n    - 期望风险：$R_{\\ell}(f) = \\mathbb{E}\\left[\\ell\\left(f(X), Y\\right)\\right]$。\n    - 逐点贝叶斯估计量：$u^{*}(x) \\in \\arg\\min_{u} \\mathbb{E}\\left[\\ell\\left(u, Y\\right) \\mid X=x\\right]$。\n- **评估**：使用与训练损失一致的验证指标。\n- **迁移学习场景**：迁移到一个相关领域，在该领域中，$p(Y \\mid X)$ 仅在微观尺度的尾部厚重度和方差上发生变化。$p(Y \\mid X)$ 的中心趋势（位置）在每个 $X$ 处大致保持不变。\n\n### 第2步：使用提取的已知条件进行验证\n1.  **科学性**：该问题在统计学习理论中有充分的依据。ERM、MSE和MAE损失、贝叶斯估计量、异方差性和迁移学习等概念是机器学习中的标准和核心内容。该设置描述了一个具有现实复杂性的常见回归问题。\n2.  **适定性**：该问题是适定的，要求基于一个明确定义的统计设置来评估几个陈述。有足够的信息来推导估计量和训练动态的属性。\n3.  **客观性**：语言精确客观，使用了统计学和机器学习的标准术语。问题陈述中没有主观或基于意见的主张。\n4.  **完整性和一致性**：问题陈述是自洽的。假设 $\\mathbb{E}[\\epsilon \\mid X=x] = 0$ 意味着 $\\mathbb{E}[Y \\mid X=x] = \\mathbb{E}[g(X) + \\epsilon \\mid X=x] = g(x)$。这与确定性目标映射 $g$ 和加性零中心噪声的概念是一致的。对迁移领域的描述也与此设置一致。\n5.  **无其他缺陷**：该问题并非微不足道、不切实际或不适定。它解决了鲁棒机器学习和模型选择中的一个核心概念问题。\n\n### 第3步：结论和行动\n问题陈述是**有效的**。将通过分析损失函数的属性及其在给定上下文中的含义来推导解决方案。\n\n### 推导与分析\n\n**1. 逐点贝叶斯估计量**\n逐点贝叶斯估计量 $u^*(x)$ 最小化以 $X=x$ 为条件的期望损失。\n\n- **对于 MSE（$\\ell_2$ 损失）**：我们寻求最小化 $J(u) = \\mathbb{E}\\left[\\|u-Y\\|^2 \\mid X=x\\right]$。假设 $Y$ 是 $\\mathbb{R}^d$ 中的一个随机向量，则 $J(u) = \\mathbb{E}\\left[\\sum_{j=1}^d (u_j - Y_j)^2 \\mid X=x\\right]$。\n对向量 $u$ 求梯度并令其为零：\n$$ \\nabla_u J(u) = \\nabla_u \\mathbb{E}\\left[(u-Y)^T(u-Y) \\mid X=x\\right] = \\mathbb{E}\\left[2(u-Y) \\mid X=x\\right] = 2(u - \\mathbb{E}[Y \\mid X=x]) $$\n令 $\\nabla_u J(u) = 0$ 得出 $u = \\mathbb{E}[Y \\mid X=x]$。因此，MSE 的贝叶斯估计量是 $Y$ 的条件期望（均值）。\n\n- **对于 MAE（$\\ell_1$ 损失）**：我们寻求最小化 $J(u) = \\mathbb{E}\\left[\\|u-Y\\|_1 \\mid X=x\\right] = \\mathbb{E}\\left[\\sum_{j=1}^d |u_j - Y_j| \\mid X=x\\right] = \\sum_{j=1}^d \\mathbb{E}\\left[|u_j - Y_j| \\mid X=x\\right]$。\n这个最小化可以逐分量进行。对于每个分量 $j$，我们最小化 $\\mathbb{E}\\left[|u_j - Y_j| \\mid X=x\\right]$。\n最小化这个表达式的 $u_j$ 值是给定 $X=x$ 时随机变量 $Y_j$ 的条件中位数。要理解这一点，设 $p(y_j|x)$ 为条件概率密度函数。对 $u_j$ 求导得：\n$$ \\frac{\\partial}{\\partial u_j} \\int_{-\\infty}^{\\infty} |u_j_ - y_j| p(y_j|x) dy_j = \\int_{-\\infty}^{u_j} p(y_j|x) dy_j - \\int_{u_j}^{\\infty} p(y_j|x) dy_j = P(Y_j \\le u_j \\mid X=x) - P(Y_j > u_j \\mid X=x) $$\n令其为零可得 $P(Y_j \\le u_j \\mid X=x) = 1/2$。这是中位数的定义。因此，MAE 的贝叶斯估计量是条件中位数的向量。\n\n**2. ERM 梯度分析**\n对于单个样本 $(x_i, y_i)$，损失为 $\\ell(f_\\theta(x_i), y_i)$。经验风险关于模型参数 $\\theta$ 的梯度包含项 $\\frac{\\partial \\ell(u_i, y_i)}{\\partial u_i} \\nabla_\\theta f_\\theta(x_i)$，其中 $u_i = f_\\theta(x_i)$。\n\n- **对于 MSE**：$\\ell_{\\text{MSE}}(u_i, y_i) = (u_i - y_i)^2$（为简单起见，考虑标量输出，其逻辑可扩展到向量）。关于预测值的导数为 $\\frac{\\partial \\ell}{\\partial u_i} = 2(u_i - y_i)$。梯度贡献与残差 $(u_i - y_i)$ 成正比。具有大残差的样本（来自高方差区域）将导致大梯度，从而主导优化过程。\n\n- **对于 MAE**：$\\ell_{\\text{MAE}}(u_i, y_i) = |u_i - y_i|$。关于预测值的次梯度为 $\\frac{\\partial \\ell}{\\partial u_i} = \\text{sign}(u_i - y_i)$，其值为 $-1$ 或 $1$（如果残差为零，则为 $[-1, 1]$ 中的任何值）。该项的量级以 $1$ 为界，与残差的大小无关。这限制了任何单个样本对梯度更新的影响，使得优化过程对异常值和异方差性更具鲁棒性。\n\n### 逐项分析\n\n**A. 在平方损失下最小化期望风险会诱导出等于条件期望 $\\mathbb{E}\\left[Y \\mid X=x\\right]$ 的逐点估计量 $u^{*}(x)$，而在绝对损失下最小化期望风险会诱导出等于给定 $X=x$ 时 $Y$ 的条件中位数的逐点估计量。在异方差多尺度噪声下，使用平方损失的ERM中的梯度贡献与残差大小成比例，导致高方差区域主导优化过程；而使用绝对损失时，每个样本的影响是有界的，从而增加了对此类异方差的鲁棒性。**\n\n该陈述由两部分组成。第一部分正确地指出了平方损失（条件均值）和绝对损失（条件中位数）的贝叶斯估计量，如我们的推导所示。第二部分正确地分析了ERM中的梯度，指出平方损失的梯度与残差大小成比例，而绝对损失的影响是有界的。这得出了正确的结论，即MAE在优化过程中对异方差性更具鲁棒性。该陈述的两个部分都是正确的。\n\n结论：**正确**。\n\n**B. 对于任何对称单峰条件分布 $p(Y \\mid X=x)$，MSE和MAE都会诱导出等于条件众数的相同最优估计量；此外，平方损失对异常值具有内在的鲁棒性，因为平均过程会减少其影响。**\n\n该陈述的第一部分是正确的。对于对称单峰分布，均值、中位数和众数重合。由于MSE估计量是均值，MAE估计量是中位数，因此它们将是相同的，并且也等于众数。然而，陈述的第二部分，“...平方损失对异常值具有内在的鲁棒性，因为平均过程会减少其影响”，是根本错误的。在MSE损失函数中对残差进行平方的行为放大了大误差（异常值）的影响，使得损失函数和相应的训练过程对异常值*敏感*，而不是鲁棒。平均是MSE和MAE的经验风险共同的特征，并不能赋予MSE任何特殊的鲁棒性。\n\n结论：**不正确**。\n\n**C. 如果在微观尺度上，$\\epsilon$ 的条件方差乘以一个因子 $c^{2}$（其中 $c>0$），而 $Y$ 的条件均值保持不变，那么平方损失下的贝叶斯估计量将按因子 $c$ 缩放（即，平方损失下的 $u^{*}(x)$ 随 $c$ 发生乘法变化），而绝对损失下的贝叶斯估计量保持不变。**\n\n平方损失下的贝叶斯估计量是条件均值，$u^*(x) = \\mathbb{E}[Y \\mid X=x]$。问题陈述指明“$Y$ 的条件均值保持不变”。因此，平方损失下的贝叶斯估计量也必须保持不变。声称它按因子 $c$ 缩放与前提直接矛盾。让我们更正式地分析一下：新的响应是 $Y' = g(X) + \\epsilon'$，其中 $\\text{Var}(\\epsilon'|X) = c^2 \\text{Var}(\\epsilon|X)$ 且 $\\mathbb{E}[Y'|X] = \\mathbb{E}[Y|X]$。新的贝叶斯估计量是 $\\mathbb{E}[Y'|X]$，根据前提，它是不变的。因此，该主张的第一部分是错误的。因此，整个陈述是不正确的。\n\n结论：**不正确**。\n\n**D. 使用MSE作为验证指标倾向于选择优先减少与高方差尺度相关的大残差的模型，这可能会以牺牲低方差尺度上的性能为代价；使用MAE作为验证指标通过线性惩罚残差来减少这种主导作用，从而减轻了对跨尺度异方差的敏感性。**\n\n该陈述涉及通过验证指标进行模型选择。验证指标指导最佳模型的选择（例如，从不同的超参数设置或训练周期中选择）。基于MSE的指标将是平方残差的平均值。一个在少数高方差点上实现非常低误差，即使以在许多低方差点上略高误差为代价的模型，也可以获得更低的总MSE。因此，MSE作为一种指标，优先考虑拟合高方差区域。相反，基于MAE的指标线性地惩罚残差，与MSE相比，赋予大残差较小的权重。这减少了高方差区域在总分中的主导地位，导致模型选择过程对异方差性不那么敏感。该陈述准确地描述了这种动态。\n\n结论：**正确**。\n\n**E. 如果目标域与源域的不同仅在于微观尺度上更重的尾部和更大的方差，同时在每个 $X$ 处保持 $Y$ 的条件位置（例如，中位数）不变，那么在其他条件相同的情况下，一个用MAE训练的MLP预期会比用MSE训练的MLP迁移得更鲁棒，因为绝对损失下有界的每样本影响使得优化和泛化对增加的尾部厚重度不那么敏感，而平方损失下无界的影响可能会被新域中罕见的大残差所主导。**\n\n该陈述探讨了训练模型的可迁移性。迁移问题的核心在于，理想的目标函数（条件位置/中心趋势）保持不变，而噪声分布变得更具挑战性（更重的尾部，更大的方差）。一个用MAE训练的模型旨在学习条件中位数。由于条件中位数在目标域中保持不变，因此用MAE训练的模型学习到的函数仍然是相关的。此外，由于其有界影响的梯度，MAE训练过程对异常值和重尾具有鲁棒性。在源域训练期间的这种鲁棒性使得模型能更好地逼近真实的、潜在的中位数函数，更少地受到特定噪声实现的影响。相反，用MSE训练的模型学习的是条件均值。虽然均值也可能保持不变，但训练过程对大残差高度敏感。它学习到的函数可能会因源数据中的异方差性而产生偏差。当在具有更重尾部的目标域中进行评估时，其性能可能会显著下降，因为平方误差指标将被现在更频繁和更大的异常值所主导。该陈述中的推理是合理的：MAE目标函数的内在鲁棒性导致模型对噪声分布的具体特征不那么敏感，使其更适合迁移到那些特征改变但底层信号不变的领域。\n\n结论：**正确**。",
            "answer": "$$\\boxed{ADE}$$"
        },
        {
            "introduction": "经典的机器学习理论提出了一个关于模型复杂性与泛化能力之间简单权衡的模型。然而，现代的高度过参数化神经网络常常挑战这一直觉。本练习  深入探讨了“双下降”现象，即随着模型大小远超训练数据点的数量，测试误差会先下降，然后上升，最后再次出人意料地下降。通过在神经正切核 (NTK) 框架下进行推理，您将对过参数化状态下的泛化行为、多尺度结构以及隐式正则化的作用获得更深刻、更现代的理解。",
            "id": "3825655",
            "problem": "您正在使用多层感知机（MLP）为双尺度材料模型构建一个代理模型。目标是根据复合材料的微观结构和载荷的描述符来预测其有效弹性模量。目标映射分解为 $y = f_{\\ell}(x) + \\eta(x) + \\varepsilon$，其中 $f_{\\ell}$ 是一个在 $x$ 上平滑的宏观尺度分量，$\\eta$ 是一个相对于学习到的表示呈现高频特性的微观尺度波动分量，而 $\\varepsilon$ 是方差为 $\\sigma^{2}$ 的独立测量噪声。您使用平方损失进行训练，不加显式正则化，并且在所有设置中，都通过增加优化步骤，在可能的情况下将训练误差驱动至（数值上的）零。假设宽网络极限证明了神经正切核（NTK）近似的合理性，因此当插值可行时，梯度下降收敛到所诱导的特征空间中的最小范数插值函数。\n\n您收集了 $N$ 个从一个固定的关于 $x$ 的训练分布中抽取的独立同分布的输入-输出对 $(x_{i},y_{i})$。考虑两个实验系列：\n\n- 系列 P：固定 $N$ 并改变可训练参数的数量 $P$，从欠参数化（$P \\ll N$）到高度过参数化（$P \\gg N$）。将 $P$ 的插值阈值定义为经验训练误差首次达到零时的最小 $P$ 值。\n\n- 系列 N：固定一个足够宽以使 NTK 稳定的架构及其初始化，并改变 $N$ 从小到大。将 $N$ 的插值阈值定义为对于该固定模型和优化协议，经验训练误差首次达到零时的最小 $N$ 值。\n\n在两个系列中，将预期测试均方误差定义为 $R = \\mathbb{E}_{x,\\mathcal{D}}[(\\hat{f}(x)-f(x))^{2}]$，其中 $f(x) = f_{\\ell}(x) + \\eta(x)$，$\\hat{f}$ 是学习到的预测器，期望是针对来自同一分布的新测试输入 $x$ 和训练数据集 $\\mathcal{D}$ 的随机抽取计算的，而 $\\varepsilon$ 仅通过其对学习到的 $\\hat{f}$ 的影响做出贡献。\n\n从偏差-方差分解 $R = \\text{bias}^{2} + \\text{variance} + \\text{irreducible noise}$ 和 NTK 特征空间中训练动态的线性化出发，推断经验格拉姆矩阵（Gram matrix）的条件数以及微观尺度分量 $\\eta$ 中的能量如何控制插值点附近的标签噪声和未解析高频内容的放大。利用这些原理评估 $R$ 作为 $P$ 或 $N$ 的函数如何表现，以及多尺度结构（例如，由大范围相关长度尺度引起的核特征值的缓慢衰减）如何改变这些行为。\n\n在当前的多尺度设置中，以下哪些陈述正确地描述了关于数据集大小和参数数量的双下降现象，并预测了预期测试误差 $R$ 先上升后下降的区间？\n\nA. 在系列 P 中，对于固定的 $N$ 和增加的 $P$，$R$ 最初减小（偏差减小），然后在插值阈值附近由于近奇异的经验格拉姆矩阵导致的方差放大而增加，然后在 $P \\gg N$ 时再次减小，因为 NTK 机制下的最小范数隐式偏置抑制了高频拟合；当组合的高频能量 $\\mathbb{E}[\\eta(x)^{2}] + \\sigma^{2}$ 更大时，峰值的高度也更高。\n\nB. 在系列 N 中，对于一个固定的、在无岭（ridgeless）NTK 机制下的极宽网络，$R$ 作为 $N$ 的函数是严格不增的，无论核的光谱以及 $\\mathbb{E}[\\eta(x)^{2}]$ 和 $\\sigma^{2}$ 的大小如何，因为更多的数据无一例外地总是会同时减少偏差和方差。\n\nC. 存在一个相对于学习到的核而言有效高频的大量微观尺度分量 $\\eta$，这会增加两个系列中插值点附近的方差项，因此减小 $\\eta$ 的振幅（例如，通过均匀化来平滑标签）会降低甚至消除双下降峰，可能恢复 $R$ 的单调下降趋势。\n\nD. 如果训练分布和测试分布仅在微观尺度内容的比例上有所不同（测试集比训练集具有更多的 $\\eta$ 能量），那么对于系列 P，在大的 $P$ 值下，$R$ 的双下降峰必然会消失，因为 MLP 的谱偏置永久性地偏好低频，并防止对高频测试内容的过拟合。\n\nE. 当训练分布上的 NTK 谱衰减缓慢时（例如，由于多尺度结构，特征值 $\\lambda_{j}$ 服从 $\\lambda_{j} \\sim j^{-p}$ 且 $p$ 很小），有效维度很大，并且经验格拉姆矩阵在插值点附近的更宽范围内是病态的（ill-conditioned），这使得 $R$ 的先升后降现象更加显著；添加显式的 $\\ell_{2}$ 正则化或早停可以收窄或抑制这个峰值。\n\n选择所有适用项。",
            "solution": "问题要求在多尺度材料背景下，对多层感知机（MLP）代理模型的预期测试误差 $R$ 进行分析，重点关注双下降现象。该分析需在神经正切核（NTK）的框架内进行。\n\n首先，我们根据问题陈述建立理论基础。我们处于宽网络极限下，其中 MLP 在梯度下降下的训练动态等效于使用 NTK（表示为 $K(x, x')$）的核方法。由于训练使用平方损失，无显式正则化，并被驱动至零误差，因此得到的预测器 $\\hat{f}$ 是 NTK 的再生核希尔伯特空间（RKHS）中的最小范数插值函数。对于一个训练数据集 $\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^N$，该预测器由以下公式给出：\n$$ \\hat{f}(x) = \\mathbf{k}(x)^T \\mathbf{K}^{-1} \\mathbf{y} $$\n其中 $\\mathbf{k}(x)$ 是核函数求值向量 $[K(x, x_1), \\dots, K(x, x_N)]^T$，$\\mathbf{K}$ 是 $N \\times N$ 的格拉姆矩阵，其元素为 $K_{ij} = K(x_i, x_j)$，$\\mathbf{y} = [y_1, \\dots, y_N]^T$ 是训练标签向量。\n\n标签 $y_i$ 由 $y_i = f_{\\ell}(x_i) + \\eta(x_i) + \\varepsilon_i$ 给出。我们旨在学习的真实函数是 $f(x) = f_{\\ell}(x) + \\eta(x)$。因此，训练标签向量可以写为 $\\mathbf{y} = \\mathbf{f} + \\boldsymbol{\\varepsilon}$，其中 $\\mathbf{f} = [f(x_1), \\dots, f(x_N)]^T$ 且 $\\boldsymbol{\\varepsilon} = [\\varepsilon_1, \\dots, \\varepsilon_N]^T$。那么预测器为：\n$$ \\hat{f}(x) = \\mathbf{k}(x)^T \\mathbf{K}^{-1} (\\mathbf{f} + \\boldsymbol{\\varepsilon}) $$\n预期测试误差为 $R = \\mathbb{E}_{x,\\mathcal{D}}[(\\hat{f}(x) - f(x))^2]$。我们可以将其分解为偏差和方差：\n$$ R = \\text{Bias}^2 + \\text{Variance} $$\n在噪声 $\\boldsymbol{\\varepsilon}$ 的多次抽取下（假设训练输入 $x_i$ 固定），平均预测器为 $\\bar{f}(x) = \\mathbb{E}_{\\boldsymbol{\\varepsilon}}[\\hat{f}(x)] = \\mathbf{k}(x)^T \\mathbf{K}^{-1} \\mathbf{f}$。\n平方偏差为 $\\text{Bias}^2 = \\mathbb{E}_{x,\\mathcal{D}}[(\\bar{f}(x) - f(x))^2]$。该项衡量了模型系统性地无法表示真实函数 $f(x) = f_{\\ell}(x) + \\eta(x)$ 的能力。由于 NTK 的谱偏置偏好平滑的低频函数，因此在逼近高频分量 $\\eta(x)$ 时会存在显著的偏差。\n方差为 $\\text{Variance} = \\mathbb{E}_{x,\\mathcal{D}}[(\\hat{f}(x) - \\bar{f}(x))^2] = \\mathbb{E}_{x,\\mathcal{D}}[(\\mathbf{k}(x)^T \\mathbf{K}^{-1} \\boldsymbol{\\varepsilon})^2]$。给定 $\\mathbb{E}[\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}^T] = \\sigma^2 \\mathbf{I}$，这变为：\n$$ \\text{Variance} = \\sigma^2 \\mathbb{E}_{\\mathcal{D}}[\\mathbf{k}(x)^T \\mathbf{K}^{-2} \\mathbf{k}(x)] $$\n该方差项量化了预测器因训练标签中的随机噪声 $\\varepsilon$ 而发生变化的程度。至关重要的是，其大小由矩阵的逆 $\\mathbf{K}^{-1}$（或其平方）控制。\n\n双下降现象源于 $\\mathbf{K}^{-1}$ 在插值阈值处的行为。当模型复杂度（在系列 P 中与 $P$ 相关，或在系列 N 中与 $N$ 成反比）刚好足以拟合 $N$ 个训练点时，就达到了这个阈值。此时，格拉姆矩阵 $\\mathbf{K}$ 变得奇异或近奇异。$\\mathbf{K}^{-1}$ 和 $\\mathbf{K}^{-2}$ 的范数会爆炸式增长，导致方差项出现一个尖锐的峰值，从而也导致总测试误差 $R$ 出现峰值。\n\n此外，模型被强制去插值 $y_i = f(x_i) + \\varepsilon_i$。与信号平滑部分 $f_{\\ell}(x_i)$ 的偏差是 $\\eta(x_i) + \\varepsilon_i$。模型必须拟合这种组合的“有效噪声”。$\\eta(x)$ 的高频特性使其从平滑核的角度看表现得像噪声，因此拟合 $\\eta$ 的挑战加剧了峰值附近的过拟合问题，类似于拟合 $\\varepsilon$。\n\n有了这个基础，我们就可以分析每个陈述了。\n\n**A. 在系列 P 中，对于固定的 $N$ 和增加的 $P$，$R$ 最初减小（偏差减小），然后在插值阈值附近由于近奇异的经验正规矩阵（empirical normal matrix）导致的方差放大而增加，然后在 $P \\gg N$ 时再次减小，因为 NTK 机制下的最小范数隐式偏置抑制了高频拟合；当组合的高频能量 $\\mathbb{E}[\\eta(x)^{2}] + \\sigma^{2}$ 更大时，峰值的高度也更高。**\n\n这个陈述准确地描述了模型层面的双下降曲线。\n1.  当 $P \\ll N$（欠参数化）时，增加 $P$ 减少偏差的速度快于方差增加的速度，因此 $R$ 下降。这是经典的学习机制。\n2.  在插值阈值附近（$P \\approx N_{crit}$），模型刚刚获得足够的容量来进行插值。格拉姆矩阵 $\\mathbf{K}$ 变得病态，其逆矩阵急剧增大，方差项也急剧飙升。\n3.  当 $P \\gg N$（过参数化）时，我们处于 NTK 机制中，梯度下降会找到最小 RKHS 范数的插值函数。这种隐式正则化偏好更平滑的解，从而抑制了方差。$R$ 再次下降。\n峰值的高度由被过拟合的内容的量级决定。模型被迫拟合与平滑函数的总偏差，这既包括测量噪声 $\\varepsilon$，也包括结构性高频分量 $\\eta$。这种有效噪声的总方差是 $\\text{Var}(\\eta(x) + \\varepsilon) = \\mathbb{E}[\\eta(x)^2] + \\sigma^2$（假设独立）。这个和的值越大，意味着有更多的高频内容需要由一个不稳定的模型来拟合，从而导致更高的方差峰值。该陈述完全正确。\n结论：**正确**。\n\n**B. 在系列 N 中，对于一个固定的、在无岭（ridgeless）NTK 机制下的极宽网络，$R$ 作为 $N$ 的函数是严格不增的，无论核的光谱以及 $\\mathbb{E}[\\eta(x)^{2}]$ 和 $\\sigma^{2}$ 的大小如何，因为更多的数据无一例外地总是会同时减少偏差和方差。**\n\n该陈述声称样本层面的双下降不会发生。这在事实上是错误的。对于一个固定的、高度过参数化的模型，双下降现象同样会作为样本数量 $N$ 的函数被观察到。其机制与模型层面的双下降类似。对于一个固定的模型，存在一个有效复杂度或维度，我们称之为 $d_{eff}$。\n1.  当 $N \\ll d_{eff}$ 时，模型实际上是过参数化的，我们处于“现代”插值机制中。\n2.  随着 $N$ 增加并接近 $d_{eff}$，格拉姆矩阵 $\\mathbf{K}$ 再次变得病态，导致方差尖峰和测试误差 $R$ 的峰值。\n3.  当 $N \\gg d_{eff}$ 时，模型相对于数据变得欠参数化，我们进入了经典学习机制，其中增加更多数据会单调地提升性能。\n“更多的数据无一例外地总是会同时减少偏差和方差”这一说法是经典统计学的一个原则，但在现代过参数化设置中，在插值阈值附近，这一原则被打破了。\n结论：**错误**。\n\n**C. 存在一个相对于学习到的核而言有效高频的大量微观尺度分量 $\\eta$，这会增加两个系列中插值点附近的方差项，因此减小 $\\eta$ 的振幅（例如，通过均匀化来平滑标签）会降低甚至消除双下降峰，可能恢复 $R$ 的单调下降趋势。**\n\n该陈述正确地指出了微观尺度分量 $\\eta$ 的作用。如前所述，$\\eta$ 充当了一种结构性噪声或模型设定误差噪声。为了实现零训练误差，模型必须在训练点上插值 $\\eta(x_i)$ 的值。由于 $\\eta$ 是高频的，这需要一个复杂的、高范数的解，而这样的解泛化能力很差，从而导致了插值点附近的误差峰值。被病态的逆矩阵 $\\mathbf{K}^{-1}$ 放大的总“噪声”实际上是 $\\eta$ 和 $\\varepsilon$ 的组合。如果 $\\eta$ 的振幅减小（例如，通过对标签进行滤波或均匀化，这将产生一个更平滑的目标函数），那么被过拟合的分量的量级就会变小。这将直接导致一个较低的方差尖峰。如果来自 $\\eta$ 和 $\\varepsilon$ 的组合有效噪声足够小，峰值可能会变得可以忽略不计，测试误差曲线可能会呈现单调性。\n结论：**正确**。\n\n**D. 如果训练分布和测试分布仅在微观尺度内容的比例上有所不同（测试集比训练集具有更多的 $\\eta$ 能量），那么对于系列 P，在大的 $P$ 值下，$R$ 的双下降峰必然会消失，因为 MLP 的谱偏置永久性地偏好低频，并防止对高频测试内容的过拟合。**\n\n这个陈述有几个缺陷。首先，双下降峰出现在插值阈值附近，而不是在“大的 $P$ 值”处。在大的 $P \\gg N$ 时，模型处于第二个下降阶段，已经越过了峰值。其提供的推理也是有缺陷的。模型是在训练分布（低 $\\eta$ 含量）上训练的。在过参数化的 NTK 机制下（$P \\gg N$），最小范数偏置将产生一个插值训练数据的平滑函数 $\\hat{f}$。因为训练数据是平滑的，$\\hat{f}$ 本身也将是平滑的。当这个平滑的预测器 $\\hat{f}(x)$ 在测试分布上被评估时，由于真实函数 $f_{test}(x) = f_{\\ell}(x) + \\eta_{test}(x)$ 在 $\\eta_{test}$ 中有很高的能量，将会出现巨大的不匹配。测试误差 $R = \\mathbb{E}[(\\hat{f}(x) - f_{test}(x))^2]$ 将会很大，主要由模型未能学到的那部分函数的能量决定，即近似为 $\\mathbb{E}[\\eta_{test}(x)^2]$。谱偏置并不能防止高误差；相反，它导致了相对于高频测试函数的巨大偏差，从而导致高测试误差。模型不是在“过拟合测试内容”，它只是一个对于测试分布而言很差的模型。\n结论：**错误**。\n\n**E. 当训练分布上的 NTK 谱衰减缓慢时（例如，由于多尺度结构，特征值 $\\lambda_{j}$ 服从 $\\lambda_{j} \\sim j^{-p}$ 且 $p$ 很小），有效维度很大，并且经验格拉姆矩阵在插值点附近的更宽范围内是病态的（ill-conditioned），这使得 $R$ 的先升后降现象更加显著；添加显式的 $\\ell_{2}$ 正则化或早停可以收窄或抑制这个峰值。**\n\n这个陈述是对核的光谱特性影响的精辟且正确的分析。\n1.  特征值 $\\lambda_j$ 的缓慢衰减（$\\lambda_j \\sim j^{-p}$ 中的 $p$ 很小）表明核可以表示复杂的、“粗糙的”函数。这对于具有多尺度结构的问题是典型的。\n2.  缓慢的光谱衰减意味着一个大的有效维度，因为许多本征模都是显著的。\n3.  这意味着从过参数化机制（$N \\ll d_{eff}$）到欠参数化机制（$N \\gg d_{eff}$）的过渡不那么尖锐。格拉姆矩阵 $\\mathbf{K}$ 将在更宽的 $N$（或 $P$）范围内是病态的，使得双下降峰更宽且更“显著”。\n4.  显式的 $\\ell_2$ 正则化（岭回归）将 $\\mathbf{K}^{-1}$ 替换为 $(\\mathbf{K} + \\lambda\\mathbf{I})^{-1}$，对于任何 $\\lambda > 0$ 这都是良态的（well-conditioned）。这直接对逆矩阵进行正则化，抑制了方差爆炸并压制了峰值。\n5.  梯度下降的早停是一种隐式正则化形式，已知它等效于一种谱滤波操作，可以防止拟合收敛到噪声和高频分量。这也抑制了方差并压制了峰值。\n整个陈述与高等学习理论是一致的。\n结论：**正确**。",
            "answer": "$$\\boxed{ACE}$$"
        }
    ]
}