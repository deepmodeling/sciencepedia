{
    "hands_on_practices": [
        {
            "introduction": "The first step in constructing a Spectral Neighbor Analysis Potential is to uniquely represent the local atomic environment. SNAP achieves this by mapping each neighbor's three-dimensional position onto the surface of a four-dimensional hypersphere using stereographic projection. This practice  makes this abstract concept tangible by guiding you through the explicit calculation of the hyperspherical neighbor density $\\rho_i(\\Omega)$ for a simple arrangement, and explores how the mapping parameter $r_0$ influences the representation of near and far neighbors.",
            "id": "3808203",
            "problem": "Consider the Spectral Neighbor Analysis Potentials (SNAP) representation of local atomic environments. In SNAP, each neighbor of a central atom at position $i$ is mapped from three-dimensional Euclidean coordinates to a point on the three-sphere via stereographic projection controlled by a positive scale parameter $r_{0}$. Let the hyperspherical coordinate be denoted by $\\Omega=(\\alpha,\\theta,\\phi)$, where $\\alpha \\in [0,\\pi)$ is the stereographic angle associated with the radial distance $r$, and $(\\theta,\\phi)$ are the usual polar and azimuthal angles on the unit sphere.\n\nAssume that the central atom $i$ has three neighbors located along three vertices of a regular tetrahedron centered at the origin, with unit direction vectors\n- $\\mathbf{v}_{1}=\\frac{1}{\\sqrt{3}}(1,1,1)$,\n- $\\mathbf{v}_{2}=\\frac{1}{\\sqrt{3}}(1,-1,-1)$,\n- $\\mathbf{v}_{3}=\\frac{1}{\\sqrt{3}}(-1,1,-1)$.\n\nThe corresponding spherical angles are defined by $\\theta=\\arccos(z/r)$ and $\\phi=\\operatorname{arctan2}(y,x)$, using the standard conventions with $\\phi \\in [0,2\\pi)$.\n\nLet the neighbor distances be $r_{1}=r_{2}=2.2$ and $r_{3}=3.6$, in Angstrom units. Assume a sharp cutoff function $f_{c}(r)$ that equals $1$ for $r<R_{c}$ and $0$ otherwise, with $R_{c}=4.5$, and assign all neighbor type weights $w_{j}=1$.\n\n1. Using the SNAP stereographic projection whose radial angle satisfies $\\tan(\\alpha/2)=r/r_{0}$, compute explicitly the SNAP neighbor density $\\rho_{i}(\\Omega)$ for the choice $r_{0}=2.0$, in terms of Dirac delta functions supported at the mapped neighbor coordinates $\\Omega_{j}=(\\alpha_{j},\\theta_{j},\\phi_{j})$.\n2. To quantify how $r_{0}$ alters the relative weighting of near and far neighbors in the $\\alpha$ coordinate, define the map-induced local scaling $w(r;r_{0})=\\frac{d\\alpha}{dr}$ and the near-to-far ratio $W(r_{0})=\\frac{w(r_{\\text{near}};r_{0})}{w(r_{\\text{far}};r_{0})}$ with $r_{\\text{near}}=2.2$ and $r_{\\text{far}}=3.6$. Compute the single scalar\n$$Q=\\frac{W(r_{0}=2.0)}{W(r_{0}=4.0)}.$$\n\nReport $Q$ as a dimensionless number rounded to four significant figures. Angles should be in radians throughout. No units should appear in your final boxed answer.",
            "solution": "The problem is first subjected to a rigorous validation process.\n\n### Step 1: Extract Givens\n- **Model**: Spectral Neighbor Analysis Potentials (SNAP).\n- **Mapping**: Stereographic projection from 3D Euclidean coordinates to the three-sphere.\n- **Hyperspherical Coordinates**: $\\Omega = (\\alpha, \\theta, \\phi)$.\n- **Stereographic Angle Formula**: $\\tan(\\alpha/2) = r/r_0$, with $\\alpha \\in [0, \\pi)$ and $r_0 > 0$.\n- **Neighbor Direction Vectors**:\n  - $\\mathbf{v}_{1} = \\frac{1}{\\sqrt{3}}(1, 1, 1)$\n  - $\\mathbf{v}_{2} = \\frac{1}{\\sqrt{3}}(1, -1, -1)$\n  - $\\mathbf{v}_{3} = \\frac{1}{\\sqrt{3}}(-1, 1, -1)$\n- **Neighbor Distances**: $r_{1} = 2.2$, $r_{2} = 2.2$, $r_{3} = 3.6$.\n- **Spherical Angle Definitions**: $\\theta = \\arccos(z/r)$, $\\phi = \\operatorname{arctan2}(y, x)$ for $\\phi \\in [0, 2\\pi)$.\n- **Cutoff Function**: $f_{c}(r) = 1$ for $r < R_{c}$ and $f_{c}(r) = 0$ for $r \\ge R_{c}$.\n- **Cutoff Radius**: $R_{c} = 4.5$.\n- **Neighbor Weights**: $w_{j} = 1$ for all neighbors.\n- **Part 1 Requirement**: Compute the neighbor density $\\rho_{i}(\\Omega)$ for $r_{0}=2.0$.\n- **Part 2 Definitions**:\n  - Map-induced local scaling: $w(r; r_{0}) = \\frac{d\\alpha}{dr}$.\n  - Near-to-far ratio: $W(r_{0}) = \\frac{w(r_{\\text{near}}; r_{0})}{w(r_{\\text{far}}; r_{0})}$.\n  - Given distances: $r_{\\text{near}} = 2.2$, $r_{\\text{far}} = 3.6$.\n- **Part 2 Requirement**: Compute $Q = \\frac{W(r_{0}=2.0)}{W(r_{0}=4.0)}$ rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed for validity.\n1.  **Scientific Grounding**: The problem is grounded in the established mathematical framework of the SNAP interatomic potential, a standard method in computational materials science. All definitions and equations are consistent with the scientific literature on this topic.\n2.  **Well-Posedness**: The problem provides all necessary data and relationships to compute the requested quantities. For Part 1, the neighbor density is fully determined by the specified neighbor positions and weights. For Part 2, the quantity $Q$ is derived from a well-defined function whose parameters are all specified. A unique solution exists.\n3.  **Objectivity**: The problem is stated in precise, mathematical language, free of ambiguity or subjective claims.\n4.  **Consistency**: The givens are self-consistent. The neighbor distances $r_1=2.2$, $r_2=2.2$, and $r_3=3.6$ are all less than the cutoff radius $R_c=4.5$, so all neighbors are included in the calculation, which is a consistent setup.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid** as it is scientifically grounded, well-posed, objective, and internally consistent. A full solution will be provided.\n\n### Solution\n\n**Part 1: Computation of the Neighbor Density $\\rho_{i}(\\Omega)$**\n\nThe SNAP neighbor density $\\rho_{i}(\\Omega)$ for a central atom $i$ is defined as a sum over its neighbors $j$:\n$$\n\\rho_{i}(\\Omega) = \\sum_{j} w_{j} f_{c}(r_{j}) \\delta(\\Omega - \\Omega_{j})\n$$\nwhere $\\Omega = (\\alpha, \\theta, \\phi)$ is a point on the three-sphere, $\\Omega_{j} = (\\alpha_{j}, \\theta_{j}, \\phi_{j})$ are the hyperspherical coordinates of neighbor $j$, $w_j$ are neighbor-type weights, $f_c(r_j)$ is a cutoff function, and $\\delta(\\cdot)$ is the Dirac delta function on the three-sphere.\n\nThe problem states that $w_{j} = 1$ for all neighbors. The cutoff radius is $R_{c} = 4.5$. The neighbor distances are $r_{1} = 2.2$, $r_{2} = 2.2$, and $r_{3} = 3.6$. Since all $r_j < R_c$, the cutoff function $f_{c}(r_j) = 1$ for all three neighbors. Thus, the expression for the density simplifies to:\n$$\n\\rho_{i}(\\Omega) = \\sum_{j=1}^{3} \\delta(\\Omega - \\Omega_{j})\n$$\nWe need to compute the hyperspherical coordinates $\\Omega_{j} = (\\alpha_{j}, \\theta_{j}, \\phi_{j})$ for each neighbor with $r_{0}=2.0$.\n\nFirst, we compute the stereographic angle $\\alpha_j$ for each neighbor using the relation $\\tan(\\alpha_j/2) = r_j / r_0$.\n\nFor neighbors $j=1$ and $j=2$, we have $r_{1}=r_{2}=2.2$.\n$$\n\\tan(\\alpha_{1,2}/2) = \\frac{2.2}{2.0} = 1.1 \\implies \\alpha_{1,2} = 2 \\arctan(1.1)\n$$\nFor neighbor $j=3$, we have $r_{3}=3.6$.\n$$\n\\tan(\\alpha_{3}/2) = \\frac{3.6}{2.0} = 1.8 \\implies \\alpha_{3} = 2 \\arctan(1.8)\n$$\n\nNext, we compute the spherical angles $(\\theta_j, \\phi_j)$ from the unit direction vectors $\\mathbf{v}_j = (\\hat{x}_j, \\hat{y}_j, \\hat{z}_j)$. The definitions are $\\theta_j = \\arccos(\\hat{z}_j)$ and $\\phi_j = \\operatorname{arctan2}(\\hat{y}_j, \\hat{x}_j)$.\n\nFor neighbor $j=1$: $\\mathbf{v}_{1} = \\frac{1}{\\sqrt{3}}(1, 1, 1)$.\n- $\\hat{z}_1 = \\frac{1}{\\sqrt{3}} \\implies \\theta_1 = \\arccos\\left(\\frac{1}{\\sqrt{3}}\\right)$.\n- $\\hat{y}_1 = \\frac{1}{\\sqrt{3}}, \\hat{x}_1 = \\frac{1}{\\sqrt{3}} \\implies \\phi_1 = \\operatorname{arctan2}(1, 1) = \\frac{\\pi}{4}$.\n\nFor neighbor $j=2$: $\\mathbf{v}_{2} = \\frac{1}{\\sqrt{3}}(1, -1, -1)$.\n- $\\hat{z}_2 = -\\frac{1}{\\sqrt{3}} \\implies \\theta_2 = \\arccos\\left(-\\frac{1}{\\sqrt{3}}\\right)$.\n- $\\hat{y}_2 = -\\frac{1}{\\sqrt{3}}, \\hat{x}_2 = \\frac{1}{\\sqrt{3}} \\implies \\phi_2 = \\operatorname{arctan2}(-1, 1) = -\\frac{\\pi}{4}$. Since $\\phi \\in [0, 2\\pi)$, we take $\\phi_2 = 2\\pi - \\frac{\\pi}{4} = \\frac{7\\pi}{4}$.\n\nFor neighbor $j=3$: $\\mathbf{v}_{3} = \\frac{1}{\\sqrt{3}}(-1, 1, -1)$.\n- $\\hat{z}_3 = -\\frac{1}{\\sqrt{3}} \\implies \\theta_3 = \\arccos\\left(-\\frac{1}{\\sqrt{3}}\\right)$.\n- $\\hat{y}_3 = \\frac{1}{\\sqrt{3}}, \\hat{x}_3 = -\\frac{1}{\\sqrt{3}} \\implies \\phi_3 = \\operatorname{arctan2}(1, -1) = \\pi - \\frac{\\pi}{4} = \\frac{3\\pi}{4}$.\n\nThe neighbor coordinates are:\n- $\\Omega_{1} = \\left(2 \\arctan(1.1), \\arccos\\left(\\frac{1}{\\sqrt{3}}\\right), \\frac{\\pi}{4}\\right)$\n- $\\Omega_{2} = \\left(2 \\arctan(1.1), \\arccos\\left(-\\frac{1}{\\sqrt{3}}\\right), \\frac{7\\pi}{4}\\right)$\n- $\\Omega_{3} = \\left(2 \\arctan(1.8), \\arccos\\left(-\\frac{1}{\\sqrt{3}}\\right), \\frac{3\\pi}{4}\\right)$\n\nThe neighbor density is therefore:\n$$\n\\rho_{i}(\\Omega) = \\delta(\\Omega - \\Omega_1) + \\delta(\\Omega - \\Omega_2) + \\delta(\\Omega - \\Omega_3)\n$$\nwith the coordinates $\\Omega_1, \\Omega_2, \\Omega_3$ as specified above.\n\n**Part 2: Computation of the Scalar $Q$**\n\nThe scalar $Q$ is defined as $Q=\\frac{W(r_{0}=2.0)}{W(r_{0}=4.0)}$, where $W(r_{0})=\\frac{w(r_{\\text{near}};r_{0})}{w(r_{\\text{far}};r_{0})}$ and $w(r;r_{0})=\\frac{d\\alpha}{dr}$.\n\nFirst, we derive an explicit expression for $w(r; r_{0})$. Starting from $\\tan(\\alpha/2) = r/r_{0}$, we differentiate with respect to $r$:\n$$\n\\frac{d}{dr} \\left( \\tan\\left(\\frac{\\alpha}{2}\\right) \\right) = \\frac{d}{dr} \\left( \\frac{r}{r_0} \\right)\n$$\n$$\n\\sec^2\\left(\\frac{\\alpha}{2}\\right) \\cdot \\frac{1}{2} \\frac{d\\alpha}{dr} = \\frac{1}{r_0}\n$$\nSolving for $\\frac{d\\alpha}{dr}$:\n$$\n\\frac{d\\alpha}{dr} = \\frac{2}{r_0 \\sec^2(\\alpha/2)}\n$$\nUsing the identity $\\sec^2(x) = 1 + \\tan^2(x)$ and substituting $\\tan(\\alpha/2) = r/r_0$:\n$$\nw(r; r_0) = \\frac{d\\alpha}{dr} = \\frac{2}{r_0 \\left(1 + \\tan^2(\\alpha/2)\\right)} = \\frac{2}{r_0 \\left(1 + (r/r_0)^2\\right)} = \\frac{2}{r_0 \\left(\\frac{r_0^2 + r^2}{r_0^2}\\right)} = \\frac{2r_0}{r^2 + r_0^2}\n$$\nNow, we can write the expression for the near-to-far ratio $W(r_0)$:\n$$\nW(r_{0}) = \\frac{w(r_{\\text{near}}; r_{0})}{w(r_{\\text{far}}; r_{0})} = \\frac{2r_0 / (r_{\\text{near}}^2 + r_0^2)}{2r_0 / (r_{\\text{far}}^2 + r_0^2)} = \\frac{r_{\\text{far}}^2 + r_0^2}{r_{\\text{near}}^2 + r_0^2}\n$$\nWe are given $r_{\\text{near}} = 2.2$ and $r_{\\text{far}} = 3.6$. So, $r_{\\text{near}}^2 = (2.2)^2 = 4.84$ and $r_{\\text{far}}^2 = (3.6)^2 = 12.96$.\nThe expression becomes:\n$$\nW(r_0) = \\frac{12.96 + r_0^2}{4.84 + r_0^2}\n$$\nNext, we evaluate $W(r_0)$ for the two specified values of $r_0$.\n\nFor $r_0=2.0$: $r_0^2 = 4.0$.\n$$\nW(r_0=2.0) = \\frac{12.96 + 4.0}{4.84 + 4.0} = \\frac{16.96}{8.84}\n$$\nFor $r_0=4.0$: $r_0^2 = 16.0$.\n$$\nW(r_0=4.0) = \\frac{12.96 + 16.0}{4.84 + 16.0} = \\frac{28.96}{20.84}\n$$\nFinally, we compute the required ratio $Q$:\n$$\nQ = \\frac{W(r_{0}=2.0)}{W(r_{0}=4.0)} = \\frac{16.96 / 8.84}{28.96 / 20.84} = \\frac{16.96}{8.84} \\times \\frac{20.84}{28.96}\n$$\n$$\nQ = \\frac{16.96 \\times 20.84}{8.84 \\times 28.96} = \\frac{353.4304}{255.9904} \\approx 1.380650005...\n$$\nRounding this result to four significant figures, we get $1.381$.",
            "answer": "$$\n\\boxed{1.381}\n$$"
        },
        {
            "introduction": "Once the neighbor positions are mapped, the next crucial step is to generate descriptors that are invariant to rotations of the atomic system. This exercise  delves into this process by having you compute a simple rotational invariant, a bispectrum-like component, for a highly symmetric atomic arrangement. By then perturbing the system, you will gain direct insight into how these powerful descriptors quantify angular anisotropy, a key factor in determining material properties.",
            "id": "3808216",
            "problem": "Consider a rotational-invariant descriptor constructed from the angular neighbor density used in Spectral Neighbor Analysis Potentials (SNAP). Let a central atom be at the origin and its neighbor environment be represented by the angular neighbor density on the unit sphere,\n$$\n\\rho(\\hat{\\mathbf{r}}) = \\sum_{i=1}^{N} w_{i} \\,\\delta\\!\\left(\\hat{\\mathbf{r}} - \\hat{\\mathbf{r}}_{i}\\right),\n$$\nwhere $\\hat{\\mathbf{r}}$ is a unit vector on the sphere, $\\hat{\\mathbf{r}}_{i}$ denotes the direction to neighbor $i$, and $w_{i}$ is a finite positive weight. The spherical harmonic projection coefficients are defined by\n$$\nu_{l m} \\equiv \\int_{S^{2}} Y_{l m}^{*}(\\hat{\\mathbf{r}})\\,\\rho(\\hat{\\mathbf{r}})\\,d\\Omega = \\sum_{i=1}^{N} w_{i}\\,Y_{l m}^{*}(\\hat{\\mathbf{r}}_{i}),\n$$\nwith $Y_{l m}$ the orthonormal spherical harmonics. In particular, for $l=1$,\n$$\nY_{1,0}(\\theta,\\phi) = \\sqrt{\\frac{3}{4\\pi}}\\,\\cos\\theta,\\qquad\nY_{1,\\pm 1}(\\theta,\\phi) = \\mp \\sqrt{\\frac{3}{8\\pi}}\\,\\sin\\theta\\,\\exp(\\pm i\\phi).\n$$\nA lowest-order rotational invariant constructed from coupling two $l=1$ channels into total angular momentum $l=0$ can be expressed as a scalar depending only on the set $\\{u_{1 m}\\}_{m=-1}^{1}$; denote this bispectrum-like scalar by $B_{1\\,1\\,0}$. Starting from the above definitions and the fact that the $l=0$ coupling eliminates directional dependence, derive an explicit expression for $B_{1\\,1\\,0}$ in terms of $\\{u_{1 m}\\}$ and evaluate it for the following symmetric triatomic configuration: two identical neighbors of equal weight $w>0$ located at a common distance $r_{0}$ on the $+z$ and $-z$ axes, i.e., at $(\\theta,\\phi)=(0,0)$ and $(\\pi,0)$. You may neglect radial structure and take only the angular projection on the unit sphere. Express your final answer as a dimensionless number.\n\nThen, briefly interpret how $B_{1\\,1\\,0}$ responds to angular anisotropy by considering a perturbation that tilts the neighbor at $(\\pi,0)$ to $(\\pi-\\varepsilon,0)$ for $0<\\varepsilon\\ll 1$. Determine the leading-order scaling of $B_{1\\,1\\,0}$ with $\\varepsilon$ and discuss the physical reason for that scaling. No rounding is required for the main computed value.",
            "solution": "The problem is first validated to be self-contained, scientifically grounded, and well-posed. The givens are extracted and analyzed. All definitions, including the angular neighbor density $\\rho(\\hat{\\mathbf{r}})$, the spherical harmonic projection coefficients $u_{l m}$, and the specific forms of $Y_{1m}$, are standard in the field of many-body atomic potentials and angular momentum theory. The configuration described is a simple, physically meaningful setup (a linear triatomic molecule). The tasks are clear and solvable using established principles. The problem is therefore deemed valid.\n\nThe first task is to derive an explicit expression for the rotational invariant $B_{1\\,1\\,0}$ in terms of the coefficients $\\{u_{1 m}\\}$. A rotational invariant formed by coupling two tensors of rank $l=1$ to a total angular momentum of $L=0$ is proportional to their scalar product. For the bispectrum-like quantity $B_{1\\,1\\,0}$, this involves the self-coupling of the set of coefficients $\\{u_{1m}\\}$. The simplest and most fundamental such invariant is the sum of the squared magnitudes of the coefficients, which represents the squared norm of the vector whose spherical tensor components are the $u_{1m}$. We thus define $B_{1\\,1\\,0}$ as:\n$$\nB_{1\\,1\\,0} \\equiv \\sum_{m=-1}^{1} |u_{1m}|^2\n$$\nThe property $u_{l,-m} = (-1)^{-m} (u_{lm})^*$ ensures this quantity is real. This is a specific case of the power spectrum in SNAP, which is $\\sum_{m=-l}^{l} |u_{lm}|^2$ for a given $l$.\n\nThe second task is to evaluate $B_{1\\,1\\,0}$ for a specific symmetric configuration. The system consists of a central atom at the origin and two neighbors, indexed $i=1,2$, with equal weight $w > 0$. Their locations on the unit sphere are given by $(\\theta_1, \\phi_1) = (0, 0)$ and $(\\theta_2, \\phi_2) = (\\pi, 0)$.\n\nThe coefficients $u_{1m}$ are calculated from their definition:\n$$\nu_{1m} = \\sum_{i=1}^{2} w_{i} Y_{1m}^*(\\hat{\\mathbf{r}}_i) = w \\left( Y_{1m}^*(\\theta_1, \\phi_1) + Y_{1m}^*(\\theta_2, \\phi_2) \\right)\n$$\nWe need to evaluate the spherical harmonics $Y_{1m}$ at the two locations.\nFor neighbor $1$ at $(\\theta,\\phi)=(0,0)$:\n$\\cos(0)=1$, $\\sin(0)=0$.\n$Y_{1,0}(0,0) = \\sqrt{\\frac{3}{4\\pi}} \\cos(0) = \\sqrt{\\frac{3}{4\\pi}}$.\n$Y_{1,\\pm 1}(0,0) = \\mp \\sqrt{\\frac{3}{8\\pi}} \\sin(0) \\exp(\\pm i \\cdot 0) = 0$.\n\nFor neighbor $2$ at $(\\theta,\\phi)=(\\pi,0)$:\n$\\cos(\\pi)=-1$, $\\sin(\\pi)=0$.\n$Y_{1,0}(\\pi,0) = \\sqrt{\\frac{3}{4\\pi}} \\cos(\\pi) = -\\sqrt{\\frac{3}{4\\pi}}$.\n$Y_{1,\\pm 1}(\\pi,0) = \\mp \\sqrt{\\frac{3}{8\\pi}} \\sin(\\pi) \\exp(\\pm i \\cdot 0) = 0$.\n\nSince all these values are real, $Y_{1m}^* = Y_{1m}$. We can now compute the coefficients $u_{1m}$:\nFor $m=0$:\n$u_{1,0} = w \\left( Y_{1,0}(0,0) + Y_{1,0}(\\pi,0) \\right) = w \\left( \\sqrt{\\frac{3}{4\\pi}} - \\sqrt{\\frac{3}{4\\pi}} \\right) = 0$.\nFor $m=+1$:\n$u_{1,1} = w \\left( Y_{1,1}(0,0) + Y_{1,1}(\\pi,0) \\right) = w \\left( 0 + 0 \\right) = 0$.\nFor $m=-1$:\n$u_{1,-1} = w \\left( Y_{1,-1}(0,0) + Y_{1,-1}(\\pi,0) \\right) = w \\left( 0 + 0 \\right) = 0$.\n\nAll coefficients $u_{1m}$ are zero for this centro-symmetric configuration. Therefore, the value of the invariant is:\n$$\nB_{1\\,1\\,0} = |u_{1,-1}|^2 + |u_{1,0}|^2 + |u_{1,1}|^2 = 0^2 + 0^2 + 0^2 = 0\n$$\nPhysically, the coefficients $u_{1m}$ are a measure of the dipole moment of the neighbor density distribution. For a centro-symmetric arrangement, the net dipole moment is zero, and thus all its components $u_{1m}$ must be zero.\n\nThe third task is to analyze the response of $B_{1\\,1\\,0}$ to a small perturbation. The neighbor at $(\\pi,0)$ is moved to $(\\pi-\\varepsilon, 0)$ for a small angle $0 < \\varepsilon \\ll 1$. We recalculate the $u_{1m}$ coefficients to leading order in $\\varepsilon$.\nThe contribution from neighbor $1$ at $(0,0)$ is unchanged. For neighbor $2$ at $(\\theta, \\phi) = (\\pi-\\varepsilon, 0)$, we use Taylor series expansions for $\\varepsilon \\ll 1$:\n$\\cos(\\pi-\\varepsilon) = -\\cos(\\varepsilon) \\approx -(1 - \\frac{\\varepsilon^2}{2}) = -1 + \\frac{\\varepsilon^2}{2}$.\n$\\sin(\\pi-\\varepsilon) = \\sin(\\varepsilon) \\approx \\varepsilon$.\nThe values of $Y_{1m}$ for neighbor $2$ are now:\n$Y_{1,0}(\\pi-\\varepsilon,0) = \\sqrt{\\frac{3}{4\\pi}}\\cos(\\pi-\\varepsilon) \\approx \\sqrt{\\frac{3}{4\\pi}}(-1 + \\frac{\\varepsilon^2}{2})$.\n$Y_{1,\\pm 1}(\\pi-\\varepsilon,0) = \\mp \\sqrt{\\frac{3}{8\\pi}}\\sin(\\pi-\\varepsilon) \\approx \\mp \\sqrt{\\frac{3}{8\\pi}}\\varepsilon$.\nAgain, these are real, so $Y_{1m}^* = Y_{1m}$. The new coefficients $u_{1m}'$ are:\n$u_{1,0}' = w(Y_{1,0}(0,0) + Y_{1,0}(\\pi-\\varepsilon,0)) \\approx w \\left( \\sqrt{\\frac{3}{4\\pi}} + \\sqrt{\\frac{3}{4\\pi}}(-1+\\frac{\\varepsilon^2}{2}) \\right) = w\\sqrt{\\frac{3}{4\\pi}}\\frac{\\varepsilon^2}{2}$.\n$u_{1,1}' = w(Y_{1,1}(0,0) + Y_{1,1}(\\pi-\\varepsilon,0)) \\approx w \\left( 0 - \\sqrt{\\frac{3}{8\\pi}}\\varepsilon \\right) = -w\\sqrt{\\frac{3}{8\\pi}}\\varepsilon$.\n$u_{1,-1}' = w(Y_{1,-1}(0,0) + Y_{1,-1}(\\pi-\\varepsilon,0)) \\approx w \\left( 0 + \\sqrt{\\frac{3}{8\\pi}}\\varepsilon \\right) = w\\sqrt{\\frac{3}{8\\pi}}\\varepsilon$.\n\nThe perturbed invariant $B_{1\\,1\\,0}'$ is:\n$$\nB_{1\\,1\\,0}' = |u_{1,-1}'|^2 + |u_{1,0}'|^2 + |u_{1,1}'|^2\n$$\n$$\nB_{1\\,1\\,0}' \\approx \\left(w\\sqrt{\\frac{3}{8\\pi}}\\varepsilon\\right)^2 + \\left(w\\sqrt{\\frac{3}{4\\pi}}\\frac{\\varepsilon^2}{2}\\right)^2 + \\left(-w\\sqrt{\\frac{3}{8\\pi}}\\varepsilon\\right)^2\n$$\n$$\nB_{1\\,1\\,0}' \\approx w^2\\frac{3}{8\\pi}\\varepsilon^2 + w^2\\frac{3}{16\\pi}\\frac{\\varepsilon^4}{4} + w^2\\frac{3}{8\\pi}\\varepsilon^2\n$$\nTo leading order in $\\varepsilon$:\n$$\nB_{1\\,1\\,0}' \\approx 2 \\left( w^2\\frac{3}{8\\pi}\\varepsilon^2 \\right) = \\frac{3w^2}{4\\pi}\\varepsilon^2\n$$\nThe leading-order scaling of $B_{1\\,1\\,0}$ with $\\varepsilon$ is $\\varepsilon^2$.\n\nThe physical reason for this quadratic scaling is that $B_{1\\,1\\,0}$ is a measure of anisotropy. The unperturbed configuration with $\\varepsilon=0$ is perfectly symmetric (isotropic with respect to the dipole moment), and $B_{1\\,1\\,0}$ is at its absolute minimum value of $0$. For any smooth function $f(x)$ that has a minimum at $x_0$, its Taylor expansion around $x_0$ is $f(x) \\approx f(x_0) + \\frac{1}{2}f''(x_0)(x-x_0)^2$. Since $B_{1\\,1\\,0}(\\varepsilon=0)=0$, any small deviation $\\varepsilon$ from this symmetric point must lead to a change in $B_{1\\,1\\,0}$ that is proportional to $\\varepsilon^2$. In this context, the breaking of the perfect centro-symmetry creates a non-zero dipole moment in the neighbor distribution that is linear in $\\varepsilon$. The invariant $B_{1\\,1\\,0}$ is proportional to the square of the magnitude of this dipole moment, hence it must scale quadratically with $\\varepsilon$.",
            "answer": "$$\n\\boxed{0}\n$$"
        },
        {
            "introduction": "The final stage in developing a SNAP model is to link the rotationally invariant descriptors to physical observables like energy and forces through a linear model. This requires fitting the model's coefficients using a reference dataset, a process central to machine-learned potentials. This hands-on exercise  simulates this critical step by guiding you through the process of performing a regularized linear regression on a synthetic dataset, revealing how hyperparameters like regularization strength $\\lambda$ and force weight $\\gamma$ impact the model's predictive accuracy and stability.",
            "id": "3808204",
            "problem": "In the Spectral Neighbor Analysis Potential (SNAP), a crystal's potential energy is modeled as a linear function of rotationally invariant descriptors of the local neighbor density built from the bispectrum of hyperspherical harmonics. In the linear SNAP setting, the total energy of a configuration is expressed as a linear form in a fixed-dimensional descriptor vector. The forces are obtained as the negative gradient of the energy, which in a linear model yields linear forms in the same coefficients acting on the derivatives of the descriptors. In practice, coefficient estimation is formulated as a regularized regression problem based on a combined dataset of energies and forces.\n\nStarting from the following fundamental base:\n- A linear measurement model for energies and forces: for each configuration, the energy is modeled as $E = \\mathbf{x}_{E}^{\\top}\\mathbf{w} + \\varepsilon_{E}$ and each force component as $F = \\mathbf{x}_{F}^{\\top}\\mathbf{w} + \\varepsilon_{F}$, where $\\mathbf{x}_{E}$ and $\\mathbf{x}_{F}$ are descriptor and descriptor-derivative feature vectors respectively, $\\mathbf{w}$ are unknown coefficients, and $\\varepsilon_{E}$, $\\varepsilon_{F}$ are zero-mean random errors.\n- A convex quadratic loss that balances energy and force residuals with a force-weighting hyperparameter $\\gamma \\ge 0$, and includes $\\ell_{2}$-regularization with strength $\\lambda \\ge 0$ to control coefficient magnitude.\n\nYour task is to compute the regularization path for a small synthetic SNAP-like dataset and report how predictive error and coefficient norms vary with the hyperparameters.\n\nData generation protocol (must be followed exactly to ensure reproducibility):\n- Use a fixed random seed $s = 2025$.\n- Let the feature dimension be $p = 7$.\n- Let the total number of configurations be $n_{E,\\text{total}} = 12$, split into $n_{E,\\text{train}} = 8$ for training and $n_{E,\\text{val}} = 4$ for validation.\n- For each configuration, include $3$ force components (e.g., Cartesian components), so the total number of force samples is $n_{F,\\text{total}} = 36$, split into $n_{F,\\text{train}} = 24$ and $n_{F,\\text{val}} = 12$.\n- Fix the true coefficient vector to\n$$\n\\mathbf{w}_{\\text{true}} = \\begin{bmatrix}0.8 \\\\ -0.5 \\\\ 0.3 \\\\ 0.0 \\\\ 0.2 \\\\ -0.1 \\\\ 0.05\\end{bmatrix}.\n$$\n- Draw the energy feature matrix $\\mathbf{X}_{E} \\in \\mathbb{R}^{12 \\times 7}$ with independent entries from a normal distribution of mean $0$ and standard deviation $1$, split as $\\mathbf{X}_{E,\\text{train}} \\in \\mathbb{R}^{8 \\times 7}$ (first $8$ rows) and $\\mathbf{X}_{E,\\text{val}} \\in \\mathbb{R}^{4 \\times 7}$ (last $4$ rows).\n- Draw the force feature matrix $\\mathbf{X}_{F} \\in \\mathbb{R}^{36 \\times 7}$ with independent entries from a normal distribution of mean $0$ and standard deviation $0.4$, split as $\\mathbf{X}_{F,\\text{train}} \\in \\mathbb{R}^{24 \\times 7}$ (first $24$ rows) and $\\mathbf{X}_{F,\\text{val}} \\in \\mathbb{R}^{12 \\times 7}$ (last $12$ rows).\n- Generate the responses with independent Gaussian noise: $\\mathbf{y}_{E} = \\mathbf{X}_{E}\\mathbf{w}_{\\text{true}} + \\boldsymbol{\\varepsilon}_{E}$ with $\\varepsilon_{E,i} \\sim \\mathcal{N}(0, \\sigma_{E}^{2})$, $\\sigma_{E} = 0.01$ electronvolts (eV), and $\\mathbf{y}_{F} = \\mathbf{X}_{F}\\mathbf{w}_{\\text{true}} + \\boldsymbol{\\varepsilon}_{F}$ with $\\varepsilon_{F,j} \\sim \\mathcal{N}(0, \\sigma_{F}^{2})$, $\\sigma_{F} = 0.02$ electronvolts per angstrom (eV/Å). Split these into training and validation partitions consistent with the feature splits, i.e., $\\mathbf{y}_{E,\\text{train}}$, $\\mathbf{y}_{E,\\text{val}}$, $\\mathbf{y}_{F,\\text{train}}$, and $\\mathbf{y}_{F,\\text{val}}$.\n\nFor each given pair of hyperparameters $(\\lambda, \\gamma)$ in the test suite below, estimate $\\mathbf{w}$ by minimizing the following objective over $\\mathbf{w} \\in \\mathbb{R}^{p}$:\n$$\n\\mathcal{L}(\\mathbf{w}) = \\left\\|\\mathbf{X}_{E,\\text{train}}\\mathbf{w} - \\mathbf{y}_{E,\\text{train}}\\right\\|_{2}^{2} + \\gamma^{2}\\left\\|\\mathbf{X}_{F,\\text{train}}\\mathbf{w} - \\mathbf{y}_{F,\\text{train}}\\right\\|_{2}^{2} + \\lambda \\left\\|\\mathbf{w}\\right\\|_{2}^{2}.\n$$\nFrom the minimizer $\\hat{\\mathbf{w}}(\\lambda,\\gamma)$, compute on the validation sets:\n- The Root Mean Square Error (RMSE) for energies, defined as\n$$\n\\mathrm{RMSE}_{E} = \\sqrt{\\frac{1}{n_{E,\\text{val}}}\\left\\|\\mathbf{X}_{E,\\text{val}}\\hat{\\mathbf{w}} - \\mathbf{y}_{E,\\text{val}}\\right\\|_{2}^{2}},\n$$\nreported in electronvolts (eV).\n- The Root Mean Square Error for forces, defined as\n$$\n\\mathrm{RMSE}_{F} = \\sqrt{\\frac{1}{n_{F,\\text{val}}}\\left\\|\\mathbf{X}_{F,\\text{val}}\\hat{\\mathbf{w}} - \\mathbf{y}_{F,\\text{val}}\\right\\|_{2}^{2}},\n$$\nreported in electronvolts per angstrom (eV/Å).\n- The coefficient $\\ell_{2}$-norm $\\left\\|\\hat{\\mathbf{w}}\\right\\|_{2}$, which is unitless under the given normalization.\n\nTest suite of hyperparameters to evaluate:\n- $(\\lambda, \\gamma) = (0.0, 1.0)$\n- $(\\lambda, \\gamma) = (0.0001, 1.0)$\n- $(\\lambda, \\gamma) = (0.01, 1.0)$\n- $(\\lambda, \\gamma) = (1.0, 1.0)$\n- $(\\lambda, \\gamma) = (0.01, 0.0)$\n- $(\\lambda, \\gamma) = (0.01, 5.0)$\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to a test case in the order listed above, and is itself a list of three floating-point numbers in the order $[\\mathrm{RMSE}_{E}, \\mathrm{RMSE}_{F}, \\|\\hat{\\mathbf{w}}\\|_{2}]$. For example, the overall shape must be\n$$\n\\left[\\left[r_{1,E}, r_{1,F}, n_{1}\\right], \\left[r_{2,E}, r_{2,F}, n_{2}\\right], \\dots \\right].\n$$\nAll energy errors must be expressed in electronvolts (eV), all force errors in electronvolts per angstrom (eV/Å), and the coefficient norm is unitless. Angles are not involved in this task. No other text should be printed.",
            "solution": "The problem as stated constitutes a valid and well-posed task in the domain of computational materials science, specifically concerning the parameterization of a linear Spectral Neighbor Analysis Potential (SNAP). It is a standard application of regularized linear regression, commonly known as ridge regression or Tikhonov regularization. The problem provides a complete and consistent set of instructions for generating a synthetic dataset and for training and evaluating the model. All parameters are clearly defined, and the objective is unambiguous. We shall therefore proceed with a complete solution.\n\nThe core of the problem is to find the coefficient vector $\\mathbf{w} \\in \\mathbb{R}^{p}$ that minimizes a convex quadratic objective function $\\mathcal{L}(\\mathbf{w})$. This function represents a trade-off between fitting training data (energies and forces) and controlling the complexity of the model (the magnitude of the coefficients).\n\nThe objective function is given by:\n$$\n\\mathcal{L}(\\mathbf{w}) = \\left\\|\\mathbf{X}_{E,\\text{train}}\\mathbf{w} - \\mathbf{y}_{E,\\text{train}}\\right\\|_{2}^{2} + \\gamma^{2}\\left\\|\\mathbf{X}_{F,\\text{train}}\\mathbf{w} - \\mathbf{y}_{F,\\text{train}}\\right\\|_{2}^{2} + \\lambda \\left\\|\\mathbf{w}\\right\\|_{2}^{2}\n$$\nHere, $\\mathbf{X}_{E,\\text{train}} \\in \\mathbb{R}^{n_{E,\\text{train}} \\times p}$ and $\\mathbf{X}_{F,\\text{train}} \\in \\mathbb{R}^{n_{F,\\text{train}} \\times p}$ are the feature matrices for training energies and forces, respectively. The vectors $\\mathbf{y}_{E,\\text{train}} \\in \\mathbb{R}^{n_{E,\\text{train}}}$ and $\\mathbf{y}_{F,\\text{train}} \\in \\mathbb{R}^{n_{F,\\text{train}}}$ are the corresponding target values. The hyperparameters are the force weight $\\gamma \\ge 0$ and the regularization strength $\\lambda \\ge 0$. The dimension of the feature space is $p=7$.\n\nTo find the optimal coefficient vector $\\hat{\\mathbf{w}}$ that minimizes $\\mathcal{L}(\\mathbf{w})$, we must find the point where the gradient of the objective function with respect to $\\mathbf{w}$ is zero. First, we expand the squared Euclidean norms:\n$$\n\\mathcal{L}(\\mathbf{w}) = (\\mathbf{X}_{E,\\text{train}}\\mathbf{w} - \\mathbf{y}_{E,\\text{train}})^{\\top}(\\mathbf{X}_{E,\\text{train}}\\mathbf{w} - \\mathbf{y}_{E,\\text{train}}) + \\gamma^{2}(\\mathbf{X}_{F,\\text{train}}\\mathbf{w} - \\mathbf{y}_{F,\\text{train}})^{\\top}(\\mathbf{X}_{F,\\text{train}}\\mathbf{w} - \\mathbf{y}_{F,\\text{train}}) + \\lambda \\mathbf{w}^{\\top}\\mathbf{w}\n$$\nExpanding these terms yields:\n$$\n\\mathcal{L}(\\mathbf{w}) = (\\mathbf{w}^{\\top}\\mathbf{X}_{E,\\text{train}}^{\\top}\\mathbf{X}_{E,\\text{train}}\\mathbf{w} - 2\\mathbf{y}_{E,\\text{train}}^{\\top}\\mathbf{X}_{E,\\text{train}}\\mathbf{w} + \\mathbf{y}_{E,\\text{train}}^{\\top}\\mathbf{y}_{E,\\text{train}}) + \\gamma^{2}(\\mathbf{w}^{\\top}\\mathbf{X}_{F,\\text{train}}^{\\top}\\mathbf{X}_{F,\\text{train}}\\mathbf{w} - 2\\mathbf{y}_{F,\\text{train}}^{\\top}\\mathbf{X}_{F,\\text{train}}\\mathbf{w} + \\mathbf{y}_{F,\\text{train}}^{\\top}\\mathbf{y}_{F,\\text{train}}) + \\lambda \\mathbf{w}^{\\top}\\mathbf{w}\n$$\nNow, we compute the gradient $\\nabla_{\\mathbf{w}} \\mathcal{L}(\\mathbf{w})$. Using the standard matrix calculus identities $\\nabla_{\\mathbf{x}}(\\mathbf{x}^{\\top}\\mathbf{A}\\mathbf{x}) = 2\\mathbf{A}\\mathbf{x}$ (for symmetric $\\mathbf{A}$) and $\\nabla_{\\mathbf{x}}(\\mathbf{b}^{\\top}\\mathbf{x}) = \\mathbf{b}$, we obtain:\n$$\n\\nabla_{\\mathbf{w}} \\mathcal{L}(\\mathbf{w}) = 2\\mathbf{X}_{E,\\text{train}}^{\\top}\\mathbf{X}_{E,\\text{train}}\\mathbf{w} - 2\\mathbf{X}_{E,\\text{train}}^{\\top}\\mathbf{y}_{E,\\text{train}} + \\gamma^{2}(2\\mathbf{X}_{F,\\text{train}}^{\\top}\\mathbf{X}_{F,\\text{train}}\\mathbf{w} - 2\\mathbf{X}_{F,\\text{train}}^{\\top}\\mathbf{y}_{F,\\text{train}}) + 2\\lambda\\mathbf{w}\n$$\nSetting the gradient to the zero vector, $\\nabla_{\\mathbf{w}} \\mathcal{L}(\\hat{\\mathbf{w}}) = \\mathbf{0}$, gives:\n$$\n2\\mathbf{X}_{E,\\text{train}}^{\\top}\\mathbf{X}_{E,\\text{train}}\\hat{\\mathbf{w}} - 2\\mathbf{X}_{E,\\text{train}}^{\\top}\\mathbf{y}_{E,\\text{train}} + 2\\gamma^{2}\\mathbf{X}_{F,\\text{train}}^{\\top}\\mathbf{X}_{F,\\text{train}}\\hat{\\mathbf{w}} - 2\\gamma^{2}\\mathbf{X}_{F,\\text{train}}^{\\top}\\mathbf{y}_{F,\\text{train}} + 2\\lambda\\hat{\\mathbf{w}} = \\mathbf{0}\n$$\nDividing by $2$ and rearranging the terms to isolate $\\hat{\\mathbf{w}}$:\n$$\n(\\mathbf{X}_{E,\\text{train}}^{\\top}\\mathbf{X}_{E,\\text{train}} + \\gamma^{2}\\mathbf{X}_{F,\\text{train}}^{\\top}\\mathbf{X}_{F,\\text{train}} + \\lambda\\mathbf{I})\\hat{\\mathbf{w}} = \\mathbf{X}_{E,\\text{train}}^{\\top}\\mathbf{y}_{E,\\text{train}} + \\gamma^{2}\\mathbf{X}_{F,\\text{train}}^{\\top}\\mathbf{y}_{F,\\text{train}}\n$$\nThis is a linear system of equations of the form $\\mathbf{A}\\mathbf{x} = \\mathbf{b}$, where:\n- $\\mathbf{A} = \\mathbf{X}_{E,\\text{train}}^{\\top}\\mathbf{X}_{E,\\text{train}} + \\gamma^{2}\\mathbf{X}_{F,\\text{train}}^{\\top}\\mathbf{X}_{F,\\text{train}} + \\lambda\\mathbf{I}$ is a $p \\times p$ matrix.\n- $\\mathbf{x} = \\hat{\\mathbf{w}}$ is the $p \\times 1$ vector of unknown coefficients.\n- $\\mathbf{b} = \\mathbf{X}_{E,\\text{train}}^{\\top}\\mathbf{y}_{E,\\text{train}} + \\gamma^{2}\\mathbf{X}_{F,\\text{train}}^{\\top}\\mathbf{y}_{F,\\text{train}}$ is a $p \\times 1$ vector.\nSince $\\mathbf{X}^{\\top}\\mathbf{X}$ is always positive semi-definite and $\\lambda\\mathbf{I}$ is positive definite for $\\lambda > 0$, the matrix $\\mathbf{A}$ is guaranteed to be positive definite and thus invertible. Even when $\\lambda = 0$, the combined data from energies and forces ensures that $\\mathbf{A}$ is invertible with probability $1$ given that the features are drawn from a continuous distribution and the number of data points ($n_{E,\\text{train}} + n_{F,\\text{train}} = 8 + 24 = 32$) is greater than the number of features ($p=7$).\n\nThe solution $\\hat{\\mathbf{w}}$ is therefore found by solving this linear system.\n\nThe algorithmic procedure is as follows:\n1.  **Generate Data**: Create the matrices $\\mathbf{X}_E$, $\\mathbf{X}_F$ and vectors $\\mathbf{y}_E$, $\\mathbf{y}_F$ according to the specified protocol. This involves using a fixed random seed $s=2025$ to ensure reproducibility.\n    -   The feature dimension is $p=7$.\n    -   The true coefficient vector is $\\mathbf{w}_{\\text{true}} = [0.8, -0.5, 0.3, 0.0, 0.2, -0.1, 0.05]^{\\top}$.\n    -   Energy features $\\mathbf{X}_E \\in \\mathbb{R}^{12 \\times 7}$ are drawn from $\\mathcal{N}(0, 1^2)$.\n    -   Force features $\\mathbf{X}_F \\in \\mathbb{R}^{36 \\times 7}$ are drawn from $\\mathcal{N}(0, 0.4^2)$.\n    -   Energy responses are $\\mathbf{y}_{E} = \\mathbf{X}_{E}\\mathbf{w}_{\\text{true}} + \\boldsymbol{\\varepsilon}_{E}$ with noise $\\varepsilon_{E,i} \\sim \\mathcal{N}(0, 0.01^2)$.\n    -   Force responses are $\\mathbf{y}_{F} = \\mathbf{X}_{F}\\mathbf{w}_{\\text{true}} + \\boldsymbol{\\varepsilon}_{F}$ with noise $\\varepsilon_{F,j} \\sim \\mathcal{N}(0, 0.02^2)$.\n    -   Split the data into training sets ($\\mathbf{X}_{E,\\text{train}}$, $\\mathbf{y}_{E,\\text{train}}$, $\\mathbf{X}_{F,\\text{train}}$, $\\mathbf{y}_{F,\\text{train}}$) and validation sets ($\\mathbf{X}_{E,\\text{val}}$, $\\mathbf{y}_{E,\\text{val}}$, $\\mathbf{X}_{F,\\text{val}}$, $\\mathbf{y}_{F,\\text{val}}$) as specified.\n\n2.  **Iterate Over Hyperparameters**: For each pair $(\\lambda, \\gamma)$ in the test suite:\n    a. Construct the matrix $\\mathbf{A} = \\mathbf{X}_{E,\\text{train}}^{\\top}\\mathbf{X}_{E,\\text{train}} + \\gamma^{2}\\mathbf{X}_{F,\\text{train}}^{\\top}\\mathbf{X}_{F,\\text{train}} + \\lambda\\mathbf{I}_{p \\times p}$ and the vector $\\mathbf{b} = \\mathbf{X}_{E,\\text{train}}^{\\top}\\mathbf{y}_{E,\\text{train}} + \\gamma^{2}\\mathbf{X}_{F,\\text{train}}^{\\top}\\mathbf{y}_{F,\\text{train}}$.\n    b. Solve the linear system $\\mathbf{A}\\hat{\\mathbf{w}} = \\mathbf{b}$ for $\\hat{\\mathbf{w}}(\\lambda,\\gamma)$. This is numerically more stable than computing the inverse of $\\mathbf{A}$.\n\n3.  **Evaluate Performance**: Using the obtained $\\hat{\\mathbf{w}}$ on the validation data:\n    a. Calculate the energy predictions $\\hat{\\mathbf{y}}_{E,\\text{val}} = \\mathbf{X}_{E,\\text{val}}\\hat{\\mathbf{w}}$.\n    b. Compute the energy RMSE: $\\mathrm{RMSE}_{E} = \\sqrt{\\frac{1}{n_{E,\\text{val}}}\\left\\|\\hat{\\mathbf{y}}_{E,\\text{val}} - \\mathbf{y}_{E,\\text{val}}\\right\\|_{2}^{2}}$. The number of validation energies is $n_{E,\\text{val}} = 4$.\n    c. Calculate the force predictions $\\hat{\\mathbf{y}}_{F,\\text{val}} = \\mathbf{X}_{F,\\text{val}}\\hat{\\mathbf{w}}$.\n    d. Compute the force RMSE: $\\mathrm{RMSE}_{F} = \\sqrt{\\frac{1}{n_{F,\\text{val}}}\\left\\|\\hat{\\mathbf{y}}_{F,\\text{val}} - \\mathbf{y}_{F,\\text{val}}\\right\\|_{2}^{2}}$. The number of validation forces is $n_{F,\\text{val}} = 12$.\n    e. Compute the coefficient norm $\\|\\hat{\\mathbf{w}}\\|_{2}$.\n\n4.  **Report Results**: Collect the triplet $[\\mathrm{RMSE}_{E}, \\mathrm{RMSE}_{F}, \\|\\hat{\\mathbf{w}}\\|_{2}]$ for each hyperparameter pair and format them as specified.\n\nThis procedure will be implemented in the provided Python environment.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the regularized linear regression problem for a synthetic SNAP-like dataset.\n    \"\"\"\n    # 1. Data Generation Protocol\n    s = 2025\n    rng = np.random.default_rng(s)\n\n    p = 7\n    n_E_total = 12\n    n_E_train = 8\n    n_E_val = 4\n    n_F_total = 36\n    n_F_train = 24\n    n_F_val = 12\n    \n    sigma_E = 0.01\n    sigma_F = 0.02\n\n    w_true = np.array([0.8, -0.5, 0.3, 0.0, 0.2, -0.1, 0.05])\n\n    # Generate feature matrices\n    X_E = rng.normal(loc=0.0, scale=1.0, size=(n_E_total, p))\n    X_F = rng.normal(loc=0.0, scale=0.4, size=(n_F_total, p))\n\n    # Split feature matrices\n    X_E_train = X_E[:n_E_train, :]\n    X_E_val = X_E[n_E_train:, :]\n    X_F_train = X_F[:n_F_train, :]\n    X_F_val = X_F[n_F_train:, :]\n\n    # Generate responses (energies and forces) with noise\n    eps_E = rng.normal(loc=0.0, scale=sigma_E, size=n_E_total)\n    eps_F = rng.normal(loc=0.0, scale=sigma_F, size=n_F_total)\n    \n    y_E = X_E @ w_true + eps_E\n    y_F = X_F @ w_true + eps_F\n\n    # Split responses\n    y_E_train = y_E[:n_E_train]\n    y_E_val = y_E[n_E_train:]\n    y_F_train = y_F[:n_F_train]\n    y_F_val = y_F[n_F_train:]\n\n    # Test suite of hyperparameters\n    test_cases = [\n        (0.0, 1.0),\n        (0.0001, 1.0),\n        (0.01, 1.0),\n        (1.0, 1.0),\n        (0.01, 0.0),\n        (0.01, 5.0)\n    ]\n\n    results = []\n    \n    # 2. Iterate Over Hyperparameters and Solve\n    for lambda_val, gamma_val in test_cases:\n        # Construct the components of the normal equations\n        # LHS = X_E.T @ X_E + gamma^2 * X_F.T @ X_F + lambda * I\n        # RHS = X_E.T @ y_E + gamma^2 * X_F.T @ y_F\n        \n        A_E = X_E_train.T @ X_E_train\n        A_F = X_F_train.T @ X_F_train\n        b_E = X_E_train.T @ y_E_train\n        b_F = X_F_train.T @ y_F_train\n\n        LHS = A_E + (gamma_val**2) * A_F + lambda_val * np.identity(p)\n        RHS = b_E + (gamma_val**2) * b_F\n\n        # Solve for the estimated coefficients w_hat\n        w_hat = np.linalg.solve(LHS, RHS)\n\n        # 3. Evaluate Performance on Validation Set\n        \n        # RMSE for energies\n        y_E_pred = X_E_val @ w_hat\n        rmse_E = np.sqrt(np.mean((y_E_pred - y_E_val)**2))\n\n        # RMSE for forces\n        y_F_pred = X_F_val @ w_hat\n        rmse_F = np.sqrt(np.mean((y_F_pred - y_F_val)**2))\n        \n        # L2-norm of coefficients\n        w_norm = np.linalg.norm(w_hat)\n        \n        results.append([rmse_E, rmse_F, w_norm])\n\n    # 4. Format and Print Final Output\n    formatted_results = [f\"[{res[0]},{res[1]},{res[2]}]\" for res in results]\n    final_output_string = f\"[{','.join(formatted_results)}]\"\n    print(final_output_string)\n\nsolve()\n```"
        }
    ]
}