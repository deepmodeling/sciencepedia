{
    "hands_on_practices": [
        {
            "introduction": "The first step in constructing Spectral Neighbor Analysis Potentials is the unique mapping of each neighbor's 3D Cartesian coordinates to a point on a 4D hypersphere. This exercise provides a concrete application of this abstract stereographic projection, allowing you to see how both radial and angular information are encoded. By performing the calculation for a simple atomic arrangement, you will gain a deeper intuition for this transformation and understand the critical role of the scaling parameter $r_0$ in weighting the contributions of near and far neighbors .",
            "id": "3808203",
            "problem": "Consider the Spectral Neighbor Analysis Potentials (SNAP) representation of local atomic environments. In SNAP, each neighbor of a central atom at position $i$ is mapped from three-dimensional Euclidean coordinates to a point on the three-sphere via stereographic projection controlled by a positive scale parameter $r_{0}$. Let the hyperspherical coordinate be denoted by $\\Omega=(\\alpha,\\theta,\\phi)$, where $\\alpha \\in [0,\\pi)$ is the stereographic angle associated with the radial distance $r$, and $(\\theta,\\phi)$ are the usual polar and azimuthal angles on the unit sphere.\n\nAssume that the central atom $i$ has three neighbors located along three vertices of a regular tetrahedron centered at the origin, with unit direction vectors\n- $\\mathbf{v}_{1}=\\frac{1}{\\sqrt{3}}(1,1,1)$,\n- $\\mathbf{v}_{2}=\\frac{1}{\\sqrt{3}}(1,-1,-1)$,\n- $\\mathbf{v}_{3}=\\frac{1}{\\sqrt{3}}(-1,1,-1)$.\n\nThe corresponding spherical angles are defined by $\\theta=\\arccos(z/r)$ and $\\phi=\\arctan2(y,x)$, using the standard conventions with $\\phi \\in [0,2\\pi)$.\n\nLet the neighbor distances be $r_{1}=r_{2}=2.2$ and $r_{3}=3.6$, in Angstrom units. Assume a sharp cutoff function $f_{c}(r)$ that equals $1$ for $r<R_{c}$ and $0$ otherwise, with $R_{c}=4.5$, and assign all neighbor type weights $w_{j}=1$.\n\n1. Using the SNAP stereographic projection whose radial angle satisfies $\\tan(\\alpha/2)=r/r_{0}$, compute explicitly the SNAP neighbor density $\\rho_{i}(\\Omega)$ for the choice $r_{0}=2.0$, in terms of Dirac delta functions supported at the mapped neighbor coordinates $\\Omega_{j}=(\\alpha_{j},\\theta_{j},\\phi_{j})$.\n2. To quantify how $r_{0}$ alters the relative weighting of near and far neighbors in the $\\alpha$ coordinate, define the map-induced local scaling $w(r;r_{0})=\\frac{d\\alpha}{dr}$ and the near-to-far ratio $W(r_{0})=\\frac{w(r_{\\text{near}};r_{0})}{w(r_{\\text{far}};r_{0})}$ with $r_{\\text{near}}=2.2$ and $r_{\\text{far}}=3.6$. Compute the single scalar\n$$Q=\\frac{W(r_{0}=2.0)}{W(r_{0}=4.0)}.$$\n\nReport $Q$ as a dimensionless number rounded to four significant figures. Angles should be in radians throughout. No units should appear in your final boxed answer.",
            "solution": "The problem is first subjected to a rigorous validation process.\n\n### Step 1: Extract Givens\n- **Model**: Spectral Neighbor Analysis Potentials (SNAP).\n- **Mapping**: Stereographic projection from 3D Euclidean coordinates to the three-sphere.\n- **Hyperspherical Coordinates**: $\\Omega = (\\alpha, \\theta, \\phi)$.\n- **Stereographic Angle Formula**: $\\tan(\\alpha/2) = r/r_0$, with $\\alpha \\in [0, \\pi)$ and $r_0 > 0$.\n- **Neighbor Direction Vectors**:\n  - $\\mathbf{v}_{1} = \\frac{1}{\\sqrt{3}}(1, 1, 1)$\n  - $\\mathbf{v}_{2} = \\frac{1}{\\sqrt{3}}(1, -1, -1)$\n  - $\\mathbf{v}_{3} = \\frac{1}{\\sqrt{3}}(-1, 1, -1)$\n- **Neighbor Distances**: $r_{1} = 2.2$, $r_{2} = 2.2$, $r_{3} = 3.6$.\n- **Spherical Angle Definitions**: $\\theta = \\arccos(z/r)$, $\\phi = \\arctan2(y, x)$ for $\\phi \\in [0, 2\\pi)$.\n- **Cutoff Function**: $f_{c}(r) = 1$ for $r < R_{c}$ and $f_{c}(r) = 0$ for $r \\ge R_{c}$.\n- **Cutoff Radius**: $R_{c} = 4.5$.\n- **Neighbor Weights**: $w_{j} = 1$ for all neighbors.\n- **Part 1 Requirement**: Compute the neighbor density $\\rho_{i}(\\Omega)$ for $r_{0}=2.0$.\n- **Part 2 Definitions**:\n  - Map-induced local scaling: $w(r; r_{0}) = \\frac{d\\alpha}{dr}$.\n  - Near-to-far ratio: $W(r_{0}) = \\frac{w(r_{\\text{near}}; r_{0})}{w(r_{\\text{far}}; r_{0})}$.\n  - Given distances: $r_{\\text{near}} = 2.2$, $r_{\\text{far}} = 3.6$.\n- **Part 2 Requirement**: Compute $Q = \\frac{W(r_{0}=2.0)}{W(r_{0}=4.0)}$ rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed for validity.\n1.  **Scientific Grounding**: The problem is grounded in the established mathematical framework of the SNAP interatomic potential, a standard method in computational materials science. All definitions and equations are consistent with the scientific literature on this topic.\n2.  **Well-Posedness**: The problem provides all necessary data and relationships to compute the requested quantities. For Part 1, the neighbor density is fully determined by the specified neighbor positions and weights. For Part 2, the quantity $Q$ is derived from a well-defined function whose parameters are all specified. A unique solution exists.\n3.  **Objectivity**: The problem is stated in precise, mathematical language, free of ambiguity or subjective claims.\n4.  **Consistency**: The givens are self-consistent. The neighbor distances $r_1=2.2$, $r_2=2.2$, and $r_3=3.6$ are all less than the cutoff radius $R_c=4.5$, so all neighbors are included in the calculation, which is a consistent setup.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid** as it is scientifically grounded, well-posed, objective, and internally consistent. A full solution will be provided.\n\n### Solution\n\n**Part 1: Computation of the Neighbor Density $\\rho_{i}(\\Omega)$**\n\nThe SNAP neighbor density $\\rho_{i}(\\Omega)$ for a central atom $i$ is defined as a sum over its neighbors $j$:\n$$\n\\rho_{i}(\\Omega) = \\sum_{j} w_{j} f_{c}(r_{j}) \\delta(\\Omega - \\Omega_{j})\n$$\nwhere $\\Omega = (\\alpha, \\theta, \\phi)$ is a point on the three-sphere, $\\Omega_{j} = (\\alpha_{j}, \\theta_{j}, \\phi_{j})$ are the hyperspherical coordinates of neighbor $j$, $w_j$ are neighbor-type weights, $f_c(r_j)$ is a cutoff function, and $\\delta(\\cdot)$ is the Dirac delta function on the three-sphere.\n\nThe problem states that $w_{j} = 1$ for all neighbors. The cutoff radius is $R_{c} = 4.5$. The neighbor distances are $r_{1} = 2.2$, $r_{2} = 2.2$, and $r_{3} = 3.6$. Since all $r_j < R_c$, the cutoff function $f_{c}(r_j) = 1$ for all three neighbors. Thus, the expression for the density simplifies to:\n$$\n\\rho_{i}(\\Omega) = \\sum_{j=1}^{3} \\delta(\\Omega - \\Omega_{j})\n$$\nWe need to compute the hyperspherical coordinates $\\Omega_{j} = (\\alpha_{j}, \\theta_{j}, \\phi_{j})$ for each neighbor with $r_{0}=2.0$.\n\nFirst, we compute the stereographic angle $\\alpha_j$ for each neighbor using the relation $\\tan(\\alpha_j/2) = r_j / r_0$.\n\nFor neighbors $j=1$ and $j=2$, we have $r_{1}=r_{2}=2.2$.\n$$\n\\tan(\\alpha_{1,2}/2) = \\frac{2.2}{2.0} = 1.1 \\implies \\alpha_{1,2} = 2 \\arctan(1.1)\n$$\nFor neighbor $j=3$, we have $r_{3}=3.6$.\n$$\n\\tan(\\alpha_{3}/2) = \\frac{3.6}{2.0} = 1.8 \\implies \\alpha_{3} = 2 \\arctan(1.8)\n$$\n\nNext, we compute the spherical angles $(\\theta_j, \\phi_j)$ from the unit direction vectors $\\mathbf{v}_j = (\\hat{x}_j, \\hat{y}_j, \\hat{z}_j)$. The definitions are $\\theta_j = \\arccos(\\hat{z}_j)$ and $\\phi_j = \\arctan2(\\hat{y}_j, \\hat{x}_j)$.\n\nFor neighbor $j=1$: $\\mathbf{v}_{1} = \\frac{1}{\\sqrt{3}}(1, 1, 1)$.\n- $\\hat{z}_1 = \\frac{1}{\\sqrt{3}} \\implies \\theta_1 = \\arccos\\left(\\frac{1}{\\sqrt{3}}\\right)$.\n- $\\hat{y}_1 = \\frac{1}{\\sqrt{3}}, \\hat{x}_1 = \\frac{1}{\\sqrt{3}} \\implies \\phi_1 = \\arctan2(1, 1) = \\frac{\\pi}{4}$.\n\nFor neighbor $j=2$: $\\mathbf{v}_{2} = \\frac{1}{\\sqrt{3}}(1, -1, -1)$.\n- $\\hat{z}_2 = -\\frac{1}{\\sqrt{3}} \\implies \\theta_2 = \\arccos\\left(-\\frac{1}{\\sqrt{3}}\\right)$.\n- $\\hat{y}_2 = -\\frac{1}{\\sqrt{3}}, \\hat{x}_2 = \\frac{1}{\\sqrt{3}} \\implies \\phi_2 = \\arctan2(-1, 1) = -\\frac{\\pi}{4}$. Since $\\phi \\in [0, 2\\pi)$, we take $\\phi_2 = 2\\pi - \\frac{\\pi}{4} = \\frac{7\\pi}{4}$.\n\nFor neighbor $j=3$: $\\mathbf{v}_{3} = \\frac{1}{\\sqrt{3}}(-1, 1, -1)$.\n- $\\hat{z}_3 = -\\frac{1}{\\sqrt{3}} \\implies \\theta_3 = \\arccos\\left(-\\frac{1}{\\sqrt{3}}\\right)$.\n- $\\hat{y}_3 = \\frac{1}{\\sqrt{3}}, \\hat{x}_3 = -\\frac{1}{\\sqrt{3}} \\implies \\phi_3 = \\arctan2(1, -1) = \\pi - \\frac{\\pi}{4} = \\frac{3\\pi}{4}$.\n\nThe neighbor coordinates are:\n- $\\Omega_{1} = \\left(2 \\arctan(1.1), \\arccos\\left(\\frac{1}{\\sqrt{3}}\\right), \\frac{\\pi}{4}\\right)$\n- $\\Omega_{2} = \\left(2 \\arctan(1.1), \\arccos\\left(-\\frac{1}{\\sqrt{3}}\\right), \\frac{7\\pi}{4}\\right)$\n- $\\Omega_{3} = \\left(2 \\arctan(1.8), \\arccos\\left(-\\frac{1}{\\sqrt{3}}\\right), \\frac{3\\pi}{4}\\right)$\n\nThe neighbor density is therefore:\n$$\n\\rho_{i}(\\Omega) = \\delta(\\Omega - \\Omega_1) + \\delta(\\Omega - \\Omega_2) + \\delta(\\Omega - \\Omega_3)\n$$\nwith the coordinates $\\Omega_1, \\Omega_2, \\Omega_3$ as specified above.\n\n**Part 2: Computation of the Scalar $Q$**\n\nThe scalar $Q$ is defined as $Q=\\frac{W(r_{0}=2.0)}{W(r_{0}=4.0)}$, where $W(r_{0})=\\frac{w(r_{\\text{near}};r_{0})}{w(r_{\\text{far}};r_{0})}$ and $w(r;r_{0})=\\frac{d\\alpha}{dr}$.\n\nFirst, we derive an explicit expression for $w(r; r_{0})$. Starting from $\\tan(\\alpha/2) = r/r_{0}$, we differentiate with respect to $r$:\n$$\n\\frac{d}{dr} \\left( \\tan\\left(\\frac{\\alpha}{2}\\right) \\right) = \\frac{d}{dr} \\left( \\frac{r}{r_0} \\right)\n$$\n$$\n\\sec^2\\left(\\frac{\\alpha}{2}\\right) \\cdot \\frac{1}{2} \\frac{d\\alpha}{dr} = \\frac{1}{r_0}\n$$\nSolving for $\\frac{d\\alpha}{dr}$:\n$$\n\\frac{d\\alpha}{dr} = \\frac{2}{r_0 \\sec^2(\\alpha/2)}\n$$\nUsing the identity $\\sec^2(x) = 1 + \\tan^2(x)$ and substituting $\\tan(\\alpha/2) = r/r_0$:\n$$\nw(r; r_0) = \\frac{d\\alpha}{dr} = \\frac{2}{r_0 \\left(1 + \\tan^2(\\alpha/2)\\right)} = \\frac{2}{r_0 \\left(1 + (r/r_0)^2\\right)} = \\frac{2}{r_0 \\left(\\frac{r_0^2 + r^2}{r_0^2}\\right)} = \\frac{2r_0}{r^2 + r_0^2}\n$$\nNow, we can write the expression for the near-to-far ratio $W(r_0)$:\n$$\nW(r_{0}) = \\frac{w(r_{\\text{near}}; r_{0})}{w(r_{\\text{far}}; r_{0})} = \\frac{2r_0 / (r_{\\text{near}}^2 + r_0^2)}{2r_0 / (r_{\\text{far}}^2 + r_0^2)} = \\frac{r_{\\text{far}}^2 + r_0^2}{r_{\\text{near}}^2 + r_0^2}\n$$\nWe are given $r_{\\text{near}} = 2.2$ and $r_{\\text{far}} = 3.6$. So, $r_{\\text{near}}^2 = (2.2)^2 = 4.84$ and $r_{\\text{far}}^2 = (3.6)^2 = 12.96$.\nThe expression becomes:\n$$\nW(r_0) = \\frac{12.96 + r_0^2}{4.84 + r_0^2}\n$$\nNext, we evaluate $W(r_0)$ for the two specified values of $r_0$.\n\nFor $r_0=2.0$: $r_0^2 = 4.0$.\n$$\nW(r_0=2.0) = \\frac{12.96 + 4.0}{4.84 + 4.0} = \\frac{16.96}{8.84}\n$$\nFor $r_0=4.0$: $r_0^2 = 16.0$.\n$$\nW(r_0=4.0) = \\frac{12.96 + 16.0}{4.84 + 16.0} = \\frac{28.96}{20.84}\n$$\nFinally, we compute the required ratio $Q$:\n$$\nQ = \\frac{W(r_{0}=2.0)}{W(r_{0}=4.0)} = \\frac{16.96 / 8.84}{28.96 / 20.84} = \\frac{16.96}{8.84} \\times \\frac{20.84}{28.96}\n$$\n$$\nQ = \\frac{16.96 \\times 20.84}{8.84 \\times 28.96} = \\frac{353.4304}{255.9904} \\approx 1.380650005...\n$$\nRounding this result to four significant figures, we get $1.381$.",
            "answer": "$$\n\\boxed{1.381}\n$$"
        },
        {
            "introduction": "Once neighbor positions are mapped, the bispectrum components are calculated as rotationally invariant descriptors of the local environment. This thought experiment delves into a sophisticated property of these descriptors: their behavior under spatial inversion, which determines their ability to distinguish chiral structures. You will investigate how the bispectrum responds to an environment and its mirror image, revealing the fundamental symmetry rules that make SNAP sensitive to chirality .",
            "id": "3808223",
            "problem": "Consider Spectral Neighbor Analysis Potentials (SNAP), which build rotationally invariant descriptors of an atomic environment from the expansion of a neighbor density on the three-sphere using four-dimensional hyperspherical harmonics. Construct two atomic environments, $A$ and $B$, around a single central atom at the origin: both environments have four identical neighbor atoms of unit weight at the same radius $r_{0}$, with angular directions given by unit vectors on the sphere. Environment $A$ has neighbors at $r_{0} \\mathbf{v}_{i}$ with $\\mathbf{v}_{1} = (1, 0, 0)$, $\\mathbf{v}_{2} = (0, 1, 0)$, $\\mathbf{v}_{3} = (0, 0, 1)$, and $\\mathbf{v}_{4} = \\frac{1}{\\sqrt{3}} (1, 1, 1)$. Environment $B$ is the inversion of $A$: its neighbors are at $-r_{0} \\mathbf{v}_{i}$ for the same four $\\mathbf{v}_{i}$. These two environments have the same radial distribution but opposite chirality. Using the canonical SNAP bispectrum construction truncated at $J_{\\max} = 1$, define the rotationally invariant scalar\n$$\n\\Delta_{B}(J_{\\max} = 1) \\equiv \\sum_{j=1}^{1} \\left( B_{j j 0}(A) - B_{j j 0}(B) \\right),\n$$\nwhere $B_{j j 0}$ denotes the bispectrum component formed by coupling two degree-$j$ hyperspherical harmonic channels into $j = 0$. Compute $\\Delta_{B}(J_{\\max} = 1)$ for the specified pair of environments. Give your answer exactly as a single real number with no units and no rounding.",
            "solution": "### Step 1: Extract Givens\n- **Model:** Spectral Neighbor Analysis Potentials (SNAP).\n- **Core Concept:** Expansion of neighbor density on a 3-sphere using 4D hyperspherical harmonics.\n- **Environment A:** A central atom at the origin with four neighbor atoms at positions $\\mathbf{r}_k(A) = r_{0} \\mathbf{v}_{k}$ for $k=1, 2, 3, 4$.\n- **Neighbor vectors for A:**\n  - $\\mathbf{v}_{1} = (1, 0, 0)$\n  - $\\mathbf{v}_{2} = (0, 1, 0)$\n  - $\\mathbf{v}_{3} = (0, 0, 1)$\n  - $\\mathbf{v}_{4} = \\frac{1}{\\sqrt{3}} (1, 1, 1)$\n- **Environment B:** The inversion of A. Neighbor atoms are at positions $\\mathbf{r}_k(B) = -r_{0} \\mathbf{v}_{k} = -\\mathbf{r}_k(A)$.\n- **Atom Properties:** All neighbor atoms are identical, have unit weight ($w_k = 1$), and are at the same radius $r_0$.\n- **Quantity to Compute:** $\\Delta_{B}(J_{\\max} = 1) \\equiv \\sum_{j=1}^{1} \\left( B_{j j 0}(A) - B_{j j 0}(B) \\right)$, which simplifies to $\\Delta_{B} = B_{110}(A) - B_{110}(B)$.\n- **Definition:** $B_{j j 0}$ is the bispectrum component formed by coupling two degree-$j$ hyperspherical harmonic channels into a total angular momentum of $j=0$. This is interpreted as the standard bispectrum component $B_{j,j,0}$.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded:** The problem is based on the established SNAP formalism, a widely used method in computational materials science. It correctly references hyperspherical harmonics, bispectrum invariants, and the concept of chirality in atomic environments. The setup is scientifically sound.\n2.  **Well-Posed:** All necessary information is provided. The atomic environments are explicitly defined. The quantity to be computed is given by a precise formula. A unique solution exists.\n3.  **Objective:** The problem is stated in an objective, mathematical language, free of ambiguity or subjective claims.\n4.  **Completeness and Consistency:** The givens are self-contained and consistent. Environment B is correctly defined as the inversion of A, which establishes them as having opposite chirality.\n5.  **No other flaws detected:** The problem is a valid, non-trivial question that tests the understanding of the symmetry properties of SNAP descriptors.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete, reasoned solution will be provided.\n\n### Solution\n\nThe problem asks for the computation of $\\Delta_{B} = B_{110}(A) - B_{110}(B)$, where $A$ and $B$ are two atomic environments that are mirror images of each other (B is the spatial inversion of A), and $B_{110}$ is a component of the SNAP bispectrum.\n\nThe SNAP formalism begins by expanding the local neighbor density function $\\rho(\\mathbf{r})$ in a basis of 4D hyperspherical harmonics, $U_{jmm'}(\\mathbf{u})$, where $\\mathbf{u}$ is a point on the unit 3-sphere $S^3$ obtained by a stereographic projection of the neighbor position vector $\\mathbf{r}$. The expansion coefficients for a given environment are:\n$$\nu_{jmm'} = \\sum_k w_k U_{jmm'}^*(\\mathbf{u}_k)\n$$\nwhere the sum is over all neighbors $k$, each with weight $w_k$. The vector $\\mathbf{u}_k$ is the 3-sphere coordinate corresponding to the position $\\mathbf{r}_k$.\n\nThe bispectrum components, $B_{j_1 j_2 j_3}$, are rotationally invariant triple products of these coefficients, constructed to be invariant under SO(4) rotations:\n$$\nB_{j_1 j_2 j_3} = \\sum_{\\text{all } m, m'} C^{j_1 m_1 m'_1, j_2 m_2 m'_2}_{j_3 m_3 m'_3} u_{j_1 m_1 m'_1} u_{j_2 m_2 m'_2} u_{j_3 m_3 m'_3}^*\n$$\nwhere $C$ denotes a Clebsch-Gordan coefficient for SO(4).\n\nThe core of this problem lies in the behavior of these quantities under spatial inversion, $\\mathbf{r} \\to -\\mathbf{r}$.\nThe stereographic projection maps a point $\\mathbf{r}$ to a point $\\mathbf{u}$ on $S^3$. The inversion operation $\\mathbf{r} \\to -\\mathbf{r}$ induces a parity transformation on the 3-sphere coordinates, $\\mathbf{u} \\to \\mathbf{u}'$. For the standard projection, this corresponds to inverting all four coordinates of $\\mathbf{u}$, so $\\mathbf{u}' = -\\mathbf{u}$.\n\nThe hyperspherical harmonics $U_{jmm'}(\\mathbf{u})$ are homogeneous polynomials of degree $2j$ in the coordinates of $\\mathbf{u}$. Therefore, they are eigenfunctions of the parity operator with an eigenvalue determined by $2j$:\n$$\nU_{jmm'}(-\\mathbf{u}) = (-1)^{2j} U_{jmm'}(\\mathbf{u})\n$$\nLet us denote the coefficients for environment A as $u_{jmm'}(A)$ and for environment B (the inversion of A) as $u_{jmm'}(B)$.\nThe coefficients for environment B are calculated using the inverted positions $\\mathbf{r}_k(B) = -\\mathbf{r}_k(A)$, which map to $\\mathbf{u}_k(B) = -\\mathbf{u}_k(A)$.\n$$\nu_{jmm'}(B) = \\sum_k w_k U_{jmm'}^*(\\mathbf{u}_k(B)) = \\sum_k w_k U_{jmm'}^*(-\\mathbf{u}_k(A))\n$$\nUsing the parity property of $U_{jmm'}^*$ (which is the same as for $U_{jmm'}$ as the coordinates are real), we get:\n$$\nu_{jmm'}(B) = \\sum_k w_k ((-1)^{2j} U_{jmm'}(\\mathbf{u}_k(A)))^* = (-1)^{2j} \\sum_k w_k U_{jmm'}^*(\\mathbf{u}_k(A)) = (-1)^{2j} u_{jmm'}(A)\n$$\nThus, the coefficients for an inverted environment are related to the original coefficients by a factor of $(-1)^{2j}$.\n\nNow we can determine the transformation property of the bispectrum component $B_{j_1 j_2 j_3}$.\n$$\nB_{j_1 j_2 j_3}(B) = \\sum C \\cdot u_{j_1}(B) \\cdot u_{j_2}(B) \\cdot u_{j_3}^*(B)\n$$\nSubstituting the transformation rule for the coefficients:\n$$\nB_{j_1 j_2 j_3}(B) = \\sum C \\cdot ((-1)^{2j_1} u_{j_1}(A)) \\cdot ((-1)^{2j_2} u_{j_2}(A)) \\cdot ((-1)^{2j_3} u_{j_3}(A))^*\n$$\nSince the complex conjugation applies to the coefficient, not the phase factor $(-1)^{2j}$, which is real:\n$$\nB_{j_1 j_2 j_3}(B) = (-1)^{2j_1} (-1)^{2j_2} (-1)^{2j_3} \\sum C \\cdot u_{j_1}(A) \\cdot u_{j_2}(A) \\cdot u_{j_3}^*(A)\n$$\n$$\nB_{j_1 j_2 j_3}(B) = (-1)^{2(j_1+j_2+j_3)} B_{j_1 j_2 j_3}(A)\n$$\nThis general result shows that a bispectrum component is even or odd under spatial inversion, depending on the parity of the sum $2(j_1+j_2+j_3)$. Since $j$ can be a half-integer, $2j$ is an integer, so $2(j_1+j_2+j_3)$ is always an even integer. This means that $(-1)^{2(j_1+j_2+j_3)}$ is always $+1$.\nTherefore, all bispectrum components constructed this way are invariant under inversion:\n$$\nB_{j_1 j_2 j_3}(B) = B_{j_1 j_2 j_3}(A)\n$$\nThis is a fundamental property of the standard SNAP bispectrum construction.\n\nThe problem asks for $\\Delta_B = B_{110}(A) - B_{110}(B)$. Based on our general finding, this must be zero.\n$$\n\\Delta_{B} = B_{110}(A) - B_{110}(B) = B_{110}(A) - B_{110}(A) = 0\n$$\nThe result is zero. This conclusion is based solely on the symmetry properties of the bispectrum and is independent of the specific coordinates of the atoms, their number, weight, or radial distance, as long as environment B is the spatial inversion of environment A. The provided atomic coordinates are consistent with this general principle but are not needed for the derivation. A bispectrum-based descriptor capable of distinguishing these chiral environments would need to involve components with odd parity, which requires a different initial construction (e.g., using a pseudoscalar density).\nThe solution provided in the source seems to have a typo in its derivation ($(-1)^j$ instead of $(-1)^{2j}$), but it coincidentally leads to the correct result for the specific component $B_{110}$ because $1+1+0=2$ is even. The reasoning presented here is more general and robust.",
            "answer": "$$\n\\boxed{0}\n$$"
        },
        {
            "introduction": "The final stage in developing a SNAP model is to determine the linear coefficients that weigh the contribution of each bispectrum descriptor to the total energy. This computational exercise bridges the gap between theory and application by guiding you through the fitting process using regularized linear regression. By training a model on a synthetic dataset of energies and forces, you will explore how hyperparameters are used to balance model accuracy and complexity, a central task in the machine learning of interatomic potentials .",
            "id": "3808204",
            "problem": "In the Spectral Neighbor Analysis Potential (SNAP), a crystal's potential energy is modeled as a linear function of rotationally invariant descriptors of the local neighbor density built from the bispectrum of hyperspherical harmonics. In the linear SNAP setting, the total energy of a configuration is expressed as a linear form in a fixed-dimensional descriptor vector. The forces are obtained as the negative gradient of the energy, which in a linear model yields linear forms in the same coefficients acting on the derivatives of the descriptors. In practice, coefficient estimation is formulated as a regularized regression problem based on a combined dataset of energies and forces.\n\nStarting from the following fundamental base:\n- A linear measurement model for energies and forces: for each configuration, the energy is modeled as $E = \\mathbf{x}_{E}^{\\top}\\mathbf{w} + \\varepsilon_{E}$ and each force component as $F = \\mathbf{x}_{F}^{\\top}\\mathbf{w} + \\varepsilon_{F}$, where $\\mathbf{x}_{E}$ and $\\mathbf{x}_{F}$ are descriptor and descriptor-derivative feature vectors respectively, $\\mathbf{w}$ are unknown coefficients, and $\\varepsilon_{E}$, $\\varepsilon_{F}$ are zero-mean random errors.\n- A convex quadratic loss that balances energy and force residuals with a force-weighting hyperparameter $\\gamma \\ge 0$, and includes $\\ell_{2}$-regularization with strength $\\lambda \\ge 0$ to control coefficient magnitude.\n\nYour task is to compute the regularization path for a small synthetic SNAP-like dataset and report how predictive error and coefficient norms vary with the hyperparameters.\n\nData generation protocol (must be followed exactly to ensure reproducibility):\n- Use a fixed random seed $s = 2025$.\n- Let the feature dimension be $p = 7$.\n- Let the total number of configurations be $n_{E,\\text{total}} = 12$, split into $n_{E,\\text{train}} = 8$ for training and $n_{E,\\text{val}} = 4$ for validation.\n- For each configuration, include $3$ force components (e.g., Cartesian components), so the total number of force samples is $n_{F,\\text{total}} = 36$, split into $n_{F,\\text{train}} = 24$ and $n_{F,\\text{val}} = 12$.\n- Fix the true coefficient vector to\n$$\n\\mathbf{w}_{\\text{true}} = \\begin{bmatrix}0.8 \\\\ -0.5 \\\\ 0.3 \\\\ 0.0 \\\\ 0.2 \\\\ -0.1 \\\\ 0.05\\end{bmatrix}.\n$$\n- Draw the energy feature matrix $\\mathbf{X}_{E} \\in \\mathbb{R}^{12 \\times 7}$ with independent entries from a normal distribution of mean $0$ and standard deviation $1$, split as $\\mathbf{X}_{E,\\text{train}} \\in \\mathbb{R}^{8 \\times 7}$ (first $8$ rows) and $\\mathbf{X}_{E,\\text{val}} \\in \\mathbb{R}^{4 \\times 7}$ (last $4$ rows).\n- Draw the force feature matrix $\\mathbf{X}_{F} \\in \\mathbb{R}^{36 \\times 7}$ with independent entries from a normal distribution of mean $0$ and standard deviation $0.4$, split as $\\mathbf{X}_{F,\\text{train}} \\in \\mathbb{R}^{24 \\times 7}$ (first $24$ rows) and $\\mathbf{X}_{F,\\text{val}} \\in \\mathbb{R}^{12 \\times 7}$ (last $12$ rows).\n- Generate the responses with independent Gaussian noise: $\\mathbf{y}_{E} = \\mathbf{X}_{E}\\mathbf{w}_{\\text{true}} + \\boldsymbol{\\varepsilon}_{E}$ with $\\varepsilon_{E,i} \\sim \\mathcal{N}(0, \\sigma_{E}^{2})$, $\\sigma_{E} = 0.01$ electronvolts (eV), and $\\mathbf{y}_{F} = \\mathbf{X}_{F}\\mathbf{w}_{\\text{true}} + \\boldsymbol{\\varepsilon}_{F}$ with $\\varepsilon_{F,j} \\sim \\mathcal{N}(0, \\sigma_{F}^{2})$, $\\sigma_{F} = 0.02$ electronvolts per angstrom (eV/Å). Split these into training and validation partitions consistent with the feature splits, i.e., $\\mathbf{y}_{E,\\text{train}}$, $\\mathbf{y}_{E,\\text{val}}$, $\\mathbf{y}_{F,\\text{train}}$, and $\\mathbf{y}_{F,\\text{val}}$.\n\nFor each given pair of hyperparameters $(\\lambda, \\gamma)$ in the test suite below, estimate $\\mathbf{w}$ by minimizing the following objective over $\\mathbf{w} \\in \\mathbb{R}^{p}$:\n$$\n\\mathcal{L}(\\mathbf{w}) = \\left\\|\\mathbf{X}_{E,\\text{train}}\\mathbf{w} - \\mathbf{y}_{E,\\text{train}}\\right\\|_{2}^{2} + \\gamma^{2}\\left\\|\\mathbf{X}_{F,\\text{train}}\\mathbf{w} - \\mathbf{y}_{F,\\text{train}}\\right\\|_{2}^{2} + \\lambda \\left\\|\\mathbf{w}\\right\\|_{2}^{2}.\n$$\nFrom the minimizer $\\hat{\\mathbf{w}}(\\lambda,\\gamma)$, compute on the validation sets:\n- The Root Mean Square Error (RMSE) for energies, defined as\n$$\n\\mathrm{RMSE}_{E} = \\sqrt{\\frac{1}{n_{E,\\text{val}}}\\left\\|\\mathbf{X}_{E,\\text{val}}\\hat{\\mathbf{w}} - \\mathbf{y}_{E,\\text{val}}\\right\\|_{2}^{2}},\n$$\nreported in electronvolts (eV).\n- The Root Mean Square Error for forces, defined as\n$$\n\\mathrm{RMSE}_{F} = \\sqrt{\\frac{1}{n_{F,\\text{val}}}\\left\\|\\mathbf{X}_{F,\\text{val}}\\hat{\\mathbf{w}} - \\mathbf{y}_{F,\\text{val}}\\right\\|_{2}^{2}},\n$$\nreported in electronvolts per angstrom (eV/Å).\n- The coefficient $\\ell_{2}$-norm $\\left\\|\\hat{\\mathbf{w}}\\right\\|_{2}$, which is unitless under the given normalization.\n\nTest suite of hyperparameters to evaluate:\n- $(\\lambda, \\gamma) = (0.0, 1.0)$\n- $(\\lambda, \\gamma) = (0.0001, 1.0)$\n- $(\\lambda, \\gamma) = (0.01, 1.0)$\n- $(\\lambda, \\gamma) = (1.0, 1.0)$\n- $(\\lambda, \\gamma) = (0.01, 0.0)$\n- $(\\lambda, \\gamma) = (0.01, 5.0)$\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to a test case in the order listed above, and is itself a list of three floating-point numbers in the order $[\\mathrm{RMSE}_{E}, \\mathrm{RMSE}_{F}, \\|\\hat{\\mathbf{w}}\\|_{2}]$. For example, the overall shape must be\n$$\n\\left[\\left[r_{1,E}, r_{1,F}, n_{1}\\right], \\left[r_{2,E}, r_{2,F}, n_{2}\\right], \\dots \\right].\n$$\nAll energy errors must be expressed in electronvolts (eV), all force errors in electronvolts per angstrom (eV/Å), and the coefficient norm is unitless. Angles are not involved in this task. No other text should be printed.",
            "solution": "The problem as stated constitutes a valid and well-posed task in the domain of computational materials science, specifically concerning the parameterization of a linear Spectral Neighbor Analysis Potential (SNAP). It is a standard application of regularized linear regression, commonly known as ridge regression or Tikhonov regularization. The problem provides a complete and consistent set of instructions for generating a synthetic dataset and for training and evaluating the model. All parameters are clearly defined, and the objective is unambiguous. We shall therefore proceed with a complete solution.\n\nThe core of the problem is to find the coefficient vector $\\mathbf{w} \\in \\mathbb{R}^{p}$ that minimizes a convex quadratic objective function $\\mathcal{L}(\\mathbf{w})$. This function represents a trade-off between fitting training data (energies and forces) and controlling the complexity of the model (the magnitude of the coefficients).\n\nThe objective function is given by:\n$$\n\\mathcal{L}(\\mathbf{w}) = \\left\\|\\mathbf{X}_{E,\\text{train}}\\mathbf{w} - \\mathbf{y}_{E,\\text{train}}\\right\\|_{2}^{2} + \\gamma^{2}\\left\\|\\mathbf{X}_{F,\\text{train}}\\mathbf{w} - \\mathbf{y}_{F,\\text{train}}\\right\\|_{2}^{2} + \\lambda \\left\\|\\mathbf{w}\\right\\|_{2}^{2}\n$$\nHere, $\\mathbf{X}_{E,\\text{train}} \\in \\mathbb{R}^{n_{E,\\text{train}} \\times p}$ and $\\mathbf{X}_{F,\\text{train}} \\in \\mathbb{R}^{n_{F,\\text{train}} \\times p}$ are the feature matrices for training energies and forces, respectively. The vectors $\\mathbf{y}_{E,\\text{train}} \\in \\mathbb{R}^{n_{E,\\text{train}}}$ and $\\mathbf{y}_{F,\\text{train}} \\in \\mathbb{R}^{n_{F,\\text{train}}}$ are the corresponding target values. The hyperparameters are the force weight $\\gamma \\ge 0$ and the regularization strength $\\lambda \\ge 0$. The dimension of the feature space is $p=7$.\n\nTo find the optimal coefficient vector $\\hat{\\mathbf{w}}$ that minimizes $\\mathcal{L}(\\mathbf{w})$, we must find the point where the gradient of the objective function with respect to $\\mathbf{w}$ is zero. First, we expand the squared Euclidean norms:\n$$\n\\mathcal{L}(\\mathbf{w}) = (\\mathbf{X}_{E,\\text{train}}\\mathbf{w} - \\mathbf{y}_{E,\\text{train}})^{\\top}(\\mathbf{X}_{E,\\text{train}}\\mathbf{w} - \\mathbf{y}_{E,\\text{train}}) + \\gamma^{2}(\\mathbf{X}_{F,\\text{train}}\\mathbf{w} - \\mathbf{y}_{F,\\text{train}})^{\\top}(\\mathbf{X}_{F,\\text{train}}\\mathbf{w} - \\mathbf{y}_{F,\\text{train}}) + \\lambda \\mathbf{w}^{\\top}\\mathbf{w}\n$$\nExpanding these terms yields:\n$$\n\\mathcal{L}(\\mathbf{w}) = (\\mathbf{w}^{\\top}\\mathbf{X}_{E,\\text{train}}^{\\top}\\mathbf{X}_{E,\\text{train}}\\mathbf{w} - 2\\mathbf{y}_{E,\\text{train}}^{\\top}\\mathbf{X}_{E,\\text{train}}\\mathbf{w} + \\mathbf{y}_{E,\\text{train}}^{\\top}\\mathbf{y}_{E,\\text{train}}) + \\gamma^{2}(\\mathbf{w}^{\\top}\\mathbf{X}_{F,\\text{train}}^{\\top}\\mathbf{X}_{F,\\text{train}}\\mathbf{w} - 2\\mathbf{y}_{F,\\text{train}}^{\\top}\\mathbf{X}_{F,\\text{train}}\\mathbf{w} + \\mathbf{y}_{F,\\text{train}}^{\\top}\\mathbf{y}_{F,\\text{train}}) + \\lambda \\mathbf{w}^{\\top}\\mathbf{w}\n$$\nNow, we compute the gradient $\\nabla_{\\mathbf{w}} \\mathcal{L}(\\mathbf{w})$. Using the standard matrix calculus identities $\\nabla_{\\mathbf{x}}(\\mathbf{x}^{\\top}\\mathbf{A}\\mathbf{x}) = 2\\mathbf{A}\\mathbf{x}$ (for symmetric $\\mathbf{A}$) and $\\nabla_{\\mathbf{x}}(\\mathbf{b}^{\\top}\\mathbf{x}) = \\mathbf{b}$, we obtain:\n$$\n\\nabla_{\\mathbf{w}} \\mathcal{L}(\\mathbf{w}) = 2\\mathbf{X}_{E,\\text{train}}^{\\top}\\mathbf{X}_{E,\\text{train}}\\mathbf{w} - 2\\mathbf{X}_{E,\\text{train}}^{\\top}\\mathbf{y}_{E,\\text{train}} + \\gamma^{2}(2\\mathbf{X}_{F,\\text{train}}^{\\top}\\mathbf{X}_{F,\\text{train}}\\mathbf{w} - 2\\mathbf{X}_{F,\\text{train}}^{\\top}\\mathbf{y}_{F,\\text{train}}) + 2\\lambda\\mathbf{w}\n$$\nSetting the gradient to the zero vector, $\\nabla_{\\mathbf{w}} \\mathcal{L}(\\hat{\\mathbf{w}}) = \\mathbf{0}$, gives:\n$$\n2\\mathbf{X}_{E,\\text{train}}^{\\top}\\mathbf{X}_{E,\\text{train}}\\hat{\\mathbf{w}} - 2\\mathbf{X}_{E,\\text{train}}^{\\top}\\mathbf{y}_{E,\\text{train}} + 2\\gamma^{2}\\mathbf{X}_{F,\\text{train}}^{\\top}\\mathbf{X}_{F,\\text{train}}\\hat{\\mathbf{w}} - 2\\gamma^{2}\\mathbf{X}_{F,\\text{train}}^{\\top}\\mathbf{y}_{F,\\text{train}} + 2\\lambda\\hat{\\mathbf{w}} = \\mathbf{0}\n$$\nDividing by $2$ and rearranging the terms to isolate $\\hat{\\mathbf{w}}$:\n$$\n(\\mathbf{X}_{E,\\text{train}}^{\\top}\\mathbf{X}_{E,\\text{train}} + \\gamma^{2}\\mathbf{X}_{F,\\text{train}}^{\\top}\\mathbf{X}_{F,\\text{train}} + \\lambda\\mathbf{I})\\hat{\\mathbf{w}} = \\mathbf{X}_{E,\\text{train}}^{\\top}\\mathbf{y}_{E,\\text{train}} + \\gamma^{2}\\mathbf{X}_{F,\\text{train}}^{\\top}\\mathbf{y}_{F,\\text{train}}\n$$\nThis is a linear system of equations of the form $\\mathbf{A}\\mathbf{x} = \\mathbf{b}$, where:\n- $\\mathbf{A} = \\mathbf{X}_{E,\\text{train}}^{\\top}\\mathbf{X}_{E,\\text{train}} + \\gamma^{2}\\mathbf{X}_{F,\\text{train}}^{\\top}\\mathbf{X}_{F,\\text{train}} + \\lambda\\mathbf{I}$ is a $p \\times p$ matrix.\n- $\\mathbf{x} = \\hat{\\mathbf{w}}$ is the $p \\times 1$ vector of unknown coefficients.\n- $\\mathbf{b} = \\mathbf{X}_{E,\\text{train}}^{\\top}\\mathbf{y}_{E,\\text{train}} + \\gamma^{2}\\mathbf{X}_{F,\\text{train}}^{\\top}\\mathbf{y}_{F,\\text{train}}$ is a $p \\times 1$ vector.\nSince $\\mathbf{X}^{\\top}\\mathbf{X}$ is always positive semi-definite and $\\lambda\\mathbf{I}$ is positive definite for $\\lambda > 0$, the matrix $\\mathbf{A}$ is guaranteed to be positive definite and thus invertible. Even when $\\lambda = 0$, the combined data from energies and forces ensures that $\\mathbf{A}$ is invertible with probability $1$ given that the features are drawn from a continuous distribution and the number of data points ($n_{E,\\text{train}} + n_{F,\\text{train}} = 8 + 24 = 32$) is greater than the number of features ($p=7$).\n\nThe solution $\\hat{\\mathbf{w}}$ is therefore found by solving this linear system.\n\nThe algorithmic procedure is as follows:\n1.  **Generate Data**: Create the matrices $\\mathbf{X}_E$, $\\mathbf{X}_F$ and vectors $\\mathbf{y}_E$, $\\mathbf{y}_F$ according to the specified protocol. This involves using a fixed random seed $s=2025$ to ensure reproducibility.\n    -   The feature dimension is $p=7$.\n    -   The true coefficient vector is $\\mathbf{w}_{\\text{true}} = [0.8, -0.5, 0.3, 0.0, 0.2, -0.1, 0.05]^{\\top}$.\n    -   Energy features $\\mathbf{X}_E \\in \\mathbb{R}^{12 \\times 7}$ are drawn from $\\mathcal{N}(0, 1^2)$.\n    -   Force features $\\mathbf{X}_F \\in \\mathbb{R}^{36 \\times 7}$ are drawn from $\\mathcal{N}(0, 0.4^2)$.\n    -   Energy responses are $\\mathbf{y}_{E} = \\mathbf{X}_{E}\\mathbf{w}_{\\text{true}} + \\boldsymbol{\\varepsilon}_{E}$ with noise $\\varepsilon_{E,i} \\sim \\mathcal{N}(0, 0.01^2)$.\n    -   Force responses are $\\mathbf{y}_{F} = \\mathbf{X}_{F}\\mathbf{w}_{\\text{true}} + \\boldsymbol{\\varepsilon}_{F}$ with noise $\\varepsilon_{F,j} \\sim \\mathcal{N}(0, 0.02^2)$.\n    -   Split the data into training sets ($\\mathbf{X}_{E,\\text{train}}$, $\\mathbf{y}_{E,\\text{train}}$, $\\mathbf{X}_{F,\\text{train}}$, $\\mathbf{y}_{F,\\text{train}}$) and validation sets ($\\mathbf{X}_{E,\\text{val}}$, $\\mathbf{y}_{E,\\text{val}}$, $\\mathbf{X}_{F,\\text{val}}$, $\\mathbf{y}_{F,\\text{val}}$) as specified.\n\n2.  **Iterate Over Hyperparameters**: For each pair $(\\lambda, \\gamma)$ in the test suite:\n    a. Construct the matrix $\\mathbf{A} = \\mathbf{X}_{E,\\text{train}}^{\\top}\\mathbf{X}_{E,\\text{train}} + \\gamma^{2}\\mathbf{X}_{F,\\text{train}}^{\\top}\\mathbf{X}_{F,\\text{train}} + \\lambda\\mathbf{I}_{p \\times p}$ and the vector $\\mathbf{b} = \\mathbf{X}_{E,\\text{train}}^{\\top}\\mathbf{y}_{E,\\text{train}} + \\gamma^{2}\\mathbf{X}_{F,\\text{train}}^{\\top}\\mathbf{y}_{F,\\text{train}}$.\n    b. Solve the linear system $\\mathbf{A}\\hat{\\mathbf{w}} = \\mathbf{b}$ for $\\hat{\\mathbf{w}}(\\lambda,\\gamma)$. This is numerically more stable than computing the inverse of $\\mathbf{A}$.\n\n3.  **Evaluate Performance**: Using the obtained $\\hat{\\mathbf{w}}$ on the validation data:\n    a. Calculate the energy predictions $\\hat{\\mathbf{y}}_{E,\\text{val}} = \\mathbf{X}_{E,\\text{val}}\\hat{\\mathbf{w}}$.\n    b. Compute the energy RMSE: $\\mathrm{RMSE}_{E} = \\sqrt{\\frac{1}{n_{E,\\text{val}}}\\left\\|\\hat{\\mathbf{y}}_{E,\\text{val}} - \\mathbf{y}_{E,\\text{val}}\\right\\|_{2}^{2}}$. The number of validation energies is $n_{E,\\text{val}} = 4$.\n    c. Calculate the force predictions $\\hat{\\mathbf{y}}_{F,\\text{val}} = \\mathbf{X}_{F,\\text{val}}\\hat{\\mathbf{w}}$.\n    d. Compute the force RMSE: $\\mathrm{RMSE}_{F} = \\sqrt{\\frac{1}{n_{F,\\text{val}}}\\left\\|\\hat{\\mathbf{y}}_{F,\\text{val}} - \\mathbf{y}_{F,\\text{val}}\\right\\|_{2}^{2}}$. The number of validation forces is $n_{F,\\text{val}} = 12$.\n    e. Compute the coefficient norm $\\|\\hat{\\mathbf{w}}\\|_{2}$.\n\n4.  **Report Results**: Collect the triplet $[\\mathrm{RMSE}_{E}, \\mathrm{RMSE}_{F}, \\|\\hat{\\mathbf{w}}\\|_{2}]$ for each hyperparameter pair and format them as specified.\n\nThis procedure will be implemented in the provided Python environment.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the regularized linear regression problem for a synthetic SNAP-like dataset.\n    \"\"\"\n    # 1. Data Generation Protocol\n    s = 2025\n    rng = np.random.default_rng(s)\n\n    p = 7\n    n_E_total = 12\n    n_E_train = 8\n    n_E_val = 4\n    n_F_total = 36\n    n_F_train = 24\n    n_F_val = 12\n    \n    sigma_E = 0.01\n    sigma_F = 0.02\n\n    w_true = np.array([0.8, -0.5, 0.3, 0.0, 0.2, -0.1, 0.05])\n\n    # Generate feature matrices\n    X_E = rng.normal(loc=0.0, scale=1.0, size=(n_E_total, p))\n    X_F = rng.normal(loc=0.0, scale=0.4, size=(n_F_total, p))\n\n    # Split feature matrices\n    X_E_train = X_E[:n_E_train, :]\n    X_E_val = X_E[n_E_train:, :]\n    X_F_train = X_F[:n_F_train, :]\n    X_F_val = X_F[n_F_train:, :]\n\n    # Generate responses (energies and forces) with noise\n    eps_E = rng.normal(loc=0.0, scale=sigma_E, size=n_E_total)\n    eps_F = rng.normal(loc=0.0, scale=sigma_F, size=n_F_total)\n    \n    y_E = X_E @ w_true + eps_E\n    y_F = X_F @ w_true + eps_F\n\n    # Split responses\n    y_E_train = y_E[:n_E_train]\n    y_E_val = y_E[n_E_train:]\n    y_F_train = y_F[:n_F_train]\n    y_F_val = y_F[n_F_train:]\n\n    # Test suite of hyperparameters\n    test_cases = [\n        (0.0, 1.0),\n        (0.0001, 1.0),\n        (0.01, 1.0),\n        (1.0, 1.0),\n        (0.01, 0.0),\n        (0.01, 5.0)\n    ]\n\n    results = []\n    \n    # 2. Iterate Over Hyperparameters and Solve\n    for lambda_val, gamma_val in test_cases:\n        # Construct the components of the normal equations\n        # LHS = X_E.T @ X_E + gamma^2 * X_F.T @ X_F + lambda * I\n        # RHS = X_E.T @ y_E + gamma^2 * X_F.T @ y_F\n        \n        A_E = X_E_train.T @ X_E_train\n        A_F = X_F_train.T @ X_F_train\n        b_E = X_E_train.T @ y_E_train\n        b_F = X_F_train.T @ y_F_train\n\n        LHS = A_E + (gamma_val**2) * A_F + lambda_val * np.identity(p)\n        RHS = b_E + (gamma_val**2) * b_F\n\n        # Solve for the estimated coefficients w_hat\n        w_hat = np.linalg.solve(LHS, RHS)\n\n        # 3. Evaluate Performance on Validation Set\n        \n        # RMSE for energies\n        y_E_pred = X_E_val @ w_hat\n        rmse_E = np.sqrt(np.mean((y_E_pred - y_E_val)**2))\n\n        # RMSE for forces\n        y_F_pred = X_F_val @ w_hat\n        rmse_F = np.sqrt(np.mean((y_F_pred - y_F_val)**2))\n        \n        # L2-norm of coefficients\n        w_norm = np.linalg.norm(w_hat)\n        \n        results.append([rmse_E, rmse_F, w_norm])\n\n    # 4. Format and Print Final Output\n    print(f\"[[{results[0][0]},{results[0][1]},{results[0][2]}],[{results[1][0]},{results[1][1]},{results[1][2]}],[{results[2][0]},{results[2][1]},{results[2][2]}],[{results[3][0]},{results[3][1]},{results[3][2]}],[{results[4][0]},{results[4][1]},{results[4][2]}],[{results[5][0]},{results[5][1]},{results[5][2]}]]\")\n\nsolve()\n```"
        }
    ]
}