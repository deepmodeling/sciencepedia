{
    "hands_on_practices": [
        {
            "introduction": "To effectively use relative entropy minimization, we must first understand its mathematical underpinnings. This exercise guides you through the fundamental derivation of the gradient of the Kullback-Leibler ($D_{\\mathrm{KL}}$) divergence with respect to the model parameters. By showing that the optimality condition sets the gradient to zero, you will uncover the celebrated moment-matching principle, which provides a clear statistical interpretation for the optimization procedure .",
            "id": "3802834",
            "problem": "Consider coarse-graining (CG) within equilibrium statistical mechanics. Let $Y$ denote a coarse observable taking values in $\\mathbb{R}^{d}$ with target coarse distribution $P_{Y}$ (the marginal of a fine-scale equilibrium measure). We approximate $P_{Y}$ by a parametric energy-based family $Q_{\\theta}$ with density\n$$\nq_{\\theta}(y) \\equiv \\frac{1}{Z(\\theta)} \\exp\\!\\big(-U_{\\theta}(y)\\big), \\quad Z(\\theta) \\equiv \\int_{\\mathbb{R}^{d}} \\exp\\!\\big(-U_{\\theta}(y)\\big)\\,\\mathrm{d}y,\n$$\nwhere $U_{\\theta}:\\mathbb{R}^{d}\\to\\mathbb{R}$ is a differentiable coarse potential and $\\theta \\in \\mathbb{R}^{p}$ is the parameter vector. The quality of approximation is quantified by the Kullback–Leibler (KL) divergence (also known as relative entropy) from $P_{Y}$ to $Q_{\\theta}$,\n$$\nD_{\\mathrm{KL}}(P_{Y}\\|Q_{\\theta}) \\equiv \\int_{\\mathbb{R}^{d}} p_{Y}(y)\\,\\ln\\!\\left(\\frac{p_{Y}(y)}{q_{\\theta}(y)}\\right)\\,\\mathrm{d}y.\n$$\n\nTask A. Starting only from the above definitions and basic calculus, derive the expression for the gradient $\\nabla_{\\theta} D_{\\mathrm{KL}}(P_{Y}\\|Q_{\\theta})$ in terms of expectations under $P_{Y}$ and $Q_{\\theta}$, and explain why the stationary condition $\\nabla_{\\theta} D_{\\mathrm{KL}}(P_{Y}\\|Q_{\\theta}) = 0$ implies a moment-matching principle when $U_{\\theta}$ is linear in a set of sufficient statistics. Your derivation must not assume or quote any pre-known formulas for the gradient or for the partition function $Z(\\theta)$, and must explicitly justify any interchange of differentiation and integration.\n\nTask B. Specialize to the case $d=1$ and the Gaussian coarse-grained model with energy\n$$\nU_{(\\mu,\\lambda)}(y) \\equiv \\frac{\\lambda}{2}\\,(y-\\mu)^{2}, \\quad \\lambda>0,\n$$\nso that $Q_{(\\mu,\\lambda)}$ is a univariate normal distribution with mean $\\mu$ and variance $\\lambda^{-1}$. Assume $P_{Y}$ admits finite first two moments\n$$\nm \\equiv \\mathbb{E}_{P_{Y}}[Y], \\quad v \\equiv \\mathrm{Var}_{P_{Y}}(Y)>0.\n$$\nUsing your result from Task A, determine the exact minimizer $(\\mu^{\\star},\\lambda^{\\star})$ of $D_{\\mathrm{KL}}(P_{Y}\\|Q_{(\\mu,\\lambda)})$ in closed form as a function of $m$ and $v$. Express your final answer as a row vector $\\big(\\mu^{\\star}\\ \\ \\lambda^{\\star}\\big)$ with no units. No numerical rounding is required.",
            "solution": "We begin from first principles. By definition, the Kullback–Leibler (KL) divergence (relative entropy) from $P_{Y}$ to $Q_{\\theta}$ is\n$$\nD_{\\mathrm{KL}}(P_{Y}\\|Q_{\\theta}) \\equiv \\int_{\\mathbb{R}^{d}} p_{Y}(y)\\,\\ln\\!\\left(\\frac{p_{Y}(y)}{q_{\\theta}(y)}\\right)\\,\\mathrm{d}y.\n$$\nUsing $q_{\\theta}(y) = Z(\\theta)^{-1} \\exp(-U_{\\theta}(y))$, we have\n$$\n\\ln q_{\\theta}(y) = -U_{\\theta}(y) - \\ln Z(\\theta),\n$$\nand hence\n$$\nD_{\\mathrm{KL}}(P_{Y}\\|Q_{\\theta}) = \\int p_{Y}(y)\\,\\big(\\ln p_{Y}(y) + U_{\\theta}(y) + \\ln Z(\\theta)\\big)\\,\\mathrm{d}y.\n$$\nSince $\\int p_{Y}(y)\\,\\ln p_{Y}(y)\\,\\mathrm{d}y$ is independent of $\\theta$, the only $\\theta$-dependence is through the terms involving $U_{\\theta}(y)$ and $\\ln Z(\\theta)$. Assuming regularity conditions that justify differentiation under the integral sign (e.g., dominated convergence and differentiability of $U_{\\theta}$ with integrable dominating functions uniform in $\\theta$ in a neighborhood), we obtain\n$$\n\\nabla_{\\theta} D_{\\mathrm{KL}}(P_{Y}\\|Q_{\\theta}) = \\int p_{Y}(y)\\,\\nabla_{\\theta} U_{\\theta}(y)\\,\\mathrm{d}y + \\nabla_{\\theta} \\ln Z(\\theta).\n$$\nWe now compute $\\nabla_{\\theta}\\ln Z(\\theta)$ from the definition $Z(\\theta)=\\int \\exp(-U_{\\theta}(y))\\,\\mathrm{d}y$. Again under the same regularity conditions,\n$$\n\\nabla_{\\theta} Z(\\theta) = \\int -\\nabla_{\\theta}U_{\\theta}(y)\\,\\exp\\!\\big(-U_{\\theta}(y)\\big)\\,\\mathrm{d}y,\n$$\nand therefore\n$$\n\\nabla_{\\theta}\\ln Z(\\theta) = \\frac{\\nabla_{\\theta}Z(\\theta)}{Z(\\theta)} \n= \\frac{\\int -\\nabla_{\\theta}U_{\\theta}(y)\\,\\exp(-U_{\\theta}(y))\\,\\mathrm{d}y}{\\int \\exp(-U_{\\theta}(y))\\,\\mathrm{d}y}\n= -\\int \\nabla_{\\theta}U_{\\theta}(y)\\,q_{\\theta}(y)\\,\\mathrm{d}y.\n$$\nHence,\n$$\n\\nabla_{\\theta} D_{\\mathrm{KL}}(P_{Y}\\|Q_{\\theta}) \n= \\int p_{Y}(y)\\,\\nabla_{\\theta} U_{\\theta}(y)\\,\\mathrm{d}y - \\int q_{\\theta}(y)\\,\\nabla_{\\theta} U_{\\theta}(y)\\,\\mathrm{d}y\n= \\mathbb{E}_{P_{Y}}\\big[\\nabla_{\\theta}U_{\\theta}(Y)\\big] - \\mathbb{E}_{Q_{\\theta}}\\big[\\nabla_{\\theta}U_{\\theta}(Y)\\big].\n$$\n\nThis establishes the desired expectation form for the gradient starting strictly from the definitions.\n\nTo see why the stationary condition implies a moment-matching principle, suppose that $U_{\\theta}$ is linear in a set of sufficient statistics $\\{\\phi_{i}(y)\\}_{i=1}^{p}$, that is,\n$$\nU_{\\theta}(y) = \\sum_{i=1}^{p} \\theta_{i}\\,\\phi_{i}(y),\n$$\nor more compactly $U_{\\theta}(y)=\\theta^{\\top}\\phi(y)$ with $\\phi(y)\\in\\mathbb{R}^{p}$. Then $\\nabla_{\\theta}U_{\\theta}(y) = \\phi(y)$, and the stationarity condition $\\nabla_{\\theta} D_{\\mathrm{KL}}(P_{Y}\\|Q_{\\theta})=0$ reads\n$$\n\\mathbb{E}_{P_{Y}}\\big[\\phi(Y)\\big] - \\mathbb{E}_{Q_{\\theta}}\\big[\\phi(Y)\\big] = 0,\n$$\ni.e., each component satisfies\n$$\n\\mathbb{E}_{P_{Y}}\\big[\\phi_{i}(Y)\\big] = \\mathbb{E}_{Q_{\\theta}}\\big[\\phi_{i}(Y)\\big], \\quad i\\in\\{1,\\dots,p\\}.\n$$\nThese are moment-matching equations: the model $Q_{\\theta}$ matches the target $P_{Y}$ on the moments induced by the sufficient statistics $\\phi$.\n\nWe now apply this to the univariate Gaussian energy model in Task B. Let $d=1$ and\n$$\nU_{(\\mu,\\lambda)}(y) = \\frac{\\lambda}{2}\\,(y-\\mu)^{2}, \\quad \\lambda>0.\n$$\nThe corresponding model $Q_{(\\mu,\\lambda)}$ is the normal distribution with mean $\\mu$ and variance $\\lambda^{-1}$. We compute the parameter gradients:\n$$\n\\frac{\\partial U_{(\\mu,\\lambda)}(y)}{\\partial \\mu} = -\\lambda\\,(y-\\mu), \n\\quad \n\\frac{\\partial U_{(\\mu,\\lambda)}(y)}{\\partial \\lambda} = \\frac{1}{2}\\,(y-\\mu)^{2}.\n$$\nBy the general gradient identity, the stationarity conditions are\n$$\n\\mathbb{E}_{P_{Y}}\\!\\left[-\\lambda\\,(Y-\\mu)\\right] - \\mathbb{E}_{Q_{(\\mu,\\lambda)}}\\!\\left[-\\lambda\\,(Y-\\mu)\\right] = 0,\n\\quad\n\\mathbb{E}_{P_{Y}}\\!\\left[\\frac{1}{2}\\,(Y-\\mu)^{2}\\right] - \\mathbb{E}_{Q_{(\\mu,\\lambda)}}\\!\\left[\\frac{1}{2}\\,(Y-\\mu)^{2}\\right] = 0.\n$$\nThe first equation simplifies to\n$$\n-\\lambda\\big(\\mathbb{E}_{P_{Y}}[Y]-\\mu\\big) + \\lambda\\big(\\mathbb{E}_{Q_{(\\mu,\\lambda)}}[Y]-\\mu\\big) = 0.\n$$\nSince $\\mathbb{E}_{Q_{(\\mu,\\lambda)}}[Y] = \\mu$, this reduces to $\\mathbb{E}_{P_{Y}}[Y] = \\mu$. With the notation $m \\equiv \\mathbb{E}_{P_{Y}}[Y]$, we conclude\n$$\n\\mu^{\\star} = m.\n$$\nThe second stationarity equation becomes\n$$\n\\mathbb{E}_{P_{Y}}\\!\\left[(Y-\\mu)^{2}\\right] = \\mathbb{E}_{Q_{(\\mu,\\lambda)}}\\!\\left[(Y-\\mu)^{2}\\right].\n$$\nFor $Q_{(\\mu,\\lambda)}$, we have $\\mathbb{E}_{Q_{(\\mu,\\lambda)}}[(Y-\\mu)^{2}] = \\mathrm{Var}_{Q_{(\\mu,\\lambda)}}(Y) = \\lambda^{-1}$. Substituting $\\mu^{\\star} = m$ gives\n$$\n\\mathbb{E}_{P_{Y}}\\!\\left[(Y-m)^{2}\\right] = \\lambda^{-1}.\n$$\nWith $v \\equiv \\mathrm{Var}_{P_{Y}}(Y) = \\mathbb{E}_{P_{Y}}[(Y-m)^{2}]>0$, we obtain\n$$\n\\lambda^{\\star} = \\frac{1}{v}.\n$$\nTherefore, the unique minimizer is\n$$\n\\big(\\mu^{\\star},\\lambda^{\\star}\\big) = \\big(m,\\, v^{-1}\\big),\n$$\nwhich explicitly realizes the moment-matching principle: the optimal Gaussian $Q_{(\\mu,\\lambda)}$ matches the target mean and variance of $P_{Y}$.",
            "answer": "$$\\boxed{\\begin{pmatrix} m & \\frac{1}{v} \\end{pmatrix}}$$"
        },
        {
            "introduction": "Theory comes to life through implementation. This practice moves from abstract derivations to a concrete computational task: building a coarse-grained model for a particle in a classic double-well potential . You will implement the entire workflow, from numerically generating the target probability distribution to optimizing model parameters by minimizing a regularized Kullback-Leibler ($D_{\\mathrm{KL}}$) divergence.",
            "id": "3802808",
            "problem": "Consider a one-dimensional particle with position $x \\in \\mathbb{R}$ evolving in a potential energy landscape given by a double-well potential. The coarse-graining (CG) map bins continuous positions $x$ into a finite number of discrete bins. The aim is to construct a discrete CG model and determine its optimal log-probabilities by minimizing a regularized Kullback-Leibler divergence (KLD). The program you write must implement the following steps, starting from first principles and well-tested definitions.\n\n1. Fundamental base and target distribution. Let the double-well potential be $U(x) = a\\,(x^2 - b^2)^2$, where $a > 0$ and $b > 0$ are parameters. The thermal equilibrium distribution at inverse temperature $\\beta > 0$ is the Boltzmann distribution\n$$\np(x) = \\frac{1}{Z}\\,\\exp\\left(-\\beta\\,U(x)\\right),\n$$\nwith partition function\n$$\nZ = \\int_{x_{\\min}}^{x_{\\max}} \\exp\\left(-\\beta\\,U(x)\\right)\\,\\mathrm{d}x,\n$$\nwhere $[x_{\\min}, x_{\\max}]$ is a finite domain known to contain essentially all the probability mass of $p(x)$ for the parameters under consideration.\n\n2. Coarse-graining map to discrete bins. Partition the interval $[x_{\\min}, x_{\\max}]$ into $K$ equal-width bins with edges $x_0 = x_{\\min} < x_1 < \\cdots < x_K = x_{\\max}$. The CG variable $i \\in \\{1,\\dots,K\\}$ denotes the bin index. The fine-to-coarse marginal probability for bin $i$ is\n$$\nP_i = \\int_{x_{i-1}}^{x_{i}} p(x)\\,\\mathrm{d}x,\n$$\nwhich defines a discrete probability mass function $\\mathbf{P} = (P_1,\\dots,P_K)$ satisfying $\\sum_{i=1}^{K} P_i = 1$ and $P_i \\ge 0$.\n\n3. Discrete model parameterization. Parameterize the CG model distribution $\\mathbf{Q} = (Q_1,\\dots,Q_K)$ via unconstrained log-probabilities $\\mathbf{l} = (l_1,\\dots,l_K)$ according to the softmax relation\n$$\nQ_i(\\mathbf{l}) = \\frac{\\exp(l_i)}{\\sum_{j=1}^{K} \\exp(l_j)}.\n$$\nThis ensures $Q_i \\ge 0$ and $\\sum_{i=1}^K Q_i = 1$.\n\n4. Objective for relative entropy minimization with regularization. Define the Kullback-Leibler divergence (KLD) from the fine-grained CG marginal $\\mathbf{P}$ to the model $\\mathbf{Q}(\\mathbf{l})$ as\n$$\nD_{\\mathrm{KL}}(\\mathbf{P}\\,\\|\\,\\mathbf{Q}(\\mathbf{l})) = \\sum_{i=1}^{K} P_i \\log\\left(\\frac{P_i}{Q_i(\\mathbf{l})}\\right).\n$$\nTo prevent overfitting and break the gauge invariance of $\\mathbf{l}$, introduce a quadratic regularization relative to a reference log-probability vector $\\mathbf{l}^{\\mathrm{ref}}$, with strength $\\lambda > 0$. The full objective is\n$$\n\\mathcal{F}(\\mathbf{l}) = D_{\\mathrm{KL}}(\\mathbf{P}\\,\\|\\,\\mathbf{Q}(\\mathbf{l})) + \\lambda \\sum_{i=1}^{K} \\left(l_i - l_i^{\\mathrm{ref}}\\right)^2.\n$$\nYou must minimize $\\mathcal{F}(\\mathbf{l})$ over $\\mathbf{l} \\in \\mathbb{R}^K$ to compute the optimal log-probabilities $\\mathbf{l}^\\star$.\n\n5. Numerical computation requirements. \n- Compute the bin probabilities $\\mathbf{P}$ by numerical integration of $\\exp(-\\beta U(x))$ over each bin and by computing the partition function $Z$ over $[x_{\\min}, x_{\\max}]$ to ensure proper normalization. Use sufficiently accurate numerical quadrature.\n- Minimize $\\mathcal{F}(\\mathbf{l})$ using a robust, gradient-based convex optimization procedure. You must explicitly construct both the objective $\\mathcal{F}(\\mathbf{l})$ and its gradient with respect to $\\mathbf{l}$ and use them in the optimizer.\n\n6. Test suite. Implement your program to compute $\\mathbf{l}^\\star$ for the following test cases, with equal-width bins over the specified domains and reference log-probabilities as given. For each case, output the optimal log-probability vector $\\mathbf{l}^\\star$ as a list of decimal floats. No physical units are required for the outputs.\n- Case A (happy path): $a = 1.0$, $b = 1.0$, $\\beta = 5.0$, $x_{\\min} = -2.5$, $x_{\\max} = 2.5$, $K = 5$, $\\lambda = 0.1$, $\\mathbf{l}^{\\mathrm{ref}} = (0,0,0,0,0)$.\n- Case B (near-uniform marginal, very weak regularization): $a = 1.0$, $b = 1.0$, $\\beta = 0.1$, $x_{\\min} = -3.0$, $x_{\\max} = 3.0$, $K = 4$, $\\lambda = 10^{-6}$, $\\mathbf{l}^{\\mathrm{ref}} = (0,0,0,0)$.\n- Case C (strong regularization with a skewed reference): $a = 1.0$, $b = 1.0$, $\\beta = 2.0$, $x_{\\min} = -2.0$, $x_{\\max} = 2.0$, $K = 6$, $\\lambda = 10.0$, $\\mathbf{l}^{\\mathrm{ref}} = (-1,0,1,2,1,0)$.\n- Case D (low temperature, sharply bimodal): $a = 1.0$, $b = 1.0$, $\\beta = 50.0$, $x_{\\min} = -2.0$, $x_{\\max} = 2.0$, $K = 8$, $\\lambda = 0.01$, $\\mathbf{l}^{\\mathrm{ref}} = (0,0,0,0,0,0,0,0)$.\n\n7. Final output format. Your program should produce a single line of output containing the list of results for the above four cases, in the following format: a comma-separated list of lists of floats, enclosed in a single pair of square brackets, with each inner list corresponding to $\\mathbf{l}^\\star$ for one case, and with no additional text. For example:\n$$\n\\text{print } [[l^\\star_{A,1},\\dots,l^\\star_{A,K_A}],[l^\\star_{B,1},\\dots,l^\\star_{B,K_B}],\\dots].\n$$\nAll angles, if any, must be in radians; however, no angles appear in this problem. The outputs must be decimal floats. No percentage signs are permitted anywhere in the output. The numerical integrals and optimizations must be performed with sufficient precision to produce stable results.",
            "solution": "The problem requires the determination of optimal log-probabilities for a coarse-grained (CG) model by minimizing a regularized relative entropy (Kullback-Leibler divergence). The procedure involves several steps, from defining the underlying continuous system to performing a numerical optimization for the discrete model parameters.\n\n### Step 1: Target Probability Density and Coarse-Graining\n\nThe system is a one-dimensional particle in a double-well potential $U(x)$, given by:\n$$\nU(x) = a(x^2 - b^2)^2\n$$\nwhere $a > 0$ and $b > 0$. At thermal equilibrium with inverse temperature $\\beta > 0$, the particle's position $x$ follows the Boltzmann distribution over a specified domain $[x_{\\min}, x_{\\max}]$:\n$$\np(x) = \\frac{1}{Z} \\exp(-\\beta U(x))\n$$\nThe normalization constant, known as the partition function, is the integral of the unnormalized Boltzmann factor over this domain:\n$$\nZ = \\int_{x_{\\min}}^{x_{\\max}} \\exp(-\\beta U(x)) \\, \\mathrm{d}x\n$$\nThe coarse-graining procedure maps the continuous domain $[x_{\\min}, x_{\\max}]$ into a set of $K$ discrete bins. The interval is partitioned by $K+1$ equally spaced points $x_0, x_1, \\dots, x_K$ where $x_0 = x_{\\min}$ and $x_K = x_{\\max}$. The width of each bin is $\\Delta x = (x_{\\max} - x_{\\min}) / K$, and the $i$-th bin corresponds to the interval $[x_{i-1}, x_i]$.\n\nThe target probability mass function for the CG model, denoted by $\\mathbf{P} = (P_1, \\dots, P_K)$, is derived by integrating the fine-grained probability density $p(x)$ over each bin:\n$$\nP_i = \\int_{x_{i-1}}^{x_i} p(x) \\, \\mathrm{d}x = \\frac{1}{Z} \\int_{x_{i-1}}^{x_i} \\exp(-\\beta U(x)) \\, \\mathrm{d}x\n$$\nThese integrals must be computed numerically. A robust adaptive quadrature method, such as the one provided by `scipy.integrate.quad`, is appropriate for this task. The calculation proceeds by first computing $Z$ and then computing the integral for each bin $i$ and dividing by $Z$.\n\n### Step 2: The Coarse-Grained Model and Objective Function\n\nThe CG model distribution, $\\mathbf{Q} = (Q_1, \\dots, Q_K)$, is parameterized by a vector of unconstrained log-probabilities $\\mathbf{l} = (l_1, \\dots, l_K)$. The softmax function ensures that $\\mathbf{Q}$ is a valid probability distribution (i.e., $Q_i \\ge 0$ and $\\sum_i Q_i = 1$):\n$$\nQ_i(\\mathbf{l}) = \\frac{\\exp(l_i)}{\\sum_{j=1}^{K} \\exp(l_j)}\n$$\nThe term $\\sum_{j=1}^{K} \\exp(l_j)$ can be numerically unstable if the values of $l_j$ are large or small. Its logarithm is the log-sum-exp function, $\\text{logsumexp}(\\mathbf{l}) = \\log(\\sum_j \\exp(l_j))$, which can be computed in a stable manner. Using this, we can write $\\log Q_i(\\mathbf{l}) = l_i - \\text{logsumexp}(\\mathbf{l})$.\n\nThe goal is to find the optimal parameters $\\mathbf{l}^\\star$ that make the model distribution $\\mathbf{Q}(\\mathbf{l})$ as close as possible to the target distribution $\\mathbf{P}$. This \"closeness\" is measured by the Kullback-Leibler divergence (KLD):\n$$\nD_{\\mathrm{KL}}(\\mathbf{P} \\,\\|\\, \\mathbf{Q}(\\mathbf{l})) = \\sum_{i=1}^{K} P_i \\log\\left(\\frac{P_i}{Q_i(\\mathbf{l})}\\right) = \\sum_{i=1}^{K} P_i (\\log P_i - \\log Q_i(\\mathbf{l}))\n$$\nThe problem specifies adding a quadratic regularization term to the objective function, which penalizes deviations of $\\mathbf{l}$ from a reference vector $\\mathbf{l}^{\\mathrm{ref}}$. The complete objective function to be minimized is:\n$$\n\\mathcal{F}(\\mathbf{l}) = D_{\\mathrm{KL}}(\\mathbf{P} \\,\\|\\, \\mathbf{Q}(\\mathbf{l})) + \\lambda \\sum_{i=1}^{K} (l_i - l_i^{\\mathrm{ref}})^2\n$$\nwhere $\\lambda > 0$ is the regularization strength. This objective function $\\mathcal{F}(\\mathbf{l})$ is strictly convex for $\\lambda > 0$, guaranteeing a unique minimizer $\\mathbf{l}^\\star$.\n\n### Step 3: Gradient-Based Optimization\n\nTo minimize $\\mathcal{F}(\\mathbf{l})$ efficiently, we use a gradient-based optimization algorithm, such as L-BFGS-B. This requires the gradient of $\\mathcal{F}(\\mathbf{l})$ with respect to each component $l_k$ of $\\mathbf{l}$.\n\nLet's derive the gradient $\\nabla_{\\mathbf{l}} \\mathcal{F}(\\mathbf{l})$. We differentiate $\\mathcal{F}(\\mathbf{l})$ with respect to a component $l_k$:\n$$\n\\frac{\\partial \\mathcal{F}}{\\partial l_k} = \\frac{\\partial}{\\partial l_k} \\left( \\sum_{i=1}^{K} P_i \\log P_i - \\sum_{i=1}^{K} P_i \\log Q_i(\\mathbf{l}) \\right) + \\frac{\\partial}{\\partial l_k} \\left( \\lambda \\sum_{i=1}^{K} (l_i - l_i^{\\mathrm{ref}})^2 \\right)\n$$\nThe term $\\sum P_i \\log P_i$ is constant with respect to $\\mathbf{l}$. The derivative of the regularization term is straightforward:\n$$\n\\frac{\\partial}{\\partial l_k} \\left( \\lambda \\sum_{i=1}^{K} (l_i - l_i^{\\mathrm{ref}})^2 \\right) = 2\\lambda (l_k - l_k^{\\mathrm{ref}})\n$$\nNow we focus on the KLD term involving $\\mathbf{Q}(\\mathbf{l})$. Recalling $\\log Q_i(\\mathbf{l}) = l_i - \\text{logsumexp}(\\mathbf{l})$, its contribution to the objective is:\n$$\n-\\sum_{i=1}^{K} P_i (l_i - \\text{logsumexp}(\\mathbf{l})) = -\\sum_{i=1}^{K} P_i l_i + \\left(\\sum_{i=1}^{K} P_i\\right) \\text{logsumexp}(\\mathbf{l}) = -\\sum_{i=1}^{K} P_i l_i + \\text{logsumexp}(\\mathbf{l})\n$$\nwhere we have used the fact that $\\sum_i P_i = 1$. The derivative of this expression with respect to $l_k$ is:\n$$\n\\frac{\\partial}{\\partial l_k} \\left( -\\sum_{i=1}^{K} P_i l_i + \\text{logsumexp}(\\mathbf{l}) \\right) = -P_k + \\frac{\\partial}{\\partial l_k} \\log\\left(\\sum_{j=1}^{K} \\exp(l_j)\\right)\n$$\nThe derivative of the log-sum-exp function with respect to $l_k$ is known to be $Q_k(\\mathbf{l})$:\n$$\n\\frac{\\partial}{\\partial l_k} \\log\\left(\\sum_{j=1}^{K} \\exp(l_j)\\right) = \\frac{1}{\\sum_{j} \\exp(l_j)} \\cdot \\exp(l_k) = Q_k(\\mathbf{l})\n$$\nThus, the derivative of the KLD term is $Q_k(\\mathbf{l}) - P_k$. Combining all parts, the $k$-th component of the gradient is:\n$$\n\\frac{\\partial \\mathcal{F}}{\\partial l_k} = Q_k(\\mathbf{l}) - P_k + 2\\lambda (l_k - l_k^{\\mathrm{ref}})\n$$\nIn vector notation, the full gradient is:\n$$\n\\nabla_{\\mathbf{l}} \\mathcal{F}(\\mathbf{l}) = \\mathbf{Q}(\\mathbf{l}) - \\mathbf{P} + 2\\lambda (\\mathbf{l} - \\mathbf{l}^{\\mathrm{ref}})\n$$\n\n### Step 4: Algorithmic Implementation\n\nThe overall algorithm for each test case is as follows:\n1.  Define the potential $U(x)$, integrand $f(x) = \\exp(-\\beta U(x))$, and domain $[x_{\\min}, x_{\\max}]$.\n2.  Compute the partition function $Z = \\int_{x_{\\min}}^{x_{\\max}} f(x) \\, \\mathrm{d}x$ using numerical quadrature.\n3.  Establish the $K$ bin edges $x_0, \\dots, x_K$ over $[x_{\\min}, x_{\\max}]$.\n4.  Compute the target probability vector $\\mathbf{P}$ by numerically integrating $f(x)$ over each bin $[x_{i-1}, x_i]$ and dividing by $Z$.\n5.  Implement a function that takes $\\mathbf{l}$ as an argument and returns both the objective value $\\mathcal{F}(\\mathbf{l})$ and its gradient $\\nabla_{\\mathbf{l}} \\mathcal{F}(\\mathbf{l})$, using the expressions derived above and numerically stable functions like `logsumexp`.\n6.  Use a numerical optimizer, such as `scipy.optimize.minimize` with the `L-BFGS-B` method, to find the vector $\\mathbf{l}^\\star$ that minimizes $\\mathcal{F}(\\mathbf{l})$. Provide the function from step $5$ along with an initial guess (e.g., $\\mathbf{l}_0 = \\mathbf{l}^{\\mathrm{ref}}$) to the optimizer.\n7.  The result of the optimization, $\\mathbf{l}^\\star$, is the solution for the given test case. This procedure is repeated for all provided test cases.",
            "answer": "```python\nimport numpy as np\nfrom scipy.integrate import quad\nfrom scipy.optimize import minimize\nfrom scipy.special import logsumexp\n\ndef solve():\n    \"\"\"\n    Computes the optimal log-probabilities for a coarse-grained model by minimizing\n    a regularized Kullback-Leibler divergence.\n    \"\"\"\n    test_cases = [\n        # Case A\n        {'a': 1.0, 'b': 1.0, 'beta': 5.0, 'xmin': -2.5, 'xmax': 2.5,\n         'K': 5, 'lam': 0.1, 'l_ref': [0.0, 0.0, 0.0, 0.0, 0.0]},\n        # Case B\n        {'a': 1.0, 'b': 1.0, 'beta': 0.1, 'xmin': -3.0, 'xmax': 3.0,\n         'K': 4, 'lam': 1e-6, 'l_ref': [0.0, 0.0, 0.0, 0.0]},\n        # Case C\n        {'a': 1.0, 'b': 1.0, 'beta': 2.0, 'xmin': -2.0, 'xmax': 2.0,\n         'K': 6, 'lam': 10.0, 'l_ref': [-1.0, 0.0, 1.0, 2.0, 1.0, 0.0]},\n        # Case D\n        {'a': 1.0, 'b': 1.0, 'beta': 50.0, 'xmin': -2.0, 'xmax': 2.0,\n         'K': 8, 'lam': 0.01, 'l_ref': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]},\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        a = case['a']\n        b = case['b']\n        beta = case['beta']\n        xmin = case['xmin']\n        xmax = case['xmax']\n        K = case['K']\n        lam = case['lam']\n        l_ref = np.array(case['l_ref'])\n\n        # 1. Define potential and integrand\n        def U(x):\n            return a * (x**2 - b**2)**2\n\n        def integrand(x):\n            return np.exp(-beta * U(x))\n\n        # 2. Compute partition function Z\n        Z, _ = quad(integrand, xmin, xmax, epsabs=1e-12, epsrel=1e-12)\n        if Z == 0:\n            # This case should not happen with the given parameters\n            raise ValueError(\"Partition function Z is zero.\")\n            \n        # 3. Compute target marginal probabilities P\n        bin_edges = np.linspace(xmin, xmax, K + 1)\n        P = np.zeros(K)\n        for i in range(K):\n            bin_integral, _ = quad(integrand, bin_edges[i], bin_edges[i+1], epsabs=1e-12, epsrel=1e-12)\n            P[i] = bin_integral / Z\n        \n        # Ensure P is a valid probability distribution\n        P[P < 0] = 0\n        P /= np.sum(P)\n\n        # 4. Define objective function and its gradient\n        # Handle the P_i * log(P_i) term, where the result is 0 if P_i = 0\n        P_log_P = np.zeros_like(P)\n        non_zero_mask = P > 0\n        P_log_P[non_zero_mask] = P[non_zero_mask] * np.log(P[non_zero_mask])\n        dkl_const_term = np.sum(P_log_P)\n\n        def objective_and_grad(l, P_vec, lam_val, l_ref_vec, dkl_const):\n            \"\"\"\n            Computes the objective function and its gradient.\n            F(l) = D_KL(P||Q(l)) + lambda * ||l - l_ref||^2\n            \"\"\"\n            # Compute Q(l) using logsumexp for stability\n            lse = logsumexp(l)\n            log_Q = l - lse\n            Q = np.exp(log_Q)\n\n            # Objective Function F(l)\n            # D_KL = sum(P * (logP - logQ))\n            dkl = dkl_const - np.sum(P_vec * log_Q)\n            reg = lam_val * np.sum((l - l_ref_vec)**2)\n            obj_val = dkl + reg\n\n            # Gradient of F(l)\n            # grad(F) = Q - P + 2*lambda*(l - l_ref)\n            grad_vec = Q - P_vec + 2 * lam_val * (l - l_ref_vec)\n            \n            return obj_val, grad_vec\n\n        # 5. Perform the optimization\n        l_initial = np.copy(l_ref)\n        \n        result = minimize(\n            fun=objective_and_grad,\n            x0=l_initial,\n            args=(P, lam, l_ref, dkl_const_term),\n            method='L-BFGS-B',\n            jac=True,  # function returns both objective and gradient\n            options={'gtol': 1e-9, 'disp': False}\n        )\n\n        l_star = result.x\n        all_results.append(l_star.tolist())\n\n    # 6. Format the final output string\n    # E.g., [[-4.7,-0.7,0.3,-0.7,-4.7],[-0.1,0.0,0.0,-0.1]]\n    output_parts = []\n    for res_list in all_results:\n        # Format each list as '[val1,val2,...]'\n        formatted_list = f'[{\",\".join(map(str, res_list))}]'\n        output_parts.append(formatted_list)\n    \n    final_output = f\"[{','.join(output_parts)}]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "A central challenge in coarse-graining is choosing an appropriate set of basis functions to construct an accurate yet simple model. This advanced practice introduces a powerful adaptive scheme that builds the model iteratively, guided by the gradient of the Kullback-Leibler divergence ($D_{\\mathrm{KL}}$) . You will explore the proof that this greedy approach improves the model and implement the algorithm, demonstrating how to systematically select the most informative features for your coarse-grained potential.",
            "id": "3802835",
            "problem": "Consider a finite state space $\\mathcal{X} = \\{1,2,\\dots,n\\}$ with a strictly positive target distribution $\\mu$ over $\\mathcal{X}$ and a collection of candidate basis functions $\\{\\varphi_j\\}_{j=1}^m$, where each $\\varphi_j:\\mathcal{X}\\to\\mathbb{R}$. Define the exponential family model $p_\\theta$ for parameters $\\theta\\in\\mathbb{R}^k$ restricted to an active set of indices $A\\subseteq\\{1,\\dots,m\\}$ as $p_\\theta(x) \\propto \\exp\\left(\\sum_{j\\in A}\\theta_j\\varphi_j(x)\\right)$ for $x\\in\\mathcal{X}$, with a normalizing partition function ensuring $\\sum_{x\\in\\mathcal{X}}p_\\theta(x)=1$. The objective for Relative Entropy Minimization is the Kullback–Leibler (KL) divergence between $\\mu$ and $p_\\theta$, denoted $F(\\theta) = \\mathrm{KL}(\\mu\\parallel p_\\theta)$.\n\nYour tasks are:\n- Starting from the core definition of Kullback–Leibler divergence and properties of the log-partition function for exponential families, derive the gradient and Hessian of the objective $F(\\theta)$ with respect to $\\theta$ restricted to an active set of basis functions. Do not assume any pre-derived formulas; derive them from first principles.\n- Propose an adaptive basis selection scheme for Coarse-Graining (CG) guided by the largest components of the gradient with respect to the candidate basis functions. The scheme should, at each iterate, select a new basis function whose feature vector is not in the linear span of the currently active features and whose associated absolute gradient component is largest among such candidates. Then re-optimize $F(\\theta)$ over the enlarged active set. Prove that adding a new basis strictly decreases the objective $F(\\theta)$ provided that the new direction is not in the current span and the directional derivative at the current optimum along this new direction is non-zero. Your proof must rely only on differentiability and convexity principles derived from the fundamental base above.\n- Implement the scheme as a complete, runnable program that performs the following computations on the specified test suite and outputs a single-line result in the required format.\n\nAssumptions:\n- The state space $\\mathcal{X}$ is finite with $n\\geq 2$.\n- The target distribution $\\mu(x)>0$ for all $x\\in\\mathcal{X}$ and $\\sum_{x\\in\\mathcal{X}}\\mu(x)=1$.\n- The candidate basis functions $\\{\\varphi_j\\}$ are real-valued on $\\mathcal{X}$. Linear span refers to linear dependence of the feature vectors $\\varphi_j$ evaluated on $\\mathcal{X}$.\n\nTest suite:\n- Test case $1$ (general/happy path): Let $n=6$, let the target distribution be $\\mu = [0.05, 0.10, 0.15, 0.25, 0.20, 0.25]$, and the candidate basis set be $\\varphi_1(x)=x$, $\\varphi_2(x)=x^2$, $\\varphi_3(x)=(-1)^x$ evaluated on $\\mathcal{X}=\\{1,2,3,4,5,6\\}$. Initialize the active set as $A=\\{1\\}$ and run one adaptive selection step to add one new basis and re-optimize. Report whether the objective strictly decreases after adding the new basis and re-optimizing.\n- Test case $2$ (boundary/in-span direction): Let $n=5$, let the target distribution be $\\mu = [0.10, 0.15, 0.25, 0.20, 0.30]$, and the candidate basis set be $\\psi_1(x)=x$ and $\\psi_2(x)=2x$ evaluated on $\\mathcal{X}=\\{1,2,3,4,5\\}$, with active set $A=\\{1\\}$. Forcibly attempt to add $\\psi_2$ (which is in the span of the current active set) and re-optimize. Report whether the objective strictly decreases; this case tests the behavior when the new direction lies in the current span.\n- Test case $3$ (edge case/zero directional derivative): Let $n=5$, let $\\tilde{\\varphi}_1(x)=x$ and $\\tilde{\\theta}=0.8$. Define $\\mu$ to be the exponential family distribution generated by $\\tilde{\\varphi}_1$ alone, i.e., $\\mu(x) \\propto \\exp\\left(\\tilde{\\theta}\\,\\tilde{\\varphi}_1(x)\\right)$ on $\\mathcal{X}=\\{1,2,3,4,5\\}$. Let the second candidate basis be $\\tilde{\\varphi}_2(x)=x^2$ and initialize $A=\\{1\\}$. Run one adaptive selection step and re-optimization, and report whether the objective strictly decreases after attempting to add $\\tilde{\\varphi}_2$. This case tests the situation where the directional derivative along an independent new direction is zero at the current optimum.\n\nOutput specification:\n- For each test case, compute a boolean indicating strict decrease of the objective after the attempted addition and re-optimization, where \"strict decrease\" means the re-optimized objective value after addition is less than the objective value before addition by at least a numerical tolerance $\\varepsilon=10^{-10}$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[\\text{result1},\\text{result2},\\text{result3}]$.\n\nAngle units are not used. No physical units are required. The outputs are booleans.",
            "solution": "The problem is valid. It is scientifically grounded in the principles of information theory and statistical modeling, well-posed due to the convexity of the objective function, and stated objectively with all necessary definitions and data.\n\n### Part 1: Derivation of the Gradient and Hessian\n\nLet the state space be $\\mathcal{X} = \\{1, 2, \\dots, n\\}$. The target distribution is $\\mu(x)$ and is strictly positive. The model distribution $p_\\theta(x)$ belongs to an exponential family defined by a set of basis functions $\\{\\varphi_j\\}_{j \\in A}$ where $A$ is the active set of indices. The model is given by:\n$$\np_\\theta(x) = \\frac{1}{Z(\\theta)} \\exp\\left(\\sum_{j \\in A} \\theta_j \\varphi_j(x)\\right)\n$$\nwhere $Z(\\theta)$ is the partition function, ensuring normalization:\n$$\nZ(\\theta) = \\sum_{x \\in \\mathcal{X}} \\exp\\left(\\sum_{j \\in A} \\theta_j \\varphi_j(x)\\right)\n$$\nThe objective is to minimize the Kullback–Leibler (KL) divergence, or relative entropy, of $p_\\theta$ from $\\mu$:\n$$\nF(\\theta) = \\mathrm{KL}(\\mu \\parallel p_\\theta) = \\sum_{x \\in \\mathcal{X}} \\mu(x) \\log\\left(\\frac{\\mu(x)}{p_\\theta(x)}\\right)\n$$\nWe can expand this expression:\n$$\nF(\\theta) = \\sum_{x \\in \\mathcal{X}} \\mu(x) \\log \\mu(x) - \\sum_{x \\in \\mathcal{X}} \\mu(x) \\log p_\\theta(x)\n$$\nThe first term, $\\sum \\mu(x) \\log \\mu(x)$, is the negative entropy of the fixed target distribution $\\mu$ and is a constant with respect to the parameters $\\theta$. Therefore, minimizing $F(\\theta)$ is equivalent to minimizing the negative cross-entropy term, or maximizing the cross-entropy:\n$$\n\\min_\\theta F(\\theta) \\iff \\min_\\theta \\left( -\\sum_{x \\in \\mathcal{X}} \\mu(x) \\log p_\\theta(x) \\right)\n$$\nLet's substitute the expression for $\\log p_\\theta(x)$:\n$$\n\\log p_\\theta(x) = \\sum_{j \\in A} \\theta_j \\varphi_j(x) - \\log Z(\\theta)\n$$\nThe quantity to be minimized becomes:\n$$\nF(\\theta) = C - \\sum_{x \\in \\mathcal{X}} \\mu(x) \\left( \\sum_{j \\in A} \\theta_j \\varphi_j(x) - \\log Z(\\theta) \\right)\n$$\n$$\nF(\\theta) = C + \\log Z(\\theta) \\sum_{x \\in \\mathcal{X}} \\mu(x) - \\sum_{j \\in A} \\theta_j \\sum_{x \\in \\mathcal{X}} \\mu(x) \\varphi_j(x)\n$$\nSince $\\sum_{x \\in \\mathcal{X}} \\mu(x) = 1$ and ignoring the constant $C$, the objective function (up to a constant) is:\n$$\nF(\\theta) = \\log Z(\\theta) - \\sum_{j \\in A} \\theta_j \\langle \\varphi_j \\rangle_\\mu\n$$\nwhere $\\langle \\cdot \\rangle_\\mu$ denotes the expectation with respect to the target distribution $\\mu$, i.e., $\\langle f \\rangle_\\mu = \\sum_{x \\in \\mathcal{X}} \\mu(x) f(x)$.\n\n**Gradient of $F(\\theta)$**\n\nTo find the gradient, we differentiate $F(\\theta)$ with respect to a component $\\theta_k$ for $k \\in A$:\n$$\n\\frac{\\partial F(\\theta)}{\\partial \\theta_k} = \\frac{\\partial}{\\partial \\theta_k} \\log Z(\\theta) - \\frac{\\partial}{\\partial \\theta_k} \\sum_{j \\in A} \\theta_j \\langle \\varphi_j \\rangle_\\mu\n$$\nThe second term is straightforward: $\\frac{\\partial}{\\partial \\theta_k} \\sum_{j \\in A} \\theta_j \\langle \\varphi_j \\rangle_\\mu = \\langle \\varphi_k \\rangle_\\mu$.\n\nFor the first term, we use the chain rule:\n$$\n\\frac{\\partial}{\\partial \\theta_k} \\log Z(\\theta) = \\frac{1}{Z(\\theta)} \\frac{\\partial Z(\\theta)}{\\partial \\theta_k}\n$$\nThe derivative of the partition function is:\n$$\n\\frac{\\partial Z(\\theta)}{\\partial \\theta_k} = \\frac{\\partial}{\\partial \\theta_k} \\sum_{x \\in \\mathcal{X}} \\exp\\left(\\sum_{j \\in A} \\theta_j \\varphi_j(x)\\right) = \\sum_{x \\in \\mathcal{X}} \\varphi_k(x) \\exp\\left(\\sum_{j \\in A} \\theta_j \\varphi_j(x)\\right)\n$$\nSubstituting this back, we get:\n$$\n\\frac{\\partial}{\\partial \\theta_k} \\log Z(\\theta) = \\frac{1}{Z(\\theta)} \\sum_{x \\in \\mathcal{X}} \\varphi_k(x) \\exp\\left(\\sum_{j \\in A} \\theta_j \\varphi_j(x)\\right) = \\sum_{x \\in \\mathcal{X}} \\varphi_k(x) p_\\theta(x) = \\langle \\varphi_k \\rangle_{p_\\theta}\n$$\nThis is a fundamental property of exponential families: the derivative of the log-partition function with respect to a parameter is the expectation of the corresponding sufficient statistic under the model distribution.\n\nCombining the terms, the $k$-th component of the gradient is:\n$$\n\\frac{\\partial F(\\theta)}{\\partial \\theta_k} = \\langle \\varphi_k \\rangle_{p_\\theta} - \\langle \\varphi_k \\rangle_\\mu\n$$\nThe gradient vector $\\nabla F(\\theta)$ has components $(\\langle \\varphi_j \\rangle_{p_\\theta} - \\langle \\varphi_j \\rangle_\\mu)_{j \\in A}$. The minimum of $F(\\theta)$ is achieved when $\\nabla F(\\theta) = \\mathbf{0}$, which implies the moment matching conditions: $\\langle \\varphi_j \\rangle_{p_\\theta} = \\langle \\varphi_j \\rangle_\\mu$ for all $j \\in A$.\n\n**Hessian of $F(\\theta)$**\n\nTo find the Hessian matrix, we compute the second partial derivatives, $\\frac{\\partial^2 F(\\theta)}{\\partial \\theta_l \\partial \\theta_k}$, for $k, l \\in A$:\n$$\n\\frac{\\partial^2 F(\\theta)}{\\partial \\theta_l \\partial \\theta_k} = \\frac{\\partial}{\\partial \\theta_l} \\left( \\langle \\varphi_k \\rangle_{p_\\theta} - \\langle \\varphi_k \\rangle_\\mu \\right) = \\frac{\\partial}{\\partial \\theta_l} \\langle \\varphi_k \\rangle_{p_\\theta} = \\frac{\\partial}{\\partial \\theta_l} \\sum_{x \\in \\mathcal{X}} \\varphi_k(x) p_\\theta(x)\n$$\nWe need the derivative of $p_\\theta(x)$ with respect to $\\theta_l$:\n$$\n\\frac{\\partial p_\\theta(x)}{\\partial \\theta_l} = \\frac{\\partial}{\\partial \\theta_l} \\left( \\frac{\\exp(\\sum_j \\theta_j \\varphi_j(x))}{Z(\\theta)} \\right)\n$$\nUsing the quotient rule:\n$$\n\\frac{\\partial p_\\theta(x)}{\\partial \\theta_l} = \\frac{\\varphi_l(x) \\exp(\\dots) Z(\\theta) - \\exp(\\dots) \\frac{\\partial Z(\\theta)}{\\partial \\theta_l}}{Z(\\theta)^2}\n$$\n$$\n= \\frac{\\varphi_l(x) \\exp(\\dots)}{Z(\\theta)} - \\frac{\\exp(\\dots)}{Z(\\theta)} \\frac{1}{Z(\\theta)} \\frac{\\partial Z(\\theta)}{\\partial \\theta_l} = p_\\theta(x) \\varphi_l(x) - p_\\theta(x) \\langle \\varphi_l \\rangle_{p_\\theta}\n$$\n$$\n= p_\\theta(x) \\left( \\varphi_l(x) - \\langle \\varphi_l \\rangle_{p_\\theta} \\right)\n$$\nNow, substitute this back into the expression for the Hessian entry:\n$$\n\\frac{\\partial^2 F(\\theta)}{\\partial \\theta_l \\partial \\theta_k} = \\sum_{x \\in \\mathcal{X}} \\varphi_k(x) \\left[ p_\\theta(x) \\left( \\varphi_l(x) - \\langle \\varphi_l \\rangle_{p_\\theta} \\right) \\right]\n$$\n$$\n= \\sum_{x \\in \\mathcal{X}} p_\\theta(x) \\varphi_k(x) \\varphi_l(x) - \\langle \\varphi_l \\rangle_{p_\\theta} \\sum_{x \\in \\mathcal{X}} p_\\theta(x) \\varphi_k(x)\n$$\n$$\n= \\langle \\varphi_k \\varphi_l \\rangle_{p_\\theta} - \\langle \\varphi_k \\rangle_{p_\\theta} \\langle \\varphi_l \\rangle_{p_\\theta}\n$$\nThis is the covariance of $\\varphi_k$ and $\\varphi_l$ under the distribution $p_\\theta$. The Hessian matrix $H$ is the covariance matrix of the basis functions:\n$$\nH_{kl} = (\\nabla^2 F(\\theta))_{kl} = \\mathrm{Cov}_{p_\\theta}(\\varphi_k, \\varphi_l)\n$$\nSince a covariance matrix is always positive semi-definite, the objective function $F(\\theta)$ is convex. If the basis functions $\\{\\varphi_j\\}_{j \\in A}$ are linearly independent as vectors on $\\mathcal{X}$, the Hessian is strictly positive definite, and $F(\\theta)$ is strictly convex, guaranteeing a unique minimum.\n\n### Part 2: Adaptive Basis Selection and Proof of Strict Decrease\n\n**Adaptive Scheme**\n\nThe adaptive basis selection scheme iteratively builds a coarse-grained model by adding basis functions that are most informative for reducing the KL divergence.\n\n1.  **Initialization**: Start with an active set $A_0$ (e.g., $A_0 = \\emptyset$, which implies $p_\\theta(x)$ is the uniform distribution) and a set of candidate basis functions $C = \\{\\varphi_1, \\dots, \\varphi_m\\}$.\n2.  **Iteration**: At step $k$, given the active set $A_k$:\n    a.  **Optimize**: Find the optimal parameters $\\theta_k^*$ that minimize $F(\\theta)$ for the model defined by $A_k$. This is achieved by solving $\\nabla_A F(\\theta) = \\mathbf{0}$, which sets $\\langle \\varphi_j \\rangle_{p_{\\theta_k^*}} = \\langle \\varphi_j \\rangle_\\mu$ for all $j \\in A_k$. Let $p_k^* = p_{\\theta_k^*}$.\n    b.  **Evaluate Candidates**: For each candidate basis function $\\varphi_j$ not in the span of the active set, calculate the corresponding gradient component at the current optimum:\n        $$\n        g_j = \\frac{\\partial F}{\\partial \\theta_j}\\bigg|_{p_k^*} = \\langle \\varphi_j \\rangle_{p_k^*} - \\langle \\varphi_j \\rangle_\\mu\n        $$\n        Note that $g_j = 0$ for all $j \\in A_k$.\n    c.  **Select**: Choose the new basis function $\\varphi_{j_{\\text{new}}}$ to add to the active set by finding the one with the largest absolute gradient component among the eligible candidates:\n        $$\n        j_{\\text{new}} = \\arg\\max_{j \\in C \\setminus A_k, \\, \\varphi_j \\notin \\text{span}(\\{\\varphi_i\\}_{i \\in A_k})} |g_j|\n        $$\n    d.  **Update**: Form the new active set $A_{k+1} = A_k \\cup \\{j_{\\text{new}}\\}$.\n3.  **Termination**: Repeat until a stopping criterion is met, such as the maximum number of basis functions is reached, or $\\max_j |g_j| < \\epsilon$ for some small tolerance $\\epsilon$.\n\n**Proof of Strict Decrease**\n\nLet $\\theta^*$ be the optimal parameter vector for an active set $A$, and let $F(\\theta^*)$ be the corresponding minimum objective value. At this point, we have $\\frac{\\partial F}{\\partial \\theta_j}|_{\\theta^*} = 0$ for all $j \\in A$.\n\nNow, we consider adding a new basis function $\\varphi_k$, where $k \\notin A$ and $\\varphi_k$ is not in the linear span of $\\{\\varphi_j\\}_{j \\in A}$. We augment the parameter space from $\\mathbb{R}^{|A|}$ to $\\mathbb{R}^{|A|+1}$. The current optimal point corresponds to an augmented vector $\\tilde{\\theta}^* = (\\theta_1^*, \\dots, \\theta_{|A|}^*, 0)$, where the last component corresponds to the new basis function $\\varphi_k$. The value of the objective function at this point is $F(\\tilde{\\theta}^*) = F(\\theta^*)$.\n\nThe problem states we choose a new basis for which the directional derivative is non-zero. Let $d$ be the unit vector in the direction of the new parameter $\\theta_k$. The directional derivative of $F$ at $\\tilde{\\theta}^*$ along $d$ is:\n$$\n\\nabla F(\\tilde{\\theta}^*) \\cdot d = \\frac{\\partial F}{\\partial \\theta_k}\\bigg|_{\\tilde{\\theta}^*}\n$$\nFrom our gradient derivation, this is:\n$$\n\\frac{\\partial F}{\\partial \\theta_k}\\bigg|_{\\tilde{\\theta}^*} = \\langle \\varphi_k \\rangle_{p_{\\tilde{\\theta}^*}} - \\langle \\varphi_k \\rangle_\\mu\n$$\nSince the $k$-th component of $\\tilde{\\theta}^*$ is zero, $p_{\\tilde{\\theta}^*} = p_{\\theta^*}$. So, the directional derivative is $g_k = \\langle \\varphi_k \\rangle_{p_{\\theta^*}} - \\langle \\varphi_k \\rangle_\\mu$. The selection criterion of the adaptive scheme ensures we pick a $\\varphi_k$ such that $g_k \\neq 0$.\n\nSince $F(\\theta)$ is a convex function and we are at a point $\\tilde{\\theta}^*$ where the gradient is non-zero in some direction (the direction of the new basis vector), $\\tilde{\\theta}^*$ is not a minimum of the function in the augmented space. Specifically, the direction $v = -g_k d$ is a descent direction. For a small step $\\alpha > 0$, a first-order Taylor expansion gives:\n$$\nF(\\tilde{\\theta}^* + \\alpha v) \\approx F(\\tilde{\\theta}^*) + \\alpha (\\nabla F(\\tilde{\\theta}^*) \\cdot v) = F(\\tilde{\\theta}^*) + \\alpha (-g_k d) \\cdot (g_k d) = F(\\tilde{\\theta}^*) - \\alpha g_k^2\n$$\nSince $\\alpha > 0$ and we chose $k$ such that $g_k \\neq 0$, the term $-\\alpha g_k^2$ is strictly negative. This shows that we can take a small step from $\\tilde{\\theta}^*$ and strictly decrease the objective function.\n\nBecause $F(\\theta)$ is convex (and strictly convex since $\\varphi_k$ is linearly independent of the existing basis functions), a descent direction from a non-optimal point guarantees that the new minimum, $F(\\theta^{**})$, found by optimizing over the augmented parameter space, will be strictly lower than the value at the starting point, $F(\\tilde{\\theta}^*)$.\n$$\nF(\\theta^{**}) < F(\\tilde{\\theta}^*) = F(\\theta^*)\n$$\nThus, adding a linearly independent basis function with a non-zero gradient component and re-optimizing strictly decreases the objective function. If the new direction is in the span of the current active set, or if the gradient component is zero, no strict decrease is guaranteed.",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Solves the adaptive basis selection problem for three test cases.\n    \"\"\"\n    \n    class RelativeEntropyMinimizer:\n        \"\"\"\n        Manages the state and calculations for relative entropy minimization.\n        \"\"\"\n        def __init__(self, mu, phis):\n            self.mu = np.array(mu, dtype=float)\n            self.n = len(mu)\n            self.X = np.arange(1, self.n + 1)\n            \n            # Evaluate basis functions over the state space\n            self.phis_matrix = np.array([phi(self.X) for phi in phis], dtype=float)\n            self.m = self.phis_matrix.shape[0]\n\n            # Pre-compute expectations w.r.t. the target distribution mu\n            self.mu_expects = self.phis_matrix @ self.mu\n\n        def _get_model_dist(self, theta, active_indices):\n            \"\"\"Calculates the model distribution p_theta for a given theta and active set.\"\"\"\n            active_phis = self.phis_matrix[active_indices, :]\n            log_p_unnorm = theta @ active_phis\n            \n            # Numerical stability: shift log probabilities before exponentiating\n            # to avoid overflow, without changing the final probability.\n            log_p_unnorm -= np.max(log_p_unnorm)\n            p_unnorm = np.exp(log_p_unnorm)\n            Z = np.sum(p_unnorm)\n            return p_unnorm / Z\n\n        def _objective_func(self, theta, active_indices):\n            \"\"\"Calculates the KL divergence KL(mu || p_theta).\"\"\"\n            p_theta = self._get_model_dist(theta, active_indices)\n            # Add a small epsilon to avoid log(0) if p_theta has zero entries.\n            p_theta = np.clip(p_theta, 1e-15, None)\n            kl_div = np.sum(self.mu * (np.log(self.mu) - np.log(p_theta)))\n            return kl_div\n\n        def _jacobian(self, theta, active_indices):\n            \"\"\"Calculates the gradient of the KL divergence.\"\"\"\n            p_theta = self._get_model_dist(theta, active_indices)\n            active_phis = self.phis_matrix[active_indices, :]\n            p_expects = active_phis @ p_theta\n            mu_expects_active = self.mu_expects[active_indices]\n            return p_expects - mu_expects_active\n\n        def _hessian(self, theta, active_indices):\n            \"\"\"Calculates the Hessian of the KL divergence.\"\"\"\n            p_theta = self._get_model_dist(theta, active_indices)\n            active_phis = self.phis_matrix[active_indices, :]\n            k = len(active_indices)\n            \n            p_expects = active_phis @ p_theta\n            \n            # H_ij = Cov_p(phi_i, phi_j) = E_p[phi_i * phi_j] - E_p[phi_i] * E_p[phi_j]\n            # Broadcast to compute E_p[phi_i * phi_j] efficiently\n            # (k, n) * (1, n) -> (k, n), then matmul with (n, k) -> (k, k)\n            e_phi_phi = (active_phis * p_theta) @ active_phis.T\n            # Outer product of expectations: E_p[phi_i] * E_p[phi_j]\n            e_phi_outer_e_phi = np.outer(p_expects, p_expects)\n\n            hess = e_phi_phi - e_phi_outer_e_phi\n            return hess\n\n        def optimize(self, active_indices):\n            \"\"\"Finds the optimal theta and KL divergence for a given active set.\"\"\"\n            theta0 = np.zeros(len(active_indices))\n            res = minimize(\n                fun=self._objective_func,\n                x0=theta0,\n                args=(active_indices,),\n                method='Newton-CG',\n                jac=self._jacobian,\n                hess=self._hessian,\n                tol=1e-12\n            )\n            return res.x, res.fun\n\n        def get_gradient_components(self, theta_opt, active_indices_opt):\n            \"\"\"Calculates gradient components for all candidate basis functions.\"\"\"\n            p_opt = self._get_model_dist(theta_opt, active_indices_opt)\n            p_expects = self.phis_matrix @ p_opt\n            return p_expects - self.mu_expects\n\n    # --- Test Cases ---\n    \n    # Test Case 1: General/happy path\n    def run_case_1():\n        n = 6\n        mu = np.array([0.05, 0.10, 0.15, 0.25, 0.20, 0.25])\n        phis = [lambda x: x, lambda x: x**2, lambda x: (-1)**x]\n        \n        solver = RelativeEntropyMinimizer(mu, phis)\n        \n        # Step 1: Optimize with initial active set A={1}\n        active_set_1 = [0]\n        theta_1, f_1 = solver.optimize(active_set_1)\n        \n        # Step 2: Adaptive selection\n        # Check linear independence of candidates\n        candidate_indices = [i for i in range(solver.m) if i not in active_set_1]\n        active_vectors = solver.phis_matrix[active_set_1]\n\n        eligible_candidates = []\n        for idx in candidate_indices:\n            candidate_vec = solver.phis_matrix[idx]\n            # Check if candidate_vec is in the span of active_vectors\n            mat = np.vstack([active_vectors, candidate_vec])\n            if np.linalg.matrix_rank(mat) > np.linalg.matrix_rank(active_vectors):\n                eligible_candidates.append(idx)\n\n        # Calculate gradients for eligible candidates\n        grads = solver.get_gradient_components(theta_1, active_set_1)\n        best_candidate = -1\n        max_grad = -1.0\n        for idx in eligible_candidates:\n            if abs(grads[idx]) > max_grad:\n                max_grad = abs(grads[idx])\n                best_candidate = idx\n\n        # Step 3: Re-optimize with the new basis function\n        active_set_2 = sorted(active_set_1 + [best_candidate])\n        _, f_2 = solver.optimize(active_set_2)\n        \n        # Step 4: Check for strict decrease\n        return (f_1 - f_2) > 1e-10\n\n    # Test Case 2: In-span direction\n    def run_case_2():\n        n = 5\n        mu = np.array([0.10, 0.15, 0.25, 0.20, 0.30])\n        phis = [lambda x: x, lambda x: 2*x]\n        \n        solver = RelativeEntropyMinimizer(mu, phis)\n        \n        # Step 1: Optimize with initial active set A={1}\n        active_set_1 = [0]\n        _, f_1 = solver.optimize(active_set_1)\n        \n        # Step 2: Forcibly add linearly dependent basis psi_2\n        active_set_2 = [0, 1]\n        \n        # Re-optimization will likely fail to find a unique theta,\n        # but the objective value should plateau at f_1.\n        _, f_2 = solver.optimize(active_set_2)\n        \n        return (f_1 - f_2) > 1e-10\n\n    # Test Case 3: Zero directional derivative\n    def run_case_3():\n        n = 5\n        tilde_theta_val = 0.8\n        X = np.arange(1, n + 1)\n        \n        log_mu_unnorm = tilde_theta_val * X\n        mu_unnorm = np.exp(log_mu_unnorm)\n        mu = mu_unnorm / np.sum(mu_unnorm)\n        \n        phis = [lambda x: x, lambda x: x**2]\n        \n        solver = RelativeEntropyMinimizer(mu, phis)\n        \n        # Step 1: Optimize with initial active set A={1}\n        active_set_1 = [0]\n        theta_1, f_1 = solver.optimize(active_set_1)\n        # We expect theta_1 ~ [0.8] and f_1 ~ 0.0\n\n        # Step 2: Attempt to add phi_2. The gradient should be zero.\n        grads = solver.get_gradient_components(theta_1, active_set_1)\n        # As shown in the proof, the gradient for candidate 1 (x^2) should be 0.\n        # grads[1] should be very close to zero.\n        \n        # Step 3: Forcibly re-optimize with the new basis function\n        active_set_2 = [0, 1]\n        _, f_2 = solver.optimize(active_set_2)\n        \n        # The new optimum should be theta=[0.8, 0.0] and f=0.0.\n        return (f_1 - f_2) > 1e-10\n\n    results = [run_case_1(), run_case_2(), run_case_3()]\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        }
    ]
}