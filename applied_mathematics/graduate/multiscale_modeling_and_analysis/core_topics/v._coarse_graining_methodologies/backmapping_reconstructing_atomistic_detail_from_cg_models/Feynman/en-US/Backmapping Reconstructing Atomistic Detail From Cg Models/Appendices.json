{
    "hands_on_practices": [
        {
            "introduction": "A common challenge in backmapping is that initial, naively reconstructed structures can contain unphysical features like high-frequency bond distortions. This exercise introduces a foundational technique to address this: constrained energy minimization. You will implement a method to relax these distortions and find a locally optimal atomistic structure that strictly adheres to the coarse-grained information, providing a practical introduction to solving constrained optimization problems in molecular modeling. ",
            "id": "3735180",
            "problem": "Consider a one-dimensional atomistic chain of $N=4$ atoms with positions collected in a vector $x \\in \\mathbb{R}^4$. The atomistic potential energy $U_{\\mathrm{AA}}(x)$ is modeled by a sum of harmonic springs between consecutive atoms with uniform stiffness $k>0$ and equilibrium spacing $a>0$, defined by\n$$\nU_{\\mathrm{AA}}(x) = \\frac{1}{2} k \\sum_{i=1}^{3} \\left(x_{i+1} - x_i - a\\right)^2.\n$$\nIntroduce the linear difference operator $D \\in \\mathbb{R}^{3 \\times 4}$ given by\n$$\nD = \\begin{bmatrix}\n-1 & 1 & 0 & 0 \\\\\n0 & -1 & 1 & 0 \\\\\n0 & 0 & -1 & 1\n\\end{bmatrix},\n$$\nso that $U_{\\mathrm{AA}}(x) = \\frac{1}{2} k \\left\\| D x - a \\mathbf{1} \\right\\|_2^2$, where $\\mathbf{1} \\in \\mathbb{R}^3$ is the vector of ones. Define a coarse-grained mapping $M \\in \\mathbb{R}^{2 \\times 4}$ that maps the atomistic positions $x$ to two coarse-grained bead positions $y \\in \\mathbb{R}^2$ via bead-wise averages,\n$$\nM = \\begin{bmatrix}\n\\frac{1}{2} & \\frac{1}{2} & 0 & 0 \\\\\n0 & 0 & \\frac{1}{2} & \\frac{1}{2}\n\\end{bmatrix}, \\quad \\text{so that} \\quad M x = y, \\quad \\text{i.e., } y_1 = \\frac{x_1 + x_2}{2}, \\; y_2 = \\frac{x_3 + x_4}{2}.\n$$\nThe task is to apply constrained energy minimization to relax high-frequency distortions while exactly preserving the coarse-grained mapping constraint $M(x) = y$, and then quantify the impact on the distribution of $U_{\\mathrm{AA}}$ across a specified set of initial distortions.\n\nFor a given coarse-grained state $y \\in \\mathbb{R}^2$, define a base atomistic configuration $x^{\\mathrm{base}}(y,a) \\in \\mathbb{R}^4$ by\n$$\nx^{\\mathrm{base}}(y,a) = \\begin{bmatrix}\ny_1 - \\frac{a}{2} \\\\\ny_1 + \\frac{a}{2} \\\\\ny_2 - \\frac{a}{2} \\\\\ny_2 + \\frac{a}{2}\n\\end{bmatrix}.\n$$\nA high-frequency distortion that preserves the coarse mapping $M x = y$ is any $d(s_1,s_2) \\in \\mathbb{R}^4$ of the form\n$$\nd(s_1,s_2) = \\begin{bmatrix}\ns_1 \\\\\n- s_1 \\\\\ns_2 \\\\\n- s_2\n\\end{bmatrix},\n$$\nfor real scalars $(s_1,s_2)$, since $M d(s_1,s_2) = 0$. For a list of distortion amplitudes $\\{(s_1^{(j)}, s_2^{(j)})\\}_{j=1}^{S}$, define initial atomistic configurations\n$$\nx^{(0)}_j = x^{\\mathrm{base}}(y,a) + d\\!\\left(s_1^{(j)}, s_2^{(j)}\\right), \\quad j = 1,\\dots,S,\n$$\nwhich all satisfy $M x^{(0)}_j = y$. For each initial configuration, evaluate the initial energy $U_{\\mathrm{AA}}\\!\\left(x^{(0)}_j\\right)$ and compute the sample mean and population variance\n$$\n\\mu_{\\mathrm{pre}} = \\frac{1}{S} \\sum_{j=1}^{S} U_{\\mathrm{AA}}\\!\\left(x^{(0)}_j\\right), \\qquad\n\\sigma^2_{\\mathrm{pre}} = \\frac{1}{S} \\sum_{j=1}^{S} \\left( U_{\\mathrm{AA}}\\!\\left(x^{(0)}_j\\right) - \\mu_{\\mathrm{pre}} \\right)^2.\n$$\n\nTo relax high-frequency distortions, perform constrained energy minimization of $U_{\\mathrm{AA}}(x)$ under the linear equality constraint $M x = y$. Starting from the fundamental definition of $U_{\\mathrm{AA}}(x)$ and linear constraints, derive the constrained minimizer $\\hat{x}(y)$ using Lagrange multipliers. Then compute the minimized energy $U_{\\mathrm{AA}}\\!\\left(\\hat{x}(y)\\right)$ and define the post-minimization distribution over the same set of initial distortions. Because the minimizer is unique for fixed $y$, the post-minimization energy is constant across the distortion set, giving\n$$\n\\mu_{\\mathrm{post}} = U_{\\mathrm{AA}}\\!\\left(\\hat{x}(y)\\right), \\qquad \\sigma^2_{\\mathrm{post}} = 0.\n$$\nQuantify the impact of minimization on the distribution by the differences\n$$\n\\Delta \\mu = \\mu_{\\mathrm{pre}} - \\mu_{\\mathrm{post}}, \\qquad \\Delta \\sigma^2 = \\sigma^2_{\\mathrm{pre}} - \\sigma^2_{\\mathrm{post}} = \\sigma^2_{\\mathrm{pre}}.\n$$\nAll computations are in dimensionless units; report energies and statistics as dimensionless numeric values.\n\nImplement a program that constructs $D$ and $M$, forms the constrained minimization, and computes $\\Delta \\mu$ and $\\Delta \\sigma^2$ for each of the following test cases. Use the population variance definition given above. No randomness is permitted; all results must be determined by the specified inputs.\n\nTest Suite:\n- Case $1$ (happy path): $k = 10$, $a = 1$, $y = [0, 3]$, distortions $\\{(s_1^{(j)}, s_2^{(j)})\\}_{j=1}^{5}$ with $(0.2,-0.1)$, $(0.4,0.0)$, $(-0.3,0.6)$, $(0.0,0.0)$, $(1.0,-0.5)$.\n- Case $2$ (boundary, zero distortions): $k = 5$, $a = 1$, $y = [1, 4]$, distortions $\\{(0.0, 0.0)\\}$.\n- Case $3$ (soft springs): $k = 0.1$, $a = 1.5$, $y = [2.0, 2.0]$, distortions $\\{(0.5, -0.5), (0.2, 0.3), (0.0, 0.0), (-0.4, 0.1)\\}$.\n- Case $4$ (stiff springs, strain): $k = 1000$, $a = 1.0$, $y = [0.0, 0.5]$, distortions $\\{(0.1, -0.1), (0.2, 0.3), (-0.2, 0.0)\\}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, output the pair $[\\Delta \\mu, \\Delta \\sigma^2]$, and aggregate all four cases in order into one list, for example, $[[\\Delta \\mu_1,\\Delta \\sigma^2_1],[\\Delta \\mu_2,\\Delta \\sigma^2_2],[\\Delta \\mu_3,\\Delta \\sigma^2_3],[\\Delta \\mu_4,\\Delta \\sigma^2_4]]$.",
            "solution": "The problem is valid as it is scientifically grounded in classical mechanics and linear algebra, well-posed, objective, and internally consistent. It presents a standard, solvable problem in multiscale modeling, specifically the backmapping from a coarse-grained representation to a detailed atomistic configuration via constrained energy minimization. All required data and definitions are provided.\n\nThe core of the task is to find the atomistic configuration $\\hat{x} \\in \\mathbb{R}^4$ that minimizes the atomistic potential energy $U_{\\mathrm{AA}}(x)$ subject to a linear constraint $M x = y$, where $y$ is a given coarse-grained state. The potential energy is a quadratic function of the atomic positions $x$:\n$$\nU_{\\mathrm{AA}}(x) = \\frac{1}{2} k \\| D x - a \\mathbf{1} \\|_2^2\n$$\nThis is a quadratic programming problem with linear equality constraints. We can solve this using the method of Lagrange multipliers. The objective function to minimize is $f(x) = U_{\\mathrm{AA}}(x)$, and the constraint is $g(x) = M x - y = 0$. The Lagrangian $\\mathcal{L}(x, \\lambda)$ is defined as:\n$$\n\\mathcal{L}(x, \\lambda) = f(x) + \\lambda^T g(x) = \\frac{1}{2} k (D x - a \\mathbf{1})^T (D x - a \\mathbf{1}) + \\lambda^T (M x - y)\n$$\nwhere $\\lambda \\in \\mathbb{R}^2$ is the vector of Lagrange multipliers. To find the minimizer $\\hat{x}$, we find the stationary point of the Lagrangian by setting its gradients with respect to $x$ and $\\lambda$ to zero.\n\nThe gradient with respect to $x$ is:\n$$\n\\nabla_x \\mathcal{L} = \\nabla_x \\left( \\frac{1}{2} k (x^T D^T D x - 2a \\mathbf{1}^T D x + a^2 \\mathbf{1}^T \\mathbf{1}) + \\lambda^T M x - \\lambda^T y \\right) = k (D^T D x - a D^T \\mathbf{1}) + M^T \\lambda\n$$\nSetting $\\nabla_x \\mathcal{L} = 0$ gives the first Karush-Kuhn-Tucker (KKT) condition:\n$$\nk (D^T D) x + M^T \\lambda = k a D^T \\mathbf{1}\n$$\nThe gradient with respect to $\\lambda$ is:\n$$\n\\nabla_\\lambda \\mathcal{L} = M x - y\n$$\nSetting $\\nabla_\\lambda \\mathcal{L} = 0$ gives the second KKT condition, which is simply the original constraint:\n$$\nM x = y\n$$\nThese two conditions form a system of linear equations for the unknown minimizer $\\hat{x}$ and the Lagrange multipliers $\\lambda$. We can write this system in a block matrix form:\n$$\n\\begin{bmatrix}\nk D^T D & M^T \\\\\nM & 0\n\\end{bmatrix}\n\\begin{bmatrix}\nx \\\\\n\\lambda\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nk a D^T \\mathbf{1} \\\\\ny\n\\end{bmatrix}\n$$\nwhere the block matrix on the left is the KKT matrix. This is a $6 \\times 6$ system, as $x \\in \\mathbb{R}^4$ and $\\lambda \\in \\mathbb{R}^2$. The matrix $D^T D$ is the discrete one-dimensional Laplacian operator $\\begin{bmatrix} 1 & -1 & 0 & 0 \\\\ -1 & 2 & -1 & 0 \\\\ 0 & -1 & 2 & -1 \\\\ 0 & 0 & -1 & 1 \\end{bmatrix}$. The KKT matrix is invertible, guaranteeing a unique solution $(\\hat{x}, \\lambda)$ for any valid set of parameters. The solution for $x$ is the constrained minimizer $\\hat{x}(y)$.\n\nThe overall algorithm is as follows:\n1.  For each test case, specified by parameters $k$, $a$, $y$, and a set of $S$ distortion pairs $\\{(s_1^{(j)}, s_2^{(j)})\\}$, first calculate the initial energies.\n    a. Construct the base configuration $x^{\\mathrm{base}}(y,a) = [y_1 - a/2, y_1 + a/2, y_2 - a/2, y_2 + a/2]^T$.\n    b. For each $j=1, \\dots, S$, construct the initial configuration $x^{(0)}_j = x^{\\mathrm{base}} + d(s_1^{(j)}, s_2^{(j)})$.\n    c. Evaluate the energy $U_{\\mathrm{AA}}(x^{(0)}_j) = \\frac{1}{2} k \\| D x^{(0)}_j - a \\mathbf{1} \\|_2^2$ for each initial configuration.\n    d. Compute the pre-minimization sample mean $\\mu_{\\mathrm{pre}}$ and population variance $\\sigma^2_{\\mathrm{pre}}$ of these initial energies.\n\n2.  Next, compute the single post-minimization energy.\n    a. Construct the $6 \\times 6$ KKT matrix and the $6 \\times 1$ right-hand side vector using the given $k$, $a$, and $y$.\n    b. Solve the linear system to find the solution vector, from which the constrained minimizer $\\hat{x}(y)$ is extracted.\n    c. Calculate the minimized energy $\\mu_{\\mathrm{post}} = U_{\\mathrm{AA}}(\\hat{x}(y))$. The post-minimization variance is $\\sigma^2_{\\mathrm{post}} = 0$, as the minimizer $\\hat{x}(y)$ is unique for a given $y$ and independent of the initial high-frequency distortion.\n\n3.  Finally, compute the required differences: $\\Delta \\mu = \\mu_{\\mathrm{pre}} - \\mu_{\\mathrm{post}}$ and $\\Delta \\sigma^2 = \\sigma^2_{\\mathrm{pre}} - \\sigma^2_{\\mathrm{post}} = \\sigma^2_{\\mathrm{pre}}$.\n\nThis procedure is implemented for each of the four test cases provided.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the constrained energy minimization problem for four test cases.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"k\": 10.0,\n            \"a\": 1.0,\n            \"y\": np.array([0.0, 3.0]),\n            \"distortions\": [(0.2, -0.1), (0.4, 0.0), (-0.3, 0.6), (0.0, 0.0), (1.0, -0.5)]\n        },\n        {\n            \"k\": 5.0,\n            \"a\": 1.0,\n            \"y\": np.array([1.0, 4.0]),\n            \"distortions\": [(0.0, 0.0)]\n        },\n        {\n            \"k\": 0.1,\n            \"a\": 1.5,\n            \"y\": np.array([2.0, 2.0]),\n            \"distortions\": [(0.5, -0.5), (0.2, 0.3), (0.0, 0.0), (-0.4, 0.1)]\n        },\n        {\n            \"k\": 1000.0,\n            \"a\": 1.0,\n            \"y\": np.array([0.0, 0.5]),\n            \"distortions\": [(0.1, -0.1), (0.2, 0.3), (-0.2, 0.0)]\n        }\n    ]\n\n    # Define constant matrices D and M\n    D = np.array([\n        [-1.0, 1.0, 0.0, 0.0],\n        [0.0, -1.0, 1.0, 0.0],\n        [0.0, 0.0, -1.0, 1.0]\n    ])\n    M = np.array([\n        [0.5, 0.5, 0.0, 0.0],\n        [0.0, 0.0, 0.5, 0.5]\n    ])\n    \n    DTD = D.T @ D\n    MT = M.T\n    \n    def compute_U_aa(x, k, a):\n        \"\"\"Computes the atomistic potential energy.\"\"\"\n        bonds = D @ x\n        a_vec = a * np.ones(3)\n        return 0.5 * k * np.sum((bonds - a_vec)**2)\n\n    results = []\n    for case in test_cases:\n        k = case[\"k\"]\n        a = case[\"a\"]\n        y = case[\"y\"]\n        distortions = case[\"distortions\"]\n\n        # --- Pre-minimization statistics ---\n        x_base = np.array([y[0] - a/2, y[0] + a/2, y[1] - a/2, y[1] + a/2])\n        \n        initial_energies = []\n        for s1, s2 in distortions:\n            d = np.array([s1, -s1, s2, -s2])\n            x0 = x_base + d\n            energy = compute_U_aa(x0, k, a)\n            initial_energies.append(energy)\n        \n        energies_arr = np.array(initial_energies)\n        mu_pre = np.mean(energies_arr)\n        # Population variance (ddof=0 is the default in np.var)\n        sigma2_pre = np.var(energies_arr)\n\n        # --- Post-minimization statistics ---\n        # Construct and solve the KKT system\n        # [ k*DTD  M^T ] [x] = [ k*a*D^T*1 ]\n        # [ M      0   ] [l]   [ y         ]\n        \n        # KKT matrix (6x6)\n        kkt_mat = np.zeros((6, 6))\n        kkt_mat[0:4, 0:4] = k * DTD\n        kkt_mat[0:4, 4:6] = MT\n        kkt_mat[4:6, 0:4] = M\n        \n        # RHS vector (6x1)\n        rhs = np.zeros(6)\n        DT_ones = D.T @ np.ones(3)\n        rhs[0:4] = k * a * DT_ones\n        rhs[4:6] = y\n        \n        # Solve the system\n        solution = np.linalg.solve(kkt_mat, rhs)\n        x_hat = solution[0:4]\n        \n        mu_post = compute_U_aa(x_hat, k, a)\n        # sigma2_post is 0 by definition\n\n        # --- Calculate differences ---\n        delta_mu = mu_pre - mu_post\n        delta_sigma2 = sigma2_pre # since sigma2_post is 0\n        \n        results.append([delta_mu, delta_sigma2])\n\n    # Format output as a string representing a list of lists.\n    # e.g., [[val1, val2], [val3, val4]]\n    result_str = \"[\" + \",\".join([f\"[{r[0]},{r[1]}]\" for r in results]) + \"]\"\n    print(result_str)\n\nsolve()\n```"
        },
        {
            "introduction": "While strict constraints are useful, a more flexible approach uses \"soft\" harmonic restraints to guide the atomistic system towards the coarse-grained target. This raises a critical question: how strong should these restraints be? This practice moves from a purely geometric perspective to a statistical one, tasking you with deriving the optimal restraint stiffness from first principles of Bayesian inference. By doing so, you will establish the rigorous connection between a practical simulation tool and the underlying posterior probability distribution, ensuring your backmapping protocol is statistically sound. ",
            "id": "3735196",
            "problem": "Consider a thermally equilibrated atomistic system with coordinates $x \\in \\mathbb{R}^{n}$ and potential energy $U(x)$ at absolute temperature $T$. The equilibrium distribution over $x$ is the Boltzmann distribution $p(x) \\propto \\exp(-\\beta U(x))$, where $\\beta = 1/(k_B T)$ and $k_B$ is the Boltzmann constant. Let $M: \\mathbb{R}^{n} \\to \\mathbb{R}^{m}$ be a known many-to-one coarse-graining map that returns coarse descriptors $m = M(x)$, such as bead positions in a coarse-grained representation.\n\nSuppose we are given target coarse-grained coordinates $y \\in \\mathbb{R}^{m}$ and we wish to reconstruct atomistic detail by sampling the conditional distribution $p(x \\mid y)$ in a way that does not bias the intended posterior. Assume that the observation model for the coarse data is isotropic Gaussian in the mapped space, meaning $y$ is generated from $M(x)$ via $y = M(x) + \\varepsilon$ with $\\varepsilon$ distributed as a zero-mean Gaussian whose covariance is $\\sigma^{2} I_{m}$, where $I_{m}$ is the $m \\times m$ identity matrix and $\\sigma^{2} > 0$ is known. Under this model, the likelihood is $p(y \\mid x) \\propto \\exp\\!\\left(-\\frac{1}{2 \\sigma^{2}} \\|M(x) - y\\|^{2}\\right)$.\n\nIn practice, a common strategy for backmapping in multiscale modeling and analysis is to add a soft harmonic restraint energy $E_{\\mathrm{rest}}(x;y) = \\frac{\\kappa}{2} \\|M(x) - y\\|^{2}$ to the atomistic potential and sample from the restrained Boltzmann distribution $q(x \\mid y) \\propto \\exp\\!\\left(-\\beta \\left[ U(x) + E_{\\mathrm{rest}}(x;y) \\right]\\right)$. To balance fidelity to the coarse target $y$ and relaxation of atomistic degrees of freedom, and to avoid biasing the conditional distribution $p(x \\mid y)$, choose $\\kappa$ so that $q(x \\mid y)$ matches $p(x \\mid y)$ under the stated observation model.\n\nDerive, from first principles and without invoking any shortcut formulas, a closed-form analytic expression for the optimal scalar stiffness $\\kappa$ in terms of $k_B$, $T$, and $\\sigma^{2}$ that ensures the restrained ensemble $q(x \\mid y)$ reproduces the Bayesian posterior $p(x \\mid y)$ implied by the Gaussian observation model. Express your final answer as a single analytic expression. Do not substitute numerical values and do not include units in your final boxed answer. For interpretive clarity, $\\kappa$ carries units of energy per squared-length (for example, kilojoules per mole per square nanometer). No numerical rounding is required.",
            "solution": "The objective is to determine the optimal scalar stiffness constant $\\kappa$ such that the restrained Boltzmann distribution $q(x \\mid y)$ is equivalent to the Bayesian posterior distribution $p(x \\mid y)$. This equivalence requires that their probability density functions be identical.\n\nFirst, we derive the functional form of the posterior distribution $p(x \\mid y)$ using Bayes' theorem. The posterior distribution of the atomistic coordinates $x$ given the observed coarse-grained data $y$ is proportional to the product of the likelihood $p(y \\mid x)$ and the prior $p(x)$:\n$$p(x \\mid y) \\propto p(y \\mid x) p(x)$$\nThe problem provides the expressions for the prior and the likelihood. The prior distribution $p(x)$ is the canonical Boltzmann distribution for the atomistic system at absolute temperature $T$:\n$$p(x) \\propto \\exp(-\\beta U(x))$$\nwhere $\\beta = 1/(k_B T)$, $k_B$ is the Boltzmann constant, and $U(x)$ is the atomistic potential energy.\n\nThe likelihood $p(y \\mid x)$ is derived from the observation model $y = M(x) + \\varepsilon$, where the noise $\\varepsilon$ follows a zero-mean Gaussian distribution with covariance $\\sigma^2 I_m$. The probability density for a given noise vector $\\varepsilon = y - M(x)$ is thus proportional to $\\exp(-\\frac{1}{2\\sigma^2} \\|\\varepsilon\\|^2)$. This gives the likelihood function:\n$$p(y \\mid x) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2} \\|y - M(x)\\|^2\\right)$$\nSince the squared Euclidean norm $\\|A-B\\|^2$ is equal to $\\|B-A\\|^2$, we can write this as:\n$$p(y \\mid x) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2} \\|M(x) - y\\|^2\\right)$$\nSubstituting the expressions for the prior and the likelihood into Bayes' theorem, we obtain the functional form of the posterior distribution:\n$$p(x \\mid y) \\propto \\exp(-\\beta U(x)) \\cdot \\exp\\left(-\\frac{1}{2\\sigma^2} \\|M(x) - y\\|^2\\right)$$\nBy the properties of the exponential function, we combine the terms in the exponent:\n$$p(x \\mid y) \\propto \\exp\\left(-\\beta U(x) - \\frac{1}{2\\sigma^2} \\|M(x) - y\\|^2\\right)$$\n\nNext, we analyze the functional form of the restrained Boltzmann distribution $q(x \\mid y)$. This distribution is defined by supplementing the atomistic potential energy $U(x)$ with a harmonic restraint energy $E_{\\mathrm{rest}}(x;y)$:\n$$q(x \\mid y) \\propto \\exp\\left(-\\beta \\left[U(x) + E_{\\mathrm{rest}}(x;y)\\right]\\right)$$\nThe restraint energy is given as $E_{\\mathrm{rest}}(x;y) = \\frac{\\kappa}{2} \\|M(x) - y\\|^2$. Substituting this into the expression for $q(x \\mid y)$ gives:\n$$q(x \\mid y) \\propto \\exp\\left(-\\beta \\left[U(x) + \\frac{\\kappa}{2} \\|M(x) - y\\|^2\\right]\\right)$$\nDistributing the inverse temperature $\\beta$ over the terms in the square brackets yields:\n$$q(x \\mid y) \\propto \\exp\\left(-\\beta U(x) - \\frac{\\beta \\kappa}{2} \\|M(x) - y\\|^2\\right)$$\n\nThe problem requires us to find the value of $\\kappa$ that makes $q(x \\mid y)$ and $p(x \\mid y)$ identical. For two probability distributions defined up to a normalization constant to be identical, the functional dependence of their densities on the variable $x$ must be the same. This means the exponents in their respective expressions must be equal, potentially up to an additive constant that is independent of $x$.\n\nComparing the exponents of $p(x \\mid y)$ and $q(x \\mid y)$:\n$$\\text{Exponent of } p(x \\mid y): \\quad -\\beta U(x) - \\frac{1}{2\\sigma^2} \\|M(x) - y\\|^2$$\n$$\\text{Exponent of } q(x \\mid y): \\quad -\\beta U(x) - \\frac{\\beta \\kappa}{2} \\|M(x) - y\\|^2$$\nFor these two expressions to be equivalent for all configurations $x$, the term $-\\beta U(x)$ already matches. We must therefore equate the coefficients of the remaining term, $\\|M(x) - y\\|^2$:\n$$\\frac{1}{2\\sigma^2} = \\frac{\\beta \\kappa}{2}$$\nWe can now solve this algebraic equation for $\\kappa$. Multiplying both sides by $2$ cancels the factor of $1/2$:\n$$\\frac{1}{\\sigma^2} = \\beta \\kappa$$\nIsolating $\\kappa$, we find:\n$$\\kappa = \\frac{1}{\\beta \\sigma^2}$$\nFinally, we substitute the definition of $\\beta = 1/(k_B T)$ into this expression:\n$$\\kappa = \\frac{1}{\\left(\\frac{1}{k_B T}\\right) \\sigma^2}$$\nSimplifying the expression leads to the final result for the optimal stiffness constant:\n$$\\kappa = \\frac{k_B T}{\\sigma^2}$$\nThis result provides a direct physical connection between the strength of the restraint $\\kappa$, the thermal energy of the system $k_B T$, and the variance $\\sigma^2$ of the measurement noise in the coarse-grained space.",
            "answer": "$$\\boxed{\\frac{k_B T}{\\sigma^2}}$$"
        },
        {
            "introduction": "The ultimate goal of backmapping is often not just one plausible structure, but the entire thermal ensemble of configurations consistent with the coarse-grained data. This exercise guides you through the advanced task of sampling this ensemble directly using a Metropolis-Hastings algorithm under *hard* constraints. You will learn how to correctly formulate the acceptance probability by accounting for the geometry of the constrained manifold, a crucial step for any rigorous simulation involving geometric constraints or internal coordinates. ",
            "id": "3735228",
            "problem": "You are given a constrained atomistic target distribution arising in backmapping from a coarse-grained model. Let $x \\in \\mathbb{R}^{n}$ denote atomistic coordinates and $y \\in \\mathbb{R}^{m}$ denote coarse-grained variables. The target distribution on $x$ conditioned on fixed coarse variables $y$ is\n$$\np(x \\mid y) \\propto e^{-\\beta U_{\\mathrm{AA}}(x)} \\, \\delta\\!\\big(y - M(x)\\big),\n$$\nwhere $U_{\\mathrm{AA}}(x)$ is an atomistic potential energy, $\\beta$ is the inverse thermal energy $1/(k_{\\mathrm{B}} T)$ with Boltzmann constant $k_{\\mathrm{B}}$ and temperature $T$, $M:\\mathbb{R}^{n}\\to \\mathbb{R}^{m}$ is the coarse-grained mapping, and $\\delta(\\cdot)$ is the Dirac delta distribution.\n\nYou must construct a Metropolis-Hastings (MH) sampler in internal coordinates that targets $p(x\\mid y)$ and derive the acceptance probability from first principles. Use the following fundamental bases:\n- The Boltzmann distribution $p(x) \\propto e^{-\\beta U(x)}$.\n- The definition of the Dirac delta distribution and the coarea formula which relates integrals over level sets of a mapping $M$.\n- The Metropolis-Hastings (MH) acceptance rule $\\alpha = \\min\\!\\left(1, \\frac{\\pi(x') q(x \\mid x')}{\\pi(x) q(x' \\mid x)}\\right)$ for a target density $\\pi$ and proposal density $q$.\n\nDerive the acceptance probability for proposals defined in internal coordinates $q \\in \\mathbb{R}^{d}$ that are mapped to atomistic configurations via a chart $x = F(q,y)$ which enforces the constraint $M\\big(F(q,y)\\big)=y$. Explicitly express the role of the Jacobian factor of the chart and the Jacobian of the coarse mapping. Your derivation must start from the standard MH acceptance rule and rely on the coarea formula to express the constrained measure on the manifold $\\{x : M(x)=y\\}$.\n\nThen, specialize to the following concrete and scientifically consistent backmapping scenario to implement a deterministic program that computes acceptance probabilities for specified proposed moves:\n\n- System: A triatomic molecule $\\mathrm{A}$-$\\mathrm{B}$-$\\mathrm{C}$ in a two-dimensional plane. The atomistic coordinate $x \\in \\mathbb{R}^{6}$ is ordered as $(x_{\\mathrm{A}}^{(1)}, x_{\\mathrm{A}}^{(2)}, x_{\\mathrm{B}}^{(1)}, x_{\\mathrm{B}}^{(2)}, x_{\\mathrm{C}}^{(1)}, x_{\\mathrm{C}}^{(2)})$.\n- Coarse-grained mapping: $M(x)$ selects the positions of atoms $\\mathrm{A}$ and $\\mathrm{C}$, i.e., $M(x) = \\big(x_{\\mathrm{A}}^{(1)}, x_{\\mathrm{A}}^{(2)}, x_{\\mathrm{C}}^{(1)}, x_{\\mathrm{C}}^{(2)}\\big)$. The constraint $M(x)=y$ fixes $\\mathrm{A}$ and $\\mathrm{C}$ and leaves $\\mathrm{B}$ free.\n- Internal coordinates: Use polar coordinates relative to atom $\\mathrm{A}$ for atom $\\mathrm{B}$, $q=(r,\\theta)$, with $r\\ge 0$ and $\\theta \\in \\mathbb{R}$ in radians. The chart is $x_{\\mathrm{A}}=y_{\\mathrm{A}}$, $x_{\\mathrm{C}}=y_{\\mathrm{C}}$, and $x_{\\mathrm{B}}=y_{\\mathrm{A}} + r\\big(\\cos\\theta, \\sin\\theta\\big)$, which ensures $M\\big(F(q,y)\\big)=y$.\n- Potential energy: \n$$\nU_{\\mathrm{AA}}(x) = \\frac{k_{\\mathrm{bond}}}{2}\\left(\\|x_{\\mathrm{B}}-x_{\\mathrm{A}}\\| - r_{0}\\right)^{2} + \\frac{k_{\\mathrm{bond}}}{2}\\left(\\|x_{\\mathrm{C}}-x_{\\mathrm{B}}\\| - r_{0}\\right)^{2} + \\frac{k_{\\mathrm{angle}}}{2}\\left(\\angle \\mathrm{ABC} - \\theta_{0}\\right)^{2},\n$$\nwith distances in nanometers (nm), angles in radians, and energy in kilojoules per mole (kJ/mol). Here, $\\angle \\mathrm{ABC}$ is the angle at $\\mathrm{B}$ formed by vectors $\\overrightarrow{\\mathrm{BA}}$ and $\\overrightarrow{\\mathrm{BC}}$ computed from $x$; use the standard arccosine-based definition and ensure numerical robustness by clipping the cosine argument to the interval $[-1,1]$.\n- Constants and units: Set $k_{\\mathrm{B}} = 8.314462618\\times 10^{-3}$ kJ/(mol·K), $T=300$ K (so $\\beta = 1/(k_{\\mathrm{B}}T)$), $k_{\\mathrm{bond}} = 1000$ kJ/(mol·nm$^{2}$), $k_{\\mathrm{angle}} = 100$ kJ/(mol·rad$^{2}$), $r_{0}=0.6$ nm, and $\\theta_{0} = \\pi$ radians.\n- Coarse variables: Fix $y_{\\mathrm{A}}=(0.0\\,\\mathrm{nm}, 0.0\\,\\mathrm{nm})$ and $y_{\\mathrm{C}}=(1.0\\,\\mathrm{nm}, 0.0\\,\\mathrm{nm})$.\n\nAssume the proposal kernel in internal coordinates $q=(r,\\theta)$ is symmetric, so $q(q' \\mid q)=q(q \\mid q')$, which allows simplification of the MH acceptance to a ratio of target densities in $q$-space.\n\nTask 1 (Derivation): Starting from the MH acceptance rule and using the coarea formula, derive the acceptance probability $\\alpha$ for a move from $q$ to $q'$ under the above chart. Express $\\alpha$ explicitly in terms of $\\Delta U = U_{\\mathrm{AA}}(F(q',y)) - U_{\\mathrm{AA}}(F(q,y))$, the inverse thermal energy $\\beta$, and any Jacobian factors introduced by the chart and the coarse constraint.\n\nTask 2 (Implementation): Implement a program that computes the acceptance probability for each proposed move in the following test suite. All angles are in radians. Distances are in nanometers. Energies are in kilojoules per mole. The acceptance probability is unitless and must be output as floating-point numbers.\n\nTest suite (each test case is a pair $(q,q')$):\n1. $(q, q') = \\big((0.6, 0.3), (0.62, 0.35)\\big)$\n2. $(q, q') = \\big((1.0\\times 10^{-6}, 0.0), (2.0\\times 10^{-6}, 0.1)\\big)$\n3. $(q, q') = \\big((0.6, 3.0), (0.6, -3.0)\\big)$\n4. $(q, q') = \\big((1.2, 0.0), (0.2, \\pi)\\big)$\n\nFinal output format: Your program should produce a single line of output containing the acceptance probabilities for the four test cases as a comma-separated list enclosed in square brackets, in the order given above (e.g., \"[a,b,c,d]\"). No additional output is permitted.",
            "solution": "The problem is valid as it is scientifically grounded, self-contained, and well-posed. It presents a standard, albeit non-trivial, problem in computational statistical physics concerning sampling on a constrained manifold, which is central to backmapping methods in multiscale modeling.\n\nThe solution is divided into two parts as requested: first, a derivation of the Metropolis-Hastings acceptance probability from first principles, and second, the implementation of a program to compute this probability for specific cases.\n\n### Part 1: Derivation of the Acceptance Probability\n\nThe objective is to derive the Metropolis-Hastings (MH) acceptance probability, $\\alpha$, for a move from a state defined by internal coordinates $q$ to a new state $q'$, targeting the constrained atomistic distribution $p(x \\mid y)$.\n\n**1. The Constrained Target Distribution**\n\nThe target probability distribution for the atomistic coordinates $x \\in \\mathbb{R}^{n}$, given the fixed coarse-grained variables $y \\in \\mathbb{R}^{m}$, is\n$$\np(x \\mid y) \\propto e^{-\\beta U_{\\mathrm{AA}}(x)} \\, \\delta\\!\\big(y - M(x)\\big)\n$$\nwhere $U_{\\mathrm{AA}}(x)$ is the atomistic potential energy, $\\beta = 1/(k_{\\mathrm{B}} T)$ is the inverse temperature, $M: \\mathbb{R}^n \\to \\mathbb{R}^m$ is the coarse-graining map, and $\\delta(\\cdot)$ is the Dirac delta distribution. This distribution defines a probability measure concentrated on the manifold $\\mathcal{S}_y = \\{x \\in \\mathbb{R}^n : M(x) = y \\}$.\n\nTo work with this distribution, we express it as a density on the manifold $\\mathcal{S}_y$. The coarea formula provides the necessary tool to relate an integral in $\\mathbb{R}^n$ to an integral over the level sets of $M$. For an arbitrary integrable function $g(x)$, the formula is:\n$$\n\\int_{\\mathbb{R}^n} g(x) \\, \\delta(y - M(x)) \\, dx = \\int_{\\mathcal{S}_y} \\frac{g(x)}{\\sqrt{\\det(J_M(x) J_M(x)^T)}} \\, d\\sigma_y(x)\n$$\nHere, $J_M(x)$ is the $m \\times n$ Jacobian matrix of the mapping $M$, with entries $(J_M)_{ij} = \\partial M_i / \\partial x_j$. The term $\\sqrt{\\det(J_M(x) J_M(x)^T)}$ is the geometric correction factor accounting for the local density of level sets. $d\\sigma_y(x)$ is the surface area element on the manifold $\\mathcal{S}_y$.\n\nApplying this to our probability distribution, the correctly normalized target density on the manifold $\\mathcal{S}_y$, with respect to the surface measure $d\\sigma_y(x)$, is:\n$$\n\\pi_{\\mathcal{S}_y}(x) \\propto \\frac{e^{-\\beta U_{\\mathrm{AA}}(x)}}{\\sqrt{\\det(J_M(x) J_M(x)^T)}}\n$$\n\n**2. Change of Variables to Internal Coordinates**\n\nThe problem specifies that sampling is performed in a space of internal coordinates $q \\in \\mathbb{R}^d$, where $d=n-m$. The chart $x = F(q, y)$ provides a parameterization of the manifold $\\mathcal{S}_y$. To find the target density in $q$-space, we must perform a change of variables on the measure. The surface element $d\\sigma_y(x)$ transforms to the volume element $dq$ according to:\n$$\nd\\sigma_y(x) = \\sqrt{\\det(J_F(q, y)^T J_F(q, y))} \\, dq\n$$\nwhere $J_F(q, y)$ is the $n \\times d$ Jacobian matrix of the chart $F$ with respect to the internal coordinates $q$, i.e., $(J_F)_{ij} = \\partial F_i / \\partial q_j$.\n\nThe probability of finding the system in an infinitesimal volume $dq$ around $q$ is proportional to $\\tilde{\\pi}(q) dq$, where $\\tilde{\\pi}(q)$ is the target density in $q$-space. This probability must equal the probability on the corresponding patch of the manifold:\n$$\n\\tilde{\\pi}(q) dq \\propto \\pi_{\\mathcal{S}_y}(F(q, y)) \\, d\\sigma_y(F(q, y))\n$$\nSubstituting the expressions for $\\pi_{\\mathcal{S}_y}$ and $d\\sigma_y$, we get:\n$$\n\\tilde{\\pi}(q)dq \\propto \\frac{e^{-\\beta U_{\\mathrm{AA}}(F(q, y))}}{\\sqrt{\\det(J_M(F(q, y)) J_M(F(q, y))^T)}} \\sqrt{\\det(J_F(q, y)^T J_F(q, y))} \\, dq\n$$\nThus, the target density in the internal coordinate space is:\n$$\n\\tilde{\\pi}(q) \\propto e^{-\\beta U_{\\mathrm{AA}}(F(q, y))} \\frac{\\sqrt{\\det(J_F(q, y)^T J_F(q, y))}}{\\sqrt{\\det(J_M(F(q, y)) J_M(F(q, y))^T)}}\n$$\n\n**3. The Metropolis-Hastings Acceptance Probability**\n\nThe MH acceptance probability for a move from $q$ to $q'$ generated by a proposal distribution $p(q' \\mid q)$ is given by:\n$$\n\\alpha(q \\to q') = \\min\\left(1, \\frac{\\tilde{\\pi}(q') p(q \\mid q')}{\\tilde{\\pi}(q) p(q' \\mid q)}\\right)\n$$\nThe problem specifies a symmetric proposal, $p(q' \\mid q) = p(q \\mid q')$, which simplifies the rule to a ratio of the target densities:\n$$\n\\alpha(q \\to q') = \\min\\left(1, \\frac{\\tilde{\\pi}(q')}{\\tilde{\\pi}(q)}\\right)\n$$\nSubstituting the expression for $\\tilde{\\pi}(q)$, we obtain the general formula for the acceptance probability:\n$$\n\\alpha(q \\to q') = \\min\\left(1, e^{-\\beta \\Delta U} \\frac{\\sqrt{\\det(J_F(q')^T J_F(q'))}}{\\sqrt{\\det(J_F(q)^T J_F(q))}} \\frac{\\sqrt{\\det(J_M(x) J_M(x)^T)}}{\\sqrt{\\det(J_M(x') J_M(x')^T)}}\\right)\n$$\nwhere $\\Delta U = U_{\\mathrm{AA}}(x') - U_{\\mathrm{AA}}(x)$, with $x = F(q, y)$ and $x' = F(q', y)$. This expression explicitly shows the roles of the Boltzmann factor, the Jacobian of the internal coordinate chart ($J_F$), and the Jacobian of the coarse-graining map ($J_M$).\n\n**4. Specialization to the Triatomic System**\n\nWe now apply this general formula to the specific system provided.\n- **Coordinates:** Atomistic $x \\in \\mathbb{R}^6$, Coarse-grained $y \\in \\mathbb{R}^4$, Internal $q \\in \\mathbb{R}^2$.\n- **Chart:** $x = F(q,y)$ with $q=(r,\\theta)$ maps to $x_A=y_A, x_C=y_C, x_B=y_A+r(\\cos\\theta, \\sin\\theta)$.\n- **Mapping:** $M(x) = (x_A, x_C)$.\n\n**Jacobian of the chart $F$:** The chart maps $q=(r,\\theta)$ to the atomistic coordinates. The Jacobian $J_F$ is the $6 \\times 2$ matrix of partial derivatives of $x$ with respect to $r$ and $\\theta$. The only non-zero components are for atom B's coordinates ($x_B^{(1)}, x_B^{(2)}$).\nLet $x_B = (y_A^{(1)} + r\\cos\\theta, y_A^{(2)} + r\\sin\\theta)$. Then\n$\\frac{\\partial x_B}{\\partial r} = (\\cos\\theta, \\sin\\theta)$ and $\\frac{\\partial x_B}{\\partial \\theta} = (-r\\sin\\theta, r\\cos\\theta)$.\nThe matrix $J_F^T J_F$ is then:\n$$\nJ_F(q)^T J_F(q) =\n\\begin{pmatrix}\n\\cos^2\\theta + \\sin^2\\theta & -r\\cos\\theta\\sin\\theta + r\\sin\\theta\\cos\\theta \\\\\n-r\\cos\\theta\\sin\\theta + r\\sin\\theta\\cos\\theta & r^2\\sin^2\\theta + r^2\\cos^2\\theta\n\\end{pmatrix} =\n\\begin{pmatrix}\n1 & 0 \\\\\n0 & r^2\n\\end{pmatrix}\n$$\nThe determinant is $\\det(J_F(q)^T J_F(q)) = r^2$. Therefore, the Jacobian factor from the chart is $\\sqrt{r^2} = |r| = r$, since radius $r \\ge 0$.\n\n**Jacobian of the coarse-graining map $M$:** The map $M(x) = (x_A^{(1)}, x_A^{(2)}, x_C^{(1)}, x_C^{(2)})$ is a linear projection. Its $4 \\times 6$ Jacobian matrix $J_M$ is constant:\n$$\nJ_M = \\frac{\\partial(x_A, x_C)}{\\partial(x_A, x_B, x_C)} = \\begin{pmatrix} I_2 & 0_2 & 0_2 \\\\ 0_2 & 0_2 & I_2 \\end{pmatrix}\n$$\n(in block matrix form, where $I_2$ is the $2 \\times 2$ identity and $0_2$ is the $2 \\times 2$ zero matrix).\nThe product $J_M J_M^T$ is the $4 \\times 4$ identity matrix: $J_M J_M^T = I_4$.\nIts determinant is $\\det(J_M J_M^T) = 1$. This term is constant and equals $1$ for all configurations $x$.\n\n**Final Acceptance Probability:**\nSubstituting these Jacobian determinants into the general formula for $\\alpha$, the factor from $J_M$ becomes $1$, and the factor from $J_F$ becomes $\\sqrt{(r')^2 / r^2} = r'/r$. The acceptance probability simplifies to:\n$$\n\\alpha(q \\to q') = \\min\\left(1, \\frac{r'}{r} e^{-\\beta (U_{\\mathrm{AA}}(x') - U_{\\mathrm{AA}}(x))}\\right)\n$$\nwhere $q=(r,\\theta)$ and $q'=(r',\\theta')$. This is the final expression used for the implementation. The potential energy $U_{\\mathrm{AA}}$ is calculated using the cartesian coordinates $x$ derived from the internal coordinates $q=(r,\\theta)$ via the chart $F$.\nSpecifically, $x_A = (0,0)$, $x_C = (1,0)$, and $x_B = (r\\cos\\theta, r\\sin\\theta)$.\nThe energy terms are:\n- $\\|x_B - x_A\\| = r$\n- $\\|x_C - x_B\\| = \\sqrt{(1-r\\cos\\theta)^2 + (-r\\sin\\theta)^2} = \\sqrt{1 - 2r\\cos\\theta + r^2}$\n- $\\angle \\mathrm{ABC} = \\arccos\\left(\\frac{(x_A-x_B)\\cdot(x_C-x_B)}{\\|x_A-x_B\\|\\|x_C-x_B\\|}\\right) = \\arccos\\left(\\frac{r-\\cos\\theta}{\\sqrt{1-2r\\cos\\theta+r^2}}\\right)$\n\nThese expressions are used in the following program to compute the numerical values.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the backmapping acceptance probability problem.\n    This involves deriving and implementing the acceptance probability for a \n    Metropolis-Hastings sampler operating in internal coordinates.\n    \"\"\"\n\n    # --- Constants and System Parameters ---\n    K_B = 8.314462618e-3  # Boltzmann constant in kJ/(mol·K)\n    T = 300.0             # Temperature in K\n    BETA = 1.0 / (K_B * T)  # Inverse thermal energy in mol/kJ\n\n    K_BOND = 1000.0  # Bond force constant in kJ/(mol·nm^2)\n    K_ANGLE = 100.0  # Angle force constant in kJ/(mol·rad^2)\n    R_0 = 0.6        # Equilibrium bond length in nm\n    THETA_0 = np.pi  # Equilibrium angle in radians\n\n    # Fixed coarse-grained variables (positions of atoms A and C)\n    Y_A = np.array([0.0, 0.0])  # Position of atom A in nm\n    Y_C = np.array([1.0, 0.0])  # Position of atom C in nm\n\n    def calculate_potential_energy(q):\n        \"\"\"\n        Calculates the atomistic potential energy U_AA(x) for a given \n        internal coordinate configuration q = (r, theta).\n        \n        Args:\n            q (tuple): A tuple (r, theta) representing internal coordinates.\n                       r is the distance in nm, theta is the angle in radians.\n        \n        Returns:\n            float: The total potential energy in kJ/mol.\n        \"\"\"\n        r, theta = q\n\n        # Chart: Map internal coordinates q to Cartesian coordinates x\n        # x_A and x_C are fixed by y_A and y_C\n        x_A = Y_A\n        x_C = Y_C\n        x_B = Y_A + np.array([r * np.cos(theta), r * np.sin(theta)])\n\n        # --- Calculate Energy Terms ---\n        \n        # 1. Bond Energy (A-B)\n        # The distance ||x_B - x_A|| is simply r.\n        energy_bond_AB = 0.5 * K_BOND * (r - R_0)**2\n\n        # 2. Bond Energy (B-C)\n        vec_BC = x_C - x_B\n        dist_BC = np.linalg.norm(vec_BC)\n        energy_bond_BC = 0.5 * K_BOND * (dist_BC - R_0)**2\n\n        # 3. Angle Energy (A-B-C)\n        # The angle is formed by vectors BA and BC.\n        # Check for the edge case r=0, where the angle is ill-defined.\n        # In this scenario, the energy contribution would be infinite.\n        if r  1e-9: # A practical threshold for r being zero\n            return np.inf\n\n        vec_BA = x_A - x_B\n        \n        # Cosine of the angle using the dot product formula\n        dot_product = np.dot(vec_BA, vec_BC)\n        norm_BA = r # ||x_A - x_B|| is r\n        \n        # Argument for arccos must be clipped to [-1, 1] for numerical stability\n        cos_angle_arg = dot_product / (norm_BA * dist_BC)\n        cos_angle_arg_clipped = np.clip(cos_angle_arg, -1.0, 1.0)\n        \n        angle_ABC = np.arccos(cos_angle_arg_clipped)\n        \n        energy_angle = 0.5 * K_ANGLE * (angle_ABC - THETA_0)**2\n        \n        total_energy = energy_bond_AB + energy_bond_BC + energy_angle\n        return total_energy\n\n    def calculate_acceptance_probability(q_current, q_proposed):\n        \"\"\"\n        Calculates the Metropolis-Hastings acceptance probability for a move\n        from q_current to q_proposed.\n        \n        The formula is alpha = min(1, (r'/r) * exp(-beta * delta_U)).\n        \n        Args:\n            q_current (tuple): The current internal coordinates (r, theta).\n            q_proposed (tuple): The proposed internal coordinates (r', theta').\n            \n        Returns:\n            float: The acceptance probability (unitless).\n        \"\"\"\n        r_current, _ = q_current\n        r_proposed, _ = q_proposed\n\n        # Handle edge case where current r is zero (though not in test cases)\n        if r_current  1e-9:\n             # Move from r=0 has infinite Jacobian ratio, accept if delta_U is not +inf\n             U_proposed = calculate_potential_energy(q_proposed)\n             return 1.0 if U_proposed != np.inf else 0.0\n\n        U_current = calculate_potential_energy(q_current)\n        U_proposed = calculate_potential_energy(q_proposed)\n\n        delta_U = U_proposed - U_current\n        \n        # Jacobian factor from the change of coordinates to polar\n        jacobian_ratio = r_proposed / r_current\n        \n        # Acceptance ratio\n        ratio = jacobian_ratio * np.exp(-BETA * delta_U)\n        \n        return min(1.0, ratio)\n\n    # Test suite provided in the problem statement\n    test_cases = [\n        ((0.6, 0.3), (0.62, 0.35)),\n        ((1.0e-6, 0.0), (2.0e-6, 0.1)),\n        ((0.6, 3.0), (0.6, -3.0)),\n        ((1.2, 0.0), (0.2, np.pi)),\n    ]\n\n    results = []\n    for q_curr, q_prop in test_cases:\n        alpha = calculate_acceptance_probability(q_curr, q_prop)\n        results.append(alpha)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.10f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}