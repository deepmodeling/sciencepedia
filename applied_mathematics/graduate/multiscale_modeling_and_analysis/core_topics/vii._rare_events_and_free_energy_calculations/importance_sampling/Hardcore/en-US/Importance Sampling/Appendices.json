{
    "hands_on_practices": [
        {
            "introduction": "A crucial skill in applying importance sampling is understanding how the choice of proposal distribution, $q(x)$, affects the estimator's variance. This exercise provides a foundational test case: estimating the fourth moment of a standard normal distribution using another Gaussian as a proposal. By deriving the variance as a function of the proposal's variance, $\\sigma_q^2$, you will directly quantify how a mismatch between the target and proposal distributions can lead to high or even infinite variance, illustrating a core principle of effective importance sampling .",
            "id": "767801",
            "problem": "Importance Sampling is a variance reduction technique used in Monte Carlo methods to estimate properties of a particular distribution, while sampling from a different distribution. Consider the problem of estimating the expectation $I = E_p[h(x)]$, where $p(x)$ is the target probability density function (PDF) and $h(x)$ is a function of interest. Instead of drawing samples from $p(x)$, we draw $N$ samples $\\{x_i\\}_{i=1}^N$ from a proposal distribution $q(x)$. The expectation is then estimated using the importance sampling estimator:\n$$\n\\hat{I}_N = \\frac{1}{N} \\sum_{i=1}^N w(x_i) h(x_i)\n$$\nwhere $w(x) = \\frac{p(x)}{q(x)}$ are the importance weights. The estimator is unbiased, i.e., $E_q[\\hat{I}_N] = I$. The variance of the estimator for a single sample ($N=1$) is given by $\\text{Var}_q(w(X)h(X))$, where $X \\sim q(x)$.\n\n**Problem:**\nLet the target distribution $p(x)$ be the standard normal distribution, $\\mathcal{N}(x|0, 1)$, whose PDF is $p(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-x^2/2}$. We wish to estimate the fourth moment of this distribution, corresponding to the function $h(x) = x^4$.\n\nThe proposal distribution $q(x)$ is chosen to be a zero-mean normal distribution with a different variance $\\sigma_q^2$, i.e., $q(x) = \\mathcal{N}(x|0, \\sigma_q^2)$, with PDF $q(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma_q} e^{-x^2/(2\\sigma_q^2)}$.\n\nDerive a closed-form expression for the variance of the single-sample importance sampling estimator, $\\text{Var}_q(w(X)h(X))$, as a function of $\\sigma_q$. You may assume that the variance is finite, which holds for $\\sigma_q^2 > 1/2$.",
            "solution": "1. The variance is  \n$$\n\\mathrm{Var}_q\\bigl(w(X)h(X)\\bigr)\n=E_q\\bigl[(w(X)h(X))^2\\bigr]-\\bigl(E_q[w(X)h(X)]\\bigr)^2.\n$$\nSince $h(x)=x^4$, $w(x)=\\frac{p(x)}{q(x)}$, and $I=E_p[x^4]=3$, we need \n$$\nE_q\\bigl[(w(X)x^4)^2\\bigr]\n=\\int q(x)\\Bigl(\\frac{p(x)}{q(x)}\\Bigr)^2 x^8\\,dx\n=\\int \\frac{p(x)^2}{q(x)}\\,x^8\\,dx.\n$$\n\n2. Substitute the Gaussian densities:\n$$\np(x)=\\frac{1}{\\sqrt{2\\pi}}e^{-x^2/2},\\quad\nq(x)=\\frac{1}{\\sqrt{2\\pi}\\,\\sigma_q}e^{-x^2/(2\\sigma_q^2)}.\n$$\nThen\n$$\n\\frac{p(x)^2}{q(x)}\n=\\frac{\\frac1{2\\pi}e^{-x^2}}{\\frac1{\\sqrt{2\\pi}\\,\\sigma_q}e^{-x^2/(2\\sigma_q^2)}}\n=\\frac{\\sigma_q}{\\sqrt{2\\pi}}\\,e^{-x^2\\bigl(1-\\tfrac1{2\\sigma_q^2}\\bigr)}.\n$$\n\n3. Let \n$$\na=1-\\frac1{2\\sigma_q^2}=\\frac{2\\sigma_q^2-1}{2\\sigma_q^2},\\quad\n\\int_{-\\infty}^{\\infty}x^8e^{-a x^2}dx\n=\\frac{7!!}{2^4}\\,a^{-4-\\frac12}\\sqrt\\pi\n=\\frac{105\\sqrt\\pi}{16\\,a^{9/2}}.\n$$\nThus\n$$\nE_q\\bigl[(w(X)x^4)^2\\bigr]\n=\\frac{\\sigma_q}{\\sqrt{2\\pi}}\\cdot\\frac{105\\sqrt\\pi}{16\\,a^{9/2}}\n=\\frac{105\\,\\sigma_q}{16\\sqrt2\\,a^{9/2}}\n=\\frac{105\\,\\sigma_q^{10}}{(2\\sigma_q^2-1)^{9/2}}.\n$$\n\n4. Hence\n$$\n\\mathrm{Var}_q(w(X)x^4)\n=\\frac{105\\,\\sigma_q^{10}}{(2\\sigma_q^2-1)^{9/2}}-3^2\n=\\frac{105\\,\\sigma_q^{10}}{(2\\sigma_q^2-1)^{9/2}}-9.\n$$",
            "answer": "$$\\boxed{\\frac{105\\,\\sigma_q^{10}}{(2\\sigma_q^2-1)^{9/2}}-9}$$"
        },
        {
            "introduction": "Moving from analysis to design, the next step is to actively minimize the variance of our estimator. This practice challenges you to find the optimal parameter for a proposal distribution from within a given family, a common task in practical applications. You will use calculus to determine the value of $\\theta$ for an exponential proposal distribution that minimizes the variance when estimating a related integral, providing a concrete example of variance reduction through optimization .",
            "id": "767686",
            "problem": "Importance sampling is a variance reduction technique used in Monte Carlo methods to estimate properties of a distribution. A primary application is the estimation of an integral $I = \\int h(x) dx$. The integral is rewritten as an expectation with respect to a proposal probability distribution $q(x)$, from which it is easy to draw samples:\n$$I = \\int \\frac{h(x)}{q(x)} q(x) dx = E_q\\left[\\frac{h(x)}{q(x)}\\right]$$\nThe quantity $w(x) = h(x)/q(x)$ can be thought of as a corrected value, and its expectation under $q(x)$ is the desired integral $I$. The variance of a single-sample Monte Carlo estimator $\\hat{I}(x) = w(x)$ where $x \\sim q(x)$ is given by:\n$$ \\text{Var}_q(\\hat{I}) = E_q[\\hat{I}^2] - (E_q[\\hat{I}])^2 $$\nSince $E_q[\\hat{I}] = I$ is a constant, minimizing the variance is equivalent to minimizing the second moment, $V = E_q[\\hat{I}^2]$.\n\nConsider the problem of estimating the integral\n$$I = \\int_0^\\infty x^a e^{-bx} dx$$\nwhere $a$ and $b$ are real-valued parameters satisfying $a > -1/2$ and $b > 0$.\n\nFor this estimation, we employ an exponential proposal distribution parameterized by $\\theta$:\n$$q(x; \\theta) = \\theta e^{-\\theta x}, \\quad x \\ge 0, \\theta > 0$$\nThe single-sample estimator is $\\hat{I}(x) = \\frac{x^a e^{-bx}}{q(x; \\theta)}$.\n\nDerive the value of the parameter $\\theta$ that minimizes the variance of this estimator.",
            "solution": "1. We have the single‐sample weight  \n$$w(x)=\\frac{x^a e^{-b x}}{q(x;\\theta)}=\\frac{x^a e^{-b x}}{\\theta e^{-\\theta x}}=\\frac{x^a}{\\theta}e^{-(b-\\theta)x}\\,. $$  \nThus the second moment under $q$ is  \n$$V(\\theta)=E_q[w(x)^2]\n=\\int_0^\\infty \\frac{x^{2a}}{\\theta^2}e^{-2(b-\\theta)x}\\,\\theta e^{-\\theta x}dx\n=\\frac{1}{\\theta}\\int_0^\\infty x^{2a}e^{-(2b-\\theta)x}dx\\,. $$  \n2. Use the gamma‐integral  \n$$\\int_0^\\infty x^{2a}e^{-\\gamma x}dx=\\frac{\\Gamma(2a+1)}{\\gamma^{2a+1}}\\,,\\quad \\Re(\\gamma)>0,$$  \nwith $\\gamma=2b-\\theta$. Hence  \n$$V(\\theta)\n=\\frac{\\Gamma(2a+1)}{\\theta\\,(2b-\\theta)^{2a+1}}\\,. $$  \n3. Differentiate $V(\\theta)$ w.r.t.\\ $\\theta$ and set to zero:  \n$$\\frac{dV}{d\\theta}\n=\\Gamma(2a+1)\\frac{d}{d\\theta}\\Bigl[\\theta^{-1}(2b-\\theta)^{-(2a+1)}\\Bigr]\n=0\\,.$$  \nCompute the derivative:  \n$$\\frac{dV}{d\\theta}\n=\\Gamma(2a+1)\\theta^{-2}(2b-\\theta)^{-(2a+2)}\\Bigl[-(2b-\\theta)+\\theta(2a+1)\\Bigr]=0\\,.$$  \nThus the bracket must vanish:  \n$$-(2b-\\theta)+\\theta(2a+1)=0\n\\;\\Longrightarrow\\;\\theta(2a+2)=2b\n\\;\\Longrightarrow\\;\\theta^*=\\frac{2b}{2(a+1)}=\\frac{b}{a+1}\\,. $$",
            "answer": "$$\\boxed{\\frac{b}{a+1}}$$"
        },
        {
            "introduction": "While optimizing a single proposal is powerful, some problems are better handled by combining multiple samplers, a technique known as Multiple Importance Sampling (MIS). This advanced practice demonstrates how to construct an estimator using two distinct but overlapping proposal distributions, weighted by the popular balance heuristic. By calculating the variance of this combined estimator, you will gain hands-on experience with a robust method that is central to state-of-the-art rendering and computational physics .",
            "id": "767887",
            "problem": "Multiple Importance Sampling (MIS) is a technique used in Monte Carlo integration to reduce the variance of an estimator by using several proposal distributions. The goal is to estimate an integral of the form $I = \\int_{\\Omega} f(x) p(x) dx$, where $p(x)$ is a probability density function.\n\nThe general form of an unbiased MIS estimator using $k$ proposal distributions $q_i(x)$ is given by:\n$$\n\\hat{I} = \\sum_{i=1}^k \\frac{1}{n_i} \\sum_{j=1}^{n_i} w_i(X_{ij}) \\frac{f(X_{ij}) p(X_{ij})}{q_i(X_{ij})}\n$$\nwhere $n_i$ is the number of samples drawn from the proposal distribution $q_i(x)$, $X_{ij} \\sim q_i(x)$, and $w_i(x)$ are weighting functions that sum to one, i.e., $\\sum_{i=1}^k w_i(x) = 1$.\n\nA widely used weighting scheme is the *balance heuristic*, where the weights are defined as:\n$$\nw_i(x) = \\frac{n_i q_i(x)}{\\sum_{l=1}^k n_l q_l(x)}\n$$\n\nConsider the task of estimating the mean of a standard uniform distribution on the interval $[0,1]$. This corresponds to setting $p(x) = 1$ for $x \\in [0,1]$ and $f(x)=x$. We will use two proposal distributions, $q_1(x)$ and $q_2(x)$, and draw an equal number of samples, $n$, from each ($n_1 = n_2 = n$). The proposal distributions are defined as:\n$$\nq_1(x) = \\frac{1}{\\alpha} \\mathbb{I}_{[0, \\alpha]}(x)\n$$\n$$\nq_2(x) = \\frac{1}{\\alpha} \\mathbb{I}_{[1-\\alpha, 1]}(x)\n$$\nwhere $\\mathbb{I}_S(x)$ is the indicator function (1 if $x \\in S$ and 0 otherwise), and $\\alpha$ is a given parameter satisfying $\\frac{1}{2}  \\alpha  1$.\n\nLet $\\hat{\\mu}$ be the MIS estimator for the mean using this setup. Your task is to derive a closed-form expression for the quantity $n \\cdot \\mathrm{Var}(\\hat{\\mu})$ as a function of the parameter $\\alpha$.",
            "solution": "1. Definitions and estimator decomposition  \nWe have $p(x)=1$, $f(x)=x$, two proposals $q_1,q_2$ with $n_1=n_2=n$. The MIS estimator is\n$$\n\\hat\\mu=\\frac1n\\sum_{j=1}^nY_{1j}+\\frac1n\\sum_{j=1}^nY_{2j},\\quad\nY_{ij}=w_i(X_{ij})\\,x_{ij}\\,\\frac{1}{q_i(X_{ij})},\n$$\nwhere $X_{1j}\\sim q_1$, $X_{2j}\\sim q_2$ and with balance weights $w_i(x)=q_i(x)/(q_1(x)+q_2(x))$.  \n\n2. Weight functions on regions  \nSince $\\tfrac12\\alpha1$, the supports overlap on $[1-\\alpha,\\alpha]$. On $[0,1-\\alpha)$ only $q_10$, so $w_1=1,w_2=0$. On $[\\alpha,1]$ only $q_20$, so $w_1=0,w_2=1$. On $[1-\\alpha,\\alpha]$ both positive so $w_1=w_2=\\tfrac12$.  \n\n3. Expression for $Y_i$  \nBecause $q_i(x)=1/\\alpha$ on its support, \n$$\nY_i(x)=w_i(x)\\,x\\,\\alpha.\n$$\n\n4. Variance in terms of single‐sample moments  \nIndependence implies\n$$\n\\mathrm{Var}(\\hat\\mu)=\\frac{1}{n}\\bigl[\\mathrm{Var}(Y_1)+\\mathrm{Var}(Y_2)\\bigr],\n\\quad\nn\\,\\mathrm{Var}(\\hat\\mu)\n=\\mathrm{Var}(Y_1)+\\mathrm{Var}(Y_2).\n$$\n\n5. Compute $E[Y_1]$ and $E[Y_1^2]$  \nOn $[0,1-\\alpha)$: $Y_1=\\alpha x$; on $[1-\\alpha,\\alpha]$: $Y_1=\\tfrac{\\alpha}{2}x$, and $q_1(x)=1/\\alpha$. Hence\n$$\nE[Y_1]=\\int_0^{1-\\alpha}x\\,dx+\\tfrac12\\int_{1-\\alpha}^{\\alpha}x\\,dx\n=\\frac{\\alpha^2+(1-\\alpha)^2}{4},\n$$\n$$\nE[Y_1^2]\n=\\alpha\\int_0^{1-\\alpha}x^2\\,dx+\\frac{\\alpha}{4}\\int_{1-\\alpha}^{\\alpha}x^2\\,dx\n=\\frac{-2\\alpha^4+9\\alpha^3-9\\alpha^2+3\\alpha}{12}.\n$$\n\n6. Compute $E[Y_2]$ and $E[Y_2^2]$  \nOn $[1-\\alpha,\\alpha]$: $Y_2=\\tfrac{\\alpha}{2}x$; on $[\\alpha,1]$: $Y_2=\\alpha x$, $q_2(x)=1/\\alpha$. Thus\n$$\nE[Y_2]=\\tfrac12\\int_{1-\\alpha}^{\\alpha}x\\,dx+\\int_{\\alpha}^1x\\,dx\n=\\frac{2\\alpha-2\\alpha^2+1}{4},\n$$\n$$\nE[Y_2^2]\n=\\frac{\\alpha}{4}\\int_{1-\\alpha}^{\\alpha}x^2\\,dx+\\alpha\\int_{\\alpha}^1x^2\\,dx\n=\\frac{-2\\alpha^4-3\\alpha^3+3\\alpha^2+3\\alpha}{12}.\n$$\n\n7. Sum of variances  \nCompute $n\\,\\mathrm{Var}(\\hat\\mu)=E[Y_1^2]+E[Y_2^2]-(E[Y_1]^2+E[Y_2]^2)$. After algebra one finds\n$$\nn\\,\\mathrm{Var}(\\hat\\mu)\n=\\frac{-20\\alpha^4+36\\alpha^3-24\\alpha^2+12\\alpha-3}{24}.\n$$",
            "answer": "$$\\boxed{\\frac{-20\\alpha^4+36\\alpha^3-24\\alpha^2+12\\alpha-3}{24}}$$"
        }
    ]
}