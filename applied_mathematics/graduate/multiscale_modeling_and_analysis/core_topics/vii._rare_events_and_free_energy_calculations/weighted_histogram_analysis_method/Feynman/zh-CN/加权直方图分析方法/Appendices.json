{
    "hands_on_practices": [
        {
            "introduction": "加权直方图分析方法（WHAM）的核心在于其自洽迭代过程。本练习将引导你亲手实现这一核心算法。通过将该算法应用于一个简化的“玩具”系统，你将具体地理解无偏概率分布和窗口自由能是如何被迭代求解的，并观察数据质量对收敛速度的影响。",
            "id": "3832602",
            "problem": "考虑使用加权直方图分析方法（WHAM）结合来自多个窗口的偏置直方图，以估计离散反应坐标上潜在的无偏置概率质量函数。加权直方图分析方法（WHAM）是一种用于无偏置分布的最大似然估计器，它校正在独立模拟窗口中施加的偏置势，并求解一个耦合了无偏置分布和窗口归一化自由能的自洽不动点问题。在此问题中，共有两个窗口，每个窗口有三个离散的区间。能量以玻尔兹曼常数与温度的乘积 $k_{\\mathrm{B}} T$ 为单位报告，因此逆温度是无量纲的。统计效率被忽略（设为 $1$），区间宽度被视为常数并被吸收到归一化中。\n\n从 $j \\in \\{1,2,3\\}$ 上的均匀初始分布 $p^{(0)}(x_j)$ 开始，实现 WHAM 方程所隐含的不动点迭代，以更新无偏置分布 $p^{(t)}(x_j)$ 和窗口自由能 $f_k^{(t)}$（其中 $k \\in \\{1,2\\}$），并在每一步对 $p^{(t)}(x_j)$ 进行归一化，以确保 $\\sum_{j=1}^{3} p^{(t)}(x_j) = 1$。当最大绝对变化量 $\\max_j \\left| p^{(t)}(x_j) - p^{(t-1)}(x_j) \\right|  10^{-6}$ 时停止。对于下方的每个测试用例，量化达到容差所需的迭代次数。您的程序应以所有 $j$ 的 $p^{(0)}(x_j) = 1/3$ 开始，使用提供的直方图计数和偏置势，并将逆温度值视为给定值。所有计算都应以纯数学形式进行，不报告任何物理单位。\n\n测试套件：\n- 案例 A（均衡计数，对称偏置，相同温度）：\n  - 窗口 $1$ 计数 $H^{(1)} = [60,120,60]$，窗口 $2$ 计数 $H^{(2)} = [40,80,40]$。\n  - 窗口 $1$ 偏置势 $U^{(1)} = [0.0,0.5,1.0]$，窗口 $2$ 偏置势 $U^{(2)} = [1.0,0.5,0.0]$。\n  - 逆温度 $\\beta^{(1)} = 1.0$，$\\beta^{(2)} = 1.0$。\n- 案例 B（稀疏计数，非对称覆盖，相同温度）：\n  - 窗口 $1$ 计数 $H^{(1)} = [1,0,0]$，窗口 $2$ 计数 $H^{(2)} = [0,1,1]$。\n  - 窗口 $1$ 偏置势 $U^{(1)} = [0.0,1.0,2.0]$，窗口 $2$ 偏置势 $U^{(2)} = [2.0,1.0,0.0]$。\n  - 逆温度 $\\beta^{(1)} = 1.0$，$\\beta^{(2)} = 1.0$。\n- 案例 C（不同温度，轻度变化的偏置）：\n  - 窗口 $1$ 计数 $H^{(1)} = [100,50,25]$，窗口 $2$ 计数 $H^{(2)} = [10,20,40]$。\n  - 窗口 $1$ 偏置势 $U^{(1)} = [0.2,0.2,0.2]$，窗口 $2$ 偏置势 $U^{(2)} = [0.0,0.5,1.0]$。\n  - 逆温度 $\\beta^{(1)} = 1.0$，$\\beta^{(2)} = 0.5$。\n- 案例 D（具有强极化计数的极端偏置，相同温度）：\n  - 窗口 $1$ 计数 $H^{(1)} = [200,10,1]$，窗口 $2$ 计数 $H^{(2)} = [1,10,200]$。\n  - 窗口 $1$ 偏置势 $U^{(1)} = [0.0,5.0,10.0]$，窗口 $2$ 偏置势 $U^{(2)} = [10.0,5.0,0.0]$。\n  - 逆温度 $\\beta^{(1)} = 1.0$，$\\beta^{(2)} = 1.0$。\n\n算法要求：\n- 初始化 $p^{(0)}(x_j) = 1/3$ 对于 $j \\in \\{1,2,3\\}$。\n- 在每次迭代 $t$ 中，使用窗口 $k$ 的归一化条件更新窗口自由能 $f_k^{(t)}$，然后通过结合两个窗口的直方图，并用窗口偏置和 $f_k^{(t)}$ 进行适当的重加权来更新无偏置分布 $p^{(t)}(x_j)$，接着进行归一化以使 $\\sum_{j=1}^{3} p^{(t)}(x_j) = 1$。\n- 使用停止准则 $\\max_j \\left| p^{(t)}(x_j) - p^{(t-1)}(x_j) \\right|  10^{-6}$，并报告达到容差所执行的总迭代次数。\n\n最终输出格式：\n您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔的整数列表（例如，$[7,42,13,5]$），其中整数按顺序对应案例 A、B、C 和 D 的迭代次数。",
            "solution": "该问题已经过验证，被确定为一个有效的、适定的科学问题。所有必要的数据和条件都已提供，任务与统计力学和计算科学中的既定原则相符。\n\n该问题要求实现加权直方图分析方法（WHAM），以找到一个具有由 $M=3$ 个区间定义的离散反应坐标的系统的无偏置概率质量函数 $p(x_j)$。数据来源于在 $K=2$ 个窗口中的模拟，每个窗口都受到一个势 $U^{(k)}(x_j)$ 的偏置。目标是为四个不同的测试用例迭代求解 WHAM 方程，并报告收敛所需的迭代次数。\n\nWHAM 方程为无偏置概率 $p(x_j)$ 和与每个模拟窗口 $k$ 相关的无量纲自由能 $f_k$ 构成了一组自洽关系。对于一个有 $K$ 个窗口和 $M$ 个区间的系统，这些方程是：\n$$\np(x_j) = \\frac{\\sum_{k=1}^{K} H^{(k)}(x_j)}{\\sum_{k=1}^{K} N_k e^{\\beta^{(k)} f_k} e^{-\\beta^{(k)} U^{(k)}(x_j)}}\n\\quad \\quad (1)\n$$\n$$\ne^{-\\beta^{(k)} f_k} = \\sum_{j=1}^{M} p(x_j) e^{-\\beta^{(k)} U^{(k)}(x_j)}\n\\quad \\quad (2)\n$$\n这里，$H^{(k)}(x_j)$ 是在窗口 $k$ 的区间 $j$ 中观测到的计数，$N_k = \\sum_{j=1}^{M} H^{(k)}(x_j)$ 是来自窗口 $k$ 的总样本数，$U^{(k)}(x_j)$ 是施加在窗口 $k$ 的区间 $j$ 上的偏置势，而 $\\beta^{(k)}$ 是相应的逆温度（作为一个无量纲量给出）。概率分布必须满足归一化条件 $\\sum_{j=1}^{M} p(x_j) = 1$。注意，方程 $(1)$ 必须被解释为使得最终得到的 $p(x_j)$ 是归一化的。\n\n这些耦合的非线性方程可以使用不动点迭代方案求解。算法过程如下：\n\n1.  **初始化**：从概率分布的初始猜测 $p^{(0)}(x_j)$ 开始。问题指定了一个均匀分布：\n    $$\n    p^{(0)}(x_j) = \\frac{1}{M}\n    $$\n    对于 $j \\in \\{1, 2, ..., M\\}$。设置迭代计数器 $t=0$ 和容差 $\\epsilon=10^{-6}$。\n\n2.  **迭代**：对于 $t = 1, 2, 3, \\dots$，执行以下步骤：\n    a.  **更新自由能**：使用前一次迭代的概率分布 $p^{(t-1)}(x_j)$ 计算每个窗口 $k$ 的自由能 $f_k^{(t)}$。重排方程 $(2)$ 可得：\n        $$\n        f_k^{(t)} = -\\frac{1}{\\beta^{(k)}} \\ln\\left( \\sum_{j=1}^{M} p^{(t-1)}(x_j) e^{-\\beta^{(k)} U^{(k)}(x_j)} \\right)\n        $$\n        为保证数值稳定性，通常最好计算量 $C_k^{(t)} = e^{\\beta^{(k)} f_k^{(t)}}$：\n        $$\n        C_k^{(t)} = \\left( \\sum_{j=1}^{M} p^{(t-1)}(x_j) e^{-\\beta^{(k)} U^{(k)}(x_j)} \\right)^{-1}\n        $$\n\n    b.  **更新概率**：使用更新后的自由能项 $C_k^{(t)}$，根据方程 $(1)$ 计算一个新的、未归一化的概率分布 $p_{\\text{un}}^{(t)}(x_j)$：\n        $$\n        p_{\\text{un}}^{(t)}(x_j) = \\frac{\\sum_{k=1}^{K} H^{(k)}(x_j)}{\\sum_{k=1}^{K} N_k C_k^{(t)} e^{-\\beta^{(k)} U^{(k)}(x_j)}}\n        $$\n        分子是所有窗口中区间 $j$ 的总计数。分母是结合了所有窗口信息的重加权因子。\n\n    c.  **归一化**：通过对 $p_{\\text{un}}^{(t)}(x_j)$ 进行归一化来获得更新后的概率分布 $p^{(t)}(x_j)$：\n        $$\n        p^{(t)}(x_j) = \\frac{p_{\\text{un}}^{(t)}(x_j)}{\\sum_{j'=1}^{M} p_{\\text{un}}^{(t)}(x_{j'})}\n        $$\n\n    d.  **收敛性检查**：当当前和前一次概率分布之间的最大绝对差小于指定的容差 $\\epsilon$ 时，迭代终止：\n        $$\n        \\max_j \\left| p^{(t)}(x_j) - p^{(t-1)}(x_j) \\right|  \\epsilon\n        $$\n        如果满足条件，过程停止。否则，增加 $t$ 并返回到步骤 2a。\n\n最终输出是为满足所提供的四个测试用例中每个用例的收敛准则所需的迭代次数。实现将利用 `numpy` 进行高效的向量化计算。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_wham_iteration(H, U, beta, tol=1e-6):\n    \"\"\"\n    Performs WHAM fixed-point iteration to find the unbiased probability distribution.\n\n    Args:\n        H (np.ndarray): A (num_windows, num_bins) array of histogram counts.\n        U (np.ndarray): A (num_windows, num_bins) array of bias potentials.\n        beta (np.ndarray): A (num_windows,) array of inverse temperatures.\n        tol (float): The convergence tolerance.\n\n    Returns:\n        int: The number of iterations required to reach convergence.\n    \"\"\"\n    num_windows, num_bins = H.shape\n    \n    # Total number of samples in each window\n    N = np.sum(H, axis=1)\n    \n    # Total histogram counts across all windows for each bin\n    total_H = np.sum(H, axis=0)\n    \n    # Initial guess for the probability distribution (uniform)\n    p = np.full(num_bins, 1.0 / num_bins, dtype=np.float64)\n    \n    iterations = 0\n    while True:\n        iterations += 1\n        p_old = p.copy()\n        \n        # Pre-compute exponential terms for efficiency\n        # exp(-beta_k * U_kj) for all k, j\n        exp_neg_beta_U = np.exp(-beta[:, np.newaxis] * U)\n        \n        # Step 2a: Update free energies. We compute C_k = exp(beta_k * f_k).\n        # C_k = 1 / sum_j(p_j * exp(-beta_k * U_kj))\n        sum_val = np.sum(p_old[np.newaxis, :] * exp_neg_beta_U, axis=1)\n        # Avoid division by zero, although p_old and exp are always positive\n        # for this problem's setup.\n        C = 1.0 / sum_val\n        \n        # Step 2b: Update unnormalized probabilities p_un(x_j)\n        # Denominator: sum_k(N_k * C_k * exp(-beta_k * U_kj))\n        denominator = np.sum(N[:, np.newaxis] * C[:, np.newaxis] * exp_neg_beta_U, axis=0)\n        \n        p_unnormalized = np.zeros(num_bins, dtype=np.float64)\n        # Avoid division by zero if a bin has no counts at all\n        non_zero_H_mask = total_H  0\n        non_zero_denom_mask = denominator  0\n        valid_mask = non_zero_H_mask  non_zero_denom_mask\n        \n        p_unnormalized[valid_mask] = total_H[valid_mask] / denominator[valid_mask]\n\n        # Step 2c: Normalize probabilities\n        p_sum = np.sum(p_unnormalized)\n        if p_sum  0:\n            p = p_unnormalized / p_sum\n        else:\n            # This would happen if total_H is all zeros, not the case for these problems.\n            # We can stop if no meaningful update is possible.\n            break\n\n        # Step 2d: Check for convergence\n        error = np.max(np.abs(p - p_old))\n        if error  tol:\n            break\n            \n    return iterations\n\ndef solve():\n    \"\"\"\n    Solves the WHAM iteration problem for all specified test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: balanced counts, symmetric biases, equal temperature\n        {\n            \"H\": np.array([[60, 120, 60], [40, 80, 40]], dtype=np.float64),\n            \"U\": np.array([[0.0, 0.5, 1.0], [1.0, 0.5, 0.0]], dtype=np.float64),\n            \"beta\": np.array([1.0, 1.0], dtype=np.float64),\n        },\n        # Case B: scarce counts, asymmetric coverage, equal temperature\n        {\n            \"H\": np.array([[1, 0, 0], [0, 1, 1]], dtype=np.float64),\n            \"U\": np.array([[0.0, 1.0, 2.0], [2.0, 1.0, 0.0]], dtype=np.float64),\n            \"beta\": np.array([1.0, 1.0], dtype=np.float64),\n        },\n        # Case C: unequal temperatures, mildly varying biases\n        {\n            \"H\": np.array([[100, 50, 25], [10, 20, 40]], dtype=np.float64),\n            \"U\": np.array([[0.2, 0.2, 0.2], [0.0, 0.5, 1.0]], dtype=np.float64),\n            \"beta\": np.array([1.0, 0.5], dtype=np.float64),\n        },\n        # Case D: extreme biases with strongly polarized counts, equal temperature\n        {\n            \"H\": np.array([[200, 10, 1], [1, 10, 200]], dtype=np.float64),\n            \"U\": np.array([[0.0, 5.0, 10.0], [10.0, 5.0, 0.0]], dtype=np.float64),\n            \"beta\": np.array([1.0, 1.0], dtype=np.float64),\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        iterations = run_wham_iteration(case[\"H\"], case[\"U\"], case[\"beta\"])\n        results.append(iterations)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在实现统计力学算法时，数值稳定性至关重要，因为计算中涉及的大能量值很容易导致浮点数溢出或下溢。本练习聚焦于一项关键技术——log-sum-exp技巧，以构建一个稳健的WHAM实现。掌握这一技巧对于确保计算结果的准确性和可靠性至关重要，尤其是在处理具有高能垒的系统时。",
            "id": "3832603",
            "problem": "您需要实现一个在加权直方图分析方法 (Weighted Histogram Analysis Method, WHAM) 中出现的数值稳定计算。在 WHAM 中，对有偏系综的重加权会引入一个位于离散区间 $j$ 上的归一化分母，该分母结合了来自多个模拟窗口 $i$ 的贡献。对于每个窗口 $i$，给定样本数 $N_i \\in \\mathbb{N}$、自由能偏移量 $f_i \\in \\mathbb{R}$、在区间点上计算的偏置势 $V_i(x_j) \\in \\mathbb{R}$ 以及逆热能 $\\beta  0$。区间 $j$ 的分母是这些输入的指数化线性组合在所有窗口上的加权和。当指数参数的绝对值很大时，直接计算该和是数值不稳定的。您的任务是采用一种能够防止上溢和下溢的对数域聚合策略，以数值稳定的方式计算该分母的自然对数，同时不改变其精确的数学值。\n\n使用的基本原理：\n- 正则系综中的正则玻尔兹曼加权指出，在逆热能 $\\beta  0$ 下，对于能量 $U \\in \\mathbb{R}$，其贡献与 $\\exp(-\\beta U)$ 成比例。\n- 在 WHAM 中，跨窗口的重加权引入了加性的自由能偏移量，并减去了在状态 $x_j$ 处施加的偏置势，从而得到一个作为输入仿射函数的指数参数。\n\n规格说明：\n- 给定数组 $\\{N_i\\}_{i=1}^K$、$\\{f_i\\}_{i=1}^K$、$\\{V_i(x_j)\\}_{i=1}^K$ 和标量 $\\beta  0$，定义\n$$\nD_j \\equiv \\sum_{i=1}^{K} N_i \\exp\\!\\left(\\beta f_i - \\beta V_i(x_j)\\right).\n$$\n- 您的程序必须在对数域中工作，并使用一种数值稳定的聚合方法来稳健地计算 $\\log D_j$，该方法避免了对极大正数或极大负数参数的指数求值。您必须将任何 $N_i = 0$ 的项视为对 $D_j$ 的贡献恰好为 $0$（等效于在聚合中排除该项）。\n- 所有地方均使用自然对数（以 $e$ 为底）。\n\n测试套件：\n请提供代码，对以下五个独立的测试用例计算 $\\log D_j$。对于每个用例，程序必须计算一个实数并将其四舍五入到 $6$ 位小数。\n\n- 用例 A（正常路径，中等数量级）：\n  - $K = 3$\n  - $\\beta = 1.0$\n  - $\\{N_i\\} = [1000, 800, 1200]$\n  - $\\{f_i\\} = [-2.0, -1.5, -1.8]$\n  - $\\{V_i(x_j)\\} = [0.5, 1.0, 0.2]$\n\n- 用例 B（朴素计算易导致上溢的大正指数）：\n  - $K = 3$\n  - $\\beta = 400.0$\n  - $\\{N_i\\} = [50, 50, 50]$\n  - $\\{f_i\\} = [0.0, 0.1, -0.2]$\n  - $\\{V_i(x_j)\\} = [-1.8, -1.9, -2.1]$\n\n- 用例 C（朴素计算易导致下溢的大负指数）：\n  - $K = 3$\n  - $\\beta = 400.0$\n  - $\\{N_i\\} = [100, 100, 100]$\n  - $\\{f_i\\} = [0.0, 0.0, 0.0]$\n  - $\\{V_i(x_j)\\} = [2.5, 2.4, 2.6]$\n\n- 用例 D（必须安全忽略的零计数窗口）：\n  - $K = 4$\n  - $\\beta = 2.0$\n  - $\\{N_i\\} = [0, 1000, 0, 500]$\n  - $\\{f_i\\} = [-1.0, -1.0, -1.0, -1.0]$\n  - $\\{V_i(x_j)\\} = [0.0, 0.1, -0.2, 0.5]$\n\n- 用例 E（计数和偏移量具有大动态范围）：\n  - $K = 3$\n  - $\\beta = 1.5$\n  - $\\{N_i\\} = [1, 10^6, 10^{12}]$\n  - $\\{f_i\\} = [10.0, -20.0, -30.0]$\n  - $\\{V_i(x_j)\\} = [0.0, -10.0, -25.0]$\n\n输出格式：\n- 您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表，顺序为 A、B、C、D、E，例如 $[r_A,r_B,r_C,r_D,r_E]$，每个 $r_\\cdot$ 都四舍五入到 $6$ 位小数。\n- 输出是相应情况下 $\\log D_j$ 的 $5$ 个浮点数值。",
            "solution": "该问题要求以数值稳定的方式计算指数加权和的自然对数，这是统计力学中的一项常见任务，特别是在加权直方图分析方法 (WHAM) 的框架内。需要计算的量是 $\\log D_j$，其中 $D_j$ 是 WHAM 方程中离散区间 $j$ 的分母，定义为：\n$$\nD_j \\equiv \\sum_{i=1}^{K} N_i \\exp\\!\\left(\\beta f_i - \\beta V_i(x_j)\\right)\n$$\n在这里，$N_i$ 表示来自模拟窗口 $i$ 的样本数，$f_i$ 是该窗口的自由能，$V_i(x_j)$ 是在窗口 $i$ 中施加的、在对应于区间 $j$ 的构型上计算的偏置势，$\\beta$ 是逆热能，$K$ 是模拟窗口的总数。\n\n$D_j$ 的直接（或朴素）计算方法是，先计算每一项 $N_i \\exp(\\beta (f_i - V_i(x_j)))$，然后将它们相加。这种方法容易产生严重的数值误差。如果指数的参数 $\\beta (f_i - V_i(x_j))$ 是一个很大的正数，指数函数将会上溢，导致结果为 `infinity`。反之，如果参数是一个很大的负数，指数将会下溢到 $0$。如果所有项都下溢，和将被错误地计算为 $0$，其对数为 $-\\infty$。所提供的测试用例旨在暴露这些弱点。\n\n为了规避这些问题，我们必须在对数域中工作，并采用一种被称为“log-sum-exp”技巧的标准数值稳定技术。其目标是在不计算大 $y_i$ 的 $\\exp(y_i)$ 的情况下，计算 $\\log(\\sum_{i} \\exp(y_i))$。\n\n首先，我们将 $D_j$ 的表达式转换为所需的 log-sum-exp 形式。对于样本数 $N_i  0$ 的每一项 $i$，我们可以写成 $N_i = \\exp(\\log N_i)$。$N_i=0$ 的情况是平凡的，因为该项对总和的贡献恰好为 $0$，应从计算中排除。令 $I = \\{i \\mid N_i  0\\}$ 为具有非零样本数的窗口索引集合。和 $D_j$ 可以写为：\n$$\nD_j = \\sum_{i \\in I} \\exp(\\log N_i) \\exp\\left(\\beta (f_i - V_i(x_j))\\right) = \\sum_{i \\in I} \\exp\\left(\\log N_i + \\beta f_i - \\beta V_i(x_j)\\right)\n$$\n我们将每个指数项的参数定义为：\n$$\ny_i = \\log N_i + \\beta(f_i - V_i(x_j))\n$$\n我们现在的任务是计算 $\\log D_j = \\log\\left(\\sum_{i \\in I} \\exp(y_i)\\right)$。\n\nlog-sum-exp 技巧依赖于从和中提出最大项。令 $y_{\\max} = \\max_{i \\in I} \\{y_i\\}$。然后我们可以写成：\n\\begin{align*}\n\\log D_j = \\log\\left(\\sum_{i \\in I} \\exp(y_i)\\right) \\\\\n= \\log\\left(\\exp(y_{\\max}) \\sum_{i \\in I} \\exp(y_i - y_{\\max})\\right) \\\\\n= \\log(\\exp(y_{\\max})) + \\log\\left(\\sum_{i \\in I} \\exp(y_i - y_{\\max})\\right) \\\\\n= y_{\\max} + \\log\\left(\\sum_{i \\in I} \\exp(y_i - y_{\\max})\\right)\n\\end{align*}\n这个公式是数值稳定的。和内指数的参数 $y_i - y_{\\max}$ 现在都小于或等于 $0$。最大的参数是 $0$，对应于 $y_i=y_{\\max}$ 的项，并且 $\\exp(0)=1$。所有其他参数都是负数。这种变换可以防止上溢，因为 `exp` 的参数没有一个是大正数。它还减轻了下溢的破坏性影响；如果某项 $y_i$ 远小于 $y_{\\max}$，则 $y_i - y_{\\max}$ 将是一个很大的负数，而 $\\exp(y_i - y_{\\max})$ 将正确地计算出一个接近 $0$ 的值，对和的贡献可以忽略不计，这在机器精度下是其数学上正确的行为。\n\n计算 $\\log D_j$ 的算法如下：\n1.  对于给定的输入集 $\\{N_i\\}$、$\\{f_i\\}$、$\\{V_i(x_j)\\}$ 和 $\\beta$，确定索引集 $I = \\{i \\mid N_i  0\\}$。如果该集合为空，则和 $D_j$ 为 $0$，$\\log D_j$ 为 $-\\infty$。\n2.  对于每个索引 $i \\in I$，计算相应的对数域项 $y_i = \\log N_i + \\beta(f_i - V_i(x_j))$。\n3.  找出所有计算项中的最大值，$y_{\\max} = \\max_{i \\in I} \\{y_i\\}$。\n4.  使用稳定公式计算最终结果：$\\log D_j = y_{\\max} + \\log\\left(\\sum_{i \\in I} \\exp(y_i - y_{\\max})\\right)$。\n\n该过程保证了对所有提供的测试用例进行稳健而准确的计算，正确处理大正指数（可能导致上溢）、大负指数（可能导致下溢）以及因计数为零而必须排除的项。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the WHAM log-denominator problem for a suite of test cases.\n    It computes the natural logarithm of the denominator term D_j from the\n    Weighted Histogram Analysis Method (WHAM) in a numerically stable way.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: ( {N_i}, {f_i}, {V_i(x_j)}, beta )\n    test_cases = [\n        # Case A: Happy path, moderate magnitudes\n        ([1000, 800, 1200], [-2.0, -1.5, -1.8], [0.5, 1.0, 0.2], 1.0),\n        \n        # Case B: Overflow-prone large positive exponents\n        ([50, 50, 50], [0.0, 0.1, -0.2], [-1.8, -1.9, -2.1], 400.0),\n        \n        # Case C: Underflow-prone large negative exponents\n        ([100, 100, 100], [0.0, 0.0, 0.0], [2.5, 2.4, 2.6], 400.0),\n        \n        # Case D: Windows with zero counts\n        ([0, 1000, 0, 500], [-1.0, -1.0, -1.0, -1.0], [0.0, 0.1, -0.2, 0.5], 2.0),\n        \n        # Case E: Large dynamic range across inputs\n        ([1., 1e6, 1e12], [10.0, -20.0, -30.0], [0.0, -10.0, -25.0], 1.5)\n    ]\n\n    results = []\n    for case in test_cases:\n        # Unpack case parameters and convert to NumPy arrays for vectorized operations.\n        # Using float64 for better precision is a good practice.\n        N, f, V, beta = (\n            np.array(case[0], dtype=np.float64),\n            np.array(case[1], dtype=np.float64),\n            np.array(case[2], dtype=np.float64),\n            np.float64(case[3])\n        )\n        \n        # Main logic to calculate the result for one case goes here.\n        \n        # Step 1: Filter out terms where N_i is 0, as they contribute 0 to the sum.\n        # In the log domain, log(0) is undefined, so these terms must be handled separately.\n        non_zero_mask = N  0\n        \n        if not np.any(non_zero_mask):\n            # Edge case: if all N_i are 0, the sum is 0, and log(0) is -infinity.\n            result = -np.inf\n        else:\n            # Apply the mask to filter the arrays.\n            N_filt = N[non_zero_mask]\n            f_filt = f[non_zero_mask]\n            V_filt = V[non_zero_mask]\n            \n            # Step 2: Calculate the log-domain arguments for the sum of exponentials.\n            # y_i = log(N_i) + beta * (f_i - V_i(x_j))\n            y = np.log(N_filt) + beta * (f_filt - V_filt)\n            \n            # Step 3: Apply the log-sum-exp trick for numerical stability.\n            # log(sum(exp(y_i))) = y_max + log(sum(exp(y_i - y_max)))\n            y_max = np.max(y)\n            result = y_max + np.log(np.sum(np.exp(y - y_max)))\n\n        # Append the result, formatted to 6 decimal places as required.\n        results.append(f\"{result:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "掌握了正确的实现方法之后，使用WHAM的一个关键环节是选择合适的分析参数，例如直方图的组距（bin width）。这一选择涉及到系统误差（偏差）和统计误差（方差）之间的基本权衡。这项高级练习将引导你从简单的代码实现走向科学探究，你将构建一个完整的工作流程来生成模拟数据、应用WHAM，并定量分析组距大小如何影响最终计算出的平均力势的准确性，从而深刻理解偏差-方差权衡。",
            "id": "2465743",
            "problem": "您将实现一项关于加权直方图分析方法 (WHAM) 的数值研究，从第一性原理出发，量化重建的平均力势 (PMF) 中的系统偏差和统计方差之间随直方图组距 $\\,\\Delta x\\,$ 变化的权衡关系。您的实现必须遵循以下数学模型和任务。\n\n背景与核心定义：\n- 在一维空间中，沿反应坐标 $\\,x\\,$ 的无偏平衡概率密度为 $\\,p(x) \\propto \\exp\\!\\left(-\\beta U(x)\\right)\\,$，其中 $\\,U(x)\\,$ 是势能，$\\,\\beta = 1/(k_\\mathrm{B} T)\\,$。在本问题中，请在约化单位下进行计算，其中 $\\,\\beta = 1\\,$，因此能量单位为 $\\,k_\\mathrm{B}T\\,$。\n- 平均力势 (PMF) 定义为（相差一个加性常数）$\\,F(x) = -\\ln p(x)\\,$。当 $\\,\\beta = 1\\,$ 且 $\\,p(x) \\propto \\exp(-U(x))\\,$ 时，真实的PMF等于 $\\,U(x)\\,$ 加上一个常数。\n- 伞形采样在 $\\,K\\,$ 个偏置势 $\\,U_k^\\text{tot}(x) = U(x) + w_k(x)\\,$ 下测量数据，其中 $\\,w_k(x)\\,$ 是一个已知的偏置势，此处为谐波势：$\\,w_k(x) = \\tfrac{1}{2}\\kappa\\,(x - x_k)^2\\,$，其弹性常数为 $\\,\\kappa  0\\,$，窗口中心为 $\\,x_k\\,$。\n- 直方图估计量将定义域划分为宽度为 $\\,\\Delta x\\,$ 的区间（bin），并使用区间内的计数。加权直方图分析方法 (WHAM) 通过求解一个关于密度和窗口自由能偏移量的自洽系统，结合来自多个偏置窗口的直方图来估计无偏密度，从而得到PMF。\n\n您的任务：\n1) 从第一性原理生成合成数据。设真实势为对称双阱势\n$$\nU(x) \\;=\\; \\alpha \\,\\bigl(x^2 - c^2\\bigr)^2,\n$$\n其中 $\\,\\alpha  0\\,$ 且 $\\,c  0\\,$，并考虑定义域 $\\,x \\in [-L, L]\\,$。对于每个具有谐波偏置 $\\,w_k(x) = \\tfrac{1}{2}\\kappa\\,(x-x_k)^2\\,$ 和指定总样本量 $\\,N_k\\,$ 的伞形窗口 $\\,k \\in \\{1,\\dots,K\\}\\,$，通过以下方式生成合成的直方图计数：\n- 通过在 $\\,[-L,L]\\,$ 上的足够精细的网格上进行数值积分，从连续密度 $\\,p_k(x) \\propto \\exp\\!\\bigl(-U(x) - w_k(x)\\bigr)\\,$ 计算精确的偏置区间概率。\n- 从总数为 $\\,N_k\\,$ 和已计算的区间概率的多项分布中，为窗口 $\\,k\\,$ 抽取每个区间的计数。这通过直接反映偏置下的玻尔兹曼分布，确保了科学真实性，而不依赖于捷径或封闭形式的采样。\n\n2) WHAM重建。对于给定的 $\\,\\Delta x\\,$，实现WHAM以估计区间中心的无偏密度。您的实现必须使用不动点迭代，该迭代强制密度进行适当的归一化，并确定每个窗口的自由能偏移量。PMF的估计值为 $\\,\\widehat{F}(x_b) = -\\ln \\widehat{p}(x_b)\\,$（相差一个加性常数）；为便于数值比较，选择该常数使得 $\\,\\min_b \\widehat{F}(x_b) = 0\\,$，同样地，将真实PMF $\\,F_\\text{true}(x)=U(x)\\,$ 在相同的区间中心上求值时，也使其满足 $\\,\\min_b F_\\text{true}(x_b) = 0\\,$。\n\n3) 作为 $\\,\\Delta x\\,$ 函数的偏差-方差分析。对于每个候选的 $\\,\\Delta x\\,$，重复合成数据集生成和WHAM重建过程 $\\,R\\,$ 次（独立重复），以凭经验估计：\n- 逐点平均PMF $\\,\\overline{F}_{\\Delta x}(x_b)\\,$ 和方差 $\\,\\operatorname{Var}_{\\Delta x}\\!\\bigl[F(x_b)\\bigr]\\,$（跨重复实验）。\n- 积分平方偏差\n$$\nB^2(\\Delta x) \\;=\\; \\sum_b \\bigl(\\,\\overline{F}_{\\Delta x}(x_b) - F_\\text{true}(x_b)\\,\\bigr)^2 \\,\\Delta x,\n$$\n和积分方差\n$$\nV(\\Delta x) \\;=\\; \\sum_b \\operatorname{Var}_{\\Delta x}\\!\\bigl[F(x_b)\\bigr] \\,\\Delta x.\n$$\n- 平均积分平方误差 (MISE) $\\,\\mathrm{MISE}(\\Delta x) = B^2(\\Delta x) + V(\\Delta x)\\,$，它反映了基本的权衡关系：增加 $\\,\\Delta x\\,$ 会增加区间平均带来的偏差，但会减少统计方差；减小 $\\,\\Delta x\\,$ 会减少偏差，但由于每个区间的计数更少而增加方差。\n\n测试套件：\n在约化单位下，采用以下固定的物理和数值参数：\n- 双阱势：$\\,\\alpha = 2\\,$, $\\,c = 1\\,$, 定义域 $\\,[-L,L] = [-2,2]\\,$。\n- 伞形窗口数量：$\\,K = 7\\,$，中心位于 $\\,x_k \\in \\{-1.5,-1.0,-0.5,0.0,0.5,1.0,1.5\\}\\,$，弹性常数 $\\,\\kappa = 20\\,$。\n- 用于偏差-方差估计的独立重复次数：$\\,R = 40\\,$。\n- 在所有情况下，假设每个窗口的计数相等，即 $\\,N_k = N\\,$。\n\n定义三个测试用例，通过改变 $\\,N\\,$ 和候选的组距，来探索方差主导、平衡和偏差主导的情形：\n- 用例 $\\,1\\,$ (中等采样)：$\\,N = 2000\\,$，候选 $\\,\\Delta x \\in \\{0.02,\\,0.05,\\,0.10,\\,0.20\\}\\,$。\n- 用例 $\\,2\\,$ (低采样)：$\\,N = 500\\,$，候选 $\\,\\Delta x \\in \\{0.02,\\,0.05,\\,0.10,\\,0.20\\}\\,$。\n- 用例 $\\,3\\,$ (高采样)：$\\,N = 10000\\,$，候选 $\\,\\Delta x \\in \\{0.02,\\,0.05,\\,0.10,\\,0.20\\}\\,$。\n\n您的程序必须做到：\n- 对于每个测试用例，以及每个候选的 $\\,\\Delta x\\,$，执行上述基于重复实验的偏差-方差分析，并计算 $\\,\\mathrm{MISE}(\\Delta x)\\,$。\n- 对于每个测试用例，选择使 $\\,\\mathrm{MISE}(\\Delta x)\\,$ 最小的 $\\,\\Delta x\\,$。如果存在多个最小值，则选择其中最小的 $\\,\\Delta x\\,$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含三个测试用例选出的最优组距，形式为逗号分隔的列表并用方括号括起，例如 `[0.05,0.10,0.05]`。不应打印任何额外文本。\n\n重要说明：\n- 所有能量单位均为 $\\,k_\\mathrm{B}T\\,$，$\\,x\\,$ 是无量纲的。\n- 不涉及角度。\n- 随机性必须在内部处理并可复现；请固定一个随机种子。\n- 实现必须是自包含的，并且不得读取或写入任何文件，也不需要用户输入。",
            "solution": "该问题要求对加权直方图分析方法（WHAM）在直方图组距 $\\Delta x$ 的函数下的偏差-方差权衡进行数值研究。该问题在科学上是有效的，定义明确，并且所有必要的参数都已提供。它代表了计算化学和统计力学中的一个标准任务，基于已建立的原理。我将继续提供一个完整的解决方案。\n\n该解决方案分三个主要阶段实现：合成数据生成、使用WHAM进行PMF重建，以及对多个独立重复实验进行偏差-方差分析。\n\n### 1. 合成数据生成\n\n为确保科学上真实的分析，合成数据必须直接从底层的玻尔兹曼分布生成。真实、无偏的势是一个对称双阱势，由 $U(x) = \\alpha(x^2 - c^2)^2$ 给出，其中 $\\alpha=2$ 且 $c=1$。定义域为 $x \\in [-2, 2]$。所有计算均在 $\\beta = (k_B T)^{-1} = 1$ 的约化单位下进行。\n\n在伞形采样中，系统在 $K$ 个不同的偏置势 $w_k(x)$ 下进行模拟，以增强对高能区域的采样。此处使用谐波偏置 $w_k(x) = \\frac{1}{2}\\kappa(x - x_k)^2$，其中 $\\kappa=20$，并且 $K=7$ 个窗口的中心位于 $x_k \\in \\{-1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5\\}$。\n\n对于每个窗口 $k$，总势为 $U_k^\\text{tot}(x) = U(x) + w_k(x)$，相应的平衡概率密度为 $p_k(x) \\propto \\exp(-U_k^\\text{tot}(x))$。为了为给定的组距 $\\Delta x$ 生成直方图计数，我们首先将定义域 $[-L, L]$ 划分为离散的区间。对于窗口 $k$，在区间 $b$（从 $x_b^{\\text{start}}$ 到 $x_b^{\\text{end}}$）中观测到样本的精确概率由归一化密度的积分给出：\n$$\nP_{k,b} = \\frac{\\int_{x_b^{\\text{start}}}^{x_b^{\\text{end}}} \\exp\\left(-U_k^\\text{tot}(x)\\right) dx}{\\int_{-L}^{L} \\exp\\left(-U_k^\\text{tot}(x)\\right) dx}\n$$\n这些积分通过在精细网格上进行数值积分来计算。我们使用一个高分辨率网格（其间距远小于任何 $\\Delta x$）通过离散求和（梯形法则）来近似积分。\n\n有了每个窗口 $k$ 的区间概率向量 $\\{P_{k,b}\\}$，就可以从具有 $N_k$ 次总试验（样本）的多项分布中抽取合成的直方图计数 $\\{N_{k,b}\\}$。这个过程精确地模拟了分子模拟中采样的统计性质。\n\n### 2. WHAM重建\n\nWHAM提供了一种方法，可以结合来自多个偏置模拟的数据，以计算无偏概率分布 $p(x)$ 的最优估计，从而得到平均力势（PMF）$F(x) = -\\ln p(x)$。该方法求解一组关于每个区间中心 $x_b$ 处的无偏概率 $p_b$ 和每个模拟窗口的无量纲自由能 $f_k$ 的自洽方程。在 $\\beta=1$ 的情况下，WHAM方程为：\n$$\np_b = \\frac{\\sum_{k=1}^K N_{k,b}}{\\sum_{k=1}^K N_k \\exp(f_k - w_k(x_b))}\n$$\n$$\n\\exp(-f_k) = \\sum_b p_b \\exp(-w_k(x_b))\n$$\n这里，$p_b$ 是与真实密度成正比的未归一化概率。这些方程使用不动点迭代求解：\n1. 初始化所有自由能 $f_k = 0$。\n2. 使用当前的 $f_k$ 重复计算概率 $p_b$。\n3. 使用新的 $p_b$ 计算更新后的自由能 $f_k^{\\text{new}}$。\n4. 为防止漂移，施加一个约束，例如固定一个自由能（如 $f_1=0$）。\n5. 迭代持续进行，直到自由能收敛到指定的容差。\n\n收敛后，最终的概率 $p_b$ 用于计算给定数据集的PMF估计值：$\\widehat{F}(x_b) = -\\ln p_b$。为便于比较，估计的PMF会通过一个加性常数进行平移，使其最小值为零。真实的PMF，$F_{\\text{true}}(x_b) = U(x_b)$，同样也被归一化，使其在同一组区间中心上的最小值为零。\n\n一个关键问题是，当某个区间 $b$ 在所有窗口中的计数总和为零时，即 $\\sum_k N_{k,b} = 0$。在这种情况下，$p_b=0$，估计的PMF $\\widehat{F}(x_b)$ 为无穷大。这表示该区间估计量的灾难性失败。\n\n### 3. 偏差-方差分析\n\n该问题的核心是量化系统误差（偏差）和统计误差（方差）之间作为组距 $\\Delta x$ 函数的权衡关系。对于每个候选的 $\\Delta x$，我们执行 $R=40$ 次独立的数据生成和WHAM重建过程的重复实验。\n\n在 $R$ 次重复实验中，我们计算：\n- 平均估计PMF：$\\overline{F}_{\\Delta x}(x_b) = \\frac{1}{R} \\sum_{r=1}^R \\widehat{F}_r(x_b)$。\n- PMF的样本方差：$\\operatorname{Var}_{\\Delta x}[F(x_b)] = \\frac{1}{R-1} \\sum_{r=1}^R (\\widehat{F}_r(x_b) - \\overline{F}_{\\Delta x}(x_b))^2$。\n\n然后将这些量在定义域上积分，得到积分平方偏差 $B^2(\\Delta x)$ 和积分方差 $V(\\Delta x)$：\n$$\nB^2(\\Delta x) = \\sum_b \\left(\\overline{F}_{\\Delta x}(x_b) - F_\\text{true}(x_b)\\right)^2 \\Delta x\n$$\n$$\nV(\\Delta x) = \\sum_b \\operatorname{Var}_{\\Delta x}[F(x_b)] \\Delta x\n$$\n平均积分平方误差（MISE）是它们的和：$\\mathrm{MISE}(\\Delta x) = B^2(\\Delta x) + V(\\Delta x)$。\n\n$\\Delta x$ 的选择决定了这种权衡：\n- **小 $\\Delta x$**：偏差低，因为区间平均误差最小。然而，由于每个区间的样本数较少，统计方差很高。这增加了出现空区间的概率，从而导致估计的PMF为无穷大。如果任何一次重复实验中任何一个区间的PMF为无穷大，那么该区间的平均PMF也将是无穷大，导致MISE为无穷大。\n- **大 $\\Delta x$**：方差低，因为更多的样本被汇集到每个区间，减少了空区间的机会。然而，由于在更宽的区域上对势进行平均，偏差会增加。\n\n我们的实现通过为任何在 $R$ 次重复实验中导致一个或多个空区间的 $\\Delta x$ 分配无穷大的MISE来处理无穷大的PMF值。最优的 $\\Delta x$ 则是在那些产生有限MISE的选项中使MISE最小化的那个。这种方法正确地惩罚了对于给定样本量而言过小的组距。最终的算法会遍历每个测试用例的候选组距，计算MISE，并根据指定标准选择最优的 $\\Delta x$。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the WHAM bias-variance analysis for all test cases.\n    \"\"\"\n    \n    # --- Fixed Physical and Numerical Parameters ---\n    ALPHA = 2.0\n    C = 1.0\n    L = 2.0\n    K_UMBRELLA = 7\n    X_CENTERS = np.array([-1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5])\n    KAPPA = 20.0\n    R_REPLICATES = 40\n    \n    # --- WHAM Solver Parameters ---\n    WHAM_MAX_ITER = 10000\n    WHAM_TOL = 1e-8\n    \n    # --- Data Generation Parameters ---\n    FINE_GRID_FACTOR = 100\n    RANDOM_SEED = 1234\n    \n    RNG = np.random.default_rng(RANDOM_SEED)\n\n    # --- Test Cases ---\n    test_cases = [\n        # Case 1 (moderate sampling)\n        (2000, [0.02, 0.05, 0.10, 0.20]),\n        # Case 2 (low sampling)\n        (500, [0.02, 0.05, 0.10, 0.20]),\n        # Case 3 (high sampling)\n        (10000, [0.02, 0.05, 0.10, 0.20]),\n    ]\n    \n    # --- Helper Functions ---\n\n    def true_potential(x, alpha, c):\n        return alpha * (x**2 - c**2)**2\n\n    def bias_potential(x, x_k, kappa):\n        return 0.5 * kappa * (x - x_k)**2\n\n    def generate_synthetic_data(n_samples, delta_x):\n        \"\"\"\n        Generates one set of synthetic histogram data for all K windows.\n        \"\"\"\n        bins = np.arange(-L, L + delta_x, delta_x)\n        n_bins = len(bins) - 1\n        \n        fine_x = np.linspace(-L, L, n_bins * FINE_GRID_FACTOR)\n        fine_dx = fine_x[1] - fine_x[0]\n\n        U_fine = true_potential(fine_x, ALPHA, C)\n        \n        hist_counts = np.zeros((K_UMBRELLA, n_bins), dtype=np.int32)\n        \n        for k in range(K_UMBRELLA):\n            w_k_fine = bias_potential(fine_x, X_CENTERS[k], KAPPA)\n            U_tot_k_fine = U_fine + w_k_fine\n            \n            # Numerically integrate to get bin probabilities\n            unnorm_p_density = np.exp(-U_tot_k_fine)\n            Z_k = np.sum(unnorm_p_density) * fine_dx\n            \n            p_k_bins = np.zeros(n_bins)\n            for b in range(n_bins):\n                bin_mask = (fine_x = bins[b])  (fine_x  bins[b+1])\n                p_k_bins[b] = np.sum(unnorm_p_density[bin_mask]) * fine_dx / Z_k\n            \n            # Ensure probabilities sum to 1 due to potential floating point errors\n            p_k_bins /= np.sum(p_k_bins)\n            \n            # Generate counts from multinomial distribution\n            hist_counts[k, :] = RNG.multinomial(n_samples, p_k_bins)\n            \n        return hist_counts\n\n    def run_wham(hist_counts, delta_x, n_samples_vec):\n        \"\"\"\n        Implements the WHAM fixed-point iteration to find the PMF.\n        \"\"\"\n        bins = np.arange(-L, L + delta_x, delta_x)\n        bin_centers = (bins[:-1] + bins[1:]) / 2.0\n        n_bins = len(bin_centers)\n\n        # Pre-compute biases at bin centers\n        w_kb = np.zeros((K_UMBRELLA, n_bins))\n        for k in range(K_UMBRELLA):\n            w_kb[k, :] = bias_potential(bin_centers, X_CENTERS[k], KAPPA)\n\n        f_k = np.zeros(K_UMBRELLA)\n        \n        for _ in range(WHAM_MAX_ITER):\n            f_k_old = f_k.copy()\n            \n            # Equation for probabilities p_b\n            numer = np.sum(hist_counts, axis=0) # N_b\n            log_denom_terms = f_k[:, np.newaxis] - w_kb\n            max_log = np.max(log_denom_terms, axis=0)\n            denom_arg = n_samples_vec[:, np.newaxis] * np.exp(log_denom_terms - max_log)\n            denom = np.exp(max_log) * np.sum(denom_arg, axis=0)\n\n            # p_b are unnormalized probabilities\n            p_b = np.zeros_like(numer, dtype=float)\n            non_zero_denom = denom  1e-15 # Avoid division by zero\n            p_b[non_zero_denom] = numer[non_zero_denom] / denom[non_zero_denom]\n\n            # Equation for free energies f_k\n            exp_neg_w_kb = np.exp(-w_kb)\n            f_k_new_arg = np.sum(p_b[np.newaxis, :] * exp_neg_w_kb, axis=1)\n\n            # Avoid log(0) for windows with no overlap with data\n            f_k = -np.log(f_k_new_arg, where=f_k_new_arg  0, out=np.full_like(f_k_new_arg, np.inf))\n            \n            # Shift to set f_1 = 0\n            f_k -= f_k[0]\n            \n            if np.linalg.norm(f_k - f_k_old)  WHAM_TOL:\n                break\n        \n        # Final PMF calculation\n        pmf = -np.log(p_b, where=p_b  0, out=np.full_like(p_b, np.inf))\n        \n        if not np.all(np.isfinite(pmf)):\n             return pmf # Return with inf values\n        \n        pmf -= np.min(pmf)\n        return pmf\n\n    def analyze_bias_variance(n_samples, delta_x):\n        \"\"\"\n        Performs the full bias-variance analysis for a given N and delta_x.\n        \"\"\"\n        bins = np.arange(-L, L + delta_x, delta_x)\n        bin_centers = (bins[:-1] + bins[1:]) / 2.0\n        n_bins = len(bin_centers)\n        \n        # True PMF on bin centers, normalized\n        F_true = true_potential(bin_centers, ALPHA, C)\n        F_true -= np.min(F_true)\n        \n        all_pmfs = np.zeros((R_REPLICATES, n_bins))\n        n_samples_vec = np.full(K_UMBRELLA, n_samples)\n        \n        for r in range(R_REPLICATES):\n            hist_counts = generate_synthetic_data(n_samples, delta_x)\n            pmf_estimate = run_wham(hist_counts, delta_x, n_samples_vec)\n            all_pmfs[r, :] = pmf_estimate\n\n        # If any replicate resulted in an empty bin, MISE is infinite\n        if np.isinf(all_pmfs).any():\n            return np.inf\n            \n        # Calculate mean and variance of PMF estimates\n        mean_pmf = np.mean(all_pmfs, axis=0)\n        var_pmf = np.var(all_pmfs, axis=0, ddof=1) # Sample variance\n        \n        # Integrated squared bias\n        bias_sq = np.sum((mean_pmf - F_true)**2) * delta_x\n        \n        # Integrated variance\n        variance = np.sum(var_pmf) * delta_x\n        \n        return bias_sq + variance\n\n    # --- Main Loop ---\n    \n    optimal_dx_results = []\n    \n    for n_samples, dx_candidates in test_cases:\n        mises = []\n        for dx in dx_candidates:\n            mise = analyze_bias_variance(n_samples, dx)\n            mises.append(mise)\n        \n        mises_array = np.array(mises)\n        \n        # Find the minimum finite MISE\n        min_mise = np.min(mises_array[np.isfinite(mises_array)]) if np.any(np.isfinite(mises_array)) else np.inf\n\n        if not np.isfinite(min_mise):\n             # This case should not happen with the given parameters\n             # If all MISE are inf, pick the largest dx as it's most robust\n            optimal_dx = dx_candidates[-1]\n        else:\n            # Find all indices matching the minimum MISE\n            best_indices = np.where(mises_array == min_mise)[0]\n            # Tie-breaking rule: choose smallest dx\n            optimal_dx = dx_candidates[best_indices[0]]\n\n        optimal_dx_results.append(optimal_dx)\n\n    # Format the final output\n    print(f\"[{','.join(f'{x:.2f}' for x in optimal_dx_results)}]\")\n\n\nsolve()\n```"
        }
    ]
}