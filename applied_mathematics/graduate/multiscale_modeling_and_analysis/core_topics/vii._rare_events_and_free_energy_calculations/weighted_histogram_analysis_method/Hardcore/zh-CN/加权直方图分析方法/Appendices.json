{
    "hands_on_practices": [
        {
            "introduction": "加权直方图分析方法（WHAM）的核心在于求解一组自洽方程。这些方程将无偏概率分布与各个模拟窗口的自由能耦合起来，通常通过迭代方法求解。这个练习将指导你亲手实现WHAM算法的核心不动点迭代过程，让你从零开始构建一个可以工作的WHAM求解器。通过在一个简化的模型上  实践，你将具体地理解算法如何通过迭代更新概率和自由能，并最终收敛到最优的无偏分布。",
            "id": "3832602",
            "problem": "考虑使用加权直方图分析方法（WHAM）结合来自多个窗口的偏置直方图，以估计离散反应坐标上的潜在无偏置概率质量函数。加权直方图分析方法（WHAM）是一种用于无偏置分布的最大似然估计器，它校正在独立模拟窗口中施加的偏置势，并解决一个耦合了无偏置分布和窗口归一化自由能的自洽不动点问题。在本问题中，有两个窗口，每个窗口有三个离散的箱（bin）。能量以玻尔兹曼常数乘以温度 $k_{\\mathrm{B}} T$ 的单位报告，因此逆温度是无量纲的。统计效率低下被忽略（设为 $1$），箱宽被认为是恒定的并被吸收到归一化中。\n\n从均匀初始分布 $p^{(0)}(x_j)$（其中 $j \\in \\{1,2,3\\}$）开始，实现 WHAM 方程所隐含的不动点迭代，以更新无偏置分布 $p^{(t)}(x_j)$ 和窗口自由能 $f_k^{(t)}$（其中 $k \\in \\{1,2\\}$）。在每一步都对 $p^{(t)}(x_j)$ 进行归一化，以确保 $\\sum_{j=1}^{3} p^{(t)}(x_j) = 1$。当最大绝对变化 $\\max_j \\left| p^{(t)}(x_j) - p^{(t-1)}(x_j) \\right|$ 小于容差 $10^{-6}$ 时停止。对于以下每个测试用例，量化达到容差所需的迭代次数。你的程序应以对所有 $j$ 均有 $p^{(0)}(x_j) = 1/3$ 开始，使用提供的直方图计数和偏置势，并将逆温度值视为给定值。所有计算都应以纯数学术语进行，不报告任何物理单位。\n\n测试套件：\n- 案例 A（平衡计数，对称偏置，等温）：\n  - 窗口 $1$ 计数 $H^{(1)} = [60,120,60]$，窗口 $2$ 计数 $H^{(2)} = [40,80,40]$。\n  - 窗口 $1$ 偏置势 $U^{(1)} = [0.0,0.5,1.0]$，窗口 $2$ 偏置势 $U^{(2)} = [1.0,0.5,0.0]$。\n  - 逆温度 $\\beta^{(1)} = 1.0$, $\\beta^{(2)} = 1.0$。\n- 案例 B（稀疏计数，非对称覆盖，等温）：\n  - 窗口 $1$ 计数 $H^{(1)} = [1,0,0]$，窗口 $2$ 计数 $H^{(2)} = [0,1,1]$。\n  - 窗口 $1$ 偏置势 $U^{(1)} = [0.0,1.0,2.0]$，窗口 $2$ 偏置势 $U^{(2)} = [2.0,1.0,0.0]$。\n  - 逆温度 $\\beta^{(1)} = 1.0$, $\\beta^{(2)} = 1.0$。\n- 案例 C（不等温，轻度变化的偏置）：\n  - 窗口 $1$ 计数 $H^{(1)} = [100,50,25]$，窗口 $2$ 计数 $H^{(2)} = [10,20,40]$。\n  - 窗口 $1$ 偏置势 $U^{(1)} = [0.2,0.2,0.2]$，窗口 $2$ 偏置势 $U^{(2)} = [0.0,0.5,1.0]$。\n  - 逆温度 $\\beta^{(1)} = 1.0$, $\\beta^{(2)} = 0.5$。\n- 案例 D（极端偏置伴随强极化计数，等温）：\n  - 窗口 $1$ 计数 $H^{(1)} = [200,10,1]$，窗口 $2$ 计数 $H^{(2)} = [1,10,200]$。\n  - 窗口 $1$ 偏置势 $U^{(1)} = [0.0,5.0,10.0]$，窗口 $2$ 偏置势 $U^{(2)} = [10.0,5.0,0.0]$。\n  - 逆温度 $\\beta^{(1)} = 1.0$, $\\beta^{(2)} = 1.0$。\n\n算法要求：\n- 初始化 $p^{(0)}(x_j) = 1/3$ 对于 $j \\in \\{1,2,3\\}$。\n- 在每次迭代 $t$ 中，使用窗口 $k$ 的归一化条件更新窗口自由能 $f_k^{(t)}$，然后通过将两个窗口的直方图与窗口偏置和 $f_k^{(t)}$ 进行适当的重加权相结合来更新无偏置分布 $p^{(t)}(x_j)$，接着进行归一化以使 $\\sum_{j=1}^{3} p^{(t)}(x_j) = 1$。\n- 使用停止准则 $\\max_j \\left| p^{(t)}(x_j) - p^{(t-1)}(x_j) \\right| < 10^{-6}$，并报告达到容差所执行的总迭代次数。\n\n最终输出格式：\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔的整数列表（例如，$[7,42,13,5]$），其中整数按顺序对应案例 A、B、C 和 D 的迭代次数。",
            "solution": "该问题已经过验证，并被确定为一个有效的、适定的科学问题。所有必要的数据和条件均已提供，并且该任务符合统计力学和计算科学的既定原则。\n\n该问题要求实现加权直方图分析方法（WHAM），以找到一个具有由 $M=3$ 个箱（bin）定义的离散反应坐标的系统的无偏置概率质量函数 $p(x_j)$。数据来源于在 $K=2$ 个窗口中的模拟，每个窗口都受到一个势 $U^{(k)}(x_j)$ 的偏置。目标是针对四个不同的测试用例迭代求解 WHAM 方程，并报告收敛所需的迭代次数。\n\nWHAM 方程为无偏置概率 $p(x_j)$ 和与每个模拟窗口 $k$ 相关的无量纲自由能 $f_k$ 构成了一组自洽关系。对于一个有 $K$ 个窗口和 $M$ 个箱的系统，这些方程是：\n$$\np(x_j) = \\frac{\\sum_{k=1}^{K} H^{(k)}(x_j)}{\\sum_{k=1}^{K} N_k e^{\\beta^{(k)} f_k} e^{-\\beta^{(k)} U^{(k)}(x_j)}}\n\\quad \\quad (1)\n$$\n$$\ne^{-\\beta^{(k)} f_k} = \\sum_{j=1}^{M} p(x_j) e^{-\\beta^{(k)} U^{(k)}(x_j)}\n\\quad \\quad (2)\n$$\n此处，$H^{(k)}(x_j)$ 是在窗口 $k$ 的箱 $j$ 中观测到的计数，$N_k = \\sum_{j=1}^{M} H^{(k)}(x_j)$ 是来自窗口 $k$ 的总样本数，$U^{(k)}(x_j)$ 是施加于窗口 $k$ 的箱 $j$ 的偏置势，而 $\\beta^{(k)}$ 是相应的逆温度（作为无量纲量给出）。概率分布必须满足归一化条件 $\\sum_{j=1}^{M} p(x_j) = 1$。注意，方程 $(1)$ 必须被解释为使得最终得到的 $p(x_j)$ 是归一化的。\n\n这些耦合的非线性方程可以使用不动点迭代方案求解。算法流程如下：\n\n1.  **初始化**：从概率分布的初始猜测 $p^{(0)}(x_j)$ 开始。问题指定了均匀分布：\n    $$\n    p^{(0)}(x_j) = \\frac{1}{M}\n    $$\n    对于 $j \\in \\{1, 2, ..., M\\}$。设置迭代计数器 $t=0$ 和容差 $\\epsilon=10^{-6}$。\n\n2.  **迭代**：对于 $t = 1, 2, 3, \\dots$，执行以下步骤：\n    a.  **更新自由能**：使用前一次迭代的概率分布 $p^{(t-1)}(x_j)$ 计算每个窗口 $k$ 的自由能 $f_k^{(t)}$。重新整理方程 $(2)$ 得到：\n        $$\n        f_k^{(t)} = -\\frac{1}{\\beta^{(k)}} \\ln\\left( \\sum_{j=1}^{M} p^{(t-1)}(x_j) e^{-\\beta^{(k)} U^{(k)}(x_j)} \\right)\n        $$\n        为保证数值稳定性，通常最好计算量 $C_k^{(t)} = e^{\\beta^{(k)} f_k^{(t)}}$：\n        $$\n        C_k^{(t)} = \\left( \\sum_{j=1}^{M} p^{(t-1)}(x_j) e^{-\\beta^{(k)} U^{(k)}(x_j)} \\right)^{-1}\n        $$\n\n    b.  **更新概率**：使用更新后的自由能项 $C_k^{(t)}$，基于方程 $(1)$ 计算一个新的、未归一化的概率分布 $p_{\\text{un}}^{(t)}(x_j)$：\n        $$\n        p_{\\text{un}}^{(t)}(x_j) = \\frac{\\sum_{k=1}^{K} H^{(k)}(x_j)}{\\sum_{k=1}^{K} N_k C_k^{(t)} e^{-\\beta^{(k)} U^{(k)}(x_j)}}\n        $$\n        分子是所有窗口中箱 $j$ 的总计数。分母是结合了所有窗口信息的重加权因子。\n\n    c.  **归一化**：通过对 $p_{\\text{un}}^{(t)}(x_j)$ 进行归一化来获得更新后的概率分布 $p^{(t)}(x_j)$：\n        $$\n        p^{(t)}(x_j) = \\frac{p_{\\text{un}}^{(t)}(x_j)}{\\sum_{j'=1}^{M} p_{\\text{un}}^{(t)}(x_{j'})}\n        $$\n\n    d.  **收敛性检查**：当当前和前一次概率分布之间的最大绝对差小于指定的容差 $\\epsilon$ 时，迭代终止：\n        $$\n        \\max_j \\left| p^{(t)}(x_j) - p^{(t-1)}(x_j) \\right|  \\epsilon\n        $$\n        如果满足条件，过程停止。否则，增加 $t$ 并返回到步骤 2a。\n\n最终输出是为满足所提供的四个测试用例中每个用例的收敛准则所需的迭代次数。实现将利用 `numpy` 进行高效的向量化计算。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_wham_iteration(H, U, beta, tol=1e-6):\n    \"\"\"\n    Performs WHAM fixed-point iteration to find the unbiased probability distribution.\n\n    Args:\n        H (np.ndarray): A (num_windows, num_bins) array of histogram counts.\n        U (np.ndarray): A (num_windows, num_bins) array of bias potentials.\n        beta (np.ndarray): A (num_windows,) array of inverse temperatures.\n        tol (float): The convergence tolerance.\n\n    Returns:\n        int: The number of iterations required to reach convergence.\n    \"\"\"\n    num_windows, num_bins = H.shape\n    \n    # Total number of samples in each window\n    N = np.sum(H, axis=1)\n    \n    # Total histogram counts across all windows for each bin\n    total_H = np.sum(H, axis=0)\n    \n    # Initial guess for the probability distribution (uniform)\n    p = np.full(num_bins, 1.0 / num_bins, dtype=np.float64)\n    \n    iterations = 0\n    while True:\n        iterations += 1\n        p_old = p.copy()\n        \n        # Pre-compute exponential terms for efficiency\n        # exp(-beta_k * U_kj) for all k, j\n        exp_neg_beta_U = np.exp(-beta[:, np.newaxis] * U)\n        \n        # Step 2a: Update free energies. We compute C_k = exp(beta_k * f_k).\n        # C_k = 1 / sum_j(p_j * exp(-beta_k * U_kj))\n        sum_val = np.sum(p_old[np.newaxis, :] * exp_neg_beta_U, axis=1)\n        # Avoid division by zero, although p_old and exp are always positive\n        # for this problem's setup.\n        C = 1.0 / sum_val\n        \n        # Step 2b: Update unnormalized probabilities p_un(x_j)\n        # Denominator: sum_k(N_k * C_k * exp(-beta_k * U_kj))\n        denominator = np.sum(N[:, np.newaxis] * C[:, np.newaxis] * exp_neg_beta_U, axis=0)\n        \n        p_unnormalized = np.zeros(num_bins, dtype=np.float64)\n        # Avoid division by zero if a bin has no counts at all\n        non_zero_H_mask = total_H > 0\n        non_zero_denom_mask = denominator > 0\n        valid_mask = non_zero_H_mask  non_zero_denom_mask\n        \n        p_unnormalized[valid_mask] = total_H[valid_mask] / denominator[valid_mask]\n\n        # Step 2c: Normalize probabilities\n        p_sum = np.sum(p_unnormalized)\n        if p_sum > 0:\n            p = p_unnormalized / p_sum\n        else:\n            # This would happen if total_H is all zeros, not the case for these problems.\n            # We can stop if no meaningful update is possible.\n            break\n\n        # Step 2d: Check for convergence\n        error = np.max(np.abs(p - p_old))\n        if error  tol:\n            break\n            \n    return iterations\n\ndef solve():\n    \"\"\"\n    Solves the WHAM iteration problem for all specified test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: balanced counts, symmetric biases, equal temperature\n        {\n            \"H\": np.array([[60, 120, 60], [40, 80, 40]], dtype=np.float64),\n            \"U\": np.array([[0.0, 0.5, 1.0], [1.0, 0.5, 0.0]], dtype=np.float64),\n            \"beta\": np.array([1.0, 1.0], dtype=np.float64),\n        },\n        # Case B: scarce counts, asymmetric coverage, equal temperature\n        {\n            \"H\": np.array([[1, 0, 0], [0, 1, 1]], dtype=np.float64),\n            \"U\": np.array([[0.0, 1.0, 2.0], [2.0, 1.0, 0.0]], dtype=np.float64),\n            \"beta\": np.array([1.0, 1.0], dtype=np.float64),\n        },\n        # Case C: unequal temperatures, mildly varying biases\n        {\n            \"H\": np.array([[100, 50, 25], [10, 20, 40]], dtype=np.float64),\n            \"U\": np.array([[0.2, 0.2, 0.2], [0.0, 0.5, 1.0]], dtype=np.float64),\n            \"beta\": np.array([1.0, 0.5], dtype=np.float64),\n        },\n        # Case D: extreme biases with strongly polarized counts, equal temperature\n        {\n            \"H\": np.array([[200, 10, 1], [1, 10, 200]], dtype=np.float64),\n            \"U\": np.array([[0.0, 5.0, 10.0], [10.0, 5.0, 0.0]], dtype=np.float64),\n            \"beta\": np.array([1.0, 1.0], dtype=np.float64),\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        iterations = run_wham_iteration(case[\"H\"], case[\"U\"], case[\"beta\"])\n        results.append(iterations)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "理论上正确的算法在实际计算中可能会因为数值问题而失败。在WHAM中，由于涉及对能量项的指数运算，我们经常会遇到数值上溢或下溢的挑战，尤其是在处理高能量垒或低温系统时。本练习  聚焦于解决这一关键的实践问题，你将学习并实现“log-sum-exp”技巧。掌握这项技术能确保你的代码在各种输入下都保持数值稳健性，这是从理论走向可靠科学计算的必经之路。",
            "id": "3832603",
            "problem": "你需要实现一个在加权直方图分析方法 (WHAM) 中出现的数值稳定计算。在 WHAM 中，对有偏系综的重加权会在一个由 $j$ 索引的离散区间引入一个归一化分母，该分母结合了来自多个由 $i$ 索引的模拟窗口的贡献。对于每个窗口 $i$，给定一个样本数 $N_i \\in \\mathbb{N}$、一个自由能偏移 $f_i \\in \\mathbb{R}$、一个在该区间评估的偏置势 $V_i(x_j) \\in \\mathbb{R}$ 以及一个逆热能 $\\beta  0$。区间 $j$ 的分母是这些输入的指数化线性组合在所有窗口上的加权和。当指数参数的绝对值很大时，直接计算这个和是数值不稳定的。你的任务是采用一种防止上溢和下溢的对数域聚合策略，以数值稳定的方式计算该分母的自然对数，同时不改变其精确的数学值。\n\n使用的基本原理：\n- 正则系综中的正则玻尔兹曼权重指出，在逆热能为 $\\beta  0$ 时，能量为 $U \\in \\mathbb{R}$ 的贡献与 $\\exp(-\\beta U)$ 成比例。\n- 在 WHAM 中，跨窗口的重加权引入了加性的自由能偏移，并减去了在状态 $x_j$ 处施加的偏置势，从而导致指数参数是输入的仿射函数。\n\n规格说明：\n- 给定数组 $\\{N_i\\}_{i=1}^K$、$\\{f_i\\}_{i=1}^K$、$\\{V_i(x_j)\\}_{i=1}^K$ 和标量 $\\beta  0$，定义\n$$\nD_j \\equiv \\sum_{i=1}^{K} N_i \\exp\\!\\left(\\beta f_i - \\beta V_i(x_j)\\right).\n$$\n- 你的程序必须在对数域中工作，并使用一种数值稳定的聚合方法来稳健地计算 $\\log D_j$，该方法避免了计算极大正参数或极大负参数的指数。你必须将任何 $N_i = 0$ 的项视为对 $D_j$ 的贡献恰好为 $0$（等效于在聚合中排除该项）。\n- 所有地方都使用自然对数（以 $e$ 为底）。\n\n测试套件：\n提供代码，为以下五个独立的测试用例计算 $\\log D_j$。对于每个用例，程序必须计算一个实数并将其四舍五入到 $6$ 位小数。\n\n- 用例 A（理想情况，中等数量级）：\n  - $K = 3$\n  - $\\beta = 1.0$\n  - $\\{N_i\\} = [1000, 800, 1200]$\n  - $\\{f_i\\} = [-2.0, -1.5, -1.8]$\n  - $\\{V_i(x_j)\\} = [0.5, 1.0, 0.2]$\n\n- 用例 B（朴素计算时易于上溢的大正指数）：\n  - $K = 3$\n  - $\\beta = 400.0$\n  - $\\{N_i\\} = [50, 50, 50]$\n  - $\\{f_i\\} = [0.0, 0.1, -0.2]$\n  - $\\{V_i(x_j)\\} = [-1.8, -1.9, -2.1]$\n\n- 用例 C（朴素计算时易于下溢的大负指数）：\n  - $K = 3$\n  - $\\beta = 400.0$\n  - $\\{N_i\\} = [100, 100, 100]$\n  - $\\{f_i\\} = [0.0, 0.0, 0.0]$\n  - $\\{V_i(x_j)\\} = [2.5, 2.4, 2.6]$\n\n- 用例 D（必须被安全忽略的零计数窗口）：\n  - $K = 4$\n  - $\\beta = 2.0$\n  - $\\{N_i\\} = [0, 1000, 0, 500]$\n  - $\\{f_i\\} = [-1.0, -1.0, -1.0, -1.0]$\n  - $\\{V_i(x_j)\\} = [0.0, 0.1, -0.2, 0.5]$\n\n- 用例 E（计数和偏移量具有大动态范围）：\n  - $K = 3$\n  - $\\beta = 1.5$\n  - $\\{N_i\\} = [1, 10^6, 10^{12}]$\n  - $\\{f_i\\} = [10.0, -20.0, -30.0]$\n  - $\\{V_i(x_j)\\} = [0.0, -10.0, -25.0]$\n\n输出格式：\n- 你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，按 A、B、C、D、E 的顺序排列结果，例如 $[r_A,r_B,r_C,r_D,r_E]$，每个 $r_\\cdot$ 都四舍五入到 $6$ 位小数。\n- 输出是对应各个用例的 $5$ 个浮点数值 $\\log D_j$。",
            "solution": "该问题要求以数值稳定的方式计算一个指数加权和的自然对数，这是统计力学中的一个常见任务，尤其是在加权直方图分析方法 (WHAM) 的框架内。需要计算的量是 $\\log D_j$，其中 $D_j$ 是 WHAM 方程中离散区间 $j$ 的分母，定义如下：\n$$\nD_j \\equiv \\sum_{i=1}^{K} N_i \\exp\\!\\left(\\beta f_i - \\beta V_i(x_j)\\right)\n$$\n在这里，$N_i$ 代表来自模拟窗口 $i$ 的样本数，$f_i$ 是该窗口的自由能，$V_i(x_j)$ 是在窗口 $i$ 中施加的、于对应区间 $j$ 的构型处评估的偏置势，$\\beta$ 是逆热能，$K$ 是模拟窗口的总数。\n\n对 $D_j$ 的直接（或朴素）计算涉及评估每一项 $N_i \\exp(\\beta (f_i - V_i(x_j)))$ 然后将它们相加。这种方法容易产生严重的数值错误。如果指数的参数 $\\beta (f_i - V_i(x_j))$ 是一个大的正数，指数函数将会上溢，导致结果为 `infinity`。相反，如果参数是一个大的负数，指数将会下溢到 $0$。如果所有项都下溢，和将被错误地计算为 $0$，其对数为 $-\\infty$。所提供的测试用例旨在暴露这些弱点。\n\n为规避这些问题，我们必须在对数域中工作，并采用一种标准的数值稳定技术，称为“log-sum-exp”技巧。其目标是在不计算大 $y_i$ 的 $\\exp(y_i)$ 的情况下，计算 $\\log(\\sum_{i} \\exp(y_i))$。\n\n首先，我们将 $D_j$ 的表达式转换为所需的 log-sum-exp 形式。对于每个样本数 $N_i  0$ 的项 $i$，我们可以写出 $N_i = \\exp(\\log N_i)$。$N_i=0$ 的情况是平凡的，因为该项对总和的贡献恰好为 $0$，应从计算中排除。设 $I = \\{i \\mid N_i  0\\}$ 为样本数非零的窗口索引集合。总和 $D_j$ 可以写为：\n$$\nD_j = \\sum_{i \\in I} \\exp(\\log N_i) \\exp\\left(\\beta (f_i - V_i(x_j))\\right) = \\sum_{i \\in I} \\exp\\left(\\log N_i + \\beta f_i - \\beta V_i(x_j)\\right)\n$$\n让我们将每个指数项的参数定义为：\n$$\ny_i = \\log N_i + \\beta(f_i - V_i(x_j))\n$$\n我们现在的任务是计算 $\\log D_j = \\log\\left(\\sum_{i \\in I} \\exp(y_i)\\right)$。\n\nlog-sum-exp 技巧依赖于从和中提出最大项。设 $y_{\\max} = \\max_{i \\in I} \\{y_i\\}$。然后我们可以写出：\n\\begin{align*}\n\\log D_j = \\log\\left(\\sum_{i \\in I} \\exp(y_i)\\right) \\\\\n= \\log\\left(\\exp(y_{\\max}) \\sum_{i \\in I} \\exp(y_i - y_{\\max})\\right) \\\\\n= \\log(\\exp(y_{\\max})) + \\log\\left(\\sum_{i \\in I} \\exp(y_i - y_{\\max})\\right) \\\\\n= y_{\\max} + \\log\\left(\\sum_{i \\in I} \\exp(y_i - y_{\\max})\\right)\n\\end{align*}\n这个公式是数值稳定的。和内部指数的参数 $y_i - y_{\\max}$ 现在都小于或等于 $0$。最大的参数是 $0$，对应于 $y_i=y_{\\max}$ 的项，且 $\\exp(0)=1$。所有其他参数都是负数。这种变换防止了上溢，因为没有传递给 $\\exp$ 的参数是大的正数。它还减轻了下溢的破坏性影响；如果一个项 $y_i$ 显著小于 $y_{\\max}$，则 $y_i - y_{\\max}$ 将是一个大的负数，而 $\\exp(y_i - y_{\\max})$ 将正确地计算为一个接近 $0$ 的值，对总和的贡献可以忽略不计，这在机器精度下是其数学上正确的行为。\n\n计算 $\\log D_j$ 的算法如下：\n1.  对于给定的输入集 $\\{N_i\\}$、$\\{f_i\\}$、$\\{V_i(x_j)\\}$ 和 $\\beta$，识别出索引集 $I = \\{i \\mid N_i  0\\}$。如果此集合为空，则和 $D_j$ 为 $0$，$\\log D_j$ 为 $-\\infty$。\n2.  对于每个索引 $i \\in I$，计算相应的对数域项 $y_i = \\log N_i + \\beta(f_i - V_i(x_j))$。\n3.  找到所有计算项中的最大值，$y_{\\max} = \\max_{i \\in I} \\{y_i\\}$。\n4.  使用稳定公式计算最终结果：$\\log D_j = y_{\\max} + \\log\\left(\\sum_{i \\in I} \\exp(y_i - y_{\\max})\\right)$。\n\n此过程保证了对所有提供的测试用例进行稳健而准确的计算，正确处理了大正指数（潜在上溢）、大负指数（潜在下溢）以及因计数为零而必须排除的项。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the WHAM log-denominator problem for a suite of test cases.\n    It computes the natural logarithm of the denominator term D_j from the\n    Weighted Histogram Analysis Method (WHAM) in a numerically stable way.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: ( {N_i}, {f_i}, {V_i(x_j)}, beta )\n    test_cases = [\n        # Case A: Happy path, moderate magnitudes\n        ([1000, 800, 1200], [-2.0, -1.5, -1.8], [0.5, 1.0, 0.2], 1.0),\n        \n        # Case B: Overflow-prone large positive exponents\n        ([50, 50, 50], [0.0, 0.1, -0.2], [-1.8, -1.9, -2.1], 400.0),\n        \n        # Case C: Underflow-prone large negative exponents\n        ([100, 100, 100], [0.0, 0.0, 0.0], [2.5, 2.4, 2.6], 400.0),\n        \n        # Case D: Windows with zero counts\n        ([0, 1000, 0, 500], [-1.0, -1.0, -1.0, -1.0], [0.0, 0.1, -0.2, 0.5], 2.0),\n        \n        # Case E: Large dynamic range across inputs\n        ([1., 1e6, 1e12], [10.0, -20.0, -30.0], [0.0, -10.0, -25.0], 1.5)\n    ]\n\n    results = []\n    for case in test_cases:\n        # Unpack case parameters and convert to NumPy arrays for vectorized operations.\n        # Using float64 for better precision is a good practice.\n        N, f, V, beta = (\n            np.array(case[0], dtype=np.float64),\n            np.array(case[1], dtype=np.float64),\n            np.array(case[2], dtype=np.float64),\n            np.float64(case[3])\n        )\n        \n        # Main logic to calculate the result for one case goes here.\n        \n        # Step 1: Filter out terms where N_i is 0, as they contribute 0 to the sum.\n        # In the log domain, log(0) is undefined, so these terms must be handled separately.\n        non_zero_mask = N > 0\n        \n        if not np.any(non_zero_mask):\n            # Edge case: if all N_i are 0, the sum is 0, and log(0) is -infinity.\n            result = -np.inf\n        else:\n            # Apply the mask to filter the arrays.\n            N_filt = N[non_zero_mask]\n            f_filt = f[non_zero_mask]\n            V_filt = V[non_zero_mask]\n            \n            # Step 2: Calculate the log-domain arguments for the sum of exponentials.\n            # y_i = log(N_i) + beta * (f_i - V_i(x_j))\n            y = np.log(N_filt) + beta * (f_filt - V_filt)\n            \n            # Step 3: Apply the log-sum-exp trick for numerical stability.\n            # log(sum(exp(y_i))) = y_max + log(sum(exp(y_i - y_max)))\n            y_max = np.max(y)\n            result = y_max + np.log(np.sum(np.exp(y - y_max)))\n\n        # Append the result, formatted to 6 decimal places as required.\n        results.append(f\"{result:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "掌握了WHAM的稳健实现后，你就可以像研究者一样使用它来探索计算方法学中的深层次问题。任何基于直方图的方法都面临一个核心抉择：箱体（bin）的宽度。这个选择体现了统计学中一个经典的“偏置-方差权衡”问题。本练习  将引导你设计一个完整的计算实验，从生成合成数据开始，到应用WHAM分析，最终量化箱宽 $\\Delta x$ 如何影响最终势能面的偏置和方差。完成这个练习意味着你不仅能使用WHAM，更能批判性地评估其结果的质量。",
            "id": "2465743",
            "problem": "您将实现加权直方图分析方法 (WHAM) 的数值研究，从第一性原理出发，量化重建的平均力势 (PMF) 中系统性偏差与统计方差之间作为直方图箱宽 $\\,\\Delta x\\,$ 函数的权衡关系。您的实现必须遵循以下数学模型和任务。\n\n背景和核心定义：\n- 在一维空间中，沿反应坐标 $\\,x\\,$ 的无偏倚平衡概率密度为 $\\,p(x) \\propto \\exp\\!\\left(-\\beta U(x)\\right)\\,$，其中 $\\,U(x)\\,$ 是势能，$\\,\\beta = 1/(k_\\mathrm{B} T)\\,$。在本问题中，请在约化单位下工作，其中 $\\,\\beta = 1\\,$，因此能量单位为 $\\,k_\\mathrm{B}T\\,$。\n- 平均力势 (PMF) 定义为 $\\,F(x) = -\\ln p(x)\\,$（相差一个加性常数）。当 $\\,\\beta = 1\\,$ 且 $\\,p(x) \\propto \\exp(-U(x))\\,$ 时，真实的 PMF 等于 $\\,U(x)\\,$ 加上一个常数。\n- 伞形采样在 $\\,K\\,$ 个偏置势 $\\,U_k^\\text{tot}(x) = U(x) + w_k(x)\\,$ 下测量数据，其中 $\\,w_k(x)\\,$ 是已知的偏置势，此处为谐波势：$\\,w_k(x) = \\tfrac{1}{2}\\kappa\\,(x - x_k)^2\\,$，其中弹性常数 $\\,\\kappa  0\\,$，窗口中心为 $\\,x_k\\,$。\n- 直方图估计器将域划分为宽度为 $\\,\\Delta x\\,$ 的箱，并使用箱计数。加权直方图分析方法 (WHAM) 结合来自多个偏置窗口的直方图来估计无偏倚密度，从而通过求解一个关于密度和窗口自由能偏移量的自洽系统来得到 PMF。\n\n您的任务：\n1) 从第一性原理生成合成数据。设真实势为对称双阱势\n$$\nU(x) \\;=\\; \\alpha \\,\\bigl(x^2 - c^2\\bigr)^2,\n$$\n其中 $\\,\\alpha  0\\,$ 和 $\\,c  0\\,$，并考虑域 $\\,x \\in [-L, L]\\,$。对于每个具有谐波偏置 $\\,w_k(x) = \\tfrac{1}{2}\\kappa\\,(x-x_k)^2\\,$ 和指定总样本量 $\\,N_k\\,$ 的伞形窗口 $\\,k \\in \\{1,\\dots,K\\}\\,$，通过以下方式生成合成直方图计数：\n- 通过在 $\\,[-L,L]\\,$ 上的足够精细的网格上进行数值积分，从连续密度 $\\,p_k(x) \\propto \\exp\\!\\bigl(-U(x) - w_k(x)\\bigr)\\,$ 计算精确的偏置箱概率。\n- 对于窗口 $\\,k\\,$，根据总样本量 $\\,N_k\\,$ 和计算出的箱概率，从多项分布中抽取每个箱的计数。这通过直接反映偏置下的玻尔兹曼分布来确保科学真实性，而不依赖于捷径或封闭形式的采样。\n\n2) WHAM 重建。对于给定的 $\\,\\Delta x\\,$，实现 WHAM 以估计箱中心的无偏倚密度。您的实现必须使用一个不动点迭代，该迭代强制执行密度的正确归一化并确定每个窗口的自由能偏移量。PMF 估计值为 $\\,\\widehat{F}(x_b) = -\\ln \\widehat{p}(x_b)\\,$（相差一个加性常数）；为了进行数值比较，选择常数使得 $\\,\\min_b \\widehat{F}(x_b) = 0\\,$，同样地，将真实 PMF $\\,F_\\text{true}(x)=U(x)\\,$ 设置为在相同箱中心上求值时 $\\,\\min_b F_\\text{true}(x_b) = 0\\,$。\n\n3) 作为 $\\,\\Delta x\\,$ 函数的偏差-方差分析。对于每个候选的 $\\,\\Delta x\\,$，重复合成数据集生成和 WHAM 重建 $\\,R\\,$ 次独立重复实验，以经验性地估计：\n- 逐点平均 PMF $\\,\\overline{F}_{\\Delta x}(x_b)\\,$ 和跨重复实验的方差 $\\,\\operatorname{Var}_{\\Delta x}\\!\\bigl[F(x_b)\\bigr]\\,$。\n- 积分平方偏差\n$$\nB^2(\\Delta x) \\;=\\; \\sum_b \\bigl(\\,\\overline{F}_{\\Delta x}(x_b) - F_\\text{true}(x_b)\\,\\bigr)^2 \\,\\Delta x,\n$$\n和积分方差\n$$\nV(\\Delta x) \\;=\\; \\sum_b \\operatorname{Var}_{\\Delta x}\\!\\bigl[F(x_b)\\bigr] \\,\\Delta x.\n$$\n- 平均积分平方误差 (MISE) $\\,\\mathrm{MISE}(\\Delta x) = B^2(\\Delta x) + V(\\Delta x)\\,$，它反映了基本的权衡：增加 $\\,\\Delta x\\,$ 会增加箱平均偏差但减少统计方差；减少 $\\,\\Delta x\\,$ 会减少偏差但由于每个箱的计数较少而增加方差。\n\n测试套件：\n在约化单位中采用以下固定的物理和数值参数：\n- 双阱势：$\\,\\alpha = 2\\,$，$\\,c = 1\\,$，域 $\\,[-L,L] = [-2,2]\\,$。\n- 伞形窗口数量：$\\,K = 7\\,$，中心位于 $\\,x_k \\in \\{-1.5,-1.0,-0.5,0.0,0.5,1.0,1.5\\}\\,$，弹性常数 $\\,\\kappa = 20\\,$。\n- 用于偏差-方差估计的独立重复实验次数：$\\,R = 40\\,$。\n- 对于所有情况，假设每个窗口的计数相等 $\\,N_k = N\\,$。\n\n通过改变 $\\,N\\,$ 和候选箱宽，定义三个测试用例，以探索方差主导、平衡和偏差主导的区域：\n- 用例 $\\,1\\,$ (中等采样)：$\\,N = 2000\\,$，候选 $\\,\\Delta x \\in \\{0.02,\\,0.05,\\,0.10,\\,0.20\\}\\,$。\n- 用例 $\\,2\\,$ (低采样)：$\\,N = 500\\,$，候选 $\\,\\Delta x \\in \\{0.02,\\,0.05,\\,0.10,\\,0.20\\}\\,$。\n- 用例 $\\,3\\,$ (高采样)：$\\,N = 10000\\,$，候选 $\\,\\Delta x \\in \\{0.02,\\,0.05,\\,0.10,\\,0.20\\}\\,$。\n\n您的程序必须做什么：\n- 对于每个测试用例，以及每个候选的 $\\,\\Delta x\\,$，执行上述基于重复实验的偏差-方差分析并计算 $\\,\\mathrm{MISE}(\\Delta x)\\,$。\n- 对于每个测试用例，选择使 $\\,\\mathrm{MISE}(\\Delta x)\\,$ 最小化的 $\\,\\Delta x\\,$。如果出现平局，选择最小化项中最小的 $\\,\\Delta x\\,$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含三个测试用例选出的最优箱宽，格式为逗号分隔的列表，并用方括号括起来，例如 $\\,\\texttt{[0.05,0.10,0.05]}\\,$. 不应打印任何额外文本。\n\n重要说明：\n- 所有能量单位均为 $\\,k_\\mathrm{B}T\\,$，$\\,x\\,$ 是无量纲的。\n- 不涉及角度。\n- 随机性必须在内部处理且可复现；请固定一个随机种子。\n- 实现必须是自包含的，不得读取或写入任何文件，也不需要用户输入。",
            "solution": "该问题要求对加权直方图分析方法 (WHAM) 的偏差-方差权衡进行数值研究，该权衡是直方图箱宽 $\\Delta x$ 的函数。问题具有科学有效性、良构性，并且提供了所有必要的参数。它代表了计算化学和统计力学中的一个标准任务，基于已建立的原理。我将着手提供一个完整的解决方案。\n\n该解决方案分三个主要阶段实现：合成数据生成、使用 WHAM 进行 PMF 重建，以及对多个重复实验进行偏差-方差分析。\n\n### 1. 合成数据生成\n\n为确保分析的科学真实性，合成数据必须直接从底层的玻尔兹曼分布生成。真实的、无偏倚的势是一个对称双阱势，由 $U(x) = \\alpha(x^2 - c^2)^2$ 给出，其中 $\\alpha=2$ 和 $c=1$。域为 $x \\in [-2, 2]$。所有计算均在约化单位下进行，其中 $\\beta = (k_B T)^{-1} = 1$。\n\n在伞形采样中，系统在 $K$ 个不同的偏置势 $w_k(x)$ 下进行模拟，以增强对高能区域的采样。这里使用谐波偏置 $w_k(x) = \\frac{1}{2}\\kappa(x - x_k)^2$，其中 $\\kappa=20$，有 $K=7$ 个窗口，中心位于 $x_k \\in \\{-1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5\\}$。\n\n对于每个窗口 $k$，总势为 $U_k^\\text{tot}(x) = U(x) + w_k(x)$，相应的平衡概率密度为 $p_k(x) \\propto \\exp(-U_k^\\text{tot}(x))$。为了生成给定箱宽 $\\Delta x$ 的直方图计数，我们首先将域 $[-L, L]$ 划分为离散的箱。对于窗口 $k$，在箱 $b$ (从 $x_b^{\\text{start}}$ 到 $x_b^{\\text{end}}$) 中观察到样本的精确概率由归一化密度的积分给出：\n$$\nP_{k,b} = \\frac{\\int_{x_b^{\\text{start}}}^{x_b^{\\text{end}}} \\exp\\left(-U_k^\\text{tot}(x)\\right) dx}{\\int_{-L}^{L} \\exp\\left(-U_k^\\text{tot}(x)\\right) dx}\n$$\n这些积分通过在精细网格上进行数值积分来计算。使用高分辨率网格（其间距远小于任何 $\\Delta x$）通过离散求和（梯形法则）来近似积分。\n\n对于每个窗口 $k$，利用其箱概率向量 $\\{P_{k,b}\\}$，从具有 $N_k$ 次总试验（样本）的多项分布中抽取合成直方图计数 $\\{N_{k,b}\\}$。该过程精确地模拟了分子模拟中采样的统计性质。\n\n### 2. WHAM 重建\n\nWHAM 提供了一种结合来自多个偏置模拟数据的方法，以计算无偏倚概率分布 $p(x)$ 的最优估计，从而得到平均力势 (PMF) $F(x) = -\\ln p(x)$。该方法求解一组关于每个箱中心 $x_b$ 处的无偏倚概率 $p_b$ 和每个模拟窗口的无量纲自由能 $f_k$ 的自洽方程。当 $\\beta=1$ 时，WHAM 方程为：\n$$\np_b = \\frac{\\sum_{k=1}^K N_{k,b}}{\\sum_{k=1}^K N_k \\exp(f_k - w_k(x_b))}\n$$\n$$\n\\exp(-f_k) = \\sum_b p_b \\exp(-w_k(x_b))\n$$\n这里，$p_b$ 是与真实密度成比例的未归一化概率。这些方程使用不动点迭代求解：\n1. 初始化所有自由能 $f_k = 0$。\n2. 使用当前的 $f_k$ 反复计算概率 $p_b$。\n3. 使用新的 $p_b$ 计算更新后的自由能 $f_k^{\\text{new}}$。\n4. 为防止漂移，施加一个约束，例如固定一个自由能（例如 $f_1=0$）。\n5. 迭代继续，直到自由能收敛到指定的容差。\n\n收敛后，最终的概率 $p_b$ 用于计算给定数据集的 PMF 估计值：$\\widehat{F}(x_b) = -\\ln p_b$。为了比较，估计的 PMF 通过一个加性常数进行平移，使其最小值为零。真实的 PMF，$F_{\\text{true}}(x_b) = U(x_b)$，也进行归一化，使其在同一组箱中心上的最小值为零。\n\n当一个箱 $b$ 在所有窗口中的计数均为零，即 $\\sum_k N_{k,b} = 0$ 时，会出现一个关键问题。在这种情况下，$p_b=0$，估计的 PMF $\\widehat{F}(x_b)$ 为无穷大。这表示该箱的估计器发生了灾难性故障。\n\n### 3. 偏差-方差分析\n\n问题的核心是量化系统误差（偏差）和统计误差（方差）之间作为箱宽 $\\Delta x$ 函数的权衡。对于每个候选的 $\\Delta x$，我们执行 $R=40$ 次独立的数据生成和 WHAM 重建过程的重复实验。\n\n在 $R$ 次重复实验中，我们计算：\n- 平均估计 PMF：$\\overline{F}_{\\Delta x}(x_b) = \\frac{1}{R} \\sum_{r=1}^R \\widehat{F}_r(x_b)$。\n- PMF 的样本方差：$\\operatorname{Var}_{\\Delta x}[F(x_b)] = \\frac{1}{R-1} \\sum_{r=1}^R (\\widehat{F}_r(x_b) - \\overline{F}_{\\Delta x}(x_b))^2$。\n\n然后将这些量在域上积分，得到积分平方偏差 $B^2(\\Delta x)$ 和积分方差 $V(\\Delta x)$：\n$$\nB^2(\\Delta x) = \\sum_b \\left(\\overline{F}_{\\Delta x}(x_b) - F_\\text{true}(x_b)\\right)^2 \\Delta x\n$$\n$$\nV(\\Delta x) = \\sum_b \\operatorname{Var}_{\\Delta x}[F(x_b)] \\Delta x\n$$\n平均积分平方误差 (MISE) 是它们的和：$\\mathrm{MISE}(\\Delta x) = B^2(\\Delta x) + V(\\Delta x)$。\n\n$\\Delta x$ 的选择决定了这种权衡：\n- **小 $\\Delta x$**：偏差低，因为箱平均误差最小。然而，由于每个箱的样本较少，统计方差很高。这增加了空箱的概率，从而导致无穷大的 PMF 估计。如果任何一次重复实验对任何箱产生无穷大的 PMF，该箱的平均 PMF 也将是无穷大，导致无穷大的 MISE。\n- **大 $\\Delta x$**：方差低，因为更多的样本被汇集到每个箱中，减少了空箱的机会。然而，由于在更宽的区域上对势进行平均，偏差会增加。\n\n我们的实现通过将无穷大的 MISE 赋给任何在 $R$ 次重复实验中导致一个或多个空箱的 $\\Delta x$ 来处理无穷大的 PMF 值。然后，最优的 $\\Delta x$ 是在产生有限 MISE 的选择中使 MISE 最小化的那个。这正确地惩罚了对于给定样本量而言过小的箱宽。最终的算法会遍历每个测试用例的候选箱宽，计算 MISE，并根据指定的标准选择最优的 $\\Delta x$。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the WHAM bias-variance analysis for all test cases.\n    \"\"\"\n    \n    # --- Fixed Physical and Numerical Parameters ---\n    ALPHA = 2.0\n    C = 1.0\n    L = 2.0\n    K_UMBRELLA = 7\n    X_CENTERS = np.array([-1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5])\n    KAPPA = 20.0\n    R_REPLICATES = 40\n    \n    # --- WHAM Solver Parameters ---\n    WHAM_MAX_ITER = 10000\n    WHAM_TOL = 1e-8\n    \n    # --- Data Generation Parameters ---\n    FINE_GRID_FACTOR = 100\n    RANDOM_SEED = 1234\n    \n    RNG = np.random.default_rng(RANDOM_SEED)\n\n    # --- Test Cases ---\n    test_cases = [\n        # Case 1 (moderate sampling)\n        (2000, [0.02, 0.05, 0.10, 0.20]),\n        # Case 2 (low sampling)\n        (500, [0.02, 0.05, 0.10, 0.20]),\n        # Case 3 (high sampling)\n        (10000, [0.02, 0.05, 0.10, 0.20]),\n    ]\n    \n    # --- Helper Functions ---\n\n    def true_potential(x, alpha, c):\n        return alpha * (x**2 - c**2)**2\n\n    def bias_potential(x, x_k, kappa):\n        return 0.5 * kappa * (x - x_k)**2\n\n    def generate_synthetic_data(n_samples, delta_x):\n        \"\"\"\n        Generates one set of synthetic histogram data for all K windows.\n        \"\"\"\n        bins = np.arange(-L, L + delta_x, delta_x)\n        n_bins = len(bins) - 1\n        \n        fine_x = np.linspace(-L, L, n_bins * FINE_GRID_FACTOR)\n        fine_dx = fine_x[1] - fine_x[0]\n\n        U_fine = true_potential(fine_x, ALPHA, C)\n        \n        hist_counts = np.zeros((K_UMBRELLA, n_bins), dtype=np.int32)\n        \n        for k in range(K_UMBRELLA):\n            w_k_fine = bias_potential(fine_x, X_CENTERS[k], KAPPA)\n            U_tot_k_fine = U_fine + w_k_fine\n            \n            # Numerically integrate to get bin probabilities\n            unnorm_p_density = np.exp(-U_tot_k_fine)\n            Z_k = np.sum(unnorm_p_density) * fine_dx\n            \n            p_k_bins = np.zeros(n_bins)\n            for b in range(n_bins):\n                bin_mask = (fine_x >= bins[b])  (fine_x  bins[b+1])\n                p_k_bins[b] = np.sum(unnorm_p_density[bin_mask]) * fine_dx / Z_k\n            \n            # Ensure probabilities sum to 1 due to potential floating point errors\n            p_k_bins /= np.sum(p_k_bins)\n            \n            # Generate counts from multinomial distribution\n            hist_counts[k, :] = RNG.multinomial(n_samples, p_k_bins)\n            \n        return hist_counts\n\n    def run_wham(hist_counts, delta_x, n_samples_vec):\n        \"\"\"\n        Implements the WHAM fixed-point iteration to find the PMF.\n        \"\"\"\n        bins = np.arange(-L, L + delta_x, delta_x)\n        bin_centers = (bins[:-1] + bins[1:]) / 2.0\n        n_bins = len(bin_centers)\n\n        # Pre-compute biases at bin centers\n        w_kb = np.zeros((K_UMBRELLA, n_bins))\n        for k in range(K_UMBRELLA):\n            w_kb[k, :] = bias_potential(bin_centers, X_CENTERS[k], KAPPA)\n\n        f_k = np.zeros(K_UMBRELLA)\n        \n        for _ in range(WHAM_MAX_ITER):\n            f_k_old = f_k.copy()\n            \n            # Equation for probabilities p_b\n            numer = np.sum(hist_counts, axis=0) # N_b\n            log_denom_terms = f_k[:, np.newaxis] - w_kb\n            max_log = np.max(log_denom_terms, axis=0)\n            denom_arg = n_samples_vec[:, np.newaxis] * np.exp(log_denom_terms - max_log)\n            denom = np.exp(max_log) * np.sum(denom_arg, axis=0)\n\n            # p_b are unnormalized probabilities\n            p_b = np.zeros_like(numer, dtype=float)\n            non_zero_denom = denom > 1e-15 # Avoid division by zero\n            p_b[non_zero_denom] = numer[non_zero_denom] / denom[non_zero_denom]\n\n            # Equation for free energies f_k\n            exp_neg_w_kb = np.exp(-w_kb)\n            f_k_new_arg = np.sum(p_b[np.newaxis, :] * exp_neg_w_kb, axis=1)\n\n            # Avoid log(0) for windows with no overlap with data\n            f_k = -np.log(f_k_new_arg, where=f_k_new_arg > 0, out=np.full_like(f_k_new_arg, np.inf))\n            \n            # Shift to set f_1 = 0\n            f_k -= f_k[0]\n            \n            if np.linalg.norm(f_k - f_k_old)  WHAM_TOL:\n                break\n        \n        # Final PMF calculation\n        pmf = -np.log(p_b, where=p_b > 0, out=np.full_like(p_b, np.inf))\n        \n        if not np.all(np.isfinite(pmf)):\n             return pmf # Return with inf values\n        \n        pmf -= np.min(pmf)\n        return pmf\n\n    def analyze_bias_variance(n_samples, delta_x):\n        \"\"\"\n        Performs the full bias-variance analysis for a given N and delta_x.\n        \"\"\"\n        bins = np.arange(-L, L + delta_x, delta_x)\n        bin_centers = (bins[:-1] + bins[1:]) / 2.0\n        n_bins = len(bin_centers)\n        \n        # True PMF on bin centers, normalized\n        F_true = true_potential(bin_centers, ALPHA, C)\n        F_true -= np.min(F_true)\n        \n        all_pmfs = np.zeros((R_REPLICATES, n_bins))\n        n_samples_vec = np.full(K_UMBRELLA, n_samples)\n        \n        for r in range(R_REPLICATES):\n            hist_counts = generate_synthetic_data(n_samples, delta_x)\n            pmf_estimate = run_wham(hist_counts, delta_x, n_samples_vec)\n            all_pmfs[r, :] = pmf_estimate\n\n        # If any replicate resulted in an empty bin, MISE is infinite\n        if np.isinf(all_pmfs).any():\n            return np.inf\n            \n        # Calculate mean and variance of PMF estimates\n        mean_pmf = np.mean(all_pmfs, axis=0)\n        var_pmf = np.var(all_pmfs, axis=0, ddof=1) # Sample variance\n        \n        # Integrated squared bias\n        bias_sq = np.sum((mean_pmf - F_true)**2) * delta_x\n        \n        # Integrated variance\n        variance = np.sum(var_pmf) * delta_x\n        \n        return bias_sq + variance\n\n    # --- Main Loop ---\n    \n    optimal_dx_results = []\n    \n    for n_samples, dx_candidates in test_cases:\n        mises = []\n        for dx in dx_candidates:\n            mise = analyze_bias_variance(n_samples, dx)\n            mises.append(mise)\n        \n        mises_array = np.array(mises)\n        \n        # Find the minimum finite MISE\n        min_mise = np.min(mises_array[np.isfinite(mises_array)]) if np.any(np.isfinite(mises_array)) else np.inf\n\n        if not np.isfinite(min_mise):\n             # This case should not happen with the given parameters\n             # If all MISE are inf, pick the largest dx as it's most robust\n            optimal_dx = dx_candidates[-1]\n        else:\n            # Find all indices matching the minimum MISE\n            best_indices = np.where(mises_array == min_mise)[0]\n            # Tie-breaking rule: choose smallest dx\n            optimal_dx = dx_candidates[best_indices[0]]\n\n        optimal_dx_results.append(optimal_dx)\n\n    # Format the final output\n    print(f\"[{','.join(f'{x:.2f}' for x in optimal_dx_results)}]\")\n\n\nsolve()\n```"
        }
    ]
}