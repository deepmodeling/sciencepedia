{
    "hands_on_practices": [
        {
            "introduction": "To build a solid foundation, we begin with a direct application of the core principles of Configurational Bias Monte Carlo (CBMC). This exercise will guide you through the calculation of the Rosenbluth sum, $W$, and the corresponding Metropolis-Hastings acceptance probability, $\\alpha$, for a single-bead displacement. By working through a lattice model with forbidden sites ($u_i = \\infty$) and comparing it to a continuous off-lattice case, you will gain a concrete understanding of how the Boltzmann weighting scheme naturally handles different energy landscapes and how attrition arises .",
            "id": "3745214",
            "problem": "Consider a single-bead displacement in Configurational Bias Monte Carlo (CBMC) for a polymer configured on a lattice with forbidden sites and compare to an off-lattice CBMC with the same number of trials. Work in the canonical ensemble, where the target equilibrium distribution is proportional to the Boltzmann factor $ \\pi(x) \\propto \\exp(-\\beta U(x)) $. Use reduced (dimensionless) energies $u_i = \\beta U_i$ so that the Boltzmann weight is $\\exp(-u_i)$ and an infinite energy $u_i = \\infty$ represents a forbidden site with zero Boltzmann weight.\n\nAt a given step, CBMC generates $K = 5$ trial positions and chooses one trial with a biased probability proportional to its Boltzmann weight. For the lattice case, some trial positions are forbidden ($u_i = \\infty$). For the off-lattice case, all trial energies are finite. Assume the proposal mechanism is symmetric aside from the CBMC bias, and that the chosen trial becomes the proposed new bead position.\n\n1. Starting from detailed balance and the Metropolis–Hastings rule, derive the CBMC acceptance probability for a single-bead displacement in terms of the Rosenbluth sums, where the Rosenbluth sum is defined as $W = \\sum_{i=1}^{K} \\exp(-u_i)$ for the set of trials generated at a configuration. Explicitly handle the case where some $u_i = \\infty$.\n\n2. For the lattice configuration, the set of trial energies at the current (old) bead position is $u_i^{\\text{latt, old}} = [\\infty, 2.0, 0.7, \\infty, 1.2]$ and at the proposed (new) bead position is $u_i^{\\text{latt, new}} = [\\infty, 0.4, 2.3, \\infty, 0.9]$. Compute the corresponding Rosenbluth sums $W^{\\text{latt}}_{\\text{old}}$ and $W^{\\text{latt}}_{\\text{new}}$, and the acceptance probability $\\alpha_{\\text{latt}}$.\n\n3. For the off-lattice configuration, the set of trial energies at the current (old) bead position is $u_i^{\\text{off, old}} = [2.0, 0.7, 1.2, 1.8, 0.5]$ and at the proposed (new) bead position is $u_i^{\\text{off, new}} = [0.4, 2.3, 0.9, 1.1, 0.2]$. Compute the corresponding Rosenbluth sums $W^{\\text{off}}_{\\text{old}}$ and $W^{\\text{off}}_{\\text{new}}$, and the acceptance probability $\\alpha_{\\text{off}}$.\n\n4. Define attrition for CBMC growth as the event that at a growth step the Rosenbluth sum is zero, $W = 0$, so that the step cannot proceed. In the lattice, consider a blocked site with $u_i^{\\text{blocked}} = [\\infty, \\infty, \\infty, \\infty, \\infty]$. Compute $W_{\\text{blocked}}$ and the corresponding attrition indicator $I_{\\text{attr}}$, where $I_{\\text{attr}} = 1$ if $W = 0$ and $I_{\\text{attr}} = 0$ otherwise.\n\nRound all Rosenbluth sums to six significant figures and all acceptance probabilities to four significant figures. Report your final results in the order $$(W^{\\text{latt}}_{\\text{old}}, W^{\\text{latt}}_{\\text{new}}, \\alpha_{\\text{latt}}, W^{\\text{off}}_{\\text{old}}, W^{\\text{off}}_{\\text{new}}, \\alpha_{\\text{off}}, W_{\\text{blocked}}, I_{\\text{attr}})$$ The acceptance probabilities and indicators are dimensionless; no physical units are required.",
            "solution": "The canonical ensemble assigns a target probability density $\\pi(x) \\propto \\exp(-\\beta U(x))$ to configuration $x$ with energy $U(x)$. In a Metropolis–Hastings framework, the acceptance probability for a proposed move from $x$ to $x'$ is\n$$\n\\alpha(x \\to x') = \\min\\left( 1,\\; \\frac{\\pi(x')\\, q(x \\mid x')}{\\pi(x)\\, q(x' \\mid x)} \\right),\n$$\nwhere $q(x' \\mid x)$ is the proposal probability density for generating $x'$ given $x$.\n\nIn Configurational Bias Monte Carlo (CBMC), at a given bead displacement step, we generate $K$ trial positions $\\{i=1,\\dots,K\\}$ and assign each trial a Boltzmann weight $w_i = \\exp(-u_i)$, where $u_i = \\beta U_i$ is the reduced energy. The Rosenbluth sum for this step is\n$$\nW = \\sum_{i=1}^{K} w_i = \\sum_{i=1}^{K} \\exp(-u_i).\n$$\nThe CBMC selection rule chooses one of the $K$ trials $i^\\star$ with probability\n$$\np(i^\\star) = \\frac{w_{i^\\star}}{W} = \\frac{\\exp(-u_{i^\\star})}{\\sum_{i=1}^{K} \\exp(-u_i)}.\n$$\nIf a trial has $u_i = \\infty$, then $\\exp(-u_i) = 0$, so forbidden trials contribute zero to $W$ and have zero selection probability. If all $K$ trials are forbidden, then $W = 0$, and the growth step cannot proceed; this is the attrition event.\n\nThe Metropolis-Hastings acceptance rule requires balancing the forward and reverse transition probabilities. For a CBMC move, which this single-bead displacement is a special case of, this balance is achieved by an acceptance probability that depends on the ratio of the Rosenbluth factors. The full derivation shows that the explicit Boltzmann factors for the total configurations cancel out, leading to the simple and elegant acceptance rule (as also shown in the main article text):\n$$\n\\alpha = \\min\\left( 1,\\; \\frac{W_{\\text{new}}}{W_{\\text{old}}} \\right).\n$$\nForbidden trials with $u_i = \\infty$ reduce $W$ by removing terms, and if they remove all terms so that $W = 0$, the step is terminated due to attrition.\n\nWe now compute the requested quantities.\n\nLattice case (some trials forbidden):\nOld configuration trials: $u_i^{\\text{latt, old}} = [\\infty, 2.0, 0.7, \\infty, 1.2]$. The Rosenbluth sum is\n\\begin{align*}\nW^{\\text{latt}}_{\\text{old}}\n&= 0 + \\exp(-2.0) + \\exp(-0.7) + 0 + \\exp(-1.2).\n\\end{align*}\nEvaluating each term,\n$ \\exp(-2.0) = 0.1353352832366127 $,\n$ \\exp(-0.7) = 0.4965853037914095 $,\n$ \\exp(-1.2) = 0.3011942119122020 $,\nso\n$$\nW^{\\text{latt}}_{\\text{old}} = 0.9331147989402242.\n$$\nNew configuration trials: $u_i^{\\text{latt, new}} = [\\infty, 0.4, 2.3, \\infty, 0.9]$. The Rosenbluth sum is\n\\begin{align*}\nW^{\\text{latt}}_{\\text{new}}\n&= 0 + \\exp(-0.4) + \\exp(-2.3) + 0 + \\exp(-0.9).\n\\end{align*}\nEvaluating each term,\n$ \\exp(-0.4) = 0.6703200460356393 $,\n$ \\exp(-2.3) = 0.1002588437228037 $,\n$ \\exp(-0.9) = 0.4065696597405991 $,\nso\n$$\nW^{\\text{latt}}_{\\text{new}} = 1.1771485494990422.\n$$\nThe acceptance probability is\n\\begin{align*}\n\\alpha_{\\text{latt}} &= \\min\\left(1,\\; \\frac{W^{\\text{latt}}_{\\text{new}}}{W^{\\text{latt}}_{\\text{old}}} \\right)\n= \\min\\left(1,\\; \\frac{1.1771485494990422}{0.9331147989402242} \\right) \\\\\n&= \\min(1, 1.261510...) = 1.\n\\end{align*}\n\nOff-lattice case (all trials finite):\nOld configuration trials: $u_i^{\\text{off, old}} = [2.0, 0.7, 1.2, 1.8, 0.5]$. The Rosenbluth sum is\n\\begin{align*}\nW^{\\text{off}}_{\\text{old}}\n&= \\exp(-2.0) + \\exp(-0.7) + \\exp(-1.2) + \\exp(-1.8) + \\exp(-0.5) \\\\\n&= 0.1353352832366127 + 0.4965853037914095 + 0.3011942119122020 + 0.1652988882215866 + 0.6065306597126334 \\\\\n&= 1.7049443468744442.\n\\end{align*}\nNew configuration trials: $u_i^{\\text{off, new}} = [0.4, 2.3, 0.9, 1.1, 0.2]$. The Rosenbluth sum is\n\\begin{align*}\nW^{\\text{off}}_{\\text{new}}\n&= \\exp(-0.4) + \\exp(-2.3) + \\exp(-0.9) + \\exp(-1.1) + \\exp(-0.2) \\\\\n&= 0.6703200460356393 + 0.1002588437228037 + 0.4065696597405991 + 0.3328710836980796 + 0.8187307530779818 \\\\\n&= 2.3287503862751035.\n\\end{align*}\nThe acceptance probability is\n\\begin{align*}\n\\alpha_{\\text{off}} &= \\min\\left(1,\\; \\frac{W^{\\text{off}}_{\\text{new}}}{W^{\\text{off}}_{\\text{old}}} \\right)\n= \\min\\left(1,\\; \\frac{2.3287503862751035}{1.7049443468744442} \\right) \\\\\n&= \\min(1, 1.365888...) = 1.\n\\end{align*}\n\nAttrition at a blocked site:\nBlocked lattice trials: $u_i^{\\text{blocked}} = [\\infty, \\infty, \\infty, \\infty, \\infty]$. Then\n$$\nW_{\\text{blocked}} = \\sum_{i=1}^{5} \\exp(-u_i) = \\sum_{i=1}^{5} 0 = 0,\n$$\nand the attrition indicator is\n$$\nI_{\\text{attr}} = 1.\n$$\n\nApplying the rounding instructions:\n- Rosenbluth sums to six significant figures:\n$W^{\\text{latt}}_{\\text{old}} = 0.933115$, $W^{\\text{latt}}_{\\text{new}} = 1.17715$, $W^{\\text{off}}_{\\text{old}} = 1.70494$, $W^{\\text{off}}_{\\text{new}} = 2.32875$, and $W_{\\text{blocked}} = 0$ (exact).\n- Acceptance probabilities to four significant figures:\n$\\alpha_{\\text{latt}} = 1.000$ and $\\alpha_{\\text{off}} = 1.000$.\n- Attrition indicator $I_{\\text{attr}} = 1$ (exact).\n\nReport in the required order $(W^{\\text{latt}}_{\\text{old}}, W^{\\text{latt}}_{\\text{new}}, \\alpha_{\\text{latt}}, W^{\\text{off}}_{\\text{old}}, W^{\\text{off}}_{\\text{new}}, \\alpha_{\\text{off}}, W_{\\text{blocked}}, I_{\\text{attr}})$.",
            "answer": "$$\\boxed{\\begin{pmatrix}0.933115 & 1.17715 & 1.000 & 1.70494 & 2.32875 & 1.000 & 0 & 1\\end{pmatrix}}$$"
        },
        {
            "introduction": "Moving from theory to practice, this problem addresses a critical challenge in implementing CBMC: numerical stability. While the Rosenbluth sum, $w_i = \\sum_{j} \\exp(-\\beta \\Delta U_i^{(j)})$, is simple on paper, its direct computation can easily fail due to floating-point overflow or underflow. This hands-on coding exercise  introduces the indispensable \"log-sum-exp\" technique, a robust method for calculating sums of exponentials that is essential for writing reliable scientific software.",
            "id": "3745270",
            "problem": "Consider a polymer growth step in Configurational Bias Monte Carlo (CBMC), where a monomer is to be placed by sampling from $k$ trial positions. For trial $j$ at segment $i$, let the incremental energy change be $\\Delta U_i^{(j)}$, measured in units of thermal energy so that it is dimensionless multiples of Boltzmann’s constant times temperature $k_B T$ and therefore the inverse thermal energy is $\\beta = 1$. The Rosenbluth sum (also called Rosenbluth factor) for segment $i$ is defined as $w_i = \\sum_{j=1}^{k} \\exp\\!\\left(-\\beta \\,\\Delta U_i^{(j)}\\right)$, and the corresponding normalized sampling probability for trial $j$ is $p_i^{(j)} = \\exp\\!\\left(-\\beta \\,\\Delta U_i^{(j)}\\right) / w_i$. In multiscale modeling and analysis, CBMC relies on correct evaluation of $w_i$ for acceptance decisions and bias corrections. From statistical mechanics, use the Boltzmann weighting principle that the relative weight of a configuration is proportional to $\\exp\\!\\left(-\\beta U\\right)$ to justify the form of $w_i$ and $p_i^{(j)}$ from first principles. Then analyze the floating-point pitfalls that occur for large-magnitude $\\Delta U_i^{(j)}$, focusing on underflow when $\\Delta U_i^{(j)}$ is large and positive (making $\\exp\\!\\left(-\\beta \\,\\Delta U_i^{(j)}\\right)$ extremely small) and overflow when $\\Delta U_i^{(j)}$ is large and negative (making $\\exp\\!\\left(-\\beta \\,\\Delta U_i^{(j)}\\right)$ extremely large). Derive a numerically stable expression for $\\log w_i$ using the log-sum-exp identity. Starting from $x_j = -\\beta \\,\\Delta U_i^{(j)}$, show how to compute\n$$\n\\log w_i = m + \\log\\!\\left(\\sum_{j=1}^{k} \\exp(x_j - m)\\right), \\quad \\text{where } m = \\max_{1 \\le j \\le k} x_j,\n$$\nand explain why computing $\\log w_i$ this way avoids overflow and underflow in the intermediate exponentials. Explain how to recover $w_i$ from $\\log w_i$ and how to detect when $w_i$ itself cannot be represented in double-precision floating-point because $\\log w_i$ exceeds the threshold at which $\\exp(\\log w_i)$ overflows. Use these derivations to design an algorithm that, given a list of trial energy differences $\\Delta U_i^{(j)}$ (dimensionless, measured in units of $k_B T$ so that $\\beta = 1$), produces for each set:\n- The stabilized Rosenbluth sum $w_i$ computed via $\\log w_i$; if $\\log w_i$ is larger than the natural logarithm of the maximum representable floating-point number, return $+\\infty$ for $w_i$.\n- The stabilized logarithm $\\log w_i$.\n- A boolean indicating whether direct naive summation of $\\sum_j \\exp\\!\\left(-\\beta \\,\\Delta U_i^{(j)}\\right)$ matches the stabilized $w_i$ within a relative tolerance of $10^{-12}$, or both are $+\\infty$, or both are exactly $0.0$.\n\nYour program must implement this algorithm and apply it to the following test suite of trial energy difference lists, each list expressed as values of $\\Delta U$ in units of $k_B T$ (dimensionless):\n- Case $1$ (moderate values, happy path): `[ -0.1, 0.0, 0.2, 0.5 ]`.\n- Case $2$ (underflow of individual terms but finite sum): `[ 100.0, 800.0, 1000.0 ]`.\n- Case $3$ (overflow in naive exponentials): `[ -800.0, -1000.0, -1200.0 ]`.\n- Case $4$ (mixed extremes): `[ -1000.0, 0.0, 1000.0 ]`.\n- Case $5$ (single trial boundary): `[ 0.0 ]`.\n- Case $6$ (large set spanning negative to positive): a list of $100$ values linearly spaced from $-50.0$ to $50.0$ inclusive.\n- Case $7$ (extreme negative values): `[ -1500.0, -1400.0 ]`.\n\nThe final output format must be a single line consisting of a comma-separated list enclosed in square brackets. Each element corresponds to one test case and must itself be a list of the form `[ w_i, log w_i, eq ]`, where $w_i$ and $\\log w_i$ are floating-point numbers (with $+\\infty$ represented as Python’s string `inf` if overflow is detected), and `eq` is a boolean. For example, the overall output should look like `[ [.,.,.], [.,.,.], ... ]` with no additional whitespace outside of commas. Angles are not involved in this problem; no angle units are required. Because all energies are given in units of $k_B T$, the outputs are dimensionless and require no physical units.\n\nYour task is to produce a complete, runnable program that carries out these computations exactly for the given test suite and prints only the single-line output in the format described.",
            "solution": "The foundation for Configurational Bias Monte Carlo (CBMC) is the Boltzmann distribution from statistical mechanics, which states that the relative probability of a microstate with energy $U$ at inverse temperature $\\beta$ is proportional to $\\exp\\!\\left(-\\beta U\\right)$. In CBMC polymer growth, at segment $i$ we propose $k$ trial positions with incremental energy changes $\\Delta U_i^{(j)}$ relative to some reference. The unnormalized weight for trial $j$ follows directly as $\\exp\\!\\left(-\\beta \\Delta U_i^{(j)}\\right)$ by the Boltzmann principle. Normalizing across the $k$ options gives the sampling probability\n$$\np_i^{(j)} = \\frac{\\exp\\!\\left(-\\beta \\Delta U_i^{(j)}\\right)}{\\sum_{\\ell=1}^{k} \\exp\\!\\left(-\\beta \\Delta U_i^{(\\ell)}\\right)},\n$$\nwhere the denominator is the Rosenbluth sum\n$$\nw_i = \\sum_{j=1}^{k} \\exp\\!\\left(-\\beta \\Delta U_i^{(j)}\\right).\n$$\nThis definition is a direct consequence of the well-tested Boltzmann weighting principle and ensures detailed balance when combined with appropriate acceptance criteria for regrowth moves in multiscale modeling.\n\nWhen implementing $w_i$, naive evaluation can fail numerically:\n- If $\\Delta U_i^{(j)}$ is large and positive, then $-\\beta \\Delta U_i^{(j)}$ is large and negative, so $\\exp\\!\\left(-\\beta \\Delta U_i^{(j)}\\right)$ may underflow to $0.0$ in double precision.\n- If $\\Delta U_i^{(j)}$ is large and negative, then $-\\beta \\Delta U_i^{(j)}$ is large and positive, so $\\exp\\!\\left(-\\beta \\Delta U_i^{(j)}\\right)$ may overflow to $+\\infty$.\n\nUnderflow or overflow of individual terms can invalidate $w_i$ if the sum is computed directly as a sum of exponentials. To stabilize the computation, define $x_j = -\\beta \\,\\Delta U_i^{(j)}$ and let $m = \\max_j x_j$. Then factor out $\\exp(m)$ from the sum:\n$$\nw_i = \\sum_{j=1}^{k} \\exp(x_j) = \\exp(m) \\sum_{j=1}^{k} \\exp(x_j - m).\n$$\nTaking the natural logarithm on both sides yields the log-sum-exp identity:\n$$\n\\log w_i = m + \\log\\!\\left(\\sum_{j=1}^{k} \\exp(x_j - m)\\right).\n$$\nThis expression is numerically stable for two reasons. First, all exponents $(x_j - m)$ are $\\le 0$, so $\\exp(x_j - m) \\in (0,1]$, eliminating overflow in the inner exponentials. Second, terms that would underflow in the naive computation become tiny but are accumulated safely inside the sum before taking the logarithm. The only remaining risk is when $\\log w_i$ itself is so large that $\\exp(\\log w_i)$ cannot be represented in double precision when converting back to $w_i$.\n\nTo recover $w_i$ from $\\log w_i$, compute $w_i = \\exp(\\log w_i)$ if $\\log w_i$ is below the overflow threshold. For IEEE $754$ double precision, the largest finite floating-point number is approximately $1.7976931348623157 \\times 10^{308}$, and the threshold for the exponential function is around $\\log(1.7976931348623157 \\times 10^{308}) \\approx 709.782712893384$. Therefore, if $\\log w_i > \\log(\\text{max float})$, we should return $+\\infty$ to indicate overflow when representing $w_i$ even though $\\log w_i$ is finite.\n\nAlgorithm design:\n1. Inputs are lists of $\\Delta U_i^{(j)}$, all dimensionless in units of $k_B T$, implying $\\beta = 1$.\n2. For each list, compute $x_j = -\\beta \\,\\Delta U_i^{(j)}$ so that $x_j = -\\Delta U_i^{(j)}$.\n3. Compute $m = \\max_j x_j$, then evaluate $s = \\sum_j \\exp(x_j - m)$ in double precision. Compute $\\log w_i = m + \\log(s)$.\n4. If $\\log w_i$ exceeds the threshold $\\log(\\text{max float})$, set $w_i = +\\infty$; otherwise set $w_i = \\exp(\\log w_i)$.\n5. For comparison, compute the naive sum $w_i^{\\text{naive}} = \\sum_j \\exp(x_j)$, which may be $0.0$, finite, or $+\\infty$ due to underflow or overflow in individual terms.\n6. Define an equivalence check:\n   - If both $w_i$ and $w_i^{\\text{naive}}$ are $+\\infty$, return true.\n   - Else if both are exactly $0.0$, return true.\n   - Else if both are finite, check relative agreement $\\left|w_i - w_i^{\\text{naive}}\\right| / \\max(1.0, |w_i|, |w_i^{\\text{naive}}|) \\le 10^{-12}$ and return true if satisfied.\n   - Otherwise return false.\n7. Output, for each test case, the triplet $\\left[w_i, \\log w_i, \\text{eq}\\right]$.\n8. Aggregate all triplets into a single list printed as one line in the required format with commas and no extraneous whitespace.\n\nTest suite analysis:\n- Case $1$: Moderate values produce a well-conditioned sum; naive and stabilized agree.\n- Case $2$: Some exponentials underflow to zero individually (e.g., $\\exp(-800)$ and $\\exp(-1000)$), but the largest term $\\exp(-100)$ is finite; stabilized and naive agree, demonstrating that underflow of small contributions does not break the sum if the dominant term is representable.\n- Case $3$: Extremely negative $\\Delta U$ yield exponents like $\\exp(800)$ which overflow; naive sum returns $+\\infty$. The stabilized $\\log w_i$ is finite (near $800$), but $w_i$ must be reported as $+\\infty$ because $\\exp(\\log w_i)$ cannot be represented; equivalence is true since both are $+\\infty$.\n- Case $4$: Mixed extremes include both overflow and underflow in naive exponentials; stabilized treatment yields finite $\\log w_i$ and $w_i = +\\infty$; equivalence holds.\n- Case $5$: Single element gives $w_i = \\exp(0) = 1.0$ exactly.\n- Case $6$: The range from $-50.0$ to $50.0$ produces a large but finite sum; stabilized and naive agree.\n- Case $7$: More extreme negatives ensure overflow in naive exponentials and $w_i = +\\infty$; stabilized $\\log w_i$ is finite and large; equivalence holds.\n\nThis procedure adheres to first principles via Boltzmann weighting, uses a well-tested numerical stabilization strategy, and produces deterministic outputs for the specified test suite.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef log_sum_exp(x: np.ndarray) -> float:\n    \"\"\"\n    Compute log(sum(exp(x))) in a numerically stable manner.\n    Returns a Python float.\n    \"\"\"\n    # Handle empty input defensively (not expected in this problem)\n    if x.size == 0:\n        return -np.inf\n    # Use max trick\n    m = np.max(x)\n    # If m is -inf (all entries -inf), then sum exp(x - m) is 0 -> log is -inf\n    if not np.isfinite(m):\n        # If any finite exists, np.max would be finite; here m is -inf => all -inf\n        return -np.inf\n    # Compute the sum of exponentials of shifted values\n    s = np.sum(np.exp(x - m))\n    # s should be >= 1.0 if at least one finite term exists\n    return float(m + np.log(s))\n\ndef stable_rosenbluth(dU: np.ndarray, beta: float = 1.0):\n    \"\"\"\n    Given an array of Delta U values (dimensionless, in units of k_B T),\n    compute the stabilized Rosenbluth sum w and its logarithm log_w.\n    Also compute the naive sum for comparison.\n    \"\"\"\n    # Convert to exponent arguments x = -beta * dU\n    x = -beta * dU\n    # Stabilized log-sum-exp\n    log_w = log_sum_exp(x)\n    # Determine overflow threshold for exp\n    log_max = np.log(np.finfo(np.float64).max)\n    if log_w > log_max:\n        w = float('inf')\n    else:\n        w = float(np.exp(log_w))\n    # Naive sum (may underflow/overflow)\n    exp_x = np.exp(x)\n    naive_w = float(np.sum(exp_x))\n    return w, log_w, naive_w\n\ndef compare_w(w: float, naive_w: float, rtol: float = 1e-12) -> bool:\n    \"\"\"\n    Compare stabilized w with naive w using relative tolerance,\n    accounting for infinities and exact zeros.\n    \"\"\"\n    if np.isinf(w) and np.isinf(naive_w):\n        return True\n    if w == 0.0 and naive_w == 0.0:\n        return True\n    if np.isfinite(w) and np.isfinite(naive_w):\n        denom = max(1.0, abs(w), abs(naive_w))\n        rel_err = abs(w - naive_w) / denom\n        return rel_err <= rtol\n    return False\n\ndef format_value(val):\n    \"\"\"\n    Format a value (float, bool, list/tuple) without spaces, as required.\n    \"\"\"\n    if isinstance(val, (list, tuple)):\n        return \"[\" + \",\".join(format_value(v) for v in val) + \"]\"\n    if isinstance(val, (np.floating, float)):\n        if np.isinf(val):\n            return \"inf\"\n        # Use repr for a compact precise representation\n        return repr(float(val))\n    if isinstance(val, (np.bool_, bool)):\n        return \"True\" if bool(val) else \"False\"\n    # Fallback for integers if any appear\n    if isinstance(val, (np.integer, int)):\n        return str(int(val))\n    # Fallback: convert to string\n    return str(val)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # All energies are dimensionless in units of k_B T (beta = 1).\n    test_cases = [\n        [-0.1, 0.0, 0.2, 0.5],               # Case 1\n        [100.0, 800.0, 1000.0],              # Case 2\n        [-800.0, -1000.0, -1200.0],          # Case 3\n        [-1000.0, 0.0, 1000.0],              # Case 4\n        [0.0],                               # Case 5\n        list(np.linspace(-50.0, 50.0, 100)), # Case 6\n        [-1500.0, -1400.0],                  # Case 7\n    ]\n\n    beta = 1.0  # energies are in k_B T units\n    results = []\n    for case in test_cases:\n        dU = np.array(case, dtype=np.float64)\n        w, log_w, naive_w = stable_rosenbluth(dU, beta=beta)\n        eq = compare_w(w, naive_w, rtol=1e-12)\n        results.append([w, log_w, eq])\n\n    # Final print statement in the exact required format: single line, no extra spaces.\n    print(format_value(results))\n\nsolve()\n```"
        },
        {
            "introduction": "With a firm grasp of how to correctly compute the Rosenbluth sum, we can now analyze and optimize the efficiency of the CBMC algorithm itself. This practice delves into the crucial trade-off between the number of trial insertions, $k$, and the computational cost of generating them. Using a simplified but illustrative energy model, you will derive an expression for the sampling efficiency and determine the optimal value of $k$ that maximizes the effective sample size per unit of computational time .",
            "id": "3745279",
            "problem": "Consider a single torsional degree of freedom with angle $\\phi \\in [0, 2\\pi)$ and a piecewise-constant energy landscape possessing two basins (minima) of equal angular width. Specifically, basin $\\mathcal{A}$ occupies the angular interval of total width $\\delta$ centered at $\\phi = 0$ and basin $\\mathcal{B}$ occupies the angular interval of total width $\\delta$ centered at $\\phi = \\pi$, with the remainder of the angular domain $[0, 2\\pi)$ having energy so high that its Boltzmann weight is negligible at the temperature considered. Assume $\\delta$ is such that the union of the two basins exhausts half the angular domain, i.e., the two basins together cover width $\\pi$, so that a uniformly drawn angle falls in either basin $\\mathcal{A}$ or $\\mathcal{B}$ with equal probability. Let the energies be $U_{\\mathcal{A}}$ in basin $\\mathcal{A}$ and $U_{\\mathcal{B}}$ in basin $\\mathcal{B}$, with $U_{\\mathcal{B}} - U_{\\mathcal{A}} = k_B T \\ln(4)$, where $k_B$ is the Boltzmann constant and $T$ is the absolute temperature. Consequently, the Boltzmann weights are $w_{\\mathcal{A}} = \\exp(-\\beta U_{\\mathcal{A}})$ and $w_{\\mathcal{B}} = \\exp(-\\beta U_{\\mathcal{B}})$ with $\\beta = 1/(k_B T)$, implying $w_{\\mathcal{A}} = 1$ and $w_{\\mathcal{B}} = 1/4$. Outside the basins, the Boltzmann weight is taken to be zero.\n\nA configurational bias Monte Carlo (CBMC) move regrows this torsion by drawing $k$ trial angles independently and uniformly from $[0, 2\\pi)$. For each trial $\\phi_{j}$, compute the weight $w(\\phi_{j}) = \\exp(-\\beta U(\\phi_{j}))$, which equals $w_{\\mathcal{A}}$ or $w_{\\mathcal{B}}$ depending on whether $\\phi_{j}$ lies in basin $\\mathcal{A}$ or $\\mathcal{B}$. The CBMC Rosenbluth factor for the new set is $W_{\\text{new}} = \\sum_{j=1}^{k} w(\\phi_{j})$, and the new angle is selected from these $k$ trials with probability proportional to $w(\\phi_{j})$. To maintain detailed balance under the multiple-try formalism, the reverse Rosenbluth factor for the old configuration is constructed as $W_{\\text{old}} = w(\\phi_{\\text{old}}) + \\sum_{j=1}^{k-1} w(\\tilde{\\phi}_{j})$, where the $\\tilde{\\phi}_{j}$ are $k-1$ independent uniform trials and $w(\\phi_{\\text{old}})$ is the Boltzmann weight of the current torsion angle. The Metropolis acceptance probability of the regrowth move is $\\min\\!\\big(1, W_{\\text{new}}/W_{\\text{old}}\\big)$.\n\nAssume the current torsion is at equilibrium under this energy landscape prior to the CBMC move. For this well-defined toy model:\n- Use the fundamental Boltzmann distribution and the configurational bias Monte Carlo (CBMC) Rosenbluth weighting to derive, from first principles, an analytic expression for the acceptance rate as a function of $k$ under a mean-field approximation that replaces the random acceptance ratio by the ratio of its expectations, namely $\\mathbb{E}[W_{\\text{new}}]/\\mathbb{E}[W_{\\text{old}}]$.\n- Let the computational cost per attempted move be affine in $k$, $t(k) = t_0 + t_1 k$, with $t_0 = 1$ and $t_1 = 1$ in arbitrary but consistent time units. Define the effective sample size per unit time as the acceptance rate divided by $t(k)$.\n- Determine the integer value of $k$ that maximizes the effective sample size per unit time for this model.\n\nProvide the final answer as the single integer $k$ that maximizes the efficiency. No rounding is needed because the quantity is an integer. The final boxed answer must contain only the integer, with no units inside the box.",
            "solution": "The problem asks for the integer number of trial angles, $k$, that maximizes the sampling efficiency of a configurational bias Monte Carlo (CBMC) move for a simplified torsional potential. The efficiency, $E(k)$, is defined as the ratio of the acceptance rate to the computational cost per move.\n\nFirst, we validate the problem statement. All givens are listed and checked for consistency. The problem is scientifically grounded in statistical mechanics and Monte Carlo methods, it is well-posed with a clear objective, and all its terms are formally defined. The problem is therefore deemed valid.\n\nThe solution proceeds in three main steps:\n1.  Derive an expression for the mean-field acceptance rate, $\\langle P_{\\text{acc}}(k) \\rangle$, as a function of $k$.\n2.  Define the efficiency, $E(k)$, using the given cost function.\n3.  Determine the integer $k \\ge 1$ that maximizes $E(k)$.\n\nThe total angular domain is $[0, 2\\pi)$. The torsional potential defines two basins, $\\mathcal{A}$ and $\\mathcal{B}$, each of width $\\delta$. The total width of the basins is given as $2\\delta = \\pi$, which implies $\\delta = \\pi/2$.\n\nA trial angle $\\phi$ is drawn uniformly from $[0, 2\\pi)$. The probability of this angle falling into basin $\\mathcal{A}$ is $P(\\phi \\in \\mathcal{A}) = \\frac{\\delta}{2\\pi} = \\frac{\\pi/2}{2\\pi} = \\frac{1}{4}$. Similarly, the probability of it falling into basin $\\mathcal{B}$ is $P(\\phi \\in \\mathcal{B}) = \\frac{\\delta}{2\\pi} = \\frac{1}{4}$. The energy outside these basins is taken to be infinite, so the Boltzmann weight $w(\\phi)$ is zero. The given Boltzmann weights within the basins are $w_{\\mathcal{A}} = 1$ and $w_{\\mathcal{B}} = 1/4$.\n\nThe expected Boltzmann weight of a single, uniformly drawn trial angle, $\\mathbb{E}[w]$, is:\n$$\n\\mathbb{E}[w] = P(\\phi \\in \\mathcal{A}) w_{\\mathcal{A}} + P(\\phi \\in \\mathcal{B}) w_{\\mathcal{B}} + P(\\phi \\notin \\mathcal{A} \\cup \\mathcal{B}) \\cdot 0\n$$\n$$\n\\mathbb{E}[w] = \\left(\\frac{1}{4}\\right)(1) + \\left(\\frac{1}{4}\\right)\\left(\\frac{1}{4}\\right) = \\frac{1}{4} + \\frac{1}{16} = \\frac{5}{16}\n$$\n\nThe problem uses a mean-field approximation for the acceptance rate, $\\langle P_{\\text{acc}}(k) \\rangle \\approx \\frac{\\mathbb{E}[W_{\\text{new}}]}{\\mathbb{E}[W_{\\text{old}}]}$. We need to compute the expectation of the new and old Rosenbluth factors, $W_{\\text{new}}$ and $W_{\\text{old}}$.\n\nThe new Rosenbluth factor is $W_{\\text{new}} = \\sum_{j=1}^{k} w(\\phi_{j})$, where the $k$ trial angles $\\phi_j$ are drawn independently and uniformly. By linearity of expectation:\n$$\n\\mathbb{E}[W_{\\text{new}}] = \\sum_{j=1}^{k} \\mathbb{E}[w(\\phi_{j})] = k \\cdot \\mathbb{E}[w] = \\frac{5k}{16}\n$$\n\nThe old Rosenbluth factor is $W_{\\text{old}} = w(\\phi_{\\text{old}}) + \\sum_{j=1}^{k-1} w(\\tilde{\\phi}_{j})$, where $\\phi_{\\text{old}}$ is the current angle (drawn from the equilibrium distribution) and the $\\tilde{\\phi}_{j}$ are $k-1$ independent uniform trial angles. The expectation is:\n$$\n\\mathbb{E}[W_{\\text{old}}] = \\mathbb{E}[w(\\phi_{\\text{old}})] + \\sum_{j=1}^{k-1} \\mathbb{E}[w(\\tilde{\\phi}_{j})] = \\mathbb{E}[w(\\phi_{\\text{old}})] + (k-1)\\mathbb{E}[w]\n$$\nTo find $\\mathbb{E}[w(\\phi_{\\text{old}})]$, we need the equilibrium probability distribution of $\\phi_{\\text{old}}$. This is the Boltzmann distribution, $P_{\\text{eq}}(\\phi) = \\frac{w(\\phi)}{Z_{\\phi}}$, where $Z_{\\phi}$ is the partition function for the torsional degree of freedom.\n$$\nZ_{\\phi} = \\int_{0}^{2\\pi} w(\\phi') d\\phi' = \\int_{\\mathcal{A}} w_{\\mathcal{A}} d\\phi' + \\int_{\\mathcal{B}} w_{\\mathcal{B}} d\\phi' = w_{\\mathcal{A}} \\delta + w_{\\mathcal{B}} \\delta\n$$\n$$\nZ_{\\phi} = (1)\\left(\\frac{\\pi}{2}\\right) + \\left(\\frac{1}{4}\\right)\\left(\\frac{\\pi}{2}\\right) = \\left(1 + \\frac{1}{4}\\right)\\frac{\\pi}{2} = \\frac{5}{4} \\frac{\\pi}{2} = \\frac{5\\pi}{8}\n$$\nThe equilibrium probability of finding the system in basin $\\mathcal{A}$ is $P_{\\text{eq}}(\\mathcal{A}) = \\frac{w_{\\mathcal{A}}\\delta}{Z_{\\phi}} = \\frac{1 \\cdot (\\pi/2)}{5\\pi/8} = \\frac{4}{5}$.\nThe equilibrium probability of finding the system in basin $\\mathcal{B}$ is $P_{\\text{eq}}(\\mathcal{B}) = \\frac{w_{\\mathcal{B}}\\delta}{Z_{\\phi}} = \\frac{(1/4) \\cdot (\\pi/2)}{5\\pi/8} = \\frac{\\pi/8}{5\\pi/8} = \\frac{1}{5}$.\nThe expected weight of the old configuration, $\\mathbb{E}[w(\\phi_{\\text{old}})]$, is calculated by averaging over the equilibrium distribution:\n$$\n\\mathbb{E}[w(\\phi_{\\text{old}})] = P_{\\text{eq}}(\\mathcal{A}) \\cdot w_{\\mathcal{A}} + P_{\\text{eq}}(\\mathcal{B}) \\cdot w_{\\mathcal{B}} = \\left(\\frac{4}{5}\\right)(1) + \\left(\\frac{1}{5}\\right)\\left(\\frac{1}{4}\\right) = \\frac{4}{5} + \\frac{1}{20} = \\frac{16+1}{20} = \\frac{17}{20}\n$$\nNow we can compute $\\mathbb{E}[W_{\\text{old}}]$:\n$$\n\\mathbb{E}[W_{\\text{old}}] = \\frac{17}{20} + (k-1)\\frac{5}{16}\n$$\nTo simplify, we use a common denominator of $80$:\n$$\n\\mathbb{E}[W_{\\text{old}}] = \\frac{17 \\cdot 4}{80} + (k-1)\\frac{5 \\cdot 5}{80} = \\frac{68}{80} + \\frac{25(k-1)}{80} = \\frac{68 + 25k - 25}{80} = \\frac{25k + 43}{80}\n$$\nThe approximate acceptance rate is:\n$$\n\\langle P_{\\text{acc}}(k) \\rangle = \\frac{\\mathbb{E}[W_{\\text{new}}]}{\\mathbb{E}[W_{\\text{old}}]} = \\frac{5k/16}{(25k+43)/80} = \\frac{25k/80}{(25k+43)/80} = \\frac{25k}{25k+43}\n$$\nThe computational cost per move is given as $t(k) = t_0 + t_1 k$ with $t_0=1$ and $t_1=1$, so $t(k) = 1+k$. The efficiency is $E(k) = \\frac{\\langle P_{\\text{acc}}(k) \\rangle}{t(k)}$:\n$$\nE(k) = \\frac{1}{1+k} \\left( \\frac{25k}{25k+43} \\right) = \\frac{25k}{(k+1)(25k+43)} = \\frac{25k}{25k^2 + 68k + 43}\n$$\nTo find the integer $k \\ge 1$ that maximizes $E(k)$, we first treat $k$ as a continuous variable and find the maximum by setting the derivative $\\frac{dE}{dk}$ to zero. The sign of the derivative is determined by its numerator. Using the quotient rule, $\\frac{d}{dx}\\frac{u}{v} = \\frac{u'v - uv'}{v^2}$, we set the numerator to zero:\n$$\n\\frac{d}{dk}(25k) (25k^2 + 68k + 43) - (25k) \\frac{d}{dk}(25k^2 + 68k + 43) = 0\n$$\n$$\n25(25k^2 + 68k + 43) - 25k(50k + 68) = 0\n$$\nDividing by $25$:\n$$\n(25k^2 + 68k + 43) - k(50k + 68) = 0\n$$\n$$\n25k^2 + 68k + 43 - 50k^2 - 68k = 0\n$$\n$$\n43 - 25k^2 = 0 \\implies 25k^2 = 43 \\implies k^2 = \\frac{43}{25}\n$$\n$$\nk = \\sqrt{\\frac{43}{25}} = \\frac{\\sqrt{43}}{5}\n$$\nSince $6^2=36$ and $7^2=49$, $\\sqrt{43}$ is between $6$ and $7$. Specifically, $\\sqrt{43} \\approx 6.557$.\n$$\nk \\approx \\frac{6.557}{5} \\approx 1.311\n$$\nThe function $E(k)$ increases for $k  \\sqrt{43}/5$ and decreases for $k > \\sqrt{43}/5$. The optimal integer value for $k$ must be one of the integers bracketing this value, i.e., $k=1$ or $k=2$. We evaluate $E(k)$ for these two values.\n\nFor $k=1$:\n$$\nE(1) = \\frac{25(1)}{(1+1)(25(1)+43)} = \\frac{25}{2(68)} = \\frac{25}{136}\n$$\n\nFor $k=2$:\n$$\nE(2) = \\frac{25(2)}{(2+1)(25(2)+43)} = \\frac{50}{3(50+43)} = \\frac{50}{3(93)} = \\frac{50}{279}\n$$\nTo compare $E(1)$ and $E(2)$, we compare the fractions $\\frac{25}{136}$ and $\\frac{50}{279}$.\nThis is equivalent to comparing $\\frac{1}{136}$ and $\\frac{2}{279}$.\nCross-multiplying, we compare $1 \\times 279$ with $2 \\times 136$.\n$$\n279 \\quad \\text{vs} \\quad 272\n$$\nSince $279 > 272$, we have $\\frac{1}{136} > \\frac{2}{279}$, which implies $\\frac{25}{136} > \\frac{50}{279}$.\nTherefore, $E(1) > E(2)$. The efficiency is maximized at $k=1$.\nFor completeness, we can check $k=3$: $E(3) = \\frac{75}{4(118)} = \\frac{75}{472} \\approx 0.159$. Since $E(2) \\approx 0.179$ and $E(1) \\approx 0.184$, the function decreases for $k \\ge 2$.\n\nThe integer value of $k$ that maximizes the effective sample size per unit time is $1$.",
            "answer": "$$\\boxed{1}$$"
        }
    ]
}