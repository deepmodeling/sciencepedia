{
    "hands_on_practices": [
        {
            "introduction": "This first exercise grounds the abstract formalism of free energy calculations in a simple, exactly solvable system. By considering a harmonic oscillator, a ubiquitous model in physics and chemistry, we can derive the free energy difference directly from the partition function. This analytical result then serves as a \"gold standard\" against which we can test the Free Energy Perturbation (FEP) method, demonstrating their equivalence and building intuition for how FEP works in a controlled setting .",
            "id": "3762113",
            "problem": "Consider a classical, one-dimensional ($1$D) coarse-grained degree of freedom $x$ that is modeled at the mesoscale by a harmonic potential $U(x;K) = \\frac{1}{2} K x^{2}$. This setting arises in multiscale modeling when a fine-grained subsystem is represented by an effective harmonic mode whose stiffness depends on the parametrization, leading to state $\\mathrm{A}$ with spring constant $K_{\\mathrm{A}}$ and state $\\mathrm{B}$ with spring constant $K_{\\mathrm{B}}$. Assume a canonical ensemble at temperature $T$, and let $k_{\\mathrm{B}}$ denote the Boltzmann constant. The Helmholtz free energy is defined by $F = - k_{\\mathrm{B}} T \\ln Z$, where $Z$ is the canonical partition function. Because only the potential energy stiffness changes between $\\mathrm{A}$ and $\\mathrm{B}$, and the kinetic energy contribution is identical in both states, focus on the configurational partition function. \n\nStarting from the canonical ensemble and the above definitions:\n- Derive the configurational partition function $Z_{\\text{conf}}(K)$ for the potential $U(x;K) = \\frac{1}{2} K x^{2}$.\n- Use this to derive the Helmholtz free energy difference $\\Delta F = F_{\\mathrm{B}} - F_{\\mathrm{A}}$ as an analytic expression in terms of $K_{\\mathrm{A}}$, $K_{\\mathrm{B}}$, $k_{\\mathrm{B}}$, and $T$.\n- Next, starting from the canonical ensemble definition of averages, derive a Free Energy Perturbation (FEP) estimator for $\\Delta F$ that expresses $\\Delta F$ through an ensemble average over state $\\mathrm{A}$, and evaluate this estimator analytically for the harmonic potential change from $K_{\\mathrm{A}}$ to $K_{\\mathrm{B}}$.\n- Provide the final closed-form expression for $\\Delta F$.\n\nYour final answer must be a single, closed-form analytical expression. Do not provide a numerical approximation. If you introduce the inverse thermal energy, define it as $\\beta = 1/(k_{\\mathrm{B}} T)$, and ensure every mathematical symbol is written in LaTeX.",
            "solution": "The problem statement is validated and found to be scientifically grounded, well-posed, and objective. It presents a standard problem in statistical mechanics, free of any factual unsoundness, ambiguity, or missing information. The premises and definitions are consistent with fundamental principles of classical statistical mechanics and multiscale modeling. We may, therefore, proceed with the solution.\n\nLet the inverse thermal energy be defined as $\\beta = \\frac{1}{k_{\\mathrm{B}} T}$, where $k_{\\mathrm{B}}$ is the Boltzmann constant and $T$ is the temperature. The problem considers a one-dimensional degree of freedom $x$ with a harmonic potential energy function $U(x;K) = \\frac{1}{2} K x^{2}$, where $K$ is the spring constant. We are asked to perform three tasks leading to a final expression for the Helmholtz free energy difference $\\Delta F$ between two states, $\\mathrm{A}$ and $\\mathrm{B}$, characterized by spring constants $K_{\\mathrmA}$ and $K_{\\mathrmB}$, respectively.\n\nFirst, we derive the configurational partition function $Z_{\\text{conf}}(K)$ for a generic spring constant $K$. The configurational partition function for a one-dimensional system is given by the integral of the Boltzmann factor over all possible configurations of the coordinate $x$:\n$$\nZ_{\\text{conf}}(K) = \\int_{-\\infty}^{+\\infty} \\exp(-\\beta U(x;K)) dx\n$$\nSubstituting the given potential energy expression, we have:\n$$\nZ_{\\text{conf}}(K) = \\int_{-\\infty}^{+\\infty} \\exp\\left(-\\beta \\frac{1}{2} K x^{2}\\right) dx\n$$\nThis is a standard Gaussian integral of the form $\\int_{-\\infty}^{+\\infty} \\exp(-ax^{2}) dx = \\sqrt{\\frac{\\pi}{a}}$. In our case, the constant $a$ is $\\frac{\\beta K}{2}$. Therefore, the integral evaluates to:\n$$\nZ_{\\text{conf}}(K) = \\sqrt{\\frac{\\pi}{\\frac{\\beta K}{2}}} = \\sqrt{\\frac{2\\pi}{\\beta K}}\n$$\nThis is the expression for the configurational partition function for a harmonic oscillator with spring constant $K$.\n\nSecond, we use this result to derive the Helmholtz free energy difference $\\Delta F = F_{\\mathrm{B}} - F_{\\mathrmA}$. The Helmholtz free energy $F$ is related to the partition function $Z$ by $F = -k_{\\mathrm{B}}T \\ln Z = -\\frac{1}{\\beta} \\ln Z$. Focusing on the configurational contribution, the free energy for a state with spring constant $K$ is:\n$$\nF(K) = -\\frac{1}{\\beta} \\ln(Z_{\\text{conf}}(K))\n$$\nSubstituting our expression for $Z_{\\text{conf}}(K)$:\n$$\nF(K) = -\\frac{1}{\\beta} \\ln\\left(\\sqrt{\\frac{2\\pi}{\\beta K}}\\right) = -\\frac{1}{2\\beta} \\ln\\left(\\frac{2\\pi}{\\beta K}\\right) = \\frac{1}{2\\beta} \\ln\\left(\\frac{\\beta K}{2\\pi}\\right)\n$$\nNow, we can write the free energies for states $\\mathrm{A}$ ($K=K_{\\mathrmA}$) and $\\mathrm{B}$ ($K=K_{\\mathrmB}}$):\n$$\nF_{\\mathrm{A}} = F(K_{\\mathrm{A}}) = \\frac{1}{2\\beta} \\ln\\left(\\frac{\\beta K_{\\mathrm{A}}}{2\\pi}\\right)\n$$\n$$\nF_{\\mathrm{B}} = F(K_{\\mathrm{B}}) = \\frac{1}{2\\beta} \\ln\\left(\\frac{\\beta K_{\\mathrm{B}}}{2\\pi}\\right)\n$$\nThe free energy difference $\\Delta F$ is then:\n$$\n\\Delta F = F_{\\mathrm{B}} - F_{\\mathrm{A}} = \\frac{1}{2\\beta} \\ln\\left(\\frac{\\beta K_{\\mathrm{B}}}{2\\pi}\\right) - \\frac{1}{2\\beta} \\ln\\left(\\frac{\\beta K_{\\mathrm{A}}}{2\\pi}\\right)\n$$\nUsing the property of logarithms, $\\ln(a) - \\ln(b) = \\ln(a/b)$:\n$$\n\\Delta F = \\frac{1}{2\\beta} \\left[ \\ln\\left(\\frac{\\beta K_{\\mathrm{B}}}{2\\pi}\\right) - \\ln\\left(\\frac{\\beta K_{\\mathrm{A}}}{2\\pi}\\right) \\right] = \\frac{1}{2\\beta} \\ln\\left(\\frac{\\frac{\\beta K_{\\mathrm{B}}}{2\\pi}}{\\frac{\\beta K_{\\mathrm{A}}}{2\\pi}}\\right) = \\frac{1}{2\\beta} \\ln\\left(\\frac{K_{\\mathrm{B}}}{K_{\\mathrm{A}}}\\right)\n$$\nSubstituting $\\beta = 1/(k_{\\mathrm{B}} T)$, we arrive at the exact analytical expression for the free energy difference:\n$$\n\\Delta F = \\frac{1}{2} k_{\\mathrm{B}} T \\ln\\left(\\frac{K_{\\mathrm{B}}}{K_{\\mathrm{A}}}\\right)\n$$\n\nThird, we derive the same result using the Free Energy Perturbation (FEP) formalism. The FEP formula expresses the free energy difference as an ensemble average over one of the states (here, state $\\mathrm{A}$):\n$$\n\\Delta F = -\\frac{1}{\\beta} \\ln \\langle \\exp(-\\beta \\Delta U) \\rangle_{\\mathrm{A}}\n$$\nwhere $\\Delta U = U_{\\mathrm{B}}(x) - U_{\\mathrm{A}}(x)$ is the difference in potential energy between the two states, and $\\langle \\dots \\rangle_{\\mathrm{A}}$ denotes a canonical ensemble average taken over the configurations of state $\\mathrm{A}$. The potential energies are $U_{\\mathrm{A}}(x) = \\frac{1}{2} K_{\\mathrm{A}} x^2$ and $U_{\\mathrm{B}}(x) = \\frac{1}{2} K_{\\mathrm{B}} x^2$, so the difference is:\n$$\n\\Delta U(x) = \\frac{1}{2} K_{\\mathrm{B}} x^2 - \\frac{1}{2} K_{\\mathrm{A}} x^2 = \\frac{1}{2}(K_{\\mathrm{B}} - K_{\\mathrm{A}})x^2\n$$\nThe ensemble average is defined as:\n$$\n\\langle \\exp(-\\beta \\Delta U) \\rangle_{\\mathrm{A}} = \\frac{\\int_{-\\infty}^{+\\infty} \\exp(-\\beta \\Delta U(x)) \\exp(-\\beta U_{\\mathrm{A}}(x)) dx}{\\int_{-\\infty}^{+\\infty} \\exp(-\\beta U_{\\mathrm{A}}(x)) dx}\n$$\nThe denominator is simply the partition function of state $\\mathrm{A}$, $Z_{\\text{conf}}(K_{\\mathrm{A}})$. The numerator is:\n$$\n\\int_{-\\infty}^{+\\infty} \\exp\\left(-\\beta \\frac{1}{2}(K_{\\mathrm{B}} - K_{\\mathrm{A}})x^2\\right) \\exp\\left(-\\beta \\frac{1}{2}K_{\\mathrm{A}}x^2\\right) dx = \\int_{-\\infty}^{+\\infty} \\exp\\left(-\\beta \\frac{1}{2}[(K_{\\mathrm{B}} - K_{\\mathrm{A}}) + K_{\\mathrm{A}}] x^2\\right) dx\n$$\nThis simplifies to:\n$$\n\\int_{-\\infty}^{+\\infty} \\exp\\left(-\\beta \\frac{1}{2} K_{\\mathrm{B}} x^2\\right) dx = Z_{\\text{conf}}(K_{\\mathrm{B}})\n$$\nThus, the ensemble average is the ratio of the partition functions:\n$$\n\\langle \\exp(-\\beta \\Delta U) \\rangle_{\\mathrm{A}} = \\frac{Z_{\\text{conf}}(K_{\\mathrm{B}})}{Z_{\\text{conf}}(K_{\\mathrm{A}})}\n$$\nUsing our previously derived expression for $Z_{\\text{conf}}(K)$:\n$$\n\\langle \\exp(-\\beta \\Delta U) \\rangle_{\\mathrm{A}} = \\frac{\\sqrt{\\frac{2\\pi}{\\beta K_{\\mathrm{B}}}}}{\\sqrt{\\frac{2\\pi}{\\beta K_{\\mathrm{A}}}}} = \\sqrt{\\frac{K_{\\mathrm{A}}}{K_{\\mathrm{B}}}} = \\left(\\frac{K_{\\mathrm{A}}}{K_{\\mathrm{B}}}\\right)^{1/2}\n$$\nFinally, we substitute this result back into the FEP formula for $\\Delta F$:\n$$\n\\Delta F = -\\frac{1}{\\beta} \\ln\\left[ \\left(\\frac{K_{\\mathrm{A}}}{K_{\\mathrm{B}}}\\right)^{1/2} \\right] = -\\frac{1}{2\\beta} \\ln\\left(\\frac{K_{\\mathrm{A}}}{K_{\\mathrm{B}}}\\right) = \\frac{1}{2\\beta} \\ln\\left(\\left(\\frac{K_{\\mathrm{A}}}{K_{\\mathrm{B}}}\\right)^{-1}\\right) = \\frac{1}{2\\beta} \\ln\\left(\\frac{K_{\\mathrm{B}}}{K_{\\mathrm{A}}}\\right)\n$$\nSubstituting $\\beta = 1/(k_{\\mathrm{B}} T)$ again gives:\n$$\n\\Delta F = \\frac{1}{2} k_{\\mathrm{B}} T \\ln\\left(\\frac{K_{\\mathrm{B}}}{K_{\\mathrm{A}}}\\right)\n$$\nBoth methods yield the same analytical result, confirming the consistency of the statistical mechanical framework. This is the required final closed-form expression.",
            "answer": "$$\n\\boxed{\\frac{1}{2} k_{\\mathrm{B}} T \\ln\\left(\\frac{K_{\\mathrm{B}}}{K_{\\mathrm{A}}}\\right)}\n$$"
        },
        {
            "introduction": "Real-world free energy calculations rely on data from molecular simulations, where successive measurements are inherently correlated. Ignoring this correlation leads to a dramatic underestimation of statistical error, yielding overly confident but incorrect results. This practice addresses this critical issue by introducing the concept of statistical inefficiency, a measure that quantifies the effect of time correlation on the variance of an observable's mean . By deriving this quantity and applying it to a model system, you will learn the essential technique for computing the effective number of independent samples and obtaining reliable uncertainty estimates.",
            "id": "3762077",
            "problem": "Consider a single thermodynamic state in a molecular simulation whose time series observable $x_{i}$ (such as a reduced work or energy contributing to a free energy estimator like the Multistate Bennett Acceptance Ratio (MBAR)) is sampled at discrete times $t_{i} = i \\Delta t$ for $i = 1, 2, \\dots, N$. Assume $x_{i}$ is a weakly stationary stochastic process with mean $\\mu = \\langle x_{i} \\rangle$, variance $\\sigma^{2} = \\langle (x_{i} - \\mu)^{2} \\rangle$, and normalized autocorrelation function $\\rho(t)$ defined by $\\rho(t) = \\langle (x_{i} - \\mu)(x_{i+t} - \\mu) \\rangle / \\sigma^{2}$ for integer lags $t \\geq 0$. Starting from the definitions of covariance and the variance of the sample mean as $\\operatorname{Var}(\\bar{x}) = \\operatorname{Var} \\left( \\frac{1}{N} \\sum_{i=1}^{N} x_{i} \\right)$, derive the statistical inefficiency $g$ as a function of the normalized autocorrelation $\\rho(t)$ and explain how it rescales the variance of the sample mean for correlated data. Using this $g$, define the effective sample size $N_{\\mathrm{eff}}$ appropriate for the asymptotic uncertainty scaling of free energy estimators that depend on averages of correlated samples.\n\nNow specialize to the case in which the normalized autocorrelation decays exponentially with lag, $\\rho(t) = \\exp\\!\\left(- t \\Delta t / \\tau_{c} \\right)$ for integer $t \\geq 1$, where $\\Delta t$ is the sampling interval and $\\tau_{c}$ is the correlation time. Consider a simulation that produced $N = 2.50 \\times 10^{5}$ samples at $\\Delta t = 2\\,\\mathrm{fs}$ with a correlation time $\\tau_{c} = 50\\,\\mathrm{fs}$. Using your derived $g$, compute the effective sample size $N_{\\mathrm{eff}}$ for this dataset.\n\nRound your final numerical answer to four significant figures. Express the final answer as a pure number with no units.",
            "solution": "The problem asks for the derivation of the statistical inefficiency $g$, its role in scaling the variance of the sample mean, the definition of the effective sample size $N_{\\mathrm{eff}}$, and the calculation of $N_{\\mathrm{eff}}$ for a specific case of exponentially decaying autocorrelation.\n\nThe starting point is the definition of the variance of the sample mean, $\\bar{x} = \\frac{1}{N} \\sum_{i=1}^{N} x_i$.\n$$\n\\operatorname{Var}(\\bar{x}) = \\operatorname{Var}\\left(\\frac{1}{N} \\sum_{i=1}^{N} x_i\\right)\n$$\nUsing the property of variance, $\\operatorname{Var}(cY) = c^2 \\operatorname{Var}(Y)$ for a constant $c$, we have:\n$$\n\\operatorname{Var}(\\bar{x}) = \\frac{1}{N^2} \\operatorname{Var}\\left(\\sum_{i=1}^{N} x_i\\right)\n$$\nThe variance of a sum of random variables is the sum of all elements in their covariance matrix: $\\operatorname{Var}\\left(\\sum_i Y_i\\right) = \\sum_i \\sum_j \\operatorname{Cov}(Y_i, Y_j)$. Applying this gives:\n$$\n\\operatorname{Var}(\\bar{x}) = \\frac{1}{N^2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\operatorname{Cov}(x_i, x_j)\n$$\nFor a weakly stationary process, the covariance $\\operatorname{Cov}(x_i, x_j) = \\langle (x_i - \\mu)(x_j - \\mu) \\rangle$ depends only on the time lag $t = |i-j|$ between the samples. It can be expressed in terms of the variance $\\sigma^2$ and the normalized autocorrelation function $\\rho(t)$ as:\n$$\n\\operatorname{Cov}(x_i, x_j) = \\sigma^2 \\rho(|i-j|)\n$$\nSubstituting this into the expression for $\\operatorname{Var}(\\bar{x})$:\n$$\n\\operatorname{Var}(\\bar{x}) = \\frac{\\sigma^2}{N^2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\rho(|i-j|)\n$$\nThe double summation can be evaluated by grouping terms with the same lag $t = |i-j|$. The lag $t$ ranges from $0$ to $N-1$. For a given lag $t > 0$, there are $2(N-t)$ pairs of indices $(i, j)$ in the sum for which $|i-j|=t$. For the lag $t=0$, there are $N$ pairs where $i=j$. The sum can thus be rewritten as:\n$$\n\\sum_{i=1}^{N} \\sum_{j=1}^{N} \\rho(|i-j|) = N\\rho(0) + \\sum_{t=1}^{N-1} 2(N-t)\\rho(t)\n$$\nBy definition, $\\rho(0)=1$. Therefore:\n$$\n\\sum_{i=1}^{N} \\sum_{j=1}^{N} \\rho(|i-j|) = N + 2 \\sum_{t=1}^{N-1} (N-t)\\rho(t)\n$$\nSubstituting this back into the equation for the variance of the mean:\n$$\n\\operatorname{Var}(\\bar{x}) = \\frac{\\sigma^2}{N^2} \\left[ N + 2 \\sum_{t=1}^{N-1} (N-t)\\rho(t) \\right] = \\frac{\\sigma^2}{N} \\left[ 1 + 2 \\sum_{t=1}^{N-1} \\left(1-\\frac{t}{N}\\right)\\rho(t) \\right]\n$$\nThis is an exact expression for a finite number of samples $N$. In the asymptotic limit, where the number of samples $N$ is much larger than the correlation time of the process (i.e., $\\rho(t) \\approx 0$ for $t$ much smaller than $N$), we can make two approximations. First, for lags $t$ where $\\rho(t)$ is non-negligible, $t/N \\ll 1$, so the term $(1-t/N) \\approx 1$. Second, since $\\rho(t)$ decays to zero, the upper limit of summation can be extended from $N-1$ to $\\infty$. This yields the asymptotic formula:\n$$\n\\operatorname{Var}(\\bar{x}) \\approx \\frac{\\sigma^2}{N} \\left[ 1 + 2 \\sum_{t=1}^{\\infty} \\rho(t) \\right]\n$$\nThe statistical inefficiency, $g$, is the factor by which the variance of the sample mean of correlated data is larger than that of uncorrelated (independent) data. For $N$ independent samples, the variance of the mean is $\\sigma^2/N$. We define $g$ such that for correlated data:\n$$\n\\operatorname{Var}(\\bar{x}) = g \\frac{\\sigma^2}{N}\n$$\nBy comparing this definition with the derived asymptotic expression, we identify the statistical inefficiency as:\n$$\ng = 1 + 2 \\sum_{t=1}^{\\infty} \\rho(t)\n$$\nThis expression demonstrates how $g$ rescales the variance. For uncorrelated data, $\\rho(t)=0$ for $t \\ge 1$, so $g=1$. For data with positive correlations, $\\rho(t)>0$, the sum is positive, and thus $g>1$, indicating an increased variance (i.e., higher statistical uncertainty) in the estimated mean.\n\nThe effective sample size, $N_{\\mathrm{eff}}$, is defined as the number of independent samples that would give the same variance in the mean as the $N$ correlated samples. The variance for $N_{\\mathrm{eff}}$ independent samples is $\\sigma^2/N_{\\mathrm{eff}}$. Equating this to the variance for the correlated samples:\n$$\n\\frac{\\sigma^2}{N_{\\mathrm{eff}}} = g \\frac{\\sigma^2}{N}\n$$\nSolving for $N_{\\mathrm{eff}}$ yields:\n$$\nN_{\\mathrm{eff}} = \\frac{N}{g}\n$$\n\nNow, we specialize to the case where the autocorrelation is exponential: $\\rho(t) = \\exp(- t \\Delta t / \\tau_c)$ for $t \\ge 1$. We compute $g$ using this model:\n$$\ng = 1 + 2 \\sum_{t=1}^{\\infty} \\exp\\left(-\\frac{t \\Delta t}{\\tau_c}\\right)\n$$\nThe summation is a geometric series with first term and ratio both equal to $r = \\exp(-\\Delta t / \\tau_c)$. The sum of this series is $\\sum_{t=1}^{\\infty} r^t = \\frac{r}{1-r}$.\nSubstituting this into the expression for $g$:\n$$\ng = 1 + 2 \\frac{r}{1-r} = \\frac{(1-r) + 2r}{1-r} = \\frac{1+r}{1-r}\n$$\nSubstituting $r = \\exp(-\\Delta t / \\tau_c)$ back gives the formula for $g$:\n$$\ng = \\frac{1 + \\exp(-\\Delta t / \\tau_c)}{1 - \\exp(-\\Delta t / \\tau_c)}\n$$\nWe are given the following values: $N = 2.50 \\times 10^5$, $\\Delta t = 2\\,\\mathrm{fs}$, and $\\tau_c = 50\\,\\mathrm{fs}$. First, we compute the ratio in the exponent:\n$$\n\\frac{\\Delta t}{\\tau_c} = \\frac{2\\,\\mathrm{fs}}{50\\,\\mathrm{fs}} = 0.04\n$$\nNow, we can calculate the numerical value of $g$:\n$$\ng = \\frac{1 + \\exp(-0.04)}{1 - \\exp(-0.04)} \\approx \\frac{1 + 0.96078944}{1 - 0.96078944} = \\frac{1.96078944}{0.03921056} \\approx 50.006665\n$$\nFinally, we compute the effective sample size $N_{\\mathrm{eff}}$:\n$$\nN_{\\mathrm{eff}} = \\frac{N}{g} = \\frac{2.50 \\times 10^5}{50.006665} \\approx 4999.3335\n$$\nRounding the result to four significant figures as requested gives $4999$.",
            "answer": "$$\\boxed{4999}$$"
        },
        {
            "introduction": "Thermodynamic Integration (TI) is a powerful method for computing free energy, but how can we trust the results? One of the most important practical skills is diagnosing the convergence of the calculation. This exercise provides a hands-on numerical demonstration of hysteresis, a common symptom of insufficient sampling where the calculated free energy depends on the direction of the alchemical path . By implementing a program to quantify the area of the hysteresis loop, you will develop a practical tool for assessing the reversibility and reliability of your TI simulations.",
            "id": "5252756",
            "problem": "You are to implement a numerical demonstration of path-dependent hysteresis in Thermodynamic Integration (TI) for a simple solute decoupling model. In TI, the Helmholtz free energy difference along a coupling parameter $\\lambda \\in [0,1]$ is obtained by integrating the ensemble average of the derivative of the potential energy with respect to $\\lambda$. In practice, forward sampling (increasing $\\lambda$) and backward sampling (decreasing $\\lambda$) can yield distinct integrands due to incomplete equilibration, leading to hysteresis. You will quantify this hysteresis as the area between the cumulative free energy curves obtained from forward and backward integrands.\n\nStarting from the fundamental definition of free energy in the canonical ensemble, the Helmholtz free energy $F(\\lambda)$ is $F(\\lambda) = -k_{\\mathrm{B}} T \\ln Z(\\lambda)$ with the configurational partition function $Z(\\lambda) = \\int \\exp\\left(-\\beta U(x;\\lambda)\\right) \\,\\mathrm{d}x$, where $U(x;\\lambda)$ is the potential energy, $k_{\\mathrm{B}}$ is Boltzmann's constant, $T$ is temperature, and $\\beta = 1/(k_{\\mathrm{B}} T)$. A well-tested relation connects the derivative of free energy to the ensemble average of the derivative of the potential energy: $\\frac{\\mathrm{d}F}{\\mathrm{d}\\lambda} = \\left\\langle \\frac{\\partial U}{\\partial \\lambda} \\right\\rangle_{\\lambda}$. For a simple solute decoupling pathway, model the ensemble average $\\left\\langle \\frac{\\partial U}{\\partial \\lambda} \\right\\rangle_{\\lambda}$ by a smooth baseline function $f(\\lambda)$ that is modified by path-specific bias terms to emulate incomplete equilibration.\n\nYour program must:\n- For each test case, construct forward and backward integrand arrays on a specified $\\lambda$-grid, compute the forward cumulative free energy curve $G_{\\mathrm{f}}(\\lambda)$ and the backward cumulative free energy curve $G_{\\mathrm{b}}(\\lambda)$ using the composite trapezoidal rule, and then compute the hysteresis loop area $\\mathcal{A}$ defined as $\\mathcal{A} = \\int_{0}^{1} \\left| G_{\\mathrm{f}}(\\lambda) - G_{\\mathrm{b}}(\\lambda) \\right| \\,\\mathrm{d}\\lambda$, using the same composite trapezoidal rule on the provided grid.\n- Express $\\mathcal{A}$ in kilojoules per mole ($\\mathrm{kJ/mol}$), rounded to $6$ decimal places.\n- Produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets (e.g., $\\left[\\mathrm{result}_1,\\mathrm{result}_2,\\mathrm{result}_3\\right]$).\n\nDefinitions and constraints:\n- Thermodynamic Integration (TI) is defined by the relation $\\Delta G = \\int_{0}^{1} \\left\\langle \\frac{\\partial U}{\\partial \\lambda} \\right\\rangle_{\\lambda} \\,\\mathrm{d}\\lambda$.\n- The hysteresis loop area $\\mathcal{A}$ must be computed between the cumulative free energy curves $G_{\\mathrm{f}}(\\lambda) = \\int_{0}^{\\lambda} \\left\\langle \\frac{\\partial U}{\\partial \\lambda} \\right\\rangle_{\\lambda'}^{\\mathrm{forward}} \\,\\mathrm{d}\\lambda'$ and $G_{\\mathrm{b}}(\\lambda) = \\int_{0}^{\\lambda} \\left\\langle \\frac{\\partial U}{\\partial \\lambda} \\right\\rangle_{\\lambda'}^{\\mathrm{backward}} \\,\\mathrm{d}\\lambda'$, both integrated from $\\lambda = 0$ to $\\lambda$ on the same $\\lambda$-grid for comparability.\n\nNumerical method:\n- Use the composite trapezoidal rule for all integrals. For a non-uniform grid $\\{\\lambda_i\\}_{i=0}^{N}$ with $\\lambda_0 = 0$ and $\\lambda_N = 1$, and function samples $\\{y_i\\}_{i=0}^{N}$, the cumulative integral at grid index $j$ is $I_j = \\sum_{i=0}^{j-1} \\frac{1}{2} (\\lambda_{i+1}-\\lambda_i) (y_{i+1}+y_i)$, and the integral over $\\left[0,1\\right]$ is $I = \\sum_{i=0}^{N-1} \\frac{1}{2} (\\lambda_{i+1}-\\lambda_i) (y_{i+1}+y_i)$.\n\nTest suite:\nImplement the following $4$ test cases. For each, the forward and backward integrands are constructed by a shared baseline $f(\\lambda)$ plus path-specific biases in $\\mathrm{kJ/mol}$.\n\n- Test case $1$ (uniform grid, symmetric biases; happy path):\n  - Grid: $\\lambda_i = i/20$ for $i \\in \\{0,1,\\ldots,20\\}$ (i.e., $21$ points from $0$ to $1$).\n  - Baseline: $f(\\lambda) = 20 (1-\\lambda)^2$.\n  - Forward bias: $b_{\\mathrm{f}}(\\lambda) = 0.5 \\lambda$.\n  - Backward bias: $b_{\\mathrm{b}}(\\lambda) = -0.5 \\lambda$.\n  - Forward integrand: $\\left\\langle \\frac{\\partial U}{\\partial \\lambda} \\right\\rangle^{\\mathrm{forward}} = f(\\lambda) + b_{\\mathrm{f}}(\\lambda)$.\n  - Backward integrand: $\\left\\langle \\frac{\\partial U}{\\partial \\lambda} \\right\\rangle^{\\mathrm{backward}} = f(\\lambda) + b_{\\mathrm{b}}(\\lambda)$.\n\n- Test case $2$ (non-uniform grid, asymmetric biases; stress non-uniform spacing):\n  - Grid: $\\lambda \\in \\{0, 0.01, 0.05, 0.1, 0.2, 0.35, 0.5, 0.7, 0.85, 0.95, 1.0\\}$.\n  - Baseline: $f(\\lambda) = \\frac{15}{1 + 5 \\lambda}$.\n  - Forward bias: $b_{\\mathrm{f}}(\\lambda) = 0.8 \\lambda (1-\\lambda)$.\n  - Backward bias: $b_{\\mathrm{b}}(\\lambda) = -0.8 \\lambda (1-\\lambda) + 0.3 \\lambda$.\n  - Forward integrand: $\\left\\langle \\frac{\\partial U}{\\partial \\lambda} \\right\\rangle^{\\mathrm{forward}} = f(\\lambda) + b_{\\mathrm{f}}(\\lambda)$.\n  - Backward integrand: $\\left\\langle \\frac{\\partial U}{\\partial \\lambda} \\right\\rangle^{\\mathrm{backward}} = f(\\lambda) + b_{\\mathrm{b}}(\\lambda)$.\n\n- Test case $3$ (uniform grid, no bias; boundary condition of perfect reversibility):\n  - Grid: $\\lambda_i = i/50$ for $i \\in \\{0,1,\\ldots,50\\}$ (i.e., $51$ points from $0$ to $1$).\n  - Baseline: $f(\\lambda) = 10 e^{-\\lambda}$.\n  - Forward bias: $b_{\\mathrm{f}}(\\lambda) = 0$.\n  - Backward bias: $b_{\\mathrm{b}}(\\lambda) = 0$.\n  - Forward integrand: $\\left\\langle \\frac{\\partial U}{\\partial \\lambda} \\right\\rangle^{\\mathrm{forward}} = f(\\lambda)$.\n  - Backward integrand: $\\left\\langle \\frac{\\partial U}{\\partial \\lambda} \\right\\rangle^{\\mathrm{backward}} = f(\\lambda)$.\n\n- Test case $4$ (nonlinear baseline with near-endpoint singularity regularized; edge case):\n  - Grid: $\\lambda_i = i/40$ for $i \\in \\{0,1,\\ldots,40\\}$ (i.e., $41$ points from $0$ to $1$).\n  - Baseline: $f(\\lambda) = \\frac{5}{\\sqrt{\\lambda + 10^{-6}}}$.\n  - Forward bias: $b_{\\mathrm{f}}(\\lambda) = \\frac{0.2}{1 + 10 \\lambda}$.\n  - Backward bias: $b_{\\mathrm{b}}(\\lambda) = -\\frac{0.2}{1 + 10 \\lambda} + 0.1 \\lambda$.\n  - Forward integrand: $\\left\\langle \\frac{\\partial U}{\\partial \\lambda} \\right\\rangle^{\\mathrm{forward}} = f(\\lambda) + b_{\\mathrm{f}}(\\lambda)$.\n  - Backward integrand: $\\left\\langle \\frac{\\partial U}{\\partial \\lambda} \\right\\rangle^{\\mathrm{backward}} = f(\\lambda) + b_{\\mathrm{b}}(\\lambda)$.\n\nRequirements for the final output:\n- Compute $\\mathcal{A}$ for each test case using the definitions above.\n- Return the list of the $4$ hysteresis loop areas, in $\\mathrm{kJ/mol}$, rounded to $6$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, e.g., $\\left[\\mathrm{a}_1,\\mathrm{a}_2,\\mathrm{a}_3,\\mathrm{a}_4\\right]$.",
            "solution": "The problem statement has been validated and is deemed sound. It is scientifically grounded in the principles of statistical mechanics and thermodynamic integration (TI), a standard technique in computational chemistry. The problem is well-posed, providing all necessary mathematical functions, parameters, and a clearly defined numerical method and objective. The use of simplified analytical functions to model the ensemble average of the potential energy derivative, $\\left\\langle \\frac{\\partial U}{\\partial \\lambda} \\right\\rangle_{\\lambda}$, and to introduce path-dependent biases is a valid and instructive approach for demonstrating the concept of hysteresis due to non-ergodicity or incomplete equilibration in practical free energy calculations. The problem is objective, self-contained, and computationally tractable.\n\nThe core task is to quantify the hysteresis between forward and backward free energy calculations. The free energy difference, $\\Delta F$, between two states characterized by a coupling parameter $\\lambda$ (here, $\\lambda=0$ and $\\lambda=1$) is given by the TI formula:\n$$ \\Delta F = \\int_{0}^{1} \\left\\langle \\frac{\\partial U}{\\partial \\lambda} \\right\\rangle_{\\lambda} \\mathrm{d}\\lambda $$\nwhere $\\left\\langle \\frac{\\partial U}{\\partial \\lambda} \\right\\rangle_{\\lambda}$ is the ensemble average of the derivative of the potential energy $U$ with respect to $\\lambda$, evaluated at a specific $\\lambda$. In an ideal, reversible process, the integrand $\\left\\langle \\frac{\\partial U}{\\partial \\lambda} \\right\\rangle_{\\lambda}$ is unique. However, in finite-time simulations, sampling may be incomplete, causing the calculated average to depend on the history of the simulation. This leads to different integrand curves when $\\lambda$ is increased from $0$ to $1$ (forward path) versus when it is decreased from $1$ to $0$ (backward path).\n\nThe problem defines models for these path-dependent integrands, denoted $\\left\\langle \\frac{\\partial U}{\\partial \\lambda} \\right\\rangle^{\\mathrm{forward}}$ and $\\left\\langle \\frac{\\partial U}{\\partial \\lambda} \\right\\rangle^{\\mathrm{backward}}$. To quantify the discrepancy, we first compute the cumulative free energy change as a function of $\\lambda$ for both paths. For the forward path, this is:\n$$ G_{\\mathrm{f}}(\\lambda) = \\int_{0}^{\\lambda} \\left\\langle \\frac{\\partial U}{\\partial \\lambda'} \\right\\rangle_{\\lambda'}^{\\mathrm{forward}} \\mathrm{d}\\lambda' $$\nAnd for the backward path, the corresponding quantity is defined as an integral starting from $\\lambda=0$ over the backward integrand for consistent comparison:\n$$ G_{\\mathrm{b}}(\\lambda) = \\int_{0}^{\\lambda} \\left\\langle \\frac{\\partial U}{\\partial \\lambda'} \\right\\rangle_{\\lambda'}^{\\mathrm{backward}} \\mathrm{d}\\lambda' $$\nThe total hysteresis $\\mathcal{A}$ is then defined as the area enclosed between these two cumulative curves over the interval $\\lambda \\in [0, 1]$:\n$$ \\mathcal{A} = \\int_{0}^{1} \\left| G_{\\mathrm{f}}(\\lambda) - G_{\\mathrm{b}}(\\lambda) \\right| \\,\\mathrm{d}\\lambda $$\nAll calculations are to be performed numerically on a discrete grid of $\\lambda$ values using the composite trapezoidal rule.\n\nThe algorithm is as follows:\n1.  For each test case, define the $\\lambda$-grid, $\\{\\lambda_i\\}_{i=0}^{N}$, where $\\lambda_0=0$ and $\\lambda_N=1$.\n2.  Evaluate the forward integrand, $y_i^{\\mathrm{f}} = \\left\\langle \\frac{\\partial U}{\\partial \\lambda} \\right\\rangle^{\\mathrm{forward}}_{\\lambda=\\lambda_i}$, and the backward integrand, $y_i^{\\mathrm{b}} = \\left\\langle \\frac{\\partial U}{\\partial \\lambda} \\right\\rangle^{\\mathrm{backward}}_{\\lambda=\\lambda_i}$, at each grid point $\\lambda_i$.\n3.  Compute the cumulative free energy curves, $G_{\\mathrm{f}}$ and $G_{\\mathrm{b}}$, at each grid point $\\lambda_j$. This is done by applying the composite trapezoidal rule cumulatively. For a generic integrand $y$, the cumulative integral $I(\\lambda_j)$ is:\n    $$ I(\\lambda_j) = \\sum_{i=0}^{j-1} \\frac{1}{2} (\\lambda_{i+1} - \\lambda_i) (y_{i+1} + y_i) \\quad \\text{for } j > 0, \\text{ and } I(\\lambda_0) = 0 $$\n    This yields two arrays of values, $\\{G_{\\mathrm{f}}(\\lambda_i)\\}_{i=0}^{N}$ and $\\{G_{\\mathrm{b}}(\\lambda_i)\\}_{i=0}^{N}$.\n4.  Calculate the pointwise absolute difference between the cumulative curves: $\\Delta G_i = |G_{\\mathrm{f}}(\\lambda_i) - G_{\\mathrm{b}}(\\lambda_i)|$.\n5.  Compute the final hysteresis area, $\\mathcal{A}$, by integrating the absolute difference array, $\\{\\Delta G_i\\}_{i=0}^{N}$, over the $\\lambda$-grid using the composite trapezoidal rule:\n    $$ \\mathcal{A} = \\sum_{i=0}^{N-1} \\frac{1}{2} (\\lambda_{i+1} - \\lambda_i) (\\Delta G_{i+1} + \\Delta G_i) $$\n6.  The final result for each test case is rounded to $6$ decimal places and reported in units of $\\mathrm{kJ/mol}$.\n\nThis procedure will be systematically applied to the four provided test cases. For Test Case $3$, where the forward and backward biases are both zero, the integrands are identical. Consequently, $G_{\\mathrm{f}}(\\lambda) = G_{\\mathrm{b}}(\\lambda)$ for all $\\lambda$, leading to an expected hysteresis area $\\mathcal{A}$ of exactly $0.0$. This serves as a control test for the implementation.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.integrate import cumulative_trapezoid\n\ndef solve():\n    \"\"\"\n    Solves the thermodynamic integration hysteresis problem for all test cases.\n    \"\"\"\n\n    def calculate_hysteresis_area(grid, baseline_func, fwd_bias_func, bwd_bias_func):\n        \"\"\"\n        Calculates the hysteresis loop area for a given set of functions and grid.\n\n        Args:\n            grid (np.ndarray): The array of lambda values.\n            baseline_func (callable): The baseline integrand function f(lambda).\n            fwd_bias_func (callable): The forward bias function b_f(lambda).\n            bwd_bias_func (callable): The backward bias function b_b(lambda).\n\n        Returns:\n            float: The calculated hysteresis loop area, A.\n        \"\"\"\n        # Step 1: Evaluate integrands on the grid\n        # Forward integrand\n        integrand_fwd = baseline_func(grid) + fwd_bias_func(grid)\n        # Backward integrand\n        integrand_bwd = baseline_func(grid) + bwd_bias_func(grid)\n\n        # Step 2: Compute cumulative free energy curves using composite trapezoidal rule\n        # The 'initial=0' argument ensures the output array has the same length\n        # as the input grid, with the integral at the first point being 0.\n        G_f = cumulative_trapezoid(integrand_fwd, grid, initial=0)\n        G_b = cumulative_trapezoid(integrand_bwd, grid, initial=0)\n\n        # Step 3: Calculate the absolute difference of the cumulative curves\n        abs_diff_G = np.abs(G_f - G_b)\n\n        # Step 4: Compute the hysteresis area by integrating the absolute difference\n        # using the composite trapezoidal rule (np.trapz).\n        hysteresis_area = np.trapz(abs_diff_G, grid)\n\n        return hysteresis_area\n\n    # Define the test cases from the problem statement\n    test_cases = [\n        {\n            \"name\": \"Test case 1\",\n            \"grid\": np.linspace(0, 1, 21),\n            \"baseline\": lambda l: 20 * (1 - l)**2,\n            \"fwd_bias\": lambda l: 0.5 * l,\n            \"bwd_bias\": lambda l: -0.5 * l,\n        },\n        {\n            \"name\": \"Test case 2\",\n            \"grid\": np.array([0, 0.01, 0.05, 0.1, 0.2, 0.35, 0.5, 0.7, 0.85, 0.95, 1.0]),\n            \"baseline\": lambda l: 15 / (1 + 5 * l),\n            \"fwd_bias\": lambda l: 0.8 * l * (1 - l),\n            \"bwd_bias\": lambda l: -0.8 * l * (1 - l) + 0.3 * l,\n        },\n        {\n            \"name\": \"Test case 3\",\n            \"grid\": np.linspace(0, 1, 51),\n            \"baseline\": lambda l: 10 * np.exp(-l),\n            \"fwd_bias\": lambda l: 0.0,\n            \"bwd_bias\": lambda l: 0.0,\n        },\n        {\n            \"name\": \"Test case 4\",\n            \"grid\": np.linspace(0, 1, 41),\n            \"baseline\": lambda l: 5 / np.sqrt(l + 1e-6),\n            \"fwd_bias\": lambda l: 0.2 / (1 + 10 * l),\n            \"bwd_bias\": lambda l: -0.2 / (1 + 10 * l) + 0.1 * l,\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        area = calculate_hysteresis_area(\n            case[\"grid\"],\n            case[\"baseline\"],\n            case[\"fwd_bias\"],\n            case[\"bwd_bias\"]\n        )\n        # Round the result to 6 decimal places as required\n        results.append(round(area, 6))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}