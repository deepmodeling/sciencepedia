{
    "hands_on_practices": [
        {
            "introduction": "Understanding how information from data updates our prior beliefs is the cornerstone of Bayesian inference. This first exercise provides a hands-on derivation of a posterior distribution in a scenario where an exact analytical solution is possible. By working through the case of a Gaussian likelihood with a conjugate Gamma prior , you will see precisely how the prior and the likelihood combine to form a posterior that neatly incorporates information from the observations. This concept of conjugacy is not only elegant but also provides a vital building block for understanding more complex models.",
            "id": "4215202",
            "problem": "In a pressurized water reactor core simulation, suppose a high-fidelity neutron transport model predicts a stable in-core detector count rate with known expected value $\\mu$ under a fixed operating condition. You collect $n$ independent detector readings $y_{1},\\dots,y_{n}$ from the same condition, and model the measurement and unresolved modeling noise as Gaussian with precision $\\tau = 1/\\sigma^{2}$, so that the likelihood for each observation satisfies $y_{i} \\mid \\tau \\sim \\mathcal{N}(\\mu,\\tau^{-1})$ and the observations are conditionally independent given $\\tau$. To reflect prior knowledge of the noise precision in this reactor configuration, you place a Gamma prior on $\\tau$ with shape-rate parameters $(a_{0}, b_{0})$, so that $p(\\tau) \\propto \\tau^{a_{0}-1} \\exp(-b_{0}\\tau)$ for $\\tau > 0$.\n\nStarting only from Bayes’ theorem and the standard definition of the Gaussian likelihood and Gamma prior density, derive the normalized posterior density $p(\\tau \\mid y_{1},\\dots,y_{n},\\mu)$ in closed form and demonstrate that the Gamma family is conjugate to the Gaussian likelihood with known mean.\n\nYour final answer must be a single closed-form analytic expression for the normalized posterior density $p(\\tau \\mid y_{1},\\dots,y_{n},\\mu)$, expressed symbolically in terms of $a_{0}$, $b_{0}$, $n$, $\\mu$, and $\\sum_{i=1}^{n} (y_{i}-\\mu)^{2}$. Do not substitute numerical values. No rounding is required. Do not include units in the final expression.",
            "solution": "The problem statement is evaluated as scientifically sound, well-posed, objective, and complete. It presents a standard, formalizable problem in Bayesian statistics applied to a plausible scenario in nuclear reactor simulation. No flaws are identified. We may therefore proceed with the derivation.\n\nThe objective is to derive the posterior probability density function (PDF) for the noise precision $\\tau$, denoted as $p(\\tau \\mid \\mathbf{y}, \\mu)$, where $\\mathbf{y}$ is the vector of observations $(y_{1}, \\dots, y_{n})$. The derivation begins with Bayes' theorem, which states that the posterior is proportional to the product of the likelihood and the prior:\n$$p(\\tau \\mid \\mathbf{y}, \\mu) \\propto p(\\mathbf{y} \\mid \\tau, \\mu) p(\\tau)$$\n\nFirst, we formulate the likelihood function, $p(\\mathbf{y} \\mid \\tau, \\mu)$. The problem states that the observations $y_{i}$ are conditionally independent given $\\tau$ and $\\mu$. Therefore, the joint likelihood of the entire dataset $\\mathbf{y}$ is the product of the individual likelihoods for each observation $y_{i}$:\n$$p(\\mathbf{y} \\mid \\tau, \\mu) = \\prod_{i=1}^{n} p(y_{i} \\mid \\tau, \\mu)$$\nEach observation $y_{i}$ is drawn from a Gaussian (Normal) distribution with a known mean $\\mu$ and precision $\\tau$, i.e., $y_{i} \\sim \\mathcal{N}(\\mu, \\tau^{-1})$. The PDF for a single observation is:\n$$p(y_{i} \\mid \\tau, \\mu) = \\frac{1}{\\sqrt{2\\pi(\\tau^{-1})}} \\exp\\left( -\\frac{(y_{i} - \\mu)^{2}}{2(\\tau^{-1})} \\right) = \\left(\\frac{\\tau}{2\\pi}\\right)^{\\frac{1}{2}} \\exp\\left( -\\frac{\\tau}{2}(y_{i} - \\mu)^{2} \\right)$$\nSubstituting this into the product for the joint likelihood gives:\n$$p(\\mathbf{y} \\mid \\tau, \\mu) = \\prod_{i=1}^{n} \\left[ \\left(\\frac{\\tau}{2\\pi}\\right)^{\\frac{1}{2}} \\exp\\left( -\\frac{\\tau}{2}(y_{i} - \\mu)^{2} \\right) \\right]$$\n$$p(\\mathbf{y} \\mid \\tau, \\mu) = \\left(\\frac{\\tau}{2\\pi}\\right)^{\\frac{n}{2}} \\exp\\left( -\\frac{\\tau}{2} \\sum_{i=1}^{n} (y_{i} - \\mu)^{2} \\right)$$\n\nNext, we specify the prior distribution for $\\tau$. The problem states that $\\tau$ follows a Gamma distribution with shape parameter $a_{0}$ and rate parameter $b_{0}$, denoted $\\tau \\sim \\text{Gamma}(a_{0}, b_{0})$. The PDF of the prior is given in its proportional form:\n$$p(\\tau) \\propto \\tau^{a_{0}-1} \\exp(-b_{0}\\tau)$$\n\nNow, we combine the likelihood and the prior using Bayes' theorem. We are interested in the posterior distribution of $\\tau$, so we can drop any factors that are not functions of $\\tau$.\n$$p(\\tau \\mid \\mathbf{y}, \\mu) \\propto p(\\mathbf{y} \\mid \\tau, \\mu) p(\\tau)$$\n$$p(\\tau \\mid \\mathbf{y}, \\mu) \\propto \\left[ \\left(\\frac{\\tau}{2\\pi}\\right)^{\\frac{n}{2}} \\exp\\left( -\\frac{\\tau}{2} \\sum_{i=1}^{n} (y_{i} - \\mu)^{2} \\right) \\right] \\times \\left[ \\tau^{a_{0}-1} \\exp(-b_{0}\\tau) \\right]$$\nDropping the constant factor $(2\\pi)^{-\\frac{n}{2}}$ and combining the terms involving $\\tau$:\n$$p(\\tau \\mid \\mathbf{y}, \\mu) \\propto \\tau^{\\frac{n}{2}} \\exp\\left( -\\frac{\\tau}{2} \\sum_{i=1}^{n} (y_{i} - \\mu)^{2} \\right) \\tau^{a_{0}-1} \\exp(-b_{0}\\tau)$$\n$$p(\\tau \\mid \\mathbf{y}, \\mu) \\propto \\tau^{\\frac{n}{2} + a_{0} - 1} \\exp\\left( -b_{0}\\tau - \\frac{\\tau}{2} \\sum_{i=1}^{n} (y_{i} - \\mu)^{2} \\right)$$\nFactoring $\\tau$ out of the exponent:\n$$p(\\tau \\mid \\mathbf{y}, \\mu) \\propto \\tau^{(a_{0} + \\frac{n}{2}) - 1} \\exp\\left( -\\left[b_{0} + \\frac{1}{2} \\sum_{i=1}^{n} (y_{i} - \\mu)^{2}\\right]\\tau \\right)$$\n\nThis expression is the kernel of a probability distribution for $\\tau$. We recognize this functional form, $\\tau^{\\text{shape}-1}\\exp(-\\text{rate} \\cdot \\tau)$, as the kernel of a Gamma distribution. The posterior distribution for $\\tau$ is therefore another Gamma distribution, $p(\\tau \\mid \\mathbf{y}, \\mu) \\sim \\text{Gamma}(a_{n}, b_{n})$, with updated parameters:\nThe posterior shape parameter, $a_{n}$, is:\n$$a_{n} = a_{0} + \\frac{n}{2}$$\nThe posterior rate parameter, $b_{n}$, is:\n$$b_{n} = b_{0} + \\frac{1}{2} \\sum_{i=1}^{n} (y_{i} - \\mu)^{2}$$\n\nSince the prior distribution for $\\tau$ is from the Gamma family and the resulting posterior distribution is also from the Gamma family, we have demonstrated that the Gamma distribution is a conjugate prior for the precision parameter of a Gaussian likelihood with known mean.\n\nTo obtain the normalized posterior density, we use the standard form of the Gamma PDF, which is $p(x \\mid a, b) = \\frac{b^{a}}{\\Gamma(a)}x^{a-1}\\exp(-bx)$, where $\\Gamma(\\cdot)$ is the gamma function. By substituting the posterior parameters $a_{n}$ and $b_{n}$, we arrive at the final closed-form expression for the normalized posterior density of $\\tau$:\n$$p(\\tau \\mid y_{1},\\dots,y_{n},\\mu) = \\frac{b_{n}^{a_{n}}}{\\Gamma(a_{n})} \\tau^{a_{n}-1} \\exp(-b_{n}\\tau)$$\n$$p(\\tau \\mid y_{1},\\dots,y_{n},\\mu) = \\frac{\\left(b_{0} + \\frac{1}{2} \\sum_{i=1}^{n} (y_{i}-\\mu)^{2}\\right)^{a_{0} + \\frac{n}{2}}}{\\Gamma\\left(a_{0} + \\frac{n}{2}\\right)} \\tau^{\\left(a_{0} + \\frac{n}{2}\\right) - 1} \\exp\\left(-\\left(b_{0} + \\frac{1}{2} \\sum_{i=1}^{n} (y_{i}-\\mu)^{2}\\right)\\tau\\right)$$\nThis is the complete analytical expression for the normalized posterior density.",
            "answer": "$$\\boxed{\\frac{\\left(b_{0} + \\frac{1}{2} \\sum_{i=1}^{n} (y_{i}-\\mu)^{2}\\right)^{a_{0} + \\frac{n}{2}}}{\\Gamma\\left(a_{0} + \\frac{n}{2}\\right)} \\tau^{\\left(a_{0} + \\frac{n}{2}\\right) - 1} \\exp\\left(-\\left(b_{0} + \\frac{1}{2} \\sum_{i=1}^{n} (y_{i}-\\mu)^{2}\\right)\\tau\\right)}$$"
        },
        {
            "introduction": "In realistic multiscale models, the number of potential parameters can be vast, leading to challenges with overfitting and identifiability. This exercise explores how the choice of prior can be used as a powerful regularization tool to promote parsimonious models. You will derive the posterior for a model using a Laplace prior , a choice that encourages sparsity by favoring parameter values at zero. This practice illuminates the deep connection between Bayesian inference and regularization methods like the LASSO, demonstrating how priors can embed principles like Occam's razor directly into the calibration process.",
            "id": "3736276",
            "problem": "Consider a multiscale calibration problem in which a parameter vector $\\theta \\in \\mathbb{R}^{p}$ controls both microscale heterogeneity and macroscale response via a computational simulator. Let $\\theta$ be partitioned as $\\theta = (\\alpha, \\beta)$, where $\\alpha \\in \\mathbb{R}^{q}$ are dictionary coefficients that parametrize a microscale material property field through $k(x) = \\bar{k} + \\sum_{i=1}^{q} \\alpha_{i} \\phi_{i}(x)$, with known basis functions $\\{\\phi_{i}(x)\\}_{i=1}^{q}$, and $\\beta \\in \\mathbb{R}^{p-q}$ are remaining non-sparse calibration parameters (for example, macroscale boundary condition coefficients or homogenized tensor hyperparameters). The macroscale forward response is modeled as $g(\\theta) \\in \\mathbb{R}^{m}$, where $g$ is obtained by homogenizing the microscale property field $k(x)$ to effective properties and propagating them through the macroscale model; the specific form of $g$ is not needed for this derivation but is assumed known and differentiable.\n\nYou observe data $y \\in \\mathbb{R}^{m}$ with an additive noise model that is well-approximated by a multivariate Gaussian distribution with known positive definite covariance $\\Sigma \\in \\mathbb{R}^{m \\times m}$, so that the conditional density is\n$$\np(y \\mid \\theta) = \\frac{1}{(2\\pi)^{m/2} |\\Sigma|^{1/2}} \\exp\\!\\left(-\\frac{1}{2} \\big(y - g(\\theta)\\big)^{\\top} \\Sigma^{-1} \\big(y - g(\\theta)\\big)\\right).\n$$\nTo promote parsimony in the microscale heterogeneity representation, assume a sparsity-inducing independent Laplace prior on the subset of parameters $\\alpha$, with scales $\\{b_{i}\\}_{i=1}^{q}$, and independent Gaussian priors on the remaining parameters $\\beta$ with means $\\{\\mu_{j}\\}_{j=1}^{p-q}$ and variances $\\{\\tau_{j}^{2}\\}_{j=1}^{p-q}$:\n$$\np(\\alpha) = \\prod_{i=1}^{q} \\frac{1}{2 b_{i}} \\exp\\!\\left(-\\frac{|\\alpha_{i}|}{b_{i}}\\right), \\qquad\np(\\beta) = \\prod_{j=1}^{p-q} \\frac{1}{\\sqrt{2\\pi \\tau_{j}^{2}}} \\exp\\!\\left(-\\frac{\\big(\\beta_{j} - \\mu_{j}\\big)^{2}}{2 \\tau_{j}^{2}}\\right).\n$$\n\nUsing Bayes’ rule and the stated likelihood and priors, derive the posterior log-density $\\ln p(\\theta \\mid y)$ up to an additive constant that is independent of $\\theta$. Your derivation must start from the definition of Bayes’ rule and the given probability density functions and proceed systematically. In addition, briefly justify the use of the Laplace prior for the microscale heterogeneity coefficients $\\alpha$ in this multiscale calibration context by appealing to foundational principles of parsimony and identifiability in ill-posed inverse problems.\n\nExpress your final answer as a single closed-form analytic expression for $\\ln p(\\theta \\mid y)$ up to a $\\theta$-independent additive constant. No numerical evaluation is required.",
            "solution": "The problem statement is evaluated as valid. It presents a well-posed, scientifically grounded problem in Bayesian statistics applied to multiscale model calibration. All necessary components, including the likelihood function and prior distributions, are explicitly and consistently defined. The task is a direct application of Bayes' rule and requires a conceptual justification based on established principles of regularization in inverse problems.\n\nThe derivation of the posterior log-density begins with Bayes' rule, which states that the posterior probability of the parameters $\\theta$ given the data $y$ is proportional to the product of the likelihood and the prior:\n$$\np(\\theta \\mid y) \\propto p(y \\mid \\theta) p(\\theta)\n$$\nThe problem states that the parameter vector $\\theta$ is partitioned as $\\theta = (\\alpha, \\beta)$ and that the priors on $\\alpha \\in \\mathbb{R}^{q}$ and $\\beta \\in \\mathbb{R}^{p-q}$ are independent. Therefore, the joint prior $p(\\theta)$ can be written as the product of the individual priors:\n$$\np(\\theta) = p(\\alpha, \\beta) = p(\\alpha) p(\\beta)\n$$\nSubstituting this into Bayes' rule gives:\n$$\np(\\theta \\mid y) \\propto p(y \\mid \\theta) p(\\alpha) p(\\beta)\n$$\nTo find the posterior log-density, we take the natural logarithm of both sides. This transforms the product of probabilities into a sum of log-probabilities:\n$$\n\\ln p(\\theta \\mid y) = \\ln p(y \\mid \\theta) + \\ln p(\\alpha) + \\ln p(\\beta) + C\n$$\nwhere $C$ is an additive constant that is independent of the parameter vector $\\theta$. This constant incorporates the logarithm of the marginal likelihood, $\\ln p(y)$, which is a normalization factor. We now analyze each term on the right-hand side.\n\n$1$. The Log-Likelihood Term, $\\ln p(y \\mid \\theta)$:\nThe likelihood is given as a multivariate Gaussian distribution:\n$$\np(y \\mid \\theta) = \\frac{1}{(2\\pi)^{m/2} |\\Sigma|^{1/2}} \\exp\\!\\left(-\\frac{1}{2} \\big(y - g(\\theta)\\big)^{\\top} \\Sigma^{-1} \\big(y - g(\\theta)\\big)\\right)\n$$\nTaking the natural logarithm, we get:\n$$\n\\ln p(y \\mid \\theta) = \\ln\\left( \\frac{1}{(2\\pi)^{m/2} |\\Sigma|^{1/2}} \\right) - \\frac{1}{2} \\big(y - g(\\theta)\\big)^{\\top} \\Sigma^{-1} \\big(y - g(\\theta)\\big)\n$$\nThe term $\\ln\\left( (2\\pi)^{-m/2} |\\Sigma|^{-1/2} \\right)$ is a constant with respect to $\\theta$. Therefore, up to an additive constant, the log-likelihood is:\n$$\n\\ln p(y \\mid \\theta) = - \\frac{1}{2} \\big(y - g(\\theta)\\big)^{\\top} \\Sigma^{-1} \\big(y - g(\\theta)\\big) + C_1\n$$\n\n$2$. The Log-Prior Term for $\\alpha$, $\\ln p(\\alpha)$:\nThe prior for $\\alpha$ is a product of independent Laplace distributions:\n$$\np(\\alpha) = \\prod_{i=1}^{q} \\frac{1}{2 b_{i}} \\exp\\!\\left(-\\frac{|\\alpha_{i}|}{b_{i}}\\right)\n$$\nThe logarithm of this prior is:\n$$\n\\ln p(\\alpha) = \\ln \\left( \\prod_{i=1}^{q} \\frac{1}{2 b_{i}} \\exp\\!\\left(-\\frac{|\\alpha_{i}|}{b_{i}}\\right) \\right) = \\sum_{i=1}^{q} \\ln \\left( \\frac{1}{2 b_{i}} \\exp\\!\\left(-\\frac{|\\alpha_{i}|}{b_{i}}\\right) \\right)\n$$\n$$\n\\ln p(\\alpha) = \\sum_{i=1}^{q} \\left( \\ln\\left(\\frac{1}{2 b_{i}}\\right) - \\frac{|\\alpha_{i}|}{b_{i}} \\right) = \\left( \\sum_{i=1}^{q} \\ln\\left(\\frac{1}{2 b_{i}}\\right) \\right) - \\sum_{i=1}^{q} \\frac{|\\alpha_{i}|}{b_{i}}\n$$\nThe term $\\sum_{i=1}^{q} \\ln(1/(2 b_{i}))$ is a constant with respect to $\\alpha$. Thus, up to an additive constant, the log-prior for $\\alpha$ is:\n$$\n\\ln p(\\alpha) = - \\sum_{i=1}^{q} \\frac{|\\alpha_{i}|}{b_{i}} + C_2\n$$\n\n$3$. The Log-Prior Term for $\\beta$, $\\ln p(\\beta)$:\nThe prior for $\\beta$ is a product of independent Gaussian distributions:\n$$\np(\\beta) = \\prod_{j=1}^{p-q} \\frac{1}{\\sqrt{2\\pi \\tau_{j}^{2}}} \\exp\\!\\left(-\\frac{\\big(\\beta_{j} - \\mu_{j}\\big)^{2}}{2 \\tau_{j}^{2}}\\right)\n$$\nThe logarithm of this prior is:\n$$\n\\ln p(\\beta) = \\ln \\left( \\prod_{j=1}^{p-q} \\frac{1}{\\sqrt{2\\pi \\tau_{j}^{2}}} \\exp\\!\\left(-\\frac{\\big(\\beta_{j} - \\mu_{j}\\big)^{2}}{2 \\tau_{j}^{2}}\\right) \\right) = \\sum_{j=1}^{p-q} \\ln \\left( \\frac{1}{\\sqrt{2\\pi \\tau_{j}^{2}}} \\exp\\!\\left(-\\frac{\\big(\\beta_{j} - \\mu_{j}\\big)^{2}}{2 \\tau_{j}^{2}}\\right) \\right)\n$$\n$$\n\\ln p(\\beta) = \\sum_{j=1}^{p-q} \\left( \\ln\\left(\\frac{1}{\\sqrt{2\\pi \\tau_{j}^{2}}}\\right) - \\frac{\\big(\\beta_{j} - \\mu_{j}\\big)^{2}}{2 \\tau_{j}^{2}} \\right) = \\left( \\sum_{j=1}^{p-q} \\ln\\left(\\frac{1}{\\sqrt{2\\pi \\tau_{j}^{2}}}\\right) \\right) - \\sum_{j=1}^{p-q} \\frac{\\big(\\beta_{j} - \\mu_{j}\\big)^{2}}{2 \\tau_{j}^{2}}\n$$\nThe term involving the sum of logarithms is independent of $\\beta$. Therefore, up to an additive constant, the log-prior for $\\beta$ is:\n$$\n\\ln p(\\beta) = - \\sum_{j=1}^{p-q} \\frac{\\big(\\beta_{j} - \\mu_{j}\\big)^{2}}{2 \\tau_{j}^{2}} + C_3\n$$\n\nCombining all three $\\theta$-dependent components, the posterior log-density, up to a single additive constant $C_{total} = C_1 + C_2 + C_3 + C$, is:\n$$\n\\ln p(\\theta \\mid y) = - \\frac{1}{2} \\big(y - g(\\theta)\\big)^{\\top} \\Sigma^{-1} \\big(y - g(\\theta)\\big) - \\sum_{i=1}^{q} \\frac{|\\alpha_{i}|}{b_{i}} - \\sum_{j=1}^{p-q} \\frac{\\big(\\beta_{j} - \\mu_{j}\\big)^{2}}{2 \\tau_{j}^{2}} + C_{total}\n$$\nThis expression is the final derived result for the posterior log-density up to a $\\theta$-independent additive constant.\n\nJustification for the Laplace Prior:\nThe use of a Laplace prior on the microscale heterogeneity coefficients, $\\{\\alpha_{i}\\}_{i=1}^{q}$, is a form of Bayesian regularization designed to address the ill-posed nature of the inverse problem.\n$1$. **Parsimony and Sparsity**: The task of determining the material property field $k(x)$ from macroscale data $y$ is an inverse problem. Representing $k(x)$ via a basis expansion with a large number of terms ($q$ is large) can lead to non-uniqueness and instability, as many different combinations of $\\alpha_{i}$ might yield similar macroscale responses $g(\\theta)$. The principle of parsimony (Occam's razor) suggests that one should prefer the simplest explanation that fits the data. In this context, a \"simpler\" microscale model is one that is described by the fewest possible basis functions, meaning most $\\alpha_i$ coefficients are zero. This is known as a sparse representation. The Laplace distribution, $p(\\alpha_i) \\propto \\exp(-|\\alpha_i|/b_i)$, has a sharp cusp at $\\alpha_i = 0$, assigning a much higher prior probability density to values at or near zero compared to a Gaussian prior. This property strongly encourages sparsity.\n$2$. **Regularization and Identifiability**: In a Bayesian framework, finding the maximum a posteriori (MAP) estimate is equivalent to minimizing the negative log-posterior. The negative log-prior for a Laplace distribution includes the term $\\sum_i |\\alpha_i|/b_i$, which is a scaled $L_1$-norm of the vector $\\alpha$. Minimizing an objective function that includes an $L_1$ penalty term is a well-known technique ($L_1$ regularization or LASSO) for inducing sparsity by driving many coefficients to be exactly zero. By enforcing sparsity on $\\alpha$, the Laplace prior acts as a regularizer. It helps select a unique, stable, and parsimonious solution from the potentially infinite set of solutions that could explain the data. This regularization mitigates the ill-posedness of the inverse problem and improves the identifiability of the most influential features of the microscale heterogeneity captured by the basis functions $\\{\\phi_i(x)\\}$.",
            "answer": "$$\n\\boxed{- \\frac{1}{2} \\left(y - g(\\theta)\\right)^{\\top} \\Sigma^{-1} \\left(y - g(\\theta)\\right) - \\sum_{i=1}^{q} \\frac{|\\alpha_{i}|}{b_{i}} - \\sum_{j=1}^{p-q} \\frac{\\left(\\beta_{j} - \\mu_{j}\\right)^{2}}{2 \\tau_{j}^{2}}}\n$$"
        },
        {
            "introduction": "While analytical posteriors are instructive, most modern Bayesian calibration relies on computational algorithms like Markov Chain Monte Carlo (MCMC). This practice challenges you to set up a Gibbs sampler, a cornerstone MCMC method, for a linear Gaussian state-space model . You will derive the full conditional distributions for both the model parameter and the latent states, which is the essential step in designing a Gibbs sampler. This exercise provides a concrete look at how to tackle inference in dynamic systems and models with latent variables, a common scenario in multiscale analysis.",
            "id": "3736324",
            "problem": "Consider a multiscale calibration scenario in which a coarse-scale latent process is used to fit fine-scale observations. Let $\\{x_{t}\\}_{t=0}^{T}$ denote a scalar latent macro-scale sequence and $\\{y_{t}\\}_{t=1}^{T}$ denote associated micro-scale observations. Assume a linear Gaussian state-space model (LGSSM) with known noise covariances given by\n$$\nx_{t} \\,=\\, \\theta\\, x_{t-1} \\,+\\, w_{t}, \\quad w_{t} \\sim \\mathcal{N}(0, q), \\quad t=1,\\dots,T,\n$$\n$$\ny_{t} \\,=\\, c\\, x_{t} \\,+\\, v_{t}, \\quad v_{t} \\sim \\mathcal{N}(0, r), \\quad t=1,\\dots,T,\n$$\nwith known scalars $q>0$, $r>0$, and $c \\neq 0$. The initial state satisfies $x_{0} \\sim \\mathcal{N}(\\mu_{0}, \\sigma_{0})$ with known $\\mu_{0} \\in \\mathbb{R}$ and $\\sigma_{0}>0$. The unknown parameter is the scalar transition coefficient $\\theta$, which has a Gaussian prior $\\theta \\sim \\mathcal{N}(m_{0}, s_{0})$ with known $m_{0} \\in \\mathbb{R}$ and $s_{0}>0$.\n\nYou are tasked with setting up a Gibbs sampler, a special case of Markov Chain Monte Carlo (MCMC), that alternates updating the latent trajectory $x_{0:T}$ and the parameter $\\theta$. Starting only from Bayes’ rule, the linear-Gaussian model definitions above, and properties of Gaussian conditioning and marginalization, perform the following:\n\n1. Derive the full conditional distribution $p(x_{0:T} \\mid \\theta, y_{1:T})$ and explain why it is multivariate Gaussian. Provide the filtering-smoothing recursion needed to compute its mean vector and covariance structure without directly inverting large matrices.\n\n2. Derive the full conditional distribution $p(\\theta \\mid x_{0:T}, y_{1:T})$ and explain why the micro-scale observations $\\{y_{t}\\}$ do not enter this conditional once $\\{x_{t}\\}$ is known.\n\nYour derivations must be from first principles and must not invoke any named algorithm as a black box without justification from Gaussian identities. Conclude by giving the closed-form analytic expression for the posterior variance of $\\theta$ under $p(\\theta \\mid x_{0:T}, y_{1:T})$ as a function of $s_{0}$, $q$, and the sufficient statistics of the latent sequence $\\{x_{t}\\}$.\n\nProvide your final answer as the exact symbolic expression for the posterior variance of $\\theta$. No numerical evaluation is required. If you introduce any acronym, spell out its full name the first time you use it. The final answer must be a single closed-form analytic expression.",
            "solution": "The problem statement is scientifically grounded, well-posed, objective, and self-contained. It describes a standard application of Bayesian inference to a Linear Gaussian State-Space Model (LGSSM), a fundamental construct in time-series analysis and multiscale modeling. All parameters and distributions are clearly defined, and the tasks are mathematically sound and unambiguous. The problem is valid.\n\nWe proceed to derive the full conditional distributions required for a Gibbs sampler. The state vector is denoted by $x_{0:T} = (x_0, x_1, \\dots, x_T)'$ and the observation vector by $y_{1:T} = (y_1, \\dots, y_T)'$.\n\n**1. Derivation of the full conditional $p(x_{0:T} \\mid \\theta, y_{1:T})$**\n\nAccording to Bayes’ rule, the posterior distribution of the latent state trajectory $x_{0:T}$ given the parameter $\\theta$ and observations $y_{1:T}$ is:\n$$\np(x_{0:T} \\mid y_{1:T}, \\theta) \\propto p(y_{1:T}, x_{0:T} \\mid \\theta) = p(y_{1:T} \\mid x_{0:T}, \\theta) p(x_{0:T} \\mid \\theta)\n$$\nThe model structure implies certain conditional independencies. The observation $y_t$ depends only on the state $x_t$, and the state $x_t$ depends only on the previous state $x_{t-1}$ and $\\theta$.\nThe term $p(y_{1:T} \\mid x_{0:T}, \\theta)$ factorizes as a product of likelihoods for each observation, which are independent given the states:\n$$\np(y_{1:T} \\mid x_{0:T}, \\theta) = \\prod_{t=1}^{T} p(y_t \\mid x_t) = \\prod_{t=1}^{T} \\mathcal{N}(y_t; c x_t, r)\n$$\nThe term $p(x_{0:T} \\mid \\theta)$ represents the prior on the state trajectory, which factorizes according to the Markov property of the process:\n$$\np(x_{0:T} \\mid \\theta) = p(x_0) \\prod_{t=1}^{T} p(x_t \\mid x_{t-1}, \\theta) = \\mathcal{N}(x_0; \\mu_0, \\sigma_0) \\prod_{t=1}^{T} \\mathcal{N}(x_t; \\theta x_{t-1}, q)\n$$\nCombining these, the log-posterior is:\n$$\n\\ln p(x_{0:T} \\mid y_{1:T}, \\theta) = \\mathrm{const} + \\ln p(x_0) + \\sum_{t=1}^{T} \\ln p(x_t \\mid x_{t-1}, \\theta) + \\sum_{t=1}^{T} \\ln p(y_t \\mid x_t)\n$$\n$$\n\\ln p(x_{0:T} \\mid y_{1:T}, \\theta) = \\mathrm{const} - \\frac{1}{2\\sigma_0}(x_0 - \\mu_0)^2 - \\frac{1}{2q}\\sum_{t=1}^{T}(x_t - \\theta x_{t-1})^2 - \\frac{1}{2r}\\sum_{t=1}^{T}(y_t - c x_t)^2\n$$\nThis expression is a quadratic function of the elements of the vector $x_{0:T}$. A probability density whose logarithm is a quadratic function is a multivariate Gaussian distribution. Therefore, $p(x_{0:T} \\mid y_{1:T}, \\theta)$ is a multivariate Gaussian. The mean and covariance of this distribution are determined by the coefficients of the linear and quadratic terms. The covariance matrix is the inverse of the precision matrix, which can be seen from the quadratic terms to be sparse (tridiagonal).\n\nDirectly forming and inverting this $(T+1) \\times (T+1)$ precision matrix is computationally expensive for large $T$, scaling as $O(T^3)$. A more efficient method, scaling as $O(T)$, is to use a two-pass recursion known as the Kalman filter and smoother. We derive this from first principles using properties of Gaussian distributions.\n\n**Forward Pass (Kalman Filtering):**\nThe goal is to compute the marginal posteriors $p(x_t \\mid y_{1:t}, \\theta) = \\mathcal{N}(x_t; \\mu_{t|t}, \\sigma_{t|t})$ for $t=0, \\dots, T$.\nLet's initialize the recursion at $t=0$ with the prior on $x_0$: $\\mu_{0|0} = \\mu_0$ and $\\sigma_{0|0} = \\sigma_0$.\nFor $t=1, \\dots, T$:\n- **Prediction Step:** We compute the predictive distribution for $x_t$ given observations up to $t-1$: $p(x_t \\mid y_{1:t-1}, \\theta) = \\mathcal{N}(x_t; \\mu_{t|t-1}, \\sigma_{t|t-1})$.\n$$\np(x_t \\mid y_{1:t-1}, \\theta) = \\int p(x_t \\mid x_{t-1}, \\theta) p(x_{t-1} \\mid y_{1:t-1}, \\theta) dx_{t-1}\n$$\nThis is the convolution of two Gaussians: $p(x_{t-1} \\mid y_{1:t-1}, \\theta) = \\mathcal{N}(x_{t-1}; \\mu_{t-1|t-1}, \\sigma_{t-1|t-1})$ and $p(x_t \\mid x_{t-1}, \\theta) = \\mathcal{N}(x_t; \\theta x_{t-1}, q)$. The result is a Gaussian with mean and variance:\n$$\n\\mu_{t|t-1} = E[\\theta x_{t-1}] = \\theta \\mu_{t-1|t-1}\n$$\n$$\n\\sigma_{t|t-1} = \\mathrm{Var}(\\theta x_{t-1} + w_t) = \\theta^2 \\mathrm{Var}(x_{t-1}) + \\mathrm{Var}(w_t) = \\theta^2 \\sigma_{t-1|t-1} + q\n$$\n- **Update Step:** We update the predictive distribution using the new observation $y_t$. Using Bayes' rule:\n$$\np(x_t \\mid y_{1:t}, \\theta) \\propto p(y_t \\mid x_t) p(x_t \\mid y_{1:t-1}, \\theta)\n$$\nThis is a product of two Gaussian likelihoods for $x_t$: $\\mathcal{N}(y_t; cx_t, r)$ and $\\mathcal{N}(x_t; \\mu_{t|t-1}, \\sigma_{t|t-1})$. The result is a Gaussian $p(x_t \\mid y_{1:t}, \\theta) = \\mathcal{N}(x_t; \\mu_{t|t}, \\sigma_{t|t})$. Its parameters can be found by considering the joint distribution of $(x_t, y_t)$ given $y_{1:t-1}$ and $\\theta$. This joint is Gaussian, and we can use standard formulas for Gaussian conditioning.\nThe moments of the joint distribution $(x_t, y_t) \\mid y_{1:t-1}, \\theta$ are:\n$E[x_t] = \\mu_{t|t-1}$, $\\mathrm{Var}(x_t) = \\sigma_{t|t-1}$\n$E[y_t] = E[cx_t+v_t] = c\\mu_{t|t-1}$, $\\mathrm{Var}(y_t) = c^2\\mathrm{Var}(x_t) + \\mathrm{Var}(v_t) = c^2\\sigma_{t|t-1} + r$\n$\\mathrm{Cov}(x_t, y_t) = \\mathrm{Cov}(x_t, cx_t+v_t) = c\\mathrm{Var}(x_t) = c\\sigma_{t|t-1}$\nThe conditional distribution $p(x_t \\mid y_t, y_{1:t-1}, \\theta)$ has mean and variance:\n$$\n\\mu_{t|t} = \\mu_{t|t-1} + \\mathrm{Cov}(x_t, y_t)[\\mathrm{Var}(y_t)]^{-1}(y_t - E[y_t]) = \\mu_{t|t-1} + \\frac{c\\sigma_{t|t-1}}{c^2\\sigma_{t|t-1} + r}(y_t - c\\mu_{t|t-1})\n$$\n$$\n\\sigma_{t|t} = \\sigma_{t|t-1} - \\mathrm{Cov}(x_t, y_t)[\\mathrm{Var}(y_t)]^{-1}\\mathrm{Cov}(y_t, x_t) = \\sigma_{t|t-1} - \\frac{(c\\sigma_{t|t-1})^2}{c^2\\sigma_{t|t-1} + r}\n$$\n\n**Backward Pass (Rauch-Tung-Striebel Smoothing):**\nThis pass computes the smoothed distributions $p(x_t \\mid y_{1:T}, \\theta) = \\mathcal{N}(x_t; \\mu_{t|T}, \\sigma_{t|T})$ for $t=T-1, \\dots, 0$.\nThe recursion starts at $t=T$ with the final filtered estimates: $\\mu_{T|T}$ and $\\sigma_{T|T}$. We have:\n$$\np(x_t \\mid y_{1:T}, \\theta) = \\int p(x_t \\mid x_{t+1}, y_{1:T}, \\theta) p(x_{t+1} \\mid y_{1:T}, \\theta) dx_{t+1}\n$$\nBy the Markov property, $x_t \\perp y_{t+1:T} \\mid x_{t+1}$. Thus, $p(x_t \\mid x_{t+1}, y_{1:T}, \\theta) = p(x_t \\mid x_{t+1}, y_{1:t}, \\theta)$.\nWe use Bayes' rule to find this distribution: $p(x_t \\mid x_{t+1}, y_{1:t}) \\propto p(x_{t+1} \\mid x_t) p(x_t \\mid y_{1:t})$. The distributions on the right are Gaussians we already know. The joint distribution of $(x_t, x_{t+1})$ given $y_{1:t}$ is Gaussian. Its moments are:\n$E[x_t \\mid y_{1:t}] = \\mu_{t|t}$, $\\mathrm{Var}(x_t \\mid y_{1:t}) = \\sigma_{t|t}$\n$E[x_{t+1} \\mid y_{1:t}] = E[\\theta x_t+w_{t+1} \\mid y_{1:t}] = \\theta\\mu_{t|t} = \\mu_{t+1|t}$\n$\\mathrm{Cov}(x_t, x_{t+1} \\mid y_{1:t}) = \\mathrm{Cov}(x_t, \\theta x_t+w_{t+1} \\mid y_{1:t}) = \\theta \\sigma_{t|t}$\nConditioning $x_t$ on $x_{t+1}$ yields a Gaussian with mean\n$E[x_t \\mid x_{t+1}, y_{1:t}] = \\mu_{t|t} + \\theta\\sigma_{t|t}(\\sigma_{t+1|t})^{-1}(x_{t+1}-\\mu_{t+1|t})$.\nTaking the expectation over $p(x_{t+1} \\mid y_{1:T}, \\theta)$ gives the smoothed mean:\n$$\n\\mu_{t|T} = E[E[x_t \\mid x_{t+1}, y_{1:t}] \\mid y_{1:T}] = \\mu_{t|t} + J_t(\\mu_{t+1|T}-\\mu_{t+1|t})\n$$\nwhere $J_t = \\theta\\sigma_{t|t}(\\sigma_{t+1|t})^{-1} = \\frac{\\theta\\sigma_{t|t}}{\\theta^2\\sigma_{t|t}+q}$ is the smoother gain.\nUsing the law of total variance, the smoothed variance is:\n$$\n\\sigma_{t|T} = \\mathrm{Var}(x_t \\mid y_{1:t}) - (\\mathrm{...}) + J_t^2 \\mathrm{Var}(x_{t+1} \\mid y_{1:T}) = \\sigma_{t|t} + J_t^2(\\sigma_{t+1|T}-\\sigma_{t+1|t})\n$$\nThe mean vector of $p(x_{0:T} \\mid y_{1:T}, \\theta)$ is $(\\mu_{0|T}, \\dots, \\mu_{T|T})'$. The variances $\\sigma_{t|T}$ are the diagonal elements of the covariance matrix. The off-diagonal elements $\\sigma_{t,s|T} = \\mathrm{Cov}(x_t, x_s \\mid y_{1:T}, \\theta)$ for $t < s$ can be found via a similar recursion: $\\sigma_{t,s|T} = J_t \\sigma_{t+1, s|T}$. This implies $\\sigma_{t,s|T} = (\\prod_{k=t}^{s-1} J_k) \\sigma_{s|T}$. This fully specifies the covariance matrix.\n\n**2. Derivation of the full conditional $p(\\theta \\mid x_{0:T}, y_{1:T})$**\n\nWe apply Bayes' rule for the parameter $\\theta$:\n$$\np(\\theta \\mid x_{0:T}, y_{1:T}) \\propto p(y_{1:T}, x_{0:T}, \\theta) = p(y_{1:T} \\mid x_{0:T}, \\theta)p(x_{0:T} \\mid \\theta)p(\\theta)\n$$\nThe crucial insight is to analyze the term $p(y_{1:T} \\mid x_{0:T}, \\theta)$. From the model definition, $y_t = c x_t + v_t$. The distribution of $y_t$ given $x_t$ is $\\mathcal{N}(y_t; cx_t, r)$, which does not depend on $\\theta$. Consequently, the entire observation likelihood $p(y_{1:T} \\mid x_{0:T}, \\theta) = \\prod_{t=1}^T p(y_t \\mid x_t)$ does not depend on $\\theta$. Thus, given the latent path $x_{0:T}$, the observations $y_{1:T}$ provide no additional information about $\\theta$. This is a property of the directed acyclic graph representing the model, where $x_{0:T}$ forms a Markov blanket for $\\theta$ relative to $y_{1:T}$.\n$$\np(\\theta \\mid x_{0:T}, y_{1:T}) \\propto p(x_{0:T} \\mid \\theta) p(\\theta) = p(\\theta \\mid x_{0:T})\n$$\nWe now derive this posterior distribution.\n$$\np(\\theta \\mid x_{0:T}) \\propto p(\\theta) \\prod_{t=1}^{T} p(x_t \\mid x_{t-1}, \\theta)\n$$\nThe prior is $p(\\theta) = \\mathcal{N}(\\theta; m_0, s_0)$ and the likelihood terms are $p(x_t \\mid x_{t-1}, \\theta) = \\mathcal{N}(x_t; \\theta x_{t-1}, q)$. The log-posterior is:\n$$\n\\ln p(\\theta \\mid x_{0:T}) = \\mathrm{const} + \\ln p(\\theta) + \\sum_{t=1}^{T} \\ln p(x_t \\mid x_{t-1}, \\theta)\n$$\n$$\n= \\mathrm{const} - \\frac{1}{2s_0}(\\theta - m_0)^2 - \\frac{1}{2q}\\sum_{t=1}^{T}(x_t - \\theta x_{t-1})^2\n$$\nThis is a quadratic function of $\\theta$, so the posterior is Gaussian. Let's write it as $p(\\theta \\mid x_{0:T}) = \\mathcal{N}(\\theta; m_N, s_N)$. To find the posterior variance $s_N$, we identify the coefficient of the $\\theta^2$ term in the log-posterior.\n$$\n\\ln p(\\theta \\mid x_{0:T}) = \\mathrm{const} - \\frac{1}{2s_0}(\\theta^2 - \\dots) - \\frac{1}{2q}\\sum_{t=1}^{T}(\\theta^2 x_{t-1}^2 - \\dots)\n$$\nThe term quadratic in $\\theta$ is:\n$$\n-\\frac{1}{2}\\left(\\frac{1}{s_0} + \\frac{1}{q}\\sum_{t=1}^{T} x_{t-1}^2\\right)\\theta^2\n$$\nFor a Gaussian $\\mathcal{N}(m_N, s_N)$, the log-density has a quadratic term equal to $-\\frac{1}{2s_N}\\theta^2$. By matching coefficients, we find the inverse posterior variance:\n$$\n\\frac{1}{s_N} = \\frac{1}{s_0} + \\frac{1}{q}\\sum_{t=1}^{T} x_{t-1}^2\n$$\nThe posterior variance $s_N$ is therefore the reciprocal of this quantity. This expression depends on the prior variance $s_0$, the process noise variance $q$, and the sufficient statistic $\\sum_{t=1}^T x_{t-1}^2$ of the latent sequence.\n\nThe posterior variance of $\\theta$ is:\n$$\ns_N = \\left(\\frac{1}{s_0} + \\frac{1}{q}\\sum_{t=1}^{T} x_{t-1}^2\\right)^{-1}\n$$",
            "answer": "$$\n\\boxed{\\left(\\frac{1}{s_0} + \\frac{1}{q}\\sum_{t=1}^{T} x_{t-1}^{2}\\right)^{-1}}\n$$"
        }
    ]
}