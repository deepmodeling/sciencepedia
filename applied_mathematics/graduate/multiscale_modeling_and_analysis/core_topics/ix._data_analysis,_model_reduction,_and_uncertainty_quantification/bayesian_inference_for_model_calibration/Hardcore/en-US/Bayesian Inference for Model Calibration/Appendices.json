{
    "hands_on_practices": [
        {
            "introduction": "We begin our hands-on practice with a foundational concept in Bayesian inference: conjugate priors. This exercise demonstrates the elegant analytical solution that arises when the prior distribution is chosen to be mathematically compatible with the likelihood, allowing us to see exactly how data updates our beliefs about a parameter . Mastering this core mechanic provides the groundwork for understanding more complex models where such analytical shortcuts are not available.",
            "id": "4215202",
            "problem": "In a pressurized water reactor core simulation, suppose a high-fidelity neutron transport model predicts a stable in-core detector count rate with known expected value $\\mu$ under a fixed operating condition. You collect $n$ independent detector readings $y_{1},\\dots,y_{n}$ from the same condition, and model the measurement and unresolved modeling noise as Gaussian with precision $\\tau = 1/\\sigma^{2}$, so that the likelihood for each observation satisfies $y_{i} \\mid \\tau \\sim \\mathcal{N}(\\mu,\\tau^{-1})$ and the observations are conditionally independent given $\\tau$. To reflect prior knowledge of the noise precision in this reactor configuration, you place a Gamma prior on $\\tau$ with shape-rate parameters $(a_{0}, b_{0})$, so that $p(\\tau) \\propto \\tau^{a_{0}-1} \\exp(-b_{0}\\tau)$ for $\\tau > 0$.\n\nStarting only from Bayes’ theorem and the standard definition of the Gaussian likelihood and Gamma prior density, derive the normalized posterior density $p(\\tau \\mid y_{1},\\dots,y_{n},\\mu)$ in closed form and demonstrate that the Gamma family is conjugate to the Gaussian likelihood with known mean.\n\nYour final answer must be a single closed-form analytic expression for the normalized posterior density $p(\\tau \\mid y_{1},\\dots,y_{n},\\mu)$, expressed symbolically in terms of $a_{0}$, $b_{0}$, $n$, $\\mu$, and $\\sum_{i=1}^{n} (y_{i}-\\mu)^{2}$. Do not substitute numerical values. No rounding is required. Do not include units in the final expression.",
            "solution": "The problem statement is evaluated as scientifically sound, well-posed, objective, and complete. It presents a standard, formalizable problem in Bayesian statistics applied to a plausible scenario in nuclear reactor simulation. No flaws are identified. We may therefore proceed with the derivation.\n\nThe objective is to derive the posterior probability density function (PDF) for the noise precision $\\tau$, denoted as $p(\\tau \\mid \\mathbf{y}, \\mu)$, where $\\mathbf{y}$ is the vector of observations $(y_{1}, \\dots, y_{n})$. The derivation begins with Bayes' theorem, which states that the posterior is proportional to the product of the likelihood and the prior:\n$$p(\\tau \\mid \\mathbf{y}, \\mu) \\propto p(\\mathbf{y} \\mid \\tau, \\mu) p(\\tau)$$\n\nFirst, we formulate the likelihood function, $p(\\mathbf{y} \\mid \\tau, \\mu)$. The problem states that the observations $y_{i}$ are conditionally independent given $\\tau$ and $\\mu$. Therefore, the joint likelihood of the entire dataset $\\mathbf{y}$ is the product of the individual likelihoods for each observation $y_{i}$:\n$$p(\\mathbf{y} \\mid \\tau, \\mu) = \\prod_{i=1}^{n} p(y_{i} \\mid \\tau, \\mu)$$\nEach observation $y_{i}$ is drawn from a Gaussian (Normal) distribution with a known mean $\\mu$ and precision $\\tau$, i.e., $y_{i} \\sim \\mathcal{N}(\\mu, \\tau^{-1})$. The PDF for a single observation is:\n$$p(y_{i} \\mid \\tau, \\mu) = \\frac{1}{\\sqrt{2\\pi(\\tau^{-1})}} \\exp\\left( -\\frac{(y_{i} - \\mu)^{2}}{2(\\tau^{-1})} \\right) = \\left(\\frac{\\tau}{2\\pi}\\right)^{\\frac{1}{2}} \\exp\\left( -\\frac{\\tau}{2}(y_{i} - \\mu)^{2} \\right)$$\nSubstituting this into the product for the joint likelihood gives:\n$$p(\\mathbf{y} \\mid \\tau, \\mu) = \\prod_{i=1}^{n} \\left[ \\left(\\frac{\\tau}{2\\pi}\\right)^{\\frac{1}{2}} \\exp\\left( -\\frac{\\tau}{2}(y_{i} - \\mu)^{2} \\right) \\right]$$\n$$p(\\mathbf{y} \\mid \\tau, \\mu) = \\left(\\frac{\\tau}{2\\pi}\\right)^{\\frac{n}{2}} \\exp\\left( -\\frac{\\tau}{2} \\sum_{i=1}^{n} (y_{i} - \\mu)^{2} \\right)$$\n\nNext, we specify the prior distribution for $\\tau$. The problem states that $\\tau$ follows a Gamma distribution with shape parameter $a_{0}$ and rate parameter $b_{0}$, denoted $\\tau \\sim \\text{Gamma}(a_{0}, b_{0})$. The PDF of the prior is given in its proportional form:\n$$p(\\tau) \\propto \\tau^{a_{0}-1} \\exp(-b_{0}\\tau)$$\n\nNow, we combine the likelihood and the prior using Bayes' theorem. We are interested in the posterior distribution of $\\tau$, so we can drop any factors that are not functions of $\\tau$.\n$$p(\\tau \\mid \\mathbf{y}, \\mu) \\propto p(\\mathbf{y} \\mid \\tau, \\mu) p(\\tau)$$\n$$p(\\tau \\mid \\mathbf{y}, \\mu) \\propto \\left[ \\left(\\frac{\\tau}{2\\pi}\\right)^{\\frac{n}{2}} \\exp\\left( -\\frac{\\tau}{2} \\sum_{i=1}^{n} (y_{i} - \\mu)^{2} \\right) \\right] \\times \\left[ \\tau^{a_{0}-1} \\exp(-b_{0}\\tau) \\right]$$\nDropping the constant factor $(2\\pi)^{-\\frac{n}{2}}$ and combining the terms involving $\\tau$:\n$$p(\\tau \\mid \\mathbf{y}, \\mu) \\propto \\tau^{\\frac{n}{2}} \\exp\\left( -\\frac{\\tau}{2} \\sum_{i=1}^{n} (y_{i} - \\mu)^{2} \\right) \\tau^{a_{0}-1} \\exp(-b_{0}\\tau)$$\n$$p(\\tau \\mid \\mathbf{y}, \\mu) \\propto \\tau^{\\frac{n}{2} + a_{0} - 1} \\exp\\left( -b_{0}\\tau - \\frac{\\tau}{2} \\sum_{i=1}^{n} (y_{i} - \\mu)^{2} \\right)$$\nFactoring $\\tau$ out of the exponent:\n$$p(\\tau \\mid \\mathbf{y}, \\mu) \\propto \\tau^{(a_{0} + \\frac{n}{2}) - 1} \\exp\\left( -\\left[b_{0} + \\frac{1}{2} \\sum_{i=1}^{n} (y_{i} - \\mu)^{2}\\right]\\tau \\right)$$\n\nThis expression is the kernel of a probability distribution for $\\tau$. We recognize this functional form, $\\tau^{\\text{shape}-1}\\exp(-\\text{rate} \\cdot \\tau)$, as the kernel of a Gamma distribution. The posterior distribution for $\\tau$ is therefore another Gamma distribution, $p(\\tau \\mid \\mathbf{y}, \\mu) \\sim \\text{Gamma}(a_{n}, b_{n})$, with updated parameters:\nThe posterior shape parameter, $a_{n}$, is:\n$$a_{n} = a_{0} + \\frac{n}{2}$$\nThe posterior rate parameter, $b_{n}$, is:\n$$b_{n} = b_{0} + \\frac{1}{2} \\sum_{i=1}^{n} (y_{i} - \\mu)^{2}$$\n\nSince the prior distribution for $\\tau$ is from the Gamma family and the resulting posterior distribution is also from the Gamma family, we have demonstrated that the Gamma distribution is a conjugate prior for the precision parameter of a Gaussian likelihood with known mean.\n\nTo obtain the normalized posterior density, we use the standard form of the Gamma PDF, which is $p(x \\mid a, b) = \\frac{b^{a}}{\\Gamma(a)}x^{a-1}\\exp(-bx)$, where $\\Gamma(\\cdot)$ is the gamma function. By substituting the posterior parameters $a_{n}$ and $b_{n}$, we arrive at the final closed-form expression for the normalized posterior density of $\\tau$:\n$$p(\\tau \\mid y_{1},\\dots,y_{n},\\mu) = \\frac{b_{n}^{a_{n}}}{\\Gamma(a_{n})} \\tau^{a_{n}-1} \\exp(-b_{n}\\tau)$$\n$$p(\\tau \\mid y_{1},\\dots,y_{n},\\mu) = \\frac{\\left(b_{0} + \\frac{1}{2} \\sum_{i=1}^{n} (y_{i}-\\mu)^{2}\\right)^{a_{0} + \\frac{n}{2}}}{\\Gamma\\left(a_{0} + \\frac{n}{2}\\right)} \\tau^{\\left(a_{0} + \\frac{n}{2}\\right) - 1} \\exp\\left(-\\left(b_{0} + \\frac{1}{2} \\sum_{i=1}^{n} (y_{i}-\\mu)^{2}\\right)\\tau\\right)$$\nThis is the complete analytical expression for the normalized posterior density.",
            "answer": "$$\\boxed{\\frac{\\left(b_{0} + \\frac{1}{2} \\sum_{i=1}^{n} (y_{i}-\\mu)^{2}\\right)^{a_{0} + \\frac{n}{2}}}{\\Gamma\\left(a_{0} + \\frac{n}{2}\\right)} \\tau^{\\left(a_{0} + \\frac{n}{2}\\right) - 1} \\exp\\left(-\\left(b_{0} + \\frac{1}{2} \\sum_{i=1}^{n} (y_{i}-\\mu)^{2}\\right)\\tau\\right)}$$"
        },
        {
            "introduction": "Real-world data is rarely as clean as our models assume, and outliers can unduly influence parameter estimates. This practice explores how to build robust models by moving from a standard Gaussian likelihood to a Student-$t$ likelihood, which has heavier tails to accommodate extreme values . By comparing these two approaches, you will gain insight into the crucial role of the likelihood in mitigating the effects of model misspecification and understand why more advanced computational methods are often necessary.",
            "id": "3736342",
            "problem": "Consider a multiscale calibration scenario in which a coarse-scale transport model aggregates microscale physics into an effective linear relation $y_i \\approx \\theta x_i$, where $y_i$ is an averaged flux observable computed from microscale simulations at condition $x_i$, and $\\theta$ is an unknown effective parameter encoding microscale heterogeneity through homogenization. Due to rare microscale defect events, measurement noise is misspecified if assumed Gaussian; occasional extreme deviations are present. To calibrate $\\theta$ using Bayesian inference, two competing likelihood models are considered: a Gaussian error model with variance $\\sigma^2$, and a robust Student-$t$ error model with degrees of freedom (DoF) $\\nu$ and scale $\\sigma$. Assume a Gaussian prior $\\theta \\sim \\mathcal{N}(\\mu_0,\\tau_0^2)$ and independent errors across $i=1,\\dots,n$.\n\nStarting from Bayes’ theorem and standard definitions of the Gaussian and Student-$t$ distributions, derive the posterior for $\\theta$ under the Gaussian error model, and express the influence of residuals $r_i = y_i - \\theta x_i$ on the gradient of the log-likelihood for both Gaussian and Student-$t$ errors. Establish a hierarchical interpretation of the Student-$t$ likelihood that reveals its robust weighting of large residuals. Finally, compare the qualitative effects of the two likelihoods on posterior concentration when outliers are present.\n\nWhich of the following statements are correct?\n\nA. Under the Gaussian error model with variance $\\sigma^2$, the posterior for $\\theta$ is Gaussian with precision $\\,\\tau_0^{-2} + \\sigma^{-2}\\sum_{i=1}^n x_i^2\\,$ and mean $\\,\\left(\\tau_0^{-2}\\mu_0 + \\sigma^{-2} \\sum_{i=1}^n x_i y_i\\right)\\big/\\left(\\tau_0^{-2} + \\sigma^{-2}\\sum_{i=1}^n x_i^2\\right)\\,$.\n\nB. A Student-$t$ error model with DoF $\\nu$ and scale $\\sigma$ admits a Gaussian scale-mixture representation by introducing latent variables $\\lambda_i \\sim \\mathrm{Gamma}\\!\\left(\\nu/2,\\nu/2\\right)$ such that, conditionally, $r_i \\mid \\lambda_i \\sim \\mathcal{N}\\!\\left(0,\\sigma^2/\\lambda_i\\right)$; given $\\lambda_1,\\dots,\\lambda_n$, the conditional posterior of $\\theta$ is Gaussian with precision $\\,\\tau_0^{-2} + \\sum_{i=1}^n (\\lambda_i/\\sigma^2) x_i^2\\,$.\n\nC. Compared to the Gaussian error model, the Student-$t$ error model assigns higher posterior density to parameter values that shift to accommodate outliers, thereby making credible intervals for $\\theta$ narrower in the presence of outliers.\n\nD. As $\\nu \\to \\infty$, the Student-$t$ likelihood converges to the Gaussian likelihood with variance $\\sigma^2$, and the posterior under the Student-$t$ error model converges to the Gaussian-error posterior.\n\nE. For fixed $\\sigma$, decreasing $\\nu$ increases the influence function at large residuals $|r_i|$, amplifying the leverage of outliers relative to the Gaussian error model.\n\nSelect all correct options.",
            "solution": "The core of the problem lies in comparing a standard Bayesian linear regression model with a robust alternative. We start by applying Bayes' theorem, which states that the posterior distribution is proportional to the product of the likelihood and the prior: $p(\\theta | \\mathbf{y}) \\propto p(\\mathbf{y} | \\theta) p(\\theta)$.\n\n**Option A Analysis: Posterior under the Gaussian Error Model**\n\nThe prior for $\\theta$ is Gaussian:\n$$ p(\\theta) \\propto \\exp\\left(-\\frac{1}{2\\tau_0^2}(\\theta - \\mu_0)^2\\right) $$\nThe likelihood, assuming independent Gaussian errors $y_i - \\theta x_i \\sim \\mathcal{N}(0, \\sigma^2)$, is:\n$$ p(\\mathbf{y} | \\theta) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\theta x_i)^2}{2\\sigma^2}\\right) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - \\theta x_i)^2\\right) $$\nThe log-posterior is proportional to the sum of the log-prior and log-likelihood:\n$$ \\log p(\\theta | \\mathbf{y}) \\propto -\\frac{1}{2\\tau_0^2}(\\theta - \\mu_0)^2 - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - \\theta x_i)^2 $$\nThis is a quadratic function of $\\theta$, which means the posterior is also Gaussian. To find its parameters, we expand the squared terms and collect terms in $\\theta^2$ and $\\theta$:\n$$ \\log p(\\theta | \\mathbf{y}) \\propto -\\frac{1}{2}\\left(\\frac{1}{\\tau_0^2}\\theta^2 - \\frac{2\\mu_0}{\\tau_0^2}\\theta\\right) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n(y_i^2 - 2y_i x_i \\theta + x_i^2 \\theta^2) + C $$\n$$ \\log p(\\theta | \\mathbf{y}) \\propto -\\frac{1}{2} \\left[ \\left(\\frac{1}{\\tau_0^2} + \\frac{\\sum x_i^2}{\\sigma^2}\\right)\\theta^2 - 2\\left(\\frac{\\mu_0}{\\tau_0^2} + \\frac{\\sum x_i y_i}{\\sigma^2}\\right)\\theta \\right] + C' $$\nA Gaussian posterior $\\theta \\sim \\mathcal{N}(\\mu_p, \\tau_p^2)$ has a log-density proportional to $-\\frac{1}{2\\tau_p^2}(\\theta - \\mu_p)^2 \\propto -\\frac{1}{2}(\\frac{1}{\\tau_p^2}\\theta^2 - \\frac{2\\mu_p}{\\tau_p^2}\\theta)$.\nBy comparing coefficients, we identify the posterior precision $\\tau_p^{-2}$:\n$$ \\tau_p^{-2} = \\frac{1}{\\tau_0^2} + \\frac{1}{\\sigma^2}\\sum_{i=1}^n x_i^2 = \\tau_0^{-2} + \\sigma^{-2}\\sum_{i=1}^n x_i^2 $$\nAnd the term $\\mu_p/\\tau_p^2$:\n$$ \\frac{\\mu_p}{\\tau_p^2} = \\frac{\\mu_0}{\\tau_0^2} + \\frac{1}{\\sigma^2}\\sum_{i=1}^n x_i y_i = \\tau_0^{-2}\\mu_0 + \\sigma^{-2}\\sum_{i=1}^n x_i y_i $$\nSolving for the posterior mean $\\mu_p$:\n$$ \\mu_p = \\frac{\\tau_0^{-2}\\mu_0 + \\sigma^{-2}\\sum_{i=1}^n x_i y_i}{\\tau_0^{-2} + \\sigma^{-2}\\sum_{i=1}^n x_i^2} $$\nThe expressions for the posterior mean and precision match those given in option A.\n\nVerdict for A: **Correct**.\n\n**Option B Analysis: Hierarchical Representation of the Student-$t$ Model**\n\nThe Student-$t$ distribution can be represented as a Gaussian scale-mixture. A random variable $Z$ has a Student-$t$ distribution with location $\\mu$, scale $\\sigma$, and $\\nu$ degrees of freedom if it can be generated hierarchically as:\n1.  Draw a scaling variable $\\lambda \\sim \\mathrm{Gamma}(\\frac{\\nu}{2}, \\frac{\\nu}{2})$ (shape, rate parameterization).\n2.  Draw the variable $Z \\sim \\mathcal{N}(\\mu, \\sigma^2/\\lambda)$.\nThe problem applies this to the residuals $r_i = y_i - \\theta x_i$, where each residual has its own latent scaling variable $\\lambda_i$. This corresponds to the model:\n$$ r_i | \\lambda_i \\sim \\mathcal{N}(0, \\sigma^2/\\lambda_i) \\quad \\text{with} \\quad \\lambda_i \\sim \\mathrm{Gamma}(\\nu/2, \\nu/2) $$\nThis matches the first part of option B. Now we derive the conditional posterior of $\\theta$ given the latent variables $\\lambda_1, \\dots, \\lambda_n$.\nGiven the $\\lambda_i$, the likelihood for $y_i$ is $y_i | \\theta, \\lambda_i \\sim \\mathcal{N}(\\theta x_i, \\sigma^2/\\lambda_i)$. The full likelihood given $\\boldsymbol{\\lambda} = \\{\\lambda_1, \\dots, \\lambda_n\\}$ is:\n$$ p(\\mathbf{y} | \\theta, \\boldsymbol{\\lambda}) \\propto \\prod_{i=1}^n \\exp\\left(-\\frac{(y_i - \\theta x_i)^2}{2(\\sigma^2/\\lambda_i)}\\right) = \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^n \\lambda_i (y_i - \\theta x_i)^2\\right) $$\nThe conditional posterior $p(\\theta | \\mathbf{y}, \\boldsymbol{\\lambda})$ is proportional to $p(\\mathbf{y} | \\theta, \\boldsymbol{\\lambda}) p(\\theta)$. The log-posterior is:\n$$ \\log p(\\theta | \\mathbf{y}, \\boldsymbol{\\lambda}) \\propto -\\frac{1}{2\\tau_0^2}(\\theta - \\mu_0)^2 - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n \\lambda_i (y_i - \\theta x_i)^2 $$\nThis is again a quadratic in $\\theta$, so the conditional posterior for $\\theta$ is Gaussian. We find its precision by collecting the $\\theta^2$ terms:\n$$ -\\frac{1}{2} \\left( \\frac{1}{\\tau_0^2} + \\frac{1}{\\sigma^2}\\sum_{i=1}^n \\lambda_i x_i^2 \\right) \\theta^2 + \\dots $$\nThe conditional posterior precision is therefore $\\tau_0^{-2} + \\sigma^{-2} \\sum_{i=1}^n \\lambda_i x_i^2$, which can be written as $\\tau_0^{-2} + \\sum_{i=1}^n (\\lambda_i/\\sigma^2) x_i^2$. This matches the expression in option B.\nThis hierarchical structure is the basis of Gibbs sampling algorithms for fitting Bayesian models with Student-$t$ likelihoods, where one iteratively samples from the conditional posteriors of $\\theta$ and the $\\lambda_i$.\n\nVerdict for B: **Correct**.\n\n**Option C Analysis: Qualitative Effect on Posterior**\n\nThe Student-$t$ distribution has heavier tails than the Gaussian. This means it assigns higher probability to extreme values (outliers). In a Bayesian context, this implies that the model is less \"surprised\" by an outlier and does not need to drastically alter its parameters to explain it. The hierarchical model in B makes this explicit: an outlier (large $|r_i| = |y_i - \\theta x_i|$) can be explained by drawing a small value for the corresponding $\\lambda_i$. A small $\\lambda_i$ increases the local variance $\\sigma^2/\\lambda_i$ for that data point, effectively down-weighting its influence on the estimation of the global parameter $\\theta$.\nConversely, the Gaussian model, with its fixed variance $\\sigma^2$, must accommodate all data points equally. An outlier exerts a strong \"pull\" on the posterior, shifting the mean of $\\theta$ to reduce the large residual.\nTherefore, the Student-$t$ model is *robust* to outliers, meaning it does *not* shift significantly to accommodate them. Option C claims the opposite: that the Student-$t$ model \"assigns higher posterior density to parameter values that shift to accommodate outliers\". This is a precise description of what the non-robust *Gaussian* model does. The conclusion about credible intervals is also questionable and context-dependent, but the premise of the statement is fundamentally incorrect.\n\nVerdict for C: **Incorrect**.\n\n**Option D Analysis: Asymptotic Behavior of the Student-$t$ Distribution**\n\nLet's examine the probability density function of a Student-$t$ distribution with location $\\mu$, scale $\\sigma$, and $\\nu$ degrees of freedom:\n$$ p(z; \\nu) = \\frac{\\Gamma((\\nu+1)/2)}{\\Gamma(\\nu/2)\\sqrt{\\pi\\nu}\\sigma} \\left(1 + \\frac{1}{\\nu}\\left(\\frac{z-\\mu}{\\sigma}\\right)^2\\right)^{-(\\nu+1)/2} $$\nWe are interested in the limit as $\\nu \\to \\infty$. Using the well-known limit $\\lim_{n \\to \\infty} (1+k/n)^n = e^k$, we have:\n$$ \\lim_{\\nu \\to \\infty} \\left(1 + \\frac{1}{\\nu}\\left(\\frac{z-\\mu}{\\sigma}\\right)^2\\right)^{-(\\nu+1)/2} = \\lim_{\\nu \\to \\infty} \\left[\\left(1 + \\frac{((z-\\mu)/\\sigma)^2}{\\nu}\\right)^{\\nu}\\right]^{-1/2} \\cdot \\left(1 + \\frac{((z-\\mu)/\\sigma)^2}{\\nu}\\right)^{-1/2} $$\n$$ = \\left[ e^{((z-\\mu)/\\sigma)^2} \\right]^{-1/2} \\cdot (1) = \\exp\\left(-\\frac{(z-\\mu)^2}{2\\sigma^2}\\right) $$\nThis is the kernel of a Gaussian distribution. Using Stirling's approximation for the Gamma function, the normalization constant also converges: $\\lim_{\\nu \\to \\infty} \\frac{\\Gamma((\\nu+1)/2)}{\\Gamma(\\nu/2)\\sqrt{\\pi\\nu}} = \\frac{1}{\\sqrt{2\\pi}}$.\nThus, as $\\nu \\to \\infty$, the Student-$t(\\mu, \\sigma, \\nu)$ distribution converges pointwise to the Gaussian $\\mathcal{N}(\\mu, \\sigma^2)$ distribution.\nSince the Student-$t$ likelihood function converges to the Gaussian likelihood function, and the prior is the same for both models, the posterior distribution under the Student-$t$ model must also converge to the posterior distribution under the Gaussian model.\n\nVerdict for D: **Correct**.\n\n**Option E Analysis: Influence of Residuals and the Role of $\\nu$**\n\nThe influence of a data point on the parameter estimate is related to the gradient of the log-likelihood. Let $\\mathcal{L}(\\theta) = \\sum_{i=1}^n \\log p(y_i|\\theta)$. The gradient is $\\frac{\\partial\\mathcal{L}}{\\partial \\theta} = \\sum_{i=1}^n \\frac{\\partial}{\\partial \\theta} \\log p(y_i|\\theta)$.\nFor the Student-$t$ model, $\\log p(y_i|\\theta) = C - \\frac{\\nu+1}{2} \\log\\left(1 + \\frac{(y_i - \\theta x_i)^2}{\\nu\\sigma^2}\\right)$. Let $r_i = y_i - \\theta x_i$.\n$$ \\frac{\\partial}{\\partial \\theta} \\log p(y_i|\\theta) = - \\frac{\\nu+1}{2} \\frac{1}{1 + r_i^2/(\\nu\\sigma^2)} \\cdot \\frac{2r_i(-x_i)}{\\nu\\sigma^2} = \\left(\\frac{\\nu+1}{\\nu\\sigma^2 + r_i^2}\\right) r_i x_i $$\nThe term multiplying $x_i$ can be seen as an influence-weighted residual, $w_i r_i$, where the weight is $w_i = \\frac{\\nu+1}{\\nu\\sigma^2 + r_i^2}$. For a large residual $|r_i| \\to \\infty$, this weight behaves like $w_i \\approx (\\nu+1)/r_i^2 \\to 0$. The overall term's magnitude is $\\approx |(\\nu+1)x_i/r_i|$, which goes to $0$. This confirms the influence is bounded and redescending.\nNow, let's see how this weight changes with $\\nu$ for a fixed, large residual $r_i$.\n$$ \\frac{\\partial w_i}{\\partial \\nu} = \\frac{1(\\nu\\sigma^2 + r_i^2) - (\\nu+1)\\sigma^2}{(\\nu\\sigma^2 + r_i^2)^2} = \\frac{r_i^2 - \\sigma^2}{(\\nu\\sigma^2 + r_i^2)^2} $$\nFor a large residual, we have $r_i^2 > \\sigma^2$, so $\\frac{\\partial w_i}{\\partial \\nu} > 0$. This means that increasing $\\nu$ *increases* the weight given to large residuals. Correspondingly, decreasing $\\nu$ *decreases* the weight and thus reduces the influence of outliers.\nA smaller $\\nu$ corresponds to heavier tails, making the model more robust. Option E claims that decreasing $\\nu$ *increases* the influence and *amplifies* the leverage of outliers. This is the exact opposite of the correct behavior.\n\nVerdict for E: **Incorrect**.",
            "answer": "$$\\boxed{ABD}$$"
        },
        {
            "introduction": "When the joint posterior distribution is too complex to analyze directly, we turn to computational techniques like Markov Chain Monte Carlo (MCMC). This exercise introduces Gibbs sampling, a powerful MCMC algorithm that breaks a high-dimensional problem into a sequence of simpler, low-dimensional ones . By deriving the conditional distributions for a linear state-space model, you will learn how to construct a sampler for dynamic systems, a common task in multiscale modeling and time-series analysis.",
            "id": "3736324",
            "problem": "Consider a multiscale calibration scenario in which a coarse-scale latent process is used to fit fine-scale observations. Let $\\{x_{t}\\}_{t=0}^{T}$ denote a scalar latent macro-scale sequence and $\\{y_{t}\\}_{t=1}^{T}$ denote associated micro-scale observations. Assume a linear Gaussian state-space model (LGSSM) with known noise covariances given by\n$$\nx_{t} \\,=\\, \\theta\\, x_{t-1} \\,+\\, w_{t}, \\quad w_{t} \\sim \\mathcal{N}(0, q), \\quad t=1,\\dots,T,\n$$\n$$\ny_{t} \\,=\\, c\\, x_{t} \\,+\\, v_{t}, \\quad v_{t} \\sim \\mathcal{N}(0, r), \\quad t=1,\\dots,T,\n$$\nwith known scalars $q>0$, $r>0$, and $c \\neq 0$. The initial state satisfies $x_{0} \\sim \\mathcal{N}(\\mu_{0}, \\sigma_{0})$ with known $\\mu_{0} \\in \\mathbb{R}$ and $\\sigma_{0}>0$. The unknown parameter is the scalar transition coefficient $\\theta$, which has a Gaussian prior $\\theta \\sim \\mathcal{N}(m_{0}, s_{0})$ with known $m_{0} \\in \\mathbb{R}$ and $s_{0}>0$.\n\nYou are tasked with setting up a Gibbs sampler, a special case of Markov Chain Monte Carlo (MCMC), that alternates updating the latent trajectory $x_{0:T}$ and the parameter $\\theta$. Starting only from Bayes’ rule, the linear-Gaussian model definitions above, and properties of Gaussian conditioning and marginalization, perform the following:\n\n1. Derive the full conditional distribution $p(x_{0:T} \\mid \\theta, y_{1:T})$ and explain why it is multivariate Gaussian. Provide the filtering-smoothing recursion needed to compute its mean vector and covariance structure without directly inverting large matrices.\n\n2. Derive the full conditional distribution $p(\\theta \\mid x_{0:T}, y_{1:T})$ and explain why the micro-scale observations $\\{y_{t}\\}$ do not enter this conditional once $\\{x_{t}\\}$ is known.\n\nYour derivations must be from first principles and must not invoke any named algorithm as a black box without justification from Gaussian identities. Conclude by giving the closed-form analytic expression for the posterior variance of $\\theta$ under $p(\\theta \\mid x_{0:T}, y_{1:T})$ as a function of $s_{0}$, $q$, and the sufficient statistics of the latent sequence $\\{x_{t}\\}$.\n\nProvide your final answer as the exact symbolic expression for the posterior variance of $\\theta$. No numerical evaluation is required. If you introduce any acronym, spell out its full name the first time you use it. The final answer must be a single closed-form analytic expression.",
            "solution": "The problem statement is scientifically grounded, well-posed, objective, and self-contained. It describes a standard application of Bayesian inference to a Linear Gaussian State-Space Model (LGSSM), a fundamental construct in time-series analysis and multiscale modeling. All parameters and distributions are clearly defined, and the tasks are mathematically sound and unambiguous. The problem is valid.\n\nWe proceed to derive the full conditional distributions required for a Gibbs sampler. The state vector is denoted by $x_{0:T} = (x_0, x_1, \\dots, x_T)'$ and the observation vector by $y_{1:T} = (y_1, \\dots, y_T)'$.\n\n**1. Derivation of the full conditional $p(x_{0:T} \\mid \\theta, y_{1:T})$**\n\nAccording to Bayes’ rule, the posterior distribution of the latent state trajectory $x_{0:T}$ given the parameter $\\theta$ and observations $y_{1:T}$ is:\n$$\np(x_{0:T} \\mid y_{1:T}, \\theta) \\propto p(y_{1:T}, x_{0:T} \\mid \\theta) = p(y_{1:T} \\mid x_{0:T}, \\theta) p(x_{0:T} \\mid \\theta)\n$$\nThe model structure implies certain conditional independencies. The observation $y_t$ depends only on the state $x_t$, and the state $x_t$ depends only on the previous state $x_{t-1}$ and $\\theta$.\nThe term $p(y_{1:T} \\mid x_{0:T}, \\theta)$ factorizes as a product of likelihoods for each observation, which are independent given the states:\n$$\np(y_{1:T} \\mid x_{0:T}, \\theta) = \\prod_{t=1}^{T} p(y_t \\mid x_t) = \\prod_{t=1}^{T} \\mathcal{N}(y_t; c x_t, r)\n$$\nThe term $p(x_{0:T} \\mid \\theta)$ represents the prior on the state trajectory, which factorizes according to the Markov property of the process:\n$$\np(x_{0:T} \\mid \\theta) = p(x_0) \\prod_{t=1}^{T} p(x_t \\mid x_{t-1}, \\theta) = \\mathcal{N}(x_0; \\mu_0, \\sigma_0) \\prod_{t=1}^{T} \\mathcal{N}(x_t; \\theta x_{t-1}, q)\n$$\nCombining these, the log-posterior is:\n$$\n\\ln p(x_{0:T} \\mid y_{1:T}, \\theta) = \\mathrm{const} + \\ln p(x_0) + \\sum_{t=1}^{T} \\ln p(x_t \\mid x_{t-1}, \\theta) + \\sum_{t=1}^{T} \\ln p(y_t \\mid x_t)\n$$\n$$\n\\ln p(x_{0:T} \\mid y_{1:T}, \\theta) = \\mathrm{const} - \\frac{1}{2\\sigma_0}(x_0 - \\mu_0)^2 - \\frac{1}{2q}\\sum_{t=1}^{T}(x_t - \\theta x_{t-1})^2 - \\frac{1}{2r}\\sum_{t=1}^{T}(y_t - c x_t)^2\n$$\nThis expression is a quadratic function of the elements of the vector $x_{0:T}$. A probability density whose logarithm is a quadratic function is a multivariate Gaussian distribution. Therefore, $p(x_{0:T} \\mid y_{1:T}, \\theta)$ is a multivariate Gaussian. The mean and covariance of this distribution are determined by the coefficients of the linear and quadratic terms. The covariance matrix is the inverse of the precision matrix, which can be seen from the quadratic terms to be sparse (tridiagonal).\n\nDirectly forming and inverting this $(T+1) \\times (T+1)$ precision matrix is computationally expensive for large $T$, scaling as $O(T^3)$. A more efficient method, scaling as $O(T)$, is to use a two-pass recursion known as the Kalman filter and smoother. We derive this from first principles using properties of Gaussian distributions.\n\n**Forward Pass (Kalman Filtering):**\nThe goal is to compute the marginal posteriors $p(x_t \\mid y_{1:t}, \\theta) = \\mathcal{N}(x_t; \\mu_{t|t}, \\sigma_{t|t})$ for $t=0, \\dots, T$.\nLet's initialize the recursion at $t=0$ with the prior on $x_0$: $\\mu_{0|0} = \\mu_0$ and $\\sigma_{0|0} = \\sigma_0$.\nFor $t=1, \\dots, T$:\n- **Prediction Step:** We compute the predictive distribution for $x_t$ given observations up to $t-1$: $p(x_t \\mid y_{1:t-1}, \\theta) = \\mathcal{N}(x_t; \\mu_{t|t-1}, \\sigma_{t|t-1})$.\n$$\np(x_t \\mid y_{1:t-1}, \\theta) = \\int p(x_t \\mid x_{t-1}, \\theta) p(x_{t-1} \\mid y_{1:t-1}, \\theta) dx_{t-1}\n$$\nThis is the convolution of two Gaussians: $p(x_{t-1} \\mid y_{1:t-1}, \\theta) = \\mathcal{N}(x_{t-1}; \\mu_{t-1|t-1}, \\sigma_{t-1|t-1})$ and $p(x_t \\mid x_{t-1}, \\theta) = \\mathcal{N}(x_t; \\theta x_{t-1}, q)$. The result is a Gaussian with mean and variance:\n$$\n\\mu_{t|t-1} = E[\\theta x_{t-1}] = \\theta \\mu_{t-1|t-1}\n$$\n$$\n\\sigma_{t|t-1} = \\mathrm{Var}(\\theta x_{t-1} + w_t) = \\theta^2 \\mathrm{Var}(x_{t-1}) + \\mathrm{Var}(w_t) = \\theta^2 \\sigma_{t-1|t-1} + q\n$$\n- **Update Step:** We update the predictive distribution using the new observation $y_t$. Using Bayes' rule:\n$$\np(x_t \\mid y_{1:t}, \\theta) \\propto p(y_t \\mid x_t) p(x_t \\mid y_{1:t-1}, \\theta)\n$$\nThis is a product of two Gaussian likelihoods for $x_t$: $\\mathcal{N}(y_t; cx_t, r)$ and $\\mathcal{N}(x_t; \\mu_{t|t-1}, \\sigma_{t|t-1})$. The result is a Gaussian $p(x_t \\mid y_{1:t}, \\theta) = \\mathcal{N}(x_t; \\mu_{t|t}, \\sigma_{t|t})$. Its parameters can be found by considering the joint distribution of $(x_t, y_t)$ given $y_{1:t-1}$ and $\\theta$. This joint is Gaussian, and we can use standard formulas for Gaussian conditioning.\nThe moments of the joint distribution $(x_t, y_t) \\mid y_{1:t-1}, \\theta$ are:\n$E[x_t] = \\mu_{t|t-1}$, $\\mathrm{Var}(x_t) = \\sigma_{t|t-1}$\n$E[y_t] = E[cx_t+v_t] = c\\mu_{t|t-1}$, $\\mathrm{Var}(y_t) = c^2\\mathrm{Var}(x_t) + \\mathrm{Var}(v_t) = c^2\\sigma_{t|t-1} + r$\n$\\mathrm{Cov}(x_t, y_t) = \\mathrm{Cov}(x_t, cx_t+v_t) = c\\mathrm{Var}(x_t) = c\\sigma_{t|t-1}$\nThe conditional distribution $p(x_t \\mid y_t, y_{1:t-1}, \\theta)$ has mean and variance:\n$$\n\\mu_{t|t} = \\mu_{t|t-1} + \\mathrm{Cov}(x_t, y_t)[\\mathrm{Var}(y_t)]^{-1}(y_t - E[y_t]) = \\mu_{t|t-1} + \\frac{c\\sigma_{t|t-1}}{c^2\\sigma_{t|t-1} + r}(y_t - c\\mu_{t|t-1})\n$$\n$$\n\\sigma_{t|t} = \\sigma_{t|t-1} - \\mathrm{Cov}(x_t, y_t)[\\mathrm{Var}(y_t)]^{-1}\\mathrm{Cov}(y_t, x_t) = \\sigma_{t|t-1} - \\frac{(c\\sigma_{t|t-1})^2}{c^2\\sigma_{t|t-1} + r}\n$$\n\n**Backward Pass (Rauch-Tung-Striebel Smoothing):**\nThis pass computes the smoothed distributions $p(x_t \\mid y_{1:T}, \\theta) = \\mathcal{N}(x_t; \\mu_{t|T}, \\sigma_{t|T})$ for $t=T-1, \\dots, 0$.\nThe recursion starts at $t=T$ with the final filtered estimates: $\\mu_{T|T}$ and $\\sigma_{T|T}$. We have:\n$$\np(x_t \\mid y_{1:T}, \\theta) = \\int p(x_t \\mid x_{t+1}, y_{1:T}, \\theta) p(x_{t+1} \\mid y_{1:T}, \\theta) dx_{t+1}\n$$\nBy the Markov property, $x_t \\perp y_{t+1:T} \\mid x_{t+1}$. Thus, $p(x_t \\mid x_{t+1}, y_{1:T}, \\theta) = p(x_t \\mid x_{t+1}, y_{1:t}, \\theta)$.\nWe use Bayes' rule to find this distribution: $p(x_t \\mid x_{t+1}, y_{1:t}) \\propto p(x_{t+1} \\mid x_t) p(x_t \\mid y_{1:t})$. The distributions on the right are Gaussians we already know. The joint distribution of $(x_t, x_{t+1})$ given $y_{1:t}$ is Gaussian. Its moments are:\n$E[x_t \\mid y_{1:t}] = \\mu_{t|t}$, $\\mathrm{Var}(x_t \\mid y_{1:t}) = \\sigma_{t|t}$\n$E[x_{t+1} \\mid y_{1:t}] = E[\\theta x_t+w_{t+1} \\mid y_{1:t}] = \\theta\\mu_{t|t} = \\mu_{t+1|t}$\n$\\mathrm{Cov}(x_t, x_{t+1} \\mid y_{1:t}) = \\mathrm{Cov}(x_t, \\theta x_t+w_{t+1} \\mid y_{1:t}) = \\theta \\sigma_{t|t}$\nConditioning $x_t$ on $x_{t+1}$ yields a Gaussian with mean\n$E[x_t \\mid x_{t+1}, y_{1:t}] = \\mu_{t|t} + \\theta\\sigma_{t|t}(\\sigma_{t+1|t})^{-1}(x_{t+1}-\\mu_{t+1|t})$.\nTaking the expectation over $p(x_{t+1} \\mid y_{1:T}, \\theta)$ gives the smoothed mean:\n$$\n\\mu_{t|T} = E[E[x_t \\mid x_{t+1}, y_{1:t}] \\mid y_{1:T}] = \\mu_{t|t} + J_t(\\mu_{t+1|T}-\\mu_{t+1|t})\n$$\nwhere $J_t = \\theta\\sigma_{t|t}(\\sigma_{t+1|t})^{-1} = \\frac{\\theta\\sigma_{t|t}}{\\theta^2\\sigma_{t|t}+q}$ is the smoother gain.\nUsing the law of total variance, the smoothed variance is:\n$$\n\\sigma_{t|T} = \\mathrm{Var}(x_t \\mid y_{1:t}) - (\\mathrm{...}) + J_t^2 \\mathrm{Var}(x_{t+1} \\mid y_{1:T}) = \\sigma_{t|t} + J_t^2(\\sigma_{t+1|T}-\\sigma_{t+1|t})\n$$\nThe mean vector of $p(x_{0:T} \\mid y_{1:T}, \\theta)$ is $(\\mu_{0|T}, \\dots, \\mu_{T|T})'$. The variances $\\sigma_{t|T}$ are the diagonal elements of the covariance matrix. The off-diagonal elements $\\sigma_{t,s|T} = \\mathrm{Cov}(x_t, x_s \\mid y_{1:T}, \\theta)$ for $t < s$ can be found via a similar recursion: $\\sigma_{t,s|T} = J_t \\sigma_{t+1, s|T}$. This implies $\\sigma_{t,s|T} = (\\prod_{k=t}^{s-1} J_k) \\sigma_{s|T}$. This fully specifies the covariance matrix.\n\n**2. Derivation of the full conditional $p(\\theta \\mid x_{0:T}, y_{1:T})$**\n\nWe apply Bayes' rule for the parameter $\\theta$:\n$$\np(\\theta \\mid x_{0:T}, y_{1:T}) \\propto p(y_{1:T}, x_{0:T}, \\theta) = p(y_{1:T} \\mid x_{0:T}, \\theta)p(x_{0:T} \\mid \\theta)p(\\theta)\n$$\nThe crucial insight is to analyze the term $p(y_{1:T} \\mid x_{0:T}, \\theta)$. From the model definition, $y_t = c x_t + v_t$. The distribution of $y_t$ given $x_t$ is $\\mathcal{N}(y_t; cx_t, r)$, which does not depend on $\\theta$. Consequently, the entire observation likelihood $p(y_{1:T} \\mid x_{0:T}, \\theta) = \\prod_{t=1}^T p(y_t \\mid x_t)$ does not depend on $\\theta$. Thus, given the latent path $x_{0:T}$, the observations $y_{1:T}$ provide no additional information about $\\theta$. This is a property of the directed acyclic graph representing the model, where $x_{0:T}$ forms a Markov blanket for $\\theta$ relative to $y_{1:T}$.\n$$\np(\\theta \\mid x_{0:T}, y_{1:T}) \\propto p(x_{0:T} \\mid \\theta) p(\\theta) = p(\\theta \\mid x_{0:T})\n$$\nWe now derive this posterior distribution.\n$$\np(\\theta \\mid x_{0:T}) \\propto p(\\theta) \\prod_{t=1}^{T} p(x_t \\mid x_{t-1}, \\theta)\n$$\nThe prior is $p(\\theta) = \\mathcal{N}(\\theta; m_0, s_0)$ and the likelihood terms are $p(x_t \\mid x_{t-1}, \\theta) = \\mathcal{N}(x_t; \\theta x_{t-1}, q)$. The log-posterior is:\n$$\n\\ln p(\\theta \\mid x_{0:T}) = \\mathrm{const} + \\ln p(\\theta) + \\sum_{t=1}^{T} \\ln p(x_t \\mid x_{t-1}, \\theta)\n$$\n$$\n= \\mathrm{const} - \\frac{1}{2s_0}(\\theta - m_0)^2 - \\frac{1}{2q}\\sum_{t=1}^{T}(x_t - \\theta x_{t-1})^2\n$$\nThis is a quadratic function of $\\theta$, so the posterior is Gaussian. Let's write it as $p(\\theta \\mid x_{0:T}) = \\mathcal{N}(\\theta; m_N, s_N)$. To find the posterior variance $s_N$, we identify the coefficient of the $\\theta^2$ term in the log-posterior.\n$$\n\\ln p(\\theta \\mid x_{0:T}) = \\mathrm{const} - \\frac{1}{2s_0}(\\theta^2 - \\dots) - \\frac{1}{2q}\\sum_{t=1}^{T}(\\theta^2 x_{t-1}^2 - \\dots)\n$$\nThe term quadratic in $\\theta$ is:\n$$\n-\\frac{1}{2}\\left(\\frac{1}{s_0} + \\frac{1}{q}\\sum_{t=1}^{T} x_{t-1}^2\\right)\\theta^2\n$$\nFor a Gaussian $\\mathcal{N}(m_N, s_N)$, the log-density has a quadratic term equal to $-\\frac{1}{2s_N}\\theta^2$. By matching coefficients, we find the inverse posterior variance:\n$$\n\\frac{1}{s_N} = \\frac{1}{s_0} + \\frac{1}{q}\\sum_{t=1}^{T} x_{t-1}^2\n$$\nThe posterior variance $s_N$ is therefore the reciprocal of this quantity. This expression depends on the prior variance $s_0$, the process noise variance $q$, and the sufficient statistic $\\sum_{t=1}^T x_{t-1}^2$ of the latent sequence.\n\nThe posterior variance of $\\theta$ is:\n$$\ns_N = \\left(\\frac{1}{s_0} + \\frac{1}{q}\\sum_{t=1}^{T} x_{t-1}^2\\right)^{-1}\n$$",
            "answer": "$$\n\\boxed{\\left(\\frac{1}{s_0} + \\frac{1}{q}\\sum_{t=1}^{T} x_{t-1}^{2}\\right)^{-1}}\n$$"
        }
    ]
}