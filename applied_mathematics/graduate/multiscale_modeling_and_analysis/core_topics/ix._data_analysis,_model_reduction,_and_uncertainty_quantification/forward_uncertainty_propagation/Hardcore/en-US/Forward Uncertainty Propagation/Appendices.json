{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of uncertainty analysis is the First-Order Second-Moment (FOSM) method, which leverages linearization to efficiently estimate output statistics. While simple and widely used, its accuracy is fundamentally limited by the degree of nonlinearity in the system. This practice  provides a crucial, hands-on exploration of these limitations by comparing the FOSM approximation directly against the exact analytical solution for an exponential response function, a common feature in models of growth or activation processes. By explicitly deriving the approximation error, you will gain a concrete understanding of how model curvature introduces biases that simple linearization cannot capture.",
            "id": "3761399",
            "problem": "In a multiscale constitutive model, a macroscale response $Y$ is induced by a microscale scalar descriptor $X$ through the mapping $Y = f(X)$, where $f(x) = \\exp(x)$. Suppose the microscale descriptor $X$ aggregates unresolved thermal fluctuations and is modeled as a Gaussian random variable $X \\sim \\mathcal{N}(0,\\sigma^{2})$ with known variance $ \\sigma^{2} $. You are tasked with forward uncertainty propagation of $X$ to quantify the mean and variance of $Y$.\n\nStarting from the definitions of expectation and variance in terms of integrals with respect to the probability density function and using only well-tested facts about the Gaussian distribution, do the following:\n\n1. Compute the exact expressions for $\\mathbb{E}[Y]$ and $\\mathrm{Var}[Y]$.\n2. Perform asymptotic expansions of the exact expressions for $\\mathbb{E}[Y]$ and $\\mathrm{Var}[Y]$ in powers of $\\sigma$ and retain terms up to order $O(\\sigma^{2})$.\n3. Using the First-Order Second-Moment (FOSM) approximation, linearize $f$ about the mean of $X$ and derive the corresponding approximations to $\\mathbb{E}[Y]$ and $\\mathrm{Var}[Y]$, retaining terms consistent up to order $O(\\sigma^{2})$.\n4. Define the leading-order comparison metrics $\\Delta_{\\mathrm{mean}} := \\mathbb{E}[Y]_{\\mathrm{exact}} - \\mathbb{E}[Y]_{\\mathrm{FOSM}}$ and $\\Delta_{\\mathrm{var}} := \\mathrm{Var}[Y]_{\\mathrm{exact}} - \\mathrm{Var}[Y]_{\\mathrm{FOSM}}$, each truncated to order $O(\\sigma^{2})$.\n\nReport your final result as a single row matrix containing the two expressions $\\Delta_{\\mathrm{mean}}$ and $\\Delta_{\\mathrm{var}}$ in that order. No numerical evaluation is required. Do not include units. Do not round. The final answer must be a closed-form symbolic expression in $\\sigma$ and must be expressed as a row matrix.",
            "solution": "The problem requires a forward propagation of uncertainty for a multiscale model. First, the problem is validated to ensure it is scientifically sound, well-posed, and complete.\n\n### Step 1: Problem Validation\n\nThe givens are:\n- The macroscale response is $Y = f(X)$, with the mapping $f(x) = \\exp(x)$.\n- The microscale descriptor is a random variable $X$ with a Gaussian distribution, $X \\sim \\mathcal{N}(0, \\sigma^2)$. This implies the mean is $\\mathbb{E}[X] = \\mu_X = 0$ and the variance is $\\mathrm{Var}[X] = \\sigma^2$.\n- The tasks are to:\n    1. Compute the exact expressions for the mean $\\mathbb{E}[Y]$ and variance $\\mathrm{Var}[Y]$.\n    2. Perform asymptotic expansions of these exact expressions up to order $O(\\sigma^2)$.\n    3. Derive the First-Order Second-Moment (FOSM) approximations for $\\mathbb{E}[Y]$ and $\\mathrm{Var}[Y]$ by linearizing $f(X)$.\n    4. Compute the leading-order differences $\\Delta_{\\mathrm{mean}} := \\mathbb{E}[Y]_{\\mathrm{exact}} - \\mathbb{E}[Y]_{\\mathrm{FOSM}}$ and $\\Delta_{\\mathrm{var}} := \\mathrm{Var}[Y]_{\\mathrm{exact}} - \\mathrm{Var}[Y]_{\\mathrm{FOSM}}$, truncated to order $O(\\sigma^2)$.\n\nThe problem is scientifically grounded in the principles of probability theory and uncertainty quantification. It is well-posed, with a clearly defined input distribution and mapping function, leading to a unique solution. The language is objective and the definitions are standard in the field. The setup is self-contained and internally consistent. Therefore, the problem is deemed valid and a solution can be constructed.\n\n### Step 2: Solution Derivation\n\nLet $X$ be a random variable with probability density function (PDF) $p(x)$ for a Gaussian distribution $\\mathcal{N}(\\mu_X, \\sigma^2)$:\n$$p(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu_X)^2}{2\\sigma^2}\\right)$$\nGiven $\\mu_X=0$, the PDF simplifies to $p(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{x^2}{2\\sigma^2}\\right)$.\n\n**Part 1: Exact Expressions for $\\mathbb{E}[Y]$ and $\\mathrm{Var}[Y]$**\n\nThe expectation of $Y=f(X)=\\exp(X)$ is computed by integrating over the PDF:\n$$\\mathbb{E}[Y] = \\mathbb{E}[\\exp(X)] = \\int_{-\\infty}^{\\infty} \\exp(x) p(x) \\,dx$$\nThis integral defines the moment-generating function (MGF) of $X$, $M_X(t) = \\mathbb{E}[\\exp(tX)]$, evaluated at $t=1$. A well-known property of the Gaussian distribution $X \\sim \\mathcal{N}(\\mu_X, \\sigma^2)$ is that its MGF is $M_X(t) = \\exp(\\mu_X t + \\frac{1}{2}\\sigma^2 t^2)$.\nWith $\\mu_X=0$, we have $M_X(t) = \\exp(\\frac{1}{2}\\sigma^2 t^2)$.\nTherefore, the exact mean of $Y$ is:\n$$\\mathbb{E}[Y]_{\\mathrm{exact}} = M_X(1) = \\exp\\left(\\frac{1}{2}\\sigma^2\\right)$$\nTo compute the variance, $\\mathrm{Var}[Y] = \\mathbb{E}[Y^2] - (\\mathbb{E}[Y])^2$, we first need $\\mathbb{E}[Y^2]$.\n$$Y^2 = (\\exp(X))^2 = \\exp(2X)$$\nThe expectation of $Y^2$ is found by evaluating the MGF at $t=2$:\n$$\\mathbb{E}[Y^2] = \\mathbb{E}[\\exp(2X)] = M_X(2) = \\exp\\left(\\frac{1}{2}\\sigma^2 (2)^2\\right) = \\exp(2\\sigma^2)$$\nNow, the exact variance of $Y$ is:\n$$\\mathrm{Var}[Y]_{\\mathrm{exact}} = \\mathbb{E}[Y^2] - (\\mathbb{E}[Y])^2 = \\exp(2\\sigma^2) - \\left[\\exp\\left(\\frac{1}{2}\\sigma^2\\right)\\right]^2 = \\exp(2\\sigma^2) - \\exp(\\sigma^2)$$\n\n**Part 2: Asymptotic Expansions up to order $O(\\sigma^2)$**\n\nWe use the Taylor series expansion for the exponential function, $\\exp(z) = 1 + z + \\frac{z^2}{2!} + O(z^3)$, for small $z$.\nFor the mean, let $z = \\frac{1}{2}\\sigma^2$. The expansion of the exact mean for small $\\sigma$ is:\n$$\\mathbb{E}[Y]_{\\mathrm{exact}} = \\exp\\left(\\frac{1}{2}\\sigma^2\\right) = 1 + \\frac{1}{2}\\sigma^2 + \\frac{1}{2!}\\left(\\frac{1}{2}\\sigma^2\\right)^2 + \\dots = 1 + \\frac{1}{2}\\sigma^2 + O(\\sigma^4)$$\nRetaining terms up to order $O(\\sigma^2)$, we get the approximation:\n$$\\mathbb{E}[Y]_{\\mathrm{exact}, O(\\sigma^2)} = 1 + \\frac{1}{2}\\sigma^2$$\nFor the variance, we expand both exponential terms:\n$$\\exp(2\\sigma^2) = 1 + 2\\sigma^2 + \\frac{(2\\sigma^2)^2}{2} + O(\\sigma^6) = 1 + 2\\sigma^2 + 2\\sigma^4 + O(\\sigma^6)$$\n$$\\exp(\\sigma^2) = 1 + \\sigma^2 + \\frac{(\\sigma^2)^2}{2} + O(\\sigma^6) = 1 + \\sigma^2 + \\frac{1}{2}\\sigma^4 + O(\\sigma^6)$$\nSubstituting these into the expression for variance:\n$$\\mathrm{Var}[Y]_{\\mathrm{exact}} = (1 + 2\\sigma^2 + 2\\sigma^4 + \\dots) - (1 + \\sigma^2 + \\frac{1}{2}\\sigma^4 + \\dots) = \\sigma^2 + \\frac{3}{2}\\sigma^4 + O(\\sigma^6)$$\nRetaining terms up to order $O(\\sigma^2)$, we get the approximation:\n$$\\mathrm{Var}[Y]_{\\mathrm{exact}, O(\\sigma^2)} = \\sigma^2$$\n\n**Part 3: First-Order Second-Moment (FOSM) Approximation**\n\nThe FOSM method, as per the instruction to \"linearize $f$\", involves approximating $f(X)$ with its first-order Taylor expansion around the mean of $X$, $\\mu_X = 0$.\nThe function is $f(x) = \\exp(x)$, and its derivative is $f'(x) = \\exp(x)$.\nAt $\\mu_X = 0$: $f(\\mu_X) = \\exp(0) = 1$ and $f'(\\mu_X) = \\exp(0) = 1$.\nThe linearized function is:\n$$Y_{\\mathrm{FOSM}} = f(\\mu_X) + f'(\\mu_X)(X - \\mu_X) = 1 + 1(X - 0) = 1 + X$$\nThe FOSM approximations for the mean and variance are the mean and variance of this linearized function:\n$$\\mathbb{E}[Y]_{\\mathrm{FOSM}} = \\mathbb{E}[1 + X] = 1 + \\mathbb{E}[X] = 1 + 0 = 1$$\n$$\\mathrm{Var}[Y]_{\\mathrm{FOSM}} = \\mathrm{Var}[1 + X] = \\mathrm{Var}[X] = \\sigma^2$$\n\n**Part 4: Leading-Order Comparison Metrics**\n\nThe metrics are defined as $\\Delta_{\\mathrm{mean}} := \\mathbb{E}[Y]_{\\mathrm{exact}} - \\mathbb{E}[Y]_{\\mathrm{FOSM}}$ and $\\Delta_{\\mathrm{var}} := \\mathrm{Var}[Y]_{\\mathrm{exact}} - \\mathrm{Var}[Y]_{\\mathrm{FOSM}}$, truncated to order $O(\\sigma^2)$.\n\nFor the mean:\n$$\\Delta_{\\mathrm{mean}} = \\mathbb{E}[Y]_{\\mathrm{exact}} - \\mathbb{E}[Y]_{\\mathrm{FOSM}} = \\exp\\left(\\frac{1}{2}\\sigma^2\\right) - 1$$\nUsing the series expansion:\n$$\\Delta_{\\mathrm{mean}} = \\left(1 + \\frac{1}{2}\\sigma^2 + \\frac{1}{8}\\sigma^4 + \\dots\\right) - 1 = \\frac{1}{2}\\sigma^2 + \\frac{1}{8}\\sigma^4 + \\dots$$\nTruncating this result to order $O(\\sigma^2)$ yields:\n$$\\Delta_{\\mathrm{mean}} = \\frac{1}{2}\\sigma^2$$\n\nFor the variance:\n$$\\Delta_{\\mathrm{var}} = \\mathrm{Var}[Y]_{\\mathrm{exact}} - \\mathrm{Var}[Y]_{\\mathrm{FOSM}} = (\\exp(2\\sigma^2) - \\exp(\\sigma^2)) - \\sigma^2$$\nUsing the series expansion:\n$$\\Delta_{\\mathrm{var}} = \\left(\\sigma^2 + \\frac{3}{2}\\sigma^4 + O(\\sigma^6)\\right) - \\sigma^2 = \\frac{3}{2}\\sigma^4 + O(\\sigma^6)$$\nTruncating this result to order $O(\\sigma^2)$ implies discarding all terms with powers of $\\sigma$ greater than $2$. The leading term is of order $\\sigma^4$, so the truncated result is:\n$$\\Delta_{\\mathrm{var}} = 0$$\n\nThe final result is the row matrix containing these two expressions in the specified order.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{2}\\sigma^{2}  0\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "When analytical methods are infeasible, we often turn to sampling-based approaches to explore the impact of input uncertainties. However, simple random sampling can be inefficient. This exercise introduces Latin Hypercube Sampling (LHS), a powerful stratified sampling technique designed to ensure that samples are spread more evenly across the input parameter space. In this constructive problem , you will build a small LHS design from first principles and master the inverse transform method used to map these samples to arbitrary input distributions, a fundamental skill for applying advanced sampling techniques to real-world models.",
            "id": "3761383",
            "problem": "A three-scale simulator models a macro-scale quantity $Y$ as $Y = \\mathcal{G}(X_{1}, X_{2}, X_{3})$, where the micro-scale inputs $(X_{1}, X_{2}, X_{3})$ are independent random variables with continuous marginal cumulative distribution functions (CDFs) $F_{1}$, $F_{2}$, and $F_{3}$. In forward uncertainty propagation, a stratified sampling design is often used to control variance while preserving marginal coverage. Latin Hypercube Sampling (LHS) is a stratified design over the unit hypercube that ensures, in each coordinate, exactly one sample per stratum.\n\nStarting from the foundational definitions of the cumulative distribution function and the inverse transform sampling principle, construct a deterministic Latin Hypercube Sampling (LHS) design in dimension $d=3$ with $N=5$ samples on the unit hypercube $[0,1]^{3}$ using the following canonical choices:\n- Use the stratum centers $c_{j} = (j - 1/2)/5$ for $j \\in \\{1,2,3,4,5\\}$.\n- Use the coordinate-wise permutations $p^{(1)} = (1,2,3,4,5)$, $p^{(2)} = (3,5,1,4,2)$, and $p^{(3)} = (5,2,4,1,3)$, where $p^{(i)}(k)$ denotes the stratum index assigned in dimension $i$ to sample $k \\in \\{1,2,3,4,5\\}$.\n\nThen, derive from first principles how to map this LHS design to the arbitrary input marginals $F_{1}$, $F_{2}$, and $F_{3}$ using inverse CDF transforms, and justify that the resulting samples have the correct one-dimensional marginals.\n\nReport your canonical stratified design as a single row matrix of length $15$ by listing the entries row-wise (sample $1$ through sample $5$), with each entry given in exact decimal form. No rounding is required and no physical units are involved.",
            "solution": "The problem is valid as it is scientifically grounded in established statistical principles, well-posed with all necessary information provided, and objective in its formulation. We will proceed with a full solution.\n\nThe solution requires constructing a Latin Hypercube Sampling (LHS) design and justifying its properties based on first principles. We begin by defining the foundational concepts.\n\nA cumulative distribution function (CDF) $F_{X}$ of a random variable $X$ is defined as $F_{X}(x) = P(X \\le x)$ for any $x \\in \\mathbb{R}$. The problem states that the marginal CDFs $F_{1}$, $F_{2}$, and $F_{3}$ are continuous.\n\nThe inverse transform sampling principle is a method to generate a sample of a random variable $X$ from its CDF $F_{X}$. Given a random variable $U$ uniformly distributed on the interval $[0,1]$, a random variable $X$ with CDF $F_{X}$ can be generated as $X = F_{X}^{-1}(U)$, where $F_{X}^{-1}$ is the inverse CDF, or quantile function, defined as $F_{X}^{-1}(p) = \\inf\\{x \\in \\mathbb{R} : F_{X}(x) \\ge p\\}$ for $p \\in [0,1]$. For a continuous CDF $F_X$, the random variable $F_X(X)$ is uniformly distributed on $[0,1]$. Conversely, if $U \\sim \\text{Uniform}[0,1]$, the random variable $X = F_X^{-1}(U)$ has the CDF $F_X$. To prove this, we find the CDF of $X$:\n$$P(X \\le x) = P(F_{X}^{-1}(U) \\le x)$$\nBecause $F_X$ is a non-decreasing function, the inequality $F_{X}^{-1}(u) \\le x$ is equivalent to $u \\le F_{X}(x)$. This holds because $F_X$ is continuous. Thus,\n$$P(F_{X}^{-1}(U) \\le x) = P(U \\le F_{X}(x))$$\nSince $U$ is a standard uniform random variable, its CDF is $F_{U}(u) = u$ for $u \\in [0,1]$. Therefore,\n$$P(U \\le F_{X}(x)) = F_{X}(x)$$\nThis confirms that $X=F_X^{-1}(U)$ has the desired CDF $F_X$. This principle is central to mapping samples from the unit hypercube to an arbitrary probability space.\n\nNext, we construct the Latin Hypercube Sampling (LHS) design. An LHS design with $N$ samples in $d$ dimensions is a set of $N$ points $\\{\\mathbf{u}_k\\}_{k=1}^{N}$ in the unit hypercube $[0,1]^d$ such that for each dimension $i \\in \\{1, \\dots, d\\}$, the set of coordinates $\\{u_{ki}\\}_{k=1}^{N}$ contains exactly one value from each stratum $[(j-1)/N, j/N)$ for $j \\in \\{1, \\dots, N\\}$.\n\nIn this problem, the dimension is $d=3$ and the number of samples is $N=5$. The unit hypercube is $[0,1]^3$. Each dimension is partitioned into $N=5$ strata: $[0, 1/5)$, $[1/5, 2/5)$, $[2/5, 3/5)$, $[3/5, 4/5)$, and $[4/5, 1]$.\n\nThe problem specifies a deterministic construction using stratum centers and given permutations. The center of the $j$-th stratum is given by the formula $c_j = (j - 1/2)/N$. For $N=5$, the stratum centers are:\n$c_{1} = (1 - 0.5)/5 = 0.5/5 = 0.1$\n$c_{2} = (2 - 0.5)/5 = 1.5/5 = 0.3$\n$c_{3} = (3 - 0.5)/5 = 2.5/5 = 0.5$\n$c_{4} = (4 - 0.5)/5 = 3.5/5 = 0.7$\n$c_{5} = (5 - 0.5)/5 = 4.5/5 = 0.9$\n\nThe $k$-th sample point is $\\mathbf{u}_k = (u_{k1}, u_{k2}, u_{k3})$. The coordinate $u_{ki}$ for sample $k$ in dimension $i$ is determined by choosing the center of the stratum indexed by $p^{(i)}(k)$, where $p^{(i)}$ is the given permutation for the $i$-th dimension. The formula is $u_{ki} = c_{p^{(i)}(k)} = (p^{(i)}(k) - 0.5)/5$.\n\nThe given permutations for $k \\in \\{1,2,3,4,5\\}$ are:\n$p^{(1)} = (1,2,3,4,5)$\n$p^{(2)} = (3,5,1,4,2)$\n$p^{(3)} = (5,2,4,1,3)$\n\nWe now compute the $N=5$ sample points:\nFor sample $k=1$:\n$u_{11} = c_{p^{(1)}(1)} = c_{1} = 0.1$\n$u_{12} = c_{p^{(2)}(1)} = c_{3} = 0.5$\n$u_{13} = c_{p^{(3)}(1)} = c_{5} = 0.9$\nSo, $\\mathbf{u}_1 = (0.1, 0.5, 0.9)$.\n\nFor sample $k=2$:\n$u_{21} = c_{p^{(1)}(2)} = c_{2} = 0.3$\n$u_{22} = c_{p^{(2)}(2)} = c_{5} = 0.9$\n$u_{23} = c_{p^{(3)}(2)} = c_{2} = 0.3$\nSo, $\\mathbf{u}_2 = (0.3, 0.9, 0.3)$.\n\nFor sample $k=3$:\n$u_{31} = c_{p^{(1)}(3)} = c_{3} = 0.5$\n$u_{32} = c_{p^{(2)}(3)} = c_{1} = 0.1$\n$u_{33} = c_{p^{(3)}(3)} = c_{4} = 0.7$\nSo, $\\mathbf{u}_3 = (0.5, 0.1, 0.7)$.\n\nFor sample $k=4$:\n$u_{41} = c_{p^{(1)}(4)} = c_{4} = 0.7$\n$u_{42} = c_{p^{(2)}(4)} = c_{4} = 0.7$\n$u_{43} = c_{p^{(3)}(4)} = c_{1} = 0.1$\nSo, $\\mathbf{u}_4 = (0.7, 0.7, 0.1)$.\n\nFor sample $k=5$:\n$u_{51} = c_{p^{(1)}(5)} = c_{5} = 0.9$\n$u_{52} = c_{p^{(2)}(5)} = c_{2} = 0.3$\n$u_{53} = c_{p^{(3)}(5)} = c_{3} = 0.5$\nSo, $\\mathbf{u}_5 = (0.9, 0.3, 0.5)$.\n\nThe resulting canonical stratified design on the unit hypercube $[0,1]^3$ is the set of these $5$ points, which can be represented as a matrix:\n$$\nU = \\begin{pmatrix}\n0.1  0.5  0.9 \\\\\n0.3  0.9  0.3 \\\\\n0.5  0.1  0.7 \\\\\n0.7  0.7  0.1 \\\\\n0.9  0.3  0.5\n\\end{pmatrix}\n$$\n\nTo map this design to samples from the joint distribution of $(X_1, X_2, X_3)$, we apply the inverse transform method component-wise for each sample point $\\mathbf{u}_k$. The corresponding sample in the physical space, $\\mathbf{x}_k = (x_{k1}, x_{k2}, x_{k3})$, is obtained as:\n$$x_{ki} = F_{i}^{-1}(u_{ki})$$\nfor $k \\in \\{1, \\dots, 5\\}$ and $i \\in \\{1, 2, 3\\}$. This maps the LHS design from the unit hypercube to the input space defined by the marginal CDFs $F_1, F_2, F_3$. The independence of the input variables $X_i$ is a prerequisite for this component-wise transformation to be a valid representation of the joint distribution.\n\nFinally, we must justify that the resulting samples $\\{\\mathbf{x}_k\\}_{k=1}^5$ have the correct one-dimensional marginals. Let's consider an arbitrary dimension $i \\in \\{1,2,3\\}$. The set of coordinates $\\{u_{ki}\\}_{k=1}^5$ is constructed using the permutation $p^{(i)}$ of the indices $\\{1,2,3,4,5\\}$. By definition of a permutation, the set of indices $\\{p^{(i)}(1), p^{(i)}(2), p^{(i)}(3), p^{(i)}(4), p^{(i)}(5)\\}$ is simply a reordering of $\\{1,2,3,4,5\\}$.\nConsequently, the set of coordinate values in dimension $i$, $\\{u_{ki}\\}_{k=1}^5 = \\{ c_{p^{(i)}(k)} \\}_{k=1}^5$, is a reordering of the set of stratum centers $\\{c_1, c_2, c_3, c_4, c_5\\}$. Specifically, for any $i$, the set $\\{u_{1i}, u_{2i}, u_{3i}, u_{4i}, u_{5i}\\}$ is equal to the set $\\{0.1, 0.3, 0.5, 0.7, 0.9\\}$.\nThis means that for each dimension $i$, the design has exactly one point in each of the $N=5$ strata $[0, 1/5), \\dots, [4/5, 1)$.\n\nThe input variables $X_i$ have continuous CDFs $F_i$. The probability mass of $X_i$ in the interval corresponding to the $j$-th stratum is:\n$$P(x_i \\in [F_i^{-1}((j-1)/N), F_i^{-1}(j/N)]) = P((j-1)/N \\le F_i(X_i) \\le j/N)$$\nSince $F_i(X_i)$ is uniformly distributed on $[0,1]$, this probability is equal to $j/N - (j-1)/N = 1/N$. Thus, the marginal distribution $F_i$ is partitioned into $N$ equiprobable bins.\nThe LHS construction ensures that the generated samples $\\{x_{ki}\\}_{k=1}^N = \\{F_i^{-1}(u_{ki})\\}_{k=1}^N$ place exactly one sample point into each of these $N$ equiprobable bins. This property, known as marginal stratification, ensures that the samples in each dimension are well-spread across the entire support of the probability distribution, providing excellent one-dimensional coverage. This is the precise sense in which the LHS design respects the one-dimensional marginals.\n\nThe final answer requires reporting the design matrix $U$ as a single row-wise flattened vector. This is:\n$(u_{11}, u_{12}, u_{13}, u_{21}, u_{22}, u_{23}, u_{31}, u_{32}, u_{33}, u_{41}, u_{42}, u_{43}, u_{51}, u_{52}, u_{53})$\n$= (0.1, 0.5, 0.9, 0.3, 0.9, 0.3, 0.5, 0.1, 0.7, 0.7, 0.7, 0.1, 0.9, 0.3, 0.5)$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.1  0.5  0.9  0.3  0.9  0.3  0.5  0.1  0.7  0.7  0.7  0.1  0.9  0.3  0.5\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Real-world multiscale models often present a daunting combination of physical complexity and high-dimensional uncertainty, seemingly requiring sophisticated computational machinery. This capstone practice  simulates such a scenario, involving a partial differential equation with random inputs where an advanced method like sparse-grid collocation appears necessary. The core pedagogical goal here is to highlight the immense power of analytical insight; a careful derivation reveals that the quantity of interest is a linear function of the inputs, making the problem exactly solvable by the numerical scheme. This exercise underscores a vital lesson for researchers: analytical pre-processing is not just an academic formality but a powerful tool for simplifying complex problems and validating computational results.",
            "id": "3761300",
            "problem": "Consider a one-dimensional elliptic Partial Differential Equation (PDE) with multiscale structure and random boundary conditions. The microscale is characterized by a periodic diffusion coefficient $a(y)$ defined for $y \\in [0,1]$ by $a(y) = 1 + \\rho \\cos(2\\pi y)$, where $0  \\rho  1$ is a deterministic parameter that controls the amplitude of microscale oscillations. The macroscale homogenized diffusion coefficient $a^{\\ast}$ is given by a standard formula from one-dimensional periodic homogenization, and the macroscale PDE with random boundary conditions is\n$$\n- \\left( a^{\\ast} u'(x;\\boldsymbol{\\xi}) \\right)' = g(x;\\boldsymbol{\\xi}), \\quad x \\in (0,1),\n$$\nwith boundary conditions $u(0;\\boldsymbol{\\xi}) = \\xi_1$ and $u(1;\\boldsymbol{\\xi}) = \\xi_2$. The source term is\n$$\ng(x;\\boldsymbol{\\xi}) = c_0 + A \\, \\xi_3 \\sin(\\pi x) + B \\, \\xi_4 \\cos(2\\pi x),\n$$\nand the Quantity of Interest (QoI) is the spatial average\n$$\nQ(\\boldsymbol{\\xi}) = \\int_0^1 u(x;\\boldsymbol{\\xi}) \\, dx,\n$$\nwhere $\\boldsymbol{\\xi} = (\\xi_1,\\xi_2,\\xi_3,\\xi_4)$ has independent components uniformly distributed on $[-1,1]$. All angles are in radians. The task is forward uncertainty propagation: compute an approximation of the mean $\\mathbb{E}[Q(\\boldsymbol{\\xi})]$ under the uniform distribution on $[-1,1]^4$ using a nested sparse-grid collocation (Smolyak) quadrature based on one-dimensional nested trapezoidal rules.\n\nDefinitions and requirements:\n- The homogenized coefficient in one dimension is defined by\n$$\na^{\\ast} = \\left( \\int_0^1 \\frac{1}{a(y)} \\, dy \\right)^{-1}.\n$$\n- The program must implement a nested one-dimensional quadrature rule $Q_{\\ell}$ at level $\\ell \\in \\{1,2,\\dots,L\\}$ using a uniform grid with $n(\\ell) = 2^{\\ell-1} + 1$ nodes on $[-1,1]$, with trapezoidal weights adapted to the uniform probability density on $[-1,1]$. Specifically, for step size $h = \\frac{2}{n(\\ell)-1}$ and nodes $x_j = -1 + j h$ for $j \\in \\{0,1,\\dots,n(\\ell)-1\\}$, the one-dimensional expectation weights are $w_0 = \\frac{h}{4}$, $w_{n(\\ell)-1} = \\frac{h}{4}$, and $w_j = \\frac{h}{2}$ for $j \\in \\{1,\\dots,n(\\ell)-2\\}$.\n- Use the Smolyak construction with level $L$ and dimension $d = 4$ to form the $d$-dimensional quadrature $A(L,d)$ applied to $Q(\\boldsymbol{\\xi})$. Let $Q_{\\ell}$ denote the one-dimensional quadrature at level $\\ell$ and define the standard Smolyak combination with multi-index $\\mathbf{i} = (i_1,\\dots,i_d)$, $i_k \\in \\{1,\\dots,L\\}$, restricted by\n$$\n\\max(d, L - d + 1) \\leq \\lVert \\mathbf{i} \\rVert_1 \\leq L,\n$$\nand combination coefficient\n$$\nc(\\mathbf{i}) = (-1)^{L - \\lVert \\mathbf{i} \\rVert_1} \\binom{d - 1}{L - \\lVert \\mathbf{i} \\rVert_1}.\n$$\nThe quadrature $A(L,d)$ applied to $Q$ is the weighted sum over tensor-product nodes formed by $Q_{i_1} \\otimes \\cdots \\otimes Q_{i_d}$ and weights given by the product of the one-dimensional expectation weights.\n- The a posteriori error bound must be estimated as the absolute difference between the level-$L$ and level-$(L-1)$ Smolyak quadrature approximations, that is,\n$$\n\\text{err}(L) = \\left| A(L,d)[Q] - A(L-1,d)[Q] \\right|,\n$$\nwith the convention $A(L',d) = 0$ if $L'  d$ yields an empty index set under the above restriction.\n\nImplement the above algorithm and apply it to the following test suite. For each test case, compute:\n- The approximate mean $\\widehat{\\mu} = A(L,d)[Q]$ as a float.\n- The a posteriori error bound $\\widehat{e} = \\text{err}(L)$ as a float.\n\nTest suite parameter sets $(\\rho, c_0, A, B, L)$:\n1. $(\\rho, c_0, A, B, L) = (0.5, 2.0, 1.0, 0.5, 5)$.\n2. $(\\rho, c_0, A, B, L) = (0.95, 1.0, 0.8, 1.3, 6)$.\n3. $(\\rho, c_0, A, B, L) = (0.1, 5.0, 0.0, 0.0, 4)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is a two-element list $[\\widehat{\\mu},\\widehat{e}]$. For example, a valid output format is\n$$\n[\\,[\\widehat{\\mu}_1,\\widehat{e}_1],[\\widehat{\\mu}_2,\\widehat{e}_2],[\\widehat{\\mu}_3,\\widehat{e}_3]\\,].\n$$\nAll numbers should be printed in floating-point format with the Python default representation. No physical units are involved; report pure numbers.",
            "solution": "The problem requires the computation of the mean and an a posteriori error estimate for a Quantity of Interest (QoI) that depends on the solution to a one-dimensional elliptic PDE with random boundary conditions and a random source term. The method specified is a nested sparse-grid (Smolyak) quadrature.\n\nThis problem, while appearing computationally intensive, possesses a remarkably simple underlying structure. The solution strategy is to first derive an analytical expression for the Quantity of Interest, $Q(\\boldsymbol{\\xi})$, and then analyze its properties. This analysis will reveal that the specified numerical machinery, when correctly applied, yields a result that can be predicted analytically, providing a powerful means of verification.\n\n**Step 1: Derivation of the Homogenized Coefficient $a^{\\ast}$**\n\nThe microscale diffusion coefficient is given by $a(y) = 1 + \\rho \\cos(2\\pi y)$. The homogenized coefficient $a^{\\ast}$ is defined by the harmonic average:\n$$\na^{\\ast} = \\left( \\int_0^1 \\frac{1}{a(y)} \\, dy \\right)^{-1} = \\left( \\int_0^1 \\frac{1}{1 + \\rho \\cos(2\\pi y)} \\, dy \\right)^{-1}\n$$\nThis integral can be evaluated by letting $\\theta = 2\\pi y$, which gives $dy = d\\theta / (2\\pi)$. The integration limits for $\\theta$ are from $0$ to $2\\pi$.\n$$\n\\int_0^1 \\frac{1}{1 + \\rho \\cos(2\\pi y)} \\, dy = \\frac{1}{2\\pi} \\int_0^{2\\pi} \\frac{1}{1 + \\rho \\cos(\\theta)} \\, d\\theta\n$$\nThis is a standard definite integral with the known solution, for $|b|  |a|$, $\\int_0^{2\\pi} \\frac{d\\theta}{a+b\\cos\\theta} = \\frac{2\\pi}{\\sqrt{a^2-b^2}}$. For our case, $a=1$ and $b=\\rho$. The problem states $0  \\rho  1$, so the condition is satisfied.\n$$\n\\frac{1}{2\\pi} \\int_0^{2\\pi} \\frac{1}{1 + \\rho \\cos(\\theta)} \\, d\\theta = \\frac{1}{2\\pi} \\frac{2\\pi}{\\sqrt{1^2 - \\rho^2}} = \\frac{1}{\\sqrt{1-\\rho^2}}\n$$\nTherefore, the homogenized coefficient is:\n$$\na^{\\ast} = \\left( \\frac{1}{\\sqrt{1-\\rho^2}} \\right)^{-1} = \\sqrt{1-\\rho^2}\n$$\n\n**Step 2: Analytical Solution for the Quantity of Interest $Q(\\boldsymbol{\\xi})$**\n\nThe macroscale problem is the boundary value problem:\n$$\n-a^{\\ast} u''(x;\\boldsymbol{\\xi}) = g(x;\\boldsymbol{\\xi}), \\quad u(0;\\boldsymbol{\\xi}) = \\xi_1, \\quad u(1;\\boldsymbol{\\xi}) = \\xi_2\n$$\nwhere $g(x;\\boldsymbol{\\xi}) = c_0 + A \\xi_3 \\sin(\\pi x) + B \\xi_4 \\cos(2\\pi x)$.\nSince $a^{\\ast}$ is a constant, we can integrate twice to find $u(x;\\boldsymbol{\\xi})$. First integration yields:\n$$\n-a^{\\ast} u'(x;\\boldsymbol{\\xi}) = \\int g(x;\\boldsymbol{\\xi}) \\, dx = c_0 x - \\frac{A \\xi_3}{\\pi} \\cos(\\pi x) + \\frac{B \\xi_4}{2\\pi} \\sin(2\\pi x) + K_1\n$$\nSecond integration gives:\n$$\n-a^{\\ast} u(x;\\boldsymbol{\\xi}) = \\frac{c_0}{2} x^2 - \\frac{A \\xi_3}{\\pi^2} \\sin(\\pi x) - \\frac{B \\xi_4}{4\\pi^2} \\cos(2\\pi x) + K_1 x + K_2\n$$\nThe constants $K_1$ and $K_2$ are determined by the boundary conditions.\nAt $x=0$: $-a^{\\ast} u(0;\\boldsymbol{\\xi}) = -a^{\\ast}\\xi_1 = -\\frac{B \\xi_4}{4\\pi^2} + K_2$, so $K_2 = -a^{\\ast}\\xi_1 + \\frac{B \\xi_4}{4\\pi^2}$.\nAt $x=1$: $-a^{\\ast} u(1;\\boldsymbol{\\xi}) = -a^{\\ast}\\xi_2 = \\frac{c_0}{2} - \\frac{B \\xi_4}{4\\pi^2} + K_1 + K_2$.\nSubstituting $K_2$: $K_1 = -a^{\\ast}\\xi_2 - \\frac{c_0}{2} + \\frac{B \\xi_4}{4\\pi^2} - K_2 = -a^{\\ast}\\xi_2 - \\frac{c_0}{2} - (-a^{\\ast}\\xi_1) = a^{\\ast}(\\xi_1 - \\xi_2) - \\frac{c_0}{2}$.\n\nSubstituting $K_1$ and $K_2$ back into the expression for $u(x;\\boldsymbol{\\xi})$ gives the full solution. However, we need the QoI, $Q(\\boldsymbol{\\xi}) = \\int_0^1 u(x;\\boldsymbol{\\xi}) dx$. We can find this more directly.\nFrom the PDE, integrating over $(0,1)$ gives:\n$$\n-a^{\\ast} \\int_0^1 u''(x) \\, dx = \\int_0^1 g(x) \\, dx\n$$\n$$\n-a^{\\ast} [u'(1) - u'(0)] = \\int_0^1 (c_0 + A \\xi_3 \\sin(\\pi x) + B \\xi_4 \\cos(2\\pi x)) \\, dx\n$$\nThe integral of the source is $[c_0 x - \\frac{A\\xi_3}{\\pi}\\cos(\\pi x) + \\frac{B\\xi_4}{2\\pi}\\sin(2\\pi x)]_0^1 = (c_0 - \\frac{A\\xi_3}{\\pi}(-1)) - (-\\frac{A\\xi_3}{\\pi}) = c_0 + \\frac{2A\\xi_3}{\\pi}$.\n\nTo find the QoI, we integrate the expression for $-a^{\\ast} u(x;\\boldsymbol{\\xi})$:\n$$\n-a^{\\ast} Q(\\boldsymbol{\\xi}) = \\int_0^1 \\left( \\frac{c_0}{2} x^2 - \\frac{A \\xi_3}{\\pi^2} \\sin(\\pi x) - \\frac{B \\xi_4}{4\\pi^2} \\cos(2\\pi x) + K_1 x + K_2 \\right) dx\n$$\n$$\n-a^{\\ast} Q(\\boldsymbol{\\xi}) = \\left[ \\frac{c_0}{6} x^3 + \\frac{A \\xi_3}{\\pi^3} \\cos(\\pi x) - \\frac{B \\xi_4}{8\\pi^3} \\sin(2\\pi x) + \\frac{K_1}{2} x^2 + K_2 x \\right]_0^1\n$$\n$$\n-a^{\\ast} Q(\\boldsymbol{\\xi}) = (\\frac{c_0}{6} - \\frac{A \\xi_3}{\\pi^3} + \\frac{K_1}{2} + K_2) - (\\frac{A \\xi_3}{\\pi^3}) = \\frac{c_0}{6} - \\frac{2A \\xi_3}{\\pi^3} + \\frac{K_1}{2} + K_2\n$$\nSubstituting $K_1=a^{\\ast}(\\xi_1 - \\xi_2) - \\frac{c_0}{2}$ and $K_2=-a^{\\ast}\\xi_1 + \\frac{B \\xi_4}{4\\pi^2}$:\n$$\n-a^{\\ast} Q(\\boldsymbol{\\xi}) = \\frac{c_0}{6} - \\frac{2A \\xi_3}{\\pi^3} + \\frac{1}{2}(a^{\\ast}(\\xi_1 - \\xi_2) - \\frac{c_0}{2}) + (-a^{\\ast}\\xi_1 + \\frac{B \\xi_4}{4\\pi^2})\n$$\n$$\n-a^{\\ast} Q(\\boldsymbol{\\xi}) = \\frac{c_0}{6} - \\frac{c_0}{4} - \\frac{2A \\xi_3}{\\pi^3} + \\frac{B \\xi_4}{4\\pi^2} + \\frac{a^{\\ast}}{2}\\xi_1 - \\frac{a^{\\ast}}{2}\\xi_2 - a^{\\ast}\\xi_1\n$$\n$$\n-a^{\\ast} Q(\\boldsymbol{\\xi}) = -\\frac{c_0}{12} - \\frac{2A \\xi_3}{\\pi^3} + \\frac{B \\xi_4}{4\\pi^2} - \\frac{a^{\\ast}}{2}\\xi_1 - \\frac{a^{\\ast}}{2}\\xi_2\n$$\nDividing by $-a^{\\ast}$:\n$$\nQ(\\boldsymbol{\\xi}) = \\frac{c_0}{12 a^{\\ast}} + \\frac{2A}{a^{\\ast} \\pi^3} \\xi_3 - \\frac{B}{4 a^{\\ast} \\pi^2} \\xi_4 + \\frac{1}{2}\\xi_1 + \\frac{1}{2}\\xi_2\n$$\nThis is a crucial result: $Q(\\boldsymbol{\\xi})$ is a linear function of the random variables $\\xi_k$.\n\n**Step 3: Analysis of the Smolyak Quadrature**\n\nThe quadrature is built from a one-dimensional rule $Q_\\ell$ based on the trapezoidal formula, with weights adjusted for the uniform probability density $p(\\xi)=\\frac{1}{2}$ on $[-1,1]$. A trapezoidal rule is exact for linear functions. For any linear function $f(\\xi) = c_1 \\xi + c_0$, the rule $Q_\\ell$ computes the exact expectation:\n$$\nQ_\\ell[f] = \\mathbb{E}[f] = \\int_{-1}^1 (c_1 \\xi + c_0) \\frac{1}{2} d\\xi = c_0\n$$\nOur QoI is $Q(\\boldsymbol{\\xi}) = C_0 + C_1 \\xi_1 + C_2 \\xi_2 + C_3 \\xi_3 + C_4 \\xi_4$, where the coefficients $C_k$ depend on the problem parameters. Any tensor-product quadrature rule $(Q_{i_1} \\otimes \\cdots \\otimes Q_{i_d})[Q]$ will also be exact because the one-dimensional rules are exact for the linear components:\n$$\n(Q_{i_1} \\otimes \\cdots \\otimes Q_{i_d})[Q] = \\mathbb{E}[Q] = C_0\n$$\nsince $\\mathbb{E}[\\xi_k] = 0$ for all $k$. The exact mean is $\\mu_{exact} = \\mathbb{E}[Q(\\boldsymbol{\\xi})] = \\frac{c_0}{12 a^{\\ast}} = \\frac{c_0}{12\\sqrt{1-\\rho^2}}$.\n\nThe Smolyak approximation is a linear combination of these tensor-product rules:\n$$\nA(L,d)[Q] = \\sum_{\\mathbf{i} \\in I(L,d)} c(\\mathbf{i}) (Q_{i_1} \\otimes \\cdots \\otimes Q_{i_d})[Q] = \\sum_{\\mathbf{i} \\in I(L,d)} c(\\mathbf{i}) \\mu_{exact} = \\mu_{exact} \\sum_{\\mathbf{i} \\in I(L,d)} c(\\mathbf{i})\n$$\nwhere $I(L,d)$ is the index set defined by $\\max(d, L - d + 1) \\leq \\lVert \\mathbf{i} \\rVert_1 \\leq L$. For this quadrature construction to be valid, the sum of combination coefficients must be $1$. We can verify this for the test cases:\nFor $L=5, d=4$, the sum is $\\sum_{q=4,5} \\binom{q-1}{3}(-1)^{5-q}\\binom{3}{5-q} = -3+4=1$.\nFor $L=6, d=4$, the sum is $\\sum_{q=4,5,6} \\binom{q-1}{3}(-1)^{6-q}\\binom{3}{6-q} = 3-12+10=1$.\nFor $L=4, d=4$, the sum is $\\sum_{q=4} \\binom{q-1}{3}(-1)^{4-q}\\binom{3}{4-q} = 1$.\nSince the coefficients sum to $1$ for all test cases, the Smolyak approximation is exact for any $L \\ge d$:\n$$\n\\widehat{\\mu} = A(L,d)[Q] = \\mu_{exact}\n$$\n\n**Step 4: A Posteriori Error Estimation**\n\nThe error is $\\widehat{e} = |A(L,d)[Q] - A(L-1,d)[Q]|$.\n- If $L-1 \\ge d$, then $A(L,d)[Q]=\\mu_{exact}$ and $A(L-1,d)[Q]=\\mu_{exact}$. The error is $\\widehat{e} = |\\mu_{exact} - \\mu_{exact}| = 0$.\n- If $L-1  d$ (which implies $L=d$ for the smallest valid $L$), then $A(L,d)[Q]=\\mu_{exact}$. Per the problem convention, $A(L-1,d)[Q]=0$. The error is $\\widehat{e} = |\\mu_{exact} - 0| = |\\mu_{exact}|$.\n\n**Step 5: Application to Test Cases**\n\nThe implementation will compute the Smolyak approximation as defined, but based on the analysis above, we can predict the outcome.\n\n**Test Case 1:** $(\\rho, c_0, A, B, L) = (0.5, 2.0, 1.0, 0.5, 5)$\n$d=4, L=5$. Since $L-1=4 \\ge d$, the error is $0$.\n$\\widehat{\\mu} = \\mu_{exact} = \\frac{2.0}{12\\sqrt{1-0.5^2}} = \\frac{1}{6\\sqrt{0.75}} = \\frac{1}{3\\sqrt{3}} \\approx 0.1924500897$.\nResult: $[\\,0.1924500897, 0.0\\,]$.\n\n**Test Case 2:** $(\\rho, c_0, A, B, L) = (0.95, 1.0, 0.8, 1.3, 6)$\n$d=4, L=6$. Since $L-1=5 \\ge d$, the error is $0$.\n$\\widehat{\\mu} = \\mu_{exact} = \\frac{1.0}{12\\sqrt{1-0.95^2}} = \\frac{1}{12\\sqrt{0.0975}} \\approx 0.2669467093$.\nResult: $[\\,0.2669467093, 0.0\\,]$.\n\n**Test Case 3:** $(\\rho, c_0, A, B, L) = (0.1, 5.0, 0.0, 0.0, 4)$\n$d=4, L=4$. Since $L-1=3  d$, the error is $|\\mu_{exact}|$.\n$\\widehat{\\mu} = \\mu_{exact} = \\frac{5.0}{12\\sqrt{1-0.1^2}} = \\frac{5.0}{12\\sqrt{0.99}} \\approx 0.4187903153$.\nResult: $[\\,0.4187903153, 0.4187903153\\,]$.\n\nThe provided Python code implements the full sparse-grid algorithm as specified, which due to the linearity of the QoI, will numerically reproduce these analytically-derived results. This provides a robust validation of the implementation's correctness.",
            "answer": "```python\nimport numpy as np\nfrom scipy.special import comb\nimport itertools\n\n# --- Caching ---\n# Cache for 1D quadrature rules (nodes, weights)\n# Key: level, Value: (nodes_array, weights_array)\nQUADRATURE_CACHE = {}\n\n# Cache for multi-index generation\n# Key: (dim, sum, min_val), Value: list of tuples\nMULTI_INDEX_CACHE = {}\n\n# Cache for tensor product evaluations. This must be cleared for each test case\n# as the qoi_func changes.\n# Key: tuple(multi_index), Value: result\nTENSOR_PROD_CACHE = {}\n\ndef get_1d_quadrature_rule(level):\n    \"\"\"\n    Computes or retrieves from cache the nodes and weights for the 1D trapezoidal rule.\n    \"\"\"\n    if level in QUADRATURE_CACHE:\n        return QUADRATURE_CACHE[level]\n    \n    n = 2**(level - 1) + 1\n    if n == 2:  # level = 1\n        nodes = np.array([-1.0, 1.0])\n        weights = np.array([0.5, 0.5]) # h=2, w=h/4\n    else:\n        h = 2.0 / (n - 1)\n        nodes = np.linspace(-1.0, 1.0, n)\n        weights = np.full(n, h / 2.0)\n        weights[0] = h / 4.0\n        weights[-1] = h / 4.0\n\n    QUADRATURE_CACHE[level] = (nodes, weights)\n    return nodes, weights\n\ndef generate_multi_indices(d, S, min_val=1):\n    \"\"\"\n    Generates all multi-indices of dimension d, sum S, with each element = min_val.\n    \"\"\"\n    key = (d, S, min_val)\n    if key in MULTI_INDEX_CACHE:\n        return MULTI_INDEX_CACHE[key]\n    \n    if d == 1:\n        if S = min_val:\n            res = [(S,)]\n            MULTI_INDEX_CACHE[key] = res\n            return res\n        else:\n            MULTI_INDEX_CACHE[key] = []\n            return []\n\n    indices = []\n    # The max value for the first element is S minus the minimum possible sum for the rest\n    for i in range(min_val, S - (d - 1) * min_val + 1):\n        for rest in generate_multi_indices(d - 1, S - i, min_val):\n            indices.append((i,) + rest)\n    \n    MULTI_INDEX_CACHE[key] = indices\n    return indices\n\ndef evaluate_tensor_product(multi_index, qoi_func, dim):\n    \"\"\"\n    Computes the value of the tensor product quadrature for a given multi-index.\n    \"\"\"\n    multi_index_tuple = tuple(multi_index)\n    if multi_index_tuple in TENSOR_PROD_CACHE:\n        return TENSOR_PROD_CACHE[multi_index_tuple]\n\n    rules = [get_1d_quadrature_rule(l) for l in multi_index]\n    \n    nodes_per_dim = [r[0] for r in rules]\n    weights_per_dim = [r[1] for r in rules]\n\n    total = 0.0\n    \n    # Use itertools.product for an efficient, non-recursive iteration over the grid\n    node_iterator = itertools.product(*nodes_per_dim)\n    weight_iterator = itertools.product(*weights_per_dim)\n\n    for node_vec, weight_vec in zip(node_iterator, weight_iterator):\n        point = np.array(node_vec)\n        weight = np.prod(weight_vec)\n        total += weight * qoi_func(point)\n\n    TENSOR_PROD_CACHE[multi_index_tuple] = total\n    return total\n\ndef smolyak_approximation(level, dim, qoi_func):\n    \"\"\"\n    Computes the Smolyak approximation for a given level, dimension, and QoI.\n    \"\"\"\n    if level  dim:\n        return 0.0\n\n    total_approximation = 0.0\n    q_min = max(dim, level - dim + 1)\n    \n    for q in range(q_min, level + 1):\n        try:\n            # Combination coefficient c(i) where |i|=q\n            c_val = comb(dim - 1, level - q, exact=True)\n            coeff = (-1)**(level - q) * c_val\n        except ValueError:\n            coeff = 0\n        \n        if coeff == 0:\n            continue\n            \n        multi_indices_for_q = generate_multi_indices(dim, q, min_val=1)\n        \n        for multi_index in multi_indices_for_q:\n            tensor_prod_val = evaluate_tensor_product(multi_index, qoi_func, dim)\n            total_approximation += coeff * tensor_prod_val\n            \n    return total_approximation\n\ndef create_qoi_function(rho, c0, A, B):\n    \"\"\"\n    Creates a callable function for the Quantity of Interest Q(xi).\n    \"\"\"\n    a_star = np.sqrt(1.0 - rho**2)\n    C0 = c0 / (12.0 * a_star)\n    C1 = 0.5\n    C2 = 0.5\n    C3 = 2.0 * A / (a_star * np.pi**3)\n    C4 = -B / (4.0 * a_star * np.pi**2)\n    \n    def qoi_func(xi):  # xi is a numpy array [xi_1, xi_2, xi_3, xi_4]\n        return C0 + C1*xi[0] + C2*xi[1] + C3*xi[2] + C4*xi[3]\n        \n    return qoi_func\n\ndef solve():\n    \"\"\"\n    Main solver function that processes all test cases.\n    \"\"\"\n    test_cases = [\n        # (rho, c0, A, B, L)\n        (0.5, 2.0, 1.0, 0.5, 5),\n        (0.95, 1.0, 0.8, 1.3, 6),\n        (0.1, 5.0, 0.0, 0.0, 4),\n    ]\n    dim = 4\n    results = []\n\n    for case in test_cases:\n        rho, c0, A, B, L = case\n        \n        # Clear tensor product cache as qoi_func changes with each case\n        global TENSOR_PROD_CACHE\n        TENSOR_PROD_CACHE = {}\n        \n        qoi_function = create_qoi_function(rho, c0, A, B)\n        \n        mu_hat_L = smolyak_approximation(L, dim, qoi_function)\n        mu_hat_L_minus_1 = smolyak_approximation(L - 1, dim, qoi_function)\n        \n        error_hat = np.abs(mu_hat_L - mu_hat_L_minus_1)\n        \n        results.append([mu_hat_L, error_hat])\n    \n    # Format the final output string exactly as requested\n    print(f\"[{','.join(f'[{mu},{e}]' for mu, e in results)}]\")\n\nsolve()\n```"
        }
    ]
}