{
    "hands_on_practices": [
        {
            "introduction": "One of the most direct approaches to forward uncertainty propagation is to approximate the complex response function with a simpler one. The First-Order Second-Moment (FOSM) method does just this by linearizing the function around the mean of the input variable. This exercise  provides a foundational analysis of this technique by tasking you with comparing the FOSM approximation to the exact analytical result for the exponential function, a canonical example of nonlinear response. By explicitly calculating the leading-order error, you will gain crucial insight into how a function's curvature dictates the accuracy of linearization methods.",
            "id": "3761399",
            "problem": "In a multiscale constitutive model, a macroscale response $Y$ is induced by a microscale scalar descriptor $X$ through the mapping $Y = f(X)$, where $f(x) = \\exp(x)$. Suppose the microscale descriptor $X$ aggregates unresolved thermal fluctuations and is modeled as a Gaussian random variable $X \\sim \\mathcal{N}(0,\\sigma^{2})$ with known variance $ \\sigma^{2} $. You are tasked with forward uncertainty propagation of $X$ to quantify the mean and variance of $Y$.\n\nStarting from the definitions of expectation and variance in terms of integrals with respect to the probability density function and using only well-tested facts about the Gaussian distribution, do the following:\n\n1. Compute the exact expressions for $\\mathbb{E}[Y]$ and $\\mathrm{Var}[Y]$.\n2. Perform asymptotic expansions of the exact expressions for $\\mathbb{E}[Y]$ and $\\mathrm{Var}[Y]$ in powers of $\\sigma$ and retain terms up to order $O(\\sigma^{2})$.\n3. Using the First-Order Second-Moment (FOSM) approximation, linearize $f$ about the mean of $X$ and derive the corresponding approximations to $\\mathbb{E}[Y]$ and $\\mathrm{Var}[Y]$, retaining terms consistent up to order $O(\\sigma^{2})$.\n4. Define the leading-order comparison metrics $\\Delta_{\\mathrm{mean}} := \\mathbb{E}[Y]_{\\mathrm{exact}} - \\mathbb{E}[Y]_{\\mathrm{FOSM}}$ and $\\Delta_{\\mathrm{var}} := \\mathrm{Var}[Y]_{\\mathrm{exact}} - \\mathrm{Var}[Y]_{\\mathrm{FOSM}}$, each truncated to order $O(\\sigma^{2})$.\n\nReport your final result as a single row matrix containing the two expressions $\\Delta_{\\mathrm{mean}}$ and $\\Delta_{\\mathrm{var}}$ in that order. No numerical evaluation is required. Do not include units. Do not round. The final answer must be a closed-form symbolic expression in $\\sigma$ and must be expressed as a row matrix.",
            "solution": "The problem requires a forward propagation of uncertainty for a multiscale model. First, the problem is validated to ensure it is scientifically sound, well-posed, and complete.\n\n### Step 1: Problem Validation\n\nThe givens are:\n- The macroscale response is $Y = f(X)$, with the mapping $f(x) = \\exp(x)$.\n- The microscale descriptor is a random variable $X$ with a Gaussian distribution, $X \\sim \\mathcal{N}(0, \\sigma^2)$. This implies the mean is $\\mathbb{E}[X] = \\mu_X = 0$ and the variance is $\\mathrm{Var}[X] = \\sigma^2$.\n- The tasks are to:\n    1. Compute the exact expressions for the mean $\\mathbb{E}[Y]$ and variance $\\mathrm{Var}[Y]$.\n    2. Perform asymptotic expansions of these exact expressions up to order $O(\\sigma^2)$.\n    3. Derive the First-Order Second-Moment (FOSM) approximations for $\\mathbb{E}[Y]$ and $\\mathrm{Var}[Y]$ by linearizing $f(X)$.\n    4. Compute the leading-order differences $\\Delta_{\\mathrm{mean}} := \\mathbb{E}[Y]_{\\mathrm{exact}} - \\mathbb{E}[Y]_{\\mathrm{FOSM}}$ and $\\Delta_{\\mathrm{var}} := \\mathrm{Var}[Y]_{\\mathrm{exact}} - \\mathrm{Var}[Y]_{\\mathrm{FOSM}}$, truncated to order $O(\\sigma^2)$.\n\nThe problem is scientifically grounded in the principles of probability theory and uncertainty quantification. It is well-posed, with a clearly defined input distribution and mapping function, leading to a unique solution. The language is objective and the definitions are standard in the field. The setup is self-contained and internally consistent. Therefore, the problem is deemed valid and a solution can be constructed.\n\n### Step 2: Solution Derivation\n\nLet $X$ be a random variable with probability density function (PDF) $p(x)$ for a Gaussian distribution $\\mathcal{N}(\\mu_X, \\sigma^2)$:\n$$p(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu_X)^2}{2\\sigma^2}\\right)$$\nGiven $\\mu_X=0$, the PDF simplifies to $p(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{x^2}{2\\sigma^2}\\right)$.\n\n**Part 1: Exact Expressions for $\\mathbb{E}[Y]$ and $\\mathrm{Var}[Y]$**\n\nThe expectation of $Y=f(X)=\\exp(X)$ is computed by integrating over the PDF:\n$$\\mathbb{E}[Y] = \\mathbb{E}[\\exp(X)] = \\int_{-\\infty}^{\\infty} \\exp(x) p(x) \\,dx$$\nThis integral defines the moment-generating function (MGF) of $X$, $M_X(t) = \\mathbb{E}[\\exp(tX)]$, evaluated at $t=1$. A well-known property of the Gaussian distribution $X \\sim \\mathcal{N}(\\mu_X, \\sigma^2)$ is that its MGF is $M_X(t) = \\exp(\\mu_X t + \\frac{1}{2}\\sigma^2 t^2)$.\nWith $\\mu_X=0$, we have $M_X(t) = \\exp(\\frac{1}{2}\\sigma^2 t^2)$.\nTherefore, the exact mean of $Y$ is:\n$$\\mathbb{E}[Y]_{\\mathrm{exact}} = M_X(1) = \\exp\\left(\\frac{1}{2}\\sigma^2\\right)$$\nTo compute the variance, $\\mathrm{Var}[Y] = \\mathbb{E}[Y^2] - (\\mathbb{E}[Y])^2$, we first need $\\mathbb{E}[Y^2]$.\n$$Y^2 = (\\exp(X))^2 = \\exp(2X)$$\nThe expectation of $Y^2$ is found by evaluating the MGF at $t=2$:\n$$\\mathbb{E}[Y^2] = \\mathbb{E}[\\exp(2X)] = M_X(2) = \\exp\\left(\\frac{1}{2}\\sigma^2 (2)^2\\right) = \\exp(2\\sigma^2)$$\nNow, the exact variance of $Y$ is:\n$$\\mathrm{Var}[Y]_{\\mathrm{exact}} = \\mathbb{E}[Y^2] - (\\mathbb{E}[Y])^2 = \\exp(2\\sigma^2) - \\left[\\exp\\left(\\frac{1}{2}\\sigma^2\\right)\\right]^2 = \\exp(2\\sigma^2) - \\exp(\\sigma^2)$$\n\n**Part 2: Asymptotic Expansions up to order $O(\\sigma^2)$**\n\nWe use the Taylor series expansion for the exponential function, $\\exp(z) = 1 + z + \\frac{z^2}{2!} + O(z^3)$, for small $z$.\nFor the mean, let $z = \\frac{1}{2}\\sigma^2$. The expansion of the exact mean for small $\\sigma$ is:\n$$\\mathbb{E}[Y]_{\\mathrm{exact}} = \\exp\\left(\\frac{1}{2}\\sigma^2\\right) = 1 + \\frac{1}{2}\\sigma^2 + \\frac{1}{2!}\\left(\\frac{1}{2}\\sigma^2\\right)^2 + \\dots = 1 + \\frac{1}{2}\\sigma^2 + O(\\sigma^4)$$\nRetaining terms up to order $O(\\sigma^2)$, we get the approximation:\n$$\\mathbb{E}[Y]_{\\mathrm{exact}, O(\\sigma^2)} = 1 + \\frac{1}{2}\\sigma^2$$\nFor the variance, we expand both exponential terms:\n$$\\exp(2\\sigma^2) = 1 + 2\\sigma^2 + \\frac{(2\\sigma^2)^2}{2} + O(\\sigma^6) = 1 + 2\\sigma^2 + 2\\sigma^4 + O(\\sigma^6)$$\n$$\\exp(\\sigma^2) = 1 + \\sigma^2 + \\frac{(\\sigma^2)^2}{2} + O(\\sigma^6) = 1 + \\sigma^2 + \\frac{1}{2}\\sigma^4 + O(\\sigma^6)$$\nSubstituting these into the expression for variance:\n$$\\mathrm{Var}[Y]_{\\mathrm{exact}} = (1 + 2\\sigma^2 + 2\\sigma^4 + \\dots) - (1 + \\sigma^2 + \\frac{1}{2}\\sigma^4 + \\dots) = \\sigma^2 + \\frac{3}{2}\\sigma^4 + O(\\sigma^6)$$\nRetaining terms up to order $O(\\sigma^2)$, we get the approximation:\n$$\\mathrm{Var}[Y]_{\\mathrm{exact}, O(\\sigma^2)} = \\sigma^2$$\n\n**Part 3: First-Order Second-Moment (FOSM) Approximation**\n\nThe FOSM method, as per the instruction to \"linearize $f$\", involves approximating $f(X)$ with its first-order Taylor expansion around the mean of $X$, $\\mu_X = 0$.\nThe function is $f(x) = \\exp(x)$, and its derivative is $f'(x) = \\exp(x)$.\nAt $\\mu_X = 0$: $f(\\mu_X) = \\exp(0) = 1$ and $f'(\\mu_X) = \\exp(0) = 1$.\nThe linearized function is:\n$$Y_{\\mathrm{FOSM}} = f(\\mu_X) + f'(\\mu_X)(X - \\mu_X) = 1 + 1(X - 0) = 1 + X$$\nThe FOSM approximations for the mean and variance are the mean and variance of this linearized function:\n$$\\mathbb{E}[Y]_{\\mathrm{FOSM}} = \\mathbb{E}[1 + X] = 1 + \\mathbb{E}[X] = 1 + 0 = 1$$\n$$\\mathrm{Var}[Y]_{\\mathrm{FOSM}} = \\mathrm{Var}[1 + X] = \\mathrm{Var}[X] = \\sigma^2$$\n\n**Part 4: Leading-Order Comparison Metrics**\n\nThe metrics are defined as $\\Delta_{\\mathrm{mean}} := \\mathbb{E}[Y]_{\\mathrm{exact}} - \\mathbb{E}[Y]_{\\mathrm{FOSM}}$ and $\\Delta_{\\mathrm{var}} := \\mathrm{Var}[Y]_{\\mathrm{exact}} - \\mathrm{Var}[Y]_{\\mathrm{FOSM}}$, truncated to order $O(\\sigma^2)$.\n\nFor the mean:\n$$\\Delta_{\\mathrm{mean}} = \\mathbb{E}[Y]_{\\mathrm{exact}} - \\mathbb{E}[Y]_{\\mathrm{FOSM}} = \\exp\\left(\\frac{1}{2}\\sigma^2\\right) - 1$$\nUsing the series expansion:\n$$\\Delta_{\\mathrm{mean}} = \\left(1 + \\frac{1}{2}\\sigma^2 + \\frac{1}{8}\\sigma^4 + \\dots\\right) - 1 = \\frac{1}{2}\\sigma^2 + \\frac{1}{8}\\sigma^4 + \\dots$$\nTruncating this result to order $O(\\sigma^2)$ yields:\n$$\\Delta_{\\mathrm{mean}} = \\frac{1}{2}\\sigma^2$$\n\nFor the variance:\n$$\\Delta_{\\mathrm{var}} = \\mathrm{Var}[Y]_{\\mathrm{exact}} - \\mathrm{Var}[Y]_{\\mathrm{FOSM}} = (\\exp(2\\sigma^2) - \\exp(\\sigma^2)) - \\sigma^2$$\nUsing the series expansion:\n$$\\Delta_{\\mathrm{var}} = \\left(\\sigma^2 + \\frac{3}{2}\\sigma^4 + O(\\sigma^6)\\right) - \\sigma^2 = \\frac{3}{2}\\sigma^4 + O(\\sigma^6)$$\nTruncating this result to order $O(\\sigma^2)$ implies discarding all terms with powers of $\\sigma$ greater than $2$. The leading term is of order $\\sigma^4$, so the truncated result is:\n$$\\Delta_{\\mathrm{var}} = 0$$\n\nThe final result is the row matrix containing these two expressions in the specified order.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{2}\\sigma^{2} & 0\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "The accuracy of the FOSM approximation, as seen previously, depends on the function's local behavior. This practice  deepens that understanding by considering a sinusoidal response function, which introduces oscillatory nonlinearity. Your task is to quantify the FOSM error and show how it depends on the dimensionless product of the input uncertainty's scale, $\\sigma$, and the function's frequency, $\\alpha$. This analysis is vital for developing an intuition for when linearization is likely to fail, particularly in systems where characteristic length or time scales are fundamental.",
            "id": "3761388",
            "problem": "Consider a single-scale input to a multiscale system modeled as a Gaussian random variable $X \\sim \\mathcal{N}(0,\\sigma^{2})$, and a sinusoidal micro-to-macro response mapping $f(x) = \\sin(\\alpha x)$, where $\\alpha$ and $\\sigma$ are positive real parameters. In forward uncertainty propagation, a common surrogate for the output uncertainty is obtained by the First-Order Second-Moment (FOSM) method, which approximates the response by a first-order Taylor expansion about the mean $ \\mu = \\mathbb{E}[X] $ and propagates input second moments to estimate output second moments. Starting from the fundamental definitions of variance and the characteristic function of the Gaussian distribution, and using the first-order Taylor expansion about $ \\mu = 0 $ to construct the FOSM variance approximation, derive the exact output variance $ \\mathrm{Var}[f(X)] $ and the FOSM variance approximation for this system. Define the FOSM error as\n$$ \\varepsilon(\\alpha,\\sigma) = \\mathrm{Var}[f(X)] - \\mathrm{Var}_{\\text{FOSM}}[f(X)], $$\nand evaluate $ \\varepsilon(\\alpha,\\sigma) $ as a closed-form function of the dimensionless product $ \\alpha \\sigma $. Express your final answer as a single analytic expression in terms of $ \\alpha \\sigma $. No rounding is required.",
            "solution": "The problem requires the derivation of the FOSM error, $\\varepsilon(\\alpha, \\sigma)$, for a specific multiscale system. The error is defined as the difference between the exact output variance and the variance approximated by the First-Order Second-Moment (FOSM) method. The derivation proceeds in three main steps: calculating the exact variance, calculating the FOSM variance, and finding their difference.\n\nThe input to the system is a Gaussian random variable $X$ with mean $\\mathbb{E}[X] = \\mu = 0$ and variance $\\mathrm{Var}[X] = \\sigma^2$. The system's response is described by the function $f(x) = \\sin(\\alpha x)$, where $\\alpha$ and $\\sigma$ are positive real parameters. The output is the random variable $Y = f(X) = \\sin(\\alpha X)$.\n\nFirst, we derive the exact variance of the output, $\\mathrm{Var}[Y] = \\mathrm{Var}[\\sin(\\alpha X)]$. The definition of variance for a random variable $Y$ is $\\mathrm{Var}[Y] = \\mathbb{E}[Y^2] - (\\mathbb{E}[Y])^2$.\n\nWe start by computing the expected value of the output, $\\mathbb{E}[Y] = \\mathbb{E}[\\sin(\\alpha X)]$. The probability density function (PDF) of $X \\sim \\mathcal{N}(0, \\sigma^2)$ is $p(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{x^2}{2\\sigma^2}\\right)$. The expectation is given by the integral:\n$$ \\mathbb{E}[\\sin(\\alpha X)] = \\int_{-\\infty}^{\\infty} \\sin(\\alpha x) \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{x^2}{2\\sigma^2}\\right) dx $$\nThe integrand is the product of an odd function, $\\sin(\\alpha x)$, and an even function, the Gaussian PDF $p(x)$. The product is therefore an odd function. The integral of an odd function over a symmetric interval $(-\\infty, \\infty)$ is zero. Thus,\n$$ \\mathbb{E}[\\sin(\\alpha X)] = 0 $$\n\nNext, we compute the second moment, $\\mathbb{E}[Y^2] = \\mathbb{E}[\\sin^2(\\alpha X)]$. We use the trigonometric identity $\\sin^2(\\theta) = \\frac{1 - \\cos(2\\theta)}{2}$:\n$$ \\mathbb{E}[\\sin^2(\\alpha X)] = \\mathbb{E}\\left[\\frac{1 - \\cos(2\\alpha X)}{2}\\right] = \\frac{1}{2} \\left(\\mathbb{E}[1] - \\mathbb{E}[\\cos(2\\alpha X)]\\right) = \\frac{1}{2} \\left(1 - \\mathbb{E}[\\cos(2\\alpha X)]\\right) $$\nTo find $\\mathbb{E}[\\cos(2\\alpha X)]$, we use the characteristic function of $X$, defined as $\\phi_X(t) = \\mathbb{E}[\\exp(itX)]$. By Euler's formula, $\\exp(itX) = \\cos(tX) + i\\sin(tX)$, so $\\phi_X(t) = \\mathbb{E}[\\cos(tX)] + i\\mathbb{E}[\\sin(tX)]$. The term we need, $\\mathbb{E}[\\cos(tX)]$, is the real part of the characteristic function, $\\mathrm{Re}[\\phi_X(t)]$.\nFor a Gaussian random variable $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$, the characteristic function is $\\phi_X(t) = \\exp\\left(i\\mu t - \\frac{1}{2}\\sigma^2 t^2\\right)$. Since $\\mu = 0$ in this problem, we have:\n$$ \\phi_X(t) = \\exp\\left(-\\frac{1}{2}\\sigma^2 t^2\\right) $$\nThis function is purely real, so $\\mathbb{E}[\\cos(tX)] = \\phi_X(t)$. To find $\\mathbb{E}[\\cos(2\\alpha X)]$, we evaluate $\\phi_X(t)$ at $t = 2\\alpha$:\n$$ \\mathbb{E}[\\cos(2\\alpha X)] = \\phi_X(2\\alpha) = \\exp\\left(-\\frac{1}{2}\\sigma^2 (2\\alpha)^2\\right) = \\exp\\left(-\\frac{4\\alpha^2\\sigma^2}{2}\\right) = \\exp(-2\\alpha^2\\sigma^2) $$\nSubstituting this result back into the expression for the second moment:\n$$ \\mathbb{E}[\\sin^2(\\alpha X)] = \\frac{1}{2}\\left(1 - \\exp(-2\\alpha^2\\sigma^2)\\right) $$\nNow we can compute the exact variance:\n$$ \\mathrm{Var}[\\sin(\\alpha X)] = \\mathbb{E}[\\sin^2(\\alpha X)] - (\\mathbb{E}[\\sin(\\alpha X)])^2 = \\frac{1}{2}\\left(1 - \\exp(-2\\alpha^2\\sigma^2)\\right) - (0)^2 $$\n$$ \\mathrm{Var}[f(X)] = \\frac{1}{2}\\left(1 - \\exp(-2(\\alpha\\sigma)^2)\\right) $$\n\nSecond, we derive the FOSM variance approximation, $\\mathrm{Var}_{\\text{FOSM}}[f(X)]$. The FOSM method approximates the function $f(x)$ with its first-order Taylor expansion around the mean $\\mu = 0$:\n$$ f(x) \\approx f(\\mu) + f'(\\mu)(x-\\mu) $$\nFor $f(x) = \\sin(\\alpha x)$, the derivative is $f'(x) = \\alpha \\cos(\\alpha x)$. We evaluate the function and its derivative at $\\mu = 0$:\n$$ f(\\mu) = f(0) = \\sin(0) = 0 $$\n$$ f'(\\mu) = f'(0) = \\alpha \\cos(0) = \\alpha \\cdot 1 = \\alpha $$\nThe linearized approximation of $f(X)$ is therefore:\n$$ f_{\\text{FOSM}}(X) = 0 + \\alpha(X - 0) = \\alpha X $$\nThe FOSM variance is the variance of this linear approximation:\n$$ \\mathrm{Var}_{\\text{FOSM}}[f(X)] = \\mathrm{Var}[\\alpha X] $$\nUsing the variance property $\\mathrm{Var}[aZ] = a^2\\mathrm{Var}[Z]$ for a constant $a$ and a random variable $Z$, we get:\n$$ \\mathrm{Var}_{\\text{FOSM}}[f(X)] = \\alpha^2 \\mathrm{Var}[X] = \\alpha^2 \\sigma^2 = (\\alpha\\sigma)^2 $$\n\nThird, we compute the FOSM error, $\\varepsilon(\\alpha, \\sigma)$, defined as the difference between the exact and the FOSM-approximated variances:\n$$ \\varepsilon(\\alpha, \\sigma) = \\mathrm{Var}[f(X)] - \\mathrm{Var}_{\\text{FOSM}}[f(X)] $$\nSubstituting the expressions derived above:\n$$ \\varepsilon(\\alpha, \\sigma) = \\frac{1}{2}\\left(1 - \\exp(-2(\\alpha\\sigma)^2)\\right) - (\\alpha\\sigma)^2 $$\nThe problem asks for this error as a function of the dimensionless product $\\alpha\\sigma$. The expression is already in the desired form.",
            "answer": "$$\\boxed{\\frac{1}{2}\\left(1 - \\exp(-2(\\alpha\\sigma)^2)\\right) - (\\alpha\\sigma)^2}$$"
        },
        {
            "introduction": "When analytical approximations like FOSM are inadequate due to strong nonlinearity or high dimensionality, we often turn to sampling-based methods. However, naive random sampling can be computationally inefficient. This exercise  introduces Latin Hypercube Sampling (LHS), a powerful stratified sampling strategy that ensures efficient coverage of the input space. You will construct a small LHS design from first principles and articulate the inverse transform method used to map these samples to arbitrary probability distributions, a cornerstone technique for practical uncertainty quantification in complex computational models.",
            "id": "3761383",
            "problem": "A three-scale simulator models a macro-scale quantity $Y$ as $Y = \\mathcal{G}(X_{1}, X_{2}, X_{3})$, where the micro-scale inputs $(X_{1}, X_{2}, X_{3})$ are independent random variables with continuous marginal cumulative distribution functions (CDFs) $F_{1}$, $F_{2}$, and $F_{3}$. In forward uncertainty propagation, a stratified sampling design is often used to control variance while preserving marginal coverage. Latin Hypercube Sampling (LHS) is a stratified design over the unit hypercube that ensures, in each coordinate, exactly one sample per stratum.\n\nStarting from the foundational definitions of the cumulative distribution function and the inverse transform sampling principle, construct a deterministic Latin Hypercube Sampling (LHS) design in dimension $d=3$ with $N=5$ samples on the unit hypercube $[0,1]^{3}$ using the following canonical choices:\n- Use the stratum centers $c_{j} = (j - 1/2)/5$ for $j \\in \\{1,2,3,4,5\\}$.\n- Use the coordinate-wise permutations $p^{(1)} = (1,2,3,4,5)$, $p^{(2)} = (3,5,1,4,2)$, and $p^{(3)} = (5,2,4,1,3)$, where $p^{(i)}(k)$ denotes the stratum index assigned in dimension $i$ to sample $k \\in \\{1,2,3,4,5\\}$.\n\nThen, derive from first principles how to map this LHS design to the arbitrary input marginals $F_{1}$, $F_{2}$, and $F_{3}$ using inverse CDF transforms, and justify that the resulting samples have the correct one-dimensional marginals.\n\nReport your canonical stratified design as a single row matrix of length $15$ by listing the entries row-wise (sample $1$ through sample $5$), with each entry given in exact decimal form. No rounding is required and no physical units are involved.",
            "solution": "The problem is valid as it is scientifically grounded in established statistical principles, well-posed with all necessary information provided, and objective in its formulation. We will proceed with a full solution.\n\nThe solution requires constructing a Latin Hypercube Sampling (LHS) design and justifying its properties based on first principles. We begin by defining the foundational concepts.\n\nA cumulative distribution function (CDF) $F_{X}$ of a random variable $X$ is defined as $F_{X}(x) = P(X \\le x)$ for any $x \\in \\mathbb{R}$. The problem states that the marginal CDFs $F_{1}$, $F_{2}$, and $F_{3}$ are continuous.\n\nThe inverse transform sampling principle is a method to generate a sample of a random variable $X$ from its CDF $F_{X}$. Given a random variable $U$ uniformly distributed on the interval $[0,1]$, a random variable $X$ with CDF $F_{X}$ can be generated as $X = F_{X}^{-1}(U)$, where $F_{X}^{-1}$ is the inverse CDF, or quantile function, defined as $F_{X}^{-1}(p) = \\inf\\{x \\in \\mathbb{R} : F_{X}(x) \\ge p\\}$ for $p \\in [0,1]$. For a continuous CDF $F_X$, the random variable $F_X(X)$ is uniformly distributed on $[0,1]$. Conversely, if $U \\sim \\text{Uniform}[0,1]$, the random variable $X = F_X^{-1}(U)$ has the CDF $F_X$. To prove this, we find the CDF of $X$:\n$$P(X \\le x) = P(F_{X}^{-1}(U) \\le x)$$\nBecause $F_X$ is a non-decreasing function, the inequality $F_{X}^{-1}(u) \\le x$ is equivalent to $u \\le F_{X}(x)$. This holds because $F_X$ is continuous. Thus,\n$$P(F_{X}^{-1}(U) \\le x) = P(U \\le F_{X}(x))$$\nSince $U$ is a standard uniform random variable, its CDF is $F_{U}(u) = u$ for $u \\in [0,1]$. Therefore,\n$$P(U \\le F_{X}(x)) = F_{X}(x)$$\nThis confirms that $X=F_X^{-1}(U)$ has the desired CDF $F_X$. This principle is central to mapping samples from the unit hypercube to an arbitrary probability space.\n\nNext, we construct the Latin Hypercube Sampling (LHS) design. An LHS design with $N$ samples in $d$ dimensions is a set of $N$ points $\\{\\mathbf{u}_k\\}_{k=1}^{N}$ in the unit hypercube $[0,1]^d$ such that for each dimension $i \\in \\{1, \\dots, d\\}$, the set of coordinates $\\{u_{ki}\\}_{k=1}^{N}$ contains exactly one value from each stratum $[(j-1)/N, j/N)$ for $j \\in \\{1, \\dots, N\\}$.\n\nIn this problem, the dimension is $d=3$ and the number of samples is $N=5$. The unit hypercube is $[0,1]^3$. Each dimension is partitioned into $N=5$ strata: $[0, 1/5)$, $[1/5, 2/5)$, $[2/5, 3/5)$, $[3/5, 4/5)$, and $[4/5, 1]$.\n\nThe problem specifies a deterministic construction using stratum centers and given permutations. The center of the $j$-th stratum is given by the formula $c_j = (j - 1/2)/N$. For $N=5$, the stratum centers are:\n$c_{1} = (1 - 0.5)/5 = 0.5/5 = 0.1$\n$c_{2} = (2 - 0.5)/5 = 1.5/5 = 0.3$\n$c_{3} = (3 - 0.5)/5 = 2.5/5 = 0.5$\n$c_{4} = (4 - 0.5)/5 = 3.5/5 = 0.7$\n$c_{5} = (5 - 0.5)/5 = 4.5/5 = 0.9$\n\nThe $k$-th sample point is $\\mathbf{u}_k = (u_{k1}, u_{k2}, u_{k3})$. The coordinate $u_{ki}$ for sample $k$ in dimension $i$ is determined by choosing the center of the stratum indexed by $p^{(i)}(k)$, where $p^{(i)}$ is the given permutation for the $i$-th dimension. The formula is $u_{ki} = c_{p^{(i)}(k)} = (p^{(i)}(k) - 0.5)/5$.\n\nThe given permutations for $k \\in \\{1,2,3,4,5\\}$ are:\n$p^{(1)} = (1,2,3,4,5)$\n$p^{(2)} = (3,5,1,4,2)$\n$p^{(3)} = (5,2,4,1,3)$\n\nWe now compute the $N=5$ sample points:\nFor sample $k=1$:\n$u_{11} = c_{p^{(1)}(1)} = c_{1} = 0.1$\n$u_{12} = c_{p^{(2)}(1)} = c_{3} = 0.5$\n$u_{13} = c_{p^{(3)}(1)} = c_{5} = 0.9$\nSo, $\\mathbf{u}_1 = (0.1, 0.5, 0.9)$.\n\nFor sample $k=2$:\n$u_{21} = c_{p^{(1)}(2)} = c_{2} = 0.3$\n$u_{22} = c_{p^{(2)}(2)} = c_{5} = 0.9$\n$u_{23} = c_{p^{(3)}(2)} = c_{2} = 0.3$\nSo, $\\mathbf{u}_2 = (0.3, 0.9, 0.3)$.\n\nFor sample $k=3$:\n$u_{31} = c_{p^{(1)}(3)} = c_{3} = 0.5$\n$u_{32} = c_{p^{(2)}(3)} = c_{1} = 0.1$\n$u_{33} = c_{p^{(3)}(3)} = c_{4} = 0.7$\nSo, $\\mathbf{u}_3 = (0.5, 0.1, 0.7)$.\n\nFor sample $k=4$:\n$u_{41} = c_{p^{(1)}(4)} = c_{4} = 0.7$\n$u_{42} = c_{p^{(2)}(4)} = c_{4} = 0.7$\n$u_{43} = c_{p^{(3)}(4)} = c_{1} = 0.1$\nSo, $\\mathbf{u}_4 = (0.7, 0.7, 0.1)$.\n\nFor sample $k=5$:\n$u_{51} = c_{p^{(1)}(5)} = c_{5} = 0.9$\n$u_{52} = c_{p^{(2)}(5)} = c_{2} = 0.3$\n$u_{53} = c_{p^{(3)}(5)} = c_{3} = 0.5$\nSo, $\\mathbf{u}_5 = (0.9, 0.3, 0.5)$.\n\nThe resulting canonical stratified design on the unit hypercube $[0,1]^3$ is the set of these $5$ points, which can be represented as a matrix:\n$$\nU = \\begin{pmatrix}\n0.1 & 0.5 & 0.9 \\\\\n0.3 & 0.9 & 0.3 \\\\\n0.5 & 0.1 & 0.7 \\\\\n0.7 & 0.7 & 0.1 \\\\\n0.9 & 0.3 & 0.5\n\\end{pmatrix}\n$$\n\nTo map this design to samples from the joint distribution of $(X_1, X_2, X_3)$, we apply the inverse transform method component-wise for each sample point $\\mathbf{u}_k$. The corresponding sample in the physical space, $\\mathbf{x}_k = (x_{k1}, x_{k2}, x_{k3})$, is obtained as:\n$$x_{ki} = F_{i}^{-1}(u_{ki})$$\nfor $k \\in \\{1, \\dots, 5\\}$ and $i \\in \\{1, 2, 3\\}$. This maps the LHS design from the unit hypercube to the input space defined by the marginal CDFs $F_1, F_2, F_3$. The independence of the input variables $X_i$ is a prerequisite for this component-wise transformation to be a valid representation of the joint distribution.\n\nFinally, we must justify that the resulting samples $\\{\\mathbf{x}_k\\}_{k=1}^5$ have the correct one-dimensional marginals. Let's consider an arbitrary dimension $i \\in \\{1,2,3\\}$. The set of coordinates $\\{u_{ki}\\}_{k=1}^5$ is constructed using the permutation $p^{(i)}$ of the indices $\\{1,2,3,4,5\\}$. By definition of a permutation, the set of indices $\\{p^{(i)}(1), p^{(i)}(2), p^{(i)}(3), p^{(i)}(4), p^{(i)}(5)\\}$ is simply a reordering of $\\{1,2,3,4,5\\}$.\nConsequently, the set of coordinate values in dimension $i$, $\\{u_{ki}\\}_{k=1}^5 = \\{ c_{p^{(i)}(k)} \\}_{k=1}^5$, is a reordering of the set of stratum centers $\\{c_1, c_2, c_3, c_4, c_5\\}$. Specifically, for any $i$, the set $\\{u_{1i}, u_{2i}, u_{3i}, u_{4i}, u_{5i}\\}$ is equal to the set $\\{0.1, 0.3, 0.5, 0.7, 0.9\\}$.\nThis means that for each dimension $i$, the design has exactly one point in each of the $N=5$ strata $[0, 1/5), \\dots, [4/5, 1)$.\n\nThe input variables $X_i$ have continuous CDFs $F_i$. The probability mass of $X_i$ in the interval corresponding to the $j$-th stratum is:\n$$P(x_i \\in [F_i^{-1}((j-1)/N), F_i^{-1}(j/N)]) = P((j-1)/N \\le F_i(X_i) \\le j/N)$$\nSince $F_i(X_i)$ is uniformly distributed on $[0,1]$, this probability is equal to $j/N - (j-1)/N = 1/N$. Thus, the marginal distribution $F_i$ is partitioned into $N$ equiprobable bins.\nThe LHS construction ensures that the generated samples $\\{x_{ki}\\}_{k=1}^N = \\{F_i^{-1}(u_{ki})\\}_{k=1}^N$ place exactly one sample point into each of these $N$ equiprobable bins. This property, known as marginal stratification, ensures that the samples in each dimension are well-spread across the entire support of the probability distribution, providing excellent one-dimensional coverage. This is the precise sense in which the LHS design respects the one-dimensional marginals.\n\nThe final answer requires reporting the design matrix $U$ as a single row-wise flattened vector. This is:\n$(u_{11}, u_{12}, u_{13}, u_{21}, u_{22}, u_{23}, u_{31}, u_{32}, u_{33}, u_{41}, u_{42}, u_{43}, u_{51}, u_{52}, u_{53})$\n$= (0.1, 0.5, 0.9, 0.3, 0.9, 0.3, 0.5, 0.1, 0.7, 0.7, 0.7, 0.1, 0.9, 0.3, 0.5)$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.1 & 0.5 & 0.9 & 0.3 & 0.9 & 0.3 & 0.5 & 0.1 & 0.7 & 0.7 & 0.7 & 0.1 & 0.9 & 0.3 & 0.5\n\\end{pmatrix}\n}\n$$"
        }
    ]
}