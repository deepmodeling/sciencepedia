{
    "hands_on_practices": [
        {
            "introduction": "The foundational task in reduced-order modeling is the extraction of a low-dimensional basis from high-dimensional data. Proper Orthogonal Decomposition (POD) provides the most efficient basis in an energy sense, capturing the dominant features of a system from a collection of snapshots. This practice provides direct experience in implementing the core POD algorithm via Singular Value Decomposition (SVD) and quantifying its approximation quality, a fundamental skill for any ROM practitioner. By working through various test cases, you will see how the structure of the data matrix impacts the effectiveness of the low-rank approximation .",
            "id": "4249036",
            "problem": "Consider a snapshot matrix $X \\in \\mathbb{R}^{n \\times m}$ that collects $m$ state snapshots of a cyber-physical system, arranged as columns, where each state is $n$-dimensional. Proper Orthogonal Decomposition (POD) is used in surrogate and reduced-order modeling to find a low-dimensional subspace that captures the dominant coherent structures in the data. The POD basis can be obtained from the Singular Value Decomposition (SVD) of $X$. The goal is to compute the first $r$ POD modes and the corresponding rank-$r$ reconstruction, then report the relative reconstruction error.\n\nStarting from the fundamental base that any real matrix admits an SVD, your program must:\n\n- Compute the Singular Value Decomposition (SVD) $X = U \\Sigma V^\\top$, where $U \\in \\mathbb{R}^{n \\times p}$, $V \\in \\mathbb{R}^{m \\times p}$, and $\\Sigma \\in \\mathbb{R}^{p \\times p}$ with $p = \\min(n,m)$, $U$ and $V$ have orthonormal columns, and $\\Sigma$ is diagonal with nonnegative entries (the singular values).\n- Extract the first $r$ POD modes as the first $r$ columns of $U$, denoted $U_r \\in \\mathbb{R}^{n \\times r}$, and the corresponding truncated diagonal matrix $\\Sigma_r \\in \\mathbb{R}^{r \\times r}$ whose diagonal entries are the top $r$ singular values, along with $V_r \\in \\mathbb{R}^{m \\times r}$ formed by the first $r$ columns of $V$.\n- Form the rank-$r$ approximation $X_r = U_r \\Sigma_r V_r^\\top$ and compute the relative Frobenius norm error\n$$\ne_r = \\frac{\\lVert X - X_r\\rVert_F}{\\lVert X\\rVert_F},\n$$\nwhere $\\lVert\\cdot\\rVert_F$ denotes the Frobenius norm.\n- If $\\lVert X\\rVert_F = 0$, define $e_r = 0$.\n\nYour program must implement the above steps for each test case in the following test suite and produce the results as specified. No external input is required; the matrices and values of $r$ are given below.\n\nTest Suite:\n\n- Case $1$ (general non-square matrix, happy path): $X_1 \\in \\mathbb{R}^{4 \\times 3}$ with\n$$\nX_1 = \\begin{bmatrix}\n2 & -1 & 0 \\\\\n0 & 1 & 3 \\\\\n4 & -2 & 1 \\\\\n1 & 0 & -1\n\\end{bmatrix},\n$$\n$r_1 = 2$.\n\n- Case $2$ (rank-deficient matrix where columns are linearly dependent): $X_2 \\in \\mathbb{R}^{3 \\times 3}$ with\n$$\nX_2 = \\begin{bmatrix}\n1 & 2 & 3 \\\\\n2 & 4 & 6 \\\\\n1 & 2 & 3\n\\end{bmatrix},\n$$\n$r_2 = 1$.\n\n- Case $3$ (boundary case $r=0$): $X_3 \\in \\mathbb{R}^{2 \\times 2}$ with\n$$\nX_3 = \\begin{bmatrix}\n3 & -1 \\\\\n0 & 2\n\\end{bmatrix},\n$$\n$r_3 = 0$.\n\n- Case $4$ (full-rank reconstruction with $r = \\min(n,m)$): $X_4 \\in \\mathbb{R}^{5 \\times 3}$ with\n$$\nX_4 = \\begin{bmatrix}\n1 & 0 & 2 \\\\\n0 & 1 & -1 \\\\\n2 & -1 & 0 \\\\\n1 & 3 & 1 \\\\\n-2 & 0 & 1\n\\end{bmatrix},\n$$\n$r_4 = 3$.\n\n- Case $5$ (zero matrix edge case): $X_5 \\in \\mathbb{R}^{3 \\times 4}$ with\n$$\nX_5 = \\begin{bmatrix}\n0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0\n\\end{bmatrix},\n$$\n$r_5 = 2$.\n\n- Case $6$ (ill-conditioned diagonal matrix with decaying scales): $X_6 \\in \\mathbb{R}^{3 \\times 3}$ with\n$$\nX_6 = \\begin{bmatrix}\n10 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 0.1\n\\end{bmatrix},\n$$\n$r_6 = 2$.\n\nFinal Output Format:\n\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each $e_r$ rounded to $8$ decimal places, in the order of the cases from $1$ to $6$, for example: $[e_1,e_2,e_3,e_4,e_5,e_6]$.\n- Each entry must be a decimal number.",
            "solution": "The problem requires the computation of the relative Frobenius norm error for a rank-$r$ approximation of a given matrix $X$, obtained via Proper Orthogonal Decomposition (POD). This is a fundamental task in reduced-order modeling, where the goal is to capture the most significant features of a high-dimensional system with a low-dimensional representation. The mathematical tool for achieving this is the Singular Value Decomposition (SVD).\n\nA given data matrix $X \\in \\mathbb{R}^{n \\times m}$ aggregates $m$ snapshots of an $n$-dimensional state vector. The SVD of $X$ is a factorization of the form:\n$$\nX = U \\Sigma V^\\top\n$$\nThe problem specifies the use of the \"thin\" or \"economy\" SVD. For a matrix $X$ with dimensions $n \\times m$, let $p = \\min(n, m)$. The components of the thin SVD are:\n- $U \\in \\mathbb{R}^{n \\times p}$: A matrix whose columns are the first $p$ left-singular vectors of $X$. These columns are orthonormal and are known as the POD modes. They form an optimal basis for the data in a least-squares sense.\n- $\\Sigma \\in \\mathbb{R}^{p \\times p}$: A diagonal matrix containing the $p$ singular values of $X$, denoted $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_p \\ge 0$, in descending order. The magnitude of each singular value $\\sigma_i$ corresponds to the importance of the $i$-th POD mode.\n- $V \\in \\mathbb{R}^{m \\times p}$: A matrix whose columns are the first $p$ right-singular vectors of $X$. These columns are also orthonormal.\n\nThe Eckart-Young-Mirsky theorem states that the best rank-$r$ approximation of a matrix $X$ in the Frobenius norm (and the spectral norm) is obtained by truncating the SVD. This rank-$r$ approximation, denoted $X_r$, is constructed using the first $r$ singular values and their corresponding singular vectors:\n$$\nX_r = U_r \\Sigma_r V_r^\\top\n$$\nwhere:\n- $U_r \\in \\mathbb{R}^{n \\times r}$ is the matrix containing the first $r$ columns of $U$.\n- $\\Sigma_r \\in \\mathbb{R}^{r \\times r}$ is the diagonal matrix containing the first $r$ singular values, $\\sigma_1, \\dots, \\sigma_r$.\n- $V_r \\in \\mathbb{R}^{m \\times r}$ is the matrix containing the first $r$ columns of $V$, so $V_r^\\top \\in \\mathbb{R}^{r \\times m}$.\n\nThe problem requires calculating the relative reconstruction error, defined as:\n$$\ne_r = \\frac{\\lVert X - X_r\\rVert_F}{\\lVert X\\rVert_F}\n$$\nwhere $\\lVert\\cdot\\rVert_F$ is the Frobenius norm of a matrix $A \\in \\mathbb{R}^{n \\times m}$, calculated as $\\lVert A\\rVert_F = \\sqrt{\\sum_{i=1}^n \\sum_{j=1}^m A_{ij}^2}$.\n\nA key property of the SVD is its relationship with the Frobenius norm. The squared Frobenius norm of a matrix is equal to the sum of its squared singular values:\n$$\n\\lVert X\\rVert_F^2 = \\sum_{i=1}^p \\sigma_i^2\n$$\nThe approximation error matrix, $X - X_r$, has singular values $\\sigma_{r+1}, \\sigma_{r+2}, \\dots, \\sigma_p$. Therefore, the squared Frobenius norm of the error is:\n$$\n\\lVert X - X_r\\rVert_F^2 = \\sum_{i=r+1}^p \\sigma_i^2\n$$\nThis provides a highly efficient way to compute the relative error $e_r$ directly from the singular values without explicitly constructing the matrix $X_r$:\n$$\ne_r = \\sqrt{\\frac{\\sum_{i=r+1}^p \\sigma_i^2}{\\sum_{i=1}^p \\sigma_i^2}}\n$$\nThis formula is valid provided that $\\lVert X\\rVert_F > 0$. If $X$ is the zero matrix, its norm is $0$ and all its singular values are $0$. In this case, the problem specifies that the error $e_r$ should be defined as $0$.\n\nThe algorithm for each test case $(X, r)$ is as follows:\n$1$. Calculate the Frobenius norm of $X$, $\\lVert X\\rVert_F$. If $\\lVert X\\rVert_F = 0$, the error $e_r$ is $0$.\n$2$. If $\\lVert X\\rVert_F > 0$, compute the singular values of $X$. Let them be the array $s = [\\sigma_1, \\sigma_2, \\dots, \\sigma_p]$.\n$3$. The number of singular values is $p = \\min(n, m)$. The index for slicing corresponds to 0-based programming indices. The sum for the error norm involves singular values from index $r$ to $p-1$.\n$4$. Calculate the numerator, the error norm: $\\lVert X - X_r\\rVert_F = \\sqrt{\\sum_{i=r}^{p-1} s_i^2}$.\n$5$. The denominator, $\\lVert X\\rVert_F$, is already computed. It can also be calculated as $\\sqrt{\\sum_{i=0}^{p-1} s_i^2}$.\n$6$. Compute the relative error $e_r = \\lVert X - X_r\\rVert_F / \\lVert X\\rVert_F$.\n\nThis procedure is applied to each test case:\n- For Case $1$ ($r_1=2$), the error will be determined by the smallest singular value, $\\sigma_3$.\n- For Case $2$ (rank-deficient), the matrix rank is $1$. Thus, only one singular value, $\\sigma_1$, will be non-zero. For $r_2=1$, we keep the entire non-zero part of the spectrum, so the error $\\lVert X - X_1\\rVert_F$ should be $0$, yielding $e_1=0$.\n- For Case $3$ ($r_3=0$), the approximation $X_0$ is the zero matrix. The error is $\\lVert X - \\mathbf{0}\\rVert_F = \\lVert X\\rVert_F$. The relative error is $e_0 = \\lVert X\\rVert_F / \\lVert X\\rVert_F = 1$, since $X_3$ is not the zero matrix.\n- For Case $4$ ($r_4=3$), we have $p = \\min(5, 3) = 3$. Since $r_4=p$, we are performing a full reconstruction. Thus, $X_3 = X$, and the error $e_3$ will be $0$ (within machine precision).\n- For Case $5$ ($X_5=\\mathbf{0}$), the matrix norm is $0$. By the problem's definition, the error $e_2$ is $0$.\n- For Case $6$ (diagonal matrix), the singular values are the absolute values of the diagonal entries: $10, 1, 0.1$. For $r_6=2$, the error is determined by the smallest singular value, $\\sigma_3=0.1$. The relative error will be $\\sigma_3 / \\sqrt{\\sigma_1^2 + \\sigma_2^2 + \\sigma_3^2}$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the relative Frobenius norm error for rank-r POD approximations.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (np.array([\n            [2., -1., 0.],\n            [0., 1., 3.],\n            [4., -2., 1.],\n            [1., 0., -1.]\n        ]), 2),\n        (np.array([\n            [1., 2., 3.],\n            [2., 4., 6.],\n            [1., 2., 3.]\n        ]), 1),\n        (np.array([\n            [3., -1.],\n            [0., 2.]\n        ]), 0),\n        (np.array([\n            [1., 0., 2.],\n            [0., 1., -1.],\n            [2., -1., 0.],\n            [1., 3., 1.],\n            [-2., 0., 1.]\n        ]), 3),\n        (np.array([\n            [0., 0., 0., 0.],\n            [0., 0., 0., 0.],\n            [0., 0., 0., 0.]\n        ]), 2),\n        (np.array([\n            [10., 0., 0.],\n            [0., 1., 0.],\n            [0., 0., 0.1]\n        ]), 2)\n    ]\n\n    results = []\n    for X, r in test_cases:\n        # Calculate the Frobenius norm of the original matrix X.\n        norm_X = np.linalg.norm(X, 'fro')\n\n        # Handle the special case where the matrix is the zero matrix.\n        # If norm_X is 0, the relative error is defined as 0.\n        if np.isclose(norm_X, 0.0):\n            relative_error = 0.0\n            results.append(relative_error)\n            continue\n\n        # Compute the singular values of X.\n        # We only need the singular values, so we don't compute U and Vh for efficiency.\n        # singular values are returned in descending order.\n        s = np.linalg.svd(X, compute_uv=False)\n        \n        # The squared Frobenius norm of the error matrix (X - X_r) is the sum\n        # of the squares of the truncated singular values (from index r onwards).\n        # s[r:] gives all singular values from index r to the end.\n        squared_error_norm = np.sum(s[r:]**2)\n        \n        # The error norm is the square root of this sum.\n        error_norm = np.sqrt(squared_error_norm)\n        \n        # The relative error is the ratio of the error norm to the original matrix norm.\n        relative_error = error_norm / norm_X\n        \n        results.append(relative_error)\n\n    # Format the results to 8 decimal places and print in the specified format.\n    formatted_results = [f\"{res:.8f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "After learning how to construct a POD basis, it is crucial to understand its limitations, as not all physical phenomena are efficiently represented by a small number of stationary modes. This exercise explores the classic challenge of a translating or advection-dominated problem, where a coherent structure moves through the domain. You will observe that this motion generates many linearly independent snapshots, leading to a slow decay of singular values and demonstrating why standard POD can be inefficient for transport phenomena. This hands-on example  motivates the need for more advanced modeling techniques in certain physical regimes.",
            "id": "3265968",
            "problem": "Consider the family of snapshot functions of a translating Gaussian pulse defined by $u(x,t) = \\exp\\!\\left(-\\big(x - c t\\big)^{2}\\right)$ on the spatial interval $x \\in [-L,L]$ and discrete times $t \\in \\{t_{0}, t_{1}, \\dots, t_{m-1}\\}$. From first principles, Proper Orthogonal Decomposition (POD) is the procedure that, for a given rank $r$, selects an $r$-dimensional orthonormal basis in space that minimizes the total squared projection error of the snapshot set. Equivalently, it produces the best rank-$r$ approximation of the snapshot data (in the Euclidean least-squares sense across all grid points and times).\n\nYour task is to implement a program that:\n- Constructs the snapshot matrix $X \\in \\mathbb{R}^{N_x \\times m}$ whose $k$-th column is the sampled snapshot $u(x,t_k)$ at $N_x$ uniformly spaced grid points in $[-L,L]$, for a given speed $c$, number of snapshots $m$, and final time $T$ with $t_k$ equally spaced in $[0,T]$.\n- Computes, for ranks $r \\in \\{1,2,5,10\\}$, the best rank-$r$ approximation $X_r$ (as defined by POD) and the corresponding relative reconstruction error\n$$E_r = \\frac{\\lVert X - X_r \\rVert_F}{\\lVert X \\rVert_F},$$\nwhere $\\lVert \\cdot \\rVert_F$ denotes the Frobenius norm.\n- Reports the errors $E_r$ for each test case as floating-point numbers rounded to six decimal places.\n\nFundamental base to use:\n- Definitions of Euclidean inner product and Frobenius norm.\n- The defining optimization property of Proper Orthogonal Decomposition (POD): among all $r$-dimensional orthonormal bases, POD minimizes the total squared projection error of the snapshots. This yields the best rank-$r$ approximation of the snapshot matrix in the least-squares sense.\n\nTest suite:\nUse $L = 10$ and $N_x = 401$ for all cases. The four test cases are:\n1. Case A (stationary pulse): $c = 0$, $T = 5$, $m = 50$.\n2. Case B (slow translation): $c = 0.5$, $T = 10$, $m = 100$.\n3. Case C (fast translation): $c = 2.0$, $T = 4$, $m = 80$.\n4. Case D (few snapshots): $c = 0.5$, $T = 10$, $m = 5$.\n\nAnswer specification:\n- For each test case, output a list $[E_{1},E_{2},E_{5},E_{10}]$ of four floats rounded to six decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the bracketed list of four errors for one test case, in the order of Cases A, B, C, D. For example, an output of the correct format would look like\n$[[e_{A,1},e_{A,2},e_{A,5},e_{A,10}],[e_{B,1},e_{B,2},e_{B,5},e_{B,10}],[e_{C,1},e_{C,2},e_{C,5},e_{C,10}],[e_{D,1},e_{D,2},e_{D,5},e_{D,10}]]$,\nwith no spaces anywhere in the line.\n\nUnits:\n- There are no physical units required in this problem.\n\nAngle units:\n- Not applicable.\n\nPercentages:\n- Not applicable; express all quantities as decimals.\n\nYour implementation must be self-contained and require no user input, external files, or network access. It must run in a modern programming language and produce the exact final output format described above in a single line.",
            "solution": "The objective is to compute the relative reconstruction error for a rank-$r$ approximation of a set of data snapshots. The foundational principle is that the optimal rank-$r$ approximation, in the least-squares sense defined by the Frobenius norm, is obtained via the Singular Value Decomposition (SVD). This result is formally stated by the Eckart-Young-Mirsky theorem.\n\n**1. Snapshot Matrix Construction**\nFirst, we discretize the problem domain. The spatial domain $x \\in [-L, L]$ is sampled at $N_x$ uniformly spaced points, forming the grid $\\{x_j\\}_{j=0}^{N_x-1}$. The time interval $t \\in [0, T]$ is sampled at $m$ discrete, equally spaced points $\\{t_k\\}_{k=0}^{m-1}$. The snapshot data at each time point $t_k$ is a vector in $\\mathbb{R}^{N_x}$ whose entries are given by the function $u(x_j, t_k)$. The collection of these snapshots forms the columns of the snapshot matrix $X \\in \\mathbb{R}^{N_x \\times m}$. An element $X_{jk}$ of this matrix is given by:\n$$X_{jk} = u(x_j, t_k) = \\exp\\!\\left(-\\big(x_j - c t_k\\big)^{2}\\right)$$\n\n**2. Singular Value Decomposition and Optimal Approximation**\nThe SVD of the snapshot matrix $X$ is given by:\n$$X = U \\Sigma V^\\top$$\nwhere $U \\in \\mathbb{R}^{N_x \\times N_x}$ is an orthogonal matrix whose columns $u_i$ are the left-singular vectors (POD modes), $V \\in \\mathbb{R}^{m \\times m}$ is an orthogonal matrix whose columns $v_i$ are the right-singular vectors, and $\\Sigma \\in \\mathbb{R}^{N_x \\times m}$ is a rectangular diagonal matrix containing the singular values $\\sigma_i$. The singular values are non-negative and ordered by convention: $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_k \\ge 0$, where $k = \\min(N_x, m)$ is the rank of the matrix.\n\nThe Eckart-Young-Mirsky theorem states that the best rank-$r$ approximation of $X$ that minimizes the Frobenius norm of the difference, $\\lVert X - X_r \\rVert_F$, is the truncated SVD:\n$$X_r = \\sum_{i=1}^{r} \\sigma_i u_i v_i^\\top$$\nThis approximation is constructed using the first $r$ singular values and their corresponding left and right singular vectors.\n\n**3. Error Calculation**\nThe relative reconstruction error $E_r$ is defined as the ratio of the Frobenius norm of the error matrix $(X - X_r)$ to the Frobenius norm of the original matrix $X$. The Frobenius norm is related to the singular values by the identity $\\lVert A \\rVert_F^2 = \\sum_{i=1}^{\\text{rank}(A)} \\sigma_i(A)^2$.\nApplying this property, the squared norm of the original matrix is the sum of the squares of all its singular values:\n$$\\lVert X \\rVert_F^2 = \\sum_{i=1}^{k} \\sigma_i^2$$\nThe error matrix is $X - X_r = \\sum_{i=r+1}^{k} \\sigma_i u_i v_i^\\top$. Due to the orthogonality of the singular vectors, the squared Frobenius norm of the error matrix is the sum of the squares of the discarded singular values:\n$$\\lVert X - X_r \\rVert_F^2 = \\sum_{i=r+1}^{k} \\sigma_i^2$$\nCombining these results, the relative reconstruction error is given by:\n$$E_r = \\frac{\\lVert X - X_r \\rVert_F}{\\lVert X \\rVert_F} = \\frac{\\sqrt{\\sum_{i=r+1}^{k} \\sigma_i^2}}{\\sqrt{\\sum_{i=1}^{k} \\sigma_i^2}}$$\nNote that if the requested rank $r$ is greater than or equal to the actual rank of the matrix, $k$, the sum in the numerator is empty and evaluates to $0$, correctly yielding an error $E_r = 0$.\n\n**4. Computational Procedure**\nThe algorithm proceeds as follows for each test case:\n1.  Define the parameters $c$, $T$, and $m$, along with the fixed constants $L=10$ and $N_x=401$.\n2.  Construct the spatial grid $x$ and temporal grid $t$.\n3.  Assemble the $N_x \\times m$ snapshot matrix $X$ using the given function $u(x,t)$.\n4.  Compute the singular values $\\sigma_i$ of $X$ using a standard numerical library function for SVD. It is most efficient to compute only the singular values, not the full $U$ and $V$ matrices.\n5.  Calculate the total energy, represented by the squared Frobenius norm, $S_{total} = \\sum_{i=1}^{k} \\sigma_i^2$.\n6.  For each required rank $r \\in \\{1, 2, 5, 10\\}$, calculate the error energy, $S_{error} = \\sum_{i=r+1}^{k} \\sigma_i^2$.\n7.  The relative error is then $E_r = \\sqrt{S_{error} / S_{total}}$.\n8.  The calculated errors for each test case are collected and formatted according to the output specification.\nThis procedure provides a direct and numerically stable method for determining the required reconstruction errors based on fundamental principles of linear algebra.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Proper Orthogonal Decomposition problem for a translating Gaussian pulse.\n    \"\"\"\n    # Global parameters for all test cases\n    L = 10.0\n    Nx = 401\n    ranks_to_compute = [1, 2, 5, 10]\n\n    # Test suite: (c, T, m)\n    # c: speed, T: final time, m: number of snapshots\n    test_cases = [\n        (0.0, 5.0, 50),   # Case A: stationary pulse\n        (0.5, 10.0, 100), # Case B: slow translation\n        (2.0, 4.0, 80),   # Case C: fast translation\n        (0.5, 10.0, 5),   # Case D: few snapshots\n    ]\n\n    all_results = []\n\n    for c, T, m in test_cases:\n        # 1. Create spatial and temporal grids\n        x = np.linspace(-L, L, Nx)\n        t = np.linspace(0.0, T, m)\n\n        # 2. Construct the snapshot matrix X using broadcasting\n        # x_col has shape (Nx, 1) and t_row has shape (1, m)\n        # Broadcasting expands them to (Nx, m) for element-wise operations\n        x_col = x[:, np.newaxis]\n        t_row = t[np.newaxis, :]\n        X = np.exp(-((x_col - c * t_row) ** 2))\n\n        # 3. Compute the singular values of X\n        # We only need the singular values, so compute_uv=False is most efficient.\n        s = np.linalg.svd(X, compute_uv=False)\n        num_singular_values = s.shape[0]\n\n        # 4. Calculate the total energy (squared Frobenius norm of X)\n        # This is the sum of the squares of all singular values.\n        norm_X_sq = np.sum(s**2)\n\n        case_errors = []\n        for r in ranks_to_compute:\n            # 5. Calculate the reconstruction error for rank r\n            \n            # If norm_X_sq is zero, all errors are zero.\n            if norm_X_sq == 0.0:\n                 error = 0.0\n            # If rank r is >= number of singular values, the approximation is perfect.\n            elif r >= num_singular_values:\n                error = 0.0\n            else:\n                # The error norm is based on the truncated singular values (from r to end).\n                # s[r:] corresponds to sigma_{r+1}, sigma_{r+2}, ...\n                norm_err_sq = np.sum(s[r:]**2)\n                error = np.sqrt(norm_err_sq / norm_X_sq)\n            \n            case_errors.append(error)\n\n        all_results.append(case_errors)\n\n    # 6. Format the output string exactly as specified.\n    # e.g., [[err1,err2,...],[err1,err2,...]] with no spaces.\n    formatted_sublists = []\n    for res_list in all_results:\n        # Format each number to 6 decimal places.\n        formatted_numbers = [f\"{err:.6f}\" for err in res_list]\n        # Join numbers with commas and enclose in brackets.\n        formatted_sublists.append(f\"[{','.join(formatted_numbers)}]\")\n    \n    # Join the sublists with commas and enclose in outer brackets.\n    final_output = f\"[{','.join(formatted_sublists)}]\"\n\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "The goal of many ROMs is to create a fast predictive model by projecting the governing equations onto a POD basis via a Galerkin projection. This advanced practice delves into a critical and subtle aspect of ROM stability, demonstrating that excellent data reconstruction does not guarantee a stable dynamic model. You will construct a scenario  where a stable full-order model (FOM) gives rise to an unstable reduced-order model (ROM), a phenomenon characteristic of systems with non-normal operators common in fields like fluid dynamics. This exercise provides a crucial lesson on the potential pitfalls of projection-based modeling and the importance of analyzing the stability of the resulting ROM.",
            "id": "2432128",
            "problem": "You are asked to implement a complete numerical experiment in reduced-order modeling that demonstrates the following phenomenon: a Proper Orthogonal Decomposition (POD) basis can be excellent for reconstructing training snapshots of a stable full-order linear time-invariant system, yet the Galerkin-projected reduced-order model (ROM) can produce unstable dynamics that blow up when integrated in time.\n\nYour implementation must start from the full-order ordinary differential equation\n$$\n\\frac{d\\mathbf{x}}{dt} = \\mathbf{A}\\mathbf{x} + \\mathbf{b},\n$$\nwhere $\\mathbf{A}\\in\\mathbb{R}^{n\\times n}$ and $\\mathbf{b}\\in\\mathbb{R}^{n}$ are constant, and $\\mathbf{x}(t)\\in\\mathbb{R}^{n}$ is the state. All computations are over the real numbers with the standard Euclidean inner product. You will use $n=2$ throughout.\n\nFundamental definitions and requirements:\n- Proper Orthogonal Decomposition (POD) basis: Given a snapshot matrix\n$$\n\\mathbf{X} = \\begin{bmatrix}\\mathbf{x}(t_1) & \\mathbf{x}(t_2) & \\cdots & \\mathbf{x}(t_m)\\end{bmatrix}\\in\\mathbb{R}^{n\\times m},\n$$\ncompute its singular value decomposition (SVD) $\\mathbf{X}=\\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^\\top$. The rank-$r$ POD basis $\\mathbf{Q}\\in\\mathbb{R}^{n\\times r}$ is taken as the first $r$ columns of $\\mathbf{U}$.\n- Galerkin projection: The reduced operator and reduced forcing are\n$$\n\\mathbf{A}_r = \\mathbf{Q}^\\top\\mathbf{A}\\mathbf{Q}\\in\\mathbb{R}^{r\\times r},\\qquad \\mathbf{b}_r=\\mathbf{Q}^\\top\\mathbf{b}\\in\\mathbb{R}^{r}.\n$$\nThe reduced state $\\mathbf{z}(t)\\in\\mathbb{R}^{r}$ evolves as\n$$\n\\frac{d\\mathbf{z}}{dt} = \\mathbf{A}_r \\mathbf{z} + \\mathbf{b}_r,\\qquad \\mathbf{x}_r(t)=\\mathbf{Q}\\mathbf{z}(t).\n$$\n- Time integration: Use the classical fourth-order Runge–Kutta method with a fixed time step $h>0$ for both the full-order model and the ROM. Set the initial condition to $\\mathbf{x}(0)=\\mathbf{0}$ and $\\mathbf{z}(0)=\\mathbf{Q}^\\top\\mathbf{x}(0)=\\mathbf{0}$.\n- Snapshot collection: Integrate the full-order model over a training horizon $[0,T_{\\text{train}}]$ with a constant time step $h$, sampling the state at every step to form $\\mathbf{X}$.\n- Reconstruction error: Measure the relative POD reconstruction error of the training snapshots as\n$$\n\\varepsilon_{\\text{rec}} = \\frac{\\lVert \\mathbf{X} - \\mathbf{Q}\\mathbf{Q}^\\top \\mathbf{X}\\rVert_F}{\\lVert \\mathbf{X}\\rVert_F},\n$$\nwhere $\\lVert\\cdot\\rVert_F$ denotes the Frobenius norm.\n- Blow-up detection: Evolve both the full-order model and the ROM over a test horizon $[0,T_{\\text{test}}]$ with the same $h$. Declare a solution “blown up” if at any time step the Euclidean norm of the current state exceeds a threshold $M$, or if any component becomes not-a-number or infinite. Use the threshold $M=10^6$.\n\nConstructed forcing to target instability under ROM:\n- For each test, you must construct the constant forcing $\\mathbf{b}$ as follows. Compute the symmetric part $\\mathbf{S}=\\frac{1}{2}(\\mathbf{A}+\\mathbf{A}^\\top)$ and its dominant unit eigenvector $\\mathbf{q}\\in\\mathbb{R}^{n}$ associated with the largest eigenvalue of $\\mathbf{S}$ (break ties arbitrarily but deterministically). Set\n$$\n\\mathbf{b}=-\\mathbf{A}\\mathbf{q}.\n$$\nThis construction ensures that the full-order steady state is $\\mathbf{x}_\\infty = -\\mathbf{A}^{-1}\\mathbf{b}=\\mathbf{q}$. When $\\mathbf{A}$ is highly non-normal and the largest eigenvalue of $\\mathbf{S}$ is positive, the scalar ROM obtained with $r=1$ and $\\mathbf{Q}=\\mathbf{q}$ has reduced dynamics $\\frac{dz}{dt} = a_r z + b_r$ with $a_r=\\mathbf{q}^\\top\\mathbf{A}\\mathbf{q}>0$ and $b_r=-a_r$, which is unstable and diverges from $z(0)=0$.\n\nNumerical specification common to all tests:\n- Use $n=2$.\n- Use $h=10^{-3}$.\n- Use classical fourth-order Runge–Kutta.\n- Use the Euclidean norm for all vector norms.\n- Use $\\mathbf{x}(0)=\\mathbf{0}$.\n\nTest suite:\nImplement the above for the following parameter sets. In each case, define $\\mathbf{A}$, compute $\\mathbf{q}$ and $\\mathbf{b}$ as specified, collect snapshots over $[0,T_{\\text{train}}]$ to form $\\mathbf{Q}$, then form the ROM and run both models over $[0,T_{\\text{test}}]$.\n\n- Test $1$ (highly non-normal, rank-$1$ POD):\n  - $\\mathbf{A}=\\begin{bmatrix}-0.1 & \\alpha \\\\ 0 & -1.0\\end{bmatrix}$ with $\\alpha=50.0$,\n  - $r=1$,\n  - $T_{\\text{train}}=4.0$,\n  - $T_{\\text{test}}=1.0$.\n- Test $2$ (highly non-normal, rank-$2$ POD):\n  - $\\mathbf{A}=\\begin{bmatrix}-0.1 & \\alpha \\\\ 0 & -1.0\\end{bmatrix}$ with $\\alpha=50.0$,\n  - $r=2$,\n  - $T_{\\text{train}}=4.0$,\n  - $T_{\\text{test}}=1.0$.\n- Test $3$ (symmetric negative definite, rank-$1$ POD):\n  - $\\mathbf{A}=\\begin{bmatrix}-1.0 & 0.0 \\\\ 0.0 & -2.0\\end{bmatrix}$,\n  - $r=1$,\n  - $T_{\\text{train}}=4.0$,\n  - $T_{\\text{test}}=1.0$.\n- Test $4$ (more highly non-normal, rank-$1$ POD):\n  - $\\mathbf{A}=\\begin{bmatrix}-0.1 & \\alpha \\\\ 0 & -1.0\\end{bmatrix}$ with $\\alpha=120.0$,\n  - $r=1$,\n  - $T_{\\text{train}}=4.0$,\n  - $T_{\\text{test}}=1.0$.\n\nRequired outputs:\n- For each test, output a list of three entries:\n  - the scalar $\\varepsilon_{\\text{rec}}$ rounded to six decimal places,\n  - a boolean indicating whether the ROM blew up on $[0,T_{\\text{test}}]$,\n  - a boolean indicating whether the full-order model blew up on $[0,T_{\\text{test}}]$.\n- Aggregate the results from all tests into a single line as a comma-separated list enclosed in square brackets, in the same order as the tests. Example format:\n$[\\varepsilon_{\\text{rec}}^{(1)},\\text{ROM}^{(1)}\\_\\text{blowup},\\text{FOM}^{(1)}\\_\\text{blowup}],[\\varepsilon_{\\text{rec}}^{(2)},\\text{ROM}^{(2)}\\_\\text{blowup},\\text{FOM}^{(2)}\\_\\text{blowup}],\\ldots$.",
            "solution": "The core of the problem lies in the distinction between the spectrum of a matrix $\\mathbf{A}$ and its numerical range (or field of values), defined as $W(\\mathbf{A}) = \\{\\mathbf{v}^\\dagger\\mathbf{A}\\mathbf{v} : \\mathbf{v} \\in \\mathbb{C}^n, \\lVert\\mathbf{v}\\rVert_2 = 1\\}$. For a linear time-invariant system $\\frac{d\\mathbf{x}}{dt} = \\mathbf{A}\\mathbf{x}$, stability is determined by the eigenvalues of $\\mathbf{A}$ (the spectrum, $\\sigma(\\mathbf{A})$). If all eigenvalues have negative real parts, the system is stable, and $\\lVert\\mathbf{x}(t)\\rVert \\to 0$ as $t\\to\\infty$. However, transient growth is possible if $\\mathbf{A}$ is non-normal (i.e., $\\mathbf{A}\\mathbf{A}^\\top \\neq \\mathbf{A}^\\top\\mathbf{A}$). The numerical range provides insight into this transient behavior. The real part of the numerical range is governed by the symmetric part of the matrix, $\\mathbf{S} = \\frac{1}{2}(\\mathbf{A} + \\mathbf{A}^\\top)$, since $\\text{Re}(\\mathbf{v}^\\top\\mathbf{A}\\mathbf{v}) = \\mathbf{v}^\\top\\mathbf{S}\\mathbf{v}$. A positive eigenvalue of $\\mathbf{S}$ implies that the numerical range of $\\mathbf{A}$ extends into the right half-plane, indicating potential for transient energy growth.\n\nA Galerkin projection with a rank-$r$ POD basis $\\mathbf{Q}$ transforms the FOM $\\frac{d\\mathbf{x}}{dt} = \\mathbf{A}\\mathbf{x} + \\mathbf{b}$ into the ROM $\\frac{d\\mathbf{z}}{dt} = \\mathbf{A}_r\\mathbf{z} + \\mathbf{b}_r$, where $\\mathbf{A}_r = \\mathbf{Q}^\\top\\mathbf{A}\\mathbf{Q}$. The stability of the ROM is determined by the eigenvalues of the reduced matrix $\\mathbf{A}_r$. Crucially, the eigenvalues of $\\mathbf{A}_r$ are contained within the numerical range of $\\mathbf{A}$, but not necessarily within the convex hull of its spectrum. If the numerical range $W(\\mathbf{A})$ crosses into the right half-plane, it is possible to find a projection subspace (basis $\\mathbf{Q}$) such that $\\mathbf{A}_r$ has eigenvalues with positive real parts, rendering the ROM unstable.\n\nThe problem's construction is designed to expose this pathology. The FOM is stable (eigenvalues of $\\mathbf{A}$ are $\\{-0.1, -1.0\\}$). The forcing term $\\mathbf{b} = -\\mathbf{A}\\mathbf{q}$ is chosen such that the FOM steady state is $\\mathbf{x}_{\\infty} = \\mathbf{q}$, where $\\mathbf{q}$ is the eigenvector corresponding to the largest eigenvalue of $\\mathbf{S}$. This drives the system dynamics towards the direction of maximum transient growth. The resulting snapshots will be dominated by this direction, causing the primary POD mode (the first column of $\\mathbf{Q}$) to align with $\\mathbf{q}$. For a rank-$1$ ROM ($r=1$), the reduced matrix $\\mathbf{A}_r$ becomes a scalar $a_r = \\mathbf{Q}^\\top\\mathbf{A}\\mathbf{Q}$. If $\\mathbf{Q} \\approx \\mathbf{q}$, then $a_r \\approx \\mathbf{q}^\\top\\mathbf{A}\\mathbf{q} = \\mathbf{q}^\\top\\mathbf{S}\\mathbf{q} = \\lambda_{\\max}(\\mathbf{S})$. For the non-normal matrices in Tests $1$ and $4$, $\\lambda_{\\max}(\\mathbf{S}) > 0$, leading to an unstable ROM.\n\nThe computational procedure for each test case is as follows:\n$1$. Define system parameters: matrix $\\mathbf{A}$, ROM rank $r$, and time horizons $T_{\\text{train}}$ and $T_{\\text{test}}$. The dimension is $n=2$ and the time step is $h=10^{-3}$.\n$2$. Construct the forcing term: Compute the symmetric part $\\mathbf{S} = \\frac{1}{2}(\\mathbf{A} + \\mathbf{A}^\\top)$. Find its eigenvalues and eigenvectors. Let $\\mathbf{q}$ be the normalized eigenvector corresponding to the largest eigenvalue. Set $\\mathbf{b} = -\\mathbf{A}\\mathbf{q}$.\n$3$. Generate training data: Integrate the FOM, $\\frac{d\\mathbf{x}}{dt} = \\mathbf{A}\\mathbf{x} + \\mathbf{b}$, from $\\mathbf{x}(0)=\\mathbf{0}$ over the time interval $[0, T_{\\text{train}}]$ using the classical fourth-order Runge-Kutta method. The states at each time step are collected into the snapshot matrix $\\mathbf{X}$.\n$4$. Compute the POD basis: Perform a singular value decomposition (SVD) on the snapshot matrix, $\\mathbf{X} = \\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^\\top$. The POD basis $\\mathbf{Q}$ of rank $r$ is formed by the first $r$ columns of $\\mathbf{U}$.\n$5$. Calculate reconstruction error: The relative Frobenius norm error is computed as $\\varepsilon_{\\text{rec}} = \\lVert \\mathbf{X} - \\mathbf{Q}\\mathbf{Q}^\\top \\mathbf{X}\\rVert_F / \\lVert \\mathbf{X}\\rVert_F$.\n$6$. Construct the ROM: The reduced system matrices are $\\mathbf{A}_r = \\mathbf{Q}^\\top\\mathbf{A}\\mathbf{Q}$ and $\\mathbf{b}_r = \\mathbf{Q}^\\top\\mathbf{b}$.\n$7$. Perform time integration for testing: Both the FOM and the ROM are integrated from zero initial conditions ($\\mathbf{x}(0)=\\mathbf{0}$, $\\mathbf{z}(0)=\\mathbf{0}$) over the interval $[0, T_{\\text{test}}]$. During integration, at each step, the Euclidean norm of the state vector is checked against the blow-up threshold $M=10^6$.\n$8$. Record results: The final outputs for the test are the computed $\\varepsilon_{\\text{rec}}$, a boolean indicating if the ROM blew up, and a boolean indicating if the FOM blew up.\n\nExpected outcomes for the tests:\n- **Test 1**: ($\\mathbf{A}$ non-normal, $r=1$): $\\mathbf{A}$ is stable. The construction of $\\mathbf{b}$ and the choice of $r=1$ are designed to produce an unstable ROM. We expect a small $\\varepsilon_{\\text{rec}}$, ROM blow-up, and no FOM blow-up.\n- **Test 2**: ($\\mathbf{A}$ non-normal, $r=2$): Here, $r=n=2$. The POD basis $\\mathbf{Q}$ will be a complete orthonormal basis for $\\mathbb{R}^2$. Thus, $\\mathbf{Q}\\mathbf{Q}^\\top = \\mathbf{I}$, meaning the reconstruction error $\\varepsilon_{\\text{rec}}$ will be zero (or of the order of machine precision). The ROM is dynamically equivalent to the FOM, simply expressed in a different basis. Since the FOM is stable, the ROM will also be stable. We expect $\\varepsilon_{\\text{rec}} \\approx 0$, no ROM blow-up, and no FOM blow-up.\n- **Test 3**: ($\\mathbf{A}$ symmetric, $r=1$): $\\mathbf{A}$ is a normal matrix. Its numerical range is the convex hull of its eigenvalues, which are $\\{-1.0, -2.0\\}$. Thus, the numerical range is the interval $[-2.0, -1.0]$ on the real axis. The reduced operator $a_r = \\mathbf{Q}^\\top\\mathbf{A}\\mathbf{Q}$ must be negative. The ROM will be stable. We expect no blow-up for either model.\n- **Test 4**: ($\\mathbf{A}$ more non-normal, $r=1$): Similar to Test $1$, but with a larger off-diagonal term $\\alpha=120.0$. This increases the non-normality, leading to a larger positive eigenvalue for $\\mathbf{S}$. The ROM instability should be even more pronounced. We expect a small $\\varepsilon_{\\text{rec}}$, ROM blow-up, and no FOM blow-up.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import eigh\n\ndef solve():\n    \"\"\"\n    Implements the full numerical experiment to demonstrate ROM instability\n    for a stable FOM.\n    \"\"\"\n\n    def rk4_step(f, y, h, A, b):\n        \"\"\"A single step of the classical fourth-order Runge-Kutta method.\"\"\"\n        k1 = f(y, A, b)\n        k2 = f(y + h / 2 * k1, A, b)\n        k3 = f(y + h / 2 * k2, A, b)\n        k4 = f(y + h * k3, A, b)\n        return y + h / 6 * (k1 + 2 * k2 + 2 * k3 + k4)\n\n    def lti_rhs(y, A, b):\n        \"\"\"RHS of the LTI system dy/dt = Ay + b.\"\"\"\n        return A @ y + b\n\n    def simulate(A, b, y0, T, h, M):\n        \"\"\"\n        Simulates an LTI system and returns snapshots and blow-up status.\n        \"\"\"\n        num_steps = int(T / h)\n        y = y0.copy()\n        snapshots = [y0.copy()]\n        blew_up = False\n        \n        for _ in range(num_steps):\n            y = rk4_step(lti_rhs, y, h, A, b)\n            if np.linalg.norm(y) > M or not np.all(np.isfinite(y)):\n                blew_up = True\n                # Continue collecting snapshots to see the blow-up, if needed for X\n                # But stop checking once blown up.\n                while len(snapshots) < num_steps + 1:\n                    snapshots.append(y.copy()) # Append the diverging state\n                    y = rk4_step(lti_rhs, y, h, A, b) # Could become inf/nan\n                return np.array(snapshots).T, True\n\n            snapshots.append(y.copy())\n            \n        return np.array(snapshots).T, blew_up\n\n    # General parameters\n    n = 2\n    h = 1e-3\n    M = 1e6\n    x0 = np.zeros(n)\n\n    # Test cases from the problem statement.\n    test_cases = [\n        # (A_params, r, T_train, T_test)\n        ({\"alpha\": 50.0}, 1, 4.0, 1.0),\n        ({\"alpha\": 50.0}, 2, 4.0, 1.0),\n        ({\"alpha\": None}, 1, 4.0, 1.0), # Symmetric case\n        ({\"alpha\": 120.0}, 1, 4.0, 1.0),\n    ]\n\n    results = []\n    \n    for i, (params, r, T_train, T_test) in enumerate(test_cases):\n        # 1. Define A\n        if i == 2: # Test 3: Symmetric case\n            A = np.array([[-1.0, 0.0], [0.0, -2.0]])\n        else: # Tests 1, 2, 4: Non-normal case\n            alpha = params[\"alpha\"]\n            A = np.array([[-0.1, alpha], [0.0, -1.0]])\n\n        # 2. Construct b\n        S = 0.5 * (A + A.T)\n        eigvals, eigvecs = eigh(S)\n        q = eigvecs[:, -1] # Dominant eigenvector (eigh sorts eigenvalues)\n        b = -A @ q\n\n        # 3. Generate FOM snapshots for training\n        X, _ = simulate(A, b, x0, T_train, h, M)\n\n        # 4. Compute POD basis Q\n        U, s, _ = np.linalg.svd(X, full_matrices=False)\n        Q = U[:, :r]\n\n        # 5. Calculate reconstruction error\n        # eps_rec = norm(X - Q @ Q.T @ X) / norm(X)\n        # Using singular values is more direct: sqrt(sum(s_i^2 for i>r)) / sqrt(sum(s_i^2))\n        if X.shape[1]>1:\n         norm_X_sq = np.sum(s**2)\n         if norm_X_sq > 0:\n            norm_err_sq = np.sum(s[r:]**2)\n            eps_rec = np.sqrt(norm_err_sq / norm_X_sq)\n         else:\n            eps_rec = 0.0\n        else:\n            eps_rec = 0.0\n\n\n        # 6. Form the ROM\n        Ar = Q.T @ A @ Q\n        br = Q.T @ b\n        z0 = np.zeros(r)\n\n        # 7. Simulate FOM and ROM for testing, check blow-up\n        _, fom_blew_up = simulate(A, b, x0, T_test, h, M)\n        _, rom_blew_up = simulate(Ar, br, z0, T_test, h, M)\n\n        # 8. Record results\n        results.append([round(eps_rec, 6), rom_blew_up, fom_blew_up])\n\n    # Final print statement in the exact required format.\n    # Convert bools to lowercase 'true'/'false' for JS-like format\n    formatted_results = []\n    for res in results:\n        eps_str = f\"{res[0]:.6f}\"\n        rom_bool_str = str(res[1]).lower()\n        fom_bool_str = str(res[2]).lower()\n        formatted_results.append(f\"[{eps_str},{rom_bool_str},{fom_bool_str}]\")\n\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}