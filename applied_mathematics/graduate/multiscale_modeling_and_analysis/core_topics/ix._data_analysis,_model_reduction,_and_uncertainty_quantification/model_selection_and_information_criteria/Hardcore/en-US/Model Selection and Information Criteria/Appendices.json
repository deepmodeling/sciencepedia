{
    "hands_on_practices": [
        {
            "introduction": "This first exercise provides foundational practice in applying and interpreting two of the most common information criteria, the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC). By working through a direct comparison of two models based on their maximized log-likelihoods and parameter counts, you will gain a concrete understanding of how each criterion operationalizes the principle of parsimony. This practice is essential for appreciating how the differing complexity penalties of AIC and BIC can lead to different model preferences, a core concept in rigorous model selection .",
            "id": "3780497",
            "problem": "In multiscale modeling and analysis, it is common to compare mechanistic models that operate at different effective resolutions of a system. Consider two candidate coarse-grained stochastic models, denoted by $\\mathcal{M}_1$ and $\\mathcal{M}_2$, each intended to describe the slow manifold statistics generated by an underlying fast-slow process. Both models are estimated by maximum likelihood from $n$ independent observations under standard regularity conditions that justify asymptotic approximations.\n\nYou are asked to compare the models using two information-theoretic criteria grounded in first principles:\n\n- One criterion is derived by approximating the expected out-of-sample Kullback–Leibler divergence between the true data-generating process and a fitted model, using a bias-corrected estimator based on the maximized log-likelihood and an explicit complexity penalty proportional to the number of free parameters.\n- The other criterion is derived by approximating the negative twice log marginal likelihood of the data under each model via a Laplace approximation, which yields a complexity penalty that scales with the logarithm of the sample size and the model dimension.\n\nLet $\\ell_1$ and $\\ell_2$ denote the maximized log-likelihoods of $\\mathcal{M}_1$ and $\\mathcal{M}_2$, respectively, computed on the same dataset of size $n$. Let $k_1$ and $k_2$ denote the number of free parameters in $\\mathcal{M}_1$ and $\\mathcal{M}_2$, respectively. Suppose that\n$$\n\\ell_1=-120,\\quad \\ell_2=-118,\\quad k_1=4,\\quad k_2=6,\\quad n=200.\n$$\n\nUsing the asymptotic definitions implied above, compute both criteria for $\\mathcal{M}_1$ and $\\mathcal{M}_2$, and determine which model is preferred by each criterion according to the usual convention that a smaller value indicates a better trade-off between fit and complexity. Report your final answer as a single row vector\n$$\n\\big[\\mathrm{AIC}_1,\\ \\mathrm{AIC}_2,\\ \\mathrm{BIC}_1,\\ \\mathrm{BIC}_2,\\ p_{\\mathrm{AIC}},\\ p_{\\mathrm{BIC}}\\big],\n$$\nwhere $p_{\\mathrm{AIC}}$ equals $1$ if $\\mathcal{M}_1$ is preferred by the Akaike Information Criterion (AIC), equals $2$ if $\\mathcal{M}_2$ is preferred, and equals $0$ if there is a tie; and $p_{\\mathrm{BIC}}$ is defined analogously for the Bayesian Information Criterion (BIC). Use exact symbolic expressions involving $\\ln$ and integers only; do not approximate or round.",
            "solution": "The problem requires the calculation of two information criteria for two competing models, $\\mathcal{M}_1$ and $\\mathcal{M}_2$, and the determination of which model is preferred by each criterion. The criteria are described by their theoretical derivations.\n\nFirst, we must identify the specific formulas for these criteria based on the provided descriptions.\n\nThe first criterion is described as \"derived by approximating the expected out-of-sample Kullback–Leibler divergence between the true data-generating process and a fitted model, using a bias-corrected estimator based on the maximized log-likelihood and an explicit complexity penalty proportional to the number of free parameters.\" This is the definition of the Akaike Information Criterion (AIC). Its standard formula is:\n$$\n\\mathrm{AIC} = -2\\ell + 2k\n$$\nwhere $k$ is the number of free parameters in the model and $\\ell$ is the maximized value of the log-likelihood function.\n\nThe second criterion is described as \"derived by approximating the negative twice log marginal likelihood of the data under each model via a Laplace approximation, which yields a complexity penalty that scales with the logarithm of the sample size and the model dimension.\" This is the definition of the Bayesian Information Criterion (BIC), also known as the Schwarz Criterion. Its formula is:\n$$\n\\mathrm{BIC} = -2\\ell + k\\ln(n)\n$$\nwhere $n$ is the number of observations (sample size).\n\nFor both AIC and BIC, the convention is that a lower value indicates a better model, representing a more favorable trade-off between goodness of fit (high $\\ell$) and model complexity (low $k$).\n\nThe problem provides the following values:\n- For model $\\mathcal{M}_1$: maximized log-likelihood $\\ell_1 = -120$ and number of parameters $k_1 = 4$.\n- For model $\\mathcal{M}_2$: maximized log-likelihood $\\ell_2 = -118$ and number of parameters $k_2 = 6$.\n- The sample size is $n=200$.\n\nNow, we compute the AIC for each model.\nFor model $\\mathcal{M}_1$:\n$$\n\\mathrm{AIC}_1 = -2\\ell_1 + 2k_1 = -2(-120) + 2(4) = 240 + 8 = 248.\n$$\nFor model $\\mathcal{M}_2$:\n$$\n\\mathrm{AIC}_2 = -2\\ell_2 + 2k_2 = -2(-118) + 2(6) = 236 + 12 = 248.\n$$\nTo determine the preferred model according to AIC, we compare $\\mathrm{AIC}_1$ and $\\mathrm{AIC}_2$. Since $\\mathrm{AIC}_1 = \\mathrm{AIC}_2 = 248$, neither model is preferred over the other. This is considered a tie. According to the problem's definition, we set $p_{\\mathrm{AIC}} = 0$.\n\nNext, we compute the BIC for each model.\nFor model $\\mathcal{M}_1$:\n$$\n\\mathrm{BIC}_1 = -2\\ell_1 + k_1\\ln(n) = -2(-120) + 4\\ln(200) = 240 + 4\\ln(200).\n$$\nFor model $\\mathcal{M}_2$:\n$$\n\\mathrm{BIC}_2 = -2\\ell_2 + k_2\\ln(n) = -2(-118) + 6\\ln(200) = 236 + 6\\ln(200).\n$$\nThe problem requires the answer in exact symbolic form, so we leave these expressions as they are.\n\nTo determine the preferred model according to BIC, we compare $\\mathrm{BIC}_1$ and $\\mathrm{BIC}_2$. Let's examine their difference:\n$$\n\\mathrm{BIC}_2 - \\mathrm{BIC}_1 = \\big(236 + 6\\ln(200)\\big) - \\big(240 + 4\\ln(200)\\big) = (236 - 240) + (6-4)\\ln(200) = -4 + 2\\ln(200).\n$$\nTo determine the sign of this difference, we need to compare $2\\ln(200)$ with $4$, which is equivalent to comparing $\\ln(200)$ with $2$. The natural logarithm function, $\\ln(x)$, is monotonically increasing. The constant $e$ is approximately $2.718$, so $e^{2} \\approx (2.718)^{2} \\approx 7.389$. Since $200 > e^{2}$, it follows that $\\ln(200) > \\ln(e^{2})$, which simplifies to $\\ln(200) > 2$.\nTherefore, $2\\ln(200) > 4$, which means the difference $-4 + 2\\ln(200)$ is positive.\nSo, $\\mathrm{BIC}_2 - \\mathrm{BIC}_1 > 0$, which implies $\\mathrm{BIC}_2 > \\mathrm{BIC}_1$.\nSince the model with the lower BIC value is preferred, model $\\mathcal{M}_1$ is preferred by the BIC. According to the problem's definition, we set $p_{\\mathrm{BIC}} = 1$.\n\nFinally, we assemble the required row vector:\n$$\n\\big[\\mathrm{AIC}_1,\\ \\mathrm{AIC}_2,\\ \\mathrm{BIC}_1,\\ \\mathrm{BIC}_2,\\ p_{\\mathrm{AIC}},\\ p_{\\mathrm{BIC}}\\big].\n$$\nSubstituting the computed values:\n- $\\mathrm{AIC}_1 = 248$\n- $\\mathrm{AIC}_2 = 248$\n- $\\mathrm{BIC}_1 = 240 + 4\\ln(200)$\n- $\\mathrm{BIC}_2 = 236 + 6\\ln(200)$\n- $p_{\\mathrm{AIC}} = 0$\n- $p_{\\mathrm{BIC}} = 1$\n\nThe final vector is $[248, 248, 240 + 4\\ln(200), 236 + 6\\ln(200), 0, 1]$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n248  248  240 + 4\\ln(200)  236 + 6\\ln(200)  0  1\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Building upon the basics, this practice explores model selection within a family of nested autoregressive models, a common task in time-series analysis and multiscale modeling. You will compute not only AIC and BIC but also the corrected Akaike Information Criterion (AICc), learning to assess when small-sample corrections are relevant . This exercise sharpens your ability to compare multiple competing models and interpret the subtle but important differences between these key information criteria.",
            "id": "3780541",
            "problem": "A univariate Gaussian autoregressive process at a single resolution is often used as a coarse-grained component within a multiscale time-series model. Consider fitting an autoregressive model of order $p$, denoted $\\mathrm{AR}(p)$, to an $n$-length series with $n=300$, where $p\\in\\{1,2,3\\}$. For each fit, assume the parametric family includes $p$ autoregressive coefficients, one intercept, and one innovation variance, so the total number of free parameters is $k=p+2$. The maximized log-likelihoods for the three fits are $-210$ for $p=1$, $-205$ for $p=2$, and $-203$ for $p=3$ under a Gaussian innovation assumption. Using the foundational principles of maximum likelihood estimation and information-theoretic model comparison, compute the values of the Akaike Information Criterion (AIC), the small-sample corrected Akaike Information Criterion (AICc), and the Bayesian Information Criterion (BIC) for each $p\\in\\{1,2,3\\}$, and briefly interpret the differences among these criteria in terms of their implications for model selection in multiscale modeling. Round every criterion value to four significant figures, and express the final numerical answer with no units. Return your final answer as a row matrix whose entries are, in order,\n$$\\text{AIC}(p=1),\\ \\text{AICc}(p=1),\\ \\text{BIC}(p=1),\\ \\text{AIC}(p=2),\\ \\text{AICc}(p=2),\\ \\text{BIC}(p=2),\\ \\text{AIC}(p=3),\\ \\text{AICc}(p=3),\\ \\text{BIC}(p=3).$$",
            "solution": "The problem statement is subjected to validation before a solution is attempted.\n\n### Step 1: Extract Givens\n-   **Process**: A univariate Gaussian autoregressive process.\n-   **Model**: Autoregressive model of order $p$, denoted $\\mathrm{AR}(p)$.\n-   **Sample size**: $n = 300$.\n-   **Model orders considered**: $p \\in \\{1, 2, 3\\}$.\n-   **Number of free parameters**: $k = p + 2$. This includes $p$ autoregressive coefficients, one intercept, and one innovation variance.\n-   **Maximized log-likelihoods** ($\\ln(L)$):\n    -   For $p=1$, $\\ln(L_1) = -210$.\n    -   For $p=2$, $\\ln(L_2) = -205$.\n    -   For $p=3$, $\\ln(L_3) = -203$.\n-   **Required computations**: Akaike Information Criterion (AIC), small-sample corrected Akaike Information Criterion (AICc), and Bayesian Information Criterion (BIC) for each $p$.\n-   **Required output format**: A row matrix containing the nine computed criterion values, rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective. It provides a complete and consistent set of data and definitions required to calculate standard, well-defined statistical metrics. The scenario is a typical application of model selection in time-series analysis, which is a foundational component of multiscale modeling. The provided values for sample size, model order, and log-likelihood are plausible. The problem does not violate any scientific principles, is not ambiguous, and is directly solvable using established formalisms.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Solution Derivation\nThe task is to compute three information criteria for three different autoregressive models and interpret the results. The criteria are defined as follows:\n\n1.  **Akaike Information Criterion (AIC)**:\n    $$ \\mathrm{AIC} = -2\\ln(L) + 2k $$\n    where $k$ is the number of estimated parameters and $\\ln(L)$ is the maximized log-likelihood.\n\n2.  **Corrected Akaike Information Criterion (AICc)**:\n    $$ \\mathrm{AICc} = \\mathrm{AIC} + \\frac{2k(k+1)}{n-k-1} $$\n    where $n$ is the sample size. This criterion adjusts the AIC for small sample sizes.\n\n3.  **Bayesian Information Criterion (BIC)**:\n    $$ \\mathrm{BIC} = -2\\ln(L) + k\\ln(n) $$\n    This criterion imposes a stronger penalty on model complexity than AIC, especially for large $n$.\n\nWe are given $n=300$ and $k=p+2$. We compute the criteria for each $p \\in \\{1, 2, 3\\}$.\n\n**Case 1: Model AR(1), $p=1$**\n-   Number of parameters: $k_1 = 1 + 2 = 3$.\n-   Maximized log-likelihood: $\\ln(L_1) = -210$.\n\n-   $\\mathrm{AIC}_1 = -2\\ln(L_1) + 2k_1 = -2(-210) + 2(3) = 420 + 6 = 426$.\n    Rounding to four significant figures gives $426.0$.\n-   $\\mathrm{AICc}_1 = \\mathrm{AIC}_1 + \\frac{2k_1(k_1+1)}{n-k_1-1} = 426 + \\frac{2(3)(3+1)}{300-3-1} = 426 + \\frac{24}{296} \\approx 426 + 0.081081... = 426.081081...$.\n    Rounding to four significant figures gives $426.1$.\n-   $\\mathrm{BIC}_1 = -2\\ln(L_1) + k_1\\ln(n) = -2(-210) + 3\\ln(300) = 420 + 3\\ln(300) \\approx 420 + 3(5.70378) = 420 + 17.11134 = 437.11134...$.\n    Rounding to four significant figures gives $437.1$.\n\n**Case 2: Model AR(2), $p=2$**\n-   Number of parameters: $k_2 = 2 + 2 = 4$.\n-   Maximized log-likelihood: $\\ln(L_2) = -205$.\n\n-   $\\mathrm{AIC}_2 = -2\\ln(L_2) + 2k_2 = -2(-205) + 2(4) = 410 + 8 = 418$.\n    Rounding to four significant figures gives $418.0$.\n-   $\\mathrm{AICc}_2 = \\mathrm{AIC}_2 + \\frac{2k_2(k_2+1)}{n-k_2-1} = 418 + \\frac{2(4)(4+1)}{300-4-1} = 418 + \\frac{40}{295} \\approx 418 + 0.135593... = 418.135593...$.\n    Rounding to four significant figures gives $418.1$.\n-   $\\mathrm{BIC}_2 = -2\\ln(L_2) + k_2\\ln(n) = -2(-205) + 4\\ln(300) = 410 + 4\\ln(300) \\approx 410 + 4(5.70378) = 410 + 22.81512 = 432.81512...$.\n    Rounding to four significant figures gives $432.8$.\n\n**Case 3: Model AR(3), $p=3$**\n-   Number of parameters: $k_3 = 3 + 2 = 5$.\n-   Maximized log-likelihood: $\\ln(L_3) = -203$.\n\n-   $\\mathrm{AIC}_3 = -2\\ln(L_3) + 2k_3 = -2(-203) + 2(5) = 406 + 10 = 416$.\n    Rounding to four significant figures gives $416.0$.\n-   $\\mathrm{AICc}_3 = \\mathrm{AIC}_3 + \\frac{2k_3(k_3+1)}{n-k_3-1} = 416 + \\frac{2(5)(5+1)}{300-5-1} = 416 + \\frac{60}{294} \\approx 416 + 0.204081... = 416.204081...$.\n    Rounding to four significant figures gives $416.2$.\n-   $\\mathrm{BIC}_3 = -2\\ln(L_3) + k_3\\ln(n) = -2(-203) + 5\\ln(300) = 406 + 5\\ln(300) \\approx 406 + 5(5.70378) = 406 + 28.51890 = 434.51890...$.\n    Rounding to four significant figures gives $434.5$.\n\n### Interpretation\nThe principle of model selection using these criteria is to choose the model with the minimum criterion value.\n\n-   **AIC and AICc Selection**:\n    -   $\\mathrm{AIC}$ values: $\\mathrm{AIC}_1 = 426.0$, $\\mathrm{AIC}_2 = 418.0$, $\\mathrm{AIC}_3 = 416.0$.\n    -   $\\mathrm{AICc}$ values: $\\mathrm{AICc}_1 = 426.1$, $\\mathrm{AICc}_2 = 418.1$, $\\mathrm{AICc}_3 = 416.2$.\n    Both AIC and AICc select the $\\mathrm{AR}(3)$ model ($p=3$) as it has the lowest value. The AICc correction is small because the sample size $n=300$ is large relative to the number of parameters $k$ (at most $5$), so $n-k-1$ is not small. Specifically, the ratio $k/n$ is small.\n\n-   **BIC Selection**:\n    -   $\\mathrm{BIC}$ values: $\\mathrm{BIC}_1 = 437.1$, $\\mathrm{BIC}_2 = 432.8$, $\\mathrm{BIC}_3 = 434.5$.\n    The BIC selects the $\\mathrm{AR}(2)$ model ($p=2$) as it corresponds to the minimum BIC value.\n\n-   **Comparison and Implications**:\n    The discrepancy in model selection arises from the different penalty terms for model complexity. The AIC penalty term is $2k$, whereas the BIC penalty term is $k\\ln(n)$. For $n=300$, $\\ln(300) \\approx 5.70$. Thus, BIC's penalty per parameter ($\\approx 5.70$) is substantially larger than AIC's penalty ($2$). This makes BIC favor more parsimonious (simpler) models.\n    In this case, moving from the $\\mathrm{AR}(2)$ to the $\\mathrm{AR}(3)$ model improves the log-likelihood from $-205$ to $-203$. For AIC, this improvement in fit (which reduces the $-2\\ln(L)$ term by 4) outweighs the penalty increase ($2(k_3-k_2) = 2(1) = 2$). For BIC, the same improvement in fit is not sufficient to overcome the larger penalty increase ($(k_3-k_2)\\ln(n) \\approx 5.70$).\n    In the context of multiscale modeling, this highlights a fundamental trade-off. Choosing component models at each scale (e.g., the coarse-grained AR process here) involves balancing fidelity and complexity. AIC might be preferred if the goal is predictive accuracy, as it can select slightly more complex models that capture finer details. BIC, with its preference for parsimony, might be chosen if the objective is to identify the most probable underlying generative structure, avoiding overfitting at each scale, which can be critical for the stability and interpretability of the overall multiscale model.\n\nThe final values for the row matrix are:\n-   $p=1$: $426.0$, $426.1$, $437.1$\n-   $p=2$: $418.0$, $418.1$, $432.8$\n-   $p=3$: $416.0$, $416.2$, $434.5$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n426.0  426.1  437.1  418.0  418.1  432.8  416.0  416.2  434.5\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "This final practice advances from calculation to implementation, guiding you through a computational sensitivity analysis for model selection in a multiscale context. By coding and applying both a generalized likelihood-based criterion and a Bayesian posterior-based score, you will investigate how the choice of penalty strength and prior assumptions can systematically alter model selection outcomes . This advanced exercise demonstrates how to assess the robustness of scientific conclusions derived from model selection, a crucial skill for research and application.",
            "id": "3780533",
            "problem": "Consider a synthetic two-scale linear regression setting in which a response signal is generated by the superposition of a coarse-scale component and a fine-scale component with additive Gaussian noise. Let there be $n$ observations indexed by $t \\in \\{1,2,\\dots,n\\}$, and define a normalized time coordinate $u_t = \\frac{t-1}{n-1} \\in [0,1]$. Construct two predictors by $x_c(t) = \\sin(2\\pi C u_t)$ and $x_f(t) = \\sin(2\\pi F u_t)$, where $C$ and $F$ are positive integers with $C \\ll F$ to represent coarse and fine temporal scales. The response is generated as $y(t) = \\beta_0 + \\beta_c x_c(t) + \\beta_f x_f(t) + \\epsilon_t$, with $\\epsilon_t \\sim \\mathcal{N}(0,\\sigma^2)$ independent and identically distributed. The candidate models considered for selection are:\n- $M_1$: intercept-only, i.e., $y(t) = \\beta_0 + \\epsilon_t$;\n- $M_2$: coarse-only, i.e., $y(t) = \\beta_0 + \\beta_c x_c(t) + \\epsilon_t$;\n- $M_3$: fine-only, i.e., $y(t) = \\beta_0 + \\beta_f x_f(t) + \\epsilon_t$;\n- $M_4$: coarse-plus-fine, i.e., $y(t) = \\beta_0 + \\beta_c x_c(t) + \\beta_f x_f(t) + \\epsilon_t$.\n\nYour task is to implement a sensitivity analysis that varies the penalty parameter and prior assumptions to assess the robustness of model selection decisions across scales. The program must construct the described dataset and select models using two complementary criteria derived from first principles:\n\n1. A likelihood-based information criterion with a tunable penalty, defined by starting from the Gaussian log-likelihood under unknown noise variance and adding a linear penalty in the number of estimated parameters. For each model, let $p$ denote the number of regression coefficients estimated (including the intercept), and treat the variance $\\sigma^2$ as an additional estimated parameter. Define a generalized information score that depends on a user-specified penalty parameter $\\alpha  0$ and use it to select the model that optimally balances quality-of-fit and complexity.\n\n2. A Bayesian posterior-based score that incorporates Gaussian priors on regression coefficients to reflect prior assumptions about scales: assign a zero-mean Gaussian prior with variance $\\tau_c^2$ to the coarse-scale coefficient and variance $\\tau_f^2$ to the fine-scale coefficient, while treating the intercept as unpenalized. Assume the noise variance $\\sigma^2$ is known for this Bayesian criterion. Derive a posterior-based score by combining the Gaussian log-likelihood with the Gaussian log prior, and include a tunable complexity penalty parameter $\\alpha$ as a separate robustness control. Select the model that minimizes this posterior-based score.\n\nFundamental base to use:\n- The Gaussian probability density function for a scalar observation with mean $\\mu$ and variance $\\sigma^2$ is $p(y \\mid \\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y-\\mu)^2}{2\\sigma^2}\\right)$, which implies the log-likelihood of independent Gaussian errors is the sum of individual log densities.\n- In linear regression with Gaussian errors, the maximum likelihood estimate (MLE) of regression coefficients minimizes the sum of squared residuals, and when $\\sigma^2$ is unknown, its MLE equals the residual mean square.\n- For a Gaussian prior $p(\\beta_j) = \\mathcal{N}(0,\\tau_j^2)$, the log prior is given by $\\log p(\\beta_j) = -\\frac{1}{2}\\log(2\\pi\\tau_j^2) - \\frac{\\beta_j^2}{2\\tau_j^2}$, and the maximum a posteriori (MAP) estimate minimizes the sum of a data fit term and a quadratic regularization term.\n\nYour program must:\n- Generate the dataset with the following fixed parameters: $n = 200$, $C = 2$, $F = 15$, $\\beta_0 = 0$, $\\beta_c = 1$, $\\beta_f = 0.3$, and $\\sigma = 0.2$. Use a deterministic pseudorandom generator seed so the results are reproducible.\n- For each candidate model $M_1$ through $M_4$, compute the MLE under unknown $\\sigma^2$, the corresponding Gaussian log-likelihood at the fitted values, and the generalized likelihood-based information score with a penalty $\\alpha$ times the total number of estimated parameters $k = p + 1$, where $p$ is the number of regression coefficients and the additional $1$ accounts for the unknown variance parameter.\n- For each candidate model $M_1$ through $M_4$, compute the MAP estimate with known $\\sigma^2$, the corresponding Gaussian log-likelihood at the MAP estimate with known variance, and the Gaussian log prior for the penalized coefficients present in the model, and then compute the posterior-based score by combining these with a penalty $\\alpha$ times the number of regression coefficients $p$.\n- For each test case, return the indices of the selected models under the likelihood-based score and under the posterior-based score, and a boolean indicating whether the selections agree, i.e., whether the selection is robust to the presence of priors at the given settings.\n\nTest suite:\nEvaluate the sensitivity across the following six test cases, each specified by the triple $(\\alpha,\\tau_c^2,\\tau_f^2)$, with the dataset fixed as above:\n- Case $1$: $(\\alpha = 2,\\ \\tau_c^2 = 10^6,\\ \\tau_f^2 = 10^6)$, representing an Akaike-type penalty and uninformative priors.\n- Case $2$: $(\\alpha = 0,\\ \\tau_c^2 = 10^6,\\ \\tau_f^2 = 10^6)$, representing no penalty and uninformative priors.\n- Case $3$: $(\\alpha = \\log n,\\ \\tau_c^2 = 10^6,\\ \\tau_f^2 = 10^6)$, representing a Bayesian-type penalty magnitude and uninformative priors.\n- Case $4$: $(\\alpha = 2,\\ \\tau_c^2 = 10^6,\\ \\tau_f^2 = 10^{-4})$, representing strong prior shrinkage at the fine scale.\n- Case $5$: $(\\alpha = 2,\\ \\tau_c^2 = 10^{-4},\\ \\tau_f^2 = 10^6)$, representing strong prior shrinkage at the coarse scale.\n- Case $6$: $(\\alpha = \\log n,\\ \\tau_c^2 = 10^{-6},\\ \\tau_f^2 = 10^{-6})$, representing strong prior shrinkage at both scales under a strong penalty.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to a test case and is itself a list of the form $[m_{\\text{like}}, m_{\\text{post}}, r]$, with $m_{\\text{like}}$ and $m_{\\text{post}}$ being integers in $\\{1,2,3,4\\}$ denoting the selected model under the likelihood-based and posterior-based scores, respectively, and $r$ being a boolean indicating agreement. For example: $[[1,1,True],[4,4,True],\\dots]$.",
            "solution": "The problem requires a comparative analysis of two model selection criteria—one likelihood-based and one posterior-based—in a synthetic two-scale linear regression setting. The analysis involves generating a dataset, fitting four candidate models of increasing complexity, and evaluating their scores under various hyperparameter configurations.\n\nFirst, we establish the general matrix formulation for the linear models. The relationship between the response vector $\\mathbf{y} \\in \\mathbb{R}^n$, the design matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$, the coefficient vector $\\boldsymbol{\\beta} \\in \\mathbb{R}^p$, and the error vector $\\boldsymbol{\\epsilon} \\in \\mathbb{R}^n$ is given by:\n$$\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\n$$\nwhere $\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2\\mathbf{I}_n)$. The number of regression coefficients, $p$, depends on the model $M_i$. The columns of $\\mathbf{X}$ are constructed from the intercept (a vector of ones), the coarse-scale predictor $x_c(t)$, and the fine-scale predictor $x_f(t)$, as specified for each model $M_1, M_2, M_3, M_4$. The data are generated using the fixed parameters $n=200$, $C=2$, $F=15$, and true coefficients $\\beta_0=0, \\beta_c=1, \\beta_f=0.3$ with noise standard deviation $\\sigma=0.2$.\n\n### 1. Likelihood-Based Information Score\n\nThis criterion is based on the principle of maximum likelihood estimation (MLE) with an explicit penalty for model complexity. For a given model with $p$ regression coefficients, we treat the noise variance $\\sigma^2$ as an additional parameter to be estimated, making the total number of estimated parameters $k = p+1$.\n\nThe log-likelihood function for the data, given parameters $\\boldsymbol{\\beta}$ and $\\sigma^2$, is:\n$$\n\\mathcal{L}(\\boldsymbol{\\beta}, \\sigma^2; \\mathbf{y}, \\mathbf{X}) = -\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\n$$\nThe maximum likelihood estimate for $\\boldsymbol{\\beta}$ is the ordinary least squares (OLS) solution, which minimizes the residual sum of squares (RSS):\n$$\n\\hat{\\boldsymbol{\\beta}}_{\\text{MLE}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\n$$\nThe RSS at this estimate is $RSS = (\\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}_{\\text{MLE}})^T(\\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}_{\\text{MLE}})$. The MLE for the variance $\\sigma^2$ is then:\n$$\n\\hat{\\sigma}^2_{\\text{MLE}} = \\frac{RSS}{n}\n$$\nSubstituting these estimates back into the log-likelihood function gives the maximized log-likelihood $\\hat{\\mathcal{L}}$:\n$$\n\\hat{\\mathcal{L}} = \\mathcal{L}(\\hat{\\boldsymbol{\\beta}}_{\\text{MLE}}, \\hat{\\sigma}^2_{\\text{MLE}}) = -\\frac{n}{2}\\left(\\log(2\\pi\\hat{\\sigma}^2_{\\text{MLE}}) + 1\\right)\n$$\nThe problem specifies a generalized information score. Following the convention of criteria like AIC, we define the score to be minimized as:\n$$\nScore_{\\text{like}} = -2\\hat{\\mathcal{L}} + \\alpha \\cdot k = n\\left(\\log(2\\pi\\hat{\\sigma}^2_{\\text{MLE}}) + 1\\right) + \\alpha(p+1)\n$$\nFor each model $M_i$, we calculate this score. The model with the minimum score is selected. The parameter $p$ is $1$ for $M_1$, $2$ for $M_2$ and $M_3$, and $3$ for $M_4$.\n\n### 2. Bayesian Posterior-Based Score\n\nThis criterion uses Bayesian principles, incorporating prior beliefs about the model parameters. The variance $\\sigma^2$ is assumed to be known (fixed at the true value $\\sigma^2=0.2^2=0.04$). The regression coefficients $\\beta_c$ and $\\beta_f$ are assigned zero-mean Gaussian priors: $\\beta_c \\sim \\mathcal{N}(0, \\tau_c^2)$ and $\\beta_f \\sim \\mathcal{N}(0, \\tau_f^2)$. The intercept $\\beta_0$ is unpenalized, which corresponds to an improper uniform prior or a Gaussian prior with infinite variance.\n\nAccording to Bayes' theorem, the posterior probability of the parameters is proportional to the product of the likelihood and the prior: $p(\\boldsymbol{\\beta} | \\mathbf{y}, \\mathbf{X}, \\sigma^2) \\propto p(\\mathbf{y} | \\mathbf{X}, \\boldsymbol{\\beta}, \\sigma^2) p(\\boldsymbol{\\beta})$. The maximum a posteriori (MAP) estimate $\\hat{\\boldsymbol{\\beta}}_{\\text{MAP}}$ maximizes this posterior probability, which is equivalent to minimizing the negative log-posterior:\n$$\n-\\log p(\\boldsymbol{\\beta} | \\mathbf{y}) \\propto -\\log p(\\mathbf{y} | \\boldsymbol{\\beta}) - \\log p(\\boldsymbol{\\beta})\n$$\nMinimizing this is equivalent to minimizing:\n$$\n\\frac{1}{2\\sigma^2}||\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}||_2^2 + \\sum_{j \\in \\text{penalized}} \\frac{\\beta_j^2}{2\\tau_j^2}\n$$\nThis is a Ridge Regression problem. The solution is:\n$$\n\\hat{\\boldsymbol{\\beta}}_{\\text{MAP}} = (\\mathbf{X}^T\\mathbf{X} + \\sigma^2\\mathbf{P})^{-1}\\mathbf{X}^T\\mathbf{y}\n$$\nwhere $\\mathbf{P}$ is a diagonal matrix of inverse prior variances. For a model with coefficients $(\\beta_0, \\beta_c, \\beta_f)$, $\\mathbf{P} = \\text{diag}(0, 1/\\tau_c^2, 1/\\tau_f^2)$. The structure of $\\mathbf{P}$ adapts to the coefficients present in each model. For $M_1$, $\\mathbf{P}$ is a zero matrix.\n\nThe posterior-based score to be minimized is defined by combining the negative log-posterior (evaluated at the MAP estimate) with an explicit complexity penalty:\n$$\nScore_{\\text{post}} = -\\left( \\mathcal{L}(\\hat{\\boldsymbol{\\beta}}_{\\text{MAP}}, \\sigma^2) + \\log p(\\hat{\\boldsymbol{\\beta}}_{\\text{MAP}}) \\right) + \\alpha \\cdot p\n$$\nwhere $p$ is the number of regression coefficients in the model. The terms are:\n- Log-likelihood at MAP: $\\mathcal{L}(\\hat{\\boldsymbol{\\beta}}_{\\text{MAP}}, \\sigma^2) = -\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}||\\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}_{\\text{MAP}}||_2^2$.\n- Log-prior at MAP: $\\log p(\\hat{\\boldsymbol{\\beta}}_{\\text{MAP}}) = \\sum_{j \\in \\text{penalized}} \\left( -\\frac{1}{2}\\log(2\\pi\\tau_j^2) - \\frac{\\hat{\\beta}_{j, \\text{MAP}}^2}{2\\tau_j^2} \\right)$. The sum is over the coefficients present in the model that have a specified prior.\n\nFor each model $M_i$, this score is calculated. The model with the minimum score is selected.\n\n### Computational Procedure\n\nFor each of the six test cases defined by $(\\alpha, \\tau_c^2, \\tau_f^2)$:\n1. The synthetic dataset is held constant.\n2. For each of the four models ($M_1$ to $M_4$):\n    a. Construct the appropriate design matrix $\\mathbf{X}$.\n    b. Calculate $Score_{\\text{like}}$ using the MLE procedure.\n    c. Calculate $Score_{\\text{post}}$ using the MAP procedure.\n3. Identify the model index ($1$ to $4$) that minimizes $Score_{\\text{like}}$ (let's call it $m_{\\text{like}}$).\n4. Identify the model index that minimizes $Score_{\\text{post}}$ (let's call it $m_{\\text{post}}$).\n5. Determine if the selections agree: $r = (m_{\\text{like}} == m_{\\text{post}})$.\n6. The result for the test case is the list $[m_{\\text{like}}, m_{\\text{post}}, r]$.\n\nThis process is repeated for all test cases, and the results are aggregated into a final list.",
            "answer": "$$\n\\boxed{\\text{[[4,4,True],[4,4,True],[2,2,True],[2,2,True],[4,1,False],[1,1,True]]}}\n$$"
        }
    ]
}