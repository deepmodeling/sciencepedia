{
    "hands_on_practices": [
        {
            "introduction": "这个练习是贝叶斯推断中的一个基础实践。它通过一个高斯似然函数和正态-逆伽马共轭先验的例子，展示了共轭性的优雅概念——即后验分布与先验分布属于同一分布族，从而允许我们以解析形式清晰地更新知识。掌握这种解析更新过程是理解数据如何系统性地修正我们对模型参数信念的关键。",
            "id": "3770713",
            "problem": "考虑一个两级多尺度模型，其中精细尺度的微观模拟产生摘要，这些摘要被聚合成 $n$ 个粗尺度观测值 $y_1, y_2, \\dots, y_n$，用于对未知的粗粒度偏差参数 $\\theta$ 和粗尺度噪声方差 $\\sigma^2$ 进行逆向不确定性量化。假设条件数据模型 $y_i \\mid \\theta, \\sigma^2$ 是独立同分布的高斯分布，即 $y_i \\mid \\theta, \\sigma^2 \\sim \\mathcal{N}(\\theta, \\sigma^2)$，并采用分层的正态–逆伽马先验，定义为 $\\theta \\mid \\sigma^2 \\sim \\mathcal{N}(m_0, \\sigma^2 / \\kappa_0)$ 和 $\\sigma^2 \\sim \\operatorname{InvGamma}(\\alpha_0, \\beta_0)$，其中 $\\operatorname{InvGamma}$ 表示形状为 $\\alpha_0$、尺度为 $\\beta_0$ 的逆伽马分布。这种正态–逆伽马（NIG）先验通过允许粗尺度上的方差为粗粒度偏差的分布范围提供信息，从而耦合了不同尺度。\n\n从贝叶斯定理以及高斯和逆伽马密度的定义出发，推导全条件分布 $p(\\theta \\mid \\sigma^2, y_1, \\dots, y_n)$ 和 $p(\\sigma^2 \\mid \\theta, y_1, \\dots, y_n)$。然后，通过将联合后验 $p(\\theta, \\sigma^2 \\mid y_1, \\dots, y_n)$ 表示为正态–逆伽马族的形式来展示其共轭性，并以 $(m_0, \\kappa_0, \\alpha_0, \\beta_0)$ 和数据的充分统计量 $(n, \\bar{y}, \\sum_{i=1}^{n} (y_i - \\bar{y})^2)$ 的函数形式，识别出更新后的超参数 $(m_n, \\kappa_n, \\alpha_n, \\beta_n)$ 的闭式解，其中 $\\bar{y} = \\frac{1}{n} \\sum_{i=1}^{n} y_i$。\n\n您的最终答案必须是更新后的超参数向量 $(m_n, \\kappa_n, \\alpha_n, \\beta_n)$，使用 $\\operatorname{pmatrix}$ 环境写成一个单行矩阵。不要在最终答案中提供中间步骤。不需要进行数值舍入，也不涉及任何单位。所有数学符号必须用 LaTeX 书写。",
            "solution": "这是一个有效的贝叶斯统计问题，因为它是一个适定的、有科学依据的、客观的问题。我们被要求推导在给定一组观测值 $y_1, \\dots, y_n$ 的情况下，参数 $\\theta$ 和 $\\sigma^2$ 的全条件后验分布和联合后验分布。该模型假设一个高斯似然和一个共轭的正态-逆伽马（NIG）先验。\n\n模型具体如下：\n似然：$y_i \\mid \\theta, \\sigma^2 \\sim \\mathcal{N}(\\theta, \\sigma^2)$，对于 $i=1, \\dots, n$，假设独立同分布。\n先验：一个分层先验，由 $p(\\theta, \\sigma^2) = p(\\theta \\mid \\sigma^2)p(\\sigma^2)$ 给出，其中：\n$\\theta \\mid \\sigma^2 \\sim \\mathcal{N}(m_0, \\sigma^2 / \\kappa_0)$\n$\\sigma^2 \\sim \\operatorname{InvGamma}(\\alpha_0, \\beta_0)$\n\n概率密度函数（PDF）为：\n-   $p(y_i \\mid \\theta, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\theta)^2}{2\\sigma^2}\\right)$\n-   $p(\\theta \\mid \\sigma^2) = \\sqrt{\\frac{\\kappa_0}{2\\pi\\sigma^2}} \\exp\\left(-\\frac{\\kappa_0(\\theta - m_0)^2}{2\\sigma^2}\\right)$\n-   $p(\\sigma^2) = \\frac{\\beta_0^{\\alpha_0}}{\\Gamma(\\alpha_0)} (\\sigma^2)^{-(\\alpha_0+1)} \\exp\\left(-\\frac{\\beta_0}{\\sigma^2}\\right)$\n\n根据贝叶斯定理，联合后验分布正比于似然与先验的乘积：\n$p(\\theta, \\sigma^2 \\mid \\mathbf{y}) \\propto p(\\mathbf{y} \\mid \\theta, \\sigma^2) p(\\theta, \\sigma^2)$\n其中 $\\mathbf{y} = (y_1, \\dots, y_n)$。\n$p(\\theta, \\sigma^2 \\mid \\mathbf{y}) \\propto \\left(\\prod_{i=1}^{n} p(y_i \\mid \\theta, \\sigma^2)\\right) p(\\theta \\mid \\sigma^2) p(\\sigma^2)$\n\n代入概率密度函数并舍去不依赖于 $\\theta$ 或 $\\sigma^2$ 的常数：\n$p(\\theta, \\sigma^2 \\mid \\mathbf{y}) \\propto \\left[ (\\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(y_i - \\theta)^2\\right) \\right] \\times \\left[ (\\sigma^2)^{-1/2} \\exp\\left(-\\frac{\\kappa_0(\\theta - m_0)^2}{2\\sigma^2}\\right) \\right] \\times \\left[ (\\sigma^2)^{-(\\alpha_0+1)} \\exp\\left(-\\frac{\\beta_0}{\\sigma^2}\\right) \\right]$\n\n合并各项：\n$p(\\theta, \\sigma^2 \\mid \\mathbf{y}) \\propto (\\sigma^2)^{-(\\alpha_0 + n/2 + 1/2 + 1)} \\exp\\left\\{ -\\frac{1}{2\\sigma^2} \\left[ \\sum_{i=1}^{n}(y_i - \\theta)^2 + \\kappa_0(\\theta - m_0)^2 + 2\\beta_0 \\right] \\right\\}$\n\n首先，我们推导全条件分布 $p(\\theta \\mid \\sigma^2, \\mathbf{y})$。我们将 $\\sigma^2$ 和 $\\mathbf{y}$ 视为给定，并将后验视为 $\\theta$ 的函数：\n$p(\\theta \\mid \\sigma^2, \\mathbf{y}) \\propto \\exp\\left\\{ -\\frac{1}{2\\sigma^2} \\left[ \\sum_{i=1}^{n}(y_i - \\theta)^2 + \\kappa_0(\\theta - m_0)^2 \\right] \\right\\}$\n指数中的项是关于 $\\theta$ 的二次式。让我们展开并按 $\\theta$ 的幂次分组：\n$\\sum_{i=1}^{n}(y_i^2 - 2y_i\\theta + \\theta^2) + \\kappa_0(\\theta^2 - 2m_0\\theta + m_0^2) = (n+\\kappa_0)\\theta^2 - 2(n\\bar{y} + \\kappa_0m_0)\\theta + (\\sum y_i^2 + \\kappa_0m_0^2)$\n其中 $\\bar{y} = \\frac{1}{n}\\sum y_i$。该表达式的形式为 $A\\theta^2 - 2B\\theta + C$。配方后：\n$A\\theta^2 - 2B\\theta + C = A(\\theta - B/A)^2 + C - B^2/A$\n这里，$A = n+\\kappa_0$，$B = n\\bar{y}+\\kappa_0m_0$。所以，$\\theta$ 的核是：\n$p(\\theta \\mid \\sigma^2, \\mathbf{y}) \\propto \\exp\\left\\{ -\\frac{n+\\kappa_0}{2\\sigma^2} \\left( \\theta - \\frac{n\\bar{y}+\\kappa_0m_0}{n+\\kappa_0} \\right)^2 \\right\\}$\n这是一个均值为 $\\frac{n\\bar{y}+\\kappa_0m_0}{n+\\kappa_0}$、方差为 $\\frac{\\sigma^2}{n+\\kappa_0}$ 的正态分布的核。\n所以，$\\theta$ 的全条件分布为：\n$\\theta \\mid \\sigma^2, \\mathbf{y} \\sim \\mathcal{N}\\left(\\frac{\\kappa_0m_0 + n\\bar{y}}{\\kappa_0+n}, \\frac{\\sigma^2}{\\kappa_0+n}\\right)$\n\n接下来，我们推导全条件分布 $p(\\sigma^2 \\mid \\theta, \\mathbf{y})$。我们将 $\\theta$ 和 $\\mathbf{y}$ 视为给定，并将后验视为 $\\sigma^2$ 的函数：\n$p(\\sigma^2 \\mid \\theta, \\mathbf{y}) \\propto (\\sigma^2)^{-(\\alpha_0 + n/2 + 1/2 + 1)} \\exp\\left\\{ -\\frac{1}{\\sigma^2} \\left[ \\beta_0 + \\frac{1}{2}\\sum_{i=1}^{n}(y_i - \\theta)^2 + \\frac{\\kappa_0}{2}(\\theta - m_0)^2 \\right] \\right\\}$\n这是一个逆伽马分布的核，$p(x) \\propto x^{-(\\alpha+1)}\\exp(-\\beta/x)$。\n通过比较，形状参数为 $\\alpha_0 + n/2 + 1/2$，尺度参数为 $\\beta_0 + \\frac{1}{2}\\sum(y_i - \\theta)^2 + \\frac{\\kappa_0}{2}(\\theta - m_0)^2$。\n所以，$\\sigma^2$ 的全条件分布为：\n$\\sigma^2 \\mid \\theta, \\mathbf{y} \\sim \\operatorname{InvGamma}\\left(\\alpha_0 + \\frac{n+1}{2}, \\beta_0 + \\frac{1}{2}\\left[\\sum_{i=1}^{n}(y_i - \\theta)^2 + \\kappa_0(\\theta - m_0)^2\\right]\\right)$\n\n最后，为了证明共轭性并找到联合后验 $p(\\theta, \\sigma^2 \\mid \\mathbf{y})$ 的更新超参数，我们重新整理完整的后验表达式。我们将 $\\theta$ 的配方形式代回到指数中。令 $S = \\sum_{i=1}^{n}(y_i - \\bar{y})^2$。一个有用的恒等式是 $\\sum(y_i-\\theta)^2 = S + n(\\bar{y}-\\theta)^2$。\n涉及 $\\theta$ 的指数项是：\n$n(\\bar{y}-\\theta)^2 + \\kappa_0(m_0-\\theta)^2 = (n+\\kappa_0)\\theta^2 - 2(n\\bar{y}+\\kappa_0m_0)\\theta + n\\bar{y}^2+\\kappa_0m_0^2$。\n配方后，这等于：\n$(\\kappa_0+n)\\left(\\theta - \\frac{\\kappa_0m_0+n\\bar{y}}{\\kappa_0+n}\\right)^2 + \\frac{n\\kappa_0}{n+\\kappa_0}(\\bar{y}-m_0)^2$\n将此代回到完整的后验指数表达式中（方括号内的项）：\n$\\left[ S + (\\kappa_0+n)\\left(\\theta - \\frac{\\kappa_0m_0+n\\bar{y}}{\\kappa_0+n}\\right)^2 + \\frac{n\\kappa_0}{n+\\kappa_0}(\\bar{y}-m_0)^2 + 2\\beta_0 \\right]$\n联合后验变为：\n$p(\\theta, \\sigma^2 \\mid \\mathbf{y}) \\propto (\\sigma^2)^{-(\\alpha_0 + n/2 + 1/2 + 1)} \\exp\\left\\{ -\\frac{1}{2\\sigma^2} \\left[ (\\kappa_0+n)\\left(\\theta - \\frac{\\kappa_0m_0+n\\bar{y}}{\\kappa_0+n}\\right)^2 + 2\\beta_0 + S + \\frac{n\\kappa_0}{\\kappa_0+n}(\\bar{y}-m_0)^2 \\right] \\right\\}$\n该表达式具有正态-逆伽马分布的形式，$p(\\theta, \\sigma^2) \\propto (\\sigma^2)^{-(\\alpha_n+1/2+1)} \\exp\\left\\{ -\\frac{1}{2\\sigma^2} \\left[ \\kappa_n(\\theta-m_n)^2 + 2\\beta_n \\right] \\right\\}$。\n\n通过将推导出的后验与通用的 NIG 形式进行比较，我们确定更新后的超参数 $(m_n, \\kappa_n, \\alpha_n, \\beta_n)$：\n1.  $\\kappa_n$：关于 $\\theta$ 的平方项的系数（除以 $2\\sigma^2$）是 $\\kappa_n/(2\\sigma^2)$。所以，\n    $\\kappa_n = \\kappa_0 + n$\n2.  $m_n$：正态分量的均值是：\n    $m_n = \\frac{\\kappa_0m_0 + n\\bar{y}}{\\kappa_0 + n}$\n3.  $\\alpha_n$：$\\sigma^2$ 的幂是 $-(\\alpha_n+1/2+1)$。所以，$\\alpha_n+1/2+1 = \\alpha_0+n/2+1/2+1$。这给出：\n    $\\alpha_n = \\alpha_0 + \\frac{n}{2}$\n4.  $\\beta_n$：指数中剩余的项是 $2\\beta_n$。所以，$2\\beta_n = 2\\beta_0 + S + \\frac{n\\kappa_0}{\\kappa_0+n}(\\bar{y}-m_0)^2$。这给出：\n    $\\beta_n = \\beta_0 + \\frac{1}{2}S + \\frac{n\\kappa_0}{2(\\kappa_0+n)}(m_0 - \\bar{y})^2 = \\beta_0 + \\frac{1}{2} \\sum_{i=1}^{n} (y_i - \\bar{y})^2 + \\frac{n\\kappa_0}{2(\\kappa_0+n)}(m_0 - \\bar{y})^2$\n\n因此，后验分布 $p(\\theta, \\sigma^2 \\mid \\mathbf{y})$ 是 $NIG(m_n, \\kappa_n, \\alpha_n, \\beta_n)$，其超参数如上推导。\n\n更新后的超参数为：\n$m_n = \\frac{\\kappa_0 m_0 + n\\bar{y}}{\\kappa_0 + n}$\n$\\kappa_n = \\kappa_0 + n$\n$\\alpha_n = \\alpha_0 + \\frac{n}{2}$\n$\\beta_n = \\beta_0 + \\frac{1}{2} \\sum_{i=1}^{n} (y_i - \\bar{y})^2 + \\frac{n\\kappa_0}{2(\\kappa_0+n)}(m_0 - \\bar{y})^2$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{\\kappa_0 m_0 + n\\bar{y}}{\\kappa_0 + n} & \\kappa_0 + n & \\alpha_0 + \\frac{n}{2} & \\beta_0 + \\frac{1}{2} \\sum_{i=1}^{n} (y_i - \\bar{y})^2 + \\frac{n\\kappa_0}{2(\\kappa_0+n)}(m_0 - \\bar{y})^2 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "现实世界的数据常常包含离群点或表现出比高斯分布更重的尾部特征。本练习将引入学生t分布作为一种更稳健的误差模型来应对这种情况。此练习的核心任务是推导对数似然函数的梯度，这是一项至关重要的技能，它将统计模型与求解复杂逆问题所需的强大计算方法（如梯度优化或梯度依赖的采样算法）联系起来。",
            "id": "3770744",
            "problem": "考虑一个多尺度建模中的逆不确定性量化问题。一个可微的多尺度正向模型将参数向量 $\\,\\theta \\in \\mathbb{R}^{p}\\,$ 映射到预测的可观测量 $\\,f(\\theta) \\in \\mathbb{R}^{n}\\,$，其中 $\\,f(\\theta)\\,$ 将微观尺度的响应聚合成宏观尺度的量。你观测到 $\\,y \\in \\mathbb{R}^{n}\\,$，并假设存在加性的、独立的、重尾的差异，这些差异由自由度为 $\\,\\nu > 0\\,$、尺度为 $\\,\\sigma > 0\\,$ 的单变量学生t分布（Student-t distribution）建模。因此，对于残差 $\\,r(\\theta) = y - f(\\theta)\\,$，你假设对 $\\,i=1,\\dots,n\\,$，$\\,r_{i}(\\theta) \\sim \\text{Student-}t(\\nu,\\sigma)\\,$ 且相互独立。使用单变量学生t分布的标准概率密度函数，\n$$\np(r \\mid \\nu,\\sigma) \\;=\\; \\frac{\\Gamma\\!\\left(\\frac{\\nu+1}{2}\\right)}{\\Gamma\\!\\left(\\frac{\\nu}{2}\\right)\\,\\sqrt{\\nu\\pi}\\,\\sigma}\\,\\left(1 + \\frac{r^{2}}{\\nu\\,\\sigma^{2}}\\right)^{-\\frac{\\nu+1}{2}},\n$$\n以及残差分量 $\\,\\{r_{i}(\\theta)\\}_{i=1}^{n}\\,$ 的独立性，来构建给定 $\\,y\\,$、$\\,\\nu\\,$ 和 $\\,\\sigma\\,$ 时 $\\,\\theta\\,$ 的似然函数。然后，从链式法则和雅可比矩阵 $\\,J(\\theta) = \\partial f(\\theta)/\\partial \\theta \\in \\mathbb{R}^{n \\times p}\\,$ （其行为 $\\,J_{i,:}(\\theta) = \\partial f_{i}(\\theta)/\\partial \\theta^{\\top}\\,$）的定义出发，推导对数似然函数关于 $\\,\\theta\\,$ 的梯度的封闭形式表达式，该表达式仅用 $\\,r(\\theta)\\,$、$\\,J(\\theta)\\,$、$\\,\\nu\\,$ 和 $\\,\\sigma\\,$ 来表示。你的推导必须从给定的学生t密度和独立性假设开始；在对对数似然函数求导时，你可以忽略不依赖于 $\\,\\theta\\,$ 的加性常数。\n\n提供：\n- 似然函数 $\\,L(\\theta;\\nu,\\sigma \\mid y)\\,$，表示为 $\\,i=1,\\dots,n\\,$ 的乘积形式。\n- 梯度 $\\,\\nabla_{\\theta}\\ln L(\\theta;\\nu,\\sigma \\mid y)\\,$，表示为显式解析表达式。\n\n将你的最终答案表示为封闭形式的解析表达式。不要包含任何数值计算或单位。",
            "solution": "该问题是有效的，因为它在统计推断方面有科学依据，问题设定良好、客观，并为逆不确定性量化背景下的标准推导提供了一个完整且一致的框架。\n\n目标是推导似然函数 $L(\\theta;\\nu,\\sigma \\mid y)$ 及其对数的梯度 $\\nabla_{\\theta}\\ln L(\\theta;\\nu,\\sigma \\mid y)$，其中梯度是关于参数向量 $\\theta$ 的。\n\n**第一部分：似然函数的推导**\n\n似然函数 $L(\\theta;\\nu,\\sigma \\mid y)$ 是在给定参数 $\\theta$、$\\nu$ 和 $\\sigma$ 的条件下观测到数据 $y$ 的概率。我们已知数据模型为 $y = f(\\theta) + \\epsilon$，其中误差分量 $\\epsilon_i$ 根据学生t分布独立同分布。这等价于将残差 $r_i(\\theta) = y_i - f_i(\\theta)$ 建模为从该分布中的独立抽样。\n\n问题陈述，对于 $i=1,\\dots,n$，每个残差分量 $r_i(\\theta)$ 均独立地从一个单变量学生t分布 $r_{i}(\\theta) \\sim \\text{Student-}t(\\nu,\\sigma)$ 中抽取，其概率密度函数（PDF）由下式给出：\n$$\np(r \\mid \\nu,\\sigma) = \\frac{\\Gamma\\left(\\frac{\\nu+1}{2}\\right)}{\\Gamma\\left(\\frac{\\nu}{2}\\right)\\sqrt{\\nu\\pi}\\sigma}\\left(1 + \\frac{r^{2}}{\\nu\\,\\sigma^{2}}\\right)^{-\\frac{\\nu+1}{2}}\n$$\n由于残差分量 $\\{r_{i}(\\theta)\\}_{i=1}^{n}$ 的独立性，残差向量 $r(\\theta)$ 的联合概率密度是各分量密度的乘积：\n$$\np(r(\\theta) \\mid \\nu,\\sigma) = \\prod_{i=1}^{n} p(r_i(\\theta) \\mid \\nu, \\sigma)\n$$\n$\\theta$ 的似然函数定义为在观测残差处评估的这个联合概率：\n$$\nL(\\theta;\\nu,\\sigma \\mid y) = p(r(\\theta) \\mid \\nu,\\sigma)\n$$\n将每个分量 $r_i(\\theta)$ 的给定PDF代入，我们得到似然函数：\n$$\nL(\\theta;\\nu,\\sigma \\mid y) = \\prod_{i=1}^{n} \\left[ \\frac{\\Gamma\\left(\\frac{\\nu+1}{2}\\right)}{\\Gamma\\left(\\frac{\\nu}{2}\\right)\\sqrt{\\nu\\pi}\\sigma} \\left(1 + \\frac{r_i(\\theta)^2}{\\nu\\sigma^2}\\right)^{-\\frac{\\nu+1}{2}} \\right]\n$$\n不依赖于索引 $i$ 的项可以从乘积中提出，从而得到似然函数的最终表达式：\n$$\nL(\\theta;\\nu,\\sigma \\mid y) = \\left( \\frac{\\Gamma\\left(\\frac{\\nu+1}{2}\\right)}{\\Gamma\\left(\\frac{\\nu}{2}\\right)\\sqrt{\\nu\\pi}\\sigma} \\right)^n \\prod_{i=1}^{n} \\left(1 + \\frac{r_i(\\theta)^2}{\\nu\\sigma^2}\\right)^{-\\frac{\\nu+1}{2}}\n$$\n\n**第二部分：对数似然函数梯度的推导**\n\n为了求梯度，我们首先通过取似然函数的自然对数来计算对数似然函数：\n$$\n\\ln L(\\theta;\\nu,\\sigma \\mid y) = \\ln \\left[ \\left( \\frac{\\Gamma\\left(\\frac{\\nu+1}{2}\\right)}{\\Gamma\\left(\\frac{\\nu}{2}\\right)\\sqrt{\\nu\\pi}\\sigma} \\right)^n \\prod_{i=1}^{n} \\left(1 + \\frac{r_i(\\theta)^2}{\\nu\\sigma^2}\\right)^{-\\frac{\\nu+1}{2}} \\right]\n$$\n利用对数的性质 $\\ln(AB) = \\ln A + \\ln B$ 和 $\\ln(A^k) = k \\ln A$，我们得到：\n$$\n\\ln L(\\theta;\\nu,\\sigma \\mid y) = n \\left( \\ln\\Gamma\\left(\\frac{\\nu+1}{2}\\right) - \\ln\\Gamma\\left(\\frac{\\nu}{2}\\right) - \\frac{1}{2}\\ln(\\nu\\pi) - \\ln\\sigma \\right) - \\frac{\\nu+1}{2} \\sum_{i=1}^{n} \\ln\\left(1 + \\frac{r_i(\\theta)^2}{\\nu\\sigma^2}\\right)\n$$\n我们需要求此表达式关于 $\\theta \\in \\mathbb{R}^p$ 的梯度。第一项相对于 $\\theta$ 是一个常数，因此其梯度为零。我们对第二项进行微分：\n$$\n\\nabla_{\\theta}\\ln L(\\theta;\\nu,\\sigma \\mid y) = \\nabla_{\\theta} \\left[ - \\frac{\\nu+1}{2} \\sum_{i=1}^{n} \\ln\\left(1 + \\frac{r_i(\\theta)^2}{\\nu\\sigma^2}\\right) \\right]\n$$\n$$\n\\nabla_{\\theta}\\ln L(\\theta;\\nu,\\sigma \\mid y) = - \\frac{\\nu+1}{2} \\sum_{i=1}^{n} \\nabla_{\\theta} \\left[ \\ln\\left(1 + \\frac{r_i(\\theta)^2}{\\nu\\sigma^2}\\right) \\right]\n$$\n我们对求和中的每一项应用链式法则。令 $u_i(\\theta) = 1 + \\frac{r_i(\\theta)^2}{\\nu\\sigma^2}$。$\\ln(u_i(\\theta))$ 的梯度是 $\\frac{1}{u_i(\\theta)}\\nabla_{\\theta}u_i(\\theta)$。\n$$\n\\nabla_{\\theta}u_i(\\theta) = \\nabla_{\\theta}\\left(1 + \\frac{r_i(\\theta)^2}{\\nu\\sigma^2}\\right) = \\frac{1}{\\nu\\sigma^2}\\nabla_{\\theta}(r_i(\\theta)^2)\n$$\n再次使用链式法则，$\\nabla_{\\theta}(r_i(\\theta)^2) = 2r_i(\\theta)\\nabla_{\\theta}r_i(\\theta)$。残差为 $r_i(\\theta) = y_i - f_i(\\theta)$。由于 $y_i$ 是常数，其导数为零：\n$$\n\\nabla_{\\theta}r_i(\\theta) = \\nabla_{\\theta}(y_i - f_i(\\theta)) = -\\nabla_{\\theta}f_i(\\theta)\n$$\n正向模型第 $i$ 个分量的梯度 $\\nabla_{\\theta}f_i(\\theta)$ 是一个列向量，其转置是雅可比矩阵的第 $i$ 行 $J_{i,:}(\\theta) = \\partial f_i(\\theta)/\\partial\\theta^{\\top}$。因此，$\\nabla_{\\theta}f_i(\\theta) = J_{i,:}(\\theta)^T$。\n将其代回得到：\n$$\n\\nabla_{\\theta}u_i(\\theta) = \\frac{2r_i(\\theta)}{\\nu\\sigma^2} (-\\nabla_{\\theta}f_i(\\theta)) = -\\frac{2r_i(\\theta)}{\\nu\\sigma^2} J_{i,:}(\\theta)^T\n$$\n现在我们组合对数项的梯度：\n$$\n\\nabla_{\\theta} \\ln\\left(u_i(\\theta)\\right) = \\frac{1}{1 + \\frac{r_i(\\theta)^2}{\\nu\\sigma^2}} \\left( -\\frac{2r_i(\\theta)}{\\nu\\sigma^2} J_{i,:}(\\theta)^T \\right) = \\frac{-\\frac{2r_i(\\theta)}{\\nu\\sigma^2} J_{i,:}(\\theta)^T}{\\frac{\\nu\\sigma^2 + r_i(\\theta)^2}{\\nu\\sigma^2}} = -\\frac{2r_i(\\theta)}{\\nu\\sigma^2 + r_i(\\theta)^2}J_{i,:}(\\theta)^T\n$$\n最后，我们将其代回对数似然梯度的表达式中：\n$$\n\\nabla_{\\theta}\\ln L(\\theta;\\nu,\\sigma \\mid y) = - \\frac{\\nu+1}{2} \\sum_{i=1}^{n} \\left[ -\\frac{2r_i(\\theta)}{\\nu\\sigma^2 + r_i(\\theta)^2}J_{i,:}(\\theta)^T \\right]\n$$\n简化表达式得到最终结果：\n$$\n\\nabla_{\\theta}\\ln L(\\theta;\\nu,\\sigma \\mid y) = (\\nu+1) \\sum_{i=1}^{n} \\frac{r_i(\\theta)}{\\nu\\sigma^2 + r_i(\\theta)^2} J_{i,:}(\\theta)^T\n$$\n这就是梯度的封闭形式表达式，用指定的量表示。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\left( \\frac{\\Gamma\\left(\\frac{\\nu+1}{2}\\right)}{\\Gamma\\left(\\frac{\\nu}{2}\\right)\\sqrt{\\nu\\pi}\\sigma} \\right)^n \\prod_{i=1}^{n} \\left(1 + \\frac{r_i(\\theta)^2}{\\nu\\sigma^2}\\right)^{-\\frac{\\nu+1}{2}} & (\\nu+1) \\sum_{i=1}^{n} \\frac{r_i(\\theta)}{\\nu\\sigma^2 + r_i(\\theta)^2} J_{i,:}(\\theta)^T \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "这项练习旨在通过一个发人深省的思想实验来加深对逆问题本质的理解。它构建了一个正向不确定性很小，但逆问题却是不适定（ill-posed）的情景，揭示了逆向建模中的一个常见陷阱。通过分析模型雅可比矩阵的结构，这项实践解释了参数不可辨识性是如何产生的，并强调了良好表现的正向模型并不保证逆问题的适定性。",
            "id": "3770709",
            "problem": "考虑一个双尺度导电复合材料，它由两个面积分数相等的平行层组成。在微观尺度上，各层的电导率由参数向量 $\\theta = (\\theta_1,\\theta_2)^{\\top}$ 表示，我们感兴趣的宏观有效性质是均质化面内有效电导率，其模型为前向映射 $G:\\mathbb{R}^2 \\to \\mathbb{R}$，由下式给出\n$$\nG(\\theta) = \\tfrac{1}{2}\\theta_1 + \\tfrac{1}{2}\\theta_2.\n$$\n假设 $\\theta$ 的概率先验形式为 $\\theta \\sim \\mathcal{N}(m, C)$，其中 $m \\in \\mathbb{R}^2$，协方差矩阵为\n$$\nC = \\begin{pmatrix}\ns^2 & -\\rho s^2 \\\\\n-\\rho s^2 & s^2\n\\end{pmatrix},\n$$\n其中 $s>0$ 和 $0 < \\rho < 1$ 表示强负相关性。数据模型是加性高斯噪声模型 $y = G(\\theta) + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0,\\sigma_y^2)$ 且噪声方差 $\\sigma_y^2 > 0$ 已知。将所有量视为无量纲。\n\n在这个最小化的多尺度设置中，您将研究前向不确定性量化（前向 UQ）和反向不确定性量化（反向 IUQ）之间的相互作用。从第一性原理出发：\n- 在给定的先验和噪声模型下，推导前向输出方差 $\\operatorname{Var}(y)$，并解释为什么当 $\\rho \\to 1$ 时它会变小。\n- 通过检验负对数似然函数海森矩阵的高斯-牛顿近似来分析反问题的局部可辨识性。具体来说，计算 $G$ 在任意 $\\theta^{\\star} \\in \\mathbb{R}^2$ 处的雅可比矩阵 $J(\\theta^{\\star})$，构建高斯-牛顿矩阵 $H = J(\\theta^{\\star})^{\\top}\\Sigma_y^{-1}J(\\theta^{\\star)}$，其中 $\\Sigma_y = \\sigma_y^2$，并确定其行列式。用此来证明为什么尽管前向输出不确定性很窄，反问题仍然是不适定的。\n\n请给出 $\\det(H)$ 的精确值作为最终答案，不要四舍五入。说明您使用的任何附加假设，并确保每个数学符号都使用 $...$ 以 LaTeX 格式书写。",
            "solution": "首先根据指定标准对问题进行验证。\n\n**第1步：提取已知条件**\n- 前向映射：$G(\\theta) = \\tfrac{1}{2}\\theta_1 + \\tfrac{1}{2}\\theta_2$ 对于 $\\theta = (\\theta_1, \\theta_2)^{\\top}$。\n- 先验分布：$\\theta \\sim \\mathcal{N}(m, C)$，其中 $m \\in \\mathbb{R}^2$。\n- 先验协方差：$C = \\begin{pmatrix} s^2 & -\\rho s^2 \\\\ -\\rho s^2 & s^2 \\end{pmatrix}$，其中 $s>0$ 且 $0 < \\rho < 1$。\n- 数据模型：$y = G(\\theta) + \\varepsilon$。\n- 噪声模型：$\\varepsilon \\sim \\mathcal{N}(0, \\sigma_y^2)$，其中 $\\sigma_y^2 > 0$ 已知。\n- 所有量都是无量纲的。\n- 任务1：推导 $\\operatorname{Var}(y)$ 并分析其在 $\\rho \\to 1$ 时的行为。\n- 任务2：计算 $G$ 的雅可比矩阵 $J(\\theta^{\\star})$，构建高斯-牛顿矩阵 $H = J(\\theta^{\\star})^{\\top}\\Sigma_y^{-1}J(\\theta^{\\star)}$，其中 $\\Sigma_y = \\sigma_y^2$，计算其行列式 $\\det(H)$，并用此分析反问题的不适定性。\n\n**第2步：使用提取的已知条件进行验证**\n- **科学依据：** 该问题具有科学依据。它为一个简单的线性前向模型提出了一个典型的贝叶斯反问题。模型 $G(\\theta) = \\frac{1}{2}(\\theta_1 + \\theta_2)$ 是层状复合材料面内有效电导率的混合律，这是均质化理论中的一个标准结果。使用高斯先验和加性高斯噪声是不确定性量化中一个常用且数学上合理的框架。\n- **适定性：** 问题陈述是适定的。它提供了执行所要求的推导和计算所需的所有必要定义和约束。所提问题具有唯一、可验证的答案。\n- **客观性：** 问题使用精确、客观的数学语言陈述，没有歧义或主观论断。\n\n**第3步：结论与行动**\n- **结论：** 该问题科学上合理、自洽且适定。这是一个有效的问题。\n- **行动：** 继续求解。\n\n**第1部分：前向不确定性量化**\n\n目标是推导可观测输出的方差 $\\operatorname{Var}(y)$。数据模型由 $y = G(\\theta) + \\varepsilon$ 给出。参数 $\\theta$ 和噪声 $\\varepsilon$ 被假定为独立的随机变量。因此，它们和的方差是它们方差的和：\n$$\n\\operatorname{Var}(y) = \\operatorname{Var}(G(\\theta)) + \\operatorname{Var}(\\varepsilon)\n$$\n我们已知 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma_y^2)$，这直接意味着 $\\operatorname{Var}(\\varepsilon) = \\sigma_y^2$。\n\n接下来，我们计算前向模型输出的方差 $\\operatorname{Var}(G(\\theta))$。前向模型是参数向量 $\\theta$ 的一个线性函数：\n$$\nG(\\theta) = \\tfrac{1}{2}\\theta_1 + \\tfrac{1}{2}\\theta_2 = \\begin{pmatrix} \\tfrac{1}{2} & \\tfrac{1}{2} \\end{pmatrix} \\begin{pmatrix} \\theta_1 \\\\ \\theta_2 \\end{pmatrix} = k^{\\top}\\theta\n$$\n其中 $k = (\\tfrac{1}{2}, \\tfrac{1}{2})^{\\top}$。随机向量线性变换的方差由公式 $\\operatorname{Var}(k^{\\top}\\theta) = k^{\\top} \\operatorname{Cov}(\\theta) k$ 给出。在本例中，$\\operatorname{Cov}(\\theta) = C$。\n代入 $k$ 和 $C$ 的表达式：\n$$\n\\operatorname{Var}(G(\\theta)) = \\begin{pmatrix} \\tfrac{1}{2} & \\tfrac{1}{2} \\end{pmatrix} \\begin{pmatrix} s^2 & -\\rho s^2 \\\\ -\\rho s^2 & s^2 \\end{pmatrix} \\begin{pmatrix} \\tfrac{1}{2} \\\\ \\tfrac{1}{2} \\end{pmatrix}\n$$\n首先，我们进行左乘：\n$$\n\\begin{pmatrix} \\tfrac{1}{2} & \\tfrac{1}{2} \\end{pmatrix} \\begin{pmatrix} s^2 & -\\rho s^2 \\\\ -\\rho s^2 & s^2 \\end{pmatrix} = \\begin{pmatrix} \\tfrac{1}{2}s^2 - \\tfrac{1}{2}\\rho s^2 & -\\tfrac{1}{2}\\rho s^2 + \\tfrac{1}{2}s^2 \\end{pmatrix} = \\begin{pmatrix} \\tfrac{s^2}{2}(1-\\rho) & \\tfrac{s^2}{2}(1-\\rho) \\end{pmatrix}\n$$\n现在，我们完成二次型：\n$$\n\\operatorname{Var}(G(\\theta)) = \\begin{pmatrix} \\tfrac{s^2}{2}(1-\\rho) & \\tfrac{s^2}{2}(1-\\rho) \\end{pmatrix} \\begin{pmatrix} \\tfrac{1}{2} \\\\ \\tfrac{1}{2} \\end{pmatrix} = \\tfrac{s^2}{2}(1-\\rho)(\\tfrac{1}{2}) + \\tfrac{s^2}{2}(1-\\rho)(\\tfrac{1}{2}) = \\tfrac{s^2}{4}(1-\\rho) + \\tfrac{s^2}{4}(1-\\rho) = \\tfrac{s^2}{2}(1-\\rho)\n$$\n结合结果，总输出方差为：\n$$\n\\operatorname{Var}(y) = \\tfrac{s^2}{2}(1-\\rho) + \\sigma_y^2\n$$\n当负相关强度 $\\rho$ 趋近其上限 $1$ 时，项 $(1-\\rho)$ 趋近于 $0$。因此，\n$$\n\\lim_{\\rho \\to 1} \\operatorname{Var}(y) = \\lim_{\\rho \\to 1} \\left( \\tfrac{s^2}{2}(1-\\rho) + \\sigma_y^2 \\right) = 0 + \\sigma_y^2 = \\sigma_y^2\n$$\n这表明，当先验相关性变为完全负相关（$\\rho \\to 1$）时，由参数贡献的模型输出不确定性 $\\operatorname{Var}(G(\\theta))$ 消失。总输出不确定性完全由测量噪声方差 $\\sigma_y^2$ 主导。这是因为如果 $\\theta_1$ 和 $\\theta_2$ 强负相关，$\\theta_1$ 高于其均值的偏差会伴随着 $\\theta_2$ 低于其均值的偏差，使得它们的和（以及它们的平均值 $G(\\theta)$）保持相对恒定。这导致前向模型输出的分布非常窄。\n\n**第2部分：反问题分析与不适定性**\n\n我们通过检验负对数似然函数海森矩阵的高斯-牛顿近似来分析反问题的局部可辨识性。该矩阵由 $H = J(\\theta^{\\star})^{\\top}\\Sigma_y^{-1}J(\\theta^{\\star})$ 给出。该矩阵的逆与参数的后验协方差有关，其奇异性表示不可辨识性。\n\n首先，我们计算前向映射 $G(\\theta) = \\tfrac{1}{2}\\theta_1 + \\tfrac{1}{2}\\theta_2$ 的雅可比矩阵。雅可比矩阵是一阶偏导数矩阵，在本例中是一个 $1 \\times 2$ 的行向量：\n$$\nJ(\\theta) = \\begin{pmatrix} \\frac{\\partial G}{\\partial \\theta_1} & \\frac{\\partial G}{\\partial \\theta_2} \\end{pmatrix} = \\begin{pmatrix} \\tfrac{1}{2} & \\tfrac{1}{2} \\end{pmatrix}\n$$\n雅可比矩阵是常数，不依赖于其求值点 $\\theta^{\\star}$。因此，$J(\\theta^{\\star}) = \\begin{pmatrix} \\tfrac{1}{2} & \\tfrac{1}{2} \\end{pmatrix}$。\n\n输出 $y$ 是一个标量，所以其协方差矩阵 $\\Sigma_y$ 是一个由 $\\Sigma_y = [\\sigma_y^2]$ 给出的 $1 \\times 1$ 矩阵。其逆是 $\\Sigma_y^{-1} = [\\tfrac{1}{\\sigma_y^2}]$。\n\n现在，我们构建高斯-牛顿矩阵 $H$。雅可比矩阵的转置是 $J(\\theta^{\\star})^{\\top} = \\begin{pmatrix} \\tfrac{1}{2} \\\\ \\tfrac{1}{2} \\end{pmatrix}$。\n$$\nH = J(\\theta^{\\star})^{\\top} \\Sigma_y^{-1} J(\\theta^{\\star}) = \\begin{pmatrix} \\tfrac{1}{2} \\\\ \\tfrac{1}{2} \\end{pmatrix} \\left( \\tfrac{1}{\\sigma_y^2} \\right) \\begin{pmatrix} \\tfrac{1}{2} & \\tfrac{1}{2} \\end{pmatrix}\n$$\n执行矩阵乘法：\n$$\nH = \\frac{1}{\\sigma_y^2} \\begin{pmatrix} \\tfrac{1}{2} \\\\ \\tfrac{1}{2} \\end{pmatrix} \\begin{pmatrix} \\tfrac{1}{2} & \\tfrac{1}{2} \\end{pmatrix} = \\frac{1}{\\sigma_y^2} \\begin{pmatrix} (\\tfrac{1}{2})(\\tfrac{1}{2}) & (\\tfrac{1}{2})(\\tfrac{1}{2}) \\\\ (\\tfrac{1}{2})(\\tfrac{1}{2}) & (\\tfrac{1}{2})(\\tfrac{1}{2}) \\end{pmatrix} = \\frac{1}{\\sigma_y^2} \\begin{pmatrix} \\tfrac{1}{4} & \\tfrac{1}{4} \\\\ \\tfrac{1}{4} & \\tfrac{1}{4} \\end{pmatrix}\n$$\n最后一步是计算这个 $2 \\times 2$ 矩阵 $H$ 的行列式：\n$$\n\\det(H) = \\det\\left( \\frac{1}{\\sigma_y^2} \\begin{pmatrix} \\tfrac{1}{4} & \\tfrac{1}{4} \\\\ \\tfrac{1}{4} & \\tfrac{1}{4} \\end{pmatrix} \\right)\n$$\n对于一个 $n \\times n$ 矩阵 $A$，使用行列式性质 $\\det(cA) = c^n\\det(A)$，其中 $n=2$ 且 $c=1/\\sigma_y^2$：\n$$\n\\det(H) = \\left( \\frac{1}{\\sigma_y^2} \\right)^2 \\det\\begin{pmatrix} \\tfrac{1}{4} & \\tfrac{1}{4} \\\\ \\tfrac{1}{4} & \\tfrac{1}{4} \\end{pmatrix} = \\frac{1}{\\sigma_y^4} \\left( (\\tfrac{1}{4})(\\tfrac{1}{4}) - (\\tfrac{1}{4})(\\tfrac{1}{4}) \\right) = \\frac{1}{\\sigma_y^4} \\left( \\tfrac{1}{16} - \\tfrac{1}{16} \\right) = 0\n$$\n行列式为零意味着矩阵 $H$ 是奇异的。一个奇异的高斯-牛顿海森矩阵表明，似然函数在参数空间中至少沿一个方向是平坦的。这意味着沿这个方向改变参数对模型输出 $G(\\theta)$ 没有影响。因此，数据 $y$ 无法提供信息来约束该方向上的参数。这是不适定反问题的一个标志，因为有无限多的参数组合会产生相同的模型预测，从而得到相同的似然值。在本例中，$H$ 的零空间由向量 $(1, -1)^{\\top}$ 张成，这意味着在直线 $\\theta_1 + \\theta_2 = \\text{常数}$ 上的任何参数都是不可区分的。\n\n这个分析揭示了一个关键的见解：尽管当前向不确定性 $\\operatorname{Var}(y)$ 随着 $\\rho \\to 1$ 变小时，表明输出是良好确定的，但由于前向模型 $G(\\theta)$ 的结构，反问题从根本上就是不适定的。测量值 $y$ 只提供了关于和 $\\theta_1 + \\theta_2$ 的信息，而没有提供关于 $\\theta_1$ 和 $\\theta_2$ 各自值的信息。",
            "answer": "$$\\boxed{0}$$"
        }
    ]
}