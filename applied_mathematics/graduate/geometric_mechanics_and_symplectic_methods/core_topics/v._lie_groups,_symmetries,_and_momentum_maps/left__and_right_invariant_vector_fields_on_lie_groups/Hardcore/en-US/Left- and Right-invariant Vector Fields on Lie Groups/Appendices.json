{
    "hands_on_practices": [
        {
            "introduction": "The formal definition of a left-invariant vector field, $X_A(g) = (L_g)_* A$, connects a point $g$ on a Lie group to the Lie algebra $\\mathfrak{g}$ via the differential of left translation. This exercise grounds this abstract definition in the concrete setting of the general linear group $\\mathrm{GL}(n, \\mathbb{R})$, a foundational example of a matrix Lie group. By applying first principles, you will derive the explicit and widely used formula for such a vector field, making the concept far more tangible and computationally accessible.",
            "id": "3752777",
            "problem": "Let $G$ be the General Linear group $\\mathrm{GL}(n,\\mathbb{R})$, viewed as a Lie group and as an open submanifold of the vector space of real $n\\times n$ matrices. Let $L_{g}:G\\to G$ denote left translation by $g\\in G$, defined by $L_{g}(h)=gh$, and let $(L_{g})_{*}:T_{e}G\\to T_{g}G$ denote its differential at the identity element $e$. Recall that the Lie algebra $\\mathfrak{gl}(n,\\mathbb{R})$ is canonically identified with the tangent space $T_{e}G$, and that a left-invariant vector field $X_{A}$ associated to an element $A\\in\\mathfrak{gl}(n,\\mathbb{R})$ is defined by $X_{A}(g)=(L_{g})_{*}A$ for each $g\\in G$. Using only these foundational definitions and the fact that $G$ is embedded as an open subset of the space of real matrices, derive an explicit expression for $X_{A}(g)$ as a tangent vector at $g$ in terms of $A$ and $g$. Express your final answer as a single analytic expression for the tangent vector at $g$ in the ambient matrix space. No intermediate formulas are provided; begin from the definitions of left translation, its differential, and the identification of tangent vectors via smooth curves.\n\nYour final answer must be a single closed-form analytic expression and contain no inequalities or equations. If you introduce a curve, ensure it is a smooth path in $G$ through $g$ whose derivative at $0$ represents the tangent vector. No rounding is required.",
            "solution": "The problem asks for an explicit expression for a left-invariant vector field $X_{A}$ on the Lie group $G = \\mathrm{GL}(n,\\mathbb{R})$ at an element $g \\in G$. The vector field is generated by an element $A$ of the Lie algebra $\\mathfrak{gl}(n,\\mathbb{R})$. The derivation must be based on first principles, using the provided definitions.\n\nFirst, we establish the setting. The general linear group $G = \\mathrm{GL}(n,\\mathbb{R})$ is the set of all invertible $n \\times n$ real matrices. It is an open submanifold of the vector space $M(n,\\mathbb{R})$ of all $n \\times n$ real matrices, since the determinant function $\\det: M(n,\\mathbb{R}) \\to \\mathbb{R}$ is continuous, and $G = \\det^{-1}(\\mathbb{R} \\setminus \\{0\\})$, the pre-image of an open set. Because $G$ is an open subset of a Euclidean space (namely $M(n,\\mathbb{R}) \\cong \\mathbb{R}^{n^2}$), the tangent space $T_pG$ at any point $p \\in G$ can be canonically identified with the vector space $M(n,\\mathbb{R})$ itself. The identity element of the group $G$ is the $n \\times n$ identity matrix, which we denote by $e$. The Lie algebra is the tangent space at the identity, $\\mathfrak{gl}(n,\\mathbb{R}) = T_eG$, which is thus identified with $M(n,\\mathbb{R})$.\n\nA tangent vector in $T_pG$ can be represented as the velocity vector of a smooth curve in $G$. That is, a vector $V \\in T_pG$ corresponds to the derivative $\\gamma'(0)$ of a smooth curve $\\gamma: (-\\epsilon, \\epsilon) \\to G$ such that $\\gamma(0) = p$ and $\\gamma'(0) = V$. The derivative is computed in the ambient space $M(n,\\mathbb{R})$.\n\nThe problem concerns a specific vector $A \\in \\mathfrak{gl}(n,\\mathbb{R}) = T_eG$. To work with this vector, we must first represent it by a suitable curve. Let $\\alpha(t)$ be a smooth curve in $G$ defined on an interval containing $0$ such that $\\alpha(0) = e$ and $\\alpha'(0) = A$. A straightforward choice for such a curve, leveraging the vector space structure of the ambient space, is $\\alpha(t) = e + tA$. Let's verify this curve:\n1.  At $t=0$, we have $\\alpha(0) = e + 0 \\cdot A = e$. The curve passes through the identity element at $t=0$.\n2.  The derivative with respect to $t$ is $\\alpha'(t) = \\frac{d}{dt}(e + tA) = A$. Thus, the velocity at $t=0$ is $\\alpha'(0) = A$.\nThis curve is guaranteed to be in $G = \\mathrm{GL}(n,\\mathbb{R})$ for $t$ in some open interval $(-\\epsilon, \\epsilon)$ around $0$, because $\\det(\\alpha(t)) = \\det(e+tA)$ is a polynomial in $t$, and since $\\det(\\alpha(0)) = \\det(e) = 1 \\neq 0$, continuity implies that $\\det(\\alpha(t)) \\neq 0$ for $t$ sufficiently close to $0$.\n\nThe left-invariant vector field $X_A$ at a point $g \\in G$ is defined by $X_A(g) = (L_g)_*A$. Here, $L_g: G \\to G$ is the left-translation map $L_g(h) = gh$, and $(L_g)_*: T_eG \\to T_gG$ is its differential (or pushforward) at the identity $e$.\n\nThe pushforward of a tangent vector is computed by applying the map to the curve representing the vector and then taking the derivative of the resulting curve. The tangent vector $A$ at $e$ is represented by the curve $\\alpha(t)$. The pushforward $(L_g)_*A$ is therefore the velocity vector at $t=0$ of the transformed curve, $\\beta(t) = L_g(\\alpha(t))$.\n\nLet's compute this new curve $\\beta(t)$:\n$$ \\beta(t) = L_g(\\alpha(t)) = g \\cdot \\alpha(t) $$\nSubstituting the expression for $\\alpha(t)$, we get:\n$$ \\beta(t) = g(e + tA) $$\nUsing the distributive property of matrix multiplication, we have:\n$$ \\beta(t) = ge + g(tA) = g + t(gA) $$\nThis new curve $\\beta(t)$ is a path in $G$ that starts at $\\beta(0) = g + 0 \\cdot (gA) = g$. The tangent vector $X_A(g)$ is the velocity of this curve at $t=0$, which is $\\beta'(0)$.\n\nWe now differentiate $\\beta(t)$ with respect to $t$. Since $g$ and $A$ are constant matrices, the product $gA$ is also a constant matrix with respect to $t$.\n$$ \\beta'(t) = \\frac{d}{dt}(g + t(gA)) = 0 + 1 \\cdot (gA) = gA $$\nThe derivative is constant for all $t$. To find the tangent vector at $g$, we evaluate this derivative at $t=0$:\n$$ X_A(g) = \\beta'(0) = gA $$\nThis result, $gA$, is an $n \\times n$ matrix, which is consistent with our identification of the tangent space $T_gG$ with the space of all $n \\times n$ matrices $M(n,\\mathbb{R})$. Thus, the explicit expression for the left-invariant vector field $X_A$ at the point $g$ is the matrix product of $g$ and $A$.",
            "answer": "$$\\boxed{gA}$$"
        },
        {
            "introduction": "One of the most powerful ideas in Lie theory is that elements of the Lie algebra act as infinitesimal generators of motion within the group. This practice brings this concept to life by asking you to find the integral curves of specific left-invariant vector fields on the special unitary group $SU(2)$, which is central to quantum mechanics and robotics. You will see firsthand how the matrix exponential maps elements of the Lie algebra $\\mathfrak{su}(2)$ to one-parameter subgroups, effectively tracing out paths on the group manifold.",
            "id": "3752766",
            "problem": "Let $G = SU(2)$ be the Special Unitary Group (SU(2)) of $2 \\times 2$ complex unitary matrices with determinant $1$, and let $\\mathfrak{g} = \\mathfrak{su}(2)$ be its Lie algebra of $2 \\times 2$ complex skew-Hermitian traceless matrices. Denote by $\\sigma_{1}$, $\\sigma_{2}$, and $\\sigma_{3}$ the Pauli matrices\n$$\n\\sigma_{1} = \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}, \\quad\n\\sigma_{2} = \\begin{pmatrix} 0 & -i \\\\ i & 0 \\end{pmatrix}, \\quad\n\\sigma_{3} = \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix}.\n$$\nFor each $k \\in \\{1,2,3\\}$, consider the Lie algebra element $X_{k} = i \\sigma_{k} \\in \\mathfrak{su}(2)$ and the corresponding left-invariant vector field $X_{k}^{L}$ on $SU(2)$ defined by the property of left invariance: for any $g \\in SU(2)$, the value of the vector field at $g$ is given by the differential of left translation by $g$ acting on $X_{k}$.\n\nStarting from first principles and core definitions of left-invariant vector fields on matrix Lie groups and the definition of the matrix exponential as the solution operator for linear systems, derive the explicit integral curve $g_{k}(t)$ of $X_{k}^{L}$ satisfying the initial condition $g_{k}(0) = g_{0}$ with $g_{0} \\in SU(2)$ fixed, for each $k \\in \\{1,2,3\\}$. Identify the associated one-parameter subgroup through the identity element for each $k$.\n\nYour final answer must be the triple of explicit $2 \\times 2$ matrix-valued functions of $t$ giving $g_{k}(t)$ for $k = 1, 2, 3$, arranged as a single row matrix. No numerical approximation is required; provide exact closed forms.",
            "solution": "An integral curve $g(t)$ of a vector field $V$ on a manifold is a smooth curve whose tangent vector at each point $g(t)$ is equal to the vector field evaluated at that point, i.e., $\\frac{d}{dt}g(t) = V_{g(t)}$. In this problem, we are given a matrix Lie group $G = SU(2)$, its Lie algebra $\\mathfrak{g} = \\mathfrak{su}(2)$, and a set of Lie algebra elements $X_k = i\\sigma_k \\in \\mathfrak{su}(2)$ for $k \\in \\{1, 2, 3\\}$. We need to find the integral curves $g_k(t)$ of the corresponding left-invariant vector fields $X_k^L$.\n\nA left-invariant vector field $X^L$ on a Lie group $G$, generated by a Lie algebra element $X \\in \\mathfrak{g} = T_e G$, is defined by the property $(X^L)_g = (L_g)_* X$ for any $g \\in G$. Here, $L_g: G \\to G$ is the left translation map $h \\mapsto gh$, and $(L_g)_*$ is its differential (pushforward) at the identity $e$. For a matrix Lie group, this relationship simplifies to matrix multiplication: $(X^L)_g = gX$.\n\nConsequently, the differential equation that defines the integral curve $g_k(t)$ for the vector field $X_k^L$ is:\n$$\n\\frac{d}{dt}g_k(t) = (X_k^L)_{g_k(t)} = g_k(t) X_k\n$$\nThis is a system of linear, first-order ordinary differential equations with constant coefficients (the matrix $X_k$ is constant). Given the initial condition $g_k(0) = g_0$, the unique solution is given by:\n$$\ng_k(t) = g_0 \\exp(tX_k)\n$$\nwhere $\\exp(\\cdot)$ denotes the matrix exponential. The one-parameter subgroup through the identity element associated with $X_k$ is the special case where $g_0 = I$ (the identity matrix), which is $\\gamma_k(t) = \\exp(tX_k)$. Our task is to compute this matrix exponential for each $X_k = i\\sigma_k$.\n\nLet's compute $\\exp(tX_k) = \\exp(it\\sigma_k)$. A crucial property of the Pauli matrices is that their square is the identity matrix $I$:\n$$\n\\sigma_1^2 = \\sigma_2^2 = \\sigma_3^2 = I\n$$\nWe use the Taylor series definition of the matrix exponential, $\\exp(A) = \\sum_{n=0}^{\\infty} \\frac{A^n}{n!}$. Let $A_k = it\\sigma_k$. Then:\n$$\nA_k^2 = (it\\sigma_k)^2 = (it)^2 \\sigma_k^2 = -t^2 I\n$$\nThe powers of $A_k$ follow a simple pattern:\n$A_k^{2n} = (A_k^2)^n = (-t^2 I)^n = (-1)^n t^{2n} I$\n$A_k^{2n+1} = A_k^{2n} A_k = (-1)^n t^{2n} (it\\sigma_k) = i (-1)^n t^{2n+1} \\sigma_k$\n\nSubstituting this into the Taylor series:\n$$\n\\exp(it\\sigma_k) = \\sum_{n=0}^{\\infty} \\frac{(it\\sigma_k)^{2n}}{(2n)!} + \\sum_{n=0}^{\\infty} \\frac{(it\\sigma_k)^{2n+1}}{(2n+1)!}\n$$\n$$\n\\exp(it\\sigma_k) = \\sum_{n=0}^{\\infty} \\frac{(-1)^n t^{2n}}{(2n)!} I + \\sum_{n=0}^{\\infty} \\frac{i (-1)^n t^{2n+1}}{(2n+1)!} \\sigma_k\n$$\nRecognizing the Taylor series for $\\cos(t)$ and $\\sin(t)$:\n$$\n\\exp(it\\sigma_k) = \\left( \\sum_{n=0}^{\\infty} \\frac{(-1)^n t^{2n}}{(2n)!} \\right) I + i \\left( \\sum_{n=0}^{\\infty} \\frac{(-1)^n t^{2n+1}}{(2n+1)!} \\right) \\sigma_k\n$$\n$$\n\\exp(it\\sigma_k) = \\cos(t)I + i\\sin(t)\\sigma_k\n$$\nThis is the general form of the one-parameter subgroup generated by $X_k=i\\sigma_k$. The integral curve is $g_k(t) = g_0 (\\cos(t)I + i\\sin(t)\\sigma_k)$. We now find the explicit matrix for each $k \\in \\{1, 2, 3\\}$.\n\nFor $k=1$:\nThe one-parameter subgroup is $\\gamma_1(t) = \\cos(t)I + i\\sin(t)\\sigma_1$:\n$$\n\\gamma_1(t) = \\cos(t)\\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} + i\\sin(t)\\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix} = \\begin{pmatrix} \\cos(t) & i\\sin(t) \\\\ i\\sin(t) & \\cos(t) \\end{pmatrix}\n$$\nThe integral curve is $g_1(t) = g_0 \\gamma_1(t) = g_0 \\begin{pmatrix} \\cos(t) & i\\sin(t) \\\\ i\\sin(t) & \\cos(t) \\end{pmatrix}$.\n\nFor $k=2$:\nThe one-parameter subgroup is $\\gamma_2(t) = \\cos(t)I + i\\sin(t)\\sigma_2$:\n$$\n\\gamma_2(t) = \\cos(t)\\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} + i\\sin(t)\\begin{pmatrix} 0 & -i \\\\ i & 0 \\end{pmatrix} = \\begin{pmatrix} \\cos(t) & \\sin(t) \\\\ -\\sin(t) & \\cos(t) \\end{pmatrix}\n$$\nThe integral curve is $g_2(t) = g_0 \\gamma_2(t) = g_0 \\begin{pmatrix} \\cos(t) & \\sin(t) \\\\ -\\sin(t) & \\cos(t) \\end{pmatrix}$. This corresponds to a rotation in $SO(2) \\subset SU(2)$.\n\nFor $k=3$:\nThe one-parameter subgroup is $\\gamma_3(t) = \\cos(t)I + i\\sin(t)\\sigma_3$:\n$$\n\\gamma_3(t) = \\cos(t)\\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} + i\\sin(t)\\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix} = \\begin{pmatrix} \\cos(t) + i\\sin(t) & 0 \\\\ 0 & \\cos(t) - i\\sin(t) \\end{pmatrix}\n$$\nUsing Euler's formula, $e^{i\\theta} = \\cos(\\theta) + i\\sin(\\theta)$, this simplifies to:\n$$\n\\gamma_3(t) = \\begin{pmatrix} e^{it} & 0 \\\\ 0 & e^{-it} \\end{pmatrix}\n$$\nThe integral curve is $g_3(t) = g_0 \\gamma_3(t) = g_0 \\begin{pmatrix} e^{it} & 0 \\\\ 0 & e^{-it} \\end{pmatrix}$. This could also be found by noting that $tX_3 = \\begin{pmatrix} it & 0 \\\\ 0 & -it \\end{pmatrix}$ is a diagonal matrix, and its exponential is the matrix of the exponential of its diagonal entries.\n\nThe triple of explicit $2 \\times 2$ matrix-valued functions for $g_k(t)$ constitutes the final answer.",
            "answer": "$$\n\\boxed{\n\\left(\ng_{0} \\begin{pmatrix} \\cos(t) & i\\sin(t) \\\\ i\\sin(t) & \\cos(t) \\end{pmatrix} ,\\;\ng_{0} \\begin{pmatrix} \\cos(t) & \\sin(t) \\\\ -\\sin(t) & \\cos(t) \\end{pmatrix} ,\\;\ng_{0} \\begin{pmatrix} e^{it} & 0 \\\\ 0 & e^{-it} \\end{pmatrix}\n\\right)\n}\n$$"
        },
        {
            "introduction": "The space of left-invariant vector fields on a Lie group $G$ is much more than just a vector space; it forms a Lie algebra itself, which is isomorphic to the tangent space algebra $\\mathfrak{g}$. This exercise solidifies this crucial structural insight by exploring the Lie bracket of two arbitrary left-invariant differential operators. By carrying out the calculation, you will explicitly demonstrate how the commutator is determined by the Lie algebra's structure constants, thus bridging the gap between the geometric definition of the bracket and its purely algebraic representation.",
            "id": "3752780",
            "problem": "Let $G$ be a finite-dimensional Lie group and let $\\mathfrak{g}$ denote its Lie algebra identified with the space of left-invariant vector fields on $G$. Fix a basis $\\{X_{1},\\dots,X_{n}\\}$ of left-invariant vector fields corresponding to a basis of $\\mathfrak{g}$, and define the structure constants $c_{ij}^{k}$ by the relations $[X_{i},X_{j}] = \\sum_{k=1}^{n} c_{ij}^{k} X_{k}$ for all $i,j \\in \\{1,\\dots,n\\}$. Consider the first-order left-invariant differential operators $D$ and $E$ on $C^{\\infty}(G)$ defined by\n$$\nD = \\sum_{i=1}^{n} a_{i} X_{i}, \\qquad E = \\sum_{j=1}^{n} b_{j} X_{j},\n$$\nwhere $a_{i}, b_{j} \\in \\mathbb{R}$ are constants. Using only the fundamental properties of Lie brackets of vector fields (bilinearity, antisymmetry, and the Leibniz property of vector fields as derivations) and the definition of the structure constants, compute the commutator differential operator $[D,E]$ and express it as a single linear combination of the basis $\\{X_{k}\\}$ with coefficients written explicitly in terms of $a_{i}$, $b_{j}$, and $c_{ij}^{k}$. Provide your final result as a single closed-form analytic expression in the basis $\\{X_{k}\\}$.",
            "solution": "The problem requires the computation of the commutator $[D,E]$ of two left-invariant first-order differential operators on a Lie group $G$. The operators are defined as linear combinations of a basis of left-invariant vector fields $\\{X_{1}, \\dots, X_{n}\\}$ corresponding to a basis of the Lie algebra $\\mathfrak{g}$:\n$$D = \\sum_{i=1}^{n} a_{i} X_{i}$$\n$$E = \\sum_{j=1}^{n} b_{j} X_{j}$$\nwhere $a_{i}, b_{j} \\in \\mathbb{R}$ are constant coefficients.\n\nThe commutator of two differential operators, which for vector fields corresponds to the Lie bracket, is the central object of study. The prompt instructs us to use the fundamental properties of the Lie bracket, specifically its bilinearity. The Lie bracket $[ \\cdot, \\cdot ]: \\mathfrak{g} \\times \\mathfrak{g} \\to \\mathfrak{g}$ is a bilinear map. This means that for any vector fields $U, V, W \\in \\mathfrak{g}$ and scalars $\\alpha, \\beta \\in \\mathbb{R}$, it satisfies:\n$$[\\alpha U + \\beta V, W] = \\alpha[U, W] + \\beta[V, W] \\quad (\\text{linearity in the first argument})$$\n$$[W, \\alpha U + \\beta V] = \\alpha[W, U] + \\beta[W, V] \\quad (\\text{linearity in the second argument})$$\n\nWe apply this property to compute the commutator $[D,E]$:\n$$[D, E] = \\left[ \\sum_{i=1}^{n} a_{i} X_{i}, \\sum_{j=1}^{n} b_{j} X_{j} \\right]$$\nUsing the linearity of the Lie bracket in the first argument, we can factor out the sum and the constant coefficients $a_{i}$:\n$$[D, E] = \\sum_{i=1}^{n} a_{i} \\left[ X_{i}, \\sum_{j=1}^{n} b_{j} X_{j} \\right]$$\nNext, for each term in the sum over $i$, we apply the linearity of the Lie bracket in the second argument. This allows us to factor out the sum over $j$ and the constant coefficients $b_{j}$:\n$$[D, E] = \\sum_{i=1}^{n} a_{i} \\left( \\sum_{j=1}^{n} b_{j} [X_{i}, X_{j}] \\right)$$\nSince the coefficients $a_{i}$ and $b_{j}$ are scalars, they commute with all other elements. We can rearrange the expression to group the scalar coefficients:\n$$[D, E] = \\sum_{i=1}^{n} \\sum_{j=1}^{n} a_{i} b_{j} [X_{i}, X_{j}]$$\nThis result expresses the commutator $[D,E]$ as a linear combination of the commutators of the basis vector fields, $[X_{i}, X_{j}]$. The Leibniz property for vector fields ensures that $[X_{i}, X_{j}]$ is itself a (first-order) vector field, and since $X_i$ and $X_j$ are left-invariant, so is their commutator. Thus, $[X_{i}, X_{j}]$ must be an element of the Lie algebra $\\mathfrak{g}$ and can be expressed as a linear combination of the basis vectors $\\{X_{k}\\}$.\n\nThe problem provides this expansion through the definition of the structure constants $c_{ij}^{k}$:\n$$[X_{i}, X_{j}] = \\sum_{k=1}^{n} c_{ij}^{k} X_{k}$$\nThese constants are fundamental to the structure of the Lie algebra $\\mathfrak{g}$.\n\nWe substitute this definition into our expression for $[D,E]$:\n$$[D, E] = \\sum_{i=1}^{n} \\sum_{j=1}^{n} a_{i} b_{j} \\left( \\sum_{k=1}^{n} c_{ij}^{k} X_{k} \\right)$$\nTo obtain the final expression as a single linear combination of the basis vectors $\\{X_{k}\\}$, we rearrange the order of the summations. As the sums are over finite sets, this is a valid operation:\n$$[D, E] = \\sum_{k=1}^{n} \\sum_{i=1}^{n} \\sum_{j=1}^{n} a_{i} b_{j} c_{ij}^{k} X_{k}$$\nFinally, we can group all the scalar terms that form the coefficient of each basis vector $X_{k}$:\n$$[D, E] = \\sum_{k=1}^{n} \\left( \\sum_{i=1}^{n} \\sum_{j=1}^{n} c_{ij}^{k} a_{i} b_{j} \\right) X_{k}$$\nThis is the final expression for the commutator differential operator $[D,E]$. It is a left-invariant vector field, and its components in the basis $\\{X_{k}\\}$ are given by the expression in the parenthesis. This result demonstrates how the commutator of linear combinations of basis vectors in a Lie algebra is determined by the algebra's structure constants.",
            "answer": "$$\n\\boxed{\\sum_{k=1}^{n} \\left( \\sum_{i=1}^{n} \\sum_{j=1}^{n} c_{ij}^{k} a_{i} b_{j} \\right) X_{k}}\n$$"
        }
    ]
}