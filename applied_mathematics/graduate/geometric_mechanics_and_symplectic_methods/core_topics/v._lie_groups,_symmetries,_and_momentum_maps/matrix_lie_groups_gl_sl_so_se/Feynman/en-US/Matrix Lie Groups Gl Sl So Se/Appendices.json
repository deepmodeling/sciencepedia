{
    "hands_on_practices": [
        {
            "introduction": "Understanding a matrix Lie group begins with grasping its nature as a smooth manifold. This exercise provides foundational practice in this area by asking you to determine the dimension of the Special Orthogonal group, $SO(n)$. By treating the defining algebraic conditions of $SO(n)$ as constraints that carve out a submanifold within the larger space of all invertible matrices, you will apply core techniques from differential geometry to count the degrees of freedom, thereby solidifying your intuition for the geometric structure of these groups .",
            "id": "3755409",
            "problem": "Let $GL(n)$ denote the General Linear Group (GL($n$)) of real invertible $n \\times n$ matrices and let $SO(n)$ denote the Special Orthogonal Group (SO($n$)), defined as the set of $Q \\in GL(n)$ satisfying $Q^{\\top}Q = I$ and $\\det(Q) = 1$. Consider the smooth map $F: GL(n) \\to \\operatorname{Sym}(n)$, where $\\operatorname{Sym}(n)$ denotes the vector space of real symmetric $n \\times n$ matrices, given by $F(Q) = Q^{\\top}Q - I$. Using first principles appropriate to geometric mechanics and matrix Lie groups (including the definition of tangent spaces via matrix curves and the derivative of a smooth matrix-valued map), analyze the constraints imposed by $Q^{\\top}Q = I$ to determine the codimension of $SO(n)$ in $GL(n)$. Justify that these constraints are independent by computing the derivative $DF_Q$ at a point $Q \\in SO(n)$ and showing its range coincides with $\\operatorname{Sym}(n)$. From this codimension, verify the dimension formula for $SO(n)$, namely $n(n-1)/2$. Provide the codimension as a single closed-form expression in terms of $n$. No numerical approximation is required.",
            "solution": "The problem requires us to determine the codimension of the Special Orthogonal Group, $SO(n)$, within the General Linear Group, $GL(n)$, by analyzing the constraints that define it. The group $GL(n)$ is the set of all real invertible $n \\times n$ matrices. It is an open subset of the vector space $M_n(\\mathbb{R})$ of all $n \\times n$ real matrices, and thus is a manifold of dimension $\\dim(GL(n)) = n^2$.\n\nThe Special Orthogonal Group $SO(n)$ is defined as the set of matrices $Q \\in GL(n)$ that satisfy two conditions:\n$1.$ $Q^{\\top}Q = I$ (orthogonality)\n$2.$ $\\det(Q) = 1$ (special condition)\n\nThe problem directs us to analyze the constraint $Q^{\\top}Q = I$. We define a map $F: GL(n) \\to \\operatorname{Sym}(n)$, where $\\operatorname{Sym}(n)$ is the vector space of real symmetric $n \\times n$ matrices, by\n$$F(Q) = Q^{\\top}Q - I$$\nThe set of matrices satisfying the orthogonality condition is the Orthogonal Group $O(n)$, which is the level set of $F$ corresponding to the zero matrix $0 \\in \\operatorname{Sym}(n)$. That is, $O(n) = F^{-1}(0)$.\n\nThe group $SO(n)$ is a subset of $O(n)$ satisfying the additional condition $\\det(Q) = 1$. The determinant of an orthogonal matrix is always $\\pm 1$. The set $O(n)$ consists of two connected components, one with determinant $1$ (which is $SO(n)$) and one with determinant $-1$. As $SO(n)$ is an open submanifold of $O(n)$, they share the same dimension: $\\dim(SO(n)) = \\dim(O(n))$. Therefore, the codimension of $SO(n)$ in $GL(n)$ is the same as the codimension of $O(n)$ in $GL(n)$.\n\nWe can determine this codimension using the regular value theorem. If $0 \\in \\operatorname{Sym}(n)$ is a regular value of the map $F$, then its preimage $F^{-1}(0) = O(n)$ is a submanifold of $GL(n)$ whose codimension is equal to the dimension of the target space, $\\operatorname{Sym}(n)$. A value is regular if the derivative of the map is surjective at every point in the preimage.\n\nLet's compute the derivative of $F$ at a point $Q \\in O(n)$. The derivative, or differential, $DF_Q$, is a linear map from the tangent space of the domain at $Q$ to the tangent space of the codomain at $F(Q)$.\nThe domain is $GL(n)$, an open set in $M_n(\\mathbb{R})$, so its tangent space at any point $Q$, $T_Q GL(n)$, can be identified with $M_n(\\mathbb{R})$.\nThe codomain is the vector space $\\operatorname{Sym}(n)$, so its tangent space at any point, including the point $F(Q) = 0$, can be identified with $\\operatorname{Sym}(n)$ itself. Thus, $DF_Q: M_n(\\mathbb{R}) \\to \\operatorname{Sym}(n)$.\n\nTo find the action of $DF_Q$, we consider a smooth curve $\\gamma(t)$ in $GL(n)$ with $\\gamma(0) = Q$ and velocity $\\gamma'(0) = V$, where $V \\in M_n(\\mathbb{R})$ is a tangent vector. The derivative of $F$ at $Q$ applied to $V$ is:\n$$DF_Q(V) = \\frac{d}{dt}\\bigg|_{t=0} F(\\gamma(t)) = \\frac{d}{dt}\\bigg|_{t=0} (\\gamma(t)^{\\top}\\gamma(t) - I)$$\nUsing the product rule for matrix differentiation:\n$$DF_Q(V) = \\gamma'(0)^{\\top}\\gamma(0) + \\gamma(0)^{\\top}\\gamma'(0) = V^{\\top}Q + Q^{\\top}V$$\nWe need to show that this map $DF_Q: V \\mapsto V^{\\top}Q + Q^{\\top}V$ is surjective for any $Q \\in O(n)$. This means that for any symmetric matrix $S \\in \\operatorname{Sym}(n)$, we must find a matrix $V \\in M_n(\\mathbb{R})$ such that $V^{\\top}Q + Q^{\\top}V = S$.\n\nSince $Q \\in O(n)$, it is invertible. Any tangent vector $V \\in M_n(\\mathbb{R})$ can be expressed as $V = QX$ for some unique $X \\in M_n(\\mathbb{R})$. Substituting this into the expression for the derivative:\n$$DF_Q(QX) = (QX)^{\\top}Q + Q^{\\top}(QX) = X^{\\top}Q^{\\top}Q + Q^{\\top}QX$$\nSince $Q \\in O(n)$, we have $Q^{\\top}Q = I$. The expression simplifies to:\n$$DF_Q(QX) = X^{\\top}I + IX = X^{\\top} + X$$\nThe problem is now reduced to showing that for any symmetric matrix $S \\in \\operatorname{Sym}(n)$, there exists a matrix $X \\in M_n(\\mathbb{R})$ such that $X^{\\top} + X = S$.\nLet's choose $X = \\frac{1}{2}S$. Since $S$ is symmetric, $S^{\\top} = S$. Then,\n$$X^{\\top} + X = \\left(\\frac{1}{2}S\\right)^{\\top} + \\frac{1}{2}S = \\frac{1}{2}S^{\\top} + \\frac{1}{2}S = \\frac{1}{2}S + \\frac{1}{2}S = S$$\nThis demonstrates that for any $S \\in \\operatorname{Sym}(n)$, we can find a matrix $X = \\frac{1}{2}S$, which corresponds to the tangent vector $V = Q(\\frac{1}{2}S)$, such that $DF_Q(V) = S$. Thus, the derivative $DF_Q$ is surjective for all $Q \\in O(n)$.\nThe surjectivity of the derivative implies that the constraints defined by the equations in $Q^{\\top}Q - I = 0$ are independent.\n\nBy the regular value theorem, the codimension of $O(n)$ (and thus $SO(n)$) in $GL(n)$ is equal to the dimension of the target space $\\operatorname{Sym}(n)$. A symmetric $n \\times n$ matrix is determined by its entries on and above the main diagonal. There are $n$ entries on the diagonal and $\\binom{n}{2} = \\frac{n(n-1)}{2}$ entries strictly above the diagonal.\nTherefore, the dimension of $\\operatorname{Sym}(n)$ is:\n$$\\dim(\\operatorname{Sym}(n)) = n + \\frac{n(n-1)}{2} = \\frac{2n + n^2 - n}{2} = \\frac{n^2 + n}{2} = \\frac{n(n+1)}{2}$$\nThis is the codimension of $SO(n)$ in $GL(n)$.\n\nFinally, we verify the dimension formula for $SO(n)$. The dimension of a submanifold is the dimension of the ambient manifold minus its codimension.\n$$\\dim(SO(n)) = \\dim(GL(n)) - \\text{codim}(SO(n))$$\n$$\\dim(SO(n)) = n^2 - \\frac{n(n+1)}{2} = \\frac{2n^2 - (n^2+n)}{2} = \\frac{n^2 - n}{2} = \\frac{n(n-1)}{2}$$\nThis confirms the dimension formula given in the problem statement. The codimension of $SO(n)$ in $GL(n)$ is $\\frac{n(n+1)}{2}$.",
            "answer": "$$\\boxed{\\frac{n(n+1)}{2}}$$"
        },
        {
            "introduction": "The exponential map is the crucial bridge connecting a Lie algebra with its corresponding Lie group. This practice demystifies this fundamental concept by focusing on the simplest non-trivial example: the group of planar rotations, $SO(2)$, and its algebra, $\\mathfrak{so}(2)$. By explicitly deriving the exponential map, you will see how the algebraic structure of skew-symmetric matrices directly generates the familiar trigonometric form of rotation matrices, and you will prove that every rotation can be generated this way .",
            "id": "3755442",
            "problem": "Let $SO(2)$ denote the Special Orthogonal group (SO), defined by $SO(2)=\\{R\\in GL(2,\\mathbb{R}) \\mid R^{\\top}R=I,\\ \\det(R)=1\\}$, and let $\\mathfrak{so}(2)$ denote its Lie algebra, defined by $\\mathfrak{so}(2)=\\{X\\in \\mathbb{R}^{2\\times 2} \\mid X^{\\top}=-X\\}$. The matrix exponential of $X\\in\\mathfrak{so}(2)$ is defined by $\\exp(X)=\\sum_{k=0}^{\\infty}\\frac{1}{k!}X^{k}$. In planar geometric mechanics, a constant angular velocity is represented by a skew-symmetric generator $X\\in\\mathfrak{so}(2)$, and its finite rotation after unit time is given by $\\exp(X)$.\n\nUsing only the definitions above and standard linear algebra facts, do the following:\n- Prove that the exponential map $\\exp:\\mathfrak{so}(2)\\to SO(2)$ is surjective, namely that for every $R\\in SO(2)$ there exists $X\\in\\mathfrak{so}(2)$ with $R=\\exp(X)$.\n- Let $J\\in\\mathfrak{so}(2)$ be the skew-symmetric generator defined by $J=\\begin{pmatrix}0 & -1\\\\ 1 & 0\\end{pmatrix}$, and let $\\theta\\in\\mathbb{R}$. Compute the explicit closed-form expression for the matrix exponential $\\exp(\\theta J)$.\n\nYour final answer must be the single closed-form expression for $\\exp(\\theta J)$, simplified as far as possible. No numerical approximation is required.",
            "solution": "The solution will proceed in two main parts. First, we will compute the explicit expression for $\\exp(\\theta J)$, as this result is instrumental for the second part, which is the proof of surjectivity.\n\n**Part 1: Computation of $\\exp(\\theta J)$**\n\nWe are asked to find a closed-form expression for the matrix exponential $\\exp(\\theta J)$, where $J = \\begin{pmatrix} 0 & -1 \\\\ 1 & 0 \\end{pmatrix}$ and $\\theta \\in \\mathbb{R}$. The matrix $X = \\theta J = \\begin{pmatrix} 0 & -\\theta \\\\ \\theta & 0 \\end{pmatrix}$ is an element of $\\mathfrak{so}(2)$, since $(\\theta J)^{\\top} = \\theta J^{\\top} = \\theta \\begin{pmatrix} 0 & 1 \\\\ -1 & 0 \\end{pmatrix} = -\\theta J$.\n\nThe matrix exponential is defined by the power series:\n$$\n\\exp(\\theta J) = \\sum_{k=0}^{\\infty} \\frac{1}{k!}(\\theta J)^k = \\sum_{k=0}^{\\infty} \\frac{\\theta^k}{k!} J^k\n$$\nTo evaluate this series, we first investigate the powers of the matrix $J$:\n$J^0 = I = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$\n$J^1 = J = \\begin{pmatrix} 0 & -1 \\\\ 1 & 0 \\end{pmatrix}$\n$J^2 = J J = \\begin{pmatrix} 0 & -1 \\\\ 1 & 0 \\end{pmatrix} \\begin{pmatrix} 0 & -1 \\\\ 1 & 0 \\end{pmatrix} = \\begin{pmatrix} -1 & 0 \\\\ 0 & -1 \\end{pmatrix} = -I$\n$J^3 = J^2 J = (-I)J = -J$\n$J^4 = J^2 J^2 = (-I)(-I) = I$\n\nThe powers of $J$ are periodic with a period of $4$: $\\{I, J, -I, -J, \\dots\\}$. We can now expand the series for $\\exp(\\theta J)$:\n$$\n\\exp(\\theta J) = \\frac{\\theta^0}{0!}J^0 + \\frac{\\theta^1}{1!}J^1 + \\frac{\\theta^2}{2!}J^2 + \\frac{\\theta^3}{3!}J^3 + \\frac{\\theta^4}{4!}J^4 + \\dots\n$$\nSubstituting the powers of $J$:\n$$\n\\exp(\\theta J) = I + \\theta J - \\frac{\\theta^2}{2!}I - \\frac{\\theta^3}{3!}J + \\frac{\\theta^4}{4!}I + \\dots\n$$\nWe can group the terms associated with the identity matrix $I$ and the terms associated with the matrix $J$:\n$$\n\\exp(\\theta J) = \\left(1 - \\frac{\\theta^2}{2!} + \\frac{\\theta^4}{4!} - \\dots\\right)I + \\left(\\theta - \\frac{\\theta^3}{3!} + \\frac{\\theta^5}{5!} - \\dots\\right)J\n$$\nThe two series in parentheses are the Maclaurin series for the cosine and sine functions, respectively:\n$\\cos(\\theta) = \\sum_{k=0}^{\\infty} \\frac{(-1)^k \\theta^{2k}}{(2k)!} = 1 - \\frac{\\theta^2}{2!} + \\frac{\\theta^4}{4!} - \\dots$\n$\\sin(\\theta) = \\sum_{k=0}^{\\infty} \\frac{(-1)^k \\theta^{2k+1}}{(2k+1)!} = \\theta - \\frac{\\theta^3}{3!} + \\frac{\\theta^5}{5!} - \\dots$\n\nTherefore, we can write the expression for $\\exp(\\theta J)$ in the compact form:\n$$\n\\exp(\\theta J) = \\cos(\\theta)I + \\sin(\\theta)J\n$$\nSubstituting the matrix representations for $I$ and $J$:\n$$\n\\exp(\\theta J) = \\cos(\\theta)\\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} + \\sin(\\theta)\\begin{pmatrix} 0 & -1 \\\\ 1 & 0 \\end{pmatrix} = \\begin{pmatrix} \\cos(\\theta) & 0 \\\\ 0 & \\cos(\\theta) \\end{pmatrix} + \\begin{pmatrix} 0 & -\\sin(\\theta) \\\\ \\sin(\\theta) & 0 \\end{pmatrix}\n$$\nThis simplifies to the final closed-form expression:\n$$\n\\exp(\\theta J) = \\begin{pmatrix} \\cos(\\theta) & -\\sin(\\theta) \\\\ \\sin(\\theta) & \\cos(\\theta) \\end{pmatrix}\n$$\nThis completes the second task of the problem.\n\n**Part 2: Proof of Surjectivity**\n\nWe must prove that the exponential map $\\exp: \\mathfrak{so}(2) \\to SO(2)$ is surjective. This means that for any matrix $R \\in SO(2)$, there exists a matrix $X \\in \\mathfrak{so}(2)$ such that $\\exp(X) = R$.\n\nFirst, let's characterize a general element $R \\in SO(2)$. Let $R = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$. The defining properties are $R^{\\top}R=I$ and $\\det(R)=1$. The first property implies:\n$$\nR^{\\top}R = \\begin{pmatrix} a & c \\\\ b & d \\end{pmatrix}\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix} = \\begin{pmatrix} a^2+c^2 & ab+cd \\\\ ab+cd & b^2+d^2 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}\n$$\nThis gives the conditions: $a^2+c^2=1$, $b^2+d^2=1$, and $ab+cd=0$.\nThe condition $a^2+c^2=1$ allows us to parameterize $a$ and $c$ by an angle $\\phi \\in \\mathbb{R}$, such that $a = \\cos(\\phi)$ and $c = \\sin(\\phi)$.\nThe condition $\\det(R)=ad-bc=1$ becomes $d\\cos(\\phi) - b\\sin(\\phi)=1$.\nThe condition $ab+cd=0$ becomes $b\\cos(\\phi) + d\\sin(\\phi)=0$.\nWe have a system of two linear equations in $b$ and $d$:\n1) $d\\cos(\\phi) - b\\sin(\\phi)=1$\n2) $d\\sin(\\phi) + b\\cos(\\phi)=0$\nMultiplying (1) by $\\cos(\\phi)$ and (2) by $\\sin(\\phi)$ and adding them gives:\n$d(\\cos^2(\\phi)+\\sin^2(\\phi)) = \\cos(\\phi) \\implies d=\\cos(\\phi)$.\nMultiplying (1) by $-\\sin(\\phi)$ and (2) by $\\cos(\\phi)$ and adding them gives:\n$b(\\sin^2(\\phi)+\\cos^2(\\phi)) = -\\sin(\\phi) \\implies b=-\\sin(\\phi)$.\nThus, any matrix $R \\in SO(2)$ can be written in the form of a standard rotation matrix:\n$$\nR = \\begin{pmatrix} \\cos(\\phi) & -\\sin(\\phi) \\\\ \\sin(\\phi) & \\cos(\\phi) \\end{pmatrix}\n$$\nfor some angle $\\phi \\in \\mathbb{R}$.\n\nNow, we need to find an $X \\in \\mathfrak{so}(2)$ such that $\\exp(X)=R$. A general element $X \\in \\mathfrak{so}(2)$ has the form $X = \\theta J$ for some $\\theta \\in \\mathbb{R}$.\nFrom Part 1, we have the expression for $\\exp(\\theta J)$:\n$$\n\\exp(\\theta J) = \\begin{pmatrix} \\cos(\\theta) & -\\sin(\\theta) \\\\ \\sin(\\theta) & \\cos(\\theta) \\end{pmatrix}\n$$\nTo achieve $\\exp(X) = R$, we must equate the two matrices:\n$$\n\\begin{pmatrix} \\cos(\\theta) & -\\sin(\\theta) \\\\ \\sin(\\theta) & \\cos(\\theta) \\end{pmatrix} = \\begin{pmatrix} \\cos(\\phi) & -\\sin(\\phi) \\\\ \\sin(\\phi) & \\cos(\\phi) \\end{pmatrix}\n$$\nBy inspection, we can satisfy this equality by choosing $\\theta = \\phi$.\nTherefore, for any given $R \\in SO(2)$, which corresponds to a rotation by an angle $\\phi$, we can choose the matrix $X = \\phi J \\in \\mathfrak{so}(2)$. The exponential of this $X$ is\n$$\n\\exp(X) = \\exp(\\phi J) = \\begin{pmatrix} \\cos(\\phi) & -\\sin(\\phi) \\\\ \\sin(\\phi) & \\cos(\\phi) \\end{pmatrix} = R\n$$\nSince for every element $R$ in the codomain $SO(2)$ we have found a pre-image $X$ in the domain $\\mathfrak{so}(2)$, the map $\\exp: \\mathfrak{so}(2) \\to SO(2)$ is surjective. This concludes the proof.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\cos(\\theta) & -\\sin(\\theta) \\\\ \\sin(\\theta) & \\cos(\\theta) \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "A key motivation for studying Lie groups in mechanics is to develop numerical methods that respect the physical constraints of a system, such as the orientation of a rigid body. This hands-on coding exercise demonstrates how to build a \"Lie group integrator\" for dynamics on $SO(3)$. Using tools like the Baker-Campbell-Hausdorff formula and the Rodrigues rotation formula, you will construct a numerical scheme that inherently preserves the group structure, preventing the unphysical drift that plagues naive integration methods and providing a robust tool for simulation in geometric mechanics .",
            "id": "3755449",
            "problem": "Consider the Special Orthogonal Group (SO(3)) defined as the set of all real $3 \\times 3$ matrices $R$ such that $R^{\\top} R = I_3$ and $\\det(R) = 1$. Its Lie algebra $\\mathfrak{so}(3)$ consists of all $3 \\times 3$ real skew-symmetric matrices. The dynamics of a rigid body orientation in geometric mechanics can be modeled as a right-invariant ordinary differential equation on $SO(3)$:\n$$\n\\dot{R}(t) = R(t) \\widehat{\\omega}(t), \\quad R(0) = I_3,\n$$\nwhere $\\omega(t) \\in \\mathbb{R}^3$ is the time-dependent body angular velocity, and $\\widehat{\\cdot} : \\mathbb{R}^3 \\to \\mathfrak{so}(3)$ is the \"hat\" isomorphism defined for $v = (v_1, v_2, v_3)^{\\top}$ by\n$$\n\\widehat{v} = \\begin{bmatrix}\n0 & -v_3 & v_2 \\\\\nv_3 & 0 & -v_1 \\\\\n-v_2 & v_1 & 0\n\\end{bmatrix}.\n$$\nYou are to design a second-order accurate Lie group integrator for this equation using the Baker–Campbell–Hausdorff (BCH) formula. The Baker–Campbell–Hausdorff (BCH) formula provides the logarithm of the product of exponentials in a Lie algebra. Truncated to second order in the step size, for $X, Y \\in \\mathfrak{so}(3)$ that scale linearly with a small parameter $h$, it reads\n$$\n\\mathrm{BCH}_2(X,Y) = X + Y + \\frac{1}{2}[X,Y],\n$$\nwhere $[X,Y] = XY - YX$ is the matrix commutator. Using this, construct a one-step method over a time step $h$ that advances $R_k \\approx R(t_k)$ to $R_{k+1} \\approx R(t_{k+1})$ via a single exponential:\n$$\nR_{k+1} = R_k \\exp\\left(\\Omega_k\\right),\n$$\nwith\n$$\n\\Omega_k = \\mathrm{BCH}_2\\!\\left(\\frac{h}{2}\\,\\widehat{\\omega}(t_k),\\,\\frac{h}{2}\\,\\widehat{\\omega}(t_{k+1})\\right).\n$$\nThis method should be globally second-order accurate in the step size $h$ for sufficiently smooth $\\omega(t)$. The matrix exponential $\\exp(\\cdot)$ applied to elements of $\\mathfrak{so}(3)$ should be implemented using the Rodrigues formula. Let $\\mathrm{vee}:\\mathfrak{so}(3)\\to\\mathbb{R}^3$ denote the inverse of the hat map, and for any $v \\in \\mathbb{R}^3$, define the exponential map on $\\mathfrak{so}(3)$ by\n$$\n\\exp(\\widehat{v}) = I_3 + \\frac{\\sin \\theta}{\\theta}\\,\\widehat{v} + \\frac{1 - \\cos \\theta}{\\theta^2}\\,\\widehat{v}^2, \\quad \\theta = \\|v\\|.\n$$\nYour program must:\n- Implement the above second-order BCH-based integrator on $SO(3)$.\n- Use angular velocity $\\omega(t)$ expressed in radians per second. Time $t$ is in seconds. All angles must be treated in radians.\n- For each test case below, compute the final rotation matrix $R(T)$ using your integrator and compare it to a reference $R_{\\mathrm{ref}}(T)$. Report the Frobenius norm of the difference:\n$$\n\\varepsilon = \\|R(T) - R_{\\mathrm{ref}}(T)\\|_F.\n$$\n- The reference $R_{\\mathrm{ref}}(T)$ is either an exact solution when available or a high-resolution numerical solution obtained by the same BCH-based integrator with a very small step size.\n\nUse the following test suite, each specified by the angular velocity function $\\omega(t)$, the final time $T$, and the number of steps $N$:\n1. Boundary condition (zero motion): $\\omega(t) = (0, 0, 0)$, $T = 1.0$, $N = 10$. Reference is $R_{\\mathrm{ref}}(T) = I_3$.\n2. Constant commuting case: $\\omega(t) = (0, 0, 1.0)$, $T = 1.0$, $N = 40$. Reference is the exact solution $R_{\\mathrm{ref}}(T) = \\exp\\left(\\widehat{(0,0,1.0)}\\,T\\right)$.\n3. Single-axis time-varying commuting case: $\\omega(t) = (2.0\\,t, 0, 0)$, $T = 1.0$, $N = 100$. Since $\\widehat{\\omega}(t)$ is always along the same axis, the exact solution is $R_{\\mathrm{ref}}(T) = \\exp\\left(\\widehat{\\left(\\frac{2.0}{2}T^2, 0, 0\\right)}\\right)$.\n4. Non-commuting oscillatory case: $\\omega(t) = (\\sin t, \\cos t, 0)$, $T = 2\\pi$, $N = 400$. Reference is a high-resolution numerical solution $R_{\\mathrm{ref}}(T)$ computed by the same BCH-based method with $N_{\\mathrm{ref}} = 20000$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4]\"). Each result must be the floating-point value of $\\varepsilon$ for the corresponding test case, expressed in dimensionless units (matrix norm), with full Python default float formatting. No other text should be printed.",
            "solution": "The problem requires the implementation of a second-order Lie group integrator for the dynamics of a rigid body on the Special Orthogonal Group $SO(3)$. The analysis and implementation will proceed as follows: first, we will simplify the expression for the integrator's update step using the properties of the Lie algebra $\\mathfrak{so}(3)$. Second, we will detail the implementation of the matrix exponential map using the provided Rodrigues formula, paying special attention to numerical stability. Finally, we will outline the complete algorithm to be implemented.\n\nThe governing ordinary differential equation is given as a right-invariant system on $SO(3)$:\n$$\n\\dot{R}(t) = R(t) \\widehat{\\omega}(t), \\quad R(0) = I_3\n$$\nwhere $R(t) \\in SO(3)$ is the orientation matrix, $\\omega(t) \\in \\mathbb{R}^3$ is the body angular velocity, and $\\widehat{\\cdot}: \\mathbb{R}^3 \\to \\mathfrak{so}(3)$ is the hat isomorphism. For a vector $v = (v_1, v_2, v_3)^{\\top} \\in \\mathbb{R}^3$, this map is defined as:\n$$\n\\widehat{v} = \\begin{bmatrix}\n0 & -v_3 & v_2 \\\\\nv_3 & 0 & -v_1 \\\\\n-v_2 & v_1 & 0\n\\end{bmatrix}\n$$\nThe Lie algebra $\\mathfrak{so}(3)$ is the space of $3 \\times 3$ real skew-symmetric matrices. The matrix commutator on $\\mathfrak{so}(3)$ corresponds to the cross product in $\\mathbb{R}^3$ via the identity $[\\widehat{u}, \\widehat{v}] = \\widehat{u \\times v}$ for any $u, v \\in \\mathbb{R}^3$.\n\nThe proposed one-step numerical integrator advances the solution from time $t_k$ to $t_{k+1} = t_k + h$ using the formula:\n$$\nR_{k+1} = R_k \\exp(\\Omega_k)\n$$\nThe increment generator $\\Omega_k \\in \\mathfrak{so}(3)$ is computed using the second-order truncated Baker–Campbell–Hausdorff (BCH) formula:\n$$\n\\Omega_k = \\mathrm{BCH}_2\\!\\left(\\frac{h}{2}\\,\\widehat{\\omega}(t_k),\\,\\frac{h}{2}\\,\\widehat{\\omega}(t_{k+1})\\right) = X + Y + \\frac{1}{2}[X,Y]\n$$\nwhere $X = \\frac{h}{2}\\,\\widehat{\\omega}(t_k)$ and $Y = \\frac{h}{2}\\,\\widehat{\\omega}(t_{k+1})$.\n\nA more computationally efficient form for $\\Omega_k$ can be derived. Let $\\omega_k = \\omega(t_k)$ and $\\omega_{k+1} = \\omega(t_{k+1})$. Using the linearity of the hat map ($\\widehat{\\alpha u + \\beta v} = \\alpha\\widehat{u} + \\beta\\widehat{v}$) and the commutator identity, we can rewrite $\\Omega_k$ as follows:\n$$\n\\Omega_k = \\frac{h}{2}\\widehat{\\omega}_k + \\frac{h}{2}\\widehat{\\omega}_{k+1} + \\frac{1}{2}\\left[\\frac{h}{2}\\widehat{\\omega}_k, \\frac{h}{2}\\widehat{\\omega}_{k+1}\\right]\n$$\n$$\n\\Omega_k = \\frac{h}{2}(\\widehat{\\omega}_k + \\widehat{\\omega}_{k+1}) + \\frac{h^2}{8}[\\widehat{\\omega}_k, \\widehat{\\omega}_{k+1}]\n$$\n$$\n\\Omega_k = \\widehat{\\frac{h}{2}(\\omega_k + \\omega_{k+1})} + \\frac{h^2}{8}\\widehat{(\\omega_k \\times \\omega_{k+1})}\n$$\n$$\n\\Omega_k = \\widehat{\\left( \\frac{h}{2}(\\omega_k + \\omega_{k+1}) + \\frac{h^2}{8}(\\omega_k \\times \\omega_{k+1}) \\right)}\n$$\nThis result shows that the matrix $\\Omega_k$ is the `hat` of a single vector, which we denote $\\delta v_k$:\n$$\n\\delta v_k = \\frac{h}{2}(\\omega_k + \\omega_{k+1}) + \\frac{h^2}{8}(\\omega_k \\times \\omega_{k+1})\n$$\nThus, the calculation of $\\Omega_k$ simplifies to computing the vector $\\delta v_k$ and then applying the hat map, i.e., $\\Omega_k = \\widehat{\\delta v_k}$. This avoids explicit matrix multiplication for the commutator.\n\nThe next step is to compute the matrix exponential $\\exp(\\Omega_k) = \\exp(\\widehat{\\delta v_k})$. This is accomplished using the Rodrigues formula for a generic $\\widehat{v} \\in \\mathfrak{so}(3)$:\n$$\n\\exp(\\widehat{v}) = I_3 + \\frac{\\sin \\theta}{\\theta}\\,\\widehat{v} + \\frac{1 - \\cos \\theta}{\\theta^2}\\,\\widehat{v}^2, \\quad \\theta = \\|v\\|_2\n$$\nDirect implementation of this formula can lead to numerical instability for small values of $\\theta$\ndue to division by zero or catastrophic cancellation. To ensure robustness, we handle the case where $\\theta$ is close to zero by using Taylor series expansions for the coefficients:\n- For $\\theta \\approx 0$, $\\frac{\\sin \\theta}{\\theta} \\approx 1 - \\frac{\\theta^2}{6} + \\frac{\\theta^4}{120}$\n- For $\\theta \\approx 0$, $\\frac{1 - \\cos \\theta}{\\theta^2} \\approx \\frac{1}{2} - \\frac{\\theta^2}{24} + \\frac{\\theta^4}{720}$\nA threshold, for instance $\\theta < 10^{-8}$, can be used to switch between the direct formula and the Taylor series approximation. If $\\theta = 0$, then $v=0$, $\\widehat{v}=0$, and $\\exp(\\widehat{v}) = I_3$.\n\nThe complete integration algorithm is as follows:\n$1$. Initialize $R_0 = I_3$ and $t_0 = 0$. The step size is $h = T/N$.\n$2$. For $k$ from $0$ to $N-1$:\n    a. Determine the time points $t_k = k h$ and $t_{k+1} = (k+1)h$.\n    b. Evaluate the angular velocities $\\omega_k = \\omega(t_k)$ and $\\omega_{k+1} = \\omega(t_{k+1})$.\n    c. Compute the increment vector $\\delta v_k = \\frac{h}{2}(\\omega_k + \\omega_{k+1}) + \\frac{h^2}{8}(\\omega_k \\times \\omega_{k+1})$.\n    d. Compute the incremental rotation matrix $\\Delta R_k = \\exp(\\widehat{\\delta v_k})$ using the stabilized Rodrigues formula.\n    e. Update the orientation: $R_{k+1} = R_k \\Delta R_k$.\n$3$. The final orientation at time $T$ is $R_N$.\n\nFor each test case, this computed $R(T) = R_N$ is compared to a reference solution $R_{\\mathrm{ref}}(T)$. The error is quantified by the Frobenius norm of the difference:\n$$\n\\varepsilon = \\|R(T) - R_{\\mathrm{ref}}(T)\\|_F = \\sqrt{\\sum_{i=1}^3 \\sum_{j=1}^3 (R_{ij}(T) - (R_{\\mathrm{ref}})_{ij}(T))^2}\n$$\nThe implementation will follow these principles to solve the test cases provided.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a second-order Lie group integrator for SO(3) and evaluates it on a test suite.\n    \"\"\"\n\n    def hat(v):\n        \"\"\"\n        Maps a 3D vector to its corresponding 3x3 skew-symmetric matrix.\n        v: A 3-element array-like object.\n        Returns: A 3x3 numpy array.\n        \"\"\"\n        v = np.asarray(v)\n        return np.array([\n            [0, -v[2], v[1]],\n            [v[2], 0, -v[0]],\n            [-v[1], v[0], 0]\n        ])\n\n    def exp_so3(v):\n        \"\"\"\n        Computes the matrix exponential for so(3) using Rodrigues' formula.\n        Handles the small-angle case for numerical stability.\n        v: A 3-element array-like object representing the rotation vector.\n        Returns: A 3x3 rotation matrix.\n        \"\"\"\n        v = np.asarray(v)\n        v_hat = hat(v)\n        theta = np.linalg.norm(v)\n\n        # Handle the case theta -> 0 to avoid division by zero and loss of precision.\n        # A threshold is used to switch to a Taylor series approximation.\n        if theta < 1e-9:\n            # For small theta, use Taylor series for the coefficients.\n            # A = sin(theta)/theta ≈ 1 - theta^2/6 + theta^4/120\n            # B = (1-cos(theta))/theta^2 ≈ 1/2 - theta^2/24 + theta^4/720\n            theta2 = theta**2\n            theta4 = theta**4\n            A = 1.0 - theta2 / 6.0 + theta4 / 120.0\n            B = 0.5 - theta2 / 24.0 + theta4 / 720.0\n        else:\n            A = np.sin(theta) / theta\n            B = (1.0 - np.cos(theta)) / (theta**2)\n\n        return np.identity(3) + A * v_hat + B * np.dot(v_hat, v_hat)\n\n    def run_integration(omega_func, T, N):\n        \"\"\"\n        Performs the numerical integration of the dynamics equation.\n        omega_func: A function that takes time t and returns the angular velocity vector.\n        T: Final time.\n        N: Number of steps.\n        Returns: The final rotation matrix R(T).\n        \"\"\"\n        h = T / N\n        R = np.identity(3)\n        \n        for k in range(N):\n            t_k = k * h\n            t_k_plus_1 = (k + 1) * h\n            \n            omega_k = np.asarray(omega_func(t_k))\n            omega_k_plus_1 = np.asarray(omega_func(t_k_plus_1))\n            \n            # Compute the increment vector delta_v_k based on the BCH formula simplification\n            delta_v_k = (h / 2.0) * (omega_k + omega_k_plus_1) + \\\n                        (h**2 / 8.0) * np.cross(omega_k, omega_k_plus_1)\n            \n            # Compute the incremental rotation and update the orientation\n            delta_R_k = exp_so3(delta_v_k)\n            R = np.dot(R, delta_R_k)\n            \n        return R\n\n    def frobenius_norm_diff(A, B):\n        \"\"\"\n        Computes the Frobenius norm of the difference between two matrices.\n        \"\"\"\n        return np.linalg.norm(A - B, 'fro')\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"omega_func\": lambda t: np.array([0.0, 0.0, 0.0]),\n            \"T\": 1.0, \"N\": 10,\n            \"ref_func\": lambda T, N: np.identity(3)\n        },\n        {\n            \"omega_func\": lambda t: np.array([0.0, 0.0, 1.0]),\n            \"T\": 1.0, \"N\": 40,\n            \"ref_func\": lambda T, N: exp_so3(np.array([0.0, 0.0, 1.0]) * T)\n        },\n        {\n            \"omega_func\": lambda t: np.array([2.0 * t, 0.0, 0.0]),\n            \"T\": 1.0, \"N\": 100,\n            \"ref_func\": lambda T, N: exp_so3(np.array([0.5 * 2.0 * T**2, 0.0, 0.0]))\n        },\n        {\n            \"omega_func\": lambda t: np.array([np.sin(t), np.cos(t), 0.0]),\n            \"T\": 2.0 * np.pi, \"N\": 400,\n            \"ref_func\": lambda T, N: run_integration(lambda t: np.array([np.sin(t), np.cos(t), 0.0]), T, 20000)\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        # Run the integrator for the current test case\n        R_T = run_integration(case[\"omega_func\"], case[\"T\"], case[\"N\"])\n        \n        # Compute the reference solution\n        R_ref = case[\"ref_func\"](case[\"T\"], case[\"N\"])\n        \n        # Calculate the Frobenius norm of the error\n        error = frobenius_norm_diff(R_T, R_ref)\n        results.append(error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}