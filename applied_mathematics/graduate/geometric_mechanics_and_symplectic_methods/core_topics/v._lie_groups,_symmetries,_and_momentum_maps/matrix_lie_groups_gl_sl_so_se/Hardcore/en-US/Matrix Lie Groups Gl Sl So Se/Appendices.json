{
    "hands_on_practices": [
        {
            "introduction": "The exponential map is the primary bridge connecting a Lie algebra, the space of infinitesimal transformations, to its corresponding Lie group of finite transformations. This first exercise provides a concrete entry point by exploring this map for the special orthogonal group $SO(2)$, the group of planar rotations. By explicitly calculating the exponential of an element from the Lie algebra $\\mathfrak{so}(2)$, you will see how the abstract infinite series definition wonderfully simplifies to the familiar 2D rotation matrix, and you will prove that any rotation can be generated this way . This practice is fundamental for building intuition on how constant angular velocities (elements of the algebra) generate finite rotations (elements of the group).",
            "id": "3755442",
            "problem": "Let $SO(2)$ denote the Special Orthogonal group (SO), defined by $SO(2)=\\{R\\in GL(2,\\mathbb{R}) \\mid R^{\\top}R=I,\\ \\det(R)=1\\}$, and let $\\mathfrak{so}(2)$ denote its Lie algebra, defined by $\\mathfrak{so}(2)=\\{X\\in \\mathbb{R}^{2\\times 2} \\mid X^{\\top}=-X\\}$. The matrix exponential of $X\\in \\mathfrak{so}(2)$ is defined by $\\exp(X)=\\sum_{k=0}^{\\infty}\\frac{1}{k!}X^{k}$. In planar geometric mechanics, a constant angular velocity is represented by a skew-symmetric generator $X\\in\\mathfrak{so}(2)$, and its finite rotation after unit time is given by $\\exp(X)$.\n\nUsing only the definitions above and standard linear algebra facts, do the following:\n- Prove that the exponential map $\\exp:\\mathfrak{so}(2)\\to SO(2)$ is surjective, namely that for every $R\\in SO(2)$ there exists $X\\in\\mathfrak{so}(2)$ with $R=\\exp(X)$.\n- Let $J\\in\\mathfrak{so}(2)$ be the skew-symmetric generator defined by $J=\\begin{pmatrix}0 & -1\\\\ 1 & 0\\end{pmatrix}$, and let $\\theta\\in\\mathbb{R}$. Compute the explicit closed-form expression for the matrix exponential $\\exp(\\theta J)$.\n\nYour final answer must be the single closed-form expression for $\\exp(\\theta J)$, simplified as far as possible. No numerical approximation is required.",
            "solution": "The problem statement is first validated for correctness and solvability.\n\n### Step 1: Extract Givens\n- Definition of the Special Orthogonal group $SO(2)$: $SO(2)=\\{R\\in GL(2,\\mathbb{R}) \\mid R^{\\top}R=I,\\ \\det(R)=1\\}$.\n- Definition of its Lie algebra $\\mathfrak{so}(2)$: $\\mathfrak{so}(2)=\\{X\\in \\mathbb{R}^{2\\times 2} \\mid X^{\\top}=-X\\}$.\n- Definition of the matrix exponential: $\\exp(X)=\\sum_{k=0}^{\\infty}\\frac{1}{k!}X^{k}$ for $X\\in \\mathfrak{so}(2)$.\n- Definition of the skew-symmetric generator $J$: $J=\\begin{pmatrix}0 & -1\\\\ 1 & 0\\end{pmatrix}$.\n- Parameter $\\theta \\in \\mathbb{R}$.\n- Task 1: Prove that the exponential map $\\exp:\\mathfrak{so}(2)\\to SO(2)$ is surjective.\n- Task 2: Compute the explicit closed-form expression for $\\exp(\\theta J)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, as the definitions of $SO(2)$, $\\mathfrak{so}(2)$, and the matrix exponential are standard in the theory of Lie groups and their application in geometric mechanics. The problem is well-posed, with clear definitions and objectives leading to a unique solution for the calculation and a standard proof for the surjectivity. The terminology is precise and objective. There are no contradictions, missing information, or unrealistic premises. The problem is a fundamental exercise in its field and is mathematically verifiable.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete, reasoned solution will be provided.\n\n### Solution\nThe solution will proceed in two main parts, as requested by the problem statement. First, we will compute the explicit expression for $\\exp(\\theta J)$, as this result is instrumental for the second part, which is the proof of surjectivity.\n\n**Part 1: Computation of $\\exp(\\theta J)$**\n\nWe are asked to find a closed-form expression for the matrix exponential $\\exp(\\theta J)$, where $J = \\begin{pmatrix} 0 & -1 \\\\ 1 & 0 \\end{pmatrix}$ and $\\theta \\in \\mathbb{R}$. The matrix $X = \\theta J = \\begin{pmatrix} 0 & -\\theta \\\\ \\theta & 0 \\end{pmatrix}$ is an element of $\\mathfrak{so}(2)$, since $(\\theta J)^{\\top} = \\theta J^{\\top} = \\theta \\begin{pmatrix} 0 & 1 \\\\ -1 & 0 \\end{pmatrix} = -\\theta J$.\n\nThe matrix exponential is defined by the power series:\n$$\n\\exp(\\theta J) = \\sum_{k=0}^{\\infty} \\frac{1}{k!}(\\theta J)^k = \\sum_{k=0}^{\\infty} \\frac{\\theta^k}{k!} J^k\n$$\nTo evaluate this series, we first investigate the powers of the matrix $J$:\n$J^0 = I = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$\n$J^1 = J = \\begin{pmatrix} 0 & -1 \\\\ 1 & 0 \\end{pmatrix}$\n$J^2 = J J = \\begin{pmatrix} 0 & -1 \\\\ 1 & 0 \\end{pmatrix} \\begin{pmatrix} 0 & -1 \\\\ 1 & 0 \\end{pmatrix} = \\begin{pmatrix} -1 & 0 \\\\ 0 & -1 \\end{pmatrix} = -I$\n$J^3 = J^2 J = (-I)J = -J$\n$J^4 = J^2 J^2 = (-I)(-I) = I$\n\nThe powers of $J$ are periodic with a period of $4$: $\\{I, J, -I, -J, \\dots\\}$. We can now expand the series for $\\exp(\\theta J)$:\n$$\n\\exp(\\theta J) = \\frac{\\theta^0}{0!}J^0 + \\frac{\\theta^1}{1!}J^1 + \\frac{\\theta^2}{2!}J^2 + \\frac{\\theta^3}{3!}J^3 + \\frac{\\theta^4}{4!}J^4 + \\dots\n$$\nSubstituting the powers of $J$:\n$$\n\\exp(\\theta J) = I + \\theta J - \\frac{\\theta^2}{2!}I - \\frac{\\theta^3}{3!}J + \\frac{\\theta^4}{4!}I + \\dots\n$$\nWe can group the terms associated with the identity matrix $I$ and the terms associated with the matrix $J$:\n$$\n\\exp(\\theta J) = \\left(1 - \\frac{\\theta^2}{2!} + \\frac{\\theta^4}{4!} - \\dots\\right)I + \\left(\\theta - \\frac{\\theta^3}{3!} + \\frac{\\theta^5}{5!} - \\dots\\right)J\n$$\nThe two series in parentheses are the Maclaurin series for the cosine and sine functions, respectively:\n$\\cos(\\theta) = \\sum_{k=0}^{\\infty} \\frac{(-1)^k \\theta^{2k}}{(2k)!} = 1 - \\frac{\\theta^2}{2!} + \\frac{\\theta^4}{4!} - \\dots$\n$\\sin(\\theta) = \\sum_{k=0}^{\\infty} \\frac{(-1)^k \\theta^{2k+1}}{(2k+1)!} = \\theta - \\frac{\\theta^3}{3!} + \\frac{\\theta^5}{5!} - \\dots$\n\nTherefore, we can write the expression for $\\exp(\\theta J)$ in the compact form:\n$$\n\\exp(\\theta J) = \\cos(\\theta)I + \\sin(\\theta)J\n$$\nSubstituting the matrix representations for $I$ and $J$:\n$$\n\\exp(\\theta J) = \\cos(\\theta)\\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} + \\sin(\\theta)\\begin{pmatrix} 0 & -1 \\\\ 1 & 0 \\end{pmatrix} = \\begin{pmatrix} \\cos(\\theta) & 0 \\\\ 0 & \\cos(\\theta) \\end{pmatrix} + \\begin{pmatrix} 0 & -\\sin(\\theta) \\\\ \\sin(\\theta) & 0 \\end{pmatrix}\n$$\nThis simplifies to the final closed-form expression:\n$$\n\\exp(\\theta J) = \\begin{pmatrix} \\cos(\\theta) & -\\sin(\\theta) \\\\ \\sin(\\theta) & \\cos(\\theta) \\end{pmatrix}\n$$\nThis completes the second task of the problem.\n\n**Part 2: Proof of Surjectivity**\n\nWe must prove that the exponential map $\\exp: \\mathfrak{so}(2) \\to SO(2)$ is surjective. This means that for any matrix $R \\in SO(2)$, there exists a matrix $X \\in \\mathfrak{so}(2)$ such that $\\exp(X) = R$.\n\nFirst, let's characterize a general element $R \\in SO(2)$. Let $R = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$. The defining properties are $R^{\\top}R=I$ and $\\det(R)=1$. The first property implies:\n$$\nR^{\\top}R = \\begin{pmatrix} a & c \\\\ b & d \\end{pmatrix}\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix} = \\begin{pmatrix} a^2+c^2 & ab+cd \\\\ ab+cd & b^2+d^2 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}\n$$\nThis gives the conditions: $a^2+c^2=1$, $b^2+d^2=1$, and $ab+cd=0$.\nThe condition $a^2+c^2=1$ allows us to parameterize $a$ and $c$ by an angle $\\phi \\in \\mathbb{R}$, such that $a = \\cos(\\phi)$ and $c = \\sin(\\phi)$.\nThe condition $\\det(R)=ad-bc=1$ becomes $d\\cos(\\phi) - b\\sin(\\phi)=1$.\nThe condition $ab+cd=0$ becomes $b\\cos(\\phi) + d\\sin(\\phi)=0$.\nWe have a system of two linear equations in $b$ and $d$:\n1) $d\\cos(\\phi) - b\\sin(\\phi)=1$\n2) $d\\sin(\\phi) + b\\cos(\\phi)=0$\nMultiplying (1) by $\\cos(\\phi)$ and (2) by $\\sin(\\phi)$ and adding them gives:\n$d(\\cos^2(\\phi)+\\sin^2(\\phi)) = \\cos(\\phi) \\implies d=\\cos(\\phi)$.\nMultiplying (1) by $-\\sin(\\phi)$ and (2) by $\\cos(\\phi)$ and adding them gives:\n$b(\\sin^2(\\phi)+\\cos^2(\\phi)) = -\\sin(\\phi) \\implies b=-\\sin(\\phi)$.\nThus, any matrix $R \\in SO(2)$ can be written in the form of a standard rotation matrix:\n$$\nR = \\begin{pmatrix} \\cos(\\phi) & -\\sin(\\phi) \\\\ \\sin(\\phi) & \\cos(\\phi) \\end{pmatrix}\n$$\nfor some angle $\\phi \\in \\mathbb{R}$.\n\nNow, we need to find an $X \\in \\mathfrak{so}(2)$ such that $\\exp(X)=R$. A general element $X \\in \\mathfrak{so}(2)$ has the form $X = \\theta J$ for some $\\theta \\in \\mathbb{R}$.\nFrom Part 1, we have the expression for $\\exp(\\theta J)$:\n$$\n\\exp(\\theta J) = \\begin{pmatrix} \\cos(\\theta) & -\\sin(\\theta) \\\\ \\sin(\\theta) & \\cos(\\theta) \\end{pmatrix}\n$$\nTo achieve $\\exp(X) = R$, we must equate the two matrices:\n$$\n\\begin{pmatrix} \\cos(\\theta) & -\\sin(\\theta) \\\\ \\sin(\\theta) & \\cos(\\theta) \\end{pmatrix} = \\begin{pmatrix} \\cos(\\phi) & -\\sin(\\phi) \\\\ \\sin(\\phi) & \\cos(\\phi) \\end{pmatrix}\n$$\nBy inspection, we can satisfy this equality by choosing $\\theta = \\phi$.\nTherefore, for any given $R \\in SO(2)$, which corresponds to a rotation by an angle $\\phi$, we can choose the matrix $X = \\phi J \\in \\mathfrak{so}(2)$. The exponential of this $X$ is\n$$\n\\exp(X) = \\exp(\\phi J) = \\begin{pmatrix} \\cos(\\phi) & -\\sin(\\phi) \\\\ \\sin(\\phi) & \\cos(\\phi) \\end{pmatrix} = R\n$$\nSince for every element $R$ in the codomain $SO(2)$ we have found a pre-image $X$ in the domain $\\mathfrak{so}(2)$, the map $\\exp: \\mathfrak{so}(2) \\to SO(2)$ is surjective. This concludes the proof.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\cos(\\theta) & -\\sin(\\theta) \\\\ \\sin(\\theta) & \\cos(\\theta) \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Having explored the local picture of generating group elements, we now turn to the global geometric structure of Lie groups. This exercise investigates the special orthogonal group $SO(n)$ as a smooth manifold embedded within the larger space of all invertible matrices, $GL(n)$ . By treating the orthogonality condition $Q^{\\top}Q = I$ as a set of constraint equations, you will use tools from differential geometry to rigorously determine the dimension of $SO(n)$. This practice is invaluable for understanding Lie groups as configuration spaces and for developing the skill to analyze their geometric properties, a cornerstone of geometric mechanics.",
            "id": "3755409",
            "problem": "Let $GL(n)$ denote the General Linear Group (GL($n$)) of real invertible $n \\times n$ matrices and let $SO(n)$ denote the Special Orthogonal Group (SO($n$)), defined as the set of $Q \\in GL(n)$ satisfying $Q^{\\top}Q = I$ and $\\det(Q) = 1$. Consider the smooth map $F: GL(n) \\to \\operatorname{Sym}(n)$, where $\\operatorname{Sym}(n)$ denotes the vector space of real symmetric $n \\times n$ matrices, given by $F(Q) = Q^{\\top}Q - I$. Using first principles appropriate to geometric mechanics and matrix Lie groups (including the definition of tangent spaces via matrix curves and the derivative of a smooth matrix-valued map), analyze the constraints imposed by $Q^{\\top}Q = I$ to determine the codimension of $SO(n)$ in $GL(n)$. Justify that these constraints are independent by computing the derivative $DF_{Q}$ at a point $Q \\in SO(n)$ and showing its range coincides with $\\operatorname{Sym}(n)$. From this codimension, verify the dimension formula for $SO(n)$, namely $n(n-1)/2$. Provide the codimension as a single closed-form expression in terms of $n$. No numerical approximation is required.",
            "solution": "The problem requires us to determine the codimension of the Special Orthogonal Group, $SO(n)$, within the General Linear Group, $GL(n)$, by analyzing the constraints that define it. The group $GL(n)$ is the set of all real invertible $n \\times n$ matrices. It is an open subset of the vector space $M_n(\\mathbb{R})$ of all $n \\times n$ real matrices, and thus is a manifold of dimension $\\dim(GL(n)) = n^2$.\n\nThe Special Orthogonal Group $SO(n)$ is defined as the set of matrices $Q \\in GL(n)$ that satisfy two conditions:\n$1.$ $Q^{\\top}Q = I$ (orthogonality)\n$2.$ $\\det(Q) = 1$ (special condition)\n\nThe problem directs us to analyze the constraint $Q^{\\top}Q = I$. We define a map $F: GL(n) \\to \\operatorname{Sym}(n)$, where $\\operatorname{Sym}(n)$ is the vector space of real symmetric $n \\times n$ matrices, by\n$$F(Q) = Q^{\\top}Q - I$$\nThe set of matrices satisfying the orthogonality condition is the Orthogonal Group $O(n)$, which is the level set of $F$ corresponding to the zero matrix $0 \\in \\operatorname{Sym}(n)$. That is, $O(n) = F^{-1}(0)$.\n\nThe group $SO(n)$ is a subset of $O(n)$ satisfying the additional condition $\\det(Q) = 1$. The determinant of an orthogonal matrix is always $\\pm 1$. The set $O(n)$ consists of two connected components, one with determinant $1$ (which is $SO(n)$) and one with determinant $-1$. As $SO(n)$ is an open submanifold of $O(n)$, they share the same dimension: $\\dim(SO(n)) = \\dim(O(n))$. Therefore, the codimension of $SO(n)$ in $GL(n)$ is the same as the codimension of $O(n)$ in $GL(n)$.\n\nWe can determine this codimension using the regular value theorem. If $0 \\in \\operatorname{Sym}(n)$ is a regular value of the map $F$, then its preimage $F^{-1}(0) = O(n)$ is a submanifold of $GL(n)$ whose codimension is equal to the dimension of the target space, $\\operatorname{Sym}(n)$. A value is regular if the derivative of the map is surjective at every point in the preimage.\n\nLet's compute the derivative of $F$ at a point $Q \\in O(n)$. The derivative, or differential, $DF_Q$, is a linear map from the tangent space of the domain at $Q$ to the tangent space of the codomain at $F(Q)$.\nThe domain is $GL(n)$, an open set in $M_n(\\mathbb{R})$, so its tangent space at any point $Q$, $T_Q GL(n)$, can be identified with $M_n(\\mathbb{R})$.\nThe codomain is the vector space $\\operatorname{Sym}(n)$, so its tangent space at any point, including the point $F(Q) = 0$, can be identified with $\\operatorname{Sym}(n)$ itself. Thus, $DF_Q: M_n(\\mathbb{R}) \\to \\operatorname{Sym}(n)$.\n\nTo find the action of $DF_Q$, we consider a smooth curve $\\gamma(t)$ in $GL(n)$ with $\\gamma(0) = Q$ and velocity $\\gamma'(0) = V$, where $V \\in M_n(\\mathbb{R})$ is a tangent vector. The derivative of $F$ at $Q$ applied to $V$ is:\n$$DF_Q(V) = \\frac{d}{dt}\\bigg|_{t=0} F(\\gamma(t)) = \\frac{d}{dt}\\bigg|_{t=0} (\\gamma(t)^{\\top}\\gamma(t) - I)$$\nUsing the product rule for matrix differentiation:\n$$DF_Q(V) = \\gamma'(0)^{\\top}\\gamma(0) + \\gamma(0)^{\\top}\\gamma'(0) = V^{\\top}Q + Q^{\\top}V$$\nWe need to show that this map $DF_Q: V \\mapsto V^{\\top}Q + Q^{\\top}V$ is surjective for any $Q \\in O(n)$. This means that for any symmetric matrix $S \\in \\operatorname{Sym}(n)$, we must find a matrix $V \\in M_n(\\mathbb{R})$ such that $V^{\\top}Q + Q^{\\top}V = S$.\n\nSince $Q \\in O(n)$, it is invertible. Any tangent vector $V \\in M_n(\\mathbb{R})$ can be expressed as $V = QX$ for some unique $X \\in M_n(\\mathbb{R})$. Substituting this into the expression for the derivative:\n$$DF_Q(QX) = (QX)^{\\top}Q + Q^{\\top}(QX) = X^{\\top}Q^{\\top}Q + Q^{\\top}QX$$\nSince $Q \\in O(n)$, we have $Q^{\\top}Q = I$. The expression simplifies to:\n$$DF_Q(QX) = X^{\\top}I + IX = X^{\\top} + X$$\nThe problem is now reduced to showing that for any symmetric matrix $S \\in \\operatorname{Sym}(n)$, there exists a matrix $X \\in M_n(\\mathbb{R})$ such that $X^{\\top} + X = S$.\nLet's choose $X = \\frac{1}{2}S$. Since $S$ is symmetric, $S^{\\top} = S$. Then,\n$$X^{\\top} + X = \\left(\\frac{1}{2}S\\right)^{\\top} + \\frac{1}{2}S = \\frac{1}{2}S^{\\top} + \\frac{1}{2}S = \\frac{1}{2}S + \\frac{1}{2}S = S$$\nThis demonstrates that for any $S \\in \\operatorname{Sym}(n)$, we can find a matrix $X = \\frac{1}{2}S$, which corresponds to the tangent vector $V = Q(\\frac{1}{2}S)$, such that $DF_Q(V) = S$. Thus, the derivative $DF_Q$ is surjective for all $Q \\in O(n)$.\nThe surjectivity of the derivative implies that the constraints defined by the equations in $Q^{\\top}Q - I = 0$ are independent.\n\nBy the regular value theorem, the codimension of $O(n)$ (and thus $SO(n)$) in $GL(n)$ is equal to the dimension of the target space $\\operatorname{Sym}(n)$. A symmetric $n \\times n$ matrix is determined by its entries on and above the main diagonal. There are $n$ entries on the diagonal and $\\binom{n}{2} = \\frac{n(n-1)}{2}$ entries strictly above the diagonal.\nTherefore, the dimension of $\\operatorname{Sym}(n)$ is:\n$$\\dim(\\operatorname{Sym}(n)) = n + \\frac{n(n-1)}{2} = \\frac{2n + n^2 - n}{2} = \\frac{n^2 + n}{2} = \\frac{n(n+1)}{2}$$\nThis is the codimension of $SO(n)$ in $GL(n)$.\n\nFinally, we verify the dimension formula for $SO(n)$. The dimension of a submanifold is the dimension of the ambient manifold minus its codimension.\n$$\\dim(SO(n)) = \\dim(GL(n)) - \\text{codim}(SO(n))$$\n$$\\dim(SO(n)) = n^2 - \\frac{n(n+1)}{2} = \\frac{2n^2 - (n^2+n)}{2} = \\frac{n^2 - n}{2} = \\frac{n(n-1)}{2}$$\nThis confirms the dimension formula given in the problem statement. The codimension of $SO(n)$ in $GL(n)$ is $\\frac{n(n+1)}{2}$.",
            "answer": "$$\\boxed{\\frac{n(n+1)}{2}}$$"
        },
        {
            "introduction": "In many practical applications, such as robotics or computational physics, numerical errors or physical measurements can yield a matrix that is only approximately a rotation. This creates the need for a systematic way to find the \"best\" valid rotation corresponding to such a matrix. This exercise tackles this important projection problem by asking you to find the closest matrix in $SO(3)$ to a given matrix in the Frobenius norm sense . The solution elegantly employs the polar decomposition and Singular Value Decomposition (SVD), demonstrating a powerful connection between Lie group theory and established linear algebra techniques to solve a concrete optimization problem.",
            "id": "3755437",
            "problem": "Let $A \\in \\mathrm{GL}(3)$ be the matrix\n$$\nA \\;=\\; R S \\,, \\quad R \\;=\\; \\mathrm{diag}(1,\\,1,\\,-1) \\,, \\quad S \\;=\\; \\mathrm{diag}(2,\\,1.5,\\,0.6) \\,,\n$$\nso that $A = \\mathrm{diag}(2,\\,1.5,\\,-0.6)$ and $\\det(A) < 0$. Here $\\mathrm{GL}(n)$ denotes the General Linear group of invertible $n \\times n$ matrices. Consider the problem of finding the nearest matrix in the Special Orthogonal group $\\mathrm{SO}(3)$ (the group of $3 \\times 3$ real orthogonal matrices with determinant $+1$) to $A$ in the Frobenius norm $\\|\\cdot\\|_{F}$.\n\nStarting from the fundamental definition of the polar decomposition (for any $A \\in \\mathrm{GL}(n)$ there exists a unique factorization $A = Q S$ with $Q \\in \\mathrm{O}(n)$ and $S$ symmetric positive-definite, where $\\mathrm{O}(n)$ is the Orthogonal group), perform the following steps:\n\n1. Compute the orthogonal polar factor $Q$ and the positive-definite factor $S$ of $A$ via the polar decomposition, using only the definition of the polar decomposition and properties of the Frobenius norm.\n\n2. Using the computed polar factors and the Singular Value Decomposition (SVD), which writes $A = U \\Sigma V^{\\top}$ with $U, V \\in \\mathrm{O}(n)$ and $\\Sigma$ diagonal with positive entries, determine a candidate $\\widehat{Q} \\in \\mathrm{SO}(3)$ that minimizes $\\|A - Q\\|_{F}$ over $Q \\in \\mathrm{SO}(3)$.\n\n3. Verify the optimality of your $\\widehat{Q}$ by checking first-order necessary conditions for constrained minimization on $\\mathrm{SO}(3)$: derive the stationarity condition using a Lagrangian with the constraint $Q^{\\top} Q = I$ and argue that the directional derivative of the objective $\\|A - Q\\|_{F}^{2}$ at $\\widehat{Q}$ vanishes for all variations in the tangent space $T_{\\widehat{Q}} \\mathrm{SO}(3)$.\n\nFinally, compute the minimal squared Frobenius distance\n$$\n\\min_{Q \\in \\mathrm{SO}(3)} \\|A - Q\\|_{F}^{2}\n$$\nfor the given $A$. Express your final scalar answer as an exact value with no rounding.",
            "solution": "The problem asks for the nearest matrix in the Special Orthogonal group $\\mathrm{SO}(3)$ to a given matrix $A \\in \\mathrm{GL}(3)$, where the distance is measured by the Frobenius norm $\\|\\cdot\\|_F$. The matrix is given by $A = \\mathrm{diag}(2, 1.5, -0.6)$. We are asked to solve this problem by following a series of specified steps.\n\n1.  **Computation of the Polar Decomposition**\n\nThe problem asks for the polar decomposition of $A$, which for an invertible matrix $A \\in \\mathrm{GL}(n)$, is the unique factorization $A = QP$ where $Q \\in \\mathrm{O}(n)$ is an orthogonal matrix and $P$ is a symmetric positive-definite matrix.\n\nThe matrix $P$ is defined as the unique positive-definite square root of $A^T A$, i.e., $P = \\sqrt{A^T A}$.\nThe given matrix is $A = \\mathrm{diag}(2, 1.5, -0.6)$.\nSince $A$ is a diagonal matrix, it is symmetric, so $A^T = A$.\nWe compute $A^T A$:\n$$\nA^T A = A^2 = \\mathrm{diag}(2^2, 1.5^2, (-0.6)^2) = \\mathrm{diag}(4, 2.25, 0.36)\n$$\nThe matrix $P$ is the square root of $A^T A$. Since $A^T A$ is a diagonal matrix with positive entries, its square root is found by taking the square root of each diagonal entry:\n$$\nP = \\sqrt{A^T A} = \\mathrm{diag}(\\sqrt{4}, \\sqrt{2.25}, \\sqrt{0.36}) = \\mathrm{diag}(2, 1.5, 0.6)\n$$\nThis matrix is symmetric, and its eigenvalues (the diagonal entries $2$, $1.5$, $0.6$) are all positive, so it is positive-definite. This matrix $P$ corresponds to the matrix $S$ given in the problem statement.\n\nThe orthogonal factor $Q$ can then be computed from $A = QP$. Since $P$ is invertible, we have $Q = AP^{-1}$.\nThe inverse of $P$ is:\n$$\nP^{-1} = \\mathrm{diag}(2^{-1}, 1.5^{-1}, 0.6^{-1}) = \\mathrm{diag}(\\frac{1}{2}, \\frac{1}{1.5}, \\frac{1}{0.6}) = \\mathrm{diag}(\\frac{1}{2}, \\frac{2}{3}, \\frac{5}{3})\n$$\nNow we compute $Q$:\n$$\nQ = AP^{-1} = \\mathrm{diag}(2, 1.5, -0.6) \\mathrm{diag}(\\frac{1}{2}, \\frac{2}{3}, \\frac{5}{3}) = \\mathrm{diag}(2 \\cdot \\frac{1}{2}, 1.5 \\cdot \\frac{2}{3}, -0.6 \\cdot \\frac{5}{3}) = \\mathrm{diag}(1, 1, -1)\n$$\nThis matrix $Q$ is orthogonal since $Q^T Q = \\mathrm{diag}(1, 1, -1) \\mathrm{diag}(1, 1, -1) = \\mathrm{diag}(1, 1, 1) = I$. This matrix $Q$ corresponds to the matrix $R$ in the problem statement.\nThus, the polar factors of $A$ are $Q = \\mathrm{diag}(1, 1, -1)$ and $P = \\mathrm{diag(2, 1.5, 0.6)}$.\n\n2.  **Determining the Candidate Matrix $\\widehat{Q} \\in \\mathrm{SO}(3)$**\n\nThe problem of finding the nearest matrix in $\\mathrm{SO}(3)$ to $A$ is equivalent to minimizing $\\|A - \\widehat{Q}\\|_F^2$ for $\\widehat{Q} \\in \\mathrm{SO}(3)$. We expand the squared norm:\n$$\n\\|A - \\widehat{Q}\\|_F^2 = \\mathrm{Tr}((A - \\widehat{Q})^T (A - \\widehat{Q})) = \\mathrm{Tr}(A^T A - A^T \\widehat{Q} - \\widehat{Q}^T A + \\widehat{Q}^T \\widehat{Q})\n$$\nSince $\\widehat{Q} \\in \\mathrm{SO}(3)$, it is orthogonal, so $\\widehat{Q}^T \\widehat{Q} = I$. The trace is $\\mathrm{Tr}(I)=3$. Also, $\\mathrm{Tr}(\\widehat{Q}^T A) = \\mathrm{Tr}(A^T \\widehat{Q})$. The expression becomes:\n$$\n\\|A - \\widehat{Q}\\|_F^2 = \\|A\\|_F^2 - 2 \\mathrm{Tr}(A^T \\widehat{Q}) + 3\n$$\nMinimizing this quantity is equivalent to maximizing $\\mathrm{Tr}(A^T \\widehat{Q})$.\n\nUsing the Singular Value Decomposition (SVD) of $A = U \\Sigma V^T$, where $U, V \\in \\mathrm{O}(3)$ and $\\Sigma$ is a diagonal matrix of singular values, the nearest matrix in $\\mathrm{SO}(3)$ is given by the formula $\\widehat{Q} = U J V^T$, with $J = \\mathrm{diag}(1, 1, \\det(UV^T))$.\n\nLet's find the SVD of $A = \\mathrm{diag}(2, 1.5, -0.6)$. The singular values $(\\sigma_i)$ are the square roots of the eigenvalues of $A^T A$. We have already computed $A^T A = \\mathrm{diag}(4, 2.25, 0.36)$. The singular values are $\\sigma_1=2$, $\\sigma_2=1.5$, $\\sigma_3=0.6$. The singular value matrix is $\\Sigma = \\mathrm{diag}(2, 1.5, 0.6)$.\nThe matrix $V$ contains the eigenvectors of $A^T A$. Since $A^T A$ is diagonal, its eigenvectors are the standard basis vectors, so we can set $V=I$.\nThe matrix $U$ is determined by the relation $A = U \\Sigma V^T$. With $V=I$, this becomes $A = U \\Sigma$.\n$$\n\\mathrm{diag}(2, 1.5, -0.6) = U \\mathrm{diag}(2, 1.5, 0.6)\n$$\nThis implies $U = \\mathrm{diag}(2/2, 1.5/1.5, -0.6/0.6) = \\mathrm{diag}(1, 1, -1)$.\nThe matrices are $U = \\mathrm{diag}(1, 1, -1)$, $\\Sigma = \\mathrm{diag}(2, 1.5, 0.6)$, and $V=I$. We note that $U$ and $V$ are indeed orthogonal.\n\nFrom the SVD, we can identify the polar factors: $Q_{polar} = UV^T = \\mathrm{diag}(1,1,-1)I^T = \\mathrm{diag}(1,1,-1)$ and $P_{polar} = V \\Sigma V^T = I \\Sigma I^T = \\Sigma$, which match our results from Part 1.\n\nTo find the closest matrix in $\\mathrm{SO}(3)$, we compute $\\det(UV^T) = \\det(U)\\det(V^T) = (-1)(1) = -1$.\nThen $J = \\mathrm{diag}(1, 1, \\det(UV^T)) = \\mathrm{diag}(1, 1, -1)$.\nThe candidate matrix $\\widehat{Q}$ is:\n$$\n\\widehat{Q} = U J V^T = \\mathrm{diag}(1, 1, -1) \\mathrm{diag}(1, 1, -1) I^T = \\mathrm{diag}(1, 1, 1) = I\n$$\nThe identity matrix $I$ is in $\\mathrm{SO}(3)$ since $I^T I = I$ and $\\det(I)=1$. Thus, our candidate is $\\widehat{Q}=I$.\n\n3.  **Verification of Optimality**\n\nTo verify that $\\widehat{Q}=I$ is an optimal solution, we check the first-order necessary conditions for constrained optimization. A point $Q \\in \\mathrm{SO}(3)$ is a stationary point for the function $f(Q) = \\|A-Q\\|_F^2$ if the gradient of $f$ is orthogonal to the tangent space of $\\mathrm{SO}(3)$ at $Q$. This is equivalent to the directional derivative of $f$ being zero for all directions in the tangent space.\n\nThe tangent space to $\\mathrm{SO}(3)$ at $\\widehat{Q}=I$ is the Lie algebra $\\mathfrak{so}(3)$, which is the space of all $3 \\times 3$ real skew-symmetric matrices. Let $X \\in \\mathfrak{so}(3)$ be an arbitrary tangent vector.\nA curve on $\\mathrm{SO}(3)$ passing through $I$ with velocity $X$ is given by $\\gamma(t) = \\exp(tX)$. The directional derivative of $f(Q) = \\|A\\|_F^2 - 2 \\mathrm{Tr}(A^T Q) + 3$ at $Q=I$ along the direction $X$ is:\n$$\n\\frac{d}{dt} f(\\exp(tX)) \\Big|_{t=0} = \\frac{d}{dt} \\left( \\|A\\|_F^2 - 2\\mathrm{Tr}(A^T \\exp(tX)) + 3 \\right) \\Big|_{t=0}\n$$\n$$\n= -2 \\mathrm{Tr}\\left(A^T \\frac{d}{dt}\\exp(tX)\\Big|_{t=0}\\right) = -2 \\mathrm{Tr}(A^T X)\n$$\nThe first-order condition for optimality is that this derivative is zero for all $X \\in \\mathfrak{so}(3)$.\nThe given matrix $A = \\mathrm{diag}(2, 1.5, -0.6)$ is symmetric, so $A^T=A$. The condition becomes $\\mathrm{Tr}(AX) = 0$ for all skew-symmetric matrices $X$.\nFor any symmetric matrix $S$ and any skew-symmetric matrix $K$, their trace product is zero:\n$\\mathrm{Tr}(SK) = \\mathrm{Tr}((SK)^T) = \\mathrm{Tr}(K^T S^T)$. Since $S^T=S$ and $K^T=-K$, this is $\\mathrm{Tr}(-KS) = -\\mathrm{Tr}(KS)$. The cyclic property of the trace gives $\\mathrm{Tr}(KS)=\\mathrm{Tr}(SK)$, so we have $\\mathrm{Tr}(SK) = -\\mathrm{Tr}(SK)$, which implies $2\\mathrm{Tr}(SK)=0$, or $\\mathrm{Tr}(SK)=0$.\nSince our matrix $A$ is symmetric and any $X \\in \\mathfrak{so}(3)$ is skew-symmetric, the condition $\\mathrm{Tr}(AX) = 0$ is satisfied. This verifies that $\\widehat{Q}=I$ is a stationary point.\nThe problem of finding the nearest matrix in $\\mathrm{SO}(n)$ to a matrix $A$ with distinct singular values has a unique global solution. Since our candidate satisfies the first-order necessary conditions, it is the optimal solution.\n\n**Final Calculation**\n\nThe minimal squared Frobenius distance is $\\|A - \\widehat{Q}\\|_F^2$, where $\\widehat{Q} = I$.\nWe calculate $\\|A - I\\|_F^2$:\n$$\nA - I = \\mathrm{diag}(2, 1.5, -0.6) - \\mathrm{diag}(1, 1, 1) = \\mathrm{diag}(2-1, 1.5-1, -0.6-1) = \\mathrm{diag}(1, 0.5, -1.6)\n$$\nThe squared Frobenius norm of a diagonal matrix is the sum of the squares of its diagonal elements:\n$$\n\\|A-I\\|_F^2 = 1^2 + (0.5)^2 + (-1.6)^2 = 1 + 0.25 + 2.56 = 3.81\n$$\nThis is an exact value.\nAlternatively, using the expression for the squared norm:\n$\\|A-\\widehat{Q}\\|_F^2 = \\|A\\|_F^2 - 2\\mathrm{Tr}(A^T \\widehat{Q}) + 3$.\n$\\|A\\|_F^2 = 2^2 + 1.5^2 + (-0.6)^2 = 4 + 2.25 + 0.36 = 6.61$.\n$\\mathrm{Tr}(A^T \\widehat{Q}) = \\mathrm{Tr}(A^T I) = \\mathrm{Tr}(A) = 2 + 1.5 - 0.6 = 2.9$.\nSo, the minimal squared distance is $6.61 - 2(2.9) + 3 = 6.61 - 5.8 + 3 = 3.81$.\nThe results are consistent.",
            "answer": "$$\n\\boxed{3.81}\n$$"
        }
    ]
}