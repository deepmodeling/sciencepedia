{"hands_on_practices": [{"introduction": "The Projection Theorem provides the theoretical foundation for finding the \"best approximation\" of a vector from a subspace. This first practice provides a concrete exercise in computing an orthogonal projection in the infinite-dimensional Hilbert space $\\ell^2$ [@problem_id:1886651]. By applying the standard projection formula, you will find the closest point in a simple one-dimensional subspace to a given vector, building a foundational skill for more complex problems.", "problem": "Consider the real Hilbert space $\\ell^2$, which consists of all infinite sequences of real numbers $x = (x_1, x_2, x_3, \\dots)$ such that the series $\\sum_{n=1}^{\\infty} x_n^2$ converges. The space is equipped with the standard inner product $\\langle x, y \\rangle = \\sum_{n=1}^{\\infty} x_n y_n$, and the induced norm is $\\|x\\| = \\sqrt{\\langle x, x \\rangle}$.\n\nLet $M$ be the subspace of $\\ell^2$ defined as:\n$$M = \\{x \\in \\ell^2 \\mid x_1 = -x_2 \\text{ and } x_n=0 \\text{ for all } n > 2\\}$$\n\nFind the best approximation to the vector $v = (1, 0, 0, 0, \\dots)$ from the subspace $M$. The best approximation is the unique vector $m_0 \\in M$ that minimizes the distance $\\|v - m\\|$ over all $m \\in M$.\n\nExpress your answer as a row matrix containing the first two components of the vector $m_0$, in the form $\\begin{pmatrix} (m_0)_1 & (m_0)_2 \\end{pmatrix}$.", "solution": "We identify the subspace $M$ as the one-dimensional subspace spanned by $u=(1,-1,0,0,\\dots)$, since $x\\in M$ if and only if $x=(a,-a,0,0,\\dots)$ for some real $a$, i.e., $M=\\operatorname{span}\\{u\\}$.\n\nIn a real Hilbert space, the best approximation (orthogonal projection) of $v$ onto $\\operatorname{span}\\{u\\}$ is given by\n$$\nm_{0}=\\frac{\\langle v,u\\rangle}{\\langle u,u\\rangle}\\,u.\n$$\nCompute the necessary inner products:\n$$\n\\langle u,u\\rangle=1^{2}+(-1)^{2}=2,\\qquad \\langle v,u\\rangle=1\\cdot 1+0\\cdot(-1)=1.\n$$\nTherefore,\n$$\nm_{0}=\\frac{1}{2}\\,u=\\left(\\frac{1}{2},-\\frac{1}{2},0,0,\\dots\\right).\n$$\nHence the first two components are $\\left(\\frac{1}{2},-\\frac{1}{2}\\right)$.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{1}{2} & -\\frac{1}{2} \\end{pmatrix}}$$", "id": "1886651"}, {"introduction": "One of the most powerful applications of the Projection Theorem is its ability to reframe optimization problems in the language of geometry. This exercise asks you to find the constant that best approximates the function $f(t) = t^3$ in a least-squares sense, which is equivalent to finding the orthogonal projection of $f(t)$ onto the subspace of constant functions in $L^2[0,1]$ [@problem_id:1898082]. This approach elegantly transforms a calculus problem into a projection problem in a Hilbert space.", "problem": "Let the function $I(c)$ be defined by the integral $I(c) = \\int_{0}^{1} |t^3 - c|^2 dt$, where $t$ is a real variable over the interval $[0, 1]$ and $c$ is a complex constant. Find the minimum possible value of $I(c)$ as $c$ varies over all complex numbers $\\mathbb{C}$. Report your answer as a decimal rounded to four significant figures.", "solution": "Define $f(t)=t^{3}$ on $[0,1]$ and use the complex $L^{2}$ inner product $\\langle g,h\\rangle=\\int_{0}^{1}g(t)\\overline{h(t)}\\,dt$. Then\n$$\nI(c)=\\int_{0}^{1}|t^{3}-c|^{2}\\,dt=\\int_{0}^{1}(t^{3}-c)(t^{3}-\\overline{c})\\,dt\n= \\|f\\|^{2}-c\\overline{\\langle f,1\\rangle}-\\overline{c}\\langle f,1\\rangle+|c|^{2}\\|1\\|^{2}.\n$$\nThis is a strictly convex quadratic in $c$, which can be written by completing the square as\n$$\nI(c)=\\|1\\|^{2}\\left|c-\\frac{\\langle f,1\\rangle}{\\|1\\|^{2}}\\right|^{2}+\\|f\\|^{2}-\\frac{|\\langle f,1\\rangle|^{2}}{\\|1\\|^{2}}.\n$$\nHence the minimizer is $c^{\\ast}=\\langle f,1\\rangle/\\|1\\|^{2}$ and the minimum value is\n$$\nI_{\\min}=\\|f\\|^{2}-\\frac{|\\langle f,1\\rangle|^{2}}{\\|1\\|^{2}}.\n$$\nCompute the needed quantities:\n$$\n\\langle f,1\\rangle=\\int_{0}^{1}t^{3}\\,dt=\\frac{1}{4},\\quad \\|1\\|^{2}=\\int_{0}^{1}1\\,dt=1,\\quad \\|f\\|^{2}=\\int_{0}^{1}t^{6}\\,dt=\\frac{1}{7}.\n$$\nTherefore\n$$\nI_{\\min}=\\frac{1}{7}-\\left(\\frac{1}{4}\\right)^{2}=\\frac{1}{7}-\\frac{1}{16}=\\frac{9}{112}.\n$$\nAs a decimal rounded to four significant figures, $\\frac{9}{112}=0.080357\\ldots$ rounds to $0.08036$.", "answer": "$$\\boxed{0.08036}$$", "id": "1898082"}, {"introduction": "The concepts of orthogonality and projection extend far beyond familiar Euclidean spaces, applying to abstract vector spaces like those of functions or matrices. This practice challenges you to work within the Hilbert space of $2 \\times 2$ matrices, equipped with the Frobenius inner product [@problem_id:1898051]. Your task is to characterize the orthogonal complement to the subspace of diagonal matrices, reinforcing the fundamental relationship between a subspace and its orthogonal counterpart.", "problem": "Consider the vector space $V = M_2(\\mathbb{R})$ of all $2 \\times 2$ matrices with real entries. This space becomes a Hilbert space when equipped with the Frobenius inner product, defined as $\\langle A, B \\rangle = \\mathrm{tr}(A^T B)$ for any two matrices $A, B \\in V$. In this expression, $\\mathrm{tr}(X)$ denotes the trace of a matrix $X$, and $X^T$ denotes its transpose.\n\nLet $U$ be the subspace of $V$ consisting of all $2 \\times 2$ diagonal matrices. The orthogonal complement of $U$, denoted $U^\\perp$, is the set of all matrices in $V$ that are orthogonal to every matrix in $U$.\n\nWhich of the following descriptions accurately characterizes the orthogonal complement $U^\\perp$?\n\nA. The subspace of all symmetric matrices (i.e., matrices $B$ such that $B^T = B$).\n\nB. The subspace of all skew-symmetric matrices (i.e., matrices $B$ such that $B^T = -B$).\n\nC. The subspace of all matrices with zero entries on the main diagonal.\n\nD. The subspace of all matrices with a trace of zero.\n\nE. The subspace of all upper-triangular matrices.", "solution": "We consider the Hilbert space $V = M_{2}(\\mathbb{R})$ with the Frobenius inner product $\\langle A, B \\rangle = \\mathrm{tr}(A^{T}B)$. Let $U$ be the subspace of all diagonal matrices:\n$$\nU = \\left\\{ \\begin{pmatrix} x & 0 \\\\ 0 & y \\end{pmatrix} : x, y \\in \\mathbb{R} \\right\\}.\n$$\nLet $B \\in V$ be arbitrary, written as\n$$\nB = \\begin{pmatrix} b_{11} & b_{12} \\\\ b_{21} & b_{22} \\end{pmatrix}.\n$$\nFor $A \\in U$, write $A = \\begin{pmatrix} x & 0 \\\\ 0 & y \\end{pmatrix}$. Since $A$ is symmetric, $A^{T} = A$, hence\n$$\n\\langle A, B \\rangle = \\mathrm{tr}(A^{T}B) = \\mathrm{tr}(AB).\n$$\nCompute $AB$:\n$$\nAB = \\begin{pmatrix} x & 0 \\\\ 0 & y \\end{pmatrix} \\begin{pmatrix} b_{11} & b_{12} \\\\ b_{21} & b_{22} \\end{pmatrix}\n= \\begin{pmatrix} x b_{11} & x b_{12} \\\\ y b_{21} & y b_{22} \\end{pmatrix}.\n$$\nTherefore,\n$$\n\\langle A, B \\rangle = \\mathrm{tr}(AB) = x b_{11} + y b_{22}.\n$$\nFor $B$ to be in $U^{\\perp}$, we require $\\langle A, B \\rangle = 0$ for all $A \\in U$, i.e., for all $x, y \\in \\mathbb{R}$,\n$$\nx b_{11} + y b_{22} = 0.\n$$\nSince $x$ and $y$ are independent, this holds for all $x, y$ if and only if\n$$\nb_{11} = 0 \\quad \\text{and} \\quad b_{22} = 0.\n$$\nThere is no restriction on $b_{12}$ and $b_{21}$. Hence\n$$\nU^{\\perp} = \\left\\{ \\begin{pmatrix} 0 & b_{12} \\\\ b_{21} & 0 \\end{pmatrix} : b_{12}, b_{21} \\in \\mathbb{R} \\right\\},\n$$\nwhich is precisely the subspace of all matrices with zero entries on the main diagonal. Among the given options, this is option C. Options A and B impose symmetry constraints not required here; option D allows nonzero diagonal entries summing to zero, which are not orthogonal to all diagonals; option E imposes a triangular form that does not enforce zero diagonal.", "answer": "$$\\boxed{C}$$", "id": "1898051"}]}