{"hands_on_practices": [{"introduction": "To truly understand the multinomial distribution, it is best to construct its probability mass function from a familiar scenario. This first exercise [@problem_id:12523] guides you through this process using the classic example of rolling a multi-sided die. By categorizing the outcomes of multiple rolls, you will derive the general formula, reinforcing the core components: the multinomial coefficient for counting arrangements and the product of probabilities for each specific sequence.", "problem": "Consider a fair die with $m$ faces, labeled with the integers $\\{1, 2, \\dots, m\\}$. The die is rolled $n$ independent times. The outcomes of the rolls are classified into three distinct categories based on two integer thresholds, $a$ and $b$, where $1 \\le a < b < m$.\n\nThe categories are defined as follows for a single roll:\n- **Category 1**: The outcome is less than or equal to $a$.\n- **Category 2**: The outcome is greater than $a$ and less than or equal to $b$.\n- **Category 3**: The outcome is greater than $b$.\n\nDerive a general expression for the probability of observing exactly $n_1$ outcomes in Category 1, $n_2$ outcomes in Category 2, and $n_3$ outcomes in Category 3, where $n_1 + n_2 + n_3 = n$. Express your answer in terms of $n$, $m$, $a$, $b$, $n_1$, $n_2$, and $n_3$.", "solution": "We partition each roll into three categories with probabilities  \n$$p_1=\\frac{a}{m},\\quad p_2=\\frac{b-a}{m},\\quad p_3=\\frac{m-b}{m},$$  \nand note that $p_1+p_2+p_3=1$ and $n_1+n_2+n_3=n$.  \n\nBy the multinomial distribution, the probability of observing exactly $n_1$ outcomes in Category 1, $n_2$ in Category 2, and $n_3$ in Category 3 is  \n$$\n\\frac{n!}{n_1!\\,n_2!\\,n_3!}\\,p_1^{n_1}\\,p_2^{n_2}\\,p_3^{n_3}.\n$$  \nSubstituting the expressions for $p_1,p_2,p_3$ yields the desired formula.", "answer": "$$\\boxed{\\frac{n!}{n_1!n_2!n_3!}\\Bigl(\\frac{a}{m}\\Bigr)^{n_1}\\Bigl(\\frac{b-a}{m}\\Bigr)^{n_2}\\Bigl(\\frac{m-b}{m}\\Bigr)^{n_3}}$$", "id": "12523"}, {"introduction": "A powerful feature of the multinomial distribution is its consistency when categories are combined. This practice [@problem_id:12547] explores this \"grouping property.\" Starting with a four-category experiment, you will see how merging two categories results in a new, valid three-category multinomial distribution, demonstrating the model's flexibility and its applicability to analyzing data at different levels of detail.", "problem": "An experiment consists of $n$ independent trials. Each trial can result in one of four mutually exclusive outcomes, labeled Category 1, Category 2, Category 3, and Category 4. For any given trial, the probability of the outcome being Category $i$ is $p_i$, where $i \\in \\{1, 2, 3, 4\\}$. The probabilities are constant for all trials, and they sum to one: $p_1 + p_2 + p_3 + p_4 = 1$.\n\nThis scenario is described by a multinomial distribution. The random variables $X_1, X_2, X_3, X_4$ represent the number of times each category is observed in the $n$ trials, where $\\sum_{i=1}^4 X_i = n$.\n\nNow, suppose we are no longer interested in distinguishing between Category 1 and Category 2. We decide to group them into a single new category, let's call it \"Group A\". The other two categories remain distinct.\n\nLet the random variable $X_A$ be the number of outcomes in Group A, $X_3$ be the number of outcomes in Category 3, and $X_4$ be the number of outcomes in Category 4.\n\nDerive the probability mass function for the specific outcome of observing $x_A$ items in Group A, $x_3$ items in Category 3, and $x_4$ items in Category 4, where $x_A + x_3 + x_4 = n$. Express your answer in terms of $n$, $x_A$, $x_3$, $x_4$, and the initial probabilities $p_1, p_2, p_3, p_4$.", "solution": "We start from the four-category multinomial pmf for counts $x_1,x_2,x_3,x_4$ with $\\sum_{i=1}^4x_i=n$:\n$$\nP(X_1=x_1,X_2=x_2,X_3=x_3,X_4=x_4)\n=\\frac{n!}{x_1!\\,x_2!\\,x_3!\\,x_4!}\\,p_1^{x_1}p_2^{x_2}p_3^{x_3}p_4^{x_4}.\n$$\nWe wish to marginalize over $X_1,X_2$ subject to $x_1+x_2=x_A$. Summing over all splits,\n$$\nP(X_A=x_A,X_3=x_3,X_4=x_4)\n=\\sum_{x_1+x_2=x_A}\\frac{n!}{x_1!\\,x_2!\\,x_3!\\,x_4!}\\,p_1^{x_1}p_2^{x_2}p_3^{x_3}p_4^{x_4}.\n$$\nFactor out terms not depending on $x_1,x_2$ and note the binomial expansion:\n$$\nP=\\frac{n!}{x_3!\\,x_4!}\\,p_3^{x_3}p_4^{x_4}\n\\sum_{x_1+x_2=x_A}\\frac{1}{x_1!\\,x_2!}p_1^{x_1}p_2^{x_2}\n=\\frac{n!}{x_A!\\,x_3!\\,x_4!}\\,(p_1+p_2)^{x_A}p_3^{x_3}p_4^{x_4}.\n$$\nThis is the pmf for the grouped counts $(X_A,X_3,X_4)$ with $x_A+x_3+x_4=n$.", "answer": "$$\\boxed{\\frac{n!}{x_A!\\,x_3!\\,x_4!}(p_1+p_2)^{x_A}p_3^{x_3}p_4^{x_4}}$$", "id": "12547"}, {"introduction": "Beyond calculating the probability of a specific outcome, we often need to understand the statistical properties and relationships between the different outcome counts. This exercise [@problem_id:12563] moves into the realm of expectations, asking you to derive the expected value of the product of two counts, $E[X_1 X_2]$. This calculation is a key step toward understanding the concept of covariance and reveals the inherent negative correlation between counts in a multinomial experiment.", "problem": "An automated manufacturing process produces electronic components. Each component is independently tested and classified into one of three categories:\n1.  **Category 1**: Perfect, with probability $p_1$.\n2.  **Category 2**: Defective but repairable, with probability $p_2$.\n3.  **Category 3**: Unusable scrap, with probability $p_3$.\n\nThese categories are mutually exclusive and exhaustive, so $p_1 + p_2 + p_3 = 1$.\n\nA batch consists of $n$ components, where each component's classification is an independent trial. Let the random variable $X_1$ denote the total number of components in Category 1, and $X_2$ denote the total number of components in Category 2 from this batch of $n$ components.\n\nUsing first principles, derive a closed-form expression for the expected value of the product of these two counts, $E[X_1 X_2]$, in terms of $n$, $p_1$, and $p_2$.", "solution": "Let $I_{i,k}$ be the indicator that component $i$ is in Category $k$.  Then\n$$X_1=\\sum_{i=1}^nI_{i,1},\\qquad X_2=\\sum_{j=1}^nI_{j,2}.$$\nHence\n$$X_1X_2=\\sum_{i=1}^n\\sum_{j=1}^nI_{i,1}I_{j,2}.$$\nTaking expectation,\n$$E[X_1X_2]\n=\\sum_{i=1}^n\\sum_{j=1}^nE[I_{i,1}I_{j,2}]\n=\\sum_{i\\neq j}E[I_{i,1}I_{j,2}]+\\sum_{i=j}E[I_{i,1}I_{i,2}].$$\nSince a single component cannot be in both categories, $I_{i,1}I_{i,2}=0$ so\n$$\\sum_{i=j}E[I_{i,1}I_{i,2}]=0.$$\nFor $i\\neq j$, trials are independent and\n$$E[I_{i,1}I_{j,2}]=P(\\text{cat }1\\text{ at }i)\\,P(\\text{cat }2\\text{ at }j)=p_1p_2.$$\nThere are $n(n-1)$ ordered pairs with $i\\neq j$, so\n$$E[X_1X_2]=n(n-1)\\,p_1p_2.$$", "answer": "$$\\boxed{n(n-1)p_1p_2}$$", "id": "12563"}]}