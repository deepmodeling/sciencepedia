{"hands_on_practices": [{"introduction": "To truly grasp a statistical distribution, it's invaluable to understand how to construct it from more basic components. This practice challenges you to think like a programmer and design a simulation algorithm based on the fundamental definition of the chi-squared distribution. By translating the mathematical definition into a computational procedure, you will solidify your understanding of its origin in the standard normal distribution. [@problem_id:1903721]", "problem": "In the development of a custom statistical simulation library, a programmer is tasked with creating a function to generate random variates from a chi-squared distribution with $k$ degrees of freedom, denoted as $\\chi^2(k)$, where $k$ is a positive integer. The only available tool is a pre-existing function, `get_std_normal()`, which returns a single random number drawn from the standard normal distribution, $N(0, 1)$.\n\nThe programmer must design an algorithm that correctly produces a single random variate from the $\\chi^2(k)$ distribution by making one or more calls to `get_std_normal()`. Which of the following proposed algorithms correctly accomplishes this task?\n\nA. Initialize a variable `sum_of_squares` to 0. Loop $k$ times: in each iteration, call `get_std_normal()` to get a value $z$, add $z^2$ to `sum_of_squares`. The final value of `sum_of_squares` is the result.\n\nB. Initialize a variable `sum_val` to 0. Loop $k$ times: in each iteration, call `get_std_normal()` to get a value $z$, and add $z$ to `sum_val`. The final value of `sum_val` is the result.\n\nC. Initialize a variable `sum_val` to 0. Loop $k$ times: in each iteration, call `get_std_normal()` to get a value $z$, and add $z$ to `sum_val`. The final result is `sum_val` squared.\n\nD. Call `get_std_normal()` exactly once to get a value $z$. The result is $k \\times z$.\n\nE. Initialize a variable `sum_of_squares` to 0. Loop $k$ times: in each iteration, call `get_std_normal()` to get a value $z$, add $z^2$ to `sum_of_squares`. The final result is `sum_of_squares` divided by $k$.", "solution": "Let $Z_{1},\\dots,Z_{k}$ be independent standard normal random variables, each obtained by a call to get_std_normal(), so $Z_{i} \\sim N(0,1)$ and the $Z_{i}$ are independent. By the defining property of the chi-squared distribution, the sum of squares\n$$\nQ=\\sum_{i=1}^{k} Z_{i}^{2}\n$$\nhas the $\\chi^{2}(k)$ distribution. This can be verified, for example, via the moment generating function: for $t\\frac{1}{2}$, the mgf of $Z_{i}^{2}$ is $(1-2t)^{-1/2}$, and by independence\n$$\nM_{Q}(t)=\\prod_{i=1}^{k}(1-2t)^{-1/2}=(1-2t)^{-k/2},\n$$\nwhich is the mgf of $\\chi^{2}(k)$. Therefore, computing the sum of the squares of $k$ independent standard normals is exactly the correct algorithm.\n\nNow assess each proposed algorithm:\n\nA. This computes $Q=\\sum_{i=1}^{k} Z_{i}^{2}$, which by definition has distribution $\\chi^{2}(k)$. Hence A is correct.\n\nB. This computes $S=\\sum_{i=1}^{k} Z_{i}$, which is normal with $S \\sim N(0,k)$, not chi-squared. Hence B is incorrect.\n\nC. This computes $S^{2}$ where $S=\\sum_{i=1}^{k} Z_{i} \\sim N(0,k)$. Then $S/\\sqrt{k} \\sim N(0,1)$ and $S^{2}=k\\,(S/\\sqrt{k})^{2}$, so $S^{2}$ has the distribution $k \\cdot \\chi^{2}(1)$, which is not $\\chi^{2}(k)$ unless $k=1$. Hence C is incorrect in general.\n\nD. This computes $k Z$ for a single $Z \\sim N(0,1)$, yielding $N(0,k^{2})$, not chi-squared. Hence D is incorrect.\n\nE. This computes $\\frac{1}{k}\\sum_{i=1}^{k} Z_{i}^{2}=\\frac{1}{k}\\,\\chi^{2}(k)$, a scaled chi-squared variable, not $\\chi^{2}(k)$. Hence E is incorrect.\n\nTherefore, the only correct algorithm is A.", "answer": "$$\\boxed{A}$$", "id": "1903721"}, {"introduction": "Having explored how a chi-squared variable is constructed, we now investigate its fundamental properties. This exercise guides you through calculating the expected value of a simple chi-squared variable directly from its definition involving squared standard normal variables. Mastering this derivation provides insight into why the mean of a $\\chi^2(k)$ distribution is simply its degrees of freedom, $k$, moving beyond mere memorization of the formula. [@problem_id:2277]", "problem": "Let $Z_1$ and $Z_2$ be two independent random variables, each following a standard normal distribution. A random variable $Z$ is said to have a standard normal distribution if its expectation (mean) is $E[Z] = 0$ and its variance is $Var(Z) = 1$. The variance is related to the expectation via the formula $Var(Z) = E[Z^2] - (E[Z])^2$.\n\nA new random variable, $Y$, is constructed from $Z_1$ and $Z_2$ as follows:\n$$Y = c(Z_1^2 + Z_2^2)$$\nwhere $c$ is a given positive real constant.\n\nDerive the expectation of the random variable $Y$, denoted as $E[Y]$.", "solution": "We have $Y = c(Z_1^2 + Z_2^2)$. By the linearity of expectation,\n$$E[Y] = c(E[Z_1^2] + E[Z_2^2])$$\nSince $Z_1$ and $Z_2$ are standard normal, we know that for each variable $Z_i$, $E[Z_i]=0$ and $\\text{Var}(Z_i)=1$. Using the relationship $\\text{Var}(Z_i)=E[Z_i^2]-(E[Z_i])^2$, we can deduce that $1 = E[Z_i^2] - 0^2$, so $E[Z_i^2]=1$.\nTherefore,\n$$E[Y] = c(1+1) = 2c$$", "answer": "$$\\boxed{2c}$$", "id": "2277"}, {"introduction": "A cornerstone of the chi-squared distribution's utility in statistics is its elegant additivity property. This problem presents a scenario where you must use this property to uncover the characteristics of an unknown random variable. By working with the relationship between independent chi-squared variables and their sum, you will practice a type of statistical reasoning that is central to techniques like the analysis of variance (ANOVA). [@problem_id:2278]", "problem": "Let $X$ and $Y$ be independent random variables. The variable $X$ is known to follow a chi-squared distribution with $k_X = 4$ degrees of freedom, denoted as $X \\sim \\chi^2(4)$. Their sum, $Z = X+Y$, is also a chi-squared random variable, following a distribution with $k_Z = 10$ degrees of freedom, i.e., $Z \\sim \\chi^2(10)$.\n\nYou are provided with the following fundamental properties of the chi-squared distribution:\n1.  **Additivity Property:** If two random variables $U \\sim \\chi^2(k_1)$ and $V \\sim \\chi^2(k_2)$ are independent, then their sum $U+V$ also follows a chi-squared distribution with degrees of freedom equal to the sum of their individual degrees of freedom: $U+V \\sim \\chi^2(k_1+k_2)$. A related property (a consequence of Cochran's Theorem) is that if $X$ and $Y$ are independent random variables, and both $X$ and their sum $Z=X+Y$ are chi-squared variables, then $Y$ must also be a chi-squared variable.\n2.  **Variance Formula:** The variance of a random variable $W$ that follows a chi-squared distribution with $k$ degrees of freedom is given by $\\text{Var}(W) = 2k$.\n\nBased on this information, derive the variance of the random variable $Y$.", "solution": "By the additivity of chi-squared distributions, if $X\\sim\\chi^2(k_X)$ and $Y\\sim\\chi^2(k_Y)$ are independent then \n$$Z=X+Y\\sim\\chi^2(k_X+k_Y)$$\nBased on the problem statement, we know $X$ and $Y$ are independent, $X \\sim \\chi^2(k_X)$, and their sum $Z = X+Y \\sim \\chi^2(k_Z)$. This implies that $Y$ must also follow a chi-squared distribution with degrees of freedom $k_Y = k_Z - k_X$.\nGiven $k_X=4$ and $k_Z=10$, we have:\n$$k_Y = k_Z-k_X = 10-4=6$$\nThe variance of a chi-squared random variable $W$ with $k$ degrees of freedom is $\\text{Var}(W)=2k$. Therefore, the variance of $Y$ is:\n$$\\text{Var}(Y)=2k_Y=2 \\cdot 6=12$$", "answer": "$$\\boxed{12}$$", "id": "2278"}]}