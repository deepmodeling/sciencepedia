{"hands_on_practices": [{"introduction": "This first exercise [@problem_id:1364493] places you in the role of a data scientist at a polling agency, tasked with a common and critical problem: determining an adequate sample size. It represents a cornerstone application of Hoeffding's inequality, where you will calculate the minimum number of participants needed to guarantee a specific margin of error and confidence level for a political poll. This practice highlights how the inequality provides robust, worst-case guarantees that are essential for real-world statistical inference.", "problem": "A political polling agency is tasked with estimating the approval rating of a city's mayor. Let the true, unknown proportion of the population that approves of the mayor be $p$. The agency will survey $n$ randomly selected voters and use the sample proportion of approvals, denoted by $\\hat{p}$, as their estimate for $p$. The agency wants to be able to claim that their estimate is within $\\pm 3\\%$ of the true value with at least $95\\%$ confidence.\n\nTo determine the required sample size, the agency uses Hoeffding's inequality. For a sample of $n$ independent observations $X_1, \\dots, X_n$ from a distribution, where each observation is bounded within an interval $[a, b]$, the inequality provides an upper bound on the probability that the sample mean $\\bar{X}$ deviates from the true mean $\\mu$ by more than a value $\\epsilon  0$:\n$$P(|\\bar{X} - \\mu| \\ge \\epsilon) \\le 2 \\exp\\left(-\\frac{2n\\epsilon^2}{(b-a)^2}\\right)$$\n\nAssuming each voter's response is an independent random variable, what is the minimum integer sample size $n$ the agency must use to guarantee its desired level of accuracy and confidence, irrespective of the true underlying approval rating $p$?", "solution": "Let $X_{i}$ be the approval indicator for voter $i$, with $X_{i} \\in \\{0,1\\}$ and $\\mathbb{E}[X_{i}]=p$. Then the sample proportion is $\\hat{p}=\\bar{X}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}$. Each $X_{i}$ is bounded in $[0,1]$, so $a=0$ and $b=1$.\n\nHoeffding's inequality states that for any $\\epsilon0$,\n$$\n\\Pr(|\\bar{X}-\\mu|\\geq \\epsilon)\\leq 2\\exp\\!\\left(-\\frac{2n\\epsilon^{2}}{(b-a)^{2}}\\right).\n$$\nHere $(b-a)=1$, so\n$$\n\\Pr(|\\hat{p}-p|\\geq \\epsilon)\\leq 2\\exp(-2n\\epsilon^{2}).\n$$\nTo ensure accuracy within $0.03$ with at least $0.95$ confidence, require\n$$\n\\Pr(|\\hat{p}-p|\\geq 0.03)\\leq 0.05.\n$$\nImpose the Hoeffding bound to be at most $0.05$:\n$$\n2\\exp(-2n\\cdot 0.03^{2})\\leq 0.05.\n$$\nDivide by $2$:\n$$\n\\exp(-2n\\cdot 0.03^{2})\\leq 0.025.\n$$\nApply $\\ln$ to both sides and use monotonicity of $\\ln$:\n$$\n-2n\\cdot 0.03^{2}\\leq \\ln(0.025).\n$$\nMultiply by $-1$ (reversing the inequality sign) and solve for $n$:\n$$\n2n\\cdot 0.03^{2}\\geq -\\ln(0.025)=\\ln(40),\n$$\n$$\nn\\geq \\frac{\\ln(40)}{2\\cdot 0.03^{2}}.\n$$\nThus the minimum integer sample size is\n$$\nn=\\left\\lceil \\frac{\\ln(40)}{2\\cdot 0.03^{2}} \\right\\rceil.\n$$\nCompute the value: $\\ln(40)\\approx 3.688879454$ and $2\\cdot 0.03^{2}=0.0018$, so\n$$\n\\frac{\\ln(40)}{0.0018}\\approx 2049.377\\ldots,\n$$\nhence the minimum integer is $2050$.", "answer": "$$\\boxed{2050}$$", "id": "1364493"}, {"introduction": "Moving from social science to computational mathematics, this problem [@problem_id:709717] demonstrates the power of Hoeffding's inequality in the context of Monte Carlo methods. You will determine the number of samples required to estimate a definite integral to a desired precision and confidence. This exercise illustrates how the inequality is not just an abstract concept, but a practical tool for quantifying the uncertainty in numerical approximations and ensuring the reliability of computational results.", "problem": "Consider the problem of estimating the definite integral $I = \\int_0^1 f(x) dx$ using the Monte Carlo method. The integral can be interpreted as the expected value of $f(X)$, where $X$ is a random variable uniformly distributed on $[0, 1]$. The Monte Carlo estimator for $I$ is the sample mean $\\hat{I}_n = \\frac{1}{n} \\sum_{i=1}^n f(X_i)$, based on $n$ independent and identically distributed samples $X_1, X_2, \\ldots, X_n$ drawn from the uniform distribution $U(0, 1)$.\n\nWe will use Hoeffding's inequality to determine a sufficient sample size $n$ for our estimate. For a set of i.i.d. random variables $Y_1, \\ldots, Y_n$ with $E[Y_i] = \\mu$ that are almost surely bounded, i.e., $P(a \\le Y_i \\le b) = 1$, Hoeffding's inequality states that for any $\\epsilon  0$:\n$$P(|\\bar{Y} - \\mu| \\ge \\epsilon) \\le 2 \\exp\\left(-\\frac{2n\\epsilon^2}{(b-a)^2}\\right)$$\nwhere $\\bar{Y} = \\frac{1}{n}\\sum_{i=1}^n Y_i$ is the sample mean.\n\nLet the function be $f(x) = \\sin(\\pi x)$. We want to find the minimum sample size $n$ required to ensure that our estimate $\\hat{I}_n$ is within an accuracy of $\\epsilon$ of the true value $I$ with a confidence of at least $1-\\delta$. That is, we require $P(|\\hat{I}_n - I|  \\epsilon) \\ge 1-\\delta$.\n\nDerive the minimum sample size $n$ as a function of the desired accuracy $\\epsilon$ and the significance level $\\delta$.", "solution": "1. Since $f(x)=\\sin(\\pi x)\\in[0,1]$, we take $a=0$, $b=1$. Hoeffding’s inequality gives\n$$P\\bigl(|\\hat I_n-I|\\ge\\epsilon\\bigr)\\le 2\\exp\\Bigl(-\\frac{2n\\epsilon^2}{(1-0)^2}\\Bigr)=2\\exp(-2n\\epsilon^2).$$\n2. We require $2\\exp(-2n\\epsilon^2)\\le\\delta$. Divide by 2:\n$$\\exp(-2n\\epsilon^2)\\le\\frac\\delta2.$$\n3. Take natural logarithms:\n$$-2n\\epsilon^2\\le\\ln\\frac\\delta2\\quad\\Longrightarrow\\quad n\\ge -\\frac{1}{2\\epsilon^2}\\ln\\frac\\delta2\n=\\frac{\\ln\\frac2\\delta}{2\\epsilon^2}.$$\nThus the minimum sample size is\n$$n=\\frac{\\ln\\frac2\\delta}{2\\epsilon^2}.$$", "answer": "$$\\boxed{\\frac{\\ln\\frac{2}{\\delta}}{2\\epsilon^2}}$$", "id": "709717"}, {"introduction": "Now that you've applied Hoeffding's inequality, it's time to appreciate its strength. This problem [@problem_id:792723] guides you through a direct comparison between the bounds provided by Hoeffding's inequality and the more general Chebyshev's inequality. By analyzing the ratio of these bounds, you will mathematically prove and quantify the superiority of Hoeffding's exponential decay, providing a deeper understanding of why it yields much tighter and more useful estimates for sums of bounded random variables.", "problem": "A Rademacher random variable is a discrete random variable that takes the value $+1$ with probability $1/2$ and $-1$ with probability $1/2$. Consider a set of $n$ independent and identically distributed (i.i.d.) Rademacher random variables, $X_1, X_2, \\ldots, X_n$. Let their sum be denoted by $S_n = \\sum_{i=1}^n X_i$.\n\nThe probability of a random variable deviating from its mean can be bounded using various concentration inequalities. Two well-known examples are Chebyshev's inequality and Hoeffding's inequality.\n\n1.  **Chebyshev's Inequality (two-sided):** For any random variable $Y$ with a finite mean $\\mu = E[Y]$ and finite non-zero variance $\\sigma^2 = \\text{Var}(Y)$, the following inequality holds for any real number $a  0$:\n    $$\n    P(|Y - \\mu| \\ge a) \\le \\frac{\\sigma^2}{a^2}\n    $$\n\n2.  **Hoeffding's Inequality (two-sided):** For a sum $S_n = \\sum_{i=1}^n X_i$ of $n$ independent random variables, where each $X_i$ is bounded within an interval $[a_i, b_i]$, the following inequality holds for any $t  0$:\n    $$\n    P(|S_n - E[S_n]| \\ge t) \\le 2\\exp\\left(-\\frac{2t^2}{\\sum_{i=1}^n (b_i - a_i)^2}\\right)\n    $$\n\nApplying these two inequalities to the tail probability $P(|S_n| \\ge t)$ for the sum of Rademacher variables yields two different upper bounds. Let's denote the bound derived from Chebyshev's inequality as $B_C(t)$ and the bound from Hoeffding's inequality as $B_H(t)$.\n\nDerive the maximum possible value of the ratio $R(t) = \\frac{B_H(t)}{B_C(t)}$ over the domain $t  0$.", "solution": "1. Chebyshev’s bound for $S_n\\sim$ sum of $n$ Rademachers (mean $0$, variance $n$):\n$$\nB_C(t)=P(|S_n|\\ge t)\\le\\frac{\\Var(S_n)}{t^2}=\\frac{n}{t^2}.\n$$\n\n2. Hoeffding’s bound with each $X_i\\in[-1,1]$ so $(b_i-a_i)^2=4$:\n$$\nB_H(t)=2\\exp\\Bigl(-\\frac{2t^2}{\\sum_{i=1}^n4}\\Bigr)\n=2\\exp\\Bigl(-\\frac{t^2}{2n}\\Bigr).\n$$\n\n3. Ratio\n$$\nR(t)=\\frac{B_H(t)}{B_C(t)}\n=\\frac{2\\exp(-t^2/(2n))}{\\,n/t^2\\,}\n=\\frac{2t^2}{n}\\exp\\Bigl(-\\frac{t^2}{2n}\\Bigr).\n$$\n\n4. Set $u=\\frac{t^2}{2n}$, so\n$$\nR=4u e^{-u},\\quad u0.\n$$\nMaximize: $\\frac{d}{du}[u e^{-u}]=e^{-u}(1-u)=0\\implies u=1$. Hence\n$$\nR_{\\max}=4\\cdot1\\cdot e^{-1}=\\frac{4}{e}.\n$$", "answer": "$$\\boxed{\\frac{4}{e}}$$", "id": "792723"}]}