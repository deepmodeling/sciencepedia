{"hands_on_practices": [{"introduction": "The Cauchy-Schwarz inequality provides a fundamental constraint on the relationship between two random variables. One of its most direct and important applications in probability theory is in bounding the covariance, which measures how two variables move together. This exercise [@problem_id:1347664] illustrates how to use the first and second moments of two variables, $X$ and $Y$, to find the maximum possible value of their cross-moment, $E[XY]$, a crucial calculation in fields like signal processing and physics where interactions between fluctuating quantities are studied.", "problem": "In the study of coupled oscillating systems, the statistical properties of two state variables, represented by random variables $X$ and $Y$, are analyzed. Experimental measurements have established the following expected values:\nThe mean of $X$ is $E[X] = 0.1$.\nThe mean of $Y$ is $E[Y] = 0.2$.\nThe second moment of $X$ is $E[X^2] = 0.05$.\nThe second moment of $Y$ is $E[Y^2] = 0.13$.\nThe interaction between the two variables is characterized by the expectation of their product, $E[XY]$. According to the fundamental principles of probability theory, there is a maximum possible value for this interaction term.\nCalculate the maximum possible value of $E[XY]$ based on the given data. Express your answer as a single real number, rounded to three significant figures.", "solution": "We are given $E[X]=0.1$, $E[Y]=0.2$, $E[X^{2}]=0.05$, and $E[Y^{2}]=0.13$. First compute the variances using $\\operatorname{Var}(X)=E[X^{2}]-(E[X])^{2}$ and $\\operatorname{Var}(Y)=E[Y^{2}]-(E[Y])^{2}$:\n$$\\operatorname{Var}(X)=0.05-(0.1)^{2}=0.05-0.01=0.04,$$\n$$\\operatorname{Var}(Y)=0.13-(0.2)^{2}=0.13-0.04=0.09.$$\nWrite $E[XY]$ in terms of the covariance:\n$$E[XY]=\\operatorname{Cov}(X,Y)+E[X]E[Y].$$\nBy the Cauchyâ€“Schwarz inequality applied to the centered variables,\n$$|\\operatorname{Cov}(X,Y)| \\leq \\sqrt{\\operatorname{Var}(X)\\operatorname{Var}(Y)}.$$\nTherefore the maximum possible value of $E[XY]$ is attained when $\\operatorname{Cov}(X,Y)=+\\sqrt{\\operatorname{Var}(X)\\operatorname{Var}(Y)}$, giving\n$$E[XY]_{\\max}=E[X]E[Y]+\\sqrt{\\operatorname{Var}(X)\\operatorname{Var}(Y)}=0.1\\cdot 0.2+\\sqrt{0.04\\cdot 0.09}=0.02+0.06=0.08.$$\nThis bound is attainable when $Y-E[Y]$ is a positive scalar multiple of $X-E[X]$, so it is the maximum consistent with the given moments. Rounding to three significant figures yields $0.0800$.", "answer": "$$\\boxed{0.0800}$$", "id": "1347664"}, {"introduction": "Beyond bounding moments, the Cauchy-Schwarz inequality can be wielded with remarkable ingenuity to establish bounds on probabilities, which requires a creative choice of random variables. This practice problem [@problem_id:1347643] introduces a powerful technique, sometimes known as the Paley-Zygmund inequality, to find a lower bound for the probability that a non-negative random variable is strictly positive, using only its first two moments. The key insight lies in cleverly relating the variable $X$ to the indicator function of the event $\\{X \\gt 0\\}$.", "problem": "A team of physicists is studying a quantum system where the energy of an emitted particle, represented by the non-negative random variable $X$, can fluctuate. The experimental setup allows for the precise measurement of the first two moments of the energy distribution. Let the mean energy be $E[X] = M_1$ and the mean squared energy be $E[X^2] = M_2$. It is known that the particle is not always at rest, so $M_1 > 0$.\n\nA crucial aspect of the system's stability depends on the particle having non-zero energy. The team needs to establish a guaranteed minimum probability that a particle emission has an energy greater than zero. Without making any other assumptions about the probability distribution of $X$, derive a general, tight lower bound for the probability $P(X > 0)$.\n\nExpress your answer as a closed-form analytic expression in terms of $M_1$ and $M_2$.", "solution": "Let $X$ be the non-negative random variable representing the particle's energy. We are given its first and second moments, $E[X] = M_1$ and $E[X^2] = M_2$. We are also given that $M_1 > 0$. Our goal is to find a lower bound for the probability $P(X > 0)$.\n\nFirst, we can express the probability $P(X > 0)$ as the expectation of an indicator random variable. Let $I_{X>0}$ be an indicator variable such that:\n$$\nI_{X>0} = \\begin{cases} 1 & \\text{if } X > 0 \\\\ 0 & \\text{if } X = 0 \\end{cases}\n$$\nBy the definition of expectation for a discrete random variable, the expected value of $I_{X>0}$ is:\n$$\nE[I_{X>0}] = 1 \\cdot P(I_{X>0} = 1) + 0 \\cdot P(I_{X>0} = 0) = P(X > 0)\n$$\nSo, our goal is to find a lower bound for $E[I_{X>0}]$.\n\nWe will use the Cauchy-Schwarz inequality for expectations. For any two random variables $Y$ and $Z$, the inequality states:\n$$\n(E[YZ])^2 \\le E[Y^2]E[Z^2]\n$$\n\nTo apply this inequality, we need to choose $Y$ and $Z$ appropriately. A clever choice involves connecting the random variable $X$ to the indicator $I_{X>0}$. Since $X$ is non-negative, we can write the identity $X = X \\cdot I_{X>0}$. This is because if $X>0$, $I_{X>0}=1$ and the identity is $X=X$. If $X=0$, both sides are zero.\n\nNow, let's take the expectation of this identity:\n$$\nE[X] = E[X \\cdot I_{X>0}]\n$$\nThis gives us a relationship between $M_1$ and the product of $X$ and our indicator. We can now apply the Cauchy-Schwarz inequality by setting $Y = X$ and $Z = I_{X>0}$.\n\nSubstituting these into the Cauchy-Schwarz inequality:\n$$\n(E[X \\cdot I_{X>0}])^2 \\le E[X^2] E[I_{X>0}^2]\n$$\n\nLet's evaluate each term in this inequality.\nThe left-hand side is $(E[X])^2 = M_1^2$.\nThe first term on the right-hand side is $E[X^2] = M_2$.\nThe second term on the right-hand side is $E[I_{X>0}^2]$. Since an indicator variable can only be 0 or 1, its square is equal to itself: $I_{X>0}^2 = I_{X>0}$. Therefore:\n$$\nE[I_{X>0}^2] = E[I_{X>0}] = P(X > 0)\n$$\n\nSubstituting these back into the inequality, we get:\n$$\nM_1^2 \\le M_2 \\cdot P(X > 0)\n$$\n\nSince we are given $M_1 > 0$, this implies that $X$ cannot be the zero random variable, so there must be some probability mass at values greater than zero. This ensures that $E[X^2] = M_2 > 0$ (if $M_2=0$, then $X=0$ almost surely, which would mean $M_1=0$, a contradiction). Thus, we can divide by $M_2$ without changing the direction of the inequality:\n$$\n\\frac{M_1^2}{M_2} \\le P(X > 0)\n$$\n\nThis provides the desired lower bound for the probability $P(X > 0)$. The bound is tight because one can construct a two-point distribution that achieves this bound.", "answer": "$$\\boxed{\\frac{M_1^2}{M_2}}$$", "id": "1347643"}, {"introduction": "The true power of a mathematical tool is revealed by its versatility, and the application of the Cauchy-Schwarz inequality often involves transforming random variables in clever ways. In this exercise [@problem_id:1347651], you will derive a useful lower bound for the expectation of $\\frac{1}{X+1}$ by applying the inequality to a carefully chosen pair of random variables involving square roots. This approach elegantly provides a sharp bound and showcases a common pattern for deriving inequalities by identifying the right functions to work with.", "problem": "Let $X$ be a random variable representing the number of successes in $n$ independent Bernoulli trials, where the probability of success in any given trial is $p$, with $0 < p < 1$. Consider a new random variable $Y$ defined as $Y = \\frac{1}{X+1}$. Determine a simple expression that serves as a lower bound for the expected value of $Y$, denoted $E[Y]$. Express your answer as a function of $n$ and $p$.", "solution": "Let $X \\sim \\mathrm{Bin}(n,p)$ with $0<p<1$. We want to find a lower bound for $E\\left[\\frac{1}{X+1}\\right]$.\nThe problem can be solved by a clever application of the Cauchy-Schwarz inequality, which states $(E[UV])^2 \\le E[U^2]E[V^2]$ for any two random variables $U$ and $V$.\n\nLet's choose our random variables $U$ and $V$ to create the terms we need. A suitable choice is:\n$U = \\sqrt{X+1}$\n$V = \\frac{1}{\\sqrt{X+1}}$\n\nWith this choice, the product $UV$ is simply:\n$UV = \\sqrt{X+1} \\cdot \\frac{1}{\\sqrt{X+1}} = 1$\n\nNow, let's find the expectations required for the Cauchy-Schwarz inequality.\nThe expectation of the product is:\n$E[UV] = E[1] = 1$\n\nThe expectation of $U^2$ is:\n$E[U^2] = E[(\\sqrt{X+1})^2] = E[X+1] = E[X] + 1$\nSince $X$ is a binomial random variable, its expectation is $E[X] = np$. So, $E[U^2] = np + 1$.\n\nThe expectation of $V^2$ is the quantity we want to bound:\n$E[V^2] = E\\left[\\left(\\frac{1}{\\sqrt{X+1}}\\right)^2\\right] = E\\left[\\frac{1}{X+1}\\right]$\n\nNow, substitute these terms into the Cauchy-Schwarz inequality:\n$(E[UV])^2 \\le E[U^2]E[V^2]$\n$1^2 \\le (np+1) \\cdot E\\left[\\frac{1}{X+1}\\right]$\n\nRearranging the inequality to solve for the expectation we are interested in gives:\n$$\nE\\left[\\frac{1}{X+1}\\right] \\geq \\frac{1}{np+1}\n$$\nThis gives the desired simple lower bound as a function of $n$ and $p$, derived using the Cauchy-Schwarz inequality as intended.", "answer": "$$\\boxed{\\frac{1}{np+1}}$$", "id": "1347651"}]}