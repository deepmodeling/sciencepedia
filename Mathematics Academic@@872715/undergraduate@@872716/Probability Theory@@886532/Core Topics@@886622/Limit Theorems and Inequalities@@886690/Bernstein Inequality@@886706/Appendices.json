{"hands_on_practices": [{"introduction": "This first practice provides a concrete and relatable scenario to build your intuition. We will apply Bernstein's inequality to determine the likelihood of a student's exam score straying far from its expected value. This exercise demonstrates the core power of the inequality: providing a quantitative guarantee on how closely a sum of random variables clusters around its average [@problem_id:1345821].", "problem": "A student is taking a long qualifying exam which consists of $N=250$ independent questions. For each question, the student has a probability of $p=0.75$ of answering it correctly. A correct answer yields 4 points, while an incorrect answer results in a penalty of 2 points. Let $T$ be the student's total score. Determine an upper bound for the probability that the student's total score deviates from its expected value by more than 150 points, i.e., find an upper bound for $P(|T - \\mathbb{E}[T]| > 150)$. Report your answer as a numerical value, rounded to three significant figures.", "solution": "Let $X_{i}$ denote the score on question $i$, with $X_{i}=4$ for a correct answer and $X_{i}=-2$ for an incorrect answer. Then $T=\\sum_{i=1}^{N}X_{i}$ with $N=250$, and $P(X_{i}=4)=p=0.75$, $P(X_{i}=-2)=1-p$.\n\nCompute the mean and variance of $X_{i}$:\n$$\n\\mathbb{E}[X_{i}]=4p-2(1-p)=6p-2,\\quad \\mathbb{E}[X_{i}^{2}]=16p+4(1-p)=4+12p.\n$$\nThus\n$$\n\\operatorname{Var}(X_{i})=\\mathbb{E}[X_{i}^{2}]-(\\mathbb{E}[X_{i}])^{2}=(4+12p)-(6p-2)^{2}.\n$$\nWith $p=\\frac{3}{4}$,\n$$\n\\mathbb{E}[X_{i}]=\\frac{5}{2},\\quad \\mathbb{E}[X_{i}^{2}]=13,\\quad \\operatorname{Var}(X_{i})=13-\\left(\\frac{5}{2}\\right)^{2}=\\frac{27}{4}.\n$$\nTherefore,\n$$\n\\mathbb{E}[T]=N\\,\\mathbb{E}[X_{i}]=250\\cdot\\frac{5}{2}=625,\\quad \\operatorname{Var}(T)=N\\,\\operatorname{Var}(X_{i})=250\\cdot\\frac{27}{4}=\\frac{3375}{2}.\n$$\n\nWe apply Bernstein’s inequality to the centered variables $Y_{i}=X_{i}-\\mathbb{E}[X_{i}]$, which are independent, mean zero, and satisfy the almost-sure bound\n$$\n|Y_{i}|\\leq \\max\\{|4-\\tfrac{5}{2}|,|-2-\\tfrac{5}{2}|\\}=\\frac{9}{2}.\n$$\nLet $\\sigma^{2}=\\sum_{i=1}^{N}\\operatorname{Var}(Y_{i})=\\operatorname{Var}(T)=\\frac{3375}{2}$ and $c=\\frac{9}{2}$. Bernstein’s inequality gives, for any $t>0$,\n$$\nP\\left(|T-\\mathbb{E}[T]|\\geq t\\right)\\leq 2\\exp\\left(-\\frac{t^{2}}{2\\sigma^{2}+\\frac{2}{3}ct}\\right).\n$$\nWith $t=150$, we obtain\n$$\nP\\left(|T-\\mathbb{E}[T]|>150\\right)\\leq 2\\exp\\left(-\\frac{150^{2}}{2\\cdot\\frac{3375}{2}+\\frac{2}{3}\\cdot\\frac{9}{2}\\cdot 150}\\right)\n=2\\exp\\left(-\\frac{22500}{3375+450}\\right)\n=2\\exp\\left(-\\frac{100}{17}\\right).\n$$\nNumerically, this equals approximately $0.00558$ when rounded to three significant figures.", "answer": "$$\\boxed{0.00558}$$", "id": "1345821"}, {"introduction": "Moving beyond simple sums, this exercise delves into the world of modern statistics and data science. We will use Bernstein's inequality to analyze the stability of the bootstrap mean, a fundamental technique for assessing uncertainty in data analysis. This practice will help you understand how concentration inequalities provide rigorous theoretical guarantees for computational statistical methods [@problem_id:1345793].", "problem": "A data scientist is analyzing the stability of an estimate derived from a dataset $S$ containing $n$ measurements, $\\{x_1, x_2, \\dots, x_n\\}$. It is known that every measurement $x_i$ is contained within the interval $[a, b]$, where $a$ and $b$ are known real numbers.\n\nLet $\\bar{x}$ be the sample mean of this dataset, i.e., $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i$. A key measure of the data's dispersion is its sum of squared deviations from the mean, which is given as $V = \\sum_{i=1}^n (x_i - \\bar{x})^2$.\n\nTo assess the stability of the mean, a bootstrap resample, $S^* = \\{X_1^*, \\dots, X_n^*\\}$, is a new dataset created by drawing $n$ samples from the original dataset $S$ uniformly and with replacement. The mean of this new dataset is the random variable $\\bar{X}^* = \\frac{1}{n}\\sum_{i=1}^n X_i^*$.\n\nDetermine a rigorous upper bound for the probability that the absolute difference between the bootstrap mean $\\bar{X}^*$ and the original sample mean $\\bar{x}$ exceeds a given positive tolerance $\\epsilon$. Your final answer should be a closed-form analytic expression in terms of $n$, $\\epsilon$, $V$, $a$, and $b$.", "solution": "Define the empirical distribution that places mass $1/n$ at each observed value $x_{i}$. A bootstrap resample $X_{1}^{*},\\dots,X_{n}^{*}$ is an independent sample from this empirical distribution, so each $X_{i}^{*}$ takes values in $\\{x_{1},\\dots,x_{n}\\}\\subseteq[a,b]$ with $\\mathbb{P}(X_{i}^{*}=x_{j})=1/n$.\n\nCompute the mean and variance of a single bootstrap draw, conditional on the fixed dataset $S$. By linearity of expectation,\n$$\n\\mathbb{E}[X_{i}^{*}\\mid S] \\;=\\; \\frac{1}{n}\\sum_{j=1}^{n} x_{j} \\;=\\; \\bar{x}.\n$$\nIts variance under the empirical distribution is\n$$\n\\operatorname{Var}(X_{i}^{*}\\mid S) \\;=\\; \\frac{1}{n}\\sum_{j=1}^{n}(x_{j}-\\bar{x})^{2} \\;=\\; \\frac{V}{n}.\n$$\nMoreover, since $x_{j}\\in[a,b]$ for all $j$, we have $X_{i}^{*}\\in[a,b]$, hence\n$$\n|X_{i}^{*}-\\bar{x}| \\;\\leq\\; \\max\\{\\bar{x}-a,\\,b-\\bar{x}\\} \\;\\leq\\; b-a.\n$$\n\nLet $Y_{i}=X_{i}^{*}-\\bar{x}$. Then $Y_{1},\\dots,Y_{n}$ are independent, centered random variables with $\\operatorname{Var}(Y_{i}\\mid S)=V/n$ and $|Y_{i}|\\leq b-a$ almost surely. We want to bound\n$$\n\\mathbb{P}\\big(|\\bar{X}^{*}-\\bar{x}|\\geq\\epsilon\\mid S\\big)\n\\;=\\;\n\\mathbb{P}\\!\\left(\\left|\\frac{1}{n}\\sum_{i=1}^{n}Y_{i}\\right|\\geq\\epsilon \\,\\middle|\\, S\\right).\n$$\n\nApply Bernstein’s inequality for independent, mean-zero, bounded variables: if $|Y_{i}|\\leq M$ almost surely and $\\sum_{i=1}^{n}\\operatorname{Var}(Y_{i})=n\\sigma^{2}$, then for all $\\epsilon>0$,\n$$\n\\mathbb{P}\\!\\left(\\left|\\frac{1}{n}\\sum_{i=1}^{n}Y_{i}\\right|\\geq\\epsilon\\right)\n\\;\\leq\\;\n2\\exp\\!\\left(-\\frac{n\\epsilon^{2}}{2\\sigma^{2}+\\frac{2}{3}M\\epsilon}\\right).\n$$\nHere, $\\sigma^{2}=V/n$ and $M=b-a$. Substituting gives the conditional bound\n$$\n\\mathbb{P}\\big(|\\bar{X}^{*}-\\bar{x}|\\geq\\epsilon\\mid S\\big)\n\\;\\leq\\;\n2\\exp\\!\\left(-\\frac{n\\epsilon^{2}}{\\frac{2V}{n}+\\frac{2}{3}(b-a)\\epsilon}\\right)\n\\;=\\;\n2\\exp\\!\\left(-\\frac{n^{2}\\epsilon^{2}}{2V+\\frac{2}{3}n(b-a)\\epsilon}\\right).\n$$\nSince the randomness is only over the bootstrap resampling given $S$, this furnishes the desired rigorous upper bound in terms of $n$, $\\epsilon$, $V$, $a$, and $b$.", "answer": "$$\\boxed{2\\exp\\!\\left(-\\frac{n\\epsilon^{2}}{\\frac{2V}{n}+\\frac{2}{3}(b-a)\\epsilon}\\right)}$$", "id": "1345793"}, {"introduction": "Our final practice extends the concept from one dimension to many, a crucial step for applications in fields like machine learning and physics. We will tackle the problem of locating the center of mass of points randomly sampled from a high-dimensional object. This exercise introduces the powerful union bound technique, demonstrating how to adapt scalar inequalities to reason about vector quantities [@problem_id:1345826].", "problem": "Consider a collection of $n$ points, $X_1, X_2, \\ldots, X_n$, sampled independently and uniformly from a $d$-dimensional solid ball of radius $R$ centered at the origin, denoted $B_d(R)$. The empirical center of mass of these points is given by the sample mean $\\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^n X_i$. By symmetry, the true center of mass (or centroid) of the ball is the origin, $\\mu = (0, \\ldots, 0)$.\n\nWe are interested in the probability that the empirical center of mass deviates from the true center of mass by a distance of at least $\\epsilon > 0$. Your task is to derive an upper bound for this probability, $P(\\|\\bar{X}_n\\| \\ge \\epsilon)$, where $\\|\\cdot\\|$ denotes the standard Euclidean norm in $\\mathbb{R}^d$.\n\nTo aid in your derivation, you are given the following two facts:\n1.  For a random vector $X$ chosen uniformly from $B_d(R)$, the expected value of its squared Euclidean norm is $\\mathbb{E}[\\|X\\|^2] = \\frac{d}{d+2}R^2$.\n2.  **Bernstein's Inequality (scalar version):** Let $Y_1, \\ldots, Y_n$ be independent scalar random variables such that for all $i$, $\\mathbb{E}[Y_i] = 0$ and $|Y_i| \\le M$ almost surely for some constant $M > 0$. Then for any $t > 0$, the following bound holds:\n    $$P\\left(\\left|\\sum_{i=1}^n Y_i\\right| \\ge t\\right) \\le 2 \\exp\\left( - \\frac{t^2}{2(V + Mt/3)} \\right)$$\n    where $V = \\sum_{i=1}^n \\mathbb{E}[Y_i^2]$.\n\nDerive a closed-form upper bound for $P(\\|\\bar{X}_n\\| \\ge \\epsilon)$ in terms of $n, d, R$, and $\\epsilon$.", "solution": "Let $X_{1},\\ldots,X_{n}$ be i.i.d. uniform on $B_{d}(R)$ with mean $\\mu=0$ and sample mean $\\bar{X}_{n}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}$. We seek an upper bound on $P(\\|\\bar{X}_{n}\\|\\ge \\epsilon)$.\n\nFirst, relate the Euclidean norm to coordinates. For any $x\\in\\mathbb{R}^{d}$,\n$$\n\\|x\\|^{2}=\\sum_{j=1}^{d}x_{j}^{2}\\,.\n$$\nHence, if $\\|x\\|\\ge \\epsilon$, then there exists some coordinate $j$ with $|x_{j}|\\ge \\epsilon/\\sqrt{d}$. Therefore,\n$$\nP(\\|\\bar{X}_{n}\\|\\ge \\epsilon)\\le \\sum_{j=1}^{d}P\\left(|\\bar{X}_{n}^{(j)}|\\ge \\frac{\\epsilon}{\\sqrt{d}}\\right)=d\\cdot P\\left(|\\bar{X}_{n}^{(1)}|\\ge \\frac{\\epsilon}{\\sqrt{d}}\\right),\n$$\nby symmetry of coordinates.\n\nFix a coordinate, say the first. Let $Y_{i}=X_{i}^{(1)}$. Then $\\mathbb{E}[Y_{i}]=0$ (by symmetry about the origin) and $|Y_{i}|\\le R$ almost surely. To apply Bernstein’s inequality, compute $V=\\sum_{i=1}^{n}\\mathbb{E}[Y_{i}^{2}]$. Using the given fact $\\mathbb{E}[\\|X\\|^{2}]=\\frac{d}{d+2}R^{2}$ and symmetry,\n$$\n\\mathbb{E}[Y_{i}^{2}]=\\mathbb{E}[X_{1}^{2}]=\\frac{1}{d}\\mathbb{E}[\\|X\\|^{2}]=\\frac{R^{2}}{d+2},\n$$\nso\n$$\nV=\\sum_{i=1}^{n}\\mathbb{E}[Y_{i}^{2}]=\\frac{nR^{2}}{d+2}.\n$$\nApply Bernstein’s inequality to $S_{n}=\\sum_{i=1}^{n}Y_{i}$ with $M=R$ and any $t>0$:\n$$\nP(|S_{n}|\\ge t)\\le 2\\exp\\left(-\\frac{t^{2}}{2\\left(V+Mt/3\\right)}\\right)\n=2\\exp\\left(-\\frac{t^{2}}{2\\left(\\frac{nR^{2}}{d+2}+\\frac{Rt}{3}\\right)}\\right).\n$$\nSince $\\bar{X}_{n}^{(1)}=\\frac{1}{n}S_{n}$, choose $t=n\\delta$ to bound $P(|\\bar{X}_{n}^{(1)}|\\ge \\delta)$:\n$$\nP\\left(|\\bar{X}_{n}^{(1)}|\\ge \\delta\\right)=P\\left(|S_{n}|\\ge n\\delta\\right)\n\\le 2\\exp\\left(-\\frac{n\\delta^{2}}{2\\left(\\frac{R^{2}}{d+2}+\\frac{R\\delta}{3}\\right)}\\right).\n$$\nSet $\\delta=\\epsilon/\\sqrt{d}$ to match the earlier reduction:\n$$\nP\\left(|\\bar{X}_{n}^{(1)}|\\ge \\frac{\\epsilon}{\\sqrt{d}}\\right)\n\\le 2\\exp\\left(-\\frac{n(\\epsilon^{2}/d)}{2\\left(\\frac{R^{2}}{d+2}+\\frac{R\\epsilon}{3\\sqrt{d}}\\right)}\\right)\n=2\\exp\\left(-\\frac{n\\epsilon^{2}}{2\\left(\\frac{dR^{2}}{d+2}+\\frac{R\\epsilon\\sqrt{d}}{3}\\right)}\\right).\n$$\nFinally, apply the union bound over the $d$ coordinates:\n$$\nP(\\|\\bar{X}_{n}\\|\\ge \\epsilon)\\le d\\cdot 2\\exp\\left(-\\frac{n\\epsilon^{2}}{2\\left(\\frac{dR^{2}}{d+2}+\\frac{R\\epsilon\\sqrt{d}}{3}\\right)}\\right)\n=2d\\,\\exp\\left(-\\frac{n\\epsilon^{2}}{2\\left(\\frac{dR^{2}}{d+2}+\\frac{R\\epsilon\\sqrt{d}}{3}\\right)}\\right).\n$$\nThis is a closed-form upper bound in terms of $n$, $d$, $R$, and $\\epsilon$.", "answer": "$$\\boxed{2d\\,\\exp\\!\\left(-\\frac{n\\epsilon^{2}}{2\\left(\\frac{dR^{2}}{d+2}+\\frac{R\\epsilon\\sqrt{d}}{3}\\right)}\\right)}$$", "id": "1345826"}]}