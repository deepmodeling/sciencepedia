{"hands_on_practices": [{"introduction": "The Strong Law of Large Numbers (SLLN) provides a bridge between probability and the physical world. This first practice offers a beautiful geometric illustration of this principle. By considering points chosen randomly within a unit disk, we can use the SLLN to predict the long-term average distance of these points from the center, a task that might seem complex at first glance. This exercise [@problem_id:1957055] demonstrates how a seemingly random process has a predictable and stable long-term average.", "problem": "Consider a sequence of points, $P_1, P_2, P_3, \\dots$, that are chosen independently from the same distribution, formally known as being independent and identically distributed (i.i.d.). The points are selected from a uniform distribution over the unit disk, defined as the set of all points $(x, y)$ in a Cartesian plane satisfying the inequality $x^2 + y^2 \\le 1$.\n\nFor each point $P_n$ in the sequence, let $D_n$ be its Euclidean distance from the center of the disk, the origin $(0,0)$. We are interested in the long-term behavior of the average of these distances. Let $\\bar{D}_N$ be the arithmetic mean of the first $N$ distances, that is, $\\bar{D}_N = \\frac{1}{N} \\sum_{n=1}^{N} D_n$.\n\nDetermine the value to which the average distance $\\bar{D}_N$ converges almost surely (i.e., with probability 1) as the number of points $N$ approaches infinity. Present your answer as a single closed-form analytic expression.", "solution": "Let $\\{P_{n}\\}_{n\\geq 1}$ be i.i.d. uniformly distributed on the unit disk $\\{(x,y): x^{2}+y^{2}\\leq 1\\}$, and let $D_{n}$ be the Euclidean distance from $P_{n}$ to the origin. The strong law of large numbers states that for i.i.d. random variables with finite mean,\n$$\n\\bar{D}_{N}=\\frac{1}{N}\\sum_{n=1}^{N}D_{n}\\xrightarrow[\\;N\\to\\infty\\;]{\\text{a.s.}}\\mathbb{E}[D_{1}],\n$$\nprovided $\\mathbb{E}[|D_{1}|]\\infty$. Since $0\\leq D_{1}\\leq 1$, we have $\\mathbb{E}[|D_{1}|]\\infty$. Therefore, it suffices to compute $\\mathbb{E}[D_{1}]$.\n\nLet $R$ denote the distance from the origin for a point uniformly distributed on the unit disk. Then\n$$\n\\mathbb{P}(R\\leq r)=\\frac{\\text{area of the disk of radius }r}{\\text{area of the unit disk}}=\\frac{\\pi r^{2}}{\\pi}=r^{2},\\quad 0\\leq r\\leq 1.\n$$\nDifferentiating gives the radial density\n$$\nf_{R}(r)=\\frac{d}{dr}\\big(r^{2}\\big)=2r,\\quad 0\\leq r\\leq 1.\n$$\nHence,\n$$\n\\mathbb{E}[R]=\\int_{0}^{1}r\\,f_{R}(r)\\,dr=\\int_{0}^{1}r\\cdot 2r\\,dr=\\int_{0}^{1}2r^{2}\\,dr=\\left.\\frac{2}{3}r^{3}\\right|_{0}^{1}=\\frac{2}{3}.\n$$\nBy the strong law of large numbers,\n$$\n\\bar{D}_{N}\\xrightarrow[\\;N\\to\\infty\\;]{\\text{a.s.}}\\mathbb{E}[D_{1}]=\\frac{2}{3}.\n$$\nThus the almost sure limit of the average distance is $\\frac{2}{3}$.", "answer": "$$\\boxed{\\frac{2}{3}}$$", "id": "1957055"}, {"introduction": "Real-world systems often involve outcomes that depend on multiple random factors. This practice explores such a scenario, taken from a manufacturing context. We will analyze the average area of electronic chips whose width and height both vary randomly. This problem [@problem_id:1406770] challenges you to apply the SLLN to a sequence of random variables that are themselves products of other random variables, reinforcing how to combine expectations to find the convergent limit.", "problem": "A factory manufactures rectangular electronic chips. Due to variations in the fabrication process, the dimensions of each chip are random. Let the sequence of pairs $(W_i, H_i)$ for $i=1, 2, 3, \\dots$ represent the width and height of the $i$-th chip produced. These pairs are independent and identically distributed (i.i.d.).\n\nFor any given chip $i$, the width $W_i$ and height $H_i$ are independent random variables.\nThe width $W_i$ (in millimeters) is a discrete random variable with the probability mass function given by $P(W_i = 1) = \\frac{1}{3}$ and $P(W_i = 4) = \\frac{2}{3}$.\nThe height $H_i$ (in millimeters) is a continuous random variable with the probability density function (PDF) given by:\n$$ f_H(h) = \\begin{cases} 2h  \\text{for } 0 \\le h \\le 1 \\\\ 0  \\text{otherwise} \\end{cases} $$\nLet $\\bar{A}_n = \\frac{1}{n} \\sum_{i=1}^{n} W_i H_i$ be the average area of the first $n$ chips manufactured. Determine the value that $\\bar{A}_n$ converges to almost surely as $n$ approaches infinity.", "solution": "Define $X_{i} = W_{i}H_{i}$. Because the pairs $(W_{i},H_{i})$ are i.i.d. and, for each $i$, $W_{i}$ and $H_{i}$ are independent, the sequence $\\{X_{i}\\}_{i \\ge 1}$ is i.i.d. Moreover, $0 \\le W_{i} \\le 4$ and $0 \\le H_{i} \\le 1$ almost surely, so $0 \\le X_{i} \\le 4$ almost surely and hence $\\mathbb{E}[|X_{1}|]  \\infty$. By the Strong Law of Large Numbers,\n$$\n\\bar{A}_{n}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}\\xrightarrow{\\text{a.s.}}\\mathbb{E}[X_{1}] = \\mathbb{E}[W_{1}H_{1}].\n$$\nUsing independence of $W_{1}$ and $H_{1}$, \n$$\n\\mathbb{E}[W_{1}H_{1}] = \\mathbb{E}[W_{1}]\\,\\mathbb{E}[H_{1}].\n$$\nCompute $\\mathbb{E}[W_{1}]$ from its probability mass function:\n$$\n\\mathbb{E}[W_{1}] = 1 \\cdot \\frac{1}{3} + 4 \\cdot \\frac{2}{3} = \\frac{1}{3} + \\frac{8}{3} = 3.\n$$\nCompute $\\mathbb{E}[H_{1}]$ from its probability density function $f_{H}(h)=2h$ on $[0,1]$:\n$\n\\mathbb{E}[H_{1}] = \\int_{0}^{1} h \\, f_{H}(h)\\, dh = \\int_{0}^{1} h \\cdot 2h \\, dh = 2 \\int_{0}^{1} h^{2}\\, dh = 2 \\cdot \\frac{1}{3} = \\frac{2}{3}.\n$\nTherefore,\n$$\n\\mathbb{E}[W_{1}H_{1}] = \\mathbb{E}[W_{1}]\\,\\mathbb{E}[H_{1}] = 3 \\cdot \\frac{2}{3} = 2.\n$$\nBy the Strong Law of Large Numbers, \n$$\n\\bar{A}_{n} \\xrightarrow{\\text{a.s.}} 2.\n$$", "answer": "$$\\boxed{2}$$", "id": "1406770"}, {"introduction": "The SLLN is a cornerstone of modern statistics and computational methods like Monte Carlo simulations. This final practice moves a step beyond the convergence of the sample mean itself. Here, we investigate the long-term behavior of a *function* of the sample mean, a scenario commonly encountered when evaluating the consistency of statistical estimators. This problem [@problem_id:1957105] introduces the power of the Continuous Mapping Theorem, which works in tandem with the SLLN to guarantee the convergence of more complex quantities.", "problem": "A company specializes in running large-scale Monte Carlo simulations to price complex financial derivatives. A key component of their simulation involves repeatedly modeling a binary event, such as a stock price moving up or down. For a particular simulation, this is modeled as a sequence of experiments. In each experiment $i$, a total of $m$ independent Bernoulli trials are run, each with a success probability of $p$. Let the random variable $X_i$ be the total number of successes in experiment $i$.\n\nThe experiments are independent of each other, so the sequence of counts $X_1, X_2, \\ldots, X_n, \\ldots$ can be modeled as a sequence of independent and identically distributed (i.i.d.) random variables, with each $X_i$ following a binomial distribution with parameters $m$ and $p$.\n\nAnalysts at the company are studying the convergence properties of their estimators. They define the average proportion of successes over the first $n$ experiments as:\n$$ \\bar{p}_n = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{X_i}{m} $$\nThey are interested in a quantity related to the variance of the underlying process. Determine the value to which the quantity $V_n = \\bar{p}_n (1 - \\bar{p}_n)$ converges almost surely as the number of experiments $n$ approaches infinity. Your answer should be a symbolic expression in terms of the given parameters.", "solution": "Let $X_{i} \\sim \\text{Binomial}(m,p)$ be i.i.d. and define $Y_{i} = \\frac{X_{i}}{m}$. Then the average proportion is the sample mean\n$$\n\\bar{p}_{n} = \\frac{1}{n}\\sum_{i=1}^{n} Y_{i}.\n$$\nWe compute the first two moments of $Y_{i}$ using linearity of expectation and the variance scaling rule:\n$$\n\\mathbb{E}[Y_{i}] = \\frac{1}{m}\\mathbb{E}[X_{i}] = \\frac{1}{m}\\cdot m p = p,\n$$\n$$\n\\operatorname{Var}(Y_{i}) = \\frac{1}{m^{2}}\\operatorname{Var}(X_{i}) = \\frac{1}{m^{2}}\\cdot m p(1-p) = \\frac{p(1-p)}{m}.\n$$\nThus $\\{Y_{i}\\}$ are i.i.d. with finite mean $\\mathbb{E}[Y_{i}]=p$. By the Strong Law of Large Numbers,\n$$\n\\bar{p}_{n} = \\frac{1}{n}\\sum_{i=1}^{n} Y_{i} \\xrightarrow{\\text{a.s.}} \\mathbb{E}[Y_{1}] = p.\n$$\n\nDefine the continuous function $g(x) = x(1-x)$. By the continuous mapping theorem applied to almost sure convergence,\n$$\nV_{n} = g(\\bar{p}_{n}) = \\bar{p}_{n}\\bigl(1-\\bar{p}_{n}\\bigr) \\xrightarrow{\\text{a.s.}} g(p) = p(1-p).\n$$\n\nTherefore, $V_{n}$ converges almost surely to $p(1-p)$.", "answer": "$$\\boxed{p(1-p)}$$", "id": "1957105"}]}