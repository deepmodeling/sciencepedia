{"hands_on_practices": [{"introduction": "Before tackling complex probability distributions, it is instructive to understand how the Cumulant Generating Function (CGF) behaves in the simplest possible scenario: a deterministic process with a single, constant outcome. This foundational exercise grounds your understanding in the basic definitions of the Moment Generating Function (MGF) and CGF [@problem_id:1354912]. By working through this case, you will gain an intuitive feel for why all cumulants beyond the first (the mean) are zero for a constant value.", "problem": "A random variable $X$ is defined to model a perfectly deterministic process. The outcome of this process is always a specific real-valued constant, $c$. Consequently, the probability mass function of $X$ is given by $P(X=c) = 1$.\n\nThe Moment Generating Function (MGF) of a random variable $Y$ is defined as $M_Y(t) = \\mathbb{E}[\\exp(tY)]$, where $\\mathbb{E}[\\cdot]$ denotes the expectation value. The Cumulant Generating Function (CGF), denoted $K_Y(t)$, is defined as the natural logarithm of the MGF, i.e., $K_Y(t) = \\ln(M_Y(t))$.\n\nDetermine the Cumulant Generating Function, $K_X(t)$, for the random variable $X$. Express your answer in terms of $t$ and the constant $c$.", "solution": "The random variable satisfies $P(X=c)=1$, so $X=c$ almost surely. For any measurable function $g$, this implies $\\mathbb{E}[g(X)]=g(c)$. Using the definition of the moment generating function,\n$$\nM_{X}(t)=\\mathbb{E}[\\exp(tX)].\n$$\nSince $X=c$ almost surely, $\\exp(tX)=\\exp(tc)$ almost surely, and the expectation of a constant equals that constant. Therefore,\n$$\nM_{X}(t)=\\exp(tc).\n$$\nBy definition, the cumulant generating function is the natural logarithm of the MGF:\n$$\nK_{X}(t)=\\ln(M_{X}(t))=\\ln(\\exp(tc))=tc.\n$$\nThus, the CGF exists for all real $t$ and is linear in $t$ with slope $c$.", "answer": "$$\\boxed{tc}$$", "id": "1354912"}, {"introduction": "A primary application of the CGF is its power to simplify the calculation of a distribution's moments. This practice provides a direct demonstration of this utility by asking you to find the mean of the Gamma distribution, a cornerstone model in statistics for waiting times and other continuous phenomena [@problem_id:1354878]. Working through this problem highlights how differentiation of the CGF can be a more direct path to finding the expected value than performing integration over the probability density function.", "problem": "Let $X$ be a continuous random variable that follows a Gamma distribution with a shape parameter $\\alpha > 0$ and a rate parameter $\\beta > 0$.\n\nThe Cumulant Generating Function (CGF) for the random variable $X$ is given by the expression:\n$$ K_X(t) = \\alpha \\ln\\left(\\frac{\\beta}{\\beta-t}\\right) \\quad \\text{for } t  \\beta $$\nUsing the properties of the CGF, determine the expected value (mean) of the random variable $X$, denoted as $\\mathbb{E}[X]$. Express your answer in terms of $\\alpha$ and $\\beta$.", "solution": "We use the defining property of the cumulant generating function: for a random variable $X$ with CGF $K_{X}(t)=\\ln M_{X}(t)$, the mean is given by the first derivative of $K_{X}(t)$ at $t=0$, namely $\\mathbb{E}[X]=K_{X}'(0)$.\n\nGiven $K_{X}(t)=\\alpha \\ln\\left(\\frac{\\beta}{\\beta-t}\\right)$ for $t\\beta$ and $\\beta0$, $t=0$ lies in the domain, so differentiation at $t=0$ is valid. Differentiate $K_{X}(t)$ with respect to $t$:\n$$\nK_{X}'(t)\n=\\alpha \\frac{\\mathrm{d}}{\\mathrm{d}t}\\left[\\ln\\beta-\\ln(\\beta-t)\\right]\n=\\alpha\\left[0-\\frac{1}{\\beta-t}\\cdot(-1)\\right]\n=\\alpha\\,\\frac{1}{\\beta-t}.\n$$\nEvaluating at $t=0$ gives\n$$\n\\mathbb{E}[X]=K_{X}'(0)=\\alpha\\,\\frac{1}{\\beta}=\\frac{\\alpha}{\\beta}.\n$$", "answer": "$$\\boxed{\\frac{\\alpha}{\\beta}}$$", "id": "1354878"}, {"introduction": "The true elegance of CGFs is revealed when we consider sums of independent random variables, as their cumulants simply add up. The CGF presented in this problem is, in fact, that of a sum of two independent random variables (a Poisson and a Bernoulli variable). This exercise challenges you to calculate both the mean ($\\kappa_1$) and the variance ($\\kappa_2$) by differentiating the given CGF, reinforcing your calculation skills and providing a concrete example of the additivity property of cumulants [@problem_id:1354908].", "problem": "In a particular stochastic model, the number of events $X$ is a random variable whose statistical properties are encapsulated by its Cumulant Generating Function (CGF), $K_X(t)$. The CGF is defined as the natural logarithm of the Moment Generating Function, $K_X(t) = \\ln(\\mathbb{E}[\\exp(tX)])$.\n\nThe CGF for the random variable $X$ is given by the expression:\n$$K_X(t) = \\lambda(\\exp(t) - 1) + \\ln(1 - p + p\\exp(t))$$\nwhere $\\lambda$ is a positive real constant representing an event rate, and $p$ is a probability satisfying $0  p  1$.\n\nThe cumulants, $\\kappa_r$, of the random variable $X$ are found by taking successive derivatives of the CGF and evaluating them at $t=0$, according to the formula $\\kappa_r = \\frac{d^r K_X(t)}{dt^r}\\bigg|_{t=0}$. The first cumulant, $\\kappa_1$, corresponds to the mean of the distribution, and the second cumulant, $\\kappa_2$, corresponds to the variance.\n\nCalculate the first cumulant, $\\kappa_1$, and the second cumulant, $\\kappa_2$, for the random variable $X$. Present your answer as a row matrix containing the two expressions, $[\\kappa_1 \\quad \\kappa_2]$.", "solution": "We use the definition of the cumulant generating function $K_{X}(t)=\\ln\\left(\\mathbb{E}[\\exp(tX)]\\right)$ and the fact that the $r$-th cumulant is $\\kappa_{r}=K_{X}^{(r)}(0)$. The given CGF is\n$$\nK_{X}(t)=\\lambda\\left(\\exp(t)-1\\right)+\\ln\\left(1-p+p\\exp(t)\\right).\n$$\nFirst derivative:\nLet $u(t)=1-p+p\\exp(t)$. Then $u'(t)=p\\exp(t)$. Using the chain rule for $\\ln(u)$, we have\n$$\nK_{X}'(t)=\\lambda\\exp(t)+\\frac{u'(t)}{u(t)}=\\lambda\\exp(t)+\\frac{p\\exp(t)}{1-p+p\\exp(t)}.\n$$\nEvaluating at $t=0$ gives $\\exp(0)=1$ and $1-p+p\\exp(0)=1$, hence\n$$\n\\kappa_{1}=K_{X}'(0)=\\lambda+\\frac{p}{1}=\\lambda+p.\n$$\nSecond derivative:\nDifferentiate again. Using $u''(t)=p\\exp(t)$ and the identity $\\frac{d^{2}}{dt^{2}}\\ln(u)=\\frac{u''}{u}-\\left(\\frac{u'}{u}\\right)^{2}$,\n$$\nK_{X}''(t)=\\lambda\\exp(t)+\\frac{u''(t)}{u(t)}-\\left(\\frac{u'(t)}{u(t)}\\right)^{2}\n=\\lambda\\exp(t)+\\frac{p\\exp(t)}{1-p+p\\exp(t)}-\\left(\\frac{p\\exp(t)}{1-p+p\\exp(t)}\\right)^{2}.\n$$\nEvaluating at $t=0$ gives\n$$\n\\kappa_{2}=K_{X}''(0)=\\lambda+\\frac{p}{1}-\\left(\\frac{p}{1}\\right)^{2}=\\lambda+p-p^{2}=\\lambda+p(1-p).\n$$\nTherefore, the requested row matrix is $[\\kappa_{1}\\ \\kappa_{2}]=[\\lambda+p\\ \\ \\lambda+p(1-p)]$.", "answer": "$$\\boxed{\\begin{pmatrix}\\lambda+p  \\lambda+p(1-p)\\end{pmatrix}}$$", "id": "1354908"}]}