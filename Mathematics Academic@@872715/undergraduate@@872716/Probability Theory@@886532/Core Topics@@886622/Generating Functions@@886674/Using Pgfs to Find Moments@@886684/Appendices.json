{"hands_on_practices": [{"introduction": "The first step in mastering Probability Generating Functions (PGFs) is to understand how to construct them from a basic probability distribution. This exercise provides a hands-on opportunity to build the PGF for a simple discrete uniform distribution and then apply the core derivative techniques to find its mean and variance. By working through this fundamental problem [@problem_id:1409525], you will solidify the mechanical link between a probability mass function, its corresponding PGF, and the calculation of key statistical moments.", "problem": "A simple electronic device models a random process where it can occupy one of four discrete energy states, labeled by the integers 1, 2, 3, and 4. At any given moment, the device is equally likely to be in any one of these four states. Let the random variable $X$ denote the energy state of the device observed at a random instant.\n\nDetermine the Probability Generating Function (PGF), $G_X(z)$, for the random variable $X$, and also determine the variance of $X$, denoted $\\text{Var}(X)$.\n\nYour final answer should be a composite expression containing the polynomial for $G_X(z)$ followed by the value of $\\text{Var}(X)$. The variance must be expressed as an exact fraction.", "solution": "The random variable $X$ takes values in the set $\\{1,2,3,4\\}$ with equal probability for each state. Therefore, for $k \\in \\{1,2,3,4\\}$, we have $\\mathbb{P}(X=k)=\\frac{1}{4}$.\n\nBy definition, the probability generating function (PGF) of $X$ is\n$$\nG_{X}(z)=\\mathbb{E}[z^{X}]=\\sum_{k=0}^{\\infty}\\mathbb{P}(X=k)z^{k}.\n$$\nSince $X$ takes values only in $\\{1,2,3,4\\}$, this reduces to\n$$\nG_{X}(z)=\\sum_{k=1}^{4}\\mathbb{P}(X=k)z^{k}=\\frac{1}{4}\\left(z+z^{2}+z^{3}+z^{4}\\right).\n$$\n\nTo compute the variance, we use $\\text{Var}(X)=\\mathbb{E}[X^{2}]-(\\mathbb{E}[X])^{2}$.\nFirst compute the mean:\n$$\n\\mathbb{E}[X]=\\sum_{k=1}^{4}k\\cdot \\mathbb{P}(X=k)=\\frac{1}{4}\\sum_{k=1}^{4}k=\\frac{1}{4}\\cdot\\frac{4\\cdot 5}{2}=\\frac{10}{4}=\\frac{5}{2}.\n$$\nNext compute the second moment:\n$$\n\\mathbb{E}[X^{2}]=\\sum_{k=1}^{4}k^{2}\\cdot \\mathbb{P}(X=k)=\\frac{1}{4}\\sum_{k=1}^{4}k^{2}=\\frac{1}{4}\\cdot\\frac{4\\cdot 5\\cdot 9}{6}=\\frac{30}{4}=\\frac{15}{2}.\n$$\nTherefore,\n$$\n\\text{Var}(X)=\\mathbb{E}[X^{2}]-(\\mathbb{E}[X])^{2}=\\frac{15}{2}-\\left(\\frac{5}{2}\\right)^{2}=\\frac{30}{4}-\\frac{25}{4}=\\frac{5}{4}.\n$$\nThus the PGF and variance are as stated.", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{4}\\left(z+z^{2}+z^{3}+z^{4}\\right) & \\frac{5}{4}\\end{pmatrix}}$$", "id": "1409525"}, {"introduction": "Many real-world stochastic systems, such as population growth or particle cascades, have a recursive nature. This practice explores such a scenario, where the PGF is not given explicitly but is defined through a functional equation that reflects the process's self-similarity [@problem_id:1409566]. You will learn a powerful technique to find the expected value by differentiating the entire equation, bypassing the need to solve for the PGF itself and demonstrating an elegant application for analyzing branching processes.", "problem": "A simplified model for a signal cascade within a specialized detector is described as follows. A single primary particle enters the detector. Two mutually exclusive events can occur:\n1.  With a probability $p$, the particle is directly absorbed, generating a fixed number $k$ of detectable signals.\n2.  With a probability $1-p$, the particle fissions into two identical daughter particles. Each of these daughter particles then independently initiates a new cascade process, behaving exactly like the original primary particle.\n\nLet $X$ be the random variable representing the total number of detectable signals generated from a single initial primary particle. The system is designed such that the probability $p$ satisfies $1/2 < p < 1$, and $k$ is a positive integer. This ensures that the cascade process does not, on average, grow indefinitely.\n\nDetermine the expected value of $X$, denoted $\\mathbb{E}[X]$, in terms of $p$ and $k$.", "solution": "Let $G_X(s)$ be the Probability Generating Function (PGF) for the random variable $X$, which is defined as $G_X(s) = \\mathbb{E}[s^X]$. We can construct a functional equation for $G_X(s)$ by conditioning on the first event in the cascade.\n\nAccording to the problem description, there are two possibilities for the initial particle:\n1.  With probability $p$, the process terminates and generates $k$ signals. The random variable is a constant $k$. The PGF for this outcome is $s^k$.\n2.  With probability $1-p$, the particle fissions into two daughter particles. Each daughter particle generates a total number of signals, let's say $X_1$ and $X_2$. The total number of signals in this case is $X = X_1 + X_2$. Since each daughter particle behaves identically and independently to the original primary particle, $X_1$ and $X_2$ are independent and identically distributed random variables, with the same distribution as $X$. The PGF of a sum of independent random variables is the product of their individual PGFs. Therefore, the PGF for this outcome is $G_{X_1}(s)G_{X_2}(s) = G_X(s)G_X(s) = (G_X(s))^2$.\n\nUsing the law of total expectation (or by the properties of mixture distributions), the overall PGF for $X$ is the weighted average of the PGFs for each case:\n$$G_X(s) = p \\cdot s^k + (1-p) \\cdot (G_X(s))^2$$\n\nThe problem asks for the expected value of $X$, which can be found from the PGF using the relation $\\mathbb{E}[X] = G'_X(1)$. We do not need to solve the functional equation for $G_X(s)$ explicitly. Instead, we can differentiate the entire equation with respect to $s$. Using the product rule and chain rule for differentiation, we get:\n$$G_X'(s) = \\frac{d}{ds} \\left( p s^k + (1-p) (G_X(s))^2 \\right)$$\n$$G_X'(s) = p k s^{k-1} + (1-p) \\cdot 2 G_X(s) G_X'(s)$$\n\nNow, we evaluate this differentiated equation at $s=1$. We use two fundamental properties of any PGF:\n1.  $G_X(1) = \\sum_{n=0}^{\\infty} \\mathbb{P}(X=n) (1)^n = \\sum \\mathbb{P}(X=n) = 1$.\n2.  $G'_X(1) = \\mathbb{E}[X]$. Let's denote $\\mathbb{E}[X]$ by $\\mu$.\n\nSubstituting $s=1$, $G_X(1)=1$, and $G'_X(1)=\\mu$ into the differentiated equation:\n$$G_X'(1) = p k (1)^{k-1} + (1-p) \\cdot 2 G_X(1) G_X'(1)$$\n$$\\mu = p k + (1-p) \\cdot 2 \\cdot (1) \\cdot \\mu$$\n$$\\mu = pk + 2(1-p)\\mu$$\n\nNow, we solve this linear equation for $\\mu$:\n$$\\mu - 2(1-p)\\mu = pk$$\n$$\\mu(1 - 2 + 2p) = pk$$\n$$\\mu(2p - 1) = pk$$\n\nSince the problem states that $1/2 < p < 1$, we have $2p - 1 > 0$, so we can divide by $(2p-1)$ to find a unique, positive, and finite mean.\n$$\\mu = \\frac{pk}{2p-1}$$\n\nThis expression gives the expected total number of signals generated from a single initial particle.", "answer": "$$\\boxed{\\frac{pk}{2p-1}}$$", "id": "1409566"}, {"introduction": "Probability Generating Functions are remarkably versatile, extending beyond the standard calculation of mean and variance through differentiation. This advanced problem reveals a different facet of their power: using integration to find expectations of more complex functions of a random variable [@problem_id:1409515]. By tackling this exercise, you will discover an elegant method to compute quantities like $\\mathbb{E}[\\frac{1}{N+1}]$, a metric important in fields like network analysis, by relating the expectation to the definite integral of the PGF.", "problem": "In the study of communication networks, a stochastic model is proposed for the number of data packets, $N$, generated by a source during a specific time slot. The number of packets $N$ is a non-negative integer-valued random variable. Its statistical properties are fully characterized by its Probability Generating Function (PGF), defined as $G_N(s) = \\mathbb{E}[s^N]$, where $\\mathbb{E}[\\cdot]$ denotes the expectation operator.\n\nFor this particular model, the PGF is given by:\n$$\nG_N(s) = 1-p + \\frac{p(1-\\alpha)}{1-\\alpha s}\n$$\nwhere $p$ and $\\alpha$ are constant parameters of the model satisfying $0 < p < 1$ and $0 < \\alpha < 1$.\n\nA key performance metric for the network's quality of service is related to the processing latency of the packets. This metric is found to be mathematically equivalent to the expected value of the quantity $\\frac{1}{N+1}$.\n\nDetermine an expression for the expected value $\\mathbb{E}\\left[\\frac{1}{N+1}\\right]$ in terms of the parameters $p$ and $\\alpha$.", "solution": "The goal is to compute the expectation $\\mathbb{E}\\left[\\frac{1}{N+1}\\right]$ for a non-negative integer random variable $N$ whose Probability Generating Function (PGF) is given.\n\nLet $\\mathbb{P}(N=k)$ be the probability that the random variable $N$ takes the value $k$, where $k \\in \\{0, 1, 2, ...\\}$. The expectation of a function of $N$, say $f(N)$, is given by the definition:\n$$\n\\mathbb{E}[f(N)] = \\sum_{k=0}^{\\infty} f(k) \\mathbb{P}(N=k)\n$$\nIn this problem, the function is $f(N) = \\frac{1}{N+1}$. Substituting this into the definition of expectation, we get:\n$$\n\\mathbb{E}\\left[\\frac{1}{N+1}\\right] = \\sum_{k=0}^{\\infty} \\frac{1}{k+1} \\mathbb{P}(N=k)\n$$\nWe can express the term $\\frac{1}{k+1}$ as a definite integral:\n$$\n\\frac{1}{k+1} = \\int_{0}^{1} s^k ds\n$$\nSubstituting this integral representation into the summation for the expectation:\n$$\n\\mathbb{E}\\left[\\frac{1}{N+1}\\right] = \\sum_{k=0}^{\\infty} \\left( \\int_{0}^{1} s^k ds \\right) \\mathbb{P}(N=k)\n$$\nSince the terms in the sum are non-negative, we can interchange the order of summation and integration (by Tonelli's theorem):\n$$\n\\mathbb{E}\\left[\\frac{1}{N+1}\\right] = \\int_{0}^{1} \\left( \\sum_{k=0}^{\\infty} s^k \\mathbb{P}(N=k) \\right) ds\n$$\nWe recognize the expression inside the parentheses as the definition of the PGF of $N$, which is $G_N(s) = \\sum_{k=0}^{\\infty} \\mathbb{P}(N=k)s^k$. Therefore, we have established a general relationship between the desired expectation and the PGF:\n$$\n\\mathbb{E}\\left[\\frac{1}{N+1}\\right] = \\int_{0}^{1} G_N(s) ds\n$$\nNow, we use the specific PGF given in the problem:\n$$\nG_N(s) = 1-p + \\frac{p(1-\\alpha)}{1-\\alpha s}\n$$\nWe substitute this into our integral expression:\n$$\n\\mathbb{E}\\left[\\frac{1}{N+1}\\right] = \\int_{0}^{1} \\left( 1-p + \\frac{p(1-\\alpha)}{1-\\alpha s} \\right) ds\n$$\nWe can split the integral into two parts:\n$$\n\\mathbb{E}\\left[\\frac{1}{N+1}\\right] = \\int_{0}^{1} (1-p) ds + \\int_{0}^{1} \\frac{p(1-\\alpha)}{1-\\alpha s} ds\n$$\nThe first integral is straightforward:\n$$\n\\int_{0}^{1} (1-p) ds = [(1-p)s]_{0}^{1} = (1-p)(1) - (1-p)(0) = 1-p\n$$\nFor the second integral, we can pull out the constants:\n$$\n\\int_{0}^{1} \\frac{p(1-\\alpha)}{1-\\alpha s} ds = p(1-\\alpha) \\int_{0}^{1} \\frac{1}{1-\\alpha s} ds\n$$\nThe antiderivative of $\\frac{1}{1-\\alpha s}$ with respect to $s$ is $-\\frac{1}{\\alpha}\\ln(1-\\alpha s)$. So, we evaluate the definite integral:\n$$\np(1-\\alpha) \\left[ -\\frac{1}{\\alpha}\\ln(1-\\alpha s) \\right]_{0}^{1} = -\\frac{p(1-\\alpha)}{\\alpha} [\\ln(1-\\alpha s)]_{0}^{1}\n$$\n$$\n= -\\frac{p(1-\\alpha)}{\\alpha} \\left( \\ln(1-\\alpha(1)) - \\ln(1-\\alpha(0)) \\right)\n$$\n$$\n= -\\frac{p(1-\\alpha)}{\\alpha} (\\ln(1-\\alpha) - \\ln(1))\n$$\nSince $\\ln(1)=0$, this simplifies to:\n$$\n= -\\frac{p(1-\\alpha)}{\\alpha} \\ln(1-\\alpha)\n$$\nFinally, we combine the results of the two integrals to get the final answer:\n$$\n\\mathbb{E}\\left[\\frac{1}{N+1}\\right] = (1-p) + \\left( -\\frac{p(1-\\alpha)}{\\alpha} \\ln(1-\\alpha) \\right) = 1-p - \\frac{p(1-\\alpha)}{\\alpha} \\ln(1-\\alpha)\n$$", "answer": "$$\\boxed{1-p - \\frac{p(1-\\alpha)}{\\alpha}\\ln(1-\\alpha)}$$", "id": "1409515"}]}