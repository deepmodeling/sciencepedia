{"hands_on_practices": [{"introduction": "The primary purpose of the Moment Generating Function (MGF) is to provide a systematic way to calculate the moments of a random variable. This first exercise provides direct practice with this core skill. By differentiating a given MGF and evaluating the derivatives at $t=0$, you can find the raw moments and subsequently compute key statistics like the variance, as demonstrated in this problem concerning a model for signal noise [@problem_id:1409242].", "problem": "In a simplified model for noise in a digital communication channel, a random variable $N$ representing the noise signal is characterized by its moment generating function (MGF). The MGF of $N$ is given by the expression:\n$$ M_N(t) = \\exp(\\mu t) \\cosh(\\sigma t) $$\nHere, $\\mu$ represents a constant DC bias in the signal, and $\\sigma$ represents the amplitude of the noise fluctuations. For a particular measurement, the parameters are determined to be $\\mu = -5$ and $\\sigma = 7$.\nCalculate the variance of the noise, $\\operatorname{Var}(N)$.", "solution": "The variance can be obtained from the moment generating function $M_{N}(t)$ using the identities $E[N]=M_{N}'(0)$ and $\\operatorname{Var}(N)=M_{N}''(0)-\\left(M_{N}'(0)\\right)^{2}$. Given $M_{N}(t)=\\exp(\\mu t)\\cosh(\\sigma t)$, differentiate:\n$$\nM_{N}'(t)=\\frac{d}{dt}\\left(\\exp(\\mu t)\\cosh(\\sigma t)\\right)=\\exp(\\mu t)\\left(\\mu \\cosh(\\sigma t)+\\sigma \\sinh(\\sigma t)\\right).\n$$\nDifferentiate again:\n$$\nM_{N}''(t)=\\exp(\\mu t)\\left[\\mu\\left(\\mu \\cosh(\\sigma t)+\\sigma \\sinh(\\sigma t)\\right)+\\left(\\mu \\sigma \\sinh(\\sigma t)+\\sigma^{2}\\cosh(\\sigma t)\\right)\\right],\n$$\nwhich simplifies to\n$$\nM_{N}''(t)=\\exp(\\mu t)\\left((\\mu^{2}+\\sigma^{2})\\cosh(\\sigma t)+2\\mu \\sigma \\sinh(\\sigma t)\\right).\n$$\nEvaluate at $t=0$ using $\\exp(0)=1$, $\\cosh(0)=1$, and $\\sinh(0)=0$:\n$$\nM_{N}'(0)=\\mu,\\qquad M_{N}''(0)=\\mu^{2}+\\sigma^{2}.\n$$\nTherefore,\n$$\n\\operatorname{Var}(N)=M_{N}''(0)-\\left(M_{N}'(0)\\right)^{2}=(\\mu^{2}+\\sigma^{2})-\\mu^{2}=\\sigma^{2}.\n$$\nWith $\\mu=-5$ and $\\sigma=7$, this gives\n$$\n\\operatorname{Var}(N)=7^{2}=49.\n$$", "answer": "$$\\boxed{49}$$", "id": "1409242"}, {"introduction": "Building on direct calculation, this next problem explores a more powerful application of the MGF concept. Here, you are not given the explicit function for the MGF, but rather a differential equation that it satisfies [@problem_id:1409269]. This thought-provoking scenario demonstrates that you can still extract the moments by repeatedly differentiating the entire equation and evaluating it at $t=0$, reinforcing the fundamental relationship $E[X^n] = M_X^{(n)}(0)$ in a more abstract context.", "problem": "Let $X$ be a random variable whose Moment Generating Function (MGF), denoted by $M_X(t) = E[\\exp(tX)]$, is known to exist for all values of $t$ in an open interval containing the origin. Suppose that for all $t$ in this interval, the MGF satisfies the following first-order ordinary differential equation:\n$$(1-2t)\\frac{d}{dt}M_X(t) = 3 M_X(t)$$\nWithout solving for the explicit functional form of $M_X(t)$ or identifying the distribution of $X$, calculate the variance of the random variable $X$.", "solution": "Let $M(t) \\equiv M_{X}(t) = E[\\exp(tX)]$. Since $M$ is an MGF, $M(0)=1$, $M'(0)=E[X]$, and $M''(0)=E[X^{2}]$. The given differential equation is\n$$(1-2t)M'(t)=3M(t).$$\nEvaluate at $t=0$:\n$$M'(0)=3M(0)=3,$$\nso $E[X]=3$.\n\nDifferentiate both sides with respect to $t$ and use the product rule on the left:\n$$-2M'(t)+(1-2t)M''(t)=3M'(t).$$\nRearrange to obtain\n$$(1-2t)M''(t)=5M'(t).$$\nEvaluate at $t=0$:\n$$M''(0)=5M'(0)=5\\cdot 3=15,$$\nso $E[X^{2}]=15$.\n\nTherefore, the variance is\n$$\\operatorname{Var}(X)=E[X^{2}]-(E[X])^{2}=15-3^{2}=6.$$", "answer": "$$\\boxed{6}$$", "id": "1409269"}, {"introduction": "This final practice problem completes our exploration by reversing the typical workflow, highlighting the profound connection between a distribution's moments and its MGF. Given a formula for all the raw moments of a random variable, you are tasked with first reconstructing its MGF by using the Maclaurin series definition, $M_X(t) = \\sum_{k=0}^{\\infty} \\frac{E[X^k]}{k!} t^k$ [@problem_id:1409256]. This exercise not only tests your ability to manipulate power series but also deepens your understanding of the MGF as a true \"generator\" that uniquely encapsulates the entire moment structure of a distribution.", "problem": "A continuous random variable $X$ has its raw moments about the origin, $E[X^k]$, described by the formula $E[X^k] = (k+1)! 2^k$ for all non-negative integers $k=0, 1, 2, \\dots$. The Moment Generating Function (MGF) of a random variable, denoted $M_X(t)$, is formally defined by the expectation $M_X(t) = E[\\exp(tX)]$, provided this expectation exists for $t$ in some open interval containing zero. For a random variable whose moments are all finite, the MGF can be represented by its Maclaurin series expansion. Your task is to determine the variance of the random variable $X$.", "solution": "The Moment Generating Function (MGF) of a random variable $X$, $M_X(t)$, can be expressed as a Maclaurin series where the coefficients are related to the raw moments $E[X^k]$. The general form of this expansion is:\n$$M_X(t) = \\sum_{k=0}^{\\infty} \\frac{E[X^k]}{k!} t^k$$\nWe are given that the raw moments of $X$ are $E[X^k] = (k+1)! 2^k$. Substituting this expression into the series for the MGF, we get:\n$$M_X(t) = \\sum_{k=0}^{\\infty} \\frac{(k+1)! 2^k}{k!} t^k$$\nWe can simplify the term inside the summation:\n$$\\frac{(k+1)!}{k!} = \\frac{(k+1) \\cdot k!}{k!} = k+1$$\nThus, the MGF becomes:\n$$M_X(t) = \\sum_{k=0}^{\\infty} (k+1) 2^k t^k = \\sum_{k=0}^{\\infty} (k+1) (2t)^k$$\nTo find a closed-form expression for this series, let $u = 2t$. The series is $S(u) = \\sum_{k=0}^{\\infty} (k+1) u^k$.\nWe recognize this series as being related to the geometric series. Recall the formula for a geometric series:\n$$G(u) = \\sum_{k=0}^{\\infty} u^k = \\frac{1}{1-u}, \\quad \\text{for } |u| < 1$$\nDifferentiating both sides with respect to $u$ gives:\n$$\\frac{d}{du} G(u) = \\frac{d}{du} \\sum_{k=0}^{\\infty} u^k = \\sum_{k=0}^{\\infty} \\frac{d}{du} u^k = \\sum_{k=0}^{\\infty} k u^{k-1}$$\nThis is not quite the form we have. Let's consider the series for $u G(u) = \\frac{u}{1-u} = \\sum_{k=0}^{\\infty} u^{k+1}$. Differentiating this gives:\n$$\\frac{d}{du} \\left( \\frac{u}{1-u} \\right) = \\sum_{k=0}^{\\infty} (k+1)u^k = S(u)$$\nUsing the quotient rule for differentiation on the left side:\n$$\\frac{d}{du} \\left( \\frac{u}{1-u} \\right) = \\frac{(1)(1-u) - (u)(-1)}{(1-u)^2} = \\frac{1-u+u}{(1-u)^2} = \\frac{1}{(1-u)^2}$$\nSo, we have found the closed form for our series $S(u) = \\frac{1}{(1-u)^2}$.\nSubstituting back $u = 2t$, we obtain the closed-form expression for the MGF of $X$:\n$$M_X(t) = \\frac{1}{(1-2t)^2}$$\nThis MGF is valid for $|2t| < 1$, or $|t| < 1/2$.\n\nThe variance of $X$ is given by the formula $\\operatorname{Var}(X) = E[X^2] - (E[X])^2$. The moments can be found by differentiating the MGF and evaluating at $t=0$. Specifically, $E[X^n] = M_X^{(n)}(0)$, where $M_X^{(n)}(t)$ is the $n$-th derivative of $M_X(t)$.\n\nFirst, we find the first moment (the mean), $E[X]$:\n$E[X] = M_X^{(1)}(0)$.\nLet's find the first derivative of $M_X(t) = (1-2t)^{-2}$:\n$$M_X^{(1)}(t) = \\frac{d}{dt} (1-2t)^{-2} = -2(1-2t)^{-3} \\cdot (-2) = 4(1-2t)^{-3}$$\nEvaluating at $t=0$:\n$$E[X] = M_X^{(1)}(0) = 4(1-0)^{-3} = 4$$\n\nNext, we find the second moment, $E[X^2]$:\n$E[X^2] = M_X^{(2)}(0)$.\nLet's find the second derivative of $M_X(t)$ by differentiating $M_X^{(1)}(t)$:\n$$M_X^{(2)}(t) = \\frac{d}{dt} \\left( 4(1-2t)^{-3} \\right) = 4(-3)(1-2t)^{-4} \\cdot (-2) = 24(1-2t)^{-4}$$\nEvaluating at $t=0$:\n$$E[X^2] = M_X^{(2)}(0) = 24(1-0)^{-4} = 24$$\n\nFinally, we can calculate the variance:\n$$\\operatorname{Var}(X) = E[X^2] - (E[X])^2 = 24 - (4)^2 = 24 - 16 = 8$$", "answer": "$$\\boxed{8}$$", "id": "1409256"}]}