{"hands_on_practices": [{"introduction": "Understanding how to combine random variables is a fundamental skill in probability and statistics. This first practice explores how the correlation between two variables directly influences the variance of their sum or difference. By working through this exercise, you will solidify your grasp of the formula for the variance of a linear combination of random variables, a cornerstone of statistical analysis. [@problem_id:1383136]", "problem": "In a signal processing application, two random signals, represented by the random variables $X$ and $Y$, are being analyzed. The statistical properties of these signals have been determined from experimental data. The variance of signal $X$ is found to be $\\text{Var}(X) = 1$, and the variance of signal $Y$ is $\\text{Var}(Y) = 4$. The correlation coefficient between the two signals is $\\rho(X,Y) = 0.5$. A new signal $Z$ is constructed by taking the difference of the original signals, such that $Z = X - Y$. Calculate the variance of the signal $Z$.", "solution": "We are given $Z=X-Y$. For any real constants $a$ and $b$, the variance of a linear combination satisfies\n$$\n\\operatorname{Var}(aX+bY)=a^{2}\\operatorname{Var}(X)+b^{2}\\operatorname{Var}(Y)+2ab\\,\\operatorname{Cov}(X,Y).\n$$\nWith $a=1$ and $b=-1$, this gives\n$$\n\\operatorname{Var}(Z)=\\operatorname{Var}(X)+\\operatorname{Var}(Y)-2\\,\\operatorname{Cov}(X,Y).\n$$\nThe covariance is related to the correlation coefficient by\n$$\n\\operatorname{Cov}(X,Y)=\\rho(X,Y)\\,\\sigma_{X}\\sigma_{Y},\n$$\nwhere $\\sigma_{X}=\\sqrt{\\operatorname{Var}(X)}$ and $\\sigma_{Y}=\\sqrt{\\operatorname{Var}(Y)}$. Using the given values, $\\sigma_{X}=\\sqrt{1}=1$, $\\sigma_{Y}=\\sqrt{4}=2$, and $\\rho(X,Y)=\\frac{1}{2}$, we obtain\n$$\n\\operatorname{Cov}(X,Y)=\\frac{1}{2}\\cdot 1 \\cdot 2=1.\n$$\nTherefore,\n$$\n\\operatorname{Var}(Z)=1+4-2\\cdot 1=3.\n$$", "answer": "$$\\boxed{3}$$", "id": "1383136"}, {"introduction": "A common pitfall is to assume that if two variables are uncorrelated, they must be independent. This exercise demonstrates a classic counterexample, showing that a strong, non-linear dependence can exist even when the correlation coefficient is zero. This problem highlights that the Pearson correlation coefficient is specifically a measure of *linear* association, a crucial distinction for accurate data interpretation. [@problem_id:1383115]", "problem": "A sensor is mounted on the edge of a large circular disk of radius 1 unit, which is centered at the origin of a Cartesian coordinate system. The disk spins at a constant rate. At a random moment in time, the disk is stopped and the position of the sensor is recorded. Let the random variables $X$ and $Y$ represent the coordinates of the sensor at that instant. Due to the nature of the random stop, the sensor's final position $(X, Y)$ is a point chosen with uniform probability from the circumference of the unit circle.\n\nCalculate the Pearson correlation coefficient, $\\rho(X, Y)$, between the $X$ and $Y$ coordinates.", "solution": "Let $\\Theta$ denote the random angle of the sensor position measured from the positive $x$-axis. Since the stopping time is uniform and the point is uniformly distributed on the unit circle, we have $\\Theta \\sim \\text{Uniform}[0,2\\pi)$, and hence $X=\\cos\\Theta$ and $Y=\\sin\\Theta$.\n\nFirst, compute the means:\n$$\n\\mathbb{E}[X] = \\frac{1}{2\\pi}\\int_{0}^{2\\pi}\\cos\\theta\\,d\\theta = 0, \\quad\n\\mathbb{E}[Y] = \\frac{1}{2\\pi}\\int_{0}^{2\\pi}\\sin\\theta\\,d\\theta = 0.\n$$\n\nNext, compute the second moments using the identities $\\cos^{2}\\theta = \\frac{1+\\cos(2\\theta)}{2}$ and $\\sin^{2}\\theta = \\frac{1-\\cos(2\\theta)}{2}$:\n$$\n\\mathbb{E}[X^{2}] = \\frac{1}{2\\pi}\\int_{0}^{2\\pi}\\cos^{2}\\theta\\,d\\theta \n= \\frac{1}{2\\pi}\\int_{0}^{2\\pi}\\frac{1+\\cos(2\\theta)}{2}\\,d\\theta \n= \\frac{1}{2},\n$$\n$$\n\\mathbb{E}[Y^{2}] = \\frac{1}{2\\pi}\\int_{0}^{2\\pi}\\sin^{2}\\theta\\,d\\theta \n= \\frac{1}{2\\pi}\\int_{0}^{2\\pi}\\frac{1-\\cos(2\\theta)}{2}\\,d\\theta \n= \\frac{1}{2}.\n$$\nTherefore,\n$$\n\\operatorname{Var}(X) = \\mathbb{E}[X^{2}] - (\\mathbb{E}[X])^{2} = \\frac{1}{2}, \\quad\n\\operatorname{Var}(Y) = \\mathbb{E}[Y^{2}] - (\\mathbb{E}[Y])^{2} = \\frac{1}{2}.\n$$\n\nCompute the cross-moment:\n$$\n\\mathbb{E}[XY] = \\frac{1}{2\\pi}\\int_{0}^{2\\pi}\\cos\\theta\\sin\\theta\\,d\\theta\n= \\frac{1}{4\\pi}\\int_{0}^{2\\pi}\\sin(2\\theta)\\,d\\theta = 0,\n$$\nso\n$$\n\\operatorname{Cov}(X,Y) = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y] = 0.\n$$\n\nBy definition, the Pearson correlation coefficient is\n$$\n\\rho(X,Y) = \\frac{\\operatorname{Cov}(X,Y)}{\\sqrt{\\operatorname{Var}(X)\\operatorname{Var}(Y)}} \n= \\frac{0}{\\sqrt{\\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right)}} = 0.\n$$", "answer": "$$\\boxed{0}$$", "id": "1383115"}, {"introduction": "While the previous exercise showed that dependence can exist without correlation, this practice explores the opposite scenario: creating correlation from independence. By taking linear combinations of independent and identically distributed (i.i.d.) variables, we can generate new variables that are correlated with each other. This exercise reveals the powerful properties of covariance and variance when applied to linear transformations, demonstrating how statistical relationships can be constructed. [@problem_id:1383123]", "problem": "Let $X$ and $Y$ be two independent and identically distributed (i.i.d.) random variables. Assume that the common variance of $X$ and $Y$ is finite and positive. Two new random variables, $U$ and $V$, are constructed as linear combinations of $X$ and $Y$ as follows:\n$$U = X + 2Y$$\n$$V = 2X + Y$$\nCalculate the correlation coefficient between $U$ and $V$. Express your answer as an exact fraction.", "solution": "Let $\\operatorname{Var}(X)=\\operatorname{Var}(Y)=\\sigma^{2}$ with $\\sigma^{2}0$, and note that independence implies $\\operatorname{Cov}(X,Y)=0$. The correlation coefficient is\n$$\n\\rho_{UV}=\\frac{\\operatorname{Cov}(U,V)}{\\sqrt{\\operatorname{Var}(U)\\operatorname{Var}(V)}}.\n$$\nUsing bilinearity of covariance, for constants $a,b,c,d$,\n$$\n\\operatorname{Cov}(aX+bY,\\,cX+dY)=ac\\,\\operatorname{Var}(X)+bd\\,\\operatorname{Var}(Y)+ad\\,\\operatorname{Cov}(X,Y)+bc\\,\\operatorname{Cov}(Y,X).\n$$\nWith $U=X+2Y$ and $V=2X+Y$, we have $a=1$, $b=2$, $c=2$, $d=1$, so\n$$\n\\operatorname{Cov}(U,V)=1\\cdot 2\\,\\operatorname{Var}(X)+2\\cdot 1\\,\\operatorname{Var}(Y)+1\\cdot 1\\,\\operatorname{Cov}(X,Y)+2\\cdot 2\\,\\operatorname{Cov}(Y,X).\n$$\nSince $\\operatorname{Cov}(X,Y)=\\operatorname{Cov}(Y,X)=0$ and $\\operatorname{Var}(X)=\\operatorname{Var}(Y)=\\sigma^{2}$,\n$$\n\\operatorname{Cov}(U,V)=2\\sigma^{2}+2\\sigma^{2}=4\\sigma^{2}.\n$$\nNext, using $\\operatorname{Var}(aX+bY)=a^{2}\\operatorname{Var}(X)+b^{2}\\operatorname{Var}(Y)+2ab\\,\\operatorname{Cov}(X,Y)$, we get\n$$\n\\operatorname{Var}(U)=\\operatorname{Var}(X+2Y)=1^{2}\\sigma^{2}+2^{2}\\sigma^{2}+2\\cdot 1\\cdot 2\\cdot 0=5\\sigma^{2},\n$$\n$$\n\\operatorname{Var}(V)=\\operatorname{Var}(2X+Y)=2^{2}\\sigma^{2}+1^{2}\\sigma^{2}+2\\cdot 2\\cdot 1\\cdot 0=5\\sigma^{2}.\n$$\nTherefore,\n$$\n\\rho_{UV}=\\frac{4\\sigma^{2}}{\\sqrt{(5\\sigma^{2})(5\\sigma^{2})}}=\\frac{4\\sigma^{2}}{5\\sigma^{2}}=\\frac{4}{5}.\n$$", "answer": "$$\\boxed{\\frac{4}{5}}$$", "id": "1383123"}]}