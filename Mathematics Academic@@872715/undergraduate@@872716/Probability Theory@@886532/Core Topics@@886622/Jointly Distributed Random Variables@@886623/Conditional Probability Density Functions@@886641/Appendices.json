{"hands_on_practices": [{"introduction": "To build a solid foundation, our first practice focuses on the fundamental mechanics of deriving a conditional probability density function (PDF). Starting with a joint PDF for multiple variables, this exercise walks you through the essential steps of normalization, marginalization, and finally, conditioning [@problem_id:1351401]. Mastering this direct computational process is crucial before tackling more complex conceptual problems.", "problem": "Consider a system described by three continuous random variables $X$, $Y$, and $Z$. The joint probability density function (PDF) for these variables is given by\n$$\nf_{X,Y,Z}(x,y,z) = \\begin{cases} c(x+y+z)  \\text{if } 0 \\le x \\le 1, 0 \\le y \\le 1, 0 \\le z \\le 1 \\\\ 0  \\text{otherwise} \\end{cases}\n$$\nwhere $c$ is a normalization constant.\n\nSuppose a measurement of the system reveals that the variable $X$ has the specific value $X = 1/2$. Your task is to determine the conditional joint probability density function of $Y$ and $Z$ given this information, denoted as $f_{Y,Z|X}(y,z|1/2)$. The domain for this conditional PDF is $0 \\le y \\le 1$ and $0 \\le z \\le 1$.\n\nPresent your answer as a single closed-form analytic expression in terms of $y$ and $z$.", "solution": "We first determine the normalization constant $c$ by enforcing that the joint PDF integrates to $1$ over the unit cube. Using\n$$\n\\int_{0}^{1}\\int_{0}^{1}\\int_{0}^{1} c(x+y+z)\\,dx\\,dy\\,dz = 1,\n$$\nwe compute\n$$\n\\int_{0}^{1}\\int_{0}^{1}\\int_{0}^{1} (x+y+z)\\,dx\\,dy\\,dz\n= \\int_{0}^{1}\\int_{0}^{1}\\left(\\int_{0}^{1} x\\,dx + \\int_{0}^{1} y\\,dx + \\int_{0}^{1} z\\,dx\\right)dy\\,dz\n= \\int_{0}^{1}\\int_{0}^{1}\\left(\\frac{1}{2} + y + z\\right)dy\\,dz.\n$$\nSeparating terms and using symmetry,\n$$\n\\int_{0}^{1}\\int_{0}^{1}\\left(\\frac{1}{2} + y + z\\right)dy\\,dz\n= \\frac{1}{2}\\cdot 1 + \\left(\\int_{0}^{1} y\\,dy\\right)\\cdot 1 + \\left(\\int_{0}^{1} z\\,dz\\right)\\cdot 1\n= \\frac{1}{2} + \\frac{1}{2} + \\frac{1}{2} = \\frac{3}{2}.\n$$\nThus $c \\cdot \\frac{3}{2} = 1$, so $c = \\frac{2}{3}$.\n\nNext, we compute the marginal density of $X$:\n$$\nf_{X}(x) = \\int_{0}^{1}\\int_{0}^{1} c(x+y+z)\\,dy\\,dz\n= c\\left[x\\int_{0}^{1}\\int_{0}^{1} dy\\,dz + \\int_{0}^{1}\\int_{0}^{1} y\\,dy\\,dz + \\int_{0}^{1}\\int_{0}^{1} z\\,dy\\,dz\\right]\n= c\\left(x + \\frac{1}{2} + \\frac{1}{2}\\right) = c(x+1),\n$$\nfor $0 \\le x \\le 1$. With $c=\\frac{2}{3}$, this becomes $f_{X}(x) = \\frac{2}{3}(x+1)$, so\n$$\nf_{X}\\!\\left(\\frac{1}{2}\\right) = \\frac{2}{3}\\left(\\frac{1}{2} + 1\\right) = 1.\n$$\n\nThe conditional joint PDF of $Y$ and $Z$ given $X=\\frac{1}{2}$ is, by definition,\n$$\nf_{Y,Z|X}\\!\\left(y,z \\mid \\frac{1}{2}\\right) = \\frac{f_{X,Y,Z}\\!\\left(\\frac{1}{2},y,z\\right)}{f_{X}\\!\\left(\\frac{1}{2}\\right)}\n= \\frac{c\\left(\\frac{1}{2}+y+z\\right)}{1}\n= \\frac{2}{3}\\left(y+z+\\frac{1}{2}\\right),\n$$\nfor $0 \\le y \\le 1$ and $0 \\le z \\le 1$. This integrates to $1$ over the unit square, confirming it is a valid conditional density.", "answer": "$$\\boxed{\\frac{2}{3}\\left(y+z+\\frac{1}{2}\\right)}$$", "id": "1351401"}, {"introduction": "We now move from pure mechanics to a classic application with deep relevance in fields like signal processing and communications. This problem explores what we can deduce about an individual noise component when we only know the total noise from two independent sources [@problem_id:1351407]. The solution elegantly demonstrates how conditioning on new information updates our knowledgeâ€”in this case, changing both the expected value and the variance of our variable of interest.", "problem": "Consider a system where a signal is affected by two noise sources, modeled by random variables $X$ and $Y$. These noise components are assumed to be independent and identically distributed (i.i.d.), each following a standard Normal distribution with a mean of 0 and a variance of 1. An instrument measures only the total noise, which is the sum of the two components, $S = X + Y$. Given that a specific measurement of the total noise is $S=s$, determine the conditional probability density function (PDF) of the first noise component, $f_{X|S}(x|s)$. Express your answer as a function of $x$ and $s$.", "solution": "Let $X$ and $Y$ be independent standard Normal random variables, so $f_{X}(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^{2}}{2}\\right)$ and $f_{Y}(y) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{y^{2}}{2}\\right)$. Define $S = X + Y$. We seek $f_{X|S}(x|s)$.\n\nBy the definition of conditional density,\n$$\nf_{X|S}(x|s) = \\frac{f_{X,S}(x,s)}{f_{S}(s)}.\n$$\nUsing the relation $S = X + Y$ and independence, the conditional density of $S$ given $X=x$ is $f_{S|X}(s|x) = f_{Y}(s - x)$. Hence,\n$$\nf_{X,S}(x,s) = f_{X}(x) f_{S|X}(s|x) = f_{X}(x) f_{Y}(s-x) = \\frac{1}{2\\pi} \\exp\\left(-\\frac{x^{2}}{2}\\right) \\exp\\left(-\\frac{(s-x)^{2}}{2}\\right).\n$$\nThe marginal density of $S$ is the convolution of $f_{X}$ and $f_{Y}$. Since the sum of independent Normal variables is Normal with mean equal to the sum of means and variance equal to the sum of variances, we have $S \\sim \\mathcal{N}(0,2)$, so\n$$\nf_{S}(s) = \\frac{1}{\\sqrt{4\\pi}} \\exp\\left(-\\frac{s^{2}}{4}\\right).\n$$\nTherefore,\n$$\nf_{X|S}(x|s) = \\frac{\\frac{1}{2\\pi} \\exp\\left(-\\frac{x^{2}}{2}\\right) \\exp\\left(-\\frac{(s-x)^{2}}{2}\\right)}{\\frac{1}{\\sqrt{4\\pi}} \\exp\\left(-\\frac{s^{2}}{4}\\right)}.\n$$\nCombine exponents in the numerator:\n$$\n-\\frac{x^{2}}{2} - \\frac{(s-x)^{2}}{2} = -\\frac{x^{2} + (s-x)^{2}}{2} = -\\frac{2x^{2} - 2sx + s^{2}}{2} = -x^{2} + sx - \\frac{s^{2}}{2}.\n$$\nSubtract the denominator exponent $-\\frac{s^{2}}{4}$ to get the total exponent:\n$$\n-x^{2} + sx - \\frac{s^{2}}{2} + \\frac{s^{2}}{4} = -x^{2} + sx - \\frac{s^{2}}{4}.\n$$\nComplete the square:\n$$\n-x^{2} + sx - \\frac{s^{2}}{4} = -\\left(x - \\frac{s}{2}\\right)^{2}.\n$$\nFor the prefactor,\n$$\n\\frac{\\frac{1}{2\\pi}}{\\frac{1}{\\sqrt{4\\pi}}} = \\frac{\\sqrt{4\\pi}}{2\\pi} = \\frac{1}{\\sqrt{\\pi}}.\n$$\nThus,\n$$\nf_{X|S}(x|s) = \\frac{1}{\\sqrt{\\pi}} \\exp\\left(-\\left(x - \\frac{s}{2}\\right)^{2}\\right),\n$$\nwhich is the density of a Normal distribution with mean $\\frac{s}{2}$ and variance $\\frac{1}{2}$, valid for all real $x$ and $s$.", "answer": "$$\\boxed{\\frac{1}{\\sqrt{\\pi}}\\exp\\left(-\\left(x-\\frac{s}{2}\\right)^{2}\\right)}$$", "id": "1351407"}, {"introduction": "This final practice challenges you to apply conditional probability in a scenario involving order statistics, where the random variables of interest are derived from, and thus dependent on, an underlying set of independent variables. The problem requires a blend of direct calculation and careful probabilistic reasoning, showing how to deconstruct a complex situation using the law of total expectation [@problem_id:1351388]. It serves as an excellent example of how conditional expectation can be used to solve problems that are not immediately obvious from a direct application of formulas.", "problem": "Two critical but independent electronic components of a deep-space probe have lifetimes, denoted by $T_1$ and $T_2$. These lifetimes are modeled as independent and identically distributed random variables, each following a uniform distribution over the time interval $[0, \\theta]$, where $\\theta$ is a known maximum possible lifetime. The probe is designed to function as long as at least one of these components is operational. A system failure is therefore recorded at a time $M = \\max(T_1, T_2)$.\n\nSuppose a particular probe fails, and the time of failure is recorded as $M=m$, for a specific value $m$ such that $0  m \\le \\theta$. Given this information, what is the expected lifetime of the first component, $T_1$? Your answer should be a closed-form analytic expression in terms of $m$.", "solution": "Let $T_{1}$ and $T_{2}$ be i.i.d. $\\mathrm{Uniform}(0,\\theta)$ and define the order statistics $T_{(1)}=\\min(T_{1},T_{2})$ and $T_{(2)}=\\max(T_{1},T_{2})=M$. For i.i.d. continuous variables, the joint density of $(T_{(1)},T_{(2)})$ is\n$$\nf_{T_{(1)},T_{(2)}}(x,y)=\\frac{2}{\\theta^{2}}, \\quad 0\\le x\\le y\\le \\theta.\n$$\nThe marginal density of $M=T_{(2)}$ is\n$$\nf_{M}(m)=\\int_{0}^{m}\\frac{2}{\\theta^{2}}\\,dx=\\frac{2m}{\\theta^{2}}, \\quad 0m\\le \\theta.\n$$\nHence the conditional density of $T_{(1)}$ given $M=m$ is\n$$\nf_{T_{(1)}\\mid M}(x\\mid m)=\\frac{f_{T_{(1)},M}(x,m)}{f_{M}(m)}=\\frac{\\frac{2}{\\theta^{2}}}{\\frac{2m}{\\theta^{2}}}=\\frac{1}{m}, \\quad 0\\le x\\le m,\n$$\nso\n$$\n\\mathbb{E}\\!\\left[T_{(1)}\\mid M=m\\right]=\\int_{0}^{m}x\\cdot \\frac{1}{m}\\,dx=\\frac{m}{2}.\n$$\nLet $A$ be the event that $T_{1}$ is the maximum. By exchangeability and continuity, $\\mathbb{P}(A\\mid M=m)=\\frac{1}{2}$. On $A$, $T_{1}=M=m$, so $\\mathbb{E}[T_{1}\\mid M=m,A]=m$. On $A^{c}$, $T_{1}=T_{(1)}$, so $\\mathbb{E}[T_{1}\\mid M=m,A^{c}]=\\mathbb{E}[T_{(1)}\\mid M=m]=\\frac{m}{2}$. Therefore,\n$$\n\\mathbb{E}[T_{1}\\mid M=m]=\\mathbb{P}(A\\mid M=m)\\,m+\\mathbb{P}(A^{c}\\mid M=m)\\,\\frac{m}{2}\n=\\frac{1}{2}m+\\frac{1}{2}\\cdot \\frac{m}{2}=\\frac{3m}{4}.\n$$", "answer": "$$\\boxed{\\frac{3m}{4}}$$", "id": "1351388"}]}