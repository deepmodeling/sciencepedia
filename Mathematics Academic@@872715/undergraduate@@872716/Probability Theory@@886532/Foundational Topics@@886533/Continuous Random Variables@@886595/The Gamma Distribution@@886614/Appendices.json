{"hands_on_practices": [{"introduction": "To apply the Gamma distribution to real-world scenarios, we must first determine the values of its shape parameter, $\\alpha$, and rate parameter, $\\lambda$, from observable data. The mean and variance are fundamental properties of a distribution that can often be estimated from experiments. This exercise [@problem_id:1398490] provides a direct method for fitting the distribution by using the known formulas that connect these empirical measurements to the theoretical parameters, a crucial first step in any statistical modeling task.", "problem": "The lifetime of a particular model of electronic component is modeled as a random variable, $X$, where $X$ is measured in thousands of hours. The probability distribution of $X$ is known to be a Gamma distribution. A random variable $Y$ that follows a Gamma distribution with a shape parameter $\\alpha > 0$ and a rate parameter $\\lambda > 0$, denoted as $Y \\sim \\Gamma(\\alpha, \\lambda)$, has an expected value (mean) given by $E[Y] = \\frac{\\alpha}{\\lambda}$ and a variance given by $\\text{Var}(Y) = \\frac{\\alpha}{\\lambda^2}$.\n\nFrom extensive testing on a large sample of these components, the mean lifetime is determined to be 10,000 hours, and the variance of their lifetimes is 20,000,000 hours$^2$.\n\nFind the shape parameter, $\\alpha$, and the rate parameter, $\\lambda$, for the probability distribution of the component's lifetime $X$.", "solution": "Let $T$ denote the lifetime in hours and $X$ denote the lifetime in thousands of hours, so $X=\\frac{T}{10^{3}}$. Using linearity of expectation and the scaling rule for variance, we have\n$$E[X]=\\frac{E[T]}{10^{3}}, \\quad \\text{Var}(X)=\\frac{\\text{Var}(T)}{10^{6}}.$$\nGiven $E[T]=10{,}000$ hours and $\\text{Var}(T)=20{,}000{,}000$ hours$^{2}$, it follows that\n$$E[X]=\\frac{10{,}000}{10^{3}}=10, \\quad \\text{Var}(X)=\\frac{20{,}000{,}000}{10^{6}}=20.$$\n\nAssuming $X \\sim \\Gamma(\\alpha,\\lambda)$ with rate parameter $\\lambda$, the mean and variance are\n$$E[X]=\\frac{\\alpha}{\\lambda}, \\quad \\text{Var}(X)=\\frac{\\alpha}{\\lambda^{2}}.$$\nSet these equal to the empirical values:\n$$\\frac{\\alpha}{\\lambda}=10, \\quad \\frac{\\alpha}{\\lambda^{2}}=20.$$\nFrom $\\frac{\\alpha}{\\lambda}=10$ we get $\\alpha=10\\lambda$. Substitute into the variance equation:\n$$\\frac{10\\lambda}{\\lambda^{2}}=20 \\quad \\Longrightarrow \\quad \\frac{10}{\\lambda}=20 \\quad \\Longrightarrow \\quad \\lambda=\\frac{1}{2}.$$\nThen\n$$\\alpha=10\\left(\\frac{1}{2}\\right)=5.$$\n\nTherefore, the shape and rate parameters are $\\alpha=5$ and $\\lambda=\\frac{1}{2}$.", "answer": "$$\\boxed{\\begin{pmatrix}5 & \\frac{1}{2}\\end{pmatrix}}$$", "id": "1398490"}, {"introduction": "The Gamma distribution often arises dynamically from underlying random processes, most notably in describing the waiting time for a series of events. This practice explores the fundamental and elegant relationship between the Gamma distribution and the Poisson process, where the waiting time for the $\\alpha$-th event in a Poisson process with rate $\\lambda$ follows a Gamma($\\alpha, \\lambda$) distribution. By working through this scenario [@problem_id:1303888], you will solidify your understanding of how these two key concepts are intertwined and learn to solve practical problems involving event rates and waiting times.", "problem": "A research satellite is designed to study rare cosmic phenomena. The arrivals of a specific type of high-energy particle at its detector are well-modeled by a Poisson process. The waiting time, $T$, until the $k$-th particle is detected follows a Gamma distribution. The convention used for the Gamma distribution is the (shape, rate) parameterization. The shape parameter is determined by the number of arrivals, $k$, and the rate parameter is the average arrival rate of the underlying Poisson process, $\\lambda$.\n\nFrom long-term observation data, the expected value of this waiting time $T$ is measured to be $\\tau$.\n\nAn observation window of duration $t_{obs}$ is scheduled. Find an expression for the probability that the detector registers at least one particle during this window. Your answer must be a self-contained analytic expression in terms of the parameters $k$, $\\tau$, and $t_{obs}$.", "solution": "Let the random variable $T$ be the waiting time until the $k$-th particle detection. The problem states that $T$ follows a Gamma distribution with shape parameter $k$ and rate parameter $\\lambda$. The probability density function of such a distribution is given by $f(t) = \\frac{\\lambda^k t^{k-1} e^{-\\lambda t}}{\\Gamma(k)}$ for $t \\ge 0$.\n\nThe expected value (mean) of a random variable following a Gamma($k, \\lambda$) distribution is given by:\n$$E[T] = \\frac{k}{\\lambda}$$\n\nThe problem states that this expected waiting time is $\\tau$. Therefore, we can establish a relationship between the given parameters and the underlying process rate $\\lambda$:\n$$\\tau = \\frac{k}{\\lambda}$$\n\nWe can rearrange this equation to solve for the rate parameter $\\lambda$ in terms of the known quantities $k$ and $\\tau$:\n$$\\lambda = \\frac{k}{\\tau}$$\nThis parameter $\\lambda$ represents the average number of particle detections per unit time.\n\nNow, we need to find the probability of detecting at least one particle in an observation window of duration $t_{obs}$. The number of events occurring in a fixed interval of time in a Poisson process follows a Poisson distribution. Let $N(t)$ be the random variable representing the number of particles detected in a time interval of length $t$. Then $N(t)$ follows a Poisson distribution with parameter $\\mu = \\lambda t$. The probability mass function is:\n$$P(N(t) = n) = \\frac{(\\lambda t)^n e^{-\\lambda t}}{n!}$$\n\nWe are interested in the probability of detecting at least one particle in the time interval $t_{obs}$, which can be written as $P(N(t_{obs}) \\ge 1)$. It is easier to calculate this probability by considering the complementary event: the probability of detecting zero particles, $P(N(t_{obs}) = 0)$.\n$$P(N(t_{obs}) \\ge 1) = 1 - P(N(t_{obs}) = 0)$$\n\nUsing the Poisson probability mass function with $n=0$ and $t=t_{obs}$:\n$$P(N(t_{obs}) = 0) = \\frac{(\\lambda t_{obs})^0 e^{-\\lambda t_{obs}}}{0!} = \\frac{1 \\cdot e^{-\\lambda t_{obs}}}{1} = e^{-\\lambda t_{obs}}$$\n\nSo, the probability of detecting at least one particle is:\n$$P(N(t_{obs}) \\ge 1) = 1 - e^{-\\lambda t_{obs}}$$\n\nFinally, we substitute the expression for $\\lambda$ that we found earlier, $\\lambda = k/\\tau$, into this equation to get the final answer in terms of the given parameters $k$, $\\tau$, and $t_{obs}$:\n$$P(N(t_{obs}) \\ge 1) = 1 - \\exp\\left(-\\left(\\frac{k}{\\tau}\\right) t_{obs}\\right) = 1 - \\exp\\left(-\\frac{k t_{obs}}{\\tau}\\right)$$\nThis is the desired analytic expression for the probability.", "answer": "$$\\boxed{1 - \\exp\\left(-\\frac{k t_{obs}}{\\tau}\\right)}$$", "id": "1303888"}, {"introduction": "A distribution is fully characterized by its entire collection of moments, not just its mean and variance. While these moments can be calculated through direct integration of the probability density function, a more powerful and often simpler method involves the use of moment-generating functions (MGFs). This exercise [@problem_id:1919334] demonstrates how the MGF elegantly \"generates\" all the moments of the Gamma distribution through differentiation, providing hands-on practice with an essential technique in mathematical statistics.", "problem": "A continuous random variable $X$ is said to follow a Gamma distribution with a shape parameter $\\alpha > 0$ and a rate parameter $\\lambda > 0$. The probability density function (PDF) for this distribution is given by\n$$f(x; \\alpha, \\lambda) = \\frac{\\lambda^{\\alpha}}{\\Gamma(\\alpha)} x^{\\alpha-1} \\exp(-\\lambda x), \\quad \\text{for } x > 0$$\nwhere $\\Gamma(\\cdot)$ is the Gamma function. The moment-generating function (MGF) of $X$, which is defined as $M_X(t) = E[\\exp(tX)]$, is given by\n$$M_X(t) = \\left(\\frac{\\lambda}{\\lambda - t}\\right)^{\\alpha}, \\quad \\text{for } t < \\lambda$$\n\nWithout using direct integration on the PDF, find a general expression for the $k$-th raw moment of $X$, denoted by $E[X^k]$, for any positive integer $k$. Your final expression should be in terms of $\\alpha$, $\\lambda$, $k$, and the Gamma function $\\Gamma(\\cdot)$.", "solution": "The $k$-th raw moment of a random variable $X$, denoted $E[X^k]$, can be found from its moment-generating function (MGF), $M_X(t)$, by computing the $k$-th derivative of the MGF with respect to $t$ and then evaluating it at $t=0$. The relationship is:\n$$E[X^k] = \\frac{d^k}{dt^k} M_X(t) \\bigg|_{t=0} = M_X^{(k)}(0)$$\n\nWe are given the MGF of the Gamma distribution:\n$$M_X(t) = \\left(\\frac{\\lambda}{\\lambda - t}\\right)^{\\alpha}$$\nFor easier differentiation, we can rewrite this function using a negative exponent:\n$$M_X(t) = \\lambda^{\\alpha} (\\lambda - t)^{-\\alpha}$$\n\nNow, we compute the first few derivatives with respect to $t$ to identify a pattern.\n\nFor $k=1$, the first derivative is:\n$$M_X^{(1)}(t) = \\frac{d}{dt} \\left[ \\lambda^{\\alpha} (\\lambda - t)^{-\\alpha} \\right] = \\lambda^{\\alpha} (-\\alpha) (\\lambda - t)^{-\\alpha-1} (-1) = \\alpha \\lambda^{\\alpha} (\\lambda - t)^{-(\\alpha+1)}$$\n\nFor $k=2$, the second derivative is:\n$$M_X^{(2)}(t) = \\frac{d}{dt} \\left[ \\alpha \\lambda^{\\alpha} (\\lambda - t)^{-(\\alpha+1)} \\right] = \\alpha \\lambda^{\\alpha} (-(\\alpha+1)) (\\lambda - t)^{-(\\alpha+2)} (-1) = \\alpha(\\alpha+1) \\lambda^{\\alpha} (\\lambda - t)^{-(\\alpha+2)}$$\n\nFor $k=3$, the third derivative is:\n$$M_X^{(3)}(t) = \\frac{d}{dt} \\left[ \\alpha(\\alpha+1) \\lambda^{\\alpha} (\\lambda - t)^{-(\\alpha+2)} \\right] = \\alpha(\\alpha+1) \\lambda^{\\alpha} (-(\\alpha+2)) (\\lambda - t)^{-(\\alpha+3)} (-1) = \\alpha(\\alpha+1)(\\alpha+2) \\lambda^{\\alpha} (\\lambda - t)^{-(\\alpha+3)}$$\n\nBy observing the pattern, we can generalize for an arbitrary positive integer $k$. The $k$-th derivative is:\n$$M_X^{(k)}(t) = \\alpha(\\alpha+1)(\\alpha+2) \\cdots (\\alpha+k-1) \\lambda^{\\alpha} (\\lambda - t)^{-(\\alpha+k)}$$\n\nThe product term $\\alpha(\\alpha+1)(\\alpha+2) \\cdots (\\alpha+k-1)$ is known as the rising factorial, which can be expressed using the Gamma function:\n$$\\alpha(\\alpha+1)\\cdots(\\alpha+k-1) = \\frac{\\Gamma(\\alpha+k)}{\\Gamma(\\alpha)}$$\n\nSubstituting this into our expression for the $k$-th derivative gives:\n$$M_X^{(k)}(t) = \\frac{\\Gamma(\\alpha+k)}{\\Gamma(\\alpha)} \\lambda^{\\alpha} (\\lambda - t)^{-(\\alpha+k)}$$\n\nTo find the $k$-th raw moment, $E[X^k]$, we evaluate this expression at $t=0$:\n$$E[X^k] = M_X^{(k)}(0) = \\frac{\\Gamma(\\alpha+k)}{\\Gamma(\\alpha)} \\lambda^{\\alpha} (\\lambda - 0)^{-(\\alpha+k)}$$\n$$E[X^k] = \\frac{\\Gamma(\\alpha+k)}{\\Gamma(\\alpha)} \\lambda^{\\alpha} \\lambda^{-(\\alpha+k)}$$\n\nUsing the property of exponents, $a^m a^n = a^{m+n}$, we simplify the terms involving $\\lambda$:\n$$E[X^k] = \\frac{\\Gamma(\\alpha+k)}{\\Gamma(\\alpha)} \\lambda^{\\alpha - (\\alpha+k)} = \\frac{\\Gamma(\\alpha+k)}{\\Gamma(\\alpha)} \\lambda^{-k}$$\n\nThis gives the final expression for the $k$-th raw moment:\n$$E[X^k] = \\frac{\\Gamma(\\alpha+k)}{\\Gamma(\\alpha) \\lambda^k}$$", "answer": "$$\\boxed{\\frac{\\Gamma(\\alpha+k)}{\\Gamma(\\alpha) \\lambda^{k}}}$$", "id": "1919334"}]}