{"hands_on_practices": [{"introduction": "A key property of a Bernoulli trial is its variance, which measures the spread or unpredictability of the outcome. This exercise provides a practical scenario where we work backward from a measured variance to deduce the underlying probability of success, $p$. Solving this problem will reinforce your understanding of the formula for variance, $Var(X) = p(1-p)$, and reveal an interesting symmetry in how different probabilities can lead to the same level of uncertainty. [@problem_id:1392758]", "problem": "A data analytics firm is studying consumer behavior for a new online subscription service. The action of a single, randomly selected consumer either purchasing the subscription or not is modeled as a discrete random event. A random variable $X$ is defined to represent this outcome: $X=1$ if the consumer makes a purchase, and $X=0$ if they do not. After analyzing a large sample of consumers, the firm determines that the variance of this random variable, $\\text{Var}(X)$, is $0.21$.\n\nLet $p$ represent the probability that a consumer makes a purchase. Based on the given variance, determine the two possible values for $p$. Present your two answers as decimal numbers in ascending order.", "solution": "Let $X$ be a Bernoulli random variable with success probability $p$. The variance of a Bernoulli random variable is given by\n$$\n\\mathrm{Var}(X)=p(1-p).\n$$\nWe are given $\\mathrm{Var}(X)=0.21$, so\n$$\np(1-p)=0.21.\n$$\nRewriting,\n$$\np-p^{2}=0.21 \\;\\;\\Longrightarrow\\;\\; p^{2}-p+0.21=0.\n$$\nSolving the quadratic equation using the quadratic formula,\n$$\np=\\frac{1\\pm \\sqrt{1-4\\cdot 0.21}}{2}=\\frac{1\\pm \\sqrt{0.16}}{2}=\\frac{1\\pm 0.4}{2}.\n$$\nThus the two solutions are\n$$\np=\\frac{1-0.4}{2}=0.3 \\quad \\text{and} \\quad p=\\frac{1+0.4}{2}=0.7,\n$$\nboth of which lie in the interval $[0,1]$. In ascending order, these are $0.3$ and $0.7$.", "answer": "$$\\boxed{\\begin{pmatrix}0.3  0.7\\end{pmatrix}}$$", "id": "1392758"}, {"introduction": "While single Bernoulli trials are fundamental, most real-world applications involve sequences of events. This problem serves as an essential bridge from the single-trial Bernoulli distribution to the multi-trial Binomial distribution. By calculating the probability of a specific outcome from the sum of two independent Bernoulli variables, you will practice applying the concepts of independence and mutually exclusive events, which are cornerstones of probability theory. [@problem_id:682]", "problem": "A discrete random variable $X$ is said to follow a Bernoulli distribution with parameter $p$, denoted as $X \\sim \\text{Bernoulli}(p)$, if it describes a single trial with two possible outcomes: success (value 1) or failure (value 0). The probability of success is $P(X=1) = p$, and the probability of failure is $P(X=0) = 1-p$, where $0 \\le p \\le 1$.\n\nConsider two random variables, $X_1$ and $X_2$, that are independent and identically distributed (i.i.d.) according to a Bernoulli distribution with parameter $p$.\n\nDerive an expression for the probability that the sum of these two variables is exactly equal to one. That is, find $P(X_1 + X_2 = 1)$.", "solution": "Let $X_1$ and $X_2$ be two independent and identically distributed random variables, both following a Bernoulli distribution with parameter $p$.\nThe probability mass function (PMF) for each variable is given by:\n$$\nP(X_i=1) = p\n$$\n$$\nP(X_i=0) = 1-p\n$$\nfor $i=1, 2$.\n\nWe want to find the probability that their sum is equal to one, $P(X_1 + X_2 = 1)$. Since $X_1$ and $X_2$ can only take values of 0 or 1, their sum can be 0, 1, or 2. The event $X_1 + X_2 = 1$ can occur in two mutually exclusive ways:\n1.  $X_1 = 1$ and $X_2 = 0$.\n2.  $X_1 = 0$ and $X_2 = 1$.\n\nThe probability of the event $X_1 + X_2 = 1$ is the sum of the probabilities of these two mutually exclusive outcomes.\n$$\nP(X_1 + X_2 = 1) = P( (X_1=1 \\text{ and } X_2=0) \\text{ or } (X_1=0 \\text{ and } X_2=1) )\n$$\nBecause the two cases are mutually exclusive, we can write this as:\n$$\nP(X_1 + X_2 = 1) = P(X_1=1, X_2=0) + P(X_1=0, X_2=1)\n$$\n\nSince $X_1$ and $X_2$ are independent, the joint probability of any combination of their outcomes is the product of their individual probabilities.\nFor the first case, $(X_1=1, X_2=0)$:\n$$\nP(X_1=1, X_2=0) = P(X_1=1) \\times P(X_2=0)\n$$\nSubstituting the probabilities from the Bernoulli distribution definition:\n$$\nP(X_1=1, X_2=0) = p \\times (1-p) = p(1-p)\n$$\n\nFor the second case, $(X_1=0, X_2=1)$:\n$$\nP(X_1=0, X_2=1) = P(X_1=0) \\times P(X_2=1)\n$$\nSubstituting the probabilities:\n$$\nP(X_1=0, X_2=1) = (1-p) \\times p = p(1-p)\n$$\n\nNow, we sum the probabilities of these two cases to find the total probability of $X_1 + X_2 = 1$:\n$$\nP(X_1 + X_2 = 1) = p(1-p) + p(1-p)\n$$\n$$\nP(X_1 + X_2 = 1) = 2p(1-p)\n$$", "answer": "$$\\boxed{2p(1-p)}$$", "id": "682"}, {"introduction": "This final practice problem presents a classic and elegant challenge from information theory: how can we create a perfectly fair, unbiased random bit from a source that is inherently biased? This famous procedure, known as the von Neumann randomness extractor, is a beautiful application of probabilistic reasoning to a real-world engineering problem. Tackling this problem will require you to synthesize concepts like conditional probability, independence, and the expected value of a geometric process, providing a deeper appreciation for the versatility of the Bernoulli framework. [@problem_id:1392786]", "problem": "An engineer is designing a subsystem for a deep-space probe that relies on a fundamentally noisy physical process to generate random bits. The raw output is a sequence of bits, $X_1, X_2, X_3, \\dots$, which can be modeled as a sequence of independent and identically distributed Bernoulli random variables. For each bit $X_i$, the probability of it being a '1' is an unknown but constant value $p$, and the probability of it being a '0' is $1-p$. The system operates under the constraint that $p$ is strictly between 0 and 1, i.e., $p \\in (0,1)$.\n\nTo produce a \"fair\" bit (i.e., a bit with a probability of 0.5 of being '1'), the engineer implements the following algorithm, often attributed to John von Neumann:\n1.  The raw bits are consumed in non-overlapping consecutive pairs: $(X_1, X_2), (X_3, X_4), (X_5, X_6), \\dots$.\n2.  Each pair is processed according to the following rules:\n    *   If the pair is $(0,1)$, the algorithm outputs a single bit '0' and halts.\n    *   If the pair is $(1,0)$, the algorithm outputs a single bit '1' and halts.\n    *   If the pair is $(0,0)$ or $(1,1)$, the pair is discarded, and the algorithm proceeds to the next pair to repeat the process.\n\nThis procedure is guaranteed to eventually produce one output bit. Let's call this first generated output bit $Y$.\n\n(a) Calculate the probability that the output bit is a '1', i.e., find $P(Y=1)$.\n(b) Calculate the expected number of raw bits from the source sequence ($X_i$) that must be consumed to produce the single output bit $Y$.\n\nProvide your answer as a row matrix containing the symbolic expressions for Part (a) and Part (b), in that order. Express your answer for Part (b) in terms of $p$.", "solution": "Let the raw bits be independent and identically distributed with $P(X_{i}=1)=p$ and $P(X_{i}=0)=1-p$, with $p\\in(0,1)$. Consider disjoint consecutive pairs $(X_{1},X_{2}), (X_{3},X_{4}), \\dots$. By independence and identical distribution of the $X_{i}$, these pairs are independent and identically distributed.\n\nFor a single pair, the probabilities of pair outcomes are:\n$$\nP((1,0))=p(1-p),\\quad P((0,1))=(1-p)p,\\quad P(\\text{equal})=P((0,0))+P((1,1))=p^{2}+(1-p)^{2}.\n$$\nDefine $q=P(\\text{unequal})=P((1,0))+P((0,1))=2p(1-p)$.\n\nThe algorithm halts at the first pair that is unequal. Let $K$ be the index of this first unequal pair. Then $K$ is a geometric random variable on $\\{1,2,\\dots\\}$ with success probability $q=2p(1-p)$:\n$$\nP(K=k)=\\left(p^{2}+(1-p)^{2}\\right)^{k-1}\\cdot 2p(1-p).\n$$\n\n(a) The algorithm outputs $Y=1$ exactly when the first unequal pair is $(1,0)$. Thus\n$$\nP(Y=1)=\\sum_{k=1}^{\\infty}\\left(p^{2}+(1-p)^{2}\\right)^{k-1}\\cdot p(1-p)\n= p(1-p)\\sum_{k=1}^{\\infty}\\left(p^{2}+(1-p)^{2}\\right)^{k-1}.\n$$\nThis is a geometric series with ratio $r=p^{2}+(1-p)^{2}$, so\n$$\n\\sum_{k=1}^{\\infty}r^{\\,k-1}=\\frac{1}{1-r}=\\frac{1}{1-\\left(p^{2}+(1-p)^{2}\\right)}=\\frac{1}{2p(1-p)}.\n$$\nTherefore,\n$$\nP(Y=1)=p(1-p)\\cdot \\frac{1}{2p(1-p)}=\\frac{1}{2}.\n$$\n\n(b) Each processed pair consumes exactly $2$ raw bits. Since $K$ is geometric with success probability $q=2p(1-p)$, its expectation is $E[K]=\\frac{1}{q}=\\frac{1}{2p(1-p)}$. Hence the expected number of raw bits consumed is\n$$\nE[\\text{bits}]=2E[K]=\\frac{2}{2p(1-p)}=\\frac{1}{p(1-p)}.\n$$\nThus the required expressions are $P(Y=1)=\\frac{1}{2}$ and $E[\\text{bits}]=\\frac{1}{p(1-p)}$.", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{2}  \\frac{1}{p(1-p)}\\end{pmatrix}}$$", "id": "1392786"}]}