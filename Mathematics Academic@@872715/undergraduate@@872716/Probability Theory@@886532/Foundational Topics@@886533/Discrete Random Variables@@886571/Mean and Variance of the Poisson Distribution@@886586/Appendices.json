{"hands_on_practices": [{"introduction": "A deep understanding of any probability distribution begins with its fundamental properties. This exercise guides you through the derivation of the mean and variance for a Poisson distribution directly from its probability mass function. By working through this foundational practice, you will prove the critical and unique identity that for a Poisson random variable $X$ with parameter $\\lambda$, its expectation and variance are both equal to $\\lambda$, i.e., $E[X] = \\operatorname{Var}(X) = \\lambda$ [@problem_id:6536].", "problem": "A discrete random variable $X$ is said to follow a Poisson distribution with a parameter $\\lambda > 0$ if its probability mass function (PMF) is given by:\n$$P(X=k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}, \\quad \\text{for } k = 0, 1, 2, \\dots$$\n\nThe expectation (or mean) of the random variable $X$ is defined as:\n$$E[X] = \\sum_{k=0}^{\\infty} k P(X=k)$$\n\nThe variance of $X$ is defined as:\n$$\\operatorname{Var}(X) = E[(X - E[X])^2]$$\nA common formula for calculating the variance is:\n$$\\operatorname{Var}(X) = E[X^2] - (E[X])^2$$\nwhere $E[X^2] = \\sum_{k=0}^{\\infty} k^2 P(X=k)$.\n\nYou may find the following Taylor series expansion for the exponential function useful:\n$$\\sum_{n=0}^{\\infty} \\frac{x^n}{n!} = e^x$$\n\n**Task:**\nFor a random variable $X$ that follows a Poisson distribution, it is given that the sum of its expectation and its variance is equal to a known positive constant $S$. That is, $E[X] + \\operatorname{Var}(X) = S$.\n\nStarting from the fundamental definitions provided, first derive the expressions for the expectation $E[X]$ and the variance $\\operatorname{Var}(X)$ in terms of the parameter $\\lambda$. Then, use these results to derive the value of $\\lambda$ in terms of $S$.", "solution": "The expectation is defined by\n$$E[X]=\\sum_{k=0}^\\infty kP(X=k)=\\sum_{k=1}^\\infty k\\frac{\\lambda^k e^{-\\lambda}}{k!}\n=e^{-\\lambda}\\sum_{k=1}^\\infty\\frac{\\lambda^k}{(k-1)!}\n=\\lambda e^{-\\lambda}\\sum_{m=0}^\\infty\\frac{\\lambda^m}{m!}\n=\\lambda.$$\n\nNext, observe $k^2=k(k-1)+k$ so\n$$E[X^2]=\\sum_{k=0}^\\infty k^2P(X=k)\n=\\sum_{k=2}^\\infty\\frac{k(k-1)\\lambda^k e^{-\\lambda}}{k!}+\\sum_{k=1}^\\infty k\\frac{\\lambda^k e^{-\\lambda}}{k!}\n=e^{-\\lambda}\\sum_{k=2}^\\infty\\frac{\\lambda^k}{(k-2)!}+\\lambda\n=\\lambda^2+\\lambda.$$\n\nTherefore\n$$\\operatorname{Var}(X)=E[X^2]-(E[X])^2=(\\lambda^2+\\lambda)-\\lambda^2=\\lambda.$$\n\nSince $S=E[X]+\\operatorname{Var}(X)=\\lambda+\\lambda=2\\lambda$, it follows that\n$$\\lambda=\\frac{S}{2}.$$", "answer": "$$\\boxed{\\frac{S}{2}}$$", "id": "6536"}, {"introduction": "Many real-world systems can be modeled by the interaction of multiple random processes. This problem presents a practical scenario involving the net change in a queue, represented by the difference between two independent Poisson processes [@problem_id:1373948]. Solving it requires you to synthesize your knowledge of the Poisson distribution's properties with the rules for the expectation and variance of combinations of independent random variables, a common task in fields like operations research and computational biology.", "problem": "A computational biology server processes gene sequencing jobs. The number of jobs arriving at the server in a given one-hour interval, denoted by the random variable $X$, is modeled by a Poisson distribution. Concurrently, the number of jobs completed by the server in the same one-hour interval, denoted by $Y$, is also modeled by a Poisson distribution. The two processes are statistically independent.\n\nAn analyst observes the system and determines two key properties of the net change in the number of jobs in the server's queue, which is given by the difference $D = X - Y$. The expected value of this difference is found to be $E[D] = -2$, and the variance of this difference is $\\operatorname{Var}(D) = 12$.\n\nBased on this information, determine the variance of the number of arriving jobs, $\\operatorname{Var}(X)$, and the variance of the number of completed jobs, $\\operatorname{Var}(Y)$. Present your answer as a row matrix $\\begin{pmatrix} \\operatorname{Var}(X) & \\operatorname{Var}(Y) \\end{pmatrix}$.", "solution": "Let $X \\sim \\text{Poisson}(\\lambda_{X})$ and $Y \\sim \\text{Poisson}(\\lambda_{Y})$, with $X$ and $Y$ independent. For a Poisson random variable with rate parameter $\\lambda$, the mean and variance are both equal to $\\lambda$, so $E[X] = \\operatorname{Var}(X) = \\lambda_{X}$ and $E[Y] = \\operatorname{Var}(Y) = \\lambda_{Y}$.\n\nDefine $D = X - Y$. By linearity of expectation,\n$$\nE[D] = E[X - Y] = E[X] - E[Y] = \\lambda_{X} - \\lambda_{Y}.\n$$\nGiven $E[D] = -2$, we have\n$$\n\\lambda_{X} - \\lambda_{Y} = -2.\n$$\n\nSince $X$ and $Y$ are independent, the variance of their difference is the sum of their variances:\n$$\n\\operatorname{Var}(D) = \\operatorname{Var}(X - Y) = \\operatorname{Var}(X) + \\operatorname{Var}(Y) = \\lambda_{X} + \\lambda_{Y}.\n$$\nGiven $\\operatorname{Var}(D) = 12$, we have\n$$\n\\lambda_{X} + \\lambda_{Y} = 12.\n$$\n\nSolving the system\n$$\n\\begin{cases}\n\\lambda_{X} - \\lambda_{Y} = -2, \\\\\n\\lambda_{X} + \\lambda_{Y} = 12,\n\\end{cases}\n$$\nadd the equations to obtain $2\\lambda_{X} = 10$, hence $\\lambda_{X} = 5$. Substituting into $\\lambda_{X} + \\lambda_{Y} = 12$ gives $\\lambda_{Y} = 7$.\n\nTherefore, $\\operatorname{Var}(X) = 5$ and $\\operatorname{Var}(Y) = 7$.", "answer": "$$\\boxed{\\begin{pmatrix} 5 & 7 \\end{pmatrix}}$$", "id": "1373948"}, {"introduction": "Moment Generating Functions (MGFs) offer a powerful and elegant method for characterizing probability distributions and their moments. This exercise challenges you to work backwards, identifying a Poisson distribution and its parameter from a given MGF [@problem_id:1373933]. Furthermore, it provides valuable practice in applying the properties of variance to a linear transformation of a random variable, a fundamental skill for manipulating and understanding data.", "problem": "A discrete random variable $X$ is used to model a certain stochastic process. The statistical properties of $X$ are fully described by its Moment Generating Function (MGF), which is given by the expression:\n$$M_X(t) = \\exp(4(\\exp(t)-1))$$\nA new random variable $Y$ is defined as a linear transformation of $X$ according to the equation $Y = 3 - 2X$.\n\nCalculate the variance of the random variable $Y$.", "solution": "We are given the MGF of $X$ as $M_{X}(t)=\\exp(4(\\exp(t)-1))$. By the definition of the MGF, the first and second moments can be obtained from derivatives at zero:\n$$\nE[X]=M_{X}'(0), \\quad E[X^{2}]=M_{X}''(0), \\quad \\operatorname{Var}(X)=E[X^{2}]-(E[X])^{2}.\n$$\nDifferentiate $M_{X}(t)$:\n$$\nM_{X}'(t)=\\frac{d}{dt}\\left[\\exp(4(\\exp(t)-1))\\right]=\\exp(4(\\exp(t)-1))\\cdot 4\\exp(t).\n$$\nEvaluating at $t=0$ gives\n$$\nE[X]=M_{X}'(0)=\\exp(4(\\exp(0)-1))\\cdot 4\\exp(0)=4.\n$$\nDifferentiate again to get $M_{X}''(t)$:\n$$\nM_{X}''(t)=\\frac{d}{dt}\\left[\\exp(4(\\exp(t)-1))\\cdot 4\\exp(t)\\right]=\\exp(4(\\exp(t)-1))\\left[(4\\exp(t))^{2}+4\\exp(t)\\right].\n$$\nEvaluating at $t=0$ yields\n$$\nE[X^{2}]=M_{X}''(0)=\\exp(4(\\exp(0)-1))\\left[(4\\exp(0))^{2}+4\\exp(0)\\right]=16+4=20.\n$$\nThus,\n$$\n\\operatorname{Var}(X)=E[X^{2}]-(E[X])^{2}=20-16=4.\n$$\nFor the linear transformation $Y=3-2X$, the variance transforms as\n$$\n\\operatorname{Var}(Y)=\\operatorname{Var}(3-2X)=(-2)^{2}\\operatorname{Var}(X)=4\\cdot 4=16.\n$$", "answer": "$$\\boxed{16}$$", "id": "1373933"}]}