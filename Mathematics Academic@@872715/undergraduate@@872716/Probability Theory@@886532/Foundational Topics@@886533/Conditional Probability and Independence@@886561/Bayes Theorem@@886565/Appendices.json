{"hands_on_practices": [{"introduction": "This first exercise serves as a fundamental workout for applying Bayes' theorem. It presents a classic scenario where you must update your initial belief about an event—in this case, having chosen a loaded die—after receiving new, relevant evidence. By working through this problem ([@problem_id:345]), you will practice the core mechanics of calculating posterior probability, which is the cornerstone of Bayesian reasoning.", "problem": "A bag contains $N$ six-sided dice, which are visually indistinguishable. One of these dice is \"loaded,\" while the other $N-1$ dice are \"fair.\"\n\nThe probability of rolling a six with a fair die is $p_F$. The probability of rolling a six with the loaded die is $p_L$, where $p_L  p_F$.\n\nAn experiment is conducted as follows: a single die is chosen at random from the bag, with each die having an equal probability of being selected. The chosen die is then rolled once. The outcome of the roll is observed to be a six.\n\nDerive an expression for the posterior probability that the die chosen from the bag was the loaded one, given this observation. Express your answer in terms of $N$, $p_L$, and $p_F$.", "solution": "Let $L$ be the event “chosen die is loaded” and $F$ the event “chosen die is fair,” and let $S$ be the event “roll result is six.”  We have\n$$P(L)=\\frac1N,\\quad P(F)=\\frac{N-1}N,$$\n$$P(S\\mid L)=p_L,\\quad P(S\\mid F)=p_F.$$\nBy Bayes’ theorem,\n$$\nP(L\\mid S)\n=\\frac{P(S\\mid L)\\,P(L)}{P(S)}\n=\\frac{p_L\\,(1/N)}{P(S\\mid L)\\,(1/N)+P(S\\mid F)\\,((N-1)/N)}\n=\\frac{p_L/N}{(p_L+(N-1)p_F)/N}\n=\\frac{p_L}{p_L+(N-1)p_F}.\n$$", "answer": "$$\\boxed{\\frac{p_L}{p_L + (N-1)p_F}}$$", "id": "345"}, {"introduction": "Moving from an abstract setup to a tangible application, this problem explores the reliability of a predictive model, a common challenge in fields from meteorology to medical diagnostics. This exercise ([@problem_id:357]) demonstrates a crucial insight of Bayesian analysis: the probability of an event given a positive test is not simply the test's accuracy. It forces you to account for both the base rate of the event and the rate of false alarms, providing a more complete and realistic understanding of diagnostic-style problems.", "problem": "A local weather station is evaluating the accuracy of its new forecasting model. Let $R$ be the event that it rains on a given day, and let $F$ be the event that the model forecasts rain for that day.\n\nThe station has collected historical data and determined the following probabilities:\n1.  The unconditional probability of rain on any given day is $P(R) = p_r$.\n2.  The probability that the model correctly forecasts rain, given that it actually rains, is $P(F|R) = p_t$. This represents the true positive rate.\n3.  The probability that the model incorrectly forecasts rain, given that it does not rain, is $P(F|R^c) = p_f$, where $R^c$ is the event that it does not rain. This represents the false positive rate.\n\nUsing these probabilities, derive an expression for the probability that it will actually rain, given that the model has forecasted rain, i.e., $P(R|F)$. Express your answer in terms of $p_r$, $p_t$, and $p_f$.", "solution": "We seek $P(R\\mid F)$.  By Bayes’ theorem,\n$$\nP(R\\mid F)=\\frac{P(F\\mid R)\\,P(R)}{P(F)}.\n$$\nThe total probability of a forecast of rain is\n$$\nP(F)=P(F\\mid R)\\,P(R)+P(F\\mid R^c)\\,P(R^c).\n$$\nSubstitute $P(R)=p_r$, $P(F\\mid R)=p_t$, $P(F\\mid R^c)=p_f$, and $P(R^c)=1-p_r$ to obtain\n$$\nP(F)=p_t\\,p_r+p_f\\,(1-p_r).\n$$\nTherefore\n$$\nP(R\\mid F)\n=\\frac{p_t\\,p_r}{p_t\\,p_r+p_f\\,(1-p_r)}.\n$$", "answer": "$$\\boxed{\\frac{p_t\\,p_r}{p_t\\,p_r+p_f\\,(1-p_r)}}$$", "id": "357"}, {"introduction": "This final practice problem tackles a variation of Bertrand's Box Paradox, a famous puzzle that often trips up our natural intuition. Such paradoxes highlight why a formal, systematic approach like Bayes' theorem is an indispensable tool in probability theory. By determining the probability that the second coin in a box is gold after seeing the first ([@problem_id:691205]), you will learn to trust the mathematical process over potentially misleading mental shortcuts.", "problem": "Consider a variation of the classic Bertrand's Box Paradox. There are three boxes, indistinguishable from the outside.\n- Box 1 contains two gold coins (GG).\n- Box 2 contains two silver coins (SS).\n- Box 3 contains one gold and one silver coin (GS).\n\nUnlike the classic paradox, the boxes are not chosen with equal probability. The prior probability of selecting the GG box is $p_G$, of selecting the SS box is $p_S$, and of selecting the GS box is $p_M$. These probabilities are known constants, greater than zero, and sum to one: $p_G + p_S + p_M = 1$.\n\nAn experiment is conducted where a box is chosen according to these prior probabilities, and then a single coin is drawn uniformly at random from the selected box. Upon inspection, the drawn coin is found to be gold.\n\nDerive a closed-form analytic expression for the posterior probability that the *other* coin in the chosen box is also gold, given the observation that the first drawn coin is gold. Express your answer in terms of the prior probabilities $p_G$ and $p_M$.", "solution": "We denote the events $B_{GG},B_{GS},B_{SS}$ for choosing the respective boxes, and $D$ for drawing a gold coin. The priors and conditional probabilities are\n$$P(B_{GG})=p_G,\\quad P(B_{GS})=p_M,\\quad P(B_{SS})=p_S,$$\n$$P(D\\mid B_{GG})=1,\\quad P(D\\mid B_{GS})=\\frac{1}{2},\\quad P(D\\mid B_{SS})=0.$$\n\nThe total probability of drawing gold is\n$$P(D)=p_G\\cdot1 + p_M\\cdot\\frac{1}{2} + p_S\\cdot0 = p_G + \\frac{1}{2}p_M.$$\n\nBy Bayes' theorem,\n$$P(B_{GG}\\mid D)=\\frac{P(D\\mid B_{GG})\\,P(B_{GG})}{P(D)} = \\frac{p_G}{p_G + \\frac{1}{2}p_M}.$$\n\nSince only the $GG$ box yields the other coin gold, the desired posterior probability is\n$$P(\\text{other coin is gold}\\mid D)=P(B_{GG}\\mid D)=\\frac{p_G}{p_G + \\frac{1}{2}p_M}.$$", "answer": "$$\\boxed{\\frac{p_G}{p_G + \\frac{1}{2}p_M}}$$", "id": "691205"}]}