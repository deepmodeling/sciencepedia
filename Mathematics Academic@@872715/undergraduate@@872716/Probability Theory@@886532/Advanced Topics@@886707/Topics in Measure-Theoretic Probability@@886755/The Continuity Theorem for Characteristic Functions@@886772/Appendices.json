{"hands_on_practices": [{"introduction": "The first step in mastering the Continuity Theorem is recognizing the unique correspondence between a characteristic function and its distribution. This exercise [@problem_id:1319208] provides a limiting characteristic function and asks you to identify the corresponding probability distribution from a list of common ones. This is a foundational test of the theorem's core principle: the convergence of characteristic functions implies the convergence of distributions to a unique limit.", "problem": "In the study of stochastic processes, the convergence of sequences of random variables is a fundamental concept. One powerful tool for analyzing this is the characteristic function of a random variable. The characteristic function of a random variable $X$ is defined as $\\phi_X(t) = E[\\exp(itX)]$, where $t$ is a real number and $i$ is the imaginary unit.\n\nConsider a sequence of random variables $X_1, X_2, X_3, \\dots$, denoted by $\\{X_n\\}_{n=1}^{\\infty}$. Let $\\phi_{X_n}(t)$ be the characteristic function of the random variable $X_n$. It is observed that for every real number $t$, the sequence of functions $\\phi_{X_n}(t)$ converges to a limiting function $\\phi(t)$ as $n \\to \\infty$. The limiting function is given by:\n$$ \\phi(t) = \\exp(-|t|) $$\n\nAccording to Lévy's continuity theorem, this convergence implies that the sequence of random variables $\\{X_n\\}$ converges in distribution to a random variable $X$, whose characteristic function is $\\phi(t)$.\n\nIdentify the probability distribution of the limiting random variable $X$.\n\nA. The standard Normal distribution, with probability density function $f(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-x^2/2)$.\n\nB. The Laplace distribution with location parameter 0 and scale parameter 1, with probability density function $f(x) = \\frac{1}{2} \\exp(-|x|)$.\n\nC. The Cauchy distribution with location parameter 0 and scale parameter 1, with probability density function $f(x) = \\frac{1}{\\pi(1+x^2)}$.\n\nD. The Exponential distribution with rate parameter $\\lambda=1$, with probability density function $f(x) = \\exp(-x)$ for $x \\ge 0$.\n\nE. The Logistic distribution with location 0 and scale 1, with probability density function $f(x) = \\frac{\\exp(-x)}{(1+\\exp(-x))^2}$.", "solution": "We are given that for each real $t$, the characteristic functions $\\phi_{X_{n}}(t)$ converge pointwise to $\\phi(t)=\\exp(-|t|)$. By Lévy’s continuity theorem, the sequence $\\{X_{n}\\}$ converges in distribution to a random variable $X$ whose characteristic function is $\\phi_{X}(t)=\\exp(-|t|)$. Since characteristic functions determine distributions uniquely, it suffices to identify the distribution whose characteristic function equals $\\exp(-|t|)$.\n\nConsider a standard Cauchy random variable $Y$ with location parameter $0$ and scale parameter $1$, having probability density function\n$$\nf_{Y}(x)=\\frac{1}{\\pi}\\frac{1}{1+x^{2}}.\n$$\nIts characteristic function is\n$$\n\\phi_{Y}(t)=\\mathbb{E}[\\exp(i t Y)]=\\int_{-\\infty}^{\\infty}\\exp(i t x)\\frac{1}{\\pi}\\frac{1}{1+x^{2}}\\,dx.\n$$\nWrite $\\exp(i t x)=\\cos(t x)+i\\sin(t x)$; the sine term integrates to zero by oddness, so\n$$\n\\phi_{Y}(t)=\\frac{1}{\\pi}\\int_{-\\infty}^{\\infty}\\frac{\\cos(t x)}{1+x^{2}}\\,dx.\n$$\nUsing the standard integral $\\int_{-\\infty}^{\\infty}\\frac{\\cos(t x)}{1+x^{2}}\\,dx=\\pi\\exp(-|t|)$ for real $t$, we obtain\n$$\n\\phi_{Y}(t)=\\exp(-|t|).\n$$\nTherefore, the limiting characteristic function $\\phi(t)=\\exp(-|t|)$ corresponds exactly to the standard Cauchy distribution with location $0$ and scale $1$. Among the given options, this is option C.\n\nFor completeness, we note that other listed distributions have different characteristic functions: Normal has $\\exp(-t^{2}/2)$, Laplace$(0,1)$ has $1/(1+t^{2})$, Exponential has $1/(1-i t)$ for rate $1$, and Logistic$(0,1)$ has a different form; none equal $\\exp(-|t|)$.", "answer": "$$\\boxed{C}$$", "id": "1319208"}, {"introduction": "Moving beyond simple recognition, this practice challenges you to actively compute the limit of a sequence of characteristic functions. In this problem [@problem_id:824959], you will use essential calculus tools like Taylor series to find the limiting function and then identify the resulting distribution's density. This process beautifully illustrates a key mechanism behind limit theorems, where a non-obvious sequence of distributions converges to the ubiquitous normal distribution.", "problem": "Consider a sequence of probability measures $(\\mu_n)_{n \\in \\mathbb{N}}$ on the real line $\\mathbb{R}$. The characteristic function $\\phi_n(t)$ corresponding to each measure $\\mu_n$ is given by:\n$$\n\\phi_n(t) = \\left(\\cos\\left(\\frac{t}{n}\\right)\\right)^{n^2}\n$$\nThe characteristic function of a probability measure $\\mu$ on $\\mathbb{R}$ with a density function $f(x)$ is defined as $\\phi(t) = \\int_{-\\infty}^{\\infty} e^{itx} f(x) dx$.\n\nAccording to Lévy's continuity theorem, if the sequence of characteristic functions $\\phi_n(t)$ converges pointwise for all $t \\in \\mathbb{R}$ to a function $\\phi(t)$ that is continuous at $t=0$, then the sequence of measures $\\mu_n$ converges weakly to a measure $\\mu$ whose characteristic function is $\\phi(t)$.\n\nAssuming that the weak limit measure $\\mu$ has a probability density function, denoted by $f(x)$, derive the closed-form expression for $f(x)$.", "solution": "1. The characteristic function of $\\mu_n$ is\n$$\n\\phi_n(t)=\\Bigl(\\cos\\frac{t}{n}\\Bigr)^{n^2}.\n$$\n2. Take the logarithm and use the expansion $\\cos u=1-\\tfrac{u^2}{2}+O(u^4)$:\n$$\n\\ln\\phi_n(t)\n=n^2\\ln\\!\\Bigl(1-\\tfrac{t^2}{2n^2}+O(n^{-4})\\Bigr)\n=n^2\\Bigl(-\\tfrac{t^2}{2n^2}+O(n^{-4})\\Bigr)\n=-\\tfrac{t^2}{2}+O(n^{-2}).\n$$\n3. As $n\\to\\infty$,\n$$\n\\phi(t)=\\lim_{n\\to\\infty}\\phi_n(t)\n=\\exp\\Bigl(-\\tfrac{t^2}{2}\\Bigr).\n$$\n4. The inverse Fourier transform gives the density:\n$$\nf(x)\n=\\frac{1}{2\\pi}\\int_{-\\infty}^\\infty e^{-itx}\\,e^{-t^2/2}\\,dt\n=\\frac{1}{\\sqrt{2\\pi}}\\,e^{-x^2/2}.\n$$", "answer": "$$\\boxed{\\frac{1}{\\sqrt{2\\pi}}\\exp\\!\\bigl(-\\tfrac{x^2}{2}\\bigr)}$$", "id": "824959"}, {"introduction": "The power of the Continuity Theorem extends to scenarios where characteristic functions are defined more abstractly. This exercise [@problem_id:1395659] presents characteristic functions as solutions to a family of differential equations, a common situation in the modeling of stochastic processes. Your task is to first solve for the characteristic functions and then analyze their limit, demonstrating how to combine analytical skills with probabilistic reasoning to determine the limiting behavior of a system.", "problem": "Consider a sequence of real-valued random variables $\\{X_n\\}_{n=1}^{\\infty}$. The characteristic function of each random variable $X_n$, denoted by $\\phi_{X_n}(t) = E[\\exp(itX_n)]$, is the unique solution to the following initial value problem:\n$$\n\\frac{d\\phi_{X_n}(t)}{dt} = (i \\mu_n - \\lambda_n t) \\phi_{X_n}(t)\n$$\nwith the initial condition $\\phi_{X_n}(0) = 1$.\n\nIn this differential equation, $t$ is a real variable, $i$ is the imaginary unit, and $\\{\\mu_n\\}_{n=1}^{\\infty}$ and $\\{\\lambda_n\\}_{n=1}^{\\infty}$ are sequences of real constants. It is given that these sequences converge to finite limits as $n \\to \\infty$:\n$$\n\\lim_{n\\to\\infty} \\mu_n = \\mu\n$$\n$$\n\\lim_{n\\to\\infty} \\lambda_n = \\lambda, \\quad \\text{where } \\lambda  0\n$$\nThe sequence of random variables $\\{X_n\\}$ converges in distribution to a limiting random variable $X$. Determine the mean and the variance of this limiting random variable $X$. Express your answer as an ordered pair (mean, variance) in terms of $\\mu$ and $\\lambda$.", "solution": "We are given, for each $n$, the characteristic function $\\phi_{X_{n}}(t)=E[\\exp(itX_{n})]$ as the unique solution of the linear first-order ODE\n$$\n\\frac{d\\phi_{X_{n}}(t)}{dt}=(i\\mu_{n}-\\lambda_{n}t)\\,\\phi_{X_{n}}(t),\\quad \\phi_{X_{n}}(0)=1.\n$$\nSolve this ODE via the standard integrating approach for linear equations:\n$$\n\\phi_{X_{n}}(t)=\\phi_{X_{n}}(0)\\,\\exp\\left(\\int_{0}^{t}(i\\mu_{n}-\\lambda_{n}s)\\,ds\\right)\n=\\exp\\left(i\\mu_{n}t-\\frac{1}{2}\\lambda_{n}t^{2}\\right).\n$$\nThis is the characteristic function of a normal distribution with mean $\\mu_{n}$ and variance $\\lambda_{n}$, since the characteristic function of $N(m,\\sigma^{2})$ is $\\exp\\left(i m t-\\frac{1}{2}\\sigma^{2}t^{2}\\right)$. Therefore $X_{n}\\sim N(\\mu_{n},\\lambda_{n})$.\n\nAlternatively, the moments follow directly from derivatives at $t=0$. For any $X$ with characteristic function $\\phi_{X}(t)$,\n$$\nE[X]=-i\\,\\phi_{X}'(0),\\qquad \\operatorname{Var}(X)=-\\phi_{X}''(0)-\\left(E[X]\\right)^{2}.\n$$\nUsing $\\phi_{X_{n}}'(t)=(i\\mu_{n}-\\lambda_{n}t)\\phi_{X_{n}}(t)$ gives $\\phi_{X_{n}}'(0)=i\\mu_{n}$, hence $E[X_{n}]=\\mu_{n}$. Differentiating again,\n$$\n\\phi_{X_{n}}''(t)=-\\lambda_{n}\\phi_{X_{n}}(t)+(i\\mu_{n}-\\lambda_{n}t)^{2}\\phi_{X_{n}}(t),\n$$\nso $\\phi_{X_{n}}''(0)=-\\lambda_{n}-\\mu_{n}^{2}$ and thus $\\operatorname{Var}(X_{n})=\\lambda_{n}$.\n\nGiven $\\mu_{n}\\to\\mu$ and $\\lambda_{n}\\to\\lambda$ with $\\lambda0$, we have pointwise convergence of characteristic functions\n$$\n\\phi_{X_{n}}(t)\\to \\exp\\left(i\\mu t-\\frac{1}{2}\\lambda t^{2}\\right),\n$$\nwhich is the characteristic function of $N(\\mu,\\lambda)$. Hence the limiting random variable $X$ is normal with mean $\\mu$ and variance $\\lambda$. Therefore, the mean and variance of $X$ are $(\\mu,\\lambda)$.", "answer": "$$\\boxed{\\begin{pmatrix}\\mu  \\lambda\\end{pmatrix}}$$", "id": "1395659"}]}