{"hands_on_practices": [{"introduction": "The essence of an exchangeable sequence lies in its inherent symmetry, meaning the joint distribution is invariant under any permutation of the variables' indices. This practice invites you to explore this core principle in a tangible way by calculating a conditional expectation [@problem_id:1360777]. By leveraging the symmetry property, you will discover how knowing the sum of a set of exchangeable variables provides an equal amount of information about each individual component, leading to a surprisingly simple and intuitive result.", "problem": "Consider a system of $n$ interacting agents, for instance, traders in a financial market or individuals in a social network. Let the random variable $X_i$ represent a specific numerical attribute of agent $i$, for $i = 1, 2, \\dots, n$. The agents' attributes are not necessarily independent due to their mutual interactions. However, the system is known to be symmetric, meaning that if we were to arbitrarily relabel the agents, the joint statistical properties of their attributes would remain the same. This implies that the sequence of random variables $(X_1, X_2, \\dots, X_n)$ is finite exchangeable.\n\nA sequence of random variables $(X_1, \\dots, X_n)$ is defined as finite exchangeable if for any permutation $\\pi$ of the indices $\\{1, 2, \\dots, n\\}$, the joint probability distribution of $(X_{\\pi(1)}, X_{\\pi(2)}, \\dots, X_{\\pi(n)})$ is identical to the joint probability distribution of $(X_1, X_2, \\dots, X_n)$. Assume that the expectation $E[X_i]$ exists for all $i$.\n\nSuppose a global measurement of the system reveals only the total sum of the attributes, which is found to be a specific value $s$. That is, we have the condition that $\\sum_{i=1}^n X_i = s$.\n\nGiven this information, determine the conditional expectation of the attribute of the first agent, $E\\left[X_1 \\mid \\sum_{i=1}^n X_i = s\\right]$. Express your answer as an analytic expression in terms of $n$ and $s$.", "solution": "Let $S \\equiv \\sum_{i=1}^{n} X_{i}$. By finite exchangeability, for any permutation $\\pi$ of $\\{1,\\dots,n\\}$, the joint law of $(X_{1},\\dots,X_{n})$ equals that of $(X_{\\pi(1)},\\dots,X_{\\pi(n)})$. In particular, for any bounded measurable function $h$ and any $j \\in \\{1,\\dots,n\\}$, choose a permutation that swaps indices $1$ and $j$. Since $S$ is symmetric under permutations, $S$ is invariant under such swaps. Therefore,\n$$\nE\\!\\left[X_{1} h(S)\\right] \\;=\\; E\\!\\left[X_{j} h(S)\\right].\n$$\nBy the defining property of conditional expectation and the fact that $h(S)$ is measurable with respect to $\\sigma(S)$, this implies\n$$\nE\\!\\left[E\\!\\left[X_{1}\\mid S\\right] h(S)\\right] \\;=\\; E\\!\\left[E\\!\\left[X_{j}\\mid S\\right] h(S)\\right]\n$$\nfor all bounded measurable $h$. Hence,\n$$\nE\\!\\left[X_{1}\\mid S\\right] \\;=\\; E\\!\\left[X_{j}\\mid S\\right] \\quad \\text{almost surely for all } j,\n$$\nso there exists a function $m$ such that $E[X_{i}\\mid S]=m(S)$ almost surely for every $i$.\n\nUsing linearity of conditional expectation,\n$$\nE\\!\\left[S \\mid S\\right] \\;=\\; E\\!\\left[\\sum_{i=1}^{n} X_{i} \\,\\bigg|\\, S\\right] \\;=\\; \\sum_{i=1}^{n} E\\!\\left[X_{i}\\mid S\\right] \\;=\\; \\sum_{i=1}^{n} m(S) \\;=\\; n\\,m(S).\n$$\nBut $E[S\\mid S]=S$ almost surely, so\n$$\nn\\,m(S) \\;=\\; S \\quad \\Rightarrow \\quad m(S) \\;=\\; \\frac{S}{n} \\quad \\text{almost surely}.\n$$\nThus, for any specific value $s$ of the sum,\n$$\nE\\!\\left[X_{1}\\mid S=s\\right] \\;=\\; \\frac{s}{n}.\n$$", "answer": "$$\\boxed{\\frac{s}{n}}$$", "id": "1360777"}, {"introduction": "While independence simplifies many calculations, real-world variables are often correlated in a symmetric way. Exchangeability provides a tractable model for this kind of dependence, implying a constant variance for each variable and a constant covariance between any distinct pair. This exercise [@problem_id:1360765] guides you through calculating the variance of the sum of an exchangeable sequence, a fundamental task that highlights how correlation affects the uncertainty of an aggregate measurement.", "problem": "An agricultural technology company deploys a network of $N$ sensors to monitor soil moisture levels in a large field. Let $X_i$ be the random variable representing the measurement from the $i$-th sensor, for $i=1, 2, \\dots, N$. Due to the manufacturing process and calibration, the sensors are considered statistically identical. This implies that the variance of the measurement from any single sensor is the same, which we denote as $\\sigma^2$. Formally, $\\text{Var}(X_i) = \\sigma^2$ for all $i$.\n\nBecause the sensors are located in the same field, they are subject to common environmental factors, such as rainfall and irrigation patterns. Consequently, their readings are not statistically independent. The model used by the company assumes that the covariance between the readings of any two distinct sensors is a constant value, which we denote as $\\gamma$. Formally, $\\text{Cov}(X_i, X_j) = \\gamma$ for all $i \\neq j$.\n\nThe company aggregates the data by calculating the sum of all sensor readings, $S_N = \\sum_{i=1}^N X_i$. Determine an expression for the variance of this sum, $\\text{Var}(S_N)$, in terms of $N$, $\\sigma^2$, and $\\gamma$.", "solution": "Let $S_{N}=\\sum_{i=1}^{N} X_{i}$. The variance of a sum of random variables satisfies\n$$\n\\operatorname{Var}\\!\\left(\\sum_{i=1}^{N} X_{i}\\right)=\\sum_{i=1}^{N} \\operatorname{Var}(X_{i})+2\\sum_{1 \\leq i<j \\leq N} \\operatorname{Cov}(X_{i},X_{j}).\n$$\nGiven $\\operatorname{Var}(X_{i})=\\sigma^{2}$ for all $i$ and $\\operatorname{Cov}(X_{i},X_{j})=\\gamma$ for all $i \\neq j$, we compute each term.\n\nFirst,\n$$\n\\sum_{i=1}^{N} \\operatorname{Var}(X_{i})=\\sum_{i=1}^{N} \\sigma^{2}=N\\sigma^{2}.\n$$\nSecond,\n$$\n2\\sum_{1 \\leq i<j \\leq N} \\operatorname{Cov}(X_{i},X_{j})=2\\sum_{1 \\leq i<j \\leq N} \\gamma=2 \\binom{N}{2} \\gamma=2 \\cdot \\frac{N(N-1)}{2}\\gamma=N(N-1)\\gamma.\n$$\nTherefore,\n$$\n\\operatorname{Var}(S_{N})=N\\sigma^{2}+N(N-1)\\gamma.\n$$", "answer": "$$\\boxed{N\\sigma^{2}+N(N-1)\\gamma}$$", "id": "1360765"}, {"introduction": "A profound insight from de Finetti's theorem is that exchangeable sequences can often be viewed as mixtures of simpler, independent and identically distributed (IID) processes. This exercise [@problem_id:1360778] provides a concrete demonstration of this principle through the lens of a Bayesian hierarchical model. You will derive the joint distribution for a sequence that is conditionally IID, revealing how integrating out a shared random parameter produces the signature symmetry of an exchangeable sequence.", "problem": "In a semiconductor manufacturing plant, a new etching process is being developed. The probability, $P$, that a single microchip passes the quality control check is unknown and can fluctuate daily due to environmental sensitivities. This daily probability $P$ is modeled as a random variable following a Beta distribution with known shape parameters $\\alpha > 0$ and $\\beta > 0$. The probability density function of the Beta distribution is given by $f(p; \\alpha, \\beta) = \\frac{p^{\\alpha-1}(1-p)^{\\beta-1}}{B(\\alpha, \\beta)}$ for $p \\in (0,1)$, where $B(\\alpha, \\beta)$ is the Beta function.\n\nOn a given day, with a fixed but unknown success probability $P$, a series of $n$ independent experiments are conducted. In each experiment $i$ (where $i=1, \\dots, n$), chips are tested sequentially until the first passing chip is found. Let $X_i$ be the number of the trial on which the first passing chip is observed in the $i$-th experiment. Given $P$, each $X_i$ follows a Geometric distribution with a probability mass function (PMF) defined as $P(X_i = k | P) = (1-P)^{k-1}P$ for $k \\in \\{1, 2, 3, \\dots\\}$.\n\nAssuming the random variables $X_1, X_2, \\dots, X_n$ are conditionally independent given the value of $P$, determine their joint unconditional probability mass function, $p(x_1, x_2, \\dots, x_n)$.\n\nExpress your answer as a closed-form analytic expression in terms of $x_1, \\dots, x_n, n, \\alpha, \\beta,$ and the Beta function $B(a,b)$.", "solution": "We are given that, conditional on a fixed success probability $P=p$, the random variables $X_{1},\\dots,X_{n}$ are independent and each has a Geometric distribution with support $\\{1,2,3,\\dots\\}$ and PMF\n$$\n\\Pr(X_{i}=x_{i}\\mid P=p)=(1-p)^{x_{i}-1}p.\n$$\nTherefore, the joint conditional PMF of $(X_{1},\\dots,X_{n})$ given $P=p$ is\n$$\n\\Pr(X_{1}=x_{1},\\dots,X_{n}=x_{n}\\mid P=p)=\\prod_{i=1}^{n}(1-p)^{x_{i}-1}p\n=p^{n}(1-p)^{\\sum_{i=1}^{n}x_{i}-n}.\n$$\nThe prior for $P$ is $\\operatorname{Beta}(\\alpha,\\beta)$ with density\n$$\nf(p;\\alpha,\\beta)=\\frac{p^{\\alpha-1}(1-p)^{\\beta-1}}{B(\\alpha,\\beta)},\\quad p\\in(0,1).\n$$\nBy the law of total probability (marginalizing over $P$), the unconditional joint PMF is\n$$\np(x_{1},\\dots,x_{n})\n=\\int_{0}^{1}\\Pr(X_{1}=x_{1},\\dots,X_{n}=x_{n}\\mid P=p)\\,f(p;\\alpha,\\beta)\\,dp.\n$$\nSubstituting the expressions derived above,\n$$\np(x_{1},\\dots,x_{n})\n=\\int_{0}^{1}p^{n}(1-p)^{\\sum_{i=1}^{n}x_{i}-n}\\cdot \\frac{p^{\\alpha-1}(1-p)^{\\beta-1}}{B(\\alpha,\\beta)}\\,dp\n=\\frac{1}{B(\\alpha,\\beta)}\\int_{0}^{1}p^{\\alpha+n-1}(1-p)^{\\beta+\\sum_{i=1}^{n}x_{i}-n-1}\\,dp.\n$$\nRecognizing the integral as the Beta function definition $B(a,b)=\\int_{0}^{1}t^{a-1}(1-t)^{b-1}\\,dt$, we obtain\n$$\np(x_{1},\\dots,x_{n})=\\frac{B\\!\\left(\\alpha+n,\\ \\beta+\\sum_{i=1}^{n}x_{i}-n\\right)}{B(\\alpha,\\beta)},\n$$\nfor $x_{i}\\in\\{1,2,3,\\dots\\}$, $i=1,\\dots,n$. This is a closed-form expression in terms of $x_{1},\\dots,x_{n},n,\\alpha,\\beta$, and the Beta function $B(a,b)$.", "answer": "$$\\boxed{\\frac{B\\!\\left(\\alpha+n,\\ \\beta+\\sum_{i=1}^{n}x_{i}-n\\right)}{B(\\alpha,\\beta)}}$$", "id": "1360778"}]}