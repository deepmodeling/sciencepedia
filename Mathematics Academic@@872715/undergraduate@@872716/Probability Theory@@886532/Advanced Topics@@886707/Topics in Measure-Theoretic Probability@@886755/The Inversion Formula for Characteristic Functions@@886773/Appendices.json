{"hands_on_practices": [{"introduction": "The inversion formula provides the crucial bridge back from the characteristic function to the probability distribution itself. For continuous random variables with an integrable characteristic function, this involves a Fourier inversion integral. This practice [@problem_id:1399489] offers a classic exercise in applying this formula to derive a probability density function (PDF), reinforcing the fundamental duality between a distribution and its transform.", "problem": "Let $X$ be a continuous random variable. The characteristic function of $X$, denoted by $\\phi_X(t)$, is defined as the expected value of $\\exp(itX)$, where $t$ is a real number and $i$ is the imaginary unit.\n\nSuppose the characteristic function for $X$ is given by the triangular function:\n$$ \\phi_X(t) = \\max(0, 1-|t|) $$\nDetermine the probability density function (PDF), $f_X(x)$, corresponding to this characteristic function. Present your answer as a function of $x$.", "solution": "We use the Fourier inversion formula for characteristic functions. By definition, $\\phi_{X}(t) = \\int_{-\\infty}^{\\infty} \\exp(i t x) f_{X}(x)\\,dx$, and when $\\phi_{X}$ is integrable, the density is recovered by\n$$\nf_{X}(x) = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} \\exp(-i t x)\\,\\phi_{X}(t)\\,dt.\n$$\nHere $\\phi_{X}(t) = \\max(0, 1 - |t|) = (1 - |t|)\\mathbf{1}_{\\{|t|\\leq 1\\}}$, so\n$$\nf_{X}(x) = \\frac{1}{2\\pi} \\int_{-1}^{1} (1 - |t|)\\,\\exp(-i t x)\\,dt.\n$$\nThe integrand is even in $t$ after taking the real part, hence\n$$\nf_{X}(x) = \\frac{1}{\\pi} \\int_{0}^{1} (1 - t)\\,\\cos(t x)\\,dt.\n$$\nCompute the integral by splitting and integrating by parts:\n$$\n\\int_{0}^{1} (1 - t)\\cos(tx)\\,dt = \\int_{0}^{1} \\cos(tx)\\,dt - \\int_{0}^{1} t\\cos(tx)\\,dt.\n$$\nThe first term is\n$$\n\\int_{0}^{1} \\cos(tx)\\,dt = \\frac{\\sin(x)}{x}.\n$$\nFor the second term, integrate by parts with $u = t$ and $dv = \\cos(tx)\\,dt$, so $du = dt$ and $v = \\frac{\\sin(tx)}{x}$. Then\n$$\n\\int_{0}^{1} t\\cos(tx)\\,dt = \\left.\\frac{t\\sin(tx)}{x}\\right|_{0}^{1} - \\int_{0}^{1} \\frac{\\sin(tx)}{x}\\,dt = \\frac{\\sin(x)}{x} - \\frac{1}{x}\\int_{0}^{1} \\sin(tx)\\,dt.\n$$\nEvaluate the remaining integral:\n$$\n\\int_{0}^{1} \\sin(tx)\\,dt = \\left.-\\frac{\\cos(tx)}{x}\\right|_{0}^{1} = \\frac{1 - \\cos(x)}{x}.\n$$\nTherefore,\n$$\n\\int_{0}^{1} t\\cos(tx)\\,dt = \\frac{\\sin(x)}{x} - \\frac{1 - \\cos(x)}{x^{2}}.\n$$\nSubtracting gives\n$$\n\\int_{0}^{1} (1 - t)\\cos(tx)\\,dt = \\frac{\\sin(x)}{x} - \\left(\\frac{\\sin(x)}{x} - \\frac{1 - \\cos(x)}{x^{2}}\\right) = \\frac{1 - \\cos(x)}{x^{2}}.\n$$\nHence\n$$\nf_{X}(x) = \\frac{1}{\\pi}\\cdot \\frac{1 - \\cos(x)}{x^{2}}.\n$$\nUsing the identity $1 - \\cos(x) = 2\\sin^{2}(x/2)$, an equivalent nonnegative form is\n$$\nf_{X}(x) = \\frac{2}{\\pi}\\,\\frac{\\sin^{2}(x/2)}{x^{2}},\n$$\nwith the continuous extension at $x = 0$ equal to $\\lim_{x\\to 0} f_{X}(x) = \\frac{1}{2\\pi}$.", "answer": "$$\\boxed{f_{X}(x)=\\frac{2}{\\pi}\\,\\frac{\\sin^{2}\\!\\left(\\frac{x}{2}\\right)}{x^{2}}}$$", "id": "1399489"}, {"introduction": "While continuous variables use an integral over the entire real line, discrete variables require a different form of the inversion formula defined over a finite interval, typically $[-\\pi, \\pi]$. This exercise [@problem_id:1399475] demonstrates how to apply this discrete inversion formula to recover the probability mass function (PMF) of the well-known Poisson distribution. Mastering this technique is essential, as it often involves combining the integral formula with powerful tools like series expansions.", "problem": "Let $X$ be a discrete random variable that can take any non-negative integer value $k$, where $k \\in \\{0, 1, 2, \\dots\\}$. The characteristic function of $X$ is given by\n$$\n\\phi_X(t) = \\exp(\\lambda(e^{it}-1))\n$$\nwhere $t$ is a real number and $\\lambda$ is a positive real constant.\n\nUsing the properties of characteristic functions, determine the probability mass function (PMF), $P(X=k)$, for this random variable. Express your answer as a function of the integer $k$ and the constant $\\lambda$.", "solution": "The problem asks for the probability mass function (PMF), $P(X=k)$, of a discrete random variable $X$ given its characteristic function $\\phi_X(t)$. For a discrete random variable taking integer values, the PMF can be recovered from the characteristic function using the Fourier inversion formula:\n$$\nP(X=k) = \\frac{1}{2\\pi} \\int_{-\\pi}^{\\pi} e^{-ikt} \\phi_X(t) dt\n$$\nWe are given $\\phi_X(t) = \\exp(\\lambda(e^{it}-1))$. Substituting this into the inversion formula gives:\n$$\nP(X=k) = \\frac{1}{2\\pi} \\int_{-\\pi}^{\\pi} e^{-ikt} \\exp(\\lambda(e^{it}-1)) dt\n$$\nWe can separate the exponential term into two parts:\n$$\n\\exp(\\lambda(e^{it}-1)) = \\exp(\\lambda e^{it} - \\lambda) = \\exp(-\\lambda) \\exp(\\lambda e^{it})\n$$\nThe term $\\exp(-\\lambda)$ is a constant with respect to $t$, so it can be moved outside the integral.\n$$\nP(X=k) = \\frac{\\exp(-\\lambda)}{2\\pi} \\int_{-\\pi}^{\\pi} e^{-ikt} \\exp(\\lambda e^{it}) dt\n$$\nNow, we can expand the term $\\exp(\\lambda e^{it})$ using its Maclaurin series expansion, $e^z = \\sum_{n=0}^{\\infty} \\frac{z^n}{n!}$, with $z = \\lambda e^{it}$:\n$$\n\\exp(\\lambda e^{it}) = \\sum_{n=0}^{\\infty} \\frac{(\\lambda e^{it})^n}{n!} = \\sum_{n=0}^{\\infty} \\frac{\\lambda^n e^{int}}{n!}\n$$\nSubstitute this series back into the integral expression for $P(X=k)$:\n$$\nP(X=k) = \\frac{\\exp(-\\lambda)}{2\\pi} \\int_{-\\pi}^{\\pi} e^{-ikt} \\left( \\sum_{n=0}^{\\infty} \\frac{\\lambda^n e^{int}}{n!} \\right) dt\n$$\nAssuming uniform convergence of the series, we can interchange the order of integration and summation:\n$$\nP(X=k) = \\frac{\\exp(-\\lambda)}{2\\pi} \\sum_{n=0}^{\\infty} \\frac{\\lambda^n}{n!} \\int_{-\\pi}^{\\pi} e^{-ikt} e^{int} dt\n$$\nCombine the exponential terms inside the integral:\n$$\nP(X=k) = \\frac{\\exp(-\\lambda)}{2\\pi} \\sum_{n=0}^{\\infty} \\frac{\\lambda^n}{n!} \\int_{-\\pi}^{\\pi} e^{i(n-k)t} dt\n$$\nNow we evaluate the integral $\\int_{-\\pi}^{\\pi} e^{i(n-k)t} dt$. We must consider two cases based on the value of the integer $n-k$.\n\nCase 1: $n \\neq k$. In this case, $n-k$ is a non-zero integer.\n$$\n\\int_{-\\pi}^{\\pi} e^{i(n-k)t} dt = \\left[ \\frac{e^{i(n-k)t}}{i(n-k)} \\right]_{-\\pi}^{\\pi} = \\frac{e^{i(n-k)\\pi} - e^{-i(n-k)\\pi}}{i(n-k)}\n$$\nUsing Euler's formula, $e^{i\\theta} = \\cos(\\theta) + i\\sin(\\theta)$, the numerator becomes $2i\\sin((n-k)\\pi)$. Since $n-k$ is a non-zero integer, $\\sin((n-k)\\pi) = 0$. Therefore, the integral evaluates to 0 for $n \\neq k$.\n\nCase 2: $n = k$. In this case, $n-k = 0$.\n$$\n\\int_{-\\pi}^{\\pi} e^{i(0)t} dt = \\int_{-\\pi}^{\\pi} e^0 dt = \\int_{-\\pi}^{\\pi} 1 dt = [t]_{-\\pi}^{\\pi} = \\pi - (-\\pi) = 2\\pi\n$$\nThis means that the integral acts as an orthogonal filter. It is zero for all terms in the summation except for the single term where $n=k$. The infinite sum collapses to just this one term.\n\nSubstituting this result back into the expression for $P(X=k)$:\n$$\nP(X=k) = \\frac{\\exp(-\\lambda)}{2\\pi} \\left( \\frac{\\lambda^k}{k!} \\times 2\\pi \\right)\n$$\nAll other terms in the summation are multiplied by zero. The $2\\pi$ terms cancel out, leaving the final expression for the PMF:\n$$\nP(X=k) = \\frac{\\exp(-\\lambda) \\lambda^k}{k!}\n$$\nThis is the probability mass function for a Poisson distribution with parameter $\\lambda$.", "answer": "$$\\boxed{\\frac{\\exp(-\\lambda) \\lambda^k}{k!}}$$", "id": "1399475"}, {"introduction": "Having seen the separate formulas for continuous and discrete cases, it is natural to wonder about the connection between them. This thought experiment [@problem_id:1399524] explores what happens when we formally apply the continuous inversion formula to a simple discrete random variable, whose characteristic function is not absolutely integrable. The result is a generalized PDF expressed with Dirac delta functions, elegantly unifying the two perspectives by representing discrete probability masses as densities concentrated at single points.", "problem": "A discrete random variable $X$ can take one of two values, $-1$ or $1$, with equal probability. That is, its probability mass function (PMF) is given by $P(X=-1) = P(X=1) = 1/2$. The characteristic function of a random variable $X$, denoted $\\phi_X(t)$, is defined as the expectation value $\\phi_X(t) = E[\\exp(itX)]$, where $t$ is a real variable and $i$ is the imaginary unit.\n\nFor a continuous random variable with a probability density function (PDF) $f_X(x)$, the PDF can be recovered from the characteristic function using the inversion formula:\n$$f_X(x) = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} \\exp(-itx) \\phi_X(t) dt$$\nNormally, this formula is not applied to discrete random variables, as their characteristic functions are not guaranteed to be absolutely integrable, which is a condition for the formula to yield a conventional function.\n\nYour task is to formally apply the continuous inversion formula to the characteristic function of the discrete random variable $X$ defined above. Evaluate the integral to find the resulting expression, which can be interpreted as a generalized probability density function for $X$. For the evaluation, you may use the following integral identity involving the Dirac delta function, $\\delta(u)$:\n$$ \\int_{-\\infty}^{\\infty} \\exp(ikut) dt = 2\\pi\\delta(ku) $$\nwhere $k$ is a non-zero real constant. Express your final answer in terms of $x$ and the Dirac delta function.", "solution": "The problem asks us to formally apply the continuous inversion formula to a specific discrete random variable $X$. Let's call the resulting generalized function $f(x)$.\n\n**Step 1: Calculate the characteristic function $\\phi_X(t)$**\n\nThe characteristic function is defined as $\\phi_X(t) = E[\\exp(itX)]$. For a discrete random variable, this is calculated by summing over all possible values of $X$:\n$$ \\phi_X(t) = \\sum_{k} P(X=x_k) \\exp(itx_k) $$\nIn our case, the possible values are $x_1 = -1$ and $x_2 = 1$, with probabilities $P(X=-1) = 1/2$ and $P(X=1) = 1/2$.\nSubstituting these values into the formula:\n$$ \\phi_X(t) = P(X=-1)\\exp(it(-1)) + P(X=1)\\exp(it(1)) $$\n$$ \\phi_X(t) = \\frac{1}{2}\\exp(-it) + \\frac{1}{2}\\exp(it) $$\nUsing Euler's formula, which states that $\\cos(t) = \\frac{\\exp(it) + \\exp(-it)}{2}$, we can simplify this expression:\n$$ \\phi_X(t) = \\cos(t) $$\n\n**Step 2: Set up the inversion integral**\n\nThe continuous inversion formula is given as $f(x) = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} \\exp(-itx) \\phi_X(t) dt$. We substitute our calculated characteristic function, $\\phi_X(t) = \\cos(t)$, into this integral:\n$$ f(x) = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} \\exp(-itx) \\cos(t) dt $$\n\n**Step 3: Evaluate the integral**\n\nTo evaluate this integral, we first express $\\cos(t)$ back in its exponential form using Euler's formula:\n$$ f(x) = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} \\exp(-itx) \\left( \\frac{\\exp(it) + \\exp(-it)}{2} \\right) dt $$\nWe can take the constant factor $1/2$ out of the integral:\n$$ f(x) = \\frac{1}{4\\pi} \\int_{-\\infty}^{\\infty} \\exp(-itx) (\\exp(it) + \\exp(-it)) dt $$\nNow, we distribute the $\\exp(-itx)$ term and split the integral into two parts:\n$$ f(x) = \\frac{1}{4\\pi} \\left( \\int_{-\\infty}^{\\infty} \\exp(-itx)\\exp(it) dt + \\int_{-\\infty}^{\\infty} \\exp(-itx)\\exp(-it) dt \\right) $$\nCombine the exponents in each integral:\n$$ f(x) = \\frac{1}{4\\pi} \\left( \\int_{-\\infty}^{\\infty} \\exp(it(1-x)) dt + \\int_{-\\infty}^{\\infty} \\exp(-it(1+x)) dt \\right) $$\n\n**Step 4: Apply the given Dirac delta function identity**\n\nThe problem provides the identity $\\int_{-\\infty}^{\\infty} \\exp(ikut) dt = 2\\pi\\delta(ku)$. We apply this to both integrals.\n\nFor the first integral, $\\int_{-\\infty}^{\\infty} \\exp(it(1-x)) dt$, we can identify $u=t$ as the variable of integration. We match the exponent $it(1-x)$ to the form $ikut$. Here, we can set $k = (1-x)$. Applying the identity gives:\n$$ \\int_{-\\infty}^{\\infty} \\exp(it(1-x)) dt = 2\\pi\\delta(1-x) $$\nFor the second integral, $\\int_{-\\infty}^{\\infty} \\exp(-it(1+x)) dt$, we match the exponent $-it(1+x)$ to the form $ikut$. We can write the exponent as $it(-(1+x))$. Here, we set $k = -(1+x)$. Applying the identity gives:\n$$ \\int_{-\\infty}^{\\infty} \\exp(it(-(1+x))) dt = 2\\pi\\delta(-(1+x)) $$\nThe Dirac delta function is an even function, meaning $\\delta(-v) = \\delta(v)$. Therefore, $\\delta(-(1+x)) = \\delta(1+x)$.\n\n**Step 5: Combine the results to get the final expression**\n\nNow we substitute these results back into our expression for $f(x)$:\n$$ f(x) = \\frac{1}{4\\pi} \\left( 2\\pi\\delta(1-x) + 2\\pi\\delta(1+x) \\right) $$\nFactor out $2\\pi$ from the parentheses:\n$$ f(x) = \\frac{2\\pi}{4\\pi} (\\delta(1-x) + \\delta(1+x)) $$\n$$ f(x) = \\frac{1}{2} (\\delta(1-x) + \\delta(1+x)) $$\nFinally, we use the property $\\delta(a-x) = \\delta(x-a)$ to write the expression in a more standard form:\n$$ f(x) = \\frac{1}{2} (\\delta(x-1) + \\delta(x-(-1))) $$\n$$ f(x) = \\frac{1}{2} (\\delta(x-1) + \\delta(x+1)) $$\nThis generalized function represents the probability \"density\" of the discrete random variable, with point masses of size $1/2$ at $x=1$ and $x=-1$.", "answer": "$$\\boxed{\\frac{1}{2}\\left(\\delta(x-1) + \\delta(x+1)\\right)}$$", "id": "1399524"}]}