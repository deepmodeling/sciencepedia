{"hands_on_practices": [{"introduction": "We begin with a foundational exercise to build intuition for weak convergence. This problem uses a hypothetical model of a particle whose measured position gets progressively closer to zero. By calculating the limit of the expected value of an observable quantity, you will see firsthand how the Portmanteau Theorem's condition on bounded continuous functions confirms the convergence of the underlying probability measures to a single point [@problem_id:1404947].", "problem": "In a simplified model of a physical system, a particle's position is probed at discrete time steps $n=1, 2, 3, \\ldots$. At each step $n$, the measurement can only yield one of two outcomes: a position of $x = 1/n$ or a position of $x = -1/n$. Both outcomes are equally likely, each occurring with a probability of $1/2$.\n\nThe probability measure associated with the measurement at step $n$ is denoted by $P_n$. This can be expressed as $P_n = \\frac{1}{2}\\delta_{1/n} + \\frac{1}{2}\\delta_{-1/n}$, where $\\delta_c$ is the Dirac measure which concentrates all probability mass at the point $c$.\n\nConsider a bounded continuous observable quantity represented by the function $f(x) = \\exp(-x^2) \\cos\\left(\\frac{\\pi x}{4} + \\frac{\\pi}{6}\\right)$. The expected value of this observable at step $n$ is given by $E_n = \\int_{-\\infty}^{\\infty} f(x) \\, dP_n(x)$.\n\nDetermine the limiting value, $L$, of this sequence of expected values as $n$ approaches infinity. That is, find $L = \\lim_{n \\to \\infty} E_n$.\n\nProvide the final answer as a single closed-form analytic expression.", "solution": "By definition of the probability measure $P_{n}=\\frac{1}{2}\\delta_{1/n}+\\frac{1}{2}\\delta_{-1/n}$ and the property of the Dirac measure, the expected value is\n$$\nE_{n}=\\int_{-\\infty}^{\\infty} f(x)\\,dP_{n}(x)=\\frac{1}{2}f\\!\\left(\\frac{1}{n}\\right)+\\frac{1}{2}f\\!\\left(-\\frac{1}{n}\\right).\n$$\nWith $f(x)=\\exp(-x^{2})\\cos\\!\\left(\\frac{\\pi x}{4}+\\frac{\\pi}{6}\\right)$, this becomes\n$$\nE_{n}=\\frac{1}{2}\\exp\\!\\left(-\\frac{1}{n^{2}}\\right)\\left[\\cos\\!\\left(\\frac{\\pi}{4n}+\\frac{\\pi}{6}\\right)+\\cos\\!\\left(-\\frac{\\pi}{4n}+\\frac{\\pi}{6}\\right)\\right].\n$$\nUsing the trigonometric identity $\\cos(A+B)+\\cos(A-B)=2\\cos A\\cos B$ with $A=\\frac{\\pi}{6}$ and $B=\\frac{\\pi}{4n}$, we get\n$$\nE_{n}=\\exp\\!\\left(-\\frac{1}{n^{2}}\\right)\\cos\\!\\left(\\frac{\\pi}{6}\\right)\\cos\\!\\left(\\frac{\\pi}{4n}\\right).\n$$\nTaking the limit $n\\to\\infty$ and using continuity of $\\exp$ and $\\cos$ at $0$, we have $\\exp(-1/n^{2})\\to 1$ and $\\cos(\\pi/(4n))\\to 1$, hence\n$$\nL=\\lim_{n\\to\\infty}E_{n}=\\cos\\!\\left(\\frac{\\pi}{6}\\right)=\\frac{\\sqrt{3}}{2}.\n$$\nEquivalently, one can note that $P_{n}\\Rightarrow\\delta_{0}$ and, since $f$ is bounded and continuous, $\\int f\\,dP_{n}\\to f(0)=\\exp(0)\\cos(\\pi/6)=\\frac{\\sqrt{3}}{2}$, which agrees with the computation above.", "answer": "$$\\boxed{\\frac{\\sqrt{3}}{2}}$$", "id": "1404947"}, {"introduction": "Moving beyond convergence to a single point, this practice explores how a sequence of distributions can approach a continuous distribution, in this case, the uniform distribution on $[0, 1]$. You will apply a different but equivalent condition from the Portmanteau Theorem, which concerns the convergence of probabilities for certain types of sets. This exercise [@problem_id:1404886] will strengthen your understanding of how weak convergence describes the limiting behavior of the overall 'shape' of distributions.", "problem": "Consider a sequence of random variables $\\{X_n\\}_{n=1}^{\\infty}$ where, for each positive integer $n$, the random variable $X_n$ follows a uniform probability distribution over the interval $[1/n, 1]$. Your task is to find the specific real constant $c$ that satisfies the following limiting condition:\n$$\n\\lim_{n \\to \\infty} P(X_n  c) = \\frac{1}{3}\n$$\nProvide the exact value of $c$.", "solution": "For each positive integer $n$, the random variable $X_{n}$ is uniformly distributed on $[1/n, 1]$. For a constant $c \\in \\mathbb{R}$, the tail probability $P(X_{n}  c)$ is computed using the uniform distribution on $[a,b]$ where $a = 1/n$ and $b = 1$. The general form is\n$$\nP(X_{n}  c) =\n\\begin{cases}\n1,  c \\leq \\frac{1}{n}, \\\\\n\\displaystyle \\int_{c}^{1} \\frac{1}{1 - \\frac{1}{n}} \\, dx,  \\frac{1}{n}  c  1, \\\\\n0,  c \\geq 1.\n\\end{cases}\n$$\nEvaluating the integral in the middle case gives\n$$\nP(X_{n}  c) = \\frac{1 - c}{1 - \\frac{1}{n}} = \\frac{n(1 - c)}{n - 1}, \\quad \\text{for } \\frac{1}{n}  c  1.\n$$\nWe now analyze the limit as $n \\to \\infty$ for different ranges of $c$:\n- If $c \\leq 0$, then for all sufficiently large $n$ we have $c \\leq \\frac{1}{n}$, hence $P(X_{n}  c) = 1$ and $\\lim_{n \\to \\infty} P(X_{n}  c) = 1$.\n- If $0  c  1$, then for all sufficiently large $n$ we have $\\frac{1}{n}  c  1$, hence\n$$\nP(X_{n}  c) = \\frac{n(1 - c)}{n - 1} \\to 1 - c \\quad \\text{as } n \\to \\infty,\n$$\nsince $\\frac{n}{n - 1} \\to 1$.\n- If $c \\geq 1$, then $P(X_{n}  c) = 0$ for all $n$, so the limit is $0$.\n\nTherefore,\n$$\n\\lim_{n \\to \\infty} P(X_{n}  c) =\n\\begin{cases}\n1,  c \\leq 0, \\\\\n1 - c,  0  c  1, \\\\\n0,  c \\geq 1.\n\\end{cases}\n$$\nTo satisfy $\\lim_{n \\to \\infty} P(X_{n}  c) = \\frac{1}{3}$, we must have $0  c  1$ and solve $1 - c = \\frac{1}{3}$, which yields $c = \\frac{2}{3}$. This value is consistent with the required range and uniquely satisfies the condition.", "answer": "$$\\boxed{\\frac{2}{3}}$$", "id": "1404886"}, {"introduction": "This final exercise serves as an important cautionary tale and deepens your understanding of what 'weak' convergence truly means. By analyzing a sequence of random variables that converges in distribution, you will discover that its expected value behaves unexpectedly. This classic example [@problem_id:1404921] is crucial for learning that convergence in distribution does not imply the convergence of moments, a subtlety that is fundamental to the correct application of probability theory.", "problem": "Consider a sequence of discrete random variables $\\{X_n\\}_{n=1}^{\\infty}$. For each integer $n \\geq 1$, the random variable $X_n$ can take one of two values: $1/n$ or $n$. The probabilities are given by $P(X_n = 1/n) = 1 - 1/n$ and $P(X_n = n) = 1/n$. This sequence could represent a simplified model for the error in an iterative algorithm at step $n$, where there is a high probability of a small, decreasing error, but a small probability of a large, diverging error.\n\nWhich of the following statements accurately describes the limiting behavior of the sequence $X_n$ as $n \\to \\infty$?\n\nA. $X_n$ converges in distribution to a random variable with a standard normal distribution.\nB. $X_n$ converges in distribution to a random variable that is a point mass at 0.\nC. $X_n$ converges in distribution to a random variable that is a point mass at 1.\nD. The sequence of random variables $\\{X_n\\}$ does not converge in distribution.\nE. $X_n$ converges in distribution to a point mass at 0, because the expected value of $X_n$ converges to 0.", "solution": "To determine the limiting behavior of the sequence of random variables $\\{X_n\\}$, we will use the definition of convergence in distribution (also known as weak convergence). A sequence of random variables $X_n$ converges in distribution to a random variable $X$, denoted $X_n \\xrightarrow{d} X$, if the Cumulative Distribution Function (CDF) of $X_n$, $F_{X_n}(x)$, converges to the CDF of $X$, $F_X(x)$, at every point $x$ where $F_X(x)$ is continuous.\n\nFirst, let's form an intuition about the limit. As $n \\to \\infty$, the probability that $X_n = 1/n$ approaches 1, while the value $1/n$ approaches 0. The probability that $X_n = n$ approaches 0. This suggests that the entire probability mass is concentrating at the point 0. So, we propose that the limiting random variable $X$ is a point mass at 0, meaning $P(X=0)=1$.\n\nThe CDF of the proposed limiting random variable $X$ is:\n$$\nF_X(x) = P(X \\le x) =\n\\begin{cases}\n0  \\text{if } x  0 \\\\\n1  \\text{if } x \\ge 0\n\\end{cases}\n$$\nThis CDF, $F_X(x)$, is continuous for all $x \\in \\mathbb{R}$ except at the point $x=0$, where it has a jump discontinuity. According to the definition of convergence in distribution, we need to check if $\\lim_{n \\to \\infty} F_{X_n}(x) = F_X(x)$ for all $x \\neq 0$.\n\nNext, let's find the CDF of $X_n$. Since $X_n$ can only take values $1/n$ and $n$, its CDF, $F_{X_n}(x) = P(X_n \\le x)$, is a step function:\n$$\nF_{X_n}(x) =\n\\begin{cases}\n0  \\text{if } x  1/n \\\\\n1 - 1/n  \\text{if } 1/n \\le x  n \\\\\n1  \\text{if } x \\ge n\n\\end{cases}\n$$\nNow, we analyze the limit of $F_{X_n}(x)$ as $n \\to \\infty$ for all $x \\neq 0$.\n\nCase 1: $x  0$.\nFor any $n \\geq 1$, we have $1/n  0  x$. Therefore, for any $x  0$, $F_{X_n}(x) = 0$ for all $n$.\n$$ \\lim_{n \\to \\infty} F_{X_n}(x) = \\lim_{n \\to \\infty} 0 = 0 $$\nThis matches $F_X(x)$ for $x  0$.\n\nCase 2: $x  0$.\nFor any given $x  0$, we can choose an integer $N$ large enough such that for all $n  N$, we have $1/n  x$. Specifically, this holds for all $n  1/x$. Also, for $n  x$, we have $x  n$. Thus, for any $n  \\max(x, 1/x)$, the inequality $1/n \\le x  n$ is satisfied.\nFor such large $n$, the CDF of $X_n$ at point $x$ is $F_{X_n}(x) = 1 - 1/n$.\nTaking the limit as $n \\to \\infty$:\n$$ \\lim_{n \\to \\infty} F_{X_n}(x) = \\lim_{n \\to \\infty} (1 - 1/n) = 1 $$\nThis matches $F_X(x)$ for $x  0$.\n\nSince $\\lim_{n \\to \\infty} F_{X_n}(x) = F_X(x)$ for all continuity points of $F_X$ (i.e., for all $x \\neq 0$), we conclude that $X_n$ converges in distribution to a point mass at 0. This means statement B is correct.\n\nNow let's evaluate the other options to be certain.\nA. The limit is a point mass, not a continuous distribution like the normal distribution. So, A is incorrect.\nC. The limit is a point mass at 0, not at 1. So, C is incorrect.\nD. We have just shown that the sequence does converge in distribution. So, D is incorrect.\nE. This statement claims that the convergence is due to the expectation of $X_n$ converging to 0. Let's calculate the expectation of $X_n$:\n$$ E[X_n] = \\sum_{k} k \\cdot P(X_n=k) = \\left(\\frac{1}{n}\\right) P\\left(X_n = \\frac{1}{n}\\right) + (n) P(X_n = n) $$\n$$ E[X_n] = \\left(\\frac{1}{n}\\right) \\left(1 - \\frac{1}{n}\\right) + (n) \\left(\\frac{1}{n}\\right) = \\frac{1}{n} - \\frac{1}{n^2} + 1 $$\nNow we take the limit of the expectation:\n$$ \\lim_{n \\to \\infty} E[X_n] = \\lim_{n \\to \\infty} \\left(1 + \\frac{1}{n} - \\frac{1}{n^2}\\right) = 1 + 0 - 0 = 1 $$\nThe expectation of $X_n$ converges to 1, not 0. Therefore, the reasoning provided in statement E is false. An accurate statement must have correct reasoning. Even though the first part of the statement (\"$X_n$ converges in distribution to a point mass at 0\") is true, the overall statement is not accurate because the justification is incorrect. This example famously illustrates that convergence in distribution does not imply convergence of moments (in this case, the mean).\n\nThus, statement B is the only accurate description of the limiting behavior.", "answer": "$$\\boxed{B}$$", "id": "1404921"}]}