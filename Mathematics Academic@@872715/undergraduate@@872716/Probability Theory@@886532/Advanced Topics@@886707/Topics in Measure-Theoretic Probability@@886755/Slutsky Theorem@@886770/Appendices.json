{"hands_on_practices": [{"introduction": "We begin with a foundational exercise that isolates the core mechanic of Slutsky's Theorem. This problem explores the limiting behavior of a ratio of two random sequences, a common structure in statistics where a centered statistic is scaled by an estimated parameter. By working through this example ([@problem_id:1955680]), you will see how convergence in probability allows us to treat a random denominator as a constant in the limit, simplifying the analysis significantly.", "problem": "In an advanced signal processing application, an engineer analyzes two independent sequences of measurements. The first sequence, represented by random variables $\\{A_n\\}_{n=1}^{\\infty}$, captures a noisy signal that has been centered. Theoretical models predict that as the number of measurements $n$ grows large, the distribution of $A_n$ approaches a Normal distribution with a mean of 0 and a variance of $\\sigma^2$. This is formally stated as $A_n \\xrightarrow{d} N(0, \\sigma^2)$, where $\\sigma > 0$.\n\nThe second sequence, $\\{B_n\\}_{n=1}^{\\infty}$, is an adaptive estimate of a system parameter. This estimator is known to be consistent, converging in probability to a non-zero positive constant $c$. This is formally stated as $B_n \\xrightarrow{p} c$, where $c > 0$.\n\nThe engineer is interested in the statistical properties of the calibrated signal, which is defined as the ratio $Z_n = \\frac{A_n}{B_n}$. Determine the limiting distribution of the sequence $\\{Z_n\\}$ as $n \\to \\infty$.\n\nSelect the correct description of the limiting distribution from the options below.\n\nA. A Normal distribution with mean 0 and variance $\\sigma^2 c^2$.\n\nB. A Student's t-distribution with $n-1$ degrees of freedom.\n\nC. A Normal distribution with mean 0 and variance $\\frac{\\sigma^2}{c^2}$.\n\nD. A Chi-squared distribution.\n\nE. The distribution degenerates to a constant value of 0.\n\nF. A Normal distribution with mean 0 and variance $\\sigma^2$.", "solution": "We are given $A_{n} \\xrightarrow{d} N(0,\\sigma^{2})$ with $\\sigma>0$ and $B_{n} \\xrightarrow{p} c$ with $c>0$. Define $Z_{n}=\\frac{A_{n}}{B_{n}}$.\n\nBecause $B_{n} \\xrightarrow{p} c$ with $c \\neq 0$, for any $\\varepsilon$ such that $0<\\varepsilon<c$, we have\n$$\n\\Pr\\big(|B_{n}-c|<\\varepsilon\\big) \\to 1,\n$$\nwhich implies\n$$\n\\Pr\\big(|B_{n}|>c-\\varepsilon\\big) \\to 1.\n$$\nIn particular, choosing $\\varepsilon=\\frac{c}{2}$ yields $\\Pr(|B_{n}|>\\frac{c}{2}) \\to 1$, so division by $B_{n}$ is well-defined with probability tending to $1$.\n\nBy Slutsky’s theorem (or the continuous mapping theorem applied to $f(x,y)=x/y$, which is continuous at $(x,c)$ for $c \\neq 0$), since $A_{n} \\xrightarrow{d} X$ with $X \\sim N(0,\\sigma^{2})$ and $B_{n} \\xrightarrow{p} c$, it follows that\n$$\n\\frac{A_{n}}{B_{n}} \\xrightarrow{d} \\frac{X}{c}.\n$$\nIf $X \\sim N(0,\\sigma^{2})$, then scaling by a constant gives\n$$\n\\frac{X}{c} \\sim N\\!\\left(0,\\frac{\\sigma^{2}}{c^{2}}\\right),\n$$\nbecause for any constant $a$, $\\operatorname{Var}(aX)=a^{2}\\operatorname{Var}(X)$ and $\\operatorname{E}[aX]=a\\,\\operatorname{E}[X]$.\n\nTherefore, the limiting distribution of $Z_{n}$ is Normal with mean $0$ and variance $\\frac{\\sigma^{2}}{c^{2}}$, which corresponds to option C.", "answer": "$$\\boxed{C}$$", "id": "1955680"}, {"introduction": "Real-world problems often require combining multiple statistical principles. This next exercise ([@problem_id:840102]) demonstrates how Slutsky's Theorem serves as a bridge between the Central Limit Theorem and the Law of Large Numbers. You will analyze a statistic formed by the product of a normalized sample mean and a sample proportion, a scenario that highlights how different limit theorems work in concert to determine the ultimate distribution of a complex estimator.", "problem": "Let $X_1, X_2, \\dots, X_n$ be a sequence of independent and identically distributed (i.i.d.) random variables from a population with a finite mean $\\mu$ and a finite, non-zero variance $\\sigma^2$.\nLet $Y_1, Y_2, \\dots, Y_n$ be a second sequence of i.i.d. random variables, independent of the first sequence, drawn from a Bernoulli distribution with success probability $p$, where $p \\in (0, 1)$.\n\nDefine the sample mean of the first sequence as $\\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^n X_i$, and the sample proportion of successes of the second sequence as $\\hat{p}_n = \\frac{1}{n}\\sum_{i=1}^n Y_i$.\n\nConsider the sequence of random variables $W_n$ defined by:\n$$W_n = \\hat{p}_n \\left( \\frac{\\sqrt{n}(\\bar{X}_n - \\mu)}{\\sigma} \\right)$$\nUsing the appropriate limit theorems, derive the probability density function (PDF) of the limiting distribution of $W_n$ as $n \\to \\infty$. Express your answer as a function of the variable $w$ and the parameter $p$.", "solution": "1. By the Central Limit Theorem for $\\{X_i\\}$,\n$$Z_n=\\frac{\\sqrt{n}(\\bar X_n-\\mu)}{\\sigma}\\;\\xrightarrow{d}\\;Z\\sim N(0,1).$$\n2. By the Law of Large Numbers for $\\{Y_i\\}$,\n$$\\hat p_n=\\frac1n\\sum_{i=1}^nY_i\\;\\xrightarrow{p}\\;p.$$\n3. Since $\\{X_i\\}$ and $\\{Y_i\\}$ are independent, $Z_n$ and $\\hat p_n$ are independent.  By Slutsky’s theorem,\n$$W_n=\\hat p_n\\,Z_n\\;\\xrightarrow{d}\\;p\\,Z.$$\n4. Hence the limit $pZ$ is normal with mean $0$ and variance $p^2$, so its PDF is\n$$f(w)=\\frac1{\\sqrt{2\\pi\\,p^2}}\\exp\\!\\biggl(-\\frac{w^2}{2p^2}\\biggr)\n=\\frac1{p\\sqrt{2\\pi}}\\exp\\!\\biggl(-\\frac{w^2}{2p^2}\\biggr).$$", "answer": "$$\\boxed{\\frac{1}{p\\sqrt{2\\pi}}\\exp\\!\\bigl(-\\tfrac{w^2}{2p^2}\\bigr)}$$", "id": "840102"}, {"introduction": "Slutsky's theorem is not just a theoretical curiosity; it is a critical tool for analyzing the behavior of statistical methods. This advanced problem ([@problem_id:840045]) delves into a practical scenario: the consequences of using a two-sample t-test when its assumption of equal variances is violated. By dissecting the test statistic, you will use Slutsky's theorem to derive its true asymptotic distribution and understand why the test may lead to incorrect conclusions.", "problem": "Consider two independent samples: $X_1, X_2, \\dots, X_{n_1}$ are independent and identically distributed (i.i.d.) random variables from a population with mean $\\mu_1$ and variance $\\sigma_1^2$, and $Y_1, Y_2, \\dots, Y_{n_2}$ are i.i.d. random variables from a population with mean $\\mu_2$ and variance $\\sigma_2^2$. Assume the population variances are unequal, i.e., $\\sigma_1^2 \\neq \\sigma_2^2$.\n\nA researcher, incorrectly assuming equal variances, uses the pooled-variance t-statistic to test the null hypothesis $H_0: \\mu_1 = \\mu_2$. The statistic is defined as:\n$$\nT_{n_1, n_2} = \\frac{\\bar{X}_{n_1} - \\bar{Y}_{n_2}}{\\sqrt{S_p^2 \\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}}\n$$\nwhere $\\bar{X}_{n_1}$ and $\\bar{Y}_{n_2}$ are the sample means, and $S_p^2$ is the pooled sample variance estimator:\n$$\nS_p^2 = \\frac{(n_1-1)S_1^2 + (n_2-1)S_2^2}{n_1 + n_2 - 2}\n$$\nwith $S_1^2$ and $S_2^2$ being the unbiased sample variances.\n\nWe are interested in the asymptotic behavior of this statistic as both sample sizes grow large. Assume that $n_1 \\to \\infty$ and $n_2 \\to \\infty$ in such a way that their relative proportion remains constant. Specifically, let $N = n_1 + n_2$ and assume that as $N \\to \\infty$, the ratio $\\frac{n_1}{N}$ converges to a constant $\\lambda \\in (0, 1)$.\n\nUnder the null hypothesis $H_0: \\mu_1 = \\mu_2$, the statistic $T_{n_1, n_2}$ converges in distribution to a normal distribution, $T_{n_1, n_2} \\xrightarrow{d} N(0, V)$.\n\nDerive the expression for the asymptotic variance $V$ in terms of $\\sigma_1^2$, $\\sigma_2^2$, and $\\lambda$.", "solution": "The problem asks for the asymptotic variance of the incorrectly used pooled-variance t-statistic. The statistic is given by:\n$$\nT_{n_1, n_2} = \\frac{\\bar{X}_{n_1} - \\bar{Y}_{n_2}}{\\sqrt{S_p^2 \\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}}\n$$\nWe will use Slutsky's theorem to find the limiting distribution. The theorem states that if $A_n \\xrightarrow{d} A$ and $B_n \\xrightarrow{p} b$ (a constant), then $A_n / B_n \\xrightarrow{d} A/b$.\n\n**Step 1: Analyze the numerator**\n\nLet's first establish the limiting distribution of the numerator, $\\bar{X}_{n_1} - \\bar{Y}_{n_2}$, when properly scaled. Under the null hypothesis $H_0: \\mu_1 = \\mu_2$, the mean of $\\bar{X}_{n_1} - \\bar{Y}_{n_2}$ is $E[\\bar{X}_{n_1} - \\bar{Y}_{n_2}] = \\mu_1 - \\mu_2 = 0$. The variance is $\\text{Var}(\\bar{X}_{n_1} - \\bar{Y}_{n_2}) = \\text{Var}(\\bar{X}_{n_1}) + \\text{Var}(\\bar{Y}_{n_2}) = \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}$.\n\nBy the Central Limit Theorem (CLT), the standardized difference of means converges to a standard normal distribution:\n$$\nZ_{n_1, n_2} = \\frac{\\bar{X}_{n_1} - \\bar{Y}_{n_2}}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}} \\xrightarrow{d} N(0, 1)\n$$\n\n**Step 2: Rewrite the statistic and analyze the denominator**\n\nWe can rewrite the statistic $T_{n_1, n_2}$ in terms of $Z_{n_1, n_2}$:\n$$\nT_{n_1, n_2} = \\frac{\\bar{X}_{n_1} - \\bar{Y}_{n_2}}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}} \\cdot \\frac{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}}{\\sqrt{S_p^2 \\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}} = Z_{n_1, n_2} \\cdot \\sqrt{\\frac{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}{S_p^2 \\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}}\n$$\nLet's define the term in the square root as $W_{n_1, n_2}^2$:\n$$\nW_{n_1, n_2}^2 = \\frac{S_p^2 \\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}\n$$\nAccording to Slutsky's theorem, if $W_{n_1, n_2}$ converges in probability to a constant $W$, then $T_{n_1, n_2} \\xrightarrow{d} N(0, 1) / W \\sim N(0, 1/W^2)$. Our goal is thus to find the probability limit of $W_{n_1, n_2}^2$.\n\n**Step 3: Find the probability limit of the pooled variance estimator $S_p^2$**\n\nThe pooled variance estimator is:\n$$\nS_p^2 = \\frac{n_1-1}{n_1+n_2-2} S_1^2 + \\frac{n_2-1}{n_1+n_2-2} S_2^2\n$$\nBy the Law of Large Numbers (LLN), the sample variances are consistent estimators of the population variances: $S_1^2 \\xrightarrow{p} \\sigma_1^2$ and $S_2^2 \\xrightarrow{p} \\sigma_2^2$.\n\nNow, let's analyze the coefficients as $n_1, n_2 \\to \\infty$. Let $N = n_1 + n_2$. We are given that $\\frac{n_1}{N} \\to \\lambda$, which implies $\\frac{n_2}{N} = \\frac{N-n_1}{N} = 1 - \\frac{n_1}{N} \\to 1-\\lambda$.\n$$\n\\lim_{N\\to\\infty} \\frac{n_1-1}{N-2} = \\lim_{N\\to\\infty} \\frac{n_1/N - 1/N}{1 - 2/N} = \\frac{\\lambda - 0}{1-0} = \\lambda\n$$\n$$\n\\lim_{N\\to\\infty} \\frac{n_2-1}{N-2} = \\lim_{N\\to\\infty} \\frac{n_2/N - 1/N}{1-2/N} = \\frac{1-\\lambda - 0}{1-0} = 1-\\lambda\n$$\nBy the Continuous Mapping Theorem (a consequence of Slutsky's theorem), the probability limit of $S_p^2$ is:\n$$\nS_p^2 \\xrightarrow{p} \\lambda \\sigma_1^2 + (1-\\lambda) \\sigma_2^2\n$$\n\n**Step 4: Find the limit of the ratio of variance sums**\n\nLet's analyze the ratio $\\frac{\\frac{1}{n_1} + \\frac{1}{n_2}}{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}$. We can multiply the numerator and denominator by $N = n_1+n_2$:\n$$\n\\frac{N(\\frac{1}{n_1} + \\frac{1}{n_2})}{N(\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2})} = \\frac{\\frac{N}{n_1} + \\frac{N}{n_2}}{\\frac{N}{n_1}\\sigma_1^2 + \\frac{N}{n_2}\\sigma_2^2}\n$$\nAs $N \\to \\infty$, we have $\\frac{N}{n_1} \\to \\frac{1}{\\lambda}$ and $\\frac{N}{n_2} \\to \\frac{1}{1-\\lambda}$. Therefore, the limit of the ratio is:\n$$\n\\lim_{N \\to \\infty} \\frac{\\frac{1}{n_1} + \\frac{1}{n_2}}{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}} = \\frac{\\frac{1}{\\lambda} + \\frac{1}{1-\\lambda}}{\\frac{\\sigma_1^2}{\\lambda} + \\frac{\\sigma_2^2}{1-\\lambda}} = \\frac{\\frac{1-\\lambda+\\lambda}{\\lambda(1-\\lambda)}}{\\frac{(1-\\lambda)\\sigma_1^2 + \\lambda\\sigma_2^2}{\\lambda(1-\\lambda)}} = \\frac{1}{(1-\\lambda)\\sigma_1^2 + \\lambda\\sigma_2^2}\n$$\n\n**Step 5: Combine results to find the limit of $W_{n_1, n_2}^2$**\n\nNow we can find the probability limit of $W_{n_1, n_2}^2$:\n$$\nW_{n_1, n_2}^2 = S_p^2 \\cdot \\frac{\\frac{1}{n_1} + \\frac{1}{n_2}}{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}\n$$\nUsing the limits we derived in steps 3 and 4:\n$$\nW^2 = \\lim_{n_1,n_2 \\to \\infty} W_{n_1, n_2}^2 \\xrightarrow{p} (\\lambda \\sigma_1^2 + (1-\\lambda) \\sigma_2^2) \\cdot \\frac{1}{(1-\\lambda)\\sigma_1^2 + \\lambda\\sigma_2^2}\n$$\n$$\nW^2 = \\frac{\\lambda \\sigma_1^2 + (1-\\lambda) \\sigma_2^2}{(1-\\lambda)\\sigma_1^2 + \\lambda\\sigma_2^2}\n$$\n\n**Step 6: Final application of Slutsky's Theorem**\n\nWe have $T_{n_1, n_2} = Z_{n_1, n_2} / W_{n_1, n_2}$. Since $Z_{n_1, n_2} \\xrightarrow{d} N(0, 1)$ and $W_{n_1, n_2} \\xrightarrow{p} W$, by Slutsky's theorem:\n$$\nT_{n_1, n_2} \\xrightarrow{d} \\frac{N(0, 1)}{W} \\sim N\\left(0, \\frac{1}{W^2}\\right)\n$$\nThe asymptotic variance $V$ is therefore $1/W^2$.\n$$\nV = \\frac{1}{W^2} = \\frac{(1-\\lambda)\\sigma_1^2 + \\lambda\\sigma_2^2}{\\lambda \\sigma_1^2 + (1-\\lambda) \\sigma_2^2}\n$$\nThis expression gives the variance of the limiting normal distribution for the incorrectly applied t-statistic.", "answer": "$$\n\\boxed{\\frac{(1-\\lambda)\\sigma_1^2 + \\lambda\\sigma_2^2}{\\lambda \\sigma_1^2 + (1-\\lambda) \\sigma_2^2}}\n$$", "id": "840045"}]}