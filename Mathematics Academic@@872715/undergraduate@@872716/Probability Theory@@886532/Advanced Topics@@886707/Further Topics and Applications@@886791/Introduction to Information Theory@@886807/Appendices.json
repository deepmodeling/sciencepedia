{"hands_on_practices": [{"introduction": "The journey into information theory begins with its most fundamental concept: Shannon entropy. Entropy provides a precise mathematical way to quantify the average uncertainty or \"surprise\" inherent in a random variable's outcomes. This first exercise [@problem_id:1367065] will guide you through a core skill: calculating the entropy of a system. More importantly, it demonstrates how a simple mathematical operation on a random source can change its underlying probability distribution, and consequently, its information content.", "problem": "A hardware-based true random number generator (TRNG) produces an integer, represented by the random variable $X$, which is uniformly distributed over the set $\\{1, 2, 3, 4, 5, 6, 7, 8\\}$. A post-processing algorithm takes this integer and computes a new value, represented by the random variable $Y$, using the operation $Y = X \\pmod 3$.\n\nCalculate the Shannon entropy of the random variable $Y$. Express your answer in bits, rounded to four significant figures.", "solution": "The random variable $X$ is uniformly distributed on $\\{1,2,3,4,5,6,7,8\\}$, so $P(X=x)=\\frac{1}{8}$ for each $x$. The post-processing defines $Y=X \\pmod 3$, so $Y\\in\\{0,1,2\\}$. Enumerating:\n- $Y=0$ occurs for $X\\in\\{3,6\\}$, hence $P(Y=0)=\\frac{2}{8}=\\frac{1}{4}$.\n- $Y=1$ occurs for $X\\in\\{1,4,7\\}$, hence $P(Y=1)=\\frac{3}{8}$.\n- $Y=2$ occurs for $X\\in\\{2,5,8\\}$, hence $P(Y=2)=\\frac{3}{8}$.\n\nThe Shannon entropy in bits is\n$$\nH(Y)=-\\sum_{y\\in\\{0,1,2\\}} P(Y=y)\\log_{2}\\big(P(Y=y)\\big).\n$$\nSubstituting the probabilities gives\n$$\nH(Y)=-\\frac{1}{4}\\log_{2}\\!\\left(\\frac{1}{4}\\right)-\\frac{3}{8}\\log_{2}\\!\\left(\\frac{3}{8}\\right)-\\frac{3}{8}\\log_{2}\\!\\left(\\frac{3}{8}\\right)\n=-\\frac{1}{4}\\log_{2}\\!\\left(\\frac{1}{4}\\right)-\\frac{3}{4}\\log_{2}\\!\\left(\\frac{3}{8}\\right).\n$$\nUsing $\\log_{2}\\!\\left(\\frac{1}{4}\\right)=\\log_{2}\\!\\left(2^{-2}\\right)=-2$ and $\\log_{2}\\!\\left(\\frac{3}{8}\\right)=\\log_{2}(3)-3$, we get\n$$\nH(Y)=-\\frac{1}{4}(-2)-\\frac{3}{4}\\big(\\log_{2}(3)-3\\big)\n=\\frac{11}{4}-\\frac{3}{4}\\log_{2}(3).\n$$\nNumerically, $\\log_{2}(3)\\approx 1.5849625$, so\n$$\nH(Y)\\approx 2.75-\\frac{3}{4}\\times 1.5849625 \\approx 1.561278125,\n$$\nwhich rounds to four significant figures as $1.561$ bits.", "answer": "$$\\boxed{1.561}$$", "id": "1367065"}, {"introduction": "Once we can measure the information content of a source, the next logical step is to represent that information efficiently. This is the central goal of data compression. This practice [@problem_id:1367067] introduces you to Huffman coding, a brilliant and widely used algorithm for creating optimal prefix-free codes. By working through this problem, you will not just calculate an average length, but you will engage with the constructive process of designing a code that minimizes redundancy based on the statistical properties of the source.", "problem": "In a simplified model for compressing genomic data, a source generates a stream of symbols from a five-symbol alphabet, $\\{\\text{A, C, G, T, U}\\}$. Each symbol represents a specific functional marker and appears with a known probability. The probabilities are as follows: $P(\\text{A}) = 0.4$, $P(\\text{C}) = 0.2$, $P(\\text{G}) = 0.2$, $P(\\text{T}) = 0.1$, and $P(\\text{U}) = 0.1$.\n\nTo transmit this data efficiently, a variable-length binary prefix code is to be designed. For such an encoding scheme, what is the minimum possible average codeword length, in bits per symbol?\n\nProvide your answer as a numerical value rounded to three significant figures.", "solution": "We are asked for the minimum possible average codeword length, in bits per symbol, for a binary variable-length prefix code given the source probabilities. The optimal such code is a Huffman code, whose expected length is minimal among all binary prefix codes.\n\nGiven probabilities: $P(\\text{A})=0.4$, $P(\\text{C})=0.2$, $P(\\text{G})=0.2$, $P(\\text{T})=0.1$, $P(\\text{U})=0.1$.\n\nApply the Huffman algorithm by repeatedly merging the two least-probable symbols/nodes and assigning an extra bit to all leaves within the merged node each time:\n- Merge $\\text{T}$ and $\\text{U}$: $0.1+0.1=0.2$. This gives both $\\text{T}$ and $\\text{U}$ one extra bit of length so far.\n- Now the multiset of weights is $\\{0.4,0.2,0.2,0.2\\}$. Merge two of the $0.2$ weights, say $\\text{C}$ and $\\text{G}$: $0.2+0.2=0.4$. This gives $\\text{C}$ and $\\text{G}$ one extra bit of length so far.\n- Now the multiset is $\\{0.4,0.4,0.2\\}$, where the $0.2$ corresponds to the $(\\text{T},\\text{U})$ node, one $0.4$ is $(\\text{C},\\text{G})$, and the other $0.4$ is $\\text{A}$. Merge $0.2$ and one $0.4$ (without loss of optimality), for instance the pair $(\\text{T},\\text{U})$ with $\\text{A}$: $0.2+0.4=0.6$. This adds one more bit to $\\text{T}$ and $\\text{U}$ and one bit to $\\text{A}$.\n- Finally merge $0.6$ and $0.4$: $0.6+0.4=1$. This adds one more bit to $(\\text{T},\\text{U},\\text{A})$ and one bit to $(\\text{C},\\text{G})$.\n\nTracking codeword lengths from these merges yields:\n- $\\text{A}$ receives $1$ bit from the third merge and $1$ bit from the final merge, so $l(\\text{A})=2$.\n- $\\text{C}$ and $\\text{G}$ each receive $1$ bit from the second merge and $1$ bit from the final merge, so $l(\\text{C})=l(\\text{G})=2$.\n- $\\text{T}$ and $\\text{U}$ each receive $1$ bit from the first merge, $1$ bit from the third merge, and $1$ bit from the final merge, so $l(\\text{T})=l(\\text{U})=3$.\n\nThe resulting expected codeword length is\n$$\nL=\\sum_{x} P(x)\\,l(x)\n=0.4\\cdot 2+0.2\\cdot 2+0.2\\cdot 2+0.1\\cdot 3+0.1\\cdot 3\n=1.6+0.6=2.2\\ \\text{bits per symbol}.\n$$\nAs a consistency check with Shannonâ€™s bound, the entropy is\n$$\nH=-\\sum_{x}P(x)\\log_{2}P(x)\n=0.4\\log_{2}\\!\\left(\\frac{1}{0.4}\\right)+0.2\\log_{2}\\!\\left(\\frac{1}{0.2}\\right)+0.2\\log_{2}\\!\\left(\\frac{1}{0.2}\\right)+0.1\\log_{2}\\!\\left(\\frac{1}{0.1}\\right)+0.1\\log_{2}\\!\\left(\\frac{1}{0.1}\\right)\\approx 2.122,\n$$\nand indeed $H \\leq L < H+1$ holds. Therefore, the minimum possible average codeword length for a binary prefix code with these probabilities is $2.20$ bits per symbol when rounded to three significant figures.", "answer": "$$\\boxed{2.20}$$", "id": "1367067"}, {"introduction": "Information is only useful if it can be reliably transmitted from a source to a destination. However, all real-world communication channels are subject to noise, which can corrupt the data. This final practice [@problem_id:1367053] shifts our focus to the transmission process, using the Binary Symmetric Channel (BSC) as a simple but powerful model for noise. You will analyze a system where information passes through two such channels in sequence, a common scenario in relay-based communications. Determining the overall error probability of the combined system is a crucial step in understanding and engineering robust communication networks.", "problem": "A deep-space probe is transmitting scientific data back to a central command on Earth. The communication is prone to errors and consists of two stages. First, the probe transmits a binary signal (a sequence of 0s and 1s) to a relay satellite orbiting Earth. Second, the relay satellite amplifies and re-transmits the received signal to the ground station at central command.\n\nEach of these two communication links (probe-to-satellite and satellite-to-ground) can be modeled as an independent and identical Binary Symmetric Channel (BSC). A BSC is a communication channel that transmits one binary digit at a time, where each bit has a certain probability of being \"flipped\" (i.e., a 0 becomes a 1, or a 1 becomes a 0). This flipping event is called a crossover. For each of the two links in this system, the crossover probability is given by $p$, where $0 < p < 1$.\n\nThe entire two-stage process, from the probe to the central command, can be viewed as a single, equivalent communication channel. This equivalent channel is also a BSC. Determine the crossover probability of this equivalent channel, expressed as a function of $p$.", "solution": "Let $X \\in \\{0, 1\\}$ be the bit transmitted by the deep-space probe, $Y \\in \\{0, 1\\}$ be the bit received by the relay satellite, and $Z \\in \\{0, 1\\}$ be the bit received by the central command.\n\nThe system consists of two independent Binary Symmetric Channels (BSCs) connected in series. The first channel maps $X$ to $Y$, and the second channel maps $Y$ to $Z$.\n\nFor the first BSC (probe-to-satellite), the crossover probability is $p$. This can be expressed using conditional probabilities:\n$P(Y=1|X=0) = p$ (a '0' is flipped to a '1')\n$P(Y=0|X=1) = p$ (a '1' is flipped to a '0')\nThe probability of correct transmission for the first channel is therefore $1-p$:\n$P(Y=0|X=0) = 1-p$\n$P(Y=1|X=1) = 1-p$\n\nThe second BSC (satellite-to-ground) is identical and independent, with input $Y$ and output $Z$. Its probabilities are:\n$P(Z=1|Y=0) = p$\n$P(Z=0|Y=1) = p$\nAnd for correct transmission:\n$P(Z=0|Y=0) = 1-p$\n$P(Z=1|Y=1) = 1-p$\n\nWe need to find the crossover probability of the equivalent channel, which we can denote as $p_{eq}$. By the definition of a BSC, this is the probability that the final output bit $Z$ is different from the initial input bit $X$. Due to the symmetry of the system, this probability is the same whether the input is 0 or 1. Let's calculate $p_{eq} = P(Z=1|X=0)$.\n\nTo find $P(Z=1|X=0)$, we can use the law of total probability by marginalizing over the intermediate variable $Y$. The bit $Y$ can be either 0 or 1.\n$$p_{eq} = P(Z=1|X=0) = \\sum_{y \\in \\{0,1\\}} P(Z=1, Y=y | X=0)$$\nUsing the chain rule for conditional probability, $P(A, B | C) = P(A | B, C)P(B | C)$, this becomes:\n$$p_{eq} = P(Z=1|Y=0, X=0)P(Y=0|X=0) + P(Z=1|Y=1, X=0)P(Y=1|X=0)$$\nSince the two channels are independent, the output of the second channel $Z$ depends only on its input $Y$, and not on the original signal $X$. This is a characteristic of a Markov chain $X \\to Y \\to Z$. Thus, we can simplify the conditional probabilities:\n$P(Z=1|Y=0, X=0) = P(Z=1|Y=0)$\n$P(Z=1|Y=1, X=0) = P(Z=1|Y=1)$\n\nSubstituting these back into the expression for $p_{eq}$:\n$$p_{eq} = P(Z=1|Y=0)P(Y=0|X=0) + P(Z=1|Y=1)P(Y=1|X=0)$$\nNow we substitute the known probabilities for each channel:\n- $P(Z=1|Y=0) = p$ (a crossover occurs in the second channel)\n- $P(Y=0|X=0) = 1-p$ (correct transmission in the first channel)\n- $P(Z=1|Y=1) = 1-p$ (correct transmission in the second channel)\n- $P(Y=1|X=0) = p$ (a crossover occurs in the first channel)\n\nPlugging these values into the equation:\n$$p_{eq} = (p)(1-p) + (1-p)(p)$$\n$$p_{eq} = p - p^{2} + p - p^{2}$$\n$$p_{eq} = 2p - 2p^{2}$$\nThis can be written in factored form as:\n$$p_{eq} = 2p(1-p)$$\nThis expression represents the total probability of a single effective crossover. This occurs if there is a crossover in the first stage but not the second, or in the second stage but not the first.", "answer": "$$\\boxed{2p(1-p)}$$", "id": "1367053"}]}