{"hands_on_practices": [{"introduction": "To begin our hands-on exploration, we'll start with a foundational exercise. This problem provides a direct scenario involving student study habits and quiz scores, complete with a joint probability distribution. Your task is to calculate the conditional entropy $H(X|Y)$, which measures the average uncertainty about a student's score ($X$) once you know how many hours they studied ($Y$). This practice [@problem_id:1368955] is crucial for mastering the step-by-step mechanics of calculating conditional entropy directly from a given data table.", "problem": "In an information theory course, a professor analyzes the relationship between the number of hours a student studies for a quiz and their resulting score. Let $Y$ be a discrete random variable representing the number of hours studied, with possible values $\\{0, 1, 2\\}$. Let $X$ be a discrete random variable representing the quiz score, with possible values $\\{1, 2, 3\\}$.\n\nThe joint probability distribution $p(x, y) = P(X=x, Y=y)$ for these two variables was determined from a large sample of students and is given in the table below:\n\n| $p(x, y)$ | $X=1$ | $X=2$ | $X=3$ |\n| :--- | :---: | :---: | :---: |\n| **$Y=0$** | $\\frac{4}{16}$ | $\\frac{4}{16}$ | $0$ |\n| **$Y=1$** | $\\frac{1}{16}$ | $\\frac{2}{16}$ | $\\frac{1}{16}$ |\n| **$Y=2$** | $\\frac{4}{16}$ | $0$ | $0$ |\n\nCalculate the conditional entropy $H(X|Y)$, which measures the remaining uncertainty in the quiz score $X$ given that the number of study hours $Y$ is known. The entropy is to be calculated using the base-2 logarithm.\n\nExpress your answer as an exact fraction.", "solution": "We are given the joint pmf $p(x,y)$ of discrete random variables $X \\in \\{1,2,3\\}$ and $Y \\in \\{0,1,2\\}$. The conditional entropy of $X$ given $Y$ (with base-2 logarithm) is\n$$\nH(X|Y) = \\sum_{y} p(y)\\,H(X|Y=y) = -\\sum_{y} p(y) \\sum_{x} p(x|y)\\,\\log_{2}\\big(p(x|y)\\big),\n$$\nwith the convention that $0 \\log_{2}(0) = 0$.\n\nFirst compute the marginal distribution of $Y$ using $p(y) = \\sum_{x} p(x,y)$:\n$$\np(0) = \\frac{4}{16} + \\frac{4}{16} + 0 = \\frac{8}{16} = \\frac{1}{2}, \\quad\np(1) = \\frac{1}{16} + \\frac{2}{16} + \\frac{1}{16} = \\frac{4}{16} = \\frac{1}{4}, \\quad\np(2) = \\frac{4}{16} + 0 + 0 = \\frac{4}{16} = \\frac{1}{4}.\n$$\n\nFor $y=0$, the conditional probabilities are\n$$\np(1|0) = \\frac{\\frac{4}{16}}{\\frac{8}{16}} = \\frac{1}{2}, \\quad\np(2|0) = \\frac{\\frac{4}{16}}{\\frac{8}{16}} = \\frac{1}{2}, \\quad\np(3|0) = \\frac{0}{\\frac{8}{16}} = 0.\n$$\nThus,\n$$\nH(X|Y=0) = -\\left[\\frac{1}{2}\\log_{2}\\!\\left(\\frac{1}{2}\\right) + \\frac{1}{2}\\log_{2}\\!\\left(\\frac{1}{2}\\right) + 0\\right]\n= -\\left[\\frac{1}{2}(-1) + \\frac{1}{2}(-1)\\right] = 1.\n$$\n\nFor $y=1$, the conditional probabilities are\n$$\np(1|1) = \\frac{\\frac{1}{16}}{\\frac{4}{16}} = \\frac{1}{4}, \\quad\np(2|1) = \\frac{\\frac{2}{16}}{\\frac{4}{16}} = \\frac{1}{2}, \\quad\np(3|1) = \\frac{\\frac{1}{16}}{\\frac{4}{16}} = \\frac{1}{4}.\n$$\nThus,\n$$\nH(X|Y=1) = -\\left[\\frac{1}{4}\\log_{2}\\!\\left(\\frac{1}{4}\\right) + \\frac{1}{2}\\log_{2}\\!\\left(\\frac{1}{2}\\right) + \\frac{1}{4}\\log_{2}\\!\\left(\\frac{1}{4}\\right)\\right]\n= -\\left[\\frac{1}{4}(-2) + \\frac{1}{2}(-1) + \\frac{1}{4}(-2)\\right] = \\frac{3}{2}.\n$$\n\nFor $y=2$, the conditional probabilities are\n$$\np(1|2) = \\frac{\\frac{4}{16}}{\\frac{4}{16}} = 1, \\quad p(2|2) = 0, \\quad p(3|2) = 0,\n$$\nso\n$$\nH(X|Y=2) = -\\left[1\\cdot \\log_{2}(1) + 0 + 0\\right] = 0.\n$$\n\nTherefore,\n$$\nH(X|Y) = p(0)\\,H(X|Y=0) + p(1)\\,H(X|Y=1) + p(2)\\,H(X|Y=2)\n= \\frac{1}{2}\\cdot 1 + \\frac{1}{4}\\cdot \\frac{3}{2} + \\frac{1}{4}\\cdot 0\n= \\frac{1}{2} + \\frac{3}{8} = \\frac{7}{8}.\n$$\nThis is an exact fraction in bits.", "answer": "$$\\boxed{\\frac{7}{8}}$$", "id": "1368955"}, {"introduction": "In many real-world applications, we aren't simply given a joint probability table. Instead, we must derive it from a more fundamental understanding of a system. This problem [@problem_id:1368969] challenges you to do just that, using the familiar example of a fair six-sided die. By defining two variables based on the properties of the die's outcome (even or prime), you will first construct the joint probability distribution and then calculate the joint entropy $H(X, Y)$, which quantifies the total uncertainty contained in this two-variable system.", "problem": "Consider a single roll of a fair, standard six-sided die, for which the set of possible outcomes is $\\{1, 2, 3, 4, 5, 6\\}$. Let $X$ be a binary random variable that takes the value 1 if the outcome is an even number, and 0 otherwise. Let $Y$ be another binary random variable that takes the value 1 if the outcome is a prime number, and 0 otherwise. For the purposes of this problem, the prime numbers in the set of outcomes are $\\{2, 3, 5\\}$. Calculate the joint entropy $H(X, Y)$ in bits. Express your final answer as a single real number rounded to three significant figures.", "solution": "Define the random variables on the sample space of a fair six-sided die: outcomes are $\\{1,2,3,4,5,6\\}$, each with probability $\\frac{1}{6}$. Let $X=1$ if the outcome is even and $0$ otherwise; thus $X=1$ for outcomes $\\{2,4,6\\}$ and $X=0$ for $\\{1,3,5\\}$. Let $Y=1$ if the outcome is prime and $0$ otherwise; with primes $\\{2,3,5\\}$, we have $Y=1$ for $\\{2,3,5\\}$ and $Y=0$ for $\\{1,4,6\\}$.\n\nList the joint events:\n- $(X,Y)=(1,1)$ occurs only for outcome $2$, so $p_{11}=\\frac{1}{6}$.\n- $(X,Y)=(1,0)$ occurs for outcomes $4,6$, so $p_{10}=\\frac{2}{6}=\\frac{1}{3}$.\n- $(X,Y)=(0,1)$ occurs for outcomes $3,5$, so $p_{01}=\\frac{2}{6}=\\frac{1}{3}$.\n- $(X,Y)=(0,0)$ occurs only for outcome $1$, so $p_{00}=\\frac{1}{6}$.\n\nThe joint entropy in bits is\n$$\nH(X,Y)=-\\sum_{x\\in\\{0,1\\}}\\sum_{y\\in\\{0,1\\}} p_{xy}\\,\\log_{2}(p_{xy})\n= -\\left[\\tfrac{1}{6}\\log_{2}\\tfrac{1}{6}+\\tfrac{1}{3}\\log_{2}\\tfrac{1}{3}+\\tfrac{1}{3}\\log_{2}\\tfrac{1}{3}+\\tfrac{1}{6}\\log_{2}\\tfrac{1}{6}\\right].\n$$\nCombine like terms:\n$$\nH(X,Y)=-\\left[\\tfrac{1}{3}\\log_{2}\\tfrac{1}{6}+\\tfrac{2}{3}\\log_{2}\\tfrac{1}{3}\\right]\n=\\tfrac{1}{3}\\log_{2}6+\\tfrac{2}{3}\\log_{2}3.\n$$\nUsing $\\log_{2}6=\\log_{2}(2\\cdot 3)=1+\\log_{2}3$, we get\n$$\nH(X,Y)=\\tfrac{1}{3}\\left(1+\\log_{2}3\\right)+\\tfrac{2}{3}\\log_{2}3=\\tfrac{1}{3}+\\log_{2}3.\n$$\nNumerically, $\\log_{2}3\\approx 1.5849625$, hence\n$$\nH(X,Y)\\approx 1.5849625+\\tfrac{1}{3}\\approx 1.9182958,\n$$\nwhich rounded to three significant figures is $1.92$ bits.", "answer": "$$\\boxed{1.92}$$", "id": "1368969"}, {"introduction": "This final practice moves from concrete data to a more abstract but powerful concept about information itself. Here, we generate two new random variables, $Y$ and $Z$, by applying mathematical operations (modulo) to an initial random variable $X$. Calculating the joint entropy $H(Y, Z)$ in this scenario [@problem_id:1368950] reveals a fascinating insight: under certain transformations, the total information is perfectly preserved. This exercise demonstrates that entropy is not just about a single variable, but also about how information is structured and related across multiple variables.", "problem": "Consider a discrete random variable $X$ that is chosen uniformly at random from the set of integers $\\{0, 1, 2, 3, 4, 5\\}$. Two other discrete random variables, $Y$ and $Z$, are derived from $X$ as follows:\n- $Y = X \\pmod 2$ (the remainder when $X$ is divided by 2)\n- $Z = X \\pmod 3$ (the remainder when $X$ is divided by 3)\n\nCalculate the joint entropy $H(Y, Z)$ of the random variables $Y$ and $Z$. Your answer should be a closed-form analytic expression using the natural logarithm ($\\ln$).", "solution": "The joint entropy $H(Y, Z)$ of two discrete random variables $Y$ and $Z$ is defined by the formula:\n$$H(Y, Z) = - \\sum_{y \\in \\mathcal{Y}} \\sum_{z \\in \\mathcal{Z}} p(y, z) \\ln(p(y, z))$$\nwhere $\\mathcal{Y}$ and $\\mathcal{Z}$ are the sets of possible values for $Y$ and $Z$ respectively, and $p(y, z)$ is the joint probability mass function $P(Y=y, Z=z)$.\n\nFirst, we need to determine the joint probability distribution of $Y$ and $Z$. The random variable $X$ is chosen uniformly from the set $\\{0, 1, 2, 3, 4, 5\\}$. This means that for any $x \\in \\{0, 1, 2, 3, 4, 5\\}$, the probability is $P(X=x) = \\frac{1}{6}$.\n\nThe possible values for $Y = X \\pmod 2$ are $\\{0, 1\\}$.\nThe possible values for $Z = X \\pmod 3$ are $\\{0, 1, 2\\}$.\n\nWe can find the pair of values $(Y, Z)$ for each possible value of $X$:\n- If $X=0$: $Y = 0 \\pmod 2 = 0$, $Z = 0 \\pmod 3 = 0$. So, $(Y, Z)=(0, 0)$.\n- If $X=1$: $Y = 1 \\pmod 2 = 1$, $Z = 1 \\pmod 3 = 1$. So, $(Y, Z)=(1, 1)$.\n- If $X=2$: $Y = 2 \\pmod 2 = 0$, $Z = 2 \\pmod 3 = 2$. So, $(Y, Z)=(0, 2)$.\n- If $X=3$: $Y = 3 \\pmod 2 = 1$, $Z = 3 \\pmod 3 = 0$. So, $(Y, Z)=(1, 0)$.\n- If $X=4$: $Y = 4 \\pmod 2 = 0$, $Z = 4 \\pmod 3 = 1$. So, $(Y, Z)=(0, 1)$.\n- If $X=5$: $Y = 5 \\pmod 2 = 1$, $Z = 5 \\pmod 3 = 2$. So, $(Y, Z)=(1, 2)$.\n\nEach of these six outcomes for the pair $(Y, Z)$ corresponds to exactly one outcome for $X$. Since each outcome for $X$ has a probability of $\\frac{1}{6}$, the probability of each of these six pairs $(y, z)$ is also $\\frac{1}{6}$.\nThe joint probability mass function $p(y, z) = P(Y=y, Z=z)$ is:\n- $p(0, 0) = P(X=0) = \\frac{1}{6}$\n- $p(0, 1) = P(X=4) = \\frac{1}{6}$\n- $p(0, 2) = P(X=2) = \\frac{1}{6}$\n- $p(1, 0) = P(X=3) = \\frac{1}{6}$\n- $p(1, 1) = P(X=1) = \\frac{1}{6}$\n- $p(1, 2) = P(X=5) = \\frac{1}{6}$\n\nFor any other pair $(y, z)$, the probability $p(y, z)$ is 0. The sum of non-zero probabilities is $6 \\times \\frac{1}{6} = 1$, as expected.\n\nNow we can calculate the joint entropy. Since there are 6 possible pairs of $(Y,Z)$ outcomes, each with the same probability $\\frac{1}{6}$, the sum for the entropy calculation will have 6 identical terms.\n$$H(Y, Z) = - \\sum_{(y,z) \\in \\{(0,0), ..., (1,2)\\}} p(y, z) \\ln(p(y, z))$$\n$$H(Y, Z) = - \\left[ \\left(\\frac{1}{6}\\ln\\frac{1}{6}\\right) + \\left(\\frac{1}{6}\\ln\\frac{1}{6}\\right) + \\left(\\frac{1}{6}\\ln\\frac{1}{6}\\right) + \\left(\\frac{1}{6}\\ln\\frac{1}{6}\\right) + \\left(\\frac{1}{6}\\ln\\frac{1}{6}\\right) + \\left(\\frac{1}{6}\\ln\\frac{1}{6}\\right) \\right]$$\n$$H(Y, Z) = - 6 \\times \\left( \\frac{1}{6} \\ln\\left(\\frac{1}{6}\\right) \\right)$$\n$$H(Y, Z) = - \\ln\\left(\\frac{1}{6}\\right)$$\nUsing the logarithm property $\\ln(1/a) = -\\ln(a)$:\n$$H(Y, Z) = - (-\\ln(6)) = \\ln(6)$$\nThis result makes sense, as the joint distribution of $(Y, Z)$ is a uniform distribution over 6 possible outcomes. The entropy of a uniform distribution over $k$ states is $\\ln(k)$.", "answer": "$$\\boxed{\\ln(6)}$$", "id": "1368950"}]}