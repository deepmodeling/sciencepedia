{"hands_on_practices": [{"introduction": "Mastering a new concept begins with a firm grasp of its fundamental definition. This first exercise provides a direct application of the Shannon entropy formula to a simple, discrete probability distribution. By calculating the entropy of a simplified weather model, you will practice the core mechanics of the entropy equation and begin to build an intuition for how it quantifies uncertainty. [@problem_id:1386596]", "problem": "A simplified meteorological model for a specific region classifies the weather on any given day into one of three distinct categories: 'Sunny', 'Cloudy', or 'Rainy'. Based on extensive historical data, the probability of a day being 'Sunny' is $\\frac{1}{2}$. The probabilities of a day being 'Cloudy' or 'Rainy' are equal to each other. A discrete random variable $X$ represents the weather category for a randomly chosen day.\n\nCalculate the Shannon entropy of this random variable $X$. Express your answer as a single number in units of bits, which corresponds to using the base-2 logarithm in the entropy formula.", "solution": "Let the outcomes be $S$ (Sunny), $C$ (Cloudy), and $R$ (Rainy). We are given $P(S)=\\frac{1}{2}$ and $P(C)=P(R)$. Using the total probability rule,\n$$\nP(S)+P(C)+P(R)=1,\n$$\nand substituting $P(S)=\\frac{1}{2}$ and $P(C)=P(R)$ gives\n$$\n\\frac{1}{2}+2P(C)=1 \\implies 2P(C)=\\frac{1}{2} \\implies P(C)=\\frac{1}{4}, \\quad P(R)=\\frac{1}{4}.\n$$\nThe Shannon entropy in bits is defined by\n$$\nH(X)=-\\sum_{x} P(x)\\log_{2}\\big(P(x)\\big).\n$$\nSubstituting the probabilities,\n$$\nH(X)=-\\left[\\frac{1}{2}\\log_{2}\\left(\\frac{1}{2}\\right)+\\frac{1}{4}\\log_{2}\\left(\\frac{1}{4}\\right)+\\frac{1}{4}\\log_{2}\\left(\\frac{1}{4}\\right)\\right].\n$$\nUsing $\\log_{2}\\left(\\frac{1}{2}\\right)=-1$ and $\\log_{2}\\left(\\frac{1}{4}\\right)=-2$, we get\n$$\nH(X)=-\\left[\\frac{1}{2}(-1)+\\frac{1}{4}(-2)+\\frac{1}{4}(-2)\\right]\n=-\\left[-\\frac{1}{2}-\\frac{1}{2}-\\frac{1}{2}\\right]\n=\\frac{3}{2}.\n$$\nThus, the entropy is $\\frac{3}{2}$ bits.", "answer": "$$\\boxed{\\frac{3}{2}}$$", "id": "1386596"}, {"introduction": "Real-world systems often follow well-known statistical patterns. This practice problem bridges the concept of entropy with one of the most fundamental discrete distributions in probability: the binomial distribution. You will first need to model the scenario to determine the probability of each outcome, and then compute the associated entropy, thereby strengthening your ability to apply information theory to common probabilistic models. [@problem_id:1386600]", "problem": "A manufacturing process produces electronic components. A quality control inspection reveals that each component has an independent probability $p = 1/4$ of being defective. The components are packaged in small boxes, each containing $n=3$ components. Let the random variable $X$ represent the number of defective components in a randomly selected box. Calculate the Shannon entropy of the random variable $X$. Express your answer in nats, rounded to four significant figures.", "solution": "The Shannon entropy $H(X)$ of a discrete random variable $X$ with a set of possible outcomes $\\{x_1, x_2, \\ldots, x_m\\}$ and probability mass function $P(X=x_i) = p_i$ is defined as:\n$$ H(X) = - \\sum_{i=1}^{m} p_i \\ln(p_i) $$\nThe use of the natural logarithm ($\\ln$) means the entropy will be measured in units of \"nats\".\n\nThe random variable $X$ represents the number of defective components in a box of $n=3$ components, where each has an independent probability of being defective $p=1/4$. This scenario is described by a binomial distribution, so $X \\sim B(n=3, p=1/4)$.\n\nThe possible values for $X$ (the number of defective components) are $k \\in \\{0, 1, 2, 3\\}$. The probability mass function (PMF) for a binomial distribution is given by:\n$$ P(X=k) = \\binom{n}{k} p^k (1-p)^{n-k} $$\nHere, $n=3$, $p=1/4$, and $1-p=3/4$. We calculate the probabilities for each value of $k$:\n\nFor $k=0$:\n$$ P(X=0) = \\binom{3}{0} \\left(\\frac{1}{4}\\right)^0 \\left(\\frac{3}{4}\\right)^3 = 1 \\cdot 1 \\cdot \\frac{27}{64} = \\frac{27}{64} $$\n\nFor $k=1$:\n$$ P(X=1) = \\binom{3}{1} \\left(\\frac{1}{4}\\right)^1 \\left(\\frac{3}{4}\\right)^2 = 3 \\cdot \\frac{1}{4} \\cdot \\frac{9}{16} = \\frac{27}{64} $$\n\nFor $k=2$:\n$$ P(X=2) = \\binom{3}{2} \\left(\\frac{1}{4}\\right)^2 \\left(\\frac{3}{4}\\right)^1 = 3 \\cdot \\frac{1}{16} \\cdot \\frac{3}{4} = \\frac{9}{64} $$\n\nFor $k=3$:\n$$ P(X=3) = \\binom{3}{3} \\left(\\frac{1}{4}\\right)^3 \\left(\\frac{3}{4}\\right)^0 = 1 \\cdot \\frac{1}{64} \\cdot 1 = \\frac{1}{64} $$\n\nNow, we can compute the Shannon entropy by summing the contributions from each possible outcome:\n$$ H(X) = - \\sum_{k=0}^{3} P(X=k) \\ln(P(X=k)) $$\n$$ H(X) = - \\left[ P(0)\\ln(P(0)) + P(1)\\ln(P(1)) + P(2)\\ln(P(2)) + P(3)\\ln(P(3)) \\right] $$\nSubstituting the calculated probabilities:\n$$ H(X) = - \\left[ \\frac{27}{64} \\ln\\left(\\frac{27}{64}\\right) + \\frac{27}{64} \\ln\\left(\\frac{27}{64}\\right) + \\frac{9}{64} \\ln\\left(\\frac{9}{64}\\right) + \\frac{1}{64} \\ln\\left(\\frac{1}{64}\\right) \\right] $$\nWe can evaluate this expression numerically:\nThe term $p_i \\ln(p_i)$ for $p_i=0$ is taken to be $0$, but no probabilities are zero in this case.\n\nTerm 1 (for $k=0$):\n$$ - \\frac{27}{64} \\ln\\left(\\frac{27}{64}\\right) \\approx - (0.421875) \\cdot (-0.863046) \\approx 0.364104 $$\n\nTerm 2 (for $k=1$):\n$$ - \\frac{27}{64} \\ln\\left(\\frac{27}{64}\\right) \\approx - (0.421875) \\cdot (-0.863046) \\approx 0.364104 $$\n\nTerm 3 (for $k=2$):\n$$ - \\frac{9}{64} \\ln\\left(\\frac{9}{64}\\right) \\approx - (0.140625) \\cdot (-1.961601) \\approx 0.275869 $$\n\nTerm 4 (for $k=3$):\n$$ - \\frac{1}{64} \\ln\\left(\\frac{1}{64}\\right) \\approx - (0.015625) \\cdot (-4.158883) \\approx 0.064983 $$\n\nSumming these values gives the total entropy:\n$$ H(X) \\approx 0.364104 + 0.364104 + 0.275869 + 0.064983 = 1.06906 $$\nRounding the result to four significant figures, we get $1.069$.", "answer": "$$\\boxed{1.069}$$", "id": "1386600"}, {"introduction": "Beyond mere calculation, a deep understanding of entropy involves comparing the uncertainty of different systems. This exercise challenges you to compute and compare the entropies of two distinct random processes: one uniform and one non-uniform. This comparison is not just an arithmetic task; it is designed to reveal a fundamental principle of information theory about which distributions maximize uncertainty. [@problem_id:1386619]", "problem": "In probability theory, the uncertainty associated with a discrete random variable is quantified by its Shannon entropy. For a random variable $Z$ that can take values $z_1, z_2, \\ldots, z_n$ with corresponding probabilities $p(z_1), p(z_2), \\ldots, p(z_n)$, the Shannon entropy $H(Z)$ in units of bits is defined by the formula:\n$$H(Z) = - \\sum_{i=1}^{n} p(z_i) \\log_2(p(z_i))$$\nwhere the logarithm is taken to base 2.\n\nConsider two different random processes:\n1.  Let the random variable $X$ represent the outcome of a single roll of a fair 8-sided die, whose faces are numbered from 1 to 8.\n2.  Let the random variable $Y$ represent the total number of heads observed after flipping three separate fair coins.\n\nCalculate the positive difference $\\Delta H = |H(X) - H(Y)|$ between the Shannon entropies of these two random variables. Express your answer in bits, rounded to three significant figures.", "solution": "By definition, for a discrete random variable with probabilities $\\{p_{i}\\}$, the Shannon entropy in bits is $H=-\\sum_{i} p_{i}\\log_{2}(p_{i})$.\n\nFor $X$ (a fair 8-sided die), each outcome has probability $p_{i}=\\frac{1}{8}$ for $i=1,\\ldots,8$. Thus,\n$$\nH(X)=-\\sum_{i=1}^{8}\\frac{1}{8}\\log_{2}\\!\\left(\\frac{1}{8}\\right)\n=-\\,\\log_{2}\\!\\left(\\frac{1}{8}\\right)\n=\\log_{2}(8)\n=3.\n$$\n\nFor $Y$ (the number of heads in three fair coin flips), $Y$ takes values $k\\in\\{0,1,2,3\\}$ with binomial probabilities\n$$\n\\mathbb{P}(Y=k)=\\frac{\\binom{3}{k}}{2^{3}}, \\quad k=0,1,2,3,\n$$\nso the distribution is $\\left\\{\\frac{1}{8},\\frac{3}{8},\\frac{3}{8},\\frac{1}{8}\\right\\}$. Hence,\n$$\nH(Y)=-\\left[2\\cdot\\frac{1}{8}\\log_{2}\\!\\left(\\frac{1}{8}\\right)+2\\cdot\\frac{3}{8}\\log_{2}\\!\\left(\\frac{3}{8}\\right)\\right].\n$$\nUsing $\\log_{2}\\!\\left(\\frac{1}{8}\\right)=-3$ and $\\log_{2}\\!\\left(\\frac{3}{8}\\right)=\\log_{2}(3)-3$, we obtain\n$$\nH(Y)=-2\\cdot\\frac{1}{8}\\cdot(-3)-2\\cdot\\frac{3}{8}\\cdot\\left(\\log_{2}(3)-3\\right)\n=\\frac{3}{4}-\\frac{3}{4}\\log_{2}(3)+\\frac{9}{4}\n=3-\\frac{3}{4}\\log_{2}(3).\n$$\n\nTherefore, the positive difference between the entropies is\n$$\n\\Delta H=\\left|H(X)-H(Y)\\right|\n=\\left|3-\\left(3-\\frac{3}{4}\\log_{2}(3)\\right)\\right|\n=\\frac{3}{4}\\log_{2}(3).\n$$\nEvaluating numerically and rounding to three significant figures:\n$$ \\Delta H=\\frac{3}{4}\\log_{2}(3)\\approx 1.188721875 \\Rightarrow 1.19 \\text{ bits} $$", "answer": "$$\\boxed{1.19}$$", "id": "1386619"}]}