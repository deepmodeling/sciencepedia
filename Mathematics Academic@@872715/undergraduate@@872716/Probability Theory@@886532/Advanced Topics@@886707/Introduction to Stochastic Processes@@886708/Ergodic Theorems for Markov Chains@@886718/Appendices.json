{"hands_on_practices": [{"introduction": "The ergodic theorem's most direct application is in determining the long-term behavior of a system. For any ergodic Markov chain, the proportion of time spent in a particular state converges to a specific, unique probability, known as the stationary distribution. This exercise [@problem_id:1360494] provides foundational practice in finding this long-run proportion by setting up and solving the system of linear equations $\\pi = \\pi P$ for a simple, discrete-state system.", "problem": "A small automaton moves a token among three squares on a track, labeled 1, 2, and 3 from left to right. The position of the token is updated at discrete time steps according to a set of fixed probabilistic rules. The process can be modeled as a Markov chain where the state is the label of the square the token occupies.\n\nThe rules for the token's movement are as follows:\n- If the token is on square 1, it will always move to square 2 in the next step.\n- If the token is on square 2, it will move to square 1 with a probability of $1/2$ or to square 3 with a probability of $1/2$ in the next step.\n- If the token is on square 3, it will move to square 2 with a probability of $1/3$ or remain on square 3 with a probability of $2/3$ in the next step.\n\nAssuming the automaton runs for a very long time, what is the long-run proportion of time that the token is on square 2? Express your answer as a fraction in simplest form.", "solution": "Model the token’s location as a finite Markov chain with states labeled $1,2,3$. The transition matrix $P$ (rows indexed by current state, columns by next state) is\n$$\nP=\\begin{pmatrix}\n0 & 1 & 0 \\\\\n\\frac{1}{2} & 0 & \\frac{1}{2} \\\\\n0 & \\frac{1}{3} & \\frac{2}{3}\n\\end{pmatrix}\n$$.\nThe chain is irreducible (each state can reach the others) and aperiodic (state $3$ has a self-loop), so it has a unique stationary distribution $\\pi=(\\pi_{1},\\pi_{2},\\pi_{3})$ with $\\pi=\\pi P$ and $\\pi_{1}+\\pi_{2}+\\pi_{3}=1$. Writing $\\pi=\\pi P$ componentwise gives\n$$\n\\pi_{1}=\\frac{1}{2}\\pi_{2},\\qquad\n\\pi_{2}=\\pi_{1}+\\frac{1}{3}\\pi_{3},\\qquad\n\\pi_{3}=\\frac{1}{2}\\pi_{2}+\\frac{2}{3}\\pi_{3}.\n$$\nFrom the first equation, $\\pi_{1}=\\frac{1}{2}\\pi_{2}$. From the third equation,\n$$\n\\pi_{3}-\\frac{2}{3}\\pi_{3}=\\frac{1}{2}\\pi_{2}\\;\\;\\Longrightarrow\\;\\;\\frac{1}{3}\\pi_{3}=\\frac{1}{2}\\pi_{2}\\;\\;\\Longrightarrow\\;\\;\\pi_{3}=\\frac{3}{2}\\pi_{2}.\n$$\nImpose normalization:\n$$\n\\pi_{1}+\\pi_{2}+\\pi_{3}=\\frac{1}{2}\\pi_{2}+\\pi_{2}+\\frac{3}{2}\\pi_{2}=3\\pi_{2}=1,\n$$\nso $\\pi_{2}=\\frac{1}{3}$. This is the long-run proportion of time the token is on square $2$.", "answer": "$$\\boxed{\\frac{1}{3}}$$", "id": "1360494"}, {"introduction": "Beyond telling us where a system spends its time, the stationary distribution also reveals how long it takes, on average, to return to a starting point. This problem [@problem_id:1360475] explores the elegant connection between the stationary probability of a state, $\\pi_{s}$, and its mean recurrence time, which is simply $1/\\pi_{s}$. You will apply this powerful result to a card-shuffling model, discovering a surprisingly neat solution by analyzing the structure of the transition matrix.", "problem": "A simplified model for a card shuffling process is studied using a small deck of four distinct cards, which are initially in a sorted order. A single \"shuffle\" operation consists of taking the card currently at the top of the deck and re-inserting it into one of the four possible positions (i.e., back at the top, between the first and second cards, between the second and third cards, or at the bottom of the deck). Each of these four positions is chosen with equal probability.\n\nThe process is repeated, with each shuffle being independent of the previous ones. We are interested in how long it takes for the deck to return to its initial sorted order.\n\nCalculate the expected number of shuffles required for the deck to return to its initial sorted order for the first time. The initial state is considered step 0, so the first possible return is at step 1. Provide your answer as an exact integer.", "solution": "Let the state space be the set of all permutations of the four distinct cards; hence there are $4!=24$ states.\n\nOne shuffle is defined as follows: from a state $s=(x_{1},x_{2},x_{3},x_{4})$, choose $j\\in\\{1,2,3,4\\}$ uniformly and move $x_{1}$ into position $j$ among the four positions. Denote the resulting state by $T_{j}(s)$. The transition probabilities are\n$$\nP(s,t)=\\begin{cases}\n\\frac{1}{4}, & \\text{if } t=T_{j}(s) \\text{ for some } j\\in\\{1,2,3,4\\},\\\\\n0, & \\text{otherwise}.\n\\end{cases}\n$$\n\nWe show the transition matrix is doubly stochastic. Fix any target state $t=(t_{1},t_{2},t_{3},t_{4})$. For each $j\\in\\{1,2,3,4\\}$, define a unique predecessor\n$$\ns_{j}(t)=(t_{j},\\,t_{1},\\,\\ldots,\\,t_{j-1},\\,t_{j+1},\\,\\ldots,\\,t_{4}).\n$$\nThen $T_{j}(s_{j}(t))=t$. These $s_{j}(t)$ are distinct for distinct $j$ because the top card differs. Therefore the total incoming probability to $t$ is\n$$\n\\sum_{s}P(s,t)=\\sum_{j=1}^{4}\\frac{1}{4}=1,\n$$\nso every column sum equals $1$. Since every row sum is $1$ by construction, the transition matrix is doubly stochastic.\n\nIt follows that the uniform distribution $\\mu$ on the $24$ states, given by $\\mu(s)=\\frac{1}{24}$ for all states $s$, is stationary.\n\nThe chain is aperiodic because $P(s,s)=\\frac{1}{4}>0$ (choosing $j=1$ leaves the state unchanged). It is also irreducible (from any permutation, repeated top insertions can realize any permutation). Hence the chain is positive recurrent and ergodic.\n\nBy the standard recurrence-time identity (Kac’s lemma for Markov chains), the expected time to return to a given state $s$ (the expected first return time after time $0$), starting from $s$, equals the reciprocal of its stationary probability:\n$$\n\\mathbb{E}_{s}[T_{s}^{+}]=\\frac{1}{\\mu(s)}.\n$$\nSince $\\mu(s)=\\frac{1}{24}$ for every state, we obtain\n$$\n\\mathbb{E}_{s}[T_{s}^{+}]=24.\n$$\n\nTherefore, the expected number of shuffles required for the deck to return to its initial sorted order for the first time is $24$.", "answer": "$$\\boxed{24}$$", "id": "1360475"}, {"introduction": "Ergodic theorems are not limited to calculating the frequency of states; they allow us to find the long-run average of any function of the state, a concept with vast applications in physics and engineering. This advanced problem [@problem_id:1360530] challenges you to apply this principle to a random walk on an infinite set of states. You will determine the long-run average of the particle's squared position by first finding the stationary distribution and then calculating the expectation of the function $f(X) = X^2$ with respect to that distribution.", "problem": "A particle performs a random walk on the set of integers $\\mathbb{Z}$. The state of the system at time $n$ is given by the particle's position, $X_n \\in \\mathbb{Z}$. The movement of the particle is governed by a state-dependent bias that always pulls it toward the origin.\n\nThe transition probabilities are defined as follows:\n- If the particle is at the origin ($i=0$), it will jump to $i+1$ with probability $1/2$ or to $i-1$ with probability $1/2$.\n- If the particle is at any non-zero integer state $i \\neq 0$, the probability of it jumping one step closer to the origin is $\\frac{1+c}{2}$, and the probability of it jumping one step further from the origin is $\\frac{1-c}{2}$. The constant $c$ is a parameter satisfying $0 < c < 1$.\n\nThis Markov chain is ergodic, possessing a unique stationary distribution $\\pi_i = \\lim_{n \\to \\infty} P(X_n = i)$. According to the ergodic theorem, the long-run time average of any function of the particle's position converges to the expectation of that function with respect to the stationary distribution.\n\nDetermine the long-run average of the squared position of the particle, $\\lim_{N \\to \\infty} \\frac{1}{N} \\sum_{n=1}^{N} X_n^2$. Express your answer as a closed-form analytic expression in terms of the parameter $c$.", "solution": "By the ergodic theorem, the long-run time average equals the stationary expectation, so\n$$\n\\lim_{N \\to \\infty} \\frac{1}{N} \\sum_{n=1}^{N} X_{n}^{2}=\\sum_{i \\in \\mathbb{Z}} i^{2} \\pi_{i}.\n$$\nWe find the stationary distribution $\\pi_{i}$ via detailed balance on the nearest-neighbor edges. The chain is symmetric, so $\\pi_{-i}=\\pi_{i}$. For $i \\geq 0$, the edge between $i$ and $i+1$ satisfies\n- $P_{0,1}=\\frac{1}{2}$ and $P_{1,0}=\\frac{1+c}{2}$,\n- for $i \\geq 1$, $P_{i,i+1}=\\frac{1-c}{2}$ and $P_{i+1,i}=\\frac{1+c}{2}$.\n\nImposing $\\pi_{i} P_{i,i+1}=\\pi_{i+1} P_{i+1,i}$ gives the ratios\n$$\n\\frac{\\pi_{1}}{\\pi_{0}}=\\frac{P_{0,1}}{P_{1,0}}=\\frac{1}{1+c}, \\qquad \\frac{\\pi_{i+1}}{\\pi_{i}}=\\frac{P_{i,i+1}}{P_{i+1,i}}=\\frac{1-c}{1+c} \\quad (i \\geq 1).\n$$\nDefine $q=\\frac{1-c}{1+c}$. Then for $i \\geq 1$,\n$$\n\\pi_{i}=\\pi_{0}\\,\\frac{1}{1+c}\\,q^{\\,i-1}, \\qquad \\pi_{-i}=\\pi_{i}.\n$$\nNormalize to find $\\pi_{0}$. Using $\\sum_{i \\in \\mathbb{Z}} \\pi_{i}=1$,\n$$\n1=\\pi_{0}+2 \\sum_{i=1}^{\\infty} \\pi_{i}=\\pi_{0}+2\\,\\pi_{0}\\,\\frac{1}{1+c}\\sum_{n=0}^{\\infty} q^{\\,n}.\n$$\nSince $|q|<1$, $\\sum_{n=0}^{\\infty} q^{\\,n}=\\frac{1}{1-q}$, and\n$$\n1=\\pi_{0}+2\\,\\pi_{0}\\,\\frac{1}{1+c}\\,\\frac{1}{1-q}.\n$$\nNow $1-q=1-\\frac{1-c}{1+c}=\\frac{2c}{1+c}$, hence\n$$\n1=\\pi_{0}+2\\,\\pi_{0}\\,\\frac{1}{1+c}\\,\\frac{1+c}{2c}=\\pi_{0}+\\frac{\\pi_{0}}{c} \\quad \\Rightarrow \\quad \\pi_{0}=\\frac{c}{1+c}.\n$$\nTherefore, for $i \\geq 1$,\n$$\n\\pi_{i}=\\frac{c}{(1+c)^{2}}\\,q^{\\,i-1}, \\qquad \\pi_{-i}=\\pi_{i}, \\qquad q=\\frac{1-c}{1+c}.\n$$\nCompute the stationary second moment:\n$$\n\\sum_{i \\in \\mathbb{Z}} i^{2} \\pi_{i}=2 \\sum_{i=1}^{\\infty} i^{2} \\pi_{i}=2 \\frac{c}{(1+c)^{2}} \\sum_{i=1}^{\\infty} i^{2} q^{\\,i-1}.\n$$\nTo evaluate $\\sum_{i=1}^{\\infty} i^{2} q^{\\,i-1}$, start from $S_{0}(r)=\\sum_{i=0}^{\\infty} r^{i}=\\frac{1}{1-r}$ for $|r|<1$. Differentiate to get\n$$\n\\sum_{i=1}^{\\infty} i r^{\\,i-1}=\\frac{1}{(1-r)^{2}}, \\quad \\sum_{i=1}^{\\infty} i r^{\\,i}=\\frac{r}{(1-r)^{2}}.\n$$\nDifferentiate $\\sum_{i=1}^{\\infty} i r^{\\,i}=\\frac{r}{(1-r)^{2}}$ to obtain\n$$\n\\sum_{i=1}^{\\infty} i^{2} r^{\\,i-1}=\\frac{1+r}{(1-r)^{3}}.\n$$\nThus\n$$\n\\sum_{i=1}^{\\infty} i^{2} q^{\\,i-1}=\\frac{1+q}{(1-q)^{3}}.\n$$\nSubstitute $q=\\frac{1-c}{1+c}$:\n$$\n1+q=\\frac{2}{1+c}, \\qquad 1-q=\\frac{2c}{1+c}, \\qquad \\frac{1+q}{(1-q)^{3}}=\\frac{\\frac{2}{1+c}}{\\left(\\frac{2c}{1+c}\\right)^{3}}=\\frac{(1+c)^{2}}{4 c^{3}}.\n$$\nTherefore,\n$$\n\\sum_{i \\in \\mathbb{Z}} i^{2} \\pi_{i}=2 \\frac{c}{(1+c)^{2}} \\cdot \\frac{(1+c)^{2}}{4 c^{3}}=\\frac{1}{2 c^{2}}.\n$$\nBy ergodicity, the long-run time average equals this expectation, so\n$$\n\\lim_{N \\to \\infty} \\frac{1}{N} \\sum_{n=1}^{N} X_{n}^{2}=\\frac{1}{2 c^{2}}.\n$$", "answer": "$$\\boxed{\\frac{1}{2 c^{2}}}$$", "id": "1360530"}]}