{"hands_on_practices": [{"introduction": "Our first practice dive into Bayesian estimation tackles a common scenario: determining an unknown probability. When a process has two outcomes (like a coin flip or a chip being functional or defective), the parameter of interest is the probability of success, $p$. This exercise [@problem_id:1345485] demonstrates how to use the Beta-Bernoulli model to update your belief about $p$ after observing new evidence, forming the cornerstone of Bayesian analysis for proportions.", "problem": "A quality control engineer is evaluating a new manufacturing process for producing semiconductor chips. The true, unknown probability that a newly manufactured chip is functional is denoted by $p$. Based on initial theoretical models, the engineer assumes that any value of $p$ in the interval $[0, 1]$ is equally likely. This prior belief is formally described by a Beta distribution with parameters $\\alpha=1$ and $\\beta=1$, i.e., $p \\sim \\text{Beta}(1, 1)$.\n\nThe engineer then tests a small batch of 5 chips produced by this new process and finds that 4 of them are functional.\n\nUsing this new data, calculate the updated Bayesian estimate for the probability $p$. Specifically, find the posterior mean of $p$. Express your answer as a single fraction in simplest form.", "solution": "Let the probability of a functional chip be $p$. The prior is $p \\sim \\text{Beta}(\\alpha, \\beta)$ with $\\alpha=1$ and $\\beta=1$. The density of a Beta$(\\alpha,\\beta)$ prior is proportional to $p^{\\alpha-1}(1-p)^{\\beta-1}$ on $[0,1]$.\n\nFrom testing $n=5$ chips with $x=4$ functional, the binomial likelihood is\n$$\nL(p \\mid x) = \\binom{n}{x} p^{x} (1-p)^{n-x},\n$$\nwhich, as a function of $p$, is proportional to $p^{x}(1-p)^{n-x}$.\n\nBy Bayes’ rule with the Beta-Binomial conjugacy,\n$$\nf(p \\mid x) \\propto L(p \\mid x)\\, f(p) \\propto p^{x+\\alpha-1}(1-p)^{n-x+\\beta-1},\n$$\nso the posterior is\n$$\np \\mid x \\sim \\text{Beta}(\\alpha+x,\\ \\beta+n-x).\n$$\nSubstituting $\\alpha=1$, $\\beta=1$, $x=4$, and $n=5$ gives\n$$\np \\mid x \\sim \\text{Beta}(1+4,\\ 1+5-4) = \\text{Beta}(5, 2).\n$$\nThe posterior mean of a $\\text{Beta}(a,b)$ distribution is $\\frac{a}{a+b}$, hence\n$$\n\\mathbb{E}[p \\mid x] = \\frac{5}{5+2} = \\frac{5}{7}.\n$$\nTherefore, the updated Bayesian estimate (posterior mean) is $\\frac{5}{7}$.", "answer": "$$\\boxed{\\frac{5}{7}}$$", "id": "1345485"}, {"introduction": "Moving from binary outcomes to continuous measurements, we now explore how to estimate an unknown mean, $\\mu$. In many scientific contexts, our belief about a quantity and the measurements we take are well-approximated by normal distributions. This practice [@problem_id:1345529] asks you to derive the general formula for the updated posterior mean, revealing the beautiful intuition that it is a weighted average of your prior belief and the observed data, balanced by their respective precisions.", "problem": "A data scientist at a fintech company is evaluating a new automated High-Frequency Trading (HFT) algorithm. The weekly return of the algorithm, expressed as a percentage, is modeled as a random variable drawn from a normal distribution with an unknown true mean $\\mu$ and a known variance $\\sigma^2$.\n\nBased on simulations and analysis of similar strategies, the scientist's prior belief about the mean return $\\mu$ is quantified by a normal distribution with a mean of $\\mu_0$ and a variance of $\\sigma_0^2$.\n\nTo test the algorithm in a live market, it is run for a period of $n$ weeks. The average weekly return observed during this test period is $\\bar{x}$. Your task is to update the scientist's belief about the mean return $\\mu$ in light of this new data.\n\nDetermine the updated expected value of the mean weekly return, $\\mu$. Express your answer as a symbolic expression in terms of the prior mean $\\mu_0$, the prior variance $\\sigma_0^2$, the number of weeks $n$, the known variance of weekly returns $\\sigma^2$, and the observed average weekly return $\\bar{x}$.", "solution": "We model weekly returns as independent observations $x_{1},\\dots,x_{n}$ with $x_{i}\\,|\\,\\mu \\sim \\mathcal{N}(\\mu,\\sigma^{2})$. The prior for the mean is $\\mu \\sim \\mathcal{N}(\\mu_{0},\\sigma_{0}^{2})$. By Bayes' theorem and conjugacy of the normal likelihood with a normal prior, the posterior for $\\mu$ is normal. To derive its mean, write the likelihood in terms of the sufficient statistic $\\bar{x}$:\n$$\np(x_{1:n}\\,|\\,\\mu)\\propto \\exp\\!\\left(-\\frac{1}{2\\sigma^{2}}\\sum_{i=1}^{n}(x_{i}-\\mu)^{2}\\right)\n= \\exp\\!\\left(-\\frac{n}{2\\sigma^{2}}(\\bar{x}-\\mu)^{2}\\right)\\times C,\n$$\nwhere $C$ does not depend on $\\mu$. The prior density is\n$$\np(\\mu)\\propto \\exp\\!\\left(-\\frac{1}{2\\sigma_{0}^{2}}(\\mu-\\mu_{0})^{2}\\right).\n$$\nHence the unnormalized posterior kernel in $\\mu$ is\n$$\np(\\mu\\,|\\,x_{1:n}) \\propto \\exp\\!\\left[-\\frac{1}{2}\\left(\\frac{n}{\\sigma^{2}}(\\mu-\\bar{x})^{2}+\\frac{1}{\\sigma_{0}^{2}}(\\mu-\\mu_{0})^{2}\\right)\\right].\n$$\nExpanding and collecting terms in $\\mu$ gives\n$$\n-\\frac{1}{2}\\left[\\left(\\frac{n}{\\sigma^{2}}+\\frac{1}{\\sigma_{0}^{2}}\\right)\\mu^{2}\n-2\\left(\\frac{n\\bar{x}}{\\sigma^{2}}+\\frac{\\mu_{0}}{\\sigma_{0}^{2}}\\right)\\mu + \\text{const}\\right].\n$$\nCompleting the square, this equals\n$$\n-\\frac{1}{2}\\left(\\frac{n}{\\sigma^{2}}+\\frac{1}{\\sigma_{0}^{2}}\\right)\\left(\\mu-\\frac{\\frac{n\\bar{x}}{\\sigma^{2}}+\\frac{\\mu_{0}}{\\sigma_{0}^{2}}}{\\frac{n}{\\sigma^{2}}+\\frac{1}{\\sigma_{0}^{2}}}\\right)^{2}+\\text{const},\n$$\nso the posterior is normal with mean\n$$\n\\mathbb{E}[\\mu\\,|\\,x_{1:n}] \\;=\\; \\frac{\\frac{\\mu_{0}}{\\sigma_{0}^{2}}+\\frac{n\\bar{x}}{\\sigma^{2}}}{\\frac{1}{\\sigma_{0}^{2}}+\\frac{n}{\\sigma^{2}}}.\n$$\nThis is the updated expected value of the mean weekly return.", "answer": "$$\\boxed{\\frac{\\frac{\\mu_{0}}{\\sigma_{0}^{2}}+\\frac{n\\bar{x}}{\\sigma^{2}}}{\\frac{1}{\\sigma_{0}^{2}}+\\frac{n}{\\sigma^{2}}}}$$", "id": "1345529"}, {"introduction": "Beyond simply updating our beliefs, Bayesian methods provide a powerful framework for experimental design. A crucial question for any researcher is, \"How much data is enough?\" This exercise [@problem_id:1345481] challenges you to apply your understanding of the Normal-Normal model to answer that very question, calculating the number of measurements needed to reduce your uncertainty—as quantified by the posterior variance—to a desired level.", "problem": "A materials scientist is investigating the elasticity of a newly developed alloy. Based on theoretical models and previous experience with similar materials, the scientist's prior belief about the mean Young's modulus, $\\mu$, is modeled by a normal distribution with a mean of $\\mu_0 = 150 \\text{ GPa}$ and a variance of $\\sigma_0^2 = 0.8 \\text{ GPa}^2$.\n\nTo refine this estimate, the scientist performs a series of tensile tests. Each measurement, $X_i$, is assumed to be drawn from a normal distribution with mean $\\mu$ and a known measurement variance of $\\sigma^2 = 12.0 \\text{ GPa}^2$, which accounts for the instrument's precision and minor environmental fluctuations. The measurements are independent and identically distributed.\n\nThe scientist aims to reduce the uncertainty about $\\mu$. What is the smallest integer number of measurements, $n$, that must be collected to ensure the posterior variance of $\\mu$ is less than one-fourth of the prior variance?", "solution": "Let the prior be $\\mu \\sim \\mathcal{N}(\\mu_{0},\\sigma_{0}^{2})$ and the data model be $X_{i} \\mid \\mu \\sim \\mathcal{N}(\\mu,\\sigma^{2})$ independently for $i=1,\\dots,n$, with known $\\sigma^{2}$. In the normal–normal conjugate model with known variance, the posterior variance of $\\mu$ after $n$ observations is\n$$\n\\sigma_{n}^{2}=\\left(\\frac{1}{\\sigma_{0}^{2}}+\\frac{n}{\\sigma^{2}}\\right)^{-1}.\n$$\nWe require the posterior variance to be less than one-fourth of the prior variance:\n$$\n\\left(\\frac{1}{\\sigma_{0}^{2}}+\\frac{n}{\\sigma^{2}}\\right)^{-1}<\\frac{1}{4}\\sigma_{0}^{2}.\n$$\nSince all quantities are positive, invert both sides to obtain\n$$\n\\frac{1}{\\sigma_{0}^{2}}+\\frac{n}{\\sigma^{2}}>\\frac{4}{\\sigma_{0}^{2}}.\n$$\nRearrange to isolate $n$:\n$$\n\\frac{n}{\\sigma^{2}}>\\frac{4}{\\sigma_{0}^{2}}-\\frac{1}{\\sigma_{0}^{2}}=\\frac{3}{\\sigma_{0}^{2}}\n\\quad\\Longrightarrow\\quad\nn>\\frac{3\\sigma^{2}}{\\sigma_{0}^{2}}.\n$$\nSubstitute the given variances $\\sigma^{2}=12.0$ and $\\sigma_{0}^{2}=0.8$:\n$$\nn>\\frac{3\\times 12.0}{0.8}=\\frac{36.0}{0.8}=45.\n$$\nBecause the inequality is strict, the smallest integer $n$ satisfying it is $46$.", "answer": "$$\\boxed{46}$$", "id": "1345481"}]}