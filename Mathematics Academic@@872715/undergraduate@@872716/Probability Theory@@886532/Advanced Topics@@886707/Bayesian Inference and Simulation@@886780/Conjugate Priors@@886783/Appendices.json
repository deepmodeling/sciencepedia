{"hands_on_practices": [{"introduction": "A common task in many fields, from marketing to medicine, is to estimate an unknown proportion or probability, such as the click-through rate of an ad. The Beta-Binomial model provides a powerful and elegant framework for this. This exercise [@problem_id:1909036] will guide you through a classic A/B testing scenario, demonstrating how to update your initial beliefs (the prior) about a probability using observed data (the likelihood) to arrive at a more informed posterior belief.", "problem": "An e-commerce company is conducting an A/B test to evaluate a new design for its \"Add to Cart\" button. The key metric is the click-through rate, which is the probability $p$ that a user who sees the button will click it.\n\nBefore running the experiment, the product management team codifies their prior beliefs about $p$ using a Beta distribution with parameters $\\alpha = 2$ and $\\beta = 18$.\n\nThe company then runs an experiment where the new button design is shown to $n=100$ randomly selected users. Out of these users, $k=15$ are observed to click the button.\n\nAssuming the number of clicks follows a Binomial distribution, and using the team's Beta prior, the updated belief about $p$ is described by a posterior distribution. This posterior is also a Beta distribution with new parameters, which we will call $\\alpha_{post}$ and $\\beta_{post}$.\n\nWhich of the following represents the correct pair of posterior hyperparameters $(\\alpha_{post}, \\beta_{post})$?\n\nA. $(17, 103)$\n\nB. $(87, 33)$\n\nC. $(16, 86)$\n\nD. $(17, 85)$\n\nE. $(2, 18)$", "solution": "This problem requires us to find the parameters of the posterior distribution of a probability $p$ by combining a Beta prior with a Binomial likelihood. This is a classic example of Bayesian inference using conjugate priors.\n\nThe core principle is Bayes' theorem, which states that the posterior distribution is proportional to the product of the likelihood of the data and the prior distribution of the parameter.\n$$\n\\text{Posterior} \\propto \\text{Likelihood} \\times \\text{Prior}\n$$\n\nLet's define the components for our specific problem.\n\nFirst, the **prior distribution** for the probability $p$ is given as a Beta distribution, $p \\sim \\text{Beta}(\\alpha, \\beta)$. The Probability Density Function (PDF) of a Beta distribution is:\n$$\nf(p; \\alpha, \\beta) = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} p^{\\alpha-1} (1-p)^{\\beta-1}\n$$\nFor our analysis, we only need the kernel of the distribution, which contains the parts of the function that depend on the parameter $p$. The kernel of the Beta prior is:\n$$\nf(p; \\alpha, \\beta) \\propto p^{\\alpha-1} (1-p)^{\\beta-1}\n$$\nWe are given the prior hyperparameters $\\alpha = 2$ and $\\beta = 18$.\n\nSecond, the **likelihood function** is derived from the experimental data. The data consists of observing $k$ successes (clicks) in $n$ independent trials (users). This is modeled by a Binomial distribution, $k \\sim \\text{Binomial}(n, p)$. The probability mass function for the Binomial distribution gives us the likelihood of observing $k$ successes for a given $p$:\n$$\nL(p | k, n) = \\binom{n}{k} p^k (1-p)^{n-k}\n$$\nAgain, we are interested in the part that depends on $p$, so the kernel of the likelihood is:\n$$\nL(p | k, n) \\propto p^k (1-p)^{n-k}\n$$\nWe are given the experimental data $n = 100$ and $k = 15$.\n\nNow, we apply Bayes' theorem to find the posterior distribution, $p(p | k, n, \\alpha, \\beta)$:\n$$\np(p | \\text{data}) \\propto L(p | \\text{data}) \\times f(p)\n$$\nSubstituting the kernels of the likelihood and the prior:\n$$\np(p | k, n, \\alpha, \\beta) \\propto \\left( p^k (1-p)^{n-k} \\right) \\times \\left( p^{\\alpha-1} (1-p)^{\\beta-1} \\right)\n$$\nWe can combine the terms by adding the exponents:\n$$\np(p | k, n, \\alpha, \\beta) \\propto p^{k + \\alpha - 1} (1-p)^{n - k + \\beta - 1}\n$$\nThis resulting functional form is the kernel of another Beta distribution. A general Beta distribution with parameters $\\alpha_{post}$ and $\\beta_{post}$ has a kernel proportional to $p^{\\alpha_{post}-1} (1-p)^{\\beta_{post}-1}$.\n\nBy comparing our derived posterior kernel with the general Beta kernel, we can identify the posterior hyperparameters:\n$$\n\\alpha_{post} - 1 = k + \\alpha - 1 \\implies \\alpha_{post} = k + \\alpha\n$$\n$$\n\\beta_{post} - 1 = n - k + \\beta - 1 \\implies \\beta_{post} = n - k + \\beta\n$$\nThese are the general update rules for a Beta-Binomial conjugate pair.\n\nFinally, we substitute the numerical values given in the problem:\n- Prior hyperparameters: $\\alpha = 2$, $\\beta = 18$\n- Data: $n = 100$, $k = 15$\n\nThe posterior hyperparameter $\\alpha_{post}$ is:\n$$\n\\alpha_{post} = 15 + 2 = 17\n$$\nThe posterior hyperparameter $\\beta_{post}$ is:\n$$\n\\beta_{post} = (100 - 15) + 18 = 85 + 18 = 103\n$$\nThus, the posterior distribution is a Beta distribution with parameters $\\alpha_{post} = 17$ and $\\beta_{post} = 103$. The pair of posterior hyperparameters is $(17, 103)$.\n\nThis corresponds to option A.", "answer": "$$\\boxed{A}$$", "id": "1909036"}, {"introduction": "Beyond proportions, we often need to model the rate at which events occur over time or space, like customer arrivals or system failures. The Poisson distribution is the standard model for such count data, and its conjugate prior is the Gamma distribution. In this practice problem [@problem_id:1909064], you will model an email arrival rate, reinforcing the pattern of Bayesian updating and seeing how the Gamma-Poisson pair works in a practical context.", "problem": "A data scientist is modeling the rate at which a corporate employee receives emails. The arrival of emails is assumed to follow a Poisson process. The unknown average email arrival rate, $\\lambda$, is measured in emails per hour.\n\nThe data scientist's prior belief about $\\lambda$ is modeled by a Gamma distribution with a shape parameter $\\alpha = 10$ and a rate parameter $\\beta = 2$. The probability density function for this Gamma($\\alpha, \\beta$) distribution is given by:\n$$p(\\lambda) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\lambda^{\\alpha-1} \\exp(-\\beta \\lambda) \\quad \\text{for } \\lambda  0$$\n\nTo update this belief, the data scientist observes the employee over a period of $T = 8$ hours. During this observation period, the employee receives a total of $n = 52$ emails.\n\nAssuming the number of emails received in $T$ hours follows a Poisson distribution with parameter $\\lambda T$, calculate the updated estimate for the average hourly email rate, which is the mean of the posterior distribution of $\\lambda$. Express your answer in emails per hour, rounded to three significant figures.", "solution": "The problem asks for the mean of the posterior distribution of the email arrival rate $\\lambda$. We can find the posterior distribution using Bayes' theorem, which states that the posterior is proportional to the likelihood of the data multiplied by the prior distribution of the parameter.\n\nLet the observed data be $D$, which is the observation of $n=52$ emails in $T=8$ hours. According to Bayes' theorem:\n$$p(\\lambda | D) \\propto P(D | \\lambda) \\times p(\\lambda)$$\nHere, $p(\\lambda | D)$ is the posterior distribution, $P(D | \\lambda)$ is the likelihood function, and $p(\\lambda)$ is the prior distribution.\n\nFirst, we define the likelihood function. The number of emails $n$ in a time interval $T$ is assumed to follow a Poisson distribution with parameter $\\lambda T$. The probability mass function for a Poisson distribution is $P(k; \\mu) = \\frac{\\mu^k e^{-\\mu}}{k!}$. So, the likelihood of observing $n=52$ emails in $T=8$ hours is:\n$$L(\\lambda; n, T) = P(n | \\lambda, T) = \\frac{(T\\lambda)^n}{n!} \\exp(-T\\lambda)$$\n\nNext, we define the prior distribution. The prior for $\\lambda$ is given as a Gamma distribution with shape parameter $\\alpha=10$ and rate parameter $\\beta=2$:\n$$p(\\lambda) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\lambda^{\\alpha-1} \\exp(-\\beta \\lambda)$$\n\nNow, we compute the posterior distribution by multiplying the likelihood and the prior. We can ignore any constant factors that do not depend on $\\lambda$, as they will be absorbed into the normalization constant of the posterior distribution.\n$$p(\\lambda | n, T) \\propto \\left( \\frac{(T\\lambda)^n}{n!} \\exp(-T\\lambda) \\right) \\times \\left( \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\lambda^{\\alpha-1} \\exp(-\\beta \\lambda) \\right)$$\nDropping the constants $\\frac{T^n}{n!}$ and $\\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}$:\n$$p(\\lambda | n, T) \\propto (\\lambda^n \\exp(-T\\lambda)) \\times (\\lambda^{\\alpha-1} \\exp(-\\beta \\lambda))$$\nCombining the terms involving $\\lambda$:\n$$p(\\lambda | n, T) \\propto \\lambda^n \\lambda^{\\alpha-1} \\exp(-T\\lambda) \\exp(-\\beta \\lambda)$$\n$$p(\\lambda | n, T) \\propto \\lambda^{n+\\alpha-1} \\exp(-(T+\\beta)\\lambda)$$\n\nThis resulting functional form is the kernel of a Gamma distribution. This means the posterior distribution is also a Gamma distribution. A distribution $f(x) \\propto x^{a-1} \\exp(-bx)$ is a Gamma distribution with shape parameter $a$ and rate parameter $b$. By comparing the exponents, we can identify the parameters of the posterior distribution, let's call them $\\alpha'$ and $\\beta'$:\nShape parameter: $\\alpha' = n + \\alpha$\nRate parameter: $\\beta' = T + \\beta$\n\nNow, we substitute the given numerical values:\n$\\alpha = 10$\n$\\beta = 2$\n$n = 52$\n$T = 8$\n\nThe parameters for the posterior Gamma distribution are:\n$\\alpha' = 52 + 10 = 62$\n$\\beta' = 8 + 2 = 10$\n\nSo, the posterior distribution of $\\lambda$ is a Gamma distribution with shape $\\alpha' = 62$ and rate $\\beta' = 10$.\n$$\\lambda | n, T \\sim \\text{Gamma}(62, 10)$$\n\nThe problem asks for the mean of this posterior distribution. The mean of a Gamma distribution with shape parameter $a$ and rate parameter $b$ is given by the formula $E[\\lambda] = \\frac{a}{b}$.\nTherefore, the posterior mean of $\\lambda$ is:\n$$E[\\lambda | n, T] = \\frac{\\alpha'}{\\beta'} = \\frac{62}{10} = 6.2$$\n\nThe problem requires the answer to be rounded to three significant figures. The calculated value is 6.2, which, expressed with three significant figures, is 6.20. The units are emails per hour as specified.", "answer": "$$\\boxed{6.20}$$", "id": "1909064"}, {"introduction": "Many scientific and engineering problems involve estimating a true, unobservable quantity, like a physical constant or a material property, from noisy measurements. When our data and our prior beliefs about the mean can both be modeled as normal distributions, we have another convenient conjugate pairing. This problem [@problem_id:1352230] asks you to derive the posterior mean not as a number, but as a symbolic expression, revealing an elegant and intuitive principle: the posterior estimate is a weighted average of your prior belief and your data, with the weights determined by their respective precisions.", "problem": "A materials scientist is investigating the properties of a newly synthesized superconducting alloy. The key parameter of interest is the true mean critical temperature, denoted by $T_c$, at which the material transitions to a superconducting state.\n\nBased on theoretical models and properties of similar alloys, the scientist's initial belief about $T_c$ can be described by a normal distribution with a mean of $\\mu_0$ and a variance of $\\sigma_0^2$.\n\nTo refine this estimate, the scientist conducts an experiment, preparing $N$ identical samples of the alloy and measuring their individual critical temperatures. The measurement process is subject to experimental noise. It is known that any single measurement is a random variable drawn from a normal distribution whose mean is the true mean critical temperature $T_c$ and whose variance is $\\sigma^2$. After conducting the experiment, the sample mean of the $N$ measurements is calculated to be $\\bar{T}$.\n\nYour task is to determine the scientist's updated estimate for the mean critical temperature by combining the initial belief with the new experimental data. Express your answer as a symbolic expression in terms of $\\mu_0, \\sigma_0, N, \\sigma,$ and $\\bar{T}$.", "solution": "We model the true mean critical temperature as a random variable with prior $T_{c} \\sim \\mathcal{N}(\\mu_{0},\\sigma_{0}^{2})$. The experiment produces $N$ i.i.d. measurements with noise variance $\\sigma^{2}$, so the sample mean satisfies $\\bar{T} \\mid T_{c} \\sim \\mathcal{N}(T_{c},\\sigma^{2}/N)$.\n\nUsing Bayesâ€™ rule with the sufficient statistic $\\bar{T}$, the posterior density is proportional to the product of the prior and the likelihood:\n$$\np(T_{c}\\mid \\bar{T}) \\propto \\exp\\!\\left(-\\frac{(T_{c}-\\mu_{0})^{2}}{2\\sigma_{0}^{2}}\\right)\\,\\exp\\!\\left(-\\frac{(\\bar{T}-T_{c})^{2}}{2(\\sigma^{2}/N)}\\right)\n= \\exp\\!\\left(-\\frac{(T_{c}-\\mu_{0})^{2}}{2\\sigma_{0}^{2}}-\\frac{N(T_{c}-\\bar{T})^{2}}{2\\sigma^{2}}\\right).\n$$\nCollecting terms in $T_{c}$, we obtain a quadratic form:\n$$\n-\\frac{1}{2}\\left(\\left(\\frac{1}{\\sigma_{0}^{2}}+\\frac{N}{\\sigma^{2}}\\right)T_{c}^{2}\n-2\\left(\\frac{\\mu_{0}}{\\sigma_{0}^{2}}+\\frac{N\\bar{T}}{\\sigma^{2}}\\right)T_{c}\\right)+\\text{const}.\n$$\nCompleting the square shows that the posterior is normal with precision\n$$\n\\tau_{n}=\\frac{1}{\\sigma_{0}^{2}}+\\frac{N}{\\sigma^{2}}\n$$\nand mean\n$$\n\\mu_{n}=\\frac{\\frac{\\mu_{0}}{\\sigma_{0}^{2}}+\\frac{N\\bar{T}}{\\sigma^{2}}}{\\frac{1}{\\sigma_{0}^{2}}+\\frac{N}{\\sigma^{2}}}.\n$$\nTherefore, the updated estimate (posterior mean) for the true mean critical temperature is\n$$\n\\mu_{n}=\\frac{\\frac{\\mu_{0}}{\\sigma_{0}^{2}}+\\frac{N\\bar{T}}{\\sigma^{2}}}{\\frac{1}{\\sigma_{0}^{2}}+\\frac{N}{\\sigma^{2}}}.\n$$\nEquivalently, it is the precision-weighted average $\\mu_{n}=\\left(\\sigma^{2}\\mu_{0}+N\\sigma_{0}^{2}\\bar{T}\\right)\\big/\\left(\\sigma^{2}+N\\sigma_{0}^{2}\\right)$.", "answer": "$$\\boxed{\\frac{\\frac{\\mu_{0}}{\\sigma_{0}^{2}}+\\frac{N\\bar{T}}{\\sigma^{2}}}{\\frac{1}{\\sigma_{0}^{2}}+\\frac{N}{\\sigma^{2}}}}$$", "id": "1352230"}]}