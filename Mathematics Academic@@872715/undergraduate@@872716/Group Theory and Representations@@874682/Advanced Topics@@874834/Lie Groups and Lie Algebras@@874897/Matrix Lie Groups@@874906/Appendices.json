{"hands_on_practices": [{"introduction": "This first practice focuses on the matrix exponential, the fundamental map that connects a Lie algebra to its corresponding Lie group. By computing $e^X$ for a matrix $X$ that generates both scaling and rotation, you will gain direct experience with this crucial tool and see how group elements with clear geometric interpretations arise from simpler algebraic objects. This exercise is foundational for understanding how continuous transformations are built from infinitesimal generators. [@problem_id:1629845]", "problem": "In the study of continuous symmetries, the matrix exponential provides a bridge from a Lie algebra, which is a vector space of infinitesimal generators, to a Lie group, which is a group of transformations. This problem explores the exponentiation of a matrix that generates both rotations and uniform scaling in a two-dimensional plane.\n\nConsider the Lie algebra consisting of all $2 \\times 2$ real matrices of the form:\n$$X = \\begin{pmatrix} \\alpha  \\beta \\\\ -\\beta  \\alpha \\end{pmatrix}$$\nwhere $\\alpha$ and $\\beta$ are arbitrary real numbers.\n\nYour task is to compute the matrix exponential $G = e^X$. The resulting matrix $G$ will be an element of the corresponding Lie group of similarity transformations in the plane. Express your final answer as a $2 \\times 2$ matrix whose entries are functions of $\\alpha$ and $\\beta$.", "solution": "The matrix exponential $e^X$ is defined by its Taylor series expansion:\n$$e^X = \\sum_{k=0}^{\\infty} \\frac{X^k}{k!} = I + X + \\frac{X^2}{2!} + \\frac{X^3}{3!} + \\dots$$\nTo compute this for the given matrix $X$, we can decompose $X$ into two simpler, commuting parts. Let's write $X$ as a sum of a scalar matrix and a skew-symmetric matrix:\n$$X = \\begin{pmatrix} \\alpha  0 \\\\ 0  \\alpha \\end{pmatrix} + \\begin{pmatrix} 0  \\beta \\\\ -\\beta  0 \\end{pmatrix}$$\nWe can factor out the scalar values $\\alpha$ and $\\beta$:\n$$X = \\alpha \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} + \\beta \\begin{pmatrix} 0  1 \\\\ -1  0 \\end{pmatrix}$$\nLet's define $A = \\alpha I$ and $B = \\beta J$, where $I = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}$ is the identity matrix and $J = \\begin{pmatrix} 0  1 \\\\ -1  0 \\end{pmatrix}$. Thus, $X = A + B$.\n\nNext, we check if $A$ and $B$ commute.\n$$AB = (\\alpha I)(\\beta J) = \\alpha \\beta (IJ) = \\alpha \\beta J$$\n$$BA = (\\beta J)(\\alpha I) = \\beta \\alpha (JI) = \\alpha \\beta J$$\nSince $AB = BA$, the matrices commute. For commuting matrices, the exponential of their sum is the product of their exponentials: $e^{A+B} = e^A e^B$. We can compute $e^A$ and $e^B$ separately.\n\nFirst, let's compute $e^A = e^{\\alpha I}$:\n$$e^{\\alpha I} = \\sum_{k=0}^{\\infty} \\frac{(\\alpha I)^k}{k!} = \\sum_{k=0}^{\\infty} \\frac{\\alpha^k I^k}{k!}$$\nSince $I^k = I$ for any $k \\ge 1$ and $I^0=I$, we can factor out $I$:\n$$e^{\\alpha I} = I \\left( \\sum_{k=0}^{\\infty} \\frac{\\alpha^k}{k!} \\right) = e^{\\alpha} I = \\begin{pmatrix} e^{\\alpha}  0 \\\\ 0  e^{\\alpha} \\end{pmatrix}$$\n\nSecond, let's compute $e^B = e^{\\beta J}$. We need the powers of $J$:\n$J^0 = I = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}$\n$J^1 = J = \\begin{pmatrix} 0  1 \\\\ -1  0 \\end{pmatrix}$\n$J^2 = \\begin{pmatrix} 0  1 \\\\ -1  0 \\end{pmatrix} \\begin{pmatrix} 0  1 \\\\ -1  0 \\end{pmatrix} = \\begin{pmatrix} -1  0 \\\\ 0  -1 \\end{pmatrix} = -I$\n$J^3 = J^2 J = (-I)J = -J$\n$J^4 = J^2 J^2 = (-I)(-I) = I$\nThe powers of $J$ are periodic with a period of 4, similar to the imaginary unit $i$ in complex numbers.\n\nNow we can write out the series for $e^{\\beta J}$:\n$$e^{\\beta J} = \\sum_{k=0}^{\\infty} \\frac{(\\beta J)^k}{k!} = \\frac{\\beta^0 J^0}{0!} + \\frac{\\beta^1 J^1}{1!} + \\frac{\\beta^2 J^2}{2!} + \\frac{\\beta^3 J^3}{3!} + \\dots$$\n$$e^{\\beta J} = I + \\beta J - \\frac{\\beta^2}{2!}I - \\frac{\\beta^3}{3!}J + \\frac{\\beta^4}{4!}I + \\dots$$\nLet's group the terms with $I$ and the terms with $J$:\n$$e^{\\beta J} = \\left(1 - \\frac{\\beta^2}{2!} + \\frac{\\beta^4}{4!} - \\dots \\right)I + \\left(\\beta - \\frac{\\beta^3}{3!} + \\frac{\\beta^5}{5!} - \\dots \\right)J$$\nWe recognize the Taylor series for cosine and sine:\n$$\\cos(\\beta) = 1 - \\frac{\\beta^2}{2!} + \\frac{\\beta^4}{4!} - \\dots$$\n$$\\sin(\\beta) = \\beta - \\frac{\\beta^3}{3!} + \\frac{\\beta^5}{5!} - \\dots$$\nSo, we can write:\n$$e^{\\beta J} = \\cos(\\beta) I + \\sin(\\beta) J$$\nSubstituting the matrices for $I$ and $J$:\n$$e^{\\beta J} = \\cos(\\beta) \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} + \\sin(\\beta) \\begin{pmatrix} 0  1 \\\\ -1  0 \\end{pmatrix} = \\begin{pmatrix} \\cos(\\beta)  \\sin(\\beta) \\\\ -\\sin(\\beta)  \\cos(\\beta) \\end{pmatrix}$$\n\nFinally, we find $e^X$ by multiplying $e^A$ and $e^B$:\n$$e^X = e^A e^B = \\begin{pmatrix} e^{\\alpha}  0 \\\\ 0  e^{\\alpha} \\end{pmatrix} \\begin{pmatrix} \\cos(\\beta)  \\sin(\\beta) \\\\ -\\sin(\\beta)  \\cos(\\beta) \\end{pmatrix}$$\n$$e^X = \\begin{pmatrix} e^{\\alpha}\\cos(\\beta)  e^{\\alpha}\\sin(\\beta) \\\\ -e^{\\alpha}\\sin(\\beta)  e^{\\alpha}\\cos(\\beta) \\end{pmatrix}$$\nThis is the final matrix $G$. It represents a rotation by angle $\\beta$ and a uniform scaling by a factor of $e^{\\alpha}$.", "answer": "$$\\boxed{\\begin{pmatrix} \\exp(\\alpha) \\cos(\\beta)  \\exp(\\alpha) \\sin(\\beta) \\\\ -\\exp(\\alpha) \\sin(\\beta)  \\exp(\\alpha) \\cos(\\beta) \\end{pmatrix}}$$", "id": "1629845"}, {"introduction": "While the exponential map takes us from the algebra to the group, the Lie bracket defines the essential structure within the Lie algebra itself. This problem uses a hypothetical scenario involving a robotic arm to provide a powerful intuition for the Lie bracket, demonstrating that it quantifies the failure of two infinitesimal transformations to commute. Calculating the result of performing two operations in different orders reveals a non-zero term directly related to the bracket of their generators, making an abstract concept tangible. [@problem_id:1629899]", "problem": "In a simplified model of a 2D robotic arm's control system, its end-effector's pose is altered by a series of small, continuous transformations. These transformations, acting on a point $(x, y)$, are described as:\n$$ x' = x + ay + c $$\n$$ y' = y + b $$\nwhere $a, b, c$ are real parameters. The state of the system is represented by a homogeneous coordinate vector $(x, y, 1)^T$, and each transformation is encoded by a $3 \\times 3$ matrix $M(a, b, c)$. The set of all such matrices, $G = \\{M(a, b, c) \\mid a, b, c \\in \\mathbb{R}\\}$, forms a matrix Lie group.\n\nConsider two fundamental control operations, $C_1$ and $C_2$, which are applied over a time interval $t$.\n- Operation $C_1(t)$ corresponds to a simultaneous horizontal shear and vertical translation, represented by the matrix $G_1(t) = M(t, t, 0)$.\n- Operation $C_2(t)$ corresponds to a simultaneous horizontal shear and horizontal translation, represented by the matrix $G_2(t) = M(t, 0, t)$.\n\nExecuting these operations in a different order may lead to a different final state. Let's quantify this non-commutativity. Consider two sequences of operations over an infinitesimally small time $\\delta t$:\n- Sequence A: Apply $C_1(\\delta t)$, then $C_2(\\delta t)$. The total transformation is $M_A = G_2(\\delta t)G_1(\\delta t)$.\n- Sequence B: Apply $C_2(\\delta t)$, then $C_1(\\delta t)$. The total transformation is $M_B = G_1(\\delta t)G_2(\\delta t)$.\n\nThe difference between these two sequences for small $\\delta t$ reveals the underlying structure of the control space. Your task is to compute the infinitesimal difference, which is mathematically captured by the following limit:\n$$ D = \\lim_{\\delta t \\to 0} \\frac{M_B - M_A}{(\\delta t)^2} $$\nPresent your final answer for the matrix $D$.", "solution": "The affine transformation on homogeneous coordinates $(x,y,1)^{T}$ given by $x' = x + ay + c$ and $y' = y + b$ is represented by the $3 \\times 3$ matrix\n$$\nM(a,b,c) = \\begin{pmatrix}\n1  a  c \\\\\n0  1  b \\\\\n0  0  1\n\\end{pmatrix},\n$$\nsince this matrix yields $x' = 1\\cdot x + a\\cdot y + c\\cdot 1$ and $y' = 0\\cdot x + 1\\cdot y + b\\cdot 1$.\n\nThe two operations are thus\n$$\nG_{1}(t) = M(t,t,0) = \\begin{pmatrix}\n1  t  0 \\\\\n0  1  t \\\\\n0  0  1\n\\end{pmatrix}, \\quad\nG_{2}(t) = M(t,0,t) = \\begin{pmatrix}\n1  t  t \\\\\n0  1  0 \\\\\n0  0  1\n\\end{pmatrix}.\n$$\nSequence A applies $C_{1}(\\delta t)$ then $C_{2}(\\delta t)$, giving total matrix $M_{A} = G_{2}(\\delta t)G_{1}(\\delta t)$, while Sequence B applies $C_{2}(\\delta t)$ then $C_{1}(\\delta t)$, giving $M_{B} = G_{1}(\\delta t)G_{2}(\\delta t)$. Let $t = \\delta t$.\n\nCompute $M_{A} = G_{2}(t)G_{1}(t)$:\n$$\nG_{2}(t)G_{1}(t)\n=\n\\begin{pmatrix}\n1  t  t \\\\\n0  1  0 \\\\\n0  0  1\n\\end{pmatrix}\n\\begin{pmatrix}\n1  t  0 \\\\\n0  1  t \\\\\n0  0  1\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1  2t  t + t^{2} \\\\\n0  1  t \\\\\n0  0  1\n\\end{pmatrix}.\n$$\n\nCompute $M_{B} = G_{1}(t)G_{2}(t)$:\n$$\nG_{1}(t)G_{2}(t)\n=\n\\begin{pmatrix}\n1  t  0 \\\\\n0  1  t \\\\\n0  0  1\n\\end{pmatrix}\n\\begin{pmatrix}\n1  t  t \\\\\n0  1  0 \\\\\n0  0  1\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1  2t  t \\\\\n0  1  t \\\\\n0  0  1\n\\end{pmatrix}.\n$$\n\nThus,\n$$\nM_{B} - M_{A}\n=\n\\begin{pmatrix}\n0  0  t - (t + t^{2}) \\\\\n0  0  0 \\\\\n0  0  0\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0  0  -t^{2} \\\\\n0  0  0 \\\\\n0  0  0\n\\end{pmatrix}.\n$$\nDividing by $t^{2}$ and taking the limit as $t \\to 0$ yields\n$$\nD = \\lim_{t \\to 0} \\frac{M_{B} - M_{A}}{t^{2}} = \\begin{pmatrix}\n0  0  -1 \\\\\n0  0  0 \\\\\n0  0  0\n\\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}0  0  -1 \\\\ 0  0  0 \\\\ 0  0  0\\end{pmatrix}}$$", "id": "1629899"}, {"introduction": "The algebraic structure defined by the Lie bracket has profound consequences for how the group can be represented by matrices. This final exercise introduces the quadratic Casimir operator, an operator built from the algebra's generators that commutes with all of them. As a consequence of Schur's Lemma, it must act as a scalar multiple of the identity in any irreducible representation. Calculating this scalar value for a specific representation of $SU(2)$, the group of special unitary $2 \\times 2$ matrices, connects the abstract theory to a concrete, measurable quantity in the quantum theory of angular momentum. [@problem_id:1629855]", "problem": "In the quantum mechanical theory of angular momentum, the operators representing the components of an angular momentum vector, denoted by $J_1, J_2, J_3$, form a basis for the Lie algebra $\\mathfrak{su}(2)$. These operators are Hermitian and satisfy the commutation relations\n$$\n[J_i, J_j] = i \\sum_{k=1}^{3} \\varepsilon_{ijk} J_k,\n$$\nwhere $\\varepsilon_{ijk}$ is the Levi-Civita symbol.\n\nConsider the operator $C$ defined as the sum of the squares of these generators:\n$$\nC = J_1^2 + J_2^2 + J_3^2.\n$$\nWithin any finite-dimensional irreducible representation of the corresponding Lie group $SU(2)$, the operator $C$ acts as a scalar multiple of the identity matrix. That is, the action of $C$ on any state in the representation space is equivalent to multiplication by a single constant value, which is the eigenvalue of $C$ for that representation.\n\nDetermine the numerical value of this eigenvalue for the unique irreducible representation of dimension $d=5$.", "solution": "We use the given commutation relations\n$$\n[J_{i},J_{j}] = i \\sum_{k=1}^{3} \\varepsilon_{ijk} J_{k},\n$$\nwhich define the Lie algebra $\\mathfrak{su}(2)$ in units where $\\hbar=1$. The operator\n$$\nC = J_{1}^{2} + J_{2}^{2} + J_{3}^{2}\n$$\nis the quadratic Casimir of $\\mathfrak{su}(2)$. In any finite-dimensional irreducible representation (irrep) labeled by spin $j \\in \\{0,\\tfrac{1}{2},1,\\tfrac{3}{2},\\dots\\}$, the standard angular momentum theory yields that $C$ commutes with all $J_{i}$ and acts as a scalar multiple of the identity on the irrep.\n\nThe dimension $d$ of the irrep is related to $j$ by\n$$\nd = 2j + 1.\n$$\nTherefore, for a given dimension $d$, we have\n$$\nj = \\frac{d - 1}{2}.\n$$\nThe eigenvalue of the Casimir operator $C$ in the spin-$j$ irrep is\n$$\nC \\mapsto j(j+1).\n$$\nFor the unique irrep of dimension $d=5$, we compute\n$$\nj = \\frac{5 - 1}{2} = 2,\n$$\nso the Casimir eigenvalue is\n$$\nj(j+1) = 2 \\cdot 3 = 6.\n$$\nThus the numerical value of the eigenvalue of $C$ in the $d=5$ irrep is $6$.", "answer": "$$\\boxed{6}$$", "id": "1629855"}]}