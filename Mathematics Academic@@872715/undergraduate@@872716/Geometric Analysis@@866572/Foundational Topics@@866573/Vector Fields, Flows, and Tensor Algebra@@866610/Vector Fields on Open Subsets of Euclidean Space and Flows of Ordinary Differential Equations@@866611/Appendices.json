{"hands_on_practices": [{"introduction": "We begin our hands-on exploration with the most fundamental class of systems beyond constant vector fields: linear systems. This practice examines the flow generated by a vector field $X(x) = Ax$, exploring how the solution's sensitivity to initial conditions—captured by the Jacobian matrix—evolves in time. By deriving the variational equation from first principles, you will uncover the profound and elegant connection between the Jacobian of the flow and the matrix exponential, a cornerstone of linear dynamical systems theory [@problem_id:3078144].", "problem": "Let $n \\in \\mathbb{N}$ and let $A \\in \\mathbb{R}^{n \\times n}$ be a fixed constant matrix. Consider the smooth vector field $X : \\mathbb{R}^{n} \\to \\mathbb{R}^{n}$ given by $X(x) = A x$ on the open subset $\\mathbb{R}^{n} \\subset \\mathbb{R}^{n}$. Let $\\varphi_{t} : \\mathbb{R}^{n} \\to \\mathbb{R}^{n}$ denote the flow of the ordinary differential equation (ODE) $\\dot{x} = X(x)$, characterized by the properties $\\frac{d}{dt}\\varphi_{t}(x) = X(\\varphi_{t}(x))$ and $\\varphi_{0}(x) = x$ for all $x \\in \\mathbb{R}^{n}$ and all $t \\in \\mathbb{R}$ for which the solution exists. Using only the fundamental definitions of flow, Jacobian (derivative with respect to initial condition), and the characterization of the matrix exponential via its power series, derive the linear variational equation governing the Jacobian $D\\varphi_{t}(x)$ and solve it explicitly. Prove from first principles that $D\\varphi_{t}(x)$ does not depend on $x$. Express your final result in a single closed-form analytic expression in terms of $A$ and $t$. No rounding is required, and no units are involved.", "solution": "The task is to derive the equation governing the Jacobian of the flow of a linear ODE, demonstrate its independence from the initial condition, and solve it explicitly using first principles.\n\nLet the flow be denoted by $\\varphi_t(x)$. By definition, it satisfies the initial value problem:\n$$\n\\frac{d}{dt}\\varphi_{t}(x) = X(\\varphi_{t}(x)), \\quad \\varphi_{0}(x) = x\n$$\nHere, the vector field is $X(x) = Ax$. Thus, the equation for the flow is:\n$$\n\\frac{d}{dt}\\varphi_{t}(x) = A\\varphi_{t}(x)\n$$\nThe Jacobian of the flow, denoted by $J(t, x) = D\\varphi_{t}(x)$, is the matrix of partial derivatives of the components of $\\varphi_t(x)$ with respect to the components of the initial condition $x$. That is, $(J(t, x))_{ij} = \\frac{\\partial (\\varphi_t)_i}{\\partial x_j}(x)$.\n\nTo find the differential equation governing $J(t, x)$, we differentiate the flow's ODE with respect to $x$. Since the flow of an autonomous ODE is smooth with respect to both $t$ and $x$, we can interchange the order of differentiation (by Schwarz's theorem on the equality of mixed partials):\n$$\n\\frac{d}{dt} (D\\varphi_{t}(x)) = D\\left(\\frac{d}{dt}\\varphi_{t}(x)\\right)\n$$\nSubstituting the definition of the flow:\n$$\n\\frac{d}{dt} J(t, x) = D(X(\\varphi_{t}(x)))\n$$\nWe apply the chain rule to the right-hand side. The derivative is with respect to the variable $x$:\n$$\nD(X(\\varphi_{t}(x))) = (DX)(\\varphi_{t}(x)) \\cdot D\\varphi_{t}(x)\n$$\nwhere $(DX)(y)$ is the Jacobian of the vector field $X$ evaluated at a point $y$.\nThus, the general linear variational equation is:\n$$\n\\frac{d}{dt} J(t, x) = (DX)(\\varphi_{t}(x)) \\cdot J(t, x)\n$$\nFor the specific vector field given, $X(x) = Ax$, we compute its Jacobian. Let $X_i(x) = \\sum_{k=1}^n A_{ik} x_k$. The partial derivative with respect to $x_j$ is:\n$$\n\\frac{\\partial X_i}{\\partial x_j} = \\frac{\\partial}{\\partial x_j} \\left( \\sum_{k=1}^n A_{ik} x_k \\right) = A_{ik} \\delta_{kj} = A_{ij}\n$$\nwhere $\\delta_{kj}$ is the Kronecker delta. Therefore, the Jacobian matrix $DX$ is the constant matrix $A$ itself, i.e., $DX(y) = A$ for all $y \\in \\mathbb{R}^n$.\n\nSubstituting this result into the variational equation, we find:\n$$\n\\frac{d}{dt} J(t, x) = A \\cdot J(t, x)\n$$\nThis is the linear variational equation for the given system.\n\nNext, we determine the initial condition for $J(t, x)$. The flow at time $t=0$ is given by $\\varphi_{0}(x) = x$. We take the Jacobian with respect to $x$:\n$$\nJ(0, x) = D\\varphi_{0}(x) = D(x) = I\n$$\nwhere $I$ is the $n \\times n$ identity matrix.\n\nThe Jacobian $J(t, x) = D\\varphi_t(x)$ is therefore the solution to the matrix initial value problem (IVP):\n$$\n\\frac{dJ}{dt} = AJ, \\quad J(0) = I\n$$\nThis IVP for $J$ is a system of linear homogeneous ODEs with constant coefficients, and the initial condition is a constant matrix. Crucially, neither the differential equation nor the initial condition depends on the variable $x$ or the state $\\varphi_t(x)$. By the theory of ODEs, this IVP has a unique solution. Since the formulation of the IVP is independent of $x$, its unique solution must also be independent of $x$. This proves from first principles that $D\\varphi_t(x)$ does not depend on $x$. We may thus write $J(t)$ instead of $J(t, x)$.\n\nFinally, we solve this IVP using the power series definition of the matrix exponential. The matrix exponential is defined as:\n$$\n\\exp(tA) = \\sum_{k=0}^{\\infty} \\frac{(tA)^k}{k!} = \\sum_{k=0}^{\\infty} \\frac{t^k}{k!} A^k\n$$\nWe propose the solution $J(t) = \\exp(tA)$ and verify that it satisfies the IVP.\n\nFirst, we check the initial condition at $t=0$:\n$$\nJ(0) = \\exp(0 \\cdot A) = \\sum_{k=0}^{\\infty} \\frac{0^k}{k!} A^k\n$$\nFor $k=0$, the term is $\\frac{0^0}{0!}A^0 = \\frac{1}{1}I = I$. For all $k \\ge 1$, the term $\\frac{0^k}{k!}A^k$ is the zero matrix. Thus, $J(0) = I$, which matches the initial condition.\n\nSecond, we check the differential equation. We differentiate the power series term by term with respect to $t$. This is permissible as the series has an infinite radius of convergence for any matrix $A$.\n$$\n\\frac{dJ}{dt} = \\frac{d}{dt} \\exp(tA) = \\frac{d}{dt} \\left( \\sum_{k=0}^{\\infty} \\frac{t^k}{k!} A^k \\right)\n$$\n$$\n= \\sum_{k=0}^{\\infty} \\frac{d}{dt}\\left(\\frac{t^k}{k!}\\right) A^k = \\sum_{k=1}^{\\infty} \\frac{k t^{k-1}}{k!} A^k\n$$\nThe summation starts from $k=1$ because the $k=0$ term is the constant matrix $I$, whose derivative is the zero matrix.\n$$\n\\frac{dJ}{dt} = \\sum_{k=1}^{\\infty} \\frac{t^{k-1}}{(k-1)!} A^k = A \\left( \\sum_{k=1}^{\\infty} \\frac{t^{k-1}}{(k-1)!} A^{k-1} \\right)\n$$\nLet's define a new index $j = k-1$. As $k$ goes from $1$ to $\\infty$, $j$ goes from $0$ to $\\infty$.\n$$\n\\frac{dJ}{dt} = A \\left( \\sum_{j=0}^{\\infty} \\frac{t^j}{j!} A^j \\right) = A \\exp(tA)\n$$\nSince we defined $J(t) = \\exp(tA)$, we have shown that $\\frac{dJ}{dt} = AJ$.\nThe function $J(t) = \\exp(tA)$ satisfies both the differential equation and the initial condition. By the existence and uniqueness theorem for linear ODEs, this is the unique solution.\n\nTherefore, the Jacobian of the flow, $D\\varphi_t(x)$, is given by the matrix exponential $\\exp(tA)$. This expression is a closed-form analytic solution in terms of $A$ and $t$.", "answer": "$$\\boxed{\\exp(tA)}$$", "id": "3078144"}, {"introduction": "While linear systems guarantee solutions that exist for all time, the world of nonlinear vector fields is far more intricate. This next practice confronts a quintessential nonlinear phenomenon: finite-time blowup, where a trajectory escapes to infinity in a finite duration. By analyzing the simple yet illustrative vector field $X(x) = x^2+1$, you will explicitly calculate a solution that cannot be extended indefinitely and determine its maximal interval of existence, providing a stark contrast to the global nature of linear flows [@problem_id:3078146].", "problem": "Consider the smooth autonomous vector field $X : \\mathbb{R} \\to \\mathbb{R}$ on the open subset $\\mathbb{R} \\subset \\mathbb{R}$ given by $X(x) = x^{2} + 1$. Let a trajectory be a maximal integral curve $t \\mapsto x(t)$ solving the ordinary differential equation $\\dot{x}(t) = X(x(t))$ with an initial condition $x(0) = a \\in \\mathbb{R}$. Using only fundamental principles of ordinary differential equations on Euclidean space—namely, existence and uniqueness for locally Lipschitz vector fields, separation of variables, and basic integration—derive the explicit form of the trajectory and justify from first principles that it blows up in finite time in both forward and backward directions. Then determine the maximal open interval of existence as a function of the initial condition $a$. Express any angle in radians. Your final reported quantity must be the ordered pair of endpoints of this maximal interval as a closed-form analytic expression in terms of $a$.", "solution": "The problem is to solve the initial value problem (IVP) given by the differential equation:\n$$ \\frac{dx}{dt} = x^{2} + 1 $$\nwith the initial condition $x(0) = a$.\n\nFirst, we derive the explicit form of the trajectory $x(t)$. The equation is a first-order autonomous ODE and is separable. We note that the right-hand side, $x^2 + 1$, is strictly positive for all real values of $x$. This implies that there are no equilibrium points, and any solution $x(t)$ must be strictly monotonic. Since $\\dot{x} = x^2 + 1 > 0$, the solution $x(t)$ is strictly increasing for all $t$ in its domain of existence.\n\nWe can separate the variables $x$ and $t$:\n$$ \\frac{dx}{x^{2} + 1} = dt $$\nTo find the particular solution satisfying the initial condition $x(0) = a$, we integrate both sides from the initial state $(t,x) = (0,a)$ to a general state $(t, x(t))$:\n$$ \\int_{a}^{x(t)} \\frac{d\\xi}{\\xi^{2} + 1} = \\int_{0}^{t} d\\tau $$\nThe integral on the left is the standard form for the arctangent function, and the integral on the right is trivial. Performing the integration yields:\n$$ \\left[ \\arctan(\\xi) \\right]_{a}^{x(t)} = \\left[ \\tau \\right]_{0}^{t} $$\n$$ \\arctan(x(t)) - \\arctan(a) = t - 0 $$\nRearranging for $\\arctan(x(t))$ gives the implicit solution:\n$$ \\arctan(x(t)) = t + \\arctan(a) $$\nTo obtain the explicit form of the trajectory $x(t)$, we take the tangent of both sides of the equation. Note that all angles are in radians.\n$$ x(t) = \\tan(t + \\arctan(a)) $$\nThis is the explicit solution to the IVP. We can verify the initial condition: $x(0) = \\tan(0 + \\arctan(a)) = \\tan(\\arctan(a)) = a$.\n\nNext, we determine the maximal open interval of existence and justify the finite-time blowup from first principles. The explicit solution $x(t) = \\tan(t + \\arctan(a))$ involves the tangent function. The tangent function $\\tan(\\theta)$ is defined and continuous for arguments $\\theta$ that are not of the form $\\frac{\\pi}{2} + k\\pi$ for any integer $k$. A solution to an ODE must be continuous. The trajectory $x(t)$ starting at $t=0$ must therefore exist on a single continuous branch of the tangent function.\n\nAt the initial time $t=0$, the argument of the tangent function is $\\theta_0 = 0 + \\arctan(a) = \\arctan(a)$. The principal value of the arctangent function has a range of $(-\\frac{\\pi}{2}, \\frac{\\pi}{2})$. Thus, the initial argument $\\arctan(a)$ lies strictly within this interval.\n$$ -\\frac{\\pi}{2}  \\arctan(a)  \\frac{\\pi}{2} $$\nFor the solution $x(t)$ to be continuous, its argument, $t + \\arctan(a)$, must remain within this same open interval, $(-\\frac{\\pi}{2}, \\frac{\\pi}{2})$. This provides the following inequality for the time variable $t$:\n$$ -\\frac{\\pi}{2}  t + \\arctan(a)  \\frac{\\pi}{2} $$\nSolving this inequality for $t$ gives the bounds on its domain:\n$$ -\\frac{\\pi}{2} - \\arctan(a)  t  \\frac{\\pi}{2} - \\arctan(a) $$\nThis defines the maximal open interval of existence for the solution $x(t)$, which we denote as $I_{max}$:\n$$ I_{max} = \\left( -\\frac{\\pi}{2} - \\arctan(a), \\frac{\\pi}{2} - \\arctan(a) \\right) $$\nLet the endpoints of this interval be $t_{min} = -\\frac{\\pi}{2} - \\arctan(a)$ and $t_{max} = \\frac{\\pi}{2} - \\arctan(a)$.\n\nNow we justify, \"from first principles,\" that the solution blows up in finite time in both forward and backward directions. This justification stems directly from the behavior of the derived explicit solution at the endpoints of its maximal interval of existence.\n\nFor forward time, we examine the limit of $x(t)$ as $t$ approaches $t_{max}$ from below:\n$$ \\lim_{t \\to t_{max}^{-}} x(t) = \\lim_{t \\to \\left(\\frac{\\pi}{2} - \\arctan(a)\\right)^{-}} \\tan(t + \\arctan(a)) $$\nAs $t \\to (\\frac{\\pi}{2} - \\arctan(a))^{-}$, the argument of the tangent function, $t + \\arctan(a)$, approaches $\\frac{\\pi}{2}$ from the left side. The limit of the tangent function as its argument approaches $\\frac{\\pi}{2}$ from the left is $+\\infty$.\n$$ \\lim_{t \\to t_{max}^{-}} x(t) = +\\infty $$\nThis shows that the solution \"blows up\" (escapes to positive infinity) at the finite forward time $t_{max}$.\n\nFor backward time, we examine the limit of $x(t)$ as $t$ approaches $t_{min}$ from above:\n$$ \\lim_{t \\to t_{min}^{+}} x(t) = \\lim_{t \\to \\left(-\\frac{\\pi}{2} - \\arctan(a)\\right)^{+}} \\tan(t + \\arctan(a)) $$\nAs $t \\to (-\\frac{\\pi}{2} - \\arctan(a))^{+}$, the argument of the tangent function, $t + \\arctan(a)$, approaches $-\\frac{\\pi}{2}$ from the right side. The limit of the tangent function as its argument approaches $-\\frac{\\pi}{2}$ from the right is $-\\infty$.\n$$ \\lim_{t \\to t_{min}^{+}} x(t) = -\\infty $$\nThis shows that the solution \"blows up\" (escapes to negative infinity) at the finite backward time $t_{min}$.\n\nAccording to the theory of ODEs, a solution defined on a maximal interval $(t_{min}, t_{max})$ must have the property that the trajectory leaves any compact subset of its domain as $t \\to t_{min}$ or $t \\to t_{max}$. Since the domain is $\\mathbb{R}$, this means that if $t_{max}$ is finite, $|x(t)| \\to \\infty$ as $t \\to t_{max}$, and similarly for $t_{min}$. Our explicit calculation confirms this behavior. The solution cannot be extended beyond this interval because it becomes undefined at the endpoints. Thus, the interval we found is indeed the maximal interval of existence.\n\nThe problem asks for the ordered pair of endpoints of this maximal interval. The endpoints are $t_{min} = -\\frac{\\pi}{2} - \\arctan(a)$ and $t_{max} = \\frac{\\pi}{2} - \\arctan(a)$. The ordered pair is $(t_{min}, t_{max})$.", "answer": "$$\n\\boxed{\\left(-\\frac{\\pi}{2} - \\arctan(a), \\frac{\\pi}{2} - \\arctan(a)\\right)}\n$$", "id": "3078146"}, {"introduction": "Having explored the dynamics of individual flows, we now turn to the geometric interplay between two distinct vector fields. The Lie bracket serves as a powerful differential operator that quantifies the failure of infinitesimal motions to commute. This exercise will guide you through the computation of a Lie bracket for two specific vector fields and, crucially, connect this algebraic result to its geometric meaning—the net displacement resulting from an infinitesimal commutator loop [@problem_id:3078163].", "problem": "Let $U \\subset \\mathbb{R}^{2}$ be an open set and let a smooth vector field $X$ on $U$ act on a smooth function $f \\in C^{\\infty}(U)$ by the directional derivative $X(f) = X^{1} \\partial_{x} f + X^{2} \\partial_{y} f$, where $X = (X^{1}, X^{2})$ and $\\partial_{x}$, $\\partial_{y}$ denote the standard partial derivatives. The Lie bracket of two smooth vector fields $X$ and $Y$ is defined by the commutator of derivations on smooth functions, namely $[X,Y](f) = X(Y(f)) - Y(X(f))$. A smooth vector field $X$ on $U$ generates a local flow $\\Phi_{X}^{t}$ via the ordinary differential equation (ODE) $d\\gamma/dt = X(\\gamma(t))$ with initial condition $\\gamma(0) = p \\in U$, so that $\\gamma(t) = \\Phi_{X}^{t}(p)$.\n\nConsider the polynomial vector fields $X$ and $Y$ on $\\mathbb{R}^{2}$ given by\n$$\nX(x,y) = \\big(x^{2} + y,\\; x y^{2}\\big), \\qquad Y(x,y) = \\big(x y,\\; y^{2} + x\\big).\n$$\nStarting only from the above fundamental definitions, compute the Lie bracket vector field $[X,Y]$ by determining its action on the coordinate functions and extracting its components. Then interpret the nonzero bracket in terms of noncommuting infinitesimal motions: for a small parameter $\\varepsilon$, explain how the leading-order net displacement produced by the commutator loop $\\Phi_{Y}^{-\\varepsilon} \\circ \\Phi_{X}^{-\\varepsilon} \\circ \\Phi_{Y}^{\\varepsilon} \\circ \\Phi_{X}^{\\varepsilon}$ at a point $p$ is related to $[X,Y](p)$. Your final answer must be the explicit analytic expression for $[X,Y](x,y)$ as a two-component vector field. No rounding is required.", "solution": "We are given smooth polynomial vector fields $X$ and $Y$ on $\\mathbb{R}^{2}$, and the Lie bracket $[X,Y]$ is defined on smooth functions $f$ by $[X,Y](f) = X(Y(f)) - Y(X(f))$. To compute the components of the bracket as a vector field, it suffices to evaluate $[X,Y]$ on the coordinate functions $f(x,y) = x$ and $g(x,y) = y$, because for a vector field $V = (V^{1}, V^{2})$ acting as $V(h) = V^{1} \\partial_{x} h + V^{2} \\partial_{y} h$, we have $V(x) = V^{1}$ and $V(y) = V^{2}$.\n\nFirst, write the components:\n$$\nX(x,y) = \\big(P,Q\\big) = \\big(x^{2} + y,\\; x y^{2}\\big), \\qquad Y(x,y) = \\big(R,S\\big) = \\big(x y,\\; y^{2} + x\\big).\n$$\nBy definition of the action on functions,\n$$\nX(f) = P \\,\\partial_{x} f + Q \\,\\partial_{y} f, \\qquad Y(f) = R \\,\\partial_{x} f + S \\,\\partial_{y} f.\n$$\n\nCompute the first component $[X,Y](x)$:\n- Evaluate $Y(x)$:\n$$\nY(x) = R \\,\\partial_{x} x + S \\,\\partial_{y} x = R \\cdot 1 + S \\cdot 0 = R = x y.\n$$\n- Then $X(Y(x)) = X(x y)$:\n$$\nX(x y) = P \\,\\partial_{x}(x y) + Q \\,\\partial_{y}(x y) = P \\cdot y + Q \\cdot x = (x^{2} + y) y + (x y^{2}) x = x^{2} y + y^{2} + x^{2} y^{2}.\n$$\n- Next compute $X(x) = P = x^{2} + y$, so\n$$\nY(X(x)) = Y(P) = R \\,\\partial_{x} P + S \\,\\partial_{y} P.\n$$\nWe have $\\partial_{x} P = 2 x$ and $\\partial_{y} P = 1$, hence\n$$\nY(P) = R \\cdot 2 x + S \\cdot 1 = 2 x (x y) + (y^{2} + x) = 2 x^{2} y + y^{2} + x.\n$$\nTherefore,\n$$\n[X,Y](x) = X(Y(x)) - Y(X(x)) = \\big(x^{2} y + y^{2} + x^{2} y^{2}\\big) - \\big(2 x^{2} y + y^{2} + x\\big) = x^{2} y^{2} - x^{2} y - x.\n$$\n\nCompute the second component $[X,Y](y)$:\n- Evaluate $Y(y)$:\n$$\nY(y) = R \\,\\partial_{x} y + S \\,\\partial_{y} y = R \\cdot 0 + S \\cdot 1 = S = y^{2} + x.\n$$\n- Then $X(Y(y)) = X(y^{2} + x)$:\n$$\nX(y^{2} + x) = P \\,\\partial_{x}(y^{2} + x) + Q \\,\\partial_{y}(y^{2} + x) = P \\cdot 1 + Q \\cdot 2 y = (x^{2} + y) + 2 x y^{3} = x^{2} + y + 2 x y^{3}.\n$$\n- Next compute $X(y) = Q = x y^{2}$, so\n$$\nY(X(y)) = Y(Q) = R \\,\\partial_{x} Q + S \\,\\partial_{y} Q.\n$$\nWe have $\\partial_{x} Q = y^{2}$ and $\\partial_{y} Q = 2 x y$, hence\n$$\nY(Q) = R \\cdot y^{2} + S \\cdot 2 x y = (x y) y^{2} + (y^{2} + x) 2 x y = x y^{3} + 2 x y^{3} + 2 x^{2} y = 3 x y^{3} + 2 x^{2} y.\n$$\nTherefore,\n$$\n[X,Y](y) = X(Y(y)) - Y(X(y)) = \\big(x^{2} + y + 2 x y^{3}\\big) - \\big(3 x y^{3} + 2 x^{2} y\\big) = x^{2} + y - x y^{3} - 2 x^{2} y.\n$$\n\nCombining the two components, we obtain the Lie bracket vector field\n$$\n[X,Y](x,y) = \\big(x^{2} y^{2} - x^{2} y - x,\\; x^{2} + y - x y^{3} - 2 x^{2} y\\big).\n$$\n\nInterpretation in terms of noncommuting infinitesimal motions: The local flows $\\Phi_{X}^{t}$ and $\\Phi_{Y}^{t}$ associated to $X$ and $Y$ solve the ordinary differential equations $d\\gamma_{X}/dt = X(\\gamma_{X}(t))$ and $d\\gamma_{Y}/dt = Y(\\gamma_{Y}(t))$ with given initial point $p$. For sufficiently small $\\varepsilon$, one can expand the flow near $t=0$ using the Taylor expansion of solutions to ODEs:\n$$\n\\Phi_{X}^{\\varepsilon}(p) = p + \\varepsilon X(p) + \\tfrac{1}{2} \\varepsilon^{2} D X(p)\\,X(p) + \\mathcal{O}(\\varepsilon^{3}),\n$$\nand similarly for $\\Phi_{Y}^{\\varepsilon}$. Composing the flows in the commutator loop $\\Phi_{Y}^{-\\varepsilon} \\circ \\Phi_{X}^{-\\varepsilon} \\circ \\Phi_{Y}^{\\varepsilon} \\circ \\Phi_{X}^{\\varepsilon}$ and retaining terms up to order $\\varepsilon^{2}$ yields the leading-order net displacement\n$$\n\\Delta(p) = \\varepsilon^{2} [X,Y](p) + \\mathcal{O}(\\varepsilon^{3}).\n$$\nThus, a nonzero Lie bracket $[X,Y](p)$ quantifies the failure of the infinitesimal motions generated by $X$ and $Y$ to commute: traversing a small loop composed of flows of $X$ and $Y$ results in a second-order displacement proportional to $[X,Y](p)$.\n\nTherefore, the explicit analytic expression for the Lie bracket vector field for the given $X$ and $Y$ is as computed above.", "answer": "$$\\boxed{\\begin{pmatrix}x^{2} y^{2} - x^{2} y - x  x^{2} + y - x y^{3} - 2 x^{2} y\\end{pmatrix}}$$", "id": "3078163"}]}