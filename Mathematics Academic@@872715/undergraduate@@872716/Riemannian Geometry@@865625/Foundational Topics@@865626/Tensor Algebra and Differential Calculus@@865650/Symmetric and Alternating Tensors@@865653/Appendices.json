{"hands_on_practices": [{"introduction": "The abstract definition of a symmetric tensor is that its value is unchanged when its arguments are swapped. This hands-on practice provides a concrete link between this abstract property and the more familiar concept of a symmetric matrix. By starting from the matrix components of a symmetric bilinear form, you will derive its coordinate expression and verify its symmetry, reinforcing the fundamental definition of bilinearity [@problem_id:3064500].", "problem": "Let $V=\\mathbb{R}^{2}$ with the standard basis $\\{e_{1},e_{2}\\}$. Consider the bilinear form $B:V\\times V\\to \\mathbb{R}$ whose matrix relative to this basis is the symmetric matrix\n$$\n\\begin{pmatrix}\n2 & 1 \\\\\n1 & 3\n\\end{pmatrix}.\n$$\nLet $u=(a,b)$ and $v=(c,d)$ denote arbitrary vectors in $V$ written in the standard coordinates, so that $u=a\\,e_{1}+b\\,e_{2}$ and $v=c\\,e_{1}+d\\,e_{2}$. Using only the foundational definitions that a bilinear form is linear in each argument and that the matrix entries $B_{ij}$ record the values $B(e_{i},e_{j})$ relative to a basis, compute an explicit expression for $B(u,v)$ in terms of $a$, $b$, $c$, and $d$. Then, starting from these definitions, verify in coordinates that $B(u,v)=B(v,u)$.\n\nProvide your final answer as the single closed-form expression for $B(u,v)$ in terms of $a$, $b$, $c$, and $d$ (no units and no numeric rounding are required).", "solution": "The problem is well-posed and mathematically sound, containing all necessary information for a unique solution. We proceed with the derivation.\n\nLet $V = \\mathbb{R}^2$ be a real vector space with the standard basis $\\{e_1, e_2\\}$. We are given two vectors $u, v \\in V$ with coordinate representations $u = a e_1 + b e_2$ and $v = c e_1 + d e_2$ for some scalars $a, b, c, d \\in \\mathbb{R}$.\n\nWe are also given a bilinear form $B: V \\times V \\to \\mathbb{R}$. The matrix representation of $B$ with respect to the basis $\\{e_1, e_2\\}$ is given as:\n$$\n[B] = \\begin{pmatrix} 2 & 1 \\\\ 1 & 3 \\end{pmatrix}\n$$\nBy the foundational definition of the matrix of a bilinear form, the entries $B_{ij}$ are the values of the form on the basis vectors: $B_{ij} = B(e_i, e_j)$. Therefore, we have:\n$B(e_1, e_1) = 2$\n$B(e_1, e_2) = 1$\n$B(e_2, e_1) = 1$\n$B(e_2, e_2) = 3$\n\nThe problem requires the computation of $B(u, v)$ using only the definition that $B$ is linear in each of its arguments.\n\nFirst, we express $B(u, v)$ using the coordinate representations of $u$ and $v$:\n$$\nB(u, v) = B(a e_1 + b e_2, c e_1 + d e_2)\n$$\nUsing the linearity of $B$ in the first argument, we can expand this expression:\n$$\nB(a e_1 + b e_2, c e_1 + d e_2) = B(a e_1, c e_1 + d e_2) + B(b e_2, c e_1 + d e_2)\n$$\nApplying the scalar multiplication property of linearity in the first argument, we get:\n$$\n= a B(e_1, c e_1 + d e_2) + b B(e_2, c e_1 + d e_2)\n$$\nNext, we apply the linearity of $B$ in the second argument to each of the two terms:\n$$\na B(e_1, c e_1 + d e_2) = a \\left( B(e_1, c e_1) + B(e_1, d e_2) \\right) = a \\left( c B(e_1, e_1) + d B(e_1, e_2) \\right)\n$$\n$$\nb B(e_2, c e_1 + d e_2) = b \\left( B(e_2, c e_1) + B(e_2, d e_2) \\right) = b \\left( c B(e_2, e_1) + d B(e_2, e_2) \\right)\n$$\nCombining these expanded terms, we obtain the full expansion of $B(u, v)$:\n$$\nB(u, v) = a c B(e_1, e_1) + a d B(e_1, e_2) + b c B(e_2, e_1) + b d B(e_2, e_2)\n$$\nNow, we substitute the given values for $B(e_i, e_j)$ from the matrix $[B]$:\n$$\nB(u, v) = a c (2) + a d (1) + b c (1) + b d (3)\n$$\nSimplifying this expression yields the explicit formula for $B(u, v)$ in terms of the coordinates $a, b, c, d$:\n$$\nB(u, v) = 2ac + ad + bc + 3bd\n$$\nThis completes the first part of the problem.\n\nFor the second part, we must verify that $B(u, v) = B(v, u)$ using the same foundational definitions. We start by computing $B(v, u)$. The vectors are $v = c e_1 + d e_2$ and $u = a e_1 + b e_2$.\n$$\nB(v, u) = B(c e_1 + d e_2, a e_1 + b e_2)\n$$\nFollowing the same procedure of applying bilinearity, we expand the expression:\n$$\nB(v, u) = c B(e_1, a e_1 + b e_2) + d B(e_2, a e_1 + b e_2)\n$$\n$$\n= c \\left( a B(e_1, e_1) + b B(e_1, e_2) \\right) + d \\left( a B(e_2, e_1) + b B(e_2, e_2) \\right)\n$$\n$$\nB(v, u) = c a B(e_1, e_1) + c b B(e_1, e_2) + d a B(e_2, e_1) + d b B(e_2, e_2)\n$$\nNow, we substitute the values for $B(e_i, e_j)$:\n$$\nB(v, u) = c a (2) + c b (1) + d a (1) + d b (3)\n$$\n$$\nB(v, u) = 2ca + cb + da + 3db\n$$\nSince multiplication of real numbers is commutative ($ac = ca$, $ad = da$, $bc = cb$, $bd = db$), we can rearrange the terms to match the expression for $B(u, v)$:\n$$\nB(v, u) = 2ac + bc + ad + 3bd = 2ac + ad + bc + 3bd\n$$\nComparing this with our result for $B(u, v)$, we see that:\n$$\nB(u, v) = 2ac + ad + bc + 3bd = B(v, u)\n$$\nThe verification is complete. The symmetry of the bilinear form, $B(u, v) = B(v, u)$, is a direct consequence of the symmetry of its matrix representation with respect to the chosen basis, i.e., $B(e_i, e_j) = B_{ij} = B_{ji} = B(e_j, e_i)$. In this specific case, the fact that $B(e_1, e_2) = 1$ and $B(e_2, e_1) = 1$ ensures the symmetry.\nThe final required answer is the explicit expression for $B(u, v)$.", "answer": "$$\n\\boxed{2ac + ad + bc + 3bd}\n$$", "id": "3064500"}, {"introduction": "A cornerstone of tensor algebra is the ability to decompose any tensor into parts with specific symmetries. This exercise guides you through the process of splitting a general rank-2 tensor into its unique symmetric and alternating components [@problem_id:3066950]. Furthermore, by examining how these components behave under a coordinate rotation, you will verify a crucial concept: symmetry and alternation are intrinsic, geometric properties of a tensor, not mere artifacts of the basis in which it is expressed.", "problem": "Let $(V,g)$ be a $3$-dimensional real inner product space modeling a tangent space of a Riemannian manifold at a point, with metric components $g_{ij}=\\delta_{ij}$ in an orthonormal basis $\\{e_{1},e_{2},e_{3}\\}$. Consider a $(0,2)$-tensor $B$ with components in this basis given by the matrix\n$$\n\\big(B_{ij}\\big)=\\begin{pmatrix}\n2 & 1 & -1\\\\\n3 & 0 & 4\\\\\n1 & -4 & 5\n\\end{pmatrix}.\n$$\nPerform the following tasks using only foundational definitions of symmetry/alternation for tensors and the definition of how tensor components transform under a change of basis.\n\n1) Compute the symmetric and alternating parts $B_{(ij)}$ and $B_{[ij]}$ of $B$ in the given orthonormal basis.\n\n2) Let $\\{e'_{1},e'_{2},e'_{3}\\}$ be another orthonormal basis obtained by rotating $\\{e_{1},e_{2},e_{3}\\}$ about $e_{3}$ by angle $\\theta=\\pi/3$. Denote the associated rotation matrix by\n$$\nR=\\begin{pmatrix}\n\\cos(\\pi/3) & -\\sin(\\pi/3) & 0\\\\\n\\sin(\\pi/3) & \\cos(\\pi/3) & 0\\\\\n0 & 0 & 1\n\\end{pmatrix}.\n$$\nUsing the definition of a $(0,2)$-tensor and the transformation of basis vectors, derive how the components of $B_{(ij)}$ and $B_{[ij]}$ transform from the unprimed to the primed basis, and explain why symmetry and alternation are preserved under this change of basis.\n\n3) With respect to $g$, define the squared norm of the alternating part by\n$$\n\\|B_{[\\,\\cdot\\,\\cdot\\,]}\\|^{2}:=g^{ik}g^{jl}B_{[ij]}B_{[kl]}.\n$$\nCompute this quantity using the components found in part (1). Give your final answer as a single exact integer with no units.", "solution": "The problem is assessed to be valid as it is mathematically well-defined, self-contained, and grounded in the standard principles of tensor algebra on an inner product space. All provided information is consistent and sufficient to arrive at a unique solution.\n\nThe problem is addressed in three parts as requested.\n\n1) Computation of symmetric and alternating parts of $B$.\n\nA $(0,2)$-tensor $B$ with components $B_{ij}$ can be decomposed into its symmetric part, denoted $B_{(ij)}$, and its alternating (or anti-symmetric) part, denoted $B_{[ij]}$. The definitions for these components are:\n$$\nB_{(ij)} = \\frac{1}{2}(B_{ij} + B_{ji})\n$$\n$$\nB_{[ij]} = \\frac{1}{2}(B_{ij} - B_{ji})\n$$\nThe given components of $B$ in the orthonormal basis $\\{e_1, e_2, e_3\\}$ form the matrix:\n$$\n(B_{ij}) = \\begin{pmatrix}\n2 & 1 & -1 \\\\\n3 & 0 & 4 \\\\\n1 & -4 & 5\n\\end{pmatrix}\n$$\nThe transpose of this matrix gives the components $B_{ji}$:\n$$\n(B_{ji}) = \\begin{pmatrix}\n2 & 3 & 1 \\\\\n1 & 0 & -4 \\\\\n-1 & 4 & 5\n\\end{pmatrix}\n$$\nNow, we compute the component matrices for the symmetric and alternating parts.\n\nFor the symmetric part $B_{(ij)}$:\n$$\n(B_{(ij)}) = \\frac{1}{2} \\left[ \\begin{pmatrix}\n2 & 1 & -1 \\\\\n3 & 0 & 4 \\\\\n1 & -4 & 5\n\\end{pmatrix} + \\begin{pmatrix}\n2 & 3 & 1 \\\\\n1 & 0 & -4 \\\\\n-1 & 4 & 5\n\\end{pmatrix} \\right] = \\frac{1}{2} \\begin{pmatrix}\n2+2 & 1+3 & -1+1 \\\\\n3+1 & 0+0 & 4-4 \\\\\n1-1 & -4+4 & 5+5\n\\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix}\n4 & 4 & 0 \\\\\n4 & 0 & 0 \\\\\n0 & 0 & 10\n\\end{pmatrix}\n$$\n$$\n(B_{(ij)}) = \\begin{pmatrix}\n2 & 2 & 0 \\\\\n2 & 0 & 0 \\\\\n0 & 0 & 5\n\\end{pmatrix}\n$$\nFor the alternating part $B_{[ij]}$:\n$$\n(B_{[ij]}) = \\frac{1}{2} \\left[ \\begin{pmatrix}\n2 & 1 & -1 \\\\\n3 & 0 & 4 \\\\\n1 & -4 & 5\n\\end{pmatrix} - \\begin{pmatrix}\n2 & 3 & 1 \\\\\n1 & 0 & -4 \\\\\n-1 & 4 & 5\n\\end{pmatrix} \\right] = \\frac{1}{2} \\begin{pmatrix}\n2-2 & 1-3 & -1-1 \\\\\n3-1 & 0-0 & 4-(-4) \\\\\n1-(-1) & -4-4 & 5-5\n\\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix}\n0 & -2 & -2 \\\\\n2 & 0 & 8 \\\\\n2 & -8 & 0\n\\end{pmatrix}\n$$\n$$\n(B_{[ij]}) = \\begin{pmatrix}\n0 & -1 & -1 \\\\\n1 & 0 & 4 \\\\\n1 & -4 & 0\n\\end{pmatrix}\n$$\n\n2) Transformation of components and preservation of symmetry/alternation.\n\nLet $\\{e_i\\}$ be the original orthonormal basis and $\\{e'_j\\}$ be the new orthonormal basis. The transformation relating them is given by the rotation matrix $R$. The new basis vectors are expressed in terms of the old basis vectors as:\n$$\ne'_j = \\sum_{i=1}^3 R^i_{\\;j} e_i\n$$\nwhere $R^i_{\\;j}$ are the elements of the matrix $R$. A tensor is a geometric object, and its value is independent of the basis used to express it. For a $(0,2)$-tensor $B$ and two vectors $v, w \\in V$, we have $B(v,w)$. Let the components of the vectors be $v^i$ and $w^j$ in the $\\{e_i\\}$ basis and $v'^k$ and $w'^l$ in the $\\{e'_j\\}$ basis. The vectors themselves are invariant: $v = v^i e_i = v'^j e'_j$. This implies a transformation law for the components: $v^i = \\sum_j R^i_{\\;j} v'^j$.\n\nThe value of the tensor acting on the vectors is also invariant:\n$$\nB(v,w) = B_{ij} v^i w^j = B'_{kl} v'^k w'^l\n$$\nSubstituting the transformation for the vector components into the expression in the unprimed basis:\n$$\nB_{ij} \\left(\\sum_k R^i_{\\;k} v'^k\\right) \\left(\\sum_l R^j_{\\;l} w'^l\\right) = \\sum_{k,l} \\left(\\sum_{i,j} B_{ij} R^i_{\\;k} R^j_{\\;l}\\right) v'^k w'^l\n$$\nComparing this with $B'_{kl} v'^k w'^l$, we deduce the transformation law for the components of a $(0,2)$-tensor:\n$$\nB'_{kl} = \\sum_{i,j} R^i_{\\;k} R^j_{\\;l} B_{ij}\n$$\nThis can be written in matrix notation. Let $B_{comp}$ and $B'_{comp}$ be the matrices of components $(B_{ij})$ and $(B'_{kl})$ respectively. The transformation rule is $B'_{comp} = R^T B_{comp} R$.\n\nNow we show that symmetry and alternation are preserved under this transformation.\nLet $S$ be a symmetric $(0,2)$-tensor, with components $S_{ij} = S_{ji}$. Its components in the primed basis are $S'_{kl} = \\sum_{i,j} R^i_{\\;k} R^j_{\\;l} S_{ij}$. We check if $S'_{kl}$ is symmetric in its indices $k, l$:\n$$\nS'_{lk} = \\sum_{i,j} R^i_{\\;l} R^j_{\\;k} S_{ij}\n$$\nSince $S_{ij} = S_{ji}$, we can swap the summation indices $i$ and $j$:\n$$\nS'_{lk} = \\sum_{j,i} R^j_{\\;l} R^i_{\\;k} S_{ji}\n$$\nRenaming the dummy indices ($j \\to i$, $i \\to j$) and reordering the terms, we get:\n$$\nS'_{lk} = \\sum_{i,j} R^i_{\\;l} R^j_{\\;k} S_{ij} = \\sum_{i,j} R^j_{\\;k} R^i_{\\;l} S_{ij} = S'_{kl}\n$$\nThus, $S'$ is symmetric. Symmetry is a coordinate-independent property.\n\nLet $A$ be an alternating $(0,2)$-tensor, with components $A_{ij} = -A_{ji}$. Its transformed components are $A'_{kl} = \\sum_{i,j} R^i_{\\;k} R^j_{\\;l} A_{ij}$. We check if $A'_{kl}$ is alternating:\n$$\nA'_{lk} = \\sum_{i,j} R^i_{\\;l} R^j_{\\;k} A_{ij}\n$$\nUsing $A_{ij} = -A_{ji}$ and swapping summation indices $i, j$:\n$$\nA'_{lk} = \\sum_{j,i} R^j_{\\;l} R^i_{\\;k} A_{ji} = \\sum_{j,i} R^j_{\\;l} R^i_{\\;k} (-A_{ij})\n$$\nRenaming dummy indices ($j \\to i$, $i \\to j$) and reordering:\n$$\nA'_{lk} = -\\sum_{i,j} R^i_{\\;l} R^j_{\\;k} A_{ij} = -\\sum_{i,j} R^j_{\\;k} R^i_{\\;l} A_{ij} = -A'_{kl}\n$$\nThus, $A'$ is alternating. Alternation is also a coordinate-independent property.\nThis shows that applying the symmetrization or alternation operations commutes with a change of basis. Therefore, the symmetric part of $B$ in one basis transforms to the symmetric part of the transformed tensor $B'$ in the new basis, and similarly for the alternating part.\n\n3) Computation of the squared norm of the alternating part.\n\nThe squared norm of the alternating part $B_{[ij]}$ is defined as:\n$$\n\\|B_{[\\,\\cdot\\,\\cdot\\,]}\\|^{2} := g^{ik}g^{jl}B_{[ij]}B_{[kl]}\n$$\nThe problem states that we are in an orthonormal basis $\\{e_i\\}$, in which the metric components are $g_{ij} = \\delta_{ij}$. The matrix of components of the contravariant metric tensor, $g^{ij}$, is the inverse of the matrix of $g_{ij}$. Since the matrix $(g_{ij})$ is the identity matrix $I$, its inverse is also the identity matrix. Thus, $g^{ij} = \\delta^{ij}$, where $\\delta^{ij}$ are the components of the identity matrix (numerically same as $\\delta_{ij}$).\n\nSubstituting $g^{ik} = \\delta^{ik}$ and $g^{jl} = \\delta^{jl}$ into the norm definition:\n$$\n\\|B_{[\\,\\cdot\\,\\cdot\\,]}\\|^{2} = \\delta^{ik}\\delta^{jl}B_{[ij]}B_{[kl]}\n$$\nThe summation is over all four indices $i,j,k,l$. The Kronecker deltas simplify the sum. The term $\\delta^{ik}$ is non-zero (equal to $1$) only when $k=i$. Summing over $k$:\n$$\n\\|B_{[\\,\\cdot\\,\\cdot\\,]}\\|^{2} = \\sum_{i,j,l} \\delta^{jl}B_{[ij]}B_{[il]}\n$$\nSimilarly, the term $\\delta^{jl}$ is non-zero (equal to $1$) only when $l=j$. Summing over $l$:\n$$\n\\|B_{[\\,\\cdot\\,\\cdot\\,]}\\|^{2} = \\sum_{i,j} B_{[ij]}B_{[ij]} = \\sum_{i=1}^3 \\sum_{j=1}^3 (B_{[ij]})^2\n$$\nThis is the sum of the squares of all the components of the matrix for $B_{[ij]}$. From part (1), we have:\n$$\n(B_{[ij]}) = \\begin{pmatrix}\n0 & -1 & -1 \\\\\n1 & 0 & 4 \\\\\n1 & -4 & 0\n\\end{pmatrix}\n$$\nThe sum of the squares of these components is:\n$$\n\\|B_{[\\,\\cdot\\,\\cdot\\,]}\\|^{2} = (0)^2 + (-1)^2 + (-1)^2 + (1)^2 + (0)^2 + (4)^2 + (1)^2 + (-4)^2 + (0)^2\n$$\n$$\n\\|B_{[\\,\\cdot\\,\\cdot\\,]}\\|^{2} = 0 + 1 + 1 + 1 + 0 + 16 + 1 + 16 + 0\n$$\n$$\n\\|B_{[\\,\\cdot\\,\\cdot\\,]}\\|^{2} = 36\n$$\nThe squared norm is a scalar invariant, meaning its value does not depend on the choice of orthonormal basis. The calculation in the primed basis would yield the same result.", "answer": "$$\\boxed{36}$$", "id": "3066950"}, {"introduction": "Moving to higher-rank tensors, the relationship between symmetry and alternation becomes richer. This practice explores a rank-3 tensor that possesses a partial symmetry, meaning it is symmetric in only two of its three slots [@problem_id:3066954]. You will use the formal projection operators to compute its symmetric and alternating parts, discovering through direct calculation why its partial symmetry forces its fully alternating part to vanish entirelyâ€”a result with deep connections to the structure of tensor spaces.", "problem": "Let $(V,\\langle\\cdot,\\cdot\\rangle)$ be the real inner-product space $V=\\mathbb{R}^{2}$ with the standard Euclidean inner product, and let $\\{e_{1},e_{2}\\}$ be the standard basis with dual basis $\\{e^{1},e^{2}\\}$. Consider the covariant $3$-tensor $T \\in T^{3}(V^{*})$ defined by\n$$\nT \\;=\\; e^{1}\\otimes e^{2}\\otimes e^{1} \\;+\\; e^{2}\\otimes e^{1}\\otimes e^{1}.\n$$\nUsing only the fundamental definitions of the action of the symmetric group on tensor slots, the alternating and symmetric projections onto $\\Lambda^{3}(V^{*})$ and $S^{3}(V^{*})$, and the inner product on $T^{3}(V^{*})$ induced from $\\langle\\cdot,\\cdot\\rangle$, do the following:\n- Justify that $T$ is symmetric in its first two arguments and explain why its alternation must vanish.\n- Compute explicitly the alternating projection $\\operatorname{Alt}(T)$ and the symmetric projection $\\operatorname{Sym}(T)$.\n- Compute the squared norm $\\| \\operatorname{Sym}(T)\\|^{2}$ with respect to the induced inner product on $T^{3}(V^{*})$ determined by $\\langle\\cdot,\\cdot\\rangle$ and the orthonormal dual basis $\\{e^{1},e^{2}\\}$.\n\nProvide your final answer as the single real number equal to $\\| \\operatorname{Sym}(T)\\|^{2}$. No rounding is required.", "solution": "The problem statement is well-posed and all concepts are standard in tensor algebra. We address the tasks in order.\n\n**Symmetry of T and Vanishing Alternation**\n\nThe action of the symmetric group $S_k$ on the space of $k$-tensors $T^k(V^*)$ is defined by permuting the tensor slots. For a permutation $\\sigma \\in S_k$ and a simple tensor $f^1 \\otimes \\cdots \\otimes f^k$, the action is given by:\n$$\n\\sigma \\cdot (f^1 \\otimes \\cdots \\otimes f^k) = f^{\\sigma^{-1}(1)} \\otimes \\cdots \\otimes f^{\\sigma^{-1}(k)}\n$$\nLet $T = e^{1}\\otimes e^{2}\\otimes e^{1} + e^{2}\\otimes e^{1}\\otimes e^{1}$. To check for symmetry in the first two arguments, we apply the transposition $\\tau = (12) \\in S_3$. Note that $\\tau^{-1} = \\tau$.\n$$\n\\tau \\cdot T = \\tau \\cdot (e^{1}\\otimes e^{2}\\otimes e^{1}) + \\tau \\cdot (e^{2}\\otimes e^{1}\\otimes e^{1})\n$$\nApplying the action to each term:\n$$\n\\tau \\cdot (e^{1}\\otimes e^{2}\\otimes e^{1}) = e^{2}\\otimes e^{1}\\otimes e^{1}\n$$\n$$\n\\tau \\cdot (e^{2}\\otimes e^{1}\\otimes e^{1}) = e^{1}\\otimes e^{2}\\otimes e^{1}\n$$\nThus, $\\tau \\cdot T = (e^{2}\\otimes e^{1}\\otimes e^{1}) + (e^{1}\\otimes e^{2}\\otimes e^{1}) = T$. Since the tensor is invariant under the transposition of its first two slots, it is symmetric in its first two arguments.\n\nThe full alternation of $T$, denoted $\\operatorname{Alt}(T)$, must vanish for two independent reasons:\n1.  **Structural Reason:** The tensor lives on a 2-dimensional vector space $V=\\mathbb{R}^2$. The space of alternating 3-tensors on $V$, denoted $\\Lambda^3(V^*)$, has dimension $\\binom{\\dim V^*}{3} = \\binom{2}{3} = 0$. Since $\\operatorname{Alt}(T)$ is an element of this zero-dimensional space, it must be the zero tensor.\n2.  **Symmetry Reason:** The alternation operator has the property that $\\operatorname{Alt}(\\sigma \\cdot U) = \\operatorname{sgn}(\\sigma) \\operatorname{Alt}(U)$ for any tensor $U$ and permutation $\\sigma$. Since we showed that $\\tau \\cdot T = T$ for the transposition $\\tau=(12)$, and knowing that $\\operatorname{sgn}(\\tau) = -1$, we have:\n    $$\n    \\operatorname{Alt}(T) = \\operatorname{Alt}(\\tau \\cdot T) = \\operatorname{sgn}(\\tau)\\operatorname{Alt}(T) = -\\operatorname{Alt}(T)\n    $$\n    This implies $2\\operatorname{Alt}(T) = 0$, so $\\operatorname{Alt}(T) = 0$.\n\n**Explicit Computation of Projections**\n\nThe alternating projection is $\\operatorname{Alt}(T) = \\frac{1}{3!} \\sum_{\\sigma \\in S_3} \\operatorname{sgn}(\\sigma) (\\sigma \\cdot T)$. As proven above, this must be zero.\n\nThe symmetric projection is $\\operatorname{Sym}(T) = \\frac{1}{3!} \\sum_{\\sigma \\in S_3} \\sigma \\cdot T$. We list the action of each permutation in $S_3 = \\{e, (12), (13), (23), (123), (132)\\}$ on $T$:\n*   $e \\cdot T = T = e^{1}\\otimes e^{2}\\otimes e^{1} + e^{2}\\otimes e^{1}\\otimes e^{1}$\n*   $(12) \\cdot T = T = e^{1}\\otimes e^{2}\\otimes e^{1} + e^{2}\\otimes e^{1}\\otimes e^{1}$\n*   $(13) \\cdot T = (e^{1}\\otimes e^{2}\\otimes e^{1}) + (e^{1}\\otimes e^{1}\\otimes e^{2})$\n*   $(23) \\cdot T = (e^{1}\\otimes e^{1}\\otimes e^{2}) + (e^{2}\\otimes e^{1}\\otimes e^{1})$\n*   $(123) \\cdot T = (e^{2}\\otimes e^{1}\\otimes e^{1}) + (e^{1}\\otimes e^{1}\\otimes e^{2})$\n*   $(132) \\cdot T = (e^{1}\\otimes e^{1}\\otimes e^{2}) + (e^{1}\\otimes e^{2}\\otimes e^{1})$\n\nSumming these six tensors, we count the occurrences of each basis tensor:\n*   $e^{1}\\otimes e^{2}\\otimes e^{1}$ appears 4 times.\n*   $e^{2}\\otimes e^{1}\\otimes e^{1}$ appears 4 times.\n*   $e^{1}\\otimes e^{1}\\otimes e^{2}$ appears 4 times.\n\nThe sum is $4(e^{1}\\otimes e^{2}\\otimes e^{1} + e^{2}\\otimes e^{1}\\otimes e^{1} + e^{1}\\otimes e^{1}\\otimes e^{2})$.\nTherefore, the symmetric projection is:\n$$\n\\operatorname{Sym}(T) = \\frac{4}{6} (e^{1}\\otimes e^{2}\\otimes e^{1} + e^{2}\\otimes e^{1}\\otimes e^{1} + e^{1}\\otimes e^{1}\\otimes e^{2}) = \\frac{2}{3} (e^{1}\\otimes e^{2}\\otimes e^{1} + e^{2}\\otimes e^{1}\\otimes e^{1} + e^{1}\\otimes e^{1}\\otimes e^{2})\n$$\n\n**Computation of the Squared Norm**\n\nThe standard inner product on $V = \\mathbb{R}^2$ makes the basis $\\{e_1, e_2\\}$ orthonormal. This induces an inner product on the dual space $V^*$ for which the dual basis $\\{e^1, e^2\\}$ is orthonormal. This inner product is extended to the tensor product space $T^3(V^*)$ such that the basis of simple tensors $\\{e^{i_1} \\otimes e^{i_2} \\otimes e^{i_3} | i_j \\in \\{1,2\\}\\}$ is orthonormal.\n\nThe tensor $\\operatorname{Sym}(T)$ is a linear combination of three distinct, and therefore orthogonal, basis tensors. We compute its squared norm:\n$$\n\\| \\operatorname{Sym}(T)\\|^{2} = \\left\\| \\frac{2}{3} (e^{1}\\otimes e^{2}\\otimes e^{1} + e^{2}\\otimes e^{1}\\otimes e^{1} + e^{1}\\otimes e^{1}\\otimes e^{2}) \\right\\|^2\n$$\nBy orthogonality and the fact that each basis tensor has norm 1:\n$$\n\\| \\operatorname{Sym}(T)\\|^{2} = \\left(\\frac{2}{3}\\right)^2 \\left( \\|e^{1}\\otimes e^{2}\\otimes e^{1}\\|^2 + \\|e^{2}\\otimes e^{1}\\otimes e^{1}\\|^2 + \\|e^{1}\\otimes e^{1}\\otimes e^{2}\\|^2 \\right)\n$$\n$$\n\\| \\operatorname{Sym}(T)\\|^{2} = \\frac{4}{9} (1^2 + 1^2 + 1^2) = \\frac{4}{9} (3) = \\frac{12}{9} = \\frac{4}{3}\n$$\nThe final answer is $\\frac{4}{3}$.", "answer": "$$\n\\boxed{\\frac{4}{3}}\n$$", "id": "3066954"}]}