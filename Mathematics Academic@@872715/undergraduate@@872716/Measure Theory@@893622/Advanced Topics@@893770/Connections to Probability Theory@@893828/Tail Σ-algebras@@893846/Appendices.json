{"hands_on_practices": [{"introduction": "The first step in working with tail σ-algebras is building a solid intuition for what constitutes a tail event. This practice challenges you to classify several events based on whether their occurrence depends on the long-term behavior of a sequence or is determined by a finite number of its initial terms [@problem_id:1445799]. Mastering this distinction is crucial for knowing when powerful limit theorems, such as Kolmogorov's 0-1 Law, can be applied.", "problem": "Let $(\\Omega, \\mathcal{F}, P)$ be a probability space and let $(X_n)_{n \\ge 1}$ be a sequence of real-valued random variables defined on it. For each $n \\ge 1$, let $\\mathcal{F}_n = \\sigma(X_n, X_{n+1}, X_{n+2}, \\dots)$ denote the $\\sigma$-algebra generated by the random variables from index $n$ onwards. The tail $\\sigma$-algebra of the sequence, denoted by $\\mathcal{T}$, is defined as the intersection of these $\\sigma$-algebras:\n$$ \\mathcal{T} = \\bigcap_{n=1}^{\\infty} \\mathcal{F}_n $$\nAn event $A \\in \\mathcal{F}$ is called a tail event if $A \\in \\mathcal{T}$. Intuitively, a tail event is an event whose occurrence is determined by the \"tail\" of the sequence $(X_n)$, meaning its outcome is unaffected by changing any finite number of the random variables $X_1, X_2, \\dots, X_m$ for any finite $m$.\n\nConsider the following events. Which of these are tail events? Select all that apply.\n\nA. $A = \\left\\{ \\omega \\in \\Omega \\mid \\sum_{k=1}^{100} X_k(\\omega)  20 \\right\\}$\n\nB. $B = \\left\\{ \\omega \\in \\Omega \\mid \\limsup_{n \\to \\infty} \\frac{1}{n} \\sum_{k=1}^n X_k(\\omega) \\le 5 \\right\\}$\n\nC. $C = \\left\\{ \\omega \\in \\Omega \\mid \\text{the series } \\sum_{n=1}^\\infty X_n(\\omega) \\text{ diverges} \\right\\}$\n\nD. $D = \\left\\{ \\omega \\in \\Omega \\mid X_n(\\omega)  0 \\text{ for infinitely many } n \\right\\}$\n\nE. $E = \\left\\{ \\omega \\in \\Omega \\mid \\sup_{n \\ge 10} X_n(\\omega)  0 \\right\\}$", "solution": "We use the definition $\\mathcal{T}=\\bigcap_{n=1}^{\\infty}\\mathcal{F}_{n}$, where $\\mathcal{F}_{n}=\\sigma(X_{n},X_{n+1},\\dots)$. An event $A$ is a tail event if and only if $A\\in\\mathcal{F}_{n}$ for every $n\\geq 1$, equivalently, its membership is unaffected by changing any finite initial segment $X_{1},\\dots,X_{m}$ for any finite $m$.\n\nAnalyze each option:\n\nA. The event $A=\\{\\sum_{k=1}^{100}X_{k}20\\}$ depends only on $X_{1},\\dots,X_{100}$. It can change if one alters $X_{1}$ while keeping $\\{X_{n}\\}_{n\\geq 101}$ fixed. Formally, $A\\notin\\mathcal{F}_{101}$, since $\\mathcal{F}_{101}$ is generated by $\\{X_{n}\\}_{n\\geq 101}$ and these do not determine $\\sum_{k=1}^{100}X_{k}$. Therefore $A\\notin\\mathcal{T}$.\n\nB. Let $S_{n}=\\sum_{k=1}^{n}X_{k}$. For any fixed $m\\geq 1$,\n$$\n\\frac{1}{n}S_{n}=\\frac{1}{n}\\sum_{k=1}^{m-1}X_{k}+\\frac{1}{n}\\sum_{k=m}^{n}X_{k}.\n$$\nSince $\\frac{1}{n}\\sum_{k=1}^{m-1}X_{k}\\to 0$ as $n\\to\\infty$, we have\n$$\n\\limsup_{n\\to\\infty}\\frac{1}{n}\\sum_{k=1}^{n}X_{k}\n=\\limsup_{n\\to\\infty}\\frac{1}{n}\\sum_{k=m}^{n}X_{k}.\n$$\nThe right-hand side is measurable with respect to $\\mathcal{F}_{m}$ because it is a functional of $(X_{m},X_{m+1},\\dots)$. Therefore the event\n$$\nB=\\left\\{\\limsup_{n\\to\\infty}\\frac{1}{n}\\sum_{k=1}^{n}X_{k}\\leq 5\\right\\}\n$$\nbelongs to $\\mathcal{F}_{m}$ for every $m$, hence $B\\in\\mathcal{T}$. Intuitively, changing finitely many initial $X_{k}$ alters the averages by at most $O(1/n)$, which does not affect the limsup.\n\nC. Consider the series $\\sum_{n=1}^{\\infty}X_{n}$. For any fixed $m\\geq 1$, the series converges if and only if $\\sum_{n=m}^{\\infty}X_{n}$ converges; adding finitely many terms does not affect convergence. Hence\n$$\nC=\\left\\{\\text{the series } \\sum_{n=1}^{\\infty}X_{n} \\text{ diverges}\\right\\}\n=\\left\\{\\text{the series } \\sum_{n=m}^{\\infty}X_{n} \\text{ diverges}\\right\\}\n$$\nfor every $m$. The right-hand event is measurable with respect to $\\mathcal{F}_{m}$, so $C\\in\\mathcal{F}_{m}$ for all $m$, and therefore $C\\in\\mathcal{T}$.\n\nD. Define $E_{+}=\\{X_{n}0\\text{ i.o.}\\}$. For any fixed $m\\geq 1$,\n$$\n\\{X_{n}0 \\text{ for infinitely many } n\\}\n=\n\\{X_{n}0 \\text{ for infinitely many } n\\geq m\\},\n$$\nsince removing finitely many indices does not change the property of occurring infinitely often. The event on the right depends only on $(X_{m},X_{m+1},\\dots)$, hence is in $\\mathcal{F}_{m}$. Therefore $D\\in\\mathcal{F}_{m}$ for all $m$, so $D\\in\\mathcal{T}$.\n\nE. The event\n$$\nE=\\left\\{\\sup_{n\\geq 10}X_{n}0\\right\\}\n$$\nbelongs to $\\mathcal{F}_{10}$, but to be a tail event it must belong to every $\\mathcal{F}_{m}$. For $m\\geq 11$, $\\mathcal{F}_{m}$ is generated by $(X_{m},X_{m+1},\\dots)$; however, $E$ can depend on $X_{10}$ (for instance, if $X_{10}0$ and $X_{n}\\leq 0$ for all $n\\geq 11$, then $E$ occurs but cannot be determined from $(X_{n})_{n\\geq 11}$). Thus $E\\notin\\mathcal{F}_{11}$, so $E\\notin\\mathcal{T}$.\n\nConclusion: The tail events among the options are $B$, $C$, and $D$.", "answer": "$$\\boxed{BCD}$$", "id": "1445799"}, {"introduction": "Having practiced identifying tail events, we now explore a deeper structural property. This problem investigates a fundamental question: can a random variable determined by the *start* of an independent sequence, like $X_1$, also be determined by its *tail*? The solution reveals a profound consequence of independence, demonstrating that such a variable must be trivial (i.e., constant), a result that foreshadows the all-or-nothing nature of tail events [@problem_id:1445767].", "problem": "Let $(\\Omega, \\mathcal{F}, P)$ be a probability space, and let $(X_n)_{n \\ge 1}$ be a sequence of independent and identically distributed (i.i.d.) random variables defined on this space. For each $k \\ge 1$, let $\\mathcal{G}_k = \\sigma(X_k, X_{k+1}, \\dots)$ denote the $\\sigma$-algebra generated by the sequence of random variables starting from $X_k$. The tail $\\sigma$-algebra, denoted by $\\mathcal{T}$, is defined as the intersection of all such $\\sigma$-algebras: $\\mathcal{T} = \\bigcap_{k=1}^{\\infty} \\mathcal{G}_k$.\n\nWhich of the following statements provides the most complete and accurate description regarding the measurability of the first random variable, $X_1$, with respect to the tail $\\sigma$-algebra $\\mathcal{T}$?\n\nA. $X_1$ is always measurable with respect to $\\mathcal{T}$ because the definition of $\\mathcal{T}$ involves the $\\sigma$-algebra $\\mathcal{G}_1$ which is generated in part by $X_1$.\n\nB. $X_1$ is never measurable with respect to $\\mathcal{T}$ because for any $k \\ge 2$, $X_1$ is by definition excluded from the events that generate $\\mathcal{G}_k$.\n\nC. $X_1$ is measurable with respect to $\\mathcal{T}$ if and only if $X_1$ is a constant random variable (almost surely).\n\nD. $X_1$ is measurable with respect to $\\mathcal{T}$ if the sequence $(X_n)$ converges to a constant almost surely.\n\nE. Any event determined solely by $X_1$, such as $\\{X_1  0\\}$, must be an element of $\\mathcal{T}$.", "solution": "We are given a probability space and an i.i.d. sequence $(X_{n})_{n \\geq 1}$ with tail $\\sigma$-algebra $\\mathcal{T} = \\bigcap_{k=1}^{\\infty} \\mathcal{G}_{k}$, where $\\mathcal{G}_{k} = \\sigma(X_{k}, X_{k+1}, \\dots)$. We analyze when $X_{1}$ is $\\mathcal{T}$-measurable.\n\nFirst, for each $n \\in \\mathbb{N}$, independence of the sequence implies that the finite $\\sigma$-algebra $\\sigma(X_{1}, \\dots, X_{n})$ is independent of $\\mathcal{G}_{n+1}$. Since $\\mathcal{T} \\subset \\mathcal{G}_{n+1}$ for every $n$, it follows that $\\sigma(X_{1}, \\dots, X_{n})$ is independent of $\\mathcal{T}$. In particular,\n$$\n\\sigma(X_{1}) \\text{ is independent of } \\mathcal{T}.\n$$\n\nSuppose that $X_{1}$ is $\\mathcal{T}$-measurable. Then $\\sigma(X_{1}) \\subset \\mathcal{T}$, hence $\\sigma(X_{1})$ is independent of itself. Therefore, for every Borel set $B$,\n$$\nP(X_{1} \\in B) = P(\\{X_{1} \\in B\\} \\cap \\{X_{1} \\in B\\}) = P(X_{1} \\in B)^{2}.\n$$\nThus $P(X_{1} \\in B) \\in \\{0,1\\}$ for all Borel $B$. The only random variables whose distributions assign probabilities $0$ or $1$ to all Borel sets are almost surely constant random variables. Hence, if $X_{1}$ is $\\mathcal{T}$-measurable, then there exists $c$ such that $P(X_{1} = c) = 1$.\n\nConversely, if $X_{1}$ is almost surely constant, say $X_{1} = c$ almost surely, then for any Borel set $B$ the event $\\{X_{1} \\in B\\}$ is either $\\Omega$ or the empty set, both of which belong to every $\\sigma$-algebra. Hence $\\sigma(X_{1}) \\subset \\mathcal{T}$, and $X_{1}$ is $\\mathcal{T}$-measurable.\n\nTherefore, $X_{1}$ is $\\mathcal{T}$-measurable if and only if it is almost surely constant. This validates option C and rules out A, B, D, and E for the noted reasons: A ignores the intersection nature of $\\mathcal{T}$, B ignores the degenerate case, D does not imply $X_{1}$ equals the limit, and E falsely asserts tail measurability for arbitrary $X_{1}$-events.", "answer": "$$\\boxed{C}$$", "id": "1445767"}, {"introduction": "Now we apply our understanding to a concrete probabilistic calculation. The event that a sequence \"stabilizes,\" or becomes constant from some point onward, is a quintessential tail event. For a sequence of independent trials, Kolmogorov's 0-1 Law tells us the probability of such an event must be either 0 or 1. This exercise guides you through a calculation using the Borel-Cantelli lemma to determine which of these two possibilities is the reality [@problem_id:1445783].", "problem": "In a simplified model of digital evolution, a lineage of an organism is represented by an infinite sequence of bits, $(X_n)_{n \\ge 1}$, where $n$ denotes the generation. The bit $X_n$ is a newly acquired genetic trait. The process of acquiring a new trait is random and memoryless: the random variables $(X_n)_{n \\ge 1}$ are independent and identically distributed (i.i.d.). For each generation, the probability of acquiring a '1' trait is $P(X_n=1)=p$, and the probability of acquiring a '0' trait is $P(X_n=0)=1-p$. The parameter $p$ is a fixed constant satisfying $0  p  1$.\n\nA lineage is said to have *stabilized* if its genetic sequence becomes constant after a finite number of generations. That is, there exists some generation $N \\ge 1$ such that all bits $X_n$ for $n \\ge N$ are identical.\n\nCalculate the probability that a lineage stabilizes.", "solution": "Let $X_{n}$ be i.i.d. with $P(X_{n}=1)=p$ and $P(X_{n}=0)=1-p$, where $0p1$. Define the stabilization event\n$$\nS=\\{\\exists N\\geq 1:\\ \\forall n\\geq N,\\ X_{n}\\ \\text{are identical}\\}.\n$$\nDecompose $S$ into the disjoint union\n$$\nS=S_{1}\\cup S_{0},\n$$\nwhere\n$$\nS_{1}=\\{\\exists N\\geq 1:\\ \\forall n\\geq N,\\ X_{n}=1\\},\\qquad S_{0}=\\{\\exists N\\geq 1:\\ \\forall n\\geq N,\\ X_{n}=0\\}.\n$$\nIf $S_{1}$ occurs, then there are only finitely many indices $n$ with $X_{n}=0$. Thus\n$$\nS_{1}=\\{\\text{only finitely many }n\\text{ with }X_{n}=0\\}.\n$$\nSimilarly,\n$$\nS_{0}=\\{\\text{only finitely many }n\\text{ with }X_{n}=1\\}.\n$$\nDefine events $Z_{n}=\\{X_{n}=0\\}$ and $O_{n}=\\{X_{n}=1\\}$. Since the $X_{n}$ are i.i.d., the events $(Z_{n})$ are independent with\n$$\n\\sum_{n=1}^{\\infty}P(Z_{n})=\\sum_{n=1}^{\\infty}(1-p)=\\infty,\n$$\nand the events $(O_{n})$ are independent with\n$$\n\\sum_{n=1}^{\\infty}P(O_{n})=\\sum_{n=1}^{\\infty}p=\\infty.\n$$\nBy the second Borel–Cantelli lemma (which applies due to independence), we have\n$$\nP(Z_{n}\\ \\text{i.o.})=1\\quad\\text{and}\\quad P(O_{n}\\ \\text{i.o.})=1,\n$$\nwhere “i.o.” denotes “infinitely often.” The event $\\{ \\text{only finitely many } Z_n \\text{ occur} \\}$ is the complement of $\\{ Z_n \\text{ i.o.} \\}$. Therefore,\n$$\nP(S_1) = P(\\text{only finitely many }Z_{n}\\text{ occur})=1 - P(Z_n \\text{ i.o.}) = 1-1=0.\n$$\nSimilarly,\n$$\nP(S_0) = P(\\text{only finitely many }O_{n}\\text{ occur})=1 - P(O_n \\text{ i.o.}) = 1-1=0.\n$$\nIt follows that\n$$\nP(S)=P(S_{1}\\cup S_{0})\\leq P(S_{1})+P(S_{0})=0+0=0.\n$$\nTherefore $P(S)=0$.", "answer": "$$\\boxed{0}$$", "id": "1445783"}]}