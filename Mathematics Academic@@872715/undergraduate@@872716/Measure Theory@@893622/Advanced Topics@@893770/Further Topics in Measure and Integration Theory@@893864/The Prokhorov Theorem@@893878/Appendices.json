{"hands_on_practices": [{"introduction": "Understanding a new mathematical concept often begins with a simple, concrete example. This first exercise provides just that for the concept of tightness. We will examine the family of all Bernoulli measures, which are among the simplest non-trivial probability distributions, and verify that this collection is tight by explicitly constructing a single compact set that uniformly captures the probability mass for every measure in the family [@problem_id:1458415].", "problem": "In measure-theoretic probability, the concept of tightness is crucial for studying the weak convergence of probability measures. A family of probability measures $\\mathcal{P} = \\{\\mu_i\\}_{i \\in I}$ on the real line $\\mathbb{R}$ is defined as tight if, for every $\\epsilon  0$, there exists a compact set $K_\\epsilon \\subseteq \\mathbb{R}$ such that $\\mu_i(K_\\epsilon) \\geq 1 - \\epsilon$ for all measures $\\mu_i$ in the family.\n\nConsider the family of Bernoulli measures $\\mathcal{P} = \\{\\mu_p\\}_{p \\in [0,1]}$ on $\\mathbb{R}$. For each $p \\in [0,1]$, the measure $\\mu_p$ is defined by its action on singleton sets as $\\mu_p(\\{1\\}) = p$ and $\\mu_p(\\{0\\}) = 1-p$, with $\\mu_p(A) = 0$ if neither $0$ nor $1$ is in the set $A$.\n\nWe are interested in demonstrating the tightness of this family using a specific form of compact set: a closed ball. A closed ball in $\\mathbb{R}$ is an interval of the form $B[c, R] = \\{x \\in \\mathbb{R} : |x-c| \\le R\\}$, which is determined by its center $c \\in \\mathbb{R}$ and its radius $R \\ge 0$.\n\nDetermine the minimum possible radius, let's call it $R_{min}$, for which there exists an optimally chosen center $c$ such that the single closed ball $K = B[c, R_{min}]$ satisfies the tightness condition $\\mu_p(K) \\geq 1 - \\epsilon$ for all $p \\in [0,1]$ and for all possible choices of $\\epsilon \\in (0, 1)$.", "solution": "We must find the smallest radius $R_{\\min}$ for which there exists a center $c$ such that a single closed ball $K=B[c,R_{\\min}]$ satisfies $\\mu_{p}(K) \\geq 1-\\epsilon$ for all $p \\in [0,1]$ and for all $\\epsilon \\in (0,1)$.\n\nFix any $p \\in [0,1]$. The requirement “for all $\\epsilon \\in (0,1)$, $\\mu_{p}(K) \\geq 1-\\epsilon$” implies $\\mu_{p}(K) \\geq \\sup_{\\epsilon \\in (0,1)}(1-\\epsilon) = 1$. Since $\\mu_{p}(K) \\leq 1$, this forces $\\mu_{p}(K)=1$ for every $p \\in [0,1]$. Therefore, a necessary and sufficient condition for the single ball $K$ to work for all $p$ and all $\\epsilon \\in (0,1)$ is that $\\mu_{p}(K)=1$ for all $p \\in [0,1]$.\n\nEach $\\mu_{p}$ is supported on $\\{0,1\\}$, with $\\mu_{p}(\\{1\\})=p$ and $\\mu_{p}(\\{0\\})=1-p$. Hence $\\mu_{p}(K)=1$ for all $p$ if and only if $K$ contains both atoms $0$ and $1$. Equivalently, $K=B[c,R]$ must satisfy\n$$\n|0-c| \\leq R \\quad \\text{and} \\quad |1-c| \\leq R,\n$$\nwhich is the same as $R \\geq \\max\\{|c|,\\;|1-c|\\}$. Minimizing the radius over $c \\in \\mathbb{R}$ gives\n$$\nR_{\\min} \\;=\\; \\inf_{c \\in \\mathbb{R}} \\max\\{|c|,\\;|1-c|\\}.\n$$\nThe function $c \\mapsto \\max\\{|c|,|1-c|\\}$ is minimized when $|c|=|1-c|$, because if one of $|c|$ or $|1-c|$ is strictly larger, shifting $c$ toward the midpoint reduces the maximum. Solving $|c|=|1-c|$ yields $c=\\frac{1}{2}$, and then\n$$\nR_{\\min} \\;=\\; \\max\\left\\{\\left|\\tfrac{1}{2}\\right|,\\;\\left|1-\\tfrac{1}{2}\\right|\\right\\} \\;=\\; \\frac{1}{2}.\n$$\nWith $c=\\frac{1}{2}$ and $R=\\frac{1}{2}$, the ball $K=[0,1]$ contains both atoms and hence satisfies $\\mu_{p}(K)=1 \\geq 1-\\epsilon$ for all $p \\in [0,1]$ and all $\\epsilon \\in (0,1)$. No smaller radius can work because any ball of radius $R\\frac{1}{2}$ has length $2R1$ and cannot contain both $0$ and $1$, which would force $\\mu_{p}(K)1$ for some $p$ and thus violate the requirement for sufficiently small $\\epsilon$.", "answer": "$$\\boxed{\\frac{1}{2}}$$", "id": "1458415"}, {"introduction": "Just as important as understanding when a property holds is understanding when it fails. This practice provides a classic counterexample to illustrate what it means for a family of measures *not* to be tight. By analyzing a sequence of uniform distributions whose supports march off to infinity, we can directly see how probability mass can \"escape\" in a way that no single compact set can control, which is a crucial intuition for applying Prokhorov's theorem [@problem_id:1458410].", "problem": "In the study of probability theory on metric spaces, the concept of tightness is crucial for characterizing the weak convergence of measures. A family of probability measures $\\{\\mu_\\alpha\\}_{\\alpha \\in I}$ on the real line $\\mathbb{R}$ (equipped with its standard Borel $\\sigma$-algebra) is said to be **tight** if for every $\\epsilon  0$, there exists a single compact set $K_\\epsilon \\subseteq \\mathbb{R}$ such that $\\mu_\\alpha(K_\\epsilon)  1 - \\epsilon$ for all measures $\\mu_\\alpha$ in the family.\n\nConsider a sequence of probability measures $(\\mu_n)_{n \\in \\mathbb{N}}$, where $\\mathbb{N} = \\{1, 2, 3, \\dots\\}$. For each $n \\in \\mathbb{N}$, the measure $\\mu_n$ is the uniform probability distribution on the closed interval $[n, n+1]$. That is, for any Borel set $A \\subseteq \\mathbb{R}$, the measure $\\mu_n(A)$ is equal to the Lebesgue measure of the intersection $A \\cap [n, n+1]$.\n\nBased on the definition provided, determine whether the family of measures $\\{\\mu_n\\}_{n \\in \\mathbb{N}}$ is tight, and select the correct reasoning.\n\nA. Yes, the family is tight because each individual measure $\\mu_n$ has a compact support, namely $[n, n+1]$.\n\nB. No, the family is not tight because for any given compact set $K$, one can always find measures $\\mu_n$ in the family whose supports are completely disjoint from $K$.\n\nC. Yes, the family is tight because the total measure of each $\\mu_n$ is 1, which is finite.\n\nD. No, the family is not tight because the union of the supports $\\bigcup_{n=1}^{\\infty} [n, n+1]$ is not a compact set.\n\nE. The tightness of the family cannot be determined because the measures are defined on unbounded intervals as $n \\to \\infty$.", "solution": "We recall the definition of tightness on $\\mathbb{R}$: a family $\\{\\mu_{\\alpha}\\}_{\\alpha \\in I}$ is tight if for every $\\epsilon  0$ there exists a compact set $K_{\\epsilon} \\subset \\mathbb{R}$ such that $\\mu_{\\alpha}(K_{\\epsilon})  1 - \\epsilon$ for all $\\alpha \\in I$.\n\nIn our case, for each $n \\in \\mathbb{N}$, $\\mu_{n}$ is the uniform probability measure on $[n, n+1]$, so for any Borel set $A \\subset \\mathbb{R}$,\n$$\n\\mu_{n}(A) = \\lambda\\big(A \\cap [n, n+1]\\big),\n$$\nwhere $\\lambda$ denotes Lebesgue measure on $\\mathbb{R}$. Since $\\lambda([n,n+1]) = 1$, each $\\mu_{n}$ is indeed a probability measure.\n\nAssume, for the sake of checking the tightness condition, that there exists a compact set $K \\subset \\mathbb{R}$. By compactness in $\\mathbb{R}$, $K$ is bounded, so there exists $M  0$ such that $K \\subset [-M, M]$. Choose any integer $n$ with $n  M$. Then $[n, n+1] \\subset (M, \\infty)$ and hence\n$$\n[n, n+1] \\cap K = \\varnothing.\n$$\nTherefore,\n$$\n\\mu_{n}(K) = \\lambda\\big(K \\cap [n, n+1]\\big) = 0.\n$$\nFor any $\\epsilon \\in (0,1)$, the tightness requirement $\\mu_{n}(K)  1 - \\epsilon$ fails for such $n$ because $0 \\not 1 - \\epsilon$. Since this happens for every compact $K$, there is no single compact set $K_{\\epsilon}$ that captures mass greater than $1 - \\epsilon$ uniformly over all $n$. Consequently, the family $\\{\\mu_{n}\\}_{n \\in \\mathbb{N}}$ is not tight.\n\nThis exactly matches reasoning option B: for any given compact set $K$, one can pick $n$ large enough so that the support $[n, n+1]$ is disjoint from $K$, yielding $\\mu_{n}(K) = 0$. Options A and C are incorrect because having compact support for each individual measure or having total mass equal to $1$ does not imply uniform tightness across the family. Option D is not a valid justification because noncompactness of the union of supports does not preclude tightness in general. Option E is incorrect because tightness is determinable and, as shown, fails.", "answer": "$$\\boxed{B}$$", "id": "1458410"}, {"introduction": "Tightness is not just an abstract definition; it is a powerful tool with deep connections to probability theory and limit theorems. This exercise moves beyond simple cases to demonstrate how to prove tightness for a sequence of measures derived from Poisson random variables, a scenario common in statistical modeling. By applying a fundamental probabilistic tool—Chebyshev's inequality—we can establish tightness by showing that the measures become increasingly concentrated, a key step in proving convergence [@problem_id:1458427].", "problem": "Consider a sequence of random variables $\\{X_n\\}_{n \\in \\mathbb{N}}$, where each $X_n$ follows a Poisson distribution with parameter $n$. The probability mass function for $X_n$ is given by $P(X_n=k) = \\frac{n^k e^{-n}}{k!}$ for $k = 0, 1, 2, \\dots$.\n\nFrom this, a new sequence of random variables $\\{Y_n\\}_{n \\in \\mathbb{N}}$ is constructed, where $Y_n = X_n / n$. Let $\\mu_n$ denote the probability distribution (also known as the law) of the random variable $Y_n$.\n\nIn measure theory, a family of probability measures $\\{\\nu_k\\}_{k \\in I}$ on the real line $\\mathbb{R}$ is called **tight** if for every $\\epsilon  0$, there exists a compact set $K \\subset \\mathbb{R}$ such that $\\nu_k(K) \\geq 1 - \\epsilon$ for all $k \\in I$. For probability measures on the real line, this condition is equivalent to the following: for every $\\epsilon  0$, there exists a real number $M  0$ such that $\\nu_k([-M, M]) \\geq 1 - \\epsilon$ for all $k \\in I$.\n\nBased on these definitions, determine whether the family of probability measures $\\{\\mu_n\\}_{n \\in \\mathbb{N}}$ is tight.\n\nChoose the correct statement from the options below.\n\nA. Yes, the family is tight.\n\nB. No, the family is not tight.\n\nC. The family is tight only for even values of $n$.\n\nD. There is not enough information to determine if the family is tight.", "solution": "We are given $X_{n} \\sim \\text{Poisson}(n)$, so by properties of the Poisson distribution,\n$$\n\\mathbb{E}[X_{n}] = n, \\qquad \\operatorname{Var}(X_{n}) = n.\n$$\nDefine $Y_{n} = X_{n}/n$. Then linearity of expectation and the variance scaling rule $\\operatorname{Var}(aZ) = a^{2}\\operatorname{Var}(Z)$ yield\n$$\n\\mathbb{E}[Y_{n}] = \\frac{\\mathbb{E}[X_{n}]}{n} = 1, \\qquad \\operatorname{Var}(Y_{n}) = \\frac{\\operatorname{Var}(X_{n})}{n^{2}} = \\frac{1}{n}.\n$$\nBy Chebyshev's inequality, for any $\\delta  0$,\n$$\n\\mathbb{P}\\big(|Y_{n} - 1| \\geq \\delta\\big) \\leq \\frac{\\operatorname{Var}(Y_{n})}{\\delta^{2}} = \\frac{1}{n \\delta^{2}}.\n$$\nFix $\\epsilon  0$ and choose $M  1$. By the triangle inequality, if $|Y_{n}|  M$, then $|Y_{n} - 1| \\geq |Y_{n}| - 1  M - 1$. Hence,\n$$\n\\mathbb{P}(|Y_{n}|  M) \\leq \\mathbb{P}(|Y_{n} - 1|  M - 1) \\leq \\frac{1}{n (M - 1)^{2}} \\leq \\frac{1}{(M - 1)^{2}}.\n$$\nChoose $M$ so that $\\frac{1}{(M - 1)^{2}} \\leq \\epsilon$, for instance $M = 1 + \\epsilon^{-1/2}$. Then for all $n \\in \\mathbb{N}$,\n$$\n\\mu_{n}([-M, M]) = \\mathbb{P}(|Y_{n}| \\leq M) \\geq 1 - \\epsilon.\n$$\nThis verifies the tightness criterion on $\\mathbb{R}$: for every $\\epsilon  0$, there exists $M  0$ such that $\\mu_{n}([-M, M]) \\geq 1 - \\epsilon$ for all $n$. Therefore, the family $\\{\\mu_{n}\\}_{n \\in \\mathbb{N}}$ is tight.", "answer": "$$\\boxed{A}$$", "id": "1458427"}]}