{"hands_on_practices": [{"introduction": "Jensen's inequality provides powerful insights into systems with random variables. This exercise offers a physical context, exploring the kinetic energy of particles with varying velocities. By applying Jensen's inequality to the convex function for kinetic energy, $f(v) = \\frac{1}{2}mv^2$, you can establish a universal relationship between the average kinetic energy and the kinetic energy calculated from the average velocity, highlighting how variability in a system impacts its average properties.", "problem": "Consider a system involving a stream of particles, all with identical mass $m$. The particles travel along a single dimension, and the velocity of any randomly selected particle can be described by a random variable $V$. The velocity distribution is not necessarily symmetric or centered at zero, and we make no assumptions about its specific form, other than that the mean and variance are finite.\n\nLet the expected kinetic energy of a particle be denoted by $K_{avg}$, which is defined as $K_{avg} = E\\left[\\frac{1}{2}mV^2\\right]$.\nLet the kinetic energy corresponding to the mean velocity be denoted by $K_{mean}$, which is defined as $K_{mean} = \\frac{1}{2}m \\left(E[V]\\right)^2$.\n\nWhich of the following statements correctly describes the universal relationship between $K_{avg}$ and $K_{mean}$?\n\nA. $K_{avg}  K_{mean}$\n\nB. $K_{avg} = K_{mean}$\n\nC. $K_{avg}  K_{mean}$\n\nD. $K_{avg} \\ge K_{mean}$\n\nE. The relationship cannot be determined without knowing the specific probability distribution of $V$.", "solution": "We start from the definitions:\n$$\nK_{avg} = E\\!\\left[\\frac{1}{2} m V^{2}\\right] = \\frac{1}{2} m\\, E[V^{2}], \\qquad K_{mean} = \\frac{1}{2} m \\left(E[V]\\right)^{2}.\n$$\nConsider their difference:\n$$\nK_{avg} - K_{mean} = \\frac{1}{2} m \\left(E[V^{2}] - \\left(E[V]\\right)^{2}\\right).\n$$\nBy the definition of variance,\n$$\n\\operatorname{Var}(V) = E[V^{2}] - \\left(E[V]\\right)^{2} \\ge 0,\n$$\nwith equality if and only if $V$ is almost surely constant. Since physical mass satisfies $m \\ge 0$, it follows that\n$$\nK_{avg} - K_{mean} = \\frac{1}{2} m\\, \\operatorname{Var}(V) \\ge 0.\n$$\nTherefore,\n$$\nK_{avg} \\ge K_{mean},\n$$\nwith equality if and only if $\\operatorname{Var}(V)=0$ (i.e., all particles have the same velocity) or $m=0$. This conclusion is universal and does not depend on the specific form of the distribution of $V$, provided its mean and variance are finite.", "answer": "$$\\boxed{D}$$", "id": "1368159"}, {"introduction": "One of the most practical applications of Jensen's inequality is in optimization. This problem presents a scenario of allocating resources in a distributed system to minimize total operational cost. You will see how recognizing the cost function $C(p) = p + \\frac{\\alpha^2}{p}$ as a convex function allows for a surprisingly elegant solution using the discrete form of Jensen's inequality, demonstrating that an even distribution of resources is optimal.", "problem": "A distributed computing system consists of $n$ independent processing nodes. The total processing power allocated across all nodes is a fixed positive value, $S$. Let $p_i$ be the processing power allocated to the $i$-th node, where $p_i > 0$ for all $i=1, 2, \\ldots, n$. The allocations must satisfy the constraint $\\sum_{i=1}^n p_i = S$. The operational cost for the $i$-th node is found to be a function of its allocated power, given by the expression $C_i(p_i) = p_i + \\frac{\\alpha^2}{p_i}$, where $\\alpha$ is a positive real constant representing a system-specific inefficiency factor. Your task is to determine the minimum possible total operational cost for the entire system, which is the sum of the costs of all nodes, $\\sum_{i=1}^n C_i(p_i)$. Express your answer as a function of $n$, $S$, and $\\alpha$.", "solution": "We are to minimize the total cost\n$$\nF(p_{1},\\ldots,p_{n})=\\sum_{i=1}^{n}\\left(p_{i}+\\frac{\\alpha^{2}}{p_{i}}\\right)\n$$\nsubject to $p_{i}0$ for all $i$ and the linear constraint $\\sum_{i=1}^{n}p_{i}=S$, where $S0$ and $\\alpha0$ are fixed.\n\nFirst, define $f(p)=p+\\frac{\\alpha^{2}}{p}$ for $p0$. Its second derivative is\n$$\nf''(p)=\\frac{2\\alpha^{2}}{p^{3}}0 \\quad \\text{for all } p0,\n$$\nso $f$ is strictly convex on $(0,\\infty)$. Therefore, by Jensenâ€™s inequality,\n$$\n\\frac{1}{n}\\sum_{i=1}^{n}f(p_{i}) \\geq f\\!\\left(\\frac{1}{n}\\sum_{i=1}^{n}p_{i}\\right)=f\\!\\left(\\frac{S}{n}\\right),\n$$\nwith equality if and only if $p_{1}=\\cdots=p_{n}=\\frac{S}{n}$. Hence the unique minimizer under the constraint is\n$$\np_{i}^{\\star}=\\frac{S}{n} \\quad \\text{for all } i=1,\\ldots,n.\n$$\nEvaluating the total cost at this allocation gives\n$$\nF_{\\min}= \\sum_{i=1}^{n}\\left(\\frac{S}{n}+\\frac{\\alpha^{2}}{S/n}\\right)\n= n\\cdot \\frac{S}{n} + n\\cdot \\frac{\\alpha^{2}n}{S}\n= S + \\frac{n^{2}\\alpha^{2}}{S}.\n$$\nBecause $f$ is strictly convex and the feasible set defined by $\\sum_{i=1}^{n}p_{i}=S$, $p_{i}0$ is convex and nonempty, this value is the unique global minimum.", "answer": "$$\\boxed{S+\\frac{n^{2}\\alpha^{2}}{S}}$$", "id": "2304653"}, {"introduction": "Beyond discrete sums, Jensen's inequality holds for integrals, making it a valuable tool in mathematical analysis. This exercise asks you to find a lower bound for a seemingly complicated integral by re-framing it as an expected value problem. By confirming the convexity of the integrand and applying the continuous form of Jensen's inequality, you can derive a sharp numerical estimate, showcasing the inequality's power in analytical approximation.", "problem": "Let a random variable $X$ be uniformly distributed over the interval $[0,1]$. The expected value of the function $g(X) = \\sqrt{1+X^4}$ is defined as $\\mathbb{E}[g(X)] = \\int_0^1 \\sqrt{1+x^4} dx$. By leveraging the convexity of a relevant function, determine a numerical lower bound for this expected value. Round your final answer to four significant figures.", "solution": "We are given $X \\sim \\mathrm{Unif}([0,1])$ and $g(x) = \\sqrt{1 + x^{4}}$. The expected value is\n$$\n\\mathbb{E}[g(X)] = \\int_{0}^{1} \\sqrt{1 + x^{4}} \\, dx.\n$$\nTo obtain a numerical lower bound using convexity, we verify that $g$ is convex on $[0,1]$.\n\nCompute derivatives. Let $g(x) = \\sqrt{1 + x^{4}} = (1 + x^{4})^{1/2}$. Then\n$$\ng'(x) = \\frac{d}{dx}\\left( (1 + x^{4})^{1/2} \\right) = \\frac{1}{2}(1 + x^{4})^{-1/2} \\cdot 4x^{3} = \\frac{2x^{3}}{\\sqrt{1 + x^{4}}}.\n$$\nDifferentiate again using the quotient rule.\n$$\ng''(x) = \\frac{6x^{2}\\sqrt{1 + x^{4}} - 2x^3 \\cdot \\frac{2x^3}{\\sqrt{1+x^4}}}{1+x^4} = \\frac{6x^{2}(1 + x^{4}) - 4x^{6}}{(1 + x^{4})^{3/2}} = \\frac{6x^{2} + 2x^{6}}{(1 + x^{4})^{3/2}} = \\frac{2x^{2}(3 + x^{4})}{(1 + x^{4})^{3/2}}.\n$$\nFor $x \\in [0,1]$, the numerator is nonnegative and the denominator is positive, so $g''(x) \\geq 0$ on $[0,1]$. Therefore $g$ is convex on $[0,1]$.\n\nBy Jensen's inequality for convex $g$,\n$$\n\\mathbb{E}[g(X)] \\geq g(\\mathbb{E}[X]).\n$$\nFor $X \\sim \\mathrm{Unif}([0,1])$, we have\n$$\n\\mathbb{E}[X] = \\int_{0}^{1} x \\, dx = \\frac{1}{2}.\n$$\nTherefore,\n$$\n\\mathbb{E}[g(X)] \\geq g\\!\\left( \\frac{1}{2} \\right) = \\sqrt{1 + \\left( \\frac{1}{2} \\right)^{4}} = \\sqrt{1 + \\frac{1}{16}} = \\sqrt{\\frac{17}{16}} = \\frac{\\sqrt{17}}{4}.\n$$\nNumerically,\n$$\n\\frac{\\sqrt{17}}{4} \\approx 1.030776406\\ldots\n$$\nRounded to four significant figures, the convexity-based lower bound is $1.031$.", "answer": "$$\\boxed{1.031}$$", "id": "1425650"}]}