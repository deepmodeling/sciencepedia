{"hands_on_practices": [{"introduction": "The Gauss-Markov theorem provides a powerful guarantee: under certain conditions, the Ordinary Least Squares (OLS) estimator has the smallest variance among all linear unbiased estimators. This exercise grounds this theoretical guarantee in a practical calculation. By working through a simple linear regression scenario, you will compute the variance of the slope estimator, seeing firsthand how it depends on the underlying error variance and the specific choices of your experimental data points, which form the design matrix [@problem_id:1919549].", "problem": "In an introductory physics laboratory, a student investigates the relationship between the elongation of a spring and the mass attached to it. The student posits a simple linear model relating the dependent variable, $y_i$, to an independent variable, $x_i$. The model is given by $y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$ for $i=1, 2, 3$. Here, $\\beta_0$ and $\\beta_1$ are the unknown intercept and slope parameters, respectively. The term $\\epsilon_i$ represents the random measurement error for the $i$-th observation.\n\nThe standard assumptions of the classical linear regression model hold:\n1. The expected value of the error term is zero: $E[\\epsilon_i] = 0$.\n2. The errors have a constant variance (homoscedasticity): $\\text{Var}(\\epsilon_i) = \\sigma^2$ for some positive constant $\\sigma^2$.\n3. The errors are uncorrelated with each other: $\\text{Cov}(\\epsilon_i, \\epsilon_j) = 0$ for all $i \\neq j$.\n\nThe student performs three measurements, choosing to set the independent variable $x$ to the specific values $x_1 = -1$, $x_2 = 0$, and $x_3 = 1$. The parameters $\\beta_0$ and $\\beta_1$ are estimated using the method of Ordinary Least Squares (OLS). According to the Gauss-Markov theorem, the OLS estimator for the slope, denoted $\\hat{\\beta_1}$, has the minimum variance among all linear unbiased estimators.\n\nDetermine the variance of the OLS estimator $\\hat{\\beta_1}$. Express your answer as an analytic expression in terms of the error variance $\\sigma^2$.", "solution": "We model the data as $y_{i}=\\beta_{0}+\\beta_{1}x_{i}+\\epsilon_{i}$ with $E[\\epsilon_{i}]=0$, $\\text{Var}(\\epsilon_{i})=\\sigma^{2}$, and $\\text{Cov}(\\epsilon_{i},\\epsilon_{j})=0$ for $i\\neq j$. In matrix form, let $y=X\\beta+\\epsilon$ with\n$$\nX=\\begin{pmatrix}\n1 & x_{1}\\\\\n1 & x_{2}\\\\\n1 & x_{3}\n\\end{pmatrix},\\quad\n\\beta=\\begin{pmatrix}\\beta_{0}\\\\ \\beta_{1}\\end{pmatrix},\\quad\n\\epsilon=\\begin{pmatrix}\\epsilon_{1}\\\\ \\epsilon_{2}\\\\ \\epsilon_{3}\\end{pmatrix}.\n$$\nThe OLS estimator is $\\hat{\\beta}=(X^{\\top}X)^{-1}X^{\\top}y$, and under the stated assumptions its covariance matrix is\n$$\n\\text{Var}(\\hat{\\beta})=\\sigma^{2}(X^{\\top}X)^{-1}.\n$$\nWith $x_{1}=-1$, $x_{2}=0$, $x_{3}=1$, we compute\n$$\nX^{\\top}X=\\begin{pmatrix}\n\\sum_{i=1}^{3}1 & \\sum_{i=1}^{3}x_{i}\\\\[4pt]\n\\sum_{i=1}^{3}x_{i} & \\sum_{i=1}^{3}x_{i}^{2}\n\\end{pmatrix}\n=\\begin{pmatrix}\n3 & (-1)+0+1\\\\\n(-1)+0+1 & (-1)^{2}+0^{2}+1^{2}\n\\end{pmatrix}\n=\\begin{pmatrix}\n3 & 0\\\\\n0 & 2\n\\end{pmatrix}.\n$$\nSince $X^{\\top}X$ is diagonal, its inverse is\n$$\n(X^{\\top}X)^{-1}=\\begin{pmatrix}\n\\frac{1}{3} & 0\\\\\n0 & \\frac{1}{2}\n\\end{pmatrix}.\n$$\nTherefore,\n$$\n\\text{Var}(\\hat{\\beta})=\\sigma^{2}\\begin{pmatrix}\n\\frac{1}{3} & 0\\\\\n0 & \\frac{1}{2}\n\\end{pmatrix},\n$$\nso the variance of the OLS slope estimator is the $(2,2)$ entry,\n$$\n\\text{Var}(\\hat{\\beta}_{1})=\\sigma^{2}\\cdot\\frac{1}{2}=\\frac{\\sigma^{2}}{2}.\n$$", "answer": "$$\\boxed{\\frac{\\sigma^{2}}{2}}$$", "id": "1919549"}, {"introduction": "A crucial part of mastering any theorem is understanding its boundaries and the consequences of violating its assumptions. This problem presents a thought experiment where one of the key Gauss-Markov assumptions—that the error terms have a mean of zero—is not met [@problem_id:1919602]. By investigating whether the OLS slope estimator remains unbiased in the presence of such a systematic error, you will develop a deeper intuition for the specific role each assumption plays and appreciate the surprising robustness of certain properties of OLS estimators.", "problem": "Consider a simple linear regression model used to analyze a dataset of $n$ observations $(x_i, y_i)$:\n$$y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$$\nfor $i = 1, 2, \\dots, n$. The predictor variables $x_i$ are considered fixed (non-stochastic). The Ordinary Least Squares (OLS) estimators for the intercept and slope are denoted by $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$, respectively.\n\nTypically, one of the core assumptions for the Gauss-Markov theorem is that the expected value of the error term is zero, i.e., $E[\\epsilon_i] = 0$. Suppose this assumption is violated and there is a systematic measurement error such that the error term has a constant, non-zero mean:\n$$E[\\epsilon_i] = c$$\nwhere $c$ is a non-zero real constant. All other standard assumptions hold, including that the errors have a constant variance (homoscedasticity) and are uncorrelated. Let $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i$.\n\nWhat is the expected value of the OLS slope estimator, $E[\\hat{\\beta}_1]$?\n\nA. $\\beta_1$\n\nB. $\\beta_1 + c$\n\nC. $\\beta_1 + \\frac{c}{\\bar{x}}$\n\nD. $\\beta_1 - c$\n\nE. $\\beta_1 + \\frac{c n}{\\sum_{i=1}^n (x_i - \\bar{x})}$", "solution": "We are given the simple linear regression model with fixed regressors:\n$$\ny_i = \\beta_{0} + \\beta_{1} x_i + \\epsilon_i, \\quad E[\\epsilon_i] = c, \\quad i = 1,\\dots,n,\n$$\nand the OLS slope estimator\n$$\n\\hat{\\beta}_{1} = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n} (x_i - \\bar{x})^{2}}.\n$$\nUsing the identity $\\sum_{i=1}^{n} (x_i - \\bar{x}) \\bar{y} = \\bar{y} \\sum_{i=1}^{n} (x_i - \\bar{x}) = 0$, we can equivalently write\n$$\n\\hat{\\beta}_{1} = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x}) y_i}{\\sum_{i=1}^{n} (x_i - \\bar{x})^{2}}.\n$$\nSubstitute $y_i = \\beta_{0} + \\beta_{1} x_i + \\epsilon_i$:\n$$\n\\sum_{i=1}^{n} (x_i - \\bar{x}) y_i\n= \\sum_{i=1}^{n} (x_i - \\bar{x})(\\beta_{0} + \\beta_{1} x_i + \\epsilon_i)\n= \\beta_{0} \\sum_{i=1}^{n} (x_i - \\bar{x}) + \\beta_{1} \\sum_{i=1}^{n} (x_i - \\bar{x}) x_i + \\sum_{i=1}^{n} (x_i - \\bar{x}) \\epsilon_i.\n$$\nSince $\\sum_{i=1}^{n} (x_i - \\bar{x}) = 0$, the first term is zero. For the second term, write $x_i = (x_i - \\bar{x}) + \\bar{x}$ to obtain\n$$\n\\sum_{i=1}^{n} (x_i - \\bar{x}) x_i\n= \\sum_{i=1}^{n} (x_i - \\bar{x})\\big((x_i - \\bar{x}) + \\bar{x}\\big)\n= \\sum_{i=1}^{n} (x_i - \\bar{x})^{2} + \\bar{x} \\sum_{i=1}^{n} (x_i - \\bar{x})\n= \\sum_{i=1}^{n} (x_i - \\bar{x})^{2}.\n$$\nHence,\n$$\n\\sum_{i=1}^{n} (x_i - \\bar{x}) y_i = \\beta_{1} \\sum_{i=1}^{n} (x_i - \\bar{x})^{2} + \\sum_{i=1}^{n} (x_i - \\bar{x}) \\epsilon_i,\n$$\nand therefore\n$$\n\\hat{\\beta}_{1} = \\beta_{1} + \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x}) \\epsilon_i}{\\sum_{i=1}^{n} (x_i - \\bar{x})^{2}}.\n$$\nTaking expectations conditional on the fixed $x_i$ and using $E[\\epsilon_i] = c$,\n$$\nE\\left[\\sum_{i=1}^{n} (x_i - \\bar{x}) \\epsilon_i\\right]\n= \\sum_{i=1}^{n} (x_i - \\bar{x}) E[\\epsilon_i]\n= c \\sum_{i=1}^{n} (x_i - \\bar{x})\n= 0.\n$$\nThus,\n$$\nE[\\hat{\\beta}_{1}] = \\beta_{1} + \\frac{0}{\\sum_{i=1}^{n} (x_i - \\bar{x})^{2}} = \\beta_{1}.\n$$\nTherefore the correct option is A.", "answer": "$$\\boxed{A}$$", "id": "1919602"}, {"introduction": "What happens when the assumptions of the Gauss-Markov theorem don't hold? In that case, OLS is no longer guaranteed to be the \"Best\" estimator. This exercise challenges you to derive the Best Linear Unbiased Estimator (BLUE) from first principles in a scenario with heteroscedastic errors—that is, where each observation has a different variance [@problem_id:1919575]. By solving this problem, you will uncover the elegant and intuitive principle of inverse-variance weighting, a fundamental concept in statistical estimation for optimally combining information from sources of varying precision.", "problem": "An array of $n$ independent quantum sensors is used to measure a fundamental physical constant, whose true value is $\\mu$. Each sensor $i$ provides a single measurement $y_i$. The measurement model for each sensor is given by $y_i = \\mu + \\epsilon_i$, where $\\epsilon_i$ is the measurement error. The errors are known to have zero mean, $E[\\epsilon_i] = 0$, and are uncorrelated across sensors, meaning $Cov(\\epsilon_i, \\epsilon_j) = 0$ for $i \\neq j$. However, due to slight manufacturing variations, the sensors have different precisions. The variance of the error for sensor $i$ is known and given by $Var(\\epsilon_i) = \\sigma_i^2$.\n\nTo estimate $\\mu$, we decide to use a linear estimator, which has the general form $\\hat{\\mu} = \\sum_{i=1}^n c_i y_i$, where $c_i$ are constant coefficients to be determined. We require this estimator to be unbiased, which means its expected value must be equal to the true value, $E[\\hat{\\mu}] = \\mu$. Among all possible linear unbiased estimators, we seek the \"best\" one, defined as the estimator with the minimum possible variance, $Var(\\hat{\\mu})$.\n\nFind the analytical expression for this Best Linear Unbiased Estimator (BLUE) for $\\mu$ in terms of the measurements $y_i$ and their corresponding error variances $\\sigma_i^2$.", "solution": "We model each measurement as $y_{i}=\\mu+\\epsilon_{i}$ with $E[\\epsilon_{i}]=0$, $Var(\\epsilon_{i})=\\sigma_{i}^{2}$, and $Cov(\\epsilon_{i},\\epsilon_{j})=0$ for $i\\neq j$. Consider a linear estimator $\\hat{\\mu}=\\sum_{i=1}^{n}c_{i}y_{i}$.\n\nUnbiasedness requires $E[\\hat{\\mu}]=\\mu$. Since $E[y_{i}]=\\mu$, we have\n$$\nE[\\hat{\\mu}]=\\sum_{i=1}^{n}c_{i}E[y_{i}]=\\mu\\sum_{i=1}^{n}c_{i}.\n$$\nThus the unbiasedness constraint is\n$$\n\\sum_{i=1}^{n}c_{i}=1.\n$$\n\nThe variance of $\\hat{\\mu}$, using uncorrelated errors, is\n$$\nVar(\\hat{\\mu})=Var\\!\\left(\\sum_{i=1}^{n}c_{i}y_{i}\\right)=Var\\!\\left(\\sum_{i=1}^{n}c_{i}\\epsilon_{i}\\right)=\\sum_{i=1}^{n}c_{i}^{2}\\sigma_{i}^{2}.\n$$\nWe minimize $\\sum_{i=1}^{n}c_{i}^{2}\\sigma_{i}^{2}$ subject to $\\sum_{i=1}^{n}c_{i}=1$ using a Lagrange multiplier $\\lambda$. Define\n$$\n\\mathcal{L}(c_{1},\\dots,c_{n},\\lambda)=\\sum_{i=1}^{n}c_{i}^{2}\\sigma_{i}^{2}-\\lambda\\left(\\sum_{i=1}^{n}c_{i}-1\\right).\n$$\nSetting partial derivatives to zero gives, for each $i$,\n$$\n\\frac{\\partial\\mathcal{L}}{\\partial c_{i}}=2c_{i}\\sigma_{i}^{2}-\\lambda=0\\quad\\Rightarrow\\quad c_{i}=\\frac{\\lambda}{2\\sigma_{i}^{2}}.\n$$\nImposing the constraint,\n$$\n\\sum_{i=1}^{n}c_{i}=1\\quad\\Rightarrow\\quad \\frac{\\lambda}{2}\\sum_{i=1}^{n}\\frac{1}{\\sigma_{i}^{2}}=1\\quad\\Rightarrow\\quad \\lambda=\\frac{2}{\\sum_{j=1}^{n}\\frac{1}{\\sigma_{j}^{2}}}.\n$$\nTherefore,\n$$\nc_{i}=\\frac{\\frac{1}{\\sigma_{i}^{2}}}{\\sum_{j=1}^{n}\\frac{1}{\\sigma_{j}^{2}}}.\n$$\nThe BLUE is then the inverse-variance weighted average\n$$\n\\hat{\\mu}=\\sum_{i=1}^{n}c_{i}y_{i}=\\frac{\\sum_{i=1}^{n}\\frac{y_{i}}{\\sigma_{i}^{2}}}{\\sum_{i=1}^{n}\\frac{1}{\\sigma_{i}^{2}}}.\n$$\nIts minimum variance is $Var(\\hat{\\mu})=\\left(\\sum_{i=1}^{n}\\frac{1}{\\sigma_{i}^{2}}\\right)^{-1}$, confirming optimality among linear unbiased estimators.", "answer": "$$\\boxed{\\frac{\\sum_{i=1}^{n}\\frac{y_{i}}{\\sigma_{i}^{2}}}{\\sum_{i=1}^{n}\\frac{1}{\\sigma_{i}^{2}}}}$$", "id": "1919575"}]}