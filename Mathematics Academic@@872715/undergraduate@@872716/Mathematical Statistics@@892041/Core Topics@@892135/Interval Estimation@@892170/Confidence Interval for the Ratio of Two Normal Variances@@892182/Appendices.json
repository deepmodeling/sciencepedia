{"hands_on_practices": [{"introduction": "Mastering the comparison of variances begins with the fundamental skill of constructing a confidence interval. This first practice guides you through a typical scenario, applying the core formula that links the ratio of sample variances to the F-distribution [@problem_id:1908225]. By working through this calculation, you will build a concrete understanding of how to quantify the uncertainty in the comparison of variability between two independent, normally distributed populations.", "problem": "A team of analysts is evaluating the performance consistency of two different financial risk models, labeled Model A and Model B. They run a series of simulations on each model to estimate the value-at-risk of a portfolio, and the primary metric for consistency is the variance of these estimates. It is assumed that the estimates from both models are independent and are drawn from normal distributions.\n\nA random sample of $n_A = 16$ simulations is run using Model A, yielding a sample variance of $s_A^2 = 4.80$ (in units of squared millions of dollars). A second independent random sample of $n_B = 11$ simulations is run using Model B, resulting in a sample variance of $s_B^2 = 2.10$ (in the same units).\n\nThe team wants to construct a 95% confidence interval for the ratio of the true population variances, $\\frac{\\sigma_A^2}{\\sigma_B^2}$, to compare the models' precision.\n\nYou are given the following critical values from the F-distribution, where the notation $F_{\\alpha, \\nu_1, \\nu_2}$ represents the critical value with an upper tail probability of $\\alpha$, for $\\nu_1$ numerator degrees of freedom and $\\nu_2$ denominator degrees of freedom:\n- $F_{0.025, 15, 10} = 3.522$\n- $F_{0.05, 15, 10} = 2.848$\n- $F_{0.025, 16, 11} = 3.379$\n- $F_{0.025, 10, 15} = 3.060$\n- $F_{0.05, 10, 15} = 2.544$\n\nCalculate the lower and upper bounds of the 95% confidence interval for the ratio $\\frac{\\sigma_A^2}{\\sigma_B^2}$. Report the lower bound and the upper bound, in that order, with each value rounded to three significant figures.", "solution": "We are comparing the variances of two independent normal populations using the ratio $R = \\frac{\\sigma_{A}^{2}}{\\sigma_{B}^{2}}$. For independent normal samples with sizes $n_{A}$ and $n_{B}$, the statistics\n$$\nX = \\frac{(n_{A}-1)S_{A}^{2}}{\\sigma_{A}^{2}} \\sim \\chi^{2}_{n_{A}-1}, \\quad Y = \\frac{(n_{B}-1)S_{B}^{2}}{\\sigma_{B}^{2}} \\sim \\chi^{2}_{n_{B}-1},\n$$\nand\n$$\n\\frac{X/(n_{A}-1)}{Y/(n_{B}-1)} = \\frac{(S_{A}^{2}/\\sigma_{A}^{2})}{(S_{B}^{2}/\\sigma_{B}^{2})} \\sim F_{n_{A}-1,\\; n_{B}-1}.\n$$\nSet $Q = \\frac{(S_{A}^{2}/\\sigma_{A}^{2})}{(S_{B}^{2}/\\sigma_{B}^{2})}$. Then $Q \\sim F_{\\nu_{1},\\nu_{2}}$ with $\\nu_{1} = n_{A}-1 = 15$ and $\\nu_{2} = n_{B}-1 = 10$. For a two-sided confidence interval with confidence $1-\\alpha = 0.95$ (so $\\alpha = 0.05$ and $\\alpha/2 = 0.025$), the central probability statement is\n$$\nP\\!\\left(q_{L} \\leq Q \\leq q_{U}\\right) = 1 - \\alpha,\n$$\nwhere $q_{U}$ is the upper critical value with upper-tail probability $\\alpha/2$, and $q_{L}$ is the lower critical value with lower-tail probability $\\alpha/2$. Using the problemâ€™s notation $F_{\\alpha,\\nu_{1},\\nu_{2}}$ for the upper-tail critical (so $P(F  F_{\\alpha,\\nu_{1},\\nu_{2}}) = \\alpha$), we have\n$$\nq_{U} = F_{0.025,\\,15,\\,10}, \\quad q_{L} = \\frac{1}{F_{0.025,\\,10,\\,15}},\n$$\nby the reciprocity property $F_{1-p;\\,\\nu_{1},\\nu_{2}} = \\frac{1}{F_{p;\\,\\nu_{2},\\nu_{1}}}$. Since\n$$\nQ = \\frac{(S_{A}^{2}/S_{B}^{2})}{(\\sigma_{A}^{2}/\\sigma_{B}^{2})},\n$$\nthe inequality $q_{L} \\leq Q \\leq q_{U}$ is equivalent to\n$$\n\\frac{S_{A}^{2}/S_{B}^{2}}{q_{U}} \\leq \\frac{\\sigma_{A}^{2}}{\\sigma_{B}^{2}} \\leq \\frac{S_{A}^{2}/S_{B}^{2}}{q_{L}}.\n$$\nThus the $95$ percent confidence interval for $\\frac{\\sigma_{A}^{2}}{\\sigma_{B}^{2}}$ is\n$$\n\\left[\\,\\frac{s_{A}^{2}/s_{B}^{2}}{F_{0.025,\\,15,\\,10}},\\; \\left(s_{A}^{2}/s_{B}^{2}\\right)\\,F_{0.025,\\,10,\\,15}\\,\\right].\n$$\nInsert the given values $s_{A}^{2} = 4.80$, $s_{B}^{2} = 2.10$, $F_{0.025,\\,15,\\,10} = 3.522$, and $F_{0.025,\\,10,\\,15} = 3.060$:\n$$\n\\frac{s_{A}^{2}}{s_{B}^{2}} = \\frac{4.80}{2.10} = \\frac{16}{7} \\approx 2.285714,\n$$\n$$\n\\text{Lower bound} = \\frac{16/7}{3.522} \\approx 0.64898 \\approx 0.649 \\text{ (three significant figures)},\n$$\n$$\n\\text{Upper bound} = \\left(\\frac{16}{7}\\right)\\,3.060 \\approx 6.99429 \\approx 6.99 \\text{ (three significant figures)}.\n$$\nTherefore, the $95$ percent confidence interval for $\\frac{\\sigma_{A}^{2}}{\\sigma_{B}^{2}}$ is approximately $\\left(0.649,\\; 6.99\\right)$, reported as lower bound then upper bound, each to three significant figures.", "answer": "$$\\boxed{\\begin{pmatrix}0.649  6.99\\end{pmatrix}}$$", "id": "1908225"}, {"introduction": "Statistical methods are built on a foundation of assumptions, and a skilled practitioner knows when those foundations are solid. The standard confidence interval for the ratio of variances assumes the two groups are independent. This practice explores what happens when this assumption is violated, as is often the case with paired data (e.g., before-and-after measurements) [@problem_id:1908233]. Through a direct mathematical derivation, you will discover why the standard F-test is inappropriate for such data, reinforcing the critical importance of aligning your statistical tools with your experimental design.", "problem": "In many experimental designs, researchers collect paired data. For example, in a clinical study, measurements might be taken on the same subject before and after a treatment. Let the pair of measurements for the $i$-th subject be denoted by $(X_i, Y_i)$. A common assumption is that these pairs are independent and identically distributed draws from a bivariate normal distribution.\n\nLet a random sample of $n$ such pairs, $(X_1, Y_1), (X_2, Y_2), \\dots, (X_n, Y_n)$, be drawn from a bivariate normal distribution with mean vector $(\\mu_X, \\mu_Y)$, positive variances $\\sigma_X^2$ and $\\sigma_Y^2$, and correlation coefficient $\\rho$. The unbiased sample variances for the $X$ and $Y$ measurements are given by:\n$$S_X^2 = \\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\bar{X})^2$$\n$$S_Y^2 = \\frac{1}{n-1} \\sum_{i=1}^n (Y_i - \\bar{Y})^2$$\nwhere $\\bar{X}$ and $\\bar{Y}$ are the respective sample means.\n\nThe standard procedure for comparing variances of two independent normal populations relies on the statistical independence of their sample variances. To investigate the validity of this assumption in the context of paired data, consider the simplest non-trivial case with a sample of size $n=2$. Derive a closed-form analytic expression for the covariance between the two sample variances, $\\text{Cov}(S_X^2, S_Y^2)$, in terms of the population parameters $\\sigma_X^2$, $\\sigma_Y^2$, and $\\rho$.", "solution": "The goal is to compute the covariance $\\text{Cov}(S_X^2, S_Y^2)$ for a sample of size $n=2$.\n\nFirst, let's simplify the expressions for the sample variances $S_X^2$ and $S_Y^2$ when $n=2$.\nThe sample mean for $X$ is $\\bar{X} = \\frac{X_1 + X_2}{2}$.\nThe sample variance $S_X^2$ is:\n$$S_X^2 = \\frac{1}{2-1} \\left[ \\left(X_1 - \\frac{X_1+X_2}{2}\\right)^2 + \\left(X_2 - \\frac{X_1+X_2}{2}\\right)^2 \\right]$$\n$$S_X^2 = \\left( \\frac{X_1-X_2}{2} \\right)^2 + \\left( \\frac{X_2-X_1}{2} \\right)^2 = 2 \\left( \\frac{X_1-X_2}{2} \\right)^2 = \\frac{1}{2}(X_1 - X_2)^2$$\nSimilarly, for the $Y$ measurements:\n$$S_Y^2 = \\frac{1}{2}(Y_1 - Y_2)^2$$\n\nTo find the covariance between $S_X^2$ and $S_Y^2$, we define two new random variables:\n$U = X_1 - X_2$\n$V = Y_1 - Y_2$\nWith these definitions, we have $S_X^2 = \\frac{1}{2}U^2$ and $S_Y^2 = \\frac{1}{2}V^2$.\nThe covariance can now be written as:\n$$\\text{Cov}(S_X^2, S_Y^2) = \\text{Cov}\\left(\\frac{1}{2}U^2, \\frac{1}{2}V^2\\right) = \\frac{1}{4}\\text{Cov}(U^2, V^2)$$\nBy definition, $\\text{Cov}(U^2, V^2) = E[U^2 V^2] - E[U^2] E[V^2]$.\n\nNext, we characterize the joint distribution of $(U, V)$. Since $X_i$ and $Y_i$ are from a bivariate normal distribution, any linear combination of them will also be normally distributed. Thus, $(U, V)$ has a bivariate normal distribution. Let's find its parameters.\nThe means are:\n$E[U] = E[X_1 - X_2] = E[X_1] - E[X_2] = \\mu_X - \\mu_X = 0$\n$E[V] = E[Y_1 - Y_2] = E[Y_1] - E[Y_2] = \\mu_Y - \\mu_Y = 0$\n\nThe variances are:\n$\\text{Var}(U) = \\text{Var}(X_1 - X_2) = \\text{Var}(X_1) + \\text{Var}(X_2) - 2\\text{Cov}(X_1, X_2)$. Since $(X_1, Y_1)$ and $(X_2, Y_2)$ are independent pairs, $X_1$ and $X_2$ are independent, so $\\text{Cov}(X_1, X_2) = 0$.\n$\\text{Var}(U) = \\sigma_X^2 + \\sigma_X^2 = 2\\sigma_X^2$.\nSimilarly, $\\text{Var}(V) = \\text{Var}(Y_1 - Y_2) = \\sigma_Y^2 + \\sigma_Y^2 = 2\\sigma_Y^2$.\n\nThe covariance between $U$ and $V$ is:\n$\\text{Cov}(U, V) = \\text{Cov}(X_1 - X_2, Y_1 - Y_2) = \\text{Cov}(X_1, Y_1) - \\text{Cov}(X_1, Y_2) - \\text{Cov}(X_2, Y_1) + \\text{Cov}(X_2, Y_2)$.\nDue to the independence of the pairs $(X_1, Y_1)$ and $(X_2, Y_2)$, we have $\\text{Cov}(X_1, Y_2) = 0$ and $\\text{Cov}(X_2, Y_1) = 0$.\nThe definition of the correlation coefficient is $\\rho = \\frac{\\text{Cov}(X,Y)}{\\sigma_X \\sigma_Y}$, so $\\text{Cov}(X_i, Y_i) = \\rho \\sigma_X \\sigma_Y$.\nTherefore, $\\text{Cov}(U, V) = \\rho \\sigma_X \\sigma_Y - 0 - 0 + \\rho \\sigma_X \\sigma_Y = 2\\rho \\sigma_X \\sigma_Y$.\n\nNow we can compute the terms for $\\text{Cov}(U^2, V^2)$.\n$E[U^2] = \\text{Var}(U) + (E[U])^2 = 2\\sigma_X^2 + 0^2 = 2\\sigma_X^2$.\n$E[V^2] = \\text{Var}(V) + (E[V])^2 = 2\\sigma_Y^2 + 0^2 = 2\\sigma_Y^2$.\n\nTo find the mixed fourth moment $E[U^2 V^2]$, we use Isserlis' Theorem for zero-mean multivariate normal random variables, which states for variables $Z_1, Z_2, Z_3, Z_4$:\n$E[Z_1 Z_2 Z_3 Z_4] = E[Z_1 Z_2]E[Z_3 Z_4] + E[Z_1 Z_3]E[Z_2 Z_4] + E[Z_1 Z_4]E[Z_2 Z_3]$.\nApplying this to $E[U^2 V^2] = E[U U V V]$:\n$E[U^2 V^2] = E[UU]E[VV] + E[UV]E[UV] + E[UV]E[UV]$\n$E[U^2 V^2] = E[U^2]E[V^2] + 2(E[UV])^2$.\nWe know $E[UV] = \\text{Cov}(U, V)$ since the means are zero.\n$E[U^2 V^2] = (2\\sigma_X^2)(2\\sigma_Y^2) + 2(2\\rho \\sigma_X \\sigma_Y)^2$\n$E[U^2 V^2] = 4\\sigma_X^2\\sigma_Y^2 + 2(4\\rho^2 \\sigma_X^2 \\sigma_Y^2) = 4\\sigma_X^2\\sigma_Y^2(1 + 2\\rho^2)$.\n\nNow, we can compute $\\text{Cov}(U^2, V^2)$:\n$\\text{Cov}(U^2, V^2) = E[U^2 V^2] - E[U^2]E[V^2]$\n$\\text{Cov}(U^2, V^2) = 4\\sigma_X^2\\sigma_Y^2(1 + 2\\rho^2) - (2\\sigma_X^2)(2\\sigma_Y^2)$\n$\\text{Cov}(U^2, V^2) = 4\\sigma_X^2\\sigma_Y^2 + 8\\rho^2\\sigma_X^2\\sigma_Y^2 - 4\\sigma_X^2\\sigma_Y^2 = 8\\rho^2\\sigma_X^2\\sigma_Y^2$.\n\nFinally, we substitute this back into the expression for $\\text{Cov}(S_X^2, S_Y^2)$:\n$$\\text{Cov}(S_X^2, S_Y^2) = \\frac{1}{4}\\text{Cov}(U^2, V^2) = \\frac{1}{4}(8\\rho^2\\sigma_X^2\\sigma_Y^2) = 2\\rho^2\\sigma_X^2\\sigma_Y^2$$\n\nThis result shows that if the correlation $\\rho$ between the paired measurements is non-zero, then the sample variances $S_X^2$ and $S_Y^2$ are not independent, as their covariance is non-zero. This violates a fundamental assumption required for using the standard F-distribution to construct a confidence interval for the ratio of the variances.", "answer": "$$\\boxed{2\\rho^{2}\\sigma_{X}^{2}\\sigma_{Y}^{2}}$$", "id": "1908233"}, {"introduction": "Interpreting statistical results requires careful and sometimes counter-intuitive logic. This final practice delves into a common conceptual trap: the idea of transitivity in statistical comparisons. If we conclude that variance $\\sigma_A^2$ is not significantly different from $\\sigma_B^2$, and $\\sigma_B^2$ is not significantly different from $\\sigma_C^2$, is it safe to assume that $\\sigma_A^2$ and $\\sigma_C^2$ are also statistically similar? This exercise [@problem_id:1908245] uses confidence intervals to demonstrate that this is not always the case, offering a crucial lesson in the cautious interpretation of statistical evidence.", "problem": "Three independent research labs, A, B, and C, are evaluating the consistency of a new manufacturing process. Each lab produces a set of test items and measures a key performance metric. The measurements from each lab are assumed to be independent random samples from three normally distributed populations, $N(\\mu_A, \\sigma_A^2)$, $N(\\mu_B, \\sigma_B^2)$, and $N(\\mu_C, \\sigma_C^2)$, respectively. The population variances $\\sigma_A^2$, $\\sigma_B^2$, and $\\sigma_C^2$ represent the variability of the process as assessed by each lab.\n\nThe labs report the following results from their experiments:\n- Sample sizes: $n_A = 16$, $n_B = 16$, $n_C = 16$.\n- Sample variances: $S_A^2 = 80.0$, $S_B^2 = 40.0$, $S_C^2 = 20.0$.\n\nFor constructing 95% confidence intervals, you are given the relevant quantile of the F-distribution: $F_{0.025, 15, 15} = 2.862$.\n\nLet $CI(X/Y)$ denote the 95% confidence interval for the ratio of population variances $\\sigma_X^2 / \\sigma_Y^2$. Based on the data, which one of the following statements is true?\n\nA. $CI(A/B)$, $CI(B/C)$, and $CI(A/C)$ all contain the value 1.\nB. $CI(A/B)$ and $CI(B/C)$ contain the value 1, but $CI(A/C)$ does not.\nC. $CI(B/C)$ and $CI(A/C)$ contain the value 1, but $CI(A/B)$ does not.\nD. $CI(A/B)$ and $CI(A/C)$ contain the value 1, but $CI(B/C)$ does not.\nE. $CI(A/B)$ contains the value 1, but $CI(B/C)$ and $CI(A/C)$ do not.", "solution": "To determine if the value 1 is contained within a 95% confidence interval for the ratio of variances $\\sigma_X^2 / \\sigma_Y^2$, we must check if the ratio of sample variances, $R = S_X^2 / S_Y^2$, falls within a specific range determined by the critical values of the F-distribution.\n\nA 95% confidence interval for the ratio of two variances will contain the value 1 if the corresponding two-tailed hypothesis test (at significance level $\\alpha=0.05$) fails to reject the null hypothesis $H_0: \\sigma_X^2 = \\sigma_Y^2$. This occurs when the test statistic, which is the ratio of sample variances $R$, is not extreme.\nThe confidence interval is given by:\n$$ \\left( \\frac{S_X^2}{S_Y^2} \\frac{1}{F_{\\alpha/2, n_X-1, n_Y-1}}, \\frac{S_X^2}{S_Y^2} F_{\\alpha/2, n_Y-1, n_X-1} \\right) $$\nThe value 1 is in this interval if and only if:\n$$ \\frac{S_X^2}{S_Y^2} \\frac{1}{F_{\\alpha/2, n_X-1, n_Y-1}} \\le 1 \\quad \\text{and} \\quad 1 \\le \\frac{S_X^2}{S_Y^2} F_{\\alpha/2, n_Y-1, n_X-1} $$\nLet $R = S_X^2 / S_Y^2$. This simplifies to the condition:\n$$ \\frac{1}{F_{\\alpha/2, n_Y-1, n_X-1}} \\le R \\le F_{\\alpha/2, n_X-1, n_Y-1} $$\nIn this problem, all sample sizes are $n=16$, so the degrees of freedom are $n_X-1 = n_Y-1 = 15$ for all comparisons. The confidence level is 95%, so $\\alpha = 0.05$ and $\\alpha/2 = 0.025$. The relevant critical value is $F_{0.025, 15, 15} = 2.862$.\nBecause the degrees of freedom are equal, the condition for the confidence interval to contain 1 simplifies to:\n$$ \\frac{1}{F_{0.025, 15, 15}} \\le R \\le F_{0.025, 15, 15} $$\nSubstituting the given value, the condition is:\n$$ \\frac{1}{2.862} \\le R \\le 2.862 \\quad \\implies \\quad 0.3494 \\le R \\le 2.862 $$\nNow, we calculate the sample variance ratios:\n- $R_{A/B} = S_A^2 / S_B^2 = 80.0 / 40.0 = 2.0$\n- $R_{B/C} = S_B^2 / S_C^2 = 40.0 / 20.0 = 2.0$\n- $R_{A/C} = S_A^2 / S_C^2 = 80.0 / 20.0 = 4.0$\n\nFinally, we check the condition for each ratio:\n- For $CI(A/B)$: $R_{A/B} = 2.0$. Since $0.3494 \\le 2.0 \\le 2.862$, the interval contains 1.\n- For $CI(B/C)$: $R_{B/C} = 2.0$. Since $0.3494 \\le 2.0 \\le 2.862$, the interval contains 1.\n- For $CI(A/C)$: $R_{A/C} = 4.0$. Since $4.0 > 2.862$, the interval does not contain 1.\n\nTherefore, $CI(A/B)$ and $CI(B/C)$ contain the value 1, but $CI(A/C)$ does not. This corresponds to statement B.", "answer": "$$\\boxed{B}$$", "id": "1908245"}]}