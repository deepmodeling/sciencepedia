{"hands_on_practices": [{"introduction": "The invariance property of Maximum Likelihood Estimators (MLEs) is a powerful shortcut in statistical inference. This first exercise provides a foundational application of this principle, starting with a familiar setting—estimating a proportion from a binomial sample—and then using the invariance property to directly find the MLE for the variance of the underlying Bernoulli trial [@problem_id:1925576]. This practice builds the fundamental skill of first finding the MLE for a parameter and then effortlessly obtaining the MLE for a function of that parameter.", "problem": "In a semiconductor manufacturing plant, a quality control engineer is assessing the yield of a specific lithography process. The process is designed to produce a large number of identical microchips. The engineer assumes that each chip produced has a constant, but unknown, probability $p$ of being defective, independent of all other chips. To estimate this probability's associated variance, the engineer takes a random sample of $n$ chips from the production line and observes that exactly $x$ of them are defective. The number of defective chips, $x$, can be modeled as a single observation from a binomial distribution, $B(n, p)$, where the number of trials $n$ is known.\n\nThe variance of the underlying Bernoulli trial (i.e., the outcome for a single chip being defective or not) is given by the function $g(p) = p(1-p)$. Your task is to find the Maximum Likelihood Estimator (MLE) for this variance, $p(1-p)$, based on the observed data $x$ and the known sample size $n$. Express your answer as a symbolic expression in terms of $x$ and $n$.", "solution": "We model $x$ as a single observation from $B(n,p)$ with known $n$ and unknown $p \\in [0,1]$. The likelihood function for $p$ given $x$ is\n$$\nL(p \\mid x) = \\binom{n}{x} p^{x} (1-p)^{n-x}.\n$$\nMaximizing $L$ is equivalent to maximizing the log-likelihood\n$$\n\\ell(p) = \\ln L(p \\mid x) = \\ln \\binom{n}{x} + x \\ln p + (n-x) \\ln(1-p).\n$$\nDifferentiate with respect to $p$ and set to zero:\n$$\n\\frac{d\\ell}{dp} = \\frac{x}{p} - \\frac{n-x}{1-p} = 0.\n$$\nSolving,\n$$\n\\frac{x}{p} = \\frac{n-x}{1-p} \\;\\;\\Longrightarrow\\;\\; x(1-p) = (n-x)p \\;\\;\\Longrightarrow\\;\\; x - np = 0 \\;\\;\\Longrightarrow\\;\\; \\hat{p} = \\frac{x}{n}.\n$$\nThe second derivative is\n$$\n\\frac{d^{2}\\ell}{dp^{2}} = -\\frac{x}{p^{2}} - \\frac{n-x}{(1-p)^{2}} < 0\n$$\nfor $p \\in (0,1)$ and $x \\in \\{0,1,\\dots,n\\}$, confirming a maximum. For boundary cases $x=0$ or $x=n$, the maximizers are $\\hat{p}=0$ and $\\hat{p}=1$, respectively, which are also given by $\\hat{p}=x/n$.\n\nWe seek the MLE of $g(p) = p(1-p)$, the Bernoulli variance. By the invariance property of maximum likelihood estimators, the MLE of $g(p)$ is $g(\\hat{p})$. Therefore,\n$$\n\\widehat{g(p)} \\;=\\; \\hat{p}\\bigl(1-\\hat{p}\\bigr) \\;=\\; \\frac{x}{n}\\left(1-\\frac{x}{n}\\right) \\;=\\; \\frac{x(n-x)}{n^{2}}.\n$$\nThis expression also correctly yields $0$ when $x=0$ or $x=n$.", "answer": "$$\\boxed{\\frac{x(n-x)}{n^{2}}}$$", "id": "1925576"}, {"introduction": "Finding an MLE does not always involve differentiating the log-likelihood function. This problem explores a scenario with a uniform distribution where the MLE is determined by directly inspecting the likelihood function, a common situation for parameters that define the support of a distribution [@problem_id:1925562]. This exercise demonstrates that the invariance property holds true regardless of how the initial MLE is found, highlighting the generality and robustness of the principle.", "problem": "Let $X_1, X_2, \\dots, X_n$ be a random sample of size $n$ drawn from a continuous uniform distribution on the interval $(0, \\theta)$, where $\\theta > 0$ is an unknown parameter. The probability density function for any single observation $X_i$ is given by $f(x | \\theta) = \\frac{1}{\\theta}$ for $0 < x < \\theta$, and $f(x|\\theta) = 0$ otherwise.\n\nLet $X_{(n)}$ denote the maximum value in the sample, i.e., $X_{(n)} = \\max(X_1, X_2, \\dots, X_n)$.\n\nDetermine the Maximum Likelihood Estimator (MLE) for the variance of the distribution. Express your answer in terms of $X_{(n)}$ and numerical constants.", "solution": "We begin by writing the joint likelihood of the sample. For independent observations from the Uniform distribution on $(0,\\theta)$,\n$$\nL(\\theta \\mid x_{1},\\dots,x_{n})=\\prod_{i=1}^{n} f(x_{i}\\mid \\theta)=\\prod_{i=1}^{n}\\frac{1}{\\theta}\\,\\mathbf{1}_{\\{0<x_{i}<\\theta\\}}=\\theta^{-n}\\,\\mathbf{1}_{\\{\\theta\\geq x_{(n)}\\}},\n$$\nwhere $x_{(n)}=\\max\\{x_{1},\\dots,x_{n}\\}$. For fixed data, $L(\\theta)$ is proportional to $\\theta^{-n}$ on the interval $[x_{(n)},\\infty)$ and zero otherwise. Since $\\theta^{-n}$ is strictly decreasing in $\\theta$ for $\\theta>0$, the likelihood is maximized at the smallest allowable value of $\\theta$, namely\n$$\n\\hat{\\theta}_{\\text{MLE}}=X_{(n)}.\n$$\n\nNext, compute the variance of a single Uniform$(0,\\theta)$ variable. Using the definitions,\n$$\n\\mathbb{E}[X]=\\int_{0}^{\\theta} x\\,\\frac{1}{\\theta}\\,dx=\\frac{1}{\\theta}\\cdot\\frac{\\theta^{2}}{2}=\\frac{\\theta}{2},\\quad\n\\mathbb{E}[X^{2}]=\\int_{0}^{\\theta} x^{2}\\,\\frac{1}{\\theta}\\,dx=\\frac{1}{\\theta}\\cdot\\frac{\\theta^{3}}{3}=\\frac{\\theta^{2}}{3}.\n$$\nTherefore,\n$$\n\\operatorname{Var}(X)=\\mathbb{E}[X^{2}]-(\\mathbb{E}[X])^{2}=\\frac{\\theta^{2}}{3}-\\left(\\frac{\\theta}{2}\\right)^{2}=\\frac{\\theta^{2}}{12}.\n$$\n\nBy the invariance property of maximum likelihood estimators, if $g(\\theta)$ is a function of the parameter, then the MLE of $g(\\theta)$ is $g(\\hat{\\theta}_{\\text{MLE}})$. Taking $g(\\theta)=\\theta^{2}/12$, the MLE for the variance is\n$$\n\\widehat{\\operatorname{Var}}_{\\text{MLE}}=\\frac{\\hat{\\theta}_{\\text{MLE}}^{2}}{12}=\\frac{X_{(n)}^{2}}{12}.\n$$", "answer": "$$\\boxed{\\frac{X_{(n)}^{2}}{12}}$$", "id": "1925562"}, {"introduction": "The true power of the invariance property becomes evident in the context of complex, multi-parameter models used in real-world data analysis. This advanced problem tackles a Zero-Inflated Poisson (ZIP) distribution, a model frequently used in fields like ecology and economics to handle datasets with excess zeros [@problem_id:1925553]. By applying the invariance principle, we can derive the MLE for the distribution's overall variance—a complex function of the model's parameters—showcasing the property's elegance and utility in sophisticated statistical modeling.", "problem": "A random variable $Y$ is said to follow a Zero-Inflated Poisson (ZIP) distribution with parameters $\\pi \\in [0, 1)$ and $\\lambda > 0$ if its probability mass function (PMF) is given by:\n$$\nP(Y=y) = \n\\begin{cases}\n\\pi + (1-\\pi)\\exp(-\\lambda) & \\text{if } y=0 \\\\\n(1-\\pi) \\frac{\\lambda^y \\exp(-\\lambda)}{y!} & \\text{if } y \\in \\{1, 2, 3, \\ldots\\}\n\\end{cases}\n$$\nHere, $\\pi$ represents the probability of an excess zero count that is not from the Poisson process, and $\\lambda$ is the mean of the underlying Poisson distribution.\n\nConsider a random sample $y_1, y_2, \\ldots, y_n$ drawn from a ZIP distribution. Let $\\bar{y} = \\frac{1}{n} \\sum_{i=1}^n y_i$ be the sample mean. The parameters $\\pi$ and $\\lambda$ are unknown. Let $\\hat{\\pi}$ and $\\hat{\\lambda}$ denote their respective Maximum Likelihood Estimators (MLEs).\n\nUsing the invariance property of MLEs, find the MLE for the variance of the distribution, $\\sigma^2 = \\text{Var}(Y)$. Express your answer as a closed-form analytic expression in terms of the sample mean $\\bar{y}$ and the MLE $\\hat{\\lambda}$.", "solution": "For a Zero-Inflated Poisson (ZIP) random variable $Y$ with parameters $\\pi \\in [0,1)$ and $\\lambda > 0$, write $Y = Z X$ where $Z \\sim \\text{Bernoulli}(1-\\pi)$ and, conditional on $Z=1$, $X \\sim \\text{Poisson}(\\lambda)$; otherwise $Y=0$. Using this mixture representation:\n$$\n\\mathbb{E}[Y] = (1 - \\pi)\\lambda \\equiv \\mu,\n$$\nand since for $X \\sim \\text{Poisson}(\\lambda)$, $\\mathbb{E}[X^{2}] = \\lambda + \\lambda^{2}$, we have\n$$\n\\mathbb{E}[Y^{2}] = (1 - \\pi)\\mathbb{E}[X^{2}] = (1 - \\pi)(\\lambda + \\lambda^{2}).\n$$\nTherefore,\n$$\n\\operatorname{Var}(Y) = \\mathbb{E}[Y^{2}] - \\{\\mathbb{E}[Y]\\}^{2} = (1 - \\pi)\\lambda + (1 - \\pi)\\lambda^{2} - (1 - \\pi)^{2}\\lambda^{2} = (1 - \\pi)\\lambda\\{1 + \\pi\\lambda\\}.\n$$\nEquivalently, in terms of $\\mu = (1 - \\pi)\\lambda$, note that $\\pi\\lambda = \\lambda - \\mu$, so\n$$\n\\sigma^{2} = \\operatorname{Var}(Y) = \\mu\\{1 + \\lambda - \\mu\\}.\n$$\n\nBy the invariance property of MLEs, the MLE of $\\sigma^{2}$ is obtained by plugging the MLEs of the parameters into this function. To express the result in terms of the sample mean $\\bar{y}$ and $\\hat{\\lambda}$, we show that the MLE of $\\mu$ equals $\\bar{y}$.\n\nReparameterize by $\\mu = (1 - \\pi)\\lambda$ so that $\\pi = 1 - \\mu/\\lambda$. Let $n_{0} = \\sum_{i=1}^{n} \\mathbf{1}\\{y_{i}=0\\}$. The log-likelihood (up to terms not involving $\\mu$) is\n$$\n\\ell(\\mu,\\lambda) = n_{0}\\ln\\!\\left(1 - \\frac{\\mu}{\\lambda}\\{1 - \\exp(-\\lambda)\\}\\right) + (n - n_{0})\\ln \\mu + C(\\lambda,y).\n$$\nDifferentiating with respect to $\\mu$ and setting to zero gives\n$$\n\\frac{\\partial \\ell}{\\partial \\mu} = -\\,\\frac{n_{0}\\,\\{(1/\\lambda)(1 - \\exp(-\\lambda))\\}}{1 - \\frac{\\mu}{\\lambda}(1 - \\exp(-\\lambda))} + \\frac{n - n_{0}}{\\mu} = 0,\n$$\nwhich yields\n$$\n(n - n_{0})\\lambda = \\mu\\,n\\,(1 - \\exp(-\\lambda)).\n$$\nFrom the score equation for $\\lambda$, one obtains\n$$\n\\frac{n\\bar{y}}{\\lambda} = \\frac{n - n_{0}}{1 - \\exp(-\\lambda)}.\n$$\nCombining the two identities gives $\\mu = \\bar{y}$. Therefore, at the MLEs, $\\hat{\\mu} = (1 - \\hat{\\pi})\\hat{\\lambda} = \\bar{y}$, i.e.,\n$$\n\\hat{\\pi} = 1 - \\frac{\\bar{y}}{\\hat{\\lambda}}.\n$$\n\nApplying invariance to $\\sigma^{2} = \\mu(1 + \\lambda - \\mu)$ yields\n$$\n\\hat{\\sigma}^{2} = \\hat{\\mu}\\{1 + \\hat{\\lambda} - \\hat{\\mu}\\} = \\bar{y}\\{1 + \\hat{\\lambda} - \\bar{y}\\}.\n$$\nThis is a closed-form analytic expression in terms of $\\bar{y}$ and $\\hat{\\lambda}$.", "answer": "$$\\boxed{\\bar{y}\\left(1+\\hat{\\lambda}-\\bar{y}\\right)}$$", "id": "1925553"}]}