{"hands_on_practices": [{"introduction": "A central question in estimation theory is \"how good is my estimator?\". The Cramér-Rao Lower Bound (CRLB) provides a powerful answer by establishing a theoretical floor for the variance of any unbiased estimator. This exercise [@problem_id:1914827] walks you through the essential process of calculating the CRLB for the rate parameter of an exponential distribution and using it to evaluate the efficiency of a proposed estimator. Mastering this skill allows you to quantitatively benchmark an estimator's performance against the best possible outcome.", "problem": "The lifetime of a certain type of electronic component is modeled by an exponential distribution with a probability density function (PDF) given by $f(x; \\lambda) = \\lambda \\exp(-\\lambda x)$ for $x > 0$. Here, $\\lambda > 0$ is the constant failure rate. A random sample of $n$ such components, denoted by $X_1, X_2, \\dots, X_n$, is tested, and their lifetimes are recorded.\n\nAn engineer proposes the following estimator for the failure rate $\\lambda$:\n$$\n\\hat{\\lambda} = \\frac{n-1}{\\sum_{i=1}^n X_i}\n$$\nIt is given that for a sample size $n > 1$, this estimator is unbiased for $\\lambda$.\n\nAssuming a sample size of $n > 2$, calculate the efficiency of this estimator $\\hat{\\lambda}$. The efficiency is defined as the ratio of the Cramér-Rao Lower Bound (CRLB) for an unbiased estimator of $\\lambda$ to the actual variance of the estimator $\\hat{\\lambda}$. Express your final answer as a function of $n$.", "solution": "Let $X_{1},\\dots,X_{n}$ be iid with density $f(x;\\lambda)=\\lambda\\exp(-\\lambda x)$ for $x0$. The sum $S=\\sum_{i=1}^{n}X_{i}$ has a Gamma distribution with shape $n$ and rate $\\lambda$, i.e., with density\n$$\ng(s;n,\\lambda)=\\frac{\\lambda^{n}}{\\Gamma(n)}s^{n-1}\\exp(-\\lambda s),\\quad s0.\n$$\nThe proposed estimator is $\\hat{\\lambda}=(n-1)/S$. For $n2$, we compute $\\operatorname{Var}(\\hat{\\lambda})$ by using moments of $S^{-1}$. For $mn$,\n$$\n\\mathbb{E}\\!\\left[S^{-m}\\right]=\\int_{0}^{\\infty}s^{-m}g(s;n,\\lambda)\\,ds=\\frac{\\lambda^{m}\\Gamma(n-m)}{\\Gamma(n)}.\n$$\nThus,\n$$\n\\mathbb{E}\\!\\left[\\frac{1}{S}\\right]=\\frac{\\lambda\\,\\Gamma(n-1)}{\\Gamma(n)}=\\frac{\\lambda}{n-1},\\qquad\n\\mathbb{E}\\!\\left[\\frac{1}{S^{2}}\\right]=\\frac{\\lambda^{2}\\Gamma(n-2)}{\\Gamma(n)}=\\frac{\\lambda^{2}}{(n-1)(n-2)}.\n$$\nTherefore,\n$$\n\\operatorname{Var}\\!\\left(\\frac{1}{S}\\right)=\\mathbb{E}\\!\\left[\\frac{1}{S^{2}}\\right]-\\left(\\mathbb{E}\\!\\left[\\frac{1}{S}\\right]\\right)^{2}\n=\\frac{\\lambda^{2}}{(n-1)(n-2)}-\\frac{\\lambda^{2}}{(n-1)^{2}}\n=\\frac{\\lambda^{2}}{(n-1)^{2}(n-2)}.\n$$\nIt follows that\n$$\n\\operatorname{Var}(\\hat{\\lambda})=\\operatorname{Var}\\!\\left(\\frac{n-1}{S}\\right)=(n-1)^{2}\\operatorname{Var}\\!\\left(\\frac{1}{S}\\right)=\\frac{\\lambda^{2}}{n-2}.\n$$\n\nNext, compute the Cramér-Rao Lower Bound for unbiased estimators of $\\lambda$. The log-likelihood for the sample is\n$$\n\\ell(\\lambda)=\\sum_{i=1}^{n}\\ln f(X_{i};\\lambda)=n\\ln\\lambda-\\lambda\\sum_{i=1}^{n}X_{i}.\n$$\nThen\n$$\n\\frac{\\partial^{2}\\ell}{\\partial\\lambda^{2}}=-\\frac{n}{\\lambda^{2}},\\quad\n\\mathcal{I}_{n}(\\lambda)=-\\mathbb{E}\\!\\left[\\frac{\\partial^{2}\\ell}{\\partial\\lambda^{2}}\\right]=\\frac{n}{\\lambda^{2}}.\n$$\nHence the CRLB for any unbiased estimator of $\\lambda$ is\n$$\n\\operatorname{CRLB}=\\frac{1}{\\mathcal{I}_{n}(\\lambda)}=\\frac{\\lambda^{2}}{n}.\n$$\n\nThe efficiency, defined as $\\operatorname{CRLB}/\\operatorname{Var}(\\hat{\\lambda})$, is therefore\n$$\n\\text{efficiency}=\\frac{\\lambda^{2}/n}{\\lambda^{2}/(n-2)}=\\frac{n-2}{n}.\n$$\nThis depends only on $n$ and is valid for $n2$.", "answer": "$$\\boxed{\\frac{n-2}{n}}$$", "id": "1914827"}, {"introduction": "While unbiasedness is a desirable property for an estimator, it is not the only measure of quality. The Mean Squared Error ($MSE$) provides a more complete picture by combining both an estimator's variance and its bias. This practice [@problem_id:1914869] explores the crucial bias-variance trade-off by comparing the performance of a biased but intuitive Maximum Likelihood Estimator with its corrected, unbiased counterpart. You will see firsthand how a small amount of bias can sometimes lead to a more efficient estimator overall.", "problem": "Let $X_1, X_2, \\dots, X_n$ be a random sample drawn from a continuous Uniform distribution on the interval $[0, \\theta]$, where $\\theta  0$ is an unknown parameter.\nThe Maximum Likelihood Estimator (MLE) for $\\theta$ is given by $\\hat{\\theta}_{MLE} = X_{(n)}$, where $X_{(n)}$ is the maximum value in the sample, i.e., $X_{(n)} = \\max\\{X_1, X_2, \\dots, X_n\\}$.\nThis estimator is known to be biased. A corresponding unbiased estimator for $\\theta$ is given by $\\hat{\\theta}_{U} = \\frac{n+1}{n}X_{(n)}$.\n\nTo compare the performance of these two estimators, we can use the Mean Squared Error (MSE), which for a generic estimator $\\hat{\\theta}$ is defined as $\\text{MSE}(\\hat{\\theta}) = E[(\\hat{\\theta} - \\theta)^2]$.\n\nCalculate the ratio of the MSE of the biased estimator to the MSE of the unbiased estimator, $\\frac{\\text{MSE}(\\hat{\\theta}_{MLE})}{\\text{MSE}(\\hat{\\theta}_{U})}$. Express your answer as a function of the sample size $n$.", "solution": "Let $X_{(n)}=\\max\\{X_{1},\\dots,X_{n}\\}$. For $X_{i}\\sim \\text{Uniform}(0,\\theta)$ i.i.d., the density of $X_{(n)}$ is\n$$\nf_{X_{(n)}}(x)=\\frac{n}{\\theta^{n}}x^{n-1},\\quad 0x\\theta.\n$$\nEquivalently, write $X_{(n)}=\\theta Z$ where $Z\\sim \\text{Beta}(n,1)$. Using the moments of the Beta distribution,\n$$\n\\mathbb{E}[Z]=\\frac{n}{n+1},\\qquad \\operatorname{Var}(Z)=\\frac{n}{(n+1)^{2}(n+2)}.\n$$\nBy scaling, this yields\n$$\n\\mathbb{E}[X_{(n)}]=\\theta\\,\\frac{n}{n+1},\\qquad \\operatorname{Var}(X_{(n)})=\\theta^{2}\\,\\frac{n}{(n+1)^{2}(n+2)}.\n$$\n\nFor the MLE $\\hat{\\theta}_{\\text{MLE}}=X_{(n)}$, the bias is\n$$\n\\operatorname{Bias}(\\hat{\\theta}_{\\text{MLE}})=\\mathbb{E}[X_{(n)}]-\\theta=\\theta\\left(\\frac{n}{n+1}-1\\right)=-\\frac{\\theta}{n+1},\n$$\nso\n$$\n\\operatorname{MSE}(\\hat{\\theta}_{\\text{MLE}})=\\operatorname{Var}(X_{(n)})+\\operatorname{Bias}(\\hat{\\theta}_{\\text{MLE}})^{2}\n=\\theta^{2}\\left(\\frac{n}{(n+1)^{2}(n+2)}+\\frac{1}{(n+1)^{2}}\\right).\n$$\nCombine the terms:\n$$\n\\operatorname{MSE}(\\hat{\\theta}_{\\text{MLE}})=\\frac{\\theta^{2}}{(n+1)^{2}}\\left(\\frac{n}{n+2}+1\\right)\n=\\frac{\\theta^{2}}{(n+1)^{2}}\\cdot\\frac{2n+2}{n+2}\n=\\theta^{2}\\,\\frac{2}{(n+1)(n+2)}.\n$$\n\nFor the unbiased estimator $\\hat{\\theta}_{U}=\\frac{n+1}{n}X_{(n)}$, note that it is unbiased by construction. Hence its MSE equals its variance:\n$$\n\\operatorname{MSE}(\\hat{\\theta}_{U})=\\operatorname{Var}\\!\\left(\\frac{n+1}{n}X_{(n)}\\right)\n=\\left(\\frac{n+1}{n}\\right)^{2}\\operatorname{Var}(X_{(n)})\n=\\left(\\frac{n+1}{n}\\right)^{2}\\theta^{2}\\,\\frac{n}{(n+1)^{2}(n+2)}\n=\\theta^{2}\\,\\frac{1}{n(n+2)}.\n$$\n\nTherefore, the ratio of MSEs is\n$$\n\\frac{\\operatorname{MSE}(\\hat{\\theta}_{\\text{MLE}})}{\\operatorname{MSE}(\\hat{\\theta}_{U})}\n=\\frac{\\theta^{2}\\,\\frac{2}{(n+1)(n+2)}}{\\theta^{2}\\,\\frac{1}{n(n+2)}}\n=\\frac{2n}{n+1}.\n$$", "answer": "$$\\boxed{\\frac{2n}{n+1}}$$", "id": "1914869"}, {"introduction": "Instead of just evaluating existing estimators, how can we systematically find the *best* possible unbiased estimator? The Lehmann-Scheffé theorem offers a powerful and constructive path toward this goal. This hands-on problem [@problem_id:1914858] guides you through the process of finding the Uniformly Minimum-Variance Unbiased Estimator (UMVUE) for a parameter in a Poisson process. This practice demonstrates how to leverage the properties of complete sufficient statistics to derive an estimator that is optimal within the class of all unbiased estimators.", "problem": "Let $X_1, X_2, \\ldots, X_n$ be a random sample of size $n$ from a Poisson distribution with an unknown parameter $\\lambda  0$. The probability mass function for a single observation $X$ is given by:\n$$P(X=k) = \\frac{e^{-\\lambda}\\lambda^k}{k!}, \\quad \\text{for } k = 0, 1, 2, \\ldots$$\nA key metric for a process following this distribution is the probability of observing a zero-count event. Let this probability be denoted by $\\theta = P(X=0)$.\n\nYour task is to find the Uniformly Minimum-Variance Unbiased Estimator (UMVUE) for $\\theta$. Express your final answer as a function of the total sum of the observations, $T = \\sum_{i=1}^{n} X_i$.", "solution": "Let $X_{1},\\ldots,X_{n}$ be i.i.d. $\\operatorname{Poisson}(\\lambda)$ and let $\\theta=P(X=0)=\\exp(-\\lambda)$. We seek the UMVUE for $\\theta$ as a function of $T=\\sum_{i=1}^{n}X_{i}$.\n\nFirst, identify a complete sufficient statistic. By the additivity of independent Poisson random variables, $T\\sim\\operatorname{Poisson}(n\\lambda)$. By the factorization theorem, $T$ is sufficient for $\\lambda$. To see completeness, suppose $g$ is any function with $\\mathbb{E}_{\\lambda}[g(T)]=0$ for all $\\lambda0$. Then\n$$\n\\sum_{t=0}^{\\infty}g(t)\\,\\exp(-n\\lambda)\\frac{(n\\lambda)^{t}}{t!}=0\\quad\\text{for all }\\lambda0.\n$$\nMultiplying by $\\exp(n\\lambda)$ gives\n$$\n\\sum_{t=0}^{\\infty}g(t)\\,\\frac{(n\\lambda)^{t}}{t!}=0\\quad\\text{for all }\\lambda0,\n$$\nwhich is the zero power series in $\\lambda$, hence all coefficients vanish and $g(t)=0$ for all $t$. Thus $T$ is complete sufficient.\n\nNext, take an unbiased estimator of $\\theta$: the indicator $U=\\mathbf{1}\\{X_{1}=0\\}$ satisfies\n$$\n\\mathbb{E}_{\\lambda}[U]=P_{\\lambda}(X_{1}=0)=\\exp(-\\lambda)=\\theta.\n$$\nBy the Lehmann–Scheffé theorem, the UMVUE is $\\mathbb{E}[U\\mid T]$. Compute $\\mathbb{E}[U\\mid T=t]=P(X_{1}=0\\mid T=t)$. Conditional on $T=t$, the vector $(X_{1},\\ldots,X_{n})$ has the multinomial distribution with $t$ trials and equal cell probabilities $1/n$, so\n$$\nP(X_{1}=0\\mid T=t)=\\left(1-\\frac{1}{n}\\right)^{t}.\n$$\nTherefore, the candidate UMVUE is\n$$\n\\delta(T)=\\left(1-\\frac{1}{n}\\right)^{T}.\n$$\nVerify unbiasedness directly via the probability generating function of $T\\sim\\operatorname{Poisson}(n\\lambda)$: for $s\\in\\mathbb{R}$,\n$$\n\\mathbb{E}\\!\\left[s^{T}\\right]=\\exp\\!\\left(n\\lambda(s-1)\\right).\n$$\nWith $s=1-\\frac{1}{n}$,\n$$\n\\mathbb{E}\\!\\left[\\left(1-\\frac{1}{n}\\right)^{T}\\right]=\\exp\\!\\left(n\\lambda\\left(1-\\frac{1}{n}-1\\right)\\right)=\\exp(-\\lambda)=\\theta.\n$$\nSince $\\delta(T)$ is a function of the complete sufficient statistic $T$ and is unbiased for $\\theta$, it is the UMVUE.", "answer": "$$\\boxed{\\left(1-\\frac{1}{n}\\right)^{T}}$$", "id": "1914858"}]}