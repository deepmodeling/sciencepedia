{"hands_on_practices": [{"introduction": "The sample mean is a cornerstone of statistical estimation, valued for its property of being an unbiased estimator of the population mean $\\mu$. This exercise invites you to explore what happens when we deviate from its standard definition. By analyzing a generally scaled version of the sample mean [@problem_id:1900487], you will uncover the precise condition required for a linear estimator to be unbiased, reinforcing the fundamental principles of estimator design.", "problem": "In statistical analysis, an estimator is a rule for calculating an estimate of a given quantity based on observed data. The bias of an estimator is the difference between this estimator's expected value and the true value of the parameter being estimated.\n\nConsider a random sample of size $n$, denoted by $X_1, X_2, \\ldots, X_n$. These are independent and identically distributed random variables drawn from a population with a true mean $\\mu$ and a finite, non-zero variance $\\sigma^2$.\n\nAn analyst proposes a new estimator, $\\hat{\\mu}$, for the population mean $\\mu$, defined as:\n$$\n\\hat{\\mu} = \\frac{1}{n+a} \\sum_{i=1}^{n} X_i\n$$\nwhere $a$ is a known positive constant.\n\nDetermine the bias of this estimator, $\\text{Bias}(\\hat{\\mu})$. Express your answer as an analytic expression in terms of $\\mu$, $n$, and $a$.", "solution": "The bias of an estimator $\\hat{\\theta}$ for a parameter $\\theta$ is defined as $\\text{Bias}(\\hat{\\theta}) = E[\\hat{\\theta}] - \\theta$. In this problem, the parameter is the population mean $\\mu$, and the estimator is $\\hat{\\mu} = \\frac{1}{n+a} \\sum_{i=1}^{n} X_i$. Therefore, we need to calculate $\\text{Bias}(\\hat{\\mu}) = E[\\hat{\\mu}] - \\mu$.\n\nFirst, let's find the expected value of the estimator $\\hat{\\mu}$.\n$$\nE[\\hat{\\mu}] = E\\left[ \\frac{1}{n+a} \\sum_{i=1}^{n} X_i \\right]\n$$\nUsing the linearity property of expectation, we can pull the constant factor $\\frac{1}{n+a}$ out of the expectation, and the expectation of the sum is the sum of the expectations.\n$$\nE[\\hat{\\mu}] = \\frac{1}{n+a} E\\left[ \\sum_{i=1}^{n} X_i \\right] = \\frac{1}{n+a} \\sum_{i=1}^{n} E[X_i]\n$$\nWe are given that each observation $X_i$ is drawn from a population with a true mean $\\mu$. This means that for every $i$ from $1$ to $n$, the expected value of $X_i$ is $\\mu$.\n$$\nE[X_i] = \\mu \\quad \\text{for } i = 1, 2, \\ldots, n\n$$\nSubstituting this into our expression for $E[\\hat{\\mu}]$:\n$$\nE[\\hat{\\mu}] = \\frac{1}{n+a} \\sum_{i=1}^{n} \\mu\n$$\nThe sum consists of $n$ identical terms, each equal to $\\mu$. Therefore, the sum is $n\\mu$.\n$$\nE[\\hat{\\mu}] = \\frac{1}{n+a} (n\\mu) = \\frac{n\\mu}{n+a}\n$$\nNow we can compute the bias by subtracting the true parameter value $\\mu$ from the expected value of the estimator.\n$$\n\\text{Bias}(\\hat{\\mu}) = E[\\hat{\\mu}] - \\mu = \\frac{n\\mu}{n+a} - \\mu\n$$\nTo simplify this expression, we find a common denominator:\n$$\n\\text{Bias}(\\hat{\\mu}) = \\frac{n\\mu}{n+a} - \\frac{\\mu(n+a)}{n+a} = \\frac{n\\mu - (\\mu n + \\mu a)}{n+a}\n$$\n$$\n\\text{Bias}(\\hat{\\mu}) = \\frac{n\\mu - n\\mu - a\\mu}{n+a} = \\frac{-a\\mu}{n+a}\n$$\nThus, the bias of the estimator $\\hat{\\mu}$ is $-\\frac{a\\mu}{n+a}$. Note that the population variance $\\sigma^2$ was not needed for this calculation.", "answer": "$$\\boxed{-\\frac{a\\mu}{n+a}}$$", "id": "1900487"}, {"introduction": "When estimating a function of a parameter, such as $\\lambda^2$, a natural impulse is to apply the same function to a known estimator for $\\lambda$. This practice [@problem_id:1900434] challenges that intuition by examining the simple case of using $X^2$ to estimate $\\lambda^2$ from a single Poisson observation. It provides a concrete example of how non-linear transformations can introduce bias, illustrating the important statistical principle that the expectation of a function is generally not the function of the expectation, i.e., $\\mathbb{E}[f(X)] \\neq f(\\mathbb{E}[X])$.", "problem": "In a particle physics experiment, the number of rare decay events, $X$, observed in a fixed time interval is modeled by a Poisson distribution with an unknown mean rate $\\lambda  0$. An experimenter proposes an estimator for the quantity $\\lambda^2$, which is of theoretical interest. Based on a single observation $X$, the proposed estimator is $T = X^2$. The bias of an estimator for a parameter is defined as the difference between the estimator's expected value and the true value of the parameter. Determine the bias of the estimator $T$ for the parameter $\\lambda^2$. Express your answer as a function of $\\lambda$.", "solution": "Let $X \\sim \\text{Poisson}(\\lambda)$ with $\\lambda  0$, and consider the estimator $T = X^{2}$ for the parameter $\\lambda^{2}$. The bias of $T$ for $\\lambda^{2}$ is defined as\n$$\n\\text{Bias}(T) = \\mathbb{E}[T] - \\lambda^{2} = \\mathbb{E}[X^{2}] - \\lambda^{2}.\n$$\nUsing the identity that for any random variable $X$, $\\mathbb{E}[X^{2}] = \\operatorname{Var}(X) + \\left(\\mathbb{E}[X]\\right)^{2}$, and the properties of the Poisson distribution $\\mathbb{E}[X] = \\lambda$ and $\\operatorname{Var}(X) = \\lambda$, we obtain\n$$\n\\mathbb{E}[X^{2}] = \\lambda + \\lambda^{2}.\n$$\nTherefore, the bias is\n$$\n\\text{Bias}(T) = (\\lambda + \\lambda^{2}) - \\lambda^{2} = \\lambda.\n$$\nSo the estimator $T = X^{2}$ has bias equal to $\\lambda$ for estimating $\\lambda^{2}$.", "answer": "$$\\boxed{\\lambda}$$", "id": "1900434"}, {"introduction": "The Method of Moments (MOM) is a powerful and intuitive technique for constructing estimators based on matching sample moments to population moments. This exercise demonstrates that estimators derived from this method are not automatically unbiased, especially for non-linear functions of parameters. By calculating the bias of a MOM estimator for $\\theta^2$ [@problem_id:1900439], you will see how bias can depend on the sample size $n$, introducing the important concept of asymptotic unbiasedness, where bias vanishes as the sample size grows.", "problem": "Let $X_1, X_2, \\ldots, X_n$ be a random sample of size $n$ drawn from a continuous uniform distribution on the interval $[0, \\theta]$, where $\\theta  0$ is an unknown parameter. The goal is to estimate the parameter $\\theta^2$.\n\nDetermine the bias of the method of moments estimator of $\\theta^2$. Express your answer as a function of $\\theta$ and the sample size $n$.", "solution": "Let $X_{1},\\ldots,X_{n}$ be i.i.d. $\\operatorname{Uniform}(0,\\theta)$. The method of moments equates the sample mean $\\bar{X}$ to the population mean. For $\\operatorname{Uniform}(0,\\theta)$,\n$$\n\\mathbb{E}[X]=\\frac{\\theta}{2}, \\quad \\operatorname{Var}(X)=\\frac{\\theta^{2}}{12}.\n$$\nEquating $\\bar{X}$ to $\\mathbb{E}[X]$ gives the method of moments estimator for $\\theta$:\n$$\n\\hat{\\theta}_{\\text{MOM}}=2\\bar{X}.\n$$\nTo estimate $\\theta^{2}$, use the plug-in estimator\n$$\n\\hat{\\theta^{2}}_{\\text{MOM}}=(\\hat{\\theta}_{\\text{MOM}})^{2}=4\\bar{X}^{2}.\n$$\nThe bias of $\\hat{\\theta^{2}}_{\\text{MOM}}$ is\n$$\n\\operatorname{Bias}(\\hat{\\theta^{2}}_{\\text{MOM}})=\\mathbb{E}[4\\bar{X}^{2}]-\\theta^{2}.\n$$\nUsing $\\mathbb{E}[\\bar{X}]=\\frac{\\theta}{2}$ and $\\operatorname{Var}(\\bar{X})=\\frac{\\operatorname{Var}(X)}{n}=\\frac{\\theta^{2}}{12n}$, and the identity $\\mathbb{E}[\\bar{X}^{2}]=\\operatorname{Var}(\\bar{X})+(\\mathbb{E}[\\bar{X}])^{2}$, we obtain\n$$\n\\mathbb{E}[4\\bar{X}^{2}]=4\\left(\\frac{\\theta^{2}}{12n}+\\left(\\frac{\\theta}{2}\\right)^{2}\\right)=\\frac{\\theta^{2}}{3n}+\\theta^{2}.\n$$\nTherefore,\n$$\n\\operatorname{Bias}(\\hat{\\theta^{2}}_{\\text{MOM}})=\\left(\\frac{\\theta^{2}}{3n}+\\theta^{2}\\right)-\\theta^{2}=\\frac{\\theta^{2}}{3n}.\n$$", "answer": "$$\\boxed{\\frac{\\theta^{2}}{3 n}}$$", "id": "1900439"}]}