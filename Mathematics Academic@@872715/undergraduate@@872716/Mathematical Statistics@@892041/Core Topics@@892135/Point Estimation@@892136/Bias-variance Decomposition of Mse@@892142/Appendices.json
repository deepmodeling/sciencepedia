{"hands_on_practices": [{"introduction": "To begin our exploration, let's consider an extreme case: an estimator that completely ignores the data and provides a constant value. While this might seem counterintuitive, analyzing such an estimator is a powerful pedagogical tool. It allows us to isolate the concepts of bias and variance, demonstrating how an estimator can have zero variance but still perform poorly due to high bias, providing a clear illustration of why minimizing variance alone is not sufficient. [@problem_id:1900788]", "problem": "Suppose we are interested in estimating a single, unknown real parameter $\\theta$. A particularly simple estimator is proposed, which completely ignores any sample data and always outputs the same value. Let this constant estimator be denoted by $\\hat{\\theta} = 10$.\n\nYour task is to analyze the performance of this estimator. Find expressions for the following three quantities in terms of the true parameter $\\theta$:\n1. The bias of the estimator, $\\text{Bias}(\\hat{\\theta})$.\n2. The variance of the estimator, $\\text{Var}(\\hat{\\theta})$.\n3. The Mean Squared Error (MSE) of the estimator, $\\text{MSE}(\\hat{\\theta})$.\n\nPresent your final answer as three distinct expressions for the bias, variance, and MSE, in that order, using a row matrix.", "solution": "We are given a constant estimator that ignores any sample data and always returns the fixed value $\\hat{\\theta}=10$. Since the estimator is a constant function of the data, it is a degenerate random variable that takes the value $10$ with probability $1$ under any sampling distribution. The expectation and variance of such an estimator are computed with respect to the sampling distribution, while the parameter $\\theta$ is a fixed (nonrandom) constant.\n\nBy definition, the bias of an estimator is\n$$\n\\text{Bias}(\\hat{\\theta})= \\mathbb{E}[\\hat{\\theta}] - \\theta.\n$$\nSince $\\hat{\\theta}=10$ with probability $1$, we have $\\mathbb{E}[\\hat{\\theta}] = 10$, hence\n$$\n\\text{Bias}(\\hat{\\theta}) = 10 - \\theta.\n$$\n\nThe variance of the estimator is\n$$\n\\text{Var}(\\hat{\\theta}) = \\mathbb{E}\\big[(\\hat{\\theta} - \\mathbb{E}[\\hat{\\theta}])^{2}\\big].\n$$\nUsing $\\hat{\\theta}=10$ and $\\mathbb{E}[\\hat{\\theta}]=10$, we obtain\n$$\n\\text{Var}(\\hat{\\theta}) = \\mathbb{E}[(10-10)^{2}] = 0.\n$$\n\nThe Mean Squared Error (MSE) is defined by\n$$\n\\text{MSE}(\\hat{\\theta}) = \\mathbb{E}\\big[(\\hat{\\theta} - \\theta)^{2}\\big].\n$$\nBecause $\\hat{\\theta}=10$ deterministically and $\\theta$ is a fixed constant, we have\n$$\n\\text{MSE}(\\hat{\\theta}) = \\mathbb{E}[(10 - \\theta)^{2}] = (10 - \\theta)^{2}.\n$$\nEquivalently, using the identity $\\text{MSE}(\\hat{\\theta}) = \\text{Bias}(\\hat{\\theta})^{2} + \\text{Var}(\\hat{\\theta})$, we get\n$$\n\\text{MSE}(\\hat{\\theta}) = (10 - \\theta)^{2} + 0 = (10 - \\theta)^{2}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}10 - \\theta  0  (10 - \\theta)^{2}\\end{pmatrix}}$$", "id": "1900788"}, {"introduction": "In practice, we often have choices about which data to include or how to weight it. This exercise explores the consequences of selectively ignoring data by comparing the standard sample mean to an alternative estimator that uses only a subset of the observations. By calculating and comparing their Mean Squared Errors ($MSE$), you will see firsthand how the efficiency of an unbiased estimator, captured by its variance, is critical for its overall performance and that using all available information appropriately generally leads to a lower $MSE$. [@problem_id:1900727]", "problem": "A researcher is analyzing a process where measurements are taken in triplets. Let $X_1, X_2, X_3$ represent an independent and identically distributed (i.i.d.) random sample of size $n=3$ drawn from a population with a true mean $\\mu$ and a finite, non-zero variance $\\sigma^2$.\n\nThe standard estimator for the mean is the sample mean, $\\bar{X} = \\frac{1}{3}(X_1 + X_2 + X_3)$. However, suspecting that the middle measurement might be less reliable, the researcher proposes an alternative estimator, $\\hat{\\mu}$, which is constructed by averaging only the first and third measurements:\n$$\n\\hat{\\mu} = \\frac{1}{2}(X_1 + X_3)\n$$\nTo evaluate the performance of this new estimator, you are asked to compare their Mean Squared Error (MSE). The MSE of an estimator is a measure of its average squared difference from the true parameter, combining both its bias and variance.\n\nCalculate the ratio of the MSE of the proposed estimator $\\hat{\\mu}$ to the MSE of the standard sample mean $\\bar{X}$. Your final answer should be a numerical value.", "solution": "Let $X_{1},X_{2},X_{3}$ be i.i.d. with $\\mathbb{E}[X_{i}]=\\mu$ and $\\operatorname{Var}(X_{i})=\\sigma^{2}$.\n\nFor any estimator $T$, the mean squared error is $\\operatorname{MSE}(T)=\\operatorname{Var}(T)+\\{\\mathbb{E}[T]-\\mu\\}^{2}$.\n\nFirst, consider $\\hat{\\mu}=\\frac{1}{2}(X_{1}+X_{3})$. Its expectation is\n$$\n\\mathbb{E}[\\hat{\\mu}]=\\frac{1}{2}\\left(\\mathbb{E}[X_{1}]+\\mathbb{E}[X_{3}]\\right)=\\frac{1}{2}(\\mu+\\mu)=\\mu,\n$$\nso the bias is zero and $\\operatorname{MSE}(\\hat{\\mu})=\\operatorname{Var}(\\hat{\\mu})$. Using independence and $\\operatorname{Var}(aY)=a^{2}\\operatorname{Var}(Y)$,\n$$\n\\operatorname{Var}(\\hat{\\mu})=\\operatorname{Var}\\!\\left(\\frac{1}{2}(X_{1}+X_{3})\\right)=\\frac{1}{4}\\left(\\operatorname{Var}(X_{1})+\\operatorname{Var}(X_{3})\\right)=\\frac{1}{4}( \\sigma^{2}+\\sigma^{2})=\\frac{\\sigma^{2}}{2}.\n$$\n\nNext, consider $\\bar{X}=\\frac{1}{3}(X_{1}+X_{2}+X_{3})$. Its expectation is $\\mathbb{E}[\\bar{X}]=\\mu$, so $\\operatorname{MSE}(\\bar{X})=\\operatorname{Var}(\\bar{X})$. Using independence,\n$$\n\\operatorname{Var}(\\bar{X})=\\operatorname{Var}\\!\\left(\\frac{1}{3}(X_{1}+X_{2}+X_{3})\\right)=\\frac{1}{9}\\left(\\operatorname{Var}(X_{1})+\\operatorname{Var}(X_{2})+\\operatorname{Var}(X_{3})\\right)=\\frac{1}{9}(3\\sigma^{2})=\\frac{\\sigma^{2}}{3}.\n$$\n\nTherefore, the required ratio is\n$$\n\\frac{\\operatorname{MSE}(\\hat{\\mu})}{\\operatorname{MSE}(\\bar{X})}=\\frac{\\sigma^{2}/2}{\\sigma^{2}/3}=\\frac{3}{2}.\n$$", "answer": "$$\\boxed{\\frac{3}{2}}$$", "id": "1900727"}, {"introduction": "We now move to the heart of the bias-variance trade-off by examining an estimator that includes a tunable parameter, $\\alpha$, which directly introduces bias. This practice is crucial for understanding how a statistician might intentionally use a biased estimator in the hope of achieving a lower overall $MSE$. By deriving the $MSE$ as a function of the true parameter $\\lambda$, sample size $n$, and the constant $\\alpha$, you will see precisely how the squared bias and variance terms contribute to the total error, setting the stage for more advanced concepts like model tuning and regularization. [@problem_id:1900753]", "problem": "Let $X_1, X_2, \\ldots, X_n$ be a random sample of size $n$ drawn from a Poisson distribution with parameter $\\lambda > 0$. A statistician proposes an estimator $\\hat{\\lambda}$ for the parameter $\\lambda$, defined as:\n$$\n\\hat{\\lambda} = \\frac{\\left(\\sum_{i=1}^n X_i\\right) + \\alpha}{n}\n$$\nwhere $\\alpha$ is a known real constant.\n\nFind a simplified expression for the Mean Squared Error (MSE), defined as $\\text{MSE}(\\hat{\\lambda}) = E\\left[(\\hat{\\lambda} - \\lambda)^2\\right]$, of this estimator. Express your answer in terms of $n$, $\\lambda$, and $\\alpha$.", "solution": "We use the bias-variance decomposition for mean squared error: for any estimator $\\hat{\\lambda}$ of $\\lambda$, the mean squared error is $\\text{MSE}(\\hat{\\lambda}) = \\operatorname{Var}(\\hat{\\lambda}) + \\left(\\operatorname{Bias}(\\hat{\\lambda})\\right)^{2}$, where $\\operatorname{Bias}(\\hat{\\lambda}) = E[\\hat{\\lambda}] - \\lambda$.\n\nFirst compute the expectation of $\\hat{\\lambda}$. Since $X_{1},\\ldots,X_{n}$ are independent and identically distributed with $E[X_{i}] = \\lambda$, we have $E\\left[\\sum_{i=1}^{n} X_{i}\\right] = n\\lambda$. Therefore,\n$$\nE[\\hat{\\lambda}] = E\\left[\\frac{\\sum_{i=1}^{n} X_{i} + \\alpha}{n}\\right] = \\frac{E\\left[\\sum_{i=1}^{n} X_{i}\\right] + \\alpha}{n} = \\frac{n\\lambda + \\alpha}{n} = \\lambda + \\frac{\\alpha}{n}.\n$$\nThus the bias is\n$$\n\\operatorname{Bias}(\\hat{\\lambda}) = E[\\hat{\\lambda}] - \\lambda = \\frac{\\alpha}{n}.\n$$\n\nNext compute the variance of $\\hat{\\lambda}$. Using $\\operatorname{Var}\\left(\\sum_{i=1}^{n} X_{i}\\right) = \\sum_{i=1}^{n} \\operatorname{Var}(X_{i})$ for independent variables and $\\operatorname{Var}(X_{i}) = \\lambda$ for Poisson$(\\lambda)$,\n$$\n\\operatorname{Var}\\left(\\sum_{i=1}^{n} X_{i}\\right) = n\\lambda.\n$$\nSince adding a constant does not change variance and scaling by a constant $c$ multiplies variance by $c^{2}$, we get\n$$\n\\operatorname{Var}(\\hat{\\lambda}) = \\operatorname{Var}\\left(\\frac{\\sum_{i=1}^{n} X_{i} + \\alpha}{n}\\right) = \\frac{1}{n^{2}} \\operatorname{Var}\\left(\\sum_{i=1}^{n} X_{i}\\right) = \\frac{1}{n^{2}} \\cdot n\\lambda = \\frac{\\lambda}{n}.\n$$\n\nCombining these,\n$$\n\\text{MSE}(\\hat{\\lambda}) = \\operatorname{Var}(\\hat{\\lambda}) + \\left(\\operatorname{Bias}(\\hat{\\lambda})\\right)^{2} = \\frac{\\lambda}{n} + \\left(\\frac{\\alpha}{n}\\right)^{2} = \\frac{\\lambda}{n} + \\frac{\\alpha^{2}}{n^{2}}.\n$$\n\nEquivalently, one can verify directly:\n$$\n\\text{MSE}(\\hat{\\lambda}) = E\\left[\\left(\\frac{\\sum_{i=1}^{n} X_{i} + \\alpha}{n} - \\lambda\\right)^{2}\\right] = \\frac{1}{n^{2}} E\\left[\\left(\\sum_{i=1}^{n} X_{i} - n\\lambda + \\alpha\\right)^{2}\\right] = \\frac{1}{n^{2}}\\left(\\operatorname{Var}\\left(\\sum_{i=1}^{n} X_{i}\\right) + \\alpha^{2}\\right) = \\frac{1}{n^{2}}(n\\lambda + \\alpha^{2}) = \\frac{\\lambda}{n} + \\frac{\\alpha^{2}}{n^{2}}.\n$$", "answer": "$$\\boxed{\\frac{\\lambda}{n}+\\frac{\\alpha^{2}}{n^{2}}}$$", "id": "1900753"}]}