{"hands_on_practices": [{"introduction": "To begin, we apply the Karlin-Rubin theorem to a foundational scenario involving a discrete distribution. This exercise demonstrates the complete process of constructing a Uniformly Most Powerful (UMP) test for the parameter of a geometric distribution. By working through this problem, you will practice identifying the monotone likelihood ratio (MLR) property, using it to determine the structure of the rejection region, and calculating a precise critical value to satisfy a given significance level, $\\alpha$.", "problem": "Let $X$ be a random variable representing the number of trials required to get the first success in a sequence of independent Bernoulli trials. The probability mass function (PMF) of $X$ follows a geometric distribution with parameter $p$, given by $f(x|p) = p(1-p)^{x-1}$ for $x \\in \\{1, 2, 3, \\dots\\}$, where $p \\in (0, 1)$ is the unknown probability of success. The geometric distribution is a member of the one-parameter exponential family, and for a single observation $X$, a sufficient statistic for the parameter $p$ is $T(X) = X$.\n\nAn analyst wants to construct a Uniformly Most Powerful (UMP) test for the hypotheses:\n$H_0: p \\ge 0.4$\nversus\n$H_1: p  0.4$\n\nThe test should have a significance level (size) of $\\alpha = 0.1296$. What is the rejection region $R$ for this UMP test?\n\nA. $R = \\{x \\in \\mathbb{N} : x \\le 2\\}$\n\nB. $R = \\{x \\in \\mathbb{N} : x \\ge 3\\}$\n\nC. $R = \\{x \\in \\mathbb{N} : x \\le 4\\}$\n\nD. $R = \\{x \\in \\mathbb{N} : x \\ge 5\\}$\n\nE. $R = \\{x \\in \\mathbb{N} : x = 5\\}$", "solution": "We have one observation from a geometric distribution with pmf $f(x \\mid p) = p(1-p)^{x-1}$ for $x \\in \\{1,2,\\dots\\}$ and $p \\in (0,1)$. Write this in exponential family form:\n$$\nf(x \\mid p) = \\exp\\{\\ln p + (x-1)\\ln(1-p)\\} = \\exp\\{\\ln p - \\ln(1-p) + x \\ln(1-p)\\}.\n$$\nThus it is a one-parameter exponential family with natural parameter $\\eta = \\ln(1-p)$ and statistic $T(X) = X$. The family has a monotone likelihood ratio in $T(X) = X$ because for $\\eta_{2} > \\eta_{1}$,\n$$\n\\frac{f(x \\mid \\eta_{2})}{f(x \\mid \\eta_{1})} = \\exp\\{x(\\eta_{2}-\\eta_{1})\\} \\times \\text{(constant in $x$)},\n$$\nwhich is increasing in $x$. By the Karlin–Rubin theorem, for testing one-sided hypotheses in $\\eta$, the UMP test rejects for large values of $T(X)$ when $H_{1}$ corresponds to larger $\\eta$.\n\nNow consider $H_0: p \\ge p_0$ versus $H_1: p  p_0$ with $p_0 = 0.4$. Since $\\eta = \\ln(1-p)$ is decreasing in $p$, we have $H_0: \\eta \\le \\eta_0$ versus $H_1: \\eta > \\eta_0$, where $\\eta_0 = \\ln(1-p_0)$. Therefore, the UMP test rejects for large $X$, i.e., has rejection region of the form $R = \\{x \\ge c\\}$.\n\nTo achieve size $\\alpha = 0.1296$, we choose $c$ so that the probability under the boundary $p = p_0$ equals $\\alpha$. For a geometric distribution,\n$$\n\\mathbb{P}_{p_0}(X \\ge c) = \\sum_{x=c}^{\\infty} p_0(1-p_0)^{x-1} = (1-p_0)^{c-1}.\n$$\nSet $(1-p_0)^{c-1} = \\alpha$. With $p_0 = 0.4$, we have $1-p_0 = 0.6$, and we note that $0.6^{4} = 0.1296$. Hence $(1-p_0)^{c-1} = \\alpha$ is satisfied by $c-1 = 4$, i.e., $c = 5$.\n\nTherefore, the UMP level-$\\alpha$ rejection region is $R = \\{x \\in \\mathbb{N} : x \\ge 5\\}$, which corresponds to option D.", "answer": "$$\\boxed{D}$$", "id": "1927190"}, {"introduction": "Building on the fundamentals, this practice problem explores a crucial nuance of the Karlin-Rubin theorem. It is a common misconception that for an alternative hypothesis like $H_1: \\theta \\gt \\theta_0$, the UMP test must always reject for large values of the test statistic. This exercise, using a Pareto distribution, reveals that the form of the rejection region depends on whether the likelihood ratio is an increasing or decreasing function of the statistic [@problem_id:1927216]. Successfully navigating this problem will deepen your understanding of the mechanics behind the theorem.", "problem": "Let $X_1, \\dots, X_n$ be a random sample from a distribution with a probability density function (PDF) given by:\n$$ f(x|\\theta) = \\theta x^{-(\\theta+1)} \\quad \\text{for } x \\ge 1 $$\nwhere $\\theta  0$ is an unknown parameter. We wish to test the null hypothesis $H_0: \\theta \\le \\theta_0$ against the alternative hypothesis $H_1: \\theta  \\theta_0$ for a specified constant $\\theta_0 > 0$.\n\nConsider the test statistic $T(\\mathbf{X}) = \\sum_{i=1}^{n} \\ln(X_i)$.\n\nBased on the Karlin-Rubin theorem, which of the following describes the form of the rejection region for the Uniformly Most Powerful (UMP) test of size $\\alpha$? The constants $c$, $c_1$, and $c_2$ are chosen appropriately to satisfy the size $\\alpha$ condition.\n\nA. Reject $H_0$ if $T(\\mathbf{X})  c$.\n\nB. Reject $H_0$ if $T(\\mathbf{X})  c$.\n\nC. Reject $H_0$ if $|T(\\mathbf{X})|  c$.\n\nD. Reject $H_0$ if $c_1  T(\\mathbf{X})  c_2$.\n\nE. A UMP test does not exist for this hypothesis test.", "solution": "We have a random sample from the Pareto family with support $x \\geq 1$ and density $f(x \\mid \\theta) = \\theta x^{-(\\theta+1)}$ for $\\theta > 0$. The joint density for $\\mathbf{X} = (X_{1},\\dots,X_{n})$ is\n$$\nL(\\theta \\mid \\mathbf{X}) \\;=\\; \\prod_{i=1}^{n} \\theta X_{i}^{-(\\theta+1)}\n\\;=\\; \\theta^{n} \\prod_{i=1}^{n} X_{i}^{-(\\theta+1)}\n\\;=\\; \\theta^{n} \\exp\\!\\left( -(\\theta+1) \\sum_{i=1}^{n} \\ln X_{i} \\right).\n$$\nLet $T(\\mathbf{X}) = \\sum_{i=1}^{n} \\ln X_{i}$. Then\n$$\nL(\\theta \\mid \\mathbf{X}) \\;=\\; \\exp\\!\\left( n \\ln \\theta - \\theta T(\\mathbf{X}) - T(\\mathbf{X}) \\right),\n$$\nso for fixed data, all $\\theta$-dependence is through $n \\ln \\theta - \\theta T(\\mathbf{X})$, showing that $T(\\mathbf{X})$ is a sufficient statistic for $\\theta$ and the family is a one-parameter exponential family admitting a monotone likelihood ratio in $T(\\mathbf{X})$.\n\nTo verify the monotone likelihood ratio property, consider $\\theta_{1} > \\theta_{0}$ and form the likelihood ratio as a function of $T$:\n$$\n\\Lambda(T) \\;=\\; \\frac{L(\\theta_{1} \\mid \\mathbf{X})}{L(\\theta_{0} \\mid \\mathbf{X})}\n\\;=\\; \\left( \\frac{\\theta_{1}}{\\theta_{0}} \\right)^{n} \\exp\\!\\left( -(\\theta_{1}-\\theta_{0}) T \\right).\n$$\nFor fixed $\\theta_{1} > \\theta_{0}$, the factor $\\left( \\frac{\\theta_{1}}{\\theta_{0}} \\right)^{n}$ is constant in $T$, while $\\exp\\!\\left( -(\\theta_{1}-\\theta_{0}) T \\right)$ is strictly decreasing in $T$. Therefore, the family has a monotone likelihood ratio in $T$ that is decreasing in $T$.\n\nBy the Karlin-Rubin theorem, for testing the one-sided hypothesis $H_{0}: \\theta \\le \\theta_{0}$ versus $H_{1}: \\theta > \\theta_{0}$ in a one-parameter family with monotone likelihood ratio in a statistic $T$, a uniformly most powerful level $\\alpha$ test exists with a rejection region that is monotone in $T$ in the direction that makes the likelihood ratio large under the alternative. Since $\\Lambda(T)$ is decreasing in $T$ for $\\theta_{1} > \\theta_{0}$, larger values of the likelihood ratio correspond to smaller values of $T$. Hence, the UMP test of size $\\alpha$ rejects for small values of $T$, that is,\n$$\n\\text{Reject } H_{0} \\text{ if } T(\\mathbf{X})  c,\n$$\nwith $c$ chosen to satisfy the size $\\alpha$ condition.\n\nTherefore, the correct choice is B.", "answer": "$$\\boxed{B}$$", "id": "1927216"}, {"introduction": "A true master of a theorem understands not only when it applies but also when it does not. The Karlin-Rubin theorem is powerful, but its applicability is contingent on the family of distributions having a monotone likelihood ratio (MLR). This final exercise challenges you to investigate a scenario—a mixture of two normal distributions—where this property may fail [@problem_id:1927195]. By determining the conditions under which the MLR property breaks down, you will gain a critical perspective on the theorem's scope and its limitations in practical modeling situations.", "problem": "Let $f_1(x)$ be the probability density function (PDF) of a standard normal distribution, $N(0,1)$, and let $f_2(x)$ be the PDF of a normal distribution with mean 0 and variance $\\sigma^2$, denoted $N(0, \\sigma^2)$, where $\\sigma > 0$.\n\nConsider a random variable $X$ whose PDF is a mixture of these two distributions, given by:\n$$f(x|\\theta) = \\theta f_1(x) + (1-\\theta) f_2(x)$$\nwhere $\\theta \\in (0,1)$ is a mixing proportion.\n\nFor any pair of distinct proportions $\\theta_1$ and $\\theta_2$ from the interval $(0,1)$ such that $\\theta_1  \\theta_2$, we define the ratio function:\n$$L(x) = \\frac{f(x|\\theta_2)}{f(x|\\theta_1)}$$\nThis function $L(x)$ is said to be monotonic if it is either non-increasing for all $x \\in (-\\infty, \\infty)$ or non-decreasing for all $x \\in (-\\infty, \\infty)$.\n\nFor which values of the parameter $\\sigma$ does the ratio function $L(x)$ FAIL to be monotonic in $x$?\n\nA. For all $\\sigma  0$.\n\nB. Only for $\\sigma  1$.\n\nC. Only for $\\sigma  1$ and $\\sigma  0$.\n\nD. For all $\\sigma  0$ such that $\\sigma \\neq 1$.\n\nE. There are no values of $\\sigma$ for which $L(x)$ fails to be monotonic.", "solution": "Let $f_{1}(x)=(2\\pi)^{-1/2}\\exp(-x^{2}/2)$ and $f_{2}(x)=(\\sigma\\sqrt{2\\pi})^{-1}\\exp(-x^{2}/(2\\sigma^{2}))$. For $\\theta\\in(0,1)$,\n$$\nf(x\\mid\\theta)=\\theta f_{1}(x)+(1-\\theta)f_{2}(x)=(1-\\theta)f_{2}(x)\\left(1+\\frac{\\theta}{1-\\theta}\\frac{f_{1}(x)}{f_{2}(x)}\\right).\n$$\nFix $\\theta_{1}\\theta_{2}$ and define $a_{i}=\\theta_{i}/(1-\\theta_{i})$ for $i=1,2$, and $r(x)=f_{1}(x)/f_{2}(x)$. Then\n$$\nL(x)=\\frac{f(x\\mid\\theta_{2})}{f(x\\mid\\theta_{1})}=\\frac{1-\\theta_{2}}{1-\\theta_{1}}\\cdot\\frac{1+a_{2}r(x)}{1+a_{1}r(x)}.\n$$\nThe prefactor $\\frac{1-\\theta_{2}}{1-\\theta_{1}}$ is constant in $x$. Let $h(r)=(1+a_{2}r)/(1+a_{1}r)$. Then\n$$\n\\frac{\\mathrm{d}h}{\\mathrm{d}r}=\\frac{a_{2}-a_{1}}{(1+a_{1}r)^{2}}0,\n$$\nsince $a_{2}a_{1}0$. Therefore, the monotonicity of $L(x)$ in $x$ is equivalent to the monotonicity of $r(x)$ in $x$.\n\nCompute $r(x)$ explicitly:\n$$\nr(x)=\\frac{f_{1}(x)}{f_{2}(x)}=\\sigma\\exp\\!\\left(x^{2}\\left(\\frac{1}{2\\sigma^{2}}-\\frac{1}{2}\\right)\\right).\n$$\nDifferentiate:\n$$\n\\frac{\\mathrm{d}r}{\\mathrm{d}x}=r(x)\\cdot x\\left(\\frac{1}{\\sigma^{2}}-1\\right).\n$$\nHence:\n- If $\\sigma1$, then $\\frac{1}{\\sigma^{2}}-10$, so $\\frac{\\mathrm{d}r}{\\mathrm{d}x}0$ for $x0$ and $\\frac{\\mathrm{d}r}{\\mathrm{d}x}0$ for $x0$. Thus $r(x)$ decreases on $(-\\infty,0)$ and increases on $(0,\\infty)$, so it is not monotonic on $\\mathbb{R}$.\n- If $\\sigma1$, then $\\frac{1}{\\sigma^{2}}-10$, so $\\frac{\\mathrm{d}r}{\\mathrm{d}x}0$ for $x0$ and $\\frac{\\mathrm{d}r}{\\mathrm{d}x}0$ for $x0$. Thus $r(x)$ increases on $(-\\infty,0)$ and decreases on $(0,\\infty)$, so it is not monotonic on $\\mathbb{R}$.\n- If $\\sigma=1$, then $r(x)\\equiv 1$ is constant, hence monotonic, and in fact $f_{1}=f_{2}$ so $L(x)\\equiv 1$.\n\nSince $h$ is strictly increasing, $L(x)$ is monotonic in $x$ if and only if $r(x)$ is monotonic in $x$. Therefore, $L(x)$ fails to be monotonic for all $\\sigma0$ with $\\sigma\\neq 1$, and is monotonic (constant) only when $\\sigma=1$. This corresponds to option D.", "answer": "$$\\boxed{D}$$", "id": "1927195"}]}