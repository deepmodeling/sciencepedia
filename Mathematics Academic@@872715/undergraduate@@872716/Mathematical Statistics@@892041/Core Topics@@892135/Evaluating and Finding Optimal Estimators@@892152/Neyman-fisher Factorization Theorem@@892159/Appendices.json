{"hands_on_practices": [{"introduction": "Applying a new theorem to a familiar setting is often the best way to build understanding. This first practice invites you to use the Neyman-Fisher Factorization Theorem on a random sample from a Binomial distribution [@problem_id:1939675]. By working through this fundamental example, you'll see how the theorem formally proves our intuition that the total number of successes is the key piece of information for estimating the unknown success probability.", "problem": "A company manufactures integrated circuits. The probability that any given circuit is defective is $p$, where $0 < p < 1$. This probability $p$ is constant but unknown. To monitor the production process, $m$ independent batches are sampled. For each batch $i$ (where $i = 1, 2, \\dots, m$), a fixed number of $n$ circuits are randomly selected and tested. The number of defective circuits in batch $i$ is denoted by the random variable $X_i$. Each $X_i$ is assumed to follow a Binomial distribution with $n$ trials and a \"success\" probability $p$ of being defective. The number of trials $n$ is a known positive integer.\n\nYou are given a set of observations $x_1, x_2, \\dots, x_m$, which are specific outcomes of the random variables $X_1, X_2, \\dots, X_m$. The Neyman-Fisher Factorization Theorem provides a method for finding a sufficient statistic, which is a function of the data that summarizes all of the information the sample contains about the unknown parameter $p$. The theorem states that a statistic $T = T(x_1, x_2, \\dots, x_m)$ is sufficient for $p$ if and only if the joint Probability Mass Function (PMF) of the sample, $f(x_1, \\dots, x_m | p)$, can be factored into two non-negative functions:\n$$f(x_1, \\dots, x_m | p) = g(T(x_1, \\dots, x_m), p) \\cdot h(x_1, \\dots, x_m)$$\nwhere the function $g$ depends on the data only through the value of the statistic $T$, and the function $h$ does not depend on the parameter $p$.\n\nYour task is to apply this theorem to the given scenario. Find the simplest non-constant statistic $T(x_1, x_2, \\dots, x_m)$ that captures all the information about the unknown defect probability $p$. Your answer should be an expression in terms of the observations $x_1, x_2, \\dots, x_m$.", "solution": "We model each batch count as $X_{i} \\sim \\mathrm{Bin}(n,p)$, independently for $i=1,\\ldots,m$. Hence, for observed values $x_{i} \\in \\{0,1,\\ldots,n\\}$, the individual PMF is\n$$\nf_{X_{i}}(x_{i}\\mid p)=\\binom{n}{x_{i}} p^{x_{i}} (1-p)^{n-x_{i}}.\n$$\nBy independence, the joint PMF is the product\n$$\nf(x_{1},\\ldots,x_{m}\\mid p)=\\prod_{i=1}^{m} \\binom{n}{x_{i}} p^{x_{i}} (1-p)^{n-x_{i}}.\n$$\nCollecting like terms yields\n$$\nf(x_{1},\\ldots,x_{m}\\mid p)=\\left[\\prod_{i=1}^{m} \\binom{n}{x_{i}}\\right] p^{\\sum_{i=1}^{m} x_{i}} (1-p)^{\\sum_{i=1}^{m} (n-x_{i})}\n=\\left[\\prod_{i=1}^{m} \\binom{n}{x_{i}}\\right] p^{T} (1-p)^{mn-T},\n$$\nwhere we define the statistic\n$$\nT=\\sum_{i=1}^{m} x_{i}.\n$$\nThe support $\\{x_{i}\\in\\{0,\\ldots,n\\}\\text{ for all }i\\}$ does not depend on $p$, so the Neyman-Fisher factorization theorem applies directly with\n$$\ng(T,p)=p^{T}(1-p)^{mn-T},\\qquad h(x_{1},\\ldots,x_{m})=\\prod_{i=1}^{m} \\binom{n}{x_{i}}.\n$$\nSince $g$ depends on the data only through $T$ and $h$ does not depend on $p$, the statistic $T=\\sum_{i=1}^{m} x_{i}$ is sufficient for $p$. Any one-to-one function of $T$ (such as the sample proportion $T/(mn)$) is also sufficient, but the simplest non-constant choice is the total number of defectives across all batches, $T=\\sum_{i=1}^{m} x_{i}$.", "answer": "$$\\boxed{\\sum_{i=1}^{m} x_{i}}$$", "id": "1939675"}, {"introduction": "The power of the Neyman-Fisher theorem extends seamlessly from discrete to continuous distributions. In this exercise, you will analyze a sample from a Laplace distribution, whose 'double-exponential' shape presents a unique challenge [@problem_id:1939653]. This practice will demonstrate how to handle probability density functions (PDFs) and reveal that the sufficient statistic can sometimes be a non-obvious function of the data, in this case involving absolute values.", "problem": "Consider a random sample $X_1, X_2, \\dots, X_n$ drawn from a Laplace distribution. The probability density function (PDF) of this distribution is given by\n$$f(x | \\mu, b) = \\frac{1}{2b} \\exp\\left(-\\frac{|x - \\mu|}{b}\\right)$$\nfor $x \\in (-\\infty, \\infty)$. In a specific experimental setup, it is known that the location parameter $\\mu$ is fixed at $\\mu = 0$, but the scale parameter $b > 0$ is unknown.\n\nBased on this information, which of the following statistics is a sufficient statistic for the unknown parameter $b$?\n\nA. $T_A(\\mathbf{X}) = \\sum_{i=1}^{n} X_i$\n\nB. $T_B(\\mathbf{X}) = \\left(\\prod_{i=1}^{n} X_i\\right)^{1/n}$\n\nC. $T_C(\\mathbf{X}) = \\sum_{i=1}^{n} X_i^2$\n\nD. $T_D(\\mathbf{X}) = \\sum_{i=1}^{n} |X_i|$\n\nE. $T_E(\\mathbf{X}) = \\max(|X_1|, |X_2|, \\dots, |X_n|)$", "solution": "Let $X_{1},\\dots,X_{n}$ be i.i.d. from the Laplace distribution with known location $\\mu=0$ and unknown scale $b>0$, having density for each $i$ given by\n$$\nf(x_{i}\\mid b)=\\frac{1}{2b}\\exp\\left(-\\frac{|x_{i}|}{b}\\right), \\quad x_{i}\\in(-\\infty,\\infty).\n$$\nBy independence, the joint density (likelihood) for $\\mathbf{x}=(x_{1},\\dots,x_{n})$ is\n$$\nL(b;\\mathbf{x})=\\prod_{i=1}^{n}\\frac{1}{2b}\\exp\\left(-\\frac{|x_{i}|}{b}\\right)\n=\\left(\\frac{1}{2b}\\right)^{n}\\exp\\left(-\\frac{1}{b}\\sum_{i=1}^{n}|x_{i}|\\right).\n$$\nBy the Neyman–Fisher factorization theorem, a statistic $T(\\mathbf{X})$ is sufficient for $b$ if the joint density can be written as\n$$\nL(b;\\mathbf{x})=g\\big(T(\\mathbf{x}),b\\big)\\,h(\\mathbf{x}),\n$$\nwhere $g$ depends on the data only through $T(\\mathbf{x})$ and $b$, and $h$ does not depend on $b$. From the expression above, take\n$$\nT(\\mathbf{X})=\\sum_{i=1}^{n}|X_{i}|,\\quad g(t,b)=\\left(\\frac{1}{2b}\\right)^{n}\\exp\\left(-\\frac{t}{b}\\right),\\quad h(\\mathbf{x})=1.\n$$\nHence $T(\\mathbf{X})=\\sum_{i=1}^{n}|X_{i}|$ is sufficient for $b$. Among the given options, this corresponds to option D. The other listed statistics do not appear in the likelihood and therefore are not sufficient for $b$ in this model.", "answer": "$$\\boxed{D}$$", "id": "1939653"}, {"introduction": "A particularly insightful application of the factorization theorem arises when the parameter of interest defines the support of the distribution. This problem explores a random sample from a Uniform distribution on an interval $[-\\theta, \\theta]$, where $\\theta$ is unknown [@problem_id:1939672]. You will learn how to use indicator functions to incorporate the data's boundaries into the likelihood, a crucial technique for finding the sufficient statistic in such cases.", "problem": "Let $X_1, X_2, \\ldots, X_n$ be a random sample drawn from a Uniform distribution on the symmetric interval $[-\\theta, \\theta]$, where the parameter $\\theta > 0$ is unknown. Identify which one of the following statistics is a sufficient statistic for $\\theta$.\n\nA. The sample mean, $\\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_i$\n\nB. The sample variance, $S^2 = \\frac{1}{n-1}\\sum_{i=1}^{n} (X_i - \\bar{X})^2$\n\nC. The maximum order statistic, $X_{(n)} = \\max(X_1, X_2, \\ldots, X_n)$\n\nD. The maximum of the absolute values of the observations, $\\max(|X_1|, |X_2|, \\ldots, |X_n|)$\n\nE. The sample range, $X_{(n)} - X_{(1)}$, where $X_{(1)} = \\min(X_1, X_2, \\ldots, X_n)$", "solution": "Let $X_{1},\\ldots,X_{n}$ be i.i.d. with density\n$$\nf_{X}(x;\\theta)=\\frac{1}{2\\theta}\\,\\mathbf{1}\\{-\\theta\\leq x\\leq \\theta\\},\\quad \\theta>0.\n$$\nThe joint density of the sample $x=(x_{1},\\ldots,x_{n})$ is\n$$\nf(x;\\theta)=\\prod_{i=1}^{n}\\frac{1}{2\\theta}\\,\\mathbf{1}\\{-\\theta\\leq x_{i}\\leq \\theta\\}\n=(2\\theta)^{-n}\\,\\prod_{i=1}^{n}\\mathbf{1}\\{|x_{i}|\\leq \\theta\\}\n=(2\\theta)^{-n}\\,\\mathbf{1}\\left\\{\\max_{1\\leq i\\leq n}|x_{i}|\\leq \\theta\\right\\}.\n$$\nDefine the statistic $T(x)=\\max_{1\\leq i\\leq n}|x_{i}|$. Then the joint density can be written as\n$$\nf(x;\\theta)=g_{\\theta}(T(x))\\,h(x),\\quad\\text{with }g_{\\theta}(t)=(2\\theta)^{-n}\\,\\mathbf{1}\\{\\theta\\geq t\\},\\; h(x)=1.\n$$\nBy the Neyman–Fisher factorization theorem, $T(X)=\\max_{1\\leq i\\leq n}|X_{i}|$ is sufficient for $\\theta$. This corresponds to option D.\n\nTo see why the other options are not sufficient, observe that the likelihood depends on $\\theta$ only through $(2\\theta)^{-n}$ and the support constraint $\\theta\\geq \\max_{i}|x_{i}|$. Any statistic that does not determine $\\max_{i}|x_{i}|$ cannot be sufficient:\n- The sample mean $\\bar{X}$ does not determine $\\max_{i}|x_{i}|$; samples can share the same mean but have different maximum absolute observations, leading to different support constraints.\n- The sample variance $S^{2}$ likewise does not determine $\\max_{i}|x_{i}|$.\n- The maximum order statistic $X_{(n)}$ fails when the largest magnitude observation is negative (e.g., $x=(-5,-4)$ gives $X_{(n)}=-4$ but $\\max|x_{i}|=5$).\n- The range $X_{(n)}-X_{(1)}$ does not determine $\\max_{i}|x_{i}|$ (e.g., $(-5,3)$ and $(-4,4)$ have the same range but different $\\max|x_{i}|$).\n\nTherefore, only $\\max(|X_{1}|,\\ldots,|X_{n}|)$ is sufficient for $\\theta$ among the given choices.", "answer": "$$\\boxed{D}$$", "id": "1939672"}]}