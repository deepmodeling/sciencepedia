{"hands_on_practices": [{"introduction": "To begin our practice, we'll tackle a scenario common in experimental sciences: combining results from several measurements to estimate a single, underlying rate. This problem explores how to find the best unbiased estimate for a decay rate $\\lambda$ when you have multiple observations, each from a Poisson distribution but with different exposure times. This exercise is a perfect illustration of how the principle of sufficiency allows us to elegantly aggregate information from non-identical experiments into a single, optimal estimator that is both intuitive and theoretically sound [@problem_id:1966016].", "problem": "A scientist is studying a radioactive material and wants to estimate its true average decay rate, $\\lambda$, in decays per second. To do this, they conduct a series of $n$ independent experiments. In the $i$-th experiment (for $i=1, 2, \\dots, n$), the scientist uses a detector to count the number of decays, $N_i$, over a specific time interval of duration $T_i$ seconds. The time intervals $T_1, T_2, \\dots, T_n$ are known, positive constants but are not necessarily equal.\n\nAssume that the number of decays $N_i$ in the $i$-th experiment follows a Poisson distribution with a mean proportional to the observation time, i.e., $N_i \\sim \\text{Poisson}(T_i \\lambda)$. The parameter $\\lambda > 0$ is the unknown constant of proportionality representing the decay rate we wish to estimate.\n\nBased on the observed counts $N_1, N_2, \\dots, N_n$ and the known durations $T_1, T_2, \\dots, T_n$, find the Uniformly Minimum Variance Unbiased Estimator (UMVUE) for $\\lambda$. Your final answer should be an analytic expression in terms of the observations $N_i$ and the constants $T_i$.", "solution": "Let $N_{i} \\sim \\text{Poisson}(T_{i}\\lambda)$ independently for $i=1,\\dots,n$, where $T_{i}0$ are known constants and $\\lambda0$ is unknown.\n\nThe joint pmf of $N_{1},\\dots,N_{n}$ is\n$$\nL(\\lambda; n_{1},\\dots,n_{n})=\\prod_{i=1}^{n}\\frac{(T_{i}\\lambda)^{n_{i}}\\exp(-T_{i}\\lambda)}{n_{i}!}\n=\\left(\\prod_{i=1}^{n}\\frac{T_{i}^{n_{i}}}{n_{i}!}\\right)\\lambda^{\\sum_{i=1}^{n}n_{i}}\\exp\\!\\left(-\\lambda\\sum_{i=1}^{n}T_{i}\\right).\n$$\nBy the Neyman–Fisher factorization theorem, the statistic $S=\\sum_{i=1}^{n}N_{i}$ is sufficient for $\\lambda$.\n\nSince the sum of independent Poisson random variables is Poisson with parameter equal to the sum of their means, we have\n$$\nS \\sim \\text{Poisson}\\!\\left(\\lambda \\sum_{i=1}^{n}T_{i}\\right).\n$$\nLet $T=\\sum_{i=1}^{n}T_{i}$. To show completeness, suppose $h$ is a function with $\\mathbb{E}_{\\lambda}[h(S)]=0$ for all $\\lambda0$. Then\n$$\n0=\\sum_{s=0}^{\\infty}h(s)\\frac{\\exp(-\\lambda T)(\\lambda T)^{s}}{s!} \\quad \\text{for all } \\lambda0.\n$$\nMultiplying by $\\exp(\\lambda T)$ and setting $x=\\lambda$ gives\n$$\n\\sum_{s=0}^{\\infty}h(s)\\frac{(Tx)^{s}}{s!}=0 \\quad \\text{for all } x0.\n$$\nThe left-hand side is a power series in $x$ that vanishes on an interval, so all its coefficients must be zero, implying $h(s)=0$ for all $s$. Therefore $S$ is complete.\n\nAn unbiased estimator of $\\lambda$ is obtained from $S$:\n$$\n\\mathbb{E}\\!\\left[\\frac{S}{T}\\right]=\\frac{1}{T}\\mathbb{E}[S]=\\frac{1}{T}\\left(\\lambda T\\right)=\\lambda.\n$$\nSince $\\frac{S}{T}$ is a function of the complete sufficient statistic $S$ and is unbiased for $\\lambda$, by the Lehmann–Scheffé theorem it is the UMVUE. Writing it in terms of the observations and known constants yields\n$$\n\\frac{\\sum_{i=1}^{n}N_{i}}{\\sum_{i=1}^{n}T_{i}}.\n$$", "answer": "$$\\boxed{\\frac{\\sum_{i=1}^{n}N_{i}}{\\sum_{i=1}^{n}T_{i}}}$$", "id": "1966016"}, {"introduction": "Next, we will venture beyond estimating a simple parameter to the more nuanced task of estimating a function of that parameter. In this problem, based on a sample from a Bernoulli distribution, our goal is not to estimate the success probability $p$, but rather $p^2$, which could represent the probability of two independent successes. This practice is crucial because it demonstrates a key insight: the best estimator for a function of a parameter, $f(\\theta)$, is not always the function of the best estimator, $f(\\hat{\\theta})$. You will learn a powerful technique involving factorial moments to construct the UMVUE in this situation [@problem_id:1966071].", "problem": "A large-scale manufacturing process for a specific type of microchip produces a very small, but non-zero, fraction of defective units. Let the probability that a randomly selected microchip is defective be $p$. To estimate certain quality control metrics, a random sample of $n$ microchips, where $n \\ge 2$, is selected from the production line. Let $X_i$ be a random variable that equals 1 if the $i$-th chip in the sample is defective and 0 otherwise, for $i = 1, 2, \\ldots, n$. The collection $\\{X_1, X_2, \\ldots, X_n\\}$ can be modeled as a random sample from a Bernoulli distribution with parameter $p$.\n\nYour task is to find the Uniformly Minimum Variance Unbiased Estimator (UMVUE) for the parameter $\\theta = p^2$. This parameter $\\theta$ represents the probability that two independently produced microchips are both defective. Express your answer as a function of the total number of defective chips in the sample, $T = \\sum_{i=1}^{n} X_i$, and the sample size $n$.", "solution": "Let $X_{1},\\ldots,X_{n}$ be i.i.d. $\\text{Bernoulli}(p)$ and $T=\\sum_{i=1}^{n}X_{i}$. Then $T\\sim\\text{Binomial}(n,p)$ and the parameter of interest is $\\theta=p^{2}$.\n\nFirst, $T$ is sufficient for $p$ by the factorization theorem, since the joint pmf of $(X_{1},\\ldots,X_{n})$ can be written as\n$$\n\\prod_{i=1}^{n} p^{X_{i}}(1-p)^{1-X_{i}} \\;=\\; p^{\\sum X_{i}}(1-p)^{n-\\sum X_{i}} \\;=\\; p^{T}(1-p)^{n-T},\n$$\nwhich depends on the sample only through $T$.\n\nSecond, $T$ is complete. If $g$ is any function with $\\mathbb{E}_{p}[g(T)]=0$ for all $p\\in(0,1)$, then\n$$\n\\sum_{t=0}^{n} g(t)\\binom{n}{t}p^{t}(1-p)^{n-t}=0 \\quad \\text{for all } p\\in(0,1).\n$$\nMultiplying by $(1-p)^{-n}$ and setting $u=p/(1-p)$ gives\n$$\n\\sum_{t=0}^{n} g(t)\\binom{n}{t} u^{t}=0 \\quad \\text{for all } u0,\n$$\nwhich is a polynomial identity. Hence all coefficients vanish, implying $g(t)=0$ for all $t$, so $T$ is complete.\n\nTo construct an unbiased estimator of $\\theta$, use factorial moments of $T$:\n$$\nT(T-1)=\\sum_{i\\neq j} X_{i}X_{j}.\n$$\nTaking expectations and using independence,\n$$\n\\mathbb{E}[T(T-1)] \\;=\\; \\sum_{i\\neq j} \\mathbb{E}[X_{i}X_{j}] \\;=\\; \\sum_{i\\neq j} p^{2} \\;=\\; n(n-1)p^{2}.\n$$\nTherefore,\n$$\n\\mathbb{E}\\!\\left[\\frac{T(T-1)}{n(n-1)}\\right]=p^{2}=\\theta,\n$$\nso $g(T)=\\frac{T(T-1)}{n(n-1)}$ is unbiased for $\\theta$ and is a function of the sufficient statistic $T$.\n\nBy the Lehmann–Scheffé theorem, since $T$ is complete and sufficient and $g(T)$ is unbiased for $\\theta$, the uniformly minimum variance unbiased estimator (UMVUE) of $\\theta$ is\n$$\n\\frac{T(T-1)}{n(n-1)}.\n$$", "answer": "$$\\boxed{\\frac{T(T-1)}{n(n-1)}}$$", "id": "1966071"}, {"introduction": "Our final practice problem introduces a fundamentally different, yet common, type of estimation challenge involving a \"non-regular\" distribution. Here, the parameter $\\theta$ we wish to estimate is part of the boundary of the distribution's support, representing a minimum possible value or a threshold. This scenario breaks the mold of our previous examples, where the sum of observations was the key statistic. Instead, you will discover that an order statistic—specifically, the sample minimum—holds the key to finding the UMVUE, demonstrating the remarkable versatility of the Lehmann-Scheffé theorem [@problem_id:1966035].", "problem": "An electronics manufacturer is conducting reliability tests on a new type of semiconductor device. The testing apparatus has a fixed, but unknown, startup delay of $\\theta$ seconds before it can begin recording the lifetime of a device. As a result, the recorded lifetime $X$ of any given device is always greater than $\\theta$. It is determined that the recorded lifetimes follow a shifted exponential distribution.\n\nLet $X_1, X_2, \\ldots, X_n$ be a random sample of $n$ recorded lifetimes from a population with the probability density function (PDF):\n$$\nf(x; \\theta) = \\exp(-(x-\\theta)) \\quad \\text{for } x > \\theta\n$$\nand $f(x; \\theta) = 0$ otherwise. Here, $\\theta > 0$ is the unknown startup delay parameter.\n\nYour task is to find the Uniformly Minimum Variance Unbiased Estimator (UMVUE) for the parameter $\\theta$. Express your answer in terms of the sample size $n$ and the order statistics of the sample. Use $X_{(1)}$ to denote the minimum value in the sample, i.e., $X_{(1)} = \\min(X_1, X_2, \\ldots, X_n)$.", "solution": "We have a random sample from the shifted exponential family with density\n$$\nf(x;\\theta)=\\exp\\!\\big(-(x-\\theta)\\big)\\quad \\text{for }x\\theta,\\quad \\theta0.\n$$\nThe joint density of $X_{1},\\ldots,X_{n}$ is\n$$\nL(\\theta;x_{1},\\ldots,x_{n})=\\prod_{i=1}^{n}\\exp\\!\\big(-(x_{i}-\\theta)\\big)\\,\\mathbf{1}\\{x_{i}\\theta\\}\n=\\exp\\!\\Big(-\\sum_{i=1}^{n}x_{i}+n\\theta\\Big)\\,\\mathbf{1}\\{\\thetaX_{(1)}\\}.\n$$\nBy the factorization theorem, $T=X_{(1)}$ is a sufficient statistic for $\\theta$.\n\nThe distribution of $T$ is found via the survival function:\n$$\n\\mathbb{P}_{\\theta}(Tt)=\\prod_{i=1}^{n}\\mathbb{P}_{\\theta}(X_{i}t)\n=\\big(\\exp\\!\\big(-(t-\\theta)\\big)\\big)^{n}=\\exp\\!\\big(-n(t-\\theta)\\big),\\quad t\\theta.\n$$\nHence the density of $T$ is\n$$\nf_{T}(t;\\theta)=n\\exp\\!\\big(-n(t-\\theta)\\big),\\quad t\\theta,\n$$\nso that $T-\\theta$ is $\\mathrm{Exp}(n)$ and its distribution does not depend on $\\theta$.\n\nTo show completeness of $T$, suppose a measurable function $a$ satisfies $\\mathbb{E}_{\\theta}[a(T)]=0$ for all $\\theta$. Then\n$$\n0=\\mathbb{E}_{\\theta}[a(T)]=\\int_{\\theta}^{\\infty}a(t)\\,n\\exp\\!\\big(-n(t-\\theta)\\big)\\,\\mathrm{d}t\n=n\\exp(n\\theta)\\int_{\\theta}^{\\infty}a(t)\\exp(-nt)\\,\\mathrm{d}t\n=:F(\\theta).\n$$\nDifferentiate $F(\\theta)$:\n$$\nF'(\\theta)=F(\\theta)-n\\,a(\\theta).\n$$\nSince $F(\\theta)\\equiv 0$ for all $\\theta$, it follows that $0=F'(\\theta)=-n\\,a(\\theta)$ for all $\\theta$, hence $a(\\theta)=0$ for all $\\theta$. Therefore $T$ is complete.\n\nNext, compute the expectation of $X_{(1)}$:\n$$\n\\mathbb{E}_{\\theta}[X_{(1)}]=\\int_{\\theta}^{\\infty}x\\,n\\exp\\!\\big(-n(x-\\theta)\\big)\\,\\mathrm{d}x\n=\\int_{0}^{\\infty}(y+\\theta)\\,n\\exp(-ny)\\,\\mathrm{d}y\n=\\theta\\,n\\int_{0}^{\\infty}\\exp(-ny)\\,\\mathrm{d}y+n\\int_{0}^{\\infty}y\\exp(-ny)\\,\\mathrm{d}y.\n$$\nUsing $\\int_{0}^{\\infty}\\exp(-ny)\\,\\mathrm{d}y=\\frac{1}{n}$ and $\\int_{0}^{\\infty}y\\exp(-ny)\\,\\mathrm{d}y=\\frac{1}{n^{2}}$, we obtain\n$$\n\\mathbb{E}_{\\theta}[X_{(1)}]=\\theta+\\frac{1}{n}.\n$$\nTherefore $X_{(1)}-\\frac{1}{n}$ is an unbiased estimator of $\\theta$. Since it is a function of the complete sufficient statistic $T=X_{(1)}$, by the Lehmann–Scheffé theorem it is the UMVUE for $\\theta$.\n\nThus, the UMVUE for $\\theta$ is $X_{(1)}-\\frac{1}{n}$.", "answer": "$$\\boxed{X_{(1)}-\\frac{1}{n}}$$", "id": "1966035"}]}