{"hands_on_practices": [{"introduction": "Our first practice grounds the theory of Uniformly Minimum Variance Unbiased Estimators (UMVUEs) in a familiar setting: the binomial distribution. This exercise [@problem_id:1917710] demonstrates how to construct an optimal estimator not for the success probability $p$ itself, but for a function of it, $\\theta = p^2$, which can arise in risk or reliability models. By leveraging the properties of the sample count, a complete sufficient statistic, we can systematically find the unique best unbiased estimator.", "problem": "A quality control engineer is inspecting large batches of manufactured components. A random sample of size $n$ is drawn from a batch, where $n$ is a known integer constant such that $n \\geq 2$. The number of defective components in the sample, denoted by the random variable $X$, is assumed to follow a Binomial distribution, $X \\sim \\text{Binomial}(n, p)$, where $p$ is the unknown proportion of defective components in the entire batch, with $0 < p < 1$.\n\nA cost analysis model for future warranty claims suggests that the long-term financial risk associated with a production process is proportional to the parameter $\\theta = p^2$. To estimate this risk, the engineer needs an efficient estimator for $\\theta$.\n\nFind the Uniformly Minimum-Variance Unbiased Estimator (UMVUE) for $\\theta = p^2$. Express your answer as a function of the observed number of defects, $X$, and the sample size, $n$.", "solution": "We have $X \\sim \\text{Binomial}(n,p)$ with $n \\geq 2$ and the target parameter is $\\theta = p^{2}$. The statistic $X$ is sufficient for $p$ by the factorization theorem, since the joint pmf of the $n$ Bernoulli trials depends on the sample only through $X = \\sum_{i=1}^{n} Y_{i}$. Moreover, for fixed $n$, the binomial family $\\{\\text{Binomial}(n,p): 0<p<1\\}$ is complete with respect to $X$, because if $\\sum_{x=0}^{n} g(x) \\binom{n}{x} p^{x} (1-p)^{n-x} = 0$ for all $p \\in (0,1)$, then the resulting polynomial in $p$ is identically zero, implying $g(x)=0$ for all $x$.\n\nTo find an unbiased estimator of $p^{2}$ as a function of $X$, write $X = \\sum_{i=1}^{n} Y_{i}$ with $Y_{i} \\sim \\text{Bernoulli}(p)$ independent. Consider the second factorial moment:\n$$\nX(X-1) = \\sum_{i \\neq j} Y_{i} Y_{j}.\n$$\nTaking expectations and using independence gives\n$$\n\\mathbb{E}[X(X-1)] = \\sum_{i \\neq j} \\mathbb{E}[Y_{i} Y_{j}] = \\sum_{i \\neq j} p^{2} = n(n-1) p^{2}.\n$$\nTherefore,\n$$\n\\mathbb{E}\\!\\left[\\frac{X(X-1)}{n(n-1)}\\right] = p^{2}.\n$$\nHence $X(X-1)/[n(n-1)]$ is an unbiased estimator of $p^{2}$ and is a function of the complete sufficient statistic $X$. By the Lehmann–Scheffé theorem, this estimator is the UMVUE for $\\theta = p^{2}$.\n\nThus, the UMVUE is\n$$\n\\frac{X(X-1)}{n(n-1)}.\n$$", "answer": "$$\\boxed{\\frac{X(X-1)}{n(n-1)}}$$", "id": "1917710"}, {"introduction": "Next, we move to a continuous distribution often seen in economics, the Pareto distribution. This problem [@problem_id:1917712] introduces a powerful strategy for finding UMVUEs: transforming the data to simplify the underlying statistical structure. You will see how a logarithmic transformation converts the problem into a more familiar one involving the gamma distribution, making the path to the UMVUE for the shape parameter $\\alpha$ clear.", "problem": "In economic modeling, the Pareto distribution is often used to describe the distribution of wealth or income. Assume that the income level $X$ in a certain population, measured in multiples of a minimum subsistence income, follows a Pareto Type I distribution. A random sample of $n$ individuals, where $n \\ge 2$, is drawn from this population, yielding income levels $X_1, X_2, \\ldots, X_n$. The probability density function (PDF) for the income level $X$ of a single individual is given by\n$$ f(x | \\alpha) = \\alpha x^{-(\\alpha+1)} \\quad \\text{for } x \\ge 1 $$\nand $f(x|\\alpha) = 0$ for $x < 1$. The parameter $\\alpha > 0$ is an unknown shape parameter that characterizes the degree of income inequality.\n\nYour task is to find the Uniformly Minimum-Variance Unbiased Estimator (UMVUE) for the parameter $\\alpha$. Express your answer as a symbolic expression in terms of the sample size $n$ and the observations $X_1, X_2, \\ldots, X_n$.", "solution": "We begin by transforming the Pareto sample to an exponential sample. Let $Y_{i}=\\ln X_{i}$ for each $i$. Since $X$ has density $f(x\\mid \\alpha)=\\alpha x^{-(\\alpha+1)}$ for $x\\geq 1$, the density of $Y$ is obtained by the change of variables $x=\\exp(y)$, $dx/dy=\\exp(y)$:\n$$\nf_{Y}(y\\mid \\alpha)=f_{X}(\\exp(y)\\mid \\alpha)\\exp(y)=\\alpha \\exp\\big(-( \\alpha+1)y\\big)\\exp(y)=\\alpha \\exp(-\\alpha y),\\quad y\\geq 0.\n$$\nThus $Y_{1},\\ldots,Y_{n}$ are independent and identically distributed $\\operatorname{Exp}(\\alpha)$ random variables.\n\nLet $T=\\sum_{i=1}^{n}Y_{i}=\\sum_{i=1}^{n}\\ln X_{i}$. Then $T$ has a Gamma distribution with shape $n$ and rate $\\alpha$, i.e.,\n$$\nf_{T}(t\\mid \\alpha)=\\frac{\\alpha^{n}}{\\Gamma(n)}t^{n-1}\\exp(-\\alpha t),\\quad t\\geq 0.\n$$\nBy the factorization theorem, $T$ is a sufficient statistic for $\\alpha$. Moreover, the Gamma family with fixed shape $n$ and unknown rate $\\alpha$ is a full one-parameter exponential family; hence $T$ is complete for $\\alpha>0$.\n\nTo find an unbiased estimator of $\\alpha$ as a function of $T$, compute $E_{\\alpha}[T^{-1}]$ using the Gamma density:\n$$\nE_{\\alpha}[T^{-1}]=\\int_{0}^{\\infty} t^{-1}\\frac{\\alpha^{n}}{\\Gamma(n)}t^{n-1}\\exp(-\\alpha t)\\,dt=\\frac{\\alpha^{n}}{\\Gamma(n)}\\int_{0}^{\\infty} t^{n-2}\\exp(-\\alpha t)\\,dt.\n$$\nWith the substitution $u=\\alpha t$ (so $t=u/\\alpha$ and $dt=du/\\alpha$), we obtain\n$$\nE_{\\alpha}[T^{-1}]=\\frac{\\alpha^{n}}{\\Gamma(n)}\\int_{0}^{\\infty}\\left(\\frac{u}{\\alpha}\\right)^{n-2}\\exp(-u)\\frac{du}{\\alpha}\n=\\frac{\\alpha^{n}}{\\Gamma(n)}\\alpha^{-(n-1)}\\int_{0}^{\\infty}u^{n-2}\\exp(-u)\\,du\n=\\frac{\\alpha}{\\Gamma(n)}\\Gamma(n-1).\n$$\nSince $\\Gamma(n)=(n-1)\\Gamma(n-1)$, this simplifies to\n$$\nE_{\\alpha}[T^{-1}]=\\frac{\\alpha}{n-1},\\quad \\text{for } n\\geq 2.\n$$\nTherefore,\n$$\nE_{\\alpha}\\!\\left[\\frac{n-1}{T}\\right]=\\alpha.\n$$\nBecause $\\frac{n-1}{T}$ is a function of the complete sufficient statistic $T$ and is unbiased for $\\alpha$, it is the UMVUE by the Lehmann–Scheffé theorem. Recalling that $T=\\sum_{i=1}^{n}\\ln X_{i}$, the UMVUE in terms of the original observations is\n$$\n\\frac{n-1}{\\sum_{i=1}^{n}\\ln X_{i}}.\n$$", "answer": "$$\\boxed{\\frac{n-1}{\\sum_{i=1}^{n}\\ln X_{i}}}$$", "id": "1917712"}, {"introduction": "Our final practice explores a different class of problems where the parameter to be estimated, $\\theta$, defines the boundary of the data's support. In such cases, the sum of observations is no longer sufficient, and we must turn to order statistics—specifically, the sample maximum. This exercise [@problem_id:1917722] will guide you through finding the UMVUE for $\\theta$ by first identifying the maximum as the complete sufficient statistic and then scaling it to achieve unbiasedness.", "problem": "In a particle physics experiment, the energy deposited by a certain type of exotic particle in a detector is modeled as a continuous random variable $X$. The probability density function (PDF) for this energy is given by\n$$f(x|\\theta) = \\frac{2x}{\\theta^2}$$\nfor $0 < x < \\theta$, and $f(x|\\theta) = 0$ otherwise. The parameter $\\theta > 0$ represents the unknown maximum possible energy that can be deposited by a single particle.\n\nAn experiment is conducted where $n$ such particles are independently detected, yielding a random sample of energy measurements $X_1, X_2, \\ldots, X_n$. Let $X_{(n)} = \\max(X_1, X_2, \\ldots, X_n)$ denote the maximum observed energy in the sample.\n\nYour task is to find the Uniformly Minimum-Variance Unbiased Estimator (UMVUE) for the parameter $\\theta$. Express your final answer as an analytic expression in terms of the sample size $n$ and the statistic $X_{(n)}$.", "solution": "We begin by identifying a sufficient statistic for the parameter. The joint density of the sample is\n$$\nf(x_{1},\\ldots,x_{n}\\mid \\theta)=\\prod_{i=1}^{n}\\frac{2x_{i}}{\\theta^{2}}\\mathbf{1}_{(0,\\theta)}(x_{i})\n=\\left(2^{n}\\prod_{i=1}^{n}x_{i}\\right)\\theta^{-2n}\\mathbf{1}_{(0,\\theta)}\\!\\left(x_{(n)}\\right),\n$$\nwhere $x_{(n)}=\\max\\{x_{1},\\ldots,x_{n}\\}$. By the Neyman–Fisher factorization theorem, $X_{(n)}$ is a sufficient statistic for $\\theta$.\n\nNext, we show that $X_{(n)}$ is complete. Let $g$ be any measurable function such that $\\mathbb{E}_{\\theta}[g(X_{(n)})]=0$ for all $\\theta>0$. The density of $X_{(n)}$ is\n$$\nf_{X_{(n)}|\\theta}(x)=\\frac{2n}{\\theta^{2n}}x^{2n-1}\\mathbf{1}_{(0,\\theta)}(x).\n$$\nThus,\n$$\n0=\\mathbb{E}_{\\theta}[g(X_{(n)})]=\\int_{0}^{\\theta} g(x)\\frac{2n}{\\theta^{2n}}x^{2n-1}\\,dx\n\\quad\\text{for all}\\ \\theta>0.\n$$\nDefine $H(\\theta)=\\int_{0}^{\\theta} g(x)x^{2n-1}\\,dx$. Then the condition becomes $H(\\theta)=0$ for all $\\theta>0$. Differentiating with respect to $\\theta$ yields $g(\\theta)\\theta^{2n-1}=0$ for all $\\theta>0$, hence $g(x)=0$ almost everywhere. Therefore $X_{(n)}$ is complete.\n\nBy the Lehmann–Scheffé theorem, the UMVUE of $\\theta$ is the unique unbiased estimator that is a function of $X_{(n)}$. We now compute the distribution of $X_{(n)}$ and its expectation. The cumulative distribution function of $X$ is $F(x)=x^{2}/\\theta^{2}$ on $(0,\\theta)$, so\n$$\n\\mathbb{P}_{\\theta}(X_{(n)}\\le t)=\\left(F(t)\\right)^{n}=\\left(\\frac{t^{2}}{\\theta^{2}}\\right)^{n}=\\frac{t^{2n}}{\\theta^{2n}},\\quad 0<t<\\theta,\n$$\nand the corresponding density is\n$$\nf_{X_{(n)}|\\theta}(t)=\\frac{d}{dt}\\left(\\frac{t^{2n}}{\\theta^{2n}}\\right)=\\frac{2n}{\\theta^{2n}}t^{2n-1},\\quad 0<t<\\theta.\n$$\nTherefore,\n$$\n\\mathbb{E}_{\\theta}[X_{(n)}]\n=\\int_{0}^{\\theta} t\\cdot \\frac{2n}{\\theta^{2n}}t^{2n-1}\\,dt\n=\\frac{2n}{\\theta^{2n}}\\int_{0}^{\\theta} t^{2n}\\,dt\n=\\frac{2n}{\\theta^{2n}}\\cdot \\frac{\\theta^{2n+1}}{2n+1}\n=\\frac{2n}{2n+1}\\,\\theta.\n$$\nIt follows that the scaled statistic\n$$\n\\widehat{\\theta}_{\\text{UMVUE}}=\\frac{2n+1}{2n}\\,X_{(n)}\n$$\nsatisfies\n$$\n\\mathbb{E}_{\\theta}\\!\\left[\\frac{2n+1}{2n}X_{(n)}\\right]=\\theta,\n$$\nso it is unbiased. Since it is a function of the complete sufficient statistic $X_{(n)}$, it is the UMVUE for $\\theta$.", "answer": "$$\\boxed{\\frac{2n+1}{2n}\\,X_{(n)}}$$", "id": "1917722"}]}