{"hands_on_practices": [{"introduction": "To solidify our understanding of completeness, we begin with a foundational exercise involving the Gamma distribution. This problem is a classic illustration of how to prove completeness for a statistic derived from a distribution belonging to the exponential family. By working through this example, you will practice applying the uniqueness property of the Laplace transform, a powerful and common technique in mathematical statistics, to satisfy the definition of completeness [@problem_id:1905409].", "problem": "Let $X_1, X_2, \\ldots, X_n$ be a random sample from a Gamma distribution with a known shape parameter $\\alpha  0$ and an unknown rate parameter $\\beta  0$. The Probability Density Function (PDF) of this distribution is given by\n$$f(x | \\alpha, \\beta) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} x^{\\alpha-1} \\exp(-\\beta x)$$\nfor $x  0$. Let $T = \\sum_{i=1}^n X_i$ be the sum of the random variables in the sample. Which of the following assertions about the statistic $T$ is true?\n\nA. $T$ is a complete statistic for $\\beta$ because the family of distributions for the sample belongs to a regular one-parameter exponential family, and $T$ is the corresponding sufficient statistic.\n\nB. $T$ is not a complete statistic for $\\beta$ because the definition of completeness, which requires that $E_{\\beta}[g(T)]=0$ for all $\\beta0$ implies $g(T)=0$ almost surely, cannot be verified without a specific function $g$.\n\nC. $T$ is a sufficient statistic for $\\beta$, but it is not complete because the parameter space for $\\beta$, which is $(0, \\infty)$, is not the entire real line $\\mathbb{R}$.\n\nD. $T$ is a complete statistic for $\\beta$ only if the sample size $n$ is greater than the shape parameter $\\alpha$.\n\nE. $T$ is not a complete statistic for $\\beta$ because only minimal sufficient statistics can be complete, and $T$ is not minimal sufficient.", "solution": "Let $X_{1},\\dots,X_{n}$ be i.i.d. with density $f(x\\mid\\alpha,\\beta)=\\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}x^{\\alpha-1}\\exp(-\\beta x)$ for $x0$, with known $\\alpha0$ and unknown $\\beta0$. Let $T=\\sum_{i=1}^{n}X_{i}$.\n\n1) Sufficient statistic by factorization:\nThe joint density is\n$$\nL(\\beta\\mid x_{1},\\dots,x_{n})=\\prod_{i=1}^{n}\\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}x_{i}^{\\alpha-1}\\exp(-\\beta x_{i})\n=\\left(\\frac{1}{\\Gamma(\\alpha)^{n}}\\prod_{i=1}^{n}x_{i}^{\\alpha-1}\\right)\\beta^{n\\alpha}\\exp\\!\\left(-\\beta\\sum_{i=1}^{n}x_{i}\\right).\n$$\nBy the Neymanâ€“Fisher factorization theorem, $T=\\sum_{i=1}^{n}X_{i}$ is sufficient for $\\beta$.\n\n2) Minimal sufficiency:\nFor two samples $x=(x_{1},\\dots,x_{n})$ and $y=(y_{1},\\dots,y_{n})$, the likelihood ratio is\n$$\n\\frac{L(\\beta\\mid x)}{L(\\beta\\mid y)}=\\frac{\\prod_{i=1}^{n}x_{i}^{\\alpha-1}}{\\prod_{i=1}^{n}y_{i}^{\\alpha-1}}\\exp\\!\\left(-\\beta\\left(\\sum_{i=1}^{n}x_{i}-\\sum_{i=1}^{n}y_{i}\\right)\\right).\n$$\nThis ratio is free of $\\beta$ if and only if $\\sum_{i=1}^{n}x_{i}=\\sum_{i=1}^{n}y_{i}$. Hence $T$ is a minimal sufficient statistic for $\\beta$.\n\n3) Distribution of $T$:\nThe Laplace transform of $X_{i}$ is $E[\\exp(-sX_{i})]=\\left(\\frac{\\beta}{\\beta+s}\\right)^{\\alpha}$ for $s- \\beta$. Independence gives\n$$\nE[\\exp(-sT)]=\\prod_{i=1}^{n}\\left(\\frac{\\beta}{\\beta+s}\\right)^{\\alpha}=\\left(\\frac{\\beta}{\\beta+s}\\right)^{n\\alpha},\n$$\nwhich is the Laplace transform of a Gamma distribution with shape $n\\alpha$ and rate $\\beta$. Therefore\n$$\nT\\sim\\text{Gamma}(n\\alpha,\\beta),\\quad f_{T}(t\\mid\\beta)=\\frac{\\beta^{n\\alpha}}{\\Gamma(n\\alpha)}t^{n\\alpha-1}\\exp(-\\beta t),\\quad t0.\n$$\n\n4) Completeness of $T$:\nAssume a measurable function $g$ satisfies $E_{\\beta}[g(T)]=0$ for all $\\beta0$ and $E_{\\beta}[|g(T)|]\\infty$ for at least one (hence all) $\\beta0$. Using the density of $T$,\n$$\n0=E_{\\beta}[g(T)]=\\int_{0}^{\\infty}g(t)\\frac{\\beta^{n\\alpha}}{\\Gamma(n\\alpha)}t^{n\\alpha-1}\\exp(-\\beta t)\\,dt\n\\quad\\text{for all }\\beta0.\n$$\nEquivalently,\n$$\n\\int_{0}^{\\infty}\\bigl(g(t)t^{n\\alpha-1}\\bigr)\\exp(-\\beta t)\\,dt=0\n\\quad\\text{for all }\\beta0.\n$$\nDefine $h(t)=g(t)t^{n\\alpha-1}$. The function $\\beta\\mapsto\\int_{0}^{\\infty}h(t)\\exp(-\\beta t)\\,dt$ is the Laplace transform of $h$ on $(0,\\infty)$, and it equals $0$ for all $\\beta0$. By the uniqueness theorem for the Laplace transform, $h(t)=0$ for almost all $t0$. Since $t^{n\\alpha-1}0$ for all $t0$, this implies $g(t)=0$ almost surely with respect to the distribution of $T$. Hence $T$ is complete.\n\n5) Evaluation of the options:\n- A is true: the family is a regular one-parameter exponential family with open natural parameter space and $T$ is the natural sufficient statistic; as shown, $T$ is complete.\n- B is false: the completeness is verified via the Laplace transform argument.\n- C is false: completeness does not require the parameter space to be all of $\\mathbb{R}$; $(0,\\infty)$ (or the open natural parameter space $(-\\infty,0)$) suffices.\n- D is false: the completeness proof holds for all $n\\in\\mathbb{N}$ and $\\alpha0$; there is no requirement that $n\\alpha$.\n- E is false: completeness does not require minimality, and in fact $T$ is minimal sufficient here.\n\nTherefore, the correct assertion is A.", "answer": "$$\\boxed{A}$$", "id": "1905409"}, {"introduction": "Moving from continuous to discrete distributions, this next practice challenges you to assess completeness in a different context. Here, we examine a single observation from a discrete uniform distribution whose support depends on the unknown parameter $\\theta$. This scenario requires a direct algebraic approach rather than relying on integral transforms, providing valuable practice in applying the definition of completeness from first principles [@problem_id:1905386].", "problem": "A simple random number generator produces a single integer, $X$, drawn from a discrete uniform distribution on the set of integers $\\{1, 2, \\dots, \\theta\\}$, where the upper bound $\\theta$ is an unknown parameter that can be any integer greater than or equal to 1.\n\nIn statistical theory, a statistic $T$ is defined as being 'complete' for a family of distributions indexed by a parameter $\\theta$ if, for any suitably defined function $g$, the condition $E_{\\theta}[g(T)] = 0$ for all possible values of $\\theta$ implies that the probability $P_{\\theta}(g(T) = 0)$ is equal to 1 for all $\\theta$.\n\nConsider the statistic $T = X$, which is simply the observed integer itself. Which of the following statements about the completeness of $T$ for the parameter $\\theta$ is correct?\n\nA. Yes, $T$ is a complete statistic.\n\nB. No, $T$ is not a complete statistic because for the function defined as $g(1)=1$, $g(2)=-1$, and $g(t)=0$ for all $t  2$, the expectation $E_{\\theta}[g(T)]$ is zero for most values of $\\theta$, yet the function is not identically zero.\n\nC. No, $T$ is not a complete statistic because the support of the distribution, $\\{1, 2, \\dots, \\theta\\}$, depends on the parameter $\\theta$.\n\nD. No, $T$ is not a complete statistic because the function $g$ in the definition can depend on $\\theta$, and a valid non-zero choice is $g(t, \\theta) = t - E_{\\theta}[T]$, which always has an expectation of zero.", "solution": "Let $X$ have the discrete uniform distribution on $\\{1,2,\\dots,\\theta\\}$ with unknown integer parameter $\\theta \\geq 1$. Its probability mass function is\n$$\np_{\\theta}(x)=\\begin{cases}\n\\frac{1}{\\theta},  x \\in \\{1,2,\\dots,\\theta\\},\\\\\n0,  \\text{otherwise}.\n\\end{cases}\n$$\nBy definition, the statistic $T=X$ is complete for the family $\\{p_{\\theta}:\\theta \\geq 1\\}$ if for every function $g$ (not depending on $\\theta$) such that $E_{\\theta}[g(T)]=0$ for all $\\theta \\geq 1$, it follows that $P_{\\theta}(g(T)=0)=1$ for all $\\theta \\geq 1$.\n\nCompute the expectation:\n$$\nE_{\\theta}[g(T)] = \\sum_{t=1}^{\\theta} g(t) \\cdot \\frac{1}{\\theta} = \\frac{1}{\\theta} \\sum_{t=1}^{\\theta} g(t).\n$$\nIf $E_{\\theta}[g(T)]=0$ for all $\\theta \\geq 1$, then\n$$\n\\sum_{t=1}^{\\theta} g(t) = 0 \\quad \\text{for all } \\theta \\geq 1.\n$$\nDefine $S_{\\theta} = \\sum_{t=1}^{\\theta} g(t)$. Then $S_{\\theta}=0$ for all $\\theta \\geq 1$. For $k \\geq 2$,\n$$\ng(k) = S_{k} - S_{k-1} = 0 - 0 = 0,\n$$\nand for $k=1$,\n$$\ng(1) = S_{1} = 0.\n$$\nHence $g(k)=0$ for all positive integers $k$, which implies $P_{\\theta}(g(T)=0)=1$ for all $\\theta$. Therefore, $T=X$ is a complete statistic.\n\nRegarding the options:\n- A is correct, as shown.\n- B is invalid because its proposed $g$ yields $E_{\\theta}[g(T)]=1$ when $\\theta=1$, so the expectation is not zero for all $\\theta$.\n- C is incorrect because dependence of support on $\\theta$ does not preclude completeness; in fact, $T$ is complete here.\n- D is incorrect because the definition of completeness requires $g$ to be a function of $T$ only, not depending on $\\theta$.\n\nThus the correct choice is A.", "answer": "$$\\boxed{A}$$", "id": "1905386"}, {"introduction": "Not all sufficient statistics are complete. This final practice is crucial for understanding the boundaries of completeness by demonstrating a case of *incompleteness*. You will investigate the minimal sufficient statistic for a uniform distribution defined on the interval $[\\theta, 2\\theta]$ and show it is incomplete by constructing a non-zero function of the statistic whose expectation is zero for all values of $\\theta$. This exercise sharpens your intuition about why some statistics fail to be complete and highlights that even minimal sufficiency does not guarantee completeness [@problem_id:1905414].", "problem": "Let $X_1, X_2, \\dots, X_n$ be a random sample of size $n  1$ from a Uniform distribution on the interval $[\\theta, 2\\theta]$, where $\\theta  0$ is an unknown parameter. The order statistics are denoted by $X_{(1)} \\le X_{(2)} \\le \\dots \\le X_{(n)}$. The minimal sufficient statistic for $\\theta$ is the pair $T = (X_{(1)}, X_{(n)})$.\n\nA statistic $T$ is defined as incomplete if there exists a function $g(T)$, which is not identically zero for all possible outcomes of $T$, such that its expected value $E[g(T)] = 0$ for all possible values of the parameter $\\theta$.\n\nWhich of the following functions $g(T)$ demonstrates that the statistic $T=(X_{(1)}, X_{(n)})$ is incomplete for the family of Uniform distributions $U(\\theta, 2\\theta)$?\n\nA. $g(X_{(1)}, X_{(n)}) = (n+2)X_{(1)} - (2n+1)X_{(n)}$\n\nB. $g(X_{(1)}, X_{(n)}) = (2n+1)X_{(1)} - (n+2)X_{(n)}$\n\nC. $g(X_{(1)}, X_{(n)}) = (n+1)X_{(1)} - (2n+1)X_{(n)}$\n\nD. $g(X_{(1)}, X_{(n)}) = X_{(n)} - 2X_{(1)}$\n\nE. $g(X_{(1)}, X_{(n)}) = (n+2)X_{(1)} - (2n-1)X_{(n)}$", "solution": "We sample $X_{1},\\dots,X_{n}$ iid from $U(\\theta,2\\theta)$ with $\\theta0$ and denote the order statistics by $X_{(1)}\\leq \\dots \\leq X_{(n)}$. For a Uniform$(a,b)$ sample, the $k$-th order statistic satisfies $X_{(k)}=a+(b-a)U_{(k)}$ with $U_{(k)}\\sim \\mathrm{Beta}(k,n+1-k)$, hence\n$$\n\\mathbb{E}[X_{(k)}]=a+(b-a)\\frac{k}{n+1}.\n$$\nApplying this with $a=\\theta$ and $b=2\\theta$ gives\n$$\n\\mathbb{E}[X_{(1)}]=\\theta+\\theta\\frac{1}{n+1}=\\theta\\frac{n+2}{n+1},\\qquad\n\\mathbb{E}[X_{(n)}]=\\theta+\\theta\\frac{n}{n+1}=\\theta\\frac{2n+1}{n+1}.\n$$\nConsider a linear function $g(X_{(1)},X_{(n)})=\\alpha X_{(1)}-\\beta X_{(n)}$. Its expectation is\n$$\n\\mathbb{E}[g]=\\alpha\\,\\mathbb{E}[X_{(1)}]-\\beta\\,\\mathbb{E}[X_{(n)}]\n=\\theta\\left[\\alpha\\frac{n+2}{n+1}-\\beta\\frac{2n+1}{n+1}\\right].\n$$\nFor $\\mathbb{E}[g]=0$ for all $\\theta0$, we require\n$$\n\\alpha(n+2)-\\beta(2n+1)=0.\n$$\nA nontrivial choice is $\\alpha=2n+1$ and $\\beta=n+2$, which yields\n$$\ng(X_{(1)},X_{(n)})=(2n+1)X_{(1)}-(n+2)X_{(n)}.\n$$\nThis function is not identically zero and satisfies $\\mathbb{E}[g]=0$ for all $\\theta0$, thereby demonstrating incompleteness of $T=(X_{(1)},X_{(n)})$. Among the given options, this corresponds to option B.", "answer": "$$\\boxed{B}$$", "id": "1905414"}]}