{"hands_on_practices": [{"introduction": "This first exercise provides a foundational walkthrough of applying Basu's Theorem. By performing a clever transformation on the Weibull random variables, we can simplify the problem into a classic scenario involving the well-understood Exponential distribution. This practice reinforces the core procedure: identifying a complete sufficient statistic and an ancillary statistic to prove independence [@problem_id:1898189].", "problem": "Let $X_1, X_2, \\ldots, X_n$ be a random sample of size $n \\geq 2$ from a Weibull distribution. The probability density function (PDF) for a single observation $X$ is given by:\n$$f(x | \\lambda) = \\frac{k}{\\lambda^k} x^{k-1} \\exp\\left(-\\left(\\frac{x}{\\lambda}\\right)^k\\right), \\quad \\text{for } x > 0$$\nIn this model, the shape parameter $k > 0$ is a known constant, while the scale parameter $\\lambda > 0$ is unknown.\n\nConsider the following two statistics:\n1.  The sum statistic, $T = \\sum_{i=1}^{n} X_i^k$.\n2.  The ratio statistic, $V = \\frac{X_1^k}{X_2^k}$.\n\nWhich of the following statements correctly describes the statistical relationship between $T$ and $V$?\n\nA. $T$ and $V$ are statistically independent.\n\nB. $T$ and $V$ are statistically dependent.\n\nC. The independence of $T$ and $V$ depends on the specific value of the known shape parameter $k$.\n\nD. The independence of $T$ and $V$ depends on the sample size $n$.", "solution": "To determine the relationship between the statistics $T = \\sum_{i=1}^{n} X_i^k$ and $V = \\frac{X_1^k}{X_2^k}$, we will use Basu's Theorem. The theorem states that if a statistic $T(\\mathbf{X})$ is a complete sufficient statistic for a parameter $\\theta$, and another statistic $V(\\mathbf{X})$ is ancillary (i.e., its distribution does not depend on $\\theta$), then $T(\\mathbf{X})$ and $V(\\mathbf{X})$ are statistically independent.\n\nOur parameter is $\\lambda$. We will proceed in three steps:\n1.  Find a sufficient statistic for $\\lambda$ and check if it is complete.\n2.  Check if the statistic $V$ is ancillary.\n3.  Apply Basu's Theorem.\n\n**Step 1: Finding a Complete Sufficient Statistic**\n\nFirst, let's find the distribution of $Y_i = X_i^k$. This is a transformation of a random variable. Let $y = g(x) = x^k$. The inverse transformation is $x = g^{-1}(y) = y^{1/k}$ for $y>0$. The Jacobian of the transformation is $\\frac{dx}{dy} = \\frac{1}{k} y^{\\frac{1}{k}-1}$.\n\nThe PDF of $Y_i$ is given by $f_Y(y) = f_X(g^{-1}(y)) \\left|\\frac{dx}{dy}\\right|$:\n$$f_Y(y) = \\frac{k}{\\lambda^k} (y^{1/k})^{k-1} \\exp\\left(-\\left(\\frac{y^{1/k}}{\\lambda}\\right)^k\\right) \\cdot \\left|\\frac{1}{k} y^{\\frac{1}{k}-1}\\right|$$\n$$f_Y(y) = \\frac{k}{\\lambda^k} y^{1 - 1/k} \\exp\\left(-\\frac{y}{\\lambda^k}\\right) \\cdot \\frac{1}{k} y^{\\frac{1}{k}-1}$$\n$$f_Y(y) = \\frac{1}{\\lambda^k} y^{(1 - 1/k) + (1/k - 1)} \\exp\\left(-\\frac{y}{\\lambda^k}\\right)$$\n$$f_Y(y) = \\frac{1}{\\lambda^k} \\exp\\left(-\\frac{y}{\\lambda^k}\\right)$$\nThis is the PDF of an exponential distribution with rate parameter $1/\\lambda^k$ or, equivalently, scale parameter $\\theta = \\lambda^k$. So, $Y_i = X_i^k \\sim \\text{Exponential}(\\text{scale}=\\lambda^k)$.\n\nNow, we consider the family of distributions for $Y_i$. Since the parameter is $\\lambda>0$, the scale parameter $\\theta = \\lambda^k$ can take any value in $(0, \\infty)$. The joint PDF of the sample $X_1, \\ldots, X_n$ is:\n$$L(\\lambda | \\mathbf{x}) = \\prod_{i=1}^n f(x_i | \\lambda) = \\prod_{i=1}^n \\frac{k}{\\lambda^k} x_i^{k-1} \\exp\\left(-\\frac{x_i^k}{\\lambda^k}\\right)$$\n$$L(\\lambda | \\mathbf{x}) = \\left(\\frac{k}{\\lambda^k}\\right)^n \\left(\\prod_{i=1}^n x_i\\right)^{k-1} \\exp\\left(-\\frac{1}{\\lambda^k} \\sum_{i=1}^n x_i^k\\right)$$\nBy the Fisher-Neyman Factorization Theorem, $L(\\lambda | \\mathbf{x}) = g(T(\\mathbf{x}), \\lambda) h(\\mathbf{x})$, we can identify $T(\\mathbf{X}) = \\sum_{i=1}^n X_i^k$ as a sufficient statistic for $\\lambda$.\n\nTo check for completeness, we examine the distribution of $T = \\sum_{i=1}^n X_i^k = \\sum_{i=1}^n Y_i$. Since the $Y_i$ are i.i.d. Exponential random variables with scale $\\theta = \\lambda^k$, their sum $T$ follows a Gamma distribution with shape parameter $n$ and scale parameter $\\theta = \\lambda^k$.\nThe PDF of $T$ is:\n$$f_T(t) = \\frac{1}{\\Gamma(n)\\theta^n} t^{n-1} \\exp\\left(-\\frac{t}{\\theta}\\right), \\quad t > 0$$\nThis family of distributions for $T$ indexed by $\\theta = \\lambda^k \\in (0, \\infty)$ forms a one-parameter exponential family. The support of $T$, $(0, \\infty)$, does not depend on the parameter $\\theta$. The parameter space for $\\theta$ is an open interval. A well-known result states that the one-parameter exponential family is complete if the parameter space contains an open interval. Thus, $T$ is a complete sufficient statistic for $\\theta = \\lambda^k$, and therefore for $\\lambda$.\n\n**Step 2: Checking if V is Ancillary**\n\nAn ancillary statistic is one whose distribution does not depend on the parameter $\\lambda$. Let's examine the statistic $V = \\frac{X_1^k}{X_2^k}$.\nUsing our earlier transformation, $V = \\frac{Y_1}{Y_2}$, where $Y_1, Y_2$ are i.i.d. $\\text{Exponential}(\\text{scale}=\\lambda^k)$.\nWe can write $Y_1 = (\\lambda^k) Z_1$ and $Y_2 = (\\lambda^k) Z_2$, where $Z_1, Z_2$ are i.i.d. from a standard exponential distribution, i.e., $\\text{Exponential}(\\text{scale}=1)$.\nSubstituting these into the expression for $V$:\n$$V = \\frac{(\\lambda^k) Z_1}{(\\lambda^k) Z_2} = \\frac{Z_1}{Z_2}$$\nThe distribution of $V$ is the distribution of a ratio of two independent standard exponential random variables. This distribution does not depend on $\\lambda^k$ or $\\lambda$. Therefore, $V$ is an ancillary statistic.\n\n**Step 3: Applying Basu's Theorem**\n\nWe have established that:\n1.  $T = \\sum_{i=1}^n X_i^k$ is a complete sufficient statistic for the parameter $\\lambda$.\n2.  $V = \\frac{X_1^k}{X_2^k}$ is an ancillary statistic.\n\nAccording to Basu's Theorem, a complete sufficient statistic is independent of any ancillary statistic. Therefore, $T$ and $V$ are statistically independent. This holds for any sample size $n \\geq 2$ and for any known shape parameter $k>0$.\n\nThis means statement A is correct, and B, C, and D are incorrect.", "answer": "$$\\boxed{A}$$", "id": "1898189"}, {"introduction": "Building upon the basic framework, this problem explores a more subtle source of ancillary statistics: scale invariance. We will see how statistics constructed to be invariant to the scale parameter $\\sigma$ of a distribution family, such as the Rayleigh distribution, naturally have distributions that do not depend on $\\sigma$. This exercise demonstrates that recognizing underlying structural properties of a model is key to effectively applying Basu's Theorem [@problem_id:1898157].", "problem": "Let $X_1, X_2, \\dots, X_n$ be a random sample of size $n > 1$ from a Rayleigh distribution with an unknown scale parameter $\\sigma > 0$. The probability density function (PDF) for a single observation $X$ is given by\n$$ f(x; \\sigma) = \\frac{x}{\\sigma^2} \\exp\\left(-\\frac{x^2}{2\\sigma^2}\\right) \\quad \\text{for } x \\ge 0 $$\nConsider two statistics calculated from this sample:\n1.  The sum of squares, $T = \\sum_{i=1}^{n} X_i^2$.\n2.  The ratio of the sample mean to the sample standard deviation, $V = \\frac{\\bar{X}}{S}$, where $\\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_i$ is the sample mean and $S = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n} (X_i - \\bar{X})^2}$ is the sample standard deviation.\n\nA statistic is called *sufficient* for a parameter if it captures all the information about the parameter that is contained in the sample. A statistic is called *complete* if the only real-valued function of the statistic that has an expected value of zero for all values of the parameter is the function that is identically zero. A statistic is called *ancillary* for a parameter if its sampling distribution does not depend on the parameter.\n\nWhich of the following statements about the statistics $T$ and $V$ are true? Select all that apply.\n\nA. The statistic $T$ is ancillary for $\\sigma$.\n\nB. The statistic $V$ is ancillary for $\\sigma$.\n\nC. The statistic $T$ is a complete sufficient statistic for $\\sigma$.\n\nD. The statistics $T$ and $V$ are statistically independent.", "solution": "We start by identifying a key transformation for Rayleigh random variables. For a single observation with density\n$$\nf(x;\\sigma)=\\frac{x}{\\sigma^{2}}\\exp\\!\\left(-\\frac{x^{2}}{2\\sigma^{2}}\\right),\\quad x\\ge 0,\n$$\nlet $Y=X^{2}$. Then $x=\\sqrt{y}$ and $\\frac{dx}{dy}=\\frac{1}{2\\sqrt{y}}$, so the density of $Y$ is\n$$\nf_{Y}(y;\\sigma)=f_{X}(\\sqrt{y};\\sigma)\\cdot\\frac{1}{2\\sqrt{y}}=\\frac{1}{2\\sigma^{2}}\\exp\\!\\left(-\\frac{y}{2\\sigma^{2}}\\right),\\quad y\\ge 0,\n$$\nthat is, $Y\\sim\\text{Exp}(\\text{rate }1/(2\\sigma^{2}))$, or equivalently $Y\\sim\\text{Gamma}(\\text{shape }1,\\text{ rate }1/(2\\sigma^{2}))$.\n\nFor a sample $X_{1},\\dots,X_{n}$, the variables $Y_{i}=X_{i}^{2}$ are independent exponentials with common rate $1/(2\\sigma^{2})$, hence\n$$\nT=\\sum_{i=1}^{n}X_{i}^{2}=\\sum_{i=1}^{n}Y_{i}\\sim\\text{Gamma}\\!\\left(\\text{shape }n,\\ \\text{rate }\\frac{1}{2\\sigma^{2}}\\right),\n$$\nwith density\n$$\nf_{T}(t;\\sigma)=\\frac{\\left(\\frac{1}{2\\sigma^{2}}\\right)^{n}}{\\Gamma(n)}\\,t^{n-1}\\exp\\!\\left(-\\frac{t}{2\\sigma^{2}}\\right),\\quad t\\ge 0.\n$$\n\nAssessment of statement A (T is ancillary): Since the distribution of $T$ depends on $\\sigma$ through the rate (or scale) parameter, $T$ is not ancillary for $\\sigma$. Therefore A is false.\n\nSufficiency of $T$: The joint likelihood of $X_{1},\\dots,X_{n}$ is\n$$\nL(\\sigma;x_{1},\\dots,x_{n})=\\prod_{i=1}^{n}\\frac{x_{i}}{\\sigma^{2}}\\exp\\!\\left(-\\frac{x_{i}^{2}}{2\\sigma^{2}}\\right)\\mathbf{1}_{\\{x_{i}\\ge 0\\}}\n=\\left(\\prod_{i=1}^{n}x_{i}\\right)\\sigma^{-2n}\\exp\\!\\left(-\\frac{1}{2\\sigma^{2}}\\sum_{i=1}^{n}x_{i}^{2}\\right)\\prod_{i=1}^{n}\\mathbf{1}_{\\{x_{i}\\ge 0\\}}.\n$$\nBy the Neyman–Fisher factorization criterion, the data enter the likelihood involving $\\sigma$ only through $T=\\sum_{i=1}^{n}x_{i}^{2}$, so $T$ is sufficient for $\\sigma$.\n\nCompleteness of $T$: Suppose $g$ is a measurable function such that $\\mathbb{E}_{\\sigma}[g(T)]=0$ for all $\\sigma>0$. Using the density of $T$,\n$$\n0=\\int_{0}^{\\infty}g(t)\\frac{\\left(\\frac{1}{2\\sigma^{2}}\\right)^{n}}{\\Gamma(n)}\\,t^{n-1}\\exp\\!\\left(-\\frac{t}{2\\sigma^{2}}\\right)\\,dt\\quad\\text{for all }\\sigma>0.\n$$\nLet $\\lambda=\\frac{1}{2\\sigma^{2}}$, so $\\lambda>0$. Multiply both sides by $\\Gamma(n)$ and define $h(t)=g(t)t^{n-1}$. Then\n$$\n\\int_{0}^{\\infty}h(t)\\exp(-\\lambda t)\\,dt=0\\quad\\text{for all }\\lambda>0.\n$$\nThe left-hand side is the Laplace transform of $h$ evaluated at $\\lambda$, and uniqueness of the Laplace transform on $[0,\\infty)$ implies $h(t)=0$ almost everywhere, hence $g(T)=0$ almost surely. Therefore, $T$ is complete. Combining sufficiency and completeness, statement C is true.\n\nAssessment of statement B (V is ancillary): Note that the Rayleigh family is a scale family. Write $X_{i}=\\sigma R_{i}$ with $R_{i}\\sim\\text{Rayleigh}(1)$ i.i.d. Then\n$$\n\\bar{X}=\\sigma\\bar{R},\\qquad S=\\sigma S_{R},\n$$\nwhere $\\bar{R}$ and $S_{R}$ are the sample mean and sample standard deviation computed from $R_{1},\\dots,R_{n}$. Therefore\n$$\nV=\\frac{\\bar{X}}{S}=\\frac{\\bar{R}}{S_{R}},\n$$\nwhose sampling distribution does not depend on $\\sigma$. Hence $V$ is ancillary, and B is true.\n\nAssessment of statement D (independence of T and V): By Basu’s theorem, any complete sufficient statistic is independent of any ancillary statistic. Since $T$ is complete and sufficient for $\\sigma$ and $V$ is ancillary for $\\sigma$, it follows that $T$ and $V$ are independent. Therefore D is true.\n\nCollecting the results: A is false, B is true, C is true, D is true.", "answer": "$$\\boxed{BCD}$$", "id": "1898157"}, {"introduction": "The true power of Basu's Theorem lies not just in proving independence, but in using that independence to simplify complex calculations. This advanced problem challenges you to compute the moment of a cleverly constructed statistic from a Uniform distribution. The key insight is that the statistic is ancillary, and its independence from the complete sufficient statistic makes the calculation tractable and elegant [@problem_id:1898164].", "problem": "A random sample $X_1, X_2, \\dots, X_{11}$ of size $n=11$ is drawn from a continuous uniform distribution on the interval $(0, \\theta)$, where $\\theta > 0$ is an unknown parameter. Let $X_{(11)}$ denote the maximum value in the sample, i.e., $X_{(11)} = \\max\\{X_1, X_2, \\dots, X_{11}\\}$. A statistician defines a new random variable, $V$, as the count of observations in the sample that are strictly less than one-third of the sample maximum. Specifically, $V = \\sum_{i=1}^{11} I(X_i < X_{(11)}/3)$, where $I(\\cdot)$ is the indicator function.\n\nCalculate the expected value of the square of this statistic, $E[V^2]$. Express your answer as a fraction in simplest form.", "solution": "Let $n=11$, and let $M=X_{(11)}=\\max\\{X_{1},\\dots,X_{11}\\}$. Define $I_{i}=I(X_{i}<M/3)$ so that $V=\\sum_{i=1}^{11} I_{i}$. In a continuous sample, the maximum is unique with probability one, so there exists an index $K$ such that $X_{K}=M$ and hence $I_{K}=0$.\n\nCondition on both $M=m$ and the index $K$. For $i\\neq K$, the conditional distribution of $X_{i}$ given $M=m$ and $K$ is $\\operatorname{Uniform}(0,m)$, independently across $i\\neq K$. Therefore, for $i\\neq K$,\n$$\n\\mathbb{P}(I_{i}=1\\mid M=m,K)=\\mathbb{P}(X_{i}<m/3\\mid M=m,K)=\\frac{1}{3},\n$$\nand the indicators $\\{I_{i}:i\\neq K\\}$ are independent. Hence,\n$$\nV\\mid(M=m,K)\\sim \\operatorname{Binomial}(N,p)\\quad\\text{with}\\quad N=10,\\;p=\\frac{1}{3}.\n$$\nThus,\n$$\n\\mathbb{E}[V\\mid M,K]=N p=\\frac{10}{3},\\qquad \\operatorname{Var}(V\\mid M,K)=N p(1-p)=10\\cdot\\frac{1}{3}\\cdot\\frac{2}{3}=\\frac{20}{9}.\n$$\nUsing $\\mathbb{E}[V^{2}\\mid M,K]=\\operatorname{Var}(V\\mid M,K)+\\bigl(\\mathbb{E}[V\\mid M,K]\\bigr)^{2}$ gives\n$$\n\\mathbb{E}[V^{2}\\mid M,K]=\\frac{20}{9}+\\left(\\frac{10}{3}\\right)^{2}=\\frac{20}{9}+\\frac{100}{9}=\\frac{120}{9}=\\frac{40}{3}.\n$$\nSince the right-hand side does not depend on $M$ or $K$, taking expectations yields\n$$\n\\mathbb{E}[V^{2}]=\\frac{40}{3}.\n$$", "answer": "$$\\boxed{\\frac{40}{3}}$$", "id": "1898164"}]}