{"hands_on_practices": [{"introduction": "Our first practice problem provides a classic application of the Sufficiency Principle. Here, we analyze data from a Negative Binomial distribution, a common model for 'time-to-success' experiments, and seek to condense multiple observations into a single informative number. This exercise is a direct application of the Neyman-Fisher Factorization Theorem, offering a perfect opportunity to practice the core technique of separating the likelihood function into its constituent parts.", "problem": "In a study of particle decay, a physicist observes a specific type of rare decay event. The experiment consists of observing particles one by one until a fixed number, $r$, of these rare decay events have been recorded. The probability that any given observed particle undergoes this specific decay is $p$, which is unknown. This entire experimental procedure is repeated independently $n$ times to create a statistically significant dataset.\n\nFor the $i$-th repetition (where $i=1, \\dots, n$), let the random variable $Y_i$ represent the total number of particles that had to be observed to see the $r$-th decay event. Each $Y_i$ is independent and follows a Negative Binomial distribution, with the probability mass function (PMF) given by:\n$$f(y_i; p) = \\binom{y_i-1}{r-1} p^r (1-p)^{y_i-r}, \\quad \\text{for } y_i \\in \\{r, r+1, r+2, \\dots\\}$$\nHere, the parameter $r$ is a known, fixed positive integer.\n\nBased on a random sample of observations $Y_1, Y_2, \\dots, Y_n$, determine a sufficient statistic for the unknown parameter $p$. Your answer should be an expression in terms of the sample random variables $Y_1, \\dots, Y_n$.", "solution": "Each $Y_{i}$ has pmf $f(y_{i};p)=\\binom{y_{i}-1}{r-1}p^{r}(1-p)^{y_{i}-r}$ for $y_{i}\\in\\{r,r+1,\\dots\\}$, with fixed known $r$. For independent observations $Y_{1},\\dots,Y_{n}$, the joint likelihood is\n$$\nL(p;y_{1},\\dots,y_{n})=\\prod_{i=1}^{n}\\binom{y_{i}-1}{r-1}p^{r}(1-p)^{y_{i}-r}.\n$$\nCollecting terms in $p$ and $(1-p)$ gives\n$$\nL(p;y_{1},\\dots,y_{n})=\\left[\\prod_{i=1}^{n}\\binom{y_{i}-1}{r-1}\\right]\\,p^{nr}\\,(1-p)^{\\sum_{i=1}^{n}y_{i}-nr}.\n$$\nThis factors as $L(p;y)=h(y)\\,g(T(y),p)$ with\n$$\nh(y)=\\prod_{i=1}^{n}\\binom{y_{i}-1}{r-1},\\quad T(y)=\\sum_{i=1}^{n}y_{i},\\quad g(T,p)=p^{nr}(1-p)^{T-nr},\n$$\nwhere $h$ does not depend on $p$ and $g$ depends on the data only through $T(y)$. By the Neyman–Fisher factorization theorem, $T(Y)=\\sum_{i=1}^{n}Y_{i}$ is sufficient for $p$. Since $n$ and $r$ are known constants, any one-to-one function of $T$, such as $\\sum_{i=1}^{n}(Y_{i}-r)$, is also sufficient; a conventional choice is $T=\\sum_{i=1}^{n}Y_{i}$.", "answer": "$$\\boxed{\\sum_{i=1}^{n} Y_{i}}$$", "id": "1963683"}, {"introduction": "Moving beyond standard distributions, this problem explores sufficiency in a geometric context where the parameter of interest defines a physical boundary. By analyzing particle impacts within a circular detector, you will see how to handle parameters that appear in the support of the distribution rather than just its functional form. This scenario [@problem_id:1963659] highlights the critical role of the indicator function in the likelihood, a common feature in problems involving uniform distributions on bounded sets.", "problem": "A circular particle detector with an unknown radius $R$ is centered at the origin of a 2D coordinate system. It is known from the physics of the experiment that when particles are detected, their impact locations $(X, Y)$ are uniformly distributed over the area of the detector. An experiment is conducted, and a random sample of $n$ particle impact locations is recorded: $(X_1, Y_1), (X_2, Y_2), \\dots, (X_n, Y_n)$.\n\nAccording to the principle of sufficiency in statistical inference, a sufficient statistic for a parameter is a statistic that captures all the information about the parameter that is contained in the sample. Which one of the following statistics is a sufficient statistic for the unknown radius $R$?\n\nA. $T_A = \\max_{i=1,...,n} \\sqrt{X_i^2 + Y_i^2}$\n\nB. $T_B = \\sum_{i=1}^n (X_i^2 + Y_i^2)$\n\nC. $T_C = \\left(\\frac{1}{n}\\sum_{i=1}^n X_i, \\frac{1}{n}\\sum_{i=1}^n Y_i\\right)$\n\nD. $T_D = \\frac{1}{n} \\sum_{i=1}^n \\sqrt{X_i^2 + Y_i^2}$\n\nE. $T_E = \\max_{i=1,...,n} |X_i|$", "solution": "Let $(X_{i},Y_{i})$, $i=1,\\dots,n$, be i.i.d. uniform over the disk of radius $R$ centered at the origin. The joint density of a single observation is\n$$\nf_{X,Y}(x,y;R)=\\frac{1}{\\pi R^{2}}\\mathbf{1}\\{x^{2}+y^{2}\\le R^{2}\\},\n$$\nso the joint density (likelihood) for the sample is\n$$\nL(R;\\{(x_{i},y_{i})\\}_{i=1}^{n})=\\prod_{i=1}^{n}\\frac{1}{\\pi R^{2}}\\mathbf{1}\\{x_{i}^{2}+y_{i}^{2}\\le R^{2}\\}=(\\pi R^{2})^{-n}\\,\\mathbf{1}\\Big\\{\\max_{1\\le i\\le n}(x_{i}^{2}+y_{i}^{2})\\le R^{2}\\Big\\}.\n$$\nDefine the sample statistic\n$$\nT_{A}=\\max_{1\\le i\\le n}\\sqrt{X_{i}^{2}+Y_{i}^{2}}.\n$$\nThen the likelihood can be written as\n$$\nL(R;\\text{data})=(\\pi R^{2})^{-n}\\,\\mathbf{1}\\{T_{A}\\le R\\},\n$$\nwhich is of the form $g(T_{A},R)h(\\text{data})$ with $g(T_{A},R)=(\\pi R^{2})^{-n}\\mathbf{1}\\{T_{A}\\le R\\}$ and $h(\\text{data})=1$. By the Neyman–Fisher factorization theorem, $T_{A}$ is sufficient for $R$.\n\nTo see that the other options are not sufficient, note that the indicator in the likelihood depends on the sample only through $\\max_{i}(X_{i}^{2}+Y_{i}^{2})$, equivalently $T_{A}$. Any statistic that does not determine this maximum radius cannot render the factorization independent of the remaining sample details:\n- $T_{B}=\\sum_{i=1}^{n}(X_{i}^{2}+Y_{i}^{2})$ does not determine the maximum; different samples can have the same sum of squares but different maxima, leading to different support sets $\\{R:T_{A}\\le R\\}$.\n- $T_{C}=(\\bar{X},\\bar{Y})$ does not determine the support boundary and does not capture the constraint $\\max_{i}(X_{i}^{2}+Y_{i}^{2})\\le R^{2}$.\n- $T_{D}=\\frac{1}{n}\\sum_{i=1}^{n}\\sqrt{X_{i}^{2}+Y_{i}^{2}}$ again averages radii and does not determine the maximum radius.\n- $T_{E}=\\max_{i}|X_{i}|$ ignores the $Y$-coordinates in the radial constraint; two samples can have the same $T_{E}$ but different $\\max_{i}\\sqrt{X_{i}^{2}+Y_{i}^{2}}$, so it cannot determine the support.\n\nTherefore, among the given options, the sufficient statistic for $R$ is $T_{A}$.", "answer": "$$\\boxed{A}$$", "id": "1963659"}, {"introduction": "This final exercise serves as a crucial test of your understanding and a warning against relying on unchecked intuition. While the sample mean, $\\bar{X}$, is often a sufficient statistic for a location parameter, this is not always the case. The Cauchy distribution [@problem_id:1963688] is a famous counterexample, and this problem challenges you to use the factorization theorem to prove precisely why $\\bar{X}$ fails to capture all the information about the location parameter $\\theta$.", "problem": "Consider a random sample $X_1, X_2, \\dots, X_n$ drawn from a Cauchy distribution with a location parameter $\\theta$ and a fixed scale parameter of 1. The Probability Density Function (PDF) for a single observation $X$ is given by:\n$$ f(x|\\theta) = \\frac{1}{\\pi(1+(x-\\theta)^2)}, \\quad \\text{for } x \\in (-\\infty, \\infty) $$\nA statistician investigates whether the sample mean, $T(\\mathbf{X}) = \\bar{X} = \\frac{1}{n}\\sum_{i=1}^n X_i$, is a sufficient statistic for the unknown parameter $\\theta$.\n\nThe Neyman-Fisher Factorization Theorem states that a statistic $T(\\mathbf{X})$ is sufficient for a parameter $\\theta$ if and only if the joint PDF of the sample, $L(\\theta; \\mathbf{x}) = f(\\mathbf{x}|\\theta)$, can be factored into two non-negative functions:\n$$ L(\\theta; \\mathbf{x}) = g(T(\\mathbf{x}), \\theta) \\cdot h(\\mathbf{x}) $$\nwhere the function $g$ depends on the data $\\mathbf{x}$ only through the statistic $T(\\mathbf{x})$, and the function $h$ does not depend on the parameter $\\theta$.\n\nAfter attempting to apply this theorem, the statistician correctly concludes that the sample mean $\\bar{X}$ is not a sufficient statistic for $\\theta$. Which of the following statements provides the correct mathematical justification for this conclusion?\n\nA. The sample mean of a Cauchy distribution is an unbiased estimator of $\\theta$, but it is not a complete statistic, which is a necessary condition for sufficiency.\n\nB. The joint PDF, $L(\\theta; \\mathbf{x}) = \\prod_{i=1}^n \\frac{1}{\\pi(1+(x_i-\\theta)^2)}$, cannot be rewritten such that its entire dependence on $\\theta$ is encapsulated in a function that interacts with the data $\\mathbf{x}$ solely through the value of the sample mean $\\bar{x}$.\n\nC. The function $h(\\mathbf{x})$ in the factorization theorem must be independent of any statistic, including the sample mean, a condition that cannot be met.\n\nD. The factorization theorem is not applicable because the expected value of the Cauchy distribution is undefined. A statistic can only be sufficient if the distribution has a finite mean.\n\nE. The likelihood function can be factored as required, but the resulting sufficient statistic is the set of all order statistics, $\\{X_{(1)}, \\dots, X_{(n)}\\}$, not the sample mean. While this is true, it is not the reason the factorization fails for the sample mean itself.", "solution": "We are given an iid sample $X_{1},\\dots,X_{n}$ from the Cauchy$(\\theta,1)$ family with density\n$$\nf(x\\mid\\theta)=\\frac{1}{\\pi\\left(1+(x-\\theta)^{2}\\right)}.\n$$\nThe Neyman–Fisher factorization theorem says that a statistic $T(\\mathbf{X})$ is sufficient for $\\theta$ if and only if the likelihood $L(\\theta;\\mathbf{x})=\\prod_{i=1}^{n}f(x_{i}\\mid\\theta)$ can be written as\n$$\nL(\\theta;\\mathbf{x})=g\\bigl(T(\\mathbf{x}),\\theta\\bigr)\\,h(\\mathbf{x}),\n$$\nwhere $h$ does not depend on $\\theta$. An equivalent characterization is: if $T$ is sufficient, then for any two samples $\\mathbf{x}$ and $\\mathbf{y}$ with $T(\\mathbf{x})=T(\\mathbf{y})$, the likelihood ratio\n$$\nR(\\theta;\\mathbf{x},\\mathbf{y})=\\frac{L(\\theta;\\mathbf{x})}{L(\\theta;\\mathbf{y})}\n$$\nmust be independent of $\\theta$.\n\nWe test $T(\\mathbf{X})=\\bar{X}$. It suffices to show failure for $n=2$ (if $\\bar{X}$ were sufficient for general $n$, it would in particular be sufficient when $n=2$). Let $\\mathbf{x}=(x_{1},x_{2})$ and $\\mathbf{y}=(y_{1},y_{2})$ satisfy $x_{1}+x_{2}=y_{1}+y_{2}$ (same sample mean) but with $\\{x_{1},x_{2}\\}\\neq\\{y_{1},y_{2}\\}$. The likelihood is\n$$\nL(\\theta;\\mathbf{x})=\\prod_{i=1}^{2}\\frac{1}{\\pi\\left(1+(x_{i}-\\theta)^{2}\\right)},\\qquad\nL(\\theta;\\mathbf{y})=\\prod_{i=1}^{2}\\frac{1}{\\pi\\left(1+(y_{i}-\\theta)^{2}\\right)}.\n$$\nHence\n$$\nR(\\theta;\\mathbf{x},\\mathbf{y})=\\frac{L(\\theta;\\mathbf{x})}{L(\\theta;\\mathbf{y})}\n=\\frac{\\prod_{i=1}^{2}\\left(1+(y_{i}-\\theta)^{2}\\right)}{\\prod_{i=1}^{2}\\left(1+(x_{i}-\\theta)^{2}\\right)}\n=\\frac{P_{\\mathbf{y}}(\\theta)}{P_{\\mathbf{x}}(\\theta)},\n$$\nwhere\n$$\nP_{\\mathbf{x}}(\\theta)=\\prod_{i=1}^{2}\\left[(\\theta-x_{i})^{2}+1\\right],\\qquad\nP_{\\mathbf{y}}(\\theta)=\\prod_{i=1}^{2}\\left[(\\theta-y_{i})^{2}+1\\right].\n$$\nIf $R(\\theta;\\mathbf{x},\\mathbf{y})$ were independent of $\\theta$, then $P_{\\mathbf{y}}(\\theta)=c\\,P_{\\mathbf{x}}(\\theta)$ for some constant $c$ and all $\\theta$. Since both polynomials have leading coefficient $1$ (in $\\theta^{4}$), necessarily $c=1$, so $P_{\\mathbf{y}}(\\theta)\\equiv P_{\\mathbf{x}}(\\theta)$ as polynomials.\n\nWrite $S_{\\mathbf{x}}=x_{1}+x_{2}$ and $P_{\\mathbf{x}}^{(1)}=x_{1}x_{2}$, and similarly $S_{\\mathbf{y}}=y_{1}+y_{2}$ and $P_{\\mathbf{y}}^{(1)}=y_{1}y_{2}$. A direct expansion gives\n$$\nP_{\\mathbf{x}}(\\theta)=\\theta^{4}-2S_{\\mathbf{x}}\\theta^{3}+\\left(S_{\\mathbf{x}}^{2}+2\\right)\\theta^{2}-2S_{\\mathbf{x}}\\left(P_{\\mathbf{x}}^{(1)}+1\\right)\\theta+\\left[\\left(P_{\\mathbf{x}}^{(1)}\\right)^{2}+S_{\\mathbf{x}}^{2}-2P_{\\mathbf{x}}^{(1)}+1\\right],\n$$\nand an analogous expression for $P_{\\mathbf{y}}(\\theta)$ with $S_{\\mathbf{y}}$ and $P_{\\mathbf{y}}^{(1)}$. Equality $P_{\\mathbf{y}}(\\theta)\\equiv P_{\\mathbf{x}}(\\theta)$ implies equality of coefficients. Given $S_{\\mathbf{x}}=S_{\\mathbf{y}}$ (same mean), the $\\theta^{3}$ and $\\theta^{2}$ coefficients already match. Matching the $\\theta^{1}$ coefficients yields\n$$\nS_{\\mathbf{x}}\\left(P_{\\mathbf{x}}^{(1)}+1\\right)=S_{\\mathbf{y}}\\left(P_{\\mathbf{y}}^{(1)}+1\\right)\\quad\\Longrightarrow\\quad P_{\\mathbf{x}}^{(1)}=P_{\\mathbf{y}}^{(1)}.\n$$\nWith equal sums and equal products, the unordered pairs of roots of $t^{2}-S t+P$ coincide, hence $\\{x_{1},x_{2}\\}=\\{y_{1},y_{2}\\}$. This contradicts our choice of distinct samples with the same mean. Therefore, for such $\\mathbf{x},\\mathbf{y}$, $R(\\theta;\\mathbf{x},\\mathbf{y})$ depends on $\\theta$, and $\\bar{X}$ cannot be sufficient.\n\nThe correct justification is precisely that the joint likelihood\n$$\nL(\\theta;\\mathbf{x})=\\prod_{i=1}^{n}\\frac{1}{\\pi\\left(1+(x_{i}-\\theta)^{2}\\right)}\n$$\ncannot be expressed with its dependence on $\\theta$ passing only through $\\bar{x}$; equivalently, there exist samples with the same $\\bar{x}$ for which the likelihood ratio depends on $\\theta$. This corresponds to option B. The other options are incorrect for the following reasons: A invokes unbiasedness and completeness, neither of which is necessary for sufficiency, and the Cauchy mean is undefined; C misstates the role of $h(\\mathbf{x})$ (it may depend on any statistic, just not on $\\theta$); D incorrectly claims the factorization theorem requires a finite mean; E describes a true fact about sufficiency of the order statistics for iid samples but does not constitute the reason why $\\bar{X}$ fails to be sufficient here.\n\nTherefore, the correct choice is B.", "answer": "$$\\boxed{B}$$", "id": "1963688"}]}