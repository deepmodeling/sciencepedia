{"hands_on_practices": [{"introduction": "Our first practice is a foundational exercise dealing with the normal distribution, a cornerstone of statistics. We explore a scenario common in scientific measurement where an instrument's average reading is known and stable, but its precision, represented by the variance $\\sigma^2$, is what we need to determine. This problem [@problem_id:1935601] will guide you through using the Neyman-Fisher Factorization Theorem to distill a full dataset into a single, elegant summary statistic for the variance.", "problem": "A research team is testing a new sensor designed to measure a specific physical constant. Based on established theory, the true value of this constant is known to be $\\mu_0$. However, the sensor's measurements are subject to random noise, resulting in variability. To characterize this variability, the team collects $n$ independent measurements, denoted as a random sample $X_1, X_2, \\ldots, X_n$. These measurements are assumed to be drawn from a normal distribution with a known mean $\\mu_0$ and an unknown variance $\\sigma^2 > 0$.\n\nTo efficiently store the experimental results for future analysis of the sensor's precision, the team wants to find the most concise summary of the data that still contains all the information about the unknown variance $\\sigma^2$. This concept is captured by a minimal sufficient statistic.\n\nDetermine a minimal sufficient statistic for the parameter $\\sigma^2$. Express your answer as a function of the sample data $X_1, X_2, \\ldots, X_n$ and the known constant $\\mu_0$.", "solution": "We model the data as $X_{1},\\ldots,X_{n}$ independent and identically distributed with $X_{i} \\sim \\mathcal{N}(\\mu_{0},\\sigma^{2})$, where $\\mu_{0}$ is known and $\\sigma^{2}>0$ is unknown.\n\nThe joint density of $(X_{1},\\ldots,X_{n})$ under $\\sigma^{2}$ is\n$$\nf_{\\sigma^{2}}(x_{1},\\ldots,x_{n})=\\prod_{i=1}^{n}\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}\\exp\\!\\left(-\\frac{(x_{i}-\\mu_{0})^{2}}{2\\sigma^{2}}\\right).\n$$\nThis simplifies to\n$$\nf_{\\sigma^{2}}(x_{1},\\ldots,x_{n})=(2\\pi)^{-n/2}(\\sigma^{2})^{-n/2}\\exp\\!\\left(-\\frac{1}{2\\sigma^{2}}\\sum_{i=1}^{n}(x_{i}-\\mu_{0})^{2}\\right).\n$$\nBy the Neyman–Fisher factorization theorem, a statistic $T(x_{1},\\ldots,x_{n})$ is sufficient for $\\sigma^{2}$ if the likelihood can be written as $g(T(x_{1},\\ldots,x_{n}),\\sigma^{2})h(x_{1},\\ldots,x_{n})$ with $h$ free of $\\sigma^{2}$. Taking\n$$\nT(x_{1},\\ldots,x_{n})=\\sum_{i=1}^{n}(x_{i}-\\mu_{0})^{2},\n$$\nwe can write\n$$\nf_{\\sigma^{2}}(x_{1},\\ldots,x_{n})=\\underbrace{(\\sigma^{2})^{-n/2}\\exp\\!\\left(-\\frac{T(x_{1},\\ldots,x_{n})}{2\\sigma^{2}}\\right)}_{g(T,\\sigma^{2})}\\;\\underbrace{(2\\pi)^{-n/2}}_{h(x_{1},\\ldots,x_{n})},\n$$\nso $T$ is sufficient for $\\sigma^{2}$.\n\nTo establish minimal sufficiency, consider two samples $x=(x_{1},\\ldots,x_{n})$ and $y=(y_{1},\\ldots,y_{n})$. The likelihood ratio is\n$$\n\\frac{f_{\\sigma^{2}}(x)}{f_{\\sigma^{2}}(y)}=\\exp\\!\\left(-\\frac{1}{2\\sigma^{2}}\\left[\\sum_{i=1}^{n}(x_{i}-\\mu_{0})^{2}-\\sum_{i=1}^{n}(y_{i}-\\mu_{0})^{2}\\right]\\right).\n$$\nThis ratio is independent of $\\sigma^{2}$ if and only if $\\sum_{i=1}^{n}(x_{i}-\\mu_{0})^{2}=\\sum_{i=1}^{n}(y_{i}-\\mu_{0})^{2}$. By the Lehmann–Scheffé characterization of minimal sufficiency, the statistic generating these equivalence classes is minimal sufficient. Therefore, a minimal sufficient statistic for $\\sigma^{2}$ is\n$$\nT(X_{1},\\ldots,X_{n})=\\sum_{i=1}^{n}(X_{i}-\\mu_{0})^{2}.\n$$\nAny one-to-one function of this $T$ (such as $\\frac{1}{n}\\sum_{i=1}^{n}(X_{i}-\\mu_{0})^{2}$) is also minimal sufficient, but the canonical choice is the sum of squared deviations from the known mean.", "answer": "$$\\boxed{\\sum_{i=1}^{n}\\left(X_{i}-\\mu_{0}\\right)^{2}}$$", "id": "1935601"}, {"introduction": "Moving beyond distributions with fixed support, this next problem explores what happens when the parameter itself defines the boundaries of possible data values. Using a uniform distribution whose range depends on a parameter $\\theta$, we will see that the most informative summary is not a sample average or sum. Instead, as this exercise [@problem_id:1935627] demonstrates, the extreme values of the sample—the minimum and maximum—become critically important for inference.", "problem": "A quality control engineer is analyzing a manufacturing process that produces high-precision metal rods. The length of each rod is modeled as a random variable from a Uniform distribution on the interval $[\\theta, 2\\theta]$, where the parameter $\\theta > 0$ is an unknown characteristic of the machine's current calibration. To monitor and estimate $\\theta$, the engineer collects a random sample of $n$ rod lengths, denoted by $X_1, X_2, \\dots, X_n$.\n\nTo summarize the data efficiently without losing information about the parameter $\\theta$, the engineer needs to compute a minimal sufficient statistic. Let $X_{(1)}$ be the minimum value in the sample (the first order statistic) and $X_{(n)}$ be the maximum value in the sample (the last order statistic).\n\nWhich of the following represents a minimal sufficient statistic for the parameter $\\theta$?\n\nA. $X_{(n)}$\n\nB. $\\frac{X_{(1)}}{X_{(n)}}$\n\nC. $(X_{(1)}, X_{(n)})$\n\nD. $X_{(1)} + X_{(n)}$\n\nE. $X_{(n)} - X_{(1)}$\n\nF. $\\frac{1}{n} \\sum_{i=1}^{n} X_i$", "solution": "Let $X_{1},\\dots,X_{n}$ be i.i.d. with density\n$$\nf(x\\mid \\theta)=\\frac{1}{\\theta}\\,\\mathbf{1}\\{\\theta \\leq x \\leq 2\\theta\\},\\quad \\theta>0.\n$$\nFor a sample $x=(x_{1},\\dots,x_{n})$, the joint density is\n$$\nf(x\\mid \\theta)=\\prod_{i=1}^{n}\\frac{1}{\\theta}\\,\\mathbf{1}\\{\\theta \\leq x_{i} \\leq 2\\theta\\}\n=\\theta^{-n}\\,\\mathbf{1}\\{\\theta \\leq x_{(1)}\\}\\,\\mathbf{1}\\{2\\theta \\geq x_{(n)}\\}.\n$$\nEquivalently,\n$$\nf(x\\mid \\theta)=\\theta^{-n}\\,\\mathbf{1}\\left\\{\\frac{x_{(n)}}{2} \\leq \\theta \\leq x_{(1)}\\right\\}.\n$$\nThis shows the likelihood depends on the data only through $(x_{(1)},x_{(n)})$. Define $T(x)=(x_{(1)},x_{(n)})$. Then we can write\n$$\nf(x\\mid \\theta)=g(T(x),\\theta)\\,h(x),\n$$\nwith $g((a,b),\\theta)=\\theta^{-n}\\,\\mathbf{1}\\{b/2 \\leq \\theta \\leq a\\}$ and $h(x)=1$. By the Neyman–Fisher factorization theorem, $T=(X_{(1)},X_{(n)})$ is sufficient for $\\theta$.\n\nTo establish minimal sufficiency, use the Lehmann–Scheffé criterion: $T$ is minimal sufficient if for any two samples $x$ and $y$,\n$$\n\\frac{f(x\\mid \\theta)}{f(y\\mid \\theta)} \\text{ is free of } \\theta \\quad \\Longleftrightarrow \\quad T(x)=T(y).\n$$\nCompute\n$$\n\\frac{f(x\\mid \\theta)}{f(y\\mid \\theta)}=\\frac{\\theta^{-n}\\,\\mathbf{1}\\{x_{(n)}/2 \\leq \\theta \\leq x_{(1)}\\}}{\\theta^{-n}\\,\\mathbf{1}\\{y_{(n)}/2 \\leq \\theta \\leq y_{(1)}\\}}=\\frac{\\mathbf{1}\\{x_{(n)}/2 \\leq \\theta \\leq x_{(1)}\\}}{\\mathbf{1}\\{y_{(n)}/2 \\leq \\theta \\leq y_{(1)}\\}}.\n$$\nThis ratio is constant in $\\theta$ if and only if the indicator functions coincide for all $\\theta$ (up to sets of measure zero), which occurs if and only if the intervals $[x_{(n)}/2,x_{(1)}]$ and $[y_{(n)}/2,y_{(1)}]$ are identical. That is equivalent to $x_{(1)}=y_{(1)}$ and $x_{(n)}=y_{(n)}$, i.e., $T(x)=T(y)$. Hence $T=(X_{(1)},X_{(n)})$ is minimal sufficient.\n\nAmong the given options, only option C includes both $X_{(1)}$ and $X_{(n)}$, so the minimal sufficient statistic is $(X_{(1)},X_{(n)})$.", "answer": "$$\\boxed{C}$$", "id": "1935627"}, {"introduction": "This final practice demonstrates the power of minimal sufficient statistics in a real-world scientific context: population genetics. By modeling genotype counts using the Hardy-Weinberg equilibrium principle, we can estimate an unobserved allele frequency, $\\theta$. This problem [@problem_id:1935612] shows how to derive a minimal sufficient statistic from count data, revealing a single, interpretable quantity that captures all the information about $\\theta$ contained in the sample.", "problem": "In population genetics, a fundamental concept is the Hardy-Weinberg equilibrium, which describes the genetic makeup of a population that is not evolving. For a gene with two alleles, A and a, let the frequency of allele A in the population be $\\theta$ and the frequency of allele a be $1-\\theta$, where the parameter $\\theta$ lies in the interval $(0, 1)$. Under the Hardy-Weinberg principle, the expected frequencies of the three possible genotypes—AA, Aa, and aa—are $\\theta^2$, $2\\theta(1-\\theta)$, and $(1-\\theta)^2$, respectively.\n\nA geneticist collects a random sample of $n$ individuals from this large population to estimate the allele frequency $\\theta$. Let $N_1$ be the number of individuals in the sample with genotype AA, $N_2$ be the number with genotype Aa, and $N_3$ be the number with genotype aa. The total sample size is $n = N_1 + N_2 + N_3$. The vector of observed counts $(N_1, N_2, N_3)$ can be modeled as a single draw from a multinomial distribution.\n\nBased on the observed counts $(N_1, N_2, N_3)$, determine a minimal sufficient statistic for the parameter $\\theta$. Your answer should be an expression in terms of $N_1$, $N_2$, and $N_3$.", "solution": "Let the genotype probabilities under Hardy–Weinberg equilibrium be $p_{1}=\\theta^{2}$ for AA, $p_{2}=2\\theta(1-\\theta)$ for Aa, and $p_{3}=(1-\\theta)^{2}$ for aa. For observed counts $(N_{1},N_{2},N_{3})$ with fixed total $n=N_{1}+N_{2}+N_{3}$, the joint pmf under a multinomial model is\n$$\nf_{\\theta}(N_{1},N_{2},N_{3})=\\frac{n!}{N_{1}!N_{2}!N_{3}!}\\left(\\theta^{2}\\right)^{N_{1}}\\left(2\\theta(1-\\theta)\\right)^{N_{2}}\\left((1-\\theta)^{2}\\right)^{N_{3}}.\n$$\nThis simplifies to\n$$\nf_{\\theta}(N_{1},N_{2},N_{3})=\\frac{n!}{N_{1}!N_{2}!N_{3}!}\\,2^{N_{2}}\\,\\theta^{2N_{1}+N_{2}}(1-\\theta)^{2N_{3}+N_{2}}.\n$$\nBy the Neyman–Fisher factorization theorem, with $n$ fixed and known, we can write\n$$\nf_{\\theta}(N_{1},N_{2},N_{3})=\\underbrace{\\theta^{T}(1-\\theta)^{2n-T}}_{g_{\\theta}(T)}\\;\\underbrace{\\frac{n!}{N_{1}!N_{2}!N_{3}!}\\,2^{N_{2}}}_{h(N_{1},N_{2},N_{3})},\n$$\nwhere $T=2N_{1}+N_{2}$. Hence $T=2N_{1}+N_{2}$ is a sufficient statistic for $\\theta$.\n\nTo show minimal sufficiency, use the Lehmann–Scheffé–Bahadur criterion: for two outcomes $(n_{1},n_{2},n_{3})$ and $(m_{1},m_{2},m_{3})$ with the same $n$, consider\n$$\n\\frac{f_{\\theta}(n_{1},n_{2},n_{3})}{f_{\\theta}(m_{1},m_{2},m_{3})}\n=\\frac{\\frac{n!}{n_{1}!n_{2}!n_{3}!}2^{n_{2}}\\theta^{2n_{1}+n_{2}}(1-\\theta)^{2n_{3}+n_{2}}}{\\frac{n!}{m_{1}!m_{2}!m_{3}!}2^{m_{2}}\\theta^{2m_{1}+m_{2}}(1-\\theta)^{2m_{3}+m_{2}}}\n=C(n,m)\\left(\\frac{\\theta}{1-\\theta}\\right)^{(2n_{1}+n_{2})-(2m_{1}+m_{2})},\n$$\nwhere $C(n,m)$ is free of $\\theta$. This ratio is independent of $\\theta$ if and only if $2n_{1}+n_{2}=2m_{1}+m_{2}$. Therefore, the equivalence classes induced by the family are exactly those with the same value of $T=2N_{1}+N_{2}$, proving $T$ is minimal sufficient.\n\nThus, a minimal sufficient statistic for $\\theta$ is the total number of $A$ alleles in the sample, namely $2N_{1}+N_{2}$.", "answer": "$$\\boxed{2N_{1}+N_{2}}$$", "id": "1935612"}]}