{"hands_on_practices": [{"introduction": "This exercise serves as a fundamental building block for understanding Principal Component Analysis. We will begin with a simple, idealized two-variable system, typical of redundant sensors, to analytically derive the principal components. By working through this problem ([@problem_id:1946278]), you will see exactly how PCA transforms correlated data and how the amount of variance captured by the first component is directly determined by the correlation coefficient $\\rho$.", "problem": "An autonomous environmental monitoring drone uses a pair of identical sensors to measure atmospheric pressure. Let the readings of the two sensors, after being centered by subtracting their long-term average, be represented by the random variables $X_1$ and $X_2$.\n\nThe joint behavior of these readings is described by a bivariate random vector $(X_1, X_2)$ with a covariance matrix $\\Sigma$. Because the sensors are of the same type and subject to similar environmental fluctuations, they have the same variance, $\\text{Var}(X_1) = \\text{Var}(X_2) = \\sigma^2$, for some constant $\\sigma > 0$. Their readings are also correlated, with a correlation coefficient $\\rho$ such that $0 < \\rho < 1$. The covariance matrix is therefore given by:\n$$\n\\Sigma = \\begin{pmatrix} \\sigma^2 & \\rho\\sigma^2 \\\\ \\rho\\sigma^2 & \\sigma^2 \\end{pmatrix}\n$$\nTo reduce data redundancy and identify the primary axis of variation, the engineering team applies Principal Component Analysis (PCA). PCA transforms the original correlated variables $(X_1, X_2)$ into a new set of uncorrelated variables, known as principal components. The first principal component is defined as the linear combination of $X_1$ and $X_2$ that captures the maximum possible variance.\n\nDetermine the proportion of the total variance in the data that is explained by the first principal component. Express your answer as a symbolic expression in terms of $\\rho$.", "solution": "We are given a centered bivariate random vector with covariance matrix\n$$\n\\Sigma=\\begin{pmatrix}\\sigma^{2} & \\rho\\sigma^{2} \\\\ \\rho\\sigma^{2} & \\sigma^{2}\\end{pmatrix},\n$$\nwhere $0<\\rho<1$ and $\\sigma>0$. In PCA, the variances of the principal components are the eigenvalues of the covariance matrix. The proportion of total variance explained by the first principal component equals its eigenvalue divided by the total variance, which is the trace of $\\Sigma$.\n\nFirst, compute the eigenvalues of $\\Sigma$ by solving the characteristic equation\n$$\n\\det(\\Sigma-\\lambda I)=0.\n$$\nWe have\n$$\n\\det\\begin{pmatrix}\\sigma^{2}-\\lambda & \\rho\\sigma^{2} \\\\ \\rho\\sigma^{2} & \\sigma^{2}-\\lambda\\end{pmatrix}\n=(\\sigma^{2}-\\lambda)^{2}-(\\rho\\sigma^{2})^{2}=0.\n$$\nThus,\n$$\n(\\sigma^{2}-\\lambda)^{2}=\\rho^{2}\\sigma^{4}\n\\quad\\Longrightarrow\\quad\n\\sigma^{2}-\\lambda=\\pm\\rho\\sigma^{2}\n\\quad\\Longrightarrow\\quad\n\\lambda=\\sigma^{2}(1\\pm\\rho).\n$$\nSince $0<\\rho<1$, the largest eigenvalue is\n$$\n\\lambda_{1}=\\sigma^{2}(1+\\rho).\n$$\nThe total variance equals the trace of $\\Sigma$,\n$$\n\\operatorname{tr}(\\Sigma)=\\sigma^{2}+\\sigma^{2}=2\\sigma^{2},\n$$\nwhich also equals the sum of the eigenvalues $\\sigma^{2}(1+\\rho)+\\sigma^{2}(1-\\rho)=2\\sigma^{2}$. Therefore, the proportion of total variance explained by the first principal component is\n$$\n\\frac{\\lambda_{1}}{\\operatorname{tr}(\\Sigma)}=\\frac{\\sigma^{2}(1+\\rho)}{2\\sigma^{2}}=\\frac{1+\\rho}{2}.\n$$", "answer": "$$\\boxed{\\frac{1+\\rho}{2}}$$", "id": "1946278"}, {"introduction": "Having explored how PCA capitalizes on correlation, we now investigate the opposite scenario: what happens when the variables are already uncorrelated? This conceptual problem ([@problem_id:2421744]) is a crucial thought experiment that clarifies the fundamental purpose of PCA. It demonstrates that when the covariance matrix is diagonal, PCA simply identifies the original variables as the principal components, highlighting that the technique's power lies in rotating the data to find new, meaningful axes of variance.", "problem": "In a risk model for a portfolio of $n$ assets, let $\\mathbf{r}\\in\\mathbb{R}^n$ denote the vector of centered asset returns and let the population covariance matrix be $\\Sigma\\in\\mathbb{R}^{n\\times n}$. Suppose all off-diagonal entries of $\\Sigma$ are $0$, so assets are pairwise uncorrelated, and the diagonal entries are finite and nonnegative. Consider conducting Principal Component Analysis (PCA) on $\\Sigma$ to extract orthonormal factors that successively maximize the variance of their corresponding linear combinations of $\\mathbf{r}$, subject to orthogonality constraints.\n\nFrom first principles, using only the definitions of covariance, eigenvalue decomposition for real symmetric matrices, and the variance-maximization characterization of principal components, determine which of the following statements are true in this setting.\n\nSelect all that apply.\n\nA. The principal component loading vectors coincide (up to signs and ordering) with the canonical basis vectors; equivalently, each principal component equals one of the original centered variables, and the associated eigenvalue equals that variable’s variance.\n\nB. Even when the diagonal variances are all distinct, any orthonormal rotation of the variables yields an equally valid set of principal components.\n\nC. The principal components are undefined because a covariance matrix with zero off-diagonal entries is necessarily singular.\n\nD. If two diagonal variances are exactly equal, any orthonormal pair spanning the corresponding $2$-variable subspace is an admissible choice of principal directions within that subspace.\n\nE. Performing Principal Component Analysis on the correlation matrix instead of the covariance matrix necessarily yields the same principal directions in this setting.", "solution": "The problem statement is subjected to validation before proceeding.\n\n### Step 1: Extract Givens\n- A portfolio consists of $n$ assets.\n- $\\mathbf{r}\\in\\mathbb{R}^n$ is the vector of centered asset returns, meaning $E[\\mathbf{r}] = \\mathbf{0}$.\n- $\\Sigma\\in\\mathbb{R}^{n\\times n}$ is the population covariance matrix of $\\mathbf{r}$, so $\\Sigma = E[\\mathbf{r}\\mathbf{r}^T]$.\n- All off-diagonal entries of $\\Sigma$ are $0$. This means $\\Sigma_{ij} = 0$ for $i \\neq j$.\n- The diagonal entries of $\\Sigma$, which are the variances $\\sigma_i^2 = \\text{Var}(r_i)$, are finite and nonnegative.\n- Principal Component Analysis (PCA) is to be performed on $\\Sigma$.\n- The definition of PCA to be used is the variance-maximization principle: find orthonormal factors (loading vectors) that successively maximize the variance of the projected data.\n- The derivation must rely only on first principles of covariance, eigenvalue decomposition of real symmetric matrices, and the variance-maximization formulation of PCA.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded. The setup describes a standard, albeit simplified, scenario in portfolio theory and multivariate statistics. A diagonal covariance matrix simply represents a system of uncorrelated random variables, a common theoretical starting point. The problem is well-posed, providing all necessary information ($\\Sigma$ is fully specified in its structure) to determine the properties of its principal components. The language is objective and precise. The problem does not violate any of the criteria for invalidity. It is a formalizable question directly relevant to the topic of PCA.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be derived.\n\n### Derivation from First Principles\n\nLet a principal component be a linear combination of the centered returns, $p = \\mathbf{w}^T \\mathbf{r}$, where $\\mathbf{w} \\in \\mathbb{R}^n$ is a vector of weights, or a \"loading vector\". The variance of this principal component is given by:\n$$ \\text{Var}(p) = \\text{Var}(\\mathbf{w}^T \\mathbf{r}) = \\mathbf{w}^T \\text{Var}(\\mathbf{r}) \\mathbf{w} = \\mathbf{w}^T \\Sigma \\mathbf{w} $$\nThe first principal component is found by maximizing this variance subject to the constraint that the loading vector has unit length, i.e., $\\mathbf{w}^T \\mathbf{w} = 1$. This is a constrained optimization problem:\n$$ \\max_{\\mathbf{w}} \\mathbf{w}^T \\Sigma \\mathbf{w} \\quad \\text{subject to} \\quad \\mathbf{w}^T \\mathbf{w} = 1 $$\nThis is the problem of maximizing the Rayleigh quotient. The solution, from the spectral theorem for real symmetric matrices, is that the optimal vector $\\mathbf{w}_1$ is the eigenvector of $\\Sigma$ corresponding to the largest eigenvalue, $\\lambda_1$. The maximum variance is $\\text{Var}(p_1) = \\lambda_1$.\n\nSubsequent principal components are found by maximizing the same variance expression, with the additional constraints that their loading vectors are orthonormal to all previous ones. The $k$-th loading vector, $\\mathbf{w}_k$, is the unit-norm eigenvector of $\\Sigma$ corresponding to the $k$-th largest eigenvalue, $\\lambda_k$, and it is orthogonal to the previous loading vectors $\\mathbf{w}_1, \\dots, \\mathbf{w}_{k-1}$. The set of loading vectors $\\{\\mathbf{w}_1, \\dots, \\mathbf{w}_n\\}$ forms an orthonormal basis of eigenvectors of $\\Sigma$, ordered according to their corresponding eigenvalues $\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_n \\ge 0$.\n\nIn this specific problem, the covariance matrix $\\Sigma$ is diagonal. Let its diagonal entries be $\\sigma_1^2, \\sigma_2^2, \\dots, \\sigma_n^2$.\n$$ \\Sigma = \\begin{pmatrix} \\sigma_1^2 & 0 & \\cdots & 0 \\\\ 0 & \\sigma_2^2 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & \\sigma_n^2 \\end{pmatrix} $$\nThe eigenvalues and eigenvectors of a diagonal matrix are known from elementary linear algebra. The eigenvalues are the diagonal entries themselves: $\\{\\sigma_1^2, \\sigma_2^2, \\dots, \\sigma_n^2\\}$. The corresponding eigenvectors are the canonical basis vectors, $\\mathbf{e}_1, \\mathbf{e}_2, \\dots, \\mathbf{e}_n$, where $\\mathbf{e}_i$ is a column vector with a $1$ in the $i$-th position and $0$s elsewhere. This is verified by observing that $\\Sigma \\mathbf{e}_i = \\sigma_i^2 \\mathbf{e}_i$.\n\nThe principal component loading vectors are therefore the canonical basis vectors $\\{\\mathbf{e}_i\\}$, ordered according to the magnitude of the variances $\\{\\sigma_i^2\\}$. The principal components themselves are $p_k = \\mathbf{w}_k^T \\mathbf{r}$. If, for instance, $\\sigma_i^2$ is the $k$-th largest variance, then $\\mathbf{w}_k = \\mathbf{e}_i$ (or $-\\mathbf{e}_i$) and the $k$-th principal component is $p_k = \\mathbf{e}_i^T \\mathbf{r} = r_i$.\n\n### Evaluation of Options\n\n**A. The principal component loading vectors coincide (up to signs and ordering) with the canonical basis vectors; equivalently, each principal component equals one of the original centered variables, and the associated eigenvalue equals that variable’s variance.**\nAs derived above, the eigenvectors of the diagonal matrix $\\Sigma$ are the canonical basis vectors $\\mathbf{e}_1, \\dots, \\mathbf{e}_n$. These are the principal component loading vectors. The ordering depends on the relative magnitudes of the diagonal variances $\\sigma_i^2$. The sign is arbitrary as if $\\mathbf{v}$ is an eigenvector, so is $-\\mathbf{v}$. If the loading vector is $\\mathbf{w}_k = \\mathbf{e}_i$, the corresponding principal component is $p_k = \\mathbf{e}_i^T \\mathbf{r} = r_i$, which is one of the original centered variables. The eigenvalue associated with this component is the maximized variance, which is $\\lambda_k = \\mathbf{e}_i^T \\Sigma \\mathbf{e}_i = \\sigma_i^2$, precisely the variance of the variable $r_i$. This statement is entirely consistent with the first principles.\nVerdict: **Correct**.\n\n**B. Even when the diagonal variances are all distinct, any orthonormal rotation of the variables yields an equally valid set of principal components.**\nIf the diagonal variances $\\sigma_i^2$ are all distinct, then the eigenvalues of $\\Sigma$ are distinct. For a real symmetric matrix, this implies that the corresponding eigenspaces are all one-dimensional, and the eigenvectors are unique up to sign. The principal component loading vectors are therefore uniquely determined to be the set of canonical basis vectors $\\{\\mathbf{e}_1, \\dots, \\mathbf{e}_n\\}$, up to sign and ordering. Any non-trivial orthonormal rotation of these vectors (e.g., a Givens rotation mixing $\\mathbf{e}_i$ and $\\mathbf{e}_j$) would produce vectors that are *not* eigenvectors of $\\Sigma$. Thus, they would not maximize the variance and would not be principal components. This statement is fundamentally incorrect.\nVerdict: **Incorrect**.\n\n**C. The principal components are undefined because a covariance matrix with zero off-diagonal entries is necessarily singular.**\nThis statement contains two falsehoods. First, a diagonal matrix is not *necessarily* singular. It is singular if and only if at least one of its diagonal entries is zero. Since the problem states the variances $\\sigma_i^2$ are nonnegative, it is possible for all of them to be strictly positive, in which case $\\det(\\Sigma) = \\prod \\sigma_i^2 > 0$ and $\\Sigma$ is non-singular. Second, even if $\\Sigma$ were singular (i.e., some $\\sigma_i^2 = 0$), PCA is still perfectly well-defined. A singular symmetric matrix still has a complete orthonormal basis of eigenvectors. The only consequence of singularity is that one or more eigenvalues are zero, meaning the corresponding principal components have zero variance. They are not \"undefined\".\nVerdict: **Incorrect**.\n\n**D. If two diagonal variances are exactly equal, any orthonormal pair spanning the corresponding $2$-variable subspace is an admissible choice of principal directions within that subspace.**\nSuppose $\\sigma_i^2 = \\sigma_j^2 = \\lambda$ for $i \\neq j$. This corresponds to a repeated eigenvalue $\\lambda$. The eigenspace associated with this eigenvalue is the set of all vectors $\\mathbf{v}$ such that $\\Sigma \\mathbf{v} = \\lambda \\mathbf{v}$. Any vector of the form $\\mathbf{v} = c_i \\mathbf{e}_i + c_j \\mathbf{e}_j$ satisfies this equation: $\\Sigma(c_i \\mathbf{e}_i + c_j \\mathbf{e}_j) = c_i(\\Sigma \\mathbf{e}_i) + c_j(\\Sigma \\mathbf{e}_j) = c_i(\\sigma_i^2 \\mathbf{e}_i) + c_j(\\sigma_j^2 \\mathbf{e}_j) = \\lambda(c_i \\mathbf{e}_i + c_j \\mathbf{e}_j) = \\lambda \\mathbf{v}$. Thus, the eigenspace is the $2$-dimensional subspace spanned by $\\mathbf{e}_i$ and $\\mathbf{e}_j$. When performing PCA, we must choose an orthonormal basis for this degenerate eigenspace to serve as the principal directions. Any such choice is valid. For example, $\\{\\mathbf{e}_i, \\mathbf{e}_j\\}$ is one choice. A rotated pair, such as $\\{\\frac{1}{\\sqrt{2}}(\\mathbf{e}_i + \\mathbf{e}_j), \\frac{1}{\\sqrt{2}}(\\mathbf{e}_i - \\mathbf{e}_j)\\}$, is another equally valid choice. The statement is a correct description of handling degenerate eigenspaces in eigendecomposition and PCA.\nVerdict: **Correct**.\n\n**E. Performing Principal Component Analysis on the correlation matrix instead of the covariance matrix necessarily yields the same principal directions in this setting.**\nThe correlation matrix $P$ has entries $P_{ij} = \\frac{\\Sigma_{ij}}{\\sqrt{\\Sigma_{ii}\\Sigma_{jj}}}$. Given $\\Sigma$ is diagonal with entries $\\Sigma_{ii} = \\sigma_i^2$, the off-diagonal entries of $P$ are $P_{ij} = 0$. Assuming all variances are strictly positive ($\\sigma_i^2 > 0$), the diagonal entries are $P_{ii} = \\frac{\\sigma_i^2}{\\sigma_i^2} = 1$. Thus, the correlation matrix is the identity matrix, $P = I$. The eigenvalues of the $n \\times n$ identity matrix are all equal to $1$. The corresponding eigenspace is the entire space $\\mathbb{R}^n$. This means *any* orthonormal basis for $\\mathbb{R}^n$ is a valid set of principal directions for the correlation matrix.\nNow, compare this to the principal directions from the covariance matrix $\\Sigma$. Unless all variances $\\sigma_i^2$ are equal, the principal directions for $\\Sigma$ are not arbitrary; they are fixed as the canonical basis vectors (when variances are distinct). Since the set of valid directions for $P=I$ is infinitely large, while the set for $\\Sigma$ (with distinct variances) is unique, the directions are not \"necessarily the same\".\nVerdict: **Incorrect**.", "answer": "$$\\boxed{AD}$$", "id": "2421744"}, {"introduction": "Theoretical models often assume clean, unitless data, but real-world applications are rarely so simple. This computational practice ([@problem_id:2421735]) addresses one of the most critical and practical aspects of applying PCA: the necessity of data standardization. By simulating data with different scales, you will quantitatively see how failing to standardize can fatally corrupt the analysis, causing the variable with the largest arbitrary scale to dominate the first principal component.", "problem": "You are asked to demonstrate, using first principles of Principal Component Analysis (PCA), how failing to standardize variables measured in different units can distort the estimated principal directions and the explained variance. Work in a purely mathematical framework with a synthetic data-generating process that models typical financial variables such as prices and volumes. You will implement the full pipeline and report quantitative diagnostics that compare PCA on raw data versus PCA on standardized data.\n\nFundamentals:\n- PCA seeks orthonormal directions that maximize sample variance. Given a centered data matrix $X \\in \\mathbb{R}^{T \\times n}$, the sample covariance matrix is $\\Sigma = \\frac{1}{T-1} X^\\top X$. The principal components are the eigenvectors of $\\Sigma$ corresponding to its eigenvalues, ordered from largest to smallest.\n- Standardization transforms each variable $x_j$ to $\\tilde{x}_j = \\frac{x_j - \\bar{x}_j}{\\hat{\\sigma}_j}$, where $\\bar{x}_j$ is the sample mean and $\\hat{\\sigma}_j$ is the sample standard deviation, so that each standardized variable has unit sample variance. PCA on standardized data is PCA on the sample correlation matrix.\n- Diagonal rescaling $D = \\operatorname{diag}(s_1,\\dots,s_n)$ applied to variables, $X \\mapsto X D$, multiplies the covariance entries by $s_i s_j$, thereby altering eigenvectors unless all $s_j$ are equal.\n\nData-generating process:\n- For each test case $k$, fix $T_k \\in \\mathbb{N}$, number of variables $n_k \\in \\mathbb{N}$, factor loadings $b^{(k)} \\in \\mathbb{R}^{n_k}$, idiosyncratic standard deviations $u^{(k)} \\in \\mathbb{R}^{n_k}$, and unit scales $s^{(k)} \\in \\mathbb{R}^{n_k}$.\n- Generate a single common factor $f_t \\sim \\mathcal{N}(0,1)$ for $t = 1,\\dots,T_k$, and idiosyncratic noises $e_{t,j} \\sim \\mathcal{N}(0,(u^{(k)}_j)^2)$, all mutually independent across $t$ and $j$.\n- Construct raw observations $x_{t,j} = s^{(k)}_j \\left( b^{(k)}_j f_t + e_{t,j} \\right)$ for $t=1,\\dots,T_k$ and $j=1,\\dots,n_k$.\n- Center each column of $X$ by subtracting its sample mean before computing any covariance.\n\nComputation tasks per test case:\n- Compute the sample covariance matrix $\\Sigma_{\\text{raw}}$ from the centered raw data $X$ and obtain the first principal component eigenvector $v_{\\text{raw}}$ (unit norm) and its eigenvalue $\\lambda_{\\text{raw}}$.\n- Standardize each column of $X$ to unit sample variance to obtain $Z$, compute $\\Sigma_{\\text{std}} = \\frac{1}{T_k-1} Z^\\top Z$ (the sample correlation matrix), and obtain the first principal component eigenvector $v_{\\text{std}}$ (unit norm) and its eigenvalue $\\lambda_{\\text{std}}$.\n- Compute the angle $\\theta = \\arccos\\!\\left( \\left| v_{\\text{raw}}^\\top v_{\\text{std}} \\right| \\right)$; report $\\theta$ in radians.\n- Compute the difference in explained variance shares as $\\Delta = \\left| \\frac{\\lambda_{\\text{raw}}}{\\operatorname{tr}(\\Sigma_{\\text{raw}})} - \\frac{\\lambda_{\\text{std}}}{\\operatorname{tr}(\\Sigma_{\\text{std}})} \\right|$, which must be reported as a decimal fraction (not a percentage).\n\nRandomness and reproducibility:\n- Use a fixed pseudorandom number generator seed equal to $314159$ for the entire experiment to ensure reproducible results.\n\nTest suite:\n- There are $3$ test cases. For each test case $k$, use the following parameters $(T_k, n_k, b^{(k)}, u^{(k)}, s^{(k)})$:\n  - Case $1$ (similar units, two variables):\n    - $T_1 = 500$, $n_1 = 2$,\n    - $b^{(1)} = [1.0, 0.9]$,\n    - $u^{(1)} = [0.1, 0.1]$,\n    - $s^{(1)} = [1.0, 1.2]$.\n  - Case $2$ (mismatched units, two variables: one dominates by scale):\n    - $T_2 = 500$, $n_2 = 2$,\n    - $b^{(2)} = [1.0, 0.9]$,\n    - $u^{(2)} = [0.1, 0.1]$,\n    - $s^{(2)} = [1000.0, 1.0]$.\n  - Case $3$ (mismatched units, three variables: one huge-scale, one moderate, one tiny-scale):\n    - $T_3 = 800$, $n_3 = 3$,\n    - $b^{(3)} = [0.2, 1.0, 1.0]$,\n    - $u^{(3)} = [0.3, 0.2, 0.2]$,\n    - $s^{(3)} = [1000.0, 1.0, 0.01]$.\n\nRequired outputs per test case:\n- A list of two floats $[\\theta, \\Delta]$ where $\\theta$ is the angle in radians and $\\Delta$ is the absolute difference in explained variance shares. Both values must be rounded to exactly $6$ decimal places.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of the per-case lists, enclosed in square brackets, for example, $[[\\theta_1,\\Delta_1],[\\theta_2,\\Delta_2],[\\theta_3,\\Delta_3]]$, with each float rounded to exactly $6$ decimal places and angles in radians.", "solution": "The problem presented is a valid and well-posed exercise in computational statistics, specifically demonstrating the sensitivity of Principal Component Analysis (PCA) to the scaling of variables. It is scientifically sound, resting on foundational principles of linear algebra and statistics, and all parameters and procedures are specified with sufficient clarity to permit a unique, verifiable solution. We will proceed with the analysis.\n\nThe central thesis is that PCA, as a variance-maximization technique, is not scale-invariant. When variables are measured in disparate units (e.g., a stock price in dollars versus its trading volume in millions of shares), the variable with the largest variance will mechanically dominate the first principal component. This is often an artifact of the units chosen rather than an indicator of true underlying importance. Standardization is the standard remedy, transforming all variables to a common scale (unit variance) so that the analysis focuses on the correlation structure of the data, not the arbitrary measurement scales.\n\nWe begin by formalizing the data generation and analysis pipeline.\n\n**1. Data-Generating Process**\n\nFor each test case $k$, we are given a sample size $T_k$, number of variables $n_k$, factor loadings $b^{(k)} \\in \\mathbb{R}^{n_k}$, idiosyncratic standard deviations $u^{(k)} \\in \\mathbb{R}^{n_k}$, and unit scales $s^{(k)} \\in \\mathbb{R}^{n_k}$.\n\nThe data are generated from a single-factor model. A common latent factor $f_t$ is drawn from a standard normal distribution, $f_t \\sim \\mathcal{N}(0, 1)$, for each time point $t=1, \\dots, T_k$. For each variable $j=1, \\dots, n_k$, an idiosyncratic noise term $e_{t,j}$ is drawn from $\\mathcal{N}(0, (u^{(k)}_j)^2)$. All $f_t$ and $e_{t,j}$ are mutually independent.\n\nThe observed value for variable $j$ at time $t$ is constructed as:\n$$\nx_{t,j} = s^{(k)}_j \\left( b^{(k)}_j f_t + e_{t,j} \\right)\n$$\nThis forms a data matrix $X \\in \\mathbb{R}^{T_k \\times n_k}$ whose columns represent the different variables. The scale factor $s^{(k)}_j$ represents the arbitrary unit of measurement for variable $j$.\n\n**2. PCA on Raw Data (Covariance-Based PCA)**\n\nThe first step in PCA is to center the data by subtracting the column-wise sample mean. Let $\\bar{x}_j = \\frac{1}{T_k} \\sum_{t=1}^{T_k} x_{t,j}$ be the sample mean of the $j$-th variable. The centered data matrix, denoted $X_c$, has entries $(X_c)_{t,j} = x_{t,j} - \\bar{x}_j$.\n\nThe sample covariance matrix $\\Sigma_{\\text{raw}}$ is then computed:\n$$\n\\Sigma_{\\text{raw}} = \\frac{1}{T_k-1} X_c^\\top X_c\n$$\nThe principal components are the eigenvectors of $\\Sigma_{\\text{raw}}$. We perform an eigendecomposition of this matrix:\n$$\n\\Sigma_{\\text{raw}} V = V \\Lambda\n$$\nwhere $V$ is the matrix of orthonormal eigenvectors and $\\Lambda$ is the diagonal matrix of corresponding eigenvalues. The eigenvalues are sorted in descending order, $\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_{n_k}$. The first principal component is the eigenvector $v_1$ associated with the largest eigenvalue $\\lambda_1$. For this problem, we denote this eigenvector as $v_{\\text{raw}}$ and the eigenvalue as $\\lambda_{\\text{raw}}$.\n\n**3. PCA on Standardized Data (Correlation-Based PCA)**\n\nTo remove the effect of arbitrary scaling, we standardize the data. For each column $j$ of the original data matrix $X$, we compute its sample standard deviation, $\\hat{\\sigma}_j = \\sqrt{\\frac{1}{T_k-1} \\sum_{t=1}^{T_k} (x_{t,j} - \\bar{x}_j)^2}$.\n\nThe standardized data matrix $Z$ is constructed with entries:\n$$\nz_{t,j} = \\frac{x_{t,j} - \\bar{x}_j}{\\hat{\\sigma}_j}\n$$\nBy construction, each column of $Z$ has a sample mean of $0$ and a sample variance of $1$.\n\nPCA is then performed on this standardized data $Z$. The relevant matrix is the sample covariance matrix of $Z$, which we denote $\\Sigma_{\\text{std}}$:\n$$\n\\Sigma_{\\text{std}} = \\frac{1}{T_k-1} Z^\\top Z\n$$\nSince each column of $Z$ has unit variance, the diagonal elements of $\\Sigma_{\\text{std}}$ are all $1$, and the off-diagonal elements $(i, j)$ are the sample correlation coefficients between the original variables $x_i$ and $x_j$. Thus, $\\Sigma_{\\text{std}}$ is the sample correlation matrix of $X$.\n\nWe perform an eigendecomposition of $\\Sigma_{\\text{std}}$ to find its largest eigenvalue, $\\lambda_{\\text{std}}$, and the corresponding eigenvector, $v_{\\text{std}}$.\n\n**4. Diagnostic Metrics**\n\nTo quantify the distortion caused by failing to standardize, we compute two metrics:\n\n- **Angle between Principal Components**: The principal component directions $v_{\\text{raw}}$ and $v_{\\text{std}}$ are unit vectors in $\\mathbb{R}^{n_k}$. The angle between them measures how much the direction of maximum variance shifts. Since eigenvectors are defined only up to a sign (i.e., if $v$ is an eigenvector, so is $-v$), we compute the acute angle between the lines they span:\n  $$\n  \\theta = \\arccos\\left( \\left| v_{\\text{raw}}^\\top v_{\\text{std}} \\right| \\right)\n  $$\n  A value of $\\theta=0$ indicates perfect alignment, while a large angle (approaching $\\pi/2$) indicates severe misalignment.\n\n- **Difference in Explained Variance Share**: The fraction of total variance explained by the first principal component is given by its eigenvalue divided by the sum of all eigenvalues. The sum of eigenvalues is equal to the trace of the matrix, $\\operatorname{tr}(\\Sigma) = \\sum_{j=1}^{n_k} \\Sigma_{jj}$, which represents the total variance in the data. We compute the absolute difference in the explained variance share:\n  $$\n  \\Delta = \\left| \\frac{\\lambda_{\\text{raw}}}{\\operatorname{tr}(\\Sigma_{\\text{raw}})} - \\frac{\\lambda_{\\text{std}}}{\\operatorname{tr}(\\Sigma_{\\text{std}})} \\right|\n  $$\n  Note that for standardized data, $\\operatorname{tr}(\\Sigma_{\\text{std}}) = n_k$, the number of variables. A large $\\Delta$ indicates that the two methods give a very different assessment of the importance of the first component.\n\nThe procedure will be executed for each test case using the specified parameters and a fixed random seed for reproducibility. The results are expected to show minimal distortion for Case $1$ (similar scales) and significant distortion for Cases $2$ and $3$ (disparate scales), validating the necessity of standardization in practice.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the PCA problem by comparing results from raw and standardized data.\n\n    This function iterates through a set of predefined test cases. For each case, it:\n    1. Generates synthetic financial data based on a single-factor model with specified scales.\n    2. Performs PCA on the raw, centered data.\n    3. Performs PCA on the standardized data (equivalent to using the correlation matrix).\n    4. Computes two diagnostic metrics:\n        - The angle between the first principal components from the raw and standardized analyses.\n        - The absolute difference in the fraction of variance explained by the first component.\n    5. Collects and formats the results according to the problem specification.\n    \"\"\"\n    # Use a fixed pseudorandom number generator seed for reproducibility.\n    seed = 314159\n    rng = np.random.default_rng(seed)\n\n    # Test cases defined as (T_k, n_k, b^(k), u^(k), s^(k)))\n    test_cases = [\n        (500, 2, np.array([1.0, 0.9]), np.array([0.1, 0.1]), np.array([1.0, 1.2])),\n        (500, 2, np.array([1.0, 0.9]), np.array([0.1, 0.1]), np.array([1000.0, 1.0])),\n        (800, 3, np.array([0.2, 1.0, 1.0]), np.array([0.3, 0.2, 0.2]), np.array([1000.0, 1.0, 0.01]))\n    ]\n\n    all_results = []\n\n    for T, n, b, u, s in test_cases:\n        # 1. Data Generation\n        # Generate common factor f_t ~ N(0,1)\n        f = rng.normal(loc=0.0, scale=1.0, size=T)\n        \n        # Generate idiosyncratic noises e_{t,j} ~ N(0, u_j^2)\n        # E is a T x n matrix\n        E = rng.normal(loc=0.0, scale=u, size=(T, n))\n        \n        # Construct raw observations x_{t,j} = s_j * (b_j * f_t + e_{t,j})\n        X = s * (np.outer(f, b) + E)\n\n        # 2. PCA on Raw Data\n        # Center the data matrix X\n        X_centered = X - np.mean(X, axis=0)\n        \n        # Compute the sample covariance matrix (ddof=1 for unbiased estimator)\n        Sigma_raw = np.cov(X_centered, rowvar=False, ddof=1)\n        \n        # Eigendecomposition of the symmetric covariance matrix\n        # eigh returns eigenvalues in ascending order and corresponding eigenvectors in columns\n        eigvals_raw, eigvecs_raw = np.linalg.eigh(Sigma_raw)\n        \n        # The first principal component corresponds to the largest eigenvalue\n        lambda_raw = eigvals_raw[-1]\n        v_raw = eigvecs_raw[:, -1]\n\n        # 3. PCA on Standardized Data\n        # Standardize the data matrix X\n        stds = np.std(X, axis=0, ddof=1)\n        Z = X_centered / stds\n        \n        # Compute the sample correlation matrix (covariance of standardized data)\n        Sigma_std = np.cov(Z, rowvar=False, ddof=1)\n        \n        # Eigendecomposition of the correlation matrix\n        eigvals_std, eigvecs_std = np.linalg.eigh(Sigma_std)\n        \n        # The first principal component corresponds to the largest eigenvalue\n        lambda_std = eigvals_std[-1]\n        v_std = eigvecs_std[:, -1]\n\n        # 4. Diagnostic Computations\n        # Angle theta between the first principal components\n        # Take absolute value of dot product to handle sign ambiguity of eigenvectors\n        cos_theta = np.abs(np.dot(v_raw, v_std))\n        # Clip to prevent domain errors with arccos due to potential floating point inaccuracies\n        theta = np.arccos(np.clip(cos_theta, -1.0, 1.0))\n\n        # Difference in explained variance shares\n        total_var_raw = np.trace(Sigma_raw)\n        total_var_std = np.trace(Sigma_std) # This is always n for a correlation matrix\n        \n        share_raw = lambda_raw / total_var_raw\n        share_std = lambda_std / total_var_std\n        \n        delta = np.abs(share_raw - share_std)\n\n        # Append results, rounded to 6 decimal places\n        all_results.append([round(theta, 6), round(delta, 6)])\n\n    # Format the final output string as specified\n    formatted_results = [f\"[{theta:.6f},{delta:.6f}]\" for theta, delta in all_results]\n    print(f\"[[{','.join(formatted_results)}]]\")\n\nsolve()\n```", "id": "2421735"}]}