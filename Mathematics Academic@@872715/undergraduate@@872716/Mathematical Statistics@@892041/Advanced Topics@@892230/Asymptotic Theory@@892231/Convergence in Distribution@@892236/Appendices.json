{"hands_on_practices": [{"introduction": "The Central Limit Theorem (CLT) is arguably one of the most important results in all of statistics, stating that the standardized sample mean of independent and identically distributed variables approaches a standard normal distribution, regardless of the original distribution's shape. This exercise [@problem_id:1910206] provides hands-on practice with this concept, requiring you to first compute the necessary population moments from a given probability density function before applying the theorem. It's a fundamental workflow in statistical inference.", "problem": "Let $X_1, X_2, \\dots, X_n$ be a sequence of independent and identically distributed (i.i.d.) random variables, each with a probability density function (PDF) given by:\n$$\nf(x) = \n\\begin{cases} \n3x^2  \\text{for } 0 \\le x \\le 1 \\\\\n0  \\text{otherwise}\n\\end{cases}\n$$\nLet $\\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i$ denote the sample mean. Furthermore, let $\\mu$ and $\\sigma$ represent the true mean and standard deviation of an individual random variable $X_i$, respectively.\n\nConsider the transformed random variable $Z_n = \\frac{\\sqrt{n}(\\bar{X}_n - \\mu)}{\\sigma}$. Which of the following describes the distribution of $Z_n$ in the limit as the sample size $n$ approaches infinity?\n\nA. The standard normal distribution, $N(0,1)$.\n\nB. A normal distribution with mean $3/4$ and variance $3/80$.\n\nC. The uniform distribution on the interval $[0,1]$.\n\nD. The chi-squared distribution with 1 degree of freedom.\n\nE. A distribution with the PDF $f(x)=3x^2$ on $[0,1]$.\n\nF. A degenerate distribution at the point $3/4$.", "solution": "We first verify that $f(x)$ is a valid probability density function by computing its integral over its support. Using the definition of a PDF,\n$$\n\\int_{-\\infty}^{\\infty} f(x)\\,dx=\\int_{0}^{1} 3x^{2}\\,dx=\\left[x^{3}\\right]_{0}^{1}=1.\n$$\nThe mean $\\mu=\\mathbb{E}[X]$ is computed using the definition of expectation for a continuous random variable:\n$$\n\\mu=\\int_{-\\infty}^{\\infty} x f(x)\\,dx=\\int_{0}^{1} x\\cdot 3x^{2}\\,dx=\\int_{0}^{1} 3x^{3}\\,dx=\\left[\\frac{3}{4}x^{4}\\right]_{0}^{1}=\\frac{3}{4}.\n$$\nThe second moment is\n$$\n\\mathbb{E}[X^{2}]=\\int_{0}^{1} x^{2}\\cdot 3x^{2}\\,dx=\\int_{0}^{1} 3x^{4}\\,dx=\\left[\\frac{3}{5}x^{5}\\right]_{0}^{1}=\\frac{3}{5}.\n$$\nTherefore, the variance is\n$$\n\\sigma^{2}=\\text{Var}(X)=\\mathbb{E}[X^{2}]-\\mu^{2}=\\frac{3}{5}-\\left(\\frac{3}{4}\\right)^{2}=\\frac{3}{5}-\\frac{9}{16}=\\frac{48}{80}-\\frac{45}{80}=\\frac{3}{80},\n$$\nand the standard deviation is $\\sigma=\\sqrt{\\frac{3}{80}}$.\n\nDefine the standardized statistic\n$$\nZ_{n}=\\frac{\\sqrt{n}\\left(\\bar{X}_{n}-\\mu\\right)}{\\sigma}.\n$$\nBy the Lindebergâ€“Levy Central Limit Theorem, if $X_{1},X_{2},\\dots,X_{n}$ are i.i.d. with finite mean $\\mu$ and finite, positive variance $\\sigma^{2}$, then as $n\\to\\infty$,\n$$\n\\frac{\\sqrt{n}\\left(\\bar{X}_{n}-\\mu\\right)}{\\sigma}\\ \\Rightarrow\\ N(0,1),\n$$\nthat is, $Z_{n}$ converges in distribution to the standard normal distribution. The conditions are satisfied here because $\\mu=\\frac{3}{4}$ and $\\sigma^{2}=\\frac{3}{80}$ are finite and $\\sigma^{2}0$. Therefore, the limiting distribution of $Z_{n}$ is $N(0,1)$, which corresponds to option A.", "answer": "$$\\boxed{A}$$", "id": "1910206"}, {"introduction": "What happens when we apply a function to a sequence of random variables that we already know converges? The Continuous Mapping Theorem (CMT) provides a powerful and elegant answer, stating that convergence is preserved by continuous functions. This problem [@problem_id:1910213] presents a classic application of the CMT, asking you to determine the limiting distribution of the square of a t-distributed random variable as its degrees of freedom increase.", "problem": "Let $\\{T_n\\}_{n=1}^{\\infty}$ be a sequence of random variables where each $T_n$ follows a t-distribution with $n$ degrees of freedom. It is a known result that this sequence of random variables converges in distribution to a standard normal random variable, $Z$, which has a probability density function given by $\\phi(z) = \\frac{1}{\\sqrt{2\\pi}}\\exp(-z^2/2)$. Symbolically, this is written as $T_n \\xrightarrow{d} Z$.\n\nConsider a new sequence of random variables $\\{Y_n\\}_{n=1}^{\\infty}$ defined by the transformation $Y_n = T_n^2$.\n\nIdentify the limiting distribution of the sequence $Y_n$ as $n$ approaches infinity from the options below.\n\nA. A standard normal distribution, $N(0, 1)$.\n\nB. A chi-squared distribution with 1 degree of freedom, $\\chi^2(1)$.\n\nC. A t-distribution with 1 degree of freedom (which is also known as the Cauchy distribution).\n\nD. An F-distribution with $(1, 1)$ degrees of freedom.\n\nE. An exponential distribution with a rate parameter $\\lambda=1/2$.", "solution": "We are given that $T_{n} \\xrightarrow{d} Z$ where $Z \\sim N(0,1)$. Define the transformation $g:\\mathbb{R} \\to \\mathbb{R}$ by $g(x)=x^{2}$. The function $g$ is continuous on $\\mathbb{R}$. By the Continuous Mapping Theorem, if $X_{n} \\xrightarrow{d} X$ and $g$ is continuous, then $g(X_{n}) \\xrightarrow{d} g(X)$. Applying this with $X_{n}=T_{n}$ and $X=Z$, we obtain\n$$\nY_{n}=T_{n}^{2} \\xrightarrow{d} Z^{2}.\n$$\nIt is a defining property of the chi-squared distribution that if $Z \\sim N(0,1)$, then $Z^{2} \\sim \\chi^{2}(1)$. Therefore, the limiting distribution of $Y_{n}$ is $\\chi^{2}(1)$, which corresponds to option B.", "answer": "$$\\boxed{B}$$", "id": "1910213"}, {"introduction": "While the Central Limit Theorem is powerful, many important limiting distributions in statistics are not normal. In such cases, we often return to first principles, working directly with cumulative distribution functions (CDFs) or probability density functions (PDFs). This practice problem [@problem_id:1910246] guides you through such a derivation, showing how a sequence of scaled Beta-distributed random variables converges to a standard exponential distribution, a result with applications in areas like reliability theory and survival analysis.", "problem": "Let $\\{X_n\\}_{n=1}^{\\infty}$ be a sequence of random variables, where each $X_n$ follows a Beta distribution with parameters $\\alpha=1$ and $\\beta=n$. Consider a new sequence of random variables $\\{Y_n\\}_{n=1}^{\\infty}$ defined by the transformation $Y_n = nX_n$.\n\nAs $n \\to \\infty$, the sequence $Y_n$ converges in distribution to a random variable $Y$. Identify the probability density function (PDF) of the limiting random variable $Y$ for its domain of $y  0$.\n\nA. $f(y) = \\exp(-y)$\n\nB. $f(y) = y \\exp(-y)$\n\nC. $f(y) = 1$\n\nD. $f(y) = \\frac{1}{2}\\exp(-\\frac{y}{2})$\n\nE. $f(y) = \\frac{1}{\\sqrt{2\\pi y}}\\exp(-\\frac{y}{2})$", "solution": "We start with the probability density function of the Beta distribution. For $X_{n} \\sim \\mathrm{Beta}(\\alpha,\\beta)$ with $\\alpha=1$ and $\\beta=n$, the density is\n$$\nf_{X_{n}}(x)=\\frac{x^{\\alpha-1}(1-x)^{\\beta-1}}{B(\\alpha,\\beta)}, \\quad 0x1,\n$$\nwhere $B(\\alpha,\\beta)=\\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}$. Substituting $\\alpha=1$ and $\\beta=n$ gives\n$$\nB(1,n)=\\frac{\\Gamma(1)\\Gamma(n)}{\\Gamma(n+1)}=\\frac{1\\cdot \\Gamma(n)}{n\\Gamma(n)}=\\frac{1}{n},\n$$\nhence\n$$\nf_{X_{n}}(x)=n(1-x)^{n-1}, \\quad 0x1.\n$$\nDefine $Y_{n}=nX_{n}$. Using the change of variables $y=nx$, so $x=y/n$ and $\\frac{dx}{dy}=\\frac{1}{n}$, the density of $Y_{n}$ is\n$$\nf_{Y_{n}}(y)=f_{X_{n}}\\!\\left(\\frac{y}{n}\\right)\\frac{1}{n}=n\\left(1-\\frac{y}{n}\\right)^{n-1}\\frac{1}{n}=\\left(1-\\frac{y}{n}\\right)^{n-1}, \\quad 0yn,\n$$\nand $f_{Y_{n}}(y)=0$ otherwise.\n\nTo find the limiting distribution, it is convenient to consider the cumulative distribution function. For $y\\geq 0$,\n$$\nF_{Y_{n}}(y)=\\mathbb{P}(Y_{n}\\leq y)=\\mathbb{P}\\!\\left(X_{n}\\leq \\frac{y}{n}\\right)=\\int_{0}^{y/n} n(1-t)^{n-1}\\,dt=1-\\left(1-\\frac{y}{n}\\right)^{n}.\n$$\nUsing the standard limit $\\lim_{n\\to\\infty}\\left(1+\\frac{z}{n}\\right)^{n}=\\exp(z)$, with $z=-y$, we obtain for each fixed $y\\geq 0$,\n$$\n\\lim_{n\\to\\infty}F_{Y_{n}}(y)=1-\\exp(-y).\n$$\nThus $Y_{n}$ converges in distribution to a random variable $Y$ with cumulative distribution function $F_{Y}(y)=1-\\exp(-y)$ for $y\\geq 0$, which has probability density function\n$$\nf_{Y}(y)=\\exp(-y), \\quad y0.\n$$\nThis matches option A.", "answer": "$$\\boxed{A}$$", "id": "1910246"}]}