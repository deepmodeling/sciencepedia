{"hands_on_practices": [{"introduction": "Understanding the efficiency of an estimator begins with a fundamental concept: Fisher Information. This quantity measures how much information a random sample provides about an unknown parameter. This exercise provides foundational practice by guiding you through the direct calculation of Fisher Information for the scale parameter of a Weibull distribution, a common model in reliability engineering and survival analysis [@problem_id:1896446].", "problem": "In reliability engineering, the lifetime of certain components is often modeled using the Weibull distribution. The probability density function (PDF) for a Weibull-distributed random variable $X$ is given by\n$$f(x; k, \\lambda) = \\frac{k}{\\lambda} \\left(\\frac{x}{\\lambda}\\right)^{k-1} \\exp\\left(-\\left(\\frac{x}{\\lambda}\\right)^k\\right)$$\nfor $x \\ge 0$, where $k  0$ is the shape parameter and $\\lambda  0$ is the scale parameter.\n\nConsider a specific type of solid-state device whose lifetime, measured in hours, is known to follow a Weibull distribution with a shape parameter $k=2$. The scale parameter $\\lambda$ is unknown and is a characteristic of a particular manufacturing batch. To estimate this parameter, a random sample of $n$ devices, $X_1, X_2, \\ldots, X_n$, is taken from the batch and their lifetimes are recorded.\n\nCalculate the Fisher Information, $I(\\lambda)$, that this random sample provides about the unknown scale parameter $\\lambda$. Present your answer as a closed-form analytic expression in terms of $n$ and $\\lambda$.", "solution": "The devices have lifetimes modeled by a Weibull distribution with known shape parameter $k=2$ and unknown scale parameter $\\lambda0$. For one observation $X$, the probability density function is\n$$\nf(x;2,\\lambda)=\\frac{2}{\\lambda}\\left(\\frac{x}{\\lambda}\\right)^{1}\\exp\\!\\left(-\\left(\\frac{x}{\\lambda}\\right)^{2}\\right), \\quad x\\ge 0.\n$$\nThe log-likelihood for a single observation is\n$$\n\\ell(\\lambda\\mid x) = \\ln f(x;2,\\lambda) = \\ln 2 + \\ln x - 2\\ln\\lambda - \\frac{x^2}{\\lambda^2}.\n$$\nDifferentiate with respect to $\\lambda$ to obtain the score for one observation:\n$$\n\\frac{\\partial \\ell}{\\partial \\lambda} = -\\frac{2}{\\lambda} + 2 x^{2} \\lambda^{-3}.\n$$\nDifferentiate again to obtain the observed information (negative Hessian) for one observation:\n$$\n\\frac{\\partial^{2} \\ell}{\\partial \\lambda^{2}} = \\frac{2}{\\lambda^{2}} - 6 x^{2} \\lambda^{-4}.\n$$\nThe Fisher information for one observation is the negative expected second derivative:\n$$\nI_{1}(\\lambda) = -\\mathbb{E}\\!\\left[\\frac{\\partial^{2} \\ell}{\\partial \\lambda^{2}}\\right]\n= -\\left(\\frac{2}{\\lambda^{2}} - 6\\,\\mathbb{E}[X^{2}]\\,\\lambda^{-4}\\right).\n$$\nFor a Weibull distribution with shape $k=2$ and scale $\\lambda$, the moment $\\mathbb{E}[X^{r}]$ is given by $\\mathbb{E}[X^{r}] = \\lambda^{r}\\,\\Gamma\\!\\left(1+\\frac{r}{2}\\right)$. Setting $r=2$ gives\n$$\n\\mathbb{E}[X^{2}] = \\lambda^{2}\\,\\Gamma(2) = \\lambda^{2}.\n$$\nSubstitute into the information expression:\n$$\nI_{1}(\\lambda) = -\\frac{2}{\\lambda^{2}} + 6 \\cdot \\lambda^{2} \\cdot \\lambda^{-4}\n= -\\frac{2}{\\lambda^{2}} + \\frac{6}{\\lambda^{2}} = \\frac{4}{\\lambda^{2}}.\n$$\nFor $n$ independent observations, Fisher information adds, so the total information is\n$$\nI(\\lambda) = n\\,I_{1}(\\lambda) = \\frac{4n}{\\lambda^{2}}.\n$$", "answer": "$$\\boxed{\\frac{4n}{\\lambda^{2}}}$$", "id": "1896446"}, {"introduction": "The true power of Fisher Information lies in its connection to estimator precision, as established by the Cram√©r-Rao Lower Bound. For large samples, the variance of a Maximum Likelihood Estimator (MLE) achieves this bound, making it asymptotically efficient. This practice solidifies this key relationship by asking you to determine the asymptotic variance of the MLE for a Binomial proportion, which is simply the inverse of the Fisher Information [@problem_id:1896452].", "problem": "In a quality control setting, the number of defective items, $X$, found in a fixed-size batch of $k$ items is modeled by a Binomial distribution, $X \\sim \\text{Binomial}(k, p)$. Here, $k$ is a known positive integer representing the batch size, and $p \\in (0, 1)$ is the unknown probability of a single item being defective.\n\nA statistician obtains a single observation of $X$ to construct the Maximum Likelihood Estimator (MLE) for the parameter $p$. A fundamental property of this estimator is its asymptotic variance, which quantifies its expected precision.\n\nDetermine the asymptotic variance of the MLE of $p$ based on a single observation from this distribution. Express your answer as a symbolic expression in terms of $p$ and $k$.", "solution": "We observe a single count $X$ from $X \\sim \\text{Binomial}(k,p)$. The likelihood and log-likelihood for $p \\in (0,1)$ are\n$$\nL(p; x) = \\binom{k}{x} p^{x} (1-p)^{k-x}, \\quad \\ell(p; x) = \\ln L(p; x) = \\ln \\binom{k}{x} + x \\ln p + (k-x) \\ln(1-p).\n$$\nDifferentiate the log-likelihood with respect to $p$ to obtain the score and the second derivative:\n$$\n\\frac{\\partial \\ell}{\\partial p} = \\frac{x}{p} - \\frac{k-x}{1-p}, \\quad \\frac{\\partial^{2} \\ell}{\\partial p^{2}} = -\\frac{x}{p^{2}} - \\frac{k-x}{(1-p)^{2}}.\n$$\nThe Fisher information for one observation is\n$$\nI(p) = -\\mathbb{E}\\!\\left[\\frac{\\partial^{2} \\ell}{\\partial p^{2}}\\right] = \\mathbb{E}\\!\\left[\\frac{x}{p^{2}} + \\frac{k-x}{(1-p)^{2}}\\right].\n$$\nUsing $\\mathbb{E}[X] = k p$ and hence $\\mathbb{E}[k-X] = k(1-p)$, we get\n$$\nI(p) = \\frac{k p}{p^{2}} + \\frac{k(1-p)}{(1-p)^{2}} = \\frac{k}{p} + \\frac{k}{1-p} = \\frac{k}{p(1-p)}.\n$$\nBy the standard asymptotic normality of the MLE, for $m$ independent observations the asymptotic variance is $\\left(m I(p)\\right)^{-1}$. With a single observation ($m=1$), the asymptotic variance of the MLE is therefore the inverse Fisher information for one observation:\n$$\n\\left(I(p)\\right)^{-1} = \\frac{p(1-p)}{k}.\n$$", "answer": "$$\\boxed{\\frac{p(1-p)}{k}}$$", "id": "1896452"}, {"introduction": "In many statistical applications, we are interested not only in a parameter itself but also in a function of that parameter. The Delta Method is an indispensable tool that allows us to approximate the variance of such transformed estimators. This problem demonstrates this technique by asking you to find the asymptotic variance of the logarithm of a sample proportion, a transformation often used to stabilize variance or model odds [@problem_id:1896436].", "problem": "Let $X_1, X_2, \\dots, X_n$ be a sequence of $n$ independent and identically distributed Bernoulli random variables with success probability $p$, where $0  p  1$. The Maximum Likelihood Estimator (MLE) for $p$ is given by the sample proportion, $\\hat{p} = \\frac{1}{n} \\sum_{i=1}^n X_i$.\n\nConsider the transformed estimator $T = \\log(\\hat{p})$, where $\\log$ denotes the natural logarithm. Determine the asymptotic variance of $T$ as a function of $p$ and $n$.", "solution": "Let $X_{1},\\dots,X_{n}$ be i.i.d. Bernoulli$(p)$ with $0p1$. The MLE is $\\hat{p}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}$. Its mean and variance are $\\mathbb{E}[\\hat{p}]=p$ and $\\operatorname{Var}(\\hat{p})=\\frac{p(1-p)}{n}$. By the Central Limit Theorem,\n$$\n\\sqrt{n}\\,(\\hat{p}-p)\\;\\xrightarrow{d}\\;N\\!\\left(0,\\;p(1-p)\\right).\n$$\nConsider $g(x)=\\ln(x)$, which is differentiable at $p$ with $g'(p)=\\frac{1}{p}$. By the Delta method,\n$$\n\\sqrt{n}\\,\\big(g(\\hat{p})-g(p)\\big)\\;\\xrightarrow{d}\\;N\\!\\left(0,\\;\\left(g'(p)\\right)^{2}p(1-p)\\right)\n= N\\!\\left(0,\\;\\frac{p(1-p)}{p^{2}}\\right)\n= N\\!\\left(0,\\;\\frac{1-p}{p}\\right).\n$$\nTherefore, the asymptotic variance of $T=\\ln(\\hat{p})$ is\n$$\n\\operatorname{avar}\\big(\\ln(\\hat{p})\\big)\\;=\\;\\frac{1}{n}\\left(g'(p)\\right)^{2}p(1-p)\\;=\\;\\frac{1-p}{n\\,p}.\n$$\nThis holds since $0p1$ ensures differentiability and $\\hat{p}\\in(0,1)$ with probability approaching one as $n\\to\\infty$.", "answer": "$$\\boxed{\\frac{1-p}{n\\,p}}$$", "id": "1896436"}]}