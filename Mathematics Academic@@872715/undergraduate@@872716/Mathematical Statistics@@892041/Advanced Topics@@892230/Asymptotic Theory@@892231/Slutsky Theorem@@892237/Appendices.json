{"hands_on_practices": [{"introduction": "This first exercise explores the core mechanism of Slutsky's Theorem. We will practice combining a sequence of random variables that converges in distribution with a consistent estimator, which converges in probability to a constant. This fundamental skill is the building block for nearly all applications of the theorem in statistical inference. [@problem_id:1955717]", "problem": "In statistical inference, the properties of estimators and test statistics as the sample size $n$ grows to infinity are of fundamental interest. A sequence of estimators $\\{\\hat{\\theta}_n\\}_{n=1}^{\\infty}$ for a parameter $\\theta$ is defined as *consistent* if it converges in probability to $\\theta$. This is written as $\\hat{\\theta}_n \\to_p \\theta$ and formally means that for any arbitrary small positive number $\\epsilon$, the probability $P(|\\hat{\\theta}_n - \\theta| \\ge \\epsilon)$ approaches zero as $n \\to \\infty$.\n\nLet $\\{Z_n\\}_{n=1}^{\\infty}$ be a sequence of random variables that is known to converge in distribution to a standard normal random variable, $Z \\sim N(0, 1)$. This is denoted as $Z_n \\to_d Z$.\n\nNow, suppose you have a consistent estimator $\\hat{\\theta}_n$ for a parameter $\\theta$, where $\\theta$ is a fixed, non-zero real number. Consider a new sequence of random variables $\\{T_n\\}_{n=1}^{\\infty}$ constructed as the ratio:\n$$T_n = \\frac{Z_n}{\\hat{\\theta}_n}$$\nDetermine the limiting distribution of $T_n$ as $n \\to \\infty$. Express your answer using standard notation for statistical distributions, for example, $N(\\mu, \\sigma^2)$ for a normal distribution with mean $\\mu$ and variance $\\sigma^2$.", "solution": "We are given two convergences: $Z_{n} \\to_{d} Z$ where $Z \\sim N(0,1)$, and $\\hat{\\theta}_{n} \\to_{p} \\theta$ where $\\theta \\in \\mathbb{R}\\setminus\\{0\\}$. Define $T_{n} = Z_{n}/\\hat{\\theta}_{n}$. To find the limiting distribution of $T_{n}$, apply Slutsky’s theorem (or the continuous mapping theorem in its two-sequence form).\n\nFirst, since $\\hat{\\theta}_{n} \\to_{p} \\theta$ with $\\theta \\neq 0$, the function $f(x,y) = x/y$ is continuous at all points with $y \\neq 0$, in particular at $(x,\\theta)$ for any $x \\in \\mathbb{R}$. Slutsky’s theorem states that if $X_{n} \\to_{d} X$ and $Y_{n} \\to_{p} c$ for a constant $c$, then $X_{n}/Y_{n} \\to_{d} X/c$. Taking $X_{n} = Z_{n}$, $Y_{n} = \\hat{\\theta}_{n}$, and $c = \\theta$, we obtain\n$$\nT_{n} \\;=\\; \\frac{Z_{n}}{\\hat{\\theta}_{n}} \\;\\to_{d}\\; \\frac{Z}{\\theta}.\n$$\nSince $Z \\sim N(0,1)$ and for any constant $a$ we have $aZ \\sim N(0, a^{2})$, it follows that\n$$\n\\frac{Z}{\\theta} \\;\\sim\\; N\\!\\left(0,\\;\\frac{1}{\\theta^{2}}\\right).\n$$\nTherefore, the limiting distribution of $T_{n}$ is $N\\!\\left(0, \\theta^{-2}\\right)$.", "answer": "$$\\boxed{N\\!\\left(0,\\,\\theta^{-2}\\right)}$$", "id": "1955717"}, {"introduction": "Building upon the basics, this problem illustrates how Slutsky's Theorem acts as a powerful bridge between two other cornerstone results: the Central Limit Theorem (CLT) and the Law of Large Numbers (LLN). You will see how a term derived from the CLT can be combined with a consistent estimator derived from the LLN. This type of manipulation is essential for deriving the distributions of many common test statistics. [@problem_id:1955718]", "problem": "Let $X_1, X_2, \\dots, X_n$ be a sequence of independent and identically distributed random variables from a Bernoulli distribution with parameter $p$, where $0 < p < 1$. The parameter $p$ represents the probability of success, i.e., $P(X_i = 1) = p$ and $P(X_i = 0) = 1-p$.\n\nLet $\\hat{p}_n = \\frac{1}{n} \\sum_{i=1}^{n} X_i$ be the sample proportion of successes in the first $n$ trials.\n\nConsider the new random variable $Y_n = (1-\\hat{p}_n)\\sqrt{n}(\\hat{p}_n - p)$. Which of the following describes the limiting distribution of $Y_n$ as $n \\to \\infty$?\n\nA. A Normal distribution with mean $0$ and variance $p(1-p)$.\n\nB. A Normal distribution with mean $0$ and variance $p^2(1-p)^2$.\n\nC. A Normal distribution with mean $0$ and variance $p(1-p)^2$.\n\nD. A Normal distribution with mean $0$ and variance $p(1-p)^3$.\n\nE. The sequence of random variables $Y_n$ does not converge in distribution to a Normal distribution.", "solution": "We have $X_{i} \\sim \\text{Bernoulli}(p)$ i.i.d., so $E[X_{i}] = p$ and $\\operatorname{Var}(X_{i}) = p(1-p)$. The sample proportion $\\hat{p}_{n} = \\frac{1}{n}\\sum_{i=1}^{n} X_{i}$ satisfies the law of large numbers:\n$$\n\\hat{p}_{n} \\xrightarrow{p} p \\quad \\text{as } n \\to \\infty,\n$$\nwhich implies\n$$\n1 - \\hat{p}_{n} \\xrightarrow{p} 1 - p.\n$$\n\nBy the central limit theorem for i.i.d. Bernoulli variables,\n$$\n\\sqrt{n}\\,(\\hat{p}_{n} - p) \\xrightarrow{d} Z \\quad \\text{with } Z \\sim \\mathcal{N}\\bigl(0,\\,p(1-p)\\bigr).\n$$\n\nDefine $Y_{n} = (1 - \\hat{p}_{n}) \\sqrt{n} (\\hat{p}_{n} - p)$. Since $(1 - \\hat{p}_{n}) \\xrightarrow{p} (1 - p)$ and $\\sqrt{n}(\\hat{p}_{n} - p) \\xrightarrow{d} Z$, by Slutsky's theorem,\n$$\nY_{n} \\xrightarrow{d} (1 - p)\\,Z.\n$$\nIf $Z \\sim \\mathcal{N}(0, \\sigma^{2})$, then for any constant $c$, $cZ \\sim \\mathcal{N}(0, c^{2}\\sigma^{2})$. Applying this with $c = 1 - p$ and $\\sigma^{2} = p(1-p)$, we conclude\n$$\n(1 - p)\\,Z \\sim \\mathcal{N}\\bigl(0,\\,(1 - p)^{2}\\,p(1-p)\\bigr) = \\mathcal{N}\\bigl(0,\\,p(1-p)^{3}\\bigr).\n$$\n\nTherefore, the limiting distribution of $Y_{n}$ is Normal with mean $0$ and variance $p(1-p)^{3}$, which corresponds to option D.", "answer": "$$\\boxed{D}$$", "id": "1955718"}, {"introduction": "To truly appreciate the power of Slutsky's Theorem, we must see it in action beyond the familiar realm of the Normal distribution. This advanced problem challenges you to apply the theorem in a scenario from Extreme Value Theory, where the limiting distribution is a Gumbel distribution. This demonstrates the theorem's generality and its crucial role in analyzing a wide variety of statistical models. [@problem_id:1388331]", "problem": "Let $Z_1, Z_2, \\dots, Z_n$ be a sequence of independent and identically distributed random variables drawn from an Exponential distribution with an unknown rate parameter $\\lambda > 0$. The probability density function is given by $f(z; \\lambda) = \\lambda \\exp(-\\lambda z)$ for $z \\ge 0$.\n\nLet $M_n = \\max\\{Z_1, \\dots, Z_n\\}$ denote the sample maximum and $\\bar{Z}_n = \\frac{1}{n} \\sum_{i=1}^n Z_i$ denote the sample mean.\n\nFrom Extreme Value Theory, it is a known result that the sequence of centered and scaled maxima, $U_n = \\lambda M_n - \\ln n$, converges in distribution to a standard Gumbel random variable $G$. The cumulative distribution function (CDF) of $G$ is $F_G(x) = \\exp(-\\exp(-x))$ for all real $x$.\n\nConsider the statistic $T_n$ defined as:\n$$T_n = \\bar{Z}_n (\\lambda M_n - \\ln n)$$\n\nDetermine the limiting cumulative distribution function, $F(t) = \\lim_{n \\to \\infty} P(T_n \\le t)$, of the statistic $T_n$. Express your answer as a function of $t$ and the parameter $\\lambda$.", "solution": "We start with the i.i.d. sample $Z_{1},\\dots,Z_{n}$ from $\\mathrm{Exp}(\\lambda)$ with density $f(z;\\lambda)=\\lambda \\exp(-\\lambda z)$ for $z\\ge 0$. The sample mean is $\\bar{Z}_{n}=\\frac{1}{n}\\sum_{i=1}^{n}Z_{i}$ and the sample maximum is $M_{n}=\\max\\{Z_{1},\\dots,Z_{n}\\}$.\n\nFirst, by the Strong Law of Large Numbers, since $\\mathbb{E}[Z_{1}]=\\frac{1}{\\lambda}<\\infty$, we have\n$$\n\\bar{Z}_{n}\\xrightarrow{\\text{a.s.}}\\frac{1}{\\lambda},\n$$\nwhich implies $\\bar{Z}_{n}\\xrightarrow{p}\\frac{1}{\\lambda}$.\n\nSecond, for the maximum of exponentials, for any $x\\ge 0$,\n$$\n\\mathbb{P}(M_{n}\\le x)=\\left(1-\\exp(-\\lambda x)\\right)^{n}.\n$$\nSetting $x=\\frac{y+\\ln n}{\\lambda}$ for $y\\in\\mathbb{R}$, we obtain\n$$\n\\mathbb{P}(\\lambda M_{n}-\\ln n\\le y)=\\left(1-n^{-1}\\exp(-y)\\right)^{n}\\xrightarrow[n\\to\\infty]{}\\exp\\!\\left(-\\exp(-y)\\right),\n$$\nhence\n$$\nU_{n}:=\\lambda M_{n}-\\ln n\\xrightarrow{d}G,\n$$\nwhere $G$ is standard Gumbel with CDF $F_{G}(x)=\\exp(-\\exp(-x))$.\n\nNow consider the statistic $T_{n}=\\bar{Z}_{n}\\,U_{n}$. By Slutsky’s theorem, if $U_{n}\\xrightarrow{d}G$ and $\\bar{Z}_{n}\\xrightarrow{p}\\frac{1}{\\lambda}$, then\n$$\nT_{n}=\\bar{Z}_{n}\\,U_{n}\\xrightarrow{d}\\frac{1}{\\lambda}\\,G.\n$$\nFor a positive constant $c=\\frac{1}{\\lambda}$ and any $t\\in\\mathbb{R}$, the limiting CDF is\n$$\nF(t)=\\mathbb{P}\\!\\left(\\frac{1}{\\lambda}G\\le t\\right)=\\mathbb{P}\\!\\left(G\\le \\lambda t\\right)=F_{G}(\\lambda t)=\\exp\\!\\left(-\\exp(-\\lambda t)\\right).\n$$\nTherefore, the limiting cumulative distribution function of $T_{n}$ is\n$$\nF(t)=\\exp\\!\\left(-\\exp(-\\lambda t)\\right),\\quad t\\in\\mathbb{R}.\n$$", "answer": "$$\\boxed{\\exp\\!\\left(-\\exp(-\\lambda t)\\right)}$$", "id": "1388331"}]}