{"hands_on_practices": [{"introduction": "The best way to understand the Expectation-Maximization algorithm is to perform the calculations by hand. This first practice provides a concrete, step-by-step walkthrough of a single iteration for a Gaussian Mixture Model (GMM), one of the most common applications of EM. By manually computing the responsibilities in the E-step and then using them to update the model parameters in the M-step [@problem_id:1960172], you will gain a tangible feel for how the algorithm softly assigns data points to clusters and refines its estimates.", "problem": "A researcher is analyzing a dataset of measurements, which is believed to be sampled from a mixture of two distinct populations, A and B. They decide to model the data using a two-component Gaussian Mixture Model (GMM). The probability density function (PDF) for a single data point $x$ is given by:\n$$p(x | \\theta) = \\pi_A \\mathcal{N}(x | \\mu_A, \\sigma_A^2) + \\pi_B \\mathcal{N}(x | \\mu_B, \\sigma_B^2)$$\nwhere $\\pi_A$ and $\\pi_B$ are the mixing proportions (with $\\pi_A + \\pi_B = 1$), and $\\mathcal{N}(x | \\mu, \\sigma^2)$ is the Gaussian PDF:\n$$\\mathcal{N}(x | \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)$$\n\nThe dataset consists of the following five measurements: $\\{4.0, 4.5, 5.0, 8.0, 9.0\\}$.\nFor simplicity, the variances of the two components are assumed to be known and fixed at $\\sigma_A^2 = \\sigma_B^2 = 1.0$.\n\nThe researcher initializes the model parameters for the Expectation-Maximization (EM) algorithm as follows:\n- Initial means: $\\mu_A^{(0)} = 4.2$ and $\\mu_B^{(0)} = 8.8$.\n- Initial mixing proportions: $\\pi_A^{(0)} = 0.5$ and $\\pi_B^{(0)} = 0.5$.\n\nYour task is to perform a single, full iteration of the EM algorithm, which consists of one Expectation step (E-step) followed by one Maximization step (M-step), to compute the updated parameters.\n\nCalculate the updated value for the mean of the first component, $\\mu_A^{(1)}$. Report your answer as a real number, rounded to four significant figures.", "solution": "We apply one Expectation-Maximization (EM) iteration for the two-component Gaussian Mixture Model with fixed variances $\\sigma_{A}^{2}=\\sigma_{B}^{2}=1$ and initial parameters $\\mu_{A}^{(0)}=4.2$, $\\mu_{B}^{(0)}=8.8$, $\\pi_{A}^{(0)}=\\pi_{B}^{(0)}=0.5$.\n\nE-step (responsibilities):\nFor each data point $x_{i}$, the responsibility of component $A$ is\n$$\nr_{iA} \\equiv \\gamma_{iA} = \\frac{\\pi_{A}^{(0)} \\mathcal{N}(x_{i} \\mid \\mu_{A}^{(0)},1)}{\\pi_{A}^{(0)} \\mathcal{N}(x_{i} \\mid \\mu_{A}^{(0)},1) + \\pi_{B}^{(0)} \\mathcal{N}(x_{i} \\mid \\mu_{B}^{(0)},1)}.\n$$\nWith $\\pi_{A}^{(0)}=\\pi_{B}^{(0)}$ and equal variances, the common factors cancel:\n$$\nr_{iA}=\\frac{\\exp\\!\\left(-\\frac{(x_{i}-\\mu_{A}^{(0)})^{2}}{2}\\right)}{\\exp\\!\\left(-\\frac{(x_{i}-\\mu_{A}^{(0)})^{2}}{2}\\right)+\\exp\\!\\left(-\\frac{(x_{i}-\\mu_{B}^{(0)})^{2}}{2}\\right)}=\\frac{1}{1+\\exp\\!\\left(\\frac{(x_{i}-\\mu_{A}^{(0)})^{2}-(x_{i}-\\mu_{B}^{(0)})^{2}}{2}\\right)}.\n$$\nFor the dataset $\\{4.0,4.5,5.0,8.0,9.0\\}$ and $(\\mu_{A}^{(0)},\\mu_{B}^{(0)})=(4.2,8.8)$:\n- For $x_{1}=4.0$: $(x_{1}-\\mu_{A}^{(0)})^{2}=0.04$, $(x_{1}-\\mu_{B}^{(0)})^{2}=23.04$, so\n$$\nr_{1A}=\\frac{1}{1+\\exp(-11.5)}\\approx 0.9999898625.\n$$\n- For $x_{2}=4.5$: $(x_{2}-\\mu_{A}^{(0)})^{2}=0.09$, $(x_{2}-\\mu_{B}^{(0)})^{2}=18.49$, so\n$$\nr_{2A}=\\frac{1}{1+\\exp(-9.2)}\\approx 0.9998989606.\n$$\n- For $x_{3}=5.0$: $(x_{3}-\\mu_{A}^{(0)})^{2}=0.64$, $(x_{3}-\\mu_{B}^{(0)})^{2}=14.44$, so\n$$\nr_{3A}=\\frac{1}{1+\\exp(-6.9)}\\approx 0.9989932309.\n$$\n- For $x_{4}=8.0$: $(x_{4}-\\mu_{A}^{(0)})^{2}=14.44$, $(x_{4}-\\mu_{B}^{(0)})^{2}=0.64$, so\n$$\nr_{4A}=\\frac{1}{1+\\exp(6.9)}=1-r_{3A}\\approx 0.0010067691.\n$$\n- For $x_{5}=9.0$: $(x_{5}-\\mu_{A}^{(0)})^{2}=23.04$, $(x_{5}-\\mu_{B}^{(0)})^{2}=0.04$, so\n$$\nr_{5A}=\\frac{1}{1+\\exp(11.5)}=1-r_{1A}\\approx 0.0000101375.\n$$\nNote $r_{1A}+r_{5A}=1$ and $r_{3A}+r_{4A}=1$ by symmetry.\n\nM-step (update mean of component A):\nWith fixed variance, the updated mean is the responsibility-weighted average:\n$$\n\\mu_{A}^{(1)}=\\frac{\\sum_{i=1}^{5} r_{iA} x_{i}}{\\sum_{i=1}^{5} r_{iA}}.\n$$\nCompute the denominator using symmetry:\n$$\n\\sum_{i=1}^{5} r_{iA}=(r_{1A}+r_{5A})+(r_{3A}+r_{4A})+r_{2A}=2+r_{2A}\\approx 2.9998989606.\n$$\nCompute the numerator; group symmetric pairs:\n$$\n\\sum_{i=1}^{5} r_{iA} x_{i}=(r_{1A}\\cdot 4.0 + r_{5A}\\cdot 9.0) + (r_{3A}\\cdot 5.0 + r_{4A}\\cdot 8.0) + r_{2A}\\cdot 4.5.\n$$\nUse $r_{5A}=1-r_{1A}$ and $r_{4A}=1-r_{3A}$:\n$$\nr_{1A}\\cdot 4.0 + r_{5A}\\cdot 9.0 = 9 - 5 r_{1A},\\quad\nr_{3A}\\cdot 5.0 + r_{4A}\\cdot 8.0 = 8 - 3 r_{3A}.\n$$\nThus\n$$\n\\sum_{i=1}^{5} r_{iA} x_{i} = 17 - 5 r_{1A} - 3 r_{3A} + 4.5 r_{2A} \\approx 13.5026163178.\n$$\nTherefore,\n$$\n\\mu_{A}^{(1)}=\\frac{13.5026163178}{2.9998989606}\\approx 4.501023693.\n$$\nRounded to four significant figures, the updated mean is $4.501$.", "answer": "$$\\boxed{4.501}$$", "id": "1960172"}, {"introduction": "After grasping the mechanics of a single iteration, the next step is to understand how the update formulas are derived in the first place. This exercise moves from a numerical example to a symbolic derivation, showing the versatility of the EM algorithm beyond Gaussian mixtures. You will derive the M-step update rule for the parameters of a mixture of geometric distributions, which could model, for instance, failure rates in a manufacturing process [@problem_id:1960170]. This practice is crucial for developing the skill to adapt the EM algorithm to new models with different underlying probability distributions.", "problem": "A quality control engineer is analyzing the reliability of a batch of electronic components. These components are sourced from two suppliers, A and B. It is known that the number of operational cycles until a component fails, denoted by $X$, follows a geometric distribution. The probability mass function (PMF) is given by $P(X=k) = p(1-p)^{k-1}$ for $k=1, 2, 3, \\dots$, where $p$ is the probability of failure on any given cycle. The failure probability for components from supplier A is $p_A$, and for supplier B is $p_B$.\n\nIn a mixed inventory, the proportion of components from supplier A is $\\pi$, and the proportion from supplier B is $1-\\pi$. The engineer has a dataset of failure times $\\{x_1, x_2, \\ldots, x_n\\}$ for $n$ components drawn from this mixed inventory, but the supplier for each specific component is unknown.\n\nTo estimate the unknown parameters $\\theta = (\\pi, p_A, p_B)$, the engineer decides to use the Expectation-Maximization (EM) algorithm. The algorithm iteratively refines the parameter estimates. Let the estimates at iteration $t$ be $\\theta^{(t)} = (\\pi^{(t)}, p_A^{(t)}, p_B^{(t)})$.\n\nIn the E-step of the $(t+1)$-th iteration, one computes the \"responsibility\" that component A has for observation $x_i$, denoted by $\\gamma_i$. This is the posterior probability that the $i$-th component came from supplier A, given the observation $x_i$ and the current parameter estimates $\\theta^{(t)}$. This is given by:\n$$\n\\gamma_i = \\frac{\\pi^{(t)} p_A^{(t)}(1-p_A^{(t)})^{x_i-1}}{\\pi^{(t)} p_A^{(t)}(1-p_A^{(t)})^{x_i-1} + (1-\\pi^{(t)}) p_B^{(t)}(1-p_B^{(t)})^{x_i-1}}\n$$\nIn the M-step, one updates the parameters to maximize the expected complete-data log-likelihood. Your task is to derive the update rule for the parameter $p_A$.\n\nFind the expression for the updated estimate $p_A^{(t+1)}$ in terms of the observed data $\\{x_1, \\ldots, x_n\\}$ and the responsibilities $\\{\\gamma_1, \\ldots, \\gamma_n\\}$ computed in the E-step.", "solution": "Introduce latent indicators $z_{i}\\in\\{0,1\\}$ with $z_{i}=1$ if component $i$ is from supplier A and $z_{i}=0$ otherwise. The complete-data log-likelihood is\n$$\n\\ell_{c}(\\pi,p_{A},p_{B})=\\sum_{i=1}^{n}\\left[z_{i}\\left(\\ln \\pi+\\ln p_{A}+(x_{i}-1)\\ln(1-p_{A})\\right)+(1-z_{i})\\left(\\ln(1-\\pi)+\\ln p_{B}+(x_{i}-1)\\ln(1-p_{B})\\right)\\right].\n$$\nIn the E-step, replace $z_{i}$ by its conditional expectation $\\gamma_{i}=\\mathbb{E}[z_{i}\\mid x_{i},\\theta^{(t)}]$. The expected complete-data log-likelihood contribution that depends on $p_{A}$ is\n$$\nQ(p_{A})=\\sum_{i=1}^{n}\\gamma_{i}\\left(\\ln p_{A}+(x_{i}-1)\\ln(1-p_{A})\\right).\n$$\nDifferentiate with respect to $p_{A}$:\n$$\n\\frac{\\partial Q}{\\partial p_{A}}=\\sum_{i=1}^{n}\\gamma_{i}\\left(\\frac{1}{p_{A}}-\\frac{x_{i}-1}{1-p_{A}}\\right)=\\frac{1}{p_{A}}\\sum_{i=1}^{n}\\gamma_{i}-\\frac{1}{1-p_{A}}\\sum_{i=1}^{n}\\gamma_{i}(x_{i}-1).\n$$\nSet the derivative to zero and solve for $p_{A}$. Let $S_{1}=\\sum_{i=1}^{n}\\gamma_{i}$ and $S_{x}=\\sum_{i=1}^{n}\\gamma_{i}(x_{i}-1)$. Then\n$$\n\\frac{S_{1}}{p_{A}}-\\frac{S_{x}}{1-p_{A}}=0\\quad\\Longrightarrow\\quad S_{1}(1-p_{A})=S_{x}p_{A}\\quad\\Longrightarrow\\quad S_{1}=p_{A}(S_{1}+S_{x}).\n$$\nHence\n$$\np_{A}^{(t+1)}=\\frac{S_{1}}{S_{1}+S_{x}}.\n$$\nNoting that $S_{1}+S_{x}=\\sum_{i=1}^{n}\\gamma_{i}+\\sum_{i=1}^{n}\\gamma_{i}(x_{i}-1)=\\sum_{i=1}^{n}\\gamma_{i}x_{i}$, we obtain\n$$\np_{A}^{(t+1)}=\\frac{\\sum_{i=1}^{n}\\gamma_{i}}{\\sum_{i=1}^{n}\\gamma_{i}x_{i}}.\n$$\nThe second derivative is\n$$\n\\frac{\\partial^{2}Q}{\\partial p_{A}^{2}}=-\\sum_{i=1}^{n}\\gamma_{i}\\left(\\frac{1}{p_{A}^{2}}+\\frac{x_{i}-1}{(1-p_{A})^{2}}\\right)<0\\quad\\text{for }0<p_{A}<1,\n$$\nconfirming this is a maximizer.", "answer": "$$\\boxed{\\frac{\\sum_{i=1}^{n}\\gamma_{i}}{\\sum_{i=1}^{n}\\gamma_{i}x_{i}}}$$", "id": "1960170"}, {"introduction": "The EM algorithm is a powerful framework that can be extended beyond standard maximum likelihood estimation. This final practice demonstrates how to incorporate prior knowledge into the estimation process by adapting the M-step to perform Maximum a Posteriori (MAP) estimation. You will work with a mixture of Poisson distributions, a common model in fields like biophysics, and derive the update rule for the rate parameters when they are governed by Gamma prior distributions [@problem_id:1960196]. This exercise highlights the flexibility of the EM framework and its important connection to Bayesian inference, where prior beliefs are combined with observed data.", "problem": "A biophysicist is analyzing data from a single-molecule fluorescence experiment. The observed data consists of a series of $n$ independent photon counts, $X_1, X_2, \\dots, X_n$, recorded over consecutive time intervals. The molecule is known to stochastically switch between two distinct conformational states, State 1 and State 2. When in State $k$, the photon counts are modeled by a Poisson distribution with an unknown rate parameter $\\lambda_k$. The probability that any given measurement is from State 1 is a known constant $\\pi$, and from State 2 is $1-\\pi$.\n\nTo estimate the unknown rates $\\lambda_1$ and $\\lambda_2$, an iterative algorithm is used. Since prior knowledge about the system is available, a Bayesian approach is adopted. The goal is to find the Maximum a Posteriori (MAP) estimates of the parameters, not just the maximum likelihood estimates. The prior distributions for the rate parameters are chosen to be Gamma distributions, as they are conjugate to the Poisson likelihood. Specifically, the prior for $\\lambda_k$ is $\\text{Gamma}(\\alpha_k, \\beta_k)$, with the probability density function given by $p(\\lambda_k) \\propto \\lambda_k^{\\alpha_k-1} \\exp(-\\beta_k \\lambda_k)$ for $\\lambda_k > 0$.\n\nThe estimation is performed using a modified version of the Expectation-Maximization (EM) algorithm. The standard M-step, which maximizes the expected complete-data log-likelihood, is replaced with a step that maximizes the expected complete-data log-posterior.\n\nLet $\\theta^{(t)} = (\\lambda_1^{(t)}, \\lambda_2^{(t)})$ be the parameter estimates at iteration $t$. In the Expectation step (E-step), one computes the \"responsibilities,\" which are the posterior probabilities that observation $X_i$ belongs to State $k$, given the data and the current parameter estimates. Let's denote the responsibility for State 1 as $\\gamma_{i,1}^{(t)} = P(\\text{State}=1 | X_i, \\theta^{(t)})$.\n\nYour task is to derive the update equation for the parameter $\\lambda_1$ during the modified Maximization step (M-step). Express the updated estimate $\\lambda_1^{(t+1)}$ in terms of the observed data $\\{X_i\\}_{i=1}^n$, the responsibilities $\\{\\gamma_{i,1}^{(t)}\\}_{i=1}^n$, and the hyperparameters of the prior distribution for $\\lambda_1$, which are $\\alpha_1$ and $\\beta_1$.", "solution": "Introduce latent indicators $z_{i,1} \\in \\{0,1\\}$ with $z_{i,1}=1$ if observation $X_{i}$ comes from State 1 and $z_{i,1}=0$ otherwise. The complete-data log-likelihood terms involving $\\lambda_{1}$ are\n$$\n\\sum_{i=1}^{n} z_{i,1} \\left( X_{i} \\ln \\lambda_{1} - \\lambda_{1} \\right) + \\text{const},\n$$\nsince for a Poisson model $\\ln p(X_{i} \\mid \\lambda_{1}) = X_{i} \\ln \\lambda_{1} - \\lambda_{1} - \\ln X_{i}!.\n$\nThe log-prior for $\\lambda_{1} \\sim \\text{Gamma}(\\alpha_{1},\\beta_{1})$ (shape-rate) is\n$$\n(\\alpha_{1}-1)\\ln \\lambda_{1} - \\beta_{1} \\lambda_{1} + \\text{const}.\n$$\nIn the E-step, replace $z_{i,1}$ by its conditional expectation given current parameters, $\\gamma_{i,1}^{(t)} = P(\\text{State}=1 \\mid X_{i}, \\theta^{(t)})$. The expected complete-data log-posterior in $\\lambda_{1}$ is then\n$$\nQ(\\lambda_{1}) = \\left( \\alpha_{1}-1 + \\sum_{i=1}^{n} \\gamma_{i,1}^{(t)} X_{i} \\right) \\ln \\lambda_{1} - \\left( \\beta_{1} + \\sum_{i=1}^{n} \\gamma_{i,1}^{(t)} \\right) \\lambda_{1} + \\text{const}.\n$$\nDifferentiate with respect to $\\lambda_{1}$, set to zero, and solve:\n$$\n\\frac{\\partial Q}{\\partial \\lambda_{1}} = \\frac{\\alpha_{1}-1 + \\sum_{i=1}^{n} \\gamma_{i,1}^{(t)} X_{i}}{\\lambda_{1}} - \\left( \\beta_{1} + \\sum_{i=1}^{n} \\gamma_{i,1}^{(t)} \\right) = 0\n$$\nwhich yields\n$$\n\\lambda_{1}^{(t+1)} = \\frac{\\alpha_{1}-1 + \\sum_{i=1}^{n} \\gamma_{i,1}^{(t)} X_{i}}{\\beta_{1} + \\sum_{i=1}^{n} \\gamma_{i,1}^{(t)}}.\n$$\nThe second derivative is\n$$\n\\frac{\\partial^{2} Q}{\\partial \\lambda_{1}^{2}} = -\\frac{\\alpha_{1}-1 + \\sum_{i=1}^{n} \\gamma_{i,1}^{(t)} X_{i}}{\\lambda_{1}^{2}},\n$$\nwhich is negative whenever the numerator is positive, confirming a maximum under the usual conditions.", "answer": "$$\\boxed{\\frac{\\alpha_{1}-1+\\sum_{i=1}^{n}\\gamma_{i,1}^{(t)} X_{i}}{\\beta_{1}+\\sum_{i=1}^{n}\\gamma_{i,1}^{(t)}}}$$", "id": "1960196"}]}