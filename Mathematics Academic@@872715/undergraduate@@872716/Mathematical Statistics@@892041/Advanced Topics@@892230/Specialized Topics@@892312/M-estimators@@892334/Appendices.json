{"hands_on_practices": [{"introduction": "The concept of an M-estimator can seem abstract at first, but it often generalizes familiar statistical ideas. This first practice problem serves as a bridge, demonstrating that the well-known sample median is, in fact, a foundational type of M-estimator. By working through this exercise [@problem_id:1932002], you will see how minimizing the sum of absolute deviations, defined by the function $\\rho(u) = |u|$, leads directly to the median, providing a concrete entry point into the world of robust estimation.", "problem": "In robust statistics, M-estimators provide a general framework for defining estimators of location. For a given data sample $\\{x_1, x_2, \\dots, x_n\\}$ and a chosen convex function $\\rho$, the M-estimator of location, denoted as $\\hat{\\theta}$, is the value of $\\theta$ that minimizes the objective function $S(\\theta) = \\sum_{i=1}^n \\rho(x_i - \\theta)$.\n\nConsider an M-estimator defined by the absolute value function, $\\rho(u) = |u|$. Calculate the value of this M-estimator for the data sample $X = \\{2.5, 9.8, 15.1, 4.0, 21.3\\}$. Express your answer as a single number.", "solution": "We are given the objective function for the M-estimator with $\\rho(u)=|u|$ as\n$$\nS(\\theta)=\\sum_{i=1}^{n} |x_{i}-\\theta|.\n$$\nSince $|\\,\\cdot\\,|$ is convex, $S(\\theta)$ is convex in $\\theta$. A necessary and sufficient condition for $\\hat{\\theta}$ to minimize $S(\\theta)$ is the subgradient optimality condition\n$$\n0 \\in \\partial S(\\theta)=\\sum_{i=1}^{n} \\partial |x_{i}-\\theta|.\n$$\nFor each term, the subgradient with respect to $\\theta$ is\n$$\n\\partial |x_{i}-\\theta|=\n\\begin{cases}\n\\{-1\\},  \\theta  x_{i},\\\\\n[-1,1],  \\theta = x_{i},\\\\\n\\{1\\},  \\theta  x_{i}.\n\\end{cases}\n$$\nLet $L(\\theta)$ be the number of $x_{i}$ strictly less than $\\theta$, $G(\\theta)$ the number strictly greater than $\\theta$, and $E(\\theta)$ the number equal to $\\theta$. Then the subgradient sum is an interval\n$$\n\\partial S(\\theta)=\\left[\\, -L(\\theta)+G(\\theta)-E(\\theta),\\; -L(\\theta)+G(\\theta)+E(\\theta)\\,\\right].\n$$\nThe condition $0 \\in \\partial S(\\theta)$ is equivalent to\n$$\n-L(\\theta)+G(\\theta) \\in [-E(\\theta),\\,E(\\theta)],\n$$\nwhich in turn is equivalent to the pair of inequalities\n$$\nL(\\theta) \\leq \\frac{n}{2} \\quad \\text{and} \\quad G(\\theta) \\leq \\frac{n}{2}.\n$$\nThus any minimizer $\\hat{\\theta}$ must be a median of the sample. For an odd sample size $n$ with all distinct values, the unique minimizer is the sample median, i.e., the middle order statistic.\n\nFor the data $X=\\{2.5, 9.8, 15.1, 4.0, 21.3\\}$, sort the values to obtain\n$$\n2.5  4.0  9.8  15.1  21.3.\n$$\nWith $n=5$, the median is the third order statistic, which is $9.8$. Therefore, the M-estimator defined by $\\rho(u)=|u|$ is\n$$\n\\hat{\\theta}=9.8.\n$$", "answer": "$$\\boxed{9.8}$$", "id": "1932002"}, {"introduction": "While the median offers strong protection against outliers, it can be inefficient for less contaminated data. The Huber M-estimator provides a popular and practical compromise, behaving like the mean for small errors and like the median for large ones. This exercise [@problem_id:1952425] gives you hands-on experience calculating a Huber estimate, requiring you to apply its 'clipping' influence function $\\psi(u)$ to see how it systematically down-weights an outlier's influence.", "problem": "In robust statistics, M-estimators are a broad class of estimators that are robust to outliers. An M-estimator of location, denoted by $\\hat{\\theta}$, for a dataset $\\{x_1, x_2, \\ldots, x_n\\}$ is defined as the solution to the equation:\n$$\n\\sum_{i=1}^{n} \\psi\\left(\\frac{x_i - \\theta}{S}\\right) = 0\n$$\nwhere $S$ is a robust estimate of scale and $\\psi$ is a chosen influence function.\n\nConsider Huber's $\\psi$-function, defined for a given tuning constant $k  0$ as:\n$$\n\\psi_k(u) = \\begin{cases}\n-k  \\text{if } u  -k \\\\\nu  \\text{if } |u| \\leq k \\\\\nk  \\text{if } u  k\n\\end{cases}\n$$\nThis function limits the influence of large residuals by \"clipping\" them at $\\pm k$.\n\nAn engineer is analyzing a set of five independent measurements of a component's length. The measurements obtained are:\n$$\n\\{2.1, 2.5, 2.3, 2.6, 4.5\\}\n$$\nTo obtain a location estimate that is not overly sensitive to the single large value, the engineer decides to use an M-estimator. For the calculation, a fixed scale estimate of $S=1$ and a tuning constant of $k=1.5$ are to be used.\n\nCalculate the M-estimate of location, $\\hat{\\theta}$, for this dataset. Report your answer rounded to three significant figures.", "solution": "We seek $\\hat{\\theta}$ solving the M-estimating equation\n$$\n\\sum_{i=1}^{5} \\psi_{k}\\!\\left(\\frac{x_{i}-\\theta}{S}\\right)=0,\n$$\nwith $S=1$, $k=1.5$, and $x=\\{2.1,2.5,2.3,2.6,4.5\\}$. With $S=1$, this becomes\n$$\n\\sum_{i=1}^{5} \\psi_{k}(x_{i}-\\theta)=0,\n$$\nwhere Huberâ€™s function is $\\psi_{k}(u)=u$ if $|u|\\leq k$ and $\\psi_{k}(u)=\\pm k$ if $|u|k$, with the sign of $u$.\n\nFor each $x_{i}$, the term is linear (i.e., unclipped) when $|x_{i}-\\theta|\\leq 1.5$, equivalently $\\theta\\in[x_{i}-1.5,\\,x_{i}+1.5]$. These intervals are:\n- $x_{1}=2.1$: $\\theta\\in[0.6,\\,3.6]$,\n- $x_{2}=2.5$: $\\theta\\in[1.0,\\,4.0]$,\n- $x_{3}=2.3$: $\\theta\\in[0.8,\\,3.8]$,\n- $x_{4}=2.6$: $\\theta\\in[1.1,\\,4.1]$,\n- $x_{5}=4.5$: $\\theta\\in[3.0,\\,6.0]$.\n\nConsider $\\theta\\in[1.1,3.0)$, where the first four terms are linear and the fifth is clipped at $+1.5$ because $4.5-\\theta1.5$. Then\n$$\n\\sum_{i=1}^{5} \\psi_{k}(x_{i}-\\theta)=\\sum_{i=1}^{4} (x_{i}-\\theta) + 1.5.\n$$\nUsing $\\sum_{i=1}^{4} x_{i}=2.1+2.5+2.3+2.6=9.5$, we get\n$$\n9.5 - 4\\theta + 1.5 = 0 \\;\\;\\Longrightarrow\\;\\; 11 - 4\\theta = 0 \\;\\;\\Longrightarrow\\;\\; \\hat{\\theta}=\\frac{11}{4}=2.75.\n$$\nCheck the regime consistency at $\\hat{\\theta}=2.75$:\n- For $x_{1},x_{2},x_{3},x_{4}$, $|x_{i}-\\hat{\\theta}|\\in\\{0.65,0.25,0.45,0.15\\}\\leq 1.5$, so these are correctly treated as linear.\n- For $x_{5}$, $|4.5-2.75|=1.751.5$, so it is correctly clipped to $+1.5$.\n\nThus the solution is valid and unique in this region. Rounding to three significant figures yields $2.75$.", "answer": "$$\\boxed{2.75}$$", "id": "1952425"}, {"introduction": "The true power of the M-estimator framework lies in its flexibility to define new estimators tailored to specific needs. This problem [@problem_id:1931982] moves beyond standard choices and asks you to work with a novel objective function, $\\rho(u) = \\cosh(u) - 1$. To find the solution, you will need to return to first principles: derive the estimating equation by differentiating the objective function and then solve for the location parameter, thereby reinforcing your understanding of how M-estimators are constructed from the ground up.", "problem": "In mathematical statistics, M-estimators provide a general class of robust estimators for a location parameter $\\theta$. For a set of observations $x_1, x_2, \\ldots, x_n$, the M-estimate $\\hat{\\theta}$ is the value of $\\theta$ that minimizes the objective function $S(\\theta) = \\sum_{i=1}^n \\rho(x_i - \\theta)$, where $\\rho$ is a given real-valued function. For the resulting estimate to have desirable statistical properties, the function $\\rho(u)$ is typically chosen to be a symmetric (even) function that is convex and has a unique minimum at $u=0$. A common choice, $\\rho(u) = \\frac{1}{2}u^2$, leads to the sample mean as the M-estimate.\n\nConsider a novel M-estimator defined by the function $\\rho(u) = \\cosh(u) - 1$.\nGiven the small dataset of three observations $\\{-\\ln(2), \\ln(2), \\ln(5)\\}$, determine the exact value of the M-estimate $\\hat{\\theta}$ for the location parameter based on this specific $\\rho$ function.", "solution": "We are given the M-estimator defined by minimizing $S(\\theta)=\\sum_{i=1}^{n}\\rho(x_{i}-\\theta)$ with $\\rho(u)=\\cosh(u)-1$. Since $\\cosh$ is convex and has a unique minimum at $u=0$, $S(\\theta)$ is strictly convex in $\\theta$, so the minimizer is unique and characterized by the first-order condition.\n\nCompute the derivative:\n$$\n\\frac{d}{d\\theta}\\rho(x_{i}-\\theta)=-\\rho'(x_{i}-\\theta)=-\\sinh(x_{i}-\\theta).\n$$\nThus,\n$$\nS'(\\theta)=-\\sum_{i=1}^{n}\\sinh(x_{i}-\\theta).\n$$\nThe estimating equation is therefore\n$$\n\\sum_{i=1}^{n}\\sinh(x_{i}-\\theta)=0.\n$$\nUsing $\\sinh(y)=\\frac{\\exp(y)-\\exp(-y)}{2}$, we obtain\n$$\n\\sum_{i=1}^{n}\\left(\\frac{\\exp(x_{i}-\\theta)-\\exp(\\theta-x_{i})}{2}\\right)=0\n\\;\\Longleftrightarrow\\;\n\\sum_{i=1}^{n}\\left(\\frac{\\exp(x_{i})}{\\exp(\\theta)}-\\frac{\\exp(\\theta)}{\\exp(x_{i})}\\right)=0.\n$$\nLet $T=\\exp(\\theta)$. Then the equation becomes\n$$\n\\frac{1}{T}\\sum_{i=1}^{n}\\exp(x_{i})-T\\sum_{i=1}^{n}\\exp(-x_{i})=0\n\\;\\Longleftrightarrow\\;\n\\sum_{i=1}^{n}\\exp(x_{i})-T^{2}\\sum_{i=1}^{n}\\exp(-x_{i})=0,\n$$\nso\n$$\nT^{2}=\\frac{\\sum_{i=1}^{n}\\exp(x_{i})}{\\sum_{i=1}^{n}\\exp(-x_{i})}\n\\quad\\Longrightarrow\\quad\n\\theta=\\frac{1}{2}\\ln\\!\\left(\\frac{\\sum_{i=1}^{n}\\exp(x_{i})}{\\sum_{i=1}^{n}\\exp(-x_{i})}\\right).\n$$\n\nFor the data $\\{-\\ln(2),\\,\\ln(2),\\,\\ln(5)\\}$, compute\n$$\n\\sum_{i=1}^{3}\\exp(x_{i})=\\exp(-\\ln 2)+\\exp(\\ln 2)+\\exp(\\ln 5)=\\frac{1}{2}+2+5=\\frac{15}{2},\n$$\n$$\n\\sum_{i=1}^{3}\\exp(-x_{i})=\\exp(\\ln 2)+\\exp(-\\ln 2)+\\exp(-\\ln 5)=2+\\frac{1}{2}+\\frac{1}{5}=\\frac{27}{10}.\n$$\nHence,\n$$\n\\frac{\\sum_{i=1}^{3}\\exp(x_{i})}{\\sum_{i=1}^{3}\\exp(-x_{i})}\n=\\frac{\\frac{15}{2}}{\\frac{27}{10}}\n=\\frac{15}{2}\\cdot\\frac{10}{27}\n=\\frac{150}{54}\n=\\frac{25}{9},\n$$\nand therefore\n$$\n\\hat{\\theta}=\\frac{1}{2}\\ln\\!\\left(\\frac{25}{9}\\right)=\\ln\\!\\left(\\frac{5}{3}\\right).\n$$\nBy strict convexity, this is the unique minimizer.", "answer": "$$\\boxed{\\ln\\!\\left(\\frac{5}{3}\\right)}$$", "id": "1931982"}]}