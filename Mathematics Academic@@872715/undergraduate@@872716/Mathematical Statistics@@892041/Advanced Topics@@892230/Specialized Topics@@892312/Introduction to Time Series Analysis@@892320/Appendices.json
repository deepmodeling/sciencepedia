{"hands_on_practices": [{"introduction": "A cornerstone of time series analysis is the concept of stationarity, which implies that a process's statistical properties, like its mean and variance, do not change over time. For autoregressive (AR) models, stationarity is determined by the model's coefficients. This practice provides a direct application of the standard procedure for checking stationarity, a critical first step in model validation that involves finding the roots of the characteristic polynomial. [@problem_id:1312127]", "problem": "An analyst is modeling a financial time series, denoted by a process $\\{X_t\\}$, using an Autoregressive (AR) model of order 2. An AR(2) process is generally defined by the equation $X_t = c + \\phi_1 X_{t-1} + \\phi_2 X_{t-2} + Z_t$, where $Z_t$ is a white noise process with zero mean. For this particular model, the constant term $c$ is found to be zero, and the model is given by:\n\n$$X_t = X_{t-1} - 0.25 X_{t-2} + Z_t$$\n\nThe stability and properties of this model are determined by its characteristic polynomial, $P(z)$, which is derived from the autoregressive coefficients. A key property is stationarity, which requires all roots of the characteristic equation $P(z)=0$ to lie outside the unit circle in the complex plane (i.e., the magnitude of every root must be greater than 1).\n\nGiven the model, determine its characteristic polynomial and whether the process is stationary. Which of the following statements is correct?\n\nA. The characteristic polynomial is $P(z) = 1 - z + 0.25z^2$, and the process is stationary.\n\nB. The characteristic polynomial is $P(z) = 1 - z + 0.25z^2$, and the process is not stationary.\n\nC. The characteristic polynomial is $P(z) = 1 + z - 0.25z^2$, and the process is not stationary.\n\nD. The characteristic polynomial is $P(z) = z^2 - z + 0.25$, and the process is not stationary.", "solution": "For an AR(2) process $X_{t}=\\phi_{1}X_{t-1}+\\phi_{2}X_{t-2}+Z_{t}$, the autoregressive polynomial in the lag operator $B$ is $\\phi(B)=1-\\phi_{1}B-\\phi_{2}B^{2}$, obtained by rearranging to $\\phi(B)X_{t}=Z_{t}$. The characteristic polynomial in the complex variable $z$ is then $P(z)=1-\\phi_{1}z-\\phi_{2}z^{2}$.\n\nIn the given model $X_{t}=X_{t-1}-0.25\\,X_{t-2}+Z_{t}$, we identify $\\phi_{1}=1$ and $\\phi_{2}=-0.25$. Therefore,\n$$\nP(z)=1-\\phi_{1}z-\\phi_{2}z^{2}=1-z+0.25\\,z^{2}.\n$$\n\nTo assess stationarity, we solve $P(z)=0$ and require all roots to satisfy $|z|1$. Solve\n$$\n0.25\\,z^{2}-z+1=0.\n$$\nWith $a=0.25$, $b=-1$, $c=1$, the discriminant is\n$$\n\\Delta=b^{2}-4ac=1-4\\cdot 0.25\\cdot 1=0,\n$$\nso there is a repeated root\n$$\nz=\\frac{-b}{2a}=\\frac{1}{0.5}=2.\n$$\nSince $|2|1$, all roots lie outside the unit circle, and the process is stationary.\n\nThus the correct statement is that $P(z)=1-z+0.25z^{2}$ and the process is stationary.", "answer": "$$\\boxed{A}$$", "id": "1312127"}, {"introduction": "After establishing that a process is stationary, the next step is to characterize its internal dependency structure. The primary tool for this is the autocorrelation function (ACF), which measures the correlation between observations as a function of the time lag separating them. This exercise challenges you to derive the ACF from first principles for a specific moving average (MA) process, reinforcing the fundamental definitions of autocovariance and autocorrelation and illustrating how a model's structure directly dictates its correlation signature. [@problem_id:1925260]", "problem": "In a signal processing application, a sensor takes a series of measurements, denoted by $\\{X_t\\}$, at discrete integer time steps $t$. The measured value at any given time $t$ is modeled as a sum of a primary random signal and a delayed, attenuated echo of a past signal. Specifically, the process is described by the equation:\n$$X_t = W_t + \\alpha W_{t-2}$$\nHere, $\\{W_t\\}$ represents a white noise process, which is a sequence of uncorrelated random variables, each with an expected value of 0 and a constant, finite variance of $\\sigma_W^2$. The parameter $\\alpha$ is a real-valued constant that represents the attenuation factor of the echo. For this particular sensor, the attenuation factor is given as $\\alpha = 0.5$.\n\nCalculate the theoretical autocorrelation of the process $\\{X_t\\}$ at lag 2.", "solution": "We are given a white noise process $\\{W_t\\}$ with $\\mathbb{E}[W_t]=0$, $\\operatorname{Var}(W_t)=\\sigma_{W}^{2}$, and $\\mathbb{E}[W_t W_s]=0$ for $t\\neq s$. The observed process is $X_t=W_t+\\alpha W_{t-2}$ with $\\alpha=0.5$.\n\nThe theoretical autocorrelation at lag $h$ is defined as $\\rho_{X}(h)=\\frac{\\gamma_{X}(h)}{\\gamma_{X}(0)}$, where $\\gamma_{X}(h)=\\mathbb{E}[X_t X_{t-h}]$ since the mean is zero.\n\nFirst, compute the autocovariance at lag $2$:\n$$\n\\gamma_{X}(2)=\\mathbb{E}[X_t X_{t-2}]\n=\\mathbb{E}\\big[(W_t+\\alpha W_{t-2})(W_{t-2}+\\alpha W_{t-4})\\big].\n$$\nExpanding and using the white noise properties,\n$$\n\\gamma_{X}(2)=\\mathbb{E}[W_t W_{t-2}]+\\alpha \\mathbb{E}[W_t W_{t-4}]+\\alpha \\mathbb{E}[W_{t-2} W_{t-2}]+\\alpha^{2}\\mathbb{E}[W_{t-2} W_{t-4}]\n=\\alpha \\sigma_{W}^{2},\n$$\nsince only the term with matching time indices survives.\n\nNext, compute the variance $\\gamma_{X}(0)=\\operatorname{Var}(X_t)$:\n$$\n\\gamma_{X}(0)=\\operatorname{Var}(W_t+\\alpha W_{t-2})\n=\\operatorname{Var}(W_t)+\\alpha^{2}\\operatorname{Var}(W_{t-2})+2\\operatorname{Cov}(W_t,\\alpha W_{t-2})\n=(1+\\alpha^{2})\\sigma_{W}^{2},\n$$\nbecause $\\operatorname{Cov}(W_t,W_{t-2})=0$.\n\nTherefore, the theoretical autocorrelation at lag $2$ is\n$$\n\\rho_{X}(2)=\\frac{\\gamma_{X}(2)}{\\gamma_{X}(0)}=\\frac{\\alpha \\sigma_{W}^{2}}{(1+\\alpha^{2})\\sigma_{W}^{2}}=\\frac{\\alpha}{1+\\alpha^{2}}.\n$$\nSubstituting $\\alpha=0.5$ gives\n$$\n\\rho_{X}(2)=\\frac{0.5}{1+0.25}=\\frac{2}{5}.\n$$", "answer": "$$\\boxed{\\frac{2}{5}}$$", "id": "1925260"}, {"introduction": "By combining autoregressive and moving average components, we can construct powerful and flexible ARMA models. However, it is crucial to ensure that these models are as simple as possible, a principle known as parsimony. This exercise explores a special case where the AR and MA components effectively cancel each other out, making a seemingly complex ARMA(1,1) process equivalent to simple white noise. This demonstrates the important concept of model redundancy and highlights the need for careful model specification in practice. [@problem_id:1312141]", "problem": "Consider a weakly stationary Autoregressive-Moving-Average, or ARMA(1,1), process $\\{X_t\\}$ defined by the equation:\n$$X_t - \\phi X_{t-1} = Z_t + \\theta Z_{t-1}$$\nwhere $\\{Z_t\\}$ is a white noise process with mean zero and constant variance $\\sigma_Z^2$, and $t$ is an integer index representing time. The parameters $\\phi$ and $\\theta$ are real constants.\n\nIn a particular case, the moving-average parameter is set to be the negative of the autoregressive parameter, so that $\\theta = -\\phi$. The process is thus described by:\n$$X_t - \\phi X_{t-1} = Z_t - \\phi Z_{t-1}$$\n\nGiven that $|\\phi| \\ne 1$, determine the autocovariance function, $\\gamma_X(h) = \\text{Cov}(X_t, X_{t-h})$, for this process. Express your answer as a piecewise function of the integer lag $h$, in terms of $\\sigma_Z^2$.", "solution": "Start from the given ARMA(1,1) model with $\\theta=-\\phi$:\n$$(1-\\phi B)X_t=(1-\\phi B)Z_t,$$\nwhere $B$ is the backshift operator defined by $BX_t=X_{t-1}$ and $\\{Z_t\\}$ is white noise with mean zero and variance $\\sigma_Z^{2}$.\n\nDefine $Y_t=X_t-Z_t$. Subtracting the right-hand side from the left-hand side gives the homogeneous equation\n$$(1-\\phi B)Y_t=0.$$\nEquivalently, in the time domain,\n$$Y_t=\\phi Y_{t-1}\\quad\\text{for all }t.$$\n\nAssuming weak stationarity of $\\{Y_t\\}$ and $|\\phi|\\ne 1$, take variances on both sides to get\n$$\\operatorname{Var}(Y_t)=\\operatorname{Var}(\\phi Y_{t-1})=\\phi^{2}\\operatorname{Var}(Y_{t-1}).$$\nStationarity implies $\\operatorname{Var}(Y_t)=\\operatorname{Var}(Y_{t-1})=:v$, hence\n$$(1-\\phi^{2})v=0.$$\nGiven $|\\phi|\\ne 1$, it follows that $v=0$, so $Y_t=0$ almost surely for all $t$. Therefore,\n$$X_t=Z_t\\quad\\text{for all }t.$$\n\nUsing the definition of the autocovariance function and the white-noise property,\n$$\\gamma_X(h)=\\operatorname{Cov}(X_t,X_{t-h})=\\operatorname{Cov}(Z_t,Z_{t-h})=\\begin{cases}\n\\sigma_Z^{2},  h=0,\\\\\n0,  h\\ne 0,\n\\end{cases}$$\nwhich is the desired piecewise form.", "answer": "$$\\boxed{\\gamma_X(h)=\\begin{cases}\\sigma_Z^{2},  h=0,\\\\ 0,  h\\neq 0.\\end{cases}}$$", "id": "1312141"}]}