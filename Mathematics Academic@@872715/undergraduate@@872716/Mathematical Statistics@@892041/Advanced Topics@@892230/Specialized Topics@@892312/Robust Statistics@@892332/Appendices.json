{"hands_on_practices": [{"introduction": "Our journey into robust statistics begins with a practical method for handling outliers without completely discarding them. The Winsorized mean offers a compromise between the sensitive sample mean and the more drastic trimmed mean. This exercise [@problem_id:1952397] will guide you through the process of 'Winsorizing' a dataset, where you replace a certain percentage of the smallest and largest values with their less extreme neighbors, providing a more stable estimate of the central tendency.", "problem": "In a data analysis course, a professor is evaluating the performance of students on a final exam. The professor suspects that the traditional arithmetic mean of the scores is being skewed by a few exceptionally low scores from students who did not prepare, as well as a few exceptionally high scores from students with prior professional experience. To get a more robust measure of the class's central tendency, the professor decides to compute a Winsorized mean.\n\nThe Winsorized mean is a robust statistical measure of central tendency. A $p\\%$ Winsorized mean is calculated as follows: First, sort the data from smallest to largest. Let $n$ be the number of data points. Calculate the number of points to be adjusted on each end of the distribution, $k$, which is the largest integer less than or equal to $n \\times (p/100)$. The $k$ smallest data points are then replaced by the $(k+1)$-th smallest data point. Similarly, the $k$ largest data points are replaced by the $(k+1)$-th largest data point (which is also the $(n-k)$-th data point in the sorted list). The arithmetic mean of this newly formed dataset is the Winsorized mean.\n\nGiven the following fifteen exam scores (out of 100):\n$$ \\{ 70, 62, 99, 58, 78, 12, 65, 81, 71, 25, 68, 98, 73, 31, 75 \\} $$\n\nCalculate the 20% Winsorized mean of these scores. Round your final answer to four significant figures.", "solution": "A Winsorized mean at proportion $p$ uses $k=\\lfloor n p \\rfloor$ replacements at each tail after sorting the data. With $n=15$ and $p=0.20$, we have\n$$\nk=\\lfloor 15 \\cdot 0.20 \\rfloor=\\lfloor 3 \\rfloor=3.\n$$\nSorting the scores ascending gives\n$$\n\\{12,25,31,58,62,65,68,70,71,73,75,78,81,98,99\\}.\n$$\nReplace the $k=3$ smallest values $\\{12,25,31\\}$ by the $(k+1)$-th smallest value $x_{(4)}=58$, and the $k=3$ largest values $\\{81,98,99\\}$ by the $(n-k)$-th value $x_{(12)}=78$. The Winsorized dataset is thus\n$$\n\\{58,58,58,58,62,65,68,70,71,73,75,78,78,78,78\\}.\n$$\nThe sum of this dataset is calculated as:\n$$\nS_{W} = (4 \\times 58) + (62+65+68+70+71+73+75) + (4 \\times 78) = 232 + 484 + 312 = 1028.\n$$\nThe $20\\%$ Winsorized mean (with $p=0.20$) is\n$$\n\\bar{x}_{W}=\\frac{S_{W}}{n}=\\frac{1028}{15} \\approx 68.5333.\n$$\nRounding to four significant figures gives $68.53$.", "answer": "$$\\boxed{68.53}$$", "id": "1952397"}, {"introduction": "While the mean is sensitive to outliers, so is its counterpart for measuring spread: the standard deviation. A single extreme value can inflate the standard deviation, giving a misleading picture of the data's variability. This practice problem [@problem_id:1952404] introduces a robust alternative, the Median Absolute Deviation (MAD), and challenges you to identify datasets where the standard deviation dramatically overestimates the dispersion compared to the MAD. This skill is crucial for diagnosing the influence of outliers.", "problem": "In robust statistics, one often compares the sample standard deviation with alternative measures of statistical dispersion that are less sensitive to outliers. One such robust measure is the Median Absolute Deviation (MAD). For a dataset $\\{x_1, x_2, \\dots, x_n\\}$, the sample standard deviation is given by $s = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n}(x_i - \\bar{x})^2}$, where $\\bar{x}$ is the sample mean. The MAD is defined as the median of the absolute deviations from the data's median: $\\text{MAD} = \\text{median}(|x_i - M|)$, where $M = \\text{median}(\\{x_1, x_2, \\dots, x_n\\})$.\n\nA dataset is considered to have a significant outlier problem if its sample standard deviation is much larger than its MAD. Consider the following datasets, each with five data points:\n\nA. $\\{10, 11, 12, 13, 100\\}$\n\nB. $\\{10, 11, 12, 13, 14\\}$\n\nC. $\\{-50, 0, 1, 2, 50\\}$\n\nD. $\\{1, 3, 5, 7, 9\\}$\n\nWhich of these datasets satisfy the condition that the sample standard deviation is more than ten times the Median Absolute Deviation (i.e., $s > 10 \\times \\text{MAD}$)? Select all that apply. If you find more than one such dataset, concatenate the corresponding letters in alphabetical order for your answer (e.g., BD).", "solution": "We must test, for each dataset, whether $s > 10 \\times \\text{MAD}$. Since $s \\geq 0$ and $\\text{MAD} \\geq 0$, this is equivalent to checking $s^{2} > 100 \\times \\text{MAD}^{2}$. For a dataset $\\{x_{1},\\dots,x_{n}\\}$ with $n=5$, the sample variance is\n$$\ns^{2}=\\frac{1}{n-1}\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2},\\quad \\bar{x}=\\frac{1}{n}\\sum_{i=1}^{n}x_{i},\n$$\nand $\\text{MAD}=\\text{median}(|x_{i}-M|)$ with $M=\\text{median}(\\{x_{i}\\})$.\n\nDataset A: $\\{10,11,12,13,100\\}$. Compute $\\bar{x}=\\frac{10+11+12+13+100}{5}=\\frac{146}{5}$. Then\n$$\n\\sum_{i=1}^{5}(x_{i}-\\bar{x})^{2}=\\frac{156770}{25}\\quad\\Rightarrow\\quad s^{2}=\\frac{1}{4}\\cdot\\frac{156770}{25}=\\frac{156770}{100}=\\frac{15677}{10}.\n$$\nThe median is $M=12$, so the absolute deviations are $\\{2,1,0,1,88\\}$, whose median is $1$, hence $\\text{MAD}=1$. Check:\n$$\ns^{2}=\\frac{15677}{10}>100=100\\cdot \\text{MAD}^{2}\\quad\\Rightarrow\\quad s>10\\cdot \\text{MAD}.\n$$\nThus A satisfies the condition.\n\nDataset B: $\\{10,11,12,13,14\\}$. Compute $\\bar{x}=12$. Then\n$$\n\\sum_{i=1}^{5}(x_{i}-\\bar{x})^{2}=10\\quad\\Rightarrow\\quad s^{2}=\\frac{10}{4}=\\frac{5}{2}.\n$$\nMedian $M=12$, absolute deviations $\\{2,1,0,1,2\\}$ with median $1$, so $\\text{MAD}=1$. Check:\n$$\ns^{2}=\\frac{5}{2}<100=100\\cdot \\text{MAD}^{2},\n$$\nso B does not satisfy the condition.\n\nDataset C: $\\{-50,0,1,2,50\\}$. Compute $\\bar{x}=\\frac{-50+0+1+2+50}{5}=\\frac{3}{5}$. Then\n$$\n\\sum_{i=1}^{5}(x_{i}-\\bar{x})^{2}=\\frac{125080}{25}\\quad\\Rightarrow\\quad s^{2}=\\frac{1}{4}\\cdot\\frac{125080}{25}=\\frac{6254}{5}.\n$$\nMedian $M=1$, absolute deviations $\\{51,1,0,1,49\\}$ with median $1$, so $\\text{MAD}=1$. Check:\n$$\ns^{2}=\\frac{6254}{5}>100=100\\cdot \\text{MAD}^{2},\n$$\nso C satisfies the condition.\n\nDataset D: $\\{1,3,5,7,9\\}$. Compute $\\bar{x}=5$. Then\n$$\n\\sum_{i=1}^{5}(x_{i}-\\bar{x})^{2}=40\\quad\\Rightarrow\\quad s^{2}=\\frac{40}{4}=10.\n$$\nMedian $M=5$, absolute deviations $\\{4,2,0,2,4\\}$ with median $2$, so $\\text{MAD}=2$. Check:\n$$\ns^{2}=10<100\\cdot 2^{2}=400,\n$$\nso D does not satisfy the condition.\n\nTherefore, the datasets that satisfy $s>10\\times \\text{MAD}$ are A and C.", "answer": "$$\\boxed{AC}$$", "id": "1952404"}, {"introduction": "We now combine our understanding of robust location and scale estimators into a powerful, real-world algorithm: Iteratively Reweighted Least Squares (IRLS). This method refines an estimate by systematically reducing the influence of outliers in successive steps. In this exercise [@problem_id:1952412], you will perform one full iteration of the IRLS algorithm, using the Median Absolute Deviation (MAD) to gauge the 'outlyingness' of each data point and Huber's weight function to gently down-weight extreme values. This practice provides a window into the sophisticated machinery that makes modern robust statistics so effective.", "problem": "A scientist is analyzing a small dataset of five measurements of a physical quantity: $X = \\{10.1, 10.3, 9.9, 10.2, 15.8\\}$. To mitigate the effect of a potential outlier, the scientist opts for a robust estimation method called Iteratively Reweighted Least Squares (IRLS) to determine the central location, $\\mu$.\n\nThe IRLS procedure updates the location estimate in each step using a weighted average: $\\mu_{t+1} = \\frac{\\sum w_i x_i}{\\sum w_i}$. The weights, $w_i$, are determined by Huber's weight function applied to standardized residuals from the previous estimate, $\\mu_t$.\n\nThe standardized residual for the $i$-th data point is defined as $u_i = (x_i - \\mu_t)/S$, where $S$ is a robust estimate of scale. For this problem, the scale $S$ is calculated as $S = c \\times \\text{MAD}$, where MAD stands for the Median Absolute Deviation of the residuals, $r_i = x_i - \\mu_t$. The MAD is defined as $\\text{MAD} = \\text{median}\\{|r_i - \\text{median}(r_i)|\\}$. The constant $c$ is given as $1.4826$.\n\nHuber's weight function is given by:\n$$w(u) = \\begin{cases} 1 & \\text{if } |u| \\le k \\\\ k/|u| & \\text{if } |u| > k \\end{cases}$$\nFor this analysis, the tuning constant is set to $k = 1.345$.\n\nThe initial estimate for the location, $\\mu_0$, is defined as the sample mean of the dataset $X$.\n\nCalculate the updated location estimate, $\\mu_1$, after one complete iteration of the IRLS algorithm. Round your final answer to four significant figures.", "solution": "We are given the dataset $X=\\{10.1, 10.3, 9.9, 10.2, 15.8\\}$ and the IRLS update\n$$\n\\mu_{t+1}=\\frac{\\sum_{i=1}^{5} w_{i} x_{i}}{\\sum_{i=1}^{5} w_{i}},\n$$\nwith weights from Huberâ€™s function based on standardized residuals $u_{i}=(x_{i}-\\mu_{t})/S$, where $S=c\\times \\text{MAD}$, $c=1.4826$, and $k=1.345$.\n\nStep 1: Initialize with the sample mean\n$$\n\\mu_{0}=\\frac{10.1+10.3+9.9+10.2+15.8}{5}=\\frac{56.3}{5}=11.26.\n$$\n\nStep 2: Compute residuals $r_{i}=x_{i}-\\mu_{0}$:\n$$\nr=\\{-1.16,\\,-0.96,\\,-1.36,\\,-1.06,\\,4.54\\}.\n$$\nThe median of $r$ is $\\operatorname{median}(r)=-1.06$.\n\nStep 3: Compute the MAD as\n$$\n\\text{MAD}=\\operatorname{median}\\{|r_{i}-\\operatorname{median}(r)|\\}=\\operatorname{median}\\{0.10,\\,0.10,\\,0.30,\\,0,\\,5.60\\}=0.10.\n$$\nHence\n$$\nS=c\\times \\text{MAD}=1.4826\\times 0.10=0.14826.\n$$\n\nStep 4: Standardized residuals $u_{i}=r_{i}/S$ have magnitudes\n$$\n|u|\\approx\\{7.824,\\,6.475,\\,9.173,\\,7.150,\\,30.622\\},\n$$\nso all satisfy $|u_{i}|>k$. Therefore Huber weights use $w_{i}=k/|u_{i}|$, equivalently\n$$\nw_{i}=\\frac{kS}{|r_{i}|},\\quad kS=1.345\\times 0.14826=0.1994097.\n$$\nThus\n$$\nw=\\left\\{\\frac{0.1994097}{1.16},\\,\\frac{0.1994097}{0.96},\\,\\frac{0.1994097}{1.36},\\,\\frac{0.1994097}{1.06},\\,\\frac{0.1994097}{4.54}\\right\\}\n\\approx\\{0.171904914,\\,0.2077184375,\\,0.146624779,\\,0.1881223185,\\,0.04392284\\}.\n$$\n\nStep 5: Compute the weighted average\n$$\n\\sum w_{i}=\\;0.171904914+0.2077184375+0.146624779+0.1881223185+0.04392284\\;\\approx 0.75829328879,\n$$\n$$\n\\sum w_{i}x_{i}=\\;0.171904914\\cdot 10.1+0.2077184375\\cdot 10.3+0.146624779\\cdot 9.9+0.1881223185\\cdot 10.2+0.04392284\\cdot 15.8\n$$\n$$\n\\approx 1.736239629279+2.13949990625+1.4515853121+1.9188476487+0.693980872\n=7.940153368329.\n$$\nTherefore\n$$\n\\mu_{1}=\\frac{7.940153368329}{0.75829328879}\\approx 10.47108485.\n$$\nRounded to four significant figures, this is $10.47$.", "answer": "$$\\boxed{10.47}$$", "id": "1952412"}]}