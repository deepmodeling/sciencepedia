{"hands_on_practices": [{"introduction": "Understanding the influence function begins with applying it to the most fundamental of all statistical estimators: the mean. This first exercise provides a direct, hands-on calculation of the influence function for the sample mean in the context of a Poisson distribution. By working through this problem, you will see how an individual data point's value directly and linearly affects the estimate, providing a baseline for assessing the robustness of more complex estimators.", "problem": "Consider a set of independent and identically distributed random variables, $X_1, X_2, \\dots, X_n$, drawn from a Poisson distribution with an unknown rate parameter $\\lambda  0$. The probability mass function for this distribution is given by $P(X=k) = \\frac{\\lambda^k \\exp(-\\lambda)}{k!}$ for non-negative integers $k$. The expected value of a Poisson($\\lambda$) random variable is $E[X] = \\lambda$.\n\nThe Method of Moments estimator for $\\lambda$, denoted $\\hat{\\lambda}$, is found by equating the first population moment, $E[X]$, to the first sample moment, $\\bar{X} = \\frac{1}{n}\\sum_{i=1}^n X_i$. This estimation procedure can be represented by a statistical functional, $T$, which maps a cumulative distribution function (CDF), $G$, to a parameter value. For this problem, the functional is defined by the expected value under $G$: $T(G) = \\int_{-\\infty}^{\\infty} x \\, dG(x)$. When applied to the empirical CDF of the data, this functional yields the sample mean.\n\nThe influence function (IF) measures the effect of an infinitesimal contamination at a point $x$ on the estimator. For a functional $T$ and a true underlying distribution $F$, the influence function is defined as:\n$$ \\text{IF}(x; T, F) = \\lim_{\\epsilon \\to 0^+} \\frac{T((1-\\epsilon)F + \\epsilon \\delta_x) - T(F)}{\\epsilon} $$\nwhere $\\delta_x$ is the CDF of a point mass at $x$.\n\nYour task is to determine the analytic form of the influence function for the Method of Moments estimator of the parameter $\\lambda$ of a Poisson($\\lambda$) distribution. Let $F$ be the CDF of the Poisson($\\lambda$) distribution. Express your answer in terms of the variable $x$ and the parameter $\\lambda$.", "solution": "We are estimating the Poisson parameter by the Method of Moments, where the functional is the mean:\n$$\nT(G) = \\int t \\, dG(t).\n$$\nFor the true distribution $F$ of a Poisson$(\\lambda)$ random variable, we have\n$$\nT(F) = \\int t \\, dF(t) = \\mathbb{E}_{F}[X] = \\lambda.\n$$\nTo compute the influence function at a point $x$, consider the contaminated distribution\n$$\nG_{\\epsilon} = (1 - \\epsilon)F + \\epsilon \\delta_{x},\n$$\nwith $\\epsilon \\in (0,1)$ and $\\delta_{x}$ the point mass at $x$. By linearity of the integral,\n$$\nT(G_{\\epsilon}) = \\int t \\, dG_{\\epsilon}(t) = (1 - \\epsilon) \\int t \\, dF(t) + \\epsilon \\int t \\, d\\delta_{x}(t) = (1 - \\epsilon)\\lambda + \\epsilon x = \\lambda + \\epsilon(x - \\lambda).\n$$\nTherefore,\n$$\n\\text{IF}(x; T, F) = \\lim_{\\epsilon \\to 0^{+}} \\frac{T(G_{\\epsilon}) - T(F)}{\\epsilon} = \\lim_{\\epsilon \\to 0^{+}} \\frac{\\lambda + \\epsilon(x - \\lambda) - \\lambda}{\\epsilon} = x - \\lambda.\n$$\nThus, the influence function for the Method of Moments estimator of $\\lambda$ (the sample mean) under a Poisson$(\\lambda)$ model is $x - \\lambda$.", "answer": "$$\\boxed{x-\\lambda}$$", "id": "1923520"}, {"introduction": "Having calculated the influence function for the mean [@problem_id:1923520], we now explore how it compares to that of another common estimator, the variance. This practice problem asks you to analyze the functional forms of the influence functions for both the mean and the variance, specifically for a standard normal distribution. This comparison is key to developing an intuition for robustness, as it highlights how the influence of an outlier on the variance grows quadratically, making it far more sensitive than the mean.", "problem": "In robust statistics, the influence function is a crucial tool for analyzing the effect of a single observation on a statistical estimator. For a statistical functional $T$ and a target distribution with cumulative distribution function (CDF) $F$, the influence function at a point $x$ is defined as:\n$$ IF(x; T, F) = \\lim_{\\epsilon \\to 0} \\frac{T((1-\\epsilon)F + \\epsilon \\delta_x) - T(F)}{\\epsilon} $$\nwhere $\\delta_x$ represents the CDF of a distribution with all its mass concentrated at the point $x$. This function measures the sensitivity of the functional $T$ to an infinitesimal contamination at point $x$.\n\nConsider two fundamental statistical estimators: the mean and the variance. Their corresponding functionals are the expected value, $T_{\\mu}(F) = \\int_{-\\infty}^{\\infty} y \\, dF(y)$, and the variance, $T_{\\sigma^2}(F) = \\int_{-\\infty}^{\\infty} (y - T_{\\mu}(F))^2 \\, dF(y)$.\n\nLet's analyze these functionals in the context of a standard normal distribution, denoted by $F=\\Phi$, which has a mean of 0 and a variance of 1. Let $IF_{\\mu}(x)$ denote the influence function for the mean and $IF_{\\sigma^2}(x)$ denote the influence function for the variance, both evaluated with respect to the standard normal distribution.\n\nWhich of the following statements accurately characterizes the properties of $IF_{\\mu}(x)$ and $IF_{\\sigma^2}(x)$?\n\nA. Both $IF_{\\mu}(x)$ and $IF_{\\sigma^2}(x)$ are bounded functions, indicating that both the mean and variance are robust estimators.\n\nB. $IF_{\\mu}(x)$ is an even function of $x$ (i.e., $IF_{\\mu}(-x) = IF_{\\mu}(x)$), while $IF_{\\sigma^2}(x)$ is an odd function of $x$ (i.e., $IF_{\\sigma^2}(-x) = -IF_{\\sigma^2}(x)$).\n\nC. An observation at the center of the distribution ($x=0$) has no influence on the estimate of the mean, but it tends to decrease the estimate of the variance.\n\nD. For large values of $|x|$, the influence of an outlier on the variance estimator grows at a slower rate than its influence on the mean estimator.\n\nE. Both influence functions are always non-negative for any real number $x$.", "solution": "We use the definition of the influence function. For any functional $T$ and distribution $F$, define $G_{\\epsilon}=(1-\\epsilon)F+\\epsilon \\delta_{x}$ and\n$$\nIF(x;T,F)=\\lim_{\\epsilon \\to 0}\\frac{T(G_{\\epsilon})-T(F)}{\\epsilon}.\n$$\n\nMean functional. Let $T_{\\mu}(F)=\\int y\\,dF(y)$ and denote $\\mu=T_{\\mu}(F)$. Then\n$$\nT_{\\mu}(G_{\\epsilon})=(1-\\epsilon)\\int y\\,dF(y)+\\epsilon\\int y\\,d\\delta_{x}(y)=(1-\\epsilon)\\mu+\\epsilon x=\\mu+\\epsilon(x-\\mu).\n$$\nTherefore,\n$$\nIF_{\\mu}(x;F)=x-\\mu.\n$$\nFor the standard normal $F=\\Phi$, we have $\\mu=0$, hence\n$$\nIF_{\\mu}(x)=x.\n$$\n\nVariance functional. Let $T_{\\sigma^{2}}(F)=\\int (y-\\mu)^{2}\\,dF(y)$ with $\\mu=T_{\\mu}(F)$ and denote $\\sigma^{2}=T_{\\sigma^{2}}(F)$. For $G_{\\epsilon}$, its mean is\n$$\n\\mu_{\\epsilon}=T_{\\mu}(G_{\\epsilon})=\\mu+\\epsilon(x-\\mu).\n$$\nThen\n$$\nT_{\\sigma^{2}}(G_{\\epsilon})=\\int (y-\\mu_{\\epsilon})^{2}\\,dG_{\\epsilon}(y)=(1-\\epsilon)\\int (y-\\mu_{\\epsilon})^{2}\\,dF(y)+\\epsilon(x-\\mu_{\\epsilon})^{2}.\n$$\nExpand to first order in $\\epsilon$. First,\n$$\n\\int (y-\\mu_{\\epsilon})^{2}\\,dF(y)=\\int \\big[(y-\\mu)-\\epsilon(x-\\mu)\\big]^{2}\\,dF(y)\n=\\int (y-\\mu)^{2}\\,dF(y)-2\\epsilon(x-\\mu)\\int (y-\\mu)\\,dF(y)+O(\\epsilon^{2})\n=\\sigma^{2}+O(\\epsilon^{2}),\n$$\nsince $\\int (y-\\mu)\\,dF(y)=0$. Next,\n$$\n(x-\\mu_{\\epsilon})^{2}=\\big[(x-\\mu)-\\epsilon(x-\\mu)\\big]^{2}=(x-\\mu)^{2}-2\\epsilon(x-\\mu)^{2}+O(\\epsilon^{2}).\n$$\nTherefore,\n$$\nT_{\\sigma^{2}}(G_{\\epsilon})=(1-\\epsilon)\\big[\\sigma^{2}+O(\\epsilon^{2})\\big]+\\epsilon\\big[(x-\\mu)^{2}-2\\epsilon(x-\\mu)^{2}+O(\\epsilon^{2})\\big]\n=\\sigma^{2}+\\epsilon\\big[(x-\\mu)^{2}-\\sigma^{2}\\big]+O(\\epsilon^{2}),\n$$\nand hence\n$$\nIF_{\\sigma^{2}}(x;F)=(x-\\mu)^{2}-\\sigma^{2}.\n$$\nFor the standard normal, $\\mu=0$ and $\\sigma^{2}=1$, so\n$$\nIF_{\\sigma^{2}}(x)=x^{2}-1.\n$$\n\nNow evaluate the statements using $IF_{\\mu}(x)=x$ and $IF_{\\sigma^{2}}(x)=x^{2}-1$:\n- Both are unbounded functions, so A is false.\n- $IF_{\\mu}$ is odd, not even; $IF_{\\sigma^{2}}$ is even, not odd; hence B is false.\n- At $x=0$, $IF_{\\mu}(0)=0$ while $IF_{\\sigma^{2}}(0)=-10$, so an observation at the center has no influence on the mean and tends to decrease the variance; C is true.\n- For large $|x|$, $|IF_{\\mu}(x)|\\sim |x|$ while $|IF_{\\sigma^{2}}(x)|\\sim x^{2}$, which grows faster, not slower; D is false.\n- Signs are not always non-negative (e.g., $x0$ for the mean; $|x|1$ for the variance), so E is false.\n\nTherefore, the correct choice is C.", "answer": "$$\\boxed{C}$$", "id": "1923527"}, {"introduction": "After observing the unbounded influence of outliers on classical estimators like the mean and variance [@problem_id:1923527], the next logical step is to explore estimators designed to be robust. This exercise introduces M-estimators, a flexible class of estimators whose properties can be tailored by choosing a specific score function, $\\psi(x)$. You will work with Tukey's biweight function, a \"redescending\" score function that completely downweights extreme outliers, demonstrating how influence functions can be used not just for analysis, but for the principled design of robust statistical methods.", "problem": "In robust statistics, M-estimators are a broad class of estimators for location parameters that are resistant to outliers. A location M-estimator $T_n$ for a dataset $\\{x_1, \\dots, x_n\\}$ is defined as the value $t$ that minimizes the objective function $\\sum_{i=1}^n \\rho(x_i - t)$, where $\\rho$ is a symmetric, non-negative loss function. Equivalently, $T_n$ is a solution to the implicit equation $\\sum_{i=1}^n \\psi(x_i - t) = 0$, where $\\psi(x) = \\frac{d\\rho(x)}{dx}$ is the score function.\n\nThe influence function, $IF(x; T, F)$, of an estimator $T$ at a distribution $F$ measures the effect of an infinitesimal contamination at point $x$ on the estimate. For an M-estimator with score function $\\psi$, evaluated at a distribution $F$ that is symmetric about the true location parameter (which we can assume to be 0 without loss of generality), the influence function is directly proportional to the score function: $IF(x; T, F) \\propto \\psi(x)$.\n\nTo completely reject extreme outliers, \"redescending\" M-estimators are used, for which the influence function is zero for observations sufficiently far from the center. This implies that their score function $\\psi(x)$ must be zero for $|x|$ larger than some positive constant $k$. One of the most common redescending score functions is the Tukey's biweight (or bisquare) function, defined as:\n$$\n\\psi_k(x) = \\begin{cases}\nx \\left(1 - \\left(\\frac{x}{k}\\right)^2\\right)^2  \\text{if } |x| \\leq k \\\\\n0  \\text{if } |x|  k\n\\end{cases}\n$$\nThe corresponding loss function $\\rho_k(x)$ is obtained by integrating the score function, subject to the condition $\\rho_k(0) = 0$. For a redescending M-estimator, the loss function $\\rho_k(x)$ is bounded, which means it becomes constant for $|x|  k$.\n\nWe wish to normalize this estimator by setting the maximum possible value of its loss function, $\\sup_{x \\in \\mathbb{R}} \\rho_k(x)$, to exactly 1. Determine the value of the positive constant $k$ that achieves this normalization.", "solution": "The problem asks for the value of the positive constant $k$ for the Tukey's biweight M-estimator such that the maximum value of its loss function $\\rho_k(x)$ is equal to 1.\n\nThe score function is given by:\n$$\n\\psi_k(x) = \\begin{cases}\nx \\left(1 - \\left(\\frac{x}{k}\\right)^2\\right)^2  \\text{if } |x| \\leq k \\\\\n0  \\text{if } |x|  k\n\\end{cases}\n$$\nThe loss function $\\rho_k(x)$ is related to the score function $\\psi_k(x)$ by $\\psi_k(x) = \\frac{d\\rho_k(x)}{dx}$. We can find $\\rho_k(x)$ by integrating $\\psi_k(u)$ from 0 to $x$, using the condition $\\rho_k(0) = 0$.\n$$\n\\rho_k(x) = \\int_0^x \\psi_k(u) du\n$$\nWe first expand the expression for $\\psi_k(u)$ for $|u| \\leq k$:\n$$\n\\psi_k(u) = u \\left(1 - \\frac{2u^2}{k^2} + \\frac{u^4}{k^4}\\right) = u - \\frac{2u^3}{k^2} + \\frac{u^5}{k^4}\n$$\nNow, we integrate this polynomial to find $\\rho_k(x)$ for the interval $|x| \\leq k$.\n$$\n\\rho_k(x) = \\int_0^x \\left(u - \\frac{2u^3}{k^2} + \\frac{u^5}{k^4}\\right) du\n$$\n$$\n\\rho_k(x) = \\left[ \\frac{u^2}{2} - \\frac{2u^4}{4k^2} + \\frac{u^6}{6k^4} \\right]_0^x\n$$\n$$\n\\rho_k(x) = \\frac{x^2}{2} - \\frac{x^4}{2k^2} + \\frac{x^6}{6k^4} \\quad \\text{for } |x| \\leq k\n$$\nFor $|x|  k$, the score function $\\psi_k(x)$ is 0. This means the loss function $\\rho_k(x)$ ceases to change. The value of $\\rho_k(x)$ for $x  k$ is thus constant and equal to its value at $x=k$.\n$$\n\\rho_k(x) = \\int_0^x \\psi_k(u) du = \\int_0^k \\psi_k(u) du + \\int_k^x 0 \\, du = \\int_0^k \\psi_k(u) du = \\rho_k(k)\n$$\nSince $\\rho_k(x)$ is an even function, the same holds for $x  -k$, where $\\rho_k(x) = \\rho_k(-k) = \\rho_k(k)$.\nThe function $\\rho_k(x)$ is non-decreasing for $x \\ge 0$ because its derivative, $\\psi_k(x) = x(1-(x/k)^2)^2$, is non-negative for $x \\in [0, k]$. Therefore, the maximum value of $\\rho_k(x)$ occurs for any $|x| \\ge k$ and is equal to $\\rho_k(k)$.\n$$\n\\rho_{\\max} = \\sup_{x \\in \\mathbb{R}} \\rho_k(x) = \\rho_k(k)\n$$\nWe can find this maximum value by substituting $x=k$ into our expression for $\\rho_k(x)$:\n$$\n\\rho_{\\max} = \\frac{k^2}{2} - \\frac{k^4}{2k^2} + \\frac{k^6}{6k^4}\n$$\n$$\n\\rho_{\\max} = \\frac{k^2}{2} - \\frac{k^2}{2} + \\frac{k^2}{6}\n$$\n$$\n\\rho_{\\max} = \\frac{k^2}{6}\n$$\nThe problem requires this maximum value to be normalized to 1.\n$$\n\\rho_{\\max} = 1\n$$\n$$\n\\frac{k^2}{6} = 1\n$$\nSolving for $k$:\n$$\nk^2 = 6\n$$\nSince $k$ is defined as a positive constant, we take the positive square root.\n$$\nk = \\sqrt{6}\n$$\nThus, the value of the constant $k$ that normalizes the maximum loss to 1 is $\\sqrt{6}$.", "answer": "$$\\boxed{\\sqrt{6}}$$", "id": "1923553"}]}