{"hands_on_practices": [{"introduction": "A cornerstone of Bayesian inference is the process of updating our beliefs in light of new evidence. This first exercise provides a practical demonstration of this process using the classic Beta-Binomial conjugate pair, a fundamental model in Bayesian statistics. By starting with a state of complete uncertainty about a proportion—modeled by a Uniform prior, which is a special case of the Beta distribution—we can see precisely how collecting data shapes our posterior belief, as measured by its mean. This problem [@problem_id:1946628] asks a common and practical question: how much evidence do we need to be convinced?", "problem": "A materials science company has developed a new, experimental process for manufacturing optical fibers. The key quality metric is the proportion, $p$, of manufactured fibers that meet a stringent signal transmission standard. As the process is entirely new, the engineering team has no prior knowledge about its effectiveness. They decide to model their initial belief about $p$ using a continuous uniform distribution over the interval $[0, 1]$, which is equivalent to a Beta distribution with parameters $\\alpha=1$ and $\\beta=1$.\n\nTo update their belief, they test a sequence of newly manufactured fibers. They find that the first $n$ fibers they test all meet the transmission standard. What is the smallest integer number of consecutive successful tests, $n$, required for the posterior mean of the proportion $p$ to exceed $0.98$?", "solution": "The prior for the success proportion is $p \\sim \\mathrm{Beta}(\\alpha,\\beta)$ with $\\alpha=1$ and $\\beta=1$. Under the Beta-Binomial conjugate model, after observing $S$ successes and $F$ failures, the posterior is $\\mathrm{Beta}(\\alpha+S,\\beta+F)$. Here, the first $n$ tests are all successes, so $S=n$ and $F=0$, giving the posterior\n$$\np \\mid \\text{data} \\sim \\mathrm{Beta}(1+n,1).\n$$\nThe mean of a $\\mathrm{Beta}(a,b)$ distribution is $\\frac{a}{a+b}$, hence the posterior mean is\n$$\n\\mathbb{E}[p \\mid \\text{data}] = \\frac{n+1}{n+2}.\n$$\nWe require the smallest integer $n$ such that\n$$\n\\frac{n+1}{n+2} > 0.98.\n$$\nSince $n+2>0$ for all integer $n \\geq 0$, we can multiply both sides by $n+2$ without changing the inequality direction:\n$$\nn+1 > 0.98(n+2).\n$$\nExpanding and rearranging,\n$$\nn+1 > 0.98n + 1.96 \\quad \\Rightarrow \\quad n - 0.98n > 1.96 - 1 \\quad \\Rightarrow \\quad 0.02\\,n > 0.96.\n$$\nDividing by $0.02$ (positive) yields\n$$\nn > 48.\n$$\nTherefore, the smallest integer $n$ satisfying the strict inequality is $n=49$. Note that for $n=48$, the mean is $\\frac{49}{50}=0.98$, which does not exceed $0.98$, confirming the need for $n=49$.", "answer": "$$\\boxed{49}$$", "id": "1946628"}, {"introduction": "While conjugate priors offer mathematical convenience, not all real-world problems fit neatly into these pairs. This next practice problem challenges you to apply Bayes' theorem from first principles to a scenario with uniform distributions for both the prior and the likelihood. The key to solving this problem [@problem_id:1946605] lies in carefully considering the support of the distributions, as the observed data directly constrains the possible values of the unknown parameter. This exercise is invaluable for developing a robust understanding of how likelihood and prior interact to form the posterior, moving beyond simple formula application.", "problem": "An engineer is testing a new type of experimental microchip. The maximum operational lifespan of this chip, denoted by the parameter $\\theta$ (in hours), is unknown. Based on the manufacturing process, the engineer's prior belief is that $\\theta$ is equally likely to be any value in the interval $[0, 10]$. The lifetime of any individual chip, $X$, is known to follow a uniform distribution on the interval $[0, \\theta]$. A single chip is tested and is observed to fail at exactly $x=3$ hours. Given this observation, what is the updated expected value for the maximum operational lifespan $\\theta$?\n\nProvide your answer in hours, rounded to four significant figures.", "solution": "This problem requires the use of Bayesian inference to update our belief about the parameter $\\theta$ after observing data. We need to find the posterior distribution of $\\theta$ given the observation $x=3$, and then calculate its expected value.\n\nStep 1: Define the prior distribution.\nThe problem states that the prior belief for $\\theta$ is that it is equally likely to be any value in the interval $[0, 10]$. This corresponds to a uniform distribution. The Probability Density Function (PDF) of the prior distribution, $p(\\theta)$, is:\n$$\np(\\theta) = \n\\begin{cases} \n\\frac{1}{10 - 0} = \\frac{1}{10} & \\text{for } 0 \\le \\theta \\le 10 \\\\\n0 & \\text{otherwise}\n\\end{cases}\n$$\n\nStep 2: Define the likelihood function.\nThe lifetime of a single chip, $X$, follows a uniform distribution on $[0, \\theta]$. The PDF for a single observation $x$ given the parameter $\\theta$ is the likelihood function, $p(x|\\theta)$:\n$$\np(x|\\theta) = \n\\begin{cases} \n\\frac{1}{\\theta} & \\text{for } 0 \\le x \\le \\theta \\\\\n0 & \\text{otherwise}\n\\end{cases}\n$$\nWe are given a single observation $x=3$. For this observation to be possible, the parameter $\\theta$ must be greater than or equal to 3. If $\\theta < 3$, the probability of observing $x=3$ is zero. Therefore, the likelihood function for our specific data $x=3$ is:\n$$\np(x=3|\\theta) = \n\\begin{cases} \n\\frac{1}{\\theta} & \\text{for } \\theta \\ge 3 \\\\\n0 & \\text{otherwise}\n\\end{cases}\n$$\n\nStep 3: Determine the posterior distribution.\nAccording to Bayes' theorem, the posterior distribution, $p(\\theta|x)$, is proportional to the product of the likelihood and the prior distribution:\n$$\np(\\theta|x) \\propto p(x|\\theta) \\cdot p(\\theta)\n$$\nWe need to consider the ranges where both functions are non-zero.\nThe prior $p(\\theta)$ is non-zero for $0 \\le \\theta \\le 10$.\nThe likelihood $p(x=3|\\theta)$ is non-zero for $\\theta \\ge 3$.\nThe product is non-zero only on the intersection of these two ranges, which is $3 \\le \\theta \\le 10$.\n\nFor $\\theta$ in the interval $[3, 10]$, we have:\n$$\np(\\theta|x=3) \\propto \\left(\\frac{1}{\\theta}\\right) \\cdot \\left(\\frac{1}{10}\\right) \\propto \\frac{1}{\\theta}\n$$\nSo, the posterior PDF has the form $p(\\theta|x=3) = \\frac{k}{\\theta}$ for $3 \\le \\theta \\le 10$, and 0 otherwise, where $k$ is a normalization constant.\n\nStep 4: Calculate the normalization constant.\nTo be a valid PDF, the posterior distribution must integrate to 1 over its domain.\n$$\n\\int_{3}^{10} p(\\theta|x=3) d\\theta = 1\n$$\n$$\n\\int_{3}^{10} \\frac{k}{\\theta} d\\theta = 1\n$$\n$$\nk \\int_{3}^{10} \\frac{1}{\\theta} d\\theta = 1\n$$\n$$\nk [\\ln|\\theta|]_{3}^{10} = 1\n$$\n$$\nk (\\ln(10) - \\ln(3)) = 1\n$$\n$$\nk \\ln\\left(\\frac{10}{3}\\right) = 1\n$$\n$$\nk = \\frac{1}{\\ln(10/3)}\n$$\nThus, the full posterior PDF is:\n$$\np(\\theta|x=3) = \n\\begin{cases} \n\\frac{1}{\\theta \\ln(10/3)} & \\text{for } 3 \\le \\theta \\le 10 \\\\\n0 & \\text{otherwise}\n\\end{cases}\n$$\n\nStep 5: Calculate the expected value of the posterior distribution.\nThe question asks for the updated expected value of $\\theta$, which is the mean of the posterior distribution, $E[\\theta|x=3]$.\n$$\nE[\\theta|x=3] = \\int_{-\\infty}^{\\infty} \\theta \\cdot p(\\theta|x=3) d\\theta\n$$\n$$\nE[\\theta|x=3] = \\int_{3}^{10} \\theta \\cdot \\frac{1}{\\theta \\ln(10/3)} d\\theta\n$$\n$$\nE[\\theta|x=3] = \\frac{1}{\\ln(10/3)} \\int_{3}^{10} 1 d\\theta\n$$\n$$\nE[\\theta|x=3] = \\frac{1}{\\ln(10/3)} [\\theta]_{3}^{10}\n$$\n$$\nE[\\theta|x=3] = \\frac{10 - 3}{\\ln(10/3)} = \\frac{7}{\\ln(10/3)}\n$$\n\nStep 6: Compute the numerical value.\nNow we calculate the numerical value and round it to four significant figures.\n$$\n\\ln(10/3) \\approx 1.203972804\n$$\n$$\nE[\\theta|x=3] \\approx \\frac{7}{1.203972804} \\approx 5.8140645\n$$\nRounding to four significant figures, we get 5.814. The unit is hours.", "answer": "$$\\boxed{5.814}$$", "id": "1946605"}, {"introduction": "The choice of a prior distribution is a defining feature of Bayesian analysis, reflecting our initial assumptions about a parameter. This final exercise explores the practical implications of this choice by comparing two different priors for the rate parameter of a Poisson distribution. You will contrast a standard exponential prior, which is a conjugate choice, with a non-informative Jeffreys' prior, which is designed to have minimal influence on the posterior [@problem_id:1946616]. Calculating the difference in the resulting estimates will highlight how your initial modeling assumptions propagate through the analysis and affect your conclusions.", "problem": "A digital marketing analyst is modeling the number of users who click on a specific advertisement link per hour. The number of clicks, $X$, is assumed to follow a Poisson distribution with an unknown average rate $\\lambda$. To estimate this rate, the analyst collects data over $n$ independent and identically distributed (i.i.d.) one-hour intervals, recording the click counts $x_1, x_2, \\dots, x_n$.\n\nTwo different Bayesian models are proposed to infer the value of $\\lambda$. The estimate for $\\lambda$ in each model is taken to be the mode of the posterior distribution, also known as the Maximum a Posteriori (MAP) estimate.\n\n- **Model A** uses a standard exponential prior distribution for the rate: $p_A(\\lambda) = \\exp(-\\lambda)$, for $\\lambda > 0$. Let the resulting posterior mode be denoted by $\\hat{\\lambda}_A$.\n- **Model B** uses a common non-informative improper prior, Jeffreys' prior for the Poisson rate: $p_B(\\lambda) \\propto \\frac{1}{\\sqrt{\\lambda}}$, for $\\lambda > 0$. Let this posterior mode be denoted by $\\hat{\\lambda}_B$.\n\nYour task is to calculate the difference between the two estimators, $\\hat{\\lambda}_A - \\hat{\\lambda}_B$. Express your answer as an analytic expression in terms of the number of intervals, $n$, and the sample mean, $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i$.", "solution": "Let $X_{1},\\dots,X_{n}$ be i.i.d. given $\\lambda$ with $X_{i}\\mid\\lambda\\sim\\text{Poisson}(\\lambda)$. Denote $S=\\sum_{i=1}^{n}x_{i}=n\\bar{x}$. The likelihood is\n$$\nL(\\lambda\\mid x_{1:n})=\\prod_{i=1}^{n}\\frac{\\exp(-\\lambda)\\lambda^{x_{i}}}{x_{i}!}\\propto \\exp(-n\\lambda)\\lambda^{S}.\n$$\nIts log, up to an additive constant, is\n$$\n\\ell(\\lambda)=-n\\lambda+S\\ln\\lambda.\n$$\n\nModel A uses $p_{A}(\\lambda)=\\exp(-\\lambda)$ for $\\lambda>0$. The posterior (up to a multiplicative constant) is\n$$\np(\\lambda\\mid x_{1:n},A)\\propto \\exp(-n\\lambda)\\lambda^{S}\\cdot \\exp(-\\lambda)=\\exp(-(n+1)\\lambda)\\lambda^{S},\n$$\nwith log-posterior\n$$\n\\ell_{A}(\\lambda)=-(n+1)\\lambda+S\\ln\\lambda.\n$$\nDifferentiate and set to zero:\n$$\n\\frac{d\\ell_{A}}{d\\lambda}=-(n+1)+\\frac{S}{\\lambda}=0\\quad\\Longrightarrow\\quad \\hat{\\lambda}_{A}=\\frac{S}{n+1}.\n$$\nFor $S>0$, the second derivative $-\\frac{S}{\\lambda^{2}}<0$ confirms a mode in the interior; for $S=0$ the mode is at the boundary $\\lambda=0$.\n\nModel B uses $p_{B}(\\lambda)\\propto \\lambda^{-1/2}$ for $\\lambda>0$. The posterior (up to a multiplicative constant) is\n$$\np(\\lambda\\mid x_{1:n},B)\\propto \\exp(-n\\lambda)\\lambda^{S}\\cdot \\lambda^{-1/2}=\\exp(-n\\lambda)\\lambda^{S-\\frac{1}{2}},\n$$\nwith log-posterior\n$$\n\\ell_{B}(\\lambda)=-n\\lambda+\\left(S-\\frac{1}{2}\\right)\\ln\\lambda.\n$$\nDifferentiate and set to zero:\n$$\n\\frac{d\\ell_{B}}{d\\lambda}=-n+\\frac{S-\\frac{1}{2}}{\\lambda}=0\\quad\\Longrightarrow\\quad \\hat{\\lambda}_{B}=\\frac{S-\\frac{1}{2}}{n}.\n$$\nFor $S\\geq 1$ this is an interior mode; for $S=0$ the mode is at the boundary $\\lambda=0$.\n\nFor $S\\geq 1$ (the typical case with at least one observed click), the difference is\n$$\n\\hat{\\lambda}_{A}-\\hat{\\lambda}_{B}=\\frac{S}{n+1}-\\frac{S-\\frac{1}{2}}{n}.\n$$\nSubstitute $S=n\\bar{x}$ and simplify:\n$$\n\\hat{\\lambda}_{A}-\\hat{\\lambda}_{B}=\\frac{n\\bar{x}}{n+1}-\\left(\\bar{x}-\\frac{1}{2n}\\right)\n=-\\frac{\\bar{x}}{n+1}+\\frac{1}{2n}.\n$$\nThus, expressed in terms of $n$ and $\\bar{x}$,\n$$\n\\hat{\\lambda}_{A}-\\hat{\\lambda}_{B}=\\frac{1}{2n}-\\frac{\\bar{x}}{n+1}.\n$$\n(If $S=0$, both modes are at $0$ and the difference is $0$; the expression above applies when $S\\geq 1$.)", "answer": "$$\\boxed{\\frac{1}{2n}-\\frac{\\bar{x}}{n+1}}$$", "id": "1946616"}]}