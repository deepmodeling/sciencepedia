{"hands_on_practices": [{"introduction": "Improper priors are powerful tools in Bayesian analysis, but their use comes with a critical checkpoint: the resulting posterior distribution must be proper. This exercise explores the relationship between the prior, the likelihood, and the amount of data. You will determine the minimum sample size required to ensure a valid posterior, illustrating the crucial idea that data can \"tame\" an improper prior and make inference possible. [@problem_id:1922090]", "problem": "A statistician is analyzing the lifetime of a certain type of electronic component. The lifetime $X$ of a single component is modeled by an Exponential distribution with an unknown rate parameter $\\lambda > 0$. The probability density function (PDF) for a single observation is given by $f(x|\\lambda) = \\lambda \\exp(-\\lambda x)$ for $x > 0$.\n\nTo perform a Bayesian analysis on a random sample of $n$ components, $X_1, X_2, \\ldots, X_n$, the statistician chooses an improper prior distribution for the parameter $\\lambda$. This prior is defined by the relationship $p(\\lambda) \\propto \\lambda^{-3}$ for $\\lambda > 0$. An improper prior is one that does not integrate to a finite value over its domain.\n\nFor a Bayesian inference to be valid, the resulting posterior distribution, $p(\\lambda | X_1, \\ldots, X_n)$, must be a proper distribution. A distribution is considered 'proper' if its integral over its entire domain is finite, which allows for normalization into a valid probability distribution.\n\nAssuming that for any sample taken (i.e., for $n \\geq 1$), the sum of the observed lifetimes is positive, determine the minimum integer sample size $n$ required to ensure that the posterior distribution of $\\lambda$ is proper.", "solution": "Let $X_{1},\\ldots,X_{n}$ be i.i.d. $\\operatorname{Exp}(\\lambda)$ with density $f(x \\mid \\lambda)=\\lambda \\exp(-\\lambda x)$ for $x0$. The joint likelihood is\n$$\nL(\\lambda \\mid x_{1},\\ldots,x_{n})=\\prod_{i=1}^{n}\\lambda \\exp(-\\lambda x_{i})=\\lambda^{n}\\exp\\!\\left(-\\lambda \\sum_{i=1}^{n}x_{i}\\right).\n$$\nLet $S=\\sum_{i=1}^{n}x_{i}$. The prior is improper with density kernel $p(\\lambda)\\propto \\lambda^{-3}$ for $\\lambda0$. The posterior density kernel is then\n$$\np(\\lambda \\mid x_{1},\\ldots,x_{n}) \\propto L(\\lambda \\mid x_{1},\\ldots,x_{n})\\,p(\\lambda)\n= \\lambda^{n}\\exp(-\\lambda S)\\cdot \\lambda^{-3}\n= \\lambda^{n-3}\\exp(-\\lambda S), \\quad \\lambda0.\n$$\nAssuming $S0$, this is proportional to a Gamma density with shape parameter $\\alpha=n-2$ and rate parameter $\\beta=S$, since the Gamma kernel is $\\lambda^{\\alpha-1}\\exp(-\\beta \\lambda)$. The posterior is proper if and only if its integral over $(0,\\infty)$ is finite:\n$$\n\\int_{0}^{\\infty}\\lambda^{n-3}\\exp(-S\\lambda)\\,d\\lambda\\infty.\n$$\nUsing the standard Gamma integral,\n$$\n\\int_{0}^{\\infty}\\lambda^{a-1}\\exp(-\\beta \\lambda)\\,d\\lambda=\\frac{\\Gamma(a)}{\\beta^{a}} \\quad \\text{for } a0,\\ \\beta0,\n$$\nwe identify $a=n-2$ and $\\beta=S$. Since $S0$ by assumption, finiteness requires $a>0$, i.e.,\n$$\nn-2>0 \\quad \\Longleftrightarrow \\quad n>2.\n$$\nBecause $n$ is an integer, the minimum $n$ ensuring a proper posterior is $n=3$. For $n=2$, the integrand behaves like $\\lambda^{-1}$ near $0$, and the integral diverges; for $n=1$, it behaves like $\\lambda^{-2}$ near $0$, which also diverges.", "answer": "$$\\boxed{3}$$", "id": "1922090"}, {"introduction": "A key principle in modeling is consistency. If we have a principled reason for choosing a prior for a parameter, such as the standard deviation $\\sigma$, that principle should ideally carry over if we decide to work with its square, the variance $\\phi = \\sigma^2$. This practice demonstrates how to transform a prior distribution under reparameterization using the change-of-variables formula, revealing how the widely used Jeffreys prior for a scale parameter behaves under this important transformation. [@problem_id:1922146]", "problem": "In Bayesian statistical inference, a prior distribution is assigned to an unknown parameter to represent beliefs about it before data is observed. Sometimes, for parameters that can take any value in an un-bounded range, analysts use what are known as improper priors. These are functions that resemble probability densities but do not integrate to a finite value over their domain. Despite this, they can often lead to valid posterior distributions.\n\nConsider a statistical model that involves a positive scale parameter, denoted by $\\sigma$, where $\\sigma  0$. A widely-used non-informative improper prior for such a scale parameter is the Jeffreys prior, which has a probability density function $p(\\sigma)$ that is proportional to $\\frac{1}{\\sigma}$.\n\nSuppose an analyst decides to reparameterize the model. Instead of using the scale parameter $\\sigma$, they choose to work with the variance parameter, $\\phi$, which is defined by the transformation $\\phi = \\sigma^2$. Your task is to find the corresponding improper prior for the variance parameter $\\phi$. Specifically, determine the function $f(\\phi)$ such that the prior density for $\\phi$, denoted by $p(\\phi)$, is proportional to $f(\\phi)$.", "solution": "We start from the Jeffreys prior for a positive scale parameter $\\sigma$, given up to proportionality by $p_{\\sigma}(\\sigma) \\propto \\frac{1}{\\sigma}$ for $\\sigma  0$.\n\nIntroduce the reparameterization $\\phi = \\sigma^2$ with $\\phi  0$. Then $\\sigma = \\phi^{1/2}$, and the change-of-variables formula for densities gives\n$$\np_{\\phi}(\\phi) \\propto p_{\\sigma}(\\sigma(\\phi)) \\left|\\frac{d\\sigma}{d\\phi}\\right|.\n$$\nCompute the Jacobian:\n$$\n\\frac{d\\sigma}{d\\phi} = \\frac{d}{d\\phi}\\left(\\phi^{1/2}\\right) = \\frac{1}{2}\\phi^{-1/2}.\n$$\nSubstitute $p_{\\sigma}(\\sigma) \\propto \\frac{1}{\\sigma}$ with $\\sigma = \\phi^{1/2}$:\n$$\np_{\\phi}(\\phi) \\propto \\left(\\frac{1}{\\sigma(\\phi)}\\right)\\left|\\frac{d\\sigma}{d\\phi}\\right| = \\left(\\frac{1}{\\phi^{1/2}}\\right)\\left(\\frac{1}{2}\\phi^{-1/2}\\right) = \\frac{1}{2}\\phi^{-1}.\n$$\nSince proportionality constants are immaterial for specifying the prior up to proportionality, we conclude\n$$\np_{\\phi}(\\phi) \\propto \\frac{1}{\\phi}, \\quad \\phi  0.\n$$\nTherefore, the required function is $f(\\phi) = \\frac{1}{\\phi}$.", "answer": "$$\\boxed{\\frac{1}{\\phi}}$$", "id": "1922146"}, {"introduction": "While often called 'non-informative,' different improper priors can encode different assumptions and lead to different results. This exercise provides a direct comparison of two different improper priors for the mean of a Normal distribution. By calculating and contrasting the posterior expectations, you will gain a hands-on understanding of how the choice of an improper prior, even a seemingly simple one, actively influences the final inference. [@problem_id:1922111]", "problem": "Consider a single observation $x$ drawn from a Normal distribution with an unknown mean $\\mu$ and a known variance of 1. The Probability Density Function (PDF) of this observation given the parameter $\\mu$ is $f(x|\\mu) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{1}{2}(x-\\mu)^2\\right)$.\n\nTwo different Bayesian models are proposed to conduct inference on the unknown parameter $\\mu$.\n\n- **Model 1**: This model uses an improper prior distribution for $\\mu$ defined by the density $p_1(\\mu) \\propto 1$ for all real numbers $\\mu$.\n- **Model 2**: This model uses a different improper prior distribution for $\\mu$ defined by the density $p_2(\\mu) \\propto I(\\mu  0)$, where $I(\\cdot)$ is the indicator function which evaluates to 1 if its argument is true and 0 otherwise.\n\nLet $E_1[\\mu]$ be the expected value of $\\mu$ under the posterior distribution derived from Model 1, and let $E_2[\\mu]$ be the expected value of $\\mu$ under the posterior distribution derived from Model 2.\n\nDetermine the ratio $\\frac{E_2[\\mu]}{E_1[\\mu]}$ as a function of the observation $x$. The final expression may involve the standard normal PDF, $\\phi(z) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-z^2/2)$, and the standard normal Cumulative Distribution Function (CDF), $\\Phi(z) = \\int_{-\\infty}^{z} \\phi(t) dt$.", "solution": "We have one observation $x$ from $X \\mid \\mu \\sim \\mathcal{N}(\\mu,1)$, so the likelihood as a function of $\\mu$ is proportional to $f(x \\mid \\mu) \\propto \\exp\\!\\left(-\\frac{1}{2}(x-\\mu)^{2}\\right)$, equivalently $f(x \\mid \\mu) \\propto \\phi(x-\\mu)$ where $\\phi(z) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\!\\left(-\\frac{z^{2}}{2}\\right)$.\n\nModel 1 has prior $p_{1}(\\mu) \\propto 1$ for $\\mu \\in \\mathbb{R}$. The posterior is\n$$\np_{1}(\\mu \\mid x) \\propto f(x \\mid \\mu) p_{1}(\\mu) \\propto \\phi(x-\\mu).\n$$\nSince $\\int_{-\\infty}^{\\infty} \\phi(x-\\mu)\\,d\\mu = 1$, the normalized posterior is $p_{1}(\\mu \\mid x) = \\phi(\\mu - x)$, i.e., $\\mu \\mid x \\sim \\mathcal{N}(x,1)$. Therefore,\n$$\nE_{1}[\\mu] \\;=\\; \\int_{-\\infty}^{\\infty} \\mu\\, \\phi(\\mu - x)\\, d\\mu.\n$$\nWith the change of variable $t = \\mu - x$ (so $\\mu = t + x$ and $d\\mu = dt$), we get\n$$\nE_{1}[\\mu] = \\int_{-\\infty}^{\\infty} (t + x)\\, \\phi(t)\\, dt \\;=\\; x \\int_{-\\infty}^{\\infty} \\phi(t)\\, dt \\;+\\; \\int_{-\\infty}^{\\infty} t\\, \\phi(t)\\, dt \\;=\\; x + 0 \\;=\\; x.\n$$\n\nModel 2 has prior $p_{2}(\\mu) \\propto I(\\mu  0)$. The posterior is\n$$\np_{2}(\\mu \\mid x) \\propto \\phi(x-\\mu)\\, I(\\mu  0).\n$$\nTo normalize, compute\n$$\n\\int_{0}^{\\infty} \\phi(x-\\mu)\\, d\\mu \\;=\\; \\int_{0}^{\\infty} \\phi(\\mu - x)\\, d\\mu \\;=\\; \\int_{-x}^{\\infty} \\phi(t)\\, dt \\;=\\; 1 - \\Phi(-x) \\;=\\; \\Phi(x),\n$$\nwhere $\\Phi$ is the standard normal CDF and we used $\\Phi(-x) = 1 - \\Phi(x)$. Hence the normalized posterior is\n$$\np_{2}(\\mu \\mid x) \\;=\\; \\frac{\\phi(\\mu - x)}{\\Phi(x)}\\, I(\\mu  0).\n$$\nTherefore,\n$$\nE_{2}[\\mu] \\;=\\; \\int_{0}^{\\infty} \\mu\\, \\frac{\\phi(\\mu - x)}{\\Phi(x)}\\, d\\mu \\;=\\; \\frac{1}{\\Phi(x)} \\int_{0}^{\\infty} \\mu\\, \\phi(\\mu - x)\\, d\\mu.\n$$\nApply the change of variable $t = \\mu - x$ (so $\\mu = t + x$, $d\\mu = dt$, and the lower limit becomes $t = -x$):\n$$\nE_{2}[\\mu] \\;=\\; \\frac{1}{\\Phi(x)} \\int_{-x}^{\\infty} (t + x)\\, \\phi(t)\\, dt \\;=\\; \\frac{1}{\\Phi(x)} \\left[ x \\int_{-x}^{\\infty} \\phi(t)\\, dt \\;+\\; \\int_{-x}^{\\infty} t\\, \\phi(t)\\, dt \\right].\n$$\nEvaluate the integrals:\n- $\\int_{-x}^{\\infty} \\phi(t)\\, dt = 1 - \\Phi(-x) = \\Phi(x)$.\n- For the second integral, use that $\\frac{d}{dt}\\phi(t) = -t\\,\\phi(t)$, hence $\\int t\\, \\phi(t)\\, dt = -\\phi(t) + C$, giving\n$$\n\\int_{-x}^{\\infty} t\\, \\phi(t)\\, dt \\;=\\; \\left[-\\phi(t)\\right]_{-x}^{\\infty} \\;=\\; 0 - \\left(-\\phi(-x)\\right) \\;=\\; \\phi(-x) \\;=\\; \\phi(x).\n$$\nThus,\n$$\nE_{2}[\\mu] \\;=\\; \\frac{1}{\\Phi(x)} \\left[ x\\, \\Phi(x) + \\phi(x) \\right] \\;=\\; x \\;+\\; \\frac{\\phi(x)}{\\Phi(x)}.\n$$\n\nFinally, the ratio is\n$$\n\\frac{E_{2}[\\mu]}{E_{1}[\\mu]} \\;=\\; \\frac{x + \\frac{\\phi(x)}{\\Phi(x)}}{x} \\;=\\; 1 + \\frac{\\phi(x)}{x\\, \\Phi(x)},\n$$\nwhich is defined for $x \\neq 0$; at $x=0$ the ratio is undefined because $E_{1}[\\mu]=0$.", "answer": "$$\\boxed{1+\\frac{\\phi(x)}{x\\,\\Phi(x)}}$$", "id": "1922111"}]}