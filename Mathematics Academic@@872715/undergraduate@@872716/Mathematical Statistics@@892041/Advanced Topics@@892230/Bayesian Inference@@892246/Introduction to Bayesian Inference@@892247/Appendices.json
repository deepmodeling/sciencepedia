{"hands_on_practices": [{"introduction": "The cornerstone of Bayesian inference is updating our beliefs in light of new evidence. This first exercise provides a practical demonstration of this core mechanism using conjugate priors, where the prior and posterior distributions belong to the same family, simplifying the update process. By working through the Beta-Geometric model, you will see how observing the number of trials until the first success allows us to refine our initial beliefs about an unknown probability [@problem_id:1923967].", "problem": "A quantum engineer is testing a new protocol for initializing a quantum bit (qubit). The probability of a successful initialization on any given attempt is an unknown constant $p$, where $0 < p < 1$. Each initialization attempt is an independent trial.\n\nThe engineer's prior belief about the probability $p$ is described by a Beta distribution with known hyperparameters $\\alpha_0 > 0$ and $\\beta_0 > 0$. The probability density function of a $\\text{Beta}(\\alpha, \\beta)$ distribution for a variable $x \\in [0, 1]$ is proportional to $x^{\\alpha-1}(1-x)^{\\beta-1}$.\n\nIn an experiment, the engineer performs initialization attempts repeatedly until the first successful initialization is achieved. It is observed that the first success occurs on the $k$-th attempt.\n\nGiven this observation, the engineer updates their belief about $p$. The posterior distribution for $p$ is also a Beta distribution. Which of the following represents the new parameters, $(\\alpha_{\\text{post}}, \\beta_{\\text{post}})$, of this posterior distribution?\n\nA. $(\\alpha_0 + k, \\beta_0 + 1)$\n\nB. $(\\alpha_0 + k - 1, \\beta_0 + 1)$\n\nC. $(\\alpha_0 + 1, \\beta_0 + k)$\n\nD. $(\\alpha_0 + 1, \\beta_0 + k - 1)$\n\nE. $(\\alpha_0, \\beta_0 + k - 1)$", "solution": "Let $p \\in (0,1)$ denote the success probability per attempt. The prior for $p$ is $\\text{Beta}(\\alpha_{0},\\beta_{0})$ with density (up to normalization)\n$$\nf(p)\\propto p^{\\alpha_{0}-1}(1-p)^{\\beta_{0}-1}.\n$$\nThe observation is that the first success occurs on the $k$-th attempt. Under independent Bernoulli trials with success probability $p$, this event has likelihood given by the geometric model:\n$$\n\\mathcal{L}(p;k)=P(K=k\\mid p)=(1-p)^{k-1}p,\n$$\nsince it consists of $k-1$ failures followed by one success.\n\nBy Bayes’ rule, the posterior density is\n$$\nf(p\\mid k)=\\frac{\\mathcal{L}(p;k)f(p)}{\\int_{0}^{1}\\mathcal{L}(p;k)f(p)\\,dp}\\propto \\mathcal{L}(p;k)f(p).\n$$\nSubstituting the prior kernel and the likelihood,\n$$\nf(p\\mid k)\\propto \\left[p^{\\alpha_{0}-1}(1-p)^{\\beta_{0}-1}\\right]\\left[(1-p)^{k-1}p\\right]\n= p^{\\alpha_{0}-1+1}(1-p)^{\\beta_{0}-1+k-1}\n= p^{\\alpha_{0}}(1-p)^{\\beta_{0}+k-2}.\n$$\nRecognizing the Beta kernel $p^{\\alpha_{\\text{post}}-1}(1-p)^{\\beta_{\\text{post}}-1}$, we identify\n$$\n\\alpha_{\\text{post}}-1=\\alpha_{0}\\quad\\Rightarrow\\quad \\alpha_{\\text{post}}=\\alpha_{0}+1,\\qquad\n\\beta_{\\text{post}}-1=\\beta_{0}+k-2\\quad\\Rightarrow\\quad \\beta_{\\text{post}}=\\beta_{0}+k-1.\n$$\nEquivalently, the observation “first success at $k$” encodes $1$ success and $k-1$ failures, yielding the standard Beta update $\\alpha_{0}\\mapsto \\alpha_{0}+1$ and $\\beta_{0}\\mapsto \\beta_{0}+k-1$. Therefore the correct choice is $(\\alpha_{0}+1,\\beta_{0}+k-1)$.", "answer": "$$\\boxed{D}$$", "id": "1923967"}, {"introduction": "Beyond estimating single parameters, a primary application of Bayesian analysis is comparing different groups or treatments, such as in A/B testing. This practice problem moves from simple parameter estimation to comparative inference, a crucial skill in data analysis. You will determine the posterior distribution for the difference between the means of two groups, which allows us to quantify our uncertainty about the magnitude and direction of the effect [@problem_id:1924011].", "problem": "An educational data scientist is analyzing the effectiveness of two different online learning modules, Module A and Module B. After running an experiment and collecting data on student scores, a Bayesian analysis is performed. The uncertainty about the true mean score for students using Module A, denoted by $\\mu_A$, is captured by a posterior distribution. Similarly, the uncertainty for the mean score of Module B, $\\mu_B$, is captured by its own posterior distribution.\n\nThe analysis yields the following posterior distributions:\n- The posterior distribution for $\\mu_A$ is a Normal distribution with a mean of $\\theta_A$ and a precision of $\\tau_A$.\n- The posterior distribution for $\\mu_B$ is a Normal distribution with a mean of $\\theta_B$ and a precision of $\\tau_B$.\n\nPrecision is defined as the reciprocal of the variance, i.e., $\\tau = 1/\\sigma^2$. The two posterior distributions are assumed to be independent.\n\nThe data scientist is interested in the difference in effectiveness, which is quantified by the variable $\\delta = \\mu_A - \\mu_B$. Determine the posterior probability distribution of $\\delta$.\n\nWhich of the following describes the posterior distribution for $\\delta = \\mu_A - \\mu_B$?\n\nA. A Normal distribution with mean $\\theta_A - \\theta_B$ and variance $\\frac{\\tau_A \\tau_B}{\\tau_A + \\tau_B}$.\n\nB. A Normal distribution with mean $\\theta_A - \\theta_B$ and variance $\\frac{1}{\\tau_A + \\tau_B}$.\n\nC. A Student's t-distribution with mean $\\theta_A - \\theta_B$.\n\nD. A Normal distribution with mean $\\theta_A - \\theta_B$ and variance $\\frac{1}{\\tau_A} + \\frac{1}{\\tau_B}$.\n\nE. A Normal distribution with mean $\\theta_A + \\theta_B$ and variance $\\frac{1}{\\tau_A} + \\frac{1}{\\tau_B}$.", "solution": "Let the posterior distributions be given as follows:\n$$\n\\mu_{A} \\sim \\mathcal{N}\\!\\left(\\theta_{A}, \\frac{1}{\\tau_{A}}\\right), \\quad \\mu_{B} \\sim \\mathcal{N}\\!\\left(\\theta_{B}, \\frac{1}{\\tau_{B}}\\right),\n$$\nwith independence between $\\mu_{A}$ and $\\mu_{B}$.\n\nDefine the difference $\\delta = \\mu_{A} - \\mu_{B}$. Use the linearity property of the Normal distribution: if $X \\sim \\mathcal{N}(m_{X}, v_{X})$ and $Y \\sim \\mathcal{N}(m_{Y}, v_{Y})$ are independent, then for constants $a$ and $b$, the linear combination $aX + bY$ satisfies\n$$\naX + bY \\sim \\mathcal{N}\\!\\left(a m_{X} + b m_{Y}, a^{2} v_{X} + b^{2} v_{Y}\\right).\n$$\nApply this with $X = \\mu_{A}$, $Y = \\mu_{B}$, $a = 1$, and $b = -1$. The mean of $\\delta$ is\n$$\n\\mathbb{E}[\\delta] = 1 \\cdot \\theta_{A} + (-1) \\cdot \\theta_{B} = \\theta_{A} - \\theta_{B}.\n$$\nThe variance of $\\delta$ is\n$$\n\\operatorname{Var}(\\delta) = 1^{2} \\cdot \\frac{1}{\\tau_{A}} + (-1)^{2} \\cdot \\frac{1}{\\tau_{B}} = \\frac{1}{\\tau_{A}} + \\frac{1}{\\tau_{B}}.\n$$\nTherefore,\n$$\n\\delta \\sim \\mathcal{N}\\!\\left(\\theta_{A} - \\theta_{B}, \\frac{1}{\\tau_{A}} + \\frac{1}{\\tau_{B}}\\right).\n$$\nComparing with the options, this corresponds to a Normal distribution with mean $\\theta_{A} - \\theta_{B}$ and variance $\\frac{1}{\\tau_{A}} + \\frac{1}{\\tau_{B}}$, which is option D.", "answer": "$$\\boxed{D}$$", "id": "1924011"}, {"introduction": "One of the most powerful features of the Bayesian framework is its natural approach to forecasting. After updating our beliefs about model parameters, we can use this posterior knowledge to predict future, unobserved data. This final exercise introduces the concept of the posterior predictive distribution, where you will calculate the probability of a future event after learning from initial observations [@problem_id:1923970].", "problem": "A software development company is running a closed beta test for its new application. They model the number of bugs reported per month, $X$, by a Poisson distribution with an unknown average rate $\\lambda$. The probability mass function is given by $P(X=k | \\lambda) = \\frac{\\exp(-\\lambda) \\lambda^{k}}{k!}$ for $k=0, 1, 2, \\dots$.\n\nBased on historical data from similar projects, the project manager's prior belief about the rate $\\lambda$ is described by a Gamma distribution with a shape parameter $\\alpha = 2$ and a rate parameter $\\beta = 1$. The probability density function of this prior is $f(\\lambda) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\lambda^{\\alpha-1} \\exp(-\\beta \\lambda)$ for $\\lambda > 0$.\n\nDuring the first month of beta testing, the testers report a total of 3 bugs.\n\nAssuming that the underlying bug rate $\\lambda$ remains constant, calculate the probability that zero bugs will be reported in the second month of testing. Round your final answer to four significant figures.", "solution": "We model $X \\mid \\lambda \\sim \\text{Poisson}(\\lambda)$ with probability mass function $P(X=k \\mid \\lambda)=\\frac{\\exp(-\\lambda)\\lambda^{k}}{k!}$ and prior $\\lambda \\sim \\text{Gamma}(\\alpha,\\beta)$ with density $f(\\lambda)=\\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}\\lambda^{\\alpha-1}\\exp(-\\beta\\lambda)$, where $\\alpha=2$ and $\\beta=1$.\n\nGiven the first-month observation $X_{1}=3$, the likelihood is\n$$\nL(\\lambda \\mid X_{1}=3)\\propto \\exp(-\\lambda)\\lambda^{3}.\n$$\nCombining with the prior,\n$$\np(\\lambda \\mid X_{1}=3)\\propto \\lambda^{\\alpha-1}\\exp(-\\beta\\lambda)\\cdot \\exp(-\\lambda)\\lambda^{3}\n=\\lambda^{\\alpha+3-1}\\exp\\big(-(\\beta+1)\\lambda\\big).\n$$\nRecognizing the kernel of a Gamma density, the posterior is\n$$\n\\lambda \\mid X_{1}=3 \\sim \\text{Gamma}(\\alpha',\\beta'), \\quad \\alpha'=\\alpha+3,\\ \\beta'=\\beta+1,\n$$\nso here $\\alpha'=5$ and $\\beta'=2$.\n\nThe posterior predictive probability for zero bugs in the second month is\n$$\nP(X_{2}=0 \\mid X_{1}=3)=\\int_{0}^{\\infty}P(X_{2}=0 \\mid \\lambda)\\,p(\\lambda \\mid X_{1}=3)\\,d\\lambda\n=\\int_{0}^{\\infty}\\exp(-\\lambda)\\,\\frac{(\\beta')^{\\alpha'}}{\\Gamma(\\alpha')}\\lambda^{\\alpha'-1}\\exp(-\\beta'\\lambda)\\,d\\lambda.\n$$\nEvaluate the integral using the Gamma integral identity $\\int_{0}^{\\infty}\\lambda^{\\alpha'-1}\\exp(-c\\lambda)\\,d\\lambda=\\frac{\\Gamma(\\alpha')}{c^{\\alpha'}}$ for $c>0$:\n$$\nP(X_{2}=0 \\mid X_{1}=3)=\\frac{(\\beta')^{\\alpha'}}{\\Gamma(\\alpha')}\\cdot \\frac{\\Gamma(\\alpha')}{(\\beta'+1)^{\\alpha'}}=\\left(\\frac{\\beta'}{\\beta'+1}\\right)^{\\alpha'}.\n$$\nSubstituting $\\alpha'=5$ and $\\beta'=2$ gives\n$$\nP(X_{2}=0 \\mid X_{1}=3)=\\left(\\frac{2}{3}\\right)^{5}=\\frac{32}{243}\\approx 0.131687\\ldots\n$$\nRounded to four significant figures, this is $0.1317$.", "answer": "$$\\boxed{0.1317}$$", "id": "1923970"}]}