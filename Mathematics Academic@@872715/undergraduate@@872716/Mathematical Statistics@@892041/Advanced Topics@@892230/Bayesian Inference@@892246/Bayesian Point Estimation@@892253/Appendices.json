{"hands_on_practices": [{"introduction": "Bayesian estimation begins with a prior belief, which is then updated by data. This exercise demonstrates the profound impact of that initial choice by comparing how a \"skeptical\" and a \"non-informative\" prior lead to different conclusions after observing the same evidence [@problem_id:1899686]. You will calculate the posterior mean for each case, providing a clear, quantitative look at how prior assumptions shape our learning from data.", "problem": "A materials science lab is developing a new type of semiconductor for a a high-frequency application. A critical manufacturing step has a certain probability, $p$, of success. Due to resource constraints, only a single attempt to produce the semiconductor can be made. The attempt is successful.\n\nTwo statisticians, an optimist and a skeptic, are tasked with providing an estimate for $p$ based on this single successful outcome. They both agree to model the outcome of the attempt as a Bernoulli trial and to use a Bayesian framework. Their disagreement lies in the prior belief about the success probability $p$.\n\nThe optimist, having confidence in the lab's general capabilities, assumes a \"non-informative\" prior for $p$, which is a Beta distribution with parameters $\\alpha_O = 1$ and $\\beta_O = 1$. This prior represents a uniform belief over all possible values of $p$ between 0 and 1.\n\nThe skeptic, aware of the high failure rates common in developing novel materials, assumes a \"skeptical\" prior for $p$, which is a Beta distribution with parameters $\\alpha_S = 1$ and $\\beta_S = 5$. This prior expresses a belief that low values of $p$ are more likely.\n\nAfter observing the single successful trial, both statisticians update their beliefs and calculate their respective posterior expected values for the success probability $p$. What is the absolute difference between the skeptic's posterior expected value and the optimist's posterior expected value? Express your answer as an exact fraction in simplest form.", "solution": "Model the outcome $X$ as a Bernoulli trial with success probability $p$. With a Beta prior $\\operatorname{Beta}(\\alpha,\\beta)$ and observing $s$ successes and $f$ failures, conjugacy gives the posterior $\\operatorname{Beta}(\\alpha+s,\\beta+f)$. The posterior mean for $\\operatorname{Beta}(a,b)$ is $a/(a+b)$.\n\nThere is a single attempt and it is successful, so $s=1$ and $f=0$.\n\nFor the optimist: prior $\\operatorname{Beta}(\\alpha_{O},\\beta_{O})$ with $\\alpha_{O}=1$, $\\beta_{O}=1$. The posterior is\n$$\n\\operatorname{Beta}(\\alpha_{O}+s,\\beta_{O}+f)=\\operatorname{Beta}(2,1),\n$$\nso the posterior expected value is\n$$\n\\mathbb{E}_{O}[p]=\\frac{2}{2+1}=\\frac{2}{3}.\n$$\n\nFor the skeptic: prior $\\operatorname{Beta}(\\alpha_{S},\\beta_{S})$ with $\\alpha_{S}=1$, $\\beta_{S}=5$. The posterior is\n$$\n\\operatorname{Beta}(\\alpha_{S}+s,\\beta_{S}+f)=\\operatorname{Beta}(2,5),\n$$\nso the posterior expected value is\n$$\n\\mathbb{E}_{S}[p]=\\frac{2}{2+5}=\\frac{2}{7}.\n$$\n\nThe absolute difference between the skeptic's and optimist's posterior expected values is\n$$\n\\left|\\mathbb{E}_{S}[p]-\\mathbb{E}_{O}[p]\\right|=\\left|\\frac{2}{7}-\\frac{2}{3}\\right|=\\left|\\frac{6-14}{21}\\right|=\\frac{8}{21}.\n$$", "answer": "$$\\boxed{\\frac{8}{21}}$$", "id": "1899686"}, {"introduction": "A key strength of the Bayesian framework is its ability to sequentially incorporate new information. This practice problem simulates a common real-world scenario where an initial study provides a posterior belief, which then serves as the prior for a follow-up experiment [@problem_id:1899661]. By working through this, you will master the mechanics of Bayesian updating and see how our certainty about a parameter evolves as we gather more data.", "problem": "A software company is testing a new user interface feature. The proportion of users who successfully interact with this feature is an unknown parameter $p$. A data scientist models the uncertainty in $p$ using a Bayesian approach.\n\nInitially, after a preliminary study, the data scientist's belief about $p$ is described by a Beta posterior distribution with parameters $\\alpha_1 = 29$ and $\\beta_1 = 121$.\n\nTo gather more evidence, the company conducts a second, independent experiment where the feature is shown to a new group of $n_2 = 500$ users. In this second group, $k_2 = 110$ users are observed to successfully interact with the feature.\n\nAssuming the number of successful interactions in the second experiment follows a Binomial distribution, calculate the updated posterior mean of $p$ after incorporating the results from this second experiment. Round your final answer to four significant figures.", "solution": "Let $p$ denote the success probability. The initial belief after the preliminary study is modeled as a Beta distribution with parameters $\\alpha_{1}=29$ and $\\beta_{1}=121$. The second experiment produces $k_{2}=110$ successes out of $n_{2}=500$ trials, modeled as $k_{2} \\sim \\operatorname{Binomial}(n_{2},p)$.\n\nBy Beta-Binomial conjugacy, updating a $\\operatorname{Beta}(\\alpha_{1},\\beta_{1})$ prior with Binomial data $(n_{2},k_{2})$ yields a posterior $\\operatorname{Beta}(\\alpha',\\beta')$ with\n$$\n\\alpha'=\\alpha_{1}+k_{2}, \\quad \\beta'=\\beta_{1}+n_{2}-k_{2}.\n$$\nSubstituting the given values,\n$$\n\\alpha'=29+110=139, \\quad \\beta'=121+(500-110)=121+390=511.\n$$\nThe posterior mean of $p$ under a $\\operatorname{Beta}(\\alpha',\\beta')$ distribution is\n$$\n\\mathbb{E}[p \\mid \\text{data}]=\\frac{\\alpha'}{\\alpha'+\\beta'}=\\frac{139}{139+511}=\\frac{139}{650}.\n$$\nA decimal approximation rounded to four significant figures is\n$$\n\\frac{139}{650}=0.213846\\ldots \\approx 0.2138.\n$$", "answer": "$$\\boxed{0.2138}$$", "id": "1899661"}, {"introduction": "While many problems involve estimating continuous parameters like probabilities, Bayesian methods are also powerful for discrete parameters. This exercise shifts our focus to estimating a discrete quantity—the number of faces on a die—and introduces a different type of point estimate: the Maximum A Posteriori (MAP) [@problem_id:1899685]. Instead of finding the average value of the parameter (the posterior mean), you will find its single most likely value, a concept that is highly intuitive and useful in many decision-making contexts.", "problem": "A company specializing in tabletop games is testing a new manufacturing process for producing non-standard dice. The number of faces on any given die, denoted by the parameter $N$, is unknown. Based on the manufacturing specifications, it is known that $N$ can only take one of three possible values: 6, 8, or 12. Initially, with no experimental data, each of these possibilities is considered equally likely.\n\nTo determine the most probable number of faces for a particular die, a single experiment is conducted: the die is rolled once, and the outcome is observed to be a '5'. Assume the die is fair, meaning that for a given $N$, each of the faces numbered 1 to $N$ has an equal probability of being rolled.\n\nYour task is to calculate the Maximum A Posteriori (MAP) estimate for the number of faces, $N$, given this single observation.", "solution": "Let the hypothesis space be $\\mathcal{H}=\\{6,8,12\\}$. The prior over $N$ is uniform, so for each $N \\in \\mathcal{H}$,\n$$\nP(N)=\\frac{1}{3}.\n$$\nLet the single observation be $X=5$. Given a fair $N$-sided die, the likelihood is\n$$\nP(X=5 \\mid N)=\\begin{cases}\n\\frac{1}{N},  \\text{if } 5 \\leq N,\\\\\n0,  \\text{if } 5  N.\n\\end{cases}\n$$\nSince $5 \\leq 6,8,12$, we have $P(X=5 \\mid N)=\\frac{1}{N}$ for all $N \\in \\{6,8,12\\}$.\n\nBy Bayes' rule, for each $N \\in \\mathcal{H}$,\n$$\nP(N \\mid X=5)=\\frac{P(N)\\,P(X=5 \\mid N)}{\\sum_{n \\in \\mathcal{H}} P(n)\\,P(X=5 \\mid n)}=\\frac{\\frac{1}{3}\\cdot \\frac{1}{N}}{\\sum_{n \\in \\{6,8,12\\}} \\frac{1}{3}\\cdot \\frac{1}{n}}=\\frac{\\frac{1}{N}}{\\sum_{n \\in \\{6,8,12\\}} \\frac{1}{n}}.\n$$\nTherefore, the posterior is proportional to $\\frac{1}{N}$. The Maximum A Posteriori (MAP) estimate is the $N$ that maximizes $P(N \\mid X=5)$, equivalently maximizes $\\frac{1}{N}$, which is achieved by the smallest feasible $N$. Among $\\{6,8,12\\}$, the smallest is $6$.\n\nHence, the MAP estimate is $N=6$.", "answer": "$$\\boxed{6}$$", "id": "1899685"}]}