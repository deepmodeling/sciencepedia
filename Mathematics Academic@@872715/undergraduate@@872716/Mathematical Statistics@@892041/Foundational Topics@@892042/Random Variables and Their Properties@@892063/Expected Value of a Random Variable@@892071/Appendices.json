{"hands_on_practices": [{"introduction": "One of the most powerful tools in probability theory is the linearity of expectation. This property states that the expected value of a sum of random variables is simply the sum of their individual expected values, regardless of whether the variables are independent. This practice problem provides a tangible scenario with non-standard dice to demonstrate how this principle can dramatically simplify calculations, allowing you to find the expected sum without enumerating all possible outcomes [@problem_id:1916150].", "problem": "A game designer is creating a new board game that uses two distinct, non-standard six-sided dice, referred to as Die A and Die B. Both dice are fair, meaning that for each die, any of its six faces is equally likely to land facing up after a roll. The set of integers on the six faces of Die A is $\\{0, 1, 1, 2, 4, 5\\}$. The set of integers on the six faces of Die B is $\\{2, 3, 4, 5, 6, 6\\}$. If a player rolls both dice simultaneously, what is the expected value of the sum of the numbers that appear on their top faces?", "solution": "Let $X$ be the random variable representing the numerical outcome of rolling Die A, and let $Y$ be the random variable representing the numerical outcome of rolling Die B. We are asked to find the expected value of the sum of these two random variables, which is denoted as $E[X+Y]$.\n\nA fundamental property of expected values is the linearity of expectation. This property states that for any two random variables $X$ and $Y$, the expected value of their sum is equal to the sum of their individual expected values:\n$$E[X+Y] = E[X] + E[Y]$$\nThis holds true regardless of whether the variables are independent or not. In this case, the rolls of the two dice are independent events.\n\nFirst, we calculate the expected value of the outcome of Die A, $E[X]$. The expected value of a discrete random variable is calculated by summing the product of each possible value and its probability. Since Die A is a fair six-sided die, the probability of any specific face landing up is $\\frac{1}{6}$. The values on the faces are $\\{0, 1, 1, 2, 4, 5\\}$.\nThe expected value $E[X]$ is the average of these values:\n$$E[X] = 0 \\cdot \\frac{1}{6} + 1 \\cdot \\frac{1}{6} + 1 \\cdot \\frac{1}{6} + 2 \\cdot \\frac{1}{6} + 4 \\cdot \\frac{1}{6} + 5 \\cdot \\frac{1}{6}$$\n$$E[X] = \\frac{1}{6} (0 + 1 + 1 + 2 + 4 + 5)$$\n$$E[X] = \\frac{13}{6}$$\n\nNext, we calculate the expected value of the outcome of Die B, $E[Y]$. The values on the faces of Die B are $\\{2, 3, 4, 5, 6, 6\\}$. Since Die B is also a fair six-sided die, the probability of any face landing up is $\\frac{1}{6}$.\nThe expected value $E[Y]$ is the average of these values:\n$$E[Y] = 2 \\cdot \\frac{1}{6} + 3 \\cdot \\frac{1}{6} + 4 \\cdot \\frac{1}{6} + 5 \\cdot \\frac{1}{6} + 6 \\cdot \\frac{1}{6} + 6 \\cdot \\frac{1}{6}$$\n$$E[Y] = \\frac{1}{6} (2 + 3 + 4 + 5 + 6 + 6)$$\n$$E[Y] = \\frac{26}{6}$$\n$$E[Y] = \\frac{13}{3}$$\n\nFinally, we use the linearity of expectation to find the expected sum $E[X+Y]$:\n$$E[X+Y] = E[X] + E[Y]$$\n$$E[X+Y] = \\frac{13}{6} + \\frac{26}{6}$$\n$$E[X+Y] = \\frac{13 + 26}{6}$$\n$$E[X+Y] = \\frac{39}{6}$$\nTo simplify the fraction, we divide the numerator and the denominator by their greatest common divisor, which is 3:\n$$E[X+Y] = \\frac{39 \\div 3}{6 \\div 3} = \\frac{13}{2}$$\n\nThus, the expected sum of the numbers shown on the top faces of the two dice is $\\frac{13}{2}$.", "answer": "$$\\boxed{\\frac{13}{2}}$$", "id": "1916150"}, {"introduction": "Moving from discrete to continuous random variables, the concept of expected value is extended through integration. This exercise explores finding the expected value of a function of a continuous random variable, a common task in fields like physics and engineering. Using a simplified model for particle speeds, you will calculate the expected kinetic energy, $E[\\frac{1}{2}mV^2]$, which requires first normalizing the probability density function and then applying the definition of expectation $E[g(V)] = \\int g(v) f(v) dv$ [@problem_id:1916121].", "problem": "In a simplified one-dimensional model of a rarefied gas, the speeds of non-interacting particles are described by a continuous random variable $V$. The probability density function (PDF) for the speed of a randomly selected particle is given by\n$$ f(v) = \\begin{cases} A v^{2}  \\text{if } 0 \\le v \\le v_{max} \\\\ 0  \\text{otherwise} \\end{cases} $$\nwhere $v_{max}$ is a positive constant representing the maximum possible speed of any particle, and $A$ is a normalization constant. Each particle has a mass $m$.\n\nDetermine the expected kinetic energy of a randomly selected particle. Express your answer as a closed-form analytic expression in terms of the particle mass $m$ and the maximum speed $v_{max}$.", "solution": "The probability density function is $f(v)=A v^{2}$ for $0\\leq v \\leq v_{max}$ and zero otherwise. The normalization condition $\\int_{-\\infty}^{\\infty} f(v)\\,dv=1$ reduces to\n$$\n\\int_{0}^{v_{max}} A v^{2}\\,dv = 1.\n$$\nEvaluating the integral gives\n$$\nA \\int_{0}^{v_{max}} v^{2}\\,dv = A \\left[\\frac{v^{3}}{3}\\right]_{0}^{v_{max}} = A \\frac{v_{max}^{3}}{3} = 1,\n$$\nso\n$$\nA = \\frac{3}{v_{max}^{3}}.\n$$\n\nThe kinetic energy of a particle with speed $v$ is $K(v)=\\frac{1}{2} m v^{2}$. The expected kinetic energy is\n$$\n\\mathbb{E}[K] = \\int_{0}^{v_{max}} \\frac{1}{2} m v^{2} f(v)\\,dv = \\frac{1}{2} m \\int_{0}^{v_{max}} v^{2} \\left(A v^{2}\\right)\\,dv = \\frac{1}{2} m A \\int_{0}^{v_{max}} v^{4}\\,dv.\n$$\nCompute the integral and substitute $A$:\n$$\n\\frac{1}{2} m A \\left[\\frac{v^{5}}{5}\\right]_{0}^{v_{max}} = \\frac{1}{2} m A \\frac{v_{max}^{5}}{5} = \\frac{1}{2} m \\left(\\frac{3}{v_{max}^{3}}\\right) \\frac{v_{max}^{5}}{5} = \\frac{3}{10} m v_{max}^{2}.\n$$\nTherefore, the expected kinetic energy is $\\frac{3}{10} m v_{max}^{2}$.", "answer": "$$\\boxed{\\frac{3}{10} m v_{max}^{2}}$$", "id": "1916121"}, {"introduction": "Some problems involving expected value are not easily solved by direct summation or integration. This practice introduces a more advanced and elegant technique: conditioning, also known as first-step analysis. By breaking down the problem recursively based on the outcome of the first event, you can establish a system of equations to find the desired expectation. This exercise, which asks for the expected number of trials to observe a specific pattern, is a classic example that showcases the power and efficiency of this recursive approach to problem-solving [@problem_id:1916144].", "problem": "In a simplified model of a digital communication system, individual bits are transmitted sequentially through a noisy channel. Each bit is received as a '1' with a constant probability $p$, and as a '0' with probability $1-p$. The reception of each bit is an independent event.\n\nAn error-check protocol is designed to monitor the stream of incoming bits. The protocol triggers an alarm the very first time it observes the specific two-bit sequence '10' (a '1' followed immediately by a '0').\n\nLet $N$ be the random variable representing the total number of bits that must be received to trigger the alarm for the first time. Determine the expected value of $N$, denoted as $E[N]$, as a function of $p$.", "solution": "Let the incoming bits be independent, with $\\Pr\\{1\\}=p$ and $\\Pr\\{0\\}=1-p$, and let $N$ be the number of bits until the first occurrence of the pattern \"10\". Consider the following states for a first-step analysis:\n- State $0$: no trailing $1$ has been seen (start or the last received bit is $0$). Let $E_{0}$ be the expected additional number of bits to see \"10\" from this state.\n- State $1$: the last received bit is $1$ (we have the prefix \"1\"). Let $E_{1}$ be the expected additional number of bits to see \"10\" from this state.\n\nWe seek $E[N]=E_{0}$ starting from the beginning (which is state $0$).\n\nFrom state $1$, the next bit is $0$ with probability $1-p$, which completes \"10\" in exactly $1$ more bit; with probability $p$, the next bit is $1$, which consumes $1$ bit and stays in state $1$. Therefore,\n$$\nE_{1}=(1-p)\\cdot 1+p\\cdot(1+E_{1})=1+pE_{1},\n$$\nwhich simplifies to\n$$\n(1-p)E_{1}=1\\quad\\Rightarrow\\quad E_{1}=\\frac{1}{1-p}.\n$$\n\nFrom state $0$, the next bit is $0$ with probability $1-p$, which consumes $1$ bit and remains in state $0$; with probability $p$, the next bit is $1$, which consumes $1$ bit and moves to state $1$. Therefore,\n$$\nE_{0}=(1-p)\\cdot(1+E_{0})+p\\cdot(1+E_{1})=1+(1-p)E_{0}+pE_{1}.\n$$\nRearranging,\n$$\nE_{0}-(1-p)E_{0}=1+pE_{1}\\quad\\Rightarrow\\quad pE_{0}=1+pE_{1}\\quad\\Rightarrow\\quad E_{0}=\\frac{1}{p}+E_{1}.\n$$\nSubstituting $E_{1}=\\frac{1}{1-p}$,\n$$\nE_{0}=\\frac{1}{p}+\\frac{1}{1-p}=\\frac{(1-p)+p}{p(1-p)}=\\frac{1}{p(1-p)}.\n$$\n\nHence, the expected number of bits until the first occurrence of \"10\" is\n$$\nE[N]=\\frac{1}{p(1-p)}.\n$$", "answer": "$$\\boxed{\\frac{1}{p(1-p)}}$$", "id": "1916144"}]}