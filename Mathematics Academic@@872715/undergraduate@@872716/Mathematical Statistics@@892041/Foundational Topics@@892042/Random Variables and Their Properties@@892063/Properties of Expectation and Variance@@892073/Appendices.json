{"hands_on_practices": [{"introduction": "Mastering the properties of expectation begins with a solid grasp of its fundamental definition. This first practice provides a concrete physical scenario—a particle detection event—to guide you through calculating the expected value of a function of a continuous random variable. By working through this problem [@problem_id:1947856], you will apply the core formula $E[g(X)] = \\int_{-\\infty}^{\\infty} g(x) f(x) dx$ and see how an abstract concept translates into a tangible prediction.", "problem": "A one-dimensional particle detector of length $L$ is used to locate a particle emission event. The event occurs at a position $X$ along the detector, where $X$ is a random variable uniformly distributed over the interval $[0, L]$. The detector has sensors at its two endpoints, located at positions $0$ and $L$. When an emission occurs at position $X$, it generates a signal that propagates in both directions along the detector towards the sensors at a constant speed $v$. Let $T_0$ be the time it takes for the signal to reach the sensor at position $0$, and $T_L$ be the time it takes for the signal to reach the sensor at position $L$.\n\nCalculate the expected value of the product of these two travel times, $E[T_0 T_L]$.\n\nExpress your answer as a symbolic expression in terms of the parameters $L$ and $v$.", "solution": "The signal travels at constant speed $v$, so each travel time equals distance divided by speed. If the emission occurs at position $X$, then the time to reach the sensor at $0$ is $T_{0}=\\frac{X}{v}$, and the time to reach the sensor at $L$ is $T_{L}=\\frac{L-X}{v}$. Their product is\n$$\nT_{0}T_{L}=\\frac{X(L-X)}{v^{2}}.\n$$\nSince $X$ is uniformly distributed on $[0,L]$ with density $f_{X}(x)=\\frac{1}{L}$ for $x\\in[0,L]$, the expectation is\n$$\nE[T_{0}T_{L}]=\\int_{0}^{L}\\frac{X(L-X)}{v^{2}}\\cdot \\frac{1}{L}\\,dX=\\frac{1}{L v^{2}}\\int_{0}^{L}\\big(LX-X^{2}\\big)\\,dX.\n$$\nEvaluate the integral explicitly:\n$$\n\\int_{0}^{L}\\big(LX-X^{2}\\big)\\,dX=\\left[\\frac{L X^{2}}{2}-\\frac{X^{3}}{3}\\right]_{0}^{L}=\\frac{L^{3}}{2}-\\frac{L^{3}}{3}=L^{3}\\left(\\frac{1}{2}-\\frac{1}{3}\\right)=\\frac{L^{3}}{6}.\n$$\nTherefore,\n$$\nE[T_{0}T_{L}]=\\frac{1}{L v^{2}}\\cdot \\frac{L^{3}}{6}=\\frac{L^{2}}{6 v^{2}}.\n$$", "answer": "$$\\boxed{\\frac{L^{2}}{6 v^{2}}}$$", "id": "1947856"}, {"introduction": "Building on the basics, we now explore the behavior of sums of random variables, a common occurrence in fields from finance to physics. This exercise [@problem_id:1947875] uses the classic model of a one-dimensional random walk to investigate the expected squared displacement after many steps. The solution is a beautiful illustration of the power of linearity of expectation and the properties of variance, which allow us to break down a complex problem into manageable parts.", "problem": "A materials science lab is studying a one-dimensional self-assembly process for creating nanowires. The process involves a robotic assembler that starts at an origin point (position 0 on a number line) and adds segments of unit length, one after another. For each segment added, a random fluctuation in the assembly process causes it to be oriented in one of two ways with equal probability: either in the positive direction (a displacement of $+1$) or in the negative direction (a displacement of $-1$). Each choice of direction is independent of all previous choices.\n\nAfter a total of $n$ such segments have been added, the assembler stops. Let the final position of the end of the nanowire be $S_n$. Determine the expected value of the square of this final position, $E[S_n^2]$. Your final answer should be a closed-form expression in terms of the total number of steps, $n$.", "solution": "Let $X_{i}$ denote the displacement at step $i$, where $X_{i} \\in \\{-1, +1\\}$ with $\\mathbb{P}(X_{i}=1) = \\mathbb{P}(X_{i}=-1) = \\frac{1}{2}$, and the $X_{i}$ are independent. The final position after $n$ steps is\n$$\nS_{n} = \\sum_{i=1}^{n} X_{i}.\n$$\nFirst, compute the first and second moments of $X_{i}$. Since $X_{i}$ takes values $\\pm 1$ symmetrically,\n$$\nE[X_{i}] = \\left(1\\right)\\frac{1}{2} + \\left(-1\\right)\\frac{1}{2} = 0,\n$$\nand\n$$\nE[X_{i}^{2}] = \\left(1^{2}\\right)\\frac{1}{2} + \\left((-1)^{2}\\right)\\frac{1}{2} = 1.\n$$\nWe now compute $E[S_{n}^{2}]$:\n$$\nE[S_{n}^{2}] = E\\left[\\left(\\sum_{i=1}^{n} X_{i}\\right)^{2}\\right] = E\\left[\\sum_{i=1}^{n} X_{i}^{2} + 2\\sum_{1 \\leq i  j \\leq n} X_{i}X_{j}\\right].\n$$\nBy linearity of expectation,\n$$\nE[S_{n}^{2}] = \\sum_{i=1}^{n} E[X_{i}^{2}] + 2\\sum_{1 \\leq i  j \\leq n} E[X_{i}X_{j}].\n$$\nFor $i \\neq j$, independence gives $E[X_{i}X_{j}] = E[X_{i}]E[X_{j}] = 0$, hence all cross terms vanish. Therefore,\n$$\nE[S_{n}^{2}] = \\sum_{i=1}^{n} E[X_{i}^{2}] = \\sum_{i=1}^{n} 1 = n.\n$$\nThus, the expected square displacement after $n$ steps is $n$.", "answer": "$$\\boxed{n}$$", "id": "1947875"}, {"introduction": "While the previous problem relied on the simplifying assumption of independence, many real-world scenarios involve variables that are related. To analyze these situations, we use covariance, a measure of how two random variables move in relation to each other. This final practice [@problem_id:1947870] challenges you to calculate the covariance between a single observation and the sample mean of a dataset. Understanding this non-obvious relationship is a cornerstone concept in statistical sampling and the theory of estimation.", "problem": "Consider a set of $n$ random variables, $X_1, X_2, \\dots, X_n$, which are independent and identically distributed (i.i.d.). Each variable has an expected value (mean) of $E[X_i] = \\mu$ and a finite, non-zero variance of $\\text{Var}(X_i) = \\sigma^2$ for all $i \\in \\{1, 2, \\dots, n\\}$.\n\nThe sample mean, denoted by $\\bar{X}$, is defined as the arithmetic average of these variables:\n$$\n\\bar{X} = \\frac{1}{n} \\sum_{j=1}^{n} X_j\n$$\n\nDetermine the covariance between a single observation, $X_i$, and the sample mean, $\\bar{X}$. Express your answer as a closed-form analytic expression in terms of $n$ and $\\sigma^2$.", "solution": "We seek $\\operatorname{Cov}(X_{i},\\bar{X})$, where $\\bar{X}=\\frac{1}{n}\\sum_{j=1}^{n}X_{j}$. Using bilinearity of covariance, in particular $\\operatorname{Cov}(A, cB)=c\\,\\operatorname{Cov}(A,B)$ and $\\operatorname{Cov}\\!\\left(A,\\sum_{j}B_{j}\\right)=\\sum_{j}\\operatorname{Cov}(A,B_{j})$, we have\n$$\n\\operatorname{Cov}(X_{i},\\bar{X})=\\operatorname{Cov}\\!\\left(X_{i},\\frac{1}{n}\\sum_{j=1}^{n}X_{j}\\right)=\\frac{1}{n}\\sum_{j=1}^{n}\\operatorname{Cov}(X_{i},X_{j}).\n$$\nBecause the $X_{j}$ are independent, $\\operatorname{Cov}(X_{i},X_{j})=0$ for $j\\neq i$, and $\\operatorname{Cov}(X_{i},X_{i})=\\operatorname{Var}(X_{i})=\\sigma^{2}$. Therefore,\n$$\n\\operatorname{Cov}(X_{i},\\bar{X})=\\frac{1}{n}\\sigma^{2}.\n$$", "answer": "$$\\boxed{\\frac{\\sigma^{2}}{n}}$$", "id": "1947870"}]}