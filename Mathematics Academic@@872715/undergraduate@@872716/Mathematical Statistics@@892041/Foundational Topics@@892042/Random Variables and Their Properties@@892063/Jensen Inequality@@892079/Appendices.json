{"hands_on_practices": [{"introduction": "The core statement of Jensen's inequality is that for a convex function $f$, the expectation of the function is greater than or equal to the function of the expectation, i.e., $E[f(X)] \\geq f(E[X])$. This first exercise provides a direct opportunity to verify this principle through calculation. By working with a simple continuous random variable and the convex function $f(x) = x^4$, you will compute both sides of the inequality and see the abstract theorem yield a concrete numerical result. [@problem_id:1926117]", "problem": "A certain electronic component in a communication system experiences a fluctuating input signal strength, which we model as a random variable $X$. The signal strength $X$ is uniformly distributed over the interval $[0, 1]$ in normalized units. The instantaneous power consumption of the component, denoted by $P$, is found to be directly proportional to the fourth power of the signal strength, such that $P = \\alpha X^4$, where $\\alpha$ is a positive constant representing the component's characteristics.\n\nAn engineer is interested in comparing two different measures of power consumption. The first is the true average power consumption over time, which is given by the expected value of $P$, denoted as $E[P]$. The second is a simplified, hypothetical power consumption, $P_{\\text{avg}}$, which would occur if the component were subjected to a constant signal strength equal to the average signal strength, $E[X]$. This hypothetical power is thus given by $P_{\\text{avg}} = \\alpha (E[X])^4$.\n\nCalculate the specific numerical value of the ratio $\\frac{E[P]}{P_{\\text{avg}}}$. Provide your answer as a decimal value rounded to two significant figures.", "solution": "We model $X$ as uniformly distributed on $[0,1]$, so its probability density function is $f_{X}(x)=1$ for $x\\in[0,1]$ and $0$ otherwise. The instantaneous power is $P=\\alpha X^{4}$ with $\\alpha0$.\n\nUsing linearity of expectation,\n$$\nE[P]=E[\\alpha X^{4}]=\\alpha E[X^{4}].\n$$\nFor $X\\sim\\text{Uniform}(0,1)$,\n$$\nE[X^{4}]=\\int_{0}^{1}x^{4}f_{X}(x)\\,dx=\\int_{0}^{1}x^{4}\\,dx=\\left.\\frac{x^{5}}{5}\\right|_{0}^{1}=\\frac{1}{5},\n$$\nso\n$$\nE[P]=\\frac{\\alpha}{5}.\n$$\nThe average signal strength is\n$$\nE[X]=\\int_{0}^{1}x\\,dx=\\left.\\frac{x^{2}}{2}\\right|_{0}^{1}=\\frac{1}{2}.\n$$\nThus the hypothetical power is\n$$\nP_{\\text{avg}}=\\alpha\\left(E[X]\\right)^{4}=\\alpha\\left(\\frac{1}{2}\\right)^{4}=\\frac{\\alpha}{16}.\n$$\nTherefore, the ratio is\n$$\n\\frac{E[P]}{P_{\\text{avg}}}=\\frac{\\alpha/5}{\\alpha/16}=\\frac{16}{5}=3.2.\n$$\nRounded to two significant figures, the value remains $3.2$.", "answer": "$$\\boxed{3.2}$$", "id": "1926117"}, {"introduction": "Moving from pure mathematics to physics, this problem illustrates Jensen's inequality in a tangible context: kinetic energy. You will explore why the average kinetic energy of a system of particles is not the same as the kinetic energy calculated from their average velocity. This discrepancy, which is a direct consequence of the convexity of the kinetic energy function ($K \\propto v^2$), reveals a fundamental idea: variability in a system often leads to a higher expected \"cost,\" a concept with wide-ranging applications in science and engineering. [@problem_id:1926157]", "problem": "In a simplified model of a rarefied gas, a collection of identical, non-interacting particles is analyzed. Each particle has a mass of $m = 0.500 \\text{ kg}$. The one-dimensional velocity $V$ of any given particle is a random variable that can take on one of two distinct values. Specifically, a particle has a velocity of $v_1 = 10.0 \\text{ m/s}$ with a probability of $p_1 = 0.700$, or a velocity of $v_2 = -8.00 \\text{ m/s}$ with a probability of $p_2 = 0.300$.\n\nWe define two distinct energy quantities for this system:\n1. The *mean kinetic energy*, $\\langle K \\rangle$, which is the statistical expectation of the kinetic energy of a particle, given by $\\langle K \\rangle = E\\left[\\frac{1}{2}mV^2\\right]$.\n2. The *kinetic energy of the mean velocity*, $K_{\\langle v \\rangle}$, which is the kinetic energy calculated using the statistical expectation of the velocity, given by $K_{\\langle v \\rangle} = \\frac{1}{2}m\\left(E[V]\\right)^2$.\n\nCalculate the difference $\\Delta K = \\langle K \\rangle - K_{\\langle v \\rangle}$. Express your final answer in Joules (J), rounded to three significant figures.", "solution": "We use the definitions $\\langle K \\rangle = E\\!\\left[\\frac{1}{2}mV^{2}\\right]$ and $K_{\\langle v \\rangle} = \\frac{1}{2}m\\left(E[V]\\right)^{2}$. The difference is\n$$\n\\Delta K = \\langle K \\rangle - K_{\\langle v \\rangle} = \\frac{1}{2}m\\left(E[V^{2}] - \\left(E[V]\\right)^{2}\\right).\n$$\nWith $P(V=v_{1})=p_{1}=0.700$, $P(V=v_{2})=p_{2}=0.300$, $v_{1}=10.0$, $v_{2}=-8.00$, and $m=0.500$, we compute\n$$\nE[V] = p_{1}v_{1} + p_{2}v_{2} = 0.700\\cdot 10.0 + 0.300\\cdot(-8.00) = 4.6,\n$$\n$$\nE[V^{2}] = p_{1}v_{1}^{2} + p_{2}v_{2}^{2} = 0.700\\cdot 100 + 0.300\\cdot 64 = 89.2.\n$$\nThus\n$$\nE[V^{2}] - \\left(E[V]\\right)^{2} = 89.2 - (4.6)^{2} = 89.2 - 21.16 = 68.04,\n$$\nand therefore\n$$\n\\Delta K = \\frac{1}{2}\\cdot 0.500 \\cdot 68.04 = 0.25 \\cdot 68.04 = 17.01.\n$$\nRounded to three significant figures, this yields $17.0$.", "answer": "$$\\boxed{17.0}$$", "id": "1926157"}, {"introduction": "Jensen's inequality is a powerful tool in theoretical statistics for analyzing the properties of estimators. This exercise applies the inequality to a *concave* function, $f(p) = p(1-p)$, which is the variance of a Bernoulli trial. By doing so, you will uncover why the simple \"plug-in\" estimator for variance is inherently biased, consistently underestimating the true population variance on average. This practice demonstrates that Jensen's inequality is not just for convex functions and is essential for understanding the subtle but critical properties of statistical inference. [@problem_id:1926116]", "problem": "In a quality control process for manufacturing smartphones, each phone is tested to determine if it is defective or non-defective. The outcome for a single phone can be modeled as a Bernoulli trial, where the probability of a phone being defective is $p$. To estimate the variability of this manufacturing process, a random sample of $n$ phones is drawn from the production line.\n\nLet $X_1, X_2, \\ldots, X_n$ be the sequence of independent and identically distributed Bernoulli random variables representing the outcomes for the $n$ phones, where $X_i=1$ if the $i$-th phone is defective and $X_i=0$ otherwise. The sample proportion of defective phones is given by $\\hat{p} = \\frac{1}{n}\\sum_{i=1}^{n} X_i$.\n\nA common way to estimate the true variance of the process, $\\sigma^2 = p(1-p)$, is to use the \"plug-in\" estimator, which is formed by substituting the sample proportion $\\hat{p}$ into the formula for the variance. Let this estimator be denoted by $\\hat{\\sigma}^2 = \\hat{p}(1-\\hat{p})$. The bias of this estimator is defined as $\\text{Bias}(\\hat{\\sigma}^2) = E[\\hat{\\sigma}^2] - \\sigma^2$.\n\nDetermine the bias of the estimator $\\hat{\\sigma}^2$ as a function of the true proportion $p$ and the sample size $n$.", "solution": "We have independent and identically distributed Bernoulli random variables $X_{1},\\ldots,X_{n}$ with parameter $p$, so $E[X_{i}]=p$ and $\\operatorname{Var}(X_{i})=p(1-p)$ for each $i$. The sample proportion is $\\hat{p}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}$, and the plug-in estimator of the variance is $\\hat{\\sigma}^{2}=\\hat{p}(1-\\hat{p})$.\n\nFirst compute $E[\\hat{p}]$ and $\\operatorname{Var}(\\hat{p})$:\n$$\nE[\\hat{p}]=E\\!\\left[\\frac{1}{n}\\sum_{i=1}^{n}X_{i}\\right]=\\frac{1}{n}\\sum_{i=1}^{n}E[X_{i}]=\\frac{1}{n}\\cdot n p=p.\n$$\nUsing independence,\n$$\n\\operatorname{Var}(\\hat{p})=\\operatorname{Var}\\!\\left(\\frac{1}{n}\\sum_{i=1}^{n}X_{i}\\right)=\\frac{1}{n^{2}}\\sum_{i=1}^{n}\\operatorname{Var}(X_{i})=\\frac{1}{n^{2}}\\cdot n\\,p(1-p)=\\frac{p(1-p)}{n}.\n$$\n\nNext compute $E[\\hat{p}^{2}]$ via the identity $\\operatorname{Var}(\\hat{p})=E[\\hat{p}^{2}]-\\left(E[\\hat{p}]\\right)^{2}$:\n$$\nE[\\hat{p}^{2}]=\\operatorname{Var}(\\hat{p})+\\left(E[\\hat{p}]\\right)^{2}=\\frac{p(1-p)}{n}+p^{2}.\n$$\n\nNow evaluate $E[\\hat{\\sigma}^{2}]$:\n$$\nE[\\hat{\\sigma}^{2}]=E[\\hat{p}(1-\\hat{p})]=E[\\hat{p}]-E[\\hat{p}^{2}]=p-\\left(\\frac{p(1-p)}{n}+p^{2}\\right)=p(1-p)-\\frac{p(1-p)}{n}.\n$$\nThus\n$$\nE[\\hat{\\sigma}^{2}]=p(1-p)\\left(1-\\frac{1}{n}\\right)=p(1-p)\\frac{n-1}{n}.\n$$\n\nThe bias is\n$$\n\\text{Bias}(\\hat{\\sigma}^{2})=E[\\hat{\\sigma}^{2}]-\\sigma^{2}=p(1-p)\\frac{n-1}{n}-p(1-p)=-\\frac{p(1-p)}{n}.\n$$\nThis shows the plug-in estimator is downward biased by $\\frac{p(1-p)}{n}$.", "answer": "$$\\boxed{-\\frac{p(1-p)}{n}}$$", "id": "1926116"}]}