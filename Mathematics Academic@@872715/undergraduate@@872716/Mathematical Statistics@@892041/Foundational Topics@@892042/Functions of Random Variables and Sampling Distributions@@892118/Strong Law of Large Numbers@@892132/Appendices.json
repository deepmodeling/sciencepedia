{"hands_on_practices": [{"introduction": "The Strong Law of Large Numbers (SLLN) provides a foundational link between theoretical probability and real-world statistics. It guarantees that, under certain conditions, the average of a long sequence of random outcomes will converge to the expected value. This first practice provides a direct application of this principle, where you will calculate the theoretical mean of a given probability distribution to predict the long-term average of a series of measurements. [@problem_id:1460774]", "problem": "A research team is studying the output of a novel signal processor. Each time the processor runs, it generates a normalized value, which can be modeled as a random variable. A sequence of these values, $X_1, X_2, X_3, \\dots$, is recorded. These values are considered to be independent and identically distributed (i.i.d.) random variables. The theoretical model for the probability distribution of any single value $X_i$ is described by the probability density function (PDF):\n$$\nf(x) = \\begin{cases} 3x^2  \\text{for } 0 \\le x \\le 1 \\\\ 0  \\text{otherwise} \\end{cases}\n$$\nThe long-term average of these measurements after $n$ trials is given by the sample mean $S_n = \\frac{1}{n}\\sum_{i=1}^{n} X_i$. As the number of trials $n$ grows infinitely large, the value of $S_n$ will converge almost surely to a specific constant.\n\nDetermine the value of this constant. Express your answer as a fraction in simplest form.", "solution": "We are given independent and identically distributed random variables $X_{1},X_{2},\\dots$ with common density\n$$\nf(x)=\\begin{cases}\n3x^{2},  0\\le x\\le 1,\\\\\n0,  \\text{otherwise.}\n\\end{cases}\n$$\nFirst, verify that $f$ is a valid probability density by checking normalization:\n$$\n\\int_{-\\infty}^{\\infty} f(x)\\,dx=\\int_{0}^{1}3x^{2}\\,dx=3\\left[\\frac{x^{3}}{3}\\right]_{0}^{1}=1.\n$$\nBy the Strong Law of Large Numbers, since the $X_{i}$ are i.i.d. with finite mean $E[|X_{1}|]\\infty$ (which holds because $0\\le X_{1}\\le 1$ almost surely), the sample mean\n$$\nS_{n}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}\n$$\nconverges almost surely to $E[X_{1}]$ as $n\\to\\infty$.\n\nCompute the expectation:\n$$\nE[X_{1}]=\\int_{-\\infty}^{\\infty} x f(x)\\,dx=\\int_{0}^{1} x\\cdot 3x^{2}\\,dx=3\\int_{0}^{1} x^{3}\\,dx=3\\left[\\frac{x^{4}}{4}\\right]_{0}^{1}=\\frac{3}{4}.\n$$\nTherefore,\n$$\n\\lim_{n\\to\\infty} S_{n}=\\frac{3}{4}\\quad \\text{almost surely}.\n$$", "answer": "$$\\boxed{\\frac{3}{4}}$$", "id": "1460774"}, {"introduction": "Moving beyond one-dimensional scenarios, the SLLN is equally powerful in geometric contexts. Imagine scattering points randomly across a surface; the SLLN can predict the average properties of these points. In this exercise, you'll first need to derive the probability distribution for a point's distance from the center of a disk and then use this to find the long-term average distance you'd expect to observe. [@problem_id:1957055]", "problem": "Consider a sequence of points, $P_1, P_2, P_3, \\dots$, that are chosen independently from the same distribution, formally known as being independent and identically distributed (i.i.d.). The points are selected from a uniform distribution over the unit disk, defined as the set of all points $(x, y)$ in a Cartesian plane satisfying the inequality $x^2 + y^2 \\le 1$.\n\nFor each point $P_n$ in the sequence, let $D_n$ be its Euclidean distance from the center of the disk, the origin $(0,0)$. We are interested in the long-term behavior of the average of these distances. Let $\\bar{D}_N$ be the arithmetic mean of the first $N$ distances, that is, $\\bar{D}_N = \\frac{1}{N} \\sum_{n=1}^{N} D_n$.\n\nDetermine the value to which the average distance $\\bar{D}_N$ converges almost surely (i.e., with probability 1) as the number of points $N$ approaches infinity. Present your answer as a single closed-form analytic expression.", "solution": "Let $\\{P_{n}\\}_{n\\geq 1}$ be i.i.d. uniformly distributed on the unit disk $\\{(x,y): x^{2}+y^{2}\\leq 1\\}$, and let $D_{n}$ be the Euclidean distance from $P_{n}$ to the origin. The strong law of large numbers states that for i.i.d. random variables with finite mean,\n$$\n\\bar{D}_{N}=\\frac{1}{N}\\sum_{n=1}^{N}D_{n}\\xrightarrow[\\;N\\to\\infty\\;]{\\text{a.s.}}\\mathbb{E}[D_{1}],\n$$\nprovided $\\mathbb{E}[|D_{1}|]\\infty$. Since $0\\leq D_{1}\\leq 1$, we have $\\mathbb{E}[|D_{1}|]\\infty$. Therefore, it suffices to compute $\\mathbb{E}[D_{1}]$.\n\nLet $R$ denote the distance from the origin for a point uniformly distributed on the unit disk. Then\n$$\n\\mathbb{P}(R\\leq r)=\\frac{\\text{area of the disk of radius }r}{\\text{area of the unit disk}}=\\frac{\\pi r^{2}}{\\pi}=r^{2},\\quad 0\\leq r\\leq 1.\n$$\nDifferentiating gives the radial density\n$$\nf_{R}(r)=\\frac{d}{dr}\\big(r^{2}\\big)=2r,\\quad 0\\leq r\\leq 1.\n$$\nHence,\n$$\n\\mathbb{E}[R]=\\int_{0}^{1}r\\,f_{R}(r)\\,dr=\\int_{0}^{1}r\\cdot 2r\\,dr=\\int_{0}^{1}2r^{2}\\,dr=\\left.\\frac{2}{3}r^{3}\\right|_{0}^{1}=\\frac{2}{3}.\n$$\nBy the strong law of large numbers,\n$$\n\\bar{D}_{N}\\xrightarrow[\\;N\\to\\infty\\;]{\\text{a.s.}}\\mathbb{E}[D_{1}]=\\frac{2}{3}.\n$$\nThus the almost sure limit of the average distance is $\\frac{2}{3}$.", "answer": "$$\\boxed{\\frac{2}{3}}$$", "id": "1957055"}, {"introduction": "Often in statistics and data analysis, we are interested not just in a sample mean, but in a quantity derived from it, such as the sample variance. This final practice explores how the SLLN works in conjunction with the Continuous Mapping Theorem. You will see how the almost sure convergence of a sample average implies the convergence of a continuous function applied to that average, a vital tool for analyzing more complex estimators. [@problem_id:1957105]", "problem": "A company specializes in running large-scale Monte Carlo simulations to price complex financial derivatives. A key component of their simulation involves repeatedly modeling a binary event, such as a stock price moving up or down. For a particular simulation, this is modeled as a sequence of experiments. In each experiment $i$, a total of $m$ independent Bernoulli trials are run, each with a success probability of $p$. Let the random variable $X_i$ be the total number of successes in experiment $i$.\n\nThe experiments are independent of each other, so the sequence of counts $X_1, X_2, \\ldots, X_n, \\ldots$ can be modeled as a sequence of independent and identically distributed (i.i.d.) random variables, with each $X_i$ following a binomial distribution with parameters $m$ and $p$.\n\nAnalysts at the company are studying the convergence properties of their estimators. They define the average proportion of successes over the first $n$ experiments as:\n$$ \\bar{p}_n = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{X_i}{m} $$\nThey are interested in a quantity related to the variance of the underlying process. Determine the value to which the quantity $V_n = \\bar{p}_n (1 - \\bar{p}_n)$ converges almost surely as the number of experiments $n$ approaches infinity. Your answer should be a symbolic expression in terms of the given parameters.", "solution": "Let $X_{i} \\sim \\text{Binomial}(m,p)$ be i.i.d. and define $Y_{i} = \\frac{X_{i}}{m}$. Then the average proportion is the sample mean\n$$\n\\bar{p}_{n} = \\frac{1}{n}\\sum_{i=1}^{n} Y_{i}.\n$$\nWe compute the first two moments of $Y_{i}$ using linearity of expectation and the variance scaling rule:\n$$\n\\mathbb{E}[Y_{i}] = \\frac{1}{m}\\mathbb{E}[X_{i}] = \\frac{1}{m}\\cdot m p = p,\n$$\n$$\n\\operatorname{Var}(Y_{i}) = \\frac{1}{m^{2}}\\operatorname{Var}(X_{i}) = \\frac{1}{m^{2}}\\cdot m p(1-p) = \\frac{p(1-p)}{m}.\n$$\nThus $\\{Y_{i}\\}$ are i.i.d. with finite mean $\\mathbb{E}[Y_{i}]=p$. By the Strong Law of Large Numbers,\n$$\n\\bar{p}_{n} = \\frac{1}{n}\\sum_{i=1}^{n} Y_{i} \\xrightarrow{\\text{a.s.}} \\mathbb{E}[Y_{1}] = p.\n$$\n\nDefine the continuous function $g(x) = x(1-x)$. By the continuous mapping theorem applied to almost sure convergence,\n$$\nV_{n} = g(\\bar{p}_{n}) = \\bar{p}_{n}\\bigl(1-\\bar{p}_{n}\\bigr) \\xrightarrow{\\text{a.s.}} g(p) = p(1-p).\n$$\n\nTherefore, $V_{n}$ converges almost surely to $p(1-p)$.", "answer": "$$\\boxed{p(1-p)}$$", "id": "1957105"}]}