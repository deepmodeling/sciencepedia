{"hands_on_practices": [{"introduction": "A fundamental task in order statistics is to determine the properties of the extreme values in a random sample. This first practice focuses on the smallest value, or the first order statistic. Understanding the behavior of the minimum is crucial in fields like reliability engineering, where it can model the time to first failure of a system with multiple components. This exercise provides a concrete opportunity to derive the probability density function (PDF) for the minimum of two variables and then use it to calculate its expected value, a core skill for any statistician. [@problem_id:13330]", "problem": "Consider two independent and identically distributed (i.i.d.) continuous random variables, $X_1$ and $X_2$. Their common probability density function (PDF) is given by\n$$\nf(x) = \\begin{cases} 2x  \\text{for } 0 \\le x \\le 1 \\\\ 0  \\text{otherwise} \\end{cases}\n$$\nLet a new random variable $Y$ be defined as the minimum of these two variables, that is, $Y = \\min(X_1, X_2)$.\n\nDerive the expected value of the random variable $Y$, denoted as $E[Y]$.", "solution": "The CDF of $X_i$ for $0\\le x\\le1$ is \n$$F(x)=\\int_{0}^{x}2t\\,dt=x^2.$$\nFor $n=2$ and the minimum $Y=\\min(X_1,X_2)$, the PDF is\n$$f_Y(y)=2\\bigl[1-F(y)\\bigr]\\,f(y)\n=2\\bigl(1-y^2\\bigr)\\,2y=4y(1-y^2),\\quad 0\\le y\\le1.$$\nHence\n$$E[Y]=\\int_{0}^{1}y\\,f_Y(y)\\,dy\n=\\int_{0}^{1}4y^2(1-y^2)\\,dy\n=4\\Bigl[\\tfrac{1}{3}-\\tfrac{1}{5}\\Bigr]\n=4\\cdot\\tfrac{2}{15}\n=\\tfrac{8}{15}.$$", "answer": "$$\\boxed{\\frac{8}{15}}$$", "id": "13330"}, {"introduction": "After examining the minimum, we now turn our attention to the maximum value in a sample, the $n$-th order statistic. The sample maximum is a vital statistic, frequently used as an estimator for population parameters, particularly in cases like the uniform distribution. This problem generalizes our scope from a fixed sample of two to an arbitrary sample of size $n$. You will practice using the cumulative distribution function (CDF) to derive the distribution of the maximum, a powerful and efficient technique that is especially useful for maximums. [@problem_id:13340]", "problem": "Consider a set of $n$ random variables, $X_1, X_2, \\ldots, X_n$. These variables are independent and identically distributed (i.i.d.), each drawn from a continuous Uniform distribution on the interval $(0, \\theta)$, where $\\theta  0$. The probability density function (PDF) for any given $X_i$ is:\n$$\nf_X(x) = \\begin{cases} \\frac{1}{\\theta}  \\text{if } 0  x  \\theta \\\\ 0  \\text{otherwise} \\end{cases}\n$$\nLet a new random variable, $Y$, be defined as the maximum value among this set:\n$$\nY = \\max(X_1, X_2, \\ldots, X_n)\n$$\nThis variable $Y$ is known as the $n$-th order statistic of the sample.\n\nDerive the expected value of $Y$, denoted as $E[Y]$, in terms of the number of variables $n$ and the parameter $\\theta$.", "solution": "The maximum $Y=\\max\\{X_1,\\dots,X_n\\}$ has CDF\n$$\nF_Y(y)=P(Y\\le y)=P(X_1\\le y,\\dots,X_n\\le y)\n=\\bigl(F_X(y)\\bigr)^n\n=\\Bigl(\\frac y\\theta\\Bigr)^n,\\quad 0y\\theta.\n$$\nDifferentiating gives the PDF\n$$\nf_Y(y)=\\frac{d}{dy}F_Y(y)\n=\\frac{d}{dy}\\Bigl(\\frac y\\theta\\Bigr)^n\n=\\frac{n\\,y^{n-1}}{\\theta^n},\\quad 0y\\theta.\n$$\nHence the expectation is\n$$\nE[Y]\n=\\int_0^\\theta y\\,f_Y(y)\\,dy\n=\\int_0^\\theta y\\frac{n\\,y^{n-1}}{\\theta^n}\\,dy\n=\\frac{n}{\\theta^n}\\int_0^\\theta y^n\\,dy\n=\\frac{n}{\\theta^n}\\cdot\\frac{\\theta^{n+1}}{n+1}\n=\\frac{n\\,\\theta}{n+1}.\n$$", "answer": "$$\\boxed{\\frac{n\\theta}{n+1}}$$", "id": "13340"}, {"introduction": "Moving beyond the properties of individual order statistics, we can explore the relationship between them. The minimum, $X_{(1)}$, and maximum, $X_{(2)}$, of a sample define a random interval $[X_{(1)}, X_{(2)}]$. This exercise shifts from calculation to a more conceptual understanding of how these statistics behave together. By calculating the probability that this random interval \"covers\" a specific point, you will gain insight into the foundational ideas behind statistical confidence intervals and coverage probability. [@problem_id:13355]", "problem": "Let $X_1$ and $X_2$ be two independent and identically distributed (i.i.d.) random variables, each following a continuous uniform distribution on the interval $(0, 1)$. The probability density function (PDF) for each variable $X_i$ is given by:\n$$\nf(x) = \\begin{cases} \n1  \\text{if } 0  x  1 \\\\\n0  \\text{otherwise} \n\\end{cases}\n$$\nLet $X_{(1)}$ be the minimum of these two variables, $X_{(1)} = \\min(X_1, X_2)$, and let $X_{(2)}$ be the maximum, $X_{(2)} = \\max(X_1, X_2)$. These are the order statistics for a sample of size $n=2$.\n\nDerive the probability that the random interval $[X_{(1)}, X_{(2)}]$ contains the point $p = \\frac{1}{2}$.", "solution": "We want the probability that the random interval $[X_{(1)},X_{(2)}]$ contains $p=\\tfrac12$, i.e. \n$$P\\bigl(X_{(1)}\\le p\\le X_{(2)}\\bigr).$$\nSince $\\{X_{(1)}\\le p\\le X_{(2)}\\}$ is the complement of “both observations lie strictly on one side of $p$”, we write\n$$\nP\\bigl(X_{(1)}\\le p\\le X_{(2)}\\bigr)\n=1-P\\bigl(X_{(2)}p\\bigr)-P\\bigl(X_{(1)}p\\bigr).\n$$\nFor two iid $U(0,1)$ variables,\n$$\nP\\bigl(X_{(2)}p\\bigr)=P(X_1p,\\;X_2p)=p^2,\n\\quad\nP\\bigl(X_{(1)}p\\bigr)=P(X_1p,\\;X_2p)=(1-p)^2.\n$$\nHence\n$$\nP\\bigl(X_{(1)}\\le p\\le X_{(2)}\\bigr)\n=1-\\bigl(p^2+(1-p)^2\\bigr)\n=1-\\bigl(p^2+1-2p+p^2\\bigr)\n=2p-2p^2\n=2p(1-p).\n$$\nSubstituting $p=\\tfrac12$ gives\n$$\nP=\\;2\\cdot\\tfrac12\\cdot\\bigl(1-\\tfrac12\\bigr)\n=\\tfrac12.\n$$", "answer": "$$\\boxed{\\tfrac12}$$", "id": "13355"}]}