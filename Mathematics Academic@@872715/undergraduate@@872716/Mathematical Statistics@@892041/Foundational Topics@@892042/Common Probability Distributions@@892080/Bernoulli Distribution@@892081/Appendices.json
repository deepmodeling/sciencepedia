{"hands_on_practices": [{"introduction": "Variance is a key measure that quantifies the uncertainty or unpredictability of a random outcome. For a binary event modeled by a Bernoulli distribution, when is the outcome the most difficult to predict? This practice guides you to answer this fundamental question by finding the success probability $p$ that maximizes the variance, a concept crucial for understanding information and randomness. [@problem_id:1899936]", "problem": "A team of data scientists is designing a machine learning model to classify emails as either \"spam\" or \"not spam\". The outcome for a single email is modeled as a Bernoulli random variable, $X$, where $X=1$ corresponds to the email being classified as \"spam\" (a \"success\") and $X=0$ corresponds to \"not spam\". The probability that the model classifies a randomly selected email as spam is given by a parameter $p$, where $p$ is a real number in the range $0 \\le p \\le 1$.\n\nFor testing the robustness of their system, the team is interested in the scenario that represents the highest level of unpredictability in the model's classification output. In statistical terms, this corresponds to finding the value of the parameter $p$ that maximizes the variance of the random variable $X$.\n\nDetermine this value of $p$. Express your answer as a single decimal or a fraction.", "solution": "Let $X$ be Bernoulli with parameter $p$, so $\\Pr(X=1)=p$ and $\\Pr(X=0)=1-p$. The mean and second moment are\n$$\n\\mathbb{E}[X]=0\\cdot(1-p)+1\\cdot p=p, \\quad \\mathbb{E}[X^{2}]=0^{2}\\cdot(1-p)+1^{2}\\cdot p=p.\n$$\nThe variance is\n$$\n\\operatorname{Var}(X)=\\mathbb{E}[X^{2}]-(\\mathbb{E}[X])^{2}=p-p^{2}=p(1-p).\n$$\nTo maximize $\\operatorname{Var}(X)$ over $p\\in[0,1]$, define $v(p)=p(1-p)$. Then\n$$\nv'(p)=1-2p, \\quad v''(p)=-2.\n$$\nSetting $v'(p)=0$ gives $p=\\frac{1}{2}$. Since $v''(p)=-2<0$, this critical point is a local maximum. Checking the endpoints,\n$$\nv(0)=0, \\quad v(1)=0, \\quad v\\!\\left(\\frac{1}{2}\\right)=\\frac{1}{4},\n$$\nso the global maximum on $[0,1]$ occurs at $p=\\frac{1}{2}$.", "answer": "$$\\boxed{\\frac{1}{2}}$$", "id": "1899936"}, {"introduction": "Building on the concept of maximum variance, we can further explore the relationship between the parameter $p$ and the distribution's spread. Because the variance function $v(p) = p(1-p)$ is symmetric around its maximum, a specific level of variance (below the maximum) will correspond to two distinct probability values. This exercise challenges you to work backward from a given variance to find the two possible underlying probabilities, reinforcing your understanding of the variance formula. [@problem_id:1392758]", "problem": "A data analytics firm is studying consumer behavior for a new online subscription service. The action of a single, randomly selected consumer either purchasing the subscription or not is modeled as a discrete random event. A random variable $X$ is defined to represent this outcome: $X=1$ if the consumer makes a purchase, and $X=0$ if they do not. After analyzing a large sample of consumers, the firm determines that the variance of this random variable, $\\text{Var}(X)$, is $0.21$.\n\nLet $p$ represent the probability that a consumer makes a purchase. Based on the given variance, determine the two possible values for $p$. Present your two answers as decimal numbers in ascending order.", "solution": "Let $X$ be a Bernoulli random variable with success probability $p$. The variance of a Bernoulli random variable is given by\n$$\n\\mathrm{Var}(X)=p(1-p).\n$$\nWe are given $\\mathrm{Var}(X)=0.21$, so\n$$\np(1-p)=0.21.\n$$\nRewriting,\n$$\np-p^{2}=0.21 \\;\\;\\Longrightarrow\\;\\; p^{2}-p+0.21=0.\n$$\nSolving the quadratic equation using the quadratic formula,\n$$\np=\\frac{1\\pm \\sqrt{1-4\\cdot 0.21}}{2}=\\frac{1\\pm \\sqrt{0.16}}{2}=\\frac{1\\pm 0.4}{2}.\n$$\nThus the two solutions are\n$$\np=\\frac{1-0.4}{2}=0.3 \\quad \\text{and} \\quad p=\\frac{1+0.4}{2}=0.7,\n$$\nboth of which lie in the interval $[0,1]$. In ascending order, these are $0.3$ and $0.7$.", "answer": "$$\\boxed{\\begin{pmatrix}0.3 & 0.7\\end{pmatrix}}$$", "id": "1392758"}, {"introduction": "The principles of Bernoulli trials enable elegant solutions to practical engineering challenges, such as generating unbiased random numbers. Imagine you have a biased coin but need to simulate a fair one; how could you achieve this? This problem introduces a classic and ingenious algorithm, attributed to John von Neumann, to construct a perfectly fair random process from a biased source by cleverly grouping outcomes. [@problem_id:1392786]", "problem": "An engineer is designing a subsystem for a deep-space probe that relies on a fundamentally noisy physical process to generate random bits. The raw output is a sequence of bits, $X_1, X_2, X_3, \\dots$, which can be modeled as a sequence of independent and identically distributed Bernoulli random variables. For each bit $X_i$, the probability of it being a '1' is an unknown but constant value $p$, and the probability of it being a '0' is $1-p$. The system operates under the constraint that $p$ is strictly between 0 and 1, i.e., $p \\in (0,1)$.\n\nTo produce a \"fair\" bit (i.e., a bit with a probability of 0.5 of being '1'), the engineer implements the following algorithm, often attributed to John von Neumann:\n1.  The raw bits are consumed in non-overlapping consecutive pairs: $(X_1, X_2), (X_3, X_4), (X_5, X_6), \\dots$.\n2.  Each pair is processed according to the following rules:\n    *   If the pair is $(0,1)$, the algorithm outputs a single bit '0' and halts.\n    *   If the pair is $(1,0)$, the algorithm outputs a single bit '1' and halts.\n    *   If the pair is $(0,0)$ or $(1,1)$, the pair is discarded, and the algorithm proceeds to the next pair to repeat the process.\n\nThis procedure is guaranteed to eventually produce one output bit. Let's call this first generated output bit $Y$.\n\n(a) Calculate the probability that the output bit is a '1', i.e., find $P(Y=1)$.\n(b) Calculate the expected number of raw bits from the source sequence ($X_i$) that must be consumed to produce the single output bit $Y$.\n\nProvide your answer as a row matrix containing the symbolic expressions for Part (a) and Part (b), in that order. Express your answer for Part (b) in terms of $p$.", "solution": "Let the raw bits be independent and identically distributed with $P(X_{i}=1)=p$ and $P(X_{i}=0)=1-p$, with $p\\in(0,1)$. Consider disjoint consecutive pairs $(X_{1},X_{2}), (X_{3},X_{4}), \\dots$. By independence and identical distribution of the $X_{i}$, these pairs are independent and identically distributed.\n\nFor a single pair, the probabilities of pair outcomes are:\n$$\nP((1,0))=p(1-p),\\quad P((0,1))=(1-p)p,\\quad P(\\text{equal})=P((0,0))+P((1,1))=p^{2}+(1-p)^{2}.\n$$\nDefine $q=P(\\text{unequal})=P((1,0))+P((0,1))=2p(1-p)$.\n\nThe algorithm halts at the first pair that is unequal. Let $K$ be the index of this first unequal pair. Then $K$ is a geometric random variable on $\\{1,2,\\dots\\}$ with success probability $q=2p(1-p)$:\n$$\nP(K=k)=\\left(p^{2}+(1-p)^{2}\\right)^{k-1}\\cdot 2p(1-p).\n$$\n\n(a) The algorithm outputs $Y=1$ exactly when the first unequal pair is $(1,0)$. Thus\n$$\nP(Y=1)=\\sum_{k=1}^{\\infty}\\left(p^{2}+(1-p)^{2}\\right)^{k-1}\\cdot p(1-p)\n= p(1-p)\\sum_{k=1}^{\\infty}\\left(p^{2}+(1-p)^{2}\\right)^{k-1}.\n$$\nThis is a geometric series with ratio $r=p^{2}+(1-p)^{2}$, so\n$$\n\\sum_{k=1}^{\\infty}r^{\\,k-1}=\\frac{1}{1-r}=\\frac{1}{1-\\left(p^{2}+(1-p)^{2}\\right)}=\\frac{1}{2p(1-p)}.\n$$\nTherefore,\n$$\nP(Y=1)=p(1-p)\\cdot \\frac{1}{2p(1-p)}=\\frac{1}{2}.\n$$\n\n(b) Each processed pair consumes exactly $2$ raw bits. Since $K$ is geometric with success probability $q=2p(1-p)$, its expectation is $E[K]=\\frac{1}{q}=\\frac{1}{2p(1-p)}$. Hence the expected number of raw bits consumed is\n$$\nE[\\text{bits}]=2E[K]=\\frac{2}{2p(1-p)}=\\frac{1}{p(1-p)}.\n$$\nThus the required expressions are $P(Y=1)=\\frac{1}{2}$ and $E[\\text{bits}]=\\frac{1}{p(1-p)}$.", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{2} & \\frac{1}{p(1-p)}\\end{pmatrix}}$$", "id": "1392786"}]}