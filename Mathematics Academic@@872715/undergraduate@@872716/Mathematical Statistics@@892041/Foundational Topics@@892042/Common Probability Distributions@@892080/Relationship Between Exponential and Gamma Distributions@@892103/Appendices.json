{"hands_on_practices": [{"introduction": "The cornerstone of the relationship between the exponential and Gamma distributions lies in the summation of independent, identically distributed (i.i.d.) exponential random variables. When we sum $k$ such variables, the result is a random variable that follows a Gamma distribution. This first exercise provides a direct, hands-on application of this principle, asking you to calculate the standard deviation for the total waiting time of a series of events modeled by a Poisson process [@problem_id:1950915].", "problem": "The time intervals, in minutes, between consecutive arrivals of a city bus at a specific stop are modeled as independent and identically distributed exponential random variables. The expected time between any two consecutive arrivals is 10 minutes. A statistician begins observing the stop at time $t=0$, just as one bus is departing. What is the standard deviation of the waiting time until the 16th bus arrives? Provide your answer in minutes.", "solution": "Let $X_{1}, X_{2}, \\ldots, X_{16}$ be the i.i.d. interarrival times, each exponentially distributed with rate $\\lambda$ and mean $E[X_{i}] = \\frac{1}{\\lambda} = 10$. Hence $\\lambda = \\frac{1}{10}$ per minute. The waiting time until the 16th bus arrives is the sum\n$$\nT_{16} = \\sum_{i=1}^{16} X_{i}.\n$$\nBy independence and the variance of an exponential random variable, $\\operatorname{Var}(X_{i}) = \\frac{1}{\\lambda^{2}}$, we have\n$$\n\\operatorname{Var}(T_{16}) = \\sum_{i=1}^{16} \\operatorname{Var}(X_{i}) = \\frac{16}{\\lambda^{2}}.\n$$\nTherefore, the standard deviation is\n$$\n\\sigma_{T_{16}} = \\sqrt{\\operatorname{Var}(T_{16})} = \\sqrt{\\frac{16}{\\lambda^{2}}} = \\frac{4}{\\lambda}.\n$$\nSubstituting $\\lambda = \\frac{1}{10}$ gives\n$$\n\\sigma_{T_{16}} = \\frac{4}{\\frac{1}{10}} = 40,\n$$\nin minutes.", "answer": "$$\\boxed{40}$$", "id": "1950915"}, {"introduction": "Moving from fundamental properties to practical applications, we now explore how the Gamma distribution is a powerful tool for modeling system reliability. Many real-world systems, from industrial machinery to spacecraft components, rely on backup parts that activate sequentially. This problem challenges you to calculate the probability that a system with two such components, each with an exponential lifetime, will function beyond a critical time threshold, demonstrating a common task in reliability engineering [@problem_id:1950932].", "problem": "A deep-space probe is equipped with a primary and a secondary communication module. The modules operate in a sequential, 'cold standby' mode: the secondary module is only activated upon the failure of the primary one. The operational lifetime of each module is independent and can be modeled by an exponential distribution with a mean lifetime of $\\mu = 4000$ hours.\n\nCalculate the probability that the total operational lifetime provided by both modules exceeds 5000 hours. Round your final answer to four significant figures.", "solution": "Let $X_{1}$ and $X_{2}$ denote the independent operational lifetimes (in hours) of the primary and secondary modules, respectively. Each is exponentially distributed with mean $\\mu=4000$, so the rate is $\\lambda=\\frac{1}{\\mu}$. In cold standby, the total operational lifetime is the sum $T=X_{1}+X_{2}$.\n\nSince $X_{1}$ and $X_{2}$ are independent and identically distributed exponential random variables with rate $\\lambda$, $T$ has an Erlang (Gamma) distribution with shape $k=2$ and rate $\\lambda$. The density of $T$ is\n$$\nf_{T}(t)=\\lambda^{2} t \\exp(-\\lambda t), \\quad t \\ge 0.\n$$\nThe survival function is\n$$\nS_{T}(t)=P(T>t)=\\int_{t}^{\\infty} \\lambda^{2} s \\exp(-\\lambda s)\\, ds.\n$$\nIntegrating by parts with $u=s$ and $dv=\\lambda^{2}\\exp(-\\lambda s)\\,ds$ gives $v=-\\lambda \\exp(-\\lambda s)$, hence\n$$\nS_{T}(t)=\\left[s(-\\lambda \\exp(-\\lambda s))\\right]_{t}^{\\infty}-\\int_{t}^{\\infty}(-\\lambda \\exp(-\\lambda s))\\, ds\n= \\lambda t \\exp(-\\lambda t)+\\exp(-\\lambda t)\n=\\exp(-\\lambda t)\\left(1+\\lambda t\\right).\n$$\nTherefore,\n$$\nP(T>5000)=\\exp\\!\\left(-\\lambda \\cdot 5000\\right)\\left(1+\\lambda \\cdot 5000\\right)\n=\\exp\\!\\left(-\\frac{5000}{\\mu}\\right)\\left(1+\\frac{5000}{\\mu}\\right).\n$$\nWith $\\mu=4000$, this becomes\n$$\nP(T>5000)=\\exp(-1.25)\\cdot 2.25.\n$$\nNumerically, $\\exp(-1.25)\\approx 0.2865047969$, so\n$$\nP(T>5000)\\approx 2.25 \\times 0.2865047969=0.6446357926,\n$$\nwhich rounds to four significant figures as $0.6446$.", "answer": "$$\\boxed{0.6446}$$", "id": "1950932"}, {"introduction": "Our final practice explores the deeper statistical structure of arrival times in a Poisson process. The time to the $k$-th event, $T_k$, and the time to a later $n$-th event, $T_n$, are inherently related because they share the same initial sequence of arrivals. This exercise guides you to quantify this dependence by calculating their covariance, revealing a simple and elegant relationship that deepens your understanding of stochastic processes [@problem_id:1384707].", "problem": "A specialized detector records the arrival times of individual photons from a distant, stable star. The arrivals of these photons follow a Poisson process with a constant average rate of $\\lambda$ photons per second. Let $T_m$ denote the random variable representing the time (in seconds) at which the $m$-th photon is detected, with the measurement starting at time $t=0$ (so $T_0 = 0$).\n\nConsider two positive integers $k$ and $n$ such that $n > k$. Your task is to determine the covariance between the detection time of the $k$-th photon, $T_k$, and the detection time of the $n$-th photon, $T_n$. Provide your answer as a symbolic expression in terms of $k$, $n$, and $\\lambda$.", "solution": "In a Poisson process with constant rate $\\lambda$, the interarrival times $X_{i}$ between successive photon detections are independent and identically distributed with the exponential distribution of rate $\\lambda$. Thus $X_{i}$ are independent with $E[X_{i}] = \\frac{1}{\\lambda}$ and $\\operatorname{Var}(X_{i}) = \\frac{1}{\\lambda^{2}}$.\n\nThe detection times are partial sums of these interarrival times:\n$$\nT_{k} = \\sum_{i=1}^{k} X_{i}, \\quad T_{n} = \\sum_{i=1}^{n} X_{i}, \\quad \\text{with } n>k.\n$$\nRewrite $T_{n}$ as\n$$\nT_{n} = T_{k} + Y, \\quad \\text{where } Y = \\sum_{i=k+1}^{n} X_{i}.\n$$\nBy independence of the interarrival times, $T_{k}$, which is a function of $X_{1},\\dots,X_{k}$, is independent of $Y$, which is a function of $X_{k+1},\\dots,X_{n}$. Using bilinearity of covariance and the fact that covariance of independent random variables is zero,\n$$\n\\operatorname{Cov}(T_{k}, T_{n}) = \\operatorname{Cov}(T_{k}, T_{k} + Y) = \\operatorname{Cov}(T_{k}, T_{k}) + \\operatorname{Cov}(T_{k}, Y) = \\operatorname{Var}(T_{k}) + 0 = \\operatorname{Var}(T_{k}).\n$$\nTo compute $\\operatorname{Var}(T_{k})$, use variance additivity for independent sums:\n$$\n\\operatorname{Var}(T_{k}) = \\operatorname{Var}\\!\\left(\\sum_{i=1}^{k} X_{i}\\right) = \\sum_{i=1}^{k} \\operatorname{Var}(X_{i}) = \\sum_{i=1}^{k} \\frac{1}{\\lambda^{2}} = \\frac{k}{\\lambda^{2}}.\n$$\nTherefore,\n$$\n\\operatorname{Cov}(T_{k}, T_{n}) = \\frac{k}{\\lambda^{2}} \\quad \\text{for } n>k.\n$$", "answer": "$$\\boxed{\\frac{k}{\\lambda^{2}}}$$", "id": "1384707"}]}