{"hands_on_practices": [{"introduction": "Understanding how random variables interact is central to statistics. A common task is to create a new variable by combining existing ones, such as calculating a portfolio's return from the returns of individual assets. This first exercise [@problem_id:1488] guides you through deriving the variance of a linear combination of two bivariate normal variables. Completing this practice will reinforce your understanding of how individual variances and their covariance contribute to the overall variability of the composite variable.", "problem": "Let the random variables $X$ and $Y$ follow a bivariate normal distribution. The joint probability density function (PDF) is given by $f(x, y)$, but for this problem, you only need to know the parameters that define this distribution:\n- The mean of $X$ is $E[X] = \\mu_X$.\n- The mean of $Y$ is $E[Y] = \\mu_Y$.\n- The variance of $X$ is $\\text{Var}(X) = \\sigma_X^2$.\n- The variance of $Y$ is $\\text{Var}(Y) = \\sigma_Y^2$.\n- The correlation coefficient between $X$ and $Y$ is $\\rho$.\n\nRecall that the covariance between $X$ and $Y$ is defined as $\\text{Cov}(X, Y) = E[(X - \\mu_X)(Y - \\mu_Y)]$, and it is related to the correlation coefficient by $\\text{Cov}(X, Y) = \\rho \\sigma_X \\sigma_Y$.\n\nConsider a new random variable $Z$ which is a linear combination of $X$ and $Y$, defined as:\n$$\nZ = aX + bY\n$$\nwhere $a$ and $b$ are arbitrary real constants.\n\nUsing the fundamental properties of variance and covariance, derive an expression for the variance of $Z$, denoted as $\\text{Var}(Z)$, in terms of the constants $a, b$ and the parameters of the distribution $\\sigma_X, \\sigma_Y, \\rho$.", "solution": "We seek $\\operatorname{Var}(Z)$ for $Z=aX+bY$.  Using the bilinearity of variance and covariance, we have  \n$$\n\\operatorname{Var}(Z)=\\operatorname{Var}(aX+bY)\n=\\operatorname{Var}(aX)+\\operatorname{Var}(bY)+2\\,\\operatorname{Cov}(aX,bY).\n$$\nBy properties of variance and covariance,  \n$$\n\\operatorname{Var}(aX)=a^2\\operatorname{Var}(X)=a^2\\sigma_X^2,\\quad\n\\operatorname{Var}(bY)=b^2\\operatorname{Var}(Y)=b^2\\sigma_Y^2,\n$$\n$$\n\\operatorname{Cov}(aX,bY)=ab\\,\\operatorname{Cov}(X,Y)=ab\\,\\rho\\,\\sigma_X\\sigma_Y.\n$$\nCombining these terms gives  \n$$\n\\operatorname{Var}(Z)=a^2\\sigma_X^2+b^2\\sigma_Y^2+2ab\\,\\rho\\,\\sigma_X\\sigma_Y.\n$$", "answer": "$$\\boxed{a^2\\sigma_X^2 + b^2\\sigma_Y^2 + 2ab\\,\\rho\\,\\sigma_X\\sigma_Y}$$", "id": "1488"}, {"introduction": "One of the most powerful features of the bivariate normal distribution is its application to prediction. If we know the value of one variable, what is our best estimate for the other? This next practice [@problem_id:1521] explores this by asking you to derive the conditional expectation $E[Y|X=x]$. This derivation reveals a remarkable and fundamental result: the expected value of $Y$ given $X=x$ is a simple linear function of $x$, providing the theoretical foundation for linear regression.", "problem": "Two random variables, $X$ and $Y$, are said to follow a bivariate normal distribution if their joint probability density function (PDF), $f_{X,Y}(x,y)$, is given by:\n$$\nf_{X,Y}(x,y) = \\frac{1}{2\\pi\\sigma_X\\sigma_Y\\sqrt{1-\\rho^2}} \\exp\\left( -\\frac{1}{2(1-\\rho^2)} \\left[ \\left(\\frac{x-\\mu_X}{\\sigma_X}\\right)^2 - 2\\rho\\left(\\frac{x-\\mu_X}{\\sigma_X}\\right)\\left(\\frac{y-\\mu_Y}{\\sigma_Y}\\right) + \\left(\\frac{y-\\mu_Y}{\\sigma_Y}\\right)^2 \\right] \\right)\n$$\nHere, $\\mu_X$ and $\\mu_Y$ are the means of $X$ and $Y$, respectively. $\\sigma_X$ and $\\sigma_Y$ are their standard deviations, and $\\rho$ is the correlation coefficient between them, with $|\\rho|  1$.\n\nThe conditional expectation of $Y$ given that $X$ has taken on a specific value $x$, denoted as $E[Y|X=x]$, is defined as the mean of the conditional probability distribution $f_{Y|X}(y|x)$. This is calculated by the integral:\n$$\nE[Y|X=x] = \\int_{-\\infty}^{\\infty} y \\cdot f_{Y|X}(y|x) \\, dy\n$$\nwhere the conditional PDF is given by $f_{Y|X}(y|x) = \\frac{f_{X,Y}(x,y)}{f_X(x)}$. The marginal PDF for $X$, $f_X(x)$, is known to be a normal distribution $N(\\mu_X, \\sigma_X^2)$.\n\nYour task is to derive the expression for the conditional expectation $E[Y|X=x]$. You can achieve this by recognizing that the conditional distribution $f_{Y|X}(y|x)$ is itself a normal distribution. By algebraically manipulating the exponent of the joint PDF $f_{X,Y}(x,y)$, you can identify the form of this conditional normal distribution and thereby find its mean without performing the full integration.", "solution": "By definition, the conditional PDF is\n$$\nf_{Y|X}(y|x)=\\frac{f_{X,Y}(x,y)}{f_X(x)}\n\\propto\\exp\\Bigl(-\\frac{1}{2(1-\\rho^2)}\\Bigl[\\bigl(\\tfrac{x-\\mu_X}{\\sigma_X}\\bigr)^2\n-2\\rho\\bigl(\\tfrac{x-\\mu_X}{\\sigma_X}\\bigr)\\bigl(\\tfrac{y-\\mu_Y}{\\sigma_Y}\\bigr)\n+\\bigl(\\tfrac{y-\\mu_Y}{\\sigma_Y}\\bigr)^2\\Bigr]\\Bigr).\n$$\n\nFocus on the terms involving $y$.  Set\n$$\nA=\\frac{y-\\mu_Y}{\\sigma_Y},\\qquad B=\\frac{x-\\mu_X}{\\sigma_X},\n$$\nso that the exponent in $y$ is proportional to\n$$\nA^2-2\\rho\\,A B.\n$$\n\nComplete the square:\n$$\nA^2-2\\rho\\,A B=(A-\\rho B)^2-\\rho^2B^2.\n$$\nHence\n$$\nf_{Y|X}(y|x)\\propto\\exp\\Bigl(-\\frac{1}{2(1-\\rho^2)}(A-\\rho B)^2\\Bigr),\n$$\nwhich is the kernel of a normal distribution with mean determined by\n$$\nA-\\rho B=0\\quad\\Longrightarrow\\quad\n\\frac{y-\\mu_Y}{\\sigma_Y}=\\rho\\frac{x-\\mu_X}{\\sigma_X}\n\\quad\\Longrightarrow\\quad\ny=\\mu_Y+\\rho\\frac{\\sigma_Y}{\\sigma_X}(x-\\mu_X).\n$$\n\nTherefore,\n$$\nE[Y\\mid X=x]=\\mu_Y+\\rho\\frac{\\sigma_Y}{\\sigma_X}(x-\\mu_X).\n$$", "answer": "$$\\boxed{\\mu_Y+\\rho\\frac{\\sigma_Y}{\\sigma_X}(x-\\mu_X)}$$", "id": "1521"}, {"introduction": "The relationship between correlation and independence is often subtle, but for the normal distribution, it simplifies beautifully. This final exercise [@problem_id:1901219] explores this unique property through a transformation of variables. By defining new variables $U = X+Y$ and $V = X-Y$, you will determine the precise condition under which they become independent, a powerful insight into the structure of this distribution.", "problem": "Let the random pair $(X, Y)$ follow a bivariate normal distribution with means $\\mu_X$ and $\\mu_Y$, positive variances $\\sigma_X^2$ and $\\sigma_Y^2$, and correlation coefficient $\\rho$. We define two new random variables as linear combinations of $X$ and $Y$:\n$$U = X + Y$$\n$$V = X - Y$$\nIt is a known property that since $(X, Y)$ is a bivariate normal pair, the transformed pair $(U, V)$ also follows a bivariate normal distribution. For variables that are jointly normally distributed, the condition of zero correlation is both necessary and sufficient for statistical independence.\n\nBased on this information, which of the following statements correctly specifies the condition under which the random variables $U$ and $V$ are independent?\n\nA. $U$ and $V$ are independent if and only if $X$ and $Y$ are uncorrelated ($\\rho = 0$).\n\nB. $U$ and $V$ are independent if and only if the means are equal ($\\mu_X = \\mu_Y$).\n\nC. $U$ and $V$ are independent if and only if the variances are equal ($\\sigma_X^2 = \\sigma_Y^2$).\n\nD. $U$ and $V$ are always independent, regardless of the parameters of the distribution of $(X, Y)$.\n\nE. $U$ and $V$ can never be independent unless both $X$ and $Y$ are constant random variables.", "solution": "Because $(X,Y)$ is bivariate normal, any linear transformation is also jointly normal. Therefore $(U,V)$ is jointly normal. For jointly normal variables, zero covariance is equivalent to independence.\n\nCompute the covariance:\n$$\n\\operatorname{Cov}(U,V)=\\operatorname{Cov}(X+Y, X-Y).\n$$\nUsing bilinearity and symmetry of covariance,\n$$\n\\operatorname{Cov}(X+Y, X-Y)=\\operatorname{Cov}(X,X)-\\operatorname{Cov}(X,Y)+\\operatorname{Cov}(Y,X)-\\operatorname{Cov}(Y,Y).\n$$\nSince $\\operatorname{Cov}(X,X)=\\sigma_{X}^{2}$, $\\operatorname{Cov}(Y,Y)=\\sigma_{Y}^{2}$, and $\\operatorname{Cov}(X,Y)=\\operatorname{Cov}(Y,X)=\\rho\\,\\sigma_{X}\\sigma_{Y}$, we get\n$$\n\\operatorname{Cov}(U,V)=\\sigma_{X}^{2}-\\sigma_{Y}^{2}.\n$$\nThis covariance does not depend on $\\rho$ or the means. Hence, $\\operatorname{Cov}(U,V)=0$ if and only if $\\sigma_{X}^{2}=\\sigma_{Y}^{2}$. Since $(U,V)$ is jointly normal, zero covariance implies independence. Therefore, $U$ and $V$ are independent if and only if $\\sigma_{X}^{2}=\\sigma_{Y}^{2}$, which corresponds to option C.", "answer": "$$\\boxed{C}$$", "id": "1901219"}]}