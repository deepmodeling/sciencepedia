{"hands_on_practices": [{"introduction": "We begin our practice with a classic scenario involving sampling without replacement, which serves as an excellent foundation for understanding discrete conditional probabilities. This exercise demonstrates the core principle of conditioning: how gaining information about one part of an experiment fundamentally alters the probabilities for another part. By conditioning on the outcome of the first draw, we can precisely recalculate the probabilities for the overall composition of the sample, leading to a new, updated probability distribution [@problem_id:1906185].", "problem": "An urn contains a total of $N$ balls, of which $K$ are red and the remaining $N-K$ are black. A random sample of $n$ balls is drawn from the urn without replacement, where $1 \\le n \\le N$ and $1 \\le K < N$. Let $X$ be the random variable representing the total number of red balls in the drawn sample of $n$ balls. Given the event $E$, defined as the first ball drawn being red, determine the conditional probability $P(X=k | E)$. Express your answer as a function of $k, n, N,$ and $K$. Assume $k$ is an integer within the valid support of this conditional distribution.", "solution": "Let the urn contain $K$ red and $N-K$ black balls, and let $n$ balls be drawn without replacement. Define $E$ as the event that the first ball drawn is red. Condition on $E$.\n\nGiven $E$, after the first (red) draw, there remain $N-1$ balls, of which $K-1$ are red and $N-K$ are black. The remaining $n-1$ draws are made without replacement from these $N-1$ balls. Let $Y$ denote the number of red balls among these remaining $n-1$ draws. Then the total number of red balls in the full sample satisfies\n$$\nX=1+Y.\n$$\nTherefore, for any integer $k$ in the valid support,\n$$\nP(X=k \\mid E)=P(Y=k-1).\n$$\nSince $Y$ has a hypergeometric distribution with population size $N-1$, number of successes $K-1$, and draw size $n-1$, its probability mass function is\n$$\nP(Y=y)=\\frac{\\binom{K-1}{y}\\binom{N-K}{(n-1)-y}}{\\binom{N-1}{n-1}}.\n$$\nSubstituting $y=k-1$ gives\n$$\nP(X=k \\mid E)=\\frac{\\binom{K-1}{k-1}\\binom{N-K}{n-k}}{\\binom{N-1}{n-1}},\n$$\nfor integers $k$ satisfying $\\max\\{1,\\,n-(N-K)\\}\\leq k \\leq \\min\\{n,\\,K\\}$.", "answer": "$$\\boxed{\\frac{\\binom{K-1}{k-1}\\binom{N-K}{n-k}}{\\binom{N-1}{n-1}}}$$", "id": "1906185"}, {"introduction": "This next problem transitions us from discrete to continuous variables, using a geometric setup to build powerful intuition. We will explore how to find the conditional expectation of one variable, $Y$, given the value of another, $X$, by 'slicing' their joint probability space. This method provides a clear visual for understanding how the expected value of $Y$ changes as our knowledge of $X$ varies across its range, a key concept in regression and prediction [@problem_id:1906194].", "problem": "A point $(X, Y)$ is selected at random from a uniform distribution over the region $\\mathcal{R}$ in the $xy$-plane. The region $\\mathcal{R}$ is a parallelogram with vertices at the coordinates $(0,0)$, $(2,0)$, $(3,1)$, and $(1,1)$. Determine the conditional expectation of $Y$ given $X=x$, denoted as $E[Y|X=x]$. Your answer should be an expression in terms of $x$, valid for all values of $x$ for which the conditional expectation is defined.", "solution": "The region is the parallelogram with vertices at $(0,0)$, $(2,0)$, $(3,1)$, and $(1,1)$. Its sides lie on the lines $y=0$, $y=1$, $y=x$, and $y=x-2$. The area of this parallelogram is the absolute value of the determinant formed by the spanning vectors $(2,0)$ and $(1,1)$, which is $|2\\cdot 1-0\\cdot 1|=2$. Hence the joint density is constant and equal to $f_{X,Y}(x,y)=\\frac{1}{2}$ on $\\mathcal{R}$ and zero elsewhere.\n\nFor a fixed $x$, the vertical slice through $\\mathcal{R}$ gives the $y$-interval:\n- For $0 \\leq x \\leq 1$, $y$ ranges from $a(x)=0$ to $b(x)=x$.\n- For $1 \\leq x \\leq 2$, $y$ ranges from $a(x)=0$ to $b(x)=1$.\n- For $2 \\leq x \\leq 3$, $y$ ranges from $a(x)=x-2$ to $b(x)=1$.\n\nFor any $x$ with a nonempty slice, the marginal density is\n$$\nf_{X}(x)=\\int f_{X,Y}(x,y)\\,dy=\\frac{1}{2}\\bigl(b(x)-a(x)\\bigr),\n$$\nand the conditional expectation is\n$$\nE[Y\\mid X=x]=\\frac{1}{f_{X}(x)}\\int y\\,f_{X,Y}(x,y)\\,dy\n=\\frac{1}{\\frac{1}{2}(b-a)}\\cdot\\frac{1}{2}\\int_{a}^{b}y\\,dy\n=\\frac{1}{b-a}\\cdot\\frac{1}{2}\\bigl(b^{2}-a^{2}\\bigr)\n=\\frac{a(x)+b(x)}{2},\n$$\nwhich is the midpoint of the slice.\n\nSubstituting the bounds for each $x$-range:\n- For $0 \\leq x \\leq 1$, $E[Y\\mid X=x]=\\frac{0+x}{2}=\\frac{x}{2}$.\n- For $1 \\leq x \\leq 2$, $E[Y\\mid X=x]=\\frac{0+1}{2}=\\frac{1}{2}$.\n- For $2 \\leq x \\leq 3$, $E[Y\\mid X=x]=\\frac{(x-2)+1}{2}=\\frac{x-1}{2}$.\n\nThese formulas also hold at the boundary points $x=0,1,2,3$. Outside $[0,3]$ the conditional expectation is undefined because $f_{X}(x)=0$.", "answer": "$$\\boxed{\\begin{cases}\\frac{x}{2}, & 0 \\leq x \\leq 1,\\\\[4pt]\\frac{1}{2}, & 1 \\leq x \\leq 2,\\\\[4pt]\\frac{x-1}{2}, & 2 \\leq x \\leq 3.\\end{cases}}$$", "id": "1906194"}, {"introduction": "Our final practice problem explores a cornerstone result in statistics with wide-ranging applications, particularly in signal processing and communications theory. Imagine a clean signal, modeled by a random variable $X$, which is corrupted by additive random noise $Y$. An observer only measures the total signal $S = X+Y$. This exercise reveals how, if both the signal and noise are normally distributed, our updated knowledge about the original signal $X$ is also neatly captured by a normal distribution, but with a new mean and reduced variance. This powerful result is fundamental to estimation theory and filtering techniques [@problem_id:1906118].", "problem": "In a simplified model for a communication system, an original signal $X$ is corrupted by additive noise $Y$. Assume that both the signal $X$ and the noise $Y$ can be modeled as independent and identically distributed random variables, each following a standard normal distribution. A standard normal distribution is a normal distribution with a mean of 0 and a variance of 1.\n\nAn observer at a receiver does not see the original signal $X$ or the noise $Y$ separately. Instead, they only observe the total received signal, which is the sum $S = X+Y$. To estimate the original signal, it is crucial to understand the statistical properties of $X$ given a specific observation.\n\nSuppose the receiver measures the total signal to be $S=s$, where $s$ is a real constant. Determine the conditional probability distribution of the original signal $X$ given this measurement. Which of the following correctly describes this conditional distribution?\n\nA. A Normal distribution with mean $s$ and variance $1$.\n\nB. A Normal distribution with mean $\\frac{s}{2}$ and variance $\\frac{1}{2}$.\n\nC. A Normal distribution with mean $\\frac{s}{2}$ and variance $2$.\n\nD. A Uniform distribution on the interval $[s-\\sqrt{3}, s+\\sqrt{3}]$.\n\nE. A Normal distribution with mean $0$ and variance $\\frac{1}{2}$.\n\nF. A Chi-squared distribution with 1 degree of freedom.", "solution": "Let $X$ and $Y$ be independent with $X \\sim \\mathcal{N}(0,1)$ and $Y \\sim \\mathcal{N}(0,1)$. Define $S=X+Y$. Since $(X,Y)$ is jointly normal and $S$ is a linear function of $(X,Y)$, the pair $(X,S)$ is jointly normal.\n\nCompute the first and second moments:\n- $\\mathbb{E}[X]=0$, $\\mathbb{E}[S]=\\mathbb{E}[X+Y]=0$.\n- $\\operatorname{Var}(X)=1$, $\\operatorname{Var}(S)=\\operatorname{Var}(X)+\\operatorname{Var}(Y)=2$ (by independence).\n- $\\operatorname{Cov}(X,S)=\\operatorname{Cov}(X,X+Y)=\\operatorname{Var}(X)+\\operatorname{Cov}(X,Y)=1+0=1$ (by independence).\n\nFor jointly normal variables, the conditional distribution $X \\mid S=s$ is normal with\n$$\n\\mathbb{E}[X \\mid S=s]=\\mathbb{E}[X]+\\frac{\\operatorname{Cov}(X,S)}{\\operatorname{Var}(S)}\\left(s-\\mathbb{E}[S]\\right)=0+\\frac{1}{2}s=\\frac{s}{2},\n$$\nand\n$$\n\\operatorname{Var}(X \\mid S)=\\operatorname{Var}(X)-\\frac{\\operatorname{Cov}(X,S)^{2}}{\\operatorname{Var}(S)}=1-\\frac{1^{2}}{2}=\\frac{1}{2}.\n$$\n\nTherefore, $X \\mid S=s \\sim \\mathcal{N}\\!\\left(\\frac{s}{2},\\frac{1}{2}\\right)$, which corresponds to option B.", "answer": "$$\\boxed{B}$$", "id": "1906118"}]}