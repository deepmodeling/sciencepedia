{"hands_on_practices": [{"introduction": "The functional equation is the cornerstone of the analytic theory of the Riemann zeta function, revealing a profound symmetry about the critical line $\\operatorname{Re}(s) = 1/2$. By working through its derivation from first principles, you will gain a deep, hands-on appreciation for the analytic machinery—including Mellin transforms and properties of the Gamma function—that underpins the study of $\\zeta(s)$. This exercise [@problem_id:3027774] is fundamental to understanding why the Lindelöf hypothesis is posed on the critical line and what tools are required to investigate it.", "problem": "Let $\\zeta(s)$ denote the Riemann zeta function and $\\Gamma(s)$ the Euler gamma function. Consider the Jacobi theta function defined for $t>0$ by $\\theta(t)=\\sum_{n\\in\\mathbb{Z}}\\exp(-\\pi n^{2} t)$. You may use the following foundational facts without proof: (i) the Poisson summation formula applied to the Gaussian implies the theta transformation $\\theta(t)=t^{-1/2}\\theta(1/t)$ for all $t>0$; (ii) for $\\operatorname{Re}(s)>0$ and $a>0$, $\\int_{0}^{\\infty}\\exp(-a t)\\, t^{s-1}\\, dt=\\Gamma(s)\\, a^{-s}$; (iii) the Euler reflection formula $\\Gamma(s)\\Gamma(1-s)=\\pi/\\sin(\\pi s)$ and the Legendre duplication formula $\\Gamma(z)\\Gamma(z+1/2)=2^{1-2z}\\sqrt{\\pi}\\,\\Gamma(2z)$. Starting from these facts and no others, perform the following tasks for complex $s$ with $0\\operatorname{Re}(s)1$.\n\n1. Show that the Mellin transform identity\n$$\\frac{1}{2}\\int_{0}^{\\infty}\\big(\\theta(t)-1\\big)\\, t^{\\frac{s}{2}-1}\\, dt=\\pi^{-\\frac{s}{2}}\\Gamma\\!\\left(\\frac{s}{2}\\right)\\zeta(s)$$\nholds by interchanging sum and integral on the left-hand side and justifying absolute convergence.\n\n2. Use the theta transformation $\\theta(t)=t^{-1/2}\\theta(1/t)$, together with an appropriate change of variables and splitting of the integral at $t=1$, to prove the functional equation in its symmetric form\n$$\\Lambda(s)=\\Lambda(1-s),\\quad\\text{where}\\quad \\Lambda(s)=\\pi^{-\\frac{s}{2}}\\Gamma\\!\\left(\\frac{s}{2}\\right)\\zeta(s).$$\n\n3. Rearrange the symmetric functional equation to the form $\\zeta(s)=\\chi(s)\\,\\zeta(1-s)$ and explicitly compute the factor $\\chi(s)$ as a closed-form analytic expression in terms of elementary functions and $\\Gamma$. Simplify your expression using only the reflection and duplication formulas stated above.\n\nProvide your final expression for $\\chi(s)$ in a single closed form. No numerical approximation is required, and no rounding is needed. Your final answer must be a single analytic expression.", "solution": "The problem is assessed to be valid as it is scientifically grounded in established mathematical principles of analytic number theory, is well-posed with clear objectives, and is formulated using precise, objective language. All given information and definitions are standard and consistent. I will proceed with the solution.\n\nThe solution is presented in three parts, as requested by the problem statement.\n\n**Part 1: Mellin Transform Identity**\n\nWe are asked to prove the identity $\\frac{1}{2}\\int_{0}^{\\infty}\\big(\\theta(t)-1\\big)\\, t^{\\frac{s}{2}-1}\\, dt=\\pi^{-\\frac{s}{2}}\\Gamma\\!\\left(\\frac{s}{2}\\right)\\zeta(s)$.\nWe begin with the left-hand side (LHS). The Jacobi theta function is defined as $\\theta(t)=\\sum_{n\\in\\mathbb{Z}}\\exp(-\\pi n^{2} t)$.\nThe term $\\theta(t)-1$ can be written as:\n$$ \\theta(t)-1 = \\left(\\sum_{n\\in\\mathbb{Z}}\\exp(-\\pi n^{2} t)\\right) - 1 = \\left(1 + \\sum_{n\\in\\mathbb{Z}, n\\neq 0}\\exp(-\\pi n^{2} t)\\right) - 1 $$\n$$ = \\sum_{n=-\\infty, n\\neq 0}^{\\infty}\\exp(-\\pi n^{2} t) = \\sum_{n=1}^{\\infty}\\exp(-\\pi (-n)^{2} t) + \\sum_{n=1}^{\\infty}\\exp(-\\pi n^{2} t) = 2\\sum_{n=1}^{\\infty}\\exp(-\\pi n^{2} t) $$\nSubstituting this into the integral on the LHS gives:\n$$ \\text{LHS} = \\frac{1}{2}\\int_{0}^{\\infty}\\left(2\\sum_{n=1}^{\\infty}\\exp(-\\pi n^{2} t)\\right)\\, t^{\\frac{s}{2}-1}\\, dt = \\int_{0}^{\\infty}\\sum_{n=1}^{\\infty}\\exp(-\\pi n^{2} t)\\, t^{\\frac{s}{2}-1}\\, dt $$\nWe now interchange the order of summation and integration. This step is justified by the Fubini-Tonelli theorem if the integral of the sum of absolute values converges. For real $t0$, the terms are positive, so we must check the convergence of $\\sum_{n=1}^{\\infty}\\int_{0}^{\\infty}\\exp(-\\pi n^{2} t)\\, t^{\\operatorname{Re}(s/2)-1}\\, dt$.\nLet's proceed formally with the interchange:\n$$ \\text{LHS} = \\sum_{n=1}^{\\infty}\\int_{0}^{\\infty}\\exp(-\\pi n^{2} t)\\, t^{\\frac{s}{2}-1}\\, dt $$\nFor each integral in the sum, we use the given identity $\\int_{0}^{\\infty}\\exp(-a t)\\, t^{z-1}\\, dt=\\Gamma(z)\\, a^{-z}$ with $z = s/2$ and $a = \\pi n^2$.\n$$ \\int_{0}^{\\infty}\\exp(-\\pi n^{2} t)\\, t^{\\frac{s}{2}-1}\\, dt = \\Gamma\\left(\\frac{s}{2}\\right) (\\pi n^2)^{-s/2} = \\Gamma\\left(\\frac{s}{2}\\right) \\pi^{-s/2} (n^2)^{-s/2} = \\pi^{-s/2} \\Gamma\\left(\\frac{s}{2}\\right) n^{-s} $$\nThis is valid for $\\operatorname{Re}(s/2) > 0$, i.e., $\\operatorname{Re}(s) > 0$.\nSubstituting this back into the sum:\n$$ \\text{LHS} = \\sum_{n=1}^{\\infty} \\pi^{-s/2} \\Gamma\\left(\\frac{s}{2}\\right) n^{-s} = \\pi^{-s/2} \\Gamma\\left(\\frac{s}{2}\\right) \\sum_{n=1}^{\\infty} n^{-s} $$\nThe sum $\\sum_{n=1}^{\\infty} n^{-s}$ is the definition of the Riemann zeta function, $\\zeta(s)$, which converges for $\\operatorname{Re}(s)1$.\nTherefore, for $\\operatorname{Re}(s)1$, we have:\n$$ \\text{LHS} = \\pi^{-s/2} \\Gamma\\left(\\frac{s}{2}\\right) \\zeta(s) $$\nThis confirms the identity. The interchange of summation and integration is justified for $\\operatorname{Re}(s)1$ because the sum $\\sum_{n=1}^{\\infty} n^{-\\operatorname{Re}(s)}$ converges. Although the problem is stated for $0  \\operatorname{Re}(s)  1$, the identity is established in the region of absolute convergence $\\operatorname{Re}(s)1$ and then extended by analytic continuation to other values of $s$.\n\n**Part 2: Symmetric Functional Equation**\n\nWe start with the identity from Part 1, writing $\\Lambda(s) = \\pi^{-s/2}\\Gamma(s/2)\\zeta(s)$:\n$$ \\Lambda(s) = \\frac{1}{2}\\int_{0}^{\\infty}\\big(\\theta(t)-1\\big)\\, t^{\\frac{s}{2}-1}\\, dt $$\nThe integral defines an analytic function of $s$. To obtain its analytic continuation, we split the integral at $t=1$:\n$$ \\Lambda(s) = \\frac{1}{2}\\int_{0}^{1}\\big(\\theta(t)-1\\big)\\, t^{\\frac{s}{2}-1}\\, dt + \\frac{1}{2}\\int_{1}^{\\infty}\\big(\\theta(t)-1\\big)\\, t^{\\frac{s}{2}-1}\\, dt $$\nWe transform the first integral using the given theta transformation $\\theta(t)=t^{-1/2}\\theta(1/t)$:\n$$ \\int_{0}^{1}\\big(\\theta(t)-1\\big)\\, t^{\\frac{s}{2}-1}\\, dt = \\int_{0}^{1}\\left(t^{-1/2}\\theta(1/t) - 1\\right)\\, t^{\\frac{s}{2}-1}\\, dt = \\int_{0}^{1} t^{s/2-3/2}\\theta(1/t)\\, dt - \\int_{0}^{1} t^{s/2-1}\\, dt $$\nThe second term is $\\int_{0}^{1} t^{s/2-1}\\, dt = \\left[\\frac{t^{s/2}}{s/2}\\right]_0^1 = \\frac{2}{s}$ for $\\operatorname{Re}(s)0$.\nFor the first term, we make the change of variables $u=1/t$, so $t=1/u$ and $dt=-u^{-2}du$. The limits of integration change from $[0, 1]$ to $[\\infty, 1]$.\n$$ \\int_{0}^{1} t^{s/2-3/2}\\theta(1/t)\\, dt = \\int_{\\infty}^{1} (u^{-1})^{s/2-3/2}\\theta(u)\\,(-u^{-2}du) = \\int_{1}^{\\infty} u^{-s/2+3/2}\\theta(u)\\,u^{-2}du = \\int_{1}^{\\infty} u^{-s/2-1/2}\\theta(u)\\,du $$\nWe rewrite $\\theta(u) = (\\theta(u)-1)+1$:\n$$ \\int_{1}^{\\infty} u^{-s/2-1/2}\\big((\\theta(u)-1)+1\\big)\\,du = \\int_{1}^{\\infty} (\\theta(u)-1)u^{-s/2-1/2}\\,du + \\int_{1}^{\\infty} u^{-s/2-1/2}\\,du $$\nThe second integral is $\\int_{1}^{\\infty} u^{-(s+1)/2}\\,du = \\left[\\frac{u^{-(s+1)/2+1}}{-(s+1)/2+1}\\right]_1^\\infty = \\left[\\frac{u^{(1-s)/2}}{(1-s)/2}\\right]_1^\\infty$. For $\\operatorname{Re}(1-s)0$, or $\\operatorname{Re}(s)1$, this evaluates to $0 - \\frac{1}{(1-s)/2} = -\\frac{2}{1-s}$. By analytic continuation, this result holds.\nCombining these results for the integral from $0$ to $1$:\n$$ \\frac{1}{2}\\int_{0}^{1}\\big(\\theta(t)-1\\big)\\, t^{\\frac{s}{2}-1}\\, dt = \\frac{1}{2}\\left( \\int_{1}^{\\infty} (\\theta(t)-1)t^{-s/2-1/2}\\,dt - \\frac{2}{1-s} \\right) - \\frac{1}{2}\\left(\\frac{2}{s}\\right) = \\frac{1}{2}\\int_{1}^{\\infty} (\\theta(t)-1)t^{-(s+1)/2}\\,dt - \\frac{1}{1-s} - \\frac{1}{s} $$\nNow, we substitute this back into the expression for $\\Lambda(s)$:\n$$ \\Lambda(s) = \\left( \\frac{1}{2}\\int_{1}^{\\infty} (\\theta(t)-1)t^{-(s+1)/2}\\,dt - \\frac{1}{s} - \\frac{1}{1-s} \\right) + \\frac{1}{2}\\int_{1}^{\\infty}\\big(\\theta(t)-1\\big)\\, t^{\\frac{s}{2}-1}\\, dt $$\n$$ \\Lambda(s) = \\frac{1}{2}\\int_{1}^{\\infty}(\\theta(t)-1)\\left( t^{(s/2)-1} + t^{(-s/2)-(1/2)} \\right)dt - \\left(\\frac{1}{s} + \\frac{1}{1-s}\\right) $$\nThe term in parentheses can be written as $t^{s/2-1} + t^{-(s+1)/2} = t^{s/2-1} + t^{(1-s-2)/2} = t^{s/2-1} + t^{(1-s)/2 - 1}$. The term $-\\left(\\frac{1}{s} + \\frac{1}{1-s}\\right) = -\\frac{1-s+s}{s(1-s)} = -\\frac{1}{s(1-s)}$.\nLet's define $F(s) = \\frac{1}{2}\\int_{1}^{\\infty}(\\theta(t)-1)\\left( t^{s/2-1} + t^{(1-s)/2-1} \\right)dt - \\frac{1}{s(1-s)}$.\nThis expression for $\\Lambda(s)$ is valid for all $s$ where the integral converges. Since $\\theta(t)-1$ decays exponentially as $t\\to\\infty$, the integral converges for all complex $s$. This expression gives the analytic continuation of $\\Lambda(s)$ to the whole complex plane.\nWe now check the symmetry under the transformation $s \\mapsto 1-s$:\n$$ F(1-s) = \\frac{1}{2}\\int_{1}^{\\infty}(\\theta(t)-1)\\left( t^{(1-s)/2-1} + t^{(1-(1-s))/2-1} \\right)dt - \\frac{1}{(1-s)(1-(1-s))} $$\n$$ F(1-s) = \\frac{1}{2}\\int_{1}^{\\infty}(\\theta(t)-1)\\left( t^{(1-s)/2-1} + t^{s/2-1} \\right)dt - \\frac{1}{(1-s)s} = F(s) $$\nSince $\\Lambda(s) = F(s)$, we have shown that $\\Lambda(s) = \\Lambda(1-s)$.\n\n**Part 3: Calculation of $\\chi(s)$**\n\nWe start from the symmetric functional equation $\\Lambda(s)=\\Lambda(1-s)$, where $\\Lambda(s)=\\pi^{-s/2}\\Gamma(s/2)\\zeta(s)$.\n$$ \\pi^{-s/2}\\Gamma\\left(\\frac{s}{2}\\right)\\zeta(s) = \\pi^{-(1-s)/2}\\Gamma\\left(\\frac{1-s}{2}\\right)\\zeta(1-s) $$\nWe are looking for $\\chi(s)$ such that $\\zeta(s)=\\chi(s)\\zeta(1-s)$. By rearranging the equation, we get:\n$$ \\chi(s) = \\frac{\\pi^{-(1-s)/2}\\Gamma\\left(\\frac{1-s}{2}\\right)}{\\pi^{-s/2}\\Gamma\\left(\\frac{s}{2}\\right)} = \\pi^{-1/2+s/2+s/2} \\frac{\\Gamma\\left(\\frac{1-s}{2}\\right)}{\\Gamma\\left(\\frac{s}{2}\\right)} = \\pi^{s-1/2} \\frac{\\Gamma\\left(\\frac{1-s}{2}\\right)}{\\Gamma\\left(\\frac{s}{2}\\right)} $$\nTo simplify this expression, we use the given Euler reflection and Legendre duplication formulas for the Gamma function. A useful identity derived from them connects $\\Gamma(s)$ and $\\Gamma(1-s)$ arguments:\nFrom duplication, $\\Gamma(s) = 2^{s-1}\\pi^{-1/2}\\Gamma(s/2)\\Gamma((s+1)/2)$.\nFrom reflection, $\\Gamma(s)\\Gamma(1-s)=\\pi/\\sin(\\pi s)$. Also $\\sin(\\pi s) = 2\\sin(\\pi s/2)\\cos(\\pi s/2)$.\nCombining these: $\\frac{\\pi}{\\Gamma(1-s)\\sin(\\pi s)} = 2^{s-1}\\pi^{-1/2}\\Gamma(s/2)\\Gamma((s+1)/2)$.\n$$ \\frac{\\pi}{\\Gamma(1-s)2\\sin(\\pi s/2)\\cos(\\pi s/2)} = 2^{s-1}\\pi^{-1/2}\\Gamma(s/2)\\Gamma((s+1)/2) $$\nAnother reflection formula identity for $z=(s+1)/2$ is $\\Gamma((s+1)/2)\\Gamma(1-(s+1)/2) = \\pi/\\sin(\\pi(s+1)/2)$, which simplifies to $\\Gamma((s+1)/2)\\Gamma((1-s)/2) = \\pi/\\cos(\\pi s/2)$.\nWe can write $\\Gamma((s+1)/2) = \\frac{\\pi}{\\cos(\\pi s/2)\\Gamma((1-s)/2)}$. Substituting this into the combined identity:\n$$ \\frac{\\pi}{\\Gamma(1-s)2\\sin(\\pi s/2)\\cos(\\pi s/2)} = 2^{s-1}\\pi^{-1/2}\\Gamma(s/2) \\frac{\\pi}{\\cos(\\pi s/2)\\Gamma((1-s)/2)} $$\nWe can cancel $\\pi$ and $\\cos(\\pi s/2)$ from both sides (for $s$ not an odd integer):\n$$ \\frac{1}{\\Gamma(1-s)2\\sin(\\pi s/2)} = \\frac{2^{s-1}\\pi^{-1/2}\\Gamma(s/2)}{\\Gamma((1-s)/2)} $$\nWe want to find the ratio $\\frac{\\Gamma((1-s)/2)}{\\Gamma(s/2)}$. Rearranging the equation:\n$$ \\frac{\\Gamma((1-s)/2)}{\\Gamma(s/2)} = \\Gamma(1-s) \\cdot 2\\sin(\\pi s/2) \\cdot 2^{s-1}\\pi^{-1/2} = \\Gamma(1-s)\\sin(\\pi s/2) 2^s \\pi^{-1/2} $$\nNow we substitute this ratio back into our expression for $\\chi(s)$:\n$$ \\chi(s) = \\pi^{s-1/2} \\left( 2^s \\pi^{-1/2} \\Gamma(1-s)\\sin\\left(\\frac{\\pi s}{2}\\right) \\right) $$\n$$ \\chi(s) = 2^s \\pi^{s-1/2-1/2} \\Gamma(1-s)\\sin\\left(\\frac{\\pi s}{2}\\right) $$\n$$ \\chi(s) = 2^s \\pi^{s-1} \\sin\\left(\\frac{\\pi s}{2}\\right) \\Gamma(1-s) $$\nThis is the required closed-form expression for $\\chi(s)$.", "answer": "$$\\boxed{2^{s} \\pi^{s-1} \\sin\\left(\\frac{\\pi s}{2}\\right) \\Gamma(1-s)}$$", "id": "3027774"}, {"introduction": "The functional equation gives rise to the approximate functional equation (AFE), a powerful tool that expresses $\\zeta(s)$ as a finite sum, making it amenable to estimation. This exercise explores how a straightforward application of the AFE leads to the so-called \"convexity bound\" $|\\zeta(1/2+it)| \\ll t^{1/4+\\epsilon}$. By engaging with this practice [@problem_id:3027771], you will see precisely how this baseline estimate arises and understand why achieving any improvement—a \"subconvexity\" bound—requires techniques that can extract additional cancellation from the sums involved.", "problem": "Consider the Riemann zeta function $\\zeta(s)$ and its behavior on the critical line $s=\\tfrac{1}{2}+it$ for real $t\\ge 1$. The Lindelöf hypothesis predicts that $|\\zeta(\\tfrac{1}{2}+it)|\\ll_\\varepsilon t^\\varepsilon$ for any $\\varepsilon0$, while unconditional bounds are typically derived via the functional equation together with the approximate functional equation (AFE). The AFE represents $\\zeta(\\tfrac{1}{2}+it)$ as a combination of two Dirichlet polynomials equipped with smooth weights and complementary lengths, and a small error term; the complementary lengths are linked to $t$ by a product constraint of the form $NM\\asymp t$. In seeking subconvexity bounds $|\\zeta(\\tfrac{1}{2}+it)|\\ll_\\varepsilon t^\\theta$ with $\\theta\\tfrac{1}{4}$, analysts often examine how the choice of cutoff length in the AFE affects the trivial size of the Dirichlet polynomial pieces and thus the attainable exponent without additional cancellation mechanisms.\n\nWhich of the following statements best describes how the choice of cutoff length in the AFE influences the exponent in bounds for $|\\zeta(\\tfrac{1}{2}+it)|$ obtained by straightforward estimation of the Dirichlet polynomials (without using any deeper input beyond the AFE and basic inequalities)?\n\nA. Balancing the complementary lengths at $N\\asymp M\\asymp t^{1/2}$ minimizes the trivial bound, yielding the convexity exponent $\\tfrac{1}{4}$ (up to factors $t^\\varepsilon$); substantially unbalancing the lengths forces a larger trivial bound and hence a worse exponent.\n\nB. Making one length much shorter than $t^{1/2}$ and the other correspondingly longer automatically produces a subconvex exponent $\\theta\\tfrac{1}{4}$, because the shorter sum is small and the longer sum cancels completely by oscillation without requiring further input.\n\nC. The cutoff length choice in the AFE does not affect the exponent at all; only the functional equation itself determines the exponent, and all choices of lengths yield the same bound after trivial estimation.\n\nD. By a clever choice of cutoff length alone (with no additional methods), one can reach the Lindelöf-scale bound $|\\zeta(\\tfrac{1}{2}+it)|\\ll_\\varepsilon t^\\varepsilon$, since shortening one of the sums to constant length suppresses the main growth entirely.\n\nSelect the single best option.", "solution": "The user wants me to validate and solve a problem concerning the influence of the cutoff length in the Approximate Functional Equation (AFE) on the trivial bound for the Riemann zeta function on the critical line.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n*   **Function:** Riemann zeta function, $\\zeta(s)$.\n*   **Domain:** The critical line, $s=\\tfrac{1}{2}+it$ for real $t\\ge 1$.\n*   **Hypothesis:** The Lindelöf hypothesis predicts $|\\zeta(\\tfrac{1}{2}+it)|\\ll_\\varepsilon t^\\varepsilon$ for any $\\varepsilon0$.\n*   **Context:** Unconditional bounds $|\\zeta(\\tfrac{1}{2}+it)|\\ll_\\varepsilon t^\\theta$ with $\\theta\\tfrac{1}{4}$ (subconvexity).\n*   **Method:** The Approximate Functional Equation (AFE).\n*   **AFE Structure:** The AFE represents $\\zeta(\\tfrac{1}{2}+it)$ as a combination of two Dirichlet polynomials with complementary lengths $N$ and $M$ satisfying $NM\\asymp t$, plus a small error term.\n*   **Question:** How does the choice of cutoff length (e.g., $N$) in the AFE influence the exponent $\\theta$ in the bound $|\\zeta(\\tfrac{1}{2}+it)|\\ll t^\\theta$ when this bound is obtained by \"straightforward estimation of the Dirichlet polynomials (without using any deeper input beyond the AFE and basic inequalities)\"?\n\n**Step 2: Validate Using Extracted Givens**\n\n1.  **Scientifically Grounded:** The problem statement is fundamentally sound. It correctly describes the context of bounding the Riemann zeta function, including the critical line, the Lindelöf hypothesis, the concept of subconvexity (beating the convexity bound $\\theta=\\tfrac{1}{4}$), and the structure of the Approximate Functional Equation. The relation $NM \\asymp t$ for the lengths of the sums in the AFE for $\\zeta(\\tfrac{1}{2}+it)$ is a standard feature of the method.\n2.  **Well-Posed:** The question is well-posed. It asks for a qualitative and quantitative description of how a parameter ($N$, the cutoff length) affects the outcome of a specific, well-understood estimation procedure (\"straightforward estimation,\" i.e., using the triangle inequality). The problem is structured to elicit a unique conceptual understanding.\n3.  **Objective:** The language is technical, precise, and free of subjectivity. It presents established concepts from analytic number theory.\n4.  **Flaw Checklist:**\n    *   **Scientific Unsoundness:** None. The premises are accurate.\n    *   **Non-Formalizable:** The problem is perfectly formalizable within standard analytic number theory.\n    *   **Incomplete/Contradictory:** The problem provides sufficient information. The phrase \"without using any deeper input\" clearly specifies that we should only use the triangle inequality on the sums in the AFE.\n    *   **Unrealistic/Infeasible:** Not applicable (theoretical mathematics).\n    *   **Ill-Posed:** The problem is not ill-posed. The terminology is standard in the field.\n    *   **Pseudo-Profound/Trivial:** The question addresses a core, foundational concept necessary for understanding modern methods in the field. It is not trivial.\n    *   **Unverifiable:** The claims are mathematically verifiable.\n\n**Step 3: Verdict and Action**\n\nThe problem statement is **valid**. The analysis can proceed.\n\n### Solution Derivation\n\nThe Approximate Functional Equation (AFE) for the Riemann zeta function on the critical line $s=\\tfrac{1}{2}+it$ states that, for large $t$,\n$$\n\\zeta(\\tfrac{1}{2}+it) = \\sum_{n=1}^N \\frac{w_1(n)}{n^{1/2+it}} + \\chi(\\tfrac{1}{2}+it) \\sum_{m=1}^M \\frac{w_2(m)}{m^{1/2-it}} + O(t^{-A})\n$$\nfor any $A0$. Here, $w_1(n)$ and $w_2(m)$ are smooth weight functions, $\\chi(s)$ is the factor from the functional equation $\\zeta(s)=\\chi(s)\\zeta(1-s)$, and the lengths of the sums are related by $NM \\asymp t$. On the critical line, we have $|\\chi(\\tfrac{1}{2}+it)| = 1$. The smooth weights are typically bounded, i.e., $w_j(k) \\ll 1$.\n\nThe problem asks for the bound obtained by \"straightforward estimation,\" which means applying the triangle inequality. We ignore the negligible error term and focus on the two sums.\n$$\n|\\zeta(\\tfrac{1}{2}+it)| \\lesssim \\left| \\sum_{n=1}^N \\frac{w_1(n)}{n^{1/2+it}} \\right| + \\left| \\sum_{m=1}^M \\frac{w_2(m)}{m^{1/2-it}} \\right|\n$$\nApplying the triangle inequality to the first sum:\n$$\n\\left| \\sum_{n=1}^N \\frac{w_1(n)}{n^{1/2+it}} \\right| \\le \\sum_{n=1}^N \\frac{|w_1(n)|}{|n^{1/2+it}|} = \\sum_{n=1}^N \\frac{|w_1(n)|}{n^{1/2}} \\ll \\sum_{n=1}^N \\frac{1}{n^{1/2}}\n$$\nThis sum can be estimated by the corresponding integral:\n$$\n\\sum_{n=1}^N \\frac{1}{n^{1/2}} \\approx \\int_1^N x^{-1/2} dx = [2x^{1/2}]_1^N = 2\\sqrt{N} - 2 \\asymp N^{1/2}\n$$\nSimilarly, the second sum is bounded by $M^{1/2}$. Therefore, the trivial bound from the AFE is:\n$$\n|\\zeta(\\tfrac{1}{2}+it)| \\ll N^{1/2} + M^{1/2}\n$$\nUsing the constraint $M \\asymp t/N$, we can express this bound in terms of $N$ and $t$:\n$$\n|\\zeta(\\tfrac{1}{2}+it)| \\ll N^{1/2} + (t/N)^{1/2}\n$$\nWe wish to find the choice of cutoff length $N$ that minimizes this upper bound. Let $f(N) = N^{1/2} + t^{1/2}N^{-1/2}$. To find the minimum, we can use calculus, setting the derivative with respect to $N$ to zero:\n$$\n\\frac{df}{dN} = \\frac{1}{2}N^{-1/2} - \\frac{1}{2}t^{1/2}N^{-3/2} = 0\n$$\n$$\nN^{-1/2} = t^{1/2}N^{-3/2} \\implies N = t^{1/2}\n$$\nThus, the bound is minimized when we choose the cutoff length $N \\asymp t^{1/2}$. At this balanced point, the other length is $M \\asymp t/N \\asymp t / t^{1/2} = t^{1/2}$. The minimal bound is:\n$$\n|\\zeta(\\tfrac{1}{2}+it)| \\ll (t^{1/2})^{1/2} + (t^{1/2})^{1/2} = t^{1/4} + t^{1/4} \\asymp t^{1/4}\n$$\nThis exponent $\\theta = \\tfrac{1}{4}$ is known as the \"convexity exponent.\"\n\nNow, consider an unbalanced choice, e.g., $N=t^\\alpha$ for some $\\alpha \\in (0,1)$ where $\\alpha \\ne \\tfrac{1}{2}$. Then $M = t/N = t^{1-\\alpha}$. The bound becomes:\n$$\n|\\zeta(\\tfrac{1}{2}+it)| \\ll (t^\\alpha)^{1/2} + (t^{1-\\alpha})^{1/2} = t^{\\alpha/2} + t^{(1-\\alpha)/2}\n$$\nThe resulting exponent is $\\max(\\alpha/2, (1-\\alpha)/2)$. If $\\alpha \\ne \\tfrac{1}{2}$, one of these exponents must be strictly greater than $\\tfrac{1}{4}$. For instance, if $\\alpha  \\tfrac{1}{2}$, then $1-\\alpha  \\tfrac{1}{2}$, and the exponent is $(1-\\alpha)/2  \\tfrac{1}{4}$. If $\\alpha  \\tfrac{1}{2}$, the exponent is $\\alpha/2  \\tfrac{1}{4}$. Therefore, any choice of cutoff length other than the balanced choice $N \\asymp t^{1/2}$ results in a trivial bound with an exponent worse than $\\tfrac{1}{4}$.\n\n### Option-by-Option Analysis\n\n**A. Balancing the complementary lengths at $N\\asymp M\\asymp t^{1/2}$ minimizes the trivial bound, yielding the convexity exponent $\\tfrac{1}{4}$ (up to factors $t^\\varepsilon$); substantially unbalancing the lengths forces a larger trivial bound and hence a worse exponent.**\nThis statement precisely matches our derivation. Balancing the lengths at $N \\asymp M \\asymp t^{1/2}$ minimizes the expression $N^{1/2} + M^{1/2}$, yielding the bound $t^{1/4}$. Unbalancing the lengths leads to a bound of $t^{\\max(\\alpha/2, (1-\\alpha)/2)}$ for $N=t^\\alpha$, which is always greater than $t^{1/4}$ if $\\alpha \\ne \\tfrac{1}{2}$.\n**Verdict: Correct.**\n\n**B. Making one length much shorter than $t^{1/2}$ and the other correspondingly longer automatically produces a subconvex exponent $\\theta\\tfrac{1}{4}$, because the shorter sum is small and the longer sum cancels completely by oscillation without requiring further input.**\nThis is incorrect. As shown above, making one length shorter, say $N=t^{1/3}$, forces the other to be longer, $M=t^{2/3}$. The trivial bound is then $t^{1/6} + t^{1/3} \\asymp t^{1/3}$, which is substantially worse than the convexity bound (exponent $\\tfrac{1}{3}  \\tfrac{1}{4}$), not subconvex. The claim of automatic cancellation in the longer sum is false; achieving such cancellation is the very core of the subconvexity problem and requires sophisticated techniques far beyond \"straightforward estimation.\"\n**Verdict: Incorrect.**\n\n**C. The cutoff length choice in the AFE does not affect the exponent at all; only the functional equation itself determines the exponent, and all choices of lengths yield the same bound after trivial estimation.**\nThis is false. Our analysis explicitly shows that the exponent of the trivial bound, $\\max(\\alpha/2, (1-\\alpha)/2)$, is a direct function of the parameter $\\alpha$ that governs the cutoff length $N=t^\\alpha$. Different choices of $\\alpha$ yield different exponents.\n**Verdict: Incorrect.**\n\n**D. By a clever choice of cutoff length alone (with no additional methods), one can reach the Lindelöf-scale bound $|\\zeta(\\tfrac{1}{2}+it)|\\ll_\\varepsilon t^\\varepsilon$, since shortening one of the sums to constant length suppresses the main growth entirely.**\nThis is profoundly incorrect. The Lindelöf hypothesis is one of the deepest unsolved problems in mathematics. If it were solvable by such a trivial manipulation, it would have been proven a century ago. Let's test the specific claim: setting $N$ to be a constant, e.g., $N=1$. Then $M \\asymp t$. The trivial bound becomes $1^{1/2} + t^{1/2} \\asymp t^{1/2}$. This yields an exponent of $\\theta=\\tfrac{1}{2}$, which is a very weak bound, far from the Lindelöf scale $\\theta=\\varepsilon$. Shortening one sum merely transfers the size of the problem to the other sum.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{A}$$", "id": "3027771"}, {"introduction": "The Lindelöf hypothesis is, at its heart, a statement about profound cancellation among the terms of the Dirichlet series for $\\zeta(s)$. This exercise provides a concrete, calculable demonstration of this cancellation principle in an averaged sense. By computing the mean value of the \"off-diagonal\" terms of a model for $|\\zeta(1/2+it)|^2$, you will prove that their contribution averages to zero, explaining why the mean-square value of the zeta function is so much smaller than one might naively expect and reinforcing the plausibility of the Lindelöf hypothesis [@problem_id:3027778].", "problem": "Let $T \\geq 3$ be a large real parameter and define the Dirichlet polynomial\n$$\nS_{T}(t) \\coloneqq \\sum_{n \\leq \\sqrt{T}} n^{-1/2 - i t}.\n$$\nWrite\n$$\n|S_{T}(t)|^{2} \\;=\\; \\sum_{m \\leq \\sqrt{T}} \\sum_{n \\leq \\sqrt{T}} (mn)^{-1/2} \\exp\\!\\big(-i t \\ln(n/m)\\big) \\;=\\; D_{T} \\;+\\; \\mathrm{Off}_{T}(t),\n$$\nwhere the diagonal part is\n$$\nD_{T} \\coloneqq \\sum_{n \\leq \\sqrt{T}} n^{-1},\n$$\nand the off-diagonal part is\n$$\n\\mathrm{Off}_{T}(t) \\coloneqq \\sum_{\\substack{m,n \\leq \\sqrt{T} \\\\ m \\neq n}} (mn)^{-1/2} \\exp\\!\\big(-i t \\ln(n/m)\\big).\n$$\nStarting only from core properties of exponential integrals and basic estimates for logarithms, compute the following averaged off-diagonal contribution:\n$$\nL \\coloneqq \\lim_{T \\to \\infty} \\frac{1}{T} \\int_{T}^{2T} \\mathrm{Off}_{T}(t)\\, dt.\n$$\nYour derivation must be self-contained and justify all steps, including any exchange of limits and summations and any use of comparison tests.\n\nFinally, explain, based on your computation and standard facts such as the approximate functional equation for the Riemann zeta function $\\zeta(s)$, how cancellation among off-diagonal terms influences mean-square bounds for $|\\zeta(1/2 + i t)|$ and why this is consistent with the Lindelöf hypothesis. Provide the exact value of $L$ as your final answer. No rounding is required.", "solution": "The problem asks for the computation of a limit $L$ related to the mean value of the off-diagonal part of the squared modulus of a Dirichlet polynomial, and an explanation of its connection to the theory of the Riemann zeta function.\n\nFirst, we validate the problem. The problem is formulated in the context of analytic number theory, specifically concerning mean value theorems for Dirichlet polynomials. All definitions are standard and mathematically sound. The decomposition of $|S_T(t)|^2$ into diagonal and off-diagonal parts is a standard technique. The question is well-posed, objective, and scientifically grounded. It requires a rigorous mathematical derivation based on fundamental principles. Therefore, the problem is valid and we may proceed with the solution.\n\nThe primary task is to compute the limit:\n$$\nL \\coloneqq \\lim_{T \\to \\infty} \\frac{1}{T} \\int_{T}^{2T} \\mathrm{Off}_{T}(t)\\, dt\n$$\nwhere\n$$\n\\mathrm{Off}_{T}(t) \\coloneqq \\sum_{\\substack{m,n \\leq \\sqrt{T} \\\\ m \\neq n}} (mn)^{-1/2} \\exp\\!\\big(-i t \\ln(n/m)\\big).\n$$\nLet us denote the expression inside the limit by $L_T$.\n$$\nL_T \\coloneqq \\frac{1}{T} \\int_{T}^{2T} \\left( \\sum_{\\substack{m,n \\leq \\sqrt{T} \\\\ m \\neq n}} (mn)^{-1/2} \\exp\\!\\big(-i t \\ln(n/m)\\big) \\right) dt.\n$$\nThe sum is over a finite number of terms, specifically $(\\lfloor\\sqrt{T}\\rfloor)^2 - \\lfloor\\sqrt{T}\\rfloor$ terms. The integrand in each term is a continuous function of $t$. Therefore, we can interchange the order of summation and integration:\n$$\nL_T = \\sum_{\\substack{m,n \\leq \\sqrt{T} \\\\ m \\neq n}} (mn)^{-1/2} \\frac{1}{T} \\int_{T}^{2T} \\exp\\!\\big(-i t \\ln(n/m)\\big) dt.\n$$\nSince $m \\neq n$, the term $\\ln(n/m)$ is non-zero. We can evaluate the integral directly:\n$$\n\\int_{T}^{2T} \\exp\\!\\big(-i t \\ln(n/m)\\big) dt = \\left[ \\frac{\\exp(-i t \\ln(n/m))}{-i \\ln(n/m)} \\right]_{t=T}^{t=2T} = \\frac{\\exp(-2iT \\ln(n/m)) - \\exp(-iT \\ln(n/m))}{-i \\ln(n/m)}.\n$$\nSubstituting this back into the expression for $L_T$:\n$$\nL_T = \\frac{1}{T} \\sum_{\\substack{m,n \\leq \\sqrt{T} \\\\ m \\neq n}} \\frac{(mn)^{-1/2}}{-i \\ln(n/m)} \\left( \\exp\\!\\big(-2iT \\ln(n/m)\\big) - \\exp\\!\\big(-iT \\ln(n/m)\\big) \\right).\n$$\nTo find the limit $L = \\lim_{T \\to \\infty} L_T$, we will bound the magnitude of $L_T$. Using the triangle inequality and the fact that $|\\exp(ix)| = 1$ for real $x$, we have:\n$$\n|\\exp\\!\\big(-2iT \\ln(n/m)\\big) - \\exp\\!\\big(-iT \\ln(n/m)\\big)| \\leq |\\exp\\!\\big(-2iT \\ln(n/m)\\big)| + |\\exp\\!\\big(-iT \\ln(n/m)\\big)| \\leq 1 + 1 = 2.\n$$\nThus, the magnitude of $L_T$ is bounded by:\n$$\n|L_T| \\leq \\frac{1}{T} \\sum_{\\substack{m,n \\leq \\sqrt{T} \\\\ m \\neq n}} \\frac{(mn)^{-1/2}}{|\\ln(n/m)|} \\cdot 2.\n$$\nLet $X = \\lfloor\\sqrt{T}\\rfloor$. The sum can be written as:\n$$\n\\sum_{\\substack{1 \\leq m,n \\leq X \\\\ m \\neq n}} \\frac{(mn)^{-1/2}}{|\\ln(n/m)|}.\n$$\nBy swapping the roles of $m$ and $n$, we note that the sum over $mn$ is equal to the sum over $nm$, because $|\\ln(n/m)| = |\\ln(m/n)|$. So, we can write:\n$$\n|L_T| \\leq \\frac{4}{T} \\sum_{1 \\leq m  n \\leq X} \\frac{(mn)^{-1/2}}{\\ln(n/m)}.\n$$\nLet us estimate the sum $S(X) = \\sum_{1 \\leq m  n \\leq X} \\frac{(mn)^{-1/2}}{\\ln(n/m)}$. We split the sum over $n$ into two ranges: $m  n \\leq 2m$ and $2m  n \\leq X$.\n\nCase 1: $2m  n \\leq X$. This requires $m  X/2$. In this range, $n/m  2$, so $\\ln(n/m)  \\ln(2)$.\n$$\n\\sum_{m=1}^{\\lfloor X/2 \\rfloor} \\sum_{n=2m+1}^{X} \\frac{(mn)^{-1/2}}{\\ln(n/m)} \\leq \\frac{1}{\\ln(2)} \\sum_{m=1}^{\\lfloor X/2 \\rfloor} m^{-1/2} \\sum_{n=2m+1}^{X} n^{-1/2}.\n$$\nUsing the comparison of a sum with an integral, $\\sum_{k=a}^b k^{-1/2} \\leq \\int_{a-1}^b x^{-1/2} dx = 2(\\sqrt{b}-\\sqrt{a-1})$ for $a1$. A simpler bound is $\\sum_{k=a}^b k^{-1/2} = O(\\sqrt{b})$. The inner sum is $\\sum_{n=2m+1}^X n^{-1/2} = O(\\sqrt{X})$. The outer sum is then $\\sum_{m=1}^{\\lfloor X/2 \\rfloor} m^{-1/2} O(\\sqrt{X}) = O(\\sqrt{X}) \\sum_{m=1}^{\\lfloor X/2 \\rfloor} m^{-1/2} = O(\\sqrt{X}) \\cdot O(\\sqrt{X/2}) = O(X)$.\n\nCase 2: $m  n \\leq 2m$. Let $n=m+k$ for $1 \\leq k \\leq m$. We use the inequality $\\ln(1+u) \\geq \\frac{u}{2}$ for $u \\in [0, 1]$. Since $k/m \\in (0, 1]$, we have $\\ln(n/m) = \\ln(1+k/m) \\geq \\frac{k}{2m}$.\nThe term $(mn)^{-1/2}$ is $\\frac{1}{\\sqrt{m(m+k)}} \\approx \\frac{1}{m}$ for $k \\leq m$.\nThe sum for this case is $\\sum_{m=1}^{X-1} \\sum_{n=m+1}^{\\min(X, 2m)} \\frac{(mn)^{-1/2}}{\\ln(n/m)}$.\n$$\n\\sum_{n=m+1}^{\\min(X, 2m)} \\frac{(mn)^{-1/2}}{\\ln(n/m)} = \\sum_{k=1}^{\\min(X-m, m)} \\frac{1}{\\sqrt{m(m+k)}\\ln(1+k/m)} \\leq \\sum_{k=1}^{m} \\frac{1}{\\sqrt{m^2}} \\frac{2m}{k} = \\sum_{k=1}^{m} \\frac{2}{k}.\n$$\nThe sum $\\sum_{k=1}^m \\frac{2}{k} = O(\\ln m)$. Summing over $m$ from $1$ to $X-1$:\n$$\n\\sum_{m=1}^{X-1} O(\\ln m) \\approx \\int_1^X \\ln x \\, dx = [x \\ln x - x]_1^X = X \\ln X - X + 1 = O(X \\ln X).\n$$\nCombining both cases, the total sum is $S(X) = O(X) + O(X \\ln X) = O(X \\ln X)$.\nSince $X = \\lfloor\\sqrt{T}\\rfloor$, we have $S(\\lfloor\\sqrt{T}\\rfloor) = O(\\sqrt{T}\\ln(\\sqrt{T})) = O(\\sqrt{T}\\ln T)$.\nSubstituting this bound back into the inequality for $|L_T|$:\n$$\n|L_T| \\leq \\frac{4}{T} S(\\lfloor\\sqrt{T}\\rfloor) = \\frac{4}{T} O(\\sqrt{T} \\ln T) = O\\left(\\frac{\\ln T}{\\sqrt{T}}\\right).\n$$\nAs $T \\to \\infty$, the term $(\\ln T)/\\sqrt{T}$ approaches $0$. Therefore, by the Squeeze Theorem, we have:\n$$\nL = \\lim_{T \\to \\infty} L_T = 0.\n$$\nThe average contribution of the off-diagonal terms is zero.\n\nNow, we explain the significance of this result in the context of the Riemann zeta function $\\zeta(s)$ and the Lindelöf hypothesis.\n\nThe Dirichlet polynomial $S_T(t) = \\sum_{n \\leq \\sqrt{T}} n^{-1/2 - i t}$ serves as a model for the Riemann zeta function $\\zeta(1/2+it)$. According to the approximate functional equation for $\\zeta(s)$, for large $t$, $\\zeta(1/2+it)$ can be approximated by a sum of two such Dirichlet polynomials of length about $\\sqrt{t/(2\\pi)}$. Thus, the mean-square behavior of $|S_T(t)|^2$ with $T \\approx t$ is indicative of the behavior of $|\\zeta(1/2+it)|^2$.\n\nWe have shown that $\\lim_{T \\to \\infty} \\frac{1}{T} \\int_T^{2T} \\mathrm{Off}_T(t) dt = 0$. This means that, on average, the off-diagonal terms do not contribute to the mean value of $|S_T(t)|^2$. The entire contribution comes from the diagonal part, $D_T = \\sum_{n \\leq \\sqrt{T}} n^{-1}$.\nThe mean value of the diagonal part is simply:\n$$\n\\frac{1}{T} \\int_T^{2T} D_T dt = D_T = \\sum_{n \\leq \\sqrt{T}} \\frac{1}{n} \\approx \\ln(\\sqrt{T}) + \\gamma = \\frac{1}{2}\\ln T + \\gamma,\n$$\nwhere $\\gamma$ is the Euler-Mascheroni constant.\nThis implies $\\frac{1}{T} \\int_T^{2T} |S_T(t)|^2 dt \\sim \\frac{1}{2} \\ln T$. This result for the model polynomial mirrors the classic result of Hardy and Littlewood for the zeta function itself: $\\frac{1}{T}\\int_0^T |\\zeta(1/2+it)|^2 dt \\sim \\ln T$.\n\nThe Lindelöf hypothesis is the conjecture that for any $\\epsilon > 0$, $|\\zeta(1/2+it)| = O(|t|^\\epsilon)$ as $|t| \\to \\infty$. A trivial estimation using the triangle inequality on the Dirichlet series for $\\zeta(1/2+it)$ gives $|\\zeta(1/2+it)| \\le \\sum_{n \\le t} n^{-1/2} = O(t^{1/2})$, far from the conjectured bound. The Lindelöf hypothesis is thus a profound statement about the vast amount of cancellation among the terms $n^{-it}$ due to their oscillating phases.\n\nOur calculation of $L=0$ is a precise demonstration of this cancellation principle, albeit in an averaged (mean-square) sense. The `off-diagonal` terms in $|S_T(t)|^2$ are all formed by products like $m^{-1/2-it} n^{-1/2+it}$ with $m \\ne n$. These terms contain oscillatory factors $\\exp(-it\\ln(n/m))$. The fact that their integral average is zero shows that these oscillations lead to large-scale cancellation over long intervals of $t$. If these terms did not cancel, the mean-square value of $\\zeta(1/2+it)$ could grow much faster, potentially as fast as $O(T)$, which would correspond to $|\\zeta(1/2+it)|$ having a typical size of $O(t^{1/2})$. The logarithmic growth of the mean square, which results from the cancellation of off-diagonal terms, is consistent with the much smaller growth rate of $|\\zeta(1/2+it)|$ predicted by the Lindelöf hypothesis. In summary, cancellation among off-diagonal terms is the essential mechanism that tames the growth of the zeta function on the critical line.", "answer": "$$\n\\boxed{0}\n$$", "id": "3027778"}]}