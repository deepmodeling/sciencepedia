{"hands_on_practices": [{"introduction": "The journey into Lie algebras begins with its most fundamental operation: the Lie bracket. For matrix Lie groups, the corresponding Lie algebra bracket is elegantly realized as the matrix commutator, $[X, Y] = XY - YX$. This exercise provides a direct, hands-on computation of the bracket for two basis elements of $\\mathfrak{sl}(2, \\mathbb{R})$, a foundational example that appears throughout mathematics and physics. [@problem_id:1523080]", "problem": "Consider the real vector space $V = \\mathbb{R}^2$ equipped with the standard basis $\\{e_1, e_2\\}$, where $e_1 = (1, 0)$ and $e_2 = (0, 1)$. Two linear transformations, $T_1: V \\to V$ and $T_2: V \\to V$, are defined by their action on these basis vectors as follows:\n\n$T_1(e_1) = (0, 0)$ and $T_1(e_2) = e_1$.\n$T_2(e_1) = e_2$ and $T_2(e_2) = (0, 0)$.\n\nLet $X$ and $Y$ be the $2 \\times 2$ matrix representations of the linear transformations $T_1$ and $T_2$ with respect to the standard basis, respectively. These matrices are elements of the special linear Lie algebra $\\mathfrak{sl}(2, \\mathbb{R})$, which is the set of all $2 \\times 2$ real matrices with a trace of zero.\n\nThe Lie bracket of two matrices $A$ and $B$ is defined by the commutator operation $[A, B] = AB - BA$.\n\nCalculate the Lie bracket $[X, Y]$. Your answer should be a $2 \\times 2$ matrix.", "solution": "We use the fact that the matrix of a linear transformation with respect to a basis has columns given by the images of the basis vectors expressed in that basis.\n\nGiven $T_{1}(e_{1})=(0,0)$ and $T_{1}(e_{2})=e_{1}=(1,0)$, the matrix $X$ representing $T_{1}$ in the standard basis is\n$$\nX=\\begin{pmatrix}\n0  1 \\\\\n0  0\n\\end{pmatrix}.\n$$\nGiven $T_{2}(e_{1})=e_{2}=(0,1)$ and $T_{2}(e_{2})=(0,0)$, the matrix $Y$ representing $T_{2}$ in the standard basis is\n$$\nY=\\begin{pmatrix}\n0  0 \\\\\n1  0\n\\end{pmatrix}.\n$$\nThe Lie bracket is the commutator $[X,Y]=XY-YX$. Compute\n$$\nXY=\\begin{pmatrix}\n0  1 \\\\\n0  0\n\\end{pmatrix}\n\\begin{pmatrix}\n0  0 \\\\\n1  0\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1  0 \\\\\n0  0\n\\end{pmatrix},\n\\quad\nYX=\\begin{pmatrix}\n0  0 \\\\\n1  0\n\\end{pmatrix}\n\\begin{pmatrix}\n0  1 \\\\\n0  0\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0  0 \\\\\n0  1\n\\end{pmatrix}.\n$$\nTherefore,\n$$\n[X,Y]=XY-YX=\\begin{pmatrix}\n1  0 \\\\\n0  -1\n\\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}1  0 \\\\ 0  -1\\end{pmatrix}}$$", "id": "1523080"}, {"introduction": "Beyond single computations, understanding a Lie algebra involves dissecting its entire structure. This practice delves into $\\mathfrak{so}(3)$, the Lie algebra of the rotation group $SO(3)$, which is crucial for describing rotational dynamics in physics and geometry. By systematically deriving its form, basis, structure constants, and verifying the all-important Jacobi identity, you will gain a deep, practical understanding of a truly canonical simple Lie algebra. [@problem_id:3031872]", "problem": "Let $G$ denote the Special Orthogonal group (SO(3)) consisting of all real $3 \\times 3$ matrices $A$ with $A^{\\mathsf{T}}A=I$ and $\\det(A)=1$, viewed as a Lie group inside the General Linear group (GL) $GL(3,\\mathbb{R})$. Using only the definition of the Lie algebra of a matrix Lie group as the tangent space at the identity and the commutator bracket of matrices, proceed as follows:\n\n1. Identify the Lie algebra $\\mathfrak{so}(3)$ as the subspace of $M_{3}(\\mathbb{R})$ of real $3 \\times 3$ matrices tangent to $G$ at the identity, and characterize it intrinsically by an equation involving matrix transpose.\n\n2. Show that every element $X \\in \\mathfrak{so}(3)$ can be written uniquely in the form\n$$\nX=\\begin{pmatrix}\n0  -x_{3}  x_{2} \\\\\nx_{3}  0  -x_{1} \\\\\n-x_{2}  x_{1}  0\n\\end{pmatrix},\n$$\nfor some $(x_{1},x_{2},x_{3}) \\in \\mathbb{R}^{3}$. Using this, choose the standard basis $\\{E_{1},E_{2},E_{3}\\}$ given by the matrices obtained by setting one of $x_{1},x_{2},x_{3}$ equal to $1$ and the others to $0$.\n\n3. Compute the Lie brackets $[E_{i},E_{j}]$ using the commutator $[X,Y]=XY-YX$ and hence determine the structure constants $c_{ij}^{k}$ with respect to the basis $\\{E_{1},E_{2},E_{3}\\}$ defined by $[E_{i},E_{j}]=c_{ij}^{k}E_{k}$ (Einstein summation convention in force).\n\n4. Verify the Jacobi identity in terms of the structure constants, namely that\n$$\nc_{ij}^{m}c_{mk}^{\\;\\;l}+c_{jk}^{m}c_{mi}^{\\;\\;l}+c_{ki}^{m}c_{mj}^{\\;\\;l}=0\n$$\nfor all indices $i,j,k,l \\in \\{1,2,3\\}$, using only multilinear algebra identities valid in $\\mathbb{R}^{3}$.\n\nFinally, define the scalar\n$$\nS:=c_{ij}^{k}c_{ij}^{k},\n$$\nwhere repeated indices are summed from $1$ to $3$. Report the exact value of $S$ as your final answer. Express your final answer as a single number without units. No rounding is required for this problem.", "solution": "The problem asks for a step-by-step analysis of the Lie algebra $\\mathfrak{so}(3)$ of the special orthogonal group $SO(3)$, culminating in the calculation of a scalar invariant $S$ derived from its structure constants.\n\n**Part 1: Characterization of the Lie Algebra $\\mathfrak{so}(3)$**\n\nThe Lie algebra $\\mathfrak{so}(3)$ is the tangent space to the Lie group $SO(3)$ at the identity element, $I$. An element $X \\in \\mathfrak{so}(3)$ is the tangent vector at $t=0$ of a smooth curve $\\gamma(t)$ in $SO(3)$ such that $\\gamma(0) = I$. That is, $X = \\gamma'(0)$.\n\nThe defining property of any matrix $A \\in SO(3)$ is $A^{\\mathsf{T}}A = I$ and $\\det(A)=1$. For a curve $\\gamma(t)$ lying entirely in $SO(3)$, this means $\\gamma(t)^{\\mathsf{T}}\\gamma(t) = I$ for all $t$ in some interval around $0$. Differentiating this equation with respect to $t$ using the product rule yields:\n$$\n\\frac{d}{dt}(\\gamma(t)^{\\mathsf{T}}\\gamma(t)) = \\frac{d}{dt}(I)\n$$\n$$\n\\gamma'(t)^{\\mathsf{T}}\\gamma(t) + \\gamma(t)^{\\mathsf{T}}\\gamma'(t) = 0\n$$\nEvaluating this at $t=0$, we use $\\gamma(0)=I$ and $\\gamma'(0)=X$:\n$$\nX^{\\mathsf{T}}I + I^{\\mathsf{T}}X = 0\n$$\n$$\nX^{\\mathsf{T}} + X = 0\n$$\nThis demonstrates that any element $X$ of the tangent space must be a skew-symmetric matrix.\n\nConversely, for any skew-symmetric matrix $X \\in M_3(\\mathbb{R})$ (i.e., $X^{\\mathsf{T}} = -X$), the matrix exponential $\\gamma(t) = \\exp(tX)$ defines a curve in $GL(3,\\mathbb{R})$. We check if it lies in $SO(3)$.\nFirst, $(\\exp(tX))^{\\mathsf{T}} = \\exp(tX^{\\mathsf{T}}) = \\exp(-tX) = (\\exp(tX))^{-1}$. Thus, $\\gamma(t)^{\\mathsf{T}}\\gamma(t)=I$.\nSecond, $\\det(\\exp(tX)) = \\exp(\\text{tr}(tX)) = \\exp(t \\cdot \\text{tr}(X))$. For any skew-symmetric matrix, all diagonal elements are zero, so $\\text{tr}(X) = 0$. This gives $\\det(\\exp(tX)) = \\exp(0) = 1$.\nThe curve $\\gamma(t)$ with $\\gamma(0) = I$ lies in $SO(3)$, and its tangent vector at $t=0$ is $\\gamma'(0) = X \\exp(0 \\cdot X) = X$. Thus, any real $3 \\times 3$ skew-symmetric matrix is in $\\mathfrak{so}(3)$.\n\nTherefore, the Lie algebra $\\mathfrak{so}(3)$ is precisely the space of all $3 \\times 3$ real skew-symmetric matrices, characterized by the equation $X^{\\mathsf{T}} + X = 0$.\n\n**Part 2: Parametrization and Basis for $\\mathfrak{so}(3)$**\n\nLet $X$ be an arbitrary element of $\\mathfrak{so}(3)$. The condition $X^{\\mathsf{T}} + X = 0$ implies that its components $x_{ij}$ must satisfy $x_{ji} + x_{ij} = 0$.\nFor the diagonal elements ($i=j$), this means $2x_{ii}=0$, so $x_{11}=x_{22}=x_{33}=0$.\nFor the off-diagonal elements, we have $x_{21}=-x_{12}$, $x_{31}=-x_{13}$, and $x_{32}=-x_{23}$.\nThe space of such matrices is $3$-dimensional. We can introduce three independent parameters $x_1, x_2, x_3 \\in \\mathbb{R}$ by setting $x_{32}=x_1$, $x_{13}=x_2$, and $x_{21}=x_3$. This yields the specified form:\n$$\nX = \\begin{pmatrix}\nx_{11}  x_{12}  x_{13} \\\\\nx_{21}  x_{22}  x_{23} \\\\\nx_{31}  x_{32}  x_{33}\n\\end{pmatrix} = \\begin{pmatrix}\n0  -x_3  x_2 \\\\\nx_3  0  -x_1 \\\\\n-x_2  x_1  0\n\\end{pmatrix}\n$$\nThis representation is unique for any given $X \\in \\mathfrak{so}(3)$. The standard basis $\\{E_1, E_2, E_3\\}$ is obtained by setting one of the parameters to $1$ and the others to $0$:\n- For $(x_1, x_2, x_3) = (1, 0, 0)$: $E_1 = \\begin{pmatrix} 0  0  0 \\\\ 0  0  -1 \\\\ 0  1  0 \\end{pmatrix}$\n- For $(x_1, x_2, x_3) = (0, 1, 0)$: $E_2 = \\begin{pmatrix} 0  0  1 \\\\ 0  0  0 \\\\ -1  0  0 \\end{pmatrix}$\n- For $(x_1, x_2, x_3) = (0, 0, 1)$: $E_3 = \\begin{pmatrix} 0  -1  0 \\\\ 1  0  0 \\\\ 0  0  0 \\end{pmatrix}$\n\n**Part 3: Lie Brackets and Structure Constants**\n\nThe Lie bracket is the matrix commutator, $[X,Y]=XY-YX$. We compute the brackets for the basis elements:\n$[E_1, E_2] = E_1E_2 - E_2E_1$:\n$$\nE_1E_2 = \\begin{pmatrix} 0  0  0 \\\\ 0  0  -1 \\\\ 0  1  0 \\end{pmatrix} \\begin{pmatrix} 0  0  1 \\\\ 0  0  0 \\\\ -1  0  0 \\end{pmatrix} = \\begin{pmatrix} 0  0  0 \\\\ 1  0  0 \\\\ 0  0  0 \\end{pmatrix}\n$$\n$$\nE_2E_1 = \\begin{pmatrix} 0  0  1 \\\\ 0  0  0 \\\\ -1  0  0 \\end{pmatrix} \\begin{pmatrix} 0  0  0 \\\\ 0  0  -1 \\\\ 0  1  0 \\end{pmatrix} = \\begin{pmatrix} 0  1  0 \\\\ 0  0  0 \\\\ 0  0  0 \\end{pmatrix}\n$$\n$$\n[E_1, E_2] = \\begin{pmatrix} 0  -1  0 \\\\ 1  0  0 \\\\ 0  0  0 \\end{pmatrix} = E_3\n$$\nBy cyclic permutation of indices $(1,2,3)$, or by direct calculation:\n$[E_2, E_3] = E_1$\n$[E_3, E_1] = E_2$\n\nThe structure constants $c_{ij}^k$ are defined by the relation $[E_i, E_j] = c_{ij}^k E_k$ (summation over $k$ is implied).\nFrom our calculations:\n- $[E_1, E_2] = E_3 = 0 \\cdot E_1 + 0 \\cdot E_2 + 1 \\cdot E_3 \\implies c_{12}^1=0, c_{12}^2=0, c_{12}^3=1$.\n- $[E_2, E_3] = E_1 = 1 \\cdot E_1 + 0 \\cdot E_2 + 0 \\cdot E_3 \\implies c_{23}^1=1, c_{23}^2=0, c_{23}^3=0$.\n- $[E_3, E_1] = E_2 = 0 \\cdot E_1 + 1 \\cdot E_2 + 0 \\cdot E_3 \\implies c_{31}^1=0, c_{31}^2=1, c_{31}^3=0$.\n\nUsing the anti-symmetry of the bracket, $[E_j, E_i] = -[E_i, E_j]$, we have $c_{ji}^k = -c_{ij}^k$. Also, $[E_i, E_i]=0 \\implies c_{ii}^k=0$.\nThe full set of non-zero structure constants can be compactly described using the Levi-Civita symbol $\\epsilon_{ijk}$:\n$c_{ij}^k = \\epsilon_{ijk}$, where $\\epsilon_{123}=1$ and it is anti-symmetric under the exchange of any two indices.\n\n**Part 4: Verification of the Jacobi Identity**\n\nThe Jacobi identity for a Lie algebra is $[[X,Y],Z] + [[Y,Z],X] + [[Z,X],Y] = 0$. In terms of the structure constants, this is expressed as:\n$$\nc_{ij}^{m}c_{mk}^{\\;\\;l}+c_{jk}^{m}c_{mi}^{\\;\\;l}+c_{ki}^{m}c_{mj}^{\\;\\;l}=0\n$$\nfor all $i,j,k,l \\in \\{1,2,3\\}$.\nSubstituting $c_{ij}^k = \\epsilon_{ijk}$, the identity becomes:\n$$\n\\epsilon_{ijm}\\epsilon_{mkl} + \\epsilon_{jkm}\\epsilon_{mil} + \\epsilon_{kim}\\epsilon_{mjl} = 0\n$$\nThis corresponds to the Jacobi identity for the vector cross product in $\\mathbb{R}^3$. Let $\\{e_1, e_2, e_3\\}$ be an orthonormal basis for $\\mathbb{R}^3$. The cross product is given by $e_i \\times e_j = \\sum_k \\epsilon_{ijk} e_k$. The Lie algebra structure of $(\\mathbb{R}^3, \\times)$ is isomorphic to $(\\mathfrak{so}(3), [\\cdot,\\cdot])$ via the map $e_i \\mapsto E_i$.\nThe Jacobi identity for vectors $a, b, c \\in \\mathbb{R}^3$ is $a \\times (b \\times c) + b \\times (c \\times a) + c \\times (a \\times b) = 0$.\nWe can prove this using the \"BAC-CAB\" rule, which is a fundamental identity of multilinear algebra in $\\mathbb{R}^3$: $u \\times (v \\times w) = v(u \\cdot w) - w(u \\cdot v)$.\nApplying this rule to each term:\n- $a \\times (b \\times c) = b(a \\cdot c) - c(a \\cdot b)$\n- $b \\times (c \\times a) = c(b \\cdot a) - a(b \\cdot c)$\n- $c \\times (a \\times b) = a(c \\cdot b) - b(c \\cdot a)$\nSumming these three equations and using the commutativity of the dot product (e.g., $a \\cdot c = c \\cdot a$):\n$$\n[b(a \\cdot c) - b(c \\cdot a)] + [-c(a \\cdot b) + c(b \\cdot a)] + [-a(b \\cdot c) + a(c \\cdot b)] = 0 + 0 + 0 = 0\n$$\nThis identity holds for any vectors $a,b,c$. By choosing $a=e_i$, $b=e_j$, $c=e_k$ and taking the $l$-th component, we recover the indexed form of the Jacobi identity for the structure constants. The verification is thus complete.\n\n**Final Calculation of $S$**\n\nThe scalar $S$ is defined as $S := c_{ij}^{k}c_{ij}^{k}$, with summation over all repeated indices $i,j,k$ from $1$ to $3$.\nSubstituting $c_{ij}^k = \\epsilon_{ijk}$, we have:\n$$\nS = \\sum_{i=1}^3 \\sum_{j=1}^3 \\sum_{k=1}^3 \\epsilon_{ijk} \\epsilon_{ijk} = \\sum_{i,j,k} (\\epsilon_{ijk})^2\n$$\nThe value of $(\\epsilon_{ijk})^2$ is $1$ if $\\{i,j,k\\}$ is a permutation of $\\{1,2,3\\}$, and $0$ if any two indices are equal.\nTherefore, the sum consists of adding $1$ for each distinct permutation of $(1,2,3)$. The number of such permutations is $3! = 6$. These are $(1,2,3)$, $(2,3,1)$, $(3,1,2)$ (even permutations, $\\epsilon=1$) and $(1,3,2)$, $(3,2,1)$, $(2,1,3)$ (odd permutations, $\\epsilon=-1$). In all six cases, $(\\epsilon_{ijk})^2 = 1$.\nThus, $S$ is the sum of six terms, each equal to $1$.\n$$\nS = 1+1+1+1+1+1 = 6\n$$\nAlternatively, using the identity for the contraction of two Levi-Civita symbols, $\\sum_{k=1}^3 \\epsilon_{ijk} \\epsilon_{abk} = \\delta_{ia}\\delta_{jb} - \\delta_{ib}\\delta_{ja}$:\n$$\nS = \\sum_{i=1}^3 \\sum_{j=1}^3 \\left( \\sum_{k=1}^3 \\epsilon_{ijk}\\epsilon_{ijk} \\right)\n$$\nFor the inner sum, we set $a=i$ and $b=j$:\n$$\n\\sum_{k=1}^3 \\epsilon_{ijk}\\epsilon_{ijk} = \\delta_{ii}\\delta_{jj} - \\delta_{ij}\\delta_{ji}\n$$\nNow we sum over $i$ and $j$:\n$$\nS = \\sum_{i=1}^3 \\sum_{j=1}^3 (\\delta_{ii}\\delta_{jj} - \\delta_{ij}\\delta_{ji}) = \\left(\\sum_{i=1}^3 \\delta_{ii}\\right)\\left(\\sum_{j=1}^3 \\delta_{jj}\\right) - \\sum_{i=1}^3 \\sum_{j=1}^3 \\delta_{ij}\\delta_{ji}\n$$\nThe sum $\\sum_{i=1}^3 \\delta_{ii} = \\delta_{11}+\\delta_{22}+\\delta_{33} = 1+1+1 = 3$.\nThe term $\\sum_{i,j} \\delta_{ij}\\delta_{ji} = \\sum_{i,j} \\delta_{ij}$ (since $\\delta_{ji}=\\delta_{ij}$ and $\\delta_{ij}^2 = \\delta_{ij}$). This sum is non-zero only when $i=j$, so it becomes $\\sum_{i=1}^3 \\delta_{ii} = 3$.\nTherefore,\n$$\nS = (3)(3) - 3 = 9 - 3 = 6\n$$\nBoth methods yield the same result.", "answer": "$$\\boxed{6}$$", "id": "3031872"}, {"introduction": "The universe of Lie algebras is not limited to simple ones like $\\mathfrak{so}(3)$; other structural classifications, such as solvability, are equally vital. A Lie algebra is solvable if its derived series—a sequence of subalgebras formed by repeated commutation—terminates at the trivial algebra. This exercise makes the abstract concept of solvability concrete by asking you to compute the derived series for the Lie algebra of $2 \\times 2$ upper-triangular matrices, thereby proving its solvability and determining its derived length. [@problem_id:1523101]", "problem": "Consider the set of all $2 \\times 2$ real upper-triangular matrices, denoted by $\\mathfrak{g}$. This set forms a Lie algebra over the real numbers, where the Lie bracket operation is defined by the matrix commutator: $[A, B] = AB - BA$ for any two matrices $A, B \\in \\mathfrak{g}$.\n\nThe derived series of this Lie algebra is a sequence of subalgebras defined recursively. The series starts with $\\mathfrak{g}^{(0)} = \\mathfrak{g}$. For any non-negative integer $k$, the next term in the series is defined as $\\mathfrak{g}^{(k+1)} = [\\mathfrak{g}^{(k)}, \\mathfrak{g}^{(k)}]$. The space $[\\mathfrak{h}, \\mathfrak{h}]$ for any subalgebra $\\mathfrak{h} \\subseteq \\mathfrak{g}$ is the vector space spanned by all possible commutators $[X, Y]$ where $X, Y \\in \\mathfrak{h}$.\n\nA Lie algebra is called solvable if its derived series eventually becomes the trivial algebra containing only the zero matrix. The smallest integer $n$ such that $\\mathfrak{g}^{(n)} = \\{0\\}$ is known as the derived length of the algebra.\n\nDetermine the derived length of the Lie algebra $\\mathfrak{g}$ of $2 \\times 2$ real upper-triangular matrices.", "solution": "Let $\\mathfrak{g}$ be the set of all real $2 \\times 2$ upper-triangular matrices. Any $A,B \\in \\mathfrak{g}$ can be written as\n$$\nA=\\begin{pmatrix} a  b \\\\ 0  d \\end{pmatrix}, \\quad B=\\begin{pmatrix} a'  b' \\\\ 0  d' \\end{pmatrix}.\n$$\nCompute their products:\n$$\nAB=\\begin{pmatrix} a a'  a b' + b d' \\\\ 0  d d' \\end{pmatrix}, \\quad BA=\\begin{pmatrix} a' a  a' b + b' d \\\\ 0  d' d \\end{pmatrix}.\n$$\nTherefore the commutator is\n$$\n[A,B]=AB-BA=\\begin{pmatrix} 0  a b' + b d' - a' b - b' d \\\\ 0  0 \\end{pmatrix}.\n$$\nHence $[A,B]$ is always strictly upper-triangular, so $[\\mathfrak{g},\\mathfrak{g}] \\subseteq \\mathbb{R}E_{12}$, where $E_{12}=\\begin{pmatrix} 0  1 \\\\ 0  0 \\end{pmatrix}$. To see equality, take $H=\\operatorname{diag}(1,0)$ and $E=E_{12}$; then\n$$\n[H,E]=HE - EH = \\begin{pmatrix} 0  1 \\\\ 0  0 \\end{pmatrix}=E \\neq 0,\n$$\nso $E \\in [\\mathfrak{g},\\mathfrak{g}]$ and thus $[\\mathfrak{g},\\mathfrak{g}] = \\mathbb{R}E_{12}$. Therefore\n$$\n\\mathfrak{g}^{(0)}=\\mathfrak{g}, \\quad \\mathfrak{g}^{(1)}=[\\mathfrak{g},\\mathfrak{g}]=\\mathbb{R}E_{12}.\n$$\nNow compute the next derived algebra:\n$$\n\\mathfrak{g}^{(2)}=[\\mathfrak{g}^{(1)},\\mathfrak{g}^{(1)}]=[\\mathbb{R}E_{12},\\mathbb{R}E_{12}].\n$$\nFor any scalars $\\alpha,\\beta$, one has $[\\alpha E_{12},\\beta E_{12}]=\\alpha\\beta[E_{12},E_{12}]=0$ because $E_{12}^{2}=0$, hence $[E_{12},E_{12}]=E_{12}^{2}-E_{12}^{2}=0$. Therefore\n$$\n\\mathfrak{g}^{(2)}=\\{0\\}.\n$$\nSince $\\mathfrak{g}^{(1)} \\neq \\{0\\}$ while $\\mathfrak{g}^{(2)}=\\{0\\}$, the smallest $n$ with $\\mathfrak{g}^{(n)}=\\{0\\}$ is $n=2$. Thus the derived length of $\\mathfrak{g}$ is $2$.", "answer": "$$\\boxed{2}$$", "id": "1523101"}]}