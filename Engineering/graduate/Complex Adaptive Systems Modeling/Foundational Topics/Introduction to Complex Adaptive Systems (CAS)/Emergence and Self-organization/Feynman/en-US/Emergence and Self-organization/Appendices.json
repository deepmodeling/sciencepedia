{
    "hands_on_practices": [
        {
            "introduction": "A key challenge in complex systems is bridging the gap between individual agent behaviors and the collective phenomena they produce. This exercise  provides practice with the mean-field approximation, a cornerstone technique for deriving the macroscopic dynamics of an order parameter directly from the microscopic rules of interaction. Mastering this derivation is essential for creating simplified, analytically tractable models of emergence in large populations of interacting agents.",
            "id": "4121309",
            "problem": "Consider a fully-connected population of $N$ identical agents with scalar micro-state $x_i(t) \\in \\mathbb{R}$ evolving under the coupled Ordinary Differential Equation (ODE) system\n$$\n\\dot{x}_i(t) \\;=\\; f\\!\\big(x_i(t)\\big) \\;+\\; \\sum_{j=1}^{N} g\\!\\big(x_i(t),\\,x_j(t)\\big),\n$$\nwhere $f:\\mathbb{R}\\to\\mathbb{R}$ and $g:\\mathbb{R}\\times\\mathbb{R}\\to\\mathbb{R}$ are smooth functions representing self-dynamics and pairwise interaction, respectively. As a macroscopic order parameter, define\n$$\nm(t) \\;=\\; \\frac{1}{N}\\sum_{i=1}^{N} x_i(t).\n$$\nAssume homogeneous mixing and weak, dense coupling that remains $\\mathcal{O}(1)$ as $N\\to\\infty$ by the normalization\n$$\ng\\!\\big(x_i,x_j\\big) \\;=\\; \\frac{\\kappa}{N}\\,h\\!\\big(x_i,x_j\\big),\n$$\nwith $\\kappa\\in\\mathbb{R}$ a constant and $h:\\mathbb{R}\\times\\mathbb{R}\\to\\mathbb{R}$ a symmetric, Lipschitz function such that $h(x,y)=h(y,x)$. Using a mean-field closure justified by exchangeability and concentration of the empirical distribution around $m(t)$ in the large-$N$ limit, derive an approximate autonomous dynamics for the order parameter $m(t)$ by expressing its drift as a closed function of $m(t)$ under first-order mean-field closure (neglecting variance and covariance corrections). Clearly state the modeling assumptions required for the closure to hold, starting from fundamental definitions and well-tested facts (e.g., the Law of Large Numbers). Your final answer must be the single closed-form analytic expression for the macroscopic drift of $m(t)$ as a function of $m$, obtained under the stated assumptions. Do not include an equality sign in your final expression. No numerical evaluation is required.",
            "solution": "The problem statement is a valid and well-posed problem in the domain of complex systems modeling, specifically concerning the derivation of macroscopic dynamics from microscopic rules using a mean-field approximation. The premises are scientifically grounded in the standard framework of statistical physics and dynamical systems theory. The terminology is precise, and the provided information is self-contained and sufficient for the derivation.\n\nThe objective is to derive an approximate autonomous dynamical equation for the macroscopic order parameter $m(t)$ defined as the population average of the micro-states $x_i(t)$. The starting point is the system of coupled Ordinary Differential Equations (ODEs) for a population of $N$ identical agents:\n$$\n\\dot{x}_i(t) \\;=\\; f\\!\\big(x_i(t)\\big) \\;+\\; \\sum_{j=1}^{N} g\\!\\big(x_i(t),\\,x_j(t)\\big)\n$$\nwhere $i=1, \\dots, N$. The interaction function $g$ is specified as $g(x_i, x_j) = \\frac{\\kappa}{N} h(x_i, x_j)$.\n\nFirst, we determine the time evolution of the order parameter $m(t)$ by differentiating its definition with respect to time $t$:\n$$\nm(t) \\;=\\; \\frac{1}{N}\\sum_{i=1}^{N} x_i(t)\n$$\n$$\n\\dot{m}(t) \\;=\\; \\frac{d}{dt} \\left( \\frac{1}{N}\\sum_{i=1}^{N} x_i(t) \\right) \\;=\\; \\frac{1}{N}\\sum_{i=1}^{N} \\dot{x}_i(t)\n$$\nSubstituting the expression for $\\dot{x}_i(t)$ from the given ODE system:\n$$\n\\dot{m}(t) \\;=\\; \\frac{1}{N}\\sum_{i=1}^{N} \\left[ f\\big(x_i(t)\\big) \\;+\\; \\sum_{j=1}^{N} g\\big(x_i(t), x_j(t)\\big) \\right]\n$$\nNow, we substitute the specific form of the coupling function $g\\big(x_i, x_j\\big) = \\frac{\\kappa}{N} h\\big(x_i, x_j\\big)$:\n$$\n\\dot{m}(t) \\;=\\; \\frac{1}{N}\\sum_{i=1}^{N} \\left[ f\\big(x_i(t)\\big) \\;+\\; \\sum_{j=1}^{N} \\frac{\\kappa}{N} h\\big(x_i(t), x_j(t)\\big) \\right]\n$$\nWe can separate the terms by distributing the outer summation:\n$$\n\\dot{m}(t) \\;=\\; \\frac{1}{N}\\sum_{i=1}^{N} f\\big(x_i(t)\\big) \\;+\\; \\frac{1}{N}\\sum_{i=1}^{N} \\sum_{j=1}^{N} \\frac{\\kappa}{N} h\\big(x_i(t), x_j(t)\\big)\n$$\nRearranging the constants, we obtain the exact expression for the rate of change of the mean:\n$$\n\\dot{m}(t) \\;=\\; \\left(\\frac{1}{N}\\sum_{i=1}^{N} f\\big(x_i(t)\\big)\\right) \\;+\\; \\kappa \\left(\\frac{1}{N^2}\\sum_{i=1}^{N} \\sum_{j=1}^{N} h\\big(x_i(t), x_j(t)\\big)\\right)\n$$\nThis expression is exact but not closed, as its right-hand side depends on the individual micro-states $x_i(t)$, not just on their mean $m(t)$. To obtain a closed-form dynamics for $m(t)$, we must introduce approximations based on the assumptions outlined in the problem.\n\nThe required assumptions for a first-order mean-field closure are as follows:\n1.  **Large Population Limit ($N \\to \\infty$):** The number of agents is assumed to be infinite. In this limit, by the Law of Large Numbers, empirical averages over the population converge to expectations over an underlying probability distribution of states. For instance, the term $\\frac{1}{N}\\sum_{i=1}^{N} \\phi(x_i)$ converges to the expectation $\\mathbb{E}[\\phi(x)]$.\n2.  **Mean-Field and Statistical Independence:** The core of the mean-field approximation is that the influence of the entire population on a single agent can be replaced by the influence of an average, or mean, field. A key consequence of this, known as the \"propagation of chaos\", is that in the limit $N \\to \\infty$, any two distinct agents $i \\neq j$ become statistically independent. The joint probability distribution of their states, $p(x_i, x_j)$, can be factored into the product of their identical marginal distributions, $p(x_i) p(x_j)$. The symmetry of the interaction function $h(x, y)$ and the identical nature of the agents (exchangeability) are crucial for this property to hold.\n3.  **First-Order Closure (Neglect of Fluctuations):** The problem explicitly requires a first-order closure, which neglects variance and higher-order central moments of the state distribution. This is equivalent to assuming that the distribution of states $p(x,t)$ is sharply peaked around its mean $m(t)$, such that it can be approximated by a Dirac delta function: $p(x,t) \\approx \\delta(x - m(t))$. This is the strongest assumption, which simplifies expectations of functions to functions of expectations.\n\nApplying these assumptions to the terms in the equation for $\\dot{m}(t)$:\n\nFor the first term, the empirical average of $f(x_i)$:\n$$\n\\frac{1}{N}\\sum_{i=1}^{N} f\\big(x_i(t)\\big) \\quad \\xrightarrow{N\\to\\infty} \\quad \\mathbb{E}\\big[f(x)\\big] \\;=\\; \\int f(x) p(x,t) dx\n$$\nApplying the first-order closure assumption, $p(x,t) \\approx \\delta(x - m(t))$:\n$$\n\\mathbb{E}\\big[f(x)\\big] \\approx \\int f(x) \\delta\\big(x - m(t)\\big) dx \\;=\\; f\\big(m(t)\\big)\n$$\n\nFor the second term, the double summation involving $h(x_i, x_j)$:\n$$\n\\frac{1}{N^2}\\sum_{i=1}^{N} \\sum_{j=1}^{N} h\\big(x_i(t), x_j(t)\\big) \\quad \\xrightarrow{N\\to\\infty} \\quad \\mathbb{E}\\big[h(x,y)\\big] \\;=\\; \\iint h(x,y) p(x,t)p(y,t) dx dy\n$$\nHere, we have already used the statistical independence assumption to write the joint expectation as an integral over the product of marginal densities. Now, applying the first-order closure assumption:\n$$\n\\mathbb{E}\\big[h(x,y)\\big] \\approx \\iint h(x,y) \\delta\\big(x - m(t)\\big)\\delta\\big(y - m(t)\\big) dx dy \\;=\\; h\\big(m(t), m(t)\\big)\n$$\n\nSubstituting these approximations back into the equation for $\\dot{m}(t)$:\n$$\n\\dot{m}(t) \\approx f\\big(m(t)\\big) + \\kappa h\\big(m(t), m(t)\\big)\n$$\nThis is the approximate autonomous dynamics for the order parameter $m(t)$. The macroscopic drift is the right-hand side of this equation, expressed as a function of the order parameter $m$. Suppressing the explicit time dependence, the drift function is $f(m) + \\kappa h(m, m)$.",
            "answer": "$$\\boxed{f(m) + \\kappa h(m, m)}$$"
        },
        {
            "introduction": "The spontaneous formation of patterns is a hallmark of self-organization, driven by a delicate balance of feedback mechanisms. This coding practice  explores one of the most fundamental mechanisms: the interplay between local positive feedback (activation) and global negative feedback (inhibition). By implementing and simulating this system, you will gain hands-on experience in how complex, stable spatial patterns can emerge from simple local rules and random initial conditions, without any external template or central control.",
            "id": "4121327",
            "problem": "Consider a one-dimensional ring of $N$ sites with periodic boundary conditions, indexed by $i \\in \\{0,1,\\dots,N-1\\}$. Each site carries an activation level $u_i(t) \\in \\mathbb{R}_{\\ge 0}$ that evolves over time due to local diffusion and local autocatalytic growth, with a single global resource $r(t) \\in \\mathbb{R}_{\\ge 0}$ that couples to all sites to provide negative feedback. The model is defined by the following system of ordinary differential equations and a discrete Laplacian approximation:\n$$\n\\frac{d u_i}{d t} = D \\left(u_{i-1} - 2 u_i + u_{i+1}\\right) + \\kappa\\, r(t)\\, u_i + \\alpha\\, u_i^2 - \\mu\\, u_i^3 - \\nu\\, u_i,\n$$\nwith periodic boundary conditions $u_{-1} \\equiv u_{N-1}$ and $u_{N} \\equiv u_{0}$, and the global resource dynamics\n$$\n\\frac{d r}{d t} = \\rho\\, (r_0 - r) - \\eta\\, r\\, \\bar{u}(t),\n\\quad\\text{where}\\quad\n\\bar{u}(t) = \\frac{1}{N}\\sum_{i=0}^{N-1} u_i(t).\n$$\nAll variables are dimensionless. The term with coefficient $D$ implements local diffusion according to Fick's law; the term $\\alpha\\, u_i^2$ provides local positive feedback (autocatalysis); the term $-\\mu\\, u_i^3$ saturates growth and prevents unbounded activation; the term $-\\nu\\, u_i$ provides baseline decay; and the term $\\kappa\\, r(t)\\, u_i$ couples global resource to local growth. The resource $r(t)$ relaxes toward the baseline $r_0$ at rate $\\rho$, and is depleted at a rate proportional to $r(t)$ and the global mean activation $\\bar{u}(t)$, representing global negative feedback via shared competition.\n\nYour task is to construct a complete program that:\n- Simulates the above model on a ring of size $N$ with periodic boundary conditions, using an explicit Euler time discretization with a fixed time step $\\Delta t$ for a total of $T$ steps. The discrete Laplacian should be implemented as $u_{i-1} - 2 u_i + u_{i+1}$ with periodic wrap-around.\n- Initializes the activation levels with small random perturbations around a small baseline: $u_i(0) = u_0 \\,(1 + \\epsilon_i)$ where $\\epsilon_i$ are independent and identically distributed uniform random variables in $[-\\varepsilon, \\varepsilon]$, and initializes the resource with $r(0) = r_0$. Use a deterministic random seed to ensure reproducibility.\n- After simulation, determines whether a stable self-organized pattern has emerged. Define a “stable self-organized pattern” as a non-homogeneous stationary configuration with persistent spatial structure, quantified by:\n  1. A nontrivial spatial variance at final time, $\\mathrm{Var}(u(T)) = \\frac{1}{N}\\sum_{i=0}^{N-1}\\left(u_i(T) - \\bar{u}(T)\\right)^2$, strictly greater than a threshold $\\theta_{\\mathrm{var}}$, and\n  2. A small mean per-step change in the final window of the simulation, defined as $\\Delta_{\\mathrm{avg}} = \\frac{1}{W}\\sum_{t=T-W}^{T-1} \\left(\\frac{1}{N}\\sum_{i=0}^{N-1} \\left|u_i(t+1) - u_i(t)\\right|\\right)$ strictly less than a threshold $\\theta_{\\mathrm{chg}}$, for a specified window length $W$.\n- If a stable pattern exists, compute the integer number of dominant peaks $P$ in the final spatial profile $u_i(T)$ by counting indices $i$ such that $u_i(T) > u_{i-1}(T)$ and $u_i(T) > u_{i+1}(T)$ and $u_i(T) > \\tau$, where $\\tau$ is a threshold defined as $\\tau = \\phi \\cdot \\max_i u_i(T)$, and ties are broken by strict inequality. If the configuration is homogeneous to within tolerance, defined by $\\max_i u_i(T) - \\min_i u_i(T) < \\theta_{\\mathrm{hom}}$, then set $P = 0$.\n\nBase your reasoning and parameter choice on the following fundamental principles and definitions:\n- Local diffusion follows Fick’s law, producing the discrete Laplacian term $u_{i-1} - 2 u_i + u_{i+1}$.\n- Autocatalytic growth captures local positive feedback via mass-action-like kinetics, represented by a superlinear term in $u_i$, here $\\alpha\\, u_i^2$.\n- Saturating nonlinearity models limited capacity and prevents blow-up, here $-\\mu\\, u_i^3$.\n- Global negative feedback arises from competition for a shared resource whose availability decreases with total usage, here $-\\eta\\, r\\, \\bar{u}$.\n- Stability of homogeneous equilibria can be analyzed via linearization: diffusion penalizes high-wavenumber modes, while global coupling alters the uniform mode.\n\nImplement your simulation with the following fixed numerical parameters and test suite, which must be embedded in the program and not read from input:\n\n- Discretization and detection parameters:\n  - Domain size: $N = 128$.\n  - Time step: $\\Delta t = 0.005$.\n  - Number of steps: $T = 10000$.\n  - Initialization baseline: $u_0 = 10^{-4}$.\n  - Initialization noise amplitude: $\\varepsilon = 10^{-2}$.\n  - Random seed: $42$.\n  - Final window length: $W = 500$.\n  - Variance threshold: $\\theta_{\\mathrm{var}} = 10^{-6}$.\n  - Change threshold: $\\theta_{\\mathrm{chg}} = 10^{-6}$.\n  - Homogeneity tolerance: $\\theta_{\\mathrm{hom}} = 10^{-8}$.\n  - Peak threshold fraction: $\\phi = 0.2$.\n\n- Test suite of model parameters $(D, \\kappa, \\alpha, \\mu, \\nu, \\rho, \\eta, r_0)$:\n  1. Case A (anticipated pattern-sustaining regime): $(D, \\kappa, \\alpha, \\mu, \\nu, \\rho, \\eta, r_0) = (0.02, 1.0, 2.0, 1.0, 0.1, 0.5, 1.0, 1.0)$.\n  2. Case B (high diffusion boundary, pattern suppressed): $(D, \\kappa, \\alpha, \\mu, \\nu, \\rho, \\eta, r_0) = (0.40, 1.0, 2.0, 1.0, 0.1, 0.5, 1.0, 1.0)$.\n  3. Case C (weak global negative feedback, homogeneous activation expected): $(D, \\kappa, \\alpha, \\mu, \\nu, \\rho, \\eta, r_0) = (0.02, 1.0, 2.0, 1.0, 0.1, 0.5, 0.05, 1.0)$.\n  4. Case D (strong decay, extinction expected): $(D, \\kappa, \\alpha, \\mu, \\nu, \\rho, \\eta, r_0) = (0.02, 1.0, 2.0, 1.0, 1.0, 0.5, 1.0, 1.0)$.\n\nYour program should simulate each case independently, apply the stability and peak-count criteria, and produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each entry being the integer number of dominant peaks $P$ for the corresponding case in order (Case A through Case D). For example, the program should print an output of the form $[p_A,p_B,p_C,p_D]$, where each $p_\\bullet$ is an integer.",
            "solution": "The problem is valid. It presents a well-posed system of reaction-diffusion partial differential equations with global coupling, a standard model in the study of complex adaptive systems and pattern formation. The model is scientifically grounded in established principles of local autocatalysis, diffusion, saturation, and global inhibition. All parameters, initial conditions, numerical methods, and analysis criteria are specified with sufficient precision and are mutually consistent, making the problem self-contained and solvable. There are no violations of scientific principles, mathematical logic, or objectivity.\n\nThe solution is constructed by first discretizing the continuous-time model and then implementing a numerical simulation, followed by a quantitative analysis of the simulation output to characterize the emergent patterns.\n\n**1. Model Discretization and Simulation Algorithm**\n\nThe model consists of a system of coupled ordinary differential equations (ODEs). To simulate its evolution, we employ the explicit Euler method for time discretization. A state variable $y(t)$ governed by $\\frac{dy}{dt} = f(y(t), t)$ is updated at discrete time steps $t_k = k \\Delta t$ according to the rule $y(t_{k+1}) = y(t_k) + \\Delta t \\cdot f(y(t_k), t_k)$.\n\nApplying this to the activation levels $u_i$ and the global resource $r$, we obtain the following update equations:\n$$\nu_i(t+\\Delta t) = u_i(t) + \\Delta t \\left[ D \\left(u_{i-1}(t) - 2 u_i(t) + u_{i+1}(t)\\right) + \\kappa\\, r(t)\\, u_i(t) + \\alpha\\, u_i(t)^2 - \\mu\\, u_i(t)^3 - \\nu\\, u_i(t) \\right]\n$$\n$$\nr(t+\\Delta t) = r(t) + \\Delta t \\left[ \\rho\\, (r_0 - r(t)) - \\eta\\, r(t)\\, \\bar{u}(t) \\right]\n$$\nwhere $\\bar{u}(t) = \\frac{1}{N}\\sum_{j=0}^{N-1} u_j(t)$. All variables on the right-hand side are evaluated at the current time step $t$.\n\nThe entire system of $N$ sites is represented by a vector $u(t) = [u_0(t), u_1(t), \\dots, u_{N-1}(t)]$. This allows for efficient, vectorized computation. The discrete Laplacian term, $L(u)_i = u_{i-1} - 2 u_i + u_{i+1}$, which models diffusion, can be computed for all sites simultaneously. Given the periodic boundary conditions ($u_{-1} \\equiv u_{N-1}$ and $u_{N} \\equiv u_{0}$), this operation is equivalent to `np.roll(u, 1) + np.roll(u, -1) - 2*u` in a numerical library like NumPy.\n\n**2. Algorithmic Implementation**\n\nThe simulation proceeds as follows:\n\n- **Initialization**:\n  1. A random number generator is seeded with the specified value, $42$, to ensure reproducibility.\n  2. The activation vector $u$ of size $N=128$ is initialized. For each site $i$, $u_i(0) = u_0 (1 + \\epsilon_i)$, where $u_0 = 10^{-4}$ and $\\epsilon_i$ is drawn from a uniform distribution $U[-\\varepsilon, \\varepsilon]$ with $\\varepsilon = 10^{-2}$.\n  3. The global resource $r$ is initialized to its baseline value, $r(0) = r_0$.\n\n- **Time Evolution Loop**:\n  The system state $(u, r)$ is evolved for a total of $T = 10000$ steps with a time step of $\\Delta t = 0.005$. In each step:\n  1. The mean activation $\\bar{u}(t)$ is calculated from the current activation vector $u(t)$.\n  2. The time derivatives $\\frac{du}{dt}$ (a vector) and $\\frac{dr}{dt}$ (a scalar) are computed using the current state $(u(t), r(t))$ and the discretized equations.\n  3. The state is updated to $(u(t+\\Delta t), r(t+\\Delta t))$ using the Euler step.\n  4. For the final $W=500$ steps of the simulation (i.e., for $t$ from $T-W$ to $T-1$), the mean absolute change per site, $\\frac{1}{N}\\sum_{i=0}^{N-1} |u_i(t+1) - u_i(t)|$, is computed and stored. This is used to calculate the stationarity metric $\\Delta_{\\mathrm{avg}}$.\n\n**3. Post-Simulation Analysis and Pattern Characterization**\n\nAfter completing $T$ simulation steps, the final state $u(T)$ is analyzed to determine the number of peaks, $P$. This analysis follows a strict, hierarchical set of criteria:\n\n1.  **Homogeneity Check**: First, we test if the final configuration is effectively homogeneous. The difference between the maximum and minimum activation levels, $\\max_i u_i(T) - \\min_i u_i(T)$, is computed. If this value is less than the homogeneity tolerance $\\theta_{\\mathrm{hom}} = 10^{-8}$, the state is considered homogeneous, and the number of peaks $P$ is set to $0$.\n\n2.  **Stable Pattern Verification**: If the configuration is not homogeneous by the above criterion, we check if it qualifies as a \"stable self-organized pattern\" as defined in the problem. This requires two conditions to be met simultaneously:\n    a. **Non-trivial Spatial Variance**: The spatial variance of the final activation profile, $\\mathrm{Var}(u(T)) = \\frac{1}{N}\\sum_{i=0}^{N-1}\\left(u_i(T) - \\bar{u}(T)\\right)^2$, must be strictly greater than the threshold $\\theta_{\\mathrm{var}} = 10^{-6}$. If not, the pattern lacks significant spatial structure, and $P$ is set to $0$.\n    b. **Stationarity**: The system must have settled into a nearly stationary state. This is quantified by the average change metric $\\Delta_{\\mathrm{avg}} = \\frac{1}{W}\\sum_{t=T-W}^{T-1} \\left(\\frac{1}{N}\\sum_{i=0}^{N-1} \\left|u_i(t+1) - u_i(t)\\right|\\right)$. This value must be strictly less than the change threshold $\\theta_{\\mathrm{chg}} = 10^{-6}$. If the system is still evolving too rapidly, it is not stable, and $P$ is set to $0$.\n\n3.  **Peak Counting**: If the configuration passes all the preceding checks (it is non-homogeneous, has sufficient variance, and is stationary), we proceed to count the number of dominant peaks. A site $i$ is counted as a peak if it satisfies three conditions:\n    a. It is a local maximum: $u_i(T) > u_{i-1}(T)$ and $u_i(T) > u_{i+1}(T)$ (with periodic indices).\n    b. Its activation is significant: $u_i(T) > \\tau$, where the threshold $\\tau$ is defined as a fraction of the global maximum activation, $\\tau = \\phi \\cdot \\max_j u_j(T)$, with $\\phi = 0.2$.\n\nThe total count of sites satisfying these conditions gives the integer number of peaks, $P$. This entire procedure is applied independently to each of the four test cases provided.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run simulations for all test cases and print the results.\n    \"\"\"\n\n    # Discretization and detection parameters from the problem statement\n    sim_config = {\n        'N': 128,          # Domain size\n        'dt': 0.005,       # Time step\n        'T_steps': 10000,  # Number of steps\n        'u0': 1e-4,        # Initialization baseline\n        'epsilon': 1e-2,   # Initialization noise amplitude\n        'seed': 42,        # Random seed\n        'W': 500,          # Final window length for stability check\n        'theta_var': 1e-6, # Variance threshold\n        'theta_chg': 1e-6, # Change threshold\n        'theta_hom': 1e-8, # Homogeneity tolerance\n        'phi': 0.2,        # Peak threshold fraction\n    }\n\n    # Test suite of model parameters (D, kappa, alpha, mu, nu, rho, eta, r0)\n    test_cases = [\n        # Case A: anticipated pattern-sustaining regime\n        (0.02, 1.0, 2.0, 1.0, 0.1, 0.5, 1.0, 1.0),\n        # Case B: high diffusion boundary, pattern suppressed\n        (0.40, 1.0, 2.0, 1.0, 0.1, 0.5, 1.0, 1.0),\n        # Case C: weak global negative feedback, homogeneous activation expected\n        (0.02, 1.0, 2.0, 1.0, 0.1, 0.5, 0.05, 1.0),\n        # Case D: strong decay, extinction expected\n        (0.02, 1.0, 2.0, 1.0, 1.0, 0.5, 1.0, 1.0),\n    ]\n\n    results = []\n    # Set the seed once for reproducibility across all cases\n    np.random.seed(sim_config['seed'])\n    # Generate the initial random perturbations once\n    initial_perturbations = sim_config['u0'] * (\n        1 + (np.random.rand(sim_config['N']) * 2 - 1) * sim_config['epsilon']\n    )\n\n    for params in test_cases:\n        p = run_simulation(params, sim_config, initial_perturbations.copy())\n        results.append(p)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef run_simulation(params, config, initial_u):\n    \"\"\"\n    Simulates the model for one set of parameters and returns the number of peaks.\n    \"\"\"\n    D, kappa, alpha, mu, nu, rho, eta, r0 = params\n    N, dt, T_steps, W = config['N'], config['dt'], config['T_steps'], config['W']\n    \n    # Initialization\n    u = initial_u\n    r = r0\n    \n    changes_in_window = []\n\n    # Time evolution loop\n    for t_step in range(T_steps):\n        u_old = u.copy()\n        \n        # Calculate discrete Laplacian with periodic boundary conditions\n        u_left_shifted = np.roll(u, 1)\n        u_right_shifted = np.roll(u, -1)\n        laplacian = u_left_shifted - 2 * u + u_right_shifted\n\n        # Calculate time derivative for u\n        du_dt = (D * laplacian + \n                 kappa * r * u + \n                 alpha * u**2 - \n                 mu * u**3 - \n                 nu * u)\n        \n        # Calculate time derivative for r\n        u_bar = np.mean(u)\n        dr_dt = rho * (r0 - r) - eta * r * u_bar\n\n        # Update u and r using Euler's method\n        u = u + dt * du_dt\n        r = r + dt * dr_dt\n        \n        # Ensure activations remain non-negative\n        u[u < 0] = 0\n\n        # Collect data for stability check in the final window\n        if t_step >= T_steps - W:\n            step_change = np.mean(np.abs(u - u_old))\n            changes_in_window.append(step_change)\n\n    # Post-simulation analysis\n    u_final = u\n\n    # 1. Homogeneity check\n    if np.max(u_final) - np.min(u_final) < config['theta_hom']:\n        return 0\n\n    # 2. Stable pattern verification\n    # a. Variance check\n    var_u = np.var(u_final)\n    if var_u <= config['theta_var']:\n        return 0\n    \n    # b. Stationarity check\n    delta_avg = np.mean(changes_in_window)\n    if delta_avg >= config['theta_chg']:\n        return 0\n        \n    # 3. Peak counting\n    u_max = np.max(u_final)\n    if u_max == 0: # Extinct state\n        return 0\n        \n    peak_threshold = config['phi'] * u_max\n    \n    u_final_left = np.roll(u_final, 1)\n    u_final_right = np.roll(u_final, -1)\n\n    is_peak = ( (u_final > u_final_left) & \n                (u_final > u_final_right) & \n                (u_final > peak_threshold) )\n    \n    num_peaks = np.sum(is_peak)\n    \n    return num_peaks\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "Many complex adaptive systems are best understood as networks, where emergent global properties arise from local connectivity patterns. This exercise  introduces spectral graph theory as a powerful tool for detecting these hidden structures. You will learn to use the eigenvectors of the graph Laplacian to uncover emergent features like community partitions, demonstrating how to mathematically quantify the self-organized architecture of a complex network.",
            "id": "4121314",
            "problem": "Consider a network modeled as an undirected, unweighted graph with $n$ nodes, represented by a symmetric adjacency matrix $A \\in \\mathbb{R}^{n \\times n}$ with entries $A_{ij} \\in \\{0,1\\}$ and $A_{ii} = 0$ for all $i$. Define the degree vector $d \\in \\mathbb{R}^{n}$ by $d_i = \\sum_{j=1}^n A_{ij}$, the diagonal degree matrix $D = \\mathrm{diag}(d)$, the (unnormalized) combinatorial Laplacian $L = D - A$, and the normalized Laplacian $\\mathcal{L} = I - D^{-1/2} A D^{-1/2}$, with the convention that for any node $i$ with $d_i = 0$ one sets $(D^{-1/2})_{ii} = 0$. These constructions arise in spectral graph theory and are fundamental to studying emergence and self-organization in complex adaptive systems: simple local interaction rules encoded by $A$ can lead to emergent modularity (communities) or bipartite structure, which are detectable by spectral quantities of $L$ and $\\mathcal{L}$.\n\nUse the following well-tested facts as the foundational base:\n- For any undirected graph, $L$ is symmetric positive semidefinite, and the multiplicity of the eigenvalue $0$ of $L$ equals the number of connected components.\n- The second smallest eigenvalue of $L$, denoted $\\lambda_2$, and its associated eigenvector (the Fiedler vector) provide a relaxation of cut-based community discovery: thresholding the Fiedler vector yields an informative bipartition that often aligns with emergent community structure.\n- The normalized Laplacian $\\mathcal{L}$ has eigenvalues in the interval $[0,2]$, and a connected graph is bipartite if and only if the largest eigenvalue of $\\mathcal{L}$ equals $2$.\n\nYour task is to implement a complete program that, for a given small test suite of graphs, detects emergent bipartite structure and produces a spectral bipartition indicative of community structure via the Fiedler vector. For each test case, perform the following steps:\n1. Compute $L$ and $\\mathcal{L}$ from $A$.\n2. Compute the eigenvalues of $L$ and determine the number of connected components $c$ by counting how many eigenvalues satisfy $\\lambda < \\epsilon$, where $\\epsilon = 10^{-8}$.\n3. Compute the eigenvalues of $\\mathcal{L}$ and set a bipartiteness indicator $b$ to true if $\\max \\mathrm{spec}(\\mathcal{L})$ is within $\\delta$ of $2$, i.e., $|2 - \\lambda_{\\max}(\\mathcal{L})| \\le \\delta$ with $\\delta = 10^{-8}$, and false otherwise.\n4. Compute the Fiedler vector $v$ of $L$ (the eigenvector associated with the second smallest eigenvalue $\\lambda_2$). Construct a bipartition $(S_+, S_-)$ by thresholding at $0$: $S_+ = \\{ i \\mid v_i \\ge 0 \\}$ and $S_- = \\{ i \\mid v_i < 0 \\}$. If one of these sets is empty, instead threshold at the median of $v$ to ensure both parts are nonempty.\n5. Compute the normalized cut value of the partition, defined by\n$$\n\\mathrm{NCut}(S_+, S_-) = \\mathrm{cut}(S_+, S_-) \\left( \\frac{1}{\\mathrm{vol}(S_+)} + \\frac{1}{\\mathrm{vol}(S_-)} \\right),\n$$\nwhere\n$$\n\\mathrm{cut}(S_+, S_-) = \\sum_{i \\in S_+} \\sum_{j \\in S_-} A_{ij}, \\quad \\mathrm{vol}(S) = \\sum_{i \\in S} d_i.\n$$\nIf either $\\mathrm{vol}(S_+)$ or $\\mathrm{vol}(S_-)$ is zero, define $\\mathrm{NCut}(S_+, S_-) = 0$ if $\\mathrm{cut}(S_+, S_-) = 0$ and otherwise define it as $+\\infty$.\n\nFor each test case, produce a result list $[\\;b,\\;c,\\;\\lambda_2,\\;\\mathrm{NCut}(S_+, S_-)\\;]$, where $b$ is a boolean, $c$ is an integer, and $\\lambda_2$ and $\\mathrm{NCut}(S_+, S_-)$ are floats rounded to $6$ decimal places.\n\nTest suite specification:\n- Case $1$ (complete bipartite graph $K_{3,3}$): nodes $\\{0,1,2\\}$ and $\\{3,4,5\\}$ with all cross edges present, i.e., edges $(i,j)$ for all $i \\in \\{0,1,2\\}$ and $j \\in \\{3,4,5\\}$.\n- Case $2$ (two dense communities weakly bridged): nodes $\\{0,1,2,3\\}$ form a clique and nodes $\\{4,5,6,7\\}$ form a clique, with two bridging edges $(1,5)$ and $(2,6)$.\n- Case $3$ ($K_{3,3}$ plus one intra-part edge that breaks bipartiteness): the edges of Case $1$ plus the edge $(0,1)$.\n- Case $4$ (path graph $P_6$): nodes $\\{0,1,2,3,4,5\\}$ with edges $(0,1)$, $(1,2)$, $(2,3)$, $(3,4)$, $(4,5)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a sublist in the order specified above. For example, the format is $[\\,[\\ldots],\\,[\\ldots],\\,[\\ldots],\\,[\\ldots]\\,]$. No physical units or angle units are involved in this problem. All floating-point outputs must be rounded to $6$ decimal places as specified. The tolerance parameters must be taken as $\\epsilon = 10^{-8}$ and $\\delta = 10^{-8}$ exactly. The program must be self-contained and require no input.",
            "solution": "The problem posed is valid. It is scientifically grounded in the principles of spectral graph theory, a well-established discipline in mathematics and computer science used to study the properties of complex networks. The problem is well-posed, providing a complete and unambiguous set of definitions, data, and procedural instructions. It is objective, free of subjective claims, and computationally feasible. The task is to analyze a series of graphs using their spectral properties, which is a core technique in understanding emergent structures in complex adaptive systems.\n\nThe solution proceeds by implementing the specified analytical steps for each graph in the test suite. We will first construct the adjacency matrix $A$ for each graph. From $A$, we derive the fundamental matrices for spectral analysis: the degree matrix $D$, the combinatorial Laplacian $L = D - A$, and the normalized Laplacian $\\mathcal{L} = I - D^{-1/2} A D^{-1/2}$.\n\n**1. Matrix Construction and Eigen-decomposition**\n\nFor a graph with $n$ nodes, the adjacency matrix $A$ is an $n \\times n$ matrix where $A_{ij}=1$ if an edge exists between nodes $i$ and $j$, and $A_{ij}=0$ otherwise. The degree $d_i$ of a node $i$ is the number of edges connected to it, calculated as $d_i = \\sum_{j=1}^n A_{ij}$. The degree matrix $D$ is a diagonal matrix with degrees $d_i$ on its diagonal, i.e., $D_{ii} = d_i$.\n\nThe combinatorial Laplacian is $L = D - A$. The normalized Laplacian $\\mathcal{L}$ is constructed using the matrix $D^{-1/2}$, which is a diagonal matrix with elements $(D^{-1/2})_{ii} = 1/\\sqrt{d_i}$ if $d_i > 0$ and $0$ if $d_i=0$. Then, $\\mathcal{L} = I - D^{-1/2} A D^{-1/2}$.\n\nSince both $L$ and $\\mathcal{L}$ are real symmetric matrices for any undirected graph, their eigenvalues are real, and they possess a full set of orthogonal eigenvectors. We can compute these using numerical linear algebra libraries, specifically by finding the solution to the eigenvalue problem $Mx = \\lambda x$.\n\n**2. Algorithmic Procedure**\n\nFor each graph, we execute the following sequence of computations:\n\n- **Step 1: Compute Laplacians.** Given the adjacency matrix $A$, we compute the degree vector $d$, the degree matrix $D$, the combinatorial Laplacian $L$, and the normalized Laplacian $\\mathcal{L}$ according to their definitions.\n\n- **Step 2: Determine Connectivity.** We compute the eigenvalues of $L$. According to spectral graph theory, the number of connected components $c$ in the graph is equal to the multiplicity of the eigenvalue $0$. Numerically, we count the number of eigenvalues $\\lambda$ that are smaller than a small tolerance $\\epsilon = 10^{-8}$.\n\n- **Step 3: Test for Bipartiteness.** We compute the eigenvalues of the normalized Laplacian $\\mathcal{L}$. A connected graph is bipartite if and only if its largest eigenvalue is $2$. We set a boolean indicator $b$ to true if the maximum eigenvalue $\\lambda_{\\max}(\\mathcal{L})$ satisfies $|2 - \\lambda_{\\max}(\\mathcal{L})| \\le \\delta$, where $\\delta = 10^{-8}$.\n\n- **Step 4: Spectral Bipartitioning.** The eigenvector corresponding to the second smallest eigenvalue ($\\lambda_2$) of $L$ is called the Fiedler vector, denoted $v$. This vector provides a continuous relaxation for the discrete graph partitioning problem. We construct a bipartition of the graph's nodes, $(S_+, S_-)$, by thresholding the components of the Fiedler vector. Nodes $i$ for which $v_i \\ge 0$ are placed in set $S_+$, and nodes for which $v_i < 0$ are placed in set $S_-$. If this procedure results in one of the sets being empty, we switch to thresholding at the median of the Fiedler vector's components to ensure a non-trivial partition.\n\n- **Step 5: Compute Normalized Cut.** The quality of the partition $(S_+, S_-)$ is quantified by the normalized cut value, $\\mathrm{NCut}$. It is defined as:\n$$\n\\mathrm{NCut}(S_+, S_-) = \\mathrm{cut}(S_+, S_-) \\left( \\frac{1}{\\mathrm{vol}(S_+)} + \\frac{1}{\\mathrm{vol}(S_-)} \\right)\n$$\nHere, $\\mathrm{cut}(S_+, S_-) = \\sum_{i \\in S_+} \\sum_{j \\in S_-} A_{ij}$ is the number of edges connecting the two partitions, and $\\mathrm{vol}(S) = \\sum_{i \\in S} d_i$ is the sum of degrees of nodes in a partition $S$. The special case where a volume is zero is handled as specified: if $\\mathrm{vol}(S_+)$ or $\\mathrm{vol}(S_-)$ is zero, $\\mathrm{NCut}$ is $0$ if the cut is also zero, and $\\infty$ otherwise. For the given test cases, all graphs are connected, so the volume of any non-empty partition will be positive, and this special case will not be encountered.\n\nThe final output for each graph is a list containing the bipartiteness indicator $b$, the number of connected components $c$, the Fiedler value $\\lambda_2$, and the normalized cut value $\\mathrm{NCut}$, with floating-point numbers rounded to $6$ decimal places.\n\n**Application to Test Cases**\n\nThe procedure is applied to the adjacency matrices representing the four test cases:\n1.  **$K_{3,3}$**: A complete bipartite graph. Expected: $c=1$, $b=\\text{true}$. The Fiedler vector will perfectly separate the two node sets of the bipartition.\n2.  **Two weakly bridged cliques**: A graph with strong community structure. Expected: $c=1$, $b=\\text{false}$. $\\lambda_2$ should be small, indicating a good but non-trivial cut. The Fiedler vector should separate the two cliques.\n3.  **$K_{3,3}$ with an extra edge**: A nearly bipartite graph. Expected: $c=1$, $b=\\text{false}$. The properties should be close to those of $K_{3,3}$, but bipartiteness is broken.\n4.  **Path graph $P_6$**: A simple bipartite graph. Expected: $c=1$, $b=\\text{true}$. The Fiedler vector will yield a partition corresponding to the natural coloring of the path graph.\n\nThe implementation will use `numpy` for all matrix operations and eigen-decompositions.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run analysis, and print results.\n    \"\"\"\n\n    # --- Test Case Definitions ---\n\n    # Case 1: Complete bipartite graph K_3,3\n    # Nodes {0,1,2} and {3,4,5}\n    A1 = np.zeros((6, 6), dtype=int)\n    for i in range(3):\n        for j in range(3, 6):\n            A1[i, j] = 1\n            A1[j, i] = 1\n\n    # Case 2: Two dense communities weakly bridged\n    # Cliques {0,1,2,3} and {4,5,6,7}. Bridges (1,5) and (2,6).\n    A2 = np.zeros((8, 8), dtype=int)\n    # Clique 1\n    for i in range(4):\n        for j in range(i + 1, 4):\n            A2[i, j] = 1\n            A2[j, i] = 1\n    # Clique 2\n    for i in range(4, 8):\n        for j in range(i + 1, 8):\n            A2[i, j] = 1\n            A2[j, i] = 1\n    # Bridges\n    A2[1, 5] = A2[5, 1] = 1\n    A2[2, 6] = A2[6, 2] = 1\n\n    # Case 3: K_3,3 plus one intra-part edge (0,1)\n    A3 = A1.copy()\n    A3[0, 1] = A3[1, 0] = 1\n\n    # Case 4: Path graph P_6\n    # Nodes {0,1,2,3,4,5}\n    A4 = np.zeros((6, 6), dtype=int)\n    for i in range(5):\n        A4[i, i + 1] = 1\n        A4[i + 1, i] = 1\n\n    test_cases = [A1, A2, A3, A4]\n    \n    results = []\n    for A in test_cases:\n        result = analyze_graph(A)\n        results.append(result)\n\n    # Format output as a list of lists.\n    # str() correctly converts booleans to 'True'/'False' and numbers to strings.\n    formatted_results = [f\"[{','.join(map(str, res))}]\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\n\ndef analyze_graph(A, epsilon=1e-8, delta=1e-8):\n    \"\"\"\n    Performs spectral analysis on a graph given its adjacency matrix A.\n    \n    Args:\n        A (np.ndarray): The symmetric adjacency matrix of the graph.\n        epsilon (float): Tolerance for counting zero eigenvalues of L.\n        delta (float): Tolerance for checking bipartiteness from eigenvalues of L_norm.\n\n    Returns:\n        list: A list containing [b, c, lambda_2, NCut].\n    \"\"\"\n    n = A.shape[0]\n    if n == 0:\n        return [False, 0, 0.0, 0.0]\n\n    # --- Step 1: Compute Laplacians ---\n    d = A.sum(axis=1)\n    D = np.diag(d)\n    L = D - A\n\n    # Normalized Laplacian L_norm = I - D^(-1/2) * A * D^(-1/2)\n    # Handle nodes with degree 0\n    d_inv_sqrt_vals = np.zeros(n)\n    d_gt_0_indices = d > 0\n    d_inv_sqrt_vals[d_gt_0_indices] = 1.0 / np.sqrt(d[d_gt_0_indices])\n    D_inv_sqrt = np.diag(d_inv_sqrt_vals)\n    L_norm = np.eye(n) - D_inv_sqrt @ A @ D_inv_sqrt\n\n    # --- Step 2: Determine Connectivity ---\n    # np.linalg.eigh is for symmetric matrices, returns sorted eigenvalues.\n    eigvals_L, eigvecs_L = np.linalg.eigh(L)\n    c = np.sum(eigvals_L < epsilon)\n    \n    # Second smallest eigenvalue (Fiedler value)\n    lambda_2 = 0.0\n    if n > 1:\n        lambda_2 = eigvals_L[1]\n\n    # --- Step 3: Test for Bipartiteness ---\n    eigvals_L_norm = np.linalg.eigvalsh(L_norm)\n    lambda_max_L_norm = np.max(eigvals_L_norm) if eigvals_L_norm.size > 0 else 0.0\n    b = np.abs(lambda_max_L_norm - 2.0) <= delta\n\n    # --- Step 4: Spectral Bipartitioning ---\n    S_plus_indices = np.array([], dtype=int)\n    S_minus_indices = np.array([], dtype=int)\n    if n > 1:\n        # Fiedler vector is the eigenvector for lambda_2\n        v_fiedler = eigvecs_L[:, 1]\n        \n        S_plus_indices = np.where(v_fiedler >= 0)[0]\n        S_minus_indices = np.where(v_fiedler < 0)[0]\n\n        # If one set is empty, threshold at the median\n        if S_plus_indices.size == 0 or S_minus_indices.size == 0:\n            median = np.median(v_fiedler)\n            # This refined logic handles cases where median is min/max\n            if np.all(v_fiedler >= median):\n                 # move the element with smallest value to S_minus\n                 min_val_idx = np.argmin(v_fiedler)\n                 S_plus_indices = np.setdiff1d(np.arange(n), [min_val_idx])\n                 S_minus_indices = np.array([min_val_idx])\n            else:\n                 S_plus_indices = np.where(v_fiedler >= median)[0]\n                 S_minus_indices = np.where(v_fiedler < median)[0]\n\n\n    # --- Step 5: Compute Normalized Cut ---\n    cut_val = 0.0\n    ncut_val = 0.0\n    if S_plus_indices.size > 0 and S_minus_indices.size > 0:\n        # A[S_plus, S_minus] is not valid, need to expand slicing\n        cut_val = A[S_plus_indices[:, np.newaxis], S_minus_indices].sum()\n\n    vol_S_plus = d[S_plus_indices].sum()\n    vol_S_minus = d[S_minus_indices].sum()\n\n    if vol_S_plus == 0 or vol_S_minus == 0:\n        if cut_val > 0:\n            ncut_val = np.inf\n        else:\n            ncut_val = 0.0\n    else:\n        ncut_val = cut_val * (1.0 / vol_S_plus + 1.0 / vol_S_minus)\n\n    # Format results with rounding\n    return [b, int(c), round(lambda_2, 6), round(ncut_val, 6)]\n\n# Execute the solution\nsolve()\n\n```"
        }
    ]
}