{
    "hands_on_practices": [
        {
            "introduction": "在设计智能体时，一个核心问题是它如何记忆和处理历史信息。本练习将指导您从第一性原理出发，通过最小化一个带折扣的二次损失函数，来推导出一个指数遗忘机制。这个过程不仅揭示了指数加权移动平均（EWMA）的优化基础，还帮助您量化“有效记忆时域”这一关键概念，这对于设计具有学习和适应能力的智能体至关重要。",
            "id": "4120395",
            "problem": "考虑一个复杂适应系统中的智能体，它接收一个标量观测值流 $\\{x_t\\}_{t \\in \\mathbb{Z}_{\\ge 0}}$。该智能体维持一个标量内部属性 $m_t$，该属性总结了过去的观测值，并每隔一个时间步更新一次。假设在时间 $t+1$，智能体选择 $m_{t+1}$ 以最小化半无限历史上的折扣二次损失，\n$$\nJ_{t+1}(m) \\equiv \\sum_{k=0}^{\\infty} \\delta^{k} \\left(m - x_{t-k}\\right)^{2},\n$$\n其中 $\\delta \\in (0,1)$ 是一个固定的折扣因子，反映了指数新近加权。\n\n任务：\n1. 从凸优化和几何级数性质的第一性原理出发，推导 $J_{t+1}(m)$ 的闭式最小化子，并证明它是一个过去观测值的几何加权平均。然后将此表示代数转换为一个关联 $m_{t+1}$ 与 $m_t$ 和 $x_t$ 的单步递推关系；将 $m_t$ 上的系数解释为一个遗忘因子 $\\alpha \\in (0,1)$，并用 $\\delta$ 表示 $\\alpha$。陈述当 $x_t \\equiv 0$ 时得到的齐次递推的稳定性条件。\n2. 为了形式化属性 $m_{t}$ 的有效记忆时域的概念，将在时间 $t+1$ 分配给观测值 $x_{t-k}$ 的归一化权重定义为 $p_k$，因此 $p_k$ 与应用于延迟 $k$ 的折扣成正比，并满足 $\\sum_{k=0}^{\\infty} p_k = 1$。将有效记忆时域 $H(\\alpha)$ 定义为该分布下的期望延迟，\n$$\nH(\\alpha) \\equiv \\sum_{k=0}^{\\infty} k \\, p_k.\n$$\n计算 $H(\\alpha)$ 作为 $\\alpha$ 的函数的闭式解。您最终报告的答案必须仅为 $H(\\alpha)$ 作为 $\\alpha$ 的函数的表达式（无单位）。无需四舍五入。",
            "solution": "该问题要求对智能体的内部属性进行两部分的推导，该属性被更新以最小化一个折扣二次损失函数。我们将首先验证问题，然后进行求解。\n\n### 问题验证\n问题陈述给出了以下已知条件：\n- 一个标量观测值流 $\\{x_t\\}_{t \\in \\mathbb{Z}_{\\ge 0}}$。\n- 一个标量内部属性 $m_t$。\n- 一个在时间 $t+1$ 最小化以确定 $m_{t+1}$ 的损失函数：$J_{t+1}(m) \\equiv \\sum_{k=0}^{\\infty} \\delta^{k} \\left(m - x_{t-k}\\right)^{2}$。\n- 一个固定的折扣因子 $\\delta \\in (0,1)$。\n- 有效记忆时域的定义：$H(\\alpha) \\equiv \\sum_{k=0}^{\\infty} k \\, p_k$，其中 $p_k$ 是归一化权重。\n\n该问题具有科学依据，因为它从优化的第一性原理出发，描述了指数加权移动平均（EWMA）的推导过程，而EWMA是时间序列分析和信号处理中的一个基本工具。该问题是适定的；损失函数是关于 $m$ 的严格凸二次函数，这保证了唯一最小值的存在。使用比率为 $\\delta \\in (0,1)$ 的几何级数确保了所有和的收敛性。问题陈述是客观、完整且在数学上自洽的。没有违反科学原理、隐藏的歧义或事实上的不健全之处。因此，该问题被认为是有效的。\n\n### 第1部分：递推更新规则的推导\n\n智能体在时间 $t+1$ 的属性，记为 $m_{t+1}$，被选择来最小化损失函数 $J_{t+1}(m)$：\n$$\nJ_{t+1}(m) = \\sum_{k=0}^{\\infty} \\delta^{k} (m - x_{t-k})^{2}\n$$\n该函数是关于 $m$ 的平方项之和，可以展开为：\n$$\nJ_{t+1}(m) = \\sum_{k=0}^{\\infty} \\delta^{k} (m^2 - 2mx_{t-k} + x_{t-k}^2) = m^2 \\sum_{k=0}^{\\infty} \\delta^k - 2m \\sum_{k=0}^{\\infty} \\delta^k x_{t-k} + \\sum_{k=0}^{\\infty} \\delta^k x_{t-k}^2\n$$\n这是关于 $m$ 的二次函数。由于 $\\delta \\in (0,1)$，几何级数 $\\sum_{k=0}^{\\infty} \\delta^k$ 收敛到 $\\frac{1}{1-\\delta}  0$。$m^2$ 项的系数为正，因此抛物线开口向上，且 $J_{t+1}(m)$ 是严格凸的。其关于 $m$ 的导数等于零处可找到唯一最小值。\n\n我们计算一阶导数：\n$$\n\\frac{dJ_{t+1}}{dm} = \\frac{d}{dm} \\sum_{k=0}^{\\infty} \\delta^{k} (m - x_{t-k})^{2} = \\sum_{k=0}^{\\infty} \\delta^{k} \\cdot 2(m - x_{t-k})\n$$\n将导数设为零以找到最小化子，我们称之为 $m_{t+1}$：\n$$\n\\sum_{k=0}^{\\infty} \\delta^{k} \\cdot 2(m_{t+1} - x_{t-k}) = 0\n$$\n$$\nm_{t+1} \\sum_{k=0}^{\\infty} \\delta^{k} - \\sum_{k=0}^{\\infty} \\delta^{k} x_{t-k} = 0\n$$\n使用几何级数求和公式 $\\sum_{k=0}^{\\infty} \\delta^k = \\frac{1}{1-\\delta}$，我们求解 $m_{t+1}$：\n$$\nm_{t+1} \\left( \\frac{1}{1-\\delta} \\right) = \\sum_{k=0}^{\\infty} \\delta^{k} x_{t-k}\n$$\n$$\nm_{t+1} = (1-\\delta) \\sum_{k=0}^{\\infty} \\delta^{k} x_{t-k}\n$$\n此表达式表明 $m_{t+1}$ 是所有过去和当前观测值 $\\{x_t, x_{t-1}, \\dots\\}$ 的几何加权平均。对观测值 $x_{t-k}$ 的权重是 $(1-\\delta)\\delta^k$。所有权重的总和为 $\\sum_{k=0}^{\\infty} (1-\\delta)\\delta^k = (1-\\delta) \\sum_{k=0}^{\\infty} \\delta^k = (1-\\delta)\\frac{1}{1-\\delta} = 1$。\n\n现在，我们将其转换为单步递推。让我们展开 $m_{t+1}$ 的和：\n$$\nm_{t+1} = (1-\\delta) \\left( \\delta^0 x_t + \\delta^1 x_{t-1} + \\delta^2 x_{t-2} + \\dots \\right)\n$$\n$$\nm_{t+1} = (1-\\delta)x_t + (1-\\delta) \\sum_{k=1}^{\\infty} \\delta^{k} x_{t-k}\n$$\n从求和项中提出因子 $\\delta$：\n$$\nm_{t+1} = (1-\\delta)x_t + \\delta \\left( (1-\\delta) \\sum_{k=1}^{\\infty} \\delta^{k-1} x_{t-k} \\right)\n$$\n让我们定义一个新索引 $j = k-1$。和变为 $\\sum_{j=0}^{\\infty} \\delta^{j} x_{t-1-j}$。\n$$\nm_{t+1} = (1-\\delta)x_t + \\delta \\left( (1-\\delta) \\sum_{j=0}^{\\infty} \\delta^{j} x_{t-1-j} \\right)\n$$\n根据定义，前一个时间步的最小化子 $m_t$ 由下式给出：\n$$\nm_t = (1-\\delta) \\sum_{k=0}^{\\infty} \\delta^{k} x_{t-1-k}\n$$\n将此代入 $m_{t+1}$ 的表达式中：\n$$\nm_{t+1} = (1-\\delta)x_t + \\delta m_t\n$$\n这就是所求的单步递推表示。问题要求将 $m_t$ 上的系数解释为遗忘因子 $\\alpha$。在我们推导的方程中，$m_t$ 上的系数是 $\\delta$。因此，我们确定 $\\alpha = \\delta$。由于 $\\delta \\in (0,1)$，我们有 $\\alpha \\in (0,1)$，符合要求。该递推通常写为 $m_{t+1} = \\alpha m_t + (1-\\alpha) x_t$。\n\n对于 $x_t \\equiv 0$ 对所有 $t$ 成立的齐次情况，递推变为 $m_{t+1} = \\delta m_t$。这是一个一阶线性齐次差分方程。其解为 $m_t = m_0 \\delta^t$。稳定性的条件是对于任何有限的初始条件 $m_0$，解在 $t \\to \\infty$ 时收敛于 $0$。这要求 $|\\delta|  1$。由于问题陈述 $\\delta \\in (0,1)$，此条件得到满足。\n\n### 第2部分：有效记忆时域的计算\n\n有效记忆时域 $H(\\alpha)$ 定义为期望延迟，$H(\\alpha) = \\sum_{k=0}^{\\infty} k \\, p_k$。根据第1部分的推导，分配给观测值 $x_{t-k}$ 的归一化权重 $p_k$ 为 $p_k = (1-\\delta)\\delta^k$。这在非负整数 $k=0, 1, 2, \\dots$上定义了一个几何概率分布。\n\n我们需要计算这个和：\n$$\nH(\\alpha) = \\sum_{k=0}^{\\infty} k \\, p_k = \\sum_{k=0}^{\\infty} k (1-\\delta)\\delta^k = (1-\\delta) \\sum_{k=0}^{\\infty} k \\delta^k\n$$\n为计算总和 $S = \\sum_{k=0}^{\\infty} k \\delta^k$，我们使用一种涉及几何级数公式的标准技巧。对于 $|\\delta|1$：\n$$\n\\sum_{k=0}^{\\infty} \\delta^k = \\frac{1}{1-\\delta}\n$$\n两边对 $\\delta$ 求导：\n$$\n\\frac{d}{d\\delta} \\sum_{k=0}^{\\infty} \\delta^k = \\sum_{k=0}^{\\infty} \\frac{d}{d\\delta} (\\delta^k) = \\sum_{k=1}^{\\infty} k \\delta^{k-1}\n$$\n以及：\n$$\n\\frac{d}{d\\delta} \\left( \\frac{1}{1-\\delta} \\right) = - (1-\\delta)^{-2} (-1) = \\frac{1}{(1-\\delta)^2}\n$$\n因此，我们有：\n$$\n\\sum_{k=1}^{\\infty} k \\delta^{k-1} = \\frac{1}{(1-\\delta)^2}\n$$\n为了找到我们的和 $S = \\sum_{k=0}^{\\infty} k \\delta^k = \\sum_{k=1}^{\\infty} k \\delta^k$，我们将上述结果乘以 $\\delta$：\n$$\n\\delta \\sum_{k=1}^{\\infty} k \\delta^{k-1} = \\sum_{k=1}^{\\infty} k \\delta^k = \\frac{\\delta}{(1-\\delta)^2}\n$$\n所以，$S = \\frac{\\delta}{(1-\\delta)^2}$。\n\n现在我们将其代回 $H(\\alpha)$ 的表达式中：\n$$\nH(\\alpha) = (1-\\delta) S = (1-\\delta) \\frac{\\delta}{(1-\\delta)^2} = \\frac{\\delta}{1-\\delta}\n$$\n最后，我们将此结果表示为 $\\alpha$ 的函数。从第1部分我们发现 $\\alpha = \\delta$。代入这个关系，得到有效记忆时域的最终表达式：\n$$\nH(\\alpha) = \\frac{\\alpha}{1-\\alpha}\n$$\n这个量代表了对智能体当前内部状态 $m_t$ 有贡献的观测值的平均“年龄”。较大的 $\\alpha$（接近于 $1$）意味着更长的记忆时域，因为过去的值被“遗忘”得更慢。较小的 $\\alpha$（接近于 $0$）意味着更短的记忆，因为更多的权重被放在了最新的观测值上。",
            "answer": "$$\\boxed{\\frac{\\alpha}{1-\\alpha}}$$"
        },
        {
            "introduction": "智能体的状态更新规则必须确保其行为是稳定和可预测的。本练习以一个简单的习惯形成模型为例，探讨了适应速率 $\\gamma$ 如何影响状态变量向新均衡收敛的动态过程。通过分析，您将学会确定保证系统稳定且无振荡收敛的参数范围，这是在调整智能体行为以避免不切实际的波动或发散时的一项基本技能。",
            "id": "4120455",
            "problem": "考虑一个复杂自适应系统（CAS）环境中的智能体，其标量习惯属性状态 $h_t \\in \\mathbb{R}$ 按离散时间规则 $h_{t+1} = h_t + \\gamma \\left( x_t - h_t \\right)$ 更新，其中 $x_t \\in \\mathbb{R}$ 是一个外部提供的可观测刺激，$\\gamma \\ge 0$ 是一个可调的适应率。假设 $x_t$ 是有界且分段常数，在时间 $t_0 \\in \\mathbb{Z}_{\\ge 0}$ 发生阶跃变化，使得当 $t  t_0$ 时 $x_t = x^{-}$，当 $t \\ge t_0$ 时 $x_t = x^{+}$，其中 $x^{-}, x^{+} \\in \\mathbb{R}$。将在 $t_0$ 处发生阶跃后的跟踪误差定义为 $e_t := h_t - x_t$，适用于 $t \\ge t_0$。\n\n仅使用离散时间线性动力系统的基本原理，确定最大值 $\\gamma^{\\star}$，使得对于 $x_t$ 的每一次阶跃变化和每一个初始条件 $h_{t_0} \\in \\mathbb{R}$，轨迹 $h_t$ 都能无振荡地收敛到新水平 $x^{+}$。这里的“无振荡”被理解为误差 $e_t$ 在 $t \\ge t_0$ 时不改变符号的性质（等价地，响应是单调趋向于 $x^{+}$ 的）。将单个实数 $\\gamma^{\\star}$ 作为你的最终答案。不需要四舍五入，也不涉及物理单位。",
            "solution": "智能体的状态更新公式为 $h_{t+1} = h_t + \\gamma \\left( x_t - h_t \\right)$，可以改写为\n$$\nh_{t+1} = (1 - \\gamma) h_t + \\gamma x_t.\n$$\n我们分析在时间 $t_0$ 发生阶跃变化后的响应，其中对于所有 $t \\ge t_0$ 都有 $x_t = x^{+}$。对于 $t \\ge t_0$，该系统成为一个带有常数输入 $x^{+}$ 的线性时不变仿射递推：\n$$\nh_{t+1} = (1 - \\gamma) h_t + \\gamma x^{+}.\n$$\n将阶跃后的误差定义为 $e_t := h_t - x^{+}$，适用于 $t \\ge t_0$。将 $h_t = e_t + x^{+}$ 代入更新公式得到\n\\begin{align*}\ne_{t+1} + x^{+} = (1 - \\gamma)(e_t + x^{+}) + \\gamma x^{+}, \\\\\ne_{t+1} = (1 - \\gamma) e_t + (1 - \\gamma) x^{+} + \\gamma x^{+} - x^{+}, \\\\\ne_{t+1} = (1 - \\gamma) e_t.\n\\end{align*}\n因此，误差根据一个常数乘子为 $(1 - \\gamma)$ 的齐次线性递推演化：\n$$\ne_t = (1 - \\gamma)^{t - t_0} e_{t_0}, \\quad t \\ge t_0.\n$$\n跟踪的基本要求是：\n\n- 稳定性（$h_t$ 收敛到 $x^{+}$）：由 $e_t \\to 0$ 推出，这当且仅当 $|1 - \\gamma|  1$ 时发生，即：\n$$\n-1  1 - \\gamma  1 \\quad \\Longleftrightarrow \\quad 0  \\gamma  2.\n$$\n\n- 非振荡性（单调趋向 $x^{+}$）：误差 $e_t$ 在 $t \\ge t_0$ 时不能改变符号。因为 $e_{t+1} = (1 - \\gamma) e_t$，如果 $(1 - \\gamma)  0$，那么 $e_t$ 的符号每一步都会翻转，产生振荡。因此，非振荡性要求\n$$\n1 - \\gamma \\ge 0 \\quad \\Longleftrightarrow \\quad \\gamma \\le 1.\n$$\n结合这两个条件，得到既能收敛又无振荡的 $\\gamma$ 的集合：\n$$\n0  \\gamma \\le 1.\n$$\n在这些值中，保持非振荡跟踪的最大的允许值 $\\gamma$ 是\n$$\n\\gamma^{\\star} = 1.\n$$\n为了验证 $\\gamma = 1$ 是非振荡且稳定的，我们观察到当 $\\gamma = 1$ 时，更新公式简化为 $h_{t+1} = x_t$。因此，在阶跃变化之后，对于 $t_1 = t_0 + 1$，有 $h_{t_1} = x^{+}$，并且对于所有 $t \\ge t_1$，我们有 $h_t = x^{+}$，这意味着此后的 $e_t = 0$ 并且没有符号变化。因此 $\\gamma^{\\star} = 1$ 是满足要求的上确界值。\n\n因此，对于任何阶跃输入和任何初始条件，确保无振荡收敛的上确界适应率是 $\\gamma^{\\star} = 1$。",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "当智能体的状态由多个连续属性描述时，我们面临着如何有效表示这个高维状态空间的挑战。本练习从基本定义出发，让您亲手推导状态空间离散化误差与维度之间的关系，从而揭示“维度灾难”的根源。理解这一原理对于选择合适的模型结构至关重要，它解释了为何简单的网格化方法在高维空间中不可行，并促使我们寻求更高级的表示方法。",
            "id": "4120425",
            "problem": "考虑在复杂自适应系统（CAS）建模中设计智能体属性和状态的任务，其中每个智能体由一个连续属性向量 $\\mathbf{x} \\in [0,1]^{n}$ 和一个标量状态评估函数 $f:[0,1]^{n} \\to \\mathbb{R}$ 来描述。假设函数 $f$ 在 $[0,1]^{n}$ 上关于欧几里得范数是利普希茨连续的，其利普希茨常数为 $L  0$，这意味着对于所有的 $\\mathbf{x},\\mathbf{y} \\in [0,1]^{n}$，不等式 $|f(\\mathbf{x}) - f(\\mathbf{y})| \\leq L \\|\\mathbf{x} - \\mathbf{y}\\|_{2}$ 成立。您使用一个每个维度有 $m$ 个点的均匀轴对齐网格来离散化连续状态空间，并通过最近网格点上的值（最近邻近似）来近似 $f(\\mathbf{x})$。\n\n从定义出发，不借助任何预先推导的离散化误差公式，推导最坏情况下的近似误差如何随维度 $n$ 和网格分辨率 $m$ 变化，并以此从第一性原理展示维度灾难是如何出现的。然后，计算所需的最小总网格大小 $N$（即 $[0,1]^{n}$ 中的网格点总数），以保证对于所有 $\\mathbf{x} \\in [0,1]^{n}$，最坏情况下的近似误差的上限为一个给定的容差 $\\epsilon  0$。\n\n将您的最终答案表示为关于 $n$、$L$ 和 $\\epsilon$ 的单个符号表达式。无需进行数值四舍五入，也不涉及物理单位。",
            "solution": "问题要求推导利普希茨连续函数在均匀网格上的最坏情况近似误差，展示维度灾难，并计算达到给定误差容差 $\\epsilon$ 所需的最小总网格大小 $N$。\n\n设连续属性空间为 $n$ 维超立方体 $\\mathcal{C} = [0,1]^n$。智能体的状态由一个函数 $f: \\mathcal{C} \\to \\mathbb{R}$ 评估，该函数关于欧几里得范数 $\\|\\cdot\\|_2$ 是利普希茨连续的，常数为 $L0$。这意味着对于任意两点 $\\mathbf{x}, \\mathbf{y} \\in \\mathcal{C}$，我们有：\n$$|f(\\mathbf{x}) - f(\\mathbf{y})| \\leq L \\|\\mathbf{x} - \\mathbf{y}\\|_2$$\n连续空间 $\\mathcal{C}$ 通过一个均匀网格 $\\mathcal{G}$ 进行离散化。函数 $f(\\mathbf{x})$ 由 $\\hat{f}(\\mathbf{x}) = f(\\mathbf{g}(\\mathbf{x}))$ 近似，其中 $\\mathbf{g}(\\mathbf{x})$ 是网格 $\\mathcal{G}$ 中在欧几里得范数下离 $\\mathbf{x}$ 最近的网格点。\n在点 $\\mathbf{x}$ 处的近似误差为 $|f(\\mathbf{x}) - \\hat{f}(\\mathbf{x})|$。利用利普希茨性质，我们可以为这个误差定界：\n$$|f(\\mathbf{x}) - \\hat{f}(\\mathbf{x})| = |f(\\mathbf{x}) - f(\\mathbf{g}(\\mathbf{x}))| \\leq L \\|\\mathbf{x} - \\mathbf{g}(\\mathbf{x})\\|_2$$\n最坏情况近似误差 $E_{max}$ 是该误差在所有 $\\mathbf{x} \\in \\mathcal{C}$ 上的上确界。\n$$E_{max} = \\sup_{\\mathbf{x} \\in \\mathcal{C}} |f(\\mathbf{x}) - \\hat{f}(\\mathbf{x})| \\leq L \\left( \\sup_{\\mathbf{x} \\in \\mathcal{C}} \\|\\mathbf{x} - \\mathbf{g}(\\mathbf{x})\\|_2 \\right)$$\n对于一个适当选择的函数 $f$（例如，对于某个 $\\mathbf{x}_0$，有 $f(\\mathbf{x}) = L \\|\\mathbf{x} - \\mathbf{x}_0\\|_2$），该不等式变为等式，因此最坏情况误差恰好是 $L$ 乘以超立方体中任意点到其最近网格点的最大可能距离。\n$$E_{max} = L \\cdot \\max_{\\mathbf{x} \\in \\mathcal{C}} \\left( \\min_{\\mathbf{y} \\in \\mathcal{G}} \\|\\mathbf{x} - \\mathbf{y}\\|_2 \\right)$$\n我们的首要任务是确定这个最大距离。这取决于网格点的布局。问题指定了一个“每个维度有 $m$ 个点的均匀轴对齐网格”。为了最小化到网格点的最大距离，必须使用最优的布局策略。对于一维区间 $[0,1]$，放置 $m$ 个点以最小化到最近点的最大距离，意味着将它们放置在 $m$ 个相等子区间的中心。也就是说，对于每个维度 $i \\in \\{1, \\dots, n\\}$，网格坐标由集合 $\\{ \\frac{2k-1}{2m} \\mid k = 1, 2, \\dots, m \\}$ 给出。\n通过这种布局，区间 $[0,1]$ 被划分为 $m$ 个形式为 $[\\frac{k-1}{m}, \\frac{k}{m}]$ 的单元（边界处稍作修改）。网格点 $\\frac{2k-1}{2m}$ 是第 $k$ 个单元的中心。这个一维单元中任意点到其中心的最大距离是单元宽度的一半，即 $\\frac{1}{2m}$。\n\n这个结构可以扩展到 $n$ 维。网格 $\\mathcal{G}$ 由所有点 $\\mathbf{p} = (p_1, \\dots, p_n)$ 组成，其中每个分量 $p_i$ 都从上面定义的一维网格坐标集合中选取。这个网格将超立方体 $[0,1]^n$ 划分为 $m^n$ 个边长为 $\\frac{1}{m}$ 的较小超立方体（单元），每个单元的中心有一个网格点。\n在 $[0,1]^n$ 中，与其最近网格点距离最远的点将是这些单元的顶点之一。让我们考虑单元 $[0, \\frac{1}{m}]^n$。它的中心（即最近的网格点）是 $(\\frac{1}{2m}, \\frac{1}{2m}, \\dots, \\frac{1}{2m})$。该单元的一个顶点是原点 $\\mathbf{0}=(0, 0, \\dots, 0)$。这个顶点与单元中心之间的欧几里得距离是：\n$$d_{max} = \\left\\| \\left(\\frac{1}{2m}, \\dots, \\frac{1}{2m}\\right) - (0, \\dots, 0) \\right\\|_2 = \\sqrt{\\sum_{i=1}^n \\left(\\frac{1}{2m}\\right)^2} = \\sqrt{n \\cdot \\frac{1}{4m^2}} = \\frac{\\sqrt{n}}{2m}$$\n这就是 $[0,1]^n$ 中任意点到最近网格点的最大距离。\n\n现在，我们可以用维度 $n$ 和网格分辨率 $m$ 来表示最坏情况近似误差 $E_{max}$：\n$$E_{max} = L \\cdot d_{max} = \\frac{L\\sqrt{n}}{2m}$$\n这个方程揭示了最坏情况误差如何变化。对于每个维度固定的网格点数 $m$，误差随着维度的平方根 $\\sqrt{n}$ 增长。\n\n这直接导致了维度灾难。网格中的总点数是 $N = m^n$。我们可以将 $m$ 用 $N$ 和 $n$ 表示为 $m = N^{1/n}$。将此代入误差公式：\n$$E_{max} = \\frac{L\\sqrt{n}}{2N^{1/n}}$$\n为了在维度 $n$ 增加时保持恒定的误差水平 $E_{max} = C$，总网格点数 $N$ 必须急剧增长。对 $N$ 进行整理，我们得到 $N^{1/n} = \\frac{L\\sqrt{n}}{2C}$，这意味着 $N = \\left(\\frac{L\\sqrt{n}}{2C}\\right)^n$。$N$ 对 $n$ 的指数依赖关系表明，将空间离散化到固定精度所需的点数随维度爆炸性增长，这使得基于均匀网格的方法在高维空间中计算上不可行。这是维度灾难的一个经典表现。\n\n最后，我们计算确保最坏情况误差以容差 $\\epsilon  0$ 为界的最小总网格大小 $N$。我们设定条件：\n$$E_{max} \\leq \\epsilon$$\n$$\\frac{L\\sqrt{n}}{2m} \\leq \\epsilon$$\n我们必须找到满足此不等式的最小整数 $m$。解出 $m$：\n$$2m\\epsilon \\ge L\\sqrt{n}$$\n$$m \\ge \\frac{L\\sqrt{n}}{2\\epsilon}$$\n由于 $m$ 必须是整数（它是每个维度的点数），所需的最小值是大于或等于此下界的最小整数。这由向上取整函数（ceiling function）给出：\n$$m_{min} = \\left\\lceil \\frac{L\\sqrt{n}}{2\\epsilon} \\right\\rceil$$\n网格点总数为 $N = m^n$。因此，保证误差容差所需的最小总网格大小 $N_{min}$ 是：\n$$N_{min} = (m_{min})^n = \\left(\\left\\lceil \\frac{L\\sqrt{n}}{2\\epsilon} \\right\\rceil\\right)^n$$\n这个表达式给出了作为维度 $n$、利普希茨常数 $L$ 和所需误差容差 $\\epsilon$ 的函数的最小网格点数。",
            "answer": "$$\\boxed{\\left(\\left\\lceil \\frac{L\\sqrt{n}}{2\\epsilon} \\right\\rceil\\right)^{n}}$$"
        }
    ]
}