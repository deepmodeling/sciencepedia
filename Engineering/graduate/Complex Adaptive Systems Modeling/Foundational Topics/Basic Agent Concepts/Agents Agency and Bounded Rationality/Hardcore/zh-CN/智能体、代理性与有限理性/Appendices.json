{
    "hands_on_practices": [
        {
            "introduction": "诺贝尔奖得主 Herbert Simon 提出的“满意化”（satisficing）是有限理性的基石，它挑战了传统经济学中完全理性的“最优化”假设。这个练习  将此概念具体化，要求你为一个在搜索成本和期望回报之间权衡的满意化智能体推导出其最优期望水平。通过这个实践，你将亲手计算在有限认知资源下，如何明智地设定“足够好”的标准，从而深刻理解有限理性决策中的核心权衡。",
            "id": "4114236",
            "problem": "考虑一个在由异构选项构成的复杂自适应环境中的序贯搜索过程。一个具有有限理性的代理人采用满意原则：她设定一个抱负阈值 $\\alpha$，并接受第一个遇到的价值 $X$ 超过 $\\alpha$ 的选项。每个选项都是独立同分布的（i.i.d.），其分布为 $X \\sim \\text{Exponential}(\\lambda)$，其中率参数 $\\lambda0$ 对代理人是已知的。对每个选项的评估，包括被接受的那个，都会产生固定的搜索成本 $c0$。代理人的目标是最大化其期望净收益，该收益定义为被接受选项的期望值减去预期的累积搜索成本。假设代理人评估选项，直到有一个满足或超过抱负水平 $\\alpha$，然后停止。\n\n从概率和期望值的基本原理出发，针对独立同分布抽样和指数分布，推导期望净收益作为 $\\alpha$ 的函数表达式，并计算使期望净收益最大化的最优抱负水平 $\\alpha^{\\ast}$。假设 $0  \\lambda c  1$，以确保存在内部最优解。请用封闭形式的解析表达式给出最终答案。不需要四舍五入。",
            "solution": "该问题要求推导一个满意型代理人的期望净收益，并求出使该收益最大化的最优抱负水平 $\\alpha^{\\ast}$。该问题定义明确，并在最优停止和搜索理论中有其科学依据。\n\n首先，我们定义代理人期望净收益的组成部分，我们将其表示为 $J(\\alpha)$。收益是被接受选项的价值减去累积搜索成本。\n$$J(\\alpha) = E[V(\\alpha)] - E[C(\\alpha)]$$\n其中 $V(\\alpha)$ 是当抱负水平为 $\\alpha$ 时被接受选项的价值，而 $C(\\alpha)$ 是总搜索成本。\n\n代理人检查的选项价值 $X$ 是从率参数为 $\\lambda  0$ 的指数分布中独立抽取的。其概率密度函数（PDF）为 $f(x) = \\lambda \\exp(-\\lambda x)$（当 $x \\ge 0$），累积分布函数（CDF）为 $F(x) = 1 - \\exp(-\\lambda x)$（当 $x \\ge 0$）。\n\n如果一个选项的价值 $X$ 达到或超过抱负水平 $\\alpha$，代理人就会停止并接受该选项。问题文本在“超过 $\\alpha$”和“达到或超过 $\\alpha$”之间存在轻微的歧义。然而，对于像指数分布这样的连续分布，获得一个恰好等于 $\\alpha$ 的值的概率为零，即 $P(X = \\alpha) = 0$。因此，$P(X  \\alpha) = P(X \\ge \\alpha)$，这个歧义无关紧要。我们接下来使用条件 $X \\ge \\alpha$。\n\n设 $p$ 为单次搜索成功的概率，即一个选项的价值达到或超过 $\\alpha$ 的概率。\n$$p = P(X \\ge \\alpha) = 1 - P(X  \\alpha) = 1 - F(\\alpha) = 1 - (1 - \\exp(-\\lambda \\alpha)) = \\exp(-\\lambda \\alpha)$$\n或者，我们可以通过积分来计算：\n$$p = \\int_{\\alpha}^{\\infty} \\lambda \\exp(-\\lambda x) dx = \\left[ -\\exp(-\\lambda x) \\right]_{\\alpha}^{\\infty} = 0 - (-\\exp(-\\lambda \\alpha)) = \\exp(-\\lambda \\alpha)$$\n\n搜索次数，我们称之为 $N$，服从成功概率为 $p$ 的几何分布。代理人搜索直到第一次成功。期望搜索次数为：\n$$E[N] = \\frac{1}{p} = \\frac{1}{\\exp(-\\lambda \\alpha)} = \\exp(\\lambda \\alpha)$$\n\n每次搜索都会产生一个成本 $c  0$。预期的总搜索成本 $E[C(\\alpha)]$ 是每次搜索的成本乘以预期的搜索次数：\n$$E[C(\\alpha)] = c \\cdot E[N] = c \\exp(\\lambda \\alpha)$$\n\n接下来，我们确定被接受选项的期望值 $E[V(\\alpha)]$。这是在 $X \\ge \\alpha$ 的条件下 $X$ 的条件期望。\n$$E[V(\\alpha)] = E[X | X \\ge \\alpha] = \\frac{\\int_{\\alpha}^{\\infty} x f(x) dx}{P(X \\ge \\alpha)}$$\n我们已经计算了分母 $P(X \\ge \\alpha) = \\exp(-\\lambda \\alpha)$。现在我们使用分部积分法计算分子：$\\int u dv = uv - \\int v du$。令 $u = x$ 和 $dv = \\lambda \\exp(-\\lambda x) dx$。则 $du = dx$ 且 $v = -\\exp(-\\lambda x)$。\n\\begin{align*} \\int_{\\alpha}^{\\infty} x \\lambda \\exp(-\\lambda x) dx = \\left[ -x \\exp(-\\lambda x) \\right]_{\\alpha}^{\\infty} - \\int_{\\alpha}^{\\infty} (-\\exp(-\\lambda x)) dx \\\\ = \\left( \\lim_{x \\to \\infty} -x \\exp(-\\lambda x) - (-\\alpha \\exp(-\\lambda \\alpha)) \\right) + \\int_{\\alpha}^{\\infty} \\exp(-\\lambda x) dx \\\\ = (0 + \\alpha \\exp(-\\lambda \\alpha)) + \\left[ -\\frac{1}{\\lambda} \\exp(-\\lambda x) \\right]_{\\alpha}^{\\infty} \\\\ = \\alpha \\exp(-\\lambda \\alpha) + \\left( 0 - (-\\frac{1}{\\lambda} \\exp(-\\lambda \\alpha)) \\right) \\\\ = \\alpha \\exp(-\\lambda \\alpha) + \\frac{1}{\\lambda} \\exp(-\\lambda \\alpha) \\\\ = \\left(\\alpha + \\frac{1}{\\lambda}\\right) \\exp(-\\lambda \\alpha) \\end{align*}\n现在，我们可以求出条件期望：\n$$E[V(\\alpha)] = \\frac{\\left(\\alpha + \\frac{1}{\\lambda}\\right) \\exp(-\\lambda \\alpha)}{\\exp(-\\lambda \\alpha)} = \\alpha + \\frac{1}{\\lambda}$$\n这个结果巧妙地反映了指数分布的无记忆性。\n\n综合各组成部分，期望净收益函数 $J(\\alpha)$ 为：\n$$J(\\alpha) = E[V(\\alpha)] - E[C(\\alpha)] = \\left(\\alpha + \\frac{1}{\\lambda}\\right) - c \\exp(\\lambda \\alpha)$$\n为了找到使 $J(\\alpha)$ 最大化的最优抱负水平 $\\alpha^{\\ast}$，我们计算其关于 $\\alpha$ 的一阶导数并令其为零。\n$$\\frac{dJ}{d\\alpha} = \\frac{d}{d\\alpha} \\left(\\alpha + \\frac{1}{\\lambda} - c \\exp(\\lambda \\alpha)\\right) = 1 - c \\lambda \\exp(\\lambda \\alpha)$$\n令导数为零：\n$$1 - c \\lambda \\exp(\\lambda \\alpha^{\\ast}) = 0$$\n$$c \\lambda \\exp(\\lambda \\alpha^{\\ast}) = 1$$\n$$\\exp(\\lambda \\alpha^{\\ast}) = \\frac{1}{c \\lambda}$$\n对两边取自然对数：\n$$\\lambda \\alpha^{\\ast} = \\ln\\left(\\frac{1}{c \\lambda}\\right) = -\\ln(c \\lambda)$$\n解出 $\\alpha^{\\ast}$：\n$$\\alpha^{\\ast} = -\\frac{1}{\\lambda}\\ln(c \\lambda)$$\n问题给出了约束条件 $0  \\lambda c  1$。这确保了 $\\ln(\\lambda c)$ 为负，从而保证最优抱负水平 $\\alpha^{\\ast}$ 为正。\n\n为了确认这个临界点对应于一个最大值，我们考察 $J(\\alpha)$ 的二阶导数：\n$$\\frac{d^2J}{d\\alpha^2} = \\frac{d}{d\\alpha} \\left(1 - c \\lambda \\exp(\\lambda \\alpha)\\right) = -c \\lambda^2 \\exp(\\lambda \\alpha)$$\n鉴于 $c  0$ 且 $\\lambda  0$，指数项 $\\exp(\\lambda \\alpha)$ 总是正的。因此，$\\frac{d^2J}{d\\alpha^2}$ 总是负的。这证实了收益函数 $J(\\alpha)$ 是严格凹的，我们的解 $\\alpha^{\\ast}$ 确实是唯一的全局最大值。",
            "answer": "$$ \\boxed{-\\frac{1}{\\lambda} \\ln(\\lambda c)} $$"
        },
        {
            "introduction": "在简单的决策规则之上，我们进一步探索更丰富的智能体模型，这些智能体的行为不仅仅由外部奖励驱动。本练习  引入了“赋能”（empowerment）这一概念，它是一种基于信息论的、衡量智能体对其未来控制能力的内在动机。你将通过编程模拟一个当这种对控制的内在追求与更高额的即时回报发生冲突时智能体的选择，从而洞察目标导向行为的复杂性。",
            "id": "4114153",
            "problem": "构建一个确定性的、有限状态的、离散时间的决策环境，用以分析在有限理性条件下，作为内在驱动力的赋能（empowerment）与外在奖励之间的相互作用。请从以下基本概念出发：(i) 互信息（Mutual Information, MI）的定义，以及将赋能概念化为从动作序列到未来状态的信道容量；(ii) 用于有限理性的玻尔兹曼决策规则，该规则产生与指数化评估值成比例的随机选择。\n\n环境规范：设有一个起始状态 $S_0$，在该状态下有两个动作 $L$ 和 $R$。动作 $L$ 确定性地转移到终止状态 $T_L$；动作 $R$ 确定性地转移到中间状态 $R_1$。在 $R_1$ 状态，有且仅有两个不同的动作 $A$ 和 $B$，它们确定性地导致两个不同的终止状态 $R_{2a}$ 和 $R_{2b}$。在每个终止状态（包括 $T_L$），单一的“等待”动作确定性地返回自身，表示一个没有进一步控制的吸收态。其转移图如下：\n- $S_0 \\xrightarrow{L} T_L$, $S_0 \\xrightarrow{R} R_1$,\n- $T_L \\xrightarrow{W} T_L$,\n- $R_1 \\xrightarrow{A} R_{2a}$, $R_1 \\xrightarrow{B} R_{2b}$,\n- $R_{2a} \\xrightarrow{W} R_{2a}$, $R_{2b} \\xrightarrow{W} R_{2b}$。\n外在奖励是即时的，并且仅在 $S_0$ 处分配：$r(S_0,L)=10$ 和 $r(S_0,R)=1$，所有其他即时奖励均为 $0$。所有对数都必须是自然对数。\n\n赋能定义：对于一个状态 $s$ 和一个单步动作视界，将赋能 $E_1(s)$ 定义为从当前动作到下一状态的确定性映射的信道容量。使用互信息的标准定义 $I(X;Y)=H(Y)-H(Y|X)$，其中 $H(\\cdot)$ 是香农熵，并且在确定性转移下 $H(Y|X)=0$，因此赋能简化为由动作引起的下一状态分布的最大熵。对于一个可用的有限动作集合，其确定性结果为 $k$ 个不同的下一状态，最大值是通过映射到不同结果的动作上的均匀分布来实现的，得到 $E_1(s)=\\log k$。在吸收终止状态中，只有一个可用动作使其返回自身，此时设 $k=1$，因此 $E_1(s)=\\log 1 = 0$。\n\n有限理性与评估：处于 $S_0$ 状态的智能体通过将外在奖励与后续状态的赋能进行组合来评估每个动作，组合时使用一个权衡权重 $\\alpha \\in [0,1]$，该权重代表智能体的内在动机权重。有限理性通过带有温度 $\\tau0$ 的玻尔兹曼选择规则来建模：评估值越高的动作被选择的概率也越高，但除非 $\\tau \\to 0$，否则选择不是确定性的。您的算法必须从上述基础出发，为 $S_0$ 处的每个动作推导出一个标量评估值，该评估值遵循将 $S_0$ 处的即时外在奖励与通过该动作达到的下一状态的赋能 $E_1(\\cdot)$ 相结合的原则，然后通过玻尔兹曼规则将这些评估值转换为选择概率。\n\n任务：实现一个完整的程序，该程序：\n1. 根据上述定义编码环境并计算 $E_1(T_L)$ 和 $E_1(R_1)$。\n2. 对于每个测试用例 $(\\alpha,\\tau)$，基于推导出的评估值和温度 $\\tau$，计算在 $S_0$ 状态选择动作 $R$ 相对于 $L$ 的玻尔兹曼选择概率。\n3. 按规定将结果聚合到单行输出中。\n\n测试套件：\n- 情况1 (常规路径): $\\alpha=0.3$, $\\tau=0.5$。\n- 情况2 (纯外在奖励边界): $\\alpha=0$, $\\tau=0.1$。\n- 情况3 (纯内在动机边界): $\\alpha=1$, $\\tau=0.1$。\n- 情况4 (决策无差异边界): $\\alpha^\\star = \\dfrac{9}{9+\\log 2}$, $\\tau=0.2$。\n- 情况5 (高噪声边缘情况): $\\alpha=0.5$, $\\tau=10$。\n\n输出规范：您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表中的每个条目是对应测试用例（按上述顺序列出）在 $S_0$ 状态选择动作 $R$ 的概率（浮点数）（例如，$\\texttt{[p_1,p_2,p_3,p_4,p_5]}$）。不应打印其他任何文本。没有物理单位，也不涉及角度。百分比必须表示为小数，概率输出已隐含了这一点。",
            "solution": "该问题被判定为有效，因为它在科学上基于信息论和决策论的既定原则，在数学上是适定、客观且自洽的。唯一解所需的所有定义、约束和数据均已提供。我们按步骤进行推导。\n\n**步骤 1：环境分析与赋能计算**\n\n问题定义了一个确定性的有限状态环境。我们首先计算相关非初始状态 $T_L$ 和 $R_1$ 的单步赋能 $E_1(s)$。赋能被定义为从动作到下一状态的信道容量，对于一个具有 $k$ 个不同结果的确定性系统，这简化为 $E_1(s) = \\log k$，其中 $\\log$ 表示自然对数。\n\n1.  **状态 $T_L$**：从状态 $T_L$ 出发，只有一个可用动作 $W$，它确定性地转移回 $T_L$。只有一个可能的结果状态。因此，不同结果的数量为 $k=1$。在 $T_L$ 的赋能是：\n    $$ E_1(T_L) = \\log(1) = 0 $$\n\n2.  **状态 $R_1$**：从状态 $R_1$ 出发，有两个不同的可用动作 $A$ 和 $B$。动作 $A$ 导致终止状态 $R_{2a}$，动作 $B$ 导致终止状态 $R_{2b}$。由于 $R_{2a}$ 和 $R_{2b}$ 是不同的，所以有两个不同的结果状态。因此，不同结果的数量为 $k=2$。在 $R_1$ 的赋能是：\n    $$ E_1(R_1) = \\log(2) $$\n\n**步骤 2：动作价值函数的推导**\n\n处于起始状态 $S_0$ 的智能体基于即时外在奖励 $r(S_0, a)$ 和由结果状态 $s'$ 的赋能 $E_1(s')$ 派生出的内在价值的加权组合来评估动作 $L$ 和 $R$。评估函数（或动作价值函数）$Q(S_0, a)$ 由以下凸组合给出：\n\n$$ Q(S_0, a; \\alpha) = (1-\\alpha) \\cdot r(S_0, a) + \\alpha \\cdot E_1(s'_{S_0,a}) $$\n\n这里，$\\alpha \\in [0, 1]$ 是内在动机的权重，而 $s'_{S_0,a}$ 是在状态 $S_0$ 执行动作 $a$ 后到达的状态。\n\n1.  **动作 $L$ 的评估**：从 $S_0$ 执行动作 $L$ 会到达状态 $T_L$，并提供 $r(S_0, L) = 10$ 的即时奖励。其评估值为：\n    $$ Q(S_0, L; \\alpha) = (1-\\alpha) \\cdot r(S_0, L) + \\alpha \\cdot E_1(T_L) $$\n    $$ Q(S_0, L; \\alpha) = (1-\\alpha) \\cdot 10 + \\alpha \\cdot 0 $$\n    $$ Q(S_0, L; \\alpha) = 10(1-\\alpha) $$\n\n2.  **动作 $R$ 的评估**：从 $S_0$ 执行动作 $R$ 会到达状态 $R_1$，并提供 $r(S_0, R) = 1$ 的即时奖励。其评估值为：\n    $$ Q(S_0, R; \\alpha) = (1-\\alpha) \\cdot r(S_0, R) + \\alpha \\cdot E_1(R_1) $$\n    $$ Q(S_0, R; \\alpha) = (1-\\alpha) \\cdot 1 + \\alpha \\cdot \\log(2) $$\n\n**步骤 3：玻尔兹曼选择概率的推导**\n\n有限理性使用玻尔兹曼决策规则建模，其中选择一个动作的概率与其指数化评估值成正比，并由温度参数 $\\tau  0$ 进行缩放。在 $S_0$ 选择动作 $R$ 的概率为：\n\n$$ P(R|S_0; \\alpha, \\tau) = \\frac{\\exp(Q(S_0, R; \\alpha) / \\tau)}{\\exp(Q(S_0, L; \\alpha) / \\tau) + \\exp(Q(S_0, R; \\alpha) / \\tau)} $$\n\n通过将分子和分母同除以 $\\exp(Q(S_0, R; \\alpha) / \\tau)$，这可以通过使用逻辑S型函数（logistic sigmoid function）更紧凑地表示：\n\n$$ P(R|S_0; \\alpha, \\tau) = \\frac{1}{\\frac{\\exp(Q(S_0, L; \\alpha) / \\tau)}{\\exp(Q(S_0, R; \\alpha) / \\tau)} + 1} = \\frac{1}{1 + \\exp\\left(\\frac{Q(S_0, L; \\alpha) - Q(S_0, R; \\alpha)}{\\tau}\\right)} $$\n\n我们计算评估值的差异，$\\Delta Q(\\alpha) = Q(S_0, L; \\alpha) - Q(S_0, R; \\alpha)$：\n\n$$ \\Delta Q(\\alpha) = 10(1-\\alpha) - \\left[ (1-\\alpha) + \\alpha \\log(2) \\right] $$\n$$ \\Delta Q(\\alpha) = 10 - 10\\alpha - 1 + \\alpha - \\alpha \\log(2) $$\n$$ \\Delta Q(\\alpha) = 9 - 9\\alpha - \\alpha \\log(2) $$\n$$ \\Delta Q(\\alpha) = 9 - \\alpha(9 + \\log(2)) $$\n\n将其代回概率公式，得到选择动作 $R$ 的最终表达式：\n\n$$ P(R|S_0; \\alpha, \\tau) = \\frac{1}{1 + \\exp\\left(\\frac{9 - \\alpha(9 + \\log(2))}{\\tau}\\right)} $$\n\n此公式将用于计算给定测试用例的结果。作为检验，我们注意到当指数为零时，会出现决策无差异（$P(R)=0.5$），这意味着 $\\Delta Q(\\alpha) = 0$。求解 $9 - \\alpha(9 + \\log(2)) = 0$ 得到 $\\alpha = \\frac{9}{9 + \\log(2)}$，这与测试用例中给出的值 $\\alpha^\\star$ 相匹配，从而证实了我们推导模型的正确性。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the Boltzmann choice probability for action 'R' at state S0\n    for a set of test cases involving a trade-off between extrinsic reward\n    and intrinsic empowerment.\n    \"\"\"\n    # Pre-calculate log(2) as it's used repeatedly.\n    # The problem specifies natural logarithms.\n    log_2 = np.log(2)\n\n    # The special value of alpha for the indifference case, derived from\n    # setting the evaluations Q(L) and Q(R) to be equal.\n    # alpha_star = 9 / (9 + log(2))\n    alpha_star = 9.0 / (9.0 + log_2)\n\n    # Define the test cases from the problem statement as (alpha, tau).\n    test_cases = [\n        (0.3, 0.5),                # Case 1 (happy path)\n        (0.0, 0.1),                # Case 2 (extrinsic-only boundary)\n        (1.0, 0.1),                # Case 3 (intrinsic-only boundary)\n        (alpha_star, 0.2),         # Case 4 (decision indifference boundary)\n        (0.5, 10.0),               # Case 5 (high-noise edge)\n    ]\n\n    results = []\n    # Loop through each test case to compute the probability.\n    for alpha, tau in test_cases:\n        # Calculate the difference in evaluations, Delta Q = Q(L) - Q(R).\n        # Based on the derived formula:\n        # Delta Q = 9 - alpha * (9 + log(2))\n        delta_q = 9.0 - alpha * (9.0 + log_2)\n\n        # Calculate the probability of choosing action R using the logistic function form\n        # of the Boltzmann (softmax) rule for two actions.\n        # P(R) = 1 / (1 + exp((Q(L) - Q(R)) / tau))\n        # This form is numerically stable, especially for large exponents.\n        prob_r = 1.0 / (1.0 + np.exp(delta_q / tau))\n        \n        results.append(prob_r)\n\n    # Format the output as a comma-separated list of floats inside square brackets.\n    # The map(str, ...) converts each float result to its string representation.\n    output_str = f\"[{','.join(map(str, results))}]\"\n    \n    # Print the final result in the exact required format.\n    print(output_str)\n\n# Execute the main function.\nsolve()\n```"
        },
        {
            "introduction": "最后的练习将这些概念扩展到一个更真实但也更具挑战性的场景：在部分可观测的世界中进行规划。在这种环境下，寻找真正的最优策略往往因计算量过大而不可行，这正是有限理性的体现。此练习  让你通过实现和评估一种“基于点的价值迭代”（Point-Based Value Iteration, PBVI）算法，来直面这一挑战。通过比较近似解与精确解的差异，你将量化计算资源限制所带来的代价，并体会到有原则的近似方法 (principled approximation) 在构建高效能智能体中的关键作用。",
            "id": "4114230",
            "problem": "考虑一个有限时域部分可观马尔可夫决策过程 (POMDP)，其定义如下。该 POMDP 包含一个有限状态集 $S=\\{s_0,s_1,s_2\\}$（其中 $|S|=3$），一个有限动作集 $A=\\{a_0,a_1\\}$（其中 $|A|=2$），以及一个有限观测集 $O=\\{o_0,o_1\\}$（其中 $|O|=2$）。决策者维持一个信念 $b \\in \\Delta(S)$，其中 $\\Delta(S)$ 表示在 $S$ 上的概率单纯形，并通过行动来最大化部分可观性下的折扣累积奖励。折扣因子为 $\\gamma \\in (0,1)$。\n\n系统动态由受控转移概率 $P(s' \\mid s,a)$ 和观测概率 $P(o \\mid s',a)$ 指定。在此问题中，每个动作 $a \\in A$ 的转移模型由 $3 \\times 3$ 的行随机矩阵 $T_a$ 表示，其条目为 $[T_a]_{s,s'} = P(s' \\mid s,a)$。观测模型由向量 $z_{o}(s') = P(o \\mid s',a)$ 表示，在此问题中，该模型与动作 $a$ 无关。即时奖励函数为 $R(s,a)$，表示为向量 $r_a \\in \\mathbb{R}^{3}$，其中 $[r_a]_s = R(s,a)$。折扣因子为 $\\gamma=0.95$。\n\n具体的 POMDP 如下：\n1. 动作 $a_0$ 的转移：\n$$\nT_{a_0}=\n\\begin{bmatrix}\n0.7  0.2  0.1 \\\\\n0.2  0.6  0.2 \\\\\n0.1  0.2  0.7\n\\end{bmatrix}.\n$$\n2. 动作 $a_1$ 的转移：\n$$\nT_{a_1}=\n\\begin{bmatrix}\n0.2  0.6  0.2 \\\\\n0.3  0.5  0.2 \\\\\n0.2  0.5  0.3\n\\end{bmatrix}.\n$$\n3. 观测（与动作无关）：\n$$\nz_{o_0}=\n\\begin{bmatrix}\n0.8\\\\\n0.3\\\\\n0.4\n\\end{bmatrix},\\quad\nz_{o_1}=\n\\begin{bmatrix}\n0.2\\\\\n0.7\\\\\n0.6\n\\end{bmatrix},\n$$\n因此对于每个 $s' \\in S$，都有 $z_{o_0}(s')+z_{o_1}(s')=1$。\n4. 奖励：\n$$\nr_{a_0}=\n\\begin{bmatrix}\n0.0\\\\\n1.0\\\\\n-0.4\n\\end{bmatrix},\\quad\nr_{a_1}=\n\\begin{bmatrix}\n-0.2\\\\\n1.1\\\\\n-0.5\n\\end{bmatrix}.\n$$\n\n基本定义：\n- 在动作 $a$ 和观测 $o$ 下的信念更新遵循 Bayes 法则。将信念 $b$ 和动作 $a$ 的下一状态的预测分布定义为 $\\tilde{b}(s')=\\sum_{s \\in S} P(s' \\mid s,a) b(s)$，则在观测到 $o$ 之后的下一个信念 $b'$ 是\n$$\nb'(s')=\\frac{z_{o}(s')\\,\\tilde{b}(s')}{\\sum_{\\bar{s}' \\in S} z_{o}(\\bar{s}')\\,\\tilde{b}(\\bar{s}')}=\\eta\\,z_{o}(s')\\sum_{s \\in S}P(s' \\mid s,a)b(s),\n$$\n其中 $\\eta$ 是确保 $\\sum_{s'} b'(s')=1$ 的归一化常数。\n- 信念上的有限时域最优价值函数是分段线性和凸的，可表示为\n$$\nV_H(b)=\\max_{\\alpha \\in \\mathcal{V}_H} \\langle b,\\alpha \\rangle,\n$$\n其中 $\\mathcal{V}_H$ 是 $\\mathbb{R}^{3}$ 中的一个向量集，$\\langle \\cdot,\\cdot \\rangle$ 表示点积。从时域 $h-1$ 到 $h$ 的动态规划备份通过以下方式生成向量\n$$\n\\alpha(s)=r_a(s)+\\gamma \\sum_{s' \\in S} P(s' \\mid s,a)\\left(\\sum_{o \\in O} z_o(s')\\,\\alpha_o(s')\\right),\n$$\n对于每个动作 $a \\in A$ 和来自 $\\mathcal{V}_{h-1}$ 的每个选择 $\\{\\alpha_o\\}_{o \\in O}$。\n\n近似方法和有限理性：\n- 基于点的价值迭代 (Point-Based Value Iteration, PBVI) 通过将注意力限制在一个有界信念点集 $\\mathcal{B}$（其中 $|\\mathcal{B}| \\leq B$）上来近似备份过程，这反映了有限的计算资源。对于每个信念 $b \\in \\mathcal{B}$，PBVI 为每个观测 $o \\in O$ 选择能够最大化 $\\langle b_{a,o},\\alpha \\rangle$ 的向量 $\\alpha_o \\in \\mathcal{V}_{h-1}$，其中 $b_{a,o}$ 是从 $b$ 出发，在动作 $a$ 和观测 $o$ 下更新后的信念。然后，它使用相同的备份方程构建候选向量 $\\alpha$，并选择能够最大化 $\\langle b,\\alpha \\rangle$ 的动作 $a$。对每个 $b \\in \\mathcal{B}$ 重复此过程即可得到 $\\mathcal{V}_h$。\n\n评估网格与误差量化：\n- 定义一个评估信念网格 $\\mathcal{G}$，包含以下信念：\n$$\n\\begin{aligned}\n[1,0,0]^\\top,\\,[0,1,0]^\\top,\\,[0,0,1]^\\top,\\\\\n[0.5,0.5,0]^\\top,\\,[0.5,0,0.5]^\\top,\\,[0,0.5,0.5]^\\top,\\\\\n[0.5,0.25,0.25]^\\top,\\,[0.25,0.5,0.25]^\\top,\\,[0.25,0.25,0.5]^\\top,\\\\\n\\left[\\frac{1}{3},\\frac{1}{3},\\frac{1}{3}\\right]^\\top.\n\\end{aligned}\n$$\n- 对于有限时域 $H$，令 $\\mathcal{V}_H^{\\mathrm{exact}}$ 表示在执行完整交叉和备份后，通过剪枝仅保留那些在 $\\mathcal{G}$ 中至少一个信念上是最大的向量而得到的精确集。令 $\\mathcal{V}_H^{\\mathrm{PBVI}}$ 表示在使用信念集 $\\mathcal{B}$（其中 $|\\mathcal{B}|=B$）执行 PBVI 后得到的近似集，该集合通过确定性地从 $\\mathcal{G}$ 中选取前 $B$ 个信念来选择（如果 $B|\\mathcal{G}|$，则使用固定的种子 $0$ 从 Dirichlet 分布中采样填充剩余部分）。\n- 将 $\\mathcal{G}$ 上的最大绝对近似误差定义为\n$$\nE_{\\max}(B,H)=\\max_{b \\in \\mathcal{G}} \\left| \\max_{\\alpha \\in \\mathcal{V}_H^{\\mathrm{exact}}} \\langle b,\\alpha \\rangle - \\max_{\\beta \\in \\mathcal{V}_H^{\\mathrm{PBVI}}} \\langle b,\\beta \\rangle \\right|.\n$$\n\n任务：\n- 实现基于 $\\mathcal{G}$ 剪枝的精确有限时域备份，以及在信念点数量限制 $B$ 下的 PBVI 近似算法，如上文所定义。\n- 对每个测试用例，计算 $E_{\\max}(B,H)$ 并将其作为浮点数报告。\n\n测试套件：\n- 用例 1：$B=3, H=2$。\n- 用例 2：$B=2, H=3$。\n- 用例 3：$B=6, H=3$。\n- 用例 4：$B=1, H=3$。\n- 用例 5：$B=6, H=1$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果，并按测试套件的顺序排列，例如 $[x_1,x_2,x_3,x_4,x_5]$，其中每个 $x_i$ 是相应情况下计算出的 $E_{\\max}(B,H)$。不应打印额外的文本。不涉及物理单位，也不使用角度。所有量都应被视为实数，不带百分号。",
            "solution": "该问题要求实现并比较两种用于有限时域部分可观马尔可夫决策过程 (POMDP) 的价值迭代算法。问题的核心在于量化一种有限理性方法，即基于点的价值迭代 (Point-Based Value Iteration, PBVI)，相对于计算量更大的精确价值迭代方法所引入的近似误差。\n\n一个 POMDP 由元组 $(S, A, O, T, Z, R, \\gamma)$ 定义，其中 $S$ 是状态集， $A$ 是动作集， $O$ 是观测集， $T$ 是状态转移函数 $P(s'|s,a)$， $Z$ 是观测函数 $P(o|s',a)$， $R$ 是奖励函数 $R(s,a)$，而 $\\gamma$ 是折扣因子。由于智能体不能直接观测状态 $s$，它会维持一个信念状态 $b$，这是 $S$ 上的一个概率分布。在 POMDP 中寻找最优策略的问题可以被重新表述为在一个连续状态马尔可夫决策过程中寻找最优策略，其中状态是信念。\n\n对于一个有限时域 $H$，时域为 $h$ 的最优价值函数 $V_h(b)$ 在信念空间上是分段线性和凸的 (PWLC)。该函数可以表示为一组有限超平面（在此上下文中，称为 alpha-向量的 $\\mathbb{R}^{|S|}$中的向量）的上包络：\n$$V_h(b) = \\max_{\\alpha \\in \\mathcal{V}_h} \\langle b, \\alpha \\rangle$$\n其中 $\\mathcal{V}_h$ 是时域为 $h$ 的 alpha-向量集，而 $\\langle \\cdot, \\cdot \\rangle$ 表示点积。\n\n价值迭代算法使用动态规划备份算子从 $\\mathcal{V}_{h-1}$ 计算集合 $\\mathcal{V}_h$。问题给出了单个分量 $\\alpha(s)$ 的备份方程。这可以用更紧凑的向量形式表示。对于一个给定的动作 $a$ 和为每个观测 $o \\in O$ 从 $\\mathcal{V}_{h-1}$ 中选择一个 alpha-向量 $\\alpha_o$，会生成一个新的 alpha-向量 $\\alpha$。我们定义一个中间向量 $g_a(\\{\\alpha_o\\}_{o \\in O})$，其分量由 $[g_a]_{s'} = \\sum_{o \\in O} P(o | s', a) \\cdot [\\alpha_o]_{s'}$ 给出。根据给定的问题参数，其中观测概率 $z_o(s')$ 与动作 $a$ 无关，我们可以将其写为 $g(\\{\\alpha_o\\}_{o \\in O}) = \\sum_{o \\in O} z_o \\odot \\alpha_o$，其中 $\\odot$ 表示逐元素乘积。那么单个新 alpha-向量的备份操作为：\n$$\\alpha = r_a + \\gamma T_a \\left( \\sum_{o \\in O} z_o \\odot \\alpha_o \\right)$$\n其中 $r_a$ 是动作 $a$ 的奖励向量， $T_a$ 是动作 $a$ 的转移矩阵。\n\n时域 $h=0$ 的初始价值函数是 $V_0(b)=0$，这对应于一个初始 alpha-向量集 $\\mathcal{V}_0 = \\{ \\vec{0} \\}$，其中 $\\vec{0}$ 是维度为 $|S|=3$ 的零向量。\n\n从 $\\mathcal{V}_{h-1}$ 生成 $\\mathcal{V}_h$ 的两种方法如下：\n\n1.  **带剪枝的精确价值迭代**：该方法构造所有可能的后继 alpha-向量的完整集合。对于每个动作 $a$，它通过考虑从 $\\mathcal{V}_{h-1}$ 中为每个观测 $o$ 分配 alpha-向量的所有可能组合来形成“交叉和”。\n    $$\\mathcal{V}_{h,a}^{\\text{cross-sum}} = \\left\\{ r_a + \\gamma T_a \\left( \\sum_{o \\in O} z_o \\odot \\alpha_o \\right) \\mid \\alpha_o \\in \\mathcal{V}_{h-1} \\text{ for each } o \\in O \\right\\}$$\n    完整的候选集是 $\\mathcal{V}_{h}^{\\text{cand}} = \\bigcup_{a \\in A} \\mathcal{V}_{h,a}^{\\text{cross-sum}}$。该集合的大小呈指数增长，因为 $|\\mathcal{V}_{h}^{\\text{cand}}| = |A| \\cdot |\\mathcal{V}_{h-1}|^{|O|}$。为保持计算的可行性，会应用一个剪枝步骤。只有那些在给定代表性点集（此处为评估网格 $\\mathcal{G}$）中至少对一个信念点是最优的 alpha-向量才会被保留。\n    $$\\mathcal{V}_{h}^{\\text{exact}} = \\text{Prune}(\\mathcal{V}_{h}^{\\text{cand}}, \\mathcal{G}) = \\left\\{ \\alpha^* \\in \\mathcal{V}_{h}^{\\text{cand}} \\mid \\exists b \\in \\mathcal{G} \\text{ s.t. } \\langle b, \\alpha^* \\rangle = \\max_{\\alpha' \\in \\mathcal{V}_{h}^{\\text{cand}}} \\langle b, \\alpha' \\rangle \\right\\}$$\n\n2.  **基于点的价值迭代 (PBVI)**：此方法代表一种有限理性，其中智能体具有有限的计算资源。它通过仅在一组预先选择的有限信念点 $\\mathcal{B}$（其中 $|\\mathcal{B}| \\le B$）上执行备份，来避免交叉和的组合爆炸。对于每个信念点 $b \\in \\mathcal{B}$，它为集合 $\\mathcal{V}_h$ 精确生成一个新的 alpha-向量。对于每个动作 $a$，会构造一个候选 alpha-向量 $\\alpha_{b,a}$。这是通过首先为每个可能的后续观测找到最佳未来价值来完成的。从信念 $b$ 执行动作 $a$ 并看到观测 $o$ 后的更新信念是 $b_{a,o}$。对于这个未来信念，来自前一个集合 $\\mathcal{V}_{h-1}$ 的最佳 alpha-向量是 $\\alpha_{a,o}^* = \\arg\\max_{\\alpha \\in \\mathcal{V}_{h-1}} \\langle b_{a,o}, \\alpha \\rangle$。那么动作 $a$ 的候选向量是：\n    $$\\alpha_{b,a} = r_a + \\gamma T_a \\left( \\sum_{o \\in O} z_o \\odot \\alpha_{a,o}^* \\right)$$\n    算法然后选择在信念 $b$ 处最优的动作：$a^* = \\arg\\max_{a \\in A} \\langle b, \\alpha_{b,a} \\rangle$。由信念 $b$ 贡献的单个新向量是 $\\alpha_{b,a^*}$。\n    下一个时域的结果集是这些向量的集合：\n    $$\\mathcal{V}_{h}^{\\text{PBVI}} = \\left\\{ \\alpha_{b,a^*} \\mid b \\in \\mathcal{B} \\right\\}$$\n    该集合的大小受限于 $|\\mathcal{B}| = B$。\n\n任务是实现这两种算法。对于每个测试用例 $(B,H)$，我们计算 $\\mathcal{V}_H^{\\text{exact}}$ 和 $\\mathcal{V}_H^{\\text{PBVI}}$。然后，对于评估网格 $\\mathcal{G}$ 中的每个信念点 $b$，我们计算由每个集合提供的值（$V^{\\text{exact}}(b)$ 和 $V^{\\text{PBVI}}(b)$），并找出在整个网格 $\\mathcal{G}$ 上这些值之间的最大绝对差，即误差 $E_{\\max}(B,H)$。\n\n实现将利用 Python 和 NumPy 库来高效地处理必要的向量和矩阵运算。用于 PBVI 的信念集 $\\mathcal{B}$ 被指定为给定评估网格 $\\mathcal{G}$ 中的前 $B$ 个信念。所有计算都是确定性的。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# Execution Environment:\n# language: Python\n# version: 3.12\n# libraries:\n#   - name: numpy, version: 1.23.5\n\ndef solve():\n    \"\"\"\n    Solves the POMDP value iteration problem as specified.\n    \"\"\"\n    # Define POMDP constants and parameters\n    GAMMA = 0.95\n    T_a0 = np.array([[0.7, 0.2, 0.1], [0.2, 0.6, 0.2], [0.1, 0.2, 0.7]])\n    T_a1 = np.array([[0.2, 0.6, 0.2], [0.3, 0.5, 0.2], [0.2, 0.5, 0.3]])\n    T = [T_a0, T_a1]\n\n    z_o0 = np.array([0.8, 0.3, 0.4])\n    z_o1 = np.array([0.2, 0.7, 0.6])\n    Z = [z_o0, z_o1]\n\n    r_a0 = np.array([0.0, 1.0, -0.4])\n    r_a1 = np.array([-0.2, 1.1, -0.5])\n    R = [r_a0, r_a1]\n    \n    NUM_STATES = T[0].shape[0]\n    NUM_ACTIONS = len(T)\n    NUM_OBS = len(Z)\n\n    # Evaluation grid G\n    G = [\n        np.array([1.0, 0.0, 0.0]),\n        np.array([0.0, 1.0, 0.0]),\n        np.array([0.0, 0.0, 1.0]),\n        np.array([0.5, 0.5, 0.0]),\n        np.array([0.5, 0.0, 0.5]),\n        np.array([0.0, 0.5, 0.5]),\n        np.array([0.5, 0.25, 0.25]),\n        np.array([0.25, 0.5, 0.25]),\n        np.array([0.25, 0.25, 0.5]),\n        np.array([1/3, 1/3, 1/3]),\n    ]\n\n    # Helper functions\n    def belief_update(b, a, o):\n        b_pred = b @ T[a]\n        b_prime_unnorm = Z[o] * b_pred\n        norm = np.sum(b_prime_unnorm)\n        if norm  1e-9:\n            # Fallback to uniform belief, though should not be triggered by given data\n            return np.full(NUM_STATES, 1.0 / NUM_STATES)\n        return b_prime_unnorm / norm\n\n    def get_value(b, V_set):\n        if not V_set:\n            return 0.0\n        return np.max([np.dot(b, alpha) for alpha in V_set])\n\n    def prune(V_cand, belief_points):\n        if not V_cand:\n            return []\n        \n        pruned_V_tuples = set()\n        # Find the best alpha vector for each belief point\n        for b in belief_points:\n            values = [np.dot(b, alpha) for alpha in V_cand]\n            best_alpha_idx = np.argmax(values)\n            pruned_V_tuples.add(tuple(V_cand[best_alpha_idx]))\n        \n        return [np.array(v) for v in pruned_V_tuples]\n\n    def backup_vector(alpha_o0, alpha_o1, a):\n        intermediate_vec = Z[0] * alpha_o0 + Z[1] * alpha_o1\n        return R[a] + GAMMA * (T[a] @ intermediate_vec)\n\n    # Main algorithm implementations\n    def solve_exact(H):\n        V_h = [np.zeros(NUM_STATES)]\n        for _ in range(H):\n            V_cand = []\n            for a in range(NUM_ACTIONS):\n                # Cross-sum generation\n                for alpha_o0 in V_h:\n                    for alpha_o1 in V_h:\n                        new_alpha = backup_vector(alpha_o0, alpha_o1, a)\n                        V_cand.append(new_alpha)\n            # Prune the candidate set using the evaluation grid G\n            V_h = prune(V_cand, G)\n        return V_h\n\n    def solve_pbvi(H, B):\n        B_set = G[:B]\n        if B > len(G): # As per problem statement for B > |G|\n            rng = np.random.default_rng(seed=0)\n            for _ in range(B - len(G)):\n                B_set.append(rng.dirichlet(np.ones(NUM_STATES)))\n\n        V_h = [np.zeros(NUM_STATES)]\n        for _ in range(H):\n            V_h_next_set = set()\n            for b_pbvi in B_set:\n                best_val_for_b = -np.inf\n                best_alpha_for_b = None\n                for a in range(NUM_ACTIONS):\n                    # Compute updated beliefs\n                    b_a_o0 = belief_update(b_pbvi, a, 0)\n                    b_a_o1 = belief_update(b_pbvi, a, 1)\n\n                    # Find best alpha for each observation\n                    alpha_o0_star = V_h[np.argmax([np.dot(b_a_o0, alpha) for alpha in V_h])]\n                    alpha_o1_star = V_h[np.argmax([np.dot(b_a_o1, alpha) for alpha in V_h])]\n\n                    # Construct candidate alpha for this action\n                    alpha_b_a = backup_vector(alpha_o0_star, alpha_o1_star, a)\n                    val_b_a = np.dot(b_pbvi, alpha_b_a)\n                    \n                    # Check if this action is the best for the current belief b_pbvi\n                    if val_b_a > best_val_for_b:\n                        best_val_for_b = val_b_a\n                        best_alpha_for_b = alpha_b_a\n                \n                if best_alpha_for_b is not None:\n                    V_h_next_set.add(tuple(best_alpha_for_b))\n            \n            V_h = [np.array(v) for v in V_h_next_set]\n            if not V_h:\n                V_h = [np.zeros(NUM_STATES)]\n        return V_h\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (3, 2), # Case 1: B=3, H=2\n        (2, 3), # Case 2: B=2, H=3\n        (6, 3), # Case 3: B=6, H=3\n        (1, 3), # Case 4: B=1, H=3\n        (6, 1)  # Case 5: B=6, H=1\n    ]\n\n    results = []\n    # Pre-calculate exact solutions for all required horizons to avoid re-computation\n    max_H = max(H for B, H in test_cases)\n    V_exact_sets = {}\n    current_V_exact = [np.zeros(NUM_STATES)]\n    V_exact_sets[0] = current_V_exact\n    for h in range(1, max_H + 1):\n        V_cand = []\n        for a in range(NUM_ACTIONS):\n            for alpha_o0 in current_V_exact:\n                for alpha_o1 in current_V_exact:\n                    V_cand.append(backup_vector(alpha_o0, alpha_o1, a))\n        current_V_exact = prune(V_cand, G)\n        V_exact_sets[h] = current_V_exact\n\n    for B, H in test_cases:\n        V_exact = V_exact_sets[H]\n        V_pbvi = solve_pbvi(H, B)\n\n        max_error = 0.0\n        for b_eval in G:\n            val_exact = get_value(b_eval, V_exact)\n            val_pbvi = get_value(b_eval, V_pbvi)\n            error = abs(val_exact - val_pbvi)\n            if error > max_error:\n                max_error = error\n        results.append(max_error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.10f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}