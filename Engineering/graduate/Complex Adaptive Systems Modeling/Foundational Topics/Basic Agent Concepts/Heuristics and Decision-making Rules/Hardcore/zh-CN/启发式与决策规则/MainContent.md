## 引言
在[复杂自适应系统](@entry_id:139930)（CAS）的广阔世界中，从个体到组织，智能体的行为并非总是遵循传统经济学所描绘的完美理性与最优计算。相反，它们依赖于一系列被称为“[启发式](@entry_id:261307)”（Heuristics）的心智捷径或经验法则来进行高效决策。这些简单的规则是理解复杂系统中适应性与[涌现行为](@entry_id:138278)的关键。然而，这些[启发式](@entry_id:261307)常常被误解为不完美的、次优的策略。本文旨在填补这一认知空白，系统地揭示[启发式](@entry_id:261307)不仅是认知限制下的无奈之举，更是演化出的、在特定环境中极为有效的适应性工具。

为此，我们将通过三个章节的探索，为读者构建一个关于启发式与决策规则的全面知识体系。首先，在“原理与机制”中，我们将深入有限理性的理论核心，剖析各类启发式规则的运作方式，以及它们在多智能体互动中的作用。接着，在“应用与跨学科联系”中，我们将展示这些理论在经济学、医学、法律和人工智能等真实世界场景中的强大解释力，探讨它们如何塑造集体行为并影响专业实践。最后，通过“动手实践”，你将有机会亲手实现和分析这些模型，将理论知识转化为可操作的技能。这趟旅程将带领我们从微观的决策规则出发，逐步揭示宏观复杂现象背后的深刻秩序。

## 原理与机制

在“导论”章节中，我们确立了[复杂自适应系统](@entry_id:139930)（CAS）中智能体的核心特征是其基于[启发式](@entry_id:261307)规则进行决策，而非遵循古典经济学中无限理性的最优化范式。本章将深入探讨这些[启发式](@entry_id:261307)规则的根本原理和关键机制。我们将从“有限理性”的基本概念出发，剖析其理论基础，然后系统性地考察各类[启发式](@entry_id:261307)规则——从个体选择到[策略互动](@entry_id:141147)，再到群体动态。最终，我们将探讨智能体如何在众多[启发式](@entry_id:261307)规则之间进行选择，即“元[启发式](@entry_id:261307)”的问题。本章旨在为读者构建一个关于[启发式](@entry_id:261307)决策的系统性知识框架，揭示简单规则如何驱动复杂的自适应行为。

### [有限理性](@entry_id:139029)的基础

传统的理性选择理论，以期望[效用最大化](@entry_id:144960)为基石，假定决策者拥有无限的认知与计算资源。然而，在现实世界的[复杂自适应系统](@entry_id:139930)中，智能体（无论是人类、组织还是算法）都受到其自身处理能力和可用信息的限制。有限理性（**bounded rationality**）理论正是为了描述在这种约束下的决策行为而生。

#### 从[无约束优化](@entry_id:137083)到资源理性

与其将智能体视为不完美的优化者，一个更深刻的视角是将其视为在约束条件下进行有效优化的“资源理性”（**resource-rational**）实体。该观点认为，认知过程本身——如信息搜集、记忆存取和算法计算——都是有成本的。因此，一个真正理性的智能体在选择行动策略时，不仅要考虑该策略带来的[期望效用](@entry_id:147484)，还必须权衡实施该策略所需的认知资源成本。

我们可以将这一理念形式化。假设一个智能体面临一个决策问题，其决策程序是一个将环境状态 $s \in S$ 映射到行动 $a \in A$ 的策略 $\pi: S \rightarrow A$。实施策略 $\pi$ 需要消耗计算资源，其成本为 $C(\pi)$。智能体每个决策周期的认知资源预算为 $B$。那么，一个资源理性的智能体所选择的策略 $\pi^{\star}$ 并非简单地最大化期望效用 $\mathbb{E}[U(a,s)]$，而是在满足资源预算的前提下最大化[期望效用](@entry_id:147484) 。这可以表达为一个[约束优化问题](@entry_id:1122941)：
$$
\pi^{\star} \in \arg\max_{\pi \in \Pi} \, \mathbb{E}\big[ U(a_\pi(S), S) \big] \quad \text{subject to} \quad C(\pi) \le B
$$
其中 $\Pi$ 是所有可能策略的集合。

该问题等价于一个带惩罚项的无[约束优化问题](@entry_id:1122941)：
$$
\pi^{\star} \in \arg\max_{\pi \in \Pi} \, \mathbb{E}\big[ U(a_\pi(S), S) \big] - \lambda \, C(\pi)
$$
其中，拉格朗日乘子 $\lambda \ge 0$ 代表了计算资源的“影子价格”或边际价值。从这个角度看，各种[启发式](@entry_id:261307)规则，如“满足即可”（satisficing）、“有限前瞻”（limited-horizon lookahead）等，都可以被理解为是计算成本 $C(\pi)$ 较低的策略。它们通过简化决策过程来节约宝贵的认知资源，尽管可能无法达到无约束条件下的理论最优解，但却能在“效用-成本”的综合考量下实现最优。这与将[有限理性](@entry_id:139029)简单归结为风险厌恶（[效用函数](@entry_id:137807)的[凹性](@entry_id:139843)）或随机错误（trembles）的观点形成了鲜明对比，后者未能抓住认知约束塑造决策规则本身的根本机制 。

#### 信息论视角：理性疏忽

对认知约束的另一种严谨形式化来[自信息](@entry_id:262050)论，即“理性疏忽”（**rational inattention**）理论。该理论假定智能体处理信息的能力是有限的，这种限制可以通过一个[信息通道](@entry_id:266393)的容量来量化。具体而言，智能体选择的行动 $A$ 能够反映多少关于环境状态 $X$ 的信息，是受到约束的。这一约束通常通过状态 $X$ 和行动 $A$ 之间的互信息量 $I(X;A)$ 来设定，即要求 $I(X;A) \le \kappa$，其中 $\kappa$ 是智能体的信息处理能力上限 。

互信息量 $I(X;A) = \mathbb{E}\big[\log \frac{\pi(A \mid X)}{p(A)}\big]$ 度量了在观察到行动 $A$ 之后，关于状态 $X$ 的不确定性的期望减少量。因此，理性疏忽问题可以表述为：在满足信息处理能力上限 $I(X;A) \le \kappa$ 的前提下，选择一个随机决策规则 $\pi(a \mid x)$ 来最大化[期望效用](@entry_id:147484) $\mathbb{E}[u(X,A)]$。

通过[拉格朗日乘子法](@entry_id:176596)分析此问题，可以推导出最优决策规则的形式。其拉格朗日函数为：
$$
\mathcal{L} = \sum_{x,a} p(x) \pi(a \mid x) u(x,a) - \lambda \left( \sum_{x,a} p(x) \pi(a \mid x) \log\frac{\pi(a \mid x)}{p(a)} - \kappa \right) + \dots
$$
（此处省略了其他[标准化](@entry_id:637219)约束）。求解此优化问题得到的[最优策略](@entry_id:138495) $\pi^{*}(a \mid x)$ 具有以下形式 ：
$$
\pi^{*}(a \mid x) = \frac{p(a)\exp\big(u(x,a)/\lambda\big)}{\sum_{a'}p(a')\exp\big(u(x,a')/\lambda\big)}
$$
其中 $p(a) = \sum_x p(x) \pi(a \mid x)$ 是行动的[边际概率分布](@entry_id:271532)，必须自洽地求解。这个结果非常深刻：它表明最优的有限理性策略是一种特殊的随机[选择规则](@entry_id:140784)，其形式类似于一个“扭曲”的 softmax 函数。行动的概率不仅取决于其在当前状态下的效用 $u(x,a)$，还取决于其自身的[边际概率](@entry_id:201078) $p(a)$。这捕捉了一个核心思想：智能体会优先将有限的注意力分配给那些[先验概率](@entry_id:275634)较低但潜在回报高的“意外”状态-行动组合，从而最有效地利用其信息处理能力。

### 个体决策[启发式](@entry_id:261307)规则

在有限理性的框架下，智能体演化或学习出了各种高效的启发式规则。这些规则是心智工具箱中的利器，适用于不同类型的决策环境。

#### 满足[启发式](@entry_id:261307)：寻求“足够好”

诺贝尔奖得主 [Herbert Simon](@entry_id:1126017) 最早提出的“满足”（**satisficing**）启发式是[有限理性](@entry_id:139029)理论的基石。与寻求最优解不同，满足型决策者会设定一个“期望水平”（**aspiration level**），然后按顺序搜寻选项，一旦遇到第一个满足该期望水平的选项，就停止搜寻并做出选择。

一个典型的例子是基于期望的决策规则。假设一个智能体可以在两个行动 $i \in \{1, 2\}$ 之间选择，行动 $i$ 带来的收益 $\pi^{(i)}$ 是一个[随机变量](@entry_id:195330)。智能体维持一个内在的期望水平 $A_t$，并根据经验动态调整它，例如 $A_{t+1} = (1-\lambda) A_t + \lambda \pi_t^{(i)}$，其中 $\pi_t^{(i)}$ 是在 $t$ 时刻采取行动 $i$ 后获得的实际收益。决策规则很简单：如果在 $t$ 时刻的收益 $\pi_t^{(i)}$ 不低于期望水平 $A_t$，则在 $t+1$ 时刻重复该行动；否则，切换到另一个行动 。

这个简单的规则展现了几个深刻的特性：
1.  **[路径依赖性](@entry_id:186326)**：当前的决策（是否切换）取决于期望水平 $A_t$，而 $A_t$ 本身是过去所有行动和收益历史的函数，这使得整个决策过程具有[路径依赖性](@entry_id:186326)。
2.  **自适应性**：期望水平并非一成不变。如果智能体的期望过高（例如，高于所有行动的平均收益），它将频繁地感到失望并切换行动。在失望的过程中，它获得的收益平均来说会低于其期望，导致期望水平 $A_t$ 在期望意义上向下漂移。反之，如果期望过低，它会频繁地感到满意，获得的收益将使其期望水平[向上调整](@entry_id:637064)。这种自我[调节机制](@entry_id:926520)使得期望水平能动态地[适应环境](@entry_id:156246)，找到一个“合理”的阈值 。
3.  **近似优化**：在某些条件下，这种简单的满足规则可以引导智能体在长期内更频繁地选择更优的行动。例如，如果行动1的收益分布一阶[随机占优](@entry_id:142966)于（First-Order Stochastically Dominates）行动2，意味着对于任何收益水平，行动1的收益超过该水平的概率都更高。那么，对于任何给定的期望水平 $A_t$，智能体对行动1感到“满意”的概率会更高，从而更倾向于“坚持”行动1。长期来看，智能体花在更优行动1上的时间会多于行动2 。

#### 快思节俭启发式与[生态理性](@entry_id:1124119)

另一类重要的启发式是“快思节俭启发式”（**fast-and-frugal heuristics**），它们通过利用环境中的统计规律来实现高效决策。这类启发式的有效性源于其与特定环境结构的“匹配”，这一概念被称为“[生态理性](@entry_id:1124119)”（**ecological rationality**）。

以一个[二元分类](@entry_id:142257)任务为例，智能体需要根据一系列线索 $X_i$ 来判断结果 $Y$ 属于类别0还是1。一个典型的快思节俭[启发式](@entry_id:261307)是“取优启发式”（**Take-the-Best, TTB**）。它的决策逻辑非常简单：
1.  **搜索规则**：按照线索的有效性（如区分两类别的能力）从高到低排序。
2.  **[停止规则](@entry_id:924532)**：依次检查线索，一旦遇到第一个可以区分的线索（例如，线索值为1指向类别1，为0则不确定），立即停止搜索。
3.  **决策规则**：根据这个线索做出决策。

这种启发式的结构——序贯搜索和单线索决策——使其在计算上极为“快”和“节俭”。与此相对，一个“完全理性”的[决策树](@entry_id:265930)或统计模型（如逻辑回归）可能会试图整合所有线索，并考虑它们之间复杂的相互作用。

快思节俭启发式的[生态理性](@entry_id:1124119)体现在，当环境具有特定结构时，它的表现可以媲美甚至超越更复杂的模型。一个经典的环境是“稀疏且冗余”（**sparse and redundant**）的环境 。
*   **稀疏性**（Sparsity）意味着在众多线索中，只有少数是真正有诊断价值的。
*   **冗余性**（Redundancy）意味着线索之间高度相关。

在这种环境下：
*   **取优[启发式](@entry_id:261307)（TTB, R3）** 的表现非常出色。它通过只关注最有效的线索，天然地利用了[稀疏性](@entry_id:136793)。通过在找到第一个有效线索后就停止，它巧妙地回避了处理冗余线索带来的麻烦。
*   一个**朴素的聚合规则（R2）**，如简单地将所有线索值相加然后判断正负，表现可能非常糟糕。因为它平等地对待了所有线索，没有利用[稀疏性](@entry_id:136793)。更严重的是，当无关线索的数量 $k$ 趋于无穷大时，这些线索中的[相关噪声](@entry_id:137358)会不断累积，最终淹没来自少数有效线索的宝贵信号，导致其决策准确率趋向于随机猜测（0.5）。
*   **规范最优的贝叶斯规则（R1）** 虽然理论上表现最好，但它要求完全了解所有线索的均值、方差以及它们之间复杂的协方差结构，并进行复杂的矩阵运算。这在信息和计算资源受限时往往是不可行的。

这个例子生动地展示了“少即是多”（less-is-more）效应：在特定环境下，一个信息和计算量都更少的简单启发式，其性能可以超越一个虽然使用更多信息但使用方式不当的复杂规则。这正是[生态理性](@entry_id:1124119)的核心要义：[启发式](@entry_id:261307)的智慧不在于其内在的逻辑复杂性，而在于其与外部世界结构的契合度。

一个更具体的快思节俭启发式是“快思节俭[决策树](@entry_id:265930)”（**Fast-and-Frugal Tree, FFT**）。与传统[决策树](@entry_id:265930)在每个节点都可以进行复杂的分支不同，FFT遵循“每条线索一个出口”（**one-exit-per-cue**）的原则。对于序列中的每一条线索，其两个可能的结果中只有一个会直接导向决策出口，另一个则会继续到下一条线索。这极大地简化了[决策树](@entry_id:265930)的结构，使其易于理解和执行，同时在信息获取有成本的环境下，通过尽[早停](@entry_id:633908)止来节约资源，这与[序贯决策](@entry_id:145234)理论的精神相符 。

#### 学习[启发式](@entry_id:261307)：[探索与利用的权衡](@entry_id:1124777)

在动态变化的环境中，智能体不仅要根据现有知识做出最佳决策（利用，**exploitation**），还需要尝试新的行动以获取关于环境的更多信息（探索，**exploration**）。这是强化学习中的一个经典两难问题。一些简单的启发式规则被广泛用于处理这种权衡。

假设智能体对每个行动 $i$ 的期望回报有一个估计值 $Q_i$。
*   **$\epsilon$-贪心（$\epsilon$-greedy）** 启发式：以 $1-\epsilon$ 的概率选择当前估计回报最高的行动（利用），以 $\epsilon$ 的概率从所有行动中随机均匀地选择一个（探索）。选择最优行动的概率为 $(1-\epsilon) + \epsilon/M$，而选择其他任一行动的概率均为 $\epsilon/M$，其中 $M$ 是行动总数。这个规则的优点是简单，但缺点是它在探索时完全随机，没有区分次优选项的好坏 。
*   **[Softmax](@entry_id:636766)（或 Boltzmann）探索** [启发式](@entry_id:261307)：该规则根据一个概率分布来选择行动，该分布使得高回报估计值的行动被选中的概率更高。行动 $i$ 被选中的概率 $p_i$ 由以下公式给出：
    $$
    p_i = \frac{\exp(Q_i/\tau)}{\sum_{j=1}^M \exp(Q_j/\tau)}
    $$
    其中 $\tau > 0$ 是一个“温度”参数。当 $\tau \to 0$ 时，选择趋向于确定性地选择最优行动（纯利用）；当 $\tau \to \infty$ 时，选择趋向于均匀随机选择（纯探索）。与 $\epsilon$-贪心不同，softmax 探索是“回报敏感”的：一个回报估计值较高的次优选项被选中的概率，会高于一个回报估计值很低的次优选项。这使得探索更加“智能” 。

### [Softmax](@entry_id:636766) 规则：随机选择的深层原理

[Softmax](@entry_id:636766) 规则不仅是一个有效的探索启发式，它在决策理论中还拥有深刻的理论基础，可以从公理化和[信息最大化](@entry_id:1126494)两个角度推导出来。

#### 公理化基础与最大熵原理

[Softmax](@entry_id:636766) 规则可以从 Duncan Luce 提出的“选择公理”（**Luce's choice axiom**）推导出来。该公理也被称为“无关备择项的独立性”（Independence of Irrelevant Alternatives, IIA）。它要求从一个选项集合 $S$ 中选择 $a$ 相对于选择 $b$ 的概率比，不应受到集合中其他选项的影响。结合一个额外的“尺度[平移不变性](@entry_id:195885)”公理，可以证明，满足这些公理的唯一选择概率形式就是 [Softmax](@entry_id:636766) 形式 。

此外，[Softmax](@entry_id:636766) 规则还可以通过“最大熵原理”（**maximum entropy principle**）来推导。该原理指出，在满足已知约束条件的所有概率分布中，我们应该选择熵最大的那一个，因为它对未知的部分做出了最少的假设。如果我们约束一个选择概率分布 $P(a)$ 必须满足一个特定的期望得分 $\sum_a P(a) Q(a) = \mu$，那么最大化香农熵 $H(P) = -\sum_a P(a) \log P(a)$ 的唯一解，就是具有 [Softmax](@entry_id:636766) 形式的吉布斯分布（Gibbs distribution）。

[Softmax](@entry_id:636766) 规则的这些深刻根源，解释了为何它在机器学习、[行为经济学](@entry_id:140038)和[复杂系统建模](@entry_id:203520)中如此普遍。它的一些重要性质包括 ：
*   **向 [argmax](@entry_id:634610) 收敛**：当温度 $\tau \to 0^+$ 时，[Softmax](@entry_id:636766) 规则收敛到确定性的 [argmax](@entry_id:634610) 规则，即以概率1选择 $Q$ 值最高的行动。
*   **[平移不变性](@entry_id:195885)**：将所有行动的 $Q$ 值加上一个常数 $c$（即 $Q_i \to Q_i+c$），选择概率 $p_i$ 保持不变。
*   **尺度等价性**：将所有 $Q$ 值乘以一个正常数 $a$（即 $Q_i \to aQ_i$），等价于将温度参数 $\tau$ 替换为 $\tau/a$。

### [多智能体系统](@entry_id:170312)中的[启发式](@entry_id:261307)

当我们将视角从单个智能体扩展到由大量相互作用的智能体组成的系统时，启发式规则的应用呈现出新的维度：[策略互动](@entry_id:141147)、群体学习和宏观现象的涌现。

#### 策略[启发式](@entry_id:261307)：模拟他人的心智

在博弈论情景中，一个智能体的最优行动取决于它对其他智能体将如何行动的信念。由于完全理性的循环推理（“我认为你认为我认为……”）在认知上是不可行的，智能体通常依赖于关于他人认知能力的启发式模型。

*   **$k$ 阶推理（Level-$k$ Reasoning）**：这是一个简单的递归模型。$0$ 阶智能体被假定为完全随机或根据某个简单规则行动（例如，均匀随机选择）。$1$ 阶智能体认为所有对手都是 $0$ 阶的，并据此做出最优反应。$2$ 阶智能体认为所有对手都是 $1$ 阶的，并做出最优反应，以此类推。一个 $k$ 阶智能体通过对一个 $(k-1)$ 阶对手做出最优反应来进行决策 。

*   **认知层级（Cognitive Hierarchy, CH）** 模型：这是 $k$ 阶模型的扩展。一个 $\ell$ 阶的 CH 智能体不只是认为所有对手都是 $(\ell-1)$ 阶，而是认为对手的阶数分布在 $\{0, 1, \dots, \ell-1\}$ 这些比它低的阶数上。通常假定这个分布服从一个截断并重新归一化的[泊松分布](@entry_id:147769)。这使得 CH 模型比简单的 $k$ 阶模型更具现实性，因为它承认了低阶智能体的持续存在，而不是假设所有人都在进行高阶推理 。

这些模型将复杂的策略推理问题简化为一个有限深度的计算过程，是典型的有限理性启发式。

*   **[量子反应均衡](@entry_id:1130372)（Quantal Response Equilibrium, QRE）**：QRE 放弃了[纳什均衡](@entry_id:137872)中智能体必须确定性地选择最优反应的苛刻要求。取而代之，它假定智能体的反应是随机的，更好（期望收益更高）的行动被选择的概率也更高，但并非绝对。这种随机反应通常用 Logit 形式（即 [Softmax](@entry_id:636766) 规则）来建模，其中精度参数 $\lambda$ (等价于 $1/\tau$) 控制了反应的“理性”程度 。
    *   当 $\lambda \to \infty$ 时，反应趋于确定性的最优反应，QRE 的任何[极限点](@entry_id:177089)都是一个[纳什均衡](@entry_id:137872)。
    *   当 $\lambda$ 为有限正数时，即使是次优的行动（包括被严格劣势的行动）也会以正的概率被选择，尽管概率可能很小。
    QRE 成功地解释了实验中观察到的许多偏离[纳什均衡](@entry_id:137872)预测的行为，并为“有噪声的理性”提供了一个优雅的均衡概念。

#### 群体动态：模仿与演化选择

在大型群体中，[启发式](@entry_id:261307)规则不仅用于个体决策，还通过社会学习过程进行传播和演化。一个核心机制是“模仿成功”（**imitation of success**）。

当大量智能体在一个对称的互动环境中反复进行随机匹配时，我们可以追踪不同策略（或启发式）在群体中所占份额的演化。如果智能体倾向于采用那些当前表现更好（即获得更高平均收益）的策略，那么群体的宏观动态可以用“[复制子动态](@entry_id:142626)”（**replicator dynamics**）方程来描述 ：
$$
\dot{x}_i = x_i \big[ \pi_i(x) - \bar{\pi}(x) \big]
$$
其中，$x_i$ 是使用策略 $i$ 的智能体在群体中的份额，$\pi_i(x)$ 是策略 $i$ 在当前群体状态 $x$ 下的期望收益，而 $\bar{\pi}(x) = \sum_j x_j \pi_j(x)$ 是群体的平均收益。

这个方程的含义是：一个策略的增长率与其当前份额 $x_i$ 成正比（因为只有存在的策略才能被模仿），也与其“[相对适应度](@entry_id:153028)”或“超额收益” $(\pi_i(x) - \bar{\pi}(x))$ 成正比。表现优于平均水平的策略，其份额会增长；反之则会[萎缩](@entry_id:925206)。这提供了一个从个体层面的模仿启发式到群体层面演化选择的直接联系，是连接微观行为与宏观动态的桥梁。

#### 从局部规则到涌现现象

[复杂自适应系统](@entry_id:139930)最迷人的特征之一是“涌现”（**emergence**）：没有中央协调、仅通过大量智能体遵循简单的局部规则进行互动，从而在宏观层面产生出复杂的、有序的模式。

一个经典的例子是基于阈值的级联模型 。想象一个社交网络中的个体，每个人都有一个采纳新行为（如购买新产品、接受新观念）的“阈值” $\theta_i$。该个体的决策[启发式](@entry_id:261307)是：当其邻居中采纳该行为的比例达到或超过其阈值 $\theta_i$ 时，他（她）也采纳该行为。

在这个模型中：
*   **微观规则**：每个智能体仅根据其邻居的局部信息和一个固定的阈值启发式做出决策。
*   **宏观现象**：从这个极其简单的微观规则中，可以涌现出两种截然不同的宏观结果：
    1.  **局部传播**：最初的少数采纳者可能只会影响到他们周围的一小部分人，传播很快就会终止。
    2.  **全局级联**（Global Cascade）：在某些条件下，最初的星星之火可以燎原，引发一场席卷整个网络的大规模采纳浪潮，最终达到一个高比例的“协调”状态。

这种宏观结果的出现并非由任何权威或全局信息所引导，完全是微观互动自组织的结果。通过使用分支过程等数学工具，我们可以精确地分析出发生全局级联的条件。例如，在稀疏网络中，级联能否发生取决于一个关键参数：一个被激活的个体平均能激活多少个新的“易感”邻居。当这个“繁殖数”超过1时，全局级联就会发生 。这清晰地展示了宏观层面的涌现现象是如何根植于微观层面的[启发式](@entry_id:261307)规则和互动结构的。

### 元启发式：选择如何选择的问题

一个成熟的自适应智能体可能拥有不止一种启发式规则。这就引出了一个更高层次的决策问题：在特定时刻，应该使用哪个启发式？解决这个问题的规则被称为“元[启发式](@entry_id:261307)”（**meta-heuristic**）。

一个理性的元[启发式](@entry_id:261307)规则应该基于近期各启发式的表现来做决策，同时考虑到切换[启发式](@entry_id:261307)本身可能带来的成本。假设智能体可以从一组[启发式](@entry_id:261307) $\{h_1, \dots, h_m\}$ 中选择，切换的成本为 $c > 0$。环境是一个遍历的[马尔可夫过程](@entry_id:1127634)，具有混合时间 $T_{\text{mix}}$，这意味着在大约 $T_{\text{mix}}$ 的时间窗口内，环境的统计特性是相对稳定的。

一个合理的确定性元启发式规则可以这样构建 ：
1.  **评估**：对于每个候选[启发式](@entry_id:261307) $h_i$，计算其在最近一个长度为 $w$ 的时间窗口内的平均收益。选择 $w \approx T_{\text{mix}}$ 是理性的，因为它既能平滑短期噪声，又不会被太陈旧的无关信息所污染。
2.  **成本考量**：对于每个不是当前正在使用的[启发式](@entry_id:261307)，从其评估收益中减去切换成本 $c$。
3.  **选择**：选择经过成本调整后，评估收益最高的那个[启发式](@entry_id:261307)。

这个选择过程可以形式化为：
$$
h_t \in \arg\max_{i \in \{1,\dots,m\}} \left\{ \frac{1}{w} \sum_{k=0}^{w-1} p_i(t-k) \;-\; c \cdot \mathbf{1}\{i \neq h_{t-1}\} \right\}
$$
其中 $p_i(t-k)$ 是[启发式](@entry_id:261307) $i$ 在过去时刻的收益，$\mathbf{1}\{i \neq h_{t-1}\}$ 是一个指示函数，当需要切换时其值为1，否则为0。这个元启发式规则本身也是一个在资源理性框架下的高级决策过程，它平衡了对更优策略的追求和维持现状以避免切换成本的需求。