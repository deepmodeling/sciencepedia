{
    "hands_on_practices": [
        {
            "introduction": "An agent's behavioral rule can range from perfectly predictable to completely random. To formally quantify this spectrum of uncertainty, we employ the concept of Shannon entropy. This exercise  provides foundational practice in applying entropy to a simple stochastic policy, allowing you to derive how its informational content changes with its parameters and to identify the point of maximum unpredictability.",
            "id": "4119714",
            "problem": "In a single-state decision environment modeled as a Markov decision process, an agent defines a behavioral rule at state $s$ by a stationary stochastic policy $\\,\\pi(\\cdot \\mid s)\\,$ over a binary action set $\\{a_{0}, a_{1}\\}$. The agent selects $a_{1}$ with probability $p \\in [0,1]$ and $a_{0}$ with probability $1-p$, which parameterizes a family of behavioral rules ranging from deterministic ($p \\in \\{0,1\\}$) to stochastic ($p \\in (0,1)$). Use the core definition of Shannon entropy for a discrete distribution with natural logarithm (entropy in nats) as the foundational starting point. From this base, derive the expression for the entropy $H(\\pi(\\cdot \\mid s))$ as a function of $p$, and, by reasoning from first principles, characterize how the entropy compares between deterministic and stochastic rules. Identify the value of $p$ that maximizes the entropy and justify it from the derived expression without assuming any result about its maximum. Finally, compute the exact closed-form value of $H(\\pi(\\cdot \\mid s))$ at $p=\\frac{1}{2}$ using natural logarithms. Express the final entropy value in nats and provide the exact expression; do not numerically approximate or round.",
            "solution": "The problem statement is first validated for correctness and completeness.\n\n### Step 1: Extract Givens\n- **Environment:** A single-state decision environment modeled as a Markov decision process.\n- **State:** $s$.\n- **Policy:** A stationary stochastic policy $\\pi(\\cdot \\mid s)$ over a binary action set.\n- **Action Set:** $\\{a_{0}, a_{1}\\}$.\n- **Policy Parameterization:** The agent selects action $a_1$ with probability $p$ and action $a_0$ with probability $1-p$, where $p \\in [0,1]$.\n- **Rule Types:**\n    - Deterministic rules correspond to $p \\in \\{0,1\\}$.\n    - Stochastic rules correspond to $p \\in (0,1)$.\n- **Entropy Definition:** Shannon entropy for a discrete distribution, using the natural logarithm (base $e$), with entropy measured in nats.\n- **Objectives:**\n    1.  Derive the expression for the entropy $H(\\pi(\\cdot \\mid s))$ as a function of $p$.\n    2.  Compare the entropy of deterministic versus stochastic rules.\n    3.  Find the value of $p$ that maximizes the entropy, justifying from first principles.\n    4.  Compute the exact closed-form value of $H(\\pi(\\cdot \\mid s))$ at $p=\\frac{1}{2}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded in information theory and reinforcement learning. The concepts of Shannon entropy and stochastic policies are standard and well-defined. The problem is well-posed, providing all necessary information to derive the requested expressions and values. The language is objective and formal. The problem contains no scientific fallacies, contradictions, or ambiguities. It is a standard, formalizable exercise in applying the definition of entropy.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A solution will be derived.\n\n### Derivation and Analysis\n\nThe foundational definition of Shannon entropy for a discrete probability distribution $P = \\{p_1, p_2, \\dots, p_n\\}$ is given by:\n$$H(P) = -\\sum_{i=1}^{n} p_i \\log_b(p_i)$$\nThe problem specifies the use of the natural logarithm, so the base $b$ is Euler's number, $e$, and the entropy is measured in nats. The policy $\\pi(\\cdot \\mid s)$ defines a discrete probability distribution over the action set $\\{a_0, a_1\\}$. The probabilities of the two actions are:\n$$ \\pi(a_1 \\mid s) = p $$\n$$ \\pi(a_0 \\mid s) = 1-p $$\nApplying the definition of Shannon entropy to this policy, we obtain the entropy $H(\\pi(\\cdot \\mid s))$ as a function of $p$, which we denote as $H(p)$:\n$$ H(p) = - \\left( \\pi(a_0 \\mid s) \\ln(\\pi(a_0 \\mid s)) + \\pi(a_1 \\mid s) \\ln(\\pi(a_1 \\mid s)) \\right) $$\nSubstituting the given probabilities yields the expression for the entropy:\n$$ H(p) = - \\left( (1-p)\\ln(1-p) + p\\ln(p) \\right) $$\nThis expression is valid for $p \\in (0,1)$. For the endpoints $p=0$ and $p=1$, we must evaluate the limit of $x\\ln(x)$ as $x \\to 0^+$. Using L'Hôpital's rule:\n$$ \\lim_{x \\to 0^+} x\\ln(x) = \\lim_{x \\to 0^+} \\frac{\\ln(x)}{1/x} = \\lim_{x \\to 0^+} \\frac{1/x}{-1/x^2} = \\lim_{x \\to 0^+} (-x) = 0 $$\nBy this standard convention, we define $0\\ln(0) = 0$.\n\nNext, we compare the entropy for deterministic and stochastic rules.\n- **Deterministic rules:** $p \\in \\{0, 1\\}$.\n  - If $p=0$, the policy is to always choose $a_0$. The entropy is $H(0) = -((1-0)\\ln(1-0) + 0\\ln(0)) = -(1\\ln(1) + 0) = 0$.\n  - If $p=1$, the policy is to always choose $a_1$. The entropy is $H(1) = -((1-1)\\ln(1-1) + 1\\ln(1)) = -(0\\ln(0) + 0) = 0$.\n  Thus, for deterministic rules, the entropy is $0$ nats. This reflects the complete certainty of the action to be taken.\n- **Stochastic rules:** $p \\in (0, 1)$.\n  - For any $p$ in this open interval, both $p$ and $1-p$ are numbers strictly between $0$ and $1$.\n  - The natural logarithm of a number between $0$ and $1$ is negative. Therefore, $\\ln(p)  0$ and $\\ln(1-p)  0$.\n  - The terms $p\\ln(p)$ and $(1-p)\\ln(1-p)$ are both negative.\n  - Their sum is strictly negative.\n  - Consequently, $H(p) = -(\\text{negative value}) > 0$.\nThe entropy of any stochastic rule is strictly greater than the entropy of any deterministic rule. Entropy quantifies uncertainty, and stochastic rules inherently possess more uncertainty than deterministic ones.\n\nTo find the value of $p$ that maximizes the entropy, we analyze the function $H(p) = -p\\ln(p) - (1-p)\\ln(1-p)$ on the interval $[0,1]$. We find the critical points by taking the first derivative with respect to $p$ and setting it to zero.\n$$ \\frac{dH}{dp} = \\frac{d}{dp} \\left( -p\\ln(p) - (1-p)\\ln(1-p) \\right) $$\nUsing the product rule for differentiation:\n$$ \\frac{dH}{dp} = -\\left[\\left(1\\cdot\\ln(p) + p\\cdot\\frac{1}{p}\\right) + \\left(-1\\cdot\\ln(1-p) + (1-p)\\cdot\\frac{-1}{1-p}\\right)\\right] $$\n$$ \\frac{dH}{dp} = -\\left[ (\\ln(p) + 1) + (-\\ln(1-p) - 1) \\right] $$\n$$ \\frac{dH}{dp} = -[\\ln(p) - \\ln(1-p)] = \\ln(1-p) - \\ln(p) = \\ln\\left(\\frac{1-p}{p}\\right) $$\nSetting the derivative to zero to find the extremum:\n$$ \\ln\\left(\\frac{1-p}{p}\\right) = 0 $$\n$$ \\frac{1-p}{p} = \\exp(0) = 1 $$\n$$ 1-p = p \\implies 2p = 1 \\implies p = \\frac{1}{2} $$\nTo confirm this is a maximum, we examine the second derivative:\n$$ \\frac{d^2H}{dp^2} = \\frac{d}{dp}\\left( \\ln(1-p) - \\ln(p) \\right) = \\frac{-1}{1-p} - \\frac{1}{p} = -\\left(\\frac{1}{1-p} + \\frac{1}{p}\\right) $$\nFor any $p \\in (0, 1)$, both $p$ and $1-p$ are positive. Therefore, the term $\\left(\\frac{1}{1-p} + \\frac{1}{p}\\right)$ is strictly positive. This makes the second derivative $\\frac{d^2H}{dp^2}$ strictly negative for all $p \\in (0,1)$. A negative second derivative indicates that the function $H(p)$ is strictly concave, and thus the critical point $p=\\frac{1}{2}$ is a unique global maximum. The entropy is maximized when there is maximum uncertainty about the outcome, which occurs when both actions are equally probable.\n\nFinally, we compute the exact value of the maximum entropy at $p = \\frac{1}{2}$.\n$$ H\\left(\\frac{1}{2}\\right) = - \\left( \\left(1-\\frac{1}{2}\\right)\\ln\\left(1-\\frac{1}{2}\\right) + \\frac{1}{2}\\ln\\left(\\frac{1}{2}\\right) \\right) $$\n$$ H\\left(\\frac{1}{2}\\right) = - \\left( \\frac{1}{2}\\ln\\left(\\frac{1}{2}\\right) + \\frac{1}{2}\\ln\\left(\\frac{1}{2}\\right) \\right) $$\n$$ H\\left(\\frac{1}{2}\\right) = - \\left( 2 \\cdot \\frac{1}{2}\\ln\\left(\\frac{1}{2}\\right) \\right) = -\\ln\\left(\\frac{1}{2}\\right) $$\nUsing the logarithm property $-\\ln(x) = \\ln(x^{-1})$:\n$$ H\\left(\\frac{1}{2}\\right) = \\ln\\left(\\left(\\frac{1}{2}\\right)^{-1}\\right) = \\ln(2) $$\nThe exact closed-form value of the maximum entropy is $\\ln(2)$ nats.",
            "answer": "$$\\boxed{\\ln(2)}$$"
        },
        {
            "introduction": "Many agent-based models are built on the principle of rationality, where agents act to maximize their utility subject to various constraints. This practice  delves into the mathematical machinery behind this paradigm, using the Karush–Kuhn–Tucker (KKT) conditions to solve for an agent's optimal action. By working through this constrained optimization problem, you will gain hands-on experience in deriving behavioral rules from first principles of economic decision-making.",
            "id": "4119693",
            "problem": "Consider a single representative agent in a complex adaptive system who, at state $s \\in \\mathcal{S}$, selects a scalar action $a \\in \\mathbb{R}$ according to a behavioral rule that maximizes a differentiable utility $u(s,a)$ subject to $m$ differentiable inequality constraints $g_i(s,a) \\le 0$ for $i \\in \\{1,\\dots,m\\}$. Assume the feasible set $\\{a \\in \\mathbb{R} \\mid g_i(s,a) \\le 0, \\, i=1,\\dots,m\\}$ is nonempty and convex, and that a constraint qualification such as Slater's condition holds (there exists an $a$ with $g_i(s,a)  0$ for all $i$). Starting from the foundational definition of constrained optimality and Lagrangian duality, derive the Karush–Kuhn–Tucker (KKT) conditions that characterize an optimal action $a^{\\star}(s)$ for the agent under these inequality constraints.\n\nThen instantiate the setup with a single inequality constraint and a concave quadratic utility. Let\n$$\nu(s,a) \\equiv -\\frac{1}{2}\\,\\alpha \\,\\big(a - \\mu\\big)^{2} + \\beta,\n$$\nwhere $\\alpha > 0$, and let the constraint be\n$$\ng(s,a) \\equiv \\rho\\, a - \\gamma \\le 0,\n$$\nwith $\\rho > 0$. Here $\\mu, \\beta, \\gamma \\in \\mathbb{R}$ may depend on the state $s$, but treat them as given parameters when solving for $a^{\\star}(s)$. Using the KKT conditions you derived, compute the agent’s optimal action $a^{\\star}(s)$ in exact closed form as a function of $(\\mu, \\alpha, \\rho, \\gamma, \\beta)$, simplifying the expression as far as possible. Express the final answer as a single analytic expression; no rounding is required.",
            "solution": "The problem is divided into two parts. First, we must derive the general Karush–Kuhn–Tucker (KKT) conditions for a constrained optimization problem. Second, we apply these conditions to a specific instance of utility maximization to find the agent's optimal action.\n\n### Part 1: Derivation of the KKT Conditions\n\nThe agent's problem is to choose an action $a \\in \\mathbb{R}$ to solve the following optimization problem for a given state $s \\in \\mathcal{S}$:\n$$\n\\max_{a \\in \\mathbb{R}} u(s,a)\n$$\nsubject to $m$ inequality constraints:\n$$\ng_i(s,a) \\le 0, \\quad \\text{for } i \\in \\{1, \\dots, m\\}\n$$\nWe are given that the functions $u(s,a)$ and $g_i(s,a)$ are differentiable with respect to $a$. The feasible set is nonempty and convex, and a constraint qualification (such as Slater's condition) holds. For notational simplicity, we will suppress the explicit dependence on the state $s$, treating it as fixed.\n\nTo find the conditions for an optimal action $a^{\\star}$, we use the method of Lagrange multipliers. We formulate the Lagrangian function, $\\mathcal{L}$, which incorporates the objective function and the constraints. For a maximization problem with \"less than or equal to\" constraints, the Lagrangian is defined as:\n$$\n\\mathcal{L}(a, \\boldsymbol{\\lambda}) = u(a) - \\sum_{i=1}^{m} \\lambda_i g_i(a)\n$$\nwhere $\\boldsymbol{\\lambda} = (\\lambda_1, \\lambda_2, \\dots, \\lambda_m)$ is the vector of Lagrange multipliers, also known as dual variables.\n\nThe KKT conditions are the first-order necessary conditions for a point $a^{\\star}$ to be a local maximum. Given that the objective function is concave and the feasible set is convex (as specified for the second part of the problem, and a general condition for KKT to be sufficient), these necessary conditions are also sufficient for a global maximum. The conditions are derived from the principles of stationarity with respect to the choice variable $a$ and the complementary nature of the constraints.\n\nLet $a^{\\star}$ be an optimal solution and $\\boldsymbol{\\lambda}^{\\star}$ be the corresponding vector of optimal Lagrange multipliers. The KKT conditions are:\n\n1.  **Stationarity:** At an optimal point $a^{\\star}$, the gradient of the Lagrangian with respect to $a$ must be zero. Since $a$ is a scalar, this simplifies to the derivative being zero.\n    $$\n    \\frac{\\partial \\mathcal{L}}{\\partial a} \\bigg|_{a=a^{\\star}, \\boldsymbol{\\lambda}=\\boldsymbol{\\lambda}^{\\star}} = \\frac{\\partial u}{\\partial a}(a^{\\star}) - \\sum_{i=1}^{m} \\lambda_i^{\\star} \\frac{\\partial g_i}{\\partial a}(a^{\\star}) = 0\n    $$\n\n2.  **Primal Feasibility:** The optimal action $a^{\\star}$ must satisfy all the original constraints.\n    $$\n    g_i(a^{\\star}) \\le 0, \\quad \\text{for all } i \\in \\{1, \\dots, m\\}\n    $$\n\n3.  **Dual Feasibility:** The Lagrange multipliers associated with the inequality constraints must be non-negative.\n    $$\n    \\lambda_i^{\\star} \\ge 0, \\quad \\text{for all } i \\in \\{1, \\dots, m\\}\n    $$\n\n4.  **Complementary Slackness:** For each constraint, the product of the multiplier and the constraint function value must be zero. This means that if a constraint is not active (i.e., $g_i(a^{\\star})  0$), its corresponding multiplier must be zero ($\\lambda_i^{\\star} = 0$). Conversely, if a multiplier is positive ($\\lambda_i^{\\star}  0$), its corresponding constraint must be active (i.e., $g_i(a^{\\star}) = 0$).\n    $$\n    \\lambda_i^{\\star} g_i(a^{\\star}) = 0, \\quad \\text{for all } i \\in \\{1, \\dots, m\\}\n    $$\n\nThese four sets of conditions together constitute the KKT conditions for the given constrained optimization problem.\n\n### Part 2: Application to a Specific Case\n\nWe are given the specific utility and constraint functions:\nUtility: $u(s,a) = -\\frac{1}{2}\\alpha(a - \\mu)^{2} + \\beta$, with $\\alpha > 0$.\nConstraint: $g(s,a) = \\rho a - \\gamma \\le 0$, with $\\rho > 0$.\nThe parameters $\\mu, \\beta, \\gamma, \\alpha, \\rho$ are treated as given constants for the optimization at a fixed state $s$. Note that the utility function $u(a)$ is strictly concave in $a$ since its second derivative, $\\frac{d^2u}{da^2} = -\\alpha$, is negative. The constraint function $g(a)$ is linear, hence it is a convex function. This fulfills the conditions for the KKT conditions to be sufficient for a unique global maximum.\n\nWe now apply the KKT conditions. There is a single constraint, so we have a single Lagrange multiplier $\\lambda \\ge 0$.\n\nThe Lagrangian is:\n$$\n\\mathcal{L}(a, \\lambda) = \\left(-\\frac{1}{2}\\alpha(a - \\mu)^{2} + \\beta\\right) - \\lambda(\\rho a - \\gamma)\n$$\n\nThe KKT conditions for the optimal action $a^{\\star}$ and multiplier $\\lambda^{\\star}$ are:\n\n1.  **Stationarity:**\n    $$\n    \\frac{\\partial \\mathcal{L}}{\\partial a} = -\\alpha(a^{\\star} - \\mu) - \\lambda^{\\star}\\rho = 0 \\implies \\alpha(a^{\\star} - \\mu) + \\lambda^{\\star}\\rho = 0\n    $$\n\n2.  **Primal Feasibility:**\n    $$\n    \\rho a^{\\star} - \\gamma \\le 0\n    $$\n\n3.  **Dual Feasibility:**\n    $$\n    \\lambda^{\\star} \\ge 0\n    $$\n\n4.  **Complementary Slackness:**\n    $$\n    \\lambda^{\\star}(\\rho a^{\\star} - \\gamma) = 0\n    $$\n\nWe solve this system of conditions by considering two cases based on the complementary slackness condition.\n\n**Case 1: The constraint is not binding (slack).**\nIn this case, $\\rho a^{\\star} - \\gamma  0$. Complementary slackness implies that $\\lambda^{\\star} = 0$.\nSubstituting $\\lambda^{\\star} = 0$ into the stationarity condition gives:\n$$\n\\alpha(a^{\\star} - \\mu) = 0\n$$\nSince $\\alpha > 0$, we must have $a^{\\star} - \\mu = 0$, which implies $a^{\\star} = \\mu$.\nThis solution is valid only if it satisfies the primal feasibility condition under which this case is defined: $\\rho a^{\\star} - \\gamma \\le 0$. Substituting $a^{\\star} = \\mu$, we get:\n$$\n\\rho \\mu - \\gamma \\le 0 \\quad \\text{or} \\quad \\mu \\le \\frac{\\gamma}{\\rho}\n$$\nSo, if $\\mu \\le \\frac{\\gamma}{\\rho}$, the optimal action is $a^{\\star} = \\mu$. This corresponds to the unconstrained maximum of the utility function being feasible. Note that the parameter $\\beta$ does not influence the location of the optimum.\n\n**Case 2: The constraint is binding (active).**\nIn this case, $\\rho a^{\\star} - \\gamma = 0$. This directly gives the value for the optimal action:\n$$\na^{\\star} = \\frac{\\gamma}{\\rho}\n$$\nFor this to be the solution, the corresponding Lagrange multiplier $\\lambda^{\\star}$ must be non-negative ($\\lambda^{\\star} \\ge 0$). From the stationarity condition:\n$$\n\\lambda^{\\star}\\rho = -\\alpha(a^{\\star} - \\mu)\n$$\nSubstituting $a^{\\star} = \\gamma/\\rho$:\n$$\n\\lambda^{\\star}\\rho = -\\alpha\\left(\\frac{\\gamma}{\\rho} - \\mu\\right) = \\alpha\\left(\\mu - \\frac{\\gamma}{\\rho}\\right)\n$$\nSince $\\rho > 0$, we can solve for $\\lambda^{\\star}$:\n$$\n\\lambda^{\\star} = \\frac{\\alpha}{\\rho}\\left(\\mu - \\frac{\\gamma}{\\rho}\\right)\n$$\nThe dual feasibility condition $\\lambda^{\\star} \\ge 0$ implies that $\\frac{\\alpha}{\\rho}\\left(\\mu - \\frac{\\gamma}{\\rho}\\right) \\ge 0$. Since $\\alpha > 0$ and $\\rho > 0$, this is equivalent to:\n$$\n\\mu - \\frac{\\gamma}{\\rho} \\ge 0 \\quad \\text{or} \\quad \\mu \\ge \\frac{\\gamma}{\\rho}\n$$\nSo, if $\\mu \\ge \\frac{\\gamma}{\\rho}$, the optimal action is $a^{\\star} = \\frac{\\gamma}{\\rho}$. This corresponds to the unconstrained maximum being outside the feasible set, so the optimum lies on the boundary.\n\n**Combining the results:**\nWe have found that:\n- If $\\mu \\le \\frac{\\gamma}{\\rho}$, then $a^{\\star} = \\mu$.\n- If $\\mu > \\frac{\\gamma}{\\rho}$, then $a^{\\star} = \\frac{\\gamma}{\\rho}$.\n\nThis logic can be expressed concisely using the minimum function. The agent's problem is to maximize a quadratic centered at $\\mu$, subject to the constraint $a \\le \\gamma/\\rho$. This is equivalent to finding the point in the feasible interval $(-\\infty, \\gamma/\\rho]$ that is closest to $\\mu$. This point is simply the minimum of $\\mu$ and the upper bound $\\gamma/\\rho$.\n\nTherefore, the optimal action $a^{\\star}$ can be written as a single analytic expression:\n$$\na^{\\star}(s) = \\min\\left(\\mu, \\frac{\\gamma}{\\rho}\\right)\n$$\nThe parameters $\\alpha$ and $\\beta$ do not appear in the final expression for the optimal action $a^{\\star}$. The parameter $\\alpha$ scales the utility function but does not change the location of its peak, and $\\beta$ is an additive constant that shifts the utility value but also does not affect the optimal action.",
            "answer": "$$\n\\boxed{\\min\\left(\\mu, \\frac{\\gamma}{\\rho}\\right)}\n$$"
        },
        {
            "introduction": "While theoretical models of agent behavior are powerful, their practical value often lies in our ability to fit them to real-world data. This exercise  tackles the fundamental inverse problem: inferring an agent's internal decision rule from its observed actions. You will derive the Maximum Likelihood Estimator (MLE) for a logistic policy, a core technique in machine learning and econometrics for connecting behavioral data to parametric models.",
            "id": "4119704",
            "problem": "Consider a population of agents in a complex adaptive system. At discrete times indexed by $t \\in \\{1,2,\\dots,T\\}$, each agent observes a binary state $s_t \\in \\{0,1\\}$ and chooses a binary action $a_t \\in \\{0,1\\}$. Suppose the agent’s behavioral rule is parametric and given by a logistic policy\n$$\n\\pi_{\\theta}(a=1 \\mid s) \\;=\\; \\sigma(\\theta s),\n$$\nwhere $\\theta \\in \\mathbb{R}$ is an unknown parameter and $\\sigma(x)$ denotes the logistic sigmoid function defined by $\\sigma(x) = \\frac{1}{1+\\exp(-x)}$. Assume conditional independence of actions given states across time, so that the joint probability of observed actions $(a_1,\\dots,a_T)$ given states $(s_1,\\dots,s_T)$ and parameter $\\theta$ factorizes as $\\prod_{t=1}^{T} \\pi_{\\theta}(a_t \\mid s_t)$.\n\nStarting from the fundamental definitions of the Bernoulli distribution and the logistic sigmoid, perform the following:\n- Construct the likelihood function $\\mathcal{L}(\\theta) = \\prod_{t=1}^{T} \\pi_{\\theta}(a_t \\mid s_t)$ for the observed data.\n- Derive from first principles the Maximum Likelihood Estimator (MLE) for $\\theta$, that is, the value $\\hat{\\theta}$ that maximizes the likelihood (or equivalently the log-likelihood), expressed in closed form in terms of the observed data.\n\nLet $n_1 = \\sum_{t=1}^{T} \\mathbf{1}\\{s_t=1\\}$ and $k_1 = \\sum_{t=1}^{T} a_t \\mathbf{1}\\{s_t=1\\}$, where $\\mathbf{1}\\{\\cdot\\}$ is the indicator function. Assume $n_1 \\ge 1$ and that the conditional sample mean among the times with $s_t=1$ satisfies $0  \\frac{k_1}{n_1}  1$ so that the MLE exists and is finite. Express your final answer as a single closed-form analytic expression in terms of $n_1$ and $k_1$. No rounding is required. Do not include any units.",
            "solution": "The problem requires the derivation of the Maximum Likelihood Estimator (MLE) for the parameter $\\theta$ of a logistic policy model. The derivation will proceed from first principles, starting with the construction of the likelihood function for the observed data.\n\nThe agent's behavioral rule is a logistic policy given by:\n$$\n\\pi_{\\theta}(a=1 \\mid s) = \\sigma(\\theta s) = \\frac{1}{1+\\exp(-\\theta s)}\n$$\nwhere $a_t \\in \\{0,1\\}$ is the agent's action and $s_t \\in \\{0,1\\}$ is the observed state at time $t$. The action $a_t$ for a given state $s_t$ can be modeled as a Bernoulli random variable, $a_t \\sim \\text{Bernoulli}(p_t)$, with the probability of success $p_t = \\pi_{\\theta}(a_t=1 \\mid s_t)$.\n\nThe probability mass function for a single observation $(a_t, s_t)$ can be written in a compact form:\n$$\n\\pi_{\\theta}(a_t \\mid s_t) = \\left[ \\sigma(\\theta s_t) \\right]^{a_t} \\left[ 1 - \\sigma(\\theta s_t) \\right]^{1-a_t}\n$$\nThis expression correctly gives $\\sigma(\\theta s_t)$ for $a_t=1$ and $1-\\sigma(\\theta s_t)$ for $a_t=0$.\n\nThe problem states that actions are conditionally independent across time given the states. Therefore, the total likelihood function $\\mathcal{L}(\\theta)$ for the entire sequence of observations $\\{(a_t, s_t)\\}_{t=1}^{T}$ is the product of the individual probabilities:\n$$\n\\mathcal{L}(\\theta) = \\prod_{t=1}^{T} \\pi_{\\theta}(a_t \\mid s_t) = \\prod_{t=1}^{T} \\left[ \\sigma(\\theta s_t) \\right]^{a_t} \\left[ 1 - \\sigma(\\theta s_t) \\right]^{1-a_t}\n$$\nThis is the likelihood function for the observed data.\n\nTo find the MLE $\\hat{\\theta}$, it is more convenient to maximize the log-likelihood function, $\\ell(\\theta) = \\ln(\\mathcal{L}(\\theta))$, since the logarithm is a strictly increasing function and will yield the same maximizing parameter.\n$$\n\\ell(\\theta) = \\ln\\left( \\prod_{t=1}^{T} \\left[ \\sigma(\\theta s_t) \\right]^{a_t} \\left[ 1 - \\sigma(\\theta s_t) \\right]^{1-a_t} \\right)\n$$\nUsing the properties of logarithms, this becomes:\n$$\n\\ell(\\theta) = \\sum_{t=1}^{T} \\left[ a_t \\ln(\\sigma(\\theta s_t)) + (1-a_t) \\ln(1 - \\sigma(\\theta s_t)) \\right]\n$$\nWe can simplify the terms involving the sigmoid function. A useful identity is $1-\\sigma(x) = \\sigma(-x)$. Another is to express the terms in relation to $\\ln(1+\\exp(x))$.\nUsing $\\sigma(x) = \\frac{\\exp(x)}{1+\\exp(x)}$ and $1-\\sigma(x) = \\frac{1}{1+\\exp(x)}$, we have:\n$\\ln(\\sigma(x)) = \\ln(\\exp(x)) - \\ln(1+\\exp(x)) = x - \\ln(1+\\exp(x))$.\n$\\ln(1-\\sigma(x)) = -\\ln(1+\\exp(x))$.\nSubstituting $x = \\theta s_t$:\n$$\n\\ell(\\theta) = \\sum_{t=1}^{T} \\left[ a_t (\\theta s_t - \\ln(1+\\exp(\\theta s_t))) + (1-a_t)(-\\ln(1+\\exp(\\theta s_t))) \\right]\n$$\n$$\n\\ell(\\theta) = \\sum_{t=1}^{T} \\left[ a_t \\theta s_t - a_t \\ln(1+\\exp(\\theta s_t)) - \\ln(1+\\exp(\\theta s_t)) + a_t \\ln(1+\\exp(\\theta s_t)) \\right]\n$$\n$$\n\\ell(\\theta) = \\sum_{t=1}^{T} \\left[ a_t \\theta s_t - \\ln(1+\\exp(\\theta s_t)) \\right]\n$$\nTo proceed, we can split the summation based on the value of the state $s_t \\in \\{0, 1\\}$.\nLet $T_1 = \\{t \\mid s_t=1\\}$ and $T_0 = \\{t \\mid s_t=0\\}$.\n$$\n\\ell(\\theta) = \\sum_{t \\in T_0} \\left[ a_t \\theta (0) - \\ln(1+\\exp(\\theta \\cdot 0)) \\right] + \\sum_{t \\in T_1} \\left[ a_t \\theta (1) - \\ln(1+\\exp(\\theta \\cdot 1)) \\right]\n$$\nFor $t \\in T_0$, the term is $0 - \\ln(1+\\exp(0)) = -\\ln(2)$.\nFor $t \\in T_1$, the term is $a_t \\theta - \\ln(1+\\exp(\\theta))$.\nLet $n_1 = |T_1| = \\sum_{t=1}^{T} \\mathbf{1}\\{s_t=1\\}$ and $n_0 = |T_0| = T-n_1$. The log-likelihood becomes:\n$$\n\\ell(\\theta) = \\sum_{t \\in T_0} (-\\ln(2)) + \\sum_{t \\in T_1} (a_t \\theta - \\ln(1+\\exp(\\theta)))\n$$\n$$\n\\ell(\\theta) = -n_0 \\ln(2) + \\theta \\sum_{t \\in T_1} a_t - \\sum_{t \\in T_1} \\ln(1+\\exp(\\theta))\n$$\nThe summation $\\sum_{t \\in T_1} a_t$ is the number of times the action was $1$ when the state was $1$. This is precisely the given quantity $k_1 = \\sum_{t=1}^{T} a_t \\mathbf{1}\\{s_t=1\\}$. The second summation is over a constant term, so it equals $n_1 \\ln(1+\\exp(\\theta))$.\n$$\n\\ell(\\theta) = -n_0 \\ln(2) + k_1 \\theta - n_1 \\ln(1+\\exp(\\theta))\n$$\nTo find the value $\\hat{\\theta}$ that maximizes $\\ell(\\theta)$, we differentiate with respect to $\\theta$ and set the result to zero.\n$$\n\\frac{d\\ell}{d\\theta} = \\frac{d}{d\\theta} [-n_0 \\ln(2) + k_1 \\theta - n_1 \\ln(1+\\exp(\\theta))]\n$$\n$$\n\\frac{d\\ell}{d\\theta} = 0 + k_1 - n_1 \\frac{1}{1+\\exp(\\theta)} \\cdot \\exp(\\theta) = k_1 - n_1 \\frac{\\exp(\\theta)}{1+\\exp(\\theta)}\n$$\nSetting the derivative to zero to find the critical point $\\hat{\\theta}$:\n$$\nk_1 - n_1 \\frac{\\exp(\\hat{\\theta})}{1+\\exp(\\hat{\\theta})} = 0\n$$\n$$\nk_1 = n_1 \\frac{\\exp(\\hat{\\theta})}{1+\\exp(\\hat{\\theta})}\n$$\nGiven the assumption $n_1 \\ge 1$, we can divide by $n_1$:\n$$\n\\frac{k_1}{n_1} = \\frac{\\exp(\\hat{\\theta})}{1+\\exp(\\hat{\\theta})}\n$$\nLet $p = \\frac{k_1}{n_1}$. The equation is $p = \\frac{\\exp(\\hat{\\theta})}{1+\\exp(\\hat{\\theta})}$. We solve for $\\hat{\\theta}$:\n$$\np(1+\\exp(\\hat{\\theta})) = \\exp(\\hat{\\theta})\n$$\n$$\np + p \\exp(\\hat{\\theta}) = \\exp(\\hat{\\theta})\n$$\n$$\np = \\exp(\\hat{\\theta}) - p \\exp(\\hat{\\theta}) = \\exp(\\hat{\\theta})(1-p)\n$$\n$$\n\\frac{p}{1-p} = \\exp(\\hat{\\theta})\n$$\nTaking the natural logarithm of both sides gives the MLE $\\hat{\\theta}$:\n$$\n\\hat{\\theta} = \\ln\\left(\\frac{p}{1-p}\\right)\n$$\nSubstituting $p = k_1/n_1$ back into the expression:\n$$\n\\hat{\\theta} = \\ln\\left(\\frac{k_1/n_1}{1 - k_1/n_1}\\right) = \\ln\\left(\\frac{k_1/n_1}{(n_1-k_1)/n_1}\\right)\n$$\n$$\n\\hat{\\theta} = \\ln\\left(\\frac{k_1}{n_1-k_1}\\right)\n$$\nThe assumption $0  k_1/n_1  1$ ensures that $k_1 > 0$ and $k_1  n_1$, which means $n_1-k_1 > 0$. Thus, the argument of the logarithm is positive and finite, guaranteeing that $\\hat{\\theta}$ is a well-defined, finite real number. The second derivative of the log-likelihood is $\\frac{d^2\\ell}{d\\theta^2} = -n_1 \\sigma(\\theta)(1-\\sigma(\\theta))$, which is strictly negative for any finite $\\theta$ (since $n_1 \\ge 1$ and $\\sigma(\\theta) \\in (0,1)$), confirming that the critical point is a unique global maximum.",
            "answer": "$$\n\\boxed{\\ln\\left(\\frac{k_1}{n_1-k_1}\\right)}\n$$"
        }
    ]
}