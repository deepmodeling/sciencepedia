{
    "hands_on_practices": [
        {
            "introduction": "本练习深入探讨演化博弈论的核心，旨在阐明群体层面的结构如何从个体智能体的互动中涌现。通过分析经典的鹰鸽博弈 ，你将推导复制子动态方程，并发现攻击性与被动性策略在何种条件下能够稳定共存。这项实践对于理解复杂适应系统中社会多样性与行为多态性的涌现至关重要。",
            "id": "4113883",
            "problem": "考虑一个由个体组成的大型、均匀混合的种群，个体有两种纯策略：鹰策略（$H$）和鸽策略（$D$）。它们成对竞争一个收益为 $V>0$ 的资源。如果两只鹰相遇，由于存在受伤的可能性（代价为 $C>0$），每只鹰获得的期望收益为 $(V-C)/2$。如果一只鹰和一只鸽相遇，鹰获得 $V$，鸽获得 $0$。如果两只鸽相遇，每只鸽获得 $V/2$。令 $x \\in [0,1]$ 表示种群中鹰策略个体的比例。假设选择动力学遵循源自演化博弈论的复制子方程，其中一个策略的增长率与其相对于种群平均收益的优势成正比。\n\n从随机匹配下的期望收益和复制子动力学的核心定义出发，推导关于 $x(t)$ 的一维常微分方程，并用它计算所有的不动点。根据 $V$ 和 $C$ 显式分析每个不动点的线性稳定性。确定存在混合内部均衡的参数范围，并判断其局部稳定性。在复杂自适应系统中个体间相互作用的背景下解释你的结果，说明相互作用的收益如何塑造种群构成。\n\n你最终报告的量必须是在其存在的参数范围内，内部不动点处鹰策略个体比例 $x^{\\ast}$ 作为 $V$ 和 $C$ 函数的解析表达式。请用闭式表达式给出你的最终答案，不要包含任何单位。无需四舍五入。",
            "solution": "问题陈述已经过验证，是自洽的、在演化博弈论中有科学依据且提法恰当的。我们可以开始求解。\n\n该问题描述了鹰鸽博弈。令 $H$ 表示鹰策略，$D$ 表示鸽策略。成对竞赛的收益是已知的。我们可以将这些收益表示在一个矩阵 $A$ 中，其中条目 $A_{ij}$ 是使用策略 $i$ 的参与者对抗使用策略 $j$ 的参与者时获得的收益。令第一行/列对应鹰策略，第二行/列对应鸽策略。\n各项收益如下：\n- 鹰 vs. 鹰：每只获得 $\\frac{V-C}{2}$。因此，$A_{HH} = \\frac{V-C}{2}$。\n- 鹰 vs. 鸽：鹰获得 $V$，鸽获得 $0$。因此，$A_{HD} = V$ 且 $A_{DH} = 0$。\n- 鸽 vs. 鸽：每只获得 $\\frac{V}{2}$。因此，$A_{DD} = \\frac{V}{2}$。\n\n因此，收益矩阵为：\n$$ A = \\begin{pmatrix} \\frac{V-C}{2}  V \\\\ 0  \\frac{V}{2} \\end{pmatrix} $$\n种群由比例为 $x$ 的鹰策略个体和比例为 $1-x$ 的鸽策略个体组成，其中 $x \\in [0,1]$。我们假设随机匹配。鹰策略个体的期望收益 $E_H(x)$ 是其对抗鹰策略个体的收益乘以遇到鹰策略个体的概率（$x$），加上其对抗鸽策略个体的收益乘以遇到鸽策略个体的概率（$1-x$）：\n$$ E_H(x) = x \\cdot A_{HH} + (1-x) \\cdot A_{HD} = x \\left(\\frac{V-C}{2}\\right) + (1-x)V $$\n$$ E_H(x) = \\frac{Vx}{2} - \\frac{Cx}{2} + V - Vx = V - \\frac{Cx}{2} - \\frac{Vx}{2} $$\n类似地，鸽策略个体的期望收益 $E_D(x)$ 为：\n$$ E_D(x) = x \\cdot A_{DH} + (1-x) \\cdot A_{DD} = x(0) + (1-x)\\frac{V}{2} = \\frac{V}{2} - \\frac{Vx}{2} $$\n复制子方程描述了鹰策略个体比例 $x$ 的演化过程。一个策略的种群份额增长率与其期望收益和种群平均收益 $\\bar{E}(x)$ 之差成正比。该方程为：\n$$ \\frac{dx}{dt} = x(E_H(x) - \\bar{E}(x)) $$\n其中 $\\bar{E}(x) = x E_H(x) + (1-x) E_D(x)$。复制子方程一个更方便的形式是：\n$$ \\frac{dx}{dt} = x(1-x)(E_H(x) - E_D(x)) $$\n我们来计算期望收益的差值：\n$$ E_H(x) - E_D(x) = \\left(V - \\frac{Cx}{2} - \\frac{Vx}{2}\\right) - \\left(\\frac{V}{2} - \\frac{Vx}{2}\\right) = V - \\frac{Cx}{2} - \\frac{V}{2} = \\frac{V}{2} - \\frac{Cx}{2} = \\frac{1}{2}(V-Cx) $$\n将此代入复制子方程，得到关于 $x(t)$ 的一维常微分方程：\n$$ \\frac{dx}{dt} = \\frac{1}{2} x(1-x)(V-Cx) $$\n不动点（记为 $x^*$）是方程 $\\frac{dx}{dt} = 0$ 的解。\n$$ \\frac{1}{2} x^*(1-x^*)(V-Cx^*) = 0 $$\n该方程产生三个潜在的不动点：\n1. $x^*_1 = 0$（纯鸽策略种群）\n2. $x^*_2 = 1$（纯鹰策略种群）\n3. $x^*_3 = \\frac{V}{C}$（内部，或混合，种群）\n\n为了分析这些不动点的稳定性，我们考察常微分方程右侧函数的导数符号，记为 $f(x) = \\frac{1}{2} x(1-x)(V-Cx)$。如果 $f'(x^*)  0$，则不动点 $x^*$ 是局部稳定的；如果 $f'(x^*) > 0$，则是不稳定的。\n我们用乘法法则计算 $f'(x)$：\n$f(x) = \\frac{1}{2} (x-x^2)(V-Cx)$\n$f'(x) = \\frac{1}{2} \\left[ (1-2x)(V-Cx) + (x-x^2)(-C) \\right]$\n\n现在，我们在每个不动点处计算 $f'(x)$ 的值：\n- $x^*_1 = 0$ 的稳定性：\n$$ f'(0) = \\frac{1}{2} \\left[ (1-0)(V-0) + (0-0)(-C) \\right] = \\frac{1}{2}V $$\n由于给定 $V>0$，所以 $f'(0) > 0$。因此，不动点 $x^*_1=0$ 是**不稳定**的。这意味着如果在一个鸽策略种群中引入少数鹰策略个体，它们的比例将会增长。\n\n- $x^*_2 = 1$ 的稳定性：\n$$ f'(1) = \\frac{1}{2} \\left[ (1-2)(V-C) + (1-1)(-C) \\right] = \\frac{1}{2} (-1)(V-C) = \\frac{1}{2}(C-V) $$\n稳定性取决于 $C$ 和 $V$ 的相对大小。\n  - 如果 $V > C$，则 $C-V  0$，所以 $f'(1)  0$。不动点 $x^*_2=1$ 是**稳定**的。\n  - 如果 $C > V$，则 $C-V > 0$，所以 $f'(1) > 0$。不动点 $x^*_2=1$ 是**不稳定**的。\n\n- 内部不动点 $x^*_3 = V/C$ 的稳定性：\n这个不动点只有在区间 $(0, 1)$ 内才具有生物学意义。因为 $V>0$ 且 $C>0$，所以 $x^*_3 > 0$ 总是成立的。条件 $x^*_3  1$ 要求 $\\frac{V}{C}  1$，即 $C > V$。因此，只有在受伤代价大于资源价值的参数范围，即 $C > V$ 时，才存在混合内部均衡。\n\n我们在这个范围（$C > V$）内分析其稳定性：\n$$ f'\\left(\\frac{V}{C}\\right) = \\frac{1}{2} \\left[ \\left(1-2\\frac{V}{C}\\right)\\left(V-C\\frac{V}{C}\\right) + \\left(\\frac{V}{C}-\\left(\\frac{V}{C}\\right)^2\\right)(-C) \\right] $$\n项 $(V-C\\frac{V}{C})$ 等于 $(V-V)=0$。\n$$ f'\\left(\\frac{V}{C}\\right) = \\frac{1}{2} \\left[ 0 + \\left(\\frac{V}{C}-\\frac{V^2}{C^2}\\right)(-C) \\right] = \\frac{1}{2} \\left[ \\frac{V}{C}\\left(1-\\frac{V}{C}\\right)(-C) \\right] = -\\frac{V}{2}\\left(1-\\frac{V}{C}\\right) $$\n在该不动点存在的范围内，$C > V$，这意味着 $\\frac{V}{C}  1$，所以 $1 - \\frac{V}{C} > 0$。由于给定 $V>0$，导数表达式中除开头的负号外，所有项都是正的。\n因此，$f'(V/C)  0$。内部不动点 $x^*_3 = V/C$ 在其存在时是**局部稳定**的。\n\n在个体间相互作用背景下的解释：\n该模型展示了种群层面的结构如何从简单的局部互动规则（收益）中涌现。\n- **如果 $V > C$**：资源的收益超过了冲突的代价。攻击性行为（鹰策略）总是更有利。系统演化为一个纯鹰策略的种群（$x=1$）。这种相互作用会驱逐合作/非攻击性（鸽策略）的个体。\n- **如果 $C > V$**：冲突的代价很高。虽然纯鹰策略种群是不稳定的（一个变异的鸽策略个体会因为免于高昂的打斗代价而入侵），纯鸽策略种群也是不稳定的（一个变异的鹰策略个体可以轻易地剥削鸽策略个体）。系统自组织到一个稳定的多态状态，其中鹰和鸽共存。鹰策略个体的比例 $x^* = V/C$ 被精确地平衡，使得两种策略的期望收益相等（$E_H(x^*) = E_D(x^*)$）。这种稳定的混合状态是演化稳定状态的一个例子，这是复杂自适应系统中的一个核心概念，其中个体策略受到选择。局部的个体间互动（收益）动态地调节全局的种群构成，从而维持多样性。\n\n问题要求的是在内部不动点存在的参数范围内，鹰策略个体比例 $x^*$ 的解析表达式。这个不动点是 $x^*_3 = V/C$，它存在于 $C > V$ 的范围内。",
            "answer": "$$\\boxed{\\frac{V}{C}}$$"
        },
        {
            "introduction": "时间延迟是智能体-环境反馈回路中一个关键且常常引发不稳定的特征。本练习  提供了一个实践分析，揭示了这些延迟如何通过霍普夫分岔（Hopf bifurcation）将一个稳定系统转变为一个具有持续振荡的系统。通过对一个时滞微分方程进行稳定性分析，你将掌握一项核心技能，用于模拟从商业周期到种群动态等各种涌现的时间模式。",
            "id": "4113902",
            "problem": "考虑一个由相互作用的智能体组成的大规模群体，该群体耦合到一个共享的环境中。设群体聚合状态相对于平衡点的宏观偏差表示为 $x(t)$，环境驱动信号 $u(t)$ 由一个策略映射 $\\pi(\\cdot)$ 决定，该策略应用于对聚合状态的延迟观测，即 $u(t)=\\pi\\!\\left(x(t-\\tau)\\right)$。其中 $\\tau>0$ 是一个感知与执行延迟，源于智能体-环境以及智能体-智能体之间的信息处理延迟。假设在平衡点 $x^{\\ast}=0$ 附近，联合的智能体-环境动力学可以线性化为\n$x'(t)=\\alpha\\,x(t)+\\beta\\,u(t)$\n且策略可以线性化为\n$u(t)\\approx k\\,x(t-\\tau)$\n其中常数系数 $\\alpha,\\beta,k\\in\\mathbb{R}$ 由微观到宏观的闭合关系确定（例如，$\\alpha$ 编码了智能体的内禀弛豫，$\\beta$ 是环境到智能体的增益，而 $k=\\pi'(0)$ 是局部策略斜率）。假设该线性化有效，且相关的局部失稳机制是 Hopf 分岔，其定义为一对共轭复特征根穿过虚轴。\n\n仅从线性化的延迟耦合系统和 Hopf 分岔的定义出发，推导出一对纯虚特征根存在的必要条件，并求出使平衡点 $x^{\\ast}=0$ 通过 Hopf 分岔失稳的最小正临界延迟 $\\tau_{c}$，结果用 $\\alpha$、$\\beta$ 和 $k$ 表示。你的推导必须从第一性原理出发，通过假设指数解并分离实部和虚部来进行，并且必须确定纯虚根存在所需的任何非退化条件。\n\n然后，对于高增益负反馈的特定情况，即 $\\alpha=-0.4\\,\\text{s}^{-1}$、$\\beta=1.0\\,\\text{s}^{-1}$ 和 $k=-1.0$（使得 $u(t)\\approx -x(t-\\tau)$），计算最小正临界延迟 $\\tau_{c}$ 的数值（以秒为单位）。所有角度都以弧度表示。将最终数值答案四舍五入到 4 位有效数字，并以秒为单位报告。",
            "solution": "用户要求我解决以下问题：\n考虑一个由相互作用的智能体组成的大规模群体，该群体耦合到一个共享的环境中。设群体聚合状态相对于平衡点的宏观偏差表示为 $x(t)$，环境驱动信号 $u(t)$ 由一个策略映射 $\\pi(\\cdot)$ 决定，该策略应用于对聚合状态的延迟观测，即 $u(t)=\\pi\\!\\left(x(t-\\tau)\\right)$。其中 $\\tau>0$ 是一个感知与执行延迟，源于智能体-环境以及智能体-智能体之间的信息处理延迟。假设在平衡点 $x^{\\ast}=0$ 附近，联合的智能体-环境动力学可以线性化为\n$x'(t)=\\alpha\\,x(t)+\\beta\\,u(t)$\n且策略可以线性化为\n$u(t)\\approx k\\,x(t-\\tau)$\n其中常数系数 $\\alpha,\\beta,k\\in\\mathbb{R}$ 由微观到宏观的闭合关系确定（例如，$\\alpha$ 编码了智能体的内禀弛豫，$\\beta$ 是环境到智能体的增益，而 $k=\\pi'(0)$ 是局部策略斜率）。假设该线性化有效，且相关的局部失稳机制是 Hopf 分岔，其定义为一对共轭复特征根穿过虚轴。\n\n仅从线性化的延迟耦合系统和 Hopf 分岔的定义出发，推导出一对纯虚特征根存在的必要条件，并求出使平衡点 $x^{\\ast}=0$ 通过 Hopf 分岔失稳的最小正临界延迟 $\\tau_{c}$，结果用 $\\alpha$、$\\beta$ 和 $k$ 表示。你的推导必须从第一性原理出发，通过假设指数解并分离实部和虚部来进行，并且必须确定纯虚根存在所需的任何非退化条件。\n\n然后，对于高增益负反馈的特定情况，即 $\\alpha=-0.4\\,\\text{s}^{-1}$、$\\beta=1.0\\,\\text{s}^{-1}$ 和 $k=-1.0$（使得 $u(t)\\approx -x(t-\\tau)$），计算最小正临界延迟 $\\tau_{c}$ 的数值（以秒为单位）。所有角度都以弧度表示。将最终数值答案四舍五入到 4 位有效数字，并以秒为单位报告。\n\n所述问题在动力系统理论框架内是形式合理且良定的。该问题是对一个线性延迟微分方程的标准分析，这是许多STEM领域中一个有效且广泛使用的模型。所有给定的参数和定义都清晰、一致且有科学依据。因此，有理由着手进行求解。\n\n线性化系统由以下两个方程描述：\n$x'(t) = \\alpha\\,x(t) + \\beta\\,u(t)$\n$u(t) = k\\,x(t-\\tau)$\n\n将第二个方程代入第一个方程，得到一个关于聚合状态偏差 $x(t)$ 的单一线性延迟微分方程 (DDE)：\n$$\nx'(t) = \\alpha\\,x(t) + \\beta k\\,x(t-\\tau)\n$$\n为分析平衡点 $x(t) = 0$ 的稳定性，我们寻找形如 $x(t) = C e^{\\lambda t}$ 的解，其中 $\\lambda \\in \\mathbb{C}$ 是一个特征根，$C$ 是一个非零常数。将此试探解代入 DDE 得到：\n$$\n\\lambda C e^{\\lambda t} = \\alpha C e^{\\lambda t} + \\beta k C e^{\\lambda(t-\\tau)}\n$$\n两边同除以 $C e^{\\lambda t}$，得到关于根 $\\lambda$ 的特征方程：\n$$\n\\lambda = \\alpha + \\beta k e^{-\\lambda \\tau}\n$$\n当一个参数（此处为延迟 $\\tau$）变化时，若一对共轭复根穿过虚轴，则发生 Hopf 分岔。在分岔点，存在一个纯虚根 $\\lambda = i\\omega_c$，其中 $\\omega_c \\in \\mathbb{R}$ 且 $\\omega_c \\neq 0$。不失一般性，我们取 $\\omega_c > 0$ 来表示失稳临界点的振荡频率。\n\n将 $\\lambda = i\\omega_c$ 代入特征方程：\n$$\ni\\omega_c = \\alpha + \\beta k e^{-i\\omega_c \\tau}\n$$\n使用欧拉公式 $e^{-i\\theta} = \\cos(\\theta) - i\\sin(\\theta)$，我们展开指数项：\n$$\ni\\omega_c = \\alpha + \\beta k (\\cos(\\omega_c \\tau) - i\\sin(\\omega_c \\tau))\n$$\n$$\ni\\omega_c = (\\alpha + \\beta k \\cos(\\omega_c \\tau)) - i(\\beta k \\sin(\\omega_c \\tau))\n$$\n为使此等式成立，方程两边的实部和虚部必须相等。\n\n令实部相等：\n$$\n0 = \\alpha + \\beta k \\cos(\\omega_c \\tau) \\quad \\implies \\quad \\cos(\\omega_c \\tau) = -\\frac{\\alpha}{\\beta k}\n$$\n令虚部相等：\n$$\n\\omega_c = - \\beta k \\sin(\\omega_c \\tau) \\quad \\implies \\quad \\sin(\\omega_c \\tau) = -\\frac{\\omega_c}{\\beta k}\n$$\n为使这些方程相容，延迟 $\\tau$ 必须满足对于某个 $\\omega_c > 0$ 这两个条件同时成立。我们可以通过使用基本三角恒等式 $\\cos^2(\\theta) + \\sin^2(\\theta) = 1$（其中 $\\theta = \\omega_c \\tau$）来消去 $\\tau$：\n$$\n\\left(-\\frac{\\alpha}{\\beta k}\\right)^2 + \\left(-\\frac{\\omega_c}{\\beta k}\\right)^2 = 1\n$$\n$$\n\\frac{\\alpha^2}{(\\beta k)^2} + \\frac{\\omega_c^2}{(\\beta k)^2} = 1\n$$\n$$\n\\alpha^2 + \\omega_c^2 = (\\beta k)^2\n$$\n解出 $\\omega_c^2$：\n$$\n\\omega_c^2 = (\\beta k)^2 - \\alpha^2\n$$\n为使 Hopf 分岔发生，我们需要一个非零的实频率 $\\omega_c$，这意味着 $\\omega_c^2 > 0$。这导出了必要的非退化条件：\n$$\n(\\beta k)^2 > \\alpha^2 \\quad \\text{或等价地} \\quad |\\beta k| > |\\alpha|\n$$\n如果此条件成立，分岔点处出现的振荡频率为：\n$$\n\\omega_c = \\sqrt{(\\beta k)^2 - \\alpha^2}\n$$\n现在我们求解临界延迟 $\\tau_c$。我们有如下方程组：\n$$\n\\cos(\\omega_c \\tau) = -\\frac{\\alpha}{\\beta k} \\quad \\text{和} \\quad \\sin(\\omega_c \\tau) = -\\frac{\\omega_c}{\\beta k}\n$$\n$\\omega_c \\tau$ 的值由这两个方程确定。设 $\\theta_0 = \\arccos\\left(-\\frac{\\alpha}{\\beta k}\\right)$，其中 $\\arccos$ 的值域为 $[0, \\pi]$。这个角的正弦值非负，即 $\\sin(\\theta_0) \\geq 0$。我们必须匹配 $\\sin(\\omega_c \\tau) = - \\omega_c / (\\beta k)$ 的符号。\n\n情况1：$\\beta k  0$。在这种情况下，$-\\frac{\\omega_c}{\\beta k} > 0$。这与 $\\sin(\\theta_0)$ 的符号相匹配。因此，所有可能的角度集合为 $\\omega_c \\tau = \\theta_0 + 2n\\pi$，$n \\in \\mathbb{Z}$。最小的正延迟 $\\tau_c$ 对应于 $n=0$：\n$$\n\\tau_c = \\frac{1}{\\omega_c} \\arccos\\left(-\\frac{\\alpha}{\\beta k}\\right) \\quad \\text{当} \\quad \\beta k  0\n$$\n情况2：$\\beta k > 0$。在这种情况下，$-\\frac{\\omega_c}{\\beta k}  0$。这要求一个具有负正弦值的角度。最小的正角是 $2\\pi - \\theta_0$。因此，$\\omega_c \\tau = 2\\pi - \\theta_0 + 2n\\pi$。最小的正延迟 $\\tau_c$ 对应于 $n=0$：\n$$\n\\tau_c = \\frac{1}{\\omega_c} \\left(2\\pi - \\arccos\\left(-\\frac{\\alpha}{\\beta k}\\right)\\right) \\quad \\text{当} \\quad \\beta k > 0\n$$\n这些是最小正临界延迟的通用表达式。\n\n现在，我们将其应用于给定的特定情况：$\\alpha = -0.4\\,\\text{s}^{-1}$，$\\beta = 1.0\\,\\text{s}^{-1}$，以及 $k = -1.0$。\n\n首先，计算乘积 $\\beta k$：\n$$\n\\beta k = (1.0\\,\\text{s}^{-1})(-1.0) = -1.0\\,\\text{s}^{-1}\n$$\n由于 $\\beta k  0$，我们将使用情况1的公式。\n\n接下来，检查非退化条件 $|\\beta k| > |\\alpha|$：\n$$\n|\\beta k| = |-1.0\\,\\text{s}^{-1}| = 1.0\\,\\text{s}^{-1}\n$$\n$$\n|\\alpha| = |-0.4\\,\\text{s}^{-1}| = 0.4\\,\\text{s}^{-1}\n$$\n条件 $1.0 > 0.4$ 成立。因此 Hopf 分岔是可能发生的。\n\n计算临界频率 $\\omega_c$：\n$$\n\\omega_c = \\sqrt{(\\beta k)^2 - \\alpha^2} = \\sqrt{(-1.0)^2 - (-0.4)^2} = \\sqrt{1.0 - 0.16} = \\sqrt{0.84}\\,\\text{s}^{-1}\n$$\n现在，计算 arccosine 函数的自变量：\n$$\n-\\frac{\\alpha}{\\beta k} = -\\frac{-0.4}{-1.0} = -0.4\n$$\n使用 $\\beta k  0$ 时的 $\\tau_c$ 公式：\n$$\n\\tau_c = \\frac{1}{\\omega_c} \\arccos\\left(-\\frac{\\alpha}{\\beta k}\\right) = \\frac{1}{\\sqrt{0.84}} \\arccos(-0.4)\n$$\n我们现在计算数值。角度必须以弧度表示。\n$$\n\\arccos(-0.4) \\approx 1.98231368 \\text{ rad}\n$$\n$$\n\\sqrt{0.84} \\approx 0.916515139 \\text{ s}^{-1}\n$$\n$$\n\\tau_c \\approx \\frac{1.98231368}{0.916515139} \\text{ s} \\approx 2.1628731 \\text{ s}\n$$\n将结果四舍五入到 4 位有效数字，我们得到：\n$$\n\\tau_c \\approx 2.163 \\text{ s}\n$$",
            "answer": "$$\n\\boxed{2.163}\n$$"
        },
        {
            "introduction": "在不断变化的环境中，智能体如何学习做出最优决策？本练习  介绍了强化学习中强大的策略梯度方法，这是现代自适应智能体设计的基石。你将推导智能体预期回报的梯度，并分析一个关键的实际问题：在非平稳环境中使用过时信息（“陈旧”基线）所产生的影响，从而深入理解学习算法的稳健性。",
            "id": "4113917",
            "problem": "考虑在强化学习（RL）的背景下，两个嵌入在非平稳环境中的自适应智能体之间的单步交互。智能体 $1$（学习者）观察到一个状态 $S \\in \\{0,1\\}$，该状态从当前状态占用分布 $d(S)$ 中抽取，其中 $d(0) = \\frac{3}{5}$ 且 $d(1) = \\frac{2}{5}$。在观察到 $S=i$ 后，智能体 1 根据一个参数化策略 $\\pi_{\\theta}(A \\mid S=i)$ 选择一个二元动作 $A \\in \\{0,1\\}$。该策略由一个具有分状态 logit 的 logistic 参数化定义：动作 $A=1$ 的 logit 是 $\\theta_i$，动作 $A=0$ 的 logit 是 $0$。因此，对于每个 $i \\in \\{0,1\\}$，$\\pi_{\\theta}(A=1 \\mid S=i) = \\sigma(\\theta_i)$ 且 $\\pi_{\\theta}(A=0 \\mid S=i) = 1 - \\sigma(\\theta_i)$，其中 $\\sigma(x) = \\frac{1}{1 + \\exp(-x)}$。\n\n智能体 2（对手）独立于 $S$ 和 $A$ 选择一个二元动作 $B \\in \\{0,1\\}$，其分布随时间变化 $\\mathbb{P}(B=1) = p(t)$。环境在两个工作点之间漂移：一个旧的工作点，其 $\\mathbb{P}_{\\text{old}}(B=1) = p_{\\text{old}} = \\frac{3}{5}$；以及一个新的当前工作点，其 $\\mathbb{P}_{\\text{new}}(B=1) = p_{\\text{new}} = \\frac{7}{10}$。智能体 1 接收到的瞬时奖励为\n$$\nr(S,A,B) = w_S \\, A \\, (2B - 1),\n$$\n交互权重为 $w_0 = \\frac{3}{2}$ 和 $w_1 = 1$。没有额外的偏置项或外生奖励。\n\n智能体 1 的目标是在当前环境下最大化期望奖励，\n$$\nJ(\\theta) = \\mathbb{E}_{S \\sim d,\\, A \\sim \\pi_{\\theta}(\\cdot \\mid S),\\, B \\sim \\text{Bernoulli}(p_{\\text{new}})}\\left[ r(S,A,B) \\right].\n$$\n智能体 1 使用一个基线 $b(S)$ 来减少梯度估计器的方差。然而，当前可用的基线是过时的，是在旧的环境工作点计算的，具体如下\n$$\nb_{\\text{old}}(S=i) = \\mathbb{E}_{A \\sim \\pi_{\\theta}(\\cdot \\mid S=i),\\, B \\sim \\text{Bernoulli}(p_{\\text{old}})}\\left[ r(S=i,A,B) \\right].\n$$\n假设在当前环境下，梯度是使用似然比方法估计的，其优势为 $\\hat{A}(S,A) = r(S,A,B) - b_{\\text{old}}(S)$。仅从上述定义和标准的可微概率演算出发，推导出当前工作点的策略梯度向量 $\\nabla_{\\theta} J(\\theta)$ 的精确解析表达式，并在 $\\theta_0 = \\ln(2)$ 和 $\\theta_1 = \\ln(3)$ 处进行求值。然后，从第一性原理出发，论证在这个单步设置中使用过时的基线 $b_{\\text{old}}(S)$ 是否会在梯度估计中引入偏差，并提供偏差项向量的精确闭式表达式（如果非零）或精确的零向量（如果为零）。\n\n将你的最终答案表示为一个单行矩阵，按顺序包含梯度向量的两个分量，后跟偏差项向量的两个分量。不需要四舍五入，必须使用解析表达式或适当的有理数来呈现精确值。这个纯粹的无量纲计算不需要单位说明。",
            "solution": "用户希望我解决一个强化学习问题。\n\n### 问题验证\n\n**第 1 步：提取已知条件**\n\n*   **状态空间**：$S \\in \\{0,1\\}$\n*   **状态分布**：$d(0) = \\frac{3}{5}$， $d(1) = \\frac{2}{5}$\n*   **动作空间（智能体 1）**：$A \\in \\{0,1\\}$\n*   **策略（智能体 1）**：$\\pi_{\\theta}(A|S)$ 是一个参数化策略，参数向量为 $\\theta = (\\theta_0, \\theta_1)$。\n    *   $\\pi_{\\theta}(A=1 \\mid S=i) = \\sigma(\\theta_i) = \\frac{1}{1 + \\exp(-\\theta_i)}$\n    *   $\\pi_{\\theta}(A=0 \\mid S=i) = 1 - \\sigma(\\theta_i)$\n*   **动作空间（智能体 2）**：$B \\in \\{0,1\\}$\n*   **对手策略（旧）**：$\\mathbb{P}_{\\text{old}}(B=1) = p_{\\text{old}} = \\frac{3}{5}$\n*   **对手策略（新/当前）**：$\\mathbb{P}_{\\text{new}}(B=1) = p_{\\text{new}} = \\frac{7}{10}$\n*   **奖励函数**：$r(S,A,B) = w_S \\, A \\, (2B - 1)$\n*   **交互权重**：$w_0 = \\frac{3}{2}$， $w_1 = 1$\n*   **目标函数**：$J(\\theta) = \\mathbb{E}_{S \\sim d,\\, A \\sim \\pi_{\\theta}(\\cdot \\mid S),\\, B \\sim \\text{Bernoulli}(p_{\\text{new}})}\\left[ r(S,A,B) \\right]$\n*   **过时基线**：$b_{\\text{old}}(S=i) = \\mathbb{E}_{A \\sim \\pi_{\\theta}(\\cdot \\mid S=i),\\, B \\sim \\text{Bernoulli}(p_{\\text{old}})}\\left[ r(S=i,A,B) \\right]$\n*   **梯度估计器优势**：$\\hat{A}(S,A) = r(S,A,B) - b_{\\text{old}}(S)$\n*   **求值点**：$\\theta_0 = \\ln(2)$， $\\theta_1 = \\ln(3)$\n\n**第 2 步：使用提取的已知条件进行验证**\n\n1.  **科学性**：该问题牢固地植根于强化学习的标准理论，特别是策略梯度方法。所有概念（状态、动作、策略、奖励、目标函数、基线、非平稳性）在该领域都是标准的。\n2.  **适定性**：该问题是适定的。它要求推导和评估一个特定的策略梯度并分析偏差，为此所有必要的函数、参数和分布都已明确定义。存在一个唯一的、有意义的解。\n3.  **客观性**：该问题使用精确的数学和技术语言陈述，没有任何主观性或模糊性。\n4.  **未发现缺陷**：该问题不违反科学原理，不是比喻性的，是完整的，在理论模型的范围内是现实的，结构良好，非平凡，并且是科学可验证的。“过时”基线的使用在实际强化学习中是一种常见情况，并提出了一个关于偏差的有效理论问题。\n\n**第 3 步：结论与行动**\n\n该问题是 **有效的**。将提供解答。\n\n### 解答推导\n\n根据要求，分析分为两部分：首先，推导和评估真实策略梯度 $\\nabla_{\\theta} J(\\theta)$；其次，分析由过时基线引入的偏差。\n\n**第 1 部分：策略梯度计算**\n\n需要最大化的目标函数是在当前环境动态下的期望奖励：\n$$\nJ(\\theta) = \\mathbb{E}_{S \\sim d,\\, A \\sim \\pi_{\\theta}(\\cdot \\mid S),\\, B \\sim \\text{Bernoulli}(p_{\\text{new}})}\\left[ r(S,A,B) \\right]\n$$\n我们可以通过对所有可能性求和来明确写出这个期望：\n$$\nJ(\\theta) = \\sum_{i=0}^{1} d(i) \\sum_{a=0}^{1} \\pi_{\\theta}(a \\mid S=i) \\sum_{b=0}^{1} \\mathbb{P}_{\\text{new}}(B=b) \\, r(i,a,b)\n$$\n策略参数向量是 $\\theta = (\\theta_0, \\theta_1)$，其中 $\\theta_k$ 仅影响状态 $S=k$ 下的策略。为了找到梯度向量 $\\nabla_{\\theta} J(\\theta)$，我们计算其分量 $\\frac{\\partial J(\\theta)}{\\partial \\theta_k}$，对于 $k \\in \\{0,1\\}$。\n\n$$\n\\frac{\\partial J(\\theta)}{\\partial \\theta_k} = \\frac{\\partial}{\\partial \\theta_k} \\sum_{i=0}^{1} d(i) \\mathbb{E}_{A \\sim \\pi_{\\theta}(\\cdot|i), B \\sim \\text{new}}[r(i, A, B)]\n$$\n由于指定的参数化，导数仅影响 $i=k$ 的项：\n$$\n\\frac{\\partial J(\\theta)}{\\partial \\theta_k} = d(k) \\frac{\\partial}{\\partial \\theta_k} \\mathbb{E}_{A \\sim \\pi_{\\theta}(\\cdot|k), B \\sim \\text{new}}[r(k, A, B)]\n$$\n我们使用对数导数技巧（REINFORCE 算法的基础）将梯度移到期望内部。令 $J_k(\\theta_k) = \\mathbb{E}_{A,B}[r(k,A,B)] = \\sum_{a} \\pi_{\\theta_k}(a|k) \\mathbb{E}_{B}[r(k,a,B)]$。\n$$\n\\frac{d J_k(\\theta_k)}{d \\theta_k} = \\sum_{a} \\frac{d \\pi_{\\theta_k}(a|k)}{d \\theta_k} \\mathbb{E}_{B}[r(k,a,B)]\n$$\n使用 $\\frac{df}{dx} = f \\frac{d \\ln f}{dx}$，我们有 $\\frac{d \\pi_{\\theta_k}}{d \\theta_k} = \\pi_{\\theta_k} \\frac{d \\ln \\pi_{\\theta_k}}{d \\theta_k}$。\n$$\n\\frac{d J_k(\\theta_k)}{d \\theta_k} = \\sum_{a} \\pi_{\\theta_k}(a|k) \\left( \\frac{d \\ln \\pi_{\\theta_k}(a|k)}{d \\theta_k} \\right) \\mathbb{E}_{B}[r(k,a,B)] = \\mathbb{E}_{A \\sim \\pi_{\\theta}(\\cdot|k), B \\sim \\text{new}}\\left[ r(k,A,B) \\frac{d \\ln \\pi_{\\theta}(A|k)}{d \\theta_k} \\right]\n$$\n奖励函数是 $r(S,A,B) = w_S A (2B-1)$。当 $A=0$ 时，奖励为 $0$。因此，我们只需要考虑 $A=1$ 的情况。\n$$\n\\frac{\\partial J(\\theta)}{\\partial \\theta_k} = d(k) \\pi_{\\theta}(1|k) \\left( \\frac{d \\ln \\pi_{\\theta}(1|k)}{d \\theta_k} \\right) \\mathbb{E}_{B \\sim \\text{new}}[r(k,1,B)] + d(k) \\pi_{\\theta}(0|k) \\left( \\frac{d \\ln \\pi_{\\theta}(0|k)}{d \\theta_k} \\right) \\cdot \\mathbb{E}_{B \\sim \\text{new}}[r(k,0,B)]\n$$\n由于 $r(k,0,B) = 0$，第二项消失。\n$$\n\\frac{\\partial J(\\theta)}{\\partial \\theta_k} = d(k) \\pi_{\\theta}(1|k) \\left( \\frac{d \\ln \\pi_{\\theta}(1|k)}{d \\theta_k} \\right) \\mathbb{E}_{B \\sim \\text{new}}[w_k \\cdot 1 \\cdot (2B-1)]\n$$\n让我们评估各个组成部分。\n1.  策略是 $\\pi_{\\theta}(1|k) = \\sigma(\\theta_k)$。其导数是 $\\frac{d\\sigma(\\theta_k)}{d\\theta_k} = \\sigma(\\theta_k)(1-\\sigma(\\theta_k))$。\n2.  对数导数是 $\\frac{d \\ln \\sigma(\\theta_k)}{d \\theta_k} = \\frac{1}{\\sigma(\\theta_k)}\\frac{d \\sigma(\\theta_k)}{d \\theta_k} = 1-\\sigma(\\theta_k)$。\n3.  在新环境下，$A=1$ 的期望奖励乘数是：\n    $$\n    \\mathbb{E}_{B \\sim \\text{new}}[w_k(2B-1)] = w_k \\left( (2 \\cdot 1 - 1)p_{\\text{new}} + (2 \\cdot 0 - 1)(1-p_{\\text{new}}) \\right) = w_k(p_{\\text{new}} - (1-p_{\\text{new}})) = w_k(2p_{\\text{new}}-1)\n    $$\n将这些代入梯度分量的表达式中：\n$$\n\\frac{\\partial J(\\theta)}{\\partial \\theta_k} = d(k) \\sigma(\\theta_k) \\left(1-\\sigma(\\theta_k)\\right) w_k(2p_{\\text{new}}-1) = d(k) w_k (2p_{\\text{new}}-1) \\sigma'(\\theta_k)\n$$\n这是梯度分量的一般解析表达式。现在，我们用给定的值来计算它。\n$p_{\\text{new}} = \\frac{7}{10}$，所以 $2p_{\\text{new}}-1 = 2(\\frac{7}{10})-1 = \\frac{14}{10}-1 = \\frac{4}{10} = \\frac{2}{5}$。\n\n对于 $k=0$：\n$\\theta_0=\\ln(2)$， $d(0)=\\frac{3}{5}$， $w_0=\\frac{3}{2}$。\n$\\sigma(\\theta_0) = \\sigma(\\ln 2) = \\frac{1}{1+\\exp(-\\ln 2)} = \\frac{1}{1+1/2} = \\frac{2}{3}$。\n$\\sigma(\\theta_0)(1-\\sigma(\\theta_0)) = \\frac{2}{3}(1-\\frac{2}{3}) = \\frac{2}{3} \\cdot \\frac{1}{3} = \\frac{2}{9}$。\n$$\n\\frac{\\partial J(\\theta)}{\\partial \\theta_0} = d(0) w_0 (2p_{\\text{new}}-1) \\sigma(\\theta_0)(1-\\sigma(\\theta_0)) = \\left(\\frac{3}{5}\\right) \\left(\\frac{3}{2}\\right) \\left(\\frac{2}{5}\\right) \\left(\\frac{2}{9}\\right) = \\frac{3 \\cdot 3 \\cdot 2 \\cdot 2}{5 \\cdot 2 \\cdot 5 \\cdot 9} = \\frac{36}{450} = \\frac{2}{25}\n$$\n\n对于 $k=1$：\n$\\theta_1=\\ln(3)$， $d(1)=\\frac{2}{5}$， $w_1=1$。\n$\\sigma(\\theta_1) = \\sigma(\\ln 3) = \\frac{1}{1+\\exp(-\\ln 3)} = \\frac{1}{1+1/3} = \\frac{3}{4}$。\n$\\sigma(\\theta_1)(1-\\sigma(\\theta_1)) = \\frac{3}{4}(1-\\frac{3}{4}) = \\frac{3}{4} \\cdot \\frac{1}{4} = \\frac{3}{16}$。\n$$\n\\frac{\\partial J(\\theta)}{\\partial \\theta_1} = d(1) w_1 (2p_{\\text{new}}-1) \\sigma(\\theta_1)(1-\\sigma(\\theta_1)) = \\left(\\frac{2}{5}\\right) (1) \\left(\\frac{2}{5}\\right) \\left(\\frac{3}{16}\\right) = \\frac{2 \\cdot 1 \\cdot 2 \\cdot 3}{5 \\cdot 5 \\cdot 16} = \\frac{12}{400} = \\frac{3}{100}\n$$\n因此，在指定点的策略梯度向量是 $\\nabla_{\\theta} J(\\theta) = \\left( \\frac{2}{25}, \\frac{3}{100} \\right)$。\n\n**第 2 部分：偏差分析**\n\n策略梯度是使用似然比方法和过时的基线 $b_{\\text{old}}(S)$ 来估计的。梯度的单样本估计是：\n$$\n\\hat{g} = (r(S,A,B) - b_{\\text{old}}(S)) \\nabla_{\\theta} \\ln \\pi_{\\theta}(A \\mid S)\n$$\n该估计器的偏差是其期望（在当前环境下）与真实梯度之间的差值：\n$$\n\\text{Bias}(\\theta) = \\mathbb{E}_{S,A,B \\sim \\text{new}}[\\hat{g}] - \\nabla_{\\theta} J(\\theta)\n$$\n代入 $\\hat{g}$ 的定义：\n$$\n\\text{Bias}(\\theta) = \\mathbb{E}_{\\text{new}}[(r(S,A,B) - b_{\\text{old}}(S)) \\nabla_{\\theta} \\ln \\pi_{\\theta}(A \\mid S)] - \\nabla_{\\theta} J(\\theta)\n$$\n根据期望的线性性质：\n$$\n\\text{Bias}(\\theta) = \\mathbb{E}_{\\text{new}}[r(S,A,B) \\nabla_{\\theta} \\ln \\pi_{\\theta}(A \\mid S)] - \\mathbb{E}_{\\text{new}}[b_{\\text{old}}(S) \\nabla_{\\theta} \\ln \\pi_{\\theta}(A \\mid S)] - \\nabla_{\\theta} J(\\theta)\n$$\n根据第 1 部分推导的策略梯度定理，我们知道 $\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{\\text{new}}[r(S,A,B) \\nabla_{\\theta} \\ln \\pi_{\\theta}(A \\mid S)]$。将此代入偏差表达式中：\n$$\n\\text{Bias}(\\theta) = \\nabla_{\\theta} J(\\theta) - \\mathbb{E}_{\\text{new}}[b_{\\text{old}}(S) \\nabla_{\\theta} \\ln \\pi_{\\theta}(A \\mid S)] - \\nabla_{\\theta} J(\\theta)\n$$\n$$\n\\text{Bias}(\\theta) = - \\mathbb{E}_{S \\sim d, \\, A \\sim \\pi_{\\theta}(\\cdot|S)} [b_{\\text{old}}(S) \\nabla_{\\theta} \\ln \\pi_{\\theta}(A \\mid S)]\n$$\n请注意，由于 $b_{\\text{old}}(S)$ 和 $\\pi_{\\theta}(A|S)$ 都不依赖于 $B$，因此对 $B$ 的期望被去掉了。我们可以使用迭代期望重写它：\n$$\n\\text{Bias}(\\theta) = - \\mathbb{E}_{S \\sim d} \\left[ \\mathbb{E}_{A \\sim \\pi_{\\theta}(\\cdot|S)} [b_{\\text{old}}(S) \\nabla_{\\theta} \\ln \\pi_{\\theta}(A \\mid S)] \\right]\n$$\n关键步骤是分析内部期望。由于基线 $b_{\\text{old}}(S)$ 仅是状态 $S$ 的函数（以及参数 $\\theta$），因此它相对于对动作 $A$ 的期望是一个常数：\n$$\n\\mathbb{E}_{A \\sim \\pi_{\\theta}(\\cdot|S)} [b_{\\text{old}}(S) \\nabla_{\\theta} \\ln \\pi_{\\theta}(A \\mid S)] = b_{\\text{old}}(S) \\mathbb{E}_{A \\sim \\pi_{\\theta}(\\cdot|S)} [\\nabla_{\\theta} \\ln \\pi_{\\theta}(A \\mid S)]\n$$\n现在我们分析 $\\mathbb{E}_{A \\sim \\pi_{\\theta}(\\cdot|S)} [\\nabla_{\\theta} \\ln \\pi_{\\theta}(A \\mid S)]$ 项。根据期望的定义：\n$$\n\\mathbb{E}_{A \\sim \\pi_{\\theta}(\\cdot|S)} [\\nabla_{\\theta} \\ln \\pi_{\\theta}(A \\mid S)] = \\sum_a \\pi_{\\theta}(a|S) \\nabla_{\\theta} \\ln \\pi_{\\theta}(a|S)\n$$\n使用恒等式 $\\nabla f = f \\nabla \\ln f$：\n$$\n= \\sum_a \\nabla_{\\theta} \\pi_{\\theta}(a|S) = \\nabla_{\\theta} \\sum_a \\pi_{\\theta}(a|S)\n$$\n因为对于给定状态，所有动作的概率之和总是 $1$：\n$$\n= \\nabla_{\\theta}(1) = \\vec{0}\n$$\n这表明得分函数的期望是零向量。因此，对于任何状态 $S$，内部期望为零：\n$$\nb_{\\text{old}}(S) \\mathbb{E}_{A \\sim \\pi_{\\theta}(\\cdot|S)} [\\nabla_{\\theta} \\ln \\pi_{\\theta}(A \\mid S)] = b_{\\text{old}}(S) \\cdot \\vec{0} = \\vec{0}\n$$\n将此代回偏差 Bias$(\\theta)$ 的表达式中：\n$$\n\\text{Bias}(\\theta) = - \\mathbb{E}_{S \\sim d} [\\vec{0}] = \\vec{0}\n$$\n这证明了使用仅是状态函数 $b(S)$ 的基线不会在策略梯度估计中引入偏差。即使基线是“过时的”或次优的，这一点也成立。基线的选择影响估计器的方差，而不是其期望值。\n因此，偏差项向量是零向量 $(0, 0)$。\n\n最终答案组装：\n问题要求一个单行矩阵，其中包含梯度向量的两个分量，后跟偏差项向量的两个分量。\n- $\\theta_0$ 的梯度分量：$\\frac{2}{25}$\n- $\\theta_1$ 的梯度分量：$\\frac{3}{100}$\n- $\\theta_0$ 的偏差分量：$0$\n- $\\theta_1$ 的偏差分量：$0$\n\n最终矩阵是 $(\\frac{2}{25}, \\frac{3}{100}, 0, 0)$。",
            "answer": "$$ \\boxed{\\begin{pmatrix} \\frac{2}{25}  \\frac{3}{100}  0  0 \\end{pmatrix}} $$"
        }
    ]
}