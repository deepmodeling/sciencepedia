{
    "hands_on_practices": [
        {
            "introduction": "Before analyzing the dynamics of a Markov chain, we must first understand its fundamental structure. The state space of any chain can be partitioned into communicating classes, which reveals the long-term possibilities for the process. This exercise  hones the foundational skill of dissecting a transition graph to classify states as either transient or recurrent and to determine the periodicity of recurrent classes, which is the essential first step in predicting a system's qualitative behavior.",
            "id": "4144500",
            "problem": "Consider a discrete-time Markov chain (MC) modeling regime switching in a complex adaptive system with finite state space $\\mathcal{S} = \\{1,2,3,4,5,6,7,8\\}$. Transitions are specified by the directed graph with labeled edges indicating one-step transition probabilities. From each state, the outgoing transition probabilities sum to $1$. The transition structure is as follows:\n- From state $1$: to $1$ with probability $0.6$; to $2$ with probability $0.4$.\n- From state $2$: to $1$ with probability $0.5$; to $3$ with probability $0.5$.\n- From state $3$: to $1$ with probability $0.3$; to $2$ with probability $0.7$.\n- From state $4$: to $5$ with probability $1$.\n- From state $5$: to $4$ with probability $1$.\n- From state $6$: to $6$ with probability $1$.\n- From state $7$: to $1$ with probability $0.5$; to $4$ with probability $0.5$.\n- From state $8$: to $8$ with probability $0.2$; to $7$ with probability $0.4$; to $6$ with probability $0.4$.\n\nWork from first principles appropriate to stochastic processes and Markov chains within complex adaptive systems modeling:\n\n- Use the definition that states $i$ and $j$ communicate if both $i \\to j$ and $j \\to i$, where $i \\to j$ denotes a positive-probability path from $i$ to $j$.\n- A communicating class is a maximal set of states that all mutually communicate. A class $\\mathcal{C}$ is closed if there are no transitions from $\\mathcal{C}$ to $\\mathcal{S} \\setminus \\mathcal{C}$. States not contained in any closed communicating class are transient.\n- The period of a state $i$ is defined as $d(i) = \\gcd\\{n \\ge 1 : P^{n}(i,i)  0\\}$ (greatest common divisor (GCD) over return times with positive $n$-step return probability). For a closed communicating class, all states share the same period. A class is aperiodic if its period equals $1$.\n\nTask:\n1. Partition the state space $\\mathcal{S}$ into transient states and closed communicating classes.\n2. Determine, for each closed communicating class, whether it is aperiodic.\n3. Let $\\Theta$ be defined as the sum of the sizes of all closed communicating classes that are aperiodic, plus the number of transient states. Compute the value of $\\Theta$.\n\nYour final answer must be the single real number $\\Theta$. No rounding is required. No units are involved.",
            "solution": "The problem requires the analysis of a discrete-time Markov chain defined on the state space $\\mathcal{S} = \\{1, 2, 3, 4, 5, 6, 7, 8\\}$. The task is to partition this state space into transient states and closed communicating classes, determine the periodicity of each closed class, and compute a quantity $\\Theta$.\n\nThe solution proceeds in three steps as outlined by the problem statement.\n\n**1. Partition of the State Space**\n\nFirst, we identify the communicating classes of the Markov chain. Two states $i$ and $j$ communicate, denoted $i \\leftrightarrow j$, if there is a path with positive probability from $i$ to $j$ ($i \\to j$) and a path with positive probability from $j$ to $i$ ($j \\to i$). A communicating class is a maximal set of states where all states mutually communicate.\n\n-   **States $1, 2, 3$**:\n    -   $1 \\to 2$ ($P(1,2)=0.4  0$) and $2 \\to 1$ ($P(2,1)=0.5  0$). Thus, $1 \\leftrightarrow 2$.\n    -   $2 \\to 3$ ($P(2,3)=0.5  0$) and $3 \\to 2$ ($P(3,2)=0.7  0$). Thus, $2 \\leftrightarrow 3$.\n    -   By transitivity of communication, $1 \\leftrightarrow 3$. We verify this: $1 \\to 2 \\to 3$ establishes $1 \\to 3$, and $3 \\to 1$ is direct ($P(3,1)=0.3  0$).\n    -   Therefore, $\\{1, 2, 3\\}$ is a communicating class. Let's call it $\\mathcal{C}_1$.\n\n-   **States $4, 5$**:\n    -   $4 \\to 5$ ($P(4,5)=1  0$) and $5 \\to 4$ ($P(5,4)=1  0$).\n    -   Thus, $4 \\leftrightarrow 5$.\n    -   Therefore, $\\{4, 5\\}$ is a communicating class, which we label $\\mathcal{C}_2$.\n\n-   **State $6$**:\n    -   The only transition from state $6$ is to itself ($P(6,6)=1  0$). It cannot reach any other state, and no other state can reach it.\n    -   Thus, $\\{6\\}$ is a communicating class, labeled $\\mathcal{C}_3$.\n\n-   **State $7$**:\n    -   State $7$ can transition to state $1$ and state $4$. However, there are no paths from $\\{1,2,3\\}$ or $\\{4,5\\}$ back to $7$. So, $7$ does not communicate with any other state.\n    -   Thus, $\\{7\\}$ is a singleton communicating class.\n\n-   **State $8$**:\n    -   State $8$ can transition to states $6$, $7$, and itself. There are no paths from states $6$ or $7$ back to $8$.\n    -   Thus, $\\{8\\}$ is a singleton communicating class.\n\nThe communicating classes are $\\mathcal{C}_1=\\{1, 2, 3\\}$, $\\mathcal{C}_2=\\{4, 5\\}$, $\\mathcal{C}_3=\\{6\\}$, $\\{7\\}$, and $\\{8\\}$.\n\nNext, we identify which of these classes are closed. A class $\\mathcal{C}$ is closed if no transitions lead from a state in $\\mathcal{C}$ to any state outside of $\\mathcal{C}$.\n\n-   **Class $\\mathcal{C}_1 = \\{1, 2, 3\\}$**: All transitions from states $1$, $2$, and $3$ lead to states within $\\{1, 2, 3\\}$. This class is **closed**.\n-   **Class $\\mathcal{C}_2 = \\{4, 5\\}$**: Transitions from $4$ go to $5$, and from $5$ go to $4$. All transitions remain within $\\{4, 5\\}$. This class is **closed**.\n-   **Class $\\mathcal{C}_3 = \\{6\\}$**: The only transition from state $6$ is to itself. This class is **closed**.\n-   **Class $\\{7\\}$**: Transitions from $7$ go to $1$ and $4$, which are outside $\\{7\\}$. This class is not closed.\n-   **Class $\\{8\\}$**: Transitions from $8$ go to $6$ and $7$, which are outside $\\{8\\}$. This class is not closed.\n\nThe closed communicating classes are $\\mathcal{C}_1$, $\\mathcal{C}_2$, and $\\mathcal{C}_3$. The states contained in these classes, $\\{1, 2, 3, 4, 5, 6\\}$, are recurrent.\n\nAccording to the provided definition, states not contained in any closed communicating class are transient. The states not in $\\mathcal{C}_1 \\cup \\mathcal{C}_2 \\cup \\mathcal{C}_3 = \\{1, 2, 3, 4, 5, 6\\}$ are $7$ and $8$.\nThus, the set of transient states is $T=\\{7, 8\\}$.\n\n**2. Periodicity of Closed Communicating Classes**\n\nThe period $d(i)$ of a state $i$ is the greatest common divisor (GCD) of the lengths of all possible paths from $i$ back to $i$. All states in a communicating class share the same period. A class is aperiodic if its period is $1$.\n\n-   **Class $\\mathcal{C}_1=\\{1, 2, 3\\}$**:\n    -   We examine the period of state $1$. A return to state $1$ is possible in $n=1$ step, since $P(1,1)=0.6  0$.\n    -   The set of possible return times is $\\{n \\ge 1 : P^{n}(1,1)  0\\}$. Since $1$ is in this set, the GCD must be $1$.\n    -   $d(1) = \\gcd(1, \\dots) = 1$.\n    -   Therefore, the class $\\mathcal{C}_1$ is **aperiodic**.\n\n-   **Class $\\mathcal{C}_2=\\{4, 5\\}$**:\n    -   Let's find the period of state $4$.\n    -   To return to state $4$, one must follow the path $4 \\to 5 \\to 4$. The length of this path is $n=2$. $P^2(4,4) = P(4,5)P(5,4) = 1 \\times 1 = 1  0$.\n    -   There is no self-loop, so a return in $n=1$ step is impossible.\n    -   Any path from $4$ to $4$ must consist of a sequence of $4 \\to 5 \\to 4$ cycles. The lengths of such paths are $2, 4, 6, \\dots$.\n    -   The set of return times is $\\{2, 4, 6, \\dots\\}$.\n    -   $d(4) = \\gcd(\\{2, 4, 6, \\dots\\}) = 2$.\n    -   Therefore, the class $\\mathcal{C}_2$ is **periodic** with period $2$.\n\n-   **Class $\\mathcal{C}_3=\\{6\\}$**:\n    -   For state $6$, there is a self-loop with $P(6,6)=1  0$. A return in $n=1$ step is possible.\n    -   $d(6) = \\gcd(1, \\dots) = 1$.\n    -   Therefore, the class $\\mathcal{C}_3$ is **aperiodic**.\n\n**3. Computation of $\\Theta$**\n\nThe value $\\Theta$ is defined as the sum of the sizes of all aperiodic closed communicating classes, plus the number of transient states.\n\n-   The aperiodic closed communicating classes are $\\mathcal{C}_1=\\{1, 2, 3\\}$ and $\\mathcal{C}_3=\\{6\\}$.\n-   The size of $\\mathcal{C}_1$ is $|\\mathcal{C}_1| = 3$.\n-   The size of $\\mathcal{C}_3$ is $|\\mathcal{C}_3| = 1$.\n-   The sum of the sizes of these classes is $3 + 1 = 4$.\n\n-   The set of transient states is $T=\\{7, 8\\}$.\n-   The number of transient states is $|T| = 2$.\n\n-   The value of $\\Theta$ is the sum of these two quantities:\n    $$ \\Theta = (|\\mathcal{C}_1| + |\\mathcal{C}_3|) + |T| $$\n    $$ \\Theta = (3 + 1) + 2 $$\n    $$ \\Theta = 4 + 2 = 6 $$\n\nThe final value is $6$.",
            "answer": "$$\\boxed{6}$$"
        },
        {
            "introduction": "Once the structure of a Markov chain is understood, we can ask quantitative questions about the system's trajectory, such as the likelihood of reaching a desirable \"lock-in\" state before an undesirable one. Such problems can be elegantly solved using first-step analysis, a powerful recursive method that formulates a system of linear equations for the desired probabilities. This practice  develops your ability to apply this technique, transforming a complex probabilistic question about an infinite-horizon path into a tractable algebraic problem.",
            "id": "4144532",
            "problem": "In a coarse-grained model of coordination in a complex adaptive system, a large population of agents adaptively updates behavior through local interactions and occasional lock-in events. The macrostate of the population is modeled as a time-homogeneous Markov chain on the finite state space $\\{0,1,2,3,4,5\\}$. States $4$ and $5$ represent lock-in to behavior $\\mathcal{A}$ and lock-in to behavior $\\mathcal{B}$, respectively. The transition dynamics for one update step are as follows:\n- From state $0$: transition to state $1$ with probability $\\frac{1}{3}$, to state $2$ with probability $\\frac{1}{3}$, and to state $5$ with probability $\\frac{1}{3}$.\n- From state $1$: transition to state $0$ with probability $\\frac{1}{4}$, to state $2$ with probability $\\frac{1}{4}$, to state $3$ with probability $\\frac{1}{4}$, and to state $4$ with probability $\\frac{1}{4}$.\n- From state $2$: transition to state $1$ with probability $\\frac{1}{2}$, to state $3$ with probability $\\frac{1}{4}$, and to state $5$ with probability $\\frac{1}{4}$.\n- From state $3$: transition to state $2$ with probability $\\frac{1}{3}$, to state $4$ with probability $\\frac{1}{3}$, and to state $5$ with probability $\\frac{1}{3}$.\n- State $4$ is absorbing, i.e., it transitions to state $4$ with probability $1$.\n- State $5$ is absorbing, i.e., it transitions to state $5$ with probability $1$.\n\nLet $\\mathcal{A}=\\{4\\}$ and $\\mathcal{B}=\\{5\\}$. Define the hitting times $T_{\\mathcal{A}}=\\inf\\{t\\ge 0: X_t\\in \\mathcal{A}\\}$ and $T_{\\mathcal{B}}=\\inf\\{t\\ge 0: X_t\\in \\mathcal{B}\\}$, where $\\{X_t\\}_{t\\ge 0}$ is the Markov chain. Using the fundamental definitions of a time-homogeneous Markov chain and the first-step analysis based on the law of total expectation, derive the linear system characterizing the function $h(i)=\\mathbb{P}_i(T_{\\mathcal{A}}T_{\\mathcal{B}})$ on the state space with appropriate boundary conditions, justify why the solution is unique on this finite state space with absorbing boundary conditions, and solve explicitly for $h(0)$.\n\nAnswer form requirement: Express the final probability $h(0)$ exactly as a reduced fraction. Do not round.",
            "solution": "The problem asks for the probability that a time-homogeneous Markov chain enters the absorbing state $4$ before it enters the absorbing state $5$, given that it starts in state $0$. Let the state space be $S = \\{0, 1, 2, 3, 4, 5\\}$. The target sets are $\\mathcal{A}=\\{4\\}$ and $\\mathcal{B}=\\{5\\}$. The quantity to be determined is $h(i) = \\mathbb{P}_i(T_{\\mathcal{A}}  T_{\\mathcal{B}})$, where $T_{\\mathcal{A}} = \\inf\\{t \\ge 0 : X_t \\in \\mathcal{A}\\}$ and $T_{\\mathcal{B}} = \\inf\\{t \\ge 0 : X_t \\in \\mathcal{B}\\}$ are the first hitting times of the sets $\\mathcal{A}$ and $\\mathcal{B}$, respectively, and $\\mathbb{P}_i(\\cdot) = \\mathbb{P}(\\cdot | X_0 = i)$.\n\nThe states $4$ and $5$ are absorbing, while the states $S_T = \\{0, 1, 2, 3\\}$ are transient. The function $h(i)$ represents the probability of absorption into set $\\mathcal{A}$ before set $\\mathcal{B}$, starting from state $i$.\n\nFirst, we establish the boundary conditions for the absorbing states.\nIf the process starts in state $i=4$, it is already in set $\\mathcal{A}$ at time $t=0$. Thus, $T_{\\mathcal{A}} = 0$. Since state $4 \\notin \\mathcal{B}$, the process is not in $\\mathcal{B}$ at $t=0$, so $T_{\\mathcal{B}}  0$. Therefore, the condition $T_{\\mathcal{A}}  T_{\\mathcal{B}}$ is met with certainty. The probability is $1$.\n$$h(4) = \\mathbb{P}_4(T_{\\mathcal{A}}  T_{\\mathcal{B}}) = 1$$\nIf the process starts in state $i=5$, it is already in set $\\mathcal{B}$ at time $t=0$. Thus, $T_{\\mathcal{B}} = 0$. Since state $5 \\notin \\mathcal{A}$, $T_{\\mathcal{A}}  0$. The condition $T_{\\mathcal{A}}  T_{\\mathcal{B}}$ is not met. The probability is $0$.\n$$h(5) = \\mathbb{P}_5(T_{\\mathcal{A}}  T_{\\mathcal{B}}) = 0$$\n\nFor the transient states $i \\in S_T = \\{0, 1, 2, 3\\}$, we use a first-step analysis. By applying the law of total expectation and conditioning on the state at time $t=1$, we get:\n$$h(i) = \\mathbb{E}_i[\\mathbb{I}(T_{\\mathcal{A}}  T_{\\mathcal{B}})] = \\sum_{j \\in S} \\mathbb{P}(X_1=j | X_0=i) \\mathbb{P}(T_{\\mathcal{A}}  T_{\\mathcal{B}} | X_1=j)$$\nwhere $\\mathbb{I}(\\cdot)$ is the indicator function. Due to the time-homogeneous Markov property, $\\mathbb{P}(T_{\\mathcal{A}}  T_{\\mathcal{B}} | X_1=j) = \\mathbb{P}_j(T_{\\mathcal{A}}  T_{\\mathcal{B}}) = h(j)$.\nThus, the system of equations is given by $h(i) = \\sum_{j \\in S} p_{ij} h(j)$ for $i \\in \\{0, 1, 2, 3\\}$, where $p_{ij}$ are the given transition probabilities.\n\nLet's write out the system explicitly:\nFor $i=0$:\n$h(0) = p_{01}h(1) + p_{02}h(2) + p_{05}h(5) = \\frac{1}{3}h(1) + \\frac{1}{3}h(2) + \\frac{1}{3}(0)$\n$$3h(0) = h(1) + h(2) \\quad (1)$$\nFor $i=1$:\n$h(1) = p_{10}h(0) + p_{12}h(2) + p_{13}h(3) + p_{14}h(4) = \\frac{1}{4}h(0) + \\frac{1}{4}h(2) + \\frac{1}{4}h(3) + \\frac{1}{4}(1)$\n$$4h(1) = h(0) + h(2) + h(3) + 1 \\quad (2)$$\nFor $i=2$:\n$h(2) = p_{21}h(1) + p_{23}h(3) + p_{25}h(5) = \\frac{1}{2}h(1) + \\frac{1}{4}h(3) + \\frac{1}{4}(0)$\n$$4h(2) = 2h(1) + h(3) \\quad (3)$$\nFor $i=3$:\n$h(3) = p_{32}h(2) + p_{34}h(4) + p_{35}h(5) = \\frac{1}{3}h(2) + \\frac{1}{3}(1) + \\frac{1}{3}(0)$\n$$3h(3) = h(2) + 1 \\quad (4)$$\n\nThis is a system of 4 linear equations for the 4 unknowns $h(0)$, $h(1)$, $h(2)$, and $h(3)$. Such a system for absorption probabilities in a finite-state absorbing Markov chain has a unique solution. This is because from any transient state, there is a non-zero probability of reaching an absorbing state, which ensures that the matrix $(I-Q)$ (where $Q$ is the submatrix of transition probabilities between transient states) is invertible.\n\nWe now solve this system. We use substitution to express $h(1)$, $h(2)$, and $h(3)$ in terms of one variable, and ultimately find $h(0)$.\nFrom equation $(4)$, we express $h(3)$ in terms of $h(2)$:\n$$h(3) = \\frac{1}{3}h(2) + \\frac{1}{3}$$\nSubstitute this expression for $h(3)$ into equation $(3)$:\n$4h(2) = 2h(1) + \\left(\\frac{1}{3}h(2) + \\frac{1}{3}\\right)$\n$4h(2) - \\frac{1}{3}h(2) = 2h(1) + \\frac{1}{3}$\n$\\frac{11}{3}h(2) = 2h(1) + \\frac{1}{3}$\nMultiplying by $3$ gives $11h(2) = 6h(1) + 1$, from which we express $h(1)$ in terms of $h(2)$:\n$$h(1) = \\frac{11h(2) - 1}{6}$$\nNow, substitute the expressions for $h(1)$ and $h(3)$ into equation $(2)$:\n$4\\left(\\frac{11h(2) - 1}{6}\\right) = h(0) + h(2) + \\left(\\frac{1}{3}h(2) + \\frac{1}{3}\\right) + 1$\n$\\frac{2(11h(2) - 1)}{3} = h(0) + \\frac{4}{3}h(2) + \\frac{4}{3}$\nMultiply by $3$:\n$22h(2) - 2 = 3h(0) + 4h(2) + 4$\n$18h(2) - 6 = 3h(0)$\n$$h(0) = 6h(2) - 2$$\nFinally, we substitute the expressions for $h(0)$ and $h(1)$ into equation $(1)$:\n$3(6h(2) - 2) = \\left(\\frac{11h(2) - 1}{6}\\right) + h(2)$\n$18h(2) - 6 = \\frac{11h(2) - 1 + 6h(2)}{6}$\n$6(18h(2) - 6) = 17h(2) - 1$\n$108h(2) - 36 = 17h(2) - 1$\n$108h(2) - 17h(2) = 36 - 1$\n$91h(2) = 35$\n$$h(2) = \\frac{35}{91} = \\frac{5 \\times 7}{13 \\times 7} = \\frac{5}{13}$$\nNow that we have the value for $h(2)$, we can find the required value of $h(0)$:\n$h(0) = 6h(2) - 2 = 6\\left(\\frac{5}{13}\\right) - 2 = \\frac{30}{13} - \\frac{26}{13}$\n$$h(0) = \\frac{4}{13}$$\nThe probability of being absorbed into state $4$ before state $5$, starting from state $0$, is $\\frac{4}{13}$.",
            "answer": "$$\\boxed{\\frac{4}{13}}$$"
        },
        {
            "introduction": "Beyond analyzing transient paths, a central goal in modeling complex systems is to understand their long-term equilibrium behavior. For many Markov chains, the probability distribution over states converges to a unique stationary distribution, representing the long-run proportion of time the system spends in each state. This computational exercise  bridges theory and practice by tasking you with finding this stationary distribution as an eigenvector and numerically verifying the convergence, providing direct insight into how different structural properties of a chain, like aperiodicity or slow mixing, manifest in its dynamic behavior.",
            "id": "4144541",
            "problem": "Consider a discrete-time finite-state Markov chain (MC) with state space of size $m$ and a row-stochastic transition matrix $P \\in \\mathbb{R}^{m \\times m}$, where each row contains nonnegative entries that sum to $1$. A probability distribution over states is represented as a row vector $\\mu \\in \\mathbb{R}^{1 \\times m}$ with nonnegative entries summing to $1$. The one-step evolution of the distribution is given by the fundamental law of Markov dynamics: the next-step distribution equals the current distribution propagated by the transition mechanism. In complex adaptive systems modeling, the asymptotic behavior of such stochastic processes under repeated interactions captures long-run aggregate behavior, and an invariant probability distribution that remains unchanged under one step of the dynamics, while conserving total probability mass, plays a central role.\n\nYour task is to write a program that, for each provided test case:\n- Computes a probability vector that is invariant under one step of the transition dynamics and is properly normalized.\n- Propagates a given initial distribution forward by a specified number of steps and assesses whether the propagated distribution is sufficiently close to the invariant vector in the total variation norm (equal to half of the $\\ell_{1}$ norm) up to a prescribed tolerance. Because the total variation norm is half the $\\ell_{1}$ norm, it suffices to check the $\\ell_{1}$ distance directly for the given tolerance.\n\nUse the following foundational base:\n- The distribution evolves by repeated application of the transition mechanism.\n- An invariant distribution is unchanged by one step of the dynamics and must be a properly normalized probability vector.\n- For an irreducible and aperiodic transition mechanism on a finite state space, there exists a unique invariant distribution, and the evolved distribution converges to it as the number of steps grows; for periodic mechanisms, convergence of the evolved distribution need not hold.\n\nFor each test case below, your program must:\n1. Compute an invariant probability vector consistent with the transition mechanism and normalization.\n2. Compute the evolved distribution after the specified number of steps starting from the given initial distribution.\n3. Compute the $\\ell_{1}$ distance between the evolved distribution and the invariant distribution.\n4. Return a boolean indicating whether the evolved distribution is within the specified tolerance in $\\ell_{1}$ distance to the invariant distribution.\n\nExpress all numerical outputs as floats rounded to six decimal places. The final output for each test case must be a list whose first element is the invariant distribution (as a list of floats rounded to six decimal places) and whose second element is a boolean indicating whether convergence within tolerance was observed.\n\nTest suite:\n- Test case $1$ (irreducible and aperiodic, $m=4$):\n  - Transition matrix $P_1$:\n    $$\n    P_1=\\begin{bmatrix}\n    0.3  0.2  0.4  0.1 \\\\\n    0.1  0.5  0.2  0.2 \\\\\n    0.25  0.25  0.25  0.25 \\\\\n    0.4  0.2  0.1  0.3\n    \\end{bmatrix}\n    $$\n  - Initial distribution $\\mu_1 = [0.05,\\,0.45,\\,0.25,\\,0.25]$.\n  - Number of steps $n_1 = 50$.\n  - Tolerance $\\varepsilon_1 = 10^{-6}$.\n- Test case $2$ (irreducible but periodic, $m=2$):\n  - Transition matrix $P_2$:\n    $$\n    P_2=\\begin{bmatrix}\n    0  1 \\\\\n    1  0\n    \\end{bmatrix}\n    $$\n  - Initial distribution $\\mu_2 = [1,\\,0]$.\n  - Number of steps $n_2 = 50$.\n  - Tolerance $\\varepsilon_2 = 10^{-6}$.\n- Test case $3$ (slow mixing but aperiodic, $m=2$):\n  - Transition matrix $P_3$:\n    $$\n    P_3=\\begin{bmatrix}\n    0.001  0.999 \\\\\n    0.999  0.001\n    \\end{bmatrix}\n    $$\n  - Initial distribution $\\mu_3 = [1,\\,0]$.\n  - Number of steps $n_3 = 3000$.\n  - Tolerance $\\varepsilon_3 = 10^{-2}$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element corresponds to one test case in order and must itself be a list of the form described above. For example, the output should look like\n$$\n[\\,[\\text{invariant\\_1},\\text{boolean\\_1}],\\,[\\text{invariant\\_2},\\text{boolean\\_2}],\\,[\\text{invariant\\_3},\\text{boolean\\_3}]\\,]\n$$\nwith no spaces in the printed line.",
            "solution": "The problem requires the analysis of several discrete-time finite-state Markov chains. For each given test case, we must compute the invariant probability distribution, simulate the evolution of a given initial distribution over a specified number of steps, and assess its convergence to the invariant distribution. The solution is derived from the fundamental principles of Markov chain theory.\n\n### Step 1: Computation of the Invariant Distribution ($\\pi$)\n\nAn invariant probability distribution, often denoted by the row vector $\\pi \\in \\mathbb{R}^{1 \\times m}$, is a distribution that remains unchanged after one step of the Markovian dynamics. Mathematically, it satisfies the equation:\n$$\n\\pi P = \\pi\n$$\nwhere $P \\in \\mathbb{R}^{m \\times m}$ is the row-stochastic transition matrix. Additionally, $\\pi$ must be a valid probability distribution, meaning its components $\\pi_i$ are non-negative and sum to $1$:\n$$\n\\sum_{i=1}^{m} \\pi_i = 1 \\quad \\text{and} \\quad \\pi_i \\ge 0 \\text{ for all } i \\in \\{1, \\dots, m\\}\n$$\nThe equation $\\pi P = \\pi$ can be rewritten as $\\pi (P - I) = 0$, where $I$ is the $m \\times m$ identity matrix. This indicates that $\\pi$ is a left eigenvector of the matrix $P$ corresponding to the eigenvalue $\\lambda = 1$.\n\nFrom linear algebra, the left eigenvectors of a matrix $P$ are the transposes of the right eigenvectors of its transpose, $P^T$. Let $v = \\pi^T$ be the column vector corresponding to $\\pi$. Taking the transpose of the eigenvector equation gives:\n$$\n(\\pi P)^T = \\pi^T \\implies P^T \\pi^T = \\pi^T \\implies P^T v = v\n$$\nThis shows that $v = \\pi^T$ is a right eigenvector of $P^T$ associated with the eigenvalue $\\lambda = 1$. For any irreducible finite-state Markov chain, the Perron-Frobenius theorem guarantees that the eigenvalue $1$ is simple and there exists a unique (up to scaling) corresponding eigenvector whose components are all positive.\n\nThe computational procedure is as follows:\n1.  Compute the transpose of the transition matrix, $P^T$.\n2.  Find the eigenvalues and right eigenvectors of $P^T$.\n3.  Identify the eigenvector $v$ corresponding to the eigenvalue $\\lambda = 1$.\n4.  Normalize this eigenvector to ensure its components sum to $1$. The resulting vector is $\\pi^T$. Specifically, if $v = [v_1, v_2, \\dots, v_m]^T$, the components of $\\pi$ are given by $\\pi_i = v_i / \\sum_{j=1}^{m} v_j$.\n5.  The resulting row vector $\\pi$ is the required invariant distribution.\n\n### Step 2: Evolution of the State Distribution\n\nThe probability distribution $\\mu_t$ at time step $t$ evolves to the distribution $\\mu_{t+1}$ at the next step via multiplication by the transition matrix:\n$$\n\\mu_{t+1} = \\mu_t P\n$$\nStarting from an initial distribution $\\mu_0$, the distribution after $n$ steps, $\\mu_n$, is found by repeatedly applying this rule:\n$$\n\\mu_n = \\mu_0 P^n\n$$\nwhere $P^n$ is the $n$-th power of the matrix $P$. This can be computed efficiently using numerical methods such as exponentiation by squaring, which is implemented in standard scientific computing libraries.\n\n### Step 3: Convergence Assessment\n\nThe problem requires us to determine if the evolved distribution $\\mu_n$ is \"sufficiently close\" to the invariant distribution $\\pi$. This is quantified using the $\\ell_1$ distance between the two vectors:\n$$\n\\|\\mu_n - \\pi\\|_1 = \\sum_{i=1}^{m} |\\mu_{n,i} - \\pi_i|\n$$\nThe problem specifies a tolerance $\\varepsilon$ for each test case. The evolved distribution is considered to have converged if this distance is within the tolerance:\n$$\n\\|\\mu_n - \\pi\\|_1 \\le \\varepsilon\n$$\nThis comparison results in a boolean value (`True` or `False`).\n\nFor an irreducible and aperiodic Markov chain on a finite state space, the distribution $\\mu_n$ is guaranteed to converge to the unique invariant distribution $\\pi$ as $n \\to \\infty$, regardless of the initial distribution $\\mu_0$. However, if the chain is periodic (as in Test Case $2$), convergence of $\\mu_n$ to $\\pi$ is not guaranteed; the distribution may instead cycle through a set of distributions.\n\n### Summary of the Algorithm\nFor each test case defined by $(P, \\mu_0, n, \\varepsilon)$:\n1.  Compute the invariant distribution $\\pi$ by finding the normalized left eigenvector of $P$ for the eigenvalue $\\lambda=1$. This is done by finding the right eigenvector of $P^T$ for $\\lambda=1$.\n2.  Compute the evolved distribution $\\mu_n = \\mu_0 P^n$ using matrix exponentiation.\n3.  Calculate the $\\ell_1$ distance $\\|\\mu_n - \\pi\\|_1$.\n4.  Compare this distance to the tolerance $\\varepsilon$ to obtain a boolean result.\n5.  Format the components of $\\pi$ by rounding to six decimal places.\n6.  Assemble the final output for the test case as a list containing the rounded invariant distribution and the boolean convergence result.\n\nThis procedure is systematically applied to all provided test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the Markov chain problems as described.\n    \"\"\"\n\n    def solve_case(P, mu0, n, tol):\n        \"\"\"\n        Solves a single test case for a Markov chain.\n\n        Args:\n            P (np.ndarray): The m x m transition matrix.\n            mu0 (np.ndarray): The 1 x m initial distribution row vector.\n            n (int): The number of steps to evolve the distribution.\n            tol (float): The tolerance for the L1 distance convergence check.\n\n        Returns:\n            list: A list containing [invariant_distribution, is_converged].\n                  The invariant_distribution is a list of floats rounded to 6 decimal places.\n                  is_converged is a boolean.\n        \"\"\"\n        m = P.shape[0]\n\n        # 1. Compute the invariant distribution pi\n        # This corresponds to the left eigenvector of P for eigenvalue 1.\n        # We find the right eigenvector of P.T for eigenvalue 1.\n        P_T = P.T\n        eigenvalues, eigenvectors = np.linalg.eig(P_T)\n\n        # Find the index of the eigenvalue that is close to 1\n        one_idx = np.isclose(eigenvalues, 1)\n\n        # Extract the corresponding eigenvector (it's a column in eigenvectors)\n        stationary_vec_T = eigenvectors[:, one_idx]\n\n        # Eigenvector can be complex, take the real part. For a stochastic matrix,\n        # the eigenvector for eigenvalue 1 can be chosen to be real-valued.\n        stationary_vec_T = np.real(stationary_vec_T)\n\n        # Normalize the eigenvector so that its components sum to 1\n        pi_T = stationary_vec_T / np.sum(stationary_vec_T)\n\n        # pi is a row vector, so we transpose and flatten to a 1D array\n        pi = pi_T.T.flatten()\n\n        # 2. Compute the evolved distribution mu_n = mu_0 * P^n\n        P_n = np.linalg.matrix_power(P, n)\n        mu_n = mu0 @ P_n\n\n        # 3. Compute the L1 distance between mu_n and pi\n        l1_dist = np.sum(np.abs(mu_n - pi))\n\n        # 4. Return boolean indicating if the distance is within tolerance\n        is_converged = l1_dist = tol\n\n        # Format the invariant distribution as a list of floats rounded to six decimal places\n        pi_rounded = [round(x, 6) for x in pi]\n\n        return [pi_rounded, is_converged]\n\n    # Test case 1: Irreducible and aperiodic\n    P1 = np.array([\n        [0.3, 0.2, 0.4, 0.1],\n        [0.1, 0.5, 0.2, 0.2],\n        [0.25, 0.25, 0.25, 0.25],\n        [0.4, 0.2, 0.1, 0.3]\n    ])\n    mu1 = np.array([0.05, 0.45, 0.25, 0.25])\n    n1 = 50\n    tol1 = 1e-6\n\n    # Test case 2: Irreducible but periodic\n    P2 = np.array([\n        [0.0, 1.0],\n        [1.0, 0.0]\n    ])\n    mu2 = np.array([1.0, 0.0])\n    n2 = 50\n    tol2 = 1e-6\n\n    # Test case 3: Slow mixing but aperiodic\n    P3 = np.array([\n        [0.001, 0.999],\n        [0.999, 0.001]\n    ])\n    mu3 = np.array([1.0, 0.0])\n    n3 = 3000\n    tol3 = 1e-2\n    \n    test_cases = [\n        (P1, mu1, n1, tol1),\n        (P2, mu2, n2, tol2),\n        (P3, mu3, n3, tol3),\n    ]\n\n    results = []\n    for P, mu0, n, tol in test_cases:\n        result = solve_case(P, mu0, n, tol)\n        results.append(result)\n\n    # Final print statement must be a single line with no spaces\n    # Example: [[list1,bool1],[list2,bool2]]\n    final_output_str = str(results).replace(\" \", \"\")\n    print(final_output_str)\n\nsolve()\n```"
        }
    ]
}