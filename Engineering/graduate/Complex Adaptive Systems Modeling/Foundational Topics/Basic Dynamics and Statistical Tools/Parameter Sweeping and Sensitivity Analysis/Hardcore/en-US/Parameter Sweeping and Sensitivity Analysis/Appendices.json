{
    "hands_on_practices": [
        {
            "introduction": "Many complex adaptive system models are inherently stochastic, meaning a single simulation run provides only one realization from a distribution of possible outcomes. Before analyzing sensitivity across a parameter space, we must first be confident in our estimate of the system's average behavior at a single point. This exercise  provides the fundamental statistical tool for this task, guiding you to derive the number of replications required to guarantee your results achieve a specified level of precision and are not merely an artifact of random noise.",
            "id": "4135741",
            "problem": "Consider a stochastic Agent-Based Model (ABM) in which, for a fixed control parameter $\\theta$ in a parameter sweep domain $\\Theta$, the scalar outcome $Y$ is a random variable due to stochastic agent interactions and random initial conditions. In sensitivity analysis, the quantity of interest is the conditional expectation $\\mathbb{E}[Y \\mid \\theta]$, which is used to measure how system-level outcomes change with $\\theta$. Explain, from first principles of probability and sampling, why multiple independent replications are necessary to estimate $\\mathbb{E}[Y \\mid \\theta]$ when the model is stochastic. Then, suppose you run $R$ independent replications at a given $\\theta$, yielding independent and identically distributed samples $Y_{1}(\\theta), \\dots, Y_{R}(\\theta)$ with conditional mean $\\mu(\\theta) = \\mathbb{E}[Y \\mid \\theta]$ and conditional variance $\\sigma^{2}(\\theta) = \\operatorname{Var}(Y \\mid \\theta)$. Using a two-sided confidence interval for $\\mu(\\theta)$ based on the normal approximation justified by the Central Limit Theorem for large $R$, derive the minimal number of replications $R(\\theta)$ required so that the half-width (margin of error) of the confidence interval is at most a prescribed tolerance $\\epsilon > 0$ with confidence level $1 - \\alpha$. Let $z_{p}$ denote the $p$-quantile of the standard normal distribution. Express your final answer as a single closed-form analytic expression in terms of $\\epsilon$, $\\alpha$, and $\\sigma^{2}(\\theta)$. No numerical evaluation is required. If your expression involves a non-integer value of $R(\\theta)$, return the minimal integer satisfying the requirement. Your final answer must be a calculation and contain no units inside the box.",
            "solution": "The problem is valid as it is scientifically grounded in probability theory and statistics, well-posed with a clear objective, and formally expressed. It directly pertains to standard methodologies in the sensitivity analysis of stochastic models, a core topic in complex adaptive systems modeling.\n\nFirst, we address the necessity of multiple independent replications for estimating the conditional expectation $\\mathbb{E}[Y \\mid \\theta]$ in a stochastic model. The quantity of interest, $\\mu(\\theta) = \\mathbb{E}[Y \\mid \\theta]$, represents the true mean of the distribution of the scalar outcome $Y$, given a fixed value of the control parameter $\\theta$. A single run or replication of the stochastic Agent-Based Model (ABM) produces a single realization, say $y_1(\\theta)$, which is a random draw from the underlying probability distribution of $Y \\mid \\theta$. This single value $y_1(\\theta)$ is an unbiased estimate of $\\mu(\\theta)$, since $\\mathbb{E}[Y_1(\\theta)] = \\mu(\\theta)$. However, it is a very noisy estimate, as its variance is the full conditional variance of the process, $\\operatorname{Var}(Y_1(\\theta)) = \\sigma^2(\\theta)$. A single sample provides no information about the spread or central tendency of the distribution; it is just one point within that distribution. Relying on a single replication would be akin to estimating the average height of a population by measuring only one person.\n\nTo obtain a more reliable and precise estimate of the true mean $\\mu(\\theta)$, we must perform multiple, independent replications. Let the outcomes of $R$ independent replications be $Y_1(\\theta), Y_2(\\theta), \\dots, Y_R(\\theta)$. These are independent and identically distributed (i.i.d.) random variables, each with mean $\\mu(\\theta)$ and variance $\\sigma^2(\\theta)$. The standard estimator for the population mean $\\mu(\\theta)$ is the sample mean, defined as:\n$$\n\\hat{\\mu}_R(\\theta) = \\frac{1}{R} \\sum_{i=1}^{R} Y_i(\\theta)\n$$\nBy the Law of Large Numbers, as the number of replications $R$ approaches infinity, the sample mean $\\hat{\\mu}_R(\\theta)$ converges in probability to the true mean $\\mu(\\theta)$. More practically, the variance of the sample mean estimator is given by:\n$$\n\\operatorname{Var}(\\hat{\\mu}_R(\\theta)) = \\operatorname{Var}\\left(\\frac{1}{R} \\sum_{i=1}^{R} Y_i(\\theta)\\right) = \\frac{1}{R^2} \\sum_{i=1}^{R} \\operatorname{Var}(Y_i(\\theta)) = \\frac{1}{R^2} (R \\sigma^2(\\theta)) = \\frac{\\sigma^2(\\theta)}{R}\n$$\nThis equation demonstrates a fundamental principle of sampling: the variance of the sample mean is inversely proportional to the number of samples $R$. Therefore, by increasing $R$, we can arbitrarily reduce the variance of our estimator, making it a more precise and reliable measure of the true mean $\\mu(\\theta)$. Multiple replications are thus essential to \"average out\" the stochastic noise inherent in the model and obtain a statistically stable estimate of the system-level outcome.\n\nNext, we derive the minimal number of replications, $R(\\theta)$, required to achieve a specified precision. The problem requires that the half-width of a $1-\\alpha$ confidence interval for $\\mu(\\theta)$ be at most $\\epsilon$. We use the normal approximation for the sample mean, justified by the Central Limit Theorem (CLT) for large $R$. The CLT states that the distribution of the standardized sample mean converges to a standard normal distribution:\n$$\n\\frac{\\hat{\\mu}_R(\\theta) - \\mu(\\theta)}{\\sigma(\\theta)/\\sqrt{R}} \\xrightarrow{d} \\mathcal{N}(0, 1) \\quad \\text{as } R \\to \\infty\n$$\nHere, $\\sigma(\\theta) = \\sqrt{\\sigma^2(\\theta)}$ is the conditional standard deviation. A two-sided confidence interval for $\\mu(\\theta)$ with confidence level $1-\\alpha$ is constructed based on this approximation. We find the critical values from the standard normal distribution that enclose a central probability of $1-\\alpha$. The remaining probability, $\\alpha$, is split equally into the two tails, with $\\alpha/2$ in each. The upper critical value is the $(1-\\alpha/2)$-quantile of the standard normal distribution, denoted as $z_{1-\\alpha/2}$. Due to symmetry, the lower critical value is $z_{\\alpha/2} = -z_{1-\\alpha/2}$.\n\nThe confidence interval is defined by the following probability statement:\n$$\nP\\left( -z_{1-\\alpha/2} \\le \\frac{\\hat{\\mu}_R(\\theta) - \\mu(\\theta)}{\\sigma(\\theta)/\\sqrt{R}} \\le z_{1-\\alpha/2} \\right) \\approx 1-\\alpha\n$$\nRearranging the inequality to isolate the true mean $\\mu(\\theta)$, we get:\n$$\nP\\left( \\hat{\\mu}_R(\\theta) - z_{1-\\alpha/2}\\frac{\\sigma(\\theta)}{\\sqrt{R}} \\le \\mu(\\theta) \\le \\hat{\\mu}_R(\\theta) + z_{1-\\alpha/2}\\frac{\\sigma(\\theta)}{\\sqrt{R}} \\right) \\approx 1-\\alpha\n$$\nThe confidence interval is thus given by $[\\hat{\\mu}_R(\\theta) - H, \\hat{\\mu}_R(\\theta) + H]$, where the half-width (or margin of error) $H$ is:\n$$\nH = z_{1-\\alpha/2}\\frac{\\sigma(\\theta)}{\\sqrt{R}}\n$$\nThe problem states that this half-width must be at most a prescribed tolerance $\\epsilon$:\n$$\nH \\le \\epsilon\n$$\nSubstituting the expression for $H$, we have:\n$$\nz_{1-\\alpha/2}\\frac{\\sigma(\\theta)}{\\sqrt{R}} \\le \\epsilon\n$$\nWe now solve this inequality for $R$.\n$$\n\\sqrt{R} \\ge \\frac{z_{1-\\alpha/2}\\sigma(\\theta)}{\\epsilon}\n$$\nSquaring both sides yields:\n$$\nR \\ge \\left(\\frac{z_{1-\\alpha/2}\\sigma(\\theta)}{\\epsilon}\\right)^2\n$$\nSubstituting $\\sigma^2(\\theta)$ for $\\sigma(\\theta)^2$:\n$$\nR \\ge \\frac{z_{1-\\alpha/2}^2 \\sigma^2(\\theta)}{\\epsilon^2}\n$$\nSince the number of replications $R$ must be an integer, the minimal number of replications $R(\\theta)$ required to satisfy this condition is the smallest integer greater than or equal to the value on the right-hand side. This is given by the ceiling function.\n$$\nR(\\theta) = \\left\\lceil \\frac{z_{1-\\alpha/2}^2 \\sigma^2(\\theta)}{\\epsilon^2} \\right\\rceil\n$$\nThis expression provides the minimal number of replications needed as a function of the desired tolerance $\\epsilon$, the confidence level $1-\\alpha$ (which determines $z_{1-\\alpha/2}$), and the intrinsic model variance $\\sigma^2(\\theta)$.",
            "answer": "$$\n\\boxed{\\left\\lceil \\frac{z_{1-\\alpha/2}^{2} \\sigma^{2}(\\theta)}{\\epsilon^{2}} \\right\\rceil}\n$$"
        },
        {
            "introduction": "Once we can reliably estimate model output at a single point, the challenge becomes exploring the entire parameter space efficiently, as brute-force methods are often computationally infeasible. This practice  introduces the principles of Design of Experiments (DOE) for constructing an optimal parameter sweep. You will build an orthogonal design that maximizes the information gained from each simulation run, ensuring your sensitivity estimates are stable and that the measured effects of different parameters are not entangled with one another.",
            "id": "4135739",
            "problem": "Consider a complex adaptive system (CAS) metamodel with $k=7$ controllable parameters, each at two levels coded as $-1$ and $+1$. You will use a parameter sweep to estimate the main effects via a linear approximation of the response, where the design matrix $X$ stacks the coded settings for $N$ runs. The modeling of the response $y$ is given by the standard linear metamodel $y = \\beta_{0} + \\sum_{j=1}^{k} \\beta_{j} x_{j} + \\varepsilon$, and the coefficients are estimated by Ordinary Least Squares (OLS), assuming independent, mean-zero noise with finite variance. \n\nYour tasks are:\n1. From first principles of Design of Experiments (DOE), determine the minimal $N$ that admits an $N \\times (k+1)$ two-level design matrix $X$ whose columns (including the intercept column) are mutually orthogonal and whose non-intercept columns are balanced (equal numbers of $-1$ and $+1$). Justify this minimality and construct such an $X$ explicitly.\n2. Using your constructed $X$, form the Gram matrix $G = X^{\\top} X$. Determine the exact spectral condition number $\\kappa(G)$, defined as the ratio of the largest to the smallest eigenvalue of $G$.\n3. Based on the orthogonality properties of your $X$, briefly discuss the implications for the stability of sensitivity estimation of the main effects within this CAS metamodeling context (you may use the OLS estimator and its covariance as foundational facts).\n\nExpress your final numerical answer as the exact value of $\\kappa(G)$ with no rounding and no units.",
            "solution": "The user has provided a problem statement that requires validation before a solution is attempted.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- System: A complex adaptive system (CAS) metamodel.\n- Number of controllable parameters: $k=7$.\n- Parameter levels: Two levels, coded as $-1$ and $+1$.\n- Analysis method: Parameter sweep to estimate main effects via a linear approximation.\n- Design matrix: $X$, with dimensions $N \\times (k+1)$, stacking coded settings for $N$ runs.\n- Response model: $y = \\beta_{0} + \\sum_{j=1}^{k} \\beta_{j} x_{j} + \\varepsilon$. This is a standard linear metamodel.\n- Estimation method: Ordinary Least Squares (OLS).\n- Error term assumption: $\\varepsilon$ consists of independent, mean-zero random variables with finite variance $\\sigma^2$.\n- Task 1 constraints on $X$:\n    - Minimal number of runs $N$.\n    - The $N \\times (k+1)$ two-level design matrix $X$ must have mutually orthogonal columns (including the intercept column).\n    - The non-intercept columns must be balanced (equal numbers of $-1$ and $+1$).\n- Task 2 requirement:\n    - Form the Gram matrix $G = X^{\\top} X$.\n    - Determine the exact spectral condition number $\\kappa(G) = \\lambda_{\\text{max}}(G) / \\lambda_{\\text{min}}(G)$.\n- Task 3 requirement:\n    - Discuss implications of the orthogonality of $X$ for the stability of sensitivity estimation.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically and mathematically sound. It is firmly grounded in the established principles of statistical Design of Experiments (DOE) and linear regression analysis, which are standard techniques for building metamodels and performing sensitivity analysis in the study of complex systems.\n\n1.  **Scientific or Factual Unsoundness**: The problem is free of any scientific or factual errors. The linear model, OLS estimation, and the concept of orthogonal designs are cornerstones of applied statistics and modeling.\n2.  **Non-Formalizable or Irrelevant**: The problem is highly formalizable and directly relevant to the topic of parameter sweeping and sensitivity analysis for CAS models. A linear metamodel is a common approach for this purpose.\n3.  **Incomplete or Contradictory Setup**: The problem is self-contained and consistent. It provides all necessary information ($k=7$) and constraints (orthogonality, balance) to uniquely determine the minimal design size $N$ and the structure of the resulting Gram matrix. The \"balanced\" constraint is, in fact, a necessary consequence of requiring the parameter columns to be orthogonal to the intercept column, showing internal consistency.\n4.  **Unrealistic or Infeasible**: The required experimental design (an orthogonal array for $k=7$ factors) is not only feasible but is a well-known and widely used design (e.g., a Plackett-Burman design or a $2^{7-4}$ fractional factorial design).\n5.  **Ill-Posed or Poorly Structured**: The problem is well-posed. It asks for a minimal $N$, the construction of a specific matrix $X$, a uniquely defined condition number $\\kappa(G)$, and a discussion based on standard statistical theory. A unique and meaningful solution exists.\n6.  **Pseudo-Profound, Trivial, or Tautological**: The problem requires non-trivial synthesis of concepts from linear algebra and regression analysis within the context of DOE.\n7.  **Outside Scientific Verifiability**: All parts of the problem are mathematically verifiable.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\n### Solution\n\nThe problem is addressed in three parts as requested.\n\n**1. Minimal Design Size $N$ and Construction of Matrix $X$**\n\nWe are tasked with finding the minimal number of runs, $N$, for a design matrix $X$ of size $N \\times (k+1)$ that satisfies specific orthogonality and balance conditions. Given $k=7$ parameters, the matrix $X$ must have $k+1 = 8$ columns. Let these columns be denoted by the vectors $\\mathbf{x}_0, \\mathbf{x}_1, \\dots, \\mathbf{x}_7$, where $\\mathbf{x}_0$ is the intercept column and $\\mathbf{x}_1, \\dots, \\mathbf{x}_7$ correspond to the $7$ parameters.\n\nThe intercept column, $\\mathbf{x}_0$, is a vector of $N$ ones, i.e., $\\mathbf{x}_0 = [1, 1, \\dots, 1]^\\top$. The parameter columns $\\mathbf{x}_j$ for $j \\in \\{1, \\dots, 7\\}$ have entries $x_{ij} \\in \\{-1, +1\\}$.\n\nThe condition of mutual orthogonality states that $\\mathbf{x}_i^\\top \\mathbf{x}_j = 0$ for all $i \\neq j$, where $i, j \\in \\{0, 1, \\dots, 7\\}$. Let us consider the orthogonality of a parameter column $\\mathbf{x}_j$ (where $j>0$) with the intercept column $\\mathbf{x}_0$:\n$$\n\\mathbf{x}_0^\\top \\mathbf{x}_j = \\sum_{i=1}^{N} (1)(x_{ij}) = 0\n$$\nSince $x_{ij}$ can only be $-1$ or $+1$, for their sum to be zero, the column vector $\\mathbf{x}_j$ must contain an equal number of $+1$s and $-1$s. This implies that $N$ must be an even integer. This also confirms that the \"balanced column\" constraint is a direct and necessary consequence of orthogonality with the intercept.\n\nWe require a set of $8$ mutually orthogonal vectors in an $N$-dimensional space $\\mathbb{R}^N$. A fundamental theorem of linear algebra states that the maximum number of mutually orthogonal non-zero vectors in an $N$-dimensional space is $N$. Therefore, we must have $N \\geq 8$.\n\nThe problem now is to find the smallest integer $N \\geq 8$ for which there exists an $N \\times 8$ matrix $X$ with entries $\\pm 1$ (after appropriate coding of the first column) whose columns are mutually orthogonal. This is a classic problem in the theory of Hadamard matrices. A Hadamard matrix of order $N$, denoted $H_N$, is an $N \\times N$ matrix with entries $\\pm 1$ such that its columns (and rows) are mutually orthogonal. This property is captured by the equation $H_N H_N^\\top = N I_N$, where $I_N$ is the $N \\times N$ identity matrix. For such a matrix to exist, $N$ must be $1$, $2$, or a multiple of $4$.\n\nSince we need $N \\geq 8$, the smallest integer that is a multiple of $4$ is $N=8$. A Hadamard matrix of order $8$ exists and can be constructed using the Sylvester (or Kronecker product) method:\nLet $H_2 = \\begin{pmatrix} 1 & 1 \\\\ 1 & -1 \\end{pmatrix}$. Then $H_{2m} = H_2 \\otimes H_m = \\begin{pmatrix} H_m & H_m \\\\ H_m & -H_m \\end{pmatrix}$.\nThis yields $H_4 = \\begin{pmatrix} H_2 & H_2 \\\\ H_2 & -H_2 \\end{pmatrix} = \\begin{pmatrix} 1 & 1 & 1 & 1 \\\\ 1 & -1 & 1 & -1 \\\\ 1 & 1 & -1 & -1 \\\\ 1 & -1 & -1 & 1 \\end{pmatrix}$.\nAnd finally, $H_8 = \\begin{pmatrix} H_4 & H_4 \\\\ H_4 & -H_4 \\end{pmatrix}$.\n\nThe resulting matrix $H_8$ is an $8 \\times 8$ matrix whose columns are mutually orthogonal. We can set our design matrix $X$ to be this $H_8$.\n$$\nX = H_8 = \\begin{pmatrix}\n 1 &  1 &  1 &  1 &  1 &  1 &  1 &  1 \\\\\n 1 & -1 &  1 & -1 &  1 & -1 &  1 & -1 \\\\\n 1 &  1 & -1 & -1 &  1 &  1 & -1 & -1 \\\\\n 1 & -1 & -1 &  1 &  1 & -1 & -1 &  1 \\\\\n 1 &  1 &  1 &  1 & -1 & -1 & -1 & -1 \\\\\n 1 & -1 &  1 & -1 & -1 &  1 & -1 &  1 \\\\\n 1 &  1 & -1 & -1 & -1 & -1 &  1 &  1 \\\\\n 1 & -1 & -1 &  1 & -1 &  1 &  1 & -1\n\\end{pmatrix}\n$$\nThe first column of this matrix is a vector of all $+1$s, which is suitable for the intercept term $\\mathbf{x}_0$. The remaining $7$ columns provide the settings for the parameters $\\mathbf{x}_1, \\dots, \\mathbf{x}_7$. Each of these $7$ columns contains four $+1$s and four $-1$s, satisfying the balance condition. All columns are mutually orthogonal.\nThus, the minimal number of runs is $N=8$. This design corresponds to a saturated orthogonal array, specifically a $2^{7-4}$ resolution III fractional factorial design.\n\n**2. Gram Matrix $G$ and its Spectral Condition Number $\\kappa(G)$**\n\nThe Gram matrix is defined as $G = X^\\top X$. Since the matrix $X$ we constructed is a Hadamard matrix of order $8$, it satisfies the property $X^\\top X = 8 I_8$, where $I_8$ is the $8 \\times 8$ identity matrix.\n\nAlternatively, we can compute the elements of $G$ directly. The element $G_{ij}$ is the dot product of column $\\mathbf{x}_i$ and column $\\mathbf{x}_j$ of $X$.\nFor the diagonal elements ($i=j$):\n$$\nG_{jj} = \\mathbf{x}_j^\\top \\mathbf{x}_j = \\sum_{l=1}^{8} (x_{lj})^2\n$$\nFor any column $\\mathbf{x}_j$ (including the intercept), the entries are $\\pm 1$. Therefore, $(x_{lj})^2=1$.\n$$\nG_{jj} = \\sum_{l=1}^{8} 1 = 8\n$$\nFor the off-diagonal elements ($i \\neq j$), by the orthogonality property of our constructed $X$:\n$$\nG_{ij} = \\mathbf{x}_i^\\top \\mathbf{x}_j = 0\n$$\nTherefore, the Gram matrix is a diagonal matrix with all diagonal entries equal to $8$:\n$$\nG = 8I_8 = \\begin{pmatrix} 8 & 0 & \\dots & 0 \\\\ 0 & 8 & \\dots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\dots & 8 \\end{pmatrix}\n$$\nThe eigenvalues of a diagonal matrix are its diagonal entries. Thus, all $8$ eigenvalues of $G$ are equal to $8$. Let $\\lambda$ denote an eigenvalue. Then $\\lambda_1 = \\lambda_2 = \\dots = \\lambda_8 = 8$.\nThis means the largest eigenvalue is $\\lambda_{\\text{max}}(G) = 8$ and the smallest eigenvalue is $\\lambda_{\\text{min}}(G) = 8$.\n\nThe spectral condition number $\\kappa(G)$ is the ratio of the largest to the smallest eigenvalue:\n$$\n\\kappa(G) = \\frac{\\lambda_{\\text{max}}(G)}{\\lambda_{\\text{min}}(G)} = \\frac{8}{8} = 1\n$$\n\n**3. Implications for Stability of Sensitivity Estimation**\n\nIn this CAS metamodel, the sensitivities of the response to changes in the parameters are estimated by the coefficients $\\beta_j$ in the linear model. The stability of this estimation is directly related to the statistical properties of the OLS estimator, $\\hat{\\boldsymbol{\\beta}}$.\n\nThe OLS estimator for the vector of coefficients is given by the formula $\\hat{\\boldsymbol{\\beta}} = (X^\\top X)^{-1} X^\\top \\mathbf{y}$. The covariance matrix of this estimator is $\\text{Cov}(\\hat{\\boldsymbol{\\beta}}) = \\sigma^2 (X^\\top X)^{-1}$, where $\\sigma^2$ is the variance of the error term $\\varepsilon$.\n\nFrom our previous analysis, we established that $X^\\top X = G = 8I_8$. The inverse is $(X^\\top X)^{-1} = (8I_8)^{-1} = \\frac{1}{8}I_8$.\nSubstituting this into the covariance formula yields:\n$$\n\\text{Cov}(\\hat{\\boldsymbol{\\beta}}) = \\sigma^2 \\left(\\frac{1}{8}I_8\\right) = \\frac{\\sigma^2}{N}I_8\n$$\nThis diagonal structure of the covariance matrix has profound and highly desirable implications for sensitivity analysis:\n\n1.  **Uncorrelated Estimates**: Since the covariance matrix is diagonal, all off-diagonal elements are zero. This means $\\text{Cov}(\\hat{\\beta}_i, \\hat{\\beta}_j) = 0$ for $i \\neq j$. The estimates of the main effects (and the intercept) are statistically uncorrelated. This allows for the unambiguous estimation of each parameter's sensitivity; the value of $\\hat{\\beta}_i$ is not statistically dependent on the value of $\\hat{\\beta}_j$. This property is known as estimability separation and is a hallmark of orthogonal designs.\n\n2.  **Maximum Precision and Stability**: The variance of each coefficient estimator is given by the diagonal elements of the covariance matrix: $\\text{Var}(\\hat{\\beta}_j) = \\frac{\\sigma^2}{8} = \\frac{\\sigma^2}{N}$ for all $j \\in \\{0, \\dots, 7\\}$. This variance is the minimum possible for any experimental design with $N$ runs. The orthogonal design is maximally efficient in extracting information, leading to the most precise and stable estimates possible. The spectral condition number $\\kappa(G) = 1$ is the mathematical signature of this ideal state. A value of $1$ signifies a perfectly well-conditioned problem, where the estimates are robust and insensitive to small perturbations in the input data $\\mathbf{y}$. Any non-orthogonality (multicollinearity) would result in $\\kappa(G) > 1$, leading to variance inflation and less stable parameter estimates.\n\nIn conclusion, the use of this orthogonal design for parameter sweeping provides the most robust and efficient method for estimating the main sensitivities in the CAS metamodel, ensuring that the effects are estimated independently and with the highest possible precision.",
            "answer": "$$\n\\boxed{1}\n$$"
        },
        {
            "introduction": "The previous exercises established how to obtain reliable estimates and design an efficient sweep, but they do not address the practical question of when to stop collecting data. This final hands-on challenge  moves from static experimental design to a dynamic, adaptive simulation process. You will implement a formal statistical stopping criterion in code, learning to run a parameter sweep just long enough for your sensitivity metrics to converge, thereby balancing the need for accuracy with the conservation of computational resources.",
            "id": "4135767",
            "problem": "Consider a complex adaptive system in which a sensitivity metric is defined for each parameter as the expectation of a measurable response function. For a parameter vector with components indexed by $i \\in \\{1,\\dots,k\\}$, let the sensitivity metric be $S_i(\\theta_i) = \\mathbb{E}[g_i(X;\\theta_i)]$, where $g_i$ is a real-valued function of the system microstate $X$ and parameter $\\theta_i$. In a Monte Carlo (MC) parameter sweep, at batch $b$ you collect $n_b$ independent samples per metric and form the cumulative estimator for each $i$ as the sample mean $\\hat{S}_{i,n} = \\frac{1}{n}\\sum_{j=1}^{n} Y_{i,j}$, where $Y_{i,j}$ are independent and identically distributed observations of $g_i$. The unknown population variance is estimated by the sample standard deviation $\\hat{\\sigma}_{i,n} = \\sqrt{\\frac{1}{n-1}\\sum_{j=1}^{n} (Y_{i,j} - \\hat{S}_{i,n})^2}$ for $n \\ge 2$. Assume the Central Limit Theorem (CLT) and use the Student's $t$-distribution to construct a two-sided confidence interval (CI) for each sensitivity metric at level $1-\\alpha$:\n$$\n\\hat{S}_{i,n} \\pm t_{1-\\alpha/2,\\,n-1}\\,\\frac{\\hat{\\sigma}_{i,n}}{\\sqrt{n}},\n$$\nwhere $t_{p,\\,\\nu}$ denotes the $p$-quantile of the Student's $t$-distribution with $\\nu$ degrees of freedom. The confidence band width is therefore $W_{i,n} = 2\\,t_{1-\\alpha/2,\\,n-1}\\,\\frac{\\hat{\\sigma}_{i,n}}{\\sqrt{n}}$.\n\nYou are to formalize and implement a stopping criterion for incremental sweeps based on convergence of the estimated sensitivity metrics within a confidence band of width $\\epsilon$ at level $1-\\alpha$, expressed in purely mathematical terms. Specifically, for a fixed batch size $b$, after each batch update of $n \\leftarrow n + b$ samples per metric, declare convergence when two conditions hold simultaneously for all $i \\in \\{1,\\dots,k\\}$:\n- Width condition: $W_{i,n} \\le \\epsilon$.\n- Mutual-inclusion condition: letting $h_{i,m} = t_{1-\\alpha/2,\\,m-1}\\,\\frac{\\hat{\\sigma}_{i,m}}{\\sqrt{m}}$ denote the half-width at sample size $m$, both inclusions must hold\n$$\n\\hat{S}_{i,n} \\in [\\,\\hat{S}_{i,n-b} - h_{i,n-b}\\,,\\,\\hat{S}_{i,n-b} + h_{i,n-b}\\,]\n\\quad\\text{and}\\quad\n\\hat{S}_{i,n-b} \\in [\\,\\hat{S}_{i,n} - h_{i,n}\\,,\\,\\hat{S}_{i,n} + h_{i,n}\\,].\n$$\nEquivalently, both inequalities $|\\hat{S}_{i,n} - \\hat{S}_{i,n-b}| \\le h_{i,n-b}$ and $|\\hat{S}_{i,n} - \\hat{S}_{i,n-b}| \\le h_{i,n}$ must be satisfied.\n\nStarting from these fundamental definitions and facts, implement a program that simulates incremental sweeps for independent Gaussian observation models $Y_{i,j} \\sim \\mathcal{N}(\\mu_i,\\sigma_i^2)$ and applies the stopping criterion above. Use a fixed random number generator seed to ensure reproducibility. For each test case below, return the smallest total sample size $n$ (an integer multiple of the batch size $b$) at which the stopping criterion is met for all metrics simultaneously. If the criterion is not met within a given maximum number of batches, return $-1$.\n\nUse the following test suite, where each case is specified as $(\\{\\mu_i\\}_{i=1}^k,\\{\\sigma_i\\}_{i=1}^k,b,\\epsilon,\\alpha,\\text{max\\_batches},\\text{seed})$:\n- Case $1$ (general \"happy path\"): $(\\{0.5,-0.2,1.0\\},\\{0.3,0.2,0.5\\},50,0.1,0.05,200,101)$.\n- Case $2$ (stringent width, potential non-convergence): $(\\{0.0\\},\\{0.5\\},100,0.02,0.05,50,202)$.\n- Case $3$ (moderate coverage level): $(\\{0.0,0.0\\},\\{0.4,0.4\\},40,0.1,0.5,100,303)$.\n- Case $4$ (single metric, high variance): $(\\{1.0\\},\\{1.0\\},20,0.2,0.05,200,404)$.\n- Case $5$ (stringent confidence level): $(\\{0.0,1.0,-1.0\\},\\{0.5,0.8,0.6\\},30,0.15,0.01,200,505)$.\n\nYour program must:\n- Use the above definitions and stopping conditions exactly.\n- Simulate Gaussian observations using the given $(\\mu_i,\\sigma_i)$ for each metric $i$ and batch size $b$ per update.\n- Maintain running estimates $\\hat{S}_{i,n}$ and $\\hat{\\sigma}_{i,n}$ for each metric using numerically stable streaming updates.\n- Evaluate the two stopping conditions only when both current and previous batches have at least $2$ samples (so that degrees of freedom are valid).\n- Return the smallest total $n$ satisfying both conditions for all $i$, or $-1$ if not achieved within the specified maximum number of batches.\n\nFinal output format: Your program should produce a single line containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases $1$ through $5$ (e.g., $[n_1,n_2,n_3,n_4,n_5]$). No physical units or angles are involved. Express all numeric results as plain integers.",
            "solution": "The problem requires the implementation of a statistical stopping criterion for a Monte Carlo parameter sweep. This criterion determines when the estimates of sensitivity metrics have converged. The solution involves simulating the data generation process and applying the specified rules in an iterative, batch-wise manner.\n\nThe core of the problem is to determine the smallest total sample size, $n$, at which a set of convergence conditions are met simultaneously for all $k$ sensitivity metrics. The process is incremental, adding a batch of $b$ new samples for each metric in each step, thus increasing the total sample size from $n-b$ to $n$.\n\nThe algorithm for a single test case with parameters $(\\{\\mu_i\\}_{i=1}^k, \\{\\sigma_i\\}_{i=1}^k, b, \\epsilon, \\alpha, \\text{max\\_batches}, \\text{seed})$ is as follows:\n\n1.  **Initialization**:\n    -   Set the random number generator seed to ensure reproducibility.\n    -   Initialize an empty list for each of the $k$ metrics to store the sequence of generated observations $Y_{i,j}$.\n    -   The process proceeds in batches. Let the batch number be indexed by $j_{batch}$, starting from $j_{batch}=1$. The total number of samples after $j_{batch}$ batches is $n = j_{batch} \\times b$.\n\n2.  **Iterative Batch Processing**:\n    -   Loop from $j_{batch} = 1$ to $\\text{max\\_batches}$.\n    -   In each iteration, generate a new batch of $b$ samples for each metric $i \\in \\{1, \\dots, k\\}$. The samples $Y_{i,j}$ are drawn from a normal distribution $\\mathcal{N}(\\mu_i, \\sigma_i^2)$ as specified. These new samples are appended to their respective storage lists.\n    -   The stopping conditions are first evaluated after the second batch has been processed (i.e., for $j_{batch} \\ge 2$), as they require comparison between the current state (with $n$ samples) and the previous state (with $n-b$ samples). The condition that both sample sizes must be at least $2$ is satisfied, since all test cases have $b \\ge 20$.\n\n3.  **Convergence Check (for $j_{batch} \\ge 2$)**:\n    -   For each metric $i$, we compute statistical properties for the current and previous sample sets.\n    -   **Current state (sample size $n = j_{batch} \\times b$)**:\n        -   Sample mean: $\\hat{S}_{i,n} = \\frac{1}{n} \\sum_{j=1}^{n} Y_{i,j}$.\n        -   Sample standard deviation: $\\hat{\\sigma}_{i,n} = \\sqrt{\\frac{1}{n-1} \\sum_{j=1}^{n} (Y_{i,j} - \\hat{S}_{i,n})^2}$.\n    -   **Previous state (sample size $n_{prev} = (j_{batch}-1) \\times b = n-b$)**:\n        -   Sample mean: $\\hat{S}_{i,n-b} = \\frac{1}{n-b} \\sum_{j=1}^{n-b} Y_{i,j}$.\n        -   Sample standard deviation: $\\hat{\\sigma}_{i,n-b} = \\sqrt{\\frac{1}{n-b-1} \\sum_{j=1}^{n-b} (Y_{i,j} - \\hat{S}_{i,n-b})^2}$.\n    -   The quantile from the Student's $t$-distribution, $t_{p,\\nu}$, where $p=1-\\alpha/2$ is the probability and $\\nu$ is the degrees of freedom, is required.\n    -   The two stopping conditions are then evaluated for each metric $i$:\n        a.  **Width Condition**: The width of the $1-\\alpha$ confidence interval, $W_{i,n}$, must be no larger than a specified tolerance $\\epsilon$.\n            $$\n            W_{i,n} = 2 \\cdot t_{1-\\alpha/2, n-1} \\cdot \\frac{\\hat{\\sigma}_{i,n}}{\\sqrt{n}} \\le \\epsilon\n            $$\n        b.  **Mutual-Inclusion Condition**: The difference between the current and previous mean estimates must be smaller than the half-width of both the current and previous confidence intervals. This ensures stability of the estimate.\n            $$\n            |\\hat{S}_{i,n} - \\hat{S}_{i,n-b}| \\le h_{i,n} \\quad \\text{and} \\quad |\\hat{S}_{i,n} - \\hat{S}_{i,n-b}| \\le h_{i,n-b}\n            $$\n            where the half-widths are $h_{i,n} = W_{i,n}/2 = t_{1-\\alpha/2, n-1} \\frac{\\hat{\\sigma}_{i,n}}{\\sqrt{n}}$ and $h_{i,n-b} = t_{1-\\alpha/2, n-b-1} \\frac{\\hat{\\sigma}_{i,n-b}}{\\sqrt{n-b}}$.\n\n4.  **Termination**:\n    -   If both conditions (a) and (b) hold true for **all** metrics $i \\in \\{1, \\dots, k\\}$, the simulation for the current test case is declared to have converged. The result is the current total sample size, $n$.\n    -   If the loop completes through all $\\text{max\\_batches}$ iterations without achieving convergence, the result for the test case is defined as $-1$.\n\nThis procedure is applied to each of the $5$ test cases provided. The implementation uses `numpy` for numerical operations and random number generation, and `scipy.stats.t.ppf` to compute the quantiles of the Student's $t$-distribution. To ensure correctness and simplicity, sample statistics ($\\hat{S}_{i,n}$ and $\\hat{\\sigma}_{i,n}$) are recomputed from the full list of stored samples at each step, which is a numerically stable approach for the sample sizes considered and directly corresponds to the provided mathematical definitions.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t\n\ndef solve():\n    \"\"\"\n    Solves the problem by simulating incremental parameter sweeps and applying the specified stopping criterion.\n    \"\"\"\n    # Each case is specified as: (mus, sigmas, b, epsilon, alpha, max_batches, seed)\n    test_cases = [\n        # Case 1 (general \"happy path\")\n        ([0.5, -0.2, 1.0], [0.3, 0.2, 0.5], 50, 0.1, 0.05, 200, 101),\n        # Case 2 (stringent width, potential non-convergence)\n        ([0.0], [0.5], 100, 0.02, 0.05, 50, 202),\n        # Case 3 (moderate coverage level)\n        ([0.0, 0.0], [0.4, 0.4], 40, 0.1, 0.5, 100, 303),\n        # Case 4 (single metric, high variance)\n        ([1.0], [1.0], 20, 0.2, 0.05, 200, 404),\n        # Case 5 (stringent confidence level)\n        ([0.0, 1.0, -1.0], [0.5, 0.8, 0.6], 30, 0.15, 0.01, 200, 505),\n    ]\n\n    results = []\n\n    for case in test_cases:\n        mus, sigmas, b, epsilon, alpha, max_batches, seed = case\n        k = len(mus)\n        rng = np.random.default_rng(seed)\n\n        all_samples = [[] for _ in range(k)]\n        converged_n = -1\n\n        for batch_num in range(1, max_batches + 1):\n            n = batch_num * b\n\n            # Generate new samples for the current batch and append them\n            for i in range(k):\n                new_samples = rng.normal(loc=mus[i], scale=sigmas[i], size=b)\n                all_samples[i].extend(new_samples)\n            \n            # The check requires a previous batch, so we start from the second batch.\n            # The condition n-b >= 2 is always met since b_min = 20.\n            if batch_num == 1:\n                continue\n\n            n_prev = (batch_num - 1) * b\n            \n            all_metrics_converged = True\n            for i in range(k):\n                # Current statistics (n samples)\n                samples_curr = np.array(all_samples[i])\n                s_hat_curr = np.mean(samples_curr)\n                sigma_hat_curr = np.std(samples_curr, ddof=1) if n > 1 else 0.0\n\n                # Previous statistics (n-b samples)\n                samples_prev = np.array(all_samples[i][:n_prev])\n                s_hat_prev = np.mean(samples_prev)\n                sigma_hat_prev = np.std(samples_prev, ddof=1) if n_prev > 1 else 0.0\n                \n                #\n                # Evaluate stopping criteria for metric i\n                #\n\n                t_quantile = 1.0 - alpha / 2.0\n\n                # --- 1. Width Condition ---\n                df_curr = n - 1\n                if df_curr <= 0:\n                    all_metrics_converged = False\n                    break # Cannot compute t-value\n                \n                t_curr = t.ppf(t_quantile, df_curr)\n                w_curr = 2.0 * t_curr * sigma_hat_curr / np.sqrt(n)\n\n                if w_curr > epsilon:\n                    all_metrics_converged = False\n                    break\n\n                # --- 2. Mutual-Inclusion Condition ---\n                df_prev = n_prev - 1\n                if df_prev <= 0:\n                    all_metrics_converged = False\n                    break # Cannot compute t-value\n\n                t_prev = t.ppf(t_quantile, df_prev)\n                \n                h_curr = w_curr / 2.0\n                h_prev = t_prev * sigma_hat_prev / np.sqrt(n_prev)\n                \n                delta_s = np.abs(s_hat_curr - s_hat_prev)\n\n                if not (delta_s <= h_prev and delta_s <= h_curr):\n                    all_metrics_converged = False\n                    break\n            \n            if all_metrics_converged:\n                converged_n = n\n                break\n        \n        results.append(converged_n)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}