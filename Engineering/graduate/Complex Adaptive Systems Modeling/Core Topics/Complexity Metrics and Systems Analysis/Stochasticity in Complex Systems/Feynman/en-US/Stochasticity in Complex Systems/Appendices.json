{
    "hands_on_practices": [
        {
            "introduction": "Many complex systems exhibit growth or decay rates that are not only proportional to their current state but are also subject to random perturbations. This exercise explores the canonical model for such dynamics, known as Geometric Brownian Motion. You will practice applying Itô's lemma to solve a multiplicative stochastic differential equation (SDE) and derive the mean and variance of the process, revealing how multiplicative noise can lead to a significant divergence between the average behavior and a typical system trajectory. This practice is foundational for understanding models in finance, population biology, and physics, and it highlights the crucial differences between the Itô and Stratonovich interpretations of stochastic calculus. ",
            "id": "4145296",
            "problem": "Consider a mesoscopic state variable $x_t$ representing the aggregate activity level of a complex adaptive system composed of many interacting agents. Microscopic heterogeneity and endogenous feedback produce multiplicative fluctuations that are well-modeled by the Stochastic Differential Equation (SDE) $dx_t = a\\,x_t\\,dt + b\\,x_t\\,dW_t$, where $W_t$ is a standard Wiener process (Brownian motion), $a \\in \\mathbb{R}$ and $b \\in \\mathbb{R}$ are constants, and the initial condition $x_00$ is deterministic. Assume the usual filtered probability space satisfying the standard conditions under which Itô calculus is valid.\n\nStarting only from fundamental definitions of Itô and Stratonovich stochastic integrals, Itô’s lemma, and the fact that $W_t$ has stationary independent increments with $W_t \\sim \\mathcal{N}(0,t)$, carry out the following:\n\n- Derive the drift term when the same dynamics are represented in Stratonovich form $dx_t = \\alpha(x_t)\\,dt + \\beta(x_t)\\circ dW_t$, showing precisely how the Stratonovich drift differs from the Itô drift for multiplicative noise.\n- Obtain a closed-form expression for $x_t$ by transforming the SDE appropriately, and use this to compute the first and second moments.\n- From these moments, compute $\\mathbb{E}[x_t]$ and $\\mathrm{Var}(x_t)$ explicitly in terms of $x_0$, $a$, $b$, and $t$.\n\nExpress the final answer as a single analytic expression. No rounding is required, and no units are involved. The final answer must be given as a single row matrix containing the Itô drift, the Stratonovich drift, the mean $\\mathbb{E}[x_t]$, and the variance $\\mathrm{Var}(x_t)$, in that order.",
            "solution": "The problem statement has been validated and is deemed to be scientifically grounded, well-posed, and objective. It presents a standard problem in stochastic calculus, specifically the analysis of Geometric Brownian Motion, which is a fundamental model in the study of complex systems. The problem is self-contained and provides all necessary information to derive a unique, meaningful solution.\n\nThe given stochastic differential equation (SDE) is in Itô form:\n$$dx_t = a\\,x_t\\,dt + b\\,x_t\\,dW_t$$\nThis corresponds to the general Itô SDE $dx_t = \\mu(x_t, t)\\,dt + \\sigma(x_t, t)\\,dW_t$, with an Itô drift coefficient $\\mu(x_t) = a\\,x_t$ and a diffusion coefficient $\\sigma(x_t) = b\\,x_t$. The initial condition is $x_0  0$.\n\nFirst, we derive the drift term for the equivalent Stratonovich SDE, $dx_t = \\alpha(x_t)\\,dt + \\beta(x_t)\\circ dW_t$. The diffusion coefficient is the same in both representations, so $\\beta(x_t) = \\sigma(x_t) = b\\,x_t$. The relationship between the Itô integral and the Stratonovich integral is fundamental to the conversion. The Stratonovich integral is defined using the midpoint rule, which leads to the following identity:\n$$\\int_0^t \\sigma(x_s) \\circ dW_s = \\int_0^t \\sigma(x_s) dW_s + \\frac{1}{2} \\int_0^t \\frac{\\partial \\sigma}{\\partial x}(x_s) \\sigma(x_s) ds$$\nThe second term on the right-hand side is the Itô-Stratonovich correction term, often called the noise-induced drift.\nBy substituting this relationship into the Stratonovich SDE, we get:\n$$dx_t = \\alpha(x_t) dt + \\int_0^t \\sigma(x_s) dW_s + \\frac{1}{2} \\int_0^t \\frac{\\partial \\sigma}{\\partial x}(x_s) \\sigma(x_s) ds$$\nIn differential form, this is:\n$$dx_t = \\left(\\alpha(x_t) + \\frac{1}{2} \\frac{\\partial \\sigma}{\\partial x}(x_t) \\sigma(x_t)\\right) dt + \\sigma(x_t) dW_t$$\nComparing this with the Itô SDE $dx_t = \\mu(x_t) dt + \\sigma(x_t) dW_t$, we can identify the relationship between the drift coefficients:\n$$\\mu(x_t) = \\alpha(x_t) + \\frac{1}{2} \\frac{\\partial \\sigma}{\\partial x}(x_t) \\sigma(x_t)$$\nTherefore, the Stratonovich drift coefficient $\\alpha(x_t)$ can be expressed in terms of the Itô drift $\\mu(x_t)$ and the diffusion term $\\sigma(x_t)$ as:\n$$\\alpha(x_t) = \\mu(x_t) - \\frac{1}{2} \\frac{\\partial \\sigma}{\\partial x}(x_t) \\sigma(x_t)$$\nFor the given problem, we have $\\mu(x_t) = a\\,x_t$ and $\\sigma(x_t) = b\\,x_t$. The partial derivative of $\\sigma$ with respect to $x$ is:\n$$\\frac{\\partial \\sigma}{\\partial x}(x_t) = \\frac{\\partial}{\\partial x} (b\\,x_t) = b$$\nSubstituting these into the conversion formula for the drift:\n$$\\alpha(x_t) = a\\,x_t - \\frac{1}{2} (b) (b\\,x_t) = a\\,x_t - \\frac{1}{2}b^2 x_t = \\left(a - \\frac{1}{2}b^2\\right) x_t$$\nThe Itô drift coefficient is $\\mu(x_t) = a\\,x_t$. The Stratonovich drift coefficient is $\\alpha(x_t) = (a - \\frac{1}{2}b^2)x_t$. The difference, $-\\frac{1}{2}b^2 x_t$, arises from the correlation between the process $x_t$ and the stochastic increment $dW_t$ when using the midpoint evaluation of the Stratonovich integral, a feature absent in the non-anticipating Itô integral.\n\nNext, we obtain a closed-form solution for $x_t$. The multiplicative nature of the noise suggests a logarithmic transformation. Let $Y_t = f(x_t) = \\ln(x_t)$. We apply Itô's lemma to find the dynamics of $Y_t$. For a twice-differentiable function $f(x_t)$, Itô's lemma states:\n$$df(x_t) = f'(x_t) dx_t + \\frac{1}{2} f''(x_t) (dx_t)^2$$\nFor $f(x) = \\ln(x)$, the derivatives are $f'(x) = \\frac{1}{x}$ and $f''(x) = -\\frac{1}{x^2}$. The quadratic variation term $(dx_t)^2$ is computed using the Itô multiplication rules $dt \\cdot dt = 0$, $dt \\cdot dW_t = 0$, and $dW_t \\cdot dW_t = dt$:\n$$(dx_t)^2 = (a\\,x_t\\,dt + b\\,x_t\\,dW_t)^2 = a^2 x_t^2 (dt)^2 + 2ab\\,x_t^2\\,dt\\,dW_t + b^2 x_t^2 (dW_t)^2 = b^2 x_t^2 dt$$\nSubstituting these into Itô's lemma for $dY_t = d(\\ln x_t)$:\n$$d(\\ln x_t) = \\frac{1}{x_t} dx_t + \\frac{1}{2} \\left(-\\frac{1}{x_t^2}\\right) (b^2 x_t^2 dt)$$\nNow, we substitute the expression for $dx_t$:\n$$d(\\ln x_t) = \\frac{1}{x_t} (a\\,x_t\\,dt + b\\,x_t\\,dW_t) - \\frac{1}{2} b^2 dt$$\n$$d(\\ln x_t) = a\\,dt + b\\,dW_t - \\frac{1}{2} b^2 dt = \\left(a - \\frac{1}{2}b^2\\right)dt + b\\,dW_t$$\nThis is a simple linear SDE for $Y_t = \\ln(x_t)$. We can integrate it from $t=0$ to $t$:\n$$\\int_0^t d(\\ln x_s) = \\int_0^t \\left(a - \\frac{1}{2}b^2\\right) ds + \\int_0^t b\\,dW_s$$\n$$\\ln(x_t) - \\ln(x_0) = \\left(a - \\frac{1}{2}b^2\\right)t + b(W_t - W_0)$$\nSince $W_0=0$ by definition of a standard Wiener process, this simplifies to:\n$$\\ln\\left(\\frac{x_t}{x_0}\\right) = \\left(a - \\frac{1}{2}b^2\\right)t + bW_t$$\nExponentiating both sides gives the closed-form solution for $x_t$:\n$$x_t = x_0 \\exp\\left(\\left(a - \\frac{1}{2}b^2\\right)t + bW_t\\right)$$\n\nFinally, we compute the first and second moments, $\\mathbb{E}[x_t]$ and $\\mathrm{Var}(x_t)$.\nFor the first moment (the mean), we take the expectation of $x_t$:\n$$\\mathbb{E}[x_t] = \\mathbb{E}\\left[x_0 \\exp\\left(\\left(a - \\frac{1}{2}b^2\\right)t + bW_t\\right)\\right]$$\nSince $x_0$ and $t$ are deterministic, we can rearrange:\n$$\\mathbb{E}[x_t] = x_0 \\exp\\left(\\left(a - \\frac{1}{2}b^2\\right)t\\right) \\mathbb{E}[\\exp(bW_t)]$$\nTo evaluate $\\mathbb{E}[\\exp(bW_t)]$, we use the fact that $W_t$ follows a normal distribution, $W_t \\sim \\mathcal{N}(0, t)$. The random variable $bW_t$ thus follows $\\mathcal{N}(0, b^2 t)$. The expectation $\\mathbb{E}[\\exp(Z)]$ for a normal random variable $Z \\sim \\mathcal{N}(\\mu_Z, \\sigma_Z^2)$ is given by its moment-generating function evaluated at $1$, which is $\\exp(\\mu_Z + \\frac{1}{2}\\sigma_Z^2)$. Here, for $Z=bW_t$, $\\mu_Z=0$ and $\\sigma_Z^2=b^2 t$.\nAlternatively, we can use the moment-generating function of $W_t \\sim \\mathcal{N}(0, t)$, which is $M_{W_t}(k) = \\mathbb{E}[\\exp(kW_t)] = \\exp(\\frac{1}{2}k^2 t)$. Evaluating this at $k=b$:\n$$\\mathbb{E}[\\exp(bW_t)] = \\exp\\left(\\frac{1}{2}b^2 t\\right)$$\nSubstituting this back into the expression for $\\mathbb{E}[x_t]$:\n$$\\mathbb{E}[x_t] = x_0 \\exp\\left(\\left(a - \\frac{1}{2}b^2\\right)t\\right) \\exp\\left(\\frac{1}{2}b^2 t\\right) = x_0 \\exp\\left(at - \\frac{1}{2}b^2 t + \\frac{1}{2}b^2 t\\right)$$\n$$\\mathbb{E}[x_t] = x_0 \\exp(at)$$\n\nTo find the variance, $\\mathrm{Var}(x_t) = \\mathbb{E}[x_t^2] - (\\mathbb{E}[x_t])^2$, we first need the second moment, $\\mathbb{E}[x_t^2]$.\n$$x_t^2 = \\left(x_0 \\exp\\left(\\left(a - \\frac{1}{2}b^2\\right)t + bW_t\\right)\\right)^2 = x_0^2 \\exp\\left(2\\left(a - \\frac{1}{2}b^2\\right)t + 2bW_t\\right) = x_0^2 \\exp\\left((2a - b^2)t + 2bW_t\\right)$$\nTaking the expectation:\n$$\\mathbb{E}[x_t^2] = x_0^2 \\exp((2a - b^2)t) \\mathbb{E}[\\exp(2bW_t)]$$\nWe evaluate $\\mathbb{E}[\\exp(2bW_t)]$ using the MGF of $W_t$ with $k=2b$:\n$$\\mathbb{E}[\\exp(2bW_t)] = M_{W_t}(2b) = \\exp\\left(\\frac{1}{2}(2b)^2 t\\right) = \\exp\\left(\\frac{1}{2}(4b^2)t\\right) = \\exp(2b^2 t)$$\nSubstituting this into the expression for $\\mathbb{E}[x_t^2]$:\n$$\\mathbb{E}[x_t^2] = x_0^2 \\exp((2a - b^2)t) \\exp(2b^2 t) = x_0^2 \\exp(2at - b^2 t + 2b^2 t) = x_0^2 \\exp((2a+b^2)t)$$\nNow we can compute the variance:\n$$\\mathrm{Var}(x_t) = \\mathbb{E}[x_t^2] - (\\mathbb{E}[x_t])^2 = x_0^2 \\exp((2a+b^2)t) - (x_0 \\exp(at))^2$$\n$$\\mathrm{Var}(x_t) = x_0^2 \\exp(2at) \\exp(b^2 t) - x_0^2 \\exp(2at)$$\nFactoring out the common term $x_0^2 \\exp(2at)$:\n$$\\mathrm{Var}(x_t) = x_0^2 \\exp(2at) (\\exp(b^2 t) - 1)$$\n\nTo summarize, the four requested quantities are:\n1.  Itô drift coefficient: $a\\,x_t$\n2.  Stratonovich drift coefficient: $\\left(a - \\frac{1}{2}b^2\\right)x_t$\n3.  Mean: $x_0 \\exp(at)$\n4.  Variance: $x_0^2 \\exp(2at) (\\exp(b^2 t) - 1)$",
            "answer": "$$\\boxed{\\begin{pmatrix} a\\, x_t  \\left(a - \\frac{1}{2}b^2\\right)x_t  x_0 \\exp(at)  x_0^2 \\exp(2at) \\left(\\exp(b^2 t) - 1\\right) \\end{pmatrix}}$$"
        },
        {
            "introduction": "Building on the model of a state evolving with multiplicative noise, we now shift our focus from short-term moments to the ultimate fate of the system. This practice introduces the concept of the Lyapunov exponent, a powerful tool for determining the long-term exponential growth or decay rate of a trajectory in a random dynamical system. By solving for the Lyapunov exponent, you will be able to make a definitive statement about the almost sure stability of the system's origin. This exercise culminates in the discovery of noise-induced stabilization, a deeply counter-intuitive yet fundamental phenomenon where a deterministically unstable system (one with a positive growth rate, $a \\gt 0$) can be forced to decay towards zero by sufficiently strong random fluctuations. ",
            "id": "4145305",
            "problem": "Consider a scalar agent-level state $x_t$ evolving under multiplicative environmental fluctuations modeled by the Itô interpretation of a Stochastic Differential Equation (SDE),\n$$\ndx_t \\;=\\; a\\,x_t\\,dt \\;+\\; b\\,x_t\\,dW_t,\n$$\nwhere $a \\in \\mathbb{R}$ is a constant deterministic growth parameter, $b \\in \\mathbb{R}$ is a constant noise intensity, and $W_t$ is standard Brownian motion (Wiener process). Assume $x_0 \\neq 0$ and that the system is interpreted in the Itô sense.\n\nIn the framework of complex adaptive systems modeling, the largest Lyapunov exponent $\\lambda$ of the random linear cocycle associated with the scalar process is defined via the almost sure limit\n$$\n\\lambda \\;=\\; \\lim_{t \\to \\infty} \\frac{1}{t}\\,\\ln|x_t|.\n$$\n\nStarting from the fundamental definition of Brownian motion as a continuous-time martingale with independent Gaussian increments and from Itô’s lemma for twice continuously differentiable functions, derive the closed-form expression of the largest Lyapunov exponent $\\lambda$ for the given SDE. Then, using the sign of $\\lambda$ as the criterion for almost sure exponential stability of the origin in the sense of Lyapunov, determine which parameter configurations qualitatively admit stabilization of the origin through noise when the deterministic system would otherwise be unstable.\n\nExpress your final answer as a single analytic expression in terms of $a$ and $b$. No rounding is required. Do not include physical units.",
            "solution": "The problem asks for the derivation of the largest Lyapunov exponent, $\\lambda$, for a scalar stochastic process $x_t$, and for the conditions under which noise induces stability.\n\nThe state $x_t$ evolves according to the Itô stochastic differential equation (SDE) for geometric Brownian motion:\n$$\ndx_t = a\\,x_t\\,dt + b\\,x_t\\,dW_t\n$$\nwhere $a$ and $b$ are real constants and $W_t$ is a standard Wiener process. The initial state is $x_0 \\neq 0$.\n\nThe largest Lyapunov exponent is defined by the almost sure limit:\n$$\n\\lambda = \\lim_{t \\to \\infty} \\frac{1}{t}\\,\\ln|x_t|\n$$\n\nTo find a closed-form expression for $\\lambda$, we must first find an explicit solution for $\\ln|x_t|$. We can achieve this by applying Itô's lemma to the function $f(x_t) = \\ln|x_t|$. Since the SDE is linear in $x_t$ and $x_0 \\neq 0$, the process $x_t$ will almost surely never be zero for any finite time $t  0$. Therefore, the function $\\ln|x_t|$ is well-defined and its derivatives can be computed for all relevant values of $x_t$.\n\nLet $y_t = f(x_t) = \\ln|x_t|$. The first and second derivatives of $f(x)$ with respect to $x$ are:\n$$\nf'(x) = \\frac{d}{dx}(\\ln|x|) = \\frac{1}{x}\n$$\n$$\nf''(x) = \\frac{d^2}{dx^2}(\\ln|x|) = -\\frac{1}{x^2}\n$$\nItô's lemma for a function $f(x_t)$ states that:\n$$\ndf(x_t) = f'(x_t)\\,dx_t + \\frac{1}{2}f''(x_t)\\,(dx_t)^2\n$$\nIn the Itô interpretation, the quadratic variation term $(dx_t)^2$ is computed using the rules $(dt)^2 = 0$, $dt\\,dW_t = 0$, and $(dW_t)^2 = dt$.\n$$\n(dx_t)^2 = (a\\,x_t\\,dt + b\\,x_t\\,dW_t)^2 = (a\\,x_t)^2(dt)^2 + 2ab\\,x_t^2\\,dt\\,dW_t + (b\\,x_t)^2(dW_t)^2\n$$\nRetaining only the lowest order term in $dt$, which is $(dW_t)^2=dt$, gives:\n$$\n(dx_t)^2 = b^2 x_t^2 dt\n$$\nSubstituting the derivatives of $f(x)$ and the expression for $(dx_t)^2$ into Itô's lemma:\n$$\nd(\\ln|x_t|) = \\left(\\frac{1}{x_t}\\right) (a\\,x_t\\,dt + b\\,x_t\\,dW_t) + \\frac{1}{2}\\left(-\\frac{1}{x_t^2}\\right) (b^2 x_t^2 dt)\n$$\nSimplifying the terms, we obtain the SDE for $y_t = \\ln|x_t|$:\n$$\nd(\\ln|x_t|) = (a\\,dt + b\\,dW_t) - \\frac{1}{2}b^2 dt\n$$\n$$\nd(\\ln|x_t|) = \\left(a - \\frac{1}{2}b^2\\right)dt + b\\,dW_t\n$$\nThis is a simple linear SDE for $\\ln|x_t|$, which can be solved by direct integration from $t=0$ to a general time $t$:\n$$\n\\int_0^t d(\\ln|x_s|) = \\int_0^t \\left(a - \\frac{1}{2}b^2\\right)ds + \\int_0^t b\\,dW_s\n$$\n$$\n\\ln|x_t| - \\ln|x_0| = \\left(a - \\frac{1}{2}b^2\\right)t + b(W_t - W_0)\n$$\nBy definition of a standard Wiener process, $W_0=0$. Thus, the explicit solution for $\\ln|x_t|$ is:\n$$\n\\ln|x_t| = \\ln|x_0| + \\left(a - \\frac{1}{2}b^2\\right)t + b\\,W_t\n$$\nNow, we substitute this expression into the definition of the Lyapunov exponent $\\lambda$:\n$$\n\\lambda = \\lim_{t \\to \\infty} \\frac{1}{t} \\left[ \\ln|x_0| + \\left(a - \\frac{1}{2}b^2\\right)t + b\\,W_t \\right]\n$$\nWe evaluate the limit of each term separately:\n$$\n\\lambda = \\lim_{t \\to \\infty} \\frac{\\ln|x_0|}{t} + \\lim_{t \\to \\infty} \\frac{\\left(a - \\frac{1}{2}b^2\\right)t}{t} + \\lim_{t \\to \\infty} \\frac{b\\,W_t}{t}\n$$\nThe first term vanishes as $t \\to \\infty$ because $\\ln|x_0|$ is a constant:\n$$\n\\lim_{t \\to \\infty} \\frac{\\ln|x_0|}{t} = 0\n$$\nThe second term is a constant:\n$$\n\\lim_{t \\to \\infty} \\left(a - \\frac{1}{2}b^2\\right) = a - \\frac{1}{2}b^2\n$$\nThe third term involves the long-term behavior of the Wiener process. The Strong Law of Large Numbers for Brownian motion (a continuous-time martingale) states that $\\frac{W_t}{t}$ converges to $0$ almost surely as $t \\to \\infty$.\n$$\n\\lim_{t \\to \\infty} \\frac{W_t}{t} = 0 \\quad (\\text{almost surely})\n$$\nCombining these results, we find the closed-form expression for the Lyapunov exponent:\n$$\n\\lambda = 0 + \\left(a - \\frac{1}{2}b^2\\right) + b \\cdot 0 = a - \\frac{1}{2}b^2\n$$\nThe second part of the problem asks for the conditions that admit noise-induced stabilization. This phenomenon occurs when a system that is unstable in the absence of noise becomes stable due to the presence of noise.\nThe deterministic system corresponds to $b=0$, yielding the ODE $dx_t = a\\,x_t\\,dt$. The origin is an unstable equilibrium if $a  0$.\nThe stochastic system is considered almost surely exponentially stable if its largest Lyapunov exponent is negative, i.e., $\\lambda  0$.\nTherefore, noise-induced stabilization occurs when both conditions are met:\n1. Unstable deterministic dynamics: $a  0$.\n2. Stable stochastic dynamics: $\\lambda  0$.\n\nSubstituting our derived expression for $\\lambda$, the second condition becomes:\n$$\na - \\frac{1}{2}b^2  0 \\quad \\implies \\quad a  \\frac{1}{2}b^2\n$$\nThe parameter configuration for noise-induced stabilization is thus given by the inequality $0  a  \\frac{1}{2}b^2$. This region of the parameter space $(a, b)$ characterizes systems where a positive growth rate $a$ is overcome by sufficiently strong multiplicative noise (with intensity $b$), leading to the almost sure exponential decay of the state towards the origin. The problem asks for the single analytic expression derived, which is that for $\\lambda$.",
            "answer": "$$\\boxed{a - \\frac{1}{2}b^{2}}$$"
        },
        {
            "introduction": "This final practice bridges the gap between abstract theory and experimental data analysis, addressing a central question in systems biology: how to distinguish different sources of randomness in gene expression. Using a classic experimental design involving two identical reporter genes in a single cell, you will derive a method to decompose the total observed variance in protein levels into its intrinsic and extrinsic components. This exercise applies the law of total variance and the concept of conditional independence to show that the covariance between the two reporter signals directly estimates the extrinsic noise. By implementing this computation, you will gain practical experience in using statistical tools to extract meaningful insights about the structure of noise from real-world complex systems data. ",
            "id": "4145310",
            "problem": "You are given paired time series of protein counts from two identical fluorescent reporter genes measured simultaneously in the same single cell over discrete times. Model the pair at time index $t$ as random variables $X_t$ and $Y_t$ representing counts for reporter $1$ and reporter $2$, respectively. Assume a latent extrinsic state $Z_t$ that affects both reporters identically at time $t$, and assume conditional independence of $X_t$ and $Y_t$ given $Z_t$. The task is to design a computation that decomposes the total observable variance of a single reporter into its intrinsic component and its extrinsic component using covariance, starting from first principles.\n\nFundamental base to use:\n- The law of total variance: for any random variable $X$ and a latent variable $Z$, $$\\mathrm{Var}(X) = \\mathbb{E}\\big[\\mathrm{Var}(X \\mid Z)\\big] + \\mathrm{Var}\\big(\\mathbb{E}[X \\mid Z]\\big).$$\n- Conditional independence and exchangeability of the two reporters given the extrinsic state: $$X \\perp\\!\\!\\!\\perp Y \\mid Z \\quad \\text{and} \\quad X \\mid Z \\stackrel{d}{=} Y \\mid Z.$$\n- Properties of covariance: for any two random variables $X$ and $Y$, $$\\mathrm{Cov}(X,Y) = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y].$$\n\nDefinitions for this problem:\n- Define the intrinsic variance component as $$\\sigma_{\\mathrm{int}}^2 \\equiv \\mathbb{E}\\big[\\mathrm{Var}(X \\mid Z)\\big].$$\n- Define the extrinsic variance component as $$\\sigma_{\\mathrm{ext}}^2 \\equiv \\mathrm{Var}\\big(\\mathbb{E}[X \\mid Z]\\big).$$\n- The observable total variance of a single reporter is $$\\sigma_{\\mathrm{tot}}^2 \\equiv \\mathrm{Var}(X).$$\n\nYour program must, for each paired time series, compute estimators of $\\sigma_{\\mathrm{int}}^2$, $\\sigma_{\\mathrm{ext}}^2$, and $\\sigma_{\\mathrm{tot}}^2$ using only sample statistics (means, variances, covariances) of the paired data. Begin your derivation from the law of total variance and the conditional independence assumptions, and derive estimators based on the covariance between the two reporters. Then implement the resulting computation.\n\nScientific realism and constraints:\n- The two reporters are identical and measured simultaneously, so the conditional distributions given the extrinsic state are the same, and the intrinsic noise sources are independent across reporters.\n- Because a variance is nonnegative, the final reported estimates must lie in the nonnegative orthant. If any unconstrained estimator yields a negative value due to sampling variability, project your estimates to satisfy $$\\sigma_{\\mathrm{int}}^2 \\ge 0, \\quad \\sigma_{\\mathrm{ext}}^2 \\ge 0, \\quad \\sigma_{\\mathrm{int}}^2 + \\sigma_{\\mathrm{ext}}^2 = \\sigma_{\\mathrm{tot}}^2.$$\n\nUnits:\n- Express all variance values in counts squared.\n\nAngle units:\n- No angles are involved.\n\nPercentages:\n- Do not express any outputs as percentages.\n\nTest suite:\nUse the following four paired time series, each of length $n = 10$, to exercise different regimes. All values are protein counts (nonnegative integers). For each case, $x$ denotes the series for reporter $1$ and $y$ denotes the series for reporter $2$.\n\n- Case $1$ (general case, mixed intrinsic and extrinsic): \n  $$x = [52,54,53,53,56,57,53,57,53,54], \\quad y = [49,55,54,50,60,55,54,58,50,57].$$\n- Case $2$ (near-zero covariance, predominantly intrinsic):\n  $$x = [42,38,43,37,41,39,44,36,42,38], \\quad y = [37,43,38,42,39,41,36,44,38,42].$$\n- Case $3$ (perfectly shared extrinsic, no intrinsic; reporters are identical):\n  $$x = [20,22,25,23,21,24,26,22,23,25], \\quad y = [20,22,25,23,21,24,26,22,23,25].$$\n- Case $4$ (negative covariance due to anti-correlation; tests projection to nonnegativity):\n  $$x = [10,20,30,40,50,60,70,80,90,100], \\quad y = [100,90,80,70,60,50,40,30,20,10].$$\n\nRequired final output format:\n- Your program should produce a single line of output containing the results for the four cases as a comma-separated list enclosed in square brackets. Each element of this list must itself be a list of three floating-point numbers $[\\sigma_{\\mathrm{int}}^2, \\sigma_{\\mathrm{ext}}^2, \\sigma_{\\mathrm{tot}}^2]$ rounded to six decimal places and expressed in counts squared. For example, output should look like $$[[a_1,b_1,c_1],[a_2,b_2,c_2],[a_3,b_3,c_3],[a_4,b_4,c_4]].$$",
            "solution": "The problem statement is evaluated as valid. It presents a well-posed, scientifically grounded problem from the field of systems biology concerning the decomposition of variance in gene expression. The premises are physically motivated, mathematically sound, and the task is to derive and implement estimators based on fundamental principles of probability theory. There are no contradictions, ambiguities, or factual inaccuracies.\n\nThe solution proceeds by first deriving the theoretical relationships between the desired variance components and observable sample statistics, namely the covariance. These relationships are then used to construct estimators, which are implemented in the final program.\n\n**1. Theoretical Derivation**\n\nThe objective is to find expressions for the intrinsic variance, $\\sigma_{\\mathrm{int}}^2$, and extrinsic variance, $\\sigma_{\\mathrm{ext}}^2$, in terms of measurable quantities from the paired time series data for reporters $X$ and $Y$. The cornerstone of this derivation is the analysis of the covariance between $X$ and $Y$, $\\mathrm{Cov}(X, Y)$.\n\nWe begin with the Law of Total Covariance, which states for any random variables $X$, $Y$, and $Z$:\n$$\n\\mathrm{Cov}(X, Y) = \\mathbb{E}[\\mathrm{Cov}(X, Y \\mid Z)] + \\mathrm{Cov}(\\mathbb{E}[X \\mid Z], \\mathbb{E}[Y \\mid Z])\n$$\n\nThe problem provides two critical assumptions about the relationship between the reporters $X$ and $Y$ and the latent extrinsic state $Z$:\n\n1.  **Conditional Independence**: Given the state $Z$, the reporters are independent, i.e., $X \\perp\\!\\!\\!\\perp Y \\mid Z$. By definition, the covariance of two independent variables is $0$. Therefore, the conditional covariance $\\mathrm{Cov}(X, Y \\mid Z)$ is $0$ for any given state of $Z$. This implies that the expectation of the conditional covariance is also zero:\n    $$\n    \\mathbb{E}[\\mathrm{Cov}(X, Y \\mid Z)] = \\mathbb{E}[0] = 0\n    $$\n    Substituting this into the law of total covariance simplifies the expression to:\n    $$\n    \\mathrm{Cov}(X, Y) = \\mathrm{Cov}(\\mathbb{E}[X \\mid Z], \\mathbb{E}[Y \\mid Z])\n    $$\n\n2.  **Exchangeability (Identical Reporters)**: The reporters are identical and respond to the extrinsic state in the same way, meaning their conditional probability distributions given $Z$ are identical, i.e., $X \\mid Z \\stackrel{d}{=} Y \\mid Z$. A direct consequence is that their conditional expectations are equal:\n    $$\n    \\mathbb{E}[X \\mid Z] = \\mathbb{E}[Y \\mid Z]\n    $$\n    Substituting this into our simplified covariance expression yields:\n    $$\n    \\mathrm{Cov}(X, Y) = \\mathrm{Cov}(\\mathbb{E}[X \\mid Z], \\mathbb{E}[X \\mid Z])\n    $$\n    The covariance of a random variable with itself is its variance, $\\mathrm{Cov}(A, A) = \\mathrm{Var}(A)$. Therefore,\n    $$\n    \\mathrm{Cov}(X, Y) = \\mathrm{Var}(\\mathbb{E}[X \\mid Z])\n    $$\n\nThe problem defines the extrinsic variance component as $\\sigma_{\\mathrm{ext}}^2 \\equiv \\mathrm{Var}(\\mathbb{E}[X \\mid Z])$. Thus, we have derived the central result: the extrinsic variance is equal to the covariance between the two reporter signals.\n$$\n\\sigma_{\\mathrm{ext}}^2 = \\mathrm{Cov}(X, Y)\n$$\n\nWith the extrinsic component identified, we can find the intrinsic component using the Law of Total Variance, which is given as a fundamental principle:\n$$\n\\mathrm{Var}(X) = \\mathbb{E}[\\mathrm{Var}(X \\mid Z)] + \\mathrm{Var}(\\mathbb{E}[X \\mid Z])\n$$\nUsing the problem's definitions, this is:\n$$\n\\sigma_{\\mathrm{tot}}^2 = \\sigma_{\\mathrm{int}}^2 + \\sigma_{\\mathrm{ext}}^2\n$$\nRearranging for the intrinsic variance gives:\n$$\n\\sigma_{\\mathrm{int}}^2 = \\sigma_{\\mathrm{tot}}^2 - \\sigma_{\\mathrm{ext}}^2\n$$\nSubstituting our findings and definitions, $\\sigma_{\\mathrm{tot}}^2 = \\mathrm{Var}(X)$ and $\\sigma_{\\mathrm{ext}}^2 = \\mathrm{Cov}(X, Y)$, we get:\n$$\n\\sigma_{\\mathrm{int}}^2 = \\mathrm{Var}(X) - \\mathrm{Cov}(X, Y)\n$$\n\n**2. Construction of Estimators**\n\nThe theoretical relationships derived above guide the construction of estimators from the sample data, denoted here by time series $\\{x_t\\}_{t=1}^n$ and $\\{y_t\\}_{t=1}^n$. We use sample statistics (denoted by a hat, $\\hat{\\cdot}$) to estimate the population parameters.\n\n-   **Extrinsic Variance Estimator**: Based on $\\sigma_{\\mathrm{ext}}^2 = \\mathrm{Cov}(X, Y)$, the unconstrained estimator for extrinsic variance is the sample covariance between the two time series:\n    $$\n    \\hat{\\sigma}_{\\mathrm{ext, u}}^2 = \\widehat{\\mathrm{Cov}}(X, Y)\n    $$\n-   **Total Variance Estimator**: The total variance is defined for a single reporter, $\\sigma_{\\mathrm{tot}}^2 = \\mathrm{Var}(X)$. Since the theory implies $\\mathrm{Var}(X) = \\mathrm{Var}(Y)$ due to exchangeability, a more robust estimator uses data from both reporters by averaging their sample variances. This reduces the impact of sampling noise.\n    $$\n    \\hat{\\sigma}_{\\mathrm{tot}}^2 = \\frac{1}{2} \\left( \\widehat{\\mathrm{Var}}(X) + \\widehat{\\mathrm{Var}}(Y) \\right)\n    $$\n-   **Intrinsic Variance Estimator**: Following the relationship $\\sigma_{\\mathrm{int}}^2 = \\sigma_{\\mathrm{tot}}^2 - \\sigma_{\\mathrm{ext}}^2$, the unconstrained estimator for intrinsic variance is:\n    $$\n    \\hat{\\sigma}_{\\mathrm{int, u}}^2 = \\hat{\\sigma}_{\\mathrm{tot}}^2 - \\hat{\\sigma}_{\\mathrm{ext, u}}^2\n    $$\n\nAll sample variances and covariances are computed using the unbiased formula with a denominator of $n-1$, where $n$ is the number of time points.\n\n**3. Projection to Non-Negative Orthant**\n\nSince variance must be non-negative, the estimators must be constrained to $\\hat{\\sigma}_{\\mathrm{int}}^2 \\ge 0$ and $\\hat{\\sigma}_{\\mathrm{ext}}^2 \\ge 0$. Sampling error can cause the unconstrained estimators $\\hat{\\sigma}_{\\mathrm{int, u}}^2$ or $\\hat{\\sigma}_{\\mathrm{ext, u}}^2$ to be negative. We apply a projection that enforces non-negativity while preserving the total variance identity, $\\hat{\\sigma}_{\\mathrm{int}}^2 + \\hat{\\sigma}_{\\mathrm{ext}}^2 = \\hat{\\sigma}_{\\mathrm{tot}}^2$.\n\nThe procedure is as follows:\n1.  Calculate the three unconstrained estimators: $\\hat{\\sigma}_{\\mathrm{tot}}^2$, $\\hat{\\sigma}_{\\mathrm{ext, u}}^2$, and $\\hat{\\sigma}_{\\mathrm{int, u}}^2 = \\hat{\\sigma}_{\\mathrm{tot}}^2 - \\hat{\\sigma}_{\\mathrm{ext, u}}^2$.\n2.  If $\\hat{\\sigma}_{\\mathrm{ext, u}}^2  0$: The negative sample covariance is unphysical for this model. We project it to the nearest valid value, which is $0$.\n    -   $\\hat{\\sigma}_{\\mathrm{ext}}^2 = 0$\n    -   $\\hat{\\sigma}_{\\mathrm{int}}^2 = \\hat{\\sigma}_{\\mathrm{tot}}^2$\n3.  Else if $\\hat{\\sigma}_{\\mathrm{int, u}}^2  0$ (which is equivalent to $\\hat{\\sigma}_{\\mathrm{ext, u}}^2  \\hat{\\sigma}_{\\mathrm{tot}}^2$): The intrinsic component cannot be negative. We project it to $0$.\n    -   $\\hat{\\sigma}_{\\mathrm{int}}^2 = 0$\n    -   $\\hat{\\sigma}_{\\mathrm{ext}}^2 = \\hat{\\sigma}_{\\mathrm{tot}}^2$\n4.  Otherwise (if $0 \\le \\hat{\\sigma}_{\\mathrm{ext, u}}^2 \\le \\hat{\\sigma}_{\\mathrm{tot}}^2$): The unconstrained estimates are physically valid and are retained.\n    -   $\\hat{\\sigma}_{\\mathrm{ext}}^2 = \\hat{\\sigma}_{\\mathrm{ext, u}}^2$\n    -   $\\hat{\\sigma}_{\\mathrm{int}}^2 = \\hat{\\sigma}_{\\mathrm{int, u}}^2$\n\nThe final algorithm will compute $\\hat{\\sigma}_{\\mathrm{tot}}^2$, $\\hat{\\sigma}_{\\mathrm{ext}}^2$, and $\\hat{\\sigma}_{\\mathrm{int}}^2$ for each test case according to this logic.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and decomposes the variance of reporter gene expression into\n    intrinsic and extrinsic components for several test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (general case, mixed intrinsic and extrinsic)\n        (\n            [52, 54, 53, 53, 56, 57, 53, 57, 53, 54],\n            [49, 55, 54, 50, 60, 55, 54, 58, 50, 57]\n        ),\n        # Case 2 (near-zero covariance, predominantly intrinsic)\n        (\n            [42, 38, 43, 37, 41, 39, 44, 36, 42, 38],\n            [37, 43, 38, 42, 39, 41, 36, 44, 38, 42]\n        ),\n        # Case 3 (perfectly shared extrinsic, no intrinsic)\n        (\n            [20, 22, 25, 23, 21, 24, 26, 22, 23, 25],\n            [20, 22, 25, 23, 21, 24, 26, 22, 23, 25]\n        ),\n        # Case 4 (negative covariance due to anti-correlation)\n        (\n            [10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n            [100, 90, 80, 70, 60, 50, 40, 30, 20, 10]\n        )\n    ]\n\n    results = []\n    for x_data, y_data in test_cases:\n        # Convert lists to numpy arrays for vectorized operations.\n        x = np.array(x_data, dtype=float)\n        y = np.array(y_data, dtype=float)\n\n        # The ddof=1 argument ensures the use of the unbiased sample\n        # variance/covariance (division by n-1).\n        # np.cov returns the 2x2 covariance matrix:\n        # [[Var(x), Cov(x,y)],\n        #  [Cov(y,x), Var(y)]]\n        cov_matrix = np.cov(x, y, ddof=1)\n        var_x = cov_matrix[0, 0]\n        var_y = cov_matrix[1, 1]\n        cov_xy = cov_matrix[0, 1]\n\n        # 1. Calculate unconstrained estimators based on the derivation.\n        # Total variance is estimated by averaging the reporters' variances.\n        sigma_tot_sq = 0.5 * (var_x + var_y)\n        \n        # Extrinsic variance is estimated by the covariance.\n        sigma_ext_sq_unconstrained = cov_xy\n        \n        # Intrinsic variance is the remainder.\n        sigma_int_sq_unconstrained = sigma_tot_sq - sigma_ext_sq_unconstrained\n\n        # 2. Apply projection to the non-negative orthant.\n        # These are the final, constrained estimators.\n        sigma_int_sq, sigma_ext_sq = 0.0, 0.0\n\n        if sigma_ext_sq_unconstrained  0:\n            # Case where sample covariance is negative (e.g., anti-correlation).\n            # This is unphysical for the model, so project to 0.\n            sigma_ext_sq = 0.0\n            sigma_int_sq = sigma_tot_sq\n        elif sigma_int_sq_unconstrained  0:\n            # Case where covariance is larger than total variance due to noise.\n            # Intrinsic noise cannot be negative, so project to 0.\n            sigma_int_sq = 0.0\n            sigma_ext_sq = sigma_tot_sq\n        else:\n            # The unconstrained estimates are physically valid.\n            sigma_int_sq = sigma_int_sq_unconstrained\n            sigma_ext_sq = sigma_ext_sq_unconstrained\n\n        # Assemble the triplet for the current case, rounded to 6 decimal places.\n        case_result = [\n            round(sigma_int_sq, 6),\n            round(sigma_ext_sq, 6),\n            round(sigma_tot_sq, 6)\n        ]\n        results.append(case_result)\n\n    # Format the final output string as specified in the problem statement.\n    # The output from Python's list-to-string conversion matches the required format.\n    # No extra spaces after commas.\n    results_str = str(results).replace(\" \", \"\")\n\n    print(results_str)\n\nsolve()\n```"
        }
    ]
}