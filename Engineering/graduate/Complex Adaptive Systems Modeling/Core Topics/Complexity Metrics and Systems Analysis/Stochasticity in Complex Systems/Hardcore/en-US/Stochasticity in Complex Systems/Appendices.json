{
    "hands_on_practices": [
        {
            "introduction": "Many complex systems exhibit fluctuations whose magnitude scales with the system's current state, a phenomenon known as multiplicative noise. This exercise explores the canonical model for such dynamics: Geometric Brownian Motion. By solving this stochastic differential equation and calculating its key statistical moments, you will gain hands-on practice with Itô's lemma, a cornerstone of stochastic calculus. Furthermore, this problem illuminates the critical distinction between the Itô and Stratonovich interpretations of stochastic integrals, a fundamental concept for correctly modeling physical and biological systems .",
            "id": "4145296",
            "problem": "Consider a mesoscopic state variable $x_t$ representing the aggregate activity level of a complex adaptive system composed of many interacting agents. Microscopic heterogeneity and endogenous feedback produce multiplicative fluctuations that are well-modeled by the Stochastic Differential Equation (SDE) $dx_t = a\\,x_t\\,dt + b\\,x_t\\,dW_t$, where $W_t$ is a standard Wiener process (Brownian motion), $a \\in \\mathbb{R}$ and $b \\in \\mathbb{R}$ are constants, and the initial condition $x_00$ is deterministic. Assume the usual filtered probability space satisfying the standard conditions under which Itô calculus is valid.\n\nStarting only from fundamental definitions of Itô and Stratonovich stochastic integrals, Itô’s lemma, and the fact that $W_t \\sim \\mathcal{N}(0,t)$, carry out the following:\n\n- Derive the drift term when the same dynamics are represented in Stratonovich form $dx_t = \\alpha(x_t)\\,dt + \\beta(x_t)\\circ dW_t$, showing precisely how the Stratonovich drift differs from the Itô drift for multiplicative noise.\n- Obtain a closed-form expression for $x_t$ by transforming the SDE appropriately, and use this to compute the first and second moments.\n- From these moments, compute $\\mathbb{E}[x_t]$ and $\\mathrm{Var}(x_t)$ explicitly in terms of $x_0$, $a$, $b$, and $t$.\n\nExpress the final answer as a single analytic expression. No rounding is required, and no units are involved. The final answer must be given as a single row matrix containing the Itô drift, the Stratonovich drift, the mean $\\mathbb{E}[x_t]$, and the variance $\\mathrm{Var}(x_t)$, in that order.",
            "solution": "The problem statement has been validated and is deemed to be scientifically grounded, well-posed, and objective. It presents a standard problem in stochastic calculus, specifically the analysis of Geometric Brownian Motion, which is a fundamental model in the study of complex systems. The problem is self-contained and provides all necessary information to derive a unique, meaningful solution.\n\nThe given stochastic differential equation (SDE) is in Itô form:\n$$dx_t = a\\,x_t\\,dt + b\\,x_t\\,dW_t$$\nThis corresponds to the general Itô SDE $dx_t = \\mu(x_t, t)\\,dt + \\sigma(x_t, t)\\,dW_t$, with an Itô drift coefficient $\\mu(x_t) = a\\,x_t$ and a diffusion coefficient $\\sigma(x_t) = b\\,x_t$. The initial condition is $x_0  0$.\n\nFirst, we derive the drift term for the equivalent Stratonovich SDE, $dx_t = \\alpha(x_t)\\,dt + \\beta(x_t)\\circ dW_t$. The diffusion coefficient is the same in both representations, so $\\beta(x_t) = \\sigma(x_t) = b\\,x_t$. The relationship between the Itô integral and the Stratonovich integral is fundamental to the conversion. The Stratonovich integral is defined using the midpoint rule, which leads to the following identity:\n$$\\int_0^t \\sigma(x_s) \\circ dW_s = \\int_0^t \\sigma(x_s) dW_s + \\frac{1}{2} \\int_0^t \\frac{\\partial \\sigma}{\\partial x}(x_s) \\sigma(x_s) ds$$\nThe second term on the right-hand side is the Itô-Stratonovich correction term, often called the noise-induced drift.\nBy substituting this relationship into the Stratonovich SDE, we get:\n$$dx_t = \\alpha(x_t) dt + \\int_0^t \\sigma(x_s) dW_s + \\frac{1}{2} \\int_0^t \\frac{\\partial \\sigma}{\\partial x}(x_s) \\sigma(x_s) ds$$\nIn differential form, this is:\n$$dx_t = \\left(\\alpha(x_t) + \\frac{1}{2} \\frac{\\partial \\sigma}{\\partial x}(x_t) \\sigma(x_t)\\right) dt + \\sigma(x_t) dW_t$$\nComparing this with the Itô SDE $dx_t = \\mu(x_t) dt + \\sigma(x_t) dW_t$, we can identify the relationship between the drift coefficients:\n$$\\mu(x_t) = \\alpha(x_t) + \\frac{1}{2} \\frac{\\partial \\sigma}{\\partial x}(x_t) \\sigma(x_t)$$\nTherefore, the Stratonovich drift coefficient $\\alpha(x_t)$ can be expressed in terms of the Itô drift $\\mu(x_t)$ and the diffusion term $\\sigma(x_t)$ as:\n$$\\alpha(x_t) = \\mu(x_t) - \\frac{1}{2} \\frac{\\partial \\sigma}{\\partial x}(x_t) \\sigma(x_t)$$\nFor the given problem, we have $\\mu(x_t) = a\\,x_t$ and $\\sigma(x_t) = b\\,x_t$. The partial derivative of $\\sigma$ with respect to $x$ is:\n$$\\frac{\\partial \\sigma}{\\partial x}(x_t) = \\frac{\\partial}{\\partial x} (b\\,x_t) = b$$\nSubstituting these into the conversion formula for the drift:\n$$\\alpha(x_t) = a\\,x_t - \\frac{1}{2} (b) (b\\,x_t) = a\\,x_t - \\frac{1}{2}b^2 x_t = \\left(a - \\frac{1}{2}b^2\\right) x_t$$\nThe Itô drift coefficient is $\\mu(x_t) = a\\,x_t$. The Stratonovich drift coefficient is $\\alpha(x_t) = (a - \\frac{1}{2}b^2)x_t$. The difference, $-\\frac{1}{2}b^2 x_t$, arises from the correlation between the process $x_t$ and the stochastic increment $dW_t$ when using the midpoint evaluation of the Stratonovich integral, a feature absent in the non-anticipating Itô integral.\n\nNext, we obtain a closed-form solution for $x_t$. The multiplicative nature of the noise suggests a logarithmic transformation. Let $Y_t = f(x_t) = \\ln(x_t)$. We apply Itô's lemma to find the dynamics of $Y_t$. For a twice-differentiable function $f(x_t)$, Itô's lemma states:\n$$df(x_t) = f'(x_t) dx_t + \\frac{1}{2} f''(x_t) (dx_t)^2$$\nFor $f(x) = \\ln(x)$, the derivatives are $f'(x) = \\frac{1}{x}$ and $f''(x) = -\\frac{1}{x^2}$. The quadratic variation term $(dx_t)^2$ is computed using the Itô multiplication rules $dt \\cdot dt = 0$, $dt \\cdot dW_t = 0$, and $dW_t \\cdot dW_t = dt$:\n$$(dx_t)^2 = (a\\,x_t\\,dt + b\\,x_t\\,dW_t)^2 = a^2 x_t^2 (dt)^2 + 2ab\\,x_t^2\\,dt\\,dW_t + b^2 x_t^2 (dW_t)^2 = b^2 x_t^2 dt$$\nSubstituting these into Itô's lemma for $dY_t = d(\\ln x_t)$:\n$$d(\\ln x_t) = \\frac{1}{x_t} dx_t + \\frac{1}{2} \\left(-\\frac{1}{x_t^2}\\right) (b^2 x_t^2 dt)$$\nNow, we substitute the expression for $dx_t$:\n$$d(\\ln x_t) = \\frac{1}{x_t} (a\\,x_t\\,dt + b\\,x_t\\,dW_t) - \\frac{1}{2} b^2 dt$$\n$$d(\\ln x_t) = a\\,dt + b\\,dW_t - \\frac{1}{2} b^2 dt = \\left(a - \\frac{1}{2}b^2\\right)dt + b\\,dW_t$$\nThis is a simple linear SDE for $Y_t = \\ln(x_t)$. We can integrate it from $t=0$ to $t$:\n$$\\int_0^t d(\\ln x_s) = \\int_0^t \\left(a - \\frac{1}{2}b^2\\right) ds + \\int_0^t b\\,dW_s$$\n$$\\ln(x_t) - \\ln(x_0) = \\left(a - \\frac{1}{2}b^2\\right)t + b(W_t - W_0)$$\nSince $W_0=0$ by definition of a standard Wiener process, this simplifies to:\n$$\\ln\\left(\\frac{x_t}{x_0}\\right) = \\left(a - \\frac{1}{2}b^2\\right)t + bW_t$$\nExponentiating both sides gives the closed-form solution for $x_t$:\n$$x_t = x_0 \\exp\\left(\\left(a - \\frac{1}{2}b^2\\right)t + bW_t\\right)$$\n\nFinally, we compute the first and second moments, $\\mathbb{E}[x_t]$ and $\\mathrm{Var}(x_t)$.\nFor the first moment (the mean), we take the expectation of $x_t$:\n$$\\mathbb{E}[x_t] = \\mathbb{E}\\left[x_0 \\exp\\left(\\left(a - \\frac{1}{2}b^2\\right)t + bW_t\\right)\\right]$$\nSince $x_0$ and $t$ are deterministic, we can rearrange:\n$$\\mathbb{E}[x_t] = x_0 \\exp\\left(\\left(a - \\frac{1}{2}b^2\\right)t\\right) \\mathbb{E}[\\exp(bW_t)]$$\nTo evaluate $\\mathbb{E}[\\exp(bW_t)]$, we use the fact that $W_t$ follows a normal distribution, $W_t \\sim \\mathcal{N}(0, t)$. The random variable $bW_t$ thus follows $\\mathcal{N}(0, b^2 t)$. The expectation $\\mathbb{E}[\\exp(Z)]$ for a normal random variable $Z \\sim \\mathcal{N}(\\mu_Z, \\sigma_Z^2)$ is given by its moment-generating function evaluated at $1$, which is $\\exp(\\mu_Z + \\frac{1}{2}\\sigma_Z^2)$. Here, for $Z=bW_t$, $\\mu_Z=0$ and $\\sigma_Z^2=b^2 t$.\nAlternatively, we can use the moment-generating function of $W_t \\sim \\mathcal{N}(0, t)$, which is $M_{W_t}(k) = \\mathbb{E}[\\exp(kW_t)] = \\exp(\\frac{1}{2}k^2 t)$. Evaluating this at $k=b$:\n$$\\mathbb{E}[\\exp(bW_t)] = \\exp\\left(\\frac{1}{2}b^2 t\\right)$$\nSubstituting this back into the expression for $\\mathbb{E}[x_t]$:\n$$\\mathbb{E}[x_t] = x_0 \\exp\\left(\\left(a - \\frac{1}{2}b^2\\right)t\\right) \\exp\\left(\\frac{1}{2}b^2 t\\right) = x_0 \\exp\\left(at - \\frac{1}{2}b^2 t + \\frac{1}{2}b^2 t\\right)$$\n$$\\mathbb{E}[x_t] = x_0 \\exp(at)$$\n\nTo find the variance, $\\mathrm{Var}(x_t) = \\mathbb{E}[x_t^2] - (\\mathbb{E}[x_t])^2$, we first need the second moment, $\\mathbb{E}[x_t^2]$.\n$$x_t^2 = \\left(x_0 \\exp\\left(\\left(a - \\frac{1}{2}b^2\\right)t + bW_t\\right)\\right)^2 = x_0^2 \\exp\\left(2\\left(a - \\frac{1}{2}b^2\\right)t + 2bW_t\\right) = x_0^2 \\exp\\left((2a - b^2)t + 2bW_t\\right)$$\nTaking the expectation:\n$$\\mathbb{E}[x_t^2] = x_0^2 \\exp((2a - b^2)t) \\mathbb{E}[\\exp(2bW_t)]$$\nWe evaluate $\\mathbb{E}[\\exp(2bW_t)]$ using the MGF of $W_t$ with $k=2b$:\n$$\\mathbb{E}[\\exp(2bW_t)] = M_{W_t}(2b) = \\exp\\left(\\frac{1}{2}(2b)^2 t\\right) = \\exp\\left(\\frac{1}{2}(4b^2)t\\right) = \\exp(2b^2 t)$$\nSubstituting this into the expression for $\\mathbb{E}[x_t^2]$:\n$$\\mathbb{E}[x_t^2] = x_0^2 \\exp((2a - b^2)t) \\exp(2b^2 t) = x_0^2 \\exp(2at - b^2 t + 2b^2 t) = x_0^2 \\exp((2a+b^2)t)$$\nNow we can compute the variance:\n$$\\mathrm{Var}(x_t) = \\mathbb{E}[x_t^2] - (\\mathbb{E}[x_t])^2 = x_0^2 \\exp((2a+b^2)t) - (x_0 \\exp(at))^2$$\n$$\\mathrm{Var}(x_t) = x_0^2 \\exp(2at) \\exp(b^2 t) - x_0^2 \\exp(2at)$$\nFactoring out the common term $x_0^2 \\exp(2at)$:\n$$\\mathrm{Var}(x_t) = x_0^2 \\exp(2at) (\\exp(b^2 t) - 1)$$\n\nTo summarize, the four requested quantities are:\n1.  Itô drift coefficient: $a\\,x_t$\n2.  Stratonovich drift coefficient: $\\left(a - \\frac{1}{2}b^2\\right)x_t$\n3.  Mean: $x_0 \\exp(at)$\n4.  Variance: $x_0^2 \\exp(2at) (\\exp(b^2 t) - 1)$",
            "answer": "$$\\boxed{\\begin{pmatrix} a\\, x_t  \\left(a - \\frac{1}{2}b^2\\right)x_t  x_0 \\exp(at)  x_0^2 \\exp(2at) \\left(\\exp(b^2 t) - 1\\right) \\end{pmatrix}}$$"
        },
        {
            "introduction": "While the previous exercise focused on the path of a single stochastic trajectory, it is often crucial to understand the evolution of the entire ensemble of possibilities. This practice shifts our perspective from individual paths to the probability density function, governed by the Fokker-Planck equation. You will analyze the Ornstein-Uhlenbeck process, a fundamental model for systems with mean-reversion or stabilizing feedback, and derive how an initial distribution of states evolves over time . This exercise will equip you with a powerful alternative framework for analyzing stochastic dynamics, bridging the microscopic world of stochastic differential equations with the mesoscopic world of statistical mechanics.",
            "id": "4145308",
            "problem": "In many complex adaptive systems, mean-reverting fluctuations arise from the interplay between stabilizing feedback and stochastic micro-shocks. A canonical model capturing such dynamics is the one-dimensional Ornstein-Uhlenbeck (OU) process. Consider the Itô stochastic differential equation (SDE)\n$$\ndx(t) = -\\gamma\\, x(t)\\, dt + \\sqrt{2D}\\, dW_t,\n$$\nwhere $\\gamma  0$ is the mean-reversion rate, $D  0$ is the noise intensity, and $W_t$ is a standard Wiener process (Brownian motion). Let the initial state $x(0)$ be drawn from a Gaussian density\n$$\np(x,0) = \\frac{1}{\\sqrt{2\\pi\\, \\sigma_0^2}} \\exp\\!\\left( -\\frac{(x - \\mu_0)^2}{2\\, \\sigma_0^2} \\right),\n$$\nwith mean $\\mu_0 \\in \\mathbb{R}$ and variance $\\sigma_0^2  0$.\n\nStarting from the Fokker-Planck equation that governs the time evolution of the probability density $p(x,t)$ associated with the given Itô SDE, derive the exact time-dependent solution $p(x,t)$ for all $t \\ge 0$ under the specified Gaussian initial condition. Your derivation must start from the fundamental form of the Fokker-Planck equation and proceed from first principles, without relying on any pre-tabulated OU transition kernel or any shortcut formulas. Express the final $p(x,t)$ in closed form in terms of $x$, $t$, $\\gamma$, $D$, $\\mu_0$, and $\\sigma_0^2$. The final answer must be given as a single analytic expression. Do not include units. No rounding is required.",
            "solution": "The problem statement is subjected to validation.\n\n### Step 1: Extract Givens\n- **Stochastic Differential Equation (SDE):** $dx(t) = -\\gamma\\, x(t)\\, dt + \\sqrt{2D}\\, dW_t$\n- **Parameters:** $\\gamma  0$ (mean-reversion rate), $D  0$ (noise intensity).\n- **Stochastic Process:** $W_t$ is a standard Wiener process.\n- **Initial Condition:** The initial state $x(0)$ is drawn from a Gaussian probability density $p(x,0)$.\n- **Initial Density Function:** $p(x,0) = \\frac{1}{\\sqrt{2\\pi\\, \\sigma_0^2}} \\exp\\!\\left( -\\frac{(x - \\mu_0)^2}{2\\, \\sigma_0^2} \\right)$, with mean $\\mu_0 \\in \\mathbb{R}$ and variance $\\sigma_0^2  0$.\n- **Objective:** Derive the time-dependent solution for the probability density $p(x,t)$ for all $t \\ge 0$, starting from the Fokker-Planck equation.\n- **Constraint:** The derivation must proceed from first principles, without using pre-tabulated formulas for the Ornstein-Uhlenbeck (OU) process kernel.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed against the required criteria.\n- **Scientifically Grounded:** The problem is based on the Ornstein-Uhlenbeck process and its corresponding Fokker-Planck equation, which are standard and fundamental concepts in statistical physics, stochastic calculus, and the modeling of complex systems. The formulation is entirely sound.\n- **Well-Posed:** The problem provides a well-defined SDE and a proper initial condition (a normalizable Gaussian distribution). The request to find the time-evolving probability density is a standard problem in this context, for which a unique, stable, and meaningful solution is known to exist.\n- **Objective:** The problem is stated in precise mathematical language, free from any ambiguity, subjectivity, or non-scientific claims.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. It is a self-contained, scientifically correct, and well-posed mathematical physics problem. The solution process may proceed.\n\n---\n\nThe evolution of the probability density function $p(x,t)$ for a stochastic process described by the Itô SDE $dx(t) = a(x,t) dt + b(x,t) dW_t$ is governed by the Fokker-Planck equation (FPE), also known as the forward Kolmogorov equation:\n$$\n\\frac{\\partial}{\\partial t} p(x,t) = -\\frac{\\partial}{\\partial x} [a(x,t) p(x,t)] + \\frac{1}{2} \\frac{\\partial^2}{\\partial x^2} [b(x,t)^2 p(x,t)].\n$$\nFor the given Ornstein-Uhlenbeck SDE, the drift term is $a(x,t) = -\\gamma x$ and the diffusion term is $b(x,t) = \\sqrt{2D}$. Substituting these into the general FPE yields:\n$$\n\\frac{\\partial p}{\\partial t} = -\\frac{\\partial}{\\partial x} [-\\gamma x p] + \\frac{1}{2} \\frac{\\partial^2}{\\partial x^2} [(\\sqrt{2D})^2 p]\n$$\n$$\n\\frac{\\partial p}{\\partial t} = \\gamma \\frac{\\partial}{\\partial x} (x p) + D \\frac{\\partial^2 p}{\\partial x^2}.\n$$\nExpanding the first term on the right-hand side gives:\n$$\n\\frac{\\partial p}{\\partial t} = \\gamma \\left( p + x \\frac{\\partial p}{\\partial x} \\right) + D \\frac{\\partial^2 p}{\\partial x^2}.\n$$\nThis is the partial differential equation (PDE) we must solve with the given initial condition. A standard method for solving this type of linear PDE is the use of Fourier transforms. Let $\\Phi(k,t)$ be the characteristic function, which is the spatial Fourier transform of $p(x,t)$:\n$$\n\\Phi(k,t) = \\mathcal{F}[p(x,t)](k) = \\int_{-\\infty}^{\\infty} p(x,t) \\exp(ikx) dx.\n$$\nWe transform the FPE term by term. The time derivative becomes:\n$$\n\\mathcal{F}\\left[\\frac{\\partial p}{\\partial t}\\right] = \\frac{\\partial \\Phi}{\\partial t}.\n$$\nThe diffusion term transforms as:\n$$\n\\mathcal{F}\\left[D \\frac{\\partial^2 p}{\\partial x^2}\\right] = D (ik)^2 \\Phi(k,t) = -D k^2 \\Phi(k,t).\n$$\nFor the drift term, $\\gamma \\frac{\\partial}{\\partial x} (x p)$, we use the properties of the Fourier transform. First, $\\mathcal{F}[\\frac{d}{dx}f(x)] = ik \\mathcal{F}[f(x)]$. Also, $\\mathcal{F}[xf(x)] = i \\frac{d}{dk}\\mathcal{F}[f(x)]$.\nAn alternative, more direct approach is to use integration by parts:\n$$\n\\mathcal{F}\\left[\\gamma \\frac{\\partial}{\\partial x} (x p)\\right] = \\gamma \\int_{-\\infty}^{\\infty} \\exp(ikx) \\frac{\\partial}{\\partial x} (x p) dx.\n$$\nIntegrating by parts, with boundary terms vanishing at $\\pm \\infty$:\n$$\n= \\gamma \\left[ \\exp(ikx) (xp) \\right]_{-\\infty}^{\\infty} - \\gamma \\int_{-\\infty}^{\\infty} (ik \\exp(ikx)) (xp) dx = -i\\gamma k \\int_{-\\infty}^{\\infty} xp \\exp(ikx) dx.\n$$\nThe remaining integral is the Fourier transform of $xp$. Using the derivative property of the Fourier transform: $\\mathcal{F}[xp] = \\frac{1}{i} \\frac{\\partial}{\\partial k} \\Phi(k,t)$.\n$$\n\\mathcal{F}\\left[\\gamma \\frac{\\partial}{\\partial x} (x p)\\right] = -i\\gamma k \\left(\\frac{1}{i} \\frac{\\partial \\Phi}{\\partial k}\\right) = -\\gamma k \\frac{\\partial \\Phi}{\\partial k}.\n$$\nCombining the transformed terms, the FPE becomes a first-order PDE for $\\Phi(k,t)$:\n$$\n\\frac{\\partial \\Phi}{\\partial t} = -\\gamma k \\frac{\\partial \\Phi}{\\partial k} - D k^2 \\Phi.\n$$\nThis can be rewritten as:\n$$\n\\frac{\\partial \\Phi}{\\partial t} + \\gamma k \\frac{\\partial \\Phi}{\\partial k} = -D k^2 \\Phi.\n$$\nWe solve this using the method of characteristics. The characteristic equations are:\n$$\n\\frac{dt}{1} = \\frac{dk}{\\gamma k} = \\frac{d\\Phi}{-D k^2 \\Phi}.\n$$\nFrom the first two parts, $\\frac{dt}{1} = \\frac{dk}{\\gamma k}$, we integrate to find the characteristic curves:\n$$\n\\gamma \\int dt = \\int \\frac{dk}{k} \\implies \\gamma t = \\ln(k) + C_1 \\implies k \\exp(-\\gamma t) = C_2.\n$$\nThus, $k \\exp(-\\gamma t)$ is constant along the characteristics.\nNext, we solve for $\\Phi$ along these curves. Using $\\frac{dt}{1} = \\frac{d\\Phi}{-D k^2 \\Phi}$, we have:\n$$\n\\frac{d\\Phi}{\\Phi} = -D k^2 dt.\n$$\nAlong a characteristic curve, $k(t) = C_2 \\exp(\\gamma t)$. Substituting this into the equation for $\\Phi$:\n$$\n\\frac{d\\Phi}{\\Phi} = -D (C_2 \\exp(\\gamma t))^2 dt = -D C_2^2 \\exp(2\\gamma t) dt.\n$$\nIntegrating this equation with respect to $t$:\n$$\n\\ln(\\Phi) = \\int -D C_2^2 \\exp(2\\gamma t) dt = -\\frac{D C_2^2}{2\\gamma} \\exp(2\\gamma t) + f(C_2),\n$$\nwhere $f$ is an arbitrary function of the integration constant $C_2$. Substituting back $C_2 = k \\exp(-\\gamma t)$:\n$$\n\\ln(\\Phi(k,t)) = -\\frac{D}{2\\gamma} \\left(k \\exp(-\\gamma t)\\right)^2 \\exp(2\\gamma t) + f(k \\exp(-\\gamma t))\n$$\n$$\n\\ln(\\Phi(k,t)) = -\\frac{D k^2}{2\\gamma} + f(k \\exp(-\\gamma t)).\n$$\nTo determine the function $f$, we use the initial condition at $t=0$. The initial density is a Gaussian:\n$$\np(x,0) = \\frac{1}{\\sqrt{2\\pi \\sigma_0^2}} \\exp\\left(-\\frac{(x - \\mu_0)^2}{2\\sigma_0^2}\\right).\n$$\nIts characteristic function $\\Phi(k,0)$ is known to be:\n$$\n\\Phi(k,0) = \\exp\\left(ik\\mu_0 - \\frac{1}{2}k^2\\sigma_0^2\\right).\n$$\nTaking the logarithm: $\\ln(\\Phi(k,0)) = ik\\mu_0 - \\frac{1}{2}k^2\\sigma_0^2$.\nFrom our general solution for $\\Phi(k,t)$, at $t=0$:\n$$\n\\ln(\\Phi(k,0)) = -\\frac{Dk^2}{2\\gamma} + f(k).\n$$\nEquating the two expressions for $\\ln(\\Phi(k,0))$ allows us to solve for $f(k)$:\n$$\nik\\mu_0 - \\frac{1}{2}k^2\\sigma_0^2 = -\\frac{Dk^2}{2\\gamma} + f(k)\n$$\n$$\nf(k) = ik\\mu_0 - \\frac{1}{2}k^2\\sigma_0^2 + \\frac{Dk^2}{2\\gamma} = ik\\mu_0 - \\frac{1}{2}k^2\\left(\\sigma_0^2 - \\frac{D}{\\gamma}\\right).\n$$\nNow we substitute this functional form back into the solution for $\\ln(\\Phi(k,t))$, replacing the argument $k$ with $k \\exp(-\\gamma t)$:\n$$\nf(k \\exp(-\\gamma t)) = i(k \\exp(-\\gamma t))\\mu_0 - \\frac{1}{2}(k \\exp(-\\gamma t))^2\\left(\\sigma_0^2 - \\frac{D}{\\gamma}\\right)\n$$\n$$\nf(k \\exp(-\\gamma t)) = ik\\mu_0\\exp(-\\gamma t) - \\frac{1}{2}k^2\\exp(-2\\gamma t)\\left(\\sigma_0^2 - \\frac{D}{\\gamma}\\right).\n$$\nFinally, we assemble the complete expression for $\\ln(\\Phi(k,t))$:\n$$\n\\ln(\\Phi(k,t)) = -\\frac{D k^2}{2\\gamma} + ik\\mu_0\\exp(-\\gamma t) - \\frac{1}{2}k^2\\exp(-2\\gamma t)\\left(\\sigma_0^2 - \\frac{D}{\\gamma}\\right)\n$$\n$$\n\\ln(\\Phi(k,t)) = i k \\left(\\mu_0 \\exp(-\\gamma t)\\right) - \\frac{1}{2}k^2 \\left[\\frac{D}{\\gamma} + \\exp(-2\\gamma t)\\left(\\sigma_0^2 - \\frac{D}{\\gamma}\\right)\\right]\n$$\n$$\n\\ln(\\Phi(k,t)) = i k \\left(\\mu_0 \\exp(-\\gamma t)\\right) - \\frac{1}{2}k^2 \\left[\\sigma_0^2 \\exp(-2\\gamma t) + \\frac{D}{\\gamma}\\left(1 - \\exp(-2\\gamma t)\\right)\\right].\n$$\nThis expression is of the form $\\ln(\\Phi(k,t)) = ik\\mu(t) - \\frac{1}{2}k^2\\sigma^2(t)$, which is the logarithm of the characteristic function of a Gaussian distribution with time-dependent mean $\\mu(t)$ and variance $\\sigma^2(t)$. By comparison, we identify:\n$$\n\\mu(t) = \\mu_0 \\exp(-\\gamma t)\n$$\n$$\n\\sigma^2(t) = \\sigma_0^2 \\exp(-2\\gamma t) + \\frac{D}{\\gamma}\\left(1 - \\exp(-2\\gamma t)\\right).\n$$\nSince the characteristic function $\\Phi(k,t)$ corresponds to a Gaussian distribution, the probability density $p(x,t)$ must be a Gaussian function of $x$ for all $t \\ge 0$. The general form of a Gaussian density with mean $\\mu$ and variance $\\sigma^2$ is $\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)$.\nTherefore, the desired solution $p(x,t)$ is a Gaussian distribution with the mean $\\mu(t)$ and variance $\\sigma^2(t)$ derived above:\n$$\np(x,t) = \\frac{1}{\\sqrt{2\\pi \\sigma^2(t)}} \\exp\\left(-\\frac{(x - \\mu(t))^2}{2\\sigma^2(t)}\\right).\n$$\nSubstituting the expressions for $\\mu(t)$ and $\\sigma^2(t)$ gives the final closed-form solution.",
            "answer": "$$\n\\boxed{\n\\frac{1}{\\sqrt{2\\pi \\left(\\sigma_0^2 \\exp(-2\\gamma t) + \\frac{D}{\\gamma} (1 - \\exp(-2\\gamma t))\\right)}} \\exp\\left( - \\frac{\\left(x - \\mu_0 \\exp(-\\gamma t)\\right)^2}{2 \\left(\\sigma_0^2 \\exp(-2\\gamma t) + \\frac{D}{\\gamma} (1 - \\exp(-2\\gamma t))\\right)} \\right)\n}\n$$"
        },
        {
            "introduction": "The theoretical tools you've practiced are essential for interpreting real-world data, where noise is ubiquitous but its sources are often hidden. This final practice moves from analytical derivation to computational data analysis, tackling a classic problem in systems biology: decomposing noise. Using paired time-series data, you will derive and implement a method to distinguish between *intrinsic* noise, inherent to a specific process, and *extrinsic* noise, arising from shared environmental fluctuations . This exercise demonstrates how the fundamental laws of probability and covariance can be transformed into a powerful tool for dissecting the architecture of complex biological systems.",
            "id": "4145310",
            "problem": "You are given paired time series of protein counts from two identical fluorescent reporter genes measured simultaneously in the same single cell over discrete times. Model the pair at time index $t$ as random variables $X_t$ and $Y_t$ representing counts for reporter $1$ and reporter $2$, respectively. Assume a latent extrinsic state $Z_t$ that affects both reporters identically at time $t$, and assume conditional independence of $X_t$ and $Y_t$ given $Z_t$. The task is to design a computation that decomposes the total observable variance of a single reporter into its intrinsic component and its extrinsic component using covariance, starting from first principles.\n\nFundamental base to use:\n- The law of total variance: for any random variable $X$ and a latent variable $Z$, $$\\mathrm{Var}(X) = \\mathbb{E}\\big[\\mathrm{Var}(X \\mid Z)\\big] + \\mathrm{Var}\\big(\\mathbb{E}[X \\mid Z]\\big).$$\n- Conditional independence and exchangeability of the two reporters given the extrinsic state: $$X \\perp\\!\\!\\!\\perp Y \\mid Z \\quad \\text{and} \\quad X \\mid Z \\stackrel{d}{=} Y \\mid Z.$$\n- Properties of covariance: for any two random variables $X$ and $Y$, $$\\mathrm{Cov}(X,Y) = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y].$$\n\nDefinitions for this problem:\n- Define the intrinsic variance component as $$\\sigma_{\\mathrm{int}}^2 \\equiv \\mathbb{E}\\big[\\mathrm{Var}(X \\mid Z)\\big].$$\n- Define the extrinsic variance component as $$\\sigma_{\\mathrm{ext}}^2 \\equiv \\mathrm{Var}\\big(\\mathbb{E}[X \\mid Z]\\big).$$\n- The observable total variance of a single reporter is $$\\sigma_{\\mathrm{tot}}^2 \\equiv \\mathrm{Var}(X).$$\n\nYour program must, for each paired time series, compute estimators of $\\sigma_{\\mathrm{int}}^2$, $\\sigma_{\\mathrm{ext}}^2$, and $\\sigma_{\\mathrm{tot}}^2$ using only sample statistics (means, variances, covariances) of the paired data. Begin your derivation from the law of total variance and the conditional independence assumptions, and derive estimators based on the covariance between the two reporters. Then implement the resulting computation.\n\nScientific realism and constraints:\n- The two reporters are identical and measured simultaneously, so the conditional distributions given the extrinsic state are the same, and the intrinsic noise sources are independent across reporters.\n- Because a variance is nonnegative, the final reported estimates must lie in the nonnegative orthant. If any unconstrained estimator yields a negative value due to sampling variability, project your estimates to satisfy $$\\sigma_{\\mathrm{int}}^2 \\ge 0, \\quad \\sigma_{\\mathrm{ext}}^2 \\ge 0, \\quad \\sigma_{\\mathrm{int}}^2 + \\sigma_{\\mathrm{ext}}^2 = \\sigma_{\\mathrm{tot}}^2.$$\n\nUnits:\n- Express all variance values in counts squared.\n\nAngle units:\n- No angles are involved.\n\nPercentages:\n- Do not express any outputs as percentages.\n\nTest suite:\nUse the following four paired time series, each of length $n = 10$, to exercise different regimes. All values are protein counts (nonnegative integers). For each case, $x$ denotes the series for reporter $1$ and $y$ denotes the series for reporter $2$.\n\n- Case $1$ (general case, mixed intrinsic and extrinsic): \n  $$x = [52,54,53,53,56,57,53,57,53,54], \\quad y = [49,55,54,50,60,55,54,58,50,57].$$\n- Case $2$ (near-zero covariance, predominantly intrinsic):\n  $$x = [42,38,43,37,41,39,44,36,42,38], \\quad y = [37,43,38,42,39,41,36,44,38,42].$$\n- Case $3$ (perfectly shared extrinsic, no intrinsic; reporters are identical):\n  $$x = [20,22,25,23,21,24,26,22,23,25], \\quad y = [20,22,25,23,21,24,26,22,23,25].$$\n- Case $4$ (negative covariance due to anti-correlation; tests projection to nonnegativity):\n  $$x = [10,20,30,40,50,60,70,80,90,100], \\quad y = [100,90,80,70,60,50,40,30,20,10].$$\n\nRequired final output format:\n- Your program should produce a single line of output containing the results for the four cases as a comma-separated list enclosed in square brackets. Each element of this list must itself be a list of three floating-point numbers $[\\sigma_{\\mathrm{int}}^2, \\sigma_{\\mathrm{ext}}^2, \\sigma_{\\mathrm{tot}}^2]$ rounded to six decimal places and expressed in counts squared. For example, output should look like $$[[a_1,b_1,c_1],[a_2,b_2,c_2],[a_3,b_3,c_3],[a_4,b_4,c_4]].$$",
            "solution": "The problem statement is evaluated as valid. It presents a well-posed, scientifically grounded problem from the field of systems biology concerning the decomposition of variance in gene expression. The premises are physically motivated, mathematically sound, and the task is to derive and implement estimators based on fundamental principles of probability theory. There are no contradictions, ambiguities, or factual inaccuracies.\n\nThe solution proceeds by first deriving the theoretical relationships between the desired variance components and observable sample statistics, namely the covariance. These relationships are then used to construct estimators, which are implemented in the final program.\n\n**1. Theoretical Derivation**\n\nThe objective is to find expressions for the intrinsic variance, $\\sigma_{\\mathrm{int}}^2$, and extrinsic variance, $\\sigma_{\\mathrm{ext}}^2$, in terms of measurable quantities from the paired time series data for reporters $X$ and $Y$. The cornerstone of this derivation is the analysis of the covariance between $X$ and $Y$, $\\mathrm{Cov}(X, Y)$.\n\nWe begin with the Law of Total Covariance, which states for any random variables $X$, $Y$, and $Z$:\n$$\n\\mathrm{Cov}(X, Y) = \\mathbb{E}[\\mathrm{Cov}(X, Y \\mid Z)] + \\mathrm{Cov}(\\mathbb{E}[X \\mid Z], \\mathbb{E}[Y \\mid Z])\n$$\n\nThe problem provides two critical assumptions about the relationship between the reporters $X$ and $Y$ and the latent extrinsic state $Z$:\n\n1.  **Conditional Independence**: Given the state $Z$, the reporters are independent, i.e., $X \\perp\\!\\!\\!\\perp Y \\mid Z$. By definition, the covariance of two independent variables is $0$. Therefore, the conditional covariance $\\mathrm{Cov}(X, Y \\mid Z)$ is $0$ for any given state of $Z$. This implies that the expectation of the conditional covariance is also zero:\n    $$\n    \\mathbb{E}[\\mathrm{Cov}(X, Y \\mid Z)] = \\mathbb{E}[0] = 0\n    $$\n    Substituting this into the law of total covariance simplifies the expression to:\n    $$\n    \\mathrm{Cov}(X, Y) = \\mathrm{Cov}(\\mathbb{E}[X \\mid Z], \\mathbb{E}[Y \\mid Z])\n    $$\n\n2.  **Exchangeability (Identical Reporters)**: The reporters are identical and respond to the extrinsic state in the same way, meaning their conditional probability distributions given $Z$ are identical, i.e., $X \\mid Z \\stackrel{d}{=} Y \\mid Z$. A direct consequence is that their conditional expectations are equal:\n    $$\n    \\mathbb{E}[X \\mid Z] = \\mathbb{E}[Y \\mid Z]\n    $$\n    Substituting this into our simplified covariance expression yields:\n    $$\n    \\mathrm{Cov}(X, Y) = \\mathrm{Cov}(\\mathbb{E}[X \\mid Z], \\mathbb{E}[X \\mid Z])\n    $$\n    The covariance of a random variable with itself is its variance, $\\mathrm{Cov}(A, A) = \\mathrm{Var}(A)$. Therefore,\n    $$\n    \\mathrm{Cov}(X, Y) = \\mathrm{Var}(\\mathbb{E}[X \\mid Z])\n    $$\n\nThe problem defines the extrinsic variance component as $\\sigma_{\\mathrm{ext}}^2 \\equiv \\mathrm{Var}(\\mathbb{E}[X \\mid Z])$. Thus, we have derived the central result: the extrinsic variance is equal to the covariance between the two reporter signals.\n$$\n\\sigma_{\\mathrm{ext}}^2 = \\mathrm{Cov}(X, Y)\n$$\n\nWith the extrinsic component identified, we can find the intrinsic component using the Law of Total Variance, which is given as a fundamental principle:\n$$\n\\mathrm{Var}(X) = \\mathbb{E}[\\mathrm{Var}(X \\mid Z)] + \\mathrm{Var}(\\mathbb{E}[X \\mid Z])\n$$\nUsing the problem's definitions, this is:\n$$\n\\sigma_{\\mathrm{tot}}^2 = \\sigma_{\\mathrm{int}}^2 + \\sigma_{\\mathrm{ext}}^2\n$$\nRearranging for the intrinsic variance gives:\n$$\n\\sigma_{\\mathrm{int}}^2 = \\sigma_{\\mathrm{tot}}^2 - \\sigma_{\\mathrm{ext}}^2\n$$\nSubstituting our findings and definitions, $\\sigma_{\\mathrm{tot}}^2 = \\mathrm{Var}(X)$ and $\\sigma_{\\mathrm{ext}}^2 = \\mathrm{Cov}(X, Y)$, we get:\n$$\n\\sigma_{\\mathrm{int}}^2 = \\mathrm{Var}(X) - \\mathrm{Cov}(X, Y)\n$$\n\n**2. Construction of Estimators**\n\nThe theoretical relationships derived above guide the construction of estimators from the sample data, denoted here by time series $\\{x_t\\}_{t=1}^n$ and $\\{y_t\\}_{t=1}^n$. We use sample statistics (denoted by a hat, $\\hat{\\cdot}$) to estimate the population parameters.\n\n-   **Extrinsic Variance Estimator**: Based on $\\sigma_{\\mathrm{ext}}^2 = \\mathrm{Cov}(X, Y)$, the unconstrained estimator for extrinsic variance is the sample covariance between the two time series:\n    $$\n    \\hat{\\sigma}_{\\mathrm{ext, u}}^2 = \\widehat{\\mathrm{Cov}}(X, Y)\n    $$\n-   **Total Variance Estimator**: The total variance is defined for a single reporter, $\\sigma_{\\mathrm{tot}}^2 = \\mathrm{Var}(X)$. Since the theory implies $\\mathrm{Var}(X) = \\mathrm{Var}(Y)$ due to exchangeability, a more robust estimator uses data from both reporters by averaging their sample variances. This reduces the impact of sampling noise.\n    $$\n    \\hat{\\sigma}_{\\mathrm{tot}}^2 = \\frac{1}{2} \\left( \\widehat{\\mathrm{Var}}(X) + \\widehat{\\mathrm{Var}}(Y) \\right)\n    $$\n-   **Intrinsic Variance Estimator**: Following the relationship $\\sigma_{\\mathrm{int}}^2 = \\sigma_{\\mathrm{tot}}^2 - \\sigma_{\\mathrm{ext}}^2$, the unconstrained estimator for intrinsic variance is:\n    $$\n    \\hat{\\sigma}_{\\mathrm{int, u}}^2 = \\hat{\\sigma}_{\\mathrm{tot}}^2 - \\hat{\\sigma}_{\\mathrm{ext, u}}^2\n    $$\n\nAll sample variances and covariances are computed using the unbiased formula with a denominator of $n-1$, where $n$ is the number of time points.\n\n**3. Projection to Non-Negative Orthant**\n\nSince variance must be non-negative, the estimators must be constrained to $\\hat{\\sigma}_{\\mathrm{int}}^2 \\ge 0$ and $\\hat{\\sigma}_{\\mathrm{ext}}^2 \\ge 0$. Sampling error can cause the unconstrained estimators $\\hat{\\sigma}_{\\mathrm{int, u}}^2$ or $\\hat{\\sigma}_{\\mathrm{ext, u}}^2$ to be negative. We apply a projection that enforces non-negativity while preserving the total variance identity, $\\hat{\\sigma}_{\\mathrm{int}}^2 + \\hat{\\sigma}_{\\mathrm{ext}}^2 = \\hat{\\sigma}_{\\mathrm{tot}}^2$.\n\nThe procedure is as follows:\n1.  Calculate the three unconstrained estimators: $\\hat{\\sigma}_{\\mathrm{tot}}^2$, $\\hat{\\sigma}_{\\mathrm{ext, u}}^2$, and $\\hat{\\sigma}_{\\mathrm{int, u}}^2 = \\hat{\\sigma}_{\\mathrm{tot}}^2 - \\hat{\\sigma}_{\\mathrm{ext, u}}^2$.\n2.  If $\\hat{\\sigma}_{\\mathrm{ext, u}}^2  0$: The negative sample covariance is unphysical for this model. We project it to the nearest valid value, which is $0$.\n    -   $\\hat{\\sigma}_{\\mathrm{ext}}^2 = 0$\n    -   $\\hat{\\sigma}_{\\mathrm{int}}^2 = \\hat{\\sigma}_{\\mathrm{tot}}^2$\n3.  Else if $\\hat{\\sigma}_{\\mathrm{int, u}}^2  0$ (which is equivalent to $\\hat{\\sigma}_{\\mathrm{ext, u}}^2  \\hat{\\sigma}_{\\mathrm{tot}}^2$): The intrinsic component cannot be negative. We project it to $0$.\n    -   $\\hat{\\sigma}_{\\mathrm{int}}^2 = 0$\n    -   $\\hat{\\sigma}_{\\mathrm{ext}}^2 = \\hat{\\sigma}_{\\mathrm{tot}}^2$\n4.  Otherwise (if $0 \\le \\hat{\\sigma}_{\\mathrm{ext, u}}^2 \\le \\hat{\\sigma}_{\\mathrm{tot}}^2$): The unconstrained estimates are physically valid and are retained.\n    -   $\\hat{\\sigma}_{\\mathrm{ext}}^2 = \\hat{\\sigma}_{\\mathrm{ext, u}}^2$\n    -   $\\hat{\\sigma}_{\\mathrm{int}}^2 = \\hat{\\sigma}_{\\mathrm{int, u}}^2$\n\nThe final algorithm will compute $\\hat{\\sigma}_{\\mathrm{tot}}^2$, $\\hat{\\sigma}_{\\mathrm{ext}}^2$, and $\\hat{\\sigma}_{\\mathrm{int}}^2$ for each test case according to this logic.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and decomposes the variance of reporter gene expression into\n    intrinsic and extrinsic components for several test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (general case, mixed intrinsic and extrinsic)\n        (\n            [52, 54, 53, 53, 56, 57, 53, 57, 53, 54],\n            [49, 55, 54, 50, 60, 55, 54, 58, 50, 57]\n        ),\n        # Case 2 (near-zero covariance, predominantly intrinsic)\n        (\n            [42, 38, 43, 37, 41, 39, 44, 36, 42, 38],\n            [37, 43, 38, 42, 39, 41, 36, 44, 38, 42]\n        ),\n        # Case 3 (perfectly shared extrinsic, no intrinsic)\n        (\n            [20, 22, 25, 23, 21, 24, 26, 22, 23, 25],\n            [20, 22, 25, 23, 21, 24, 26, 22, 23, 25]\n        ),\n        # Case 4 (negative covariance due to anti-correlation)\n        (\n            [10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n            [100, 90, 80, 70, 60, 50, 40, 30, 20, 10]\n        )\n    ]\n\n    results = []\n    for x_data, y_data in test_cases:\n        # Convert lists to numpy arrays for vectorized operations.\n        x = np.array(x_data, dtype=float)\n        y = np.array(y_data, dtype=float)\n\n        # The ddof=1 argument ensures the use of the unbiased sample\n        # variance/covariance (division by n-1).\n        # np.cov returns the 2x2 covariance matrix:\n        # [[Var(x), Cov(x,y)],\n        #  [Cov(y,x), Var(y)]]\n        cov_matrix = np.cov(x, y, ddof=1)\n        var_x = cov_matrix[0, 0]\n        var_y = cov_matrix[1, 1]\n        cov_xy = cov_matrix[0, 1]\n\n        # 1. Calculate unconstrained estimators based on the derivation.\n        # Total variance is estimated by averaging the reporters' variances.\n        sigma_tot_sq = 0.5 * (var_x + var_y)\n        \n        # Extrinsic variance is estimated by the covariance.\n        sigma_ext_sq_unconstrained = cov_xy\n        \n        # Intrinsic variance is the remainder.\n        sigma_int_sq_unconstrained = sigma_tot_sq - sigma_ext_sq_unconstrained\n\n        # 2. Apply projection to the non-negative orthant.\n        # These are the final, constrained estimators.\n        sigma_int_sq, sigma_ext_sq = 0.0, 0.0\n\n        if sigma_ext_sq_unconstrained  0:\n            # Case where sample covariance is negative (e.g., anti-correlation).\n            # This is unphysical for the model, so project to 0.\n            sigma_ext_sq = 0.0\n            sigma_int_sq = sigma_tot_sq\n        elif sigma_int_sq_unconstrained  0:\n            # Case where covariance is larger than total variance due to noise.\n            # Intrinsic noise cannot be negative, so project to 0.\n            sigma_int_sq = 0.0\n            sigma_ext_sq = sigma_tot_sq\n        else:\n            # The unconstrained estimates are physically valid.\n            sigma_int_sq = sigma_int_sq_unconstrained\n            sigma_ext_sq = sigma_ext_sq_unconstrained\n\n        # Assemble the triplet for the current case, rounded to 6 decimal places.\n        case_result = [\n            round(sigma_int_sq, 6),\n            round(sigma_ext_sq, 6),\n            round(sigma_tot_sq, 6)\n        ]\n        results.append(case_result)\n\n    # Format the final output string as specified in the problem statement.\n    # The output from Python's list-to-string conversion matches the required format.\n    # No extra spaces after commas.\n    results_str = str(results).replace(\" \", \"\")\n\n    print(results_str)\n\nsolve()\n```"
        }
    ]
}