{
    "hands_on_practices": [
        {
            "introduction": "要将整体论与还原论的讨论从哲学思辨提升到科学分析，我们需要精确的量化工具。本练习介绍了一种强大的方法——部分信息分解（PID），它使用信息论来剖析一个系统的整体（宏观变量）与其部分（微观变量）之间的关系。通过分析一个简单的异或门（XOR gate），你将亲手计算出系统中的“协同”与“冗余”，从而清晰地看到，有时整体所包含的信息确实会大于各部分信息之和。",
            "id": "4139452",
            "problem": "考虑一个双输入布尔系统中的微观到宏观映射，其中微观层面的变量为 $X_1 \\in \\{0,1\\}$ 和 $X_2 \\in \\{0,1\\}$，宏观层面的变量为 $Y = X_1 \\oplus X_2$，这里的 $\\oplus$ 表示异或。假设 $X_1$ 和 $X_2$ 是独立的，并且各自是参数为 $1/2$ 的伯努利随机变量，即 $X_1 \\sim \\text{Bernoulli}(1/2)$ 和 $X_2 \\sim \\text{Bernoulli}(1/2)$。使用以 2 为底的对数的香农熵和互信息，因此所有信息量均以比特为单位度量。从熵和互信息的核心定义出发，即 $H(Z) = -\\sum_{z} p(z)\\log_2 p(z)$ 和 $I(U;V) = H(U) - H(U|V)$，并在适当时使用熵和互信息的链式法则。\n\n采用部分信息分解（PID）框架以及 Williams 和 Beer 的最小互信息冗余度量，其中 $X_1$ 和 $X_2$ 关于 $Y$ 的冗余度定义为 $R = \\min\\{I(Y;X_1), I(Y;X_2)\\}$。在此框架下，总互信息 $I(Y;X_1,X_2)$ 分解为冗余、唯一贡献和协同。从第一性原理出发，为该系统推导协同项 $S$ 和冗余项 $R$，不要使用任何非从给定的核心定义和 PID 假设推导出的快捷公式。\n\n解释所得数值如何反映在复杂自适应系统建模中还原论与整体论之间的张力，具体方法是阐释宏观层面 $Y$ 对 $(X_1,X_2)$ 的依赖性是否可以通过单独分析微观层面变量来捕捉，还是需要一个联合的、整体性的解释。\n\n将 $S$ 和 $R$ 的最终数值以比特为单位，表示为行向量 $\\begin{pmatrix} S  R \\end{pmatrix}$。无需四舍五入，但如果选择对任何中间计算进行四舍五入，请确保报告的任何值都保留四位有效数字。",
            "solution": "用户要求为一个简单的布尔系统解决一个与部分信息分解（PID）相关的问题。\n\n我的计划如下：\n1.  **验证问题：**\n    *   提取所有给定信息。\n    *   检查其科学合理性、内部一致性和客观性。这似乎是一个在特定框架（PID）背景下的标准信息论问题，因此应该是有效的。\n2.  **如果有效，则进行求解：**\n    *   **第1步：描述系统特性。**\n        *   列出可能的微观状态 $(X_1, X_2)$。\n        *   确定每个微观状态的概率。由于 $X_1$ 和 $X_2$ 是独立的伯努利(1/2)变量，四个微观状态 $(0,0), (0,1), (1,0), (1,1)$ 中每一个的概率都是 $(1/2) \\times (1/2) = 1/4$。\n        *   列出可能的宏观状态 $Y = X_1 \\oplus X_2$。$Y$ 可以是 $0$ 或 $1$。\n        *   确定 $Y$ 的概率分布。\n            *   $P(Y=0) = P(X_1=0, X_2=0) + P(X_1=1, X_2=1) = 1/4 + 1/4 = 1/2$。\n            *   $P(Y=1) = P(X_1=0, X_2=1) + P(X_1=1, X_2=0) = 1/4 + 1/4 = 1/2$。\n    *   **第2步：从第一性原理计算必要的信息论量。**\n        *   计算宏观变量的熵 $H(Y)$。\n        *   分别计算宏观变量与每个微观变量之间的互信息 $I(Y;X_1)$ 和 $I(Y;X_2)$。\n        *   计算宏观变量与联合微观变量之间的总互信息 $I(Y;X_1, X_2)$。\n    *   **第3步：应用 PID 框架。**\n        *   使用给定的冗余度定义：$R = \\min\\{I(Y;X_1), I(Y;X_2)\\}$。\n        *   问题陈述总互信息分解为 $I(Y;X_1, X_2) = R + U_1 + U_2 + S$，其中 $U_1$ 是来自 $X_1$ 的唯一信息，$U_2$ 是来自 $X_2$ 的唯一信息，$S$ 是协同。\n        *   问题也通过标准的 PID 关系隐含地定义了唯一信息分量：\n            *   $I(Y;X_1) = R + U_1$\n            *   $I(Y;X_2) = R + U_2$\n        *   将这些代入总信息分解中：\n            *   $I(Y;X_1, X_2) = R + (I(Y;X_1) - R) + (I(Y;X_2) - R) + S$\n            *   $I(Y;X_1, X_2) = I(Y;X_1) + I(Y;X_2) - R + S$\n        *   因此，协同 $S$ 可以计算为：$S = I(Y;X_1, X_2) - I(Y;X_1) - I(Y;X_2) + R$。\n    *   **第4步：计算 R 和 S。**\n        *   将计算出的 $I(Y;X_1)$、$I(Y;X_2)$ 和 $I(Y;X_1, X_2)$ 的值代入 $R$ 和 $S$ 的公式中。\n    *   **第5步：解释结果。**\n        *   在还原论与整体论的背景下，讨论计算出的 $R$ 和 $S$ 值的含义。\n            *   $R > 0$ 表明部分信息由各部分冗余地提供。\n            *   $S > 0$ 表明整体大于其各部分之和；信息只有在同时考虑各部分时才可用。\n            *   在这个异或案例中，我预计协同会很高，冗余会很低（或为零）。只知道 $X_1$ 并不能告诉你任何关于 $Y$ 的信息，只知道 $X_2$ 也不能。但同时知道 $X_1$ 和 $X_2$ 就能完全确定 $Y$。这是协同的经典例子。\n3.  **最终答案的构建：**\n    *   以 `pmatrix` 形式呈现 $S$ 和 $R$ 的最终值。\n\n---\n**详细计算**\n\n**1. 概率分布**\n*   $P(X_1=0) = 1/2$, $P(X_1=1) = 1/2$。\n*   $P(X_2=0) = 1/2$, $P(X_2=1) = 1/2$。\n*   由于它们是独立的，联合分布 $P(X_1, X_2)$ 为：\n    *   $P(0,0) = 1/4$\n    *   $P(0,1) = 1/4$\n    *   $P(1,0) = 1/4$\n    *   $P(1,1) = 1/4$\n*   宏观变量 $Y = X_1 \\oplus X_2$：\n    *   如果 $(X_1,X_2) \\in \\{(0,0), (1,1)\\}$，则 $Y=0$。所以，$P(Y=0) = 1/4 + 1/4 = 1/2$。\n    *   如果 $(X_1,X_2) \\in \\{(0,1), (1,0)\\}$，则 $Y=1$。所以，$P(Y=1) = 1/4 + 1/4 = 1/2$。\n\n**2. 熵**\n*   $H(Y) = -\\sum_{y \\in \\{0,1\\}} P(Y=y)\\log_2 P(Y=y)$\n    *   $H(Y) = - [ (1/2)\\log_2(1/2) + (1/2)\\log_2(1/2) ]$\n    *   $H(Y) = - [ (1/2)(-1) + (1/2)(-1) ] = -(-1) = 1$ 比特。\n*   $H(X_1) = - [ (1/2)\\log_2(1/2) + (1/2)\\log_2(1/2) ] = 1$ 比特。\n*   $H(X_2) = - [ (1/2)\\log_2(1/2) + (1/2)\\log_2(1/2) ] = 1$ 比特。\n*   $H(X_1, X_2) = -\\sum_{x_1,x_2} P(x_1,x_2)\\log_2 P(x_1,x_2)$\n    *   $H(X_1, X_2) = - [ (1/4)\\log_2(1/4) + (1/4)\\log_2(1/4) + (1/4)\\log_2(1/4) + (1/4)\\log_2(1/4) ]$\n    *   $H(X_1, X_2) = - [ 4 \\times (1/4)\\log_2(1/4) ] = -\\log_2(1/4) = -(-2) = 2$ 比特。\n    *   另外，由于 $X_1$ 和 $X_2$ 是独立的，所以 $H(X_1, X_2) = H(X_1) + H(X_2) = 1 + 1 = 2$ 比特。\n\n**3. 互信息计算**\n*   **$I(Y;X_1)$**:\n    *   $I(Y;X_1) = H(Y) - H(Y|X_1)$。\n    *   需要计算 $H(Y|X_1) = -\\sum_{x_1,y} P(x_1,y)\\log_2 P(y|x_1)$。\n    *   让我们找到条件分布 $P(Y|X_1)$：\n        *   如果 $X_1=0$：$Y = 0 \\oplus X_2 = X_2$。所以在给定 $X_1=0$ 的条件下，$Y$ 的分布与 $X_2$ 的分布相同，即伯努利(1/2)。$P(Y=0|X_1=0)=1/2$, $P(Y=1|X_1=0)=1/2$。\n        *   如果 $X_1=1$：$Y = 1 \\oplus X_2$。如果 $X_2=0, Y=1$。如果 $X_2=1, Y=0$。所以 $Y$ 同样是一个伯努利(1/2)随机变量。$P(Y=0|X_1=1)=1/2$, $P(Y=1|X_1=1)=1/2$。\n    *   $H(Y|X_1=0) = -[(1/2)\\log_2(1/2) + (1/2)\\log_2(1/2)] = 1$ 比特。\n    *   $H(Y|X_1=1) = -[(1/2)\\log_2(1/2) + (1/2)\\log_2(1/2)] = 1$ 比特。\n    *   $H(Y|X_1) = \\sum_{x_1} P(x_1) H(Y|X_1=x_1) = P(X_1=0)H(Y|X_1=0) + P(X_1=1)H(Y|X_1=1)$\n    *   $H(Y|X_1) = (1/2)(1) + (1/2)(1) = 1$ 比特。\n    *   所以，$I(Y;X_1) = H(Y) - H(Y|X_1) = 1 - 1 = 0$ 比特。这是合理的：如果另一个输入是随机的，知道一个输入的值并不能告诉你任何关于异或输出的信息。\n*   **$I(Y;X_2)$**:\n    *   根据对称性，计算是相同的。$I(Y;X_2) = H(Y) - H(Y|X_2) = 1 - 1 = 0$ 比特。\n*   **$I(Y;X_1, X_2)$**:\n    *   这是输入提供给输出的总信息。\n    *   $I(Y;X_1,X_2) = H(Y) - H(Y|X_1,X_2)$。\n    *   我们需要 $H(Y|X_1,X_2)$。\n    *   给定一个特定的微观状态 $(x_1, x_2)$，$Y = x_1 \\oplus x_2$ 的值是完全确定的。例如，如果 $(X_1,X_2)=(0,1)$，那么 $Y$ 保证为 $1$。概率分布 $P(Y|X_1=x_1, X_2=x_2)$ 是一个在某个值上的脉冲（概率为1），其他地方为零。\n    *   这样一个确定性分布的熵是 $H(Y|X_1=x_1, X_2=x_2) = -[1\\log_2(1) + 0\\log_2(0)] = 0$。注意：我们定义 $0\\log 0 = 0$。\n    *   因为这对任何特定的微观状态 $(x_1, x_2)$ 都成立，所以平均条件熵也为零：$H(Y|X_1,X_2) = \\sum_{x_1,x_2} P(x_1,x_2) H(Y|X_1=x_1, X_2=x_2) = 0$。\n    *   因此，$I(Y;X_1,X_2) = H(Y) - H(Y|X_1,X_2) = 1 - 0 = 1$ 比特。\n    *   另外，使用互信息的另一个公式：$I(Y;X_1,X_2) = H(X_1,X_2) - H(X_1,X_2|Y)$。\n        *   $H(X_1, X_2) = 2$ 比特。\n        *   我们需要 $H(X_1,X_2|Y)$。\n        *   如果 $Y=0$，可能的微观状态是 $(0,0)$ 和 $(1,1)$。在总共四种可能性中有两种。因为 $P(Y=0)=1/2$，所以 $P((0,0)|Y=0) = P(0,0, Y=0)/P(Y=0) = P(0,0)/P(Y=0) = (1/4)/(1/2) = 1/2$。类似地，$P((1,1)|Y=0) = 1/2$。所以，$H(X_1,X_2|Y=0) = -[(1/2)\\log_2(1/2) + (1/2)\\log_2(1/2)] = 1$ 比特。\n        *   如果 $Y=1$，可能的微观状态是 $(0,1)$ 和 $(1,0)$。类似地，$H(X_1,X_2|Y=1) = 1$ 比特。\n        *   $H(X_1,X_2|Y) = P(Y=0)H(X_1,X_2|Y=0) + P(Y=1)H(X_1,X_2|Y=1) = (1/2)(1) + (1/2)(1) = 1$ 比特。\n        *   所以，$I(Y;X_1,X_2) = H(X_1,X_2) - H(X_1,X_2|Y) = 2 - 1 = 1$ 比特。这证实了之前的结果。\n\n**4. PID 计算**\n*   **冗余度 ($R$)**:\n    *   $R = \\min\\{I(Y;X_1), I(Y;X_2)\\}$\n    *   $R = \\min\\{0, 0\\} = 0$ 比特。\n*   **协同 ($S$)**:\n    *   问题没有直接给出 $S$ 的公式，但它给出了分解式：$I(Y;X_1,X_2) = S + R + U_1 + U_2$。\n    *   并且 PID 的一个标准部分是 $I(Y;X_1) = R + U_1$ 和 $I(Y;X_2) = R + U_2$。\n    *   结合这些，我们得到 $I(Y;X_1,X_2) = S + (R+U_1) + U_2 = S + I(Y;X_1) + U_2$。\n    *   而 $U_2 = I(Y;X_2) - R$。\n    *   所以，$I(Y;X_1,X_2) = S + I(Y;X_1) + I(Y;X_2) - R$。\n    *   这就给出了协同的公式：$S = I(Y;X_1,X_2) - I(Y;X_1) - I(Y;X_2) + R$。\n    *   代入数值：\n    *   $S = 1 - 0 - 0 + 0 = 1$ 比特。\n\n**5. 解释**\n*   冗余度，$R=0$ 比特。这意味着没有关于输出 $Y$ 的信息可以*同时*从 $X_1$ 和 $X_2$ 各自单独获得。这是正确的，因为单独知道 $X_1$ 的状态会使 $Y$ 的状态完全不确定（50/50 的概率），对 $X_2$ 也是如此。一种孤立地检查每个输入对输出影响的还原论方法，会发现它们之间没有关系（$I=0$）。\n*   协同，$S=1$ 比特。这意味着关于 $Y$ 的 1 比特信息*只有*在 $X_1$ 和 $X_2$ 被一同考虑时才会出现。这是系统的全部信息内容，因为 $I(Y;X_1,X_2) = 1$ 比特。整体不仅仅是其各部分之和；在这种情况下，整体*就是*信息，而各部分单独贡献为*零*。\n*   这完美地说明了一个整体性系统。独立地分析微观层面的变量 $X_1$ 和 $X_2$（一种还原论方法）完全无法解释宏观层面 $Y$ 的行为。$Y$ 对其输入的依赖是纯粹协同的，需要对 $(X_1, X_2)$ 进行联合的、整体性的解释才能被理解。异或门是信息论中协同交互的典型例子。\n\n**对步骤和逻辑的最终检查：**\n*   问题验证：问题定义明确，科学上合理（这是一个标准的信息论练习），且客观。它指定了对数的底和要使用的确切定义。问题是有效的。\n*   计算：\n    *   概率是正确的。\n    *   $H(Y)=1$。正确。\n    *   $I(Y;X_1)=0$。正确。\n    *   $I(Y;X_2)=0$。正确。\n    *   $I(Y;X_1, X_2)=1$。正确。\n    *   $R = \\min(0,0)=0$。正确。\n    *   $S = I(Y;X_1,X_2) - (I(Y;X_1)+I(Y;X_2)-R) = 1 - (0+0-0)=1$。正确。公式是 $S = I(Y;X_1,X_2) - I(Y;X_1) - I(Y;X_2) + R$。等等，标准的分解是 $I(Y;X_1,X_2) = U_1 + U_2 + R + S$。并且 $I(Y;X_1) = U_1 + R$ 和 $I(Y;X_2) = U_2 + R$。将后两者相加得到 $I(Y;X_1) + I(Y;X_2) = U_1 + U_2 + 2R$。所以 $U_1+U_2 = I(Y;X_1)+I(Y;X_2)-2R$。将此代入第一个方程：$I(Y;X_1,X_2) = (I(Y;X_1)+I(Y;X_2)-2R) + R + S = I(Y;X_1)+I(Y;X_2) - R + S$。所以我推导的 $S$ 的公式是正确的：$S = I(Y;X_1,X_2) - I(Y;X_1) - I(Y;X_2) + R$。\n    *   在一些文献中，所谓的“协同”也被定义为协信息（co-information）$S_{co-info} = I(Y;X_1) + I(Y;X_2) - I(Y;X_1, X_2)$。在这种情况下，它将是 $0 + 0 - 1 = -1$。但是问题指定了 Williams 和 Beer 的 PID 框架，在该框架下，我推导出的 $S$ 公式是正确的。\n*   解释：该解释与整体论和还原论的概念完美契合。一种对 $I(Y;X_1)$ 和 $I(Y;X_2)$ 的还原论分析会得出没有关系的结论，而一种对 $I(Y;X_1,X_2)$ 的整体论分析则揭示了完全的确定性。PID 框架通过将全部信息归于协同项来量化这一点。\n*   最终答案格式：行向量 $\\begin{pmatrix} S  R \\end{pmatrix}$。我计算出的值是 $S=1$ 和 $R=0$。所以最终答案应该是 $\\begin{pmatrix} 1  0 \\end{pmatrix}$。\n\n根据用户的规范，解决方案是完整和正确的。我现在将把它写出来，确保遵循所有格式规则。所有的数学实体都必须使用 LaTeX。这将需要对细节的仔细关注。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1  0\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "在复杂系统建模中，我们常常希望从详尽的微观模型中推导出一个更简洁的宏观模型，但这何时才是有效的呢？本练习将带你探讨“马尔可夫链的可集总性”（lumpability）这一核心概念，它为我们提供了从还原论描述到整体论描述的数学桥梁。通过推导精确集总的代数条件 ，你将理解在何种条件下，系统的宏观动态可以被视为一个独立的马尔可夫过程，而不丢失关键的预测能力。",
            "id": "4139517",
            "problem": "考虑一个在有限微观状态空间 $X = \\{1,2,3,4,5,6\\}$ 上的离散时间马尔可夫链，其转移核 $P$ 由下式给出：\n$$\nP \\;=\\;\n\\begin{pmatrix}\n\\frac{1}{4}  \\frac{1}{4}  \\frac{1}{6}  \\frac{1}{6}  \\frac{1}{12}  \\frac{1}{12} \\\\\n\\frac{1}{5}  \\frac{3}{10}  \\frac{1}{6}  \\frac{1}{6}  \\frac{1}{12}  \\frac{1}{12} \\\\\n\\frac{1}{8}  \\frac{1}{8}  \\frac{2}{5}  \\frac{1}{10}  \\frac{1}{8}  \\frac{1}{8} \\\\\n\\frac{1}{10}  \\frac{3}{20}  \\frac{1}{5}  \\frac{3}{10}  \\frac{1}{8}  \\frac{1}{8} \\\\\n\\frac{1}{6}  \\frac{1}{6}  \\frac{1}{12}  \\frac{1}{12}  \\frac{1}{4}  \\frac{1}{4} \\\\\n\\frac{1}{12}  \\frac{1}{4}  \\frac{1}{12}  \\frac{1}{12}  \\frac{1}{8}  \\frac{3}{8}\n\\end{pmatrix}.\n$$\n设粗粒化宏观状态空间为 $Y = \\{A,B,C\\}$，它由 $X$ 的一个划分产生，该划分将 $X$ 分为三个块 $C_A = \\{1,2\\}$、$C_B = \\{3,4\\}$ 和 $C_C = \\{5,6\\}$。聚合映射 $\\rho : X \\to Y$ 定义为：若 $i \\in C_A$，则 $\\rho(i) = A$；若 $i \\in C_B$，则 $\\rho(i) = B$；若 $i \\in C_C$，则 $\\rho(i) = C$。在复杂自适应系统（CAS）的建模中，这种聚合代表了由聚集的微观状态在宏观层面上产生的整体性描述。\n\n仅从微观过程的马尔可夫性质和划分的精确可集结性定义出发，推导在 $Y$ 上的粗粒化马尔可夫转移核 $Q$，并陈述保证精确可集结性的关于 $P$ 和划分 $\\{C_A, C_B, C_C\\}$ 的代数充要条件。然后，对于上面给出的具体 $P$ 和划分，计算从宏观状态 $A$ 到宏观状态 $B$ 的单个粗粒化转移概率。\n\n将你的最终答案表示为最简分数。无需四舍五入。",
            "solution": "该问题要求我们首先推导马尔可夫链精确可集结性的一般条件以及由此产生的粗粒化转移核的形式，然后应用该理论，根据给定的微观转移矩阵 $P$ 及其状态空间的划分，计算一个特定的转移概率。\n\n设 $\\{X_t\\}_{t \\geq 0}$ 是在微观状态空间 $X = \\{1, 2, 3, 4, 5, 6\\}$ 上的离散时间马尔可夫链，其转移矩阵为给定的 $6 \\times 6$ 矩阵 $P$。该矩阵的元素 $P_{ij}$ 表示在一个时间步内从状态 $i$ 转移到状态 $j$ 的概率，即 $P_{ij} = \\mathbb{P}(X_{t+1} = j | X_t = i)$。\n\n状态空间被划分为三个不相交的集合 $C_A = \\{1, 2\\}$、$C_B = \\{3, 4\\}$ 和 $C_C = \\{5, 6\\}$，它们定义了宏观状态 $Y = \\{A, B, C\\}$。聚合映射 $\\rho: X \\to Y$ 将每个微观状态映射到其对应的宏观状态。粗粒化或宏观过程定义为 $Y_t = \\rho(X_t)$。\n\n为使过程 $\\{Y_t\\}_{t \\geq 0}$ 成为一个马尔可夫链，其转移概率必须仅依赖于当前的宏观状态，而不是过程的历史。这个性质被称为精确可集结性。从宏观状态 $I \\in Y$ 到宏观状态 $J \\in Y$ 的转移概率记为 $Q_{IJ} = \\mathbb{P}(Y_{t+1} = J | Y_t = I)$。\n\n为了推导这个条件，我们可以通过对特定微观状态 $X_t = i$（其中 $i \\in C_I$）进行条件化来表示 $Q_{IJ}$。根据全概率公式，我们有：\n$$\nQ_{IJ} = \\mathbb{P}(Y_{t+1} = J | Y_t = I) = \\sum_{i \\in C_I} \\mathbb{P}(Y_{t+1} = J | Y_t = I, X_t = i) \\mathbb{P}(X_t = i | Y_t = I)\n$$\n由于事件 $X_t = i$ 包含了 $Y_t = I$ 的所有信息（甚至更多），并且因为底层过程 $\\{X_t\\}$ 是马尔可夫的，求和中的第一项可以简化为：\n$$\n\\mathbb{P}(Y_{t+1} = J | Y_t = I, X_t = i) = \\mathbb{P}(X_{t+1} \\in C_J | X_t = i) = \\sum_{j \\in C_J} \\mathbb{P}(X_{t+1} = j | X_t = i) = \\sum_{j \\in C_J} P_{ij}\n$$\n设 $\\pi_i^{(I)}(t) = \\mathbb{P}(X_t = i | Y_t = I)$ 是在时间 $t$ 宏观状态为 $I$ 的条件下，处于微观状态 $i$ 的条件概率。那么宏观转移概率为：\n$$\nQ_{IJ} = \\sum_{i \\in C_I} \\left( \\sum_{j \\in C_J} P_{ij} \\right) \\pi_i^{(I)}(t)\n$$\n为了使 $\\{Y_t\\}$ 成为一个马尔可夫过程，$Q_{IJ}$ 必须独立于过程的历史。历史可以影响宏观状态 $I$ 内微观状态的分布 $\\pi_i^{(I)}(t)$。要使 $Q_{IJ}$ 独立于此分布，项 $\\left( \\sum_{j \\in C_J} P_{ij} \\right)$ 对于起始宏观状态 $C_I$ 内的所有微观状态 $i$ 必须是常数。如果情况如此，我们可以将这个常数项从对 $i$ 的求和中提出来：\n$$\nQ_{IJ} = \\left( \\sum_{j \\in C_J} P_{ij} \\right) \\sum_{i \\in C_I} \\pi_i^{(I)}(t)\n$$\n由于 $\\sum_{i \\in C_I} \\pi_i^{(I)}(t) = \\sum_{i \\in C_I} \\mathbb{P}(X_t = i | Y_t = I) = 1$， $Q_{IJ}$ 的表达式变得与微观状态分布 $\\pi_i^{(I)}(t)$ 无关。\n\n因此，划分 $\\{C_I\\}_{I \\in Y}$ 相对于马尔可夫核 $P$ 是精确可集结的代数充要条件是：\n对于每一对宏观状态 $I, J \\in Y$，和 $\\sum_{j \\in C_J} P_{ij}$ 对于所有微观状态 $i \\in C_I$ 都是常数。\n\n如果满足此条件，在 $Y$ 上的粗粒化马尔可夫链的转移核 $Q$ 的元素由这个常数和给出：\n$$\nQ_{IJ} = \\sum_{j \\in C_J} P_{ij} \\quad \\text{对于任意 } i \\in C_I\n$$\n\n现在，我们必须计算从宏观状态 $A$ 到宏观状态 $B$ 的具体粗粒化转移概率，即 $Q_{AB}$。\n起始宏观状态是 $I=A$，所以初始微观状态集合是 $C_A = \\{1, 2\\}$。\n目标宏观状态是 $J=B$，所以最终微观状态集合是 $C_B = \\{3, 4\\}$。\n\n根据可集结性条件，对于所有 $i \\in C_A$，$\\sum_{j \\in C_B} P_{ij}$ 的值必须相同。我们来验证 $i=1$ 和 $i=2$ 的情况。\n这个和是 $P_{i3} + P_{i4}$。\n\n对于 $i=1$:\n$$\nP_{13} + P_{14} = \\frac{1}{6} + \\frac{1}{6} = \\frac{2}{6} = \\frac{1}{3}\n$$\n对于 $i=2$:\n$$\nP_{23} + P_{24} = \\frac{1}{6} + \\frac{1}{6} = \\frac{2}{6} = \\frac{1}{3}\n$$\n这些值确实相同，证实了对于从 $A$ 到 $B$ 的转移，该过程是可集结的。粗粒化转移概率 $Q_{AB}$ 就是这个共同的值。\n$$\nQ_{AB} = \\frac{1}{3}\n$$\n答案要求为最简分数，而 $\\frac{1}{3}$ 就是一个最简分数。",
            "answer": "$$\\boxed{\\frac{1}{3}}$$"
        },
        {
            "introduction": "还原论方法在实践中最大的诱惑之一是“平均场近似”，它通过忽略局部细节和相关性来简化问题。然而，这种简化有时会付出沉重的代价。本练习是一个编程实践，要求你构建一个反例，证明在一个具有特定网络模体（motif）的传染病模型中，朴素的平均场模型会做出与真实情况完全相反的预测 。通过亲手实现并对比两种模型，你将深刻体会到，忽略网络结构和非线性反馈等整体性特征，可能导致对系统行为的根本性误判。",
            "id": "4139484",
            "problem": "请构建一个具体的反例，其中朴素平均场近似在定性上错误地预测了在一个具有特定模体（motif）的网络上的非线性易感-感染-易感（SIS）过程的长期行为。该过程有 $N$ 个节点，节点状态为二元 $x_i(t) \\in \\{0,1\\}$，邻接矩阵为 $A \\in \\{0,1\\}^{N \\times N}$ 且对角线元素为零，感染强度参数为 $\\lambda \\gt 0$，恢复概率为 $\\mu \\in (0,1)$，以及一个协同参数 $s \\ge 0$，该参数在存在三元闭环的情况下会增强感染风险。\n\n基本原理和定义：\n- 在时间 $t$ 的每个易感节点 $i$ 在时间 $t+1$ 以如下概率被感染：\n$$\n\\pi_i(t) \\;=\\; 1 - \\exp\\!\\left(-\\lambda \\left[ k_i^A(t) \\;+\\; s\\,\\tau_i^A(t) \\right]\\right),\n$$\n其中 $k_i^A(t) = \\sum_{j} A_{ij}\\, x_j(t)$ 是节点 $i$ 的受感染邻居的数量，而 $\\tau_i^A(t) = \\sum_{j<k} A_{ij} A_{ik} A_{jk}\\, x_j(t)\\, x_k(t)$ 是节点 $i$ 的已感染邻居中也相互连接的对数（即围绕 $i$ 的三元闭环）。受感染的节点在每个时间步独立地以概率 $\\mu$ 恢复。\n- 朴素平均场近似忽略网络模体并假设独立性，用期望值替代局部计数。由此产生的关于预期感染比例 $p_t \\in [0,1]$ 的平均场映射为：\n$$\np_{t+1} \\;=\\; p_t \\;+\\; (1-p_t)\\left(1 - \\exp\\left(-\\lambda\\,\\bar{k}\\,p_t\\right)\\right) \\;-\\; \\mu\\,p_t,\n$$\n其中 $\\bar{k}$ 是平均度，$\\bar{k} = \\frac{1}{N}\\sum_{i=1}^N \\sum_{j=1}^N A_{ij}$。\n\n您的任务是：\n1. 在显式网络上实现上述定义的SIS过程的随机同步更新。对于每个时间步，为易感节点计算感染概率 $\\pi_i(t)$，抽取独立的感染事件，并为受感染的节点抽取独立的恢复事件，所有这些都基于时间 $t$ 的状态。对于一个固定的时间范围重复此过程。\n2. 从相同的初始感染比例 $p_0$ 开始，实现朴素平均场对 $p_t$ 的迭代，并迭代相同的时间范围。\n3. 使用阈值将长期定性行为分类：“灭绝”（extinct），如果最后 $50$ 个时间步的平均感染比例低于 $\\varepsilon = 0.05$；“地方性流行”（endemic），如果高于 $\\theta = 0.3$；否则为“中间状态”（intermediate）。如果网络模拟和平均场迭代产生不同的分类，则认为发生了定性失效（qualitative failure）。\n\n程序必须评估以下测试套件，该套件探索了一个反例和一些边界情况：\n\n- 测试用例1（带模体和非线性反馈的反例）：\n    - 网络：带三元闭环的环形网络，$N=30$，边 $A_{i,i\\pm 1}=1$ 和 $A_{i,i\\pm 2}=1$，索引对 $N$ 取模。\n    - 参数：$\\lambda = 0.04$，$\\mu = 0.2$，$s = 6.0$。\n    - 初始化：在 $t=0$ 时，受感染的节点为 $\\{0,1,2\\}$。\n    - 模拟：时间范围 $T=300$ 步；重复 $R=50$ 次独立运行并取平均值；伪随机种子为 $42$ 以保证可复现性。\n    - 预期现象：聚类引起的协同作用应导致持续存在（“地方性流行”），尽管朴素平均场预测会衰减（“灭绝”）。\n\n- 测试用例2（边界情况：无协同作用）：\n    - 与测试用例1相同的网络和初始化，但 $s = 0.0$。\n    - 所有其他参数均相同。\n\n- 测试用例3（边缘情况：无三元组的星形网络）：\n    - 网络：$N=30$ 个节点的星形网络，中心节点 $0$ 与所有其他节点相连，无其他边。\n    - 参数：$\\lambda = 0.04$，$\\mu = 0.2$，$s = 6.0$。\n    - 初始化：在 $t=0$ 时，受感染的节点为 $\\{0,1,2\\}$。\n    - 模拟：$T=300$，$R=50$，种子为 $123$。\n\n- 测试用例4（低聚类随机图）：\n    - 网络：Erdős–Rényi $G(N,p)$ 图，其中 $N=30$，$p=0.1$，使用种子 $7$ 生成，无向，无自环。\n    - 参数：$\\lambda = 0.04$，$\\mu = 0.2$，$s = 6.0$。\n    - 初始化：使用种子 $7$ 均匀随机选择 $3$ 个初始感染节点。\n    - 模拟：$T=300$，$R=50$，种子为 $7$。\n\n对于每个测试用例，计算：\n- 网络模拟的定性分类，基于 $R$ 次运行中最后 $50$ 个时间步的平均感染比例。\n- 平均场迭代的定性分类，基于最后 $50$ 个时间步的平均感染比例。\n- 返回一个布尔值，指示是否存在定性失效（如果分类不同则为True，否则为False）。\n\n您的程序应生成单行输出，其中包含一个方括号内的逗号分隔列表形式的结果（例如，\"[result1,result2,result3,result4]\"）。此问题不涉及物理单位；所有量均为无量纲的概率或计数。不涉及角度。不得使用百分比；任何分数都必须表示为 $[0,1]$ 区间内的小数。",
            "solution": "我们从网络上易感-感染-易感过程的定义开始，该过程具有二元节点状态 $x_i(t) \\in \\{0,1\\}$。邻接矩阵 $A$ 编码了相互作用。令 $k_i^A(t) = \\sum_j A_{ij} x_j(t)$ 为节点 $i$ 的受感染邻居的数量，并令 $\\tau_i^A(t) = \\sum_{j<k} A_{ij} A_{ik} A_{jk}\\, x_j(t)\\, x_k(t)$ 为相互连接的受感染邻居对的数量，即在时间 $t$ 涉及节点 $i$ 的三元闭环计数。感染概率由下式给出：\n$$\n\\pi_i(t) \\;=\\; 1 - \\exp\\!\\left(-\\lambda \\left[ k_i^A(t) \\;+\\; s\\,\\tau_i^A(t) \\right]\\right),\n$$\n这代表一个风险模型，其包含一个加性基线 $k_i^A(t)$ 和一个由网络模体产生的协同项 $s\\,\\tau_i^A(t)$。受感染的节点在每个时间步独立地以概率 $\\mu$ 恢复。同步更新使用时间 $t$ 的状态来计算所有的感染和恢复事件，并同时将所有节点更新到时间 $t+1$。\n\n朴素平均场近似从独立性假设开始：邻居的状态被近似为均值为 $p_t$（时间 $t$ 的感染比例）的独立伯努利随机变量。在此假设下并忽略网络模体，受感染邻居的期望数量是 $\\mathbb{E}[k_i^A(t)] = \\bar{k}\\,p_t$，其中 $\\bar{k}$ 是平均度。协同项 $\\tau_i^A(t)$ 涉及邻居之间的相关性，并依赖于三元闭环；忽略模体的朴素平均场实际上将其设置为零。因此，平均场映射变为：\n$$\np_{t+1} \\;=\\; p_t \\;+\\; (1-p_t)\\left(1 - \\exp\\left(-\\lambda\\,\\bar{k}\\,p_t\\right)\\right) \\;-\\; \\mu\\,p_t,\n$$\n这是一个关于 $p_t$ 的确定性递推关系。\n\n定性行为使用阈值进行分类。定义一个时间范围 $T$，并考虑最后 $50$ 个时间步的平均感染比例。如果平均值低于 $\\varepsilon = 0.05$，则分类为“灭绝”（extinct）；如果高于 $\\theta = 0.3$，则分类为“地方性流行”（endemic）；否则为“中间状态”（intermediate）。当网络模拟的分类与平均场分类不同时，就发生了定性失效。\n\n我们现在介绍测试用例1中反例的推理过程。该网络是一个带三元闭环的环形网络：每个节点都与其在两个方向上的第一和第二邻居有边连接，从而产生高的聚类系数。参数为 $\\lambda = 0.04$，$\\mu = 0.2$ 和 $s = 6.0$。初始化感染了由节点 $\\{0,1,2\\}$ 组成的三角形。对于这个图，朴素平均场使用 $\\bar{k} = 4$，因为每个节点连接到 $i\\pm 1$ 和 $i\\pm 2$，得出 $\\bar{k} = 4$。每个易感者的平均场风险大约是 $1 - \\exp(-\\lambda\\,\\bar{k}\\,p_t)$；对于小的 $p_t$，这接近于 $\\lambda\\,\\bar{k}\\,p_t$。在 $p_t$ 很小时的净增长由导数决定：\n$$\n\\frac{dp_{t+1}}{dp_t}\\Big|_{p_t \\approx 0} \\approx 1 - \\mu + \\lambda\\,\\bar{k} \\;=\\; 1 - 0.2 + 0.04 \\times 4 \\;=\\; 0.96,\n$$\n该值小于1，意味着向零衰减，并被分类为“灭绝”。然而，实际的网络过程不依赖于小的 $p_t$ 平均值；围绕初始感染三角形的三元闭环产生了一种协同风险。对于一个与两个相互连接的受感染邻居相邻的易感节点，其风险参数为\n$$\nH \\;=\\; \\lambda\\,\\left(k_i^A(t) + s\\,\\tau_i^A(t)\\right),\n$$\n如果 $k_i^A(t) = 2$ 且 $\\tau_i^A(t) = 1$（一个相连的受感染邻居对），则得出 $H = 0.04 \\times (2 + 6 \\times 1) = 0.32$，导致感染概率为 $1 - e^{-0.32} \\approx 0.274$。这种局部强化放大了三角形周围的感染。随着感染的传播，新的三角形与多个受感染的邻居一起形成，进一步增加了 $H$。这种由依赖于模体的项 $s\\,\\tau_i^A(t)$ 驱动的正反馈，将系统推向一个持续的高流行率状态。最终的长期平均感染比例超过了地方性流行阈值 $\\theta$，从而产生了“地方性流行”行为。\n\n这种差异证明了忽略模体诱导的相关性的简化论平均场假设的失败。整体性方法明确地考虑了网络结构和非线性反馈，捕捉了相连邻居对之间的协同效应。通过将邻接矩阵和三元闭环整合到动力学中，整体性模型识别出了诸如在聚类邻域内的强化等涌现现象。因此，整体论能更好地解释这个反例中观察到的动力学。\n\n边界和边缘情况证实了该分析。在测试用例2中，设置 $s=0$ 移除了非线性协同作用；该过程与平均场预测一致，产生“灭绝”。在测试用例3中，星形网络的三元闭环为零（对所有 $i$ 都有 $\\tau_i^A(t) = 0$），因此协同作用没有效果，行为与平均场相匹配。在测试用例4中，Erdős–Rényi 图的聚类性很低；即使 $s=6.0$，三角形的稀缺性也限制了协同作用，在给定参数下，两种方法都得出了相似的分类。\n\n算法设计：\n- 实现函数来构建指定的网络，并为每个节点计算 $k_i^A(t)$ 和 $\\tau_i^A(t)$。\n- 模拟随机网络SIS动力学 $R$ 次，在每个时间步上对所有运行的感染比例进行平均，并根据最后 $50$ 个时间步对结果进行分类。\n- 计算 $\\bar{k}$ 并对相同的时间范围迭代平均场映射，使用相同的阈值进行分类。\n- 对于每个测试用例，返回一个布尔值，指示定性分类是否不同，从而识别平均场的定性失效。\n\n最终程序按测试用例的顺序生成单行布尔值列表，展示了反例和边界行为。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef build_triadic_ring(N: int) -> np.ndarray:\n    \"\"\"Adjacency for ring with triadic closures: edges to i±1 and i±2 (mod N).\"\"\"\n    A = np.zeros((N, N), dtype=int)\n    for i in range(N):\n        for d in [1, 2]:\n            j1 = (i + d) % N\n            j2 = (i - d) % N\n            A[i, j1] = 1\n            A[j1, i] = 1\n            A[i, j2] = 1\n            A[j2, i] = 1\n    np.fill_diagonal(A, 0)\n    return A\n\ndef build_star(N: int) -> np.ndarray:\n    \"\"\"Star network: center 0 connected to all others.\"\"\"\n    A = np.zeros((N, N), dtype=int)\n    for j in range(1, N):\n        A[0, j] = 1\n        A[j, 0] = 1\n    np.fill_diagonal(A, 0)\n    return A\n\ndef build_er_graph(N: int, p: float, rng: np.random.RandomState) -> np.ndarray:\n    \"\"\"Undirected Erdos-Renyi graph G(N,p), no self-loops.\"\"\"\n    A = np.zeros((N, N), dtype=int)\n    for i in range(N):\n        for j in range(i + 1, N):\n            if rng.rand() < p:\n                A[i, j] = 1\n                A[j, i] = 1\n    np.fill_diagonal(A, 0)\n    return A\n\ndef neighbors_list(A: np.ndarray):\n    \"\"\"Precompute neighbor lists for each node.\"\"\"\n    N = A.shape[0]\n    neigh = []\n    for i in range(N):\n        neigh.append(np.where(A[i] == 1)[0])\n    return neigh\n\ndef count_triads_active(i: int, x: np.ndarray, A: np.ndarray, neigh_list) -> int:\n    \"\"\"Count number of connected infected neighbor pairs around node i.\"\"\"\n    neigh = neigh_list[i]\n    # Early exit if fewer than 2 neighbors\n    if len(neigh) < 2:\n        return 0\n    # Filter infected neighbors\n    infected_neigh = [j for j in neigh if x[j] == 1]\n    m = len(infected_neigh)\n    if m < 2:\n        return 0\n    # Count pairs among infected neighbors that are connected\n    triads = 0\n    for idx_a in range(m):\n        j = infected_neigh[idx_a]\n        for idx_b in range(idx_a + 1, m):\n            k = infected_neigh[idx_b]\n            if A[j, k] == 1:\n                triads += 1\n    return triads\n\ndef simulate_network(A: np.ndarray, lam: float, mu: float, s: float,\n                     initial_infected: np.ndarray, T: int, runs: int, seed: int) -> np.ndarray:\n    \"\"\"Simulate stochastic SIS with triadic synergy; return average prevalence over time.\"\"\"\n    N = A.shape[0]\n    neigh_list = neighbors_list(A)\n    prevalence_accum = np.zeros(T, dtype=float)\n    base_rng = np.random.RandomState(seed)\n    # Each run uses a derived seed for reproducibility\n    for r in range(runs):\n        rng = np.random.RandomState(base_rng.randint(0, 2**31 - 1))\n        x = np.zeros(N, dtype=int)\n        x[initial_infected] = 1\n        for t in range(T):\n            new_x = x.copy()\n            # Compute infection probabilities for susceptibles\n            for i in range(N):\n                if x[i] == 0:\n                    k_active = int(np.dot(A[i], x))\n                    triads_active = count_triads_active(i, x, A, neigh_list)\n                    hazard = lam * (k_active + s * triads_active)\n                    # Infection probability\n                    pi = 1.0 - np.exp(-hazard)\n                    if rng.rand() < pi:\n                        new_x[i] = 1\n                else:\n                    # Recovery\n                    if rng.rand() < mu:\n                        new_x[i] = 0\n            x = new_x\n            prevalence_accum[t] += x.mean()\n    prevalence_avg = prevalence_accum / runs\n    return prevalence_avg\n\ndef iterate_mean_field(bar_k: float, lam: float, mu: float, p0: float, T: int) -> np.ndarray:\n    \"\"\"Iterate naive mean-field map ignoring synergy.\"\"\"\n    p = p0\n    series = np.zeros(T, dtype=float)\n    for t in range(T):\n        series[t] = p\n        infection_term = (1.0 - p) * (1.0 - np.exp(-lam * bar_k * p))\n        recovery_term = mu * p\n        p = p + infection_term - recovery_term\n        # Clamp to [0,1]\n        if p < 0.0:\n            p = 0.0\n        elif p > 1.0:\n            p = 1.0\n    return series\n\ndef classify_long_run(series: np.ndarray, tail_len: int = 50, eps: float = 0.05, theta: float = 0.3) -> str:\n    \"\"\"Classify as 'extinct', 'endemic', or 'intermediate' based on tail average.\"\"\"\n    tail = series[-tail_len:]\n    avg = float(np.mean(tail))\n    if avg < eps:\n        return \"extinct\"\n    elif avg > theta:\n        return \"endemic\"\n    else:\n        return \"intermediate\"\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (name, adjacency_builder_or_matrix, params, init_infected, sim_params)\n    test_cases = []\n\n    # Test Case 1: Triadic ring, high synergy\n    N1 = 30\n    A1 = build_triadic_ring(N1)\n    lam1 = 0.04\n    mu1 = 0.2\n    s1 = 6.0\n    init1 = np.array([0, 1, 2], dtype=int)\n    T = 300\n    runs = 50\n    seed1 = 42\n    test_cases.append((\"triadic_high_synergy\", A1, lam1, mu1, s1, init1, T, runs, seed1))\n\n    # Test Case 2: Triadic ring, no synergy\n    s2 = 0.0\n    seed2 = 99  # distinct seed; not critical\n    test_cases.append((\"triadic_no_synergy\", A1, lam1, mu1, s2, init1, T, runs, seed2))\n\n    # Test Case 3: Star network, synergy ineffective\n    N3 = 30\n    A3 = build_star(N3)\n    lam3 = 0.04\n    mu3 = 0.2\n    s3 = 6.0\n    init3 = np.array([0, 1, 2], dtype=int)\n    seed3 = 123\n    test_cases.append((\"star_high_synergy\", A3, lam3, mu3, s3, init3, T, runs, seed3))\n\n    # Test Case 4: Erdos-Renyi low clustering\n    N4 = 30\n    rng_graph = np.random.RandomState(7)\n    A4 = build_er_graph(N4, p=0.1, rng=rng_graph)\n    lam4 = 0.04\n    mu4 = 0.2\n    s4 = 6.0\n    # Choose 3 infected nodes at random with seed 7\n    rng_init = np.random.RandomState(7)\n    init4 = rng_init.choice(N4, size=3, replace=False)\n    seed4 = 7\n    test_cases.append((\"er_low_clustering\", A4, lam4, mu4, s4, init4, T, runs, seed4))\n\n    results = []\n    for name, A, lam, mu, s, init, Tsteps, R, seed in test_cases:\n        # Network simulation\n        prevalence_net = simulate_network(A, lam, mu, s, init, Tsteps, R, seed)\n        cat_net = classify_long_run(prevalence_net, tail_len=50, eps=0.05, theta=0.3)\n        # Mean-field iteration\n        bar_k = float(np.mean(np.sum(A, axis=1)))\n        p0 = len(init) / A.shape[0]\n        prevalence_mf = iterate_mean_field(bar_k, lam, mu, p0, Tsteps)\n        cat_mf = classify_long_run(prevalence_mf, tail_len=50, eps=0.05, theta=0.3)\n        # Qualitative failure boolean\n        mismatch = (cat_net != cat_mf)\n        results.append(mismatch)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}