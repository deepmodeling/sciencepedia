{
    "hands_on_practices": [
        {
            "introduction": "This first exercise is a fundamental warm-up, designed to solidify your understanding of Shannon entropy. By computing the entropy for a given non-uniform distribution and comparing it to the maximum entropy of a uniform distribution, you will gain hands-on experience with the core formula and the concept of redundancy. This practice reinforces the core definition of entropy and the distinction between different information units (bits vs. nats) .",
            "id": "4126716",
            "problem": "In a complex adaptive system where agents adaptively select among actions based on experience, consider a single agent whose action-selection mechanism has converged to a discrete distribution over an action set of size $3$, with probabilities $p=(1/2,1/3,1/6)$ assigned to the three actions. Using the fundamental definition of Shannon entropy as the expected self-information and the standard conversion rules between natural logarithm (nats) and base-$2$ logarithm (bits), do the following: compute the entropy of this distribution in nats and in bits, and compare it to the entropy of the uniform distribution over $3$ outcomes. You must derive all expressions from first principles and clearly justify any base conversion used. Your final reported answer must be the exact closed-form expression for the difference in bits between the entropy of the uniform distribution and the entropy of $p$, namely $H(u)-H(p)$ in bits, where $u$ denotes the uniform distribution over $3$ outcomes. Do not round; provide an exact expression. The final answer must be a single expression with no units.",
            "solution": "The problem is subjected to validation before a solution is attempted.\n\n### Step 1: Extract Givens\n-   System: A single agent in a complex adaptive system.\n-   Action set size: $3$.\n-   Probability distribution of actions, $p$: $(1/2, 1/3, 1/6)$.\n-   Task 1: Compute the Shannon entropy of the distribution $p$ in nats and bits.\n-   Task 2: Compare the entropy of $p$ to the entropy of the uniform distribution over $3$ outcomes, denoted as $u$.\n-   Task 3: Derive all expressions from first principles and justify the conversion between logarithmic bases.\n-   Final Answer Requirement: Provide the exact closed-form expression for the difference $H(u) - H(p)$ in units of bits.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded:** The problem is firmly rooted in information theory, a core discipline in mathematics and physics, and is central to the quantitative study of complex systems. The concepts of Shannon entropy, self-information, and logarithmic base conversion are standard and well-established.\n-   **Well-Posed:** The problem is well-posed. The given probability distribution $p = (1/2, 1/3, 1/6)$ is valid, as the probabilities are non-negative and sum to unity: $\\frac{1}{2} + \\frac{1}{3} + \\frac{1}{6} = \\frac{3}{6} + \\frac{2}{6} + \\frac{1}{6} = \\frac{6}{6} = 1$. The tasks are specific and lead to a unique, meaningful solution.\n-   **Objective:** The problem statement is expressed in precise, objective language, free from ambiguity or subjective claims.\n\nThe problem does not exhibit any of the flaws listed in the validation criteria. It is scientifically sound, formally structured, internally consistent, and requires a rigorous application of fundamental principles.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be provided.\n\n### Solution Derivation\n\nThe self-information of an outcome $i$ occurring with probability $p_i$ is defined as $I(p_i) = -\\log(p_i)$. The base of the logarithm determines the units of information. For this problem, we will use base $e$ (natural logarithm, $\\ln$) for nats and base $2$ (binary logarithm, $\\log_2$) for bits.\n\nThe Shannon entropy, $H$, of a discrete probability distribution $P = \\{p_1, p_2, \\dots, p_n\\}$ is the expected value of the self-information over all possible outcomes:\n$$H(P) = E[I(p_i)] = \\sum_{i=1}^{n} p_i I(p_i) = -\\sum_{i=1}^{n} p_i \\log(p_i)$$\n\nFirst, we compute the entropy of the given distribution $p = (1/2, 1/3, 1/6)$ in nats. This requires using the natural logarithm. Let $H_e(p)$ be the entropy in nats.\n$$H_e(p) = -\\left( \\frac{1}{2} \\ln\\left(\\frac{1}{2}\\right) + \\frac{1}{3} \\ln\\left(\\frac{1}{3}\\right) + \\frac{1}{6} \\ln\\left(\\frac{1}{6}\\right) \\right)$$\nUsing the logarithmic identity $\\ln(1/x) = -\\ln(x)$:\n$$H_e(p) = \\frac{1}{2} \\ln(2) + \\frac{1}{3} \\ln(3) + \\frac{1}{6} \\ln(6)$$\nWe can simplify $\\ln(6)$ using the identity $\\ln(ab) = \\ln(a) + \\ln(b)$:\n$$H_e(p) = \\frac{1}{2} \\ln(2) + \\frac{1}{3} \\ln(3) + \\frac{1}{6} (\\ln(2) + \\ln(3))$$\nCollecting terms with $\\ln(2)$ and $\\ln(3)$:\n$$H_e(p) = \\left(\\frac{1}{2} + \\frac{1}{6}\\right) \\ln(2) + \\left(\\frac{1}{3} + \\frac{1}{6}\\right) \\ln(3)$$\n$$H_e(p) = \\left(\\frac{3}{6} + \\frac{1}{6}\\right) \\ln(2) + \\left(\\frac{2}{6} + \\frac{1}{6}\\right) \\ln(3)$$\n$$H_e(p) = \\frac{4}{6} \\ln(2) + \\frac{3}{6} \\ln(3) = \\frac{2}{3} \\ln(2) + \\frac{1}{2} \\ln(3)$$\nThis is the entropy of the distribution $p$ in nats.\n\nNext, we convert this entropy to bits. The conversion between logarithms of different bases $a$ and $b$ is given by the change of base formula: $\\log_b(x) = \\frac{\\log_a(x)}{\\log_a(b)}$. To convert from nats (base $e$) to bits (base $2$), we set $a=e$ and $b=2$:\n$$\\log_2(x) = \\frac{\\ln(x)}{\\ln(2)}$$\nThe entropy in bits, $H_2(P)$, is defined as $H_2(P) = -\\sum p_i \\log_2(p_i)$. Substituting the change of base formula:\n$$H_2(P) = -\\sum p_i \\frac{\\ln(p_i)}{\\ln(2)} = \\frac{1}{\\ln(2)} \\left( -\\sum p_i \\ln(p_i) \\right) = \\frac{H_e(P)}{\\ln(2)}$$\nThis is the fundamental justification for the conversion: the entropy in bits is the entropy in nats divided by the constant $\\ln(2)$.\nUsing this conversion, we find the entropy of $p$ in bits, $H_2(p)$:\n$$H_2(p) = \\frac{H_e(p)}{\\ln(2)} = \\frac{\\frac{2}{3} \\ln(2) + \\frac{1}{2} \\ln(3)}{\\ln(2)}$$\n$$H_2(p) = \\frac{2}{3} \\frac{\\ln(2)}{\\ln(2)} + \\frac{1}{2} \\frac{\\ln(3)}{\\ln(2)}$$\nUsing the change of base formula again, $\\frac{\\ln(3)}{\\ln(2)} = \\log_2(3)$.\n$$H_2(p) = \\frac{2}{3} + \\frac{1}{2} \\log_{2}(3)$$\nThis is the entropy of the distribution $p$ in bits.\n\nNow, we compute the entropy of the uniform distribution over $3$ outcomes, $u = (1/3, 1/3, 1/3)$, in bits. Let this be $H_2(u)$.\n$$H_2(u) = -\\sum_{i=1}^{3} u_i \\log_{2}(u_i) = -\\sum_{i=1}^{3} \\frac{1}{3} \\log_{2}\\left(\\frac{1}{3}\\right)$$\n$$H_2(u) = -3 \\cdot \\frac{1}{3} \\log_{2}\\left(\\frac{1}{3}\\right) = -\\log_{2}\\left(\\frac{1}{3}\\right)$$\nUsing the logarithmic identity $\\log(1/x) = -\\log(x)$:\n$$H_2(u) = \\log_{2}(3)$$\nThis confirms the general result that the entropy of a uniform distribution over $N$ states is $\\log_{2}(N)$ bits.\n\nFinally, we compute the required difference, $H(u) - H(p)$, in bits. This corresponds to $H_2(u) - H_2(p)$.\n$$H_2(u) - H_2(p) = \\log_{2}(3) - \\left(\\frac{2}{3} + \\frac{1}{2} \\log_{2}(3)\\right)$$\n$$H_2(u) - H_2(p) = \\log_{2}(3) - \\frac{2}{3} - \\frac{1}{2} \\log_{2}(3)$$\nCollecting the $\\log_{2}(3)$ terms:\n$$H_2(u) - H_2(p) = \\left(1 - \\frac{1}{2}\\right) \\log_{2}(3) - \\frac{2}{3}$$\n$$H_2(u) - H_2(p) = \\frac{1}{2} \\log_{2}(3) - \\frac{2}{3}$$\nThis is the final, exact, closed-form expression for the difference in entropy. This quantity is also known as the redundancy of the distribution $p$ or its Kullback-Leibler divergence from the uniform distribution.",
            "answer": "$$\\boxed{\\frac{1}{2} \\log_{2}(3) - \\frac{2}{3}}$$"
        },
        {
            "introduction": "Moving beyond simple unpredictability, this practice delves into the structural complexity of a stochastic process using the framework of computational mechanics. You will identify the 'causal states' of a system—its effective memory—and calculate the statistical complexity ($C_{\\mu}$), which quantifies the amount of historical information the process must store to optimally predict its future. This exercise provides a concrete example of deriving an $\\epsilon$-machine, the minimal optimal model of a process, and demonstrates how to quantify the complexity of a system's internal organization .",
            "id": "4126673",
            "problem": "Consider a stationary binary stochastic process $\\{X_t\\}_{t \\in \\mathbb{Z}}$ with alphabet $\\{0,1\\}$, generated by the following conditional distributions that depend on the parity of the number of consecutive $1$'s since the most recent $0$ in the semi-infinite past. Let $N_t$ denote the number of consecutive $1$'s in the past ending at time $t$ since the most recent $0$. The generation rule is:\n- If $N_t$ is even (including $N_t = 0$), then $\\mathbb{P}(X_{t+1} = 0 \\mid N_t \\text{ even}) = \\alpha$ and $\\mathbb{P}(X_{t+1} = 1 \\mid N_t \\text{ even}) = 1 - \\alpha$.\n- If $N_t$ is odd, then $\\mathbb{P}(X_{t+1} = 1 \\mid N_t \\text{ odd}) = 1$.\n\nAssume $0  \\alpha  1$ and that the process is in its stationary regime. For the specific parameter value $\\alpha = \\frac{2}{3}$:\n\n- Using the predictive equivalence relation over pasts, identify the causal states of this process.\n- Construct the minimal unifilar state-transition graph (the predictive model whose nodes are the causal states) by specifying, for each causal state, the outgoing transitions labeled by the emitted symbol and their probabilities, along with the destination causal state.\n- Determine the stationary distribution over the causal states induced by the transition dynamics and compute the statistical complexity $C_{\\mu}$ (the information storage in the causal-state distribution) in nats, using the standard definition of information and complexity appropriate to this context.\n\nExpress your final answer for $C_{\\mu}$ in nats and round your result to six significant figures.",
            "solution": "The problem statement is analyzed for validity.\n\n## Problem Validation\n\n### Step 1: Extract Givens\n- A stationary binary stochastic process $\\{X_t\\}_{t \\in \\mathbb{Z}}$ with alphabet $\\{0,1\\}$.\n- A variable $N_t$ is defined as the number of consecutive $1$'s in the past ending at time $t$ since the most recent $0$.\n- The conditional probability distribution for the next symbol $X_{t+1}$ depends on the parity of $N_t$:\n    - If $N_t$ is even (including $N_t = 0$), then $\\mathbb{P}(X_{t+1} = 0 \\mid N_t \\text{ even}) = \\alpha$ and $\\mathbb{P}(X_{t+1} = 1 \\mid N_t \\text{ even}) = 1 - \\alpha$.\n    - If $N_t$ is odd, then $\\mathbb{P}(X_{t+1} = 1 \\mid N_t \\text{ odd}) = 1$ (which implies $\\mathbb{P}(X_{t+1} = 0 \\mid N_t \\text{ odd}) = 0$).\n- A constraint on the parameter $\\alpha$ is given: $0  \\alpha  1$.\n- A specific value for the parameter is provided for calculations: $\\alpha = \\frac{2}{3}$.\n- The tasks are to:\n    1. Identify the causal states of the process.\n    2. Construct the minimal unifilar state-transition graph.\n    3. Determine the stationary distribution over causal states and compute the statistical complexity $C_{\\mu}$ in nats, rounded to six significant figures.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded and Objective**: The problem is a well-defined exercise in computational mechanics, a subfield of complex systems theory and statistical physics. It deals with standard concepts such as stochastic processes, causal states, and statistical complexity ($C_{\\mu}$). The language is precise and objective.\n- **Well-Posed and Complete**: The process is fully specified by the conditional probabilities. The parameter $\\alpha$ is constrained, and a specific value is given for the final calculation. The objectives are clearly stated and the required concepts (causal states, statistical complexity) have standard definitions in the field. The problem is self-contained and provides all necessary information to derive a unique solution.\n- **No other flaws**: The problem does not violate any physical laws or mathematical logic. It is not trivial, as it requires the application of specific theoretical constructs. It is not underspecified, overconstrained, or ambiguous.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A full solution will be provided.\n\n## Solution\n\nThe solution proceeds by first identifying the causal states, then constructing the state transition graph, calculating the stationary state distribution, and finally computing the statistical complexity.\n\n### 1. Causal State Identification\nCausal states are equivalence classes of past histories. Two pasts, $h_t = (\\dots, x_{t-2}, x_{t-1})$ and $h'_t = (\\dots, x'_{t-2}, x'_{t-1})$, are in the same causal state if and only if they yield the same conditional probability distribution for all future events: $\\mathbb{P}(X_{t:\\infty} \\mid h_t) = \\mathbb{P}(X_{t:\\infty} \\mid h'_t)$.\n\nThe generation rule depends only on the parity of the number of consecutive $1$'s at the end of the past sequence. Let $k(h_t)$ be this number. We can define equivalence classes based on this property.\n\n- **Class A**: Pasts where $k(h_t)$ is even. This includes pasts ending in a $0$ (for which $k=0$) and pasts ending in an even number of $1$'s, e.g., $(\\dots, 0, 1, 1)$. For any past $h_t$ in this class, the distribution of the next symbol $X_t$ is $\\mathbb{P}(X_t=0 \\mid h_t) = \\alpha$ and $\\mathbb{P}(X_t=1 \\mid h_t) = 1-\\alpha$.\n    - If $X_t=0$ is generated, the new past $h_{t+1}=(\\dots, x_{t-1}, 0)$ has $k(h_{t+1})=0$, which is even. The process remains in Class A.\n    - If $X_t=1$ is generated, the new past $h_{t+1}=(\\dots, x_{t-1}, 1)$ will have an odd number of trailing $1$'s (since the previous number was even). The process transitions to Class B.\n\n- **Class B**: Pasts where $k(h_t)$ is odd, e.g., $(\\dots, 0, 1)$ or $(\\dots, 0, 1, 1, 1)$. For any past $h_t$ in this class, the distribution of the next symbol $X_t$ is $\\mathbb{P}(X_t=1 \\mid h_t) = 1$ and $\\mathbb{P}(X_t=0 \\mid h_t) = 0$.\n    - If $X_t=1$ is generated (which it must be), the new past $h_{t+1}=(\\dots, x_{t-1}, 1)$ will have an even number of trailing $1$'s (since the previous number was odd). The process transitions to Class A.\n\nSince the future probability distribution is determined entirely by which class the current past belongs to, these two classes are the causal states of the process. Let us denote them as $S_A$ and $S_B$.\n- **Causal State $S_A$**: The set of all pasts ending in an even number of consecutive $1$'s.\n- **Causal State $S_B$**: The set of all pasts ending in an odd number of consecutive $1$'s.\n\n### 2. State-Transition Graph\nThe minimal unifilar predictive model (the $\\epsilon$-machine) consists of the two causal states $S_A$ and $S_B$ and their transitions. The transitions are labeled by `symbol|probability`.\n\n- From state $S_A$:\n    - On emitting a $0$ (with probability $\\alpha$), the new past ends in $0$, so the number of trailing $1$'s is $0$ (even). The system transitions to state $S_A$. Transition: $S_A \\xrightarrow{0|\\alpha} S_A$.\n    - On emitting a $1$ (with probability $1-\\alpha$), the new past has an odd number of trailing $1$'s. The system transitions to state $S_B$. Transition: $S_A \\xrightarrow{1|1-\\alpha} S_B$.\n\n- From state $S_B$:\n    - Emitting a $0$ has probability $0$.\n    - On emitting a $1$ (with probability $1$), the new past has an even number of trailing $1$'s. The system transitions to state $S_A$. Transition: $S_B \\xrightarrow{1|1} S_A$.\n\nThe stochastic transition matrix $T$ between causal states, where $T_{ij} = \\mathbb{P}(\\text{next state is } j \\mid \\text{current state is } i)$, is given by:\n$$ T = \\begin{pmatrix} \\alpha  1-\\alpha \\\\ 1  0 \\end{pmatrix} $$\nwhere the first row/column corresponds to $S_A$ and the second to $S_B$.\n\n### 3. Stationary Distribution and Statistical Complexity\nThe stationary distribution $\\pi = \\begin{pmatrix} \\pi_A  \\pi_B \\end{pmatrix}$ over the causal states is the left eigenvector of $T$ with eigenvalue $1$. It must satisfy $\\pi T = \\pi$ and the normalization condition $\\pi_A + \\pi_B = 1$.\n\nThe equation $\\pi T = \\pi$ yields the system:\n$$ \\pi_A \\alpha + \\pi_B (1) = \\pi_A $$\n$$ \\pi_A (1-\\alpha) + \\pi_B (0) = \\pi_B $$\nFrom the second equation, we get $\\pi_B = \\pi_A(1-\\alpha)$. Substituting this into the normalization condition:\n$$ \\pi_A + \\pi_A(1-\\alpha) = 1 $$\n$$ \\pi_A (1 + 1 - \\alpha) = 1 $$\n$$ \\pi_A (2 - \\alpha) = 1 \\implies \\pi_A = \\frac{1}{2-\\alpha} $$\nThen, the probability of being in state $S_B$ is:\n$$ \\pi_B = 1 - \\pi_A = 1 - \\frac{1}{2-\\alpha} = \\frac{2-\\alpha-1}{2-\\alpha} = \\frac{1-\\alpha}{2-\\alpha} $$\nSo, the stationary distribution is $\\pi = \\left( \\frac{1}{2-\\alpha}, \\frac{1-\\alpha}{2-\\alpha} \\right)$.\n\nFor the specific value $\\alpha = \\frac{2}{3}$:\n$$ \\pi_A = \\frac{1}{2 - \\frac{2}{3}} = \\frac{1}{\\frac{4}{3}} = \\frac{3}{4} $$\n$$ \\pi_B = \\frac{1 - \\frac{2}{3}}{2 - \\frac{2}{3}} = \\frac{\\frac{1}{3}}{\\frac{4}{3}} = \\frac{1}{4} $$\nThe stationary distribution is $\\pi = \\left( \\frac{3}{4}, \\frac{1}{4} \\right)$.\n\nThe statistical complexity $C_{\\mu}$ is the Shannon entropy of this stationary distribution, measured in nats (using the natural logarithm, $\\ln$).\n$$ C_{\\mu} = H(\\pi) = - \\sum_{i \\in \\{A, B\\}} \\pi_i \\ln(\\pi_i) $$\n$$ C_{\\mu} = - \\left( \\pi_A \\ln(\\pi_A) + \\pi_B \\ln(\\pi_B) \\right) $$\nSubstituting the values for $\\pi_A$ and $\\pi_B$:\n$$ C_{\\mu} = - \\left( \\frac{3}{4} \\ln\\left(\\frac{3}{4}\\right) + \\frac{1}{4} \\ln\\left(\\frac{1}{4}\\right) \\right) $$\nWe can expand this expression:\n$$ C_{\\mu} = - \\frac{3}{4}(\\ln(3) - \\ln(4)) - \\frac{1}{4}(\\ln(1) - \\ln(4)) $$\nSince $\\ln(1)=0$ and $\\ln(4) = \\ln(2^2) = 2\\ln(2)$:\n$$ C_{\\mu} = - \\frac{3}{4}(\\ln(3) - 2\\ln(2)) - \\frac{1}{4}(-2\\ln(2)) $$\n$$ C_{\\mu} = - \\frac{3}{4}\\ln(3) + \\frac{6}{4}\\ln(2) + \\frac{2}{4}\\ln(2) $$\n$$ C_{\\mu} = - \\frac{3}{4}\\ln(3) + \\frac{8}{4}\\ln(2) $$\n$$ C_{\\mu} = 2\\ln(2) - \\frac{3}{4}\\ln(3) $$\nTo find the numerical value, we use $\\ln(2) \\approx 0.69314718$ and $\\ln(3) \\approx 1.09861229$.\n$$ C_{\\mu} \\approx 2(0.69314718) - \\frac{3}{4}(1.09861229) $$\n$$ C_{\\mu} \\approx 1.38629436 - 0.82395922 $$\n$$ C_{\\mu} \\approx 0.56233514 $$\nRounding to six significant figures, the statistical complexity is $0.562335$ nats.",
            "answer": "$$\\boxed{0.562335}$$"
        },
        {
            "introduction": "Algorithmic (or Kolmogorov) complexity offers a powerful, universal definition of randomness, but it is fundamentally non-computable. This final practice explores computable approximations by having you implement the classic Lempel-Ziv parsing algorithm and compare its results to a standard data compressor. This coding exercise provides empirical insight into how these tools quantify regularity in different data sequences, from simple periodic patterns to pseudo-random and structurally complex strings .",
            "id": "4126693",
            "problem": "You are asked to implement and compare two computable proxies for algorithmic complexity of finite binary strings in the context of complex adaptive systems: the Lempel–Ziv 1976 (LZ76) parsing complexity and the normalized compressed length obtained from a standard universal compressor. The purpose is to empirically illustrate, on a controlled test suite, how both proxies behave across strings of differing structural regularity, in connection with entropy rate as a measure of complexity.\n\nStarting from the following fundamental base:\n- Algorithmic (Kolmogorov) Complexity (KC) is the length of the shortest program that outputs a given string, which is non-computable but can be approximated from above by universal compression and by universal parsing schemes.\n- For stationary ergodic sources over a finite alphabet of size $b$, the Lempel–Ziv incremental parsing complexity $C_{LZ}(s)$, when suitably normalized by $\\log_b n$ with $n = |s|$, converges almost surely to the entropy rate (in units of $\\log_b$) of the source; similarly, the per-symbol compressed length of a universal compressor converges in probability to the entropy rate.\n\nDefinitions to implement:\n1) For a finite binary string $s$ of length $n$, the Lempel–Ziv 1976 parsing complexity $C_{LZ}(s)$ is defined as the number of phrases obtained by the incremental parsing that proceeds left-to-right, selecting at each step the shortest next phrase $s[i:j]$ that does not occur as a substring anywhere in the already parsed prefix $s[0:j-1]$. Formally, initialize $i \\leftarrow 0$, $C_{LZ} \\leftarrow 0$. While $i  n$, choose the smallest $j  i$ such that $s[i:j]$ is not a substring of $s[0:j-1]$, set $C_{LZ} \\leftarrow C_{LZ} + 1$, and update $i \\leftarrow j$. If no such $j \\le n$ exists, take $j \\leftarrow n$ for the final phrase.\n2) The normalized Lempel–Ziv complexity is\n$$\nc_{LZ}(s) \\equiv \n\\begin{cases}\n0,  n \\le 1, \\\\\n\\displaystyle \\frac{C_{LZ}(s)\\,\\log_2 n}{n},  n \\ge 2.\n\\end{cases}\n$$\nThis estimator is dimensionless and, under broad conditions, converges to the entropy rate in bits per symbol for binary sources.\n\n3) Let $Z$ be a fixed, deterministic implementation of the DEFLATE algorithm that operates on byte sequences (for concreteness, use the standard zlib implementation). Define the normalized compressed length\n$$\nc_{zip}(s) \\equiv \n\\begin{cases}\n0,  n = 0, \\\\\n\\displaystyle \\frac{8\\,|Z(\\mathrm{pack}(s))|}{n},  n \\ge 1,\n\\end{cases}\n$$\nwhere $\\mathrm{pack}(s)$ maps the binary string $s$ into a byte sequence by taking successive blocks of $8$ bits of $s$ (most significant bit first within each byte) and padding the final block on the right with zeros to complete a byte if necessary. The operator $|\\cdot|$ denotes the length in bytes. The normalization yields a dimensionless per-bit compressed length.\n\nTasks:\n- Implement the exact Lempel–Ziv 1976 incremental parsing count $C_{LZ}(s)$ as defined above, and compute $c_{LZ}(s)$.\n- Implement $\\mathrm{pack}(s)$ with most significant bit first within each byte and right-zero padding of the final byte as specified.\n- Using the standard zlib DEFLATE compressor with a fixed compression level, compute $c_{zip}(s)$.\n- For each test case below, return the triple $[c_{LZ}(s), c_{zip}(s), c_{zip}(s) - c_{LZ}(s)]$ in this exact order.\n\nTest suite (provide results in this order):\n- Case A (highly regular): $s_A$ is the string of $n = 512$ zeros.\n- Case B (period-$2$ regular): $s_B$ is the length-$512$ prefix of the infinite periodic string $\\dots 010101 \\dots$, i.e., $s_B = (01)^{256}$.\n- Case C (pseudo-random by a linear congruential generator): $s_C$ is constructed by the recurrence\n$$\nx_{0} = 1,\\quad x_{k+1} \\equiv (1664525 \\cdot x_k + 1013904223) \\bmod 2^{32},\\quad b_k \\equiv \\left\\lfloor \\frac{x_k}{2^{31}} \\right\\rfloor \\bmod 2,\n$$\nand $s_C = b_0 b_1 \\dots b_{511}$ has length $n = 512$.\n- Case D (Thue–Morse sequence): $s_D$ is the length-$512$ prefix defined by $b_i$ equal to the parity of the number of ones in the binary expansion of $i$ (that is, $b_i \\equiv \\left(\\sum_j \\mathrm{bit}_j(i)\\right) \\bmod 2$), for $i = 0,1,\\dots,511$; set $s_D = b_0 b_1 \\dots b_{511}$.\n- Case E (boundary): $s_E$ is the empty string of length $n = 0$.\n- Case F (block-repeat): $s_F$ is obtained by first generating a block $u$ of length $64$ via the same linear congruential generator but with $x_0 = 12345$ and the same update and bit-extraction rules as Case C, then repeating $u$ exactly $8$ times to obtain a string of length $n = 512$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is, in order, the three-element list $[c_{LZ}(s), c_{zip}(s), c_{zip}(s) - c_{LZ}(s)]$ for the six cases $A$ through $F$. For example, a syntactically correct output shape is\n$[[x_{A},y_{A},y_{A}-x_{A}],[x_{B},y_{B},y_{B}-x_{B}],\\dots,[x_{F},y_{F},y_{F}-x_{F}]]$,\nwith each $x_{\\cdot}$ and $y_{\\cdot}$ a floating-point number. The quantities are dimensionless; no physical units are involved. Angles are not used. Percentages must not be used; all ratios are to be reported as decimal floating-point numbers.",
            "solution": "The problem presents a task of implementing and comparing two computable approximations of algorithmic complexity for finite binary strings: the Lempel-Ziv 1976 parsing complexity and the normalized compressed length via the DEFLATE algorithm. The comparison is to be performed on a suite of six test strings exhibiting different structural properties.\n\nThe problem is deemed valid as it is scientifically grounded in established principles of information theory and complexity, is well-posed with precise and formalizable definitions, and is objective in its language and requirements. All necessary data and definitions for a unique solution are provided.\n\nThe solution proceeds by systematically implementing the required components and applying them to the specified test cases.\n\nFirst, we address the generation of the six test strings, denoted $s_A$ through $s_F$. Let $n$ be the length of a string $s$.\n\n-   Case A (Highly regular): $s_A$ is a string of $n=512$ zeros. $s_A = 0^{512}$.\n-   Case B (Period-2 regular): $s_B$ is the periodic string $(01)^{256}$, with length $n=512$.\n-   Case C (Pseudo-random): $s_C$ is a binary string of length $n=512$, $s_C = b_0 b_1 \\dots b_{511}$, generated using a linear congruential generator (LCG). The state $x_k \\in \\mathbb{Z}_{2^{32}}$ is updated via the recurrence $x_{k+1} \\equiv (a \\cdot x_k + c) \\bmod M$, with initial seed $x_0 = 1$ and parameters $a = 1664525$, $c = 1013904223$, and $M = 2^{32}$. The $k$-th bit $b_k$ is the most significant bit of $x_k$, extracted as $b_k = (x_k \\gg 31) \\ 1$.\n-   Case D (Thue–Morse sequence): $s_D$ is a binary string of length $n=512$, $s_D = b_0 b_1 \\dots b_{511}$. The bit $b_i$ is determined by the parity of the number of ones in the binary representation of the index $i$. Formally, $b_i \\equiv (\\sum_j \\mathrm{bit}_j(i)) \\bmod 2$, where $\\mathrm{bit}_j(i)$ is the $j$-th bit of $i$.\n-   Case E (Boundary): $s_E$ is the empty string, with length $n=0$.\n-   Case F (Block-repeat): $s_F$ is a string of length $n=512$ constructed by repeating a $64$-bit block $u$ a total of $8$ times. The block $u$ itself is generated using the same LCG method as in Case C, but with a different initial seed, $x_0 = 12345$.\n\nNext, we implement the two complexity measures.\n\n1.  **Lempel-Ziv 1976 Parsing Complexity ($C_{LZ}(s)$)**\n    The problem defines a specific incremental parsing algorithm. For a string $s$ of length $n$, we start with a parsed string of length $i=0$ and a complexity count $C_{LZ}=0$. In a loop, while $i  n$, we increment $C_{LZ}$ and find the smallest integer $j  i$ such that the candidate phrase $s[i:j]$ is not a substring of the prefix $s[0:j-1]$. It must be noted that this prefix $s[0:j-1]$ contains the portion of the string already parsed, $s[0:i]$, as well as the partial phrase being constructed, $s[i:j-1]$. This definition, while unusual, is unambiguous. If such a $j \\le n$ is found, the parser advances to that position by setting $i \\leftarrow j$. If the loop over all possible $j$ from $i+1$ to $n$ completes without finding such a phrase (i.e., every candidate phrase $s[i:j]$ is found within its corresponding prefix $s[0:j-1]$), the remainder of the string $s[i:n]$ is taken as the final phrase, and the process terminates by setting $i \\leftarrow n$.\n\n    The normalized Lempel-Ziv complexity, $c_{LZ}(s)$, is then computed.\n    $$\n    c_{LZ}(s) \\equiv \n    \\begin{cases}\n    0,  n \\le 1, \\\\\n    \\displaystyle \\frac{C_{LZ}(s)\\,\\log_2 n}{n},  n \\ge 2.\n    \\end{cases}\n    $$\n    For $n=512$, this simplifies to $c_{LZ}(s) = C_{LZ}(s) \\cdot (\\log_2 512) / 512 = 9 \\cdot C_{LZ}(s) / 512$.\n\n\n2.  **Normalized Compressed Length ($c_{zip}(s)$)**\n    This measure requires two steps: packing the binary string into a byte sequence and then compressing it.\n    \n    The packing function, $\\mathrm{pack}(s)$, converts the binary string $s$ into a sequence of bytes. The string is processed in chunks of $8$ bits. Each chunk is interpreted as a byte with the most significant bit first. If the length $n$ of $s$ is not a multiple of $8$, the final partial chunk is padded on the right with zeros to form a full $8$-bit byte.\n    \n    The resulting byte sequence, $\\mathrm{pack}(s)$, is then compressed using a fixed, deterministic DEFLATE implementation. We use Python's standard `zlib.compress` function with its default compression level, which provides a consistent implementation as required.\n    \n    The normalized compressed length, $c_{zip}(s)$, is defined as:\n    $$\n    c_{zip}(s) \\equiv \n    \\begin{cases}\n    0,  n = 0, \\\\\n    \\displaystyle \\frac{8\\,|Z(\\mathrm{pack}(s))|}{n},  n \\ge 1,\n    \\end{cases}\n    $$\n    where $|Z(\\mathrm{pack}(s))|$ is the length in bytes of the compressed data. The factor of $8$ converts this length from bytes to bits, yielding a dimensionless complexity measure in units of bits per bit.\n\nFinally, for each test case string $s_k$ ($k \\in \\{A, B, C, D, E, F\\}$), we compute the pair of complexity values, $(c_{LZ}(s_k), c_{zip}(s_k))$, and their difference, $c_{zip}(s_k) - c_{LZ}(s_k)$. The results are then aggregated into a list of triples $[c_{LZ}(s_k), c_{zip}(s_k), c_{zip}(s_k) - c_{LZ}(s_k)]$ in the specified order.\n\nThe implementation encapsulates these steps into a single program that generates each string, applies the complexity algorithms, and formats the output as required.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport zlib\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate the generation of test strings,\n    computation of complexity measures, and final output formatting.\n    \"\"\"\n\n    def compute_CLZ(s: str) - int:\n        \"\"\"\n        Computes the Lempel-Ziv 1976 parsing complexity C_LZ(s)\n        based on the exact, literal definition provided in the problem.\n        \"\"\"\n        n = len(s)\n        if n == 0:\n            return 0\n        \n        i = 0\n        c_lz = 0\n        while i  n:\n            c_lz += 1\n            # Find the smallest j  i such that s[i:j] is not a substring of s[0:j-1]\n            best_j = -1\n            for j_candidate in range(i + 1, n + 1):\n                phrase = s[i:j_candidate]\n                context = s[0:j_candidate - 1]\n                if phrase not in context:\n                    best_j = j_candidate\n                    break  # Found the shortest new phrase\n            \n            if best_j != -1:\n                # A new phrase was found, advance i\n                i = best_j\n            else:\n                # No such phrase exists, the rest of the string is the last phrase\n                i = n\n        return c_lz\n\n    def compute_c_LZ(s: str) - float:\n        \"\"\"Computes the normalized Lempel-Ziv complexity c_LZ(s).\"\"\"\n        n = len(s)\n        if n = 1:\n            return 0.0\n        \n        clz_count = compute_CLZ(s)\n        return (clz_count * np.log2(n)) / n\n\n    def pack_string(s: str) - bytes:\n        \"\"\"Packs a binary string into a byte sequence with MSB first and zero padding.\"\"\"\n        n = len(s)\n        if n == 0:\n            return b''\n        \n        byte_list = []\n        for i in range(0, n, 8):\n            chunk = s[i:i+8]\n            # Pad the final chunk with zeros on the right\n            if len(chunk)  8:\n                chunk = chunk.ljust(8, '0')\n            byte_val = int(chunk, 2)\n            byte_list.append(byte_val)\n        \n        return bytes(byte_list)\n\n    def compute_c_zip(s: str) - float:\n        \"\"\"Computes the normalized compressed length c_zip(s).\"\"\"\n        n = len(s)\n        if n == 0:\n            return 0.0\n\n        packed_data = pack_string(s)\n        # Use zlib with default, fixed compression level for determinism\n        compressed_data = zlib.compress(packed_data)\n        compressed_len_bytes = len(compressed_data)\n        \n        return (8 * compressed_len_bytes) / n\n\n    def generate_lcg_string(length: int, seed: int) - str:\n        \"\"\"Generates a pseudo-random binary string using a linear congruential generator.\"\"\"\n        a = 1664525\n        c = 1013904223\n        m = 2**32\n        \n        bits = []\n        x = seed\n        for _ in range(length):\n            # Extract the most significant bit\n            bit = (x  31)  1\n            bits.append(str(bit))\n            # Update the LCG state\n            x = (a * x + c) % m\n        return \"\".join(bits)\n\n    # --- Generate Test Case Strings ---\n\n    n_val = 512\n    # Case A: Highly regular (all zeros)\n    s_A = '0' * n_val\n    \n    # Case B: Period-2 regular\n    s_B = '01' * (n_val // 2)\n\n    # Case C: Pseudo-random by LCG\n    s_C = generate_lcg_string(n_val, seed=1)\n\n    # Case D: Thue-Morse sequence\n    thue_morse_bits = [(bin(i).count('1') % 2) for i in range(n_val)]\n    s_D = \"\".join(map(str, thue_morse_bits))\n\n    # Case E: Empty string\n    s_E = \"\"\n\n    # Case F: Block-repeat\n    block_u = generate_lcg_string(length=64, seed=12345)\n    s_F = block_u * 8\n    \n    test_cases_strings = [s_A, s_B, s_C, s_D, s_E, s_F]\n    \n    results = []\n    for s in test_cases_strings:\n        c_lz_val = compute_c_LZ(s)\n        c_zip_val = compute_c_zip(s)\n        diff = c_zip_val - c_lz_val\n        results.append([c_lz_val, c_zip_val, diff])\n\n    # Final print statement in the exact required format.\n    # The default string representation of a list includes spaces, so we custom format it.\n    formatted_results = []\n    for res_triple in results:\n        # Format each triple as a string like \"[num1,num2,num3]\"\n        formatted_triple = f\"[{res_triple[0]},{res_triple[1]},{res_triple[2]}]\"\n        formatted_results.append(formatted_triple)\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n\n```"
        }
    ]
}