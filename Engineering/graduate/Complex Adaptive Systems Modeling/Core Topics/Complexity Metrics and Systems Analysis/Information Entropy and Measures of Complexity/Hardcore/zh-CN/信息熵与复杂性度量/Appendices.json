{
    "hands_on_practices": [
        {
            "introduction": "香农熵是信息论和复杂性科学的基石，它量化了系统的不确定性或不可预测性。这项练习将通过一个离散随机变量的简单例子，引导你从第一性原理出发，亲手计算香农熵。通过这个实践，你将掌握信息单位（奈特与比特）之间的转换，并理解为何均匀分布在所有可能结果中熵最大，这是理解信息内容的基础。",
            "id": "4126716",
            "problem": "在一个复杂自适应系统中，主体根据经验自适应地选择行动。考虑一个单一主体，其行动选择机制已收敛到一个大小为 $3$ 的行动集上的离散分布，三个行动的概率分别为 $p=(1/2,1/3,1/6)$。使用香农熵作为期望自信息的基本定义，以及自然对数（奈特）和以2为底的对数（比特）之间的标准转换规则，完成以下任务：计算该分布的熵（以奈特和比特为单位），并将其与3个结果上的均匀分布的熵进行比较。你必须从第一性原理推导所有表达式，并明确证明所使用的任何底数转换的合理性。你最终报告的答案必须是均匀分布的熵与分布 $p$ 的熵之间的差值（以比特为单位）的精确封闭形式表达式，即 $H(u)-H(p)$（以比特为单位），其中 $u$ 表示3个结果上的均匀分布。不要四舍五入；提供一个精确表达式。最终答案必须是一个不带单位的单一表达式。",
            "solution": "在尝试解答之前，对问题进行验证。\n\n### 步骤 1：提取已知条件\n-   系统：一个复杂自适应系统中的单一主体。\n-   行动集大小：$3$。\n-   行动概率分布，$p$：$(1/2, 1/3, 1/6)$。\n-   任务1：计算分布 $p$ 的香non熵（以奈特和比特为单位）。\n-   任务2：将 $p$ 的熵与3个结果上的均匀分布（表示为 $u$）的熵进行比较。\n-   任务3：从第一性原理推导所有表达式，并证明对数底数之间转换的合理性。\n-   最终答案要求：提供差值 $H(u) - H(p)$（以比特为单位）的精确封闭形式表达式。\n\n### 步骤 2：使用提取的已知条件进行验证\n-   **科学依据：** 该问题牢固地植根于信息论，这是数学和物理学的一门核心学科，也是复杂系统定量研究的核心。香农熵、自信息和对数底数转换等概念都是标准且已确立的。\n-   **良构性：** 该问题是良构的。给定的概率分布 $p = (1/2, 1/3, 1/6)$ 是有效的，因为概率是非负的且总和为1：$\\frac{1}{2} + \\frac{1}{3} + \\frac{1}{6} = \\frac{3}{6} + \\frac{2}{6} + \\frac{1}{6} = \\frac{6}{6} = 1$。任务是具体的，并导向一个唯一且有意义的解。\n-   **客观性：** 问题陈述以精确、客观的语言表达，没有歧义或主观论断。\n\n该问题没有表现出验证标准中列出的任何缺陷。它在科学上是合理的，形式上是结构化的，内部是一致的，并且需要严格应用基本原理。\n\n### 步骤 3：结论与行动\n问题是 **有效的**。将提供解答。\n\n### 解答推导\n\n一个以概率 $p_i$ 发生的结果 $i$ 的自信息定义为 $I(p_i) = -\\log(p_i)$。对数的底决定了信息的单位。对于本问题，我们将使用底为 $e$（自然对数，$\\ln$）来计算奈特，使用底为 $2$（二进制对数，$\\log_2$）来计算比特。\n\n一个离散概率分布 $P = \\{p_1, p_2, \\dots, p_n\\}$ 的香农熵 $H$ 是所有可能结果的自信息的期望值：\n$$H(P) = E[I(p_i)] = \\sum_{i=1}^{n} p_i I(p_i) = -\\sum_{i=1}^{n} p_i \\log(p_i)$$\n\n首先，我们以奈特为单位计算给定分布 $p = (1/2, 1/3, 1/6)$ 的熵。这需要使用自然对数。设 $H_e(p)$ 是以奈特为单位的熵。\n$$H_e(p) = -\\left( \\frac{1}{2} \\ln\\left(\\frac{1}{2}\\right) + \\frac{1}{3} \\ln\\left(\\frac{1}{3}\\right) + \\frac{1}{6} \\ln\\left(\\frac{1}{6}\\right) \\right)$$\n使用对数恒等式 $\\ln(1/x) = -\\ln(x)$：\n$$H_e(p) = \\frac{1}{2} \\ln(2) + \\frac{1}{3} \\ln(3) + \\frac{1}{6} \\ln(6)$$\n我们可以使用恒等式 $\\ln(ab) = \\ln(a) + \\ln(b)$ 来简化 $\\ln(6)$：\n$$H_e(p) = \\frac{1}{2} \\ln(2) + \\frac{1}{3} \\ln(3) + \\frac{1}{6} (\\ln(2) + \\ln(3))$$\n合并含有 $\\ln(2)$ 和 $\\ln(3)$ 的项：\n$$H_e(p) = \\left(\\frac{1}{2} + \\frac{1}{6}\\right) \\ln(2) + \\left(\\frac{1}{3} + \\frac{1}{6}\\right) \\ln(3)$$\n$$H_e(p) = \\left(\\frac{3}{6} + \\frac{1}{6}\\right) \\ln(2) + \\left(\\frac{2}{6} + \\frac{1}{6}\\right) \\ln(3)$$\n$$H_e(p) = \\frac{4}{6} \\ln(2) + \\frac{3}{6} \\ln(3) = \\frac{2}{3} \\ln(2) + \\frac{1}{2} \\ln(3)$$\n这是分布 $p$ 以奈特为单位的熵。\n\n接下来，我们将这个熵转换为比特。不同底数 $a$ 和 $b$ 的对数之间的转换由换底公式给出：$\\log_b(x) = \\frac{\\log_a(x)}{\\log_a(b)}$。要从奈特（底为 $e$）转换为比特（底为 $2$），我们设 $a=e$ 和 $b=2$：\n$$\\log_2(x) = \\frac{\\ln(x)}{\\ln(2)}$$\n以比特为单位的熵 $H_2(P)$ 定义为 $H_2(P) = -\\sum p_i \\log_2(p_i)$。代入换底公式：\n$$H_2(P) = -\\sum p_i \\frac{\\ln(p_i)}{\\ln(2)} = \\frac{1}{\\ln(2)} \\left( -\\sum p_i \\ln(p_i) \\right) = \\frac{H_e(P)}{\\ln(2)}$$\n这就是转换的基本理由：以比特为单位的熵等于以奈特为单位的熵除以常数 $\\ln(2)$。\n使用此转换，我们求出分布 $p$ 以比特为单位的熵 $H_2(p)$：\n$$H_2(p) = \\frac{H_e(p)}{\\ln(2)} = \\frac{\\frac{2}{3} \\ln(2) + \\frac{1}{2} \\ln(3)}{\\ln(2)}$$\n$$H_2(p) = \\frac{2}{3} \\frac{\\ln(2)}{\\ln(2)} + \\frac{1}{2} \\frac{\\ln(3)}{\\ln(2)}$$\n再次使用换底公式，$\\frac{\\ln(3)}{\\ln(2)} = \\log_2(3)$。\n$$H_2(p) = \\frac{2}{3} + \\frac{1}{2} \\log_{2}(3)$$\n这是分布 $p$ 以比特为单位的熵。\n\n现在，我们以比特为单位计算3个结果上的均匀分布 $u = (1/3, 1/3, 1/3)$ 的熵。设其为 $H_2(u)$。\n$$H_2(u) = -\\sum_{i=1}^{3} u_i \\log_{2}(u_i) = -\\sum_{i=1}^{3} \\frac{1}{3} \\log_{2}\\left(\\frac{1}{3}\\right)$$\n$$H_2(u) = -3 \\cdot \\frac{1}{3} \\log_{2}\\left(\\frac{1}{3}\\right) = -\\log_{2}\\left(\\frac{1}{3}\\right)$$\n使用对数恒等式 $\\log(1/x) = -\\log(x)$：\n$$H_2(u) = \\log_{2}(3)$$\n这证实了一个普遍结论，即 $N$ 个状态上的均匀分布的熵为 $\\log_{2}(N)$ 比特。\n\n最后，我们以比特为单位计算所需的差值 $H(u) - H(p)$。这对应于 $H_2(u) - H_2(p)$。\n$$H_2(u) - H_2(p) = \\log_{2}(3) - \\left(\\frac{2}{3} + \\frac{1}{2} \\log_{2}(3)\\right)$$\n$$H_2(u) - H_2(p) = \\log_{2}(3) - \\frac{2}{3} - \\frac{1}{2} \\log_{2}(3)$$\n合并 $\\log_{2}(3)$ 项：\n$$H_2(u) - H_2(p) = \\left(1 - \\frac{1}{2}\\right) \\log_{2}(3) - \\frac{2}{3}$$\n$$H_2(u) - H_2(p) = \\frac{1}{2} \\log_{2}(3) - \\frac{2}{3}$$\n这就是熵差的最终、精确的封闭形式表达式。这个量也被称为分布 $p$ 的冗余度，或其相对于均匀分布的库尔贝克-莱布勒散度。",
            "answer": "$$\\boxed{\\frac{1}{2} \\log_{2}(3) - \\frac{2}{3}}$$"
        },
        {
            "introduction": "复杂系统通常表现为随时间演化的动态过程，而不仅仅是静态的概率分布。计算力学框架为我们提供了一种强大的方法，用以揭示并量化这些过程内在的结构和记忆。本练习要求你通过分析一个随机过程的生成规则，识别出其“因果态”——即决定未来统计行为的最小历史集合，并计算出衡量系统记忆量的统计复杂性 $C_{\\mu}$。",
            "id": "4126673",
            "problem": "考虑一个字母表为 $\\{0,1\\}$ 的平稳二元随机过程 $\\{X_t\\}_{t \\in \\mathbb{Z}}$，它由以下条件分布生成，这些分布取决于半无限过去中自最近的 $0$ 以来连续 $1$ 的数量的奇偶性。设 $N_t$ 表示在时间 $t$ 结束的过去中自最近的 $0$ 以来的连续 $1$ 的数量。生成规则如下：\n- 如果 $N_t$ 是偶数（包括 $N_t = 0$），则 $\\mathbb{P}(X_{t+1} = 0 \\mid N_t \\text{ even}) = \\alpha$ 且 $\\mathbb{P}(X_{t+1} = 1 \\mid N_t \\text{ even}) = 1 - \\alpha$。\n- 如果 $N_t$ 是奇数，则 $\\mathbb{P}(X_{t+1} = 1 \\mid N_t \\text{ odd}) = 1$。\n\n假设 $0  \\alpha  1$ 且该过程处于其平稳状态。对于特定参数值 $\\alpha = \\frac{2}{3}$：\n\n- 使用关于过去的预测等价关系，识别该过程的因果状态。\n- 通过为每个因果状态指定由发射符号及其概率标记的出向转移，以及目标因果状态，构建最小单线态转移图（其节点为因果状态的预测模型）。\n- 确定由转移动力学诱导的因果状态上的平稳分布，并使用在此背景下适用的信息和复杂度的标准定义，计算统计复杂度 $C_{\\mu}$（因果状态分布中的信息存储），以奈特为单位。\n\n给出你最终的 $C_{\\mu}$ 答案，以奈特为单位，并将结果四舍五入到六位有效数字。",
            "solution": "对问题陈述的有效性进行分析。\n\n## 问题验证\n\n### 步骤 1：提取给定条件\n- 一个字母表为 $\\{0,1\\}$ 的平稳二元随机过程 $\\{X_t\\}_{t \\in \\mathbb{Z}}$。\n- 定义变量 $N_t$ 为在时间 t 结束的过去中自最近的 0 以来的连续 1 的数量。\n- 下一个符号 $X_{t+1}$ 的条件概率分布取决于 $N_t$ 的奇偶性：\n    - 如果 $N_t$ 是偶数（包括 $N_t = 0$），则 $\\mathbb{P}(X_{t+1} = 0 \\mid N_t \\text{ even}) = \\alpha$ 且 $\\mathbb{P}(X_{t+1} = 1 \\mid N_t \\text{ even}) = 1 - \\alpha$。\n    - 如果 $N_t$ 是奇数，则 $\\mathbb{P}(X_{t+1} = 1 \\mid N_t \\text{ odd}) = 1$（这意味着 $\\mathbb{P}(X_{t+1} = 0 \\mid N_t \\text{ odd}) = 0$）。\n- 给定参数 $\\alpha$ 的约束条件：$0  \\alpha  1$。\n- 为计算提供了参数的具体值：$\\alpha = \\frac{2}{3}$。\n- 任务是：\n    1. 识别过程的因果状态。\n    2. 构建最小单线态转移图。\n    3. 确定因果状态上的平稳分布，并计算统计复杂度 $C_{\\mu}$（以奈特为单位，四舍五入到六位有效数字）。\n\n### 步骤 2：使用提取的给定条件进行验证\n- **科学依据和客观性**：该问题是计算力学中一个明确定义的练习，计算力学是复杂系统理论和统计物理学的一个子领域。它涉及随机过程、因果状态和统计复杂度 ($C_{\\mu}$) 等标准概念。语言精确客观。\n- **适定性和完整性**：该过程由条件概率完全指定。参数 $\\alpha$ 受到约束，并为最终计算给出了一个具体值。目标陈述清晰，所需概念（因果状态、统计复杂度）在该领域有标准定义。问题是自洽的，并提供了推导唯一解所需的所有必要信息。\n- **无其他缺陷**：该问题不违反任何物理定律或数理逻辑。它并非微不足道，因为它需要应用特定的理论结构。它没有未指定、过约束或含糊不清之处。\n\n### 步骤 3：结论和行动\n该问题被认为是 **有效的**。将提供完整的解决方案。\n\n## 解答\n\n解答过程首先识别因果状态，然后构建状态转移图，计算平稳状态分布，最后计算统计复杂度。\n\n### 1. 因果状态识别\n因果状态是过去历史的等价类。两个过去 $h_t = (\\dots, x_{t-2}, x_{t-1})$ 和 $h'_t = (\\dots, x'_{t-2}, x'_{t-1})$ 处于相同的因果状态，当且仅当它们对所有未来事件产生相同的条件概率分布：$\\mathbb{P}(X_{t:\\infty} \\mid h_t) = \\mathbb{P}(X_{t:\\infty} \\mid h'_t)$。\n\n生成规则仅取决于过去序列末尾连续 1 的数量的奇偶性。设 $k(h_t)$ 为此数。我们可以基于此属性定义等价类。\n\n- **A 类**：$k(h_t)$ 为偶数的过去。这包括以 0 结尾的过去（此时 $k=0$）和以偶数个 1 结尾的过去，例如 $(\\dots, 0, 1, 1)$。对于此类中的任何过去 $h_t$，下一个符号 $X_t$ 的分布为 $\\mathbb{P}(X_t=0 \\mid h_t) = \\alpha$ 和 $\\mathbb{P}(X_t=1 \\mid h_t) = 1-\\alpha$。\n    - 如果生成了 $X_t=0$，新的过去 $h_{t+1}=(\\dots, x_{t-1}, 0)$ 的 $k(h_{t+1})=0$，是偶数。过程保持在 A 类。\n    - 如果生成了 $X_t=1$，新的过去 $h_{t+1}=(\\dots, x_{t-1}, 1)$ 将有奇数个结尾的 1（因为之前的数量是偶数）。过程转移到 B 类。\n\n- **B 类**：$k(h_t)$ 为奇数的过去，例如 $(\\dots, 0, 1)$ 或 $(\\dots, 0, 1, 1, 1)$。对于此类中的任何过去 $h_t$，下一个符号 $X_t$ 的分布为 $\\mathbb{P}(X_t=1 \\mid h_t) = 1$ 和 $\\mathbb{P}(X_t=0 \\mid h_t) = 0$。\n    - 如果生成了 $X_t=1$（必须如此），新的过去 $h_{t+1}=(\\dots, x_{t-1}, 1)$ 将有偶数个结尾的 1（因为之前的数量是奇数）。过程转移到 A 类。\n\n由于未来的概率分布完全由当前过去所属的类别决定，因此这两个类别是该过程的因果状态。我们将其表示为 $S_A$ 和 $S_B$。\n- **因果状态 $S_A$**：所有以偶数个连续 1 结尾的过去的集合。\n- **因果状态 $S_B$**：所有以奇数个连续 1 结尾的过去的集合。\n\n### 2. 状态转移图\n最小单线态预测模型（$\\epsilon$-机）由两个因果状态 $S_A$ 和 $S_B$ 及其转移组成。转移用 `符号|概率` 标记。\n\n- 从状态 $S_A$：\n    - 发射 0（概率为 $\\alpha$）时，新的过去以 0 结尾，因此结尾 1 的数量为 0（偶数）。系统转移到状态 $S_A$。转移：$S_A \\xrightarrow{0|\\alpha} S_A$。\n    - 发射 1（概率为 $1-\\alpha$）时，新的过去有奇数个结尾的 1。系统转移到状态 $S_B$。转移：$S_A \\xrightarrow{1|1-\\alpha} S_B$。\n\n- 从状态 $S_B$：\n    - 发射 0 的概率为 0。\n    - 发射 1（概率为 1）时，新的过去有偶数个结尾的 1。系统转移到状态 $S_A$。转移：$S_B \\xrightarrow{1|1} S_A$。\n\n因果状态之间的随机转移矩阵 $T$，其中 $T_{ij} = \\mathbb{P}(\\text{下一个状态是 } j \\mid \\text{当前状态是 } i)$，由下式给出：\n$$ T = \\begin{pmatrix} \\alpha  1-\\alpha \\\\ 1  0 \\end{pmatrix} $$\n其中第一行/列对应 $S_A$，第二行/列对应 $S_B$。\n\n### 3. 平稳分布和统计复杂度\n因果状态上的平稳分布 $\\pi = \\begin{pmatrix} \\pi_A  \\pi_B \\end{pmatrix}$ 是 $T$ 的特征值为 $1$ 的左特征向量。它必须满足 $\\pi T = \\pi$ 和归一化条件 $\\pi_A + \\pi_B = 1$。\n\n方程 $\\pi T = \\pi$ 产生以下系统：\n$$ \\pi_A \\alpha + \\pi_B (1) = \\pi_A $$\n$$ \\pi_A (1-\\alpha) + \\pi_B (0) = \\pi_B $$\n从第二个方程，我们得到 $\\pi_B = \\pi_A(1-\\alpha)$。将此代入归一化条件：\n$$ \\pi_A + \\pi_A(1-\\alpha) = 1 $$\n$$ \\pi_A (1 + 1 - \\alpha) = 1 $$\n$$ \\pi_A (2 - \\alpha) = 1 \\implies \\pi_A = \\frac{1}{2-\\alpha} $$\n那么，处于状态 $S_B$ 的概率是：\n$$ \\pi_B = 1 - \\pi_A = 1 - \\frac{1}{2-\\alpha} = \\frac{2-\\alpha-1}{2-\\alpha} = \\frac{1-\\alpha}{2-\\alpha} $$\n因此，平稳分布为 $\\pi = \\left( \\frac{1}{2-\\alpha}, \\frac{1-\\alpha}{2-\\alpha} \\right)$。\n\n对于特定值 $\\alpha = \\frac{2}{3}$：\n$$ \\pi_A = \\frac{1}{2 - \\frac{2}{3}} = \\frac{1}{\\frac{4}{3}} = \\frac{3}{4} $$\n$$ \\pi_B = \\frac{1 - \\frac{2}{3}}{2 - \\frac{2}{3}} = \\frac{\\frac{1}{3}}{\\frac{4}{3}} = \\frac{1}{4} $$\n平稳分布为 $\\pi = \\left( \\frac{3}{4}, \\frac{1}{4} \\right)$。\n\n统计复杂度 $C_{\\mu}$ 是此平稳分布的香农熵，以奈特（使用自然对数 $\\ln$）为单位。\n$$ C_{\\mu} = H(\\pi) = - \\sum_{i \\in \\{A, B\\}} \\pi_i \\ln(\\pi_i) $$\n$$ C_{\\mu} = - \\left( \\pi_A \\ln(\\pi_A) + \\pi_B \\ln(\\pi_B) \\right) $$\n代入 $\\pi_A$ 和 $\\pi_B$ 的值：\n$$ C_{\\mu} = - \\left( \\frac{3}{4} \\ln\\left(\\frac{3}{4}\\right) + \\frac{1}{4} \\ln\\left(\\frac{1}{4}\\right) \\right) $$\n我们可以展开这个表达式：\n$$ C_{\\mu} = - \\frac{3}{4}(\\ln(3) - \\ln(4)) - \\frac{1}{4}(\\ln(1) - \\ln(4)) $$\n由于 $\\ln(1)=0$ 且 $\\ln(4) = \\ln(2^2) = 2\\ln(2)$：\n$$ C_{\\mu} = - \\frac{3}{4}(\\ln(3) - 2\\ln(2)) - \\frac{1}{4}(-2\\ln(2)) $$\n$$ C_{\\mu} = - \\frac{3}{4}\\ln(3) + \\frac{6}{4}\\ln(2) + \\frac{2}{4}\\ln(2) $$\n$$ C_{\\mu} = - \\frac{3}{4}\\ln(3) + \\frac{8}{4}\\ln(2) $$\n$$ C_{\\mu} = 2\\ln(2) - \\frac{3}{4}\\ln(3) $$\n为了求数值，我们使用 $\\ln(2) \\approx 0.69314718$ 和 $\\ln(3) \\approx 1.09861229$。\n$$ C_{\\mu} \\approx 2(0.69314718) - \\frac{3}{4}(1.09861229) $$\n$$ C_{\\mu} \\approx 1.38629436 - 0.82395922 $$\n$$ C_{\\mu} \\approx 0.56233514 $$\n四舍五入到六位有效数字，统计复杂度为 $0.562335$ 奈特。",
            "answer": "$$\\boxed{0.562335}$$"
        },
        {
            "introduction": "在理论模型之外，我们更常面对的是从真实世界系统中观测到的有限数据。这项实践将理论与应用联系起来，指导你如何从经验数据中估计一个过程的关键复杂性度量——熵率。你将实现一个“置入式”估计器，并直面有限样本带来的一个核心挑战：系统性偏差。通过应用 Miller-Madow 偏差校正，你将学会一种更精确地从数据中提取复杂性的实用技术。",
            "id": "4126703",
            "problem": "给定从一个平稳二元随机过程的单次长实现中采样的所有长度为 $2$ 和 $3$ 的二元块的经验计数。您的任务是使用一个直接从平稳过程的香农熵和熵率的基本定义中导出的插件估计器来估计熵率，然后使用一个从渐近偏差分析中导出的有原则的修正来量化和解释有限样本偏差。\n\n从以下基本依据出发：\n- 对于有限集 $\\mathcal{X}$ 上的离散分布 $P$，香农熵定义为 $H(P) = -\\sum_{x \\in \\mathcal{X}} p(x) \\log_2 p(x)$。\n- 对于有限字母表上的平稳随机过程 $\\{X_t\\}_{t \\in \\mathbb{Z}}$，熵率定义为 $h = \\lim_{L \\to \\infty} H(X_1^L) - H(X_1^{L-1})$，其中 $X_1^L$ 表示长度为 $L$ 的块，$H(X_1^L)$ 是该块分布的香农熵。\n- 插件估计器用观测计数形成的经验块分布替代真实的块分布，并从相应的经验块熵构建熵率估计器。\n- 经验块熵必须使用极限约定 $\\lim_{p \\to 0^+} p \\log p = 0$ 来处理零概率事件，以避免未定义的项。\n\n基于这些依据，设计一个程序，该程序：\n1. 根据提供的计数计算长度为 $2$ 和 $3$ 的块的经验块概率，对于二元字母表 $\\{0,1\\}$，块的顺序固定为：长度为 $2$ 时为 $[00,01,10,11]$，长度为 $3$ 时为 $[000,001,010,011,100,101,110,111]$。\n2. 计算长度为 $2$ 和 $3$ 的经验块熵（单位为比特）。\n3. 根据这些经验块熵构建插件熵率估计。\n4. 使用分别应用于长度为 $2$ 和长度为 $3$ 的经验块熵的 Miller–Madow 修正来量化有限样本偏差。对于一个有 $N$ 个样本和 $K$ 个观测类别（非零计数）的离散分布，修正后的熵（单位为比特）为 $H_{\\text{MM}} = H_{\\text{plug-in}} + \\frac{K-1}{2N \\log 2}$，其中 $\\log$ 表示自然对数。使用此修正来生成一个偏差调整后的熵率估计，以及对插件熵率估计器的隐含偏差修正。\n5. 将所有熵和熵率以比特为单位表示为浮点数。\n\n科学现实主义说明：尽管通过滑动窗口形成的块样本在有限的 $L$ 下是相关的，但 Miller–Madow 修正为离散分布的熵提供了一种有原则的、广泛使用的一阶偏差调整。在重叠块的情况下，这种修正是近似的，您应该谨慎地解释它。\n\n您的程序必须使用以下测试套件。每个测试用例提供两个列表：长度为 $2$ 的块的经验计数和长度为 $3$ 的块的经验计数，按上述指定顺序排列。\n\n测试套件（五个用例）：\n- 用例 A（近似独立的公平硬币）：\n  - 长度为 $2$ 的计数：$[2500,2500,2500,2500]$\n  - 长度为 $3$ 的计数：$[1250,1250,1250,1250,1250,1250,1250,1250]$\n- 用例 B（近似确定性的交替序列 $0101\\ldots$）：\n  - 长度为 $2$ 的计数：$[0,5000,5000,0]$\n  - 长度为 $3$ 的计数：$[0,2500,0,0,0,2500,0,0]$\n- 用例 C（强持续性的类二元马尔可夫行为）：\n  - 长度为 $2$ 的计数：$[4500,500,500,4500]$\n  - 长度为 $3$ 的计数：$[4000,500,250,250,250,250,500,4000]$\n- 用例 D（混合零的小样本）：\n  - 长度为 $2$ 的计数：$[4,1,1,4]$\n  - 长度为 $3$ 的计数：$[3,0,1,0,1,0,0,3]$\n- 用例 E（具有极端偏斜的近确定性）：\n  - 长度为 $2$ 的计数：$[9,0,0,1]$\n  - 长度为 $3$ 的计数：$[9,0,0,0,0,0,0,1]$\n\n输出规范：\n- 对于每个测试用例，生成一个包含三个浮点数的列表：插件熵率估计（单位为比特）、使用 Miller–Madow 修正的偏差调整后熵率估计（单位为比特），以及隐含的偏差修正（偏差调整后值减去插件值）。\n- 您的程序应生成单行输出，其中包含五个测试用例的结果，格式为一个由逗号分隔的列表组成的列表，并用方括号括起来，每个浮点数四舍五入到六位小数，例如：`[[hA_plugin,hA_MM,hA_corr],[hB_plugin,hB_MM,hB_corr],...]`。\n\n不需要物理单位或角度。熵必须以比特表示。不得使用百分比；任何比例在适用时必须表示为小数。",
            "solution": "该问题是有效的。它在科学上基于信息论的原理，定义清晰，解题路径唯一，表述客观。解决该问题所需的所有数据和常量均已提供。\n\n任务是根据经验块计数估计平稳二元随机过程的熵率。这将通过三个主要步骤完成：\n1.  基于长度为 $2$ 和 $3$ 的块的经验熵，计算熵率的插件估计。\n2.  将 Miller-Madow 偏差修正应用于经验块熵，以考虑有限样本效应。\n3.  从修正后的块熵构建偏差调整后的熵率估计，并量化总修正量。\n\n设 $\\{X_t\\}$ 为一个平稳二元随机过程。\n\n**1. 插件熵率估计**\n\n一个平稳过程的熵率 $h$ 由以下极限定义：\n$$h = \\lim_{L \\to \\infty} h_L = \\lim_{L \\to \\infty} [H(X_1^L) - H(X_1^{L-1})]$$\n其中 $X_1^L$ 表示一个由 $L$ 个连续变量 $(X_1, X_2, \\ldots, X_L)$ 组成的块，$H(X_1^L)$ 是其香non熵。在实践中，我们通过在较小的块长度处截断此极限来从有限数据中估计 $h$，本例中使用 $L=3$。因此，熵率的估计器为 $h \\approx h_3 = H(X_1^3) - H(X_1^2)$。\n\n我们被给予了长度为 $L \\in \\{2, 3\\}$ 的每个二元块 $w$ 的经验计数 $c_L(w)$。首先，我们计算观测到的长度为 $L$ 的块的总数：\n$$N_L = \\sum_{w \\in \\{0,1\\}^L} c_L(w)$$\n那么，块 $w$ 的经验概率（或相对频率）为：\n$$\\hat{p}_L(w) = \\frac{c_L(w)}{N_L}$$\n块熵的插件估计器 $\\hat{H}_L$ 是该经验分布的香non熵：\n$$\\hat{H}_L = -\\sum_{w \\in \\{0,1\\}^L} \\hat{p}_L(w) \\log_2 \\hat{p}_L(w)$$\n其中 $\\hat{p}_L(w) = 0$ 的项对总和没有贡献，遵循约定 $\\lim_{p \\to 0^+} p \\log p = 0$。\n\n使用 $L=2$ 和 $L=3$ 的这些经验块熵，熵率的插件估计器为：\n$$\\hat{h}_{\\text{plugin}} = \\hat{H}_3 - \\hat{H}_2$$\n\n**2. 有限样本偏差修正**\n\n已知插件估计器 $\\hat{H}_L$ 是有偏的。它系统地低估了真实的熵，特别是对于小样本量 $N_L$。对此偏差的一阶修正由 Miller-Madow 公式给出。对于一个有 $|\\mathcal{X}_L|$ 种可能块的字母表，其中 $K_L$ 种在样本中被观测到（即具有非零计数），偏差近似为：\n$$\\text{Bias}(\\hat{H}_L) \\approx -\\frac{K_L - 1}{2N_L \\ln 2}$$\n分母中包含 $\\ln 2$ 是为了将以奈特（nats）为自然单位的偏差转换为比特（bits），这与我们使用 $\\log_2$ 计算熵是一致的。因此，修正后的熵 $\\hat{H}_{L, \\text{MM}}$ 为：\n$$\\hat{H}_{L, \\text{MM}} = \\hat{H}_L + \\frac{K_L - 1}{2N_L \\ln 2}$$\n此修正必须分别应用于 $\\hat{H}_2$ 和 $\\hat{H}_3$，使用它们各自的样本量（$N_2$、$N_3$）和观测到的块类型数（$K_2$、$K_3$）。\n\n**3. 偏差调整后的熵率和隐含修正**\n\n偏差调整后的熵率估计是通过取修正后的块熵的差来构建的：\n$$\\hat{h}_{\\text{MM}} = \\hat{H}_{3, \\text{MM}} - \\hat{H}_{2, \\text{MM}}$$\n代入修正后熵的表达式：\n$$\\hat{h}_{\\text{MM}} = \\left(\\hat{H}_3 + \\frac{K_3 - 1}{2N_3 \\ln 2}\\right) - \\left(\\hat{H}_2 + \\frac{K_2 - 1}{2N_2 \\ln 2}\\right)$$\n对熵率估计的隐含总修正 $\\hat{h}_{\\text{corr}}$ 是调整后估计与插件估计之间的差：\n$$\\hat{h}_{\\text{corr}} = \\hat{h}_{\\text{MM}} - \\hat{h}_{\\text{plugin}}$$\n这可以简化为各个偏差修正的差：\n$$\\hat{h}_{\\text{corr}} = \\left(\\frac{K_3 - 1}{2N_3 \\ln 2}\\right) - \\left(\\frac{K_2 - 1}{2N_2 \\ln 2}\\right)$$\n\n程序将为每个提供的测试用例实现这一系列计算。它将首先计算 $\\hat{H}_2$ 和 $\\hat{H}_3$，从中导出 $\\hat{h}_{\\text{plugin}}$。然后，它将计算各自的 Miller-Madow 修正，以找到 $\\hat{H}_{2, \\text{MM}}$ 和 $\\hat{H}_{3, \\text{MM}}$，从而得到 $\\hat{h}_{\\text{MM}}$。最后，它将计算差值 $\\hat{h}_{\\text{corr}}$。对于每个用例，将返回三个量（$\\hat{h}_{\\text{plugin}}$、$\\hat{h}_{\\text{MM}}$、$\\hat{h}_{\\text{corr}}$）。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes plug-in and bias-corrected entropy rate estimates for a binary process.\n    \"\"\"\n\n    test_cases = [\n        # Case A: (approximately independent fair coin)\n        (\n            [2500, 2500, 2500, 2500],\n            [1250, 1250, 1250, 1250, 1250, 1250, 1250, 1250]\n        ),\n        # Case B: (approximately deterministic alternating sequence 0101...)\n        (\n            [0, 5000, 5000, 0],\n            [0, 2500, 0, 0, 0, 2500, 0, 0]\n        ),\n        # Case C: (strongly persistent binary Markov-like behavior)\n        (\n            [4500, 500, 500, 4500],\n            [4000, 500, 250, 250, 250, 250, 500, 4000]\n        ),\n        # Case D: (small-sample mixed with zeros)\n        (\n            [4, 1, 1, 4],\n            [3, 0, 1, 0, 1, 0, 0, 3]\n        ),\n        # Case E: (nearly deterministic with extreme skew)\n        (\n            [9, 0, 0, 1],\n            [9, 0, 0, 0, 0, 0, 0, 1]\n        )\n    ]\n\n    def compute_metrics(counts):\n        \"\"\"\n        Computes empirical entropy and Miller-Madow correction parameters.\n\n        Args:\n            counts (np.ndarray): Array of empirical counts for blocks.\n\n        Returns:\n            tuple: (empirical_entropy, num_samples, num_observed_categories)\n        \"\"\"\n        counts = np.array(counts, dtype=float)\n        num_samples = np.sum(counts)\n\n        if num_samples == 0:\n            return 0.0, 0, 0\n\n        # Non-zero counts and their corresponding probabilities\n        non_zero_counts = counts[counts  0]\n        probs = non_zero_counts / num_samples\n\n        # Empirical entropy (plug-in)\n        empirical_entropy = -np.sum(probs * np.log2(probs))\n\n        # Number of observed categories (non-zero counts)\n        num_observed_categories = len(non_zero_counts)\n\n        return empirical_entropy, num_samples, num_observed_categories\n\n    results = []\n    for counts2, counts3 in test_cases:\n        # 1. Compute metrics for L=2 and L=3 blocks\n        H2, N2, K2 = compute_metrics(counts2)\n        H3, N3, K3 = compute_metrics(counts3)\n\n        # 2. Compute plug-in entropy rate estimate\n        h_plugin = H3 - H2\n\n        # 3. Compute Miller-Madow bias corrections for H2 and H3\n        bias_H2 = 0.0\n        if N2  0 and K2  1:\n            bias_H2 = (K2 - 1) / (2 * N2 * np.log(2))\n\n        bias_H3 = 0.0\n        if N3  0 and K3  1:\n            bias_H3 = (K3 - 1) / (2 * N3 * np.log(2))\n\n        # 4. Compute bias-adjusted entropies and the resulting entropy rate\n        H2_mm = H2 + bias_H2\n        H3_mm = H3 + bias_H3\n        h_mm = H3_mm - H2_mm\n\n        # 5. Compute the implied correction on the entropy rate estimate\n        h_corr = h_mm - h_plugin\n        \n        results.append([h_plugin, h_mm, h_corr])\n\n    # Format the output string as a comma-separated list of lists,\n    # with each float rounded to six decimal places.\n    formatted_sublists = []\n    for sublist in results:\n        formatted_numbers = [f\"{num:.6f}\" for num in sublist]\n        formatted_sublists.append(f\"[{','.join(formatted_numbers)}]\")\n    \n    final_output = f\"[{','.join(formatted_sublists)}]\"\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}