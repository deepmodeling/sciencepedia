{
    "hands_on_practices": [
        {
            "introduction": "The foundation of information theory is Shannon entropy, a measure that quantifies the average uncertainty or 'surprise' inherent in a random variable's possible outcomes. Before we can analyze the complexity of dynamic processes, we must first be proficient in calculating this fundamental quantity for a static probability distribution. This exercise provides practice in computing entropy from first principles, converting between the common units of nats and bits, and benchmarking the result against the case of maximum uncertainty, represented by the uniform distribution .",
            "id": "4126716",
            "problem": "In a complex adaptive system where agents adaptively select among actions based on experience, consider a single agent whose action-selection mechanism has converged to a discrete distribution over an action set of size $3$, with probabilities $p=(1/2,1/3,1/6)$ assigned to the three actions. Using the fundamental definition of Shannon entropy as the expected self-information and the standard conversion rules between natural logarithm (nats) and base-$2$ logarithm (bits), do the following: compute the entropy of this distribution in nats and in bits, and compare it to the entropy of the uniform distribution over $3$ outcomes. You must derive all expressions from first principles and clearly justify any base conversion used. Your final reported answer must be the exact closed-form expression for the difference in bits between the entropy of the uniform distribution and the entropy of $p$, namely $H(u)-H(p)$ in bits, where $u$ denotes the uniform distribution over $3$ outcomes. Do not round; provide an exact expression. The final answer must be a single expression with no units.",
            "solution": "The problem is subjected to validation before a solution is attempted.\n\n### Step 1: Extract Givens\n-   System: A single agent in a complex adaptive system.\n-   Action set size: $3$.\n-   Probability distribution of actions, $p$: $(1/2, 1/3, 1/6)$.\n-   Task 1: Compute the Shannon entropy of the distribution $p$ in nats and bits.\n-   Task 2: Compare the entropy of $p$ to the entropy of the uniform distribution over $3$ outcomes, denoted as $u$.\n-   Task 3: Derive all expressions from first principles and justify the conversion between logarithmic bases.\n-   Final Answer Requirement: Provide the exact closed-form expression for the difference $H(u) - H(p)$ in units of bits.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded:** The problem is firmly rooted in information theory, a core discipline in mathematics and physics, and is central to the quantitative study of complex systems. The concepts of Shannon entropy, self-information, and logarithmic base conversion are standard and well-established.\n-   **Well-Posed:** The problem is well-posed. The given probability distribution $p = (1/2, 1/3, 1/6)$ is valid, as the probabilities are non-negative and sum to unity: $\\frac{1}{2} + \\frac{1}{3} + \\frac{1}{6} = \\frac{3}{6} + \\frac{2}{6} + \\frac{1}{6} = \\frac{6}{6} = 1$. The tasks are specific and lead to a unique, meaningful solution.\n-   **Objective:** The problem statement is expressed in precise, objective language, free from ambiguity or subjective claims.\n\nThe problem does not exhibit any of the flaws listed in the validation criteria. It is scientifically sound, formally structured, internally consistent, and requires a rigorous application of fundamental principles.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be provided.\n\n### Solution Derivation\n\nThe self-information of an outcome $i$ occurring with probability $p_i$ is defined as $I(p_i) = -\\log(p_i)$. The base of the logarithm determines the units of information. For this problem, we will use base $e$ (natural logarithm, $\\ln$) for nats and base $2$ (binary logarithm, $\\log_2$) for bits.\n\nThe Shannon entropy, $H$, of a discrete probability distribution $P = \\{p_1, p_2, \\dots, p_n\\}$ is the expected value of the self-information over all possible outcomes:\n$$H(P) = E[I(p_i)] = \\sum_{i=1}^{n} p_i I(p_i) = -\\sum_{i=1}^{n} p_i \\log(p_i)$$\n\nFirst, we compute the entropy of the given distribution $p = (1/2, 1/3, 1/6)$ in nats. This requires using the natural logarithm. Let $H_e(p)$ be the entropy in nats.\n$$H_e(p) = -\\left( \\frac{1}{2} \\ln\\left(\\frac{1}{2}\\right) + \\frac{1}{3} \\ln\\left(\\frac{1}{3}\\right) + \\frac{1}{6} \\ln\\left(\\frac{1}{6}\\right) \\right)$$\nUsing the logarithmic identity $\\ln(1/x) = -\\ln(x)$:\n$$H_e(p) = \\frac{1}{2} \\ln(2) + \\frac{1}{3} \\ln(3) + \\frac{1}{6} \\ln(6)$$\nWe can simplify $\\ln(6)$ using the identity $\\ln(ab) = \\ln(a) + \\ln(b)$:\n$$H_e(p) = \\frac{1}{2} \\ln(2) + \\frac{1}{3} \\ln(3) + \\frac{1}{6} (\\ln(2) + \\ln(3))$$\nCollecting terms with $\\ln(2)$ and $\\ln(3)$:\n$$H_e(p) = \\left(\\frac{1}{2} + \\frac{1}{6}\\right) \\ln(2) + \\left(\\frac{1}{3} + \\frac{1}{6}\\right) \\ln(3)$$\n$$H_e(p) = \\left(\\frac{3}{6} + \\frac{1}{6}\\right) \\ln(2) + \\left(\\frac{2}{6} + \\frac{1}{6}\\right) \\ln(3)$$\n$$H_e(p) = \\frac{4}{6} \\ln(2) + \\frac{3}{6} \\ln(3) = \\frac{2}{3} \\ln(2) + \\frac{1}{2} \\ln(3)$$\nThis is the entropy of the distribution $p$ in nats.\n\nNext, we convert this entropy to bits. The conversion between logarithms of different bases $a$ and $b$ is given by the change of base formula: $\\log_b(x) = \\frac{\\log_a(x)}{\\log_a(b)}$. To convert from nats (base $e$) to bits (base $2$), we set $a=e$ and $b=2$:\n$$\\log_2(x) = \\frac{\\ln(x)}{\\ln(2)}$$\nThe entropy in bits, $H_2(P)$, is defined as $H_2(P) = -\\sum p_i \\log_2(p_i)$. Substituting the change of base formula:\n$$H_2(P) = -\\sum p_i \\frac{\\ln(p_i)}{\\ln(2)} = \\frac{1}{\\ln(2)} \\left( -\\sum p_i \\ln(p_i) \\right) = \\frac{H_e(P)}{\\ln(2)}$$\nThis is the fundamental justification for the conversion: the entropy in bits is the entropy in nats divided by the constant $\\ln(2)$.\nUsing this conversion, we find the entropy of $p$ in bits, $H_2(p)$:\n$$H_2(p) = \\frac{H_e(p)}{\\ln(2)} = \\frac{\\frac{2}{3} \\ln(2) + \\frac{1}{2} \\ln(3)}{\\ln(2)}$$\n$$H_2(p) = \\frac{2}{3} \\frac{\\ln(2)}{\\ln(2)} + \\frac{1}{2} \\frac{\\ln(3)}{\\ln(2)}$$\nUsing the change of base formula again, $\\frac{\\ln(3)}{\\ln(2)} = \\log_2(3)$.\n$$H_2(p) = \\frac{2}{3} + \\frac{1}{2} \\log_{2}(3)$$\nThis is the entropy of the distribution $p$ in bits.\n\nNow, we compute the entropy of the uniform distribution over $3$ outcomes, $u = (1/3, 1/3, 1/3)$, in bits. Let this be $H_2(u)$.\n$$H_2(u) = -\\sum_{i=1}^{3} u_i \\log_{2}(u_i) = -\\sum_{i=1}^{3} \\frac{1}{3} \\log_{2}\\left(\\frac{1}{3}\\right)$$\n$$H_2(u) = -3 \\cdot \\frac{1}{3} \\log_{2}\\left(\\frac{1}{3}\\right) = -\\log_{2}\\left(\\frac{1}{3}\\right)$$\nUsing the logarithmic identity $\\log(1/x) = -\\log(x)$:\n$$H_2(u) = \\log_{2}(3)$$\nThis confirms the general result that the entropy of a uniform distribution over $N$ states is $\\log_{2}(N)$ bits.\n\nFinally, we compute the required difference, $H(u) - H(p)$, in bits. This corresponds to $H_2(u) - H_2(p)$.\n$$H_2(u) - H_2(p) = \\log_{2}(3) - \\left(\\frac{2}{3} + \\frac{1}{2} \\log_{2}(3)\\right)$$\n$$H_2(u) - H_2(p) = \\log_{2}(3) - \\frac{2}{3} - \\frac{1}{2} \\log_{2}(3)$$\nCollecting the $\\log_{2}(3)$ terms:\n$$H_2(u) - H_2(p) = \\left(1 - \\frac{1}{2}\\right) \\log_{2}(3) - \\frac{2}{3}$$\n$$H_2(u) - H_2(p) = \\frac{1}{2} \\log_{2}(3) - \\frac{2}{3}$$\nThis is the final, exact, closed-form expression for the difference in entropy. This quantity is also known as the redundancy of the distribution $p$ or its Kullback-Leibler divergence from the uniform distribution.",
            "answer": "$$\\boxed{\\frac{1}{2} \\log_{2}(3) - \\frac{2}{3}}$$"
        },
        {
            "introduction": "While entropy rate quantifies a process's intrinsic randomness, it does not measure the complexity of the mechanism generating that randomness. Two processes can have the same entropy rate but vastly different internal structures. This practice introduces the concept of statistical complexity, $C_{\\mu}$, from computational mechanics, which measures the amount of information about the past a process must store to optimally predict its future . By identifying the 'causal states' of a process, you will build a minimal model of its hidden structure and quantify its memory.",
            "id": "4126673",
            "problem": "Consider a stationary binary stochastic process $\\{X_t\\}_{t \\in \\mathbb{Z}}$ with alphabet $\\{0,1\\}$, generated by the following conditional distributions that depend on the parity of the number of consecutive $1$'s since the most recent $0$ in the semi-infinite past. Let $N_t$ denote the number of consecutive $1$'s in the past ending at time $t$ since the most recent $0$. The generation rule is:\n- If $N_t$ is even (including $N_t = 0$), then $\\mathbb{P}(X_{t+1} = 0 \\mid N_t \\text{ even}) = \\alpha$ and $\\mathbb{P}(X_{t+1} = 1 \\mid N_t \\text{ even}) = 1 - \\alpha$.\n- If $N_t$ is odd, then $\\mathbb{P}(X_{t+1} = 1 \\mid N_t \\text{ odd}) = 1$.\n\nAssume $0 < \\alpha < 1$ and that the process is in its stationary regime. For the specific parameter value $\\alpha = \\frac{2}{3}$:\n\n- Using the predictive equivalence relation over pasts, identify the causal states of this process.\n- Construct the minimal unifilar state-transition graph (the predictive model whose nodes are the causal states) by specifying, for each causal state, the outgoing transitions labeled by the emitted symbol and their probabilities, along with the destination causal state.\n- Determine the stationary distribution over the causal states induced by the transition dynamics and compute the statistical complexity $C_{\\mu}$ (the information storage in the causal-state distribution) in nats, using the standard definition of information and complexity appropriate to this context.\n\nExpress your final answer for $C_{\\mu}$ in nats and round your result to six significant figures.",
            "solution": "The problem statement is analyzed for validity.\n\n## Problem Validation\n\n### Step 1: Extract Givens\n- A stationary binary stochastic process $\\{X_t\\}_{t \\in \\mathbb{Z}}$ with alphabet $\\{0,1\\}$.\n- A variable $N_t$ is defined as the number of consecutive $1$'s in the past ending at time $t$ since the most recent $0$.\n- The conditional probability distribution for the next symbol $X_{t+1}$ depends on the parity of $N_t$:\n    - If $N_t$ is even (including $N_t = 0$), then $\\mathbb{P}(X_{t+1} = 0 \\mid N_t \\text{ even}) = \\alpha$ and $\\mathbb{P}(X_{t+1} = 1 \\mid N_t \\text{ even}) = 1 - \\alpha$.\n    - If $N_t$ is odd, then $\\mathbb{P}(X_{t+1} = 1 \\mid N_t \\text{ odd}) = 1$ (which implies $\\mathbb{P}(X_{t+1} = 0 \\mid N_t \\text{ odd}) = 0$).\n- A constraint on the parameter $\\alpha$ is given: $0 < \\alpha < 1$.\n- A specific value for the parameter is provided for calculations: $\\alpha = \\frac{2}{3}$.\n- The tasks are to:\n    1. Identify the causal states of the process.\n    2. Construct the minimal unifilar state-transition graph.\n    3. Determine the stationary distribution over causal states and compute the statistical complexity $C_{\\mu}$ in nats, rounded to six significant figures.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded and Objective**: The problem is a well-defined exercise in computational mechanics, a subfield of complex systems theory and statistical physics. It deals with standard concepts such as stochastic processes, causal states, and statistical complexity ($C_{\\mu}$). The language is precise and objective.\n- **Well-Posed and Complete**: The process is fully specified by the conditional probabilities. The parameter $\\alpha$ is constrained, and a specific value is given for the final calculation. The objectives are clearly stated and the required concepts (causal states, statistical complexity) have standard definitions in the field. The problem is self-contained and provides all necessary information to derive a unique solution.\n- **No other flaws**: The problem does not violate any physical laws or mathematical logic. It is not trivial, as it requires the application of specific theoretical constructs. It is not underspecified, overconstrained, or ambiguous.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A full solution will be provided.\n\n## Solution\n\nThe solution proceeds by first identifying the causal states, then constructing the state transition graph, calculating the stationary state distribution, and finally computing the statistical complexity.\n\n### 1. Causal State Identification\nCausal states are equivalence classes of past histories. Two pasts, $h_t = (\\dots, x_{t-2}, x_{t-1})$ and $h'_t = (\\dots, x'_{t-2}, x'_{t-1})$, are in the same causal state if and only if they yield the same conditional probability distribution for all future events: $\\mathbb{P}(X_{t:\\infty} \\mid h_t) = \\mathbb{P}(X_{t:\\infty} \\mid h'_t)$.\n\nThe generation rule depends only on the parity of the number of consecutive $1$'s at the end of the past sequence. Let $k(h_t)$ be this number. We can define equivalence classes based on this property.\n\n- **Class A**: Pasts where $k(h_t)$ is even. This includes pasts ending in a $0$ (for which $k=0$) and pasts ending in an even number of $1$'s, e.g., $(\\dots, 0, 1, 1)$. For any past $h_t$ in this class, the distribution of the next symbol $X_t$ is $\\mathbb{P}(X_t=0 \\mid h_t) = \\alpha$ and $\\mathbb{P}(X_t=1 \\mid h_t) = 1-\\alpha$.\n    - If $X_t=0$ is generated, the new past $h_{t+1}=(\\dots, x_{t-1}, 0)$ has $k(h_{t+1})=0$, which is even. The process remains in Class A.\n    - If $X_t=1$ is generated, the new past $h_{t+1}=(\\dots, x_{t-1}, 1)$ will have an odd number of trailing $1$'s (since the previous number was even). The process transitions to Class B.\n\n- **Class B**: Pasts where $k(h_t)$ is odd, e.g., $(\\dots, 0, 1)$ or $(\\dots, 0, 1, 1, 1)$. For any past $h_t$ in this class, the distribution of the next symbol $X_t$ is $\\mathbb{P}(X_t=1 \\mid h_t) = 1$ and $\\mathbb{P}(X_t=0 \\mid h_t) = 0$.\n    - If $X_t=1$ is generated (which it must be), the new past $h_{t+1}=(\\dots, x_{t-1}, 1)$ will have an even number of trailing $1$'s (since the previous number was odd). The process transitions to Class A.\n\nSince the future probability distribution is determined entirely by which class the current past belongs to, these two classes are the causal states of the process. Let us denote them as $S_A$ and $S_B$.\n- **Causal State $S_A$**: The set of all pasts ending in an even number of consecutive $1$'s.\n- **Causal State $S_B$**: The set of all pasts ending in an odd number of consecutive $1$'s.\n\n### 2. State-Transition Graph\nThe minimal unifilar predictive model (the $\\epsilon$-machine) consists of the two causal states $S_A$ and $S_B$ and their transitions. The transitions are labeled by `symbol|probability`.\n\n- From state $S_A$:\n    - On emitting a $0$ (with probability $\\alpha$), the new past ends in $0$, so the number of trailing $1$'s is $0$ (even). The system transitions to state $S_A$. Transition: $S_A \\xrightarrow{0|\\alpha} S_A$.\n    - On emitting a $1$ (with probability $1-\\alpha$), the new past has an odd number of trailing $1$'s. The system transitions to state $S_B$. Transition: $S_A \\xrightarrow{1|1-\\alpha} S_B$.\n\n- From state $S_B$:\n    - Emitting a $0$ has probability $0$.\n    - On emitting a $1$ (with probability $1$), the new past has an even number of trailing $1$'s. The system transitions to state $S_A$. Transition: $S_B \\xrightarrow{1|1} S_A$.\n\nThe stochastic transition matrix $T$ between causal states, where $T_{ij} = \\mathbb{P}(\\text{next state is } j \\mid \\text{current state is } i)$, is given by:\n$$ T = \\begin{pmatrix} \\alpha & 1-\\alpha \\\\ 1 & 0 \\end{pmatrix} $$\nwhere the first row/column corresponds to $S_A$ and the second to $S_B$.\n\n### 3. Stationary Distribution and Statistical Complexity\nThe stationary distribution $\\pi = \\begin{pmatrix} \\pi_A & \\pi_B \\end{pmatrix}$ over the causal states is the left eigenvector of $T$ with eigenvalue $1$. It must satisfy $\\pi T = \\pi$ and the normalization condition $\\pi_A + \\pi_B = 1$.\n\nThe equation $\\pi T = \\pi$ yields the system:\n$$ \\pi_A \\alpha + \\pi_B (1) = \\pi_A $$\n$$ \\pi_A (1-\\alpha) + \\pi_B (0) = \\pi_B $$\nFrom the second equation, we get $\\pi_B = \\pi_A(1-\\alpha)$. Substituting this into the normalization condition:\n$$ \\pi_A + \\pi_A(1-\\alpha) = 1 $$\n$$ \\pi_A (1 + 1 - \\alpha) = 1 $$\n$$ \\pi_A (2 - \\alpha) = 1 \\implies \\pi_A = \\frac{1}{2-\\alpha} $$\nThen, the probability of being in state $S_B$ is:\n$$ \\pi_B = 1 - \\pi_A = 1 - \\frac{1}{2-\\alpha} = \\frac{2-\\alpha-1}{2-\\alpha} = \\frac{1-\\alpha}{2-\\alpha} $$\nSo, the stationary distribution is $\\pi = \\left( \\frac{1}{2-\\alpha}, \\frac{1-\\alpha}{2-\\alpha} \\right)$.\n\nFor the specific value $\\alpha = \\frac{2}{3}$:\n$$ \\pi_A = \\frac{1}{2 - \\frac{2}{3}} = \\frac{1}{\\frac{4}{3}} = \\frac{3}{4} $$\n$$ \\pi_B = \\frac{1 - \\frac{2}{3}}{2 - \\frac{2}{3}} = \\frac{\\frac{1}{3}}{\\frac{4}{3}} = \\frac{1}{4} $$\nThe stationary distribution is $\\pi = \\left( \\frac{3}{4}, \\frac{1}{4} \\right)$.\n\nThe statistical complexity $C_{\\mu}$ is the Shannon entropy of this stationary distribution, measured in nats (using the natural logarithm, $\\ln$).\n$$ C_{\\mu} = H(\\pi) = - \\sum_{i \\in \\{A, B\\}} \\pi_i \\ln(\\pi_i) $$\n$$ C_{\\mu} = - \\left( \\pi_A \\ln(\\pi_A) + \\pi_B \\ln(\\pi_B) \\right) $$\nSubstituting the values for $\\pi_A$ and $\\pi_B$:\n$$ C_{\\mu} = - \\left( \\frac{3}{4} \\ln\\left(\\frac{3}{4}\\right) + \\frac{1}{4} \\ln\\left(\\frac{1}{4}\\right) \\right) $$\nWe can expand this expression:\n$$ C_{\\mu} = - \\frac{3}{4}(\\ln(3) - \\ln(4)) - \\frac{1}{4}(\\ln(1) - \\ln(4)) $$\nSince $\\ln(1)=0$ and $\\ln(4) = \\ln(2^2) = 2\\ln(2)$:\n$$ C_{\\mu} = - \\frac{3}{4}(\\ln(3) - 2\\ln(2)) - \\frac{1}{4}(-2\\ln(2)) $$\n$$ C_{\\mu} = - \\frac{3}{4}\\ln(3) + \\frac{6}{4}\\ln(2) + \\frac{2}{4}\\ln(2) $$\n$$ C_{\\mu} = - \\frac{3}{4}\\ln(3) + \\frac{8}{4}\\ln(2) $$\n$$ C_{\\mu} = 2\\ln(2) - \\frac{3}{4}\\ln(3) $$\nTo find the numerical value, we use $\\ln(2) \\approx 0.69314718$ and $\\ln(3) \\approx 1.09861229$.\n$$ C_{\\mu} \\approx 2(0.69314718) - \\frac{3}{4}(1.09861229) $$\n$$ C_{\\mu} \\approx 1.38629436 - 0.82395922 $$\n$$ C_{\\mu} \\approx 0.56233514 $$\nRounding to six significant figures, the statistical complexity is $0.562335$ nats.",
            "answer": "$$\\boxed{0.562335}$$"
        },
        {
            "introduction": "In most scientific applications, the true probability distributions governing a complex system are unknown and must be estimated from finite data. This exercise bridges the gap between theory and practice by tasking you with estimating the entropy rate of a stochastic process from empirical block counts . You will implement the standard 'plug-in' estimator and confront a critical challenge in empirical information theory: the systematic underestimation bias in finite samples, applying the Miller-Madow correction to improve your estimate.",
            "id": "4126703",
            "problem": "You are given empirical counts of all binary blocks of lengths $2$ and $3$ sampled from a single, long realization of a stationary binary stochastic process. Your task is to estimate the entropy rate using a plug-in estimator that is derived directly from the foundational definitions of Shannon entropy and entropy rate for stationary processes, and then quantify and interpret finite-sample bias using a principled correction derived from asymptotic bias analysis.\n\nStart from the following fundamental bases:\n- Shannon entropy for a discrete distribution $P$ over a finite set $\\mathcal{X}$ is defined as $H(P) = -\\sum_{x \\in \\mathcal{X}} p(x) \\log_2 p(x)$.\n- For a stationary stochastic process $\\{X_t\\}_{t \\in \\mathbb{Z}}$ on a finite alphabet, the entropy rate is defined as $h = \\lim_{L \\to \\infty} H(X_1^L) - H(X_1^{L-1})$, where $X_1^L$ denotes the length $L$ block and $H(X_1^L)$ is the Shannon entropy of the block distribution.\n- The plug-in estimator replaces the true block distribution by the empirical block distribution formed from observed counts, and constructs the entropy rate estimator from the corresponding empirical block entropies.\n- Empirical block entropies must treat zero-probability events using the limit convention $\\lim_{p \\to 0^+} p \\log p = 0$ to avoid undefined terms.\n\nUsing these bases, derive a program that:\n1. Computes empirical block probabilities for length-$2$ and length-$3$ blocks from the provided counts, for a binary alphabet $\\{0,1\\}$ with block orders fixed as $[00,01,10,11]$ for length $2$ and $[000,001,010,011,100,101,110,111]$ for length $3$.\n2. Computes empirical block entropies in bits for lengths $2$ and $3$.\n3. Constructs the plug-in entropy rate estimate from these empirical block entropies.\n4. Quantifies finite-sample bias using the Miller–Madow correction applied separately to the length-$2$ and length-$3$ empirical block entropies. For a discrete distribution with $N$ samples and $K$ observed categories (nonzero counts), the corrected entropy in bits is $H_{\\text{MM}} = H_{\\text{plug-in}} + \\frac{K-1}{2N \\log 2}$, where $\\log$ denotes the natural logarithm. Use this correction to produce a bias-adjusted entropy rate estimate and the implied bias correction for the plug-in entropy rate estimator.\n5. Express all entropies and entropy rates in bits as floats.\n\nScientific realism note: Although block samples formed by sliding windows are dependent for finite $L$, the Miller–Madow correction provides a principled, widely used first-order bias adjustment for entropies of discrete distributions. In overlapping blocks this correction is approximate, and you should interpret it cautiously.\n\nYour program must use the following test suite. Each test case provides two lists: empirical counts for length-$2$ blocks and empirical counts for length-$3$ blocks, ordered as specified above.\n\nTest Suite (five cases):\n- Case A (approximately independent fair coin):\n  - Length-$2$ counts: $[2500,2500,2500,2500]$\n  - Length-$3$ counts: $[1250,1250,1250,1250,1250,1250,1250,1250]$\n- Case B (approximately deterministic alternating sequence $0101\\ldots$):\n  - Length-$2$ counts: $[0,5000,5000,0]$\n  - Length-$3$ counts: $[0,2500,0,0,0,2500,0,0]$\n- Case C (strongly persistent binary Markov-like behavior):\n  - Length-$2$ counts: $[4500,500,500,4500]$\n  - Length-$3$ counts: $[4000,500,250,250,250,250,500,4000]$\n- Case D (small-sample mixed with zeros):\n  - Length-$2$ counts: $[4,1,1,4]$\n  - Length-$3$ counts: $[3,0,1,0,1,0,0,3]$\n- Case E (nearly deterministic with extreme skew):\n  - Length-$2$ counts: $[9,0,0,1]$\n  - Length-$3$ counts: $[9,0,0,0,0,0,0,1]$\n\nOutput specification:\n- For each test case, produce a list with three floats: the plug-in entropy rate estimate in bits, the bias-adjusted entropy rate estimate in bits using the Miller–Madow correction, and the implied bias correction (bias-adjusted minus plug-in).\n- Your program should produce a single line of output containing the results for the five cases as a comma-separated list of lists enclosed in square brackets, with each float rounded to six decimal places, for example: `[[hA_plugin,hA_MM,hA_corr],[hB_plugin,hB_MM,hB_corr],...]`.\n\nNo physical units or angles are needed. Entropy must be expressed in bits. Percentages must not be used; any proportions must be expressed as decimals when applicable.",
            "solution": "The problem is valid. It is scientifically grounded in the principles of information theory, well-posed with clear definitions and a unique solution path, and objective in its formulation. All data and constants required for the solution are provided.\n\nThe task is to estimate the entropy rate of a stationary binary stochastic process from empirical block counts. This will be accomplished in three main steps:\n1.  Calculating a plug-in estimate of the entropy rate based on empirical entropies of length-$2$ and length-$3$ blocks.\n2.  Applying the Miller-Madow bias correction to the empirical block entropies to account for finite-sample effects.\n3.  Constructing a bias-adjusted entropy rate estimate from the corrected block entropies and quantifying the total correction.\n\nLet $\\{X_t\\}$ be a stationary binary stochastic process.\n\n**1. Plug-in Entropy Rate Estimation**\n\nThe entropy rate, $h$, of a stationary process is defined by the limit:\n$$h = \\lim_{L \\to \\infty} h_L = \\lim_{L \\to \\infty} [H(X_1^L) - H(X_1^{L-1})]$$\nwhere $X_1^L$ denotes a block of $L$ consecutive variables $(X_1, X_2, \\ldots, X_L)$, and $H(X_1^L)$ is its Shannon entropy. In practice, we estimate $h$ from finite data by truncating this limit at a small block length, in this case using $L=3$. The estimator for the entropy rate is thus $h \\approx h_3 = H(X_1^3) - H(X_1^2)$.\n\nWe are given empirical counts, $c_L(w)$, for each binary block $w$ of length $L \\in \\{2, 3\\}$.\nFirst, we compute the total number of observed blocks of length $L$:\n$$N_L = \\sum_{w \\in \\{0,1\\}^L} c_L(w)$$\nThe empirical probability (or relative frequency) of block $w$ is then:\n$$\\hat{p}_L(w) = \\frac{c_L(w)}{N_L}$$\nThe plug-in estimator for the block entropy, $\\hat{H}_L$, is the Shannon entropy of this empirical distribution:\n$$\\hat{H}_L = -\\sum_{w \\in \\{0,1\\}^L} \\hat{p}_L(w) \\log_2 \\hat{p}_L(w)$$\nThe terms where $\\hat{p}_L(w) = 0$ contribute nothing to the sum, following the convention $\\lim_{p \\to 0^+} p \\log p = 0$.\n\nUsing these empirical block entropies for $L=2$ and $L=3$, the plug-in estimator for the entropy rate is:\n$$\\hat{h}_{\\text{plugin}} = \\hat{H}_3 - \\hat{H}_2$$\n\n**2. Finite-Sample Bias Correction**\n\nThe plug-in estimator $\\hat{H}_L$ is known to be biased. It systematically underestimates the true entropy, especially for small sample sizes $N_L$. A first-order correction for this bias is given by the Miller-Madow formula. For an alphabet with $|\\mathcal{X}_L|$ possible blocks, of which $K_L$ are observed in the sample (i.e., have non-zero counts), the bias is approximately:\n$$\\text{Bias}(\\hat{H}_L) \\approx -\\frac{K_L - 1}{2N_L \\ln 2}$$\nThe denominator contains $\\ln 2$ to convert the bias, which is naturally expressed in nats, to bits, consistent with our use of $\\log_2$ for entropy. The corrected entropy, $\\hat{H}_{L, \\text{MM}}$, is therefore:\n$$\\hat{H}_{L, \\text{MM}} = \\hat{H}_L + \\frac{K_L - 1}{2N_L \\ln 2}$$\nThis correction must be applied separately to $\\hat{H}_2$ and $\\hat{H}_3$, using their respective sample sizes ($N_2$, $N_3$) and numbers of observed block types ($K_2$, $K_3$).\n\n**3. Bias-Adjusted Entropy Rate and Implied Correction**\n\nThe bias-adjusted estimate for the entropy rate is constructed by taking the difference of the corrected block entropies:\n$$\\hat{h}_{\\text{MM}} = \\hat{H}_{3, \\text{MM}} - \\hat{H}_{2, \\text{MM}}$$\nSubstituting the expressions for the corrected entropies:\n$$\\hat{h}_{\\text{MM}} = \\left(\\hat{H}_3 + \\frac{K_3 - 1}{2N_3 \\ln 2}\\right) - \\left(\\hat{H}_2 + \\frac{K_2 - 1}{2N_2 \\ln 2}\\right)$$\nThe implied total correction for the entropy rate estimate, $\\hat{h}_{\\text{corr}}$, is the difference between the adjusted and plug-in estimates:\n$$\\hat{h}_{\\text{corr}} = \\hat{h}_{\\text{MM}} - \\hat{h}_{\\text{plugin}}$$\nThis simplifies to the difference of the individual bias corrections:\n$$\\hat{h}_{\\text{corr}} = \\left(\\frac{K_3 - 1}{2N_3 \\ln 2}\\right) - \\left(\\frac{K_2 - 1}{2N_2 \\ln 2}\\right)$$\n\nThe program will implement this sequence of calculations for each provided test case. It will first compute $\\hat{H}_2$ and $\\hat{H}_3$, from which $\\hat{h}_{\\text{plugin}}$ is derived. It will then compute the respective Miller-Madow corrections to find $\\hat{H}_{2, \\text{MM}}$ and $\\hat{H}_{3, \\text{MM}}$, leading to $\\hat{h}_{\\text{MM}}$. Finally, it will compute the difference $\\hat{h}_{\\text{corr}}$. The three quantities ($\\hat{h}_{\\text{plugin}}$, $\\hat{h}_{\\text{MM}}$, $\\hat{h}_{\\text{corr}}$) will be returned for each case.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes plug-in and bias-corrected entropy rate estimates for a binary process.\n    \"\"\"\n\n    test_cases = [\n        # Case A: (approximately independent fair coin)\n        (\n            [2500, 2500, 2500, 2500],\n            [1250, 1250, 1250, 1250, 1250, 1250, 1250, 1250]\n        ),\n        # Case B: (approximately deterministic alternating sequence 0101...)\n        (\n            [0, 5000, 5000, 0],\n            [0, 2500, 0, 0, 0, 2500, 0, 0]\n        ),\n        # Case C: (strongly persistent binary Markov-like behavior)\n        (\n            [4500, 500, 500, 4500],\n            [4000, 500, 250, 250, 250, 250, 500, 4000]\n        ),\n        # Case D: (small-sample mixed with zeros)\n        (\n            [4, 1, 1, 4],\n            [3, 0, 1, 0, 1, 0, 0, 3]\n        ),\n        # Case E: (nearly deterministic with extreme skew)\n        (\n            [9, 0, 0, 1],\n            [9, 0, 0, 0, 0, 0, 0, 1]\n        )\n    ]\n\n    def compute_metrics(counts):\n        \"\"\"\n        Computes empirical entropy and Miller-Madow correction parameters.\n\n        Args:\n            counts (np.ndarray): Array of empirical counts for blocks.\n\n        Returns:\n            tuple: (empirical_entropy, num_samples, num_observed_categories)\n        \"\"\"\n        counts = np.array(counts, dtype=float)\n        num_samples = np.sum(counts)\n\n        if num_samples == 0:\n            return 0.0, 0, 0\n\n        # Non-zero counts and their corresponding probabilities\n        non_zero_counts = counts[counts > 0]\n        probs = non_zero_counts / num_samples\n\n        # Empirical entropy (plug-in)\n        empirical_entropy = -np.sum(probs * np.log2(probs))\n\n        # Number of observed categories (non-zero counts)\n        num_observed_categories = len(non_zero_counts)\n\n        return empirical_entropy, num_samples, num_observed_categories\n\n    results = []\n    for counts2, counts3 in test_cases:\n        # 1. Compute metrics for L=2 and L=3 blocks\n        H2, N2, K2 = compute_metrics(counts2)\n        H3, N3, K3 = compute_metrics(counts3)\n\n        # 2. Compute plug-in entropy rate estimate\n        h_plugin = H3 - H2\n\n        # 3. Compute Miller-Madow bias corrections for H2 and H3\n        bias_H2 = 0.0\n        if N2 > 0 and K2 > 1:\n            bias_H2 = (K2 - 1) / (2 * N2 * np.log(2))\n\n        bias_H3 = 0.0\n        if N3 > 0 and K3 > 1:\n            bias_H3 = (K3 - 1) / (2 * N3 * np.log(2))\n\n        # 4. Compute bias-adjusted entropies and the resulting entropy rate\n        H2_mm = H2 + bias_H2\n        H3_mm = H3 + bias_H3\n        h_mm = H3_mm - H2_mm\n\n        # 5. Compute the implied correction on the entropy rate estimate\n        h_corr = h_mm - h_plugin\n        \n        results.append([h_plugin, h_mm, h_corr])\n\n    # Format the output string as a comma-separated list of lists,\n    # with each float rounded to six decimal places.\n    formatted_sublists = []\n    for sublist in results:\n        formatted_numbers = [f\"{num:.6f}\" for num in sublist]\n        formatted_sublists.append(f\"[{','.join(formatted_numbers)}]\")\n    \n    final_output = f\"[{','.join(formatted_sublists)}]\"\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}