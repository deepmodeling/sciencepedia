{
    "hands_on_practices": [
        {
            "introduction": "本练习将带我们回到集体智慧最核心的原则之一：“群众的智慧”。我们将通过第一性原理，推导群体平均估计的方差。这个练习旨在揭示一个深刻的见解：一个群体的集体准确性不仅取决于其成员的个体准确性（以方差 $\\sigma^2$ 表示），更关键地取决于他们之间误差的相关性 $\\rho$ 。通过这个推导，你将亲手量化为何在构建有效的集体智慧时，多样性（即低相关性）与个体能力同等重要。",
            "id": "4128693",
            "problem": "一个由 $n$ 个代理组成的群体协作估计一个固定的标量 $\\theta$，每个代理报告其个体估计值 $Y_{i} = \\theta + \\varepsilon_{i}$，其中 $i \\in \\{1,\\dots,n\\}$。误差 $\\varepsilon_{i}$ 是无偏的，$ \\mathbb{E}[\\varepsilon_{i}] = 0$，并且是可交换的，具有共同方差 $\\operatorname{Var}(\\varepsilon_{i}) = \\sigma^{2}$ 和对所有 $i \\neq j$ 的恒定成对相关性 $\\operatorname{Corr}(\\varepsilon_{i},\\varepsilon_{j}) = \\rho$。该群体将其集体估计值定义为简单平均值 $ \\bar{Y} = \\frac{1}{n} \\sum_{i=1}^{n} Y_{i} $。\n\n从方差和协方差的核心定义出发，并且不使用任何快捷公式，推导出一个用 $n$、$\\sigma^{2}$ 和 $\\rho$ 表示的 $\\operatorname{Var}(\\bar{Y})$ 的闭式表达式。然后，对 $n=25$，$\\sigma^{2}=1$ 和 $\\rho=0.2$ 的情况计算该表达式的值。最后，计算群体平均方差与单个代理方差的比率，即 $\\operatorname{Var}(\\bar{Y}) / \\sigma^{2}$。\n\n请以精确值的形式给出 $\\big(\\operatorname{Var}(\\bar{Y}),\\ \\operatorname{Var}(\\bar{Y}) / \\sigma^{2}\\big)$ 这对答案。不要四舍五入。你的最终答案必须使用 $\\text{pmatrix}$ 环境以行矩阵的形式给出。",
            "solution": "该问题被评估为有效。它在科学上基于标准概率论，问题陈述清晰，包含了所有必要信息，并且表述客观。提供的数值（$n=25$, $\\sigma^2=1$, $\\rho=0.2$）是一致的；具体来说，相关系数 $\\rho$ 位于一个 $n$ 维可交换相关矩阵的有效范围内，即 $\\rho \\in [-\\frac{1}{n-1}, 1]$。对于 $n=25$，这个范围是 $[-\\frac{1}{24}, 1]$，而 $\\rho=0.2$ 完全在此界限内。\n\n任务是推导群体平均值方差 $\\operatorname{Var}(\\bar{Y})$ 的表达式，然后进行求值。群体平均值定义为 $\\bar{Y} = \\frac{1}{n} \\sum_{i=1}^{n} Y_{i}$。\n\n首先，我们用真实量 $\\theta$ 和误差项 $\\varepsilon_i$ 来表示 $\\bar{Y}$。\n$$\n\\bar{Y} = \\frac{1}{n} \\sum_{i=1}^{n} (\\theta + \\varepsilon_{i}) = \\frac{1}{n} \\left( \\sum_{i=1}^{n} \\theta + \\sum_{i=1}^{n} \\varepsilon_{i} \\right) = \\frac{1}{n} \\left( n\\theta + \\sum_{i=1}^{n} \\varepsilon_{i} \\right) = \\theta + \\frac{1}{n} \\sum_{i=1}^{n} \\varepsilon_{i}\n$$\n一个随机变量加上一个常数后的方差等于该随机变量自身的方差。因此，\n$$\n\\operatorname{Var}(\\bar{Y}) = \\operatorname{Var}\\left(\\theta + \\frac{1}{n} \\sum_{i=1}^{n} \\varepsilon_{i}\\right) = \\operatorname{Var}\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\varepsilon_{i}\\right)\n$$\n使用属性 $\\operatorname{Var}(aX) = a^{2}\\operatorname{Var}(X)$，其中 $a = \\frac{1}{n}$ 是一个常数，我们有：\n$$\n\\operatorname{Var}(\\bar{Y}) = \\frac{1}{n^2} \\operatorname{Var}\\left(\\sum_{i=1}^{n} \\varepsilon_{i}\\right)\n$$\n问题要求从核心定义出发推导和的方差。设 $S = \\sum_{i=1}^{n} \\varepsilon_{i}$。$S$ 的方差定义为 $\\operatorname{Var}(S) = \\mathbb{E}[(S - \\mathbb{E}[S])^2]$。\n首先，我们求 $S$ 的期望：\n$$\n\\mathbb{E}[S] = \\mathbb{E}\\left[\\sum_{i=1}^{n} \\varepsilon_{i}\\right] = \\sum_{i=1}^{n} \\mathbb{E}[\\varepsilon_{i}]\n$$\n已知对于所有 $i$，$\\mathbb{E}[\\varepsilon_{i}] = 0$，所以我们有 $\\mathbb{E}[S] = 0$。\n$S$ 的方差简化为：\n$$\n\\operatorname{Var}(S) = \\mathbb{E}[(S - 0)^2] = \\mathbb{E}[S^2]\n$$\n我们现在展开 $S^2$：\n$$\nS^2 = \\left(\\sum_{i=1}^{n} \\varepsilon_{i}\\right)^2 = \\left(\\sum_{i=1}^{n} \\varepsilon_{i}\\right)\\left(\\sum_{j=1}^{n} \\varepsilon_{j}\\right) = \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\varepsilon_{i}\\varepsilon_{j}\n$$\n我们可以将这个双重求和分解为索引相等（$i=j$）的项和索引不相等（$i \\neq j$）的项：\n$$\nS^2 = \\sum_{i=1}^{n} \\varepsilon_{i}^2 + \\sum_{i \\neq j} \\varepsilon_{i}\\varepsilon_{j}\n$$\n现在，我们求 $S^2$ 的期望：\n$$\n\\mathbb{E}[S^2] = \\mathbb{E}\\left[ \\sum_{i=1}^{n} \\varepsilon_{i}^2 + \\sum_{i \\neq j} \\varepsilon_{i}\\varepsilon_{j} \\right] = \\sum_{i=1}^{n} \\mathbb{E}[\\varepsilon_{i}^2] + \\sum_{i \\neq j} \\mathbb{E}[\\varepsilon_{i}\\varepsilon_{j}]\n$$\n我们将这些期望项与给定的方差和相关性联系起来。\n对于任何随机变量 $X$，$\\operatorname{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$。因为 $\\mathbb{E}[\\varepsilon_{i}] = 0$ 且 $\\operatorname{Var}(\\varepsilon_{i}) = \\sigma^2$，我们有：\n$$\n\\sigma^2 = \\mathbb{E}[\\varepsilon_{i}^2] - 0^2 \\implies \\mathbb{E}[\\varepsilon_{i}^2] = \\sigma^2\n$$\n对于任意两个随机变量 $X$ 和 $Y$，$\\operatorname{Cov}(X,Y) = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y]$。对于 $i \\neq j$，因为 $\\mathbb{E}[\\varepsilon_{i}] = \\mathbb{E}[\\varepsilon_{j}] = 0$，我们有：\n$$\n\\operatorname{Cov}(\\varepsilon_{i}, \\varepsilon_{j}) = \\mathbb{E}[\\varepsilon_{i}\\varepsilon_{j}] - 0 \\cdot 0 \\implies \\mathbb{E}[\\varepsilon_{i}\\varepsilon_{j}] = \\operatorname{Cov}(\\varepsilon_{i}, \\varepsilon_{j})\n$$\n相关性定义为 $\\operatorname{Corr}(\\varepsilon_{i}, \\varepsilon_{j}) = \\frac{\\operatorname{Cov}(\\varepsilon_{i}, \\varepsilon_{j})}{\\sqrt{\\operatorname{Var}(\\varepsilon_{i})\\operatorname{Var}(\\varepsilon_{j})}}$。根据给定的值 $\\operatorname{Corr}(\\varepsilon_{i}, \\varepsilon_{j}) = \\rho$ 和 $\\operatorname{Var}(\\varepsilon_{i}) = \\sigma^2$，我们求出 $i \\neq j$ 时的协方差：\n$$\n\\operatorname{Cov}(\\varepsilon_{i}, \\varepsilon_{j}) = \\rho \\sqrt{\\sigma^2 \\cdot \\sigma^2} = \\rho \\sigma^2\n$$\n现在我们将这些结果代回到 $\\mathbb{E}[S^2]$ 的表达式中：\n$$\n\\operatorname{Var}(S) = \\mathbb{E}[S^2] = \\sum_{i=1}^{n} \\sigma^2 + \\sum_{i \\neq j} \\rho\\sigma^2\n$$\n第一个和式有 $n$ 个相同的项，所以其值为 $n\\sigma^2$。第二个和式是对所有不同索引对 $(i,j)$ 求和。共有 $n(n-1)$ 个这样的对。因此，第二个和式的值为 $n(n-1)\\rho\\sigma^2$。\n$$\n\\operatorname{Var}(S) = n\\sigma^2 + n(n-1)\\rho\\sigma^2\n$$\n最后，我们将其代回到 $\\operatorname{Var}(\\bar{Y})$ 的表达式中：\n$$\n\\operatorname{Var}(\\bar{Y}) = \\frac{1}{n^2} \\operatorname{Var}(S) = \\frac{1}{n^2} [n\\sigma^2 + n(n-1)\\rho\\sigma^2]\n$$\n从括号内的项中提出公因子 $n\\sigma^2$，得到闭式表达式：\n$$\n\\operatorname{Var}(\\bar{Y}) = \\frac{n\\sigma^2}{n^2} [1 + (n-1)\\rho] = \\frac{\\sigma^2}{n}[1 + (n-1)\\rho]\n$$\n推导到此完成。\n\n接下来，我们对 $n=25$, $\\sigma^2=1$, 和 $\\rho=0.2$ 计算该表达式的值。\n$$\n\\operatorname{Var}(\\bar{Y}) = \\frac{1}{25}[1 + (25-1)(0.2)] = \\frac{1}{25}[1 + (24)(0.2)] = \\frac{1}{25}[1 + 4.8] = \\frac{5.8}{25}\n$$\n为了给出精确值，我们将小数转换为分数：\n$$\n\\operatorname{Var}(\\bar{Y}) = \\frac{5.8}{25} = \\frac{58}{250} = \\frac{29}{125}\n$$\n最后，我们计算群体平均方差与单个代理方差的比率 $\\operatorname{Var}(\\bar{Y}) / \\sigma^2$。\n使用推导出的闭式表达式：\n$$\n\\frac{\\operatorname{Var}(\\bar{Y})}{\\sigma^2} = \\frac{1}{\\sigma^2} \\left( \\frac{\\sigma^2}{n}[1 + (n-1)\\rho] \\right) = \\frac{1}{n}[1 + (n-1)\\rho]\n$$\n使用所给的值：\n$$\n\\frac{\\operatorname{Var}(\\bar{Y})}{\\sigma^2} = \\frac{1}{25}[1 + (24)(0.2)] = \\frac{5.8}{25} = \\frac{29}{125}\n$$\n或者，使用之前计算出的 $\\operatorname{Var}(\\bar{Y})$ 值和给定的 $\\sigma^2=1$：\n$$\n\\frac{\\operatorname{Var}(\\bar{Y})}{\\sigma^2} = \\frac{29/125}{1} = \\frac{29}{125}\n$$\n这对答案是 $\\left(\\operatorname{Var}(\\bar{Y}), \\frac{\\operatorname{Var}(\\bar{Y})}{\\sigma^2}\\right) = \\left(\\frac{29}{125}, \\frac{29}{125}\\right)$。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{29}{125} & \\frac{29}{125} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "在简单平均法的基础上，我们现在更进一步，探索一种更精密的聚合方法：最优线性加权。当我们可以获取关于不同个体误差结构的信息（即他们的协方差矩阵 $\\Sigma$）时，我们就不再局限于“一人一票”的朴素平均，而是可以计算出最小化最终估计方差的最优权重 。本练习将引导你从第一性原理出发，推导广义最小二乘（GLS）估计中的最优权重，并让你深入理解如何利用个体间的相关性信息来构建更精确的集体判断。",
            "id": "4128739",
            "problem": "在一个集体智能场景中，一个协调者希望聚合 $2$ 个代理关于一个标量潜在状态 $\\theta$ 的估计。设观测向量为 $\\mathbf{y} = \\theta \\mathbf{1} + \\boldsymbol{\\varepsilon}$，其中 $\\mathbf{1} \\in \\mathbb{R}^{2}$ 是全一向量，$\\mathbb{E}[\\boldsymbol{\\varepsilon}] = \\mathbf{0}$，且 $\\operatorname{Cov}(\\boldsymbol{\\varepsilon}) = \\Sigma$。协调者使用一个线性无偏聚合器 $\\hat{\\theta} = \\mathbf{w}^{\\top} \\mathbf{y}$，其无偏性约束为 $\\mathbf{1}^{\\top}\\mathbf{w} = 1$。仅使用无偏性、线性形式的协方差和一阶最优性条件的定义，从第一性原理推导广义最小二乘（GLS）权重（广义最小二乘（GLS）是在已知误差协方差下具有最小方差的线性无偏估计量），该权重最小化估计量方差 $\\operatorname{Var}(\\hat{\\theta})$，然后计算所得方差。\n\n给定协方差矩阵\n$$\n\\Sigma = \\begin{pmatrix} 1 & 0.5 \\\\ 0.5 & 1 \\end{pmatrix}.\n$$\n执行以下步骤：\n- 在无偏性约束下推导最优的 $\\mathbf{w}$。\n- 计算最优估计量的方差。\n- 计算使用 $\\mathbf{w}_{\\mathrm{eq}} = \\begin{pmatrix}1/2\\\\1/2\\end{pmatrix}$ 的朴素等权重估计量的方差。\n提供精确值（不要四舍五入）。按顺序将您的最终答案表示为有序四元组 $(w_1, w_2, V_{\\mathrm{GLS}}, V_{\\mathrm{eq}})$。",
            "solution": "问题要求从向量观测 $\\mathbf{y}$ 中推导标量参数 $\\theta$ 的线性无偏估计量的最优权重，并计算相关方差，以及一个简单的等权重估计量的方差。\n\n模型由 $\\mathbf{y} = \\theta \\mathbf{1} + \\boldsymbol{\\varepsilon}$ 给出，其中 $\\mathbf{y} \\in \\mathbb{R}^{2}$，$\\mathbf{1}$ 是全一向量，$\\mathbb{E}[\\boldsymbol{\\varepsilon}] = \\mathbf{0}$，且 $\\operatorname{Cov}(\\boldsymbol{\\varepsilon}) = \\Sigma$。估计量是一个线性聚合器 $\\hat{\\theta} = \\mathbf{w}^{\\top} \\mathbf{y}$。\n\n首先，我们建立无偏性条件。估计量的期望是：\n$$\n\\mathbb{E}[\\hat{\\theta}] = \\mathbb{E}[\\mathbf{w}^{\\top} \\mathbf{y}] = \\mathbf{w}^{\\top} \\mathbb{E}[\\mathbf{y}]\n$$\n观测向量 $\\mathbf{y}$ 的期望是：\n$$\n\\mathbb{E}[\\mathbf{y}] = \\mathbb{E}[\\theta \\mathbf{1} + \\boldsymbol{\\varepsilon}] = \\theta \\mathbf{1} + \\mathbb{E}[\\boldsymbol{\\varepsilon}] = \\theta \\mathbf{1}\n$$\n将此代入 $\\mathbb{E}[\\hat{\\theta}]$ 的表达式中：\n$$\n\\mathbb{E}[\\hat{\\theta}] = \\mathbf{w}^{\\top} (\\theta \\mathbf{1}) = \\theta (\\mathbf{w}^{\\top} \\mathbf{1})\n$$\n为使估计量无偏，对于任意 $\\theta$ 值，我们必须有 $\\mathbb{E}[\\hat{\\theta}] = \\theta$。这要求乘以 $\\theta$ 的项为1：\n$$\n\\mathbf{w}^{\\top} \\mathbf{1} = 1\n$$\n这就是无偏性约束，对于 $\\mathbf{w} = \\begin{pmatrix} w_1 \\\\ w_2 \\end{pmatrix}$，可以写成 $w_1 + w_2 = 1$。\n\n接下来，我们求估计量的方差，这是需要最小化的量。\n$$\n\\operatorname{Var}(\\hat{\\theta}) = \\operatorname{Var}(\\mathbf{w}^{\\top} \\mathbf{y}) = \\operatorname{Var}(\\mathbf{w}^{\\top} (\\theta \\mathbf{1} + \\boldsymbol{\\varepsilon}))\n$$\n因为 $\\theta$ 是一个非随机（潜在）状态，它对方差没有贡献。因此：\n$$\n\\operatorname{Var}(\\hat{\\theta}) = \\operatorname{Var}(\\mathbf{w}^{\\top} \\boldsymbol{\\varepsilon}) = \\mathbf{w}^{\\top} \\operatorname{Cov}(\\boldsymbol{\\varepsilon}) \\mathbf{w} = \\mathbf{w}^{\\top} \\Sigma \\mathbf{w}\n$$\n问题是在无偏性约束下最小化这个方差。这是一个约束优化问题：最小化 $f(\\mathbf{w}) = \\mathbf{w}^{\\top} \\Sigma \\mathbf{w}$，约束条件为 $g(\\mathbf{w}) = \\mathbf{w}^{\\top} \\mathbf{1} - 1 = 0$。\n\n我们使用拉格朗日乘子法。拉格朗日函数 $\\mathcal{L}$ 是：\n$$\n\\mathcal{L}(\\mathbf{w}, \\lambda) = \\mathbf{w}^{\\top} \\Sigma \\mathbf{w} - \\lambda (\\mathbf{w}^{\\top} \\mathbf{1} - 1)\n$$\n为了找到最优权重 $\\mathbf{w}$，我们将 $\\mathcal{L}$ 对 $\\mathbf{w}$ 的梯度设为零（一阶最优性条件）。由于 $\\Sigma$ 是对称矩阵，二次型 $\\mathbf{w}^{\\top} \\Sigma \\mathbf{w}$ 对 $\\mathbf{w}$ 的梯度是 $2 \\Sigma \\mathbf{w}$。\n$$\n\\nabla_{\\mathbf{w}} \\mathcal{L} = 2 \\Sigma \\mathbf{w} - \\lambda \\mathbf{1} = \\mathbf{0}\n$$\n这得到 $2 \\Sigma \\mathbf{w} = \\lambda \\mathbf{1}$。假设 $\\Sigma$ 是可逆的，我们可以解出 $\\mathbf{w}$：\n$$\n\\mathbf{w} = \\frac{\\lambda}{2} \\Sigma^{-1} \\mathbf{1}\n$$\n为了求出拉格朗日乘子 $\\lambda$，我们将这个 $\\mathbf{w}$ 的表达式代回约束条件 $\\mathbf{w}^{\\top} \\mathbf{1} = 1$：\n$$\n\\left(\\frac{\\lambda}{2} \\Sigma^{-1} \\mathbf{1}\\right)^{\\top} \\mathbf{1} = 1\n$$\n$$\n\\frac{\\lambda}{2} \\mathbf{1}^{\\top} (\\Sigma^{-1})^{\\top} \\mathbf{1} = 1\n$$\n因为 $\\Sigma$ 是协方差矩阵，所以它是对称的，其逆矩阵也是对称的，即 $(\\Sigma^{-1})^{\\top} = \\Sigma^{-1}$。\n$$\n\\frac{\\lambda}{2} \\mathbf{1}^{\\top} \\Sigma^{-1} \\mathbf{1} = 1 \\implies \\lambda = \\frac{2}{\\mathbf{1}^{\\top} \\Sigma^{-1} \\mathbf{1}}\n$$\n将这个 $\\lambda$ 代回 $\\mathbf{w}$ 的表达式，得到最优权重向量，记为 $\\mathbf{w}_{\\mathrm{GLS}}$：\n$$\n\\mathbf{w}_{\\mathrm{GLS}} = \\frac{1}{2} \\left( \\frac{2}{\\mathbf{1}^{\\top} \\Sigma^{-1} \\mathbf{1}} \\right) \\Sigma^{-1} \\mathbf{1} = \\frac{\\Sigma^{-1} \\mathbf{1}}{\\mathbf{1}^{\\top} \\Sigma^{-1} \\mathbf{1}}\n$$\n这就完成了从第一性原理推导最优权重的过程。\n\n现在我们计算具体数值。给定的协方差矩阵是：\n$$\n\\Sigma = \\begin{pmatrix} 1 & 0.5 \\\\ 0.5 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 1/2 \\\\ 1/2 & 1 \\end{pmatrix}\n$$\n首先，我们求它的逆矩阵 $\\Sigma^{-1}$。行列式是 $\\det(\\Sigma) = (1)(1) - (1/2)(1/2) = 1 - 1/4 = 3/4$。\n$$\n\\Sigma^{-1} = \\frac{1}{3/4} \\begin{pmatrix} 1 & -1/2 \\\\ -1/2 & 1 \\end{pmatrix} = \\frac{4}{3} \\begin{pmatrix} 1 & -1/2 \\\\ -1/2 & 1 \\end{pmatrix} = \\begin{pmatrix} 4/3 & -2/3 \\\\ -2/3 & 4/3 \\end{pmatrix}\n$$\n接下来，我们计算 $\\mathbf{w}_{\\mathrm{GLS}}$ 表达式中的各项。其中 $\\mathbf{1} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$：\n$$\n\\Sigma^{-1} \\mathbf{1} = \\begin{pmatrix} 4/3 & -2/3 \\\\ -2/3 & 4/3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 4/3 - 2/3 \\\\ -2/3 + 4/3 \\end{pmatrix} = \\begin{pmatrix} 2/3 \\\\ 2/3 \\end{pmatrix}\n$$\n分母是：\n$$\n\\mathbf{1}^{\\top} \\Sigma^{-1} \\mathbf{1} = \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\begin{pmatrix} 2/3 \\\\ 2/3 \\end{pmatrix} = 2/3 + 2/3 = 4/3\n$$\n最优权重是：\n$$\n\\mathbf{w}_{\\mathrm{GLS}} = \\frac{\\begin{pmatrix} 2/3 \\\\ 2/3 \\end{pmatrix}}{4/3} = \\begin{pmatrix} (2/3)/(4/3) \\\\ (2/3)/(4/3) \\end{pmatrix} = \\begin{pmatrix} 1/2 \\\\ 1/2 \\end{pmatrix}\n$$\n所以，最优权重是 $w_1 = 1/2$ 和 $w_2 = 1/2$。\n\n最优估计量的方差 $V_{\\mathrm{GLS}}$ 是 $\\mathbf{w}_{\\mathrm{GLS}}^{\\top} \\Sigma \\mathbf{w}_{\\mathrm{GLS}}$。\n$$\nV_{\\mathrm{GLS}} = \\begin{pmatrix} 1/2 & 1/2 \\end{pmatrix} \\begin{pmatrix} 1 & 1/2 \\\\ 1/2 & 1 \\end{pmatrix} \\begin{pmatrix} 1/2 \\\\ 1/2 \\end{pmatrix} = \\begin{pmatrix} 1/2 & 1/2 \\end{pmatrix} \\begin{pmatrix} 1(1/2) + (1/2)(1/2) \\\\ (1/2)(1/2) + 1(1/2) \\end{pmatrix} = \\begin{pmatrix} 1/2 & 1/2 \\end{pmatrix} \\begin{pmatrix} 3/4 \\\\ 3/4 \\end{pmatrix}\n$$\n$$\nV_{\\mathrm{GLS}} = (1/2)(3/4) + (1/2)(3/4) = 3/8 + 3/8 = 6/8 = 3/4\n$$\n另外，可以证明最小方差通常为 $V_{\\mathrm{GLS}} = (\\mathbf{1}^{\\top} \\Sigma^{-1} \\mathbf{1})^{-1}$，在这里是 $(4/3)^{-1} = 3/4$。\n\n最后，我们计算朴素等权重估计量 $\\mathbf{w}_{\\mathrm{eq}} = \\begin{pmatrix} 1/2 \\\\ 1/2 \\end{pmatrix}$ 的方差。这个权重向量恰好与我们刚才推导出的最优权重向量相同。因此，其方差 $V_{\\mathrm{eq}}$ 必须与 $V_{\\mathrm{GLS}}$ 相同。\n$$\nV_{\\mathrm{eq}} = \\mathbf{w}_{\\mathrm{eq}}^{\\top} \\Sigma \\mathbf{w}_{\\mathrm{eq}} = \\begin{pmatrix} 1/2 & 1/2 \\end{pmatrix} \\begin{pmatrix} 1 & 1/2 \\\\ 1/2 & 1 \\end{pmatrix} \\begin{pmatrix} 1/2 \\\\ 1/2 \\end{pmatrix} = 3/4\n$$\n有序四元组 $(w_1, w_2, V_{\\mathrm{GLS}}, V_{\\mathrm{eq}})$ 是 $(1/2, 1/2, 3/4, 3/4)$。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{2} & \\frac{1}{2} & \\frac{3}{4} & \\frac{3}{4} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "现在，我们将视角从静态、一次性的信息聚合转向社会网络中动态的观点演化过程。本练习采用经典的DeGroot线性观点动态模型，并引入了一个关键的现实元素：“固执的个体”，他们从不改变自己的看法，如同网络中的意见领袖或外部信息源。通过分析这个包含固执个体的系统，你将计算出其他“适应性”个体的观点最终会收敛到何处 。这个练习揭示了网络结构和影响力“锚点”如何深刻地决定一个群体的最终共识，而这个共识往往是固执观点的一个复杂加权平均。",
            "id": "4128756",
            "problem": "考虑一个由线性加权平均动态（通常称为 DeGroot 模型）建模的集体意见形成过程：在离散时间 $t \\in \\{0,1,2,\\dots\\}$，意见向量 $x(t) \\in \\mathbb{R}^{n}$ 根据 $x(t+1) = W x(t)$ 演化，其中 $W \\in \\mathbb{R}^{n \\times n}$ 是一个行随机影响矩阵，其元素非负且每行之和为 $1$。在此设置中，一个固执代理人由 $W$ 中等于标准基向量的一行来编码，这使其自身权重为 $1$，对所有其他代理人的权重为零，从而使其意见随时间保持不变。这样一个固执代理人子集代表了群体中的外生信号，其余的代理人是适应性的。\n\n构建以下包含 $n=5$ 个代理人的网络，其中有两个固执代理人和三个适应性代理人。代理人 $1$ 和 $4$ 是固执的。影响矩阵 $W$ 由下式给出\n$$\nW \\;=\\;\n\\begin{pmatrix}\n1  &0  &0  &0  &0\\\\\n0.3  &0.4  &0.2  &0.1  &0\\\\\n0  &0.3  &0.5  &0.2  &0\\\\\n0  &0  &0  &1  &0\\\\\n0.1  &0  &0.5  &0.4  &0\n\\end{pmatrix}.\n$$\n设固执意见为 $x_{1}(0) = s_{1} = 0.3$ 和 $x_{4}(0) = s_{4} = 0.9$，由于 $W$ 中对固执性的编码，这些意见在所有时间内保持不变。适应性代理人 $2$、$3$ 和 $5$ 的初始意见是任意有限实数。\n\n从线性加权平均动态、由固执代理人引起的吸收行为以及稳态条件 $x^{\\ast} = W x^{\\ast}$ 的核心定义出发，推导适应性代理人的稳态意见，并确定代理人 $5$ 的稳态意见。将代理人 $5$ 的最终值表示为精确分数。不需要单位。",
            "solution": "问题陈述经评估有效。它在科学上基于已建立的线性共识模型（DeGroot 模型）理论，问题提法得当且有唯一解，并以客观、正式的语言表达。没有矛盾、缺失数据或其他会使其无效的缺陷。因此，我们可以进行推导。\n\n集体意见形成过程的动态由线性系统 $x(t+1) = W x(t)$ 描述，其中 $x(t) \\in \\mathbb{R}^{5}$ 是时间 $t$ 时的意见向量，$W \\in \\mathbb{R}^{5 \\times 5}$ 是影响矩阵。稳态意见向量，记为 $x^{\\ast}$，必须满足条件 $x^{\\ast} = W x^{\\ast}$。这代表了动力系统的一个不动点，在该点上意见不再随时间变化。\n\n影响矩阵 $W$ 如下：\n$$\nW \\;=\\;\n\\begin{pmatrix}\n1  &0  &0  &0  &0\\\\\n0.3  &0.4  &0.2  &0.1  &0\\\\\n0  &0.3  &0.5  &0.2  &0\\\\\n0  &0  &0  &1  &0\\\\\n0.1  &0  &0.5  &0.4  &0\n\\end{pmatrix}\n$$\n代理人 $1$ 和 $4$ 是固执的。这反映在 $W$ 的第一行和第四行，它们分别是标准基向量 $e_1^T = (1, 0, 0, 0, 0)$ 和 $e_4^T = (0, 0, 0, 1, 0)$。这种结构确保了他们的意见是固定的，即对所有 $t$ 都有 $x_1(t+1) = x_1(t)$ 和 $x_4(t+1) = x_4(t)$。他们的意见作为外生信号给出：对所有 $t \\geq 0$，有 $x_1(t) = s_1 = 0.3$ 和 $x_4(t) = s_4 = 0.9$。因此，他们的稳态意见是 $x_1^{\\ast} = 0.3$ 和 $x_4^{\\ast} = 0.9$。\n\n代理人 $2$、$3$ 和 $5$ 是适应性的，他们的稳态意见由整个网络的影响决定。我们可以将稳态条件 $x^{\\ast} = W x^{\\ast}$ 写成一个线性方程组：\n$x_1^{\\ast} = 1 \\cdot x_1^{\\ast}$\n$x_2^{\\ast} = 0.3 x_1^{\\ast} + 0.4 x_2^{\\ast} + 0.2 x_3^{\\ast} + 0.1 x_4^{\\ast} + 0 x_5^{\\ast}$\n$x_3^{\\ast} = 0 x_1^{\\ast} + 0.3 x_2^{\\ast} + 0.5 x_3^{\\ast} + 0.2 x_4^{\\ast} + 0 x_5^{\\ast}$\n$x_4^{\\ast} = 1 \\cdot x_4^{\\ast}$\n$x_5^{\\ast} = 0.1 x_1^{\\ast} + 0 x_2^{\\ast} + 0.5 x_3^{\\ast} + 0.4 x_4^{\\ast} + 0 x_5^{\\ast}$\n\n第一个和第四个方程是恒等式，与代理人 $1$ 和 $4$ 的固执性质一致。我们将已知的 $x_1^{\\ast} = 0.3$ 和 $x_4^{\\ast} = 0.9$ 的值代入适应性代理人的方程中：\n对于代理人 $2$：\n$x_2^{\\ast} = 0.3(0.3) + 0.4 x_2^{\\ast} + 0.2 x_3^{\\ast} + 0.1(0.9)$\n$x_2^{\\ast} = 0.09 + 0.4 x_2^{\\ast} + 0.2 x_3^{\\ast} + 0.09$\n整理各项以求解 $x_2^{\\ast}$：\n$(1 - 0.4) x_2^{\\ast} - 0.2 x_3^{\\ast} = 0.18$\n$0.6 x_2^{\\ast} - 0.2 x_3^{\\ast} = 0.18$\n为清晰起见，乘以 $10$：\n$6 x_2^{\\ast} - 2 x_3^{\\ast} = 1.8 \\implies 3 x_2^{\\ast} - x_3^{\\ast} = 0.9 \\quad (1)$\n\n对于代理人 $3$：\n$x_3^{\\ast} = 0.3 x_2^{\\ast} + 0.5 x_3^{\\ast} + 0.2(0.9)$\n$x_3^{\\ast} = 0.3 x_2^{\\ast} + 0.5 x_3^{\\ast} + 0.18$\n整理各项：\n$(1 - 0.5) x_3^{\\ast} - 0.3 x_2^{\\ast} = 0.18$\n$0.5 x_3^{\\ast} - 0.3 x_2^{\\ast} = 0.18$\n乘以 $10$：\n$5 x_3^{\\ast} - 3 x_2^{\\ast} = 1.8 \\quad (2)$\n\n我们现在得到了一个关于 $x_2^{\\ast}$ 和 $x_3^{\\ast}$ 的二元线性方程组。从方程 $(1)$，我们可以用 $x_2^{\\ast}$ 表示 $x_3^{\\ast}$：\n$x_3^{\\ast} = 3 x_2^{\\ast} - 0.9$\n\n将这个 $x_3^{\\ast}$ 的表达式代入方程 $(2)$：\n$5 (3 x_2^{\\ast} - 0.9) - 3 x_2^{\\ast} = 1.8$\n$15 x_2^{\\ast} - 4.5 - 3 x_2^{\\ast} = 1.8$\n$12 x_2^{\\ast} = 1.8 + 4.5$\n$12 x_2^{\\ast} = 6.3$\n$x_2^{\\ast} = \\frac{6.3}{12} = \\frac{63}{120}$\n为简化此分数，我们将分子和分母同除以它们的最大公约数 $3$：\n$x_2^{\\ast} = \\frac{63 \\div 3}{120 \\div 3} = \\frac{21}{40}$\n\n现在我们可以求出 $x_3^{\\ast}$：\n$x_3^{\\ast} = 3 x_2^{\\ast} - 0.9 = 3 \\left(\\frac{21}{40}\\right) - \\frac{9}{10} = \\frac{63}{40} - \\frac{36}{40} = \\frac{27}{40}$\n\n最后，我们使用代理人 $5$ 对应的方程来确定其稳态意见：\n$x_5^{\\ast} = 0.1 x_1^{\\ast} + 0.5 x_3^{\\ast} + 0.4 x_4^{\\ast}$\n代入已知的 $x_1^{\\ast}$、$x_3^{\\ast}$ 和 $x_4^{\\ast}$ 的值：\n$x_5^{\\ast} = 0.1(0.3) + 0.5\\left(\\frac{27}{40}\\right) + 0.4(0.9)$\n$x_5^{\\ast} = 0.03 + \\frac{1}{2}\\left(\\frac{27}{40}\\right) + 0.36$\n$x_5^{\\ast} = \\frac{3}{100} + \\frac{27}{80} + \\frac{36}{100}$\n合并分母为 $100$ 的项：\n$x_5^{\\ast} = \\frac{39}{100} + \\frac{27}{80}$\n为了将这些分数相加，我们找到一个公分母。$100 = 2^2 \\cdot 5^2$ 和 $80 = 2^4 \\cdot 5$ 的最小公倍数是 $2^4 \\cdot 5^2 = 16 \\cdot 25 = 400$。\n$x_5^{\\ast} = \\frac{39 \\cdot 4}{100 \\cdot 4} + \\frac{27 \\cdot 5}{80 \\cdot 5}$\n$x_5^{\\ast} = \\frac{156}{400} + \\frac{135}{400}$\n$x_5^{\\ast} = \\frac{156 + 135}{400} = \\frac{291}{400}$\n\n分数 $\\frac{291}{400}$ 是最简形式，因为分子 $291$ 的质因数分解为 $291 = 3 \\cdot 97$，而分母 $400$ 的质因数分解为 $400 = 2^4 \\cdot 5^2$。它们没有共同的质因数。\n因此，代理人 $5$ 的稳态意见是 $\\frac{291}{400}$。",
            "answer": "$$\\boxed{\\frac{291}{400}}$$"
        }
    ]
}