{
    "hands_on_practices": [
        {
            "introduction": "A fundamental first step in network analysis is to understand its large-scale organization, such as identifying separate, non-interacting groups of nodes. The graph Laplacian is a matrix representation of a network that elegantly encodes its connectivity properties. This practice demonstrates how to use the eigenvalues of the Laplacian to determine the number of connected components in a network, a key piece of structural information .",
            "id": "4133191",
            "problem": "Consider an undirected weighted network on $5$ nodes, where the weights represent nonnegative interaction strengths estimated from observational data in a complex adaptive system. Let the weighted adjacency matrix $A$ be\n$$\nA \\;=\\; \\begin{pmatrix}\n0 & 2 & 0 & 0 & 0 \\\\\n2 & 0 & 1 & 0 & 0 \\\\\n0 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 3 \\\\\n0 & 0 & 0 & 3 & 0\n\\end{pmatrix}.\n$$\nUsing the standard combinatorial graph Laplacian $L$ defined by $L \\;=\\; D \\;-\\; A$, where $D$ is the diagonal degree matrix with $D_{ii}$ equal to the weighted degree of node $i$, do the following:\n- Compute $L$ explicitly.\n- Compute the eigenvalues $\\lambda_{i}$ of $L$.\n- Based on foundational properties of the graph Laplacian for weighted undirected graphs with nonnegative weights, explain how the multiplicity of the eigenvalue $\\lambda \\;=\\; 0$ informs the number of connected components, and articulate the implication of this count for network structure reconstruction in complex adaptive systems.\n\nReport your final answer as the inferred number of connected components. No rounding is required, and no units are needed. Your reasoning must start from core definitions and well-tested facts relevant to network inference and structure reconstruction, avoiding any shortcut formulas presented in the problem statement itself.",
            "solution": "The problem asks for the computation of the graph Laplacian, its eigenvalues, and an interpretation of the multiplicity of the zero eigenvalue in the context of network structure. The final answer is the number of connected components inferred from this analysis.\n\nWe begin by defining the necessary components from first principles. An undirected weighted network can be represented by a symmetric weighted adjacency matrix $A$, where $A_{ij} = A_{ji}$ is the weight of the edge between nodes $i$ and $j$. For this problem, the network has $5$ nodes and the adjacency matrix is given as:\n$$\nA \\;=\\; \\begin{pmatrix}\n0 & 2 & 0 & 0 & 0 \\\\\n2 & 0 & 1 & 0 & 0 \\\\\n0 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 3 \\\\\n0 & 0 & 0 & 3 & 0\n\\end{pmatrix}\n$$\nThe weighted degree of a node $i$, denoted $d_i$, is the sum of the weights of all edges connected to it. It is calculated from the adjacency matrix as $d_i = \\sum_{j=1}^{N} A_{ij}$ for a network with $N$ nodes. For the given network with $N=5$:\n-   $d_1 = A_{11} + A_{12} + A_{13} + A_{14} + A_{15} = 0 + 2 + 0 + 0 + 0 = 2$\n-   $d_2 = A_{21} + A_{22} + A_{23} + A_{24} + A_{25} = 2 + 0 + 1 + 0 + 0 = 3$\n-   $d_3 = A_{31} + A_{32} + A_{33} + A_{34} + A_{35} = 0 + 1 + 0 + 0 + 0 = 1$\n-   $d_4 = A_{41} + A_{42} + A_{43} + A_{44} + A_{45} = 0 + 0 + 0 + 0 + 3 = 3$\n-   $d_5 = A_{51} + A_{52} + A_{53} + A_{54} + A_{55} = 0 + 0 + 0 + 3 + 0 = 3$\n\nThe degree matrix $D$ is a diagonal matrix where the diagonal elements $D_{ii}$ are the weighted degrees $d_i$.\n$$\nD \\;=\\; \\begin{pmatrix}\n2 & 0 & 0 & 0 & 0 \\\\\n0 & 3 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 3 & 0 \\\\\n0 & 0 & 0 & 0 & 3\n\\end{pmatrix}\n$$\nThe combinatorial graph Laplacian $L$ is defined as $L = D - A$. Substituting the matrices $D$ and $A$:\n$$\nL \\;=\\; \\begin{pmatrix}\n2 & 0 & 0 & 0 & 0 \\\\\n0 & 3 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 3 & 0 \\\\\n0 & 0 & 0 & 0 & 3\n\\end{pmatrix} - \\begin{pmatrix}\n0 & 2 & 0 & 0 & 0 \\\\\n2 & 0 & 1 & 0 & 0 \\\\\n0 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 3 \\\\\n0 & 0 & 0 & 3 & 0\n\\end{pmatrix} \\;=\\; \\begin{pmatrix}\n2 & -2 & 0 & 0 & 0 \\\\\n-2 & 3 & -1 & 0 & 0 \\\\\n0 & -1 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 3 & -3 \\\\\n0 & 0 & 0 & -3 & 3\n\\end{pmatrix}\n$$\nTo find the eigenvalues $\\lambda$ of $L$, we solve the characteristic equation $\\det(L - \\lambda I) = 0$. The matrix $L$ is block-diagonal, which simplifies the computation.\n$$\nL = \\begin{pmatrix} L_1 & \\mathbf{0} \\\\ \\mathbf{0} & L_2 \\end{pmatrix} \\quad \\text{where} \\quad L_1 = \\begin{pmatrix} 2 & -2 & 0 \\\\ -2 & 3 & -1 \\\\ 0 & -1 & 1 \\end{pmatrix} \\quad \\text{and} \\quad L_2 = \\begin{pmatrix} 3 & -3 \\\\ -3 & 3 \\end{pmatrix}\n$$\nThe set of eigenvalues of $L$ is the union of the eigenvalues of $L_1$ and $L_2$.\n\nFor $L_1$, we solve $\\det(L_1 - \\lambda I) = 0$:\n$$\n\\det \\begin{pmatrix} 2-\\lambda & -2 & 0 \\\\ -2 & 3-\\lambda & -1 \\\\ 0 & -1 & 1-\\lambda \\end{pmatrix} = (2-\\lambda)((3-\\lambda)(1-\\lambda)-1) - (-2)(-2(1-\\lambda)) = 0\n$$\n$$\n(2-\\lambda)(\\lambda^2 - 4\\lambda + 2) - 4(1-\\lambda) = 2\\lambda^2 - 8\\lambda + 4 - \\lambda^3 + 4\\lambda^2 - 2\\lambda - 4 + 4\\lambda = 0\n$$\n$$\n-\\lambda^3 + 6\\lambda^2 - 6\\lambda = 0 \\implies -\\lambda(\\lambda^2 - 6\\lambda + 6) = 0\n$$\nThis gives one eigenvalue $\\lambda_1 = 0$. The other two are roots of $\\lambda^2 - 6\\lambda + 6 = 0$, which, by the quadratic formula, are $\\lambda = \\frac{6 \\pm \\sqrt{36-24}}{2} = \\frac{6 \\pm \\sqrt{12}}{2} = 3 \\pm \\sqrt{3}$. So, the eigenvalues from $L_1$ are $\\{0, 3-\\sqrt{3}, 3+\\sqrt{3}\\}$.\n\nFor $L_2$, we solve $\\det(L_2 - \\lambda I) = 0$:\n$$\n\\det \\begin{pmatrix} 3-\\lambda & -3 \\\\ -3 & 3-\\lambda \\end{pmatrix} = (3-\\lambda)^2 - 9 = 0\n$$\n$$\n\\lambda^2 - 6\\lambda + 9 - 9 = 0 \\implies \\lambda(\\lambda-6) = 0\n$$\nThe eigenvalues from $L_2$ are $\\{0, 6\\}$.\n\nThe complete set of eigenvalues for $L$, sorted in non-decreasing order, is $\\{0, 0, 3-\\sqrt{3}, 3+\\sqrt{3}, 6\\}$. The multiplicity of the eigenvalue $\\lambda=0$ is $2$.\n\nThe relationship between the multiplicity of the zero eigenvalue and the number of connected components is a fundamental result in spectral graph theory. For a weighted undirected graph with Laplacian $L$, the quadratic form associated with $L$ can be expressed as:\n$$\n\\mathbf{x}^T L \\mathbf{x} = \\sum_{i,j} w_{ij} (x_i - x_j)^2\n$$\nwhere $\\mathbf{x}$ is a vector in $\\mathbb{R}^N$ and $w_{ij}$ is the weight of the edge $(i,j)$ (from matrix $A$). Since all weights $w_{ij}$ are nonnegative, every term in the sum is nonnegative. Thus, $\\mathbf{x}^T L \\mathbf{x} \\ge 0$, which means $L$ is a positive-semidefinite matrix and all its eigenvalues are non-negative, $\\lambda_k \\ge 0$.\n\nAn eigenvector $\\mathbf{v}$ corresponding to the eigenvalue $\\lambda=0$ must satisfy $L\\mathbf{v} = \\mathbf{0}$. This implies $\\mathbf{v}^T L \\mathbf{v} = 0$. From the quadratic form, we have:\n$$\n\\sum_{i,j} w_{ij} (v_i - v_j)^2 = 0\n$$\nSince $w_{ij} > 0$ for all existing edges, this equation holds if and only if $v_i = v_j$ for every pair of nodes $(i, j)$ connected by an edge. This means that for any connected component of the graph, all nodes within that component must have the same value in the eigenvector $\\mathbf{v}$.\n\nIf the graph has $k$ connected components, say $C_1, C_2, \\ldots, C_k$, we can construct $k$ linearly independent eigenvectors for $\\lambda=0$. For each component $C_m$, define a vector $\\mathbf{v}^{(m)}$ such that its $i$-th element is $1$ if node $i$ belongs to $C_m$, and $0$ otherwise. For any such vector, the condition $v_i = v_j$ is satisfied for all connected nodes $(i,j)$, as they must belong to the same component. These $k$ vectors form a basis for the null space of $L$. The dimension of the null space is, by definition, the geometric multiplicity of the eigenvalue $\\lambda=0$. For a symmetric matrix like the Laplacian, the geometric multiplicity is equal to the algebraic multiplicity.\n\nTherefore, the number of connected components in the graph is equal to the multiplicity of the eigenvalue $\\lambda=0$. In this problem, the multiplicity of $\\lambda=0$ is $2$. This implies that the network consists of two disjoint sub-networks.\n\nThe implication for network structure reconstruction in complex adaptive systems is profound. The adjacency matrix $A$ represents inferred interaction strengths. A primary goal of reconstruction is to identify the global topology. The number of connected components is a fundamental topological invariant that reveals whether the system is fully integrated or fragmented into non-interacting sub-systems. A count greater than one, as found here ($2$), indicates that the observed system behaves as two independent modules. This structural information is a critical first step in understanding the system's organization and function, guiding further analysis of these separate sub-networks. The calculation is a direct and robust method to extract this primary structural feature from the interaction data.\n\nThe number of connected components is $2$.",
            "answer": "$$\\boxed{2}$$"
        },
        {
            "introduction": "Moving from analysis to inference, a central task is to reconstruct a network from observational data. In Gaussian Graphical Models, the absence of an edge between two nodes corresponds to their conditional independence given all other nodes. This exercise guides you through the crucial connection between the entries of the precision matrix, $\\Theta$, and partial correlations, showing how to infer the network structure by thresholding these values .",
            "id": "4133196",
            "problem": "Consider a zero-mean multivariate normal random vector $X \\in \\mathbb{R}^{5}$ forming a Gaussian Graphical Model (GGM). Suppose a regularized maximum likelihood estimator yields a symmetric positive definite precision matrix $\\widehat{\\Theta} \\in \\mathbb{R}^{5 \\times 5}$ for the index set $V=\\{1,2,3,4,5\\}$:\n$$\n\\widehat{\\Theta} \\;=\\;\n\\begin{pmatrix}\n1.5 & -0.3 & 0 & 0 & -0.2 \\\\\n-0.3 & 1.2 & -0.25 & 0 & 0 \\\\\n0 & -0.25 & 1.0 & -0.4 & 0 \\\\\n0 & 0 & -0.4 & 1.3 & -0.35 \\\\\n-0.2 & 0 & 0 & -0.35 & 1.1\n\\end{pmatrix}.\n$$\nWork from foundational properties of the multivariate normal distribution and the definition of partial correlation to express the pairwise partial correlation $\\rho_{ij \\cdot V \\setminus \\{i,j\\}}$ in terms of entries of the precision matrix, without assuming any specialized formula. Then, using your derived relation, compute all pairwise partial correlations implied by $\\widehat{\\Theta}$.\n\nAdopt the following thresholding rule to infer an undirected edge set: include an undirected edge between nodes $i$ and $j$ if and only if $\\left|\\rho_{ij \\cdot V \\setminus \\{i,j\\}}\\right| \\ge \\tau$, where $\\tau = 0.2$. Count each undirected edge once (i.e., consider only pairs with $i<j$).\n\nWhat is the total number of inferred undirected edges $m$ under this criterion? Provide $m$ as a single integer. No rounding is necessary and no units are required.",
            "solution": "### Step 1: Derivation of the Partial Correlation Formula\n\nLet $X = (X_1, X_2, \\dots, X_p)^T$ be a $p$-dimensional random vector following a multivariate normal distribution $\\mathcal{N}(\\mu, \\Sigma)$, where $\\Sigma$ is the covariance matrix. The precision matrix is defined as the inverse of the covariance matrix, $\\Theta = \\Sigma^{-1}$. In this problem, $p=5$ and the mean is zero, $\\mu=0$.\n\nThe partial correlation between two variables $X_i$ and $X_j$ given a set of other variables $X_S$ is denoted by $\\rho_{ij \\cdot S}$. By definition, this is the Pearson correlation between $X_i$ and $X_j$ conditional on $X_S$. For a multivariate normal distribution, this is equivalent to the correlation of the residuals obtained after linearly regressing $X_i$ and $X_j$ on $X_S$.\n$$ \\rho_{ij \\cdot S} = \\mathrm{Corr}(X_i, X_j | X_S) = \\frac{\\mathrm{Cov}(X_i, X_j | X_S)}{\\sqrt{\\mathrm{Var}(X_i | X_S) \\mathrm{Var}(X_j | X_S)}} $$\nWe are interested in the case where $S = V \\setminus \\{i,j\\}$, i.e., all other variables in the model.\n\nA key property of the multivariate normal distribution is that the conditional distribution of a subset of variables, given another subset, is also normal. Let us partition the random vector $X$ into two sub-vectors, $X_A = (X_i, X_j)^T$ and $X_B = X_{V \\setminus \\{i,j\\}}$. The covariance matrix $\\Sigma$ and precision matrix $\\Theta$ can be partitioned conformably:\n$$ \\Sigma = \\begin{pmatrix} \\Sigma_{AA} & \\Sigma_{AB} \\\\ \\Sigma_{BA} & \\Sigma_{BB} \\end{pmatrix}, \\quad \\Theta = \\begin{pmatrix} \\Theta_{AA} & \\Theta_{AB} \\\\ \\Theta_{BA} & \\Theta_{BB} \\end{pmatrix} $$\nHere, $\\Sigma_{AA}$ is the $2 \\times 2$ block of $\\Sigma$ corresponding to variables $i$ and $j$, and $\\Theta_{AA}$ is the $2 \\times 2$ block of $\\Theta$ corresponding to the same variables.\n\nThe conditional distribution of $X_A$ given $X_B = x_B$ is a normal distribution with covariance matrix $\\Sigma_{A|B} = \\Sigma_{AA} - \\Sigma_{AB}\\Sigma_{BB}^{-1}\\Sigma_{BA}$. This matrix $\\Sigma_{A|B}$ is the conditional covariance matrix of $(X_i, X_j)^T$ given all other variables, which contains the terms we need for the partial correlation formula:\n$$ \\Sigma_{A|B} = \\mathrm{Cov}(X_A | X_B) = \\begin{pmatrix} \\mathrm{Var}(X_i | X_B) & \\mathrm{Cov}(X_i, X_j | X_B) \\\\ \\mathrm{Cov}(X_j, X_i | X_B) & \\mathrm{Var}(X_j | X_B) \\end{pmatrix} $$\nA fundamental result from the block matrix inversion formula relates the sub-blocks of $\\Theta = \\Sigma^{-1}$ to the sub-blocks of $\\Sigma$. Specifically, the top-left block of the precision matrix, $\\Theta_{AA}$, is the inverse of the Schur complement of $\\Sigma_{BB}$ in $\\Sigma$, which is exactly the conditional covariance matrix $\\Sigma_{A|B}$. That is,\n$$ \\Theta_{AA} = (\\Sigma_{AA} - \\Sigma_{AB}\\Sigma_{BB}^{-1}\\Sigma_{BA})^{-1} = (\\Sigma_{A|B})^{-1} $$\nTherefore, the conditional covariance matrix we seek is the inverse of the corresponding submatrix of the precision matrix: $\\Sigma_{A|B} = (\\Theta_{AA})^{-1}$.\n\nThe submatrix $\\Theta_{AA}$ is given by:\n$$ \\Theta_{AA} = \\begin{pmatrix} \\Theta_{ii} & \\Theta_{ij} \\\\ \\Theta_{ji} & \\Theta_{jj} \\end{pmatrix} $$\nSince $\\Theta$ is symmetric, $\\Theta_{ij} = \\Theta_{ji}$. We can now compute the inverse of this $2 \\times 2$ matrix:\n$$ \\Sigma_{A|B} = (\\Theta_{AA})^{-1} = \\frac{1}{\\Theta_{ii}\\Theta_{jj} - \\Theta_{ij}\\Theta_{ji}} \\begin{pmatrix} \\Theta_{jj} & -\\Theta_{ij} \\\\ -\\Theta_{ji} & \\Theta_{ii} \\end{pmatrix} = \\frac{1}{\\Theta_{ii}\\Theta_{jj} - \\Theta_{ij}^2} \\begin{pmatrix} \\Theta_{jj} & -\\Theta_{ij} \\\\ -\\Theta_{ij} & \\Theta_{ii} \\end{pmatrix} $$\nFrom this matrix, we can identify the conditional variances and covariance:\n$$ \\mathrm{Var}(X_i | X_B) = \\frac{\\Theta_{jj}}{\\Theta_{ii}\\Theta_{jj} - \\Theta_{ij}^2} $$\n$$ \\mathrm{Var}(X_j | X_B) = \\frac{\\Theta_{ii}}{\\Theta_{ii}\\Theta_{jj} - \\Theta_{ij}^2} $$\n$$ \\mathrm{Cov}(X_i, X_j | X_B) = \\frac{-\\Theta_{ij}}{\\Theta_{ii}\\Theta_{jj} - \\Theta_{ij}^2} $$\nSubstituting these expressions into the formula for partial correlation:\n$$ \\rho_{ij \\cdot B} = \\frac{\\frac{-\\Theta_{ij}}{\\Theta_{ii}\\Theta_{jj} - \\Theta_{ij}^2}}{\\sqrt{\\left(\\frac{\\Theta_{jj}}{\\Theta_{ii}\\Theta_{jj} - \\Theta_{ij}^2}\\right) \\left(\\frac{\\Theta_{ii}}{\\Theta_{ii}\\Theta_{jj} - \\Theta_{ij}^2}\\right)}} = \\frac{\\frac{-\\Theta_{ij}}{\\Theta_{ii}\\Theta_{jj} - \\Theta_{ij}^2}}{\\frac{\\sqrt{\\Theta_{ii}\\Theta_{jj}}}{|\\Theta_{ii}\\Theta_{jj} - \\Theta_{ij}^2|}} $$\nSince $\\Theta$ is positive definite, all its principal submatrices have positive determinants. Thus, $\\det(\\Theta_{AA}) = \\Theta_{ii}\\Theta_{jj} - \\Theta_{ij}^2 > 0$. Also, the diagonal entries $\\Theta_{ii}$ and $\\Theta_{jj}$ must be positive. This allows us to simplify the expression:\n$$ \\rho_{ij \\cdot V \\setminus \\{i,j\\}} = \\frac{-\\Theta_{ij}}{\\sqrt{\\Theta_{ii}\\Theta_{jj}}} $$\nThis is the desired relationship, derived from foundational principles.\n\n### Step 2: Computation of Partial Correlations and Edge Inference\n\nWe now apply this formula to the given estimated precision matrix $\\widehat{\\Theta}$. We need to compute $\\rho_{ij \\cdot V \\setminus \\{i,j\\}}$ for all pairs $(i, j)$ with $1 \\le i < j \\le 5$. There are $\\binom{5}{2} = 10$ such pairs.\n\nThe diagonal elements of $\\widehat{\\Theta}$ are:\n$\\widehat{\\Theta}_{11} = 1.5$, $\\widehat{\\Theta}_{22} = 1.2$, $\\widehat{\\Theta}_{33} = 1.0$, $\\widehat{\\Theta}_{44} = 1.3$, $\\widehat{\\Theta}_{55} = 1.1$.\n\nThe relevant off-diagonal elements $\\widehat{\\Theta}_{ij}$ with $i < j$ are:\n$\\widehat{\\Theta}_{12} = -0.3$, $\\widehat{\\Theta}_{13} = 0$, $\\widehat{\\Theta}_{14} = 0$, $\\widehat{\\Theta}_{15} = -0.2$, $\\widehat{\\Theta}_{23} = -0.25$, $\\widehat{\\Theta}_{24} = 0$, $\\widehat{\\Theta}_{25} = 0$, $\\widehat{\\Theta}_{34} = -0.4$, $\\widehat{\\Theta}_{35} = 0$, $\\widehat{\\Theta}_{45} = -0.35$.\n\nIf $\\widehat{\\Theta}_{ij} = 0$, then $\\rho_{ij \\cdot V \\setminus \\{i,j\\}} = 0$. This applies to the pairs $(1,3)$, $(1,4)$, $(2,4)$, $(2,5)$, and $(3,5)$. For these pairs, the absolute partial correlation is $0$, which is less than the threshold $\\tau = 0.2$, so no edges are inferred.\n\nNow we compute the partial correlations for the pairs with non-zero $\\widehat{\\Theta}_{ij}$:\n\n1.  Pair $(1,2)$:\n    $\\rho_{12 \\cdot \\{3,4,5\\}} = \\frac{-(-0.3)}{\\sqrt{\\widehat{\\Theta}_{11}\\widehat{\\Theta}_{22}}} = \\frac{0.3}{\\sqrt{1.5 \\times 1.2}} = \\frac{0.3}{\\sqrt{1.8}}$.\n    To check against the threshold $\\tau=0.2$: $|\\rho_{12 \\cdot \\{3,4,5\\}}| \\ge 0.2 \\iff \\frac{0.3}{\\sqrt{1.8}} \\ge 0.2 \\iff 1.5 \\ge \\sqrt{1.8} \\iff 1.5^2 \\ge 1.8 \\iff 2.25 \\ge 1.8$. This is true. An edge $(1,2)$ is inferred.\n\n2.  Pair $(1,5)$:\n    $\\rho_{15 \\cdot \\{2,3,4\\}} = \\frac{-(-0.2)}{\\sqrt{\\widehat{\\Theta}_{11}\\widehat{\\Theta}_{55}}} = \\frac{0.2}{\\sqrt{1.5 \\times 1.1}} = \\frac{0.2}{\\sqrt{1.65}}$.\n    To check: $|\\rho_{15 \\cdot \\{2,3,4\\}}| \\ge 0.2 \\iff \\frac{0.2}{\\sqrt{1.65}} \\ge 0.2 \\iff 1 \\ge \\sqrt{1.65} \\iff 1 \\ge 1.65$. This is false. No edge $(1,5)$ is inferred.\n\n3.  Pair $(2,3)$:\n    $\\rho_{23 \\cdot \\{1,4,5\\}} = \\frac{-(-0.25)}{\\sqrt{\\widehat{\\Theta}_{22}\\widehat{\\Theta}_{33}}} = \\frac{0.25}{\\sqrt{1.2 \\times 1.0}} = \\frac{0.25}{\\sqrt{1.2}}$.\n    To check: $|\\rho_{23 \\cdot \\{1,4,5\\}}| \\ge 0.2 \\iff \\frac{0.25}{\\sqrt{1.2}} \\ge 0.2 \\iff 1.25 \\ge \\sqrt{1.2} \\iff 1.25^2 \\ge 1.2 \\iff 1.5625 \\ge 1.2$. This is true. An edge $(2,3)$ is inferred.\n\n4.  Pair $(3,4)$:\n    $\\rho_{34 \\cdot \\{1,2,5\\}} = \\frac{-(-0.4)}{\\sqrt{\\widehat{\\Theta}_{33}\\widehat{\\Theta}_{44}}} = \\frac{0.4}{\\sqrt{1.0 \\times 1.3}} = \\frac{0.4}{\\sqrt{1.3}}$.\n    To check: $|\\rho_{34 \\cdot \\{1,2,5\\}}| \\ge 0.2 \\iff \\frac{0.4}{\\sqrt{1.3}} \\ge 0.2 \\iff 2 \\ge \\sqrt{1.3} \\iff 4 \\ge 1.3$. This is true. An edge $(3,4)$ is inferred.\n\n5.  Pair $(4,5)$:\n    $\\rho_{45 \\cdot \\{1,2,3\\}} = \\frac{-(-0.35)}{\\sqrt{\\widehat{\\Theta}_{44}\\widehat{\\Theta}_{55}}} = \\frac{0.35}{\\sqrt{1.3 \\times 1.1}} = \\frac{0.35}{\\sqrt{1.43}}$.\n    To check: $|\\rho_{45 \\cdot \\{1,2,3\\}}| \\ge 0.2 \\iff \\frac{0.35}{\\sqrt{1.43}} \\ge 0.2 \\iff 1.75 \\ge \\sqrt{1.43} \\iff 1.75^2 \\ge 1.43 \\iff 3.0625 \\ge 1.43$. This is true. An edge $(4,5)$ is inferred.\n\n### Step 3: Count the Total Number of Edges\n\nThe inferred undirected edges are:\n- $(1,2)$\n- $(2,3)$\n- $(3,4)$\n- $(4,5)$\n\nThe total number of inferred edges, $m$, is the count of these edges. Counting them, we find there are $4$ edges.\nThus, $m = 4$.",
            "answer": "$$\n\\boxed{4}\n$$"
        },
        {
            "introduction": "Often, the process of network inference leaves us with several competing candidate models, each representing a different hypothesis about the underlying structure. This practice introduces two powerful tools for model selection, the Akaike Information Criterion ($AIC$) and the Bayesian Information Criterion ($BIC$). You will learn how these criteria penalize model complexity to prevent overfitting, allowing for a principled selection of the most plausible network structure from a set of candidates .",
            "id": "4133224",
            "problem": "Consider an undirected binary network model (the Ising model) for a complex adaptive system with $p=10$ nodes. The generative mechanism assigns one local field parameter $h_i$ to each node $i \\in \\{1,\\dots,10\\}$ and one symmetric pairwise interaction parameter $J_{ij}$ to each undirected edge $\\{i,j\\}$, and produces $n=500$ independent and identically distributed node-state observations. Three candidate network structures $\\mathcal{E}_1$, $\\mathcal{E}_2$, and $\\mathcal{E}_3$ are fitted by maximum likelihood under the same data set, yielding maximized log-likelihood values $\\ell_1=-134.2$, $\\ell_2=-128.9$, and $\\ell_3=-120.0$, respectively. The corresponding edge counts are $|\\mathcal{E}_1|=12$, $|\\mathcal{E}_2|=16$, and $|\\mathcal{E}_3|=25$. Assume that the total number of free parameters in a candidate is the sum of node-local fields and edge couplings, that is $k_m=p+|\\mathcal{E}_m|$ for model $m\\in\\{1,2,3\\}$.\n\nStarting from the foundational principles that the Akaike Information Criterion arises as an asymptotically unbiased estimator of the expected Kullbackâ€“Leibler divergence risk between the candidate model and the data-generating process, and that the Bayesian Information Criterion emerges from a Laplace approximation to the model evidence under a regular prior with fixed dimension, compute both criteria for each candidate structure and identify the preferred structure according to the Bayesian Information Criterion. Round all information criterion values to four significant figures. Express your final answer as the index $m \\in \\{1,2,3\\}$ of the selected candidate structure.",
            "solution": "The problem requires a comparison of three candidate network structures using two standard information criteria: the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC). The goal is to identify the model preferred by the BIC.\n\nThe general definition of the Akaike Information Criterion is given by:\n$$\n\\text{AIC} = -2\\ell_{\\text{max}} + 2k\n$$\nwhere $\\ell_{\\text{max}}$ is the maximized value of the log-likelihood function for the model, and $k$ is the number of free parameters in the model. AIC estimates the prediction error and thereby the relative quality of statistical models for a given set of data. A lower AIC value indicates a better fit, penalizing models with more parameters to avoid overfitting.\n\nThe general definition of the Bayesian Information Criterion is given by:\n$$\n\\text{BIC} = -2\\ell_{\\text{max}} + k \\ln(n)\n$$\nwhere $n$ is the number of data points or observations. The BIC is derived from a Bayesian framework and penalizes model complexity more heavily than AIC for $n > \\exp(2) \\approx 7.4$. A lower BIC value is preferred, indicating a model that is more likely to be the true model.\n\nThe problem provides the following data:\n- Number of nodes: $p=10$.\n- Number of observations: $n=500$.\n- For candidate structure $m=1$: maximized log-likelihood $\\ell_1 = -134.2$ and edge count $|\\mathcal{E}_1|=12$.\n- For candidate structure $m=2$: maximized log-likelihood $\\ell_2 = -128.9$ and edge count $|\\mathcal{E}_2|=16$.\n- For candidate structure $m=3$: maximized log-likelihood $\\ell_3 = -120.0$ and edge count $|\\mathcal{E}_3|=25$.\n\nThe number of free parameters, $k_m$, for each model $m$ is defined as the sum of node-local fields (one per node) and edge couplings (one per edge):\n$$\nk_m = p + |\\mathcal{E}_m|\n$$\nWe first compute the number of parameters for each of the three candidate models.\nFor model $m=1$:\n$$\nk_1 = p + |\\mathcal{E}_1| = 10 + 12 = 22\n$$\nFor model $m=2$:\n$$\nk_2 = p + |\\mathcal{E}_2| = 10 + 16 = 26\n$$\nFor model $m=3$:\n$$\nk_3 = p + |\\mathcal{E}_3| = 10 + 25 = 35\n$$\nNext, we compute the AIC for each model.\nFor model $m=1$:\n$$\n\\text{AIC}_1 = -2\\ell_1 + 2k_1 = -2(-134.2) + 2(22) = 268.4 + 44 = 312.4\n$$\nFor model $m=2$:\n$$\n\\text{AIC}_2 = -2\\ell_2 + 2k_2 = -2(-128.9) + 2(26) = 257.8 + 52 = 309.8\n$$\nFor model $m=3$:\n$$\n\\text{AIC}_3 = -2\\ell_3 + 2k_3 = -2(-120.0) + 2(35) = 240.0 + 70 = 310.0\n$$\nThe values rounded to four significant figures are $\\text{AIC}_1 = 312.4$, $\\text{AIC}_2 = 309.8$, and $\\text{AIC}_3 = 310.0$. According to AIC, model $2$ would be the preferred model as it has the lowest score.\n\nNow, we compute the BIC for each model. The number of observations is $n=500$. The natural logarithm term is $\\ln(n) = \\ln(500)$.\nFor model $m=1$:\n$$\n\\text{BIC}_1 = -2\\ell_1 + k_1 \\ln(n) = -2(-134.2) + 22 \\ln(500) = 268.4 + 22 \\times 6.214608... \\approx 268.4 + 136.721378 = 405.121378...\n$$\nFor model $m=2$:\n$$\n\\text{BIC}_2 = -2\\ell_2 + k_2 \\ln(n) = -2(-128.9) + 26 \\ln(500) = 257.8 + 26 \\times 6.214608... \\approx 257.8 + 161.579810 = 419.379810...\n$$\nFor model $m=3$:\n$$\n\\text{BIC}_3 = -2\\ell_3 + k_3 \\ln(n) = -2(-120.0) + 35 \\ln(500) = 240.0 + 35 \\times 6.214608... \\approx 240.0 + 217.511283 = 457.511283...\n$$\nRounding these values to four significant figures as requested:\n$$\n\\text{BIC}_1 \\approx 405.1\n$$\n$$\n\\text{BIC}_2 \\approx 419.4\n$$\n$$\n\\text{BIC}_3 \\approx 457.5\n$$\nThe selection criterion is to choose the model with the minimum BIC value. Comparing the computed values:\n$$\n405.1 < 419.4 < 457.5\n$$\nThe minimum value is $\\text{BIC}_1$. Therefore, the Bayesian Information Criterion selects the first candidate structure, $\\mathcal{E}_1$. The index of this preferred structure is $m=1$. Note that the BIC, with its stronger penalty for complexity ($k \\ln(500)$ vs. $2k$), has disfavored the more complex models that AIC ranked higher.",
            "answer": "$$\\boxed{1}$$"
        }
    ]
}