{
    "hands_on_practices": [
        {
            "introduction": "在从连续变量数据推断网络结构时，高斯图形模型（GGM）提供了一个强有力的框架。这个练习的核心在于一个关键原理：在GGM中，变量对之间的条件独立性直接反映在精度矩阵（协方差矩阵的逆）的结构中。通过计算部分相关性——这可以从精度矩阵的元素中直接得出——我们可以量化在控制了所有其他变量后，两个节点之间线性关系的强度，并由此重建网络的条件独立性结构。",
            "id": "4133196",
            "problem": "考虑一个零均值多元正态随机向量 $X \\in \\mathbb{R}^{5}$，它构成一个高斯图模型（GGM）。假设一个正则化最大似然估计器为索引集 $V=\\{1,2,3,4,5\\}$ 得到了一个对称正定精度矩阵 $\\widehat{\\Theta} \\in \\mathbb{R}^{5 \\times 5}$：\n$$\n\\widehat{\\Theta} \\;=\\;\n\\begin{pmatrix}\n1.5  -0.3  0  0  -0.2 \\\\\n-0.3  1.2  -0.25  0  0 \\\\\n0  -0.25  1.0  -0.4  0 \\\\\n0  0  -0.4  1.3  -0.35 \\\\\n-0.2  0  0  -0.35  1.1\n\\end{pmatrix}.\n$$\n从多元正态分布的基本性质和偏相关的定义出发，将逐对偏相关 $\\rho_{ij \\cdot V \\setminus \\{i,j\\}}$ 用精度矩阵的元素表示出来，过程中不假设任何专用公式。然后，使用你推导出的关系，计算由 $\\widehat{\\Theta}$ 蕴含的所有逐对偏相关。\n\n采用以下阈值规则来推断一个无向边集：当且仅当 $\\left|\\rho_{ij \\cdot V \\setminus \\{i,j\\}}\\right| \\ge \\tau$（其中 $\\tau = 0.2$）时，在节点 $i$ 和 $j$ 之间包含一条无向边。每条无向边只计数一次（即，只考虑 $i  j$）。",
            "solution": "用户希望我解决以下问题：\n给定一个零均值多元正态随机向量 $X \\in \\mathbb{R}^{5}$，它构成一个高斯图模型（GGM）。一个正则化最大似然估计器为索引集 $V=\\{1,2,3,4,5\\}$ 得到了一个对称正定精度矩阵 $\\widehat{\\Theta} \\in \\mathbb{R}^{5 \\times 5}$：\n$$\n\\widehat{\\Theta} \\;=\\;\n\\begin{pmatrix}\n1.5  -0.3  0  0  -0.2 \\\\\n-0.3  1.2  -0.25  0  0 \\\\\n0  -0.25  1.0  -0.4  0 \\\\\n0  0  -0.4  1.3  -0.35 \\\\\n-0.2  0  0  -0.35  1.1\n\\end{pmatrix}.\n$$\n该任务包含两部分。第一，从基本原理出发，推导逐对偏相关 $\\rho_{ij \\cdot V \\setminus \\{i,j\\}}$ 关于精度矩阵元素的表达式。第二，使用这个推导出的关系，根据 $\\widehat{\\Theta}$ 计算所有逐对偏相关，应用阈值 $\\tau = 0.2$ 的阈值规则来推断一个无向边集，并计算推断出的边的总数 $m$。\n\n### 第 1 步：偏相关公式的推导\n\n设 $X = (X_1, X_2, \\dots, X_p)^T$ 是一个服从多元正态分布 $\\mathcal{N}(\\mu, \\Sigma)$ 的 $p$ 维随机向量，其中 $\\Sigma$ 是协方差矩阵。精度矩阵定义为协方差矩阵的逆，即 $\\Theta = \\Sigma^{-1}$。在本问题中，$p=5$ 且均值为零，$\\mu=0$。\n\n两个变量 $X_i$ 和 $X_j$ 在给定其他变量集 $X_S$ 条件下的偏相关记为 $\\rho_{ij \\cdot S}$。根据定义，这是在给定 $X_S$ 条件下 $X_i$ 和 $X_j$ 之间的 Pearson 相关。对于多元正态分布，这等价于将 $X_i$ 和 $X_j$ 分别对 $X_S$ 进行线性回归后得到的残差之间的相关性。\n$$ \\rho_{ij \\cdot S} = \\mathrm{Corr}(X_i, X_j | X_S) = \\frac{\\mathrm{Cov}(X_i, X_j | X_S)}{\\sqrt{\\mathrm{Var}(X_i | X_S) \\mathrm{Var}(X_j | X_S)}} $$\n我们感兴趣的情况是 $S = V \\setminus \\{i,j\\}$，即模型中所有其他变量。\n\n多元正态分布的一个关键性质是，一个变量子集在给定另一个子集条件下的条件分布也是正态分布。我们将随机向量 $X$ 划分为两个子向量，$X_A = (X_i, X_j)^T$ 和 $X_B = X_{V \\setminus \\{i,j\\}}$。协方差矩阵 $\\Sigma$ 和精度矩阵 $\\Theta$ 可以相应地进行分块：\n$$ \\Sigma = \\begin{pmatrix} \\Sigma_{AA}  \\Sigma_{AB} \\\\ \\Sigma_{BA}  \\Sigma_{BB} \\end{pmatrix}, \\quad \\Theta = \\begin{pmatrix} \\Theta_{AA}  \\Theta_{AB} \\\\ \\Theta_{BA}  \\Theta_{BB} \\end{pmatrix} $$\n此处，$\\Sigma_{AA}$ 是 $\\Sigma$ 中对应于变量 $i$ 和 $j$ 的 $2 \\times 2$ 分块，而 $\\Theta_{AA}$ 是 $\\Theta$ 中对应于相同变量的 $2 \\times 2$ 分块。\n\n在给定 $X_B = x_B$ 的条件下，$X_A$ 的条件分布是一个正态分布，其协方差矩阵为 $\\Sigma_{A|B} = \\Sigma_{AA} - \\Sigma_{AB}\\Sigma_{BB}^{-1}\\Sigma_{BA}$。这个矩阵 $\\Sigma_{A|B}$ 是 $(X_i, X_j)^T$ 在给定所有其他变量条件下的条件协方差矩阵，它包含了我们计算偏相关公式所需的项：\n$$ \\Sigma_{A|B} = \\mathrm{Cov}(X_A | X_B) = \\begin{pmatrix} \\mathrm{Var}(X_i | X_B)  \\mathrm{Cov}(X_i, X_j | X_B) \\\\ \\mathrm{Cov}(X_j, X_i | X_B)  \\mathrm{Var}(X_j | X_B) \\end{pmatrix} $$\n分块矩阵求逆公式的一个基本结果将 $\\Theta = \\Sigma^{-1}$ 的子块与 $\\Sigma$ 的子块联系起来。具体来说，精度矩阵的左上角分块 $\\Theta_{AA}$ 是 $\\Sigma_{BB}$ 在 $\\Sigma$ 中的舒尔补的逆，而这个舒尔补正是条件协方差矩阵 $\\Sigma_{A|B}$。也就是说，\n$$ \\Theta_{AA} = (\\Sigma_{AA} - \\Sigma_{AB}\\Sigma_{BB}^{-1}\\Sigma_{BA})^{-1} = (\\Sigma_{A|B})^{-1} $$\n因此，我们寻求的条件协方差矩阵是精度矩阵相应子矩阵的逆：$\\Sigma_{A|B} = (\\Theta_{AA})^{-1}$。\n\n子矩阵 $\\Theta_{AA}$ 由下式给出：\n$$ \\Theta_{AA} = \\begin{pmatrix} \\Theta_{ii}  \\Theta_{ij} \\\\ \\Theta_{ji}  \\Theta_{jj} \\end{pmatrix} $$\n由于 $\\Theta$ 是对称的，$\\Theta_{ij} = \\Theta_{ji}$。我们现在可以计算这个 $2 \\times 2$ 矩阵的逆：\n$$ \\Sigma_{A|B} = (\\Theta_{AA})^{-1} = \\frac{1}{\\Theta_{ii}\\Theta_{jj} - \\Theta_{ij}\\Theta_{ji}} \\begin{pmatrix} \\Theta_{jj}  -\\Theta_{ij} \\\\ -\\Theta_{ji}  \\Theta_{ii} \\end{pmatrix} = \\frac{1}{\\Theta_{ii}\\Theta_{jj} - \\Theta_{ij}^2} \\begin{pmatrix} \\Theta_{jj}  -\\Theta_{ij} \\\\ -\\Theta_{ij}  \\Theta_{ii} \\end{pmatrix} $$\n从这个矩阵中，我们可以确定条件方差和协方差：\n$$ \\mathrm{Var}(X_i | X_B) = \\frac{\\Theta_{jj}}{\\Theta_{ii}\\Theta_{jj} - \\Theta_{ij}^2} $$\n$$ \\mathrm{Var}(X_j | X_B) = \\frac{\\Theta_{ii}}{\\Theta_{ii}\\Theta_{jj} - \\Theta_{ij}^2} $$\n$$ \\mathrm{Cov}(X_i, X_j | X_B) = \\frac{-\\Theta_{ij}}{\\Theta_{ii}\\Theta_{jj} - \\Theta_{ij}^2} $$\n将这些表达式代入偏相关公式：\n$$ \\rho_{ij \\cdot B} = \\frac{\\frac{-\\Theta_{ij}}{\\Theta_{ii}\\Theta_{jj} - \\Theta_{ij}^2}}{\\sqrt{\\left(\\frac{\\Theta_{jj}}{\\Theta_{ii}\\Theta_{jj} - \\Theta_{ij}^2}\\right) \\left(\\frac{\\Theta_{ii}}{\\Theta_{ii}\\Theta_{jj} - \\Theta_{ij}^2}\\right)}} = \\frac{\\frac{-\\Theta_{ij}}{\\Theta_{ii}\\Theta_{jj} - \\Theta_{ij}^2}}{\\frac{\\sqrt{\\Theta_{ii}\\Theta_{jj}}}{|\\Theta_{ii}\\Theta_{jj} - \\Theta_{ij}^2|}} $$\n由于 $\\Theta$ 是正定的，其所有主子矩阵的行列式都为正。因此，$\\det(\\Theta_{AA}) = \\Theta_{ii}\\Theta_{jj} - \\Theta_{ij}^2 > 0$。此外，对角线元素 $\\Theta_{ii}$ 和 $\\Theta_{jj}$ 也必须为正。这使我们能够简化表达式：\n$$ \\rho_{ij \\cdot V \\setminus \\{i,j\\}} = \\frac{-\\Theta_{ij}}{\\sqrt{\\Theta_{ii}\\Theta_{jj}}} $$\n这就是从基本原理推导出的所求关系。\n\n### 第 2 步：偏相关的计算与边的推断\n\n我们现在将此公式应用于给定的估计精度矩阵 $\\widehat{\\Theta}$。我们需要为所有满足 $1 \\le i  j \\le 5$ 的配对 $(i, j)$ 计算 $\\rho_{ij \\cdot V \\setminus \\{i,j\\}}$。总共有 $\\binom{5}{2} = 10$ 个这样的配对。\n\n$\\widehat{\\Theta}$ 的对角线元素是：\n$\\widehat{\\Theta}_{11} = 1.5$, $\\widehat{\\Theta}_{22} = 1.2$, $\\widehat{\\Theta}_{33} = 1.0$, $\\widehat{\\Theta}_{44} = 1.3$, $\\widehat{\\Theta}_{55} = 1.1$。\n\n满足 $i  j$ 的相关非对角线元素 $\\widehat{\\Theta}_{ij}$ 是：\n$\\widehat{\\Theta}_{12} = -0.3$, $\\widehat{\\Theta}_{13} = 0$, $\\widehat{\\Theta}_{14} = 0$, $\\widehat{\\Theta}_{15} = -0.2$, $\\widehat{\\Theta}_{23} = -0.25$, $\\widehat{\\Theta}_{24} = 0$, $\\widehat{\\Theta}_{25} = 0$, $\\widehat{\\Theta}_{34} = -0.4$, $\\widehat{\\Theta}_{35} = 0$, $\\widehat{\\Theta}_{45} = -0.35$。\n\n如果 $\\widehat{\\Theta}_{ij} = 0$，那么 $\\rho_{ij \\cdot V \\setminus \\{i,j\\}} = 0$。这适用于配对 $(1,3)$、$(1,4)$、$(2,4)$、$(2,5)$ 和 $(3,5)$。对于这些配对，偏相关的绝对值为 $0$，小于阈值 $\\tau = 0.2$，因此不会推断出边。\n\n现在我们为 $\\widehat{\\Theta}_{ij}$ 不为零的配对计算偏相关：\n\n1.  配对 $(1,2)$:\n    $\\rho_{12 \\cdot \\{3,4,5\\}} = \\frac{-(-0.3)}{\\sqrt{\\widehat{\\Theta}_{11}\\widehat{\\Theta}_{22}}} = \\frac{0.3}{\\sqrt{1.5 \\times 1.2}} = \\frac{0.3}{\\sqrt{1.8}}$。\n    为了与阈值 $\\tau=0.2$ 比较：$|\\rho_{12 \\cdot \\{3,4,5\\}}| \\ge 0.2 \\iff \\frac{0.3}{\\sqrt{1.8}} \\ge 0.2 \\iff 1.5 \\ge \\sqrt{1.8} \\iff 1.5^2 \\ge 1.8 \\iff 2.25 \\ge 1.8$。这是成立的。推断出一条边 $(1,2)$。\n\n2.  配对 $(1,5)$:\n    $\\rho_{15 \\cdot \\{2,3,4\\}} = \\frac{-(-0.2)}{\\sqrt{\\widehat{\\Theta}_{11}\\widehat{\\Theta}_{55}}} = \\frac{0.2}{\\sqrt{1.5 \\times 1.1}} = \\frac{0.2}{\\sqrt{1.65}}$。\n    检查：$|\\rho_{15 \\cdot \\{2,3,4\\}}| \\ge 0.2 \\iff \\frac{0.2}{\\sqrt{1.65}} \\ge 0.2 \\iff 1 \\ge \\sqrt{1.65} \\iff 1 \\ge 1.65$。这是不成立的。没有推断出边 $(1,5)$。\n\n3.  配对 $(2,3)$:\n    $\\rho_{23 \\cdot \\{1,4,5\\}} = \\frac{-(-0.25)}{\\sqrt{\\widehat{\\Theta}_{22}\\widehat{\\Theta}_{33}}} = \\frac{0.25}{\\sqrt{1.2 \\times 1.0}} = \\frac{0.25}{\\sqrt{1.2}}$。\n    检查：$|\\rho_{23 \\cdot \\{1,4,5\\}}| \\ge 0.2 \\iff \\frac{0.25}{\\sqrt{1.2}} \\ge 0.2 \\iff 1.25 \\ge \\sqrt{1.2} \\iff 1.25^2 \\ge 1.2 \\iff 1.5625 \\ge 1.2$。这是成立的。推断出一条边 $(2,3)$。\n\n4.  配对 $(3,4)$:\n    $\\rho_{34 \\cdot \\{1,2,5\\}} = \\frac{-(-0.4)}{\\sqrt{\\widehat{\\Theta}_{33}\\widehat{\\Theta}_{44}}} = \\frac{0.4}{\\sqrt{1.0 \\times 1.3}} = \\frac{0.4}{\\sqrt{1.3}}$。\n    检查：$|\\rho_{34 \\cdot \\{1,2,5\\}}| \\ge 0.2 \\iff \\frac{0.4}{\\sqrt{1.3}} \\ge 0.2 \\iff 2 \\ge \\sqrt{1.3} \\iff 4 \\ge 1.3$。这是成立的。推断出一条边 $(3,4)$。\n\n5.  配对 $(4,5)$:\n    $\\rho_{45 \\cdot \\{1,2,3\\}} = \\frac{-(-0.35)}{\\sqrt{\\widehat{\\Theta}_{44}\\widehat{\\Theta}_{55}}} = \\frac{0.35}{\\sqrt{1.3 \\times 1.1}} = \\frac{0.35}{\\sqrt{1.43}}$。\n    检查：$|\\rho_{45 \\cdot \\{1,2,3\\}}| \\ge 0.2 \\iff \\frac{0.35}{\\sqrt{1.43}} \\ge 0.2 \\iff 1.75 \\ge \\sqrt{1.43} \\iff 1.75^2 \\ge 1.43 \\iff 3.0625 \\ge 1.43$。这是成立的。推断出一条边 $(4,5)$。\n\n### 第 3 步：计算边的总数\n\n推断出的无向边是：\n- $(1,2)$\n- $(2,3)$\n- $(3,4)$\n- $(4,5)$\n\n推断出的无向边总数 $m$ 是这些边的数量。计算后，我们发现有 4 条边。\n因此，$m = 4$。",
            "answer": "$$\n\\boxed{4}\n$$"
        },
        {
            "introduction": "一旦网络结构被推断出来，分析其拓扑特性就变得至关重要，例如确定系统是否是完全连接的，还是由多个独立的子系统组成。这个练习介绍了谱图论中的一个基本工具——图拉普拉斯算子，它的谱（即特征值）包含了关于图连通性的丰富信息。通过计算拉普拉斯矩阵的零特征值的数量，我们可以直接确定网络中连通分量的个数，这是理解复杂系统组织和功能的关键一步。",
            "id": "4133191",
            "problem": "考虑一个包含 $5$ 个节点的无向加权网络，其中的权重代表从一个复杂自适应系统的观测数据中估计出的非负交互强度。设其加权邻接矩阵 $A$ 为\n$$\nA \\;=\\; \\begin{pmatrix}\n0  2  0  0  0 \\\\\n2  0  1  0  0 \\\\\n0  1  0  0  0 \\\\\n0  0  0  0  3 \\\\\n0  0  0  3  0\n\\end{pmatrix}.\n$$\n使用标准的组合图拉普拉斯算子 $L$（定义为 $L \\;=\\; D \\;-\\; A$，其中 $D$ 是对角度矩阵，其对角元素 $D_{ii}$ 等于节点 $i$ 的加权度），完成以下任务：\n- 显式计算 $L$。\n- 计算 $L$ 的特征值 $\\lambda_{i}$。\n- 基于具有非负权重的加权无向图的图拉普拉斯算子的基本性质，解释特征值 $\\lambda \\;=\\; 0$ 的重数如何反映连通分量的数量，并阐述该数量对于复杂自适应系统中网络结构重建的意义。\n\n将推断出的连通分量数量作为最终答案。无需四舍五入，也无需单位。你的推理必须从与网络推断和结构重建相关的核心定义和经过验证的事实出发，避免使用问题陈述中给出的任何快捷公式。",
            "solution": "该问题被评估为有效，因为它在科学上基于谱图论，问题明确且客观。它提出了网络分析中的一个标准任务，该任务是可形式化的，并且与复杂自适应系统的研究相关。所提供的数据是完整且一致的。\n\n问题要求计算图拉普拉斯算子、其特征值，并解释零特征值的重数在网络结构背景下的意义。最终答案是从该分析中推断出的连通分量数量。\n\n我们从第一性原理开始定义必要的组成部分。一个无向加权网络可以用一个对称的加权邻接矩阵 $A$ 来表示，其中 $A_{ij} = A_{ji}$ 是节点 $i$ 和 $j$ 之间边的权重。对于本问题，该网络有 $5$ 个节点，其邻接矩阵给出如下：\n$$\nA \\;=\\; \\begin{pmatrix}\n0  2  0  0  0 \\\\\n2  0  1  0  0 \\\\\n0  1  0  0  0 \\\\\n0  0  0  0  3 \\\\\n0  0  0  3  0\n\\end{pmatrix}\n$$\n节点 $i$ 的加权度，记为 $d_i$，是所有连接到该节点的边的权重之和。对于一个有 $N$ 个节点的网络，它可以从邻接矩阵计算得出：$d_i = \\sum_{j=1}^{N} A_{ij}$。对于给定的 $N=5$ 的网络：\n-   $d_1 = A_{11} + A_{12} + A_{13} + A_{14} + A_{15} = 0 + 2 + 0 + 0 + 0 = 2$\n-   $d_2 = A_{21} + A_{22} + A_{23} + A_{24} + A_{25} = 2 + 0 + 1 + 0 + 0 = 3$\n-   $d_3 = A_{31} + A_{32} + A_{33} + A_{34} + A_{35} = 0 + 1 + 0 + 0 + 0 = 1$\n-   $d_4 = A_{41} + A_{42} + A_{43} + A_{44} + A_{45} = 0 + 0 + 0 + 0 + 3 = 3$\n-   $d_5 = A_{51} + A_{52} + A_{53} + A_{54} + A_{55} = 0 + 0 + 0 + 3 + 0 = 3$\n\n度矩阵 $D$ 是一个对角矩阵，其对角元素 $D_{ii}$ 是加权度 $d_i$。\n$$\nD \\;=\\; \\begin{pmatrix}\n2  0  0  0  0 \\\\\n0  3  0  0  0 \\\\\n0  0  1  0  0 \\\\\n0  0  0  3  0 \\\\\n0  0  0  0  3\n\\end{pmatrix}\n$$\n组合图拉普拉斯算子 $L$ 定义为 $L = D - A$。代入矩阵 $D$ 和 $A$：\n$$\nL \\;=\\; \\begin{pmatrix}\n2  0  0  0  0 \\\\\n0  3  0  0  0 \\\\\n0  0  1  0  0 \\\\\n0  0  0  3  0 \\\\\n0  0  0  0  3\n\\end{pmatrix} - \\begin{pmatrix}\n0  2  0  0  0 \\\\\n2  0  1  0  0 \\\\\n0  1  0  0  0 \\\\\n0  0  0  0  3 \\\\\n0  0  0  3  0\n\\end{pmatrix} \\;=\\; \\begin{pmatrix}\n2  -2  0  0  0 \\\\\n-2  3  -1  0  0 \\\\\n0  -1  1  0  0 \\\\\n0  0  0  3  -3 \\\\\n0  0  0  -3  3\n\\end{pmatrix}\n$$\n为了找到 $L$ 的特征值 $\\lambda$，我们求解特征方程 $\\det(L - \\lambda I) = 0$。矩阵 $L$ 是块对角矩阵，这简化了计算。\n$$\nL = \\begin{pmatrix} L_1  \\mathbf{0} \\\\ \\mathbf{0}  L_2 \\end{pmatrix} \\quad \\text{其中} \\quad L_1 = \\begin{pmatrix} 2  -2  0 \\\\ -2  3  -1 \\\\ 0  -1  1 \\end{pmatrix} \\quad \\text{和} \\quad L_2 = \\begin{pmatrix} 3  -3 \\\\ -3  3 \\end{pmatrix}\n$$\n$L$ 的特征值集合是 $L_1$ 和 $L_2$ 特征值的并集。\n\n对于 $L_1$，我们求解 $\\det(L_1 - \\lambda I) = 0$：\n$$\n\\det \\begin{pmatrix} 2-\\lambda  -2  0 \\\\ -2  3-\\lambda  -1 \\\\ 0  -1  1-\\lambda \\end{pmatrix} = (2-\\lambda)((3-\\lambda)(1-\\lambda)-1) - (-2)(-2(1-\\lambda)) = 0\n$$\n$$\n(2-\\lambda)(\\lambda^2 - 4\\lambda + 2) - 4(1-\\lambda) = 2\\lambda^2 - 8\\lambda + 4 - \\lambda^3 + 4\\lambda^2 - 2\\lambda - 4 + 4\\lambda = 0\n$$\n$$\n-\\lambda^3 + 6\\lambda^2 - 6\\lambda = 0 \\implies -\\lambda(\\lambda^2 - 6\\lambda + 6) = 0\n$$\n这给出了一个特征值 $\\lambda_1 = 0$。另外两个是 $\\lambda^2 - 6\\lambda + 6 = 0$ 的根，根据二次公式，它们是 $\\lambda = \\frac{6 \\pm \\sqrt{36-24}}{2} = \\frac{6 \\pm \\sqrt{12}}{2} = 3 \\pm \\sqrt{3}$。因此，来自 $L_1$ 的特征值为 $\\{0, 3-\\sqrt{3}, 3+\\sqrt{3}\\}$。\n\n对于 $L_2$，我们求解 $\\det(L_2 - \\lambda I) = 0$：\n$$\n\\det \\begin{pmatrix} 3-\\lambda  -3 \\\\ -3  3-\\lambda \\end{pmatrix} = (3-\\lambda)^2 - 9 = 0\n$$\n$$\n\\lambda^2 - 6\\lambda + 9 - 9 = 0 \\implies \\lambda(\\lambda-6) = 0\n$$\n来自 $L_2$ 的特征值为 $\\{0, 6\\}$。\n\n$L$ 的完整特征值集合，按非递减顺序排序为 $\\{0, 0, 3-\\sqrt{3}, 3+\\sqrt{3}, 6\\}$。特征值 $\\lambda=0$ 的重数为 $2$。\n\n零特征值的重数与连通分量数量之间的关系是谱图论中的一个基本结果。对于具有拉普拉斯算子 $L$ 的加权无向图，与 $L$ 相关的二次型可以表示为：\n$$\n\\mathbf{x}^T L \\mathbf{x} = \\sum_{i,j} w_{ij} (x_i - x_j)^2\n$$\n其中 $\\mathbf{x}$ 是 $\\mathbb{R}^N$ 中的一个向量，$w_{ij}$ 是边 $(i,j)$ 的权重（来自矩阵 $A$）。由于所有权重 $w_{ij}$ 都是非负的，所以和中的每一项都是非负的。因此，$\\mathbf{x}^T L \\mathbf{x} \\ge 0$，这意味着 $L$ 是一个半正定矩阵，其所有特征值都是非负的，即 $\\lambda_k \\ge 0$。\n\n对应于特征值 $\\lambda=0$ 的特征向量 $\\mathbf{v}$ 必须满足 $L\\mathbf{v} = \\mathbf{0}$。这意味着 $\\mathbf{v}^T L \\mathbf{v} = 0$。从二次型中，我们得到：\n$$\n\\sum_{i,j} w_{ij} (v_i - v_j)^2 = 0\n$$\n由于对于所有存在的边，$w_{ij} > 0$，所以该方程成立的充要条件是，对于每一对由边连接的节点 $(i, j)$，都有 $v_i = v_j$。这意味着对于图的任何连通分量，该分量内的所有节点在特征向量 $\\mathbf{v}$ 中必须具有相同的值。\n\n如果图有 $k$ 个连通分量，比如说 $C_1, C_2, \\ldots, C_k$，我们可以为 $\\lambda=0$ 构造 $k$ 个线性无关的特征向量。对于每个分量 $C_m$，定义一个向量 $\\mathbf{v}^{(m)}$，如果节点 $i$ 属于 $C_m$，则其第 $i$ 个元素为 $1$，否则为 $0$。对于任何这样的向量，条件 $v_i = v_j$ 对所有相连的节点 $(i,j)$ 都成立，因为它们必须属于同一个分量。这 $k$ 个向量构成了 $L$ 的零空间的一组基。根据定义，零空间的维数就是特征值 $\\lambda=0$ 的几何重数。对于像拉普拉斯算子这样的对称矩阵，几何重数等于代数重数。\n\n因此，图中连通分量的数量等于特征值 $\\lambda=0$ 的重数。在本问题中，$\\lambda=0$ 的重数为 $2$。这意味着该网络由两个不相交的子网络组成。\n\n这对复杂自适应系统中的网络结构重建具有深远的意义。邻接矩阵 $A$ 代表推断出的交互强度。重建的一个主要目标是识别全局拓扑结构。连通分量的数量是一个基本的拓扑不变量，它揭示了系统是完全集成的，还是分裂成不相互作用的子系统。大于一的计数（如此处得到的 $2$）表明所观察到的系统表现为两个独立的模块。这一结构信息是理解系统组织和功能的关键第一步，指导对这些独立子网络的进一步分析。该计算是从交互数据中提取这一主要结构特征的直接而稳健的方法。\n\n连通分量的数量是 $2$。",
            "answer": "$$\\boxed{2}$$"
        },
        {
            "introduction": "在网络重建的实践中，我们常常会得到多个候选模型，它们在复杂性（例如，边的数量）和对观测数据的拟合优度上有所不同。为了在这些模型中做出选择，我们需要一个原则性的方法来平衡模型的解释力和简洁性。这个练习将引导您应用两种广泛使用的信息准则——赤池信息准则（AIC）和贝叶斯信息准则（BIC）——来评估和比较不同的网络结构，从而选择出在拟合数据与惩罚复杂性之间达到最佳平衡的模型。",
            "id": "4133224",
            "problem": "考虑一个用于复杂自适应系统的无向二元网络模型（伊辛模型），该系统有 $p=10$ 个节点。其生成机制为每个节点 $i \\in \\{1,\\dots,10\\}$ 分配一个局部场参数 $h_i$，为每条无向边 $\\{i,j\\}$ 分配一个对称的成对交互参数 $J_{ij}$，并产生 $n=500$ 个独立同分布的节点状态观测值。在同一数据集下，通过最大似然法拟合了三个候选网络结构 $\\mathcal{E}_1$、$\\mathcal{E}_2$ 和 $\\mathcal{E}_3$，得到的最大化对数似然值分别为 $\\ell_1=-134.2$、$\\ell_2=-128.9$ 和 $\\ell_3=-120.0$。相应的边数分别为 $|\\mathcal{E}_1|=12$、$|\\mathcal{E}_2|=16$ 和 $|\\mathcal{E}_3|=25$。假设一个候选模型中的自由参数总数是节点局部场和边耦合之和，即对于模型 $m\\in\\{1,2,3\\}$ 有 $k_m=p+|\\mathcal{E}_m|$。\n\n从以下基本原理出发：赤池信息准则（Akaike Information Criterion）是候选模型与数据生成过程之间期望Kullback–Leibler散度风险的渐近无偏估计量，而贝叶斯信息准则（Bayesian Information Criterion）是在固定维度的正则先验下对模型证据进行拉普拉斯近似的结果。请为每个候选结构计算这两个准则，并根据贝叶斯信息准则确定最优结构。将所有信息准则值四舍五入到四位有效数字。用所选候选结构的索引 $m \\in \\{1,2,3\\}$ 作为最终答案。",
            "solution": "我们已对问题陈述进行了严格审查，并确定其是有效的。该问题在科学上基于网络推断的统计模型选择原理，问题提出得当，提供了所有必要信息，且表述客观。数据是一致的，所需任务是明确的。因此，我们可以进行正式求解。\n\n该问题要求使用两个标准信息准则：赤池信息准则（AIC）和贝叶斯信息准则（BIC），来比较三个候选网络结构。目标是确定BIC偏好的模型。\n\n赤池信息准则的一般定义如下：\n$$\n\\text{AIC} = -2\\ell_{\\text{max}} + 2k\n$$\n其中 $\\ell_{\\text{max}}$ 是模型对数似然函数的最大化值， $k$ 是模型中的自由参数数量。AIC估计预测误差，从而衡量给定数据集下统计模型的相对质量。较低的AIC值表示拟合效果更好，它通过惩罚具有更多参数的模型来避免过拟合。\n\n贝叶斯信息准则的一般定义如下：\n$$\n\\text{BIC} = -2\\ell_{\\text{max}} + k \\ln(n)\n$$\n其中 $n$ 是数据点或观测值的数量。BIC源于贝叶斯框架，并且当 $n  \\exp(2) \\approx 7.4$ 时，它比AIC更严厉地惩罚模型复杂度。BIC值越低越好，表示该模型更有可能是真实模型。\n\n问题提供了以下数据：\n- 节点数：$p=10$。\n- 观测数：$n=500$。\n- 对于候选结构 $m=1$：最大化对数似然 $\\ell_1 = -134.2$，边数 $|\\mathcal{E}_1|=12$。\n- 对于候选结构 $m=2$：最大化对数似然 $\\ell_2 = -128.9$，边数 $|\\mathcal{E}_2|=16$。\n- 对于候选结构 $m=3$：最大化对数似然 $\\ell_3 = -120.0$，边数 $|\\mathcal{E}_3|=25$。\n\n每个模型 $m$ 的自由参数数量 $k_m$ 定义为节点局部场（每个节点一个）和边耦合（每条边一个）的总和：\n$$\nk_m = p + |\\mathcal{E}_m|\n$$\n我们首先计算三个候选模型中每个模型的参数数量。\n对于模型 $m=1$：\n$$\nk_1 = p + |\\mathcal{E}_1| = 10 + 12 = 22\n$$\n对于模型 $m=2$：\n$$\nk_2 = p + |\\mathcal{E}_2| = 10 + 16 = 26\n$$\n对于模型 $m=3$：\n$$\nk_3 = p + |\\mathcal{E}_3| = 10 + 25 = 35\n$$\n接下来，我们计算每个模型的AIC。\n对于模型 $m=1$：\n$$\n\\text{AIC}_1 = -2\\ell_1 + 2k_1 = -2(-134.2) + 2(22) = 268.4 + 44 = 312.4\n$$\n对于模型 $m=2$：\n$$\n\\text{AIC}_2 = -2\\ell_2 + 2k_2 = -2(-128.9) + 2(26) = 257.8 + 52 = 309.8\n$$\n对于模型 $m=3$：\n$$\n\\text{AIC}_3 = -2\\ell_3 + 2k_3 = -2(-120.0) + 2(35) = 240.0 + 70 = 310.0\n$$\n四舍五入到四位有效数字后，值为 $\\text{AIC}_1 = 312.4$，$\\text{AIC}_2 = 309.8$ 和 $\\text{AIC}_3 = 310.0$。根据AIC，模型2是首选模型，因为它的得分最低。\n\n现在，我们计算每个模型的BIC。观测数为 $n=500$。自然对数项是 $\\ln(n) = \\ln(500)$。\n对于模型 $m=1$：\n$$\n\\text{BIC}_1 = -2\\ell_1 + k_1 \\ln(n) = -2(-134.2) + 22 \\ln(500) = 268.4 + 22 \\times 6.214608... \\approx 268.4 + 136.721378 = 405.121378...\n$$\n对于模型 $m=2$：\n$$\n\\text{BIC}_2 = -2\\ell_2 + k_2 \\ln(n) = -2(-128.9) + 26 \\ln(500) = 257.8 + 26 \\times 6.214608... \\approx 257.8 + 161.579810 = 419.379810...\n$$\n对于模型 $m=3$：\n$$\n\\text{BIC}_3 = -2\\ell_3 + k_3 \\ln(n) = -2(-120.0) + 35 \\ln(500) = 240.0 + 35 \\times 6.214608... \\approx 240.0 + 217.511283 = 457.511283...\n$$\n根据要求将这些值四舍五入到四位有效数字：\n$$\n\\text{BIC}_1 \\approx 405.1\n$$\n$$\n\\text{BIC}_2 \\approx 419.4\n$$\n$$\n\\text{BIC}_3 \\approx 457.5\n$$\n选择标准是选择具有最小BIC值的模型。比较计算出的值：\n$$\n405.1  419.4  457.5\n$$\n最小值为 $\\text{BIC}_1$。因此，贝叶斯信息准则选择了第一个候选结构 $\\mathcal{E}_1$。这个最优结构的索引是 $m=1$。请注意，BIC 因其对复杂度的更强惩罚（$k \\ln(500)$ vs. $2k$），而不支持 AIC 评价较高的更复杂的模型。",
            "answer": "$$\\boxed{1}$$"
        }
    ]
}