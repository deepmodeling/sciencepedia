{
    "hands_on_practices": [
        {
            "introduction": "Before an agent can perform any meaningful inference, it must have a well-calibrated model of its own perceptual apparatus. This foundational exercise guides you through the statistical process of characterizing sensor noise from observational data. By deriving the maximum likelihood estimator for the noise covariance matrix, you will gain hands-on experience with a critical step in building robust perceptual systems. ",
            "id": "4128168",
            "problem": "An agent in a complex adaptive system uses a $p$-dimensional sensor to perceive environmental features. During calibration, the agent observes $J$ static calibration targets, indexed by $j \\in \\{1,\\dots,J\\}$, each with a known latent state $\\mathbf{s}_j$ and a known forward observation mapping $h(\\cdot)$ that predicts a noise-free measurement $\\boldsymbol{\\mu}_j = h(\\mathbf{s}_j) \\in \\mathbb{R}^p$. For target $j$, the agent collects $n_j$ repeated measurements $\\{\\mathbf{z}_{j,k}\\}_{k=1}^{n_j}$, which are modeled as independent draws from a multivariate normal distribution with mean $\\boldsymbol{\\mu}_j$ and unknown symmetric positive definite covariance matrix $R \\in \\mathbb{R}^{p \\times p}$, i.e., $\\mathbf{z}_{j,k} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_j, R)$, independently over $j$ and $k$. Define residuals $\\mathbf{r}_{j,k} = \\mathbf{z}_{j,k} - \\boldsymbol{\\mu}_j$ and let the total sample size be $N = \\sum_{j=1}^J n_j$.\n\nStarting from the definition of the likelihood under multivariate normality and the definition of the Maximum Likelihood Estimator (MLE), derive the estimator $\\hat{R}$ that calibrates the sensor noise covariance from these repeated measurements. Then, quantify the uncertainty in $\\hat{R}$ using the asymptotic normality of the MLE by expressing the limiting distribution of $\\sqrt{N}\\,\\mathrm{vec}(\\hat{R} - R)$, where $\\mathrm{vec}(\\cdot)$ denotes vectorization. In your derivation, use the following foundational facts:\n- The multivariate normal probability density function for $\\mathbf{z} \\in \\mathbb{R}^p$ with mean $\\boldsymbol{\\mu} \\in \\mathbb{R}^p$ and covariance $R \\in \\mathbb{R}^{p \\times p}$ is $f(\\mathbf{z}; \\boldsymbol{\\mu}, R) = \\frac{1}{(2\\pi)^{p/2} |R|^{1/2}} \\exp\\left(-\\frac{1}{2} (\\mathbf{z} - \\boldsymbol{\\mu})^\\top R^{-1} (\\mathbf{z} - \\boldsymbol{\\mu})\\right)$.\n- The Maximum Likelihood Estimator (MLE) is obtained by maximizing the joint likelihood over independent samples.\n- Under regularity conditions, the MLE is consistent and asymptotically normal with covariance given by the inverse of the Fisher information, where Fisher information is defined as $I(\\theta) = \\mathbb{E}\\left[-\\frac{\\partial^2 \\ell(\\theta)}{\\partial \\theta \\partial \\theta^\\top}\\right]$ and $\\ell(\\theta)$ is the log-likelihood.\n\nFor the asymptotic covariance, you may express the result using standard linear-algebraic operators:\n- $\\mathrm{vec}(\\cdot)$ is the vectorization operator.\n- $\\otimes$ is the Kronecker product.\n- $K_p \\in \\mathbb{R}^{p^2 \\times p^2}$ is the commutation matrix defined by $K_p\\,\\mathrm{vec}(A) = \\mathrm{vec}(A^\\top)$ for any $A \\in \\mathbb{R}^{p \\times p}$.\n- $\\mathrm{vech}(\\cdot)$ is the half-vectorization that stacks the upper-triangular elements of a symmetric matrix.\n- $D_p \\in \\mathbb{R}^{p^2 \\times p(p+1)/2}$ is the duplication matrix satisfying $\\mathrm{vec}(S) = D_p\\,\\mathrm{vech}(S)$ for any symmetric $S \\in \\mathbb{R}^{p \\times p}$.\n\nFinally, identify the single option that correctly states both the MLE $\\hat{R}$ and a valid asymptotic covariance for $\\mathrm{vec}(\\hat{R})$ (or equivalently, for $\\mathrm{vech}(\\hat{R})$ via the duplication matrix). Select exactly one option.\n\nA. $\\hat{R} = \\frac{1}{N} \\sum_{j=1}^{J} \\sum_{k=1}^{n_j} \\mathbf{r}_{j,k}\\,\\mathbf{r}_{j,k}^\\top$, and $\\sqrt{N}\\,\\mathrm{vec}(\\hat{R} - R) \\xrightarrow{d} \\mathcal{N}\\!\\left(\\mathbf{0},\\,(I_p + K_p)\\,(R \\otimes R)\\right)$; equivalently, for $\\mathrm{vech}$, the asymptotic covariance is $I_{\\mathrm{sym}}^{-1}$ where $I_{\\mathrm{sym}} = \\frac{N}{2} D_p^\\top (R^{-1} \\otimes R^{-1}) D_p$, so an approximate $(1-\\alpha)$ confidence ellipsoid for $\\mathrm{vech}(R)$ is $\\left\\{\\theta \\in \\mathbb{R}^{p(p+1)/2} : (\\theta - \\mathrm{vech}(\\hat{R}))^\\top I_{\\mathrm{sym}}\\,(\\theta - \\mathrm{vech}(\\hat{R})) \\leq \\chi^2_{p(p+1)/2,\\,1-\\alpha}\\right\\}$.\n\nB. $\\hat{R} = \\frac{1}{N-1} \\sum_{j=1}^{J} \\sum_{k=1}^{n_j} \\mathbf{r}_{j,k}\\,\\mathbf{r}_{j,k}^\\top$, and $\\sqrt{N}\\,\\mathrm{vec}(\\hat{R} - R) \\xrightarrow{d} \\mathcal{N}\\!\\left(\\mathbf{0},\\, (R^{-1} \\otimes R^{-1})\\right)$; thus the asymptotic covariance of $\\mathrm{vec}(\\hat{R})$ is $\\frac{1}{N}(R^{-1} \\otimes R^{-1})$.\n\nC. $\\hat{R} = \\frac{1}{J} \\sum_{j=1}^{J} \\left(\\frac{1}{n_j} \\sum_{k=1}^{n_j} \\mathbf{r}_{j,k}\\,\\mathbf{r}_{j,k}^\\top\\right)$, and $\\sqrt{N}\\,\\mathrm{vec}(\\hat{R} - R) \\xrightarrow{d} \\mathcal{N}\\!\\left(\\mathbf{0},\\, \\frac{1}{\\sum_{j=1}^{J} n_j^2}\\,(I_p + K_p)\\,(R \\otimes R)\\right)$.\n\nD. $\\hat{R} = \\frac{1}{N} \\sum_{j=1}^{J} \\sum_{k=1}^{n_j} (\\mathbf{z}_{j,k} - \\bar{\\mathbf{z}})\\,(\\mathbf{z}_{j,k} - \\bar{\\mathbf{z}})^\\top$ with $\\bar{\\mathbf{z}} = \\frac{1}{N}\\sum_{j=1}^{J}\\sum_{k=1}^{n_j}\\mathbf{z}_{j,k}$, and $\\sqrt{N}\\,\\mathrm{vec}(\\hat{R} - R) \\xrightarrow{d} \\mathcal{N}\\!\\left(\\mathbf{0},\\, (R \\otimes R)\\right)$; hence the asymptotic covariance of $\\mathrm{vec}(\\hat{R})$ is $\\frac{1}{N}(R \\otimes R)$.",
            "solution": "### Step 1: Extract Givens\n\nThe problem statement provides the following information:\n- An agent uses a $p$-dimensional sensor.\n- There are $J$ static calibration targets, indexed by $j \\in \\{1,\\dots,J\\}$.\n- Each target $j$ has a known latent state $\\mathbf{s}_j$ and a known forward observation mapping $h(\\cdot)$.\n- The noise-free measurement (mean) for target $j$ is known and given by $\\boldsymbol{\\mu}_j = h(\\mathbf{s}_j) \\in \\mathbb{R}^p$.\n- For each target $j$, $n_j$ repeated measurements are collected: $\\{\\mathbf{z}_{j,k}\\}_{k=1}^{n_j}$.\n- The measurements are modeled as independent draws from a multivariate normal distribution: $\\mathbf{z}_{j,k} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_j, R)$.\n- The covariance matrix $R \\in \\mathbb{R}^{p \\times p}$ is unknown, symmetric, positive definite, and common to all measurements.\n- The samples are independent over both indices $j$ and $k$.\n- Residuals are defined as $\\mathbf{r}_{j,k} = \\mathbf{z}_{j,k} - \\boldsymbol{\\mu}_j$.\n- The total sample size is $N = \\sum_{j=1}^J n_j$.\n- The probability density function for $\\mathbf{z} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, R)$ is $f(\\mathbf{z}; \\boldsymbol{\\mu}, R) = \\frac{1}{(2\\pi)^{p/2} |R|^{1/2}} \\exp\\left(-\\frac{1}{2} (\\mathbf{z} - \\boldsymbol{\\mu})^\\top R^{-1} (\\mathbf{z} - \\boldsymbol{\\mu})\\right)$.\n- Standard definitions for MLE, Fisher information, and its relation to asymptotic normality are provided.\n- Standard matrix operators are defined: $\\mathrm{vec}(\\cdot)$, $\\otimes$, $K_p$, $\\mathrm{vech}(\\cdot)$, $D_p$.\n\n### Step 2: Validate Using Extracted Givens\n\n- **Scientifically Grounded**: The problem is firmly rooted in the theory of maximum likelihood estimation for parameters of a multivariate normal distribution. This is a fundamental and well-established topic in statistics. All concepts and tools mentioned are standard in this field. The setup is scientifically and mathematically sound.\n- **Well-Posed**: The problem is to derive a specific estimator ($\\hat{R}$) and its asymptotic properties. The givens are sufficient and consistent to perform this derivation. A unique MLE exists and its asymptotic properties are well-defined under standard regularity conditions, which are implicitly assumed by the reference to asymptotic normality.\n- **Objective**: The language is precise, mathematical, and free of any subjectivity or ambiguity.\n\nThe problem is a standard exercise in multivariate statistical inference. It is complete, consistent, and scientifically sound. No flaws are identified.\n\n### Step 3: Verdict and Action\n\nThe problem statement is **valid**. Proceeding with the solution.\n\n### Derivation of the MLE $\\hat{R}$\n\nThe set of all observations is $\\{\\mathbf{z}_{j,k}\\}_{j=1, k=1}^{J, n_j}$. Since the observations are independent, the joint likelihood function $L(R)$ is the product of the individual probability densities.\n$$ L(R) = \\prod_{j=1}^{J} \\prod_{k=1}^{n_j} f(\\mathbf{z}_{j,k}; \\boldsymbol{\\mu}_j, R) $$\nThe log-likelihood function $\\ell(R) = \\ln L(R)$ is:\n$$ \\ell(R) = \\sum_{j=1}^{J} \\sum_{k=1}^{n_j} \\ln f(\\mathbf{z}_{j,k}; \\boldsymbol{\\mu}_j, R) $$\nSubstituting the multivariate normal PDF:\n$$ \\ell(R) = \\sum_{j=1}^{J} \\sum_{k=1}^{n_j} \\left[ -\\frac{p}{2} \\ln(2\\pi) - \\frac{1}{2}\\ln|R| - \\frac{1}{2} (\\mathbf{z}_{j,k} - \\boldsymbol{\\mu}_j)^\\top R^{-1} (\\mathbf{z}_{j,k} - \\boldsymbol{\\mu}_j) \\right] $$\nLet $N = \\sum_{j=1}^J n_j$ be the total number of samples. Combining the terms:\n$$ \\ell(R) = -\\frac{Np}{2}\\ln(2\\pi) - \\frac{N}{2}\\ln|R| - \\frac{1}{2} \\sum_{j=1}^{J} \\sum_{k=1}^{n_j} (\\mathbf{z}_{j,k} - \\boldsymbol{\\mu}_j)^\\top R^{-1} (\\mathbf{z}_{j,k} - \\boldsymbol{\\mu}_j) $$\nUsing the trace property $\\mathbf{x}^\\top A \\mathbf{x} = \\mathrm{tr}(\\mathbf{x}^\\top A \\mathbf{x}) = \\mathrm{tr}(A\\mathbf{x}\\mathbf{x}^\\top)$ and the definition of the residual $\\mathbf{r}_{j,k} = \\mathbf{z}_{j,k} - \\boldsymbol{\\mu}_j$:\n$$ \\ell(R) = C - \\frac{N}{2}\\ln|R| - \\frac{1}{2} \\sum_{j=1}^{J} \\sum_{k=1}^{n_j} \\mathrm{tr}(R^{-1} \\mathbf{r}_{j,k}\\mathbf{r}_{j,k}^\\top) $$\n$$ \\ell(R) = C - \\frac{N}{2}\\ln|R| - \\frac{1}{2} \\mathrm{tr}\\left(R^{-1} \\left(\\sum_{j=1}^{J} \\sum_{k=1}^{n_j} \\mathbf{r}_{j,k}\\mathbf{r}_{j,k}^\\top\\right)\\right) $$\nLet $S_r = \\sum_{j=1}^{J} \\sum_{k=1}^{n_j} \\mathbf{r}_{j,k}\\mathbf{r}_{j,k}^\\top$. This is the matrix of sum of squares of the residuals.\nTo find the maximum, we differentiate $\\ell(R)$ with respect to $R$ and set the derivative to zero. Using standard matrix calculus derivatives $\\frac{\\partial \\ln|R|}{\\partial R} = (R^{-1})^\\top = R^{-1}$ and $\\frac{\\partial \\mathrm{tr}(AR^{-1})}{\\partial R} = -(R^{-1}AR^{-1})^\\top = -R^{-1}AR^{-1}$ (for symmetric $A, R$):\n$$ \\frac{\\partial \\ell(R)}{\\partial R} = -\\frac{N}{2}R^{-1} - \\frac{1}{2} (-R^{-1}S_r R^{-1}) = \\frac{1}{2}R^{-1}(S_r - NR)R^{-1} $$\nSetting the gradient to zero:\n$$ \\frac{1}{2}\\hat{R}^{-1}(S_r - N\\hat{R})\\hat{R}^{-1} = 0 $$\nSince $\\hat{R}^{-1}$ is invertible, this implies $S_r - N\\hat{R} = 0$, which gives the Maximum Likelihood Estimator (MLE):\n$$ \\hat{R} = \\frac{1}{N} S_r = \\frac{1}{N} \\sum_{j=1}^{J} \\sum_{k=1}^{n_j} \\mathbf{r}_{j,k}\\mathbf{r}_{j,k}^\\top $$\n\n### Asymptotic Distribution of $\\hat{R}$\n\nThe asymptotic distribution of an MLE is determined by the inverse of the Fisher information matrix. For an MLE $\\hat{\\theta}$ of a parameter vector $\\theta$, under regularity conditions, $\\sqrt{N}(\\hat{\\theta} - \\theta) \\xrightarrow{d} \\mathcal{N}(0, I_1(\\theta)^{-1})$, where $I_1(\\theta)$ is the Fisher information for a single observation.\n\nThe parameters of interest are the unique elements of the symmetric matrix $R$, denoted by $\\theta = \\mathrm{vech}(R)$, which is a vector of length $p(p+1)/2$.\nA standard result from multivariate analysis gives the Fisher information matrix for $\\mathrm{vech}(R)$ for a single observation from $\\mathcal{N}(\\boldsymbol{\\mu}, R)$ (with known $\\boldsymbol{\\mu}$) as:\n$$ I_1(\\mathrm{vech}(R)) = \\frac{1}{2} D_p^\\top (R^{-1} \\otimes R^{-1}) D_p $$\nwhere $D_p$ is the duplication matrix.\nThe asymptotic covariance of $\\sqrt{N}(\\mathrm{vech}(\\hat{R}) - \\mathrm{vech}(R))$ is thus $I_1(\\mathrm{vech}(R))^{-1}$.\n$$ \\mathrm{AsyCov}(\\sqrt{N}(\\mathrm{vech}(\\hat{R}) - \\mathrm{vech}(R))) = \\left( \\frac{1}{2} D_p^\\top (R^{-1} \\otimes R^{-1}) D_p \\right)^{-1} = 2 \\left( D_p^\\top (R^{-1} \\otimes R^{-1}) D_p \\right)^{-1} $$\nUsing the identity for the inverse of such a matrix, $(D_p^\\top(A \\otimes A)D_p)^{-1} = D_p^+(A^{-1} \\otimes A^{-1})(D_p^+)^\\top$, where $D_p^+ = (D_p^\\top D_p)^{-1}D_p^\\top$ is the Moore-Penrose inverse of $D_p$:\n$$ \\mathrm{AsyCov}(\\sqrt{N}(\\mathrm{vech}(\\hat{R}) - \\mathrm{vech}(R))) = 2 D_p^+ (R \\otimes R) (D_p^+)^\\top $$\nWe need the asymptotic covariance of $\\sqrt{N}(\\mathrm{vec}(\\hat{R}) - \\mathrm{vec}(R))$. We use the relationship $\\mathrm{vec}(A) = D_p \\mathrm{vech}(A)$ for any symmetric matrix $A$.\n$$ \\sqrt{N}(\\mathrm{vec}(\\hat{R}) - \\mathrm{vec}(R)) = \\sqrt{N} D_p (\\mathrm{vech}(\\hat{R}) - \\mathrm{vech}(R)) $$\nThe covariance of this transformed vector is:\n$$ \\mathrm{AsyCov}(\\sqrt{N}(\\mathrm{vec}(\\hat{R}) - \\mathrm{vec}(R))) = D_p \\left( 2 D_p^+ (R \\otimes R) (D_p^+)^\\top \\right) D_p^\\top $$\nUsing the identity $D_p D_p^+ = \\frac{1}{2}(I_{p^2} + K_p)$, where $K_p$ is the commutation matrix:\n$$ = 2 (D_p D_p^+) (R \\otimes R) (D_p D_p^+)^\\top $$\n$$ = 2 \\left( \\frac{1}{2}(I_{p^2} + K_p) \\right) (R \\otimes R) \\left( \\frac{1}{2}(I_{p^2} + K_p) \\right)^\\top $$\nSince $K_p$ is symmetric ($K_p^\\top=K_p$), this simplifies to:\n$$ = \\frac{1}{2} (I_{p^2} + K_p) (R \\otimes R) (I_{p^2} + K_p) $$\nFor symmetric matrices like $R$, it holds that $(R \\otimes R)K_p = K_p(R \\otimes R)$. Also, $K_p^2=I_{p^2}$.\n$$ = \\frac{1}{2} (I_{p^2} + K_p)^2 (R \\otimes R) = \\frac{1}{2} (I_{p^2} + 2K_p + K_p^2) (R \\otimes R) $$\n$$ = \\frac{1}{2} (2I_{p^2} + 2K_p) (R \\otimes R) = (I_{p^2} + K_p) (R \\otimes R) $$\nThus, the limiting distribution is:\n$$ \\sqrt{N}\\,\\mathrm{vec}(\\hat{R} - R) \\xrightarrow{d} \\mathcal{N}(\\mathbf{0}, (I_p + K_p)(R \\otimes R)) $$\n\n### Evaluation of Options\n\n- **Option A**:\n  - MLE: $\\hat{R} = \\frac{1}{N} \\sum_{j=1}^{J} \\sum_{k=1}^{n_j} \\mathbf{r}_{j,k}\\,\\mathbf{r}_{j,k}^\\top$. This matches our derived MLE.\n  - Asymptotic distribution: $\\sqrt{N}\\,\\mathrm{vec}(\\hat{R} - R) \\xrightarrow{d} \\mathcal{N}\\!\\left(\\mathbf{0},\\,(I_p + K_p)\\,(R \\otimes R)\\right)$. This matches our derived asymptotic covariance.\n  - Confidence ellipsoid: The total Fisher information for the sample of size $N$ for the parameter vector $\\theta = \\mathrm{vech}(R)$ is $I_N(\\theta) = N \\cdot I_1(\\theta)$. Using our earlier result for $I_1$:\n    $$ I_{\\mathrm{sym}} = I_N(\\mathrm{vech}(R)) = N \\left( \\frac{1}{2} D_p^\\top (R^{-1} \\otimes R^{-1}) D_p \\right) = \\frac{N}{2} D_p^\\top (R^{-1} \\otimes R^{-1}) D_p $$\n    This matches the definition of $I_{\\mathrm{sym}}$ in the option. The asymptotic distribution of $\\mathrm{vech}(\\hat{R})$ is $\\mathcal{N}(\\mathrm{vech}(R), I_{\\mathrm{sym}}^{-1})$. Using this, the quadratic form $(\\mathrm{vech}(\\hat{R}) - \\mathrm{vech}(R))^\\top I_{\\mathrm{sym}} (\\mathrm{vech}(\\hat{R}) - \\mathrm{vech}(R))$ is asymptotically distributed as $\\chi^2$ with $p(p+1)/2$ degrees of freedom. This forms the basis for the confidence ellipsoid for $\\mathrm{vech}(R)$. The expression given is correct for a $(1-\\alpha)$ confidence ellipsoid.\n  - Verdict: **Correct**.\n\n- **Option B**:\n  - MLE: $\\hat{R} = \\frac{1}{N-1} \\sum \\mathbf{r}_{j,k}\\,\\mathbf{r}_{j,k}^\\top$. The normalization factor is $N-1$, which is typical for an unbiased estimator when the mean is also estimated. Since the means $\\boldsymbol{\\mu}_j$ are known, the MLE has a factor of $N$. This estimator is incorrect.\n  - Asymptotic covariance: The form $\\frac{1}{N}(R^{-1} \\otimes R^{-1})$ is incorrect. The covariance of an estimator for $R$ must scale with powers of $R$, not $R^{-1}$.\n  - Verdict: **Incorrect**.\n\n- **Option C**:\n  - MLE: $\\hat{R} = \\frac{1}{J} \\sum_{j=1}^{J} \\left(\\frac{1}{n_j} \\sum_{k=1}^{n_j} \\mathbf{r}_{j,k}\\,\\mathbf{r}_{j,k}^\\top\\right)$. This is an unweighted average of per-target covariance estimates. The true MLE pools all $N$ data points, effectively weighting each per-target estimate by its sample size $n_j$. This estimator is only correct if all $n_j$ are equal. In the general case, it is not the MLE.\n  - Asymptotic covariance: The scaling factor is incorrect.\n  - Verdict: **Incorrect**.\n\n- **Option D**:\n  - MLE: $\\hat{R} = \\frac{1}{N} \\sum_{j,k} (\\mathbf{z}_{j,k} - \\bar{\\mathbf{z}})\\,(\\mathbf{z}_{j,k} - \\bar{\\mathbf{z}})^\\top$. This estimator incorrectly assumes that all observations share a single common mean, which it estimates as $\\bar{\\mathbf{z}}$. The problem states that the observations are from different populations with known, distinct means $\\boldsymbol{\\mu}_j$. This estimator is therefore incorrect.\n  - Asymptotic covariance: The expression $(R \\otimes R)$ is missing the $(I_p + K_p)$ factor, which is necessary to account for the symmetry of $R$.\n  - Verdict: **Incorrect**.\n\nBased on the derivations, only option A provides the correct MLE, the correct asymptotic distribution for its vectorized form, and a correct formulation for the corresponding confidence ellipsoid.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Perception is an active inferential process, where sensory data is interpreted through the lens of an agent's internal models. This practice explores what happens when an agent's model of the world—its \"memory\" of how stimuli are generated—is flawed. You will compute how an agent's belief (the posterior distribution) is shaped by a misspecified likelihood and quantify the resulting error using the Kullback–Leibler divergence, a fundamental tool for comparing probability distributions. ",
            "id": "4128046",
            "problem": "Consider a single-agent Bayesian estimator in a complex adaptive system that must infer a latent stimulus $s \\in \\mathbb{R}$ from a single cue $y$. The agent maintains a memory-based prior over $s$ given by a Gaussian distribution with mean $m_{0}$ and variance $v_{0}$. The physical environment generates the cue according to a true likelihood that is Gaussian with mean $s$ and variance $\\sigma_{\\text{true}}^{2}$. However, due to perception bias and imperfect internal modeling, the agent uses a misspecified likelihood that is Gaussian with mean $s + b$ (a fixed shift bias) and variance $\\sigma_{\\text{mis}}^{2}$. You are given the following parameter values: $m_{0} = 0$, $v_{0} = 1$, $\\sigma_{\\text{true}}^{2} = 1$, $\\sigma_{\\text{mis}}^{2} = 4$, $b = 0.5$, and a realized cue $y = 1$.\n\nStarting only from Bayes’ rule and the definition of the Gaussian probability density function, first derive the true posterior $p_{\\text{T}}(s \\mid y)$ and the agent’s misspecified posterior $p_{\\text{A}}(s \\mid y)$ as Gaussian densities by completing the square, identifying their means and variances explicitly. Then, using the definition of the Kullback–Leibler divergence (KLD), compute the divergence from the true posterior to the agent’s posterior, $D_{\\mathrm{KL}}(p_{\\text{T}}(\\cdot \\mid y) \\,\\|\\, p_{\\text{A}}(\\cdot \\mid y))$, as a single scalar value.\n\nRound your final numerical answer to $4$ significant figures. Express the final result as a pure number without units.",
            "solution": "The problem requires the derivation of a true posterior and a misspecified posterior distribution, followed by the calculation of the Kullback-Leibler (KL) divergence between them. The analysis starts from Bayes' rule, which states that the posterior probability is proportional to the product of the likelihood and the prior: $p(s \\mid y) \\propto p(y \\mid s) p(s)$. Since all distributions involved are Gaussian, their product will also be Gaussian. A Gaussian probability density function (PDF) for a variable $x$ with mean $\\mu$ and variance $\\sigma^2$ is given by $\\mathcal{N}(x \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)$.\n\nWe can find the parameters of the posterior Gaussian distribution by examining the logarithm of the posterior, which will be a quadratic function of the latent variable $s$.\n$\\ln p(s \\mid y) = \\ln p(y \\mid s) + \\ln p(s) + \\text{constant}$.\nThe terms dependent on $s$ in the log-posterior have the form:\n$$ \\ln p(s \\mid y) = -\\frac{(s-\\mu_{\\text{post}})^2}{2v_{\\text{post}}} + C = -\\frac{1}{2v_{\\text{post}}}s^2 + \\frac{\\mu_{\\text{post}}}{v_{\\text{post}}}s - \\frac{\\mu_{\\text{post}}^2}{2v_{\\text{post}}} + C $$\nwhere $\\mu_{\\text{post}}$ and $v_{\\text{post}}$ are the posterior mean and variance, respectively. By combining the log-likelihood and log-prior and then matching the coefficients of $s^2$ and $s$, we can derive the posterior parameters.\n\nThe problem provides the prior as $p(s) = \\mathcal{N}(s \\mid m_0, v_0)$, where $m_0=0$ and $v_0=1$.\nThe log-prior is $\\ln p(s) = -\\frac{(s-m_0)^2}{2v_0} + C_1$.\n\nFirst, we derive the true posterior, $p_{\\text{T}}(s \\mid y)$.\nThe true likelihood is $p_{\\text{T}}(y \\mid s) = \\mathcal{N}(y \\mid s, \\sigma_{\\text{true}}^2)$, with $\\sigma_{\\text{true}}^2=1$.\nThe log-likelihood is $\\ln p_{\\text{T}}(y \\mid s) = -\\frac{(y-s)^2}{2\\sigma_{\\text{true}}^2} + C_2$.\nThe log-posterior is:\n$$ \\ln p_{\\text{T}}(s \\mid y) \\propto -\\frac{(s-m_0)^2}{2v_0} - \\frac{(y-s)^2}{2\\sigma_{\\text{true}}^2} $$\n$$ = -\\frac{1}{2}\\left( \\frac{s^2 - 2sm_0 + m_0^2}{v_0} + \\frac{s^2 - 2sy + y^2}{\\sigma_{\\text{true}}^2} \\right) $$\nGrouping terms by powers of $s$:\n$$ = -\\frac{1}{2}\\left[ s^2\\left(\\frac{1}{v_0} + \\frac{1}{\\sigma_{\\text{true}}^2}\\right) - 2s\\left(\\frac{m_0}{v_0} + \\frac{y}{\\sigma_{\\text{true}}^2}\\right) \\right] + \\text{constant} $$\nComparing this to the general form of a log-Gaussian, we identify the inverse of the true posterior variance, $v_{\\text{T}}$, and the mean, $m_{\\text{T}}$:\n$$ \\frac{1}{v_{\\text{T}}} = \\frac{1}{v_0} + \\frac{1}{\\sigma_{\\text{true}}^2} \\implies v_{\\text{T}} = \\left(\\frac{1}{v_0} + \\frac{1}{\\sigma_{\\text{true}}^2}\\right)^{-1} $$\n$$ \\frac{m_{\\text{T}}}{v_{\\text{T}}} = \\frac{m_0}{v_0} + \\frac{y}{\\sigma_{\\text{true}}^2} \\implies m_{\\text{T}} = v_{\\text{T}}\\left(\\frac{m_0}{v_0} + \\frac{y}{\\sigma_{\\text{true}}^2}\\right) $$\nSubstituting the given values $m_0=0$, $v_0=1$, $\\sigma_{\\text{true}}^2=1$, and $y=1$:\n$$ v_{\\text{T}} = \\left(\\frac{1}{1} + \\frac{1}{1}\\right)^{-1} = (2)^{-1} = 0.5 $$\n$$ m_{\\text{T}} = 0.5 \\left(\\frac{0}{1} + \\frac{1}{1}\\right) = 0.5(1) = 0.5 $$\nThus, the true posterior is $p_{\\text{T}}(s \\mid y) = \\mathcal{N}(s \\mid m_{\\text{T}}, v_{\\text{T}}) = \\mathcal{N}(s \\mid 0.5, 0.5)$.\n\nNext, we derive the agent's misspecified posterior, $p_{\\text{A}}(s \\mid y)$.\nThe agent uses the same prior $p(s) = \\mathcal{N}(s \\mid m_0, v_0)$ but a misspecified likelihood $p_{\\text{A}}(y \\mid s) = \\mathcal{N}(y \\mid s+b, \\sigma_{\\text{mis}}^2)$, with $b=0.5$ and $\\sigma_{\\text{mis}}^2=4$.\nThe log-likelihood used by the agent is $\\ln p_{\\text{A}}(y \\mid s) = -\\frac{(y-(s+b))^2}{2\\sigma_{\\text{mis}}^2} + C_3 = -\\frac{((y-b)-s)^2}{2\\sigma_{\\text{mis}}^2} + C_3$.\nThis has the same functional form as the true likelihood, but with the observation $y$ replaced by an \"effective observation\" $y' = y-b$ and variance $\\sigma_{\\text{true}}^2$ replaced by $\\sigma_{\\text{mis}}^2$.\nThe derivation structure is identical. The agent's posterior variance $v_{\\text{A}}$ and mean $m_{\\text{A}}$ are:\n$$ v_{\\text{A}} = \\left(\\frac{1}{v_0} + \\frac{1}{\\sigma_{\\text{mis}}^2}\\right)^{-1} $$\n$$ m_{\\text{A}} = v_{\\text{A}}\\left(\\frac{m_0}{v_0} + \\frac{y-b}{\\sigma_{\\text{mis}}^2}\\right) $$\nSubstituting the given values $m_0=0$, $v_0=1$, $\\sigma_{\\text{mis}}^2=4$, $b=0.5$, and $y=1$:\n$$ v_{\\text{A}} = \\left(\\frac{1}{1} + \\frac{1}{4}\\right)^{-1} = \\left(\\frac{5}{4}\\right)^{-1} = \\frac{4}{5} = 0.8 $$\n$$ m_{\\text{A}} = 0.8 \\left(\\frac{0}{1} + \\frac{1-0.5}{4}\\right) = 0.8 \\left(\\frac{0.5}{4}\\right) = 0.8(0.125) = 0.1 $$\nThus, the agent's posterior is $p_{\\text{A}}(s \\mid y) = \\mathcal{N}(s \\mid m_{\\text{A}}, v_{\\text{A}}) = \\mathcal{N}(s \\mid 0.1, 0.8)$.\n\nFinally, we compute the KL divergence from the true posterior to the agent's posterior, $D_{\\mathrm{KL}}(p_{\\text{T}} \\| p_{\\text{A}})$.\nFor two one-dimensional Gaussian distributions $p_1 = \\mathcal{N}(\\mu_1, v_1)$ and $p_2 = \\mathcal{N}(\\mu_2, v_2)$, the KL divergence is given by the formula:\n$$ D_{\\mathrm{KL}}(p_1 \\| p_2) = \\frac{1}{2} \\left( \\ln\\left(\\frac{v_2}{v_1}\\right) - 1 + \\frac{v_1}{v_2} + \\frac{(\\mu_1-\\mu_2)^2}{v_2} \\right) $$\nIn our case, $p_1$ is the true posterior $p_{\\text{T}}$ and $p_2$ is the agent's posterior $p_{\\text{A}}$. The parameters are:\n$\\mu_1 = m_{\\text{T}} = 0.5$, $v_1 = v_{\\text{T}} = 0.5$\n$\\mu_2 = m_{\\text{A}} = 0.1$, $v_2 = v_{\\text{A}} = 0.8$\n\nSubstituting these values into the KL divergence formula:\n$$ D_{\\mathrm{KL}}(p_{\\text{T}} \\| p_{\\text{A}}) = \\frac{1}{2} \\left( \\ln\\left(\\frac{0.8}{0.5}\\right) - 1 + \\frac{0.5}{0.8} + \\frac{(0.5-0.1)^2}{0.8} \\right) $$\nWe compute each term inside the parenthesis:\n$$ \\frac{v_{\\text{A}}}{v_{\\text{T}}} = \\frac{0.8}{0.5} = 1.6 $$\n$$ \\frac{v_{\\text{T}}}{v_{\\text{A}}} = \\frac{0.5}{0.8} = 0.625 $$\n$$ \\frac{(m_{\\text{T}}-m_{\\text{A}})^2}{v_{\\text{A}}} = \\frac{(0.4)^2}{0.8} = \\frac{0.16}{0.8} = 0.2 $$\nNow substitute these back into the expression for the divergence:\n$$ D_{\\mathrm{KL}}(p_{\\text{T}} \\| p_{\\text{A}}) = \\frac{1}{2} \\left( \\ln(1.6) - 1 + 0.625 + 0.2 \\right) $$\n$$ D_{\\mathrm{KL}}(p_{\\text{T}} \\| p_{\\text{A}}) = \\frac{1}{2} \\left( \\ln(1.6) - 0.175 \\right) $$\nUsing a calculator for the natural logarithm:\n$$ \\ln(1.6) \\approx 0.470003629 $$\n$$ D_{\\mathrm{KL}}(p_{\\text{T}} \\| p_{\\text{A}}) \\approx \\frac{1}{2} (0.470003629 - 0.175) = \\frac{1}{2} (0.295003629) \\approx 0.1475018145 $$\nThe problem requires the answer to be rounded to $4$ significant figures.\n$$ D_{\\mathrm{KL}}(p_{\\text{T}} \\| p_{\\text{A}}) \\approx 0.1475 $$\nThis value represents the information loss, measured in nats, when approximating the true posterior distribution with the agent's misspecified one.",
            "answer": "$$\\boxed{0.1475}$$"
        },
        {
            "introduction": "A truly adaptive agent must learn continually without catastrophically forgetting past knowledge, a challenge that necessitates mechanisms for memory stabilization. This problem connects the computational concept of 'elastic weight consolidation' back to its Bayesian roots, providing a principled justification for penalizing changes to parameters crucial for previous tasks. You will implement a model to demonstrate how this consolidation, combined with experience replay, can effectively mitigate forgetting in a sequential learning environment. ",
            "id": "4128155",
            "problem": "Consider an adaptive agent with parameter vector $\\boldsymbol{\\theta} \\in \\mathbb{R}^d$ that sequentially learns tasks. For each task $t$, model the perceptual loss as a strictly convex quadratic of the form $L_t(\\boldsymbol{\\theta}) = \\frac{1}{2}(\\boldsymbol{\\theta} - \\boldsymbol{\\mu}_t)^\\top A_t(\\boldsymbol{\\theta} - \\boldsymbol{\\mu}_t)$, where $A_t \\in \\mathbb{R}^{d \\times d}$ is positive semidefinite and $\\boldsymbol{\\mu}_t \\in \\mathbb{R}^d$ is the task-specific preferred parameter vector. Assume a previously learned parameter $\\boldsymbol{\\theta}_{\\text{prev}}$ summarizes the solution to past tasks in the absence of the new task, obtained by minimizing $\\sum_{t=1}^{T_{\\text{past}}} L_t(\\boldsymbol{\\theta})$.\n\nStarting from Bayesian principles and the Maximum A Posteriori (MAP) estimator, and using the Laplace approximation with diagonal Fisher information, derive an optimization objective for consolidation that penalizes changes in parameters important to past tasks. The importance weights are given by the diagonal of the Fisher information matrix $\\operatorname{diag}(F)$, where $F \\in \\mathbb{R}^{d \\times d}$ is diagonal and positive semidefinite. Formally justify an objective of the form\n$$\nJ(\\boldsymbol{\\theta}) = L_{\\text{new}}(\\boldsymbol{\\theta}) + \\frac{\\lambda}{2}(\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_{\\text{prev}})^\\top F(\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_{\\text{prev}}),\n$$\nwhere $L_{\\text{new}}$ is the loss of the current task, $F$ captures parameter importance from past tasks, and $\\lambda > 0$ is a consolidation strength. Then incorporate experience replay by including a weighted sum of past task losses into the objective:\n$$\nJ_{\\text{replay}}(\\boldsymbol{\\theta}) = L_{\\text{new}}(\\boldsymbol{\\theta}) + \\sum_{t=1}^{T_{\\text{past}}} \\alpha_t L_t(\\boldsymbol{\\theta}) + \\frac{\\lambda}{2}(\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_{\\text{prev}})^\\top F(\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_{\\text{prev}}),\n$$\nwhere $\\alpha_t \\ge 0$ are replay weights. For strictly convex quadratics, the minimizers of these objectives exist and are unique.\n\nDefine catastrophic forgetting as the increase in cumulative past-task loss when adapting to the new task:\n$$\n\\Delta(\\boldsymbol{\\theta}_\\star) = \\left(\\sum_{t=1}^{T_{\\text{past}}} L_t(\\boldsymbol{\\theta}_\\star)\\right) - \\left(\\sum_{t=1}^{T_{\\text{past}}} L_t(\\boldsymbol{\\theta}_{\\text{prev}})\\right),\n$$\nwhere $\\boldsymbol{\\theta}_\\star$ is the parameter after learning the new task under a specified objective. Compute how replay reduces catastrophic forgetting by evaluating the difference\n$$\nR = \\Delta(\\boldsymbol{\\theta}_\\star^{\\text{no-replay}}) - \\Delta(\\boldsymbol{\\theta}_\\star^{\\text{replay}}),\n$$\nwith larger $R$ indicating greater reduction.\n\nYour program must implement the closed-form solutions for the minimizers of the quadratic objectives and compute $R$ for each test case. No physical units are involved. Angles are not involved. Express all numerical outputs as floats rounded to six decimal places.\n\nUse $d=3$, $T_{\\text{past}}=2$, and the following parameter values across the test suite. Vectors are given as row arrays and diagonal matrices are specified by their diagonal entries.\n\nShared across all test cases unless explicitly overridden:\n- $\\boldsymbol{\\mu}_1 = [1.0,-1.0,0.5]$\n- $\\boldsymbol{\\mu}_2 = [0.0,1.0,-0.5]$\n- $\\boldsymbol{\\mu}_{\\text{new}} = [1.5,0.5,0.0]$\n- $\\operatorname{diag}(A_1) = [1.0,0.5,2.0]$\n- $\\operatorname{diag}(A_2) = [0.8,1.2,1.5]$\n- $\\operatorname{diag}(A_{\\text{new}}) = [1.0,1.0,1.0]$\n\nDefine $\\boldsymbol{\\theta}_{\\text{prev}}$ as the unique minimizer of $L_1(\\boldsymbol{\\theta}) + L_2(\\boldsymbol{\\theta})$.\n\nThe test suite consists of $5$ cases:\n- Case $1$ (happy path): $\\lambda = 2.0$, $\\alpha_1 = 0.5$, $\\alpha_2 = 0.5$, $\\operatorname{diag}(F) = [1.8,1.7,3.5]$.\n- Case $2$ (no consolidation, no replay): $\\lambda = 0.0$, $\\alpha_1 = 0.0$, $\\alpha_2 = 0.0$, $\\operatorname{diag}(F) = [1.8,1.7,3.5]$.\n- Case $3$ (strong replay, weak consolidation): $\\lambda = 0.1$, $\\alpha_1 = 5.0$, $\\alpha_2 = 5.0$, $\\operatorname{diag}(F) = [1.8,1.7,3.5]$.\n- Case $4$ (anisotropic importance, no replay): $\\lambda = 1.0$, $\\alpha_1 = 0.0$, $\\alpha_2 = 0.0$, $\\operatorname{diag}(F) = [0.1,0.1,100.0]$.\n- Case $5$ (near-singular new-task curvature, consolidation only): override $\\operatorname{diag}(A_{\\text{new}}) = [0.0,0.0,0.1]$, $\\lambda = 0.2$, $\\alpha_1 = 0.0$, $\\alpha_2 = 0.0$, $\\operatorname{diag}(F) = [0.2,0.2,0.2]$.\n\nAlgorithmic requirements:\n- Compute $\\boldsymbol{\\theta}_{\\text{prev}}$ once from $L_1 + L_2$.\n- For each case, form the objective without replay and with replay, solve for their unique minimizers in closed form, and evaluate $R$.\n- Use exact linear algebra for quadratic minimization. All matrices are diagonal and positive semidefinite; ensure invertibility of the Hessian by the given parameters.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4,r_5]$), where each $r_i$ is the float $R$ for case $i$ rounded to six decimal places.",
            "solution": "The problem asks us to first provide a theoretical justification for a specific quadratic regularization objective used in continual learning, derived from Bayesian principles. Second, it requires the derivation of closed-form solutions for the optimal parameters under this objective and an extended version including experience replay. Finally, we are to implement these solutions to compute a measure of catastrophic forgetting reduction for a series of test cases.\n\n### Theoretical Justification of the Consolidation Objective\n\nThe goal is to justify an optimization objective for learning a new task, which penalizes changes to parameters important for past tasks. The proposed objective is:\n$$\nJ(\\boldsymbol{\\theta}) = L_{\\text{new}}(\\boldsymbol{\\theta}) + \\frac{\\lambda}{2}(\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_{\\text{prev}})^\\top F(\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_{\\text{prev}})\n$$\nThis form, central to methods like Elastic Weight Consolidation (EWC), can be derived from a Bayesian perspective.\n\n1.  **Bayesian Inference**: When learning a new task represented by data $D_{\\text{new}}$, we seek the posterior distribution of the parameters $\\boldsymbol{\\theta}$ using Bayes' theorem:\n    $$\n    p(\\boldsymbol{\\theta} | D_{\\text{new}}) = \\frac{p(D_{\\text{new}} | \\boldsymbol{\\theta}) p(\\boldsymbol{\\theta})}{p(D_{\\text{new}})} \\propto p(D_{\\text{new}} | \\boldsymbol{\\theta}) p(\\boldsymbol{\\theta})\n    $$\n    Here, $p(D_{\\text{new}} | \\boldsymbol{\\theta})$ is the likelihood of the new data given the parameters, and $p(\\boldsymbol{\\theta})$ is the prior distribution of the parameters.\n\n2.  **Maximum A Posteriori (MAP) Estimation**: The MAP estimate, $\\boldsymbol{\\theta}_{\\text{MAP}}$, is the parameter vector that maximizes the posterior. This is equivalent to minimizing the negative log-posterior:\n    $$\n    \\boldsymbol{\\theta}_{\\text{MAP}} = \\arg\\max_{\\boldsymbol{\\theta}} p(\\boldsymbol{\\theta} | D_{\\text{new}}) = \\arg\\min_{\\boldsymbol{\\theta}} \\left[ -\\log p(D_{\\text{new}} | \\boldsymbol{\\theta}) - \\log p(\\boldsymbol{\\theta}) \\right]\n    $$\n\n3.  **Likelihood and Prior in Continual Learning**:\n    -   The negative log-likelihood, $-\\log p(D_{\\text{new}} | \\boldsymbol{\\theta})$, is the error or loss function for the new task. Assuming the data likelihood follows a Gaussian distribution, this term becomes a squared-error loss. The problem defines this as the quadratic loss $L_{\\text{new}}(\\boldsymbol{\\theta})$.\n    -   In a continual learning context, the prior $p(\\boldsymbol{\\theta})$ should encapsulate the knowledge gained from previous tasks. A natural choice for this prior is the posterior distribution after learning the past tasks, $p(\\boldsymbol{\\theta} | D_{\\text{past}})$.\n\n4.  **Laplace Approximation**: The posterior distribution $p(\\boldsymbol{\\theta} | D_{\\text{past}})$ is generally intractable. The Laplace approximation approximates this posterior with a Gaussian distribution centered at its mode. The mode of the posterior is the MAP estimate for the past tasks. The problem defines this parameter vector as $\\boldsymbol{\\theta}_{\\text{prev}}$. The approximation is:\n    $$\n    p(\\boldsymbol{\\theta} | D_{\\text{past}}) \\approx \\mathcal{N}(\\boldsymbol{\\theta} | \\boldsymbol{\\theta}_{\\text{prev}}, H_{\\text{past}}^{-1})\n    $$\n    where $H_{\\text{past}}$ is the Hessian matrix of the negative log-posterior for the past tasks, evaluated at $\\boldsymbol{\\theta}_{\\text{prev}}$. The Hessian measures the curvature of the loss surface; a high curvature indicates a parameter is sensitive and important.\n\n5.  **Fisher Information Matrix**: Under certain regularity conditions and in the limit of large data, the Hessian of the negative log-likelihood can be approximated by the Fisher Information Matrix (FIM), $F$. Thus, we set $H_{\\text{past}} \\approx F$, where $F$ is the FIM for the past tasks. The problem specifies a diagonal FIM, which corresponds to an assumption that the parameter importances are independent.\n\n6.  **Constructing the Objective**: Using the Gaussian approximation for the prior, the negative log-prior term becomes:\n    $$\n    -\\log p(\\boldsymbol{\\theta}) \\approx -\\log \\mathcal{N}(\\boldsymbol{\\theta} | \\boldsymbol{\\theta}_{\\text{prev}}, F^{-1}) = \\frac{1}{2}(\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_{\\text{prev}})^\\top F (\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_{\\text{prev}}) + \\text{constant}\n    $$\n    Substituting the loss $L_{\\text{new}}(\\boldsymbol{\\theta})$ and this negative log-prior into the MAP objective, and dropping the constant, we get:\n    $$\n    \\arg\\min_{\\boldsymbol{\\theta}} \\left[ L_{\\text{new}}(\\boldsymbol{\\theta}) + \\frac{1}{2}(\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_{\\text{prev}})^\\top F (\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_{\\text{prev}}) \\right]\n    $$\n    The hyperparameter $\\lambda > 0$ is introduced to control the relative importance of the new task's loss versus the preservation of old knowledge (the prior). This can be viewed as adjusting the confidence in the prior. This leads directly to the specified objective function $J(\\boldsymbol{\\theta})$.\n\n### Derivation of Closed-Form Minimizers\n\nAll objectives are sums of quadratic forms, resulting in a total objective that is also a quadratic of the form $Q(\\boldsymbol{\\theta}) = \\frac{1}{2}\\boldsymbol{\\theta}^{\\top} H \\boldsymbol{\\theta} - \\boldsymbol{b}^{\\top}\\boldsymbol{\\theta} + c$. Such a function is minimized by solving $\\nabla Q(\\boldsymbol{\\theta}) = H\\boldsymbol{\\theta} - \\boldsymbol{b} = \\boldsymbol{0}$, which gives the solution $\\boldsymbol{\\theta}_{\\star} = H^{-1}\\boldsymbol{b}$, provided the Hessian $H$ is invertible. Given that all matrices $A_t$ and $F$ are positive semidefinite, the Hessians will be at least positive semidefinite. The problem parameters ensure they are in fact positive definite, guaranteeing unique minimizers.\n\nThe loss for task $t$ is $L_t(\\boldsymbol{\\theta}) = \\frac{1}{2}(\\boldsymbol{\\theta} - \\boldsymbol{\\mu}_t)^\\top A_t(\\boldsymbol{\\theta} - \\boldsymbol{\\mu}_t)$. Its gradient is $\\nabla L_t(\\boldsymbol{\\theta}) = A_t(\\boldsymbol{\\theta} - \\boldsymbol{\\mu}_t)$.\n\n1.  **Minimizer of Past Tasks, $\\boldsymbol{\\theta}_{\\text{prev}}$**:\n    $\\boldsymbol{\\theta}_{\\text{prev}}$ minimizes the cumulative past loss $L_{\\text{past}}(\\boldsymbol{\\theta}) = \\sum_{t=1}^{T_{\\text{past}}} L_t(\\boldsymbol{\\theta})$.\n    Setting the gradient to zero:\n    $$\n    \\nabla L_{\\text{past}}(\\boldsymbol{\\theta}) = \\sum_{t=1}^{T_{\\text{past}}} A_t(\\boldsymbol{\\theta} - \\boldsymbol{\\mu}_t) = \\left(\\sum_{t=1}^{T_{\\text{past}}} A_t\\right)\\boldsymbol{\\theta} - \\sum_{t=1}^{T_{\\text{past}}} A_t\\boldsymbol{\\mu}_t = \\boldsymbol{0}\n    $$\n    Solving for $\\boldsymbol{\\theta}$:\n    $$\n    \\boldsymbol{\\theta}_{\\text{prev}} = \\left(\\sum_{t=1}^{T_{\\text{past}}} A_t\\right)^{-1} \\left(\\sum_{t=1}^{T_{\\text{past}}} A_t\\boldsymbol{\\mu}_t\\right)\n    $$\n    For $T_{\\text{past}}=2$, this is $\\boldsymbol{\\theta}_{\\text{prev}} = (A_1 + A_2)^{-1} (A_1\\boldsymbol{\\mu}_1 + A_2\\boldsymbol{\\mu}_2)$.\n\n2.  **Minimizer without Replay, $\\boldsymbol{\\theta}_\\star^{\\text{no-replay}}$**:\n    This is the minimizer of $J(\\boldsymbol{\\theta}) = L_{\\text{new}}(\\boldsymbol{\\theta}) + \\frac{\\lambda}{2}(\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_{\\text{prev}})^\\top F(\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_{\\text{prev}})$.\n    Setting the gradient to zero:\n    $$\n    \\nabla J(\\boldsymbol{\\theta}) = A_{\\text{new}}(\\boldsymbol{\\theta} - \\boldsymbol{\\mu}_{\\text{new}}) + \\lambda F (\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_{\\text{prev}}) = (A_{\\text{new}} + \\lambda F)\\boldsymbol{\\theta} - (A_{\\text{new}}\\boldsymbol{\\mu}_{\\text{new}} + \\lambda F\\boldsymbol{\\theta}_{\\text{prev}}) = \\boldsymbol{0}\n    $$\n    Solving for $\\boldsymbol{\\theta}$:\n    $$\n    \\boldsymbol{\\theta}_\\star^{\\text{no-replay}} = (A_{\\text{new}} + \\lambda F)^{-1} (A_{\\text{new}}\\boldsymbol{\\mu}_{\\text{new}} + \\lambda F\\boldsymbol{\\theta}_{\\text{prev}})\n    $$\n\n3.  **Minimizer with Replay, $\\boldsymbol{\\theta}_\\star^{\\text{replay}}$**:\n    This is the minimizer of $J_{\\text{replay}}(\\boldsymbol{\\theta}) = L_{\\text{new}}(\\boldsymbol{\\theta}) + \\sum_{t=1}^{T_{\\text{past}}} \\alpha_t L_t(\\boldsymbol{\\theta}) + \\frac{\\lambda}{2}(\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_{\\text{prev}})^\\top F(\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_{\\text{prev}})$.\n    Setting the gradient to zero:\n    $$\n    \\nabla J_{\\text{replay}}(\\boldsymbol{\\theta}) = A_{\\text{new}}(\\boldsymbol{\\theta} - \\boldsymbol{\\mu}_{\\text{new}}) + \\sum_{t=1}^{T_{\\text{past}}} \\alpha_t A_t(\\boldsymbol{\\theta} - \\boldsymbol{\\mu}_t) + \\lambda F(\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_{\\text{prev}}) = \\boldsymbol{0}\n    $$\n    Let $H_{\\text{replay}} = A_{\\text{new}} + \\sum_{t=1}^{T_{\\text{past}}} \\alpha_t A_t + \\lambda F$ and $\\boldsymbol{b}_{\\text{replay}} = A_{\\text{new}}\\boldsymbol{\\mu}_{\\text{new}} + \\sum_{t=1}^{T_{\\text{past}}} \\alpha_t A_t\\boldsymbol{\\mu}_t + \\lambda F\\boldsymbol{\\theta}_{\\text{prev}}$.\n    The gradient equation is $H_{\\text{replay}}\\boldsymbol{\\theta} - \\boldsymbol{b}_{\\text{replay}} = \\boldsymbol{0}$. Solving for $\\boldsymbol{\\theta}$:\n    $$\n    \\boldsymbol{\\theta}_\\star^{\\text{replay}} = H_{\\text{replay}}^{-1} \\boldsymbol{b}_{\\text{replay}}\n    $$\n    Since all matrices ($A_t$, $F$) are diagonal, their sums and inverses are also diagonal. This significantly simplifies computation, as the $d$-dimensional problem decouples into $d$ independent scalar equations.\n\n### Calculation of Forgetting Reduction $R$\n\nCatastrophic forgetting is quantified by the increase in the cumulative loss on past tasks after adapting to the new task:\n$$\n\\Delta(\\boldsymbol{\\theta}_\\star) = \\left(\\sum_{t=1}^{T_{\\text{past}}} L_t(\\boldsymbol{\\theta}_\\star)\\right) - \\left(\\sum_{t=1}^{T_{\\text{past}}} L_t(\\boldsymbol{\\theta}_{\\text{prev}})\\right)\n$$\nThe reduction in forgetting due to replay, $R$, is the difference between this value for the no-replay and replay solutions:\n$$\nR = \\Delta(\\boldsymbol{\\theta}_\\star^{\\text{no-replay}}) - \\Delta(\\boldsymbol{\\theta}_\\star^{\\text{replay}})\n$$\nSubstituting the definition of $\\Delta$ and cancelling the common term $-\\sum L_t(\\boldsymbol{\\theta}_{\\text{prev}})$:\n$$\nR = \\left(\\sum_{t=1}^{T_{\\text{past}}} L_t(\\boldsymbol{\\theta}_\\star^{\\text{no-replay}})\\right) - \\left(\\sum_{t=1}^{T_{\\text{past}}} L_t(\\boldsymbol{\\theta}_\\star^{\\text{replay}})\\right)\n$$\nTo compute $R$ for each test case, we will:\n1.  Compute the shared parameter $\\boldsymbol{\\theta}_{\\text{prev}}$ once.\n2.  For each case, use the case-specific parameters ($\\lambda, \\alpha_t, F$, and potentially $A_{\\text{new}}$) to compute $\\boldsymbol{\\theta}_\\star^{\\text{no-replay}}$ and $\\boldsymbol{\\theta}_\\star^{\\text{replay}}$ using the derived closed-form solutions.\n3.  Calculate the cumulative past loss $\\sum_{t=1}^{2} L_t(\\boldsymbol{\\theta})$ for both $\\boldsymbol{\\theta}_\\star^{\\text{no-replay}}$ and $\\boldsymbol{\\theta}_\\star^{\\text{replay}}$.\n4.  Compute their difference to find $R$.\nThe loss for a given $\\boldsymbol{\\theta}$ is computed as $L_t(\\boldsymbol{\\theta}) = \\frac{1}{2}(\\boldsymbol{\\theta} - \\boldsymbol{\\mu}_t)^\\top A_t(\\boldsymbol{\\theta} - \\boldsymbol{\\mu}_t)$.\nThe implementation will use these formulas directly.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the reduction in catastrophic forgetting due to experience replay\n    for a set of test cases.\n    \"\"\"\n    # Shared parameters\n    mu1 = np.array([1.0, -1.0, 0.5])\n    mu2 = np.array([0.0, 1.0, -0.5])\n    munew_base = np.array([1.5, 0.5, 0.0])\n\n    diag_A1 = np.array([1.0, 0.5, 2.0])\n    diag_A2 = np.array([0.8, 1.2, 1.5])\n    diag_Anew_base = np.array([1.0, 1.0, 1.0])\n    \n    A1 = np.diag(diag_A1)\n    A2 = np.diag(diag_A2)\n\n    # Test cases\n    test_cases = [\n        # Case 1: happy path\n        {'lambda': 2.0, 'alpha1': 0.5, 'alpha2': 0.5, 'diag_F': np.array([1.8, 1.7, 3.5])},\n        # Case 2: no consolidation, no replay\n        {'lambda': 0.0, 'alpha1': 0.0, 'alpha2': 0.0, 'diag_F': np.array([1.8, 1.7, 3.5])},\n        # Case 3: strong replay, weak consolidation\n        {'lambda': 0.1, 'alpha1': 5.0, 'alpha2': 5.0, 'diag_F': np.array([1.8, 1.7, 3.5])},\n        # Case 4: anisotropic importance, no replay\n        {'lambda': 1.0, 'alpha1': 0.0, 'alpha2': 0.0, 'diag_F': np.array([0.1, 0.1, 100.0])},\n        # Case 5: near-singular new-task curvature, consolidation only\n        {'lambda': 0.2, 'alpha1': 0.0, 'alpha2': 0.0, 'diag_F': np.array([0.2, 0.2, 0.2]), \n         'diag_Anew': np.array([0.0, 0.0, 0.1])},\n    ]\n\n    # --- Step 1: Compute theta_prev, the minimizer of L1 + L2\n    # H_prev * theta_prev = b_prev\n    # (A1 + A2) * theta_prev = A1*mu1 + A2*mu2\n    H_prev_inv = np.diag(1.0 / (diag_A1 + diag_A2))\n    b_prev = A1 @ mu1 + A2 @ mu2\n    theta_prev = H_prev_inv @ b_prev\n\n    results = []\n\n    # Helper function to compute loss\n    def compute_task_loss(theta, mu, A):\n        diff = theta - mu\n        return 0.5 * diff.T @ A @ diff\n\n    for case in test_cases:\n        lam = case['lambda']\n        alpha1 = case['alpha1']\n        alpha2 = case['alpha2']\n        diag_F = case['diag_F']\n        F = np.diag(diag_F)\n\n        # Handle overrides for specific cases\n        if 'diag_Anew' in case:\n            diag_Anew = case['diag_Anew']\n        else:\n            diag_Anew = diag_Anew_base\n        Anew = np.diag(diag_Anew)\n        munew = munew_base # Does not change in test cases\n\n        # --- Step 2: Compute theta_star_no_replay\n        # (A_new + lambda*F) * theta = A_new*mu_new + lambda*F*theta_prev\n        diag_H_no_replay = diag_Anew + lam * diag_F\n        # Add a small epsilon to avoid division by zero, although problem setup avoids it.\n        diag_H_no_replay[diag_H_no_replay == 0] = 1e-12 \n        H_no_replay_inv = np.diag(1.0 / diag_H_no_replay)\n        \n        b_no_replay = Anew @ munew + lam * F @ theta_prev\n        theta_star_no_replay = H_no_replay_inv @ b_no_replay\n        \n        # --- Step 3: Compute theta_star_replay\n        # (A_new + a1*A1 + a2*A2 + lambda*F) * theta = A_new*mu_new + a1*A1*mu1 + a2*A2*mu2 + lambda*F*theta_prev\n        diag_H_replay = diag_Anew + alpha1 * diag_A1 + alpha2 * diag_A2 + lam * diag_F\n        diag_H_replay[diag_H_replay == 0] = 1e-12\n        H_replay_inv = np.diag(1.0 / diag_H_replay)\n\n        b_replay = Anew @ munew + alpha1 * A1 @ mu1 + alpha2 * A2 @ mu2 + lam * F @ theta_prev\n        theta_star_replay = H_replay_inv @ b_replay\n\n        # --- Step 4: Calculate R\n        past_loss_no_replay = compute_task_loss(theta_star_no_replay, mu1, A1) + \\\n                              compute_task_loss(theta_star_no_replay, mu2, A2)\n        \n        past_loss_replay = compute_task_loss(theta_star_replay, mu1, A1) + \\\n                           compute_task_loss(theta_star_replay, mu2, A2)\n\n        R = past_loss_no_replay - past_loss_replay\n        results.append(f\"{R:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}