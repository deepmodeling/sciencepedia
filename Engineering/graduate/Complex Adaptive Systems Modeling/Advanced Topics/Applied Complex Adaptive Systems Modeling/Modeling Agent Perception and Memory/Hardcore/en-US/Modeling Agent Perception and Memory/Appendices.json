{
    "hands_on_practices": [
        {
            "introduction": "Before an agent can reason about its environment, it must possess a reliable model of its own perceptual hardware. This first practice focuses on a fundamental task in robotics and agent design: calibrating a sensor to estimate its noise characteristics from empirical data. By working through the derivation of the Maximum Likelihood Estimator (MLE) for a sensor's covariance matrix, you will ground abstract statistical theory in a critical, hands-on application that forms the bedrock of reliable perception. ",
            "id": "4128168",
            "problem": "An agent in a complex adaptive system uses a $p$-dimensional sensor to perceive environmental features. During calibration, the agent observes $J$ static calibration targets, indexed by $j \\in \\{1,\\dots,J\\}$, each with a known latent state $\\mathbf{s}_j$ and a known forward observation mapping $h(\\cdot)$ that predicts a noise-free measurement $\\boldsymbol{\\mu}_j = h(\\mathbf{s}_j) \\in \\mathbb{R}^p$. For target $j$, the agent collects $n_j$ repeated measurements $\\{\\mathbf{z}_{j,k}\\}_{k=1}^{n_j}$, which are modeled as independent draws from a multivariate normal distribution with mean $\\boldsymbol{\\mu}_j$ and unknown symmetric positive definite covariance matrix $R \\in \\mathbb{R}^{p \\times p}$, i.e., $\\mathbf{z}_{j,k} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_j, R)$, independently over $j$ and $k$. Define residuals $\\mathbf{r}_{j,k} = \\mathbf{z}_{j,k} - \\boldsymbol{\\mu}_j$ and let the total sample size be $N = \\sum_{j=1}^J n_j$.\n\nStarting from the definition of the likelihood under multivariate normality and the definition of the Maximum Likelihood Estimator (MLE), derive the estimator $\\hat{R}$ that calibrates the sensor noise covariance from these repeated measurements. Then, quantify the uncertainty in $\\hat{R}$ using the asymptotic normality of the MLE by expressing the limiting distribution of $\\sqrt{N}\\,\\mathrm{vec}(\\hat{R} - R)$, where $\\mathrm{vec}(\\cdot)$ denotes vectorization. In your derivation, use the following foundational facts:\n- The multivariate normal probability density function for $\\mathbf{z} \\in \\mathbb{R}^p$ with mean $\\boldsymbol{\\mu} \\in \\mathbb{R}^p$ and covariance $R \\in \\mathbb{R}^{p \\times p}$ is $f(\\mathbf{z}; \\boldsymbol{\\mu}, R) = \\frac{1}{(2\\pi)^{p/2} |R|^{1/2}} \\exp\\left(-\\frac{1}{2} (\\mathbf{z} - \\boldsymbol{\\mu})^\\top R^{-1} (\\mathbf{z} - \\boldsymbol{\\mu})\\right)$.\n- The Maximum Likelihood Estimator (MLE) is obtained by maximizing the joint likelihood over independent samples.\n- Under regularity conditions, the MLE is consistent and asymptotically normal with covariance given by the inverse of the Fisher information, where Fisher information is defined as $I(\\theta) = \\mathbb{E}\\left[-\\frac{\\partial^2 \\ell(\\theta)}{\\partial \\theta \\partial \\theta^\\top}\\right]$ and $\\ell(\\theta)$ is the log-likelihood.\n\nFor the asymptotic covariance, you may express the result using standard linear-algebraic operators:\n- $\\mathrm{vec}(\\cdot)$ is the vectorization operator.\n- $\\otimes$ is the Kronecker product.\n- $K_p \\in \\mathbb{R}^{p^2 \\times p^2}$ is the commutation matrix defined by $K_p\\,\\mathrm{vec}(A) = \\mathrm{vec}(A^\\top)$ for any $A \\in \\mathbb{R}^{p \\times p}$.\n- $\\mathrm{vech}(\\cdot)$ is the half-vectorization that stacks the upper-triangular elements of a symmetric matrix.\n- $D_p \\in \\mathbb{R}^{p^2 \\times p(p+1)/2}$ is the duplication matrix satisfying $\\mathrm{vec}(S) = D_p\\,\\mathrm{vech}(S)$ for any symmetric $S \\in \\mathbb{R}^{p \\times p}$.\n\nFinally, identify the single option that correctly states both the MLE $\\hat{R}$ and a valid asymptotic covariance for $\\mathrm{vec}(\\hat{R})$ (or equivalently, for $\\mathrm{vech}(\\hat{R})$ via the duplication matrix). Select exactly one option.\n\nA. $\\hat{R} = \\frac{1}{N} \\sum_{j=1}^{J} \\sum_{k=1}^{n_j} \\mathbf{r}_{j,k}\\,\\mathbf{r}_{j,k}^\\top$, and $\\sqrt{N}\\,\\mathrm{vec}(\\hat{R} - R) \\xrightarrow{d} \\mathcal{N}\\!\\left(\\mathbf{0},\\,(I_p + K_p)\\,(R \\otimes R)\\right)$; equivalently, for $\\mathrm{vech}$, the asymptotic covariance is $I_{\\mathrm{sym}}^{-1}$ where $I_{\\mathrm{sym}} = \\frac{N}{2} D_p^\\top (R^{-1} \\otimes R^{-1}) D_p$, so an approximate $(1-\\alpha)$ confidence ellipsoid for $\\mathrm{vech}(R)$ is $\\left\\{\\theta \\in \\mathbb{R}^{p(p+1)/2} : (\\theta - \\mathrm{vech}(\\hat{R}))^\\top I_{\\mathrm{sym}}\\,(\\theta - \\mathrm{vech}(\\hat{R})) \\leq \\chi^2_{p(p+1)/2,\\,1-\\alpha}\\right\\}$.\n\nB. $\\hat{R} = \\frac{1}{N-1} \\sum_{j=1}^{J} \\sum_{k=1}^{n_j} \\mathbf{r}_{j,k}\\,\\mathbf{r}_{j,k}^\\top$, and $\\sqrt{N}\\,\\mathrm{vec}(\\hat{R} - R) \\xrightarrow{d} \\mathcal{N}\\!\\left(\\mathbf{0},\\, (R^{-1} \\otimes R^{-1})\\right)$; thus the asymptotic covariance of $\\mathrm{vec}(\\hat{R})$ is $\\frac{1}{N}(R^{-1} \\otimes R^{-1})$.\n\nC. $\\hat{R} = \\frac{1}{J} \\sum_{j=1}^{J} \\left(\\frac{1}{n_j} \\sum_{k=1}^{n_j} \\mathbf{r}_{j,k}\\,\\mathbf{r}_{j,k}^\\top\\right)$, and $\\sqrt{N}\\,\\mathrm{vec}(\\hat{R} - R) \\xrightarrow{d} \\mathcal{N}\\!\\left(\\mathbf{0},\\, \\frac{1}{\\sum_{j=1}^{J} n_j^2}\\,(I_p + K_p)\\,(R \\otimes R)\\right)$.\n\nD. $\\hat{R} = \\frac{1}{N} \\sum_{j=1}^{J} \\sum_{k=1}^{n_j} (\\mathbf{z}_{j,k} - \\bar{\\mathbf{z}})\\,(\\mathbf{z}_{j,k} - \\bar{\\mathbf{z}})^\\top$ with $\\bar{\\mathbf{z}} = \\frac{1}{N}\\sum_{j=1}^{J}\\sum_{k=1}^{n_j}\\mathbf{z}_{j,k}$, and $\\sqrt{N}\\,\\mathrm{vec}(\\hat{R} - R) \\xrightarrow{d} \\mathcal{N}\\!\\left(\\mathbf{0},\\, (R \\otimes R)\\right)$; hence the asymptotic covariance of $\\mathrm{vec}(\\hat{R})$ is $\\frac{1}{N}(R \\otimes R)$.",
            "solution": "### Step 1: Extract Givens\n\nThe problem statement provides the following information:\n- An agent uses a $p$-dimensional sensor.\n- There are $J$ static calibration targets, indexed by $j \\in \\{1,\\dots,J\\}$.\n- Each target $j$ has a known latent state $\\mathbf{s}_j$ and a known forward observation mapping $h(\\cdot)$.\n- The noise-free measurement (mean) for target $j$ is known and given by $\\boldsymbol{\\mu}_j = h(\\mathbf{s}_j) \\in \\mathbb{R}^p$.\n- For each target $j$, $n_j$ repeated measurements are collected: $\\{\\mathbf{z}_{j,k}\\}_{k=1}^{n_j}$.\n- The measurements are modeled as independent draws from a multivariate normal distribution: $\\mathbf{z}_{j,k} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_j, R)$.\n- The covariance matrix $R \\in \\mathbb{R}^{p \\times p}$ is unknown, symmetric, positive definite, and common to all measurements.\n- The samples are independent over both indices $j$ and $k$.\n- Residuals are defined as $\\mathbf{r}_{j,k} = \\mathbf{z}_{j,k} - \\boldsymbol{\\mu}_j$.\n- The total sample size is $N = \\sum_{j=1}^J n_j$.\n- The probability density function for $\\mathbf{z} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, R)$ is $f(\\mathbf{z}; \\boldsymbol{\\mu}, R) = \\frac{1}{(2\\pi)^{p/2} |R|^{1/2}} \\exp\\left(-\\frac{1}{2} (\\mathbf{z} - \\boldsymbol{\\mu})^\\top R^{-1} (\\mathbf{z} - \\boldsymbol{\\mu})\\right)$.\n- Standard definitions for MLE, Fisher information, and its relation to asymptotic normality are provided.\n- Standard matrix operators are defined: $\\mathrm{vec}(\\cdot)$, $\\otimes$, $K_p$, $\\mathrm{vech}(\\cdot)$, $D_p$.\n\n### Step 2: Validate Using Extracted Givens\n\n- **Scientifically Grounded**: The problem is firmly rooted in the theory of maximum likelihood estimation for parameters of a multivariate normal distribution. This is a fundamental and well-established topic in statistics. All concepts and tools mentioned are standard in this field. The setup is scientifically and mathematically sound.\n- **Well-Posed**: The problem is to derive a specific estimator ($\\hat{R}$) and its asymptotic properties. The givens are sufficient and consistent to perform this derivation. A unique MLE exists and its asymptotic properties are well-defined under standard regularity conditions, which are implicitly assumed by the reference to asymptotic normality.\n- **Objective**: The language is precise, mathematical, and free of any subjectivity or ambiguity.\n\nThe problem is a standard exercise in multivariate statistical inference. It is complete, consistent, and scientifically sound. No flaws are identified.\n\n### Step 3: Verdict and Action\n\nThe problem statement is **valid**. Proceeding with the solution.\n\n### Derivation of the MLE $\\hat{R}$\n\nThe set of all observations is $\\{\\mathbf{z}_{j,k}\\}_{j=1, k=1}^{J, n_j}$. Since the observations are independent, the joint likelihood function $L(R)$ is the product of the individual probability densities.\n$$ L(R) = \\prod_{j=1}^{J} \\prod_{k=1}^{n_j} f(\\mathbf{z}_{j,k}; \\boldsymbol{\\mu}_j, R) $$\nThe log-likelihood function $\\ell(R) = \\ln L(R)$ is:\n$$ \\ell(R) = \\sum_{j=1}^{J} \\sum_{k=1}^{n_j} \\ln f(\\mathbf{z}_{j,k}; \\boldsymbol{\\mu}_j, R) $$\nSubstituting the multivariate normal PDF:\n$$ \\ell(R) = \\sum_{j=1}^{J} \\sum_{k=1}^{n_j} \\left[ -\\frac{p}{2} \\ln(2\\pi) - \\frac{1}{2}\\ln|R| - \\frac{1}{2} (\\mathbf{z}_{j,k} - \\boldsymbol{\\mu}_j)^\\top R^{-1} (\\mathbf{z}_{j,k} - \\boldsymbol{\\mu}_j) \\right] $$\nLet $N = \\sum_{j=1}^J n_j$ be the total number of samples. Combining the terms:\n$$ \\ell(R) = -\\frac{Np}{2}\\ln(2\\pi) - \\frac{N}{2}\\ln|R| - \\frac{1}{2} \\sum_{j=1}^{J} \\sum_{k=1}^{n_j} (\\mathbf{z}_{j,k} - \\boldsymbol{\\mu}_j)^\\top R^{-1} (\\mathbf{z}_{j,k} - \\boldsymbol{\\mu}_j) $$\nUsing the trace property $\\mathbf{x}^\\top A \\mathbf{x} = \\mathrm{tr}(\\mathbf{x}^\\top A \\mathbf{x}) = \\mathrm{tr}(A\\mathbf{x}\\mathbf{x}^\\top)$ and the definition of the residual $\\mathbf{r}_{j,k} = \\mathbf{z}_{j,k} - \\boldsymbol{\\mu}_j$:\n$$ \\ell(R) = C - \\frac{N}{2}\\ln|R| - \\frac{1}{2} \\sum_{j=1}^{J} \\sum_{k=1}^{n_j} \\mathrm{tr}(R^{-1} \\mathbf{r}_{j,k}\\mathbf{r}_{j,k}^\\top) $$\n$$ \\ell(R) = C - \\frac{N}{2}\\ln|R| - \\frac{1}{2} \\mathrm{tr}\\left(R^{-1} \\left(\\sum_{j=1}^{J} \\sum_{k=1}^{n_j} \\mathbf{r}_{j,k}\\mathbf{r}_{j,k}^\\top\\right)\\right) $$\nLet $S_r = \\sum_{j=1}^{J} \\sum_{k=1}^{n_j} \\mathbf{r}_{j,k}\\mathbf{r}_{j,k}^\\top$. This is the matrix of sum of squares of the residuals.\nTo find the maximum, we differentiate $\\ell(R)$ with respect to $R$ and set the derivative to zero. Using standard matrix calculus derivatives $\\frac{\\partial \\ln|R|}{\\partial R} = (R^{-1})^\\top = R^{-1}$ and $\\frac{\\partial \\mathrm{tr}(AR^{-1})}{\\partial R} = -(R^{-1}AR^{-1})^\\top = -R^{-1}AR^{-1}$ (for symmetric $A, R$):\n$$ \\frac{\\partial \\ell(R)}{\\partial R} = -\\frac{N}{2}R^{-1} - \\frac{1}{2} (-R^{-1}S_r R^{-1}) = \\frac{1}{2}R^{-1}(S_r - NR)R^{-1} $$\nSetting the gradient to zero:\n$$ \\frac{1}{2}\\hat{R}^{-1}(S_r - N\\hat{R})\\hat{R}^{-1} = 0 $$\nSince $\\hat{R}^{-1}$ is invertible, this implies $S_r - N\\hat{R} = 0$, which gives the Maximum Likelihood Estimator (MLE):\n$$ \\hat{R} = \\frac{1}{N} S_r = \\frac{1}{N} \\sum_{j=1}^{J} \\sum_{k=1}^{n_j} \\mathbf{r}_{j,k}\\mathbf{r}_{j,k}^\\top $$\n\n### Asymptotic Distribution of $\\hat{R}$\n\nThe asymptotic distribution of an MLE is determined by the inverse of the Fisher information matrix. For an MLE $\\hat{\\theta}$ of a parameter vector $\\theta$, under regularity conditions, $\\sqrt{N}(\\hat{\\theta} - \\theta) \\xrightarrow{d} \\mathcal{N}(0, I_1(\\theta)^{-1})$, where $I_1(\\theta)$ is the Fisher information for a single observation.\n\nThe parameters of interest are the unique elements of the symmetric matrix $R$, denoted by $\\theta = \\mathrm{vech}(R)$, which is a vector of length $p(p+1)/2$.\nA standard result from multivariate analysis gives the Fisher information matrix for $\\mathrm{vech}(R)$ for a single observation from $\\mathcal{N}(\\boldsymbol{\\mu}, R)$ (with known $\\boldsymbol{\\mu}$) as:\n$$ I_1(\\mathrm{vech}(R)) = \\frac{1}{2} D_p^\\top (R^{-1} \\otimes R^{-1}) D_p $$\nwhere $D_p$ is the duplication matrix.\nThe asymptotic covariance of $\\sqrt{N}(\\mathrm{vech}(\\hat{R}) - \\mathrm{vech}(R))$ is thus $I_1(\\mathrm{vech}(R))^{-1}$.\n$$ \\mathrm{AsyCov}(\\sqrt{N}(\\mathrm{vech}(\\hat{R}) - \\mathrm{vech}(R))) = \\left( \\frac{1}{2} D_p^\\top (R^{-1} \\otimes R^{-1}) D_p \\right)^{-1} = 2 \\left( D_p^\\top (R^{-1} \\otimes R^{-1}) D_p \\right)^{-1} $$\nUsing the identity for the inverse of such a matrix, $(D_p^\\top(A \\otimes A)D_p)^{-1} = D_p^+(A^{-1} \\otimes A^{-1})(D_p^+)^\\top$, where $D_p^+ = (D_p^\\top D_p)^{-1}D_p^\\top$ is the Moore-Penrose inverse of $D_p$:\n$$ \\mathrm{AsyCov}(\\sqrt{N}(\\mathrm{vech}(\\hat{R}) - \\mathrm{vech}(R))) = 2 D_p^+ (R \\otimes R) (D_p^+)^\\top $$\nWe need the asymptotic covariance of $\\sqrt{N}(\\mathrm{vec}(\\hat{R}) - \\mathrm{vec}(R))$. We use the relationship $\\mathrm{vec}(A) = D_p \\mathrm{vech}(A)$ for any symmetric matrix $A$.\n$$ \\sqrt{N}(\\mathrm{vec}(\\hat{R}) - \\mathrm{vec}(R)) = \\sqrt{N} D_p (\\mathrm{vech}(\\hat{R}) - \\mathrm{vech}(R)) $$\nThe covariance of this transformed vector is:\n$$ \\mathrm{AsyCov}(\\sqrt{N}(\\mathrm{vec}(\\hat{R}) - \\mathrm{vec}(R))) = D_p \\left( 2 D_p^+ (R \\otimes R) (D_p^+)^\\top \\right) D_p^\\top $$\nUsing the identity $D_p D_p^+ = \\frac{1}{2}(I_{p^2} + K_p)$, where $K_p$ is the commutation matrix:\n$$ = 2 (D_p D_p^+) (R \\otimes R) (D_p D_p^+)^\\top $$\n$$ = 2 \\left( \\frac{1}{2}(I_{p^2} + K_p) \\right) (R \\otimes R) \\left( \\frac{1}{2}(I_{p^2} + K_p) \\right)^\\top $$\nSince $K_p$ is symmetric ($K_p^\\top=K_p$), this simplifies to:\n$$ = \\frac{1}{2} (I_{p^2} + K_p) (R \\otimes R) (I_{p^2} + K_p) $$\nFor symmetric matrices like $R$, it holds that $(R \\otimes R)K_p = K_p(R \\otimes R)$. Also, $K_p^2=I_{p^2}$.\n$$ = \\frac{1}{2} (I_{p^2} + K_p)^2 (R \\otimes R) = \\frac{1}{2} (I_{p^2} + 2K_p + K_p^2) (R \\otimes R) $$\n$$ = \\frac{1}{2} (2I_{p^2} + 2K_p) (R \\otimes R) = (I_{p^2} + K_p) (R \\otimes R) $$\nThus, the limiting distribution is:\n$$ \\sqrt{N}\\,\\mathrm{vec}(\\hat{R} - R) \\xrightarrow{d} \\mathcal{N}(\\mathbf{0}, (I_p + K_p)(R \\otimes R)) $$\n\n### Evaluation of Options\n\n- **Option A**:\n  - MLE: $\\hat{R} = \\frac{1}{N} \\sum_{j=1}^{J} \\sum_{k=1}^{n_j} \\mathbf{r}_{j,k}\\,\\mathbf{r}_{j,k}^\\top$. This matches our derived MLE.\n  - Asymptotic distribution: $\\sqrt{N}\\,\\mathrm{vec}(\\hat{R} - R) \\xrightarrow{d} \\mathcal{N}\\!\\left(\\mathbf{0},\\,(I_p + K_p)\\,(R \\otimes R)\\right)$. This matches our derived asymptotic covariance.\n  - Confidence ellipsoid: The total Fisher information for the sample of size $N$ for the parameter vector $\\theta = \\mathrm{vech}(R)$ is $I_N(\\theta) = N \\cdot I_1(\\theta)$. Using our earlier result for $I_1$:\n    $$ I_{\\mathrm{sym}} = I_N(\\mathrm{vech}(R)) = N \\left( \\frac{1}{2} D_p^\\top (R^{-1} \\otimes R^{-1}) D_p \\right) = \\frac{N}{2} D_p^\\top (R^{-1} \\otimes R^{-1}) D_p $$\n    This matches the definition of $I_{\\mathrm{sym}}$ in the option. The asymptotic distribution of $\\mathrm{vech}(\\hat{R})$ is $\\mathcal{N}(\\mathrm{vech}(R), I_{\\mathrm{sym}}^{-1})$. Using this, the quadratic form $(\\mathrm{vech}(\\hat{R}) - \\mathrm{vech}(R))^\\top I_{\\mathrm{sym}} (\\mathrm{vech}(\\hat{R}) - \\mathrm{vech}(R))$ is asymptotically distributed as $\\chi^2$ with $p(p+1)/2$ degrees of freedom. This forms the basis for the confidence ellipsoid for $\\mathrm{vech}(R)$. The expression given is correct for a $(1-\\alpha)$ confidence ellipsoid.\n  - Verdict: **Correct**.\n\n- **Option B**:\n  - MLE: $\\hat{R} = \\frac{1}{N-1} \\sum \\mathbf{r}_{j,k}\\,\\mathbf{r}_{j,k}^\\top$. The normalization factor is $N-1$, which is typical for an unbiased estimator when the mean is also estimated. Since the means $\\boldsymbol{\\mu}_j$ are known, the MLE has a factor of $N$. This estimator is incorrect.\n  - Asymptotic covariance: The form $\\frac{1}{N}(R^{-1} \\otimes R^{-1})$ is incorrect. The covariance of an estimator for $R$ must scale with powers of $R$, not $R^{-1}$.\n  - Verdict: **Incorrect**.\n\n- **Option C**:\n  - MLE: $\\hat{R} = \\frac{1}{J} \\sum_{j=1}^{J} \\left(\\frac{1}{n_j} \\sum_{k=1}^{n_j} \\mathbf{r}_{j,k}\\,\\mathbf{r}_{j,k}^\\top\\right)$. This is an unweighted average of per-target covariance estimates. The true MLE pools all $N$ data points, effectively weighting each per-target estimate by its sample size $n_j$. This estimator is only correct if all $n_j$ are equal. In the general case, it is not the MLE.\n  - Asymptotic covariance: The scaling factor is incorrect.\n  - Verdict: **Incorrect**.\n\n- **Option D**:\n  - MLE: $\\hat{R} = \\frac{1}{N} \\sum_{j,k} (\\mathbf{z}_{j,k} - \\bar{\\mathbf{z}})\\,(\\mathbf{z}_{j,k} - \\bar{\\mathbf{z}})^\\top$. This estimator incorrectly assumes that all observations share a single common mean, which it estimates as $\\bar{\\mathbf{z}}$. The problem states that the observations are from different populations with known, distinct means $\\boldsymbol{\\mu}_j$. This estimator is therefore incorrect.\n  - Asymptotic covariance: The expression $(R \\otimes R)$ is missing the $(I_p + K_p)$ factor, which is necessary to account for the symmetry of $R$.\n  - Verdict: **Incorrect**.\n\nBased on the derivations, only option A provides the correct MLE, the correct asymptotic distribution for its vectorized form, and a correct formulation for the corresponding confidence ellipsoid.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "With a calibrated sensor model, an agent can begin to interpret incoming signals. This exercise explores how an agent's prior beliefs—its memory of the world's statistical regularities—shape its perception and can make it more robust. By analyzing a scenario that combines a Gaussian likelihood with a heavy-tailed (Cauchy) prior, you will discover how certain priors can protect an agent's inference process from being skewed by outlier observations, a vital trait for navigating complex and unpredictable environments. ",
            "id": "4128032",
            "problem": "Consider an agent in a complex adaptive system whose perceptual inference about a latent scalar stimulus $s$ is modeled by Bayesian principles. The agent’s internal memory of environmental variability is represented by a heavy-tailed prior, specifically a Cauchy distribution with location $0$ and scale $\\gamma$, that is, $p(s) \\propto \\left[1 + \\left(s/\\gamma\\right)^{2}\\right]^{-1}$. The observation model is Gaussian with known variance $\\sigma^{2}$, that is, $p(o \\mid s) = \\mathcal{N}(o; s, \\sigma^{2})$. The agent uses either the Maximum a Posteriori (MAP) estimate $\\hat{s}_{\\mathrm{MAP}}(o)$ or the posterior mean estimate $\\hat{s}_{\\mathrm{PM}}(o) = \\mathbb{E}[s \\mid o]$ to form its perception.\n\nStarting only from Bayes’ rule and the definitions of MAP and posterior mean, derive the implicit equation that characterizes $\\hat{s}_{\\mathrm{MAP}}(o)$ by setting the derivative of the log-posterior with respect to $s$ equal to zero. Then, define the Influence Function (IF) with respect to input perturbations as $\\mathrm{IF}(o) = \\frac{\\partial \\hat{s}(o)}{\\partial o}$, and, using implicit differentiation for $\\hat{s}_{\\mathrm{MAP}}(o)$ together with appropriate calculus and tail-behavior arguments for $\\hat{s}_{\\mathrm{PM}}(o)$, analyze the robustness of each estimator to outliers in $o$. In particular, determine whether $\\mathrm{IF}(o)$ is bounded or unbounded for each estimator and characterize its asymptotic behavior as $\\lvert o \\rvert \\to \\infty$.\n\nWhich of the following statements are correct?\n\nA. The Influence Function of the Maximum a Posteriori estimator, $\\mathrm{IF}_{\\mathrm{MAP}}(o) = \\frac{\\partial \\hat{s}_{\\mathrm{MAP}}(o)}{\\partial o}$, is uniformly bounded in $o$ whenever $\\gamma^{2} > \\sigma^{2}/4$, with $\\sup_{o} \\mathrm{IF}_{\\mathrm{MAP}}(o) = \\left(1 - \\sigma^{2}/(4\\gamma^{2})\\right)^{-1}$; if $\\gamma^{2} \\le \\sigma^{2}/4$, $\\mathrm{IF}_{\\mathrm{MAP}}(o)$ can become unbounded.\n\nB. The Influence Function of the posterior mean estimator, $\\mathrm{IF}_{\\mathrm{PM}}(o) = \\frac{\\partial \\hat{s}_{\\mathrm{PM}}(o)}{\\partial o}$, equals $1$ for all $o$, independent of the heavy-tailed prior.\n\nC. As $\\lvert o \\rvert \\to \\infty$, both $\\mathrm{IF}_{\\mathrm{MAP}}(o)$ and $\\mathrm{IF}_{\\mathrm{PM}}(o)$ approach $1$, and both estimates satisfy $\\hat{s}(o) = o + \\mathcal{O}(1/o)$, meaning extreme outliers are transmitted asymptotically with unit gain.\n\nD. The posterior mean $\\hat{s}_{\\mathrm{PM}}(o)$ does not exist under a Cauchy prior because the Cauchy distribution has no finite first moment.\n\nE. Under a Cauchy prior and Gaussian likelihood, the Maximum a Posteriori estimator equals the posterior mean for all $o$.\n\nSelect all that apply.",
            "solution": "The problem asks for an analysis of the Maximum a Posteriori (MAP) and posterior mean (PM) estimators for a latent variable $s$ given an observation $o$. The model combines a heavy-tailed Cauchy prior for $s$ with a Gaussian likelihood for $o$ given $s$.\n\n**1. Givens and Model Specification**\n\nThe components of the Bayesian model are:\n- Latent variable: $s \\in \\mathbb{R}$\n- Observation: $o \\in \\mathbb{R}$\n- Prior distribution: $p(s) = \\frac{1}{\\pi\\gamma} \\frac{1}{1 + (s/\\gamma)^2} \\propto \\left[1 + \\left(\\frac{s}{\\gamma}\\right)^2\\right]^{-1}$. This is a Cauchy distribution with location $0$ and scale $\\gamma$.\n- Likelihood (observation model): $p(o \\mid s) = \\mathcal{N}(o; s, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(o-s)^2}{2\\sigma^2}\\right)$. This is a Gaussian distribution with mean $s$ and variance $\\sigma^2$.\n\n**2. Posterior Distribution and Estimators**\n\nAccording to Bayes' rule, the posterior distribution of $s$ given $o$ is proportional to the product of the likelihood and the prior:\n$$p(s \\mid o) \\propto p(o \\mid s) p(s)$$\n$$p(s \\mid o) \\propto \\exp\\left(-\\frac{(o-s)^2}{2\\sigma^2}\\right) \\left[1 + \\left(\\frac{s}{\\gamma}\\right)^2\\right]^{-1}$$\n\nThe two estimators to be analyzed are:\n- Maximum a Posteriori (MAP): $\\hat{s}_{\\mathrm{MAP}}(o) = \\arg\\max_{s} p(s \\mid o) = \\arg\\max_{s} \\log p(s \\mid o)$.\n- Posterior Mean (PM): $\\hat{s}_{\\mathrm{PM}}(o) = \\mathbb{E}[s \\mid o] = \\int_{-\\infty}^{\\infty} s \\, p(s \\mid o) \\, ds$.\n\n**3. Analysis of the MAP Estimator ($\\hat{s}_{\\mathrm{MAP}}$)**\n\nTo find the MAP estimate, we maximize the log-posterior. The log-posterior, up to an additive constant, is:\n$$L(s) = \\log p(s \\mid o) = C - \\frac{(o-s)^2}{2\\sigma^2} - \\log\\left(1 + \\frac{s^2}{\\gamma^2}\\right)$$\nWe find the maximum by setting the derivative with respect to $s$ to zero:\n$$\\frac{dL(s)}{ds} = \\frac{o-s}{\\sigma^2} - \\frac{1}{1+s^2/\\gamma^2} \\cdot \\frac{2s}{\\gamma^2} = \\frac{o-s}{\\sigma^2} - \\frac{2s}{\\gamma^2+s^2}$$\nSetting the derivative to zero for $s = \\hat{s}_{\\mathrm{MAP}}$ gives the implicit equation:\n$$\\frac{o-\\hat{s}_{\\mathrm{MAP}}}{\\sigma^2} = \\frac{2\\hat{s}_{\\mathrm{MAP}}}{\\gamma^2+\\hat{s}_{\\mathrm{MAP}}^2}$$\nThis can be rearranged to $o = \\hat{s}_{\\mathrm{MAP}} + \\frac{2\\sigma^2 \\hat{s}_{\\mathrm{MAP}}}{\\gamma^2+\\hat{s}_{\\mathrm{MAP}}^2}$. This is the required implicit equation.\n\nNow we analyze the Influence Function (IF) for the MAP estimator, $\\mathrm{IF}_{\\mathrm{MAP}}(o) = \\frac{\\partial \\hat{s}_{\\mathrm{MAP}}(o)}{\\partial o}$. We apply implicit differentiation to the rearranged equation, letting $\\hat{s} = \\hat{s}_{\\mathrm{MAP}}(o)$:\n$$1 = \\frac{d\\hat{s}}{do} + \\frac{d}{do}\\left(\\frac{2\\sigma^2 \\hat{s}}{\\gamma^2 + \\hat{s}^2}\\right) = \\frac{d\\hat{s}}{do} + \\frac{d}{d\\hat{s}}\\left(\\frac{2\\sigma^2 \\hat{s}}{\\gamma^2 + \\hat{s}^2}\\right) \\frac{d\\hat{s}}{do}$$\n$$1 = \\frac{d\\hat{s}}{do} \\left[ 1 + \\frac{(\\gamma^2+\\hat{s}^2)(2\\sigma^2) - (2\\sigma^2\\hat{s})(2\\hat{s})}{(\\gamma^2+\\hat{s}^2)^2} \\right]$$\n$$1 = \\frac{d\\hat{s}}{do} \\left[ 1 + \\frac{2\\sigma^2(\\gamma^2-\\hat{s}^2)}{(\\gamma^2+\\hat{s}^2)^2} \\right]$$\nThe Influence Function is therefore:\n$$\\mathrm{IF}_{\\mathrm{MAP}}(o) = \\frac{d\\hat{s}}{do} = \\left[ 1 + \\frac{2\\sigma^2(\\gamma^2-\\hat{s}^2)}{(\\gamma^2+\\hat{s}^2)^2} \\right]^{-1}$$\nThe IF becomes unbounded if the denominator can become zero. Let $u = \\hat{s}^2 \\ge 0$. The denominator is zero if $1 + \\frac{2\\sigma^2(\\gamma^2-u)}{(\\gamma^2+u)^2} = 0$, which is equivalent to the quadratic equation in $u$:\n$$u^2 + (2\\gamma^2 - 2\\sigma^2)u + (\\gamma^4 + 2\\sigma^2\\gamma^2) = 0$$\nThis equation has real roots for $u$ if its discriminant $\\Delta \\ge 0$:\n$$\\Delta = (2\\gamma^2-2\\sigma^2)^2 - 4(\\gamma^4+2\\sigma^2\\gamma^2) = 4\\sigma^2(\\sigma^2-4\\gamma^2)$$\nFor real roots, we need $\\sigma^2-4\\gamma^2 \\ge 0$, or $\\gamma^2 \\le \\sigma^2/4$. If this condition holds, there exist values of $\\hat{s}$ for which the IF is unbounded.\nIf $\\gamma^2 > \\sigma^2/4$, $\\Delta < 0$, the denominator is never zero and the IF is bounded. To find its supremum, we find the minimum of the denominator, $D(\\hat{s}) = 1 + \\frac{2\\sigma^2(\\gamma^2-\\hat{s}^2)}{(\\gamma^2+\\hat{s}^2)^2}$. The minimum occurs at $\\hat{s}^2=3\\gamma^2$, giving a minimum value of $D_{\\min} = 1 - \\sigma^2/(4\\gamma^2)$. The supremum of the IF is then $\\sup_o \\mathrm{IF}_{\\mathrm{MAP}}(o) = (D_{\\min})^{-1} = \\left(1 - \\frac{\\sigma^2}{4\\gamma^2}\\right)^{-1}$.\n\nAsymptotic behavior of $\\hat{s}_{\\mathrm{MAP}}$: As $|o| \\to \\infty$, we must have $|\\hat{s}| \\to \\infty$. From the implicit equation $o = \\hat{s} + \\frac{2\\sigma^2 \\hat{s}}{\\gamma^2+\\hat{s}^2}$, the second term behaves like $2\\sigma^2\\hat{s}/\\hat{s}^2 = \\mathcal{O}(1/\\hat{s})$. Thus, $\\hat{s} \\approx o$. Substituting this back gives $\\hat{s}(o) \\approx o - \\frac{2\\sigma^2}{o}$, which is of the form $\\hat{s}(o) = o + \\mathcal{O}(1/o)$.\nAsymptotic behavior of $\\mathrm{IF}_{\\mathrm{MAP}}$: As $|o| \\to \\infty$, $|\\hat{s}| \\to \\infty$. The term $\\frac{2\\sigma^2(\\gamma^2-\\hat{s}^2)}{(\\gamma^2+\\hat{s}^2)^2} \\to 0$. Thus, $\\lim_{|o|\\to\\infty} \\mathrm{IF}_{\\mathrm{MAP}}(o) = 1$.\n\n**4. Analysis of the Posterior Mean Estimator ($\\hat{s}_{\\mathrm{PM}}$)**\n\nThe posterior mean is $\\hat{s}_{\\mathrm{PM}}(o) = \\mathbb{E}[s \\mid o]$. Its existence is guaranteed because the posterior density $p(s|o)$ has tails that decay as $\\exp(-s^2/(2\\sigma^2))$ due to the Gaussian likelihood, which ensures that $\\int |s| p(s|o) ds < \\infty$.\n\nThe Influence Function for the PM estimator with a Gaussian likelihood has a known form (Tweedie's formula):\n$$\\mathrm{IF}_{\\mathrm{PM}}(o) = \\frac{\\partial \\hat{s}_{\\mathrm{PM}}(o)}{\\partial o} = \\frac{1}{\\sigma^2} \\mathrm{Var}(s \\mid o)$$\nwhere $\\mathrm{Var}(s \\mid o) = \\mathbb{E}[s^2 \\mid o] - (\\mathbb{E}[s \\mid o])^2$ is the posterior variance of $s$.\n\nAsymptotic behavior of $\\hat{s}_{\\mathrm{PM}}$: As $|o| \\to \\infty$, the likelihood term $p(o|s)$ becomes sharply peaked around $s=o$. The prior $p(s)$ is very flat in this region. This makes the posterior distribution $p(s|o)$ approximately Gaussian, $p(s \\mid o) \\approx \\mathcal{N}(s; o, \\sigma^2)$. Therefore, $\\mathbb{E}[s \\mid o] \\approx o$ and $\\mathrm{Var}(s \\mid o) \\approx \\sigma^2$. A more detailed expansion (as done for the MAP case, by expanding the prior term around the peak of the likelihood) shows that $\\hat{s}_{\\mathrm{PM}}(o) \\approx o - \\frac{2\\sigma^2}{o}$, which means $\\hat{s}_{\\mathrm{PM}}(o) = o + \\mathcal{O}(1/o)$.\nAsymptotic behavior of $\\mathrm{IF}_{\\mathrm{PM}}$: Using the asymptotic posterior variance, we find:\n$$\\lim_{|o|\\to\\infty} \\mathrm{IF}_{\\mathrm{PM}}(o) = \\lim_{|o|\\to\\infty} \\frac{\\mathrm{Var}(s \\mid o)}{\\sigma^2} = \\frac{\\sigma^2}{\\sigma^2} = 1$$\nSince $\\mathrm{Var}(s \\mid o)$ is always finite and positive for finite $\\sigma^2$, $\\mathrm{IF}_{\\mathrm{PM}}(o)$ is always bounded.\n\n**5. Evaluation of Options**\n\n**A. The Influence Function of the Maximum a Posteriori estimator, $\\mathrm{IF}_{\\mathrm{MAP}}(o) = \\frac{\\partial \\hat{s}_{\\mathrm{MAP}}(o)}{\\partial o}$, is uniformly bounded in $o$ whenever $\\gamma^{2} > \\sigma^{2}/4$, with $\\sup_{o} \\mathrm{IF}_{\\mathrm{MAP}}(o) = \\left(1 - \\sigma^{2}/(4\\gamma^{2})\\right)^{-1}$; if $\\gamma^{2} \\le \\sigma^{2}/4$, $\\mathrm{IF}_{\\mathrm{MAP}}(o)$ can become unbounded.**\nOur derivation in section 3 confirms every part of this statement. The condition for boundedness is correct, the formula for the supremum is correct, and the condition for being unbounded is correct.\n**Verdict: Correct.**\n\n**B. The Influence Function of the posterior mean estimator, $\\mathrm{IF}_{\\mathrm{PM}}(o) = \\frac{\\partial \\hat{s}_{\\mathrm{PM}}(o)}{\\partial o}$, equals $1$ for all $o$, independent of the heavy-tailed prior.**\nOur derivation shows $\\mathrm{IF}_{\\mathrm{PM}}(o) = \\mathrm{Var}(s|o)/\\sigma^2$. The posterior variance $\\mathrm{Var}(s|o)$ depends on the interplay between the prior and the likelihood and is generally not equal to $\\sigma^2$ (except in the limit of a flat prior or $|o|\\to\\infty$). For instance, for $o=0$, the prior pulls the posterior towards the origin, reducing its variance compared to the likelihood alone, so $\\mathrm{Var}(s|0) < \\sigma^2$ and $\\mathrm{IF}_{\\mathrm{PM}}(0) < 1$. Thus, the IF is not constant and not always equal to $1$.\n**Verdict: Incorrect.**\n\n**C. As $\\lvert o \\rvert \\to \\infty$, both $\\mathrm{IF}_{\\mathrm{MAP}}(o)$ and $\\mathrm{IF}_{\\mathrm{PM}}(o)$ approach $1$, and both estimates satisfy $\\hat{s}(o) = o + \\mathcal{O}(1/o)$, meaning extreme outliers are transmitted asymptotically with unit gain.**\nOur analysis in sections 3 and 4 showed that $\\lim_{|o|\\to\\infty} \\mathrm{IF}_{\\mathrm{MAP}}(o) = 1$ and $\\lim_{|o|\\to\\infty} \\mathrm{IF}_{\\mathrm{PM}}(o) = 1$. We also derived that for both estimators, $\\hat{s}(o) \\approx o - 2\\sigma^2/o$, which is of the form $o + \\mathcal{O}(1/o)$. An IF approaching $1$ and an estimate $\\hat{s}(o) \\approx o$ signifies that extreme observations are passed through with nearly unit gain.\n**Verdict: Correct.**\n\n**D. The posterior mean $\\hat{s}_{\\mathrm{PM}}(o)$ does not exist under a Cauchy prior because the Cauchy distribution has no finite first moment.**\nThis is a common misconception. While the *prior* mean $\\mathbb{E}[s]$ does not exist for a Cauchy distribution, the *posterior* mean $\\mathbb{E}[s \\mid o]$ does. The posterior density $p(s|o)$ decays exponentially for large $|s|$ due to the Gaussian likelihood term, which is sufficient to ensure the convergence of the integral $\\int_{-\\infty}^{\\infty} s p(s \\mid o) ds$. Thus, the posterior mean is well-defined.\n**Verdict: Incorrect.**\n\n**E. Under a Cauchy prior and Gaussian likelihood, the Maximum a Posteriori estimator equals the posterior mean for all $o$.**\nThe mean and mode of a distribution are equal if the distribution is symmetric. The posterior $p(s|o) \\propto \\exp(-\\frac{(o-s)^2}{2\\sigma^2}) [1 + (s/\\gamma)^2]^{-1}$ is the product of a Gaussian centered at $o$ and a Cauchy centered at $0$. For any $o \\neq 0$, the resulting distribution is asymmetric (skewed). Therefore, its mean and mode will not coincide. They only become equal in the special case $o=0$ (where both are $0$) and in the limit $|o|\\to\\infty$. The statement \"for all $o$\" is false.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{AC}$$"
        },
        {
            "introduction": "Agents in the real world almost never operate with a perfect internal model of their environment or their own sensory mechanisms. This final practice provides a concrete method for quantifying the mismatch between an agent's subjective beliefs and objective reality. By calculating the Kullback-Leibler (KL) divergence between an agent's posterior distribution (formed using a biased, misspecified model) and the true posterior, you will learn to measure the \"information cost\" of perceptual inaccuracies, a key concept in diagnosing and understanding agent failures. ",
            "id": "4128046",
            "problem": "Consider a single-agent Bayesian estimator in a complex adaptive system that must infer a latent stimulus $s \\in \\mathbb{R}$ from a single cue $y$. The agent maintains a memory-based prior over $s$ given by a Gaussian distribution with mean $m_{0}$ and variance $v_{0}$. The physical environment generates the cue according to a true likelihood that is Gaussian with mean $s$ and variance $\\sigma_{\\text{true}}^{2}$. However, due to perception bias and imperfect internal modeling, the agent uses a misspecified likelihood that is Gaussian with mean $s + b$ (a fixed shift bias) and variance $\\sigma_{\\text{mis}}^{2}$. You are given the following parameter values: $m_{0} = 0$, $v_{0} = 1$, $\\sigma_{\\text{true}}^{2} = 1$, $\\sigma_{\\text{mis}}^{2} = 4$, $b = 0.5$, and a realized cue $y = 1$.\n\nStarting only from Bayes’ rule and the definition of the Gaussian probability density function, first derive the true posterior $p_{\\text{T}}(s \\mid y)$ and the agent’s misspecified posterior $p_{\\text{A}}(s \\mid y)$ as Gaussian densities by completing the square, identifying their means and variances explicitly. Then, using the definition of the Kullback–Leibler divergence (KLD), compute the divergence from the true posterior to the agent’s posterior, $D_{\\mathrm{KL}}(p_{\\text{T}}(\\cdot \\mid y) \\,\\|\\, p_{\\text{A}}(\\cdot \\mid y))$, as a single scalar value.\n\nRound your final numerical answer to $4$ significant figures. Express the final result as a pure number without units.",
            "solution": "The problem requires the derivation of a true posterior and a misspecified posterior distribution, followed by the calculation of the Kullback-Leibler (KL) divergence between them. The analysis starts from Bayes' rule, which states that the posterior probability is proportional to the product of the likelihood and the prior: $p(s \\mid y) \\propto p(y \\mid s) p(s)$. Since all distributions involved are Gaussian, their product will also be Gaussian. A Gaussian probability density function (PDF) for a variable $x$ with mean $\\mu$ and variance $\\sigma^2$ is given by $\\mathcal{N}(x \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)$.\n\nWe can find the parameters of the posterior Gaussian distribution by examining the logarithm of the posterior, which will be a quadratic function of the latent variable $s$.\n$\\ln p(s \\mid y) = \\ln p(y \\mid s) + \\ln p(s) + \\text{constant}$.\nThe terms dependent on $s$ in the log-posterior have the form:\n$$ \\ln p(s \\mid y) = -\\frac{(s-\\mu_{\\text{post}})^2}{2v_{\\text{post}}} + C = -\\frac{1}{2v_{\\text{post}}}s^2 + \\frac{\\mu_{\\text{post}}}{v_{\\text{post}}}s - \\frac{\\mu_{\\text{post}}^2}{2v_{\\text{post}}} + C $$\nwhere $\\mu_{\\text{post}}$ and $v_{\\text{post}}$ are the posterior mean and variance, respectively. By combining the log-likelihood and log-prior and then matching the coefficients of $s^2$ and $s$, we can derive the posterior parameters.\n\nThe problem provides the prior as $p(s) = \\mathcal{N}(s \\mid m_0, v_0)$, where $m_0=0$ and $v_0=1$.\nThe log-prior is $\\ln p(s) = -\\frac{(s-m_0)^2}{2v_0} + C_1$.\n\nFirst, we derive the true posterior, $p_{\\text{T}}(s \\mid y)$.\nThe true likelihood is $p_{\\text{T}}(y \\mid s) = \\mathcal{N}(y \\mid s, \\sigma_{\\text{true}}^2)$, with $\\sigma_{\\text{true}}^2=1$.\nThe log-likelihood is $\\ln p_{\\text{T}}(y \\mid s) = -\\frac{(y-s)^2}{2\\sigma_{\\text{true}}^2} + C_2$.\nThe log-posterior is:\n$$ \\ln p_{\\text{T}}(s \\mid y) \\propto -\\frac{(s-m_0)^2}{2v_0} - \\frac{(y-s)^2}{2\\sigma_{\\text{true}}^2} $$\n$$ = -\\frac{1}{2}\\left( \\frac{s^2 - 2sm_0 + m_0^2}{v_0} + \\frac{s^2 - 2sy + y^2}{\\sigma_{\\text{true}}^2} \\right) $$\nGrouping terms by powers of $s$:\n$$ = -\\frac{1}{2}\\left[ s^2\\left(\\frac{1}{v_0} + \\frac{1}{\\sigma_{\\text{true}}^2}\\right) - 2s\\left(\\frac{m_0}{v_0} + \\frac{y}{\\sigma_{\\text{true}}^2}\\right) \\right] + \\text{constant} $$\nComparing this to the general form of a log-Gaussian, we identify the inverse of the true posterior variance, $v_{\\text{T}}$, and the mean, $m_{\\text{T}}$:\n$$ \\frac{1}{v_{\\text{T}}} = \\frac{1}{v_0} + \\frac{1}{\\sigma_{\\text{true}}^2} \\implies v_{\\text{T}} = \\left(\\frac{1}{v_0} + \\frac{1}{\\sigma_{\\text{true}}^2}\\right)^{-1} $$\n$$ \\frac{m_{\\text{T}}}{v_{\\text{T}}} = \\frac{m_0}{v_0} + \\frac{y}{\\sigma_{\\text{true}}^2} \\implies m_{\\text{T}} = v_{\\text{T}}\\left(\\frac{m_0}{v_0} + \\frac{y}{\\sigma_{\\text{true}}^2}\\right) $$\nSubstituting the given values $m_0=0$, $v_0=1$, $\\sigma_{\\text{true}}^2=1$, and $y=1$:\n$$ v_{\\text{T}} = \\left(\\frac{1}{1} + \\frac{1}{1}\\right)^{-1} = (2)^{-1} = 0.5 $$\n$$ m_{\\text{T}} = 0.5 \\left(\\frac{0}{1} + \\frac{1}{1}\\right) = 0.5(1) = 0.5 $$\nThus, the true posterior is $p_{\\text{T}}(s \\mid y) = \\mathcal{N}(s \\mid m_{\\text{T}}, v_{\\text{T}}) = \\mathcal{N}(s \\mid 0.5, 0.5)$.\n\nNext, we derive the agent's misspecified posterior, $p_{\\text{A}}(s \\mid y)$.\nThe agent uses the same prior $p(s) = \\mathcal{N}(s \\mid m_0, v_0)$ but a misspecified likelihood $p_{\\text{A}}(y \\mid s) = \\mathcal{N}(y \\mid s+b, \\sigma_{\\text{mis}}^2)$, with $b=0.5$ and $\\sigma_{\\text{mis}}^2=4$.\nThe log-likelihood used by the agent is $\\ln p_{\\text{A}}(y \\mid s) = -\\frac{(y-(s+b))^2}{2\\sigma_{\\text{mis}}^2} + C_3 = -\\frac{((y-b)-s)^2}{2\\sigma_{\\text{mis}}^2} + C_3$.\nThis has the same functional form as the true likelihood, but with the observation $y$ replaced by an \"effective observation\" $y' = y-b$ and variance $\\sigma_{\\text{true}}^2$ replaced by $\\sigma_{\\text{mis}}^2$.\nThe derivation structure is identical. The agent's posterior variance $v_{\\text{A}}$ and mean $m_{\\text{A}}$ are:\n$$ v_{\\text{A}} = \\left(\\frac{1}{v_0} + \\frac{1}{\\sigma_{\\text{mis}}^2}\\right)^{-1} $$\n$$ m_{\\text{A}} = v_{\\text{A}}\\left(\\frac{m_0}{v_0} + \\frac{y-b}{\\sigma_{\\text{mis}}^2}\\right) $$\nSubstituting the given values $m_0=0$, $v_0=1$, $\\sigma_{\\text{mis}}^2=4$, $b=0.5$, and $y=1$:\n$$ v_{\\text{A}} = \\left(\\frac{1}{1} + \\frac{1}{4}\\right)^{-1} = \\left(\\frac{5}{4}\\right)^{-1} = \\frac{4}{5} = 0.8 $$\n$$ m_{\\text{A}} = 0.8 \\left(\\frac{0}{1} + \\frac{1-0.5}{4}\\right) = 0.8 \\left(\\frac{0.5}{4}\\right) = 0.8(0.125) = 0.1 $$\nThus, the agent's posterior is $p_{\\text{A}}(s \\mid y) = \\mathcal{N}(s \\mid m_{\\text{A}}, v_{\\text{A}}) = \\mathcal{N}(s \\mid 0.1, 0.8)$.\n\nFinally, we compute the KL divergence from the true posterior to the agent's posterior, $D_{\\mathrm{KL}}(p_{\\text{T}} \\| p_{\\text{A}})$.\nFor two one-dimensional Gaussian distributions $p_1 = \\mathcal{N}(\\mu_1, v_1)$ and $p_2 = \\mathcal{N}(\\mu_2, v_2)$, the KL divergence is given by the formula:\n$$ D_{\\mathrm{KL}}(p_1 \\| p_2) = \\frac{1}{2} \\left( \\ln\\left(\\frac{v_2}{v_1}\\right) - 1 + \\frac{v_1}{v_2} + \\frac{(\\mu_1-\\mu_2)^2}{v_2} \\right) $$\nIn our case, $p_1$ is the true posterior $p_{\\text{T}}$ and $p_2$ is the agent's posterior $p_{\\text{A}}$. The parameters are:\n$\\mu_1 = m_{\\text{T}} = 0.5$, $v_1 = v_{\\text{T}} = 0.5$\n$\\mu_2 = m_{\\text{A}} = 0.1$, $v_2 = v_{\\text{A}} = 0.8$\n\nSubstituting these values into the KL divergence formula:\n$$ D_{\\mathrm{KL}}(p_{\\text{T}} \\| p_{\\text{A}}) = \\frac{1}{2} \\left( \\ln\\left(\\frac{0.8}{0.5}\\right) - 1 + \\frac{0.5}{0.8} + \\frac{(0.5-0.1)^2}{0.8} \\right) $$\nWe compute each term inside the parenthesis:\n$$ \\frac{v_{\\text{A}}}{v_{\\text{T}}} = \\frac{0.8}{0.5} = 1.6 $$\n$$ \\frac{v_{\\text{T}}}{v_{\\text{A}}} = \\frac{0.5}{0.8} = 0.625 $$\n$$ \\frac{(m_{\\text{T}}-m_{\\text{A}})^2}{v_{\\text{A}}} = \\frac{(0.4)^2}{0.8} = \\frac{0.16}{0.8} = 0.2 $$\nNow substitute these back into the expression for the divergence:\n$$ D_{\\mathrm{KL}}(p_{\\text{T}} \\| p_{\\text{A}}) = \\frac{1}{2} \\left( \\ln(1.6) - 1 + 0.625 + 0.2 \\right) $$\n$$ D_{\\mathrm{KL}}(p_{\\text{T}} \\| p_{\\text{A}}) = \\frac{1}{2} \\left( \\ln(1.6) - 0.175 \\right) $$\nUsing a calculator for the natural logarithm:\n$$ \\ln(1.6) \\approx 0.470003629 $$\n$$ D_{\\mathrm{KL}}(p_{\\text{T}} \\| p_{\\text{A}}) \\approx \\frac{1}{2} (0.470003629 - 0.175) = \\frac{1}{2} (0.295003629) \\approx 0.1475018145 $$\nThe problem requires the answer to be rounded to $4$ significant figures.\n$$ D_{\\mathrm{KL}}(p_{\\text{T}} \\| p_{\\text{A}}) \\approx 0.1475 $$\nThis value represents the information loss, measured in nats, when approximating the true posterior distribution with the agent's misspecified one.",
            "answer": "$$\\boxed{0.1475}$$"
        }
    ]
}