{
    "hands_on_practices": [
        {
            "introduction": "Many complex models are calibrated using Bayesian methods, where Markov Chain Monte Carlo (MCMC) algorithms are used to sample from the posterior distribution of model parameters. Before we can use these samples to quantify uncertainty, we must verify that the algorithm has converged to the target distribution. This practice guides you through implementing the essential Gelman-Rubin diagnostic, a cornerstone for assessing MCMC convergence, by comparing the variance within parallel chains to the variance between them to detect signs of non-convergence and non-stationarity .",
            "id": "4150998",
            "problem": "You are tasked with constructing a program that computes the univariate Gelman–Rubin convergence diagnostic (Gelman–Rubin is also known as the potential scale reduction factor) for multiple independent Markov chains used to approximate the posterior of a parameter in complex adaptive systems modeling. Your goal is to derive the diagnostic from first principles and implement two versions: the classical Gelman–Rubin statistic and the split-chain Gelman–Rubin statistic. You must also interpret reliability thresholds for both diagnostics.\n\nStart from the following fundamental base: sample means and sample variances as estimators of expected values and variances for independent and identically distributed draws, and the law of total variance. Suppose there are $N$ independent chains, each of length $M$, with values $\\{x_{j,t}\\}$ where $j \\in \\{1,\\dots,N\\}$ indexes the chain and $t \\in \\{1,\\dots,M\\}$ indexes time. Let the chain-specific sample mean be $m_{j} = \\frac{1}{M}\\sum_{t=1}^{M} x_{j,t}$, the chain-specific sample variance be $s_{j}^{2} = \\frac{1}{M-1}\\sum_{t=1}^{M} (x_{j,t} - m_{j})^{2}$, and the pooled mean be $\\bar{m} = \\frac{1}{N}\\sum_{j=1}^{N} m_{j}$. The between-chain variance is defined by\n$$\nB = \\frac{M}{N-1} \\sum_{j=1}^{N} (m_{j} - \\bar{m})^{2},\n$$\nand the within-chain variance is\n$$\nW = \\frac{1}{N} \\sum_{j=1}^{N} s_{j}^{2}.\n$$\nThe marginal posterior variance estimator is\n$$\n\\widehat{\\operatorname{Var}}^{+} = \\frac{M-1}{M}W + \\frac{1}{M}B,\n$$\nand the classical Gelman–Rubin statistic is\n$$\n\\hat{R} = \\sqrt{\\frac{\\widehat{\\operatorname{Var}}^{+}}{W}}.\n$$\nThe split-chain Gelman–Rubin statistic, denoted $\\hat{R}_{\\mathrm{split}}$, is obtained by splitting each chain into two halves of length $M/2$ (assuming $M$ is even), resulting in $2N$ chains, and then applying the same formulas with $M$ replaced by $M/2$ and $N$ replaced by $2N$ to compute $\\hat{R}_{\\mathrm{split}}$.\n\nYou must implement both $\\hat{R}$ and $\\hat{R}_{\\mathrm{split}}$ and interpret two reliability thresholds for uncertainty estimates:\n- a conventional threshold $\\tau_{1} = 1.05$, and\n- a stringent threshold $\\tau_{2} = 1.01$.\nFor each test case, report both $\\hat{R}$ and $\\hat{R}_{\\mathrm{split}}$ along with two boolean indicators: whether $\\hat{R}_{\\mathrm{split}} \\le \\tau_{1}$ and whether $\\hat{R}_{\\mathrm{split}} \\le \\tau_{2}$, which you should interpret as sufficient for reliable uncertainty estimates at the respective threshold levels.\n\nConstruct the following deterministic test suite of $5$ cases. For all cases, use zero-based time indexing $t \\in \\{0,1,\\dots,M-1\\}$ and define chain values by\n$$\nx_{j,t} = \\mu_{j} + A_{j} \\sin\\!\\left(2\\pi \\frac{k\\, t}{M} + \\phi_{j}\\right) + D_{j}\\left(2\\frac{t}{M} - 1\\right),\n$$\nwith parameters $(\\mu_{j}, A_{j}, \\phi_{j}, D_{j})$ specified below. Angles are in radians. All integer counts and constants must be used exactly as given.\n\nTest Suite:\n- Case $1$: $N=4$, $M=200$, $k=5$. Phases $\\phi_{j} \\in \\{0, \\frac{\\pi}{8}, \\frac{\\pi}{4}, \\frac{3\\pi}{8}\\}$ in chain index order. For all chains, $\\mu_{j}=0$, $A_{j}=1$, $D_{j}=0$.\n- Case $2$: $N=4$, $M=200$, $k=5$. Phases as in Case $1$. For chains $j \\in \\{1,2,3\\}$, $\\mu_{j}=0$, $A_{j}=1$, $D_{j}=0$. For chain $j=4$, $\\mu_{4}=1.5$, $A_{4}=1$, $D_{4}=0$.\n- Case $3$: $N=4$, $M=200$, $k=5$. Chains: \n  - $j=1$: $\\mu_{1}=0$, $A_{1}=1$, $\\phi_{1}=0$, $D_{1}=0$;\n  - $j=2$: $\\mu_{2}=0$, $A_{2}=1$, $\\phi_{2}=\\frac{\\pi}{3}$, $D_{2}=0$;\n  - $j=3$: $\\mu_{3}=0$, $A_{3}=0.5$, $\\phi_{3}=\\frac{\\pi}{7}$, $D_{3}=0.9$;\n  - $j=4$: $\\mu_{4}=0$, $A_{4}=0.5$, $\\phi_{4}=\\frac{\\pi}{5}$, $D_{4}=-0.9$.\n- Case $4$: $N=4$, $M=10$, $k=1$. Phases $\\phi_{j} \\in \\{0, \\frac{\\pi}{8}, \\frac{\\pi}{4}, \\frac{3\\pi}{8}\\}$. For all chains, $\\mu_{j}=0$, $A_{j}=1$, $D_{j}=0$.\n- Case $5$: $N=4$, $M=200$, $k=5$. Phases as in Case $1$. Chains $j \\in \\{1,2\\}$ use $A_{j}=1$ and chains $j \\in \\{3,4\\}$ use $A_{j}=3$. For all chains, $\\mu_{j}=0$, $D_{j}=0$.\n\nComputation and Output Requirements:\n- For each case, compute the classical Gelman–Rubin statistic $\\hat{R}$, the split-chain statistic $\\hat{R}_{\\mathrm{split}}$, and the two booleans indicating whether $\\hat{R}_{\\mathrm{split}} \\le \\tau_{1}$ and whether $\\hat{R}_{\\mathrm{split}} \\le \\tau_{2}$, with $\\tau_{1} = 1.05$ and $\\tau_{2} = 1.01$.\n- Your program should produce a single line of output containing the results as a comma-separated flat list enclosed in square brackets, ordered as $[\\hat{R}_{1}, \\hat{R}_{\\mathrm{split},1}, b_{1}^{(1.05)}, b_{1}^{(1.01)}, \\hat{R}_{2}, \\hat{R}_{\\mathrm{split},2}, b_{2}^{(1.05)}, b_{2}^{(1.01)}, \\dots, \\hat{R}_{5}, \\hat{R}_{\\mathrm{split},5}, b_{5}^{(1.05)}, b_{5}^{(1.01)}]$, where $\\hat{R}_{i}$ and $\\hat{R}_{\\mathrm{split},i}$ are floats and $b_{i}^{(1.05)}$, $b_{i}^{(1.01)}$ are booleans.\n- There are no physical units involved in this problem.",
            "solution": "The problem is subjected to validation before a solution is attempted.\n\n### Step 1: Extract Givens\n\n- **Objective**: Compute the classical Gelman–Rubin statistic ($\\hat{R}$) and the split-chain Gelman–Rubin statistic ($\\hat{R}_{\\mathrm{split}}$).\n- **Inputs**: $N$ independent Markov chains, each of length $M$. The data for a chain is $\\{x_{j,t}\\}$ for chain index $j \\in \\{1,\\dots,N\\}$ and time index $t \\in \\{1,\\dots,M\\}$.\n- **Definitions**:\n  - Chain-specific sample mean: $m_{j} = \\frac{1}{M}\\sum_{t=1}^{M} x_{j,t}$\n  - Chain-specific sample variance: $s_{j}^{2} = \\frac{1}{M-1}\\sum_{t=1}^{M} (x_{j,t} - m_{j})^{2}$\n  - Pooled mean: $\\bar{m} = \\frac{1}{N}\\sum_{j=1}^{N} m_{j}$\n- **Formulas**:\n  - Between-chain variance: $B = \\frac{M}{N-1} \\sum_{j=1}^{N} (m_{j} - \\bar{m})^{2}$\n  - Within-chain variance: $W = \\frac{1}{N} \\sum_{j=1}^{N} s_{j}^{2}$\n  - Marginal posterior variance estimator: $\\widehat{\\operatorname{Var}}^{+} = \\frac{M-1}{M}W + \\frac{1}{M}B$\n  - Classical Gelman–Rubin statistic: $\\hat{R} = \\sqrt{\\frac{\\widehat{\\operatorname{Var}}^{+}}{W}}$\n- **Split-Chain Statistic ($\\hat{R}_{\\mathrm{split}}$)**: Each chain of length $M$ is split into two halves of length $M/2$ (assuming $M$ is even). This results in $2N$ chains. The same formulas are applied with $N$ replaced by $2N$ and $M$ replaced by $M/2$.\n- **Reliability Thresholds**:\n  - Conventional threshold: $\\tau_{1} = 1.05$\n  - Stringent threshold: $\\tau_{2} = 1.01$\n- **Chain Data Generation**:\n  - Time indexing: $t \\in \\{0,1,\\dots,M-1\\}$\n  - Formula: $x_{j,t} = \\mu_{j} + A_{j} \\sin\\!\\left(2\\pi \\frac{k\\, t}{M} + \\phi_{j}\\right) + D_{j}\\left(2\\frac{t}{M} - 1\\right)$\n- **Test Suite**:\n  - Case 1: $N=4$, $M=200$, $k=5$. $\\phi_{j} \\in \\{0, \\frac{\\pi}{8}, \\frac{\\pi}{4}, \\frac{3\\pi}{8}\\}$. For all chains, $\\mu_{j}=0$, $A_{j}=1$, $D_{j}=0$.\n  - Case 2: $N=4$, $M=200$, $k=5$. $\\phi_{j}$ as in Case 1. For $j \\in \\{1,2,3\\}$: $\\mu_{j}=0, A_{j}=1, D_{j}=0$. For $j=4$: $\\mu_{4}=1.5, A_{4}=1, D_{4}=0$.\n  - Case 3: $N=4$, $M=200$, $k=5$.\n    - $j=1$: $\\mu_{1}=0, A_{1}=1, \\phi_{1}=0, D_{1}=0$.\n    - $j=2$: $\\mu_{2}=0, A_{2}=1, \\phi_{2}=\\frac{\\pi}{3}, D_{2}=0$.\n    - $j=3$: $\\mu_{3}=0, A_{3}=0.5, \\phi_{3}=\\frac{\\pi}{7}, D_{3}=0.9$.\n    - $j=4$: $\\mu_{4}=0, A_{4}=0.5, \\phi_{4}=\\frac{\\pi}{5}, D_{4}=-0.9$.\n  - Case 4: $N=4$, $M=10$, $k=1$. $\\phi_{j}$ as in Case 1. For all chains, $\\mu_{j}=0, A_{j}=1, D_{j}=0$.\n  - Case 5: $N=4$, $M=200$, $k=5$. $\\phi_{j}$ as in Case 1. For $j \\in \\{1,2\\}$: $A_{j}=1$. For $j \\in \\{3,4\\}$: $A_{j}=3$. For all chains, $\\mu_{j}=0, D_{j}=0$.\n- **Output Requirements**: A single-line, comma-separated flat list: $[\\hat{R}_{1}, \\hat{R}_{\\mathrm{split},1}, b_{1}^{(1.05)}, b_{1}^{(1.01)}, \\dots]$, where $b$ are booleans for $\\hat{R}_{\\mathrm{split}} \\le \\tau$.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem is reviewed against the validation criteria.\n\n1.  **Scientific or Factual Unsoundness**: The Gelman–Rubin diagnostic is a standard, well-established method in Bayesian statistics for assessing the convergence of Markov Chain Monte Carlo (MCMC) simulations. The formulas provided for $W$, $B$, $\\widehat{\\operatorname{Var}}^{+}$, and $\\hat{R}$ are correct and taken directly from statistical literature. The use of deterministic, sinusoidal test data instead of stochastic MCMC output is an abstraction, but it serves to create a well-defined numerical problem to test the implementation of the algorithm. This does not violate any scientific principle; it merely simplifies the data generation for the purpose of the exercise.\n2.  **Non-Formalizable or Irrelevant**: The problem is highly formalizable and directly pertains to uncertainty quantification, a core topic in modeling complex systems. The Gelman-Rubin statistic is a key tool for ensuring the reliability of posterior distributions obtained from MCMC, which are used to quantify parameter uncertainty.\n3.  **Incomplete or Contradictory Setup**: All parameters, constants, and formulas required for the computation are explicitly provided. The test cases are fully specified. There are no contradictions.\n4.  **Unrealistic or Infeasible**: The computations are numerically feasible. The parameters are within reasonable computational bounds.\n5.  **Ill-Posed or Poorly Structured**: The problem is well-posed. For the given deterministic inputs, a unique numerical solution exists. The definitions are unambiguous.\n6.  **Pseudo-Profound, Trivial, or Tautological**: The problem requires a careful and correct implementation of a non-trivial statistical algorithm. It involves several distinct computational steps based on established theory, making it a substantive task.\n7.  **Outside Scientific Verifiability**: The results are numerically deterministic and can be independently verified.\n\n### Step 3: Verdict and Action\n\nThe problem is **valid**. It is a well-defined, scientifically grounded computational problem. A complete solution will be provided.\n\n### Principle-Based Solution Design\n\nThe Gelman–Rubin convergence diagnostic, or potential scale reduction factor ($\\hat{R}$), is a fundamental tool for assessing the convergence of multiple Markov chains to a common stationary distribution. The underlying principle is to compare the variance within individual chains to the variance between the chains.\n\nLet there be $N$ parallel chains, each with $M$ post-warmup samples, $\\{x_{j,t}\\}$, where $j$ indexes the chain ($1, \\dots, N$) and $t$ indexes the sample ($1, \\dots, M$).\n\n1.  **Within-Chain Variance ($W$)**: We first compute the variance within each chain. The sample variance for chain $j$ is given by $s_{j}^{2} = \\frac{1}{M-1}\\sum_{t=1}^{M} (x_{j,t} - m_{j})^{2}$, where $m_{j}$ is the mean of chain $j$. The within-chain variance, $W = \\frac{1}{N} \\sum_{j=1}^{N} s_{j}^{2}$, is the average of these individual variances. In the early stages of a simulation, before chains have fully explored the target distribution, $W$ tends to be an underestimate of the true posterior variance.\n\n2.  **Between-Chain Variance ($B$)**: Next, we compute the variance between the means of the chains. With the pooled mean $\\bar{m} = \\frac{1}{N}\\sum_{j=1}^{N} m_{j}$, the between-chain variance is $B = \\frac{M}{N-1} \\sum_{j=1}^{N} (m_{j} - \\bar{m})^{2}$. The factor $M$ scales $B$ to be comparable to $W$. If chains have not converged, their means $m_j$ will be dispersed, leading to a large value of $B$. As they converge to the same distribution, their means will approach each other, and $B$ will approach zero.\n\n3.  **Posterior Variance Estimator ($\\widehat{\\operatorname{Var}}^{+}$)**: An estimate of the marginal posterior variance of the parameter is constructed by combining $W$ and $B$: $\\widehat{\\operatorname{Var}}^{+} = \\frac{M-1}{M}W + \\frac{1}{M}B$. This is a weighted average of the within-chain and between-chain variances. If the chains were started from an overdispersed distribution, $\\widehat{\\operatorname{Var}}^{+}$ is an overestimate of the true variance, which corrects for the underestimation provided by $W$ alone.\n\n4.  **Potential Scale Reduction Factor ($\\hat{R}$)**: The Gelman–Rubin statistic is the ratio of the overestimated variance to the underestimated variance: $\\hat{R} = \\sqrt{\\frac{\\widehat{\\operatorname{Var}}^{+}}{W}}$. As the simulation converges, the discrepancy between the chains vanishes, causing $B$ to become small. Consequently, $\\widehat{\\operatorname{Var}}^{+}$ approaches $W$, and $\\hat{R}$ approaches $1$. A value of $\\hat{R}$ substantially greater than $1$ indicates that the within-chain and between-chain variances are different, signaling a lack of convergence.\n\n5.  **Split-Chain Statistic ($\\hat{R}_{\\mathrm{split}}$)**: This variant addresses potential non-stationarity within chains. Each of the $N$ chains of length $M$ is split into two non-overlapping halves, creating a new set of $2N$ chains, each of length $M/2$. The $\\hat{R}$ statistic is then computed for this new set of chains. A value of $\\hat{R}_{\\mathrm{split}}$ near $1$ suggests that the first and second halves of the chains are statistically similar, providing evidence of stationarity.\n\n### Algorithmic Implementation\n\nA helper function will be designed to compute $\\hat{R}$ for a given set of chains, represented as a $2$D array. This function will be called twice for each test case: once with the original chains to compute $\\hat{R}$, and once with the split chains to compute $\\hat{R}_{\\mathrm{split}}$.\n\nFor each test case, the chain data $x_{j,t}$ will first be generated according to the specified deterministic formula. The main loop will iterate through the five test cases, performing the following steps for each:\n1.  Generate the $N \\times M$ matrix of chain data.\n2.  Calculate $\\hat{R}$ using the full matrix.\n3.  Split the matrix into two halves, creating a $2N \\times (M/2)$ matrix.\n4.  Calculate $\\hat{R}_{\\mathrm{split}}$ using the split matrix.\n5.  Evaluate the boolean conditions $\\hat{R}_{\\mathrm{split}} \\le \\tau_{1}$ (where $\\tau_1=1.05$) and $\\hat{R}_{\\mathrm{split}} \\le \\tau_{2}$ (where $\\tau_2=1.01$).\n6.  Collect the four resulting values: $\\hat{R}$, $\\hat{R}_{\\mathrm{split}}$, and the two booleans.\n\nThe final list of results from all test cases will be flattened and formatted into the required output string. All calculations will be performed using the `numpy` library for efficiency and correctness.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes Gelman-Rubin diagnostics for a deterministic test suite.\n    \"\"\"\n\n    def _calculate_r_hat(chains: np.ndarray) -> float:\n        \"\"\"\n        Calculates the Gelman-Rubin statistic for a set of chains.\n\n        Args:\n            chains: A 2D numpy array of shape (N, M) where N is the number of\n                    chains and M is the length of each chain.\n\n        Returns:\n            The Gelman-Rubin statistic_hat.\n        \"\"\"\n        if chains.ndim != 2:\n            raise ValueError(\"Input `chains` must be a 2D array.\")\n        \n        N, M = chains.shape\n        \n        if N < 2:\n            # Cannot compute between-chain variance with fewer than 2 chains.\n            # In a converged state, R-hat should be 1.\n            return 1.0\n\n        # Calculate chain-specific means\n        m_j = np.mean(chains, axis=1)\n\n        # Calculate chain-specific sample variances (ddof=1 for sample variance)\n        s_j_sq = np.var(chains, axis=1, ddof=1)\n\n        # Calculate within-chain variance W\n        W = np.mean(s_j_sq)\n\n        # If W is zero, all chains are constant.\n        if W == 0:\n            # If all chain means are also equal, B is 0, converged.\n            if np.all(m_j == m_j[0]):\n                return 1.0\n            # Otherwise, chains are at different constants, not converged.\n            else:\n                return np.inf\n\n        # Calculate pooled mean\n        m_bar = np.mean(m_j)\n\n        # Calculate between-chain variance B\n        B = (M / (N - 1)) * np.sum((m_j - m_bar)**2)\n        \n        # Alternative calculation for B using numpy's sample variance\n        # B = M * np.var(m_j, ddof=1)\n\n        # Calculate marginal posterior variance estimator\n        var_plus = ((M - 1) / M) * W + (1 / M) * B\n\n        # Calculate Gelman-Rubin statistic R-hat\n        r_hat = np.sqrt(var_plus / W)\n\n        return r_hat\n\n    # Define the reliability thresholds\n    tau1 = 1.05\n    tau2 = 1.01\n\n    # Define the 5 test cases\n    # Parameters for x_{j,t} = mu_j + A_j*sin(2*pi*k*t/M + phi_j) + D_j*(2*t/M - 1)\n    test_cases = [\n        # Case 1\n        {\n            \"N\": 4, \"M\": 200, \"k\": 5,\n            \"params\": [\n                {\"mu\": 0, \"A\": 1, \"phi\": 0 * np.pi / 8, \"D\": 0},\n                {\"mu\": 0, \"A\": 1, \"phi\": 1 * np.pi / 8, \"D\": 0},\n                {\"mu\": 0, \"A\": 1, \"phi\": 2 * np.pi / 8, \"D\": 0},\n                {\"mu\": 0, \"A\": 1, \"phi\": 3 * np.pi / 8, \"D\": 0},\n            ]\n        },\n        # Case 2\n        {\n            \"N\": 4, \"M\": 200, \"k\": 5,\n            \"params\": [\n                {\"mu\": 0,   \"A\": 1, \"phi\": 0 * np.pi / 8, \"D\": 0},\n                {\"mu\": 0,   \"A\": 1, \"phi\": 1 * np.pi / 8, \"D\": 0},\n                {\"mu\": 0,   \"A\": 1, \"phi\": 2 * np.pi / 8, \"D\": 0},\n                {\"mu\": 1.5, \"A\": 1, \"phi\": 3 * np.pi / 8, \"D\": 0},\n            ]\n        },\n        # Case 3\n        {\n            \"N\": 4, \"M\": 200, \"k\": 5,\n            \"params\": [\n                {\"mu\": 0, \"A\": 1.0, \"phi\": 0 * np.pi/7, \"D\":  0.0},\n                {\"mu\": 0, \"A\": 1.0, \"phi\": 1 * np.pi/3, \"D\":  0.0},\n                {\"mu\": 0, \"A\": 0.5, \"phi\": 1 * np.pi/7, \"D\":  0.9},\n                {\"mu\": 0, \"A\": 0.5, \"phi\": 1 * np.pi/5, \"D\": -0.9},\n            ]\n        },\n        # Case 4\n        {\n            \"N\": 4, \"M\": 10, \"k\": 1,\n            \"params\": [\n                {\"mu\": 0, \"A\": 1, \"phi\": 0 * np.pi / 8, \"D\": 0},\n                {\"mu\": 0, \"A\": 1, \"phi\": 1 * np.pi / 8, \"D\": 0},\n                {\"mu\": 0, \"A\": 1, \"phi\": 2 * np.pi / 8, \"D\": 0},\n                {\"mu\": 0, \"A\": 1, \"phi\": 3 * np.pi / 8, \"D\": 0},\n            ]\n        },\n        # Case 5\n        {\n            \"N\": 4, \"M\": 200, \"k\": 5,\n            \"params\": [\n                {\"mu\": 0, \"A\": 1, \"phi\": 0 * np.pi / 8, \"D\": 0},\n                {\"mu\": 0, \"A\": 1, \"phi\": 1 * np.pi / 8, \"D\": 0},\n                {\"mu\": 0, \"A\": 3, \"phi\": 2 * np.pi / 8, \"D\": 0},\n                {\"mu\": 0, \"A\": 3, \"phi\": 3 * np.pi / 8, \"D\": 0},\n            ]\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        N, M, k = case[\"N\"], case[\"M\"], case[\"k\"]\n        params = case[\"params\"]\n        \n        # Generate chain data\n        t = np.arange(M)\n        chains = np.zeros((N, M))\n        for j in range(N):\n            p = params[j]\n            mu_j, A_j, phi_j, D_j = p[\"mu\"], p[\"A\"], p[\"phi\"], p[\"D\"]\n            chains[j, :] = mu_j + A_j * np.sin(2 * np.pi * k * t / M + phi_j) \\\n                           + D_j * (2 * t / M - 1)\n\n        # Calculate classical R-hat\n        r_hat = _calculate_r_hat(chains)\n        \n        # Calculate split-chain R-hat\n        if M % 2 != 0:\n            raise ValueError(f\"M must be even for split-chain analysis. M={M}\")\n        \n        M_half = M // 2\n        chains_split1 = chains[:, :M_half]\n        chains_split2 = chains[:, M_half:]\n        split_chains = np.vstack((chains_split1, chains_split2))\n        \n        r_hat_split = _calculate_r_hat(split_chains)\n        \n        # Perform threshold checks\n        b1 = r_hat_split <= tau1\n        b2 = r_hat_split <= tau2\n        \n        # Append results for this case\n        results.extend([r_hat, r_hat_split, b1, b2])\n\n    # Format output string\n    # Convert booleans to lowercase strings as per common convention, although not strictly required\n    # str(True) -> 'True', so map(str,...) is used directly as per template\n    formatted_results = [f\"{x:.10f}\" if isinstance(x, float) else str(x) for x in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Once we have obtained reliable estimates for a model's parameters, the next step is often to understand the uncertainty in a key performance metric or quantity of interest derived from them. This exercise demonstrates the delta method, a powerful and widely used technique in statistics for propagating uncertainty from a set of parameters to a nonlinear function of those parameters. By applying this method, you will learn to approximate the distribution of a complex model output and construct confidence intervals, translating parameter uncertainty into practical, interpretable bounds on model predictions .",
            "id": "4150972",
            "problem": "Consider a complex contagion agent-based model in which each agent’s activation hazard depends on two parameters: an intrinsic activation propensity $\\alpha$ and a social influence sensitivity $\\beta$. Suppose we fit this model to a large collection of independent simulation batches using the method of Maximum Likelihood Estimation (MLE). Under standard regularity conditions and by appeal to the Central Limit Theorem (CLT), the MLE $\\hat{\\theta} = (\\hat{\\alpha}, \\hat{\\beta})^{\\top}$ is consistent and admits an asymptotically normal approximation. You are interested in uncertainty quantification for a nonlinear performance functional of the system, the cascade amplification factor defined by\n$$\ng(\\alpha, \\beta) \\equiv \\frac{\\exp(\\beta)}{1 - \\alpha \\beta^{2}}.\n$$\nStarting from the asymptotic normality of the MLE and using only first principles and well-tested facts about large-sample theory, derive the asymptotic distribution of $g(\\hat{\\theta})$ via the delta method, explicitly identifying the limiting mean and variance in terms of $\\theta$ and the covariance of $\\hat{\\theta}$. Then, for the following fitted values and estimated covariance,\n$$\n\\hat{\\alpha} = 0.3, \\quad \\hat{\\beta} = 0.8, \\quad \\widehat{\\mathrm{Cov}}(\\hat{\\theta}) = \\begin{pmatrix} 0.0016 & -0.0003 \\\\ -0.0003 & 0.0025 \\end{pmatrix},\n$$\ncompute an approximate two-sided $95\\%$ confidence interval for $g(\\theta)$ by substituting $\\hat{\\theta}$ and $\\widehat{\\mathrm{Cov}}(\\hat{\\theta})$ into your asymptotic distribution and using the standard normal quantile for the confidence level. Round both endpoints of the confidence interval to four significant figures. Express your final answer as a row vector using the $\\mathrm{pmatrix}$ environment, with the first entry being the lower endpoint and the second entry being the upper endpoint. The cascade amplification factor $g(\\theta)$ is dimensionless, so no physical units are required.",
            "solution": "The problem is scientifically grounded, well-posed, and complete. We can proceed with the solution. The task is to first derive the asymptotic distribution of a function of Maximum Likelihood Estimators (MLEs) and then use this result to compute a confidence interval. The core theoretical tool for this is the multivariate delta method.\n\nLet the parameter vector be $\\theta = (\\alpha, \\beta)^{\\top}$. The MLE is $\\hat{\\theta} = (\\hat{\\alpha}, \\hat{\\beta})^{\\top}$. The problem states that, under standard conditions, the MLE is asymptotically normal. This is typically expressed as $\\sqrt{n}(\\hat{\\theta} - \\theta) \\xrightarrow{d} \\mathcal{N}(0, \\Sigma)$, where $\\Sigma$ is the asymptotic covariance matrix. For a large sample size $n$, this implies the approximate distribution for $\\hat{\\theta}$ is $\\mathcal{N}(\\theta, \\text{Cov}(\\hat{\\theta}))$, where $\\text{Cov}(\\hat{\\theta})$ is the covariance matrix for the specific sample size, estimated by $\\widehat{\\text{Cov}}(\\hat{\\theta})$.\n\nThe performance functional is given by $g(\\theta) = g(\\alpha, \\beta) = \\frac{\\exp(\\beta)}{1 - \\alpha \\beta^{2}}$. We are interested in the distribution of $g(\\hat{\\theta})$. The delta method provides the first-order Taylor approximation for the variance of a function of random variables. If $\\hat{\\theta}$ is approximately $\\mathcal{N}(\\theta, \\text{Cov}(\\hat{\\theta}))$, then for a differentiable function $g$, the random variable $g(\\hat{\\theta})$ is approximately normal with mean $g(\\theta)$ and variance $\\nabla g(\\theta)^{\\top} \\text{Cov}(\\hat{\\theta}) \\nabla g(\\theta)$.\nSo, $g(\\hat{\\theta}) \\stackrel{\\cdot}{\\sim} \\mathcal{N}\\left(g(\\theta), \\nabla g(\\theta)^{\\top} \\text{Cov}(\\hat{\\theta}) \\nabla g(\\theta)\\right)$.\nThe limiting mean is $g(\\theta)$ and the limiting variance is $\\sigma_g^2 = \\nabla g(\\theta)^{\\top} \\text{Cov}(\\hat{\\theta}) \\nabla g(\\theta)$.\n\nTo apply this, we must first compute the gradient of $g(\\alpha, \\beta)$, which is $\\nabla g = \\left(\\frac{\\partial g}{\\partial \\alpha}, \\frac{\\partial g}{\\partial \\beta}\\right)^{\\top}$.\n\nThe partial derivative with respect to $\\alpha$ is:\n$$\n\\frac{\\partial g}{\\partial \\alpha} = \\frac{\\partial}{\\partial \\alpha} \\left( \\exp(\\beta) (1 - \\alpha \\beta^2)^{-1} \\right) = \\exp(\\beta) \\left( -1 \\cdot (1 - \\alpha \\beta^2)^{-2} \\cdot (-\\beta^2) \\right) = \\frac{\\beta^2 \\exp(\\beta)}{(1 - \\alpha \\beta^2)^2}\n$$\n\nThe partial derivative with respect to $\\beta$ is found using the quotient rule:\n$$\n\\frac{\\partial g}{\\partial \\beta} = \\frac{(\\frac{d}{d\\beta}\\exp(\\beta))(1 - \\alpha \\beta^2) - \\exp(\\beta)(\\frac{d}{d\\beta}(1 - \\alpha \\beta^2))}{(1 - \\alpha \\beta^2)^2} = \\frac{\\exp(\\beta)(1 - \\alpha \\beta^2) - \\exp(\\beta)(-2 \\alpha \\beta)}{(1 - \\alpha \\beta^2)^2} = \\frac{\\exp(\\beta)(1 - \\alpha \\beta^2 + 2 \\alpha \\beta)}{(1 - \\alpha \\beta^2)^2}\n$$\n\nSo, the gradient vector is:\n$$\n\\nabla g(\\theta) = \\begin{pmatrix} \\frac{\\beta^2 \\exp(\\beta)}{(1 - \\alpha \\beta^2)^2} \\\\ \\frac{\\exp(\\beta)(1 + 2 \\alpha \\beta - \\alpha \\beta^2)}{(1 - \\alpha \\beta^2)^2} \\end{pmatrix}\n$$\nThe asymptotic variance of $g(\\hat{\\theta})$ is then given by the quadratic form $\\sigma_g^2 = \\nabla g(\\theta)^{\\top} \\text{Cov}(\\hat{\\theta}) \\nabla g(\\theta)$. This completes the first part of the problem.\n\nFor the second part, we compute a $95\\%$ confidence interval for $g(\\theta)$. An approximate $(1 - \\gamma) \\times 100\\%$ confidence interval for $g(\\theta)$ is constructed as $g(\\hat{\\theta}) \\pm z_{\\gamma/2} \\widehat{\\text{SE}}(g(\\hat{\\theta}))$. The standard error $\\widehat{\\text{SE}}(g(\\hat{\\theta})) = \\sqrt{\\widehat{\\text{Var}}(g(\\hat{\\theta}))}$ is estimated by substituting the MLEs into the variance formula:\n$$\n\\widehat{\\text{Var}}(g(\\hat{\\theta})) = \\nabla g(\\hat{\\theta})^{\\top} \\widehat{\\text{Cov}}(\\hat{\\theta}) \\nabla g(\\hat{\\theta})\n$$\nFor a $95\\%$ confidence level, $\\gamma = 0.05$, so $\\gamma/2 = 0.025$. The corresponding standard normal quantile is $z_{0.025} \\approx 1.96$.\n\nWe are given the fitted values and estimated covariance:\n$$\n\\hat{\\alpha} = 0.3, \\quad \\hat{\\beta} = 0.8, \\quad \\widehat{\\mathrm{Cov}}(\\hat{\\theta}) = \\begin{pmatrix} 0.0016 & -0.0003 \\\\ -0.0003 & 0.0025 \\end{pmatrix}\n$$\nLet $\\hat{\\sigma}_\\alpha^2 = 0.0016$, $\\hat{\\sigma}_\\beta^2 = 0.0025$, and $\\hat{\\sigma}_{\\alpha\\beta} = -0.0003$.\n\nFirst, we compute the point estimate $g(\\hat{\\theta})$:\n$$\ng(\\hat{\\alpha}, \\hat{\\beta}) = \\frac{\\exp(0.8)}{1 - (0.3)(0.8)^2} = \\frac{\\exp(0.8)}{1 - 0.3(0.64)} = \\frac{\\exp(0.8)}{1 - 0.192} = \\frac{\\exp(0.8)}{0.808} \\approx 2.754382\n$$\n\nNext, we evaluate the gradient at the point estimate $(\\hat{\\alpha}, \\hat{\\beta})$:\nThe denominator of the gradient components is $(1 - \\hat{\\alpha}\\hat{\\beta}^2)^2 = (0.808)^2 \\approx 0.652864$.\n$$\n\\frac{\\partial g}{\\partial \\alpha}\\bigg|_{\\hat{\\theta}} = \\frac{(0.8)^2 \\exp(0.8)}{(0.808)^2} = \\frac{0.64 \\exp(0.8)}{0.652864} \\approx 2.181650\n$$\n$$\n\\frac{\\partial g}{\\partial \\beta}\\bigg|_{\\hat{\\theta}} = \\frac{\\exp(0.8)(1 + 2(0.3)(0.8) - (0.3)(0.8)^2)}{(0.808)^2} = \\frac{\\exp(0.8)(1 + 0.48 - 0.192)}{0.652864} = \\frac{\\exp(0.8)(1.288)}{0.652864} \\approx 4.392342\n$$\nSo, $\\nabla g(\\hat{\\theta}) \\approx \\begin{pmatrix} 2.181650 \\\\ 4.392342 \\end{pmatrix}$.\n\nNow, we compute the estimated variance of $g(\\hat{\\theta})$:\n$$\n\\widehat{\\text{Var}}(g(\\hat{\\theta})) = \\begin{pmatrix} 2.181650 & 4.392342 \\end{pmatrix} \\begin{pmatrix} 0.0016 & -0.0003 \\\\ -0.0003 & 0.0025 \\end{pmatrix} \\begin{pmatrix} 2.181650 \\\\ 4.392342 \\end{pmatrix}\n$$\n$$\n\\widehat{\\text{Var}}(g(\\hat{\\theta})) = \\left(\\frac{\\partial g}{\\partial \\alpha}\\right)^2 \\hat{\\sigma}_\\alpha^2 + \\left(\\frac{\\partial g}{\\partial \\beta}\\right)^2 \\hat{\\sigma}_\\beta^2 + 2\\left(\\frac{\\partial g}{\\partial \\alpha}\\right)\\left(\\frac{\\partial g}{\\partial \\beta}\\right)\\hat{\\sigma}_{\\alpha\\beta}\n$$\n$$\n\\widehat{\\text{Var}}(g(\\hat{\\theta})) \\approx (2.181650)^2(0.0016) + (4.392342)^2(0.0025) + 2(2.181650)(4.392342)(-0.0003)\n$$\n$$\n\\widehat{\\text{Var}}(g(\\hat{\\theta})) \\approx (4.75959)(0.0016) + (19.29266)(0.0025) - 2(9.58281)(0.0003)\n$$\n$$\n\\widehat{\\text{Var}}(g(\\hat{\\theta})) \\approx 0.0076153 + 0.0482317 - 0.0057497 \\approx 0.0500973\n$$\n\nThe estimated standard error is the square root of the variance:\n$$\n\\widehat{\\text{SE}}(g(\\hat{\\theta})) = \\sqrt{0.0500973} \\approx 0.2238243\n$$\n\nThe margin of error for the $95\\%$ confidence interval is $z_{0.025} \\times \\widehat{\\text{SE}}(g(\\hat{\\theta}))$:\n$$\n\\text{ME} \\approx 1.959964 \\times 0.2238243 \\approx 0.438686\n$$\n\nFinally, the $95\\%$ confidence interval for $g(\\theta)$ is:\nLower bound: $g(\\hat{\\theta}) - \\text{ME} \\approx 2.754382 - 0.438686 = 2.315696$\nUpper bound: $g(\\hat{\\theta}) + \\text{ME} \\approx 2.754382 + 0.438686 = 3.193068$\n\nRounding both endpoints to four significant figures gives:\nLower bound: $2.316$\nUpper bound: $3.193$\n\nThe confidence interval is expressed as a row vector.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 2.316 & 3.193 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "After propagating and quantifying the total uncertainty in a model's output, a critical task is to attribute this uncertainty to its various sources. This practice delves into variance-based sensitivity analysis, a powerful technique for decomposing output variance into contributions from different uncertain input factors. You will derive a first-order Sobol' index from fundamental principles, tackling the important real-world complication of hard constraints that induce dependencies among model inputs and alter the sensitivity allocation .",
            "id": "4150979",
            "problem": "Consider a stylized complex adaptive system with two interacting agents whose controllable inputs are represented by the random vector $\\mathbf{X} = (X_1, X_2)$. Feasible joint adaptations are restricted by a hard resource constraint, modeled as the triangular domain $D = \\{(x_1,x_2) \\in \\mathbb{R}^2 : x_1 \\ge 0,\\, x_2 \\ge 0,\\, x_1 + x_2 \\le 1\\}$. Assume the system samples adaptations uniformly over $D$, that is, the joint density is constant over $D$ and zero elsewhere.\n\nLet the scalar system output be $Y = f(X_1,X_2) = X_1 + X_2$. You are asked to quantify how the hard constraint alters variance-based sensitivity allocation to $X_1$ by deriving and evaluating the first-order variance-based sensitivity index (often called the Sobol' index) with respect to the constrained input law on $D$.\n\nStarting only from core definitions of variance, conditional expectation, conditional variance, and the definition of the first-order variance-based sensitivity index,\n$$\nS_1 \\equiv \\frac{\\mathrm{Var}\\!\\left(\\mathbb{E}[Y \\mid X_1]\\right)}{\\mathrm{Var}(Y)},\n$$\nand from the geometric interpretation of the uniform measure on $D$, do the following:\n\n1. Derive the marginal density of $X_1$ induced by the uniform distribution on $D$ and the conditional density of $X_2 \\mid X_1 = x_1$ over the feasible interval determined by the hard constraint.\n2. Using only these foundational facts, derive $\\mathbb{E}[Y \\mid X_1]$, $\\mathrm{Var}\\!\\left(\\mathbb{E}[Y \\mid X_1]\\right)$, and $\\mathrm{Var}(Y)$ under the constrained input law on $D$.\n3. Construct $S_1$ from the derived quantities. Provide the exact value.\n\nSeparately (no computation required for the final numerical value), propose an adjusted Monte Carlo (Monte Carlo (MC)) estimator that is unbiased for $\\mathrm{Var}\\!\\left(\\mathbb{E}[Y \\mid X_1]\\right)$ under the constrained law on $D$ by sampling pairs of conditionally independent replicates $(Y, Y')$ given $X_1$ using the conditional distribution $X_2 \\mid X_1 = x_1$. Clearly specify which conditional and marginal distributions your estimator requires.\n\nProvide the exact value of $S_1$ as your final answer. Do not round. No units are required.",
            "solution": "The user wants to solve a problem in the field of *uncertainty quantification in complex models*, which is a topic in *complex adaptive systems modeling*.\n\n### Step 1: Extract Givens\n-   **Random Inputs**: $\\mathbf{X} = (X_1, X_2)$\n-   **Feasible Domain**: $D = \\{(x_1,x_2) \\in \\mathbb{R}^2 : x_1 \\ge 0,\\, x_2 \\ge 0,\\, x_1 + x_2 \\le 1\\}$\n-   **Input Distribution**: Uniform over the domain $D$. The joint probability density function (PDF), $p(x_1, x_2)$, is a constant on $D$ and $0$ elsewhere.\n-   **System Output**: $Y = f(X_1,X_2) = X_1 + X_2$\n-   **Target Quantity**: The first-order variance-based sensitivity index, $S_1 = \\frac{\\mathrm{Var}\\!\\left(\\mathbb{E}[Y \\mid X_1]\\right)}{\\mathrm{Var}(Y)}$.\n-   **Task 1**: Derive the marginal density of $X_1$, $p(x_1)$, and the conditional density of $X_2$ given $X_1$, $p(x_2 \\mid x_1)$.\n-   **Task 2**: Derive $\\mathbb{E}[Y \\mid X_1]$, $\\mathrm{Var}\\!\\left(\\mathbb{E}[Y \\mid X_1]\\right)$, and $\\mathrm{Var}(Y)$ using foundational definitions.\n-   **Task 3**: Construct the exact value of $S_1$.\n-   **Task 4**: Propose an unbiased Monte Carlo estimator for $\\mathrm{Var}\\!\\left(\\mathbb{E}[Y \\mid X_1]\\right)$ based on conditionally independent replicates.\n\n### Step 2: Validate Using Extracted Givens\nThis is a standard problem in probability theory and global sensitivity analysis.\n-   **Scientifically Grounded**: The problem is based on the well-established mathematical framework of probability theory and Sobol' sensitivity indices. The model, though stylized, is mathematically sound.\n-   **Well-Posed**: The domain, probability distribution, and output function are all clearly defined. The quantities to be derived are standard statistical measures. A unique and stable solution exists.\n-   **Objective**: The problem is stated in precise, formal mathematical language, free of ambiguity or subjective claims.\nThe problem does not violate any of the invalidity criteria. It is a well-posed, self-contained, and scientifically valid exercise in applied probability.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A full solution will be provided.\n\n### Solution Derivation\n\nThe analysis begins by defining the joint probability density function (PDF) of the inputs $(X_1, X_2)$. The inputs are uniformly distributed over the domain $D$, which is a right triangle with vertices at $(0,0)$, $(1,0)$, and $(0,1)$. The area of this triangle is $\\text{Area}(D) = \\frac{1}{2} \\times \\text{base} \\times \\text{height} = \\frac{1}{2} \\times 1 \\times 1 = \\frac{1}{2}$.\nSince the distribution is uniform, the joint PDF $p(x_1, x_2)$ is the reciprocal of the area of $D$ for any point within $D$, and $0$ otherwise.\n$$\np(x_1, x_2) =\n\\begin{cases}\n\\frac{1}{\\text{Area}(D)} = 2, & \\text{if } x_1 \\ge 0, x_2 \\ge 0, x_1+x_2 \\le 1 \\\\\n0, & \\text{otherwise}\n\\end{cases}\n$$\n\n**1. Derivation of Marginal and Conditional Densities**\n\nTo find the marginal density of $X_1$, $p(x_1)$, we integrate the joint PDF over all possible values of $x_2$. For a given $x_1 \\in [0, 1]$, the variable $x_2$ is constrained to the interval $[0, 1-x_1]$.\n$$\np(x_1) = \\int_{-\\infty}^{\\infty} p(x_1, x_2) \\,dx_2 = \\int_0^{1-x_1} 2 \\,dx_2 = 2[x_2]_0^{1-x_1} = 2(1-x_1) \\quad \\text{for } x_1 \\in [0, 1]\n$$\nFor $x_1$ outside $[0, 1]$, $p(x_1)=0$.\n\nThe conditional density $p(x_2 \\mid x_1)$ is defined by the formula $p(x_2 \\mid x_1) = \\frac{p(x_1, x_2)}{p(x_1)}$, for $p(x_1) > 0$. This is valid for $x_1 \\in [0, 1)$.\n$$\np(x_2 \\mid x_1) = \\frac{2}{2(1-x_1)} = \\frac{1}{1-x_1} \\quad \\text{for } x_2 \\in [0, 1-x_1]\n$$\nThis shows that for a given value $X_1=x_1$, the random variable $X_2$ is uniformly distributed on the interval $[0, 1-x_1]$.\n\n**2. Derivation of Variance Components**\n\nWe now derive the quantities needed for $S_1$.\n\nFirst, we compute the conditional expectation of $Y$ given $X_1$.\n$$\n\\mathbb{E}[Y \\mid X_1 = x_1] = \\mathbb{E}[X_1 + X_2 \\mid X_1 = x_1] = x_1 + \\mathbb{E}[X_2 \\mid X_1 = x_1]\n$$\nSince $X_2$ conditioned on $X_1=x_1$ is uniformly distributed on $[0, 1-x_1]$, its expectation is the midpoint of the interval:\n$$\n\\mathbb{E}[X_2 \\mid X_1 = x_1] = \\frac{0 + (1-x_1)}{2} = \\frac{1-x_1}{2}\n$$\nSubstituting this back, we get:\n$$\n\\mathbb{E}[Y \\mid X_1 = x_1] = x_1 + \\frac{1-x_1}{2} = \\frac{2x_1 + 1 - x_1}{2} = \\frac{x_1+1}{2}\n$$\nThe quantity $\\mathbb{E}[Y \\mid X_1]$ is a random variable, which is a function of $X_1$: $\\mathbb{E}[Y \\mid X_1] = \\frac{X_1+1}{2}$.\n\nNext, we compute its variance, $\\mathrm{Var}\\!\\left(\\mathbb{E}[Y \\mid X_1]\\right)$.\n$$\n\\mathrm{Var}\\!\\left(\\mathbb{E}[Y \\mid X_1]\\right) = \\mathrm{Var}\\!\\left(\\frac{X_1+1}{2}\\right) = \\mathrm{Var}\\!\\left(\\frac{1}{2}X_1 + \\frac{1}{2}\\right) = \\left(\\frac{1}{2}\\right)^2 \\mathrm{Var}(X_1) = \\frac{1}{4}\\mathrm{Var}(X_1)\n$$\nTo find $\\mathrm{Var}(X_1)$, we first need $\\mathbb{E}[X_1]$ and $\\mathbb{E}[X_1^2]$. Using the marginal density $p(x_1)=2(1-x_1)$:\n$$\n\\mathbb{E}[X_1] = \\int_0^1 x_1 p(x_1) \\,dx_1 = \\int_0^1 x_1 \\cdot 2(1-x_1) \\,dx_1 = 2 \\int_0^1 (x_1 - x_1^2) \\,dx_1 = 2 \\left[\\frac{x_1^2}{2} - \\frac{x_1^3}{3}\\right]_0^1 = 2\\left(\\frac{1}{2} - \\frac{1}{3}\\right) = \\frac{1}{3}\n$$\n$$\n\\mathbb{E}[X_1^2] = \\int_0^1 x_1^2 p(x_1) \\,dx_1 = \\int_0^1 x_1^2 \\cdot 2(1-x_1) \\,dx_1 = 2 \\int_0^1 (x_1^2 - x_1^3) \\,dx_1 = 2 \\left[\\frac{x_1^3}{3} - \\frac{x_1^4}{4}\\right]_0^1 = 2\\left(\\frac{1}{3} - \\frac{1}{4}\\right) = \\frac{1}{6}\n$$\nNow we compute the variance of $X_1$:\n$$\n\\mathrm{Var}(X_1) = \\mathbb{E}[X_1^2] - (\\mathbb{E}[X_1])^2 = \\frac{1}{6} - \\left(\\frac{1}{3}\\right)^2 = \\frac{1}{6} - \\frac{1}{9} = \\frac{3-2}{18} = \\frac{1}{18}\n$$\nTherefore, the first-order conditional variance is:\n$$\n\\mathrm{Var}\\!\\left(\\mathbb{E}[Y \\mid X_1]\\right) = \\frac{1}{4}\\mathrm{Var}(X_1) = \\frac{1}{4} \\cdot \\frac{1}{18} = \\frac{1}{72}\n$$\nFinally, we compute the total variance, $\\mathrm{Var}(Y)$. We use the Law of Total Variance: $\\mathrm{Var}(Y) = \\mathrm{Var}\\!\\left(\\mathbb{E}[Y \\mid X_1]\\right) + \\mathbb{E}\\left[\\mathrm{Var}(Y \\mid X_1)\\right]$. We need the second term.\n$$\n\\mathrm{Var}(Y \\mid X_1=x_1) = \\mathrm{Var}(x_1 + X_2 \\mid X_1=x_1) = \\mathrm{Var}(X_2 \\mid X_1=x_1)\n$$\nThis is the variance of a uniform distribution on $[0, 1-x_1]$. The variance of a uniform distribution on $[a,b]$ is $\\frac{(b-a)^2}{12}$.\n$$\n\\mathrm{Var}(Y \\mid X_1=x_1) = \\frac{(1-x_1 - 0)^2}{12} = \\frac{(1-x_1)^2}{12}\n$$\nNow we take the expectation of this quantity over $X_1$:\n$$\n\\mathbb{E}\\left[\\mathrm{Var}(Y \\mid X_1)\\right] = \\mathbb{E}\\left[\\frac{(1-X_1)^2}{12}\\right] = \\frac{1}{12}\\mathbb{E}[(1-X_1)^2] = \\frac{1}{12}\\mathbb{E}[1 - 2X_1 + X_1^2]\n$$\nUsing the linearity of expectation and our previous results for $\\mathbb{E}[X_1]$ and $\\mathbb{E}[X_1^2]$:\n$$\n\\mathbb{E}\\left[\\mathrm{Var(Y \\mid X_1)}\\right] = \\frac{1}{12}\\left(1 - 2\\mathbb{E}[X_1] + \\mathbb{E}[X_1^2]\\right) = \\frac{1}{12}\\left(1 - 2\\left(\\frac{1}{3}\\right) + \\frac{1}{6}\\right) = \\frac{1}{12}\\left(1 - \\frac{2}{3} + \\frac{1}{6}\\right) = \\frac{1}{12}\\left(\\frac{6-4+1}{6}\\right) = \\frac{1}{12}\\left(\\frac{3}{6}\\right) = \\frac{1}{24}\n$$\nThe total variance is:\n$$\n\\mathrm{Var}(Y) = \\mathrm{Var}\\!\\left(\\mathbb{E}[Y \\mid X_1]\\right) + \\mathbb{E}\\left[\\mathrm{Var}(Y \\mid X_1)\\right] = \\frac{1}{72} + \\frac{1}{24} = \\frac{1+3}{72} = \\frac{4}{72} = \\frac{1}{18}\n$$\n\n**3. Construction of the Sensitivity Index $S_1$**\n\nWe can now construct the Sobol' index $S_1$ using the derived quantities.\n$$\nS_1 = \\frac{\\mathrm{Var}\\!\\left(\\mathbb{E}[Y \\mid X_1]\\right)}{\\mathrm{Var}(Y)} = \\frac{1/72}{1/18} = \\frac{1}{72} \\times \\frac{18}{1} = \\frac{18}{72} = \\frac{1}{4}\n$$\n\n**4. Proposed Monte Carlo Estimator**\n\nTo construct an unbiased Monte Carlo estimator for $V_1 = \\mathrm{Var}\\!\\left(\\mathbb{E}[Y \\mid X_1]\\right)$, we use the property that $V_1 = \\mathrm{Cov}(Y, Y')$, where $Y = f(X_1, X_2)$ and $Y' = f(X_1, X'_2)$, with $X_2$ and $X'_2$ being independent and identically distributed draws from the conditional distribution $p(x_2 \\mid x_1)$. An unbiased estimator for the covariance of two random variables $A$ and $B$ from a sample of $N$ pairs $(a_i, b_i)$ is the sample covariance:\n$$\n\\widehat{\\mathrm{Cov}}(A, B) = \\frac{1}{N-1} \\sum_{i=1}^N (a_i - \\bar{a})(b_i - \\bar{b})\n$$\nThe estimation procedure is as follows:\n1.  For $i=1, \\dots, N$:\n    a. Draw a sample $x_1^{(i)}$ from the marginal distribution $p(x_1) = 2(1-x_1)$ for $x_1 \\in [0, 1]$.\n    b. Draw two independent samples $x_2^{(i)}$ and $x_2^{\\prime(i)}$ from the conditional distribution $p(x_2 \\mid X_1=x_1^{(i)})$, which is a uniform distribution on $[0, 1-x_1^{(i)}]$.\n    c. Compute the model outputs $y^{(i)} = f(x_1^{(i)}, x_2^{(i)})$ and $y^{\\prime(i)} = f(x_1^{(i)}, x_2^{\\prime(i)})$.\n\n2.  Compute the sample means $\\bar{y} = \\frac{1}{N} \\sum_{i=1}^N y^{(i)}$ and $\\bar{y}' = \\frac{1}{N} \\sum_{i=1}^N y^{\\prime(i)}$.\n\n3.  The unbiased estimator for $V_1$ is then the sample covariance between the two sets of outputs:\n$$\n\\hat{V}_1 = \\frac{1}{N-1} \\sum_{i=1}^N (y^{(i)} - \\bar{y})(y^{\\prime(i)} - \\bar{y}')\n$$\nThis estimator is unbiased for $\\mathrm{Var}\\!\\left(\\mathbb{E}[Y \\mid X_1]\\right)$ and relies on being able to sample from the marginal distribution $p(x_1)$ and the conditional distribution $p(x_2 \\mid x_1)$, which were derived in the first part of the problem.",
            "answer": "$$\\boxed{\\frac{1}{4}}$$"
        }
    ]
}