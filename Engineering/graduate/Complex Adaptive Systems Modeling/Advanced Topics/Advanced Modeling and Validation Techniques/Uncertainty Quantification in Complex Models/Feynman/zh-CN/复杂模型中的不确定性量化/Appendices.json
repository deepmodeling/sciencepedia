{
    "hands_on_practices": [
        {
            "introduction": "不确定性量化（UQ）的一个核心任务是“前向传播”：即理解模型输入的随机性如何转化为输出的不确定性。本练习将引导你应用一种强大的非侵入式谱投影方法，在一个多尺度模型中传播不确定性，将微观尺度上的随机性与由偏微分方程描述的宏观行为联系起来。通过这个实践，你将掌握处理复杂“黑箱”模型不确定性的关键技能，在这类模型中，输入和输出之间的关系并非由简单公式给出，而是需要通过数值求解器确定。",
            "id": "4150946",
            "problem": "构建一个自洽程序，该程序实现一个多尺度不确定性传播框架，将微观层面的随机性与宏观层面的偏微分方程动力学耦合。宏观层面系统是一个在单位区间 $\\left[0,1\\right]$ 上具有齐次 Dirichlet 边界条件的一维稳态扩散问题。控制方程是由 Fick 定律和守恒定律推导出的稳态平衡方程，即 $- \\dfrac{d}{dx}\\left(D_{\\mathrm{eff}}(U)\\dfrac{du}{dx}\\right) = q(x)$，其中 $u(0) = 0$ 且 $u(1) = 0$。$D_{\\mathrm{eff}}(U)$ 是一个空间上均匀的均质化扩散系数，它依赖于微观层面的随机参数 $U$，$q(x)$ 是一个确定性源项。假设源项为常数 $q(x) \\equiv q$，其中 $q \\in \\mathbb{R}$，并使用无量纲变量。微观层面由 $M \\in \\mathbb{N}$ 个串联排列的微单元（层状介质）组成。每个微单元具有随机电导率 $k_i(U)$，定义为 $k_i(U) = k_0 \\exp(U)\\left(1 + \\alpha \\sin\\left(2\\pi \\dfrac{i}{M}\\right)\\right)$，其中 $i \\in \\{0,1,\\dots,M-1\\}$，$k_0 > 0$ 是一个基线扩散系数，$\\alpha \\in [0,1)$ 是控制非均质性的振幅参数。有效扩散系数由适用于串联耦合的调和平均值定义，即 $D_{\\mathrm{eff}}(U) = \\dfrac{M}{\\sum_{i=0}^{M-1} \\dfrac{1}{k_i(U)}}$。微观层面的随机参数 $U$ 是高斯分布的，其均值为 $\\mu$，标准差为 $\\sigma > 0$。感兴趣量是宏观层面解的空间平均值，记为 $Y(U) = \\int_{0}^{1} u(x;U)\\,dx$。您必须使用基于阶数为 $n_q \\in \\mathbb{N}$ 的 Gauss-Hermite 求积的非侵入式谱投影（也称为非侵入式多项式混沌）方法，将 $U$ 中的不确定性传播到宏观量 $Y(U)$。该求积方法应用于标准正态分布，并映射到 $U \\sim \\mathcal{N}(\\mu,\\sigma^2)$。您的实现必须：\n- 构建从微观到宏观的闭合关系 $U \\mapsto D_{\\mathrm{eff}}(U)$，\n- 对每个求积节点，使用具有用户指定的网格点数 $N_x \\in \\mathbb{N}$ 的有限差分方法对宏观层面边值问题进行数值求解，以及\n- 使用求积法则以数值稳定的方式计算 $Y(U)$ 的均值和方差。\n\n您必须从以下基本依据开始您的推导和算法设计：(i) 一维质量守恒，表示为 $-\\dfrac{d}{dx}\\left(J\\right) = q(x)$，其中通量 $J = -D_{\\mathrm{eff}}(U)\\dfrac{du}{dx}$ (Fick 定律)，(ii) 串联介质的调和平均值定义，以及 (iii) 针对高斯随机变量函数的高斯积分恒等式与 Gauss-Hermite 求积。不要在问题陈述中假设或使用任何关于 $u(x)$ 或 $Y(U)$ 矩的简化公式。所有符号必须明确定义。\n\n数值指令和约束：\n- 将所有量视为无量纲，因此不需要进行物理单位转换。\n- 有限差分法必须在 $\\left[0,1\\right]$ 上使用包含边界在内的 $N_x$ 个点的均匀网格，并且必须强制执行 $u(0)=0$ 和 $u(1)=0$。\n- 使用 Gauss-Hermite 求积节点和权重 $\\{x_j,w_j\\}_{j=1}^{n_q}$（针对标准权重 $\\exp\\left(-x^2\\right)$），并进行适当变换以积分关于 $U \\sim \\mathcal{N}(\\mu,\\sigma^2)$ 的期望值。\n- 为保证数值稳定性，通过将 $\\alpha$ 限制在 $[0,1)$ 来确保 $1+\\alpha \\sin\\left(2\\pi \\dfrac{i}{M}\\right) > 0$。\n\n测试套件和要求输出：评估框架对以下参数集的表现，并为每个案例报告两个浮点数，顺序为 $Y(U)$ 的均值和方差，并按如下规定汇总到单行中。\n\n- 案例 #1 (一般情况): $M = 50$, $\\alpha = 0.3$, $k_0 = 1.0$, $\\mu = 0.0$, $\\sigma = 0.5$, $q = 1.0$, $n_q = 9$, $N_x = 201$.\n- 案例 #2 (无微观非均质性边界): $M = 1$, $\\alpha = 0.0$, $k_0 = 2.0$, $\\mu = 0.0$, $\\sigma = 0.2$, $q = 0.5$, $n_q = 9$, $N_x = 201$.\n- 案例 #3 (高非均质性，较大不确定性): $M = 200$, $\\alpha = 0.9$, $k_0 = 0.5$, $\\mu = -0.2$, $\\sigma = 0.7$, $q = 2.0$, $n_q = 11$, $N_x = 401$.\n\n最终输出格式：您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。结果必须是每个测试案例的均值和方差的串联，顺序为 $[\\mathrm{mean}_1,\\mathrm{var}_1,\\mathrm{mean}_2,\\mathrm{var}_2,\\mathrm{mean}_3,\\mathrm{var}_3]$，每个条目都是一个浮点数。",
            "solution": "该问题要求构建一个多尺度不确定性量化框架。这涉及将不确定性从随机的微观层面模型传播到确定性的宏观层面模型。该框架将以一个自洽的 Python 程序实现。\n\n**问题陈述验证**\n\n*   **第 1 步：提取已知条件**\n    *   **宏观层面模型**：一维稳态扩散方程：$- \\dfrac{d}{dx}\\left(D_{\\mathrm{eff}}(U)\\dfrac{du}{dx}\\right) = q(x)$，在域 $x \\in [0,1]$ 上。\n    *   **边界条件**：齐次 Dirichlet，$u(0) = 0$ 且 $u(1) = 0$。\n    *   **源项**：常数，$q(x) = q$。\n    *   **微观层面模型**：$M$ 个串联微单元。第 $i$ 个单元的电导率为 $k_i(U) = k_0 \\exp(U)\\left(1 + \\alpha \\sin\\left(2\\pi \\dfrac{i}{M}\\right)\\right)$，其中 $i \\in \\{0,1,\\dots,M-1\\}$。\n    *   **模型参数**：$k_0 > 0$，$\\alpha \\in [0,1)$。\n    *   **微观到宏观的联系**：有效扩散系数 $D_{\\mathrm{eff}}(U)$ 是微观电导率的调和平均值：$D_{\\mathrm{eff}}(U) = \\dfrac{M}{\\sum_{i=0}^{M-1} \\dfrac{1}{k_i(U)}}$。\n    *   **随机输入**：参数 $U$ 是一个高斯随机变量，$U \\sim \\mathcal{N}(\\mu,\\sigma^2)$，其中 $\\sigma > 0$。\n    *   **感兴趣量 (QoI)**：宏观层面解的空间平均值，$Y(U) = \\int_{0}^{1} u(x;U)\\,dx$。\n    *   **不确定性传播方法**：使用 $n_q$ 阶 Gauss-Hermite 求积的非侵入式谱投影。\n    *   **数值离散化**：在具有 $N_x$ 个点的均匀网格上对宏观层面边值问题使用有限差分法。\n    *   **测试案例**：提供了三组参数。\n    *   **输出**：每个测试案例中 $Y(U)$ 的均值和方差。\n\n*   **第 2 步：使用提取的已知条件进行验证**\n    *   **科学依据**：该问题建立在物理学（Fick 扩散定律、质量守恒）和材料科学（通过调和平均对层状介质进行均质化）的基本原理之上。使用随机方法（多项式混沌、求积）进行不确定性传播是计算科学与工程中的一种标准且成熟的技术。该问题在科学上是合理的。\n    *   **适定性**：具有正扩散系数 $D_{\\mathrm{eff}} > 0$ 和 Dirichlet 边界条件的扩散方程是一个经典的适定椭圆边值问题，存在唯一解。约束条件 $\\alpha \\in [0,1)$ 确保对所有 $i$ 都有 $k_i > 0$，这反过来又保证了 $D_{\\mathrm{eff}} > 0$。只要感兴趣量本身是随机变量的可测函数（事实的确如此），其统计矩（均值、方差）就是明确定义的。\n    *   **目标**：问题陈述使用了精确的数学和科学语言。所有参数和目标都得到了明确的定义。\n    *   **完整性**：为每个测试案例提供了所有必要的参数（$M, \\alpha, k_0, \\mu, \\sigma, q, n_q, N_x$）。控制方程和数值方法都已指定。该问题是自洽的。\n    *   **其他**：该问题并非无关紧要、比喻性或无法验证的。它在多尺度建模中提出了一个标准但完整的计算挑战。\n\n*   **第 3 步：结论与行动**\n    *   该问题是**有效的**。将开发并实现一个解决方案。\n\n**推导与算法设计**\n\n解决方案是基于指定的基本原理构建的。\n\n**1. 宏观层面模型：稳态扩散**\n系统由质量守恒控制，表示为 $-\\dfrac{dJ}{dx} = q(x)$，其中 $J$ 是扩散通量。根据 Fick 第一定律，通量与浓度梯度成正比：$J = -D_{\\mathrm{eff}}(U)\\dfrac{du}{dx}$。将两者结合，得到浓度场 $u(x)$ 的一维稳态扩散方程：\n$$ - \\frac{d}{dx}\\left(D_{\\mathrm{eff}}(U)\\frac{du}{dx}\\right) = q(x) $$\n考虑到对于给定的 $U$ 实现，有效扩散系数 $D_{\\mathrm{eff}}(U)$ 在空间上是均匀的，并且源项 $q(x)$ 是一个常数 $q$，该方程简化为：\n$$ -D_{\\mathrm{eff}}(U)\\frac{d^2u}{dx^2} = q $$\n这是一个二阶常微分方程 (ODE)，受边界条件 $u(0)=0$ 和 $u(1)=0$ 的约束。\n\n**2. 微观到宏观的耦合**\n宏观层面的扩散系数 $D_{\\mathrm{eff}}(U)$ 是底层微观结构的一个涌现属性。微观结构由 $M$ 个串联的层组成。对于通过一系列层（一维层状介质）的输运，有效电阻是各个电阻的总和。由于电导率是电阻率的倒数，有效电导率是各层电导率的调和平均值。给定每个微单元的电导率为 $k_i(U) = k_0 \\exp(U)\\left(1 + \\alpha \\sin\\left(2\\pi \\dfrac{i}{M}\\right)\\right)$，有效扩散系数为：\n$$ D_{\\mathrm{eff}}(U) = \\frac{M}{\\sum_{i=0}^{M-1} \\frac{1}{k_i(U)}} = \\frac{M}{\\sum_{i=0}^{M-1} \\frac{1}{k_0 \\exp(U)(1 + \\alpha \\sin(2\\pi i/M))}} = k_0 \\exp(U) \\frac{M}{\\sum_{i=0}^{M-1} \\frac{1}{1 + \\alpha \\sin(2\\pi i/M)}} $$\n对于给定的微观结构几何形状（$\\alpha, M$），涉及求和的项是一个常数。\n\n**3. 宏观模型的数值解**\n该常微分方程使用有限差分法进行数值求解。域 $[0,1]$ 被离散化为 $N_x$ 个点，均匀网格间距为 $\\Delta x = 1/(N_x-1)$。网格点为 $x_k = k \\Delta x$，其中 $k=0, 1, \\dots, N_x-1$。令 $u_k$ 表示 $u(x_k)$ 的数值近似。\n二阶导数使用二阶中心差分格式近似：\n$$ \\frac{d^2u}{dx^2}\\bigg|_{x_k} \\approx \\frac{u_{k+1} - 2u_k + u_{k-1}}{(\\Delta x)^2} $$\n将此代入内部网格点（$k=1, \\dots, N_x-2$）的常微分方程，得到一个线性代数方程组：\n$$ -D_{\\mathrm{eff}}(U) \\left( \\frac{u_{k+1} - 2u_k + u_{k-1}}{(\\Delta x)^2} \\right) = q $$\n整理后得到：\n$$ -u_{k-1} + 2u_k - u_{k+1} = \\frac{q (\\Delta x)^2}{D_{\\mathrm{eff}}(U)} $$\n并入边界条件 $u_0 = 0$ 和 $u_{N_x-1} = 0$。这会产生一个三对角线性方程组 $A\\mathbf{u}_{\\text{int}} = \\mathbf{b}$，其中 $\\mathbf{u}_{\\text{int}} = [u_1, u_2, \\dots, u_{N_x-2}]^T$ 是未知内部值的向量。这个 $(N_x-2) \\times (N_x-2)$ 矩阵 $A$ 的主对角线元素为 $2$，次对角线和次次对角线元素为 $-1$。右侧向量 $\\mathbf{b}$ 的所有元素都等于 $q(\\Delta x)^2/D_{\\mathrm{eff}}(U)$。该系统可以高效地求解出 $\\mathbf{u}_{\\text{int}}$。\n\n**4. 通过 Gauss-Hermite 求积进行不确定性传播**\n目标是计算感兴趣量 $Y(U)$ 的均值 $\\mathbb{E}[Y]$ 和方差 $\\mathbb{V}[Y]$。随机变量为 $U \\sim \\mathcal{N}(\\mu, \\sigma^2)$。我们引入标准正态变量 $\\xi \\sim \\mathcal{N}(0,1)$，使得 $U = \\mu + \\sigma \\xi$。函数 $g(U)$ 的期望由积分给出：\n$$ \\mathbb{E}[g(U)] = \\int_{-\\infty}^{\\infty} g(\\mu + \\sigma \\xi) \\frac{1}{\\sqrt{2\\pi}} e^{-\\xi^2/2} d\\xi $$\n该积分可以使用 Gauss-Hermite 求积进行近似。标准的 Gauss-Hermite 求积法则近似形式为 $\\int_{-\\infty}^{\\infty} f(x)e^{-x^2}dx$ 的积分。为了匹配此形式，我们进行替换 $x = \\xi/\\sqrt{2}$，因此 $\\xi = \\sqrt{2}x$ 且 $d\\xi = \\sqrt{2}dx$：\n$$ \\mathbb{E}[g(U)] = \\int_{-\\infty}^{\\infty} g(\\mu + \\sigma\\sqrt{2}x) \\frac{1}{\\sqrt{2\\pi}} e^{-x^2} \\sqrt{2}dx = \\frac{1}{\\sqrt{\\pi}} \\int_{-\\infty}^{\\infty} g(\\mu + \\sigma\\sqrt{2}x) e^{-x^2} dx $$\n应用具有节点 $\\{x_j\\}_{j=1}^{n_q}$ 和权重 $\\{w_j\\}_{j=1}^{n_q}$ 的 $n_q$ 点 Gauss-Hermite 求积法则，得到：\n$$ \\mathbb{E}[g(U)] \\approx \\frac{1}{\\sqrt{\\pi}} \\sum_{j=1}^{n_q} w_j g(\\mu + \\sigma\\sqrt{2}x_j) $$\n我们将 $U$ 空间中的求积点定义为 $U_j = \\mu + \\sigma\\sqrt{2}x_j$，相应的概率权重定义为 $\\hat{w}_j = w_j/\\sqrt{\\pi}$。然后，期望值通过加权和近似：\n$$ \\mathbb{E}[g(U)] \\approx \\sum_{j=1}^{n_q} \\hat{w}_j g(U_j) $$\n对于我们的问题，函数 $g(U)$ 是感兴趣量 $Y(U)$。$Y$ 的均值为：\n$$ \\mathbb{E}[Y] \\approx \\sum_{j=1}^{n_q} \\hat{w}_j Y(U_j) $$\n$Y$ 的方差为 $\\mathbb{V}[Y] = \\mathbb{E}[(Y - \\mathbb{E}[Y])^2]$。对这个新的期望使用相同的求积法则，可以得到一个数值稳定的方差公式：\n$$ \\mathbb{V}[Y] \\approx \\sum_{j=1}^{n_q} \\hat{w}_j (Y(U_j) - \\mathbb{E}[Y])^2 $$\n其中 $\\mathbb{E}[Y]$ 是上一步计算出的均值。\n\n**5. 计算算法摘要**\n对于每个测试案例，完整的算法流程如下：\n1.  给定求积阶数 $n_q$，获取权重函数为 $e^{-x^2}$ 的标准 Gauss-Hermite 节点 $\\{x_j\\}$ 和权重 $\\{w_j\\}$。\n2.  将节点转换到 $U$ 的物理空间：$U_j = \\mu + \\sigma\\sqrt{2}x_j$。将权重转换为概率权重：$\\hat{w}_j = w_j/\\sqrt{\\pi}$。\n3.  初始化一个空列表 `Y_evals` 以存储 QoI 的评估值。\n4.  对于每个转换后的节点 $U_j$（从 $j=1$ 到 $n_q$）：\n    a.  使用微观模型和均质化公式计算有效扩散系数 $D_{\\mathrm{eff}}(U_j)$。\n    b.  求解有限差分系统 $A\\mathbf{u}_{\\text{int}} = \\mathbf{b}$，其中右侧项依赖于 $D_{\\mathrm{eff}}(U_j)$，以找到内部解值 $\\mathbf{u}_{\\text{int}}$。\n    c.  通过对完整的数值解向量 $[0, u_1, \\dots, u_{N_x-2}, 0]$ 应用梯形法则，计算 QoI，$Y(U_j) = \\int_0^1 u(x; U_j) dx$。\n    d.  将结果 $Y(U_j)$ 附加到 `Y_evals`。\n5.  循环结束后，计算 QoI 的均值：$\\mathbb{E}[Y] \\approx \\sum_j \\hat{w}_j Y(U_j)$。\n6.  计算 QoI 的方差：$\\mathbb{V}[Y] \\approx \\sum_j \\hat{w}_j (Y(U_j) - \\mathbb{E}[Y])^2$。\n7.  存储计算出的均值和方差以备最终输出。\n此过程将微观尺度的随机性与宏观尺度的感兴趣量耦合起来，并量化其产生的统计矩。",
            "answer": "```python\nimport numpy as np\nfrom numpy.polynomial.hermite import hermgauss\n\ndef solve():\n    \"\"\"\n    Main function to solve the multiscale uncertainty quantification problem\n    for a given set of test cases.\n    \"\"\"\n    # Test cases defined in the problem statement.\n    # Format: (M, alpha, k0, mu, sigma, q, nq, Nx)\n    test_cases = [\n        (50, 0.3, 1.0, 0.0, 0.5, 1.0, 9, 201),  # Case #1\n        (1, 0.0, 2.0, 0.0, 0.2, 0.5, 9, 201),   # Case #2\n        (200, 0.9, 0.5, -0.2, 0.7, 2.0, 11, 401)  # Case #3\n    ]\n\n    results = []\n    for case in test_cases:\n        M, alpha, k0, mu, sigma, q, nq, Nx = case\n\n        # 1. Get standard Gauss-Hermite quadrature nodes and weights.\n        # These are for the weighting function exp(-x^2).\n        x_gh, w_gh = hermgauss(nq)\n\n        # 2. Transform nodes and weights for the standard normal distribution N(0,1).\n        # U = mu + sigma * xi, where xi ~ N(0,1).\n        # E[g(U)] = Integral[g(mu + sigma*xi)*pdf_normal(xi)]dxi\n        # Change of vars: x = xi/sqrt(2) => xi = sqrt(2)*x.\n        # E[g(U)] = (1/sqrt(pi)) * Integral[g(mu + sigma*sqrt(2)*x)*exp(-x^2)]dx\n        # Approximation: (1/sqrt(pi)) * sum(w_j * g(mu + sigma*sqrt(2)*x_j))\n        U_nodes = mu + sigma * np.sqrt(2) * x_gh\n        prob_weights = w_gh / np.sqrt(np.pi)\n\n        Y_values_at_nodes = []\n        \n        # 3. Loop over quadrature nodes to evaluate the Quantity of Interest (QoI).\n        for U_j in U_nodes:\n            # 3a. Evaluate micro-model: compute conductivities k_i for each microcell.\n            i_vals = np.arange(M)\n            # The term (1 + alpha * sin(...)) is guaranteed > 0 by alpha in [0,1).\n            k_i = k0 * np.exp(U_j) * (1.0 + alpha * np.sin(2.0 * np.pi * i_vals / M))\n            \n            # 3b. Homogenization: compute effective diffusivity D_eff.\n            # D_eff is the harmonic mean of the k_i values.\n            D_eff = M / np.sum(1.0 / k_i)\n            \n            # 3c. Solve the macro-model BVP: -D_eff * u'' = q, with u(0)=u(1)=0.\n            # Use a finite difference method.\n            dx = 1.0 / (Nx - 1)\n            num_interior_points = Nx - 2\n            \n            if num_interior_points == 0:\n                # Handle edge cases like Nx=1 or Nx=2 where there are no interior points.\n                u_interior = np.array([])\n            else:\n                # Construct the tridiagonal matrix A for a 1D Laplacian.\n                main_diag = 2.0 * np.ones(num_interior_points)\n                off_diag = -1.0 * np.ones(num_interior_points - 1)\n                A = np.diag(main_diag) + np.diag(off_diag, k=1) + np.diag(off_diag, k=-1)\n                \n                # Construct the right-hand side vector b.\n                b = (q * dx**2 / D_eff) * np.ones(num_interior_points)\n                \n                # Solve the linear system A * u_interior = b.\n                u_interior = np.linalg.solve(A, b)\n            \n            # 3d. Compute the QoI, Y(U_j), which is the spatial average of u(x).\n            # This is integral of u from 0 to 1.\n            # We use the trapezoidal rule on the full solution vector [0, ...u_interior..., 0].\n            u_full = np.concatenate(([0.0], u_interior, [0.0]))\n            Y_j = np.trapz(u_full, dx=dx)\n            \n            Y_values_at_nodes.append(Y_j)\n\n        Y_values_at_nodes = np.array(Y_values_at_nodes)\n\n        # 4. Compute the mean and variance of the QoI using the quadrature rule.\n        # Mean E[Y]\n        mean_Y = np.dot(prob_weights, Y_values_at_nodes)\n        \n        # Variance Var[Y] = E[(Y - E[Y])^2]\n        # This is a numerically stable formulation.\n        var_Y = np.dot(prob_weights, (Y_values_at_nodes - mean_Y)**2)\n        \n        results.extend([mean_Y, var_Y])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在量化了模型输出的不确定性之后，下一个关键步骤是“归因”：确定哪些输入参数是输出不确定性的主要来源，这便是敏感性分析的范畴。本练习深入探讨了基于方差的敏感性分析，使用索博尔指数（Sobol' indices）将输出方差分解到不同输入源上。我们将特别处理一个重要且常见的情况：由于存在约束，输入变量之间并非相互独立。通过解析求解此问题，你将对敏感性指数的工作原理，以及输入依赖性如何显著影响分析结果，建立深刻的基础性理解。",
            "id": "4150979",
            "problem": "考虑一个程式化的复杂自适应系统，其中有两个交互主体，其可控输入由随机向量 $\\mathbf{X} = (X_1, X_2)$ 表示。可行的联合适应受到一个硬性资源约束的限制，该约束被建模为三角形域 $D = \\{(x_1,x_2) \\in \\mathbb{R}^2 : x_1 \\ge 0,\\, x_2 \\ge 0,\\, x_1 + x_2 \\le 1\\}$。假设系统在 $D$ 上均匀采样适应，即联合密度在 $D$ 上为常数，在其他地方为零。\n\n设标量系统输出为 $Y = f(X_1,X_2) = X_1 + X_2$。你需要通过推导和评估关于 $D$ 上受约束输入法则的一阶基于方差的灵敏度指数（常称为 Sobol' 指数），来量化该硬性约束如何改变对 $X_1$ 的基于方差的灵敏度分配。\n\n仅从方差、条件期望、条件方差的核心定义，一阶基于方差的灵敏度指数的定义\n$$\nS_1 \\equiv \\frac{\\mathrm{Var}\\!\\left(\\mathbb{E}[Y \\mid X_1]\\right)}{\\mathrm{Var}(Y)},\n$$\n以及 $D$ 上均匀测度的几何解释出发，完成以下任务：\n\n1. 推导由 $D$ 上的均匀分布导出的 $X_1$ 的边缘密度，以及在硬性约束确定的可行区间上 $X_2 \\mid X_1 = x_1$ 的条件密度。\n2. 仅使用这些基本事实，推导在 $D$ 上的受约束输入法则下 $\\mathbb{E}[Y \\mid X_1]$、$\\mathrm{Var}\\!\\left(\\mathbb{E}[Y \\mid X_1]\\right)$ 和 $\\mathrm{Var}(Y)$。\n3. 根据推导出的量构建 $S_1$。提供其精确值。\n\n单独地（最终数值无需计算），通过使用条件分布 $X_2 \\mid X_1 = x_1$ 对给定 $X_1$ 的条件独立复制对 $(Y, Y')$进行采样，提出一个在 $D$ 上受约束法则下对 $\\mathrm{Var}\\!\\left(\\mathbb{E}[Y \\mid X_1]\\right)$ 无偏的调整后蒙特卡洛（MC）估计器。明确说明您的估计器需要哪些条件分布和边缘分布。\n\n提供 $S_1$ 的精确值作为您的最终答案。不要四舍五入。无需单位。",
            "solution": "用户想要解决一个*复杂模型中的不确定性量化*领域的问题，这是*复杂自适应系统建模*中的一个课题。\n\n### 第1步：提取已知条件\n-   **随机输入**：$\\mathbf{X} = (X_1, X_2)$\n-   **可行域**：$D = \\{(x_1,x_2) \\in \\mathbb{R}^2 : x_1 \\ge 0,\\, x_2 \\ge 0,\\, x_1 + x_2 \\le 1\\}$\n-   **输入分布**：在域 $D$ 上均匀分布。联合概率密度函数（PDF），$p(x_1, x_2)$，在 $D$ 上为常数，在其他地方为 $0$。\n-   **系统输出**：$Y = f(X_1,X_2) = X_1 + X_2$\n-   **目标量**：一阶基于方差的灵敏度指数，$S_1 = \\frac{\\mathrm{Var}\\!\\left(\\mathbb{E}[Y \\mid X_1]\\right)}{\\mathrm{Var}(Y)}$。\n-   **任务1**：推导 $X_1$ 的边缘密度 $p(x_1)$ 和给定 $X_1$ 时 $X_2$ 的条件密度 $p(x_2 \\mid x_1)$。\n-   **任务2**：使用基本定义推导 $\\mathbb{E}[Y \\mid X_1]$、$\\mathrm{Var}\\!\\left(\\mathbb{E}[Y \\mid X_1]\\right)$ 和 $\\mathrm{Var}(Y)$。\n-   **任务3**：构建 $S_1$ 的精确值。\n-   **任务4**：基于条件独立复制，为 $\\mathrm{Var}\\!\\left(\\mathbb{E}[Y \\mid X_1]\\right)$ 提出一个无偏蒙特卡洛估计器。\n\n### 第2步：使用提取的已知条件进行验证\n这是概率论和全局灵敏度分析中的一个标准问题。\n-   **有科学依据**：该问题基于概率论和 Sobol' 灵敏度指数的成熟数学框架。该模型虽然是程式化的，但在数学上是合理的。\n-   **适定性**：域、概率分布和输出函数都有清晰的定义。待推导的量是标准的统计量。存在唯一且稳定的解。\n-   **客观性**：该问题以精确、形式化的数学语言陈述，没有歧义或主观陈述。\n该问题不违反任何无效标准。它是一个适定的、自洽的、在应用概率论中科学上有效的练习。\n\n### 第3步：结论与行动\n该问题是**有效的**。将提供完整解答。\n\n### 解题推导\n\n分析首先定义输入 $(X_1, X_2)$ 的联合概率密度函数（PDF）。输入在域 $D$ 上均匀分布，该域是一个顶点为 $(0,0)$、$(1,0)$ 和 $(0,1)$ 的直角三角形。这个三角形的面积是 $\\text{Area}(D) = \\frac{1}{2} \\times \\text{底} \\times \\text{高} = \\frac{1}{2} \\times 1 \\times 1 = \\frac{1}{2}$。\n由于分布是均匀的，联合PDF $p(x_1, x_2)$ 在 $D$ 内的任何点都是 $D$ 面积的倒数，在其他地方则为 $0$。\n$$\np(x_1, x_2) =\n\\begin{cases}\n\\frac{1}{\\text{Area}(D)} = 2,  \\text{if } x_1 \\ge 0, x_2 \\ge 0, x_1+x_2 \\le 1 \\\\\n0,  \\text{otherwise}\n\\end{cases}\n$$\n\n**1. 边缘和条件密度的推导**\n\n为了求 $X_1$ 的边缘密度 $p(x_1)$，我们将联合PDF对 $x_2$ 的所有可能值进行积分。对于给定的 $x_1 \\in [0, 1]$，变量 $x_2$ 被约束在区间 $[0, 1-x_1]$ 内。\n$$\np(x_1) = \\int_{-\\infty}^{\\infty} p(x_1, x_2) \\,dx_2 = \\int_0^{1-x_1} 2 \\,dx_2 = 2[x_2]_0^{1-x_1} = 2(1-x_1) \\quad \\text{for } x_1 \\in [0, 1]\n$$\n对于在 $[0, 1]$ 之外的 $x_1$，$p(x_1)=0$。\n\n条件密度 $p(x_2 \\mid x_1)$ 由公式 $p(x_2 \\mid x_1) = \\frac{p(x_1, x_2)}{p(x_1)}$ 定义，其中 $p(x_1) > 0$。这对于 $x_1 \\in [0, 1)$ 是有效的。\n$$\np(x_2 \\mid x_1) = \\frac{2}{2(1-x_1)} = \\frac{1}{1-x_1} \\quad \\text{for } x_2 \\in [0, 1-x_1]\n$$\n这表明对于给定的值 $X_1=x_1$，随机变量 $X_2$ 在区间 $[0, 1-x_1]$ 上均匀分布。\n\n**2. 方差分量的推导**\n\n我们现在推导 $S_1$ 所需的量。\n\n首先，我们计算给定 $X_1$ 时 $Y$ 的条件期望。\n$$\n\\mathbb{E}[Y \\mid X_1 = x_1] = \\mathbb{E}[X_1 + X_2 \\mid X_1 = x_1] = x_1 + \\mathbb{E}[X_2 \\mid X_1 = x_1]\n$$\n由于以 $X_1=x_1$ 为条件的 $X_2$ 在 $[0, 1-x_1]$ 上均匀分布，其期望是该区间的中点：\n$$\n\\mathbb{E}[X_2 \\mid X_1 = x_1] = \\frac{0 + (1-x_1)}{2} = \\frac{1-x_1}{2}\n$$\n将此代回，我们得到：\n$$\n\\mathbb{E}[Y \\mid X_1 = x_1] = x_1 + \\frac{1-x_1}{2} = \\frac{2x_1 + 1 - x_1}{2} = \\frac{x_1+1}{2}\n$$\n量 $\\mathbb{E}[Y \\mid X_1]$ 是一个随机变量，它是 $X_1$ 的函数：$\\mathbb{E}[Y \\mid X_1] = \\frac{X_1+1}{2}$。\n\n接下来，我们计算其方差 $\\mathrm{Var}\\!\\left(\\mathbb{E}[Y \\mid X_1]\\right)$。\n$$\n\\mathrm{Var}\\!\\left(\\mathbb{E}[Y \\mid X_1]\\right) = \\mathrm{Var}\\!\\left(\\frac{X_1+1}{2}\\right) = \\mathrm{Var}\\!\\left(\\frac{1}{2}X_1 + \\frac{1}{2}\\right) = \\left(\\frac{1}{2}\\right)^2 \\mathrm{Var}(X_1) = \\frac{1}{4}\\mathrm{Var}(X_1)\n$$\n为了求 $\\mathrm{Var}(X_1)$，我们首先需要 $\\mathbb{E}[X_1]$ 和 $\\mathbb{E}[X_1^2]$。使用边缘密度 $p(x_1)=2(1-x_1)$：\n$$\n\\mathbb{E}[X_1] = \\int_0^1 x_1 p(x_1) \\,dx_1 = \\int_0^1 x_1 \\cdot 2(1-x_1) \\,dx_1 = 2 \\int_0^1 (x_1 - x_1^2) \\,dx_1 = 2 \\left[\\frac{x_1^2}{2} - \\frac{x_1^3}{3}\\right]_0^1 = 2\\left(\\frac{1}{2} - \\frac{1}{3}\\right) = \\frac{1}{3}\n$$\n$$\n\\mathbb{E}[X_1^2] = \\int_0^1 x_1^2 p(x_1) \\,dx_1 = \\int_0^1 x_1^2 \\cdot 2(1-x_1) \\,dx_1 = 2 \\int_0^1 (x_1^2 - x_1^3) \\,dx_1 = 2 \\left[\\frac{x_1^3}{3} - \\frac{x_1^4}{4}\\right]_0^1 = 2\\left(\\frac{1}{3} - \\frac{1}{4}\\right) = \\frac{1}{6}\n$$\n现在我们计算 $X_1$ 的方差：\n$$\n\\mathrm{Var}(X_1) = \\mathbb{E}[X_1^2] - (\\mathbb{E}[X_1])^2 = \\frac{1}{6} - \\left(\\frac{1}{3}\\right)^2 = \\frac{1}{6} - \\frac{1}{9} = \\frac{3-2}{18} = \\frac{1}{18}\n$$\n因此，一阶效应对应的方差为：\n$$\n\\mathrm{Var}\\!\\left(\\mathbb{E}[Y \\mid X_1]\\right) = \\frac{1}{4}\\mathrm{Var}(X_1) = \\frac{1}{4} \\cdot \\frac{1}{18} = \\frac{1}{72}\n$$\n最后，我们计算总方差 $\\mathrm{Var}(Y)$。我们使用全方差公式：$\\mathrm{Var}(Y) = \\mathrm{Var}\\!\\left(\\mathbb{E}[Y \\mid X_1]\\right) + \\mathbb{E}\\left[\\mathrm{Var}(Y \\mid X_1)\\right]$。我们需要计算第二项。\n$$\n\\mathrm{Var}(Y \\mid X_1=x_1) = \\mathrm{Var}(x_1 + X_2 \\mid X_1=x_1) = \\mathrm{Var}(X_2 \\mid X_1=x_1)\n$$\n这是在 $[0, 1-x_1]$ 上的均匀分布的方差。在 $[a,b]$ 上的均匀分布的方差是 $\\frac{(b-a)^2}{12}$。\n$$\n\\mathrm{Var}(Y \\mid X_1=x_1) = \\frac{(1-x_1 - 0)^2}{12} = \\frac{(1-x_1)^2}{12}\n$$\n现在我们对这个量关于 $X_1$ 取期望：\n$$\n\\mathbb{E}\\left[\\mathrm{Var}(Y \\mid X_1)\\right] = \\mathbb{E}\\left[\\frac{(1-X_1)^2}{12}\\right] = \\frac{1}{12}\\mathbb{E}[(1-X_1)^2] = \\frac{1}{12}\\mathbb{E}[1 - 2X_1 + X_1^2]\n$$\n利用期望的线性性质以及我们先前得到的 $\\mathbb{E}[X_1]$ 和 $\\mathbb{E}[X_1^2]$ 的结果：\n$$\n\\mathbb{E}\\left[\\mathrm{Var(Y \\mid X_1)}\\right] = \\frac{1}{12}\\left(1 - 2\\mathbb{E}[X_1] + \\mathbb{E}[X_1^2]\\right) = \\frac{1}{12}\\left(1 - 2\\left(\\frac{1}{3}\\right) + \\frac{1}{6}\\right) = \\frac{1}{12}\\left(1 - \\frac{2}{3} + \\frac{1}{6}\\right) = \\frac{1}{12}\\left(\\frac{6-4+1}{6}\\right) = \\frac{1}{12}\\left(\\frac{3}{6}\\right) = \\frac{1}{24}\n$$\n总方差为：\n$$\n\\mathrm{Var}(Y) = \\mathrm{Var}\\!\\left(\\mathbb{E}[Y \\mid X_1]\\right) + \\mathbb{E}\\left[\\mathrm{Var}(Y \\mid X_1)\\right] = \\frac{1}{72} + \\frac{1}{24} = \\frac{1+3}{72} = \\frac{4}{72} = \\frac{1}{18}\n$$\n\n**3. 灵敏度指数 $S_1$ 的构建**\n\n现在我们可以用推导出的量来构建 Sobol' 指数 $S_1$。\n$$\nS_1 = \\frac{\\mathrm{Var}\\!\\left(\\mathbbE[Y \\mid X_1]\\right)}{\\mathrm{Var}(Y)} = \\frac{1/72}{1/18} = \\frac{1}{72} \\times \\frac{18}{1} = \\frac{18}{72} = \\frac{1}{4}\n$$\n\n**4. 提出的蒙特卡洛估计器**\n\n为了构建 $V_1 = \\mathrm{Var}\\!\\left(\\mathbb{E}[Y \\mid X_1]\\right)$ 的无偏蒙特卡洛估计器，我们利用性质 $V_1 = \\mathrm{Cov}(Y, Y')$，其中 $Y = f(X_1, X_2)$ 和 $Y' = f(X_1, X'_2)$，而 $X_2$ 和 $X'_2$ 是从条件分布 $p(x_2 \\mid x_1)$ 中抽取的独立同分布样本。对于来自 $N$ 对样本 $(a_i, b_i)$ 的两个随机变量 $A$ 和 $B$，其协方差的无偏估计器是样本协方差：\n$$\n\\widehat{\\mathrm{Cov}}(A, B) = \\frac{1}{N-1} \\sum_{i=1}^N (a_i - \\bar{a})(b_i - \\bar{b})\n$$\n估计过程如下：\n1.  对于 $i=1, \\dots, N$：\n    a. 从边缘分布 $p(x_1) = 2(1-x_1)$（对于 $x_1 \\in [0, 1]$）中抽取一个样本 $x_1^{(i)}$。\n    b. 从条件分布 $p(x_2 \\mid X_1=x_1^{(i)})$（即在 $[0, 1-x_1^{(i)}]$ 上的均匀分布）中抽取两个独立样本 $x_2^{(i)}$ 和 $x_2^{\\prime(i)}$。\n    c. 计算模型输出 $y^{(i)} = f(x_1^{(i)}, x_2^{(i)})$ 和 $y^{\\prime(i)} = f(x_1^{(i)}, x_2^{\\prime(i)})$。\n\n2.  计算样本均值 $\\bar{y} = \\frac{1}{N} \\sum_{i=1}^N y^{(i)}$ 和 $\\bar{y}' = \\frac{1}{N} \\sum_{i=1}^N y^{\\prime(i)}$。\n\n3.  $V_1$ 的无偏估计器即为两组输出之间的样本协方差：\n$$\n\\hat{V}_1 = \\frac{1}{N-1} \\sum_{i=1}^N (y^{(i)} - \\bar{y})(y^{\\prime(i)} - \\bar{y}')\n$$\n该估计器对于 $\\mathrm{Var}\\!\\left(\\mathbb{E}[Y \\mid X_1]\\right)$ 是无偏的，并且依赖于能够从边缘分布 $p(x_1)$ 和条件分布 $p(x_2 \\mid x_1)$ 中进行采样，这些分布已在问题的第一部分推导出。",
            "answer": "$$\\boxed{\\frac{1}{4}}$$"
        },
        {
            "introduction": "许多高级的UQ方法，尤其是在贝叶斯推断中，都依赖于马尔可夫链蒙特卡洛（MCMC）等计算机模拟。一个至关重要的问题是：我们如何知道模拟运行的时间是否足够长，从而得到了可靠的结果？本练习要求你实现吉尔曼-鲁宾诊断（Gelman-Rubin diagnostic, $\\hat{R}$），这是一个用于评估多个MCMC链是否收敛到同一后验分布的标准且必要的工具。掌握此诊断方法对于任何贝叶斯建模的实践者都至关重要，因为它确保了从MCMC模拟中获得的不确定性估计是可信的。",
            "id": "4150998",
            "problem": "你的任务是构建一个程序，为用于在复杂自适应系统建模中近似参数后验分布的多个独立马尔可夫链计算单变量 Gelman–Rubin 收敛诊断（Gelman–Rubin 也称为潜在尺度缩减因子）。你的目标是从第一性原理推导该诊断，并实现两个版本：经典 Gelman–Rubin 统计量和分裂链 Gelman–Rubin 统计量。你还必须解释这两种诊断的可靠性阈值。\n\n从以下基本基础开始：样本均值和样本方差作为独立同分布抽样的期望值和方差的估计量，以及全方差定律。假设有 $N$ 条独立的链，每条长度为 $M$，其值为 $\\{x_{j,t}\\}$，其中 $j \\in \\{1,\\dots,N\\}$ 为链的索引，$t \\in \\{1,\\dots,M\\}$ 为时间的索引。设特定链的样本均值为 $m_{j} = \\frac{1}{M}\\sum_{t=1}^{M} x_{j,t}$，特定链的样本方差为 $s_{j}^{2} = \\frac{1}{M-1}\\sum_{t=1}^{M} (x_{j,t} - m_{j})^{2}$，混合均值为 $\\bar{m} = \\frac{1}{N}\\sum_{j=1}^{N} m_{j}$。链间方差定义为\n$$\nB = \\frac{M}{N-1} \\sum_{j=1}^{N} (m_{j} - \\bar{m})^{2},\n$$\n链内方差为\n$$\nW = \\frac{1}{N} \\sum_{j=1}^{N} s_{j}^{2}.\n$$\n边际后验方差估计量为\n$$\n\\widehat{\\operatorname{Var}}^{+} = \\frac{M-1}{M}W + \\frac{1}{M}B,\n$$\n经典 Gelman–Rubin 统计量为\n$$\n\\hat{R} = \\sqrt{\\frac{\\widehat{\\operatorname{Var}}^{+}}{W}}.\n$$\n分裂链 Gelman–Rubin 统计量，表示为 $\\hat{R}_{\\mathrm{split}}$，通过将每条链分成长度为 $M/2$ 的两半（假设 $M$ 是偶数）得到，从而产生 $2N$ 条链，然后应用相同的公式，但用 $M/2$ 替换 $M$，用 $2N$ 替换 $N$ 来计算 $\\hat{R}_{\\mathrm{split}}$。\n\n你必须实现 $\\hat{R}$ 和 $\\hat{R}_{\\mathrm{split}}$，并解释用于不确定性估计的两个可靠性阈值：\n- 一个常规阈值 $\\tau_{1} = 1.05$，以及\n- 一个严格阈值 $\\tau_{2} = 1.01$。\n对于每个测试用例，报告 $\\hat{R}$ 和 $\\hat{R}_{\\mathrm{split}}$，以及两个布尔指标：$\\hat{R}_{\\mathrm{split}} \\le \\tau_{1}$ 和 $\\hat{R}_{\\mathrm{split}} \\le \\tau_{2}$ 是否成立，你应将它们解释为在相应阈值水平上，不确定性估计是充分可靠的。\n\n构建以下包含 $5$ 个案例的确定性测试套件。对于所有案例，使用基于零的时间索引 $t \\in \\{0,1,\\dots,M-1\\}$，并通过以下公式定义链值\n$$\nx_{j,t} = \\mu_{j} + A_{j} \\sin\\!\\left(2\\pi \\frac{k\\, t}{M} + \\phi_{j}\\right) + D_{j}\\left(2\\frac{t}{M} - 1\\right),\n$$\n参数 $(\\mu_{j}, A_{j}, \\phi_{j}, D_{j})$ 具体规定如下。角度以弧度为单位。所有整数计数和常量必须完全按给定值使用。\n\n测试套件：\n- 案例 1：$N=4$，$M=200$，$k=5$。相位 $\\phi_{j} \\in \\{0, \\frac{\\pi}{8}, \\frac{\\pi}{4}, \\frac{3\\pi}{8}\\}$ 按链索引顺序排列。对所有链，$\\mu_{j}=0$，$A_{j}=1$，$D_{j}=0$。\n- 案例 2：$N=4$，$M=200$，$k=5$。相位与案例 1 相同。对于链 $j \\in \\{1,2,3\\}$，$\\mu_{j}=0$，$A_{j}=1$，$D_{j}=0$。对于链 $j=4$，$\\mu_{4}=1.5$，$A_{4}=1$，$D_{4}=0$。\n- 案例 3：$N=4$，$M=200$，$k=5$。链：\n  - $j=1$：$\\mu_{1}=0$，$A_{1}=1$，$\\phi_{1}=0$，$D_{1}=0$；\n  - $j=2$：$\\mu_{2}=0$，$A_{2}=1$，$\\phi_{2}=\\frac{\\pi}{3}$，$D_{2}=0$；\n  - $j=3$：$\\mu_{3}=0$，$A_{3}=0.5$，$\\phi_{3}=\\frac{\\pi}{7}$，$D_{3}=0.9$；\n  - $j=4$：$\\mu_{4}=0$，$A_{4}=0.5$，$\\phi_{4}=\\frac{\\pi}{5}$，$D_{4}=-0.9$。\n- 案例 4：$N=4$，$M=10$，$k=1$。相位 $\\phi_{j} \\in \\{0, \\frac{\\pi}{8}, \\frac{\\pi}{4}, \\frac{3\\pi}{8}\\}$。对所有链，$\\mu_{j}=0$，$A_{j}=1$，$D_{j}=0$。\n- 案例 5：$N=4$，$M=200$，$k=5$。相位与案例 1 相同。链 $j \\in \\{1,2\\}$ 使用 $A_{j}=1$，链 $j \\in \\{3,4\\}$ 使用 $A_{j}=3$。对所有链，$\\mu_{j}=0$，$D_{j}=0$。\n\n计算与输出要求：\n- 对于每个案例，计算经典 Gelman–Rubin 统计量 $\\hat{R}$、分裂链统计量 $\\hat{R}_{\\mathrm{split}}$，以及两个指示 $\\hat{R}_{\\mathrm{split}} \\le \\tau_{1}$ 和 $\\hat{R}_{\\mathrm{split}} \\le \\tau_{2}$ 是否成立的布尔值，其中 $\\tau_{1} = 1.05$，$\\tau_{2} = 1.01$。\n- 你的程序应生成单行输出，其中包含一个用方括号括起来的、逗号分隔的扁平列表，其顺序为 $[\\hat{R}_{1}, \\hat{R}_{\\mathrm{split},1}, b_{1}^{(1.05)}, b_{1}^{(1.01)}, \\hat{R}_{2}, \\hat{R}_{\\mathrm{split},2}, b_{2}^{(1.05)}, b_{2}^{(1.01)}, \\dots, \\hat{R}_{5}, \\hat{R}_{\\mathrm{split},5}, b_{5}^{(1.05)}, b_{5}^{(1.01)}]$，其中 $\\hat{R}_{i}$ 和 $\\hat{R}_{\\mathrm{split},i}$ 是浮点数，$b_{i}^{(1.05)}$ 和 $b_{i}^{(1.01)}$ 是布尔值。\n- 此问题不涉及物理单位。",
            "solution": "在尝试提供解决方案之前，需对问题进行验证。\n\n### 步骤 1：提取已知条件\n\n- **目标**：计算经典 Gelman–Rubin 统计量（$\\hat{R}$）和分裂链 Gelman–Rubin 统计量（$\\hat{R}_{\\mathrm{split}}$）。\n- **输入**：$N$ 条独立的马尔可夫链，每条长度为 $M$。一条链的数据为 $\\{x_{j,t}\\}$，其中链索引 $j \\in \\{1,\\dots,N\\}$，时间索引 $t \\in \\{1,\\dots,M\\}$。\n- **定义**：\n  - 特定链的样本均值：$m_{j} = \\frac{1}{M}\\sum_{t=1}^{M} x_{j,t}$\n  - 特定链的样本方差：$s_{j}^{2} = \\frac{1}{M-1}\\sum_{t=1}^{M} (x_{j,t} - m_{j})^{2}$\n  - 混合均值：$\\bar{m} = \\frac{1}{N}\\sum_{j=1}^{N} m_{j}$\n- **公式**：\n  - 链间方差：$B = \\frac{M}{N-1} \\sum_{j=1}^{N} (m_{j} - \\bar{m})^{2}$\n  - 链内方差：$W = \\frac{1}{N} \\sum_{j=1}^{N} s_{j}^{2}$\n  - 边际后验方差估计量：$\\widehat{\\operatorname{Var}}^{+} = \\frac{M-1}{M}W + \\frac{1}{M}B$\n  - 经典 Gelman–Rubin 统计量：$\\hat{R} = \\sqrt{\\frac{\\widehat{\\operatorname{Var}}^{+}}{W}}$\n- **分裂链统计量 ($\\hat{R}_{\\mathrm{split}}$)**：将每条长度为 $M$ 的链分成长度为 $M/2$ 的两半（假设 $M$ 是偶数）。这会产生 $2N$ 条链。然后应用相同的公式，但用 $2N$ 替换 $N$，用 $M/2$ 替换 $M$。\n- **可靠性阈值**：\n  - 常规阈值：$\\tau_{1} = 1.05$\n  - 严格阈值：$\\tau_{2} = 1.01$\n- **链数据生成**：\n  - 时间索引：$t \\in \\{0,1,\\dots,M-1\\}$\n  - 公式：$x_{j,t} = \\mu_{j} + A_{j} \\sin\\!\\left(2\\pi \\frac{k\\, t}{M} + \\phi_{j}\\right) + D_{j}\\left(2\\frac{t}{M} - 1\\right)$\n- **测试套件**：\n  - 案例 1：$N=4$，$M=200$，$k=5$。$\\phi_{j} \\in \\{0, \\frac{\\pi}{8}, \\frac{\\pi}{4}, \\frac{3\\pi}{8}\\}$。对所有链，$\\mu_{j}=0$，$A_{j}=1$，$D_{j}=0$。\n  - 案例 2：$N=4$，$M=200$，$k=5$。$\\phi_{j}$ 与案例 1 相同。对于 $j \\in \\{1,2,3\\}$：$\\mu_{j}=0, A_{j}=1, D_{j}=0$。对于 $j=4$：$\\mu_{4}=1.5, A_{4}=1, D_{4}=0$。\n  - 案例 3：$N=4$，$M=200$，$k=5$。\n    - $j=1$：$\\mu_{1}=0, A_{1}=1, \\phi_{1}=0, D_{1}=0$。\n    - $j=2$：$\\mu_{2}=0, A_{2}=1, \\phi_{2}=\\frac{\\pi}{3}, D_{2}=0$。\n    - $j=3$：$\\mu_{3}=0, A_{3}=0.5, \\phi_{3}=\\frac{\\pi}{7}, D_{3}=0.9$。\n    - $j=4$：$\\mu_{4}=0, A_{4}=0.5, \\phi_{4}=\\frac{\\pi}{5}, D_{4}=-0.9$。\n  - 案例 4：$N=4$，$M=10$，$k=1$。$\\phi_{j}$ 与案例 1 相同。对所有链，$\\mu_{j}=0, A_{j}=1, D_{j}=0$。\n  - 案例 5：$N=4$，$M=200$，$k=5$。$\\phi_{j}$ 与案例 1 相同。对于 $j \\in \\{1,2\\}$：$A_{j}=1$。对于 $j \\in \\{3,4\\}$：$A_{j}=3$。对所有链，$\\mu_{j}=0, D_{j}=0$。\n- **输出要求**：单行、逗号分隔的扁平列表：$[\\hat{R}_{1}, \\hat{R}_{\\mathrm{split},1}, b_{1}^{(1.05)}, b_{1}^{(1.01)}, \\dots]$，其中 $b$ 是 $\\hat{R}_{\\mathrm{split}} \\le \\tau$ 的布尔值。\n\n### 步骤 2：使用提取的已知条件进行验证\n\n根据验证标准对问题进行审查。\n\n1.  **科学或事实上的不健全**：Gelman–Rubin 诊断是贝叶斯统计学中一种标准的、行之有效的方法，用于评估马尔可夫链蒙特卡洛（MCMC）模拟的收敛性。所提供的 $W$、$B$、$\\widehat{\\operatorname{Var}}^{+}$ 和 $\\hat{R}$ 的公式是正确的，直接取自统计学文献。使用确定性的正弦测试数据而非随机的 MCMC 输出是一种抽象，但它有助于创建一个定义明确的数值问题，以测试算法的实现。这并未违反任何科学原理；它仅仅是为了本次练习而简化了数据生成过程。\n2.  **无法形式化或不相关**：该问题高度可形式化，并直接与不确定性量化相关，这是复杂系统建模中的一个核心课题。Gelman-Rubin 统计量是确保从 MCMC 获得的后验分布可靠性的关键工具，这些后验分布用于量化参数不确定性。\n3.  **不完整或矛盾的设置**：计算所需的所有参数、常数和公式都已明确提供。测试案例已完全指定。没有矛盾之处。\n4.  **不切实际或不可行**：计算在数值上是可行的。参数在合理的计算范围内。\n5.  **不适定或结构不良**：问题是适定的。对于给定的确定性输入，存在唯一的数值解。定义清晰明确。\n6.  **伪深刻、琐碎或同义反复**：该问题要求对一个非琐碎的统计算法进行仔细和正确的实现。它涉及基于既定理论的几个不同计算步骤，使其成为一项实质性任务。\n7.  **超出科学可验证性**：结果是数值上确定的，并且可以独立验证。\n\n### 步骤 3：结论与行动\n\n问题是**有效的**。这是一个定义明确、有科学依据的计算问题。将提供完整的解决方案。\n\n### 基于原理的解决方案设计\n\nGelman–Rubin 收敛诊断，或潜在尺度缩减因子（$\\hat{R}$），是评估多个马尔可夫链是否收敛到一个共同的平稳分布的基本工具。其基本原理是比较单个链内的方差与链之间的方差。\n\n假设有 $N$ 条并行链，每条链有 $M$ 个预热后的样本，记为 $\\{x_{j,t}\\}$，其中 $j$ 是链的索引（$1, \\dots, N$），$t$ 是样本的索引（$1, \\dots, M$）。\n\n1.  **链内方差（$W$）**：我们首先计算每条链内的方差。链 $j$ 的样本方差由 $s_{j}^{2} = \\frac{1}{M-1}\\sum_{t=1}^{M} (x_{j,t} - m_{j})^{2}$ 给出，其中 $m_{j}$ 是链 $j$ 的均值。链内方差 $W = \\frac{1}{N} \\sum_{j=1}^{N} s_{j}^{2}$ 是这些单个方差的平均值。在模拟的早期阶段，当链尚未完全探索目标分布时，$W$ 往往是对真实后验方差的低估。\n\n2.  **链间方差（$B$）**：接下来，我们计算各链均值之间的方差。使用混合均值 $\\bar{m} = \\frac{1}{N}\\sum_{j=1}^{N} m_{j}$，链间方差为 $B = \\frac{M}{N-1} \\sum_{j=1}^{N} (m_{j} - \\bar{m})^{2}$。因子 $M$ 对 $B$ 进行缩放，使其与 $W$ 可比。如果链尚未收敛，它们的均值 $m_j$ 将会分散，导致 $B$ 值较大。当它们收敛到相同的分布时，它们的均值将相互接近，$B$ 将趋近于零。\n\n3.  **后验方差估计量（$\\widehat{\\operatorname{Var}}^{+}$）**：通过组合 $W$ 和 $B$ 来构造参数的边际后验方差的估计：$\\widehat{\\operatorname{Var}}^{+} = \\frac{M-1}{M}W + \\frac{1}{M}B$。这是链内方差和链间方差的加权平均。如果链从一个过离散分布开始，$\\widehat{\\operatorname{Var}}^{+}$ 是对真实方差的高估，这修正了仅由 $W$ 提供的低估。\n\n4.  **潜在尺度缩减因子（$\\hat{R}$）**：Gelman–Rubin 统计量是高估的方差与低估的方差之比：$\\hat{R} = \\sqrt{\\frac{\\widehat{\\operatorname{Var}}^{+}}{W}}$。随着模拟收敛，链之间的差异消失，导致 $B$ 变小。因此，$\\widehat{\\operatorname{Var}}^{+}$ 接近 $W$，$\\hat{R}$ 接近 $1$。一个远大于 $1$ 的 $\\hat{R}$ 值表明链内方差和链间方差不同，这标志着缺乏收敛。\n\n5.  **分裂链统计量（$\\hat{R}_{\\mathrm{split}}$）**：该变体解决了链内的潜在非平稳性问题。将 $N$ 条长度为 $M$ 的链各自拆分为两个不重叠的一半，从而创建一个新的包含 $2N$ 条链的集合，每条链长度为 $M/2$。然后为这个新的链集合计算 $\\hat{R}$ 统计量。接近 $1$ 的 $\\hat{R}_{\\mathrm{split}}$ 值表明链的前半部分和后半部分在统计上是相似的，为平稳性提供了证据。\n\n### 算法实现\n\n将设计一个辅助函数来计算给定链集合的 $\\hat{R}$，该集合表示为一个二维数组。对于每个测试案例，该函数将被调用两次：一次使用原始链计算 $\\hat{R}$，一次使用分裂链计算 $\\hat{R}_{\\mathrm{split}}$。\n\n对于每个测试案例，将首先根据指定的确定性公式生成链数据 $x_{j,t}$。主循环将遍历五个测试案例，对每个案例执行以下步骤：\n1.  生成 $N \\times M$ 的链数据矩阵。\n2.  使用完整矩阵计算 $\\hat{R}$。\n3.  将矩阵分成两半，创建一个 $2N \\times (M/2)$ 的矩阵。\n4.  使用分裂矩阵计算 $\\hat{R}_{\\mathrm{split}}$。\n5.  评估布尔条件 $\\hat{R}_{\\mathrm{split}} \\le \\tau_{1}$（其中 $\\tau_1=1.05$）和 $\\hat{R}_{\\mathrm{split}} \\le \\tau_{2}$（其中 $\\tau_2=1.01$）。\n6.  收集四个结果值：$\\hat{R}$、$\\hat{R}_{\\mathrm{split}}$ 和两个布尔值。\n\n所有测试用例的最终结果列表将被展平并格式化为所需的输出字符串。所有计算都将使用 `numpy` 库来完成，以保证效率和正确性。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes Gelman-Rubin diagnostics for a deterministic test suite.\n    \"\"\"\n\n    def _calculate_r_hat(chains: np.ndarray) -> float:\n        \"\"\"\n        Calculates the Gelman-Rubin statistic for a set of chains.\n\n        Args:\n            chains: A 2D numpy array of shape (N, M) where N is the number of\n                    chains and M is the length of each chain.\n\n        Returns:\n            The Gelman-Rubin statistic_hat.\n        \"\"\"\n        if chains.ndim != 2:\n            raise ValueError(\"Input `chains` must be a 2D array.\")\n        \n        N, M = chains.shape\n        \n        if N  2:\n            # Cannot compute between-chain variance with fewer than 2 chains.\n            # In a converged state, R-hat should be 1.\n            return 1.0\n\n        # Calculate chain-specific means\n        m_j = np.mean(chains, axis=1)\n\n        # Calculate chain-specific sample variances (ddof=1 for sample variance)\n        s_j_sq = np.var(chains, axis=1, ddof=1)\n\n        # Calculate within-chain variance W\n        W = np.mean(s_j_sq)\n\n        # If W is zero, all chains are constant.\n        if W == 0:\n            # If all chain means are also equal, B is 0, converged.\n            if np.all(m_j == m_j[0]):\n                return 1.0\n            # Otherwise, chains are at different constants, not converged.\n            else:\n                return np.inf\n\n        # Calculate pooled mean\n        m_bar = np.mean(m_j)\n\n        # Calculate between-chain variance B\n        B = (M / (N - 1)) * np.sum((m_j - m_bar)**2)\n        \n        # Alternative calculation for B using numpy's sample variance\n        # B = M * np.var(m_j, ddof=1)\n\n        # Calculate marginal posterior variance estimator\n        var_plus = ((M - 1) / M) * W + (1 / M) * B\n\n        # Calculate Gelman-Rubin statistic R-hat\n        r_hat = np.sqrt(var_plus / W)\n\n        return r_hat\n\n    # Define the reliability thresholds\n    tau1 = 1.05\n    tau2 = 1.01\n\n    # Define the 5 test cases\n    # Parameters for x_{j,t} = mu_j + A_j*sin(2*pi*k*t/M + phi_j) + D_j*(2*t/M - 1)\n    test_cases = [\n        # Case 1\n        {\n            \"N\": 4, \"M\": 200, \"k\": 5,\n            \"params\": [\n                {\"mu\": 0, \"A\": 1, \"phi\": 0 * np.pi / 8, \"D\": 0},\n                {\"mu\": 0, \"A\": 1, \"phi\": 1 * np.pi / 8, \"D\": 0},\n                {\"mu\": 0, \"A\": 1, \"phi\": 2 * np.pi / 8, \"D\": 0},\n                {\"mu\": 0, \"A\": 1, \"phi\": 3 * np.pi / 8, \"D\": 0},\n            ]\n        },\n        # Case 2\n        {\n            \"N\": 4, \"M\": 200, \"k\": 5,\n            \"params\": [\n                {\"mu\": 0,   \"A\": 1, \"phi\": 0 * np.pi / 8, \"D\": 0},\n                {\"mu\": 0,   \"A\": 1, \"phi\": 1 * np.pi / 8, \"D\": 0},\n                {\"mu\": 0,   \"A\": 1, \"phi\": 2 * np.pi / 8, \"D\": 0},\n                {\"mu\": 1.5, \"A\": 1, \"phi\": 3 * np.pi / 8, \"D\": 0},\n            ]\n        },\n        # Case 3\n        {\n            \"N\": 4, \"M\": 200, \"k\": 5,\n            \"params\": [\n                {\"mu\": 0, \"A\": 1.0, \"phi\": 0 * np.pi/7, \"D\":  0.0},\n                {\"mu\": 0, \"A\": 1.0, \"phi\": 1 * np.pi/3, \"D\":  0.0},\n                {\"mu\": 0, \"A\": 0.5, \"phi\": 1 * np.pi/7, \"D\":  0.9},\n                {\"mu\": 0, \"A\": 0.5, \"phi\": 1 * np.pi/5, \"D\": -0.9},\n            ]\n        },\n        # Case 4\n        {\n            \"N\": 4, \"M\": 10, \"k\": 1,\n            \"params\": [\n                {\"mu\": 0, \"A\": 1, \"phi\": 0 * np.pi / 8, \"D\": 0},\n                {\"mu\": 0, \"A\": 1, \"phi\": 1 * np.pi / 8, \"D\": 0},\n                {\"mu\": 0, \"A\": 1, \"phi\": 2 * np.pi / 8, \"D\": 0},\n                {\"mu\": 0, \"A\": 1, \"phi\": 3 * np.pi / 8, \"D\": 0},\n            ]\n        },\n        # Case 5\n        {\n            \"N\": 4, \"M\": 200, \"k\": 5,\n            \"params\": [\n                {\"mu\": 0, \"A\": 1, \"phi\": 0 * np.pi / 8, \"D\": 0},\n                {\"mu\": 0, \"A\": 1, \"phi\": 1 * np.pi / 8, \"D\": 0},\n                {\"mu\": 0, \"A\": 3, \"phi\": 2 * np.pi / 8, \"D\": 0},\n                {\"mu\": 0, \"A\": 3, \"phi\": 3 * np.pi / 8, \"D\": 0},\n            ]\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        N, M, k = case[\"N\"], case[\"M\"], case[\"k\"]\n        params = case[\"params\"]\n        \n        # Generate chain data\n        t = np.arange(M)\n        chains = np.zeros((N, M))\n        for j in range(N):\n            p = params[j]\n            mu_j, A_j, phi_j, D_j = p[\"mu\"], p[\"A\"], p[\"phi\"], p[\"D\"]\n            chains[j, :] = mu_j + A_j * np.sin(2 * np.pi * k * t / M + phi_j) \\\n                           + D_j * (2 * t / M - 1)\n\n        # Calculate classical R-hat\n        r_hat = _calculate_r_hat(chains)\n        \n        # Calculate split-chain R-hat\n        if M % 2 != 0:\n            raise ValueError(f\"M must be even for split-chain analysis. M={M}\")\n        \n        M_half = M // 2\n        chains_split1 = chains[:, :M_half]\n        chains_split2 = chains[:, M_half:]\n        split_chains = np.vstack((chains_split1, chains_split2))\n        \n        r_hat_split = _calculate_r_hat(split_chains)\n        \n        # Perform threshold checks\n        b1 = r_hat_split = tau1\n        b2 = r_hat_split = tau2\n        \n        # Append results for this case\n        results.extend([r_hat, r_hat_split, b1, b2])\n\n    # Format output string\n    # Convert booleans to lowercase strings as per common convention, although not strictly required\n    # str(True) -> 'True', so map(str,...) is used directly as per template\n    formatted_results = [f\"{x:.10f}\" if isinstance(x, float) else str(x).lower() for x in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}