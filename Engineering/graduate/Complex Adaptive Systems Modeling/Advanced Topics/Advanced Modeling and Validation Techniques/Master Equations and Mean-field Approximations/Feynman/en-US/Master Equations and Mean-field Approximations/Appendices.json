{
    "hands_on_practices": [
        {
            "introduction": "The first step in analyzing any stochastic system is to map out its \"phase space\"—the set of all possible microscopic configurations it can occupy. This foundational exercise  involves constructing and counting the microstates for a simple epidemic model. While seemingly basic, this practice powerfully illustrates the \"curse of dimensionality,\" as you will see how the system's complexity grows exponentially, making direct computation on the full state space intractable and motivating the need for the approximation methods explored next.",
            "id": "4127080",
            "problem": "Consider a Susceptible–Infected–Susceptible (SIS) model with $N$ labeled agents evolving as a continuous-time Markov chain governed by a master equation for the probability distribution over microscopic configurations. Each agent $i\\in\\{1,2,\\dots,N\\}$ has a binary health state $\\,\\sigma_{i}\\in\\{0,1\\}\\,$, where $\\,\\sigma_{i}=0\\,$ denotes susceptible and $\\,\\sigma_{i}=1\\,$ denotes infected. A microstate is the $N$-dimensional binary vector $\\,\\boldsymbol{\\sigma}=(\\sigma_{1},\\sigma_{2},\\dots,\\sigma_{N})\\in\\{0,1\\}^{N}\\,$. Let the microstate space be\n$$\n\\Omega_{N}=\\{\\boldsymbol{\\sigma}\\in\\{0,1\\}^{N}\\}.\n$$\nThe master equation evolves the probability $\\,P(\\boldsymbol{\\sigma},t)\\,$ on $\\,\\Omega_{N}\\,$. A standard coarse-graining used in mean-field approximations maps $\\,\\boldsymbol{\\sigma}\\,$ to the macrostate infection count $\\,m(\\boldsymbol{\\sigma})=\\sum_{i=1}^{N}\\sigma_{i}\\,$.\n\nUsing first principles (without invoking any shortcut formulas in the problem statement), construct $\\,\\Omega_{N}\\,$ and derive a closed-form expression, in terms of $\\,N\\,$, for the total number of distinct microstates $\\,|\\Omega_{N}|\\,$. Your final answer must be a single closed-form expression. No rounding is required. Do not include units.",
            "solution": "The problem requires the construction of the microstate space $\\Omega_N$ and the derivation of its size, $|\\Omega_N|$, from first principles.\n\n**Problem Validation**\n\nFirst, I will validate the problem statement.\n\n*   **Step 1: Extract Givens**\n    *   Model: Susceptible–Infected–Susceptible (SIS) model.\n    *   Number of agents: $N$.\n    *   Agents are labeled and indexed by $i \\in \\{1, 2, \\dots, N\\}$.\n    *   The state of agent $i$ is $\\sigma_i \\in \\{0, 1\\}$, where $\\sigma_i=0$ is susceptible and $\\sigma_i=1$ is infected.\n    *   A microstate is an $N$-dimensional binary vector $\\boldsymbol{\\sigma} = (\\sigma_1, \\sigma_2, \\dots, \\sigma_N) \\in \\{0, 1\\}^N$.\n    *   The microstate space is $\\Omega_N = \\{\\boldsymbol{\\sigma} \\in \\{0, 1\\}^N\\}$.\n    *   The task is to derive a closed-form expression for $|\\Omega_N|$ in terms of $N$.\n\n*   **Step 2: Validate Using Extracted Givens**\n    *   **Scientific Grounding**: The problem is well-grounded. The SIS model and the concept of a microscopic state space are fundamental pillars of statistical mechanics and the study of complex adaptive systems. The formulation is standard and correct.\n    *   **Well-Posedness**: The problem is well-posed. The set $\\Omega_N$ is unambiguously defined as the set of all possible $N$-tuples where each element is either $0$ or $1$. Determining the cardinality of this finite set is a well-defined mathematical task with a unique solution.\n    *   **Objectivity**: The problem statement is objective, using precise mathematical language and established scientific terminology.\n    *   **Conclusion**: The problem is free from scientific unsoundness, ambiguity, and contradiction. It is a valid, formalizable problem within the specified domain.\n\n*   **Step 3: Verdict and Action**\n    *   The problem is valid. I will proceed with the derivation.\n\n**Derivation**\n\nThe problem defines a microscopic configuration, or microstate, of the system as an $N$-dimensional vector $\\boldsymbol{\\sigma} = (\\sigma_1, \\sigma_2, \\dots, \\sigma_N)$. Each component $\\sigma_i$ of this vector represents the state of the $i$-th agent. The agents are labeled, meaning the order of the components in the vector is significant; for example, if $N=2$, the state $(1, 0)$ (agent $1$ infected, agent $2$ susceptible) is distinct from the state $(0, 1)$ (agent $1$ susceptible, agent $2$ infected).\n\nThe state of each individual agent $i$ is restricted to the set $\\{0, 1\\}$. Thus, there are exactly two possible states for each agent.\n\nTo construct the entire space of microstates, $\\Omega_N$, we must consider all possible combinations of states for the $N$ agents. We can derive the total number of such combinations, $|\\Omega_N|$, using the fundamental principle of counting (also known as the rule of product).\n\nLet's build the argument from first principles:\n\n1.  For the first agent ($i=1$), its state $\\sigma_1$ can be either $0$ or $1$. There are $2$ choices.\n2.  For the second agent ($i=2$), its state $\\sigma_2$ can also be either $0$ or $1$, independently of the state of the first agent. There are $2$ choices for $\\sigma_2$.\n3.  Continuing this for all $N$ agents, for each agent $i \\in \\{1, 2, \\dots, N\\}$, there are $2$ independent choices for its state $\\sigma_i$.\n\nThe total number of distinct microstates is the product of the number of choices for each agent's state. Since there are $N$ agents and each has $2$ possible states, the total number of microstates is the product of $2$ with itself $N$ times.\n\nTotal number of microstates $= \\underbrace{2 \\times 2 \\times \\dots \\times 2}_{N \\text{ times}}$.\n\nThis product is, by definition, $2^N$.\n\nFormally, the microstate space $\\Omega_N$ is defined as the set of all functions from the set of agent indices $\\{1, 2, \\dots, N\\}$ to the set of states $\\{0, 1\\}$. A more direct formalization given in the problem is that $\\Omega_N$ is the $N$-fold Cartesian product of the set $\\{0, 1\\}$ with itself:\n$$\n\\Omega_N = \\{0, 1\\}^N = \\underbrace{\\{0, 1\\} \\times \\{0, 1\\} \\times \\dots \\times \\{0, 1\\}}_{N \\text{ times}}\n$$\nThe cardinality of a Cartesian product of finite sets is the product of their individual cardinalities. The set of states for a single agent is $\\{0, 1\\}$, and its cardinality is $|\\{0, 1\\}| = 2$.\n\nTherefore, the cardinality of the microstate space $\\Omega_N$ is:\n$$\n|\\Omega_N| = |\\{0, 1\\}^N| = |\\{0, 1\\}|^N = 2^N\n$$\n\nFor example:\n- If $N=1$, the microstates are $(0)$ and $(1)$. $|\\Omega_1| = 2^1 = 2$.\n- If $N=2$, the microstates are $(0, 0)$, $(0, 1)$, $(1, 0)$, and $(1, 1)$. $|\\Omega_2| = 2^2 = 4$.\n- If $N=3$, the microstates are $(0, 0, 0)$, $(0, 0, 1)$, $(0, 1, 0)$, $(0, 1, 1)$, $(1, 0, 0)$, $(1, 0, 1)$, $(1, 1, 0)$, and $(1, 1, 1)$. $|\\Omega_3| = 2^3 = 8$.\n\nThe closed-form expression for the total number of distinct microstates, $|\\Omega_N|$, in terms of $N$ is thus $2^N$.",
            "answer": "$$\\boxed{2^{N}}$$"
        },
        {
            "introduction": "Since tracking the probability of every microstate is often impossible, a common strategy is to instead describe the system using its statistical moments, like the mean and variance. This practice  guides you in deriving the time-evolution equations for these moments directly from the master equation. In doing so, you will encounter the fundamental \"moment hierarchy problem\" and then apply a Gaussian moment-closure, a classic mean-field technique used to create a self-contained, solvable system of equations.",
            "id": "4127059",
            "problem": "Consider a continuous-time Markov birth–death process on the nonnegative integers with state variable $n \\in \\{0,1,2,\\dots\\}$ and probability mass function $P_{n}(t)$. The process has density-dependent rates given by the birth rate $b(n) = \\lambda n$ and the death rate $d(n) = \\mu n + \\gamma n(n-1)$, where $\\lambda$, $\\mu$, and $\\gamma$ are nonnegative constants. The master equation is the standard forward Kolmogorov equation for one-step processes.\n\nLet $m(t) \\equiv \\langle n \\rangle$ denote the mean, and let $V(t) \\equiv \\langle (n - m)^{2} \\rangle$ denote the variance. Define the third and fourth central moments as $C_{3}(t) \\equiv \\langle (n - m)^{3} \\rangle$ and $C_{4}(t) \\equiv \\langle (n - m)^{4} \\rangle$, respectively.\n\nStarting from the master equation and the definition of moments, derive the exact time-evolution equations for $m(t)$ and $V(t)$ and show that these equations depend on $C_{3}(t)$ and, at the next order, on $C_{4}(t)$. Then, impose a Gaussian moment-closure assumption: assume that, for each $t$, the distribution of $n$ is well-approximated by a normal distribution with mean $m(t)$ and variance $V(t)$.\n\nUnder this Gaussian closure, express $C_{3}(t)$ and $C_{4}(t)$ solely in terms of $m(t)$ and $V(t)$. Provide your final answer as a single row vector containing the two expressions in terms of $m$ and $V$. No rounding is required, and no units are needed.",
            "solution": "The user wants to solve a problem about a birth-death process.\n\n### Step 1: Extract Givens\n-   **Process:** Continuous-time Markov birth–death process.\n-   **State space:** Nonnegative integers, $n \\in \\{0, 1, 2, \\dots\\}$.\n-   **Probability mass function:** $P_n(t)$.\n-   **Birth rate:** $b(n) = \\lambda n$, where $\\lambda \\ge 0$.\n-   **Death rate:** $d(n) = \\mu n + \\gamma n(n-1)$, where $\\mu, \\gamma \\ge 0$.\n-   **Master equation:** Standard forward Kolmogorov equation for one-step processes.\n-   **Definitions of moments:**\n    -   Mean: $m(t) \\equiv \\langle n \\rangle = \\sum_{n=0}^\\infty n P_n(t)$.\n    -   Variance: $V(t) \\equiv \\langle (n - m)^2 \\rangle = \\langle n^2 \\rangle - m^2$.\n    -   Third central moment: $C_3(t) \\equiv \\langle (n - m)^3 \\rangle$.\n    -   Fourth central moment: $C_4(t) \\equiv \\langle (n - m)^4 \\rangle$.\n-   **Task 1:** Derive the time-evolution equations for $m(t)$ and $V(t)$ from the master equation and show their dependency on higher-order central moments ($C_3$ and $C_4$).\n-   **Task 2:** Impose a Gaussian moment-closure assumption, where the distribution of $n$ is approximated by a normal distribution with mean $m(t)$ and variance $V(t)$.\n-   **Task 3:** Under this closure, express $C_3(t)$ and $C_4(t)$ solely in terms of $m(t)$ and $V(t)$.\n-   **Final Answer Format:** A single row vector containing the expressions for $C_3(t)$ and $C_4(t)$.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded:** The problem describes a standard birth-death process, a fundamental model in stochastic processes, statistical physics, and population biology. The rates $b(n) = \\lambda n$ (linear birth) and $d(n) = \\mu n + \\gamma n(n-1)$ (linear death plus quadratic death due to competition) are standard and physically meaningful. The use of a master equation, moment analysis, and moment-closure approximations (specifically Gaussian closure) are all standard, well-established techniques in the field of complex adaptive systems modeling. The problem is scientifically and mathematically sound.\n2.  **Well-Posed:** The problem provides all necessary definitions and asks for a specific derivation and the application of a clearly defined approximation. The properties of the normal distribution, on which the Gaussian closure relies, are unambiguous, leading to a unique solution for the stated task.\n3.  **Objective:** The problem is stated in precise mathematical language, free of ambiguity, subjectivity, or opinion.\n4.  **Consistency and Completeness:** The givens are self-consistent and sufficient to solve the problem. There are no missing definitions or contradictory constraints. The reference to $C_4$ in the first part, while slightly imprecise in its phrasing (the equations for $m$ and $V$ do not explicitly depend on $C_4$), correctly points to the next level of the moment hierarchy, which is relevant context for the concept of closure. This slight imprecision does not invalidate the problem's core task.\n5.  **Other Flaws:** The problem is not trivial, metaphorical, or tautological. It is a formalizable scientific problem that is verifiable using standard mathematical methods.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. I will proceed to provide a complete, reasoned solution.\n\n### Solution\nThe master equation for a general birth-death process is:\n$$ \\frac{dP_n(t)}{dt} = b(n-1)P_{n-1}(t) + d(n+1)P_{n+1}(t) - [b(n) + d(n)]P_n(t) $$\nThe time evolution of the expectation value of any function of $n$, $\\langle f(n) \\rangle$, can be derived from the master equation. A convenient method is to use the adjoint of the generator of the process, which yields the following general formula for the rate of change of $\\langle f(n) \\rangle$:\n$$ \\frac{d\\langle f(n) \\rangle}{dt} = \\langle b(n)(f(n+1) - f(n)) + d(n)(f(n-1) - f(n)) \\rangle $$\n\nWe first derive the equations for the mean $m(t)$ and variance $V(t)$ to illustrate the moment hierarchy.\n\n**Time Evolution of the Mean, $m(t)$**\nLet $f(n)=n$. Then $f(n+1)-f(n) = 1$ and $f(n-1)-f(n) = -1$.\n$$ \\frac{dm}{dt} = \\frac{d\\langle n \\rangle}{dt} = \\langle b(n)(1) + d(n)(-1) \\rangle = \\langle b(n) - d(n) \\rangle $$\nSubstituting the given rates $b(n) = \\lambda n$ and $d(n) = \\mu n + \\gamma n(n-1)$:\n$$ \\frac{dm}{dt} = \\langle \\lambda n - (\\mu n + \\gamma n(n-1)) \\rangle = \\langle (\\lambda - \\mu)n - \\gamma(n^2 - n) \\rangle = \\langle (\\lambda - \\mu + \\gamma)n - \\gamma n^2 \\rangle $$\nExpressing this in terms of the mean $m = \\langle n \\rangle$ and variance $V = \\langle n^2 \\rangle - m^2$ (so $\\langle n^2 \\rangle = V + m^2$):\n$$ \\frac{dm}{dt} = (\\lambda - \\mu + \\gamma)\\langle n \\rangle - \\gamma \\langle n^2 \\rangle = (\\lambda - \\mu + \\gamma)m - \\gamma(V + m^2) $$\nThis equation shows that the rate of change of the mean, $m$, depends on the variance, $V$, which is the second central moment.\n\n**Time Evolution of the Variance, $V(t)$**\nThe time evolution of the variance is given by $\\frac{dV}{dt} = \\frac{d\\langle n^2 \\rangle}{dt} - 2m \\frac{dm}{dt}$. First, we find the equation for $\\langle n^2 \\rangle$ by setting $f(n)=n^2$.\n$f(n+1)-f(n) = (n+1)^2-n^2 = 2n+1$.\n$f(n-1)-f(n) = (n-1)^2-n^2 = -2n+1$.\n$$ \\frac{d\\langle n^2 \\rangle}{dt} = \\langle b(n)(2n+1) + d(n)(-2n+1) \\rangle = \\langle (b(n)-d(n))(2n) + b(n)+d(n) \\rangle $$\n$$ \\frac{d\\langle n^2 \\rangle}{dt} = 2\\langle n(b(n)-d(n)) \\rangle + \\langle b(n)+d(n) \\rangle $$\nUsing the expressions for $b(n)$ and $d(n)$:\n$b(n)-d(n) = (\\lambda - \\mu + \\gamma)n - \\gamma n^2$.\n$b(n)+d(n) = \\lambda n + \\mu n + \\gamma n(n-1) = (\\lambda + \\mu - \\gamma)n + \\gamma n^2$.\nSo,\n$$ \\frac{d\\langle n^2 \\rangle}{dt} = 2\\langle n((\\lambda - \\mu + \\gamma)n - \\gamma n^2) \\rangle + \\langle (\\lambda + \\mu - \\gamma)n + \\gamma n^2 \\rangle $$\n$$ \\frac{d\\langle n^2 \\rangle}{dt} = 2(\\lambda - \\mu + \\gamma)\\langle n^2 \\rangle - 2\\gamma \\langle n^3 \\rangle + (\\lambda + \\mu - \\gamma)\\langle n \\rangle + \\gamma \\langle n^2 \\rangle $$\n$$ \\frac{d\\langle n^2 \\rangle}{dt} = (\\lambda + \\mu - \\gamma)m + (2\\lambda - 2\\mu + 3\\gamma)(V+m^2) - 2\\gamma\\langle n^3 \\rangle $$\nThis equation for $\\langle n^2 \\rangle$ depends on the third raw moment, $\\langle n^3 \\rangle$. Now we find $\\frac{dV}{dt}$:\n$$ \\frac{dV}{dt} = \\frac{d\\langle n^2 \\rangle}{dt} - 2m \\frac{dm}{dt} $$\n$$ \\frac{dV}{dt} = \\left[ (\\lambda + \\mu - \\gamma)m + (2\\lambda - 2\\mu + 3\\gamma)(V+m^2) - 2\\gamma\\langle n^3 \\rangle \\right] - 2m \\left[ (\\lambda - \\mu + \\gamma)m - \\gamma(V+m^2) \\right] $$\nTo show the dependency on central moments, we express $\\langle n^3 \\rangle$ in terms of $C_3$:\n$C_3 = \\langle (n-m)^3 \\rangle = \\langle n^3 - 3m n^2 + 3m^2 n - m^3 \\rangle = \\langle n^3 \\rangle - 3m\\langle n^2 \\rangle + 3m^2\\langle n \\rangle - m^3$.\n$\\langle n^3 \\rangle = C_3 + 3m(V+m^2) - 3m^3 + m^3 = C_3 + 3mV + m^3$.\nSubstituting this into the equation for $\\frac{d\\langle n^2 \\rangle}{dt}$ reveals its dependence on $C_3$. Consequently, the equation for $\\frac{dV}{dt}$ also depends on $C_3$. This creates an open hierarchy of moment equations: the equation for the $k$-th moment depends on the $(k+1)$-th moment. The equation for $C_3$ would similarly depend on $C_4$. To obtain a closed system of equations for $m$ and $V$, a moment-closure approximation is required.\n\n**Gaussian Moment Closure**\nThe problem requires imposing a Gaussian moment-closure assumption. This means we approximate the discrete probability distribution $P_n(t)$ at any time $t$ by a continuous normal (Gaussian) distribution with the same mean $m(t)$ and variance $V(t)$. The central moments of a normal distribution have well-known properties.\nThe $k$-th central moment is defined as $C_k = \\langle (n-m)^k \\rangle$.\n\nFor a normal distribution:\n1.  All odd central moments are zero. This is due to the symmetry of the Gaussian probability density function around its mean. The integrand for an odd central moment is an odd function (with respect to $n-m$) times a symmetric function, which integrates to zero. Therefore, the third central moment is:\n    $$ C_3(t) = 0 $$\n2.  The even central moments are given by the formula $C_{2k} = (2k-1)!! V^k$, where $(2k-1)!! = (2k-1)(2k-3)\\dots1$. For the fourth central moment ($k=2$):\n    $$ C_4(t) = (2(2)-1)!! V(t)^2 = 3!! V(t)^2 = (3 \\cdot 1)V(t)^2 $$\n    $$ C_4(t) = 3V(t)^2 $$\nThis result can also be stated through the kurtosis, $\\kappa = \\frac{C_4}{V^2} - 3$. For a normal distribution, the kurtosis is zero by definition, which yields $C_4 = 3V^2$.\n\nUnder the Gaussian closure approximation, the third and fourth central moments are expressed solely in terms of the variance $V(t)$ (and trivially, the mean $m(t)$). The time dependencies are usually suppressed for notational clarity.\n\nThe requested expressions are:\n-   $C_3 = 0$\n-   $C_4 = 3V^2$\n\nThese are presented as a single row vector.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0 & 3V^{2}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Mean-field approximations rest on the crucial assumption that correlations between individual agents become negligible in large systems—a principle known as \"propagation of chaos.\" This final practice  provides a direct, hands-on test of this cornerstone concept. By exactly solving the stationary master equation for a finite number of interacting agents, you will compute the two-agent correlation function and numerically verify that it vanishes as the system size $N$ increases, bridging the gap between the exact microscopic description and its simplified mean-field counterpart.",
            "id": "4127125",
            "problem": "Consider a fully connected, exchangeable, continuous-time Markov jump system of $N$ binary agents with states $x_i(t) \\in \\{0,1\\}$ for $i \\in \\{1,\\dots,N\\}$. Define the empirical mean $m_N(t) = \\frac{1}{N}\\sum_{i=1}^{N} x_i(t)$. Each agent flips according to mean-field rates: a $0 \\to 1$ transition for agent $i$ occurs with instantaneous rate $a + b \\, m_N(t)$, and a $1 \\to 0$ transition occurs with instantaneous rate $c$, where $a>0$, $b \\ge 0$, and $c>0$ are constants independent of $N$. Let $K(t) = \\sum_{i=1}^{N} x_i(t)$ denote the total number of ones. Because the system is exchangeable and fully connected, $K(t)$ evolves as a birth-death process on $\\{0,1,\\dots,N\\}$ with birth rate $B_k$ from $k$ to $k+1$ and death rate $D_k$ from $k$ to $k-1$. \n\nTask A (Modeling from first principles): Starting from the forward Kolmogorov master equation for continuous-time Markov jump processes and the definitions above, derive $B_k$ and $D_k$ for the process $K(t)$. Express $B_k$ and $D_k$ in terms of $N$, $k$, $a$, $b$, and $c$, without invoking any mean-field closure assumptions. Clearly identify any exchangeability properties you use.\n\nTask B (Stationary measure and correlations): For fixed $N$, assume the chain is ergodic. Using the birth-death structure and detailed balance for one-dimensional birth-death chains, construct the stationary distribution $\\pi_N(k)$ up to normalization in terms of a product of the ratios $\\frac{B_r}{D_{r+1}}$ for $r \\in \\{0,\\dots,k-1\\}$. Then normalize $\\pi_N$ to a probability distribution. Using exchangeability and combinatorial identities, express:\n- the stationary mean $m_N^\\star = \\mathbb{E}_{\\pi_N}[K]/N$,\n- the stationary pairwise moment $\\mathbb{E}_{\\pi_N}[x_i x_j]$ for $i\\neq j$,\n- the stationary connected two-point correlation $C_2^{(N)} = \\mathbb{E}_{\\pi_N}[x_i x_j] - \\left(\\mathbb{E}_{\\pi_N}[x_i]\\right)^2$,\nin terms of $\\pi_N$ and $N$. Do not assume factorization a priori.\n\nTask C (Propagation of chaos assessment via scaling): The propagation of chaos hypothesis in the mean-field setting predicts that the connected two-point correlation scales as $C_2^{(N)} = \\mathcal{O}(1/N)$ for large $N$ at stationarity. Design an algorithm that, for given $(N,a,b,c)$, computes $\\pi_N(k)$ exactly (up to floating-point precision) using the birth-death stationary measure construction, evaluates $C_2^{(N)}$, and returns the scaled quantity $N \\, C_2^{(N)}$. Your algorithm should be numerically stable for moderate $N$ and avoid overflow or underflow. You must compute the stationary distribution and the correlations exactly for the finite system; do not simulate trajectories.\n\nTest Suite and Output Specification:\n- Use the parameter sets below. For each case, compute the stationary scaled correlation $N \\, C_2^{(N)}$ as a floating-point number. No physical units are involved.\n- Test cases (each case is $(N,a,b,c)$):\n  1. $(20, 0.05, 2.0, 1.0)$,\n  2. $(50, 0.05, 2.0, 1.0)$,\n  3. $(200, 0.05, 2.0, 1.0)$,\n  4. $(20, 0.05, 1.2, 1.0)$,\n  5. $(50, 0.05, 1.2, 1.0)$,\n  6. $(200, 0.05, 1.2, 1.0)$,\n  7. $(2, 0.05, 2.0, 1.0)$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each floating-point number rounded to six decimal places, in the same order as the test cases. For example, the output format must be exactly like $[r_1,r_2,\\dots,r_7]$ where each $r_i$ is the computed value for the corresponding test case, rounded to six decimal places.\n\nYour implementation must be a complete, runnable program. It must not read any input and must not access any files or the network. It must contain the test suite specified above and print the required single-line output.",
            "solution": "The problem statement is assessed to be valid. It is scientifically grounded in the theory of continuous-time Markov processes, specifically birth-death processes, which are fundamental to modeling complex adaptive systems. The problem is well-posed, providing all necessary parameters and definitions to derive a unique, meaningful solution for the requested quantities. The language is objective and precise. The tasks are non-trivial and constitute a standard analysis workflow in statistical physics and mean-field theory. We may, therefore, proceed with the solution.\n\n### Task A: Derivation of Birth and Death Rates\n\nWe are given a system of $N$ agents, where the state of agent $i$ is $x_i(t) \\in \\{0, 1\\}$. The total number of agents in state $1$ is $K(t) = \\sum_{i=1}^{N} x_i(t)$. The evolution of $K(t)$ is a birth-death process on the state space $\\{0, 1, \\dots, N\\}$. We seek the birth rate $B_k$ (transition from $k \\to k+1$) and the death rate $D_k$ (transition from $k \\to k-1$).\n\nLet's consider the system when it is in the macroscopic state $K(t) = k$. This means there are $k$ agents in state $1$ and $N-k$ agents in state $0$. The empirical mean is $m_N(t) = K(t)/N = k/N$.\n\nA birth event corresponds to a single agent transitioning from state $0$ to state $1$.\nThe instantaneous rate for any specific agent $i$ with $x_i=0$ to transition to $x_i=1$ is given as $a + b \\, m_N(t) = a + b(k/N)$.\nSince the system is fully connected and exchangeable, all agents in state $0$ are identical and independent in their transition propensity. There are $N-k$ such agents.\nThe total rate of any $0 \\to 1$ transition occurring in the system is the sum of the individual transition rates of all agents in state $0$. Due to exchangeability, this is simply the number of agents in state $0$ multiplied by their common transition rate.\nTherefore, the birth rate $B_k$, which is the total rate of transition from state $k$ to $k+1$, is:\n$$B_k = (N-k) \\left( a + \\frac{b k}{N} \\right)$$\nThis rate is valid for $k \\in \\{0, 1, \\dots, N-1\\}$. For $k=N$, all agents are in state $1$, so $B_N = 0$.\n\nA death event corresponds to a single agent transitioning from state $1$ to state $0$.\nThe instantaneous rate for any specific agent $i$ with $x_i=1$ to transition to $x_i=0$ is given as the constant $c$.\nAt state $k$, there are $k$ agents in state $1$. As with the birth process, the exchangeability property implies that all agents in state $1$ are identical.\nThe total rate of any $1 \\to 0$ transition is the sum of the individual rates, which is the number of agents in state $1$ multiplied by their common transition rate.\nTherefore, the death rate $D_k$, which is the total rate of transition from state $k$ to $k-1$, is:\n$$D_k = k \\cdot c$$\nThis rate is valid for $k \\in \\{1, \\dots, N\\}$. For $k=0$, no agents are in state $1$, so $D_0 = 0$.\n\nThe derived rates are based solely on the problem definition and the property of exchangeability, without any mean-field closure assumptions.\n\n### Task B: Stationary Distribution and Correlation Function\n\nFor a one-dimensional birth-death process, the stationary distribution $\\pi_N(k)$ must satisfy the detailed balance condition for all $k \\in \\{0, 1, \\dots, N-1\\}$:\n$$\\pi_N(k) B_k = \\pi_N(k+1) D_{k+1}$$\nThis equation expresses the balance of probability flux between adjacent states at equilibrium. From this, we can establish a recurrence relation for $\\pi_N(k+1)$:\n$$\\pi_N(k+1) = \\pi_N(k) \\frac{B_k}{D_{k+1}}$$\nBy unrolling this recurrence from $k=0$ to an arbitrary state $k$, we express $\\pi_N(k)$ in terms of $\\pi_N(0)$:\n$$\\pi_N(k) = \\pi_N(0) \\prod_{r=0}^{k-1} \\frac{B_r}{D_{r+1}}$$\nThis gives the structure of the stationary distribution up to a normalization constant $\\pi_N(0)$. The product is defined as $1$ for $k=0$.\n\nTo normalize this distribution, we enforce the condition $\\sum_{k=0}^{N} \\pi_N(k) = 1$. Let $u_k = \\prod_{r=0}^{k-1} \\frac{B_r}{D_{r+1}}$ for $k > 0$ and $u_0 = 1$. Then:\n$$\\pi_N(0) \\sum_{k=0}^{N} u_k = 1 \\implies \\pi_N(0) = \\frac{1}{\\sum_{j=0}^{N} u_j}$$\nThe fully normalized stationary distribution is then:\n$$\\pi_N(k) = \\frac{u_k}{\\sum_{j=0}^{N} u_j} = \\frac{\\prod_{r=0}^{k-1} \\frac{B_r}{D_{r+1}}}{\\sum_{j=0}^{N} \\left( \\prod_{r=0}^{j-1} \\frac{B_r}{D_{r+1}} \\right)}$$\nUsing the expressions for $B_r$ and $D_{r+1}$:\n$$\\frac{B_r}{D_{r+1}} = \\frac{(N-r)\\left(a + \\frac{br}{N}\\right)}{(r+1)c}$$\n\nNow, we express the required stationary quantities.\n1.  **Stationary mean**: The expectation of the number of ones is $\\mathbb{E}_{\\pi_N}[K] = \\sum_{k=0}^{N} k \\, \\pi_N(k)$. The stationary mean density is:\n    $$m_N^\\star = \\frac{\\mathbb{E}_{\\pi_N}[K]}{N} = \\frac{1}{N} \\sum_{k=0}^{N} k \\, \\pi_N(k)$$\n    By exchangeability, the probability of any single agent $i$ being in state $1$ is the same, i.e., $\\mathbb{E}_{\\pi_N}[x_i]$ is independent of $i$. Thus, $\\mathbb{E}_{\\pi_N}[K] = \\mathbb{E}_{\\pi_N}[\\sum_{i=1}^{N} x_i] = \\sum_{i=1}^{N} \\mathbb{E}_{\\pi_N}[x_i] = N \\mathbb{E}_{\\pi_N}[x_i]$. It follows that $\\mathbb{E}_{\\pi_N}[x_i] = m_N^\\star$.\n\n2.  **Stationary pairwise moment**: We need $\\mathbb{E}_{\\pi_N}[x_i x_j]$ for $i \\neq j$. We use the moments of $K$.\n    $$K^2 = \\left(\\sum_{i=1}^{N} x_i\\right)^2 = \\sum_{i=1}^{N} x_i^2 + \\sum_{i \\neq j} x_i x_j$$\n    Since $x_i \\in \\{0, 1\\}$, we have $x_i^2 = x_i$. Thus:\n    $$K^2 = K + \\sum_{i \\neq j} x_i x_j$$\n    Taking the expectation with respect to $\\pi_N$:\n    $$\\mathbb{E}_{\\pi_N}[K^2] = \\mathbb{E}_{\\pi_N}[K] + \\mathbb{E}_{\\pi_N}\\left[\\sum_{i \\neq j} x_i x_j\\right]$$\n    There are $N(N-1)$ distinct pairs $(i,j)$ with $i \\neq j$. Due to exchangeability, $\\mathbb{E}_{\\pi_N}[x_i x_j]$ is identical for all such pairs.\n    $$\\mathbb{E}_{\\pi_N}\\left[\\sum_{i \\neq j} x_i x_j\\right] = N(N-1) \\mathbb{E}_{\\pi_N}[x_i x_j]$$\n    Rearranging to solve for the pairwise moment:\n    $$\\mathbb{E}_{\\pi_N}[x_i x_j] = \\frac{\\mathbb{E}_{\\pi_N}[K^2] - \\mathbb{E}_{\\pi_N}[K]}{N(N-1)}$$\n    where $\\mathbb{E}_{\\pi_N}[K^2] = \\sum_{k=0}^{N} k^2 \\, \\pi_N(k)$. This expression is valid for $N>1$.\n\n3.  **Stationary connected two-point correlation**: This is defined as $C_2^{(N)} = \\mathbb{E}_{\\pi_N}[x_i x_j] - (\\mathbb{E}_{\\pi_N}[x_i])^2$ for $i \\neq j$.\n    Substituting the expressions we found:\n    $$C_2^{(N)} = \\frac{\\mathbb{E}_{\\pi_N}[K^2] - \\mathbb{E}_{\\pi_N}[K]}{N(N-1)} - \\left(\\frac{\\mathbb{E}_{\\pi_N}[K]}{N}\\right)^2$$\n\n### Task C: Algorithm Design\n\nThe task is to compute the scaled correlation $N \\, C_2^{(N)}$. A direct computation of the products in the formula for $\\pi_N(k)$ can lead to numerical overflow or underflow, especially for moderate to large $N$. A stable algorithm should operate in log-space.\n\nThe algorithm proceeds as follows:\n1.  **Compute Unnormalized Log-Probabilities**: We compute $\\log u_k = \\log\\left(\\prod_{r=0}^{k-1} \\frac{B_r}{D_{r+1}}\\right)$. Instead of a direct product, we use a sum of logarithms, which is numerically stable. Let $\\text{log_u}[k]$ be an array storing $\\log u_k$.\n    -   Initialize $\\text{log_u}[0] = 0$.\n    -   For $k = 1, \\dots, N$, compute iteratively:\n        $$\\log u_k = \\log u_{k-1} + \\log\\left(\\frac{B_{k-1}}{D_k}\\right)$$\n        where $\\log\\left(\\frac{B_{k-1}}{D_k}\\right) = \\log\\left(N-(k-1)\\right) + \\log\\left(a + \\frac{b(k-1)}{N}\\right) - \\log(k) - \\log(c)$.\n\n2.  **Normalize Probabilities**: To compute $\\pi_N(k) = \\frac{\\exp(\\log u_k)}{\\sum_j \\exp(\\log u_j)}$, we use a standard stabilization trick. Let $L_{\\max} = \\max_k(\\log u_k)$.\n    $$\\pi_N(k) = \\frac{\\exp(\\log u_k - L_{\\max})}{\\sum_{j=0}^{N} \\exp(\\log u_j - L_{\\max})}$$\n    The exponents are now all less than or equal to $0$, preventing overflow. The denominator can be computed safely.\n\n3.  **Compute Moments**: Once the normalized probability vector $\\pi_N$ is obtained, the first two moments of $K$ are computed by summation:\n    -   $\\mathbb{E}_{\\pi_N}[K] = \\sum_{k=0}^{N} k \\cdot \\pi_N(k)$\n    -   $\\mathbb{E}_{\\pi_N}[K^2] = \\sum_{k=0}^{N} k^2 \\cdot \\pi_N(k)$\n\n4.  **Calculate Scaled Correlation**: Substitute the computed moments into the expression for $N \\, C_2^{(N)}$:\n    $$N \\, C_2^{(N)} = N \\left( \\frac{\\mathbb{E}_{\\pi_N}[K^2] - \\mathbb{E}_{\\pi_N}[K]}{N(N-1)} - \\left(\\frac{\\mathbb{E}_{\\pi_N}[K]}{N}\\right)^2 \\right)$$\n    $$N \\, C_2^{(N)} = \\frac{\\mathbb{E}_{\\pi_N}[K^2] - \\mathbb{E}_{\\pi_N}[K]}{N-1} - \\frac{(\\mathbb{E}_{\\pi_N}[K])^2}{N}$$\n    This final expression is used in the implementation. This is well-defined for all test cases, where $N \\ge 2$.\n\nThis algorithmic design is robust, exact up to floating-point precision, and avoids simulation, as required.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test suite.\n    This function encapsulates the entire logic as requested.\n    \"\"\"\n\n    # Test cases: (N, a, b, c)\n    test_cases = [\n        (20, 0.05, 2.0, 1.0),\n        (50, 0.05, 2.0, 1.0),\n        (200, 0.05, 2.0, 1.0),\n        (20, 0.05, 1.2, 1.0),\n        (50, 0.05, 1.2, 1.0),\n        (200, 0.05, 1.2, 1.0),\n        (2, 0.05, 2.0, 1.0),\n    ]\n\n    def compute_scaled_correlation(N, a, b, c):\n        \"\"\"\n        Computes the stationary scaled correlation N * C_2^(N) for a finite system.\n\n        Args:\n            N (int): Number of agents.\n            a (float): Base rate for 0 -> 1 transition.\n            b (float): Mean-field coupling for 0 -> 1 transition.\n            c (float): Rate for 1 -> 0 transition.\n\n        Returns:\n            float: The value of N * C_2^(N).\n        \"\"\"\n        if N <= 1:\n            # The correlation C_2^(N) involves a sum over pairs i!=j and is\n            # not well-defined for N <= 1. The problem's test cases all have N >= 2.\n            # Returning 0.0 for completeness, though this case is not tested.\n            return 0.0\n\n        # Step 1: Compute unnormalized log-probabilities in a stable way.\n        # u_k = prod_{r=0}^{k-1} (B_r / D_{r+1})\n        # log_u[k] = log(u_k)\n        log_u = np.zeros(N + 1)\n        for k in range(1, N + 1):\n            r = k - 1\n            # B_r = (N - r) * (a + b * r / N)\n            # D_k = k * c\n            log_Br = np.log(N - r) + np.log(a + b * r / N)\n            log_Dk = np.log(k) + np.log(c)\n            log_u[k] = log_u[k - 1] + log_Br - log_Dk\n            \n        # Step 2: Normalize probabilities using a stabilization trick to avoid overflow/underflow.\n        log_u_max = np.max(log_u)\n        u_shifted = np.exp(log_u - log_u_max)\n        Z = np.sum(u_shifted)  # Normalization constant\n        pi_N = u_shifted / Z\n\n        # Step 3: Compute the first two moments of K.\n        k_vals = np.arange(N + 1, dtype=np.float64)\n        E_K = np.sum(k_vals * pi_N)\n        E_K2 = np.sum(k_vals**2 * pi_N)\n\n        # Step 4: Calculate the connected two-point correlation C_2^(N) and scale by N.\n        # C_2^(N) = (E[K^2] - E[K]) / (N(N-1)) - (E[K]/N)^2\n        # N * C_2^(N) = (E[K^2] - E[K]) / (N-1) - (E[K])^2 / N\n        scaled_correlation = (E_K2 - E_K) / (N - 1) - (E_K**2) / N\n        \n        return scaled_correlation\n\n    results = []\n    for case in test_cases:\n        N, a, b, c = case\n        result = compute_scaled_correlation(N, a, b, c)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}