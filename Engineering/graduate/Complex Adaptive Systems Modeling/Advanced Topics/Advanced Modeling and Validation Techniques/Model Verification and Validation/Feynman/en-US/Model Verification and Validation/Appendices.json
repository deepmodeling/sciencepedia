{
    "hands_on_practices": [
        {
            "introduction": "In any complex model, some parameters contribute far more to output uncertainty than others. This exercise  introduces a rigorous method based on the law of total variance, $\\operatorname{Var}(Y) = \\mathbb{E}\\![\\operatorname{Var}(Y \\mid X)] + \\operatorname{Var}(\\mathbb{E}[Y \\mid X])$, to quantify the variance contribution from a specific parameter subset. Mastering this technique allows you to strategically prioritize validation efforts, focusing resources on the parameters that matter most.",
            "id": "4127760",
            "problem": "Consider a complex adaptive system model with stochastic output $Y$ that depends on a vector of uncertain parameters $X = (X_{S}, X_{T})$, where $X_{S}$ is a subset of parameters selected for possible targeted validation and $X_{T}$ denotes the remaining parameters. Assume the following fundamental bases: the definition of variance $\\operatorname{Var}(Y) = \\mathbb{E}\\!\\left[(Y - \\mathbb{E}[Y])^{2}\\right]$, conditional expectation and variance $\\mathbb{E}[Y \\mid X_{S}]$ and $\\operatorname{Var}(Y \\mid X_{S})$, and the law of total variance $\\operatorname{Var}(Y) = \\mathbb{E}\\!\\left[\\operatorname{Var}(Y \\mid X_{S})\\right] + \\operatorname{Var}\\!\\left(\\mathbb{E}[Y \\mid X_{S}]\\right)$.\n\nYou are provided a scientifically sound verification scenario: a large, converged ensemble of simulation runs yields an empirically stable estimate of the total output variance $\\operatorname{Var}(Y) = 4.50 \\times 10^{2}$. Independent validation experiments constrain the conditional variance across a calibrated credible set $\\mathcal{C}$ for $X_{S}$ with $\\mathbb{P}(X_{S} \\in \\mathcal{C}) = 1$, and establish that for every $x_{s} \\in \\mathcal{C}$, the conditional variance satisfies $v_{\\min} \\leq \\operatorname{Var}(Y \\mid X_{S} = x_{s}) \\leq v_{\\max}$ with $v_{\\min} = 1.40 \\times 10^{2}$ and $v_{\\max} = 2.20 \\times 10^{2}$.\n\nStarting only from these fundamental bases, derive rigorous lower and upper bounds on the variance contribution attributable to $X_{S}$, quantified by $\\operatorname{Var}\\!\\left(\\mathbb{E}[Y \\mid X_{S}]\\right)$, and then express the corresponding bounds on the fractional contribution of $X_{S}$ to the total output variance, defined as\n$$F_{S} \\equiv \\frac{\\operatorname{Var}\\!\\left(\\mathbb{E}[Y \\mid X_{S}]\\right)}{\\operatorname{Var}(Y)}.$$\nCompute the resulting two numerical bounds for $F_{S}$ using the given values, and round your results to four significant figures. Express the two bounds as a row matrix using the LaTeX `pmatrix` environment. Finally, briefly interpret how these bounds inform targeted validation decisions for $X_{S}$ in terms of potential reduction of output uncertainty. The interpretation is part of your reasoning but is not part of the final answer output specification.",
            "solution": "The problem as stated is scientifically grounded, well-posed, and contains all necessary information for a unique solution. The premises are based on the fundamental law of total variance, a standard theorem in probability theory, and the scenario described is a conventional application in the field of uncertainty quantification and model validation. Therefore, the problem is valid, and we proceed with its solution.\n\nOur starting point is the law of total variance, which decomposes the total variance of a random variable $Y$ based on a conditioning variable, in this case, the parameter vector subset $X_{S}$. The law is given as:\n$$\n\\operatorname{Var}(Y) = \\mathbb{E}\\!\\left[\\operatorname{Var}(Y \\mid X_{S})\\right] + \\operatorname{Var}\\!\\left(\\mathbb{E}[Y \\mid X_{S}]\\right)\n$$\nThe term $\\operatorname{Var}\\!\\left(\\mathbb{E}[Y \\mid X_{S}]\\right)$ represents the portion of the variance in the output $Y$ that is explained by the uncertainty in the parameters $X_{S}$. This is the quantity of interest. We can isolate it by rearranging the equation:\n$$\n\\operatorname{Var}\\!\\left(\\mathbb{E}[Y \\mid X_{S}]\\right) = \\operatorname{Var}(Y) - \\mathbb{E}\\!\\left[\\operatorname{Var}(Y \\mid X_{S})\\right]\n$$\nWe are given the total variance $\\operatorname{Var}(Y) = 4.50 \\times 10^{2}$. To find bounds on $\\operatorname{Var}\\!\\left(\\mathbb{E}[Y \\mid X_{S}]\\right)$, we must first establish bounds for the term $\\mathbb{E}\\!\\left[\\operatorname{Var}(Y \\mid X_{S})\\right]$.\n\nThe problem states that independent validation experiments have established that for any realization $x_{s}$ of the random vector $X_{S}$ within its credible set $\\mathcal{C}$ (where $\\mathbb{P}(X_{S} \\in \\mathcal{C}) = 1$), the conditional variance $\\operatorname{Var}(Y \\mid X_{S} = x_{s})$ is bounded. The random variable $Z \\equiv \\operatorname{Var}(Y \\mid X_{S})$ thus satisfies the inequality:\n$$\nv_{\\min} \\leq Z \\leq v_{\\max}\n$$\nalmost surely. The values for these bounds are given as $v_{\\min} = 1.40 \\times 10^{2}$ and $v_{\\max} = 2.20 \\times 10^{2}$.\n\nThe expectation operator is monotone. If a random variable $A$ is bounded by constants $c_{1}$ and $c_{2}$ such that $\\mathbb{P}(c_{1} \\leq A \\leq c_{2}) = 1$, then its expectation is bounded by the same constants: $c_{1} \\leq \\mathbb{E}[A] \\leq c_{2}$. Applying this property to the random variable $Z = \\operatorname{Var}(Y \\mid X_{S})$, we obtain bounds on its expectation:\n$$\n\\mathbb{E}[v_{\\min}] \\leq \\mathbb{E}\\!\\left[\\operatorname{Var}(Y \\mid X_{S})\\right] \\leq \\mathbb{E}[v_{\\max}]\n$$\nSince $v_{\\min}$ and $v_{\\max}$ are constants, their expectations are simply their own values. Therefore:\n$$\nv_{\\min} \\leq \\mathbb{E}\\!\\left[\\operatorname{Var}(Y \\mid X_{S})\\right] \\leq v_{\\max}\n$$\nNow we can substitute these bounds back into our expression for $\\operatorname{Var}\\!\\left(\\mathbb{E}[Y \\mid X_{S}]\\right)$.\nTo obtain the lower bound on $\\operatorname{Var}\\!\\left(\\mathbb{E}[Y \\mid X_{S}]\\right)$, we must subtract the largest possible value of $\\mathbb{E}\\!\\left[\\operatorname{Var}(Y \\mid X_{S})\\right]$, which is $v_{\\max}$:\n$$\n\\left(\\operatorname{Var}\\!\\left(\\mathbb{E}[Y \\mid X_{S}]\\right)\\right)_{\\min} = \\operatorname{Var}(Y) - v_{\\max}\n$$\nTo obtain the upper bound, we must subtract the smallest possible value of $\\mathbb{E}\\!\\left[\\operatorname{Var}(Y \\mid X_{S})\\right]$, which is $v_{\\min}$:\n$$\n\\left(\\operatorname{Var}\\!\\left(\\mathbb{E}[Y \\mid X_{S}]\\right)\\right)_{\\max} = \\operatorname{Var}(Y) - v_{\\min}\n$$\nThis establishes the rigorous inequality for the variance contribution from $X_{S}$:\n$$\n\\operatorname{Var}(Y) - v_{\\max} \\leq \\operatorname{Var}\\!\\left(\\mathbb{E}[Y \\mid X_{S}]\\right) \\leq \\operatorname{Var}(Y) - v_{\\min}\n$$\nThe problem asks for bounds on the fractional contribution, $F_{S}$, defined as:\n$$\nF_{S} \\equiv \\frac{\\operatorname{Var}\\!\\left(\\mathbb{E}[Y \\mid X_{S}]\\right)}{\\operatorname{Var}(Y)}\n$$\nSince $\\operatorname{Var}(Y)$ is a positive constant, we can divide the entire inequality by it without changing the direction of the inequalities:\n$$\n\\frac{\\operatorname{Var}(Y) - v_{\\max}}{\\operatorname{Var}(Y)} \\leq F_{S} \\leq \\frac{\\operatorname{Var}(Y) - v_{\\min}}{\\operatorname{Var}(Y)}\n$$\nThis simplifies to:\n$$\n1 - \\frac{v_{\\max}}{\\operatorname{Var}(Y)} \\leq F_{S} \\leq 1 - \\frac{v_{\\min}}{\\operatorname{Var}(Y)}\n$$\nNow, we substitute the provided numerical values:\n$\\operatorname{Var}(Y) = 4.50 \\times 10^{2} = 450$\n$v_{\\min} = 1.40 \\times 10^{2} = 140$\n$v_{\\max} = 2.20 \\times 10^{2} = 220$\n\nThe lower bound for $F_{S}$ is:\n$$\n(F_{S})_{\\min} = 1 - \\frac{220}{450} = 1 - \\frac{22}{45} = \\frac{23}{45} \\approx 0.51111...\n$$\nRounding to four significant figures, we get $(F_{S})_{\\min} = 0.5111$.\n\nThe upper bound for $F_{S}$ is:\n$$\n(F_{S})_{\\max} = 1 - \\frac{140}{450} = 1 - \\frac{14}{45} = \\frac{31}{45} \\approx 0.68888...\n$$\nRounding to four significant figures, we get $(F_{S})_{\\max} = 0.6889$.\n\nTherefore, the fractional contribution of the parameters $X_S$ to the total output variance lies within the interval $[0.5111, 0.6889]$.\n\nInterpretation: The bounds on $F_S$ provide a quantitative assessment of the importance of the parameter subset $X_S$. The results indicate that the uncertainty in $X_S$ is responsible for at least $51.11\\%$ and at most $68.89\\%$ of the total output variance. This is a substantial contribution. If one were to perfectly determine the true values of the parameters in $X_S$ through validation experiments, the total variance of the model's output would be reduced by an amount within this range. This provides strong justification for prioritizing further experimental and validation resources to reduce the uncertainty in the parameters $X_S$, as doing so is guaranteed to significantly reduce the overall model uncertainty.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.5111 & 0.6889\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "A key challenge in model validation is determining the necessary number of simulation runs to confidently detect a meaningful discrepancy from empirical data. This practice  guides you through a statistical power analysis, accounting for the temporal autocorrelation that is characteristic of many complex systems. By implementing this, you will develop the essential skill of designing validation experiments that are neither wastefully large nor insufficiently sensitive.",
            "id": "4127738",
            "problem": "You are tasked with developing a program to compute statistical power for detecting a specified discrepancy in an emergent time series and to justify the sample size requirements for validation experiments under complex adaptive systems modeling. The emergent observation per experimental run is a time series $\\{Y_{r,t}\\}_{t=1}^{T}$, where $r \\in \\{1,\\ldots,R\\}$ indexes independent runs. Each series is assumed to be generated as $Y_{r,t} = \\mu + \\varepsilon_{r,t}$, where $\\{\\varepsilon_{r,t}\\}$ is a stationary Gaussian autoregressive process of order one (AR(1)) with marginal standard deviation $\\sigma$ and autocorrelation parameter $\\rho \\in (-1,1)$. The validation experiment aims to detect a discrepancy $\\delta$ in the long-run mean level between a reference model and empirical data, formalized by the hypothesis test on the difference in grand means across two conditions. Assume that runs in the two conditions are independent and identically distributed with the same $\\sigma$, $\\rho$, and $T$. Use a two-sided test at significance level $\\alpha$.\n\nStarting from fundamental definitions of covariance and the Central Limit Theorem (CLT), treat the per-run time-average $\\bar{Y}_{r} = \\frac{1}{T}\\sum_{t=1}^{T}Y_{r,t}$ as the basic estimator for the mean in a run. Use the fact that, for a stationary process, the variance of the time-average can be expressed via the autocovariance function, and for AR(1) with marginal variance $\\sigma^{2}$ and autocorrelation parameter $\\rho$, the autocovariance between lagged terms satisfies $\\operatorname{Cov}(\\varepsilon_{r,t},\\varepsilon_{r,t+k}) = \\sigma^{2}\\rho^{|k|}$. Consequently, the variance of the time-average must incorporate the autocorrelation structure. The grand mean across $R$ independent runs is $\\bar{M} = \\frac{1}{R}\\sum_{r=1}^{R}\\bar{Y}_{r}$. Under the two-condition comparison, consider the difference of grand means, and derive an expression for the statistical power under the alternative where the true mean difference is $\\delta \\neq 0$.\n\nYour program must:\n- Compute the variance of the per-run time-average $\\bar{Y}_{r}$ for AR(1) using the covariance summation identity.\n- Compute the variance of the difference of grand means across two conditions, using independence across runs and conditions.\n- Compute the statistical power (expressed as a decimal fraction, not a percentage) of the two-sided test at level $\\alpha$ for a specified discrepancy $\\delta$, given $(\\sigma,\\rho,T,R)$.\n- Compute the minimal integer number of independent runs $R_{\\min}$ (with $T$ fixed) required to achieve at least a target power $p_{\\star}$ for detecting $\\delta$ at level $\\alpha$.\n\nExpress all power values in decimal units (for example, $0.8$), not percentages. No physical units are involved, and no angles are involved.\n\nTest Suite:\nUse the following parameter sets, each represented as $(\\delta,\\sigma,\\rho,T,R,\\alpha,p_{\\star})$:\n1. $(0.5, 1.0, 0.3, 200, 10, 0.05, 0.8)$\n2. $(0.5, 1.0, 0.8, 10, 10, 0.05, 0.8)$\n3. $(0.1, 1.0, 0.95, 1000, 20, 0.05, 0.9)$\n4. $(0.2, 1.0, 0.0, 100, 5, 0.05, 0.8)$\n5. $(0.01, 1.0, 0.5, 10000, 50, 0.01, 0.9)$\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must be a two-element list $[\\text{power}, R_{\\min}]$, where $\\text{power}$ is a float for the given $R$ and $R_{\\min}$ is the minimal integer number of runs needed to achieve at least $p_{\\star}$ for the same $(\\delta,\\sigma,\\rho,T,\\alpha)$. Format the output line as, for example, $[[0.812345,9],[0.123456,87],\\ldots]$, where each power is rounded to six decimal places.",
            "solution": "The goal is to compute statistical power for detecting a discrepancy in the emergent mean of a time series from a complex adaptive system, with a justification for the sample size in terms of the number of independent runs. We begin with fundamental bases: the definition of covariance in stationary processes and the Central Limit Theorem (CLT), then derive the variance of the run-level time-average, construct the variance for the difference of grand means across two conditions, and finally deduce the statistical power under the alternative hypothesis.\n\nConsider a single run $r$ with time series $Y_{r,t} = \\mu + \\varepsilon_{r,t}$ for $t = 1,\\ldots,T$, where $\\{\\varepsilon_{r,t}\\}$ is a stationary Gaussian autoregressive process of order one (AR(1)) with marginal variance $\\sigma^{2}$ and autocorrelation parameter $\\rho \\in (-1,1)$. The per-run time-average is $\\bar{Y}_{r} = \\frac{1}{T}\\sum_{t=1}^{T}Y_{r,t}$. Since $\\varepsilon_{r,t}$ has zero mean, $\\mathbb{E}[\\bar{Y}_{r}] = \\mu$, and the variance of $\\bar{Y}_{r}$ depends on the autocovariance structure. The variance identity for a stationary process states\n$$\n\\operatorname{Var}(\\bar{Y}_{r}) \\;=\\; \\operatorname{Var}\\left(\\frac{1}{T}\\sum_{t=1}^{T}Y_{r,t}\\right) \\;=\\; \\frac{1}{T^{2}} \\sum_{i=1}^{T} \\sum_{j=1}^{T} \\operatorname{Cov}(Y_{r,i}, Y_{r,j}).\n$$\nBecause $Y_{r,t}$ differs from $\\varepsilon_{r,t}$ only by a constant offset $\\mu$, the covariance comes entirely from $\\varepsilon_{r,t}$. For AR(1) with marginal variance $\\sigma^{2}$, the autocovariance at lag $k$ is $\\operatorname{Cov}(\\varepsilon_{r,t},\\varepsilon_{r,t+k}) = \\sigma^{2}\\rho^{|k|}$. Using the symmetry of covariances and grouping by lag, we obtain\n$$\n\\operatorname{Var}(\\bar{Y}_{r}) \\;=\\; \\frac{1}{T^{2}} \\left( T \\cdot \\sigma^{2} + 2 \\sum_{k=1}^{T-1} (T - k) \\cdot \\sigma^{2} \\rho^{k} \\right)\n\\;=\\; \\frac{\\sigma^{2}}{T}\\left( 1 + 2 \\sum_{k=1}^{T-1}\\left(1 - \\frac{k}{T}\\right)\\rho^{k} \\right).\n$$\nThis identity shows how autocorrelation inflates the variance of the time-average relative to the independent case (which corresponds to $\\rho = 0$ and yields $\\operatorname{Var}(\\bar{Y}_{r}) = \\sigma^{2}/T$).\n\nLet there be two conditions, denoted $A$ and $B$, each with $R$ independent runs of length $T$, and suppose the long-run mean in condition $B$ differs by $\\delta$ from condition $A$ so that $\\mu_{B} = \\mu_{A} + \\delta$. Within each condition, the grand mean across runs is\n$$\n\\bar{M}_{A} = \\frac{1}{R}\\sum_{r=1}^{R}\\bar{Y}^{(A)}_{r}, \\qquad \\bar{M}_{B} = \\frac{1}{R}\\sum_{r=1}^{R}\\bar{Y}^{(B)}_{r}.\n$$\nBy independence of runs, $\\operatorname{Var}(\\bar{M}_{A}) = \\operatorname{Var}(\\bar{Y}_{r})/R$ and $\\operatorname{Var}(\\bar{M}_{B}) = \\operatorname{Var}(\\bar{Y}_{r})/R$ when both conditions share the same $\\sigma$, $\\rho$, and $T$. The difference of grand means, $\\hat{\\Delta} = \\bar{M}_{B} - \\bar{M}_{A}$, has variance\n$$\n\\operatorname{Var}(\\hat{\\Delta}) \\;=\\; \\operatorname{Var}(\\bar{M}_{B}) + \\operatorname{Var}(\\bar{M}_{A})\n\\;=\\; \\frac{2}{R}\\operatorname{Var}(\\bar{Y}_{r})\n\\;=\\; \\frac{2\\sigma^{2}}{R T}\\left( 1 + 2 \\sum_{k=1}^{T-1}\\left(1 - \\frac{k}{T}\\right)\\rho^{k} \\right).\n$$\nUnder the alternative hypothesis where the true mean difference is $\\delta \\neq 0$, the standardized statistic based on $\\hat{\\Delta}$ and its standard deviation is approximately normal by the Central Limit Theorem (CLT) for averages of weakly dependent sequences across independent runs. Denote the standard deviation by\n$$\ns_{\\Delta} \\;=\\; \\sqrt{\\operatorname{Var}(\\hat{\\Delta})},\n$$\nand the noncentrality parameter by\n$$\n\\lambda \\;=\\; \\frac{|\\delta|}{s_{\\Delta}}.\n$$\nUsing a two-sided test at level $\\alpha$, the critical value for the standard normal test is $z_{1-\\alpha/2}$ (the $(1-\\alpha/2)$-quantile of the standard normal distribution). Under the alternative, the standardized statistic $Z$ is approximately $\\mathcal{N}(\\lambda,1)$, so the statistical power (probability of rejecting the null under the alternative) is\n$$\n\\text{Power} \\;=\\; \\mathbb{P}\\left(|Z| > z_{1-\\alpha/2}\\right)\n\\;=\\; \\Phi\\left(-z_{1-\\alpha/2} - \\lambda\\right) + \\left(1 - \\Phi\\left(z_{1-\\alpha/2} - \\lambda\\right)\\right),\n$$\nwhere $\\Phi(\\cdot)$ is the cumulative distribution function of the standard normal distribution. This form follows directly from the definition of a two-sided rejection region and the distribution of $Z$ under the alternative.\n\nTo justify sample size in terms of the number of independent runs, observe that $s_{\\Delta}$ decreases monotonically in $R$ (specifically like $R^{-1/2}$), so the power is monotonically increasing in $R$. Therefore, for a fixed $(\\delta,\\sigma,\\rho,T,\\alpha)$ and target power $p_{\\star}$, there exists a minimal integer $R_{\\min}$ such that the computed power at $R_{\\min}$ is at least $p_{\\star}$. We compute $R_{\\min}$ efficiently by:\n- Evaluating power at $R = 1$ and doubling $R$ until the power exceeds $p_{\\star}$ or a preset large upper bound is reached (exponential search to find an interval that brackets the solution).\n- Performing a binary search on the bracketed interval to find the smallest integer $R$ such that the power is at least $p_{\\star}$.\n\nAlgorithmic steps implemented in the program:\n1. Compute $\\operatorname{Var}(\\bar{Y}_{r})$ using the AR(1) covariance summation formula:\n   $$\n   \\operatorname{Var}(\\bar{Y}_{r}) \\;=\\; \\frac{\\sigma^{2}}{T}\\left( 1 + 2 \\sum_{k=1}^{T-1}\\left(1 - \\frac{k}{T}\\right)\\rho^{k} \\right).\n   $$\n2. Compute $\\operatorname{Var}(\\hat{\\Delta}) = \\frac{2}{R}\\operatorname{Var}(\\bar{Y}_{r})$ and $s_{\\Delta} = \\sqrt{\\operatorname{Var}(\\hat{\\Delta})}$.\n3. Compute $\\lambda = \\frac{|\\delta|}{s_{\\Delta}}$ and $z_{1-\\alpha/2}$, then evaluate power via\n   $$\n   \\text{Power} \\;=\\; \\Phi\\left(-z_{1-\\alpha/2} - \\lambda\\right) + \\left(1 - \\Phi\\left(z_{1-\\alpha/2} - \\lambda\\right)\\right).\n   $$\n4. For each test case, compute the power at the given $R$, and compute $R_{\\min}$ by exponential search followed by binary search as described. The monotonicity of power in $R$ guarantees the algorithm’s correctness.\n5. Output the list of results $[\\text{power}, R_{\\min}]$ for all test cases on a single line, with each power rounded to six decimal places.\n\nThis procedure is grounded in standard properties of stationary Gaussian AR(1) processes, the covariance-based variance formula for time-averages, and the Central Limit Theorem (CLT), yielding scientifically sound and numerically stable computations suitable for model verification and validation in complex adaptive systems.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef var_mean_ar1(T: int, rho: float, sigma: float) -> float:\n    \"\"\"\n    Compute Var(bar{Y}_r) for an AR(1) process with marginal std sigma and autocorrelation rho.\n    Var(bar{Y}_r) = (sigma^2 / T) * [1 + 2 * sum_{k=1}^{T-1} (1 - k/T) * rho^k]\n    \"\"\"\n    if T <= 0:\n        raise ValueError(\"T must be a positive integer\")\n    if abs(rho) >= 1.0:\n        raise ValueError(\"rho must be in (-1,1) for stationarity\")\n    # Sum the weighted autocorrelation terms\n    if T == 1:\n        # No autocovariance terms when only one observation\n        return (sigma ** 2) / T\n    k = np.arange(1, T, dtype=np.float64)\n    weights = 1.0 - k / T\n    # Use power with float for stability\n    rho_powers = rho ** k\n    s = np.sum(weights * rho_powers)\n    return (sigma ** 2 / T) * (1.0 + 2.0 * s)\n\ndef two_sided_power(delta: float, sigma: float, rho: float, T: int, R: int, alpha: float) -> float:\n    \"\"\"\n    Compute the two-sided normal-approx power at significance alpha for discrepancy delta,\n    given AR(1) parameters (sigma, rho), time length T, and number of runs R.\n    \"\"\"\n    vmean = var_mean_ar1(T, rho, sigma)\n    vdiff = 2.0 * vmean / float(R)\n    sd = np.sqrt(vdiff)\n    lam = abs(delta) / sd if sd > 0 else np.inf\n    z = norm.ppf(1.0 - alpha / 2.0)\n    # Power = P(|Z| > z) with Z ~ N(lam, 1)\n    power = norm.cdf(-z - lam) + (1.0 - norm.cdf(z - lam))\n    return float(power)\n\ndef minimal_R_for_power(delta: float, sigma: float, rho: float, T: int, alpha: float, target_power: float,\n                        Rmax: int = 1_000_000) -> int:\n    \"\"\"\n    Find the minimal integer R such that two_sided_power(...) >= target_power,\n    using exponential search to bracket and then binary search to refine.\n    \"\"\"\n    # Handle degenerate case: if delta is zero, power equals alpha; require Rmin as 1 if target <= alpha else Rmax\n    # However, we follow the general algorithm; as R grows, lam grows ~ sqrt(R), power increases.\n    # Exponential search to find an upper bound where power >= target_power.\n    upper = 1\n    p_upper = two_sided_power(delta, sigma, rho, T, upper, alpha)\n    if p_upper >= target_power:\n        return upper\n    while p_upper < target_power and upper < Rmax:\n        upper *= 2\n        p_upper = two_sided_power(delta, sigma, rho, T, upper, alpha)\n    if p_upper < target_power and upper >= Rmax:\n        # Could not achieve target within Rmax; return Rmax as a conservative cap.\n        return Rmax\n    lower = upper // 2\n    if lower < 1:\n        lower = 1\n    # Binary search for minimal R in [lower, upper]\n    rmin = upper\n    while lower <= upper:\n        mid = (lower + upper) // 2\n        p_mid = two_sided_power(delta, sigma, rho, T, mid, alpha)\n        if p_mid >= target_power:\n            rmin = mid\n            upper = mid - 1\n        else:\n            lower = mid + 1\n    return int(rmin)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each test case is (delta, sigma, rho, T, R, alpha, p_star)\n    test_cases = [\n        (0.5, 1.0, 0.3, 200, 10, 0.05, 0.8),\n        (0.5, 1.0, 0.8, 10, 10, 0.05, 0.8),\n        (0.1, 1.0, 0.95, 1000, 20, 0.05, 0.9),\n        (0.2, 1.0, 0.0, 100, 5, 0.05, 0.8),\n        (0.01, 1.0, 0.5, 10000, 50, 0.01, 0.9),\n    ]\n\n    results = []\n    for case in test_cases:\n        delta, sigma, rho, T, R, alpha, p_star = case\n        power = two_sided_power(delta, sigma, rho, T, R, alpha)\n        Rmin = minimal_R_for_power(delta, sigma, rho, T, alpha, p_star)\n        results.append((power, Rmin))\n\n    # Format the final result exactly as required: [[power,Rmin],...], with power rounded to 6 decimals.\n    formatted_results = \"[\" + \",\".join(\"[\" + f\"{p:.6f},\" + f\"{r}\" + \"]\" for p, r in results) + \"]\"\n    print(formatted_results)\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "Validation is not merely about fitting a model to historical data; it is also about ensuring the model's conclusions are robust. This practice  introduces an adversarial approach to validation, where you will systematically test your model’s sensitivity to small, plausible perturbations in its core parameters. By quantifying the worst-case change in key outputs, you will gain a deeper understanding of your model's structural stability and the reliability of its inferences.",
            "id": "4127794",
            "problem": "You are tasked with designing and implementing a rigorous validation procedure for a simple but nontrivial Agent-Based Model (ABM) under adversarial perturbations of agent decision rules, and computing the sensitivity of key observables to bounded perturbations quantified by an appropriate norm. The ABM consists of $N$ agents with continuous action states $a_i^{(t)} \\in [0,1]$ that evolve over discrete time steps $t \\in \\{0,1,\\dots,T\\}$. Each agent $i$ possesses a fixed trait $x_i \\in \\mathbb{R}$ drawn independently from a standard normal distribution. The decision rule is a deterministic mean-field update: at each time step, every agent updates synchronously according to\n$$\na_i^{(t+1)} \\;=\\; \\sigma\\!\\left(\\theta_0 + \\theta_1 \\cdot m^{(t)} + \\theta_2 \\cdot x_i\\right),\n$$\nwhere $\\sigma(z) = \\frac{1}{1+e^{-z}}$ is the logistic function, $\\theta = (\\theta_0,\\theta_1,\\theta_2) \\in \\mathbb{R}^3$ are the rule parameters to be validated, and $m^{(t)} = \\frac{1}{N}\\sum_{j=1}^N a_j^{(t)}$ is the population mean at time $t$. The initial actions are set to $a_i^{(0)} = 0.5$ for all agents, so that $m^{(0)} = 0.5$. Fix a random seed to ensure reproducibility of the trait draws.\n\nDefine two observables to be validated:\n- The final mean action,\n$$\nO_1(\\theta) \\;=\\; m^{(T)}(\\theta).\n$$\n- The time-averaged cross-sectional variance of actions,\n$$\nO_2(\\theta) \\;=\\; \n\\begin{cases}\n\\frac{1}{T}\\sum_{t=1}^{T} \\operatorname{Var}\\!\\left(a^{(t)}(\\theta)\\right), & \\text{if } T > 0,\\\\\n0.0, & \\text{if } T = 0,\n\\end{cases}\n$$\nwhere $\\operatorname{Var}(a^{(t)}(\\theta)) = \\frac{1}{N}\\sum_{i=1}^N \\left(a_i^{(t)}(\\theta) - m^{(t)}(\\theta)\\right)^2$.\n\nYou must implement a validation under adversarial perturbations to the rule parameters $\\theta$, with perturbations bounded in the $L_\\infty$ norm. Specifically, let $\\delta \\in \\mathbb{R}^3$ denote a perturbation vector, and impose the bound $\\|\\delta\\|_\\infty \\le \\varepsilon$, where $\\varepsilon > 0$ is given. Use the following requirements:\n\n1. Model simulation: Implement a deterministic simulator for the ABM that, given $(N,T,\\theta,\\text{seed})$, returns the values $O_1(\\theta)$ and $O_2(\\theta)$ as defined above. The traits $x_i$ must be independently drawn from a standard normal distribution using the specified random seed, and the initial actions must all be $0.5$.\n\n2. Sensitivity computation: At a given baseline $\\theta$, numerically estimate the gradient $\\nabla O_k(\\theta)$ for $k \\in \\{1,2\\}$ using a central finite difference scheme with a small symmetric step $h$ for each coordinate of $\\theta$. Then use the appropriate dual norm reasoning to compute an upper bound on the worst-case first-order change in $O_k$ under all perturbations satisfying $\\|\\delta\\|_\\infty \\le \\varepsilon$. Report these two sensitivity bounds.\n\n3. Adversarial validation: Compute the actual worst-case changes in $O_1$ and $O_2$ over the set of perturbations that are corners of the hypercube $\\{-\\varepsilon,+\\varepsilon\\}^3$, that is, over all $\\delta$ with each component equal to either $-\\varepsilon$ or $+\\varepsilon$. For each observable $O_k$, report the maximum absolute change $\\max_{\\delta \\in \\{-\\varepsilon,+\\varepsilon\\}^3} \\left|O_k(\\theta+\\delta) - O_k(\\theta)\\right|$.\n\n4. Validation decision: Given tolerances $\\tau_1 > 0$ and $\\tau_2 > 0$, declare the model as passing validation if both of the following inequalities hold:\n$$\n\\max_{\\delta \\in \\{-\\varepsilon,+\\varepsilon\\}^3} \\left|O_1(\\theta+\\delta) - O_1(\\theta)\\right| \\le \\tau_1,\n\\qquad\n\\max_{\\delta \\in \\{-\\varepsilon,+\\varepsilon\\}^3} \\left|O_2(\\theta+\\delta) - O_2(\\theta)\\right| \\le \\tau_2.\n$$\nReturn a boolean indicating pass or fail.\n\nYour program must implement the above logic and apply it to the following test suite. For each test case, the parameters are given as $(N,T,\\theta_0,\\theta_1,\\theta_2,\\varepsilon,\\tau_1,\\tau_2,\\text{seed})$:\n\n- Test case $1$: $(N=\\;500,\\;T=\\;50,\\;\\theta_0=\\;0.2,\\;\\theta_1=\\;1.1,\\;\\theta_2=\\;0.7,\\;\\varepsilon=\\;0.05,\\;\\tau_1=\\;0.03,\\;\\tau_2=\\;0.02,\\;\\text{seed}=\\;42)$.\n- Test case $2$: $(N=\\;600,\\;T=\\;20,\\;\\theta_0=\\;-0.1,\\;\\theta_1=\\;0.9,\\;\\theta_2=\\;0.3,\\;\\varepsilon=\\;0.0,\\;\\tau_1=\\;0.0,\\;\\tau_2=\\;0.0,\\;\\text{seed}=\\;123)$.\n- Test case $3$: $(N=\\;1000,\\;T=\\;100,\\;\\theta_0=\\;0.0,\\;\\theta_1=\\;1.5,\\;\\theta_2=\\;0.5,\\;\\varepsilon=\\;0.5,\\;\\tau_1=\\;0.05,\\;\\tau_2=\\;0.05,\\;\\text{seed}=\\;7)$.\n- Test case $4$: $(N=\\;300,\\;T=\\;0,\\;\\theta_0=\\;0.4,\\;\\theta_1=\\;1.2,\\;\\theta_2=\\;-0.6,\\;\\varepsilon=\\;0.2,\\;\\tau_1=\\;10^{-6},\\;\\tau_2=\\;10^{-6},\\;\\text{seed}=\\;2025)$.\n- Test case $5$: $(N=\\;10,\\;T=\\;10,\\;\\theta_0=\\;-0.5,\\;\\theta_1=\\;0.8,\\;\\theta_2=\\;1.1,\\;\\varepsilon=\\;0.01,\\;\\tau_1=\\;0.005,\\;\\tau_2=\\;0.005,\\;\\text{seed}=\\;999)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element of the list corresponds to a test case and must itself be a list of five entries:\n$[S_1^{\\text{bound}}, S_1^{\\text{corner}}, S_2^{\\text{bound}}, S_2^{\\text{corner}}, \\text{pass}]$,\nwhere $S_k^{\\text{bound}}$ is the sensitivity upper bound for $O_k$, $S_k^{\\text{corner}}$ is the observed worst-case corner change for $O_k$, and $\\text{pass}$ is a boolean. For example, the final output line should look like\n$[[\\dots],[\\dots],[\\dots],[\\dots],[\\dots]]$\nwith exactly five inner lists, one per test case. No other text should be printed.",
            "solution": "The problem statement has been evaluated and is determined to be **valid**. It is scientifically grounded, well-posed, objective, and internally consistent. It presents a clear, formalizable task in the domain of complex adaptive systems model validation. The only minor ambiguity is the unspecified step size $h$ for the finite difference calculation, which is a standard implementation detail. A reasonable value of $h=10^{-6}$ will be used for this purpose.\n\nThe solution proceeds by implementing the four requirements specified in the problem statement. This involves creating a deterministic simulation of the Agent-Based Model (ABM), using this simulation to compute the sensitivity of two key observables to parameter perturbations, and making a validation decision based on the observed changes against given tolerances.\n\n### 1. Model Simulation\n\nFirst, a function is designed to simulate the ABM dynamics. This function accepts the number of agents $N$, the number of time steps $T$, the parameter vector $\\theta = (\\theta_0, \\theta_1, \\theta_2)$, and a random seed.\n\n- **Initialization**: Using the provided seed, a pseudo-random number generator is initialized to ensure reproducibility. The fixed traits $x_i$ for each of the $N$ agents are drawn independently from a standard normal distribution $\\mathcal{N}(0, 1)$. The initial action states for all agents are set to $a_i^{(0)} = 0.5$.\n\n- **Time Evolution**: The simulation iterates for $t$ from $0$ to $T-1$. In each step, it performs a synchronous update for all agents.\n    1.  The population mean action $m^{(t)} = \\frac{1}{N}\\sum_{j=1}^N a_j^{(t)}$ is calculated.\n    2.  The action state of each agent $i$ for the next time step, $a_i^{(t+1)}$, is computed according to the deterministic update rule:\n        $$\n        a_i^{(t+1)} = \\sigma(\\theta_0 + \\theta_1 \\cdot m^{(t)} + \\theta_2 \\cdot x_i)\n        $$\n        where $\\sigma(z) = \\frac{1}{1+e^{-z}}$ is the logistic sigmoid function.\n    3.  The cross-sectional variance of the newly computed actions, $\\operatorname{Var}(a^{(t+1)}(\\theta)) = \\frac{1}{N}\\sum_{i=1}^N (a_i^{(t+1)} - m^{(t+1)})^2$, is computed and stored. Note that $m^{(t+1)} = \\frac{1}{N}\\sum_{j=1}^N a_j^{(t+1)}$.\n\n- **Observables Calculation**: After the final time step $T$, the two observables are calculated.\n    -   $O_1(\\theta) = m^{(T)}(\\theta)$, the mean of the agent actions at the final time step $T$.\n    -   $O_2(\\theta) = \\frac{1}{T}\\sum_{t=1}^{T} \\operatorname{Var}(a^{(t)}(\\theta))$, the time-average of the cross-sectional variances computed from step $t=1$ to $t=T$.\n    \n- **Edge Cases**: Special handling is required for $T=0$. In this case, the simulation loop does not run. The final state is the initial state, so $O_1(\\theta) = m^{(0)} = 0.5$. By definition, $O_2(\\theta) = 0.0$. Both observables are constant and independent of $\\theta$ when $T=0$.\n\nThe simulation logic is encapsulated in a function `run_simulation(N, T, theta, seed)` that returns the tuple $(O_1(\\theta), O_2(\\theta))$.\n\n### 2. Sensitivity Upper Bound ($S_k^{\\text{bound}}$)\n\nThe sensitivity of an observable $O_k$ to small perturbations in $\\theta$ is analyzed using a first-order Taylor expansion: $\\Delta O_k \\approx \\nabla O_k(\\theta) \\cdot \\delta$, where $\\delta$ is the perturbation vector.\n\n- **Gradient Estimation**: The gradient $\\nabla O_k(\\theta) \\in \\mathbb{R}^3$ is estimated numerically for each observable $k \\in \\{1, 2\\}$ using the central finite difference method. The partial derivative with respect to each parameter component $\\theta_j$ (for $j \\in \\{0, 1, 2\\}$) is approximated as:\n    $$\n    \\frac{\\partial O_k}{\\partial \\theta_j} \\approx \\frac{O_k(\\theta + h \\cdot e_j) - O_k(\\theta - h \\cdot e_j)}{2h}\n    $$\n    where $e_j$ is the $j$-th standard basis vector in $\\mathbb{R}^3$ and $h$ is a small step size, chosen to be $h=10^{-6}$. This requires $2 \\times 3 = 6$ calls to the simulation function for each pair of observables.\n\n- **Bound Calculation**: We need to find an upper bound on the change $|\\Delta O_k|$ for all perturbations $\\delta$ satisfying the constraint $\\|\\delta\\|_\\infty \\le \\varepsilon$. The maximum of $|\\nabla O_k(\\theta) \\cdot \\delta|$ under this constraint is given by the product of $\\varepsilon$ and the dual norm of the gradient. The dual norm of the $L_\\infty$ norm is the $L_1$ norm. Thus, the sensitivity bound is:\n    $$\n    S_k^{\\text{bound}} = \\max_{\\|\\delta\\|_\\infty \\le \\varepsilon} |\\nabla O_k(\\theta) \\cdot \\delta| = \\varepsilon \\cdot \\|\\nabla O_k(\\theta)\\|_1\n    $$\n    where $\\|\\vec{v}\\|_1 = \\sum_j |v_j|$.\n\n### 3. Adversarial Corner Validation ($S_k^{\\text{corner}}$)\n\nThis step computes the true maximum change in observables by testing a specific, finite set of adversarial perturbations. The set of perturbations consists of the corners of the hypercube defined by the $L_\\infty$ bound, i.e., $\\delta \\in \\{-\\varepsilon, +\\varepsilon\\}^3$. This corresponds to the $2^3=8$ perturbations where each component $\\delta_j$ is either $-\\varepsilon$ or $+\\varepsilon$.\n\nThe procedure is as follows:\n1.  Compute the baseline observables $O_k(\\theta)$.\n2.  For each of the $8$ corner perturbation vectors $\\delta$, compute the perturbed observables $O_k(\\theta + \\delta)$.\n3.  The worst-case corner change for each observable $O_k$ is the maximum absolute difference found:\n    $$\n    S_k^{\\text{corner}} = \\max_{\\delta \\in \\{-\\varepsilon, +\\varepsilon\\}^3} \\left|O_k(\\theta+\\delta) - O_k(\\theta)\\right|\n    $$\n\nThis requires an additional $8$ calls to the simulation function. A special case exists if $\\varepsilon=0$, where the only perturbation is $\\delta = (0, 0, 0)$, resulting in $S_k^{\\text{corner}}=0$.\n\n### 4. Validation Decision\n\nThe final step is to make a binary validation decision. The model is declared to pass validation if and only if the empirically observed worst-case changes for both observables are within their respective specified tolerances, $\\tau_1$ and $\\tau_2$.\n$$\n\\text{pass} = \\begin{cases} \\text{True}, & \\text{if } S_1^{\\text{corner}} \\le \\tau_1 \\text{ and } S_2^{\\text{corner}} \\le \\tau_2 \\\\ \\text{False}, & \\text{otherwise} \\end{cases}\n$$\nThe final algorithm iterates through each test case, performs these four steps, and compiles the five result values—$S_1^{\\text{bound}}$, $S_1^{\\text{corner}}$, $S_2^{\\text{bound}}$, $S_2^{\\text{corner}}$, and the boolean 'pass'—into a list for each case. The collection of these lists forms the final output.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import expit  # Numerically stable logistic function\nfrom itertools import product\n\ndef solve():\n    \"\"\"\n    Solves the ABM validation problem for all test cases.\n    \"\"\"\n    \n    # Test cases parameters:\n    # (N, T, theta_0, theta_1, theta_2, epsilon, tau_1, tau_2, seed)\n    test_cases = [\n        (500, 50, 0.2, 1.1, 0.7, 0.05, 0.03, 0.02, 42),\n        (600, 20, -0.1, 0.9, 0.3, 0.0, 0.0, 0.0, 123),\n        (1000, 100, 0.0, 1.5, 0.5, 0.5, 0.05, 0.05, 7),\n        (300, 0, 0.4, 1.2, -0.6, 0.2, 1e-6, 1e-6, 2025),\n        (10, 10, -0.5, 0.8, 1.1, 0.01, 0.005, 0.005, 999),\n    ]\n\n    h = 1e-6 # Step size for central finite difference\n\n    # Cache to avoid re-running simulations with identical parameters\n    simulation_cache = {}\n\n    def run_simulation(N, T, theta_tuple, seed):\n        \"\"\"\n        Runs the ABM simulation for a given set of parameters.\n        \"\"\"\n        # Create a tuple key for caching\n        cache_key = (N, T, theta_tuple, seed)\n        if cache_key in simulation_cache:\n            return simulation_cache[cache_key]\n\n        theta = np.array(theta_tuple)\n        \n        # Handle T=0 edge case\n        if T == 0:\n            o1_final_mean = 0.5\n            o2_avg_var = 0.0\n            simulation_cache[cache_key] = (o1_final_mean, o2_avg_var)\n            return o1_final_mean, o2_avg_var\n\n        # Initialization\n        rng = np.random.default_rng(seed)\n        traits_x = rng.standard_normal(N)\n        actions = np.full(N, 0.5)\n        \n        # Time evolution\n        variances = []\n        for _ in range(T):\n            mean_action = np.mean(actions)\n            z = theta[0] + theta[1] * mean_action + theta[2] * traits_x\n            actions = expit(z)\n            variances.append(np.var(actions))\n\n        # Calculate final observables\n        o1_final_mean = np.mean(actions)\n        o2_avg_var = np.mean(variances)\n        \n        simulation_cache[cache_key] = (o1_final_mean, o2_avg_var)\n        return o1_final_mean, o2_avg_var\n\n    results = []\n    for case in test_cases:\n        N, T, theta0, theta1, theta2, epsilon, tau1, tau2, seed = case\n        theta_base = np.array([theta0, theta1, theta2])\n\n        # Shortcut for T=0: observables are constant, so all changes are 0.\n        if T == 0:\n            s1_bound, s1_corner = 0.0, 0.0\n            s2_bound, s2_corner = 0.0, 0.0\n            passed = (s1_corner <= tau1) and (s2_corner <= tau2)\n            results.append([s1_bound, s1_corner, s2_bound, s2_corner, passed])\n            continue\n\n        # Shortcut for epsilon=0: no perturbation, so all changes are 0.\n        if epsilon == 0.0:\n            s1_bound, s1_corner = 0.0, 0.0\n            s2_bound, s2_corner = 0.0, 0.0\n            passed = (s1_corner <= tau1) and (s2_corner <= tau2)\n            results.append([s1_bound, s1_corner, s2_bound, s2_corner, passed])\n            continue\n            \n        # 1. Baseline calculation\n        o1_base, o2_base = run_simulation(N, T, tuple(theta_base), seed)\n\n        # 2. Sensitivity computation (gradient and bounds)\n        grad_o1 = np.zeros(3)\n        grad_o2 = np.zeros(3)\n        for i in range(3):\n            theta_plus = theta_base.copy()\n            theta_plus[i] += h\n            theta_minus = theta_base.copy()\n            theta_minus[i] -= h\n            \n            o1_plus, o2_plus = run_simulation(N, T, tuple(theta_plus), seed)\n            o1_minus, o2_minus = run_simulation(N, T, tuple(theta_minus), seed)\n            \n            grad_o1[i] = (o1_plus - o1_minus) / (2 * h)\n            grad_o2[i] = (o2_plus - o2_minus) / (2 * h)\n        \n        s1_bound = epsilon * np.linalg.norm(grad_o1, ord=1)\n        s2_bound = epsilon * np.linalg.norm(grad_o2, ord=1)\n\n        # 3. Adversarial validation (corner check)\n        max_abs_delta_o1 = 0.0\n        max_abs_delta_o2 = 0.0\n        \n        # Generate 2^3=8 corners of the perturbation hypercube\n        for signs in product([-1, 1], repeat=3):\n            delta = epsilon * np.array(signs)\n            theta_perturbed = theta_base + delta\n            \n            o1_pert, o2_pert = run_simulation(N, T, tuple(theta_perturbed), seed)\n            \n            max_abs_delta_o1 = max(max_abs_delta_o1, abs(o1_pert - o1_base))\n            max_abs_delta_o2 = max(max_abs_delta_o2, abs(o2_pert - o2_base))\n            \n        s1_corner = max_abs_delta_o1\n        s2_corner = max_abs_delta_o2\n        \n        # 4. Validation decision\n        passed = (s1_corner <= tau1) and (s2_corner <= tau2)\n        \n        results.append([s1_bound, s1_corner, s2_bound, s2_corner, passed])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        }
    ]
}