{
    "hands_on_practices": [
        {
            "introduction": "To build a solid understanding of computational mechanics, we begin with the simplest possible case: a process with no memory. This exercise grounds the core concepts of statistical complexity ($C_\\mu$), entropy rate ($h_\\mu$), and excess entropy ($\\mathbf{E}$) by applying them to an independent and identically distributed (IID) process. By deriving the $\\epsilon$-machine and its associated information measures for a biased coin, you will establish a clear baseline for what it means for a process to be memoryless in this formal framework .",
            "id": "4118511",
            "problem": "Consider a stationary binary stochastic process $\\{X_t\\}_{t \\in \\mathbb{Z}}$ generated by independent flips of a biased coin, with $\\mathbb{P}(X_t = 1) = p$ and $\\mathbb{P}(X_t = 0) = 1 - p$ for every integer $t$, where $p \\in (0,1)$. Within the framework of computational mechanics, an $\\epsilon$-machine is the minimal unifilar predictive model whose states are the equivalence classes of semi-infinite pasts under the predictive equivalence relation. Starting from first principles—namely, the construction of causal states by grouping pasts that induce identical conditional distributions over futures—and using only the foundational definitions of the statistical complexity $C_{\\mu}$, the entropy rate $h_{\\mu}$, and the excess entropy $\\mathbf{E}$ as measures of memory, unpredictability, and predictable structure, respectively, perform the following:\n\n- Derive the $\\epsilon$-machine for this process by identifying its causal states and symbol-labeled transition structure.\n- Compute the statistical complexity $C_{\\mu}$ of the resulting $\\epsilon$-machine as the Shannon entropy (in nats) of its stationary distribution over causal states.\n- Compute the entropy rate $h_{\\mu}$ (in nats) as the limiting conditional entropy of the next symbol given the semi-infinite past.\n- Compute the excess entropy $\\mathbf{E}$ (in nats) as the mutual information between the semi-infinite past and the semi-infinite future.\n- Interpret each measure to explain what it captures about the process.\n\nReport all entropic quantities in nats by using natural logarithms. Express your final answer as a single row matrix containing, in order, the values of $C_{\\mu}$, $h_{\\mu}$, and $\\mathbf{E}$, in closed form as functions of $p$. No rounding is required.",
            "solution": "The problem statement is scientifically grounded, well-posed, objective, and complete. It presents a standard task in computational mechanics: to analyze an independent and identically distributed (IID) Bernoulli process. The problem is valid, and we may proceed with the solution.\n\nThe process is a stationary binary stochastic process $\\{X_t\\}_{t \\in \\mathbb{Z}}$, where each random variable $X_t$ is drawn independently from a common distribution. The alphabet is $\\mathcal{A} = \\{0, 1\\}$, with probabilities $\\mathbb{P}(X_t = 1) = p$ and $\\mathbb{P}(X_t = 0) = 1 - p$ for some constant $p \\in (0, 1)$.\n\n**1. Derivation of the $\\epsilon$-machine**\n\nThe core of computational mechanics is the concept of a causal state. Causal states are equivalence classes of semi-infinite pasts, where two pasts are considered equivalent if they lead to the same prediction for the future.\n\nLet a semi-infinite past be a specific realization $\\overleftarrow{x} = (\\dots, x_{-2}, x_{-1})$, where each $x_t \\in \\mathcal{A}$. Let the semi-infinite future be the random variable $\\overrightarrow{X} = (X_0, X_1, X_2, \\dots)$.\n\nThe predictive equivalence relation, denoted by $\\sim_{\\epsilon}$, is defined as follows: two pasts, $\\overleftarrow{x}$ and $\\overleftarrow{x}'$, are equivalent if and only if the conditional probability distributions they induce on the future are identical. Formally:\n$$ \\overleftarrow{x} \\sim_{\\epsilon} \\overleftarrow{x}' \\iff \\mathbb{P}(\\overrightarrow{X} | \\overleftarrow{X} = \\overleftarrow{x}) = \\mathbb{P}(\\overrightarrow{X} | \\overleftarrow{X} = \\overleftarrow{x}') $$\nThe causal states are the equivalence classes of this relation: $\\mathcal{S} = \\{ s = [\\overleftarrow{x}]_{\\sim_{\\epsilon}} \\}$.\n\nFor the given process, the random variables $\\{X_t\\}$ are independent and identically distributed. This means that the probability of any future sequence is independent of any past sequence. For any past $\\overleftarrow{x}$, the conditional probability of the future is simply the unconditional probability:\n$$ \\mathbb{P}(\\overrightarrow{X} | \\overleftarrow{X} = \\overleftarrow{x}) = \\mathbb{P}(\\overrightarrow{X}) $$\nThis is because for any finite future string $x_0x_1\\dots x_{L-1}$, its probability is $\\mathbb{P}(X_0=x_0, \\dots, X_{L-1}=x_{L-1}) = \\prod_{t=0}^{L-1} \\mathbb{P}(X_t=x_t)$, which does not depend on the past $\\overleftarrow{x}$.\n\nSince this holds for every possible past $\\overleftarrow{x}$, it follows that for any two pasts $\\overleftarrow{x}$ and $\\overleftarrow{x}'$:\n$$ \\mathbb{P}(\\overrightarrow{X} | \\overleftarrow{X} = \\overleftarrow{x}) = \\mathbb{P}(\\overrightarrow{X}) = \\mathbb{P}(\\overrightarrow{X} | \\overleftarrow{X} = \\overleftarrow{x}') $$\nTherefore, all possible semi-infinite pasts are predictively equivalent and belong to the same equivalence class. This means there is only a single causal state. Let us denote this state as $\\mathcal{S}_A$.\n\nThe $\\epsilon$-machine for this process is a one-state machine. From state $\\mathcal{S}_A$, the process emits a symbol $1$ with probability $p$ and a symbol $0$ with probability $1-p$. Since there is only one state, any transition must return to $\\mathcal{S}_A$. The transition structure is:\n- On emitting a $1$, transition from $\\mathcal{S}_A$ to $\\mathcal{S}_A$ with probability $p$.\n- On emitting a $0$, transition from $\\mathcal{S}_A$ to $\\mathcal{S}_A$ with probability $1-p$.\n\n**2. Statistical Complexity ($C_{\\mu}$)**\n\nThe statistical complexity $C_{\\mu}$ is the Shannon entropy of the stationary distribution over the causal states, $\\pi(\\mathcal{S})$. It quantifies the amount of memory (in nats) required for optimal prediction.\n$$ C_{\\mu} = H[\\pi] = - \\sum_{s \\in \\mathcal{S}} \\pi(s) \\ln(\\pi(s)) $$\nIn this case, the set of causal states is $\\mathcal{S} = \\{\\mathcal{S}_A\\}$. The stationary distribution is trivial, as the process is always in the single state $\\mathcal{S}_A$. Thus, $\\pi(\\mathcal{S}_A) = 1$.\nThe statistical complexity is therefore:\n$$ C_{\\mu} = - \\pi(\\mathcal{S}_A) \\ln(\\pi(\\mathcal{S}_A)) = -1 \\ln(1) = 0 $$\nInterpretation: A statistical complexity of $0$ signifies that the process has no memory. Knowledge of the past does not refine predictions about the future, so no information about the past needs to be stored. This is the hallmark of a memoryless (IID) process.\n\n**3. Entropy Rate ($h_{\\mu}$)**\n\nThe entropy rate $h_{\\mu}$ is the limiting conditional entropy of the next symbol given the semi-infinite past. It measures the irreducible uncertainty or intrinsic randomness of the process per time step.\n$$ h_{\\mu} = \\lim_{L \\to \\infty} H[X_0 | X_{-1}, X_{-2}, \\dots, X_{-L}] $$\nFor an IID process, the next symbol $X_0$ is, by definition, independent of all past symbols. Therefore, the conditional entropy is equal to the unconditional entropy of a single variable:\n$$ H[X_0 | X_{-1}, \\dots, X_{-L}] = H[X_0] $$\nThe entropy rate is simply the entropy of a single Bernoulli trial with success probability $p$.\n$$ h_{\\mu} = H[X_0] = - \\sum_{x \\in \\{0, 1\\}} \\mathbb{P}(X_0 = x) \\ln(\\mathbb{P}(X_0 = x)) $$\n$$ h_{\\mu} = -[\\mathbb{P}(X_0=1)\\ln(\\mathbb{P}(X_0=1)) + \\mathbb{P}(X_0=0)\\ln(\\mathbb{P}(X_0=0))] $$\n$$ h_{\\mu} = -[p \\ln(p) + (1-p) \\ln(1-p)] $$\nInterpretation: The entropy rate $h_{\\mu}$ is the fundamental unpredictability of the process. For an IID process, this is simply the uncertainty associated with a single event, as there are no temporal correlations to exploit for prediction.\n\n**4. Excess Entropy ($\\mathbf{E}$)**\n\nThe excess entropy $\\mathbf{E}$ is the mutual information between the semi-infinite past $\\overleftarrow{X}$ and the semi-infinite future $\\overrightarrow{X}$.\n$$ \\mathbf{E} = I[\\overleftarrow{X} ; \\overrightarrow{X}] $$\nIt quantifies the amount of predictable structure in the process, or how much information the past and future share. The mutual information $I[A; B]$ between two random variables $A$ and $B$ is zero if and only if they are independent.\n\nFor the given IID process, any block of past variables $\\overleftarrow{X} = (\\dots, X_{-2}, X_{-1})$ is independent of any block of future variables $\\overrightarrow{X} = (X_0, X_1, X_2, \\dots)$.\nTherefore, the mutual information between the past and future is zero.\n$$ \\mathbf{E} = I[\\overleftarrow{X} ; \\overrightarrow{X}] = 0 $$\nInterpretation: An excess entropy of $0$ indicates that there is no shared information between the past and the future. Observing the entire past provides no information that helps in predicting the entire future, beyond what is already known from the basic statistics of the process. This confirms the lack of temporal correlation or structure.\n\nIn summary, for an IID Bernoulli process with parameter $p$:\n- Statistical Complexity $C_{\\mu} = 0$: The process is memoryless.\n- Entropy Rate $h_{\\mu} = -p \\ln(p) - (1-p) \\ln(1-p)$: The process's randomness is the uncertainty of a single biased coin flip.\n- Excess Entropy $\\mathbf{E} = 0$: The process lacks any temporal structure or correlation between past and future.\nThe final answer is a row matrix containing these three values.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0 & -p \\ln(p) - (1-p) \\ln(1-p) & 0\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Having established a memoryless baseline, we now tackle a process with a simple but hidden internal structure. This practice challenges you to apply the principle of predictive equivalence to uncover the causal states of a noisy periodic system, moving beyond IID assumptions . The key insight you will develop is how observing the output sequence allows an observer to synchronize with the system's hidden phase, demonstrating the power of $\\epsilon$-machines as optimal models for revealing structure.",
            "id": "4118524",
            "problem": "Consider a stationary binary stochastic process $\\{X_t\\}_{t \\in \\mathbb{Z}}$ over alphabet $\\{0,1\\}$ generated by a two-state hidden source with an internal phase variable $\\{S_t\\}_{t \\in \\mathbb{Z}}$ taking values in $\\{A,B\\}$. The internal phase alternates deterministically: $S_{t+1} = \\sigma(S_t)$ where $\\sigma(A) = B$ and $\\sigma(B) = A$. Emission is as follows:\n- If $S_t = A$ (the \"$1$-phase\"), then $X_t = 1$ with probability $1-p$ and $X_t = 0$ with probability $p$.\n- If $S_t = B$ (the \"$0$-phase\"), then $X_t = 0$ with probability $1$.\n\nAssume $p \\in (0,1)$ is fixed and that the process has been running indefinitely so that it is in its stationary regime. This source realizes a noisy period-$2$ baseline in which the symbol $1$ would alternate deterministically with $0$ but is occasionally flipped to $0$ with small probability $p$ when the internal phase is $A$.\n\nUsing the fundamental definitions of computational mechanics, construct the minimal predictive model—the computational mechanics epsilon-machine (ε-machine)—by identifying causal states as equivalence classes of semi-infinite pasts that yield identical conditional distributions over semi-infinite futures. Then, from first principles of information theory, derive the entropy rate $h_{\\mu}$ of the observed process as a function of $p$.\n\nExpress $h_{\\mu}$ in bits per symbol. Provide your final answer as a single closed-form expression in terms of $p$. No rounding is required.",
            "solution": "The user wants to solve this problem.\nThe problem is validated as follows:\nStep 1: Extract Givens.\n- Process: Stationary binary stochastic process $\\{X_t\\}_{t \\in \\mathbb{Z}}$ over alphabet $\\mathcal{A} = \\{0,1\\}$.\n- Hidden Source: Two-state phase variable $\\{S_t\\}_{t \\in \\mathbb{Z}}$ with states $\\{A,B\\}$.\n- Phase Dynamics: $S_{t+1} = \\sigma(S_t)$ where $\\sigma(A) = B$ and $\\sigma(B) = A$.\n- Emission Probabilities:\n  - $P(X_t=1 | S_t=A) = 1-p$, $P(X_t=0 | S_t=A) = p$.\n  - $P(X_t=0 | S_t=B) = 1$, $P(X_t=1 | S_t=B) = 0$.\n- Parameter: $p \\in (0,1)$.\n- Assumption: The process is in its stationary regime.\n- Task 1: Construct the ε-machine by identifying causal states.\n- Task 2: Derive the entropy rate $h_{\\mu}$ as a function of $p$ in bits per symbol.\n\nStep 2: Validate Using Extracted Givens.\n- Scientific or Factual Unsoundness: The problem describes a Hidden Markov Model (HMM) with specific deterministic state transitions and stochastic emissions. This is a well-defined and standard model in the study of stochastic processes and computational mechanics. No scientific principles are violated.\n- Non-Formalizable or Irrelevant: The problem is a direct and standard application of the principles of computational mechanics and information theory, specifically related to ε-machines.\n- Incomplete or Contradictory Setup: The process is fully specified. The dynamics, emission probabilities, and the stationarity assumption provide a complete basis for analysis.\n- Unrealistic or Infeasible: The model is a valid theoretical construct, representing a noisy periodic process. It is not physically impossible or scientifically implausible.\n- Ill-Posed or Poorly Structured: The tasks of finding causal states and the entropy rate are well-defined. For a given stationary process, the ε-machine and its entropy rate are unique, so a unique solution exists.\n- Pseudo-Profound, Trivial, or Tautological: The problem requires a non-trivial application of the definition of causal states to discover the minimal predictive model, followed by a standard information-theoretic calculation. It is not trivial.\n- Outside Scientific Verifiability: The results are mathematically derivable and verifiable.\n\nStep 3: Verdict and Action.\nThe problem is **valid**. A full solution will be provided.\n\nA causal state is an equivalence class of semi-infinite pasts, $\\overleftarrow{x}_t = \\dots x_{t-2}x_{t-1}$. Two pasts $\\overleftarrow{x}_t$ and $\\overleftarrow{x}'_t$ are in the same causal state, denoted $\\overleftarrow{x}_t \\sim \\overleftarrow{x}'_t$, if and only if the conditional probability distributions of all possible futures are identical given either past: $P(\\overrightarrow{X}_t|\\overleftarrow{X}_t=\\overleftarrow{x}_t) = P(\\overrightarrow{X}_t|\\overleftarrow{X}_t=\\overleftarrow{x}'_t)$, where $\\overrightarrow{X}_t = X_t X_{t+1} \\dots$.\n\nFor the given process, the future evolution of the observed sequence $\\{X_t\\}$ depends on the future evolution of the hidden phase $\\{S_t\\}$. Since the phase dynamics $S_{t+1} = \\sigma(S_t)$ are deterministic, the entire future phase sequence $\\{S_t, S_{t+1}, \\dots\\}$ is known if the current phase $S_t$ is known. Therefore, the predictive capacity of a past $\\overleftarrow{x}_t$ is entirely captured by the conditional probability distribution, or belief, it induces on the current phase $S_t$, i.e., the vector $(P(S_t=A|\\overleftarrow{x}_t), P(S_t=B|\\overleftarrow{x}_t))$. Causal states correspond to distinct belief distributions that can be formed based on observing the past.\n\nWe seek to identify these distinct belief distributions. The key is to understand what information a past provides about $S_t$. The process is stationary and, since $p \\in (0,1)$, there is a non-zero probability of emitting a $1$ whenever the phase is $A$. As the phase alternates between $A$ and $B$, the system will almost surely not produce an infinite sequence of zeros. Thus, any past observed in the stationary regime will, with probability $1$, contain at least one symbol $1$.\n\nLet us analyze a past $\\overleftarrow{x}_t$ by locating the most recent occurrence of the symbol $1$. Let this occur at time $t-k-1$ for some integer $k \\ge 0$. The past thus has the form $\\dots 1 \\underbrace{00\\dots0}_{k}$.\nThe emission $X_{t-k-1}=1$ could only have been generated from the hidden phase $S_{t-k-1}=A$.\nUsing the deterministic phase dynamics $S_{i+1}=\\sigma(S_i)$, we can uniquely determine the phase at any subsequent time. The phase at time $t$ is found by applying the transition map $\\sigma$ for $t - (t-k-1) = k+1$ steps.\nThe phase sequence is $S_{t-k-1}=A$, $S_{t-k}=B$, $S_{t-k+1}=A$, and so on.\nThe phase $S_j$ is $A$ if $j-(t-k-1)$ is even, and $B$ if it is odd.\nFor the current time $t$, the difference is $t-(t-k-1) = k+1$.\n- If $k$ is even, $k+1$ is odd, which implies $S_t = \\sigma(A) = B$.\n- If $k$ is odd, $k+1$ is even, which implies $S_t = A$.\n\nThis analysis reveals that observing any past containing a $1$ is sufficient to synchronize our knowledge of the hidden phase, collapsing our belief to a certainty. We have only two possible belief distributions:\n1.  All pasts ending in $10^k$ where $k$ is an odd integer ($k=1, 3, 5, \\dots$). These pasts imply with certainty that the current hidden phase is $A$. We group these into a single causal state, $\\mathcal{C}_A$. For any $\\overleftarrow{x}_t \\in \\mathcal{C}_A$, we have $P(S_t=A|\\overleftarrow{x}_t)=1$.\n2.  All pasts ending in $10^k$ where $k$ is an non-negative even integer ($k=0, 2, 4, \\dots$). These pasts imply with certainty that the current hidden phase is $B$. We group these into a single causal state, $\\mathcal{C}_B$. For any $\\overleftarrow{x}_t \\in \\mathcal{C}_B$, we have $P(S_t=B|\\overleftarrow{x}_t)=1$.\n\nThese two states, $\\mathcal{C}_A$ and $\\mathcal{C}_B$, form the set of recurrent causal states for the ε-machine. We now determine the transitions between them. Let the system be in state $\\mathcal{C}_s$ at time $t$. We observe symbol $X_t=x$ and transition to state $\\mathcal{C}_{s'}$ at time $t+1$. The transition is labeled $x|P(x|\\mathcal{C}_s)$.\n- If the system is in $\\mathcal{C}_A$ at time $t$, we know $S_t=A$.\n  - It emits $X_t=1$ with probability $1-p$. The new past at time $t+1$ ends in $1$. This corresponds to $k=0$ (even), so the new state is $\\mathcal{C}_B$. The transition is $\\mathcal{C}_A \\xrightarrow{1|(1-p)} \\mathcal{C}_B$.\n  - It emits $X_t=0$ with probability $p$. The previous past ended in $10^k$ with $k$ odd. The new past ends in $10^{k+1}$, where $k+1$ is now even. The new state is therefore $\\mathcal{C}_B$. The transition is $\\mathcal{C}_A \\xrightarrow{0|p} \\mathcal{C}_B$.\n- If the system is in $\\mathcal{C}_B$ at time $t$, we know $S_t=B$.\n  - It emits $X_t=0$ with probability $1$. The previous past ended in $10^k$ with $k$ even. The new past ends in $10^{k+1}$, where $k+1$ is now odd. The new state is therefore $\\mathcal{C}_A$. The transition is $\\mathcal{C}_B \\xrightarrow{0|1} \\mathcal{C}_A$.\n  - It cannot emit $X_t=1$ as $P(X_t=1|S_t=B)=0$.\n\nThe ε-machine has two states $\\{\\mathcal{C}_A, \\mathcal{C}_B\\}$ with the following transitions:\n- $\\mathcal{C}_A \\xrightarrow{1|(1-p)} \\mathcal{C}_B$\n- $\\mathcal{C}_A \\xrightarrow{0|p} \\mathcal{C}_B$\n- $\\mathcal{C}_B \\xrightarrow{0|1} \\mathcal{C}_A$\n\nTo find the entropy rate $h_\\mu$, we first need the stationary probabilities of these causal states, denoted $\\pi_A = P(\\mathcal{C}_A)$ and $\\pi_B = P(\\mathcal{C}_B)$. Let $\\vec{\\pi} = (\\pi_A, \\pi_B)$ be the stationary distribution vector. It must be a left eigenvector of the state transition matrix $T$ with eigenvalue $1$, where $T_{ij} = P(\\text{next state is } j | \\text{current state is } i)$.\n$P(\\mathcal{C}_B | \\mathcal{C}_A) = P(X_t=1|\\mathcal{C}_A) + P(X_t=0|\\mathcal{C}_A) = (1-p) + p = 1$.\n$P(\\mathcal{C}_A | \\mathcal{C}_A) = 0$.\n$P(\\mathcal{C}_A | \\mathcal{C}_B) = P(X_t=0|\\mathcal{C}_B) = 1$.\n$P(\\mathcal{C}_B | \\mathcal{C}_B) = 0$.\nThe transition matrix is $T = \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}$. The stationary distribution must satisfy $\\vec{\\pi}T = \\vec{\\pi}$, which is $(\\pi_A, \\pi_B) \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix} = (\\pi_B, \\pi_A)$. This gives $\\pi_A = \\pi_B$. Together with the normalization $\\pi_A + \\pi_B = 1$, we find $\\pi_A = \\pi_B = \\frac{1}{2}$.\n\nThe entropy rate $h_{\\mu}$ of a process described by an ε-machine is the average uncertainty of the next symbol, given knowledge of the current causal state. The average is taken over the stationary distribution of the causal states. The formula, in bits per symbol, is:\n$$h_{\\mu} = \\sum_{s \\in \\{\\mathcal{C}_A, \\mathcal{C}_B\\}} \\pi_s H(X|s)$$\nwhere $H(X|s) = -\\sum_{x \\in \\{0,1\\}} P(x|s) \\log_2 P(x|s)$ is the Shannon entropy of the emission distribution from state $s$.\n\nWe calculate $H(X|s)$ for each state:\n- For state $\\mathcal{C}_A$:\n  The emission probabilities are $P(X_t=0|\\mathcal{C}_A)=p$ and $P(X_t=1|\\mathcal{C}_A)=1-p$.\n  $H(X|\\mathcal{C}_A) = -[p \\log_2(p) + (1-p)\\log_2(1-p)]$. This is the binary entropy function, $H_b(p)$.\n- For state $\\mathcal{C}_B$:\n  The emission probabilities are $P(X_t=0|\\mathcal{C}_B)=1$ and $P(X_t=1|\\mathcal{C}_B)=0$.\n  $H(X|\\mathcal C_B) = -[1 \\log_2(1) + 0 \\cdot \\lim_{q\\to 0} \\log_2(q)] = 0$. There is no uncertainty in the emission from state $\\mathcal{C}_B$.\n\nSubstituting these components into the formula for $h_{\\mu}$:\n$$h_{\\mu} = \\pi_A H(X|\\mathcal{C}_A) + \\pi_B H(X|\\mathcal{C}_B)$$\n$$h_{\\mu} = \\frac{1}{2} H_b(p) + \\frac{1}{2} \\cdot 0$$\n$$h_{\\mu} = \\frac{1}{2} H_b(p) = -\\frac{1}{2} [p \\log_2(p) + (1-p)\\log_2(1-p)]$$\nThis is the entropy rate of the observed process in bits per symbol.",
            "answer": "$$\n\\boxed{-\\frac{1}{2}\\left(p \\log_2(p) + (1-p)\\log_2(1-p)\\right)}\n$$"
        },
        {
            "introduction": "The final practice bridges the gap from abstract theory to practical application by having you implement a core component of an $\\epsilon$-machine reconstruction algorithm. You will develop a procedure to automatically induce approximate causal states directly from empirical count data, grappling with the real-world statistical challenges of finite samples and multiple hypothesis testing . This hands-on coding exercise provides a concrete method for applying the principles of computational mechanics to analyze observational time-series data.",
            "id": "4118473",
            "problem": "You are tasked with implementing an end-to-end approximate causal state induction procedure, grounded in Computational Mechanics and Epsilon-machines (ε-machines), using finite samples. In Computational Mechanics, causal states are defined as equivalence classes of pasts that yield identical predictive distributions of futures. In this problem, we restrict attention to the next-symbol predictive distribution as a finite-sample approximation. For a given set of empirical counts of next-symbol observations conditioned on each length-$L$ past, use hypothesis testing on distributional equivalence to induce approximate causal states.\n\nDefinitions and foundational base:\n- A past of length $L$ is denoted by $\\overleftarrow{x}^{L}$. The next-symbol is denoted by $x_{0}$.\n- For each past $\\overleftarrow{x}^{L}$, the predictive distribution is $P(x_{0} \\mid \\overleftarrow{x}^{L})$. In finite samples, this is estimated by empirical frequencies over a discrete alphabet of size $K$.\n- The Computational Mechanics equivalence relation-based definition of causal states is: two pasts $\\overleftarrow{x}^{L}$ and $\\overleftarrow{y}^{L}$ are equivalent if and only if $P(x_{0} \\mid \\overleftarrow{x}^{L}) = P(x_{0} \\mid \\overleftarrow{y}^{L})$ for all $x_{0}$ in the alphabet. An Epsilon-machine (ε-machine) is the minimal representation whose nodes are causal states and whose edges encode transition structure induced by the process.\n\nProcedure to implement:\n1. Use the chi-squared test for homogeneity to test the null hypothesis $H_{0}: P(x_{0} \\mid \\overleftarrow{x}^{L}_{i}) = P(x_{0} \\mid \\overleftarrow{x}^{L}_{j})$ for all $x_{0}$ between every pair of pasts $(i,j)$. Construct a $2 \\times K$ contingency table from empirical counts for the two pasts. For stability in low-count regimes, apply symmetric Dirichlet smoothing with pseudocount $\\lambda$ by adding $\\lambda$ to each cell before testing. Take $\\lambda = 0.5$.\n2. For multiple hypothesis testing, control the False Discovery Rate (FDR) using the Benjamini-Hochberg (BH) procedure at level $q$. Benjamini-Hochberg (BH) sorts all pairwise $p$-values $p_{(1)} \\le p_{(2)} \\le \\cdots \\le p_{(M)}$ and rejects all $H_{0}$ with $p_{(k)} \\le \\frac{k}{M} q$ for the largest such $k$, where $M$ is the number of tested pairs.\n3. Build an undirected acceptance graph on past indices $\\{0,1,\\dots,m-1\\}$: connect nodes $i$ and $j$ with an edge if the BH-corrected decision fails to reject $H_{0}$ for pair $(i,j)$ (i.e., they are not significantly different).\n4. Induce approximate causal states as the connected components of this acceptance graph. Each component is one approximate causal state. Sort the indices within each component in ascending order. Sort the list of components by their smallest index.\n\nScientific realism and consistency requirement:\n- Use the chi-squared test for homogeneity with Dirichlet smoothing $\\lambda = 0.5$.\n- Use BH control at level $q$.\n- The procedure is consistent under increasing sample sizes because the chi-squared test is consistent for distinguishing unequal multinomial distributions, and BH controls the false discovery rate; taking connected components of the acceptance graph converges to the true equivalence classes as the number of samples grows, under standard regularity conditions.\n\nInput data for implementation and test suite:\nYou are given three test cases. Each case specifies the count matrix $C^{(t)}$ of shape $m_{t} \\times K_{t}$, where row $i$ contains the counts for $P(x_{0} \\mid \\overleftarrow{x}^{L}_{i})$ over the alphabet categories, and the BH level $q^{(t)}$.\n\n- Test case $1$:\n  - $C^{(1)} = \\begin{bmatrix}\n  100 & 50 & 50 \\\\\n  100 & 50 & 50 \\\\\n  150 & 0 & 0 \\\\\n  0 & 0 & 200 \\\\\n  0 & 0 & 200 \\\\\n  150 & 0 & 0\n  \\end{bmatrix}$,\n  - $q^{(1)} = 0.1$.\n\n- Test case $2$ (low-count edge case):\n  - $C^{(2)} = \\begin{bmatrix}\n  1 & 0 & 0 \\\\\n  1 & 0 & 0 \\\\\n  0 & 1 & 0 \\\\\n  0 & 0 & 1\n  \\end{bmatrix}$,\n  - $q^{(2)} = 0.1$.\n\n- Test case $3$ (boundary with near-equivalence):\n  - $C^{(3)} = \\begin{bmatrix}\n  50 & 50 & 0 & 0 \\\\\n  51 & 49 & 0 & 0 \\\\\n  25 & 75 & 0 & 0 \\\\\n  0 & 0 & 40 & 60 \\\\\n  0 & 0 & 38 & 62\n  \\end{bmatrix}$,\n  - $q^{(3)} = 0.1$.\n\nOutput specification:\n- For each test case, output the induced partition as a list of lists of $0$-based past indices, with each inner list representing one approximate causal state.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with exactly three entries corresponding to the three test cases. For example, the format must be like $[result_{1},result_{2},result_{3}]$, where each $result_{t}$ is the list of lists described above.",
            "solution": "We begin from the core definition of causal states in Computational Mechanics: two pasts $\\overleftarrow{x}^{L}$ and $\\overleftarrow{y}^{L}$ are in the same causal state if and only if their predictive distributions over futures are identical. In practice, we approximate the future by the next symbol $x_{0}$ and use finite samples to estimate $P(x_{0} \\mid \\overleftarrow{x}^{L})$ via empirical counts over a discrete alphabet of size $K$.\n\nPrincipled statistical testing:\n- For any pair of pasts $(i,j)$, denote their empirical count vectors by $\\mathbf{c}_{i} \\in \\mathbb{N}^{K}$ and $\\mathbf{c}_{j} \\in \\mathbb{N}^{K}$. Under the null $H_{0}: P(x_{0} \\mid \\overleftarrow{x}^{L}_{i}) = P(x_{0} \\mid \\overleftarrow{x}^{L}_{j})$, the two multinomial distributions are equal. The test of homogeneity via the chi-squared statistic on the $2 \\times K$ contingency table is well-tested for comparing multinomial distributions.\n- Low counts and zero cells can destabilize expected counts. A standard remedy is symmetric Dirichlet smoothing: add a pseudocount $\\lambda$ to each cell, yielding $\\tilde{\\mathbf{c}}_{i} = \\mathbf{c}_{i} + \\lambda \\cdot \\mathbf{1}_{K}$ and $\\tilde{\\mathbf{c}}_{j} = \\mathbf{c}_{j} + \\lambda \\cdot \\mathbf{1}_{K}$ with $\\lambda = 0.5$ (Jeffreys prior). This maintains the test’s consistency while improving numerical stability.\n\nMultiple testing control:\n- For $m$ pasts, there are $M = \\frac{m(m-1)}{2}$ pairwise tests. To control the False Discovery Rate (FDR), use the Benjamini-Hochberg (BH) procedure at level $q$. Sort the $p$-values $p_{(1)} \\le p_{(2)} \\le \\cdots \\le p_{(M)}$ and find the largest $k$ such that $p_{(k)} \\le \\frac{k}{M}q$. Reject all $H_{0}$ with ranks $\\le k$; the remaining hypotheses are not rejected.\n- The BH procedure is well-tested and, under independence or certain positive dependence structures of $p$-values, controls the FDR at level $q$. In our context, it appropriately balances Type I and Type II errors across many pairwise comparisons.\n\nGraph-based induction of approximate causal states:\n- Construct an undirected acceptance graph $G$ on vertex set $\\{0,1,\\dots,m-1\\}$, connecting $i$ and $j$ with an edge if the BH decision fails to reject $H_{0}$ for that pair. This edge represents empirical evidence that the two pasts have indistinguishable next-symbol distributions at the chosen FDR level.\n- Induce approximate causal states by taking the connected components of $G$. Each connected component is a set of pasts mutually linked (possibly transitively) by non-rejected equivalence tests. Sorting indices within each component and sorting components by their smallest index fixes a canonical representation.\n\nConsistency:\n- The chi-squared test for homogeneity is consistent, meaning that for any two unequal multinomial distributions, as the total counts grow, the test rejects $H_{0}$ with probability tending to $1$. Conversely, for equal distributions, the $p$-values are asymptotically uniform on $[0,1]$, and BH controls the FDR, preventing over-merging with high probability.\n- As sample sizes increase, all truly unequal pairs will be rejected, and all truly equal pairs will fail to be rejected (up to the controlled FDR). The acceptance graph converges to a union of cliques corresponding to true equivalence classes. The connected components coincide with these classes in the large-sample limit. Dirichlet smoothing with fixed $\\lambda$ does not alter asymptotic decisions.\n\nAlgorithmic steps:\n1. For each test case, read $C^{(t)}$ and $q^{(t)}$.\n2. For each pair $(i,j)$, form the $2 \\times K$ contingency table with smoothed counts $\\lambda = 0.5$ and compute the chi-squared homogeneity test $p$-value.\n3. Apply BH at level $q^{(t)}$ to classify pairs into rejections and non-rejections.\n4. Build the acceptance graph from non-rejected pairs and compute connected components.\n5. Sort indices within components and sort components by their smallest index.\n6. Output the list of components for each test case.\n\nExpected behavior on the provided test suite:\n- Test case $1$: There are three distinct groups with identical distributions: indices $\\{0,1\\}$, $\\{2,5\\}$, and $\\{3,4\\}$. Cross-group pairs differ markedly and are rejected; within-group pairs are accepted. The induced partition is $[[0,1],[2,5],[3,4]]$.\n- Test case $2$: Low counts yield deterministic distributions for three distinct categories. The pair $\\{0,1\\}$ is identical and accepted; others are rejected. The induced partition is $[[0,1],[2],[3]]$.\n- Test case $3$: Near equivalence between $\\{0,1\\}$ and between $\\{3,4\\}$, with a clearly different $\\{2\\}$. The induced partition is $[[0,1],[2],[3,4]]$.\n\nThe final program implements this procedure and prints a single line containing the three induced partitions in the required format.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import chi2_contingency\nimport json\nfrom itertools import combinations\n\ndef benjamini_hochberg(pvals, pairs, q):\n    \"\"\"\n    Benjamini-Hochberg FDR control.\n    Parameters:\n        pvals: list of p-values\n        pairs: list of (i,j) corresponding to pvals\n        q: target FDR level\n    Returns:\n        rejected_pairs: set of pairs (i,j) where H0 is rejected\n    \"\"\"\n    m = len(pvals)\n    # Sort p-values with their pairs\n    sorted_indices = np.argsort(pvals)\n    p_sorted = np.array(pvals)[sorted_indices]\n    pairs_sorted = [pairs[idx] for idx in sorted_indices]\n\n    # Find largest k such that p_(k) = (k/m)*q\n    k_star = -1\n    for rank, p in enumerate(p_sorted, start=1):\n        if p = (rank / m) * q:\n            k_star = rank\n\n    rejected_pairs = set()\n    if k_star >= 1:\n        for idx in range(k_star):\n            rejected_pairs.add(pairs_sorted[idx])\n\n    return rejected_pairs\n\ndef induce_states(counts, q, lambda_smooth=0.5):\n    \"\"\"\n    Induce approximate causal states from counts via BH-corrected chi-square tests.\n    Parameters:\n        counts: numpy array of shape (m, K)\n        q: FDR level for BH\n        lambda_smooth: Dirichlet smoothing pseudocount added to each cell\n    Returns:\n        partition: list of lists of indices (each inner list is a connected component)\n    \"\"\"\n    m, K = counts.shape\n    # Prepare all pairwise tests\n    pvals = []\n    pairs = []\n    for i, j in combinations(range(m), 2):\n        # Apply symmetric Dirichlet smoothing\n        row_i = counts[i, :] + lambda_smooth\n        row_j = counts[j, :] + lambda_smooth\n        table = np.vstack([row_i, row_j])\n        # Chi-squared test for homogeneity\n        # correction=False to use Pearson's chi2 without Yates correction\n        chi2, p, dof, expected = chi2_contingency(table, correction=False)\n        # Handle potential numerical issues: ensure p is finite\n        if not np.isfinite(p):\n            # Fallback: if p is nan due to degeneracy, treat as highly different\n            p = 0.0\n        pvals.append(p)\n        pairs.append((i, j))\n\n    # Apply BH to determine which pairs are significantly different\n    rejected_pairs = benjamini_hochberg(pvals, pairs, q)\n\n    # Build acceptance graph: edge if NOT rejected\n    adjacency = {i: set() for i in range(m)}\n    for (i, j), p in zip(pairs, pvals):\n        if (i, j) not in rejected_pairs:\n            adjacency[i].add(j)\n            adjacency[j].add(i)\n\n    # Connected components via BFS\n    visited = [False] * m\n    components = []\n    for start in range(m):\n        if not visited[start]:\n            queue = [start]\n            visited[start] = True\n            comp = []\n            while queue:\n                u = queue.pop(0)\n                comp.append(u)\n                for v in adjacency[u]:\n                    if not visited[v]:\n                        visited[v] = True\n                        queue.append(v)\n            comp.sort()\n            components.append(comp)\n\n    # Sort components by their smallest index for canonical ordering\n    components.sort(key=lambda lst: lst[0] if lst else -1)\n    return components\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1\n        (\n            np.array([\n                [100, 50, 50],\n                [100, 50, 50],\n                [150, 0, 0],\n                [0,   0, 200],\n                [0,   0, 200],\n                [150, 0, 0]\n            ], dtype=float),\n            0.1\n        ),\n        # Test case 2 (low-count edge case)\n        (\n            np.array([\n                [1, 0, 0],\n                [1, 0, 0],\n                [0, 1, 0],\n                [0, 0, 1]\n            ], dtype=float),\n            0.1\n        ),\n        # Test case 3 (boundary with near-equivalence)\n        (\n            np.array([\n                [50, 50, 0, 0],\n                [51, 49, 0, 0],\n                [25, 75, 0, 0],\n                [0,  0, 40, 60],\n                [0,  0, 38, 62]\n            ], dtype=float),\n            0.1\n        )\n    ]\n\n    results = []\n    for counts, q in test_cases:\n        partition = induce_states(counts, q, lambda_smooth=0.5)\n        results.append(partition)\n\n    # Final print statement in the exact required format: single line, JSON-like with no spaces.\n    print(json.dumps(results, separators=(',', ':')))\n\nsolve()\n```"
        }
    ]
}