{
    "hands_on_practices": [
        {
            "introduction": "在分析经验数据时，一个核心任务是估计模型参数。对于幂律分布，最关键的参数是标度指数 $\\alpha$。虽然在对数-对数坐标下对数据进行线性回归是一种看似直观的方法，但它在统计上存在偏差且不够稳健。相比之下，最大似然估计（Maximum Likelihood Estimation, MLE）提供了更优良的统计属性，是参数估计的黄金标准。这个练习将引导你从第一性原理出发，推导当幂律尾部的下界 $x_{\\min}$ 已知时，指数 $\\alpha$ 的最大似然估计量 。掌握这个推导是进行更复杂幂律分析的基石。",
            "id": "4137125",
            "problem": "在一个复杂的自适应系统（如通信网络或生态系统）中，极端事件（例如，级联规模或城市规模）通常被观察到在某个下限阈值之上遵循重尾分布。假设一位研究人员从一个连续分布的尾部收集了 $n$ 个独立同分布 (i.i.d.) 的观测值 $\\{x_i\\}_{i=1}^{n}$，该分布在一个已知的下界 $x_{\\min}$ 之上表现出尺度不变性。具体来说，已知对于 $x \\ge x_{\\min}$，其概率密度函数 (PDF) 与 $x^{-\\alpha}$ 成正比（其中 $\\alpha  1$ 是某个未知的尾部指数），而在其他情况下为零。研究人员将此尾部建模为在 $[x_{\\min}, \\infty)$ 上的一个经过适当归一化的连续幂律（帕累托型）分布，并试图通过最大似然原理来估计 $\\alpha$。\n\n从第一性原理出发——即PDF的定义及其在 $[x_{\\min}, \\infty)$ 上的归一化、独立同分布样本的似然函数构造，以及对数似然函数相对于参数的最大化——推导最大似然估计量 (MLE) $\\hat{\\alpha}$ 关于 $n$、$x_{\\min}$ 和观测值 $\\{x_i\\}_{i=1}^{n}$ 的闭式表达式。在构建似然函数之前，您必须明确确定尾部PDF的归一化常数。确保您的推导过程验证了在约束条件 $\\alpha  1$ 下，所求得的解对应于一个全局最大值。\n\n您的最终答案必须是 $\\hat{\\alpha}$ 关于 $n$、$x_{\\min}$ 和 $\\{x_i\\}_{i=1}^{n}$ 的单个闭式解析表达式。不需要进行数值评估。除了上述基本定义和性质外，不要使用任何现成的公式或中间结果。不需要四舍五入，也不需要单位。",
            "solution": "问题陈述已经过分析并确定为有效。它具有科学依据，问题提法适定且客观，为标准的统计推导提供了一套完整且一致的条件。所有必要信息均已提供，不存在矛盾、歧义或事实错误。因此，我们可以开始求解。\n\n任务是为一个连续幂律分布的尾部指数 $\\alpha$ 推导最大似然估计量 (MLE)。给定概率密度函数 (PDF) 在 $x \\ge x_{\\min}$（其中下界 $x_{\\min} > 0$）时与 $x^{-\\alpha}$ 成正比，而在其他情况下为零。参数 $\\alpha$ 被约束为大于 $1$。\n\n首先，我们必须确定PDF的归一化常数。设PDF为 $p(x | \\alpha, x_{\\min}) = C x^{-\\alpha}$（对于 $x \\ge x_{\\min}$），其中 $C$ 是归一化常数。要使 $p(x)$ 成为一个有效的PDF，它在其支撑集上的积分必须等于 $1$。\n$$\n\\int_{x_{\\min}}^{\\infty} p(x | \\alpha, x_{\\min}) dx = 1\n$$\n代入PDF的形式，我们得到：\n$$\n\\int_{x_{\\min}}^{\\infty} C x^{-\\alpha} dx = 1\n$$\n我们可以将常数 $C$ 从积分中提出：\n$$\nC \\int_{x_{\\min}}^{\\infty} x^{-\\alpha} dx = 1\n$$\n该积分的计算如下：\n$$\n\\int x^{-\\alpha} dx = \\frac{x^{-\\alpha+1}}{-\\alpha+1} + \\text{constant}\n$$\n因为问题陈述 $\\alpha  1$，所以指数 $-\\alpha+1$ 是负数。这确保了当 $x \\to \\infty$ 时积分收敛。计算该定积分：\n$$\n\\int_{x_{\\min}}^{\\infty} x^{-\\alpha} dx = \\left[ \\frac{x^{1-\\alpha}}{1-\\alpha} \\right]_{x_{\\min}}^{\\infty} = \\lim_{b \\to \\infty} \\left( \\frac{b^{1-\\alpha}}{1-\\alpha} \\right) - \\frac{x_{\\min}^{1-\\alpha}}{1-\\alpha}\n$$\n当 $b \\to \\infty$ 且 $1-\\alpha  0$ 时，项 $b^{1-\\alpha} \\to 0$。积分变为：\n$$\n0 - \\frac{x_{\\min}^{1-\\alpha}}{1-\\alpha} = \\frac{x_{\\min}^{1-\\alpha}}{\\alpha-1}\n$$\n将此结果代回归一化方程：\n$$\nC \\left( \\frac{x_{\\min}^{1-\\alpha}}{\\alpha-1} \\right) = 1\n$$\n求解 $C$，我们得到归一化常数：\n$$\nC = (\\alpha-1) x_{\\min}^{\\alpha-1}\n$$\n因此，经过适当归一化的幂律尾部PDF为：\n$$\np(x | \\alpha, x_{\\min}) = (\\alpha-1) x_{\\min}^{\\alpha-1} x^{-\\alpha} \\quad \\text{for } x \\ge x_{\\min}\n$$\n\n接下来，我们为一组 $n$ 个独立同分布 (i.i.d.) 的观测值 $\\{x_i\\}_{i=1}^{n}$（其中每个 $x_i \\ge x_{\\min}$）构造似然函数。似然函数 $L(\\alpha | \\{x_i\\})$ 是在每个观测值处计算的单个概率密度的乘积：\n$$\nL(\\alpha | \\{x_i\\}) = \\prod_{i=1}^{n} p(x_i | \\alpha, x_{\\min})\n$$\n代入PDF的表达式：\n$$\nL(\\alpha) = \\prod_{i=1}^{n} \\left[ (\\alpha-1) x_{\\min}^{\\alpha-1} x_i^{-\\alpha} \\right]\n$$\n我们可以将依赖于 $\\alpha$ 和数据的项分开：\n$$\nL(\\alpha) = \\left[ (\\alpha-1) x_{\\min}^{\\alpha-1} \\right]^n \\left( \\prod_{i=1}^{n} x_i \\right)^{-\\alpha}\n$$\n\n为了简化最大化过程，我们使用对数似然函数 $\\mathcal{L}(\\alpha) = \\ln(L(\\alpha))$。对似然函数取自然对数得到：\n$$\n\\mathcal{L}(\\alpha) = \\ln \\left( \\left[ (\\alpha-1) x_{\\min}^{\\alpha-1} \\right]^n \\right) + \\ln \\left( \\left( \\prod_{i=1}^{n} x_i \\right)^{-\\alpha} \\right)\n$$\n使用对数性质 $\\ln(a^b) = b\\ln(a)$ 和 $\\ln(ab) = \\ln(a) + \\ln(b)$：\n$$\n\\mathcal{L}(\\alpha) = n \\ln\\left( (\\alpha-1) x_{\\min}^{\\alpha-1} \\right) - \\alpha \\ln\\left( \\prod_{i=1}^{n} x_i \\right)\n$$\n$$\n\\mathcal{L}(\\alpha) = n \\left[ \\ln(\\alpha-1) + \\ln(x_{\\min}^{\\alpha-1}) \\right] - \\alpha \\sum_{i=1}^{n} \\ln(x_i)\n$$\n$$\n\\mathcal{L}(\\alpha) = n \\ln(\\alpha-1) + n(\\alpha-1) \\ln(x_{\\min}) - \\alpha \\sum_{i=1}^{n} \\ln(x_i)\n$$\n为了找到最大似然估计量 $\\hat{\\alpha}$，我们将 $\\mathcal{L}(\\alpha)$ 对 $\\alpha$ 求导，并令结果为零。\n$$\n\\frac{d\\mathcal{L}}{d\\alpha} = \\frac{d}{d\\alpha} \\left[ n \\ln(\\alpha-1) + n\\alpha \\ln(x_{\\min}) - n \\ln(x_{\\min}) - \\alpha \\sum_{i=1}^{n} \\ln(x_i) \\right]\n$$\n$$\n\\frac{d\\mathcal{L}}{d\\alpha} = \\frac{n}{\\alpha-1} + n \\ln(x_{\\min}) - \\sum_{i=1}^{n} \\ln(x_i)\n$$\n将此导数设为零以找到临界点，我们将其记为 $\\hat{\\alpha}$：\n$$\n\\frac{n}{\\hat{\\alpha}-1} + n \\ln(x_{\\min}) - \\sum_{i=1}^{n} \\ln(x_i) = 0\n$$\n现在，我们求解 $\\hat{\\alpha}$：\n$$\n\\frac{n}{\\hat{\\alpha}-1} = \\sum_{i=1}^{n} \\ln(x_i) - n \\ln(x_{\\min})\n$$\n右侧可以使用性质 $n \\ln(a) = \\ln(a^n) = \\sum_{i=1}^n \\ln(a)$ 进行简化：\n$$\n\\frac{n}{\\hat{\\alpha}-1} = \\sum_{i=1}^{n} \\ln(x_i) - \\sum_{i=1}^{n} \\ln(x_{\\min}) = \\sum_{i=1}^{n} \\left[ \\ln(x_i) - \\ln(x_{\\min}) \\right]\n$$\n使用性质 $\\ln(a) - \\ln(b) = \\ln(a/b)$：\n$$\n\\frac{n}{\\hat{\\alpha}-1} = \\sum_{i=1}^{n} \\ln\\left(\\frac{x_i}{x_{\\min}}\\right)\n$$\n两边取倒数：\n$$\n\\frac{\\hat{\\alpha}-1}{n} = \\frac{1}{\\sum_{i=1}^{n} \\ln\\left(\\frac{x_i}{x_{\\min}}\\right)}\n$$\n$$\n\\hat{\\alpha}-1 = \\frac{n}{\\sum_{i=1}^{n} \\ln\\left(\\frac{x_i}{x_{\\min}}\\right)}\n$$\n最后，$\\alpha$ 的最大似然估计量为：\n$$\n\\hat{\\alpha} = 1 + \\frac{n}{\\sum_{i=1}^{n} \\ln\\left(\\frac{x_i}{x_{\\min}}\\right)}\n$$\n为验证这对应于一个最大值，我们计算对数似然函数的二阶导数：\n$$\n\\frac{d^2\\mathcal{L}}{d\\alpha^2} = \\frac{d}{d\\alpha} \\left( \\frac{n}{\\alpha-1} + n \\ln(x_{\\min}) - \\sum_{i=1}^{n} \\ln(x_i) \\right) = \\frac{d}{d\\alpha} \\left( n(\\alpha-1)^{-1} \\right) = -n(\\alpha-1)^{-2}\n$$\n$$\n\\frac{d^2\\mathcal{L}}{d\\alpha^2} = -\\frac{n}{(\\alpha-1)^2}\n$$\n由于 $n$（样本数量）是正数，且对于任何 $\\alpha \\neq 1$，$(\\alpha-1)^2$ 都是正数，因此二阶导数 $\\frac{d^2\\mathcal{L}}{d\\alpha^2}$ 在其定义域 $(\\alpha  1)$ 内对任何 $\\alpha$ 的值都恒为负。这表明对数似然函数是严格凹函数，因此我们找到的临界点是一个唯一的全局最大值。由于每个 $x_i \\ge x_{\\min}$，我们有 $\\frac{x_i}{x_{\\min}} \\ge 1$ 且 $\\ln(\\frac{x_i}{x_{\\min}}) \\ge 0$。只要至少有一个观测值 $x_i$ 严格大于 $x_{\\min}$（对于连续分布这是预期的），分母中的和就将为正，从而确保 $\\hat{\\alpha}  1$，这与初始参数约束一致。",
            "answer": "$$\n\\boxed{1 + \\frac{n}{\\sum_{i=1}^{n} \\ln\\left(\\frac{x_i}{x_{\\min}}\\right)}}\n$$"
        },
        {
            "introduction": "在处理真实世界的数据时，我们通常不仅不知道标度指数 $\\alpha$，连幂律行为开始的下界 $x_{\\min}$ 也是未知的。错误地选择 $x_{\\min}$ 会严重扭曲对 $\\alpha$ 的估计，并可能导致对数据分布的错误结论。此外，即使找到了最佳的参数组合，我们仍需一个严谨的方法来检验幂律模型本身是否是数据的合理解释。这个练习将介绍由 Clauset、Shalizi 和 Newman 提出的综合性方法，它被广泛认为是拟合和验证幂律分布的现代标准 。通过这个练习，你将理解如何系统地估计 $x_{\\min}$ 和 $\\alpha$，并利用参数化自举法（parametric bootstrap）进行拟合优度检验。",
            "id": "4137200",
            "problem": "一个大型自适应网络中的交互群体产生了代表事件规模的正值观测值 $\\{x_i\\}$，据假设，这些事件规模在一个未知的阈值 $x_{\\min}$ 之上遵循一个无标度尾部。在复杂自适应系统中，重尾现象通常由于聚集、反馈和异质性而产生，这促使我们将 $x \\ge x_{\\min}$ 的上尾部建模为一个指数为 $\\alpha$ 的连续幂律（帕累托）分布。考虑使用标准统计定义（包括参数模型的似然、累积分布函数（CDF）和 Kolmogorov–Smirnov（KS）统计量）来估计 $x_{\\min}$ 和 $\\alpha$ 并评估拟合优度的问题。哪个选项最能描述用于此任务的 Clauset–Shalizi–Newman 程序，并正确地论证了使用 KS 距离的合理性？\n\nA. 对于从观测值 $\\{x_i\\}$ 中抽取的每个候选阈值 $x_{\\min}$，使用子样本 $\\{x_i : x_i \\ge x_{\\min}\\}$ 通过最大似然估计 (MLE) 来估计 $\\alpha$，在 $x \\ge x_{\\min}$ 上构建模型 CDF $F(x \\mid \\alpha, x_{\\min})$，计算 Kolmogorov–Smirnov (KS) 距离 $D = \\sup_{x \\ge x_{\\min}} |S(x) - F(x \\mid \\alpha, x_{\\min})|$，其中 $S(x)$ 是尾部的经验 CDF，然后选择使 $D$ 最小化的 $x_{\\min}$。选择 $x_{\\min}$ 后，在尾部数据上通过 MLE 重新估计 $\\alpha$，并通过参数自举法评估拟合优度：从 $F(x \\mid \\hat{\\alpha}, \\hat{x}_{\\min})$ 重复模拟合成数据集，对每个数据集重新估计 $\\alpha$ 和 $x_{\\min}$，计算它们的 KS 统计量，并通过超过经验 $D$ 的合成 KS 距离的比例获得 $p$ 值。KS 距离的使用是合理的，因为它是一种非参数、无需分箱的 CDF 上的上确界范数，对所有尺度上的差异都很敏感；并且，在连续零假设下，当参数已知时，其参考分布是无分布的；当参数被估计时，参数自举法恢复了检验的有效性。\n\nB. 选择最小观测值作为 $x_{\\min}$，通过对所有 $\\{x_i\\}$ 的 $\\log$-频率 对 $\\log x$ 进行线性回归来估计 $\\alpha$，并在整个样本 $\\{x_i\\}$ 上计算单个 KS 统计量，而不限制于 $x \\ge x_{\\min}$。直接使用渐近 KS 临界值来获得 $p$ 值。KS 距离的使用是合理的，因为它比其他度量方法计算上更简单，并且对数-对数线性回归为幂律提供了无偏的斜率估计。\n\nC. 通过最小化 $\\{x_i\\}$ 的对数分箱直方图的 Pearson $\\chi^2$ 统计量来选择 $x_{\\min}$，通过矩估计法对尾部数据估计 $\\alpha$，并使用 KS 统计量，因为它对观测值之间的依赖性具有鲁棒性并强调尾部偏差。$p$ 值从标准 KS 表中计算，不对参数估计或阈值选择进行调整。\n\nD. 通过最大化所有 $\\{x_i\\}$ 上的总似然来联合选择 $(x_{\\min}, \\alpha)$，然后使用 Anderson–Darling (AD) 统计量验证拟合，但为方便起见称之为 KS。其使用 KS 距离的理由是，它相对于中心部分加权了尾部，因此在检测重尾数据中的尾部差异方面更具功效。\n\nE. 通过最小化经验互补累积分布函数 (CCDF) 与拟合的幂律 CCDF 在 $x \\ge x_{\\min}$ 上的均方误差来确定 $x_{\\min}$，通过对 $\\log$-CCDF 进行最小二乘法来估计 $\\alpha$，并在选定的尾部上计算单个 KS 统计量。使用 KS 距离的主要理由是它计算速度快，且其参考分布不受参数估计或阈值选择的影响。",
            "solution": "用户提供了一个关于将幂律分布拟合到经验数据尾部的统计程序的问题，具体是关于 Clauset、Shalizi 和 Newman (CSN) 开发的方法。我将首先验证问题陈述，然后进行详细解答。\n\n### 问题验证\n\n**步骤1：提取已知条件**\n- **数据**：一组代表事件规模的正值观测值 $\\{x_i\\}$。\n- **假设**：数据的上尾部，对于 $x \\ge x_{\\min}$，遵循一个无标度（幂律）分布。\n- **模型**：对于 $x \\ge x_{\\min}$，数据由指数为 $\\alpha$ 的连续幂律（帕累托）分布建模。阈值 $x_{\\min}$ 未知。\n- **背景**：该问题设定在复杂自适应系统领域，这类分布在该领域很常见。\n- **任务**：识别哪个选项最能描述用于估计 $x_{\\min}$ 和 $\\alpha$ 并评估拟合优度的 Clauset–Shalizi–Newman 程序。\n- **工具**：问题明确提到了使用似然、累积分布函数 (CDF) 和 Kolmogorov–Smirnov (KS) 统计量。\n\n**步骤2：使用已知条件进行验证**\n- **科学依据**：该问题在科学上和数学上都是合理的。拟合幂律分布是统计物理和复杂系统科学中一个成熟且重要的问题。Clauset–Shalizi–Newman 程序是完成此任务的经典、高被引用的参考方法，发表于《经验数据中的幂律分布》（SIAM Review, 2009）。最大似然估计 (MLE)、Kolmogorov–Smirnov 统计量和参数自举法都是标准的、严谨的统计技术。\n- **适定性**：该问题是适定的。它要求从一组选项中选出一个特定、明确定义的算法的正确描述。预期有一个唯一的正确答案。\n- **客观性**：问题以客观、技术性的语言陈述，没有歧义或主观论断。\n\n**步骤3：结论与行动**\n问题陈述是**有效**的。它清晰而准确地表述了复杂系统数据分析中的一个标准问题。我现在将进行解答。\n\n### 推导与选项分析\n\nClauset–Shalizi–Newman (CSN) 程序是一个三步法，用于将幂律分布拟合到经验数据并评估拟合的合理性。\n\n**1. 固定 $x_{\\min}$ 时幂律指数 $\\alpha$ 的估计**\n对于一个在 $x \\ge x_{\\min}$ 上遵循幂律分布的连续变量 $x$，其概率密度函数 (PDF) 为：\n$$p(x) = C x^{-\\alpha} = \\frac{\\alpha-1}{x_{\\min}} \\left( \\frac{x}{x_{\\min}} \\right)^{-\\alpha}$$\n对于一组 $n$ 个数据点 $\\{x_i\\}$，其中每个 $x_i \\ge x_{\\min}$，指数 $\\alpha$ 的最大似然估计量 (MLE) 为：\n$$ \\hat{\\alpha} = 1 + n \\left( \\sum_{i=1}^{n} \\ln \\frac{x_i}{x_{\\min}} \\right)^{-1} $$\n已知对于固定的 $x_{\\min}$，该估计量是一致的且偏差最小。\n\n**2. 下界 $x_{\\min}$ 的估计**\nCSN 方法将 $x_{\\min}$ 视为一个待从数据中估计的未知参数。其步骤如下：\n- 对于每个可能的 $x_{\\min}$ 值（通常是数据集中的每个唯一值），考虑数据子集 $\\{x_i : x_i \\ge x_{\\min}\\}$。\n- 对于这个子集，使用上述公式计算 MLE $\\hat{\\alpha}$。\n- 这给出了一个由参数 $(x_{\\min}, \\hat{\\alpha})$ 定义的候选幂律模型。通过测量经验数据分布与理论幂律模型之间的距离来评估该模型的质量。\n- CSN 方法为此目的使用 Kolmogorov–Smirnov (KS) 统计量 $D$。KS 统计量是数据累积分布函数 (CDF) 与拟合模型 CDF 之间的最大距离：\n$$ D = \\sup_{x \\ge x_{\\min}} |S(x) - P(x)| $$\n其中 $S(x)$ 是数据在 $x \\ge x_{\\min}$ 上的经验 CDF，而 $P(x)$ 是参数为 $\\hat{\\alpha}$ 和 $x_{\\min}$ 的拟合幂律模型的 CDF。\n- 下界的最优值 $\\hat{x}_{\\min}$ 是使这个距离 $D$ 最小化的值。即 $\\hat{x}_{\\min} = \\arg\\min_{x_{\\min}} D$。\n\n**3. 拟合优度检验**\n在估计了 $\\hat{x}_{\\min}$ 和相应的 $\\hat{\\alpha}$ 之后，必须评估幂律模型是否是对数据的一个合理解释。KS 统计量 $D$ 的一个较小值本身并不是充分的证据。\n- 一个关键问题是参数 $(\\alpha, x_{\\min})$ 是从数据本身估计出来的。这个拟合过程将模型 CDF “拉”向经验 CDF，从而使 KS 统计量的标准统计表失效，因为这些表格假设模型参数是*先验*已知的。\n- CSN 方法通过使用**参数自举法**生成一个 $p$ 值来解决这个问题。其步骤是：\n    a. 计算原始数据与其拟合模型 $(\\hat{x}_{\\min}, \\hat{\\alpha})$ 之间的 KS 统计量 $D_{\\text{empirical}}$。\n    b. 生成大量合成数据集。每个合成数据集的构造都使其尾部观测数量 ($n_{\\text{tail}}$) 与原始数据相同。这些观测值是从参数为 $(\\hat{x}_{\\min}, \\hat{\\alpha})$ 的拟合幂律分布中抽取的。非尾部部分也可以从经验非尾部数据中合成，以保持整体数据结构。\n    c. 对*每个*合成数据集，重复*整个*拟合过程：通过最小化 KS 统计量来估计其自身的 $(\\hat{x}_{\\min}^{\\text{syn}}, \\hat{\\alpha}^{\\text{syn}})$，就像对真实数据所做的那样。\n    d. 计算该合成数据集与其*自身拟合模型*之间的 KS 统计量 $D_{\\text{syn}}$。\n    e. $p$ 值是 $D_{\\text{syn}} \\ge D_{\\text{empirical}}$ 的合成数据集所占的比例。一个高的 $p$ 值（例如，$p  0.1$）表明观测到的偏差与真实幂律过程中预期的统计波动一致，因此该模型被认为是合理的拟合。一个低的 $p$ 值则表明数据不能很好地被幂律分布所描述。\n\n**KS 统计量的合理性**\n选择 KS 统计量有几个原因：\n- 它是**非参数**的，不对数据的分布做任何假设。\n- 它**无需分箱**，直接对排序后的数据点进行操作，从而避免了为 $\\chi^2$ 检验对数据进行分箱时所带来的任意选择和信息损失。\n- 作为一种**上确界范数**，它对尾部整个范围内的经验 CDF 和模型 CDF 之间的差异都很敏感。\n- 虽然在估计参数时其标准分布无效，但这可以通过**参数自举法**来纠正，从而实现有效的拟合优度检验。\n\n---\n\n### 选项评估\n\n**A.** 此选项陈述：\n1. 对于每个候选 $x_{\\min}$，通过 MLE 估计 $\\alpha$。\n2. 通过最小化 KS 距离 $D = \\sup_{x \\ge x_{\\min}} |S(x) - F(x)|$ 来选择 $x_{\\min}$。\n3. 通过参数自举法评估拟合优度，其中对每个合成数据集重新估计 $\\alpha$ 和 $x_{\\min}$，并将得到的 KS 值与经验值进行比较。\n4. KS 的合理性在于其非参数、无需分箱的性质，以及自举法纠正了参数估计的影响。\n此描述与上面概述的 CSN 程序完全一致。关于在选择 $x_{\\min}$ 后重新估计 $\\alpha$ 的说法是一个小的冗余，但并非错误，因为与所选 $\\hat{x}_{\\min}$ 对应的 $\\hat{\\alpha}$ 就是最终的估计值。\n**结论：正确**\n\n**B.** 此选项建议选择最小观测值作为 $x_{\\min}$，通过对数-对数线性回归估计 $\\alpha$，并使用标准 KS 表。CSN 论文和其他文献明确警告不要使用这些方法，因为它们在统计上是有偏的且不健全的。对数-对数回归会产生有偏的 $\\alpha$ 估计值，而在参数估计后使用标准 KS 表会导致不正确的 $p$ 值。\n**结论：错误**\n\n**C.** 此选项建议使用 $\\chi^2$ 统计量来估计 $x_{\\min}$，并使用矩估计法估计 $\\alpha$。这与 CSN 是一个完全不同的程序。CSN 方法特别提倡使用 KS 统计量以避免 $\\chi^2$ 检验所需的分箱，并提倡使用 MLE 而非矩估计法。此外，它错误地声称 KS 检验对依赖性具有鲁棒性，并且可以使用标准表。\n**结论：错误**\n\n**D.** 该选项建议对所有数据进行联合似然最大化，这不是 CSN 估计 $x_{\\min}$ 的方法。它错误地将 Anderson-Darling (AD) 统计量与 KS 统计量混为一谈。其提供的理由——该统计量对尾部加权——是 AD 统计量的一个特性，而不是 KS 统计量的。KS 统计量对整个 CDF 范围内的偏差给予同等权重。\n**结论：错误**\n\n**E.** 此选项建议通过最小化 CCDF 的均方误差来找到 $x_{\\min}$，并使用对数-CCDF 的最小二乘法来找到 $\\alpha$。这些不是 CSN 程序中使用的方法（CSN 分别使用 KS 最小化和 MLE）。至关重要的是，它错误地声称 KS 参考分布不受参数估计的影响，而这正是 CSN 自举程序旨在纠正的核心统计错误。\n**结论：错误**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "即使一个幂律模型通过了拟合优度检验，这仅表明该模型与数据是相容的，但并不意味着它是唯一或最佳的解释。在许多情况下，其他重尾分布，如对数正态分布（log-normal distribution），也能很好地拟合相同的数据，尤其是在数据范围有限时。因此，严谨的科学分析要求我们对这些替代理论进行直接比较。这个练习将引导你深入探讨一个关键的统计问题：如何在幂律和对数正态这两个非嵌套模型（non-nested models）之间做出选择 。你将构建一个基于似然比的检验统计量，并推导其在特定零假设下的渐近分布，这是高级模型选择技术的基石。",
            "id": "4137168",
            "problem": "考虑一个复杂适应系统，其中观测到的事件在已知阈值 $x_{\\min}  0$ 之上呈现重尾分布。假设一位研究人员使用幂律分布或截断对数正态分布对尾部 $x \\geq x_{\\min}$ 进行建模。幂律尾部的概率密度函数 (PDF) 为\n$$\nf_{\\mathrm{PL}}(x \\,|\\, \\alpha, x_{\\min}) = (\\alpha - 1) x_{\\min}^{\\alpha - 1} x^{-\\alpha}, \\quad x \\geq x_{\\min}, \\quad \\alpha  1,\n$$\n截断对数正态尾部的概率密度函数 (PDF) 为\n$$\nf_{\\mathrm{LN}}(x \\,|\\, \\mu, \\sigma, x_{\\min}) = \\frac{1}{x \\sigma \\sqrt{2 \\pi}} \\frac{\\exp\\!\\left( - \\frac{(\\ln x - \\mu)^{2}}{2 \\sigma^{2}} \\right)}{1 - \\Phi\\!\\left( \\frac{\\ln x_{\\min} - \\mu}{\\sigma} \\right)}, \\quad x \\geq x_{\\min}, \\quad \\sigma  0,\n$$\n其中 $\\Phi(\\cdot)$ 是标准正态分布的累积分布函数 (CDF)。设 $x_{1}, \\dots, x_{n}$ 是从尾部区域 $x \\geq x_{\\min}$ 中抽取的独立同分布 (i.i.d.) 观测值，其来自一个未知分布，该分布要么是幂律分布族的一员，要么是截断对数正态分布族的一员。\n\n从统计推断和独立同分布样本的渐近理论中的核心定义出发，包括Kullback–Leibler (KL) 散度、最大似然估计量 (MLEs) 的性质、大数定律 (LLN) 和中心极限定理 (CLT)，完成以下任务：\n\n1. 为拟合尾部数据的两个模型构建对数似然函数，并将每个观测值 $x_i$ 的样本对数似然比 $m_{i}$ 写为拟合对数密度之差。使用从尾部数据 $\\{ x_{i} \\geq x_{\\min} \\}$ 估计得到的幂律模型的 MLE $\\hat{\\alpha}$ 和截断对数正态模型的 MLE $(\\hat{\\mu}, \\hat{\\sigma})$。\n2. 定义平均对数似然比 $\\bar{m}_{n} = \\frac{1}{n} \\sum_{i=1}^{n} m_{i}$ 及其经验标准差 $s_{n} = \\left( \\frac{1}{n} \\sum_{i=1}^{n} (m_{i} - \\bar{m}_{n})^{2} \\right)^{1/2}$。构建用于比较两个非嵌套尾部模型的标准化似然比统计量 $V_{n} = \\frac{\\sqrt{n} \\, \\bar{m}_{n}}{s_{n}}$。\n3. 在原假设下，即两个候选模型在KL散度的意义上与真实数据生成过程同样接近（也就是说，最佳拟合幂律分布下的每个观测值的期望对数似然等于最佳拟合截断对数正态分布下的期望对数似然），通过援引适当的正则性条件和渐近结果，严格推导 $V_{n}$ 的渐近分布，过程中不使用任何简便公式。\n\n你的最终输出必须是在原假设下 $V_{n}$ 极限分布的渐近概率密度函数的一个单一闭式解析表达式。不需要进行数值近似。",
            "solution": "我们首先指定尾部模型的PDF、它们的对数似然函数以及每个观测值的对数似然比。对于幂律尾部，\n$$\nf_{\\mathrm{PL}}(x \\,|\\, \\alpha, x_{\\min}) = (\\alpha - 1) x_{\\min}^{\\alpha - 1} x^{-\\alpha}, \\quad x \\geq x_{\\min}, \\quad \\alpha  1,\n$$\n它在 $[x_{\\min}, \\infty)$ 上是归一化的，因为\n$$\n\\int_{x_{\\min}}^{\\infty} (\\alpha - 1) x_{\\min}^{\\alpha - 1} x^{-\\alpha} \\, dx = (\\alpha - 1) x_{\\min}^{\\alpha - 1} \\int_{x_{\\min}}^{\\infty} x^{-\\alpha} \\, dx = 1 \\quad \\text{for } \\alpha  1.\n$$\n对于截断对数正态尾部，\n$$\nf_{\\mathrm{LN}}(x \\,|\\, \\mu, \\sigma, x_{\\min}) = \\frac{1}{x \\sigma \\sqrt{2 \\pi}} \\frac{\\exp\\!\\left( - \\frac{(\\ln x - \\mu)^{2}}{2 \\sigma^{2}} \\right)}{1 - \\Phi\\!\\left( \\frac{\\ln x_{\\min} - \\mu}{\\sigma} \\right)}, \\quad x \\geq x_{\\min}, \\quad \\sigma  0,\n$$\n它也是归一化的，因为\n$$\n\\int_{x_{\\min}}^{\\infty} \\frac{1}{x \\sigma \\sqrt{2 \\pi}} \\exp\\!\\left( - \\frac{(\\ln x - \\mu)^{2}}{2 \\sigma^{2}} \\right) dx = 1 - \\Phi\\!\\left( \\frac{\\ln x_{\\min} - \\mu}{\\sigma} \\right),\n$$\n而除以 $1 - \\Phi\\!\\left( \\frac{\\ln x_{\\min} - \\mu}{\\sigma} \\right)$ 确保了尾部截断在 $[x_{\\min}, \\infty)$ 上对分布进行重新归一化。\n\n对于尾部的独立同分布观测值 $x_{1}, \\dots, x_{n}$，其对数似然函数为\n$$\n\\ell_{\\mathrm{PL}}(\\alpha) = \\sum_{i=1}^{n} \\ln f_{\\mathrm{PL}}(x_{i} \\,|\\, \\alpha, x_{\\min}), \\quad \\ell_{\\mathrm{LN}}(\\mu, \\sigma) = \\sum_{i=1}^{n} \\ln f_{\\mathrm{LN}}(x_{i} \\,|\\, \\mu, \\sigma, x_{\\min}).\n$$\n幂律指数的最大似然估计量 (MLE) 是通过在 $\\alpha  1$ 的范围内最大化 $\\ell_{\\mathrm{PL}}(\\alpha)$ 得到的。写作\n$$\n\\ln f_{\\mathrm{PL}}(x \\,|\\, \\alpha, x_{\\min}) = \\ln(\\alpha - 1) + (\\alpha - 1) \\ln x_{\\min} - \\alpha \\ln x,\n$$\n我们有\n$$\n\\ell_{\\mathrm{PL}}(\\alpha) = n \\ln(\\alpha - 1) + n (\\alpha - 1) \\ln x_{\\min} - \\alpha \\sum_{i=1}^{n} \\ln x_{i}.\n$$\n对 $\\alpha$ 求导并令其为零，\n$$\n\\frac{d \\ell_{\\mathrm{PL}}}{d \\alpha} = \\frac{n}{\\alpha - 1} + n \\ln x_{\\min} - \\sum_{i=1}^{n} \\ln x_{i} = 0,\n$$\n从而得到\n$$\n\\hat{\\alpha} = 1 + \\frac{n}{\\sum_{i=1}^{n} \\ln x_{i} - n \\ln x_{\\min}} = 1 + \\frac{n}{\\sum_{i=1}^{n} \\ln\\!\\left( \\frac{x_{i}}{x_{\\min}} \\right)}.\n$$\n对于截断对数正态分布，其 MLE $(\\hat{\\mu}, \\hat{\\sigma})$ 解得分方程\n$$\n\\frac{\\partial \\ell_{\\mathrm{LN}}}{\\partial \\mu} = 0, \\quad \\frac{\\partial \\ell_{\\mathrm{LN}}}{\\partial \\sigma} = 0,\n$$\n由于截断项 $1 - \\Phi\\!\\left( \\frac{\\ln x_{\\min} - \\mu}{\\sigma} \\right)$ 的存在，这些方程通常需要数值求解。在带有截断的正确设定的参数族中，对于 MLE 的标准正则性条件下，$(\\hat{\\mu}, \\hat{\\sigma})$ 是一致且渐近正态的。\n\n定义每个观测值的拟合对数似然比\n$$\nm_{i} = \\ln f_{\\mathrm{PL}}(x_{i} \\,|\\, \\hat{\\alpha}, x_{\\min}) - \\ln f_{\\mathrm{LN}}(x_{i} \\,|\\, \\hat{\\mu}, \\hat{\\sigma}, x_{\\min}).\n$$\n那么平均对数似然比为\n$$\n\\bar{m}_{n} = \\frac{1}{n} \\sum_{i=1}^{n} m_{i},\n$$\n及其经验标准差为\n$$\ns_{n} = \\left( \\frac{1}{n} \\sum_{i=1}^{n} (m_{i} - \\bar{m}_{n})^{2} \\right)^{1/2}.\n$$\n标准化似然比统计量定义为\n$$\nV_{n} = \\frac{\\sqrt{n} \\, \\bar{m}_{n}}{s_{n}}.\n$$\n\n我们现在用Kullback–Leibler散度来陈述原假设。设 $p^{\\star}(x)$ 表示限制在 $x \\geq x_{\\min}$ 上的真实数据生成密度。对于一个模型族 $f(x \\,|\\, \\theta)$，从 $p^{\\star}$ 到 $f(\\cdot \\,|\\, \\theta)$ 的KL散度为\n$$\nD_{\\mathrm{KL}}\\!\\left( p^{\\star} \\,\\|\\, f(\\cdot \\,|\\, \\theta) \\right) = \\int_{x_{\\min}}^{\\infty} p^{\\star}(x) \\ln \\frac{p^{\\star}(x)}{f(x \\,|\\, \\theta)} \\, dx.\n$$\n每个族内的KL最优参数最小化该散度，等价于最大化期望对数似然，\n$$\n\\theta_{\\mathrm{PL}}^{\\star} \\in \\arg \\max_{\\alpha  1} \\int p^{\\star}(x) \\ln f_{\\mathrm{PL}}(x \\,|\\, \\alpha, x_{\\min}) \\, dx, \\quad \\theta_{\\mathrm{LN}}^{\\star} \\in \\arg \\max_{\\mu \\in \\mathbb{R}, \\sigma  0} \\int p^{\\star}(x) \\ln f_{\\mathrm{LN}}(x \\,|\\, \\mu, \\sigma, x_{\\min}) \\, dx.\n$$\n在标准条件下，MLE 依概率收敛于这些KL最优参数：\n$$\n\\hat{\\alpha} \\xrightarrow{p} \\alpha_{\\mathrm{PL}}^{\\star}, \\quad (\\hat{\\mu}, \\hat{\\sigma}) \\xrightarrow{p} (\\mu_{\\mathrm{LN}}^{\\star}, \\sigma_{\\mathrm{LN}}^{\\star}).\n$$\n令\n$$\nM(x) = \\ln f_{\\mathrm{PL}}(x \\,|\\, \\alpha_{\\mathrm{PL}}^{\\star}, x_{\\min}) - \\ln f_{\\mathrm{LN}}(x \\,|\\, \\mu_{\\mathrm{LN}}^{\\star}, \\sigma_{\\mathrm{LN}}^{\\star}, x_{\\min}),\n$$\n并注意到，由于估计量的一致性和对数似然函数的光滑性，对于大的 $n$，有 $m_{i} \\approx M(x_{i})$。两个分布族在KL意义上同样接近的原假设是\n$$\nH_{0}: \\quad \\int_{x_{\\min}}^{\\infty} p^{\\star}(x) \\ln f_{\\mathrm{PL}}(x \\,|\\, \\alpha_{\\mathrm{PL}}^{\\star}, x_{\\min}) \\, dx = \\int_{x_{\\min}}^{\\infty} p^{\\star}(x) \\ln f_{\\mathrm{LN}}(x \\,|\\, \\mu_{\\mathrm{LN}}^{\\star}, \\sigma_{\\mathrm{LN}}^{\\star}, x_{\\min}) \\, dx,\n$$\n等价地，\n$$\n\\mathbb{E}_{p^{\\star}}[ M(X) ] = 0.\n$$\n\n我们在 $H_0$ 下推导 $V_n$ 的渐近分布。假设以下正则性条件成立：\n- 独立同分布的尾部数据 $x_{1}, \\dots, x_{n}$ 是从支撑在 $[x_{\\min}, \\infty)$ 上的 $p^{\\star}$ 中抽取的。\n- KL最优参数存在且唯一，对数密度函数在参数上二次连续可微，并有适当的控制收敛保证。\n- 拟合对数似然比 $m_i$ 在 $p^{\\star}$ 下具有有限方差，即 $\\mathbb{E}_{p^{\\star}}[ M(X)^{2} ]  \\infty$。\n- MLE 是一致的并且满足渐近线性，因此用 MLE 替换真实的KL最优参数对均值的影响为 $o_{p}(1)$。\n\n在 $H_0$ 和这些条件下，序列 $\\{ m_{i} \\}$ 是渐近独立同分布的，其均值为零，方差为 $\\sigma^{2} = \\mathrm{Var}_{p^{\\star}}( M(X) )$。根据大数定律，\n$$\n\\bar{m}_{n} \\xrightarrow{p} \\mathbb{E}_{p^{\\star}}[ M(X) ] = 0,\n$$\n根据中心极限定理，\n$$\n\\sqrt{n} \\, \\bar{m}_{n} \\xrightarrow{d} \\mathcal{N}(0, \\sigma^{2}).\n$$\n此外，样本方差 $s_{n}^{2} = \\frac{1}{n} \\sum_{i=1}^{n} (m_{i} - \\bar{m}_{n})^{2}$ 满足 $s_{n}^{2} \\xrightarrow{p} \\sigma^{2}$（在有限二阶矩条件下经验方差的一致性）。根据斯卢茨基(Slutsky)定理，\n$$\nV_{n} = \\frac{\\sqrt{n} \\, \\bar{m}_{n}}{s_{n}} \\xrightarrow{d} \\mathcal{N}(0, 1).\n$$\n因此，标准化似然比统计量 $V_n$ 的渐近零分布是标准正态分布。标准正态分布的概率密度函数 (PDF) 是\n$$\nf_{V}(z) = \\frac{1}{\\sqrt{2 \\pi}} \\exp\\!\\left( - \\frac{z^{2}}{2} \\right).\n$$\n\n此推导仅使用了基础的渐近结果：用大数定律确立均值的收敛性，用中心极限定理获得尺度化均值的正态极限，以及用斯卢茨基(Slutsky)定理来证明用一致估计量 $s_n$ 替换未知标准差的合理性，所有这些都在原假设（即每个观测值的期望拟合对数似然相等）下进行，该假设表达了两个非嵌套尾部模型在Kullback–Leibler接近度上的相等性。",
            "answer": "$$\\boxed{\\frac{1}{\\sqrt{2 \\pi}} \\exp\\!\\left(-\\frac{z^{2}}{2}\\right)}$$"
        }
    ]
}