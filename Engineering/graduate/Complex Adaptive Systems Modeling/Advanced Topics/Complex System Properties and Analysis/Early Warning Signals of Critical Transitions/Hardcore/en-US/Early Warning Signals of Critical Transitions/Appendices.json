{
    "hands_on_practices": [
        {
            "introduction": "The theory of critical slowing down is not just a qualitative concept; it has a precise mathematical foundation rooted in the theory of stochastic processes. This first exercise guides you through a foundational derivation that reveals why variance is a key early warning signal. By solving the Fokker-Planck equation for an Ornstein-Uhlenbeck process—a canonical model for fluctuations around a stable equilibrium—you will analytically prove how the variance diverges as the system's stability (represented by the parameter $\\alpha$) approaches zero . This practice provides a direct link between the abstract loss of stability and a measurable statistical signature.",
            "id": "4121027",
            "problem": "In the study of early warning signals of critical transitions in complex adaptive systems, stochastic fluctuations around a stable equilibrium can often be approximated by a linear Itô stochastic differential equation known as the Ornstein–Uhlenbeck (OU) process. Consider the OU process defined by the stochastic differential equation $$dx = \\alpha x \\, dt + \\sigma \\, dW_t,$$ where $x$ is the state variable, $t$ is time, $W_t$ is standard Brownian motion (also called the Wiener process), $\\alpha0$ is a constant drift parameter representing the local linear stability of the equilibrium, and $\\sigma0$ is a constant noise amplitude. Assume natural boundary conditions so that a stationary probability density exists for $\\alpha0$. Starting from fundamental definitions of Itô calculus and the Fokker–Planck equation, derive the stationary variance $\\mathrm{Var}[x]$ of $x$ under the stationary distribution. Then, interpret the behavior of $\\mathrm{Var}[x]$ as $\\alpha\\to 0^{-}$ in the context of early warning signals of critical transitions. Express your final answer as a closed-form analytic expression for the stationary variance in terms of $\\alpha$ and $\\sigma$. No rounding is required, and no units are needed.",
            "solution": "The problem statement is found to be valid as it is scientifically grounded in the established theory of stochastic processes and complex systems, is well-posed with all necessary information provided, and is expressed with objective, unambiguous language. We may therefore proceed with the derivation and analysis.\n\nThe system is described by the Ornstein–Uhlenbeck process, a linear Itô stochastic differential equation (SDE) given by:\n$$\ndx = \\alpha x \\, dt + \\sigma \\, dW_t\n$$\nHere, $x$ is the state variable, $\\alpha  0$ is the drift parameter indicating stability, $\\sigma  0$ is the constant noise intensity, and $W_t$ is a standard Wiener process. We are tasked with finding the stationary variance of $x$ and interpreting its behavior as $\\alpha \\to 0^{-}$.\n\nOur primary tool is the Fokker–Planck equation (FPE), which describes the time evolution of the probability density function, $p(x, t)$, of the state variable. For a general SDE of the form $dX_t = f(X_t, t) \\, dt + g(X_t, t) \\, dW_t$, the FPE is:\n$$\n\\frac{\\partial p(x, t)}{\\partial t} = -\\frac{\\partial}{\\partial x} [f(x, t) p(x, t)] + \\frac{1}{2} \\frac{\\partial^2}{\\partial x^2} [g(x, t)^2 p(x, t)]\n$$\nIn our specific case, the drift function is $f(x) = \\alpha x$ and the diffusion function is $g(x) = \\sigma$. Both are time-independent. Substituting these into the general FPE yields the FPE for the Ornstein–Uhlenbeck process:\n$$\n\\frac{\\partial p(x, t)}{\\partial t} = -\\frac{\\partial}{\\partial x} [\\alpha x p(x, t)] + \\frac{1}{2} \\frac{\\partial^2}{\\partial x^2} [\\sigma^2 p(x, t)]\n$$\nWe seek the stationary probability density, $p_s(x)$, which is defined as the solution where the probability density no longer changes with time. Thus, we set $\\frac{\\partial p_s(x)}{\\partial t} = 0$:\n$$\n0 = -\\frac{d}{dx} [\\alpha x p_s(x)] + \\frac{1}{2} \\frac{d^2}{dx^2} [\\sigma^2 p_s(x)]\n$$\nSince $\\sigma$ is a constant, we can write:\n$$\n0 = -\\frac{d}{dx} [\\alpha x p_s(x)] + \\frac{\\sigma^2}{2} \\frac{d^2 p_s(x)}{d x^2}\n$$\nThis equation can be expressed in terms of the probability current, $J(x)$:\n$$\n\\frac{dJ(x)}{dx} = 0 \\quad \\text{where} \\quad J(x) = \\alpha x p_s(x) - \\frac{\\sigma^2}{2} \\frac{d p_s(x)}{d x}\n$$\nThe condition $\\frac{dJ(x)}{dx} = 0$ implies that the probability current $J(x)$ is a constant. For a process with a stationary distribution, we must impose natural boundary conditions, which state that $p_s(x) \\to 0$ as $x \\to \\pm\\infty$. This physically means that the probability of finding the particle at infinity is zero. Under these conditions, the probability current must be zero everywhere, i.e., $J(x) = 0$. This gives us a first-order ordinary differential equation for $p_s(x)$:\n$$\n\\alpha x p_s(x) - \\frac{\\sigma^2}{2} \\frac{d p_s(x)}{d x} = 0\n$$\nRearranging the terms to solve for $p_s(x)$:\n$$\n\\frac{d p_s(x)}{d x} = \\frac{2\\alpha}{\\sigma^2} x p_s(x)\n$$\nThis is a separable differential equation:\n$$\n\\frac{1}{p_s(x)} \\, dp_s(x) = \\frac{2\\alpha}{\\sigma^2} x \\, dx\n$$\nIntegrating both sides:\n$$\n\\int \\frac{1}{p_s(x)} \\, dp_s(x) = \\int \\frac{2\\alpha}{\\sigma^2} x \\, dx\n$$\n$$\n\\ln(p_s(x)) = \\frac{\\alpha}{\\sigma^2} x^2 + C_0\n$$\nwhere $C_0$ is the constant of integration. Exponentiating both sides gives the form of the stationary distribution:\n$$\np_s(x) = C \\exp\\left(\\frac{\\alpha x^2}{\\sigma^2}\\right)\n$$\nwhere $C = \\exp(C_0)$ is the normalization constant. We can identify this as the probability density function of a Gaussian (normal) distribution. A general Gaussian distribution with mean $\\mu$ and variance $\\Sigma^2$ has a density function of the form:\n$$\np(x) = \\frac{1}{\\sqrt{2\\pi\\Sigma^2}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\Sigma^2}\\right)\n$$\nBy comparing the exponent of our solution for $p_s(x)$ with the standard Gaussian form, we can determine the mean and variance. The absence of a term linear in $x$ in the exponent implies the mean is $\\mu=0$. We can then equate the quadratic terms:\n$$\n\\frac{\\alpha x^2}{\\sigma^2} = -\\frac{x^2}{2\\Sigma^2}\n$$\nFrom this, we deduce the variance $\\Sigma^2$ of the stationary distribution:\n$$\n\\frac{\\alpha}{\\sigma^2} = -\\frac{1}{2\\Sigma^2} \\implies \\Sigma^2 = -\\frac{\\sigma^2}{2\\alpha}\n$$\nThus, the stationary variance of the state variable $x$, denoted $\\mathrm{Var}[x]$, is:\n$$\n\\mathrm{Var}[x] = \\Sigma^2 = -\\frac{\\sigma^2}{2\\alpha}\n$$\nNote that since $\\alpha  0$ and $\\sigma  0$, the variance is guaranteed to be positive, as required.\n\nNow, we interpret the behavior of this variance as $\\alpha \\to 0^{-}$. The parameter $\\alpha$ represents the strength of the linear restoring force that pulls the system state $x$ back towards the stable equilibrium at $x=0$. A value of $\\alpha$ that is large in magnitude (i.e., very negative) corresponds to a strong restoring force and a very stable equilibrium. As a system approaches a critical transition (or tipping point), the stability of its equilibrium state diminishes. In this linearized model, this loss of stability is represented by the weakening of the restoring force, which corresponds to the parameter $\\alpha$ approaching zero from the negative side, i.e., $\\alpha \\to 0^{-}$. This phenomenon is known as \"critical slowing down,\" as the characteristic timescale of return to equilibrium, which is proportional to $1/|\\alpha|$, diverges.\n\nExamining the expression for the stationary variance, $\\mathrm{Var}[x] = -\\frac{\\sigma^2}{2\\alpha}$, we can take the limit as $\\alpha \\to 0^{-}$:\n$$\n\\lim_{\\alpha \\to 0^{-}} \\mathrm{Var}[x] = \\lim_{\\alpha \\to 0^{-}} \\left(-\\frac{\\sigma^2}{2\\alpha}\\right) = +\\infty\n$$\nThis result demonstrates that as the system's stability decreases ($\\alpha \\to 0^{-}$), the variance of the stochastic fluctuations around the equilibrium point diverges to infinity, even for a constant noise level $\\sigma$. Intuitively, as the restoring force weakens, the random kicks from the noise term are able to push the system much further from its equilibrium before being pulled back. This dramatic increase in variance serves as a key generic early warning signal for an impending critical transition. Measuring the variance of a system's output over time can thus provide an indication of its proximity to a tipping point.",
            "answer": "$$\\boxed{-\\frac{\\sigma^2}{2\\alpha}}$$"
        },
        {
            "introduction": "Translating theory into practice is a core skill in complex systems modeling. This exercise challenges you to build a complete computational pipeline to detect early warning signals in simulated time series data . You will generate data from a system undergoing a saddle-node bifurcation, apply standard processing techniques like detrending, and compute the canonical rolling-window indicators of variance and autocorrelation. This hands-on implementation will solidify your understanding of the practical workflow used in EWS research and build intuition for how these statistical signals appear before a critical transition.",
            "id": "4120962",
            "problem": "Consider a one-dimensional stochastic dynamical system that undergoes a regime shift due to a slowly changing control parameter. Let the state be a scalar time series $P(t)$ generated by the stochastic differential equation (SDE) in the saddle-node normal form with additive noise,\n$$\n\\mathrm{d}P = \\left(\\mu(t) - P^2\\right) \\, \\mathrm{d}t + \\sigma \\, \\mathrm{d}W_t,\n$$\nwhere $\\mu(t)$ is a slowly varying control parameter, $\\sigma$ is a nonnegative constant noise amplitude, and $W_t$ is a standard Wiener process (Brownian motion). Assume $\\mu(t)$ decreases linearly over time, $\\mu(t) = \\mu_0 - v t$, with $\\mu_0  0$ and $v \\ge 0$. A regime shift occurs when the stable equilibrium disappears at $\\mu(t_c) = 0$, that is, at $t_c = \\mu_0 / v$ if $v  0$.\n\nYou will construct a discrete-time approximation using the Euler–Maruyama scheme (a first-order method for SDEs), with time step $\\Delta t  0$:\n$$\nP_{t+1} = P_t + \\Delta t\\left(\\mu(t) - P_t^2\\right) + \\sigma \\sqrt{\\Delta t} \\, \\xi_t,\n$$\nwhere $\\xi_t \\sim \\mathcal{N}(0,1)$ are independent standard normal random variables. Initialize at the deterministic equilibrium $P_0 = \\sqrt{\\mu_0}$.\n\nTo assess early warning signals, implement detrending followed by rolling-window indicators over the pre-transition segment. Use a moving average of width $w_d$ to estimate the trend,\n$$\n\\widehat{m}(t) = \\frac{1}{w_d}\\sum_{j = t - \\lfloor w_d/2 \\rfloor}^{t + \\lfloor w_d/2 \\rfloor} P(j),\n$$\nwith appropriate boundary handling, and compute the detrended series\n$$\nX(t) = P(t) - \\widehat{m}(t).\n$$\nOver rolling windows of width $w$, ending at times $t$, compute the following indicators on the detrended data within each window:\n- The sample variance with Bessel’s correction,\n$$\n\\operatorname{Var}_w(t) = \\frac{1}{w - 1}\\sum_{j = t-w+1}^{t} \\left(X(j) - \\overline{X}_{w}(t)\\right)^2, \\quad \\text{where} \\quad \\overline{X}_{w}(t) = \\frac{1}{w}\\sum_{j = t-w+1}^{t} X(j).\n$$\n- The lag-$1$ autocorrelation,\n$$\n\\rho_1^{(w)}(t) = \\frac{\\sum_{j = t-w+1}^{t-1} \\left(X(j) - \\overline{X}_{w}(t)\\right)\\left(X(j+1) - \\overline{X}_{w}(t)\\right)}{\\sum_{j = t-w+1}^{t} \\left(X(j) - \\overline{X}_{w}(t)\\right)^2}.\n$$\n\nEvaluate monotonic trends in these indicators over the pre-transition window $t \\in \\{w, w+1, \\dots, t_c - L\\}$, where $L$ is a small integer margin to ensure the analysis excludes the near-singular neighborhood of the bifurcation, by using the Kendall rank correlation coefficient (KRCC), commonly referred to as Kendall’s tau. Let $\\tau_{\\mathrm{var}}$ and $p_{\\mathrm{var}}$ denote the Kendall’s tau and its two-sided $p$-value for $\\operatorname{Var}_w(t)$ versus $t$, and let $\\tau_{\\mathrm{ac1}}$ and $p_{\\mathrm{ac1}}$ denote these quantities for $\\rho_1^{(w)}(t)$ versus $t$. Define a detection rule with threshold $\\tau_{\\min}$ and significance level $\\alpha$:\n- Declare that an early warning is detected if both $\\tau_{\\mathrm{var}} \\ge \\tau_{\\min}$ and $p_{\\mathrm{var}}  \\alpha$, and $\\tau_{\\mathrm{ac1}} \\ge \\tau_{\\min}$ and $p_{\\mathrm{ac1}}  \\alpha$.\n\nThe objective is to implement a complete, deterministic program that:\n1. Simulates the time series $P(t)$ according to the Euler–Maruyama scheme for specified parameter sets.\n2. Detrends the series using a moving average with width $w_d$ and computes rolling-window indicators $\\operatorname{Var}_w(t)$ and $\\rho_1^{(w)}(t)$ over the pre-transition window up to $t_c - L$.\n3. Computes Kendall’s tau and its $p$-value for each indicator against time over the set of window end-times.\n4. Applies the detection rule with the given $\\tau_{\\min}$ and $\\alpha$, returning a boolean result per test case.\n\nUse the following test suite (each case includes a fixed random seed for reproducibility):\n- Case $1$ (happy path, clear early warning before regime shift): $N = 2500$, $\\Delta t = 1$, $\\mu_0 = 1.0$, $v = 5\\times 10^{-4}$, $\\sigma = 0.05$, $w_d = 101$, $w = 200$, $L = 25$, $\\tau_{\\min} = 0.6$, $\\alpha = 0.01$, seed $= 42$.\n- Case $2$ (no regime shift, stationary baseline): $N = 2500$, $\\Delta t = 1$, $\\mu_0 = 1.0$, $v = 0$, $\\sigma = 0.05$, $w_d = 101$, $w = 200$, $L = 25$, $\\tau_{\\min} = 0.6$, $\\alpha = 0.01$, seed $= 7$.\n- Case $3$ (high noise, challenging detection): $N = 2500$, $\\Delta t = 1$, $\\mu_0 = 1.0$, $v = 5\\times 10^{-4}$, $\\sigma = 0.4$, $w_d = 101$, $w = 200$, $L = 25$, $\\tau_{\\min} = 0.6$, $\\alpha = 0.01$, seed $= 123$.\n- Case $4$ (small window, edge case for indicator reliability): $N = 2500$, $\\Delta t = 1$, $\\mu_0 = 1.0$, $v = 5\\times 10^{-4}$, $\\sigma = 0.05$, $w_d = 51$, $w = 50$, $L = 25$, $\\tau_{\\min} = 0.6$, $\\alpha = 0.01$, seed $= 99$.\n\nFor cases with $v = 0$, treat $t_c$ as exceeding the simulation horizon (that is, use $t_c = N\\Delta t$) so the pre-transition window is the full series. Ensure that at least $10$ rolling windows exist; otherwise, return a boolean value of $False$ for that case.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[r_1,r_2,r_3,r_4]$, where each $r_i$ is a boolean indicating whether the early warning detection rule is satisfied for the corresponding test case.",
            "solution": "The user has provided a well-defined problem in the domain of complex systems analysis, specifically concerning the detection of early warning signals (EWS) for critical transitions. The problem is scientifically sound, mathematically precise, and computationally verifiable. It is therefore deemed valid.\n\nThe core of the problem lies in analyzing a time series generated from a stochastic model that approaches a bifurcation. The phenomenon of \"critical slowing down\" near such a transition point predicts that the system's recovery from small perturbations becomes increasingly slow. This manifests as an increase in the variance and temporal autocorrelation of the system's fluctuations. The task is to implement a standard pipeline to detect these statistical signatures.\n\nThe solution is structured as follows:\n1.  **System Simulation**: Generate a time series representing the state of the system over time.\n2.  **Signal Processing**: Detrend the time series to isolate the fluctuations around the system's slowly changing equilibrium.\n3.  **Indicator Calculation**: Compute rolling-window variance and lag-$1$ autocorrelation on the detrended fluctuation data.\n4.  **Trend Detection**: Quantify the monotonic trend in these indicators as the system approaches the bifurcation point using Kendall's rank correlation coefficient.\n5.  **Hypothesis Testing**: Apply a decision rule based on the strength and statistical significance of the observed trends to declare whether an early warning is detected.\n\n**Step 1: Simulation using the Euler-Maruyama Scheme**\nThe system's state, $P(t)$, evolves according to the stochastic differential equation (SDE) for a saddle-node bifurcation:\n$$\n\\mathrm{d}P = (\\mu(t) - P^2)\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_t\n$$\nThe control parameter $\\mu(t) = \\mu_0 - v t$ slowly decreases, driving the system towards a critical transition at $\\mu(t_c) = 0$, where the stable equilibrium at $P = \\sqrt{\\mu}$ and the unstable one at $P = -\\sqrt{\\mu}$ merge and annihilate.\n\nWe simulate this continuous-time process using the discrete-time Euler-Maruyama approximation with a time step $\\Delta t$:\n$$\nP_{i+1} = P_i + f(P_i, t_i)\\Delta t + g(P_i, t_i)\\sqrt{\\Delta t}\\,\\xi_i\n$$\nHere, $t_i = i \\Delta t$, $P_i \\approx P(t_i)$, the drift term is $f(P_i, t_i) = \\mu(t_i) - P_i^2 = (\\mu_0 - v t_i) - P_i^2$, the diffusion term is $g(P_i, t_i) = \\sigma$, and $\\xi_i$ are independent random variables drawn from a standard normal distribution $\\mathcal{N}(0,1)$. The simulation is initialized at the stable equilibrium at $t=0$, which is $P_0 = \\sqrt{\\mu(0)} = \\sqrt{\\mu_0}$.\n\n**Step 2: Detrending**\nAs the control parameter $\\mu(t)$ drifts, the stable equilibrium state of the system also drifts. The resulting time series $P(t)$ thus contains both this slow trend and the fluctuations around it. To analyze the properties of the fluctuations, the trend must first be estimated and removed. The problem specifies a centered moving average of width $w_d$ for this purpose:\n$$\n\\widehat{m}(i) = \\frac{1}{w_d} \\sum_{j = i - \\lfloor w_d/2 \\rfloor}^{i + \\lfloor w_d/2 \\rfloor} P_j\n$$\nThis provides a local estimate of the mean. Boundary conditions, for indices $i$ where the window extends beyond the data range, are handled by extending the time series with the nearest boundary value. The detrended series is then:\n$$\nX_i = P_i - \\widehat{m}(i)\n$$\n\n**Step 3: Rolling-Window Indicators**\nThe theory of critical slowing down predicts that as $\\mu(t) \\to 0^+$, the variance and lag-$1$ autocorrelation of the fluctuations $X_t$ will increase. To track these changes, we compute their sample estimates over rolling windows of a fixed width $w$. For a window ending at time index $t$, covering data from $X_{t-w+1}$ to $X_t$:\n\n- **Sample Variance (with Bessel's correction)**: The variance measures the magnitude of fluctuations. Bessel's correction ($1/(w-1)$ instead of $1/w$) provides an unbiased estimate.\n$$\n\\operatorname{Var}_w(t) = \\frac{1}{w - 1}\\sum_{j = t-w+1}^{t} \\left(X_j - \\overline{X}_{w}(t)\\right)^2\n$$\n\n- **Lag-$1$ Autocorrelation**: This measures the \"memory\" or temporal correlation in the fluctuations. Increased memory is a direct consequence of critical slowing down.\n$$\n\\rho_1^{(w)}(t) = \\frac{\\sum_{j = t-w+1}^{t-1} \\left(X_j - \\overline{X}_{w}(t)\\right)\\left(X_{j+1} - \\overline{X}_{w}(t)\\right)}{\\sum_{j = t-w+1}^{t} \\left(X_j - \\overline{X}_{w}(t)\\right)^2}\n$$\nThese indicators are computed for each time step $t$ within the pre-transition analysis window, creating two new time series: $\\operatorname{Var}_w(t)$ and $\\rho_1^{(w)}(t)$.\n\n**Step 4: Trend Analysis with Kendall's Tau**\nTo confirm the presence of EWS, we must detect a statistically significant increasing trend in both indicators as the system approaches the bifurcation. The problem specifies using the Kendall rank correlation coefficient, $\\tau$, a non-parametric measure of the monotonic relationship between two variables. We compute $\\tau$ for each indicator series against time over the pre-transition analysis window, defined by time indices $t \\in \\{w, w+1, \\dots, t_{c,idx} - L\\}$, where $t_{c,idx} = \\lfloor (\\mu_0/v)/\\Delta t \\rfloor$ is the index of the critical point and $L$ provides a safety margin. This yields two correlation coefficients, $\\tau_{\\mathrm{var}}$ and $\\tau_{\\mathrm{ac1}}$, and their corresponding two-sided $p$-values, $p_{\\mathrm{var}}$ and $p_{\\mathrm{ac1}}$. The $p$-value quantifies the probability of observing a trend at least as strong as the one measured, under the null hypothesis of no trend.\n\n**Step 5: The Detection Rule**\nAn early warning is formally detected if and only if both indicators show a positive trend that is both strong enough and statistically significant. The specific conditions are:\n1.  $\\tau_{\\mathrm{var}} \\ge \\tau_{\\min}$ and $p_{\\mathrm{var}}  \\alpha$\n2.  $\\tau_{\\mathrm{ac1}} \\ge \\tau_{\\min}$ and $p_{\\mathrm{ac1}}  \\alpha$\n\nBoth conditions must be met simultaneously. The thresholds $\\tau_{\\min}$ and $\\alpha$ control the trade-off between sensitivity (detecting true warnings) and specificity (avoiding false alarms). The entire procedure is encapsulated in a deterministic function that, given a set of parameters and a random seed, returns a single boolean value indicating the outcome of this detection rule.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import kendalltau\nfrom scipy.ndimage import uniform_filter1d\n\ndef run_analysis(N, dt, mu0, v, sigma, w_d, w, L, tau_min, alpha, seed):\n    \"\"\"\n    Runs the full simulation and early warning signal analysis for a single test case.\n    \"\"\"\n    # 1. Setup and Simulation\n    np.random.seed(seed)\n    \n    # Time vector and parameter vector\n    t_sim = np.arange(N) * dt\n    mu_t = mu0 - v * t_sim\n\n    # Initialize state vector and set initial condition\n    P = np.zeros(N)\n    if mu0 = 0:\n        P[0] = np.sqrt(mu0)\n    else:\n        # Should not happen with given test cases, but for robustness\n        P[0] = 0.0\n\n    # Generate random increments for SDE\n    xi = np.random.randn(N - 1)\n\n    # Euler-Maruyama simulation loop\n    for i in range(N - 1):\n        drift = mu_t[i] - P[i]**2\n        diffusion = sigma * np.sqrt(dt) * xi[i]\n        P[i+1] = P[i] + dt * drift + diffusion\n\n    # 2. Pre-transition window definition\n    if v  0:\n        tc_val = mu0 / v\n        tc_idx = int(tc_val / dt)\n    else: # v = 0, no bifurcation within simulation time\n        tc_val = N * dt\n        tc_idx = N\n\n    # The analysis will be on indicator values computed for these time indices\n    analysis_indices = np.arange(w, tc_idx - L)\n    \n    if len(analysis_indices)  10:\n        return False\n\n    # 3. Detrending\n    # Use uniform_filter1d for a centered moving average. mode='nearest' handles boundaries.\n    trend = uniform_filter1d(P, size=w_d, mode='nearest')\n    X = P - trend\n\n    # 4. Rolling-window indicators\n    variances = []\n    autocorrs = []\n    \n    for t_idx in analysis_indices:\n        # Window of detrended data X ending at t_idx\n        window_X = X[t_idx - w + 1 : t_idx + 1]\n        \n        # Calculate sample variance with Bessel's correction (ddof=1)\n        var = np.var(window_X, ddof=1)\n        variances.append(var)\n        \n        # Calculate lag-1 autocorrelation\n        mean_X = np.mean(window_X)\n        devs = window_X - mean_X\n        # The sum in the denominator covers the full window (w elements)\n        # The sum in the numerator covers w-1 pairs\n        numerator = np.sum(devs[:-1] * devs[1:])\n        denominator = np.sum(devs**2)\n        \n        if denominator == 0:\n            ac1 = np.nan # Avoid division by zero, will be ignored by kendalltau\n        else:\n            ac1 = numerator / denominator\n        autocorrs.append(ac1)\n\n    # 5. Trend analysis with Kendall's Tau and Detection\n    variances = np.array(variances)\n    autocorrs = np.array(autocorrs)\n    \n    # Filter out any NaNs that might have occurred\n    valid_mask_var = ~np.isnan(variances)\n    valid_mask_ac1 = ~np.isnan(autocorrs)\n\n    if np.sum(valid_mask_var)  2 or np.sum(valid_mask_ac1)  2:\n        # Not enough data for correlation\n        return False\n\n    tau_var, p_var = kendalltau(analysis_indices[valid_mask_var], variances[valid_mask_var])\n    tau_ac1, p_ac1 = kendalltau(analysis_indices[valid_mask_ac1], autocorrs[valid_mask_ac1])\n    \n    # Detection Rule\n    warning_detected = (tau_var = tau_min and p_var  alpha and\n                        tau_ac1 = tau_min and p_ac1  alpha)\n                        \n    return warning_detected\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final result.\n    \"\"\"\n    test_cases = [\n        # Case 1: happy path, clear early warning\n        {'N': 2500, 'dt': 1, 'mu0': 1.0, 'v': 5e-4, 'sigma': 0.05, 'w_d': 101, 'w': 200, 'L': 25, 'tau_min': 0.6, 'alpha': 0.01, 'seed': 42},\n        # Case 2: no regime shift, stationary baseline\n        {'N': 2500, 'dt': 1, 'mu0': 1.0, 'v': 0, 'sigma': 0.05, 'w_d': 101, 'w': 200, 'L': 25, 'tau_min': 0.6, 'alpha': 0.01, 'seed': 7},\n        # Case 3: high noise, challenging detection\n        {'N': 2500, 'dt': 1, 'mu0': 1.0, 'v': 5e-4, 'sigma': 0.4, 'w_d': 101, 'w': 200, 'L': 25, 'tau_min': 0.6, 'alpha': 0.01, 'seed': 123},\n        # Case 4: small window, edge case\n        {'N': 2500, 'dt': 1, 'mu0': 1.0, 'v': 5e-4, 'sigma': 0.05, 'w_d': 51, 'w': 50, 'L': 25, 'tau_min': 0.6, 'alpha': 0.01, 'seed': 99},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_analysis(\n            N=case['N'], dt=case['dt'], mu0=case['mu0'], v=case['v'], \n            sigma=case['sigma'], w_d=case['w_d'], w=case['w'], L=case['L'],\n            tau_min=case['tau_min'], alpha=case['alpha'], seed=case['seed']\n        )\n        results.append(result)\n\n    # Format output as a comma-separated list of lowercase booleans in brackets\n    print(f\"[{','.join(map(lambda b: str(b).lower(), results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "Observing a trend in an indicator is not enough; a scientist must ask whether that trend is statistically significant or merely a product of chance. This final practice addresses that crucial question by having you implement a moving-block bootstrap, a robust method for hypothesis testing in time series with temporal autocorrelation . By generating a null distribution for your test statistic, you will learn how to rigorously assess if an observed increase in variance is a true signal or a statistical fluke. This skill is essential for avoiding false alarms when applying EWS methods to noisy, real-world data.",
            "id": "4121017",
            "problem": "You are given the task of constructing a moving-block bootstrap that preserves short-term autocorrelation in a univariate time series and using it to test whether an observed increase in variance near the end of the series exceeds what would be expected by chance under a stationary null. This problem is motivated by early warning signals of critical transitions in complex adaptive systems, where an increase in variance is often observed as recovery rates slow down near a bifurcation. Your solution must start from the following fundamental base and definitions without invoking shortcut results.\n\nFundamental base and definitions:\n- A univariate time series $\\{x_t\\}_{t=1}^N$ exhibits short-term autocorrelation if $\\operatorname{Cov}(x_t, x_{t+k}) \\neq 0$ for small integer lags $k$. A commonly used model for short-term autocorrelation is the first-order autoregressive process (AR(1)), defined by $x_t = \\phi x_{t-1} + \\sigma_t \\varepsilon_t$, where $|\\phi|  1$, $\\varepsilon_t \\sim \\mathcal{N}(0,1)$ are independent and identically distributed Gaussian innovations, and $\\sigma_t  0$ is a time-dependent noise scale. The variance of a stationary AR(1) with constant $\\sigma_t \\equiv \\sigma$ is $\\operatorname{Var}(x_t) = \\sigma^2/(1-\\phi^2)$.\n- An early warning signal of a critical transition is an increase in variance, which, in the simplest linearized setting near a stable equilibrium, follows from the reduction of the linear recovery rate and the corresponding increase of the stationary variance of an Ornstein–Uhlenbeck process.\n- To test for an increase in variance from the beginning of the series to the end, define the test statistic $T = S^2_{\\text{end}} - S^2_{\\text{start}}$, where $S^2_{\\text{start}}$ is the unbiased sample variance computed over the first $w$ observations and $S^2_{\\text{end}}$ is the unbiased sample variance computed over the last $w$ observations, both using the divisor $(w-1)$.\n- Because short-term autocorrelation invalidates independent and identically distributed resampling, construct a circular moving-block bootstrap with block length $l$ to approximate the sampling distribution of $T$ under the null hypothesis that the series is stationary with short-range dependence. In a circular moving-block bootstrap of length $N$, independently sample starting indices $s_1, \\dots, s_B$ uniformly from $\\{1,\\dots,N\\}$, and for each start $s$, take the consecutive block $(x_s, x_{s+1}, \\dots, x_{s+l-1})$ with indices interpreted modulo $N$. Concatenate enough blocks to obtain a bootstrap replicate of length $N$ and truncate if necessary. This procedure preserves short-term autocorrelation up to the block length $l$ in expectation under suitable mixing conditions.\n\nTesting objective:\n- For a given time series realization and chosen $w$ and $l$, estimate the one-sided bootstrap $p$-value for $H_0$ (no systematic increase in variance) versus $H_A$ (variance in the last window is larger) by\n$$\n\\hat{p} = \\frac{1 + \\sum_{b=1}^{B} \\mathbf{1}\\{T^{(b)} \\ge T_{\\text{obs}}\\}}{B+1},\n$$\nwhere $T_{\\text{obs}}$ is the observed statistic and $T^{(b)}$ are the bootstrap statistics computed from each bootstrap replicate. Reject $H_0$ at level $\\alpha$ if $\\hat{p} \\le \\alpha$.\n\nTime series generation for the test suite:\n- For each test case, generate $\\{x_t\\}$ of length $N$ using the AR(1) recursion $x_t = \\phi x_{t-1} + \\sigma_t \\varepsilon_t$ with $x_0 = 0$, $\\varepsilon_t \\sim \\mathcal{N}(0,1)$ independent, and a specified piecewise schedule for $\\sigma_t$:\n  - No change: $\\sigma_t \\equiv \\sigma$ for all $t$.\n  - Jump change: $\\sigma_t = \\sigma$ for $1 \\le t \\le N-L$, and $\\sigma_t = \\sigma \\cdot f$ for $N-L+1 \\le t \\le N$, where $f  1$ is the final multiplicative factor and $L$ is the length of the high-variance tail.\n  - Ramp change: $\\sigma_t = \\sigma$ for $1 \\le t \\le N-L$, and $\\sigma_t = \\sigma \\cdot \\left(1 + (f-1)\\cdot \\frac{t-(N-L)}{L}\\right)$ for $N-L+1 \\le t \\le N$, increasing linearly from $\\sigma$ to $\\sigma f$ over the last $L$ points.\n\nComputation of the test statistic:\n- Compute $S^2_{\\text{start}}$ using the first $w$ samples $\\{x_1,\\dots,x_w\\}$ and $S^2_{\\text{end}}$ using the last $w$ samples $\\{x_{N-w+1},\\dots,x_N\\}$. Use the unbiased estimator $S^2 = \\frac{1}{w-1}\\sum_{i=1}^{w} (x_i - \\bar{x})^2$.\n\nBootstrap procedure:\n- Implement the circular moving-block bootstrap with block length $l$, number of bootstrap replicates $B$, and compute the one-sided bootstrap $p$-value as defined above. Decide significance by comparing to the specified level $\\alpha$.\n\nYour program must:\n- Implement the above generator, statistic, and bootstrap test.\n- Use the provided test suite, with each case entirely specified, and no external input.\n\nTest suite:\nFor each case, use the given random seed to initialize the generator, then generate the series and perform the test with the specified parameters. All numbers below are exact and must be used as written.\n\n- Case $1$ (null, moderate autocorrelation):\n  - Seed $= 123$\n  - $N = 800$, $\\phi = 0.6$, $\\sigma = 1.0$\n  - Change: none\n  - Window size $w = 200$\n  - Block length $l = 40$\n  - Bootstrap replicates $B = 600$\n  - Significance level $\\alpha = 0.05$\n\n- Case $2$ (alternative, ramped variance increase):\n  - Seed $= 456$\n  - $N = 800$, $\\phi = 0.6$, $\\sigma = 1.0$\n  - Change: ramp with length $L = 300$ and final factor $f = 1.8$\n  - Window size $w = 200$\n  - Block length $l = 40$\n  - Bootstrap replicates $B = 600$\n  - Significance level $\\alpha = 0.05$\n\n- Case $3$ (null, strong autocorrelation):\n  - Seed $= 789$\n  - $N = 800$, $\\phi = 0.9$, $\\sigma = 1.0$\n  - Change: none\n  - Window size $w = 200$\n  - Block length $l = 60$\n  - Bootstrap replicates $B = 700$\n  - Significance level $\\alpha = 0.05$\n\n- Case $4$ (alternative, white noise with jump in variance):\n  - Seed $= 222$\n  - $N = 600$, $\\phi = 0.0$, $\\sigma = 1.0$\n  - Change: jump with length $L = 220$ and final factor $f = 1.7$\n  - Window size $w = 150$\n  - Block length $l = 16$\n  - Bootstrap replicates $B = 700$\n  - Significance level $\\alpha = 0.05$\n\n- Case $5$ (alternative, short series boundary case with jump):\n  - Seed $= 333$\n  - $N = 220$, $\\phi = 0.5$, $\\sigma = 1.0$\n  - Change: jump with length $L = 100$ and final factor $f = 1.6$\n  - Window size $w = 80$\n  - Block length $l = 16$\n  - Bootstrap replicates $B = 800$\n  - Significance level $\\alpha = 0.05$\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of booleans enclosed in square brackets, in the order of the cases above. For example: \"[True,False,True,False,True]\".",
            "solution": "The problem requires the implementation and application of a circular moving-block bootstrap to test for a significant increase in variance in a univariate time series. This procedure is common in the analysis of complex systems, where increasing variance can serve as an early warning signal for an impending critical transition, such as a bifurcation. The solution proceeds by first generating time series data according to specified models, then computing an observed test statistic, and finally estimating the statistical significance of this statistic using the bootstrap method.\n\nFirst, we define the procedure for generating the time series data. The data $\\{x_t\\}_{t=1}^N$ are generated from a first-order autoregressive, or AR($1$), process. This model is defined by the recurrence relation:\n$$\nx_t = \\phi x_{t-1} + \\sigma_t \\varepsilon_t\n$$\nwhere $t$ ranges from $1$ to the series length $N$. The parameter $\\phi$ is the autoregressive coefficient, satisfying $|\\phi|  1$ for stationarity. The term $\\varepsilon_t$ represents a sequence of independent and identically distributed (i.i.d.) random variables drawn from a standard normal distribution, $\\mathcal{N}(0,1)$. The initial condition is set to $x_0 = 0$. The parameter $\\sigma_t  0$ represents the time-dependent standard deviation of the noise term. The problem specifies three distinct schedules for $\\sigma_t$:\n1.  **No change (Null Hypothesis):** $\\sigma_t$ is constant, $\\sigma_t = \\sigma$ for all $t \\in \\{1, \\dots, N\\}$. This represents a stationary process.\n2.  **Jump change (Alternative Hypothesis):** $\\sigma_t$ undergoes an abrupt increase near the end of the series. For a given change length $L$ and factor $f  1$, the schedule is:\n    $$\n    \\sigma_t = \\begin{cases} \\sigma  \\text{if } 1 \\le t \\le N-L \\\\ \\sigma \\cdot f  \\text{if } N-L+1 \\le t \\le N \\end{cases}\n    $$\n3.  **Ramp change (Alternative Hypothesis):** $\\sigma_t$ increases linearly over the final portion of the series. For a given change length $L$ and factor $f  1$, the schedule is:\n    $$\n    \\sigma_t = \\begin{cases} \\sigma  \\text{if } 1 \\le t \\le N-L \\\\ \\sigma \\left(1 + (f-1) \\frac{t-(N-L)}{L}\\right)  \\text{if } N-L+1 \\le t \\le N \\end{cases}\n    $$\nReproducibility of the time series generation is ensured by initializing the pseudo-random number generator with a specific seed for each test case.\n\nSecond, we define the test statistic $T$ to quantify the change in variance between the beginning and the end of the time series. This statistic is the difference between the sample variances computed over two windows of size $w$:\n$$\nT = S^2_{\\text{end}} - S^2_{\\text{start}}\n$$\n$S^2_{\\text{start}}$ is the unbiased sample variance of the first $w$ data points, $\\{x_1, \\dots, x_w\\}$, and $S^2_{\\text{end}}$ is the unbiased sample variance of the last $w$ data points, $\\{x_{N-w+1}, \\dots, x_N\\}$. The unbiased sample variance for a set of $w$ observations $\\{y_1, \\dots, y_w\\}$ with sample mean $\\bar{y}$ is given by:\n$$\nS^2 = \\frac{1}{w-1} \\sum_{i=1}^{w} (y_i - \\bar{y})^2\n$$\nA large positive value of the observed statistic, $T_{\\text{obs}}$, suggests that the variance has increased.\n\nThird, to determine if $T_{\\text{obs}}$ is statistically significant, we must assess its probability of occurring by chance under the null hypothesis ($H_0$) that the underlying process is stationary (i.e., no systematic trend in variance). Because the data points are autocorrelated, standard tests assuming independence are invalid. We therefore construct the sampling distribution of $T$ under $H_0$ using a circular moving-block bootstrap. This method preserves the dependence structure of the original series up to the chosen block length $l$. The procedure, repeated $B$ times, is as follows:\n1.  For each bootstrap replicate $b \\in \\{1, \\dots, B\\}$, create a new time series $x^{(b)}$ of length $N$.\n2.  The new series is constructed by concatenating $K = \\lceil N/l \\rceil$ blocks of length $l$.\n3.  Each of these $K$ blocks is drawn from the original time series $\\{x_t\\}_{t=1}^N$. A starting index $s$ is chosen uniformly at random from $\\{1, \\dots, N\\}$. The corresponding block consists of the $l$ consecutive observations $(x_s, x_{s+1}, \\dots, x_{s+l-1})$. Indices are interpreted cyclically, so an index $j  N$ is mapped to $j \\pmod N$ (or more precisely, $(j-1) \\pmod N + 1$ with $1$-based indexing).\n4.  The concatenated series of length $K \\cdot l$ is truncated to the original length $N$ to form the bootstrap replicate $x^{(b)}$.\n5.  For each replicate $x^{(b)}$, the test statistic $T^{(b)}$ is computed in the same way as $T_{\\text{obs}}$.\n\nFinally, the one-sided bootstrap $p$-value is estimated. This value represents the probability of observing a test statistic as large as or larger than $T_{\\text{obs}}$, assuming the null hypothesis is true. It is calculated as:\n$$\n\\hat{p} = \\frac{1 + \\sum_{b=1}^{B} \\mathbf{1}\\{T^{(b)} \\ge T_{\\text{obs}}\\}}{B+1}\n$$\nwhere $\\mathbf{1}\\{\\cdot\\}$ is the indicator function, which is $1$ if its argument is true and $0$ otherwise. The additions of $1$ to the numerator and denominator are a standard convention to prevent a $p$-value of $0$ and provide a more stable estimate. The null hypothesis $H_0$ is rejected at a significance level $\\alpha$ if $\\hat{p} \\le \\alpha$. For each test case, we perform this entire procedure and determine whether to reject $H_0$, yielding a boolean result.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef generate_ar1_series(N, phi, sigma_params):\n    \"\"\"\n    Generates a time series from an AR(1) process with a specified sigma_t schedule.\n    x_t = phi * x_{t-1} + sigma_t * epsilon_t\n    \"\"\"\n    change_type = sigma_params.get(\"change\")\n    sigma_base = sigma_params.get(\"sigma\", 1.0)\n    L = sigma_params.get(\"L\")\n    f = sigma_params.get(\"f\")\n\n    # Generate noise schedule sigmas of length N (for t=1..N)\n    sigmas = np.full(N, sigma_base, dtype=float)\n    if change_type == \"jump\":\n        start_idx = N - L\n        sigmas[start_idx:] = sigma_base * f\n    elif change_type == \"ramp\":\n        start_idx = N - L\n        ramp_values = np.linspace(1.0, f, L)\n        sigmas[start_idx:] = sigma_base * ramp_values\n\n    # Generate innovations epsilon_t for t=1..N\n    eps = np.random.randn(N)\n    \n    # Generate the time series x_t for t=1..N using 0-based indexing for arrays\n    x = np.zeros(N)\n    # x[0] corresponds to x_1. Since x_0 = 0, x_1 = sigma_1 * epsilon_1\n    if N  0:\n        x[0] = sigmas[0] * eps[0]\n        for t in range(1, N):\n            x[t] = phi * x[t-1] + sigmas[t] * eps[t]\n            \n    return x\n\ndef calculate_T_statistic(series, w):\n    \"\"\"\n    Computes the test statistic T = Var(end) - Var(start).\n    \"\"\"\n    if len(series)  w:\n        raise ValueError(\"Series length must be at least window size w.\")\n        \n    start_window = series[:w]\n    end_window = series[-w:]\n    \n    # np.var with ddof=1 computes the unbiased sample variance\n    var_start = np.var(start_window, ddof=1)\n    var_end = np.var(end_window, ddof=1)\n    \n    return var_end - var_start\n\ndef run_bootstrap_test(series, w, l, B, alpha):\n    \"\"\"\n    Performs the circular moving-block bootstrap test.\n    \"\"\"\n    N = len(series)\n    \n    # 1. Calculate the observed statistic\n    T_obs = calculate_T_statistic(series, w)\n    \n    # 2. Perform bootstrap resampling\n    num_blocks_needed = int(np.ceil(N / l))\n    count_ge = 0\n    \n    for _ in range(B):\n        # Create one bootstrap replicate\n        start_indices = np.random.randint(0, N, size=num_blocks_needed)\n        \n        replicate_blocks = []\n        for s in start_indices:\n            block_indices = (np.arange(l) + s) % N\n            replicate_blocks.append(series[block_indices])\n            \n        bootstrap_series = np.concatenate(replicate_blocks)[:N]\n        \n        # Calculate statistic for the replicate\n        T_b = calculate_T_statistic(bootstrap_series, w)\n        \n        if T_b = T_obs:\n            count_ge += 1\n            \n    # 3. Calculate p-value\n    p_value = (1.0 + count_ge) / (1.0 + B)\n    \n    # 4. Return decision\n    return p_value = alpha\n\ndef solve():\n    test_cases = [\n        # Case 1: null, moderate autocorrelation\n        {'seed': 123, 'N': 800, 'phi': 0.6, 'sigma_params': {'change': 'none', 'sigma': 1.0},\n         'w': 200, 'l': 40, 'B': 600, 'alpha': 0.05},\n        # Case 2: alternative, ramped variance increase\n        {'seed': 456, 'N': 800, 'phi': 0.6, 'sigma_params': {'change': 'ramp', 'sigma': 1.0, 'L': 300, 'f': 1.8},\n         'w': 200, 'l': 40, 'B': 600, 'alpha': 0.05},\n        # Case 3: null, strong autocorrelation\n        {'seed': 789, 'N': 800, 'phi': 0.9, 'sigma_params': {'change': 'none', 'sigma': 1.0},\n         'w': 200, 'l': 60, 'B': 700, 'alpha': 0.05},\n        # Case 4: alternative, white noise with jump in variance\n        {'seed': 222, 'N': 600, 'phi': 0.0, 'sigma_params': {'change': 'jump', 'sigma': 1.0, 'L': 220, 'f': 1.7},\n         'w': 150, 'l': 16, 'B': 700, 'alpha': 0.05},\n        # Case 5: alternative, short series boundary case with jump\n        {'seed': 333, 'N': 220, 'phi': 0.5, 'sigma_params': {'change': 'jump', 'sigma': 1.0, 'L': 100, 'f': 1.6},\n         'w': 80, 'l': 16, 'B': 800, 'alpha': 0.05},\n    ]\n\n    results = []\n    for case in test_cases:\n        np.random.seed(case['seed'])\n        \n        # Generate the time series\n        time_series = generate_ar1_series(case['N'], case['phi'], case['sigma_params'])\n        \n        # Run the test\n        is_significant = run_bootstrap_test(time_series, case['w'], case['l'], case['B'], case['alpha'])\n        results.append(is_significant)\n\n    # Format output as specified\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}