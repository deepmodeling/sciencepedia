{
    "hands_on_practices": [
        {
            "introduction": "This practice focuses on lacunarity, a measure that quantifies the heterogeneity or \"gappiness\" of a spatial pattern across different scales. Unlike the fractal dimension, which captures how space is filled, lacunarity describes the texture and translational invariance of that filling. By implementing the gliding box algorithm from first principles, you will gain a practical understanding of how to differentiate between deterministic fractals, random patterns, and homogeneous structures, a foundational skill for characterizing spatial complexity .",
            "id": "4141583",
            "problem": "You are tasked with designing and implementing a complete, runnable program that computes lacunarity using a gliding box algorithm and quantifies how lacunarity changes across scales in synthetic binary images representative of fractal textures. The aim is to ground the computation and interpretation in the principles of scale invariance and fractal geometry within Complex Adaptive Systems (CAS). You must adhere to the precise definitions stated below and implement the algorithm from first principles.\n\nBegin from the following fundamental base:\n\n- Consider a binary occupancy field $I \\in \\{0,1\\}^{N \\times N}$, where $I_{x,y} = 1$ denotes occupied and $I_{x,y} = 0$ denotes empty. Such binary fields are commonly used to model spatial patterns in Complex Adaptive Systems (CAS) where heterogeneous clustering and scale-dependent structure emerge from local interactions.\n- For a given box size $r \\in \\mathbb{N}$, define the gliding box mass $M_{i,j}(r)$ as the sum of occupancies within the $r \\times r$ window whose top-left corner is at $(i,j)$:\n$$\nM_{i,j}(r) = \\sum_{u=0}^{r-1}\\sum_{v=0}^{r-1} I_{i+u,j+v},\n$$\nfor all $(i,j)$ such that the $r \\times r$ window lies fully within the image, i.e., $0 \\le i \\le N - r$ and $0 \\le j \\le N - r$.\n- Define the scale-dependent mean and second moment of these masses over all valid $(i,j)$ as:\n$$\n\\mu_r = \\mathbb{E}[M(r)] = \\frac{1}{(N - r + 1)^2} \\sum_{i=0}^{N-r} \\sum_{j=0}^{N-r} M_{i,j}(r),\n$$\n$$\n\\mathbb{E}[M(r)^2] = \\frac{1}{(N - r + 1)^2} \\sum_{i=0}^{N-r} \\sum_{j=0}^{N-r} M_{i,j}(r)^2.\n$$\n- The lacunarity at scale $r$ is defined as:\n$$\n\\Lambda(r) = \\frac{\\mathbb{E}[M(r)^2]}{\\mu_r^2}.\n$$\nThis definition is equivalent to $1 + \\left(\\frac{\\sigma_r}{\\mu_r}\\right)^2$, where $\\sigma_r^2$ is the variance of $M(r)$, and quantifies the degree of translational invariance of mass distribution at scale $r$. For a perfectly homogeneous field, $\\Lambda(r) = 1$ for all $r$. For heterogeneous or clustered fields, $\\Lambda(r) > 1$, typically decreasing with $r$ for many self-similar fractals.\n- To summarize scale dependence, fit a linear model in log-space:\n$$\n\\log \\Lambda(r) \\approx \\alpha \\log r + \\beta,\n$$\nand estimate the slope $\\hat{\\alpha}$ and coefficient of determination $\\hat{R}^2$ using ordinary least squares on the pairs $(\\log r, \\log \\Lambda(r))$ for a set of $r$ values.\n\nYour program must:\n\n1. Construct the specified synthetic binary images:\n   - A Sierpiński carpet fractal image using a $3 \\times 3$ generator with the middle cell empty and all others occupied, iterated for a specified number of levels to produce a self-similar binary pattern of size $3^k \\times 3^k$ for integer $k$. Formally, if $G \\in \\{0,1\\}^{3 \\times 3}$ is given by $G_{1,1}=0$ and $G_{i,j}=1$ for $(i,j) \\neq (1,1)$, then the $k$-fold Kronecker product of $G$ with itself yields a Sierpiński carpet of depth $k$.\n   - A uniform image with all entries equal to $1$.\n   - A site percolation image on a square lattice with independent occupancy where each site is occupied with probability $p$, generated using a fixed random seed for reproducibility.\n2. Implement the gliding box algorithm to compute $M_{i,j}(r)$ for all valid $(i,j)$ and compute $\\Lambda(r)$ exactly as defined above for a set of box sizes $r$.\n3. For each image, compute:\n   - The list of lacunarity values $[\\Lambda(r_1), \\Lambda(r_2), \\dots]$ for all tested box sizes.\n   - The estimated slope $\\hat{\\alpha}$ and coefficient of determination $\\hat{R}^2$ from the linear regression of $\\log \\Lambda(r)$ versus $\\log r$.\n   - A boolean indicating whether lacunarity is nonincreasing across scales, i.e., whether $\\Lambda(r_{i+1}) \\le \\Lambda(r_i)$ for all consecutive tested scales $r_i$ (use exact comparisons on the computed floating-point values).\n\nUse the following test suite of parameter values:\n- Test case 1 (happy path, self-similar fractal): Sierpiński carpet with depth $k=5$ yielding image size $N=3^5=243$. Use box sizes $r \\in \\{1,3,9,27,81\\}$.\n- Test case 2 (boundary condition, homogeneous field): Uniform image of size $N=243$. Use box sizes $r \\in \\{1,3,9,27,81\\}$.\n- Test case 3 (edge case, disordered cluster near criticality): Site percolation with image size $N=256$, occupancy probability $p=0.6$, and random seed fixed at $12345$. Use box sizes $r \\in \\{1,4,16,64\\}$.\n\nYour program must output, for each test case, a list containing:\n- The estimated slope $\\hat{\\alpha}$ as a floating-point number.\n- The monotonicity boolean for nonincreasing lacunarity across the tested scales.\n- The coefficient of determination $\\hat{R}^2$ as a floating-point number.\n- The list of lacunarity values at the specified scales.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The top-level list should contain one entry per test case in the order listed above, with each entry itself being a list in the order described. For example, the final output format should be like:\n$$\n[\\,[\\hat{\\alpha}_1,\\mathrm{mono}_1,\\hat{R}^2_1,[\\Lambda_1(r_1),\\dots]],\\,[\\hat{\\alpha}_2,\\mathrm{mono}_2,\\hat{R}^2_2,[\\Lambda_2(r_1),\\dots]],\\,[\\hat{\\alpha}_3,\\mathrm{mono}_3,\\hat{R}^2_3,[\\Lambda_3(r_1),\\dots]]\\,]\n$$\nNo physical units are involved in this computation, and all angles are irrelevant for this task. All numerical outputs must be returned in the specified single-line format and must not include any additional text.",
            "solution": "The task is to compute the lacunarity, $\\Lambda(r)$, for several synthetic binary images across a range of scales, $r$. Lacunarity quantifies the \"gappiness\" or translational invariance of a spatial pattern. Its variation with scale provides insight into the fractal nature and structural heterogeneity of the pattern. The analysis involves three main stages: synthetic image generation, efficient lacunarity computation, and a scaling analysis via linear regression.\n\nAn efficient algorithm is required for the computation of the gliding box mass, $M_{i,j}(r)$. A naive implementation, which re-computes the sum for each of the $(N-r+1)^2$ boxes of size $r \\times r$, would have a complexity of $O((N-r+1)^2 \\cdot r^2)$ for each scale $r$. This is computationally prohibitive for the given image and box sizes. A standard and computationally superior method utilizes a summed-area table, also known as an integral image. An integral image $S$ is constructed from the input image $I$ such that each element $S_{x,y}$ stores the sum of all pixel values in the rectangle from the origin $(0,0)$ to $(x,y)$:\n$$\nS_{x,y} = \\sum_{u=0}^{x} \\sum_{v=0}^{y} I_{u,v}.\n$$\nThis table can be computed in a single pass over the image, with a total complexity of $O(N^2)$. Once $S$ is computed, the mass $M$ of any rectangular region with top-left corner $(i,j)$ and bottom-right corner $(i+r-1, j+r-1)$ can be calculated in constant time, $O(1)$, using four lookups into a padded version of $S$:\n$$\nM_{i,j}(r) = S_{i+r-1, j+r-1} - S_{i-1, j+r-1} - S_{i+r-1, j-1} + S_{i-1, j-1}.\n$$\nThis reduces the complexity of finding all $(N-r+1)^2$ mass values for a given scale $r$ to $O((N-r+1)^2)$, a significant improvement.\n\nWith the set of all masses $\\{M_{i,j}(r)\\}$ for a given scale $r$, the mean $\\mu_r$ and the second moment $\\mathbb{E}[M(r)^2]$ are computed as defined:\n$$\n\\mu_r = \\frac{1}{(N - r + 1)^2} \\sum_{i,j} M_{i,j}(r),\n$$\n$$\n\\mathbb{E}[M(r)^2] = \\frac{1}{(N - r + 1)^2} \\sum_{i,j} M_{i,j}(r)^2.\n$$\nThe lacunarity $\\Lambda(r)$ is then given by the ratio:\n$$\n\\Lambda(r) = \\frac{\\mathbb{E}[M(r)^2]}{\\mu_r^2}.\n$$\nThis calculation is repeated for each specified box size $r$ to obtain a set of lacunarity values.\n\nThe problem requires analyzing three types of images:\n1.  **Sierpiński Carpet**: A deterministic fractal generated by the iterative Kronecker product of a generator matrix $G \\in \\{0,1\\}^{3 \\times 3}$, where $G_{1,1}=0$ and all other entries are $1$. The $k$-th iteration, $S_k$, is a $3^k \\times 3^k$ matrix given by $S_k = G \\otimes S_{k-1}$, with $S_1 = G$. This produces a canonical self-similar structure.\n2.  **Uniform Image**: A non-fractal, perfectly homogeneous image where $I_{x,y}=1$ for all $(x,y)$. For this image, every box of size $r$ has mass $M=r^2$. Consequently, the variance of masses is zero, and theory predicts $\\Lambda(r)=1$ for all $r$. This serves as a vital baseline and a test for the algorithm's correctness.\n3.  **Site Percolation Image**: A stochastic field where each site is occupied with probability $p$. The chosen probability $p=0.6$ is near the critical threshold for site percolation on a square lattice ($p_c \\approx 0.5927$), at which scale-invariant clusters are known to form.\n\nFinally, a scaling analysis is performed on the computed pairs $(\\log r, \\log \\Lambda(r))$ by fitting the linear model $\\log \\Lambda(r) \\approx \\alpha \\log r + \\beta$ using Ordinary Least Squares (OLS). This yields the estimated scaling exponent, $\\hat{\\alpha}$, and the coefficient of determination, $\\hat{R}^2$, which quantifies the goodness of the power-law fit. For the uniform image where $\\Lambda(r)=1$, $\\log \\Lambda(r)=0$ for all $r$. In this special case, the data points lie perfectly on a horizontal line, so the slope $\\hat{\\alpha}$ is exactly $0$ and the fit is perfect, yielding $\\hat{R}^2=1$. A boolean flag is also determined by checking if the sequence of lacunarity values is nonincreasing for the tested scales, i.e., $\\Lambda(r_{i+1}) \\le \\Lambda(r_i)$ for all $i$.\n\nThe implementation will proceed by first defining functions to generate each image type. A core function will compute the lacunarity values for a given image and set of box sizes using the summed-area table optimization. Another function will perform the linear regression and monotonicity check. The main function will iterate through the specified test cases, orchestrate these computations, and format the results into the required single-line output string.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import linregress\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n\n    def generate_sierpinski(k: int) -> np.ndarray:\n        \"\"\"\n        Generates a Sierpiński carpet of depth k.\n        \"\"\"\n        g = np.ones((3, 3), dtype=np.int8)\n        g[1, 1] = 0\n        \n        image = g.copy()\n        for _ in range(k - 1):\n            image = np.kron(image, g)\n        return image\n\n    def generate_uniform(n: int) -> np.ndarray:\n        \"\"\"\n        Generates a uniform image of size n x n.\n        \"\"\"\n        return np.ones((n, n), dtype=np.int8)\n\n    def generate_percolation(n: int, p: float, seed: int) -> np.ndarray:\n        \"\"\"\n        Generates a site percolation image of size n x n.\n        \"\"\"\n        rng = np.random.default_rng(seed)\n        return (rng.random((n, n)) < p).astype(np.int8)\n\n    def compute_lacunarity(image: np.ndarray, box_sizes: list[int]) -> list[float]:\n        \"\"\"\n        Computes lacunarity for a given image and list of box sizes using a\n        summed-area table (integral image) for efficiency.\n        \"\"\"\n        n = image.shape[0]\n        # Pad with 1 row/col of zeros on top and left for easier calculation\n        integral_image = np.pad(image.cumsum(axis=0).cumsum(axis=1), 1, 'constant')\n        \n        lac_values = []\n        for r in box_sizes:\n            if r > n:\n                continue\n\n            # Efficiently get all r x r box sums using the integral image\n            bottom_right = integral_image[r:n + 1, r:n + 1]\n            bottom_left = integral_image[r:n + 1, 0:n - r + 1]\n            top_right = integral_image[0:n - r + 1, r:n + 1]\n            top_left = integral_image[0:n - r + 1, 0:n - r + 1]\n            \n            masses = bottom_right - bottom_left - top_right + top_left\n            masses = masses.astype(np.float64) # Use float64 for precision\n\n            mu_r = np.mean(masses)\n            \n            if mu_r == 0:\n                # This case happens only for an all-zero image within the gliding boxes\n                # Lacunarity is ill-defined, often set to 1 by convention.\n                lacunarity = 1.0\n            else:\n                e_m2_r = np.mean(np.square(masses))\n                lacunarity = e_m2_r / (mu_r**2)\n            \n            lac_values.append(lacunarity)\n            \n        return lac_values\n\n    def analyze_scaling(scales: list[int], lacunarities: list[float]):\n        \"\"\"\n        Performs linear regression on log-log data and checks for monotonicity.\n        \"\"\"\n        # 1. Scaling analysis\n        log_scales = np.log(scales)\n        log_lacunarities = np.log(lacunarities)\n        \n        # Handle the special case of a uniform field where lacunarity is always 1,\n        # leading to log_lacunarities being all zeros. In this case, R^2 is 1.\n        if np.all(log_lacunarities == 0):\n            alpha = 0.0\n            r_squared = 1.0\n        else:\n            res = linregress(log_scales, log_lacunarities)\n            alpha = res.slope\n            r_squared = res.rvalue**2\n\n        # 2. Monotonicity check (nonincreasing)\n        is_nonincreasing = all(\n            lacunarities[i] <= lacunarities[i-1] for i in range(1, len(lacunarities))\n        )\n\n        return alpha, is_nonincreasing, r_squared\n    \n    test_cases = [\n        {\n            'type': 'sierpinski', 'params': {'k': 5}, \n            'box_sizes': [1, 3, 9, 27, 81]\n        },\n        {\n            'type': 'uniform', 'params': {'n': 243},\n            'box_sizes': [1, 3, 9, 27, 81]\n        },\n        {\n            'type': 'percolation', 'params': {'n': 256, 'p': 0.6, 'seed': 12345},\n            'box_sizes': [1, 4, 16, 64]\n        }\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        image_type = case['type']\n        params = case['params']\n        box_sizes = case['box_sizes']\n        \n        if image_type == 'sierpinski':\n            image = generate_sierpinski(**params)\n        elif image_type == 'uniform':\n            image = generate_uniform(**params)\n        elif image_type == 'percolation':\n            image = generate_percolation(**params)\n\n        lac_vals = compute_lacunarity(image, box_sizes)\n        \n        alpha, mono_bool, r_sq = analyze_scaling(box_sizes, lac_vals)\n        \n        all_results.append([alpha, mono_bool, r_sq, lac_vals])\n\n    # Format the final output string exactly as required\n    formatted_cases = []\n    for res in all_results:\n        alpha, mono, r2, lac_list = res\n        lac_list_str = \"[\" + \",\".join(map(repr, lac_list)) + \"]\"\n        case_str = f\"[{repr(alpha)},{repr(mono)},{repr(r2)},{lac_list_str}]\"\n        formatted_cases.append(case_str)\n\n    final_output_string = f\"[{','.join(formatted_cases)}]\"\n    print(final_output_string)\n\nsolve()\n```"
        },
        {
            "introduction": "We now transition from static spatial patterns to the dynamics of time series, where identifying true scale invariance is a common challenge. This exercise introduces Detrended Fluctuation Analysis (DFA), a robust method for detecting long-range correlations, and pairs it with surrogate data testing to avoid spurious conclusions . Through this practice, you will learn to computationally distinguish between stationary long-memory processes and nonstationary trends, a critical skill for the rigorous analysis of complex system dynamics.",
            "id": "4141518",
            "problem": "You are given a directive to distinguish between true long-range dependence and nonstationary trends in time series by constructing surrogate tests that preserve marginal distributions. The task is to design and implement a program that, for a set of synthetic time series representative of complex adaptive systems modeling, decides whether each series exhibits true scale-invariant long-range dependence or whether apparent scaling arises from nonstationary trends or short-range effects. The decision should be based on first principles of scale invariance and fractal geometry, using surrogate data analysis that preserves the marginal distribution of the observed series.\n\nStart from the following fundamental base:\n\n- A discrete-time stochastic process $\\{x_t\\}_{t=1}^N$ is (weakly) stationary if its mean $\\mathbb{E}[x_t]$ is constant and its autocovariance $\\gamma(k) = \\mathbb{E}[(x_t - \\mathbb{E}[x_t])(x_{t+k} - \\mathbb{E}[x_{t+k}])]$ depends only on the lag $k$ and not on $t$.\n- True long-range dependence is characterized by slowly decaying autocovariances such that $\\sum_{k=1}^{\\infty} \\gamma(k)$ diverges, equivalently by a low-frequency spectral density that behaves as $f(\\lambda) \\sim C \\lvert \\lambda \\rvert^{-\\beta}$ as $\\lambda \\to 0^+$ for some $C > 0$ and $\\beta \\in (0,1)$, and by scale-invariant fluctuation statistics. This regime is often associated with a Hurst exponent $H \\in (0.5,1)$ for stationary long-memory processes such as fractional Gaussian noise.\n- Nonstationary trends (e.g., random walks or deterministic drifts) can mimic apparent scale invariance but violate stationarity. Their fluctuation statistics generally scale faster than stationary long-memory processes at large windows.\n\nTo operationalize detection, use Detrended Fluctuation Analysis (DFA). In DFA, one constructs the integrated profile $y(k) = \\sum_{t=1}^{k} (x_t - \\bar{x})$ for $k = 1,\\dots,N$, partitions $y(k)$ into non-overlapping windows of size $s$, fits and removes a polynomial trend of specified degree within each window, and computes the root-mean-square fluctuation $F(s)$ of the detrended residuals. Scale invariance manifests as a power law $F(s) \\propto s^{\\alpha}$ with slope $\\alpha$ estimated by linear regression of $\\log F(s)$ versus $\\log s$ over a range of scales. For stationary long-memory processes, $\\alpha \\in (0.5,1)$, whereas for nonstationary integrated processes such as random walk (a fractional Brownian motion with $H \\approx 0.5$), $\\alpha \\in (1,2)$ when using linear DFA.\n\nSurrogate testing that preserves marginal distributions must be used to ensure that detected scaling is not an artifact of the one-point distribution. Two surrogates will be considered:\n\n1. A fully shuffled surrogate, which randomly permutes $\\{x_t\\}$, preserves the marginal distribution exactly, and destroys all serial dependence. If apparent scaling arises solely from the marginal distribution, then the shuffled surrogate should exhibit $\\alpha$ near $0.5$.\n2. A block-shuffled surrogate, which partitions $\\{x_t\\}$ into contiguous blocks of fixed size $B$ and randomly permutes the blocks, thereby preserving both marginal distributions and short-range within-block structure while disrupting long-range ordering. This helps probe whether large-scale scaling is due to long-range dependence or large-scale nonstationary trends.\n\nImplement a program that performs the following for each time series in the test suite:\n\n- Compute the Detrended Fluctuation Analysis (DFA) exponent $\\alpha$ using linear detrending and scales $s \\in \\{16,32,64,128,256,512\\}$.\n- Generate $M$ shuffled surrogates and $M$ block-shuffled surrogates that preserve the marginal distribution of the original series (random permutation and random permutation of blocks, respectively), and compute their DFA exponents.\n- Compute a one-sided $p$-value against the shuffled surrogates, defined as the fraction of surrogate exponents greater than or equal to the original exponent. If this $p$-value is less than or equal to a given significance level $\\eta$, declare that the scaling exceeds what is explainable solely by the marginal distribution.\n- Distinguish nonstationary trend versus true long-range dependence by a two-step rule grounded in first principles:\n  - If the original DFA exponent $\\alpha \\ge \\alpha_{\\text{ns}}$ (nonstationarity threshold), classify the series as “nonstationary trend” because such exponents are inconsistent with stationary long-memory.\n  - Otherwise, if the shuffled test is significant and the difference between the original exponent and the mean block-surrogate exponent is at least $\\Delta_{\\text{LRD}}$, classify the series as “true long-range dependence”. If not, classify as “no long-range dependence” (including short-range dependent processes).\n\nUse the following parameter values and test suite:\n\n- Series length $N = 2048$.\n- DFA polynomial order equal to $1$ (linear detrending).\n- DFA scales $s \\in \\{16,32,64,128,256,512\\}$.\n- Number of surrogates $M = 40$ for each surrogate type.\n- Block size $B = 64$.\n- Significance level $\\eta = 0.01$.\n- Nonstationarity threshold $\\alpha_{\\text{ns}} = 1.05$.\n- Long-range dependence threshold $\\Delta_{\\text{LRD}} = 0.15$.\n\nConstruct four synthetic time series representing different regimes:\n\n- Case A (“Stationary long-range dependence”): Fractionally integrated noise (FARIMA$(0,d,0)$) with fractional difference parameter $d = 0.35$, generated by convolving white noise with the fractional binomial weights. This is a stationary process with true long-range dependence.\n- Case B (“Nonstationary trend”): Random walk defined by $x_t = x_{t-1} + \\varepsilon_t$ with $\\varepsilon_t$ standard normal and $x_1 = 0$, yielding a nonstationary process with large-scale trend.\n- Case C (“Short-range dependence”): Autoregressive process of order $1$, $x_t = \\phi x_{t-1} + \\varepsilon_t$ with $\\phi = 0.9$ and $\\varepsilon_t$ standard normal, a stationary process with short-range dependence.\n- Case D (“Heavy-tailed i.i.d.”): Independent and identically distributed Student-$t$ noise with degrees of freedom $\\nu = 3$, standardized to unit variance, with no serial dependence.\n\nYour program must implement the above logic and produce a single line of output containing the classification results for the four test cases as a comma-separated list enclosed in square brackets. Encode the classification using integers:\n- $2$ for “true long-range dependence,”\n- $1$ for “nonstationary trend,”\n- $0$ for “no long-range dependence.”\n\nThe final output must therefore be of the form $[\\text{r}_A,\\text{r}_B,\\text{r}_C,\\text{r}_D]$ where $\\text{r}_\\cdot \\in \\{0,1,2\\}$ are the classifications for the cases A–D. No physical units apply to this problem, and angles or percentages do not appear; all outputs are integer codes. Your program should be self-contained, require no input, and use only the specified runtime environment. It must follow the exact output formatting with no additional text in the output line.",
            "solution": "The problem asks to implement a classification algorithm to distinguish between true long-range dependence, nonstationary trends, and short-range or no dependence in synthetic time series. The solution will be constructed following a systematic, principle-based approach. First, the four specified categories of time series will be generated. Second, the Detrended Fluctuation Analysis (DFA) algorithm, the core tool for quantifying scaling, will be implemented. Third, the surrogate data generation methods—shuffling and block-shuffling—will be prepared. Finally, these components will be integrated into a classification function that executes the decision logic as specified in the problem statement.\n\n### 1. Generation of Synthetic Time Series\n\nThe problem requires four synthetic time series of length $N=2048$, each representing a distinct class of temporal dynamics. A pseudorandom number generator with a fixed seed will be used to ensure reproducibility.\n\n- **Case A: Stationary Long-Range Dependence (FARIMA(0,d,0))**: This process is generated by convolving a sequence of independent and identically distributed (i.i.d.) Gaussian white noise, $\\varepsilon_t$, with a filter whose weights, $\\psi_k$, decay as a power law. The process is defined by $x_t = (1-L)^{-d} \\varepsilon_t$, where $L$ is the backshift operator and $d$ is the fractional differencing parameter. The filter weights are the coefficients of the Taylor expansion of $(1-z)^{-d}$, given by $\\psi_k = \\frac{\\Gamma(k+d)}{\\Gamma(k+1)\\Gamma(d)}$ for $k \\ge 0$. These can be efficiently computed recursively: $\\psi_0=1$ and $\\psi_k = \\psi_{k-1} \\frac{k-1+d}{k}$ for $k \\ge 1$. For this problem, $d=0.35$. The resulting stationary series exhibits true long-range dependence, with an expected DFA exponent $\\alpha \\approx d+0.5 = 0.85$.\n\n- **Case B: Nonstationary Trend (Random Walk)**: This process is generated by the cumulative sum of i.i.d. Gaussian white noise: $x_t = \\sum_{i=1}^{t} \\varepsilon_i$. This is equivalent to an integrated process, $x_t = x_{t-1} + \\varepsilon_t$ with $x_0=0$. A random walk is the canonical example of a nonstationary process whose integrated profile exhibits strong trends. Its DFA exponent is theoretically $\\alpha=1.5$.\n\n- **Case C: Short-Range Dependence (AR(1))**: An autoregressive process of order $1$ is defined by the recurrence relation $x_t = \\phi x_{t-1} + \\varepsilon_t$, with $\\phi=0.9$. Since $|\\phi| < 1$, the process is stationary. Its autocovariance function decays exponentially, $\\gamma(k) \\propto \\phi^{|k|}$, representing short-range memory. Its DFA exponent is expected to be greater than $0.5$ but significantly less than found in long-range dependent processes.\n\n- **Case D: Heavy-Tailed I.I.D. Noise**: This series is composed of i.i.d. random variables drawn from a Student-$t$ distribution with $\\nu=3$ degrees of freedom. This distribution has heavy tails (infinite fourth moment), a property that can sometimes create spurious scaling artifacts. The series is standardized to have unit variance by dividing each variate by $\\sqrt{\\nu/(\\nu-2)} = \\sqrt{3}$. As an i.i.d. series, its theoretical DFA exponent is $\\alpha = 0.5$.\n\n### 2. Detrended Fluctuation Analysis (DFA)\n\nDFA is a method to quantify long-range correlations in a time series. The procedure, for a given series $\\{x_t\\}_{t=1}^N$, is as follows:\n\n1.  **Integration**: First, the mean $\\bar{x}$ is subtracted from the series, and the cumulative sum (or profile) is computed:\n    $$y(k) = \\sum_{t=1}^{k} (x_t - \\bar{x}) \\quad \\text{for } k=1, \\dots, N$$\n2.  **Segmentation**: The profile $y(k)$ is divided into $\\lfloor N/s \\rfloor$ non-overlapping segments of length $s$, where $s$ is the time scale of observation. The scales used are $s \\in \\{16, 32, 64, 128, 256, 512\\}$.\n3.  **Detrending**: Within each segment, a polynomial trend is fitted to the local section of $y(k)$. The problem specifies a polynomial of order $1$ (a straight line). Let $p_{v}(k)$ be the linear fit for segment $v$.\n4.  **Fluctuation Calculation**: The root-mean-square (RMS) fluctuation for a given scale $s$ is calculated by averaging the variance of the residuals over all segments:\n    $$F(s) = \\sqrt{\\frac{1}{\\lfloor N/s \\rfloor \\cdot s} \\sum_{v=1}^{\\lfloor N/s \\rfloor} \\sum_{k=(v-1)s+1}^{vs} [y(k) - p_v(k)]^2}$$\n5.  **Scaling Exponent Estimation**: If the series exhibits scale-invariance, $F(s)$ will follow a power law, $F(s) \\propto s^{\\alpha}$. The scaling exponent $\\alpha$ is estimated as the slope of a linear fit to the points $(\\log(s), \\log(F(s)))$ over the specified range of scales.\n\n### 3. Surrogate Data Testing\n\nSurrogate data are used to test hypotheses about the origin of observed scaling.\n\n-   **Shuffled Surrogates**: A shuffled surrogate is created by randomly permuting the original time series $\\{x_t\\}$. This procedure preserves the exact marginal distribution (the histogram of values) but completely destroys the temporal ordering and thus any serial dependence. If the original series's exponent $\\alpha$ is significantly larger than the distribution of exponents from shuffled surrogates, we can reject the null hypothesis that the scaling is a mere artifact of a non-Gaussian marginal distribution. The significance is quantified by a $p$-value: the fraction of $M=40$ surrogate exponents that are greater than or equal to the original exponent. A $p$-value below the significance level $\\eta=0.01$ indicates significant scaling beyond that explained by the value distribution alone.\n\n-   **Block-Shuffled Surrogates**: A block-shuffled surrogate is created by dividing the series into non-overlapping blocks of a fixed size $B=64$ and then randomly permuting these blocks. This preserves the marginal distribution and the correlation structure *within* each block, but disrupts long-range correlations on scales larger than $B$. If a series has true long-range dependence, its $\\alpha$ exponent should be substantially higher than the mean exponent of its block-shuffled surrogates, as these surrogates have had their long-range ordering shattered.\n\n### 4. Classification Algorithm\n\nThe core of the problem is a decision-making algorithm that combines the outputs of DFA and surrogate tests to classify each time series. For a given time series, the steps are:\n\n1.  Compute the DFA exponent $\\alpha_{\\text{orig}}$ for the original series.\n2.  Generate $M=40$ shuffled surrogates and compute their exponents $\\{\\alpha_{\\text{shuffled},i}\\}$.\n3.  Generate $M=40$ block-shuffled surrogates and compute their exponents $\\{\\alpha_{\\text{block},i}\\}$.\n4.  Calculate the $p$-value against the shuffled surrogates: $p = \\frac{1}{M} \\sum_{i=1}^{M} \\mathbb{I}(\\alpha_{\\text{shuffled},i} \\ge \\alpha_{\\text{orig}})$, where $\\mathbb{I}(\\cdot)$ is the indicator function.\n5.  Apply the following classification rules sequentially:\n    a. If $\\alpha_{\\text{orig}} \\ge \\alpha_{\\text{ns}}$ (where $\\alpha_{\\text{ns}}=1.05$), the rapid growth of fluctuations is characteristic of a nonstationary process. The series is classified as **\"nonstationary trend\" (code $1$)**.\n    b. Else, if $p \\le \\eta$ (where $\\eta=0.01$) AND $(\\alpha_{\\text{orig}} - \\text{mean}(\\{\\alpha_{\\text{block},i}\\})) \\ge \\Delta_{\\text{LRD}}$ (where $\\Delta_{\\text{LRD}}=0.15$), the conditions indicate statistically significant scaling that is not an artifact of the marginal distribution, and this scaling is demonstrably long-range (i.e., it is destroyed by block-shuffling). The series is classified as **\"true long-range dependence\" (code $2$)**.\n    c. Otherwise, the series either lacks significant scaling or its scaling can be attributed to short-range correlations or distributional artifacts. It is classified as **\"no long-range dependence\" (code $0$)**.\n\nThis structured procedure allows for a robust, automated distinction between the different underlying processes based on their fundamental scaling properties.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef solve():\n    \"\"\"\n    Main function to generate, analyze, and classify time series.\n    \"\"\"\n    # Set a random seed for reproducibility of the synthetic time series.\n    np.random.seed(42)\n\n    # --- Problem Parameters ---\n    N = 2048\n    DFA_SCALES = np.array([16, 32, 64, 128, 256, 512], dtype=int)\n    DFA_ORDER = 1\n    M = 40\n    BLOCK_SIZE = 64\n    ETA = 0.01\n    ALPHA_NS = 1.05\n    DELTA_LRD = 0.15\n\n    # --- Time Series Generation Functions ---\n\n    def generate_farima_0d0(d, n_points):\n        \"\"\"Generates a FARIMA(0,d,0) series using fractional binomial weights.\"\"\"\n        if not (0 < d < 0.5):\n            raise ValueError(\"d must be between 0 and 0.5 for stationary LRD.\")\n        \n        # Calculate filter weights recursively for numerical stability\n        weights = np.zeros(n_points)\n        weights[0] = 1.0\n        for k in range(1, n_points):\n            weights[k] = weights[k - 1] * (k - 1 + d) / k\n        \n        noise = np.random.randn(n_points)\n        series = np.convolve(weights, noise)[:n_points]\n        return series\n\n    def generate_ar1(phi, n_points):\n        \"\"\"Generates an AR(1) series.\"\"\"\n        if not (abs(phi) < 1.0):\n            raise ValueError(\"phi must be less than 1 for a stationary AR(1) process.\")\n        \n        noise = np.random.randn(n_points)\n        series = np.zeros(n_points)\n        series[0] = noise[0]\n        for t in range(1, n_points):\n            series[t] = phi * series[t - 1] + noise[t]\n        return series\n\n    # --- Core Analysis Functions ---\n\n    def dfa(x, scales, order=1):\n        \"\"\"\n        Performs Detrended Fluctuation Analysis (DFA).\n        Returns the scaling exponent alpha.\n        \"\"\"\n        y = np.cumsum(x - np.mean(x))\n        fluctuations = []\n\n        for s in scales:\n            num_windows = len(x) // s\n            if num_windows == 0:\n                continue\n\n            # Reshape into non-overlapping windows\n            y_reshaped = y[:num_windows * s].reshape(num_windows, s)\n            \n            # Time axis for each window\n            time_axis = np.arange(s)\n            \n            # Detrend each window\n            residuals_sq = 0\n            for window in y_reshaped:\n                coeffs = np.polyfit(time_axis, window, order)\n                fit = np.polyval(coeffs, time_axis)\n                residuals_sq += np.sum((window - fit) ** 2)\n            \n            rms = np.sqrt(residuals_sq / (num_windows * s))\n            fluctuations.append(rms)\n\n        if not fluctuations:\n            return np.nan\n\n        # Fit log(F(s)) vs log(s) to find alpha\n        valid_scales = scales[:len(fluctuations)]\n        log_scales = np.log(valid_scales)\n        log_fluctuations = np.log(fluctuations)\n        \n        # Filter out NaN/inf values from log_fluctuations\n        finite_mask = np.isfinite(log_fluctuations)\n        if np.sum(finite_mask) < 2:\n            return np.nan\n\n        alpha, _ = np.polyfit(log_scales[finite_mask], log_fluctuations[finite_mask], 1)\n        return alpha\n\n    def classify_series(x, scales, m, b, eta, alpha_ns, delta_lrd):\n        \"\"\"\n        Classifies a time series based on DFA and surrogate analysis.\n        \"\"\"\n        alpha_orig = dfa(x, scales, order=DFA_ORDER)\n\n        # 1. Nonstationarity Check\n        if alpha_orig >= alpha_ns:\n            return 1  # nonstationary trend\n\n        # 2. Surrogate Analysis\n        alpha_shuffled = np.zeros(m)\n        alpha_block = np.zeros(m)\n        \n        # Prepare for block shuffling\n        n_blocks = len(x) // b\n        truncated_len = n_blocks * b\n        x_truncated = x[:truncated_len]\n\n        for i in range(m):\n            # Shuffled surrogate\n            shuffled_series = np.random.permutation(x)\n            alpha_shuffled[i] = dfa(shuffled_series, scales, order=DFA_ORDER)\n            \n            # Block-shuffled surrogate\n            block_indices = np.random.permutation(n_blocks)\n            x_reshaped = x_truncated.reshape(n_blocks, b)\n            block_shuffled_series = x_reshaped[block_indices, :].flatten()\n            alpha_block[i] = dfa(block_shuffled_series, scales, order=DFA_ORDER)\n\n        # p-value against fully shuffled surrogates\n        p_value = np.sum(alpha_shuffled >= alpha_orig) / m\n\n        # Mean exponent of block-shuffled surrogates\n        mean_alpha_block = np.nanmean(alpha_block)\n\n        # 3. LRD vs. Short-Range/No Dependence Check\n        if p_value <= eta and (alpha_orig - mean_alpha_block) >= delta_lrd:\n            return 2  # true long-range dependence\n        else:\n            return 0  # no long-range dependence\n\n    # --- Generate Test Cases ---\n    test_cases = [\n        # Case A: Stationary long-range dependence (FARIMA)\n        generate_farima_0d0(d=0.35, n_points=N),\n        # Case B: Nonstationary trend (Random Walk)\n        np.cumsum(np.random.randn(N)),\n        # Case C: Short-range dependence (AR1)\n        generate_ar1(phi=0.9, n_points=N),\n        # Case D: Heavy-tailed i.i.d. noise (Student-t)\n        np.random.standard_t(df=3, size=N) / np.sqrt(3.0 / (3.0 - 2.0)),\n    ]\n\n    # --- Perform Classification ---\n    results = []\n    for case_data in test_cases:\n        classification = classify_series(case_data, DFA_SCALES, M, BLOCK_SIZE, ETA, ALPHA_NS, DELTA_LRD)\n        results.append(classification)\n\n    # --- Print Final Output ---\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Building on the analysis of monofractal scaling, this final practice explores the richer phenomenon of multifractality, where a full spectrum of scaling exponents is required to characterize a system's dynamics. You will implement Multifractal Detrended Fluctuation Analysis (MF-DFA) alongside a sophisticated surrogate data methodology designed to disentangle two distinct origins of multifractal behavior: long-range temporal correlations and heavy-tailed probability distributions . This advanced exercise equips you with a powerful diagnostic tool for probing the deeper structural complexity of time series data.",
            "id": "4141515",
            "problem": "You are asked to design and implement a principled computational test that separates multifractality originating from temporal correlations from multifractality originating from heavy-tailed marginal distributions in time series data. The task is grounded in complex adaptive systems modeling and the study of scale invariance and fractal geometry.\n\nThe fundamental base for this task is as follows:\n- Scale invariance in a time series is observed when appropriately defined fluctuation functions obey power-law scaling with window size. Consider a real-valued discrete time series $\\{x_t\\}_{t=1}^N$. Define the integrated profile $Y(i) = \\sum_{t=1}^{i} (x_t - \\bar{x})$, where $\\bar{x}$ is the sample mean. For a window (segment) size $s$, divide the profile into non-overlapping segments of length $s$, detrend each segment by fitting a polynomial of order $m$ (typically $m = 1$ for linear detrending), and compute the variance of the residuals in each segment. Let $F^2(\\nu,s)$ denote the residual variance in segment index $\\nu$ at scale $s$. The $q$-order fluctuation function is defined by\n$$\nF_q(s) =\n\\begin{cases}\n\\left[ \\frac{1}{M(s)} \\sum\\limits_{\\nu=1}^{M(s)} \\left( F^2(\\nu,s) \\right)^{\\frac{q}{2}} \\right]^{\\frac{1}{q}}, & q \\neq 0, \\\\\n\\exp\\left( \\frac{1}{2 M(s)} \\sum\\limits_{\\nu=1}^{M(s)} \\log\\left( F^2(\\nu,s) \\right) \\right), & q = 0,\n\\end{cases}\n$$\nwhere $M(s)$ is the number of segments for scale $s$. A time series exhibits scale invariance if $F_q(s) \\propto s^{h(q)}$ over a range of scales for each $q$, where $h(q)$ is the generalized Hurst exponent. Monofractal processes have $h(q)$ independent of $q$, while multifractal processes have $h(q)$ varying with $q$.\n\nThe objective is to explicitly quantify the multifractality and attribute it to either temporal correlations or heavy-tailed marginal distributions. To achieve this without providing shortcuts to the target concept, you must:\n1. Estimate $h(q)$ for a set of moments $q$ over a range of scales $s$, using only the definitions above and ordinary least squares on the log-log relation $\\log F_q(s)$ versus $\\log s$.\n2. Define a multifractality strength measure\n$$\nM(x) = \\max_{q \\in \\mathcal{Q}} h_x(q) - \\min_{q \\in \\mathcal{Q}} h_x(q),\n$$\nwhere $h_x(q)$ is the generalized Hurst exponent estimated for time series $x$ and $\\mathcal{Q}$ is a symmetric set of positive and negative moments (including $q=0$).\n3. Construct two surrogates of $x$:\n   - A shuffled surrogate $x^{(\\mathrm{shuf})}$ obtained by applying a random permutation to $x$, which preserves the marginal distribution and destroys temporal correlations.\n   - A Gaussianized surrogate $x^{(\\mathrm{gauss})}$ obtained by the rank-based quantile transform $x^{(\\mathrm{gauss})}_t = \\Phi^{-1} \\left( \\hat{F}_x(x_t) \\right)$, where $\\hat{F}_x$ is the empirical cumulative distribution function of $x$ and $\\Phi^{-1}$ is the inverse cumulative distribution function of the standard normal distribution. This preserves the temporal ordering (and thus dependencies) while approximately replacing the marginal distribution with a standard normal distribution.\n4. Attribute multifractality to temporal correlations and heavy-tailed marginals via\n$$\nC(x) := M\\left( x^{(\\mathrm{gauss})} \\right), \\quad D(x) := M\\left( x^{(\\mathrm{shuf})} \\right),\n$$\nwhere $C(x)$ estimates correlation-driven multifractality and $D(x)$ estimates distribution-driven multifractality.\n5. Classify the dominant source:\n   - Return integer code $2$ if $C(x)$ exceeds $D(x)$ by at least a margin $\\delta$ and $C(x)$ exceeds a significance threshold $\\epsilon$ (correlation-dominant multifractality).\n   - Return integer code $1$ if $D(x)$ exceeds $C(x)$ by at least $\\delta$ and $D(x)$ exceeds $\\epsilon$ (heavy-tail-dominant multifractality).\n   - Return integer code $3$ if both $C(x)$ and $D(x)$ exceed $\\epsilon$ and $|C(x) - D(x)|  \\delta$ (both sources present with similar strength).\n   - Return integer code $0$ otherwise (no significant multifractality detected).\nYou must select reasonable values for $\\epsilon$ and $\\delta$ justified by the estimation variability and typical magnitudes of $M(x)$ for monofractal and multifractal signals of the specified lengths.\n\nAngle units are not applicable. No physical units are involved. All numerical outputs should be dimensionless floats or integers.\n\nImplement the program with the following test suite. Each case specifies a generator type and parameters for producing a length-$N$ time series. Use the indicated random seeds for reproducibility:\n- Case 1 (happy path, baseline): Independent and identically distributed Gaussian noise with $N = 4096$, seed $123$.\n- Case 2 (distribution-driven multifractality): Independent and identically distributed Student's $t$ noise with degrees of freedom $\\nu = 1.5$ and $N = 4096$, seed $456$.\n- Case 3 (boundary condition: correlated but monofractal): Gaussian $1/f^\\beta$ noise with $\\beta = 1.0$ and $N = 4096$, seed $789$.\n- Case 4 (edge case: both heavy tails and correlations): Lognormal-volatility autoregressive volatility driving Gaussian innovations, defined by $w_t = \\phi w_{t-1} + \\sigma_w \\xi_t$ with $\\xi_t \\sim \\mathcal{N}(0,1)$, $\\phi = 0.9$, $\\sigma_w = 0.5$, and $x_t = \\exp(w_t) \\epsilon_t$ with $\\epsilon_t \\sim \\mathcal{N}(0,1)$, $N = 4096$, seed $321$.\n\nYour program must:\n- Implement the multifractal detrended fluctuation analysis estimator for $h(q)$ as described.\n- Use moments $\\mathcal{Q} = \\{-4, -2, -1, 0, 1, 2, 4\\}$ and scales $s$ spaced logarithmically in the interval $[16, \\lfloor N/4 \\rfloor]$, using integer scales.\n- Compute $M(x)$, $C(x)$, and $D(x)$ as defined, then classify each case according to the rules above with chosen $\\epsilon$ and $\\delta$.\n\nFinal output format:\nYour program should produce a single line of output containing the four integer classification codes for the four test cases, as a comma-separated list enclosed in square brackets (e.g., \"[c1,c2,c3,c4]\").",
            "solution": "The problem requires designing and implementing a computational method to discern and quantify the contributions of temporal correlations and heavy-tailed probability distributions to the multifractal character of a time series. The core of the methodology is Multifractal Detrended Fluctuation Analysis (MF-DFA), which quantifies how the statistical fluctuations of a time series scale with the size of the observation window. A key feature of complex, highly correlated systems is that this scaling behavior may differ for small and large fluctuations, a property known as multifractality.\n\nFirst, we formalize the MF-DFA procedure as specified. Given a time series $\\{x_t\\}_{t=1}^N$, we compute its integrated profile, often called the \"walk\" or \"trajectory,\" as $Y(i) = \\sum_{t=1}^{i} (x_t - \\bar{x})$, where $\\bar{x}$ is the mean of the series. This initial step transforms a stationary, noisy signal into a non-stationary, random walk-like process, which is necessary for detecting long-range correlations.\n\nThe profile $Y(i)$ is then partitioned into $M(s) = \\lfloor N/s \\rfloor$ non-overlapping segments, each of length $s$. Within each segment $\\nu$, we remove the local trend by fitting a polynomial of order $m$ (here, $m=1$ for linear detrending, which is suitable for removing trends in the integrated profile of a stationary process) and calculating the variance of the residuals. This residual variance, denoted $F^2(\\nu,s)$, quantifies the magnitude of fluctuations at scale $s$ in that specific segment.\n\nTo characterize the scaling behavior across fluctuations of different magnitudes, we compute the $q$-order fluctuation function, $F_q(s)$. This is a generalized average of the segment variances, weighted by the moment parameter $q$. For $q \\neq 0$:\n$$F_q(s) = \\left[ \\frac{1}{M(s)} \\sum_{\\nu=1}^{M(s)} \\left( F^2(\\nu,s) \\right)^{\\frac{q}{2}} \\right]^{\\frac{1}{q}}$$\nFor $q=0$, the definition takes a logarithmic form, which corresponds to the geometric mean:\n$$F_q(s) = \\exp\\left( \\frac{1}{2 M(s)} \\sum_{\\nu=1}^{M(s)} \\log\\left( F^2(\\nu,s) \\right) \\right)$$\nPositive values of $q$ amplify the contribution of segments with large variance (large fluctuations), while negative values of $q$ amplify segments with small variance (small fluctuations).\n\nIf the time series exhibits scale invariance, $F_q(s) \\propto s^{h(q)}$. The exponent $h(q)$ is the generalized Hurst exponent. We estimate $h(q)$ for each $q$ by performing an ordinary least-squares linear regression on the log-transformed variables: $\\log F_q(s)$ versus $\\log s$. For a monofractal series, $h(q)$ is constant. For a multifractal series, $h(q)$ is a decreasing function of $q$.\n\nThe strength of multifractality for a time series $x$ is quantified by the measure $M(x)$, defined as the range of the estimated exponents over a symmetric set of moments $\\mathcal{Q}$:\n$$M(x) = \\max_{q \\in \\mathcal{Q}} h_x(q) - \\min_{q \\in \\mathcal{Q}} h_x(q)$$\n\nThe critical step is to attribute the observed multifractality, $M(x)$, to its sources. Two primary sources are known:\n1.  **Temporal Correlations**: Long-range correlations in the series or its volatility can generate multifractality.\n2.  **Heavy-tailed Distributions**: Broad, non-Gaussian probability distributions of the values $x_t$ can also generate multifractality.\n\nTo disentangle these sources, we employ a surrogate data methodology. We generate two surrogate series from the original series $x$:\n- The **shuffled surrogate**, $x^{(\\mathrm{shuf})}$, is created by randomly permuting the elements of $x$. This procedure preserves the exact marginal probability distribution (and thus any effects from heavy tails) while destroying the temporal ordering and hence any long-range correlations.\n- The **Gaussianized surrogate**, $x^{(\\mathrm{gauss})}$, is created via a rank-based inverse transform sampling. Each value $x_t$ is replaced by $x^{(\\mathrm{gauss})}_t = \\Phi^{-1} \\left( \\hat{F}_x(x_t) \\right)$, where $\\hat{F}_x$ is the empirical cumulative distribution function (CDF) of $x$ and $\\Phi^{-1}$ is the inverse CDF of the standard normal distribution. This procedure preserves the temporal rank-ordering of the original series (and thus its correlation structure) while forcing the marginal distribution to be Gaussian, thereby removing any heavy-tail effects.\n\nBy analyzing these surrogates, we can isolate the contribution of each source. The multifractality strength measured from the Gaussianized surrogate, $C(x) = M(x^{(\\mathrm{gauss})})$, quantifies correlation-driven multifractality. The strength from the shuffled surrogate, $D(x) = M(x^{(\\mathrm{shuf})})$, quantifies distribution-driven multifractality.\n\nThe final step is to classify the dominant source of multifractality based on the relative magnitudes of $C(x)$ and $D(x)$. We must establish a significance threshold, $\\epsilon$, and a comparison margin, $\\delta$.\n- For a monofractal series, estimations on finite-length data will yield small, non-zero values of $M(x)$, $C(x)$, and $D(x)$. Based on empirical results for series of length $N=4096$, a baseline \"noise\" level for these measures is around $0.02-0.04$. We select a significance threshold $\\epsilon=0.05$ to ensure that only multifractality stronger than this noise floor is considered significant.\n- The margin $\\delta$ determines if one source is decisively stronger than the other. Choosing $\\delta = \\epsilon = 0.05$ provides a consistent criterion: to be dominant, a source's contribution must not only be significant on its own but also exceed the other's contribution by more than the significance threshold.\n\nThe classification rules are then applied as specified in the problem statement:\n- Code 2 (Correlations dominant): $C(x) > D(x) + \\delta$ and $C(x) > \\epsilon$.\n- Code 1 (Distribution dominant): $D(x) > C(x) + \\delta$ and $D(x) > \\epsilon$.\n- Code 3 (Both sources): $C(x) > \\epsilon$, $D(x) > \\epsilon$, and $|C(x) - D(x)| \\le \\delta$.\n- Code 0 (No significant multifractality): All other cases.\n\nThis principled approach allows for a robust, quantitative classification of multifractal time series, which is implemented in the following program.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats\n\ndef generate_series(N, seed, series_type, params=None):\n    \"\"\"\n    Generates a time series based on the specified type and parameters.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    if series_type == 'iid_gauss':\n        return rng.standard_normal(N)\n    \n    elif series_type == 'iid_t':\n        return rng.standard_t(df=params['nu'], size=N)\n    \n    elif series_type == 'fgn':\n        beta = params['beta']\n        # Generate Gaussian white noise\n        white_noise = rng.standard_normal(N)\n        # FFT of the noise\n        fft_white_noise = np.fft.rfft(white_noise)\n        # Frequencies for rfft\n        fft_freq = np.fft.rfftfreq(N)\n        \n        # Create the 1/f^beta filter in frequency domain\n        # The filter scales the amplitude, so it's f^(-beta/2)\n        filter_scaling = fft_freq**(-beta / 2.0)\n        # Set DC component (f=0) to zero to avoid infinity and a mean offset\n        filter_scaling[0] = 0\n        \n        # Apply the filter\n        fft_fgn = fft_white_noise * filter_scaling\n        \n        # Inverse FFT to get the time series\n        fgn = np.fft.irfft(fft_fgn, n=N)\n        \n        # Normalize to have unit variance for consistency\n        fgn = (fgn - np.mean(fgn)) / np.std(fgn)\n        return fgn\n        \n    elif series_type == 'stoch_vol':\n        phi = params['phi']\n        sigma_w = params['sigma_w']\n        \n        xi = rng.standard_normal(N)\n        eps = rng.standard_normal(N)\n        \n        w = np.zeros(N)\n        # Initialize w[0] from the stationary distribution of the AR(1) process\n        w[0] = (sigma_w / np.sqrt(1 - phi**2)) * xi[0]\n        \n        for t in range(1, N):\n            w[t] = phi * w[t-1] + sigma_w * xi[t]\n        \n        # The final series is x_t = exp(w_t) * epsilon_t\n        x = np.exp(w) * eps\n        return x\n        \n    else:\n        raise ValueError(f\"Unknown series type: {series_type}\")\n\ndef create_surrogates(x, seed):\n    \"\"\"\n    Creates shuffled and Gaussianized surrogates of a time series.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # Shuffled surrogate\n    x_shuf = rng.permutation(x)\n    \n    # Gaussianized surrogate\n    N = len(x)\n    # Get ranks, convert to probabilities in (0, 1) to avoid -inf/inf from ppf\n    ranks = stats.rankdata(x, method='average')\n    p = ranks / (N + 1)\n    # Apply inverse normal CDF (percent point function)\n    x_gauss = stats.norm.ppf(p)\n    \n    return x_shuf, x_gauss\n\ndef mf_dfa(x, q_values, scales, m=1):\n    \"\"\"\n    Performs Multifractal Detrended Fluctuation Analysis (MF-DFA).\n    \"\"\"\n    N = len(x)\n    # 1. Compute integrated profile\n    Y = np.cumsum(x - np.mean(x))\n    \n    h_q_list = []\n    \n    for q in q_values:\n        F_s_list = []\n        valid_scales = []\n        \n        for s in scales:\n            if s  m + 2:  # Not enough points for polyfit\n                continue\n            \n            # 2. Divide profile into non-overlapping segments\n            num_segments = N // s\n            segments_Y = Y[:num_segments * s].reshape((num_segments, s))\n            \n            seg_indices = np.arange(s)\n            F2_nu_list = []\n            \n            for nu in range(num_segments):\n                # 3. Detrend each segment\n                segment = segments_Y[nu, :]\n                try:\n                    p_coeffs = np.polyfit(seg_indices, segment, m)\n                    fit = np.polyval(p_coeffs, seg_indices)\n                    residuals = segment - fit\n                    # 4. Compute variance of residuals\n                    F2_nu = np.mean(residuals**2)\n                    F2_nu_list.append(F2_nu)\n                except np.linalg.LinAlgError:\n                    continue # Skip segment if fit fails\n            \n            F2_nu_array = np.array(F2_nu_list)\n            # Filter out zero-variance segments which cause issues with log or negative powers\n            F2_nu_array = F2_nu_array[F2_nu_array  1e-12] # Use a small epsilon\n            \n            if len(F2_nu_array) == 0:\n                continue\n            \n            # 5. Calculate q-order fluctuation function\n            if q == 0:\n                F_q_s = np.exp(0.5 * np.mean(np.log(F2_nu_array)))\n            else:\n                F_q_s = (np.mean(F2_nu_array**(q / 2.0)))**(1.0 / q)\n\n            if F_q_s  0:\n                F_s_list.append(F_q_s)\n                valid_scales.append(s)\n\n        if len(valid_scales)  2:\n            h_q_list.append(np.nan)\n            continue\n            \n        # 6. Estimate h(q) via log-log regression\n        log_F = np.log(F_s_list)\n        log_s = np.log(valid_scales)\n        \n        try:\n            h_q, _ = np.polyfit(log_s, log_F, 1)\n            h_q_list.append(h_q)\n        except np.linalg.LinAlgError:\n            h_q_list.append(np.nan)\n    \n    return np.array(h_q_list)\n\ndef classify(C, D, epsilon, delta):\n    \"\"\"\n    Classifies the dominant source of multifractality.\n    \"\"\"\n    is_C_sig = C  epsilon\n    is_D_sig = D  epsilon\n    \n    if is_C_sig and not is_D_sig and C  D + delta:\n        return 2 # Correlation-dominant\n    if C  D + delta and is_C_sig: # Simplified rule from problem\n        return 2\n\n    if is_D_sig and not is_C_sig and D  C + delta:\n        return 1 # Distribution-dominant\n    if D  C + delta and is_D_sig:\n        return 1\n\n    if is_C_sig and is_D_sig and abs(C - D) = delta:\n        return 3 # Both sources present\n    \n    return 0 # No significant multifractality\n\ndef solve():\n    # Define parameters for the analysis\n    N = 4096\n    detrend_order = 1\n    q_values = np.array([-4, -2, -1, 0, 1, 2, 4])\n    \n    # Generate logarithmically spaced integer scales\n    min_scale, max_scale = 16, N // 4\n    num_scales = 20\n    scales = np.logspace(np.log10(min_scale), np.log10(max_scale), num_scales)\n    scales = np.unique(scales.astype(int))\n\n    # Define thresholds\n    epsilon = 0.05\n    delta = 0.05\n\n    test_cases = [\n        {'type': 'iid_gauss', 'seed': 123, 'params': None},\n        {'type': 'iid_t', 'seed': 456, 'params': {'nu': 1.5}},\n        {'type': 'fgn', 'seed': 789, 'params': {'beta': 1.0}},\n        {'type': 'stoch_vol', 'seed': 321, 'params': {'phi': 0.9, 'sigma_w': 0.5}},\n    ]\n    \n    results = []\n    \n    for i, case in enumerate(test_cases):\n        # Generate the original time series\n        x = generate_series(N, case['seed'], case['type'], case['params'])\n        \n        # Create surrogates. Use a fixed seed for surrogate creation for reproducibility\n        surrogate_seed = i + 1000\n        x_shuf, x_gauss = create_surrogates(x, surrogate_seed)\n\n        # Analyze Gaussianized surrogate for correlation effects\n        h_gauss = mf_dfa(x_gauss, q_values, scales, detrend_order)\n        C = np.nanmax(h_gauss) - np.nanmin(h_gauss) if np.all(np.isfinite(h_gauss)) and len(h_gauss)0 else 0\n        \n        # Analyze shuffled surrogate for distribution effects\n        h_shuf = mf_dfa(x_shuf, q_values, scales, detrend_order)\n        D = np.nanmax(h_shuf) - np.nanmin(h_shuf) if np.all(np.isfinite(h_shuf)) and len(h_shuf)0 else 0\n        \n        # Classify the result\n        code = classify(C, D, epsilon, delta)\n        results.append(code)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        }
    ]
}