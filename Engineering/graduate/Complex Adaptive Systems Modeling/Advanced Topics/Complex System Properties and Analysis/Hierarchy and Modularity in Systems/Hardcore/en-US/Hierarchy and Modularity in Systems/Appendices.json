{
    "hands_on_practices": [
        {
            "introduction": "Understanding the hierarchical organization of a complex system often begins with quantifying the relationships between its components. Hierarchical clustering provides a powerful, bottom-up algorithm for revealing this structure from a matrix of pairwise distances or dissimilarities. This exercise  offers direct practice with the average-linkage method, a fundamental technique where you will build a hierarchy by iteratively merging the closest clusters, providing a tangible sense of how macro-level organization emerges from micro-level interactions.",
            "id": "4126046",
            "problem": "Consider a complex adaptive system composed of five interacting modules $M_{1}, M_{2}, M_{3}, M_{4}, M_{5}$. Suppose inter-module dissimilarity has been quantified using a scientifically validated metric derived from long-run behavioral divergence, yielding the following symmetric distance matrix $D$ (entries are dimensionless and the diagonal is $0$):\n$$\nD=\\begin{pmatrix}\n0 & 0.18 & 0.55 & 0.60 & 0.40 \\\\\n0.18 & 0 & 0.58 & 0.59 & 0.42 \\\\\n0.55 & 0.58 & 0 & 0.22 & 0.45 \\\\\n0.60 & 0.59 & 0.22 & 0 & 0.47 \\\\\n0.40 & 0.42 & 0.45 & 0.47 & 0\n\\end{pmatrix}.\n$$\nFrom first principles of hierarchical organization in complex adaptive systems, use the core definition of average-linkage hierarchical clustering: the distance between two clusters $U$ and $V$ is the mean of $D(i,j)$ over all pairs $(i,j)$ with $i \\in U$ and $j \\in V$. Begin with each module as a singleton cluster and iteratively merge the two clusters with the smallest inter-cluster distance. Perform exactly three merges and report the dendrogram merge heights, defined as the cluster-to-cluster distance at the moment of each merge. Express the three heights together as a single row matrix and provide exact decimal values. No rounding is required, and the values are dimensionless.",
            "solution": "The task is to perform agglomerative hierarchical clustering with average linkage on the given modules. We start from the fundamental definition: in average-linkage clustering, for any two clusters $U$ and $V$, the inter-cluster distance is\n$$\nd(U,V) = \\frac{1}{|U|\\,|V|} \\sum_{i \\in U} \\sum_{j \\in V} D(i,j),\n$$\nwhere $|U|$ and $|V|$ denote the number of elements in clusters $U$ and $V$, respectively. The dendrogram merge height at each agglomeration step is the value of $d(U,V)$ for the pair of clusters $U$ and $V$ that are merged at that step.\n\nWe begin with five singleton clusters: $\\{M_{1}\\}$, $\\{M_{2}\\}$, $\\{M_{3}\\}$, $\\{M_{4}\\}$, $\\{M_{5}\\}$. For singleton clusters, the inter-cluster distance equals the corresponding entry in the matrix $D$.\n\nStep $1$: Identify the smallest distance among all pairs of singletons. Inspecting $D$, we list the relevant off-diagonal entries:\n- $D(1,2) = 0.18$,\n- $D(1,3) = 0.55$,\n- $D(1,4) = 0.60$,\n- $D(1,5) = 0.40$,\n- $D(2,3) = 0.58$,\n- $D(2,4) = 0.59$,\n- $D(2,5) = 0.42$,\n- $D(3,4) = 0.22$,\n- $D(3,5) = 0.45$,\n- $D(4,5) = 0.47$.\nThe minimum is $D(1,2) = 0.18$. Therefore, we merge $\\{M_{1}\\}$ and $\\{M_{2}\\}$ at height $0.18$. The first dendrogram merge height is $0.18$. We now have clusters $\\{M_{1},M_{2}\\}$, $\\{M_{3}\\}$, $\\{M_{4}\\}$, $\\{M_{5}\\}$.\n\nCompute the distances from $\\{M_{1},M_{2}\\}$ to each singleton via average linkage:\n- $d(\\{M_{1},M_{2}\\},\\{M_{3}\\}) = \\frac{D(1,3) + D(2,3)}{2} = \\frac{0.55 + 0.58}{2} = 0.565$,\n- $d(\\{M_{1},M_{2}\\},\\{M_{4}\\}) = \\frac{D(1,4) + D(2,4)}{2} = \\frac{0.60 + 0.59}{2} = 0.595$,\n- $d(\\{M_{1},M_{2}\\},\\{M_{5}\\}) = \\frac{D(1,5) + D(2,5)}{2} = \\frac{0.40 + 0.42}{2} = 0.41$.\n\nStep $2$: Among all current inter-cluster distances, include remaining singleton-to-singleton values:\n- $D(3,4) = 0.22$,\n- $D(3,5) = 0.45$,\n- $D(4,5) = 0.47$,\nand the computed distances to $\\{M_{1},M_{2}\\}$ listed above. The minimum is $D(3,4) = 0.22$. Merge $\\{M_{3}\\}$ and $\\{M_{4}\\}$ at height $0.22$. The second dendrogram merge height is $0.22$. We now have clusters $\\{M_{1},M_{2}\\}$, $\\{M_{3},M_{4}\\}$, $\\{M_{5}\\}$.\n\nStep $3$: Compute average-linkage distances among the three current clusters:\n- $d(\\{M_{1},M_{2}\\},\\{M_{5}\\}) = \\frac{D(1,5) + D(2,5)}{2} = \\frac{0.40 + 0.42}{2} = 0.41$,\n- $d(\\{M_{3},M_{4}\\},\\{M_{5}\\}) = \\frac{D(3,5) + D(4,5)}{2} = \\frac{0.45 + 0.47}{2} = 0.46$,\n- $d(\\{M_{1},M_{2}\\},\\{M_{3},M_{4}\\}) = \\frac{D(1,3) + D(1,4) + D(2,3) + D(2,4)}{4} = \\frac{0.55 + 0.60 + 0.58 + 0.59}{4} = \\frac{2.32}{4} = 0.58$.\nThe minimum is $0.41$, corresponding to merging $\\{M_{1},M_{2}\\}$ with $\\{M_{5}\\}$. Merge at height $0.41$. The third dendrogram merge height is $0.41$.\n\nCollecting the three merge heights in order of occurrence, we obtain the row matrix of heights:\n$$\n\\begin{pmatrix}\n0.18 & 0.22 & 0.41\n\\end{pmatrix}.\n$$\nNo rounding is required, and all values are dimensionless.",
            "answer": "$$\\boxed{\\begin{pmatrix}0.18 & 0.22 & 0.41\\end{pmatrix}}$$"
        },
        {
            "introduction": "A key test of a modular partition's significance is whether it simplifies the system's dynamics. For systems modeled as Markov chains, the concept of lumpability provides a rigorous criterion for this simplification, determining when a coarse-grained model of transitions between modules is dynamically consistent. This practice  will guide you through the formal matrix-based approach to test for exact lumpability and derive the resulting macro-level dynamics, connecting abstract partitions to tangible predictive models.",
            "id": "4126084",
            "problem": "Consider a discrete-time Markov chain on a micro-level state space of size $6$ with transition matrix $P \\in \\mathbb{R}^{6 \\times 6}$. The system is organized hierarchically into $3$ modules (macro-states), each containing exactly $2$ micro-states: module $1$ contains micro-states $\\{1,2\\}$, module $2$ contains micro-states $\\{3,4\\}$, and module $3$ contains micro-states $\\{5,6\\}$. Let the partition matrix $S \\in \\mathbb{R}^{3 \\times 6}$ encode this grouping by $S_{g,i} = 1$ if micro-state $i$ belongs to module $g$ and $S_{g,i} = 0$ otherwise. Assume the Moore–Penrose pseudoinverse (MPPI) $S^{\\dagger} \\in \\mathbb{R}^{6 \\times 3}$ is used to form a macro-dynamics operator from the micro-dynamics.\n\nThe micro-level transition matrix $P$ is given by\n$$\nP = \n\\begin{pmatrix}\n0.25 & 0.25 & 0.15 & 0.15 & 0.10 & 0.10 \\\\\n0.25 & 0.25 & 0.15 & 0.15 & 0.10 & 0.10 \\\\\n0.10 & 0.10 & 0.25 & 0.25 & 0.15 & 0.15 \\\\\n0.10 & 0.10 & 0.25 & 0.25 & 0.15 & 0.15 \\\\\n0.15 & 0.15 & 0.10 & 0.10 & 0.25 & 0.25 \\\\\n0.15 & 0.15 & 0.10 & 0.10 & 0.25 & 0.25\n\\end{pmatrix}.\n$$\n\nUsing only fundamental definitions of lumpability in Markov chains and the standard construction of macro-dynamics induced by a partition via the Moore–Penrose pseudoinverse, do the following:\n\n- Determine whether the chain is exactly lumpable with respect to the given partition; that is, whether there exists a macro-level transition matrix $P' \\in \\mathbb{R}^{3 \\times 3}$ such that the macro-dynamics intertwine the micro-dynamics through the partition in the sense required by exact lumpability.\n- If it is lumpable, derive the macro-level transition matrix from first principles of aggregation using $S$ and $S^{\\dagger}$, without assuming any shortcut formulas.\n\nReport as your final answer the single entry of the macro-level transition matrix corresponding to the transition from module $1$ to module $3$, namely the $(1,3)$ element of $P'$. Express your final answer as a simplified fraction. No units are required, and no rounding is needed.",
            "solution": "We start from the definition of a discrete-time Markov chain with transition matrix $P \\in \\mathbb{R}^{n \\times n}$ that advances the probability distribution over micro-states by left multiplication. A partition of the micro-state space into $m$ disjoint modules is encoded by a partition matrix $S \\in \\mathbb{R}^{m \\times n}$ whose rows are indicator vectors of modules. For exact lumpability with respect to the partition, it is necessary and sufficient that for any two micro-states $i$ and $j$ in the same module $g$, the aggregated transition probabilities from $i$ and $j$ into any other module $h$ coincide. This ensures the existence of a macro-level transition matrix $P' \\in \\mathbb{R}^{m \\times m}$ that governs the evolution of aggregated module-level probabilities and satisfies an intertwining relation between micro- and macro-dynamics consistent with the partition.\n\nConcretely, let the modules be $G_{1} = \\{1,2\\}$, $G_{2} = \\{3,4\\}$, and $G_{3} = \\{5,6\\}$. The partition matrix $S \\in \\mathbb{R}^{3 \\times 6}$ is\n$$\nS = \n\\begin{pmatrix}\n1 & 1 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 1 & 1\n\\end{pmatrix}.\n$$\nFor such a partition, the Moore–Penrose pseudoinverse $S^{\\dagger} \\in \\mathbb{R}^{6 \\times 3}$ reduces to averaging within modules. Since each module contains exactly $2$ micro-states, the diagonal matrix of module sizes is $D = \\operatorname{diag}(2,2,2)$, and one verifies that\n$$\nS^{\\dagger} = D^{-1} S^{\\top} = \\frac{1}{2} S^{\\top} =\n\\frac{1}{2}\n\\begin{pmatrix}\n1 & 0 & 0 \\\\\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n0 & 0 & 1\n\\end{pmatrix}.\n$$\n\nTo test exact lumpability, we check that for any $i,j \\in G_{g}$, the sums of transition probabilities from $i$ and $j$ into a module $h$ are equal. Define, for $i \\in \\{1,\\dots,6\\}$ and $h \\in \\{1,2,3\\}$,\n$$\n\\pi(i \\to G_{h}) := \\sum_{k \\in G_{h}} P_{i,k}.\n$$\nWe compute these module-summed probabilities for each micro-state using the given $P$:\n\n- For $i \\in G_{1}$ (rows $1$ and $2$ of $P$), we have\n$$\n\\pi(1 \\to G_{1}) = 0.25 + 0.25 = 0.50,\\quad \\pi(1 \\to G_{2}) = 0.15 + 0.15 = 0.30,\\quad \\pi(1 \\to G_{3}) = 0.10 + 0.10 = 0.20,\n$$\nand by symmetry row $2$ is identical:\n$$\n\\pi(2 \\to G_{1}) = 0.50,\\quad \\pi(2 \\to G_{2}) = 0.30,\\quad \\pi(2 \\to G_{3}) = 0.20.\n$$\n\n- For $i \\in G_{2}$ (rows $3$ and $4$ of $P$), we have\n$$\n\\pi(3 \\to G_{1}) = 0.10 + 0.10 = 0.20,\\quad \\pi(3 \\to G_{2}) = 0.25 + 0.25 = 0.50,\\quad \\pi(3 \\to G_{3}) = 0.15 + 0.15 = 0.30,\n$$\nand similarly for row $4$:\n$$\n\\pi(4 \\to G_{1}) = 0.20,\\quad \\pi(4 \\to G_{2}) = 0.50,\\quad \\pi(4 \\to G_{3}) = 0.30.\n$$\n\n- For $i \\in G_{3}$ (rows $5$ and $6$ of $P$), we have\n$$\n\\pi(5 \\to G_{1}) = 0.15 + 0.15 = 0.30,\\quad \\pi(5 \\to G_{2}) = 0.10 + 0.10 = 0.20,\\quad \\pi(5 \\to G_{3}) = 0.25 + 0.25 = 0.50,\n$$\nand similarly for row $6$:\n$$\n\\pi(6 \\to G_{1}) = 0.30,\\quad \\pi(6 \\to G_{2}) = 0.20,\\quad \\pi(6 \\to G_{3}) = 0.50.\n$$\n\nFor each module $g$, the vectors $\\big(\\pi(i \\to G_{1}), \\pi(i \\to G_{2}), \\pi(i \\to G_{3})\\big)$ are identical for all $i \\in G_{g}$. This verifies the necessary and sufficient condition for exact lumpability with respect to the partition defined by $S$. Therefore, a macro-level transition matrix $P' \\in \\mathbb{R}^{3 \\times 3}$ exists that advances module-level probability distributions consistently with the micro-dynamics.\n\nNext, we derive the macro-level transition matrix from first principles of aggregation using $S$ and $S^{\\dagger}$. Aggregation proceeds by first summing micro-level probabilities within modules via left multiplication by $S$, then redistributing uniformly within modules by right multiplication by $S^{\\dagger}$, which accomplishes averaging over module memberships. Explicitly,\n$$\nP' = S P S^{\\dagger}.\n$$\nWe compute $S P$ by summing the appropriate rows of $P$:\n$$\nS P =\n\\begin{pmatrix}\n\\text{row}_{1}(P) + \\text{row}_{2}(P) \\\\\n\\text{row}_{3}(P) + \\text{row}_{4}(P) \\\\\n\\text{row}_{5}(P) + \\text{row}_{6}(P)\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0.50 & 0.50 & 0.30 & 0.30 & 0.20 & 0.20 \\\\\n0.20 & 0.20 & 0.50 & 0.50 & 0.30 & 0.30 \\\\\n0.30 & 0.30 & 0.20 & 0.20 & 0.50 & 0.50\n\\end{pmatrix}.\n$$\nMultiplying by $S^{\\dagger} = \\frac{1}{2} S^{\\top}$ averages the pairs of columns corresponding to each module:\n$$\nP' = S P S^{\\dagger}\n= \\frac{1}{2}\n\\begin{pmatrix}\n0.50 + 0.50 & 0.30 + 0.30 & 0.20 + 0.20 \\\\\n0.20 + 0.20 & 0.50 + 0.50 & 0.30 + 0.30 \\\\\n0.30 + 0.30 & 0.20 + 0.20 & 0.50 + 0.50\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0.50 & 0.30 & 0.20 \\\\\n0.20 & 0.50 & 0.30 \\\\\n0.30 & 0.20 & 0.50\n\\end{pmatrix}.\n$$\nTherefore, the macro-level transition from module $1$ to module $3$ is the $(1,3)$ entry of $P'$, namely $0.20$. As a simplified fraction, this entry is\n$$\n\\frac{1}{5}.\n$$\nThis completes the verification of exact lumpability and the derivation of the macro-level transition via the Moore–Penrose pseudoinverse-based aggregation operator.",
            "answer": "$$\\boxed{\\frac{1}{5}}$$"
        },
        {
            "introduction": "While maximizing modularity is a common goal in network analysis, the optimization landscape is often rugged and complex, a phenomenon known as modularity degeneracy. This means multiple, structurally different partitions can yield nearly identical, high modularity scores, challenging the notion of a single \"correct\" community structure. This computational exercise  delves into this critical issue, tasking you with demonstrating degeneracy on a synthetic graph and using the Variation of Information metric to quantify the differences between alternative high-modularity partitions.",
            "id": "4126097",
            "problem": "Construct a program that, for a family of graphs exhibiting hierarchical and modular structure, explicitly demonstrates the degeneracy of high modularity by producing distinct community partitions with near-identical modularity and quantifies their difference using Variation of Information. The graphs are rings of cliques and the partitions are: one where each clique is its own module and one where adjacent cliques are merged into larger modules. Your solution must rely only on fundamental definitions and well-tested formulas to compute the required quantities and must not assume any heuristic community detection algorithm.\n\nFundamental base and core definitions to use:\n- Consider an undirected, weighted graph represented by an adjacency matrix $A \\in \\mathbb{R}^{n \\times n}$ with $A_{ij} \\ge 0$ and $A_{ii} = 0$. The degree of node $i$ is $k_i = \\sum_{j=1}^{n} A_{ij}$. The total edge weight is $m = \\tfrac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} A_{ij}$.\n- Given a partition (a community label assignment) $g \\in \\{1,\\dots,G\\}^n$ of the $n$ nodes, define the modularity $Q$ of the partition as\n$$\nQ = \\frac{1}{2m} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\left(A_{ij} - \\frac{k_i k_j}{2m}\\right) \\,\\delta(g_i,g_j),\n$$\nwhere $\\delta(g_i,g_j)$ is $1$ if $g_i=g_j$ and $0$ otherwise.\n- Define the Shannon entropy of a partition $X$ with community probabilities $\\{p_c\\}$ as $H(X) = - \\sum_{c} p_c \\ln p_c$. Define the mutual information of partitions $X$ and $Y$ with joint probabilities $\\{p_{cd}\\}$ as $I(X;Y) = \\sum_{c} \\sum_{d} p_{cd} \\ln \\frac{p_{cd}}{p_c p_d}$. The Variation of Information (VI) between partitions $X$ and $Y$ is $VI(X,Y) = H(X) + H(Y) - 2 I(X;Y)$ in natural logarithm units (nats).\n\nGraph family specification:\n- A ring of cliques is parameterized by three integers $(c,s,b)$ with $c$ even. There are $c$ cliques arranged cyclically; each clique is a complete subgraph of $s$ nodes (so the number of internal edges per clique is $e = \\tfrac{s(s-1)}{2}$). Between each pair of adjacent cliques on the ring, exactly $b$ distinct inter-clique edges connect the two cliques. The total number of nodes is $n = c s$. Each inter-clique edge has unit weight, and all internal clique edges have unit weight.\n- You must construct the adjacency matrix $A$ deterministically as follows: for each clique, connect all pairs of distinct nodes within that clique; for each ring adjacency (clique $i$ to clique $(i+1) \\bmod c$), connect node indices $0,1,\\dots,(b-1)$ of clique $i$ to the corresponding indices $0,1,\\dots,(b-1)$ of clique $(i+1) \\bmod c$.\n\nPartitions to compare on the same graph:\n- Partition $\\mathcal{P}_1$: each clique is one community; that is, there are $c$ communities, each of size $s$.\n- Partition $\\mathcal{P}_2$: merge adjacent cliques pairwise; that is, there are $\\tfrac{c}{2}$ communities, each of size $2s$, formed by consecutive pairs $(0,1)$, $(2,3)$, $\\dots$, $(c-2,c-1)$ of cliques.\n\nRequired computations for each parameter set $(c,s,b)$:\n- Compute $Q(\\mathcal{P}_1)$, $Q(\\mathcal{P}_2)$, the absolute difference $\\Delta Q = |Q(\\mathcal{P}_1) - Q(\\mathcal{P}_2)|$, and the Variation of Information $VI(\\mathcal{P}_1,\\mathcal{P}_2)$ in nats.\n- Using a tolerance parameter $\\varepsilon$, determine the boolean degeneracy indicator $D$ defined as $D = (\\Delta Q \\le \\varepsilon)$.\n\nTest suite:\nProvide the following parameter sets and a single tolerance $\\varepsilon$ shared by all cases:\n- Case $1$: $(c,s,b) = (8,3,1)$ with $\\varepsilon = 10^{-2}$.\n- Case $2$: $(c,s,b) = (14,4,1)$ with $\\varepsilon = 10^{-2}$.\n- Case $3$: $(c,s,b) = (20,5,1)$ with $\\varepsilon = 10^{-2}$.\n\nDesign for coverage:\n- Case $1$ demonstrates exact degeneracy of high modularity across distinct partitions for a small clique size.\n- Case $2$ demonstrates exact degeneracy for a larger internal clique size.\n- Case $3$ demonstrates near-degeneracy (nonzero but small $\\Delta Q$) close to the analytic equality condition.\n\nFinal output format:\nYour program should produce a single line of output containing the results for all cases as a comma-separated list enclosed in square brackets, where each case result is itself a list of the form $[Q(\\mathcal{P}_1),Q(\\mathcal{P}_2),\\Delta Q,VI(\\mathcal{P}_1,\\mathcal{P}_2),D]$. There must be no spaces anywhere in the line. An example schematic format is $[[q_{11},q_{12},dq_1,vi_1,d_1],[q_{21},q_{22},dq_2,vi_2,d_2],[q_{31},q_{32},dq_3,vi_3,d_3]]$ with all values computed by your program.",
            "solution": "The problem statement has been critically examined and is determined to be valid. It is scientifically grounded in network science, mathematically well-posed, and all terms and procedures are defined unambiguously. The problem is a non-trivial exercise in implementing and interpreting fundamental metrics of network structure and information theory, and the test cases are well-designed to illustrate the scientific concept of modularity degeneracy.\n\nThe core of this problem is to investigate modularity degeneracy in a specific family of synthetic graphs: rings of cliques. This structure is a canonical model for systems with both strong local clustering (modularity, via cliques) and hierarchical organization (cliques forming a larger cyclic structure). Degeneracy, in this context, refers to the existence of multiple, structurally-distinct community partitions that yield nearly identical, high modularity scores. This phenomenon complicates the search for a single \"optimal\" community structure and highlights the complex landscape of modularity optimization.\n\nWe are asked to compare two natural partitions on this graph.\nPartition $\\mathcal{P}_1$ treats each clique as a distinct community. This is a fine-grained partition that emphasizes the most local structure.\nPartition $\\mathcal{P}_2$ merges adjacent pairs of cliques, representing a coarser-grained view of the network's structure.\n\nThe analysis will proceed in two stages: first, an analytical derivation of the required quantities to build a conceptual understanding and to verify the final numerical results; second, the development of a computational algorithm that constructs the graph and computes the metrics from their fundamental definitions.\n\n**Analytical Formulation**\n\nLet the ring of cliques be parameterized by $(c, s, b)$, where $c$ is the number of cliques, $s$ is the size of each clique, and $b$ is the number of edges connecting adjacent cliques. The number of nodes is $n=cs$.\n\nThe total weight of edges, $2m$, is the sum of weights from internal clique edges and inter-clique edges. Each of the $c$ cliques has $\\binom{s}{2}$ edges of weight $1$. There are $c$ connections between adjacent cliques, each with $b$ edges of weight $1$.\n$$2m = c s(s-1) + 2cb = c(s^2-s+2b)$$\n\nNode degrees ($k_i$) depend on whether a node connects to other cliques. There are $b$ \"bridge\" nodes per clique and $s-b$ \"internal\" nodes. A bridge node connects to $s-1$ nodes in its clique and to two nodes in adjacent cliques, so its degree is $k_b = (s-1)+2 = s+1$. An internal node only connects within its clique, so its degree is $k_{int} = s-1$.\n\nThe modularity $Q$ of a partition is given by $Q = \\sum_{k=1}^{G} \\left[ \\frac{L_k}{m} - \\left(\\frac{K_k}{2m}\\right)^2 \\right]$, where $L_k$ is the sum of edge weights within community $k$ and $K_k$ is the sum of degrees of nodes in community $k$.\n\nFor Partition $\\mathcal{P}_1$ ($c$ communities of size $s$):\nEach community is a single clique. By symmetry, all $c$ communities are equivalent. For one such community:\nThe internal weight is $L_1 = s(s-1)/2$.\nThe sum of degrees is $K_1 = b \\cdot k_b + (s-b) \\cdot k_{int} = b(s+1) + (s-b)(s-1) = s^2-s+2b$.\nThe modularity is:\n$$Q(\\mathcal{P}_1) = c \\left[ \\frac{s(s-1)/2}{m} - \\left(\\frac{s^2-s+2b}{2m}\\right)^2 \\right] = c \\left[ \\frac{s(s-1)}{c(s^2-s+2b)} - \\left(\\frac{s^2-s+2b}{c(s^2-s+2b)}\\right)^2 \\right]$$\n$$Q(\\mathcal{P}_1) = \\frac{s(s-1)}{s^2-s+2b} - \\frac{1}{c}$$\n\nFor Partition $\\mathcal{P}_2$ ($c/2$ communities of size $2s$):\nEach community is a pair of adjacent cliques. By symmetry, all $c/2$ communities are equivalent. For one such community:\nThe internal weight is a sum of weights within the two cliques plus the $b$ connecting edges: $L_2 = 2 \\cdot \\frac{s(s-1)}{2} + b = s(s-1)+b$.\nThe sum of degrees is the sum for two cliques: $K_2 = 2(s^2-s+2b)$.\nThe modularity is:\n$$Q(\\mathcal{P}_2) = \\frac{c}{2} \\left[ \\frac{s(s-1)+b}{m} - \\left(\\frac{2(s^2-s+2b)}{2m}\\right)^2 \\right] = \\frac{c}{2} \\left[ \\frac{2(s(s-1)+b)}{c(s^2-s+2b)} - \\left(\\frac{2}{c}\\right)^2 \\right]$$\n$$Q(\\mathcal{P}_2) = \\frac{s(s-1)+b}{s^2-s+2b} - \\frac{2}{c}$$\n\nThe absolute difference in modularity is:\n$$\\Delta Q = |Q(\\mathcal{P}_1) - Q(\\mathcal{P}_2)| = \\left| \\left(\\frac{s(s-1)}{s^2-s+2b} - \\frac{1}{c}\\right) - \\left(\\frac{s(s-1)+b}{s^2-s+2b} - \\frac{2}{c}\\right) \\right|$$\n$$\\Delta Q = \\left| \\frac{-b}{s^2-s+2b} + \\frac{1}{c} \\right|$$\nDegeneracy ($Q(\\mathcal{P}_1) \\approx Q(\\mathcal{P}_2)$) occurs when $\\Delta Q \\approx 0$, which implies $c \\approx \\frac{s^2-s+2b}{b}$. The test cases are chosen to probe this condition. For $b=1$, this simplifies to $c \\approx s^2-s+2$. For cases 1 and 2, this equality holds exactly, leading to $\\Delta Q = 0$. For case 3, $c=20$ while $s^2-s+2 = 22$, yielding a small, non-zero $\\Delta Q$.\n\nThe Variation of Information, $VI(X,Y) = H(X) + H(Y) - 2I(X,Y)$, quantifies the distance between two partitions.\nFor $\\mathcal{P}_1$, there are $c$ communities of equal size $s$. The probability of a node being in any given community is $p_c = s/n = 1/c$. The entropy is $H(\\mathcal{P}_1) = - \\sum_{i=1}^c \\frac{1}{c} \\ln(\\frac{1}{c}) = \\ln c$.\nFor $\\mathcal{P}_2$, there are $c/2$ communities of equal size $2s$. The probability is $p_d = 2s/n = 2/c$. The entropy is $H(\\mathcal{P}_2) = - \\sum_{j=1}^{c/2} \\frac{2}{c} \\ln(\\frac{2}{c}) = \\ln(c/2)$.\nPartition $\\mathcal{P}_1$ is a refinement of $\\mathcal{P}_2$. This means that knowing a node's community in $\\mathcal{P}_1$ determines its community in $\\mathcal{P}_2$. Consequently, the mutual information $I(\\mathcal{P}_1, \\mathcal{P}_2)$ is equal to the entropy of the coarser partition, $H(\\mathcal{P}_2)$.\n$$I(\\mathcal{P}_1, \\mathcal{P}_2) = H(\\mathcal{P}_2) = \\ln(c/2)$$\nSubstituting these into the $VI$ formula:\n$$VI(\\mathcal{P}_1, \\mathcal{P}_2) = H(\\mathcal{P}_1) + H(\\mathcal{P}_2) - 2H(\\mathcal{P}_2) = H(\\mathcal{P}_1) - H(\\mathcal{P}_2) = \\ln c - \\ln(c/2) = \\ln(c / (c/2)) = \\ln 2$$\nThe Variation of Information between these two partitions is a constant, $\\ln 2 \\approx 0.693147$ nats, irrespective of the graph parameters $(c, s, b)$. This indicates that the two partitions are always distinct in a structurally consistent manner, even when their modularity scores are identical.\n\n**Computational Strategy**\n\nThe program will implement the general formulas provided, validating the analytical results.\n1.  **Graph Construction**: For each $(c,s,b)$, an adjacency matrix $A$ of size $n \\times n$ (where $n=cs$) is constructed.\n    - Intra-clique edges: For each clique $i \\in \\{0, \\dots, c-1\\}$, all pairs of nodes with indices from $is$ to $(i+1)s-1$ are connected.\n    - Inter-clique edges: For each clique $i \\in \\{0, \\dots, c-1\\}$ and for each $k \\in \\{0, \\dots, b-1\\}$, an edge is added between node $is+k$ and node $((i+1)\\pmod c)s+k$.\n2.  **Partition Generation**: Two integer arrays, $g_1$ and $g_2$, of length $n$ are created to represent $\\mathcal{P}_1$ and $\\mathcal{P}_2$.\n    - For $\\mathcal{P}_1$: node $i$ is in community $\\lfloor i/s \\rfloor$.\n    - For $\\mathcal{P}_2$: node $i$ is in community $\\lfloor \\lfloor i/s \\rfloor / 2 \\rfloor$.\n3.  **Modularity Calculation**: A function computes $Q$ for a given partition $g$. It first computes the total edge weight $m$ and the degree vector $k$. Then, it calculates the sum:\n    $$Q = \\frac{1}{2m} \\sum_{i,j} \\left(A_{ij} - \\frac{k_i k_j}{2m}\\right) \\delta(g_i, g_j)$$\n    This is implemented efficiently using matrix operations.\n4.  **Variation of Information Calculation**: A function computes $VI(\\mathcal{P}_1, \\mathcal{P}_2)$.\n    - It first computes $H(\\mathcal{P}_1)$ and $H(\\mathcal{P}_2)$ from the community size distributions.\n    - It then computes the joint probability distribution of the two partitions by building a contingency table, which counts how many nodes are shared between each pair of communities from $\\mathcal{P}_1$ and $\\mathcal{P}_2$.\n    - The mutual information $I(\\mathcal{P}_1, \\mathcal{P}_2)$ is calculated from the joint and marginal probabilities. Care is taken to handle terms where probabilities are zero, as $0\\ln 0=0$.\n    - Finally, $VI$ is assembled from $H(\\mathcal{P}_1)$, $H(\\mathcal{P}_2)$, and $I(\\mathcal{P}_1, \\mathcal{P}_2)$.\n5.  **Result Aggregation**: The computed values—$Q(\\mathcal{P}_1)$, $Q(\\mathcal{P}_2)$, $\\Delta Q = |Q(\\mathcal{P}_1)-Q(\\mathcal{P}_2)|$, $VI(\\mathcal{P}_1, \\mathcal{P}_2)$, and the boolean degeneracy indicator $D = (\\Delta Q \\le \\varepsilon)$—are collected for each test case and formatted into the required output string.\n\nThis computational approach, while more demanding than using the analytical formulas directly, robustly verifies the principles from first definitions and adheres to the problem's implied methodology of starting from the constructed adjacency matrix.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef compute_modularity(A, k, m, partition):\n    \"\"\"\n    Computes the modularity of a given partition on a graph.\n    \n    Args:\n        A (np.ndarray): The adjacency matrix of the graph.\n        k (np.ndarray): The degree vector of the graph nodes.\n        m (float): The total edge weight of the graph.\n        partition (np.ndarray): An array where partition[i] is the community ID of node i.\n        \n    Returns:\n        float: The modularity Q.\n    \"\"\"\n    if m == 0:\n        return 0.0\n    \n    n = A.shape[0]\n    Q = 0.0\n    for i in range(n):\n        for j in range(n):\n            if partition[i] == partition[j]:\n                Q += (A[i, j] - (k[i] * k[j]) / (2 * m))\n    \n    return Q / (2 * m)\n\ndef compute_vi(p1, p2):\n    \"\"\"\n    Computes the Variation of Information between two partitions.\n    \n    Args:\n        p1 (np.ndarray): The first partition vector.\n        p2 (np.ndarray): The second partition vector.\n        \n    Returns:\n        float: The Variation of Information in nats.\n    \"\"\"\n    n = len(p1)\n    if n == 0:\n        return 0.0\n\n    # Entropy H(P1)\n    _, counts1 = np.unique(p1, return_counts=True)\n    probs1 = counts1 / n\n    h1 = -np.sum(probs1 * np.log(probs1))\n\n    # Entropy H(P2)\n    _, counts2 = np.unique(p2, return_counts=True)\n    probs2 = counts2 / n\n    h2 = -np.sum(probs2 * np.log(probs2))\n\n    # Mutual Information I(P1, P2)\n    # Use unique inverse to handle non-contiguous community labels\n    c1_labels, c1_inverse = np.unique(p1, return_inverse=True)\n    c2_labels, c2_inverse = np.unique(p2, return_inverse=True)\n    nc1 = len(c1_labels)\n    nc2 = len(c2_labels)\n\n    contingency = np.zeros((nc1, nc2), dtype=float)\n    np.add.at(contingency, (c1_inverse, c2_inverse), 1)\n    \n    joint_probs = contingency / n\n    \n    # Marginal probabilities can be re-derived from joint_probs\n    p1_marginal = np.sum(joint_probs, axis=1)\n    p2_marginal = np.sum(joint_probs, axis=0)\n\n    # Compute I(P1, P2)\n    I = 0.0\n    outer_prod = np.outer(p1_marginal, p2_marginal)\n    # Mask for non-zero joint probabilities to avoid log(0)\n    nz_mask = joint_probs > 0\n    I = np.sum(joint_probs[nz_mask] * np.log(joint_probs[nz_mask] / outer_prod[nz_mask]))\n\n    # VI(P1, P2) = H(P1) + H(P2) - 2*I(P1, P2)\n    vi = h1 + h2 - 2 * I\n    return vi\n\ndef process_case(params):\n    \"\"\"\n    Processes a single case, computes all required quantities.\n    \n    Args:\n        params (tuple): A tuple (c, s, b, epsilon).\n        \n    Returns:\n        list: A list containing [Q1, Q2, dQ, VI, D].\n    \"\"\"\n    c, s, b, epsilon = params\n    n = c * s\n    \n    # 1. Construct adjacency matrix A\n    A = np.zeros((n, n), dtype=float)\n    \n    # Intra-clique edges\n    for i in range(c):\n        start_node = i * s\n        end_node = (i + 1) * s\n        for u in range(start_node, end_node):\n            for v in range(u + 1, end_node):\n                A[u, v] = 1.0\n                A[v, u] = 1.0\n    \n    # Inter-clique edges\n    for i in range(c):\n        clique1_idx = i\n        clique2_idx = (i + 1) % c\n        for k in range(b):\n            node1 = clique1_idx * s + k\n            node2 = clique2_idx * s + k\n            A[node1, node2] = 1.0\n            A[node2, node1] = 1.0\n\n    # 2. Calculate graph-wide properties\n    k = A.sum(axis=1)\n    m = k.sum() / 2.0\n\n    # 3. Define partitions\n    nodes = np.arange(n)\n    p1 = nodes // s\n    p2 = (nodes // s) // 2\n\n    # 4. Compute modularities\n    q1 = compute_modularity(A, k, m, p1)\n    q2 = compute_modularity(A, k, m, p2)\n    delta_q = abs(q1 - q2)\n    degeneracy_indicator = delta_q = epsilon\n    \n    # 5. Compute Variation of Information\n    vi = compute_vi(p1, p2)\n    \n    return [q1, q2, delta_q, vi, degeneracy_indicator]\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test suite.\n    \"\"\"\n    test_cases = [\n        (8, 3, 1, 1e-2),  # Case 1\n        (14, 4, 1, 1e-2), # Case 2\n        (20, 5, 1, 1e-2)  # Case 3\n    ]\n\n    results = []\n    for case in test_cases:\n        result = process_case(case)\n        results.append(result)\n\n    # Format the output string to strictly match requirements (no spaces)\n    result_strings = []\n    for res in results:\n        # Convert each element to string, bool becomes 'True'/'False'\n        # The list comprehension handles the formatting of each value\n        # to achieve a compact string representation.\n        s_list = [str(v) for v in res]\n        result_strings.append(f\"[{','.join(s_list)}]\")\n    \n    final_output = f\"[{','.join(result_strings)}]\"\n    print(final_output)\n\nsolve()\n\n```"
        }
    ]
}