{
    "hands_on_practices": [
        {
            "introduction": "This first practice goes to the heart of agent decision-making by exploring the trade-off between maximizing rewards and maintaining exploratory behavior. By working through a simple single-state scenario with entropy regularization, you will derive the optimal policy parameterization from first principles . This exercise illuminates how an agent's choices directly reflect the balance between exploitation of known rewards and the intrinsic drive to explore, a cornerstone of advanced reinforcement learning.",
            "id": "4113170",
            "problem": "Consider a single-state Markov Decision Process (MDP) with state $s$, action set $\\mathcal{A} = \\{0,1\\}$, and deterministic rewards $r(s,1) = r_{1}$ and $r(s,0) = r_{0}$, where $r_{1}$ and $r_{0}$ are real numbers. The environment transitions deterministically back to $s$ regardless of the chosen action. The agent employs a stationary stochastic policy $\\pi_{\\theta}(a \\mid s)$ parameterized by a scalar parameter $\\theta \\in \\mathbb{R}$ via a Boltzmann (softmax) parameterization over action logits, with logits defined by $\\ell(1) = \\theta$ and $\\ell(0) = 0$. The temperature is implicitly absorbed into the parameterization. Thus, the policy satisfies\n$$\n\\pi_{\\theta}(1 \\mid s) = \\frac{\\exp(\\theta)}{\\exp(\\theta) + \\exp(0)}, \\quad \\pi_{\\theta}(0 \\mid s) = \\frac{\\exp(0)}{\\exp(\\theta) + \\exp(0)}.\n$$\nThe agent aims to maximize a discounted entropy-regularized objective with discount factor $\\gamma \\in (0,1)$ and entropy coefficient $\\lambda > 0$. Specifically, the agent maximizes the expected discounted sum of per-step immediate rewards plus a Shannon entropy bonus weighted by $\\lambda$, where the per-step Shannon entropy of the policy at state $s$ is given by\n$$\nH(\\pi_{\\theta}(\\cdot \\mid s)) = - \\sum_{a \\in \\mathcal{A}} \\pi_{\\theta}(a \\mid s) \\ln \\big(\\pi_{\\theta}(a \\mid s)\\big).\n$$\nStarting strictly from the foundational definitions of expected discounted return, stationary policies, and Shannon entropy, derive the exact closed-form expression for the unique optimal parameter $\\theta^{\\star}$ that maximizes the objective. Express your final answer as a single analytic expression in terms of $r_{1}$, $r_{0}$, and $\\lambda$. No numerical evaluation is required, and no rounding is necessary. The answer must be a single closed-form expression.",
            "solution": "The user wants me to solve a reinforcement learning problem.\n\nHere's my plan:\n1.  **Validate the problem:** I need to check if the problem is well-defined, scientifically sound, and complete.\n    *   **Extract Givens:** I'll list all the definitions and parameters provided.\n        *   MDP: single state $s$.\n        *   Action set: $\\mathcal{A} = \\{0, 1\\}$.\n        *   Rewards: $r(s, 1) = r_1$, $r(s, 0) = r_0$. Both are real numbers.\n        *   Transitions: Deterministic, always back to state $s$. So $P(s' = s | s, a) = 1$ for all $a \\in \\mathcal{A}$.\n        *   Policy: Stationary stochastic policy $\\pi_{\\theta}(a|s)$.\n        *   Parameterization: Boltzmann (softmax) with logits $\\ell(1) = \\theta$ and $\\ell(0) = 0$.\n        *   Policy probabilities:\n            *   $\\pi_{\\theta}(1|s) = \\frac{\\exp(\\theta)}{\\exp(\\theta) + \\exp(0)} = \\frac{\\exp(\\theta)}{\\exp(\\theta) + 1}$.\n            *   $\\pi_{\\theta}(0|s) = \\frac{\\exp(0)}{\\exp(\\theta) + \\exp(0)} = \\frac{1}{\\exp(\\theta) + 1}$.\n        *   Objective: Maximize discounted entropy-regularized return.\n        *   Discount factor: $\\gamma \\in (0, 1)$.\n        *   Entropy coefficient: $\\lambda  0$.\n        *   Per-step Shannon entropy: $H(\\pi_{\\theta}(\\cdot|s)) = - \\sum_{a \\in \\mathcal{A}} \\pi_{\\theta}(a|s) \\ln(\\pi_{\\theta}(a|s))$.\n        *   Task: Find the optimal parameter $\\theta^{\\star}$ that maximizes the objective. The answer should be a closed-form expression in terms of $r_1$, $r_0$, and $\\lambda$.\n\n    *   **Validate:**\n        *   **Scientifically Grounded:** Yes, this is a standard problem setup in entropy-regularized reinforcement learning. The concepts of MDP, softmax policy, and Shannon entropy are all well-established.\n        *   **Well-Posed:** The problem asks to maximize a function with respect to a single parameter $\\theta$. The objective function seems well-defined. The domain of $\\theta$ is $\\mathbb{R}$. I'll need to check if a unique maximum exists. The objective involves logarithms and exponentials, which are smooth functions. The domain of the entropy term's logarithm is handled since policy probabilities are always positive. It looks well-posed.\n        *   **Objective:** The language is precise and mathematical. No subjective claims.\n        *   **Incomplete/Contradictory:** The setup is self-contained. All necessary components (state, action, reward, transition, policy, objective) are defined.\n        *   **Unrealistic/Infeasible:** This is a theoretical problem, so physical realism isn't the primary concern. The mathematical setup is sound.\n        *   **Ill-Posed/Poorly Structured:** The structure is clear. The goal is to maximize an objective function. I will need to set up this function and then use calculus to find the maximum.\n        *   **Pseudo-Profound/Trivial:** While a single-state MDP is a simplification, adding entropy regularization and a specific policy parameterization makes it a non-trivial calculus problem that tests understanding of core RL concepts. It's not a tautology.\n        *   **Outside Scientific Verifiability:** The solution can be verified through standard mathematical derivation (calculus).\n\n    *   **Verdict:** The problem is **valid**.\n\n2.  **Solve the problem:**\n    *   **Define the Objective Function:** The objective is the expected discounted sum of rewards and entropy bonuses. Since the state is always $s$, the value function at state $s$ for a policy $\\pi_{\\theta}$ is the objective we want to maximize. Let's call it $J(\\theta)$.\n        *   The agent is always in state $s$. At each time step $t=0, 1, 2, ...$, the agent takes an action $a_t \\sim \\pi_{\\theta}(\\cdot|s)$.\n        *   The immediate reward at step $t$ is a random variable $R_{t+1}$ which depends on the action $a_t$. The expectation of the immediate reward is:\n            $E_{\\pi_{\\theta}}[R_{t+1}] = \\sum_{a \\in \\mathcal{A}} \\pi_{\\theta}(a|s) r(s, a) = \\pi_{\\theta}(1|s) r_1 + \\pi_{\\theta}(0|s) r_0$.\n        *   The entropy bonus at each step is also constant since the policy is stationary and there's only one state: $\\lambda H(\\pi_{\\theta}(\\cdot|s))$.\n        *   The total per-step augmented reward is $E[R_{t+1}] + \\lambda H(\\pi_{\\theta}(\\cdot|s))$.\n        *   The objective function is the expected discounted sum of these augmented rewards:\n            $J(\\theta) = E_{\\pi_{\\theta}} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t (R_{t+1} + \\lambda H(\\pi_{\\theta}(\\cdot|s))) \\right]$.\n        *   Since the per-step augmented reward is constant in expectation, we can pull it out of the sum:\n            $J(\\theta) = \\left( E[R_{t+1}] + \\lambda H(\\pi_{\\theta}(\\cdot|s)) \\right) \\sum_{t=0}^{\\infty} \\gamma^t$.\n        *   The geometric series sums to $\\sum_{t=0}^{\\infty} \\gamma^t = \\frac{1}{1-\\gamma}$.\n        *   So, $J(\\theta) = \\frac{1}{1-\\gamma} \\left( \\pi_{\\theta}(1|s) r_1 + \\pi_{\\theta}(0|s) r_0 + \\lambda H(\\pi_{\\theta}(\\cdot|s)) \\right)$.\n        *   To maximize $J(\\theta)$, we can ignore the constant positive factor $\\frac{1}{1-\\gamma}$ and just maximize the term inside the parenthesis. Let's define the simplified objective function $L(\\theta)$:\n            $L(\\theta) = \\pi_{\\theta}(1|s) r_1 + \\pi_{\\theta}(0|s) r_0 + \\lambda H(\\pi_{\\theta}(\\cdot|s))$.\n\n    *   **Substitute Policy and Entropy Expressions:**\n        *   Let $p_1 = \\pi_{\\theta}(1|s) = \\frac{\\exp(\\theta)}{\\exp(\\theta) + 1}$ and $p_0 = \\pi_{\\theta}(0|s) = \\frac{1}{\\exp(\\theta) + 1}$.\n        *   Also note that $p_1$ is the sigmoid function of $\\theta$, often written as $\\sigma(\\theta)$. So $p_0 = 1 - p_1 = 1 - \\sigma(\\theta) = \\sigma(-\\theta)$.\n        *   The entropy term is:\n            $H(\\pi_{\\theta}(\\cdot|s)) = -(p_1 \\ln(p_1) + p_0 \\ln(p_0))$.\n        *   Substitute these into $L(\\theta)$:\n            $L(\\theta) = p_1 r_1 + p_0 r_0 - \\lambda (p_1 \\ln(p_1) + p_0 \\ln(p_0))$.\n        *   Now substitute the expressions for $p_1$ and $p_0$ in terms of $\\theta$:\n            $p_1 = \\frac{\\exp(\\theta)}{\\exp(\\theta)+1}$\n            $p_0 = \\frac{1}{\\exp(\\theta)+1}$\n            $\\ln(p_1) = \\ln\\left(\\frac{\\exp(\\theta)}{\\exp(\\theta) + 1}\\right) = \\theta - \\ln(\\exp(\\theta) + 1)$\n            $\\ln(p_0) = \\ln\\left(\\frac{1}{\\exp(\\theta) + 1}\\right) = - \\ln(\\exp(\\theta) + 1)$\n        *   Let's plug these into the entropy term:\n            $H = -[ p_1(\\theta - \\ln(\\exp(\\theta)+1)) + p_0(-\\ln(\\exp(\\theta)+1)) ]$\n            $H = -[ p_1 \\theta - (p_1+p_0)\\ln(\\exp(\\theta)+1) ]$\n            Since $p_1+p_0 = 1$:\n            $H = -[ p_1 \\theta - \\ln(\\exp(\\theta)+1) ]$\n            $H = \\ln(\\exp(\\theta)+1) - p_1 \\theta$\n        *   Now substitute this entropy expression back into $L(\\theta)$:\n            $L(\\theta) = p_1 r_1 + p_0 r_0 + \\lambda (\\ln(\\exp(\\theta)+1) - p_1 \\theta)$\n        *   Use $p_0 = 1-p_1$:\n            $L(\\theta) = p_1 r_1 + (1-p_1)r_0 + \\lambda (\\ln(\\exp(\\theta)+1) - p_1 \\theta)$\n            $L(\\theta) = p_1(r_1 - r_0) + r_0 + \\lambda \\ln(\\exp(\\theta)+1) - \\lambda p_1 \\theta$\n            $L(\\theta) = p_1 (r_1 - r_0 - \\lambda \\theta) + r_0 + \\lambda \\ln(\\exp(\\theta)+1)$\n        *   Substitute $p_1 = \\frac{\\exp(\\theta)}{\\exp(\\theta)+1}$:\n            $L(\\theta) = \\frac{\\exp(\\theta)}{\\exp(\\theta)+1} (r_1 - r_0 - \\lambda \\theta) + r_0 + \\lambda \\ln(\\exp(\\theta)+1)$\n            This form looks a bit complex to differentiate. Let's try another approach.\n\n    *   **Alternative expression for L(theta):**\n        *   $L(\\theta) = \\frac{\\exp(\\theta)}{\\exp(\\theta)+1} r_1 + \\frac{1}{\\exp(\\theta)+1} r_0 - \\lambda \\left[ \\frac{\\exp(\\theta)}{\\exp(\\theta)+1} \\ln\\left(\\frac{\\exp(\\theta)}{\\exp(\\theta)+1}\\right) + \\frac{1}{\\exp(\\theta)+1} \\ln\\left(\\frac{1}{\\exp(\\theta)+1}\\right) \\right]$\n        *   $L(\\theta) = \\frac{1}{\\exp(\\theta)+1} \\left( r_1\\exp(\\theta) + r_0 \\right) - \\frac{\\lambda}{\\exp(\\theta)+1} \\left[ \\exp(\\theta)(\\theta - \\ln(\\exp(\\theta)+1)) + 1(-\\ln(\\exp(\\theta)+1)) \\right]$\n        *   $L(\\theta) = \\frac{1}{\\exp(\\theta)+1} \\left( r_1\\exp(\\theta) + r_0 \\right) - \\frac{\\lambda}{\\exp(\\theta)+1} \\left[ \\theta\\exp(\\theta) - (\\exp(\\theta)+1)\\ln(\\exp(\\theta)+1) \\right]$\n        *   $L(\\theta) = \\frac{r_1\\exp(\\theta) + r_0 - \\lambda\\theta\\exp(\\theta)}{\\exp(\\theta)+1} + \\lambda\\ln(\\exp(\\theta)+1)$\n        This looks correct.\n\n    *   **Find the Maximum:** To find the optimal $\\theta^{\\star}$, we need to compute the derivative of $L(\\theta)$ with respect to $\\theta$ and set it to zero: $\\frac{dL}{d\\theta} = 0$.\n        *   Let's differentiate $L(\\theta)$ term by term.\n        *   $L(\\theta) = p_1 r_1 + p_0 r_0 - \\lambda (p_1 \\ln(p_1) + p_0 \\ln(p_0))$.\n        *   We need the derivative of $p_1$ and $p_0$ with respect to $\\theta$.\n        *   $p_1 = \\frac{\\exp(\\theta)}{\\exp(\\theta)+1}$.\n            $\\frac{dp_1}{d\\theta} = \\frac{\\exp(\\theta)(\\exp(\\theta)+1) - \\exp(\\theta)\\exp(\\theta)}{(\\exp(\\theta)+1)^2} = \\frac{\\exp(\\theta)}{(\\exp(\\theta)+1)^2} = \\frac{\\exp(\\theta)}{\\exp(\\theta)+1} \\frac{1}{\\exp(\\theta)+1} = p_1 p_0$.\n        *   $p_0 = 1 - p_1$.\n            $\\frac{dp_0}{d\\theta} = - \\frac{dp_1}{d\\theta} = -p_1 p_0$.\n        *   Now differentiate $L(\\theta)$:\n            $\\frac{dL}{d\\theta} = r_1 \\frac{dp_1}{d\\theta} + r_0 \\frac{dp_0}{d\\theta} - \\lambda \\frac{d}{d\\theta} (p_1 \\ln(p_1) + p_0 \\ln(p_0))$\n        *   Focus on the entropy derivative part first:\n            $\\frac{d}{d\\theta}(p_1 \\ln p_1) = \\frac{dp_1}{d\\theta} \\ln p_1 + p_1 \\frac{1}{p_1} \\frac{dp_1}{d\\theta} = \\frac{dp_1}{d\\theta} (\\ln p_1 + 1)$.\n            $\\frac{d}{d\\theta}(p_0 \\ln p_0) = \\frac{dp_0}{d\\theta} \\ln p_0 + p_0 \\frac{1}{p_0} \\frac{dp_0}{d\\theta} = \\frac{dp_0}{d\\theta} (\\ln p_0 + 1)$.\n        *   So, $\\frac{d}{d\\theta} (p_1 \\ln p_1 + p_0 \\ln p_0) = \\frac{dp_1}{d\\theta} (\\ln p_1 + 1) + \\frac{dp_0}{d\\theta} (\\ln p_0 + 1)$.\n        *   Substitute $\\frac{dp_1}{d\\theta} = p_1 p_0$ and $\\frac{dp_0}{d\\theta} = -p_1 p_0$:\n            $= p_1 p_0 (\\ln p_1 + 1) - p_1 p_0 (\\ln p_0 + 1)$\n            $= p_1 p_0 (\\ln p_1 + 1 - \\ln p_0 - 1)$\n            $= p_1 p_0 (\\ln p_1 - \\ln p_0) = p_1 p_0 \\ln(p_1/p_0)$.\n        *   Now, let's find $\\ln(p_1/p_0)$:\n            $p_1/p_0 = \\frac{\\exp(\\theta)/(\\exp(\\theta)+1)}{1/(\\exp(\\theta)+1)} = \\exp(\\theta)$.\n            So, $\\ln(p_1/p_0) = \\ln(\\exp(\\theta)) = \\theta$.\n        *   The derivative of the entropy part is $p_1 p_0 \\theta$.\n        *   Let's return to the full derivative of $L(\\theta)$:\n            $\\frac{dL}{d\\theta} = r_1 (p_1 p_0) + r_0 (-p_1 p_0) - \\lambda (p_1 p_0 \\theta)$\n            $\\frac{dL}{d\\theta} = p_1 p_0 (r_1 - r_0 - \\lambda \\theta)$.\n        *   Set the derivative to zero to find the critical points:\n            $p_1 p_0 (r_1 - r_0 - \\lambda \\theta) = 0$.\n        *   $p_1 = \\frac{\\exp(\\theta)}{\\exp(\\theta)+1}$ is never zero.\n        *   $p_0 = \\frac{1}{\\exp(\\theta)+1}$ is never zero.\n        *   Therefore, for the product to be zero, the third term must be zero:\n            $r_1 - r_0 - \\lambda \\theta = 0$.\n        *   Solving for $\\theta$:\n            $\\lambda \\theta = r_1 - r_0$\n            $\\theta = \\frac{r_1 - r_0}{\\lambda}$.\n\n    *   **Check if it's a maximum:**\n        *   We need to check the sign of the second derivative, $\\frac{d^2L}{d\\theta^2}$.\n        *   $\\frac{dL}{d\\theta} = p_1 p_0 (r_1 - r_0 - \\lambda \\theta)$.\n        *   Let $f(\\theta) = p_1 p_0 = \\frac{\\exp(\\theta)}{(\\exp(\\theta)+1)^2}$ and $g(\\theta) = r_1 - r_0 - \\lambda \\theta$.\n        *   $\\frac{d^2L}{d\\theta^2} = \\frac{df}{d\\theta} g(\\theta) + f(\\theta) \\frac{dg}{d\\theta}$.\n        *   At the critical point $\\theta^{\\star} = \\frac{r_1 - r_0}{\\lambda}$, we have $g(\\theta^{\\star}) = 0$.\n        *   So, $\\frac{d^2L}{d\\theta^2}\\bigg|_{\\theta=\\theta^{\\star}} = f(\\theta^{\\star}) \\frac{dg}{d\\theta}\\bigg|_{\\theta=\\theta^{\\star}}$.\n        *   $f(\\theta) = p_1 p_0 = \\frac{\\exp(\\theta)}{(\\exp(\\theta)+1)^2}$. Since $\\exp(\\theta)  0$, we have $f(\\theta)  0$ for all $\\theta \\in \\mathbb{R}$.\n        *   $\\frac{dg}{d\\theta} = \\frac{d}{d\\theta} (r_1 - r_0 - \\lambda \\theta) = -\\lambda$.\n        *   So, $\\frac{d^2L}{d\\theta^2}\\bigg|_{\\theta=\\theta^{\\star}} = f(\\theta^{\\star}) (-\\lambda) = - \\lambda \\frac{\\exp(\\theta^{\\star})}{(\\exp(\\theta^{\\star})+1)^2}$.\n        *   We are given that $\\lambda  0$. And as argued, the fraction term is always positive.\n        *   Therefore, $\\frac{d^2L}{d\\theta^2}\\bigg|_{\\theta=\\theta^{\\star}}  0$.\n        *   This confirms that the critical point $\\theta^{\\star} = \\frac{r_1 - r_0}{\\lambda}$ is a local maximum. Since this is the only critical point on $\\mathbb{R}$, and the function is smooth, it must be the unique global maximum.\n\n    *   **Final Answer Formulation:**\n        *   The question asks for the exact closed-form expression for the unique optimal parameter $\\theta^{\\star}$.\n        *   My derivation gives $\\theta^{\\star} = \\frac{r_1 - r_0}{\\lambda}$.\n        *   This is an analytical expression in terms of $r_1$, $r_0$, and $\\lambda$.\n        *   It satisfies all the requirements.\n\n    *   **Double-check the derivation and interpretation.**\n        *   The objective is the value function $V^{\\pi_\\theta}(s)$.\n        *   The entropy-regularized Bellman equation for a policy $\\pi$ is:\n            $Q^{\\pi}(s,a) = r(s,a) + \\gamma \\sum_{s'} P(s'|s,a) V^{\\pi}(s')$\n            $V^{\\pi}(s) = \\sum_a \\pi(a|s) Q^{\\pi}(s,a) + \\lambda H(\\pi(\\cdot|s))$\n        *   In our case, $P(s'|s,a) = \\delta_{s,s'}$. So $\\sum_{s'} P(s'|s,a) V^{\\pi}(s') = V^{\\pi}(s)$.\n        *   $Q^{\\pi_\\theta}(s,a) = r(s,a) + \\gamma V^{\\pi_\\theta}(s)$\n        *   $V^{\\pi_\\theta}(s) = \\sum_a \\pi_{\\theta}(a|s) (r(s,a) + \\gamma V^{\\pi_\\theta}(s)) + \\lambda H(\\pi_{\\theta}(\\cdot|s))$\n        *   $V^{\\pi_\\theta}(s) = \\sum_a \\pi_{\\theta}(a|s) r(s,a) + \\gamma V^{\\pi_\\theta}(s) \\sum_a \\pi_{\\theta}(a|s) + \\lambda H(\\pi_{\\theta}(\\cdot|s))$\n        *   Since $\\sum_a \\pi_{\\theta}(a|s) = 1$:\n            $V^{\\pi_\\theta}(s) = \\sum_a \\pi_{\\theta}(a|s) r(s,a) + \\gamma V^{\\pi_\\theta}(s) + \\lambda H(\\pi_{\\theta}(\\cdot|s))$\n        *   $(1-\\gamma) V^{\\pi_\\theta}(s) = \\sum_a \\pi_{\\theta}(a|s) r(s,a) + \\lambda H(\\pi_{\\theta}(\\cdot|s))$\n        *   $V^{\\pi_\\theta}(s) = \\frac{1}{1-\\gamma} \\left( \\sum_a \\pi_{\\theta}(a|s) r(s,a) + \\lambda H(\\pi_{\\theta}(\\cdot|s)) \\right)$.\n        *   This matches my initial formulation of the objective function $J(\\theta)$. Maximizing $V^{\\pi_\\theta}(s)$ is equivalent to maximizing the term in the parenthesis, which I called $L(\\theta)$.\n        *   $L(\\theta) = \\pi_{\\theta}(1|s)r_1 + \\pi_{\\theta}(0|s)r_0 + \\lambda H(\\pi_{\\theta}(\\cdot | s))$.\n        *   Let's check the entropy calculation again.\n            $H = - \\sum_{a} \\pi(a|s) \\ln \\pi(a|s)$.\n            With $\\pi(1|s) = p_1$ and $\\pi(0|s) = p_0$:\n            $H = - p_1 \\ln p_1 - p_0 \\ln p_0$.\n            $\\frac{dH}{d\\theta} = -\\frac{d}{d\\theta}(p_1 \\ln p_1) - \\frac{d}{d\\theta}(p_0 \\ln p_0)$\n            $= - [ \\frac{dp_1}{d\\theta}(\\ln p_1 + 1) + \\frac{dp_0}{d\\theta}(\\ln p_0 + 1) ]$\n            $= - [ (p_1 p_0)(\\ln p_1 + 1) + (-p_1 p_0)(\\ln p_0 + 1) ]$\n            $= - p_1 p_0 (\\ln p_1 - \\ln p_0) = - p_1 p_0 \\ln(p_1/p_0)$.\n            Since $p_1/p_0 = \\exp(\\theta)$, we have $\\ln(p_1/p_0) = \\theta$.\n            So $\\frac{dH}{d\\theta} = - p_1 p_0 \\theta$.\n        *   Now the derivative of $L(\\theta)$:\n            $\\frac{dL}{d\\theta} = \\frac{d}{d\\theta} (p_1 r_1 + p_0 r_0) + \\lambda \\frac{dH}{d\\theta}$\n            $= ( \\frac{dp_1}{d\\theta} r_1 + \\frac{dp_0}{d\\theta} r_0 ) + \\lambda (-p_1 p_0 \\theta)$\n            $= (p_1 p_0 r_1 - p_1 p_0 r_0) - \\lambda p_1 p_0 \\theta$\n            $= p_1 p_0 (r_1 - r_0 - \\lambda \\theta)$.\n        *   This confirms my previous derivative calculation. The rest of the argument holds.\n        *   The result $\\theta^{\\star} = \\frac{r_1 - r_0}{\\lambda}$ is a well-known result for the two-action case with entropy regularization. The logic is that the optimal logit difference should be proportional to the reward difference, scaled by the entropy coefficient (which acts like a temperature). If $\\lambda$ is large (high temperature), the agent becomes more random and the logits are pushed towards zero, so $\\theta^{\\star}$ is small. If $\\lambda$ is small (low temperature), the agent is more exploitative and the optimal logit $\\theta^{\\star}$ is large, reflecting the reward difference more strongly. If $r_1  r_0$, $\\theta^{\\star}  0$, favoring action 1. If $r_1  r_0$, $\\theta^{\\star}  0$, favoring action 0. If $r_1 = r_0$, $\\theta^{\\star} = 0$, leading to a uniform policy $\\pi(1|s) = \\pi(0|s) = 0.5$, which makes sense as there's no reward incentive to prefer one action, so the agent maximizes entropy. The result is physically and intuitively consistent with the principles of entropy-regularized RL.\n\n    *   The steps are:\n        1.  Formulate the objective function $J(\\theta)$ which is the value function $V^{\\pi_\\theta}(s)$.\n        2.  Show that maximizing $J(\\theta)$ is equivalent to maximizing a simpler term $L(\\theta)$.\n        3.  Express $L(\\theta)$ in terms of the parameter $\\theta$.\n        4.  Differentiate $L(\\theta)$ with respect to $\\theta$.\n        5.  Set the derivative to zero and solve for $\\theta$.\n        6.  Verify that this solution corresponds to a maximum using the second derivative test.\n        7.  Write down the final expression.\n\nThe structure of the solution should be clear and follow these steps. All mathematical entities will be in LaTeX.\nThe final answer is a single expression $\\frac{r_1 - r_0}{\\lambda}$. I will enclose it in `$$\\boxed{...}$$`.\nEverything looks correct. I am ready to write the solution.",
            "answer": "$$\n\\boxed{\\frac{r_{1} - r_{0}}{\\lambda}}\n$$"
        },
        {
            "introduction": "Evaluating the long-term performance of a policy is a central task in reinforcement learning, with two primary frameworks being the discounted and average-reward criteria. This exercise challenges you to bridge the gap between these two perspectives by analyzing the behavior of the discounted value function $V_{\\gamma}(s)$ as the discount factor $\\gamma$ approaches one . By solving this, you will uncover a fundamental relationship that connects the transient, time-preferential view of discounted rewards to the steady-state performance captured by the average reward $\\rho$.",
            "id": "4113169",
            "problem": "Consider an infinite-horizon Markov Decision Process (MDP) under a fixed stationary policy that induces a time-homogeneous Markov chain on the finite state space $\\{s_{1}, s_{2}\\}$. The one-step transition matrix under this policy is\n$$\nP \\;=\\; \\begin{pmatrix}\n0.7  0.3 \\\\\n0.4  0.6\n\\end{pmatrix},\n$$\nand the per-step reward depends only on the current state with reward vector\n$$\nr \\;=\\; \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}.\n$$\nAssume the initial state is $s_{0} = s_{1}$.\n\nLet the discounted value function for discount factor $\\gamma \\in (0,1)$ be defined by\n$$\nV_{\\gamma}(s) \\;=\\; \\mathbb{E}\\!\\left[\\sum_{t=0}^{\\infty} \\gamma^{t} \\, r(s_{t}) \\,\\bigg|\\, s_{0} = s \\right].\n$$\nLet the average reward (gain) be defined, for this finite ergodic chain, by\n$$\n\\rho \\;=\\; \\lim_{n \\to \\infty} \\frac{1}{n} \\, \\mathbb{E}\\!\\left[\\sum_{t=0}^{n-1} r(s_{t}) \\,\\bigg|\\, s_{0} = s \\right],\n$$\nwhich is independent of the initial state for an irreducible and aperiodic $P$.\n\nStarting only from these definitions and basic facts about finite-state ergodic Markov chains (existence and uniqueness of a stationary distribution, and the convergence of powers $P^{t}$), derive an explicit expression for $V_{\\gamma}(s)$ in terms of $P$ and $r$, and then compute the limit\n$$\nL \\;\\equiv\\; \\lim_{\\gamma \\uparrow 1} \\, (1 - \\gamma)\\, V_{\\gamma}(s_{1}).\n$$\nProvide the final value of $L$ as a single exact number. Do not use numerical rounding.",
            "solution": "The problem is well-posed and scientifically grounded within the theory of Markov Decision Processes. We proceed with the solution.\n\nThe discounted value function for a state $s$ under a fixed policy is given by the expectation of the sum of discounted future rewards:\n$$\nV_{\\gamma}(s) = \\mathbb{E}\\left[\\sum_{t=0}^{\\infty} \\gamma^{t} \\, r(s_{t}) \\,\\bigg|\\, s_{0} = s \\right]\n$$\nwhere $\\gamma \\in (0,1)$ is the discount factor, $s_t$ is the state at time $t$, and $r(s_t)$ is the reward received in state $s_t$.\n\nWe can express this using the Bellman equation for a fixed policy. By expanding the sum for one step, we have:\n$$\nV_{\\gamma}(s) = r(s) + \\mathbb{E}\\left[\\sum_{t=1}^{\\infty} \\gamma^{t} \\, r(s_{t}) \\,\\bigg|\\, s_{0} = s \\right]\n$$\n$$\nV_{\\gamma}(s) = r(s) + \\gamma \\, \\mathbb{E}\\left[\\sum_{t'=0}^{\\infty} \\gamma^{t'} \\, r(s_{t'+1}) \\,\\bigg|\\, s_{0} = s \\right]\n$$\nBy the law of total expectation and the Markov property, the expectation of the future value, given the current state $s$, is the sum over all possible next states $s'$ of the transition probability $P(s'|s)$ times the value function at that next state $V_{\\gamma}(s')$.\n$$\n\\mathbb{E}\\left[\\sum_{t'=0}^{\\infty} \\gamma^{t'} \\, r(s_{t'+1}) \\,\\bigg|\\, s_{0} = s \\right] = \\sum_{s' \\in \\{s_1, s_2\\}} P(s'|s) V_{\\gamma}(s')\n$$\nThis gives the Bellman equation:\n$$\nV_{\\gamma}(s) = r(s) + \\gamma \\sum_{s' \\in \\{s_1, s_2\\}} P(s'|s) V_{\\gamma}(s')\n$$\nWe can write this in matrix-vector form. Let $V_{\\gamma}$ be the column vector of value functions, $V_{\\gamma} = \\begin{pmatrix} V_{\\gamma}(s_1) \\\\ V_{\\gamma}(s_2) \\end{pmatrix}$, and $r$ be the column vector of rewards, $r = \\begin{pmatrix} r(s_1) \\\\ r(s_2) \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$. The transition matrix is $P = \\begin{pmatrix} P(s_1|s_1)  P(s_2|s_1) \\\\ P(s_1|s_2)  P(s_2|s_2) \\end{pmatrix} = \\begin{pmatrix} 0.7  0.3 \\\\ 0.4  0.6 \\end{pmatrix}$. The Bellman equation becomes:\n$$\nV_{\\gamma} = r + \\gamma P V_{\\gamma}\n$$\nThis is a system of linear equations for the vector $V_{\\gamma}$. We can solve for $V_{\\gamma}$:\n$$\n(I - \\gamma P) V_{\\gamma} = r\n$$\nwhere $I$ is the $2 \\times 2$ identity matrix. For $\\gamma \\in (0,1)$, the matrix $(I-\\gamma P)$ is guaranteed to be invertible. The eigenvalues of a stochastic matrix $P$ have magnitude less than or equal to $1$. Thus, the eigenvalues of $\\gamma P$ have magnitude strictly less than $1$, which means $1$ is not an eigenvalue of $\\gamma P$, so $I - \\gamma P$ is non-singular. The solution is:\n$$\nV_{\\gamma} = (I - \\gamma P)^{-1} r\n$$\nThis gives the explicit expression for the value function vector in terms of $P$ and $r$.\n\nThe problem requires the calculation of the limit:\n$$\nL = \\lim_{\\gamma \\uparrow 1} \\, (1 - \\gamma)\\, V_{\\gamma}(s_{1})\n$$\nThis corresponds to the first component of the vector limit $\\lim_{\\gamma \\uparrow 1} (1 - \\gamma) V_{\\gamma}$. Substituting our expression for $V_{\\gamma}$:\n$$\n\\lim_{\\gamma \\uparrow 1} (1 - \\gamma) V_{\\gamma} = \\lim_{\\gamma \\uparrow 1} (1 - \\gamma) (I - \\gamma P)^{-1} r\n$$\nAs $\\gamma \\to 1$, the matrix $I - \\gamma P$ approaches $I - P$, which is singular because $P$ has an eigenvalue of $1$. The factor $(1 - \\gamma)$ resolves this singularity. A fundamental result in the theory of finite Markov chains states that the resolvent $(I - \\gamma P)^{-1}$ has a simple pole at $\\gamma = 1$. The residue at this pole is related to the stationary distribution. Specifically, one can show that:\n$$\n\\lim_{\\gamma \\uparrow 1} (1 - \\gamma) (I - \\gamma P)^{-1} = P^*\n$$\nwhere $P^* = \\lim_{n \\to \\infty} P^n$. For an irreducible and aperiodic Markov chain, as is the case here since all entries of $P$ are positive, this limit exists and $P^*$ is a matrix where every row is the unique stationary distribution vector $\\pi = (\\pi_1, \\pi_2)$.\n$$\nP^* = \\begin{pmatrix} \\pi_1  \\pi_2 \\\\ \\pi_1  \\pi_2 \\end{pmatrix}\n$$\nThe stationary distribution $\\pi$ is the unique left eigenvector of $P$ corresponding to the eigenvalue $1$, satisfying $\\pi P = \\pi$ and $\\sum_i \\pi_i = 1$.\n$$\n(\\pi_1, \\pi_2) \\begin{pmatrix} 0.7  0.3 \\\\ 0.4  0.6 \\end{pmatrix} = (\\pi_1, \\pi_2)\n$$\nThis yields the system of equations:\n$$\n\\begin{cases}\n0.7 \\pi_1 + 0.4 \\pi_2 = \\pi_1 \\\\\n0.3 \\pi_1 + 0.6 \\pi_2 = \\pi_2 \\\\\n\\pi_1 + \\pi_2 = 1\n\\end{cases}\n$$\nThe first two equations are equivalent to $0.4 \\pi_2 = 0.3 \\pi_1$, or $4 \\pi_2 = 3 \\pi_1$. Substituting $\\pi_1 = 1 - \\pi_2$ from the normalization condition into this equation gives:\n$$\n4 \\pi_2 = 3(1 - \\pi_2) \\implies 4 \\pi_2 = 3 - 3 \\pi_2 \\implies 7 \\pi_2 = 3 \\implies \\pi_2 = \\frac{3}{7}\n$$\nThen, $\\pi_1 = 1 - \\pi_2 = 1 - \\frac{3}{7} = \\frac{4}{7}$. So, the stationary distribution is $\\pi = (\\frac{4}{7}, \\frac{3}{7})$.\n\nNow we can compute the limit vector:\n$$\n\\lim_{\\gamma \\uparrow 1} (1 - \\gamma) V_{\\gamma} = P^* r = \\begin{pmatrix} 4/7  3/7 \\\\ 4/7  3/7 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}\n$$\n$$\nP^* r = \\begin{pmatrix} \\frac{4}{7}(2) + \\frac{3}{7}(1) \\\\ \\frac{4}{7}(2) + \\frac{3}{7}(1) \\end{pmatrix} = \\begin{pmatrix} \\frac{8}{7} + \\frac{3}{7} \\\\ \\frac{8}{7} + \\frac{3}{7} \\end{pmatrix} = \\begin{pmatrix} \\frac{11}{7} \\\\ \\frac{11}{7} \\end{pmatrix}\n$$\nThe limit is a constant vector. This constant value is the average reward $\\rho$, which is independent of the initial state for an ergodic chain. We can verify this:\n$$\n\\rho = \\sum_s \\pi(s) r(s) = \\pi r = (\\frac{4}{7}, \\frac{3}{7}) \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = \\frac{8}{7} + \\frac{3}{7} = \\frac{11}{7}\n$$\nThe problem asks for $L$, which is the limit of the value function starting in state $s_1$:\n$$\nL = \\lim_{\\gamma \\uparrow 1} \\, (1 - \\gamma)\\, V_{\\gamma}(s_{1})\n$$\nThis is the first component of the limit vector we calculated. Therefore,\n$$\nL = \\frac{11}{7}\n$$\nThis is an exact numerical value as required.",
            "answer": "$$\n\\boxed{\\frac{11}{7}}\n$$"
        },
        {
            "introduction": "Effective learning in complex environments often depends on well-designed reward signals, yet manually engineering rewards can be difficult and may inadvertently alter the optimal behavior. This practice problem investigates potential-based reward shaping, a principled method for guiding an agent without changing its optimal policy . Through a rigorous derivation, you will demonstrate exactly how this technique modifies the agent's value function in a predictable way, providing a powerful tool for accelerating learning while guaranteeing policy invariance.",
            "id": "4113139",
            "problem": "Consider a single learning agent embedded in a complex adaptive system that is modeled as a Markov Decision Process (MDP). A Markov Decision Process (MDP) is defined by a state space $\\mathcal{S}$, an action space $\\mathcal{A}$, a transition kernel $P(s' \\mid s, a)$, and a bounded one-step reward function $r(s,a,s')$ with $|r(s,a,s')| \\leq R_{\\max}  \\infty$. The agent follows a stationary policy $\\pi(a \\mid s)$ and interacts over an infinite horizon with a discount factor $\\gamma \\in (0,1)$. The realized one-step reward at time $t$ is denoted $R_t$, and the discounted return from time $t$ is defined by $G_t = \\sum_{k=0}^{\\infty} \\gamma^{k} R_{t+k}$. The corresponding value function under policy $\\pi$ is $V^{\\pi}(s) = \\mathbb{E}[G_t \\mid s_t = s]$, where the expectation is over the trajectory distribution induced by $\\pi$ and $P$.\n\nTo accelerate learning while preserving the decision-making structure, the agentâ€™s environmental reward is modified by a potential-based shaping term using a bounded potential function $\\Phi : \\mathcal{S} \\to \\mathbb{R}$ with $\\sup_{s \\in \\mathcal{S}} |\\Phi(s)|  \\infty$. The shaped reward at time $t$ is defined by $R'_t = R_t + \\gamma \\, \\Phi(s_{t+1}) - \\Phi(s_t)$, yielding a shaped discounted return $G'_t = \\sum_{k=0}^{\\infty} \\gamma^{k} R'_{t+k}$ and a shaped value function $V'^{\\pi}(s) = \\mathbb{E}[G'_t \\mid s_t = s]$.\n\nStarting from the fundamental definitions of discounted return and expectation, and without invoking any shortcut identities, derive the exact closed-form expression for the difference $V'^{\\pi}(s_0) - V^{\\pi}(s_0)$ for an arbitrary initial state $s_0 \\in \\mathcal{S}$ under any stationary policy $\\pi$ that induces a well-defined trajectory distribution. Assume all required expectations exist and that the boundedness conditions stated above hold so that infinite series converge and interchanges of limits and expectations are justified by standard theorems in measure-theoretic probability.\n\nExpress your final answer as a single analytic expression in terms of $\\Phi(s_0)$ only. No numerical approximation, rounding, units, or percentage formatting are required.",
            "solution": "The problem requires the derivation of the exact closed-form expression for the difference between the shaped value function, $V'^{\\pi}(s_0)$, and the original value function, $V^{\\pi}(s_0)$, for an arbitrary initial state $s_0$. The derivation will proceed from the fundamental definitions provided in the problem statement.\n\nThe quantity to be derived is $V'^{\\pi}(s_0) - V^{\\pi}(s_0)$.\n\nBy definition, the value functions are the expected discounted returns:\n$$V'^{\\pi}(s_0) = \\mathbb{E}[G'_t \\mid s_t = s_0]$$\n$$V^{\\pi}(s_0) = \\mathbb{E}[G_t \\mid s_t = s_0]$$\nThe expectation $\\mathbb{E}[\\cdot \\mid s_t = s_0]$ is taken over all possible trajectories starting from state $s_0$, with the trajectory distribution induced by the policy $\\pi(a \\mid s)$ and the transition kernel $P(s' \\mid s, a)$.\n\nThe difference can be written as:\n$$V'^{\\pi}(s_0) - V^{\\pi}(s_0) = \\mathbb{E}[G'_t \\mid s_t = s_0] - \\mathbb{E}[G_t \\mid s_t = s_0]$$\nBy the linearity of expectation, we can combine the terms:\n$$V'^{\\pi}(s_0) - V^{\\pi}(s_0) = \\mathbb{E}[G'_t - G_t \\mid s_t = s_0]$$\n\nNext, we substitute the definitions of the discounted returns $G'_t$ and $G_t$:\n$$G'_t = \\sum_{k=0}^{\\infty} \\gamma^{k} R'_{t+k}$$\n$$G_t = \\sum_{k=0}^{\\infty} \\gamma^{k} R_{t+k}$$\nThe difference between the returns is:\n$$G'_t - G_t = \\sum_{k=0}^{\\infty} \\gamma^{k} R'_{t+k} - \\sum_{k=0}^{\\infty} \\gamma^{k} R_{t+k} = \\sum_{k=0}^{\\infty} \\gamma^{k} (R'_{t+k} - R_{t+k})$$\nThe problem states that the shaped reward $R'_t$ is defined as $R'_t = R_t + \\gamma \\, \\Phi(s_{t+1}) - \\Phi(s_t)$. Applying this to the reward at time step $t+k$:\n$$R'_{t+k} = R_{t+k} + \\gamma \\, \\Phi(s_{t+k+1}) - \\Phi(s_{t+k})$$\nFrom this, the difference in rewards is:\n$$R'_{t+k} - R_{t+k} = \\gamma \\, \\Phi(s_{t+k+1}) - \\Phi(s_{t+k})$$\nSubstituting this expression back into the sum for $G'_t - G_t$:\n$$G'_t - G_t = \\sum_{k=0}^{\\infty} \\gamma^{k} (\\gamma \\, \\Phi(s_{t+k+1}) - \\Phi(s_{t+k}))$$\nWe can distribute the $\\gamma^k$ term into the parenthesis:\n$$G'_t - G_t = \\sum_{k=0}^{\\infty} (\\gamma^{k+1} \\Phi(s_{t+k+1}) - \\gamma^k \\Phi(s_{t+k}))$$\nThis is a telescoping series. To see this, let's define a sequence $A_k = \\gamma^k \\Phi(s_{t+k})$. The sum becomes:\n$$G'_t - G_t = \\sum_{k=0}^{\\infty} (A_{k+1} - A_k)$$\nLet's analyze the partial sum up to an arbitrary integer $N$:\n$$S_N = \\sum_{k=0}^{N} (A_{k+1} - A_k) = (A_1 - A_0) + (A_2 - A_1) + \\dots + (A_{N+1} - A_N)$$\nThe intermediate terms cancel out, leaving:\n$$S_N = A_{N+1} - A_0$$\nSubstituting back the definition of $A_k$:\n$$S_N = \\gamma^{N+1} \\Phi(s_{t+N+1}) - \\gamma^0 \\Phi(s_{t+0}) = \\gamma^{N+1} \\Phi(s_{t+N+1}) - \\Phi(s_t)$$\nTo find the value of the infinite sum, we take the limit as $N \\to \\infty$:\n$$G'_t - G_t = \\lim_{N\\to\\infty} S_N = \\lim_{N\\to\\infty} \\left( \\gamma^{N+1} \\Phi(s_{t+N+1}) - \\Phi(s_t) \\right)$$\nThe problem states that the potential function is bounded, $\\sup_{s \\in \\mathcal{S}} |\\Phi(s)|  \\infty$. Let's denote this bound by $\\Phi_{\\max}$. We also have $\\gamma \\in (0,1)$.\nThe term $\\gamma^{N+1} \\Phi(s_{t+N+1})$ is bounded in magnitude:\n$$|\\gamma^{N+1} \\Phi(s_{t+N+1})| \\leq \\gamma^{N+1} \\Phi_{\\max}$$\nSince $\\lim_{N\\to\\infty} \\gamma^{N+1} = 0$, it follows by the Squeeze Theorem that:\n$$\\lim_{N\\to\\infty} \\gamma^{N+1} \\Phi(s_{t+N+1}) = 0$$\nThe term $-\\Phi(s_t)$ is independent of $N$. Therefore, the limit of the partial sum is:\n$$G'_t - G_t = 0 - \\Phi(s_t) = -\\Phi(s_t)$$\nNow we can return to the expression for the difference in value functions:\n$$V'^{\\pi}(s_0) - V^{\\pi}(s_0) = \\mathbb{E}[G'_t - G_t \\mid s_t = s_0] = \\mathbb{E}[-\\Phi(s_t) \\mid s_t = s_0]$$\nThe expectation of the quantity $-\\Phi(s_t)$ is conditioned on the event that the state at time $t$ is $s_0$. The expression $-\\Phi(s_t)$ is a deterministic function of the state $s_t$. Given the condition $s_t = s_0$, the value of the expression is fixed at $-\\Phi(s_0)$. Therefore, the expectation is simply this fixed value:\n$$\\mathbb{E}[-\\Phi(s_t) \\mid s_t = s_0] = -\\Phi(s_0)$$\nThis leads to the final result:\n$$V'^{\\pi}(s_0) - V^{\\pi}(s_0) = -\\Phi(s_0)$$\nThis concludes the derivation from first principles as required.",
            "answer": "$$\\boxed{-\\Phi(s_0)}$$"
        }
    ]
}