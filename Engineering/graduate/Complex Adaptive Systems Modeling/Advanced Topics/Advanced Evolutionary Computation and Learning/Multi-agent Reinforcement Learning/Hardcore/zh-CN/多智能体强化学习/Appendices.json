{
    "hands_on_practices": [
        {
            "introduction": "在多智能体强化学习（MARL）中，一个核心挑战是环境的非平稳性（non-stationarity）。当多个智能体同时学习时，每个智能体都会将其他智能体的行为视为其环境的一部分，导致其学习目标不断变化。这个练习  通过一个简单的双智能体梯度上升场景，让您亲手计算由其他智能体学习所引发的局部目标梯度的变化，从而对这一“移动目标”问题获得一个定量的、直观的理解。",
            "id": "4130831",
            "problem": "两个智能体进行多智能体强化学习 (MARL)，其模型是在一个复杂自适应系统内，对一个共享的可微阶段效用函数进行同步短视梯度上升。两个智能体都选择标量连续动作，分别表示为 $a_1$ 和 $a_2$。每个智能体通过在当前联合动作下，沿着自身效用函数相对于自身动作的梯度方向移动一小步来更新其动作。形式上，对于学习率为 $\\alpha_i$ 的智能体 $i \\in \\{1,2\\}$，其更新规则为 $a_i(t+1)=a_i(t)+\\alpha_i \\nabla_{a_i} u_i(a_1(t),a_2(t))$。假设两个智能体最大化相同的阶段效用 $u_1(a_1,a_2)=u_2(a_1,a_2)=a_1 a_2$。在时间 $t=0$ 时，初始动作为 $a_1(0)=0.6$ 和 $a_2(0)=-0.4$，学习率分别为 $\\alpha_1=0.1$ 和 $\\alpha_2=0.01$。\n\n执行一次同步梯度上升更新，以获得 $a_1(1)$ 和 $a_2(1)$。然后，为了量化在 $t=0$ 到 $t=1$ 之间每个智能体局部目标景观中引起的非平稳性，将非平稳性大小定义为智能体自身动作梯度变化的欧几里得范数：\n$$\nN \\equiv \\sqrt{\\left(\\nabla_{a_1} u_1(a_1(1),a_2(1))-\\nabla_{a_1} u_1(a_1(0),a_2(0))\\right)^{2}+\\left(\\nabla_{a_2} u_2(a_1(1),a_2(1))-\\nabla_{a_2} u_2(a_1(0),a_2(0))\\right)^{2}}.\n$$\n计算给定参数下的 $N$ 值。将最终数值答案四舍五入到四位有效数字。以无单位的纯数字形式表达您的答案。",
            "solution": "用户想要解决一个多智能体强化学习中的问题。\n\n### 步骤 1：问题验证\n\n**1.1. 提取给定条件**\n\n*   **智能体和动作**：两个智能体 $i \\in \\{1,2\\}$，具有标量连续动作 $a_1$ 和 $a_2$。\n*   **效用函数**：两个智能体共享的可微效用函数：$u_1(a_1, a_2) = u_2(a_1, a_2) = a_1 a_2$。\n*   **学习动态**：同步短视梯度上升。智能体 $i$ 的更新规则是 $a_i(t+1) = a_i(t) + \\alpha_i \\nabla_{a_i} u_i(a_1(t), a_2(t))$。\n*   **$t=0$ 时的初始条件**：$a_1(0) = 0.6$ 和 $a_2(0) = -0.4$。\n*   **学习率**：$\\alpha_1 = 0.1$ 和 $\\alpha_2 = 0.01$。\n*   **目标**：计算非平稳性大小 $N$，其定义如下：\n    $$\n    N \\equiv \\sqrt{\\left(\\nabla_{a_1} u_1(a_1(1),a_2(1))-\\nabla_{a_1} u_1(a_1(0),a_2(0))\\right)^{2}+\\left(\\nabla_{a_2} u_2(a_1(1),a_2(1))-\\nabla_{a_2} u_2(a_1(0),a_2(0))\\right)^{2}}\n    $$\n*   **输出格式**：最终数值答案四舍五入到四位有效数字。\n\n**1.2. 使用提取的给定条件进行验证**\n\n*   **科学依据**：该问题在多智能体强化学习领域有充分的理论基础。独立学习者在共享效用函数上使用同步梯度上升是研究学习动态和非平稳性的标准模型。效用函数 $u(a_1, a_2) = a_1 a_2$ 代表一个简单但非平凡的交互景观（在原点处有一个鞍点）。\n*   **适定性**：问题陈述清晰，提供了所有必要的参数、初始条件和定义。计算过程是微积分和代数的直接应用，可以导出一个唯一的、明确定义的解。\n*   **客观性**：问题使用精确的数学语言表达，没有任何主观或模棱两可的陈述。\n\n**1.3. 结论与行动**\n\n该问题具有科学依据、适定性和客观性。它被认为是**有效的**。现在开始求解过程。\n\n### 步骤 2：求解推导\n\n求解过程需要一个序贯计算：首先，确定智能体在时间 $t=1$ 时的动作；其次，使用这些新动作计算梯度的变化和非平稳性大小 $N$。\n\n**2.1. 计算效用函数的梯度**\n\n共享效用函数为 $u(a_1, a_2) = a_1 a_2$。我们需要求出该效用函数相对于每个智能体自身动作的梯度。\n对智能体1，效用为 $u_1(a_1, a_2) = a_1 a_2$。相对于其自身动作 $a_1$ 的梯度为：\n$$\n\\nabla_{a_1} u_1(a_1, a_2) = \\frac{\\partial}{\\partial a_1}(a_1 a_2) = a_2\n$$\n对智能体2，效用为 $u_2(a_1, a_2) = a_1 a_2$。相对于其自身动作 $a_2$ 的梯度为：\n$$\n\\nabla_{a_2} u_2(a_1, a_2) = \\frac{\\partial}{\\partial a_2}(a_1 a_2) = a_1\n$$\n\n**2.2. 执行一次同步更新**\n\n智能体将其动作从 $t=0$ 更新到 $t=1$。首先，我们在初始状态 $(a_1(0), a_2(0)) = (0.6, -0.4)$ 处计算梯度值。\n$$\n\\nabla_{a_1} u_1(a_1(0), a_2(0)) = a_2(0) = -0.4\n$$\n$$\n\\nabla_{a_2} u_2(a_1(0), a_2(0)) = a_1(0) = 0.6\n$$\n现在，我们对每个智能体应用更新规则 $a_i(t+1) = a_i(t) + \\alpha_i \\nabla_{a_i} u_i(t)$。\n对智能体 1：\n$$\na_1(1) = a_1(0) + \\alpha_1 \\nabla_{a_1} u_1(a_1(0), a_2(0)) = 0.6 + (0.1)(-0.4) = 0.6 - 0.04 = 0.56\n$$\n对智能体 2：\n$$\na_2(1) = a_2(0) + \\alpha_2 \\nabla_{a_2} u_2(a_1(0), a_2(0)) = -0.4 + (0.01)(0.6) = -0.4 + 0.006 = -0.394\n$$\n因此，在 $t=1$ 时的新联合动作为 $(a_1(1), a_2(1)) = (0.56, -0.394)$。\n\n**2.3. 计算非平稳性大小 $N$**\n\n非平稳性大小 $N$ 定义为每个智能体自身动作梯度变化的欧几里得范数。我们已经有了 $t=0$ 时的梯度。现在我们计算新状态 $(a_1(1), a_2(1))$ 下的梯度。\n$$\n\\nabla_{a_1} u_1(a_1(1), a_2(1)) = a_2(1) = -0.394\n$$\n$$\n\\nabla_{a_2} u_2(a_1(1), a_2(1)) = a_1(1) = 0.56\n$$\n现在，我们可以计算每个梯度的变化量：\n智能体1的梯度变化量为 $\\Delta_1$：\n$$\n\\Delta_1 = \\nabla_{a_1} u_1(a_1(1), a_2(1)) - \\nabla_{a_1} u_1(a_1(0), a_2(0)) = -0.394 - (-0.4) = 0.006\n$$\n智能体2的梯度变化量为 $\\Delta_2$：\n$$\n\\Delta_2 = \\nabla_{a_2} u_2(a_1(1), a_2(1)) - \\nabla_{a_2} u_2(a_1(0), a_2(0)) = 0.56 - 0.6 = -0.04\n$$\n最后，我们将这些变化量代入 $N$ 的公式中：\n$$\nN = \\sqrt{(\\Delta_1)^2 + (\\Delta_2)^2} = \\sqrt{(0.006)^2 + (-0.04)^2}\n$$\n$$\nN = \\sqrt{0.000036 + 0.0016} = \\sqrt{0.001636}\n$$\n计算数值：\n$$\nN \\approx 0.040447496...\n$$\n问题要求答案四舍五入到四位有效数字。第一位有效数字是百分位上的 $4$。数四位数字得到 $0.04044$。下一位数字是 $7$，大于或等于 $5$，所以我们对最后一位数字进行进位。\n$$\nN \\approx 0.04045\n$$\n这是最终的数值答案。",
            "answer": "$$\n\\boxed{0.04045}\n$$"
        },
        {
            "introduction": "“中心化训练，去中心化执行”（CTDE）是解决协同多智能体任务的强大范式，它允许在训练时利用全局信息，而在执行时仅依赖局部观测。此练习  聚焦于值分解网络（如VDN和QMIX）的核心机制，即通过一个单调的混合网络将个体效用函数组合成一个全局 $Q$ 值。您将通过实践发现，只要满足单调性条件，每个智能体独立地最大化自身效用就能实现全局最优联合动作，这是实现高效去中心化执行的理论基石。",
            "id": "4130864",
            "problem": "考虑一个在集中式训练与分布式执行（CTDE）范式下的协作式双智能体马尔可夫博弈。设联合动作价值函数由一个单调线性混合网络表示，定义为 $Q_{\\text{tot}}(s,\\mathbf{a})=w_1 Q_1(s,a_1)+w_2 Q_2(s,a_2)+b$，其中 $w_1 \\ge 0$，$w_2 \\ge 0$，$b \\in \\mathbb{R}$，$Q_i(s,a_i)$ 是智能体 $i \\in \\{1,2\\}$ 在状态 $s$ 和动作 $a_i$ 下的特定于智能体的效用，$\\mathbf{a}=(a_1,a_2)$ 是联合动作。假设状态 $s$ 固定，离散动作集为 $\\mathcal{A}_1=\\{0,1,2\\}$ 和 $\\mathcal{A}_2=\\{0,1,2\\}$，且具有以下智能体效用：\n$$Q_1(s,0)=1.437,\\quad Q_1(s,1)=1.513,\\quad Q_1(s,2)=1.499,$$\n$$Q_2(s,0)=2.105,\\quad Q_2(s,1)=2.062,\\quad Q_2(s,2)=2.176,$$\n混合参数为 $w_1=0.73$，$w_2=1.27$，$b=-0.418$。仅使用联合动作价值函数、贪心选择和单调映射的基本定义，确定在分布式选择下的贪心联合动作，其中每个智能体独立选择一个最大化其自身特定效用的动作。最终答案必须是按顺序排列的智能体1和智能体2所选动作索引的有序对。无需四舍五入。",
            "solution": "在尝试求解之前，将首先验证问题的科学合理性、自洽性和清晰度。\n\n### 步骤 1：提取已知信息\n问题提供了以下信息：\n-   **系统**：协作式双智能体马尔可夫博弈。\n-   **范式**：集中式训练与分布式执行（CTDE）。\n-   **联合动作价值函数**：$Q_{\\text{tot}}(s,\\mathbf{a})=w_1 Q_1(s,a_1)+w_2 Q_2(s,a_2)+b$。\n-   **智能体效用**：$Q_i(s,a_i)$，表示智能体 $i \\in \\{1,2\\}$ 在状态 $s$ 采取动作 $a_i$ 时的效用。\n-   **联合动作**：$\\mathbf{a}=(a_1,a_2)$。\n-   **混合网络参数**：\n    -   权重：$w_1=0.73$，$w_2=1.27$。\n    -   偏置：$b=-0.418$。\n-   **参数约束**：$w_1 \\ge 0$，$w_2 \\ge 0$。\n-   **状态**：一个固定的状态 $s$。\n-   **动作集**：$\\mathcal{A}_1=\\{0,1,2\\}$ 和 $\\mathcal{A}_2=\\{0,1,2\\}$。\n-   **智能体 1 效用**：\n    -   $Q_1(s,0)=1.437$\n    -   $Q_1(s,1)=1.513$\n    -   $Q_1(s,2)=1.499$\n-   **智能体 2 效用**：\n    -   $Q_2(s,0)=2.105$\n    -   $Q_2(s,1)=2.062$\n    -   $Q_2(s,2)=2.176$\n-   **选择规则**：“分布式选择，其中每个智能体独立选择一个最大化其自身特定效用的动作。”\n-   **目标**：确定贪心联合动作 $(a_1, a_2)$。\n\n### 步骤 2：使用提取的已知信息进行验证\n根据验证标准对问题进行评估。\n-   **科学依据**：该问题描述了一个简化的线性价值混合网络，这是多智能体强化学习（MARL）中的一个基本概念，与 VDN（价值分解网络）和 QMIX 等方法相关。CTDE 范式是 MARL 中的一种标准方法。所述的单调性条件（$w_i \\ge 0$）对于确保分布式贪心动作选择与联合动作价值函数的贪心动作相对应至关重要，这一原则被称为个体-全局-最大（IGM）一致性。该问题具有科学合理性。\n-   **适定性**：所有必要的数据（智能体效用、动作集）和精确的动作选择规则都已提供。该问题是自洽的，没有歧义，能够导出一个唯一的解。\n-   **客观性**：该问题使用强化学习领域中常见的精确、形式化的语言进行陈述。它不包含主观或基于观点的论断。\n\n### 步骤 3：结论与行动\n该问题是有效的。它适定、有科学依据，并基于 MARL 中既定的原则提出了一个明确的任务。现在将推导解答。\n\n### 解答推导\n目标是找到在分布式执行下的贪心联合动作。问题明确指出了此选择的规则：“每个智能体独立选择一个最大化其自身特定效用的动作。”\n\n设 $a_1^*$ 是智能体 1 选择的动作，$a_2^*$ 是智能体 2 选择的动作。根据指定规则，这些动作按如下方式确定：\n$$a_1^* = \\arg\\max_{a_1 \\in \\mathcal{A}_1} Q_1(s, a_1)$$\n$$a_2^* = \\arg\\max_{a_2 \\in \\mathcal{A}_2} Q_2(s, a_2)$$\n\n**智能体 1 动作选择：**\n我们必须找到动作 $a_1 \\in \\{0, 1, 2\\}$ 以最大化 $Q_1(s, a_1)$。给定的智能体 1 的效用是：\n-   $Q_1(s,0) = 1.437$\n-   $Q_1(s,1) = 1.513$\n-   $Q_1(s,2) = 1.499$\n\n通过比较这些值，我们发现 $1.513$ 是最大值。\n$$\\max(1.437, 1.513, 1.499) = 1.513$$\n该最大效用对应的动作索引为 $1$。因此，智能体 1 选择动作 $a_1^*=1$。\n\n**智能体 2 动作选择：**\n我们必须找到动作 $a_2 \\in \\{0, 1, 2\\}$ 以最大化 $Q_2(s, a_2)$。给定的智能体 2 的效用是：\n-   $Q_2(s,0) = 2.105$\n-   $Q_2(s,1) = 2.062$\n-   $Q_2(s,2) = 2.176$\n\n通过比较这些值，我们发现 $2.176$ 是最大值。\n$$\\max(2.105, 2.062, 2.176) = 2.176$$\n该最大效用对应的动作索引为 $2$。因此，智能体 2 选择动作 $a_2^*=2$。\n\n**贪心联合动作：**\n在分布式选择下的贪心联合动作是各个智能体独立选择的动作的有序对，即 $(a_1^*, a_2^*)$。\n根据以上计算，联合动作为 $(1, 2)$。\n\n需要注意的是混合网络参数的作用。问题提供了 $Q_{\\text{tot}}(s,\\mathbf{a})=w_1 Q_1(s,a_1)+w_2 Q_2(s,a_2)+b$，其中权重为非负数 $w_1=0.73$ 和 $w_2=1.27$。权重的非负性确保了 $Q_{\\text{tot}}$ 是每个智能体效用 $Q_i$ 的单调函数。这保证了在联合动作空间上最大化 $Q_{\\text{tot}}$ 等价于每个智能体最大化其局部效用 $Q_i$。\n$$\\frac{\\partial Q_{\\text{tot}}}{\\partial Q_i} = w_i \\ge 0$$\n这个性质，即个体-全局-最大（IGM）一致性，是允许分布式执行的理论基础。动作对 $(1,2)$ 不仅是分布式选择的结果，也最大化了全局动作价值函数 $Q_{\\text{tot}}$。参数 $w_1$、$w_2$ 和 $b$ 是用于确认分布式方法有效性的上下文信息，但根据问题的明确指示，直接计算时并不需要它们。\n\n最终答案是智能体1和智能体2的动作索引的有序对。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1  2\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "在多智能体策略梯度方法中，信誉分配（credit assignment）是一个关键难题：如何判断每个智能体对团队的集体回报贡献了多少？此练习  引导您探索“反事实多智能体”（COMA）算法如何通过一个巧妙的基线函数来解决此问题。您需要计算COMA优势函数（advantage），它通过将智能体的实际动作价值与一个“反事实”基线（即，如果该智能体采取其他动作，团队会获得什么价值）进行比较，从而精确地分离出单个智能体的贡献。",
            "id": "4130833",
            "problem": "考虑一个完全合作的马尔可夫博弈，其中有 $n$ 个智能体，在集中式评论家和分布式执行器的框架下进行训练。在某一特定时间步，固定全局状态 $s$ 和其他智能体的联合动作 $\\mathbf{a}_{-i}$。智能体 $i$ 有一个离散动作集 $\\{a_i^{(0)}, a_i^{(1)}, a_i^{(2)}\\}$，在此时间步，其执行器策略 $\\pi_i$ 是均匀的，因此对于 $k \\in \\{0,1,2\\}$，有 $\\pi_i(a_i^{(k)} \\mid o_i) = \\frac{1}{3}$，其中 $o_i$ 表示智能体 $i$ 的局部观测。集中式评论家对智能体 $i$ 的动作进行反事实替换（同时保持 $\\mathbf{a}_{-i}$ 固定）后，输出以下动作价值：\n$$Q\\!\\left(s,\\left(a_i^{(0)},\\mathbf{a}_{-i}\\right)\\right) = 4.75,\\quad Q\\!\\left(s,\\left(a_i^{(1)},\\mathbf{a}_{-i}\\right)\\right) = 3.05,\\quad Q\\!\\left(s,\\left(a_i^{(2)},\\mathbf{a}_{-i}\\right)\\right) = 5.20.$$\n假设实现的联合动作是 $\\mathbf{a} = \\left(a_i^{(2)},\\mathbf{a}_{-i}\\right)$，因此 $Q(s,\\mathbf{a}) = 5.20$。请利用带有状态依赖基线的方差缩减多智能体策略梯度原理，以及反事实多智能体（COMA）策略梯度中的反事实基线概念，推导在 $(s,\\mathbf{a})$ 处智能体 $i$ 的优势的解析表达式，并在给定的均匀策略 $\\pi_i$ 和评论家输出下计算其数值。将最终答案表示为一个纯数，无需四舍五入。",
            "solution": "出发点是多智能体执行器-评论家策略梯度，对于智能体 $i$，它使用优势函数来减少方差，同时保持梯度无偏。强化学习目标 $J(\\theta)$（其中 $\\theta$ 表示所有策略参数）对智能体 $i$ 产生的梯度为：\n$$\\nabla_{\\theta_i} J(\\theta) = \\mathbb{E}\\!\\left[\\nabla_{\\theta_i} \\ln \\pi_i(a_i \\mid o_i)\\, A_i(s,\\mathbf{a})\\right],$$\n其中 $A_i(s,\\mathbf{a})$ 是任何依赖于 $s$ 和联合动作 $\\mathbf{a}$ 的、经过基线调整的信号，其构造方式应使其在以 $o_i$ 为条件下的期望不会使梯度产生偏差：\n$$\\mathbb{E}\\!\\left[\\nabla_{\\theta_i} \\ln \\pi_i(a_i \\mid o_i)\\, b_i(s,\\mathbf{a}_{-i})\\right] = 0,$$\n只要基线 $b_i(s,\\mathbf{a}_{-i})$ 不依赖于 $a_i$。一个常见的选择是令 $A_i(s,\\mathbf{a}) = Q(s,\\mathbf{a}) - b_i(s,\\mathbf{a}_{-i})$，其中 $Q(s,\\mathbf{a})$ 是状态 $s$ 处联合动作的集中式动作价值。\n\n反事实多智能体（COMA）基线旨在解决多智能体信誉分配问题，其方法是固定其他智能体的动作，并根据智能体 $i$ 的当前策略对其动作进行边缘化。具体来说，智能体 $i$ 的 COMA 基线定义为：\n$$b_i^{\\text{COMA}}(s,\\mathbf{a}_{-i}) = \\sum_{a_i'} \\pi_i(a_i' \\mid o_i)\\, Q\\!\\left(s,\\left(a_i',\\mathbf{a}_{-i}\\right)\\right),$$\n这是评论家价值关于智能体 $i$ 策略的期望，通过反事实地替换其动作而保持 $\\mathbf{a}_{-i}$ 固定。这保持了无偏性，因为当以 $o_i$ 为条件时，$b_i^{\\text{COMA}}(s,\\mathbf{a}_{-i})$ 与实现的 $a_i$ 无关。\n\n因此，智能体 $i$ 在 $(s,\\mathbf{a})$ 处的 COMA 优势为：\n$$A_i^{\\text{COMA}}(s,\\mathbf{a}) = Q(s,\\mathbf{a}) - b_i^{\\text{COMA}}(s,\\mathbf{a}_{-i}).$$\n\n根据给定的均匀策略 $\\pi_i(a_i' \\mid o_i) = \\frac{1}{3}$（对于所有 $a_i' \\in \\{a_i^{(0)}, a_i^{(1)}, a_i^{(2)}\\}$），我们将 COMA 基线计算为所提供的三个评论家价值的简单平均值：\n\n$$\n\\begin{aligned}\nb_i^{\\text{COMA}}(s,\\mathbf{a}_{-i})\n= \\frac{1}{3}\\, Q\\!\\left(s,\\left(a_i^{(0)},\\mathbf{a}_{-i}\\right)\\right)\n+ \\frac{1}{3}\\, Q\\!\\left(s,\\left(a_i^{(1)},\\mathbf{a}_{-i}\\right)\\right)\n+ \\frac{1}{3}\\, Q\\!\\left(s,\\left(a_i^{(2)},\\mathbf{a}_{-i}\\right)\\right) \\\\\n= \\frac{1}{3}\\left(4.75 + 3.05 + 5.20\\right).\n\\end{aligned}\n$$\n\n为保持精确，将这些小数转换为有理数：\n$$4.75 = \\frac{19}{4}, \\quad 3.05 = \\frac{61}{20}, \\quad 5.20 = \\frac{26}{5}.$$\n求和，\n\n$$\n\\begin{aligned}\n\\frac{19}{4} + \\frac{61}{20} + \\frac{26}{5}\n= \\frac{95}{20} + \\frac{61}{20} + \\frac{104}{20} \\\\\n= \\frac{260}{20} \\\\\n= 13.\n\\end{aligned}\n$$\n\n因此，\n$$b_i^{\\text{COMA}}(s,\\mathbf{a}_{-i}) = \\frac{1}{3} \\cdot 13 = \\frac{13}{3}.$$\n实现的联合动作是 $\\mathbf{a} = \\left(a_i^{(2)},\\mathbf{a}_{-i}\\right)$，所以 $Q(s,\\mathbf{a}) = Q\\!\\left(s,\\left(a_i^{(2)},\\mathbf{a}_{-i}\\right)\\right) = 5.20 = \\frac{26}{5}.$\n\n最后，计算 COMA 优势：\n\n$$\n\\begin{aligned}\nA_i^{\\text{COMA}}(s,\\mathbf{a})\n= Q(s,\\mathbf{a}) - b_i^{\\text{COMA}}(s,\\mathbf{a}_{-i}) \\\\\n= \\frac{26}{5} - \\frac{13}{3} \\\\\n= \\frac{78}{15} - \\frac{65}{15} \\\\\n= \\frac{13}{15}.\n\\end{aligned}\n$$\n\n这就是在给定条件下 COMA 优势的精确数值。",
            "answer": "$$\\boxed{\\frac{13}{15}}$$"
        }
    ]
}