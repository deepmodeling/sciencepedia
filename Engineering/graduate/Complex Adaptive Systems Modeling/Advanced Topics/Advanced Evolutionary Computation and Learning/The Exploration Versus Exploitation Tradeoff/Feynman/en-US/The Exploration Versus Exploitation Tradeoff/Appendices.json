{
    "hands_on_practices": [
        {
            "introduction": "The Gittins index provides the provably optimal solution to the multi-armed bandit problem with geometric discounting, forming the theoretical bedrock of the exploration-exploitation tradeoff. This exercise challenges you to derive the Gittins index for a canonical Beta-Bernoulli bandit from first principles, building a deep understanding of its connection to optimal stopping theory and dynamic programming . Mastering this derivation illuminates how the value of an option is dynamically assessed against the promise of future information.",
            "id": "4148030",
            "problem": "Consider a single-armed Bayesian Bernoulli bandit that is activated at discrete times $t \\in \\{0,1,2,\\dots\\}$. When activated at time $t$, the arm yields a random reward $X_t \\in \\{0,1\\}$ distributed as $\\mathrm{Bernoulli}(\\theta)$, where the unknown success probability $\\theta \\in [0,1]$ has prior distribution $\\mathrm{Beta}(\\alpha,\\beta)$ with parameters $\\alpha>0$ and $\\beta>0$. The posterior over $\\theta$ after observing a success at time $t$ updates to $\\mathrm{Beta}(\\alpha+1,\\beta)$, and after a failure updates to $\\mathrm{Beta}(\\alpha,\\beta+1)$. Future rewards are geometrically discounted with a known factor $\\gamma \\in (0,1)$. Let $\\mathcal{F}_t$ denote the $\\sigma$-algebra generated by the rewards observed up to and including time $t$, and let $\\tau$ denote a stopping time with respect to the filtration $\\{\\mathcal{F}_t\\}_{t \\ge 0}$. You may assume the validity of the Gittins index theorem for geometrically discounted multi-armed bandits.\n\nStart from first principles consisting of Bayesian belief updating for the Beta-Bernoulli model and the definition of optimal stopping, and derive a rigorous expression for the Gittins index (GI) of this arm at belief state $\\mathrm{Beta}(\\alpha,\\beta)$ with discount factor $\\gamma$. Your derivation should proceed by formulating the optimal stopping problem that compares continuing to sample the arm versus stopping, transforming it into a dynamic program on belief states. Then, using a subsidy-for-continuation formulation, derive the Bellman recursion that defines the value function of the single arm under a per-step subsidy $\\lambda \\in \\mathbb{R}$, and characterize the Gittins index as the threshold subsidy that makes the arm just indifferent between continuing and stopping. Clearly state and justify any monotonicity or contraction properties you use to ensure existence and uniqueness of the index.\n\nFinally, as your answer, provide a single analytical expression that defines the Gittins index at belief state $\\mathrm{Beta}(\\alpha,\\beta)$ with discount factor $\\gamma$ as a supremum over stopping times of a ratio of two discounted expectations. No numerical evaluation is required, and no rounding is necessary. Do not include any units in your final answer.",
            "solution": "We begin by formalizing the single-arm problem with Bayesian learning and geometric discounting. The state is the belief $(\\alpha,\\beta)$ over the unknown success probability $\\theta$, with prior and posterior distributions in the conjugate family $\\mathrm{Beta}(\\alpha,\\beta)$. The arm, when activated, yields reward $X_t \\in \\{0,1\\}$ with conditional distribution $X_t \\mid \\theta \\sim \\mathrm{Bernoulli}(\\theta)$. The posterior updating rule is a well-known fact for the Beta-Bernoulli model: if the current belief is $\\mathrm{Beta}(\\alpha,\\beta)$, observing a success updates the belief to $\\mathrm{Beta}(\\alpha+1,\\beta)$ and a failure updates it to $\\mathrm{Beta}(\\alpha,\\beta+1)$. The posterior mean at belief $(\\alpha,\\beta)$ is $\\mu(\\alpha,\\beta) = \\alpha/(\\alpha+\\beta)$.\n\nThe Gittins index provides, for geometrically discounted multi-armed bandits, a decomposition of the optimal allocation policy into a per-arm index maximization policy. For a single arm, the Gittins index at a belief state is the value of an associated one-dimensional optimal stopping problem, which we now construct from first principles.\n\nDefine a stopping time $\\tau$ (with respect to the filtration $\\{\\mathcal{F}_t\\}_{t \\ge 0}$ generated by the observed rewards) as the time at which we stop sampling the arm. Under geometric discounting with factor $\\gamma \\in (0,1)$, and starting from belief $(\\alpha,\\beta)$, the expected discounted reward obtained by sampling the arm up to (but not including) time $\\tau$ is\n$$\nR(\\alpha,\\beta;\\tau) \\equiv \\mathbb{E}_{\\alpha,\\beta}\\!\\left[ \\sum_{t=0}^{\\tau-1} \\gamma^{t} X_t \\right],\n$$\nwhere $\\mathbb{E}_{\\alpha,\\beta}[\\cdot]$ denotes expectation with respect to the joint law induced by the prior $\\theta \\sim \\mathrm{Beta}(\\alpha,\\beta)$ and the conditional draws $X_t \\mid \\theta$ under the stopping rule $\\tau$. The discounted time accumulated up to $\\tau$ is\n$$\nT(\\tau) \\equiv \\mathbb{E}_{\\alpha,\\beta}\\!\\left[ \\sum_{t=0}^{\\tau-1} \\gamma^{t} \\right].\n$$\nBoth $R(\\alpha,\\beta;\\tau)$ and $T(\\tau)$ are finite because $\\gamma \\in (0,1)$ implies $\\sum_{t=0}^{\\infty} \\gamma^t < \\infty$ and the rewards are bounded in $[0,1]$.\n\nThe optimal stopping problem underlying the Gittins index takes the form of maximizing the ratio of expected discounted reward to expected discounted time over all admissible stopping times:\n$$\n\\mathcal{G}(\\alpha,\\beta;\\gamma) \\equiv \\sup_{\\tau \\in \\mathcal{T}} \\frac{R(\\alpha,\\beta;\\tau)}{T(\\tau)},\n$$\nwhere $\\mathcal{T}$ is the set of all stopping times with respect to $\\{\\mathcal{F}_t\\}$ taking values in $\\{1,2,\\dots\\}$. This ratio interpretation can be obtained by considering the Lagrangian or subsidy formulation, which we now derive to produce the dynamic programming recursion.\n\nIntroduce a per-period subsidy $\\lambda \\in \\mathbb{R}$ for continuing to sample the arm, and define the value function under subsidy $\\lambda$ as the maximal expected discounted net reward:\n$$\nV^{\\lambda}(\\alpha,\\beta) \\equiv \\sup_{\\tau \\in \\mathcal{T}} \\mathbb{E}_{\\alpha,\\beta}\\!\\left[ \\sum_{t=0}^{\\tau-1} \\gamma^{t} \\big( X_t - \\lambda \\big) \\right].\n$$\nThis is the value of an optimal stopping problem on the belief-state Markov chain induced by Beta-Bernoulli updating. The process on beliefs is a time-homogeneous Markov chain on the countable state space $\\{(\\alpha+i,\\beta+j): i,j \\in \\mathbb{N}_0\\}$, and geometric discounting with $\\gamma \\in (0,1)$ ensures that the Bellman equation for $V^{\\lambda}$ is a contraction mapping.\n\nBy the principle of optimality, at belief $(\\alpha,\\beta)$, the Bellman recursion is\n$$\nV^{\\lambda}(\\alpha,\\beta)\n=\n\\max \\Big\\{ 0,\\, \\underbrace{\\mu(\\alpha,\\beta) - \\lambda}_{\\text{net immediate reward}} + \\gamma \\Big[ \\mu(\\alpha,\\beta)\\, V^{\\lambda}(\\alpha+1,\\beta) + \\big(1-\\mu(\\alpha,\\beta)\\big)\\, V^{\\lambda}(\\alpha,\\beta+1) \\Big] \\Big\\},\n$$\nwith $\\mu(\\alpha,\\beta) = \\alpha/(\\alpha+\\beta)$. The two terms inside the maximum correspond to stopping immediately (yielding $0$ continuation value because we define the post-stop value to be $0$) and continuing for one more play (collecting the expected immediate net reward $\\mu(\\alpha,\\beta)-\\lambda$ plus the discounted expected continuation value, where the next belief is $(\\alpha+1,\\beta)$ with probability $\\mu(\\alpha,\\beta)$ and $(\\alpha,\\beta+1)$ with probability $1-\\mu(\\alpha,\\beta)$).\n\nStandard arguments for discounted optimal stopping in a bounded-reward, time-homogeneous Markov decision process (MDP) imply that for each fixed $\\lambda$, the operator\n$$\n\\mathcal{T}^{\\lambda}[f](\\alpha,\\beta)\n=\n\\max \\Big\\{ 0,\\, \\mu(\\alpha,\\beta) - \\lambda + \\gamma \\big[ \\mu(\\alpha,\\beta)\\, f(\\alpha+1,\\beta) + \\big(1-\\mu(\\alpha,\\beta)\\big)\\, f(\\alpha,\\beta+1) \\big] \\Big\\}\n$$\nis a $\\gamma$-contraction on the space of bounded functions equipped with the sup norm. Hence, the Bellman equation has a unique bounded fixed point $V^{\\lambda}$, and value iteration converges to it.\n\nWe now connect $V^{\\lambda}$ to the ratio characterization. For any stopping time $\\tau$,\n$$\n\\mathbb{E}_{\\alpha,\\beta}\\!\\left[ \\sum_{t=0}^{\\tau-1} \\gamma^{t} \\big( X_t - \\lambda \\big) \\right]\n=\nR(\\alpha,\\beta;\\tau) - \\lambda\\, T(\\tau).\n$$\nBy definition of $V^{\\lambda}$, we have $V^{\\lambda}(\\alpha,\\beta) \\ge R(\\alpha,\\beta;\\tau) - \\lambda\\, T(\\tau)$ for all $\\tau$, hence\n$$\n\\lambda \\ge \\frac{R(\\alpha,\\beta;\\tau) - V^{\\lambda}(\\alpha,\\beta)}{T(\\tau)}.\n$$\nWhen $V^{\\lambda}(\\alpha,\\beta) = 0$, this yields\n$$\n\\lambda \\ge \\frac{R(\\alpha,\\beta;\\tau)}{T(\\tau)} \\quad \\text{for all } \\tau \\in \\mathcal{T}.\n$$\nIt follows that if $V^{\\lambda}(\\alpha,\\beta) = 0$, then $\\lambda$ is an upper bound on the ratio $R(\\alpha,\\beta;\\tau)/T(\\tau)$ over all stopping times, so $\\lambda \\ge \\sup_{\\tau \\in \\mathcal{T}} R(\\alpha,\\beta;\\tau)/T(\\tau)$. Conversely, suppose $\\lambda < \\sup_{\\tau \\in \\mathcal{T}} R(\\alpha,\\beta;\\tau)/T(\\tau)$. Then there exists a stopping time $\\tau^{\\star}$ such that $R(\\alpha,\\beta;\\tau^{\\star}) - \\lambda\\, T(\\tau^{\\star}) > 0$, and hence $V^{\\lambda}(\\alpha,\\beta) \\ge R(\\alpha,\\beta;\\tau^{\\star}) - \\lambda\\, T(\\tau^{\\star}) > 0$. Therefore, the set $\\{\\lambda \\in \\mathbb{R} : V^{\\lambda}(\\alpha,\\beta) > 0\\}$ is the open interval $(-\\infty, \\mathcal{G}(\\alpha,\\beta;\\gamma))$, and $V^{\\lambda}(\\alpha,\\beta) = 0$ for all $\\lambda \\ge \\mathcal{G}(\\alpha,\\beta;\\gamma)$.\n\nMonotonicity of $V^{\\lambda}$ in $\\lambda$ follows from the Bellman operator: if $\\lambda_1 < \\lambda_2$, then $\\mathcal{T}^{\\lambda_1}[f](\\alpha,\\beta) \\ge \\mathcal{T}^{\\lambda_2}[f](\\alpha,\\beta)$ pointwise for any $f$, and the unique fixed points satisfy $V^{\\lambda_1}(\\alpha,\\beta) \\ge V^{\\lambda_2}(\\alpha,\\beta)$. Together with boundedness and continuity in $\\lambda$, these facts imply the existence and uniqueness of a threshold $\\lambda^{\\star}$ such that $V^{\\lambda}(\\alpha,\\beta) > 0$ for $\\lambda < \\lambda^{\\star}$ and $V^{\\lambda}(\\alpha,\\beta) = 0$ for $\\lambda \\ge \\lambda^{\\star}$. This threshold is precisely the Gittins index, and it coincides with the ratio supremum:\n$$\n\\mathcal{G}(\\alpha,\\beta;\\gamma) = \\sup_{\\tau \\in \\mathcal{T}} \\frac{R(\\alpha,\\beta;\\tau)}{T(\\tau)}.\n$$\n\nFor computation via dynamic programming on belief states, one can fix a candidate subsidy $\\lambda$ and compute $V^{\\lambda}$ by value iteration:\n$$\nV^{\\lambda}_{k+1}(\\alpha,\\beta) \\leftarrow \\max \\Big\\{ 0,\\, \\mu(\\alpha,\\beta) - \\lambda + \\gamma \\big[ \\mu(\\alpha,\\beta)\\, V^{\\lambda}_{k}(\\alpha+1,\\beta) + \\big(1-\\mu(\\alpha,\\beta)\\big)\\, V^{\\lambda}_{k}(\\alpha,\\beta+1) \\big] \\Big\\},\n$$\ninitialized with $V^{\\lambda}_{0}(\\alpha,\\beta) \\equiv 0$ for all $(\\alpha,\\beta)$, which converges uniformly to $V^{\\lambda}$. By monotonicity in $\\lambda$ and the bounds $0 \\le \\mathcal{G}(\\alpha,\\beta;\\gamma) \\le 1$ (since rewards lie in $[0,1]$), a simple bisection search on $\\lambda \\in [0,1]$ yields the unique $\\lambda^{\\star}$ satisfying $V^{\\lambda^{\\star}}(\\alpha,\\beta) = 0$, which is the Gittins index at belief $(\\alpha,\\beta)$.\n\nSummarizing, the Gittins index at belief $\\mathrm{Beta}(\\alpha,\\beta)$ with discount factor $\\gamma$ is the supremum, over all stopping times adapted to the observed rewards, of the ratio of expected discounted reward to expected discounted time:\n$$\n\\mathcal{G}(\\alpha,\\beta;\\gamma) = \\sup_{\\tau \\in \\mathcal{T}} \\frac{ \\mathbb{E}_{\\alpha,\\beta}\\!\\left[ \\sum_{t=0}^{\\tau-1} \\gamma^{t} X_t \\right] }{ \\mathbb{E}_{\\alpha,\\beta}\\!\\left[ \\sum_{t=0}^{\\tau-1} \\gamma^{t} \\right] }.\n$$\nThe dynamic programming recursion that enables computation is given by the subsidy-form Bellman equation\n$$\nV^{\\lambda}(\\alpha,\\beta) = \\max \\Big\\{ 0,\\, \\frac{\\alpha}{\\alpha+\\beta} - \\lambda + \\gamma \\Big[ \\frac{\\alpha}{\\alpha+\\beta}\\, V^{\\lambda}(\\alpha+1,\\beta) + \\frac{\\beta}{\\alpha+\\beta}\\, V^{\\lambda}(\\alpha,\\beta+1) \\Big] \\Big\\},\n$$\nand the Gittins index is\n$$\n\\mathcal{G}(\\alpha,\\beta;\\gamma) = \\inf \\big\\{ \\lambda \\in \\mathbb{R} : V^{\\lambda}(\\alpha,\\beta) = 0 \\big\\} = \\sup \\big\\{ \\lambda \\in \\mathbb{R} : V^{\\lambda}(\\alpha,\\beta) > 0 \\big\\}.\n$$\nAny of these equivalent characterizations defines the same unique real value $\\mathcal{G}(\\alpha,\\beta;\\gamma)$.",
            "answer": "$$\\boxed{\\displaystyle \\mathcal{G}(\\alpha,\\beta;\\gamma)=\\sup_{\\tau \\in \\mathcal{T}}\\,\\frac{\\mathbb{E}_{\\alpha,\\beta}\\!\\left[\\sum_{t=0}^{\\tau-1}\\gamma^{t} X_t\\right]}{\\mathbb{E}_{\\alpha,\\beta}\\!\\left[\\sum_{t=0}^{\\tau-1}\\gamma^{t}\\right]}}$$"
        },
        {
            "introduction": "While the Gittins index offers a powerful theoretical tool, its real value is revealed when contrasted with simpler, more intuitive strategies. This practice problem makes the abstract concept of the Gittins index concrete by asking you to quantify the \"value of information\" in a simple, revealing scenario . By calculating the expected loss from a myopic (purely exploitative) policy, you will gain a tangible appreciation for why optimal exploration is crucial for long-term reward maximization.",
            "id": "4148037",
            "problem": "Consider a two-armed decision problem framed as a Multi-Armed Bandit (MAB) with geometric discounting, modeling the exploration versus exploitation tradeoff in complex adaptive systems. Time is discrete, and future rewards are discounted by a factor $\\gamma \\in (0,1)$. At each time step, you may select one of two arms:\n- Arm B yields a known constant reward $b$ per play, forever.\n- Arm A has an unknown type. With prior probability $p$ it is High-type and yields $h$ per play forever; with prior probability $1-p$ it is Low-type and yields $\\ell$ per play forever. Pulling arm A once perfectly reveals its type for all subsequent times (because the realized reward distinguishes the types), after which you may continue with arm A if beneficial or stop and switch to arm B.\n\nUse the foundational principle of discounted optimal stopping to define the Gittins index of an arm as the reservation value $c^{\\ast}$ of a constant alternative stream that makes the decision-maker indifferent between beginning with the arm and choosing the constant stream immediately, given the option to stop and switch thereafter. Starting from this principle and the geometric series for discounted sums, derive the reservation-value expression for the Gittins index of arm A for the one-step revealing model described.\n\nThen, specialize to the parameter values $\\gamma = 0.9$, $p = 0.3$, $h = 10$, $\\ell = 0$, and $b = 4$. Compute:\n1. The Gittins index $c^{\\ast}$ for arm A and compare it to the myopic expected immediate reward $p h + (1-p)\\ell$.\n2. The expected discounted loss incurred by the myopic exploitation policy that chooses arm B immediately and forever, rather than the optimal index policy that explores arm A once and then continues optimally.\n\nExpress the final loss as a single real number. No rounding is required.",
            "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n**Step 1: Extract Givens**\n- Two-armed bandit problem with discrete time.\n- Discount factor: $\\gamma \\in (0,1)$.\n- Arm $B$: yields a known constant reward $b$ per play.\n- Arm $A$: unknown type.\n  - High-type: reward $h$ per play, prior probability $p$.\n  - Low-type: reward $\\ell$ per play, prior probability $1-p$.\n- Pulling arm $A$ once reveals its type.\n- The Gittins index is defined as the reservation value $c^{\\ast}$ of a constant alternative that makes the agent indifferent between exploring the arm for one step and taking the alternative immediately.\n- Specific parameter values: $\\gamma = 0.9$, $p = 0.3$, $h = 10$, $\\ell = 0$, $b = 4$.\n- Required computations:\n  1. The Gittins index $c^{\\ast}$ for arm $A$ and its comparison to the myopic expected reward.\n  2. The expected discounted loss of a myopic policy (always choosing arm $B$) versus the optimal index policy.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is a standard, well-defined formulation of a Bayesian multi-armed bandit problem, a core topic in the study of complex adaptive systems and reinforcement learning. The Gittins index is the canonical solution for such discounted problems. The problem is self-contained, with all variables and conditions clearly specified. There are no scientific or logical contradictions, vagueness, or subjective elements. The problem is therefore valid.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A full solution will be provided.\n\n**Derivation of the Gittins Index for Arm $A$**\n\nThe Gittins index, $c^{\\ast}$, for arm $A$ is defined as the constant reward value of a hypothetical alternative arm that makes the agent indifferent between two initial choices:\n1. Choose the hypothetical arm, receiving reward $c^{\\ast}$ forever. The total discounted value is $V_{C} = \\sum_{t=0}^{\\infty} \\gamma^t c^{\\ast} = \\frac{c^{\\ast}}{1-\\gamma}$.\n2. Choose arm $A$ for one time step, observe the outcome, and then proceed optimally. The optimal subsequent action is to choose the arm with the higher perpetual reward stream: the revealed value of arm $A$ or the reservation value $c^{\\ast}$.\n\nThe value of choosing arm $A$ for one step, $V_{A}$, is the sum of the expected immediate reward and the discounted expected future value.\nThe expected immediate reward from pulling arm $A$ at time $t=0$ is $E[R_0] = ph + (1-p)\\ell$.\n\nAfter this pull, the type of arm $A$ is known.\n- With probability $p$, arm $A$ is High-type (reward $h$). The agent will then compare $h$ and $c^{\\ast}$. The optimal choice yields a reward of $\\max(h, c^{\\ast})$ forever. The discounted value of this future stream, as of time $t=1$, is $\\frac{\\max(h, c^{\\ast})}{1-\\gamma}$.\n- With probability $1-p$, arm $A$ is Low-type (reward $\\ell$). The agent will then compare $\\ell$ and $c^{\\ast}$. The optimal choice yields a reward of $\\max(\\ell, c^{\\ast})$ forever. The discounted value of this future stream, as of time $t=1$, is $\\frac{\\max(\\ell, c^{\\ast})}{1-\\gamma}$.\n\nThe total expected discounted value of strategy 2 is:\n$$V_{A} = (ph + (1-p)\\ell) + \\gamma \\left[ p \\frac{\\max(h, c^{\\ast})}{1-\\gamma} + (1-p) \\frac{\\max(\\ell, c^{\\ast})}{1-\\gamma} \\right]$$\nAt the point of indifference, $V_{A} = V_{C}$:\n$$\\frac{c^{\\ast}}{1-\\gamma} = (ph + (1-p)\\ell) + \\frac{\\gamma}{1-\\gamma} \\left[ p \\max(h, c^{\\ast}) + (1-p) \\max(\\ell, c^{\\ast}) \\right]$$\nMultiplying by $(1-\\gamma)$ gives the fundamental equation for the Gittins index:\n$$c^{\\ast} = (1-\\gamma)(ph + (1-p)\\ell) + \\gamma \\left[ p \\max(h, c^{\\ast}) + (1-p) \\max(\\ell, c^{\\ast}) \\right]$$\nTo solve for $c^{\\ast}$, we assume a non-trivial case where $\\ell < c^{\\ast} < h$. This is reasonable because if $c^{\\ast}$ were outside this range, the decision to switch after learning would be foregone (e.g., if $c^{\\ast} \\le \\ell$, one would never switch to the alternative). With this assumption, $\\max(h, c^{\\ast}) = h$ and $\\max(\\ell, c^{\\ast}) = c^{\\ast}$.\nSubstituting these into the equation:\n$$c^{\\ast} = (1-\\gamma)(ph + (1-p)\\ell) + \\gamma \\left[ ph + (1-p)c^{\\ast} \\right]$$\nNow, we solve for $c^{\\ast}$:\n$$c^{\\ast} = ph + (1-p)\\ell - \\gamma ph - \\gamma(1-p)\\ell + \\gamma ph + \\gamma(1-p)c^{\\ast}$$\n$$c^{\\ast} = ph + (1-p)\\ell - \\gamma(1-p)\\ell + \\gamma(1-p)c^{\\ast}$$\n$$c^{\\ast} - \\gamma(1-p)c^{\\ast} = ph + (1-p)(1-\\gamma)\\ell$$\n$$c^{\\ast}(1 - \\gamma(1-p)) = ph + (1-p)(1-\\gamma)\\ell$$\n$$c^{\\ast} = \\frac{ph + (1-p)(1-\\gamma)\\ell}{1 - \\gamma + p\\gamma}$$\nThis is the reservation-value expression for the Gittins index of arm $A$.\n\n**Computations for Specific Parameter Values**\n\nThe given parameters are $\\gamma = 0.9$, $p = 0.3$, $h = 10$, $\\ell = 0$, and $b = 4$.\n\n1. **Gittins Index and Myopic Reward Comparison**\nFirst, we compute the Gittins index $c^{\\ast}$ for arm $A$:\n$$c^{\\ast} = \\frac{(0.3)(10) + (1-0.3)(1-0.9)(0)}{1 - 0.9 + (0.3)(0.9)} = \\frac{3 + 0}{1 - 0.9 + 0.27} = \\frac{3}{0.1 + 0.27} = \\frac{3}{0.37} = \\frac{300}{37}$$\nThe value of the Gittins index is $c^{\\ast} = \\frac{300}{37} \\approx 8.108$.\nThe myopic expected immediate reward is:\n$$E_{myopic} = ph + (1-p)\\ell = (0.3)(10) + (0.7)(0) = 3$$\nComparing the two, $c^{\\ast} = \\frac{300}{37} > 3$. The difference, $c^{\\ast} - E_{myopic}$, represents the \"value of information\" or the option value inherent in being able to switch away from arm $A$ if it turns out to be a Low-type.\n\n2. **Expected Discounted Loss of Myopic Policy**\nThe optimal policy is dictated by the Gittins index rule: at any stage, play the arm with the highest index.\nThe index for arm $A$ is $c^{\\ast} = \\frac{300}{37}$.\nThe index for arm $B$ is its constant reward, $b = 4$.\nSince $c^{\\ast} = \\frac{300}{37} \\approx 8.108 > 4 = b$, the optimal policy is to explore arm $A$ first.\n\nThe problem defines the \"myopic exploitation policy\" as choosing arm $B$ immediately and forever. Let's calculate the expected total discounted value of this policy, $V_{myopic}$.\n$$V_{myopic} = \\sum_{t=0}^{\\infty} \\gamma^t b = \\frac{b}{1-\\gamma} = \\frac{4}{1-0.9} = \\frac{4}{0.1} = 40$$\n\nNow, we calculate the expected total discounted value of the optimal policy, $V_{optimal}$. This policy starts with pulling arm $A$.\n- The immediate reward is $ph + (1-p)\\ell = 3$.\n- After the pull, we re-evaluate.\n  - With probability $p=0.3$, arm $A$ is High-type ($h=10$). Since $h > b$, we continue with arm $A$. The future discounted value from $t=1$ is $\\frac{h}{1-\\gamma} = \\frac{10}{0.1} = 100$.\n  - With probability $1-p=0.7$, arm $A$ is Low-type ($\\ell=0$). Since $\\ell < b$, we switch to arm $B$. The future discounted value from $t=1$ is $\\frac{b}{1-\\gamma} = \\frac{4}{0.1} = 40$.\n\nThe total expected value for the optimal policy is:\n$$V_{optimal} = (ph + (1-p)\\ell) + \\gamma \\left[ p \\left( \\frac{h}{1-\\gamma} \\right) + (1-p) \\left( \\frac{b}{1-\\gamma} \\right) \\right]$$\n$$V_{optimal} = 3 + 0.9 \\left[ (0.3)(100) + (0.7)(40) \\right]$$\n$$V_{optimal} = 3 + 0.9 \\left[ 30 + 28 \\right]$$\n$$V_{optimal} = 3 + 0.9(58)$$\n$$V_{optimal} = 3 + 52.2 = 55.2$$\n\nThe expected discounted loss incurred by the myopic policy is the difference in value between the optimal policy and the myopic policy.\n$$\\text{Loss} = V_{optimal} - V_{myopic} = 55.2 - 40 = 15.2$$\nThis loss represents the expected value forgone by failing to explore the potentially very rewarding arm $A$. In fractional form, the loss is $15.2 = \\frac{152}{10} = \\frac{76}{5}$.",
            "answer": "$$\n\\boxed{15.2}\n$$"
        },
        {
            "introduction": "In many real-world scenarios, computing the optimal Gittins index is intractable, necessitating the use of effective heuristic algorithms like the Upper Confidence Bound (UCB). This exercise explores the design philosophy behind UCB by contrasting it with a naive \"entropy-seeking\" policy that prioritizes only uncertainty reduction. By analyzing a scenario where pure curiosity leads to suboptimal decisions , you will understand how UCB intelligently balances the pursuit of information with the drive for immediate reward.",
            "id": "4147968",
            "problem": "Consider a two-armed Bernoulli multi-armed bandit modeling a subsystem of a complex adaptive system. Arm $1$ and Arm $2$ produce independent Bernoulli rewards with unknown success probabilities $\\theta_1$ and $\\theta_2$. After an initial data-collection phase, the empirically observed sample means are $\\hat{p}_1 = 0.9$ and $\\hat{p}_2 = 0.5$, based on $n_1 = 100$ and $n_2 = 100$ observations, respectively. For the purpose of evaluating expected regret at the next decision, assume the environment is stationary and the true expected rewards equal the observed sample means, that is, $\\mu_1 = \\theta_1 = 0.9$ and $\\mu_2 = \\theta_2 = 0.5$.\n\nAt time $t = 200$, two decision policies are considered for selecting the next arm to pull:\n\n1. An entropy-seeking policy that selects the arm $i$ with maximum predictive outcome entropy $H_i$, where $H_i = - \\hat{p}_i \\ln \\hat{p}_i - (1 - \\hat{p}_i) \\ln(1 - \\hat{p}_i)$ (using natural logarithms, so entropy is in nats).\n\n2. An Upper Confidence Bound (UCB) policy that selects the arm $i$ maximizing the index\n$$\n\\mathrm{UCB}_i(t) = \\hat{p}_i + \\sqrt{\\frac{2 \\ln t}{n_i}}.\n$$\n\nDefine the one-step expected regret of choosing arm $i$ as $r_i = \\mu^\\star - \\mu_i$, where $\\mu^\\star = \\max\\{\\mu_1, \\mu_2\\}$. Let the entropy-seeking policy choose arm $i_{\\mathrm{E}}$ and the UCB policy choose arm $i_{\\mathrm{U}}$ at time $t=200$.\n\nDefine the regretâ€“information tradeoff ratio\n$$\n\\mathcal{T} = \\frac{r_{i_{\\mathrm{E}}} - r_{i_{\\mathrm{U}}}}{H_{i_{\\mathrm{E}}} - H_{i_{\\mathrm{U}}}},\n$$\nthat is, the excess expected one-step regret incurred by the entropy-seeking policy relative to the UCB policy, divided by the excess predictive outcome entropy of the observation it acquires.\n\nCompute the numerical value of $\\mathcal{T}$ for the given scenario. Use natural logarithms throughout. Round your final answer to four significant figures.",
            "solution": "The problem is first validated and found to be well-posed, scientifically grounded, and internally consistent. We can proceed with a quantitative solution.\n\nThe problem asks for the computation of a regret-information tradeoff ratio, $\\mathcal{T}$, for a specific scenario in a two-armed bandit problem. The steps to solve this are as follows: first, determine the optimal arm and the one-step expected regret for choosing each arm; second, determine the arm selection for the entropy-seeking policy and the UCB policy; third, use these selections to compute the value of $\\mathcal{T}$.\n\nFirst, we identify the true expected rewards for each arm, which are given as $\\mu_1 = 0.9$ and $\\mu_2 = 0.5$. The optimal arm is the one with the highest true expected reward. Thus, the optimal reward is $\\mu^\\star = \\max\\{\\mu_1, \\mu_2\\} = \\max\\{0.9, 0.5\\} = 0.9$. Arm $1$ is the optimal arm.\n\nThe one-step expected regret of choosing an arm $i$ is defined as $r_i = \\mu^\\star - \\mu_i$.\nFor arm $1$, the regret is $r_1 = \\mu^\\star - \\mu_1 = 0.9 - 0.9 = 0$.\nFor arm $2$, the regret is $r_2 = \\mu^\\star - \\mu_2 = 0.9 - 0.5 = 0.4$.\n\nNext, we determine the arm selected by the entropy-seeking policy, $i_{\\mathrm{E}}$. This policy selects the arm $i$ that maximizes the predictive outcome entropy $H_i = - \\hat{p}_i \\ln \\hat{p}_i - (1 - \\hat{p}_i) \\ln(1 - \\hat{p}_i)$. The sample means are given as $\\hat{p}_1 = 0.9$ and $\\hat{p}_2 = 0.5$.\n\nFor arm $1$, the entropy is:\n$$ H_1 = -0.9 \\ln(0.9) - (1-0.9) \\ln(1-0.9) = -0.9 \\ln(0.9) - 0.1 \\ln(0.1) $$\nFor arm $2$, the entropy is:\n$$ H_2 = -0.5 \\ln(0.5) - (1-0.5) \\ln(1-0.5) = -0.5 \\ln(0.5) - 0.5 \\ln(0.5) = -\\ln(0.5) = \\ln(2) $$\nThe function $f(p) = -p\\ln p - (1-p)\\ln p$ represents the entropy of a Bernoulli trial. This function is maximized at $p=0.5$. Since $\\hat{p}_2 = 0.5$, $H_2$ has the maximum possible value for a Bernoulli random variable. Since $\\hat{p}_1 = 0.9 \\neq 0.5$, we must have $H_1 < H_2$.\nNumerically, $H_2 = \\ln(2) \\approx 0.6931$ nats, and $H_1 \\approx -0.9(-0.1054) - 0.1(-2.3026) = 0.0949 + 0.2303 = 0.3252$ nats.\nSince $H_2 > H_1$, the entropy-seeking policy chooses arm $2$. Therefore, $i_{\\mathrm{E}} = 2$.\n\nNow, we determine the arm selected by the Upper Confidence Bound (UCB) policy, $i_{\\mathrm{U}}$. This policy selects the arm maximizing the index $\\mathrm{UCB}_i(t) = \\hat{p}_i + \\sqrt{\\frac{2 \\ln t}{n_i}}$. The decision is made at time $t = 200$. The number of pulls for each arm is $n_1 = 100$ and $n_2 = 100$.\n\nFor arm $1$:\n$$ \\mathrm{UCB}_1(200) = \\hat{p}_1 + \\sqrt{\\frac{2 \\ln(200)}{n_1}} = 0.9 + \\sqrt{\\frac{2 \\ln(200)}{100}} = 0.9 + \\sqrt{\\frac{\\ln(200)}{50}} $$\nFor arm $2$:\n$$ \\mathrm{UCB}_2(200) = \\hat{p}_2 + \\sqrt{\\frac{2 \\ln(200)}{n_2}} = 0.5 + \\sqrt{\\frac{2 \\ln(200)}{100}} = 0.5 + \\sqrt{\\frac{\\ln(200)}{50}} $$\nThe exploration term, $\\sqrt{\\frac{\\ln(200)}{50}}$, is identical for both arms since $n_1 = n_2$. The decision is therefore determined by the exploitation term, $\\hat{p}_i$. Since $\\hat{p}_1 = 0.9 > \\hat{p}_2 = 0.5$, it follows that $\\mathrm{UCB}_1(200) > \\mathrm{UCB}_2(200)$.\nThe UCB policy chooses arm $1$. Therefore, $i_{\\mathrm{U}} = 1$.\n\nFinally, we compute the regret-information tradeoff ratio, $\\mathcal{T}$. The definition is:\n$$ \\mathcal{T} = \\frac{r_{i_{\\mathrm{E}}} - r_{i_{\\mathrm{U}}}}{H_{i_{\\mathrm{E}}} - H_{i_{\\mathrm{U}}}} $$\nWe substitute the values we have found:\n$i_{\\mathrm{E}} = 2$, so $r_{i_{\\mathrm{E}}} = r_2 = 0.4$ and $H_{i_{\\mathrm{E}}} = H_2 = \\ln(2)$.\n$i_{\\mathrm{U}} = 1$, so $r_{i_{\\mathrm{U}}} = r_1 = 0$ and $H_{i_{\\mathrm{U}}} = H_1 = -0.9 \\ln(0.9) - 0.1 \\ln(0.1)$.\n\nSubstituting these into the expression for $\\mathcal{T}$:\n$$ \\mathcal{T} = \\frac{0.4 - 0}{\\ln(2) - (-0.9 \\ln(0.9) - 0.1 \\ln(0.1))} = \\frac{0.4}{\\ln(2) + 0.9 \\ln(0.9) + 0.1 \\ln(0.1)} $$\nWe now compute the numerical value.\nUsing the natural logarithm values:\n$\\ln(2) \\approx 0.693147$\n$\\ln(0.9) \\approx -0.105361$\n$\\ln(0.1) \\approx -2.302585$\n\nThe denominator is:\n$H_2 - H_1 \\approx 0.693147 + 0.9(-0.105361) + 0.1(-2.302585)$\n$H_2 - H_1 \\approx 0.693147 - 0.094825 - 0.230259 = 0.368063$\n\nSo, the ratio is:\n$$ \\mathcal{T} \\approx \\frac{0.4}{0.368063} \\approx 1.086766 $$\nRounding to four significant figures, we get $1.087$.\nThe quantity $\\mathcal{T}$ represents the \"price\" of information, measured in units of expected regret per unit of entropy (in nats). In this specific instance, the entropy-seeking (exploration) policy incurs an excess regret of $0.4$ relative to the UCB policy, in order to gain an excess of approximately $0.368$ nats of information, resulting in the calculated ratio.",
            "answer": "$$\n\\boxed{1.087}\n$$"
        }
    ]
}