{
    "hands_on_practices": [
        {
            "introduction": "Our first practice grounds us in the most fundamental task of differential privacy: protecting a single data release. This exercise guides you through calculating the sensitivity of a common time-series query—a rolling window sum—and using it to calibrate the Laplace mechanism for $\\epsilon$-differential privacy . By working through this problem, you will build a core intuition for the relationship between data bounds, query sensitivity, and the direct trade-off between the privacy parameter $\\epsilon$ and output utility.",
            "id": "4213204",
            "problem": "A digital twin (DT) of a cyber-physical system (CPS) maintains a time series of scalar sensor readings $\\{x_t\\}_{t=1}^{T}$, each bounded in the interval $[0,1]$. At each time index $t \\geq w$, the analytics pipeline releases a rolling window sum defined by $f_t(D) = \\sum_{i=t-w+1}^{t} x_i$, where $w \\in \\mathbb{N}$ is the fixed window length and $D$ denotes the dataset comprising the time series. The DT requires an event-level privacy guarantee, where two datasets $D$ and $D'$ are adjacent if they differ in exactly one index $k$ by an arbitrary change within $[0,1]$. For each release $t$, the mechanism outputs $\\tilde{f}_t = f_t(D) + Z_t$ with $Z_t$ drawn independently from a zero-mean Laplace distribution with unknown scale $b > 0$. The requirement is that each single release at time $t$ must satisfy $\\epsilon$-differential privacy for a fixed $\\epsilon > 0$.\n\nStarting from the definition of $\\epsilon$-differential privacy and the properties of the Laplace distribution, derive the appropriate scale $b$ that ensures the release at time $t$ is $\\epsilon$-differentially private under event-level adjacency. Then, compute the expected mean squared error per release $\\mathbb{E}\\!\\left[(\\tilde{f}_t - f_t(D))^2\\right]$ as a function of $\\epsilon$. Express the final answer as a single row matrix containing the calibrated $b$ and the expected mean squared error per release, in that order. No rounding is required and no units should be included in the final expression.",
            "solution": "The core principle for calibrating the noise in the Laplace mechanism is to match the scale of the noise to the sensitivity of the query function. We will first derive the sensitivity, then use it to find the noise scale $b$, and finally compute the resulting mean squared error.\n\nA randomized mechanism $\\mathcal{M}$ satisfies $\\epsilon$-differential privacy if for any two adjacent datasets $D$ and $D'$, and for any set of possible outputs $S$, the following inequality holds:\n$$\n\\text{Pr}[\\mathcal{M}(D) \\in S] \\leq \\exp(\\epsilon) \\cdot \\text{Pr}[\\mathcal{M}(D') \\in S]\n$$\nFor a real-valued function $f$, the Laplace mechanism, which outputs $\\mathcal{M}(D) = f(D) + Z$ where $Z$ is drawn from a Laplace distribution with mean $0$ and scale $b$, is known to satisfy $\\epsilon$-differential privacy if the scale $b$ is set as:\n$$\nb = \\frac{\\Delta f}{\\epsilon}\n$$\nHere, $\\Delta f$ is the $L_1$-sensitivity of the function $f$, defined as the maximum possible change in the function's output for any pair of adjacent datasets:\n$$\n\\Delta f = \\sup_{D, D' \\text{ adjacent}} \\|f(D) - f(D')\\|_1\n$$\nIn our case, the function $f_t(D)$ is scalar-valued, so the $L_1$-norm is simply the absolute value. The sensitivity of the rolling window sum $f_t$ is:\n$$\n\\Delta f_t = \\sup_{D, D' \\text{ adjacent}} |f_t(D) - f_t(D')|\n$$\nLet $D = \\{x_1, \\dots, x_k, \\dots, x_T\\}$ and $D' = \\{x_1, \\dots, x_k', \\dots, x_T\\}$ be two adjacent datasets, where $x_k, x_k' \\in [0,1]$ and $x_i = x_i'$ for all $i \\neq k$. The difference in the function output is:\n$$\nf_t(D) - f_t(D') = \\left(\\sum_{i=t-w+1}^{t} x_i\\right) - \\left(\\sum_{i=t-w+1}^{t} x'_i\\right)\n$$\nSince $x_i = x'_i$ for all indices except $k$, this sum simplifies.\nThere are two cases for the index $k$:\n1.  The changed index $k$ is outside the query window: $k < t-w+1$ or $k > t$. In this case, for all $i \\in [t-w+1, t]$, we have $x_i = x'_i$. Therefore, $f_t(D) - f_t(D') = 0$.\n2.  The changed index $k$ is inside the query window: $t-w+1 \\leq k \\leq t$. In this case, the sums cancel for all terms where $i \\neq k$, leaving only the term for index $k$.\n    $$\n    f_t(D) - f_t(D') = x_k - x'_k\n    $$\nThe sensitivity $\\Delta f_t$ is the supremum of the absolute value of this difference over all possible adjacent datasets. This corresponds to the maximum possible value of $|x_k - x'_k|$ for $x_k, x_k' \\in [0,1]$, which occurs when one value is $0$ and the other is $1$.\n$$\n\\Delta f_t = \\sup_{x_k, x'_k \\in [0,1]} |x_k - x'_k| = |1 - 0| = 1\n$$\nTherefore, the $L_1$-sensitivity of the rolling sum query $f_t$ under the given event-level adjacency is $1$.\n\nNow, we can determine the required scale $b$ for the Laplace noise to ensure $\\epsilon$-differential privacy.\n$$\nb = \\frac{\\Delta f_t}{\\epsilon} = \\frac{1}{\\epsilon}\n$$\nThis is the first part of the answer.\n\nNext, we compute the expected mean squared error (MSE) per release. The error at time $t$ is the difference between the noisy output $\\tilde{f}_t$ and the true value $f_t(D)$.\n$$\n\\text{Error}_t = \\tilde{f}_t - f_t(D) = (f_t(D) + Z_t) - f_t(D) = Z_t\n$$\nThe mean squared error is the expected value of the squared error:\n$$\n\\text{MSE} = \\mathbb{E}[(\\text{Error}_t)^2] = \\mathbb{E}[Z_t^2]\n$$\nThe random variable $Z_t$ is drawn from a Laplace distribution with mean $\\mu = 0$ and scale $b$. The variance of a random variable $X$ is given by $\\text{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$. For $Z_t$, we have $\\mathbb{E}[Z_t] = \\mu = 0$.\nThus, the MSE is equal to the variance of the noise variable $Z_t$:\n$$\n\\text{MSE} = \\mathbb{E}[Z_t^2] - (\\mathbb{E}[Z_t])^2 = \\mathbb{E}[Z_t^2] - 0^2 = \\text{Var}(Z_t)\n$$\nThe variance of a Laplace distribution with scale parameter $b$ is known to be $2b^2$.\n$$\n\\text{Var}(Z_t) = 2b^2\n$$\nSubstituting the expression for $b$ that we derived:\n$$\n\\text{MSE} = 2 \\left(\\frac{1}{\\epsilon}\\right)^2 = \\frac{2}{\\epsilon^2}\n$$\nThis is the second part of the answer.\n\nThe problem requires the answer to be a single row matrix containing the calibrated $b$ and the expected mean squared error. The two values are $b = \\frac{1}{\\epsilon}$ and $\\text{MSE} = \\frac{2}{\\epsilon^2}$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{\\epsilon} & \\frac{2}{\\epsilon^2} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "While the Laplace mechanism provides the strong guarantee of pure $\\epsilon$-DP, we can sometimes achieve better utility by relaxing this guarantee to $(\\epsilon, \\delta)$-DP. This practice introduces the Gaussian mechanism and asks you to directly compare its performance against the Laplace mechanism for a windowed mean query . This comparative analysis is a crucial skill, highlighting the practical considerations involved when choosing the right privacy-preserving tool for a given application.",
            "id": "4213327",
            "problem": "A cyber-physical system (CPS) digital twin monitors a normalized scalar time-series signal with each sample clipped to lie in the interval $[-B, B]$. At each time step, the twin releases the mean over a sliding window of length $W$ as a privacy-preserving analytic for downstream controllers. Consider a single such windowed mean release, and assume event-level adjacency where two datasets are adjacent if they differ in exactly one sample within the window. The system designer must choose between two mechanisms: a Laplace mechanism calibrated to satisfy pure Differential Privacy (DP) with privacy parameter $\\epsilon$, and a Gaussian mechanism calibrated to satisfy approximate Differential Privacy $(\\epsilon, \\delta)$ for the same $\\epsilon$ and a given $\\delta$. The downstream estimator uses the released noisy mean as an unbiased estimate of the true mean.\n\nStarting from the formal definition of Differential Privacy (DP), the concept of global sensitivity for the windowed mean function under the stated adjacency, and the standard calibrations of the Laplace and Gaussian mechanisms in the small $\\epsilon$ regime, derive expressions for the per-release mean squared error (MSE) of the two mechanisms, compare them, and determine the minimal achievable per-release MSE across the two mechanisms for the following parameters:\n- Bound $B = 0.5$,\n- Window length $W = 100$,\n- Privacy parameters $\\epsilon = 0.1$ and $\\delta = 10^{-5}$.\n\nAssume independent noise across releases and ignore temporal composition beyond the single window. Round your final answer to four significant figures. The signal is dimensionless, so express the final MSE as a dimensionless quantity.",
            "solution": "The objective is to determine the minimal per-release mean squared error (MSE) achievable by either a Laplace mechanism satisfying pure $(\\epsilon)$-Differential Privacy (DP) or a Gaussian mechanism satisfying approximate $(\\epsilon, \\delta)$-DP, for a given set of parameters. The estimator is unbiased, which implies that the MSE is equal to the variance of the noise added by the privacy mechanism.\n\nFirst, we must determine the global sensitivity of the query function. The query function $f$ computes the mean of a window of $W$ samples, $D = \\{x_1, x_2, \\dots, x_W\\}$, where each sample $x_i$ is in the interval $[-B, B]$. The function is given by:\n$$\nf(D) = \\frac{1}{W} \\sum_{i=1}^{W} x_i\n$$\nThe problem specifies event-level adjacency, where two datasets (windows) $D$ and $D'$ are adjacent if they differ in the value of exactly one sample. Let $D' = \\{x_1, \\dots, x_{k-1}, x'_k, x_{k+1}, \\dots, x_W\\}$. The $L_1$ global sensitivity, denoted $\\Delta f$, is the maximum possible absolute difference $|f(D) - f(D')|$ over all possible adjacent datasets $D$ and $D'$.\n$$\n\\Delta f = \\max_{D, D' \\text{ adjacent}} |f(D) - f(D')| = \\max_{x_k, x'_k \\in [-B, B]} \\left| \\frac{1}{W} \\sum_{i=1}^{W} x_i - \\frac{1}{W} \\left( \\sum_{i \\neq k} x_i + x'_k \\right) \\right|\n$$\n$$\n\\Delta f = \\max_{x_k, x'_k \\in [-B, B]} \\frac{1}{W} |x_k - x'_k|\n$$\nThe maximum value of $|x_k - x'_k|$ occurs when one sample is at the maximum possible value, $B$, and the other is at the minimum possible value, $-B$.\n$$\n\\max |x_k - x'_k| = |B - (-B)| = 2B\n$$\nTherefore, the global sensitivity of the windowed mean function is:\n$$\n\\Delta f = \\frac{2B}{W}\n$$\n\nNext, we analyze the MSE for each mechanism.\n\n**1. Laplace Mechanism for $\\epsilon$-DP**\n\nThe Laplace mechanism achieves $\\epsilon$-DP by adding noise drawn from a Laplace distribution. The privatized output is $M_L(D) = f(D) + Y$, where $Y \\sim \\text{Laplace}(\\mu, b)$. The location parameter $\\mu$ is set to $0$ to ensure the mechanism is unbiased. The scale parameter $b$ is calibrated based on the global sensitivity and the privacy parameter $\\epsilon$:\n$$\nb = \\frac{\\Delta f}{\\epsilon}\n$$\nSince the mean of the added noise is zero, the estimator is unbiased. The MSE is therefore equal to the variance of the noise. The variance of a Laplace distribution with scale $b$ is $2b^2$.\n$$\n\\text{MSE}_L = \\text{Var}(Y) = 2b^2 = 2 \\left( \\frac{\\Delta f}{\\epsilon} \\right)^2\n$$\nSubstituting the expression for $\\Delta f$:\n$$\n\\text{MSE}_L = 2 \\left( \\frac{2B}{W\\epsilon} \\right)^2 = \\frac{8B^2}{W^2\\epsilon^2}\n$$\n\n**2. Gaussian Mechanism for $(\\epsilon, \\delta)$-DP**\n\nThe Gaussian mechanism achieves $(\\epsilon, \\delta)$-DP by adding noise from a Gaussian distribution. The privatized output is $M_G(D) = f(D) + Z$, where $Z \\sim \\mathcal{N}(0, \\sigma^2)$. Similar to the Laplace mechanism, the noise is zero-mean, so the estimator is unbiased and its MSE is equal to the variance of the noise, $\\sigma^2$.\n\nFor a given $(\\epsilon, \\delta)$ with $\\epsilon \\in (0, 1)$, a standard and tight calibration sets the standard deviation $\\sigma$ of the noise as:\n$$\n\\sigma = \\frac{\\Delta f}{\\epsilon} \\sqrt{2 \\ln\\left(\\frac{1.25}{\\delta}\\right)}\n$$\nThe MSE for the Gaussian mechanism is therefore:\n$$\n\\text{MSE}_G = \\text{Var}(Z) = \\sigma^2 = \\left( \\frac{\\Delta f}{\\epsilon} \\right)^2 2 \\ln\\left(\\frac{1.25}{\\delta}\\right)\n$$\nSubstituting the expression for $\\Delta f$:\n$$\n\\text{MSE}_G = \\left( \\frac{2B}{W\\epsilon} \\right)^2 2 \\ln\\left(\\frac{1.25}{\\delta}\\right) = \\frac{8B^2}{W^2\\epsilon^2} \\ln\\left(\\frac{1.25}{\\delta}\\right)\n$$\n\n**3. Comparison and Numerical Calculation**\n\nTo find the minimal achievable MSE, we compare $\\text{MSE}_L$ and $\\text{MSE}_G$.\n$$\n\\text{MSE}_L = \\frac{8B^2}{W^2\\epsilon^2}\n$$\n$$\n\\text{MSE}_G = \\left(\\frac{8B^2}{W^2\\epsilon^2}\\right) \\ln\\left(\\frac{1.25}{\\delta}\\right)\n$$\nThe ratio of the two MSEs is:\n$$\n\\frac{\\text{MSE}_G}{\\text{MSE}_L} = \\ln\\left(\\frac{1.25}{\\delta}\\right)\n$$\nThe minimal MSE is determined by whether this factor is less than or greater than $1$. We are given the parameters: $B = 0.5$, $W = 100$, $\\epsilon = 0.1$, and $\\delta = 10^{-5}$.\n\nLet's evaluate the factor $\\ln(1.25/\\delta)$:\n$$\n\\ln\\left(\\frac{1.25}{10^{-5}}\\right) = \\ln(1.25 \\times 10^5) = \\ln(125000)\n$$\nUsing the natural logarithm, we find:\n$$\n\\ln(125000) \\approx 11.73606...\n$$\nSince $11.736... > 1$, we have $\\text{MSE}_G > \\text{MSE}_L$. Therefore, for the given parameters, the Laplace mechanism provides a lower mean squared error and thus achieves the minimal MSE.\n\nThe minimal MSE is:\n$$\n\\text{MSE}_{\\text{min}} = \\text{MSE}_L = \\frac{8B^2}{W^2\\epsilon^2}\n$$\nSubstituting the numerical values:\n$$\n\\text{MSE}_{\\text{min}} = \\frac{8(0.5)^2}{(100)^2(0.1)^2} = \\frac{8(0.25)}{(10000)(0.01)} = \\frac{2}{100} = 0.02\n$$\nThe problem requires the answer to be rounded to four significant figures. In scientific notation, this is $2.000 \\times 10^{-2}$.",
            "answer": "$$\\boxed{2.000 \\times 10^{-2}}$$"
        },
        {
            "introduction": "Real-world cyber-physical systems often release analytics continuously, and each release consumes a portion of the overall privacy budget. This exercise addresses the critical question of how to track this cumulative privacy loss over a sequence of releases . You will apply and contrast the basic and advanced composition theorems, discovering how the choice of accounting method dramatically impacts the long-term viability of a private time-series analysis system.",
            "id": "4213357",
            "problem": "A cyber-physical digital twin for a smart manufacturing line releases, at each discrete time step, a time-series statistic computed from raw sensor streams. At each time step, the mechanism is the Gaussian mechanism applied to a function with bounded squared-sensitivity, that is, the function has the same global $\\ell_{2}$-sensitivity $\\Delta_{2}$ at every time step. The noise standard deviation is calibrated per release so that each single-time-step release is $(\\epsilon, \\delta)$-Differentially Private (DP) with $\\epsilon = 0.5$ and $\\delta = 10^{-6}$. Assume the releases are produced adaptively over $T$ time steps.\n\nStarting from the definition of Differential Privacy and the standard composition theorems regarded as well-tested facts, derive the total privacy parameters under:\n- Basic composition, and\n- Advanced composition (choose an auxiliary parameter $\\delta' = 10^{-6}$).\n\nExpress each total privacy guarantee as a pair $(\\epsilon_{\\mathrm{tot}}, \\delta_{\\mathrm{tot}})$ as a function of $T$. Your final answer must be a single closed-form analytic expression presented as a row matrix containing, in order,\n- $\\epsilon_{\\mathrm{basic}}(T)$,\n- $\\epsilon_{\\mathrm{advanced}}(T)$,\n- $\\delta_{\\mathrm{basic}}(T)$,\n- $\\delta_{\\mathrm{advanced}}(T)$.\n\nNo numerical rounding is required. Do not include units in your final expression.",
            "solution": "The problem describes a sequence of $T$ adaptive mechanisms, where the $i$-th mechanism $M_i$ for each time step $i \\in \\{1, 2, \\dots, T\\}$ is $(\\epsilon, \\delta)$-differentially private. The privacy parameters for each release are given as $\\epsilon = 0.5$ and $\\delta = 10^{-6}$. We are asked to derive the total privacy cost $(\\epsilon_{\\mathrm{tot}}, \\delta_{\\mathrm{tot}})$ for the sequence of $T$ releases under two different composition rules: basic composition and advanced composition.\n\nFirst, let us recall the definition of $(\\epsilon, \\delta)$-Differential Privacy (DP). A randomized mechanism $M$ is $(\\epsilon, \\delta)$-DP if for all adjacent datasets $D_1$ and $D_2$ (differing in one individual's data), and for all subsets of outputs $S \\subseteq \\mathrm{Range}(M)$, the following inequality holds:\n$$P(M(D_1) \\in S) \\le \\exp(\\epsilon) P(M(D_2) \\in S) + \\delta$$\n\nWe will now apply the standard composition theorems to the sequence of $T$ mechanisms, where each mechanism $M_i$ is $(\\epsilon, \\delta)$-DP with $\\epsilon = 0.5$ and $\\delta = 10^{-6}$.\n\n**1. Basic Composition**\n\nThe basic composition theorem states that if a sequence of $T$ mechanisms $M_1, \\dots, M_T$ are respectively $(\\epsilon_1, \\delta_1), \\dots, (\\epsilon_T, \\delta_T)$-differentially private, their adaptive composition provides $(\\sum_{i=1}^T \\epsilon_i, \\sum_{i=1}^T \\delta_i)$-differential privacy.\n\nIn this problem, each of the $T$ mechanisms has the same privacy parameters $\\epsilon_i = \\epsilon = 0.5$ and $\\delta_i = \\delta = 10^{-6}$. Therefore, the total privacy parameters under basic composition, denoted $(\\epsilon_{\\mathrm{basic}}(T), \\delta_{\\mathrm{basic}}(T))$, are:\n$$\n\\epsilon_{\\mathrm{basic}}(T) = \\sum_{i=1}^T \\epsilon = T \\epsilon = T \\times 0.5 = 0.5T\n$$\n$$\n\\delta_{\\mathrm{basic}}(T) = \\sum_{i=1}^T \\delta = T \\delta = T \\times 10^{-6}\n$$\n\n**2. Advanced Composition**\n\nThe advanced composition theorem provides a tighter bound on the privacy loss for a sequence of mechanisms. A standard formulation of the theorem states that for any $\\epsilon, \\delta, \\delta' \\ge 0$, the $T$-fold adaptive composition of $(\\epsilon, \\delta)$-DP mechanisms is $(\\epsilon_{\\mathrm{tot}}, \\delta_{\\mathrm{tot}})$-DP, where:\n$$\n\\epsilon_{\\mathrm{tot}} = \\sqrt{2T \\ln(1/\\delta')} \\epsilon + T\\epsilon(\\exp(\\epsilon)-1)\n$$\n$$\n\\delta_{\\mathrm{tot}} = T\\delta + \\delta'\n$$\nThe problem specifies the per-step parameters $\\epsilon = 0.5$ and $\\delta = 10^{-6}$, and directs us to use an auxiliary parameter $\\delta' = 10^{-6}$. We will substitute these values into the theorem to find the total privacy parameters $(\\epsilon_{\\mathrm{advanced}}(T), \\delta_{\\mathrm{advanced}}(T))$.\n\nFirst, we compute the total delta parameter, $\\delta_{\\mathrm{advanced}}(T)$:\n$$\n\\delta_{\\mathrm{advanced}}(T) = T\\delta + \\delta' = T \\times 10^{-6} + 10^{-6} = (T+1) \\times 10^{-6}\n$$\n\nNext, we compute the total epsilon parameter, $\\epsilon_{\\mathrm{advanced}}(T)$:\n$$\n\\epsilon_{\\mathrm{advanced}}(T) = \\sqrt{2T \\ln(1/\\delta')} \\epsilon + T\\epsilon(\\exp(\\epsilon)-1)\n$$\nSubstituting the values $\\epsilon = 0.5$ and $\\delta' = 10^{-6}$:\n$$\n\\epsilon_{\\mathrm{advanced}}(T) = \\sqrt{2T \\ln(1/10^{-6})} \\times 0.5 + T \\times 0.5 \\times (\\exp(0.5)-1)\n$$\nWe can simplify the logarithmic term:\n$$\n\\ln(1/10^{-6}) = \\ln(10^6) = 6 \\ln(10)\n$$\nSubstituting this back into the expression for $\\epsilon_{\\mathrm{advanced}}(T)$:\n$$\n\\epsilon_{\\mathrm{advanced}}(T) = \\sqrt{2T \\cdot 6 \\ln(10)} \\times 0.5 + 0.5T(\\exp(0.5)-1)\n$$\n$$\n\\epsilon_{\\mathrm{advanced}}(T) = \\sqrt{12T \\ln(10)} \\times 0.5 + 0.5T(\\exp(0.5)-1)\n$$\nWe can simplify the square root term $\\sqrt{12} = \\sqrt{4 \\times 3} = 2\\sqrt{3}$:\n$$\n\\epsilon_{\\mathrm{advanced}}(T) = (2\\sqrt{3T \\ln(10)}) \\times 0.5 + 0.5T(\\exp(0.5)-1)\n$$\n$$\n\\epsilon_{\\mathrm{advanced}}(T) = \\sqrt{3T \\ln(10)} + 0.5T(\\exp(0.5)-1)\n$$\nThe problem does not require numerical evaluation, so we leave the expression in its exact analytical form.\n\nIn summary, the four requested quantities as functions of $T$ are:\n- $\\epsilon_{\\mathrm{basic}}(T) = 0.5T$\n- $\\epsilon_{\\mathrm{advanced}}(T) = \\sqrt{3T \\ln(10)} + 0.5T(\\exp(0.5) - 1)$\n- $\\delta_{\\mathrm{basic}}(T) = T \\times 10^{-6}$\n- $\\delta_{\\mathrm{advanced}}(T) = (T+1) \\times 10^{-6}$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 0.5T & \\sqrt{3T \\ln(10)} + 0.5T(\\exp(0.5) - 1) & T \\times 10^{-6} & (T+1) \\times 10^{-6} \\end{pmatrix}}\n$$"
        }
    ]
}