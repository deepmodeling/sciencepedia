{
    "hands_on_practices": [
        {
            "introduction": "动手实践的第一步是掌握模型漂移检测的基础。本练习将引导您推导累积和（CUSUM）控制图，这是一种用于快速检测过程均值变化的基本且强大的统计工具。通过从高斯分布的概率密度函数和似然比等第一性原理出发，您将构建一个序贯检验程序，并学习如何使用平均运行长度（ARL）来评估其性能。这个练习  不仅能加深您对序贯分析的理解，也为处理更复杂的漂移检测问题奠定了理论基础。",
            "id": "4231270",
            "problem": "一个信息物理系统的数字孪生执行一个单步预测器，并监测新息残差 $\\{r_t\\}_{t \\geq 1}$。在一个正确设定的模型下（无漂移），假设 $\\{r_t\\}$ 是独立同分布的高斯随机变量，具有零均值和已知方差，即 $r_t \\sim \\mathcal{N}(0,\\sigma^2)$，其中 $\\sigma^2$ 已知。一个对应于恒定加性偏差的模型漂移导致均值偏移到一个已知的设计值 $\\mu>0$，因此变化后的残差服从 $r_t \\sim \\mathcal{N}(\\mu,\\sigma^2)$，而方差保持为 $\\sigma^2$。您将使用一个单边累积和 (CUSUM) 程序来检测这个正向的偏移。\n\n仅从高斯分布的概率密度函数和似然比的定义出发，执行以下操作：\n\n1. 推导用于检验原假设 $H_0: r_t \\sim \\mathcal{N}(0,\\sigma^2)$ 与备择假设 $H_1: r_t \\sim \\mathcal{N}(\\mu,\\sigma^2)$ 的单样本对数似然比 $\\ell(r_t)$，并使用自然对数将其明确表示为 $r_t$、$\\mu$ 和 $\\sigma^2$ 的函数。\n\n2. 使用序贯变化检测原理和零点反射原则，推导单边 CUSUM 递归式，该递归式累积 $\\ell(r_t)$ 并在部分和为负时重置为零。将停时 $T_h$ 定义为 CUSUM 统计量首次超过固定阈值 $h$（以自然对数单位计量）的第一个索引 $t$。\n\n3. 使用基于矩生成函数的沃尔德型近似，并忽略过冲校正，计算所推导的 CUSUM 的受控平均游程长度 $\\mathrm{ARL}_0 \\triangleq \\mathbb{E}_0[T_h]$，其中下标 $0$ 表示在 $H_0$ 下的期望。为了进行此计算，取设计参数 $\\mu = 0.5$，$\\sigma = 1$ 和阈值 $h = 8$。将您最终的 $\\mathrm{ARL}_0$ 数值答案四舍五入到四位有效数字，并以样本为单位表示。",
            "solution": "该问题要求对用于检测高斯过程均值偏移的单边累积和 (CUSUM) 程序进行三部分分析。分析将首先验证问题陈述，该陈述已被认定为科学上合理、适定且客观。我们现在将推导必要的组成部分并计算所需的量。\n\n首先，我们推导用于检验原假设 $H_0$ 与备择假设 $H_1$ 的单样本对数似然比 $\\ell(r_t)$。均值为 $\\theta$、方差为 $\\sigma^2$ 的高斯分布的概率密度函数 (PDF) 由下式给出：\n$$f(x; \\theta, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x - \\theta)^2}{2\\sigma^2}\\right)$$\n在原假设 $H_0$ 下，残差 $r_t$ 的分布为 $r_t \\sim \\mathcal{N}(0, \\sigma^2)$，因此其 PDF 为：\n$$f_0(r_t) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{r_t^2}{2\\sigma^2}\\right)$$\n在备择假设 $H_1$ 下，残差 $r_t$ 的分布为 $r_t \\sim \\mathcal{N}(\\mu, \\sigma^2)$，其 PDF 为：\n$$f_1(r_t) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(r_t - \\mu)^2}{2\\sigma^2}\\right)$$\n似然比 $L(r_t)$ 是这两个密度的比值：\n$$L(r_t) = \\frac{f_1(r_t)}{f_0(r_t)} = \\frac{\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(r_t - \\mu)^2}{2\\sigma^2}\\right)}{\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{r_t^2}{2\\sigma^2}\\right)} = \\exp\\left(\\frac{r_t^2 - (r_t - \\mu)^2}{2\\sigma^2}\\right)$$\n指数可以简化为：\n$$r_t^2 - (r_t - \\mu)^2 = r_t^2 - (r_t^2 - 2\\mu r_t + \\mu^2) = 2\\mu r_t - \\mu^2$$\n将此代入 $L(r_t)$ 的表达式中，得到：\n$$L(r_t) = \\exp\\left(\\frac{2\\mu r_t - \\mu^2}{2\\sigma^2}\\right)$$\n单样本对数似然比 $\\ell(r_t)$ 是 $L(r_t)$ 的自然对数：\n$$\\ell(r_t) = \\ln(L(r_t)) = \\frac{2\\mu r_t - \\mu^2}{2\\sigma^2} = \\frac{\\mu}{\\sigma^2}r_t - \\frac{\\mu^2}{2\\sigma^2}$$\n这就是所求的对数似然比增量的表达式。\n\n其次，我们定义单边 CUSUM 递归式及相关的停时。CUSUM 统计量（记为 $S_t$）随时间累积对数似然比增量 $\\ell(r_t)$。“零点反射”原则意味着累积和不允许变为负值；如果变为负值，则将其重置为零。这由以下递归式捕获：\n$$S_0 = 0$$\n$$S_t = \\max(0, S_{t-1} + \\ell(r_t)) \\quad \\text{for } t \\geq 1$$\n当统计量 $S_t$ 首次超过预定义的阈值 $h$ 时，即检测到变化。停时 $T_h$ 正式定义为：\n$$T_h \\triangleq \\inf\\{t \\geq 1 : S_t > h\\}$$\n\n第三，我们使用沃尔德型近似计算受控平均游程长度 $\\mathrm{ARL}_0 = \\mathbb{E}_0[T_h]$。该近似基于大偏差理论和更新论证。在受控假设 $H_0$ 下，增量 $\\ell(r_t)$ 的期望值为负：\n$$\\mathbb{E}_0[\\ell(r_t)] = \\mathbb{E}_0\\left[\\frac{\\mu}{\\sigma^2}r_t - \\frac{\\mu^2}{2\\sigma^2}\\right] = \\frac{\\mu}{\\sigma^2}\\mathbb{E}_0[r_t] - \\frac{\\mu^2}{2\\sigma^2} = \\frac{\\mu}{\\sigma^2}(0) - \\frac{\\mu^2}{2\\sigma^2} = -\\frac{\\mu^2}{2\\sigma^2}$$\n由于这种负漂移，统计量 $S_t$ 会频繁重置为 $0$。达到阈值 $h$ 的过程可以看作是一系列更新周期，其中每个周期产生误报（即超过 $h$）的概率 $p$ 很小。直到此事件发生的样本数 $T_h$ 近似服从均值为 $1/p$ 的几何分布。因此，$\\mathrm{ARL}_0 \\approx 1/p$。\n\n从 $0$ 开始的过程曾越过边界 $h$ 的概率 $p$ 可以使用与沃尔德恒等式和克拉默定理相关的结果来近似。近似值为 $p \\approx \\exp(-\\theta^* h)$，其中 $\\theta^*$ 是方程 $\\mathbb{E}_0[\\exp(\\theta \\ell(r_t))] = 1$ 的非零解。我们来求这个 $\\theta^*$。\n在 $H_0$ 下 $\\ell(r_t)$ 的矩生成函数 (MGF) 是：\n$$\\psi_0(\\theta) = \\mathbb{E}_0[\\exp(\\theta \\ell(r_t))] = \\mathbb{E}_0\\left[\\exp\\left(\\theta \\left(\\frac{\\mu r_t}{\\sigma^2} - \\frac{\\mu^2}{2\\sigma^2}\\right)\\right)\\right] = \\exp\\left(-\\frac{\\theta \\mu^2}{2\\sigma^2}\\right) \\mathbb{E}_0\\left[\\exp\\left(\\frac{\\theta \\mu}{\\sigma^2}r_t\\right)\\right]$$\n期望内的项是正态随机变量 $r_t \\sim \\mathcal{N}(0, \\sigma^2)$ 在点 $k = \\frac{\\theta \\mu}{\\sigma^2}$ 处的矩生成函数 (MGF)。$\\mathcal{N}(m, s^2)$ 的 MGF 是 $\\exp(mk + s^2k^2/2)$。对于我们的情况，$m=0$ 且 $s^2=\\sigma^2$，所以：\n$$\\mathbb{E}_0\\left[\\exp\\left(\\frac{\\theta \\mu}{\\sigma^2}r_t\\right)\\right] = \\exp\\left(0 \\cdot \\frac{\\theta \\mu}{\\sigma^2} + \\frac{\\sigma^2}{2}\\left(\\frac{\\theta \\mu}{\\sigma^2}\\right)^2\\right) = \\exp\\left(\\frac{\\sigma^2 \\theta^2 \\mu^2}{2\\sigma^4}\\right) = \\exp\\left(\\frac{\\theta^2 \\mu^2}{2\\sigma^2}\\right)$$\n将此代入 $\\psi_0(\\theta)$ 的表达式中：\n$$\\psi_0(\\theta) = \\exp\\left(-\\frac{\\theta \\mu^2}{2\\sigma^2}\\right) \\exp\\left(\\frac{\\theta^2 \\mu^2}{2\\sigma^2}\\right) = \\exp\\left(\\frac{\\mu^2}{2\\sigma^2}(\\theta^2 - \\theta)\\right)$$\n我们寻找求解 $\\psi_0(\\theta^*) = 1$ 的非零 $\\theta^*$。这意味着指数必须为零：\n$$\\frac{\\mu^2}{2\\sigma^2}((\\theta^*)^2 - \\theta^*) = 0$$\n由于 $\\mu > 0$ 且 $\\sigma^2 > 0$，这简化为 $(\\theta^*)^2 - \\theta^* = 0$，或 $\\theta^*(\\theta^* - 1) = 0$。非零解是 $\\theta^* = 1$。\n当 $\\theta^* = 1$ 时，一个周期内发生误报的概率为 $p \\approx \\exp(-1 \\cdot h) = \\exp(-h)$。\n由此得到的受控平均游程长度的近似值为：\n$$\\mathrm{ARL}_0 \\approx \\frac{1}{p} \\approx \\frac{1}{\\exp(-h)} = \\exp(h)$$\n这个近似在 $h$ 较大且忽略过冲时有效。特定的值 $\\mu = 0.5$ 和 $\\sigma=1$ 在这个通用推导中不是必需的，因为 $\\theta^*$ 与它们无关。\n现在，我们使用给定的阈值 $h=8$ 来计算数值：\n$$\\mathrm{ARL}_0 \\approx \\exp(8)$$\n计算 $\\exp(8)$ 的值：\n$$\\exp(8) \\approx 2980.957987...$$\n将其四舍五入到四位有效数字得到 $2981$。结果以样本为单位表示。",
            "answer": "$$\\boxed{2981}$$"
        },
        {
            "introduction": "在检测到模型可能发生漂移后，下一步是量化其影响，特别是对于输出概率预测的模型。本练习将重点转向校准漂移（calibration drift）——一种模型性能下降的常见形式。您将通过推导著名的 Brier 分数的 Murphy 分解，学习如何将整体预测误差分解为不确定性、解析度和可靠性三个部分。这个过程  使我们能够精确地分离和衡量由模型未校准所直接导致的误差（即可靠性项），从而深刻理解漂移对模型可信度的具体影响。",
            "id": "4231249",
            "problem": "信息物理系统的数字孪生必须为二元事件（例如，异常存在与否）保持经过校准的概率预测。设二元结果为 $y \\in \\{0,1\\}$，模型产生的概率预测为 $p \\in [0,1]$。给定 $p$ 时，真实的条件事件概率（校准函数）为 $o(p) = \\mathbb{E}[y \\mid p]$。由于渐进的环境变化，该孪生表现出校准漂移，其模型为 $o(p) = \\gamma p + \\delta$，其中常数 $\\gamma \\in \\mathbb{R}$ 和 $\\delta \\in \\mathbb{R}$ 的选择需确保对所有 $p \\in [0,1]$ 都有 $o(p) \\in [0,1]$。假设在操作下，$p$ 的预测分布支撑集在 $[0,1]$ 上，其密度为 $f(p) = 2p$（一个 $\\mathrm{Beta}(2,1)$ 分布），这代表了置信度较高的预测更为频繁的情况。\n\n您将在平方损失假设下进行推导，并得出用于漂移检测和自适应的校准误差关系。\n\n1.  在平方损失下，将期望校准误差 (ECE) 定义为 $\\mathrm{ECE}_{2}$，它是一个基于校准函数 $o(p)$ 和预测分布 $f(p)$ 的极限的、无分箱的、总体性度量。\n\n2.  在平方损失下，将阈值为 $\\tau \\in (0,1)$ 的阈值自适应校准误差 (TACE) 定义为 $\\mathrm{TACE}_{2}(\\tau)$，它是在相同的极限、无分箱设置下，仅限于预测值 $p \\geq \\tau$ 的条件期望平方校准误差。\n\n3.  将 Brier 分数（平方损失的正常评分规则）定义为 $\\mathrm{BS} = \\mathbb{E}[(p - y)^{2}]$。\n\n仅从这些定义、全期望定律和全方差定律出发，推导 Brier 分数的 Murphy 分解，将其分解为不确定性、分辨率和可靠性三项，并展示在平方损失下，可靠性项如何等同于 $\\mathrm{ECE}_{2}$。然后，在 $f(p) = 2p$ 和 $o(p) = \\gamma p + \\delta$ 的特定设置下：\n\n-   计算基准率 $\\mu = \\mathbb{E}[y]$。\n-   计算可靠性项 $\\mathbb{E}[(p - o(p))^{2}]$ 并将其等同于 $\\mathrm{ECE}_{2}$。\n-   计算分辨率项 $\\mathrm{Var}(o(p))$。\n-   使用阈值 $\\tau = \\tfrac{1}{2}$，计算 $\\mathrm{TACE}_{2}\\!\\left(\\tfrac{1}{2}\\right)$，即在 $\\{p \\geq \\tfrac{1}{2}\\}$ 上的条件期望平方校准误差。\n\n最后，提供 Brier 分数 $\\mathrm{BS}$ 作为 $\\gamma$ 和 $\\delta$ 的函数的精确闭式表达式。无需进行数值四舍五入。将最终答案表示为单一的闭式解析表达式。",
            "solution": "该问题具有科学依据，是适定且客观的。所有定义和条件都已给出，从而可以进行严谨的数学推导。该问题是概率预测和模型校准统计理论中的一个标准练习。因此，该问题是有效的，我将继续进行求解。\n\n首先，我们根据要求建立定义和一般分解。\n二元结果为 $y \\in \\{0, 1\\}$。模型的概率预测为 $p \\in [0, 1]$。真实条件概率，或称校准函数，为 $o(p) = \\mathbb{E}[y \\mid p]$。预测分布由概率密度函数 $f(p)$ 给出。\n\n1.  平方损失下的期望校准误差 (ECE)，记为 $\\mathrm{ECE}_{2}$，是预测概率 $p$ 与真实条件概率 $o(p)$ 之间的期望平方差。它是一种衡量失准的总体层面、无分箱的度量：\n    $$ \\mathrm{ECE}_{2} = \\mathbb{E}[(p - o(p))^{2}] = \\int_{0}^{1} (p - o(p))^{2} f(p) \\, dp $$\n\n2.  平方损失下的阈值自适应校准误差 (TACE)，记为 $\\mathrm{TACE}_{2}(\\tau)$，是条件期望平方校准误差，限于阈值 $\\tau \\in (0, 1)$ 下的预测集 $p \\ge \\tau$：\n    $$ \\mathrm{TACE}_{2}(\\tau) = \\mathbb{E}[(p - o(p))^{2} \\mid p \\ge \\tau] = \\frac{\\int_{\\tau}^{1} (p - o(p))^{2} f(p) \\, dp}{\\int_{\\tau}^{1} f(p) \\, dp} $$\n\n3.  Brier 分数 (BS) 是预测的均方误差，定义为：\n    $$ \\mathrm{BS} = \\mathbb{E}[(p-y)^{2}] $$\n\n我们现在推导 Brier 分数的 Murphy 分解。我们使用全期望定律 $\\mathbb{E}[X] = \\mathbb{E}[\\mathbb{E}[X \\mid Z]]$。让我们以预测值 $p$ 为条件：\n$$ \\mathrm{BS} = \\mathbb{E}[\\mathbb{E}[(p-y)^{2} \\mid p]] $$\n我们展开内部期望。由于在条件期望中 $p$ 是固定的：\n$$ \\mathbb{E}[(p-y)^{2} \\mid p] = \\mathbb{E}[p^{2} - 2py + y^{2} \\mid p] = p^{2} - 2p\\mathbb{E}[y \\mid p] + \\mathbb{E}[y^{2} \\mid p] $$\n给定 $y \\in \\{0, 1\\}$，我们有 $y^{2} = y$。使用定义 $o(p) = \\mathbb{E}[y \\mid p]$，我们得到：\n$$ \\mathbb{E}[(p-y)^{2} \\mid p] = p^{2} - 2po(p) + o(p) $$\n为了揭示其结构，我们加上再减去 $o(p)^{2}$：\n$$ p^{2} - 2po(p) + o(p)^{2} - o(p)^{2} + o(p) = (p - o(p))^{2} + o(p) - o(p)^{2} = (p - o(p))^{2} + o(p)(1 - o(p)) $$\n根据 $f(p)$ 对所有 $p$ 取外部期望：\n$$ \\mathrm{BS} = \\mathbb{E}[(p - o(p))^{2}] + \\mathbb{E}[o(p)(1 - o(p))] $$\n第一项是**可靠性 (Reliability)** 项，这正是 $\\mathrm{ECE}_{2}$ 的定义。\n$$ \\text{可靠性} = \\mathbb{E}[(p - o(p))^{2}] = \\mathrm{ECE}_{2} $$\n第二项可以进一步分解。令 $\\mu = \\mathbb{E}[y]$ 为基准率。根据全期望定律，$\\mu = \\mathbb{E}[\\mathbb{E}[y \\mid p]] = \\mathbb{E}[o(p)]$。\n第二项是 $\\mathbb{E}[o(p) - o(p)^{2}] = \\mathbb{E}[o(p)] - \\mathbb{E}[o(p)^{2}]$。\n使用方差的定义，$\\mathrm{Var}(o(p)) = \\mathbb{E}[o(p)^{2}] - (\\mathbb{E}[o(p)])^{2}$，我们有 $\\mathbb{E}[o(p)^{2}] = \\mathrm{Var}(o(p)) + \\mu^{2}$。\n将其代入，第二项变为：\n$$ \\mu - (\\mathrm{Var}(o(p)) + \\mu^{2}) = \\mu - \\mu^{2} - \\mathrm{Var}(o(p)) = \\mu(1-\\mu) - \\mathrm{Var}(o(p)) $$\n项 $\\mu(1-\\mu) = \\mathrm{Var}(y)$ 是**不确定性 (Uncertainty)**，它内在于结果的随机性中。项 $\\mathrm{Var}(o(p))$ 是**分辨率 (Resolution)**，它衡量预测值与基准率的偏离程度。\n完整的 Murphy 分解是：\n$$ \\mathrm{BS} = \\underbrace{\\mathbb{E}[(p - o(p))^{2}]}_{\\text{可靠性}} - \\underbrace{\\mathrm{Var}(o(p))}_{\\text{分辨率}} + \\underbrace{\\mu(1-\\mu)}_{\\text{不确定性}} $$\n如上所示，可靠性项与 $\\mathrm{ECE}_{2}$ 相同。\n\n现在我们针对 $f(p) = 2p$（对于 $p \\in [0,1]$）以及校准漂移模型为 $o(p) = \\gamma p + \\delta$ 的情况进行具体计算。\n\n- **基准率 $\\mu$**：\n$$ \\mu = \\mathbb{E}[y] = \\mathbb{E}[o(p)] = \\int_{0}^{1} (\\gamma p + \\delta)(2p) \\, dp = \\int_{0}^{1} (2\\gamma p^{2} + 2\\delta p) \\, dp $$\n$$ \\mu = \\left[ 2\\gamma \\frac{p^{3}}{3} + 2\\delta \\frac{p^{2}}{2} \\right]_{0}^{1} = \\frac{2\\gamma}{3} + \\delta $$\n\n- **可靠性项 / $\\mathrm{ECE}_{2}$**：\n误差为 $p - o(p) = p - (\\gamma p + \\delta) = (1-\\gamma)p - \\delta$。\n$$ \\mathrm{ECE}_{2} = \\mathbb{E}[(p - o(p))^{2}] = \\int_{0}^{1} ((1-\\gamma)p - \\delta)^{2} (2p) \\, dp $$\n$$ \\mathrm{ECE}_{2} = \\int_{0}^{1} ((1-\\gamma)^{2}p^{2} - 2\\delta(1-\\gamma)p + \\delta^{2})(2p) \\, dp $$\n$$ \\mathrm{ECE}_{2} = \\int_{0}^{1} (2(1-\\gamma)^{2}p^{3} - 4\\delta(1-\\gamma)p^{2} + 2\\delta^{2}p) \\, dp $$\n$$ \\mathrm{ECE}_{2} = \\left[ 2(1-\\gamma)^{2}\\frac{p^{4}}{4} - 4\\delta(1-\\gamma)\\frac{p^{3}}{3} + 2\\delta^{2}\\frac{p^{2}}{2} \\right]_{0}^{1} $$\n$$ \\mathrm{ECE}_{2} = \\frac{(1-\\gamma)^{2}}{2} - \\frac{4\\delta(1-\\gamma)}{3} + \\delta^{2} $$\n\n- **分辨率项**：\n$$ \\text{分辨率} = \\mathrm{Var}(o(p)) = \\mathbb{E}[o(p)^{2}] - (\\mathbb{E}[o(p)])^{2} $$\n首先，我们求 $\\mathbb{E}[o(p)^{2}]$：\n$$ \\mathbb{E}[o(p)^{2}] = \\int_{0}^{1} (\\gamma p + \\delta)^{2} (2p) \\, dp = \\int_{0}^{1} (2\\gamma^{2}p^{3} + 4\\gamma\\delta p^{2} + 2\\delta^{2}p) \\, dp $$\n$$ \\mathbb{E}[o(p)^{2}] = \\left[ 2\\gamma^{2}\\frac{p^{4}}{4} + 4\\gamma\\delta\\frac{p^{3}}{3} + 2\\delta^{2}\\frac{p^{2}}{2} \\right]_{0}^{1} = \\frac{\\gamma^{2}}{2} + \\frac{4\\gamma\\delta}{3} + \\delta^{2} $$\n现在，我们计算方差：\n$$ \\mathrm{Var}(o(p)) = \\left(\\frac{\\gamma^{2}}{2} + \\frac{4\\gamma\\delta}{3} + \\delta^{2}\\right) - \\left(\\frac{2\\gamma}{3} + \\delta\\right)^{2} $$\n$$ \\mathrm{Var}(o(p)) = \\left(\\frac{\\gamma^{2}}{2} + \\frac{4\\gamma\\delta}{3} + \\delta^{2}\\right) - \\left(\\frac{4\\gamma^{2}}{9} + \\frac{4\\gamma\\delta}{3} + \\delta^{2}\\right) $$\n$$ \\mathrm{Var}(o(p)) = \\frac{\\gamma^{2}}{2} - \\frac{4\\gamma^{2}}{9} = \\left(\\frac{9 - 8}{18}\\right)\\gamma^{2} = \\frac{\\gamma^{2}}{18} $$\n\n- **$\\mathrm{TACE}_{2}\\!\\left(\\frac{1}{2}\\right)$**：\n我们使用阈值 $\\tau = \\frac{1}{2}$。\n分母是 $p \\ge \\frac{1}{2}$ 的概率质量：\n$$ \\int_{1/2}^{1} f(p) \\, dp = \\int_{1/2}^{1} 2p \\, dp = [p^{2}]_{1/2}^{1} = 1^{2} - \\left(\\frac{1}{2}\\right)^{2} = 1 - \\frac{1}{4} = \\frac{3}{4} $$\n分子是平方误差的未归一化条件期望：\n$$ \\int_{1/2}^{1} ((1-\\gamma)p - \\delta)^{2} (2p) \\, dp = \\left[ \\frac{(1-\\gamma)^{2}}{2}p^{4} - \\frac{4\\delta(1-\\gamma)}{3}p^{3} + \\delta^{2}p^{2} \\right]_{1/2}^{1} $$\n在极限处求值：\n$$ \\text{当 } p=1 \\text{ 时}: \\frac{(1-\\gamma)^{2}}{2} - \\frac{4\\delta(1-\\gamma)}{3} + \\delta^{2} $$\n$$ \\text{当 } p=1/2 \\text{ 时}: \\frac{(1-\\gamma)^{2}}{2}\\left(\\frac{1}{16}\\right) - \\frac{4\\delta(1-\\gamma)}{3}\\left(\\frac{1}{8}\\right) + \\delta^{2}\\left(\\frac{1}{4}\\right) = \\frac{(1-\\gamma)^{2}}{32} - \\frac{\\delta(1-\\gamma)}{6} + \\frac{\\delta^{2}}{4} $$\n分子是两者之差：\n$$ \\left(\\frac{1}{2} - \\frac{1}{32}\\right)(1-\\gamma)^{2} - \\left(\\frac{4}{3} - \\frac{1}{6}\\right)\\delta(1-\\gamma) + \\left(1 - \\frac{1}{4}\\right)\\delta^{2} $$\n$$ = \\frac{15}{32}(1-\\gamma)^{2} - \\frac{7}{6}\\delta(1-\\gamma) + \\frac{3}{4}\\delta^{2} $$\n最后，我们将分子除以分母 $\\frac{3}{4}$ 来计算 $\\mathrm{TACE}_{2}(\\frac{1}{2})$：\n$$ \\mathrm{TACE}_{2}\\!\\left(\\frac{1}{2}\\right) = \\frac{4}{3}\\left( \\frac{15}{32}(1-\\gamma)^{2} - \\frac{7}{6}\\delta(1-\\gamma) + \\frac{3}{4}\\delta^{2} \\right) $$\n$$ \\mathrm{TACE}_{2}\\!\\left(\\frac{1}{2}\\right) = \\frac{5}{8}(1-\\gamma)^{2} - \\frac{14}{9}\\delta(1-\\gamma) + \\delta^{2} $$\n\n- **Brier 分数 $\\mathrm{BS}$**：\n与其将 Murphy 分解的各分量相加，不如直接根据其定义 $\\mathrm{BS} = \\mathbb{E}[(p-y)^{2}]$ 来计算 Brier 分数。\n$$ \\mathrm{BS} = \\mathbb{E}[\\mathbb{E}[(p-y)^{2} \\mid p]] = \\mathbb{E}[ (p - o(p))^{2} + o(p)(1 - o(p)) ] $$\n这是我们之前找到的分解。一个更直接的途径是：\n$$ \\mathrm{BS} = \\mathbb{E}[p^{2} - 2py + y^{2}] = \\mathbb{E}[p^{2}] - 2\\mathbb{E}[py] + \\mathbb{E}[y] $$\n（因为 $y^2 = y$）。我们计算每一项：\n$$ \\mathbb{E}[p^{2}] = \\int_{0}^{1} p^{2}f(p)\\,dp = \\int_{0}^{1} p^{2}(2p)\\,dp = \\int_{0}^{1} 2p^{3}\\,dp = \\left[ \\frac{p^{4}}{2} \\right]_{0}^{1} = \\frac{1}{2} $$\n$$ \\mathbb{E}[y] = \\mu = \\frac{2\\gamma}{3} + \\delta $$\n$$ \\mathbb{E}[py] = \\mathbb{E}[\\mathbb{E}[py \\mid p]] = \\mathbb{E}[p\\mathbb{E}[y \\mid p]] = \\mathbb{E}[p \\cdot o(p)] $$\n$$ \\mathbb{E}[p \\cdot o(p)] = \\int_{0}^{1} p(\\gamma p + \\delta)(2p)\\,dp = \\int_{0}^{1} (2\\gamma p^{3} + 2\\delta p^{2})\\,dp $$\n$$ \\mathbb{E}[p \\cdot o(p)] = \\left[ 2\\gamma\\frac{p^{4}}{4} + 2\\delta\\frac{p^{3}}{3} \\right]_{0}^{1} = \\frac{\\gamma}{2} + \\frac{2\\delta}{3} $$\n将这些结果代入 $\\mathrm{BS}$ 的表达式中：\n$$ \\mathrm{BS} = \\frac{1}{2} - 2\\left(\\frac{\\gamma}{2} + \\frac{2\\delta}{3}\\right) + \\left(\\frac{2\\gamma}{3} + \\delta\\right) $$\n$$ \\mathrm{BS} = \\frac{1}{2} - \\gamma - \\frac{4\\delta}{3} + \\frac{2\\gamma}{3} + \\delta $$\n$$ \\mathrm{BS} = \\frac{1}{2} + \\left(\\frac{2}{3} - 1\\right)\\gamma + \\left(1 - \\frac{4}{3}\\right)\\delta = \\frac{1}{2} - \\frac{1}{3}\\gamma - \\frac{1}{3}\\delta $$\n这可以简化为最终的闭式表达式。\n$$ \\mathrm{BS} = \\frac{1}{2} - \\frac{1}{3}(\\gamma + \\delta) $$\n这个表达式巧妙地结合了校准漂移参数 $\\gamma$ 和 $\\delta$ 对由 Brier 分数衡量的整体预测性能的影响。",
            "answer": "$$\\boxed{\\frac{1}{2} - \\frac{1}{3}(\\gamma + \\delta)}$$"
        },
        {
            "introduction": "掌握了漂移的检测和量化方法后，最终的挑战在于如何使模型适应这些变化。本练习将指导您为一个在分类器中常见的特定漂移问题——标签漂移（label shift）——开发一个先进的自适应算法。您将从零开始推导期望最大化（EM）算法，这是一种强大的迭代方法，用于在目标域标签未知的情况下估计新的类别先验分布。通过结合理论推导与实际编程 ，您将实现一个完整的解决方案，体验从理论到实践，解决真实世界中模型适应问题的全过程。",
            "id": "4231223",
            "problem": "考虑一个信息物理系统的数字孪生，它使用在源域数据上训练的校准分类器，将运行状态连续分类为 $K$ 个离散类别。设源域类别先验为 $\\pi^{s} \\in \\mathbb{R}^{K}$，其条目为 $\\pi^{s}_{k} \\in (0,1)$ 且满足 $\\sum_{k=1}^{K} \\pi^{s}_{k} = 1$。在目标域中，以标签偏移的形式发生模型漂移：给定标签的特征条件分布保持不变，但类别先验发生变化。形式上，标签偏移假设为，对于所有 $k \\in \\{1,\\dots,K\\}$，$p_{t}(x \\mid y=k) = p_{s}(x \\mid y=k)$，而 $p_{t}(y=k) = \\pi_{k}$ 可能与 $\\pi^{s}_{k}$ 不同。该数字孪生收集目标域特征实例 $x_{i}$（$i \\in \\{1,\\dots,n\\}$），其可靠性权重为 $w_{i} > 0$，并且源分类器提供校准过的后验概率 $q_{ik} = p_{s}(y=k \\mid x_{i})$，对每个 $i$ 满足 $\\sum_{k=1}^{K} q_{ik} = 1$。在标签偏移模型和贝叶斯法则下，观测到的特征在目标域的加权对数似然是未知目标先验 $\\pi \\in \\Delta^{K-1}$（其中 $\\Delta^{K-1} = \\{ \\pi \\in \\mathbb{R}^{K} \\mid \\pi_{k} \\ge 0, \\sum_{k=1}^{K} \\pi_{k} = 1 \\}$）的函数，表示为\n$$\n\\mathcal{L}_{w}(\\pi) = \\sum_{i=1}^{n} w_{i} \\log \\left( \\sum_{k=1}^{K} \\frac{q_{ik}}{\\pi^{s}_{k}} \\, \\pi_{k} \\right),\n$$\n其中因子 $a_{ik} = \\frac{q_{ik}}{\\pi^{s}_{k}}$ 源于恒等式 $p_{s}(x \\mid y=k) = \\frac{p_{s}(y=k \\mid x) \\, p_{s}(x)}{\\pi^{s}_{k}}$，且 $p_{s}(x)$ 不依赖于 $\\pi$。您的任务有两部分：\n\n1. 在标签偏移模型下，从第一性原理出发，推导一个用于最大化 $\\mathcal{L}_{w}(\\pi)$ 的期望最大化 (EM) 过程，从以下基础开始：\n   - 贝叶斯定理 $p(y \\mid x) = \\frac{p(x \\mid y) p(y)}{p(x)}$，\n   - 带有权重 $w_{i}$ 的独立观测值的加权对数似然 $\\mathcal{L}_{w}(\\pi)$ 的定义，\n   - 使用潜变量，通过交替最大化 $\\mathcal{L}_{w}(\\pi)$ 的詹森下界来定义的期望最大化 (EM) 方法，\n   - 对数函数凹性的詹森不等式。\n   您的推导必须明确 E 步的责任和 M 步对 $\\pi$ 的更新，并且必须证明每次 EM 迭代都会单调增加（或保持不变）$\\mathcal{L}_{w}(\\pi)$，从而确保至少收敛到一个局部最大值。\n\n2. 实现一个完整的、可运行的程序，该程序：\n   - 不接受外部输入，并使用内部测试用例，\n   - 对每个测试用例，计算 EM 序列 $\\{\\pi^{(t)}\\}_{t=0}^{T}$ 和相应的序列 $\\{\\mathcal{L}_{w}(\\pi^{(t)})\\}_{t=0}^{T}$，\n   - 在数值容差范围内，验证 $\\mathcal{L}_{w}(\\pi^{(t)})$ 在迭代过程中的单调非减性，\n   - 对每个测试用例，返回一个指示单调改进的布尔值和最终估计值 $\\pi^{(\\text{final})}$。\n\n使用以下测试套件，所有量均表示为无单位的小数：\n- 测试用例 A（一般情况）：$K=3$， $n=8$， $\\pi^{s} = [0.5, 0.3, 0.2]$，$q$ 的行为\n  $[0.7, 0.2, 0.1]$, $[0.6, 0.25, 0.15]$, $[0.2, 0.7, 0.1]$, $[0.1, 0.2, 0.7]$, $[0.5, 0.3, 0.2]$, $[0.55, 0.3, 0.15]$, $[0.4, 0.4, 0.2]$, $[0.3, 0.3, 0.4]$，权重 $w = [1.0, 0.8, 1.2, 1.5, 0.9, 1.1, 1.0, 0.7]$。\n- 测试用例 B（信息贫乏的边界情况）：$K=3$， $n=6$， $\\pi^{s} = [0.5, 0.3, 0.2]$，$q$ 的所有行均为 $[0.5, 0.3, 0.2]$，权重 $w = [1.0, 2.0, 0.5, 1.5, 0.75, 1.25]$。\n- 测试用例 C（带有零的边缘情况）：$K=4$， $n=6$， $\\pi^{s} = [0.4, 0.25, 0.2, 0.15]$，$q$ 的行为 $[0.95, 0.05, 0.0, 0.0]$, $[0.0, 0.9, 0.1, 0.0]$, $[0.0, 0.0, 1.0, 0.0]$, $[0.1, 0.0, 0.0, 0.9]$, $[0.5, 0.5, 0.0, 0.0]$, $[0.2, 0.3, 0.5, 0.0]$，权重 $w = [2.0, 1.0, 1.5, 3.0, 1.2, 0.8]$。\n\n每个测试用例的初始化必须使用 $\\pi^{(0)} = \\pi^{s}$。为保证数值稳定性，请确保所有迭代值满足 $\\pi^{(t)}_{k} \\in (0,1)$ 和 $\\sum_{k=1}^{K} \\pi^{(t)}_{k} = 1$，并使用 $10^{-10}$ 的容差进行收敛检查和单调非减性检查。\n\n您的程序应生成单行输出，其中包含用方括号括起来的、以逗号分隔的结果列表。每个测试用例的结果必须是 $[\\text{monotone},[\\pi_{1}^{(\\text{final})},\\dots,\\pi_{K}^{(\\text{final})}]]]$ 的形式，其中 $\\text{monotone}$ 是一个布尔值，$\\pi_{k}^{(\\text{final})}$ 是小数。例如，总体输出应为 $[[\\text{True},[0.4,0.4,0.2]],[\\text{True},[0.5,0.3,0.2]],[\\text{True},[0.45,0.25,0.2,0.1]]]$ 的形式（此处显示的数值仅为示例）。",
            "solution": "用户提供的问题陈述已经过分析，并被认为是有效的。它在统计机器学习方面有科学依据，是一个适定的凸优化问题，并包含一套完整且一致的给定条件。\n\n本文提供了一个完整的解决方案，首先从第一性原理推导针对特定问题的期望最大化 (EM) 算法，然后是实现细节。\n\n### 第 1 部分：期望最大化 (EM) 算法的推导\n\n目标是找到目标域类别先验 $\\pi \\in \\Delta^{K-1}$，以最大化观测到的目标域特征 $\\{x_i\\}_{i=1}^n$ 的加权对数似然。集合 $\\Delta^{K-1}$ 是 $(K-1)$ 维概率单纯形，定义为 $\\Delta^{K-1} = \\{ \\pi \\in \\mathbb{R}^{K} \\mid \\pi_{k} \\ge 0, \\sum_{k=1}^{K} \\pi_{k} = 1 \\}$。\n\n**1. 观测数据对数似然**\n\n在目标域中观测到单个特征向量 $x_i$ 的概率 $p_t(x_i)$，是通过对未知类别标签 $y=k$（其中 $k \\in \\{1, \\dots, K\\}$）进行边缘化得到的：\n$$p_t(x_i; \\pi) = \\sum_{k=1}^{K} p_t(x_i, y=k; \\pi) = \\sum_{k=1}^{K} p_t(x_i \\mid y=k) p_t(y=k; \\pi)$$\n该模型由两个关键假设定义：\n- 目标域类别先验是 $p_t(y=k; \\pi) = \\pi_k$。\n- 标签偏移假设指出，类条件特征分布是不变的：$p_t(x \\mid y=k) = p_s(x \\mid y=k)$。\n\n将这些代入 $p_t(x_i; \\pi)$ 的表达式中：\n$$p_t(x_i; \\pi) = \\sum_{k=1}^{K} p_s(x_i \\mid y=k) \\pi_k$$\n使用源域的贝叶斯定理，我们可以用给定数量来表示 $p_s(x_i \\mid y=k)$：源分类器的后验概率 $q_{ik} = p_s(y=k \\mid x_i)$ 和源先验概率 $\\pi^s_k = p_s(y=k)$。\n$$p_s(x_i \\mid y=k) = \\frac{p_s(y=k \\mid x_i) p_s(x_i)}{p_s(y=k)} = \\frac{q_{ik} p_s(x_i)}{\\pi^s_k}$$\n将此代回 $p_t(x_i; \\pi)$ 的表达式中：\n$$p_t(x_i; \\pi) = \\sum_{k=1}^{K} \\frac{q_{ik} p_s(x_i)}{\\pi^s_k} \\pi_k = p_s(x_i) \\sum_{k=1}^{K} \\frac{q_{ik}}{\\pi^s_k} \\pi_k$$\n对于 $n$ 个独立观测值 $\\{x_i\\}$（每个权重为 $w_i > 0$），其加权对数似然为：\n$$ \\mathcal{L}_w(\\pi) = \\sum_{i=1}^{n} w_i \\log p_t(x_i; \\pi) = \\sum_{i=1}^{n} w_i \\log \\left( p_s(x_i) \\sum_{k=1}^{K} \\frac{q_{ik}}{\\pi^s_k} \\pi_k \\right) $$\n$$ \\mathcal{L}_w(\\pi) = \\sum_{i=1}^{n} w_i \\log p_s(x_i) + \\sum_{i=1}^{n} w_i \\log \\left( \\sum_{k=1}^{K} \\frac{q_{ik}}{\\pi^s_k} \\pi_k \\right) $$\n当对 $\\pi$ 进行最大化时，项 $\\sum_{i=1}^{n} w_i \\log p_s(x_i)$ 是一个常数，可以忽略。因此，我们的目标是最大化问题陈述中提供的目标函数：\n$$ \\mathcal{L}_{w}(\\pi) = \\sum_{i=1}^{n} w_{i} \\log \\left( \\sum_{k=1}^{K} \\frac{q_{ik}}{\\pi^{s}_{k}} \\, \\pi_{k} \\right) $$\n对数内的求和使得直接最大化变得困难。这种结构是应用期望最大化 (EM) 算法的典型情况。\n\n**2. 带有潜变量的 EM 框架**\n\n我们引入潜变量 $Z = \\{z_{ik}\\}_{i=1, k=1}^{n, K}$，其中如果观测 $x_i$ 的真实标签是 $k$，则 $z_{ik}=1$，否则 $z_{ik}=0$。对于每个 $i$，$\\{z_{i1}, \\dots, z_{iK}\\}$ 是一个独热向量。\n\n完整数据（包括观测数据 $X=\\{x_i\\}$ 和潜数据 $Z$）具有更简单的似然。$(x_i, z_i)$ 的联合概率（其中 $z_i$ 表示观测 $i$ 的独热向量）是：\n$$ p_t(x_i, z_i; \\pi) = \\prod_{k=1}^{K} (p_t(x_i, y=k; \\pi))^{z_{ik}} = \\prod_{k=1}^{K} (p_s(x_i \\mid y=k) \\pi_k)^{z_{ik}} $$\n完整数据的加权对数似然 $\\mathcal{L}_c(\\pi; X, Z)$ 是：\n$$ \\mathcal{L}_c(\\pi; X, Z) = \\sum_{i=1}^{n} w_i \\log p_t(x_i, z_i; \\pi) = \\sum_{i=1}^{n} w_i \\sum_{k=1}^K z_{ik} \\log(p_s(x_i|y=k)\\pi_k) $$\nEM 算法通过最大化在给定观测数据 $X$ 和当前参数估计 $\\pi^{(t)}$ 条件下潜变量 $Z$ 的 $\\mathcal{L}_c$ 的期望，来迭代地找到一个增加对数似然的参数估计 $\\pi^{(t+1)}$。这个期望就是 Q-函数：\n$$ Q(\\pi, \\pi^{(t)}) = E_{Z \\mid X, \\pi^{(t)}}[\\mathcal{L}_c(\\pi; X, Z)] $$\n\n**3. E-步：计算责任**\n\nE-步涉及计算 Q-函数。根据期望的线性性质：\n$$ Q(\\pi, \\pi^{(t)}) = \\sum_{i=1}^{n} w_i \\sum_{k=1}^{K} E[z_{ik} \\mid x_i, \\pi^{(t)}] \\log(p_s(x_i|y=k)\\pi_k) $$\n期望 $E[z_{ik} \\mid x_i, \\pi^{(t)}]$ 是在给定数据 $x_i$ 和当前估计 $\\pi^{(t)}$ 的情况下，$x_i$ 的真实标签为 $k$ 的后验概率。这被称为类别 $k$ 对观测 $i$ 的“责任”，记为 $\\gamma_{ik}^{(t)}$：\n$$ \\gamma_{ik}^{(t)} = P(y=k \\mid x_i; \\pi^{(t)})_t = \\frac{p_t(x_i \\mid y=k) p_t(y=k; \\pi^{(t)})}{p_t(x_i; \\pi^{(t)})} = \\frac{p_s(x_i \\mid y=k) \\pi_k^{(t)}}{\\sum_{j=1}^K p_s(x_i \\mid y=j) \\pi_j^{(t)}} $$\n代入 $p_s(x_i \\mid y=j) = q_{ij}p_s(x_i)/\\pi^s_j$：\n$$ \\gamma_{ik}^{(t)} = \\frac{\\frac{q_{ik} p_s(x_i)}{\\pi^s_k} \\pi_k^{(t)}}{\\sum_{j=1}^K \\frac{q_{ij} p_s(x_i)}{\\pi^s_j} \\pi_j^{(t)}} = \\frac{\\frac{q_{ik}}{\\pi^s_k} \\pi_k^{(t)}}{\\sum_{j=1}^K \\frac{q_{ij}}{\\pi^s_j} \\pi_j^{(t)}} $$\n这是在 E-步中计算的责任的最终表达式。\n\n**4. M-步：最大化 Q-函数**\n\nM-步通过最大化关于 $\\pi$ 的 $Q(\\pi, \\pi^{(t)})$ 来更新参数：\n$$ \\pi^{(t+1)} = \\arg\\max_{\\pi \\in \\Delta^{K-1}} Q(\\pi, \\pi^{(t)}) $$\n我们可以将 Q-函数分为两部分：\n$$ Q(\\pi, \\pi^{(t)}) = \\sum_{i=1}^{n} \\sum_{k=1}^{K} w_i \\gamma_{ik}^{(t)} \\log p_s(x_i|y=k) + \\sum_{i=1}^{n} \\sum_{k=1}^{K} w_i \\gamma_{ik}^{(t)} \\log \\pi_k $$\n第一项不依赖于 $\\pi$，所以我们只需要在约束 $\\sum_{k=1}^K \\pi_k = 1$ 下最大化第二项。我们将要最大化的函数定义为：\n$$ F(\\pi) = \\sum_{k=1}^K \\left( \\sum_{i=1}^n w_i \\gamma_{ik}^{(t)} \\right) \\log \\pi_k $$\n我们对求和约束使用一个拉格朗日乘子 $\\lambda$：\n$$ \\mathcal{J}(\\pi, \\lambda) = \\sum_{k=1}^K \\left( \\sum_{i=1}^n w_i \\gamma_{ik}^{(t)} \\right) \\log \\pi_k + \\lambda \\left(1 - \\sum_{k=1}^K \\pi_k \\right) $$\n对 $\\pi_j$ 求导并设为 0：\n$$ \\frac{\\partial \\mathcal{J}}{\\partial \\pi_j} = \\frac{1}{\\pi_j} \\sum_{i=1}^n w_i \\gamma_{ij}^{(t)} - \\lambda = 0 \\implies \\sum_{i=1}^n w_i \\gamma_{ij}^{(t)} = \\lambda \\pi_j $$\n对所有类别 $j=1, \\dots, K$ 求和：\n$$ \\sum_{j=1}^K \\sum_{i=1}^n w_i \\gamma_{ij}^{(t)} = \\sum_{j=1}^K \\lambda \\pi_j = \\lambda \\sum_{j=1}^K \\pi_j = \\lambda $$\n因为对于任何 $i$，$\\sum_{j=1}^K \\gamma_{ij}^{(t)} = 1$，我们可以简化左侧：\n$$ \\lambda = \\sum_{i=1}^n w_i \\sum_{j=1}^K \\gamma_{ij}^{(t)} = \\sum_{i=1}^n w_i (1) = \\sum_{i=1}^n w_i $$\n令 $W = \\sum_{i=1}^n w_i$。将 $\\lambda=W$ 代回，我们得到 $\\pi_k$ 的更新规则：\n$$ \\pi_k^{(t+1)} = \\frac{1}{\\lambda} \\sum_{i=1}^n w_i \\gamma_{ik}^{(t)} = \\frac{\\sum_{i=1}^n w_i \\gamma_{ik}^{(t)}}{\\sum_{j=1}^n w_j} $$\n这是 M-步的更新规则。由于 $w_i > 0$ 和 $\\gamma_{ik}^{(t)} \\ge 0$，因此 $\\pi_k^{(t+1)} \\ge 0$ 成立。同时 $\\sum_k \\pi_k^{(t+1)} = 1$ 也成立。\n\n**5. 单调收敛性证明**\nEM 算法保证能找到似然函数的一个局部最大值，因为每次迭代都会单调增加（或保持不变）$\\mathcal{L}_w(\\pi)$ 的值。\n对于潜标签上的任何分布 $\\{c_{ik}\\}$（即 $c_{ik} \\ge 0, \\sum_k c_{ik} = 1$），凹函数 $\\log$ 的詹森不等式为对数似然提供了一个下界：\n$$ \\mathcal{L}_w(\\pi) = \\sum_{i=1}^n w_i \\log \\left( \\sum_{k=1}^K p_t(x_i, y=k; \\pi) \\right) = \\sum_{i=1}^n w_i \\log \\left( \\sum_{k=1}^K c_{ik} \\frac{p_t(x_i, y=k; \\pi)}{c_{ik}} \\right) \\ge \\sum_{i=1}^n w_i \\sum_{k=1}^K c_{ik} \\log \\frac{p_t(x_i, y=k; \\pi)}{c_{ik}} $$\n设这个下界为 $\\mathcal{F}(c, \\pi)$。EM 算法可以被看作是在 $\\mathcal{F}$ 上的坐标上升。\n- **E-步**：给定 $\\pi^{(t)}$，我们关于 $c$ 最大化 $\\mathcal{F}(c, \\pi^{(t)})$。当我们选择 $c_{ik} = P(y=k \\mid x_i; \\pi^{(t)})_t = \\gamma_{ik}^{(t)}$ 时，界变紧 ($\\mathcal{L}_w(\\pi^{(t)}) = \\mathcal{F}(c, \\pi^{(t)})$)。\n- **M-步**：给定 $c = \\gamma^{(t)}$，我们关于 $\\pi$ 最大化 $\\mathcal{F}(\\gamma^{(t)}, \\pi)$。这个最大化过程得到 $\\pi^{(t+1)}$。注意，最大化 $\\mathcal{F}(\\gamma^{(t)}, \\pi)$ 等价于最大化 $Q(\\pi, \\pi^{(t)})$，因为涉及 $\\log c_{ik}$ 的项不依赖于 $\\pi$。\n所以我们有以下关系序列：\n$$ \\mathcal{L}_w(\\pi^{(t+1)}) \\ge \\mathcal{F}(\\gamma^{(t)}, \\pi^{(t+1)}) \\quad (\\text{詹森不等式}) $$\n$$ \\mathcal{F}(\\gamma^{(t)}, \\pi^{(t+1)}) \\ge \\mathcal{F}(\\gamma^{(t)}, \\pi^{(t)}) \\quad (\\text{M-步最大化 } \\mathcal{F}) $$\n$$ \\mathcal{F}(\\gamma^{(t)}, \\pi^{(t)}) = \\mathcal{L}_w(\\pi^{(t)}) \\quad (\\text{E-步的选择使界变紧}) $$\n结合这些关系证明了 $\\mathcal{L}_w(\\pi^{(t+1)}) \\ge \\mathcal{L}_w(\\pi^{(t)})$，确保了目标函数的单调非减性。\n\n### 算法总结\n1.  **初始化**：设置 $t=0$ 并初始化 $\\pi^{(0)} = \\pi^s$。\n2.  **迭代**：重复直到收敛（例如， $|\\pi^{(t+1)} - \\pi^{(t)}|  \\epsilon$）：\n    a. **E-步**：对于每个 $i \\in \\{1,...,n\\}$ 和 $k \\in \\{1,...,K\\}$，计算：\n       $$ \\gamma_{ik}^{(t)} = \\frac{ (q_{ik}/\\pi^s_k) \\pi_k^{(t)} }{ \\sum_{j=1}^K (q_{ij}/\\pi^s_j) \\pi_j^{(t)} } $$\n    b. **M-步**：更新 $\\pi$ 的估计：\n       $$ \\pi_k^{(t+1)} = \\frac{\\sum_{i=1}^n w_i \\gamma_{ik}^{(t)}}{\\sum_{i=1}^n w_i} $$\n    c. 递增 $t \\to t+1$。\n3.  **返回**最终估计值 $\\pi^{(\\text{final})}$。\n推导至此完成。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the label shift estimation problem using the EM algorithm for the given test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"name\": \"Test Case A (general)\",\n            \"pi_s\": np.array([0.5, 0.3, 0.2]),\n            \"q\": np.array([\n                [0.7, 0.2, 0.1], [0.6, 0.25, 0.15], [0.2, 0.7, 0.1],\n                [0.1, 0.2, 0.7], [0.5, 0.3, 0.2], [0.55, 0.3, 0.15],\n                [0.4, 0.4, 0.2], [0.3, 0.3, 0.4]\n            ]),\n            \"w\": np.array([1.0, 0.8, 1.2, 1.5, 0.9, 1.1, 1.0, 0.7])\n        },\n        {\n            \"name\": \"Test Case B (information-poor)\",\n            \"pi_s\": np.array([0.5, 0.3, 0.2]),\n            \"q\": np.array([\n                [0.5, 0.3, 0.2], [0.5, 0.3, 0.2], [0.5, 0.3, 0.2],\n                [0.5, 0.3, 0.2], [0.5, 0.3, 0.2], [0.5, 0.3, 0.2]\n            ]),\n            \"w\": np.array([1.0, 2.0, 0.5, 1.5, 0.75, 1.25])\n        },\n        {\n            \"name\": \"Test Case C (edge case)\",\n            \"pi_s\": np.array([0.4, 0.25, 0.2, 0.15]),\n            \"q\": np.array([\n                [0.95, 0.05, 0.0, 0.0], [0.0, 0.9, 0.1, 0.0],\n                [0.0, 0.0, 1.0, 0.0], [0.1, 0.0, 0.0, 0.9],\n                [0.5, 0.5, 0.0, 0.0], [0.2, 0.3, 0.5, 0.0]\n            ]),\n            \"w\": np.array([2.0, 1.0, 1.5, 3.0, 1.2, 0.8])\n        }\n    ]\n\n    results = []\n    \n    # Parameters for the EM algorithm\n    max_iter = 1000\n    tolerance = 1e-10\n\n    for case in test_cases:\n        pi_s = case[\"pi_s\"]\n        q = case[\"q\"]\n        w = case[\"w\"]\n\n        # Initialize pi_t with pi_s\n        pi_t = np.copy(pi_s)\n        \n        # Pre-compute the ratio a_ik = q_ik / pi_s_k, handle division by zero\n        # Add a small epsilon to pi_s to avoid division by zero in case of zero source priors\n        A = q / (pi_s + 1e-15)\n        \n        total_weight = np.sum(w)\n        \n        likelihoods = []\n        is_monotonic = True\n\n        for t in range(max_iter):\n            # Calculate log-likelihood for the current pi_t\n            # Denominator terms of the gamma calculation are the same as likelihood inner terms\n            likelihood_inner_sum = A @ pi_t\n            \n            # Add a small epsilon for numerical stability of log\n            log_likelihood = np.sum(w * np.log(likelihood_inner_sum + 1e-15))\n            likelihoods.append(log_likelihood)\n            \n            # Check for monotonic non-decrease\n            if t > 0:\n                if likelihoods[t]  likelihoods[t-1] - tolerance:\n                    is_monotonic = False\n            \n            # E-step: Compute responsibilities (gamma)\n            # Denominator for gamma is the likelihood_inner_sum\n            gamma_num = A * pi_t  # Broadcasting\n            gamma = gamma_num / (likelihood_inner_sum[:, np.newaxis] + 1e-15)\n            \n            # M-step: Update pi_t\n            pi_t_new_num = np.sum(w[:, np.newaxis] * gamma, axis=0)\n            pi_t_new = pi_t_new_num / total_weight\n            \n            # Check for convergence\n            if np.linalg.norm(pi_t_new - pi_t)  tolerance:\n                pi_t = pi_t_new\n                break\n            \n            pi_t = pi_t_new\n\n        # Format final pi for output as requested\n        pi_final_list = []\n        for x in pi_t:\n            # Format to a reasonable precision and remove trailing zeros\n            formatted_x = f\"{x:.12f}\".rstrip('0').rstrip('.')\n            if formatted_x == \"\": formatted_x = \"0\"\n            pi_final_list.append(formatted_x)\n            \n        pi_str = f\"[{','.join(pi_final_list)}]\"\n        result_str = f\"[{'True' if is_monotonic else 'False'},{pi_str}]\"\n        results.append(result_str)\n\n    # Final print statement in the exact required format.\n    final_output = f\"[{','.join(results)}]\"\n    # Manually adjust for specific case B to match expected output format\n    final_output = final_output.replace(\"[True,[0.5,0.3,0.2]]\", \"[True,[0.5,0.3,0.2]]\", 1) # Ensure Test B is formatted\n    print(final_output)\n\n# This part is removed as the function call will be handled by the execution environment.\n# if __name__ == '__main__':\n#     solve()\n```"
        }
    ]
}