{
    "hands_on_practices": [
        {
            "introduction": "This first practice lays the theoretical groundwork for sequential drift detection. You will derive the classic Cumulative Sum (CUSUM) control chart from first principles, starting with the fundamental concept of the log-likelihood ratio for a shift in the mean of Gaussian residuals. This exercise is crucial for understanding how to optimally accumulate evidence of change over time and for connecting the detection threshold to the in-control Average Run Length ($ARL$), a key performance metric for any monitoring procedure .",
            "id": "4231270",
            "problem": "A digital twin for a cyber-physical system executes a one-step-ahead predictor and monitors the innovation residuals $\\{r_t\\}_{t \\geq 1}$. Under a correctly specified model (no drift), assume $\\{r_t\\}$ are independent and identically distributed as Gaussian with zero mean and known variance, that is, $r_t \\sim \\mathcal{N}(0,\\sigma^2)$ with known $\\sigma^2$. A model drift corresponding to a constant additive bias causes a shift in the mean to a known design value $\\mu>0$, so that post-change residuals follow $r_t \\sim \\mathcal{N}(\\mu,\\sigma^2)$ while the variance remains $\\sigma^2$. You will use a one-sided Cumulative Sum (CUSUM) procedure to detect this shift in the positive direction.\n\nStarting only from the probability density function of the Gaussian distribution and the definition of a likelihood ratio, perform the following:\n\n1. Derive the single-sample log-likelihood ratio $\\ell(r_t)$ for testing the null hypothesis $H_0: r_t \\sim \\mathcal{N}(0,\\sigma^2)$ versus the alternative hypothesis $H_1: r_t \\sim \\mathcal{N}(\\mu,\\sigma^2)$, expressed explicitly as a function of $r_t$, $\\mu$, and $\\sigma^2$ using the natural logarithm.\n\n2. Using the principle of sequential change detection and reflection at zero, derive the one-sided CUSUM recursion that accumulates $\\ell(r_t)$ and resets to zero when the partial sum becomes negative. Define the stopping time $T_h$ as the first index $t$ at which the CUSUM statistic exceeds a fixed threshold $h$ (measured in natural logarithm units).\n\n3. Using Wald-type approximations grounded in the moment generating function and ignoring overshoot corrections, compute the in-control Average Run Length $\\mathrm{ARL}_0 \\triangleq \\mathbb{E}_0[T_h]$ for the derived CUSUM, where the subscript $0$ denotes expectation under $H_0$. For the purposes of this computation, take the design parameters $\\mu = 0.5$, $\\sigma = 1$, and threshold $h = 8$. Round your final numerical answer for $\\mathrm{ARL}_0$ to four significant figures and express it in samples.",
            "solution": "The problem requires a three-part analysis of a one-sided Cumulative Sum (CUSUM) procedure for detecting a shift in the mean of a Gaussian process. The analysis will proceed by first validating the problem statement, which has been found to be scientifically sound, well-posed, and objective. We will now derive the necessary components and compute the required quantity.\n\nFirst, we derive the single-sample log-likelihood ratio, $\\ell(r_t)$, for testing the null hypothesis $H_0$ against the alternative $H_1$. The probability density function (PDF) of a Gaussian distribution with mean $\\theta$ and variance $\\sigma^2$ is given by\n$$f(x; \\theta, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x - \\theta)^2}{2\\sigma^2}\\right)$$\nUnder the null hypothesis $H_0$, the residuals $r_t$ are distributed as $r_t \\sim \\mathcal{N}(0, \\sigma^2)$, so their PDF is\n$$f_0(r_t) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{r_t^2}{2\\sigma^2}\\right)$$\nUnder the alternative hypothesis $H_1$, the residuals $r_t$ are distributed as $r_t \\sim \\mathcal{N}(\\mu, \\sigma^2)$, with the PDF\n$$f_1(r_t) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(r_t - \\mu)^2}{2\\sigma^2}\\right)$$\nThe likelihood ratio $L(r_t)$ is the ratio of these two densities:\n$$L(r_t) = \\frac{f_1(r_t)}{f_0(r_t)} = \\frac{\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(r_t - \\mu)^2}{2\\sigma^2}\\right)}{\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{r_t^2}{2\\sigma^2}\\right)} = \\exp\\left(\\frac{r_t^2 - (r_t - \\mu)^2}{2\\sigma^2}\\right)$$\nThe exponent can be simplified:\n$$r_t^2 - (r_t - \\mu)^2 = r_t^2 - (r_t^2 - 2\\mu r_t + \\mu^2) = 2\\mu r_t - \\mu^2$$\nSubstituting this back into the expression for $L(r_t)$ gives\n$$L(r_t) = \\exp\\left(\\frac{2\\mu r_t - \\mu^2}{2\\sigma^2}\\right)$$\nThe single-sample log-likelihood ratio, $\\ell(r_t)$, is the natural logarithm of $L(r_t)$:\n$$\\ell(r_t) = \\ln(L(r_t)) = \\frac{2\\mu r_t - \\mu^2}{2\\sigma^2} = \\frac{\\mu}{\\sigma^2}r_t - \\frac{\\mu^2}{2\\sigma^2}$$\nThis is the required expression for the log-likelihood ratio increment.\n\nSecond, we define the one-sided CUSUM recursion and the associated stopping time. The CUSUM statistic, denoted by $S_t$, accumulates the log-likelihood ratio increments $\\ell(r_t)$ over time. The \"reflection at zero\" principle means the cumulative sum is not allowed to become negative; if it does, it is reset to zero. This is captured by the following recursion:\n$$S_0 = 0$$\n$$S_t = \\max(0, S_{t-1} + \\ell(r_t)) \\quad \\text{for } t \\geq 1$$\nThe detection of a change occurs when the statistic $S_t$ first exceeds a predefined threshold $h$. The stopping time, $T_h$, is formally defined as:\n$$T_h \\triangleq \\inf\\{t \\geq 1 : S_t > h\\}$$\n\nThird, we compute the in-control Average Run Length, $\\mathrm{ARL}_0 = \\mathbb{E}_0[T_h]$, using a Wald-type approximation. This approximation is based on large deviation theory and renewal arguments. Under the in-control hypothesis $H_0$, the expected value of the increment $\\ell(r_t)$ is negative:\n$$\\mathbb{E}_0[\\ell(r_t)] = \\mathbb{E}_0\\left[\\frac{\\mu}{\\sigma^2}r_t - \\frac{\\mu^2}{2\\sigma^2}\\right] = \\frac{\\mu}{\\sigma^2}\\mathbb{E}_0[r_t] - \\frac{\\mu^2}{2\\sigma^2} = \\frac{\\mu}{\\sigma^2}(0) - \\frac{\\mu^2}{2\\sigma^2} = -\\frac{\\mu^2}{2\\sigma^2}$$\nDue to this negative drift, the statistic $S_t$ will frequently reset to $0$. The process of reaching the threshold $h$ can be viewed as a sequence of renewal cycles, where each cycle has a small probability $p$ of resulting in a false alarm (i.e., crossing $h$). The number of samples until this event, $T_h$, is approximately geometrically distributed with mean $1/p$. Thus, $\\mathrm{ARL}_0 \\approx 1/p$.\n\nThe probability $p$ that the process, starting from $0$, ever crosses the boundary $h$ can be approximated using a result related to Wald's identity and Cramér's theorem. The approximation is $p \\approx \\exp(-\\theta^* h)$, where $\\theta^*$ is the non-zero solution to the equation $\\mathbb{E}_0[\\exp(\\theta \\ell(r_t))] = 1$. Let us find this $\\theta^*$.\nThe moment generating function (MGF) of $\\ell(r_t)$ under $H_0$ is:\n$$\\psi_0(\\theta) = \\mathbb{E}_0[\\exp(\\theta \\ell(r_t))] = \\mathbb{E}_0\\left[\\exp\\left(\\theta \\left(\\frac{\\mu r_t}{\\sigma^2} - \\frac{\\mu^2}{2\\sigma^2}\\right)\\right)\\right] = \\exp\\left(-\\frac{\\theta \\mu^2}{2\\sigma^2}\\right) \\mathbb{E}_0\\left[\\exp\\left(\\frac{\\theta \\mu}{\\sigma^2}r_t\\right)\\right]$$\nThe term inside the expectation is the MGF of a normal random variable $r_t \\sim \\mathcal{N}(0, \\sigma^2)$ evaluated at the point $k = \\frac{\\theta \\mu}{\\sigma^2}$. The MGF of $\\mathcal{N}(m, s^2)$ is $\\exp(mk + s^2k^2/2)$. For our case, $m=0$ and $s^2=\\sigma^2$, so:\n$$\\mathbb{E}_0\\left[\\exp\\left(\\frac{\\theta \\mu}{\\sigma^2}r_t\\right)\\right] = \\exp\\left(0 \\cdot \\frac{\\theta \\mu}{\\sigma^2} + \\frac{\\sigma^2}{2}\\left(\\frac{\\theta \\mu}{\\sigma^2}\\right)^2\\right) = \\exp\\left(\\frac{\\sigma^2 \\theta^2 \\mu^2}{2\\sigma^4}\\right) = \\exp\\left(\\frac{\\theta^2 \\mu^2}{2\\sigma^2}\\right)$$\nSubstituting this back into the expression for $\\psi_0(\\theta)$:\n$$\\psi_0(\\theta) = \\exp\\left(-\\frac{\\theta \\mu^2}{2\\sigma^2}\\right) \\exp\\left(\\frac{\\theta^2 \\mu^2}{2\\sigma^2}\\right) = \\exp\\left(\\frac{\\mu^2}{2\\sigma^2}(\\theta^2 - \\theta)\\right)$$\nWe seek the non-zero $\\theta^*$ that solves $\\psi_0(\\theta^*) = 1$. This implies the exponent must be zero:\n$$\\frac{\\mu^2}{2\\sigma^2}((\\theta^*)^2 - \\theta^*) = 0$$\nSince $\\mu > 0$ and $\\sigma^2 > 0$, this simplifies to $(\\theta^*)^2 - \\theta^* = 0$, or $\\theta^*(\\theta^* - 1) = 0$. The non-zero solution is $\\theta^* = 1$.\nWith $\\theta^* = 1$, the probability of a false alarm in a cycle is $p \\approx \\exp(-1 \\cdot h) = \\exp(-h)$.\nThe resulting approximation for the in-control Average Run Length is:\n$$\\mathrm{ARL}_0 \\approx \\frac{1}{p} \\approx \\frac{1}{\\exp(-h)} = \\exp(h)$$\nThis approximation is valid for large $h$ and when overshoot is ignored. The specific values $\\mu = 0.5$ and $\\sigma=1$ were not required for this general derivation, as $\\theta^*$ is independent of them.\nNow, we use the given threshold $h=8$ to compute the numerical value:\n$$\\mathrm{ARL}_0 \\approx \\exp(8)$$\nCalculating the value of $\\exp(8)$:\n$$\\exp(8) \\approx 2980.957987...$$\nRounding this to four significant figures yields $2981$. The result is expressed in units of samples.",
            "answer": "$$\\boxed{2981}$$"
        },
        {
            "introduction": "Real-world cyber-physical systems often produce residuals that are not independent over time, violating a key assumption of simple CUSUM. This practice addresses this common challenge by introducing the concept of residual whitening for data with autoregressive (AR) structure. By learning to transform autocorrelated residuals into a sequence of independent innovations, you will see how to apply the powerful CUSUM framework to a much broader and more realistic class of monitoring problems .",
            "id": "4231206",
            "problem": "A digital twin for a cyber-physical system produces one-step-ahead predictions $\\hat{y}_t$ of sensor measurements $y_t$, and the residuals $r_t = y_t - \\hat{y}_t$ are monitored for model drift. Under nominal operation, empirical identification yields that the residuals obey an Autoregressive order-$1$ (AR($1$)) model $r_t = \\phi r_{t-1} + \\epsilon_t$ with $|\\phi| < 1$, where the innovations $\\epsilon_t$ are independent and identically distributed (i.i.d.) Gaussian $\\epsilon_t \\sim \\mathcal{N}(0, \\sigma_{\\epsilon}^{2})$. Under drift, the system’s bias shifts the innovations’ mean to $\\mu_{\\epsilon} > 0$ while their variance remains $\\sigma_{\\epsilon}^{2}$ and the autoregressive coefficient remains $\\phi$.\n\nTo detect upward mean shifts in the innovations, apply residual whitening by defining $w_t = (r_t - \\phi r_{t-1}) / \\sigma_{\\epsilon}$. Under nominal operation, $w_t \\sim \\mathcal{N}(0, 1)$ i.i.d., while under drift, $w_t \\sim \\mathcal{N}(\\delta, 1)$ i.i.d. with $\\delta = \\mu_{\\epsilon} / \\sigma_{\\epsilon} > 0$. Consider the one-sided Cumulative Sum (CUSUM) detector based on the log-likelihood ratio increments\n$$\n\\ell(w_t) = \\ln\\!\\left(\\frac{f_1(w_t)}{f_0(w_t)}\\right),\n$$\nwhere $f_0$ and $f_1$ denote the $\\mathcal{N}(0,1)$ and $\\mathcal{N}(\\delta, 1)$ densities. Initialize $S_0 = 0$ and update\n$$\nS_t = \\max\\{0,\\, S_{t-1} + \\ell(w_t)\\},\n$$\nraising an alarm when $S_t \\ge h$ with fixed threshold $h > 0$.\n\nStarting from fundamental definitions of the Gaussian log-likelihood ratio and the large-deviation characterization of boundary crossing for random walks with negative drift, derive an analytic expression for the threshold $h$ that achieves a target false alarm probability $\\alpha \\in (0,1)$ for a single surveillance cycle that starts at $S_0 = 0$ under the nominal regime (i.e., with $w_t \\sim \\mathcal{N}(0,1)$). Your derivation must begin from the definition of $\\ell(w_t)$ and proceed through the cumulant generating function under the nominal regime, culminating in the standard large-deviation approximation for the crossing probability of $S_t$. Express the final $h$ solely as a function of $\\alpha$. No rounding is required. Clearly state any independence of the final threshold with respect to the autoregressive parameter $\\phi$ and the scale $\\sigma_{\\epsilon}$ induced by whitening, if such independence arises from the derivation.",
            "solution": "The problem statement is evaluated as valid. It is scientifically grounded in the principles of statistical process control, time series analysis, and large-deviation theory. It is well-posed, providing all necessary definitions, distributions, and constraints to derive the requested quantity. The language is objective and mathematically precise. There are no contradictions, ambiguities, or missing information. We may therefore proceed with the derivation.\n\nThe objective is to derive an analytic expression for the CUSUM threshold $h$ that yields a specified false alarm probability $\\alpha$, starting from the fundamental definitions. A false alarm occurs under the nominal regime (the null hypothesis, $H_0$), where the whitened residuals $w_t$ are independent and identically distributed (i.i.d.) as $w_t \\sim \\mathcal{N}(0, 1)$.\n\nFirst, we derive the expression for the log-likelihood ratio increment, $\\ell(w_t)$. The probability density functions (PDFs) for the nominal regime ($f_0$) and the drift regime ($f_1$) are given by the standard normal and shifted normal distributions, respectively:\n$$\nf_0(w_t) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{w_t^2}{2}\\right) \\quad \\text{for } w_t \\sim \\mathcal{N}(0, 1)\n$$\n$$\nf_1(w_t) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{(w_t - \\delta)^2}{2}\\right) \\quad \\text{for } w_t \\sim \\mathcal{N}(\\delta, 1)\n$$\nThe log-likelihood ratio increment $\\ell(w_t)$ is defined as:\n$$\n\\ell(w_t) = \\ln\\left(\\frac{f_1(w_t)}{f_0(w_t)}\\right) = \\ln\\left( \\frac{\\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{(w_t - \\delta)^2}{2}\\right)}{\\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{w_t^2}{2}\\right)} \\right)\n$$\nSimplifying the expression within the logarithm:\n$$\n\\ell(w_t) = \\ln\\left( \\exp\\left(-\\frac{(w_t - \\delta)^2}{2} + \\frac{w_t^2}{2}\\right) \\right) = -\\frac{(w_t - \\delta)^2}{2} + \\frac{w_t^2}{2}\n$$\nExpanding the squared term:\n$$\n\\ell(w_t) = -\\frac{w_t^2 - 2\\delta w_t + \\delta^2}{2} + \\frac{w_t^2}{2} = -\\frac{w_t^2}{2} + \\delta w_t - \\frac{\\delta^2}{2} + \\frac{w_t^2}{2}\n$$\nThis simplifies to:\n$$\n\\ell(w_t) = \\delta w_t - \\frac{\\delta^2}{2}\n$$\nThe CUSUM statistic $S_t = \\max\\{0, S_{t-1} + \\ell(w_t)\\}$ is a random walk with increments $\\ell(w_t)$, reflected at $0$. To find the false alarm probability, we analyze its behavior under the nominal regime $H_0$, where $w_t \\sim \\mathcal{N}(0, 1)$. The expected value of the increments under $H_0$ is:\n$$\nE_0[\\ell(w_t)] = E_0\\left[\\delta w_t - \\frac{\\delta^2}{2}\\right] = \\delta E_0[w_t] - \\frac{\\delta^2}{2}\n$$\nSince $E_0[w_t] = 0$ under the nominal regime, the drift of the random walk is negative:\n$$\nE_0[\\ell(w_t)] = -\\frac{\\delta^2}{2} < 0\n$$\nAn alarm is triggered when $S_t \\ge h$. For a random walk with negative drift, this is a rare event. We use large-deviation theory to approximate its probability. The core of this method involves the cumulant generating function (CGF) of the increments $\\ell(w_t)$ under $H_0$, defined as $\\psi(\\theta) = \\ln(E_0[\\exp(\\theta \\ell(w_t))])$.\n$$\n\\psi(\\theta) = \\ln\\left( E_0\\left[\\exp\\left(\\theta \\left(\\delta w_t - \\frac{\\delta^2}{2}\\right)\\right)\\right] \\right) = \\ln\\left( \\exp\\left(-\\frac{\\theta\\delta^2}{2}\\right) E_0\\left[\\exp(\\theta \\delta w_t)\\right] \\right)\n$$\n$$\n\\psi(\\theta) = -\\frac{\\theta\\delta^2}{2} + \\ln\\left(E_0\\left[\\exp(\\theta \\delta w_t)\\right]\\right)\n$$\nThe term $E_0[\\exp(\\theta \\delta w_t)]$ is the moment-generating function (MGF) of a standard normal variable $w_t \\sim \\mathcal{N}(0, 1)$, evaluated at the point $\\theta\\delta$. The MGF of a general normal variable $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$ is $M_X(s) = \\exp(\\mu s + \\frac{1}{2}\\sigma^2 s^2)$. For $w_t \\sim \\mathcal{N}(0, 1)$, this is $M_{w_t}(s) = \\exp(\\frac{1}{2}s^2)$. Therefore:\n$$\nE_0\\left[\\exp(\\theta \\delta w_t)\\right] = M_{w_t}(\\theta\\delta) = \\exp\\left(\\frac{1}{2}(\\theta\\delta)^2\\right) = \\exp\\left(\\frac{\\theta^2\\delta^2}{2}\\right)\n$$\nSubstituting this back into the expression for the CGF:\n$$\n\\psi(\\theta) = -\\frac{\\theta\\delta^2}{2} + \\frac{\\theta^2\\delta^2}{2}\n$$\nThe large-deviation approximation for the crossing probability depends on finding the non-zero root $\\theta^*$ of the equation $\\psi(\\theta) = 0$.\n$$\n-\\frac{\\theta\\delta^2}{2} + \\frac{\\theta^2\\delta^2}{2} = 0\n$$\nSince $\\delta = \\mu_{\\epsilon}/\\sigma_{\\epsilon} > 0$, we can divide by $\\frac{\\delta^2}{2} \\neq 0$:\n$$\n-\\theta + \\theta^2 = 0 \\implies \\theta(\\theta - 1) = 0\n$$\nThe roots are $\\theta = 0$ (the trivial root) and $\\theta^* = 1$.\n\nThe standard large-deviation approximation for the probability $\\alpha$ that the CUSUM process $S_t$, starting from $S_0=0$, ever crosses the threshold $h$ is given by:\n$$\n\\alpha = P(\\sup_{t \\ge 1} S_t \\ge h) \\approx \\exp(-\\theta^* h)\n$$\nSubstituting the non-trivial root $\\theta^* = 1$ into this approximation, we get:\n$$\n\\alpha \\approx \\exp(-h)\n$$\nSolving for the threshold $h$ by taking the natural logarithm of both sides:\n$$\n\\ln(\\alpha) = -h\n$$\n$$\nh = -\\ln(\\alpha)\n$$\nThis expression gives the threshold $h$ solely as a function of the target false alarm probability $\\alpha$.\n\nIt is crucial to note the independence of this result. The final expression for $h$ depends only on $\\alpha$. It is independent of the autoregressive parameter $\\phi$, the innovation standard deviation $\\sigma_{\\epsilon}$, and the magnitude of the shift to be detected $\\delta = \\mu_{\\epsilon}/\\sigma_{\\epsilon}$. This independence arises from the two key steps in the problem formulation:\n1.  **Residual Whitening**: The transformation $w_t = (r_t - \\phi r_{t-1}) / \\sigma_{\\epsilon}$ normalizes the problem by design. It removes the effects of the autocorrelation (parameterized by $\\phi$) and scaling (parameterized by $\\sigma_{\\epsilon}$), creating a standardized i.i.d. sequence $w_t \\sim \\mathcal{N}(0, 1)$.\n2.  **Log-Likelihood Ratio Increments**: The specific form of the LLR increments, $\\ell(w_t) = \\delta w_t - \\frac{\\delta^2}{2}$, leads to a CGF whose non-trivial root $\\theta^*$ is exactly $1$, independent of $\\delta$. This is a special property of using LLRs for exponential family distributions, which ensures optimality in certain senses (e.g., minimizing detection delay for a given false alarm rate).\n\nThus, the whitening procedure makes the test robust to the specific nominal dynamics ($\\phi$, $\\sigma_{\\epsilon}$), and the use of LLR increments makes the false alarm probability tuning independent of the specific alternative hypothesis ($\\delta$).",
            "answer": "$$\\boxed{-\\ln(\\alpha)}$$"
        },
        {
            "introduction": "Moving beyond the detection of simple parametric shifts, this exercise tackles a more complex and structured form of model drift known as label shift. You will derive the Black Box Shift Estimator (BBSE), a powerful technique that estimates changes in the underlying class distribution of a system using only the outputs of a fixed classifier. This practice is essential for learning how to diagnose and quantify drift in classification settings without needing to retrain or even access the internal workings of the model, paving the way for targeted adaptation strategies .",
            "id": "4231216",
            "problem": "A digital twin of a manufacturing Cyber-Physical System (CPS) monitors a multi-class process state using a fixed classifier with $K$ classes. The digital twin is deployed under a source environment with joint distribution $P_{s}(x, y)$ and later receives unlabeled data from a target environment with joint distribution $P_{t}(x, y)$. Assume the following formal label shift condition: $P_{t}(x \\mid y) = P_{s}(x \\mid y)$ for all classes $y \\in \\{1, \\dots, K\\}$, but $P_{t}(y)$ may differ from $P_{s}(y)$. Let the classifier’s predicted label be $\\hat{y} \\in \\{1, \\dots, K\\}$, produced by a fixed mapping from $x$ that does not change between source and target.\n\nThe source environment provides access to labeled data to estimate the classifier’s confusion matrix $C \\in \\mathbb{R}^{K \\times K}$ with entries $C_{ij} = P_{s}(\\hat{y} = i \\mid y = j)$. The target environment provides only unlabeled data, from which the digital twin can estimate the prediction marginal vector $\\boldsymbol{q}_{t} \\in \\mathbb{R}^{K}$ with entries $[\\boldsymbol{q}_{t}]_{i} = P_{t}(\\hat{y} = i)$. Define the source class prior vector $\\boldsymbol{\\mu}_{s} \\in \\mathbb{R}^{K}$ with entries $[\\boldsymbol{\\mu}_{s}]_{j} = P_{s}(y = j)$, and the target class prior vector $\\boldsymbol{\\mu}_{t} \\in \\mathbb{R}^{K}$ with entries $[\\boldsymbol{\\mu}_{t}]_{j} = P_{t}(y = j)$.\n\nStarting from the law of total probability and the formal definition of label shift, derive the Black Box Shift Estimator (BBSE) that expresses the target class prior vector $\\boldsymbol{\\mu}_{t}$ as a function of the confusion matrix $C$ and the target prediction marginal vector $\\boldsymbol{q}_{t}$. Then, state the necessary and sufficient identifiability conditions under which $\\boldsymbol{\\mu}_{t}$ is uniquely determined by $C$ and $\\boldsymbol{q}_{t}$ in this CPS digital twin setting. Your final answer must be a single closed-form analytic expression for $\\boldsymbol{\\mu}_{t}$ in terms of $C$ and $\\boldsymbol{q}_{t}$. No numerical computation is required, and no rounding is needed. Express the final answer as a symbolic expression without units.",
            "solution": "The objective is to derive an expression for the target class prior vector, $\\boldsymbol{\\mu}_{t}$, in terms of the source confusion matrix, $C$, and the target prediction marginal vector, $\\boldsymbol{q}_{t}$, under the label shift assumption. We will also state the conditions under which this estimation is uniquely possible.\n\nLet's begin with the law of total probability for the marginal probability of a prediction $\\hat{y} = i$ in the target domain. This is given by summing over all possible true classes $y=j$:\n$$\nP_{t}(\\hat{y} = i) = \\sum_{j=1}^{K} P_{t}(\\hat{y} = i, y = j)\n$$\nUsing the definition of conditional probability, $P(A, B) = P(A \\mid B) P(B)$, we can rewrite this as:\n$$\nP_{t}(\\hat{y} = i) = \\sum_{j=1}^{K} P_{t}(\\hat{y} = i \\mid y = j) P_{t}(y = j)\n$$\nNow, we will substitute the terms defined in the problem statement into this equation.\nThe $i$-th entry of the target prediction marginal vector $\\boldsymbol{q}_{t}$ is $[\\boldsymbol{q}_{t}]_{i} = P_{t}(\\hat{y} = i)$.\nThe $j$-th entry of the target class prior vector $\\boldsymbol{\\mu}_{t}$ is $[\\boldsymbol{\\mu}_{t}]_{j} = P_{t}(y = j)$.\nSubstituting these into our equation yields:\n$$\n[\\boldsymbol{q}_{t}]_{i} = \\sum_{j=1}^{K} P_{t}(\\hat{y} = i \\mid y = j) [\\boldsymbol{\\mu}_{t}]_{j}\n$$\nThe next step is to evaluate the term $P_{t}(\\hat{y} = i \\mid y = j)$. The problem states the formal label shift condition, $P_{t}(x \\mid y) = P_{s}(x \\mid y)$, and that the classifier is a fixed mapping from the feature space $x$ to a predicted label $\\hat{y}$. Let this mapping be denoted by the function $h$, so $\\hat{y}=h(x)$. The conditional probability of a prediction $\\hat{y}=i$ given a true label $y=j$ is the probability that a feature vector $x$ drawn from the distribution $P(x \\mid y=j)$ is mapped to $i$ by the classifier. This can be expressed as an integral over the subset of the feature space where $h(x)=i$:\n$$\nP(\\hat{y}=i \\mid y=j) = \\int_{\\{x \\mid h(x)=i\\}} P(x \\mid y=j) dx\n$$\nSince the function $h(x)$ is fixed and the conditional feature distribution is invariant between the source and target domains ($P_{t}(x \\mid y=j) = P_{s}(x \\mid y=j)$), it follows directly that the conditional prediction distribution is also invariant:\n$$\nP_{t}(\\hat{y} = i \\mid y = j) = P_{s}(\\hat{y} = i \\mid y = j)\n$$\nThe problem defines the source confusion matrix $C$ to have entries $C_{ij} = P_{s}(\\hat{y} = i \\mid y = j)$. Therefore, we have:\n$$\nP_{t}(\\hat{y} = i \\mid y = j) = C_{ij}\n$$\nSubstituting this back into our equation for $[\\boldsymbol{q}_{t}]_{i}$:\n$$\n[\\boldsymbol{q}_{t}]_{i} = \\sum_{j=1}^{K} C_{ij} [\\boldsymbol{\\mu}_{t}]_{j}\n$$\nThis equation holds for each prediction class $i \\in \\{1, \\dots, K\\}$. This system of $K$ linear equations can be expressed concisely in matrix-vector form. The right-hand side is the definition of the matrix-vector product $C \\boldsymbol{\\mu}_{t}$. Thus, we have:\n$$\n\\boldsymbol{q}_{t} = C \\boldsymbol{\\mu}_{t}\n$$\nTo solve for the unknown target class prior vector $\\boldsymbol{\\mu}_{t}$, we need to isolate it. If the confusion matrix $C$ is invertible, we can left-multiply both sides by its inverse, $C^{-1}$:\n$$\nC^{-1} \\boldsymbol{q}_{t} = C^{-1} C \\boldsymbol{\\mu}_{t}\n$$\n$$\nC^{-1} \\boldsymbol{q}_{t} = I \\boldsymbol{\\mu}_{t}\n$$\n$$\n\\boldsymbol{\\mu}_{t} = C^{-1} \\boldsymbol{q}_{t}\n$$\nThis expression is the Black Box Shift Estimator (BBSE).\n\nThe second part of the task is to state the necessary and sufficient identifiability conditions. The target class prior vector $\\boldsymbol{\\mu}_{t}$ is uniquely determined by $C$ and $\\boldsymbol{q}_{t}$ if and only if the system of linear equations $\\boldsymbol{q}_{t} = C \\boldsymbol{\\mu}_{t}$ has a unique solution for $\\boldsymbol{\\mu}_{t}$. From linear algebra, a unique solution exists if and only if the matrix $C$ is invertible (or non-singular). An invertible matrix has a non-zero determinant, $\\det(C) \\neq 0$, and its column vectors (and row vectors) are linearly independent.\n\nIn the context of this problem, the $j$-th column of $C$ is the vector of conditional probabilities of classifier predictions given the true class is $j$. If the columns of $C$ are not linearly independent, it implies that the classifier's responses to different true classes are statistically confounded, making it impossible to uniquely disentangle the contributions of each true class to the observed prediction marginals $\\boldsymbol{q}_{t}$. Therefore, the necessary and sufficient condition for the identifiability of $\\boldsymbol{\\mu}_{t}$ is the invertibility of the confusion matrix $C$.",
            "answer": "$$\\boxed{C^{-1} \\boldsymbol{q}_{t}}$$"
        }
    ]
}