{
    "hands_on_practices": [
        {
            "introduction": "A high-fidelity digital twin can involve dozens or even hundreds of uncertain parameters, making a comprehensive uncertainty analysis computationally intractable. Before undertaking a detailed quantitative analysis, it is essential to perform a sensitivity analysis to screen for the most influential inputs. This practice guides you through the implementation of the Method of Morris, an efficient global sensitivity analysis technique perfect for identifying the \"vital few\" parameters that drive most of the model's output variance . By building the algorithm from first principles, you will gain a deep, practical understanding of how to design and execute screening experiments for complex, high-dimensional systems.",
            "id": "4253088",
            "problem": "You are tasked with designing and executing a One-At-a-Time Morris screening experiment to identify influential inputs in a high-dimensional digital twin model under parametric uncertainty. Your design and implementation must strictly follow a finite-difference basis and be self-contained. All inputs are normalized to the unit hypercube $[0,1]^d$, all trigonometric arguments are in radians, and all outputs are unitless. The experiment must be implemented as a complete, runnable program.\n\nThe fundamental basis you must use is the following:\n- The elementary effect for input $i$ is a finite-difference quotient based on a one-dimensional perturbation while holding all other coordinates fixed. If $f$ is the model, $x \\in [0,1]^d$ is a base point, and $\\delta \\in (0,1)$ is a grid-aligned step size, then the effect of input $i$ at $x$ is defined as the change induced by moving only the $i$-th coordinate by $\\pm \\delta$, scaled by the step magnitude. The aggregated Morris statistics per input $i$ are computed from repeated trajectories across the input space.\n- The experimental design must use axis-aligned one-at-a-time trajectories of length $d$ on a uniform grid with $p$ levels in each dimension. The grid is $\\{0, \\frac{1}{p-1}, \\ldots, 1\\}$ and the step size $\\delta$ must equal $\\frac{p}{2(p-1)}$ so that each move stays on the grid and inside the unit hypercube when $p$ is even.\n\nYou must implement the following precise procedure:\n1. For each test case with dimension $d$, number of levels $p$ (an even integer), and number of trajectories $r$, define the grid with $p$ uniformly spaced levels in each coordinate and the step size $\\delta = \\frac{p}{2(p-1)}$.\n2. For each trajectory $k \\in \\{1,\\ldots,r\\}$:\n   - Sample an orientation vector $s \\in \\{-1,+1\\}^d$ with independent and identically distributed Rademacher entries.\n   - For each coordinate $i \\in \\{1,\\ldots,d\\}$, select a grid-aligned base index that guarantees the one-step move $x_i \\mapsto x_i + s_i \\delta$ stays in $[0,1]$. Formally, if $s_i=+1$, select $x_i$ from $\\{0,\\frac{1}{p-1},\\ldots,1-\\delta\\}$; if $s_i=-1$, select $x_i$ from $\\{\\delta,\\delta+\\frac{1}{p-1},\\ldots,1\\}$. Then scale to $[0,1]$.\n   - Sample a random permutation $\\pi$ of $\\{1,\\ldots,d\\}$ independently of $s$ and of previous trajectories.\n   - Construct a trajectory of $d+1$ points by sequentially moving one coordinate at a time according to the order $\\pi$ and the signs $s$, starting from the base point.\n   - Evaluate the model $f$ at each point of the trajectory, producing $d+1$ outputs. For each step along the trajectory corresponding to input $i$, compute one elementary effect sample for input $i$ using the finite-difference quotient\n     $$\\mathrm{EE}_{i} = \\frac{f(x^{\\text{after}}) - f(x^{\\text{before}})}{s_{i}\\,\\delta}.$$\n3. For each input $i$, aggregate its $r$ elementary effects across the $r$ trajectories and compute:\n   - The absolute mean\n     $$\\mu^{\\star}_i = \\frac{1}{r}\\sum_{k=1}^{r} \\left|\\mathrm{EE}_{i}^{(k)}\\right|.$$\n   - The standard deviation (population version)\n     $$\\sigma_i = \\sqrt{\\frac{1}{r}\\sum_{k=1}^{r} \\left(\\mathrm{EE}_{i}^{(k)} - \\bar{\\mathrm{EE}}_i\\right)^2}, \\quad \\text{where } \\bar{\\mathrm{EE}}_i = \\frac{1}{r}\\sum_{k=1}^{r} \\mathrm{EE}_{i}^{(k)}.$$\n\nRandomness control:\n- Use a single global pseudorandom number generator seeded with the integer $42$ and use it for all random draws across all test cases in the order they are given, so that outputs are exactly reproducible.\n\nTest Suite:\nImplement and evaluate the Morris screening on the following three test cases. In all cases, the model input is $x=(x_1,\\ldots,x_d)\\in[0,1]^d$.\n\n- Test Case A (moderate dimension with interactions and curvature):\n  - Parameters: $d=6$, $p=6$, $r=12$.\n  - Model:\n    $$f_{\\mathrm{A}}(x) = 5\\,x_1 + 0.5\\,x_2^2 + 20\\,x_3^2 + 3\\,x_4 x_5 + 0.5\\,\\sin(2\\pi x_6).$$\n    Angles are in radians.\n\n- Test Case B (higher dimension with sparse strong effects and weak nuisances):\n  - Parameters: $d=12$, $p=8$, $r=10$.\n  - Model:\n    $$f_{\\mathrm{B}}(x) = 8\\,x_1 + 4\\,x_2^2 + 0.5\\,x_3 + 10\\,x_4 x_5 + 0.1\\,x_6 + 0.05\\,x_7 + 9\\,x_8^2 + 2\\,x_9 x_{10} + 0.2\\,\\sin(2\\pi x_{11}) + 0.01\\,x_{12}.$$\n    Angles are in radians.\n\n- Test Case C (edge case with a discontinuity to probe spread of effects):\n  - Parameters: $d=4$, $p=4$, $r=8$.\n  - Model with a Heaviside step:\n    $$ H(z) = \\begin{cases} 0, & z < 0, \\\\ 1, & z \\ge 0, \\end{cases} \\quad f_{\\mathrm{C}}(x) = 3\\,x_1 + 2\\,H(x_2 - 0.5) + 5\\,x_3^2 + 0.1\\,x_4. $$\n\nRequired final output format:\n- For each test case, output first the $d$ values $\\mu^{\\star}_1,\\ldots,\\mu^{\\star}_d$ followed by the $d$ values $\\sigma_1,\\ldots,\\sigma_d$, in increasing input index order.\n- Concatenate the results for Test Case A, then B, then C, in that exact order, to form a single flat list of $2d$ numbers per test case and a combined list across all test cases.\n- Your program should produce a single line of output containing this combined list as a comma-separated sequence enclosed in a single pair of square brackets, with each number rounded to exactly $6$ decimal places and no superfluous whitespace. For example: $[\\ldots]$.\n\nAll arithmetic is real-valued. No physical units are involved. All trigonometric arguments are in radians. Angles and values inside trigonometric functions must not be converted to degrees. The program must not read any input and must be fully deterministic given the seed $42$.",
            "solution": "The problem requires the implementation of the Method of Morris for sensitivity analysis, a screening technique used to identify influential input parameters in a high-dimensional model. The implementation must be based on a finite-difference scheme and follow a specific, deterministic procedure, including random number generation seeded with a fixed value for reproducibility. The analysis will be performed on three distinct mathematical models.\n\nThe core principle of the Morris method is to approximate the global influence of each input parameter by averaging its local derivatives, called elementary effects, sampled at various points across the input space. An input is considered influential if its elementary effects are consistently large in magnitude.\n\nThe problem defines the input space as the $d$-dimensional unit hypercube, $x \\in [0,1]^d$. The analysis is performed on a uniform grid with $p$ levels in each dimension, where the set of possible values for any coordinate $x_i$ is $\\{0, \\frac{1}{p-1}, \\ldots, 1\\}$. For the method to be well-defined on this grid, the step size $\\delta$ is set to a multiple of the grid spacing. As specified, for an even number of levels $p$, the step size is $\\delta = \\frac{p}{2(p-1)}$. This choice ensures that a step of size $\\delta$ from any grid point lands on another grid point, as the step size corresponds to moving by $\\frac{p}{2}$ grid intervals.\n\nThe procedure involves generating $r$ random trajectories through the input space. Each trajectory is a sequence of $d+1$ points, constructed by changing one input parameter at a time. The generation of a single trajectory $k$ proceeds as follows:\n$1$. A random orientation vector $s \\in \\{-1, +1\\}^d$ is sampled. Each component $s_i$ determines the direction of the step for the $i$-th input.\n$2$. A random base point $x^{(0)}$ is sampled from the grid. The sampling is constrained to ensure that a one-step perturbation for any input remains within the $[0,1]$ interval. Specifically, for each coordinate $i$, if $s_i = +1$, $x_i^{(0)}$ is chosen randomly from the set of grid points in $[0, 1-\\delta]$. If $s_i = -1$, $x_i^{(0)}$ is chosen from grid points in $[\\delta, 1]$.\n$3$. A random permutation $\\pi$ of the input indices $\\{1, \\ldots, d\\}$ is generated. This permutation defines the order in which the inputs are perturbed.\n$4$. Starting from $x^{(0)}$, a trajectory of $d+1$ points, $\\{x^{(0)}, x^{(1)}, \\ldots, x^{(d)}\\}$, is built. The point $x^{(j)}$ is obtained from $x^{(j-1)}$ by perturbing a single input, $x^{(j)} = x^{(j-1)} + s_{\\pi(j)}\\delta e_{\\pi(j)}$, where $e_{\\pi(j)}$ is the standard basis vector for the input with index $\\pi(j)$.\n\nFor each step $j$ in the trajectory (from $x^{(j-1)}$ to $x^{(j)}$), we compute one elementary effect (EE) for the input that was perturbed, $i = \\pi(j)$. The problem defines the EE as the finite-difference quotient, which normalizes the change in the model output by the change in the input:\n$$ \\mathrm{EE}_{i}^{(k)} = \\frac{f(x^{(j)}) - f(x^{(j-1)})}{s_{i}\\delta} $$\nThe denominator $s_i\\delta$ is precisely the change applied to the $i$-th coordinate. After generating $r$ trajectories, we have $r$ elementary effect samples for each of the $d$ inputs.\n\nThese samples are then aggregated into two sensitivity indices for each input $i$:\n$1$. The absolute mean, $\\mu^{\\star}_i$, measures the overall influence of the input. It is calculated as:\n$$ \\mu^{\\star}_i = \\frac{1}{r}\\sum_{k=1}^{r} \\left|\\mathrm{EE}_{i}^{(k)}\\right| $$\nA high value of $\\mu^{\\star}_i$ indicates that input $i$ has a significant effect on the output.\n$2$. The standard deviation, $\\sigma_i$, measures the spread of the elementary effects. It is used to detect non-linear effects or interactions with other inputs. The population standard deviation is specified:\n$$ \\sigma_i = \\sqrt{\\frac{1}{r}\\sum_{k=1}^{r} \\left(\\mathrm{EE}_{i}^{(k)} - \\bar{\\mathrm{EE}}_i\\right)^2}, \\quad \\text{where } \\bar{\\mathrm{EE}}_i = \\frac{1}{r}\\sum_{k=1}^{r} \\mathrm{EE}_{i}^{(k)} $$\nA high value of $\\sigma_i$ suggests that the effect of input $i$ is not constant across the input space, pointing to higher-order behavior.\n\nThe implementation is carried out for three test cases, each with its own model function $f(x)$ and parameters ($d, p, r$):\n- **Test Case A:** $d=6$, $p=6$, $r=12$. Model: $f_{\\mathrm{A}}(x) = 5x_1 + 0.5x_2^2 + 20x_3^2 + 3x_4 x_5 + 0.5\\sin(2\\pi x_6)$. This model includes linear, quadratic, and interaction terms.\n- **Test Case B:** $d=12$, $p=8$, $r=10$. Model: $f_{\\mathrm{B}}(x) = 8x_1 + 4x_2^2 + 0.5x_3 + 10x_4 x_5 + 0.1x_6 + 0.05x_7 + 9x_8^2 + 2x_9 x_{10} + 0.2\\sin(2\\pi x_{11}) + 0.01x_{12}$. This is a higher-dimensional model with a mix of highly influential and nearly negligible inputs.\n- **Test Case C:** $d=4$, $p=4$, $r=8$. Model: $f_{\\mathrm{C}}(x) = 3x_1 + 2H(x_2 - 0.5) + 5x_3^2 + 0.1x_4$, where $H(z)$ is the Heaviside step function ($1$ for $z \\ge 0$, $0$ for $z  0$). This model includes a discontinuity, which can lead to a large spread in the elementary effects for the corresponding input.\n\nTo ensure deterministic and reproducible results, a single pseudorandom number generator is initialized with the seed $42$. This generator is used sequentially for all random sampling ($s$, $x^{(0)}$, $\\pi$) across all three test cases in the specified order. The final output is a single, flattened list containing the computed $\\mu^{\\star}_i$ and $\\sigma_i$ values for each test case, concatenated in order, with each numerical value formatted to $6$ decimal places.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to execute the Morris screening for all test cases and print the results.\n    \"\"\"\n\n    # --- Model Functions ---\n    def f_A(x):\n        \"\"\"Model for Test Case A with d=6.\"\"\"\n        return (5 * x[0] + 0.5 * x[1]**2 + 20 * x[2]**2 +\n                3 * x[3] * x[4] + 0.5 * np.sin(2 * np.pi * x[5]))\n\n    def f_B(x):\n        \"\"\"Model for Test Case B with d=12.\"\"\"\n        return (8 * x[0] + 4 * x[1]**2 + 0.5 * x[2] + 10 * x[3] * x[4] +\n                0.1 * x[5] + 0.05 * x[6] + 9 * x[7]**2 + 2 * x[8] * x[9] +\n                0.2 * np.sin(2 * np.pi * x[10]) + 0.01 * x[11])\n\n    def H(z):\n        \"\"\"Heaviside step function.\"\"\"\n        return 1.0 if z >= 0.0 else 0.0\n\n    def f_C(x):\n        \"\"\"Model for Test Case C with d=4.\"\"\"\n        return 3 * x[0] + 2 * H(x[1] - 0.5) + 5 * x[2]**2 + 0.1 * x[3]\n\n    # --- Core Morris Screening Logic ---\n    def morris_screening(model, d, p, r, rng):\n        \"\"\"\n        Performs the Morris screening experiment.\n\n        Args:\n            model (callable): The model function to evaluate.\n            d (int): Number of input dimensions.\n            p (int): Number of grid levels (must be even).\n            r (int): Number of trajectories.\n            rng (np.random.Generator): The random number generator instance.\n\n        Returns:\n            tuple: A tuple containing two numpy arrays (mu_star, sigma).\n        \"\"\"\n        delta = p / (2 * (p - 1))\n        grid = np.linspace(0, 1, p)\n        tol = 1e-9  # Tolerance for floating point comparisons\n\n        elementary_effects = [[] for _ in range(d)]\n\n        for _ in range(r):\n            # 1. Sample orientation vector s\n            s = rng.choice([-1, 1], size=d)\n\n            # 2. Sample grid-aligned base point x0\n            x0 = np.zeros(d)\n            for i in range(d):\n                if s[i] == 1:\n                    possible_values = grid[grid  (1 - delta + tol)]\n                else:  # s[i] == -1\n                    possible_values = grid[grid > (delta - tol)]\n                x0[i] = rng.choice(possible_values)\n\n            # 3. Sample random permutation pi\n            pi = rng.permutation(d)\n\n            # 4. Construct trajectory and compute EEs\n            x_before = x0.copy()\n            y_before = model(x_before)\n\n            for j in range(d):\n                input_idx = pi[j]\n\n                x_after = x_before.copy()\n                x_after[input_idx] += s[input_idx] * delta\n                y_after = model(x_after)\n\n                # The problem's EE definition differs from some literature but is self-consistent\n                ee = (y_after - y_before) / (s[input_idx] * delta)\n                elementary_effects[input_idx].append(ee)\n\n                # Update state for the next step in the trajectory\n                x_before = x_after\n                y_before = y_after\n\n        # 5. Aggregate statistics\n        mu_star = np.zeros(d)\n        sigma = np.zeros(d)\n\n        for i in range(d):\n            ees_i = np.array(elementary_effects[i])\n            mu_star[i] = np.mean(np.abs(ees_i))\n            # Population standard deviation (ddof=0 is default)\n            sigma[i] = np.std(ees_i)\n\n        return mu_star, sigma\n\n    # --- Main Execution Logic ---\n    # Single global RNG seeded as required for reproducibility\n    rng = np.random.default_rng(42)\n\n    test_cases = [\n        {'d': 6, 'p': 6, 'r': 12, 'model': f_A},\n        {'d': 12, 'p': 8, 'r': 10, 'model': f_B},\n        {'d': 4, 'p': 4, 'r': 8, 'model': f_C},\n    ]\n\n    all_results = []\n    for case in test_cases:\n        mu_star, sigma = morris_screening(\n            case['model'], case['d'], case['p'], case['r'], rng\n        )\n        # Per problem spec: concatenate mu* then sigma for each case\n        all_results.extend(mu_star)\n        all_results.extend(sigma)\n\n    # Format the final list for printing as a single line\n    formatted_results = [f\"{x:.6f}\" for x in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\n\nsolve()\n```"
        },
        {
            "introduction": "Once influential parameters and state variables are identified, a critical task in uncertainty quantification is to understand how their uncertainty evolves over time through the system's dynamics. This exercise focuses on the forward propagation of uncertainty in a nonlinear dynamic system, a core component of many state estimation algorithms used in digital twins. You will derive the first-order approximation for propagating a state covariance matrix through a nonlinear function, which forms the basis of the prediction step in the Extended Kalman Filter (EKF) . This practice provides fundamental insight into how a \"cloud\" of state uncertainty is transformed by system dynamics.",
            "id": "4253077",
            "problem": "A cyber-physical system (CPS) deploys a Digital Twin of a single-link rotary element whose discrete-time state $x_t$ consists of angle $\\theta_t$ and angular velocity $\\omega_t$, collected as $x_t = \\begin{pmatrix} \\theta_t \\\\ \\omega_t \\end{pmatrix}$. The Digital Twin evolves under a nonlinear discrete-time dynamic model with additive process noise,\n$$\nx_{t+1} = f(x_t) + w_t,\n$$\nwhere $w_t$ is a zero-mean Gaussian random vector with covariance matrix $Q$, independent of $x_t$. The dynamics are given by\n$$\nf(x_t) = \n\\begin{pmatrix}\n\\theta_t + \\Delta t \\,\\omega_t \\\\\n\\omega_t + \\Delta t \\left( -\\alpha \\,\\sin(\\theta_t) - \\beta \\,\\omega_t \\right)\n\\end{pmatrix},\n$$\nwith sampling interval $\\Delta t$, gravitational-like coefficient $\\alpha$, and viscous damping coefficient $\\beta$. Assume angles are in radians.\n\nAt time $t$, the Digital Twin belief (prior) is Gaussian with mean\n$$\n\\mu_t = \\begin{pmatrix} 0.3 \\\\ 0.1 \\end{pmatrix},\n$$\nand covariance\n$$\nP_t = \\begin{pmatrix} 0.01  0.002 \\\\ 0.002  0.04 \\end{pmatrix}.\n$$\nThe process noise covariance is\n$$\nQ = \\begin{pmatrix} 1 \\times 10^{-5}  0 \\\\ 0  2 \\times 10^{-5} \\end{pmatrix}.\n$$\nThe known physical parameters are $\\Delta t = 0.05$, $\\alpha = 3.0$, and $\\beta = 0.4$.\n\nStarting only from the definitions of covariance,\n$$\n\\mathrm{Cov}(y) = \\mathbb{E}\\left[ (y - \\mathbb{E}[y]) (y - \\mathbb{E}[y])^{\\top} \\right],\n$$\nthe independence of $w_t$ from $x_t$, and a first-order Taylor linearization of $f(x_t)$ about the mean $\\mu_t$, derive the first-order covariance propagation for $x_{t+1}$ and compute the determinant of the predicted covariance matrix at time $t+1$. Use a linearization valid at $\\mu_t$ and evaluate all needed quantities numerically. Round your final answer to five significant figures. Express the determinant in base units using radians for angle and radians per second for angular velocity. State the final numerical value only (do not include units in your final reported value). For context, this setup is typical of the Extended Kalman Filter (EKF) used in Digital Twins for CPS, but you must begin from the aforementioned fundamental definitions and not from any pre-packaged propagation formulas.",
            "solution": "The problem requires the derivation of the first-order covariance propagation for a discrete-time nonlinear system and the computation of the determinant of the predicted covariance matrix at time $t+1$. The derivation must start from the fundamental definition of covariance.\n\nThe state of the system at time $t$ is the random vector $x_t = \\begin{pmatrix} \\theta_t \\\\ \\omega_t \\end{pmatrix}$. Its distribution is approximated as a Gaussian with mean $\\mu_t$ and covariance $P_t$. The system evolves according to the equation:\n$$\nx_{t+1} = f(x_t) + w_t\n$$\nwhere $w_t$ is a zero-mean Gaussian noise vector with covariance $Q$, independent of $x_t$.\n\nThe predicted state covariance at time $t+1$, denoted as $P_{t+1}$, is defined as:\n$$\nP_{t+1} = \\mathrm{Cov}(x_{t+1}) = \\mathbb{E}\\left[ (x_{t+1} - \\mathbb{E}[x_{t+1}]) (x_{t+1} - \\mathbb{E}[x_{t+1}])^{\\top} \\right]\n$$\n\nFirst, we approximate the predicted mean, $\\mu_{t+1} = \\mathbb{E}[x_{t+1}]$.\n$$\n\\mu_{t+1} = \\mathbb{E}[f(x_t) + w_t] = \\mathbb{E}[f(x_t)] + \\mathbb{E}[w_t]\n$$\nGiven that $w_t$ is zero-mean, $\\mathbb{E}[w_t] = 0$. For the term $\\mathbb{E}[f(x_t)]$, we use a first-order Taylor series expansion of $f(x_t)$ around the mean $\\mu_t$:\n$$\nf(x_t) \\approx f(\\mu_t) + F_t (x_t - \\mu_t)\n$$\nwhere $F_t$ is the Jacobian matrix of $f(x_t)$ evaluated at $\\mu_t$, i.e., $F_t = \\frac{\\partial f}{\\partial x_t} \\Big|_{x_t = \\mu_t}$.\nTaking the expectation, we get:\n$$\n\\mathbb{E}[f(x_t)] \\approx \\mathbb{E}[f(\\mu_t) + F_t (x_t - \\mu_t)] = f(\\mu_t) + F_t (\\mathbb{E}[x_t] - \\mu_t)\n$$\nSince $\\mathbb{E}[x_t] = \\mu_t$, the second term becomes zero, leading to the approximation $\\mathbb{E}[f(x_t)] \\approx f(\\mu_t)$.\nTherefore, the predicted mean is $\\mu_{t+1} \\approx f(\\mu_t)$.\n\nNext, we derive the expression for the predicted covariance $P_{t+1}$. We substitute the linearized expression for $x_{t+1}$ and the approximation for $\\mu_{t+1}$ into the covariance definition:\n$$\nx_{t+1} - \\mu_{t+1} \\approx \\left( f(\\mu_t) + F_t(x_t - \\mu_t) + w_t \\right) - f(\\mu_t) = F_t(x_t - \\mu_t) + w_t\n$$\nSo, $P_{t+1}$ can be written as:\n$$\nP_{t+1} \\approx \\mathbb{E}\\left[ \\left( F_t(x_t - \\mu_t) + w_t \\right) \\left( F_t(x_t - \\mu_t) + w_t \\right)^{\\top} \\right]\n$$\nExpanding the product inside the expectation:\n$$\nP_{t+1} \\approx \\mathbb{E}\\left[ F_t(x_t - \\mu_t)(x_t - \\mu_t)^{\\top}F_t^{\\top} + F_t(x_t - \\mu_t)w_t^{\\top} + w_t(x_t - \\mu_t)^{\\top}F_t^{\\top} + w_t w_t^{\\top} \\right]\n$$\nUsing the linearity of expectation:\n$$\nP_{t+1} \\approx \\mathbb{E}[F_t(x_t - \\mu_t)(x_t - \\mu_t)^{\\top}F_t^{\\top}] + \\mathbb{E}[F_t(x_t - \\mu_t)w_t^{\\top}] + \\mathbb{E}[w_t(x_t - \\mu_t)^{\\top}F_t^{\\top}] + \\mathbb{E}[w_t w_t^{\\top}]\n$$\nWe evaluate each term:\n1.  $\\mathbb{E}[F_t(x_t - \\mu_t)(x_t - \\mu_t)^{\\top}F_t^{\\top}] = F_t \\mathbb{E}[(x_t - \\mu_t)(x_t - \\mu_t)^{\\top}] F_t^{\\top} = F_t P_t F_t^{\\top}$, since $P_t = \\mathrm{Cov}(x_t)$.\n2.  $\\mathbb{E}[F_t(x_t - \\mu_t)w_t^{\\top}] = F_t \\mathbb{E}[(x_t - \\mu_t)w_t^{\\top}]$. Since $x_t$ and $w_t$ are independent, $\\mathbb{E}[(x_t - \\mu_t)w_t^{\\top}] = \\mathbb{E}[x_t - \\mu_t]\\mathbb{E}[w_t^{\\top}] = (\\mu_t - \\mu_t) \\cdot 0^{\\top} = 0$.\n3.  The third term is the transpose of the second term, so it is also zero.\n4.  $\\mathbb{E}[w_t w_t^{\\top}]$. Since $\\mathbb{E}[w_t]=0$, this is by definition the covariance of $w_t$, which is $Q$.\n\nCombining these results, we obtain the first-order covariance propagation formula:\n$$\nP_{t+1} \\approx F_t P_t F_t^{\\top} + Q\n$$\n\nNow, we proceed with the numerical computation.\nFirst, we find the Jacobian matrix $F_t$. The function $f(x_t)$ is:\n$$\nf(x_t) = \n\\begin{pmatrix}\nf_1(\\theta_t, \\omega_t) \\\\\nf_2(\\theta_t, \\omega_t)\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\theta_t + \\Delta t \\,\\omega_t \\\\\n\\omega_t + \\Delta t \\left( -\\alpha \\,\\sin(\\theta_t) - \\beta \\,\\omega_t \\right)\n\\end{pmatrix}\n$$\nThe Jacobian matrix is $F_t = \\begin{pmatrix} \\frac{\\partial f_1}{\\partial \\theta_t}  \\frac{\\partial f_1}{\\partial \\omega_t} \\\\ \\frac{\\partial f_2}{\\partial \\theta_t}  \\frac{\\partial f_2}{\\partial \\omega_t} \\end{pmatrix}$. The partial derivatives are:\n$$\n\\frac{\\partial f_1}{\\partial \\theta_t} = 1 \\quad\\quad \\frac{\\partial f_1}{\\partial \\omega_t} = \\Delta t\n$$\n$$\n\\frac{\\partial f_2}{\\partial \\theta_t} = -\\alpha \\Delta t \\cos(\\theta_t) \\quad\\quad \\frac{\\partial f_2}{\\partial \\omega_t} = 1 - \\beta \\Delta t\n$$\nWe are given $\\Delta t = 0.05$, $\\alpha = 3.0$, and $\\beta = 0.4$. We evaluate the Jacobian at $\\mu_t = \\begin{pmatrix} 0.3 \\\\ 0.1 \\end{pmatrix}$, so we use $\\theta_t = 0.3$:\n$$\nF_t = \\begin{pmatrix}\n1  0.05 \\\\\n-3.0 \\times 0.05 \\cos(0.3)  1 - 0.4 \\times 0.05\n\\end{pmatrix}\n= \\begin{pmatrix}\n1  0.05 \\\\\n-0.15 \\cos(0.3)  0.98\n\\end{pmatrix}\n$$\nUsing $\\cos(0.3) \\approx 0.95533649$, we have:\n$$\nF_t \\approx \\begin{pmatrix}\n1  0.05 \\\\\n-0.14330047  0.98\n\\end{pmatrix}\n$$\nThe given covariance matrices are $P_t = \\begin{pmatrix} 0.01  0.002 \\\\ 0.002  0.04 \\end{pmatrix}$ and $Q = \\begin{pmatrix} 10^{-5}  0 \\\\ 0  2 \\times 10^{-5} \\end{pmatrix}$.\n\nWe calculate $F_t P_t F_t^{\\top}$:\n$$\nF_t P_t \\approx \\begin{pmatrix} 1  0.05 \\\\ -0.14330047  0.98 \\end{pmatrix} \\begin{pmatrix} 0.01  0.002 \\\\ 0.002  0.04 \\end{pmatrix} = \\begin{pmatrix} 0.0101  0.004 \\\\ 0.000526995  0.03891340 \\end{pmatrix}\n$$\n$$\nF_t P_t F_t^{\\top} \\approx \\begin{pmatrix} 0.0101  0.004 \\\\ 0.000526995  0.03891340 \\end{pmatrix} \\begin{pmatrix} 1  -0.14330047 \\\\ 0.05  0.98 \\end{pmatrix}\n$$\n$$\nF_t P_t F_t^{\\top} \\approx \\begin{pmatrix} 0.0103  0.002472665 \\\\ 0.002472665  0.03805961 \\end{pmatrix}\n$$\nNow, we compute $P_{t+1} = F_t P_t F_t^{\\top} + Q$:\n$$\nP_{t+1} \\approx \\begin{pmatrix} 0.0103  0.002472665 \\\\ 0.002472665  0.03805961 \\end{pmatrix} + \\begin{pmatrix} 0.00001  0 \\\\ 0  0.00002 \\end{pmatrix}\n$$\n$$\nP_{t+1} \\approx \\begin{pmatrix} 0.01031  0.002472665 \\\\ 0.002472665  0.03807961 \\end{pmatrix}\n$$\nFinally, we compute the determinant of $P_{t+1}$:\n$$\n\\det(P_{t+1}) \\approx (0.01031)(0.03807961) - (0.002472665)^2\n$$\n$$\n\\det(P_{t+1}) \\approx 0.00039260078 - 0.00000611410\n$$\n$$\n\\det(P_{t+1}) \\approx 0.00038648668\n$$\nRounding to five significant figures, the result is $0.00038649$.",
            "answer": "$$\n\\boxed{3.8649 \\times 10^{-4}}\n$$"
        },
        {
            "introduction": "A key function of a digital twin is to learn from real-world data, reducing uncertainty about its internal parameters through calibration. While Bayesian inference provides a principled framework for this learning process, a robust uncertainty analysis requires us to also question the assumptions we begin with. This exercise explores the concept of prior sensitivity, a crucial aspect of UQ . You will not only perform a Bayesian update for a model parameter but also quantify how a small change in your initial belief (the prior) affects your final conclusion (the posterior), making the abstract concept of model-form uncertainty tangible and computable.",
            "id": "4253100",
            "problem": "A Digital Twin (DT) of a Cyber-Physical System (CPS) is used to calibrate a scalar control gain parameter $\\theta$ from measured data. The DT employs a linear measurement model with additive Gaussian noise: the observed scalar $y$ satisfies $y = x \\theta + \\varepsilon$, where $\\varepsilon$ is zero-mean Gaussian with variance $\\sigma^{2}$, and $x$ is a known scalar input. A Gaussian prior is placed on $\\theta$ with mean $\\mu_{0}$ and precision (inverse variance) $\\lambda_{0}$, that is, $\\theta \\sim \\mathcal{N}(\\mu_{0}, 1/\\lambda_{0})$. \n\nAs part of an uncertainty quantification analysis, consider the sensitivity of the posterior mean to the prior precision. Specifically, let the prior precision be perturbed to $\\lambda = \\lambda_{0} + \\delta\\lambda$ with $|\\delta\\lambda|$ small. Starting only from Bayes' theorem and the definitions of the Gaussian likelihood and Gaussian prior, derive the functional form of the posterior mean $m(\\lambda)$ as a function of $\\lambda$, and then compute the first-order change $\\Delta m$ in the posterior mean induced by this perturbation, defined by the linear approximation $\\Delta m \\approx \\left.\\frac{d m}{d \\lambda}\\right|_{\\lambda=\\lambda_{0}} \\delta\\lambda$.\n\nUse the following numerically specified scenario representative of an online DT calibration step:\n$x = 3$, $y = 2$, $\\sigma^{2} = 1.5$, $\\mu_{0} = 0.5$, $\\lambda_{0} = 4$, and $\\delta\\lambda = 0.1$.\n\nReport the single real-valued number $\\Delta m$ obtained from the first-order approximation. Round your answer to $5$ significant figures. Express the answer as a decimal or in standard scientific notation.",
            "solution": "The problem asks for the first-order change in the posterior mean of a parameter $\\theta$ in a linear-Gaussian model, with respect to a small perturbation in its prior precision.\n\nFirst, we derive the posterior mean $m(\\lambda)$ as a function of the prior precision $\\lambda$. The model is:\n- Likelihood: $p(y|\\theta) = \\mathcal{N}(y | x\\theta, \\sigma^2) \\propto \\exp\\left(-\\frac{(y - x\\theta)^2}{2\\sigma^2}\\right)$\n- Prior: $p(\\theta) = \\mathcal{N}(\\theta | \\mu_0, \\lambda^{-1}) \\propto \\exp\\left(-\\frac{\\lambda(\\theta - \\mu_0)^2}{2}\\right)$\n\nAccording to Bayes' theorem, the posterior $p(\\theta|y)$ is proportional to the product of the likelihood and the prior:\n$$p(\\theta | y) \\propto \\exp\\left(-\\frac{(y - x\\theta)^2}{2\\sigma^2} - \\frac{\\lambda(\\theta - \\mu_0)^2}{2}\\right)$$\nTo find the posterior mean, we analyze the exponent, which is a quadratic function of $\\theta$. We expand the terms and group them by powers of $\\theta$:\n$$-\\frac{1}{2} \\left[ \\frac{y^2 - 2yx\\theta + x^2\\theta^2}{\\sigma^2} + \\lambda(\\theta^2 - 2\\mu_0\\theta + \\mu_0^2) \\right]$$\n$$= -\\frac{1}{2} \\left[ \\left(\\frac{x^2}{\\sigma^2} + \\lambda\\right)\\theta^2 - 2\\left(\\frac{yx}{\\sigma^2} + \\lambda\\mu_0\\right)\\theta + \\text{const} \\right]$$\nThis is the exponent of a Gaussian distribution for $\\theta$. For a Gaussian $\\mathcal{N}(\\theta|m, \\lambda_{\\text{post}}^{-1})$, the exponent is $-\\frac{\\lambda_{\\text{post}}}{2}(\\theta-m)^2 = -\\frac{1}{2}(\\lambda_{\\text{post}}\\theta^2 - 2\\lambda_{\\text{post}}m\\theta + \\text{const})$.\nBy comparing coefficients, we identify the posterior precision $\\lambda_{\\text{post}}$ and the posterior mean $m(\\lambda)$:\n$$ \\lambda_{\\text{post}} = \\frac{x^2}{\\sigma^2} + \\lambda $$\n$$ \\lambda_{\\text{post}} m(\\lambda) = \\frac{yx}{\\sigma^2} + \\lambda\\mu_0 $$\nSolving for the posterior mean $m(\\lambda)$:\n$$ m(\\lambda) = \\frac{\\frac{yx}{\\sigma^2} + \\lambda\\mu_0}{\\frac{x^2}{\\sigma^2} + \\lambda} $$\n\nNext, we compute the derivative of $m(\\lambda)$ with respect to $\\lambda$ using the quotient rule, $\\frac{d}{d\\lambda}\\left(\\frac{u}{v}\\right) = \\frac{u'v - uv'}{v^2}$, where $u = \\frac{yx}{\\sigma^2} + \\lambda\\mu_0$ and $v = \\frac{x^2}{\\sigma^2} + \\lambda$.\n$$ u' = \\frac{du}{d\\lambda} = \\mu_0 \\quad \\text{and} \\quad v' = \\frac{dv}{d\\lambda} = 1 $$\n$$ \\frac{dm}{d\\lambda} = \\frac{\\mu_0\\left(\\frac{x^2}{\\sigma^2} + \\lambda\\right) - \\left(\\frac{yx}{\\sigma^2} + \\lambda\\mu_0\\right)(1)}{\\left(\\frac{x^2}{\\sigma^2} + \\lambda\\right)^2} $$\n$$ \\frac{dm}{d\\lambda} = \\frac{\\frac{\\mu_0 x^2}{\\sigma^2} + \\mu_0\\lambda - \\frac{yx}{\\sigma^2} - \\mu_0\\lambda}{\\left(\\frac{x^2}{\\sigma^2} + \\lambda\\right)^2} = \\frac{\\frac{\\mu_0 x^2 - yx}{\\sigma^2}}{\\left(\\frac{x^2}{\\sigma^2} + \\lambda\\right)^2} = \\frac{x(\\mu_0 x - y)}{\\sigma^2\\left(\\frac{x^2}{\\sigma^2} + \\lambda\\right)^2} $$\n$$ \\frac{dm}{d\\lambda} = - \\frac{x(y - \\mu_0 x)}{\\sigma^2\\left(\\lambda + \\frac{x^2}{\\sigma^2}\\right)^2} $$\n\nFinally, we compute the first-order change $\\Delta m \\approx \\left.\\frac{d m}{d \\lambda}\\right|_{\\lambda=\\lambda_{0}} \\delta\\lambda$.\nWe substitute the given numerical values: $x = 3$, $y = 2$, $\\sigma^{2} = 1.5$, $\\mu_{0} = 0.5$, $\\lambda_{0} = 4$, and $\\delta\\lambda = 0.1$.\n\nFirst, evaluate the terms in the derivative at $\\lambda = \\lambda_0$:\n- Numerator term: $x(y - \\mu_0 x) = 3(2 - 0.5 \\times 3) = 3(2 - 1.5) = 1.5$.\n- Denominator term: $\\lambda_0 + \\frac{x^2}{\\sigma^2} = 4 + \\frac{3^2}{1.5} = 4 + \\frac{9}{1.5} = 4 + 6 = 10$.\n- Full derivative: $\\left.\\frac{d m}{d \\lambda}\\right|_{\\lambda=\\lambda_{0}} = - \\frac{1.5}{1.5 \\times (10)^2} = - \\frac{1}{100} = -0.01$.\n\nNow, calculate $\\Delta m$:\n$$ \\Delta m = (-0.01) \\times \\delta\\lambda = -0.01 \\times 0.1 = -0.001 $$\nRounding to 5 significant figures, we get $-0.0010000$ or $-1.0000 \\times 10^{-3}$.",
            "answer": "$$\n\\boxed{-1.0000 \\times 10^{-3}}\n$$"
        }
    ]
}