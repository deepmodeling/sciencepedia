{
    "hands_on_practices": [
        {
            "introduction": "数字孪生不仅必须预测其物理对应物的状态，还必须量化该预测中的不确定性。这项练习  提供了通过非线性动态模型传播不确定性的基础实践，这是诸如扩展卡尔曼滤波器 (EKF) 等滤波器的核心任务。您将从第一性原理推导协方差更新法则，从而加深对线性化如何影响不确定性估计的理解。",
            "id": "4253077",
            "problem": "一个信息物理系统（CPS）部署了一个单连杆旋转元件的数字孪生，其离散时间状态 $x_t$ 由角度 $\\theta_t$ 和角速度 $\\omega_t$ 组成，记为 $x_t = \\begin{pmatrix} \\theta_t \\\\ \\omega_t \\end{pmatrix}$。该数字孪生在一个带有加性过程噪声的非线性离散时间动态模型下演化，\n$$\nx_{t+1} = f(x_t) + w_t,\n$$\n其中 $w_t$ 是一个零均值高斯随机向量，其协方差矩阵为 $Q$，且与 $x_t$ 相互独立。动态特性由下式给出\n$$\nf(x_t) = \n\\begin{pmatrix}\n\\theta_t + \\Delta t \\,\\omega_t \\\\\n\\omega_t + \\Delta t \\left( -\\alpha \\,\\sin(\\theta_t) - \\beta \\,\\omega_t \\right)\n\\end{pmatrix},\n$$\n其中采样间隔为 $\\Delta t$，类重力系数为 $\\alpha$，粘性阻尼系数为 $\\beta$。假设角度单位为弧度。\n\n在时间 $t$，数字孪生的信念（先验）是高斯分布，其均值为\n$$\n\\mu_t = \\begin{pmatrix} 0.3 \\\\ 0.1 \\end{pmatrix},\n$$\n协方差为\n$$\nP_t = \\begin{pmatrix} 0.01  0.002 \\\\ 0.002  0.04 \\end{pmatrix}.\n$$\n过程噪声协方差为\n$$\nQ = \\begin{pmatrix} 1 \\times 10^{-5}  0 \\\\ 0  2 \\times 10^{-5} \\end{pmatrix}.\n$$\n已知的物理参数为 $\\Delta t = 0.05$，$\\alpha = 3.0$ 和 $\\beta = 0.4$。\n\n仅从协方差的定义\n$$\n\\mathrm{Cov}(y) = \\mathbb{E}\\left[ (y - \\mathbb{E}[y]) (y - \\mathbb{E}[y])^{\\top} \\right],\n$$\n$w_t$ 与 $x_t$ 的独立性，以及 $f(x_t)$ 关于均值 $\\mu_t$ 的一阶泰勒线性化出发，推导 $x_{t+1}$ 的一阶协方差传播，并计算时间 $t+1$ 时预测协方差矩阵的行列式。使用在 $\\mu_t$ 处有效的线性化，并数值计算所有需要的量。将最终答案四舍五入到五位有效数字。使用弧度作为角度单位、弧度/秒作为角速度单位，以基本单位表示该行列式。只给出最终数值（最终报告的值中不包含单位）。作为背景，这种设置是用于CPS数字孪生中的扩展卡尔曼滤波器（EKF）的典型情况，但您必须从上述基本定义出发，而不是从任何现成的传播公式出发。",
            "solution": "该问题要求推导一个离散时间非线性系统的一阶协方差传播，并计算在时间 $t+1$ 的预测协方差矩阵的行列式。推导必须从协方差的基本定义开始。\n\n系统在时间 $t$ 的状态是随机向量 $x_t = \\begin{pmatrix} \\theta_t \\\\ \\omega_t \\end{pmatrix}$。其分布被近似为一个均值为 $\\mu_t$、协方差为 $P_t$ 的高斯分布。系统根据以下方程演化：\n$$\nx_{t+1} = f(x_t) + w_t\n$$\n其中 $w_t$ 是一个零均值高斯噪声向量，其协方差为 $Q$，且与 $x_t$ 相互独立。\n\n在时间 $t+1$ 的预测状态协方差记为 $P_{t+1}$，定义为：\n$$\nP_{t+1} = \\mathrm{Cov}(x_{t+1}) = \\mathbb{E}\\left[ (x_{t+1} - \\mathbb{E}[x_{t+1}]) (x_{t+1} - \\mathbb{E}[x_{t+1}])^{\\top} \\right]\n$$\n\n首先，我们近似预测均值 $\\mu_{t+1} = \\mathbb{E}[x_{t+1}]$。\n$$\n\\mu_{t+1} = \\mathbb{E}[f(x_t) + w_t] = \\mathbb{E}[f(x_t)] + \\mathbb{E}[w_t]\n$$\n鉴于 $w_t$ 是零均值，$\\mathbb{E}[w_t] = 0$。对于项 $\\mathbb{E}[f(x_t)]$，我们使用 $f(x_t)$ 在均值 $\\mu_t$ 附近的一阶泰勒级数展开：\n$$\nf(x_t) \\approx f(\\mu_t) + F_t (x_t - \\mu_t)\n$$\n其中 $F_t$ 是 $f(x_t)$ 在 $\\mu_t$ 处求值的雅可比矩阵，即 $F_t = \\frac{\\partial f}{\\partial x_t} \\Big|_{x_t = \\mu_t}$。取期望，我们得到：\n$$\n\\mathbb{E}[f(x_t)] \\approx \\mathbb{E}[f(\\mu_t) + F_t (x_t - \\mu_t)] = f(\\mu_t) + F_t (\\mathbb{E}[x_t] - \\mu_t)\n$$\n由于 $\\mathbb{E}[x_t] = \\mu_t$，第二项变为零，从而得到近似 $\\mathbb{E}[f(x_t)] \\approx f(\\mu_t)$。\n因此，预测均值为 $\\mu_{t+1} \\approx f(\\mu_t)$。\n\n接下来，我们推导预测协方差 $P_{t+1}$ 的表达式。我们将 $x_{t+1}$ 的线性化表达式和 $\\mu_{t+1}$ 的近似值代入协方差定义：\n$$\nx_{t+1} - \\mu_{t+1} \\approx \\left( f(\\mu_t) + F_t(x_t - \\mu_t) + w_t \\right) - f(\\mu_t) = F_t(x_t - \\mu_t) + w_t\n$$\n所以，$P_{t+1}$ 可以写成：\n$$\nP_{t+1} \\approx \\mathbb{E}\\left[ \\left( F_t(x_t - \\mu_t) + w_t \\right) \\left( F_t(x_t - \\mu_t) + w_t \\right)^{\\top} \\right]\n$$\n展开期望内的乘积：\n$$\nP_{t+1} \\approx \\mathbb{E}\\left[ F_t(x_t - \\mu_t)(x_t - \\mu_t)^{\\top}F_t^{\\top} + F_t(x_t - \\mu_t)w_t^{\\top} + w_t(x_t - \\mu_t)^{\\top}F_t^{\\top} + w_t w_t^{\\top} \\right]\n$$\n利用期望的线性性质：\n$$\nP_{t+1} \\approx \\mathbb{E}[F_t(x_t - \\mu_t)(x_t - \\mu_t)^{\\top}F_t^{\\top}] + \\mathbb{E}[F_t(x_t - \\mu_t)w_t^{\\top}] + \\mathbb{E}[w_t(x_t - \\mu_t)^{\\top}F_t^{\\top}] + \\mathbb{E}[w_t w_t^{\\top}]\n$$\n我们计算每一项：\n1.  $\\mathbb{E}[F_t(x_t - \\mu_t)(x_t - \\mu_t)^{\\top}F_t^{\\top}] = F_t \\mathbb{E}[(x_t - \\mu_t)(x_t - \\mu_t)^{\\top}] F_t^{\\top} = F_t P_t F_t^{\\top}$，因为 $P_t = \\mathrm{Cov}(x_t)$。\n2.  $\\mathbb{E}[F_t(x_t - \\mu_t)w_t^{\\top}] = F_t \\mathbb{E}[(x_t - \\mu_t)w_t^{\\top}]$。由于 $x_t$ 和 $w_t$ 是独立的，$\\mathbb{E}[(x_t - \\mu_t)w_t^{\\top}] = \\mathbb{E}[x_t - \\mu_t]\\mathbb{E}[w_t^{\\top}] = (\\mu_t - \\mu_t) \\cdot 0^{\\top} = 0$。\n3.  第三项是第二项的转置，所以它也为零。\n4.  $\\mathbb{E}[w_t w_t^{\\top}]$。由于 $\\mathbb{E}[w_t]=0$，根据定义，这是 $w_t$ 的协方差，即 $Q$。\n\n综合这些结果，我们得到一阶协方差传播公式：\n$$\nP_{t+1} \\approx F_t P_t F_t^{\\top} + Q\n$$\n\n现在，我们进行数值计算。\n首先，我们求雅可比矩阵 $F_t$。函数 $f(x_t)$ 是：\n$$\nf(x_t) = \n\\begin{pmatrix}\nf_1(\\theta_t, \\omega_t) \\\\\nf_2(\\theta_t, \\omega_t)\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\theta_t + \\Delta t \\,\\omega_t \\\\\n\\omega_t + \\Delta t \\left( -\\alpha \\,\\sin(\\theta_t) - \\beta \\,\\omega_t \\right)\n\\end{pmatrix}\n$$\n雅可比矩阵是 $F_t = \\begin{pmatrix} \\frac{\\partial f_1}{\\partial \\theta_t}  \\frac{\\partial f_1}{\\partial \\omega_t} \\\\ \\frac{\\partial f_2}{\\partial \\theta_t}  \\frac{\\partial f_2}{\\partial \\omega_t} \\end{pmatrix}$。偏导数是：\n$$\n\\frac{\\partial f_1}{\\partial \\theta_t} = 1 \\quad\\quad \\frac{\\partial f_1}{\\partial \\omega_t} = \\Delta t\n$$\n$$\n\\frac{\\partial f_2}{\\partial \\theta_t} = -\\alpha \\Delta t \\cos(\\theta_t) \\quad\\quad \\frac{\\partial f_2}{\\partial \\omega_t} = 1 - \\beta \\Delta t\n$$\n给定 $\\Delta t = 0.05$，$\\alpha = 3.0$ 和 $\\beta = 0.4$。我们在 $\\mu_t = \\begin{pmatrix} 0.3 \\\\ 0.1 \\end{pmatrix}$ 处计算雅可比矩阵，所以我们使用 $\\theta_t = 0.3$：\n$$\nF_t = \\begin{pmatrix}\n1   0.05 \\\\\n-3.0 \\times 0.05 \\cos(0.3)  1 - 0.4 \\times 0.05\n\\end{pmatrix}\n= \\begin{pmatrix}\n1  0.05 \\\\\n-0.15 \\cos(0.3)  0.98\n\\end{pmatrix}\n$$\n使用 $\\cos(0.3) \\approx 0.95533649$，我们有：\n$$\nF_t \\approx \\begin{pmatrix}\n1   0.05 \\\\\n-0.14330047  0.98\n\\end{pmatrix}\n$$\n给定的协方差矩阵是 $P_t = \\begin{pmatrix} 0.01  0.002 \\\\ 0.002  0.04 \\end{pmatrix}$ 和 $Q = \\begin{pmatrix} 10^{-5}  0 \\\\ 0  2 \\times 10^{-5} \\end{pmatrix}$。\n\n我们计算 $F_t P_t F_t^{\\top}$：\n$$\nF_t P_t \\approx \\begin{pmatrix} 1   0.05 \\\\ -0.14330047  0.98 \\end{pmatrix} \\begin{pmatrix} 0.01  0.002 \\\\ 0.002  0.04 \\end{pmatrix} = \\begin{pmatrix} 0.0101   0.004 \\\\ 0.000526995  0.03891340 \\end{pmatrix}\n$$\n$$\nF_t P_t F_t^{\\top} \\approx \\begin{pmatrix} 0.0101  0.004 \\\\ 0.000526995  0.03891340 \\end{pmatrix} \\begin{pmatrix} 1  -0.14330047 \\\\ 0.05  0.98 \\end{pmatrix}\n$$\n$$\nF_t P_t F_t^{\\top} \\approx \\begin{pmatrix} 0.0103  0.002472665 \\\\ 0.002472665  0.03805961 \\end{pmatrix}\n$$\n现在，我们计算 $P_{t+1} = F_t P_t F_t^{\\top} + Q$：\n$$\nP_{t+1} \\approx \\begin{pmatrix} 0.0103  0.002472665 \\\\ 0.002472665  0.03805961 \\end{pmatrix} + \\begin{pmatrix} 0.00001  0 \\\\ 0  0.00002 \\end{pmatrix}\n$$\n$$\nP_{t+1} \\approx \\begin{pmatrix} 0.01031  0.002472665 \\\\ 0.002472665  0.03807961 \\end{pmatrix}\n$$\n最后，我们计算 $P_{t+1}$ 的行列式：\n$$\n\\det(P_{t+1}) \\approx (0.01031)(0.03807961) - (0.002472665)^2\n$$\n$$\n\\det(P_{t+1}) \\approx 0.00039260078 - 0.00000611410\n$$\n$$\n\\det(P_{t+1}) \\approx 0.00038648668\n$$\n四舍五入到五位有效数字，结果是 $0.00038649$。",
            "answer": "$$\n\\boxed{3.8649 \\times 10^{-4}}\n$$"
        },
        {
            "introduction": "贝叶斯推断是校准数字孪生的核心，但其结果取决于我们的先验信念。这项实践  通过计算先验精度的变化如何影响最终校准的参数估计，探讨了灵敏度分析这一关键课题。这个练习强调了审视我们的假设并理解其对数据结论影响的重要性。",
            "id": "4253100",
            "problem": "信息物理系统 (CPS) 的数字孪生 (DT) 用于从测量数据中校准标量控制增益参数 $\\theta$。该 DT 采用带有加性高斯噪声的线性测量模型：观测到的标量 $y$ 满足 $y = x \\theta + \\varepsilon$，其中 $\\varepsilon$ 是均值为零、方差为 $\\sigma^{2}$ 的高斯噪声，而 $x$ 是一个已知的标量输入。在 $\\theta$ 上设置了一个高斯先验，其均值为 $\\mu_{0}$，精度（方差的倒数）为 $\\lambda_{0}$，即 $\\theta \\sim \\mathcal{N}(\\mu_{0}, 1/\\lambda_{0})$。\n\n作为不确定性量化分析的一部分，考虑后验均值对先验精度的灵敏度。具体来说，假设先验精度被扰动为 $\\lambda = \\lambda_{0} + \\delta\\lambda$，其中 $|\\delta\\lambda|$ 很小。仅从贝叶斯定理以及高斯似然和高斯先验的定义出发，推导后验均值 $m(\\lambda)$ 作为 $\\lambda$ 的函数形式，然后计算由该扰动引起的后验均值的一阶变化 $\\Delta m$，其定义为线性近似 $\\Delta m \\approx \\left.\\frac{d m}{d \\lambda}\\right|_{\\lambda=\\lambda_{0}} \\delta\\lambda$。\n\n使用以下代表在线 DT 校准步骤的数值场景：\n$x = 3$， $y = 2$， $\\sigma^{2} = 1.5$， $\\mu_{0} = 0.5$， $\\lambda_{0} = 4$， 以及 $\\delta\\lambda = 0.1$。\n\n报告从一阶近似中获得的单个实数值 $\\Delta m$。将您的答案四舍五入到 $5$ 位有效数字。以小数或标准科学记数法表示答案。",
            "solution": "该问题要求我们推导后验均值对先验精度变化的敏感性。我们将分三步解决此问题：首先，推导后验均值作为先验精度 $\\lambda$ 的函数；其次，计算该函数关于 $\\lambda$ 的导数；最后，使用线性近似和给定数值计算后验均值的变化。\n\n**1. 后验均值的推导**\n\n我们使用贝叶斯定理来更新参数 $\\theta$ 的分布。后验分布 $p(\\theta | y)$ 正比于似然 $p(y | \\theta)$ 和先验 $p(\\theta)$ 的乘积：\n$$ p(\\theta | y) \\propto p(y | \\theta) p(\\theta) $$\n给定的模型 $y = x\\theta + \\varepsilon$ 且 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$，意味着似然函数是高斯分布 $p(y|\\theta) = \\mathcal{N}(y|x\\theta, \\sigma^2)$。先验分布也是高斯分布 $p(\\theta) = \\mathcal{N}(\\theta|\\mu_0, 1/\\lambda)$。由于高斯分布是共轭的，后验分布也将是高斯分布。我们可以通过关注概率密度函数指数部分的 $\\theta$ 相关项来确定后验分布的参数。\n\n后验的指数部分可以写为：\n$$ \\ln p(\\theta | y) \\propto -\\frac{(y - x\\theta)^2}{2\\sigma^2} - \\frac{\\lambda(\\theta - \\mu_0)^2}{2} $$\n展开并按 $\\theta$ 的幂次重新整理：\n$$ \\propto -\\frac{1}{2}\\left(\\frac{x^2}{\\sigma^2} + \\lambda\\right)\\theta^2 + \\left(\\frac{yx}{\\sigma^2} + \\lambda\\mu_0\\right)\\theta + C $$\n其中 $C$ 是与 $\\theta$ 无关的常数。这是一个关于 $\\theta$ 的二次函数，确认了后验分布是高斯分布。通过与标准高斯分布的指数形式 $-\\frac{\\lambda_{\\text{post}}(\\theta - m)^2}{2} \\propto -\\frac{\\lambda_{\\text{post}}}{2}\\theta^2 + \\lambda_{\\text{post}}m\\theta$ 进行比较，我们可以得到后验精度 $\\lambda_{\\text{post}}$ 和后验均值 $m(\\lambda)$：\n$$ \\lambda_{\\text{post}} = \\frac{x^2}{\\sigma^2} + \\lambda $$\n$$ \\lambda_{\\text{post}} m(\\lambda) = \\frac{yx}{\\sigma^2} + \\lambda\\mu_0 $$\n因此，后验均值 $m(\\lambda)$ 为：\n$$ m(\\lambda) = \\frac{\\frac{yx}{\\sigma^2} + \\lambda\\mu_0}{\\frac{x^2}{\\sigma^2} + \\lambda} $$\n\n**2. 灵敏度计算**\n\n为了计算后验均值对先验精度 $\\lambda$ 的灵敏度，我们求导数 $\\frac{dm}{d\\lambda}$。为简化求导，我们将 $m(\\lambda)$ 重写为：\n$$ m(\\lambda) = \\mu_0 + \\frac{x(y - \\mu_0 x)}{\\sigma^2}\\left(\\lambda + \\frac{x^2}{\\sigma^2}\\right)^{-1} $$\n现在求导：\n$$ \\frac{dm}{d\\lambda} = \\frac{d}{d\\lambda}\\left[ \\mu_0 + \\frac{x(y - \\mu_0 x)}{\\sigma^2}\\left(\\lambda + \\frac{x^2}{\\sigma^2}\\right)^{-1} \\right] $$\n$$ \\frac{dm}{d\\lambda} = - \\frac{x(y - \\mu_0 x)}{\\sigma^2}\\left(\\lambda + \\frac{x^2}{\\sigma^2}\\right)^{-2} $$\n\n**3. 数值计算**\n\n我们使用线性近似来估计后验均值的变化 $\\Delta m$：\n$$ \\Delta m \\approx \\left.\\frac{d m}{d \\lambda}\\right|_{\\lambda=\\lambda_{0}} \\delta\\lambda = - \\frac{x(y - \\mu_0 x)}{\\sigma^2 \\left(\\lambda_0 + \\frac{x^2}{\\sigma^2}\\right)^2} \\delta\\lambda $$\n代入给定数值 $x=3$, $y=2$, $\\sigma^2=1.5$, $\\mu_0=0.5$, $\\lambda_0=4$ 和 $\\delta\\lambda=0.1$：\n-   分子项: $x(y - \\mu_0 x) = 3(2 - 0.5 \\times 3) = 1.5$\n-   分母中的精度项: $\\lambda_0 + \\frac{x^2}{\\sigma^2} = 4 + \\frac{3^2}{1.5} = 4 + 6 = 10$\n-   代入灵敏度表达式:\n    $$ \\left.\\frac{d m}{d \\lambda}\\right|_{\\lambda=4} = - \\frac{1.5}{1.5 \\times (10)^2} = - \\frac{1}{100} = -0.01 $$\n最后，计算 $\\Delta m$：\n$$ \\Delta m = (-0.01) \\times 0.1 = -0.001 $$\n根据要求，将答案四舍五入到 $5$ 位有效数字，即 $-1.0000 \\times 10^{-3}$。",
            "answer": "$$\\boxed{-1.0000 \\times 10^{-3}}$$"
        },
        {
            "introduction": "现实世界中的数字孪生通常有数十甚至数百个不确定参数，这使得全面的不确定性量化 (UQ) 分析变得难以处理。这项动手编程练习  介绍了 Morris 方法，这是一种强大而高效的筛选技术，用于识别最具影响力的参数。通过实现该算法，您将在一个关键的全局灵敏度分析工作流程中获得实践经验，该工作流程用于简化复杂模型并将 UQ 的精力集中在最重要的地方。",
            "id": "4253088",
            "problem": "您的任务是设计并执行一个逐次单个因子（One-At-a-Time）Morris筛选实验，以识别在参数不确定性下的高维数字孪生模型中的关键输入。您的设计和实现必须严格遵循有限差分基础，并且是自包含的。所有输入都归一化到单位超立方体 $[0,1]^d$ 中，所有三角函数参数均以弧度为单位，所有输出都是无量纲的。该实验必须实现为一个完整、可运行的程序。\n\n您必须使用的基本依据如下：\n- 输入 $i$ 的基本效应是一个有限差分商，它基于在保持所有其他坐标固定的情况下的单维扰动。如果 $f$ 是模型，$x \\in [0,1]^d$ 是一个基点，$\\delta \\in (0,1)$ 是一个与网格对齐的步长，那么输入 $i$ 在 $x$ 处的影响被定义为：仅将第 $i$ 个坐标移动 $\\pm \\delta$ 所引起的变化量，再除以步长大小。每个输入 $i$ 的聚合 Morris 统计量是通过在输入空间中的重复轨迹计算得出的。\n- 实验设计必须在每个维度具有 $p$ 个水平的均匀网格上，使用长度为 $d$ 的轴对齐、逐次单个因子变化的轨迹。网格为 $\\{0, \\frac{1}{p-1}, \\ldots, 1\\}$，步长 $\\delta$ 必须等于 $\\frac{p}{2(p-1)}$，以确保当 $p$ 为偶数时，每一步都停留在网格上且位于单位超立方体内。\n\n您必须实现以下精确的步骤：\n1. 对于每个测试用例，给定维度 $d$、水平数 $p$（一个偶数）和轨迹数 $r$，定义网格，其在每个坐标上有 $p$ 个均匀间隔的水平，并定义步长 $\\delta = \\frac{p}{2(p-1)}$。\n2. 对于每条轨迹 $k \\in \\{1,\\ldots,r\\}$：\n   - 抽样一个方向向量 $s \\in \\{-1,+1\\}^d$，其分量为独立同分布的 Rademacher 随机变量。\n   - 对于每个坐标 $i \\in \\{1,\\ldots,d\\}$，选择一个与网格对齐的基点，以保证单步移动 $x_i \\mapsto x_i + s_i \\delta$ 保持在 $[0,1]$ 区间内。形式上，如果 $s_i=+1$，则从 $\\{0,\\frac{1}{p-1},\\ldots,1-\\delta\\}$ 中选择 $x_i$；如果 $s_i=-1$，则从 $\\{\\delta,\\delta+\\frac{1}{p-1},\\ldots,1\\}$ 中选择 $x_i$。然后缩放到 $[0,1]$。\n   - 抽样一个 $\\{1,\\ldots,d\\}$ 的随机排列 $\\pi$，该抽样独立于 $s$ 和之前的轨迹。\n   - 从基点开始，根据排列 $\\pi$ 和符号 $s$，每次依次移动一个坐标，构造一条包含 $d+1$ 个点的轨迹。\n   - 在轨迹的每个点上评估模型 $f$，产生 $d+1$ 个输出。对于轨迹上对应于输入 $i$ 的每一步，使用有限差分商计算输入 $i$ 的一个基本效应样本\n     $$\\mathrm{EE}_{i} = \\frac{f(x^{\\text{after}}) - f(x^{\\text{before}})}{s_{i}\\,\\delta}.$$\n3. 对于每个输入 $i$，聚合其在 $r$ 条轨迹上的 $r$ 个基本效应，并计算：\n   - 绝对均值\n     $$\\mu^{\\star}_i = \\frac{1}{r}\\sum_{k=1}^{r} \\left|\\mathrm{EE}_{i}^{(k)}\\right|.$$\n   - 标准差（总体版本）\n     $$\\sigma_i = \\sqrt{\\frac{1}{r}\\sum_{k=1}^{r} \\left(\\mathrm{EE}_{i}^{(k)} - \\bar{\\mathrm{EE}}_i\\right)^2}, \\quad \\text{其中 } \\bar{\\mathrm{EE}}_i = \\frac{1}{r}\\sum_{k=1}^{r} \\mathrm{EE}_{i}^{(k)}.$$\n\n随机性控制：\n- 使用一个以整数 $42$ 为种子的单一全局伪随机数生成器，并按照给定顺序，将其用于所有测试用例的所有随机抽样，以确保输出是完全可复现的。\n\n测试套件：\n在以下三个测试用例上实现并评估 Morris 筛选。在所有情况下，模型输入为 $x=(x_1,\\ldots,x_d)\\in[0,1]^d$。\n\n- 测试用例 A（中等维度，包含交互作用和曲率）：\n  - 参数：$d=6$，$p=6$，$r=12$。\n  - 模型：\n    $$f_{\\mathrm{A}}(x) = 5\\,x_1 + 0.5\\,x_2^2 + 20\\,x_3^2 + 3\\,x_4 x_5 + 0.5\\,\\sin(2\\pi x_6).$$\n    角度以弧度为单位。\n\n- 测试用例 B（更高维度，具有稀疏的强效应和弱干扰项）：\n  - 参数：$d=12$，$p=8$，$r=10$。\n  - 模型：\n    $$f_{\\mathrm{B}}(x) = 8\\,x_1 + 4\\,x_2^2 + 0.5\\,x_3 + 10\\,x_4 x_5 + 0.1\\,x_6 + 0.05\\,x_7 + 9\\,x_8^2 + 2\\,x_9 x_{10} + 0.2\\,\\sin(2\\pi x_{11}) + 0.01\\,x_{12}.$$\n    角度以弧度为单位。\n\n- 测试用例 C（带有不连续性的边缘案例，用以探究效应的分布）：\n  - 参数：$d=4$，$p=4$，$r=8$。\n  - 带有 Heaviside 阶跃函数的模型：\n    $$H(z) = \\begin{cases} 0,  z  0 \\\\ 1,  z \\ge 0 \\end{cases}$$\n    $$f_{\\mathrm{C}}(x) = 3\\,x_1 + 2\\,H(x_2 - 0.5) + 5\\,x_3^2 + 0.1\\,x_4.$$\n\n要求的最终输出格式：\n- 对于每个测试用例，按输入索引递增的顺序，首先输出 $d$ 个值 $\\mu^{\\star}_1,\\ldots,\\mu^{\\star}_d$，然后是 $d$ 个值 $\\sigma_1,\\ldots,\\sigma_d$。\n- 按照测试用例 A、B、C 的精确顺序连接结果，以形成一个每个测试用例包含 $2d$ 个数字的扁平列表，并最终组合成一个跨所有测试用例的单一列表。\n- 您的程序应产生单行输出，其中包含此组合列表，形式为逗号分隔的序列，并用一对单独的方括号括起来，每个数字四舍五入到恰好 $6$ 位小数，且不含多余空格。例如：$[\\ldots]$。\n\n所有算术均为实值运算。不涉及物理单位。所有三角函数参数均以弧度为单位。三角函数内部的角度和值不得转换为度。程序不得读取任何输入，并且在给定种子 $42$ 的情况下必须是完全确定性的。",
            "solution": "该问题要求实现用于敏感性分析的 Morris 方法，这是一种筛选技术，用于识别高维模型中的关键输入参数。该实现必须基于有限差分方案，并遵循一个特定的、确定性的程序，包括使用固定值作为种子的随机数生成以保证可复现性。分析将在三个不同的数学模型上执行。\n\nMorris方法的核心原理是，通过平均其在输入空间中多个点上抽样的局部导数（称为“基本效应”），来近似每个输入参数的全局影响。如果一个输入的基本效应的绝对值持续较大，则该输入被认为是有影响力的。\n\n问题将输入空间定义为 $d$ 维单位超立方体 $x \\in [0,1]^d$。分析在每个维度具有 $p$ 个水平的均匀网格上进行，其中任何坐标 $x_i$ 的可能值集合为 $\\{0, \\frac{1}{p-1}, \\ldots, 1\\}$。为使该方法在此网格上是良定义的，步长 $\\delta$ 被设为网格间距的倍数。根据规定，对于偶数水平数 $p$，步长为 $\\delta = \\frac{p}{2(p-1)}$。这个选择确保了从任何网格点出发、大小为 $\\delta$ 的一步会落在另一个网格点上，因为该步长对应于移动 $\\frac{p}{2}$ 个网格间隔。\n\n该过程涉及在输入空间中生成 $r$ 条随机轨迹。每条轨迹是一个由 $d+1$ 个点组成的序列，通过每次改变一个输入参数来构建。单个轨迹 $k$ 的生成过程如下：\n1. 抽样一个随机方向向量 $s \\in \\{-1, +1\\}^d$。每个分量 $s_i$ 决定了第 $i$ 个输入的步进方向。\n2. 从网格中抽样一个随机基点 $x^{(0)}$。抽样受到约束，以确保任何输入的单步扰动都保持在 $[0,1]$ 区间内。具体来说，对于每个坐标 $i$，如果 $s_i = +1$，$x_i^{(0)}$ 从 $[0, 1-\\delta]$ 内的网格点集合中随机选择。如果 $s_i = -1$，$x_i^{(0)}$ 从 $[\\delta, 1]$ 内的网格点中选择。\n3. 生成输入索引 $\\{1, \\ldots, d\\}$ 的一个随机排列 $\\pi$。这个排列定义了输入被扰动的顺序。\n4. 从 $x^{(0)}$ 开始，构建一条包含 $d+1$ 个点的轨迹 $\\{x^{(0)}, x^{(1)}, \\ldots, x^{(d)}\\}$。点 $x^{(j)}$ 是通过扰动单个输入从 $x^{(j-1)}$ 获得的，即 $x^{(j)} = x^{(j-1)} + s_{\\pi(j)}\\delta e_{\\pi(j)}$，其中 $e_{\\pi(j)}$ 是索引为 $\\pi(j)$ 的输入的标准基向量。\n\n对于轨迹中的每一步 $j$（从 $x^{(j-1)}$ 到 $x^{(j)}$），我们为被扰动的输入 $i = \\pi(j)$ 计算一个基本效应（EE）。问题将 EE 定义为有限差分商，它通过输入的变化来归一化模型输出的变化：\n$$ \\mathrm{EE}_{i}^{(k)} = \\frac{f(x^{(j)}) - f(x^{(j-1)})}{s_{i}\\delta} $$\n分母 $s_i\\delta$ 正是应用于第 $i$ 个坐标的变化量。在生成 $r$ 条轨迹后，我们对 $d$ 个输入中的每一个都得到了 $r$ 个基本效应样本。\n\n然后将这些样本聚合成每个输入 $i$ 的两个敏感性指数：\n1. 绝对均值 $\\mu^{\\star}_i$，衡量输入的总体影响。其计算公式为：\n$$ \\mu^{\\star}_i = \\frac{1}{r}\\sum_{k=1}^{r} \\left|\\mathrm{EE}_{i}^{(k)}\\right| $$\n较高的 $\\mu^{\\star}_i$ 值表示输入 $i$ 对输出有显著影响。\n2. 标准差 $\\sigma_i$，衡量基本效应的离散程度。它被用来检测非线性效应或与其他输入的交互作用。指定使用总体标准差：\n$$ \\sigma_i = \\sqrt{\\frac{1}{r}\\sum_{k=1}^{r} \\left(\\mathrm{EE}_{i}^{(k)} - \\bar{\\mathrm{EE}}_i\\right)^2}, \\quad \\text{其中 } \\bar{\\mathrm{EE}}_i = \\frac{1}{r}\\sum_{k=1}^{r} \\mathrm{EE}_{i}^{(k)} $$\n较高的 $\\sigma_i$ 值表明输入 $i$ 的影响在整个输入空间中并非恒定，这指向了高阶行为。\n\n该实现针对三个测试用例进行，每个用例都有自己的模型函数 $f(x)$ 和参数 ($d, p, r$)：\n- **测试用例 A：** $d=6$，$p=6$，$r=12$。模型：$f_{\\mathrm{A}}(x) = 5x_1 + 0.5x_2^2 + 20x_3^2 + 3x_4 x_5 + 0.5\\sin(2\\pi x_6)$。该模型包括线性项、二次项和交互项。\n- **测试用例 B：** $d=12$，$p=8$，$r=10$。模型：$f_{\\mathrm{B}}(x) = 8x_1 + 4x_2^2 + 0.5x_3 + 10x_4 x_5 + 0.1x_6 + 0.05x_7 + 9x_8^2 + 2x_9 x_{10} + 0.2\\sin(2\\pi x_{11}) + 0.01x_{12}$。这是一个更高维度的模型，混合了高影响力输入和几乎可以忽略的输入。\n- **测试用例 C：** $d=4$，$p=4$，$r=8$。模型：$f_{\\mathrm{C}}(x) = 3x_1 + 2H(x_2 - 0.5) + 5x_3^2 + 0.1x_4$，其中 $H(z)$ 是 Heaviside 阶跃函数（当 $z \\ge 0$ 时为 $1$，当 $z  0$ 时为 $0$）。该模型包含一个不连续点，这可能导致相应输入的基本效应分布范围很大。\n\n为确保确定性和可复现的结果，使用种子 $42$ 初始化一个伪随机数生成器。该生成器按指定顺序，依次用于所有三个测试用例的所有随机抽样（$s$、$x^{(0)}$、$\\pi$）。最终输出是一个单一的扁平化列表，其中包含按顺序连接的每个测试用例计算出的 $\\mu^{\\star}_i$ 和 $\\sigma_i$ 值，每个数值都被格式化为 $6$ 位小数。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to execute the Morris screening for all test cases and print the results.\n    \"\"\"\n\n    # --- Model Functions ---\n    def f_A(x):\n        \"\"\"Model for Test Case A with d=6.\"\"\"\n        return (5 * x[0] + 0.5 * x[1]**2 + 20 * x[2]**2 +\n                3 * x[3] * x[4] + 0.5 * np.sin(2 * np.pi * x[5]))\n\n    def f_B(x):\n        \"\"\"Model for Test Case B with d=12.\"\"\"\n        return (8 * x[0] + 4 * x[1]**2 + 0.5 * x[2] + 10 * x[3] * x[4] +\n                0.1 * x[5] + 0.05 * x[6] + 9 * x[7]**2 + 2 * x[8] * x[9] +\n                0.2 * np.sin(2 * np.pi * x[10]) + 0.01 * x[11])\n\n    def H(z):\n        \"\"\"Heaviside step function.\"\"\"\n        return 1.0 if z >= 0.0 else 0.0\n\n    def f_C(x):\n        \"\"\"Model for Test Case C with d=4.\"\"\"\n        return 3 * x[0] + 2 * H(x[1] - 0.5) + 5 * x[2]**2 + 0.1 * x[3]\n\n    # --- Core Morris Screening Logic ---\n    def morris_screening(model, d, p, r, rng):\n        \"\"\"\n        Performs the Morris screening experiment.\n\n        Args:\n            model (callable): The model function to evaluate.\n            d (int): Number of input dimensions.\n            p (int): Number of grid levels (must be even).\n            r (int): Number of trajectories.\n            rng (np.random.Generator): The random number generator instance.\n\n        Returns:\n            tuple: A tuple containing two numpy arrays (mu_star, sigma).\n        \"\"\"\n        delta = p / (2 * (p - 1))\n        grid = np.linspace(0, 1, p)\n        tol = 1e-9  # Tolerance for floating point comparisons\n\n        elementary_effects = [[] for _ in range(d)]\n\n        for _ in range(r):\n            # 1. Sample orientation vector s\n            s = rng.choice([-1, 1], size=d)\n\n            # 2. Sample grid-aligned base point x0\n            x0 = np.zeros(d)\n            for i in range(d):\n                if s[i] == 1:\n                    possible_values = grid[grid  (1 - delta + tol)]\n                else:  # s[i] == -1\n                    possible_values = grid[grid > (delta - tol)]\n                x0[i] = rng.choice(possible_values)\n\n            # 3. Sample random permutation pi\n            pi = rng.permutation(d)\n\n            # 4. Construct trajectory and compute EEs\n            x_before = x0.copy()\n            y_before = model(x_before)\n\n            for j in range(d):\n                input_idx = pi[j]\n\n                x_after = x_before.copy()\n                x_after[input_idx] += s[input_idx] * delta\n                y_after = model(x_after)\n\n                # The problem's EE definition differs from some literature but is self-consistent\n                ee = (y_after - y_before) / (s[input_idx] * delta)\n                elementary_effects[input_idx].append(ee)\n\n                # Update state for the next step in the trajectory\n                x_before = x_after\n                y_before = y_after\n\n        # 5. Aggregate statistics\n        mu_star = np.zeros(d)\n        sigma = np.zeros(d)\n\n        for i in range(d):\n            ees_i = np.array(elementary_effects[i])\n            mu_star[i] = np.mean(np.abs(ees_i))\n            # Population standard deviation (ddof=0 is default)\n            sigma[i] = np.std(ees_i)\n\n        return mu_star, sigma\n\n    # --- Main Execution Logic ---\n    # Single global RNG seeded as required for reproducibility\n    rng = np.random.default_rng(42)\n\n    test_cases = [\n        {'d': 6, 'p': 6, 'r': 12, 'model': f_A},\n        {'d': 12, 'p': 8, 'r': 10, 'model': f_B},\n        {'d': 4, 'p': 4, 'r': 8, 'model': f_C},\n    ]\n\n    all_results = []\n    for case in test_cases:\n        mu_star, sigma = morris_screening(\n            case['model'], case['d'], case['p'], case['r'], rng\n        )\n        # Per problem spec: concatenate mu* then sigma for each case\n        all_results.extend(mu_star)\n        all_results.extend(sigma)\n\n    # Format the final list for printing as a single line\n    formatted_results = [f\"{x:.6f}\" for x in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\n\nsolve()\n```"
        }
    ]
}