{
    "hands_on_practices": [
        {
            "introduction": "To provide absolute safety guarantees for a neural network, we must determine the exact range of its possible outputs for a given set of inputs. This practice introduces polyhedral propagation, a fundamental and exact verification method for networks with Rectified Linear Unit (ReLU) activations. You will explore how the piecewise-linear nature of ReLUs allows us to partition the verification problem into a finite set of linear programs, each corresponding to a specific activation pattern of the network's hidden neurons . By implementing this technique, you will gain hands-on experience with the core logic of exact verification and its reliance on optimization solvers.",
            "id": "4243524",
            "problem": "Consider a two-layer feedforward neural network with Rectified Linear Unit (ReLU) activation, used as a component in a digital twin for a cyber-physical system. The network maps an input vector $x \\in \\mathbb{R}^n$ to a scalar output $y \\in \\mathbb{R}$ by the following transformations: the hidden pre-activation $z \\in \\mathbb{R}^m$ is given by $z = W_1 x + b_1$, the hidden activation $h \\in \\mathbb{R}^m$ is elementwise $h_i = \\max(0, z_i)$ for all indices $i$, and the output is $y = W_2 h + b_2$. All operations are conducted over the real numbers. The input set is a hyper-rectangle (box) defined by lower bounds $l \\in \\mathbb{R}^n$ and upper bounds $u \\in \\mathbb{R}^n$, that is, $x$ satisfies $l \\le x \\le u$ elementwise. A safety specification requires the output to remain within a given closed interval $[L, U] \\subset \\mathbb{R}$ for all inputs in the box.\n\nStarting from the fundamental definitions of affine transformations, convex polyhedra, and piecewise-linear activations, implement polyhedral propagation to compute the exact reachable set of outputs of the network over the input box. Use the characterization that for each activation pattern of the hidden layer, the feasible set in the variables $(x, z, h, y)$ is a convex polyhedron defined by linear equalities and inequalities, and the union over all patterns covers the reachable set. Obtain tight lower and upper bounds on $y$ by optimizing a linear objective over each such polyhedron. Conclude whether the reachable output interval lies within the safety interval $[L, U]$.\n\nYour program must:\n- Enumerate all hidden-layer activation patterns for the given $m$.\n- For each pattern, set up linear constraints that capture $z = W_1 x + b_1$, the ReLU relations under the pattern, $y = W_2 h + b_2$, and the input box constraints $l \\le x \\le u$.\n- Solve two linear programs for each feasible pattern to compute $\\min y$ and $\\max y$ over that pattern’s polyhedron, and aggregate across patterns to obtain the global minimum and maximum over the reachable set.\n\nAll numerical outputs must be expressed without physical units. Angles are not involved. For each test case, the final answer must consist of three outputs: the global lower bound on $y$ as a float, the global upper bound on $y$ as a float, and a boolean indicating whether the reachable interval $[y_{\\min}, y_{\\max}]$ is contained in $[L, U]$ (i.e., $y_{\\min} \\ge L$ and $y_{\\max} \\le U$). Floats must be rounded to four decimal places and printed in decimal form. Booleans must be printed as the canonical Python literals $True$ or $False$.\n\nTest suite:\n- Case $1$ (general case):\n    - Dimensions: $n = 2$, $m = 3$.\n    - Hidden layer weights: $$W_1 = \\begin{bmatrix} 1.0  -2.0 \\\\ 0.5  1.0 \\\\ -1.5  0.5 \\end{bmatrix}$$.\n    - Hidden layer biases: $$b_1 = \\begin{bmatrix} 0.1 \\\\ -0.2 \\\\ 0.3 \\end{bmatrix}$$.\n    - Output layer weights: $$W_2 = \\begin{bmatrix} 0.8  -0.4  0.6 \\end{bmatrix}$$.\n    - Output bias: $b_2 = 0.05$.\n    - Input box: $$l = \\begin{bmatrix} -0.5 \\\\ -0.5 \\end{bmatrix}, u = \\begin{bmatrix} 0.5 \\\\ 0.5 \\end{bmatrix}$$.\n    - Safety interval: $[L, U] = [-1.0, 1.0]$.\n- Case $2$ (larger input box, tighter safety):\n    - Dimensions and network parameters identical to Case $1$.\n    - Input box: $$l = \\begin{bmatrix} -1.0 \\\\ -1.0 \\end{bmatrix}, u = \\begin{bmatrix} 1.0 \\\\ 1.0 \\end{bmatrix}$$.\n    - Safety interval: $[L, U] = [-0.3, 0.3]$.\n- Case $3$ (degenerate output layer yielding constant output):\n    - Dimensions: $n = 2$, $m = 3$.\n    - Hidden layer weights: $$W_1 = \\begin{bmatrix} 1.0  -2.0 \\\\ 0.5  1.0 \\\\ -1.5  0.5 \\end{bmatrix}$$.\n    - Hidden layer biases: $$b_1 = \\begin{bmatrix} 0.1 \\\\ -0.2 \\\\ 0.3 \\end{bmatrix}$$.\n    - Output layer weights: $$W_2 = \\begin{bmatrix} 0.0  0.0  0.0 \\end{bmatrix}$$.\n    - Output bias: $b_2 = 0.2$.\n    - Input box: $$l = \\begin{bmatrix} -1.0 \\\\ -1.0 \\end{bmatrix}, u = \\begin{bmatrix} 1.0 \\\\ 1.0 \\end{bmatrix}$$.\n    - Safety interval: $[L, U] = [0.2, 0.2]$.\n- Case $4$ (mixed signs and asymmetric box):\n    - Dimensions: $n = 2$, $m = 3$.\n    - Hidden layer weights: $$W_1 = \\begin{bmatrix} 2.0  -1.0 \\\\ 1.0  1.0 \\\\ -2.0  0.5 \\end{bmatrix}$$.\n    - Hidden layer biases: $$b_1 = \\begin{bmatrix} -0.5 \\\\ 0.0 \\\\ 0.2 \\end{bmatrix}$$.\n    - Output layer weights: $$W_2 = \\begin{bmatrix} -0.6  0.4  -0.3 \\end{bmatrix}$$.\n    - Output bias: $b_2 = 0.1$.\n    - Input box: $$l = \\begin{bmatrix} -0.2 \\\\ -0.2 \\end{bmatrix}, u = \\begin{bmatrix} 0.7 \\\\ 0.3 \\end{bmatrix}$$.\n    - Safety interval: $[L, U] = [-0.5, 0.0]$.\n\nFinal output format:\nYour program should produce a single line containing a comma-separated list enclosed in square brackets. The list must be the concatenation of the three outputs for each test case in order, i.e., $[y_{\\min}^{(1)}, y_{\\max}^{(1)}, \\text{safe}^{(1)}, y_{\\min}^{(2)}, y_{\\max}^{(2)}, \\text{safe}^{(2)}, y_{\\min}^{(3)}, y_{\\max}^{(3)}, \\text{safe}^{(3)}, y_{\\min}^{(4)}, y_{\\max}^{(4)}, \\text{safe}^{(4)}]$, where each $y_{\\min}^{(k)}$ and $y_{\\max}^{(k)}$ is rounded to four decimals and each $\\text{safe}^{(k)}$ is a boolean. No other text should be printed.",
            "solution": "The user requests the implementation of an exact reachability analysis algorithm for a two-layer feedforward neural network with Rectified Linear Unit (ReLU) activations. The goal is to compute the precise range of the network's scalar output, $y$, given that the input vector, $x$, is constrained to a hyper-rectangular set. This computed output range, $[y_{\\min}, y_{\\max}]$, is then checked against a given safety interval $[L, U]$. The specified methodology is polyhedral propagation, which involves enumerating all possible activation patterns of the hidden layer and solving a set of linear programs (LPs).\n\nThe problem is valid as it is scientifically grounded in the established field of formal verification of neural networks, mathematically well-posed, and all necessary parameters and constraints are provided. We proceed with a solution based on first principles.\n\n### Principle-Based Design\n\nThe core of the problem lies in the piecewise-linear nature of the neural network. The network function $f: \\mathbb{R}^n \\to \\mathbb{R}$ is defined by the composition of affine transformations and the elementwise ReLU activation function, $h_i = \\max(0, z_i)$. The ReLU function is itself piecewise-linear, and this property extends to the entire network.\n\nThe algorithm, known as polyhedral propagation or the exact method, leverages this structure. The input domain, a hyper-rectangle defined by $l \\le x \\le u$, is a convex polyhedron. The fundamental principle is that the image of a convex polyhedron under an affine transformation is also a convex polyhedron. The ReLU activation, $h_i = \\max(0, z_i)$, acts as a \"splitter\": it partitions the domain of its input, $z_i$, into two half-spaces, $z_i \\ge 0$ and $z_i \\le 0$. Within each half-space, the function is linear ($h_i = z_i$ or $h_i = 0$).\n\nConsequently, we can determine the exact output set by partitioning the problem based on the discrete states of the $m$ hidden neurons. An \"activation pattern\" is a vector $\\sigma \\in \\{0, 1\\}^m$ that specifies for each neuron $i \\in \\{1, \\dots, m\\}$ whether it is active ($\\sigma_i=1$, corresponding to $z_i \\ge 0$) or inactive ($\\sigma_i=0$, corresponding to $z_i \\le 0$). There are $2^m$ such patterns in total.\n\nFor each fixed activation pattern $\\sigma$, the network's behavior is governed by a set of purely linear relationships. This allows us to define a convex polyhedron in the space of variables $(x, z, h)$ that represents all possible states of the network consistent with that pattern. The constraints defining this polyhedron are:\n1.  **Input Box**: $l \\le x \\le u$. These are $2n$ linear inequalities.\n2.  **First Affine Layer**: $z = W_1 x + b_1$. These are $m$ linear equalities.\n3.  **ReLU Activation (Pattern-Dependent)**: For each neuron $i=1, \\dots, m$:\n    *   If neuron $i$ is active ($\\sigma_i=1$): $z_i \\ge 0$ (inequality) and $h_i = z_i$ (equality).\n    *   If neuron $i$ is inactive ($\\sigma_i=0$): $z_i \\le 0$ (inequality) and $h_i = 0$ (equality).\n4.  **Output Transformation**: The network output is $y = W_2 h + b_2$. This is a linear function of the variables $h_i$.\n\nThe task of finding the minimum and maximum output $y$ over this polyhedron is a Linear Program (LP). For each of the $2^m$ activation patterns, we formulate and solve two LPs:\n*   **Minimization**: $\\min_{x,z,h} (W_2 h + b_2)$ subject to the linear constraints for the pattern.\n*   **Maximization**: $\\max_{x,z,h} (W_2 h + b_2)$ subject to the linear constraints for the pattern.\n\nMany of these polyhedra may be empty (infeasible), meaning no input $x$ in the given box can produce that activation pattern. The LP solver will detect this infeasibility.\n\nThe overall, or global, minimum and maximum outputs of the network are then the minimum of all minimums and the maximum of all maximums found across all *feasible* activation patterns:\n$$y_{\\min} = \\min_{\\sigma \\in \\text{feasible patterns}} \\left( \\min_{x,z,h} \\{ W_2 h + b_2 \\mid \\text{constraints for } \\sigma \\} \\right)$$\n$$y_{\\max} = \\max_{\\sigma \\in \\text{feasible patterns}} \\left( \\max_{x,z,h} \\{ W_2 h + b_2 \\mid \\text{constraints for } \\sigma \\} \\right)$$\n\nFinally, the safety verification step involves checking if the computed reachable output interval $[y_{\\min}, y_{\\max}]$ is fully contained within the specified safety interval $[L, U]$, which holds if and only if $y_{\\min} \\ge L$ and $y_{\\max} \\le U$.\n\n### Implementation Strategy\n\nThe implementation will use `scipy.optimize.linprog` to solve the LPs. For each of the $m=3$ test cases, we have $n=2$ inputs, so there are $2^3 = 8$ activation patterns to analyze.\n\nThe variables for the LP will be the concatenated vector $[x^T, z^T, h^T]^T$, which has dimension $n+2m = 2+2(3) = 8$.\n\nFor each of the $8$ patterns:\n1.  We construct the LP matrices and vectors ($A_{ub}, b_{ub}, A_{eq}, b_{eq}$) corresponding to the constraints.\n2.  The objective function vector, $c$, for `linprog` is derived from $W_2 h$. To find $\\min y$, we minimize $c^T \\cdot [x^T, z^T, h^T]^T$. To find $\\max y$, we minimize $-c^T \\cdot [x^T, z^T, h^T]^T$. The constant bias $b_2$ is added to the LP result.\n3.  The `linprog` function is called. If it returns success, indicating a feasible polyhedron for the pattern, we update the global minimum and maximum values.\n4.  After iterating over all patterns, the resulting global bounds $[y_{\\min}, y_{\\max}]$ are compared against $[L, U]$ to determine safety.",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import linprog\n\ndef analyze_case(W1, b1, W2, b2, l, u, L, U):\n    \"\"\"\n    Computes the exact output bounds of a 2-layer ReLU network for a given\n    input box and verifies safety.\n\n    Args:\n        W1 (np.array): Weight matrix of the first layer (m x n).\n        b1 (np.array): Bias vector of the first layer (m x 1).\n        W2 (np.array): Weight matrix of the second layer (1 x m).\n        b2 (float): Bias of the second layer.\n        l (np.array): Lower bounds of the input box (n x 1).\n        u (np.array): Upper bounds of the input box (n x 1).\n        L (float): Lower bound of the safety interval.\n        U (float): Upper bound of the safety interval.\n\n    Returns:\n        tuple: (y_min_str, y_max_str, is_safe_bool)\n    \"\"\"\n    m, n = W1.shape\n    num_vars = n + 2 * m  # Variables are [x, z, h]\n\n    # Objective function vector c for linprog. The objective is to min/max W2*h.\n    # The variables are arranged as [x1..xn, z1..zm, h1..hm].\n    c = np.zeros(num_vars)\n    c[n + m:] = W2.flatten()\n    c_min = c\n    c_max = -c\n\n    # Bounds on variables. x is bounded by l and u. z and h are unbounded here,\n    # their constraints are handled by A_eq and A_ub.\n    bounds = []\n    for i in range(n):\n        bounds.append((l[i], u[i]))\n    for _ in range(2 * m):\n        bounds.append((None, None))\n\n    # Equality constraints from the first layer: z = W1*x + b1 = -W1*x + z = b1\n    # These are common to all activation patterns.\n    A_eq_base = np.zeros((m, num_vars))\n    A_eq_base[:, :n] = -W1\n    A_eq_base[:, n:n + m] = np.eye(m)\n    b_eq_base = b1.flatten()\n\n    global_min_y = np.inf\n    global_max_y = -np.inf\n\n    # Iterate through all 2^m activation patterns for the hidden layer.\n    for i in range(2 ** m):\n        pattern = [(i  j)  1 for j in range(m)]\n\n        # Build pattern-specific constraints for ReLU.\n        A_eq_pattern = np.zeros((m, num_vars))\n        b_eq_pattern = np.zeros(m)\n        A_ub_pattern = np.zeros((m, num_vars))\n        b_ub_pattern = np.zeros(m)\n\n        for j in range(m):  # neuron index\n            if pattern[j] == 1:  # Active neuron: z_j = 0, h_j = z_j\n                # z_j = 0  = -z_j = 0\n                A_ub_pattern[j, n + j] = -1\n                b_ub_pattern[j] = 0\n                # h_j = z_j  = -z_j + h_j = 0\n                A_eq_pattern[j, n + j] = -1\n                A_eq_pattern[j, n + m + j] = 1\n                b_eq_pattern[j] = 0\n            else:  # Inactive neuron: z_j = 0, h_j = 0\n                # z_j = 0\n                A_ub_pattern[j, n + j] = 1\n                b_ub_pattern[j] = 0\n                # h_j = 0\n                A_eq_pattern[j, n + m + j] = 1\n                b_eq_pattern[j] = 0\n\n        # Combine base and pattern-specific constraints\n        A_eq = np.vstack([A_eq_base, A_eq_pattern])\n        b_eq = np.concatenate([b_eq_base, b_eq_pattern])\n        A_ub = A_ub_pattern\n        b_ub = b_ub_pattern\n\n        # Solve for min_y in this polyhedron.\n        res_min = linprog(c=c_min, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs')\n        \n        # If the polyhedron is feasible (LP succeeds)\n        if res_min.success:\n            # Solve for max_y in the same polyhedron.\n            res_max = linprog(c=c_max, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs')\n            \n            # The minimization of -y gives -max_y. Don't forget to add the bias b2.\n            current_min = res_min.fun + b2\n            current_max = -res_max.fun + b2\n\n            # Update global bounds\n            global_min_y = min(global_min_y, current_min)\n            global_max_y = max(global_max_y, current_max)\n\n    # After checking all patterns, verify safety.\n    is_safe = (global_min_y = L) and (global_max_y = U)\n\n    # Format output as required.\n    y_min_str = f\"{round(global_min_y, 4):.4f}\"\n    y_max_str = f\"{round(global_max_y, 4):.4f}\"\n\n    return y_min_str, y_max_str, is_safe\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run the analysis, and print results.\n    \"\"\"\n    test_cases = [\n        # Case 1 (general case)\n        {\n            \"W1\": np.array([[1.0, -2.0], [0.5, 1.0], [-1.5, 0.5]]),\n            \"b1\": np.array([[0.1], [-0.2], [0.3]]),\n            \"W2\": np.array([[0.8, -0.4, 0.6]]),\n            \"b2\": 0.05,\n            \"l\": np.array([-0.5, -0.5]),\n            \"u\": np.array([0.5, 0.5]),\n            \"L\": -1.0, \"U\": 1.0\n        },\n        # Case 2 (larger input box, tighter safety)\n        {\n            \"W1\": np.array([[1.0, -2.0], [0.5, 1.0], [-1.5, 0.5]]),\n            \"b1\": np.array([[0.1], [-0.2], [0.3]]),\n            \"W2\": np.array([[0.8, -0.4, 0.6]]),\n            \"b2\": 0.05,\n            \"l\": np.array([-1.0, -1.0]),\n            \"u\": np.array([1.0, 1.0]),\n            \"L\": -0.3, \"U\": 0.3\n        },\n        # Case 3 (degenerate output layer yielding constant output)\n        {\n            \"W1\": np.array([[1.0, -2.0], [0.5, 1.0], [-1.5, 0.5]]),\n            \"b1\": np.array([[0.1], [-0.2], [0.3]]),\n            \"W2\": np.array([[0.0, 0.0, 0.0]]),\n            \"b2\": 0.2,\n            \"l\": np.array([-1.0, -1.0]),\n            \"u\": np.array([1.0, 1.0]),\n            \"L\": 0.2, \"U\": 0.2\n        },\n        # Case 4 (mixed signs and asymmetric box)\n        {\n            \"W1\": np.array([[2.0, -1.0], [1.0, 1.0], [-2.0, 0.5]]),\n            \"b1\": np.array([[-0.5], [0.0], [0.2]]),\n            \"W2\": np.array([[-0.6, 0.4, -0.3]]),\n            \"b2\": 0.1,\n            \"l\": np.array([-0.2, -0.2]),\n            \"u\": np.array([0.7, 0.3]),\n            \"L\": -0.5, \"U\": 0.0\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        y_min_str, y_max_str, is_safe = analyze_case(\n            case[\"W1\"], case[\"b1\"], case[\"W2\"], case[\"b2\"],\n            case[\"l\"], case[\"u\"], case[\"L\"], case[\"U\"]\n        )\n        all_results.extend([y_min_str, y_max_str, is_safe])\n\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "While exact methods provide the tightest possible bounds, their computational cost grows exponentially with the network size, making them impractical for large, deep architectures. This practice explores the world of over-approximation through abstract interpretation, where we trade some precision for significant gains in scalability. You will implement and compare two cornerstone techniques: the fast but often imprecise Interval Bound Propagation (IBP), and a more sophisticated DeepPoly-like abstract domain that maintains relational information to achieve tighter bounds . This exercise will give you a practical understanding of the critical trade-off between precision and performance in neural network verification.",
            "id": "4243461",
            "problem": "You are given a feedforward neural network with two hidden layers using Rectified Linear Unit (ReLU) activation and a single linear output. The neural network is intended to be used as a safety-critical component inside a Digital Twin (a high-fidelity virtual replica) of a cyber-physical system controller. The objective is to compute abstract output bounds under uncertain inputs using two over-approximation methods and assess their conservatism by checking false alarms for a specified safety property.\n\nFundamental base and definitions:\n- A feedforward neural network defines a function $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ composed of affine transformations $z = W a + b$ and nonlinear activations. The Rectified Linear Unit (ReLU) is defined as $\\mathrm{ReLU}(z) = \\max(0, z)$, which is monotone non-decreasing. \n- For any box input domain $X = \\{x \\in \\mathbb{R}^n \\mid \\ell \\le x \\le u\\}$ with elementwise bounds, the image of $X$ under an affine map $x \\mapsto c^\\top x + d$ has bounds derived by interval arithmetic and sign-aware maximization/minimization on a box: \n  $$\\max_{x \\in [\\ell, u]} c^\\top x + d = \\sum_{i} \\max(c_i, 0) u_i + \\min(c_i, 0) \\ell_i + d,$$\n  $$\\min_{x \\in [\\ell, u]} c^\\top x + d = \\sum_{i} \\max(c_i, 0) \\ell_i + \\min(c_i, 0) u_i + d.$$\n- Interval Bound Propagation (IBP) computes forward interval bounds layer-by-layer using sign-aware affine transformations and elementwise monotonicity of $\\mathrm{ReLU}$.\n- A DeepPoly-like abstract domain tracks, for each neuron, both a lower affine bound $a \\ge \\alpha_L^\\top x + \\beta_L$ and an upper affine bound $a \\le \\alpha_U^\\top x + \\beta_U$ with respect to the original inputs, and uses sound linear relaxations of $\\mathrm{ReLU}$:\n  - If $z$ is the pre-activation with interval bounds $[z_L, z_U]$ and $z_U \\le 0$, then $\\mathrm{ReLU}(z) = 0$ exactly.\n  - If $z_L \\ge 0$, then $\\mathrm{ReLU}(z) = z$ exactly.\n  - If $z_L  0  z_U$, then the standard convex relaxation is used: the upper bound is $a \\le \\alpha z + \\beta$ with $\\alpha = \\frac{z_U}{z_U - z_L}$ and $\\beta = -\\alpha z_L$, and the lower bound is $a \\ge 0$.\n\nSafety specification:\n- The safety property requires the scalar output $y$ of the network to lie within a fixed interval $[s_{\\min}, s_{\\max}]$ on the entire input box. A safety violation is possible if the abstract output bounds intersect the complement of $[s_{\\min}, s_{\\max}]$. A false alarm occurs when an abstract method predicts a potential violation while an empirical dense grid evaluation over the input box finds no sample violating $[s_{\\min}, s_{\\max}]$.\n\nNetwork parameters:\n- Input dimension is $n = 2$. Hidden layers have $3$ neurons each. Output dimension is $m = 1$.\n- First layer weights and biases:\n  $$W_1 = \\begin{bmatrix} 1.0  -1.0 \\\\ -1.0  1.0 \\\\ 0.2  0.2 \\end{bmatrix}, \\quad b_1 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.05 \\end{bmatrix}.$$\n- Second layer weights and biases:\n  $$W_2 = \\begin{bmatrix} 0.6  -0.6  0.2 \\\\ -0.6  0.6  -0.2 \\\\ 0.3  0.3  0.0 \\end{bmatrix}, \\quad b_2 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}.$$\n- Output layer weights and bias:\n  $$W_3 = \\begin{bmatrix} 0.5  -0.4  0.3 \\end{bmatrix}, \\quad b_3 = \\begin{bmatrix} 0.0 \\end{bmatrix}.$$\n\nTasks:\n1. Implement Interval Bound Propagation (IBP) to compute output interval bounds $[y_L^{\\mathrm{IBP}}, y_U^{\\mathrm{IBP}}]$ for a given input box $[\\ell, u]$.\n2. Implement a DeepPoly-like abstraction to compute output bounds by maintaining, for each neuron, lower and upper affine bounds in terms of the original inputs, using the ReLU linear relaxations described above and sign-aware affine composition. From these affine bounds at the output, compute the output interval bounds $[y_L^{\\mathrm{DP}}, y_U^{\\mathrm{DP}}]$ via box maximization/minimization.\n3. For each test case, evaluate the actual network output over a dense two-dimensional grid on the input box (uniformly spaced in each dimension, using exactly $21$ points per dimension, for a total of $441$ samples), and determine whether any sample violates the safety interval $[s_{\\min}, s_{\\max}]$.\n4. For each test case and method, report whether a false alarm occurred, defined as a boolean that is true iff the method’s abstract bounds predict potential violation (i.e., $y_L  s_{\\min}$ or $y_U > s_{\\max}$) while no grid sample violates the interval (i.e., all sampled outputs lie in $[s_{\\min}, s_{\\max}]$).\n\nTest suite:\n- Case $1$: $\\ell = [-0.1, -0.1]$, $u = [0.1, 0.1]$, $s_{\\min} = -0.05$, $s_{\\max} = 0.05$.\n- Case $2$ (single-point box): $\\ell = [0.0, 0.0]$, $u = [0.0, 0.0]$, $s_{\\min} = -0.01$, $s_{\\max} = 0.01$.\n- Case $3$ (larger box): $\\ell = [-0.5, -0.5]$, $u = [0.5, 0.5]$, $s_{\\min} = -0.5$, $s_{\\max} = 0.5$.\n- Case $4$ (tighter safety on a mid-size box): $\\ell = [-0.2, -0.2]$, $u = [0.2, 0.2]$, $s_{\\min} = -0.02$, $s_{\\max} = 0.02$.\n\nRequired output:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case in order, first output the Interval Bound Propagation false alarm boolean, then the DeepPoly-like false alarm boolean. For example, the output format must be\n$$[\\text{IBP\\_case1}, \\text{DP\\_case1}, \\text{IBP\\_case2}, \\text{DP\\_case2}, \\text{IBP\\_case3}, \\text{DP\\_case3}, \\text{IBP\\_case4}, \\text{DP\\_case4}]$$\nwhere each entry is a boolean literal.",
            "solution": "The objective is to analyze the safety of a given feedforward neural network under input uncertainty. This involves computing over-approximations of the network's output range using two distinct abstract interpretation techniques: Interval Bound Propagation (IBP) and a DeepPoly-like abstraction. We will then compare the conservatism of these methods by identifying false alarms, which occur when a method predicts a potential safety violation that is not observed in a dense empirical sampling of the input space.\n\nThe neural network is a function $f: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^1$ with two hidden layers. Its computation proceeds as follows:\nLet the input be $x \\in \\mathbb{R}^2$.\nThe output of the first hidden layer is $h_1 = \\mathrm{ReLU}(z_1)$, where the pre-activation is $z_1 = W_1 x + b_1$. Here $h_1, z_1, b_1 \\in \\mathbb{R}^3$ and $W_1 \\in \\mathbb{R}^{3 \\times 2}$.\nThe output of the second hidden layer is $h_2 = \\mathrm{ReLU}(z_2)$, where the pre-activation is $z_2 = W_2 h_1 + b_2$. Here $h_2, z_2, b_2 \\in \\mathbb{R}^3$ and $W_2 \\in \\mathbb{R}^{3 \\times 3}$.\nThe final output is $y = z_3$, where $z_3 = W_3 h_2 + b_3$. Here $y, b_3 \\in \\mathbb{R}^1$ and $W_3 \\in \\mathbb{R}^{1 \\times 3}$.\n\nThe input uncertainty is modeled by a box domain $X = \\{x \\in \\mathbb{R}^n \\mid \\ell \\le x \\le u\\}$, where $\\ell, u \\in \\mathbb{R}^n$ are the element-wise lower and upper bounds.\n\n### Task 1: Interval Bound Propagation (IBP)\n\nIBP computes interval bounds for the network's output by propagating intervals layer-by-layer from input to output.\n\nLet the interval bounds for the activations of layer $k$ be $[\\ell^{(k)}, u^{(k)}]$. The propagation to layer $k+1$ involves two steps:\n\n1.  **Affine Transformation**: The pre-activations are computed as $z^{(k+1)} = W^{(k+1)} a^{(k)} + b^{(k+1)}$. The bounds for each component $z_i^{(k+1)}$ are found by applying the provided formula for the minimum and maximum of an affine function over a box. Let $w_i^\\top$ be the $i$-th row of $W^{(k+1)}$ and $b_i$ be the $i$-th component of $b^{(k+1)}$. The bounds $[\\ell_{z,i}^{(k+1)}, u_{z,i}^{(k+1)}]$ for $z_i^{(k+1)}$ are:\n    $$u_{z,i}^{(k+1)} = \\sum_{j} \\max(w_{ij}, 0) u_j^{(k)} + \\min(w_{ij}, 0) \\ell_j^{(k)} + b_i$$\n    $$\\ell_{z,i}^{(k+1)} = \\sum_{j} \\max(w_{ij}, 0) \\ell_j^{(k)} + \\min(w_{ij}, 0) u_j^{(k)} + b_i$$\n\n2.  **Activation Function**: The post-activation bounds $[\\ell_a^{(k+1)}, u_a^{(k+1)}]$ are found by applying the activation function to the pre-activation bounds. Since $\\mathrm{ReLU}$ is element-wise and monotone non-decreasing, the new bounds are:\n    $$\\ell_a^{(k+1)} = \\mathrm{ReLU}(\\ell_z^{(k+1)})$$\n    $$u_a^{(k+1)} = \\mathrm{ReLU}(u_z^{(k+1)})$$\n\nThis process is initiated with the input box $a^{(0)} \\in [\\ell, u]$ and repeated for each layer until the final output bounds $[y_L^{\\mathrm{IBP}}, y_U^{\\mathrm{IBP}}]$ are computed.\n\n### Task 2: DeepPoly-like Abstraction\n\nThis method provides tighter bounds by tracking, for each neuron, an affine function of the original network inputs $x$ that bounds its activation from above and below. For a neuron $a_i$, we maintain symbolic bounds of the form $a_i \\ge (\\alpha_{i,L})^\\top x + \\beta_{i,L}$ and $a_i \\le (\\alpha_{i,U})^\\top x + \\beta_{i,U}$.\n\nThe propagation of these symbolic affine bounds proceeds layer by layer:\n\n1.  **Input Layer**: For the network inputs $x = (x_1, \\dots, x_n)^\\top$, each component $x_i$ is trivially bounded by itself. This is represented as $x_i \\ge e_i^\\top x + 0$ and $x_i \\le e_i^\\top x + 0$, where $e_i$ is the $i$-th standard basis vector. Thus, for the input layer, the matrix of lower-bound alpha coefficients is the identity matrix, $\\mathbf{A}_L^{(0)} = I$, and the vector of beta coefficients is zero, $\\mathbf{B}_L^{(0)} = \\vec{0}$. Similarly, $\\mathbf{A}_U^{(0)} = I$ and $\\mathbf{B}_U^{(0)} = \\vec{0}$.\n\n2.  **Affine Transformation**: Given affine bounds for layer $k-1$ activations, represented by coefficient matrices $\\mathbf{A}_{L/U}^{(k-1)}$ and vectors $\\mathbf{B}_{L/U}^{(k-1)}$, we compute the bounds for the pre-activations $z^{(k)} = W^{(k)} a^{(k-1)} + b^{(k)}$. By substituting the affine bounds for $a^{(k-1)}$ and rearranging terms, we derive the new affine coefficients for $z^{(k)}$:\n    Let $(W^{(k)})^+ = \\max(W^{(k)}, 0)$ and $(W^{(k)})^- = \\min(W^{(k)}, 0)$ be the element-wise positive and negative parts of the weight matrix.\n    $$\\mathbf{A}_{L,z}^{(k)} = (W^{(k)})^+\\mathbf{A}_L^{(k-1)} + (W^{(k)})^-\\mathbf{A}_U^{(k-1)}$$\n    $$\\mathbf{B}_{L,z}^{(k)} = (W^{(k)})^+\\mathbf{B}_L^{(k-1)} + (W^{(k)})^-\\mathbf{B}_U^{(k-1)} + b^{(k)}$$\n    $$\\mathbf{A}_{U,z}^{(k)} = (W^{(k)})^+\\mathbf{A}_U^{(k-1)} + (W^{(k)})^-\\mathbf{A}_L^{(k-1)}$$\n    $$\\mathbf{B}_{U,z}^{(k)} = (W^{(k)})^+\\mathbf{B}_U^{(k-1)} + (W^{(k)})^-\\mathbf{B}_L^{(k-1)} + b^{(k)}$$\n\n3.  **ReLU Transformation**: This is the core of the abstraction. For each neuron $j$ in layer $k$, we transform the pre-activation affine bounds into post-activation ($a_j^{(k)} = \\mathrm{ReLU}(z_j^{(k)})$) affine bounds. This requires first computing concrete interval bounds $[z_{j,L}, z_{j,U}]$ for each pre-activation neuron $z_j^{(k)}$ by maximizing/minimizing its affine bounds over the input box $[\\ell, u]$. Based on these concrete bounds, we apply one of the following rules:\n    *   **Case 1: $z_{j,U} \\le 0$ (ReLU is inactive)**. The output is exactly $0$. The new affine bounds are zero: $\\alpha_{j,L/U,a}^{(k)} = \\vec{0}, \\beta_{j,L/U,a}^{(k)} = 0$.\n    *   **Case 2: $z_{j,L} \\ge 0$ (ReLU is active)**. The output is the identity, $a_j^{(k)} = z_j^{(k)}$. The affine bounds for $a_j^{(k)}$ are the same as for $z_j^{(k)}$.\n    *   **Case 3: $z_{j,L}  0  z_{j,U}$ (ReLU is unstable)**. We use a linear relaxation.\n        *   **Lower Bound**: The problem specifies $a_j^{(k)} \\ge 0$. This corresponds to affine bounds $\\alpha_{j,L,a}^{(k)} = \\vec{0}$ and $\\beta_{j,L,a}^{(k)} = 0$.\n        *   **Upper Bound**: The standard convex relaxation is a line passing through $(z_{j,L}, 0)$ and $(z_{j,U}, z_{j,U})$, given by $a_j^{(k)} \\le \\lambda z_j^{(k)} + \\mu$, where $\\lambda = \\frac{z_{j,U}}{z_{j,U} - z_{j,L}}$ and $\\mu = -\\lambda z_{j,L}$. Since $\\lambda  0$, we substitute the upper affine bound for $z_j^{(k)}$ to obtain the new upper affine bound for $a_j^{(k)}$:\n            $$\\alpha_{j,U,a}^{(k)} = \\lambda \\cdot \\alpha_{j,U,z}^{(k)}$$\n            $$\\beta_{j,U,a}^{(k)} = \\lambda \\cdot \\beta_{j,U,z}^{(k)} + \\mu$$\n\n4.  **Final Output Bounds**: After propagating these symbolic bounds through the entire network, we obtain final affine bounds for the output, $y \\ge \\alpha_{L,y}^\\top x + \\beta_{L,y}$ and $y \\le \\alpha_{U,y}^\\top x + \\beta_{U,y}$. The concrete output interval $[y_L^{\\mathrm{DP}}, y_U^{\\mathrm{DP}}]$ is then found by minimizing the lower affine bound and maximizing the upper affine bound over the input box $[\\ell, u]$.\n\n### Task 3: Grid Evaluation\n\nTo get an empirical ground truth for the network's behavior, we evaluate it on a dense grid of points within the input box $[\\ell, u]$. A grid of $21 \\times 21 = 441$ points is constructed using uniformly spaced points in each of the two input dimensions. For each point $x_{sample}$ on this grid, we compute the exact network output $y_{sample} = f(x_{sample})$. We then check if any of these samples violate the safety property, i.e., if any $y_{sample}$ falls outside the specified safety interval $[s_{\\min}, s_{\\max}]$. This is quantified by a boolean `grid_violation`.\n\n### Task 4: False Alarm Assessment\n\nA false alarm for a given verification method (IBP or DeepPoly) is defined as a situation where the method predicts a potential safety violation, but no violation is found by the dense grid evaluation.\n\nFor each method, let its computed output bounds be $[y_L, y_U]$.\nA potential violation is predicted if the abstract bounds intersect the unsafe region:\n`abstract_violation` = $(y_L  s_{\\min})$ or $(y_U  s_{\\max})$.\nThe grid evaluation result is:\n`grid_violation` = `True` if any $y_{sample} \\notin [s_{\\min}, s_{\\max}]$, else `False`.\n\nThe false alarm is then computed as:\n`false_alarm` = `abstract_violation` AND (NOT `grid_violation`).\n\nThis procedure is followed for both IBP and the DeepPoly-like method for each of the four test cases, yielding a total of eight boolean results.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ... (scipy is listed but not needed for this problem)\n\n# --- Network Parameters ---\nW1 = np.array([[1.0, -1.0], [-1.0, 1.0], [0.2, 0.2]])\nb1 = np.array([0.0, 0.0, 0.05])\n\nW2 = np.array([[0.6, -0.6, 0.2], [-0.6, 0.6, -0.2], [0.3, 0.3, 0.0]])\nb2 = np.array([0.0, 0.0, 0.0])\n\nW3 = np.array([[0.5, -0.4, 0.3]])\nb3 = np.array([0.0])\n\nWs = [W1, W2, W3]\nbs = [b1, b2, b3]\n\ndef relu(x):\n    \"\"\"Element-wise Rectified Linear Unit.\"\"\"\n    return np.maximum(0, x)\n\ndef forward_pass(x, weights, biases):\n    \"\"\"Computes the exact network output for a single input vector.\"\"\"\n    a = x\n    # Hidden layers with ReLU\n    for i in range(len(weights) - 1):\n        z = weights[i] @ a + biases[i]\n        a = relu(z)\n    # Output layer (linear)\n    y = weights[-1] @ a + biases[-1]\n    return y[0]\n\ndef get_affine_bounds(c, d, l, u):\n    \"\"\"Computes min/max of c.T @ x + d over the box [l, u].\"\"\"\n    c_plus = np.maximum(c, 0)\n    c_minus = np.minimum(c, 0)\n    \n    lower_bound = c_plus @ l + c_minus @ u + d\n    upper_bound = c_plus @ u + c_minus @ l + d\n    \n    return lower_bound, upper_bound\n\ndef run_ibp(l, u, weights, biases):\n    \"\"\"Computes output bounds using Interval Bound Propagation.\"\"\"\n    l_current, u_current = l, u\n    \n    # Hidden layers with ReLU\n    for i in range(len(weights) - 1):\n        W, b = weights[i], biases[i]\n        W_plus = np.maximum(W, 0)\n        W_minus = np.minimum(W, 0)\n        \n        l_z = W_plus @ l_current + W_minus @ u_current + b\n        u_z = W_plus @ u_current + W_minus @ l_current + b\n        \n        l_current = relu(l_z)\n        u_current = relu(u_z)\n        \n    # Output layer (linear)\n    W, b = weights[-1], biases[-1]\n    W_plus = np.maximum(W, 0)\n    W_minus = np.minimum(W, 0)\n    \n    y_l = (W_plus @ l_current + W_minus @ u_current + b)[0]\n    y_u = (W_plus @ u_current + W_minus @ l_current + b)[0]\n    \n    return y_l, y_u\n\ndef run_deeppoly(l, u, weights, biases):\n    \"\"\"Computes output bounds using a DeepPoly-like abstraction.\"\"\"\n    n_in = len(l)\n    \n    # Initialize affine bounds for input layer x\n    # alpha_l, alpha_u are (n_neurons, n_in)\n    # beta_l, beta_u are (n_neurons,)\n    alpha_l, alpha_u = np.eye(n_in), np.eye(n_in)\n    beta_l, beta_u = np.zeros(n_in), np.zeros(n_in)\n\n    # Propagate through hidden layers\n    for i in range(len(weights) - 1):\n        W, b = weights[i], biases[i]\n        \n        # 1. Affine transformation\n        W_plus = np.maximum(W, 0)\n        W_minus = np.minimum(W, 0)\n        \n        alpha_lz = W_plus @ alpha_l + W_minus @ alpha_u\n        beta_lz = W_plus @ beta_l + W_minus @ beta_u + b\n        \n        alpha_uz = W_plus @ alpha_u + W_minus @ alpha_l\n        beta_uz = W_plus @ beta_u + W_minus @ beta_l + b\n\n        # 2. ReLU transformation\n        # First, compute concrete bounds for pre-activations\n        z_l, _ = get_affine_bounds(alpha_lz, beta_lz, l, u)\n        _, z_u = get_affine_bounds(alpha_uz, beta_uz, l, u)\n        \n        n_neurons_layer = W.shape[0]\n        alpha_l_new = np.zeros_like(alpha_lz)\n        beta_l_new = np.zeros_like(beta_lz)\n        alpha_u_new = np.zeros_like(alpha_uz)\n        beta_u_new = np.zeros_like(beta_uz)\n        \n        for j in range(n_neurons_layer):\n            zl_j, zu_j = z_l[j], z_u[j]\n            \n            if zu_j = 0: # ReLU is inactive\n                # Bounds are 0\n                pass # Already initialized to zero\n            elif zl_j = 0: # ReLU is active\n                alpha_l_new[j] = alpha_lz[j]\n                beta_l_new[j] = beta_lz[j]\n                alpha_u_new[j] = alpha_uz[j]\n                beta_u_new[j] = beta_uz[j]\n            else: # ReLU is unstable\n                # Lower bound: a = 0\n                # alpha_l_new[j] and beta_l_new[j] remain 0\n                \n                # Upper bound: a = lambda * z + mu\n                lambda_ = zu_j / (zu_j - zl_j)\n                mu = -lambda_ * zl_j\n                \n                # Since lambda  0, we use upper bound for z\n                alpha_u_new[j] = lambda_ * alpha_uz[j]\n                beta_u_new[j] = lambda_ * beta_uz[j] + mu\n\n        alpha_l, beta_l = alpha_l_new, beta_l_new\n        alpha_u, beta_u = alpha_u_new, beta_u_new\n\n    # Final affine transformation for output layer\n    W_out, b_out = weights[-1], biases[-1]\n    W_out_plus = np.maximum(W_out, 0)\n    W_out_minus = np.minimum(W_out, 0)\n\n    final_alpha_l = W_out_plus @ alpha_l + W_out_minus @ alpha_u\n    final_beta_l = W_out_plus @ beta_l + W_out_minus @ beta_u + b_out\n\n    final_alpha_u = W_out_plus @ alpha_u + W_out_minus @ alpha_l\n    final_beta_u = W_out_plus @ beta_u + W_out_minus @ beta_l + b_out\n\n    # Compute final concrete bounds\n    y_l, _ = get_affine_bounds(final_alpha_l[0], final_beta_l[0], l, u)\n    _, y_u = get_affine_bounds(final_alpha_u[0], final_beta_u[0], l, u)\n    \n    return y_l, y_u\n\ndef grid_evaluation(l, u, weights, biases, s_min, s_max):\n    \"\"\"Performs dense grid evaluation to check for safety violations.\"\"\"\n    grid_points = 21\n    x1_vals = np.linspace(l[0], u[0], grid_points)\n    x2_vals = np.linspace(l[1], u[1], grid_points)\n    \n    has_violation = False\n    for x1 in x1_vals:\n        for x2 in x2_vals:\n            x_sample = np.array([x1, x2])\n            y_sample = forward_pass(x_sample, weights, biases)\n            if not (s_min = y_sample = s_max):\n                has_violation = True\n                break\n        if has_violation:\n            break\n            \n    return has_violation\n\ndef solve():\n    \"\"\"Main function to run test cases and generate final output.\"\"\"\n    test_cases = [\n        # Case 1\n        {'l': np.array([-0.1, -0.1]), 'u': np.array([0.1, 0.1]), 's_min': -0.05, 's_max': 0.05},\n        # Case 2\n        {'l': np.array([0.0, 0.0]), 'u': np.array([0.0, 0.0]), 's_min': -0.01, 's_max': 0.01},\n        # Case 3\n        {'l': np.array([-0.5, -0.5]), 'u': np.array([0.5, 0.5]), 's_min': -0.5, 's_max': 0.5},\n        # Case 4\n        {'l': np.array([-0.2, -0.2]), 'u': np.array([0.2, 0.2]), 's_min': -0.02, 's_max': 0.02},\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        l, u, s_min, s_max = case['l'], case['u'], case['s_min'], case['s_max']\n\n        # Run grid evaluation to find \"ground truth\"\n        grid_has_violation = grid_evaluation(l, u, Ws, bs, s_min, s_max)\n        \n        # --- IBP Analysis ---\n        y_l_ibp, y_u_ibp = run_ibp(l, u, Ws, bs)\n        ibp_predicts_violation = (y_l_ibp  s_min) or (y_u_ibp  s_max)\n        ibp_false_alarm = ibp_predicts_violation and not grid_has_violation\n        results.append(str(ibp_false_alarm).lower())\n        \n        # --- DeepPoly Analysis ---\n        y_l_dp, y_u_dp = run_deeppoly(l, u, Ws, bs)\n        dp_predicts_violation = (y_l_dp  s_min) or (y_u_dp  s_max)\n        dp_false_alarm = dp_predicts_violation and not grid_has_violation\n        results.append(str(dp_false_alarm).lower())\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Verifying a neural network in isolation is often insufficient; in cyber-physical systems, we must ensure the safety of the entire closed-loop system where the network acts as a controller. This practice elevates the challenge to the system level by performing time-bounded reachability analysis for a linear plant controlled by a neural network. You will use zonotopes, a powerful geometric representation for sets, to propagate uncertainties through the system dynamics over time, accounting for both the initial state uncertainty and external disturbances . This capstone exercise will equip you with the skills to analyze the safety of dynamic, feedback-controlled systems, a central task in the design of reliable digital twins.",
            "id": "4243426",
            "problem": "Consider a discrete-time Cyber-Physical System (CPS) linear plant controlled by a feedforward Rectified Linear Unit (ReLU) Neural Network (NN), within the context of digital twins and cyber-physical systems safety analysis. The plant evolves according to the discrete-time linear model and bounded process disturbance:\n$$\nx_{k+1} = A x_k + B u_k + w_k,\n$$\nwhere $x_k \\in \\mathbb{R}^n$ is the state at discrete time $k$, $u_k \\in \\mathbb{R}^m$ is the control input produced by the NN controller, $A \\in \\mathbb{R}^{n \\times n}$ is the plant state transition matrix, $B \\in \\mathbb{R}^{n \\times m}$ is the input matrix, and $w_k$ is a bounded disturbance modeled as a member of a zonotope. The NN controller is a one-hidden-layer ReLU network defined by\n$$\nu_k = W_2 \\, \\sigma\\!\\left(W_1 x_k + b_1\\right) + b_2,\n$$\nwhere $W_1 \\in \\mathbb{R}^{h \\times n}$ and $b_1 \\in \\mathbb{R}^{h}$ are the first-layer weights and biases, $W_2 \\in \\mathbb{R}^{m \\times h}$ and $b_2 \\in \\mathbb{R}^{m}$ are the output layer weights and biases, $h$ is the number of hidden units, and $\\sigma(\\cdot)$ is the element-wise Rectified Linear Unit activation function defined as $\\sigma(z)_i = \\max\\{0, z_i\\}$.\n\nThe initial state set and the disturbance set are represented as zonotopes. A zonotope in $\\mathbb{R}^n$ is defined as\n$$\n\\mathcal{Z}(c, G) = \\left\\{x \\in \\mathbb{R}^n \\; \\bigg| \\; x = c + G \\beta, \\; \\|\\beta\\|_\\infty \\le 1 \\right\\},\n$$\nwhere $c \\in \\mathbb{R}^n$ is the center, $G \\in \\mathbb{R}^{n \\times p}$ is the generator matrix, and $p$ is the number of generators. The disturbance zonotope $\\mathcal{W} = \\mathcal{Z}(c_w, G_w)$ is assumed centered at the origin, i.e., $c_w = 0$.\n\nThe safety requirement is an axis-aligned safe set defined as the hyperrectangle\n$$\nS = \\left\\{x \\in \\mathbb{R}^n \\; \\big| \\; -s_i \\le x_i \\le s_i \\text{ for all } i \\in \\{1, \\dots, n\\}\\right\\},\n$$\nfor a given vector $s \\in \\mathbb{R}^n_{0}$. The task is to compute a time-bounded reachable set over a finite horizon $H \\in \\mathbb{N}$ for the closed-loop system using zonotope propagation, and to return whether the reachable set at every step $k \\in \\{0,1,\\dots,H\\}$ is contained in the safe set $S$.\n\nPropagation rules that must be implemented:\n- Affine transformation of a zonotope: for $y = M x + b$, if $x \\in \\mathcal{Z}(c, G)$ then $y \\in \\mathcal{Z}(M c + b, M G)$.\n- Addition of zonotopes: if $x \\in \\mathcal{Z}(c_x, G_x)$ and $y \\in \\mathcal{Z}(c_y, G_y)$, then $x + y \\in \\mathcal{Z}(c_x + c_y, [G_x \\; G_y])$, where $[G_x \\; G_y]$ denotes horizontal concatenation of generators.\n- ReLU mapping over a zonotope is to be over-approximated as follows. For a pre-activation zonotope $\\mathcal{Z}(c, G)$ in $\\mathbb{R}^h$, compute lower and upper interval bounds for each coordinate $i$ via\n$$\n\\ell_i = c_i - \\sum_{j=1}^p |G_{i,j}|, \\quad u_i = c_i + \\sum_{j=1}^p |G_{i,j}|.\n$$\nThen define an over-approximation zonotope for the post-activation $y = \\sigma(z)$ coordinate-wise:\n1. If $\\ell_i \\ge 0$, pass the $i$-th row unchanged (exactly linear region).\n2. If $u_i \\le 0$, set the $i$-th output to identically zero (inactive region), i.e., center coordinate $0$ and zero generators in row $i$.\n3. If $\\ell_i  0  u_i$, over-approximate by an axis-aligned interval $[0, u_i]$ for the $i$-th coordinate: set the $i$-th center entry to $u_i/2$, zero-out existing generators in row $i$, and append one new generator column with a single nonzero entry $u_i/2$ at row $i$ (uncorrelated box over-approximation).\n\nUse the above rules to compute the time-bounded reachable zonotope $\\mathcal{Z}_k$ for each $k$ up to $H$:\n$$\n\\mathcal{Z}_{k+1} = A \\,\\mathcal{Z}_k + B \\,\\mathcal{U}(\\mathcal{Z}_k) + \\mathcal{W},\n$$\nwhere $\\mathcal{U}(\\mathcal{Z}_k)$ is the zonotope over-approximation of the NN controller output induced by the current state zonotope $\\mathcal{Z}_k$. At each time $k$, test if $\\mathcal{Z}_k \\subseteq S$ using interval bounds of the zonotope.\n\nYour program must implement the above procedure and evaluate the following test suite. All values are given in exact form; implement them exactly as specified.\n\nTest Case $1$ (nominal stable dynamics, small uncertainty):\n- State dimension $n = 2$, control dimension $m = 1$, hidden size $h = 2$, horizon $H = 5$.\n- Plant matrices:\n$$\nA = \\begin{bmatrix} 0.98  0.05 \\\\ 0  0.95 \\end{bmatrix}, \\quad\nB = \\begin{bmatrix} 0.10 \\\\ 0.05 \\end{bmatrix}.\n$$\n- Controller parameters:\n$$\nW_1 = \\begin{bmatrix} 0.60  -0.25 \\\\ 0.20  0.40 \\end{bmatrix}, \\quad\nb_1 = \\begin{bmatrix} 0.00 \\\\ -0.05 \\end{bmatrix}, \\quad\nW_2 = \\begin{bmatrix} 0.70  -0.30 \\end{bmatrix}, \\quad\nb_2 = \\begin{bmatrix} 0.00 \\end{bmatrix}.\n$$\n- Initial state zonotope:\n$$\nc_0 = \\begin{bmatrix} 0.10 \\\\ -0.05 \\end{bmatrix}, \\quad\nG_0 = \\begin{bmatrix} 0.05  0.00 \\\\ 0.00  0.05 \\end{bmatrix}.\n$$\n- Disturbance zonotope (center zero):\n$$\nG_w = \\begin{bmatrix} 0.01  0.00 \\\\ 0.00  0.01 \\end{bmatrix}.\n$$\n- Safe set half-widths:\n$$\ns = \\begin{bmatrix} 0.50 \\\\ 0.50 \\end{bmatrix}.\n$$\n\nTest Case $2$ (near-boundary initial condition, moderate uncertainty):\n- Dimensions $n = 2$, $m = 1$, $h = 2$, horizon $H = 4$.\n- Plant matrices:\n$$\nA = \\begin{bmatrix} 0.98  0.05 \\\\ 0  0.95 \\end{bmatrix}, \\quad\nB = \\begin{bmatrix} 0.10 \\\\ 0.05 \\end{bmatrix}.\n$$\n- Controller parameters:\n$$\nW_1 = \\begin{bmatrix} 0.60  -0.25 \\\\ 0.20  0.40 \\end{bmatrix}, \\quad\nb_1 = \\begin{bmatrix} 0.00 \\\\ -0.05 \\end{bmatrix}, \\quad\nW_2 = \\begin{bmatrix} 0.70  -0.30 \\end{bmatrix}, \\quad\nb_2 = \\begin{bmatrix} 0.00 \\end{bmatrix}.\n$$\n- Initial state zonotope:\n$$\nc_0 = \\begin{bmatrix} 0.45 \\\\ 0.45 \\end{bmatrix}, \\quad\nG_0 = \\begin{bmatrix} 0.02  0.00 \\\\ 0.00  0.02 \\end{bmatrix}.\n$$\n- Disturbance zonotope:\n$$\nG_w = \\begin{bmatrix} 0.005  0.00 \\\\ 0.00  0.005 \\end{bmatrix}.\n$$\n- Safe set half-widths:\n$$\ns = \\begin{bmatrix} 0.50 \\\\ 0.50 \\end{bmatrix}.\n$$\n\nTest Case $3$ (mildly unstable dynamics, larger disturbance):\n- Dimensions $n = 2$, $m = 1$, $h = 2$, horizon $H = 5$.\n- Plant matrices:\n$$\nA = \\begin{bmatrix} 1.20  0.00 \\\\ 0.00  1.10 \\end{bmatrix}, \\quad\nB = \\begin{bmatrix} 0.05 \\\\ 0.02 \\end{bmatrix}.\n$$\n- Controller parameters:\n$$\nW_1 = \\begin{bmatrix} 0.50  -0.20 \\\\ 0.30  0.30 \\end{bmatrix}, \\quad\nb_1 = \\begin{bmatrix} 0.00 \\\\ 0.00 \\end{bmatrix}, \\quad\nW_2 = \\begin{bmatrix} 0.60  -0.10 \\end{bmatrix}, \\quad\nb_2 = \\begin{bmatrix} 0.00 \\end{bmatrix}.\n$$\n- Initial state zonotope:\n$$\nc_0 = \\begin{bmatrix} 0.20 \\\\ 0.20 \\end{bmatrix}, \\quad\nG_0 = \\begin{bmatrix} 0.02  0.00 \\\\ 0.00  0.02 \\end{bmatrix}.\n$$\n- Disturbance zonotope:\n$$\nG_w = \\begin{bmatrix} 0.05  0.00 \\\\ 0.00  0.05 \\end{bmatrix}.\n$$\n- Safe set half-widths:\n$$\ns = \\begin{bmatrix} 0.50 \\\\ 0.50 \\end{bmatrix}.\n$$\n\nYour program must:\n1. Implement zonotope propagation for the closed-loop system over the specified horizon $H$ for each test case, using the affine and ReLU over-approximations described above.\n2. At each time $k \\in \\{0,1,\\dots,H\\}$, compute interval bounds for the current state zonotope via\n$$\n\\ell = c - \\sum_{j=1}^p |G_{:,j}|, \\quad u = c + \\sum_{j=1}^p |G_{:,j}|,\n$$\nand check whether $\\ell_i \\ge -s_i$ and $u_i \\le s_i$ holds for all $i$.\n3. Return a boolean per test case indicating whether the system is safe at all time steps up to $H$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results for the three test cases as a comma-separated list enclosed in square brackets (for example, $[{\\tt True},{\\tt False},{\\tt True}]$). No additional text should be printed.",
            "solution": "The user-provided problem has been analyzed and deemed valid as it is scientifically grounded, well-posed, and self-contained. The task is to perform safety verification for a discrete-time Cyber-Physical System (CPS) controlled by a Rectified Linear Unit (ReLU) Neural Network (NN). This involves computing the forward reachable set of the system over a finite time horizon using zonotope-based over-approximations.\n\nThe core of the methodology lies in the propagation of zonotopes through the system's dynamics, which include linear transformations and the non-linear ReLU activation function. A zonotope $\\mathcal{Z}(c, G)$ is a set of points defined by a center $c \\in \\mathbb{R}^n$ and a generator matrix $G \\in \\mathbb{R}^{n \\times p}$ as $\\mathcal{Z}(c, G) = \\{c + G \\beta \\mid \\|\\beta\\|_\\infty \\le 1\\}$.\n\nThe overall verification procedure is as follows:\nStarting with the initial state zonotope $\\mathcal{Z}_0 = \\mathcal{Z}(c_0, G_0)$, we iteratively compute the reachable state zonotope $\\mathcal{Z}_{k+1}$ for $k = 0, 1, \\dots, H-1$. At each step $k \\in \\{0, 1, \\dots, H\\}$, we verify if the current reachable set $\\mathcal{Z}_k$ is contained within the specified safe set $S$.\n\nThe computation of the subsequent reachable set $\\mathcal{Z}_{k+1}$ from $\\mathcal{Z}_k$ involves several steps based on the closed-loop system dynamics $x_{k+1} = A x_k + B u_k + w_k$, where $u_k$ is the output of the NN controller.\n\n1.  **Safety Verification at Step $k$**: Before propagation, we check if $\\mathcal{Z}_k = \\mathcal{Z}(c_k, G_k)$ is entirely within the safe set $S = \\{x \\mid -s_i \\le x_i \\le s_i\\}$. This is achieved by computing the axis-aligned hyper-rectangle that tightly encloses the zonotope. The lower and upper bounds for each dimension $i$ are given by:\n    $$\n    \\ell_i = c_{k,i} - \\sum_{j=1}^{p} |G_{k,i,j}|\n    $$\n    $$\n    u_i = c_{k,i} + \\sum_{j=1}^{p} |G_{k,i,j}|\n    $$\n    The safety condition is satisfied if $\\ell_i \\ge -s_i$ and $u_i \\le s_i$ for all dimensions $i=1, \\dots, n$. If this condition is violated at any step $k$, the system is deemed unsafe, and the verification for the current test case terminates with a `False` result.\n\n2.  **NN Controller Output Over-approximation**: We propagate the current state zonotope $\\mathcal{Z}_k$ through the NN controller to obtain an over-approximation of the control input set, $\\mathcal{U}_k$.\n    a.  **First Affine Layer**: The pre-activation values for the hidden layer form a zonotope $\\mathcal{Z}_{pre}$. Given $\\mathcal{Z}_k = \\mathcal{Z}(c_k, G_k)$, the transformation is $z = W_1 x_k + b_1$. Using the affine transformation rule for zonotopes, we get:\n        $$\n        \\mathcal{Z}_{pre} = \\mathcal{Z}(W_1 c_k + b_1, W_1 G_k)\n        $$\n    b.  **ReLU Activation Over-approximation**: The element-wise ReLU function $\\sigma(\\cdot)$ is applied to $\\mathcal{Z}_{pre} = \\mathcal{Z}(c_{pre}, G_{pre})$. A new zonotope $\\mathcal{Z}_{post} = \\mathcal{Z}(c_{post}, G_{post})$ is constructed by over-approximating the ReLU output for each of the $h$ hidden neurons. For each neuron $i \\in \\{1, \\dots, h\\}$, we first compute its pre-activation interval bounds $[\\ell_i, u_i]$.\n        - If $\\ell_i \\ge 0$ (always active), the neuron's output is linear. The $i$-th rows of $c_{post}$ and $G_{post}$ are identical to those of $c_{pre}$ and $G_{pre}$.\n        - If $u_i \\le 0$ (always inactive), the neuron's output is zero. The $i$-th rows of $c_{post}$ and $G_{post}$ are set to zero.\n        - If $\\ell_i  0  u_i$ (uncertain), the output lies in $[0, u_i]$. This is over-approximated by a new zonotope for this dimension with center $u_i/2$ and a new, independent generator of magnitude $u_i/2$. This adds one column to the generator matrix $G_{post}$.\n    c.  **Second Affine Layer**: The final control input zonotope $\\mathcal{U}_k = \\mathcal{Z}(c_u, G_u)$ is computed by applying the output layer transformation $u_k = W_2 y_k + b_2$ to $\\mathcal{Z}_{post}$:\n        $$\n        \\mathcal{U}_k = \\mathcal{Z}(W_2 c_{post} + b_2, W_2 G_{post})\n        $$\n\n3.  **Plant Dynamics Propagation**: The state zonotope for the next time step, $\\mathcal{Z}_{k+1}$, is the Minkowski sum of three zonotopes: the effect of the plant dynamics on the current state, the effect of the control input, and the process disturbance.\n    - $A \\mathcal{Z}_k = \\mathcal{Z}(A c_k, A G_k)$\n    - $B \\mathcal{U}_k = \\mathcal{Z}(B c_u, B G_u)$\n    - $\\mathcal{W} = \\mathcal{Z}(0, G_w)$\n    \n    Using the Minkowski sum rule, which involves summing the centers and concatenating the generator matrices, we obtain $\\mathcal{Z}_{k+1} = \\mathcal{Z}(c_{k+1}, G_{k+1})$:\n    $$\n    c_{k+1} = A c_k + B c_u\n    $$\n    $$\n    G_{k+1} = [A G_k, B G_u, G_w]\n    $$\n\nThis process is repeated for $k = 0, \\dots, H-1$. If the safety check passes for all steps $k=0, \\dots, H$, the system is declared safe for the given horizon, and the result is `True`.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the verification for all test cases.\n    \"\"\"\n\n    test_cases = [\n        # Test Case 1\n        {\n            \"n\": 2, \"m\": 1, \"h\": 2, \"H\": 5,\n            \"A\": np.array([[0.98, 0.05], [0.0, 0.95]]),\n            \"B\": np.array([[0.10], [0.05]]),\n            \"W1\": np.array([[0.60, -0.25], [0.20, 0.40]]),\n            \"b1\": np.array([[0.00], [-0.05]]),\n            \"W2\": np.array([[0.70, -0.30]]),\n            \"b2\": np.array([[0.00]]),\n            \"c0\": np.array([[0.10], [-0.05]]),\n            \"G0\": np.array([[0.05, 0.00], [0.00, 0.05]]),\n            \"Gw\": np.array([[0.01, 0.00], [0.00, 0.01]]),\n            \"s\": np.array([[0.50], [0.50]]),\n        },\n        # Test Case 2\n        {\n            \"n\": 2, \"m\": 1, \"h\": 2, \"H\": 4,\n            \"A\": np.array([[0.98, 0.05], [0.0, 0.95]]),\n            \"B\": np.array([[0.10], [0.05]]),\n            \"W1\": np.array([[0.60, -0.25], [0.20, 0.40]]),\n            \"b1\": np.array([[0.00], [-0.05]]),\n            \"W2\": np.array([[0.70, -0.30]]),\n            \"b2\": np.array([[0.00]]),\n            \"c0\": np.array([[0.45], [0.45]]),\n            \"G0\": np.array([[0.02, 0.00], [0.00, 0.02]]),\n            \"Gw\": np.array([[0.005, 0.00], [0.00, 0.005]]),\n            \"s\": np.array([[0.50], [0.50]]),\n        },\n        # Test Case 3\n        {\n            \"n\": 2, \"m\": 1, \"h\": 2, \"H\": 5,\n            \"A\": np.array([[1.20, 0.00], [0.00, 1.10]]),\n            \"B\": np.array([[0.05], [0.02]]),\n            \"W1\": np.array([[0.50, -0.20], [0.30, 0.30]]),\n            \"b1\": np.array([[0.00], [0.00]]),\n            \"W2\": np.array([[0.60, -0.10]]),\n            \"b2\": np.array([[0.00]]),\n            \"c0\": np.array([[0.20], [0.20]]),\n            \"G0\": np.array([[0.02, 0.00], [0.00, 0.02]]),\n            \"Gw\": np.array([[0.05, 0.00], [0.00, 0.05]]),\n            \"s\": np.array([[0.50], [0.50]]),\n        },\n    ]\n\n    results = []\n    for params in test_cases:\n        results.append(run_verification(params))\n    \n    # Format the print statement exactly as specified.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef relu_approx(zono):\n    \"\"\"\n    Over-approximates the ReLU function for a given zonotope.\n    zono: A tuple (c, G) representing the zonotope.\n    \"\"\"\n    c, G = zono\n    h = c.shape[0]\n\n    c_post = np.zeros_like(c)\n    G_base = np.zeros_like(G)\n    new_generators = []\n\n    # Calculate interval bounds for each neuron's pre-activation\n    if G.shape[1]  0:\n        interval_radii = np.sum(np.abs(G), axis=1, keepdims=True)\n    else:\n        interval_radii = np.zeros_like(c)\n        \n    l_bounds = c - interval_radii\n    u_bounds = c + interval_radii\n\n    for i in range(h):\n        l_i, u_i = l_bounds[i, 0], u_bounds[i, 0]\n        \n        if l_i = 0:  # Case 1: Always active\n            c_post[i] = c[i]\n            G_base[i, :] = G[i, :]\n        elif u_i = 0:  # Case 2: Always inactive\n            c_post[i] = 0\n            G_base[i, :] = 0\n        else:  # Case 3: Uncertain\n            c_post[i] = u_i / 2.0\n            G_base[i, :] = 0  # Zero out existing generators for this dimension\n            new_gen = np.zeros((h, 1))\n            new_gen[i] = u_i / 2.0\n            new_generators.append(new_gen)\n\n    if new_generators:\n        G_new = np.hstack(new_generators)\n        if G_base.shape[1]  0:\n            G_post = np.hstack((G_base, G_new))\n        else:\n            G_post = G_new\n    else:\n        G_post = G_base\n\n    return (c_post, G_post)\n\ndef run_verification(params):\n    \"\"\"\n    Runs the zonotope-based reachability analysis for a single test case.\n    \"\"\"\n    H = params[\"H\"]\n    A, B = params[\"A\"], params[\"B\"]\n    W1, b1 = params[\"W1\"], params[\"b1\"]\n    W2, b2 = params[\"W2\"], params[\"b2\"]\n    Gw = params[\"Gw\"]\n    s = params[\"s\"]\n\n    current_zono = (params[\"c0\"], params[\"G0\"])\n\n    for k in range(H + 1):\n        # 1. Safety Check\n        c_k, G_k = current_zono\n        if G_k.shape[1]  0:\n            interval_radii = np.sum(np.abs(G_k), axis=1, keepdims=True)\n        else:\n            interval_radii = np.zeros_like(c_k)\n            \n        lower_bounds = c_k - interval_radii\n        upper_bounds = c_k + interval_radii\n        \n        if not np.all((lower_bounds = -s)  (upper_bounds = s)):\n            return False\n\n        # If it's the last step, no need to compute the next state\n        if k == H:\n            break\n\n        # 2. Propagate to compute next state zonotope\n        \n        # 2a. Controller output over-approximation\n        # First layer (affine)\n        c_pre = W1 @ c_k + b1\n        G_pre = W1 @ G_k\n        zono_pre = (c_pre, G_pre)\n        \n        # ReLU activation\n        c_post, G_post = relu_approx(zono_pre)\n\n        # Output layer (affine)\n        c_u = W2 @ c_post + b2\n        G_u = W2 @ G_post\n        \n        # 2b. Plant dynamics\n        # Term 1: A * x_k\n        c_ax = A @ c_k\n        G_ax = A @ G_k\n        \n        # Term 2: B * u_k\n        c_bu = B @ c_u\n        G_bu = B @ G_u\n\n        # Term 3: w_k (disturbance) is already a zonotope with center 0\n        \n        # 2c. Minkowski Sum for x_{k+1}\n        c_next = c_ax + c_bu\n        G_next = np.hstack([G_ax, G_bu, Gw])\n        \n        current_zono = (c_next, G_next)\n\n    return True\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}