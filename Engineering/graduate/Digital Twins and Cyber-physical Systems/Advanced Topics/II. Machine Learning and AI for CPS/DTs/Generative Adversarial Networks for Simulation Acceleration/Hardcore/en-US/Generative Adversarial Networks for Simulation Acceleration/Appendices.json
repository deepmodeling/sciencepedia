{
    "hands_on_practices": [
        {
            "introduction": "The theoretical design of a generative model is only the first step; for real-time digital twins, practical deployment is governed by hardware limitations. This exercise  provides a concrete method for estimating the resource requirements—parameter count, memory footprint, and computational load (FLOPs)—of a 3D convolutional generator. Mastering these calculations is essential for designing network architectures that are not only expressive but also fit within the strict memory and latency budgets of a target GPU, ensuring the feasibility of real-time simulation.",
            "id": "4224761",
            "problem": "A Digital Twin (DT) of a fluidic Cyber-Physical System (CPS) requires real-time volumetric field synthesis to accelerate high-fidelity simulation via a Generative Adversarial Network (GAN). The generator is a three-dimensional Convolutional Neural Network (CNN) that maps a latent tensor $z \\in \\mathbb{R}^{C_0 \\times D_0 \\times H_0 \\times W_0}$ to an output $y \\in \\mathbb{R}^{D \\times H \\times W}$. The network is strictly feedforward and uses three-dimensional transposed convolutions to progressively upsample $z$ to the target resolution. The Graphics Processing Unit (GPU) must satisfy real-time inference with constrained memory and compute.\n\nFundamental base for modeling:\n- A three-dimensional convolution or transposed convolution with $C_{\\mathrm{out}}$ output channels, $C_{\\mathrm{in}}$ input channels, and kernel size $k_D \\times k_H \\times k_W$ has $C_{\\mathrm{out}} \\cdot C_{\\mathrm{in}} \\cdot k_D \\cdot k_H \\cdot k_W$ weights and, if biases are enabled, $C_{\\mathrm{out}}$ biases.\n- Batch Normalization (BN) has two trainable parameters per output channel (scale and shift), and no bias is used in layers with BN.\n- The number of floating-point operations (FLOPs) for a convolutional forward pass (multiply-add counted as $2$ FLOPs) is $2 \\cdot C_{\\mathrm{out}} \\cdot D_{\\mathrm{out}} \\cdot H_{\\mathrm{out}} \\cdot W_{\\mathrm{out}} \\cdot k_D \\cdot k_H \\cdot k_W \\cdot C_{\\mathrm{in}}$.\n- Memory footprint in inference is the sum of parameter memory plus peak activation memory. Under sequential execution that retains the current and immediately previous feature maps, the peak activation footprint equals the maximum, over layers, of the sum of the element counts of the previous and current feature maps, multiplied by the bytes per element. Assume half precision ($16$-bit floating point), so each element occupies $2$ bytes.\n\nLayer specifications:\n- Latent input: $C_0 = 64$, $D_0 = 4$, $H_0 = 4$, $W_0 = 4$.\n- Layer $1$: transposed convolution with $C_1 = 256$, kernel $4 \\times 4 \\times 4$, stride $2$ on each axis, padding chosen so that $D_1 = 8$, $H_1 = 8$, $W_1 = 8$. Batch Normalization and Rectified Linear Unit (ReLU). Bias disabled.\n- Layer $2$: transposed convolution with $C_2 = 128$, kernel $4 \\times 4 \\times 4$, stride $2$, output $D_2 = 16$, $H_2 = 16$, $W_2 = 16$. Batch Normalization and Rectified Linear Unit (ReLU). Bias disabled.\n- Layer $3$: transposed convolution with $C_3 = 64$, kernel $4 \\times 4 \\times 4$, stride $2$, output $D_3 = 32$, $H_3 = 32$, $W_3 = 32$. Batch Normalization and Rectified Linear Unit (ReLU). Bias disabled.\n- Layer $4$: transposed convolution with $C_4 = 16$, kernel $4 \\times 4 \\times 4$, stride $2$, output $D_4 = 64$, $H_4 = 64$, $W_4 = 64$. Batch Normalization and Rectified Linear Unit (ReLU). Bias disabled.\n- Layer $5$: transposed convolution with $C_5 = 1$, kernel $4 \\times 4 \\times 4$, stride $1$ with padding chosen so that $D_5 = 64$, $H_5 = 64$, $W_5 = 64$. Hyperbolic tangent activation. No Batch Normalization. Bias enabled.\n\nResource constraints:\n- Half precision inference ($16$ bits per element, $2$ bytes per element).\n- GPU memory: total $M_{\\mathrm{tot}} = 6$ GiB, with $M_{\\mathrm{res}} = 2$ GiB reserved for non-network tasks; available memory for the network is $\\tilde{M} = M_{\\mathrm{tot}} - M_{\\mathrm{res}}$, expressed in GiB.\n- GPU compute throughput: $F = 10$ teraFLOP per second (TFLOP/s) sustained in half precision; real-time DT update budget per inference is $t = 33$ milliseconds.\n- Assume peak activation memory scales linearly with batch size $B$, and parameter memory does not scale with $B$.\n\nTasks:\n1. Compute the total trainable parameter count $P$ for the generator, including convolution weights, convolution biases where enabled, and Batch Normalization scale and shift parameters.\n2. Compute the peak activation element count per sample under the sequential two-feature-map retention assumption, and then its memory in bytes and in GiB for batch size $B = 1$.\n3. Compute the parameter memory in bytes and in GiB.\n4. Derive the memory-limited batch size $B_{\\mathrm{mem}}$ as the largest integer $B$ satisfying $M_{\\mathrm{params}} + B \\cdot M_{\\mathrm{act,peak}} \\leq \\tilde{M}$, where $M_{\\mathrm{params}}$ and $M_{\\mathrm{act,peak}}$ are expressed in the same unit (GiB).\n5. Compute the per-sample FLOPs for the entire forward pass, then derive the compute-limited batch size $B_{\\mathrm{comp}}$ as the largest integer $B$ satisfying $B \\cdot \\frac{\\text{FLOPs}_{\\mathrm{per\\ sample}}}{F} \\leq t$, with $F$ in FLOP/s and $t$ in seconds.\n6. Return the feasible batch size $B_{\\max} = \\min\\{B_{\\mathrm{mem}}, B_{\\mathrm{comp}}\\}$ as a single integer. Express the final answer with no units. If any intermediate numerical values are reported, express memory in GiB and, only if approximations are necessary, round to four significant figures.",
            "solution": "We begin from the stated definitions for parameter counts, Batch Normalization parameters, FLOPs for convolutions, and memory modeling. The network consists of five transposed convolution layers with the specified channel progressions and spatial sizes.\n\nStep $1$: Trainable parameter count $P$.\nFor a transposed convolution, the trainable weight count equals $C_{\\mathrm{out}} \\cdot C_{\\mathrm{in}} \\cdot k_D \\cdot k_H \\cdot k_W$. Biases are disabled for layers with Batch Normalization (BN) and enabled for the last layer without BN. Batch Normalization contributes $2 \\cdot C_{\\mathrm{out}}$ trainable parameters (scale and shift).\n\nLet $k_D = k_H = k_W = 4$, so $k_D \\cdot k_H \\cdot k_W = 4 \\cdot 4 \\cdot 4 = 64$ for all layers.\n\n- Layer $1$: $C_{\\mathrm{in}} = C_0 = 64$, $C_{\\mathrm{out}} = C_1 = 256$. Weights: $256 \\cdot 64 \\cdot 64 = 1{,}048{,}576$. BN parameters: $2 \\cdot 256 = 512$. Bias: disabled.\n- Layer $2$: $C_{\\mathrm{in}} = C_1 = 256$, $C_{\\mathrm{out}} = C_2 = 128$. Weights: $128 \\cdot 256 \\cdot 64 = 2{,}097{,}152$. BN parameters: $2 \\cdot 128 = 256$. Bias: disabled.\n- Layer $3$: $C_{\\mathrm{in}} = C_2 = 128$, $C_{\\mathrm{out}} = C_3 = 64$. Weights: $64 \\cdot 128 \\cdot 64 = 524{,}288$. BN parameters: $2 \\cdot 64 = 128$. Bias: disabled.\n- Layer $4$: $C_{\\mathrm{in}} = C_3 = 64$, $C_{\\mathrm{out}} = C_4 = 16$. Weights: $16 \\cdot 64 \\cdot 64 = 65{,}536$. BN parameters: $2 \\cdot 16 = 32$. Bias: disabled.\n- Layer $5$: $C_{\\mathrm{in}} = C_4 = 16$, $C_{\\mathrm{out}} = C_5 = 1$. Weights: $1 \\cdot 16 \\cdot 64 = 1{,}024$. BN: none. Bias: $1$.\n\nSumming weights: $1{,}048{,}576 + 2{,}097{,}152 + 524{,}288 + 65{,}536 + 1{,}024 = 3{,}736{,}576$.\nSumming BN parameters: $512 + 256 + 128 + 32 = 928$.\nBias in last layer: $1$.\nThus, total trainable parameters:\n$$\nP = 3{,}736{,}576 + 928 + 1 = 3{,}737{,}505.\n$$\n\nStep $2$: Peak activation element count per sample and memory.\nUnder the sequential retention of the current and immediately previous feature maps, the peak activation element count is computed as the maximum, over layers, of $S_{\\mathrm{prev}} + S_{\\mathrm{curr}}$, where $S_{\\mathrm{prev}}$ is the element count of the previous feature map and $S_{\\mathrm{curr}}$ is the element count of the current feature map.\n\nCompute the element counts:\n\n- Input to Layer $1$: $S_{\\mathrm{prev,1}} = C_0 \\cdot D_0 \\cdot H_0 \\cdot W_0 = 64 \\cdot 4 \\cdot 4 \\cdot 4 = 64 \\cdot 64 = 4{,}096$. Layer $1$ output: $S_{\\mathrm{curr,1}} = C_1 \\cdot D_1 \\cdot H_1 \\cdot W_1 = 256 \\cdot 8 \\cdot 8 \\cdot 8 = 256 \\cdot 512 = 131{,}072$. Sum: $S_1 = 4{,}096 + 131{,}072 = 135{,}168$.\n\n- Layer $2$: $S_{\\mathrm{prev,2}} = C_1 \\cdot D_1 \\cdot H_1 \\cdot W_1 = 256 \\cdot 8 \\cdot 8 \\cdot 8 = 131{,}072$. $S_{\\mathrm{curr,2}} = C_2 \\cdot D_2 \\cdot H_2 \\cdot W_2 = 128 \\cdot 16 \\cdot 16 \\cdot 16 = 128 \\cdot 4{,}096 = 524{,}288$. Sum: $S_2 = 131{,}072 + 524{,}288 = 655{,}360$.\n\n- Layer $3$: $S_{\\mathrm{prev,3}} = C_2 \\cdot D_2 \\cdot H_2 \\cdot W_2 = 128 \\cdot 4{,}096 = 524{,}288$. $S_{\\mathrm{curr,3}} = C_3 \\cdot D_3 \\cdot H_3 \\cdot W_3 = 64 \\cdot 32 \\cdot 32 \\cdot 32 = 64 \\cdot 32{,}768 = 2{,}097{,}152$. Sum: $S_3 = 524{,}288 + 2{,}097{,}152 = 2{,}621{,}440$.\n\n- Layer $4$: $S_{\\mathrm{prev,4}} = C_3 \\cdot D_3 \\cdot H_3 \\cdot W_3 = 64 \\cdot 32{,}768 = 2{,}097{,}152$. $S_{\\mathrm{curr,4}} = C_4 \\cdot D_4 \\cdot H_4 \\cdot W_4 = 16 \\cdot 64 \\cdot 64 \\cdot 64 = 16 \\cdot 262{,}144 = 4{,}194{,}304$. Sum: $S_4 = 2{,}097{,}152 + 4{,}194{,}304 = 6{,}291{,}456$.\n\n- Layer $5$: $S_{\\mathrm{prev,5}} = C_4 \\cdot D_4 \\cdot H_4 \\cdot W_4 = 16 \\cdot 262{,}144 = 4{,}194{,}304$. $S_{\\mathrm{curr,5}} = C_5 \\cdot D_5 \\cdot H_5 \\cdot W_5 = 1 \\cdot 64 \\cdot 64 \\cdot 64 = 262{,}144$. Sum: $S_5 = 4{,}194{,}304 + 262{,}144 = 4{,}456{,}448$.\n\nThe peak sum is $S_{\\mathrm{peak}} = \\max\\{S_1, S_2, S_3, S_4, S_5\\} = 6{,}291{,}456$ elements at Layer $4$.\n\nWith half precision ($2$ bytes per element), the peak activation memory per sample is\n$$\nM_{\\mathrm{act,peak}}^{\\mathrm{bytes}} = S_{\\mathrm{peak}} \\cdot 2 = 6{,}291{,}456 \\cdot 2 = 12{,}582{,}912 \\text{ bytes}.\n$$\nExpressed in gibibytes (GiB), using $1 \\text{ GiB} = 2^{30} \\text{ bytes}$,\n$$\nM_{\\mathrm{act,peak}}^{\\mathrm{GiB}} = \\frac{12{,}582{,}912}{2^{30}} \\ \\text{GiB} \\approx \\frac{12{,}582{,}912}{1{,}073{,}741{,}824} \\ \\text{GiB} \\approx 0.01173 \\ \\text{GiB}.\n$$\n\nStep $3$: Parameter memory in bytes and GiB.\nTotal trainable parameters $P = 3{,}737{,}505$. With half precision, each parameter uses $2$ bytes, so\n$$\nM_{\\mathrm{params}}^{\\mathrm{bytes}} = P \\cdot 2 = 3{,}737{,}505 \\cdot 2 = 7{,}475{,}010 \\text{ bytes}.\n$$\nIn GiB,\n$$\nM_{\\mathrm{params}}^{\\mathrm{GiB}} = \\frac{7{,}475{,}010}{2^{30}} \\ \\text{GiB} \\approx \\frac{7{,}475{,}010}{1{,}073{,}741{,}824} \\ \\text{GiB} \\approx 0.006963 \\ \\text{GiB}.\n$$\n\nStep $4$: Memory-limited batch size $B_{\\mathrm{mem}}$.\nAvailable memory for the network is\n$$\n\\tilde{M} = M_{\\mathrm{tot}} - M_{\\mathrm{res}} = 6 - 2 = 4 \\ \\text{GiB}.\n$$\nThe memory usage for batch size $B$ equals $M_{\\mathrm{params}}^{\\mathrm{GiB}} + B \\cdot M_{\\mathrm{act,peak}}^{\\mathrm{GiB}}$. The largest integer $B$ that satisfies the constraint is\n$$\nB_{\\mathrm{mem}} = \\left\\lfloor \\frac{\\tilde{M} - M_{\\mathrm{params}}^{\\mathrm{GiB}}}{M_{\\mathrm{act,peak}}^{\\mathrm{GiB}}} \\right\\rfloor = \\left\\lfloor \\frac{4 - 0.006963}{0.01173} \\right\\rfloor.\n$$\nCompute the numerator: $4 - 0.006963 = 3.993037$. Divide: $\\frac{3.993037}{0.01173} \\approx 340.4$. Taking the floor gives\n$$\nB_{\\mathrm{mem}} = 340.\n$$\n\nFor exactness using bytes, one can equivalently compute\n$$\nB_{\\mathrm{mem}} = \\left\\lfloor \\frac{4 \\cdot 2^{30} - 7{,}475{,}010}{12{,}582{,}912} \\right\\rfloor = \\left\\lfloor \\frac{4{,}294{,}967{,}296 - 7{,}475{,}010}{12{,}582{,}912} \\right\\rfloor = \\left\\lfloor \\frac{4{,}287{,}492{,}286}{12{,}582{,}912} \\right\\rfloor = 340,\n$$\nwhich matches.\n\nStep $5$: Compute-limited batch size $B_{\\mathrm{comp}}$.\nPer-layer FLOPs per sample are given by $2 \\cdot C_{\\mathrm{out}} \\cdot D_{\\mathrm{out}} \\cdot H_{\\mathrm{out}} \\cdot W_{\\mathrm{out}} \\cdot k_D \\cdot k_H \\cdot k_W \\cdot C_{\\mathrm{in}}$.\n\nCompute per layer:\n\n- Layer $1$: $C_{\\mathrm{in}} = 64$, $C_{\\mathrm{out}} = 256$, $D_1 H_1 W_1 = 8 \\cdot 8 \\cdot 8 = 512$, $k_D k_H k_W = 64$,\n$$\n\\text{FLOPs}_1 = 2 \\cdot 256 \\cdot 512 \\cdot 64 \\cdot 64 = 1{,}073{,}741{,}824.\n$$\n\n- Layer $2$: $C_{\\mathrm{in}} = 256$, $C_{\\mathrm{out}} = 128$, $D_2 H_2 W_2 = 16 \\cdot 16 \\cdot 16 = 4{,}096$,\n$$\n\\text{FLOPs}_2 = 2 \\cdot 128 \\cdot 4{,}096 \\cdot 64 \\cdot 256 = 17{,}179{,}869{,}184.\n$$\n\n- Layer $3$: $C_{\\mathrm{in}} = 128$, $C_{\\mathrm{out}} = 64$, $D_3 H_3 W_3 = 32 \\cdot 32 \\cdot 32 = 32{,}768$,\n$$\n\\text{FLOPs}_3 = 2 \\cdot 64 \\cdot 32{,}768 \\cdot 64 \\cdot 128 = 34{,}359{,}738{,}368.\n$$\n\n- Layer $4$: $C_{\\mathrm{in}} = 64$, $C_{\\mathrm{out}} = 16$, $D_4 H_4 W_4 = 64 \\cdot 64 \\cdot 64 = 262{,}144$,\n$$\n\\text{FLOPs}_4 = 2 \\cdot 16 \\cdot 262{,}144 \\cdot 64 \\cdot 64 = 34{,}359{,}738{,}368.\n$$\n\n- Layer $5$: $C_{\\mathrm{in}} = 16$, $C_{\\mathrm{out}} = 1$, $D_5 H_5 W_5 = 64 \\cdot 64 \\cdot 64 = 262{,}144$,\n$$\n\\text{FLOPs}_5 = 2 \\cdot 1 \\cdot 262{,}144 \\cdot 64 \\cdot 16 = 536{,}870{,}912.\n$$\n\nTotal per-sample forward FLOPs:\n$$\n\\text{FLOPs}_{\\mathrm{per\\ sample}} = 1{,}073{,}741{,}824 + 17{,}179{,}869{,}184 + 34{,}359{,}738{,}368 + 34{,}359{,}738{,}368 + 536{,}870{,}912 = 87{,}509{,}958{,}656.\n$$\n\nCompute throughput and time budget:\nThe sustained half-precision throughput is $F = 10 \\ \\text{TFLOP/s} = 10 \\times 10^{12} \\ \\text{FLOP/s}$. The time budget is $t = 33 \\ \\text{ms} = 33 \\times 10^{-3} \\ \\text{s}$. For batch size $B$, the total computation time is\n$$\nt_B = B \\cdot \\frac{\\text{FLOPs}_{\\mathrm{per\\ sample}}}{F}.\n$$\nThe largest integer $B$ satisfying $t_B \\leq t$ is\n$$\nB_{\\mathrm{comp}} = \\left\\lfloor \\frac{t \\cdot F}{\\text{FLOPs}_{\\mathrm{per\\ sample}}} \\right\\rfloor = \\left\\lfloor \\frac{33 \\times 10^{-3} \\cdot 10 \\times 10^{12}}{87{,}509{,}958{,}656} \\right\\rfloor = \\left\\lfloor \\frac{330 \\times 10^{9}}{87{,}509{,}958{,}656} \\right\\rfloor.\n$$\nCompute the ratio: $\\frac{330 \\times 10^{9}}{87{,}509{,}958{,}656} \\approx 3.7697$, so\n$$\nB_{\\mathrm{comp}} = 3.\n$$\n\nStep $6$: Feasible batch size $B_{\\max}$.\nWe take the minimum of the memory-limited and compute-limited batch sizes:\n$$\nB_{\\max} = \\min\\{B_{\\mathrm{mem}}, B_{\\mathrm{comp}}\\} = \\min\\{340, 3\\} = 3.\n$$\n\nTherefore, under the given GPU resource constraints for real-time DT inference, the feasible batch size is $B_{\\max} = 3$.",
            "answer": "$$\\boxed{3}$$"
        },
        {
            "introduction": "After training, a GAN's fidelity must be rigorously verified to ensure it can be trusted as a surrogate for a real physical process. This practice  demonstrates how to move beyond qualitative visual inspection to quantitative validation using classical statistical methods. You will apply a two-sample hypothesis test to compare a critical summary statistic—cycle-averaged power—between the real system and the GAN, providing a formal procedure to assess whether the model accurately reproduces the target data distribution.",
            "id": "4224819",
            "problem": "A Digital Twin (DT) of a power electronics module in a Cyber-Physical System (CPS) uses a Generative Adversarial Network (GAN) to accelerate simulation by proposing synthetic trajectories that approximate the true system output distribution. Let $y$ denote the trajectory of instantaneous power $p(t)$ over a control cycle of duration $T$, and consider the scalar summary statistic $s(y)$ defined by $s(y) = \\frac{1}{T} \\int_{0}^{T} p(t) \\, dt$, the cycle-averaged power. Assume independent samples of $s(y)$ are collected from the GAN-induced distribution $p_{g}$ and the real-data distribution $p_{\\text{data}}$.\n\nYou are given the following finite-sample summaries:\n- GAN samples: $n_{g} = 12$, sample mean $\\bar{s}_{g} = 103.2 \\ \\mathrm{W}$, unbiased sample variance $S_{g}^{2} = 25 \\ \\mathrm{W}^{2}$.\n- Real-data samples: $n_{\\text{data}} = 9$, sample mean $\\bar{s}_{\\text{data}} = 96.7 \\ \\mathrm{W}$, unbiased sample variance $S_{\\text{data}}^{2} = 64 \\ \\mathrm{W}^{2}$.\n\nStarting from foundational statistical principles valid for independent sampling and finite samples, establish a hypothesis test to compare $p_{g}$ and $p_{\\text{data}}$ with respect to the population mean of $s(y)$. Explicitly state the null and alternative hypotheses for the mean, construct a standardized test statistic appropriate for unknown and potentially unequal variances, compute its numerical value from the provided summaries, and specify how to control the Type I error at significance level $\\alpha = 0.05$ in the finite-sample setting by giving the symmetric two-sided rejection threshold.\n\nExpress the final reported threshold as a dimensionless number, and round your final answer to four significant figures.",
            "solution": "The objective is to test whether the population mean of the GAN-generated summary statistic, $\\mu_{g}$, is equal to the population mean of the real-data summary statistic, $\\mu_{\\text{data}}$. This is a two-sample hypothesis test for the equality of means.\n\nThe null hypothesis, $H_{0}$, posits that there is no difference between the population means. The alternative hypothesis, $H_{A}$, posits that there is a difference. As this is a two-sided test, the hypotheses are:\n$$ H_{0}: \\mu_{g} = \\mu_{\\text{data}} \\quad \\text{or} \\quad \\mu_{g} - \\mu_{\\text{data}} = 0 $$\n$$ H_{A}: \\mu_{g} \\neq \\mu_{\\text{data}} \\quad \\text{or} \\quad \\mu_{g} - \\mu_{\\text{data}} \\neq 0 $$\n\nThe samples are drawn independently. The population variances, $\\sigma_{g}^{2}$ and $\\sigma_{\\text{data}}^{2}$, are unknown. The problem explicitly requires a method appropriate for potentially unequal variances. Therefore, the Behrens-Fisher problem arises, for which the standard solution is Welch's t-test. The underlying assumption is that the two populations are approximately normally distributed, which is a reasonable assumption for invoking the t-distribution with small sample sizes.\n\nThe Welch's t-statistic is given by:\n$$ t = \\frac{(\\bar{s}_{g} - \\bar{s}_{\\text{data}}) - (\\mu_{g} - \\mu_{\\text{data}})_0}{\\sqrt{\\frac{S_{g}^{2}}{n_{g}} + \\frac{S_{\\text{data}}^{2}}{n_{\\text{data}}}}} $$\nUnder the null hypothesis, the expected difference $(\\mu_{g} - \\mu_{\\text{data}})_0$ is $0$. Substituting the given values:\n$\\bar{s}_{g} = 103.2$, $\\bar{s}_{\\text{data}} = 96.7$, $S_{g}^{2} = 25$, $n_{g} = 12$, $S_{\\text{data}}^{2} = 64$, $n_{\\text{data}} = 9$.\n\nThe numerator is the difference in sample means:\n$$ \\bar{s}_{g} - \\bar{s}_{\\text{data}} = 103.2 - 96.7 = 6.5 $$\nThe denominator is the standard error of the difference in means:\n$$ \\sqrt{\\frac{S_{g}^{2}}{n_{g}} + \\frac{S_{\\text{data}}^{2}}{n_{\\text{data}}}} = \\sqrt{\\frac{25}{12} + \\frac{64}{9}} = \\sqrt{\\frac{75}{36} + \\frac{256}{36}} = \\sqrt{\\frac{331}{36}} = \\frac{\\sqrt{331}}{6} \\approx 3.03223 $$\nThe value of the test statistic is:\n$$ t = \\frac{6.5}{\\frac{\\sqrt{331}}{6}} = \\frac{39}{\\sqrt{331}} \\approx 2.1430 $$\n\nThis statistic is approximately distributed as a Student's t-distribution with degrees of freedom, $\\nu$, calculated using the Welch-Satterthwaite equation:\n$$ \\nu \\approx \\frac{\\left(\\frac{S_{g}^{2}}{n_{g}} + \\frac{S_{\\text{data}}^{2}}{n_{\\text{data}}}\\right)^2}{\\frac{\\left(\\frac{S_{g}^{2}}{n_{g}}\\right)^2}{n_{g} - 1} + \\frac{\\left(\\frac{S_{\\text{data}}^{2}}{n_{\\text{data}}}\\right)^2}{n_{\\text{data}} - 1}} $$\nLet's compute the terms for the denominator of $\\nu$:\n$$ \\frac{\\left(\\frac{S_{g}^{2}}{n_{g}}\\right)^2}{n_{g} - 1} = \\frac{\\left(\\frac{25}{12}\\right)^2}{11} = \\frac{625/144}{11} = \\frac{625}{1584} $$\n$$ \\frac{\\left(\\frac{S_{\\text{data}}^{2}}{n_{\\text{data}}}\\right)^2}{n_{\\text{data}} - 1} = \\frac{\\left(\\frac{64}{9}\\right)^2}{8} = \\frac{4096/81}{8} = \\frac{512}{81} $$\nThe denominator of $\\nu$ is:\n$$ \\frac{625}{1584} + \\frac{512}{81} = \\frac{625 \\cdot 9 + 512 \\cdot 176}{14256} = \\frac{5625 + 90112}{14256} = \\frac{95737}{14256} $$\nThe numerator of $\\nu$ is:\n$$ \\left(\\frac{S_{g}^{2}}{n_{g}} + \\frac{S_{\\text{data}}^{2}}{n_{\\text{data}}}\\right)^2 = \\left(\\frac{331}{36}\\right)^2 = \\frac{109561}{1296} $$\nThus, the degrees of freedom are:\n$$ \\nu \\approx \\frac{109561/1296}{95737/14256} = \\frac{109561}{1296} \\cdot \\frac{14256}{95737} = \\frac{109561}{95737} \\cdot 11 \\approx 12.588 $$\n\nTo control the Type I error at a significance level of $\\alpha = 0.05$ for a two-sided test, we must find the critical value $t_{\\alpha/2, \\nu}$ from the t-distribution with $\\nu = 12.588$ degrees of freedom. The rejection region is defined by $|t| > t_{0.025, 12.588}$. This critical value is the threshold such that the area in the two tails of the distribution is $0.05$. Equivalently, we seek the value $t_{crit}$ such that $P(T > t_{crit}) = \\alpha/2 = 0.025$, where $T \\sim t(\\nu=12.588)$.\n\nUsing a computational tool or statistical calculator that can handle non-integer degrees of freedom for the inverse cumulative distribution function of the t-distribution, we find:\n$$ t_{crit} = t_{0.025, 12.588} \\approx 2.1678 $$\nThe symmetric two-sided rejection threshold is this critical value. The condition for rejecting $H_0$ is that the absolute value of our calculated test statistic exceeds this threshold. Here, $|t| = 2.1430$, which is less than the threshold of $2.1678$, so we would not reject the null hypothesis at the $\\alpha = 0.05$ level.\nThe problem, however, solely asks for the rejection threshold itself.\n\nRounding the critical value to four significant figures gives $2.168$. This is the dimensionless rejection threshold.",
            "answer": "$$\n\\boxed{2.168}\n$$"
        },
        {
            "introduction": "The ultimate purpose of a GAN surrogate in this context is to deliver tangible computational speedups to a physics-based solver. This exercise  bridges the gap between machine learning and numerical analysis by quantifying this acceleration. By modeling the physics solver as a fixed-point iteration governed by the Contraction Mapping Principle, you will derive the precise reduction in solver iterations achieved by using a high-quality initial guess from the GAN, directly connecting the model's predictive accuracy to real-world performance gains.",
            "id": "4224832",
            "problem": "Consider a cyber-physical digital twin of a thermal-fluid process in co-simulation with a controller, where each global time step requires solving an implicit nonlinear interface equilibrium. The physics subsolver is executed via a fixed-point iteration defined by a nonlinear operator $T$ on a Banach space with norm $|\\!|\\cdot|\\!|$, where the exact equilibrium $x^{\\star}$ satisfies $T(x^{\\star}) = x^{\\star}$. The solver is known to converge locally under the following foundational base: the mapping $T$ is a contraction on a neighborhood $\\mathcal{U}$ containing $x^{\\star}$, with contraction constant $c \\in (0,1)$ such that $|\\!|T(x) - T(y)|\\!| \\leq c \\, |\\!|x - y|\\!|$ for all $x,y \\in \\mathcal{U}$. The stopping criterion is a normed error threshold $|\\!|x_{k} - x^{\\star}|\\!| \\leq \\varepsilon$.\n\nTo accelerate convergence at each time step, a Generative Adversarial Network (GAN) surrogate trained offline on historical solver trajectories is used to provide an initial guess $\\tilde{x}_{0}$ for the physics solver. Empirically, the surrogate reduces the initial error magnitude by a multiplicative factor $0 < \\gamma < 1$, i.e., $|\\!|\\tilde{x}_{0} - x^{\\star}|\\!| = \\gamma \\, |\\!|x_{0} - x^{\\star}|\\!|$, where $x_{0}$ is the baseline initial guess (e.g., extrapolated from the previous time step). Assume the co-simulation loop deploys the same physics solver and operator $T$ with unchanged contraction constant $c$ when the GAN surrogate is used.\n\nStarting from the fundamental contraction properties and the stopping criterion above, derive the convergence condition under which the iteration count decreases from the baseline $N$ (without the GAN surrogate) to $N'$ (with the GAN surrogate), expressed in terms of $c$ and $\\gamma$. Then, for a scientifically realistic setting with contraction constant $c = 0.52$, baseline initial error $|\\!|x_{0} - x^{\\star}|\\!| = 1 \\times 10^{-2}$, error tolerance $\\varepsilon = 1 \\times 10^{-6}$, and GAN improvement factor $\\gamma = 0.08$, compute the resulting integer number of iterations $N'$ required by the solver when initialized by the GAN surrogate. Express the final numerical answer as an integer with no units.",
            "solution": "The core of the analysis rests on the error propagation in a fixed-point iteration governed by a contraction mapping. Let $x_k$ be the iterate at step $k$. The error with respect to the exact solution $x^{\\star}$ is $|\\!|x_k - x^{\\star}|\\!|$. Using the fact that $T(x^{\\star}) = x^{\\star}$ and the contraction property, we can establish an error bound:\n$$|\\!|x_k - x^{\\star}|\\!| = |\\!|T(x_{k-1}) - T(x^{\\star})|\\!| \\leq c \\, |\\!|x_{k-1} - x^{\\star}|\\!|$$\nApplying this inequality recursively from the initial guess ($k=0$) yields the error bound at iteration $k$:\n$$|\\!|x_k - x^{\\star}|\\!| \\leq c^k \\, |\\!|x_0 - x^{\\star}|\\!|$$\n\n**Baseline Case (without GAN)**\nLet $E_0 = |\\!|x_0 - x^{\\star}|\\!|$ be the initial error. The solver runs for $N$ iterations until the stopping criterion $|\\!|x_N - x^{\\star}|\\!| \\leq \\varepsilon$ is met. To guarantee convergence, we require the upper bound on the error to satisfy the criterion:\n$$c^N E_0 \\leq \\varepsilon$$\nTo find the required number of iterations $N$, we solve for it. Since $c \\in (0,1)$ and we assume $E_0 > \\varepsilon$, we take the natural logarithm of both sides:\n$$N \\ln(c) + \\ln(E_0) \\leq \\ln(\\varepsilon)$$\nAs $c \\in (0,1)$, its logarithm $\\ln(c)$ is negative. Dividing by $\\ln(c)$ reverses the inequality sign:\n$$N \\geq \\frac{\\ln(\\varepsilon) - \\ln(E_0)}{\\ln(c)} = \\frac{\\ln(\\varepsilon / E_0)}{\\ln(c)}$$\nSince $N$ must be an integer, it is the smallest integer that satisfies this condition:\n$$N = \\left\\lceil \\frac{\\ln(\\varepsilon / |\\!|x_0 - x^{\\star}|\\!|)}{\\ln(c)} \\right\\rceil$$\n\n**Accelerated Case (with GAN)**\nWith the GAN surrogate, the initial guess is $\\tilde{x}_0$ and the initial error is $\\tilde{E}_0 = |\\!|\\tilde{x}_0 - x^{\\star}|\\!| = \\gamma |\\!|x_0 - x^{\\star}|\\!| = \\gamma E_0$. The solver now requires $N'$ iterations. The stopping condition is guaranteed if:\n$$c^{N'} \\tilde{E}_0 \\leq \\varepsilon \\implies c^{N'} \\gamma E_0 \\leq \\varepsilon$$\nSolving for $N'$ in the same manner as for $N$:\n$$N' \\ln(c) + \\ln(\\gamma) + \\ln(E_0) \\leq \\ln(\\varepsilon)$$\n$$N' \\geq \\frac{\\ln(\\varepsilon) - \\ln(\\gamma) - \\ln(E_0)}{\\ln(c)} = \\frac{\\ln(\\varepsilon / (\\gamma E_0))}{\\ln(c)}$$\nThe number of iterations $N'$ is therefore:\n$$N' = \\left\\lceil \\frac{\\ln(\\varepsilon / (\\gamma |\\!|x_0 - x^{\\star}|\\!|))}{\\ln(c)} \\right\\rceil$$\n\n**Convergence Condition for Iteration Decrease**\nThe problem asks for the condition under which the iteration count decreases, i.e., $N' < N$. A decrease in the iteration count is expected if the GAN provides a better initial guess. The argument of the ceiling function for $N'$ can be related to that for $N$:\n$$\\frac{\\ln(\\varepsilon / (\\gamma E_0))}{\\ln(c)} = \\frac{\\ln(\\varepsilon / E_0) - \\ln(\\gamma)}{\\ln(c)} = \\frac{\\ln(\\varepsilon / E_0)}{\\ln(c)} - \\frac{\\ln(\\gamma)}{\\ln(c)}$$\nThe term $-\\frac{\\ln(\\gamma)}{\\ln(c)} = \\log_c(\\gamma)$ represents the reduction in the (real-valued) iteration count estimate. For the number of iterations to decrease, this reduction must be positive: $\\log_c(\\gamma) > 0$. This inequality holds true if and only if one of two conditions is met:\n1. $c > 1$ and $\\gamma > 1$\n2. $0 < c < 1$ and $0 < \\gamma < 1$\n\nThe problem statement explicitly gives $c \\in (0,1)$ and $\\gamma \\in (0,1)$, which corresponds to the second case. Therefore, the conditions provided in the problem statement itself, $c \\in (0,1)$ and $\\gamma \\in (0,1)$, are precisely the conditions that guarantee a reduction in the theoretical minimum number of iterations. This can also be expressed as $(c-1)(\\gamma-1) > 0$. Under these conditions, the argument of the ceiling function for $N'$ is strictly less than that for $N$, ensuring $N' \\leq N$. A strict decrease $N' < N$ occurs unless the reduction is too small to make the argument cross an integer boundary, which is an edge case.\n\n**Numerical Computation of $N'$**\nWe now compute the value of $N'$ using the given numerical data:\n$c = 0.52$, $|\\!|x_{0} - x^{\\star}|\\!| = 1 \\times 10^{-2}$, $\\varepsilon = 1 \\times 10^{-6}$, and $\\gamma = 0.08$.\n\nSubstituting these values into the derived formula for $N'$:\n$$N' = \\left\\lceil \\frac{\\ln\\left(\\frac{1 \\times 10^{-6}}{0.08 \\times (1 \\times 10^{-2})}\\right)}{\\ln(0.52)} \\right\\rceil$$\nFirst, we evaluate the term inside the logarithm:\n$$\\frac{1 \\times 10^{-6}}{0.08 \\times 1 \\times 10^{-2}} = \\frac{10^{-6}}{8 \\times 10^{-2} \\times 10^{-2}} = \\frac{10^{-6}}{8 \\times 10^{-4}} = \\frac{1}{8} \\times 10^{-2} = 0.125 \\times 10^{-2} = 1.25 \\times 10^{-3}$$\nNow, we substitute this back into the expression for $N'$:\n$$N' = \\left\\lceil \\frac{\\ln(1.25 \\times 10^{-3})}{\\ln(0.52)} \\right\\rceil$$\nUsing the values of the natural logarithms:\n$\\ln(1.25 \\times 10^{-3}) \\approx -6.68461$\n$\\ln(0.52) \\approx -0.65393$\nSo, the ratio is:\n$$\\frac{-6.68461}{-0.65393} \\approx 10.22228$$\nFinally, we apply the ceiling function to find the smallest integer number of iterations:\n$$N' = \\lceil 10.22228 \\rceil = 11$$\nTherefore, the solver initialized by the GAN surrogate requires $11$ iterations to meet the specified error tolerance.",
            "answer": "$$\\boxed{11}$$"
        }
    ]
}