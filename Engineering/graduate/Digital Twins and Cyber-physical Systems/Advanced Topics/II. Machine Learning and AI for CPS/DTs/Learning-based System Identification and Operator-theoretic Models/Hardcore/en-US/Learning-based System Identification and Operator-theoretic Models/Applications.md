## Applications and Interdisciplinary Connections

Having established the theoretical foundations of learning-based [system identification](@entry_id:201290) and [operator-theoretic models](@entry_id:1129150), we now turn to their application. This chapter explores how these principles are utilized in diverse, real-world, and interdisciplinary contexts, particularly in the development of high-fidelity digital twins for complex cyber-physical systems (CPS). The objective is not to reiterate the core mechanics, but to demonstrate their utility, extension, and integration in solving concrete scientific and engineering challenges. We will investigate how these advanced modeling techniques are adapted to handle control inputs, inherent [stochasticity](@entry_id:202258), complex hybrid dynamics, and the practical limitations of [data acquisition](@entry_id:273490). By grounding the theory in applied problems, we illuminate the path from mathematical abstraction to functional, predictive digital counterparts of physical systems.

### Modeling and Control of Complex Nonlinear Systems

A primary motivation for developing sophisticated system models is to enable the analysis and control of complex nonlinear dynamics. Operator-theoretic methods offer a compelling paradigm by transforming nonlinear problems into a linear framework, where a vast toolkit from [linear systems theory](@entry_id:172825) can be applied. This section explores the application of these methods to systems with external inputs, the design of controllers in the lifted space, and the modeling of systems with discontinuous, switching behavior.

#### Incorporating Control Inputs

The foundational Koopman [operator theory](@entry_id:139990) is defined for autonomous dynamical systems. However, most cyber-physical systems are non-autonomous, actively influenced by control inputs. A critical step in making [operator-theoretic models](@entry_id:1129150) practical is extending them to accommodate these inputs. For a control-affine system, described by $x_{t+1} = f(x_t) + G(x_t) u_t$, one common approach is to treat the input $u_t$ as a parameter. This yields a family of Koopman operators, each corresponding to a fixed control value. In the context of learning-based [system identification](@entry_id:201290), this concept is realized through models such as Koopman with Input and Control (KIC). The KIC framework posits that the dynamics of a lifted state vector of [observables](@entry_id:267133), $\psi(x_t)$, can be approximated by a linear model that is driven by the control input (or a lifted version thereof). This results in a learned model of the form $\psi(x_{t+1}) \approx K_x \psi(x_t) + K_u \phi(u_t)$, where $\psi$ and $\phi$ are dictionaries of observables for the state and input, respectively. The matrices $K_x$ and $K_u$ are identified from data, effectively creating a [linear representation](@entry_id:139970) of the [non-autonomous system](@entry_id:173309) that is amenable to analysis and [controller design](@entry_id:274982). 

#### Application to Controller Design

Once a linear lifted model that incorporates control, such as $z_{k+1} = A_\ell z_k + B_\ell u_k$ where $z_k=\psi(x_k)$, has been identified, a natural next step is to design a controller in this lifted space. For instance, one might design a linear [state-feedback controller](@entry_id:203349) $u_k = K_c z_k$ to stabilize the lifted system. A crucial question arises: under what conditions will such a controller, when applied to the original [nonlinear system](@entry_id:162704), achieve the desired effect? The answer lies in the relationship between the properties of the original system and its lifted representation. If the lifting map $\psi$ constitutes an [invertible linear transformation](@entry_id:149915) (a [similarity transformation](@entry_id:152935)) between the original state space and the lifted space, then fundamental system properties such as [controllability](@entry_id:148402) are preserved. In this scenario, a controller designed for the lifted system has an equivalent representation in the original coordinates, and stability properties are likewise preserved. This ensures that stabilizing the lifted model corresponds directly to locally stabilizing the original [nonlinear system](@entry_id:162704) around its equilibrium. However, if the lifting introduces uncontrollable modes or is non-injective, designing effective controllers in the lifted space becomes significantly more challenging, as the learned model may not accurately capture the system's response to inputs. 

#### Modeling Hybrid and Switched Systems

Many cyber-physical systems exhibit hybrid dynamics, characterized by continuous evolution within discrete operating modes and instantaneous switching between them. A robotic arm making and breaking contact or a power grid switching between different topologies are prime examples. At first glance, these discontinuous dynamics seem ill-suited to the Koopman framework. However, the flexibility of the observable dictionary provides a powerful mechanism to handle such systems. By augmenting the dictionary with [indicator functions](@entry_id:186820) that identify the current discrete mode (e.g., [observables](@entry_id:267133) $\mathbf{1}_{\{m=1\}}$ and $\mathbf{1}_{\{m=2\}}$ for a two-mode system) and products of these indicators with state-dependent observables (e.g., $x \cdot \mathbf{1}_{\{m=1\}}$), the piecewise nature of the dynamics can be explicitly encoded into the lifted state vector. A mode transition is then represented as a transfer of energy or activity between different sets of coordinates in the lifted space. A finite-dimensional Koopman [matrix approximation](@entry_id:149640), learned via Extended Dynamic Mode Decomposition (EDMD), can capture this switching behavior as a linear mapping. This approach linearizes the entire hybrid system—both the continuous flows and the discrete transitions—in the space of observables, enabling analysis and prediction without requiring explicit knowledge of the switching boundaries. 

### From Idealized Models to Real-World Data

The theoretical elegance of [operator-theoretic models](@entry_id:1129150) must confront the practical realities of noisy, incomplete, and finite data. This section delves into the techniques that bridge this gap, addressing robust [parameter estimation](@entry_id:139349) from input-output measurements, the explicit modeling of stochasticity, and the reconstruction of system dynamics from limited sensor information.

#### System Identification from Input-Output Data

Before the advent of modern Koopman-based learning methods, the field of system identification developed a rich set of data-driven techniques for modeling linear time-invariant (LTI) systems. Subspace identification methods, such as the Eigensystem Realization Algorithm (ERA), are a cornerstone of this field. These methods estimate a [state-space model](@entry_id:273798) $(A, B, C, D)$ directly from input-output data, often starting from the system's impulse response (Markov parameters). The central mathematical tool is the block Hankel matrix, which arranges the impulse response sequence in a way that reveals the system's underlying structure. The rank of a sufficiently large Hankel matrix corresponds to the order of the minimal system realization. From an operator-theoretic perspective, this finite data matrix is a section of the infinite-dimensional Hankel operator, which maps past inputs to future outputs. The [singular value decomposition](@entry_id:138057) (SVD) of the Hankel matrix provides bases for the range of the observability and [reachability](@entry_id:271693) operators, effectively identifying the state space from data. ERA leverages this decomposition and the shift-invariant structure of the dynamics to recover a balanced [state-space realization](@entry_id:166670). These classical methods provide a rigorous, non-iterative foundation for [data-driven modeling](@entry_id:184110) and serve as a conceptual precursor to more general operator-theoretic techniques. 

#### Handling Stochasticity and Noise

Real-world systems are invariably subject to noise and unmodeled disturbances. For a system with additive process noise, $x_{k+1} = f(x_t) + \xi_t$, the deterministic Koopman operator is replaced by its stochastic counterpart. The stochastic Koopman operator is formally defined via [conditional expectation](@entry_id:159140): its action on an observable $g$ is given by $(\mathcal{K}g)(x) = \mathbb{E}[g(x_{k+1}) | x_k = x]$. This means the operator averages the value of the observable at the next time step over all possible realizations of the noise process. For additive noise, this amounts to a convolution of the observable with the noise's probability distribution. 

This stochasticity has profound practical consequences. The averaging effect introduced by the noise acts as a diffusion or smoothing term on the dynamics. This process is dissipative, meaning it breaks the time-reversal symmetry that might exist in the underlying [deterministic system](@entry_id:174558). As a result, eigenvalues of the Koopman operator that would lie on the unit circle for a conservative [deterministic system](@entry_id:174558) are moved inside the unit circle, corresponding to decaying modes. In the frequency domain, this manifests as a broadening of the spectral peaks associated with Koopman eigenfunctions, with the width of the peaks related to the strength of the noise. When estimating the operator from finite, noisy data using methods like DMD, this leads to two main challenges: the estimated eigenvalues are often biased towards the origin, and the estimation itself has high variance. To combat these issues, a variety of [regularization techniques](@entry_id:261393) are employed. These include Tikhonov regularization ([ridge regression](@entry_id:140984)), which stabilizes the [matrix inversion](@entry_id:636005) in the [least-squares problem](@entry_id:164198); [total least squares](@entry_id:170210) (TLS) or [errors-in-variables](@entry_id:635892) models, which account for noise in both the regression inputs and outputs to reduce bias; and covariance [shrinkage methods](@entry_id:167472), which improve the conditioning of the sample covariance matrices used in the estimation. 

Beyond [process noise](@entry_id:270644), measurement noise also presents a significant challenge. When identifying a model structure that includes past outputs in its regressors, colored measurement noise can induce a correlation between the regressors and the error term, leading to biased estimates if standard [least squares](@entry_id:154899) is used. The Instrumental Variables (IV) method is a classical statistical technique designed to resolve this issue by introducing "instruments" that are correlated with the regressors but uncorrelated with the noise. For complex cases with colored noise, an iterative approach is highly effective. This involves first obtaining an initial consistent estimate of the system parameters, using these to estimate the noise characteristics, pre-filtering the data to "whiten" the noise, and then constructing improved instruments to re-estimate the system parameters. This [iterative refinement](@entry_id:167032) of both the system and noise models can lead to highly accurate and consistent estimates even in challenging noise environments. 

#### State Reconstruction from Partial Observations

In many practical scenarios, the full state of a system is not accessible, and only one or a few scalar measurements are available. Operator-theoretic identification requires a set of observables that can adequately describe the system's state. Time-delay embedding offers a powerful and theoretically grounded method to construct such observables from a single time series. By stacking time-delayed measurements of an output $y_k = h(x_k)$ into a vector, $z_k = [y_k, y_{k+1}, \dots, y_{k+m-1}]^\top$, we are effectively creating a dictionary of [observables](@entry_id:267133) $\{h, h \circ F, \dots, h \circ F^{m-1}\}$, where $F$ is the system's state-transition map. This dictionary has a special shift-invariant structure under the action of the Koopman operator, making it an excellent basis for approximation via EDMD. According to Takens' theorem, if the [embedding dimension](@entry_id:268956) $m$ is sufficiently large (generically, $m \ge 2d+1$ for a $d$-dimensional system), this delay-[coordinate map](@entry_id:154545) provides an embedding of the original system's attractor. This means the time-delay vectors can serve as a proxy for the unobserved state, allowing for the reconstruction and prediction of the system's dynamics from limited measurements. 

### Advanced Architectures and Physics-Informed Learning

The fusion of [operator theory](@entry_id:139990) with deep learning has given rise to a new generation of powerful modeling tools. These methods leverage the [expressive power](@entry_id:149863) of neural networks to learn complex dynamics, including those governed by partial differential equations, and can be designed to explicitly incorporate prior physical knowledge, leading to models that are not only accurate but also robust and physically plausible.

#### Learning Continuous-Time Dynamics with Neural Networks

For systems governed by continuous-time [ordinary differential equations](@entry_id:147024) (ODEs), two prominent learning paradigms have emerged. Neural Ordinary Differential Equations (Neural ODEs) directly parameterize the unknown vector field of the dynamics with a neural network, $\dot{x} = f_\theta(x, u, t)$. To train the model, this ODE is numerically integrated to produce a trajectory, which is then compared against measurement data. The parameters $\theta$ are updated by backpropagating gradients through the ODE solver, often using the [adjoint sensitivity method](@entry_id:181017) for efficiency. In contrast, Physics-Informed Neural Networks (PINNs) parameterize the solution trajectory itself with a neural network, $x_\phi(t)$. Instead of integrating an equation, a PINN is trained by minimizing a composite loss function that includes not only the mismatch with measurement data but also a "physics residual" term. This residual penalizes the extent to which the learned solution $x_\phi(t)$ violates the governing differential equation, with derivatives computed via [automatic differentiation](@entry_id:144512). PINNs thus turn the problem of solving or identifying a DE into a multi-objective optimization problem, directly enforcing the physical laws as soft constraints. 

#### Injecting Physical Priors: Invariants and Stability

A key advantage of learning-based modeling is the ability to seamlessly integrate domain knowledge into the training process. For physical systems, this knowledge often comes in the form of conservation laws (invariants) or stability properties. For example, the total energy of a Hamiltonian system should be conserved, or a dissipative system should converge to an equilibrium. These principles can be formulated in operator-theoretic terms using the [infinitesimal generator](@entry_id:270424) $\mathcal{L}_f$, where an invariant $I(x)$ satisfies $\mathcal{L}_f I(x) = 0$ and a Lyapunov function $V(x)$ satisfies a decrease condition like $\mathcal{L}_f V(x) \le 0$. By adding penalty terms to the training loss that enforce these conditions on the learned model $f_\theta$, we regularize the learning problem. This regularization constrains the [hypothesis space](@entry_id:635539) to functions that are physically plausible, reducing overfitting and improving generalization. Enforcing invariants ensures that learned trajectories do not drift away from the physically feasible manifolds, drastically improving long-horizon prediction accuracy. Enforcing stability via a Lyapunov-based penalty ensures the learned model is not spuriously unstable and inherits the robustness of the true system, leading to better generalization bounds and more reliable behavior, which is especially critical for CPS. 

#### Learning Operators for Distributed Systems

Many critical systems in science and engineering are governed by partial differential equations (PDEs), where the state is a field defined over a spatial domain. Modeling such systems requires learning operators that map between infinite-dimensional [function spaces](@entry_id:143478). Traditional neural networks, which map between fixed-size vectors, are ill-suited for this task as their performance is tied to the resolution of the specific grid they were trained on. Neural operators have emerged to address this challenge by learning mappings that are discretization-invariant. Prominent architectures include the Deep Operator Network (DeepONet) and the Fourier Neural Operator (FNO). 

The FNO, for instance, learns a solution operator by performing key parts of its computation in the Fourier domain. It approximates the action of a general [integral operator](@entry_id:147512) by parameterizing a transformation on the Fourier modes of the input function. By leveraging the [convolution theorem](@entry_id:143495), which states that convolution in the spatial domain is equivalent to multiplication in the Fourier domain, the FNO learns a global, nonlocal kernel. Crucially, because the learned parameters are associated with specific Fourier modes rather than grid points, the trained operator can be evaluated on any discretization of the domain, achieving mesh-independent learning.  This property is transformative, as it allows for a [sample complexity](@entry_id:636538) that depends on the number of training functions, but not on the resolution at which they are observed. This circumvents the curse of dimensionality associated with discretizing PDEs, enabling the learning of complex physical operators that can be reused across different simulation and experimental setups. 

### The Digital Twin Lifecycle: From Identification to Maintenance

A digital twin is not a static artifact; it is a dynamic model that must be built, validated, and maintained throughout the operational life of its physical counterpart. This lifecycle perspective involves practical considerations of model complexity, data quality, and adaptive updating.

#### The Role of Regularization in Practice

When using EDMD with a rich dictionary of [observables](@entry_id:267133), the number of lifted states ($m$) can easily exceed the number of available data snapshots ($N$), leading to an overparameterized or underdetermined regression problem. In this regime, the unregularized [least-squares](@entry_id:173916) estimate for the Koopman matrix is ill-posed, highly sensitive to noise, and generalizes poorly. Regularization is essential for obtaining a meaningful and predictive model. $\ell_2$ regularization ([ridge regression](@entry_id:140984)) adds a penalty on the squared Frobenius norm of the Koopman matrix, which stabilizes the estimation process and shrinks the solution towards zero, trading a small amount of bias for a large reduction in variance. $\ell_1$ regularization (Lasso) promotes sparsity in the operator, effectively performing [model selection](@entry_id:155601) by identifying the most influential observables. The amount of regularization is a critical hyperparameter that must be tuned carefully. Since data from dynamical systems is temporally correlated, standard random [cross-validation](@entry_id:164650) is inappropriate. Instead, [blocked cross-validation](@entry_id:1121714), which partitions the [time-series data](@entry_id:262935) into contiguous segments, should be used to obtain a realistic estimate of the model's generalization performance and select the optimal regularization strength. 

#### Fidelity, Data Regimes, and Model Updating

The ultimate fidelity of a digital twin—its multi-step predictive accuracy—depends on a delicate balance. The choice of model structure (e.g., the dictionary $\Psi$) creates a bias-variance trade-off: a more expressive dictionary reduces [structural bias](@entry_id:634128) but increases the risk of overfitting (high variance) from finite data. The quality of the identification data is equally crucial. A small dataset with rich, persistently exciting inputs may allow for the identification of a generalizable, low-complexity model, especially if guided by physical priors. Conversely, a massive dataset collected under narrow operating conditions with limited excitation may not be sufficient to identify a model that generalizes well, as the data simply does not contain the necessary information about the system's behavior across its full operational envelope.

A principled workflow for developing and maintaining a digital twin must account for these factors. It begins with initializing a model structure using available physical priors and constraints. A data-driven identification phase follows, where the model dictionary and regularization hyperparameters are systematically selected using cross-validation to optimize the [bias-variance trade-off](@entry_id:141977). Once deployed, the digital twin's performance must be continuously monitored by analyzing its prediction residuals. Statistical [change-point detection](@entry_id:172061) algorithms can identify when the physical system's behavior has drifted significantly from the model's predictions. In response to slow drifts, the model can be efficiently updated online using [recursive estimation](@entry_id:169954) techniques like [recursive least squares](@entry_id:263435) with a [forgetting factor](@entry_id:175644), which allows the model to adapt by giving more weight to recent data. For abrupt, significant changes, a more comprehensive revision of the model structure and a full re-identification may be required. This adaptive lifecycle ensures that the digital twin remains a faithful and valuable counterpart to the physical system over time. 